{"Filename": "CategoricalDataAnalysis", "Pages": [{"Page_number": 1, "text": " "}, {"Page_number": 2, "text": "categorical data analysis\n\n "}, {"Page_number": 3, "text": "p&s-cp.qxd  12/4/02  10:27 am  page 1\n\nwiley series in probability and statistics\n\nestablished by walter a. shewhart and samuel s. wilks\n\neditors: david j. balding, peter bloomfield, noel a. c. cressie, \nnicholas i. fisher, iain m. johnstone, j. b. kadane, louise m. ryan, \ndavid w. scott, adrian f. m. smith, jozef l. teugels\neditors emeriti: vic barnett, j. stuart hunter, david g. kendall\n\na complete list of the titles in this series appears at the end of this volume.\n\n "}, {"Page_number": 4, "text": "categorical data analysis\nsecond edition\n\nalan agresti\nuniversity of florida\ngainesville, florida\n\n "}, {"Page_number": 5, "text": "\u2b01\nthis book is printed on acid-free paper. \"\ncopyright \u429a 2002 john wiley & sons, inc., hoboken, new jersey. all rights reserved.\n\npublished simultaneously in canada.\n\nno part of this publication may be reproduced, stored in a retrieval system or transmitted in any\nform or by any means, electronic, mechanical, photocopying, recording, scanning or otherwise,\nexcept as permitted under section 107 or 108 of the 1976 united states copyright act, without\neither the prior written permission of the publisher, or authorization through payment of the\nappropriate per-copy fee to the copyright clearance center, 222 rosewood drive, danvers, ma\n01923, 978 750-8400, fax 978 750-4744. requests to the publisher for permission should be\naddressed to the permissions department, john wiley & sons, inc., 605 third avenue, new\nyork, ny 10158-0012, 212 850-6011, fax 212 850-6008, e-mail: permreq@wiley.com.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nfor ordering and customer service, call 1-800-call-wiley.\nlibrary of congress cataloging-in-publication data is a\u00a9ailable\n\nisbn 0-471-36093-7\n\nprinted in the united states of america\n\n10 9 8 7 6 5 4 3 2 1\n\n "}, {"Page_number": 6, "text": "to jacki\n\n "}, {"Page_number": 7, "text": "contents\n\npreface\n\n1.\n\nintroduction: distributions and inference for categorical data\n1.1 categorical response data, 1\n1.2 distributions for categorical data, 5\n1.3 statistical inference for categorical data, 9\n1.4 statistical inference for binomial parameters, 14\n1.5 statistical inference for multinomial parameters, 21\nnotes, 26\nproblems, 28\n\n2. describing contingency tables\n\n2.1 probability structure for contingency tables, 36\n2.2 comparing two proportions, 43\n2.3 partial association in stratified 2 = 2 tables, 47\n2.4 extensions for i = j tables, 54\nnotes, 59\nproblems, 60\n\n3.\n\ninference for contingency tables\n3.1 confidence intervals for association parameters, 70\n3.2 testing independence in two-way contingency\n\ntables, 78\n\n3.3 following-up chi-squared tests, 80\n3.4 two-way tables with ordered classifications, 86\n3.5 small-sample tests of independence, 91\n\nxiii\n\n1\n\n36\n\n70\n\nvii\n\n "}, {"Page_number": 8, "text": "viii\n\n4.\n\ncontents\n\n3.6 small-sample confidence intervals for 2 = 2 tables,* 98\n3.7 extensions for multiway tables and nontabulated\n\nresponses, 101\n\nnotes, 102\nproblems, 104\n\n115\n\nintroduction to generalized linear models\n4.1 generalized linear model, 116\n4.2 generalized linear models for binary data, 120\n4.3 generalized linear models for counts, 125\n4.4 moments and likelihood for generalized linear\n\nmodels,* 132\ninference for generalized linear models, 139\n\n4.5\n4.6 fitting generalized linear models, 143\n4.7 quasi-likelihood and generalized linear models,* 149\n4.8 generalized additive models,* 153\nnotes, 155\nproblems, 156\n\n5. logistic regression\n\n165\n\ninterpreting parameters in logistic regression, 166\ninference for logistic regression, 172\n\n5.1\n5.2\n5.3 logit models with categorical predictors, 177\n5.4 multiple logistic regression, 182\n5.5 fitting logistic regression models, 192\nnotes, 196\nproblems, 197\n\n6. building and applying logistic regression models\n\n211\n\n6.1 strategies in model selection, 211\n6.2 logistic regression diagnostics, 219\n6.3\n\ninference about conditional associations in 2 = 2 = k\ntables, 230\n\n6.4 using models to improve inferential power, 236\n6.5 sample size and power considerations,* 240\n6.6 probit and complementary log-log models,* 245\n\n*sections marked with an asterisk are less important for an overview.\n\n "}, {"Page_number": 9, "text": "contents\n\n6.7 conditional logistic regression and exact\n\ndistributions,* 250\n\nnotes, 257\nproblems, 259\n\n7. logit models for multinomial responses\n\nix\n\n267\n\n7.1 nominal responses: baseline-category logit models, 267\n7.2 ordinal responses: cumulative logit models, 274\n7.3 ordinal responses: cumulative link models, 282\n7.4 alternative models for ordinal responses,* 286\n7.5 testing conditional independence in i = j = k\n\ntables,* 293\n\n7.6 discrete-choice multinomial logit models,* 298\nnotes, 302\nproblems, 302\n\n8. loglinear models for contingency tables\n\n314\n\n8.1 loglinear models for two-way tables, 314\n8.2 loglinear models for independence and interaction in\n\nthree-way tables, 318\ninference for loglinear models, 324\n\n8.3\n8.4 loglinear models for higher dimensions, 326\n8.5 the loglinear\u1390logit model connection, 330\n8.6 loglinear model fitting: likelihood equations and\n\nasymptotic distributions,* 333\n\n8.7 loglinear model fitting: iterative methods and their\n\napplication,* 342\n\nnotes, 346\nproblems, 347\n\n9. building and extending loglinearrrrrrlogit models\n9.1 association graphs and collapsibility, 357\n9.2 model selection and comparison, 360\n9.3 diagnostics for checking models, 366\n9.4 modeling ordinal associations, 367\n9.5 association models,* 373\n9.6 association models, correlation models, and\n\ncorrespondence analysis,* 379\n\n357\n\n "}, {"Page_number": 10, "text": "x\n\ncontents\n\n9.7 poisson regression for rates, 385\n9.8 empty cells and sparseness in modeling contingency\n\ntables, 391\n\nnotes, 398\nproblems, 400\n\n10. models for matched pairs\n\n409\n\n10.1 comparing dependent proportions, 410\n10.2 conditional logistic regression for binary matched\n\npairs, 414\n\n10.3 marginal models for square contingency tables, 420\n10.4 symmetry, quasi-symmetry, and quasi-\n\nindependence, 423\n\n10.5 measuring agreement between observers, 431\n10.6 bradley\u1390terry model for paired preferences, 436\n10.7 marginal models and quasi-symmetry models for\n\nmatched sets,* 439\n\nnotes, 442\nproblems, 444\n\n11. analyzing repeated categorical response data\n\n455\n\n11.1 comparing marginal distributions: multiple\n\nresponses, 456\n\n11.2 marginal modeling: maximum likelihood approach, 459\n11.3 marginal modeling: generalized estimating equations\n\napproach, 466\n\n11.4 quasi-likelihood and its gee multivariate extension:\n\ndetails,* 470\n\n11.5 markov chains: transitional modeling, 476\nnotes, 481\nproblems, 482\n\n12. random effects: generalized linear mixed models for\n\ncategorical responses\n12.1 random effects modeling of clustered categorical\n\ndata, 492\n\n12.2 binary responses: logistic-normal model, 496\n12.3 examples of random effects models for binary\n\ndata, 502\n\n12.4 random effects models for multinomial data, 513\n\n491\n\n "}, {"Page_number": 11, "text": "contents\n\nxi\n\n12.5 multivariate random effects models for binary data,\n\n516\n\n12.6 glmm fitting, inference, and prediction, 520\nnotes, 526\nproblems, 527\n\n13. other mixture models for categorical data*\n\n538\n\n13.1 latent class models, 538\n13.2 nonparametric random effects models, 545\n13.3 beta-binomial models, 553\n13.4 negative binomial regression, 559\n13.5 poisson regression with random effects, 563\nnotes, 565\nproblems, 566\n\n14. asymptotic theory for parametric models\n\n576\n\n14.1 delta method, 577\n14.2 asymptotic distributions of estimators of model\n\nparameters and cell probabilities, 582\n\n14.3 asymptotic distributions of residuals and goodness-\n\nof-fit statistics, 587\n\n14.4 asymptotic distributions for logitrloglinear\n\nmodels, 592\n\nnotes, 594\nproblems, 595\n\n15. alternative estimation theory for parametric models\n\n600\n\n15.1 weighted least squares for categorical data, 600\n15.2 bayesian inference for categorical data, 604\n15.3 other methods of estimation, 611\nnotes, 615\nproblems, 616\n\n16. historical tour of categorical data analysis*\n\n619\n\n16.1 pearson\u1390yule association controversy, 619\n16.2 r. a. fisher\u2019s contributions, 622\n\n "}, {"Page_number": 12, "text": "xii\n\ncontents\n\n16.3 logistic regression, 624\n16.4 multiway contingency tables and loglinear models, 625\n16.5 recent and future? developments, 629\n\n\u017e\n\n.\n\nappendix a. using computer software to analyze categorical data\n\n632\n\nsoftware for categorical data analysis, 632\n\na.1\na.2 examples of sas code by chapter, 634\n\nappendix b. chi-squared distribution values\n\nreferences\n\nexamples index\n\nauthor index\n\nsubject index\n\n654\n\n655\n\n689\n\n693\n\n701\n\n "}, {"Page_number": 13, "text": "preface\n\nthe explosion in the development of methods for analyzing categorical data\nthat began in the 1960s has continued apace in recent years. this book\nprovides an overview of these methods, as well as older, now standard,\nmethods. it gives special emphasis to generalized linear modeling techniques,\nwhich extend linear model methods for continuous variables, and their\nextensions for multivariate responses.\n\ntoday, because of this development and the ubiquity of categorical data in\napplications, most statistics and biostatistics departments offer courses on\ncategorical data analysis. this book can be used as a text for such courses.\nthe material in chapters 1\u13907 forms the heart of most courses. chapters 1\u13903\ncover distributions for categorical responses and traditional methods for\ntwo-way contingency tables. chapters 4\u13907 introduce logistic regression and\nrelated logit models for binary and multicategory response variables. chap-\nters 8 and 9 cover loglinear models for contingency tables. over time, this\nmodel class seems to have lost importance, and this edition reduces some-\nwhat its discussion of them and expands its focus on logistic regression.\n\nin the past decade, the major area of new research has been the develop-\nment of methods for repeated measurement and other forms of clustered\ncategorical data. chapters 10\u139013 present these methods, including marginal\nmodels and generalized linear mixed models with random effects. chapters\n14 and 15 present theoretical foundations as well as alternatives to the\nmaximum likelihood paradigm that this text adopts. chapter 16 is devoted to\na historical overview of the development of the methods. it examines contri-\nbutions of noted statisticians, such as pearson and fisher, whose pioneering\nefforts\u138fand sometimes vocal debates\u138fbroke the ground for this evolution.\nevery chapter of the first edition has been extensively rewritten, and some\nsubstantial additions and changes have occurred. the major differences are:\n\n\u4887 a new chapter 1 that introduces distributions and methods of inference\n\nfor categorical data.\n\n\u4887 a unified presentation of models as special cases of generalized linear\n\nmodels, starting in chapter 4 and then throughout the text.\n\nxiii\n\n "}, {"Page_number": 14, "text": "xiv\n\npreface\n\n\u4887 greater emphasis on logistic regression for binary response variables\nand extensions for multicategory responses, with chapters 4\u13907 introduc-\ning models and chapters 10\u139013 extending them for clustered data.\n\n\u4887 three new chapters on methods for clustered, correlated categorical\n\ndata, increasingly important in applications.\n\n\u4887 a new chapter on the historical development of the methods.\n\u4887 more discussion of \u2018\u2018exact\u2019\u2019 small-sample procedures and of conditional\n\nlogistic regression.\n\nin this text, i interpret categorical data analysis to refer to methods for\ncategorical response variables. for most methods, explanatory variables can\nbe qualitative or quantitative, as in ordinary regression. thus, the focus is\nintended to be more general than contingency table analysis, although for\nsimplicity of data presentation, most examples use contingency tables. these\nexamples are often simplistic, but should help readers focus on understand-\ning the methods themselves and make it easier for them to replicate results\nwith their favorite software.\n\nspecial features of the text include:\n\n\u4887 more than 100 analyses of \u2018\u2018real\u2019\u2019 data sets.\n\u4887 more than 600 exercises at the end of the chapters, some directed\ntowards theory and methods and some towards applications and data\nanalysis.\n\n\u4887 an appendix that shows, by chapter, the use of sas for performing\n\nanalyses presented in this book.\n\n\u4887 notes at the end of each chapter that provide references for recent\n\nresearch and many topics not covered in the text.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e .\n\nappendix a summarizes statistical software needed to use the methods\ndescribed in this text. it shows how to use sas for analyses included in the\ntext and refers to a web site www.stat.ufl.edur; aarcdarcda.html\nthat\ncontains 1 information on the use of other software such as r, s-plus,\nspss, and stata , 2 data sets for examples in the form of complete sas\nprograms for conducting the analyses, 3 short answers for many of the\nodd-numbered exercises, 4 corrections of errors in early printings of the\nbook, and 5 extra exercises. i recommend that readers refer to this ap-\npendix or specialized manuals while reading the text, as an aid to implement-\ning the methods.\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\ni intend this book to be accessible to the diverse mix of students who take\ngraduate-level courses in categorical data analysis. but i have also written it\nwith practicing statisticians and biostatisticians in mind. i hope it enables\nthem to catch up with recent advances and learn about methods that\nsometimes receive inadequate attention in the traditional statistics curricu-\nlum.\n\n "}, {"Page_number": 15, "text": "preface\n\nxv\n\n\u017e\n\nthe development of new methods has influenced \u138fand been influenced\nby\u138fthe increasing availability of data sets with categorical responses in the\nsocial, behavioral, and biomedical sciences, as well as in public health, human\ngenetics, ecology, education, marketing, and industrial quality control. and\nso, although this book is directed mainly to statisticians and biostatisticians, i\nalso aim for it to be helpful to methodologists in these fields.\n\nreaders should possess a background that includes regression and analysis\nof variance models, as well as maximum likelihood methods of statistical\ntheory. those not having much theory background should be able to follow\nmost methodological discussions. sections and subsections marked with an\nasterisk are less important for an overview. readers with mainly applied\ninterests can skip most of chapter 4 on the theory of generalized linear\nmodels and proceed to other chapters. however, the book has distinctly\nhigher technical level and is more thorough and complete than my lower-level\n.\ntext, an introduction to categorical data analysis wiley, 1996 .\n\ni thank those who commented on parts of the manuscript or provided help\nof some type. special thanks to bernhard klingenberg, who read several\nchapters carefully and made many helpful suggestions, yongyi min, who\nconstructed many of the figures and helped with some software, and brian\ncaffo, who helped with some examples. many thanks to rosyln stone and\nbrian marx for each reviewing half the manuscript and brian caffo, i-ming\nliu, and yongyi min for giving insightful comments on several chapters.\nthanks to constantine gatsonis and his students for using a draft in a course\nat brown university and providing suggestions. others who provided com-\nments on chapters or help of some type include patricia altham, wicher\nbergsma, jane brockmann, brent coull, al demaris, regina dittrich,\njianping dong, herwig friedl, ralitza gueorguieva, james hobert, walter\nkatzenbeisser, harry khamis, svend kreiner, joseph lang, jason liao,\nmojtaba ganjali, jane pendergast, michael radelet, kenneth small, maura\nstokes, tom ten have, and rongling wu. i thank my co-authors on various\nprojects, especially brent coull, joseph lang, james booth, james hobert,\nbrian caffo, and ranjini natarajan, for permission to use material from\nthose articles. thanks to the many who reviewed material or suggested\nexamples for the first edition, mentioned in the preface of that edition.\nthanks also to wiley executive editor steve quigley for his steadfast\nencouragement and facilitation of this project. finally, thanks to my wife\njacki levine for continuing support of all kinds, despite the many days this\nwork has taken from our time together.\n\ngaines\u00aeille, florida\nno\u00aeember 2001\n\nalan agresti\n\n "}, {"Page_number": 16, "text": "c h a p t e r 1\n\nintroduction: distributions and\ninference for categorical data\n\nfrom helping to assess the value of new medical treatments to evaluating the\nfactors that affect our opinions and behaviors, analysts today are finding\nmyriad uses for categorical data methods. in this book we introduce these\nmethods and the theory behind them.\n\nstatistical methods for categorical responses were late in gaining the level\nof sophistication achieved early in the twentieth century by methods for\ncontinuous responses. despite influential work around 1900 by the british\nstatistician karl pearson, relatively little development of models for categori-\ncal responses occurred until the 1960s. in this book we describe the early\nfundamental work that still has importance today but place primary emphasis\non more recent modeling approaches. before outlining the topics covered, we\ndescribe the major types of categorical data.\n\n1.1 categorical response data\na categorical \u00aeariable has a measurement scale consisting of a set of cate-\ngories. for instance, political philosophy is often measured as liberal, moder-\nate, or conservative. diagnoses regarding breast cancer based on a mammo-\ngram use the categories normal, benign, probably benign, suspicious, and\nmalignant.\n\nthe development of methods for categorical variables was stimulated by\nresearch studies in the social and biomedical sciences. categorical scales are\npervasive in the social sciences for measuring attitudes and opinions. cate-\ngorical scales in biomedical sciences measure outcomes such as whether a\nmedical treatment is successful.\n\nalthough categorical data are common in the social and biomedical\nsciences, they are by no means restricted to those areas. they frequently\n\n1\n\n "}, {"Page_number": 17, "text": "2\n\nintroduction: distributions and inference for categorical data\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\noccur in the behavioral sciences e.g., type of mental illness, with the cate-\ngories schizophrenia, depression, neurosis , epidemiology and public health\n\u017ee.g., contraceptive method at last intercourse, with the categories none,\n.\ncondom, pill, iud, other , genetics type of allele inherited by an offspring ,\nzoology e.g., alligators\u2019 primary food preference, with the categories fish,\ninvertebrate, reptile , education e.g., student responses to an exam question,\nwith the categories correct and incorrect , and marketing e.g., consumer\npreference among leading brands of a product, with the categories brand a,\nbrand b, and brand c . they even occur in highly quantitative fields such as\nengineering sciences and industrial quality control. examples are the classifi-\ncation of items according to whether they conform to certain standards, and\nsubjective evaluation of some characteristic: how soft to the touch a certain\nfabric is, how good a particular food product tastes, or how easy to perform a\nworker finds a certain task to be.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\ncategorical variables are of many types. in this section we provide ways of\n\nclassifying them and other variables.\n\n\u017e\n\n1.1.1 response\u2013explanatory variable distinction\nmost statistical analyses distinguish between response or dependent \u00aeariables\nand explanatory or independent \u00aeariables. for instance, regression models\ndescribe how the mean of a response variable, such as the selling price of a\nhouse, changes according to the values of explanatory variables, such as\nsquare footage and location. in this book we focus on methods for categorical\nresponse variables. as in ordinary regression, explanatory variables can be of\nany type.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n1.1.2 nominal\u2013ordinal scale distinction\ncategorical variables have two primary types of scales. variables having\ncategories without a natural ordering are called nominal. examples are\nreligious affiliation with the categories catholic, protestant, jewish, muslim,\nother , mode of transportation to work automobile, bicycle, bus, subway,\nwalk , favorite type of music classical, country, folk, jazz, rock , and choice of\nresidence apartment, condominium, house, other . for nominal variables,\nthe order of listing the categories is irrelevant. the statistical analysis does\nnot depend on that ordering.\n\n.\n.\n\nlarge , social class upper, middle,\n\nmany categorical variables do have ordered categories. such variables are\ncalled ordinal. examples are size of automobile subcompact, compact,\nmidsize,\nlower , political philosophy\n\u017e\nliberal, moderate, conservative , and patient condition good, fair, serious,\ncritical . ordinal variables have ordered categories, but distances between\ncategories are unknown. although a person categorized as moderate is more\nliberal than a person categorized as conservative, no numerical value de-\nscribes how much more liberal that person is. methods for ordinal variables\nutilize the category ordering.\n\n\u017e\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n "}, {"Page_number": 18, "text": "\u017e\n\n.\n\ncategorical response data\n\n3\nan inter\u00aeal \u00aeariable is one that does have numerical distances between any\ntwo values. for example, blood pressure level, functional\nlife length of\ntelevision set, length of prison term, and annual income are interval vari-\nables. an internal variable is sometimes called a ratio \u00aeariable if ratios of\nvalues are also valid.\n\nthe way that a variable is measured determines its classification. for\nexample,\n\u2018\u2018education\u2019\u2019 is only nominal when measured as public school or\nprivate school; it is ordinal when measured by highest degree attained, using\nthe categories none, high school, bachelor\u2019s, master\u2019s, and doctorate; it is\ninterval when measured by number of years of education, using the integers\n0, 1, 2, . . . .\n\na variable\u2019s measurement scale determines which statistical methods are\nappropriate. in the measurement hierarchy, interval variables are highest,\nordinal variables are next, and nominal variables are lowest. statistical\nmethods for variables of one type can also be used with variables at higher\nlevels but not at lower levels. for instance, statistical methods for nominal\nvariables can be used with ordinal variables by ignoring the ordering of\ncategories. methods for ordinal variables cannot, however, be used with\nnominal variables, since their categories have no meaningful ordering. it is\nusually best to apply methods appropriate for the actual scale.\n\nsince this book deals with categorical responses, we discuss the analysis of\nnominal and ordinal variables. the methods also apply to interval variables\nhaving a small number of distinct values e.g., number of times married or\nfor which the values are grouped into ordered categories e.g., education\n.\nmeasured as - 10 years, 10\u139012 years, ) 12 years .\n\n\u017e\n\n.\n\n\u017e\n\n1.1.3 continuous\u2013discrete variable distinction\nvariables are classified as continuous or discrete, according to the number of\nvalues they can take. actual measurement of all variables occurs in a discrete\nmanner, due to precision limitations in measuring instruments. the continu-\nous\u1390discrete classification, in practice, distinguishes between variables that\ntake lots of values and variables that take few values. for instance, statisti-\ncians often treat discrete interval variables having a large number of values\n\u017e\nsuch as test scores as continuous, using them in methods for continuous\nresponses.\n\n\u017e .\nthis book deals with certain types of discretely measured responses: 1\nnominal variables, 2 ordinal variables, 3 discrete interval variables having\nrelatively few values, and 4 continuous variables grouped into a small\nnumber of categories.\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\n.\n\n1.1.4 quantitative\u2013qualitative variable distinction\nnominal variables are qualitati\u00aee\u138fdistinct categories differ in quality, not in\nquantity. interval variables are quantitati\u00aee\u138fdistinct levels have differing\namounts of the characteristic of interest. the position of ordinal variables in\n\n "}, {"Page_number": 19, "text": "4\n\nintroduction: distributions and inference for categorical data\n\nthe quantitative\u1390qualitative classification is fuzzy. analysts often treat\nthem as qualitative, using methods for nominal variables. but in many\nrespects, ordinal variables more closely resemble interval variables than they\nresemble nominal variables. they possess important quantitative features:\neach category has a greater or smaller magnitude of the characteristic than\nanother category; and although not possible to measure, an underlying\ncontinuous variable is usually present. the political philosophy classification\n\u017e\nliberal, moderate, conservative crudely measures an inherently continuous\ncharacteristic.\n\nanalysts often utilize the quantitative nature of ordinal variables by\nassigning numerical scores to categories or assuming an underlying continu-\nous distribution. this requires good judgment and guidance from researchers\nwho use the scale, but it provides benefits in the variety of methods available\nfor data analysis.\n\n.\n\n1.1.5 organization of this book\nthe models for categorical response variables discussed in this book resem-\nble regression models for continuous response variables; however,\nthey\nassume binomial, multinomial, or poisson response distributions instead of\nnormality. two types of models receive special attention, logistic regression\nand loglinear models. ordinary logistic regression models, also called logit\nmodels, apply with binary i.e., two-category responses and assume a bino-\nmial distribution. generalizations of logistic regression apply with multicate-\ngory responses and assume a multinomial distribution. loglinear models\napply with count data and assume a poisson distribution. certain equiva-\nlences exist between logistic regression and loglinear models.\n\n\u017e\n\n.\n\nthe book has four main units. in the first, chapters 1 through 3, we\nsummarize descriptive and inferential methods for univariate and bivariate\ncategorical data. these chapters cover discrete distributions, methods of\ninference, and analyses for measures of association. they summarize the\nnon-model-based methods developed prior to about 1960.\n\nin the second and primary unit, chapters 4 through 9, we introduce\nmodels for categorical responses. in chapter 4 we describe a class of\ngeneralized linear models having models of this text as special cases. we focus\non models for binary and count response variables. chapters 5 and 6 cover\nthe most important model for binary responses, logistic regression. in chap-\nter 7 we present generalizations of that model for nominal and ordinal\nmulticategory response variables. in chapter 8 we introduce the modeling of\nmultivariate categorical response data and show how to represent association\nand interaction patterns by loglinear models for counts in the table that\ncross-classifies those responses. in chapter 9 we discuss model building with\nloglinear and related logistic models and present some related models.\n\nin the third unit, chapters 10 through 13, we discuss models for handling\nrepeated measurement and other forms of clustering. in chapter 10 we\n\n "}, {"Page_number": 20, "text": "distributions for categorical data\n\n5\n\npresent models for a categorical response with matched pairs; these apply,\nfor instance, with a categorical response measured for the same subjects at\ntwo times. chapter 11 covers models for more general types of repeated\ncategorical data, such as longitudinal data from several times with explana-\ntory variables. in chapter 12 we present a broad class of models, generalized\nlinear mixed models, that use random effects to account for dependence with\nsuch data. in chapter 13 further extensions and applications of the models\nfrom chapters 10 through 12 are described.\n\nthe fourth and final unit is more theoretical. in chapter 14 we develop\nasymptotic theory for categorical data models. this theory is the basis for\nlarge-sample behavior of model parameter estimators and goodness-of-fit\nstatistics. maximum likelihood estimation receives primary attention here\nand throughout the book, but chapter 15 covers alternative methods of\nestimation, such as the bayesian paradigm. chapter 16 stands alone from the\nothers, being a historical overview of the development of categorical data\nmethods.\n\nmost categorical data methods require extensive computations, and statis-\ntical software is necessary for their effective use. in appendix a we discuss\nsoftware that can perform the analyses in this book and show the use of sas\nfor text examples. see the web site www.stat.ufl.edur; aarcdarcda.html to\ndownload sample programs and data sets and find information about other\nsoftware.\n\nchapter 1 provides background material. in section 1.2 we review the key\ndistributions for categorical data: the binomial, multinomial, and poisson. in\nsection 1.3 we review the primary mechanisms for statistical inference, using\nmaximum likelihood. in sections 1.4 and 1.5 we illustrate these by presenting\nsignificance tests and confidence intervals for binomial and multinomial\nparameters.\n\n1.2 distributions for categorical data\n\ninferential data analyses require assumptions about the random mechanism\nthat generated the data. for regression models with continuous responses,\nthe normal distribution plays the central role. in this section we review the\nthree key distributions for categorical responses: binomial, multinomial, and\npoisson.\n\n1.2.1 binomial distribution\nmany applications refer to a fixed number n of binary observations. let\ny , y , . . . , y denote responses for n independent and identical trials such\n1\nthat p y s 1 s \u2432 and p y s 0 s 1 y \u2432. we use the generic labels\n\u2018\u2018success\u2019\u2019 and \u2018\u2018failure\u2019\u2019 for outcomes 1 and 0. identical trials means that the\nprobability of success \u2432 is the same for each trial. independent trials means\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nn\n\n2\n\ni\n\ni\n\n "}, {"Page_number": 21, "text": "6\n\nintroduction: distributions and inference for categorical data\n\n\u0004\n\n4\n\nthat the y are independent random variables. these are often called\nbernoulli trials. the total number of successes, y s \u00fdn y , has the binomial\n.\ndistribution with index n and parameter \u2432, denoted by bin n,\u2432 .\n\nis1\n\ni\u017e\n\ni\n\nthe probability mass function for the possible outcomes y for y is\n\np y s\n\u017e\n\n.\n\n\u017e\n\nn\n\n/y\n\ny\n\n\u017e\n\n\u2432 1 y \u2432\n/y\n\n\u017e\n\nn\n\nwhere the binomial coefficient\ns 1 = \u2432q 0 = 1 y \u2432 s \u2432,\n\n.\n\n\u017e\n\nnyy\n\n.\n\n,\n\ny s 0, 1, 2, . . . , n,\n\n\u017e\n\n1.1\n\n.\n\ns n!r y! n y y ! . since e y s e y\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n. x\n\nw\n\ni\n\n2\n\n.\n\ni\n\ne y s \u2432 and var y s \u2432 1 y \u2432 .\n\u017e\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\ni\n\ni\n\nthe binomial distribution for y s \u00fd y has mean and variance\n\ni\n\ni\n\n3\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u242es e y s n\u2432 and \u24342 s var y s n\u2432 1 y \u2432 .\n.\n. '\n\nthe skewness is described by e y y \u242e r\u2434 s 1 y 2\u2432 r n\u2432 1 y \u2432 .\n\u017e\n.\nthe distribution converges to normality as n increases, for fixed \u2432.\n\nthere is no guarantee that successive binary observations are independent\nor identical. thus, occasionally, we will utilize other distributions. one such\ncase is sampling binary outcomes without replacement from a finite popula-\ntion, such as observations on gender for 10 students sampled from a class of\nsize 20. the hypergeometric distribution, studied in section 3.5.1,\nis then\nrelevant. in section 1.2.4 we mention another case that violates these\nbinomial assumptions.\n\n\u017e\n\n3\n\n\u017e\n\n1.2.2 multinomial distribution\nsome trials have more than two possible outcomes. suppose that each of n\nindependent, identical trials can have outcome in any of c categories. let\ny s 1 if trial\ni has outcome in category j and y s 0 otherwise. then\ni j\ny s y , y , . . . , y\nrepresents a multinomial trial, with \u00fd y s 1; for\nic\ni\n.\ninstance, 0, 0, 1, 0 denotes outcome in category 3 of four possible categories.\nnote that y\nis redundant, being linearly dependent on the others. let\nn s \u00fd y denote the number of trials having outcome in category j. the\ncounts n , n , . . . , n\nlet \u2432 s p y s 1 denote the probability of outcome in category j for\n\nhave the multinomial distribution.\n\ni j\u017e\n\n.\n.\n\ni2\n\n.\n\n\u017e\n\n\u017e\n\ni1\n\nic\n\ni j\n\ni j\n\n1\n\n2\n\nc\n\ni\n\nj\n\nj\n\nj\n\ni j\n\neach trial. the multinomial probability mass function is\n\n\u017e\np n , n , . . . , n\n\n2\n\n1\n\ns\n\n.\n\ncy1\n\n\u017e\n\nn!\n\nn ! n ! \u2b48\u2b48\u2b48 n !\n1\nc\n\n2\n\n/\n\n\u2432 \u2432 \u2b48\u2b48\u2b48 \u2432 .\n\n1\n\n2\n\nc\n\nn\nc\n\nn\n1\n\nn\n2\n\n\u017e\n\n1.2\n\n.\n\n "}, {"Page_number": 22, "text": "distributions for categorical data\n\n7\ncy1 -dimensional, with n s n y n q \u2b48\u2b48\u2b48\n\n.\n\n\u017e\n\n1\n\n. the binomial distribution is the special case with c s 2.\n\nc\n\nj\n\nj\n\n\u017e\n\nis\n\nthis\n\nsince \u00fd n s n,\n.qn\ncy1\nfor the multinomial distribution,\ne n s n\u2432 ,\n\u017e\n\n\u017e\n\n.\n\nj\n\nj\n\nvar n s n\u2432 1 y \u2432 ,\n.\n\n\u017e\n\n.\n\nj\n\nj\n\nj\n\ncov n , n s yn\u2432\u2432 .\n\n\u017e\n\n.\n\nj\n\nk\n\nj\n\nk\n1.3\u017e\n\n.\n\nwe derive the covariance in section 14.1.4. the marginal distribution of each\nn is binomial.\n\nj\n\n1.2.3 poisson distribution\nsometimes, count data do not result from a fixed number of trials. for\ninstance, if y s number of deaths due to automobile accidents on motorways\nin italy during this coming week, there is no fixed upper limit n for y as you\nare aware if you have driven in italy . since y must be a nonnegative integer,\nits distribution should place its mass on that range. the simplest such\ndistribution is the poisson. its probabilities depend on a single parameter,\nthe mean \u242e. the poisson probability mass function poisson 1837, p. 206 is\n\n.\n\n\u017e\n\n\u017e\n\n.\n\np y s\n\u017e\n\n.\n\ney\u242e\u242ey\n\ny!\n\n,\n\ny s 0, 1, 2, . . . .\n\n\u017e\n\n1.4\n\n.\n\n3\n\n3\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nit satisfies e y s var y s \u242e. it is unimodal with mode equal to the\ninteger part of \u242e. its skewness is described by e y y \u242e r\u2434 s 1r \u242e. the\n\u017e\ndistribution approaches normality as \u242e increases.\n\n'\n\nthe poisson distribution is used for counts of events that occur randomly\nover time or space, when outcomes in disjoint periods or regions are inde-\npendent. it also applies as an approximation for the binomial when n is large\nand \u2432 is small, with \u242es n\u2432. so if each of the 50 million people driving in\nitaly next week is an independent trial with probability 0.000002 of dying in a\n.\nfatal accident that week, the number of deaths y is a bin 50000000, 0.000002\nvariate, or approximately poisson with \u242es n\u2432s 50,000,000 0.000002 s 100.\na key feature of the poisson distribution is that its variance equals its\nmean. sample counts vary more when their mean is higher. when the mean\nnumber of weekly fatal accidents equals 100, greater variability occurs in the\nweekly counts than when the mean equals 10.\n\n\u017e\n\n\u017e\n\n.\n\n1.2.4 overdispersion\nin practice, count observations often exhibit variability exceeding that pre-\ndicted by the binomial or poisson. this phenomenon is called o\u00aeerdispersion.\nwe assumed above that each person has the same probability of dying in a\nfatal accident in the next week. more realistically, these probabilities vary,\n\n "}, {"Page_number": 23, "text": "8\n\nintroduction: distributions and inference for categorical data\n\ndue to factors such as amount of time spent driving, whether the person\nwears a seat belt, and geographical location. such variation causes fatality\ncounts to display more variation than predicted by the poisson model.\n\nsuppose that y is a random variable with variance var y \u242e for given \u242e,\nbut \u242e itself varies because of unmeasured factors such as those just de-\nscribed. let \u242as e \u242e . then unconditionally,\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n<\n\ne y s e e y \u242e ,\n\u017e\n\n.\n\n.\n\n\u017e\n\n<\n\nvar y s e var y \u242e q var e y \u242e .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n<\n\n<\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nwhen y is conditionally poisson given \u242e , for instance, then e y s e \u242e s\n\u242a and var y s e \u242e q var \u242e s \u242aq var \u242e ) \u242a.\nassuming a poisson distribution for a count variable is often too simplistic,\nbecause of factors that cause overdispersion. the negati\u00aee binomial\nis a\nrelated distribution for count data that permits the variance to exceed the\nmean. we introduce it in section 4.3.4.\n\nanalyses assuming binomial or multinomial distributions are also some-\ntimes invalid because of overdispersion. this might happen because the true\ndistribution is a mixture of different binomial distributions, with the parame-\nter varying because of unmeasured variables. to illustrate, suppose that an\nexperiment exposes pregnant mice to a toxin and then after a week observes\nthe number of fetuses in each mouse\u2019s litter that show signs of malformation.\nlet n denote the number of fetuses in the litter for mouse i. the mice also\nvary according to other factors that may not be measured, such as their\nweight, overall health, and genetic makeup. extra variation then occurs\nbecause of the variability from litter to litter in the probability \u2432 of malfor-\nmation. the distribution of the number of fetuses per litter showing malfor-\nmations might cluster near 0 and near n , showing more dispersion than\nexpected for binomial sampling with a single value of \u2432. overdispersion\ncould also occur when \u2432 varies among fetuses in a litter according to some\ndistribution problem 1.12 . in chapters 4, 12, and 13 we introduce methods\nfor data that are overdispersed relative to binomial and poisson assumptions.\n\n.\n\n\u017e\n\ni\n\ni\n\n2\n\n1\n\n1.2.5 connection between poisson and multinomial distributions\nin italy this next week, let y s number of people who die in automobile\naccidents, y s number who die in airplane accidents, and y s number who\ndie in railway accidents. a poisson model for y , y , y\ntreats these as\nindependent poisson random variables, with parameters \u242e , \u242e , \u242e . the\n1\njoint probability mass function for y is the product of the three mass\nfunctions of form 1.4 . the total n s \u00fdy also has a poisson distribution,\n\u017e\nwith parameter \u00fd \u242e.i\n\nwith poisson sampling the total count n is random rather than fixed. if we\nassume a poisson model but condition on n, y no longer have poisson\ndistributions, since each y cannot exceed n. given n, y are also no longer\nindependent, since the value of one affects the possible range for the others.\n\n3\n.\n3\u017e\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n1\n\n2\n\n2\n\n3\n\ni\n\ni\n\ni\n\ni\n\ni\n\n "}, {"Page_number": 24, "text": "statistical inference for categorical data\n\n9\nfor c independent poisson variates, with e y s \u242e, let\u2019s derive their\nconditional distribution given that \u00fdy s n. the conditional probability of a\nset of counts n satisfying this condition is\n\n\u017e\n\n.\n\n\u0004\n\n4\n\ni\n\ni\n\ni\n\ni\n\n1\n\n\u017e\n\np y s n , y s n , . . . , y s n\n\n. \u00fd\np y s n , y s n , . . . , y s n\n\u017e\n\ny s n\nj\n.\n\n1\n\n2\n\n2\n\nc\n\nc\n\n1\n\n1\n\n2\n\n2\n\nc\n\nc\n\ns\n\ns\n\nj\n\np \u00fdy s n\n\u017e\n.\n\u0142 exp y\u242e \u242e rn !\n.\nexp y\u00fd \u242e \u00fd \u242e rn!\n\nn i\ni\n\n.\u017e\n\n\u017e\n\n.\n\n\u017e\n\nn\n\ni\n\ni\n\ni\n\nj\n\nj\n\ns\n\nn!\n\n\u0142 n !\ni\n\ni\n\n\u0142\n\ni\n\nn i\n\u2432 ,\ni\n\n1.5\u017e\n\n.\n\nj\n\ni\n\ni\n\n\u0004\n\n\u017e\n\n.4\n\nwhere \u2432 s \u242er \u00fd \u242e . this is the multinomial n, \u2432 distribution, charac-\n4.\ni\n4\nterized by the sample size n and the probabilities \u2432 .i\n\nmany categorical data analyses assume a multinomial distribution. such\nanalyses usually have the same parameter estimates as those of analyses\nassuming a poisson distribution, because of the similarity in the likelihood\nfunctions.\n\n\u017e\n\n\u0004\n\n\u0004\n\n1.3 statistical inference for categorical data\n\nthe choice of distribution for the response variable is but one step of data\nanalysis. in practice, that distribution has unknown parameter values. in this\nsection we review methods of using sample data to make inferences about the\nparameters. sections 1.4 and 1.5 cover binomial and multinomial parameters.\n\n1.3.1 likelihood functions and maximum likelihood estimation\nin this book we use maximum likelihood for parameter estimation. under\nweak regularity conditions, such as the parameter space having fixed dimen-\nsion with true value falling in its interior, maximum likelihood estimators\nhave desirable properties: they have large-sample normal distributions; they\nare asymptotically consistent, converging to the parameter as n increases;\nand they are asymptotically efficient, producing large-sample standard errors\nno greater than those from other estimation methods.\n\ngiven the data, for a chosen probability distribution the likelihood function\nis the probability of those data, treated as a function of the unknown\nparameter. the maximum likelihood ml estimate is the parameter value\nthat maximizes this function. this is the parameter value under which the\ndata observed have the highest probability of occurrence. the parameter\nvalue that maximizes the likelihood function also maximizes the log of that\nfunction. it is simpler to maximize the log likelihood since it is a sum rather\nthan a product of terms.\n\n\u017e\n\n.\n\n "}, {"Page_number": 25, "text": "10\n\nintroduction: distributions and inference for categorical data\n\n.\n\n\u017e\n\n.x\n\n.\n\u017e\n\nw \u017e\n\nwe denote a parameter for a generic problem by \u2424 and its ml estimate\n\u02c6\nby \u2424. the likelihood function is ll \u2424 and the log-likelihood function is\nl \u2424 s log ll \u2424 . for many models, l \u2424 has concave shape and \u2424 is the\n.\n\u017e\npoint at which the derivative equals 0. the ml estimate is then the solution\nof the likelihood equation, \u2b78l \u2424 r\u2b78\u2424s 0. often, \u2424 is multidimensional,\ndenoted by \u2424, and \u2424 is the solution of a set of likelihood equations.\n\nlet se denote the standard error of \u2424, and let cov \u2424 denote the\nasymptotic covariance matrix of \u2424. under regularity conditions rao 1973,\np. 364 , cov \u2424 is the inverse of the information matrix. the j, k element of\nthe information matrix is\n\n\u02c6\u017e\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nye\n\n.\n\n\u2b782l \u2424\u017e\n\u2b78\u2424\u2b78\u2424\nk\n\nj\n\n/\n\n.\n\n1.6\u017e\n\n.\n\nthe standard errors are the square roots of the diagonal elements for the\ninverse information matrix. the greater the curvature of the log likelihood,\nthe smaller the standard errors. this is reasonable, since large curvature\nimplies that the log likelihood drops quickly as \u2424 moves away from \u2424; hence,\n\u02c6\nthe data would have been much more likely to occur if \u2424 took a value near \u2424\n\u02c6\nrather than a value far from \u2424.\n\n\u02c6\n\n1.3.2 likelihood function and ml estimate for binomial parameter\nthe part of a likelihood function involving the parameters is called the\nkernel. since the maximization of the likelihood is with respect\nto the\nparameters, the rest is irrelevant.\n\n\u017e\n\n/y\n\nto illustrate, consider the binomial distribution 1.1 . the binomial coeffi-\nhas no influence on where the maximum occurs with respect to \u2432.\ncient\nthus, we ignore it and treat the kernel as the likelihood function. the\nbinomial log likelihood is then\nl \u2432 s log \u2432 1 y \u2432\n\u017e\n\ns ylog \u2432 q n y y log 1 y \u2432 .\n.\n\n1.7\n\nnyy\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nn\n\ny\n\ndifferentiating with respect to \u2432 yields\n\n.\n\n\u017e\n\n\u017e\n\n\u2b78l \u2432 r\u2b78\u2432s yr\u2432y n y y r 1 y \u2432 s y y n\u2432 r\u2432 1 y \u2432 .\n.\n\u02c6\n\n.\n1.8\nequating this to 0 gives the likelihood equation, which has solution \u2432s yrn,\nthe sample proportion of successes for the n trials.\ncalculating \u2b78 l \u2432 r\u2b78\u2432 , taking the expectation, and combining terms,\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n2\n\n2\n\nwe get\nye \u2b78 l \u2432 r\u2b78\u2432 s e yr\u2432 q n y y r 1 y \u2432 s nr \u2432 1 y \u2432 .\n.\n1.9\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n2\n\n2\n\n2\n\n2\n\n.\n\n "}, {"Page_number": 26, "text": "11\nstatistical inference for categorical data\nthus, the asymptotic variance of \u2432 is \u2432 1 y \u2432 rn. this is no surprise. since\ne y s n\u2432 and var y s n\u2432 1 y \u2432 , the distribution of \u2432s yrn has mean\n\u017e\nand standard error\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\ne \u2432 s \u2432,\n\u017e\n\n.\n\n\u02c6\n\n\u02c6 ( n\n\n.\n\n\u017e\n\n\u2432 1 y \u2432\n\n\u2434 \u2432 s\n\n\u017e\n\n.\n\n.\n\n1.3.3 wald\u2013likelihood ratio\u2013score test triad\n\nthree standard ways exist to use the likelihood function to perform\nlarge-sample inference. we introduce these for a significance test of a null\nhypothesis h : \u2424s \u2424 and then discuss their relation to interval estimation.\nthey all exploit the large-sample normality of ml estimators.\n\n0\n\n0\n\nwith nonnull standard error se of \u2424, the test statistic\n\n\u02c6\n\nz s \u2424y \u2424 rse\n\n\u02c6\n\n.0\n\n\u017e\n\nhas an approximate standard normal distribution when \u2424s \u2424 . one refers z\nto the standard normal table to obtain one- or two-sided p-values. equiva-\nlently, for the two-sided alternative, z 2 has a chi-squared null distribution\nwith 1 degree of freedom df ; the p-value is then the right-tailed chi-squared\nprobability above the observed value. this type of statistic, using the nonnull\n.\nstandard error, is called a wald statistic wald 1943 .\nthe multivariate extension for the wald test of h : \u2424 s \u2424 has test\n\n\u017e\n\n.\n\n\u017e\n\n0\n\n0\n\n0\n\nstatistic\n\nw s \u2424 y \u2424\n\n\u02c6\n\n\u017e\n\nx\n\n.\n\n0\n\n\u017e\n\u02c6\ncov \u2424\n\n.\n\ny1\n\n\u017e\n\n.\n\u2424 y \u2424 .\n\u02c6\n\n0\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\nthe prime on a vector or matrix denotes the transpose. the nonnull\ncovariance is based on the curvature 1.6 of the log likelihood at \u2424. the\nasymptotic multivariate normal distribution for \u2424 implies an asymptotic\nchi-squared distribution for w. the df equal the rank of cov \u2424 , which is the\nnumber of nonredundant parameters in \u2424.\n\n\u02c6\u017e\n\n\u02c6\n\n\u02c6\n\n\u017e .\n\n\u017e .\n\na second general-purpose method uses the likelihood function through\nthe ratio of two maximizations: 1 the maximum over the possible parameter\nvalues under h , and 2 the maximum over the larger set of parameter\nvalues permitting h or an alternative h to be true. let ll denote the\nmaximized value of the likelihood function under h , and let ll denote the\nmaximized value generally i.e., under h j h . for instance, for parameter\n0\nvector \u2424 s \u2424 , \u2424 \u2b18 and h : \u2424 s 0, ll\nis the likelihood function calculated\nat the \u2424 value for which the data would have been most likely; ll\nis the\nlikelihood function calculated at the \u2424 value for which the data would\nhave been most likely, when \u2424 s 0. then ll\nis always at least as large as\n, since ll\nll\nresults from maximizing over a restricted set of the parameter\n0\nvalues.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\na\n\na\n\n0\n\n1\n\n0\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n "}, {"Page_number": 27, "text": "12\n\nintroduction: distributions and inference for categorical data\nthe ratio \u2333 s ll rll of the maximized likelihoods cannot exceed 1. wilks\n1935, 1938 showed that y2 log\u2333 has a limiting null chi-squared distribu-\n\u017e\ntion, as n \u2122 \u2b01. the df equal the difference in the dimensions of the\nparameter spaces under h j h and under h . the likelihood-ratio test\nstatistic equals\n\n.\n\na\n\n0\n\n1\n\n0\n\n0\n\ny2 log\u2333 s y2 log ll rll s y2 l y l ,\n.\n\n\u017e\n\n0\n\n1\n\n\u017e\n\n.\n\n1\n\n0\n\nwhere l and l denote the maximized log-likelihood functions.\n\n0\n\n1\n\nthe third method uses the score statistic, due to r. a. fisher and c. r.\nrao. the score test is based on the slope and expected curvature of the\nlog-likelihood function l \u2424 at the null value \u2424 . it utilizes the size of the\nscore function\n\n\u017e\n\n.\n\n0\n\nu \u2424 s \u2b78l \u2424 r\u2b78\u2424,\n\u017e\n\n.\n\n\u017e\n\n.\n\n0\n\n\u017e\n\n\u02c6\nevaluated at \u2424 . the value u \u2424 tends to be larger in absolute value when \u2424\nis farther from \u2424 . denote ye \u2b78 l \u2424 r\u2b78\u2424 i.e., the information evalu-\n.\nated at \u2424 by \u242b \u2424 . the score statistic is the ratio of u \u2424 to its null se,\n0\nwhich is \u242b \u2424\n. this has an approximate standard normal null distribu-\n0\ntion. the chi-squared form of the score statistic is\n\n0\u017e\n.x1r2\n\n2x \u017e\n\nw \u017e\n\n.\n0w\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n2\n\n0\n\n0\n\n2\n\ns\n\n\u017e\nu \u2424\n0\n\u017e\n\u242b \u2424\n0\n\n.\n.\n\n\u2b78l \u2424 r\u2b78\u2424\n\n0\n\n2\n\nye \u2b78 l \u2424 r\u2b78\u2424\n\n.\n\n2\n0\n\n.\n\u017e\n\n\u017e\n2\n\n,\n\n0\n\n0\n\n0\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u02c6\n\nwhere the partial derivative notation reflects derivatives with respect to \u2424\nthat are evaluated at \u2424 . in the multiparameter case, the score statistic is a\nquadratic form based on the vector of partial derivatives of the log likelihood\nwith respect to \u2424 and the inverse information matrix, both evaluated at the\nh estimates i.e., assuming that \u2424 s \u2424 .\n.\nfigure 1.1 is a generic plot of a log-likelihood l \u2424 for the univariate\ncase. it illustrates the three tests of h : \u2424s 0. the wald test uses the\nbehavior of l \u2424 at the ml estimate \u2424, having chi-squared form \u2424rse .\n.\n2\nthe se of \u2424 depends on the curvature of l \u2424 at \u2424. the score test is based\non the slope and curvature of l \u2424 at \u2424s 0. the likelihood-ratio test\ncombines information about l \u2424 at both \u2424 and \u2424 s 0. it compares the\nlog-likelihood values l at \u2424 and l at \u2424 s 0 using the chi-squared\nstatistic y2 l y l . in figure 1.1, this statistic is twice the vertical dis-\ntance between values of l \u2424 at \u2424 and at 0. in a sense, this statistic uses the\nmost information of the three types of test statistic and is the most versatile.\nas n \u2122 \u2b01, the wald, likelihood-ratio, and score tests have certain asymp-\ntotic equivalences cox and hinkley 1974, sec. 9.3 . for small to moderate\nsample sizes, the likelihood-ratio test is usually more reliable than the wald\ntest.\n\n0\u02c6\n\n\u017e\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n0\n\n1\n\n0\n\n0\n\n0\n\n1\n\n "}, {"Page_number": 28, "text": "statistical inference for categorical data\n\n13\n\nfigure 1.1 log-likelihood function and information used in three tests of h : \u2424s 0.\n\n0\n\n0\n\n0\n\n0\n\na\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n2 \u017e .\ndf\n\n1.3.4 constructing confidence intervals\nin practice,\nit is more informative to construct confidence intervals for\nparameters than to test hypotheses about their values. for any of the three\ntest methods, a confidence interval results from inverting the test. for\ninstance, a 95% confidence interval for \u2424 is the set of \u2424 for which the test\nof h : \u2424s \u2424 has a p-value exceeding 0.05.\nlet z denote the z-score from the standard normal distribution having\nright-tailed probability a; this is the 100 1 y a percentile of that distribution.\nlet \u2439 a denote the 100 1 y a percentile of the chi-squared distribution\nwith degrees of freedom df. 100 1 y \u2423 % confidence intervals based on\n, for instance z s 1.96 for 95% confidence.\nasymptotic normality use z\nthe wald confidence interval is the set of \u2424 for which \u2424y \u2424 rse - z\n.\n\u2423r2\nthis gives the interval \u2424\" z\nse . the likelihood-ratio-based confidence\nis the set of \u2424 for which y2 l \u2424 y l \u2424 - \u2439 \u2423 . recall\ninterval\nthat \u2439 \u2423 s z\n2\u017e .\n1\n\u02c6\n\nwhen \u2424 has a normal distribution, the log-likelihood function has a\nparabolic shape i.e., a second-degree polynomial . for small samples with\ncategorical data, \u2424 may be far from normality and the log-likelihood function\ncan be far from a symmetric, parabolic-shaped curve. this can also happen\nwith moderate to large samples when a model contains many parameters. in\nsuch cases, inference based on asymptotic normality of \u2424 may have inade-\nquate performance. a marked divergence in results of wald and likelihood-\nratio inference indicates that the distribution of \u2424 may not be close to\nnormality. the example in section 1.4.3 illustrates this with quite different\nconfidence intervals for different methods. in many such cases, inference can\n\n\u02c6<\n.x\n\n\u02c6\n0\n\n\u017e\n\u02c6\n\n2\n\u2423r2\n\n\u017e .\n\n\u2423r2\n\n\u2423r2\n\n0\nw\n\n0.025\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n2\n1\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nx\n\nw\n\n.\n\n0\n\n0\n\n<\n\n "}, {"Page_number": 29, "text": "14\n\nintroduction: distributions and inference for categorical data\n\ninstead utilize an exact small-sample distribution or \u2018\u2018higher-order\u2019\u2019 asymp-\ntotic methods that improve on simple normality e.g., pierce and peters\n.\n1992 .\n\nthe wald confidence interval is most common in practice because it is\nsimple to construct using ml estimates and standard errors reported by\nstatistical software. the likelihood-ratio-based interval\nis becoming more\nwidely available in software and is preferable for categorical data with small\nto moderate n. for the best known statistical model, regression for a normal\nresponse, the three types of inference necessarily provide identical results.\n\n\u017e\n\n1.4 statistical inference for binomial parameters\n\nin this section we illustrate inference methods for categorical data by\npresenting tests and confidence intervals for the binomial parameter \u2432,\nbased on y successes in n independent trials. in section 1.3.2 we obtained\nthe likelihood function and ml estimator \u2432s yrn of \u2432.\n\n\u02c6\n\n1.4.1 tests about a binomial parameter\nconsider h : \u2432s \u2432 . since h has a single parameter, we use the normal\nrather than chi-squared forms of wald and score test statistics. they permit\ntests against one-sided as well as two-sided alternatives. the wald statistic is\n\n0\n\n0\n\n0\n\nz s\n\nw\n\n\u2432y \u2432\n\u02c6\n0\n\nse\n\ns\n\n\u2432y \u2432\n\u02c6\n0\n\u017e\n.\n\u02c6\n\n'\u2432 1 y \u2432 rn\n\u02c6\n\n.\n\n\u017e\n\n1.10\n\n.\n\nevaluating the binomial score 1.8 and information 1.9 at \u2432 yields\n\n\u017e\n\n.\n\n.\n\n0\n\nu \u2432 s y\n\u017e\n\n.\n\n0\n\ny\n\u2432\n0\n\n,\n\n\u242b \u2432 s\n\u017e\n\n.\n\n0\n\nn\n\n\u2432 1 y \u2432\n\n\u017e\n\n0\n\n0\n\n.\n\n.\n\n\u017e\nn y y\n1 y \u2432\n\n0\n\nthe normal form of the score statistic simplifies to\n\nz s\n\ns\n\n\u017e\nu \u2432\n0\n\u242b \u2432\u017e\n.\n0\n\n.\n1r2\n\ns\n\ny y n\u2432\n\u017e\n\n'\nn\u2432 1 y \u2432\n\n0\n\n0\n\n0\n\ns\n\n.\n\n\u2432y \u2432\n\u02c6\n0\n\u017e\n\n'\n\u2432 1 y \u2432 rn\n\n.\n\n0\n\n0\n\n.\n\n\u017e\n\n1.11\n\n.\n\ns\n\nw\n\nuses the standard error evaluated at \u2432, the\nwhereas the wald statistic z\nscore statistic z uses it evaluated at \u2432 . the score statistic is preferable, as\nit uses the actual null se rather than an estimate. its null sampling distribu-\ntion is closer to standard normal than that of the wald statistic.\nequals l s ylog\u2432 q\nthe binomial\n0\nn y y log 1 y \u2432 under h and l s y log \u2432q n y y log 1 y \u2432 more\n\u017e\n\u017e\n\nlog-likelihood function\n.\n\n\u017e\n.\n1.7\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n0\n\n0\n\n0\n\n0\n\n1\n\n\u02c6\n\n "}, {"Page_number": 30, "text": "statistical inference for binomial parameters\n\n15\n\ngenerally. the likelihood-ratio test statistic simplifies to\n\ny2 l y l s 2 y log q n y y log\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n0\n\n1\n\n\u017e\n\n\u02c6\n\u2432\n\u2432\n0\n\nexpressed as\n\ny2 l y l s 2 y log\n\n.\n\n\u017e\n\n0\n\n1\n\n\u017e\n\ny\nn\u2432\n0\n\nq n y y log\n\n\u017e\n\n.\n\n1 y \u2432\n\u02c6\n1 y \u2432\n\n0\n\n/\n\nn y y\nn y n\u2432\n\n0\n\n.\n\n/\n\n,\n\nit compares observed success and failure counts to fitted i.e., null counts by\n\n\u017e\n\n.\n\n\u00fd\n\n2\n\nobserved log\n\nobserved\n\nfitted\n\n.\n\n\u017e\n\n1.12\n\n.\n\nwe\u2019ll see that this formula also holds for tests about poisson and multinomial\nparameters. since no unknown parameters occur under h and one occurs\nunder h , 1.12 has an asymptotic chi-squared distribution with df s 1.\n\n\u017e\n\n.\n\n0\n\na\n\n1.4.2 confidence intervals for a binomial parameter\na significance test merely indicates whether a particular \u2432 value such as\n\u2432s 0.5 is plausible. we learn more by using a confidence interval to\ndetermine the range of plausible values.\n\n.\n\n\u017e\n\ninverting the wald test statistic gives the interval of \u2432 values for which\nz - z\nw\n\n, or\n\n\u2423r2\n\n0\n\n<\n\n<\n\n(\u2423r2\n\n\u02c6\n\u2432\" z\n\n\u2432 1 y \u2432\n\u02c6\n\u02c6\n\n\u017e\n\n.\n\nn\n\n.\n\n\u017e\n\n1.13\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nhistorically, this was one of the first confidence intervals used for any\nparameter laplace 1812, p. 283 . unfortunately, it performs poorly unless n\nis very large e.g., brown et al. 2001 . the actual coverage probability usually\nfalls below the nominal confidence coefficient, much below when \u2432 is near 0\nor 1. a simple adjustment that adds\nobservations of each type to the\n.\nsample before using this formula performs much better problem 1.24 .\nthe score confidence interval contains \u2432 values for which z - z\n\n1 2\nz\n2\n\n\u2423r2\n\n.\n\n\u017e\n\n.\n\n<\n\n<\n\ns\n\n\u2423r2\n\n0\n\nits endpoints are the \u2432 solutions to the equations\n\n0\n\n\u017e\n\n\u2432y \u2432 r \u2432 1 y \u2432 rn s \"z\n\u02c6\n\n.\n\n.\n\n0\n\n0\n\n' \u017e\n\n0\n\n\u2423r2\n\n.\n\n "}, {"Page_number": 31, "text": "16\n\nintroduction: distributions and inference for categorical data\n\nthese are quadratic in \u2432 . first discussed by e. b. wilson 1927 , this\ninterval is\n\n0\n\n\u017e\n\n.\n\nq\n\n\u017e\n\n1\n2\n\nz 2\n\u2423r2\nn q z\n2\n\u2423r2\n\n/\n\n\u017e\n\n\u02c6\n\u2432\n\nn\nn q z\n\n2\n\u2423r2\n\n\" z\n\n\u2423r2\n\n/\n)\n\n1\nn q z\n\n2\n\u2423r2\n\n\u2432 1 y \u2432\n\u02c6\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\nn\nn q z\n\n2\n\u2423r2\n\n/\n\nq\n\n\u017e\n\n/\u017e\n\n/\n\n1\n2\n\n1\n2\n\n\u017e\n\n2\nz\u2423r2\nn q z\n2\n\u2423r2\n\n/\n\n.\n\n\u017e\n\n.\n\n.\n\n1\n2\n\n2\n\u2423r2\n\u02dc\n\n\u02c6\n2\n\u2423r2\n\n\u02dc\nthe midpoint \u2432 of the interval is a weighted average of \u2432 and , where the\nweight nr n q z\ngiven \u2432 increases as n increases. combining terms, this\nmidpoint equals \u2432s y q z r2 r n q z\n.\n\u017e\n\u017e\n. this is the sample proportion\nfor an adjusted sample that adds z 2\nobservations, half of each type. the\n\u2423r2\nsquare of the coefficient of z\nin this formula is a weighted average of the\nvariance of a sample proportion when \u2432s \u2432 and the variance of a sample\nproportion when \u2432s , using the adjusted sample size n q z\nin place\nof n. this interval has much better performance than the wald interval.\n\n2\n\u2423r2\n\n2\n\u2423r2\n\n\u2423r2\n\n\u02c6\n\n\u02c6\n\nthe likelihood-ratio-based confidence interval is more complex computa-\ntionally, but simple in principle. it is the set of \u2432 for which the likelihood-\nratio test has a p-value exceeding \u2423. equivalently, it is the set of \u2432 for\nwhich double the log likelihood drops by less than \u2439 \u2423 from its value at the\nml estimate \u2432s yrn.\n\n2\u017e .\n1\n\n1\n2\n\n0\n\n0\n\n\u02c6\n\n1.4.3 proportion of vegetarians example\nto collect data in an introductory statistics course, recently i gave the\nstudents a questionnaire. one question asked each student whether he or\nshe was a vegetarian. of n s 25 students, y s 0 answered \u2018\u2018yes.\u2019\u2019 they were\nnot a random sample of a particular population, but we use these data to\nillustrate 95% confidence intervals for a binomial parameter \u2432.\nsince y s 0, \u2432s 0r25 s 0. using the wald approach, the 95% confi-\n\n\u02c6\n\ndence interval for \u2432 is\n\n0 \" 1.96\n\n'\n\u017e\n\n0.0 = 1.0 r25 , or\n\n.\n\n\u017e\n\n.\n0, 0 .\n\nwhen the observation falls at the boundary of the sample space, often wald\nmethods do not provide sensible answers.\nby contrast, the 95% score interval equals 0.0, 0.133 . this is a more\nbelievable inference. for h : \u2432s 0.5, for instance, the score test statistic is\nz s 0 y 0.5 r 0.5 = 0.5 r25 s y5.0, so 0.5 does not fall in the interval.\nby contrast, for h : \u2432s 0.10, z s 0 y 0.10 r 0.10 = 0.90 r25 s y1.67,\nso 0.10 falls in the interval.\n\n. '\n\u017e\n\n. '\n\u017e\n\n0\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\ns\n\ns\n\n0\n\n "}, {"Page_number": 32, "text": "statistical inference for binomial parameters\n\n17\nwhen y s 0 and n s 25, the kernel of the likelihood function is ll \u2432 s\n\u2432 1 y \u2432 s 1 y \u2432 . the log likelihood 1.7 is l \u2432 s 25 log 1 y \u2432 .\n0\u017e\n.\nnote that l \u2432 s l 0 s 0. the 95% likelihood-ratio confidence interval is\nthe set of \u2432 for which the likelihood-ratio statistic\n\n.25\n\u017e\n\n\u017e .\n\n.25\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n0\n\ny2 l y l s y2 l \u2432 y l \u2432\n.\u02c6\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n0\n\n1\n\n0\n\ns y50 log 1 y \u2432 f \u24392 0.05 s 3.84.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n0\n\n1\n\n\u017e\n\n.\n\n\u017e\n\n. w\n\nthe upper bound is 1 y exp y3.84r50 s 0.074, and the confidence interval\nequals 0.0, 0.074 . in this book, we use the natural logarithm throughout, so\nits inverse is the exponential function exp x s e . figure 1.2 shows the\nlikelihood and log-likelihood functions and the corresponding confidence\nregion for \u2432.\n\nx x\n\nthe three large-sample methods yield quite different results. when \u2432 is\nnear 0, the sampling distribution of \u2432 is highly skewed to the right for small\nn. it is worth considering alternative methods not requiring asymptotic\napproximations.\n\n\u02c6\n\n\u017e\n\n.\n\nfigure 1.2 binomial likelihood and log likelihood when y s 0 in n s 25 trials, and confi-\ndence interval for \u2432.\n\n "}, {"Page_number": 33, "text": "18\n\nintroduction: distributions and inference for categorical data\n\n1.4.4 exact small-sample inference*1\nwith modern computational power, it is not necessary to rely on large-sam-\nple approximations for the distribution of statistics such as \u2432. tests and\nconfidence intervals can use the binomial distribution directly rather than its\nnormal approximation. such inferences occur naturally for small samples, but\napply for any n.\nwe illustrate by testing h : \u2432s 0.5 against h : \u2432/ 0.5 for the survey\nresults on vegetarianism, y s 0 with n s 25. we noted that the score statistic\nequals z s y5.0. the exact p-value for this statistic, based on the null\nbin 25, 0.5 distribution, is\n\n\u02c6\n\n\u017e\n\n.\n\na\n\n0\n\np z g 5.0 s p y s 0 or y s 25 s 0.5 q 0.5 s 0.00000006.\n\u017e\n\n25\n\n25\n\n\u017e\n\n.\n\n.\n\n<\n\n<\n\n.\n\n\u017e\n\n100 1 y \u2423 % confidence intervals consist of all \u2432 for which p-values\nexceed \u2423 in exact binomial tests. the best known interval clopper and\npearson 1934 uses the tail method for forming confidence intervals. it\nrequires each one-sided p-value to exceed \u2423r2. the lower and upper\nendpoints are the solutions in \u2432 to the equations\n\n.\n\n\u017e\n\n0\n\n0\n\n\u017e\n\n/\n\nn\nk\n\nn\n\n\u00fd\nksy\n\n\u2432 1 y \u2432\n\n\u017e\n\nk\n0\n\n0\n\nnyk\n\n.\n\ns \u2423r2 and\n\n\u017e\n\n/\n\nn\nk\n\ny\n\n\u00fd\nks0\n\n\u2432 1 y \u2432\n\n\u017e\n\nk\n0\n\n0\n\nnyk\n\n.\n\ns \u2423r2,\n\nexcept that the lower bound is 0 when y s 0 and the upper bound is 1 when\ny s n. when y s 1, 2, . . . , n y 1, from connections between binomial sums\nand the incomplete beta function and related cumulative distribution func-\ntions cdf\u2019s of beta and f distributions, the confidence interval equals\n\n\u017e\n\n.\n\n1q\n\nn y y q 1\n\nyf\n\n2 y , 2\u017e nyyq1.\n\n\u017e\n\n1 y \u2423r2\n\ny1\n\n.\n\n-\u2432- 1 q\n\n\u017e\n\nn y y\n2\u017e yq1. , 2\u017e nyy .\n\ny q 1 f\n.\n\n\u2423r2\n\u017e\n\n.\n\ny1\n\n,\n\n\u017e\n\nc denotes the 1 y c quantile from the f distribution with\n\u017e .\nwhere f\ndegrees of freedom a and b. when y s 0 with n s 25, the clopper\u1390pearson\n.\n95% confidence interval for \u2432 is 0.0, 0.137 .\n\na, b\n\nin principle this approach seems ideal. however, there is a serious\ncomplication. because of discreteness, the actual coverage probability for any\n\u2432 is at least as large as the nominal confidence level casella and berger\n2001, p. 434; neyman 1935 and it can be much greater. similarly, for a test\nof h : \u2432s \u2432 at a fixed desired size \u2423 such as 0.05, it is not usually possible\nto achieve that size. there is a finite number of possible samples, and hence\na finite number of possible p-values, of which 0.05 may not be one. in testing\nh with fixed \u2432 , one can pick a particular \u2423 that can occur as a p-value.\n\n.\n\n\u017e\n\n0\n\n0\n\n0\n\n0\n\n1sections marked with an asterisk are less important for an overview.\n\n "}, {"Page_number": 34, "text": "statistical inference for binomial parameters\n\n19\n\nfigure 1.3 plot of coverage probabilities for nominal 95% confidence intervals for binomial\nparameter \u2432 when n s 25.\n\n0\n\n0\n\n0\n\n0\n\nfor interval estimation, however, this is not an option. this is because\nconstructing the interval corresponds to inverting an entire range of \u2432\n0\nvalues in h : \u2432s \u2432 , and each distinct \u2432 value can have its own set of\npossible p-values; that is, there is not a single null parameter value \u2432 as in\none test.\nfor any fixed parameter value, the actual coverage probability can be\nmuch larger than the nominal confidence level. when n s 25, figure 1.3\nplots the coverage probabilities as a function of \u2432 for the clopper\u1390pearson\nmethod, the score method, and the wald method. at a fixed \u2432 value with a\ngiven method, the coverage probability is the sum of the binomial probabili-\nties of all those samples for which the resulting interval contains that \u2432.\nthere are 26 possible samples and 26 corresponding confidence intervals, so\nthe coverage probability is a sum of somewhere between 0 and 26 binomial\nprobabilities. as \u2432 moves from 0 to 1, this coverage probability jumps up or\ndown whenever \u2432 moves into or out of one of these intervals. figure 1.3\nshows that coverage probabilities are too low for the wald method, whereas\nthe clopper\u1390pearson method errs in the opposite direction. the score\nmethod behaves well, except for some \u2432 values close to 0 or 1. its coverage\nprobabilities tend to be near the nominal\nlevel, not being consistently\nconservative or liberal. this is a good method unless \u2432 is very close to 0 or 1\n.\n\u017e\nproblem 1.23 .\n\nin discrete problems using small-sample distributions, shorter confidence\nintervals usually result from inverting a single two-sided test rather than two\n\n "}, {"Page_number": 35, "text": "20\n\nintroduction: distributions and inference for categorical data\n\no\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\none-sided tests. the interval is then the set of parameter values for which the\np-value of a two-sided test exceeds \u2423. for the binomial parameter, see\nblaker 2000 , blyth and still 1983 , and sterne 1954 for methods. for\nobserved outcome y , with blaker\u2019s approach the p-value is the minimum of\nthe two one-tailed binomial probabilities p y g y\nplus an\nattainable probability in the other tail that is as close as possible to, but not\ngreater than, that one-tailed probability. the interval\nis computationally\n.\nmore complex, although available in software blaker gave s-plus functions .\nthe result is still conservative, but less so than the clopper\u1390pearson interval.\nfor the vegetarianism example, the 95% confidence interval using the blaker\nexact method is 0.0, 0.128 compared to the clopper\u1390pearson interval of\n.\n\u017e\n0.0, 0.137 .\n\nand p y f y\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\no\n\no\n\ninference based on the mid-p-value*\n\n1.4.5\nto adjust for discreteness in small-sample distributions, one can base infer-\nence on the mid-p-\u00aealue lancaster 1961 . for a test statistic t with observed\nvalue t and one-sided h such that large t contradicts h ,\n0\n\n\u017e\n\n.\n\no\n\na\n\nmid-p-value s p t s t q p t ) t\n\n\u017e\n\n.\n\n\u017e\n\n1\n2\n\no\n\n.\n\no\n\n,\n\n0\n\n\u017e\n\nwith probabilities calculated from the null distribution. thus, the mid-p-value\nis less than the ordinary p-value by half the probability of the observed\nresult. compared to the ordinary p-value, the mid-p-value behaves more like\nthe p-value for a test statistic having a continuous distribution. the sum of\nits two one-sided p-values equals 1.0. although discrete, under h its null\ndistribution is more like the uniform distribution that occurs in the continu-\nous case. for instance,\nit has a null expected value of 0.5, whereas this\nexpected value exceeds 0.5 for the ordinary p-value for a discrete test\nstatistic.\n\nunlike an exact test with ordinary p-value, a test using the mid-p-value\ndoes not guarantee that the probability of type i error is no greater than a\nnominal value problem 1.19 . however, it usually performs well, typically\nbeing a bit conservative. it is less conservative than the ordinary exact test.\nsimilarly, one can form less conservative confidence intervals by inverting\ntests using the exact distribution with the mid-p-value e.g., the 95% confi-\nis the set of parameter values for which the mid-p-value\ndence interval\n.\nexceeds 0.05 .\nfor testing h : \u2432s 0.5 against h : \u2432/ 0.5 in the example about the\nproportion of vegetarians, with y s 0 for n s 25, the result observed is the\nmost extreme possible. thus the mid-p-value is half the ordinary p-value, or\n0.00000003. using the clopper\u1390pearson inversion of the exact binomial test\n.\nbut with the mid-p-value yields a 95% confidence interval of 0.000, 0.113\nfor \u2432, compared to 0.000, 0.137 for the ordinary clopper\u1390pearson interval.\nthe mid-p-value seems a sensible compromise between having overly\nconservative inference and using irrelevant randomization to eliminate prob-\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\na\n\n0\n\n "}, {"Page_number": 36, "text": "statistical inference for multinomial parameters\n\n21\n\nlems from discreteness. we recommend it both for tests and confidence\nintervals with highly discrete distributions.\n\n1.5 statistical inference for multinomial parameters\nwe now present inference for multinomial parameters \u2432 . of n observa-\ntions, n occur in category j, j s 1, . . . , c.\n\n\u0004\n\n4\n\nj\n\nj\n\n1.5.1 estimation of multinomial parameters\n4\nfirst, we obtain ml estimates of \u2432 . as a function of \u2432 , the multinomial\nj\nprobability mass function 1.2 is proportional to the kernel\n\u2432 s 1.\n\nall \u2432 g 0 and\n\n\u2432n j where\n\n1.14\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n\u017e\n\n.\n\nj\n\nj\n\n\u0142\n\nj\n\nj\n\n\u00fd\n\nj\n\nj\n\n.\nthe ml estimates are the \u2432 that maximize 1.14 .\n\n\u017e\n\n\u0004\n\n4\n\nj\n\nthe multinomial log-likelihood function is\n\nl \u2432 s n log \u2432 .\n\u017e\n\n. \u00fd j\n\nj\n\nj\n\nto eliminate redundancies, we treat l as a function of \u2432 , . . . , \u2432 , since\ncy1\n\u2432 s 1 y \u2432 q \u2b48\u2b48\u2b48 q\u2432 . thus, \u2b78\u2432r\u2b78\u2432 s y1, j s 1, . . . , c y 1.\n\n.\n\n\u017e\n\n1\n\nc\n\nj\n\ncy1\n\n1\n\n\u017e\n\n.\n\nc\n\nsince\n\n\u2b78 log \u2432\nc\n\n\u2b78\u2432\nj\n\ns\n\n1 \u2b78\u2432\nc\n\u2432 \u2b78\u2432\nj\n\nc\n\n1\ns y ,\n\u2432\nc\n\ndifferentiating l \u2432 with respect to \u2432 gives the likelihood equation\n\n\u017e\n\n.\n\n.\n\n\u017e\n\u2b78l \u2432\n\u2b78\u2432\nj\n\ns y s 0 .\n\nn\nc\n\u2432\nc\n\nj\n\nn\nj\n\u2432\nj\n\nthe ml solution satisfies \u2432r\u2432 s n rn . now\n/\n\n\u017e\n\u02c6 \u00fdc\n\u2432\n\n\u02c6 \u02c6j\nc\n\nn\n\nc\n\nj\n\nj\n\n\u02c6\u00fd j\n\n\u2432 s 1 s\n\nj\n\nj\nn\n\nc\n\ns\n\n\u2432 n\u02c6\nc\nn\n\nc\n\n,\n\n\u02c6\nc\n\nso \u2432 s n rn and then \u2432 s n rn. from general results presented later in\nthe book section 8.6 , this solution does maximize the likelihood. thus, the\nml estimates of \u2432 are the sample proportions.\n\n\u02c6\n\nc\n\u017e\n\n.\n\n4\n\n\u0004\n\nj\n\nj\n\nj\n\n "}, {"Page_number": 37, "text": "22\n\nintroduction: distributions and inference for categorical data\n\n1.5.2 pearson statistic for testing a specified multinomial\nin 1900 the eminent british statistician karl pearson introduced a hypothesis\ntest that was one of the first inferential methods. it had a revolutionary\nimpact on categorical data analysis, which had focused on describing associa-\ntions. pearson\u2019s test evaluates whether multinomial parameters equal certain\nspecified values. his original motivation in developing this test was to analyze\nwhether possible outcomes on a particular monte carlo roulette wheel were\n.\nequally likely stigler 1986 .\nj s 1, . . . , c, where \u00fd \u2432 s 1. when h is true,\nconsider h : \u2432 s \u2432 ,\nj0\nj s\nthe expected values of n , called expected frequencies, are \u242e s n\u2432 ,\n\u0004\n4\n1, . . . , c. pearson proposed the test statistic\n\n\u017e\n\nj0\n\nj0\n\n0\n\n0\n\nj\n\nj\n\nj\n\nj\n\n2x s\n\n\u017e\n\n\u00fd\n\nj\n\nn y \u242e\n\nj\n\nj\n\n\u242e\nj\n\n2\n\n.\n\n.\n\n\u017e\n\n1.15\n\n.\n\nj\n\nj\n\n2\n\n2\n\no\n\n4\n\n\u0004\n\n\u017e\n\ngreater differences n y \u242e produce greater x values, for fixed n. let x\ndenote the observed value of x . the p-value is the null value of p x g\n2.x . this equals the sum of the null multinomial probabilities of all count\narrays having a sum of n with x g x .o\nfor large samples, x 2 has approximately a chi-squared distribution with\ndf s c y 1. the p-value is approximated by p \u2439 g x , where \u2439\n2\ncy1\ndenotes a chi-squared random variable with df s c y 1. statistic 1.15 is\n.\ncalled the pearson chi-squared statistic.\n\n2\ncy1\n\n2.\no\n\n2\no\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n2\n\n2\n\n2\n\n1.5.3 example: testing mendel\u2019s theories\n\n2\n\n1\n\n0\n\n2\n\n\u017e\n\n10\n\n20\n\namong its many applications, pearson\u2019s test was used in genetics to test\nmendel\u2019s theories of natural inheritance. mendel crossed pea plants of pure\nyellow strain with plants of pure green strain. he predicted that second-gen-\neration hybrid seeds would be 75% yellow and 25% green, yellow being the\ndominant strain. one experiment produced n s 8023 seeds, of which n s\n6022 were yellow and n s 2001 were green. the expected frequencies for\nh : \u2432 s 0.75, \u2432 s 0.25 are \u242e s 8023 0.75 s 6017.25 and \u242e s 2005.75.\nthe pearson statistic x s 0.015 df s 1 has a p-value of p s 0.90. this\ndoes not contradict mendel\u2019s hypothesis.\n\nmendel performed several experiments of this type. in 1936, r. a. fisher\nsummarized mendel\u2019s results. he used the reproductive property of chi-\nsquared: if x 2, . . . , x 2 are independent chi-squared statistics with degrees\nof freedom \u242f , . . . , \u242f , then \u00fd x 2 has a chi-squared distribution with df s\n\u00fd \u242f. fisher obtained a summary chi-squared statistic equal to 42, with\ndf s 84. a chi-squared distribution with df s 84 has mean 84 and standard\ndeviation 2 = 84 s 13.0, and the right-tailed probability above 42 is\np s 0.99996. in other words, the chi-squared statistic was so small that the fit\nseemed too good.\n\n.1r2\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n1\n\n1\n\n2\n\n1\n\nk\n\nk\n\ni\n\ni\n\ni\n\ni\n\n "}, {"Page_number": 38, "text": "statistical inference for multinomial parameters\n\n23\n\n\u017e\n\n.\n\nfisher commented: \u2018\u2018the general level of agreement between mendel\u2019s\nexpectations and his reported results shows that it is closer than would be\nexpected in the best of several thousand repetitions . . . . i have no doubt\nthat mendel was deceived by a gardening assistant, who knew only too well\nwhat his principal expected from each trial made.\u2019\u2019 in a letter written at the\ntime see box 1978, p. 297 , he stated: \u2018\u2018now, when data have been faked,\ni know very well how generally people underestimate the frequency of wide\nchance deviations, so that the tendency is always to make them agree too well\nwith expectations.\u2019\u2019 in summary, goodness-of-fit tests can reveal not only\nwhen a fit is inadequate, but also when it is better than random fluctuations\nwould have us expect. r. a. fisher\u2019s daughter, joan fisher box 1978,\npp. 295\u1390300 , and freedman et al. 1978, pp. 420\u1390428, 478 discussed\nfisher\u2019s analysis of mendel\u2019s data and the accompanying controversy. despite\npossible difficulties with mendel\u2019s data, subsequent work led to general\nacceptance of his theories.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nw\n\nx\n\nj\n\n\u017e\n\n\u017e\n\n1.5.4 chi-squared theoretical justification*\nwe now outline why pearson\u2019s statistic has a limiting chi-squared distribu-\ntion. for a multinomial sample n , . . . , n\nof size n, the marginal distribu-\ntion of n is the bin n,\u2432 distribution. for large n, by the normal approxima-\ntion to the binomial, n and \u2432 s n rn have approximate normal distribu-\ntions. more generally, by the central limit theorem, the sample proportions\nrn have an approximate multivariate normal distribu-\n\u2432 s n rn, . . . , n\n\u02c6\ncy1\n.\ntion section 14.1.4 . let \u233a denote the null covariance matrix of n \u2432, and\n'\nlet \u2432 s \u2432 , . . . , \u2432\nn \u2432 y \u2432 converges to a\n\u017e\n\u017e\nn 0, \u233a distribution, the quadratic form\n\n0x\n.\n. under h , since\n\ncy1,0\n\n.\nj \u017e\n\n'\n\n\u017e\n\u017e\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n.x\n\n10\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n1\n\n1\n\n0\n\n0\n\n0\n\nc\n\nj\n\nj\n\nj\n\n0\n\nn \u2432 y \u2432 \u233a\n\u017e\n\nx y1\n\n.\n\n\u02c6\n\n0\n\n0\n\n\u017e\n\n\u2432 y \u2432\n\u02c6\n\n.\n\n0\n\n\u017e\n\n1.16\n\n.\n\nhas distribution converging to chi-squared with df s c y 1.'\n\nin section 14.1.4 we show that the covariance matrix of n \u2432 has elements\n\n\u02c6\n\ny\u2432\u2432\nk\n\n\u00bd \u2432 1 y \u2432\n\n\u017e\n\nj\n\nj\n\nj\n\n\u2434 s\njk\n\nif j / k\nif j s k\n\n.\n\n.\n\n.\n\n\u017e\n\nc0\n\ny1\n0\n\u017e\n\nj, k th element 1r\u2432 when j / k and 1r\u2432 q 1r\u2432\n.\nthe matrix \u233a has\nc0\nwhen j s k. you can verify this by showing that \u233a \u233a\nequals the identity\n.\nmatrix. with this substitution, direct calculation with appropriate combining\nof terms\nshows that 1.16 simplifies to x . in section 14.3 we provide a\nformal proof in a more general setting.\n\ny1\n0\n\nthis argument is similar to pearson\u2019s in 1900. r. a. fisher 1922 gave a\nsimpler justification, the gist of which follows: suppose that n , . . . , n\nare\nindependent poisson random variables with means \u242e , . . . , \u242e . for large\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nj0\n\n2\n\n1\n\n0\n\nc\n\n1\n\nc\n\n "}, {"Page_number": 39, "text": "j\n\nj\n\nj\n\nj\n\n4\n\n4\n\n\u0004\n\n.\n\n\u017e\n\n24\n\n'\n\nintroduction: distributions and inference for categorical data\nz s n y \u242e r \u242e have approximate stan-\n\u0004\n\u242e , the standardized values\ndard normal distributions. thus, \u00fd z 2 s x 2 has an approximate chi-squared\ndistribution with c degrees of freedom. adding the single linear constraint\n\u00fd n y \u242e s 0, thus converting the poisson distributions to a multinomial,\n\u017e\nwe lose a degree of freedom.\nwhen c s 2, pearson\u2019s x 2 simplifies to the square of the normal score\nstatistic 1.11 . for mendel\u2019s data, \u2432 s 6022r8023, \u2432 s 0.75, n s 8023,\nand z s 0.123, for which x s 0.123 s 0.015. in fact, for general c the\npearson test is the score test about multinomial parameters.\n\n\u02c6 1\n\n.2\n\n10\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ns\n\n2\n\nj\n\nj\n\nj\n\nj\n\nj\n\nj\n\n1.5.5 likelihood-ratio chi-squared\nan alternative test for multinomial parameters uses the likelihood-ratio test.\nthe kernel of the multinomial likelihood is 1.14 . under h the likelihood is\nmaximized when \u2432 s \u2432 . in the general case, it is maximized when \u2432 s\nn rn. the ratio of the likelihoods equals\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\nj0\n\n0\n\nj\n\nj\n\nj\n\n\u2333 s\n\nj\n\n\u0142 \u2432\u017e\n.\nj0\n\u0142 n rn\n\n\u017e\n\nj\n\nj\n\nn j\n.n j\n.\n\nthus, the likelihood-ratio statistic, denoted by g2, is\n\ng2 s y2 log\u2333 s 2\n\n\u00fd j\n\nn log n rn\u2432 .\n.\n\n\u017e\n\nj0\n\nj\n\n\u017e\n\n1.17\n\n.\n\n\u017e\n\n.\n\nj\n\nj\n\nj\n\n0\n\n2\n\n0\n\n\u0004\n\n4\n\n.\n\n\u0004\n4\n\nthis statistic, which has form 1.12 , is called the likelihood-ratio chi-squared\nstatistic. the larger the value of g2, the greater the evidence against h .0\nin the general case, the parameter space consists of \u2432 subject to\n\u00fd \u2432 s 1, so the dimensionality is c y 1. under h , the \u2432 are specified\nj\ncompletely, so the dimension is 0. the difference in these dimensions equals\nc y 1 . for large n, g has a chi-squared null distribution with df s c y 1.\n\u017e\nwhen h holds, the pearson x 2 and the likelihood ratio g2 both have\nasymptotic chi-squared distributions with df s c y 1. in fact, they are asymp-\ntotically equivalent in that case; specifically, x 2 y g2 converges in probabil-\nity to zero section 14.3.4 . when h is false, they tend to grow proportion-\nally to n; they need not take similar values, however, even for very large n.\nfor fixed c, as n increases the distribution of x 2 usually converges to\nchi-squared more quickly than that of g2. the chi-squared approximation\nis usually poor for g2 when nrc - 5. when c is large, it can be decent for\nx 2 for nrc as small as 1 if the table does not contain both very small and\nmoderately large expected frequencies. we provide further guidelines in\nsection 9.8.4. alternatively, one can use the multinomial probabilities\n.\nto generate exact distributions of these test statistics good et al. 1970 .\n\n.\n\n\u017e\n\n\u017e\n\n0\n\n "}, {"Page_number": 40, "text": "statistical inference for multinomial parameters\n\n25\n\nj\n\nj\n\nj\n\n4\n\n\u0004\n\n4\n\n4\n\n4\n\n\u0004\n\n\u0004\n\nj0\n\nj0\n\nj0\n\nj0\n\nj0\n\nj0\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2 \u017e\n\n1.5.6 testing with estimated expected frequencies\n1.15 compares a sample distribution to a hypothetical one\npearson\u2019s x\n\u2432 . in some applications, \u2432 s \u2432 \u242a\n\u0004\n\u017e .4\nare functions of a smaller set of\n\u02c6\nunknown parameters \u242a. ml estimates \u242a of \u242a determine ml estimates\n\u2432 \u242a of \u2432 and hence ml estimates \u242e s n\u2432 \u242a of expected frequen-\n\u0004\n\u02c6\n\u02c6\n2\ncies in x . replacing \u242e by estimates \u242e affects the distribution of x .\nwhen dim \u242a s p, the true df s c y 1 y p section 14.3.3 . pearson failed\n.\nto realize this section 16.2 .\n\n2\n\u017e .\n\n\u02c6\n\u017e .4\n\n\u02c6\n\u017e .4\n\n\u0004\n\u0004\n\nwe now show a goodness-to-fit test with estimated expected frequencies.\na sample of 156 dairy calves born in okeechobee county, florida, were\nclassified according to whether they caught pneumonia within 60 days of\nbirth. calves that got a pneumonia infection were also classified according to\nwhether they got a secondary infection within 2 weeks after the first infection\ncleared up. table 1.1 shows the data. calves that did not get a primary\ninfection could not get a secondary infection, so no observations can fall in\nthe category for \u2018\u2018no\u2019\u2019 primary infection and \u2018\u2018yes\u2019\u2019 secondary infection. that\ncombination is called a structural zero.\n\na goal of this study was to test whether the probability of primary\ninfection was the same as the conditional probability of secondary infection,\ngiven that the calf got the primary infection. in other words, if \u2432 denotes\nthe probability that a calf is classified in row a and column b of this table,\nthe null hypothesis is\n\nab\n\n0\n\nh : \u2432 q \u2432 s \u2432 r \u2432 q \u2432\n.2\n\n11\n\n12\n\n11\n\n11\n\n\u017e\n\n.\n\n12\n\n\u017e\n\n12\n\n11\n\n12\n\n11\n\n11\n\nor \u2432 s \u2432 q \u2432 . let \u2432s \u2432 q \u2432 denote the probability of primary\ninfection. the null hypothesis states that the probabilities satisfy the struc-\nture that table 1.2 shows; that is, probabilities in a trinomial for the\ncategories yes\u1390yes, yes\u1390no, no\u1390no for primary\u1390secondary infection equal\n\u2432 , \u2432 1 y \u2432 , 1 y \u2432 .\n\u017e\n.\n\ndenote the number of observations in category a, b . the ml\nestimate of \u2432 is the value maximizing the kernel of the multinomial likeli-\nhood\n\n\u017e\n2\nlet n\n\nab\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n2\n\u2432\n\n.\n\nn\n\n11\n\n\u017e\n\n\u2432y \u2432\n2\n\n.\n\nn\n\n12\n\n\u017e\n\n1 y \u2432 .\n\n.\n\n22\n\nn\n\ntable 1.1 primary and secondary pneumonia infections in calves\n\nprimary infection\nyes\nno\n\nsecondary infection\n\na\n\nyes\n\u017e\n\u017e\n\n30 38.1\n.\n0 \u138f\n\n.\n\nno\n\u017e\n\u017e\n\n63 39.0\n63 78.9\n\n.\n.\n\nsource: data courtesy of thang tran and g. a. donovan, college of veterinary medicine,\nuniversity of florida.\navalues in parentheses are estimated expected frequencies.\n\n "}, {"Page_number": 41, "text": "26\n\nintroduction: distributions and inference for categorical data\n\ntable 1.2 probability structure for hypothesis\n\nsecondary infection\n\nprimary\ninfection\nyes\nno\n\nthe log likelihood is\n\nyes\n2\n\u2432\n\u138f\n\nno\n\n\u2432 1 y \u2432\n\u017e\n.\n1 y \u2432\n\ntotal\n\u2432\n\n1 y \u2432\n\nl \u2432 s n log \u24322 q n log \u2432y \u24322 q n log 1 y \u2432 .\n\u017e\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n11\n\n12\n\n22\n\ndifferentiation with respect to \u2432 gives the likelihood equation\n\n2 n\n11\n\u2432\n\nq\n\nn\n12\n\u2432\n\ny\n\nthe solution is\n\n12\n\nn\n1 y \u2432 1 y \u2432\n\ny\n\nn\n\n22\n\ns 0.\n\n\u2432s 2 n q n r 2 n q 2 n q n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n11\n\n12\n\n11\n\n12\n\n.\n\n.\n\n22\n\n\u017e\n\n12\n\n11\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n2.\n\n\u02c6\n2\n\nfor table 1.1, \u2432s 0.494. since n s 156, the estimated expected frequen-\ncies are \u242e s n\u2432 s 38.1, \u242e s n \u2432y \u2432 s 39.0, and \u242e s n 1 y \u2432 s\n78.9. table 1.1 shows them. pearson\u2019s statistic is x 2 s 19.7. since the c s 3\npossible responses have p s 1 parameter \u2432 determining the expected\nfrequencies, df s 3 y 1 y 1 s 1. there is strong evidence against h p s\n0.00001 . inspection of table 1.1 reveals that many more calves got a primary\ninfection but not a secondary infection than h predicts. the researchers\nconcluded that the primary infection had an immunizing effect that reduced\nthe likelihood of a secondary infection.\n\n\u02c6\n\n\u02c6\n\n22\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n0\n\n0\n\nnotes\n\nsection 1.1: categorical response data\n\n\u017e\n\n.\n\n\u017e\n\n1.1. stevens 1951 defined nominal, ordinal, interval\n\nscales of measurement. other scales\nresult from mixtures of these types. for instance, partially ordered scales occur when\nsubjects respond to questions having categories ordered except for don\u2019t know or undecided\ncategories.\n\n.\n\nsection 1.3: statistical inference for categorical data\n\n1.2. the score method does not use \u2424. thus, when \u2424 is a model parameter, one can usually\ncompute the score statistic for testing h : \u2424s \u2424 without fitting the model. this is\nadvantageous when fitting several models in an exploratory analysis and model fitting is\ncomputationally intensive. an advantage of the score and likelihood-ratio methods is that\n\n0\n\n0\n\n\u02c6\n\n "}, {"Page_number": 42, "text": "problems\n\n27\n\n\u02c6<\n\nthey apply even when \u2424 s \u2b01. in that case, one cannot compute the wald statistic.\nanother disadvantage of the wald method is that its results depend on the parameteriza-\ntion; inference based on \u2424 and its se is not equivalent to inference based on a nonlinear\nfunction of it, such as log \u2424 and its se.\n\n\u02c6\n\u02c6\n\n<\n\nsection 1.4: statistical inference for binomial parameters\n\n.\n\n1.3. among others, agresti and coull 1998 , blyth and still 1983 , brown et al. 2001 , ghosh\n\u017e\n1979 , and newcombe 1998a showed the superiority of the score interval to the wald\ninterval for \u2432. of the \u2018\u2018exact\u2019\u2019 methods, blaker\u2019s 2000 has particularly good properties. it is\ncontained in the clopper\u1390pearson interval and has a nestedness property whereby an\ninterval of higher nominal confidence level necessarily contains one of lower level.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n1.4. using continuity corrections with large-sample methods provides approximations to exact\nsmall-sample methods. thus, they tend to behave conservatively. we do not present them,\nsince if one prefers an exact method, with modern computational power it can be used\ndirectly rather than approximated.\n\n1.5. in theory, one can eliminate problems with discreteness in tests by performing a supplemen-\ntary randomization on the boundary of a critical region see problem 1.19 . in rejecting the\nnull at the boundary with a certain probability, one can obtain a fixed overall type i error\nprobability \u2423 even when it is not an achievable p-value. for such randomization, the\none-sided p y value is\n\n\u017e\n\n.\n\nrandomized p-value s u = p t s t q p t ) t\n\n\u017e\n\n.\n\n\u017e\n\no\n\n.\n\n,\n\no\n\nwhere u denotes a uniform 0, 1 random variable stevens 1950 . in practice, this is not\nused, as it is absurd to let this random number influence a decision. the mid p-value\nreplaces the arbitrary uniform multiple u = p t s t\n\nby its expected value.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\no\n\nsection 1.5: statistical inference for multinomial parameters\n\n1.6. the chi-squared distribution has mean df, variance 2 df, and skewness 8rdf\n\n. it is\napproximately normal when df is large. greenwood and nikulin 1996 , kendall and stuart\n\u017e\n1979 , and lancaster 1969 presented other properties. cochran 1952 presented a\nhistorical survey of chi-squared tests of fit. see also cressie and read 1989 , koch and\n.\nbhapkar 1982 , koehler 1998 , and moore 1986b .\n\n.1r2\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nproblems\n\napplications\n\n1.1\n\nidentify each variable as nominal, ordinal, or interval.\na. uk political party preference labour, conservative, social demo-\n\n\u017e\n\ncrat\n\n.\n\n\u017e\nb. anxiety rating none, mild, moderate, severe, very severe\nc. patient survival\n\u017e\nd. clinic location london, boston, madison, rochester, montreal\n\n\u017e\nin number of months\n\n.\n\n.\n\n.\n\n "}, {"Page_number": 43, "text": "28\n\nintroduction: distributions and inference for categorical data\n\ne. response of tumor to chemotherapy complete elimination, partial\n\nreduction, stable, growth progression\n\n\u017e\n.\n\nf. favorite beverage water, juice, milk, soft drink, beer, wine\ng. appraisal of company\u2019s inventory level\n\ntoo low, about right, too\n\n\u017e\n\n\u017e\n\n.\n\nhigh\n\n.\n\n1.2 each of 100 multiple-choice questions on an exam has four possible\nanswers, one of which is correct. for each question, a student guesses\nby selecting an answer randomly.\na. specify the distribution of the studentxs number of correct answers.\nb. find the mean and standard deviation of that distribution. would it\nbe surprising if the student made at least 50 correct responses?\nwhy?\n\nc. specify the distribution of n , n , n , n , where n is the number\n\nj\n\n3\nof times the student picked choice j.\n\n1\n\n2\n\n.\n\n4\n\n\u017e\n\n.\nd. find e n , var n , cov n , n , and corr n , n .\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\nj\n\nk\n\nj\n\nk\n\nj\n\nj\n\n1.3 an experiment studies the number of insects that survive a certain\ndose of an insecticide, using several batches of insects of size n each.\nthe insects are sensitive to factors that vary among batches during the\nexperiment but were not measured, such as temperature level. explain\nwhy the distribution of the number of insects per batch surviving the\nexperiment might show overdispersion relative to a bin n, \u2432 distribu-\ntion.\n\n\u017e\n\n.\n\n1.4\n\nin his autobiography a sort of life, british author graham greene\ndescribed a period of severe mental depression during which he played\nrussian roulette. this \u2018\u2018game\u2019\u2019 consists of putting a bullet in one of\nthe six chambers of a pistol, spinning the chambers to select one at\nrandom, and then firing the pistol once at one\u2019s head.\na. greene played this game six times and was lucky that none of them\n\nresulted in a bullet firing. find the probability of this outcome.\n\nb. suppose that he had kept playing this game until the bullet fired.\nlet y denote the number of the game on which it fires. show the\nprobability mass function for y, and justify.\n\n1.5 consider the statement, \u2018\u2018please tell me whether or not you think it\nshould be possible for a pregnant woman to obtain a legal abortion if\nshe is married and does not want any more children.\u2019\u2019 for the 1996\ngeneral social survey, conducted by the national opinion research\ncenter norc , 842 replied \u2018\u2018yes\u2019\u2019 and 982 replied \u2018\u2018no.\u2019\u2019 let \u2432 denote\n\n.\n\n\u017e\n\n "}, {"Page_number": 44, "text": "problems\n\n29\n\nthe population proportion who would reply \u2018\u2018yes.\u2019\u2019 find the p-value for\ntesting h : \u2432s 0.5 using the score test, and construct a 95% confi-\ndence interval for \u2432. interpret the results.\n\n0\n\n1.6 refer to the vegetarianism example in section 1.4.3. for testing\n\n0\n\nh : \u2432s 0.5 against h : \u2432/ 0.5, show that:\n.x\na. the likelihood-ratio statistic equals 2 25log 25r12.5 s 34.7.\nb. the chi-squared form of the score statistic equals 25.0.\nc. the wald z or chi-squared statistic is infinite.\n\n\u017e\n\nw\n\na\n\n1.7\n\nin a crossover trial comparing a new drug to a standard, \u2432 denotes the\nprobability that the new one is judged better. it is desired to estimate\n\u2432 and test h : \u2432s 0.5 against h : \u2432/ 0.5. in 20 independent\nobservations, the new drug is better each time.\na. find and sketch the likelihood function. give the ml estimate of\n\na\n\n0\n\n\u2432.\n\nb. conduct a wald test and construct a 95% wald confidence interval\n\nfor \u2432. are these sensible?\n\nc. conduct a score test, reporting the p-value. construct a 95% score\n\nconfidence interval. interpret.\n\nd. conduct a likelihood-ratio test and construct a likelihood-based\n\n95% confidence interval. interpret.\n\ne. construct an exact binomial test and 95% confidence interval.\n\ninterpret.\n\nf. suppose that researchers wanted a sufficiently large sample to\nestimate the probability of preferring the new drug to within 0.05,\nwith confidence 0.95. if the true probability is 0.90, about how large\na sample is needed?\n\n1.8\n\nin an experiment on chlorophyll inheritance in maize, for 1103 seedlings\nof self-fertilized heterozygous green plants, 854 seedlings were green\nand 249 were yellow. theory predicts the ratio of green to yellow is 3:1.\ntest the hypothesis that 3:1 is the true ratio. report the p-value, and\ninterpret.\n\n1.9 table 1.3 contains ladislaus von bortkiewicz\u2019s data on deaths of\nsoldiers in the prussian army from kicks by army mules fisher 1934;\nquine and seneta 1987 . the data refer to 10 army corps, each\nobserved for 20 years. in 109 corps-years of exposure, there were no\ndeaths, in 65 corps-years there was one death, and so on. estimate the\nmean and test whether probabilities of occurrences in these five\ncategories follow a poisson distribution truncated for 4 and above.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n "}, {"Page_number": 45, "text": "30\n\nintroduction: distributions and inference for categorical data\n\ntable 1.3 data for problem 1.9\n\nnumber of\n\ndeaths\n\nnumber of\ncorps-years\n\n0\n1\n2\n3\n4\ng 5\n\n109\n65\n22\n3\n1\n0\n\n1.10 a sample of 100 women suffer from dysmenorrhea. a new analgesic is\nclaimed to provide greater relief than a standard one. after using each\nanalgesic in a crossover experiment, 40 reported greater relief with the\nstandard analgesic and 60 reported greater relief with the new one.\nanalyze these data.\n\ntheory and methods\n\n1.11 why is it easier to get a precise estimate of the binomial parameter \u2432\n\n1\nwhen it is near 0 or 1 than when it is near ?2\n\n1.12 suppose that p y s 1 s 1 y p y s 0 s \u2432, i s 1, . . . , n, where y\n\u0004\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n4\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u0004\n\n4\n\nw\n\nx\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\u017e\n\nare independent. let y s \u00fd y .\na. what are var y and the distribution of y ?\nb. when y instead have pairwise correlation \u2433) 0, show that\nvar y ) n\u2432 1 y \u2432 , overdispersion relative to the binomial. al-\ntham 1978 discussed generalizations of the binomial that allow\ncorrelated trials.\n\nc. suppose that heterogeneity exists: p y s 1 \u2432 s \u2432 for all i, but \u2432\nis a random variable with density function g \u2b48 on 0, 1 having mean\n\u2433 and positive variance. show that var y ) n\u2433 1 y \u2433 . when \u2432\nhas a beta distribution, y has the beta-binomial distribution of\nsection 13.3.\n\nd. suppose that p y s 1 \u2432 s \u2432 , i s 1, . . . , n, where \u2432 are inde-\npendent from g \u2b48 . explain why y has a bin n, \u2433 distribution\nunconditionally but not conditionally on \u2432 . hint: in each case, is\ny a sum of independent, identical bernoulli trials?\n\n<\n\u017e .\n\n. \u017e\n\ni\n\u017e .\n\n4 \u017e\n\nw\n\u017e\n\n\u0004\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nx\n\n\u0004\n\n4\n\ni\n\ni\n\ni\n\ni\n\ni\n\n<\n\n1.13 for a sequence of independent bernoulli trials, y is the number of\nsuccesses before the kth failure. explain why its probability mass\n\n "}, {"Page_number": 46, "text": "problems\n\nfunction is the negati\u00aee binomial,\n\np y s\n\u017e\n\n.\n\n\u017e\n\ny q k y 1 !\n.\ny! k y 1 !\n\u017e\n.\n\n\u2432 1 y \u2432 ,\n\n\u017e\n\n.\n\nk\n\ny\n\n31\n\ny s 0, 1, 2, . . . .\n\nw\nfor it, e y s k\u2432r 1 y \u2432 and var y s k\u2432r 1 y \u2432 , so var y )\n.\ne y ; the poisson is the limit as k \u2122 \u2b01 and \u2432\u2122 0 with k\u2432s \u242e fixed.\nx\n\u017e\n\n.2\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n1.14 for the multinomial distribution, show that\n\ncorr n , n s y\u2432\u2432 r \u2432 1 y \u2432 \u2432 1 y \u2432 .\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nj\n\nk\n\nj\n\nk\n\n\u017e\n\nj\n\nj\n\nk\n\nk\n\n'\n\nshow that corr n , n s y1 when c s 2.\n\n\u017e\n\n.\n\n1\n\n2\n\n.\n1.15 show that the moment generating function mgf\n\nfor the binomial\ndistribution is m t s 1 y \u2432q \u2432e\n, and use it to obtain the first\nthe mgf for the poisson distribution is\ntwo moments. show that\nx4\nm t s exp \u242eexp t y 1 , and use it to obtain the first two moments.\n\u017e .\n\n\u017e .\n\n\u017e .\n\nt.n\n\n\u017e\n\n\u017e\n\n\u017e\n\nw\n\n1.16 a likelihood-ratio statistic equals t . at the ml estimates, show that\n\nthe data are exp t r2 times more likely under h than under h .\n\n\u017e\n\n.\n\no\n\na\n\n0\n\no\n\n1.17 assume that y , y , . . . , y are independent from a poisson distribu-\n\n1\n\n2\n\nn\n\ntion.\na. obtain the likelihood function. show that the ml estimator \u242es y.\n\u02c6\nb. construct a large-sample test statistic for h : \u242es \u242e using i\n( )\nthe\nthe likelihood-ratio\n\n0\nthe score method, and iii\n\n( )\nii\n\n(\n\n)\n\n0\n\nwald method,\nmethod.\n\n( )\nc. construct a large-sample confidence interval for \u242e using i\n\n( )\nii\n\nthe score method, and iii\n\n(\n\n)\n\nthe\nthe likelihood-ratio\n\nwald method,\nmethod.\n\n1.18 inference for poisson parameters can often be based on connections\nwith binomial and multinomial distributions. show how to test\nh : \u242e s \u242e for two populations based on independent poisson counts\n0\n.\n\u017e\n, using a corresponding test about a binomial parameter \u2432.\ny , y\n1\nw\n. x\nhint: condition on n s y q y\nand identify \u2432s \u242e r \u242e q \u242e .\nhow can one construct a confidence interval for \u242e r\u242e based on one\nfor \u2432?\n\n\u017e\n\n1\n\n2\n\n1\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\n2\n\n1.19 a researcher routinely tests using a nominal p type i error s 0.05,\nrejecting h if the p-value f 0.05. an exact test using test statistic t\n\n\u017e\n\n.\n\n0\n\n "}, {"Page_number": 47, "text": "32\n\n\u017e\n\n.\n\n\u017e\n\nintroduction: distributions and inference for categorical data\nhas null distribution p t s 0 s 0.30, p t s 1 s 0.62, and p t s 2\n.\ns 0.08, where a higher t provides more evidence against the null.\na. with the usual p-value, show that the actual p type i error s 0.\nb. with the mid-p-value, show that the actual p type i error s 0.08.\nin parts a and b when p t s 0 s 0.30,\n.\nc. find p type i error\np t s 1 s 0.66, p t s 2 s 0.04. note that the test with mid-\n\u017e\n\u017e\np-value can be conservative or liberal. the exact test with ordinary\np-value cannot be liberal.\n\n\u017e\n.\n\n\u017e .\n\n\u017e .\n\n.\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e .\n\nd. in part a , a randomized-decision test generates a uniform random\nvariable u from 0, 1 and rejects h when t s 2 and u f . show\nthe actual p type i error s 0.05. is this a sensible test?\n\n5\n8\n\n\u017e\n\n.\n\nw\n\nx\n\n0\n\n1.20 for a binomial parameter \u2432, show how the inversion process for\n\u017e .\nconstructing a confidence interval works with a the wald test, and b\nthe score test.\n\n\u017e .\n\n1.21 for a flip of a coin,\n\n0\n\na\n\nlet \u2432 denote the probability of a head. an\nexperiment tests h : \u2432s 0.5 against h : \u2432/ 0.5, using n s 5 inde-\npendent flips.\na. show that the true null probability of rejecting h at the 0.05\nusing the\n\n0\nsignificance level is 0.0 for the exact binomial test and\nlarge-sample score test.\n\nb. suppose that truly \u2432s 0.5. explain why the probability that the\n95% clopper\u1390pearson confidence interval contains \u2432 equals 1.0.\n\u017e hint: is there any possible y for which both one-sided tests of\nh : \u2432s 0.5 have p-value f 0.025?\n\n1\n16\n\n.\n\n0\n\n1.22 consider the wald confidence interval for a binomial parameter \u2432.\nsince it is degenerate when \u2432s 0 or 1, argue that for 0 - \u2432- 1 the\n.nx\nprobability the interval covers \u2432 cannot exceed 1 y \u2432 y 1 y \u2432 ;\nhence, the infimum of the coverage probability over 0 - \u2432- 1 equals\n0, regardless of n.\n\n\u02c6\n\n\u017e\n\nw\n\nn\n\n1.23 consider the 95% binomial score confidence interval for \u2432. when\ny s 1, show that the lower limit is approximately 0.18rn;\nin fact,\n0 - \u2432- 0.18rn then falls in an interval only when y s 0. argue that\nfor large n and \u2432 just barely below 0.18rn or just barely above\n1 y 0.18rn, the actual coverage probability is about ey0.18 s 0.84.\nhence, even as n \u2122 \u2b01, this method is not guaranteed to have coverage\nprobability g 0.95 agresti and coull 1998; blyth and still 1983 .\n.\n\n\u017e\n\n1.24 from section 1.4.2 the midpoint \u2432 of the score confidence interval for\n\u2432 is the sample proportion for an adjusted data set that adds z 2 r2\n\n\u02dc\n\n\u2423r2\n\n "}, {"Page_number": 48, "text": "problems\n\n33\n\nobservations of each type to the sample. this motivates an adjusted\nwald interval,\n\n\u02dc\n\u2432\" z\n\n\u2423r2\n\n'\n\u2432 1 y \u2432 rn* ,\n\u02dc\n\n\u02dc\n\n\u017e\n\n.\n\nwhere n* s n q z\n\n2\n\u2423r2\n\n.\n\n.\n\n\u017e\n\n\u02dc\n\n\u02dc\n\nshow that the variance \u2432 1 y \u2432 rn* at the weighted average is at\nleast as large as the weighted average of the variances that appears\nunder the square root sign in the score interval hint: use jensen\u2019s\ninequality . thus, this interval contains the score interval. agresti and\ncoull 1998 and brown et al. 2001 showed that it performs much\nbetter than the wald interval. it does not have the score interval\u2019s\ndisadvantage problem 1.23 of poor coverage near 0 and 1.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nw\n\nx\n\n1.25 a binomial sample of size n has y s 0 successes.\n\nw\n\n\u017e\n\n.x\n\n2\n\u2423r2\n\na. show that the confidence interval for \u2432 based on the likelihood\nfunction is 0.0, 1 y exp yz r2 n . for \u2423s 0.05, use the expan-\nsion of an exponential function to show that this is approximately\nw\nx\n0, 2rn .\nw\n0, z r n q z\n\nis\n.x\n, or approximately 0, 4r n q 4 when \u2423s 0.05.\nc. for the clopper\u1390pearson approach, show that the upper bound is\n, or approximately ylog 0.025 rn s 3.69rn when \u2423\n\nb. for the score method,\n\nthe confidence interval\n\nshow that\n\n.1r n\n\n2\n\u2423r2\n\n2\n\u2423r2\n\n.x\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\nw\n\n\u017e\n\n1 y \u2423r2\ns 0.05.\n\nd. for the adaptation of the clopper\u1390pearson approach using the\nmid-p-value, show that the upper bound is 1 y \u24231r n, or approxi-\nmately ylog 0.05 rn s 3rn when \u2423s 0.05.\n\n\u017e\n\n.\n\n1.26 for\n\n\u017e\n\nthe geometric distribution p y s \u2432 1 y \u2432 , y s 0, 1, 2, . . . ,\nthe tail method for constructing a confidence interval\nshow that\nw\ni.e., equating p y g y\n.1r y\n\u017e\n,\n. show that all \u2432 between 0 and 1 y \u2423r2 ne\u00aeer fall\n1 y \u2423r2\n\u017e\nabove a confidence interval, and hence the actual coverage probability\nexceeds 1 y \u2423r2 over this region.\n\n.\nand p y f y\n\nto \u2423r2 yields \u2423r2\n\n.1r\u017e yq1. x\n\nw\u017e\n\ny\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\nx\n\n1.27 a statistic t has discrete distribution with cdf f t . show that f t is\nstochastically larger than uniform over 0, 1 ; that is, its cdf is every-\nwhere no greater than that of the uniform casella and berger 2001,\npp. 77, 434 . explain why an implication is that a p-value based on t\nhas null distribution that is stochastically larger than uniform.\n\nx\n\u017e\n\n.\n\nw\n\n\u017e\n\n.\n\n\u017e .\n\n1.28 suppose that p t s t s \u2432, j s 1, . . . . show that e mid-p-value s\n\n0.5. hint: show that \u00fd \u2432 \u2432r2 q \u2432 q \u2b48\u2b48\u2b48 s \u00fd \u2432 r2.\n\n.2\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\nw\n\nx\n\nj\n\nj\n\nj\n\nj\n\nj\n\njq1\n\nj\n\nj\n\n "}, {"Page_number": 49, "text": "introduction: distributions and inference for categorical data\n34\n1.29 for a statistic t with cdf f t and p t s p t s t , the mid-distribu-\nt s f t y 0.5 p t parzen 1997 . given t s t ,\n.\n\u017e .\no\n\u017e\nit also satisfies\n2\u017e\n\n\u017e\nthe mid-p-value equals 1 y f t\n\u017e\n\ntion function is f\nshow that\n.x\nw\nt s 0.5 and var f\ne f\n\n.x\n.x4 .\nt s 1r12 1 y e p t .\n\n\u017e .\n\u017e . \u017e\n\n\u017e .\n\u017e .\n\n.\n.\nw\n\nmid\n\n.\u0004\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\nw\n\no\n\nmid\n\nmid\n\nw\n\n1.30 genotypes aa, aa, and aa occur with probabilities \u242a , 2\u242a 1 y \u242a ,\n.\n1 y \u242a . a multinomial sample of size n has frequencies n , n , n\n\u017e\n.\nof these three genotypes.\na. form the log likelihood. show that \u242as 2 n q n r 2 n q 2 n q\n\n.2x\n\n\u02c6\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n2\n\n1\n\n2\n\n3\n\n1\n\n2\n\n1\n\n2\n\nb. show that y\u2b78 l \u242a r\u2b78\u242a s 2 n q n r\u242a q n q 2 n r\n.\nand that its expectation is 2 nr\u242a 1 y \u242a . use this to\n\n.\n2 n .3\n1 y \u242a\n\u017e\n\u02c6\nobtain an asymptotic standard error of \u242a.\n\n.2x\n\n2x\n\nw\u017e\n\nw\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n2\n\n2\n\n1\n\n2\n\n2\n\n3\n\nc. explain how to test whether the probabilities truly have this\n\npattern.\n\n1.31 refer to section 1.5.6. using the likelihood function to obtain the\n\ninformation, find the approximate standard error of \u2432.\u02c6\n\n1.32 refer to section 1.5.6. let a denote the number of calves that got a\nprimary, secondary, and tertiary infection, b the number that received\na primary and secondary but not a tertiary infection, c the number that\nreceived a primary but not a secondary infection, and d the number\nthat did not receive a primary infection. let \u2432 be the probability of a\nprimary infection. consider the hypothesis that the probability of\ninfection at time t, given infection at times 1, . . . , t y 1, is also \u2432, for\nt s 2, 3. show that \u2432s 3a q 2 b q c r 3a q 3b q 2 c q d .\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u02c6\n\n.\n1.33 refer to quadratic form 1.16 .\n\n\u017e\n\na. verify that the matrix quoted in the text for \u233ay1 is the inverse of\n\n0\n\n\u233a .0\n\n.\nb. show that 1.16 simplifies to pearson\u2019s statistic 1.15 .\nc. for the z\n\n\u017e\nstatistic 1.11 , show that z s x for c s 2.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n2\n\ns\n\n1.34 for testing h : \u2432 s \u2432 ,\n\nj s 1, . . . , c, using sample multinomial pro-\n\n0\n\nj\n\nj0\n\nportions \u2432 , the likelihood-ratio statistic 1.17 is\n\n.\n\n\u0004\n\n4\n\n\u02c6j\n\n2\ns\n\n\u017e\n\ng2 s y2n \u2432 log \u2432 r\u2432 .\n.\n\n\u00fd j\n\u02c6\n\n\u02c6\n\n\u017e\n\nj0\n\nj\n\nj\n\n2\n\nshow that g g 0, with equality if and only if \u2432 s \u2432 for all j. hint:\napply jensen\u2019s inequality to e y2n log x , where x equals \u2432 r\u2432\u02c6\nwith probability \u2432.\u02c6j\n\n\u02c6 j\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nj0\n\nj0\n\nj\n\n "}, {"Page_number": 50, "text": "problems\n1.35 the chi-squared mgf with df s \u242f is m t s 1 y 2 t\n\n<\n1\nt - .2\nuse it to prove the reproductive property of the chi-squared distribu-\ntion.\n\ny\u242fr2\n.\n\n, for\n\n\u017e .\n\n\u017e\n\n<\n\n35\n\n1.36 for the multinomial n, \u2432 distribution with c ) 2, confidence limits\n\n\u017e\n\n\u0004\n\n4.\n\nj\nfor \u2432 are the solutions of\n\nj\n\n\u017e\n\n\u2432 y \u2432 s z\n\u017e\n\u02c6\n\n.\n\n2\n\nj\n\nj\n\n2\n\n.\n\n\u2423r2 c\n\n\u2432 1 y \u2432 rn,\n\n\u017e\n\n.\n\nj\n\nj\n\nj s 1, . . . , c.\n\n\u0004\n\n4 \u017e\n\na. using the bonferroni inequality, argue that these c intervals simul-\ntaneously contain all \u2432 for large samples with probability at\nleast 1 y \u2423.\nb. show that the standard deviation of \u2432 y \u2432 is \u2432 q \u2432 y \u2432 y\n.2x\u2432 rn. for large n, explain why the probability is at least 1 y \u2423\nthat the wald confidence intervals\n\n\u02c6\n\n\u02c6\n\n.\n\n\u017e\n\nw\n\nk\n\nk\n\nk\n\nj\n\nj\n\nj\n\nj\n\n\u017e\n\n\u2432 y \u2432 \" z\n\u02c6\n\n\u02c6\n\n.\n\nk\n\nj\n\n\u00bd\n\n\u2423r2 a\n\n\u2432 q \u2432 y \u2432 y \u2432 rn\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n2\n\nk\n\nk\n\nj\n\nj\n\n\u017e\n\n.\n\n1r2\n\n5\n\nsimultaneously contain the a s c c y 1 r2 differences \u2432 y \u2432\n.\n\u017e\nsee fitzpatrick and scott 1987; goodman 1965 .\n\n\u017e\n\n.\n\n\u0004\n\nj\n\nk\n\n4\n\n "}, {"Page_number": 51, "text": "c h a p t e r 2\n\ndescribing contingency tables\n\nin this chapter we introduce tables that display relationships between\ncategorical variables. we also define parameters that summarize their associ-\nation. parameters in section 2.2 are used to compare groups on the propor-\ntions of responses in the outcome categories. the odds ratio has special\nimportance, appearing as a parameter in models discussed later. in section\n2.3 we extend the scope by controlling for a third variable. the association\ncan change dramatically under a control. the chapter\u2019s primary focus is\nbinary variables, which have only two categories, but in section 2.4 we\npresent parameters for nominal and ordinal multicategory variables. first, in\nsection 2.1, we introduce basic terminology and notation.\n\n2.1 probability structure for contingency tables\n\nthe joint distribution between two categorical variables determines their\nrelationship. this distribution also determines the marginal and conditional\ndistributions.\n\n2.1.1 contingency tables and their distributions\nlet x and y denote two categorical response variables, x with i categories\nand y with j categories. classifications of subjects on both variables have ij\npossible combinations. the responses x, y of a subject chosen randomly\nfrom some population have a probability distribution. a rectangular table\nhaving i rows for categories of x and j columns for categories of y displays\nthis distribution. the cells of the table represent the ij possible outcomes.\nwhen the cells contain frequency counts of outcomes for a sample, the table\n.\nis called a contingency table, a term introduced by karl pearson 1904 .\nanother name is cross-classification table. a contingency table with i rows\nand j columns is called an i = j or i-by-j\n\ntable.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n36\n\n "}, {"Page_number": 52, "text": "probability structure for contingency tables\n\n37\n\ntable 2.1 cross-classification of aspirin use and\nmyocardial infarction\n\nmyocardial infarction\n\nfatal\nattack\n\n18\n5\n\nnonfatal\nattack\n\n171\n99\n\nno\n\nattack\n10,845\n10,933\n\nplacebo\naspirin\n\nsource: preliminary report: findings from the aspirin com-\nponent of the ongoing physicians\u2019 health study. new engl.\n.\nj. med. 318: 262\u1390264 1988 .\n\n\u017e\n\ntable 2.1, a 2 = 3 contingency table, is from a report on the relationship\nbetween aspirin use and heart attacks by the physicians\u2019 health study\nresearch group at harvard medical school. the physicians\u2019 health study\nwas a 5-year randomized study of whether regular aspirin intake reduces\nmortality from cardiovascular disease. every other day, physicians participat-\ning in the study took either one aspirin tablet or a placebo. the study was\nblind\u138fthose in the study did not know whether they were taking aspirin or a\nplacebo. of the 11,034 physicians taking a placebo, 18 suffered fatal heart\nattacks over the course of the study, whereas of the 11,037 taking aspirin, 5\nhad fatal heart attacks.\n\nlet \u2432 denote the probability that x, y occurs in the cell in row i and\ncolumn j. the probability distribution \u2432 is the joint distribution of x and\ny. the marginal distributions are the row and column totals that result from\nsumming the joint probabilities. we denote these by \u2432 for the row\nvariable and \u2432 for the column variable, where the subscript \u2018\u2018q\u2019\u2019 denotes\nthe sum over that index; that is,\n\u00fd\n\n\u2432 s \u2432 and \u2432 s \u2432 .\niq\n\n\u00fd\n\niq\n\n\u017e\n\u0004\n\nqj\n\nqj\n\n.\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ni j\n\ni j\n\ni j\n\ni j\n\nj\n\ni\n\nj\n\ni\n\ni j\n\n.\n\n.\n\niq\n\nj qj\n\nthese satisfy \u00fd \u2432 s \u00fd \u2432 s \u00fd \u00fd \u2432 s 1.0. the marginal distributions\ni\nprovide single-variable information.\n\nin most contingency tables\n\n\u017e\nsuch as table 2.1 , one variable, say y, is a\n\u017e\nresponse variable and the other x is an explanatory variable. when x is\nfixed rather than random, the notion of a joint distribution for x and y is no\nlonger meaningful. however, for a fixed category of x, y has a probability\ndistribution. it is germane to study how this distribution changes as the\ncategory of x changes. given that a subject is classified in row i of x, \u2432\nj < i\ndenotes the probability of classification in column j of y, j s 1, . . . , j. note\nthat \u00fd \u2432 s 1. the probabilities \u2432 , . . . , \u2432 form the conditional distribu-\ntion of y at category i of x. a principal aim of many studies is to compare\nconditional distributions of y at various levels of explanatory variables.\n\n1< i\n\nj < i\n\nj < i\n\n\u0004\n\n4\n\nj\n\n "}, {"Page_number": 53, "text": "38\n\ndescribing contingency tables\n\ntable 2.2 estimated conditional distributions for\nbreast cancer diagnoses\n\nbreast\ncancer\nyes\nno\n\ndiagnosis of test\n\npositive\n\nnegative\n\n0.82\n0.01\n\n0.18\n0.99\n\ntotal\n1.0\n1.0\n\nsource: data from w. lawrence et al., j. natl. cancer inst.\n.\n90: 1792\u13901800 1998 .\n\n\u017e\n\n.\n\n.\n\n2.1.2 sensitivity and specificity\nthe results in table 2.2 are from a recent article about various methods of\nattempting to diagnose breast cancer. based on a literature survey, the\nauthors reported these results for the impact of using mammography to-\ngether with clinical breast examination. let x s true disease status\n\u017e\ni.e.,\nand let y s diagnosis\nwhether a woman truly has breast\n\u017e\npositive, negative , where a positive outcome predicts that a woman has\nbreast cancer. the probabilities estimated in table 2.2 are conditional\nprobabilities of y given x.\n\ncancer\n\nwith diagnostic tests for a disease, the two correct diagnoses are a positive\ntest outcome when the subject has the disease and a negative test outcome\nwhen a subject does not have it. given that the subject has the disease, the\nconditional probability that\nis positive is called the\nsensiti\u00aeity; given that the subject does not have the disease, the conditional\n.\nprobability that the test is negative is called the specificity yerushalmy 1947 .\nideally, these are both high.\n\nthe diagnostic test\n\nfor a 2 = 2 table with the format of table 2.2, sensitivity is \u2432 and\nspecificity is \u2432 . in table 2.2, the estimated sensitivity of combined mam-\nmography and clinical examination is 0.82. of women with breast cancer,\n82% are diagnosed correctly. the estimated specificity is 0.99. of women not\nhaving breast cancer, 99% were diagnosed correctly.\n\n2 <2\n\n1<1\n\n\u017e\n\nindependence of categorical variables\n\n2.1.3\nwhen both variables are response variables, descriptions of the association\ncan use their joint distribution, the conditional distribution of y given x, or\nthe conditional distribution of x given y. the conditional distribution of y\ngiven x relates to the joint distribution by\n\n\u2432 s \u2432 r\u2432\niq\n\nj < i\n\ni j\n\nfor all i and j.\n\ntwo categorical response variables are defined to be independent if all\n\njoint probabilities equal the product of their marginal probabilities,\nj s 1, . . . , j.\n\nfor i s 1, . . . , i\n\n\u2432 s \u2432 \u2432\niq qj\n\nand\n\ni j\n\n2.1\u017e\n\n.\n\n "}, {"Page_number": 54, "text": "probability structure for contingency tables\n\n39\n\ntable 2.3 notation for joint, conditional, and\nmarginal probabilities\n\ncolumn\n\nrow\n1\n\n2\n\ntotal\n\n1\n\u2432\n\u017e\n\u2432\n\u2432\n\u017e\n\u2432\n\u2432\n\n11\n1<1\n21\n1<2\nq1\n\n.\n\n.\n\n2\n\u2432\n\u017e\n\u2432\n\u2432\n\u017e\n\u2432\n\u2432\n\n12\n2 <1\n22\n2 <2\nq2\n\n.\n\n.\n\ntotal\n\u2432\n1q\n\u017e\n.\n1.0\n\u2432\n2q\n\u017e\n.\n1.0\n1.0\n\nwhen x and y are independent,\n\n\u2432 s \u2432 r\u2432 s \u2432 \u2432 r\u2432 s \u2432\nqj\n\niq qj\n\niq\n\niq\n\n\u017e\n\n.\n\nj < i\n\ni j\n\nfor i s 1, . . . , i.\n\n4\n\n\u0004\n\nj <1\n\neach conditional distribution of y is identical to the marginal distribution of\ny. thus, two variables are independent when \u2432 s \u2b48\u2b48\u2b48 s \u2432 , for j s\n1, . . . , j ; that is, the probability of any given column response is the same in\neach row. when y is a response and x is an explanatory variable, this is a\nmore natural way to define independence than 2.1 . independence is then\noften referred to as homogeneity of the conditional distributions.\n\ntable 2.3 displays notation for joint, conditional, and marginal distribu-\ntions for the 2 = 2 case. sample distributions use similar notation, with p or\n\u02c6\n\u2432 in place of \u2432. for instance, p\ndenotes the sample joint distribution. the\ni j\ncell frequencies are denoted n , and n s \u00fd \u00fd n is the total sample size.\n4\ni j\nthus,\n\nj < i\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n\u0004\n\ni j\n\ni\n\nj\n\np s n rn.\n\ni j\n\ni j\n\nthe sample proportion of times that subjects in row i made response j is\n\np s p rp s n rn ,\niq\n\niq\n\nj < i\n\ni j\n\ni j\n\nwhere n s np s \u00fd n .\n\nj\n\ni j\n\niq\n\niq\n\n2.1.4 poisson, binomial, and multinomial sampling\nthe probability distributions introduced in section 1.2 extend to cell counts\nin contingency tables. for instance, a poisson sampling model treats cell\n4\ncounts y as independent poisson random variables with parameters \u242e .\nis then the\nthe joint probability mass function for potential outcomes n\nproduct of the poisson probabilities p y s n\n\nfor the ij cells, or\n\n\u017e\n\n.\n\n\u0004\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ni j\n\ni j\n\ni j\n\ni j\n\n\u0142 \u0142\n\nexp y\u242e \u242en i jrn ! .\n\n\u017e\n\ni j\n\ni j\n\ni\n\nj\n\ni j\n.\n\ni j\n\n "}, {"Page_number": 55, "text": "40\n\ndescribing contingency tables\n\nwhen the total sample size n is fixed but the row and column totals are\nnot, a multinomial sampling model applies. the ij cells are the possible\noutcomes. the probability mass function of the cell counts has the multino-\nmial form\n\nn!r n ! \u2b48\u2b48\u2b48 n !\n\n\u017e\n\n11\n\ni j\n\n. \u0142 \u0142\n\ni\n\nj\n\nn i j\n\u2432 .\ni j\n\noften, observations on a response y occur separately at each setting of an\nexplanatory variable x. this case normally treats row totals as fixed, and for\nsimplicity, we use the notation n s n . suppose that the n observations on\ny at setting i of x are independent, each with probability distribution\nsatisfying \u00fd n s n then have\n\u0004\n\u2432 , . . . , \u2432 . the counts n ,\ni j\nthe multinomial form\n\nj s 1, . . . , j\n\niq\n\n1< i\n\nj < i\n\n\u0004\n\n4\n\n4\n\ni j\n\ni\n\ni\n\ni\n\nj\n\nn !i\n\n\u0142 n !\ni j\n\nj\n\n\u0142 j < i\nn i j\u2432 .\n\nj\n\n2.2\u017e\n\n.\n\ni j\n\ni j\n\ni j\n\ni j\n\n4\n\n4\n\n\u0004\n\nwhen samples at different settings of x are independent, the joint probabil-\nity function for the entire data set is the product of the multinomial functions\n\u017e\n.2.2 from the various settings. this sampling scheme is independent multino-\nmial sampling, also called product multinomial sampling.\n\n\u0004\n4\n\nindependent multinomial sampling also results under the following condi-\ntions: suppose that n\nresult from either independent poisson sampling\nwith means \u242e or multinomial sampling over the ij cells with probabilities\n\u2432 s \u242e rn . when x is an explanatory variable, it is sensible to perform\n\u0004\nstatistical inference conditional on the totals n s \u00fd n\neven when their\nvalues are not fixed by the sampling design. conditional on n , the cell\ncounts n , j s 1, . . . , j have the multinomial distribution 2.2 with re-\nsponse probabilities \u2432 s \u242e r\u242e ,\nj s 1, . . . , j , and cell counts from dif-\niq\nferent rows are independent. with this conditioning, we treat the row totals\nas fixed and analyze the data as if they formed separate independent\nsamples.\n\n4\ni\n.\n\nsometimes both row and column margins are naturally fixed. the appro-\npriate sampling distribution is then the hypergeometric. in section 3.5.1 we\ndiscuss this case, which is less common.\n\nj < i\n\n\u017e\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\ni j\n\ni j\n\ni j\n\ni\n\nj\n\n2.1.5 seat belt example\nresearchers in the massachusetts highway department plan to study the\nrelationship between seat-belt use yes, no and outcome of an automobile\ncrash fatality, nonfatality\nfor drivers involved in accidents on the mas-\nsachusetts turnpike. they will summarize results in the format shown in\ntable 2.4. they plan to catalog all accidents on the turnpike for the next\nyear, classifying each according to these variables. the total sample size is\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n "}, {"Page_number": 56, "text": "probability structure for contingency tables\n\n41\n\ntable 2.4 seat-belt use and results of\nautomobile crashes\n\nresult of crash\n\nfatality\n\nnonfatality\n\nseat-belt use\nyes\nno\n\n\u0004\n\n22\n\n21\n\n12\n\n11\n\n22\n\n21\n\n12\n\nthen a random variable. they might treat the numbers of observations at the\nfour combinations of seat-belt use and outcome of crash as independent\n4\npoisson random variables with unknown means \u242e ,\u242e , \u242e , \u242e .\n\nsuppose, instead, that the researchers randomly sample 200 police records\nof crashes on the turnpike in the past year and classify each according to\nseat-belt use and outcome of crash. for this study, the total sample size n\nis fixed. they might then treat the four cell counts as a multinomial ran-\ndom variable with n s 200 trials and unknown joint probabilities\n4\n\u0004\n\u2432 , \u2432 , \u2432 ,\u2432 .\n\n11\nsuppose, instead, that police records for accidents involving fatalities were\nfiled separately from the others. the researchers might instead randomly\nsample 100 records of accidents with a fatality and randomly sample 100\nrecords of accidents with no fatality. this approach fixes the column totals in\ntable 2.4 at 100. they might then regard each column of table 2.4 as an\nindependent binomial sample. yet another approach, the traditional experi-\nmental design, takes 200 subjects and randomly assigns 100 of them to wear\nseat belts; the 200 then all are forced to have an accident. the recorded\nresults would then be independent binomial samples in each row, with fixed\nrow totals of 100 each. obviously, traditional designs common in some\nexperimental science may not be ethical for humans. this is especially true in\nmedical studies.\n\n\u017e\n\n.\n\n2.1.6 types of studies\ntable 2.5 comes from one of the first studies of the link between lung cancer\nand smoking, by richard doll and a. bradford hill. in 20 hospitals in\nlondon, england, patients admitted with lung cancer in the preceding year\nwere queried about their smoking behavior. for each of the 709 patients\nadmitted, researchers studied the smoking behavior of a noncancer patient at\nthe same hospital of the same gender and within the same 5-year grouping on\nage. the 709 cases in the first column of table 2.5 are those having lung\ncancer and the 709 controls in the second column are those not having it. a\nsmoker was defined as a person who had smoked at least one cigarette a day\nfor at least a year.\n\nnormally, whether lung cancer occurs is a response variable and smoking\nbehavior is an explanatory variable. in this study, however, the marginal\n\n "}, {"Page_number": 57, "text": "42\n\ndescribing contingency tables\n\ntable 2.5 cross-classification of smoking by\nlung cancer\n\nlung cancer\n\nsmoker\nyes\nno\n\ntotal\n\ncases\n688\n21\n709\n\ncontrols\n\n650\n59\n709\n\nsource: based on data reported in table iv, r. doll and a. b.\nhill, british med. j., sept. 30, 1950, pp. 739\u1390748.\n\n\u017e\n\ndistribution of lung cancer is fixed by the sampling design, and the outcome\nmeasured is whether the subject ever was a smoker. the study, which uses a\nretrospecti\u00aee design to \u2018\u2018look into the past,\u2019\u2019 is called a case\u1390control study. such\nstudies are common in health-related applications. often, the two samples\nare matched, as in this study. sometimes the samples of cases and controls\nare independent rather than matched. for instance, another early case\u1390con-\ntrol study on lung cancer and smoking sampled subjects by sending letters to\nthe estates of physicians who had died of some type of cancer in 1950 or\n1951, and observations were cross-classified on type of cancer and the\n.\nsubject\u2019s smoking behavior see, e.g., cornfield 1956 .\n\none might want to compare smokers with nonsmokers in terms of the\nproportion who suffered lung cancer. these proportions refer to the condi-\ntional distribution of lung cancer, given smoking behavior. instead, case\u1390con-\ntrol studies provide proportions in the reverse direction, for the conditional\ndistribution of smoking behavior, given lung cancer status. for those in table\n2.5 with lung cancer, the proportion who were smokers was 688r709 s 0.970,\nwhile it was 650r709 s 0.917 for the controls.\n\nwhen we know the proportion of the population having lung cancer, we\ncan use bayes\u2019 theorem to compute sample conditional distributions in the\ndirection of main interest problem 2.21 . otherwise, using a retrospective\nsample, we cannot estimate the probability of lung cancer at each category of\nsmoking behavior. for table 2.5 we do not know the population prevalence\nof lung cancer, and the patients suffering it were probably sampled at a rate\nfar in excess of their occurrence in the general population.\n\nby contrast, imagine a study that samples subjects from the population of\nteenagers and then 60 years later measures the rates of lung cancer for the\nsmokers and nonsmokers. such a sampling design is prospecti\u00aee. there are\ntwo types of prospective studies. clinical trials randomly allocate subjects to\nthe groups who will be smokers and nonsmokers. in cohort studies, subjects\nmake their own choice about whether to smoke, and the study observes in\nfuture time who develops lung cancer. yet another approach, a cross-sec-\ntional design, samples subjects and classifies them simultaneously on both\nvariables.\n\n\u017e\n\n.\n\n "}, {"Page_number": 58, "text": "comparing two proportions\n\n43\n\nj\n\ni\n\ni j\n\n4\n\n4\n\n\u0004\n\nprospecti\u00aee studies usually condition on the totals n s \u00fd n\n\nfor cate-\ngories of x and regard each row of j counts as an independent multinomial\nsample on y. retrospecti\u00aee studies usually treat the totals n\n\u0004\nfor y as fixed\nqj\nand regard each column of i counts as a multinomial sample on x. in\ncross-sectional studies, the total sample size is fixed but not the row or column\ntotals, and the ij cell counts are a multinomial sample.\ncase\u1390control, cohort, and cross-sectional studies are called obser\u00aeational\nstudies. they simply observe who chooses each group and who has the\noutcome of interest. by contrast, a clinical trial is an experimental study, the\ninvestigator having the advantage of experimental control over which subjects\nreceive each treatment. such studies can use the power of randomization to\nmake the groups balance roughly on other variables that may be associated\nwith the response. observational studies are common but have more poten-\ntial for biases of various types.\n\n2.2 comparing two proportions\nmany studies are designed to compare groups on a binary response variable.\nthen y has only two categories, such as success, failure for outcome of a\nmedical treatment. with two groups, a 2 = 2 contingency table displays the\nresults. the rows are the groups and the columns are the categories of y.\nthis section presents parameters for comparing the groups.\n\n\u017e\n\n.\n\n2.2.1 difference of proportions\nfor subjects in row i, \u2432 is the probability that the response has outcome in\ncategory 1 \u2018\u2018success\u2019\u2019 . with only two possible outcomes, \u2432 s 1 y \u2432 , and\nwe use the simpler notation \u2432 for \u2432 . the difference of proportions of\nsuccesses, \u2432 y \u2432 ,\nis a basic comparison of the two rows. comparison\non failures is equivalent to comparison on successes, since\n\n2 < i\n\n1< i\n\n1< i\n\n1< i\n\n.\n\n\u017e\n\n1\n\n2\n\ni\n\n1 y \u2432 y 1 y \u2432 s \u2432 y \u2432 .\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n2\n\n1\n\n1\n\n2\n\nthe difference of proportions falls between y1.0 and q1.0. it equals zero\nwhen the rows have identical conditional distributions. the response y is\nstatistically independent of the row classification when \u2432 y \u2432 s 0.\n\nwhen both variables are responses, conditional distributions apply in\neither direction. one can also compare the two columns, such as by the\ndifference between the proportions in row 1. this usually is not equal to the\ndifference \u2432 y \u2432 comparing the rows.\n\n1\n\n2\n\n1\n\n2\n\n2.2.2 relative risk\na value \u2432 y \u2432 of fixed size may have greater importance when both \u2432\ni\nare close to 0 or 1 than when they are not. for a study comparing two\n\n1\n\n2\n\n "}, {"Page_number": 59, "text": "44\n\ndescribing contingency tables\n\ntreatments on the proportion of subjects who die, the difference between\n0.010 and 0.001 may be more noteworthy than the difference between 0.410\nand 0.401, even though both are 0.009. in such cases, the ratio of proportions\nis also informative.\n\nthe relati\u00aee risk is defined to be the ratio\n\n\u2432 r\u2432 .\n\n1\n\n2\n\n2.3\u017e\n\n.\n\nit can be any nonnegative real number. a relative risk of 1.0 corresponds to\nindependence. for\nthe relative risks are\n0.010r0.001 s 10.0 and 0.410r0.401 s 1.02. comparing the rows on the\nsecond response category gives a different relative risk, 1 y \u2432 r 1 y \u2432 .\n.\n\nthe proportions\n\njust given,\n\n\u017e\n\n.\n\n\u017e\n\n1\n\n2\n\n2.2.3 odds ratio\nfor a probability \u2432 of success, the odds are defined to be\n\n\u2340 s \u2432r 1 y \u2432 .\n.\n\n\u017e\n\nthe odds are nonnegative, with \u2340 ) 1.0 when a success is more likely than a\nfailure. when \u2432s 0.75, for instance, then \u2340 s 0.75r0.25 s 3.0; a success is\nthree times as likely as a failure, and we expect about three successes for\nevery one failure. when \u2340 s , a failure is three times as likely as a success.\ninversely,\n\n1\n3\n\n\u2432s \u2340r \u2340 q 1 .\n.\n\n\u017e\n\nfor instance, when \u2340 s , then \u2432s 0.25.\nrefer again to a 2 = 2 table. within row i, the odds of success instead of\nfailure are \u2340 s \u2432r 1 y \u2432 . the ratio of the odds \u2340 and \u2340 in the two\nrows,\n\n1\n3\n\n\u017e\n\n.\n\n1\n\n2\n\ni\n\ni\n\ni\n\n\u242as s\n\n1\n\n\u2340\n\n\u2340\n\n2\n\n1\n\n\u2432 r 1 y \u2432\n\u2432 r 1 y \u2432\n\n\u017e\n\u017e\n\n2\n\n1\n\n2\n\n.\n.\n\n2.4\u017e\n\n.\n\nis called the odds ratio.\nfor the odds in row i is \u2340 s \u2432 r\u2432 , i s 1, 2. then the odds ratio is\n\nfor joint distributions with cell probabilities \u2432 , the equivalent definition\n\n\u0004\n\n4\n\ni j\n\ni\n\n\u242as\n\ni1\ni2\n\u2432 r\u2432\n12\n\u2432 r\u2432\n22\n\n21\n\n11\n\ns\n\n\u2432 \u2432\n\n11\n\n22\n\n\u2432 \u2432\n\n12\n\n21\n\n.\n\n2.5\u017e\n\n.\n\nan alternative name for \u242a is the cross-product ratio, since it equals the ratio\nof the products \u2432 \u2432 and \u2432 \u2432 of probabilities from diagonally opposite\n22\n.\ncells yule 1900, 1912 .\n\n11\n\n12\n\n21\n\n\u017e\n\n "}, {"Page_number": 60, "text": "comparing two proportions\n\n45\n\n2\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\n1\n\n.\n\n\u017e\n\n2.2.4 properties of the odds ratio\nthe odds ratio can equal any nonnegative number. the condition \u2340 s \u2340\nand hence when all cell probabilities are positive \u242as 1 corresponds to\nindependence of x and y. when 1 - \u242a- \u2b01, subjects in row 1 are more\nlikely to have a success than are subjects in row 2; that is, \u2432 ) \u2432 . for\ninstance, when \u242as 4, the odds of success in row 1 are four times the odds in\nis the\nrow 2. this does not mean that\ninterpretation of a relati\u00aee risk of 4.0. when 0 - \u242a- 1, \u2432 - \u2432 . when one\ncell has zero probability, \u242a equals 0 or \u2b01.\n\nthe probability \u2432 s 4\u2432 ; that\n\nvalues of \u242a farther from 1.0 in a given direction represent stronger\nassociation. two values represent the same association, but in opposite\ndirections, when one is the inverse of the other. for instance, when \u242as 0.25,\nthe odds of success in row 1 are 0.25 times the odds in row 2, or equivalently,\nthe odds of success in row 2 are 1r0.25 s 4.0 times the odds in row 1. when\nthe order of the rows is reversed or the order of the columns is reversed, the\nnew value for \u242a is the inverse of the original value.\nfor inference, we shall see it is convenient to use log \u242a. independence\ncorresponds to log \u242as 0. the log odds ratio is symmetric about this value\u138f\nreversal of rows or of columns results in a change in its sign. two values for\nlog \u242a that are the same except for sign, such as log 4 s 1.39 and log 0.25 s\ny1.39, represent the same strength of association.\n\nthe odds ratio does not change value when the orientation of the table\nreverses so that the rows become the columns and the columns become the\nrows. this is clear from the symmetric form of 2.5 . it is unnecessary to\nidentify one classification as the response variable in order to use \u242a. in fact,\nalthough 2.4 defined it in terms of odds using \u2432 s p y s 1 x s i , one\ncould just as well define it using reverse conditional probabilities. with a\njoint distribution, conditional distributions exist in each direction, and\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\n<\n\n\u242as\n\ns\n\n\u2432 \u2432\n\n11\n\n22\n\ns\n\n<\n\np y s 1 x s 1 rp y s 2 x s 1\n.\n\u017e\np y s 1 x s 2 rp y s 2 x s 2\n\u017e\n.\n\n.\n.\n\n\u017e\n\u017e\n\n<\n\n<\n\n<\n\n21\n\n\u2432 \u2432\n12\np x s 1 y s 1 rp x s 2 y s 1\n\u017e\np x s 1 y s 2 rp x s 2 y s 2\n\u017e\n\n.\n.\n\n\u017e\n\u017e\n\n<\n\n<\n\n<\n\n<\n\n.\n.\n\n.\n\n2.6\u017e\n\n.\n\nin fact, the odds ratio is equally valid for prospective, retrospective, or\ncross-sectional sampling designs. the sample odds ratio estimates the same\nparameter in each case.\n4\n\n\u0004\n\nfor cell counts n , the sample odds ratio is\n\u02c6\u242as n n rn n .\n\ni j\n\n11\n\n22\n\n12\n\n21\n\nthis does not change when both cell counts within any row are multiplied by\na nonzero constant or when both cell counts within any column are multi-\nplied by a nonzero constant. an implication is that the sample odds ratio\n\n "}, {"Page_number": 61, "text": "46\n\ndescribing contingency tables\n\n\u017e\n\n.\n\nestimates the same characteristic \u242a even when the sample is disproportion-\nately large or small from marginal categories of a variable. for a retrospec-\ntive study of the association between vaccination and catching a certain strain\nof flu, the sample odds ratio estimates the same characteristic with a random\n\u017e .\nsample of 1 100 people who got the flu and 100 people who did not, or 2\n40 people who got the flu and 160 people who did not. the sample versions\nof the difference of proportions and relative risk 2.3 are invariant to\nmultiplication of counts within rows by a constant, but they change with\nmultiplication within columns or with row\u1390column interchange.\n\n\u017e .\n\n\u017e\n\n.\n\n2.2.5 aspirin and heart attacks revisited\nwe illustrate the three association measures with table 2.1 on aspirin use\nand heart attacks. the table differentiates between fatal and nonfatal heart\nattacks, but we combine these outcomes for now. of the 11,034 physicians\ntaking placebo, 189 suffered heart attacks, a proportion of 189r11,034 s\n0.0171. of the 11,037 taking aspirin, 104 had heart attacks, a proportion of\n0.0094. the sample difference of proportions is 0.0171 y 0.0094 s 0.0077.\nthe relative risk is 0.0171r0.0094 s 1.82. the proportion suffering heart\nattacks of those taking placebo was 1.82 times the proportion suffering heart\nattacks of those taking aspirin. the sample odds ratio is 189 = 10,933 r\n.\n10,845 = 104 s 1.83. the odds of heart attack for those taking placebo was\n\u017e\n1.83 times the odds for those taking aspirin.\n\n.\n\n\u017e\n\n<\n\n<\n\n.\n\n\u017e\n\n\u017e\n\n2.2.6 case\u2013control studies and the odds ratio\nwith retrospective sampling designs, such as case\u1390control studies,\nit is\npossible to estimate conditional probabilities of form p x s i y s j . it is\n.\nusually not possible to estimate the probability p y s j x s i of an out-\ncome of interest or the difference of proportions or relative risk for that\noutcome. it is possible to estimate the odds ratio, however, since by 2.6 it is\ndetermined by conditional probabilities in either direction.\nto illustrate, we revisit table 2.5 on x s smoking behavior and y s lung\ncancer. the data were two binomial samples on x at fixed levels of y. thus,\nwe can estimate the probability a subject was a smoker, given the outcome on\nwhether the subject had lung cancer; this was 688r709 for the cases and\n650r709 for the controls. we cannot estimate the probability of lung cancer,\ngiven whether one smoked, which is more relevant. thus, we cannot estimate\ndifferences or ratios of probabilities of lung cancer. the difference of\nproportions and relative risk are limited to comparisons of the probabilities\nof being a smoker. however, we can compute the odds ratio using the sample\n.\nanalog of 2.6 ,\n\n\u017e\n\n\u017e\n\n.\n\n688r709 r 21r709\n650r709 r 59r709\n\n.\n.\n\n\u017e\n\u017e\n\n\u017e\n\u017e\n\n. s\n.\n\n688 = 59\n650 = 21\n\ns 3.0.\n\n "}, {"Page_number": 62, "text": "partial association in stratified 2 = 2 tables\n\n47\n\n\u017e\n\n.\n\nmoreover, by 2.6 , interpretations can use the direction of interest, even\nthough the study was retrospective: the estimated odds of lung cancer for\nsmokers were 3.0 times the estimated odds for nonsmokers.\n\n2.2.7 relationship between odds ratio and relative risk\n.\nfrom definitions 2.3 and 2.4 ,\n\n.\n\n\u017e\n\n\u017e\n\nodds ratio s relative risk\n\n\u017e\n\n1 y \u2432\n1 y \u2432\n\n2\n\n1\n\n/\n\n.\n\ni\n\ntheir magnitudes are similar whenever the probability \u2432 of the outcome of\ninterest is close to zero for both groups. we saw this similarity in section\n2.2.5 for the aspirin study, where the heart attack proportion was less than\n0.02 for each group. the relative risk was 1.82 and the odds ratio was 1.83.\nbecause of this similarity, when each \u2432 is small, the odds ratio provides a\nrough indication of the relative risk when it is not directly estimable, such as\nin case\u1390control studies cornfield 1951 . for instance, for table 2.5, if the\nprobability of lung cancer is small regardless of smoking behavior, 3.0 is also\na rough estimate of the relative risk; that is, smokers had about 3.0 times the\nrelative frequency of lung cancer as nonsmokers.\n\n\u017e\n\n.\n\ni\n\n2.3 partial association in stratified 2 = 2 tables\nan important part of most studies, especially observational studies, is the\nchoice of control variables. in studying the effect of x on y, one should\ncontrol any covariate that can influence that relationship. this involves using\nsome mechanism to hold the covariate constant. otherwise, an observed\neffect of x on y may actually reflect effects of that covariate on both x and\ny. the relationship between x and y then shows confounding. experimental\nstudies can remove effects of confounding covariates by randomly assigning\nsubjects to different levels of x, but this is not possible with observational\nstudies.\n\nsuppose that a study considers effects of passive smoking, the effects on a\nnonsmoker of living with a smoker. to analyze whether passive smoking is\nassociated with lung cancer, a cross-sectional study might compare lung\ncancer rates between nonsmokers whose spouses smoke and nonsmokers\nwhose spouses do not smoke. the study should attempt to control for age,\nsocioeconomic status, or other factors that might relate both to spouse\nsmoking and to developing lung cancer. otherwise, results will have limited\nusefulness. spouses of nonsmokers may tend to be younger than spouses of\nsmokers, and younger people are less likely to have lung cancer. then a\nlower proportion of lung cancer cases among spouses of nonsmokers may\nmerely reflect their lower average age.\n\n "}, {"Page_number": 63, "text": "48\n\ndescribing contingency tables\n\nin this section we discuss the analysis of the association between categori-\ncal variables x and y while controlling for a possibly confounding variable\nz. for simplicity, the examples refer to a single control variable. in later\nchapters we treat more general cases and discuss the use of models to\nperform statistical control.\n\n2.3.1 partial tables\nwe control for z by studying the xy relationship at fixed levels of z.\ntwo-way cross-sectional slices of the three-way contingency table cross clas-\nsify x and y at separate categories of z. these cross sections are called\npartial tables. they display the xy relationship while removing the effect of\nz by holding its value constant.\n\nthe two-way contingency table obtained by combining the partial tables is\ncalled the xy marginal table. each cell count in the marginal table is a sum of\ncounts from the same location in the partial tables. the marginal table,\nrather than controlling z, ignores it. the marginal table contains no informa-\ntion about z. it is simply a two-way table relating x and y but may reflect\nthe effects of z on x and y.\n\nthe associations in partial tables are called conditional associations, be-\ncause they refer to the effect of x on y conditional on fixing z at some\nlevel. conditional associations in partial tables can be quite different from\nassociations in marginal tables. in fact, it can be misleading to analyze only\nmarginal tables of a multiway contingency table. the following example\nillustrates.\n\n2.3.2 death penalty example\ntable 2.6 is a 2 = 2 = 2 contingency table\u1390two rows, two columns, and two\nlayers\u1390from an article that studied effects of racial characteristics on whether\npersons convicted of homicide received the death penalty. the 674 subjects\nclassified in table 2.6 were the defendants in indictments involving cases\n\ntable 2.6 death penalty verdict by defendant\u2019s race and victims\u2019 race\n\nvictims\u2019\nrace\nwhite\n\nblack\n\ntotal\n\ndefendant\u2019s\n\nrace\nwhite\nblack\nwhite\nblack\nwhite\nblack\n\ndeath penalty\n\nyes\n53\n11\n0\n4\n53\n15\n\nno\n414\n37\n16\n139\n430\n176\n\npercent\n\nyes\n11.3\n22.9\n0.0\n2.8\n11.0\n7.9\n\nsource: m. l. radelet and g. l. pierce, florida law re\u00ae. 43: 1\u139034 1991 . reprinted with\npermission from the florida law re\u00aeiew.\n\n\u017e\n\n.\n\n "}, {"Page_number": 64, "text": "partial association in stratified 2 = 2 tables\n\n49\n\nfigure 2.1 percent receiving death penalty.\n\n.\n\nwith multiple murders in florida between 1976 and 1987. the variables in\ntable 2.6 are y s death penalty verdict, having the categories\n\u017e\n.\nyes, no ,\nx s race of defendant, and z s race of victims, each having the categories\n\u017e\nwhite, black . we study the effect of defendant\u2019s race on the death penalty\nverdict, treating victims\u2019 race as a control variable. table 2.6 has a 2 = 2\npartial table relating defendant\u2019s race and the death penalty verdict at each\ncategory of victims\u2019 race.\n\nfor each combination of defendant\u2019s race and victims\u2019 race, table 2.6 lists\nand figure 2.1 displays the percentage of defendants who received the death\npenalty. these describe the conditional associations. when the victims were\nwhite, the death penalty was imposed 22.9% y11.3% s 11.6% more often\nfor black defendants than for white defendants. when the victims were black,\nthe death penalty was imposed 2.8% more often for black defendants than\nfor white defendants. controlling for victims\u2019 race by keeping it fixed, the\ndeath penalty was imposed more often on black defendants than on white\ndefendants.\n\nthe bottom portion of table 2.6 displays the marginal table. it results\nfrom summing the cell counts in table 2.6 over the two categories of victims\u2019\nrace, thus combining the two partial tables e.g., 11 q 4 s 15 . overall,\n11.0% of white defendants and 7.9% of black defendants received the death\npenalty. ignoring victims\u2019 race, the death penalty was imposed less often on\nblack defendants than on white defendants. the association reverses direc-\ntion compared to the partial tables.\n\nwhy does the association change so much when we ignore versus control\nvictims\u2019 race? this relates to the nature of the association between victims\u2019\nrace and each of the other variables. first, the association between victims\u2019\n\n\u017e\n\n.\n\n "}, {"Page_number": 65, "text": "50\n\ndescribing contingency tables\n\nfigure 2.2 proportion receiving death penalty by defendant\u2019s race, controlling and ignoring\nvictims\u2019 race.\n\n.\n\n\u017e\n\nrace and defendant\u2019s race is extremely strong. the marginal table relating\nthese variables has odds ratio 467 = 143 r 48 = 16 s 87.0. second, table\n2.6 shows that, regardless of defendant\u2019s race, the death penalty was much\nmore likely when the victims were white than when the victims were black. so\nwhites are tending to kill whites, and killing whites is more likely to result in\nthe death penalty. this suggests that the marginal association should show a\ngreater tendency than the conditional associations for white defendants to\nreceive the death penalty. in fact, table 2.6 has this pattern.\n\n\u017e\n\n.\n\nfigure 2.2 illustrates why the marginal association differs so from the\nconditional associations. for each defendant\u2019s race, the figure plots the\nproportion receiving the death penalty at each category of victims\u2019 race. each\nproportion is labeled by a letter symbol giving the category of victims\u2019 race.\nsurrounding each observation is a circle having area proportional to the\nnumber of observations at that combination of defendant\u2019s race and victims\u2019\nrace. for instance, the w in the largest circle represents a proportion of\n0.113 receiving the death penalty for cases with white defendants and white\nvictims. that circle is largest because the number of cases at that combina-\ntion 53 q 414 s 467 is largest. the next-largest circle relates to cases in\nwhich blacks kill blacks.\n\nwe control for victims\u2019 race by comparing circles having the same victims\u2019\nrace letter at their centers. the line connecting the two w circles has a\npositive slope, as does the line connecting the two b circles. controlling for\nvictims\u2019 race, this reflects the death penalty being more likely for black\ndefendants than for white defendants. when we add results across victims\u2019\n\n.\n\n\u017e\n\n "}, {"Page_number": 66, "text": "partial association in stratified 2 = 2 tables\n\n51\n\nrace to get a summary result for the marginal effect of defendant\u2019s race on\nthe death penalty verdict, the larger circles, having the greater number of\ncases, have greater influence. thus,\nthe summary proportions for each\ndefendant\u2019s race, marked on the figure by periods, fall closer to the center of\nthe larger circles than to the center of the smaller circles. a line connecting\nthe summary marginal proportions has negative slope, indicating that overall\nthe death penalty was more likely for white defendants than for black\ndefendants.\n\nthe result that a marginal association can have a different direction from\neach conditional association is called simpson\u2019s paradox simpson 1951, yule\n1903 . it applies to quantitative as well as categorical variables. statisticians\ncommonly use it to caution against imputing causal effects from an associa-\ntion of x with y. for instance, when doctors started to observe strong odds\nratios between smoking and lung cancer, statisticians such as r. a. fisher\nwarned that some variable e.g., a genetic factor could exist such that the\nassociation would disappear under the relevant control. however, other\n\u017e\nstatisticians\nsuch as j. cornfield showed that with a very strong xy\nassociation, a very strong association must exist between the confounding\nvariable z and both x and y in order for the effect to disappear or change\n.\nunder the control breslow and day 1980, sec. 3.4 .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2.3.3 conditional and marginal odds ratios\nodds ratios can describe marginal and conditional associations. we illustrate\nfor 2 = 2 = k tables, where k denotes the number of categories of a control\nvariable, z. let \u242e denote cell expected frequencies for some sampling\nmodel, such as binomial, multinomial, or poisson sampling.\n\ni jk\n\n4\n\n\u0004\n\nwithin a fixed category k of z, the odds ratio\n\n\u242e \u242e\n\u242a\nx y\u017e k. \u242e \u242e\n\ns\n\n11 k\n\n22 k\n\n12 k\n\n21 k\n\n2.7\u017e\n\n.\n\ndescribes conditional xy association in partial table k. the odds ratios for\nthe k partial tables are called xy conditional odds ratios. these can be quite\ndifferent from marginal odds ratios. the xy marginal table has expected\nfrequencies \u242e s \u00fd \u242e . the xy marginal odds ratio is\n\n4\n\n\u0004\n\ni jq\n\nk\n\ni jk\n\n\u242e \u242e\n\u242a s\nx y \u242e \u242e\n\n11q 22q\n12q 21q\n\n.\n\nsample values of \u242a\n\nuse similar formulas with cell counts\nsubstituted for expected frequencies. we illustrate for the association be-\ntween defendant\u2019s race and the death penalty in table 2.6. in the first partial\n\nand \u242a\n\nx y\u017e k.\n\nx y\n\n "}, {"Page_number": 67, "text": "52\n\ndescribing contingency tables\n\ntable, victims\u2019 race is white and\n\n\u02c6\u242a s\nx y \u017e1.\n\n53 = 37\n414 = 11\n\ns 0.43.\n\n\u02c6\nx y\u017e2.\n\nthe sample odds for white defendants receiving the death penalty were 43%\nof the sample odds for black defendants. in the second partial table, victims\u2019\nrace is black and the estimated odds ratio equals \u242a s 0 = 139 16 = 4\n.\ns 0.0, since the death penalty was never given to white defendants with black\nvictims.\nestimation of the marginal odds ratio uses the 2 = 2 marginal table within\ntable 2.6, collapsing over victims\u2019 race, or 53 = 176 r 430 = 15 s 1.45.\nthe sample odds of the death penalty were 45% higher for white defendants\nthan for black defendants. yet within each victims\u2019 race category, those odds\nwere smaller for white defendants. this reversal in the association after con-\ntrolling for victims\u2019 race illustrates simpson\u2019s paradox.\n\n.\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2.3.4 marginal versus conditional independence\nmore generally, x may have i categories and y may have j categories. an\ni = j = k table describes the relationship between x and y, controlling for\nz. if x and y are independent in partial table k, then x and y are called\nconditionally independent at le\u00aeel k of z. when y is a response, this means\nthat\n\np y s j x s i, z s k s p y s j z s k ,\n\u017e\n.\n\n.\n\n\u017e\n\n<\n\n<\n\nfor all i, j.\n\n2.8\u017e\n\n.\n\nmore generally, x and y are said to be conditionally independent gi\u00aeen z\nwhen they are conditionally independent at every level of z, that is, when\n\u017e\n.2.8 holds for all k. then, given z, y does not depend on x.\nwith joint probabilities \u2432 s p x s i, y s j, z s k . then\n\nsuppose that a single multinomial applies to the entire three-way table,\n\n.4\n\n\u017e\n\n\u0004\n\ni jk\n\n\u2432 s p x s i, z s k p y s j x s i, z s k ,\n.\n\n\u017e\n\n\u017e\n\n.\n\n<\n\ni jk\n\nwhich under conditional independence of x and y, given z, equals\n\ns \u2432 p y s j z s k s \u2432 p y s j, z s k rp z s k .\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n<\n\niqk\n\niqk\n\nthus, conditional independence is then equivalent to\n\n\u2432 s \u2432 \u2432 r\u2432\n\niqk qj k\n\ni jk\n\nqqk\n\nfor all i, j, and k.\n\n2.9\u017e\n\n.\n\n "}, {"Page_number": 68, "text": "partial association in stratified 2 = 2 tables\n\n53\n\ntable 2.7 expected frequencies showing that conditional independence\ndoes not imply marginal independence\n\nclinic\n1\n\n2\n\ntotal\n\ntreatment\n\nsuccess\n\nfailure\n\nresponse\n\na\nb\na\nb\na\nb\n\n18\n12\n2\n8\n20\n20\n\n12\n8\n8\n32\n20\n40\n\nconditional independence does not imply marginal independence yule\n\n1903 . for instance, summing 2.9 over k on both sides yields\n\n.\n\n\u017e\n\n\u017e\n\n.\n\u017e\n\n\u00fd\n\nk\n\n\u2432 s \u2432 \u2432 r\u2432\n\niqk qj k\n\ni jq\n\nqqk\n\n.\n\n.\n\niqq qjq\n\nall three terms in the summation involve k, and this does not simplify to\n\u2432 s \u2432 \u2432 , marginal independence.\ni jq\nfor 2 = 2 = k tables, x and y are conditionally independent when the\nodds ratio between x and y equals 1 at each category of z. the expected\nfrequencies \u242e in table 2.7 illustrate this relation for y s response\nsuccess, failure , x s drug treatment a, b , and z s clinic 1, 2 . from\n\u017e\n\u017e\n.2.7 , the conditional xy odds ratios are\n\u017e\n\ni jk\n.\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n\u242a s\nx y\u017e1.\n\n18 = 8\n12 = 12\n\ns 1.0,\n\n\u242a s\nx y\u017e2.\n\n2 = 32\n8 = 8\n\ns 1.0.\n\n.\n\n.\n\n\u017e\n\ngiven the clinic, response and treatment are conditionally independent. the\nmarginal table combines the tables for the two clinics. its odds ratio is\n\u242a s 20 = 40 r 20 = 20 s 2.0, so the variables are not marginally inde-\n\u017e\nx y\npendent.\n\nignoring the clinic, why are the odds of a success for treatment a twice\nthose for treatment b? the conditional xz and yz odds ratios give a clue.\nthe odds ratio between z and either x or y, at each fixed category of the\nother variable, equals 6.0. for instance, the xz odds ratio at the first\ncategory of y equals 18 = 8 r 12 = 2 s 6.0. the conditional odds given\nresponse of receiving treatment a at clinic 1 are six times those at clinic 2,\nand the conditional odds given treatment of success at clinic 1 are six times\nthose at clinic 2. clinic 1 tends to use treatment a more often, and clinic 1\nalso tends to have more successes. for instance, if patients at clinic 1 tended\nto be younger and in better health than those at clinic 2, perhaps they had a\nbetter success rate regardless of the treatment received.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n "}, {"Page_number": 69, "text": "54\n\ndescribing contingency tables\n\nit is misleading to study only the marginal table, concluding that successes\nare more likely with treatment a. subjects within a particular clinic are likely\nto be more homogeneous than the overall sample, and response is indepen-\ndent of treatment in each clinic.\n\n2.3.5 homogeneous association\na 2 = 2 = k table has homogeneous xy association when\n\n\u242a s \u242a s \u2b48\u2b48\u2b48 s \u242a\nx y \u017e1.\n\nx y\u017e2.\n\nx y\u017e k .\n\n.\n\nx y\u017e k.\n\nthen the effect of x on y is the same at each category of z. conditional\nindependence of x and y is the special case in which each \u242a\n\ns 1.0.\n\nunder homogeneous xy association, homogeneity also holds for the other\nassociations. for instance, the conditional odds ratio between two categories\nof x and two categories of z is identical at each category of y. for the odds\nratio, homogeneous association is a symmetric property. it applies to any pair\nof variables viewed across the categories of the third. when it occurs, there is\nsaid to be no interaction between two variables in their effects on the other\nvariable.\nwhen interaction exists, the conditional odds ratio for any pair of vari-\nables changes across categories of the third. for x s smoking yes, no ,\n.\ny s lung cancer yes, no , and z s age - 45, 45\u139065, ) 65 , suppose that\n\u242a s 1.2, \u242a s 3.9, and \u242a s 8.8. then smoking has a weak effect\nx y\u017e1.\non lung cancer for young people, but the effect strengthens considerably with\nage. age is called an effect modifier; the effect of smoking is modified\ndepending on its value.\nfor the death penalty data table 2.6 , \u242a s 0.43 and \u242a s 0.0.\nthe values are not close, but the second estimate is unstable because of the\nzero cell count. adding\nis\nunstable and because further variation occurs from sampling variability, these\npartial tables do not necessarily contradict homogeneous association in a\npopulation. in section 6.3 we show how to analyze whether sample data are\nconsistent with homogeneous association or conditional independence.\n\n\u02c6\nx y\u017e2.\nto each cell count, \u242a s 0.94. because \u242a\n\u02c6\nx y\u017e2.\n\n\u02c6\nx y\u017e2.\n\n\u02c6\nx y\u017e1.\n\nx y\u017e3.\n\nx y\u017e2.\n\n1\n2\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n2.4 extensions for i = j tables\n\nfor 2 = 2 tables, a single number such as the odds ratio can summarize the\nassociation. for i = j tables, it is rarely possible to summarize association by\na single number without some loss of information. however, a set of odds\nratios or another summary index can describe certain features of the associa-\ntion.\n\n "}, {"Page_number": 70, "text": "extensions for i = j tables\n\n55\n\n2.4.1 odds ratios in i = j tables\nodds ratios can use each of the s i i y 1 r2 pairs of rows in combina-\ntion with each of the s j j y 1 r2 pairs of columns. for rows a and b\nand columns c and d, the odds ratio \u2432 \u2432 r \u2432 \u2432 uses four cells in a\nac\nrectangular pattern. there are\nodds ratios of this type. this set of\nodds ratios contains much redundant information.\n\n/2\n\n/2\n\n/\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n/\n\ni\n2\n\nj\n2\n\nb d\n\nad\n\nbc\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nj\n\ni\n\nconsider the subset of\n\n\u017e\n\ni y 1 j y 1 local odds ratios\n\n.\u017e\n\n.\n\n\u2432 \u2432\n\n\u242a s\ni j \u2432 \u2432\n\ni j\ni , jq1\n\niq1 , jq1\niq1 , j\n\n,\n\ni s 1, . . . , i y 1,\n\nj s 1, . . . , j y 1.\n\n\u017e\n\n2.10\n\n.\n\nfigure 2.3 shows that local odds ratios use cells in adjacent rows and adjacent\ncolumns. these i y 1 j y 1 odds ratios determine all odds ratios formed\nfrom pairs of rows and pairs of columns. to illustrate,\nin table 2.1, the\nsample local odds ratio is 2.08 for the first two columns and 1.74 for the\n\n.\u017e\n\n.\n\n\u017e\n\nfigure 2.3 odds ratios for i = j tables.\n\n "}, {"Page_number": 71, "text": "56\n\ndescribing contingency tables\n\nsecond and third columns. in each case, the more serious outcome was more\nprevalent for the placebo group. the product of these two odds ratios is 3.63,\nwhich is the odds ratio for the first and third columns.\n\nconstruction 2.10 for a minimal set of odds ratios is not unique. another\n\n.\n\n\u017e\n\nbasic set is\n\n\u2432 \u2432\n\u2423 s\ni j\ni j \u2432 \u2432\ni j\n\ni j\n\ni j\n\n,\n\ni s 1, . . . , i y 1,\n\nj s 1, . . . , j y 1.\n\n\u017e\n\n2.11\n\n.\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\nqj\n\niq\n\nthis uses the rectangular pattern of cells determined by the cell in row i and\ncolumn j and the cell in the last row and last column. figure 2.3 illustrates.\ngiven the marginal distributions \u2432 and \u2432 , when \u2432 ) 0 , conver-\nsion of the probabilities into the set of odds ratios 2.10 or 2.11 does not\ndiscard information. the cell probabilities determine the odds ratios, and\ngiven the marginals, the odds ratios determine the cell probabilities. in this\ni y 1 j y 1 parameters can describe any association in an i = j\nsense,\ni y 1 j y 1 odds ratios equaling\ntable. independence is equivalent to all\n1.0.\n\nfor three-way i = j = k tables, sets of odds ratios in the partial tables\ndescribe the conditional association. homogeneous xy association means\nthat any conditional odds ratio formed using two categories of x and two\ncategories of y is the same at each category of z.\n\n.\u017e\n\n.\u017e\n\ni j\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n2.4.2 summary measures of association\nan alternative way to describe association uses a single summary index. we\ndiscuss this first for nominal variables and then ordinal variables. the most\ninterpretable indices for nominal variables have the same structure as r-\nsquared for interval variables. it and the more general intraclass correlation\ncoefficient and correlation ratio kendall and stuart 1979 describe the\nproportional reduction in variance from the marginal distribution of the\nresponse y to the conditional distributions of y given an explanatory\nvariable x.\n\n4\nlet v y denote a measure of variation for the marginal distribution \u2432qj\nof y, and let v y i denote this measure computed for the conditional\ndistribution \u2432 , . . . , \u2432 of y at the ith setting of x. a proportional\nreduction in variation measure has the form\n\n< .\n\nj < i\n\n1< i\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n\u0004\n\nv y y e v y x\n\u017e\n\n.\n\n\u017e\n\n<\n\nv y\u017e\n\n.\n\n.\n\n,\n\n\u017e\n\n2.12\n\n.\n\nw\n\n\u017e\n\n<\n\n.x\n\nwhere e v y x is the expectation of the conditional variation taken with\nrespect to the distribution of x. for the marginal distribution \u2432 of x,\nw\ne v y x s \u00fd \u2432 v y i .\n< .\n\n.x\n\niq\n\n\u017e\n\n\u017e\n\n\u0004\n\n4\n\n<\n\niq\n\ni\n\n "}, {"Page_number": 72, "text": "extensions for i = j tables\n\n57\n\nfor a nominal response, theil 1970 proposed an index using the varia-\ntion measure v y s \u00fd\u2432 log\u2432 , called the entropy. for contingency tables,\nthe proportional reduction in entropy equals\n\nqj\n\nqj\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nu s y\n\ni\n\n\u00fd \u00fd \u2432 log \u2432 r\u2432 \u2432\niq qj\ni j\n\u00fd \u2432 log \u2432\nqj\n\nj qj\n\n\u017e\n\ni j\n\nj\n\n.\n\n,\n\n\u017e\n\n2.13\n\n.\n\nqj\n\ncalled the uncertainty coefficient. this measure is well defined when more\nthan one \u2432 ) 0. it takes value between 0 and 1: u s 0 is equivalent to\nindependence of x and y; u s 1 is equivalent to a lack of conditional\nvariation, in the sense that for each i, \u2432 s 1 for some j.\n\nvarious measures of form 2.12 describe association in i = j tables e.g.,\nproblems 2.38 and 2.39 . a difficulty with them is developing intuition for\nhow large a value constitutes a strong association. what does it mean, for\ninstance, to say that there is a 30% reduction in entropy? summary measures\nseem easier to interpret and more useful when both classifications are\nordinal, as discussed next.\n\nj < i\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n2.4.3 ordinal trends: concordant and discordant pairs\nin table 2.8 the variables are income and job satisfaction, measured for the\nblack males in a national u.s. sample. both classifications are ordinal, job\n.\nsatisfaction with the categories very dissatisfied vd , little dissatisfied ld ,\n.\nmoderately satisfied ms , and very satisfied vs .\n\nwhen x and y are ordinal, a monotone trend association is common. as\nthe level of x increases, responses on y tend to increase toward higher\nlevels, or responses on y tend to decrease toward lower levels. for instance,\nperhaps job satisfaction tends to increase as income does. a single parameter\ncan describe this trend. measures analogous to the correlation describe the\ndegree to which the relationship is monotone. some measures are based on\nclassifying each pair of subjects as concordant or discordant. a pair is\nconcordant if the subject ranked higher on x also ranks higher on y. the\n\n\u017e\n\n.\n\n\u017e\n\ntable 2.8 cross-classification of job satisfaction by income\n\njob satisfaction\n\n.\n\nvery\n\nlittle\n\ndissatisfied\n\ndissatisfied\n\nincome\n\u017e\ndollars\n- 15,000\n15,000\u139025,000\n25,000\u139040,000\n) 40,000\nsource: 1996 general social survey, national opinion research center.\n\n10\n10\n14\n9\n\n1\n2\n1\n0\n\n3\n3\n6\n1\n\nmoderately\n\nsatisfied\n\nvery\n\nsatisfied\n\n6\n7\n12\n11\n\n "}, {"Page_number": 73, "text": "58\n\ndescribing contingency tables\n\n.\n\n.\n\n\u017e\n\npair is discordant\nif the subject ranking higher on x ranks lower on y.\nthe pair is tied if the subjects have the same classification on x andror y.\nwe illustrate for table 2.8. consider a pair of subjects, one in the cell\n\u017e\n- 15, vd and the other in the cell 15\u139025, ld . this pair is concordant,\nsince the second subject ranks higher than the first both on income and on\njob satisfaction. the subject in cell - 15, vd forms concordant pairs when\nmatched with each of the three subjects classified 15\u139025, ld , so these two\ncells provide 1 = 3 s 3 concordant pairs. the subject in the cell - 15, vd\n.\nis also part of a concordant pair when matched with each of the other\n10 q 7 q 6 q 14 q 12 q 1 q 9 q 11 subjects ranked higher on both vari-\n\u017e\nthe three subjects in the - 15, ld cell are part of\nables. similarly,\nconcordant pairs when matched with the 10 q 7 q 14 q 12 q 9 q 11 sub-\njects ranked higher on both variables.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\nthe total number of concordant pairs, denoted by c, equals\n\nc s 1 3 q 10 q 7 q 6 q 14 q 12 q 1 q 9 q 11\n\n\u017e\nq 3 10 q 7 q 14 q 12 q 9 q 11 q 10 7 q 12 q 11\nq 2 6 q 14 q 12 q 1 q 9 q 11 q 3 14 q 12 q 9 q 11\nq10 12 q 11 q 1 1 q 9 q 11 q 6 9 q 11 q 14 11 s 1331.\n.\n\n\u017e\n\u017e\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\nthe total number of discordant pairs of observations is\n\nd s 3 2 q 1 q 0 q 10 2 q 3 q 1 q 6 q 0 q 1 q \u2b48\u2b48\u2b48 q12 0 q 1 q 9 s 849.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nin this example, c ) d, suggesting a tendency for low income to occur with\nlow job satisfaction and high income with high job satisfaction.\n\nconsider two independent observations from a joint probability distribu-\ntion \u2432 . for that pair, the probabilities of concordance and discordance are\n\n\u0004\n\n4\n\ni j\n\n\u2338 s 2\n\nc\n\n\u00fd \u00fd\n\ni\n\nj\n\n\u017e\n\n\u2432\ni j\n\n\u00fd \u00fd\n\nh)i k)j\n\n/\n\n\u2432 ,\n\nh k\n\n\u2338 s 2\n\nd\n\n\u017e\n\n\u2432\ni j\n\n\u00fd \u00fd\n\nh)i k-j\n\n/\n\n\u2432 .\n\nh k\n\n\u00fd \u00fd\n\ni\n\nj\n\nhere i and j are fixed in the inner summations, and the factor of 2 occurs\nbecause the first observation could be in cell\ni, j and the second in cell\n\u017e\nh, k , or vice versa. several association measures for ordinal variables utilize\nthe difference \u2338 y \u2338 .\n\n.\n\n\u017e\n\n.\n\nc\n\nd\n\n2.4.4 ordinal measure of association: gamma\ngiven that a pair is untied on both variables, \u2338 r \u2338 q \u2338 is the probabil-\nity of concordance and \u2338 r \u2338 q \u2338 is the probability of discordance. the\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nd\n\nc\n\nc\n\nd\n\nc\n\nd\n\n "}, {"Page_number": 74, "text": "notes\n\ndifference between these probabilities is\n\n\u2425s\n\n\u2338 y \u2338\n\u2338 q \u2338\n\nc\n\nc\n\n59\n\n\u017e\n\n2.14\n\n.\n\n,\n\nd\n\nd\n\n\u017e\n\n\u017e\n\n.\n\n\u02c6\n\ncalled gamma goodman and kruskal 1954 . the sample version is \u2425s c y\nd r c q d .\n.\n.\n\n\u017e\nlike the correlation, gamma treats the variables symmetrically\u138fit is\nunnecessary to identify one classification as a response variable. also like the\ncorrelation, gamma has range y1 f \u2425f 1. a reversal in the category order-\nings of one variable causes a change in the sign of \u2425. whereas the absolute\nvalue of the correlation is 1 when the relationship between x and y is\nperfectly linear, only monotonicity is required for \u2425 s 1, with \u2425s 1 if\n\u2338 s 0 and \u2425s y1 if \u2338 s 0. independence implies that \u2425s 0, but the\nconverse is not true. for instance, a u-shaped joint distribution can have\n\u2338 s \u2338 and hence \u2425s 0.\n\nd\n\nc\n\n<\n\n<\n\nc\n\nd\n\n2.4.5 gamma for job satisfaction example\nfor table 2.8, c s 1331 and d s 849. hence,\n\n\u2425s 1331 y 849 r 1331 q 849 s 0.221.\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nonly a weak tendency exists for job satisfaction to increase as income\nincreases. of the untied pairs, the proportion of concordant pairs is 0.221\nhigher than the proportion of discordant pairs.\n\nnotes\n\nsection 2.2: comparing two proportions\n\n2.1. breslow 1996 presented an interesting overview of the development of methods for\n\n\u017e\n\n.\n\ncase\u1390control studies.\n\n\u017e\n\n.\n\n2.2. for 2 = 2 tables, edwards 1963 showed that functions of the odds ratio are the only\nstatistics that are invariant both to row\u1390column interchange and to multiplication within\nrows or within columns by a constant. for i = j tables, altham 1970 gave related\nresults. yule 1912, p. 587 had argued that multiplicative invariance is a desirable\nproperty for measures of association, especially when proportions sampled in various\nmarginal categories are arbitrary. goodman 2000 showed five ways of viewing associa-\ntion in a 2 = 2 table and proposed a general measure that includes all five.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nsection 2.3: partial association in stratified 2 = 2 tables\n\n\u017e\n\n.\n\n2.3. paik 1985 proposed circle diagrams of type figure 2.2 to summarize three-way tables.\nfriendly 2000 discussed graphical presentation of categorical data. for more on\n.\nsimpson\u2019s paradox and when it can happen, see blyth 1972 , davis 1989 , dong 1998 ,\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n "}, {"Page_number": 75, "text": "60\n\ndescribing contingency tables\n\n\u017e\n\n.\n\nsamuels 1993 , and simpson 1951 . good and mittal 1989 extended it to an amalga-\nmation paradox, whereby a marginal measure is greater than the maximum or less than\nthe minimum of the partial table measures.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nsection 2.4: extensions for i = j tables\n\nn\n2\n\n/\n\n\u017e\n\n/\n\n\u017e\n\ns\n\n\u02c6\n\u017e\n\nand \u2425s c y d r\n.\n\n2.4. for continuous variables, samples can be fully ranked i.e., no ties occur , so c q d\n.\n. this is kendall\u2019s tau. agresti 1984, chaps. 9 and 10\nand kruskal 1958 surveyed ordinal measures of association. these also apply when one\nvariable is ordinal and the other is binary. when y is ordinal and x is nominal with\ni ) 2, no measure presented in section 2.4 is very helpful. ordinal modeling approaches\n\u017e\nsection 7.2 use a parameter for each category of x; comparing parameters compares\nthe ordinal response for pairs of categories of x.\n\n\u017e\n.\n\nn\n2\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nproblems\n\napplications\n\n.\n\n\u017e\n\n2.1 an article in the new york times feb. 17, 1999 about the psa blood\ntest for detecting prostate cancer stated:\n\u2018\u2018the test fails to detect\nprostate cancer in 1 in 4 men who have the disease false-negative\nresults , and as many as two-thirds of the men tested receive false-posi-\ntive results.\u2019\u2019 let c c denote the event of having not having prostate\ncancer, and let q y denote a positive negative test result. which is\n\u017e\ntrue: p y c s or p c y s ? p c q s or p q c s ?\n\u017e\ndetermine the sensitivity and specificity.\n\n\u017e\n\u017e\n1\n4\n\n\u017e\n.\n\n.\n.\n\n1\n4\n\n2\n3\n\n2\n3\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n<\n\n<\n\n<\n\n<\n\n2.2 a diagnostic test has sensitivity s specificity s 0.80. find the odds\n\nratio between true disease status and the diagnostic test result.\n\n2.3 table 2.9 is based on records of accidents in 1988 compiled by the\ndepartment of highway safety and motor vehicles in florida. identify\nthe response variable, and find and interpret the difference of propor-\ntions, relative risk, and odds ratio. why are the relative risk and odds\nratio approximately equal?\n\ntable 2.9 data for problem 2.3\n\nsafety equipment\nin use\nnone\nseat belt\n\ninjury\n\nfatal\n1601\n510\n\nnonfatal\n162,527\n412,368\n\nsource: florida department of highway safety and motor vehi-\ncles.\n\n "}, {"Page_number": 76, "text": "problems\n\n61\n\n2.4 consider the following two studies reported in the new york times.\n\n.\n\n\u017e\n\na. a british study reported dec. 3, 1998 that of smokers who get\nlung cancer, \u2018\u2018women were 1.7 times more vulnerable than men to\nget small-cell lung cancer.\u2019\u2019 is 1.7 the odds ratio or the relative risk?\nb. a national cancer institute study about tamoxifen and breast\ncancer reported apr. 7, 1998 that the women taking the drug were\n45% less likely to experience invasive breast cancer then were\nwomen taking placebo. find the relative risk for i those taking the\ndrug compared to those taking placebo, and ii those taking placebo\ncompared to those taking the drug.\n\n\u017e .\n\n\u017e .\n\n\u017e\n\n.\n\n\u017e\n\n.\n2.5 a study e. g. krug et al., internat. j. epidemiol., 27: 214\u1390221, 1998\nreported that the number of gun-related deaths per 100,000 people in\n1994 was 14.24 in the united states, 4.31 in canada, 2.65 in australia,\n1.24 in germany, and 0.41 in england and wales. use the relative risk\nto compare the united states with the other countries. interpret.\n\n2.6 a newspaper article preceding the 1994 world cup semifinal match\nbetween italy and bulgaria stated that \u2018\u2018italy is favored 10\u139011 to beat\nbulgaria, which is rated at 10\u13903 to reach the final.\u2019\u2019 suppose that this\nmeans that the odds that italy wins are\nand the odds that bulgaria\n. find the probability that each team wins, and comment.\nwins are\n\n11\n10\n\n3\n10\n\n2.7\n\nin the united states, the estimated annual probability that a woman\nover the age of 35 dies of lung cancer equals 0.001304 for current\nsmokers and 0.000121 for nonsmokers m. pagano and k. gauvreau,\nprinciples of biostatistics, duxbury press, pacific grove, ca. 1993,\n.\np. 134 .\na. find and interpret the difference of proportions and the relative\n\n\u017e\n\nrisk. which measure is more informative for these data? why?\n\nb. find and interpret the odds ratio. explain why the relative risk and\n\nodds ratio take similar values.\n\n2.8 for adults who sailed on the titanic on its fateful voyage, the odds\n\u017e\nfemale, male and survival yes, no was 11.4.\n\nratio between gender\n\u017e\nfor data, see r. j. m. dawson, j. statist. ed. 3, 1995.\na. what is wrong with the interpretation, \u2018\u2018the probability of survival\nfor females was 11.4 times that for males\u2019\u2019? give the correct inter-\npretation. when would the quoted interpretation be approximately\ncorrect?\n\n.\n\n\u017e\n\n.\n\n.\n\nb. the odds of survival for females equaled 2.9. for each gender, find\n\nthe proportion who survived.\n\n "}, {"Page_number": 77, "text": "62\n\n2.9\n\ndescribing contingency tables\n\n.\n\nin an article about crime in the united states, newsweek jan. 10,\n1994 quoted fbi statistics for 1992 stating that of blacks slain, 94%\nwere slain by blacks, and of whites slain, 83% were slain by whites. let\ny s race of victim and x s race of murderer. which conditional\ndistribution do these statistics refer to, y x, or x y ? what additional\ninformation would you need to estimate the probability that the victim\nwas white given that a murderer was white? find and interpret the\nodds ratio.\n\n<\n\n<\n\n\u017e\n\n2.10 a research study estimated that under a certain condition, the proba-\nbility that a subject would be referred for heart catheterization was\n0.906 for whites and 0.847 for blacks.\na. a press release about the study stated that the odds of referral for\ncardiac catheterization for blacks are 60% of the odds for whites.\n.\nexplain how they obtained 60% more accurately, 57% .\n\n\u017e\n\nb. an associated press story later described the study and said \u2018\u2018doc-\ntors were only 60% as likely to order cardiac catheterization for\nblacks as for whites.\u2019\u2019 explain what is wrong with this interpretation.\n\u017e\ngive the correct percentage for this interpretation.\nin stating\nresults to the general public, it is better to use the relative risk than\nthe odds ratio. it is simpler to understand and less likely to be\nmisinterpreted. for details, see new engl. j. med. 341: 279\u1390283,\n1999.\n\n.\n\n\u017e\n\n.\n\n2.11 a 20-year cohort study of british male physicians r. doll and r. peto,\nbritish med. j. 2: 1525\u13901536, 1976 noted that the proportion per year\nwho died from lung cancer was 0.00140 for cigarette smokers and\n0.00010 for nonsmokers. the proportion who died from coronary heart\ndisease was 0.00669 for smokers and 0.00413 for nonsmokers.\na. describe the association of smoking with each of lung cancer and\nheart disease, using the difference of proportions, relative risk, and\nodds ratio. interpret.\n\nb. which response is more strongly related to cigarette smoking,\nin terms of the reduction in number of deaths that would occur with\nelimination of cigarettes? explain.\n\n2.12 table 2.10 refers to applicants to graduate school at the university of\ncalifornia at berkeley, for fall 1973. it presents admissions decisions\nby gender of applicant for the six largest graduate departments. de-\nnote the three variables by a s whether admitted, g s gender, and\nd s department. find the sample ag conditional odds ratios and the\nmarginal odds ratio. interpret, and explain why they give such different\nindications of the ag association.\n\n "}, {"Page_number": 78, "text": "problems\n\n63\n\ntable 2.10 data for problem 2.12\n\nwhether admitted\n\nmale\n\nfemale\n\ndepartment\n\na\nb\nc\nd\ne\nf\n\ntotal\n\nyes\n512\n353\n120\n138\n53\n22\n1198\n\nno\n313\n207\n205\n279\n138\n351\n1493\n\u017e\n\nyes\n89\n17\n202\n131\n94\n24\n557\n.\n\nno\n19\n8\n391\n244\n299\n317\n1278\n\nsource: data from freedman et al. 1978, p.14 . see also p. bickel\n.\net al., science 187: 398\u1390403 1975 .\n\n\u017e\n\n2.13 state three \u2018\u2018real-world\u2019\u2019 variables x, y, and z for which you expect a\nmarginal association between x and y but conditional independence\ncontrolling for z.\n\n2.14 based on 1987 murder rates in the united states, an associated press\nstory reported that the probability that a newborn child has of eventu-\nally being a murder victim is 0.0263 for nonwhite males, 0.0049 for\nwhite males, 0.0072 for nonwhite females, and 0.0023 for white fe-\nmales.\na. find the conditional odds ratios between race and whether a\nmurder victim, given the gender. interpret. do these variables\nexhibit homogeneous association?\n\nb. half the newborns are of each gender, for each race. find the\n\nmarginal odds ratio between race and whether a murder victim.\n\n2.15 at each age level, the death rate is higher in south carolina than in\nmaine, but overall, the death rate is higher in maine. explain how this\ncould be possible. for data, see h. wainer, chance 12: 44, 1999.\n\n\u017e\n\n.\n\n\u017e\n\n2.16 a study of the death penalty for cases in kentucky between 1976 and\n.\n1991 t. keil and g. vito, amer. j. criminal justice 20: 17\u139036, 1995\nindicated that the defendant received the death penalty in 8% of the\n391 cases in which a white killed a white, in 2% of the 108 cases in\nwhich a black killed a black, in 12% of the 57 cases in which a black\nkilled a white, and in 0% of the 18 cases in which a white killed a\nblack. form the three-way contingency table, obtain the conditional\nodds ratios between the defendant\u2019s race and the death penalty verdict,\ninterpret those associations, study whether simpson\u2019s paradox occurs,\n\n "}, {"Page_number": 79, "text": "64\n\ndescribing contingency tables\n\nand explain why the marginal association is so different from the\nconditional associations.\n\n.\n\n\u017e\n\n2.17 an estimated odds ratio for adult females between the presence of\n\u017e\nsquamous cell carcinoma yes, no and smoking behavior\nsmoker,\nnonsmoker equals 11.7 when the smoker category has subjects whose\nsmoking level s is 0 - s - 20 cigarettes per day; it is 26.1 for smokers\nwith s g 20 cigarettes per day r. c. brownson et al., epidemiology 3:\n61\u139064, 1992 . show that the estimated odds ratio between carcinoma\n\u017e\nyes, no and the smoking levels\n\ns g 20, 0 - s - 20 equals 2.2.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n2.18 table 2.11 refers to a retrospective study of lung cancer and tobacco\nsmoking among patients in several english hospitals. the table com-\npares male lung cancer patients with control patients having other\ndiseases, according to the average number of cigarettes smoked daily\nover a 10-year period preceding the onset of the disease.\na. find the sample odds of lung cancer at each smoking level and the\nfive odds ratios that pair each level of smoking with no smoking. as\nsmoking increases, is there a trend? interpret.\n\nb. if the log odds of lung cancer is linearly related to smoking level,\nthe log odds in row i satisfies log odds s \u2423q \u2424i. show that this\nimplies that the local odds ratios are identical.\n\n\u017e\n\n.\n\ni\n\nc. using these data, can you estimate the probability of lung cancer at\n\u017e .\neach level of smoking? are the estimated odds ratios in part a\nmeaningful? explain.\n\nd. show that the disease groups are stochastically ordered with respect\nto their distributions on smoking of cigarettes see problem 2.34 and\nsection 7.3.4 . interpret.\n\n.\n\n\u017e\n\ntable 2.11 data for problem 2.18\n\ndisease group\n\npatients\n\nlung cancer\n\ncontrol\npatients\n\ndaily average\nnumber of cigarettes\nnone\n- 5\n5\u139014\n15\u139024\n25\u139049\n50 q\nsource: reprinted with permission from r. doll and a. b. hill,\n.\nbritish med. j. 2: 1271\u13901286 1952 .\n\n7\n55\n489\n475\n293\n38\n\n61\n129\n570\n431\n154\n12\n\n\u017e\n\n "}, {"Page_number": 80, "text": "problems\n\ntable 2.12 data for problem 2.19\n\n65\n\nwife\u2019s rating of sexual fun\n\nnever or\n\noccasionally\n\nfairly\noften\n\nvery\noften\n\nalmost\nalways\n\nhusband\u2019s rating\nnever or occasionally\nfairly often\nvery often\nalmost always\n\n7\n8\n5\n8\n.\nsource: reprinted with permission from hout et al. 1987 .\n\n7\n2\n1\n2\n\n\u017e\n\n2\n3\n4\n9\n\n3\n7\n9\n14\n\n2.19 table 2.12 summarizes responses of 91 married couples in arizona to a\nquestion about how often sex is fun. find and interpret a measure of\nassociation between wife\u2019s response and husband\u2019s response.\n\n2.20 table 2.13 is from an early study on the death penalty in florida.\n\nanalyze these data and show that simpson\u2019s paradox occurs.\n\ntable 2.13 data for problem 2.20\n\nvictim\u2019s\nrace\nwhite\n\nblack\n\ndefendant\u2019s\n\nrace\nwhite\nblack\nwhite\nblack\n\ndeath penalty\nyes\nno\n132\n19\n52\n11\n0\n9\n97\n6\n\nsource: reprinted with permission from m. l. radelet,\namer. sociol. re\u00ae. 46: 918\u1390927 1981\n.\n\n\u017e\n\ntheory and methods\n\n2.21 for a diagnostic test of a certain disease, \u2432 denotes the probability\nthat the diagnosis is positive given that a subject has the disease, and\n\u2432 denotes the probability that the diagnosis is positive given that a\nsubject does not have it. let \u2433 denote the probability that a subject\ndoes have the disease.\na. given that the diagnosis is positive, show that the probability that a\n\n1\n\n2\n\nsubject does have the disease is\n\n\u2432 \u2433r \u2432 \u2433q \u2432 1 y \u2433 .\n\n\u017e\n\n.\n\n1\n\n1\n\n2\n\n "}, {"Page_number": 81, "text": "66\n\ndescribing contingency tables\nb. suppose that a diagnostic test for hivq status has both sensitivity\nand specificity equal to 0.95, and \u2433s 0.005. find the probability\nthat a subject is truly hivq , given that the diagnostic test is\npositive. to better understand this answer, find the joint probabili-\nties relating diagnosis to actual disease status, and discuss their\nrelative sizes.\n\n2.22 binomial parameters for two groups are graphed, with \u2432 on the\nhorizontal axis and \u2432 on the vertical axis. plot the locus of points for\na 2 = 2 table having a relative risk s 0.5, b odds ratio s 0.5, and\n\u017e .c difference of proportions s y0.5.\n\n2\u017e .\n\n\u017e .\n\n1\n\n2.23 let d denote having a certain disease and e denote having exposure\nto a certain risk factor. the attributable risk ar is the proportion of\n.\ndisease cases attributable to that exposure see benichou 1998 .\na. let p e s 1 y p e . explain why\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nar s p d y p d e rp d .\n.\n\n\u017e\n\n.\n\n\u017e\n\n<\n\n\u017e\n\n.\n\nb. show that ar relates to the relative risk rr by\n\nar s p e rr y 1 r 1 q p e rr y 1\n\n. \u017e\n\n. \u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e .\n\n2.24 for a 2 = 2 table of counts n , show that the odds ratio is invariant\nto a interchanging rows with columns, and b multiplication of cell\ncounts within rows or within columns by c / 0. show that the differ-\nence of proportions and the relative risk do not have these properties.\n\n\u017e .\n\ni j\n\n\u0004\n\n4\n\n2.25 for given \u2432 and \u2432 , show that the relative risk cannot be farther than\n\n1\n\n2\n\nthe odds ratio from their independence value of 1.0.\n\n2.26 explain why for three events e , e , and e and their complements, it\nis possible that p e e ) p e e even if both p e e e -\n.\np e e e and p e e e - p e e e . hint: use simpson\u2019s\n3\n.\nparadox for a three-way table.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n1\n\n2\n\n3\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\n3\n\n1\n\n2\n\n3\n\n1\n\n2\n\n1\n\n2\n\n3\n\n<\n\n<\n\n<\n\n<\n\n<\n\n<\n\n2.27 let \u2432 s p x s i, y s j z s k . explain why xy conditional inde-\n\n\u017e\n\n.\n\n<\n\ni j < k\n\npendence is\n\n\u2432 s \u2432 \u2432\n\niq< k qj < k\n\ni j < k\n\nfor all i and j and k.\n\n2.28 for a 2 = 2 = 2 table, show that homogeneous association is a sym-\nmetric property, by showing that equal xy conditional odds ratios is\nequivalent to equal yz conditional odds ratios.\n\n "}, {"Page_number": 82, "text": "problems\n\n67\n\n2.29 smith and jones are baseball players. smith has a higher batting\naverage than jones in each of k years. is is possible that for the\ncombined data from the k years, jones has the higher batting aver-\nage? explain, using an example to illustrate.\n\n2.30 when x and y are conditionally dependent at each level of z yet\nmarginally independent, z is called a suppressor \u00aeariable. specify joint\n\u017e .\nprobabilities for a 2 = 2 = 2 table to show that this can happen a\nwhen there is homogeneous association, and b when the association\nhas opposite direction in the partial tables.\n\n\u017e .\n\n2.31 show that the \u2423 in 2.11 determine a all\n\nodds ratios\n4\n.\nformed from pairs of rows and pairs of columns, b all \u242a in 2.10 ,\nand vice versa.\n\n\u017e\n\n\u0004\n\ni j\n\ni j\n\n\u0004\n\n4\n\n\u017e\n\n.\n\n\u017e .\n\n\u017e\n\n/\n\nj\n2\n\n/\n\n\u017e\n\ni\n2\n\u017e .\n\n2.32 refer to problem 2.31. when all rows and columns have positive\n\nprobability, show that independence is equivalent to all \u2423 s 1 .\n4\n\n\u0004\n\ni j\n\n2.33 for i = j contingency tables, explain why the variables are indepen-\ni s 1, . . . ,\n\ni y 1 j y 1 differences \u2432 y \u2432 s 0,\n\n.\n\nj < i\n\nj < i\n\n.\u017e\ndent when the\ni y 1, j s 1, . . . , j y 1.\n\n\u017e\n\n2.34 a 2 = j table has ordinal response. let f s \u2432 q \u2b48\u2b48\u2b48 q\u2432 . when\nf f f\nfor j s 1, . . . , j, the conditional distribution in row 2 is\nstochastically higher than the one in row 1. consider the cumulati\u00aee\nodds ratios\n\n1< i\n\nj <2\n\nj <1\n\nj < i\n\nj < i\n\n\u242a s\nj\n\nj <1\n\nf r 1 y f\nf r 1 y f\n\n\u017e\n\u017e\n\nj <2\n\nj <1\n\nj <2\n\n.\n.\n\n,\n\nj s 1, . . . , j y 1.\n\nj\n\na. show that log \u242a g 0 for all j is equivalent to row 2 being stochasti-\ncally higher than row 1. explain why row 2 is then more likely than\nrow 1 to have observations at the high end of the ordinal scale.\n\nb. if all local log odds ratios are nonnegative, log \u242a g 0 for 1 f j f\nj y 1 lehmann 1966 . show by counterexample that the converse\nis not true.\n\n.\n\n\u017e\n\nj\n\n4\n\n4\n2.35 suppose that y are independent poisson variates with means \u242e .\nshow that p y s n\ni, j, conditional on y s n , satisfy\nx\nindependent multinomial sampling i.e., the product of 2.2 for all i\nwithin the rows.\n\nfor all\n\niq\n\u017e\n\n\u0004\n\u017e\n\n.\n\n.\n\nw\n\n\u0004\n\n\u0004\n\n4\n\ni j\n\ni j\n\ni j\n\ni j\n\ni\n\n "}, {"Page_number": 83, "text": "68\n\ndescribing contingency tables\n\n2.36 for 2 = 2 tables, yule 1900, 1912 introduced\n\n\u017e\n\n.\n\nq s\n\n11\n\n\u2432 \u2432 y \u2432 \u2432\n21\n\u2432 \u2432 q \u2432 \u2432\n21\n\n22\n\n12\n\n11\n\n22\n\n12\n\n,\n\nwhich he labeled q in honor of the belgian statistician quetelet. it is\nnow called yule\u2019s q.\na. show that for 2 = 2 tables, goodman and kruskal\u2019s \u2425s q.\nb. show that q falls between y1 and 1.\nc. state conditions under which q s y1 or q s 1.\nd. show that q relates to the odds ratio by q s \u242ay 1 r \u242aq 1 ,\n.\na monotone transformation of \u242a from the 0, \u2b01 scale onto the\nw\ny1,q 1 scale.\n\n\u017e\nx\n\n.\n\n\u017e\n\nx\n\nw\n\n4\n2.37 when x and y are ordinal with counts n :\n\n\u0004\n\ni j\n\n\u017e\n\nn\n\n/2\n\na. explain why the\n\nt q t y t , where t s \u00fdn\nt pairs are tied on y, and t\n\nx y\n\nx\n\nx\n\ny\n\npairs of observations partition into c q d q\nn y 1 r2 pairs are tied on x,\n\n\u017e\niq iq\npairs are tied on x and y.\n\n.\n\n.\nb. for each ordered pair of observations x , y and x ,y ,\n\nlet\nx s sign x y x and y s sign y y y . show that the sam-\nple correlation for the n n y 1 distinct x , y\n\npairs is\n\nab\n\nab\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\na\n\na\n\na\n\na\n\nb\n\nb\n\nb\n\nb\n\nx y\n\ny\n\nab\n\nab\n\n\u2436 s\nb\n\n\u00bd\n\n\u017e\n\n/\n\nn\n2\n\nc y d\n/\n\u017e\nn\n2\n\ny t\n\nx\n\ny t\n\ny\n\n.\n\n1r2\n\n5\n\n\u017e\n\nthis ordinal measure, called kendall\u2019s tau-b kendall 1945 , is less\nsensitive than gamma to the choice of response categories.\n\nn\n\n\u017e\n\nc. let d s c y d r\n.\n\ny t . explain why d is the difference be-\ntween the proportions of concordant and discordant pairs out of\nthose pairs untied on x somers 1962 . for 2 = 2 tables, d equals\nthe difference of proportions, and tau-b equals the correlation\nbetween x and y.\n\n. \u017e\n\n\u017e\n\n.\n\nx\n\n\u017e\n\n/2\n\n.\n\n2.38 goodman and kruskal 1954 proposed an association measure tau\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nfor nominal variables based on variation measure\n\nv y s \u2432 1 y \u2432 s 1 y \u24322 .\n\u017e\nqj\n\n\u00fd\n\n\u00fd\n\nqj\n\nqj\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\na. show v y is the probability that two independent observations on\n.\ny fall in different categories called the gini concentration index .\n\n\u017e\n\n "}, {"Page_number": 84, "text": "problems\n\n\u017e\n\n69\nshow that v y s 0 when \u2432 s 1 for some j and v y takes\n.\nmaximum value of\n.x\nb. for the proportional reduction in variation, show that e v y x\ns 1 y \u00fd \u00fd \u2432 r\u2432 . the resulting measure 2.12 is called the\nconcentration coefficient. like u, \u2436s 0 is equivalent to indepen-\ndence. haberman 1982 presented generalized concentration and\nuncertainty coefficients.\n\nj y 1 rj when \u2432 s 1rj for all\nw\n\n.\nx\n\niq\n\nqj\n\nqj\n\n2\ni j\n\nj.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nw\n\ni\n\nj\n\n<\n\n2.39 the measure of association lambda for nominal variables goodman\nand v y i s 1 y\n.\nand kruskal 1954\nmax \u2432 . interpret lambda as a proportional reduction in prediction\nerror for predictions which select the response category that is most\nlikely. show that independence implies \u242ds 0 but that the converse is\nnot true.\n\nhas v y s 1 y max \u2432\nqj\n\n< .\n\nj < i\n\n\u017e\n\n.\n\n\u017e\n\n\u0004\n\n4\n\n\u0004\n\n4\n\nj\n\n\u017e\n\n "}, {"Page_number": 85, "text": "c h a p t e r 3\n\ninference for contingency tables\n\nin this chapter we introduce inferential methods for contingency tables.\nmany of these methods also play a vital role in analyses of later chapters for\nwhich categorical data need not have contingency table form. the methods\nassume poisson, multinomial, or independent binomial sampling.\n\nin section 3.1 we present confidence intervals for measures of association\nfor 2 = 2 tables such as the odds ratio. section 3.2 covers chi-squared tests of\nthe hypothesis of independence between two categorical variables. like any\nsignificance test, these have limited usefulness. in section 3.3 we show how\nto follow-up the test using residuals or the partitioning property of chi-squared\nto extract components that describe the evidence about the association. in\nsection 3.4 we present more powerful\ninference applicable with ordered\ncategories. the methods of sections 3.1 through 3.4 assume large samples. in\nsections 3.5 and 3.6 we introduce small-sample methods.\n\n3.1 confidence intervals for association parameters\nthe accuracy of estimators of association parameters is characterized by\nstandard errors of their sampling distributions. in this section we present\nlarge-sample standard errors and confidence intervals.\n\ni j\n\n\u02c6\n\ninterval estimation of odds ratios\n\n3.1.1\nthe sample odds ratio \u242as n n rn n\nfor a 2 = 2 table equals 0 or \u2b01 if\nany n s 0, and it is undefined if both entries in a row or column are zero.\nsince these outcomes have positive probabilities, the expected value and\nvariance of \u242a and log \u242a do not exist.\nin fact, this is also true for ml\nestimators of model parameters presented in later chapters. in terms of bias\n.\nand mean-squared error, gart and zweiful 1967 and haldane 1956\n\n\u02c6\n\n\u02c6\n\n11\n\n22\n\n12\n\n21\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n70\n\n "}, {"Page_number": 86, "text": "confidence intervals for association parameters\n\n71\n\nshowed that the amended estimators\n\n\u02dc\u242as\n\n\u017e\n\u017e\n\n11\n\nn q 0.5 n q 0.5\nn q 0.5 n q 0.5\n\n. \u017e\n. \u017e\n\n22\n\n.\n.\n\n12\n\n21\n\n.\nand log \u242a behave well problem 14.4 .\n\n\u017e\n\n\u02dc\n\n\u02c6\n\n\u02dc\n\nthe estimators \u242a and \u242a have the same asymptotic normal distribution\naround \u242a. unless n is quite large, however, their distributions are highly\nskewed. when \u242as 1, for instance, \u242a cannot be much smaller than \u242a since\n\u242ag 0 , but it could be much larger with nonnegligible probability. the log\n\u02c6\ntransform, having an additive rather than multiplicative structure, converges\nmore rapidly to normality. an estimated standard error for log \u242a is\n\n\u02c6\n\n\u02c6\n\n.\n\n\u017e\n\n\u2434 log \u242a s\n\u02c6\u017e\n\n\u02c6\n\n.\n\n\u017e\n\n1\nn\n\n11\n\nq\n\n1\nn\n\n12\n\nq\n\n1\nn\n\n21\n\nq\n\n/\n\n1\nn\n\n22\n\n1r2\n\n.\n\nwe derive this formula in section 3.1.7.\n\n\u02c6\nby the large-sample normality of log \u242a,\n\u02c6\u017e\n\n\u02c6\nlog \u242a\" z \u2434 log \u242a\n\n\u02c6\n\n\u2423r2\n\n.\n\n3.1\u017e\n\n.\n\n3.2\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u02c6\n\nis a wald confidence interval for log \u242a. exponentiating taking antilogs of its\nendpoints provides a confidence interval for \u242a. woolf 1955 proposed this\ninterval. it works quite well, usually being a bit conservative i.e., actual\n.\ncoverage probability higher than the nominal level .\nwhen \u242as 0 or \u2b01, woolf\u2019s interval does not exist. when \u242as 0, one should\ntake 0 as the lower limit and when \u242as \u2b01, one should take \u2b01 as the upper\nlimit. the other bound can use the woolf formula following some adjust-\nby n q 0.5 in the\n\u0004\nment, such as gart\u2019s 1966 , which replaces n\nestimator and standard error. a less ad hoc approach forms the interval by\n\u017e\ninverting score tests cornfield 1956 or likelihood-ratio tests for \u242a, as we\ndiscuss in section 3.1.8.\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n4\n\n\u0004\n\n4\n\ni j\n\ni j\n\n3.1.2 aspirin and myocardial infarction example\nwe illustrate inference for the odds ratio with table 3.1 based on a swedish\nstudy of the association between aspirin use and myocardial infarction similar\nto that described in section 2.2.5. the study randomly assigned 1360 patients\nwho had already suffered a stroke to an aspirin treatment one low-dose\ntablet a day or to a placebo treatment. table 3.1 reports the number of\ndeaths due to myocardial infarction during a follow-up period of about 3\nyears.\nthe sample odds ratio \u242as 1.56 is close to \u242as 1.55, since no cell count is\nespecially small. the standard error 3.1 of log\u242as 0.445 is \u2434 log\u242a s 0.307.\n\n\u02c6\n.\n\n\u02c6\n\n\u02dc\n\n\u02c6\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u02c6\n\n "}, {"Page_number": 87, "text": "72\n\ninference for contingency tables\n\ntable 3.1 swedish study on aspirin use and\nmyocardial infarction\n\nmyocardial infarction\n\nyes\n28\n18\n\nno\n656\n658\n\ntotal\n684\n676\n\nplacebo\naspirin\n\nsource: based on results described in lancet 338: 1345\u13901349\n.\n\u017e\n1991 .\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\na 95% confidence interval for log\u242a in the population this sample represents\nis 0.445 \" 1.96 0.307 , or y0.157, 1.047 . the corresponding interval for \u242a is\nw\nexp y0.157 , exp 1.047 , or 0.85, 2.85 . the estimate of the true odds ratio\n\u017e\nis rather imprecise.\n\n.x\n\nsince the confidence interval for \u242a contains 1.0, it is plausible that the true\nodds of death due to myocardial infarction are equal for aspirin and placebo.\nif there truly is a beneficial effect of aspirin but the odds ratio is not large, it\nmay require a large sample size to show that benefit because of the relatively\n.\nsmall number of myocardial infarction cases problem 3.21 .\n\n.\n\n\u017e\n\ninterval estimation of difference of proportions\n\n3.1.3\nthe difference of proportions and the relative risk compare conditional\ndistributions of a response variable for two groups. for these measures, we\ntreat the samples as independent binomials. for group i, y has a binomial\ndistribution with sample size n and a probability \u2432 of a \u2018\u2018success\u2019\u2019 response.\nthe sample proportion \u2432 s y rn has expectation \u2432 and variance\n\u2432 1 y \u2432 rn . since \u2432 and \u2432 are independent, their difference has\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\u02c6i\n\u02c6\n\n2\n\ni\n\ni\n\n\u02c6\n\n1\n\ni\n\ne \u2432 y \u2432 s \u2432 y \u2432\n\u017e\n\n.\n\n\u02c6\n\n\u02c6\n\n1\n\n2\n\n1\n\n2\n\nand standard error\n\n\u2434 \u2432 y \u2432 s\n\n\u017e\n\n\u02c6\n\n1\n\n\u02c6\n\n2\n\n.\n\n\u2432 1 y \u2432\n\n\u017e\n\n1\n\n1\n\nn\n\n1\n\n.\n\nq\n\n\u2432 1 y \u2432\n\n\u017e\n\n2\n\n2\n\nn\n\n2\n\n1r2\n\n.\n\n.\n\n3.3\u017e\n\n.\n\ni\n\n1\n\n\u017e\n\n\u02c6\n\n\u017e\n\u02c6 \u02c6\n\n.\n\u017e\n\u02c6 \u02c6\n\n.\n\u02c6\n2\n\u2432 y \u2432 \" z \u2434 \u2432 y \u2432\n\u017e\n\u02c6\n\u02c6\n\nthe estimate \u2434 \u2432 y \u2432 uses formula 3.3 with \u2432 replaced by \u2432. then\n.\n3.4\u017e\nis a wald confidence interval for \u2432 y \u2432 . like the wald interval 1.13 for a\nsingle proportion,\nit usually has true coverage probability less than the\nnominal confidence coefficient, especially when \u2432 and \u2432 are near 0 or 1.\nmore complex but better methods are cited in section 3.1.8, note 3.2, and\nproblem 3.23.\n\n\u2423r2\n\n\u02c6\n\n.\n\n.\n\n\u017e\n\n.\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\ni\n\n "}, {"Page_number": 88, "text": "confidence intervals for association parameters\n\n73\n\ninterval estimation of relative risk\n\n3.1.4\nthe sample relative risk is r s \u2432 r\u2432 . like the odds ratio, it converges to\nnormality faster on the log scale. the asymptotic standard error of log r is\n\n\u02c61\n\n\u02c6\n\n2\n\n\u2434 log r s\n\n\u017e\n\n.\n\n\u017e\n\n1 y \u2432\n\u2432 n\n\n1\n\n1\n\n1\n\nq\n\n1 y \u2432\n\u2432 n\n\n2\n\n2\n\n2\n\n1r2\n\n.\n\n/\n\n\u017e\n\n3.5\n\n.\n\n\u017e\n\n.\n\nthe wald interval exponentiates endpoints of log r \" z \u2434 log r . it works\nwell but can be somewhat conservative. we discuss an alternative method in\nsection 3.1.8.\n\n\u02c6\u2423r2\n\nfor table 3.1, the sample proportion of myocardial infarction deaths was\n0.0409 for subjects taking placebo and 0.0266 for subjects taking aspirin. the\nsample relative risk is 0.0409r0.0266 s 1.54. the 95% confidence interval for\nthe log relative risk of log 1.54 \" 1.96 0.297 translates to 0.86, 2.75 for the\nrelative risk. we infer that the death rate for those taking placebo was\nbetween 0.86 and 2.75 times that for those taking aspirin. the wald 95%\nconfidence interval for \u2432 y \u2432 is 0.014 \" 1.96 0.0098 or y0.005, 0.033 .\n.\naccording to either measure, substantial public health benefits could result\nfrom taking aspirin, but no effect or a slight negative effect are also plausible.\nresults for the larger study described in section 2.2.5 do show a benefit.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n1\n\n2\n\nn\n\n3.1.5 deriving standard errors with the delta method*\na simple and useful method exists of deriving standard errors for large-sam-\nple inferences. let t denote a statistic that is asymptotically normally\ndistributed about a parameter \u242a, the subscript n expressing its dependence\non sample size. suppose that an estimator is a function g t of t . then,\nunder mild conditions, g t itself has a large-sample normal distribution.\nthe standard error depends on how fast g t changes for t near \u242a.\nspecifically, for large n, suppose that t is normally distributed about\n\u242a with standard error \u2434r n . that is, as n \u2122 \u2b01, the cdf of\n' \u017e\nn t y \u242a\n.\nconverges to the cdf of a normal random variable with mean 0 and variance\n\u24342. this limiting behavior is an example of con\u00aeergence in distribution,\ndenoted by\n\n\u017e .\n\n'\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nn\n\nn\n\nn\n\nn\n\nn\n\n'n t y \u242a \u2122 n 0, \u2434 .\n.\n\n\u017e\n\n.\n\n\u017e\n\nd\n\n2\n\nn\n\nlet g be a function that is at least twice differentiable at \u242a. using the taylor\nin a neighborhood of t s \u242a, in section 14.1.2 we\n\u017e .\nseries expansion for g t\nshow\n\n'\nn g t y g \u242a f n t y \u242a g \u242a\n\u017e\n\n'\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nx\n\n.\n\nn\n\nn\n\n "}, {"Page_number": 89, "text": "74\n\ninference for contingency tables\n\nfigure 3.1 depiction of delta method.\n\nx\n\n2\n\n2\n\n2\n\n2\n\nn\n\nn\n\nd\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nx\u017e\n\n2.\n\n2.\n\n.x2\n\n3.6\n\nfor large n, where g \u242a s \u2b78gr\u2b78t evaluated at t s \u242a. recall if a variate\ny ; n 0, \u2434 , then cy ; n 0, c \u2434 . thus,\n\u017e\n\n'n g t y g \u242a \u2122 n 0, g \u242a \u2434 .\n.\n\u017e\n\u017e\n\nin other words, g t is approximately normal around g \u242a with variance\nw\ng \u242a \u2434 rn.\nx\u017e\n\u017e .\nfigure 3.1 portrays this result. locally around \u242a, g t\n\nis approximately\nlinear, with slope g \u242a . then g t is approximately normal, since linear\ntransformations of normal random variables are themselves normal. the\nx\u017e\n<\ndispersion of g t values about g \u242a is about\ng \u242a times the dispersion\n1\nof t values about \u242a. if the slope of g at \u242a is\n, then g maps a region of t\nn\n2\nvalues into a region of g t values only about half as wide.\nresult 3.6 is called the delta method. since g \u242a and \u2434 s \u2434 \u242a\n.\n2\u017e\nusually depend on the unknown parameter \u242a, the asymptotic variance is\nunknown. confidence intervals and tests substitute t for \u242a and use the\nresult that n g t y g \u242a r g t \u2434 t is asymptotically standard nor-\n\u017e\nmal. for instance,\n\n' w\n\n.x\n\n. <\n\n. <\n\nx\u017e\n\nx\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\nn\n\nn\n\nn\n\nn\n\nn\n\nn\n\nn\n\nn\n\n2\n\nx\n\n<\n\n.\nis a large-sample wald 95% confidence interval for g \u242a .\n\nx\n\nn\n\n.\n\n'\ng t \" 1.96 g t \u2434 t r n\n\u017e\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nn\n\nn\n\n3.1.6 delta method applied to sample logit*\nwe illustrate the delta method for a function of the ml estimator t s \u2432s\u02c6\nyrn of the binomial parameter \u2432, for y successes in n trials. since e y s n\u2432\nand var y s n\u2432 1 y \u2432 , e \u2432 s \u2432 and var \u2432 s \u2432 1 y \u2432 rn. also, \u2432\n\u02c6\n\nn\u017e\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 90, "text": "confidence intervals for association parameters\n\n75\n\nhas a large-sample normal distribution by the central limit theorem. so do\nmany functions of \u2432.\u02c6\n\nthe log odds function of \u2432,\u02c6\n\ng \u2432 s log \u2432r 1 y \u2432 ,\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\nis called the sample logit. evaluated at \u2432, its derivative equals 1r\u2432 1 y \u2432 .\n.\nis\nby the delta method,\n.x\n\u2432 1 y \u2432 rn the variance of \u2432 multiplied by the square of 1r\u2432 1 y \u2432 .\n\u017e\nthat is\n\nthe asymptotic variance of\n\nthe sample logit\n\n\u02c6\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\nw\n\n\u017e\n'n log\n\n\u02c6\n\u2432\n\n1 y \u2432\n\u02c6\n\ny log\n\n\u2432\n\n1 y \u2432\n\n/\n\nd\n\n\u2122 n 0,\n\n\u017e\n\n1\n\n\u2432 1 y \u2432\n\n\u017e\n\n/\n\n.\n\n.\n\n\u02c6\n\n\u017e\n\n\u017e\n\n\u02c6\n\n.xy1\n\nthe asymptotic normality of \u2432 propagates to asymptotic normality of\n.x\nlog \u2432r 1 y \u2432 .\n\nw\n\u02c6\nthe asymptotic variance is the variance of the normal distribution that\napproximates the true distribution, for large n. it is not an approximation for\nthe variance of the true distribution. for 0 - \u2432- 1, the asymptotic variance\nw\nn\u2432 1 y \u2432\nof the sample logit is finite. by contrast, the true variance\ndoes not exist: since \u2432s 0 or 1 with positive probability, the logit can equal\ny\u2b01 or \u2b01 with positive probability. the probability of an infinite logit\nconverges to zero rapidly as n increases. for large n, the distribution of the\nsample logit looks essentially normal with mean log \u2432r 1 y \u2432 and standard\ndeviation n\u2432 1 y \u2432\nfor the logit, the asymptotic variance\nactually has greater use than the true variance. incidentally, related to this,\nthe bootstrap is not helpful for approximating standard errors for many\ndiscrete measures, because it mimics the true rather than the more relevant\nasymptotic standard error.\n\n. thus,\n\n.xy1r2\n\n.x\n\n\u02c6\n\n\u017e\n\n\u017e\n\nw\n\nw\n\n3.1.7 delta method for log odds ratio*\nstandard errors for the log odds ratio and the log relative risk result from a\nmultiparameter version of the delta method. suppose that n , i s 1, . . . , c\n4\nhave a multinomial n, \u2432 distribution. the sample proportion \u2432 s n rn\nhas mean and variance\n\n4.\n\n\u02c6\n\n\u017e\n\n\u0004\n\n\u0004\n\ni\n\ni\n\ni\n\ni\n\ne \u2432 s \u2432 and var \u2432 s \u2432 1 y \u2432 rn.\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u02c6\n\ni\n\n\u02c6\n\ni\n\ni\n\ni\n\ni\n\nin section 14.1.4 we show that for i / j, \u2432 and \u2432 have covariance\n\n\u02c6\n\n\u02c6\ncov \u2432 , \u2432 s y\u2432\u2432rn.\n\n\u02c6 \u02c6\ni\n\n\u017e\n\n.\n\ni\n\ni\n\nj\n\nj\n\nj\n\n\u017e\n\n3.7\n\n.\n\n3.8\u017e\n\n.\n\n\u02c6\nthe sample proportions \u2432 , \u2432 , . . . , \u2432\nhave a large-sample multivariate\ncy1\nnormal distribution. for functions of them, the delta method implies the\n\n\u02c6 \u02c6\n1\n\n2\n\n\u017e\n\n.\n\n "}, {"Page_number": 91, "text": "76\n\ninference for contingency tables\n\nfollowing result, proved in section 14.1.4:\n\nlet g \u2432 denote a differentiable function of \u2432 , with sample value g \u2432 for a\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u02c6\n\n\u0004\n\n4\n\ni\n\nmultinomial sample. let\n\n\u243e s\ni\n\n.\n\n\u2b78g \u2432\u017e\n\u2b78\u2432\ni\n\n,\n\ni s 1, . . . , c.\n\nthen as n \u2122 \u2b01, the distribution of n g \u2432 y g \u2432 r\u2434 converges to standard\nnormal, where\n\n' w\n\n.x\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n\u2434 s \u2432 \u243e y\n\n2\n\n\u00fd\n\n2\ni\n\ni\n\n\u00fd\n\n.\n2\n\u2432 \u243e .\n\ni\n\ni\n\n3.9\u017e\n\n.\n\n\u017e\n4\n\n\u0004\n\nthe asymptotic variance depends on \u2432 and the partial derivatives of the\nmeasure with respect to \u2432 . in practice, replacing \u2432 and \u243e in 3.9 by\ntheir sample values yields an ml estimate \u2434 of \u2434 . then \u2434r n is an\nestimated standard error for g \u2432 . a large-sample wald confidence interval\nfor g \u2432 is\n\n4\n\u02c6\n\n'\n\n4\ni\n2\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n\u0004\n\n2\n\ni\n\ni\n\ni\n\ng \u2432 \" z \u2434r n .\n\u017e\n\n\u02c6\u2423r2\n\n.\u02c6\n\n'\n\n\u017e\n\n.\n\n\u02c6\n\nwith the substitution of \u2434 for \u2434 in 3.9 , the limiting distribution is still\nstandard normal, but convergence is slower. the equivalence in the large-\nsample distribution is justified as follows: the sample proportions converge\nin probability to \u2432 , by the weak law of large numbers. since \u2434 is a\ncontinuous function of the sample proportions, it converges in probability to\n\u2434, and \u2434r\u2434 converges in probability to 1. now\n.\n\u02c6\n\ng \u2432 y g \u2432\n\u017e\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\n.\n\n\u0004\n\n4\n\ni\n\n'\nn\n\n\u02c6\n\u2434\n\n'\ns n\n\n\u017e\n\ng \u2432 y g \u2432 \u2434\n\u017e\n\u02c6\n\u2434\n\n\u2434\n\n.\n\n.\n\n\u017e\n\nthe first term on the right-hand side converges in distribution to standard\nnormal, by 3.9 , and the second term converges in probability to 1. thus,\ntheir product also has a limiting standard normal distribution.\nwe now apply the delta method to the log odds ratio, taking g \u2432 s log \u242a\ns log \u2432 q log \u2432 y log \u2432 y log \u2432 . since\n\n\u017e\n\n.\n\n22\n\n12\n\n11\n\n21\n\n11\n\n\u243e s \u2b78 log \u242a r\u2b78\u2432 s 1r\u2432\n\n\u017e\n\u243e s y1r\u2432 ,\n21\n\u00fd \u00fd \u2432 \u243e s 0 and \u2434 s \u00fd \u00fd \u2432 \u243e s \u00fd \u00fd 1r\u2432 . the asymptotic stan-\n\u017e\n\u0004\ndard error of log \u242a for a multinomial sample n\n\n\u243e s 1r\u2432 ,\n.\n\n\u243e s y1r\u2432 ,\n\ni j\nis\n\n\u02c6\n\n2\ni j\n\n11\n\n11\n\n12\n\n12\n\n21\n\n22\n\n22\n\n.\n\n4\n\ni j\n\ni j\n\ni j\n\n2\n\ni\n\ni\n\ni\n\nj\n\nj\n\nj\n\ni j\n\n/\nsince n\u2432 s n , the estimated standard error is 3.1 .\n.\n\n\u2434 log \u242a s \u2434r n s\n\u017e\n\n1rn\u2432\n\n\u00fd \u00fd\n\n'\n\n\u017e\n\n\u02c6\n\n.\n\n\u017e\n\ni j\n\ni\n\nj\n\n\u02c6i j\n\ni j\n\n1r2\n\n.\n\n "}, {"Page_number": 92, "text": "confidence intervals for association parameters\n\n77\n\nthe delta method also applies directly with \u242a to obtain \u2434 \u242a and a wald\nconfidence interval \u242a\" z \u2434 \u242a . this is not recommended; \u242a converges\nmore slowly than log \u242a to normality, this interval could contain negative\nvalues, and it does not give results equivalent to those obtained with the\nwald interval using 1r\u242a and its standard error.\n\n\u02c6\u2423r2\n\n\u02c6\n.\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n\u02c6\u017e\n.\n\n\u02c6\n\n3.1.8 score and profile likelihood confidence intervals*\nstandard errors obtained with the delta method appear in wald confidence\nintervals. however, intervals based on inverting wald tests sometimes work\npoorly for small to moderate n. alternative intervals result from inverting\nlikelihood-ratio or score tests. although computationally more complex,\nthese methods often perform better.\nwe illustrate first with the score method for the difference of proportions.\nthe score test mee 1984; miettinen and nurminen 1985 of h : \u2432 y \u2432 s \u232c\nhas the test statistic\n\n\u017e\n\n.\n\n0\n\n1\n\n2\n\nz \u232c s\u017e\n\n.\n\n' \u017e\n\u2432 \u232c 1 y \u2432 \u232c rn q \u2432 \u232c 1 y \u2432 \u232c rn\n\u02c6\n\n\u02c6\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n1\n\n1\n\n2\n\n1\n\n\u02c6\n\n\u2432 y \u2432 y \u232c\n\u017e\n\u02c6\n.\n\n.\n\u02c6\n\n\u017e\n\n2\n\n1\n\n2\n\n2\n\n\u017e\n\n.\n\n\u02c6i\n2\n\n<\n\ni\n\n2\n\n1\n\n1\n\n2\n\n1\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u02c6\n\n\u017e\n1\n\nwhere \u2432 \u232c denotes the ml estimate of \u2432 subject to the constraint\n\u2432 y \u2432 s \u232c. that is, \u2432 \u232c and \u2432 \u232c are the values of \u2432 and \u2432\n2\nsatisfying \u2432 y \u2432 s \u232c that maximize the product of the two binomial\nprobability mass functions. these values do not have closed-form expressions\nand are determined using numerical methods. the score confidence interval\n. <\nis the set of \u232c such that\n. computations for such intervals\n.\nrequire iteration nurminen 1986 .\n\n\u017e\nz \u232c - z\n\n\u2423r2\n\n\u02c6\n\nfor the relative risk also, slightly better performance results with an\ninterval using the score method bedrick 1987; gart and nam 1988;\nkoopman 1984, miettinen and nurminen 1985; nurminen 1986 . cornfield\n\u017e\n1956 and miettinen and nurminen 1985 showed the score interval for the\nodds ratio. we prefer not to use a continuity or finite-sampling correction\nwith these intervals, as then performance is too conservative. the fact that\nthe score intervals are computationally more complex than wald intervals\nshould not be an impediment to their use in this modern era of computing, as\nthe principle behind them is simple. however, currently they are not avail-\nable in standard software.\n\nfor a confidence interval based on the likelihood-ratio test, we illustrate\nwith the odds ratio. the multinomial likelihood for a 2 = 2 table is a function\n4\nof \u2432 ,\u2432 ,\u2432 . equivalently, it can be expressed in terms of \u242a,\u2432 ,\u2432\n1q q1\nrecall section 2.4.1 . thus, in inverting a likelihood-ratio test of h : \u242as \u242a\n\u017e\n0\nto check whether \u242a belongs in the confidence interval, there are two\nnuisance parameters. their null ml estimates \u2432 \u242a and \u2432 \u242a that\n1q 0\nmaximize the likelihood under the null vary as \u242a does.\n\n\u02c6\nq1\n\n11\n\n12\n\n21\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u0004\n\n\u0004\n\n4\n\n0\n\n0\n\n0\n\n\u02c6\n0\n\n "}, {"Page_number": 93, "text": "78\n\ninference for contingency tables\n\n\u017e\n\n\u02c6\n\n\u017e\n\n.\n\n\u02c6\nq1\n\n\u017e\n\n0\n\n0\n\n0\n\n1q 0\n\nthe profile log-likelihood function is l \u242a , \u2432 \u242a , \u2432 \u242a , viewed as a\nfunction of \u242a . for each \u242a this function gives the maximum of the ordinary\nlog likelihood subject to the constraint \u242as \u242a . evaluated at \u242a s \u242a, this is\nthe maximized log likelihood l \u242a, \u2432 , \u2432 , which occurs at the sample\nproportions \u2432 s n rn and \u2432 s n rn. the profile likelihood confi-\ndence interval for \u242a is the set of \u242a for which\n\u017e\n\ny2 l \u242a , \u2432 \u242a , \u2432 \u242a y l \u242a, \u2432 , \u2432 - \u2439 \u2423 .\n\u017e .\n\n\u02c6\u017e\n\u02c6\nq1\n0\n\n1q q1\n\n1q q1\n\n\u02c6\nq1\n\n1q 0\n\n\u02c6\nq1\n\n1q\n\n1q\n\n0\n.\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n2\n1\n\n/\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n0\n\n0\n\n0\n\n0\n\n..\n\n0\n\n\u017e\n\n.\n\nthis contains all \u242a not rejected in likelihood-ratio tests of nominal size \u2423.\nthe profile likelihood approach is available with some software e.g., for\nsas, see table a.2 in appendix a . a related approach, discussed in section\n6.7.1, uses a conditional\nlikelihood function that eliminates the nuisance\nparameters by conditioning on their sufficient statistics. this is beneficial\nwhen there are many nuisance parameters. an advantage of score and\nlikelihood-based intervals is that unlike the wald, they are not adversely\naffected when the sample relative risk or odds ratio is 0 or \u2b01.\n\nin this section we have discussed interval estimation. significance tests\nnormally refer to a null hypothesis value of 0.0 for the log odds ratio, log\nrelative risk, and difference of proportions. these are special cases of\nindependence applied to 2 = 2 tables. in the next section we present tests of\nindependence for two-way contingency tables.\n\n3.2 testing independence in two-way contingency tables\n\nfor multinomial sampling with probabilities \u2432 in an i = j contingency\ntable, the null hypothesis of statistical independence is h : \u2432 s \u2432 \u2432 for\nall\ni and j. for independent multinomial samples in the i rows, indepen-\ndence corresponds to homogeneity of each outcome probability among the\nrows. our discussion refers to a single multinomial sample, but the same tests\napply with independent multinomial samples.\n\niq qj\n\ni j\n\ni j\n\n0\n\n\u0004\n\n4\n\n3.2.1 pearson and likelihood-ratio chi-squared tests\n\u017e\nin section 1.5.2 we introduced the pearson x statistic 1.15 for tests about\nmultinomial probabilities. a test of h : independence uses x 2 with n in\nplace of n and with \u242e s n\u2432 \u2432 in place of \u242e. here \u242e s e n\n\u017e\nunder\nh . usually, \u2432 and \u2432 are unknown. their ml estimates are the\nsample marginal proportions \u2432 s n rn and \u2432 s n rn, so estimated\nexpected frequencies are \u242e s n\u2432 \u2432 s n n rn . then x equals\n\niq qj\n\niq\n\niq\n\nqj\n\nqj\n\n\u02c6\n\n.\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ni j\n\ni j\n\ni j\n\ni j\n\n2\n\n2\n\n0\n\n0\n\ni\n\ni\n\n\u02c6\n\ni j\n\niq\n\u02c6 \u02c6\niq qj\n\u017e\n\n2x s\n\n\u00fd \u00fd\n\ni\n\nj\n\ni j\n\nn y \u242e\u02c6\n\u242e\u02c6i j\n\n\u02c6\nqj\niq qj\n.\n\n2\n\ni j\n\n.\n\n\u017e\n\n3.10\n\n.\n\n "}, {"Page_number": 94, "text": "testing independence in two-way contingency tables\n\n79\n\n\u017e\n\n.\n\n4\npearson 1900, 1904, 1922 claimed that replacing \u242e by estimates \u242e\u02c6\ni j\nwould not affect the distribution of x 2. since the contingency table has ij\ncategories, he argued that x 2 is asymptotically chi-squared with df s ij y 1.\non the contrary, since \u242e require estimating \u2432 and \u2432 , by section\n1.5.6\n\n\u02c6i j\n\niq\n\nqj\n\n\u0004\n\n4\n\n\u0004\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ni j\n\ndf s ij y 1 y i y 1 y j y 1 s i y 1\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n. \u017e\n\nj y 1 .\n.\n\n4\n\n\u0004\n\n4\n\n\u017e\n\n\u0004\n.\n\nqj\n\niq\n\nthe dimensions of \u2432 and \u2432 reflect the constraints \u00fd \u2432 s \u00fd \u2432 s 1.\nr. a. fisher 1922 corrected pearson\u2019s error see section 16.2 . his article\nintroduced the notion of degrees of freedom. pearson had dealt with an\nindexed family of chi-squared distributions but had not dealt explicitly with\n.\n\u2018\u2018degrees of freedom.\u2019\u2019\n\nthe score test produces the x 2 statistic. the likelihood-ratio test pro-\nduces a different one. for multinomial sampling, the kernel of the likelihood\nis\n\nj qj\n\niq\n\n\u017e\n\n.\n\n\u017e\n\ni\n\n\u0142 \u0142\n\ni\n\nj\n\n\u2432n i j, where all \u2432 g 0 and\n\ni j\n\ni j\n\n\u00fd \u00fd\n\n\u2432 s 1.\n\ni j\n\ni\n\nj\n\nunder h : independence, \u2432 s \u2432 \u2432 s n n rn2. in the general case,\n\u2432 s n rn. the ratio of the likelihoods equals\n\u02c6i j\n\n\u02c6 \u02c6\niq qj\n\niq qj\n\n\u02c6\n\ni j\n\ni j\n\n0\n\n\u2333 s\n\n\u017e\n\n.\n\u0142 \u0142 n n\niq qj\ni\nn \u0142 \u0142 n\nn i j\nn\ni j\n\ni\n\nj\n\nj\n\nn i j\n\n.\n\nthe likelihood-ratio chi-squared statistic is y2 log \u2333. denoted by g2, it\nequals\n\ng2 s y2 log \u2333 s 2\n\n\u00fd \u00fd i j\n\nn log n r\u242e\n\u02c6\n\n\u017e\n\ni j\n\ni j\n\n.\n\n\u017e\n\n3.11\n\n.\n\ni\n\nj\n\n2\n\n2\n\n\u0004\n\n\u0004\n\n\u0004\n\n4\n\ni\n\u0004\n\ni j\n4\n\n\u02c6i j\n\nj\niq\n\niq qj\n\nwhere \u242e s n n rn . the larger the values of g and x , the more\nevidence exists against independence.\n4\nin the general case, the parameter space consists of \u2432 subject to the\nlinear restriction \u00fd \u00fd \u2432 s 1, so the dimension is ij y 1. under h , \u2432\n4\ni j\ni y 1 q j y 1 .\n.\nare determined by \u2432 and \u2432 , so the dimension is\ni y 1 j y 1 . for large samples,\n.\nthe difference in these dimensions equals\ng has a chi-squared null distribution with df s i y 1 j y 1 . so g and\n.\u017e\nx 2 have the same limiting null chi-squared distribution. in fact, they are then\nasymptotically equivalent; x y g converges in probability to zero section\n14.3.4 . the limiting results for multinomial sampling also hold with other\n.\nsampling schemes roy and mitra 1956, watson 1959 .\nthese results apply as n grows, and hence \u242e s n\u2432 grow, for a fixed\nis better\n\nnumber of cells. as they grow, the multinomial distribution for n\n\n.\u017e\n\u017e\n\nqj\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\ni j\n\ni j\n\ni j\n\n2\n\n2\n\n0\n\n2\n\n2\n\ni j\n\n "}, {"Page_number": 95, "text": "80\n\ninference for contingency tables\n\napproximated by a multivariate normal, and x 2 and g2 have more nearly\nchi-squared distributions. the convergence to chi-squared is quicker for x 2\nthan g2. the approximation is usually poor for g2 when nrij - 5. when i\nor j is large, it can be decent for x 2 when some expected frequencies are as\nsmall as 1 but most exceed 5. in section 9.8.4 we provide further guidelines.\nsmall-sample methods section 3.5 are available whenever it is doubtful\nwhether n is sufficiently large.\n\n\u017e\n\n.\n\n3.2.2 education and religious fundamentalism example\ntable 3.2 cross-classifies the degree of fundamentalism of subjects\u2019 religious\nbeliefs by their highest degree of education. the table also contains the\nestimated expected frequencies\ninstance,\n\u242e s n n rn s 424 = 886 r2726 s 137.8. the chi-squared statistics are\n.\n\u02c611\nx s 69.2 and g s 69.8, with df s 3 y 1 3 y 1 s 4. the p-values\n2\nare - 0.0001. these statistics provide extremely strong evidence of an\nassociation.\n\nindependence. for\n\nfor h :\n0\n\n1q q1\n\n.\u017e\n\n\u017e\n\n\u017e\n\n.\n\n2\n\n3.3 following-up chi-squared tests\n\nlike any significance test, chi-squared tests of independence have limited\nusefulness. a small p-value indicates strong evidence of association but\nprovides little information about the nature or strength of the association.\nstatisticians have long warned about dangers of relying solely on results of\nchi-squared tests rather than studying the nature of the association e.g.,\nberkson 1938; cochran 1954 . in this section we discuss ways to follow up the\ntests to learn more about the association.\n\n.\n\n\u017e\n\ntable 3.2 education and religious beliefs\n\nhighest degree\nless than high school\n\nhigh school or junior college\n\nbachelor or graduate\n\ntotal\n\nreligious beliefs\nfundamentalist moderate\n\n1\n\n.\n\n.\n\n.\n.\n\n178\n\u017e\n137.8\n\u017e\n.\n2\n4.5\n570\n\u017e\n539.5\n\u017e\n.\n2.6\n138\n\u017e\n208.7\ny6.8\n\u017e\n886\n\n.\n.\n\n.\n\n.\n\n138\n\u017e\n161.5\ny2.6\n\u017e\n648\n\u017e\n632.1\n\u017e\n.\n1.3\n252\n\u017e\n244.5\n\u017e\n0.7\n1038\n\n.\n\n.\n.\n\nliberal\n108\n\u017e\n124.7\ny1.9\n\u017e\n442\n\u017e\n488.4\ny4.0\n\u017e\n252\n\u017e\n188.9\n\u017e\n6.3\n802\n\n.\n.\n\n.\n\n.\n\ntotal\n424\n\n1660\n\n642\n\n2726\n\nsource: 1996 general social survey, national opinion research center.\n1estimated expected frequencies for testing independence; 2standardized pearson residuals.\n\n "}, {"Page_number": 96, "text": "following-up chi-squared tests\n\n81\n\n3.3.1 pearson and standardized residuals\na cell-by-cell comparison of observed and estimated expected frequencies\nhelps show the nature of the dependence. under h ,\nlarger differences\n0\nn y \u242e tend to occur in cells with larger \u242e . for poisson sampling, for\n\u017e\ninstance, the standard deviation of n and hence n y \u242e is \u242e ; the\nstandard deviation of n y \u242e is less than that of n y \u242e but is propor-\ntional to \u242e . thus, this raw difference is insufficient. the pearson residual,\ndefined for a cell by\n\n' i j\n\n'\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\ni j\n\ni j\n\ni j\n\ni j\n\ni j\n\ni j\n\ni j\n\ni j\n\ni j\n\ni j\n\ni j\n\ne s\n\ni j\n\ni j\n\nn y \u242e\u02c6\n1r2\u242e\u02c6i j\n\ni j\n\n,\n\n\u017e\n\n3.12\n\n.\n\nj\n\ni\n\ni j\n\ni j\n\n\u0004\n\n4\n\ne\n\nattempts to adjust for this. pearson residuals relate to the pearson statistic by\n\u00fd \u00fd e2 s x 2.\nunder h ,\n0\n\nin\nsection 14.3.2 we show that their asymptotic variances are less than 1.0,\ni y 1 j y 1 r number of cells . comparing pearson residuals\naveraging\nto standard normal percentage points provides conservative indications of\ncells having lack of fit.\n\nare asymptotically normal with mean 0. however,\n\n.x\n\nw\u017e\n\n.\u017e\n\na standardized pearson residual that is asymptotically standard normal\nresults from dividing it by its standard error haberman 1973a; see also\nsection 14.3.2 . for h : independence, this is\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n0\n\nn y \u242e\u02c6\n\u242e 1 y p\n\u02c6\niq\n\n\u017e\n\n\u017e\n\n.\n\ni j\n\ni j\n\ni j\n\n1 y p\nqj\n\n.\n\n1r2\n\n.\n\n\u017e\n\n3.13\n\n.\n\na standardized pearson residual that exceeds about 2 or 3 in absolute value\nindicates lack of fit of h in that cell. larger values are more relevant when\ndf is larger and it becomes more likely that at least one is large simply by\nchance.\n\n0\n\n3.3.2 education and religious fundamentalism revisited\ntable 3.2 also shows standardized pearson residuals for testing indepen-\ndence. for instance, n s 178 and \u242e s 137.8. the relevant marginal\nproportions equal p s 424r2726 s 0.156 and p s 886r2726 s 0.325.\nthe standardized pearson residual 3.13 for this cell equals\n\n1q\n\nq1\n\n\u02c6\n\n11\n\n11\n\n\u017e\n\n.\n\n178 y 137.8 r 137.8 1 y 0.156 1 y 0.325\n\n. \u017e\n\n. \u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n1r2\n\ns 4.5.\n\nthis cell shows a much greater discrepancy between n\nexpected if the variables were truly independent.\n\nand \u242e than\n\n\u02c6\n\n11\n\n11\n\ntable 3.2 shows large positive residuals for subjects with less than a high\nschool education and fundamentalist views and for subjects with a bachelor\u2019s\n\n "}, {"Page_number": 97, "text": "82\n\ninference for contingency tables\n\n0\n\nor graduate degree and liberal views. this means that significantly more\nsubjects were at these combinations than h : independence predicts. simi-\nlarly, there were fewer subjects with high levels of education and fundamen-\ntalist views and with low levels of education and liberal views than indepen-\ndence predicts.\n\nodds ratios describe this trend. the 2 = 2 table constructed from the first\nand last rows and the first and last columns of table 3.2 has a sample odds\nratio of 178 = 252 r 108 = 138 s 3.0. for those with a bachelor\u2019s or gradu-\nate degree, the estimated odds of selecting liberal instead of fundamentalist\nwere 3.0 times the estimated odds for those with less than a high school\neducation.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n2\n\n2\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\n1\n\n1\n\n1\n\n\u242f\n\n\u242f\n\n3.3.3 partitioning chi-squared\nlet z denote a standard normal random variable. then z 2 has a chi-squared\ndistribution with df s 1. a chi-squared random variable with df s \u242f has\nrepresentation z 2 q \u2b48\u2b48\u2b48 qz 2, where z , . . . , z are independent standard\nnormal variables. thus, a chi-squared statistic having df s \u242f has partition-\nings into independent chi-squared components\u138ffor example, into \u242f compo-\nnents each having df s 1. conversely,\nif x 2 and x 2 are independent\nchi-squared random variables having degrees of freedom \u242f and \u242f , then\nx 2 s x 2 q x 2 has a chi-squared distribution with df s \u242f q \u242f . another\nsupplement to a chi-squared test partitions its test statistic so that the\ncomponents represent certain aspects of the effects. a partitioning may show\nthat an association reflects primarily differences between certain categories\nor groupings of categories.\nwe begin with a partitioning for the test of independence in 2 = j tables.\nwe partition g , which has df s j y 1 , into j y 1 components. the jth\ncomponent is g2 for a 2 = 2 table where the first column combines columns\n1 through j of the full table and the second column is column j q 1. that is,\ng2 for testing independence in a 2 = j table equals a statistic that compares\nthe first two columns, plus a statistic that combines the first two columns and\ncompares them to the third column, and so on, up to a statistic that combines\nthe first j y 1 columns and compares them to the last column. in section\n9.2.4 we justify this partitioning. each component statistic has df s 1.\nit might seem more natural to compute g for the j y 1 separate 2 = 2\ntables that pair each column with a particular one, say the last. however,\nthese component statistics are not independent and do not sum to g2 for the\nfull table. this is beyond our scope at this stage but relates to the contrasts\nof log probabilities that form the log odds ratios for the two tables not being\northogonal.\n\nfor an i = j table,\n\nindependent chi-squared components result from\ncomparing columns 1 and 2 and then combining them and comparing them to\ncolumn 3, and so on. each of the j y 1 statistics has df s i y 1. more\nrefined partitions contain i y 1 j y 1 statistics, each having df s 1. one\n\n.\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n2\n\n "}, {"Page_number": 98, "text": "following-up chi-squared tests\n83\nsuch partition lancaster 1949 applies to the i y 1 j y 1 separate 2 = 2\ntables\n\n.\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u00fd \u00fd\na-i b-j\n\nn\n\n\u00fdab\n\na-i\n\nn\n\na j\n\n\u00fd ib\nn\n\nb-j\n\nn\n\ni j\n\n\u017e\n\n3.14\n\n.\n\nfor i s 2, . . . , i and j s 2, . . . , j. for others, see gilula and haberman\n\u017e\n.\n1998 and goodman 1969a, 1971b .\n\n\u017e\n\n.\n\n3.3.4 origin of schizophrenia example\ntable 3.3 classifies a sample of psychiatrists by their school of psychiatric\nthought and by their opinion on the origin of schizophrenia. here g2 s 23.04\nwith df s 4. to understand this association better, we partition g2 into four\nindependent components. the partitioning 3.14 applies to the subtables\nshown in table 3.4.\n\nthe first subtable compares the eclectic and medical schools of psychiatric\nthought on whether the origin of schizophrenia is biogenic or environmental\ngiven that the classification was in one of these two categories. for this\nsubtable, g2 s 0.29, with df s 1. the second subtable compares these two\nschools on the proportion of times the origin was ascribed to be a combina-\ntion, rather than biogenic or environmental. this subtable has g2 s 1.36,\n\n\u017e\n\n.\n\ntable 3.3 most influential school of psychiatric thought and ascribed\norigin of schizophrenia\n\nschool of\npsychiatric thought\neclectic\nmedical\npsychoanalytic\n\norigin of schizophrenia\n\nbiogenic\n\nenvironmental\n\ncombination\n\n90\n13\n19\n\n12\n1\n13\n\n78\n6\n50\n\nsource: reprinted with permission, based on data from b. j. gallagher iii, b. j. jones, and l. p.\n.\nbarakat, j. clin. psychol. 43: 438\u1390443 1987 .\n\n\u017e\n\ntable 3.4 subtables used in partitioning chi-squared for table 3.3 a\n\nbio q\nenv com\n102\n14\n\nbio env\necl\n90\nmed 13\nabio, biogenic; com, combination; ecl, eclectic; env, environmental; psy, psychoanalytic\n\n78 ecl q med 103\n6 psy\n19\n\n13 ecl q med\n13 psy\n\n12 ecl\n1 med\n\nbio env\n\nbio q\nenv com\n84\n116\n32\n50\n\n "}, {"Page_number": 99, "text": "84\nwith df s 1. the sum of these two components equals g2\nfor testing\nindependence with the first two rows of table 3.3. there is little evidence of\na difference between the eclectic and medical schools of thought on the\nascribed origin of schizophrenia.\n\ninference for contingency tables\n\n.\n\n\u017e\n\n.\n\nnext we combine the eclectic and medical schools and compare them to\nthe psychoanalytic school. the third subtable in table 3.4 compares them for\nthe biogenic, environmental classification, giving g s 12.95 with df s 1.\n\u017e\nthe fourth subtable compares them for the biogenic or environmental,\ncombination split, giving g s 8.43 with df s 1.\n\nthe psychoanalytic school seems more likely than the other schools to\nascribe the origins of schizophrenia as being a combination. of those who\nchose either the biogenetic or environmental origin, members of the psycho-\nanalytic school were somewhat more likely than the other schools to choose\nthe environmental origin. the sum of these four g2 components equals the\nvalue of 23.04 for testing independence in the full table.\n\n2\n\n2\n\n3.3.5 rules for partitioning\ngoodman 1968, 1969a, 1971b and lancaster 1949, 1969 gave rules for\ndetermining independent components of chi-squared. for forming subtables,\namong the necessary conditions are the following:\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n1. the df for the subtables must sum to df for the full table.\n2. each cell count in the full table must be a cell count in one and only\n\none subtable.\n\n3. each marginal total of the full table must be a marginal total for one\n\nand only one subtable.\n\nfor a certain partitioning, when the subtable df values sum properly but the\ng2 values do not, the components are not independent.\n\nfor the g2 statistic, exact partitionings occur. the pearson x 2 need not\nequal the sum of the x 2 values for the subtables. it is valid to use the x 2\nstatistics for the separate subtables; they simply need not provide an exact\nalgebraic partitioning of x 2 for the full table. when the null hypotheses all\nhold, x 2 does have an asymptotic equivalence with g2, however. in addition,\nwhen the table has small counts, in large-sample chi-squared tests it is safer\nto use x 2 to study the subtables.\n\n3.3.6 limitations of chi-squared tests\nchi-squared tests of independence merely indicate the degree of evidence of\nassociation. they are rarely adequate for answering all questions about a\ndata set. rather than relying solely on results of these tests, investigate the\nnature of the association: study residuals, decompose chi-squared into com-\nponents, and estimate parameters such as odds ratios that describe the\nstrength of association.\n\n "}, {"Page_number": 100, "text": "following-up chi-squared tests\n\n85\n\n2\n\n4\n\nthey require large samples. also,\n2\n\nthe chi-squared tests also have limitations in the types of data to which\nthe \u242e s\u02c6i j\nthey apply. for instance,\nn n rn used in x and g depend on the marginal totals but not on the\niq qj\norder of listing the rows and columns. thus, x 2 and g2 do not change value\nwith arbitrary reorderings of rows or of columns. this implies that they treat\nboth classifications as nominal. when at least one variable is ordinal, test\nstatistics that utilize the ordinality are usually more appropriate. we present\nsuch tests in section 3.4.\n\n\u0004\n\n3.3.7 why consider independence?\nany idealized structure such as independence is unlikely to hold in any given\npractical situation. with large samples such as in table 3.2 it is not surprising\nto obtain a small p-value. given this and the limitations just mentioned, why\neven bother to consider independence as a possible representation for a joint\ndistribution? one reason refers to the benefits of model parsimony. if the\nindependence model approximates the true probabilities well, then unless n\nis very large, the model-based estimates \u2432 s n n rn\nof cell probabili-\nties tend to be better than the sample proportions p s n rn . the inde-\ni j\npendence ml estimates smooth the sample counts, somewhat damping the\nrandom sampling fluctuations.\n\u017e\n\nthe mean-squared error mse formula\n\niq qj\u0004\n\n\u02c6i j\n\n24\n\n.\n\n4\n\n\u0004\n\ni j\n\nmse s variance q bias\n\n\u017e\n\n.\n\n2\n\ni j\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u017e\u0004\n\n4.\n\nqj\n\nqj\n\niq\n\niq\n\nexplains why the independence estimators can have smaller mse. although\nthey may be biased, they have smaller variance because they are based on\nestimating fewer parameters \u2432 and \u2432 instead of \u2432 . hence, mse\ncan be smaller unless n is so large that the bias term dominates the variance.\n.x\nwe illustrate using table 3.5, which has \u2432 s \u2432 \u2432 1 q \u2426 i y 2 j y 2\nfor \u2432 s \u2432 s . here y1 - \u2426- 1, with \u2426s 0 equivalent to indepen-\ndence. independence approximates the relationship well when \u2426 is close to\nzero. the total mse values of the two estimators are\n\u00fd \u00fd\n\u017e\n\n\u2432 1 y \u2432 rn s 1 y\n\ne p y \u2432 s\n\u017e\n\nmse p s\n\n. \u00fd \u00fd\n\n\u00fd \u00fd\n\n\u00fd \u00fd\n\nvar p\ni j\n\niq qj\n\n\u24322\ni j\n\ns\n\n/\n\n.\u017e\n\nn\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n1\n3\n\n4\n\n\u0004\n\n\u017e\n\nw\n\ni j\n\ni j\n\ni j\n\ni j\n\ni j\n\ni j\n\n2\n\ni\n\ni\n\nj\n\nj\n\ni\n\nj\n\nmse \u2432 s\n\n4\u02c6\n\n. \u00fd \u00fd\n\n\u017e\n\n\u0004\n\ni j\n\ne \u2432 y \u2432 .\n\u02c6\u017e\n.\n\n2\n\ni j\n\ni j\n\ni\n\ni\n\nj\n\nj\n\ntable 3.5 cell probabilities for comparison of estimators\n1 q \u2426 r9\n.\n\u017e\n1r9\n1 y \u2426 r9\n\u017e\n.\n\n1 y \u2426 r9\n.\n\u017e\n1r9\n1 q \u2426 r9\n\u017e\n.\n\n1r9\n1r9\n1r9\n\n "}, {"Page_number": 101, "text": "86\n\ninference for contingency tables\n\ntable 3.6 comparison of total mse =10,000 for sample proportion\nand independence estimators\n\n(\n\n)\n\n\u2426s 0\n\nn\n10\n50\n100\n500\n\u2b01\n\np\n889\n178\n89\n18\n0\n\n\u02c6\n\u2432\n489\n91\n45\n9\n0\n\n\u2426s 0.1\n\u02c6\n\u2432\np\n493\n888\n95\n178\n50\n89\n14\n18\n0\n5\n\n\u2426s 0.2\n\u02c6\n\u2432\np\n505\n887\n110\n177\n65\n89\n28\n18\n0\n20\n\n\u2426s 0.6\n\u02c6\n\u2432\np\n634\n871\n261\n174\n220\n87\n186\n17\n0\n178\n\n\u2426s 1.0\n\u02c6\n\u2432\np\n893\n840\n565\n168\n529\n84\n500\n17\n0\n494\n\nfor table 3.5,\n\nmse p s\n\n\u0004\n\n4\n\n\u017e\n\n.\n\ni j\n\n\u00bd\n\n1\n8\nn 9\n\ny\n\n4\u24262\n81\n\n5\n\nand rather tedious calculations yield\n\nmse \u2432 s\n\n4\u02c6\u017e\n\u0004\n\n.\n\ni j\n\n\u00bd\n\n1\n4\nn 9\n\nq\n\n5\n\n4\n9n\n\nq\n\n4\u24262\n81\n\n\u00bd\n\n1 y q y\n\n2\nn\n\n2\n2\nn\n\n5\n\n.\n\n2\n3\nn\n\ni j\n\ni j\n\n\u017e\u0004\n\n\u017e\u0004\n\n\u02c6\n\n4.\n\n4.\n\ntable 3.6 lists the total mse values for various \u2426 and n. when \u2426s 0,\nmse p s 8r9n, whereas mse \u2432 f 4r9n for large n. the indepen-\ndence estimator is then much better than the sample proportions. when the\ntable is close to independence \u2426f 0 and n is not large, mse is only about\nhalf as large for the independence estimator. when \u2426/ 0, the inconsistency\nof \u2432 is reflected by mse \u2432 \u2122 4\u2426 r81 whereas mse p \u2122 0 as\nn \u2122 \u2b01. when the table is close to independence, however, the independence\nestimator has a lower total mse even for moderately large n e.g., for\nn s 500 when \u2426s 0.1 .\n.\n\n4.\n\n4.\n\n\u02c6\n\n\u02c6\n\n\u017e\u0004\n\n\u017e\u0004\n\n\u017e\n\n.\n\n\u017e\n\nw\n\nx\n\n\u0004\n\n4\n\ni j\n\ni j\n\ni j\n\n2\n\n3.4 two-way tables with ordered classifications\n\nthe x 2 and g2 chi-squared tests ignore some information when used to test\nindependence between ordinal classifications. when rows andror columns\nare ordered, more powerful tests usually exist.\n\n3.4.1 linear trend alternative to independence\nwhen the row variable x and the column variable y are ordinal, a positive\nor negative trend in the association is common. one approach to inference,\ndescribed later in this section, uses an ordinal measure of monotone trend.\n\n "}, {"Page_number": 102, "text": "two-way tables with ordered classifications\n\n87\n\ni\n\nj\n\n2\n\n1\n\n2\n\n1\n\na more popular analysis assigns scores to categories and summarizes the\nlinear trend.\na test statistic that is sensitive to positive or negative linear trends utilizes\ncorrelation information. let u f u f \u2b48\u2b48\u2b48 f u denote scores for the rows,\nand let \u00ae f \u00ae f \u2b48\u2b48\u2b48 f \u00ae denote column scores. the scores have the same\nordering as the categories. they assign distances between categories and\nactually treat the measurement scale as interval, with greater distances\nbetween categories that are farther apart.\nthe sum \u00fd \u00fd u \u00ae n weights cross-products of scores by their frequency.\nthe\nit relates to the covariation of x and y. for the scores chosen,\ncorrelation r between x and y equals the standardization of this sum to\nthe y1 to q1 scale in fact, r equals this sum when both sets of scores are\nlinearly transformed for the n subjects to have a mean of 0 and standard\ndeviation of 1 . the larger the correlation is in absolute value, the farther the\ndata fall from independence in this linear dimension.\n\n\u017e\n\n.\n\ni j\n\ni\n\ni\n\nj\n\nj\n\na statistic for testing independence against the two-sided alternative of\n\nnonzero true correlation is\n\nm 2 s n y 1 r 2 .\n\n\u017e\n\n.\n\n\u017e\n\n3.15\n\n.\n\n<\n\u017e\n\n<\nr or n do. for large samples, it is approximately\nthis statistic increases as\nchi-squared with df s 1 mantel 1963 . large values contradict indepen-\ndence, so as with x 2 and g2, the p-value is the right-tailed probability above\nthe value observed. a small p-value does not imply that the association is\nlinear, merely that searching for a linear component to the association helped\nto build power against h . the test treats the variables symmetrically.\n\n.\n\n0\n\n.\n\n\u017e\n\n.\n\njob satisfaction example revisited\n\n3.4.2\ntable 2.8 showed job satisfaction and income for 96 subjects. the ordinary\nchi-squared statistics for testing independence are x 2 s 6.0 and g2 s 6.8\nwith df s 9 p-values s 0.74 and 0.66 . these statistics show little evidence\nof association, but they ignore the ordering of rows and columns. with scores\n\u017e\n1, 2, 3, 4 for job satisfaction and scores 7.5, 20, 32.5, 60 for income that\napproximate midpoints of categories in thousands of dollars, the correlation\nis r s 0.200. the linear trend test statistic m s 96 y 1 0.200 s 3.81.\nthis shows some evidence of association p s 0.051 . the evidence is stronger\nfor the one-sided positive trend alternative, using m s n y 1 r s 1.95\np s 0.026 .\n.\n\u017e\nthe nontrivial evidence of positive association may be surprising, since x 2\nand g2 have such unimpressive values. when a positive or negative trend\nexists, analyses designed to detect that trend can provide much smaller\np-values than analyses that ignore it.\n\n.\u017e\n'\n\n.2\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n2\n\n "}, {"Page_number": 103, "text": "88\n\ninference for contingency tables\n\n.\n\n\u017e\n\n3.4.3 monotone trend alternatives to independence\nordinal variables do not have a specified metric. detecting a linear trend\nalternative to independence requires assigning scores to x and y, treating\nthem as interval variables. alternatively, a strict ordinal analysis with the\nweaker alternative of monotonicity uses an ordinal measure of association,\n.\nsuch as gamma section 2.4.4 .\n\n\u02c6\n\nfor large random samples, sample gamma has approximately a normal\nsampling distribution. the standard error se follows from the delta method\n\u017e\nproblem 3.27 . gamma is the basis of an ordinal test of independence using\ntest statistic z s \u2425rse. a confidence interval describes the strength of\npositive or negative monotone association.\nfor table 2.8 on income and job satisfaction, in section 2.4.5 we showed\nthat \u2425s 0.221. the sample has a weak tendency for job satisfaction to be\nhigher at higher income levels. software e.g., proc freq in sas reports\na standard error of 0.117 for gamma. there is some evidence that \u2425) 0,\nsince z s 0.221r0.117 s 1.89 p s 0.03 for the one-sided alternative . an\napproximate 95% confidence interval for \u2425 is 0.221 \" 1.96 0.117 , or y0.01,\n0.45 . the true association between income and job satisfaction is at best\nmoderately positive.\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nw\n\n\u017e\n\n.\n\n\u017e\n\n3.4.4 extra power with ordinal tests\nfor testing independence, x 2 and g2 refer to the most general alternative,\nwhereby cell probabilities exhibit any type of statistical dependence. their\ndf value of\nthat has\ni y 1 j y 1 more parameters than the null hypothesis\u138fthe nonredundant\n\u017e\nodds ratios that describe the association such as 2.10 . these statistics are\ndesigned to detect any pattern for these parameters. in achieving this\ngenerality, they sacrifice sensitivity for detecting particular patterns.\n\nreflects an alternative hypothesis\n\ni y 1 j y 1\n.\n\n.x\n\n.\u017e\n\n.\u017e\n\n\u02c6\n\n.2\n\nby contrast, the analyses for ordinal row and column variables attempt to\ndescribe association using a single parameter. for instance, m 2 uses the\ncorrelation. when a chi-squared test statistic refers to a single parameter\nw\nsuch as m or \u2425rse do , it has df s 1. when the association truly has a\npositive or negative trend, an ordinal test has a power advantage over the\ntests using x 2 or g2. since df equals the mean of the chi-squared distribu-\ntion, a relatively large m 2 value with df s 1 falls farther out in its right-hand\ntail than a comparable value of x or g with df s i y 1 j y 1 ; falling\nfarther out in the tail produces a smaller p-value. the potential discrepancy\nin power increases as i and j increase. in section 6.4 we present the theory\nbehind such a power comparison.\n\n.\u017e\n\n\u017e\n\n\u017e\n\n.\n\nx\n\n2\n\n2\n\n2\n\n3.4.5 choice of scores\noften, it is unclear how to assign scores to statistics that require them, such\nas m . cochran 1954 noted that \u2018\u2018any set of scores gives a \u00aealid test,\n\n.\n\n\u017e\n\n2\n\n "}, {"Page_number": 104, "text": "two-way tables with ordered classifications\n\n89\n\nprovided that they are constructed without consulting the results of the\nexperiment. if the set of scores is poor, in that it badly distorts a numerical\nscale that really does underlie the ordered classification, the test will not be\nsensitive. the scores should therefore embody the best insight available\nabout the way in which the classification was constructed and used.\u2019\u2019 ideally,\nthe scale is chosen by a consensus of experts, and subsequent interpretations\nuse that same scale.\n\nhow sensitive are analyses to the choice or scores? there is no simple\nanswer, but different scoring systems can give quite different results e.g.,\ngraubard and korn 1987 . for most data sets, different choices of monotone\nscores give similar results. scores that are linear transforms of each other,\nsuch as 1, 2, 3, 4 and 0, 2, 4, 6 , have the same absolute correlation and\nhence the same m 2. results may depend on the scores, however, when the\ndata are highly unbalanced, with some categories having many more observa-\ntions than others.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\ntable 3.7 illustrates the potential dependence. it refers to a prospective\nstudy of maternal drinking and congenital malformations. after the first\nthree months of pregnancy, the women in the sample completed a question-\nnaire about alcohol consumption. following childbirth, observations were\nrecorded on the presence or absence of congenital sex organ malformations.\nwhen a variable is nominal but has only two categories, statistics that treat it\nas ordinal are still valid. for instance, we can artificially regard malformation\nas ordinal, treating \u2018\u2018present\u2019\u2019 as \u2018\u2018high\u2019\u2019 and \u2018\u2018absent\u2019\u2019 as \u2018\u2018low.\u2019\u2019 with only two\nrows, any set of distinct row scores is a linear transformation of any other set\nand gives the same m 2 value. alcohol consumption, measured as the average\nnumber of drinks per day, is an ordinal explanatory variable. this groups a\nnaturally continuous variable, and we first use the scores \u00ae s 0, \u00ae s 0.5,\n\u00ae s 1.5, \u00ae s 4.0, \u00ae s 7.0 , the last score being somewhat arbitrary. for this\nchoice, m 2 s 6.57, for which the p-value is 0.010. by\ncontrast, for the\nequally spaced row scores 1, 2, 3, 4, 5 , m s 1.83, giving a much weaker\nconclusion p s 0.18 .\n.\n\nan alternative approach uses the data to form the scores automatically, by\nusing ranks as the category scores. all subjects in a category receive the\naverage of the ranks that would apply for a complete ranking of the sample\nfrom 1 to n. these are called midranks. the 17,114 subjects at level 0 for\n\n\u017e\n\n.\n\n\u017e\n\n\u0004\n\n4\n\n1\n\n2\n\n3\n\n4\n\n5\n\n2\n\ntable 3.7 example for which results depend on choice of scores\n\nalcohol consumption\n\ng 6\n37\n1\n.\nsource: reprinted with permission from the biometric society graubard and korn 1987 .\n\n\u017e\naverage number of drinks per day\n3\u13905\n126\n1\n\nmalformation\nabsent\npresent\n\n- 1\n14,464\n38\n\n1\u13902\n788\n5\n\n17,066\n48\n\n0\n\n.\n\n\u017e\n\n "}, {"Page_number": 105, "text": "90\n\ninference for contingency tables\n\n.\n\n\u017e\n\nalcohol consumption share ranks 1 through 17,114. each receives the average\nof these ranks, which is the midrank 1 q 17,114 r2 s 8557.5. similarly,\nthe midranks for the last\nfour categories are 24,365.5, 32,013, 32,473,\nand 32,555.5. these scores yield m 2 s 0.35 and a weaker conclusion yet\np s 0.55 .\n.\n\u017e\nwhy does this happen? adjacent categories having relatively few observa-\ntions necessarily have similar midranks. the midranks are similar for the\nfinal three categories, since those categories have few observations compared\nwith the first two categories. this scoring scheme treats alcohol consumption\nlevel 1\u13902 drinks category 3 as much closer to consumption level g 6 drinks\n\u017e\ncategory 5 than to consumption level 0 drinks category 1 . this seems\ninappropriate. it is usually better to select scores that reflect distances\nbetween categories. when uncertain about this choice, a sensitivity analysis\nshould be performed, selecting two or three sensible choices and checking\nwhether results are similar. equally spaced scores often provide a reasonable\ncompromise when the category labels do not suggest obvious choices, such as\nthe categories liberal, moderate, conservative for political philosophy.\n\nwhen x and y are both ordinal and m 2 uses midrank scores, the\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\ncorrelation on which m 2 is based is called spearman\u2019s rho.\n\nj\n\nj\n\nj\n\nj\n\ni\n\ni\n\n2\n\n1\n\n\u0004\n\n4\n\n2 j\n\n3.4.6 trend tests for i = 2 and 2 = j tables\nwhen i or j equal 2, the tests based on linear or monotonic trend simplify to\nwell-established procedures. with binary x, 2 = j tables occur in compar-\nisons of two groups, such as when the rows represent two treatments. using\nscores u s 0, u s 1 for levels of x, the covariation measure \u00fd \u00fd u \u00ae n\ni j\nin m 2 simplifies to \u00fd \u00ae n . this term sums the scores on y for all subjects\nin row 2. divided by the number of subjects in row 2, it gives the mean score\nfor that row. in fact, m 2\nis then directed toward detecting differences\nbetween the two row means of the scores on y.\n\nwith midrank scores for y, the test using m 2 for 2 = j tables is sensitive\nto differences in mean ranks for the two rows. this test is called the wilcoxon\nor mann\u1390whitney test. most nonparametric statistics textbooks present this\ntest for fully ranked response data, whereas the 2 = j table is an extended\ncase in which sets of subjects in the same category of y are tied and use\nmidranks. the large-sample version of that nonparametric test uses a stan-\ndard normal z statistic. the square of the statistic is equivalent to m 2, using\narbitrary row scores and midranks for the columns. it is also asymptotically\nequivalent to test statistics based on the numbers of concordant and discor-\ndant pairs, such as the one using gamma.\n\nwhen y has two levels, the table has size i = 2. the linear trend statistic\nthen refers to a linear trend in the probability of either response category,\nsuch as the probability of malformation as a function of alcohol consumption.\nthe test\nis\npresented in section 5.3.5.\n\nin that case, often called the cochran\u1390armitage trend test,\n\n "}, {"Page_number": 106, "text": "small-sample tests of independence\n\n91\n\n3.4.7 nominal\u2013ordinal tables\nthe tests using the correlation or gamma are appropriate when both classifi-\ncations are ordinal. when one is nominal with more than two categories,\nother statistics are needed. one is based on summarizing the variation among\nmeans on the ordinal variable in the various categories of the nominal\nvariable. we defer discussion of this case to section 7.5.3, note 3.6, and\nproblem 3.28.\n\n3.5 small-sample tests of independence\n\nthe inferential methods of the preceding four sections are large-sample\nmethods. when n is small, alternative methods use exact small-sample\ndistributions rather than large-sample approximations. in this section we\ndescribe small-sample tests of independence, starting with one that r. a.\nfisher proposed for 2 = 2 tables.\n\n3.5.1 fisher\u2019s exact test for 2 = 2 tables\nin section 3.5.7 we show that a distribution not depending on unknown\nparameters results from conditioning on the marginal totals of the contin-\ngency table. these are usually not naturally fixed. for poisson sampling\nnothing is fixed, for multinomial sampling only n is fixed, and for indepen-\ndent binomial sampling in the two rows only the row marginal totals are\nfixed. in any of these cases, under h : independence, conditioning on both\nsets of marginal totals yields the hypergeometric distribution\n\n0\n\np t s p n s t s\n\u017e .\n\n\u017e\n\n.\n\n11\n\nn\n\n1q\u017e\n\nt\n\n/\n\n.\n\n\u017e\n\n3.16\n\n.\n\n2q\n\n/\u017e\nn\u017e\n/q1\n\nn\nn y t\nq1\nn\n\n\u0004\n\n4\n\ni j\n\n\u017e\n\n11\n\n11\n\n11\n\n11\n\nq\n\nq\n\ny\n\n.\n.\n\n1q q1\n\nin terms of only n . given\nthis formula expresses the distribution of n\nthe marginal totals, n determines the other three cell counts. the range of\npossible values for n is m f n f m , where m s max 0, n q n y n\n.\nand m s min n , n\nfor 2 = 2 tables, independence is equivalent to the odds ratio \u242as 1. to\ntest h : \u242as 1, the p-value is the sum of certain hypergeometric probabili-\nties. to illustrate, consider h : \u242a) 1. for the given marginal totals, tables\nhaving larger n have larger sample odds ratios and hence stronger evidence\nin favor of h . thus, the p-value equals p n g t\n.\n, where t denotes the\nobserved value of n . this test for 2 = 2 tables is called fisher\u2019s exact test\n\u017e\n.\nfisher 1934, 1935a,c; irwin 1935; yates 1934 .\n\n1q\n\nq1\n\ny\n\n11\n\n11\n\n11\n\n\u017e\n\n\u017e\n\no\n\no\n\na\n\na\n\n0\n\n "}, {"Page_number": 107, "text": "92\n\ninference for contingency tables\n\n\u017e\n\n.\n\n3.5.2 fisher\u2019s tea drinker\nr. a. fisher 1935a described the following experiment from his days at\nrothamsted experiment station, an agriculture research lab north of lon-\ndon. muriel bristol, a colleague of fisher\u2019s, claimed that when drinking tea\n\u017e\nshe could distinguish whether milk or tea was added to the cup first\nshe\npreferred milk first . to test her claim, fisher asked her to taste eight cups of\ntea, four of which had milk added first and four of which had tea added first.\nshe knew there were four cups of each type and had to predict which four\nhad the milk added first. the order of presenting the cups to her was\nrandomized.\n\n.\n\n0\n\ntable 3.8 shows a possible result. distinguishing the order of pouring\nbetter than with pure guessing corresponds to \u242a) 1, reflecting a positive\nassociation between order of pouring and the prediction. we conduct fisher\u2019s\nexact test of h : \u242as 1 against h : \u242a) 1.\n\nthe experimental design fixed both marginal distributions, since dr.\nbristol had to predict which four cups had milk added first. thus, the\nhypergeometric applies naturally for the null distribution of n . the p-value\nfor fisher\u2019s exact test is the null probability of table 3.8 and of tables having\neven more evidence in favor of her claim. the observed table, t s 3 correct\nchoices of the cups having milk added first, has null probability\n\n11\n\no\n\na\n\n\u017e\n\n4\n3\n\n/\u017e\n/\n4\n1 s 0.229.\n8\u017e\n/4\n\nthe only table that is more extreme in the direction of h has n s 4\ncorrect. it has a probability of 0.014. the p-value is p n g 3 s 0.243. this\nresult does not establish an association between the actual order of pouring\nand her predictions. it is difficult to do so with such a small sample.\n.\naccording to fisher\u2019s daughter box 1978, p. 134 ,\nin reality bristol did\nconvince fisher of her ability.\n\na\n.\n\n11\n\n11\n\n\u017e\n\n\u017e\n\ntable 3.8 fisher\u2019s tea tasting experiment\n\npoured first\nmilk\ntea\n\ntotal\n\nguess poured first\ntea\nmilk\n1\n3\n4\n\n3\n1\n4\n\ntotal\n\n4\n4\n\n.\nsource: based on experiment described by fisher 1935a .\n\n\u017e\n\n "}, {"Page_number": 108, "text": "small-sample tests of independence\n\n93\n\n.\n\n11\n\n3.5.3 two-sided p-values for fisher\u2019s exact test\nfor the one-sided alternative, the same p-value results using tables ordered\naccording to larger n , larger odds ratio, or larger difference of proportions\n\u017e\ndavis 1986a . for the two-sided alternative, different criteria can have\ndifferent p-values.\ncounts t such that p t f p t\n\u017e\n\u017e .\nfor the observed value t . another possibility sums p t\nfarther from h ; that is,\n\nfor a two-sided p-value, a popular approach sums p n s t\nw\n\n.\n; that is, the p-value is p s p p n f p t\n.\n\u017e\n\nin 3.16 for\n.x\n11\nfor tables that are\n\n\u017e .\n\n\u017e\n.\n\n.\n\u017e\n\n11\n\n\u017e\n\no\n\no\n\n0\n\n0\n\np s p n y e n g t y e n\n\u017e\n\n\u017e\n\n.\n\n11\n\n11\n\n0\n\n.\n\n11\n\n,\n\n0\n\n2\n\no\n\no\n\no\n\no\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n11\n\n11\n\n11\n\n11\n\n2.\no\n\nwhere the hypergeometric e n s n n rn. this is identical to p x g\nfor observed pearson statistic x . a third approach takes p s\nx\n.x\nw\n2 min p n g t\n, but this can exceed 1. a fourth approach\nw\ntakes p s min p n g t\n\u017e\nplus an attainable probability in the\n11\nother tail that is as close as possible to, but not greater than, that one-tailed\nprobability.\n\n, p n f t\n.\n\u017e\n\n, p n f t\n.\n\n1q q1\n\n.x\n\n2\no\n\neach approach has advantages and disadvantages blaker 2000; davis\n1986a; dupont 1986; lloyd 1988b; mantel 1987b; yates and discussants\n1984 . they can provide different results because of the discreteness and\npotential skewness. the approach of ordering tables by a distance measure\nfrom h , such as x 2, extends naturally to i = j tables.\n\nin practice, two-sided tests are much more common than one-sided. partly\nthis is so that researchers can avoid charges of bias in giving evidence that\nsupports their predicted direction for an effect. to conduct a test of size 0.05\nwhen one truly believes that the effect has a particular direction, it is safest\nto conduct the one-sided test at the 0.025 level to guard against criticism. for\ninstance, in the 1998 document biostatistical principles for clinical trials, the\ninternational conference on harmonization ich e9 stated: \u2018\u2018the approach\nof setting type i errors for one-sided tests at half the conventional type i\nerror used in two-sided tests is preferable in regulatory settings. this pro-\nmotes consistency with two-sided confidence intervals that are generally\nappropriate for estimating the possible size of the difference between two\ntreatments.\u2019\u2019\n\n\u017e\n\n.\n\n3.5.4 discreteness and conservatism issues\nthe hypergeometric distribution 3.16 is highly discrete for small samples, as\nn and hence the p-value can assume relatively few values. it is usually not\n11\npossible to achieve a fixed significance level size such as 0.05.\n\nin the tea-tasting experiment, for instance, n can equal only 4, 3, 2, 1, 0.\nthe one-sided p-values are restricted to 0.014, 0.243, 0.757, 0.986, and 1.0. if\n\n11\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 109, "text": "94\n\ninference for contingency tables\n\n0\n\n0\n\none rejects h when the p-value does not exceed 0.05, then 0.05 is not the\nprobability of type i error. only the p-value of 0.014 does not exceed 0.05;\nthus, when h is true, the probability of falsely rejecting it is 0.014, not 0.05.\nin this sense, the traditional approach to hypothesis testing is conservative:\nthe true probability of type i error is less than the nominal level.\n\nit is possible to achieve any fixed significance level by data-unrelated\nrandomization on the boundary of the critical region, in deciding whether to\nreject h . for the tea-tasting experiment, suppose that we reject h when\nn s 4, we reject h with probability 0.157 when n s 3, and we do not\n11\nreject h otherwise; that is, when n s 3, we generate a uniform random\nvariable u over 0, 1 and reject h if u - 0.157. for expectation taken with\nrespect to the null hypergeometric distribution of n , the significance level\nequals\np reject h s e p reject h n\n\u017e\n<\n\n11\n\n11\n\n11\n\n.\n\nw\n\nx\n\n0\n\n0\n\n0\n\n0\n\n0\n\n\u017e\n\n0\n\n11\n\n0\n\ns 1.0 0.014 q 0.157 0.229 q 0.0 = p n f 2 s 0.05.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n11\n\n.\n\u017e\n\n.\n\n\u017e\n\n\u017e\nwith the randomization extension, tocher 1950 showed that fisher\u2019s test is\n.\nuniformly most powerful unbiased umpu .\n\nin practice, randomization having nothing to do with the data is unaccept-\nable. we recommend simply reporting the p-value. to reduce conservative-\nness, report the mid-p-value section 1.4.5 . the test is no longer guaranteed\nto have true p type i error no greater than the nominal value, but in\npractice it is rarely much greater. for the one-sided test with the tea-tasting\ndata,\n\n\u017e\n\n.\n\n.\n\n\u017e\n\nmid-p-value s 1r2 p n s 3 q p n ) 3 s 0.129.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n11\n\n11\n\n4\n\n\u0004\n\niq\n\n3.5.5 small-sample unconditional test of independence*\na common sampling assumption for analyses comparing two groups on a\nbinary response is that the rows are independent binomial samples. then,\nonly n\nare naturally fixed. for poisson and multinomial sampling schemes,\nneither marginal distribution is fixed. for such cases it may seem artificial to\ncondition on both sets of marginal counts. an alternative small-sample test,\ndesigned for independent binomial samples, conditions on only the row\ntotals.\nunder binomial sampling with parameter \u2432 in row i, consider testing h :\n0\n\u2432 s \u2432 using some test statistic t, such as the pearson x . for fixed n\n\u0004\n4\n,\niq\nt can take a discrete set of values, one of which is the observed value t .o\ngiven \u2432 s \u2432 s \u2432, the p-value is p t g t\n.\n, calculated using the product\nof the two binomial probability mass functions. this is the sum of the product\nbinomial probabilities for those pairs of binomial samples that have t g t .o\nsince \u2432 is unknown, the actual p-value is defined as\n\n\u017e\n\n\u2432\n\no\n\n1\n\n2\n\n2\n\n1\n\n2\n\ni\n\np s sup p t g t\n\n\u017e\n\n\u2432\n\n0f\u2432f1\n\n.\n\n.\n\no\n\n "}, {"Page_number": 110, "text": "small-sample tests of independence\n\n95\n\nw\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n3\u017e\n\nthis is an unconditional small-sample test of independence. like fisher\u2019s\nexact test, the true size is no greater than the nominal value e.g., if we reject\nwhen p f 0.05, the actual p type i error is no greater than 0.05 .\n.\nwe illustrate using test statistic x 2 for the 2 = 2 table having entries\n3, 0r0, 3 , by row, with fixed row totals 3, 3 as binomial sample sizes.\n\u017e\nthe sample x 2 s 6.0. this x 2 value for the observed table and for table\n0, 3r3, 0 is the maximum possible. for a given value \u2432 for \u2432 s \u2432 , the\n\u017e\ntable is \u2432 1 y \u2432 \u2432 1 y \u2432 s \u2432 1 y \u2432\n.3\nprobability of\n\u017e3 successes and 0 failures in the first row and 0 successes and 3 failures in\nthe second , the product of two binomial probabilities. similarly, the proba-\nbility of the second table is 1 y \u2432 \u2432 . thus, the p-value is p x g 6 s\n2\u2432 1 y \u2432 , the sum of the product binomial probabilities for those two\ntables. the supremum of this over 0 f \u2432f 1 occurs at \u2432s , giving overall\n0.5 s 0.031. by contrast, the two-sided fisher\u2019s\np-value equal to 2 0.5\nexact test has p-value equal to 2\n\nthe first\n\n.0xw\n\n\u017e\nr s 0.100.\n\nbarnard 1945, 1947 first proposed an unconditional test comparing bino-\nmial parameters, although he later 1949 refuted it in favor of fisher\u2019s exact\ntest. several authors have since proposed related tests e.g., haber 1986;\n.\nsuissa and shuster 1985 .\n\n/\u017e\n\n.3x\n\n.3\u017e\n\n\u017e\n\u2432\n\n0\u017e\n\n.3\n\n.3\n\n.3\n\n3\u017e\n\n3\u017e\n\n\u017e\n\n/\n\n/\n\n3\n0\n\n3\n3\n\n6\n3\n\n1\n2\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n1\n\n2\n\n3\n\n2\n\n.\n\n\u017e\n\n3.5.6 conditional versus unconditional tests*\nsince barnard introduced the unconditional test, statisticians have debated\nthe proper way to conduct small-sample analyses of 2 = 2 tables. fisher\ncriticized the unconditional approach, arguing that possible samples with\nquite different numbers of successes than observed were not relevant. in\nfisher\u2019s 1945 view, \u2018\u2018 . . . the existence of these less informative possibilities\nshould not affect our judgment of significance based on the series actually\nobserved . . . . the fact that such an unhelpful outcome as these might\noccur . . . is surely no reason for enhancing our judgment of significance in\ncases where it has not occurred;\n. . . it is only the sampling distribution of\nsamples of the same type that can supply a rational test of significance.\u2019\u2019\nsprott 2000, sec. 6.4.4 recently provided a similar argument.\n\n.\nan adaptation of the unconditional approach by berger and boos 1994\naddresses this criticism somewhat. they took the supremum for the p-value\nover a confidence interval of values for the nuisance parameter \u2432 rather than\nover all possible values. their unconditional p-value is\n\n.\n\n\u017e\n\n\u017e\n\np s sup p t g t q \u2425,\n\n\u017e\n\n.\n\n\u2432\n\no\n\n\u2432gc \u2425\n\n\u2425\n\nwhere c is a 100 1 y \u2425 % confidence interval for \u2432. here, \u2425 is taken to be\nvery small e.g., 0.001 , and the test maintains the guaranteed upper bound\non size.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n "}, {"Page_number": 111, "text": "96\n\ninference for contingency tables\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n11\n\nother arguments in favor of conditioning on both sets of marginal totals\nare that the conditional approach provides a simple way to eliminate nui-\nsance parameters in a variety of problems e.g., generalizing to other contin-\ngency table problems , and the margins contain little information about the\nassociation haber 1989; yates 1984 . zhu and reid 1994 noted that some\ninformation loss occurs in conditioning on the margins except when \u242as 1.\narguments against conditioning partly concern the increased discreteness\nthat occurs. the few possible values for n make it difficult to obtain a small\np-value. in repeated use with a nominal significance level, the actual type i\nerror probability may be much smaller than the nominal value and the power\nmay suffer. finally, for inference about nonnull values e.g, confidence\nintervals , we will see that the conditional approach applies only with the\nodds ratio and not other measures.\n\nthe conservatism problem is partly unavoidable. statistics having discrete\ndistributions are necessarily conservative in terms of achieving nominal\nsignificance levels. because an unconditional test fixes only one margin,\nhowever,\nit has many more tables in the reference set for its sampling\ndistribution. that distribution is less discrete, and a richer array of possible\np-values occurs than with fisher\u2019s exact test. an unconditional test tends to\nbe less conservative and more powerful than fisher\u2019s exact test. a disadvan-\ntage is that computations are very intensive for more complex problems, such\nas larger tables.\n\nif a table truly has two independent binomial samples, the unconditional\napproach seems sensible. see kempthorne 1979 for a cogent argument. the\nconditional approach is useful for other cases. in a randomized clinical trial a\nconvenience sample of n subjects is randomly allocated to two treatments.\nthe samples are not binomials, as they are not random samples from two\npopulations of interest. one could focus on the sample alone and consider\nthe probability of a result at least as extreme as observed if there truly is no\ntreatment effect. for instance, out of all possible ways of choosing n\nof the\nn subjects for treatment 1, for what proportion would n be at least as large\nas observed? under the null hypothesis of no treatment effect, the same\noverall response distribution n , n\nof successes and failures occurs\nregardless of the allocation of subjects to treatments. thus, the column mar-\ngin is also naturally fixed. this argument leads to hypergeometric null proba-\nbilities and fisher\u2019s exact test greenland 1981 . this argument does not\nextend, however, to nonnull effect values and hence to confidence intervals.\nwhen both sets of marginal totals are naturally fixed, such as in table 3.8,\nthe high degree of discreteness is unavoidable and fisher\u2019s exact test is the\nbest procedure. regardless of which margins are naturally fixed, using the\nmid-p-value helps reduce conservative effects of discreteness.\n\n1q\n\nq1\n\nq2\n\n11\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n3.5.7 derivation of exact conditional distribution*\nwe now show how the conditional test for independence yields the hypergeo-\nmetric distribution. we do this for i = j tables, since we next discuss\n\n "}, {"Page_number": 112, "text": "small-sample tests of independence\n\n97\n\nextensions of fisher\u2019s exact test for them. we assume independent multino-\nmial sampling within rows, as often applies in comparing i treatment groups.\nthen row totals n\nare fixed, and we estimate the i conditional distribu-\nindependence, \u2432 s \u2432 s \u2b48\u2b48\u2b48 s\ntions \u2432 ,\nj < i\n\u2432 s \u2432 , for j s 1, . . . , j. the product of the i multinomial probability\nqj\nfunctions then simplifies to\n\nj s 1, . . . , j . under h :\n\niq\n\nj <2\n\nj < i\n\nj <1\n\n4\n\n4\n\n\u0004\n\n\u0004\n\n0\n\n\u017e\n\n\u0142\n\ni\n\nn !\niq\n\u0142 n !\ni j\n\nj\n\nn i j\u2432 s\n\n\u0142 j < i\n\nj\n\n\u017e\n\n. \u017e\n\n\u0142 n ! \u0142 \u2432nqj\nj qj\n\ni\n\niq\n\u0142 \u0142 n !\ni j\n\ni\n\nj\n\n.\n\n.\n\n\u017e\n\n3.17\n\n.\n\n/\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ni j\n\nqj\n\ndepends on \u2432 . these are nuisance parameters,\nthis distribution for n\nsince they do not describe the association. fisher introduced the standard\nway of eliminating nuisance parameters, by conditioning on their sufficient\nstatistics. from the definition of sufficiency, the resulting conditional distri-\nbution does not depend on those parameters.\n\n.\nthe contribution of \u2432 to the product multinomial distribution 3.17\n4\n, which are their sufficient statistics.\n4.\n\n\u0004\ndepends on the data only through n\n\u0004\nthe n\n\nhave the multinomial n, \u2432 distribution, namely\n\nqj\n\n\u017e\n\n\u017e\n\n\u0004\n\n4\n\n\u0004\n\n4\n\nqj\nqj\n\nqj\n\nn!\n\n\u0142 n !\nj qj\n\n\u0142 qj\nnqj\u2432 .\n\nj\n\n\u017e\n\n3.18\n\n.\n\n\u0004\nthe joint probability function of n\n4\n\u0004\nfunction of n , since n\ni j\n\u0004\nof n , conditional on n\nqj\n\u0004\ndivided by the probability function 3.18 evaluated at n\n\n\u0004\nand n\ndetermines n\nqj\n4\n, equals the probability function 3.17 of n\n\n4\nis identical to the probability\nqj\n4\n. thus, the probability function\n4\ni j\n\n\u017e\n, or\n\n\u017e\n\n.\n\n.\n\n4\n\n\u0004\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ni j\n\ni j\n\ni j\n\nqj\n\n\u017e\n\n. \u017e\n\niq\n\n\u0142 n ! \u0142 n !\nj qj\ni\nn!\u0142 \u0142 n !\ni j\n\ni\n\nj\n\n.\n\n.\n\n\u017e\n\n3.19\n\n.\n\n4\n\n\u0004\n\n\u0004\n\n4\n\n\u0004\n\niq\n\nand n\n\n4\nqj \u017e\n\n4\nthis is the multiple hypergeometric distribution. it applies to the set of ni j\nas the observed table. for 2 = 2 tables, it is\nhaving the same n\n.\nthe hypergeometric distribution 3.16 .\nwhen a table has a single multinomial sample, the unknown parameters\nare \u2432 . for testing independence \u2432 s \u2432 \u2432 all i and j , distribution\n\u0004\n\u017e\n3.19 results from conditioning on the row and column totals. these are\nsufficient statistics for \u2432 and \u2432 , which determine the null distribution.\nfor either sampling model, both sets of margins are fixed after the condition-\ning. the end result 3.19 does not depend on unknown parameters and thus\npermits exact probability calculations.\n\niq qj\n\niq\n\nqj\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ni j\n\ni j\n\n3.5.8 exact tests of independence for i = j tables*\nexact tests for i = j tables utilize the multiple hypergeometric distribution.\nfreeman and halton 1951 defined the p-value as the probability of the set\n\n\u017e\n\n.\n\n "}, {"Page_number": 113, "text": "98\n\ninference for contingency tables\n\ntable 3.9 example for exact conditional test\n\nsmoking level\ncigarettesrday\n.\n\u017e\n) 25\n12\n3\n\n1\u139024\n25\n1\n\n0\n25\n0\n\ncontrol\nmyocardial infarction\n\nsource: reprinted with permission, based on table 5 in\n.\ns. shapiro et al., lancet 743\u1390746 1979 .\n\n\u017e\n\n2\n\n2\n\no\n\no\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nof tables with the given margins that are no more likely to occur than the\ntable observed. other exact tests order the tables using a statistic describing\ndistance from h . yates 1934 used x . the p-value is then the null value\n0\nof p x g x\nfor observed value x . when classifications have ordered\ncategories, an ordinal statistic is more relevant. for the alternative hypothesis\nof a positive association, we could use p t g t\n.\n, where t is the correlation\nor gamma and where t denotes its observed value.\n\n2.\no\n\n2\no\n\n\u017e\n.x\n\nwe illustrate an exact test for ordered categories with table 3.9, which\ncross-classifies level of smoking and myocardial infarction for a sample of\nyoung women in a case\u1390control study. the second row contains small counts,\nand large-sample tests may be inappropriate. given the marginal counts, the\nonly table having greater evidence of positive association between smoking\nand myocardial infarction has counts 25,26,11 for row 1 and 0,0,4 in row 2.\nconditional on both sets of margins, the null probability of the observed\ntable and this more extreme table based on formula 3.19\nequals 0.018.\nalthough the sample contains only four myocardial infarction patients, evi-\ndence exists of a positive association. the evidence is stronger than using\nx , which ignores the ordering of categories. the exact p x g x s\n2\np x g 6.96 s 0.052.\n\u017e\n\nspecial algorithms and software for computing exact tests for i = j tables\nare widely available e.g., mehta and patel 1983; see also appendix a . we\nrecommend these tests when asymptotic approximations may be invalid.\ncomputing time increases exponentially as n, i, or j increase. however, one\ncan use monte carlo to sample randomly from the set of tables with the\ngiven margins. the estimated p-value is then the sample proportion of tables\nhaving test statistic value at least as large as the value observed.\nas i andror j increase, the number of possible values for any test statistic\nt tends to increase. thus, the conservativeness issue for conditional tests\nbecomes less problematic.\n\n2.\no\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nw\n\n2\n\n2\n\n3.6 small-sample confidence intervals for 2 = 2 tables*\n\nsmall-sample methods also apply to estimation. exact distributions depend-\ning only on the parameter of interest result from the same arguments. these\n\n "}, {"Page_number": 114, "text": "small-sample confidence intervals for 2 = 2 tables\n\n99\n\ndistributions are the basis of confidence intervals for measures such as the\nodds ratio.\n\n3.6.1 small-sample inference for the odds ratio\n4\nfor multinomial sampling, the distribution of n\nprobabilities \u2432 . for 2 = 2 tables, the odds ratio is\n\n\u0004\n\n\u0004\n\n4\n\ni j\n\ni j\n\ndepends on n and cell\n\n\u242as\n\n\u2432 \u2432\n\n11\n\n22\n\n\u2432 \u2432\n\n12\n\n21\n\ns\n\n\u2432 1 y \u2432 y \u2432 q \u2432\n\u017e\n11\n\u2432 y \u2432 \u2432 y \u2432\n\u017e\n1q\n11\n\n1q\n. \u017e\n\nq1\n\nq1\n\n11\n\n11\n.\n\n.\n\n.\n\n\u0004\n\n4\n\ni j\n\n4\n\n11\n\n1q q1\n\n1q q1\n\nhence, \u2432 is a function of \u242a and \u2432 ,\u2432 . the same argument applies to\nany \u2432 , so the multinomial distribution of\ncan use parameters\n\u0004\n\u242a, \u2432 , \u2432 . conditional on n , n\ndepends\nonly on \u242a. since n\ndetermines all other cell counts, given the marginal\ntotals, the conditional distribution of n\nis specified by some function\np n s t s f t; n , n , n, \u242a . this distribution fisher 1935c is the non-\n\u017e\ncentral hypergeometric,\n\n, the distribution of n\n\n1q q1\n\n1q q1\n\nn\n\n11\n\n11\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ni j\n\ni j\n\ni j\n\n\u017e\n\nf\n\nt ; n , n\n\n1q q1\n\n, n, \u242a s\n\n.\n\n1q\nt\n\nn\n\n\u017e\n\u00fd \u017e\n\nmq\n\nusmy\n\n/\u017e\n\nn\n1q\nu\n\n/\n\nt\u242a\n\nn y n\n1q\nn y t\nq1\nn y n\n1q\nn y u\nq1\n\n/\u017e\n\n\u017e\n\n3.20\n\n.\n\n/\n\nu\u242a\n\ny\n\nfor m f t f m .\nq\nhaving observed n s t . for h : \u242a) \u242a , the p-value is\n\na confidence interval for \u242a results from inverting the test of h : \u242as \u242a ,\n\n0\n\n0\n\na\n\n0\n\n11\n\no\n\np s\n\n\u00fd\ntgt o\n\n\u017e\n\nf\n\nt ; n , n\n\n1q q1\n\n.\n, n, \u242a .\n\n0\n\nfor testing against h : \u242a- \u242a ,\n0\n\u00fd\ntft o\n\np s\n\n0\n\n\u017e\n\nf\n\nt ; n , n\n\n1q q1\n\n.\n, n, \u242a .\n\n0\n\n0\n\n0\n\n0\n\nwhen \u242a s 1, these are one-sided fisher\u2019s exact tests. cornfield 1956\n.\nconstructed a confidence interval using the tail method. the lower endpoint\nis \u242a for which p s \u2423r2 in testing against h : \u242a) \u242a . the upper endpoint is\n\u242a for which p s \u2423r2 for h : \u242a- \u242a . the interval is the set of \u242a for which\nboth one-sided p-values g \u2423r2.\n\nas in fisher\u2019s exact test, the conditional approach to interval estimation is\nnecessarily conservative because of discreteness. the actual confidence coef-\nficient, defined as the infimum of the coverage probabilities for all possible\n\u242a, has the nominal confidence level as a lower bound. less conservative\n\n\u017e\n\na\n\na\n\n0\n\n0\n\n0\n\n "}, {"Page_number": 115, "text": "100\n\ninference for contingency tables\n\n.\n\n\u017e\n\nbehavior and shorter intervals result from inverting a single two-sided test\nrather than inverting two one-sided tests agresti and min 2001; baptista and\npike 1977 . an alternative approach with independent binomial samples\ninverts nonnull unconditional small-sample tests. because of the reduced\ndiscreteness, such intervals are also usually shorter.\n\nthe conditional ml estimate of \u242a is the value of \u242a that maximizes\nprobability 3.20 . differentiating the log likelihood with respect to \u242a shows\nthat this estimate satisfies the equation n s e n\n\u017e\nin \u242a, where the expecta-\ntion refers to distribution 3.20 . this equation has a unique solution \u242a and is\nsolved using iterative methods cornfield 1956 . this estimator differs from\nthe unconditional ml estimator \u242as n n rn n , which uses the ml esti-\nmates of \u2432 for the multinomial distribution of n . using statistical\nsoftware, we can calculate conditional ml estimates and small-sample confi-\n.\ndence intervals for odds ratios e.g., for sas, see table a.2 .\n\n\u02c6\n\n\u02c6\n\n11\n\n11\n\n11\n\n22\n\n12\n\n21\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ni j\n\ni j\n\n\u017e\n\n3.6.2 tea tasting example\nwe illustrate with table 3.8 from fisher\u2019s tea-tasting experiment. the condi-\ntional ml estimate of \u242a is 6.4. software provides the cornfield tail-method\ninterval 0.2, 626.2 with confidence coefficient guaranteed g 0.95. not\nsurprisingly, it is very wide because of the small sample. inverting a family of\ntwo-sided \u2018\u2018exact\u2019\u2019 conditional score tests gives a more precise interval, 0.3,\n306.2 . the unconditional approach is not appropriate here because of the\nsampling design. if the table were two binomial samples, that approach gives\ninterval 0.4, 234.4 by inverting \u2018\u2018exact\u2019\u2019 unconditional score tests.\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\nw\n\nx\n\n\u017e\n\nimpact of discreteness on exact confidence intervals\n\n3.6.3\nsmall-sample inference is \u2018\u2018exact\u2019\u2019 in the sense that the conditional distribu-\ntion is free of nuisance parameters. confidence intervals and tests use exact\nprobability calculations rather than approximate ones. however, their operat-\ning characteristics are conservative because of discreteness.\n\nlarge-sample methods do not have the guarantee of bounds on error\nprobabilities. they can be conservative or liberal, and thus their results can\nappear quite different from exact methods. for example, for the tea-tasting\ndata table 3.8 , the p-value for the pearson chi-squared test equals 0.157,\ncompared to 0.486 for the two-sided exact test. the 95% large-sample\nconfidence interval 3.2 for the odds ratio is 0.4, 220.9 , compared to\ncornfield\u2019s exact interval of 0.2, 626.2 . normally, one would prefer an exact\nmethod over an approximate one. when the conditional distribution is highly\ndiscrete, however, the choice is not so obvious. exact methods then can be\nquite conservative, especially with small samples.\n\nfor highly discrete data, it seems sensible to use adjustments of exact\nmethods based on the mid-p-value. confidence intervals with the conditional\napproach then invert hypergeometric tests of \u242as \u242a using the mid-p-value.\nalthough not guaranteed to have error probabilities no greater than the\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n0\n\n "}, {"Page_number": 116, "text": "extensions for multiway tables and nontabulated responses\n\n101\n\nnominal level, this method usually comes closer than the exact method to the\ndesired level. compared to large-sample methods, it has the advantage of\nworking well as the degree of discreteness diminishes, since it then is\nessentially the same as the corresponding exact method using an ordinary\np-value.\n\ninference based on the mid-p-value compromises between the conserva-\ntiveness of exact methods and the uncertain adequacy of large-sample meth-\nods. for interval estimation of the odds ratio, this method tends to be a bit\nconservative, but for small samples can yield much shorter intervals than the\ncornfield exact interval. for the tea-tasting data, for instance, the 95%\nconfidence interval based on inverting two one-sided hypergeometric tests\nusing the mid-p-value is 0.31, 309 , compared to the cornfield interval of\n.\n\u017e\n0.21, 626 .\n\n\u017e\n\n.\n\n1\n\n0\n\n2\n\n1\n\n\u017e\n\na small-sample confidence interval\n\n3.6.4 small-sample inference for difference of proportions\nthe conditional approach to eliminating nuisance parameters works when\n.\nthose parameters have sufficient statistics. however, we\u2019ll see section 6.7.9\nthat reduced sufficient statistics occur only for certain models. for binary\ndata, such models must have odds ratios as parameters. for 2 = 2 tables, the\nconditional approach cannot yield confidence intervals for differences or\nratios of proportions. the unconditional approach is more complex but does\nnot require sufficient statistics. we used it in section 3.5.5 for testing\n\u2432 y \u2432 s 0 with independent binomial samples.\ninverts the corresponding uncondi-\ntional test of h : \u2432 y \u2432 s \u2426 , for any fixed y1 - \u2426 - 1. the probability\nfunction for the table is the product of bin n , \u2432 and bin n , \u2432 mass\nfunctions. one can express this in terms of \u2426s \u2432 y \u2432 and a nuisance\nparameter \u242d. for instance, if \u242ds \u2432 q \u2432 , one substitutes \u2432 s \u242dq \u2426 r2\nand \u2432 s \u242dy \u2426 r2. for \u2426s \u2426 and a fixed value of \u242d, one then uses this\nbinomial product to calculate the probability that the test statistic is at least\nas large as observed. the p-value is the supremum of such probabilities\ncalculated over all possible values for \u242d. this provides a family of tests for the\nvarious values of \u2426 . the confidence interval for \u2432 y \u2432 is the set of \u2426 for\nwhich this p-value exceeds \u2423.\n\nthis approach can be quite conservative. for details regarding various test\nstatistics, see agresti and min 2001 , coe and tamhane 1993 , santner and\nsnell 1980 , and santner and yamagami 1993 . it is better to invert a single\ntwo-sided test, as in coe and tamhane 1993 , than to invert two separate\none-sided tests.\n\n\u017e\n\u017e\n\n.\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2\n\n0\n\n0\n\n1\n\n1\n\n2\n\n2\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\n0\n\n0\n\n1\n\n2\n\n0\n\n3.7 extensions for multiway tables and\nnontabulated responses\n\nthe methods of this chapter extend to multiway contingency tables. for\ninstance, tests of independence for two-way tables extend to tests of condi-\n\n "}, {"Page_number": 117, "text": "102\n\ninference for contingency tables\n\ntional independence in three-way tables. in future chapters we present such\nmethods with models that provide a basis for defining relevant parameters\nand their statistical inferences. the methods then apply in a greater variety\nof situations, such as when some explanatory variables are continuous rather\nthan categorical.\n\n3.7.1 categorical data need not be contingency tables\nexamples so far have presented categorical data in the format of contingency\ntables. however, this book has broader focus than contingency table analysis.\nmodels for categorical response variables can have continuous as well as\ncategorical explanatory variables. even when all or most variables are cate-\ngorical, source data files are not usually contingency tables but have the form\nof a line of data for each subject. the first three lines in a data file containing\nresponses of a survey of subjects measuring gender, race, education 1 s less\nthan high school, 2 s high school or some college, 3 s college graduate , and\n.\nopinion about homosexuality 1 s tolerant, 2 s homophobic might be:\n\n\u017e\n\n\u017e\n\n.\n\nsubject\n\ngender\n\n1\n2\n3\n\nf\nm\nm\n\nrace\nw\nb\nw\n\neducation\n\nopinion\n\n2\n3\n1\n\n1\n1\n2\n\nsoftware can read data files of this type and then conduct analyses that may\ninvolve forming contingency tables.\n\nin the next chapter we introduce the modeling framework used in the rest\nof the book. all the methods that we\u2019ve studied in this chapter result from\ninferences for parameters in simple versions of these models.\n\nnotes\nsection 3.1: confidence inter\u00a9als for association parameters\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n3.1. adaptations of woolf\u2019s interval 3.2 for log\u242a to handle zero cell counts include agresti\n\u017e\n1999 and gart 1966, 1971 . goodman 1964a presented simultaneous confidence\nintervals for all odds ratios in an i = j table. brown and benedetti 1977 and\ngoodman and kruskal 1963, 1972 provided standard errors for many association\nmeasures. goodman and kruskal 1963, 1972 extended 3.9 for independent multino-\nmial sampling.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n3.2. agresti and caffo 2000 showed that as in the single-sample case problem 1.24 , the\nwald interval 3.4 for \u2432 y \u2432 behaves much better after adding two pseudo-observa-\n.\ntions of each type one of each type in each sample .\n\n\u017e\n\n.\n\n\u017e\n\n1\n\n2\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 118, "text": "notes\n\nsection 3.2: testing independence in two-way contingency tables\n\n3.3. for hypergeometric sampling, \u242e in tests of independence are exact\n\n\u0004\n\n\u02c6i j\n\n4\n\nestimated expected values. specifically,\n\n.\n\n103\n\n\u017e\nrather than\n\ne n s\n\u017e\n\n.\n\n11\n\nn\n\n1q q1\n\nn\nn\n\nand var n s\n\n.\n\n\u017e\n\n11\n\nn\n\n2q q2\n\nn\n\nn\nn\n1q q1\nn n y 1\n\u017e\n.\n2\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n2.\n\nhaldane 1940 derived e x s i y 1 j y 1 nr n y 1 and a complex formula for\nvar x ; dawson 1954 provided a simplified expression. lewis et al. 1984 derived\nthe third central moment. watson 1959 showed that the conditional distribution of\nx 2 also has the limiting chi-squared distribution.\n\n2.\n\n.\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n3.4. diaconis and efron 1985 presented inference based on a uniform distribution over all\npossible tables of the same i, j, and n; their \u00aeolume test considers the proportion of\nsuch tables having x 2 f x 2.o\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n3.5. specialized methods are necessary for complex sampling designs. sequential methods\nare useful in biomedical applications jennison and turnbull 2000, chap. 12 . social\nscience applications often incorporate clustering andror stratification. lavange et al.\n\u017e\n2001 and rao and thomas 1988 surveyed analyses of categorical data for complex\nsampling methods. gleser and moore 1985 showed that positive dependence causes\nnull distributions of pearson statistics to stochastically increase. see also bedrick\n\u017e\n1983 , clogg and eliason 1987 , fay 1985 , holt et al. 1980 , koehler and wilson\n\u017e\n.\n.\n1986 , rao and scott 1987 , scott and wild 2001 , shuster and downing 1976 ,\ntavare and altham 1983 , and methods of chapter 12.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n.\n\u00b4\n\nother modifications are necessary when some data are missing. watson 1956 was\nperhaps the first to study this. lipsitz and fitzmaurice 1996 derived score tests of\nindependence and conditional independence for contingency tables, assuming ignor-\nable nonresponse, and showed that\nthe test statistics have the usual asymptotic\nchi-squared null distributions. see schafer 1997, chap. 7 for a survey of methods.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nsection 3.4: two-way tables with ordered classifications\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n3.6. bhapkar 1968 and yates 1948 proposed statistics similar to m and also proposed\nstatistics for singly-ordered tables. graubard and korn 1987 listed 14 tests for 2 = j\n.\ntables that utilize a correlation-type statistic. see also nair 1987 and williams 1952 .\n\u017e\ncohen and sackrowitz\n1991, 1992 evaluated decision-theoretic aspects, such as\nadmissibility, of tests based on gamma and local log odds ratios. rayner and best\n\u017e\n2001 considered nonparametrics methods in a contingency table format.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2\n\nsection 3.5: small-sample tests of independence\n\n3.7. yates 1934 mentioned that fisher suggested the hypergeometric to him for an exact\n\n\u017e\n\n.\n\ntest. he proposed a continuity-corrected version of x 2,\n\nc \u00fd\u00fd\n\n2x s\n\n\u017e\n\n<\n\nn y \u242e y 0.5\n\n<\n\ni j\n\n\u02c6\ni j\n\u242e\u02c6i j\n\n2\n\n.\n\n,\n\n.\nto approximate the exact test. haber 1980, 1982 , plackett 1964 , and yates 1984\ndiscussed its appropriateness. since software now makes fisher\u2019s exact test feasible\neven with large samples, this correction is no longer needed.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n "}, {"Page_number": 119, "text": "104\n\ninference for contingency tables\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u00b4\n\n\u00b4\n\n\u017e\n\u017e\n\n.\n.\n\n\u017e\n\u017e\n\n3.8. the umpu property of fisher\u2019s exact test follows from conditioning on a sufficient\nstatistic that is complete and has distribution in the exponential family lehmann 1986,\n.\nsecs. 4.5\u13904.7 . fleiss 1981 , gail and gart 1973 , and suissa and shuster 1985\nstudied sample size for obtaining fixed power in fisher\u2019s test. the controversy over\n.\n\u017e\nconditioning includes barnard 1945, 1947, 1949, 1979 , berkson 1978 , fisher 1956 ,\n\u017e\n.\n1979 , lloyd 1988a , pearson 1947 , rice 1988 ,\nhoward 1998 , kempthorne\n\u017e\n1984, 1985 , and yates 1984 . yates and\nroutledge 1992 , suissa and shuster\ndiscussants also addressed the choice of two-sided p-value. discussion of unconditional\nmethods includes chan 1998 , mart\u0131n andres and silva mato 1994 , and rohmel and\nmansmann 1999 . altham 1969 and howard 1998 discussed bayesian analyses for\n2 = 2 tables see section 15.2.3 . agresti 1992, 2001 surveyed small-sample methods.\n3.9. for discussion of inference using the mid-p-value, see berry and armitage 1995 , hirji\n\u017e\n.\n1991 , hwang and wells 2002 , hwang and yang 2001 , mehta and walsh 1992 ,\nand routledge 1994 . similar benefits can accrue from alternative proposed p-values.\none approach, useful when several tables have the same value for a test statistic, uses\nthe table probability to create a more finely partitioned sample space; for tables having\nthe observed test statistic value, only those contribute to the p-value that are no more\n.\nlikely than the observed table cohen and sackrowitz 1992; kim and agresti 1995 .\nthis depends on more than the sufficient statistic, and in some cases a rao\u1390blackwel-\nlized version is the mid-p-value hwang and wells 2002 . ordinary p-values obtained\nwith higher-order asymptotic methods without continuity corrections for discreteness\nyield performance similar to that of the mid-p-value pierce and peters 1999; strawder-\n.\nman and wells 1998 .\n\n.\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u2c91\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n3.10. for exact treatment of i = j tables, see mehta and patel 1983 . for ordered cate-\ngories, see also agresti et al. 1990 . for monte carlo estimation of exact p-values, see\nagresti et al. 1979 , booth and butler 1999 , diaconis and sturmfels 1998 , forster\net al. 1996 , mehta et al. 1988 , and patefield 1982 . gail and mantel 1977 and\ngood 1976 gave approximate formulas for the number of tables having certain fixed\nmargins. freidlin and gastwirth 1999 extended the unconditional approach to a test\nfor trend in i = 2 tables and a test of conditional independence with several 2 = 2\ntables.\n\n\u017e\n\u017e\n\n.\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nsection 3.6: small-sample confidence inter\u00a9als for 2 = 2 tables\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n3.11. suppose that \u242a, \u242d has minimal sufficient statistic t, u , where \u242d is a nuisance\nparameter. cox and hinkley 1974, p. 35 defined u to be ancillary for \u242a if its\ndistribution depends only on \u242d, and the distribution of t given u depends only on \u242a.\nfor 2 = 2 tables with odds ratio \u242a and \u242ds \u2432 ,\u2432 ,\nand u s\n.\n1q q1\n\u017e\n.\n. then u is not ancillary, because its distribution depends on \u242a as well as \u242d.\nn , n\n1q q1\nusing a definition due to godambe, bhapkar 1989 referred to the marginals u as\npartial ancillary for \u242a. this means that the distribution of the data, given u, depends\nonly on \u242a, and that for fixed \u242a, the family of distributions of u for various \u242d is\ncomplete. liang 1984 gave an alternative definition referring to conditional and\nunconditional inference being equally efficient.\n\nlet t s n\n\n11\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nproblems\n\napplications\n\n3.1 refer to table 2.9. construct and interpret a 95% confidence interval\n\u017e .\nfor the population a odds ratio, b difference of proportions, and c\nrelative risk between seat-belt use and type of injury.\n\n\u017e .\n\n\u017e .\n\n "}, {"Page_number": 120, "text": "problems\n\n105\n\n3.2 refer to table 2.5 on lung cancer and smoking. construct a confidence\n\ninterval for a relevant measure of association. interpret.\n\n3.3\n\nin professional basketball games during 1980\u13901982, when larry bird\nof the boston celtics shot a pair of free throws, 5 times he missed\nboth, 251 times he made both, 34 times he made only the first, and 48\ntimes he made only the second wardrop 1995 . is it plausible that the\nsuccessive free throws are independent?\n\n\u017e\n\n.\n\n3.4 refer to table 3.10.\n\na. using x 2 and g2, test the hypothesis of independence between\n\nparty identification and race. report the p-values and interpret.\n\nb. use residuals to describe the evidence of association.\nc. partition chi-squared into components regarding the choice be-\ntween democrat and independent and between these two com-\nbined and republican. interpret.\n\nd. summarize association by constructing a 95% confidence interval\nfor the odds ratio between race and whether a democrat or\nrepublican. interpret.\n\ntable 3.10 data for problem 3.4\n\nparty identification\n\nrace\nblack\nwhite\n\ndemocrat\n\nindependent\n\nrepublican\n\n103\n341\n\n15\n105\n\n11\n405\n\nsource: 1991 general social survey, national opinion re-\nsearch center.\n\n3.5 refer to table 3.10. in the same survey, gender was cross-classified\nwith party identification. table 3.11 shows some results. explain how\nto interpret all the results on this printout.\n\n3.6\n\n\u017e\n\n.\n\nin a study of the relationship between stage of breast cancer at\ndiagnosis local or advanced and a woman\u2019s living arrangement, of 144\nwomen living alone, 41.0% had an advanced case; of 209 living with\nspouse, 52.2% were advanced; of 89 living with others, 59.6% were\nadvanced. the authors reported the p-value for the relationship as\n0.02 d. j. moritz and w. a. satariano,\nj. clin. epidemiol. 46:\n443\u1390454, 1993 . reconstruct the analysis performed to obtain this\np-value.\n\n.\n\n\u017e\n\n "}, {"Page_number": 121, "text": "106\n\ninference for contingency tables\n\ntable 3.11 results for problem 3.5\n\nfrequency\nexpected\n\nfemale\n\nmale\n\ndem\n\n279\n261.42\n\n165\n182.58\n\nindep\n\n73\n70.653\n\n47\n49.347\n\nrepub\n\n225\n244.93\n\n191\n171.07\n\nstatistic\n\nchi- square\nlikelihood ratio chi- square\n\ndf\n\n2\n2\n\nvalue\n\n7.0095\n7.0026\n\nprob\n\n0.0301\n0.0302\n\nreschi streschi observ\n\nobserv\n\n1\n2\n3\n\nresraw\n17.584\n2.347\n\n2.293\n0.465\ny19.931 y1.274 y2.618\n\n1.088\n0.279\n\n4\n5\n6\n\nresraw\n\nreschi streschi\ny17.584 y1.301 y2.293\ny2.347 y0.334 y0.464\n2.618\n19.931\n\n1.524\n\n3.7 refer to table 2.1. partition g2 for testing whether the incidence of\nheart attacks is independent of aspirin intake into two components.\ninterpret.\n\n\u017e\n\n\u017e\n.\n.\n\n3.8 project blue book: analysis of reports of unidentified aerial objects was\npublished by the u.s. air force air technical intelligence center at\nwright-patterson air force base in may 1955 to analyze reports of\nunidentified flying objects ufos . in its table ii, the report classified\n1765 sightings later regarded as known objects and 434 sightings later\n.\nregarded as unknown, according to the object color nine categories .\nthe report states: \u2018\u2018the chi-square test is applicable only to distribu-\ntions which have the same number of elements,\u2019\u2019 so the investigators\nmultiplied all counts in the known category by 434r1765 , so each row\nhas 434 observations, before computing x 2. they reported x 2 s 26.15\nwith df s 8. explain why this is incorrect. what should x 2 equal?\n\u017e\n2\nhint: for their adjusted table, first show that the contribution to x\nis the same for each cell in a column, and then show the effect on\n.\nthose contributions of multiplying each count in one row by a constant.\n\n\u017e\n\n\u017e\n\n.\n\n3.9 table 3.12 classifies a sample of psychiatric patients by their diagnosis\n\nand by whether their treatment prescribed drugs.\na. obtain standardized pearson residuals for independence, and inter-\n\npret.\n\nb. partition chi-squared into three components to describe differences\n\u017e .\nand similarities among the diagnoses, by comparing i\nthe first two\n\u017e .\nrows,\nii the third and fourth rows, and iii the last row to the first\nand second rows combined and the third and fourth rows combined.\n\n\u017e\n\n.\n\n "}, {"Page_number": 122, "text": "problems\n\n107\n\ntable 3.12 data for problem 3.9\n\ndiagnosis\nschizophrenia\naffective disorder\nneurosis\npersonality disorder\nspecial symptoms\n\ndrugs\n105\n12\n18\n47\n0\n\nno drugs\n\n8\n2\n19\n52\n13\n\nsource: reprinted with permission from e. helmes and g. c.\n.\nfekken, j. clin. psychol. 42: 569\u1390576 1986 .\n\n\u017e\n\n3.10 refer to table 7.8. for the combined data for the two genders,\nyielding a single 4 = 4 table, x s 11.5 p s 0.24 , whereas using row\nscores 3, 10, 20, 35 and column scores 1, 3, 4, 5 , m s 7.04\np s 0.008 . explain why the results are so different.\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n2\n\n2\n\n\u017e\n\n\u017e\n\n3.11 a study on educational aspirations of high school students s. crys-\ndale, internat. j. compar. sociol. 16: 19\u139036, 1975 measured aspira-\ntions with the scale some high school, high school graduate, some\ncollege, college graduate . the student counts in these categories were\n\u017e\n11, 52, 23, 22 when family income was low, 9, 44, 13, 10 when family\nincome was middle, and 9, 41, 12, 27 when family income was high.\na. test independence of educational aspirations and family income\nusing x 2 or g2. explain the deficiency of this test for these data.\nb. find the standardized pearson residuals. do they suggest any\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\nassociation pattern?\n\nc. conduct an alternative test that may be more powerful. interpret.\n\n3.12 refer to table 8.15. obtain a 95% confidence interval for gamma.\ninterpret the association between schooling and attitude toward abor-\ntion.\n\n3.13 table 3.13 shows the results of a retrospective study comparing radia-\ntion therapy with surgery in treating cancer of the larynx. the response\n\ntable 3.13 data for problem 3.13\n\ncancer\n\ncontrolled\n\ncancer not\ncontrolled\n\nsurgery\nradiation therapy\n\n21\n15\n\n2\n3\n\nsource: reprinted with permission from w. m. mendenhall,\nr. r. million, d. e. sharkey, and n. j. cassisi, internat. j.\nradiat. oncol. biol. phys. 10: 357\u1390363 1984 , pergamon\npress plc.\n\n\u017e\n\n.\n\n "}, {"Page_number": 123, "text": "108\n\ninference for contingency tables\n\ntable 3.14 sas output for problem 3.13\n\nfisher\u2019s exact test\n\ncell (1,1) frequency (f)\nleft- sided pr <= f\nright- sided pr >= f\ntable probability (p)\ntwo- sided pr<= p\n\nodds ratio\n\nasymptotic conf limits:\n\nexact conf limits:\n\n21\n0.8947\n0.3808\n0.2755\n0.6384\n\n2.1000\n\n0.3116\n95% lower conf limit\n95% upper conf limit 14.1523\n95% lower conf limit\n0.2089\n95% upper conf limit 27.5522\n\nindicates whether the cancer was controlled for at least two years\nfollowing treatment. table 3.14 shows sas output.\na. report and interpret the p-value for fisher\u2019s exact test with i h :a\n\u242a) 1, and ii h : \u242a/ 1. explain how the p-values are calculated.\nb. interpret the confidence intervals for \u242a. explain the difference\n\n\u017e .\n\n\u017e .\n\na\n\nbetween them and how they were calculated.\n\nc. find and interpret the one-sided mid-p-value. give advantages and\n\ndisadvantages of this type of p-value.\n\n3.14 a study considered the effect of prednisolone on severe hypercal-\ncaemia in women with metastatic breast cancer b. kristensen et al., j.\nintern. med. 232: 237\u1390245, 1992 . of 30 patients, 15 were randomly\nselected to receive prednisolone. the other 15 formed a control group.\nnormalization in their level of serum-ionized calcium was achieved by\n7 of the treated patients and none of the control group. analyze\nwhether results were significantly better for treatment than for control.\ninterpret.\n\n.\n\n\u017e\n\n\u017e .\n\n3.15 for problem 3.14, obtain a 95% confidence interval for the odds ratio\n\u017e\nusing a the woolf\ni.e., wald interval, b cornfield\u2019s \u2018\u2018exact\u2019\u2019 ap-\nproach, c the profile likelihood. in each case, note the effect of the\nzero cell count. summarize advantages and disadvantages of each\napproach.\n\n\u017e .\n\n\u017e .\n\n.\n\n3.16 refer to the tea-tasting data table 3.8 . construct the null distribu-\ntions of the ordinary p-value and the mid-p-value for fisher\u2019s exact\ntest with h : \u242a) 1. find and compare their expected values.\n\n\u017e\n\n.\n\na\n\n "}, {"Page_number": 124, "text": "109\nproblems\n3.17 consider a 3 = 3 table having entries, by row, of 4, 2, 0 r 2, 2, 2 r\n0, 2, 4 . conduct an exact test of independence, using x . assuming\nordered rows and columns and using equally spaced scores, conduct an\nordinal exact test. explain why results differ so much.\n\n.\n\n\u017e\n\n2\n\n3.18 an advertisement by schering corp.\n\n.\n\nin 1999 for the allergy drug\nclaritin mentioned that in a pediatric randomized clinical trial, symp-\ntoms of nervousness were shown by 4 of 188 patients on loratadine\n\u017e\nclaritin , 2 of 262 patients taking placebo, and 2 of 170 patients on\nchoropheniramine. in each part below, explain which method you\nused, and why.\na. is there inferential evidence that nervousness depends on drug?\nb. for the claritin and placebo groups, construct and interpret a 95%\nconfidence interval for the i odds ratio and ii difference of\nproportions suffering nervousness.\n\n\u017e .\n\n\u017e .\n\n3.19 refer to problem 2.19 on sexual fun. analyze these data. present a\n\nshort report summarizing results and interpretations.\n\ntheory and methods\n\n3.20 is \u242a the midpoint of large- and small-sample confidence intervals for\n\n\u02c6\n\n\u242a? why or why not?\n\n3.21 for comparing two binomial samples, show that the standard error\n\u017e\n.3.1 of a log odds ratio increases as the absolute difference of\nproportions of successes and failures for a given sample increases.\n\n3.22 using the delta method, show that the wald confidence interval for\n\nthe logit of a binomial parameter \u2432 is\n\nlog \u2432r 1 y \u2432 \" z r n\u2432 1 y \u2432 .\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u2423r2\n\n'\n\n.\n\nexplain how to use this interval to obtain one for \u2432 itself. newcombe\n\u017e\n2001 noted that the sample logit is also the midpoint of the score\ninterval for \u2432, on the logit scale. he showed that this logit interval\ncontains the score interval.\n\nx\n\nw\n\n3.23 for two parameters, a confidence interval\n.\ni\n\nsingle-sample estimate \u242a and interval\n\n\u02c6\ni\n\n\u017e\n\n\u242a y \u242a y' \u242a y ll q u y \u242a\n\n\u02c6\n2\n\n\u02c6\n1\n\n\u02c6\n1\n\n\u017e\n\n\u02c6\n2\n\n.\n\n\u017e\n\n2\n\n2\n\n1\n\n2\n\n.\n\n\u017e\n\nfor \u242a y \u242a based on\n\n1\n\n2\n\ni\n\nll , u for \u242a, i s 1, 2, is\n, \u242a y \u242a q' u y \u242a q \u242a y ll\n\n\u02c6\n2\n\n\u02c6\n2\n\n\u02c6\n1\n\n\u02c6\n1\n\n\u017e\n\n.\n\n\u017e\n\n2\n\n1\n\ni\n\n2\n\n.\n\n2\n\n/\n\n.\n\n "}, {"Page_number": 125, "text": "110\n\ninference for contingency tables\nnewcombe 1998b proposed an interval for \u2432 y \u2432 using the score\n\u017e\nll , u for \u2432 that performs much better than the wald interval\ninterval\ni\n3.4 . it is \u2432 y \u2432 y z\n\u017e\n\ns , \u2432 y \u2432 q z\n\n.\n, with\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n1\n\n2\n\ni\n\ns\n\u2423r2 u\n\n\u2423r2 l\n\ni\n\u02c6\n\n1\n\n\u02c6\n\n2\n\n\u02c6\n\n2\n\n\u02c6\n\n1\n\n)\n\ns s\n\nl\n\n\u017e\nll 1 y ll\n\n1\n\n1\n\n.\n\nn\n\n1\n\nq\n\nu 1 y u\n\n\u017e\n\n2\n\nn\n\n2\n\n.\n\n2\n\n,\n\ns s\n\nu\n\nu 1 y u\n\n\u017e\n\n1\n\nn\n\n1\n\n.\n\n1\n\nq\n\n\u017e\nll 1 y ll\n\n2\n\n2\n\n.\n\nn\n\n2\n\n.\n\n)\n\n2\n\n\u02c6\n\nshow that it has the general form above of an interval for \u242a y \u242a .\n\n1\n\n3.24 for multinomial sampling, use the asymptotic variance of log \u242a to\nshow that for yule\u2019s q problem 3.26 the asymptotic variance of\n' \u017e\nn q y q is \u2434 s \u00fd \u00fd \u2432 1 y q r4 yule 1900, 1912 .\n.\n\n.\n.\n2 2\n\n.\u017e\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n2\n\ny1\ni j\n\ni\n\nj\n\n3.25 refer to problem 2.23. for multinomial sampling, show how to obtain\na confidence interval for ar by first finding one for log 1 y ar\n.\n\u017e\n.\nfleiss 1981, p. 76 .\n\n\u017e\n\n3.26 for multinomial probabilities \u2432 s \u2432 , \u2432 , . . . with a contingency\ntable of arbitrary dimensions, suppose that a measure g \u2432 s \u242fr\u2426.\n' w\nn g \u2432 y g \u2432 is \u2434 s\nshow that\nw\n\u00fd \u2432\u2429 y \u00fd \u2432\u2429 r\u2426 , where \u2429 s \u2426 \u2b78\u242fr\u2b78\u2432 y \u242f \u2b78\u2426r\u2b78\u2432 good-\n4\n.\nman and kruskal, 1972 .\n\nthe asymptotic variance of\n\u017e\ni\n\n.2x\n\n\u017e\n.x\n\n. \u017e\n\n\u017e\n.\n\n\u02c6\n\n2\ni\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n1\n\n2\n\n2\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n3.27 for ordinal variables, consider gamma 2.14 . let\n\n\u017e\n\n.\n\n\u2432\u017ec. s\n\ni j\n\n\u00fd \u00fd\n\na-i b-j\n\n\u2432 q\nab\n\n\u00fd \u00fd\n\na)i b)j\n\n\u2432 ,\nab\n\n\u2432\u017e d. s\n\ni j\n\n\u00fd \u00fd\n\na-i b)j\n\n\u2432 q\nab\n\n\u00fd \u00fd\n\na)i b-j\n\n\u2432 ,\nab\n\nwhere i and j are fixed in the summations. show that \u2338 s \u00fd \u00fd \u2432 \u2432\u017ec.\nand \u2338 s \u00fd \u00fd \u2432 \u2432\u017e d.. use the delta method to show that the large-\nsample normality 3.9 applies for \u2425, with goodman and kruskal\n.\n1963\n\ni j\u017e\n\n\u02c6\n\n.\n\n\u017e\n\ni j\n\ni j\n\ni j\n\nd\n\nc\n\ni\n\ni\n\nj\n\nj\n\n\u243e s 4 \u2338 \u2432 y \u2338 \u2432 r \u2338 q \u2338 ,\n\n\u017e\n\n.\n\n2\n\n\u017e d.\ni j\n\n\u017ec.\ni j\n\ni j\n\nd\n\nd\n\nc\n\nc\n\n\u00fd \u00fd\n\n\u2432 \u243e s 0 ,\n\ni j\n\ni j\n\ni\n\nj\n\n\u2434 s\n\n2\n\n16\n\n\u017e\n\n\u2338 q \u2338\n\nc\n\n4\n\n.\n\nd\n\n\u00fd \u00fd i j\n\n\u2432 \u2338 \u2432 y \u2338 \u2432\n\u017e d.\ni j\n\n\u017ec.\ni j\n\nd\n\nc\n\ni\n\nj\n\n2\n\n.\n\n "}, {"Page_number": 126, "text": "problems\n\n111\n\n3.28 an i = j table has ordered columns and unordered rows. ridits bross\n1958 are data-based column scores. the jth sample ridit is the\naverage cumulative proportion within category j,\n\n.\n\n\u017e\n\njy1\nr s\n\u02c6 \u00fdj\nks1\n\np q\nqk\n\n\u017e\n\n1\n\n/2\n\np .\nqj\n\nthe sample mean ridit in row i is r s \u00fd r p . show that \u00fd p r s\n0.50 and \u00fd p r s 0.50. for ridit analyses, see agresti 1984, secs.\n9.3 and 10.2 , bross 1958 , fleiss 1981, sec. 9.4 , and landis et al.\n. x\n\u017e\n1978 .\n\n\u02c6\niq i\n\nj qj\n\n\u02c6\ni\n\nw\n.\n\n\u02c6\nj\n\n\u02c6\nj\n\nj < i\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\ni\n\nj\n\n2\n\ni j\n\n\u017e\n\n.2\n\n3.29 show that x s n\u00fd\u00fd p y p p\niq qj\n\nrp p . thus, x can be large\nwhen n is large, regardless of whether the association is practically\nimportant. explain why this test, like other tests, simply indicates the\ndegree of evidence against h and does not describe strength of\n\u017e\n\u2018\u2018like fire, the chi-square test is an excellent servant and\nassociation.\na bad master,\u2019\u2019 sir austin bradford hill, proc. roy. soc. med. 58:\n295\u1390300, 1965.\n\niq qj\n\n.\n\n2\n\n0\n\n3.30 for testing h : \u2432 s \u2432 using independent binomial variates y and\n\n0\n\n1\n\n2\n\n1\n\ny with n and n trials, the score statistic is\n2\n\n1\n\n2\n\nz s\n\n' \u017e\n\u2432 1 y \u2432 1rn q 1rn\n\u02c6\n\n\u2432 y \u2432\n\u02c6\n\u02c6\n2\n1\n. \u017e\n\u02c6\n\n1\n\n,\n\n.\n\n2\n\nwhere \u2432s y q y r n q n\nunder h . show that z 2 s x 2.\n\n\u017e\n\n.\n\n\u017e\n\n1\n\n2\n\n1\n\n\u02c6\n0\n\n2\n\n.\n\nis the pooled estimate of \u2432 s \u2432\n\n1\n\n2\n\n3.31 for a 2 = 2 table, consider h : \u2432 s \u242a , \u2432 s \u2432 s \u242a 1 y \u242a , \u2432\n\ns 1 y \u242a .\n.2\na. show that the marginal distributions are identical and that indepen-\n\n11\n\n12\n\n21\n\n22\n\n\u017e\n\n\u017e\n\n.\n\n2\n\n0\n\ndence holds.\n\nb. for a multinomial sample, under h show that \u242as p q p\nq1\nc. explain how to test h . show that df s 2 for the test statistic.\nd. refer to problem 3.3. are larry bird\u2019s pairs of free throws plausibly\n\nr2.\n.\n\n1q\n\n\u02c6\n\n\u017e\n\n0\n\n0\n\nindependent and identically distributed?\n\n3.32 for a 2 = 2 table, show that:\n\na. the four pearson residuals may take different values.\n\n "}, {"Page_number": 127, "text": "112\n\ninference for contingency tables\n\n\u017e\n\nb. all four standardized pearson residuals have the same absolute\n\nvalue. this is sensible, since df s 1.\nc. the square of each standardized pearson residual equals x 2.\nw\nnote: x s n n n y n n\nfor 2 = 2 ta-\nbles. see mirkin 2001 for alternative x formulas for i = j\ntables.\n\n1q 2q q1 q2\n\nr n n\n\u017e\n\nn\n2\n\n.2\n\nn\n\n11\n\n22\n\n12\n\n21\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\nx\n\n2\n\n2\n\n2\n\nw\n\n3.33 for testing independence, show that x f n min i y 1, j y 1 . hence\n.x\nv s x r nmin i y 1, j y 1\n.\nfalls between 0 and 1 cramer 1946 .\nfor 2 = 2 tables, x 2rn is often called phi-squared; it equals goodman\nand kruskal\u2019s tau problem 2.38 . other measures based on x in-\nclude the contingency coefficient x r x q n\n\n2\n.\npearson 1904 .\n\n.x1r2 \u017e\n\n.\n\u00b4\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\nw\n\n2\n\n2\n\n2\n\n3.34 for counts n , the power di\u00aeergence statistic for testing goodness of fit\n\n\u0004\n\n4\n\ni\n\n\u017e\ncressie and read 1984; read and cressie 1988 is\n\n.\n\n2\n\n\u242d \u242dq 1\n\u017e\n\n\u00fd i\n\nn\n\n\u017e\n\n.\n\nn r\u242e y 1\n\n.\u02c6\n\n\u242d\n\ni\n\ni\n\nfor y\u2b01 - \u242d- \u2b01.\n\nt y 1 rh.\n\u017e h\n\na. for \u242ds 1, show that this equals x 2.\nb. as \u242d\u2122 0, show that it converges to g . hint:\nlog t s lim h\u2122 0\nc. as \u242d\u2122 y1, show that it converges to 2\u00fd \u242e log \u242ern , the mini-\nmum discrimination information statistic gokhale and kullback\n.\n1978 .\n\n\u02c6\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nx\n\nw\n\n2\n\ni\n\ni\n\ni\n\nd. for \u242ds y2, show that\n\nit equals \u00fd n y \u242e rn , the neyman\n\n\u017e\n\ni\n\ni\n\n.\nmodified chi-squared statistic neyman 1949 .\n\ne. for \u242d s y ,\n\n1\n2\n\ni\n.\nfreeman\u1390tukey statistic freeman and tukey 1950 .\n\n\u017e\n\nit equals 4\u00fd n y \u242e ,\n.\n\n\u017e\n\n2\n\nthe\n\n'\n\n\u02c6\n\ni\n\n\u017e\nshow that\n\n\u02c6\n\ni\n\n.2\n'\n\nwunder regularity conditions, their asymptotic distributions are identi-\ncal see drost et al. 1989 . the chi-squared null approximation works\nbest for \u242d near\n\n.\n\n\u017e\n\n2 x\n.3\n\n3.35 use a partitioning argument to explain why g2 for testing indepen-\ndence cannot increase after combining two rows or two columns of a\ncontingency table. hint: argue that g for full table s g for col-\nlapsed table q g2 for table of the two rows that are combined in the\ncollapsed table.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2\n\n2\n\n "}, {"Page_number": 128, "text": "problems\n\n113\n\n3.36 motivate partitioning 3.14 by showing that the multiple hypergeomet-\nfactors as the product of hypergeometric\n\n.\nric distribution 3.19 for n\n.\ndistributions for the separate component tables lancaster, 1949 .\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u0004\n\n4\n\ni j\n\n3.37 explain why n\n\n\u0004\n\n4\n\n.\nare sufficient for \u2432 in 3.17 .\n\n\u017e\n\n\u0004\n\n4\n\nqj\n\nqj\n\n3.38 assume independence, and let p s n rn and \u2432 s p p .\niq qj\n\n\u02c6\n\ni j\na. show that p and \u2432 are unbiased for \u2432 s \u2432 \u2432 .\niq qj\nb. show that var p s \u2432 \u2432 1 y \u2432 \u2432 rn.\nc. using e p p s e p e p\n2 .\n\u017e\nqj\n\n\u017e\niq qj\n2 .\n\u017e\niq\n\niq qj\n\n2 .\niq\n\ni j\n.\n\n\u02c6\n\n\u017e\n\ni j\n\ni j\n\ni j\n\ni j\n\nand e p s var p q\n\n\u017e\n\n.\n\niq\n\n\u017e\n\n.\n\u017e\n.2\n, show that\n\ni j\niq qj\n\nw\n\n\u017e\n\ne p\n\niq\n\n.x2\n\n\u017e\n\n\u02c6\n\n.i j\n\nvar \u2432 s \u2432 \u2432 \u2432 1 y \u2432 q \u2432 1 y \u2432\niq\n\n\u0004\nq \u2432 1 y \u2432 \u2432 1 y \u2432 rn2.\n\niq qj\n\niq\n\nqj\n\nqj\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\niq qj\n\niq\n\nqj\n\n\u017e\n\n.\n\n.\n\n4\n\nn\n\nd. as n \u2122 \u2b01, show that lim var n \u2432 f lim var n p , with equality\nonly if \u2432 s 1 or 0. hence, if the model holds or if it nearly holds,\nthe model estimator is better than the sample proportion.\n\n'\n\u017e\n\n'\n\u017e\n\n\u02c6i j\n\n.\n\n.\n\ni j\n\ni j\n\n3.39 show that the sample value of the uncertainty coefficient 2.13 satis-\n. w\n. haberman 1982 gave its standard\n\nlog p\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nqj\n\nfies u s yg r2 n \u00fd p\n\u02c6\nqj\nx\nerror.\n\n\u017e\n\n2\n\n3.40 when a test statistic has a continuous distribution, the p-value has a\nnull uniform distribution, p p-value f \u2423 s \u2423 for 0 - \u2423- 1. for\nfisher\u2019s exact test, explain why under the null, p p-value f \u2423 f \u2423\nhint: p p-value f \u2423 s e p p-value f\nfor\n.x .\n<\n\u2423 n , n , n .\n\n0 - \u2423 - 1.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\nw\n\n1q q1\n\n.\n\n3.41 refer to note 3.3 about moments of the hypergeometric distribution\n3.16 . letting \u2433s n rn, show that n\n\u017e\nhas the same mean as a\ntrials with success probability \u2433, and\nbinomial random variable for n\nthat it has its variance multiplied by a finite population correction\nfactor n y n\nr n y 1 . the hypergeometric is similar to the bino-\n.\n\u017e\n1q\nis small compared to n.\nmial when n\n1q\n\n. \u017e\n\nq1\n\n1q\n\n11\n\n.\n\n\u017e\n\n3.42 a contingency table for two independent binomial variables has counts\n3, 0 r 0, 3 by row. for h : \u2432 s \u2432 and h : \u2432 ) \u2432 , show that the\n\u017e\np-value equals\nfor fisher\u2019s\n\n2\nfor the exact unconditional test and\n\n.\n\na\n\n0\n\n1\n\n2\n\n1\n\n1\n64\n\n1\n20\n\n "}, {"Page_number": 129, "text": "114\n\ninference for contingency tables\n\nw\n\ntest. for discussion of this example, see little\n\n\u017e\nexact\n1989 , g.\n\u017e\nbarnard\u2019s remarks at the end of yates 1984 , and sprott 2000, sec.\n. x\n6.4.4 .\n\n\u017e\n\n.\n\n.\n\n3.43 refer to problem 3.42 and exact tests using x 2 with h : \u2432 / \u2432 .\n2\nexplain why the unconditional p-value, evaluated at \u2432s 0.5, is related\nto fisher conditional p-values for various tables by\n\na\n\n1\n\np x g 6 s\n\u017e\n\n.\n\n2\n\n6\n\n\u00fd\nks0\n\np x g 6 n s k p n s k .\n\u017e\n.\n\n.\n\n\u017e\n\nq1\n\nq1\n\n2\n\n<\n\nthus, the unconditional p-value of\nis a weighted average of the\nfisher p-value for the observed column margins and p-values of 0\ncorresponding to the impossibility of getting results as extreme as\nobserved if other margins had occurred i.e., s 0.10\n.\n.\nthe fisher quote in section 3.5.6 gave his view about this.\n\n1r2\n\n/3\n\n1\n32\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n6\n\n6\n\n1\n32\n\n3.44 consider exact tests of independence, given the marginals, for the\ni = i table having n s 1 for i s 1, . . . , i, and n s 0 otherwise.\nshow that a tests that order tables by their probabilities, x , or g\nhave p-value s 1.0, and b the one-sided test that orders tables by an\nordinal statistic such as r or c y d has p-value s 1ri! .\n.\n\n\u017e .\n\n\u017e .\n\n\u017e\n\ni j\n\nii\n\n2\n\n2\n\n3.45 a monte carlo scheme randomly samples m separate i = j tables\nhaving the observed margins to approximate p s p x g x for an\nexact test. let p be the sample proportion of the m tables with\nx g x . show that p p y p f b s 1 y \u2423 requires that m f\n2\nz\n\u2423r2\n\n\u02c6\np 1 y p rb .\n\n\u02c6\u017e<\n\n2.\no\n\n2\no\n\n.\n\n.\n\n\u017e\n\n\u017e\n\no\n\no\n\n2\n\n2\n\n2\n\no\n\no\n\n<\n\n3.46 show that the conditional ml estimate of \u242a satisfies n s e n\n\u017e\n\n11\n\n.\ndistribution 3.18 .\n\n\u017e\n\n.\n\n11\n\nfor\n\n "}, {"Page_number": 130, "text": "c h a p t e r 4\n\nintroduction to generalized\nlinear models\n\nin chapters 2 and 3 we focused on methods for two-way contingency tables.\nmost studies, however, have several explanatory variables, and they may be\ncontinuous as well as categorical. the goal is usually to describe their effects\non response variables. modeling the effects helps us do this efficiently. a\ngood-fitting model evaluates effects, includes relevant interactions, and pro-\nvides smoothed estimates of response probabilities.\n\nthe rest of the book focuses on model building for categorical response\nvariables. in this chapter we introduce a family of generalized linear models\nthat contains the most important models for categorical responses as well as\nstandard models for continuous responses. section 4.1 covers three compo-\nnents common to all generalized linear models. section 4.2 illustrates with\nmodels for binary responses. the most important case is logistic regression, a\nlinear model for the logit transformation of a binomial parameter. in chap-\nters 5 through 7 we study these models in detail.\n\nin section 4.3 we present generalized linear models for counts. a poisson\nregression model called a loglinear model is a linear model for the log of a\npoisson mean. in chapters 8 and 9 we study them for modeling counts in\ncontingency tables.\n\nsections 4.4 through 4.8 are more technical. readers wanting mainly an\noverview of methods can skip them or read them lightly. for generalized\nlinear models, section 4.4 covers likelihood equations and the asymptotic\ncovariance matrix of ml model parameter estimates, and section 4.5 summa-\nrizes inferential methods. methods of solving the likelihood equations are\npresented in section 4.6. in the final two sections we introduce generaliza-\ntions, quasi-likelihood and generalized additi\u00aee models, that further extend the\nscope of models.\n\n115\n\n "}, {"Page_number": 131, "text": "116\n\nintroduction to generalized linear models\n\n4.1 generalized linear model\n\n\u017e\n\n.\n\ngeneralized linear models glms extend ordinary regression models to\nencompass nonnormal response distributions and modeling functions of the\nmean. three components specify a generalized linear model: a random\ncomponent identifies the response variable y and its probability distribution;\na systematic component specifies explanatory variables used in a linear\npredictor function; and a link function specifies the function of e y that the\n.\nmodel equates to the systematic component. nelder and wedderburn 1972\nintroduced the class of glms, although many models in the class were well\nestablished by then.\n\n\u017e\n\n.\n\n\u017e\n\n4.1.1 components of generalized linear models\nthe random component of a glm consists of a response variable y with\nindependent observations\nfrom a distribution in the natural\nexponential family. this family has probability density function or mass\nfunction of form\n\ny , . . . , y\n1\n\n\u017e\n\n.\n\nn\n\nf y ; \u242a s a \u242a b y exp y q \u242a .\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u017e\n\n4.1\n\n.\n\nseveral important distributions are special cases, including the poisson and\nbinomial. the value of the parameter \u242a may vary for i s 1, . . . , n, depend-\ning on values of explanatory variables. the term q \u242a is called the natural\nparameter. in section 4.4 we present a more general formula that also has a\ndispersion parameter, but 4.1 is sufficient for basic discrete data models.\n\nthe systematic component of a glm relates a vector \u2429 , . . . , \u2429 to the\ndenote the value of\n\nexplanatory variables through a linear model. let x\npredictor j\n\nj s 1, 2, . . . , p for subject i. then\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nn\n\ni j\n\n1\n\ni\n\n\u2429 s \u2424 x ,\n\u00fdi\n\ni j\n\nj\n\nj\n\ni s 1, . . . , n.\n\ni j\n\n.\n\nthis linear combination of explanatory variables is called the linear predictor.\nusually, one x s 1 for all\ni, for the coefficient of an intercept often\ndenoted by \u2423 in the model.\nthe third component of a glm is a link function that connects the\nrandom and systematic components. let \u242e s e y , i s 1, . . . , n. the model\nlinks \u242e to \u2429 by \u2429 s g \u242e , where the link function g is a monotonic,\ndifferentiable function. thus, g links e y to explanatory variables through\nthe formula\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ng \u242e s \u2424 x ,\n\u017e\n\n\u00fd\n\n.\n\ni j\n\ni\n\nj\n\nj\n\ni s 1, . . . , n.\n\n\u017e\n\n4.2\n\n.\n\n "}, {"Page_number": 132, "text": ".\n\n\u017e\n\ngeneralized linear model\n\n117\nthe link function g \u242e s \u242e, called the identity link, has \u2429 s \u242e. it\nspecifies a linear model for the mean itself. this is the link function for\nordinary regression with normally distributed y. the link function that\ntransforms the mean to the natural parameter is called the canonical link.\nfor it, g \u242e s q \u242a , and q \u242a s \u00fd \u2424 x . the following subsections show\n\u017e\nexamples.\n\nin summary, a glm is a linear model for a transformed mean of a\nresponse variable that has distribution in the natural exponential family. we\nnow illustrate the three components by introducing the key glms for\ndiscrete response variables.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\ni j\n\ni\n\ni\n\ni\n\ni\n\ni\n\nj\n\nj\n\n4.1.2 binomial logit models for binary data\nthe success and failure\nmany response variables are binary. represent\noutcomes by 1 and 0. the bernoulli distribution for this bernoulli\ntrial\nspecifies probabilities p y s 1 s \u2432 and p y s 0 s 1 y \u2432,\nfor which\ne y s \u2432. this is the special case of the binomial 1.1 with n s 1. the\n\u017e\nprobability mass function is\n\n.\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\nf y; \u2432 s \u2432 1 y \u2432\n\u017e\n\n.\n\n\u017e\n\ny\n\n1yy\n\n.\n\ns 1 y \u2432 \u2432r 1 y \u2432\n\n\u017e\n\n.\n\n\u017e\n\ns 1 y \u2432 exp y log\n\n\u017e\n\n.\n\n\u017e\n\n\u2432\n\n1 y \u2432\n\n/\n\ny\n\n.\n\n\u017e\n\n4.3\n\n.\n\nfor y s 0 and 1. this is in the natural exponential family 4.1 , identifying \u242a\nwith \u2432, a \u2432 s 1 y \u2432, b y s 1, and q \u2432 s log \u2432r 1 y \u2432 . the natural\nparameter log \u2432r 1 y \u2432 is the log odds of response 1, the logit of \u2432. this\nis the canonical link. glms using the logit link are often called logit models.\n\n\u017e\n.x\n\n.\n.x\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\nw\n\nw\n\n4.1.3 poisson loglinear models for count data\nsome response variables have counts as their possible outcomes. for a\nsample of silicon wafers used in manufacturing computer chips, each obser-\nvation might be the number of imperfections on a wafer. counts also occur as\nentries in contingency tables.\n\nthe simplest distribution for count data is the poisson. like counts,\npoisson variates can take any nonnegative integer value. let y denote a\ncount and let \u242es e y . the poisson probability mass function 1.4 for y is\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nf y; \u242e s\n\u017e\n\n.\n\ney\u242e\u242ey\n\ny!\n\ns exp y\u242e\n.\n\n\u017e\n\n\u017e\n\n/\n\n1\ny!\n\n.\nexp y log \u242e ,\n\n\u017e\n\ny s 0, 1, 2, . . . .\n\nthis has natural exponential form 4.1 with \u242as \u242e, a \u242e s exp y\u242e , b y s\n1ry!, and q \u242e s log \u242e. the natural parameter is log \u242e, so the canonical\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 133, "text": "118\n\nintroduction to generalized linear models\n\ntable 4.1 types of generalized linear models for statistical analysis\n\nrandom\ncomponent\nnormal\nnormal\nnormal\nbinomial\npoisson\nmultinomial\n\nsystematic\ncomponent\nlink\ncontinuous\nidentity\ncategorical\nidentity\nmixed\nidentity\nmixed\nlogit\nlog\nmixed\ngeneralized mixed\n\nlogit\n\nmodel\n\nchapters\n\nregression\nanalysis of variance\nanalysis of covariance\nlogistic regression\nloglinear\nmultinomial response\n\n5 and 6\n8 and 9\n7\n\nlink function is the log link, \u2429s log \u242e. the model using this link is\n\nlog \u242e s \u2424 x ,\n\n\u00fdi\n\ni j\n\nj\n\nj\n\ni s 1, . . . , n.\n\n4.4\u017e\n\n.\n\nthis model is called a poisson loglinear model.\n\n.\n\n\u017e\n\n4.1.4 generalized linear models for continuous responses\nthe class of glms also includes models for continuous responses. the\nnormal distribution is in a natural exponential family that includes dispersion\nparameters. its natural parameter is the mean. therefore, an ordinary\nregression model for e y is a glm using the identity link. table 4.1 lists\nthis and other standard models for a normal random component. the table\nalso lists glms for discrete responses that are presented in the next six\nchapters.\n\na traditional way to analyze data transforms y so that it has approxi-\nmately a normal distribution with constant variance; then, ordinary least-\nsquares regression is applicable. with glms, by contrast, the choice of link\nfunction is separate from the choice of random component. if a link is useful\nin the sense that a linear model for the predictors is plausible for that link, it\nis not necessary that it also stabilizes variance or produces normality. this is\nbecause the fitting process maximizes the likelihood for the choice of distri-\nbution for y, and that choice is not restricted to normality.\n\n4.1.5 deviance\nfor a particular glm for observations y s y , . . . , y\n.\n, let l \u242e; y denote\nthe log-likelihood function expressed in terms of the means \u242e s \u242e , . . . , \u242e .\n.\nlet l \u242e; y denote the maximum of the log likelihood for the model.\nconsidered for all possible models, the maximum achievable log likelihood is\n\n\u017e\n\u017e\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\n.\n\nn\n\nn\n\n1\n\n1\n\n "}, {"Page_number": 134, "text": "generalized linear model\n\n119\n\n.\n\n\u017e\nl y; y . this occurs for the most general model, having a separate parameter\nfor each observation and the perfect fit \u242e s y. such a model is called the\nsaturated model. this model is not useful, since it does not provide data\nreduction. however, it serves as a baseline for comparison with other model\nfits.\n\nthe de\u00aeiance of a poisson or binomial glm is defined to be\n\n\u02c6\n\ny2 l \u242e; y y l y; y\n\n\u017e\n\n.\n\n\u017e\n\n\u02c6\n\n.\n\n.\n\n.\n\n\u017e\n\nthis is the likelihood-ratio statistic for testing the null hypothesis that the\nmodel holds against the general alternative i.e., the saturated model . for\nsome poisson and binomial glms, the number of observations n stays fixed\nas the individual counts increase in size. then the deviance has a chi-squared\nasymptotic null distribution. the df s n y p, where p is the number of\nmodel parameters; that is, df equals the difference between the numbers of\nparameters in the saturated and unsaturated models. the deviance then\nprovides a test of model fit.\nan example is binomial counts at n fixed settings of predictors when the\nnumber of trials at each setting increases. let y be bin n , \u2432 , i s 1, . . . ,\n\u017e\nn. consider the simple model of homogeneity, \u2432 s \u2423 all\ni. it has p s 1\nparameter. the saturated model makes no assumption about \u2432 , letting\nthem be any n values between 0 and 1.0. it has n parameters. the deviance\nfor the homogeneity model has df s n y 1. in fact,\nit equals the g2\nlikelihood-ratio statistic 3.11 for testing independence in the n = 2 table\nthat these samples form. under independence, it has approximately a chi-\nsquared distribution as the n increase, for fixed n.\n\nwe use the deviance throughout the book for model checking and for\ninferential comparisons of models. components of the deviance are residual\nmeasures of lack of fit. methods for analyzing the deviance generalize\nanalysis of variance methods for normal linear models.\n\n\u017e\n\n.\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n4.1.6 advantages of the glm formulation\nglms provide a unified theory of modeling that encompasses the most\nimportant models for continuous and discrete variables. models studied in\nthis text are glms with binomial or poisson random component, or multi-\nvariate extensions of glms. the ml parameter estimates are computed with\nan algorithm, presented in section 4.6, that iteratively uses a weighted\nversion of least squares. the reason for restricting glms to the exponential\nfamily of distributions for y is that the same algorithm applies to this entire\nfamily, for any choice of link function.\n\nmost statistical software has the facility to fit glms. appendix a gives\n\ndetails.\n\n "}, {"Page_number": 135, "text": "120\n\nintroduction to generalized linear models\n\n4.2 generalized linear models for binary data\n\n.\n\n\u017e\n\nlet y denote a binary response variable. for instance, y might indicate vote\nin a british election labour, conservative , choice of automobile domestic,\nimport , or diagnosis of breast cancer present, absent . each observation has\none of two outcomes, denoted by 0 and 1, binomial for a single trial. the\nmean e y s p y s 1 . we denote p y s 1 by \u2432 x , reflecting its depen-\ndence on values x s x , . . . , x\n\nof predictors. the variance of y is\n\n\u017e .\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n1\n\np\n\nvar y s \u2432 x 1 y \u2432 x\n\u017e .\n\n\u017e .\n\n\u017e\n\n.\n\n,\n\nthe binomial variance for one trial. in introducing glms for binary data, for\nsimplicity we use a single explanatory variable.\n\n4.2.1 linear probability model\nfor a binary response, the regression model\n\n\u2432 x s \u2423q \u2424x\n\n\u017e\n\n.\n\n\u017e\n\n4.5\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nis called a linear probability model. with independent observations it is a\nglm with binomial random component and identity link function.\n\nthe linear probability model has a major structural defect. probabilities\nfall between 0 and 1, but linear functions take values over the entire real line.\nmodel 4.5 has \u2432 x - 0 and \u2432 x ) 1 for sufficiently large or small x\nvalues. for its extension with multiple predictors, difficulties often occur\nfitting this model because during the fitting process, \u2432 x falls outside the\nw\nx0, 1 range for some subjects\u2019 x values. the model can be valid over a\nrestricted range of x values. when it is plausible, an advantage is its simple\ninterpretation: \u2424 is the change in \u2432 x for a one-unit increase in x.\n\n\u017e .\n\n\u02c6\n\n.w\n\nwe defer to section 4.6 the technical details of fitting this and other\nglms. one should assume a binomial distribution for y and use maximum\nlikelihood ml rather than ordinary least squares. least squares is ml for a\nnormal distribution with constant variance. for binary responses, the con-\n\u017e\ni.e.,\nstant variance condition that makes least squares estimators optimal\nminimum variance in the class of linear unbiased estimators is not satisfied.\nsince var y s \u2432 x 1 y \u2432 x , the variance depends on x through its\ninfluence on \u2432 x . as \u2432 x moves toward 0 or 1, the distribution of y is\nmore nearly concentrated at a single point, and the variance moves toward 0.\nbecause of the nonconstant variance, the binomial ml estimator is more\nefficient than least squares. also y, being binary, is very far from normally\ndistributed. thus, the usual sampling distributions for the least squares\nestimators do not apply. the estimates and standard errors for ml and least\nsquares are usually similar, however, when \u2432 x for the sample x values falls\n.\nin the range within which the variance is relatively stable about 0.3 to 0.7 .\n\n.x\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n "}, {"Page_number": 136, "text": "generalized linear models for binary data\n\n121\n\ntable 4.2 relationship between snoring and heart disease\n\nproportion\n\nheart disease\nno\nyes\n1355\n24\n35\n603\n192\n21\n30\n224\n\nsnoring\nnever\noccasionally\nnearly every night\nevery night\namodel fits refer to proportion of yes responses.\nsource: p. g. norton and e. v. dunn, british med. j. 291: 630\u1390632 1985 , bmj publishing\ngroup.\n\nyes\n0.017\n0.055\n0.099\n0.118\n\n\u017e\n\n.\n\nlinear\na\nfit\n0.017\n0.057\n0.096\n0.116\n\nlogit\na\nfit\n0.021\n0.044\n0.093\n0.132\n\n.\n\n\u017e\n\u017e\n\n4.2.2 snoring and heart disease example\nwe illustrate the linear probability model with table 4.2, from an epidemio-\nlogical survey of 2484 subjects to investigate snoring as a risk factor for heart\ndisease. those surveyed were classified according to their spouses\u2019 report of\nhow much they snored. the model states that the probability of heart disease\nis linearly related to the level of snoring x. we treat the rows of the table as\nindependent binomial samples. no obvious choice of scores exists for cate-\ngories of x. we used 0, 2, 4, 5 , treating the last two levels as closer than the\nother adjacent pairs problem 4.4 uses equally spaced scores . ml estimates\nand standard errors are the same if we use a data file of 2484 binary\nobservations or if we enter the four binomial totals of yes and no responses\nlisted in table 4.2.\nsoftware see, e.g., table a.3 for sas reports the ml fit, \u2432 x s 0.0172\nq 0.0198 x, with a standard error se s 0.0028 for \u2424s 0.0198. for nonsnor-\nx s 0 , the estimated proportion of subjects having heart disease is\ners\n0.0172. we refer to the estimated values of e y for a glm as fitted \u00aealues.\ntable 4.2 shows the sample proportions and the fitted values for this model.\nfigure 4.1 graphs the sample and fitted values. the table and graph suggest\n\u017e\nthat the model fits well.\nin section 5.2.3 we discuss formal goodness-of-fit\nanalyses for binary-response glms. the model interpretation is simple. the\nestimated probability of heart disease is about 0.02 for nonsnorers; it in-\ncreases 2 0.0198 s 0.04 for occasional snorers, another 0.04 for those who\nsnore nearly every night, and another 0.02 for those who always snore.\n\n\u02c6\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n4.2.3 logistic regression model\nusually, binary data result from a nonlinear relationship between \u2432 x and\nx. a fixed change in x often has less impact when \u2432 x is near 0 or 1 than\nwhen \u2432 x is near 0.5. in the purchase of an automobile, consider the choice\nbetween buying new or used. let \u2432 x denote the probability of selecting\nnew when annual family income s x. an increase of $50,000 in annual\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 137, "text": "122\n\nintroduction to generalized linear models\n\nfigure 4.1 predicted probabilities for linear probability and logistic regression models.\n\nw\n\nx\nincome would have less effect when x s $1,000,000 for which \u2432 x is near 1\nthan when x s $50,000.\n\nin practice, nonlinear relationships between \u2432 x and x are often mono-\ntonic, with \u2432 x increasing continuously or \u2432 x decreasing continuously as\nx increases. the s-shaped curves in figure 4.2 are typical. the most impor-\ntant curve with this shape has the model formula\nexp \u2423q \u2424x\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u2432 x s\n\n\u017e\n\n.\n\n\u017e\n\n.\n1 q exp \u2423q \u2424x\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n4.6\n\n.\n\nthis is the logistic regression model. as x \u2122 \u2b01, \u2432 x x0 when \u2424- 0 and\n.\u2432 x \u22601 when \u2424) 0.\n\u017e\n\n\u017e\n\n.\n\nlet\u2019s find the link function for which logistic regression is a glm. for\n\n\u017e\n.4.6 the odds are\n\n\u2432 x\u017e\n\n1 y \u2432 x\u017e\n\n. s exp \u2423q \u2424x\n.\n\n\u017e\n\n.\n\n.\n\nthe log odds has the linear relationship\n\nlog\n\n\u2432 x\u017e\n\n.\n1 y \u2432 x\u017e\n\n.\n\ns \u2423q \u2424x.\n\n4.7\u017e\n\n.\n\n "}, {"Page_number": 138, "text": "generalized linear models for binary data\n\n123\n\nfigure 4.2 logistic regression functions.\n\nthus, the appropriate link is the log odds transformation, the logit. logistic\nregression models are glms with binomial random component and logit link\nfunction. logistic regression models are also called logit models.\n\nthe logit is the natural parameter of the binomial distribution, so the logit\nlink is its canonical link. whereas \u2432 x must fall in the 0, 1 range, the logit\ncan be any real number. the real numbers are also the range for linear\npredictors such as \u2423q \u2424x that form the systematic component of a glm.\nso this model does not have the structural problem that is true of the linear\nprobability model.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nfor the snoring data in table 4.2, software reports the logistic regression\n\nml fit\n\nlogit \u2432 x s y3.87 q 0.40 x.\n\n\u017e\n\n.\n\n\u02c6\n\n\u02c6\n\nthe positive \u2424s 0.40 reflects the increased incidence of heart disease at\nhigher snoring levels. in chapters 5 and 6 we study logistic regression in\ndetail and interpret such equations. estimated probabilities result from\nsubstituting x values into the estimate of probability formula 4.6 . table 4.2\nalso reports these fitted values. figure 4.1 displays the fit. the fit is close to\nlinear over this narrow range of estimated probabilities, and results are\nsimilar to those for the linear probability model.\n\n\u017e\n\n.\n\n "}, {"Page_number": 139, "text": "124\n\nintroduction to generalized linear models\n\n4.2.4 binomial glm for 2 = 2 contingency tables\namong the simplest glms for a binary response is the one having a single\nexplanatory variable x that is also binary. label its values by 0 and 1. for a\ngiven link function, the glm\n\nlink \u2432 x s \u2423q \u2424x\n\n\u017e\n\n.\n\nhas the effect of x described by\n\n\u2424s link \u2432 1 y link \u2432 0\n\u017e .\n\n\u017e .\n\n.\n\nfor the identity link, \u2424s \u2432 1 y \u2432 0 is the difference between propor-\nis the\n\n\u017e .\n\u017e .x\nw\ntions. for the log link, \u2424s log \u2432 1 y log \u2432 0 s log \u2432 1 r\u2432 0\nlog relative risk. for the logit link,\n\n\u017e .x\n\n\u017e .x\n\n\u017e .\n\n\u017e .\n\nw\n\nw\n\n\u2424s logit \u2432 1 y logit \u2432 0 s log\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\u2432 1\n1 y \u2432 1\n\u017e .\n\ny log\n\n\u017e .\n\u2432 0\n1 y \u2432 0\n\u017e .\n\n\u2432 1 r 1 y \u2432 1\n\u017e .\ns log \u2432 0 r 1 y \u2432 0\n\u017e .\n\n\u017e .\n\u017e .\n\n\u017e\n\u017e\n\n.\n.\n\nis the log odds ratio. measures of association for 2 = 2 tables are effect\nparameters in glms for binary data.\n\n.\n\n\u017e\n\n4.2.5 probit and inverse cdf link functions*\na monotone regression curve such as the first one in figure 4.2 has the shape\nfor a continuous random variable.\nof a cumulative distribution function cdf\nthis suggests a model for a binary response having form \u2432 x s f x\n\u017e\nfor\nsome cdf f.\nusing an entire class of location-scale cdf\u2019s, such as normal cdf\u2019s with their\nvariety of means and variances, permits the curve \u2432 x s f x\n\u017e\nto have\nflexibility in the rate of increase and in the location where most of that\nincrease occurs. let \u233d \u2b48 denote the standard cdf of the class, such as the\nn 0, 1 cdf. using \u233d but writing the model as\n\u2432 x s \u233d \u2423q \u2424x\n\n4.8\n\n\u017e .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nprovides the same flexibility. shapes of different cdf\u2019s in the class occur as \u2423\nand \u2424 vary. replacing x by \u2424x permits the curve to increase at a different\n\u017e\nrate than the standard cdf or even to decrease if \u2424- 0 ; varying \u2423 moves\nthe curve to the left or right.\n\n.\n\nwhen \u233d is strictly increasing over the entire real line, its inverse function\ny1\u017e .\n\n.\n\n\u017e\n\n\u233d \u2b48 exists and 4.8 is, equivalently,\n.\n\n\u017e\n\ny1\u233d \u2432 x s \u2423q \u2424x .\n\n\u017e\n\n4.9\n\n.\n\n "}, {"Page_number": 140, "text": "generalized linear models for counts\n\n125\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nfor this class of cdf shapes, the link function for the glm is \u233dy1. the link\nfunction maps the 0, 1 range of probabilities onto y\u2b01, \u2b01 , the range of\nlinear predictors. the curve has the shape of a normal cdf when \u233d is the\nstandard normal cdf. model 4.9 is then called the probit model. this curve\nhas similar appearance to the logistic regression curve. probit models are\ndiscussed in section 6.6.\nwhen \u2424) 0, the logistic regression curve 4.6 is a cdf for the logistic\ndistribution. when \u2424- 0, the curve for 1 y \u2432 x , the probability y s 0, has\nthat appearance. the cdf of the logistic distribution with mean \u242e and\ndispersion parameter \u2436) 0 is\n\u017e\n1 q exp\n\nx y \u242e r\u2436\n\u017e\n.\n\ny\u2b01 - x - \u2b01.\n\nx y \u242e r\u2436\n\nf x s\n\u017e\n\nexp\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n,\n\n' \u017e\n\nthe corresponding probability density function is symmetric and bell-shaped,\nwith standard deviation \u2436\u2432r 3\nhere, \u2432 is the mathematical constant\n.\n3.14 . . .\n. it looks much like the normal density with the same mean and\nstandard deviation but with slightly thicker tails. its kurtosis equals that of a\nt distribution with df s 9.\nthe standardized form of the logistic cdf has \u242es 0 and \u2436s 1, so\n\u233d x s e r 1 q e . for that function, the logistic regression curve 4.6 has\n.\n\u017e\nform \u2432 x s \u233d \u2423q \u2424x . by 4.9 the logit transformation is simply the\ninverse function for the standard logistic cdf; that is, when \u233d x s \u2432 x s\ne r 1 q e , then x s \u233d \u2432 x s log \u2432 x r 1 y \u2432 x\n\u017e\n\n\u017e\n.x\n\n..x\n.\n\ny1w\n\nx.\n\nx.\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nw\n\nx\n\nx\n\n4.3 generalized linear models for counts\n\nthe best known glms for count data assume a poisson distribution for y.\nwe introduced this distribution in section 1.2.3. in chapters 8 and 9 we\npresent poisson glms for counts in contingency tables with categorical\nresponse variables. in this section we introduce poisson glms using an\nalternative application: modeling count or rate data for a single discrete\nresponse variable.\n\n4.3.1 poisson loglinear models\nthe poisson distribution has a positive mean \u242e. although a glm can model\na positive mean using the identity link, it is more common to model the log of\nthe mean. like the linear predictor \u2423q \u2424x, the log mean can take any real\nvalue. the log mean is the natural parameter for the poisson distribution,\nand the log link is the canonical link for a poisson glm. a poisson loglinear\nglm assumes a poisson distribution for y and uses the log link.\n\nthe poisson loglinear model with explanatory variable x is\n\nlog \u242es \u2423q \u2424x .\n\n\u017e\n\n4.10\n\n.\n\n "}, {"Page_number": 141, "text": "126\n\nintroduction to generalized linear models\n\nfor this model, the mean satisfies the exponential relationship\n\n\u242es exp \u2423q \u2424x s e\n\n\u017e\n\n.\n\n\u2423\n\n\u017e\n\ne\n\n\u2424\n\n.\n\nx\n\n.\n\n\u017e\n\n4.11\n\n.\n\na 1-unit increase in x has a multiplicative impact of e \u2424 on \u242e: the mean at\nx q 1 equals the mean at x multiplied by e \u2424.\n\n4.3.2 horseshoe crab mating example\nwe illustrate poisson glms for table 4.3 from a study of nesting horseshoe\ncrabs. each female horseshoe crab had a male crab resident in her nest. the\nstudy investigated factors affecting whether the female crab had any other\nmales, called satellites, residing nearby. explanatory variables are the female\ncrab\u2019s color, spine condition, weight, and carapace width. the response\noutcome for each female crab is her number of satellites. for now, we use\nwidth alone as a predictor. table 4.3 lists width in centimeters. the sample\nmean width equals 26.3 and the standard deviation equals 2.1.\n\nfigure 4.3 plots the response counts of satellites against width, with\nnumbered symbols indicating the number of observations at each point. the\nsubstantial variability makes it difficult to discern a clear trend. to get a\nclearer picture, we grouped the female crabs into width categories f 23.25,\n23.25\u139024.25, 24.25\u139025.25, 25.25\u139026.25, 26.25\u139027.25, 27.25\u139028.25, 28.25\u139029.25,\n) 29.25 and calculated the sample mean number of satellites for female\ncrabs in each category. figure 4.4 plots these sample means against the\nsample mean width for crabs in each category.\n\nmore sophisticated ways of portraying the trend smooth the data without\ngrouping the width values or assuming a particular functional relationship.\nfigure 4.4 also shows a smoothed curve based on an extension of the glm\nintroduced in section 4.8. the sample means and the smoothed curve both\nshow a strong increasing trend. the means tend to fall above the curve,\nsince the response counts in a category tend to be skewed to the right; the\nsmoothed curve is less susceptible to outlying observations. the trend seems\napproximately linear, and we discuss next models for the ungrouped data for\nwhich the mean or the log of the mean is linear in width.\nlet \u242e be the expected number of satellites and\nx s width. from glm software e.g., for sas, see table a.4 , the ml fit of\nthe poisson loglinear model 4.10 is\n\nfor a female crab,\n\n\u017e\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\nlog \u242es \u2423q \u2424x s y3.305 q 0.164 x.\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\nthe effect \u2424s 0.164 of width is positive, with se s 0.020. the model fitted\nvalue at any width level is an estimated mean number of satellites \u242e. for\ninstance, the fitted value at the mean width of x s 26.3 is\n\n\u02c6\n\n\u242es exp \u2423q \u2424x s exp y3.305 q 0.164 26.3 s 2.74.\n\u02c6\n\n\u02c6\u017e\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 142, "text": "generalized linear models for counts\n\n127\n\ntable 4.3 number of crab satellites by female\u2019s characteristicsa\n\nc s w wt\n\nsa c s w wt\n\nsa c s w wt\n\nsa c s w wt\n\nsa\n\n8\n2 3 28.3 3.05\n4\n3 3 26.0 2.60\n0\n3 3 25.6 2.15\n0\n4 2 21.0 1.85\n1\n2 3 29.0 3.00\n3\n1 2 25.0 2.30\n0\n4 3 26.2 1.30\n0\n2 3 24.9 2.10\n8\n2 1 25.7 2.00\n6\n2 3 27.5 3.15\n5\n1 1 26.1 2.80\n4\n3 3 28.9 2.80\n3\n2 1 30.3 3.60\n4\n2 3 22.9 1.60\n3\n3 3 26.2 2.30\n5\n3 3 24.5 2.05\n8\n2 3 30.0 3.05\n3\n2 3 26.2 2.40\n6\n2 3 25.4 2.25\n4\n2 3 25.4 2.25\n0\n4 3 27.5 2.90\n3\n4 3 27.0 2.25\n0\n2 2 24.0 1.70\n0\n2 1 28.7 3.20\n1\n3 3 26.5 1.97\n1\n2 3 24.5 1.60\n1\n3 3 27.3 2.90\n4\n2 3 26.5 2.30\n2\n2 3 25.0 2.10\n0\n3 3 22.0 1.40\n2\n1 1 30.2 3.28\n0\n2 2 25.4 2.30\n2 1 24.9 2.30\n6\n4 3 25.8 2.25 10\n5\n3 3 27.2 2.40\n3\n2 3 30.5 3.32\n8\n4 3 25.0 2.10\n9\n2 3 30.0 3.00\n2 1 22.9 1.60\n0\n2\n2 3 23.9 1.85\n3\n2 3 26.0 2.28\n0\n2 3 25.8 2.20\n4\n3 3 29.0 3.28\n1 1 26.5 2.35\n0\n\n3 3 22.5 1.55\n2 3 23.8 2.10\n3 3 24.3 2.15\n2 1 26.0 2.30\n4 3 24.7 2.20\n2 1 22.5 1.60\n2 3 28.7 3.15\n1 1 29.3 3.20\n2 1 26.7 2.70\n4 3 23.4 1.90\n1 1 27.7 2.50\n2 3 28.2 2.60\n4 3 24.7 2.10\n2 1 25.7 2.00\n2 1 27.8 2.75\n3 1 27.0 2.45\n2 3 29.0 3.20\n3 3 25.6 2.80\n3 3 24.2 1.90\n3 3 25.7 1.20\n3 3 23.1 1.65\n2 3 28.5 3.05\n2 1 29.7 3.85\n3 3 23.1 1.55\n3 3 24.5 2.20\n2 3 27.5 2.55\n2 3 26.3 2.40\n2 3 27.8 3.25\n2 3 31.9 3.33\n2 3 25.0 2.40\n3 3 26.2 2.22\n3 3 28.4 3.20\n1 2 24.5 1.95\n2 3 27.9 3.05\n2 2 25.0 2.25\n3 3 29.0 2.92\n2 1 31.7 3.73\n2 3 27.6 2.85\n4 3 24.5 1.90\n3 3 23.8 1.80\n2 3 28.2 3.05\n3 3 24.1 1.80\n1 1 28.0 2.62\n\n0\n0\n0\n14\n0\n1\n3\n4\n5\n0\n6\n6\n5\n5\n0\n3\n10\n7\n0\n0\n0\n0\n5\n0\n1\n1\n1\n3\n2\n5\n0\n3\n6\n7\n6\n3\n4\n4\n0\n0\n8\n0\n0\n\n9\n1 1 26.0 2.30\n0\n3 2 24.7 1.90\n0\n2 3 25.8 2.65\n8\n1 1 27.1 2.95\n5\n2 3 27.4 2.70\n2\n3 3 26.7 2.60\n5\n2 1 26.8 2.70\n0\n1 3 25.8 2.60\n0\n4 3 23.7 1.85\n6\n2 3 27.9 2.80\n5\n2 1 30.0 3.30\n4\n2 3 25.0 2.10\n2 3 27.7 2.90\n5\n2 3 28.3 3.00 15\n0\n4 3 25.5 2.25\n5\n2 3 26.0 2.15\n0\n2 3 26.2 2.40\n1\n3 3 23.0 1.65\n2 2 22.9 1.60\n0\n5\n2 3 25.1 2.10\n4\n3 1 25.9 2.55\n0\n4 1 25.5 2.75\n0\n2 1 26.8 2.55\n2 1 29.0 2.80\n1\n1\n3 3 28.5 3.00\n4\n2 2 24.7 2.55\n1\n2 3 29.0 3.10\n6\n2 3 27.0 2.50\n4 3 23.7 1.80\n0\n6\n3 3 27.0 2.50\n2\n2 3 24.2 1.65\n4\n4 3 22.5 1.47\n0\n2 3 25.1 1.80\n2 3 24.9 2.20\n0\n6\n2 3 27.5 2.63\n0\n2 1 24.3 2.00\n4\n2 3 29.5 3.02\n0\n2 3 26.2 2.30\n2 3 24.7 1.95\n4\n4\n3 2 29.8 3.50\n0\n4 3 25.7 2.15\n2\n3 3 26.2 2.17\n4 3 27.0 2.63\n0\n\n3 3 24.8 2.10\n2 1 23.7 1.95\n2 3 28.2 3.05\n2 3 25.2 2.00\n2 2 23.2 1.95\n4 3 25.8 2.00\n4 3 27.5 2.60\n2 2 25.7 2.00\n2 3 26.8 2.65\n3 3 27.5 3.10\n3 1 28.5 3.25\n2 3 28.5 3.00\n1 1 27.4 2.70\n2 3 27.2 2.70\n3 3 27.1 2.55\n2 3 28.0 2.80\n2 1 26.5 1.30\n3 3 23.0 1.80\n3 2 26.0 2.20\n3 2 24.5 2.25\n2 3 25.8 2.30\n4 3 23.5 1.90\n4 3 26.7 2.45\n3 3 25.5 2.25\n2 3 28.2 2.87\n2 1 25.2 2.00\n2 3 25.3 1.90\n3 3 25.7 2.10\n4 3 29.3 3.23\n3 3 23.8 1.80\n2 3 27.4 2.90\n2 3 26.2 2.02\n2 1 28.0 2.90\n2 1 28.4 3.10\n2 1 33.5 5.20\n2 3 25.8 2.40\n3 3 24.0 1.90\n2 1 23.1 2.00\n2 3 28.3 3.20\n2 3 26.5 2.35\n2 3 26.5 2.75\n3 3 26.1 2.75\n2 2 24.5 2.00\n\n0\n0\n11\n1\n4\n3\n0\n0\n0\n3\n9\n3\n6\n3\n0\n1\n0\n0\n3\n0\n0\n0\n0\n0\n1\n1\n2\n0\n12\n6\n3\n2\n4\n5\n7\n0\n10\n0\n0\n4\n7\n3\n0\n\n\u017e\n\na\nc, color 1, light medium; 2, medium; 3, dark medium; 4, dark ; s, spine condition 1, both\n.\ngood; 2, one worn or broken; 3, both worn or broken ; w, carapace width cm ; wt, weight kg ;\nsa, number of satellites.\nsource: data courtesy of jane brockmann, zoology department, university of florida; study\n.\ndescribed in ethology 102:1\u139021 1996 .\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n "}, {"Page_number": 143, "text": "128\n\nintroduction to generalized linear models\n\nfigure 4.3 number of satellites by width of female crab.\n\n.\n\n\u02c6\u017e\n\nfor this model, exp \u2424 s exp 0.164 s 1.18 is the multiplicative effect on \u242e\u02c6\nfor a 1-cm increase in x. for instance, the fitted value at x s 27.3 s 26.3 q 1\nis exp y3.305 q 0.164 27.3 s 3.23, which equals 1.18 = 2.74. a 1-cm in-\ncrease in width yields an 18% increase in the estimated mean.\n\n.x\n\n\u017e\n\n.\n\n\u017e\n\nw\n\nfigure 4.4 shows that e y may grow approximately linearly with width.\n\n\u017e\n\n.\n\nthis suggests the poisson glm with identity link. it has ml fit\n\n\u242es \u2423q \u2424x s y11.53 q 0.55 x .\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\nthis model has an additive rather than a multiplicative effect of x on \u242e.\na 1-cm increase in x has an estimated increase of \u2424s 0.55 in \u242e. the fitted\nvalues are positive at all sampled x, and the model describes simply the\neffect: on the average, about a 2-cm increase in width is associated with an\nextra satellite.\n\nfigure 4.5 plots \u242e against width for the models with log link and identity\nlink. although they diverge somewhat for relatively small and large widths,\nthey provide similar predictions over the width range in which most observa-\ntions occur. we now study whether either model fits adequately.\n\n\u02c6\n\n\u02c6\n\n "}, {"Page_number": 144, "text": "generalized linear models for counts\n\n129\n\nfigure 4.4 smoothings of horseshoe crab counts.\n\ntable 4.4 sample mean and variance of number of satellites\n\n\u017e\n\n.\nwidth cm\n- 23.25\n\n23.25\u139024.25\n24.25\u139025.25\n25.25\u139026.25\n26.25\u139027.25\n27.25\u139028.25\n28.25\u139029.25\n) 29.25\n\nnumber of\n\ncases\n\nnumber of\nsatellites\n\n14\n14\n28\n39\n22\n24\n18\n14\n\n14\n20\n67\n105\n63\n93\n71\n72\n\nsample\nmean\n1.00\n1.43\n2.39\n2.69\n2.86\n3.87\n3.94\n5.14\n\nsample\nvariance\n\n2.77\n8.88\n6.54\n11.38\n6.88\n8.81\n16.88\n8.29\n\n "}, {"Page_number": 145, "text": "130\n\nintroduction to generalized linear models\n\nfigure 4.5 estimated mean number of satellites for log and identity links.\n\n4.3.3 overdispersion for poisson glms\nin section 1.2.4 we noted that count data often show greater variability than\nthe poisson allows. for the grouped horseshoe crab data, table 4.4 shows\nthe sample mean and variance for the counts of number of satellites for the\nfemale crabs in each width category. the variances are much larger than the\nmeans, whereas poisson distributions have identical mean and variance.\nthe greater variability than predicted by the glm random component\nreflects o\u00aeerdispersion.\n\na common cause of overdispersion is subject heterogeneity. for instance,\nsuppose that width, weight, color, and spine condition are the four predictors\nthat affect a female crab\u2019s number of satellites. suppose that y has a poisson\ndistribution at each fixed combination of those predictors. our model uses\nwidth alone as a predictor. crabs having a certain width are then a mixture of\ncrabs of various weights, colors, and spine conditions. thus, the population of\ncrabs having that width is a mixture of several poisson populations, each\nhaving its own mean for the response. this heterogeneity results in an overall\nresponse distribution at that width having greater variation than the poisson\npredicts. if the variance equals the mean when all relevant variables are\ncontrolled, it exceeds the mean when only one is controlled.\n\noverdispersion is not an issue in ordinary regression with normally dis-\n.\ntributed y, because that distribution has a separate parameter the variance\n\n\u017e\n\n "}, {"Page_number": 146, "text": "generalized linear models for counts\n\n131\n\nto describe variability. for binomial and poisson distributions, however,\nthe variance is a function of the mean. overdispersion is common in the\nmodeling of counts. when the model for the mean is correct but the true\ndistribution is not poisson, the ml estimates of model parameters are still\nconsistent but standard errors are incorrect. we next introduce an extension\nof the poisson glm that has an extra parameter and accounts better for\noverdispersion. in section 4.7 we present another approach for this, quasi-\nlikelihood inference.\n\n4.3.4 negative binomial glms\nthe negati\u00aee binomial distribution has probability mass function\n\nf y; k, \u242e s\n\u017e\n\n.\n\n.\n\u232b k \u232b y q 1\n\u017e\n\n\u232b y q k\n\u017e\n.\n\n\u017e\n\n.\n\n\u017e\n\nk\n\n\u242eq k\n\n/\n\nk\n\n\u017e\n\n1 y\n\nk\n\n\u242eq k\n\n/\n\ny\n\n,\n\ny s 0, 1, 2, . . . ,\n\n\u017e\n\n4.12\n\n.\n\nwhere k and \u242e are parameters. this distribution has\n\ne y s \u242e,\n\u017e\n\n.\n\nvar y s \u242eq \u242e2rk .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ny1\n\nis called a dispersion parameter. as k \u2122 0, var y \u2122 \u242e and\nthe index k\nthe negative binomial distribution converges to the poisson cameron and\ntrivedi 1998, p. 75 . usually, k\nis unknown. estimating it helps summarize\nthe extent of overdispersion.\n\ny1\n\ny1\n\nfor k fixed, one can express 4.12 in natural exponential family form\n\u017e\n.4.1 . then, a model with negative binomial random component is a glm.\nfor simplicity, such models let k be the same constant for all observations\nbut treat it as unknown. as in glms for binary data, a variety of link\nfunctions are possible. most common is the log link, as in poisson loglinear\nmodels, but sometimes the identity link is adequate.\n\n\u02c6.\n\nin section 13.4 we discuss negative binomial glms. we illustrate it here\nfor the crab data analyzed above with poisson glms. with the identity link\nand width as predictor, the poisson glm has \u242es y11.53 q 0.55 x se s\n0.06 for \u2424 . for the negative binomial glm, \u242es y11.15 q 0.53 x se s\n0.11 . moreover, k s 0.98, so at a predicted \u242e, the estimated variance is\nroughly \u242eq \u242e2, compared to \u242e for the poisson glm. although fitted values\nare similar, the greater se for \u2424 and the greater estimated variance in the\nnegative binomial model reflect the overdispersion uncaptured with the\npoisson glm.\n\n\u02c6\n\u02c6\n\u02c6\n\n\u02c6y1\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\u017e\n\n.\n\n4.3.5 poisson regression for rates\nwhen events of a certain type occur over time, space, or some other index of\nsize, it is usually more relevant to model the rate at which they occur than the\nnumber of them. for instance, a study of homicides in a given year for a\n\n "}, {"Page_number": 147, "text": "132\n\nintroduction to generalized linear models\n\nsample of cities might model the homicide rate, defined for a city as its\nnumber of homicides that year divided by its population size. the model\nmight describe how the rate depends on the city\u2019s unemployment rate, its\nresidents\u2019 median income, and the percentage of residents having completed\nhigh school. in section 9.7 we discuss poisson regression for modeling rates.\n\n4.3.6 poisson glm of independence in i = j contingency tables\none use of poisson loglinear models is in modeling counts in contingency\ntables. we illustrate for two-way tables with independent counts y having\npoisson distributions with means \u242e . suppose that \u242e satisfy\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ni j\n\ni j\n\ni j\n\n4\n\n\u0004\n\u242e s \u242e\u2423 \u2424 ,\n\ni j\n\ni\n\nj\n\n\u0004\n\n4\n\n\u0004\n\nwhere \u2423 and \u2424 are positive constants satisfying \u00fd \u2423 s \u00fd \u2424 s 1. this is\na multiplicative model, but a linear predictor for a glm results using the log\nlink,\n\n4\n\ni\n\ni\n\ni\n\nj\n\nj\n\nj\n\nj\n\nj\n\nj\n\nj\n\nj\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni j\n\ni j\n\n4\n\n4\n\n4\n\n\u0004\n\n\u017e\n\nsince the y\ni j\n\nlog \u242e s \u242dq \u2423u q \u2424u ,\n\n.\n4.13\nwhere \u242ds log \u242e, \u2423u s log \u2423, \u2424u s log \u2424. this poisson loglinear model\nhas additive main effects of the two classifications but no interaction.\nare independent, the total sample size \u00fd \u00fd y\nhas a\ni j\npoisson distribution with mean \u00fd \u00fd \u242e s \u242e. conditional on \u00fd \u00fd y s n,\ni j\u0004\nthe cell counts have a multinomial distribution with probabilities \u2432 s\n\u242e r\u242es \u2423 \u2424 . similarly, you can check that conditional on n, the row totals\ni j\nhave a multinomial distribution with probabilities \u2432 s \u2423 and\n\u0004\ny\niq\nthe column totals y\nhave a multinomial distribution with probabilities\n\u2432 s \u2424 .\n\u0004\n4\nqj\nconditional on n, the model is a multinomial one that satisfies \u2432 s \u2423 \u2424\ns \u2432 \u2432 . this is independence of the two classifications. in fact, in poisson\niq qj\nform independence is the loglinear model 4.13 . the inferences conducted\nin chapter 3 about independence in two-way contingency tables relate to\nglms, either poisson loglinear models or corresponding multinomial models\nthat fix n or the row or column totals. in chapters 8 and 9 we present more\ncomplex loglinear models for contingency tables.\n\niq\n\nqj\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ni j\n\ni j\n\ni\n\ni\n\ni\n\nj\n\nj\n\nj\n\nj\n\n4.4 moments and likelihood for generalized\nlinear models*\n\nhaving introduced glms for binary and count data, we now turn our\nattention to details such as likelihood equations and methods for fitting\nthem. the remainder of this chapter is somewhat technical, providing general\nresults applying to most modeling methods presented in subsequent chapters.\nsee mccullagh and nelder 1989 for further details.\n\n\u017e\n\n.\n\n "}, {"Page_number": 148, "text": "moments and likelihood for generalized linear models\n\n133\n\nit is helpful to extend the notation for a glm so that it can handle many\ndistributions that have a second parameter. the random component of the\nglm specifies that the n observations\non y are independent,\nwith probability mass or density function for y of form\n\ny , . . . , y\n1\n\n\u017e\n\n.\n\nn\n\ni\n\nf y ; \u242a , \u243e s exp\n\u017e\n\n.\n\n\u0004\n\ni\n\ni\n\ny \u242a y b \u242a ra \u243e q c y , \u243e .\n4\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\ni\n\ni\n\ni\n\ni\n\n\u017e\n\n4.14\n\n.\n\nthis is called the exponential dispersion family and \u243e is called the dispersion\nparameter jorgensen 1987 . the parameter \u242a is the natural parameter.\n\n.\n\n\u017e\n\n\u2c91\n\nwhen \u243e is known, 4.14 simplifies to the form 4.1 for the natural\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\nexponential family, which is\n\nf y ; \u242a s a \u242a b y exp y q \u242a .\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n.\n\nw \u017e\n\nwe identify q \u242a here with \u242ara \u243e in 4.14 , a \u242a with exp yb \u242a ra \u243e in\n\u017e\n.\n4.14 , and b y with exp c y, \u243e in 4.14 . the more general formula 4.14\nis not needed for one-parameter families such as the binomial and poisson.\nusually, a \u243e has form a \u243e s \u243er\u243b for a known weight \u243b. for instance,\nwhen y is a mean of n independent readings, such as a sample proportion\nfor n bernoulli trials, \u243b s n section 4.4.2 .\n.\n\n\u017e\n.x\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\ni\n\ni\n\ni\n\ni\n\n.x\n\nw\n\ni\n\ni\n\ni\n\n4.4.1 mean and variance functions for the random component\ngeneral expressions for e y and var y use terms in 4.14 . let l s\n.\nlog f y ; \u242a, \u243e denote the contribution of y to the log likelihood; that is, the\nlog-likelihood function is l s \u00fd l . then, from 4.14 ,\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nl s y \u242a y b \u242a ra \u243e q c y , \u243e .\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u017e\n\n4.15\n\n.\n\ntherefore,\n\n\u2b78l r\u2b78\u242a s y y b \u242a ra \u243e ,\n.\n\n\u017e\n\n.\n\n\u017e\n\nx\n\ni\n\ni\n\n\u2b78 l r\u2b78\u242a s yb \u242a ra \u243e ,\n.\n\n\u017e\n\n.\n\n\u017e\n\n2\n\ny\n\n2\ni\n\ni\n\ni\n\nwhere b \u242a and b \u242a denote the first two derivatives of b \u2b48 evaluated\nat \u242a. we now apply the general likelihood results\n\ni\n\ni\n\ny\u017e\n\n.\n\n\u017e .\n\ni\n\ni\n\nx\u017e\n\ni\n\n.\n\n\u017e\n\ne\n\n\u2b78l\n\u2b78\u242a\n\n/\n\ns 0 and ye\n\n\u017e\n\n2\n\n\u2b78 l\n2\n\u2b78\u242a\n\n/\n\ns e\n\n\u017e\n\n\u2b78l\n\u2b78\u242a\n\n/\n\n2\n\n,\n\nwhich hold under regularity conditions satisfied by the exponential family\n\u017e\ncox and hinkley 1974, sec. 4.8 . from the first formula applied with a single\nobservation, e y y b \u242a ra \u243e s 0, or\n\n.\n.\n\n.x\n\nx\u017e\n\nw\n\ni\n\ni\n\n\u017e\n\u242e s e y s bx \u242a .\n.\n\n\u017e\n\n.\n\n\u017e\n\ni\n\ni\n\ni\n\n\u017e\n\n4.16\n\n.\n\n "}, {"Page_number": 149, "text": "134\n\nintroduction to generalized linear models\n\nfrom the second formula,\n\ny\n\nb \u242a ra \u243e s e y y b \u242a ra \u243e s var y r a \u243e ,\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2\n\n2\n\nx\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ni\n\nso that\n\nvar y s by \u242a a \u243e .\n.\n\u017e .\nin summary, the function b \u2b48\n\n.\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\nin 4.14 determines moments of y .i\n\n\u017e\n\n4.17\n\n.\n\n4.4.2 mean and variance functions for poisson and binomial\nwe illustrate the mean and variance expressions for poisson and binomial\ndistributions. when y is poisson,\n\ni\n\nf y ; \u242e s\n\u017e\n\n.\n\ni\n\ni\n\ney\u242ei\u242ey i\n\ni\n\ny !i\n\ns exp y log \u242e y \u242e y log y !\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ns exp y \u242a y exp \u242a y log y ! ,\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\nwhere \u242a s log \u242e. this has exponential dispersion form 4.14 with b \u242a s\nexp \u242a , a \u243e s 1, and c y , \u243e s ylog y !. the natural parameter is \u242a s\n.\nlog \u242e. from 4.16 and 4.17 ,\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u017e\ne y s bx \u242a s exp \u242a s \u242e ,\n\u017e\nvar y s by \u242a s exp \u242a s \u242e .\n\u017e\n\n\u017e\n\u017e\n\n\u017e\n\u017e\n\n.\n.\n\n.\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n.\n.\n\u017e\n\ni\n\nnext, suppose that n y has a bin n , \u2432 distribution; that is, here y is\nthe sample proportion rather than number of successes, so e y is indepen-\ndent of n . let \u242a s log \u2432r 1 y \u2432 . then, \u2432 s exp \u242a r 1 q exp \u242a and\n\u017e\nlog 1 y \u2432 s ylog 1 q exp \u242a . extending 4.3 , one can show that\n\n\u017e\n\u017e\n\n.x\n\n.x\n\n.x\n\ni\nw\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\nw\n\nw\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n.\n\n\u017e\n\nn\ni\nn yi\n\ni\n\ni\n\n/\n\ni\n\ns exp\n\nf y ; \u2432 , n s\n\u017e\n\n.\n\ni\n\ni\n\ni\n\n\u2432 1 y \u2432\n\nn yi\ni\n\n\u017e\n\ni\n\ni\n\nn yn y\ni\ni\n\ni\n\n.\n\ny \u242a y log 1 q exp \u242a\n\u017e\n\ni\n\n1rn\n\ni\n\ni\n\n.\n\nq log\n\n\u017e\n\n/\n\ni\n\n.\n\n\u017e\n\n4.18\n\n.\n\nn\ni\nn y\ni\nw\n\n.x\nthis has exponential dispersion form 4.14 with b \u242a s log 1 q exp \u242a ,\n\u017e\na \u243e s 1rn , and c y , \u243e s log\n\u017e\n. the natural parameter is the logit,\n\u242a s log \u2432r 1 y \u2432 . from 4.16 and 4.17 ,\n.\n\u017e\n\nni\nn yi\n.\n\n\u017e\n.x\n\n\u017e\n\n/\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nw\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ne y s b \u242a s exp \u242a r 1 q exp \u242a s \u2432 ,\n\u017e\n\u017e\n\u017e\n\nvar y s b \u242a a \u243e s exp \u242a r 1 q exp \u242a\n\u017e\n\n.\n5\nn s \u2432 1 y \u2432 rn .\n\n\u017e\n\u00bd\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n2\n\ny\n\nx\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n.\n\n "}, {"Page_number": 150, "text": "moments and likelihood for generalized linear models\n\n135\n\n\u017e\n\n4.4.3 systematic component and link function\nlet\ndenote values of explanatory variables for observation i.\nthe systematic component of a glm relates parameters \u2429 to these\nvariables using a linear predictor\n\nx , . . . , x\ni1\n\ni p\n\n.\n\n\u0004\n\n4\n\ni\n\n\u2429 s \u2424 x ,\n\u00fdi\n\ni j\n\nj\n\nj\n\ni s 1, . . . , n.\n\nin matrix form,\n\n\u2429 s x\u2424,\n\n1\n\nn\n\n\u017e\n\n\u017e\n\n.x\n\nwhere \u2429 s \u2429 , . . . , \u2429 , \u2424 s \u2424 , . . . , \u2424 are column vectors of model\nparameters, and x is the n = p matrix of values of the explanatory variables\nfor the n subjects. in ordinary linear models, x is called the design matrix. it\nneed not refer to an experimental design, however, and the glm literature\ncalls it the model matrix.\nthe glm links \u2429 to \u242e s e y by a link function g \u2b48 . thus, \u242e relates to\n\n\u017e .\n\n.x\n\n\u017e\n\n.\n\n1\n\np\n\ni\n\ni\n\ni\n\ni\n\nthe explanatory variables by\n\n\u2429 s g \u242e s \u2424 x ,\n\n. \u00fd\n\n\u017e\n\ni j\n\ni\n\ni\n\nj\n\nj\n\ni s 1, . . . , n.\n\nthe link function g for which g \u242e s \u242a in 4.14 is the canonical link. for\nit, the direct relationship\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\n\u242a s \u2424 x\u00fdi\n\nj\n\nj\n\ni j\n\ni\n\ni\n\n.\n\n.\n\nx\u017e\n\ni\n\u017e x.y1\u017e\n\n\u242e , where b\n\noccurs between the natural parameter and the linear predictor.\nsince \u242e s b \u242a , the natural parameter is the function of the mean,\n\u242a s b\n\u2b48 denotes the inverse function to b . thus, the\ncanonical link is the inverse of b . in the poisson case, for instance, b \u242a s\nexp \u242a , so b \u242a s exp \u242a s \u242e. thus, b\nis the inverse of the expo-\nnential function, which is the log function i.e., \u242a s log \u242e . the canonical\ni\nlink is the log link.\n\n\u017e x.y1\u017e .\n\n\u017e x.y1\u017e .\n\u2b48\n\nx\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\nx\n\nx\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n4.4.4 likelihood equations for a glm\n\u017e\nfor n independent observations, from 4.15 the log likelihood is\n\n.\n\nl \u2424 s l s log f y ; \u242a , \u243e s\n\u017e\n\n\u017e\n\n.\n\n.\n\ni\n\ni\n\ni\n\n\u00fd\n\ni\n\n\u00fd\n\ni\n\ny \u242a y b \u242a\u017e\n\n.\n\ni\n\ni\n\ni\n\na \u243e\u017e\n.\n\n\u00fd\n\ni\n\nq c y , \u243e .\n.\n\n\u00fd\n\n\u017e\n\ni\n\ni\n\n\u017e\n\n4.19\n\n.\n\nthe notation l \u2424 reflects the dependence of \u242a on the model parameters \u2424.\n\n\u017e\n\n.\n\n "}, {"Page_number": 151, "text": "136\n\nintroduction to generalized linear models\n\nthe likelihood equations are\n\n\u2b78l \u2424 r\u2b78\u2424 s \u2b78l r\u2b78\u2424 s 0\n\n\u00fdj\n\n\u017e\n\n.\n\ni\n\nj\n\ni\n\nfor all\n\nj. to differentiate the log likelihood 4.19 , we use the chain rule,\n\n\u017e\n\n.\n\ns\n\n\u2b78l\ni\n\u2b78\u2424\nj\n\ni\n\n\u2b78l \u2b78\u242a \u2b78\u242e \u2b78\u2429\ni\n\u2b78\u242a \u2b78\u242e \u2b78\u2429 \u2b78\u2424\nj\n\ni\n\ni\n\ni\n\ni\n\ni\n\n.\n\n\u017e\n\n4.20\n\n.\n\nw\n\ni\n\nsince \u2b78l r\u2b78\u242a s y y b \u242a ra \u243e , and since \u242e s b \u242a and var y s\n\u017e\n. \u017e\n.\ny\u017e\nb \u242a a \u243e from 4.16 and 4.17 ,\n\u2b78l r\u2b78\u242a s y y \u242e ra \u243e ,\n.\n\n\u2b78\u242er\u2b78\u242a s by \u242a s var y ra \u243e .\n.\n\nx\u017e\n\nx\u017e\n\ni\n.\n\ni\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\n.x\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u017e\n\n.\nalso, since \u2429 s \u00fd \u2424 x ,\n\ni\n\ni\n\ni\n\nj\n\nj\n\ni j\n\ni\n\n\u2b78\u2429r\u2b78\u2424 s x .\n\ni j\n\ni\n\nj\n\nfinally, since \u2429 s g \u242e , \u2b78\u242er\u2b78\u2429 depends on the link function for the model.\nin summary, substituting into 4.20 gives us\n\ni\u017e\n\n\u017e\n\n.\n\n.\n\ni\n\ni\n\ni\n\ns\n\n\u2b78l\ni\n\u2b78\u2424\nj\n\n\u017e\n\ny y \u242e a \u243e \u2b78\u242e\ni\ni\n\u017e\na \u243e var y \u2b78\u2429\ni\n\n.\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\n\u017e\n\nx s\n\ni j\n\ny y \u242e x \u2b78\u242e\ni\ni\ni\n\u2b78\u2429\nvar y\ni\ni\n\n.\n.\n\n\u017e\n\ni j\n\n.\n\n\u017e\n\n4.21\n\n.\n\nthe likelihood equations are\n\nn\n\n\u017e\n\n.\n\u00fd var y\n\u017e\n.i\nis1\n\ny y \u242e x \u2b78\u242e\n\u2b78\u2429\ni\n\ni j\n\ni\n\ni\n\ni s 0,\n\nj s 1, . . . , p.\n\n\u017e\n\n4.22\n\n.\n\ni\n\ni\n\nalthough \u2424 does not appear in these equations, it is there implicitly through\n\u242e, since \u242e s g \u00fd \u2424 x\n.\n. different link functions yield different sets of\nequations.\n\ny1\u017e\n\ninterestingly, the likelihood equations 4.22 depend on the distribution of\ny only through \u242e and var y . the variance itself depends on the mean\ni\nthrough a particular functional form\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni j\n\ni\n\ni\n\nj\n\nj\n\nvar y s \u00ae \u242e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\n\u017e\n\nfor some function \u00ae, such as \u00ae \u242e s \u242e for the poisson, \u00ae \u242e s \u242e 1 y \u242e\n.\ni\nfor the bernoulli, and \u00ae \u242e s \u2434 i.e., constant\nfor the normal. when y\ni\nhas distribution in the natural exponential family, the relationship between\n.\n\u017e\nthe mean and the variance characterizes the distribution jorgensen 1987 .\nfor instance, if y has distribution in the natural exponential family and if\n\u00ae \u242e s \u242e, then necessarily y has the poisson distribution.\n\u017e\n\n.\ni\n2 \u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u2c91\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n "}, {"Page_number": 152, "text": "moments and likelihood for generalized linear models\n\n137\n\n4.4.5 likelihood equations for binomial glms\n.\nusing notation from section 4.4.2, suppose that n y has a bin n , \u2432\ni\ndistribution. then y is a sample proportion of successes for n trials. the\nbinomial glm 4.8 for a single predictor extends with several predictors to\n\n.\n\n\u017e\n\n\u017e\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u2432 s \u233d\n\ni\n\n\u017e\n\n\u00fd\n\nj\n\n/\n\n,\n\n\u2424 x\n\nj\n\ni j\n\n\u017e\n\n4.23\n\n.\n\nwhere \u233d is the standard cdf of some class of continuous distributions. since\n\u2432 s \u242e s \u233d \u2429 with \u2429 s \u00fd \u2424 x ,\n\n\u017e\n\n.\n\ni\n\nj\n\nj\n\ni j\n\ni\n\ni\n\ni\n\n\u2b78\u242er\u2b78\u2429 s \u243e \u2429 s \u243e \u2424 x\n\n\u017e\n\n.\n\n\u00fd\n\ni\n\ni\n\ni\n\nj\n\n\u017e\n\nj\n\n/\n\ni j\n\n,\n\n\u017e .\n\nwhere \u243e u s \u2b78\u233d u r\u2b78u i.e., the probability density function corresponding\nto the cdf \u233d . since var y s \u2432 1 y \u2432 rn , the likelihood equations 4.22\n.\nsimplify to\n\n\u017e .\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\ni\n\ni\n\ni\n\ni\n\ni\n\nn y y \u2432 x\n\u017e\n.\n\u2432 1 y \u2432\n.\n\ni\n\u017e\n\ni\n\ni\n\ni\n\n\u017e\ni j \u243e \u2424 x s 0,\n\n\u00fd j\n\n/\n\ni j\n\nj\n\n\u017e\n\n4.24\n\n.\n\n\u00fd\n\ni\n\nj\n\nj\n\ni\n\ni j\n\n\u017e\n\nwhere \u2432 s \u233d \u00fd \u2424 x\n.\n. these depend on the link function \u233d through the\nderivative of its inverse.\nfor the logit link, \u2429 s log \u2432r 1 y \u2432 , so \u2b78\u2429r\u2b78\u2432 s 1r \u2432 1 y \u2432 and\ni\n\u2b78\u242er\u2b78\u2429 s \u2b78\u2432r\u2b78\u2429 s \u2432 1 y \u2432 . then the likelihood equations 4.22 and\n\u017e\n4.24 simplify to\n\n.x\n.\n\ny1\n\n.x\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\nw\n\nw\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u00fd i\n\nn y y \u2432 x s 0,\n\n\u017e\n\n.\n\ni j\n\ni\n\ni\n\ni\n\n\u017e\n\n4.25\n\n.\n\nwhere \u2432 satisfies 4.23 with \u233d the standard logistic cdf.\n\n\u017e\n\n.\n\ni\n\n4.4.6 asymptotic covariance matrix of model parameter estimators\nthe likelihood function for the glm also determines the asymptotic covari-\nance matrix of the ml estimator \u2424. this matrix is the inverse of the\ninformation matrix iiiii, which has elements e y\u2b78 l \u2424 r\u2b78\u2424 \u2b78\u2424 . to find\nthis, for the contribution l to the log likelihood we use the helpful result\n\n\u02c6\n\n\u017e\n\n.\n\nw\n\nx\n\nh\n\n2\n\nj\n\ni\n\n\u017e\n\ne\n\n\u2b782l\ni\n\u2b78\u2424 \u2b78\u2424\nj\n\nh\n\n/\n\ns ye\n\n\u017e\n\ni\n\n\u2b78l\n\u2b78\u2424\nh\n\n/\u017e\n\n/\n\n,\n\n\u2b78l\ni\n\u2b78\u2424\nj\n\n "}, {"Page_number": 153, "text": "138\n\nintroduction to generalized linear models\n\nwhich holds for exponential families cox and hinkley 1974, sec. 4.8 . thus,\n\n\u017e\n\n.\n\n\u017e\n\ne\n\n2\u2b78 l\ni\n\u2b78\u2424 \u2b78\u2424\nj\n\nh\n\n/\n\n\u017e\n\ns ye\n\ny y \u242e x \u2b78\u242e y y \u242e x \u2b78\u242e\ni\ni\ni\n\u2b78\u2429\nvar y\ni\ni\n\ni\n\u2b78\u2429 var y\ni\n\n.\n.\n\n.\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\nih\n\ni j\n\ni\n\ni\n\ni\n\nfrom 4.21\n\n\u017e\n\n.\n\ns\n\nyx\nx\nih\n.i\n\u017e\nvar y\n\ni j\n\n\u017e\n\n/\n\n2\n\n.\n\n\u2b78\u242e\ni\n\u2b78\u2429\ni\n\nsince l \u2424 s \u00fd l ,\n\n\u017e\n\n.\n\ni\n\ni\n\n\u017e\n\ne y\n\n2\n\n\u017e\n\n.\n\u2b78 l \u2424\n\u2b78\u2424 \u2b78\u2424\nj\n\nh\n\n/\n\ns\n\nn\n\n\u00fd\nis1\n\nx\nx\ni j\nvar y\ni\n\nih\n\u017e\n\n.\n\n\u017e\n\n/\n\n2\n\n.\n\n\u2b78\u242e\ni\n\u2b78\u2429\ni\n\ngeneralizing from this typical element to the entire matrix, the information\nmatrix has the form\n\niiiii s xx wx,\n\nwhere w is the diagonal matrix with main-diagonal elements\n\nw s \u2b78\u242er\u2b78\u2429 rvar y .\n.\n\n\u017e\n\n.\n\n\u017e\n\n2\n\ni\n\ni\n\ni\n\ni\n\nthe asymptotic covariance matrix of \u2424 is estimated by\n\n\u02c6\n\n$\ncov \u2424 s iiiii s x wx\n\u02c6\n\n\u02c6\ny1\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\nx\n\ny1\n\n.\n\n,\n\n\u017e\n\n4.26\n\n.\n\n\u017e\n\n4.27\n\n.\n\n\u017e\n\n4.28\n\n.\n\n\u02c6\n\nwhere w is w evaluated at \u2424. from 4.27 , the form of w also depends on\nthe link function. we\u2019ll see an example for poisson glms next and for\nbinomial glms in section 5.5.\n\n\u017e\n\n.\n\n\u02c6\n\n4.4.7 likelihood equations and covariance matrix for\npoisson loglinear model\nthe general poisson loglinear model 4.4 has the matrix form\n\n\u017e\n\n.\n\nfor the log link, \u2429 s log \u242e, so \u242e s exp \u2429 and \u2b78\u242er\u2b78\u2429 s exp \u2429 s \u242e.\nsince var y s \u242e, the likelihood equations 4.22 simplify to\n\ni\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nlog \u242e s x\u2424.\n\u017e\n\n\u00fd i\n\u017e\n\ny y \u242e x s 0.\n\n.\n\ni j\n\ni\n\ni\n\n\u017e\n\n4.29\n\n.\n\nthese equate the sufficient statistics \u00fd y x\ni\n\ni\n\ni j\n\nfor \u2424 to their expected values.\n\n "}, {"Page_number": 154, "text": "inference for generalized linear models\n\n139\n\nalso, since\n\nw s \u2b78\u242er\u2b78\u2429 rvar y s \u242e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2\n\ni\n\ni\n\ni\n\ni\n\ni\n\nx \u02c6 y1\nthe estimated covariance matrix 4.28 of \u2424 is x wx , where w is the\ndiagonal matrix with elements of \u242e on the main diagonal.\n\n\u02c6\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\n\u017e\n\u02c6\n\n4.5\n\ninference for generalized linear models\n\n\u017e\n\n.\n\nfor most glms the likelihood equations 4.22 are nonlinear functions of \u2424.\nfor now, we put off details about solving them for the ml estimator \u2424 and\nfocus instead on using the fit for statistical inference.\n\n\u02c6\n\nthe wald, score, and likelihood-ratio methods introduced in section 1.3.3\nfor significance testing and interval estimation apply to any glm. in this\nsection we concentrate on likelihood-ratio inference, through the de\u00aeiance of\nthe glm.\n\n4.5.1 deviance and goodness of fit\nfrom section 4.1.5, the saturated glm has a separate parameter for each\nobservation. it gives a perfect fit. this sounds good, but it is not a helpful\nmodel. it does not smooth the data or have the advantages that a simpler\nmodel has, such as parsimony. nonetheless, it serves as a baseline for other\nmodels, such as for checking model fit.\n\n\u02dc\n\na saturated model explains all variation by the systematic component of\nthe model. let \u242a denote the estimate of \u242a for the saturated model,\ncorresponding to estimated means \u242e s y for all i. for a particular unsatu-\nrated model, denote the corresponding ml estimates by \u242a and \u242e. for\n.\nmaximized log likelihood l \u242e; y for that model and maximized log likeli-\nhood l y; y in the saturated case,\n\n\u02dci\n\n\u02c6i\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\ni\n\ny2 log\n\nmaximum likelihood for model\n\nmaximum likelihood for saturated model\n\ns y2 l \u242e; y y l y; y\n\n\u017e\n\n.\n\n\u017e\n\n\u02c6\n\n.\n\ndescribes lack of fit. it is the likelihood-ratio statistic for testing the null\nhypothesis that the model holds against the alternative that a more general\n.\nmodel holds. from 4.19 ,\n\n\u017e\n\ny2 l \u242e; y y l y; y\n\n\u017e\n\n\u017e\n\u02c6\ns 2\n\n.\n\u00fd\n\ni\n\n.\n\u017e\n\ny \u242a y b \u242a ra \u243e y 2\n\n\u017e\n\n.\n\n\u02dc\ni\n\n.\n\n\u02dc\ni\n\ni\n\ny \u242a y b \u242a ra \u243e .\n\u02c6\n.\ni\n\n\u02c6\ni\n\n\u017e\n\ni\n\n\u017e\n\n.\n\n\u00fd\n\ni\n\n "}, {"Page_number": 155, "text": "140\nintroduction to generalized linear models\nusually, a \u243e in 4.14 has the form a \u243e s \u243er\u243b, and this statistic equals\n\n.\n2 \u243b y \u242a y \u242a y b \u242a q b \u242a r\u243es d y; \u242e r\u243e.\n\u00fd\n\n\u017e\n\n4.30\n\n.\n\n\u02dc\ni\n\n\u02c6\ni\n\n\u02dc\ni\n\n\u02c6\ni\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\ni\n\ni\n\ni\n\n\u017e\n\n/\n\ni\n\nthis is called the scaled de\u00aeiance and d y; \u242e is called the de\u00aeiance. the\ngreater the scaled deviance, the poorer the fit. for some glms the scaled\ndeviance has an approximate chi-squared distribution.\n\n\u02c6\n\n\u017e\n\n.\n\n4.5.2 deviance for poisson models\nfor poisson glms, by section 4.4.2, \u242a s log \u242e and b \u242a s exp \u242a s \u242e.\n\u02c6\nsimilarly, \u242a s log y and b \u242a s y for the saturated model. also a \u243e s 1,\nso the deviance and scaled deviance 4.30 equal\n.\n\ny log y r\u242e y y q \u242e .\n\nd y; \u242e s 2\n\u017e\n\n4.31\n\n\u02dc\u017e\ni\n\n\u02c6\ni\n\u017e\n\n\u02c6\ni\n\n\u02dc\ni\n\n\u02c6\ni\n\n\u02c6\n\n\u00fd i\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nwhen a model with log link contains an intercept term, the likelihood\nequation 4.29 implied by that parameter is \u00fd y s \u00fd \u242e. then the deviance\nsimplifies to\n\n\u02c6\n\n\u017e\n\n.\n\ni\n\ni\n\nd y; \u242e s 2\n\u017e\n\n.\n\n\u02c6\n\n\u00fd i\n\ny log y r\u242e .\n.\n\n\u017e\n\n\u02c6\n\ni\n\ni\n\ni\n\n\u017e\n\n4.32\n\n.\n\nfor two-way contingency tables, this reduces to the g statistic 3.11 in\nsection 3.2.1, substituting cell count n for y and the independence fitted\nvalue \u242e for \u242e. for a poisson or multinomial model applied to a contin-\ngency table with a fixed number of cells n, we will see in section 14.3 that\n4\nthe deviance has an approximate chi-squared distribution for large \u242e .i\n\n\u02c6\n\n\u02c6\n\n\u0004\n\ni j\n\ni j\n\ni\n\ni\n\n2\n\n\u017e\n\n.\n\n4.5.3 deviance for binomial models: grouped and ungrouped data\n4\n4\n\u0004\nnow consider binomial glms with sample proportions\ny based on n\ni\n.x\nw\ntrials. by section 4.4.2, \u242a s log \u2432r 1 y \u2432 and b \u242a s log 1 q exp \u242a s\n\u02c6\n\u017e\n\u02c6\ni\nand b \u242a s ylog 1 y y\nylog 1 y \u2432 . similarly,\u242a s log y r 1 y y\n.\nfor\ni\nthe saturated model. also, a \u243e s 1rn , so \u243es 1 and \u243b s n . the deviance\n\u017e\n4.30 equals\n\n.x\n.x\n\n.\n\u02dc\ni\n\n\u02c6\ni\n\u017e\n\n\u02c6\nw\n\n\u02c6 i\n\n\u02c6\ni\n\n\u02dc\ni\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nw\n\n\u0004\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u00bd\n\n\u017e\n\n\u00fd i\n\nn y log\n\ni\n\n2\n\ni\n\ny\n\ni\n\n1 y y\n\ny log\n\ni\n\n\u2432\u02c6\ni\n\n1 y \u2432\u02c6\n\ni\n\n/\n\nq log 1 y y y log 1 y \u2432\n.\u02c6\n\n\u017e\n\n\u017e\n\n.\n\ni\n\ni\n\n5\n\ns 2\n\ns 2\n\n\u00fd\n\ni\n\n\u00fd\n\ni\n\nn y log\ni\n\ni\n\nn y log\ni\n\ni\n\nn y\n\ni\n\ni\n\ni\n\nn y n y\ni\nn y\ni\n\u02c6\nn \u2432\ni\ni\n\nq 2\n\ni\n\ny 2\n\n\u00fd\n\ni\n\ni\n\nn y log\ni\n\ni\n\n\u02c6\nn \u2432\ni\ni\n\nn y n \u2432\n\u02c6\n\ni\n\ni\n\ni\n\nq 2\n\nn log\ni\n\n\u00fd\n\ni\n\n1 y y\ni\n1 y \u2432\n\u02c6\n\ni\n\nn y n y\n\ni\n\ni\n\n.\n\ni\n\nlog\n\n\u017e\n\n\u00fd\n\ni\n\ni\n\nn y n y\nn y n \u2432\n\u02c6\n\ni\n\ni\n\ni\n\ni\n\ni\n\n.\n\n "}, {"Page_number": 156, "text": "inference for generalized linear models\nat setting i, n y is the number of successes and n y n y\nfailures,\nsuccesses and failures and has the same form,\n\nis the number of\ni s 1, . . . , n. thus, the deviance is a sum over the 2 n cells of\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\n141\n\nd y; \u242e s 2\n\u017e\n\n.\n\n\u02c6\n\n\u00fd\n\nobserved = log observedrfitted ,\n.\n\n\u017e\n\n\u017e\n\n4.33\n\n.\n\n\u017e\n\n.\n\nas the deviance 4.32 for poisson loglinear models with intercept term.\n\nwith binomial responses,\n\nit is possible to construct the data file as\nexpressed here with the counts of successes and failures at each setting for\nthe predictors, or with the individual bernoulli 0\u13901 observations at the\nsubject level. the deviance differs in the two cases. in the first case the\nsaturated model has a parameter at each setting for the predictors, whereas\nin the second case it has a parameter for each subject. we refer to these as\ngrouped data and ungrouped data cases. the approximate chi-squared distri-\nbution for the deviance occurs for grouped data but not for ungrouped data\n\u017e\nsee problems 4.22 and 5.37 . with grouped data, the sample size increases\nfor a fixed number of settings of the predictors and hence a fixed number of\nparameters for the saturated model.\n\n.\n\n4.5.4 likelihood-ratio model comparison using the deviance\nfor a poisson or binomial model m, \u243es 1, so the deviance 4.30 equals\n\n\u017e\n\n.\n\nd y; \u242e s y2 l \u242e; y y l y; y\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u02c6\n\n\u02c6\n\n.\n\n.\n\n\u017e\n\n4.34\n\n.\n\n0\n\n0\n\nconsider two models, m with fitted values \u242e and m with fitted values \u242e ,\n1\nwith m a special case of m . model m is said to be nested within m .\n1\n\nsince m is simpler than m , a smaller set of parameter values satisfies\nm than satisfies m . maximizing the log likelihood over a smaller space\n0\ncannot yield a larger maximum. thus, l \u242e ; y f l \u242e ; y , and it follows\nfrom 4.34 with the same l y; y for each model that\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n0\n\n1\n\n1\n\n0\n\n0\n\n1\n\n1\n\n0\n\n1\n\n\u02c6\n\n\u02c6\n\n.\n\n\u017e\nd y; \u242e f d y; \u242e .\n\u017e\n.\n\n.\n\n\u017e\n\n\u02c6\n\n\u02c6\n\n1\n\n0\n\nsimpler models have larger deviances. assuming that model m holds, the\nlikelihood-ratio test of the hypothesis that m holds uses the test statistic\n\n1\n\n0\n\n0\n\n.\n\n\u017e\n\n.\n\n\u02c6\n\ny2 l \u242e ; y y l \u242e ; y\n\n\u017e\ns y2 l \u242e ; y y l y; y y y2 l \u242e ; y y l y; y\ns d y; \u242e y d y; \u242e .\n.\n\n\u02c6\n.\n\n\u02c6\n.\n\n\u017e\n\u02c6\n\n\u017e\n\u02c6\n\n\u02c6\n\n\u0004\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n1\n\n0\n\n1\n\n0\n\n1\n\n.\n\n4\n\nthe likelihood-ratio statistic comparing the two models is simply the differ-\nence between the deviances. this statistic is large when m fits poorly\ncompared to m .1\n\n0\n\n "}, {"Page_number": 157, "text": "142\n\nintroduction to generalized linear models\n\nin fact, since the part in 4.30 involving the saturated model cancels, the\n\n\u017e\ndifference between deviances,\n\n.\n\n\u017e\nd y; \u242e y d y; \u242e s 2 \u243b y \u242a y \u242a y b \u242a q b \u242a\n\u02c6\n\u017e\n0 i\n\n\u00fd\n\n\u02c6\n0 i\n\n\u02c6\n1i\n\n\u02c6\n1i\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n0\n\n1\n\ni\n\ni\n\n.\n\n,\n\n\u017e\n\n/\n\nalso has the form of the deviance. under regularity conditions, this difference\nhas approximately a chi-squared null distribution with df equal\nto the\ndifference between the numbers of parameters in the two models.\n\nfor binomial glms and poisson loglinear glms with intercept, from\nexpression 4.33 for the deviance, the difference in deviances uses the\nobserved counts and the two sets of fitted values in the form\n\n\u017e\n\n.\n\nd y; \u242e y d y; \u242e s 2\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u02c6\n\n\u02c6\n\n0\n\n1\n\n\u00fd\n\nobserved = log fitted rfitted .\n.\n\n\u017e\n\n1\n\n0\n\nwith binomial responses, the test comparing models does not depend on\nwhether the data file has grouped or ungrouped form. the saturated model\ndiffers in the two cases, but its log likelihood cancels when one forms the\ndifference between the deviances.\n\n4.5.5 residuals for glms\nwhen a glm fits poorly according to an overall goodness-of-fit test, exami-\nnation of residuals highlights where the fit is poor. one type of residual uses\ncomponents of the deviance. in 4.30 let d y; \u242e s \u00fdd , where\n\n\u017e\n\n.\n\n.\n\ni\n\n\u017e\n\u017e\n\n\u02c6\n.\n\n.\nd s 2 \u243b y \u242a y \u242a y b \u242a q b \u242a .\n\n\u02dc\ni\n\n\u02c6\ni\n\n\u02dc\ni\n\n\u017e\n\ni\n\ni\n\n\u017e\n\n/\nthe de\u00aeiance residual for observation i is\n\u017e\n\n\u02c6\ni\n\ni\n\n'\nd = sign y y \u242e ,\n.\u02c6\n\ni\n\ni\n\ni\n\nan alternative is the pearson residual,\n\ne s\ni $ 1r2\n\ni\n\ni\n\ny y \u242e\u02c6\nvar y\u017e\n.i\n\n.\n\nfor instance, for a poisson glm, var y s \u242e and the pearson residual is\n\n\u017e\n\n.\n\ni\n\ni\n\ne s y y \u242e r \u242e .'\n\u02c6\n\n.\u02c6\n\n\u017e\n\ni\n\ni\n\ni\n\ni\n\nfor two-way contingency tables identifying y with cell count n and \u242e with\nthe independence fitted value \u242e , this has the form 3.12 ; then \u00fde s x ,\n2\nthe pearson x 2 statistic. similarly, the sum of squared deviance residuals\n\u00fdd s g2, the likelihood-ratio statistic for testing independence.\n\n\u02c6i j\n\n\u017e\n\n.\n\ni j\n\ni\n\ni\n\n\u02c6\n2\ni j\n\ni j\n\n\u017e\n\n4.35\n\n.\n\n\u017e\n\n4.36\n\n.\n\n "}, {"Page_number": 158, "text": "fitting generalized linear models\n\n143\n\nwhen the model holds, pearson and deviance residuals are less variable\nthan standard normal because they compare y to the fitted means rather\ns\n\u017e\nthan the true mean e.g., the denominator of 4.36 estimates var y\ni\nw\nvar y y \u242e\n. standardized residuals divide\nthe ordinary residuals by their asymptotic standard errors. for glms the\ny y \u242e is\nasymptotic covariance matrix of the vector of the raw residuals\n\nrather than var y y \u242e\n\u02c6\n\n.x1r2 .\n\n.x1r2\n\n.x1r2\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nw\n\nw\n\n\u0004\n\n4\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u02c6\n\ni\n\ni\n\nx\ncov y y \u242e s cov y i y hat .\nhere, i is the identity matrix and hat is the hat matrix,\n\n\u02c6\n\nw\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nx\n\n\u017e\n\n1r2\n\nhat s w x x wx\n\n.\n4.37\n\u02c6\nwhere w is the diagonal matrix with elements 4.27 pregibon 1981 . let hi\ndenote the estimated diagonal element of hat for observation i, called its\nle\u00aeerage. then, standardizing by dividing y y \u242e by its estimated se yields\nthe standardized pearson residual\n\n1r2\nx w ,\n\u017e\n. \u017e\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\nx\n\ni\n\ni\n\ny1\n\ny y \u242e\n\u02c6\ni\n1 y h\n\u02c6\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n5\n\n\u00bd\n\n1r2\n\ns\n\n.\n\u017e\n\n4.38\n\nr s\n\nvar y\ni\n\ne\n\u02c6'\n1 y h\n'\u02c6\n\u02c6 \u017e\nfor poisson glms, for instance, r s y y \u242e r \u242e 1 y h . pierce and\nschafer 1986 presented standardized deviance residuals.\nin linear models the hat matrix is so-named because hat = y projects the\ndata to the fitted values, \u242e s\u2018\u2018mu-hat.\u2019\u2019 for glms, applying the estimated\nyields \u2429 s g \u242e , the\n\u017e .\nhat matrix to a linearized approximation for g y\nmodel\u2019s estimated linear predictor values. the greater an observation\u2019s lever-\nage, the greater its potential influence on the fit. as in ordinary regression,\nthe leverages fall between 0 and 1 and sum to the number of model\nparameters. unlike ordinary regression, the hat values depend on the fit as\nwell as the model matrix, and points that have extreme predictor values need\nnot have high leverage.\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\n4.6 fitting generalized linear models\n\nfinally, we study how to find the ml estimators \u2424 of glm parameters. the\nare usually nonlinear in \u2424. we describe a\nlikelihood equations\ngeneral-purpose iterative method for solving nonlinear equations and apply it\ntwo ways to determine the maximum of a likelihood function.\n\n\u017e\n4.22\n\n\u02c6\n\n.\n\n\u02c6\n\n4.6.1 newton\u2013raphson method\nthe newton\u1390raphson method is an iterative method for solving nonlinear\nequations, such as equations whose solution determines the point at which a\nfunction takes its maximum. it begins with an initial guess for the solution. it\n\n "}, {"Page_number": 159, "text": "144\n\nintroduction to generalized linear models\n\nobtains a second guess by approximating the function to be maximized in a\nneighborhood of the initial guess by a second-degree polynomial and then\nfinding the location of that polynomial\u2019s maximum value. it then approxi-\nmates the function in a neighborhood of the second guess by another\nsecond-degree polynomial, and the third guess is the location of its maxi-\nmum. in this manner, the method generates a sequence of guesses. these\nconverge to the location of the maximum when the function is suitable\nandror the initial guess is good.\n\u02c6\nin more detail, here\u2019s how newton\u1390raphson determines the value \u2424 at\nwhich a function l \u2424 is maximized. let u s \u2b78l \u2424 r\u2b78\u2424 , \u2b78l \u2424 r\u2b78\u2424 , . . .\n.\n.\nlet h denote the matrix having entries h s \u2b78 l \u2424 r\u2b78\u2424 \u2b78\u2424 , called the\nhessian matrix. let u\u017et. and h\u017et. be u and h evaluated at \u2424\u017et., the guess t for\n\u02c6\n\u2424. step t\napproximates l \u2424 near\n\u2424\u017et. by the terms up to second order in its taylor series expansion,\n\u2424 y \u2424 h \u2424 y \u2424\n\nl \u2424 f l \u2424 q u\n\u017e\n\nin the iterative process\n\nt s 0, 1, 2, . . .\n\n\u2424 y \u2424 q\n\n.\n\u017e\n\n\u017e\n2\n\nx\n\u017et.\n\n\u017et.\n\n\u017et.\n\n\u017et.\n\n\u017et.\n\n\u017et.\n\nab\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\na\n\n1\n\n2\n\nb\n\nx\n\nx\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n1\n\n.2\n\n\u017e\n\nsolving \u2b78l \u2424 r\u2b78\u2424 f u q h \u2424 y \u2424 s 0 for \u2424 yields the next guess.\nthat guess can be expressed as\n\n\u017et..\n\n\u017et.\u017e\n\n\u017et.\n\n\u017e\n\n.\n\n\u017etq1.\n\n\u2424\n\ns \u2424 y h\n\n\u017et.\n\n\u017e\n\ny1\n\n\u017et.\n\n.\n\n\u017et.\nu ,\n\n\u017e\n\n4.39\n\n.\n\n\u017e\n\n\u017et.\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017et..\n\niterations proceed until changes in l \u2424\n\nassuming that h is nonsingular. however, computing routines use stan-\ndard methods for solving the linear equations rather than explicitly calculat-\ning the inverse.\nbetween successive cycles are\nsufficiently small. the ml estimator is the limit of \u2424\u017et. as t \u2122 \u2b01; however,\nthis need not happen if l \u2424 has other local maxima at which the derivative\nof l \u2424 equals 0. in that case, a good initial estimate is crucial. to help\nunderstand the newton\u1390raphson process, work through these steps when \u2424\nhas a single element problem 4.34 . then, figure 4.6 illustrates a cycle of the\nmethod, showing the parabolic second-order approximation at a given step.\nin the next chapter we use newton\u1390raphson for logistic regression\nmodels. for now, we illustrate it with a simpler problem for which we know\nthe answer, maximizing the log likelihood based on an observation y from a\n.\nbin n, \u2432 distribution. from section 1.3.2, the first two derivatives of l \u2432\ns y log \u2432q n y y log 1 y \u2432 are\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\nu s y y n\u2432 r\u2432 1 y \u2432 ,\n.\n\n\u017e\n\n\u017e\n\n.\n\nh s y yr\u2432 q n y y r 1 y \u2432 .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2\n\n2\n\neach newton\u1390raphson step has the form\n\n\u2432 s \u2432 q\n\n\u017etq1.\n\n\u017et.\n\ny\n\u017et.\n\n\u2432\n\n\u017e\n\n2\n\n.\n\nq\n\nn y y\n1 y \u2432\n\u017et.\n\n\u017e\n\n2\n\n.\n\ny1\n\ny y n\u2432\n\u017et.\n\u2432 1 y \u2432\n\u017e\n\u017et.\n\u017et.\n\n.\n\n.\n\n "}, {"Page_number": 160, "text": "fitting generalized linear models\n\n145\n\nfigure 4.6 cycle of newton\u1390raphson method.\n\n\u017e0.\n\nthis adjusts \u2432\u017et. up if yrn ) \u2432\u017et. and down if yrn - \u2432\u017et.. for instance,\nwith \u2432 s , you can check that \u2432 s yrn. when \u2432 s yrn, no adjust-\nment occurs and \u2432\u017etq1. s yrn, which is the correct answer for \u2432. for\nstarting values other than , adequate convergence usually takes four or five\niterations.\n\n\u02c6\n\n\u017e1.\n\n\u017et.\n\n1\n2\n\n1\n2\n\nthe convergence of \u2424 to \u2424 for the newton\u1390raphson method is usually\n\n\u02c6\n\n\u017et.\n\nfast. for large t, the convergence satisfies, for each j,\n\n\u2424 y \u2424 f c \u2424 y \u2424\n\u017etq1.\n\u02c6\nj\nj\n\n\u017et.\nj\n\n\u02c6\nj\n\n2\n\nfor some c ) 0\n\nand is referred to as second-order. this implies that the number of correct\ndecimals in the approximation roughly doubles after sufficiently many itera-\ntions. in practice,\nit often takes relatively few iterations for satisfactory\nconvergence.\n\n4.6.2 fisher scoring method\nfisher scoring is an alternative iterative method for solving likelihood equa-\ntions. it resembles the newton\u1390raphson method, the distinction being with\nthe hessian matrix. fisher scoring uses the expected \u00aealue of this matrix,\ncalled the expected information, whereas newton\u1390raphson uses the matrix\nitself, called the obser\u00aeed information.\n\ninformation matrix; that is,\n\n\u017et. denote the approximation t for the ml estimate of the expected\nhas elements ye \u2b78 l \u2424 r\u2b78\u2424 \u2b78\u2424 , evalu-\n\nlet\n\niiiii\n\n\u017et.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n2\n\na\n\nb\n\niiiii\n\n "}, {"Page_number": 161, "text": "146\n\nintroduction to generalized linear models\n\nated at \u2424\u017et.. the formula for fisher scoring is\n\n\u017etq1.\n\n\u2424\n\ns \u2424 q\n\n\u017et.\n\n\u017e\n\niiiii\n\n\u017et.\n\n.\n\ny1\n\n\u017et.\n\nu\n\nor\n\n\u017et.\u2424\u017etq1. s \u017et.\u2424\u017et. q u\u017et..\n\niiiii\n\niiiii\n\n\u017e\n\n4.40\n\n.\n\nfor estimating a binomial parameter, from section 1.3.2 the information is\nw\n\nnr \u2432 1 y \u2432 . a step of fisher scoring gives\n\n.x\n\n\u017e\n\n\u2432 s \u2432 q\n\n\u017etq1.\n\n\u017et.\n\n\u017et.s \u2432 q\n\ny1\n\ny y n\u2432\n\u017et.\n\u2432 1 y \u2432\n\u017e\n\u017et.\n\u017et.\n\n.\n\n.\n\nn\n\n\u017e\n\n\u2432 1 y \u2432\n\u017et.\n\u017et.\ny y n\u2432\u017et.\n\ny\ns .\nn\n\nn\n\n\u02c6\n\n\u017e\n\nformula 4.26 showed that s x wx. similarly,\n\u017et.\n\nthis gives the answer for \u2432 after a single iteration and stays at that value for\nsuccessive iterations.\ns x w x, where\n.\n.x\nw\nw is w see 4.27 evaluated at \u2424 . the estimated asymptotic covariance\n\u02c6 w\n\u02c6y1\nmatrix\noccurs as a by-product of this algorithm as\nii\n\u017e\nfor t at which convergence is adequate. from 4.22 , for both fisher\nii\nscoring and newton\u1390raphson, u has elements\n\nof \u2424 see 4.28\n\n\u017et..y1\n\n.x\n\niiiii\n\niiiii\n\n\u017et.\n\n\u017et.\n\n\u017et.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\nx\n\nx\n\nu s\n\nj\n\n.\n\n\u017e\n\u2b78l \u2424\n\u2b78\u2424\nj\n\ns\n\nn\n\n\u00fd\nis1\n\n\u017e\n\ny y \u242e x \u2b78\u242e\ni\ni\ni\n\u2b78\u2429\nvar y\ni\ni\n\n.\n.\n\n\u017e\n\ni j\n\n.\n\n\u017e\n\n4.41\n\n.\n\n.\n\n\u017e\n\nfor glms with a canonical link, we\u2019ll see section 4.6.4 that the observed\nand expected information are the same. for noncanonical link models, fisher\nscoring has the advantages that it produces the asymptotic covariance matrix\nas a by-product, the expected information is necessarily nonnegative definite,\nand as seen next, it is closely related to weighted least squares methods for\nordinary linear models. however, it need not have second-order convergence,\nand for complex models the observed information is often easier to calculate.\nefron and hinkley 1978 , developing arguments of r. a. fisher, gave\nreasons for preferring observed information. they argued that its variance\nestimates better approximate a relevant conditional variance conditional on\nstatistics not relevant to the parameter being estimated , it is \u2018\u2018closer to the\ndata,\u2019\u2019 and it tends to agree more closely with bayesian analyses.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n4.6.3 ml as iterative reweighted least squares*\na relation exists between weighted least squares estimation and using fisher\nscoring to find ml estimates. we refer here to the general linear model of\n\n "}, {"Page_number": 162, "text": "fitting generalized linear models\n\n147\n\nform\n\nz s x\u2424 q \u2440 .\n\nwhen the covariance matrix of \u2440 is v, the weighted least squares wls\nestimator of \u2424 is\n\n\u017e\n\n.\n\nx\n\ny1\n\nx v x\n\n\u017e\n\ny1\n\n.\n\nx\n\ny1\n\nx v z.\n\nfrom s x wx, expression 4.41 for elements of u, and since diagonal\n\nelements of w are w s \u2b78\u242er\u2b78\u2429 rvar y , it follows that in 4.40 ,\n.\n\n\u017e\n.2\n\niiiii\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nx\n\ni\n\ni\n\ni\n\ni\n\n\u017et.\u2424\u017et. q u\u017et. s xx w \u017et.z\u017et.,\n\niiiii\n\nwhere z\u017et. has elements\n\nz s x \u2424 q y y \u242e\n\u017et.\n\u017et.\ni\ni\n\n\u00fd\n\n\u017et.\nj\n\n\u017e\n\ni j\n\ni\n\nj\n\ns \u2429 q y y \u242e\n\u017et.\ni\n\n\u017et.\ni\n\n\u017e\n\ni\n\n.\n\ni\n\n\u2b78\u2429\u017et.\n\u017et.\u2b78\u242e\ni\n\ni\n\n\u2b78\u2429\u017et.\n\u017et.\u2b78\u242e\ni\n\n.\n\n.\n\n\u017e\n\n.\n\nequations 4.40 for fisher scoring then have the form\nxx w \u017et.x \u2424\u017etq1. s xx w \u017et.z\u017et..\n\n\u017e\n\n.\n\nthese are the normal equations for using weighted least squares to fit a\nlinear model for a response variable z\u017et., when the model matrix is x and the\ninverse of the covariance matrix is w \u017et.. the equations have solution\n\n\u017etq1.\n\n\u2424\n\ns x w x\n\u017et.\n\n\u017e\n\nx\n\ny1\n\n.\n\nx\n\n\u017et.\nx w z\n\n\u017et.\n\n.\n\nthe vector z in this formulation is a linearized form of the link function g,\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n. \u017e\n\nevaluated at y,\ng y f g \u242e q y y \u242e gx \u242e s \u2429 q y y \u242e \u2b78\u2429r\u2b78\u242e s z .\n.\n\u017e\n4.42\nthis adjusted or \u2018\u2018working\u2019\u2019 response \u00aeariable z has element i approximated\nby z\u017et. for cycle t of the iterative scheme. that cycle regresses z\u017et. on x with\nweight\n. this\nestimate yields a new linear predictor value \u2429\u017etq1. s x\u2424\u017etq1. and a new\nadjusted response value z\u017etq1. for the next cycle. the ml estimator results\nfrom iterative use of weighted least squares,\nin which the weight matrix\nchanges at each cycle. the process is called iterati\u00aee reweighted least squares.\na simple way to begin the iterative process uses the data y as the initial\nestimate of \u242e. this determines the first estimate of the weight matrix w and\n\n\u017e\ni.e., inverse covariance w to obtain a new estimate \u2424\n\n\u017etq1.\n\n\u017et.\n\n\u017e\n\n.\n\ni\n\ni\n\n "}, {"Page_number": 163, "text": "148\n\nintroduction to generalized linear models\n\nhence the initial estimate of \u2424. it may be necessary to alter some observa-\ntions slightly for this first cycle only so that g y , the initial value of z, is\nfinite. for instance, when g is the log link applied to counts, a count of\ny s 0 is problematic, so one could set y s . this is not a problem with the\ni\nmodel itself, since the log applies to the mean, and fitted means are usually\nstrictly positive in successive iterations.\n\n\u017e .\n\n1\n2\n\ni\n\n4.6.4 simplifications for canonical links*\ncertain simplifications result with glms using the canonical link. for that\nlink,\n\n\u2429 s \u242a s \u2424 x .\n\n\u00fd\n\ni j\n\ni\n\ni\n\nj\n\nj\n\n\u017e\n\n.\n\noften, a \u243e in the density or mass function 4.14 is identical\nfor all\nobservations, such as for poisson glms a \u243e s 1 and binomial glms with\neach n s 1 for which a \u243e s 1rn s 1 . then the part of the log likelihood\n\u017e\n4.19 involving both parameters and data is \u00fd y \u242a, which simplifies to\n\nw \u017e\nx\n\n.\n\n\u017e\n\n.\n\n.\n\nw\n\ni\n\ni\n\n.\n\n\u017e\nx\n\ni\n\ni\n\n\u017e\n\u00fd \u00fd\n\ny\n\ni\n\ni\n\nj\n\n\u2424 x s \u2424\n\nj\n\ni j\n\n\u017e\n\u00fd \u00fd\n\nj\n\n/\n\n.\n\ny x\ni\n\ni j\n\n/\n\nj\n\ni\n\nsufficient statistics for estimating \u2424 in the glm are then\n\n\u00fd i\n\ny x ,\ni j\n\ni\n\nj s 1, . . . , p.\n\nfor the canonical link,\n\n\u2b78\u242er\u2b78\u2429 s \u2b78\u242er\u2b78\u242a s \u2b78bx \u242a r\u2b78\u242a s by \u242a .\n.\n\n\u017e\n\n.\n\n\u017e\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\u017e\n\n.\n\nthus, the contribution 4.21 to the likelihood equation for \u2424 simplifies to\n\nj\n\ns\n\n\u2b78l\ni\n\u2b78\u2424\nj\n\ny y \u242e\ni\ni\n.\nvar y\ni\n\n\u017e\n\ny\n\nb \u242a x s\n\n\u017e\n\n.\n\ni j\n\ni\n\n\u017e\n\ni\n\ny y \u242e x\n.\ni\n\u017e\n.\na \u243e\n\ni j\n\n.\n\n\u017e\n\n4.43\n\n.\n\nwhen a \u243e is identical for all observations, the likelihood equations are\n\n\u017e\n\n.\n\n\u00fd\n\ni\n\nx y s x \u242e ,\n\n\u00fd\n\ni j\n\ni j\n\ni\n\ni\n\ni\n\nj s 1, . . . , p.\n\n\u017e\n\n4.44\n\n.\n\nthese equations equate the sufficient statistics for the model parameters to\ntheir expected values nelder and wedderburn 1972 . for a normal distribu-\ntion with identity link, these are the normal equations. we obtained these for\n\u017e\npoisson loglinear models in 4.29 and for binomial logistic regression models\nwhen each n s 1 in 4.25 .\n\u017e\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\ni\n\n "}, {"Page_number": 164, "text": "quasi-likelihood and generalized linear models\n\n149\nfrom expression 4.43 for \u2b78l r\u2b78\u2424, with the canonical link the second\n\n\u017e\n\n.\n\ni\n\nj\n\nderivatives of the log likelihood have components\n\n\u2b782l\ni\n\u2b78\u2424\u2b78\u2424\nh\n\nj\n\ns y\n\ni j\n\n\u2b78\u242e\nx\ni\n\u017e\na \u243e \u2b78\u2424\nh\n\n.\n\n\u017e\n\n/\n\n.\n\nthis does not depend on the observation y , soi\n\n\u2b78 l \u2424 r\u2b78\u2424 \u2b78\u2424 s e \u2b78 l \u2424 r\u2b78\u2424 \u2b78\u2424 .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2\n\n2\n\nh\n\nj\n\nh\n\nj\n\nthat is, h s y , and the newton\u1390raphson and fisher scoring algorithms\n.\nare identical for canonical link models nelder and wedderburn 1972 .\n\niiiii\n\n\u017e\n\nj\n\nj\n\ni\n\ni\n\ni j\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u02c6\n\n4.7 quasi-likelihood and generalized linear models*\na glm g \u242e s \u00fd \u2424 x\nspecifies \u242e using a link function g and linear\n.\n\u017e\npredictor. from 4.22 and 4.41 , the ml estimates \u2424 are the solutions of\nthe likelihood equations\n\u017e\n\ny y \u242e x\n.\n\u00fd\n\u00ae \u242e\n.i\n\u017e\nis1\nwhere \u242e s g \u00fd \u2424 x\nand \u00ae \u242e s var y . these equations set the score\n\u017e\nfunctions u \u2424 , which are derivatives of the log likelihood with respect to\n\u0004\n4\u2424 , equal to 0. as we noted in section 4.4.4, the likelihood equations\nj\ndepend on the assumed distribution for y only through \u242e and \u00ae \u242e . the\n.\nchoice of distribution determines the mean\u1390variance relationship \u00ae \u242e .i\n.\n\nj s 1, . . . , p,\n\nu \u2424 s\n\n\u2b78\u242e\ni\n\u2b78\u2429\ni\n\ny1\u017e\n.4\n\ns 0,\n\n4.45\n\n\u017e\n\n/\n\n\u017e\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nn\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u0004\n\ni j\n\ni j\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nj\n\nj\n\nj\n\nj\n\n\u017e\n\n.\n\n4.7.1 mean\u2013variance relationship determines quasi-likelihood estimates\nwedderburn 1974 proposed an alternative approach, quasi-likelihood esti-\nmation, which assumes only a mean\u1390variance relationship rather than a\nspecific distribution for y . it has a link function and linear predictor of the\nusual glm form, but instead of assuming a distributional type for y it\nassumes only\n\ni\n\ni\n\nvar y s \u00ae \u242e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\nfor some chosen variance function \u00ae. the equations that determine quasi-\nlikelihood estimates are the same as the likelihood equations 4.45 for\nglms. they are not likelihood equations, however, without the additional\nassumption that y has distribution in the natural exponential family.\nto illustrate, suppose we assume that the y are independent with\n\n\u017e\n\n.\n\n4\n\n\u0004\n\n4\n\n\u0004\n\ni\n\ni\n\n\u00ae \u242e s \u242e.\n\u017e\n\n.i\n\ni\n\n "}, {"Page_number": 165, "text": "i\n\n4\n\n\u0004\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n150\nintroduction to generalized linear models\nthe quasi-likelihood ql estimates are the solution of 4.45 with \u00ae \u242e\n.\ni\nreplaced by \u242e. under the additional assumption that y have distribution in\nthe exponential dispersion family 4.14 , these estimates are also ml esti-\nmates. that case is simply the poisson distribution. thus, for \u00ae \u242e s \u242e,\nquasi-likelihood estimates are also ml estimates when the random compo-\nnent has a poisson distribution.\n\nwedderburn suggested using the estimating equations 4.45 for any\nvariance function, even if it does not occur for a member of the natural\nexponential family. in fact, the purpose of the quasi-likelihood method was to\nencompass a greater variety of cases, such as discussed in section 4.7.2. the\nql estimates have asymptotic covariance matrix of the same form 4.28 as in\n.\nglms, namely x wx\n\nwith w s \u2b78\u242er\u2b78\u2429 rvar y .\n.\n\nx \u02c6 y1\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n2\n\ni\n\ni\n\ni\n\ni\n\ni\n\n4.7.2 overdispersion for poisson glms and quasi-likelihood\nfor count data, we\u2019ve seen section 4.3.3 that the poisson assumption is\noften unrealistic because of overdispersion\u138fthe variance exceeds the mean.\none cause for this is heterogeneity among subjects. this suggests an alterna-\ntive to a poisson glm in which the mean\u1390variance relationship has the form\n\n\u017e\n\n.\n\n\u00ae \u242e s \u243e\u242e\n\u017e\ni\n\n.i\n\nfor some constant \u243e. the case \u243e) 1 represents overdispersion for the\npoisson model.\nin the estimating equations 4.45 with \u00ae \u242e s \u243e\u242e, \u243e drops out. thus,\nthe equations are identical to likelihood equations for poisson models, and\nmodel parameter estimates are also identical. also,\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\nw s \u2b78\u242er\u2b78\u2429 var y s \u2b78\u242er\u2b78\u2429 r\u243e\u242e ,\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2\n\n2\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n.2\n\n\u02c6i\n\nx \u02c6 y1\n\nso the estimated cov \u2424 s x wx\n.\nis \u243e times that for the poisson model.\nwhen a variance function has the form \u00ae \u242e s \u243e\u00ae* \u242e , usually \u243e is\nalso unknown. however, \u243e is not in the estimating equations. let x 2 s\n\u00fd y y \u242e r\u00ae* \u242e , a pearson-type statistic for the simpler model with\n\u017e\n\u243es 1. then x 2r\u243e is a sum of squares of n standardized terms. when\nx 2r\u243e is approximately chi-squared or when \u242e is approximately linear in \u2424\nwith \u00ae* \u242e close to \u00ae* \u242e , then e x r\u243e f n y p, the number of observa-\ntions minus the number of model parameters p. hence, e x r n y p f \u243e.\nusing the motivation of moment estimation, wedderburn 1974 suggested\ntaking \u243es x r n y p as the estimated multiple of the covariance matrix.\nin summary, this quasi-likelihood approach for count data is simple: fit\nthe ordinary poisson model and use its p parameter estimates. multiply the\nordinary standard error estimates by x r n y p .\n.\n\n2'\n\nwe illustrate for the horseshoe crab data analyzed with poisson glms in\nsection 4.3.2. with the log link, the fit using width to predict number of\n\n.x\n\n2\n\u017e\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nw\n\n2\n\n2\n\ni\n\ni\n\n "}, {"Page_number": 166, "text": "\u02c6\n\nquasi-likelihood and generalized linear models\n151\nsatellites was log \u242es y3.305 q 0.164 x, with se s 0.020 for \u2424s 0.164. to\nimprove the adequacy of using a chi-squared statistic to summarize fit, we\nuse the satellite totals and fit for all female crabs at a given width, to increase\nthe counts and fitted values relative to those for individual female crabs. the\nn s 66 distinct width levels each have a total count y for the number of\nsatellites and a fitted total \u242e. the pearson statistic comparing these is\nx 2 s 174.3. the quasi-likelihood adjustment\nfor standard errors equals\n'174.3r 66 y 2 s 1.65. thus, se s 1.65 0.020 s 0.033 is a more plausible\n.\nstandard error for \u2424s 0.164 in this prediction equation.\n\n\u02c6i\n\n\u02c6\n\n\u02c6\n\nalternative ways of handling overdispersion include mixture models that\nallow heterogeneity in the mean at fixed settings of predictors. for count\ndata these include poisson glms having random effects section 13.5 and\nnegative binomial glms that result when a poisson parameter itself has a\n.\ngamma distribution section 4.3.4 and 13.4 .\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\ni\n\n4.7.3 overdispersion for binomial glms and quasi-likelihood\nthe quasi-likelihood approach can also handle overdispersion for counts\nbased on binary data. when y is the sample mean of n independent binary\ni s 1, . . . , n, then binomial sampling has\nobservations with parameter \u2432 ,\ni\ne y s \u2432 and var y s \u2432 1 y \u2432 rn . a simple quasi-likelihood approach\n.\n\u017e\nuses the alternative variance function\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u00ae \u2432 s \u243e\u2432 1 y \u2432 rn .\n\u017e\n\n.\n4.46\noverdispersion occurs when \u243e) 1. the quasi-likelihood estimates are the\nsame as ml estimates for the binomial model, since \u243e drops out of the\nestimating equations 4.45 . as in the overdispersed poisson case, \u243e enters\nthe denominator of w . thus, the asymptotic covariance matrix multiplies by\n\u243e, and standard errors multiply by \u243e. an estimate of \u243e using the x fit\nstatistic for the ordinary binomial model is x r n y p finney 1947 .\n.\n\n'\n\n. \u017e\n\nmethods like these that use estimates from ordinary models but inflate\ntheir standard errors are appropriate only if the model chosen describes well\nthe structural relationship between the mean of y and the predictors. if a\nlarge goodness-of-fit statistic is due to some other type of lack of fit, such as\nfailing to include a relevant interaction term, making an adjustment for\noverdispersion will not address the inadequacy.\n\nfor counts with binary data, alternative mechanisms for handling overdis-\npersion include mixture models such as binomial glms with random effects\n\u017e\nsection 12.3 and models for which a binomial parameter itself has a beta\n.\ndistribution section 13.3 .\n\n\u017e\n\n.\n\n\u017e\n\n2\n\n2\n\n4.7.4 teratology overdispersion example\ntable 4.5 shows results of a teratology experiment in which female rats on\niron-deficient diets were assigned to four groups. rats in group 1 were given\nplacebo injections, and rats in other groups were given injections of an iron\n\n "}, {"Page_number": 167, "text": "152\n\nintroduction to generalized linear models\n\ntable 4.5 response counts of litter size, number dead for 58 litters of rats\nin low-iron teratology study\n\n)\n\n(\n\ngroup 1: untreated low iron\n. \u017e\n. \u017e\n\n\u017e\n.\n10, 1 11, 4 12, 9 4, 4 10, 10 11, 9 9, 9 11, 11 10, 10 10, 7 12, 12\n\u017e\n.\n10, 9 8, 8 11, 9 6, 4 9, 7 14, 14 12, 7 11, 9 13, 8 14, 5 10, 10\n.\n\u017e\n12, 10 13, 8 10, 10 14, 3 13, 13 4, 3 8, 8 13, 5 12, 12\n\n. \u017e\n. \u017e\n. \u017e\n\n. \u017e\n. \u017e\n\n. \u017e\n. \u017e\n\n. \u017e\n. \u017e\n\n. \u017e\n. \u017e\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n\u017e\n. \u017e\n. \u017e\n\n.\n\ngroup 2: injections days 7 and 10\n\n\u017e\n.\n10, 1 3, 1 13, 1 12, 0 14, 4 9, 2 13, 2 16, 1 11, 0 4, 0 1, 0 12, 0\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n.\u017e\n\ngroup 3: injections days 0 and 7\n.\n\u017e\n8, 0 11, 1 14, 0 14, 1 11, 0\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\ngroup 4: injections weekly\n. \u017e\n\n. \u017e\n\n. \u017e\n\n\u017e\n.\n3, 0 13, 0 9, 2 17, 2 15, 0 2, 0 14, 1 8, 0 6, 0 17, 0\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n.\nsource: moore and tsiatis 1991 .\n\n\u017e\n\nsupplement; this was done weekly in group 4, only on days 7 and 10 in group\n2, and only on days 0 and 7 in group 3. the 58 rats were made pregnant,\nsacrificed after three weeks, and then the total number of dead fetuses was\ncounted in each litter. in teratology experiments, due to unmeasured covari-\nates and genetic variability the probability of death may vary from litter to\nlitter within a particular treatment group.\n\ni\u017e g .\n\nlet y\n\ndenote the proportion of dead fetuses out of the n\n\nin litter i in\ntreatment group g. let \u2432 denote the probability of death for a fetus in\n,\u2432 variate, where\nthat litter. consider the model with n\n\na bin n\n\ni\u017e g .\n\ni\u017e g .\n\ny\n\n\u017e\n\n.\n\ni\u017e g .\n\ni\u017e g .\n\ni\u017e g .\n\ni\u017e g .\n\n\u2432 s \u2432 ,\ni\u017e g .\n\ng\n\ng s 1, 2, 3, 4.\n\nthat is, the model treats all litters in a particular group g as having the\nsame probability of death \u2432 . the ml fit has estimate \u2432 equal\nto\nthe sample proportion of deaths for all fetuses from litters in that group.\nthese equal \u2432 s 0.758 se s 0.024 , \u2432 s 0.102 se s 0.028 , \u2432 s 0.034\nse s 0.024 , and \u2432 s 0.048 se s 0.021 , where for group g, se s\n\u017e\n' \u017e\n\u2432 1 y \u2432 r \u00fd n\n\u02c6\n. the estimated probability of death is considerably\ng\n\n\u02c6\n.\n.\n\n\u02c64\ni\u017e g .\n\n\u02c6\ng\n\n\u02c6\ng\n\n\u02c6\n\n\u02c6\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n1\n\n2\n\n3\n\ng\n\ni\n\nhigher for the placebo group.\n\ni\n\n\u02c6i\u017e g .\ng\n\nin group g, n \u2432 is a fitted number of deaths and\nfor litter\n1 y \u2432 is a fitted number of nondeaths. comparing these fitted values\n\u017e\n.\n\u02c6\nn\ni\u017e g .\ng\nto the observed counts of deaths and nondeaths in the n s 58 litters using\nthe pearson statistic gives x 2 s 154.7 with df s 58 y 4 s 54. there is\nconsiderable evidence of overdispersion. with the quasi-likelihood approach,\n\u2432 are the same as the binomial ml estimates; however, \u243es x r n y p\n\u017e\n\u0004\n.\n\u02c6g\ns 154.7r 58 y 4 s 2.86, so standard errors multiply by \u243e s 1.69.\n\n\u02c6\n\u02c61r2\n\neven with this adjustment for overdispersion, strong evidence remains that\nthe probability of death is substantially higher for the placebo group. for\n\n\u017e\n\n.\n\n4\n\n2\n\n "}, {"Page_number": 168, "text": "generalized additive models\ninstance, a 95% confidence interval for \u2432 y \u2432 is\n\n1\n\n2\n\n0.758 y 0.102 \" 1.96 1.69 = 0.024 q 1.69 = 0.028\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n2\n\n\u017e\n\n153\n\n1r2\n\n2\n\n.\n.\n0.54, 0.78 .\n\nor\n\n\u017e\n\n\u017e\n\nthis is wider, however, than the wald interval of 0.59, 0.73 for comparing\nindependent proportions, which ignores the overdispersion.\n\n.\n\n4.8 generalized additive models*\n\nthe glm generalizes the ordinary linear model to permit nonnormal distri-\nbutions and modeling functions of the mean. quasi-likelihood provides a\nfurther generalization, specifying how the variance depends on the mean\nwithout assuming a given distribution. another generalization replaces the\nlinear predictor by smooth functions of the predictors.\n\n4.8.1 smoothing data\nthe glm structure g \u242e s \u00fd \u2424 x generalizes to\n\u017e\n\n.\n\nj\n\nj\n\ni j\n\ni\n\ng \u242e s s\n\u00fd\n\u017e\n\n.\n\ni\n\nj\n\n\u017e\n\nj\n\n.\n\n,\n\nx\n\ni j\n\n\u017e .\n\nj\n\nwhere s \u2b48 is an unspecified smooth function of predictor j. a useful smooth\nfunction is the cubic spline. it has separate cubic polynomials over sets of\ndisjoint intervals, joined together smoothly at boundaries of those intervals.\nlike glms, this model specifies a distribution for the random component\nand a link function g. the resulting model is called a generalized additi\u00aee\nmodel, symbolized by gam hastie and tibshirani 1990 . the glm is the\nspecial case in which each s is a linear function. also possible is taking some\ns as smooth functions and others as linear functions or as dummy variables\nj\nfor qualitative predictors.\n\n\u017e\n\n.\n\nj\n\nthe details for fitting gams are beyond our scope. the fitting algorithm\nemploys a generalization of the newton\u1390raphson method that utilizes local\nsmoothing. this corresponds to subtracting from the log-likelihood function a\npenalty function that increases as the smooth function gets more wiggly. the\nmodel fit assigns a deviance and an approximate df value to each s in the\nadditive predictor, enabling inference about those terms. for instance, a\nsmooth function having df s 5 is similar in overall complexity to a fourth-\ndegree polynomial, which has five parameters. one\u2019s choice of a df value or\nsmoothing parameter determines how smooth the resulting gam fit looks.\nit is usually worth trying a variety of degrees of smoothing to find one that\nsmooths the data sufficiently so that the trend is not too irregular but does\n\n.\n\n\u017e\n\nj\n\n "}, {"Page_number": 169, "text": "154\n\nintroduction to generalized linear models\n\nnot smooth so much that it suppresses interesting patterns. this approach\nmay suggest that a linear model is adequate with a particular link or suggest\nways to improve on linearity. some software packages that do not have\ngams can smooth the data by employing a type of regression that gives\ngreater weight to nearby observations in predicting the value at a given point;\nsuch locally weighted least squares regression is often referred to as lowess. we\nprefer gams because they recognize explicitly the form of the response. for\ninstance, with a binary response, lowess can give predicted values below 0 or\nabove 1, which cannot happen with a gam.\n\neven when one plans to use glms, a gam can be helpful for exploratory\nanalysis. for instance, for continuous x with continuous responses, scatter\ndiagrams provide visual information about the dependence of y on x. for\nbinary responses, the following example shows that such diagrams are not\nvery informative. plotting the fitted smooth function for a predictor may\nreveal a general trend without assuming a particular functional relationship.\n\nfigure 4.7 whether satellites are present 1, yes; 0, no , by width of female crab, with\nsmoothing fit of generalized additive model.\n\n\u017e\n\n.\n\n "}, {"Page_number": 170, "text": "notes\n\n155\n\n4.8.2 gams for horseshoe crab example\nin section 4.3.2, figure 4.4 showed the trend relating number of satellites for\nhorseshoe crabs to their width. this smooth curve is the fit of a generalized\nadditive model, assuming a poisson distribution and using the log link.\nin the next chapter we\u2019ll use logistic regression to model the probability\nthat a crab has at least one satellite. for crab i, let y s 1 if she has at least\none satellite and y s 0 otherwise. figure 4.7 plots these data against\nx s crab width. it consists of a set of points with y s 1 and a second set of\npoints with y s 0. the numbered symbols indicate the number of observa-\ntions at each point. it appears that y s 1 tends to occur relatively more\noften at higher x values. figure 4.7 also shows a curve based on smoothing\nthe data using a gam, assuming a binomial response and logit link. this\ncurve shows a roughly increasing trend and is more informative than viewing\nthe binary data alone. it suggests that an s-shaped regression function may\ndescribe this relationship relatively well.\n\ni\n\ni\n\ni\n\ni\n\ni\n\nnotes\n\nsection 4.1: generalized linear model\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n4.2. distribution 4.1 is called a natural or linear exponential family to distinguish it from a\n\u017e\nin the exponential term. for\nmore general exponential family that replaces y by r y\nother generalizations, see jorgensen 1987 . books on glms and related models, in\napproximate order of technical level from highest to lowest, are mccullagh and nelder\n\u017e\n.\n1989 , fahrmeir and tutz 2001 , aitkin et al. 1989 , dobson 2002 , and gill 2000 .\n.\nsee also firth 1991 .\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u2c91\n\nsection 4.3: generalized linear models for counts\n\n4.2. for further discussion of poisson regression and related models for count data, see\nbreslow 1984 , cameron and trivedi 1998 , frome 1983 , hinde 1982 , lawless\n\u017e\n1987 , and seeber 1998 and references therein.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nsection 4.4: moments and likelihood for generalized linear models\n\n\u017e .\n4.3. the function b \u2b48\n\nin 4.14 is called the cumulant function, since when a \u243e s 1 its\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\nderivatives yield the cumulants of the distribution jorgensen 1987 .\n\n\u017e\n\n\u2c91\n\nfor many glms, including poisson models with log link and binary models with logit\nlink, with full-rank model matrix the hessian is negative definite and the log likelihood\nis a strictly concave function. then ml estimates of model parameters exist and are\n.\nunique under quite general conditions wedderburn 1976 .\n\n\u017e\n\nsection 4.5: inference for generalized linear models\n\n4.4. the matrix w used in cov \u2424 see 4.28 , in the hat matrix for standardized pearson\nresiduals see 4.38 , and in fisher scoring see 4.40 is the inverse of the covariance\nmatrix of the linearized form of g y\n\n.x\n.\nsee section 4.6.3 .\n\n\u017e . \u017e\n\n.x\n\n\u017e\n\n\u017e\n\nw\n\nw\n\n\u02c6\u017e\n\n. w\n\n\u017e\n\n.x\n\n "}, {"Page_number": 171, "text": "156\n\nintroduction to generalized linear models\n\n\u017e\n\nmccullagh and nelder 1989, chap. 12 discussed model checking for glms. for\ndiscussions about residuals, see also green 1984 , pierce and schafer 1986 , pregibon\n\u017e\n1980, 1981 , and williams 1987 . pregibon 1982 showed that the squared standardized\npearson residual is the score statistic for testing whether the observation is an outlier.\ndavison and hinkley 1997, sec. 7.2 discussed bootstrapping in glms.\n\n.\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\u017e\n\u017e\n\nsection 4.6: fitting generalized linear models\n\n\u017e\n\n.\n\n4.5. fisher 1935b introduced the fisher scoring method to calculate ml estimates for\nprobit models. for further discussion of glm model fitting and the relationship\n.\nbetween iterative reweighted least squares and ml estimation, see green 1984 ,\n.\n\u2c91\njorgensen 1983 , mccullagh and nelder 1989 , and nelder and wedderburn 1972 .\ngreen 1984 , jorgensen 1983 , and palmgren and ekholm 1987 also discussed this\nrelation for exponential family nonlinear models.\n\n.\n\u2c91\n\n\u017e\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nsection 4.7: quasi-likelihood and generalized linear models\n\n.\n\n4.6. for more on quasi-likelihood, see sections 11.4, 12.6.4, and 13.3, breslow 1984 , cox\n\u017e\n1983 , firth 1987 , hinde and demetrio 1998 , mccullagh 1983 , mccullagh and\nnelder 1989 , nelder and pregibon 1987 , and wedderburn 1974, 1976 . see heyde\n\u017e\n1997 for a theoretical perspective.\n\n\u00b4\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nsection 4.8: generalized additi\u00a9e models\n\n4.7. besides gams, other nonparametric smoothing methods can describe the dependence\nof a binary response on a predictor. for instance, see copas 1983 , lloyd 1999, chap.\n.\n5 , and section 15.3.3 for kernel smoothing and kauermann and tutz 2001 for models\nwith random effects.\n\n\u017e\n.\n\n\u017e\n\n.\n\n\u017e\n\nproblems\n\napplications\n\n4.1\n\n\u017e\n\n.\n\nin the 2000 u.s. presidential election, palm beach county in florida\nwas the focus of unusual voting patterns including a large number of\nillegal double votes apparently caused by a confusing \u2018\u2018butterfly ballot.\u2019\u2019\nmany voters claimed that they voted mistakenly for the reform party\ncandidate, pat buchanan, when they intended to vote for al gore.\nfigure 4.8 shows the total number of votes for buchanan plotted\nagainst the number of votes for the reform party candidate in 1996\n\u017e\nross perot , by county in florida. for details, see a. agresti and b.\npresnell, j. law public policy, volume 13, fall 2001, 117\u1390134.\na. in county i, let \u2432 denote the proportion of the vote for buchanan\nand let x denote the proportion of the vote for perot in 1996. for\nthe linear probability model fitted to all counties except palm beach\ncounty, \u2432 s y0.0003 q 0.0304 x . give the value of p in the\n\n\u017e\n\n.\n\n.\n\ni\n\ni\n\n\u02c6i\n\ni\n\n "}, {"Page_number": 172, "text": "problems\n\n157\n\nfigure 4.8 total vote, by county in florida, for reform party candidates buchanan in 2000\nand perot in 1996.\n\ninterpretation: the estimated proportion vote for buchanan in 2000\nwas roughly p% of that for perot in 1996.\n\nb. for palm beach county, \u2432 s 0.0079 and x s 0.0774. does this\n\nresult appear to be an outlier? explain.\n.x\n\nc. for logistic regression, log \u2432r 1 y \u2432 s y7.164 q 12.219 x . find\n\u02c6i\n\u2432 in palm beach county. is that county an outlier for this model?\n\n\u02c6\n\n\u02c6\n\n\u017e\n\nw\n\ni\n\ni\n\ni\n\ni\n\ni\n\n4.2 for games in baseball\u2019s national league during nine decades, table\n4.6 shows the percentage of times that the starting pitcher pitched a\ncomplete game.\n\ntable 4.6 data for problem 4.2\n\ndecade\n1900\u13901909\n1910\u13901919\n1920\u13901929\n\npercent\ncomplete\n\n72.7\n63.4\n50.0\n\ndecade\n\n1930\u13901939\n1940\u13901949\n1950\u13901959\n\npercent\ncomplete\n\n44.3\n41.6\n32.8\n\ndecade\n\n1960\u13901969\n1970\u13901979\n1980\u13901989\n\npercent\ncomplete\n\n27.2\n22.5\n13.3\n\nsource: data from george will, newsweek, apr. 10, 1989.\n\n "}, {"Page_number": 173, "text": "158\n\nintroduction to generalized linear models\n\n.\n\n\u017e\n\n\u02c6\n\na. treating the number of games as the same in each decade, the ml\nfit of the linear probability model is \u2432s 0.7578 y 0.0694 x, where\nx s decade x s 1, 2, . . . , 9 . interpret 0.7578 and y0.0694.\n\nb. substituting x s 10, 11, 12, predict the percentages of complete\ngames for the next three decades. are these predictions plausible?\nwhy?\n\nw\nc. the ml fit with logistic regression is \u2432s exp 1.148 y 0.315 x r 1\nq exp 1.148 y 0.315 x . obtain \u2432 for x s 10, 11, 12. are these\nmore plausible?\n\n\u02c6i\n\n.x\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\n4.3 for table 3.7 with scores 0, 0.5, 1.5, 4.0, 7.0 for alcohol consumption,\nml fitting of the linear probability model for malformation has output.\n\n\u017e\n\n.\n\nparameter\nintercept\nalcohol\n\nestimate\n0.0025\n0.0011\n\nstd error\n\n0.0003\n0.0007\n\nwald 95% conf limits\n0.0019 0.0032\ny0.0003 0.0025\n\ninterpret the model fit. use it to estimate the relative risk of malfor-\nmation for alcohol consumption levels 0 and 7.0.\n\n4.4 for table 4.2, refit the linear probability model or the logistic regres-\nsion model using the scores a 0, 2, 4, 6 , b 0, 1, 2, 3 , and c 1, 2,\n3, 4 . compare \u2424 for the three choices. compare fitted values. sum-\nmarize the effect of linear transformations of scores, which preserve\nrelative sizes of spacings between scores.\n\n. \u017e . \u017e\n\n\u017e . \u017e\n\n\u017e . \u017e\n\n\u02c6\n\n.\n\n.\n\n4.5 for table 4.3, let y s 1 if a crab has at least one satellite, and y s 0\n\notherwise. using x s weight, fit the linear probability model.\na. use ordinary least squares. interpret the parameter estimates. find\n.\nthe estimated probability at the highest observed weight 5.20 kg .\ncomment.\n\n\u017e\n\nb. try to fit the model using ml, treating y as binomial. the failure\nis due to a fitted probability falling outside the 0, 1 range. the fit\nin part a is ml for a normal random component, for which fitted\nvalues outside this range are permissible.\n\n\u017e .\n\n\u017e\n\n.\n\nx\n\nw\n\nc. fit the logistic regression model. show that the fitted probability at\n\na weight of 5.20 kg equals 0.9968.\n\nd. fit the probit model. find the fitted probability at 5.20 kg.\n\n4.6 an experiment analyzes imperfection rates for two processes used to\nfabricate silicon wafers for computer chips. for treatment a applied to\n10 wafers, the numbers of imperfections are 8, 7, 6, 6, 3, 4, 7, 2, 3, 4.\ntreatment b applied to 10 other wafers has 9, 9, 8, 14, 8, 13, 11, 5, 7, 6\n\n "}, {"Page_number": 174, "text": "problems\n\n159\n\na\n\nimperfections. treat the counts as independent poisson variates having\nmeans \u242e and \u242e .\nb\na. fit the model log \u242es \u2423q \u2424x, where x s 1 for treatment b and\nx s 0 for treatment a. show that exp \u2424 s \u242e r\u242e , and interpret\nits estimate.\n\nb. test h : \u242e s \u242e with the wald or likelihood ratio test of\n\n\u017e\n\n.\n\na\n\nb\n\nh : \u2424s 0. interpret.\n\na\n\nb\n\n0\n\n0\n\nstruct one for \u2424.\n\nc. construct a 95% confidence interval for \u242e r\u242e . hint: first con-\n\n\u017e\n\nb\n\na\n\nd. test h : \u242e s \u242e based on this result: if y and y are indepen-\ndent poisson with means \u242e and \u242e , then y y q y is binomial\nwith n s y q y and \u2432s \u242e r \u242e q \u242e .\n.\n\n\u017e\n\n\u017e\n\n.\n\na\n\nb\n\n0\n\n1\n\n2\n\n1\n\n2\n\n1\n\n1\n\n2\n\n<\n\n1\n\n1\n\n2\n\n1\n\n.\n\n2\n\n4.7 for table 4.3, table 4.7 shows sas output for a poisson loglinear\n\nmodel fit using x s weight and y s number of satellites.\na. estimate e y for female crabs of average weight, 2.44 kg.\nb. use \u2424 to describe the weight effect. show how to construct the\n\n\u02c6\n\n\u017e\n\n.\n\nreported confidence interval.\n\nc. construct a wald test that y is independent of x. interpret.\nd. can you conduct a likelihood-ratio test of this hypothesis? if not,\n\nwhat else do you need?\n\ne. is there evidence of overdispersion? if necessary, adjust standard\n\nerrors and interpret.\n\ntable 4.7 sas output for problem 4.7\n\ncriterion\ndeviance\npearson chi- square\nlog likelihood\n\ndf\n171\n171\n\nvalue\n\n560.8664\n535.8957\n71.9524\n\nparameter estimate std error wald 95% conf limits chi- sq pr > chisq\nintercept y0.4284\n0.5893\nweight\n\ny0.7791 y0.0777\n0.7167\n\n0.0167\n<.0001\n\n0.1789\n0.0650\n\n5.73\n82.15\n\n0.4619\n\n4.8 refer to problem 4.7. using the identity link with x s weight, \u242es\u02c6\ny2.60 q 2.264 x, where \u2424s 2.264 has se s 0.228. repeat parts a\n\u017e .\n\u017e .\nthrough c .\n\n\u02c6\n\n4.9 refer to table 4.3.\n\na. fit a poisson loglinear model using both w s weight and c s\ncolor to predict y s number of satellites. assigning dummy vari-\nables, treat c as a nominal factor. interpret parameter estimates.\n\n "}, {"Page_number": 175, "text": "160\n\nintroduction to generalized linear models\n\nb. estimate e y for female crabs of average weight 2.44 kg that are\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e .\ni medium light, and ii dark.\n\n\u017e .\n\nc. test whether color is needed in the model. hint: from section\n4.5.4, the likelihood-ratio statistic comparing models is the differ-\nence in deviances.\n\n.\n\n\u017e\n\nd. the estimated color effects are monotone across the four cate-\ngories. fit a simpler model that\ntreats c as quantitative and\nassumes a linear effect. interpret its color effect and repeat the\nanalyses of parts b and c . compare the fit to the model in part\n\u017e .a . interpret.\n\n\u017e .\n\n\u017e .\n\ne. add width to the model. what effect does the strong positive\ncorrelation between width and weight have? are both needed in the\nmodel?\n\n4.10 in section 4.3.2, refer to the poisson model with identity link. the fit\nusing least squares is \u242es y10.42 q 0.51 x se s 0.11 . explain why\nthe parameter estimates differ and why the se values are so different.\n\n\u02c6\n\n\u017e\n\n.\n\n4.11 for the negative binomial model fitted to the crab satellite counts with\nlog link and width predictor, \u2423s y4.05, \u2424s 0.192 se s 0.048 ,\n.\nk s 1.106 se s 0.197 . interpret. why is se for \u2424 so different from\n\u02c6y1\nse s 0.020 for the corresponding poisson glm in sec 4.3.2? which is\nmore appropriate? why?\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n4.12 refer to problem 4.6. the sample mean and variance are 5.0 and 4.2\n\nfor treatment a and 9.0 and 8.4 for treatment b.\na. is there evidence of overdispersion for the poisson model having a\n\ndummy variable for treatment? explain.\n\nb. fit the negative binomial loglinear model. note that the estimated\ndispersion parameter is 0 and that estimates of treatment means\nand standard errors are the same as with the poisson loglinear\nglm.\n\nc. for the overall sample of 20 observations, the sample mean and\nvariance are 7.0 and 10.2. fit the loglinear model having only an\nintercept term under poisson and negative binomial assumptions.\ncompare results, and compare confidence intervals for the overall\nmean response. why do they differ? note: this shows how the\npoisson model can deteriorate when an important covariate is\nunmeasured.\n\n.\n\n\u017e\n\n4.13 table 4.8 shows the free-throw shooting, by game, of shaq o\u2019neal of\nthe los angeles lakers during the 2000 nba basketball playoffs.\ncommentators remarked that his shooting varied dramatically from\ngame to game. in game i, suppose that y s number of free throws\n\n\u017e\n\n.\n\ni\n\n "}, {"Page_number": 176, "text": "problems\n\ntable 4.8 data for problem 4.13\n\n161\n\nnumber number of\n\nnumber number of\ngame made attempts game made attempts game made attempts\n\nnumber number of\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n4\n5\n5\n5\n2\n7\n6\n9\n\n5\n11\n14\n12\n7\n10\n14\n15\n\n9\n10\n11\n12\n13\n14\n15\n16\n\n4\n1\n13\n5\n6\n9\n7\n3\n\n12\n4\n27\n17\n12\n9\n12\n10\n\nsource: www.nba.com.\n\n17\n18\n19\n20\n21\n22\n23\n\n8\n1\n18\n3\n10\n1\n3\n\n12\n6\n39\n13\n17\n6\n12\n\n\u017e\n\n.\n\ni\n\nmade out of n attempts is a bin n , \u2432 variate and the y are\nindependent.\n\na. fit the model, \u2432 s \u2423, and find and interpret \u2423 and its standard\nerror. does the model appear to fit adequately? note: you could\ncheck this with a small-sample test of independence of the 23 = 2\n.\ntable of game and the binary outcome.\n\n\u02c6\n\n\u017e\n\ni\n\ni\n\ni\n\ni\n\n\u0004\n\n4\n\nb. adjust the standard error for overdispersion. using the original se\nand its correction, find and compare 95% confidence intervals for\n\u2423. interpret.\n\n4.14 refer to table 13.6. fit a loglinear model with a dummy variable for\nrace, a assuming a poisson distribution, and b allowing overdisper-\nsion with a quasi-likelihood approach. compare results.\n\n\u017e .\n\n\u017e .\n\n4.15 refer to problem 4.6. the wafers are also classified by thickness of\nz s 0, low; z s 1, high . the first five imperfection\nsilicon coating\ncounts reported for each treatment refer to z s 0 and the last five\nrefer to z s 1. analyze these data.\n\n\u017e\n\n.\n\n14.6 refer to table 13.9 on frequency of sexual intercourse. analyze these\n\ndata.\n\ntheory and methods\n\n4.17 describe the purpose of the link function of a glm. what is the\nidentity link? explain why it is not often used with binomial or poisson\nresponses.\n\n4.18 for known k, show that the negative binomial distribution 4.12 has\n.x\nexponential family form 4.1 with natural parameter log \u242er \u242eq k .\n\n\u017e\n\n.\n\nw\n\n.\n\n\u017e\n\u017e\n\n "}, {"Page_number": 177, "text": "162\n\nintroduction to generalized linear models\n\n4.19 for binary data, define a glm using the log link. show that effects\nrefer to the relative risk. why do you think this link is not often used?\n\u017e\nhint: what happens if the linear predictor takes a positive value?\n\n.\n\n4.20 for the logistic regression model 4.6 with \u2424) 0, show that a as\nx \u2122 \u2b01, \u2432 x is monotone increasing, and b the curve for \u2432 x is the\ncdf of a logistic distribution having mean y\u2423r\u2424 and standard devia-\ntion \u2432r \u2424 3 .\n.\n\n\u017e\n.\n'\u017e\n\n\u017e .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e .\n\n4.21 show representation 4.18 for the binomial distribution.\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\n.\n\n\u017e\n\n4.22 let y be a bin n , \u2432 variate for group i,\n\ni s 1, . . . , n, with y\n4\n\u0004\nindependent. consider the model that \u2432 s \u2b48\u2b48\u2b48 s \u2432 . denote that\ncommon value by \u2432. for observations y , show that \u2432s \u00fd y r \u00fdn .\n.\nwhen all n s 1, for testing this model\u2019s fit in the n = 2 table, show\nthat x 2 s n. thus, goodness-of-fit statistics can be completely unin-\nformative for ungrouped data. see also problem 5.37.\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n4\n\nn\n\n1\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n4.23 suppose that y is poisson with g \u242e s \u2423q \u2424x , where x s 1 for\ni s 1, . . . , n from group a and x s 0 for i s n q 1, . . . , n q n\nb\nfrom group b. show that for any link function g, the likelihood\nequations 4.22 imply that fitted means \u242e and \u242e equal the sample\nmeans.\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n.\n\n\u017e\n\na\n\na\n\na\n\na\n\nb\n\ni\n\ni\n\ni\n\ni\n\n4.24 for binary data with sample proportion y based on n trials, we use\nquasi-likelihood to fit a model using variance function 4.46 . show\nthat parameter estimates are the same as for the binomial glm but\nthat the covariance matrix multiplies by \u243e.\n\n\u017e\n\n.\n\ni\n\ni\n\n4.25 a binomial glm \u2432 s \u233d \u00fd \u2424 x with arbitrary inverse link function\n.\n\u233d assumes that n y has a bin n , \u2432 distribution. find w in 4.27\nand hence cov \u2424 . for logistic regression, show that w s n \u2432 1 y \u2432 .\n.\n\n$\n\n\u02c6\u017e\n\ni j\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nj\n\nj\n\ni\n\ni\n\ni\n\ni\n\n4.26 a glm has parameter \u2424 with sufficient statistic s. a goodness-of-fit\ntest statistic t has observed value t . if \u2424 were known, a p-value is\np s p t g t ; \u2424 . explain why p t g t s is the uniform minimum\n\u017e\nvariance unbiased estimator of p.\n\n.\n\n\u017e\n\n.\n\no\n\no\n\no\n\n<\n\ni\n\ni j\n\n4.27 let y be observation j of a count variable for group i, i s 1, . . . , i,\nj s 1, . . . , n . suppose that y are independent poisson with e y\n.\n\u017e\ns \u242e.i\na. show that the ml estimate of \u242e is \u242e s y s \u00fd y rn .\ni w\nb. simplify the expression for the deviance for this model. for testing\nthis model, it follows from fisher 1970, p. 58, originally published\n\n\u02c6\n\n\u017e\n\n\u0004\n\n4\n\ni j\n\ni j\n\ni j\n\ni\n\ni\n\ni\n\nj\n\n "}, {"Page_number": 178, "text": "problems\n\n.\n\n163\nin 1925 that the deviance and the pearson statistic \u00fd \u00fd y y y ry\nhave approximate chi-squared distributions with df s \u00fd n y 1 .\n.\nfor a single group, cochran 1954 referred to \u00fd y y y ry as\nthe \u00aeariance test\nfor the fit of a poisson distribution, since it\nx\ncompares the sample variance to the estimated poisson variance y .1\n\n\u017e\n.\n\n1 j\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\ni j\n\n2\n\n2\n\n1\n\n1\n\ni\n\ni\n\ni\n\ni\n\ni\n\nj\n\nj\n\n4.28 conditional on \u242d, y has a poisson distribution with mean \u242d. values of\n\u242d vary according to gamma density 13.12 , which has e \u242d s \u242e,\nvar \u242d s \u242e rk. show that marginally y has the negative binomial\ndistribution 4.12 . explain why the negative binomial model is a way\nto handle overdispersion for the poisson.\n\n\u017e .\n\n\u017e .\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n2\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n4.29 consider the class of binary models 4.8 and 4.9 . suppose that the\nstandard cdf \u233d corresponds to a probability density function \u243e that is\nsymmetric around 0.\na. show that x at which \u2432 x s 0.5 is x s y\u2423r\u2424.\nb. show that the rate of change in \u2432 x when \u2432 x s 0.5 is \u2424\u243e 0 .\n\u017e .\n\u017e\nshow this is 0.25\u2424 for the logit link and \u2424r 2\u2432 where \u2432s 3.14 . . .\n.\nfor the probit link.\n\n'\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\nc. show that the probit regression curve has the shape of a normal cdf\n\nwith mean y\u2423r\u2424 and standard deviation 1r \u2424 .\n<\n\n<\n\n4.30 show the normal distribution n \u242e, \u2434 with fixed \u2434 satisfies family\n\u017e\n.4.1 , and identify the components. formulate the ordinary regression\nmodel as a glm.\n\n\u017e\n\n2.\n\n4.31 in problem 4.30, when \u2434 is also a parameter, show that it satisfies the\n\n.\nexponential dispersion family 4.14 .\n\n\u017e\n\n4.32 for binary observations,\n\nthe model \u2432 x s q\n1r\u2432 tan \u2423q \u2424x . which distribution has cdf of this form? explain\n\u017e\nwhen a glm using this curve might be more appropriate than logistic\nregression.\n\nconsider\n\ny1\u017e\n\n1\n2\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e .\n4.33 find the form of the deviance residual 4.35 for an observation in a a\nbinomial glm, and b poisson glm. illustrate part b for a cell\ncount in a two-way contingency table for the model of independence.\n\n\u017e .\n\n\u017e .\n\n\u017e\n\n.\n\n4.34 consider the value \u2424 that maximizes a function l \u2424 . let \u2424 denote\n\n\u017e0.\n\n\u017e\n\n.\n\n\u02c6\n\nan initial guess.\na. using l \u2424 s l \u2424 q \u2424y \u2424 l \u2424 q \u2b48\u2b48\u2b48 , argue that for\nx \u02c6\n.\n\u017e\n\u2424 close to \u2424, approximately 0 s l \u2424 q \u2424y \u2424 l \u2424 .\n.\n.\n\u017e0.\n\u017e0.\n\u017e0.\n\u02c6\nsolve this equation to obtain an approximation \u2424 for \u2424.\n\n\u017e0.\n.\n\u017e0.\n\n\u02c6\n\u017e1.\n\ny\n\u017e\n\n\u02c6\n\n\u02c6\n\n\u017e0.\n\n\u017e0.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\ny\n\nx\n\nx\n\n "}, {"Page_number": 179, "text": "164\n\nintroduction to generalized linear models\nb. let \u2424 denote approximation t for \u2424, t s 0, 1, 2, . . . . justify that\n\n\u02c6\n\n\u017et.\n\nthe next approximation is\n\n\u2424\u017etq1. s \u2424\u017et. y lx \u2424\u017et. rly \u2424\u017et.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n4.35 for n independent observations from a poisson distribution, show that\nfisher scoring gives \u242e s y for all t ) 0. by contrast, what happens\nwith newton\u1390raphson?\n\n\u017etq1.\n\n\u017e0.\n\n\u017e .\n\n4.36 write a computer program using the newton\u1390raphson algorithm to\nmaximize the likelihood for a binomial sample. for \u2432s 0.3 based on\nn s 10, print out results of the first six iterations when the starting\nvalue \u2432 is a 0.1, b 0.2, . . . , i 0.9. summarize the effects of the\nstarting value on speed of convergence. what happens if it is 0 or 1?\n4.37 in a glm, suppose that var y s \u00ae \u242e for \u242es e y . show that the\n\u017e\n\u017e\n.xy1r2\nlink g satisfying g \u242e s \u00ae \u242e\nhas the same weight matrix w at\neach cycle. show this link for a poisson random component is g \u242e s 2\n'\u242e.\n\nw \u017e\n\n\u017e .\n\n\u017e .\n\n\u02c6\n\nx\u017e\n\n\u017et.\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n4.38 for noncanonical links in a glm, show that the observed information\nmatrix may depend on the data and hence differs from the expected\ninformation. illustrate using the probit model.\n\n "}, {"Page_number": 180, "text": "c h a p t e r 5\n\nlogistic regression\n\nin introducing generalized linear models for binary data in chapter 4 we\nhighlighted logistic regression. this is the most important model for categori-\ncal response data. it is used increasingly in a wide variety of applications.\nearly uses were in biomedical studies but the past 20 years have also seen\nmuch use in social science research and marketing.\n\nrecently, logistic regression has become a popular tool in business appli-\ncations. some credit-scoring applications use logistic regression to model the\nprobability that a subject is credit worthy. for instance, the probability that a\nsubject pays a bill on time may use predictors such as the size of the bill,\nannual income, occupation, mortgage and debt obligations, percentage of\nbills paid on time in the past, and other aspects of an applicant\u2019s credit\nhistory. a company that relies on catalog sales may determine whether to\nsend a catalog to a potential customer by modeling the probability of a sale\nas a function of indices of past buying behavior.\n\nanother area of increasing application is genetics. for instance, one\nrecent article j. m. henshall and m. e. goddard, genetics 151:885\u1390894,\n1999 used logistic regression to estimate quantitative trait\nloci effects,\nmodeling the probability that an offspring inherits an allele of one type\ninstead of another type as a function of phenotypic values on various traits\nfor that offspring. another recent article d. f. levinson et al., amer. j.\nhum. genet., 67:652\u1390663, 2000 used logistic regression for analysis of the\ngenotype data of affected sibling pairs asps and their parents from several\nresearch centers. the model studied the probability that asps have identity-\nby-descent allele sharing and tested its heterogeneity among the centers.\n\nin this chapter we study logistic regression more closely. section 5.1 covers\nparameter interpretation. in section 5.2 we present inferential methods for\nthose parameters. sections 5.3 and 5.4 generalize to multiple predictors,\nsome of which may be qualitative. finally, in section 5.5 we apply glm\nmodel-fitting methods to determine and solve likelihood equations for logis-\ntic regression.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n165\n\n "}, {"Page_number": 181, "text": "166\n\nlogistic regression\n\ninterpreting parameters in logistic regression\n\n5.1\nfor a binary response variable y and an explanatory variable x, let \u2432 x s\n.\n\u017e\np y s 1 x s x s 1 y p y s 0 x s x . the logistic regression model is\n\u017e\n\n\u017e\n\n.\n\n.\n\n<\n\n<\n\n\u2432 x s\n\n\u017e\n\n.\n\n\u017e\n\nexp \u2423q \u2424x\n\n.\n1 q exp \u2423q \u2424x\n\n\u017e\n\n.\n\n.\n\nequivalently, the log odds, called the logit, has the linear relationship\n\nlogit \u2432 x s log\n\n\u017e\n\n.\n\n\u2432 x\u017e\n\n.\n1 y \u2432 x\u017e\n\n.\n\ns \u2423q \u2424x.\n\nthis equates the logit link function to the linear predictor.\n\n\u017e\n\n5.1\n\n.\n\n\u017e\n\n5.2\n\n.\n\n<\n\n<\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\ninterpreting \u2424: odds, probabilities, and linear approximations\n5.1.1\n\u017e\n\u017e\nhow can we interpret \u2424 in 5.2 ? its sign determines whether \u2432 x\nis\nincreasing or decreasing as\nx increases. the rate of climb or descent\nincreases as \u2424 increases; as \u2424\u2122 0 the curve flattens to a horizontal straight\nline. when \u2424s 0, y is independent of x. for quantitative x with \u2424) 0, the\ncurve for \u2432 x has the shape of the cdf of the logistic distribution recall\nsection 4.2.5 . since the logistic density is symmetric, \u2432 x approaches 1 at\nthe same rate that it approaches 0.\n\n\u017e\n.\n\nexponentiating both sides of 5.2 shows that the odds are an exponential\nfunction of x. this provides a basic interpretation for the magnitude of \u2424:\nthe odds increase multiplicatively by e \u2424 for every 1-unit increase in x. in\nother words, e \u2424 is an odds ratio, the odds at x s x q 1 divided by the odds\nat x s x.\n\nmost scientists are not familiar with odds or logits, so the interpretation of\na multiplicative effect of e \u2424 on the odds scale or an additive effect of \u2424 on\nthe logit scale is not helpful to them. a simpler, although approximate slope\ninterpretation uses a linearization argument berkson 1951 . since it has a\n.\ncurved rather than a linear appearance, the logistic regression function 5.1\nimplies that the rate of change in \u2432 x per unit change in x varies. a\nstraight line drawn tangent to the curve at a particular x value, shown in\nfigure 5.1, describes the rate of change at that point. calculating \u2b78\u2432 x r\u2b78x\nusing 5.1 yields a fairly complex function of the parameters and x, but it\n.x\nsimplifies to the form \u2424\u2432 x 1 y \u2432 x .\nfor instance, the line tangent to the curve at x for which \u2432 x s has\ns \u2424r4; when \u2432 x s 0.9 or 0.1, it has slope 0.09\u2424. the slope\nslope \u2424\napproaches 0 as \u2432 x approaches 1.0 or 0. the steepest slope occurs at x for\nwhich \u2432 x s ; that x value is x s y\u2423r\u2424. to check that \u2432 x s at this\n\n\u017e .\u017e .\n1\n1\n2\n2\n\n.w\n\n1\n2\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nw\n\n1\n2\n\n1\n2\n\n "}, {"Page_number": 182, "text": "interpreting parameters in logistic regression\n\n167\n\nfigure 5.1 linear approximation to logistic regression curve.\n\nx\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n1\n2\n\n1\n2\n\n1\n4\n\n50\n\n50\n\n.\u017e\n\npoint, substitute y\u2423r\u2424 for x in 5.1 , or substitute \u2432 x s in 5.2 and\nsolve for x. this x value is sometimes called the median effecti\u00aee le\u00aeel and\ndenoted el . in toxicology studies it is called ld ld s lethal dose , the\ndose with a 50% chance of a lethal result.\nfrom this linear approximation, near x where \u2432 x s , a change in x of\n1r\u2424 corresponds to a change in \u2432 x of roughly 1r\u2424 \u2424r4 s ; that\nis, 1r\u2424 approximates the distance between x values where \u2432 x s 0.25 or\n0.75 in reality, 0.27 and 0.73 and where \u2432 x s 0.50. the linear approxima-\ntion works better for smaller changes in x, however.\n\n.\n\u017e\n\n.\n.\n\nan alternative way to interpret the effect reports the values of \u2432 x at\ncertain x values, such as their quartiles. this entails substituting those\nquartiles for x into formula 5.1 for \u2432 x . the change in \u2432 x over the\nmiddle half of x values, from the lower quartile to the upper quartile of x,\nthen describes the effect. it can be compared to the corresponding change\nover the middle half of values of other predictors.\nthe intercept parameter \u2423 is not usually of particular interest. however,\nby centering the predictor about 0 i.e., replacing x by x y x , \u2423 becomes\nthe logit at that mean, and thus e r 1 q e s \u2432 x . as in ordinary\nregression, centering is also helpful in complex models containing quadratic\nor interaction terms to reduce correlations among model parameter esti-\nmates.\n\n.x\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nw\n\n\u2423\n\n\u2423\n\n "}, {"Page_number": 183, "text": "168\n\nlogistic regression\n\n.\n\n\u017e\n\n5.1.2 looking at the data\nin practice, these interpretations use formula 5.1 with ml estimates substi-\ntuted for parameters. before fitting the model and making such interpreta-\ntions, look at the data to check that the logistic regression model is appropri-\nate. since y takes only values 0 and 1, it is difficult to check this by plotting y\nagainst x.\n\nit can be helpful to plot sample proportions or logits against x. let ni\ndenote the number of observations at setting i of x. of them, let y denote\nthe number of\nis\n.x\n.x\n. this is not finite when y s 0 or n . an\nlog p r 1 y p s log y r n y y\nad hoc adjustment adds a positive constant to the number of outcomes of the\ntwo types. the adjustment\n\np s y rn . sample logit\n\n\u2018\u20181\u2019\u2019 outcomes, with\n\n\u017e\n\n\u017e\n\nw\n\nw\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nlog\n\ny q\n\ni\n\n1\n2\n\nn y y q\n\ni\n\ni\n\n1\n2\n\n\u017e\n\n.\n\ni\n\ni\n\nis the least-biased estimator of this form of the true logit note 5.2 . the plot\nof sample logits should be roughly linear.\nwhen x is continuous and all n s 1, or when it is essentially continuous\nand all n are small, this is unsatisfactory. one could group the data with\nnearby x values into categories before calculating sample proportions and\nsample logits. a better approach that does not require choosing arbitrary\ncategories uses a smoothing mechanism to reveal trends. one such smoothing\napproach fits a generalized additive model section 4.8 , which replaces the\nlinear predictor of a glm by a smooth function. inspect a plot of the fit\nto see if severe discrepancies occur from the s-shaped trend predicted\nby logistic regression.\n\n\u017e\n\n.\n\n5.1.3 horseshoe crabs revisited\nto illustrate logistic regression, we reanalyze the horseshoe crab data intro-\nduced in section 4.3.2. the binary response is whether a female crab has any\nmale crabs residing nearby satellites : y s 1 if she has at least one satellite,\nand y s 0 if she has none. we first use as a predictor the female crab\u2019s\nwidth.\n\n\u017e\n\n.\n\n\u017e\n\nfigure 4.7 plotted the data and showed the smoothed prediction of the\nmean provided by a generalized additive model gam , assuming a binomial\nresponse and logit link. the logistic regression model appears to be ade-\nquate. this is also suggested by the grouping of the data used to investigate\nthe adequacy of poisson regression models in section 4.3.2 table 4.4 . in\neach of the eight width categories, we computed the sample proportion of\ncrabs having satellites and the mean width for the crabs in that category.\nfigure 5.2 shows eight dots representing the sample proportions of female\ncrabs having satellites plotted against the mean widths for the eight cate-\n\n\u017e\n\n.\n\n.\n\n "}, {"Page_number": 184, "text": "interpreting parameters in logistic regression\n\n169\n\nfigure 5.2 observed and fitted proportions of satellites by width of female crab.\n\ngories. the eight plotted sample proportions and the gam smoothing curve\nboth show a roughly increasing trend, so we proceed with fitting the logistic\nregression model with linear width predictor.\n\nwe defer to section 5.5 details about ml fitting. software e.g., for sas\nsee table a.8 reports output such as table 5.1 exhibits. for the ungrouped\ndata from table 4.3, let \u2432 x denote the probability that a female horseshoe\ncrab of width x has a satellite. the ml fit is\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u2432 x s\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\nexp y12.351 q 0.497x\n\n.\n1 q exp y12.351 q 0.497x\n\n\u017e\n\n.\n\n.\n\ntable 5.1 computer output for logistic regression model with horseshoe\ncrab data\n\ncriteria for assessing goodness of fit\ncriterion\ndeviance\npearson chi- square\nlog likelihood\n\nvalue\n194.4527\n165.1434\ny97.2263\n\ndf\n171\n171\n\nparameter\nestimate\nintercept y12.3508\n0.4972\nwidth\n\nstd\n\nlikelihood- ratio\nerror\n95% conf limits\n2.6287 y17.8097 y7.4573\n0.7090\n0.1017\n\n0.3084\n\nwald\n\nchi- sq\n22.07\n23.89\n\np>chisq\n<.0001\n<.0001\n\n "}, {"Page_number": 185, "text": ".\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n1\n2\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\u017e\n\n170\nlogistic regression\nsubstituting x s 26.3 cm, the mean width level in this sample, \u2432 x s 0.674.\nthe estimated probability equals when x s y\u2423r\u2424s 12.351r0.497 s 24.8.\nfigure 5.2 plots \u2432 x against width.\nthe estimated odds of a satellite multiply by exp \u2424 s exp 0.497 s 1.64\nfor each 1-cm increase in width; that is, there is a 64% increase. to convey\nthe effect less technically, we could report the incremental rate of change in\nthe probability of a satellite. at the mean width, \u2432 x s 0.674, and \u2432 x\n.\n\u017e\nincreases by about \u2424 \u2432 x 1 y \u2432 x s 0.497 0.674 0.326 s 0.11 for a\n1-cm increase in width. or, we could report \u2432 x at the quartiles of x. the\nlower quartile, median, and upper quartile for width are 24.9, 26.1, and 27.7;\n.\u2432 x at those values equals 0.51, 0.65, and 0.81, increasing by 0.30 over the x\n\u02c6\nvalues for the middle half of the sample.\nthe latter summary is useful for comparing the effects of predictors having\ndifferent units. for instance, with crab weight as the predictor, logit \u2432 x s\u02c6\ny3.695 q 1.815 x. a 1-kg increase in weight is not comparable to a 1-cm\nincrease in width, so \u2424s 0.497 for x s width is not comparable to \u2424s 1.815\nfor x s weight. the quartiles for weight are 2.00, 2.35, and 2.85; \u2432 x at\nthose values are 0.48, 0.64, and 0.81, increasing by 0.33 over the middle half\nof the sampled weights. the effect is similar to that of width.\n\n.\n.\u017e\n\n..x\n\n\u02c6w\n\n\u017e\n.\n\n.x\n\n.\u017e\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\nw\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n5.1.4 logistic regression with retrospective studies\nanother property of logistic regression relates to situations in which the\nexplanatory variable x rather than the response variable y is random. this\noccurs with retrospective sampling designs, such as case\u1390control biomedical\nstudies section 2.1.6 . for samples of subjects having y s 1 cases and\nhaving y s 0 controls , the value of x is observed. evidence exists of an\nassociation if the distribution of x values differs between cases and controls.\nin retrospective studies, one can estimate odds ratios section 2.2.4 . effects\nin the logistic regression model refer to odds ratios. thus, one can fit such\nmodels and estimate effects in case\u1390control studies.\nhere is a justification for this. let z indicate whether a subject is sampled\n1 s yes, 0 s no . let \u2433 s p z s 1 y s 1 denote the probability of sam-\n\u017e\npling a case, and let \u2433 s p z s 1 y s 0 denote the probability of sampling\na control. even though the conditional distribution of y given x s x is not\nsampled, we need a model for p y s 1 z s 1, x , assuming that p y s 1 x\n.\n<\nfollows the logistic model. by bayes\u2019 theorem,\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n1\n\n0\n\n<\n\n<\n\n<\n\np y s 1 z s 1, x s\n\u017e\n\n.\n\n<\n\n<\n\np z s 1 y s 1, x p y s 1 x\n\u017e\n<\n\u00fd\n1\njs0\n\n.\np z s 1 y s j, x p y s j x\n\u017e\n<\n\n\u017e\n.\n\n.\n\n\u017e\n\n<\n\n\u017e\n\n5.3\n\n.\n\n.\n\n.\n\nnow, suppose that p z s 1 y, x s p z s 1 y for y s 0 and 1; that is, for\neach y, the sampling probabilities do not depend on x. for instance, often x\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n<\n\n<\n\n "}, {"Page_number": 186, "text": "interpreting parameters in logistic regression\n\n171\n\nrefers to exposure of some type, such as whether someone has been a\nsmoker. then, for cases and for controls, the probability of being sampled is\nthe same for smokers and nonsmokers. under this assumption, substituting\n\u2433 and \u2433 in 5.3 and dividing numerator and denominator by p y s 0 x ,\n.\n1\n\u017e\n.5.3 simplifies to\n\n\u017e\n\n.\n\n\u017e\n\n0\n\n<\n\np y s 1 z s 1, x s\n\u017e\n\n.\n\n<\n\n\u017e\n\n\u2433 exp \u2423q \u2424x\n\n.\n\u2433 q \u2433 exp \u2423q \u2424x\n\n\u017e\n\n1\n\n0\n\n1\n\n.\n\n.\n\nthen, dividing numerator and denominator by \u2433 and using \u2433 r\u2433 s\nexp log \u2433 r\u2433 yields\n\n.x\n\n\u017e\n\nw\n\n0\n\n1\n\n0\n\n1\n\n0\n\n<\n\n0\n\n1\n\n\u017e\n\n.\n\nlogit p y s 1 z s 1, x s \u2423* q \u2424x\n\n\u017e\nwith \u2423* s \u2423q log \u2433 r\u2433 .\n.\nthus, the logistic regression model holds with the same effect parameter \u2424\nas in the model for p y s 1 x . if the sampling rate for cases is 10 times that\nfor controls, the intercept estimated is log 10 s 2.3 larger than the one\nestimated with a prospective study. for related comments, see anderson\n\u017e\n1972 , breslow and day 1980, p. 203 , breslow and powers 1978 , carroll\net al. 1995 , farewell 1979 , mantel 1973 , prentice 1976a , and prentice\n.\nand pyke 1979 .\n\n.\n\u017e\n\n\u017e\n.\n\n.\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n<\n\nwith case\u1390control studies, one cannot estimate \u2424 in other binary-\nresponse models. unlike the odds ratio,\nfor the conditional\ndistribution of x given y does not then equal that for y given x. this is an\nimportant advantage of the logit link and is a major reason why logit models\nhave surpassed other models in popularity in biomedical studies.\n\nthe effect\n\nmany case\u1390control studies employ matching. each case is matched with\none or more control subjects. the controls are like the case on key character-\nistics such as age. the model and subsequent analysis should take the\nmatching into account. in section 10.2.5 we discuss logistic regression for\nmatched case\u1390control studies.\n\n2.\ni\nequals\n\ntheorem, p y s 1 x s x\n\nregardless of the sampling mechanism, logistic regression may or may not\ndescribe a relationship well. in one special case, it necessarily holds. given\ni s 0, 1. then, by\nthat y s i, suppose that x has n \u242e, \u2434 distribution,\n5.1 with \u2424s \u242e y \u242e r\u2434\n\u017e\n2\nbayes\u2019\n\u017e\ncornfield 1962 . when a population is a mixture of two types of subjects,\none type with y s 1 that is approximately normally distributed on x and the\nother type with y s 0 that is approximately normal on x with similar\nvariance, the logistic regression function 5.1 approximates well the curve for\n.\u2432 x . if the distributions are normal but with different variances, the model\napplies also having a quadratic term anderson 1975 . in that case, the\nrelationship is nonmonotone, with \u2432 x increasing and then decreasing, or\n.\nthe reverse problem 5.33 .\n\n\u017e\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n1\n\n0\n\n<\n\n "}, {"Page_number": 187, "text": "172\n\nlogistic regression\n\n\u017e\n\ninference for logistic regression\n\n5.2\nby wald\u2019s 1943 asymptotic results for ml estimators, parameter estimators\nin logistic regression models have large-sample normal distributions. thus,\ninference can use the wald,\ntriad of methods\n.\n\u017e\nsection 1.3.3 .\n\nlikelihood-ratio, score\n\n\u017e\n\n.\n\n.\n\n5.2.1 types of inference\nfor the model with a single predictor,\n\nlogit \u2432 x s \u2423q \u2424x,\n\n\u017e\n\n.\n\n0\n\n0\n\n0\n\n\u02c6\n\nsignificance tests focus on h : \u2424s 0, the hypothesis of independence. the\nwald test uses the log likelihood at \u2424, with test statistic z s \u2424rse or its\nsquare; under h , z 2 is asymptotically \u24392. the likelihood-ratio test uses\ntwice the difference between the maximized log likelihood at \u2424 and at \u2424s 0\nand also has an asymptotic \u24392 null distribution. the score test uses the log\nlikelihood at \u2424s 0 through the derivative of the log likelihood i.e., the\nscore function at that point. the test statistic compares the sufficient\n2x\nstatistic for \u2424 to its null expected value, suitably standardized n 0, 1 or \u2439 .1\nin section 5.3.5 present this test of h : \u2424s 0.\n\n\u02c6\n\n\u02c6\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nw\n\n1\n\n1\n\n<\n\n<\n\n0\n\nw\n\n\u02c6\n\nfor large samples, the three tests usually give similar results. the likeli-\nhood-ratio test is preferred over the wald. it uses more information, since it\nincorporates the log likelihood at h as well as at \u2424. when \u2424 is relatively\nlarge, the wald test is not as powerful as the likelihood-ratio test and can\neven show aberrant behavior see hauck and donner 1977 and problem\nx\n5.38 .\nconfidence intervals are more informative than tests. an interval for \u2424\nresults from inverting a test of h : \u2424s \u2424 . the interval is the set of \u2424 for\nwhich the chi-squared test statistic is no greater than \u2439 \u2423 s z\n. for the\nis \u2424\"\nwald approach,\n\u017e\nz\n\u2423r2\nfor summarizing the relationship, other characteristics may have greater\nimportance than \u2424, such as \u2432 x at various x values. for fixed x s x ,0\nlogit \u2432 x s \u2423q \u2424x has a large-sample se given by the estimated square\n\u02c6\nroot of\n\nthis means \u2424y \u2424 rse f z\n\n2\n\u2423r2\n; the interval\n\n2\u017e .\n1\n\n.\nse .\n\n0\n\u02c6\n\n2\n\u2423r2\n\nw\u017e\n\n.x\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nx\n\nw\n\n0\n\n0\n\n2\n\n0\n\n0\n\n0\n\n\u017e\n\n.0\n\n\u02c6\n\n\u02c6\n\n\u017e .\n\u02c6\n\nvar \u2423q \u2424x s var \u2423 q x var \u2424 q 2 x cov \u2423, \u2424 .\n.\n\u02c6\n\n.\nis \u2423q \u2424x \" 1.96 se. substi-\n\u017e\n.\na 95% confidence interval for logit \u2432 x\ntuting each endpoint into the inverse transformation \u2432 x s exp logit r\n.\nw\n1 q exp logit gives a corresponding interval for \u2432 x .0\n.\neach method of inference can also produce small-sample confidence\n\n.x\n\n.x\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n\u017e\n\n2\n0\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\nw\n\n0\n\n0\n\n0\n\n0\n\nintervals and tests. we defer discussion of this until section 6.7.\n\n "}, {"Page_number": 188, "text": "inference for logistic regression\n\n173\n\n0\n\n.\n\n\u017e\n\n\u02c6\n\ninference for horseshoe crab data\n\n5.2.2\nwe illustrate logistic regression inferences with the model for the probability\na horseshoe crab has a satellite, with width as the predictor. table 5.1\nshowed the fit and standard errors. the statistic z s \u2424rse s 0.497r0.102 s\n4.9 provides strong evidence of a positive width effect p - 0.0001 . the\nequivalent wald chi-squared statistic, z 2 s 23.9, has df s 1. the maximized\nlog likelihoods equal y112.88 under h : \u2424s 0 and y97.23 for the full\nmodel. the likelihood-ratio statistic equals y2 y112.88 y 97.23 s 31.3,\n.\nwith df s 1. this provides even stronger evidence than the wald test.\n\nthe wald 95% confidence interval for \u2424 is 0.497 \" 1.96 0.102 , or 0.298,\n0.697 . table 5.1 reports a likelihood-ratio confidence interval of 0.308,\n0.709 , based on the profile likelihood function. the confidence interval for\ns\nthe effect on the odds per 1-cm increase in width equals\n\u017e\n1.36, 2.03 . we infer that a 1-cm increase in width has at least a 36%\nincrease and at most a doubling in the odds of a satellite.\n\n\u017e 0.308\ne\n\n0.709 .\n\n, e\n\n.\n.\n\n\u017e\n\u017e\n\n. \u017e\n\nmost software for logistic regression also reports estimates and confidence\n\u017e\nintervals for \u2432 x\ne.g., proc genmod in sas with the obstats\noption . consider this for crabs of width x s 26.5, near the mean width. the\nestimated logit is y12.351 q 0.497 26.5 s 0.825, and \u2432 x s 0.695. soft-\nware reports\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n$\nvar \u2423 s 6.910,\n\n\u017e .\u02c6\n\n$\nvar \u2424 s 0.01035,\n\n\u02c6\n\n\u017e\n\n.\n\n$\ncov \u2423, \u2424 s y0.2668,\n\n\u02c6\n\n\u017e\n\n\u02c6\n\n.\n\nfrom which\n$\nvar logit \u2432 x s 6.910 q x 0.01035 q 2 x y0.2668 .\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n4\n\n\u0004\n\n2\n\n\u02c6\n\nw\n\nw\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n. \u017e\n\n'\n.\n\n.x\nat x s 26.5 this is 0.038, so the 95% confidence interval for logit \u2432 26.5\nequals 0.825 \" 1.96 0.038 , or 0.44, 1.21 . this translates to the interval\n.x\n0.61, 0.77 for the probability of satellites e.g., exp 0.44 r 1 q exp 0.44 s\n\u017e\n0.61 . alternatively, for the model fit using predictor x* s x y 26.5, \u2423 and\nits se are the estimated logit and its se. figure 5.3 plots the confidence\nbands around the prediction equation for \u2432 x as a function of x. hauck\n\u017e\n1983 gave alternative bands for which the confidence coefficient applies\nsimultaneously to all possible predictor values.\n\n.\n\u017e\n\n\u02c6\n\none could ignore the model fit and simply use sample proportions i.e.,\n.\nthe saturated model\nto estimate such probabilities. six female crabs in the\nsample had x s 26.5, and four of them had satellites. the sample proportion\nestimate at x s 26.5 is \u2432s 4r6 s 0.67, similar to the model-based estimate.\nthe 95% score confidence interval section 1.4.2 based on these six observa-\n.\ntions alone equals 0.30, 0.90 .\n\n\u02c6\n\nwhen the logistic regression model truly holds, the model-based estimator\nof a probability is considerably better than the sample proportion. the model\nhas only two parameters to estimate, whereas the saturated model has a\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n "}, {"Page_number": 189, "text": "174\n\nlogistic regression\n\nfigure 5.3 prediction equation and 95% confidence bands for probability of satellite as a\nfunction of width.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u02c6\n\n\u02c6\n\n'\n\n'\n\u017e\n\nseparate parameter for every distinct value of x. for instance, at x s 26.5,\nsoftware reports se s 0.04 for the model-based estimate 0.695, whereas the\nse is \u2432 1 y \u2432 rn s 0.67 0.33 r6 s 0.19 for the sample proportion\nof 0.67 with only 6 observations. the 95% confidence intervals are 0.61,\n0.77 using the model versus 0.30, 0.90 using the sample proportion. instead\nof using only 6 observations, the model uses the information that all 173\nobservations provide in estimating the two model parameters. the result is a\nmuch more precise estimate.\n\n. \u017e\n\nreality is a bit more complicated. in practice, the model is not exactly the\ntrue relationship between \u2432 x and x. however, if it approximates the true\nprobabilities decently, its estimator still tends to be closer than the sample\nproportion to the true value. the model smooths the sample data, somewhat\ndampening the observed variability. the resulting estimators tend to be\nbetter unless each sample proportion is based on an extremely large sample.\nsection 6.4.5 discusses this advantage of using models.\n\n\u017e\n\n.\n\n\u017e\n\n5.2.3 checking goodness of fit: ungrouped and grouped data\nin practice, there is no guarantee that a certain logistic regression model fits\nthe data well. for any type of binary data, one way to detect lack of fit uses a\nlikelihood-ratio test to compare the model to more complex ones. a more\ncomplex model might contain a nonlinear effect, such as a quadratic term.\nmodels with multiple predictors would consider interaction. if more complex\nmodels do not fit better, this provides some assurance that the model chosen\nis reasonable.\n\n "}, {"Page_number": 190, "text": "inference for logistic regression\n\n175\n\nother approaches to detecting lack of fit search for any way that the\nmodel fails. this is simplest when the explanatory variables are solely\ncategorical, as we\u2019ll illustrate in section 5.4.3. at each setting of x, one can\nmultiply the estimated probabilities of the two outcomes by the number of\nsubjects at that setting to obtain estimated expected frequencies for y s 0\nand y s 1. these are fitted \u00aealues. the test of the model compares the\nobserved counts and fitted values using a pearson x 2 or likelihood-ratio g2\nstatistic. for a fixed number of settings, as the fitted counts increase, x 2 and\ng2 have limiting chi-squared null distributions. the degrees of freedom,\ncalled the residual df for the model, subtract the number of parameters in the\n\u017e\nmodel from the number of parameters in the saturated model\ni.e., the\n.\nnumber of settings of x .\n\nthe reason for the restriction to categorical predictors for a global test of\nfit relates to the distinction in section 4.5.3 that we mentioned between\ngrouped and ungrouped data for binomial models. the saturated model\ndiffers in the two cases. an asymptotic chi-squared distribution for the\ndeviance results as n \u2122 \u2b01 with a fixed number of parameters in that model\nand hence a fixed number of settings of predictor values.\n\n5.2.4 goodness of fit of model for horseshoe crabs\nwe illustrate with a goodness-of-fit analysis for the model using x s width to\npredict the probability that a female crab has a satellite. one way to check it\ncompares it\nto a more complex model, such as the model containing\na quadratic term. with width centered at 0 by subtracting its mean of 26.3,\nthat model has fit\n\nlogit \u2432 x s 0.618 q 0.533 x q 0.040 x .\n\n\u017e\n\n.\n\n2\n\n\u02c6\n\n2\n\n\u017e\n\nthe quadratic estimate has se s 0.046. there is not much evidence to\nsupport adding that term. the likelihood-ratio statistic for testing that the\ntrue coefficient of x\n\nis 0 equals 0.83 df s 1 .\n.\n\nwe next consider overall goodness of fit. width takes 66 distinct values for\nthe 173 crabs, with few observations at most widths. one can view the data as\na 66 = 2 contingency table. the two cells in each row count the number of\ncrabs with satellites and the number of crabs without satellites, at that width.\nthe chi-squared theory for x 2 and g2 applies when the number of levels of\nx is fixed, and the number of observations at each level grows. although we\ngrouped the data using the distinct width values rather than using 173\nseparate binary responses, this theory is violated here in two ways. first, most\nfitted counts are very small. second, when more data are collected, addi-\ntional width values would occur, so the contingency table would contain more\ncells rather than a fixed number. because of this, x 2 and g2 for logistic\nregression models with continuous or nearly continuous predictors do not\nhave approximate chi-squared distributions. normal approximations can be\n\n\u017e\n\n "}, {"Page_number": 191, "text": "176\n\nlogistic regression\n\ntable 5.2 grouping of observed and fitted values for fit of logistic\nregression model to horseshoe crab data\n\n\u017e\n\n.\nwidth cm\n- 23.25\n\n23.25\u139024.25\n24.25\u139025.25\n25.25\u139026.25\n26.25\u139027.25\n27.25\u139028.25\n28.25\u139029.25\n) 29.25\n\nnumber\n\nyes\n5\n4\n17\n21\n15\n20\n15\n14\n\nnumber\n\nno\n9\n10\n11\n18\n7\n4\n3\n0\n\nfitted\nyes\n3.64\n5.31\n13.78\n24.23\n15.94\n19.38\n15.65\n13.08\n\nfitted\nno\n10.36\n8.69\n14.22\n14.77\n6.06\n4.62\n2.35\n0.92\n\n.\n\nmore appropriate, but no single method has received much attention; see\nsection 9.8.6 for references.\n\none could use x 2 and g2 to compare the observed and fitted values in\ngrouped form. table 5.2 uses the groupings of table 4.4, giving an 8 = 2\ntable. in each width category, the fitted value for a yes response is the sum of\nthe estimated probabilities \u2432 x for all crabs having width in that category;\nthe fitted value for a no response is the sum of 1 y \u2432 x for those crabs. the\nfitted values are then much larger. then, x 2 and g2 have better validity,\nalthough the chi-squared theory still is not perfect since \u2432 x is not constant\nin each category. their values are x 2 s 5.3 and g2 s 6.2. table 5.2 has\neight binomial samples, one for each width setting; the model has two\nparameters, so df s 8 y 2 s 6. neither x 2 nor g2 shows evidence of lack of\nfit p ) 0.4 . thus, we can feel more comfortable about using the model for\nthe original ungrouped data.\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n5.2.5 checking goodness of fit with ungrouped data by grouping\nas just noted, with ungrouped data or with continuous or nearly continuous\npredictors, x 2 and g2 do not have limiting chi-squared distributions. they\nare still useful for comparing models, as done above for checking a quadratic\nterm and as we will discuss in sections 5.4.3 and 9.8.5. also, as just noted,\none can apply them in an approximate manner to grouped observed and\nfitted values for a partition of the space of x values. as the number of\nexplanatory variables increases, however, simultaneous grouping of values for\neach variable can produce a contingency table with a large number of cells,\nmost of which have small counts.\n\nregardless of the number of predictors, one can partition observed and\nfitted values according to the estimated probabilities of success using the\noriginal ungrouped data. one common approach forms the groups in the\npartition so they have approximately equal size. with 10 groups, the first pair\n\n "}, {"Page_number": 192, "text": "177\nlogit models with categorical predictors\nof observed counts and corresponding fitted counts refers to the nr10\nobservations having the highest estimated probabilities, the next pair refers\nto the nr10 observations having the second decile of estimated probabilities,\nand so on. each group has an observed count of subjects with each outcome\nand a fitted value for each outcome. the fitted value for an outcome is the\nsum of the estimated probabilities for that outcome for all observations in\nthat group.\n\nthis construction is the basis of a test due to hosmer and lemeshow\n\u017e\n1980 . they proposed a pearson statistic comparing the observed and fitted\ncounts for this partition. let y denote the binary outcome for observation j\nj s 1, . . . , n . let \u2432 denote the\nin group i of the partition,\ncorresponding fitted probability for the model fitted to the ungrouped data.\ntheir statistic equals\n\ni s 1, . . . , g,\n\n\u02c6\n\n.\n\ni j\n\ni j\n\ni\n\ng\n\n\u00fd \u00fd \u2432 1 y \u00fd \u2432 rn\n\n\u00fd y y \u00fd \u2432\u02c6\ni j\n.\n\u02c6\n\n\u017e\n\u02c6\n\n.\n.\n\n\u017e\n\n\u017e\n\ni j\n\n2\n\nis1\n\ni j\n\ni j\n\nj\n\nj\n\nj\n\nj\n\n.\n\ni\n\nwhen many observations have the same estimated probability, there is some\narbitrariness in forming the groups, and different software may report some-\nwhat different values. this statistic does not have a limiting chi-squared\ndistribution, because the observations in a group are not identical trials, since\nthey do not share a common success probability. however, hosmer and\nlemeshow noted that when the number of distinct patterns of covariate\nvalues equals the sample size, the null distribution is approximated by\nchi-squared with df s g y 2.\nfor the logistic regression fit to the horseshoe crab data with continuous\nwidth predictor, the hosmer\u1390lemeshow statistic with g s 10 groups equals\n3.5, with df s 8. it also indicates a decent fit.\n\nunfortunately,\n\nlike other proposed global fit statistics,\n\nthe hosmer\u1390\nlemeshow statistic does not have good power for detecting particular types\nof lack of fit hosmer et al. 1997 . in any case, a large value of a global fit\nstatistic merely indicates some lack of fit but provides no insight about its\nnature. the approach of comparing the working model to a more complex\none is more useful from a scientific perspective, since it searches for lack of\nfit of a particular type. for either approach, when the fit is poor, diagnostic\nmeasures describe the influence of individual observations on the model fit\nand highlight reasons for the inadequacy. we discuss these in section 6.2.1.\n\n.\n\n\u017e\n\n5.3 logit models with categorical predictors\n\nlike ordinary regression, logistic regression extends to include qualitative\nexplanatory variables, often called factors. in this section we use dummy\nvariables to do this.\n\n "}, {"Page_number": 193, "text": "178\n\nlogistic regression\n\n5.3.1 anova-type representation of factors\nfor simplicity, we first consider a single factor x, with i categories. in row i\nof the i = 2 table,\nis the number of outcomes in the first column\n\u017e\nsuccesses out of n trials. we treat y as binomial with parameter \u2432.\ni\ni\n\ny\n\n.\n\ni\n\ni\n\nthe logit model with a factor is\n\nlog\n\n\u2432\ni\n\n1 y \u2432\n\ni\n\ns \u2423q \u2424.\n\ni\n\n5.4\u017e\n\n.\n\ni\n\n.\nthe higher \u2424 is, the higher the value of \u2432. the right-hand side of 5.4\nresembles the model formula for cell means in one-way anova. as in\nanova, the factor has as many parameters \u2424 as categories, but one is\nredundant. with i categories, x has i y 1 nonredundant parameters. one\nparameter can be set to 0, say \u2424 s 0. if the values do not satisfy this, we can\nrecode so that it is true. for instance, set \u2424 s \u2424 y \u2424 and \u2423s \u2423q \u2424 ,\nwhich satisfy \u2424 s 0. then\n\n\u02dc\ni\n\n\u02dc\n\n\u017e\n\n\u0004\n\n4\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u02dc\ni\n\nlogit \u2432 s \u2423q \u2424 s \u2423y \u2424 q \u2424 q \u2424 s \u2423q \u2424,\n\u02dc\ni\n\n\u02dc\ni\n\n\u02dc\n\n\u02dc\n\n\u017e\n\n.\n\n.\n\n\u017e\n\ni\n\ni\n\ni\n\ni\n\n\u017e\n\n.\n\ni\n\ni\n\nwhere the newly defined parameters satisfy the constraint. when \u2424 s 0, \u2423\nequals the logit in row i, and \u2424 is the difference between the logits in rows i\nand i. thus, \u2424 equals the log odds ratio for that pair of rows.\n\n4\nfor any \u2432 ) 0 , \u2424 exist such that model 5.4 holds. the model has as\ni\nmany parameters\nas binomial observations and is saturated. when a\nfactor has no effect, \u2424 s \u2424 s \u2b48\u2b48\u2b48 s \u2424 . since this is equivalent to \u2432 s \u2b48\u2b48\u2b48\ns \u2432 , this model with only an intercept term specifies statistical indepen-\ndence of x and y.\n\n\u0004\n4\n\u017e .\ni\n\n\u017e\n\n.\n\n\u0004\n\n1\n\n2\n\n1\n\ni\n\ni\n\ni\n\ni\n\n5.3.2 dummy variables in logit models\nan equivalent expression of model 5.4 uses dummy \u00aeariables. let x s 1 for\nobservations in row i and x s 0 otherwise, i s 1, . . . , i y 1. the model is\n\n\u017e\n\n.\n\ni\n\ni\n\nlogit \u2432 s \u2423q \u2424 x q \u2424 x q \u2b48\u2b48\u2b48 q\u2424 x\niy1\n\n.i\n\n1 1\n\n\u017e\n\n2\n\n2\n\niy1\n\n.\n\ni\n\n\u017e\n\nthis accounts for parameter redundancy by not forming a dummy variable\nfor category i. the constraint \u2424 s 0 in 5.4 corresponds to this form of\ndummy variable. the choice of category to exclude for the dummy variable is\narbitrary. some software sets \u2424 s 0; this corresponds to a model with\ndummy variables for categories 2 through i, but not category 1.\nanother way to impose constraints sets \u00fd \u2424 s 0. suppose that x has\ni s 2 categories, so \u2424 s y\u2424 . this results from effect coding for a dummy\nvariable, x s 1 in category 1 and x s y1 in category 2.\n\n.\n\n1\n\n1\n\n2\n\ni\n\ni\n\n "}, {"Page_number": 194, "text": "logit models with categorical predictors\n\n179\n\ni\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n\u0004\n\n\u017e\n\n\u02c6\n\n\u02c6\na\n\nthe same substantive results occur for any coding scheme. for model\n5.4 , regardless of the constraint for \u2424 , \u2423q \u2424 and hence \u2432 are the\n\u017e\n.\n4\n\u02c6\nsame. the differences \u2424 y \u2424 for pairs\n\u017e\na, b of categories of x are\nidentical and represent estimated log odds ratios. thus, exp \u2424 y \u2424 is the\nestimated odds of success in category a of x divided by the estimated odds\nof success in category b of x. reparameterizing a model may change\nparameter estimates but does not change the model fit or the effects of\ninterest.\n\ni\n\u02c6\nb\n\n\u02c6\ni\n.\n\n\u02c6\nb\n\n\u02c6\na\n\nthe value \u2424 or \u2424 for a single category is irrelevant. different constraint\nsystems result in different values. for a binary predictor, for instance, using\ndummy variables with reference value \u2424 s 0, the log odds ratio equals\n\u2424 y \u2424 s \u2424 ; by contrast, for effect coding with \"1 dummy variable and\nhence \u2424 q \u2424 s 0, the log odds ratio equals \u2424 y \u2424 s \u2424 y y\u2424 s 2 \u2424 .\n1\na parameter or its estimate makes sense only by comparison with one for\nanother category.\n\n\u02c6\ni\n\n.\n\n\u017e\n\n.\n\n2\n\n1\n\n2\n\n1\n\n1\n\n2\n\n1\n\n2\n\n1\n\n1\n\ni\n\n5.3.3 alcohol and infant malformation example revisited\nwe return now to table 3.7 from the study of maternal alcohol consumption\nand child\u2019s congenital malformations, shown again in table 5.3. for model\n\u017e\n.5.4 , we treat malformations as the response and alcohol consumption as an\nexplanatory factor. regardless of the constraint for \u2424 , \u2423q \u2424 are the\nsample logits, reported in table 5.3. for instance,\n\n\u02c6\ni\n\n\u02c6\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ni\n\nlogit \u2432 s \u2423q \u2424 s log 48r17,066 s y5.87.\n\n\u017e\n\n.\n\n\u017e\n\n.\u02c6\n\n1\n\n\u02c6\n1\n\n\u02c6\n\n1\n\n5\n\n\u02c6\n\n\u02c6\n\nfor the coding that constrains \u2424 s 0, \u2423s y3.61 and \u2424 s y2.26. for the\ncoding \u2424 s 0, \u2423s y5.87. table 5.3 shows that except\nfor the slight\nreversal between the first and second categories of alcohol consumption, the\nlogits and hence the sample proportions of malformation cases increase as\nalcohol consumption increases.\nthe simpler model with all \u2424 s 0 specifies independence. for it, \u2423\u02c6\nequals the logit for the overall sample proportion of malformations, or\nindependence df s 4 , the pearson\nlog 93r32481 s y5.86. to test h :\n\u017e\n\n\u02c6\n1\n\n.\n\n\u017e\n\n.\n\ni\n\n0\n\ntable 5.3 logits and proportion of malformation for table 3.7\n\nalcohol\nconsumption\n0\n- 1\n1\u13902\n3\u13905\ng 6\n\npresent\n\n48\n38\n5\n1\n1\n\nabsent\n17,066\n14,464\n788\n126\n37\n\nlogit\ny5.87\ny5.94\ny5.06\ny4.84\ny3.61\n\nproportion malformed\nobserved\nfitted\n0.0026\n0.0030\n0.0041\n0.0091\n0.0231\n\n0.0028\n0.0026\n0.0063\n0.0079\n0.0263\n\n "}, {"Page_number": 195, "text": "2\n\n2\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n180\nlogistic regression\nstatistic 3.10 is x s 12.1 p s 0.02 , and the likelihood-ratio statistic\n3.11 is g s 6.2 p s 0.19 . these provide mixed signals. table 5.3 has a\n\u017e\nmixture of very small, moderate, and extremely large counts. even though\nn s 32,574, the null sampling distributions of x 2 or g2 may not be close to\nchi-squared. the p-values using the exact conditional distributions of x 2\nand g2 are 0.03 and 0.13. these are closer, but still give differing evidence.\nin any case, these statistics ignore the ordinality of alcohol consumption. the\nsample suggests that malformations may tend to be more likely with higher\nalcohol consumption. the first two percentages are similar and the next two\nare also similar, however, and any of the last three percentages changes\nsubstantially with the addition or deletion of one malformation case.\n\n.\n\n\u017e\n\n5.3.4 linear logit model for i = 2 tables\nmodel 5.4 treats the explanatory factor as nominal, since it is invariant to\nthe ordering of categories. for ordered factor categories, other models are\nmore parsimonious than this, yet more complex than the independence\nmodel. for instance, let scores\ndescribe distances between\ncategories of x. when one expects a monotone effect of x on y, it is natural\nto fit the linear logit model\n\nx , x , . . . , x\n1\n\n\u0004\n\n4\n\n2\n\ni\n\nlogit \u2432 s \u2423q \u2424x .\n\n\u017e\n\n.\n\ni\n\ni\n\n\u017e\n\n5.5\n\n.\n\nthe independence model is the special case \u2424s 0.\n\nthe near-monotone increase in sample logits in table 5.3 indicates that\nthe linear logit model 5.5 may fit better than the independence model. as\nmeasured, alcohol consumption groups a naturally continuous variable. with\nx s 0, x s 0.5, x s 1.5, x s 4.0, x s 7.0 , the last score being\nscores\nsomewhat arbitrary, table 5.4 shows results. the estimated multiplicative\n\n\u017e\n\n.\n\n4\n\n\u0004\n\n1\n\n2\n\n3\n\n4\n\n5\n\ntable 5.4 computer output for logistic regression model with infant\nmalformation data\n\ncriteria for assessing goodness of fit\n\ncriterion\n\ndeviance\npearson chi- square\nlog likelihood\n\ndf\n3\n3\n\nvalue\n1.9487\n2.0523\ny635.5968\n\nparameter\nestimate\nintercept y5.9605\n0.3166\nalcohol\n\nstd\n\nlikelihood- ratio\nerror\n95% conf limits\n0.1154 y6.1930 y5.7397\n0.5236\n0.1254\n\n0.0187\n\nwald\n\nchi- sq\n2666.41\n6.37\n\npr>chisq\n<.0001\n0.0116\n\n "}, {"Page_number": 196, "text": "logit models with categorical predictors\n\n181\n\neffect of a unit increase in daily alcohol consumption on the odds of\nmalformation is exp 0.317 s 1.37. table 5.3 shows the observed and fitted\nproportions of malformation. the model seems to fit well, as statistics\ncomparing observed and fitted counts are g2 s 1.95 and x 2 s 2.05, with\ndf s 3.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n5.3.5 cochran\u2013armitage trend test\n.\narmitage 1955 and cochran 1954 were among the first to emphasize the\nimportance of utilizing ordered categories in a contingency table. for i = 2\n4\ntables with ordered rows and i independent bin n , \u2432 variates\ny , they\ni\nproposed a trend statistic for testing independence by partitioning the\npearson statistic for that hypothesis. they used a linear probability model,\n\n\u017e\n\n\u017e\n\n.\n\n\u0004\n\ni\n\ni\n\n\u2432 s \u2423q \u2424x ,\n\ni\n\ni\n\n5.6\u017e\n\n.\n\nfitted by ordinary least squares. for this model, the null hypothesis of\nindependence is h : \u2424s 0. let x s \u00fd n x rn. let p s y rn , and let p s\n.\u00fd y rn denote the overall proportion of successes. the prediction equation\n\u017e\nis\n\n0\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u2432 s p q b x y x ,\n.\n\u02c6i\n\n\u017e\n\ni\n\nwhere\n\nb s\n\ni\n\n\u00fd n p y p\n\n. \u017e\n\u017e\n\u00fd n x y x\n\nx y x\ni\n.\n\n\u017e\n\n2\n\ni\n\ni\n\ni\n\ni\n\ni\n\n.\n\n.\n\ndenote the pearson statistic for testing independence by x i . for i = 2\n\n2\u017e .\n\ntables with ordered rows, it satisfies\n\nx i s\n\n.\n\n\u017e\n\n2\n\n1\n\np 1 y p\n\u017e\n\n.\n\nwhere\n\n\u00fd i\n\nn p y p s z q x l ,\n.\n\n\u017e\n\n.\n\n\u017e\n\n2\n\n2\n\n2\n\ni\n\ni\n\n2x l s\n\n\u017e\n\n.\n\n1\n\np 1 y p\n\u017e\n\n.\n\n\u00fd i\n\nn p y \u2432\n.\u02c6\n\n\u017e\n\ni\n\ni\n\ni\n\n2\n\n2z s\n\n2b\n\np 1 y p\n\u017e\n\n.\n\n\u00fd i\n\nn x y x s\n\n\u017e\n\n.\n\n2\n\ni\n\ni\n\n\u017e\n\n\u00fd x y x y\n.\n\u017e\n\n'\np 1 y p \u00fd n x y x\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n2\n\n.\n\n5.7\u017e\n\n.\n\n2\n\n.\n\nwhen the linear probability model holds, x l is asymptotically chi-squared\nwith df s i y 2. it tests the fit of the model. the statistic z 2, with df s 1,\n\n2\u017e\n\n.\n\n "}, {"Page_number": 197, "text": "182\nlogistic regression\ntests h : \u2424s 0 for the linear trend in the proportions 5.6 . the test of\nindependence using this statistic is called the cochran\u1390armitage trend test.\n\n\u017e\n\n.\n\n0\n\n.\n\nthis analysis seems unrelated to the linear logit model. however, the\ncochran\u1390armitage statistic is equivalent to the score statistic for testing\nh : \u2424s 0 in that model. moreover, this statistic relates to the statistic m 2 in\n0\u017e\n3.15 used to test for a linear trend in an i = j table; namely, it equals m\napplied when j s 2, except with\nn y 1 replaced by n. when i s 2,\nx l s 0 and z s x i .\n2\u017e\n2\u017e .\nfor table 5.3 on alcohol consumption and malformation, x i s 12.1.\nusing the same scores as in the linear logit model, the cochran\u1390armitage\ntrend test has z s 6.6 p-value s 0.010 . the test suggests strong evidence\nof a positive slope. in addition,\n\n2\u017e .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n2\n\n2\n\n2\n\nx 2 i s 12.1 s 6.6 q 5.5,\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n2\u017e\n\nthe cochran\u1390armitage trend test\n\nwhere x l s 5.5 df s 3 shows only slight evidence of departure of the\nproportions from linearity. the trend test agrees with m 2 for the sample\nr s 0.014 for n s 32,573 section 3.4.5 . for the chosen\ncorrelation of\nscores, the correlation seems weak. however, r has limited use as a descrip-\ntive measure for tables that are highly discrete and unbalanced.\n\u017e\ni.e., the score test usually gives results\nsimilar to the wald or likelihood-ratio test of h : \u2424s 0 in the linear logit\nmodel. the asymptotics work well even for quite small n when n are equal\nand\nare equally spaced. with table 5.3, the wald statistic equals\n\u2424rse s 0.317r0.125 s 6.4 p s 0.012 and the likelihood-ratio statis-\n\u02c6\n\u017e\ntic equals 4.25 p s 0.039 . the highly unbalanced counts suggest that it is\nsafest to use the likelihood function through the likelihood-ratio approach.\nthis is also true for estimation. the profile likelihood 95% confidence\ninterval of 0.02, 0.52 for \u2424 reported in table 5.4 is preferable to the wald\ninterval of 0.317 \" 1.96 0.125 s 0.07, 0.56 . even though n is very large,\nexact inference based on small-sample methods presented in section 6.7.4 is\nrelevant here.\n\n4\nx\ni\n.\n2\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n0\n\n2\n\ni\n\n5.4 multiple logistic regression\n\nlike ordinary regression, logistic regression extends to models with multiple\nexplanatory variables. for instance, the model for \u2432 x s p y s 1 at values\nx s x , . . . , x\n\nof p predictors is\n\n\u017e .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n1\n\np\n\nlogit \u2432 x s \u2423q \u2424 x q \u2424 x q \u2b48\u2b48\u2b48 q\u2424 x .\n\n\u017e .\n\n1 1\n\n2\n\n2\n\np\n\np\n\n\u017e\n\n5.8\n\n.\n\n "}, {"Page_number": 198, "text": "multiple logistic regression\n\nthe alternative formula, directly specifying \u2432 x , is\n\n\u017e .\n\n\u2432 x s\n\n\u017e .\n\n\u017e\n\nexp \u2423q \u2424 x q \u2424 x q \u2b48\u2b48\u2b48 q\u2424 x\n\n.\n1 q exp \u2423q \u2424 x q \u2424 x q \u2b48\u2b48\u2b48 q\u2424 x\n\n1 1\n\n2\n\n2\n\np\n\np\n\n\u017e\n\n2\n\n2\n\n1 1\n\np\n\n183\n\n\u017e\n\n5.9\n\n.\n\n.\n\n.\n\np\n\nthe parameter \u2424 refers to the effect of x on the log odds that y s 1,\ncontrolling the other x . for instance, exp \u2424 is the multiplicative effect on\nthe odds of a 1-unit increase in x , at fixed levels of other x . an explanatory\nvariable can be qualitative, using dummy variables for categories.\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ni\n\nj\n\nj\n\n5.4.1 logit models for multiway contingency tables\nwhen all variables are categorical, a multiway contingency table displays the\ndata. we illustrate ideas with binary predictors x and z. we treat the\nsample size at given combinations\ni, k of x and z as fixed and regard the\ntwo counts on y at each setting as binomial, with different binomials treated\nas independent. denote the two categories for each variable by 0, 1 , and let\ndummy variables for x and z have x s z s 1 and x s z s 0. the model\n.\n\nlogit p y s 1 s \u2423q \u2424 x q \u2424 z\n\n5.10\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n1\n\n1\n\n2\n\n2\n\n1\n\ni\n\n2\n\nk\n\nhas main effects for x and z but assumes an absence of interaction. the\neffect of one factor is the same at each level of the other.\n\nat a fixed level z of z, the effect on the logit of changing categories of\n\nx is\n\nk\n\n1\n\n\u2423q \u2424 1 q \u2424 z y \u2423q \u2424 0 q \u2424 z s \u2424 .\n\n\u017e .\n\n\u017e .\n\n2\n\nk\n\n1\n\n2\n\nk\n\n1\n\n\u017e\n\n5.11\n\n.\n\n1\n\n1\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nthis logit difference equals the difference of log odds, which is the log odds\nratio between x and y, fixing z. thus, exp \u2424 is the conditional odds ratio\nbetween x and y. controlling for z, the odds of success when x s 1 equal\nexp \u2424 times the odds when x s 0. this conditional odds ratio is the same\nat each level of z; that is, there is homogeneous xy association section\n2.3.5 . the lack of an interaction term in 5.10 implies a common odds ratio\nfor the partial tables. when \u2424 s 0, that common odds ratio equals 1. then\nx and y are independent in each partial table, or conditionally independent,\ngi\u00aeen z section 2.3.4 .\n.\n\nadditivity on the logit scale is the generally accepted definition of no\ninteraction for categorical variables. however, one could, instead, define it as\nadditivity on some other scale, such as with probit or identity link. significant\ninteraction can occur on one scale when there is none on another scale. in\nsome applications, a particular definition may be natural. for instance,\ntheory might assume an underlying normal distribution and predict that the\nprobit is an additive function of predictor effects.\n\n.\n\n\u017e\n\n\u017e\n\n1\n\n "}, {"Page_number": 199, "text": "184\n\nlogistic regression\na factor with i categories needs i y 1 dummy variables, as we showed in\nsection 5.3.2. an alternative representation of such factors resembles the way\nthat anova models often express them. the model formula\n\nlogit p y s 1 s \u2423q \u2424 q \u2424\n\n\u017e\n\n.\n\nx\n\ni\n\nz\nk\n\n\u017e\n\n5.11\n\n.\n\n\u0004\n\nx 4\n\ni\n\ni\n\n\u0004\n\n.\n\n\u017e\n\nz4 \u017e\nk.\n\nrepresents effects of x with parameters \u2424 and effects of z with parame-\nters \u2424 . the x and z superscripts are merely labels and do not represent\npowers. model form 5.11 applies for any number of categories for x and\nz. the parameter \u2424x denotes the effect on the logit of classification in\ncategory i of x. conditional independence between x and y, given z,\ncorresponds to \u2424 s \u2424 s \u2b48\u2b48\u2b48 s \u2424 , whereby p y s 1 does not change as\ni changes.\nfor each factor, one parameter in 5.11 is redundant. fixing one at 0,\nsuch as \u2424x s \u2424z s 0, represents the category not having its own dummy\nvariable. when x and z have two categories, the parameterization in model\n5.11 then corresponds to that in model 5.10 with \u2424 s \u2424 and \u2424 s 0,\n\u017e\nand with \u2424z s \u2424 and \u2424z s 0.\n\nx\ni\n\nx\n1\n\nx\n1\n\nx\n2\n\nx\n2\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\nk\n\n1\n\ni\n\n1\n\n2\n\n2\n\n5.4.2 aids and azt example\ntable 5.5 is from a study on the effects of azt in slowing the development\nof aids symptoms. in the study, 338 veterans whose immune systems were\nbeginning to falter after infection with the aids virus were randomly\nassigned either to receive azt immediately or to wait until their t cells\nshowed severe immune weakness. table 5.5 cross-classifies the veterans\u2019 race,\nwhether they received azt immediately, and whether they developed aids\nsymptoms during the 3-year study.\nx s 1 for immediate\nazt use, x s 0 otherwise and z with race z s 1 for whites, z s 0 for\nblacks , for predicting the probability that aids symptoms developed. thus,\n\u2423 is the log odds of developing aids symptoms for black subjects without\nimmediate azt use, \u2424 is the increment to the log odds for those with\nimmediate azt use, and \u2424 is the increment to the log odds for white\n\nin model 5.10 , we identify x with azt treatment\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\ntable 5.5 development of aids symptoms by azt use and race\n\nrace\nwhite\n\nblack\n\nazt use\n\nyes\nno\nyes\nno\n\nsource: new york times, feb. 15, 1991.\n\nsymptoms\n\nyes\n14\n32\n11\n12\n\nno\n93\n81\n52\n43\n\n "}, {"Page_number": 200, "text": "multiple logistic regression\n\n185\n\ntable 5.6 computer output for logit model with aids symptoms data\n\ngoodness- of- fit statistics\n\ncriterion\ndeviance\npearson\n\ndf\n1\n1\n\nvalue\n1.3835\n1.3910\n\npr ) chisq\n\n0.2395\n0.2382\n\nparameter\nintercept\nazt\nrace\n\nstd error\n\nanalysis of maximum likelihood estimates\nestimate\ny1.0736\ny0.7195\n0.0555\n\n16.6705\n6.6507\n0.0370\n\n0.2629\n0.2790\n0.2886\n\nwald chi- square\n\npr > chisq\n\n- .0001\n0.0099\n0.8476\n\nodds ratio estimates\n\neffect\nazt\nrace\n\nestimate\n\n95% wald confidence limits\n\n0.487\n1.057\n\n0.282\n0.600\n\n0.841\n1.861\n\nprofile likelihood confidence interval for odds ratios\n\neffect\nazt\nrace\n\nestimate\n\n0.487\n1.057\n\n95% confidence limits\n0.835\n0.279\n0.605\n1.884\n\nobs\n\nrace\n\nazt\n\n1\n2\n3\n4\n\n1\n1\n0\n0\n\n1\n0\n1\n0\n\ny\n14\n32\n11\n12\n\nn\n\n107\n113\n63\n55\n\n\u1390\n\npi hat\n0.14962\n0.26540\n0.14270\n0.25472\n\nlower\n\n0.09897\n0.19668\n0.08704\n0.16953\n\nupper\n\n0.21987\n0.34774\n0.22519\n0.36396\n\nw\n\n.\n\n.\n\n\u017e\n\n\u017e\n\nsubjects. table 5.6 shows output. the estimated odds ratio between immedi-\nate azt use and development of aids symptoms equals exp y0.7195 s\n0.487. for each race, the estimated odds of symptoms are half as high for\nthose who took azt immediately. the wald confidence interval for this\neffect is exp y0.720 \" 1.96 0.279 s 0.28, 0.84 . similar results occur for\n\u017e\nthe likelihood-based interval.\nthe hypothesis of conditional independence of azt treatment and devel-\nopment of aids symptoms, controlling for race, is h : \u2424 s 0 in 5.10 . the\nlikelihood-ratio statistic comparing model 5.10 with the simpler model\nhaving \u2424 s 0 equals 6.9 df s 1 , showing evidence of association p s 0.01 .\n.\nthe wald statistic \u2424 rse s y0.720r0.279 s 6.65 provides similar re-\nsults.\n\n\u02c6\n1\n\n.x\n\n.\n\u017e\n\n. \u017e .\n\ntable 5.7 shows parameter estimates for three ways of defining factor\nparameters in 5.11 : 1 setting the last parameter equal to 0, 2 setting the\nfirst parameter equal to 0, and 3 having parameters sum to zero. for each\ncoding scheme, at a given combination of azt use and race, the estimated\nprobability of developing aids symptoms is the same. for instance, the\nintercept estimate plus the estimate for immediate azt use plus the esti-\nmate for being white is y1.738 for each scheme, so the estimated probability\n\n\u017e .\n\n\u017e .\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n0\n\n1\n\n1\n\n2\n\n2\n\n "}, {"Page_number": 201, "text": "186\n\nlogistic regression\n\ntable 5.7 parameter estimates for logit model fitted to table 5.5\n\nparameter\nintercept\n\nazt yes\nno\n\nrace white\nblack\n\nlast s zero\n\ndefinition of parameters\n\nfirst s zero\n\nsum s zero\n\ny1.074\ny0.720\n0.000\n0.055\n0.000\n\ny1.738\n0.000\n0.720\n0.000\ny0.055\n\ny1.406\ny0.360\n0.360\n0.028\ny0.028\n\nfigure 5.4 estimated effects of azt use and race on probability of developing aids\n.\nsymptoms dots are sample proportions .\n\n\u017e\n\nw\n\n.\n\n\u017e\n\n\u017e\n\n.x\n\nthat white veterans with immediate azt use develop aids symptoms equals\nexp y1.738 r 1 q exp y1.738 s 0.15. the bottom of table 5.6 shows point\nand interval estimates of the probabilities. figure 5.4 shows a graphical\n\u017e\nthe four dots and the point\nrepresentation of the sample proportions\nestimates enclosed in 95% confidence intervals.\nsimilarly, for each coding scheme, \u2424x y \u2424x is identical and represents\n2\nthe conditional\nlog odds ratio of x with the response, given z. here,\nexp \u2424 y \u2424 s exp y0.720 s 0.49 estimates the common odds ratio be-\n\u02c6 x\n.\n2\ntween immediate azt use and aids symptoms, for each race.\n\n\u02c6 x\n1\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n1\n\n5.4.3 goodness of fit as a likelihood-ratio test\nthe likelihood-ratio statistic y2 l y l tests whether certain model pa-\nrameters are zero by comparing the log likelihood l for the fitted model m\n1\nwith l for a simpler model m . denote this statistic for testing m , given\n\n\u017e\n\n.\n\n0\n\n1\n\n1\n\n0\n\n0\n\n0\n\n "}, {"Page_number": 202, "text": "multiple logistic regression\n\n187\n\n2\u017e\n\n.\n\n2\u017e\n\n<\n\n.\n\n1\n\n0\n\n1\n\n0\n\n1\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\nthat m holds, by g m m . the goodness-of-fit statistic g m is a\nspecial case in which m s m and m is the saturated model. in testing\nwhether m fits, we test whether all parameters in the saturated model but\nnot in m equal zero. the asymptotic df is the difference in the number of\nparameters in the two models, which is the number of binomials modeled\nminus the number of parameters in m.\n\nwe illustrate by checking the fit of model 5.10 for the aids data. for its\nfit, white veterans with immediate azt use had estimated probability 0.150\nof developing aids symptoms during the study. since 107 white veterans\ntook azt, the fitted value is 107 0.150 s 16.0 for developing symptoms and\n107 0.850 s 91.0 for not developing them. similarly, one can obtain fitted\nvalues for all eight cells in table 5.5. the goodness-of-fit statistics comparing\nthese with the cell counts are g2 s 1.38 and x 2 s 1.39. the model has four\nbinomials, one at each combination of azt use and race. since it has three\nparameters, residual df s 4 y 3 s 1. the small g2 and x 2 values suggest\n.\nthat the model fits decently p ) 0.2 .\n\nfor model 5.10 , the odds ratio between x and y is the same at each\nlevel of z. the goodness-of-fit test checks this structure. that is, the test also\nprovides a test of homogeneous odds ratios. for table 5.5, homogeneity is\nplausible. since residual df s 1, the more complex model that adds an\ninteraction term and permits the two odds ratios to differ is saturated.\n\nlet l denote the maximized log likelihood for the saturated model. as\ndiscussed in section 4.5.4, the likelihood-ratio statistic for comparing models\nm and m is\n\n\u017e\n\n.\n\n\u017e\n\ns\n\n1\n\n0\n\ng m m s y2 l y l\n\n\u017e\n\n2\n\n<\n\n.\n\n\u017e\n\n0\n\n1\n\n0\n\n.\n\n1\n\n\u017e\n\ns y2 l y l y y2 l y l\ns g2 m y g2 m .\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\ns\n\n0\n\n1\n\n0\n\n1\n\n.\n\ns\n\n2\n\n1\n\n0\n\n2\n\n0\n\n.\n\n.\n\n.\n\n\u017e\n\nthe test statistic comparing two models is identical to the difference in g2\ngoodness-of-fit statistics deviances\nfor the two models. to illustrate, con-\nsider h : \u2424 s 0 for the race effect with the aids data. the likelihood-ratio\nstatistic equals 0.04, suggesting that the simpler model is adequate. but this\nequals g m y g m s 1.42 y 1.38, where m is the simpler model\n2\u017e\nwith \u2424 s 0.\n\n2\u017e\n\nthe model comparison statistic often has an approximate chi-squared null\ndistribution even when separate g m do not. for instance, when a\npredictor is continuous or a contingency table has very small fitted values, the\nsampling distribution of g m may be far from chi-squared. nonetheless, if\ndf for the comparison statistic is modest as in comparing two models that\ndiffer by a few parameters , the null distribution of g m m is approxi-\nmately chi-squared.\n\n2\u017e\n\n2\u017e\n\n2\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n0\n\n1\n\n0\n\ni\n\ni\n\n<\n\n "}, {"Page_number": 203, "text": "188\n\nlogistic regression\n\n5.4.4 horseshoe crab example revisited\nlike ordinary regression, logistic regression can have a mixture of quantita-\ntive and qualitative predictors. we illustrate with the horseshoe crab data\n\u017e\nsection 5.1.3 , using the female crab\u2019s width and color as predictors. color\nhas five categories: light, medium light, medium, medium dark, dark. it is a\nsurrogate for age, older crabs tending to be darker. the sample contained no\nlight crabs, so our models use only the other four categories.\n\n.\n\nwe first treat color as qualitative. the four categories use three dummy\n\nvariables. the model is\n\nlogit \u2432 s \u2423q \u2424 c q \u2424 c q \u2424 c q \u2424 x,\n\n\u017e\n\n.\n\n3 3\n\n4\n\n2 2\n\n1 1\n\n\u017e\n\n5.12\n\n.\n\nwhere \u2432s p y s 1 , x s width in centimeters, and\n\n.\n\n\u017e\n\n1\n\nc s 1 for medium-light color, and 0 otherwise,\nc s 1 for medium color, and 0 otherwise,\nc s 1 for medium-dark color, and 0 otherwise.\n\n2\n\n3\n\n3\n\n1\n\n\u017e\n\n.\n\n.\n\n\u017e\n\nthe crab color is dark category 4 when c s c s c s 0. table 5.8 shows\nlogit \u2432 s\u02c6\n.\nthe ml parameter estimates. for instance,\ny12.715 q 0.468 x; by contrast, for medium-light crabs, c s 1, and logit \u2432\u02c6\n.\n\u017e\ns y12.715 q 1.330 q 0.468 x s y11.385 q 0.468 x. at the average width\nof 26.3 cm, \u2432s 0.399 for dark crabs and 0.715 for medium-light crabs.\n\n2\nfor dark crabs,\n\nthe model assumes a lack of interaction between color and width in their\neffects. width has the same coefficient 0.468 for all colors, so the shapes of\nthe curves relating width to \u2432 are identical. for each color, a 1-cm increase\nin width has a multiplicative effect of exp 0.468 s 1.60 on the odds that\ny s 1. figure 5.5 displays the fitted model. any one curve equals any other\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n1\n\ntable 5.8 computer output for model with width and color predictors\n\ncriteria for assessing goodness of fit\n\ncriterion\ndeviance\npearson chi- square\nlog likelihood\n\ndf\n168\n168\n\nvalue\n187.4570\n168.6590\ny93.7285\n\nstandard likelihood- ratio 95%\n\nchi-\n\nparameter\nestimate\nintercept y12.7151\n1.3299\nc1\n1.4023\nc2\nc3\n1.1061\n0.4680\nwidth\n\nerror\n2.7618\n0.8525\n0.5484\n0.5921\n0.1055\n\nconfidence\nlimits\ny18.4564 y7.5788\ny0.2738\n3.1354\n2.5260\n0.3527\ny0.0279\n2.3138\n0.6870\n0.2713\n\nsquare\n21.20\n2.43\n6.54\n3.49\n19.66\n\npr>chisq\n\n<.0001\n0.1188\n0.0106\n0.0617\n<.0001\n\n "}, {"Page_number": 204, "text": "multiple logistic regression\n\n189\n\nfigure 5.5 logistic regression model using width and color predictors of satellite presence\nfor horseshoe crabs.\n\n.\n\n\u017e\n\ncurve shifted to the right or left. the parallelism of curves in the horizontal\ndimension implies that any two curves never cross. at all width values, color\n4 dark has a lower estimated probability of a satellite than the other colors.\nthere is a noticeable positive effect of width.\n\nthe exponentiated difference between two color parameter estimates is an\nodds ratio comparing those colors. for instance, the difference for medium-\nlight crabs and dark crabs equals 1.330. at any given width, the estimated\nodds that a medium-light crab has a satellite are exp 1.330 s 3.8 times the\n.\nestimated odds for a dark crab. at width x s 26.3,\nthe odds equal\n0.715r0.285 s 2.51 for a medium-light crab and 0.399r0.601 s 0.66 for a\ndark crab, for which 2.51r0.66 s 3.8.\n\n\u017e\n\n3\n\n2\n\n1\n\n0\n\n5.4.5 model comparison\nto test whether color contributes significantly to model 5.12 , we test\nh : \u2424 s \u2424 s \u2424 s 0. this states that controlling for width, the probability\nof a satellite is independent of color. we compare the maximized log-likeli-\nhood l for the full model 5.12 to l for the simpler model. the test\nstatistic y2 l y l s 7.0 has df s 3, the difference between the numbers\nof parameters in the two models. the chi-squared p-value of 0.07 provides\nslight evidence of a color effect.\n\nthe more complex model allowing color = width interaction has three\nadditional terms, the cross-products of width with the color dummy variables.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n1\n\n0\n\n0\n\n1\n\n "}, {"Page_number": 205, "text": "190\n\nlogistic regression\n\nfitting this model\nis equivalent to fitting logistic regression with width\npredictor separately for crabs of each color. each color then has a different-\nshaped curve relating width to p y s 1 , so a comparison of two colors\nvaries according to the width value. the likelihood-ratio statistic comparing\nthe models with and without the interaction terms equals 4.4, with df s 3.\nthe evidence of interaction is weak p s 0.22 .\n.\n\n\u017e\n\n.\n\n\u017e\n\n5.4.6 quantitative treatment of ordinal predictor\ncolor has ordered categories, from lightest to darkest. a simpler model yet\ntreats this predictor as quantitative. color may have a linear effect, for a set\nof monotone scores. to illustrate, for scores c s 1, 2, 3, 4 for the color\ncategories, the model\n\n\u0004\n\n4\n\nlogit \u2432 s \u2423q \u2424 c q \u2424 x\n\n\u017e\n\n.\n\n1\n\n2\n\n\u017e\n\n5.13\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u02c6\n1\n\n\u02c6\n2\n\nhas \u2424 s y0.509 se s 0.224 and \u2424 s 0.458 se s 0.104 . this shows\nstrong evidence of an effect for each. at a given width, for every one-category\nincrease in color darkness, the estimated odds of a satellite multiply by\nexp y0.509 s 0.60.\n\u017e\nthe likelihood-ratio statistic comparing this fit to the more complex model\n5.12 having a separate parameter for each color equals 1.7 df s 2 . this\n\u017e\nstatistic tests that the simpler model 5.13 is adequate, given that model\n\u017e\n5.12 holds. it tests that when plotted against the color scores, the color\n\u017e\nparameters in 5.12 follow a linear trend. the simplification seems permissi-\nble p s 0.44 .\n.\n\n\u017e\nthe color parameter estimates in the qualitative-color model 5.12 are\n\u017e\n1.33, 1.40, 1.11, 0 , the 0 value for the dark category reflecting its lack of a\ndummy variable. although these values do not depart significantly from a\nlinear trend, the first three are quite similar compared to the last one. thus,\nanother potential color scoring for model 5.13 is 1, 1, 1, 0 ; that is, score s 0\nfor dark-colored crabs, and score s 1 otherwise. the likelihood-ratio statistic\n.\ncomparing model 5.13 with these binary scores to model 5.12 equals 0.5\ndf s 2 , showing that this simpler model is also adequate. its fit is\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u0004\n\n4\n\nlogit \u2432 s y12.980 q 1.300c q 0.478 x,\n\n\u017e\n\n.\n\n\u02c6\n\n\u017e\n\n5.14\n\n.\n\nwith standard errors 0.526 and 0.104. at a given width, the estimated odds\nthat a lighter-colored crab has a satellite are exp 1.300 s 3.7 times the\nestimated odds for a dark crab.\n\nin summary, the qualitative-color model, the quantitative-color model with\nscores 1, 2, 3, 4 , and the model with binary color scores 1, 1, 1, 0 all suggest\nthat dark crabs are least likely to have satellites. a much larger sample is\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n "}, {"Page_number": 206, "text": "multiple logistic regression\n\n191\n\nneeded to determine which color scoring is most appropriate. it is advanta-\ngeous to treat ordinal predictors in a quantitative manner when such models\nfit well. the model\nis simpler and easier to interpret, and tests of the\npredictor effect are more powerful when it has a single parameter rather\nthan several parameters. in section 6.4 we discuss this issue further.\n\nj\n\nj\n\nj\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\nx j\n\nx j\n\n5.4.7 standardized and probability-based interpretations\nto compare effects of quantitative predictors having different units, it can be\nhelpful to report standardized coefficients. one approach fits the model to\nx y x rs . then, each\nstandardized predictors, replacing each x by\nregression coefficient represents the effect of a standard deviation change in\na predictor, controlling for the other variables. equivalently, for each j one\ncan multiply unstandardized estimate \u2424 by s\n\n\u017e\n.\nsee also note 5.9 .\n\nregardless of the units, many find it difficult to understand odds or odds\nratio effects. the simpler interpretation of the approximate change in the\nprobability based on a linearization of the model section 5.1.1 applies\nalso to multiple predictors. consider a setting of predictors at which\np y s 1 s \u2432. then, controlling for the other predictors, a 1-unit increase in\n\u02c6\u017e\nx corresponds approximately to a \u2424\u2432 1 y \u2432 change in \u2432. for instance, at\nj\npredictor settings at which \u2432s 0.5 for fit 5.14 , the approximate effect of\na 1-cm increase in width is 0.478 0.5 0.5 s 0.12. this is considerable,\nsince a 1-cm change in width is less than half a standard deviation.\n\n\u02c6 \u017e\n\u02c6\nj\n\n\u02c6\n\u017e\n\n.\u017e\n\n.\u017e\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\nj\n\nthis linear approximation deteriorates as the change in the predictor\nincreases. more precise interpretations use the probability formula directly.\nto describe the effect of x , one could set the other predictors at their\nsample means and compute the estimated probabilities at the smallest and\nlargest x values. these are sensitive to outliers, however. it is often more\nsensible to use the quartiles.\nfor fit 5.14 , the sample means are 26.3 for x and 0.873 for c. the lower\nand upper quartiles of x are 24.9 and 27.7. at x s 24.9 and c s c, \u2432s 0.51.\nat x s 27.7 and c s c, \u2432s 0.80. the change in \u2432 from 0.51 to 0.80 over the\nmiddle 50% of the range of width values reflects a strong width effect. since\nc takes only values 0 and 1, one could instead report this effect separately for\neach. also, when an explanatory variable is a dummy, it makes sense to\nreport the estimated probabilities at its two values rather than at quartiles,\nwhich could be identical. at x s 26.3, \u2432s 0.40 when c s 0 and \u2432s 0.71\nwhen c s 1. this color effect, differentiating dark crabs from others, is also\nsubstantial.\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\ntable 5.9 shows a way to present effects that can be understandable to\nthose not familiar with odds ratios. it also shows results of the extension of\nmodel 5.14 , permitting interaction. the estimated width effect is then\ngreater for the lighter-colored crabs. however, the interaction is not signifi-\ncant.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nj\n\nj\n\n "}, {"Page_number": 207, "text": "192\n\nlogistic regression\n\n)\ntable 5.9 summary of effects in model 5.14 with crab width\nand color as predictors of presence of satellites\n\n(\n\nvariable\nno interaction model\n\nintercept\ncolor 0 s dark,\n\u017e\n1 s other\n\n.\nwidth, x cm\n\n\u017e\n\ninteraction model\nintercept\ncolor 0 s dark,\n\u017e\n1 s other\n\n.\nwidth, x cm\nwidth = color\n\n\u017e\n\nestimate\n\nse\n\ncomparison\n\nchange in probability\n\ny12.980\n\n2.727\n\n.\n\n.\n\n1.300\n0.478\ny5.854\ny6.958\n0.200\n0.322\n\n\u017e\n0.526\n1, 0 at x\n0.104 uq, lq at c\n\n.\n.\n\n\u017e\n\n6.694\n\n7.318\n0.262 uq, lq at c s 0\n0.286 uq, lq at c s 1\n\n\u017e\n\u017e\n\n.\n.\n\n0.31 s 0.71 y 0.40\n0.29 s 0.80 y 0.51\n\n0.13 s 0.43 y 0.30\n0.29 s 0.84 y 0.55\n\n5.5 fitting logistic regression models\n\nthe mechanics of ml estimation and model fitting for logistic regression are\nspecial cases of the glm fitting results of section 4.6. with n subjects, we\ntreat the n binary responses as independent. let x s x , . . . , x\ndenote\nsetting i of values of p explanatory variables, i s 1, . . . , n. when explana-\ntory variables are continuous, a different setting may occur for each subject,\nin which case n s n. the logistic regression model 5.8 , regarding \u2423 as a\nregression parameter with unit coefficient, is\n\ni p\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni1\n\ni\n\n\u2432 x s\n\n\u017e\n\n.\n\ni\n\n\u017e\n\nexp \u00fd p \u2424 x\nj\n\n.\n1 q exp \u00fd \u2424 x\n\njs1\n\u017e\np\njs1\n\ni j\n\nj\n\n.\n\n.\n\ni j\n\n\u017e\n\n5.15\n\n.\n\n5.5.1 likelihood equations\nwhen more than one observation occurs at a fixed x value, it is sufficient to\nrecord the number of observations n and the number of successes. we then\nlet y refer to this success count rather than to an individual binary response.\nare independent binomials with e y s n \u2432 x , where\nthen y , . . . , y\nn\nn q \u2b48\u2b48\u2b48 qn s n. their joint probability mass function is proportional to the\n1\nproduct of n binomial functions,\n\ni \u0004\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n4\n\nn\n\n1\n\ni\n\ni\n\ni\n\ni\n\ni\n\nn\n\n\u0142 i\n\u017e\n\u2432 x\nis1\n\n.\n\ns\n\ny\n\ni\n\n1 y \u2432 x\n\u017e\n\nn\n\n\u0142\nis1\n\nexp log\n\ni\n\n\u017e\n\ns exp\n\n\u00fd\n\ni\n\ny log\ni\n\n\u00bd\n\u00bd\n\nn yy\ni\n\ni\n\n.\n\ni\n\n\u2432 x\u017e\n\n.\n1 y \u2432 x\u017e\n.\n1 y \u2432 x\u017e\n\n\u2432 x\u017e\n\ni\n\n/\n\ny i\n\n5\n\n\u00bd\n\n.\n\ni\n\n.\n\ni\n\n5\n\n\u00bd\n\nn\n\n\u0142\nis1\n\n1 y \u2432 x\u017e\n\n.\n\ni\n\nn\n\ni\n\nn\n\n\u0142\nis1\n\n1 y \u2432 x\n\u017e\n\n.\n\ni\n\nn\n\ni\n\n5\n\n5\n\n.\n\n "}, {"Page_number": 208, "text": "fitting logistic regression models\n\n193\n\n\u017e\n\n.\n\nfor model 5.15 , the ith logit is \u00fd \u2424 x , so the exponential term in the\nw\nlast expression equals exp \u00fd y \u00fd \u2424 x s exp \u00fd \u00fd y x \u2424 . also, since\nw\n1 y \u2432 x s 1 q exp \u00fd \u2424 x\n\n.\nj\n, the log likelihood equals\n\n.x\n\n.x\n\n\u017e\n\n\u017e\n\n\u017e\n\nw\n\nw\n\nx\n\ni j\n\ni j\n\ni j\n\ni\n\ni\n\ni\n\nj\n\nj\n\nj\n\nj\n\nj\n\nj\n\nj\n\ni j\n\ni\n\nl \u2424 s\n\u017e\n\n.\n\n\u017e\n\u00fd \u00fd\n\ni\n\nj\n\ny x \u2424 y n log 1 q exp\n\n\u00fd\n\ni j\n\ni\n\ni\n\ni\n\n\u017e\n\n\u00fd\n\nj\n\n/\n\n\u2424 x\nj\n\ni j\n\n.\n\n\u017e\n\n5.16\n\n.\n\ni\n\n\u017e\n.xy1\n/\n\nj\n\nthis depends on the binomial counts only through the sufficient statistics\n\u00fd y x , j s 1, . . . , p .\n4\n\u0004\n\nthe likelihood equations result from setting \u2b78l \u2424 r\u2b78\u2424 s 0. since\n\n\u017e\n\n.\n\ni j\n\ni\n\ni\n\n.\n\n\u017e\n\u2b78l \u2424\n\u2b78\u2424\nj\n\ns y x y n x\n\n\u00fd\n\n\u00fd\n\ni j\n\ni\n\ni\n\ni j\n\ni\n\ni\n\n\u017e\n\nexp \u00fd \u2424 x\nk\n\n.\n1 q exp \u00fd \u2424 x\n\nk\n\u017e\n\nik\n\nk\n\nk\n\nik\n\n,\n\n.\n\nthe likelihood equations are\n\n\u00fd\n\ni\n\ny x y n \u2432 x s 0,\n\n\u00fd\n\n\u02c6\n\ni j\n\ni j\n\ni\n\ni\n\ni\n\ni\n\nj s 1, . . . , p,\n\n\u017e\n\n5.17\n\n.\n\n\u017e\n\n\u02c6i\n\n\u02c6\nk\n\nwhere \u2432 s exp \u00fd \u2424 x r 1 q exp \u00fd \u2424 x\n.\nis the ml estimate of \u2432 x .\nwe observed these equations as a special case of those for binomial glms in\n\u017e\n4.25\nis the proportion of successes . the equations are\nnonlinear and require iterative solution.\n\nbut there y\n\n. \u017e\n\n\u02c6\nk\n\nik\n\nik\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nk\n\nk\n\ni\n\ni\n\n.x\n\nw\n\n\u0004\nlet x denote the n = p matrix of values of x\n\n4\n\ni j\n\n. the likelihood equations\n\n\u017e\n5.17 have form\n\n.\n\nxx y s xx\u242e,\n\u02c6\n\n.\n5.18\nwhere \u242e s n \u2432. this equation illustrates a fundamental result: for glms\nwith canonical link, the likelihood equations equate the sufficient statistics to\nthe estimates of their expected values. equation 4.44 showed this result in\nthe glm context, and 5.18 are the normal equations in ordinary regression.\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\n5.5.2 asymptotic covariance matrix of parameter estimators\nthe ml estimators \u2424 have a large-sample normal distribution with covari-\nance matrix equal to the inverse of the information matrix. the observed\ninformation matrix has elements\n\n\u02c6\n\ny\n\n\u017e\n\n.\n\u2b782l \u2424\n\u2b78\u2424 \u2b78\u2424\nb\n\na\n\nx\n\ns\n\n\u00fd\n\ni\n\n\u017e\n\nia\n\ni\n\nx n exp \u00fd \u2424 x\nj\nib\n1 q exp \u00fd \u2424 x\n.\n\n\u017e\n\nj\n\ni j\n\nj\n\nj\n\n.\n\ni j\n2\n\ns x\n\u00fd ia\n\ni\n\nx n \u2432 1 y \u2432 .\n.\n\n\u017e\n\nib\n\ni\n\ni\n\ni\n\n\u017e\n\n5.19\n\n.\n\n4\nthis is not a function of\ny , so the observed and expected information are\ni\n.\nidentical. this happens for all glms that use canonical links section 4.6.4 .\n\n\u017e\n\n\u0004\n\n "}, {"Page_number": 209, "text": "194\n\nlogistic regression\n\nthe estimated covariance matrix is the inverse of the matrix having\n\nelements 5.19 , substituting \u2424. this has form\n\n\u017e\n\n.\n\n\u02c6\n\n$\ncov \u2424 s x diag n \u2432 1 y \u2432 x\n\n\u02c6\n\nx\n\n\u017e\n\n.\n\n\u0004\n\n\u02c6\n\n\u02c6\n\n\u017e\n\ni\n\ni\n\ni\n\ny1\n\n4\n\n,\n\n\u017e\n\n5.20\n\n.\n\nw\n.4\n\ni\n\ni\n\ni\n\ni\n\n\u017e\n\n\u017e\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\nwhere diag n \u2432 1 y \u2432 denotes\nthe n = n diagonal matrix having\nn \u2432 1 y \u2432 on the main diagonal. this is the special case of the glm\n\u0004\ncovariance matrix 4.28 with estimated diagonal weight matrix w having\nelements w s n \u2432 1 y \u2432 . the square roots of the main diagonal elements\n\u02c6\nof 5.20 are estimated standard errors of \u2424.\n\n.\n\u02c6\n\n\u017e\n\u017e\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n.\n.x\n\n5.5.3 distribution of probability estimators\n\n$\n\n\u02c6\u017e\n\n.\n\n\u017e .\n\nusing cov \u2424 , one can conduct inference about \u2424 and related effects such as\nodds ratios. one can also construct confidence intervals for response proba-\nbilities \u2432 x at particular settings x.\nw\nthe estimated variance of logit \u2432 x s x\u2424 is x cov \u2424 x . for large sam-\n\u02c6$\n.\n\u017e\nx\u02c6\nples, logit \u2432 x \" z\nx cov \u2424 x is a confidence interval for the true logit.\nthe endpoints invert to a corresponding interval for \u2432 x using the transform\n.x\n\u2432s exp logit r 1 q exp logit .\n\n'\n\n\u017e .x\n\n\u017e .x\n\n$\n\n\u017e .\n\n\u2423r2\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nw\n\nw\n\nx\n\n5.5.4 newton\u2013raphson method applied to logistic regression\nwe refer back to section 4.6.1 for the newton\u1390raphson iterative method.\nlet\n\nu s\n\u017et.\nj\n\n.\n\n\u2b78l \u2424\u017e\n\u2b78\u2424\nj\n\ns\n\n\u017e\n\n\u00fd\n\ni\n\n\u017et.\n\n\u2424\n\ny y n \u2432 x\n.\n\n\u017et.\ni\n\ni\n\ni\n\ni j\n\nh s\n\u017et.\nab\n\n.\n\n2\u2b78 l \u2424\u017e\n\u2b78\u2424 \u2b78\u2424 \u017et.\n\nb \u2424\n\na\n\ns y x\n\u00fd\n\ni\n\nia\n\nx n \u2432 1 y \u2432 .\n.\n\u017et.\ni\n\n\u017et.\ni\n\n\u017e\n\nib\n\ni\n\nhere, \u2432 \u017et., approximation t for \u2432, is obtained from \u2424\u017et. through\n\n\u02c6\n\n\u017et.\u2432 s\n\ni\n\n\u017e\n\n.\nexp \u00fd p \u2424\u017et. x\ni j\n1 q exp \u00fd \u2424 x\n\u017et.\nj\n\nj\n\n.\n\n.\n\ni j\n\njs1\n\u017e\np\njs1\n.\n\n\u017et.\n\nwe use u and h with formula 4.39 to obtain the next value \u2424\nin this context is\n\n\u017et.\n\n\u017e\n\n\u017e\n\n5.21\n\n.\n\n\u017etq1.\n\n, which\n\n\u017etq1.\n\n\u2424\n\ns \u2424 q x diag n \u2432 1 y \u2432 x\n\n\u017et.\n\nx\n\n.\n\n\u017et.\ni\n\n\u017et.\ni\n\n\u017e\n\ni\n\n\u0004\n\ny1\n\n4\n\nx\n\nx y y \u242e ,\n.\n\u017et.\n\n\u017e\n\n\u017e\n\n5.22\n\n.\n\nwhere \u242e\u017et. s n \u2432\u017et.. this is used to obtain \u2432\u017etq1., and so forth.\n\ni\n\ni\n\ni\n\n "}, {"Page_number": 210, "text": "fitting logistic regression models\n\n195\n\n.\n\n\u017e0.\n\n\u017e\n\n\u017e0.\n\n\u017e\n\nwith an initial guess \u2424 , 5.21 yields \u2432 , and for t ) 0 the iterations\n\u017et.\nproceed as just described using 5.22 and 5.21 . in the limit, \u2432 and \u2424\n\u017et.\nconverge to the ml estimates \u2432 and \u2424 walker and duncan 1967 . the h\nmatrices converge to h s yx diag n \u2432 1 y \u2432 x. by 5.20 the estimated\n\u02c6\nasymptotic covariance matrix of \u2424 is a by-product of the newton\u1390raphson\nmethod, namely yh .\n\u02c6 y1\n\nfrom the argument in section 4.6.3, \u2424\u017etq1. has the iterative reweighted\n\n.\n\u02c6\n\u017e\n\u02c6\ni\n\n.\n.x\n\n\u017et.\n.\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\nw\n\nx\n\ni\n\ni\n\nleast squares form x v x\n\n\u017e\n\nx y1\n\nt\n\n.y1\n\nx y1 \u017et.\n\nx v z , where z\n\n\u017et.\n\nt\n\nhas elements\n\n\u017et.z s log\n\ni\n\ni\n\n\u2432\u017et.\n1 y \u2432\n\u017et.\ni\n\nq\n\ny y n \u2432\u017et.\ni\nn \u2432 1 y \u2432\n\u017et.\n\u017et.\ni\ni\n\n\u017e\n\ni\n\ni\n\ni\n\n\u017e\n\n5.23\n\n.\n\nt\n\nand where v is a diagonal matrix with elements 1rn \u2432 1 y \u2432 . in this\nexpression, z\u017et. is the linearized form of the logit link function for the sample\ndata, evaluated at \u2432\nsee 4.42 . from section 3.1.6 the elements of v are\nestimated asymptotic variances of the sample logits. the ml estimate is the\nlimit of a sequence of weighted least squares estimates, where the weight\nmatrix changes at each cycle.\n\n\u017et..4\ni\n\n\u017et. w\n\n.x\n\n\u017e\n\n\u0004\n\nt\n\ni\n\n,\n\n.\n\u017et.\u017e\ni\n\n.\n\n\u017e\n\n.\n\n5.5.5 convergence and existence of finite estimates\nthe log-likelihood function for logistic regression models is strictly concave.\nml estimates exist and are unique except in certain boundary cases haber-\nman 1974a; wedderburn 1976; albert and anderson 1984 . estimates do not\nexist or may be infinite when there is no overlap in the sets of explanatory\nvariable values having y s 0 and having y s 1; that is, when a hyperplane\ncan pass through the space of predictor values such that on one side of that\nhyperplane y s 0 for all observations, whereas on the other side, y s 1\nalways. there is then perfect discrimination, as one can predict the sample\noutcomes perfectly by knowing the predictor values except possibly at a\nboundary point . when there is overlap, ml estimates exist and are unique.\n.\nsimilar results occur for the probit and some other links silvapulle 1981 .\nfigure 5.6 illustrates for a single explanatory variable. here, y s 0 at\nx s 10, 20, 30, 40, and y s 1 at x s 60, 70, 80, 90. an ideal fit has \u2432s 0 for\nx f 40 and \u2432s 1 for x g 60. by letting \u2424\u2122 \u2b01 and, for fixed \u2424, letting\n\u2423s y\u2424 50 so that \u2432s 0.5 at x s 50, one generates a sequence with\n\u02c6\never-increasing value of the likelihood that comes successively closer to a\nperfect fit.\nin practice, most software fails to recognize that \u2424s \u2b01. after a few cycles\nof iterative fitting, the log likelihood looks flat at the working estimate, and\nconvergence criteria are satisfied. because the log likelihood is so flat and\nbecause variances come from the inverse of the matrix of negative second\nderivatives, software typically reports huge standard errors. for these data,\nfor instance, proc genmod in sas reports logit \u2432 s y192.2 q 3.8 x\nwith standard errors of 8.0 = 108 and 1.5 = 107.\n\n\u02c6\n\u02c6\n\n\u02c6\u017e\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n "}, {"Page_number": 211, "text": "196\n\nlogistic regression\n\nfigure 5.6 perfect discrimination resulting in infinite logistic regression parameter estimate.\n\nnotes\n\nsection 5.1: interpreting parameters in logistic regression\n\n.\n\n\u017e\n\n5.1. books focusing on applied logistic regression include collett 1991 and hosmer and\nlemeshow 2000 . books having major components on logistic regression include chris-\ntensen 1997 , cox and snell 1989 , and morgan 1992 . prentice 1976b and stukel\n\u017e\n1988 extended the scope by introducing shape parameters that modify the behavior of\nthe curve in extreme probability regions and allow for asymmetric treatment of the two\ntails.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n1\n2\n\n<\n\n<\n\ni\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n5.2. haldane 1956 recommended adding\n\nto the numerator and denominator of the\nsample logit. with this modification, the bias is on the order of only 1rn2, for large n\ni\n.\n\u017e\nsee firth 1993a and problem 14.4 .\n\n5.3. the cornfield 1962 result about normal distributions for x y s i\n\nimplying the\nlogistic curve for p y s 1 x suggests that logistic regression is useful in discrimination\nand classification problems. these use a subject\u2019s x value to predict to which of two\npopulations they belong. anderson 1975 , bull and donner 1987 , efron 1975 , and\npress and wilson 1978 compared logistic regression favorably to discriminant analysis,\nwhich assumes that explanatory variables have a normal distribution at each level of y.\n5.4. rosenbaum and rubin 1983 used logistic regression to adjust for bias in comparing\ntwo groups in observational studies. they defined the propensity as the probability of\nbeing in one group, for a given setting of the explanatory variables x, and they used\nlogistic regression to estimate how propensity depends on x. in comparing the groups on\nthe response variable, they showed that one can control for differing distributions of the\ngroups on x by adjusting for the estimated propensity. this is done by using the\npropensity to match samples from the groups or to subclassify subjects into several strata\nconsisting of intervals of propensity scores or to adjust directly by entering the propen-\nsity in the model. see d\u2019agostino 1998 for a tutorial.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n5.5. adelbasit and plackett 1983 , chaloner and larntz 1988 , minkin 1987 , and wu\n\u017e\n1985 discussed design problems for binary response experiments, such as choosing\nsettings for a predictor to optimize a criterion for estimating parameter values or\nestimating the setting at which the response probability equals some fixed value. the\nnonconstant variance makes this challenging.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 212, "text": "problems\n\nsection 5.2: inference for logistic regression\n\n197\n\n.\n\n\u017e\n\n.\n5.6. albert and anderson 1984 , berkson 1951, 1953, 1955 , cox 1958a , hodges 1958 ,\nand walker and duncan 1967 discussed ml estimation for logistic regression. for\n.\nadjustments with complex sample surveys, see hosmer and lemeshow 2000, sec. 6.4\nand lavange et al. 2001 . scott and wild 2001 discussed the analyses of case\u1390\ncontrol studies with complex sampling designs.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n5.7. tsiatis 1980 suggested an alternative goodness-of-fit test that partitions values for the\nexplanatory variables into a set of regions and adds a dummy variable to the model for\neach region. the test statistic compares the fit of this model to the simpler one, testing\nthat the extra parameters are not needed. the idea of grouping values to check model fit\nby comparing observed and fitted counts extends to any glm pregibon 1982 . hosmer\net al. 1997 compared various ways of doing this.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nsection 5.3: logit models with categorical predictors\n\n.\n\n\u017e\n\n5.8. the cochran\u1390armitage trend test is locally asymptotically efficient for both linear and\nlogistic alternatives for p y s 1 . its efficiency against linear alternatives follows from\nthe approximate normality of the sample proportions, with constant bernoulli variance\nwhen \u2424s 0. for the linear logit model 5.5 , its efficiency follows from its equivalence\nwith the score test. see problem 9.35 and cox 1958a for related remarks. tarone and\ngart 1980 showed that the score test for a binary linear trend model does not depend\non the link function. gross 1981 noted that for the linear logit model, the local\nasymptotic relative efficiency for testing independence using the statistic with an\nincorrect set of scores equals the square of the pearson correlation between the true and\n.\nincorrect scores. simon 1978 gave related asymptotic results. corcoran et al. 2001 ,\nmantel 1963 , and podgor et al. 1996 extended the trend test.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nsection 5.4: multiple logistic regression\n\n5.9. since the standardized logistic cdf has standard deviation \u2432r 3 , some software e.g.,\nproc logistic in sas defines a standardized estimate by multiplying the unstan-\ndardized estimate by s\n\n\u017e\n\n.\n'\n3r\u2432.\nx j\n\n'\n\nproblems\n\napplications\n\n5.1 for a study using logistic regression to determine characteristics asso-\nciated with remission in cancer patients, table 5.10 shows the most\nimportant explanatory variable, a labeling index li . this index mea-\nsures proliferative activity of cells after a patient receives an injection\nof tritiated thymidine, representing the percentage of cells that are\n\u2018\u2018labeled.\u2019\u2019 the response y measured whether the patient achieved\nremission 1 s yes . software reports table 5.11 for a logistic regres-\nsion model using li to predict the probability of remission.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n "}, {"Page_number": 213, "text": "198\n\nlogistic regression\n\ntable 5.10 data for problem 5.1\n\nnumber number of\n\nnumber number of\n\nnumber number of\nli of cases remissions li of cases remissions li of cases remissions\n8\n10\n12\n14\n16\n\n18\n20\n22\n24\n26\n\n28\n32\n34\n38\n\n1\n3\n2\n1\n1\n\n2\n2\n3\n3\n3\n\n0\n0\n0\n0\n0\n\n1\n2\n1\n0\n1\n\n1\n1\n1\n3\n\n1\n0\n1\n2\n\n.\nsource: data reprinted with permission from e. t. lee, comput. prog. biomed. 4: 80\u139092 1974 .\n\n\u017e\n\ntable 5.11 computer output for problem 5.1\n\nintercept\n\nintercept and\n\ncriterion\ny2 log l\n\nonly\n\n34.372\n\ncovariates\n\n26.073\n\ntesting global null hypothesis: beta = 0\n\ntest\nlikelihood ratio\nscore\nwald\n\nchi- square\n\n8.2988\n7.9311\n5.9594\n\ndf\n1\n1\n1\n\npr > chisq\n\n0.0040\n0.0049\n0.0146\n\nparameter\nintercept\nli\n\nestimate\ny3.7771\n0.1449\n\nstandard error\n\nchi- square\n\npr > chisq\n\n1.3786\n0.0593\n\n7.5064\n5.9594\n\n0.0061\n0.0146\n\neffect\nli\n\npoint estimate\n\n95% wald confidence limits\n\n1.156\n\n1.029\n\n1.298\n\nodds ratio estimates\n\nestimated covariance matrix\nli\n\nvariable\nintercept\nli\n\nintercept\n1.900616\ny0.07653\n\ny0.07653\n0.003521\n\nobs\n\n1\n2\n\nli\n8\n10\n\nremiss\n\n0\n0\n\nn\n2\n2\n\n\u1390\n\npi hat\n0.06797\n0.08879\n\nlower\n\n0.01121\n0.01809\n\nupper\n\n0.31925\n0.34010\n\na. show how software obtained \u2432s 0.068 when li s 8.\nb. show that \u2432s 0.5 when li s 26.0.\nc. show that the rate of change in \u2432 is 0.009 when li s 8 and 0.036\n\u02c6\n\n\u02c6\n\n\u02c6\n\nwhen li s 26.\n\nd. the lower quartile and upper quartile for li are 14 and 28. show\n\nthat \u2432 increases by 0.42, from 0.15 to 0.57, between those values.\n\n\u02c6\n\ne. for a unit change in li, show that the estimated odds of remission\n\nmultiply by 1.16.\n\n "}, {"Page_number": 214, "text": "problems\n\n199\n\nf. explain how to obtain the confidence interval reported for the odds\n\nratio. interpret.\n\ng. construct a wald test for the effect. interpret.\nh. conduct a likelihood-ratio test for the effect, showing how to\n\nconstruct the test statistic using the y2 log l values reported.\n\ni. show how software obtained the confidence interval for \u2432 reported\n\nat li s 8. hint: use the reported covariance matrix.\n\n\u017e\n\n.\n\ntable 5.12 data for problem 5.2 a\n\n0\n0\n1\n0\n1\n\n2\n7\n12\n17\n22\n\n66\n72\n70\n75\n75\n\nft temp td ft temp td ft temp td ft temp td ft temp td\n0\n1\n1\n6\n11\n0\n16\n0\n21\n.\na\nft, flight number; temp, temperature \u2b1af ; td, thermal distress 1, yes; 0, no .\nsource: data based on table 1 in j. amer. statist. assoc., 84: 945\u1390957, 1989 , by s. r. dalal,\ne. b. fowlkes, and b. hoadley. reprinted with permission from the journal of the american\nstatistical association.\n\n3\n8\n13\n18\n23\n\u017e\n\n69\n70\n67\n81\n58\n\n70\n73\n78\n70\n76\n\n67\n63\n67\n79\n\n68\n57\n53\n76\n\n5\n10\n15\n20\n\n4\n9\n14\n19\n\n1\n0\n0\n0\n0\n\n0\n0\n0\n0\n1\n\n0\n1\n1\n0\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n5.2 for the 23 space shuttle flights before the challenger mission disaster\nin 1986, table 5.12 shows the temperature at the time of the flight and\nwhether at least one primary o-ring suffered thermal distress.\na. use logistic regression to model the effect of temperature on the\nprobability of thermal distress. plot a figure of the fitted model, and\ninterpret.\n\nb. estimate the probability of thermal distress at 31\u2b1af, the tempera-\n\nture at the place and time of the challenger flight.\n\nc. construct a confidence interval for the effect of temperature on the\nodds of thermal distress, and test the statistical significance of the\neffect.\n\nd. check the model fit by comparing it to a more complex model.\n\n5.3 refer to table 4.2. using scores 0, 2, 4, 5 for snoring, fit the logistic\nregression model. interpret using fitted probabilities, linear approxi-\nmations, and effects on the odds. analyze the goodness of fit.\n\n\u0004\n\n4\n\n\u017e\n\n5.4 hastie and tibshirani 1990, p. 282 described a study to determine\nrisk factors for kyphosis, severe forward flexion of the spine following\ncorrective spinal surgery. the age in months at the time of the\noperation for the 18 subjects for whom kyphosis was present were 12,\n15, 42, 52, 59, 73, 82, 91, 96, 105, 114, 120, 121, 128, 130, 139, 139, 157\n\n.\n\n "}, {"Page_number": 215, "text": "200\n\nlogistic regression\n\nand for 22 of the subjects for whom kyphosis was absent were 1, 1, 2, 8,\n11, 18, 22, 31, 37, 61, 72, 81, 97, 112, 118, 127, 131, 140, 151, 159, 177,\n206.\na. fit a logistic regression model using age as a predictor of whether\n\nkyphosis is present. test whether age has a significant effect.\n\nb. plot the data. note the difference in dispersion on age at the two\nlogit \u2432 x s \u2423q \u2424 x q \u2424 x .\n2\nlevels of kyphosis. fit the model\ntest the significance of the squared age term, plot the fit, and\ninterpret. note also problem 5.33.\n\n.x\n\n\u017e\n\n.\n\n\u017e\n\nw\n\n1\n\n2\n\n\u017e\n\n.\n\n5.5 refer to table 6.11. the pearson test of independence has x i s\n6.88 p s 0.14 . for equally spaced scores, the cochran\u1390armitage\ntrend test has z s 6.67 p s 0.01 . interpret, and explain why results\ndiffer so. analyze the data using a linear logit model. test indepen-\ndence using the wald and likelihood-ratio tests, and compare results\nto the cochran\u1390armitage test. check the fit of the model, and inter-\npret.\n\n2\u017e .\n\n\u017e\n\n.\n\n2\n\n5.6 for table 5.3, conduct the trend test using alcohol consumption scores\n\u017e\n1, 2, 3, 4, 5 instead of 0.0, 0.5, 1.5, 4.0, 7.0 . compare results, noting\nthe sensitivity to the choice of scores for highly unbalanced data.\n\n.\n\n\u017e\n\n.\n\n5.7 refer to table 2.11. using scores 0, 3, 9.5, 19.5, 37, 55 for cigarette\nsmoking, analyze these data using a logit model. is the intercept\nestimate meaningful? explain.\n\n\u017e\n\n.\n\n5.8 a study used the 1998 behavioral risk factors social survey to\nconsider factors associated with women\u2019s use of oral contraceptives in\nthe united states. table 5.13 summarizes effects for a logistic regres-\nsion model for the probability of using oral contraceptives. each\npredictor uses a dummy variable, and the table lists the category\nhaving dummy outcome 1. interpret effects. construct and interpret a\nconfidence interval for the conditional odds ratio between contracep-\ntive use and education.\n\ntable 5.13 data for problem 5.8\ncoding s 1 if:\n35 or younger\nwhite\ng 1 year college\nmarried\n\nvariable\nage\nrace\neducation\nmarital status\n\nestimate\ny1.320\n0.622\n0.501\ny0.460\n\nse\n0.087\n0.098\n0.077\n0.073\n\nsource: data courtesy of debbie wilson, college of pharmacy, university of\nflorida.\n\n "}, {"Page_number": 216, "text": "problems\n\n201\n\ntable 5.14 computer output for problem 5.9\n\ncriteria for assessing goodness of fit\n\ncriterion\ndeviance\npearson chi- square\nlog likelihood\n\ndf\n1\n1\n\nvalue\n\n0.3798\n0.1978\ny209.4783\n\nparameter\nintercept\ndef\nvic\n\nestimate\ny3.5961\ny0.8678\n2.4044\n\nstandard\n\nerror\n0.5069\n0.3671\n0.6006\n\nlikelihood ratio\n95% conf limits\n\ny4.7754 y2.7349\ny1.5633 y0.1140\n3.7175\n\n1.3068\n\nchi-\n\nsquare\n50.33\n5.59\n16.03\n\nsource\ndef\nvic\n\ndf\n1\n1\n\nlr statistics\nchi- square\n\n5.01\n20.35\n\npr > chisq\n\n0.0251\n<.0001\n\n5.9 refer to table 2.6. table 5.14 shows the results of fitting a logit model,\ntreating death penalty as the response 1 s yes and defendant\u2019s race\n1 s white and victims\u2019 race 1 s white as dummy predictors.\n\u017e\na. interpret parameter estimates. which group is most likely to have\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\nthe yes response? find the estimated probability in that case.\nb. interpret 95% confidence intervals for conditional odds ratios.\nc. test the effect of defendant\u2019s race, controlling for victims\u2019 race,\n\n\u017e .\nusing a i wald test, and ii\n\n\u017e .\n\nlikelihood-ratio test. interpret.\n\nd. test the goodness of fit. interpret.\n\n5.10 model the effects of victim\u2019s race and defendant\u2019s race for table 2.13.\n\ninterpret.\n\n5.11 table 5.15 appeared in a national study of 15- and 16-year-old adoles-\ncent. the event of interest is ever having sexual intercourse. analyze,\n\ntable 5.15 data for problem 5.11\n\nrace\nwhite\n\nblack\n\ngender\nmale\nfemale\nmale\nfemale\n\nintercourse\n\nyes\n43\n26\n29\n22\n\nno\n134\n149\n23\n36\n\nsource: s. p. morgan and j. d. teachman, j. marriage fam.\n50: 929\u1390936 1988 . reprinted with permission from the\nnational council on family relations.\n\n\u017e\n\n.\n\n "}, {"Page_number": 217, "text": "202\n\nlogistic regression\n\nincluding description and inference about the effects of gender and\nrace, goodness of fit, and summary interpretations.\n\n\u017e\n\n5.12 according to the independent newspaper london, mar. 8, 1994 , the\nmetropolitan police in london reported 30,475 people as missing in\nthe year ending march 1993. for those of age 13 or less, 33 of 3271\nmissing males and 38 of 2486 missing females were still missing a year\nlater. for ages 14 to 18, the values were 63 of 7256 males and 108 of\n8877 females; for ages 19 and above, the values were 157 of 5065 males\nand 159 of 3520 females. analyze and interpret. thanks to pat\naltham for showing me these data.\n\n.\n\n\u017e\n\n.\n\n5.13 the national collegiate athletic association studied graduation rates\nfor freshman student athletes during the 1984\u13901985 academic year.\nthe sample size, number graduated totals were 796, 498 for white\nfemales, 1625, 878 for white males, 143, 54 for black females, and\n\u017e\n60, 197 for black males j. j. mcardle and f. hamagami, j. amer.\nstatist. assoc. 89: 1107\u13901123, 1994 . analyze and interpret.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n5.14 in a study designed to evaluate whether an educational program makes\nsexually active adolescents more likely to obtain condoms, adolescents\nwere randomly assigned to two experimental groups. the educational\nprogram, involving a lecture and videotape about transmission of the\nhiv virus, was provided to one group but not the other. table 5.16\nsummarizes results of a logistic regression model for factors observed\nto influence teenagers to obtain condoms.\n.\na. find the parameter estimates for the fitted model, using 1, 0\ndummy variables for the first three predictors. based on the corre-\nsponding confidence interval for the log odds ratio, determine the\nstandard error for the group effect.\n\n\u017e\n\nb. explain why either the estimate of 1.38 for the odds ratio for gender\nor the corresponding confidence interval is incorrect. show that if\nthe reported interval is correct, 1.38 is actually the log odds ratio,\nand the estimated odds ratio equals 3.98.\n\ntable 5.16 data for problem 5.14\n\n95% confidence\n\n\u017e\n\nodds ratio\n\nvariable\n.\n\u017e\ngroup education vs. none\n.\ngender males vs. females\nses high vs. low\nlifetime number of partners\n\ninterval\n\u017e\n.\n1.17, 13.9\n\u017e\n1.23. 12.88\n\u017e\n1.87, 18.28\n\u017e\n1.08, 11.31\n.\nsource: v. i. rickert et al., clin. pediatr. 31: 205\u1390210 1992 .\n\n4.04\n1.38\n5.82\n3.22\n\n.\n.\n.\n\n.\n\n\u017e\n\n\u017e\n\n "}, {"Page_number": 218, "text": "problems\n\n203\n\ntable 5.17 data for problem 5.15\n\nvariable\nintercept\nalcohol use\nsmoking\nrace\nrace = smoking\n\neffect\ny7.00\n0.10\n1.20\n0.30\n0.20\n\np-value\n- 0.01\n0.03\n- 0.01\n0.02\n0.04\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n5.15 table 5.17 shows estimated effects for a logistic regression model with\nsquamous cell esophageal cancer y s 1, yes; y s 0, no as the re-\nsponse. smoking status s equals 1 for at least one pack per day and 0\notherwise, alcohol consumption a equals the average number of\nalcoholic drinks consumed per day, and race r equals 1 for blacks\nand 0 for whites. to describe the race = smoking interaction, con-\nstruct the prediction equation when r s 1 and again when r s 0.\nfind the fitted ys conditional odds ratio for each case. similarly,\nconstruct\nthe prediction equation when s s 1 and again when s s 0. find the\nfitted yr conditional odds ratios. note that for each association,\nthe coefficient of the cross-product term is the difference between the\nlog odds ratios at the two fixed levels for the other variable. explain\nwhy the coefficient of s represents the log odds ratio between y and s\nfor whites. to what hypotheses do the p-values for r and s refer?\n\n\u017e\n\n.\n\n5.16 a survey of high school students on y s whether the subject has\ndriven a motor vehicle after consuming a substantial amount of alcohol\n1 s yes , s s gender 1 s female , r s race 1 s black; 0 s white ,\n\u017e\n.\nand g s grade g s 1, grade 9; g s 1, grade 10; g s 1, grade 11;\ng s g s g s 0, grade 12 has prediction equation\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n1\n\n2\n\n3\n\n1\n\n2\n\n3\n\nlogit p y s 1 s y0.88 y 0.40 s y 0.72 r y 2.22 g y 1.43 g y 0.58 g\n\n\u02c6\n\u017e\n\n.\n\n1\n\n2\n\n3\n\nq 0.74rg q 0.38rg q 0.01rg .\n\n1\n\n2\n\n3\n\na. carefully interpret effects. explain the interaction by describing the\n\nrace effect at each grade and the grade effect for each race.\n\nb. replace r above by r\n\n1 s black, 0 s other . the study also\n\u017e\n1 s hispanic, 0 s other , with r s r s 0 for white.\n\u017e\nmeasured r\nsuppose that the prediction equation is as above but with additional\nterms y0.29 r q 0.53 r g q 0.25 r g y 0.06 r g . interpret the\neffects.\n\n.\n\n.\n\n1\n\n2\n\n1\n\n2\n\n2\n\n2\n\n1\n\n2\n\n2\n\n2\n\n3\n\n "}, {"Page_number": 219, "text": "204\n\nlogistic regression\n\ntable 5.18 data for problem 5.17\n\npatient\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\nd\n45\n15\n40\n83\n90\n25\n35\n65\n95\n35\n75\n45\n\nt\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n1\n\ny\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\npatient\n\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\nd\n50\n75\n30\n25\n20\n60\n70\n30\n60\n61\n65\n15\n\nt\n1\n1\n0\n0\n1\n1\n1\n0\n0\n0\n0\n1\n\ny\n0\n1\n0\n1\n0\n1\n1\n1\n1\n0\n1\n0\n\npatient\n\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\nd\n20\n45\n15\n25\n15\n30\n40\n15\n135\n20\n40\n\nt\n1\n0\n1\n0\n1\n0\n0\n1\n1\n1\n1\n\ny\n0\n1\n0\n1\n0\n1\n1\n0\n1\n0\n0\n\nsource: data from d. collett, in encyclopedia of biostatistics new york: wiley: 1998 , pp.\n350\u1390358.\n\n\u017e\n\n.\n\n5.17 table 5.18 shows the results of a study about y s whether a patient\nhaving surgery with general anesthesia experienced a sore throat on\nwaking 0 s no, 1 s yes as a function of the d s duration of the\nsurgery in minutes and the t s type of device used to secure the\nairway 0 s laryngeal mask airway, 1 s tracheal\ntube . fit a logit\nmodel using these predictors, interpret parameter estimates, and con-\nduct inference about the effects.\n\n\u017e\n\u017e\n\u017e\n\n.\n\n.\n\n.\n\n5.18 refer to model 5.2 for the horseshoe crabs using x s width.\n\n.\n\n\u017e\n\u017e .\ni at the mean width 26.3 , the estimated odds of a\nii at x s 27.3, the estimated odds equal 3.40;\n\u017e .\nsince exp \u2424 s 1.64, 3.40 s 1.64 2.07, and the odds in-\n\u02c6\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\na. show that\n\nsatellite equal 2.07;\n\u017e\nand iii\ncrease by 64%.\n\n\u017e\n\n.\n\nb. based on the 95% confidence interval for \u2424, show that for x near\nwhere \u2432s 0.5, the rate of increase in the probability of a satellite\nper 1-cm increase in x falls between about 0.07 and 0.17.\n\n5.19 for table 4.3, fit a logistic regression model for the probability of a\n\nsatellite, using color alone as the predictor.\na. treat color as nominal. explain why this model is saturated. ex-\npress its parameter estimates in terms of the sample logits for each\ncolor.\n\nb. conduct a likelihood-ratio test that color has no effect.\nc. fit a model that treats color as quantitative. interpret the fit, and\n\ntest that color has no effect.\n\nd. test the goodness of fit of the model in part c . interpret.\n\n\u017e .\n\n "}, {"Page_number": 220, "text": "problems\n\n205\n\n5.20 refer to model 5.14 . describe the effect of width by finding the\nestimated probabilities of a satellite at its lower and upper quartiles,\nseparately for c s 1 and c s 0.\n\n\u017e\n\n.\n\n5.21 refer\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u02c6\n\nto the prediction equation logit \u2432 s y10.071 y 0.509c q\n0.458 x for model 5.13 . the means and standard deviations are\nc s 2.44 and s s 0.80 for color, and x s 26.30 and s s 2.11 for width.\nfor standardized predictors e.g., x s width y 26.3 r2.11 , explain\nwhy the estimated coefficients of c and x equal y0.41 and 0.97.\ninterpret\nthese by comparing the partial effects of a 1 standard\ndeviation increase in each predictor on the odds. describe the color\neffect by estimating the change in \u2432 between the first and last color\ncategories at the mean score for width.\n\n\u02c6\n\n\u017e\n\n.\n\nw\n\nx\n\n.\n5.22 refer to model 5.12 .\n\n\u017e\n\na. fit the model using x s weight. interpret effects of weight and\n\ncolor.\n\ninterpret.\n\nb. does the model permitting interaction provide an improved fit?\n\n\u017e .\n\nc. for part b , construct a confidence interval for a difference be-\ntween the slope parameters for medium-light and dark crabs.\ninterpret.\n\nd. using models that treat color as quantitative, repeat the analyses in\n\n\u017e .\nparts a to c .\n\n\u017e .\n\n\u017e\n\n.\n\n\u017e\n\n5.23 fowlkes et al. 1988 reported results of a survey of employees of a\nlarge national corporation to determine how satisfaction depends on\nrace, gender, age, and regional location. the data are at the book\u2019s\nweb site www.stat.ufl.edur; aarcdarcda.html . fit a logit model to\nthese data and carefully interpret the parameter estimates. fowlkes et\nal. 1988 reported \u2018\u2018the least-satisfied employees are less than 35\nyears of age, female, other race , and work in the northeast; . . . . the\nmost satisfied group is greater than 44 years of age, male, other, and\nworking in the pacific or mid-atlantic regions; the odds of such\nemployees being satisfied are about 3.5 to 1.\u2019\u2019 show how these inter-\npretations result from the fit of this model.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n5.24 let y denote a subject\u2019s opinion about current laws legalizing abortion\n1 s support , for gender h h s 1, female; h s 2, male , religious\n\u017e\ni s 3, jewish , and\naffiliation i\nj s 1, democrat; j s 2, republican; j s 3,\npolitical party affiliation j\nindependent . for survey data, software for fitting the model\n\n\u017e\ni s 1, protestant;\n\ni s 2, catholic;\n\n.\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\nlogit p y s 1 s \u2423q \u2424 q \u2424 q \u2424\n\n\u017e\n\n.\n\nr\n\ng\nh\n\ni\n\np\nj\n\n "}, {"Page_number": 221, "text": "206\n\nlogistic regression\nreports \u2423s 0.62, \u2424 s 0.08, \u2424 s y0.08, \u2424 s y0.16, \u2424 s\ny0.25, \u2424 s 0.41, \u2424 s 0.87, \u2424 s y1.27, \u2424 s 0.40.\na. interpret how the odds of support depends on religion.\n\u017e\nb. estimate the probability of support for the group most least\n\n\u02c6g\n1\n\u02c6p\n1\n\n\u02c6\n\u02c6r\n3\n\nlikely\n\n\u02c6g\n2\n\n\u02c6r\n1\n\n\u02c6r\n2\n\n\u02c6p\n2\n\n\u02c6p\n3\n\n.\n\nto support current laws.\n\nc. if, instead, parameters used constraints \u2424g s \u2424r s \u2424p s 0, report\n\n1\n\n1\n\n1\n\nthe estimates.\n\n5.25 table 5.19 refers to a sample of subjects randomly selected for an\nitalian study on the relation between income and whether one pos-\nsesses a travel credit card. at each level of annual income in millions\nof lira, the table indicates the number of subjects sampled and the\nnumber possessing at least one travel credit card. analyze these data.\n\ntable 5.19 data for problem 5.25\n\nincome number\n\u017e\n\nof\n\nincome number\n\u017e\n\ncredit millions\n.\n\ncases cards\n\ncredit millions\n.\n\ncases cards\n\nof\n\ncredit\ncases cards\n\n.\n\nof\n\nincome number\n\u017e\nmillions\nof lira\n24\n27\n28\n29\n30\n31\n32\n33\n34\n35\n38\n\n1\n1\n5\n3\n9\n5\n8\n1\n7\n1\n3\n\n0\n0\n2\n0\n1\n1\n0\n0\n1\n1\n1\n\nof lira\n39\n40\n41\n42\n45\n48\n49\n50\n52\n59\n60\n\nof lira\n65\n68\n70\n79\n80\n84\n94\n120\n130\n\n6\n3\n5\n1\n1\n1\n1\n6\n1\n\n6\n3\n3\n0\n0\n0\n0\n6\n1\n\n2\n5\n2\n2\n1\n1\n1\n10\n1\n1\n5\n\n0\n0\n0\n0\n1\n0\n0\n2\n0\n0\n2\n\nsource: categorical data analysis, quaderni del corso estivo di statistica e calcolo delle\nprobabilita, n. 4., istituto di metodi quantitativi, universita luigi bocconi, by r. piccarreta.\n\n`\n\n`\n\n5.26 refer to table 9.1, treating marijuana use as the response variable.\n\nanalyze these data.\n\n5.27 the book\u2019s web site www.stat.ufl.edur; aarcdarcda.html contains\na five-way table relating occupational aspirations high, low to gender,\nresidence, iq, and socioeconomic status. analyze these data.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\ntheory and methods\n.x\n5.28 for model 5.1 , show that \u2b78\u2432 x r\u2b78x s \u2424\u2432 x 1 y \u2432 x .\n\n.w\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n "}, {"Page_number": 222, "text": "problems\n\n207\n\n\u017e\n5.29 for model 5.1 , when \u2432 x\n\u017e\n\nexp \u2424 approximately as \u2432 x q 1 r\u2432 x .\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nis small, explain why you can interpret\n\n5.30 prove that the logistic regression curve 5.1 has the steepest slope\n\n\u017e\nwhere \u2432 x s . generalize to model 5.8 .\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n1\n2\n\n5.31 the calibration problem is that of estimating x at which \u2432 x s \u2432 .0\nfor the linear logit model, argue that a confidence interval is the set of\nx values for which\n\n\u017e\n\n.\n\n\u2423q \u2424x y logit \u2432 r var \u2423 q x var \u2424 q 2 x cov \u2423,\u2424\n\u02c6\n\u02c6\n\n\u017e .\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n2\n\n0\n\n1r2\n\n.\n\n- z\n\n\u2423r2\n\n.\n\nw\nmorgan 1992, sec. 2.7 surveyed other approaches.\n\n.\n\n\u017e\n\nx\n\n\u017e\n\n5.32 a study for several professional sports of the effect of a player\u2019s draft\nposition d d s 1, 2, 3, . . . of selection from the pool of potential\nplayers in a given year on the probability \u2432 of eventually being named\nan all star used the model logit \u2432 s \u2423q \u2424 log d s. m. berry, chance,\n.\n14:53\u139057, 2001 .\na. show that \u2432r 1 y \u2432 s e d . show that e s odds for the first\n\n\u2423 \u2424\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u2423\n\ndraft pick.\n\nb. in the united states, berry reported \u2423s 2.3 and \u2424s y1.1 for pro\nbasketball and \u2423s 0.7 and \u2424s y0.6 for pro baseball. this sug-\ngests that in basketball a first draft pick is more crucial and picks\nwith high d are relatively less likely to be all-stars. explain why.\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n5.33 for the population of subjects having y s j, x has a n \u242e, \u2434\n\n\u017e\n\n2.\n\nj\n\ndistribution,\n\nj s 0,1.\n\n.\n\nsatisfies the logistic\n\na. using bayes theorem, show that p y s 1 x\n<\n\nregression model with \u2424s \u242e y \u242e r\u2434 .\n\n\u017e\n\n2\n\n\u017e\n.\n0\n2.\n\n1\n\u017e\n\n<\n\nj\n\nj\n\n1\n\n0\n\n\u017e\n\n.\n\n\u017e\n\n. w\n\nb. suppose that x y s j\n\nis n \u242e,\u2434 with \u2434 / \u2434 . show that the\nlogistic model holds with a quadratic term anderson 1975 . prob-\nlem 5.4 showed that a quadratic term is helpful when x values have\nquite different dispersion at y s 0 and y s 1. this result also\nsuggests that to test equality of means of normal distributions when\nthe variances differ, one can fit a quadratic logistic regression with\nthe two groups as the response and test the quadratic term; see\n. x\no\u2019brien 1988 .\n\u017e\n\u0004w\n\nc. suppose that x y s j has exponential dispersion family density\nf x; \u242a s exp x\u242a y b \u242a ra \u243e q c x, \u243e . find the relevant lo-\n\u017e\ngistic model.\n\n.x\n\n.4\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nj\n\nj\n\nj\n\n<\n\n "}, {"Page_number": 223, "text": "208\n\nlogistic regression\nd. for multiple predictors, suppose that x y s j has a multivariate\nj s 0, 1. show that p y s 1 x satisfies lo-\ncornfield\n\n\u017e\n<\nn \u242e , \u233a distribution,\ngistic regression with effect parameters \u233a \u242e y \u242e\n.\n1962 .\n\n\u017e\ny1\u017e\n\n. \u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n1\n\n0\n\nj\n\n<\n\n5.34 suppose that \u2432 x s f x for some strictly increasing cdf f. explain\nwhy a monotone transformation of x exists such that the logistic\nregression model holds. generalize to alternative link functions.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\n1\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u017e\n\n.\n5.35 for an i = 2 contingency table, consider logit model 5.4 .\na. given \u2432 ) 0 , show how to find \u2424 satisfying \u2424 s 0.\nb. prove that \u2424 s \u2424 s \u2b48\u2b48\u2b48 s \u2424 is the independence model. find its\n\n.x\nlikelihood equation, and show that \u2423s logit \u00fd y r \u00fd n .\n.x\n\n5.36 construct the log-likelihood function for the model logit \u2432 x s \u2423q\n\u2424x with independent binomial outcomes of y successes in n trials at\nx s 0 and y\nsuccesses in n trials at x s 1. derive the likelihood\nequations, and show that \u2424 is the sample log odds ratio.\n\n1\u02c6\n\nw\u017e\n\n\u02c6\n\n.\n\n\u017e\n\n\u017e\n\nw\n\n2\n\n0\n\n0\n\n1\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n5.37 a study has n independent binary observations\n\nwhen\nx s x , i s 1, . . . , n, with n s \u00fd n . consider the model logit \u2432 s\ni\n\u2423q \u2424x , where \u2432 s p y s 1 .\n.\na. show that the kernel of the likelihood function is the same treating\n\ny , . . . , y\ni1\n\n\u017e\n\n.\n\ni j\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n4\nin i \u017e\n\n\u0004\n\nthe data as n bernoulli observations or n binomial observations.\n\nb. .for the saturated model, explain why the likelihood function is\ndifferent for these two data forms. hint: the number of parame-\nters differs. hence, the deviance reported by software depends on\nthe form of data entry.\n\n\u017e\n\n.\n\nmodels does not depend on the form of data entry.\n\nc. explain why the difference between deviances for two unsaturated\nd. suppose that each n s 1. show that the deviance depends on \u2432\u02c6\ni\nbut not y . hence, it is not useful for checking model fit see also\n.\nproblem 4.22 .\n\n\u017e\n\ni\n\ni\n\n.\n5.38 suppose that y has a bin n, \u2432 distribution. for the model, logit \u2432\n\n\u017e\n\n.\n\n\u017e\n\n0\n\n.\n\n\u017e\n\n\u017e\n\n.xy1\n\ns \u2423, consider testing h : \u2423s 0 i.e., \u2432s 0.5 . let \u2432s yrn.\na. from section 3.1.6, the asymptotic variance of \u2423s logit \u2432 is\nw\nn\u2432 1 y \u2432 . compare the estimated se for the wald test and\nx2\nthe se using the null value of \u2432, using test statistic logit \u2432 rse .\nshow that the ratio of the wald statistic to the statistic with null se\nequals 4\u2432 1 y \u2432 . what is the implication about performance of\nthe wald test if \u2423 is large and \u2432 tends to be near 0 or 1?\n\n\u02c6\nw\n\n\u02c6\n<\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n<\n\n\u02c6\n\n "}, {"Page_number": 224, "text": "problems\n\n209\n\nw\u017e\n\nb. wald inference depends on the parameterization. how does the\ncomparison of tests change with the scale \u2432y 0.5 rse , where\nse is now the estimated or null se of \u2432?\u02c6\nc. suppose that y s 0 or y s n. show that the wald test in part a\n\u017e .\ncannot reject h : \u2432s \u2432 for any 0 - \u2432 - 1, whereas the wald\ntest in part b rejects every such \u2432 . note: analogous results apply\nfor inference about the poisson mean versus the log mean; see\n. x\nmantel 1987a .\n\n\u017e .\n\nx2\n\n\u02c6\n\n\u017e\n\n.\n\nw\n\n0\n\n0\n\n0\n\n0\n\n5.39 find the likelihood equations for model 5.10 . show that they imply\nin the\n\nthe fitted values and that the sample values are identical\nmarginal two-way tables.\n\n\u017e\n\n.\n\n5.40 consider the linear logit model 5.5 for an i = 2 table, with y ai\n\n\u017e\n\n.\n\n.\n\n\u017e\n\nbin n , \u2432 variate.\na. show that the log likelihood is\n\ni\n\ni\n\nl \u2424 s y \u2423q \u2424x y n log 1 q exp \u2423q \u2424x\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\ni\n\n.\n\ni\n\n.\n\ni\n\n\u00fd\nis1\n\ni\n\ni\n\n\u00fd\nis1\n\ni\n\nb. show that the sufficient statistic for \u2424 is \u00fd y x , and explain why\nthis is essentially the variable utilized in the cochran\u1390armitage\ntest. hence that test is a score test of h : \u2424s 0.\n\nc. letting s s \u00fd y , show that the likelihood equations are\n\n\u017e\n\n.\n\n0\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u017e\n\nexp \u2423q \u2424x\n.i\ns s n\u00fd i 1 q exp \u2423q \u2424x\n.i\nexp \u2423q \u2424x\n.i\ni 1 q exp \u2423q \u2424x\n\ny x s n x\n\n\u00fd\n\n\u017e\n\n\u017e\n\n\u017e\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u00fd\n\ni\n\n.\n\n.i\n\nd. let \u242e s n \u2432 . explain why \u00fd \u242e s \u00fd y and\n\n\u0004\n\n4\n\n\u02c6\n\ni\n\ni\n\n\u02c6\n\ni\n\ni\n\n\u02c6\n\ni\n\ni\n\ni\n\nx s x\n\u00fd\n\ni\n\ny\ni\ns\n\ni\n\n\u242e\u02c6\ni\n\u00fd \u242e\u02c6a\na\n\ni\n\n.\n\n\u00fd\n\ni\n\nexplain why this implies that the mean score on x across the rows\nin the first column is the same for the model fit as for the observed\ndata. they are also identical for the second column.\n\n "}, {"Page_number": 225, "text": "210\nlogistic regression\n5.41 let y be bin n , \u2432 at x , and let p s y rn . for binomial glms\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nwith logit link:\na. for p near \u2432 , show that\n\ni\n\ni\n\ni\n\nlog\n\np\ni\n\n1 y p\n\ni\n\nf log\n\n\u2432\ni\n\n1 y \u2432\n\ni\n\nq\n\np y \u2432\n\u2432 1 y \u2432\n\u017e\n\ni\n\ni\n\ni\n\ni\n\n.\n\n.\n\n\u017et.\ni\n\nb. show that z\nlogit, evaluated at approximation \u2432\u017et. for \u2432.\u02c6\ni\n\u02c6\n.\nc. verify the formula 5.20 for cov \u2424 .\n\n$\ni\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nin 5.23 is a linearized version of the ith sample\n\n5.42 using graphs or tables, explain what is meant by no interaction in\n\nmodeling response y and explanatory x and z when:\n.\na. all variables are continuous multiple regression .\n.\nb. y and x are continuous, z is categorical analysis of covariance .\nc. y is continuous, x and z are categorical\nd. y is binary, x and z are categorical\n\n\u017e\n.\n\u017e\ntwo-way anova .\n\n\u017e\n.\nlogit model .\n\n\u017e\n\n "}, {"Page_number": 226, "text": "c h a p t e r 6\n\nbuilding and applying logistic\nregression models\n\nhaving studied the basics of fitting and interpreting logistic regression\nmodels, we now turn our attention to building and applying them. with\nseveral explanatory variables, there are many potential models. in section 6.1\nwe discuss strategies for model selection. after choosing a preliminary model,\nmodel checking addresses whether systematic lack of fit exists. section 6.2\ncovers diagnostics, such as residuals, for model checking.\n\nin practice, a common application compares two groups on a binary\nresponse, with data stratified by control variables. in section 6.3 we present\nlogit-related analyses of such data. in section 6.4 we show the advantages of\na well-chosen model in enhancing inferential power for detecting and esti-\nmating associations. section 6.5 covers power and sample size determination\nfor logistic regression. although the logit is the most popular link function\nfor probabilities, other links are sometimes more appropriate. in section 6.6\nwe present models using the probit link and links making a double log\ntransform.\n\nfor small samples or models with many parameters, ordinary large-sample\nml inference may perform poorly. in section 6.7 we discuss conditional\nlogistic regression. like small-sample methods for 2 = 2 tables, this uses\nconditioning arguments to eliminate nuisance parameters.\n\n6.1 strategies in model selection\n\nmodel selection for logistic regression faces the same issues as for ordinary\nregression. the selection process becomes harder as the number of explana-\ntory variables increases, because of the rapid increase in possible effects and\ninteractions. there are two competing goals: the model should be complex\nenough to fit the data well. on the other hand,\nit should be simple to\ninterpret, smoothing rather than overfitting the data.\n\n211\n\n "}, {"Page_number": 227, "text": "212\n\nbuilding and applying logistic regression models\n\n.\n\n\u017e\n\nmost studies are designed to answer certain questions. those questions\nguide the choice of model terms. confirmatory analyses then use a restricted\nset of models. for instance, a study hypothesis about an effect may be tested\nby comparing models with and without that effect. for studies that are\nexploratory rather than confirmatory, a search among possible models may\nprovide clues about the dependence structure and raise questions for future\nresearch.\n\nin either case, it is helpful first to study the effect on y of each predictor\nby itself using graphics incorporating smoothing for a continuous predictor\nor a contingency table for a discrete predictor. this gives a \u2018\u2018feel\u2019\u2019 for the\nmarginal effects. unbalanced data, with relatively few responses of one type,\nlimit the number of predictors for the model. one guideline suggests at least\n10 outcomes of each type should occur for every predictor peduzzi et al.\n1996 . if y s 1 only 30 times out of n s 1000, for instance, the model should\ncontain no more than about three x terms. such guidelines are approximate,\nand this does not mean that if you have 500 outcomes of each type you are\nwell served by a model with 50 predictors.\n\nmany model selection procedures exist, no one of which is always best.\ncautions that apply to ordinary regression hold for any generalized linear\nmodel. for instance, a model with several predictors may suffer from multi-\ncollinearity\u138fcorrelations among predictors making it seem that no one vari-\nable is important when all the others are in the model. a variable may seem\nto have little effect because it overlaps considerably with other predictors in\nthe model, itself being predicted well by the other predictors. deleting such a\nredundant predictor can be helpful, for instance to reduce standard errors of\nother estimated effects.\n\n.\n\n\u017e\n\n6.1.1 horseshoe crab example revisited\n\u017e\nthe horseshoe crab data set in table 4.3 has four predictors: color\nfour\ncategories , spine condition three categories , weight, and width of the\ncarapace shell. we now fit a logistic regression model using all these to\npredict whether the female crab has satellites\n\n.\ny s 1 .\n.\nwe start by fitting a model containing main effects,\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nlogit p y s 1 s \u2423q \u2424 weight q \u2424 width q \u2424 c\nq \u2424 c q \u2424 c q \u2424 s q \u2424 s ,\n\n.\n\n1\n\n2\n\n3 1\n\n5 3\n\n6 1\n\n4 2\n\n7 2\n\ni\n\n\u017e\n\n.\n\nc\n\n\u017e\nand spine condition s\n\ntreating color\nas qualitative factors , with\ndummy variables for the first three colors and the first two spine conditions.\ntable 6.1 shows results. a likelihood-ratio test that y is jointly independent\nof these predictors simultaneously tests h : \u2424 s \u2b48\u2b48\u2b48 s \u2424 s 0. the test\nstatistic equals 40.6 with df s 7 p - 0.0001 . this shows extremely strong\nevidence that at least one predictor has an effect.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n0\n\n1\n\n7\n\nj\n\n "}, {"Page_number": 228, "text": "strategies in model selection\n\n213\n\ntable 6.1 computer output from fitting model with all main\neffects to horseshoe crab data\n\ntesting global null hypothesis: beta = 0\n\ntest\nlikelihood ratio\n\nchi- square\n40.5565\n\ndf\n7\n\npr > chisq\n<.0001\n\nanalysis of maximum likelihood estimates\n\nparameter\nintercept\nweight\nwidth\ncolor 1\ncolor 2\ncolor 3\nspine 1\nspine 2\n\nestimate\ny9.2734\n0.8258\n0.2631\n1.6087\n1.5058\n1.1198\ny0.4003\ny0.4963\n\nstd error\n\nchi- square\n\npr > chisq\n\n3.8378\n0.7038\n0.1953\n0.9355\n0.5667\n0.5933\n0.5027\n0.6292\n\n5.8386\n1.3765\n1.8152\n2.9567\n7.0607\n3.5624\n0.6340\n0.6222\n\n0.0157\n0.2407\n0.1779\n0.0855\n0.0079\n0.0591\n0.4259\n0.4302\n\nalthough the overall test is highly significant, the table 6.1 results are\ndiscouraging. the estimates for weight and width are only slightly larger than\ntheir se values. the estimates for the factors compare each category to the\nfinal one as a baseline. for color, the largest difference is less than two\nstandard errors; for spine condition, the largest difference is less than a\nstandard error.\n\nthe small p-value for the overall test, yet the lack of significance for\nindividual effects, is a warning sign of multicollinearity. in section 5.2.2 we\nshowed strong evidence of a width effect. controlling for weight, color, and\nspine condition, little evidence remains of a partial width effect. however,\nweight and width have a strong correlation 0.887 . for practical purposes\nthey are equally good predictors, but it is nearly redundant to use them both.\nour further analysis uses width w with color c and spine condition s as\npredictors. for simplicity, we symbolize models by their highest-order terms,\nregarding c and s as factors. for instance, c q s q w denotes a model\nwith main effects, whereas c q s*w denotes a model that has those main\neffects plus an s = w interaction. it is not usually sensible to consider a\nmodel with interaction but not the main effects that make up that interac-\ntion.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n6.1.2 stepwise procedures\nin exploratory studies, an algorithmic method for searching among models\ncan be informative if we use results cautiously. goodman 1971a proposed\nmethods analogous to forward selection and backward elimination in ordi-\nnary regression.\n\nforward selection adds terms sequentially until further additions do not\nimprove the fit. at each stage it selects the term giving the greatest improve-\n\n\u017e\n\n.\n\n "}, {"Page_number": 229, "text": "214\n\nbuilding and applying logistic regression models\n\nment in fit. the minimum p-value for testing the term in the model is a\nsensible criterion, since reductions in deviance for different terms may have\ndifferent df values. a stepwise variation of this procedure retests, at each\nstage, terms added at previous stages to see if they are still significant.\n\nbackward elimination begins with a complex model and sequentially\nremoves terms. at each stage, it selects the term for which its removal has\nthe least damaging effect on the model e.g., largest p-value . the process\nstops when any further deletion leads to a significantly poorer fit. with either\napproach, for qualitative predictors with more than two categories, the\nprocess should consider the entire variable at any stage rather than just\nindividual dummy variables. add or drop the entire variable rather than\njust one of its dummies. otherwise, the result depends on the coding. the\nsame remark applies to interactions containing that variable.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\nmany statisticians prefer backward elimination over forward selection,\nfeeling it safer to delete terms from an overly complex model than to add\nterms to an overly simple one. forward selection can stop prematurely\nbecause a particular test in the sequence has low power. neither strategy\nnecessarily yields a meaningful model. use variable selection procedures with\ncaution! when you evaluate many terms, one or two that are not important\nmay look impressive simply due to chance. for instance, when all the true\neffects are weak, the largest sample effect may substantially overestimate its\ntrue effect. see westfall and wolfinger 1997 and westfall and young\n\u017e\n1993 for ways to adjust p-values to take multiple tests into account.\n\nsome software has additional options for selecting a model. one approach\nattempts to determine the best model with some fixed number of terms,\naccording to some criterion. if such a method and backward and forward\nselection procedures yield quite different models, this is an indication that\nsuch results are of dubious use. another such indication would be when a\nquite different model results from applying a given procedure to a bootstrap\nsample of the same size from the sample distribution.\n\nfinally, statistical significance should not be the sole criterion for inclusion\nof a term in a model. it is sensible to include a variable that is central to the\npurposes of the study and report its estimated effect even if it is not\nstatistically significant. keeping it in the model may help reduce bias in\nestimated effects of other predictors and may make it possible to compare\nresults with other studies where the effect is significant perhaps because of a\nlarger sample size. algorithmic selection procedures are no substitute for\ncareful thought in guiding the formulation of models.\n\n.\n\n\u017e\n\n6.1.3 backward elimination for horseshoe crab example\ntable 6.2 summarizes results of fitting and comparing several logit models to\nthe horseshoe crab data with predictors width, color, and spine condition.\nthe deviance g test of fit compares the model to the saturated model. as\nnoted in sections 5.2.4 and 5.2.5, this is not approximately chi-squared when\na predictor is continuous, as width is. however, the difference of deviances\n\n2.\n\n\u017e\n\n "}, {"Page_number": 230, "text": "strategies in model selection\n\n215\n\ntable 6.2 results of fitting several logistic regression models\nto horseshoe crab data\n\nmodel predictors\n.\n\na\n\ndeviance\n\n2\n\ng\n\nmodels\n\ndeviance corr.\n\ndf aic compared difference\n\n\u017e\n.\nr y,\u242e\u02c6\n\n.\n\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\n.\n.\n\n1\nc*s*w\n170.44 152 212.4\nc*s q c*w q s*w 173.68 155 209.7\n2\nc*s q s*w\n.\n3a\n177.34 158 207.3\nc*w q s*w\n3b\n181.56 161 205.6\nc*s q c*w\n173.69 157 205.7\n3c\ns q c*w\n.\n4a\n181.64 163 201.6\n4b w q c*s\n.\n177.61 160 203.6\nc q s q w\n186.61 166 200.6\n5\nc q s\n.\n208.83 167 220.8\n6a\ns q w\n194.42 169 202.4\n6b\nc q w\n6c\n187.46 168 197.5\n.\n212.06 169 220.1\n7a\nc\n.\n194.45 171 198.5\n7b w\nc s dark q w\n187.96 170 194.0\n8\n9 none\n225.76 172 227.8\n\n.\n.\n\n.\n\n.\n\n\u138f\n\n\u017e . \u017e .\n2 \u1390 1\n\u017e\n. \u017e .\n3a \u1390 2\n. \u017e .\n\u017e\n3b \u1390 2\n\u017e\n. \u017e .\n3c \u1390 2\n.\n\u017e\n. \u017e\n4a \u1390 3c\n. \u017e\n.\n\u017e\n4b \u1390 3c\n\u017e . \u017e\n.\n5 \u1390 4b\n\u017e\n. \u017e .\n6a \u1390 5\n\u017e\n. \u017e .\n6b \u1390 5\n\u017e\n. \u017e .\n6c \u1390 5\n\u017e\n. \u017e\n7a \u1390 6c\n. \u017e\n\u017e\n7b \u1390 6c\n\u017e . \u017e\n8 \u1390 6c\n\u017e . \u017e .\n9 \u1390 8\n\n.\n.\n.\n\n\u138f\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\n3.2 df s 3\n.\n3.7 df s 3\n.\n7.9 df s 6\n.\n0.0 df s 2\n.\n8.0 df s 6\n.\n3.9 df s 3\n.\n9.0 df s 6\n.\n22.2 df s 1\n.\n7.8 df s 3\n.\n0.8 df s 2\n.\n24.5 df s 1\n.\n7.0 df s 3\n.\n0.5 df s 2\n.\n37.8 df s 2\n.\n\n0.452\n0.285\n0.402\n0.447\n0.000\n\nac, color; s, spine condition; w, width.\n\n1\n\n0\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\nbetween two models that differ by a modest number of parameters is\nrelevant. that difference is the likelihood-ratio statistic y2 l y l com-\nparing the models, and it has an approximate null chi-squared distribution..\nto select a model, we use backward elimination. we test only the\nhighest-order terms for each variable. it is inappropriate, for instance, to\nremove a main effect term if the model has interactions involving that term.\nwe begin with the most complex model, symbolized by c*s*w , model 1\nin table 6.2. this model uses main effects for each term as well as the three\ntwo-factor interactions and the three-factor interaction. it allows a separate\nwidth effect at each cs combination. in fact, at some of those combinations\ny outcomes of only one type occur, so effects are not estimable. the\nlikelihood-ratio statistic comparing this model to the simpler model c*s q\nc*w q s*w removing the three-factor interaction term equals 3.2 df s 3 .\n.\nthis suggests that the three-factor term is not needed p s 0.36 , thank\ngoodness, so we continue the simplification process.\nin the next stage we consider the three models that remove a two-factor\ninteraction. of these, c*s q c*w gives essentially the same fit as the more\ncomplex model, so we drop the s = w interaction. next, we consider\ndropping one of the other two-factor interactions. the model s q c*w ,\n.\ndropping the c = s interaction, has an increased deviance of 8.0 on df s 6\np s 0.24 ; the model w q c*s , dropping the c = w interaction, has an\n\u017e\nincreased deviance of 3.9 on df s 3 p s 0.27 . neither increase is impor-\ntant, suggesting that we can drop either and proceed. in either case, drop-\nping next the remaining interaction also seems permissible. for instance,\n\n\u017e\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n "}, {"Page_number": 231, "text": "\u017e\n\n\u017e\n\n.\n\nleaving model\n\nbuilding and applying logistic regression models\n\n216\ndropping the c = s interaction from model w q c*s ,\n.\nc q s q w , increases the deviance by 9.0 on df s 6 p s 0.17 .\n.\n\u017e\nthe working model now has the main effects alone. in the next stage we\nconsider dropping one of them. table 6.2 shows little consequence of\nremoving s. both remaining variables c and w then have nonnegligible\neffects. for instance, removing c increases the deviance comparing models\n7b and 6c by 7.0 on df s 3 p s 0.07 . the analysis in section 5.4.6 revealed\na noticeable difference between dark crabs category 4 and the others. the\nsimpler model that has a single dummy variable for color, equaling 0 for dark\ncrabs and 1 otherwise, fits essentially as well. the deviance difference\nbetween models 8 and 6c equals 0.5, with df s 2. further simplification\nresults in large increases in deviance and is unjustified.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n6.1.4 aic, model selection, and the correct model\nin selecting a model, we are mistaken if we think that we have found the true\none. any model is a simplification of reality. for instance, width does not\nexactly have a linear effect on the probability of satellites, whether we use the\nlogit link or the identity link.\n\nwhat is the logic of testing the fit of a model when we know that it does\nnot truly hold? a simple model that fits adequately has the advantages of\nmodel parsimony. if a model has relatively little bias, describing reality well,\nit tends to provide more accurate estimates of the quantities of interest. this\nwas discussed in sections 3.3.7 and 5.2.2 and is examined further in section\n6.4.5.\n\n.\n\n\u017e\n\nother criteria besides significance tests can help select a good model in\nterms of estimating quantities of interest. the best known is the akaike\ninformation criterion aic . it judges a model by how close its fitted values\ntend to be to the true values, in terms of a certain expected value. even\nthough a simple model is farther from the true model than is a more complex\nmodel, it may be preferred because it tends to provide better estimates of\ncertain characteristics of the true model, such as cell probabilities. thus, the\noptimal model is the one that tends to have fit closest to reality. given a\nsample, akaike showed that this criterion selects the model that minimizes\naic s y2 maximized log likelihood\u138fnumber of parameters in model .\n.\n\n\u017e\n\n2\n\nw\n\n\u017e\n\nthis penalizes a model for having many parameters. with models for\ncategorical y, this ordering is equivalent to one based on an adjustment of\nthe deviance, g y 2 df , by twice its residual df. for cogent arguments\n.\nsupporting this criterion, see burnham and anderson 1998 .\n\n.x\n\nwe illustrate aic for model selection using the models table 6.2 lists.\nthat table also shows the aic values. of models using the three basic\nvariables, aic is smallest aic s 197.5 for c q w, having main effects of\ncolor and width. the simpler model having a dummy variable for whether a\ncrab is dark fares better yet aic s 194.0 . either model seems reasonable.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n "}, {"Page_number": 232, "text": "strategies in model selection\n\n217\n\nwe should balance the lower aic for the simpler model against its having\nbeen suggested by the fit of c q w.\n\n6.1.5 using causal hypotheses to guide model building\nalthough selection procedures are helpful exploratory tools, the model-build-\ning process should utilize theory and common sense. often, a time ordering\namong the variables suggests possible causal relationships. analyzing a cer-\ntain sequence of models helps to investigate those relationships goodman\n.\n1973 .\n\n\u017e .\n\nwe illustrate with table 6.3, from a british study. a sample of men and\nwomen who had petitioned for divorce and a similar number of married\nformer husbandrwife,\npeople were asked: a \u2018\u2018before you married your\n\u017e\n\u017e .\n.\nformer\nhad you ever made love with anyone else?\u2019\u2019; b \u2018\u2018during your\nmarriage, did you have have you had any affairs or brief sexual encounters\nwith another manrwoman?\u2019\u2019 the 2 = 2 = 2 = 2 table has variables g s\ngender, e s whether reported extramarital sex, p s whether reported pre-\nmarital sex, and m s marital status.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\nthe time points at which responses on the four variables occur suggests\n\nthe following ordering of the variables:\n\ng\n\ngender\n\npremarital\n\np\n\nsex\n\nextramarital\n\ne\n\nsex\n\nm\n\nmarital\nstatus\n\nany of these is an explanatory variable when a variable listed to its right is\nthe response. figure 6.1 shows one possible causal structure. in this figure, a\nvariable at the tip of an arrow is a response for a model at some stage. the\nexplanatory variables have arrows pointing to the response, directly or\nindirectly.\n\nwe first treat p as a response. figure 6.1 predicts that g has a direct\neffect on p, so the model of independence of these variables is inadequate.\n\n)\ntable 6.3 marital status by report of pre- and extramarital sex pms and ems\n\n(\n\ngender\n\nwomen\n\nmen\n\nyes\n\nno\n\nyes\n\nno\n\nyes\n36\n4\n\nno\n214\n322\n\nyes\n28\n11\n\nno\n60\n42\n\nyes\n17\n4\n\nno\n68\n130\n\npms:\nems:\n\nmarital status\ndivorced\nstill married\n\nyes\n17\n4\n\nno\n54\n25\n\u017e\n\nsource: g. n. gilbert, modelling society london: george allen & unwin, 1981 . reprinted with\npermission from unwin hyman ltd.\n\n.\n\n6\n6\n6\n "}, {"Page_number": 233, "text": "218\n\nbuilding and applying logistic regression models\n\nfigure 6.1 causal diagram for table 6.3.\n\nat the second stage, e is the response. figure 6.1 predicts that p and g\nhave direct effects on e. it also suggests that g has an indirect effect on e,\nthrough its effect on p. these effects on e can be analyzed using the logit\nmodel for e with additive g and p effects. if g has only an indirect effect\non e, the model with p alone as a predictor is adequate; that is, controlling\nfor p, e and g are conditionally independent. at the third stage, m is the\nresponse. figure 6.1 predicts that e has a direct effect on m, p has direct\neffects and indirect effects through its effects on e, and g has indirect\neffects through its effects on p and e. this suggests the logit model for m\nhaving additive e and p effects. for this model, g and m are independent,\ngiven p and e.\n\ntable 6.4 shows results. the first stage, having p as the response, shows\nstrong evidence of a gp association. the sample odds ratio for their\nmarginal table is 0.27; the estimated odds of premarital sex for females are\n0.27 times that for males. the second stage has e as the response. only weak\nevidence occurs that g had a direct as well as an indirect effect on e, as g2\ndrops by 2.9 df s 1 after adding g to a model already containing p as a\npredictor. for this model, the estimated ep conditional odds ratio is 4.0.\n\nthe third stage has m as the response. figure 6.1 specifies the logit model\nwith main effects of e and p, but it fits poorly. the model that allows an\n\n.\n\n\u017e\n\ntable 6.4 goodness of fit of various models for table 6.3 a\n\nresponse\nvariable\n\npotential\n\nactual\n\n2\n\n1\n\np\n\ne\n\ng\n\nstage\n\nexplanatory\n\nexplanatory\nnone\n\u017e\n.g\nnone\n.p\n\u017e\ng q p\n\u017e\ne q p\n\u017e\n\u017e\n.e*p\ne*p q g\n\u017e\nap, premarital sex; e, extramarital sex; m, marital status; g, gender.\n\ng, p, e\n\ng, p\n\nm\n\n.\n.\n\n3\n\n.\n\n2\n\ng\n75.3\n0.0\n48.9\n2.9\n0.0\n18.2\n5.2\n0.7\n\ndf\n1\n0\n3\n2\n1\n5\n4\n3\n\n "}, {"Page_number": 234, "text": "logistic regression diagnostics\n\n219\n\ne = p interaction in their effects on m but assumes conditional indepen-\ndence of g and m fits much better g decrease of 13.0, df s 1 . the model\nthat also has a main effect for g fits slightly better yet. either model is more\ncomplicated than figure 6.1 predicted, since the effects of e on m vary\naccording to the level of p. however, some preliminary thought about causal\nrelationships suggested a model similar to one giving a good fit. we leave it\nto the reader to estimate and interpret effects for the third stage.\n\n\u017e\n\n.\n\n2\n\n6.1.6 new model-building strategies for data mining\nas computing power continues to explode, enormous data sets are more\ncommon. a financial institution that markets credit cards may have observa-\ntions for millions of subjects to whom they sent advertising, on whether they\napplied for a card. for their customers, they have monthly data on whether\nthey paid their bill on time plus information on many variables measured on\nthe credit card application. the analysis of huge data sets is called data\nmining.\n\nmodel building for huge data sets is challenging. there is currently\nconsiderable study of alternatives to traditional statistical methods, including\nautomated algorithms that ignore concepts such as sampling error or model-\ning. significance tests are usually irrelevant, as nearly any variable has a\nsignificant effect if n is sufficiently large. model-building strategies view\nsome models as useful for prediction even if they have complex structure.\nnonetheless, a point of diminishing returns still occurs in adding predictors\nto models. after a point, new predictors tend to be so correlated with a linear\ncombination of ones already in the model that they do not improve predictive\npower. for large n, inference is less relevant than summary measures of\npredictive power. this is a topic of the next section.\n\n6.2 logistic regression diagnostics\n\nin section 5.2.3 we introduced statistics for checking model fit in a global\nsense. after selecting a preliminary model, we obtain further insight by\nswitching to a microscopic mode of analysis. in contingency tables, for\ninstance, the pattern of lack of fit revealed in cell-by-cell comparisons of\nobserved and fitted counts may suggest a better model. for continuous\npredictors, graphical displays are also helpful. such diagnostic analyses may\nsuggest a reason for the lack of fit, such as nonlinearity in the effect of an\nexplanatory variable.\n\n6.2.1 pearson, deviance, and standardized residuals\nwith categorical predictors, it is useful to form residuals to compare ob-\nserved and fitted counts. let y denote the binomial variate for n trials at\n\ni\n\ni\n\n "}, {"Page_number": 235, "text": "'\n.\n\n220\nbuilding and applying logistic regression models\nsetting i of the explanatory variables, i s 1, . . . , n. let \u2432 denote the model\nestimate of p y s 1 . then n \u2432 is the fitted number of successes. for a\nglm with binomial random component, the pearson residual 4.36 for this\nfit is\n\n\u02c6i\n\n\u02c6i\n\n.\n\n\u017e\n\n\u017e\n\n.\n\ni\n\ny y n \u2432\n\u02c6\ne s\ni $ 1r2\nvar y\u017e\n.i\n\ni\n\ni\n\ni\n\ns\n\ny y n \u2432\n\u02c6\ni\ni\nn \u2432 1 y \u2432\n\u02c6\n\u02c6\n\n\u017e\n\ni\n\ni\n\ni\n\ni\n\n.\n\n.\n\n6.1\u017e\n\n.\n\nthis divides the raw residual\ndeviation of y . the pearson statistic for testing the model fit satisfies\n\ny y \u242e by the estimated binomial standard\n\n\u02c6\n\n\u017e\n\ni\n\ni\n\ni\n\nn\n\nx s e .\u00fd i\n\n2\n\n2\n\nis1\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n4\n\n4\n\n\u0004\n\n\u0004\n\n\u017e\n\n\u02c6i\n\neach squared pearson residual is a component of x 2.\n.\n\nwith \u2432 replaced by \u2432 in the numerator of 6.1 , e is the difference\nbetween a binomial random variable and its expectation, divided by its\nestimated standard deviation. for large n , e\nthen has an approximate\n\u017e\nn 0, 1 distribution, when the model holds. since \u2432 is estimated by \u2432 and\ny y n \u2432 tend to be smaller than\nthe \u2432 depend on\ny y n \u2432 and the e are less variable than n 0, 1 . if x has df s \u242f, x\n\u0004\n\u0004\ns \u00fd e is asymptotically comparable to the sum of squares of \u242f rather than\n.n independent standard normal random variables. thus, when the model\nholds, e \u00fd e rn f \u242frn - 1.\n\n4\ny , however,\ni\n\n.\n\u02c6\n\n4\n\u017e\n\ni\n2\ni\n\n\u0004\n4\n\n\u02c6\n\n\u02c6\n\nthe standardized pearson residual is slightly larger in absolute value and\nis approximately n 0, 1 when the model holds. in section 4.5.5 we showed\nthe adjustment uses the leverage from an estimated hat matrix. for observa-\ntion i with leverage h , the standardized residual is\ny y n \u2432\u02c6\n\u017e\n.\n\nn \u2432 1 y \u2432 1 y h\n\u02c6\n\n'1 y h\n\u02c6\n\nr s\n\ns\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n2.\ni\n\n.\n\n\u017e\n\ne\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2\n\n2\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n'\n\nabsolute values larger than roughly 2 or 3 provide evidence of lack of fit.\nan alternative residual uses components of the g2 fit statistic. these are\nthe de\u00aeiance residuals, introduced for glms in 4.35 . the deviance residual\nfor observation i is\n\n\u017e\n\n.\n\n'\nd = sign y y n \u2432 ,\n.\u02c6\n\n\u017e\n\ni\n\ni\n\ni\n\ni\n\nwhere\n\nd s 2 y log\n\ni\n\ni\n\n\u017e\n\ny\ni\n\u02c6\nn \u2432\ni\ni\n\nq n y y\n\n\u017e\n\ni\n\n.\n\ni\n\nlog\n\ni\n\nn y y\ni\nn y n \u2432\n\u02c6\n\ni\n\ni\n\ni\n\n6.2\u017e\n\n.\n\n/\n\n.\n\nthis also tends to be less variable then n 0, 1 and can be standardized.\n\n\u017e\n\n.\n\n "}, {"Page_number": 236, "text": "logistic regression diagnostics\n\n221\n\nplots of residuals against explanatory variables or linear predictor values\nmay detect a type of lack of fit. when fitted values are very small, however,\njust as x 2 and g2\nlose relevance, so do residuals. when explanatory\nvariables are continuous, often n s 1 at each setting. then y can equal only\n0 or 1, and e can assume only two values. one must then be cautious about\nregarding either outcome as extreme, and a single residual is usually uninfor-\nmative. plots of residuals also then have limited use, consisting simply of two\nparallel lines of dots. the deviance itself is then completely uninformative\n\u017e\nproblem 5.37 . when data can be grouped into sets of observations having\ncommon predictor values, it is better to compute residuals for the grouped\ndata than for individual subjects.\n\n.\n\ni\n\ni\n\ni\n\n6.2.2 heart disease example\na sample of male residents of framingham, massachusetts, aged 40 through\n.\n59, were classified on several factors, including blood pressure table 6.5 .\nthe response variable is whether they developed coronary heart disease\nduring a six-year follow-up period.\n\nlet \u2432 be the probability of heart disease for blood pressure category i.\nthe table shows the fit and the standardized pearson residuals for two\nlogistic regression models. the first model,\n\n\u017e\n\ni\n\nlogit \u2432 s \u2423,\n\n\u017e\n\n.i\n\ntreats the response as independent of blood pressure. some residuals for that\nmodel are large. this is not surprising, since the model fits poorly g s 30.0,\nx s 33.4, df s 7 .\n.\n\n\u017e\n\n2\n\n2\n\ntable 6.5 standardized pearson residuals for logit models fitted to\ndata on blood pressure and heart disease\n\nblood\npressure\n- 117\n117\u1390126\n127\u1390136\n137\u1390146\n147\u1390156\n157\u1390166\n167\u1390186\n) 186\n\nsample\n\nsize\n156\n252\n284\n271\n139\n85\n99\n43\n\nobserved\n\nheart\ndisease\n\n3\n17\n12\n16\n12\n8\n16\n8\n\nfitted\n\nresidual\n\nindep.\nmodel\n10.8\n17.4\n19.7\n18.8\n9.6\n5.9\n6.9\n3.0\n\nlinear\nlogit\n5.2\n10.6\n15.1\n18.1\n11.6\n8.9\n14.2\n8.4\n\nindep.\nmodel\ny2.62\ny0.12\ny2.02\ny0.74\n0.84\n0.93\n3.76\n3.07\n\nlinear\nlogit\ny1.11\n2.37\ny0.95\ny0.57\n0.13\ny0.33\n0.65\ny0.18\n\n.\nsource: data from cornfield 1962 .\n\n\u017e\n\n "}, {"Page_number": 237, "text": "222\n\nbuilding and applying logistic regression models\n\ntable 6.6 residuals reported in sas for heart disease data of table 6.5 a\n\nobserv\n\ndisease\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n3\n17\n12\n16\n12\n8\n16\n8\n\nobservation statistics\n\nn\n\n156\n252\n284\n271\n139\n85\n99\n43\n\nblood\n111.5\n121.5\n131.5\n141.5\n151.5\n161.5\n176.5\n191.5\n\nreschi\ny0.9794\n2.0057\ny0.8133\ny0.5067\n0.1176\ny0.3042\n0.5135\ny0.1395\n\nresdev\ny1.0617\n1.8501\ny0.8420\ny0.5162\n0.1170\ny0.3088\n0.5050\ny0.1402\n\nstreschi\ny1.1058\n2.3746\ny0.9453\ny0.5727\n0.1261\ny0.3261\n0.6520\ny0.1773\n\nareschi, pearson residual; streschi, adjusted residual.\n\na plot of the residuals show an increasing trend. this suggests the linear\n\nlogit model,\n\nlogit \u2432 s \u2423q \u2424x ,\n\n\u017e\n\n.i\n\ni\n\ni\n\n4\n\n\u017e\n\n.\n\n\u0004\nwith scores x\nfor blood pressure level. we used scores 111.5, 121.5, 131.5,\n141.5, 151.5, 161.5, 176.5, 191.5 . the nonextreme scores are midpoints for\nthe intervals of blood pressure. the trend in residuals disappears for this\nmodel, and only the second category shows some evidence of lack of fit.\n\ntable 6.6 reports residuals for the linear logit model, as reported by sas.\nthe pearson residuals reschi , deviance residuals resdev , and standard-\nized pearson residuals streschi\nshow similar results. each is somewhat\nlarge in the second category. one relatively large residual is not surprising,\nhowever. with many residuals, some may be large purely by chance. here the\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nfigure 6.2 observed and predicted proportions of heart disease for linear logit model.\n\n "}, {"Page_number": 238, "text": "2\n\n2\n\n.\n\n\u017e\n\n223\nlogistic regression diagnostics\noverall fit statistics g s 5.9, x s 6.3 with df s 6 do not indicate prob-\nlems. in analyzing residual patterns, we should be cautious about attributing\npatterns to what might be chance variation from a model.\n\nanother useful graphical display for showing lack of fit compares observed\nand fitted proportions by plotting them against each other or by plotting both\nof them against explanatory variables. for the linear logit model, figure 6.2\nplots both the observed proportions and the estimated probabilities of heart\ndisease against blood pressure. the fit seems decent.\n\nstudying residuals helps us understand either why a model fits poorly or\nwhere there is lack of fit in a generally good-fitting model. the next example\nillustrates the second case.\n\n6.2.3 graduate admissions example\ntable 6.7 refers to graduate school applications to the 23 departments in the\ncollege of liberal arts and sciences at the university of florida during the\n1997\u13901998 academic year. it cross-classifies applicant\u2019s gender g , whether\nadmitted a , and department d to which the prospective students applied.\nwe consider logit models with a as the response variable. let y denote the\nnumber admitted and let \u2432 denote the probability of admission for gender\nik\u0004\nas independent bin n , \u2432 . other things\ni in department k. we treat y\nik\nbeing equal, one would hope the admissions decision is independent of\ngender. however, the model with no gender effect, given the department,\n\nik\n\nik\n\nik\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n4\n\nlogit \u2432 s \u2423q \u2424d,\nfits rather poorly g s 44.7, x s 40.9, df s 23 .\n.\n\n.ik\n\n\u017e\n\n\u017e\n\n2\n\n2\n\nk\n\ntable 6.7 data relating admission to gender and department\nfor model with no gender effect\n\nfemales\n\nmales\n\nstd. res\n\nfemales\n\ndept yes\n32\nanth\n6\nastr\nchem\n12\n3\nclas\n52\ncomm\n8\ncomp\n35\nengl\ngeog\n9\n6\ngeol\n17\ngerm\n9\nhist\nlati\n26\n\nno yes\n21\n81\n3\n0\n43\n34\n4\n1\n5\n149\n6\n7\n30\n100\n1\n11\n15\n3\n4\n0\n21\n9\n7\n25\n\nno (fem,yes) dept yes\n41 y0.76\n21\n25\n2.87\n8\n110 y0.27\n3\n0 y1.07\n10\n10 y0.63\n25\n2\n1.16\n12\n3\n0.94\n112\n11\n2.17\n29\n6 y0.26\n16\n23\n1\n1.89\n19 y0.18\n4\n1.65\n16\n\nling\nmath\nphil\nphys\npoli\npsyc\nreli\nroma\nsoci\nstat\nzool\n\nsource: data courtesy of james booth.\n\nmales\n\nstd. res\nno yes no (fem,yes)\n10\n18\n0\n11\n34\n123\n3\n13\n33\n9\n62\n\n8\n1.37\n37\n1.29\n6\n1.34\n53\n1.32\n49 y0.23\n41 y2.27\n1.26\n2\n3\n0.14\n17\n0.30\n14 y0.01\n54 y1.76\n\n7\n31\n9\n25\n39\n4\n0\n6\n7\n36\n10\n\n "}, {"Page_number": 239, "text": "224\n\nbuilding and applying logistic regression models\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n\u017e\n\n1 k\n\n1 k\n\n1 k\n\n1 k\n\n1 k\n\n1 k\n\n1 k\n\n1 k\n\n1 k\n\n1 k\n\n2 k\n\n2 k\n\n2 k\n\n2 k\n\n2 k\n\n1 k\n.\n\ntable 6.7 also reports standardized pearson residuals for the number of\nfemales who were admitted for this model. for instance, the astronomy\ndepartment admitted 6 females, which was 2.87 standard deviations higher\nthan the model predicted. each department has only a single nonredundant\nstandardized residual, because of marginal constraints for the model. the\nmodel has fit \u2432 s y q y rn\n\u02c6 ik\n, corresponding to an independence fit\nqk\n2 k\n\u2432 s \u2432\ny q\n\u017e\n\u02c6\n\u02c6\nin each partial\n2 k\ny rn s n rn\ny s y y y n \u2432 . thus, stan-\ny y n rn\n\u017e\n2 k qk\nqk\n2 k\ny y n \u2432 and y y n \u2432 are identical. the stan-\n\u017e\ndard errors of\ndardized residuals are identical in absolute value for males and females but\nof different sign. astronomy admitted 3 males, and their standardized resid-\nual was y2.87; the number admitted was 2.87 standard deviations fewer than\npredicted. this is another advantage of standardized over ordinary pearson\nresiduals. the model of independence in a partial table has df s 1. only one\nbit of information exists about how the data depart from independence, yet\nthe ordinary pearson residual for males need not equal the ordinary pearson\nresidual for females.\n\n.\ntable. now,\n1 k qk\n2 k\n\u017e\n\ny y n \u2432 s y y n\n\n\u017e\n\u02c6\n\n2 k\n.\n\n1 k\n.\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\ndepartments with large standardized pearson residuals reveal the reason\nfor the lack of fit. significantly more females were admitted than the model\npredicts in the astronomy and geography departments, and fewer in the\npsychology department. without these three departments, the model fits\nreasonably well g s 24.4, x s 22.8, df s 20 .\n.\nfor the complete data, adding a gender effect to the model does not\nprovide an improved fit g s 42.4, x s 39.0, df s 22 , because the de-\npartments just described have associations in different directions and of\ngreater magnitude than other departments. this model has an ml estimate\nof 1.19 for the ga conditional odds ratio, the odds of admission being 19%\nhigher for females than males, given department. by contrast, the marginal\ntable collapsed over department has a ga sample odds ratio of 0.94, the\noverall odds of admission being 6% lower for females. this illustrates\nsimpson\u2019s paradox section 2.3.2 , the conditional association having differ-\nent direction than the marginal association.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2\n\n2\n\n2\n\n2\n\n.\n\n6.2.4\ninfluence diagnostics for logistic regression\nother regression diagnostic tools are also helpful\nin assessing fit. these\ninclude plots of ordered residuals against normal percentiles haberman\n1973a and analyses that describe an observation\u2019s influence on parameter\nestimates and fit statistics. whenever a residual indicates that a model fits an\nobservation poorly, it can be informative to delete the observation and refit\nthe model to remaining ones. this is equivalent to adding a parameter to the\nmodel for that observation, forcing a perfect fit for it.\n\nas in ordinary regression, an observation may be relatively influential in\ndetermining parameter estimates. the greater an observation\u2019s leverage,\ninfluence. the fit could be quite different if an\nthe greater its potential\n\n\u017e\n\n "}, {"Page_number": 240, "text": "logistic regression diagnostics\n\n225\n\nobservation that appears to be an outlier on y and has large leverage is\ndeleted. however, a single observation can have a more exorbitant influence\nin ordinary regression than a single binary observation in logistic regression,\nsince there is no bound on the distance of y from its expected value. also, in\nsection 4.5.5 we observed that the glm estimated hat matrix\n\ni\n\n$\nhat s w x x wx\n\n1r2\n\n\u02c6\n\n\u02c6\n\nx\n\n\u017e\n\ny1\n\n.\n\nx\n\n\u02c6\nx w\n\n1r2\n\ni\n\ni\n\ni\n\ni\n\ni\n\n.\n\n\u017e\n\ndepends on the fit as well as the model matrix x. for logistic regression, in\nsection 5.5.2 we showed that the weight matrix w is diagonal with element\nw s n \u2432 1 y \u2432 for the n observations at setting i of predictors. points\n\u02c6\nthat have extreme predictor values need not have high leverage. in fact, the\nleverage can be small if \u2432 is close to 0 or 1.\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\nseveral measures that describe the effect on parameter estimates and fit\nstatistics of removing an observation from the data set are related alge-\nbraically to the observation\u2019s leverage pregibon 1981; williams 1987 . in\nlogistic regression, the observation could be a single binary response or a\nbinomial response for a set of subjects all having the same predictor values.\ninfluence measures for each observation include:\n\n\u02c6i\n\n\u017e\n\n.\n\n1. for each model parameter, the change in the parameter estimate when\nthe observation is deleted. this change, divided by its standard error, is\ncalled dfbeta.\n\n2. a measure of the change in a joint confidence interval for the parame-\nters produced by deleting the observation. this confidence interval\ndisplacement diagnostic is denoted by c.\n\n3. the change in x 2 or g2 goodness-of-fit statistics when the observation\n\nis deleted.\n\n2\n\n\u017e\n\n\u017e\n\n.\n\nfor each measure, the larger the value, the greater the influence. we\nillustrate them using the linear logit model with blood pressure as a predictor\nfor heart disease in table 6.5. table 6.8 contains simple approximations due\nto pregibon 1981 for the dfbeta measure for the coefficient of blood\npressure, the confidence interval diagnostic c, the change in g2, and the\n2 .\nchange in x . this is the square of the standardized pearson residual, r .i\nall their values show that deleting the second observation has the greatest\neffect. this is not surprising, as that observation has the only relatively large\nresidual. by contrast, table 6.8 also contains the changes in x 2 and g2 for\ndeleting observations in fitting the independence model. at the low and high\nends of the blood pressure values, several changes are very large. however,\nthese all relate to removing an entire binomial sample at a blood pressure\nlevel instead of removing a single subject\u2019s binary observation. such subject-\nlevel deletions have little effect even for this model.\n\nwith continuous or multiple predictors, it can be informative to plot these\ndiagnostics, for instance against the estimated probabilities. see cook and\n\n "}, {"Page_number": 241, "text": "226\n\nbuilding and applying logistic regression models\n\ntable 6.8 diagnostic measures for logistic regression models fitted\nto heart disease data\n\nlikelihood-ratio\n\na\n\n2\n\n2\n\nc\n\ng diff.\n\npearson likelihood-ratio\nx diff.\n\nblood\npressure dfbeta\n111.5\n121.5\n131.5\n141.5\n151.5\n161.5\n176.5\n191.5\naindependence model; other values refer to model with blood pressure predictor.\n.\nsource: data from cornfield 1962 .\n\n0.49 0.34\ny1.14 2.26\n0.33 0.31\n0.08 0.09\n0.01 0.00\ny0.07 0.02\n0.40 0.26\ny0.12 0.02\n\npearson\n2\nx diff.\n6.86\n0.02\n4.08\n0.55\n0.70\n0.87\n14.17\n9.41\n\n1.22\n5.64\n0.89\n0.33\n0.02\n0.11\n0.42\n0.03\n\n1.39\n5.04\n0.94\n0.34\n0.02\n0.11\n0.42\n0.03\n\n\u017e\n\na\n\n2\n\ng diff.\n9.13\n0.02\n4.56\n0.57\n0.66\n0.80\n10.83\n6.73\n\nweisberg 1999, chap. 22 , fowlkes 1987 , and landwehr et al. 1984 for\nexamples of useful diagnostic plots.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n4\n\n\u0004\n\n.\n\n6.2.5 summarizing predictive power: r and r-squared measures\nin ordinary regression, r2 describes the proportional reduction in variation\nin comparing the conditional variation of the response to the marginal\nvariation. it and the multiple correlation r describe the power of the\nexplanatory variables to predict the response, with r s 1 for perfect predic-\ntion. despite various attempts to define analogs for categorical response\nmodels, no proposed measure is as widely useful as r and r2. we present a\nfew proposed measures in this section.\n\u02c6\n\n\u0004\n4\nfor any glm, the correlation r y, \u242e between the observed responses y\ni\nand the model\u2019s fitted values \u242e measures predictive power. for least\nsquares regression,\nthis is the multiple correlation between y and the\npredictors. an advantage of the correlation relative to its square is the appeal\nof working on the original scale and its approximate proportionality to effect\nsize: for a small effect with a single predictor, doubling the slope corre-\nsponds roughly to doubling the correlation. this measure can be useful for\ncomparing fits of different models to the same data set.\n\n\u017e\n\u02c6i\n\nin logistic regression, \u242e for a particular model is the estimated probability\n\u02c6\n\u2432 for binary observation i. table 6.2 shows r y, \u242e for a few models fitted to\nthe horseshoe crab data. width alone has r s 0.402, and adding color to the\nmodel increases r to 0.452. the simpler model that uses color merely to\nindicate whether a crab is dark does essentially as well, with r s 0.447. the\ncomplex model containing color, spine condition, width, and all their two-\nand three-way interactions has r s 0.526. this seems considerably higher,\nbut with multiple predictors the r estimates become more highly biased in\nestimating the true correlation. it can be misleading to compare r values for\nmodels with greatly different df values. after a jackknife adjustment designed\n\n\u02c6i\n\n\u02c6\n\n\u017e\n\n.\n\ni\n\n "}, {"Page_number": 242, "text": "logistic regression diagnostics\n\n227\n\nto reduce bias, there is little difference between r for this overly complex\nmodel and the simpler model zheng and agresti 2000 . little is lost and\nmuch is gained by using the simpler model.\n\nanother way to measure the association between the binary responses\n\n4\nyi\nand their fitted values \u2432 uses the proportional reduction in squared error\n\n\u017e\n\n.\n\n\u0004\n\n\u0004\n\n4\n\n\u02c6i\n\n1 y\n\ni\n\n\u00fd y y \u2432\n\u017e\n.\u02c6\ni\n\u00fd y y y\n.\n\u017e\n\ni\n\n2\n,2\n\ni\n\ni\n\ni\n\ni\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u02c6 i\n\nobtained by using \u2432 instead of y s \u00fd y rn as a predictor of y efron 1978 .\n.\namemiya 1981 suggested a related measure that weights squared deviations\nby inverse predicted variances. for logistic regression, unlike normal glms,\nthese and r y, \u242e need not be nondecreasing as the model gets more\ncomplex. like any correlation-type measure, they can depend strongly on the\nrange of observed values of explanatory variables.\n\n\u02c6\n\nother measures directly use the likelihood function. denote the maxi-\nmized log likelihood by l for a given model, l for the saturated model,\nand l for the null model containing only an intercept term. probabilities\nare no greater than 1.0, so log likelihoods are nonpositive. as the model\ncomplexity increases, the parameter space expands, so the maximized log\nlikelihood increases. thus, l f l f l f 0. the measure\n\nm\n\ns\n\n0\n\n0\n\nm\n\ns\n\nm\n\nl y l\nl y l\n\ns\n\n0\n\n0\n\n6.3\u017e\n\n.\n\nfalls between 0 and 1. it equals 0 when the model provides no improvement\nin fit over the null model, and it equals 1 when the model fits as well as the\nsaturated model. a weakness is the log likelihood is not an easily inter-\npretable scale. interpreting the numerical value is difficult, other than in a\ncomparative sense for different models.\n\nfor n independent bernoulli observations, the maximized log likelihood is\n\nlog\n\nn\n\n\u0142\nis1\n\n\u2432 1 y \u2432\n\u02c6\n\u02c6\n\n\u017e\n\ny i\n\ni\n\ni\n\n1yy i\n\n.\n\ns\n\nn\n\n\u00fd\nis1\n\ny log \u2432 q 1 y y\n\n\u017e\n\n\u02c6\n\ni\n\ni\n\n.\n\ni\n\nlog 1 y \u2432 .\n.\n\n\u017e\n\n\u02c6\n\ni\n\nthe null model gives \u2432 s \u00fd y rn s y, so that\n\n\u017e\n\n.\n\n\u02c6i\n\ni\n\nl s n y log y q 1 y y log 1 y y\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n0\n\n.\n\n.\n\nthe saturated model has a parameter for each subject and implies that\n\n "}, {"Page_number": 243, "text": "228\n\u2432 s y for all i. thus, l s 0 and 6.3 simplifies to\n\u02c6i\n\n\u017e\n\n.\n\ns\n\ni\n\nbuilding and applying logistic regression models\n\nd s\n\n0\n\nl y l\nl0\n\nm\n\n.\n\nmcfadden 1974 proposed this measure.\n\n\u017e\n\n.\n\nwith multiple observations at each setting of explanatory variables, the\ndata file can take the grouped-data form of n binomial counts rather than n\nbernoulli indicators. the saturated model then has a parameter for each\ncount. it gives n fitted proportions equal to the n sample proportions of\nsuccess. then l is nonzero and 6.3 takes a different value than when\ncalculated using individual subjects. for n binomial counts, the maximized\nlikelihoods are related to the g goodness-of-fit statistic by g m s\ny2 l y l , so 6.3 becomes\n\n2\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\ns\n\n2\n\nm\n\ns\n\nd* s\n\ng2 0 y g2 m\n\n\u017e .\n\n\u017e\n\n2g 0\u017e .\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ngoodman 1971a and theil 1970 discussed this and related partial associa-\ntion measures.\n\nwith grouped data d* can be large even when predictive power is weak at\nthe subject level. for instance, a model can fit much better than the null\nmodel even though fitted probabilities are close to 0.5 for the entire sample.\nin particular, d* s 1 when it fits perfectly, regardless of how well one can\npredict individual subject\u2019s responses on y with that model. also, suppose\nthat the population satisfies the given model, but not the null model. as the\nsample size n increases with number of settings n fixed, g m behaves like\na chi-squared random variable but g 0 grows unboundedly. thus, d* \u2122 1\nas n \u2122 \u2b01, and its magnitude tends to depend on n. this measure confounds\nmodel goodness of fit with predictive power. similar behavior occurs for r2\nin regression analyses when calculated using means of y values rather than\nindividual subjects at n different x settings. it is more sensible to use d for\nbinary, ungrouped data.\n\n2\u017e .\n\n2\u017e\n\n.\n\n.\n\n\u017e\n\n6.2.6 summarizing predictive power: classification tables\nand roc curves\na classification table cross-classifies the binary response with a prediction of\nwhether y s 0 or 1. the prediction is y s 1 when \u2432 ) \u2432 and y s 0 when\n\u2432 f \u2432 , for some cutoff \u2432 . most classification tables use \u2432 s 0.5 and\n\u02c6i\n0\nsummarize predictive power by\n\n\u02c6\n0\n\n\u02c6\n\n\u02c6\n\n0\n\n0\n\ni\n\nsensitivity s p y s 1 y s 1\n\n\u017e\n\n<\n\n\u02c6\n\n.\n\nand specificity s p y s 0 y s 0\n\n\u017e\n\n<\n\n\u02c6\n\n.\n\n "}, {"Page_number": 244, "text": "logistic regression diagnostics\n\n229\n\nfigure 6.3 roc curve for logistic regression model with horseshoe crab data.\n\n0\n\n0\n\n0\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\u02c6\n\n\u017e\nrecall sections 2.1.2. limitations of this table are that it collapses continu-\nous predictive values \u2432 into binary ones, the choice of \u2432 is arbitrary, and it\nis highly sensitive to the relative numbers of times y s 1 and y s 0.\na recei\u00aeer operating characteristic roc curve is a plot of sensitivity as a\nfunction of 1 y specificity for the possible cutoffs \u2432 . this curve usually\nhas a concave shape connecting the points 0, 0 and 1, 1 . the higher the\narea under the curve, the better the predictions. the roc curve is more\ninformative than the classification table, since it summarizes predictive power\nfor all possible \u2432 . figure 6.3 shows how proc logistic in sas reports\nthe roc curve for the model for the horseshoe crabs using width and color\nas predictors.\n\nthe area under a roc curve is identical to the value of another measure\nof predictive power, the concordance index. consider all pairs of observations\nsuch that y s 1 and y s 0. the concordance index c estimates the\n\u017e\ni, j\nprobability that the predictions and the outcomes are concordant, the obser-\nvation with the larger y also having the larger \u2432 harrell et al. 1982 . a value\nc s 0.5 means predictions were no better than random guessing. this corre-\nsponds to a model having only an intercept term and an roc curve that is a\nstraight line connecting points 0, 0 and 1, 1 . for the horseshoe crab data,\nc s 0.639 with color alone as a predictor, 0.742 with width alone, 0.771 with\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\ni\n\nj\n\n "}, {"Page_number": 245, "text": "230\n\nbuilding and applying logistic regression models\n\nwidth and color, and 0.772 with width and a dummy for whether a crab has\ndark color.\n\nroc curves are a popular way of evaluating diagnostic tests. sometimes\nsuch tests have j ) 2 ordered response categories rather than positive, nega-\ntive . the roc curve then refers to the various possible cutoffs for defining a\nresult to be positive. it plots sensitivity against 1 y specificity for the possible\ncollapsings of the j categories to a positive, negative scale see toledano\n.x\nand gatsonis 1996 .\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nw\n\ninference about conditional associations\n\n6.3\nin 2 = 2 = k tables\n\nthe analysis of the graduate admissions data in sections 6.2.3 used the model\nof conditional independence. this model is an important one in biomedical\nstudies that investigate whether an association exists between a treatment\nvariable and a disease outcome after controlling for a possibly confounding\nvariable that might influence that association. in this section we review the\ntest of conditional independence as a logit model analysis for a 2 = 2 = k\ncontingency table. we also present a test mantel and haenszel 1959 that\nseems non-model-based but relates to the logit model.\n\nwe illustrate using table 6.9, showing results of a clinical trial with eight\ncenters. the study compared two cream preparations, an active drug and a\n\n\u017e\n\n.\n\ntable 6.9 clinical trial relating treatment to response for eight centers\n\ncenter treatment\n\nsuccess\n\nfailure\n\nodds ratio\n\nresponse\n\n4\n\n3\n\n2\n\n1\n\ndrug\ncontrol\ndrug\ncontrol\ndrug\ncontrol\ndrug\ncontrol\ndrug\ncontrol\ndrug\ncontrol\ndrug\ncontrol\ndrug\ncontrol\n\n11\n10\n16\n22\n14\n7\n2\n1\n6\n0\n1\n0\n1\n1\n4\n6\n.\nsource: beitler and landis 1985 .\n\n6\n\n7\n\n8\n\n5\n\n\u017e\n\n25\n27\n4\n10\n5\n12\n14\n16\n11\n12\n10\n10\n4\n8\n2\n1\n\n1.19\n\n1.82\n\n4.80\n\n2.29\n\n\u2b01\n\n\u2b01\n\n2.0\n\n0.33\n\n\u242e\n\n11k\n10.36\n\n14.62\n\n10.50\n\n1.45\n\n3.52\n\n0.52\n\n0.71\n\n4.62\n\n.\n\n11 k\n\n\u017e\n\nvar n\n3.79\n\n2.47\n\n2.41\n\n0.70\n\n1.20\n\n0.25\n\n0.42\n\n0.62\n\n "}, {"Page_number": 246, "text": "inference about conditional associations in 2 = 2 = k tables\n\n231\n\ncontrol, on their success in curing an infection. this table illustrates a\ncommon pharmaceutical application, comparing two treatments on a binary\nresponse with observations from several strata. the strata are often medical\ncenters or clinics; or they may be levels of age or severity of the condition\nbeing treated or combinations of levels of several control variables; or they\nmay be different studies of the same sort evaluated in a meta analysis.\n\n6.3.1 using logit models to test conditional independence\nfor a binary response y, we study the effect of a binary predictor x,\ncontrolling for a qualitative covariate z. let \u2432 s p y s 1 x s i, z s k .\n.\nconsider the model\n\nik\n\n\u017e\n\n<\n\nlogit \u2432 s \u2423q \u2424x q \u2424z,\n\n.\n\n\u017e\n\nik\n\nk\n\ni\n\ni s 1, 2,\n\nk s 1, . . . , k ,\n\n\u017e\n\n6.4\n\n.\n\n2\n\n1\n\nwhere x s 1 and x s 0. this model assumes that the xy conditional odds\nratio is the same at each category of z, namely exp \u2424 . the null hypothesis\nof xy conditional independence is h : \u2424s 0. the wald statistic is \u2424rse .\n.\n2\nthe likelihood-ratio statistic is the difference between g2 statistics for the\nreduced model\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n0\n\nlogit \u2432 s \u2423q \u2424z\n\n\u017e\n\n.\n\nik\n\nk\n\n\u017e\n\n6.5\n\n.\n\n.\n\n\u017e\n\nand the full model. these tests are sensible when x has a similar effect at\neach category of z. they have df s 1.\n\nalternatively, since the reduced model 6.5 is equivalent to conditional\nindependence of x and y, one could test conditional independence using a\ngoodness-of-fit test of that model. that test has df s k when x is binary.\nthis corresponds to comparing model 6.5 and the saturated model, which\npermits \u2424/ 0 and contains xz interaction parameters. when no interaction\nexists or when interaction exists but it has minor substantive importance, it\nfollows from results to be presented in section 6.4.2 that this approach is less\npowerful, especially when k is large. however, when the direction of the xy\nassociation varies among categories of z, it can be more powerful.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n6.3.2 cochran\u2013mantel\u2013haenszel test of conditional independence\nmantel and haenszel 1959 proposed a non-model-based test of h : condi-\ntional independence in 2 = 2 = k tables. focusing on retrospective studies\nof disease, they treated response column marginal totals as fixed. thus, in\n, their analysis conditions on both the\neach partial table k of cell counts n\n.\npredictor totals n\n.\nq1 k q2 k\n.\nthe usual sampling schemes then yield a hypergeometric distribution 3.16\nfor the first cell count n\nin each partial table. that count determines\n\u0004\nn\n\nand the response outcome totals n\n\n, given the marginal totals.\n\n, n\n\u017e\n\n1qk\n\n2qk\n\n, n\n\n, n\n\n,n\n\n\u017e\n\u0004\n\n11 k\n\ni jk\n\n.\n\n\u017e\n\n\u017e\n\n4\n\n4\n\n4\n\n0\n\n12 k\n\n21 k\n\n22 k\n\n "}, {"Page_number": 247, "text": "232\n\nbuilding and applying logistic regression models\n\nunder h , the hypergeometric mean and variance of n\n\n0\n\nare\n\n11 k\n\n\u242e s e n s n\nn\n\n11 k\nn\n\n11 k\n.\n\nvar n s n\n\n\u017e\n\n.\n\n\u017e\n\nn\n\n1qk q1 k qqk\n\u017e\n\nrn\nrn2\n\nn\n\n2qk q1 k q2 k qqk qqk\n\n1qk\n\n11 k\n\nn y 1 .\n.\n\ncell counts from different partial tables are independent. the test statistic\ncombines information from the k tables by comparing \u00fd n\nto its null\nexpected value. it equals\n\n11 k\n\nk\n\ncmh s\n\n\u00fd n y \u242e\n\u017e\nk\n\u00fd var n\u017e\n\n11 k\n\n11 k\n.\n\n11 k\n\nk\n\n2\n\n.\n\n.\n\n6.6\u017e\n\n.\n\nk\n\n\u017e\n\n.\n\n11 k\n\nx y\u017e k.\n\nx y\u017e k.\n\nthis statistic has a large-sample chi-squared null distribution with df s 1.\n\n) 1 in every partial table or \u242a\n\n) 1 in partial table k, we expect that n y\nwhen the odds ratio \u242a\n.\u242e ) 0. when \u242a\n- 1 in each table,\n11 k\n\u00fd n y \u242e tends to be relatively large in absolute value. this test works\n\u017e\nbest when the xy association is similar in each partial table. in this sense it\nis similar to the tests of h : \u2424s 0 in logit model 6.4 . when the sample\nsizes in the strata are moderately large, this test usually gives similar results.\nin fact, it is a score test section 1.3.3 of h : \u2424s 0 in that model day and\n.\nbyar 1979 .\n\nx y \u017e k.\n\n11 k\n\n11 k\n\ncochran 1954 proposed a similar statistic. he treated the rows in each\n2 = 2 table as two independent binomials rather than a hypergeometric.\ncochran\u2019s statistic is 6.6 with var n\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n0\n\n0\n\nvar n s n\n\n\u017e\n\n.\n\n11 k\n\n1qk\n\nn\n\n11 k\n\nreplaced by\nrn3\n\nn\n\nn\n\n2qk q1 k q2 k qqk\n\n.\n\n\u017e\n\n.\n\n.\n\n6.6\n\nthe similarity\n\u017e\n\nin their approaches, we call\n\nbecause of\nthe\ncochran\u1390mantel\u1390haenszel cmh statistic. the mantel and haenszel ap-\nproach using the hypergeometric is more general in that it also applies to\nsome cases in which the rows are not independent binomial samples from\ntwo populations. examples are retrospective studies and randomized clinical\ntrials with the available subjects randomly allocated to two treatments. in the\nfirst case the column totals are naturally fixed. in the second, under the null\nhypothesis the column margins are the same regardless of how subjects were\nassigned to treatments, and randomization arguments lead to the hypergeo-\nmetric in each 2 = 2 table.\n\u017e\n\nmantel and haenszel 1959 proposed 6.6 with a continuity correction.\nthe p-value from the test then better approximates an exact conditional test\n\u017e\nsection 6.7.5 but it tends to be conservative. the cmh statistic generalizes\n.\nfor i = j = k tables section 7.5.3 .\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n6.3.3 multicenter clinical trial example\nfor the multicenter clinical trial, table 6.9 reports the sample odds ratio for\neach table and the expected value and variance of the number of successes\n\n "}, {"Page_number": 248, "text": "inference about conditional associations in 2 = 2 = k tables\n\n233\n\n\u017e\n\n.\n\n0\n\n11 k\n\nfor the drug treatment n\nunder h : conditional independence. in each\ntable except the last, the sample odds ratio shows a positive association.\nit makes sense to combine results with cmh s 6.38, with df s 1.\nthus,\nthere is considerable evidence against h p s 0.012 .\n.\nsimilar results occur in testing h : \u2424s 0 in logit model 6.4 . the model\n.\nfit has \u2424s 0.777 with se s 0.307. the wald statistic is 0.777r0.307 s 6.42\np s 0.011 . the likelihood-ratio statistic equals 6.67 p s 0.010 .\n.\n\u017e\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n0\n\n0\n\n2\n\n0\n\n0\n\n\u017e\n\n.\n\n\u017e\n\n6.3.4 cmh test and sparse data*\nin summary, for logit model 6.4 , cmh is the score statistic alternative to\nthe likelihood-ratio or wald test of h : \u2424s 0. as n \u2122 \u2b01 with fixed k, the\ntests have the same asymptotic chi-squared behavior under h . an advantage\nof cmh is that its chi-squared limit also applies with an alternative asymp-\ntotic scheme in which k \u2122 \u2b01 as n \u2122 \u2b01. the asymptotic theory for likeli-\n.\nhood-ratio and wald tests requires the number of parameters and hence k\nto be fixed, so it does not apply to this scheme. an application of this type is\nwhen each stratum has a single matched pair of subjects, one in each group.\nwith strata of matched pairs, n s n s 1 for each k. then n s 2 k,\nso k \u2122 \u2b01 as n \u2122 \u2b01. table 6.10 shows the data layout for this situation.\nwhen both subjects in stratum k make the same response as in the first case\nin table 6.10 , n s 0 or n s 0. given the marginal counts,\nthe\ninternal counts are then completely determined, and \u242e s n\nand\nvar n s 0. when the subjects make differing responses as in the second\ncase , n s n s 1, so that \u242e s 0.5 and var n s 0.25. thus, a\nmatched pair contributes to the cmh statistic only when the two subjects\u2019\nresponses differ. let k * denote the number of the k tables that satisfy this.\nalthough each n\nlimit theorem\nimplies that \u00fd n\nis approximately normal for large k *. thus, the distribu-\n11 k\ntion of cmh is approximately chi-squared.\n\ncan take only two values, the central\n\n.\nq1 k\n\nq2 k\n\nq2 k\n\nq1 k\n\n1qk\n\n2qk\n\n11 k\n\n11 k\n\n11 k\n\n11 k\n\n11 k\n\n11 k\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\nk\n\nusually, when k grows with n, each stratum has few observations. there\nmay be more than two observations, such as case\u1390control studies that match\nseveral controls with each case. contingency tables with relatively few obser-\nvations are referred to as sparse. the nonstandard setting in which k \u2122 \u2b01 as\nn \u2122 \u2b01 is called sparse-data asymptotics. ordinary ml estimation then breaks\ndown because the number of parameters is not fixed, instead having the same\norder as the sample size. in particular, an approximate chi-squared distribu-\ntion holds for the likelihood-ratio and wald statistics for testing conditional\n\ntable 6.10 stratum containing a matched pair\n\nelement\nof pair\nfirst\nsecond\n\nresponse\n\nresponse\n\nsuccess\n\nfailure\n\nsuccess\n\nfailure\n\n1\n1\n\n0\n0\n\n1\n0\n\n0\n1\n\n "}, {"Page_number": 249, "text": "234\n\nbuilding and applying logistic regression models\n\nindependence only when the strata marginal totals generally exceed about 5\nto 10 and k is fixed and small relative to n.\n\n6.3.5 estimation of common odds ratio\nit is more informative to estimate the strength of association than to test\nhypotheses about it. when the association seems stable among partial tables,\nit is helpful to combine the k sample odds ratios into a summary measure of\nconditional association. the logit model 6.4 implies homogeneous associa-\ntion, \u242a s \u2b48\u2b48\u2b48 s \u242a\ns exp \u2424 . the ml estimate of the common\n\u02c6\u017e\n.\nodds ratio is exp \u2424 .\n\nx y\u017e k .\n\nx y\u017e1.\n\nother estimators of a common odds ratio are not model-based. woolf\n\u017e\n1955 proposed an exponentiated weighted average of the k sample log\nodds ratios. mantel and haenszel 1959 proposed that\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u02c6\u242a s\nmh\n\nk\n\n\u017e\n\u00fd n\n\u017e\n\u00fd n\n\nk\n\nn rn\n22 k qqk\nn rn\n21 k qqk\n\n.\n.\n\n11 k\n\n12 k\n\ns\n\nk\n\n\u00fd p\n\u00fd p\n\nk\n\np\n22 < k qqk\np\n21 < k qqk\n\nn\nn\n\n11 < k\n\n12 < k\n\n,\n\n6.7\u017e\n\n.\n\nx\n\n\u02c6\n\ni j < k\n\nw\n.\n\np\u02c6\n\n\u02c6\u017e <\n\ni jk qqk\n\nwhere p s n rn\n. this gives more weight to strata with larger sample\nsizes. it is preferred over the ml estimator when k is large and the data are\nsparse. the ml estimator \u2424 of the log odds ratio then tends to be too large\nin absolute value. for sparse-data asymptotics with only a single matched\npair in each stratum, for instance, \u2424\u2122 2 \u2424. this con\u00aeergence in probability\nmeans that for any \u2440) 0, p \u2424y 2 \u2424 - \u2440 \u2122 1 as n \u2122 \u2b01; see problem\n10.24.\n\n\u02c6\nhauck 1979 gave an asymptotic variance for log \u242a\nmh\n\nthat applies for a\n\u02c6\u017e\nfixed number of strata. in that case log \u242a\nis slightly less efficient than the\nml estimator \u2424 unless \u2424s 0 tarone et al. 1983 . robins et al. 1986\n.\nderived an estimated variance that applies both for these standard asymp-\ntotics with large n and fixed k and for sparse asymptotics in which k is also\nlarge. expressing \u242a s rrs s \u00fd r r \u00fd s with r s n\nn rn\n,\n22 k qqk\nlog \u242a y log \u242a is approximately proportional\n\u017e\ntheir derivation showed that\nto r y \u242as . they also showed that e r y \u242as s 0 and derived the vari-\nance of r y \u242as . their result is\n\n\u017e\n\u02c6\nmh\n\n\u02c6\nmh\n\nmh\n\n11 k\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\nk\n\nk\n\nk\n\nk\n\nk\n\n<\n\n\u2434 log \u242a s\n\u02c6\n\n\u02c6\nmh\n\n2\n\ny1\nn\nqqk\n\n\u017e\n\nn q n\n\n11 k\n\n.\n\nr\n\nk\n\n22 k\n\n1\n\u00fd\n22 r k\n1\n22s\n\nq\n\nk\n\n\u00fd qqk\ny1\nn\n\n\u017e\n\nn q n\n\n12 k\n\n.\n\ns\n\nk\n\n21 k\n\nq\n\n1\n2 rs k\n\n\u00fd qqk\ny1\nn\n\n\u017e\n\nn q n\n\n11 k\n\n.\n\n22 k\n\ns q n q n\n\n\u017e\n\n12 k\n\nk\n\n.\n\nr .\n\nk\n\n21 k\n\n "}, {"Page_number": 250, "text": "inference about conditional associations in 2 = 2 = k tables\n\n235\n\nfor the eight-center clinical trial summarized by table 6.9,\n\n\u02c6\u242a s\nmh\n\n\u017e\n\u017e\n\n11 = 27 r73 q \u2b48\u2b48\u2b48 q 4 = 1 r13\n25 = 10 r73 q \u2b48\u2b48\u2b48 q 2 = 6 r13\n\n.\n.\n\n\u017e\n\u017e\n\n.\n.\n\ns 2.13.\n\nx\n\nw\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n\u02c6\n\n\u017e\n.\n\n\u02c6\nmh\n\n\u02c6\nmh\u017e\n.\n\u017e\n\nfor log \u242a s 0.758, \u2434 log \u242a s 0.303. a 95% confidence interval for the\ncommon odds ratio is exp 0.758 \" 1.96 = 0.303 or 1.18, 3.87 . similar\nresults occur using model 6.4 . the 95% confidence interval for exp \u2424 is\nexp 0.777 \" 1.96 = 0.307 , or 1.19, 3.97 , using the wald interval, and 1.20,\n4.02 using the likelihood-ratio interval. although the evidence of an effect is\nconsiderable, inference about its size is rather imprecise. the odds of success\nmay be as little as 20% higher with the drug, or they may be as much as four\ntimes as high.\n\n\u02c6\nif the true odds ratios are not identical but do not vary drastically, \u242a\nm h\nstill is a useful summary of the conditional associations. similarly, the cmh\ntest is a powerful summary of evidence against h : conditional indepen-\ndence, as long as the sample associations fall primarily in a single direction. it\nis not necessary to assume equality of odds ratios to use the cmh test.\n\n\u017e\n\n.\n\n\u017e\n\n0\n\nx\n\n.\n\n\u017e\n\nx y\u017e1.\n\nx y \u017e k .\n\n6.3.6 testing homogeneity of odds ratios\nthe homogeneous association condition \u242a s \u2b48\u2b48\u2b48 s \u242a\nfor 2 = 2 = k\ntables is equivalent to logit model 6.4 . a test of homogeneous association is\nimplicitly a goodness-of-fit test of this model. the usual g2 and x 2 test\nstatistics provide this, with df s k y 1. they test that the k y 1 parameters\nin the saturated model that are the coefficients of interaction terms cross\nproducts of the dummy variable for x with k y 1 dummy variables for\ncategories of z all equal 0. breslow and day 1980, p. 142 proposed an\n.\nalternative large-sample test note 6.5 .\nfor the eight-center clinical trial data in table 6.9, g2 s 9.7 and x 2 s 8.0\ndf s 7 do not contradict the hypothesis of equal odds ratios. it is reason-\n\u017e\nable to summarize the conditional association by a single odds ratio e.g.,\n\u242a s 2.1 for all eight partial tables. in fact, even with a small p-value in a\n\u02c6\nmh\ntest of homogeneous association, if the variability in the sample odds ratios is\n\u02c6\nnot substantial, a summary measure such as \u242a\nis useful. a test of\nmh\nhomogeneity is not a prerequisite for this measure or for testing conditional\nindependence.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nw\n\n6.3.7 summarizing heterogeneity in odds ratios\nin practice, a predictor effect is often similar from stratum to stratum. in\nmulticenter clinical trials comparing a new drug to a standard, for example, if\nthe new drug is truly more beneficial, the true effect is usually positive in\neach stratum.\n\n "}, {"Page_number": 251, "text": "236\n\nbuilding and applying logistic regression models\n\n\u0004\n\n\u017e\n\nin strict terms, however, a model with homogeneous effects is unrealistic.\nfirst, we rarely expect the true odds ratio to be exactly the same in each\n.\nstratum, because of unmeasured covariates that affect it. breslow 1976\ndiscussed modeling of the log odds ratio using a set of explanatory variables.\nsecond, the model regards the strata effects \u2424 as fixed effects, treating\nthem as the only strata of interest. often the strata are merely a sampling of\nthe possible ones. multicenter clinical trials have data for certain centers but\nmany other centers could have been used. scientists would like their conclu-\nsions to apply to all such centers, not only those in the study.\n\nz4\nk\n\na somewhat different logit model treats the true log odds ratios in partial\ntables as a random sample from a n \u242e, \u2434 distribution. fitting the model\nyields an estimated mean log odds ratio and an estimated variability about\nthat mean. the inference applies to the population of strata rather than only\nthose sampled. this type of model uses random effects in the linear predictor\nto induce this extra type of variability. in chapter 12 we discuss glms with\nrandom effects, and in section 12.3.4 we fit such a model to table 6.9.\n\n2.\n\n\u017e\n\n6.4 using models to improve inferential power\n\nwhen contingency tables have ordered categories, in section 3.4 we showed\nthat tests that utilize the ordering can have improved power. testing inde-\npendence against a linear trend alternative in a linear logit model sections\n5.3.4, and 5.4.6 is a way to do this. in this section we present the reason for\nthese power improvements.\n\n.\n\n\u017e\n\n6.4.1 directed alternatives\nconsider an i = 2 contingency table for i binomial variates with parameters\n\u0004\n4\u2432 . h : independence states\n\ni\n\n0\n\nlogit \u2432 s \u2423.\n\n\u017e\n\n.i\n\nthe ordinary x 2 and g2 statistics of section 3.2.1 refer to the general\nalternative,\n\nlogit \u2432 s \u2423q \u2424,\n\n\u017e\n\n.i\n\ni\n\n1\n\n0\n\n.\n\n\u017e\n\nwhich is saturated. they test h : \u2424 s \u2424 s \u2b48\u2b48\u2b48 s \u2424 s 0 in that model,\nwith df s i y 1 . their general alternative treats both classifications as\n2\u017e .\nnominal. denote these test statistics as g i and x i . recall that g i\nis the likelihood-ratio statistic g m m s y2 l y l for comparing\nthe saturated model m with the independence i model m .\n0\nordinal test statistics refer to narrower, usually more relevant, alterna-\ntives. with ordered rows, an example is a test of h : \u2424s 0 in the linear logit\n\n\u017e\n\u017e .\n\n2\u017e .\n\n2\u017e .\n\n2\u017e\n\n.\n\n.\n\n0\n\n1\n\n0\n\n1\n\n1\n\n2\n\ni\n\n<\n\n0\n\n "}, {"Page_number": 252, "text": "i\n\ni\n\n.\n\n.\n\n\u017e\n\n2\u017e\n\n237\nusing models to improve inferential power\nmodel, logit \u2432 s \u2423q \u2424x . the likelihood-ratio statistic g i l s g i\n2\u017e .\ny g l compares the linear logit model and the independence model.\nwhen a test statistic focuses on a single parameter, such as \u2424 in that model,\nit has df s 1. now, df equals the mean of the chi-squared distribution.\na large test statistic with df s 1 falls farther out in its right-hand tail than a\n2\u017e .\ncomparable value of x i\nit has\na smaller p-value.\n\nor g i with df s i y 1 . thus,\n\n2\u017e .\n\n2\u017e\n\n.\n\n\u017e\n\n.\n\n<\n\n6.4.2 noncentral chi-squared distribution\n\n2\u017e .\n\n2\u017e\n\n<\n\n.\n\n0\n\nto compare power of g i l and g i , it is necessary to compare their\nnonnull sampling distributions. when h is false, their distributions are\napproximately noncentral chi-squared. this distribution, introduced by r. a.\nfisher in 1928, arises from the following construction: if z ; n \u242e, 1 , i s\n1, . . . , \u242f, and if z , . . . , z are independent, \u00fdz 2 has the noncentral chi-\nsquared distribution with df s \u242f and noncentrality parameter \u242ds \u00fd \u242e2. its\nmean is \u242fq \u242d and its variance is 2 \u242fq 2 \u242d . the ordinary central\nchi-\nsquared distribution, which occurs when h is true, has \u242ds 0.\nlet x 2 denote a noncentral chi-squared random variable with df s \u242f\nand noncentrality \u242d. a fundamental result for chi-squared analyses is that,\nfor fixed \u242d,\n\n\u242f, \u242d\n\ni\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u242f\n\n1\n\n0\n\ni\n\ni\n\ni\n\np x ) \u2439 \u2423 increases as \u242f decreases.\n\n2\n\u242f\n\n2\n\u242f, \u242d\n\n\u017e .\n\nthat is, the power for rejecting h at a fixed \u2423-level increases as the df of\nthe test decreases e.g., das gupta and perlman 1974 . for fixed \u242f, the\npower equals \u2423 when \u242ds 0, and it increases as \u242d increases. the inverse\nrelation between power and df suggests that focusing the noncentrality on a\nstatistic having a small df value can improve power.\n\n\u017e\n\n.\n\n0\n\nincreased power for narrower alternatives\n\n6.4.3\n.x\nsuppose that x has, at least approximately, a linear effect on logit p y s 1 .\nto test independence, it is then sensible to use a statistic having strong power\nfor that effect. this is the purpose of the tests based on the linear logit\nmodel, using the likelihood-ratio statistic g i l , the wald statistic z s\n<\n\u2424rse, and the cochran\u1390armitage score statistic.\n\u02c6\n\n2\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nw\n\nwhen is g i l more powerful than g i ? the statistics satisfy\n\n2\u017e\n\n<\n\n.\n\n2\u017e .\n\ng i s g i l q g l ,\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n2\n\n2\n\n2\n\n<\n\n2\u017e\n\n.\n\nwhere g l tests goodness of fit of the linear logit model. when the linear\nlogit model holds, g l has an asymptotic chi-squared distribution with\n\n2\u017e\n\n.\n\n "}, {"Page_number": 253, "text": "<\n\n<\n\n<\n\n<\n\n<\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n2\u017e .\n\nbuilding and applying logistic regression models\n\n238\ndf s i y 2; then if \u2424/ 0, g i\n2\u017e .\nand g i l both have approximate\nnoncentral chi-squared distributions with the same noncentrality. whereas\ndf s i y 1 for g i , df s 1 for g i l . thus, g i l is more powerful,\n<\nsince it uses fewer degrees of freedom.\n\n2\u017e .\n\n2\u017e\n\n2\u017e\n\n2\u017e\n\n2\u017e\n\n2\u017e\n\n2\u017e\n\nwhen the linear logit model does not hold, g i has greater noncentral-\nity than g i l , the discrepancy increasing as the model fits more poorly.\nhowever, when the model approximates reality fairly well, usually g i l is\nstill more powerful. that test\u2019s df value of 1 more than compensates for its\nloss in noncentrality. the closer the true relationship is to the linear logit, the\nmore nearly g i l captures the same noncentrality as g i , and the\nmore powerful it is compared to g i . to illustrate, figure 6.4 plots power\nas a function of noncentrality when df s 1 and 7. when the noncentrality of\na test having df s 1 is at least about half that of a test having df s 7, the test\nwith df s 1 is more powerful. the linear logit model then helps detect a key\ncomponent of an association. as mantel 1963 argued in a similar context,\n\u2018\u2018that a linear regression is being tested does not mean that an assumption of\nlinearity is being made. rather it is that test of a linear component of\nregression provides power for detecting any progressive association which\nmay exist.\u2019\u2019\n\n2\u017e .\n\n2\u017e .\n\nthe improved power results from sacrificing power in other cases. the\n2\u017e .\ntest can have greater power than g i l when the linear logit model\ng i\ndescribes reality very poorly.\n\n2\u017e\n\nthe remark about the desirability of focusing noncentrality holds for\nnominal variables also. for instance, consider testing conditional indepen-\ndence in 2 = 2 = k tables. one approach tests \u2424s 0 in model 6.4 , using\ndf s 1. another approach tests goodness of fit of model 6.5 , using df s k\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n<\n\nfigure 6.4 power and noncentrality, for df s 1 and df s 7, when \u2423s 0.05.\n\n "}, {"Page_number": 254, "text": "using models to improve inferential power\n\n239\n\ntable 6.11 change in clinical condition by degree of infiltration\n\nclinical change\nworse\nstationary\nslight improvement\nmoderate improvement\nmarked improvement\n\ndegree of infiltration\nlow\nhigh\n11\n1\n13\n53\n42\n16\n27\n15\n7\n11\n\nproportion\n\nhigh\n0.08\n0.20\n0.28\n0.36\n0.39\n\n.\nsource: reprinted with permission from the biometric society cochran 1954 .\n\n\u017e\n\n.\n\n\u017e\nsection 6.3.1 . when model 6.4 holds, both tests have the same noncentral-\nity. thus, the test of \u2424s 0 is more powerful, since is has fewer degrees of\nfreedom.\n\n\u017e\n\n.\n\n4\n\n\u0004\n\n6.4.4 treatment of leprosy example\ntable 6.11 refers to an experiment on the use of sulfones and streptomycin\ndrugs in the treatment of leprosy. the degree of infiltration at the start of the\nexperiment measures a type of skin damage. the response is the change in\nthe overall clinical condition of the patient after 48 weeks of treatment. we\nuse response scores y1, 0, 1, 2, 3 . the question of interest is whether\nsubjects with high infiltration changed differently from those with low infil-\ntration.\n\nhere, the clinical change response variable is ordinal. it seems natural to\ncompare the mean change for the two infiltration levels. cochran 1954 and\nyates 1948 noted that this analysis is identical to a trend test treating the\nbinary variable as the response. that test is sensitive to linearity between\nclinical change and the proportion of cases with high infiltration.\nthe test g i s 7.28 df s 4 does not show much evidence of associa-\ntion p s 0.12 , but it ignores the row ordering. the sample proportion of\nhigh infiltration increases monotonically as the clinical change improves. the\ntest of h : \u2424s 0 in the linear logit model has g i l s 6.65, with df s 1\np s 0.01 . it gives strong evidence of more positive clinical change at the\n\u017e\nhigher level of infiltration. using the ordering by decreasing df from 4 to 1\npays a strong dividend. in addition, g l s 0.63 with df s 3 suggests that\nthe linear trend model fits well.\n\n2\u017e .\n.\n\n2\u017e\n\n2\u017e\n\n0\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n<\n\n6.4.5 model smoothing improves precision of estimation\nusing directed alternatives can improve not only test power, but also estima-\ntion of cell probabilities and summary measures. in generic form, let \u2432 be\ntrue cell probabilities in a contingency table, let p denote sample proportions,\nand let \u2432 denote model-based ml estimates of \u2432.\n\n\u02c6\n\n "}, {"Page_number": 255, "text": "240\n\nbuilding and applying logistic regression models\n\n\u02c6\n\nwhen \u2432 satisfy a certain model, both \u2432 for that model and p are\nconsistent estimators of \u2432. the model-based estimator \u2432 is better, as its true\nasymptotic standard error cannot exceed that of p. this happens because of\nmodel parsimony: the unsaturated model, on which \u2432 is based, has fewer\nparameters than the saturated model, on which p is based. in fact, model-\nbased estimators are also more efficient in estimating functions g \u2432 of cell\nprobabilities. for any differentiable function g,\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\nasymp. var n g \u2432 f asymp. var n g p .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u02c6\n\n'\n\n'\n\n.\n\n\u017e\n\nin section 14.2.2 we prove this result. it holds more generally than for\ncategorical data models altham 1984 . this is one reason that statisticians\nprefer parsimonious models.\n\nin reality, of course, a chosen model is unlikely to hold exactly. however,\nwhen the model approximates \u2432 well, unless n is extremely large, \u2432 is still\nbetter than p. although \u2432 is biased, it has smaller variance than p , and\nmse \u2432 - mse p when its variance plus squared bias is smaller than\nvar p . in section 3.3.7 we showed that in two-way tables, independence-\nmodel estimates of cell probabilities can be better than sample proportions\neven when that model does not hold.\n\n\u02c6 i\n.\n\n\u02c6i\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\ni\n\ni\n\ni\n\n6.5 sample size and power considerations*\n\nin any statistical procedure, the sample size n influences the results. strong\neffects are likely to be detected even when n is small. by contrast, detection\nof weak effects requires large n. a study design should reflect the sample size\nneeded to provide good power for detecting the effect.\n\n6.5.1 sample size and power for comparing two proportions\nfor test statistics having large-sample normal distributions, power calcula-\ntions can use ordinary methods. to illustrate, consider a test comparing\nbinomial parameters \u2432 and \u2432 for two medical treatments. an experiment\nplans independent samples of size n s nr2 receiving each treatment. the\nresearchers expect \u2432 f 0.6 for each, and a difference of at least 0.10 is\nimportant. in testing h : \u2432 s \u2432 , the variance of the difference \u2432 y \u2432 in\nsample proportions is \u2432 1 y \u2432 r nr2 q \u2432 1 y \u2432 r nr2 f 0.6 = 0.4\n= 4rn s 0.96rn. in particular,\n\n\u017e\n1\n\n\u02c6\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n1\n\n2\n\n0\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\n2\n\ni\n\ni\n\n\u017e\n\nz s\n\n\u2432 y \u2432 y \u2432 y \u2432\n\u02c6\n2\n\n.\n\n1\n\n.\n\n2\n\n\u017e\n\u02c6\n0.96rn\n\u017e\n.\n\n1\n1r2\n\nhas approximately a standard normal distribution for \u2432 and \u2432 near 0.6.\n\n1\n\n2\n\n "}, {"Page_number": 256, "text": "sample size and power considerations\n\n241\n\nthe power of an \u2423-level test of h is approximately\n\n0\n\n1\n\n\u2432 y \u2432\n<\n<\n\u02c6\n\u02c6\n2\n1r2\n0.96rn\n.\n\u017e\n\np\n\ng z\n\n\u2423r2\n\n.\n\nwhen \u2432 y \u2432 s 0.10, for \u2423s 0.05, this equals\n\n1\n\n2\n\n\u2432 y \u2432 y 0.10\n\u02c6\n\n\u02c6\n\n\u017e\n\np\n\n) 1.96 y 0.10 nr0.96\n\n\u017e\n\n1r2\n\n.\n\n1\n\u017e\n\n2\n\n.\n0.96rn\n\u017e\n\nqp\n\n1r2\n\n.\n\u2432 y \u2432 y 0.10\n\u02c6\n\n\u02c6\n\n.\n0.96rn\n\n2\n\n.\n\n1\n\u017e\n\n1r2\n\n- y1.96 y 0.10 nr0.96\n\n\u017e\n\n.\n\n1r2\n\ns p z ) 1.96 y 0.10 nr0.96\n.\ns 1 y \u233d 1.96 y 0.10 nr0.96\n\n\u017e\n\n\u017e\n\n.\n\n1r2\n\n1r2\n\nq p z - y1.96 y 0.10 nr0.96\n1r2\nq \u233d y1.96 y 0.10 nr0.96\n\n\u017e\n\n\u017e\n\n.\n\n1r2\n\n.\n\n,\n\nwhere \u233d is the standard normal cdf. the power is approximately 0.11 when\nn s 50 and 0.30 when n s 200. it is not easy to attain significance when\neffects are small and the sample is not very large. figure 6.5 shows how the\npower increases in n when \u2432 y \u2432 s 0.1. by contrast, it shows how the\npower improves when \u2432 y \u2432 s 0.2.\nfor a given p type i error s \u2423 and p type ii error s \u2424 and hence\npower s 1 y \u2424 , one can determine the sample size needed to attain those\n\n2\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n1\n\n2\n\n1\n\nfigure 6.5 approximate power for testing equality of proportions, with true values near\nmiddle of range and \u2423s 0.05.\n\n "}, {"Page_number": 257, "text": "242\nvalues. a study using n s n requires approximately\n\n1\n\n2\n\nbuilding and applying logistic regression models\n\nn s n s z q z\n\n\u017e\n\n\u2423r2\n\n1\n\n2\n\n\u2424\n\n2\n\n.\n\n\u2432 1 y \u2432 q \u2432 1 y \u2432 r \u2432 y \u2432 .\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n2\n\n1\n\n2\n\n1\n\n1\n\n2\n\n2\n\n2\n\n1\n\n2\n\n1\n\nfor a test with \u2423s 0.05 and \u2424s 0.10 when \u2432 and \u2432 are truly about 0.60\nand 0.70, n s n s 473. this formula also provides the sample sizes needed\nfor a comparable confidence interval for \u2432 y \u2432 . with about 473 subjects in\neach group, a 95% confidence interval has only a 0.10 chance of containing 0\nwhen actually, \u2432 s 0.60 and \u2432 s 0.70.\n\nthis sample-size formula is approximate and may underestimate slightly\nthe actual values required. it is adequate for most practical work, though, in\n.\nwhich only rough conjectures are available for \u2432 and \u2432 . fleiss 1981\nshowed more precise formulas.\n\n\u017e\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\ni\n\nw\n\nw\n\n\u017e\n\n.x\n\n6.5.2 sample size determination in logistic regression\nconsider now the model logit \u2432 x s \u2423q \u2425x , i s 1, . . . , n, in which x is\nquantitative. we use \u2425 so as not to confuse with \u2424s p type ii error . the\nsample size needed to achieve a certain power for testing h : \u2425s 0 depends\n\u017e\non the variance of \u2425. this depends on \u2432 x\n, and formulas for n use a guess\nfor \u2432s \u2432 x and the distribution of x. the effect size is the log odds ratio \u2436\ncomparing \u2432 x to \u2432 x q s\n.\n, the probability for a standard deviation above\nthe mean of x. for a one-sided test when x is approximately normal, hsieh\n\u017e\n1989 derived\n\n.\n\u017e\n\n. x\n\n\u02c6\n\n\u02c6\n\n.4\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u0004\n\n0\n\nx\n\ni\n\ni\n\nn s z q z exp y\u2436 r4\n\n2\n\n\u2423\n\n\u2424\n\n\u017e\n\n.\n\n2\n\n\u017e\n\nwhere\n\n1 q 2\u2432\u2426 r \u2432\u2436 ,\n.\n\n.\n\n\u017e\n\n2\n\n\u02c6\n\n\u02c6\n\n\u2426s 1 q 1 q \u2436 exp 5\u2436 r4 r 1 q exp y\u2436 r4 .\n.\n\n\u017e\nthe value n decreases as \u2432\u2122 0.5 and as \u2436 increases.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2\n\n2\n\n2\n\n<\n\n<\n\n\u02c6\n\nwe illustrate for modeling the effect of x s cholesterol\n\nlevel on the\nprobability of severe heart disease for a population for which that probability\nat an average level of cholesterol is about 0.08. researchers want the test to\nbe sensitive to a 50% increase in this probability, for a standard deviation\nincrease in cholesterol. the odds of severe heart disease at the mean\ncholesterol level equal 0.08r0.92 s 0.087, and the odds one standard devia-\ntion above the mean equal 0.12r0.88 s 0.136. the odds ratio equals\n0.136r0.087 s 1.57, and \u2436s log 1.57 s 0.450. for \u2423s 0.05 and \u2424s 0.10,\n\u2426s 1.306 and n s 612.\n\n\u017e\n\n.\n\n6.5.3 sample size in multiple logistic regression\na multiple logistic regression model requires larger n to detect effects. let r\ndenote the multiple correlation between the predictor x of interest and the\n\n "}, {"Page_number": 258, "text": "\u017e\n\n243\nsample size and power considerations\nothers in the model. the formula for n above divides by 1 y r . in that\nformula, \u2432 is evaluated at the mean of all the explanatory variables, and the\nodds ratio refers to the effect of x at the mean level of the other predictors.\nconsider the example in section 6.5.2 when blood pressure is also a\npredictor. if the correlation between cholesterol and blood pressure is 0.40,\nwe need n f 612r 1 y 0.40 s 729.\n\n.2x\n\n2.\n\n\u02c6\n\nthese formulas provide, at best, rough indications of sample size. most\napplications have only a crude guess for \u2432 and r, and x may be far from\n.\nnormally distributed. for other work on this problem, see hsieh et al. 1998\n.\nand whittemore 1981 .\n\n\u02c6\n\n\u017e\n\n\u017e\n\n\u017e\n\nw\n\n6.5.4 power for chi-squared tests in contingency tables\nwhen hypotheses are false, squared normal and x 2 and g2 statistics have\nlarge-sample noncentral chi-squared distributions section 6.4.2 . suppose\nthat h is equivalent to model m for a contingency table. let \u2432 denote the\ntrue probability in cell i, and let \u2432 m denote the value to which the ml\nestimate \u2432 for model m converges, where \u00fd\u2432 s \u00fd\u2432 m s 1. for a\nmultinomial sample of size n, the noncentrality parameter for x 2 equals\n\n\u02c6i\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n0\n\ni\n\ni\n\ni\n\ni\n\n\u242ds n\n\n\u00fd\n\ni\n\ni\n\n\u2432 y \u2432 m\u017e\n.\n\ni\n\u2432 m\u017e\n\ni\n\n2\n\n.\n\n.\n\n6.8\u017e\n\n.\n\nthis has the same form as x 2, with \u2432 in place of the sample proportion p\ni\nand \u2432 m in place of \u2432. the noncentrality parameter for g equals\n\n\u017e\n\n.\n\n2\n\ni\n\n\u02c6\n\ni\n\ni\n\n\u242ds 2 n \u2432 log\n\n\u00fd i\n\ni\n\n\u2432\ni\n\u2432 m\u017e\n\ni\n\n.\n\n.\n\n6.9\u017e\n\n.\n\ntable 6.12 power of chi-squared test for \u2423 s 0.05\n\nnoncentrality\n\ndf\n\n1\n2\n3\n4\n6\n8\n10\n20\n50\n\n0.0\n\n.050\n.050\n.050\n.050\n.050\n.050\n.050\n.050\n.050\n\n0.2\n\n.073\n.065\n.062\n.060\n.058\n.057\n.056\n.053\n.052\n\n0.4\n\n.097\n.081\n.075\n.071\n.066\n.064\n.062\n.056\n.054\n\n0.6\n\n.121\n.098\n.088\n.082\n.075\n.071\n.068\n.060\n.056\n\n0.8\n\n.146\n.115\n.102\n.093\n.084\n.079\n.075\n.063\n.059\n\n1.0\n\n.170\n.133\n.116\n.106\n.094\n.087\n.082\n.066\n.061\n\n2.0\n\n.293\n.226\n.192\n.172\n.146\n.131\n.121\n.096\n.076\n\n3.0\n\n.410\n.322\n.275\n.244\n.206\n.182\n.166\n.125\n.092\n\n4.0\n\n.516\n.415\n.358\n.320\n.270\n.238\n.215\n.158\n.110\n\n5.0\n\n.609\n.504\n.440\n.396\n.336\n.296\n.268\n.193\n.129\n\n7.0\n\n.754\n.655\n.590\n.540\n.468\n.417\n.379\n.273\n.173\n\n10.0\n\n.885\n.815\n.761\n.716\n.644\n.588\n.542\n.402\n.250\n\n15.0\n\n.972\n.944\n.917\n.891\n.843\n.799\n.760\n.611\n.398\n\n25.0\n\n.998\n.996\n.993\n.989\n.980\n.968\n.956\n.883\n.687\n\nsource: reprinted with permission from g. e. haynam, z. govindarajulu, and f. c. leone, in\nselected tables in mathematical statistics, eds. h. l. harter and d. b. owen chicago: markham,\n.\n1970 .\n\n\u017e\n\n "}, {"Page_number": 259, "text": "i\n\n0\n\n.\n\n\u017e\n\n244\nbuilding and applying logistic regression models\nwhen h is true, all \u2432 s \u2432 m . then, for either statistic, \u242ds 0 and the\ncentral chi-squared distribution applies.\nto determine the approximate power for a chi-squared test with df s \u242f,\n\u017e .\n1 choose a hypothetical set of true values \u2432 , 2 calculate \u2432 m by\nfitting to \u2432 the model m for h , 3 calculate the noncentrality parameter\n\u242d, and 4 calculate p x ) \u2439 \u2423 . table 6.12 shows an excerpt from a\ntable of noncentral chi-squared probabilities for step 4 with \u2423s 0.05.\n\n\u017e .\n2\u017e .x\n\n\u0004\n\u017e .\n\n2\n\u242f, \u242d\n\n\u017e .\n\n.4\n\n\u017e\n\nw\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u242f\n\n0\n\ni\n\ni\n\ni\n\ni\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n6.5.5 power for testing conditional independence\nwe use an example based on one in o\u2019brien 1986 . a standard fetal heart\nrate monitoring test predicts whether a fetus will require nonroutine care\n.\nfollowing delivery. the standard test has categories worrisome, reassuring .\nthe response y is whether the newborn required some nonroutine medical\ncare during the first week after birth 1 s yes, 0 s no . a new fetal heart\nrate monitoring test is developed, having categories very worrisome, some-\nwhat worrisome, reassuring . a physician plans to study whether this new test\ncan help make predictions about the outcome; that is, given the result of the\nstandard test, is there an association between the response and the result of\nthe new test? a relevant statistic tests the effect of the new monitoring test in\nthe logit model having the new test n and standard test s as qualitative\npredictors.\n\nto help select n, a statistician asks the physician to conjecture about the\njoint distribution of the explanatory variables, with questions such as \u2018\u2018what\nproportion of the cases do you think will be scored \u2018reassuring\u2019 by both\ntests?\u2019\u2019 for each ns combination, the physician also guessed p y s 1 .\n.\ntable 6.13 shows one scenario for marginal and conditional probabilities.\nthese yield a joint distribution \u2432 from their product, such as 0.04 = 0.40\ns 0.016 for the proportion of cases judged worrisome by the standard test\nand very worrisome by the new test and requiring nonroutine medical care.\nthese joint probabilities yield fitted probabilities \u2432 m and \u2432 m for the\nnull and alternative logit models. one can get these by entering \u2432 in\n\n.\n\u0004\n\ni jk\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u0004\n\n4\n\n4\n\n0\n\n1\n\ni jk\n\ntable 6.13 scenario for power computation\n\nnew\n\njoint\n\nprobability\n\n\u017e\n\np nonroutine care\n\n.\n\nstandard\nworrisome\n\nreassuring\n\n0.04\n0.08\n0.04\n0.02\n0.18\n0.64\n.\nsource: reprinted with permission from o\u2019brien 1986 .\n\nvery worrisome\nsomewhat worrisome\nreassuring\nvery worrisome\nsomewhat worrisome\nreassuring\n\n\u017e\n\n0.40\n0.32\n0.27\n0.30\n0.22\n0.15\n\n "}, {"Page_number": 260, "text": "probit and complementary log-log models\n\n245\n\n1\n\n.\n\npercentage form as counts in software for logistic regression, fit the relevant\n.\nmodel, and divide the fitted counts by 100 to get the fitted joint probabilities.\n\u017e\nthe likelihood-ratio test comparing these models has noncentrality 6.9 with\n.\n\u017e\n\u2432 m playing the role of \u2432 and \u2432 m playing the role of \u2432 m .\nfor the scenario in table 6.13, the noncentrality equals 0.00816n, with\ndf s 2. for n s 400, 600, and 1000, the approximate powers when \u2423s 0.05\nare 0.35, 0.49, and 0.73. this scenario predicts 64% of the observations to\noccur at only one combination of the factors. the lack of dispersion for the\nfactors weakens the power.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n0\n\n6.5.6 effects of sample size on model selection and inference\nthe effects of sample size suggest some cautions for model selection. for\nsmall n, the most parsimonious model accepted in a goodness-of-fit test may\nbe quite simple. by contrast, larger samples usually require more complex\nmodels to pass goodness-of-fit tests. then, some effects that are statistically\nsignificant may be weak and substantively unimportant. with large n it may\nbe adequate to use a model that is simpler than models that pass goodness-\nof-fit tests. an analysis that focuses solely on goodness-of-fit tests is incom-\nplete. it is also necessary to estimate model parameters and describe strengths\nof effects.\n\nthese remarks merely reflect\n\nlimitations of significance testing. null\nhypotheses are rarely true. with large enough n, they will be rejected. a\nmore relevant concern is whether the difference between true parameter\nvalues and null hypothesis values is sufficient to be important. many method-\nologists overemphasize testing and underutilize estimation methods such as\nconfidence intervals. when the p-value is small, a confidence interval speci-\nfies the extent to which h may be false, thus helping us determine whether\n0\nrejecting it has practical\nimportance. when the p-value is not small, the\nconfidence interval indicates whether some plausible parameter values are\nfar from h . a wide confidence interval containing the h value indicates\nthat the test had weak power at important alternatives.\n\n0\n\n0\n\n6.6 probit and complementary log-log models*\n\nfor binary responses,\n.\nmodels. like the logit model, these models have form 4.8 ,\n\nin this section we discuss two alternatives to logit\n\n\u017e\n\n\u2432 x s \u233d \u2423q \u2424x\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n6.10\n\n.\n\nfor a continuous cdf \u233d. the following argument motivates this class.\n\n6.6.1 tolerance motivation for binary response models\nin toxicology, binary response models describe the effect of dosage of a toxin\non whether a subject dies. the tolerance distribution provides justification for\n\n "}, {"Page_number": 261, "text": "246\n\nbuilding and applying logistic regression models\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nmodel 6.10 . let x denote the dosage level. for a randomly selected subject,\nlet y s 1 if the subject dies. suppose that the subject has tolerance t for the\ndosage, with y s 1 equivalent to t f x . for instance, an insect survives if\nthe dosage x is less than t and dies if the dosage is at least t. tolerances\nvary among subjects, and let f t s p t f t . for fixed dosage x, the\nprobability a randomly selected subject dies is\n\u017e\n\n\u2432 x s p y s 1 x s x s p t f x s f x .\n.\n\n\u017e .\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n<\n\nthat is, the appropriate binary model is the one having the shape of the cdf\nf of the tolerance distribution. let \u233d denote the standard cdf for the family\nto which f belongs. a common standardization uses the mean and standard\ndeviation of t, so that\n\n\u2432 x s f x s \u233d x y \u242e r\u2434 .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nthen, the model has form \u2432 x s \u233d \u2423q \u2424x .\n.\n\n\u017e\n\n.\n\n\u017e\n\n6.6.2 probit models\ntoxicological experiments often measure dosage as the log concentration\n\u017e\nbliss 1935 . often, the tolerance distribution for the dosage is approximately\n\u017e\nn \u242e, \u2434 for unknown \u242e and \u2434. if f is the n \u242e, \u2434 cdf, then \u2432 x has the\nform \u2432 x s \u233d \u2423q \u2424x , where \u233d is the standard normal cdf, \u2423s y\u242er\u2434\nand \u2424s 1r\u2434. in glm form,\n\n2.\n\u017e\n\n2.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\ny1\u233d \u2432 x s \u2423q \u2424x\n\n\u017e\n\n.\n\n\u017e\n\n6.11\n\n.\n\ny1\u017e .\n\n<\n\n<\n\nx\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\nthe response curve for \u2432 x\n\nis the probit model. the probit link function is \u233d \u2b48 . whereas the cdf maps\n.\nthe real line onto the 0, 1 probability scale, the inverse cdf maps the 0, 1\nscale for \u2432 x onto the real\nline values for linear predictors in binary\nresponse models.\n. w\nor for 1 y \u2432 x , when \u2424- 0 has the\nappearance of the normal cdf with mean \u242es y\u2423r\u2424 and standard deviation\n\u2434s 1r \u2424 . since 68% of the normal density falls within a standard devia-\ntion of the mean, 1r \u2424 is the distance between x values where \u2432 x s 0.16\n<\nor 0.84 and where \u2432 x s 0.50. the rate of change in \u2432 x is \u2b78\u2432 x r\u2b78x s\n.\n\u2424\u243e \u2423q \u2424x , where \u243e \u2b48 is the standard normal density function. the rate is\n\u017e .\nhighest when \u2423q \u2424x s 0 i.e., at x s y\u2423r\u2424 , where it equals \u2424r 2\u2432 s\n0.40\u2424 for \u2432s 3.14 . . .\n. at that point, \u2432 x s .2\n.\n\u017e\n.\nby comparison, in logistic regression with parameter \u2424, the curve for \u2432 x\nis a logistic cdf with standard deviation \u2432r \u2424 3 . its rate of change in \u2432 x\n\u017e\n.\nat x s y\u2423r\u2424 is 0.25\u2424. the rates of change where \u2432 x s are the same\nfor the cdf\u2019s corresponding to the probit and logistic curves when the logistic\n\u2424 is 0.40r0.25 s 1.6 times the probit \u2424. the standard deviations are the\nsame when the logistic \u2424 is \u2432r 3 s 1.8 times the probit \u2424. when both\n\n.\n'<\n\n.1r2\n\n.\n.\n\n\u017e\n\u017e\n\n'\n\n<\n\u017e\n\n1\n2\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n1\n\n<\n\n "}, {"Page_number": 262, "text": "probit and complementary log-log models\n\n247\n\n\u017e\n\n.\n\n\u017e\n\nmodels fit well, parameter estimates in logistic regression are about 1.6 to 1.8\ntimes those in probit models.\n\nthe likelihood equations that 4.24 showed for binomial regression mod-\nels apply to probit models see also problem 6.32 . one can solve them using\n.\nthe fisher\nscoring algorithm for glms bliss 1935, fisher 1935b .\nnewton\u1390raphson yields the same ml estimates but slightly different stan-\ndard errors. for the information matrix inverted to obtain the asymptotic\ncovariance matrix, newon\u1390raphson uses observed information, whereas\nfisher scoring uses expected information. these differ for binary links other\nthan the logit.\n\n\u017e\n\n.\n\n6.6.3 beetle mortality example\ntable 6.14 reports the number of beetles killed after 5 hours of exposure to\n.\ngaseous carbon disulfide at various concentrations. figure 6.6 plots as dots\nthe proportion killed against the log concentration. the proportion jumps up\nat about x s 1.8, and it is close to 1 above there.\n\n\u017e\n\nthe ml fit of the probit model is\n\ny1\u233d \u2432 x s y34.96 q 19.74 x.\n\n\u017e\n\n.\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u02c6\n\n\u02c6\n\nfor this fit, \u2432 x s 0.5 at x s 34.96r19.74 s 1.77. the fit corresponds to a\nnormal tolerance distribution with \u242es 1.77 and \u2434s 1r19.74 s 0.05. the\n2.\ncurve for \u2432 x is that of a n 1.77, 0.05\n\u017e\n\u02c6\nat dosage x with n beetles, n \u2432 x\nis the fitted count for death,\ni\ni s 1, . . . , 8. table 6.14 reports the fitted values and figure 6.6 shows the fit.\nthe table also shows fitted values for the linear logit model. these models fit\nsimilarly and rather poorly. the g2 goodness-of-fit statistic equals 11.1 for\nthe logit model and 10.0 for the probit model, with df s 6.\n\ncdf.\n.\ni\n\n\u017e\n\ni\n\ni\n\ntable 6.14 beetles killed after exposure to carbon disulfide\n\nlog dose\n\nnumber\nof beetles\n\nnumber\nkilled\n\ncomp. log-log\n\nfitted values\n\n1.691\n1.724\n1.755\n1.784\n1.811\n1.837\n1.861\n1.884\n\n5.7\n11.3\n20.9\n30.3\n47.7\n54.2\n61.1\n59.9\n.\nsource: data reprinted with permission from bliss 1935 .\n\n59\n60\n62\n56\n63\n59\n62\n60\n\n6\n13\n18\n28\n52\n53\n61\n60\n\n\u017e\n\nprobit\n3.4\n10.7\n23.4\n33.8\n49.6\n53.4\n59.7\n59.2\n\nlogit\n3.5\n9.8\n22.4\n33.9\n50.0\n53.3\n59.2\n58.8\n\n "}, {"Page_number": 263, "text": "248\n\nbuilding and applying logistic regression models\n\nfigure 6.6 proportion of beetles killed versus log dosage, with fits of probit and complemen-\ntary log-log models.\n\n6.6.4 complementary log-log link models\nthe logit and probit links are symmetric about 0.5, in the sense that\n\nlink \u2432 x s ylink 1 y \u2432 x\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\nto illustrate,\n\nlogit \u2432 x s log \u2432 x r 1 y \u2432 x\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\u017e\n\ns ylog 1 y \u2432 x r\u2432 x s ylogit 1 y \u2432 x\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n.\n\nthis means that the response curve for \u2432 x has a symmetric appearance\nabout the point where \u2432 x s 0.5, so \u2432 x approaches 0 at the same rate it\napproaches 1. logit and probit models are inappropriate when this is badly\nviolated.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nthe response curve\n\n\u2432 x s 1 y exp yexp \u2423q \u2424x\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n6.12\n\n.\n\n.\nhas the shape shown in figure 6.7. it is asymmetric, \u2432 x approaching 0\nfairly slowly but approaching 1 quite sharply. for this model,\n\n\u017e\n\nlog ylog 1 y \u2432 x s \u2423q \u2424x.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nthe link for this glm is called the complementary log-log link, since the\n.\nlog-log link applies to the complement of \u2432 x .\n\n\u017e\n\n.\n\u017e\n\n\u017e\n\n.\n\n "}, {"Page_number": 264, "text": "probit and complementary log-log models\n\n249\n\nfigure 6.7 model with complementary log\u1390log link.\n\nto interpret model 6.12 , we note that at x and x ,\n2\n\n1\n\n\u017e\n\n.\n\ns \u2424 x y x\n\n\u017e\n\n2\n\n.\n\n,\n\n1\n\nlog ylog 1 y \u2432 x\n\u017e\n\n\u017e\n\n.\n\n.\n\n2\n\ny log ylog 1 y \u2432 x\n\u017e\n\n\u017e\n\n.\n\n.\n\n1\n\nso that\n\nand\n\nlog 1 y \u2432 x\u017e\nlog 1 y \u2432 x\u017e\n\n.2 s exp \u2424 x y x\n.1\n\n\u017e\n\n2\n\n.\n\n1\n\n1 y \u2432 x s 1 y \u2432 x\n\u017e\n\n\u017e\n\n.\n\n2\n\n.\n\n1\n\nexp \u2424 x yx\n\n\u017e\n\nw\n\n2\n\n.x\n\n1\n\n.\n\nfor x y x s 1, the complement probability at x equals the complement\nprobability at x\n\n.\nraised to the power exp \u2424 .\n\n\u017e\n\n2\n\n1\n\n2\n\na related model to 6.12 is\n\n\u017e\n\n.\n\n1\n\n\u2432 x s exp yexp \u2423q \u2424x\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n6.13\n\n.\n\n\u017e\n\n.\n\nfor it, \u2432 x approaches 0 sharply but approaches 1 slowly. as x increases,\nthe curve is monotone decreasing when \u2424) 0, and monotone increasing\nwhen \u2424- 0. in glm form it uses the log-log link\n\nlog ylog \u2432 x s \u2423q \u2424x.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nmodel 6.13 with log-log link is the special case of 6.10 with cdf of the\n\nwhen the complementary log-log model holds for the probability of a\nsuccess, the log-log model holds for the probability of a failure.\nextreme \u00aealue or gumbel distribution. the cdf equals\nf x s exp yexp y x y a rb\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n "}, {"Page_number": 265, "text": "250\nbuilding and applying logistic regression models\nfor parameters b ) 0 and y\u2b01 - a - \u2b01. it has mean a q 0.577b and\nstandard deviation \u2432br 6 . models with log-log links can be fitted using the\nfisher scoring algorithm for glms.\n\n'\n\n\u017e\n\n\u02c6\n\nsurvival\n\n6.6.5 beetle mortality example revisited\n.\nfor the beetle mortality data table 6.14 , the complementary log-log model\nhas ml estimates \u2423s y39.52 and \u2424s 22.01. at dosage x s 1.7, the fitted\nis 1 y \u2432 x s exp yexp y39.52 q 22.01 1.7 s\nprobability of\n0.885, whereas at x s 1.8 it is 0.332 and at x s 1.9 it is 5 = 10y5. the\nprobability of survival at dosage x q 0.1 equals the probability at dosage x\nraised to the power exp 22.01 = 0.1 s 9.03. for instance, 0.332 s 0.885\n.9.03\n.\ntable 6.14 shows the fitted values and figure 6.6 shows the fit. they are\nclose to the observed death counts g s 3.5, df s 6 . the fit seems ade-\nquate. aranda-ordaz 1981 and stukel 1988 discussed these data further.\n\n\u02c6\n.\n\n.x4\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nw\n\n\u0004\n\n2\n\n6.7 conditional logistic regression and exact\ndistributions*\n\nml estimators of logistic model parameters work best when the sample size\nn is large compared to the number of parameters. when n is small or when\nthe number of parameters grows as n does, improved inference results using\nconditional maximum likelihood. in this section we present this approach and\nin section 10.2 apply it with matched case\u1390control studies.\n\n\u017e\n\n6.7.1 conditional likelihood\nthis conditional\nlikelihood approach eliminates nuisance parameters by\nconditioning on their sufficient statistics. this generalizes fisher\u2019s method for\n2 = 2 tables section 3.5 . the conditional likelihood refers to a conditional\ndistribution defined for potential samples that provide the same information\nabout the nuisance parameters that occurs in the observed sample.\n\nwe begin with a general exposition and then discuss special cases. let yi\ni s 1, . . . , n. for now, each yi\ndenote the binary response for subject i,\nrefers to a single trial, so n s 1. let x be the value of predictor j for that\nsubject,\n\n.\nj s 1, . . . , p. the model is\n\n.\n\n\u017e\n\ni j\n\ni\n\np y s y s\n\u017e\n\n.\n\ni\n\ni\n\n\u017e\n\nexp y \u2423q \u00fd \u2424 x\n1 q exp \u2423q \u00fd \u2424 x\n\np\njs1\np\njs1\n\n\u017e\n\ni j\n\ni\n\nj\n\nj\n\n.\n\ni j\n\n.\n\n,\n\n\u017e\n\n6.14\n\n.\n\nwhere substituting y s 1 gives the usual expression, such as 5.15 . here, we\nexplicitly separate the intercept from the coefficients of the p predictors. for\nn independent observations,\n\n\u017e\n\n.\n\ni\n\np y s y , . . . , y s y s\n\u017e\n\n.\n\nn\n\nn\n\n1\n\n1\n\n.\n\n\u017e\n\nexp \u00fd y \u2423q \u00fd\n\u0142 1 q exp \u2423q \u00fd \u2424 x\n\n\u017e\n.\n\u00fd y x \u2424\nj\ni\n.\np\njs1\n\np\njs1\n\n\u017e\n\ni j\n\ni j\n\ni\n\ni\n\ni\n\ni\n\nj\n\n.\n\n\u017e\n\n6.15\n\n.\n\n "}, {"Page_number": 266, "text": "i\n\nconditional logistic regression and exact distributions\n\n251\nj s\nfrom this likelihood function, the sufficient statistic for \u2424 is \u00fd y x ,\ni j\n1, . . . , p. the sufficient statistic for \u2423 is \u00fd y , the total number of successes.\nusually, some parameters refer to effects of primary interest. others may\nbe there to adjust for relevant effects, but their values are not of special\ninterest. we can eliminate the latter parameters from the likelihood by\nconditioning on their sufficient statistics. we illustrate by eliminating \u2423. in\nsection 10.2.5 we show that for models for matched case\u1390control studies,\nintercept terms cause difficulties with inference about the primary parame-\nters, so it can be helpful to eliminate them. since the sufficient statistic for \u2423\nis \u00fd y , we condition on \u00fd y . suppose that \u00fd y s t. denote the conditional\nreference set of samples having the same value of \u00fd y as observed by\n\n.\n\n\u017e\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nj\n\ni\n\ni\n\ns t s y*, . . . , y* :\n\u017e .\n\n\u017e\n\n. \u00fd\n\nn\n\n1\n\n\u00bd\n\ny * s t\n\ni\n\n5\n\n.\n\ni\n\n\u0004\nwith y\n\n4\n\ni\n\nsuch that \u00fd y s t, the conditional likelihood function equals\n\ni\n\ni\n\n\u017e\n\np y s y , . . . , y s y\n\nn\n\n1\n\n1\n\ny s t s\n\ni\n\n/\n\nn\n\n\u00fd\n\ni\n\np y s y , . . . , y s y\n\u017e\n\nn\n\n1\n\n1\n\n.\n\nn\n\n\u00fd p y s y *, . . . , y s y *\ns\u017et.\n\n\u017e\n\nn\n\nn\n\n1\n\n1\n\n.\n\ns\n\nexp t\u2423q \u00fd\n\u017e\np\njs1\n\u00fd exp t\u2423q \u00fd\np\njs1\ns\u017et.\n\ni\n\ni\n\n.\n\n\u00fd y x \u2424 \u0142 1 q exp \u2423q \u00fd \u2424 x\n.\n\u00fd y *x \u2424 r\u0142 1 q exp \u2423q \u00fd \u2424 x\n\u017e\np\njs1\n\np\njs1\n\n\u017e\n\n\u017e\n\n.\n\ni j\n\ni j\n\ni j\n\ni\n\ni\n\ni\n\ni\n\nj\n\nj\n\nj\n\nj\n\n.\n\ni j\n\ns\n\nexp \u00fd\n\n\u017e\np\njs1\n\u00fd exp \u00fd\np\njs1\n\ns\u017et.\n\ni\n\ni j\n\n\u00fd y x \u2424\nj\n.\n\u017e\n\ni\n\u00fd y *x \u2424\nj\n\ni j\n\ni\n\ni\n\n.\n\n.\n\nthis does not depend on \u2423.\n\na conditional likelihood is used just like an ordinary likelihood. for the\nparameters in it, their conditional ml estimates are the values maximizing it.\ncalculated using iterative methods, the estimators are asymptotically normal\nwith covariance matrix equal to the negative inverse of the matrix of second\npartial derivatives of the conditional log likelihood.\n\n6.7.2 small-sample conditional inference for logistic regression\nfor small samples, inference for a parameter uses the conditional distribu-\ntion after eliminating all other parameters. with it, one can calculate proba-\nbilities such as p-values exactly rather than with crude approximations cox\n.\n1970 .\n\nfor instance, suppose that inference focuses on \u2424 in model 6.14 . to\nstatistics\nj s 0, . . . , p y 1 where x s 1 . with an argument like that\n\np\neliminate other parameters, we condition on their\nt s \u00fd y x ,\nj\n\nsufficient\n\ni0\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\ni j\n\ni\n\ni\n\n "}, {"Page_number": 267, "text": "252\n\nbuilding and applying logistic regression models\n\njust shown, one obtains the conditional distribution\n\np y s y , . . . , y s y t s t , j s 0, . . . , p y 1\n\u017e\n\n<\n\nn\n\nn\n\n1\n\n1\n\nj\n\nj\n\ns\n\n\u00fd\n\nexp \u00fd y x \u2424\np\n\ni p\n\ni\n\ni\n\n\u017e\n\n.\n\ns\u017et\n\n0\n\n, . . . , t\n\npy1\n\n.\n\nexp \u00fd y *x \u2424\np\n\ni p\n\ni\n\ni\n\n\u017e\n\n.\n\ns\n\n\u00fd\n\n.\nexp t \u2424\np\n\n\u017e\n\n.\n\u017e\n\np\nexp t *\u2424\np\n\np\n\n.\n\n,\n\n.\n\ns\u017et\n\n0\n\n, . . . , t\n\npy1\n\nwhere\n\n\u017e\ns t , . . . , t\n\n0\n\npy1\n\n.\n\n\u00bd\ns y*, . . . , y* :\n.\n\n\u017e\n\nn\n\n1\n\n\u00fd\n\ni\n\n5\ny *x s t , j s 0, . . . , p y 1 .\n\ni j\n\ni\n\nj\n\nthis depends only on \u2424 . inference for \u2424 uses the conditional distribution\nof its sufficient statistic, t s \u00fd y x , given the others. let c t , . . . , t\n.\n, t\nfor which t s t. the\ndenote the number of data vectors in s t , . . . , t\nconditional distribution of t is\n\npy1\n\npy1\n\ni p\n\n\u017e\n\n\u017e\n\n.\n\n0\n\n0\n\np\n\np\n\np\n\np\n\ni\n\ni\n\np\n\np t s t t s t , j s 0, . . . , p y 1 s\n\u017e\n\n.\n\n<\n\np\n\nj\n\nj\n\n\u017e\nc t , . . . , t\n\u00fd c t , . . . , t\n\n0\n\n\u017e\n\nu\n\n0\n\npy1\npy1\n\n\u017e\n\n.\n.\n, t exp t\u2424\np\n\u017e\n, u exp u\u2424\np\n\u017e\n\n.\n\n,\n\n.\n6.16\n\n.\n\nfor testing h : \u2424 s 0, the conditional distribution simplifies. for h :\n\nwhere the denominator summation refers to the possible values u of t .p\n\u2424 ) 0 and observed t s t\n\n, the exact conditional p-value is\n\na\n\n0\n\np\n\np\n\nobs\n\np\n\np t s t t s t , j s 0, . . . , p y 1 s\n\u017e\n\n.\n\n<\n\np\n\nj\n\nj\n\n\u00fd\ntgt obs\n\n\u017e\nc t , . . . , t\n\n\u00fd\nt g t\n\u00fd c t , . . . , t\n\nobs\n\n0\n\n\u017e\n\nu\n\n0\n\npy1\n\n,t\npy1\n.\n, u\n\n.\n\n,\n\nthe proportion of data configurations in the conditional set that have the\nsufficient statistic for \u2424 at least as large as observed. implementing this\ninference requires calculating c t , . . . , t\n, u . for all but the simplest\nproblems, computations are intensive and require specialized software e.g.,\nlogxact of cytel software or proc logistic in sas . in the remainder\nof this section we consider special cases for small-sample inference.\n\n\u0004 \u017e\n\npy1\n\n.4\n\n\u017e\n\n.\n\n0\n\np\n\n6.7.3 small-sample conditional inference for 2 = 2 contingency tables\nfirst, consider logistic regression with a single predictor x,\ni s 1, . . . , n,\n\nlogit p y s 1 s \u2423q \u2424x ,\n\n\u017e\n\n.\n\n\u017e\n\n6.17\n\n.\n\ni\n\ni\n\nwhen x takes only two values. the model applies to 2 = 2 tables, where\nx s 1 denotes row 1 and x s 0 denotes row 2. the sufficient statistic for \u2423\ni\n\ni\n\ni\n\n "}, {"Page_number": 268, "text": "conditional logistic regression and exact distributions\n\n253\n\ni\n\ni\n\ni\n\ni\n\ni\n\n2\n\n1\n\n2\n\n1\n\nis \u00fd y , which is the first column total. the sufficient statistic for \u2424 is\nt s \u00fd y x , which simplifies to the number of successes in the first row.\nequivalently, the sufficient statistics for the model are the numbers of\nsuccesses in the two rows. let s and s denote these binomial variates. the\nrow totals n and n are their indices.\nto eliminate \u2423, we condition on s s s q s , the first column total. since\nn s n q n is fixed, so then is the other column marginal total. fixing both\nsets of marginal totals yields hypergeometric probabilities for s\nthat depend\nonly on \u2424 see 3.20 , identifying \u242as exp \u2424 . in that case the conditional\ndistribution satisfies 6.16 with c t , t s\nand with t s s and\nt s s . the resulting exact conditional test that \u2424s 0 is fisher\u2019s exact test for\n.\n2 = 2 tables section 3.5.1 .\n\nn y n\nt y t\n0\n\n.x\n/\n\nn\n1\nt\n\n.\n\u017e\n\n2\nw\n\n/\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n1\n\n2\n\n1\n\n1\n\n0\n\n0\n\n1\n\n1\n\n\u017e\n\n6.7.4 small-sample conditional inference for linear logit model\nlogit \u2432 s \u2423q \u2424x , applies to i = 2 tables with\nthe linear logit model,\n4\nordered rows. we discussed this model in section 5.3.4. for it, the data yi\nare i independent bin n , \u2432 counts, with fixed row totals n . condition-\ning on s s \u00fd y and hence the column totals yields a conditional likelihood\nfree of \u2423. exact inference about \u2424 uses its sufficient statistic, t s \u00fd x y .\nfrom 6.16 its distribution has the form\n\n.4\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u0004\n\n\u0004\n\n\u0004\n\n4\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u017e\n\np t s t\n\ni\n\n\u00fd i\n\ny s s; \u2424 s\n\n\u2424t\n\n\u017e\n.\nc s, t e\n.\n\u00fd c s, u e\n\n\u017e\n\nu\n\n.\n\n\u2424u\n\n\u017e\n\n6.18\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\nhere, c s, u equals the sum of \u0142\nmarginal totals that have t s u.\n\nfor all tables with the given\nwhen \u2424s 0, the cell counts have the multiple hypergeometric distribu-\ntion 3.19 . to test this, ordering the tables with the given margins by t is\n\u017e\nequivalent to ordering them by the cochran\u1390armitage statistic section\n5.3.5 . thus, this test for the linear logit model is an exact trend test.\nin section 5.3.5 we applied the cochran\u1390armitage test to table 5.3 on\nmaternal alcohol consumption and infant malformation. even though n s\n32,573, the table is highly unbalanced, with both very small and very large\ncounts. it is safer to use small-sample methods. for the exact conditional\ntrend test with the same scores, the one-sided p-value for h : \u2424) 0 is\n0.0168. the two-sided p-value is 0.0172, reflecting asymmetry of the condi-\ntional distribution, given the marginal counts. this is not much different from\nthe two-sided p-value of 0.010 obtained with the large-sample cochran\u1390\narmitage test.\n\na\n\n/\ni\u017e\n\nni\n\n/yi\n\n "}, {"Page_number": 269, "text": "254\n\nbuilding and applying logistic regression models\n\nk\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n\u017e\n\n.\n\ni jk\n\niqk\n\nz4\nk\n\n, the cochran\u1390mantel\u1390haenszel test uses \u00fd n\n\n6.7.5 small-sample tests of conditional independence in 2 = 2 = k tables\nfor 2 = 2 = k tables n\n.\n11 k\nfor logit model 6.4 , this is the sufficient statistic for \u2424, the effect of x. to\nconduct a small-sample test of \u2424s 0, one needs to eliminate the other\nmodel parameters. constructing the likelihood reveals that the sufficient\nstatistics for \u2424 are the column marginal totals n\nin each partial table.\nwhen x and z are predictors, it is natural to treat the numbers of trials\n\u0004\nat each combination of xz values as fixed. thus, exact inference\nn\nabout \u2424 conditions on the row and column totals in each stratum.\n\nconditional on the strata margins, an exact test uses \u00fd n\n\n. hypergeo-\nmetric probabilities occur in each partial table for the independent null\n, k s 1, . . . , k . the product of the k mass functions\n\u0004\n4\ndistributions of n\n, k s 1, . . . , k . this is 6.19 below,\ngives the null joint distribution of n\nsetting \u242as 1. this determines\n. for\nh : \u2424) 0, the p-value is the null probability that \u00fd n\nis at least as large\nas observed, for the fixed strata marginal totals. mehta et al. 1985 pre-\nsented a fast algorithm. the test simplifies to fisher\u2019s exact test when k s 1.\n\n11 k\nthe null distribution of \u00fd n\n\nqj k\n\n11 k\n\n11 k\n\n11 k\n\n11 k\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nx\n\nw\n\n\u0004\n\n4\n\n\u0004\n\n4\n\na\n\nk\n\nk\n\nk\n\n6.7.6 promotion discrimination example\ntable 6.15 refers to u.s. government computer specialists of similar seniority\nconsidered for promotion. the table cross-classifies promotion decision by\nemployee\u2019s race, considered for three separate months. we test conditional\nindependence of promotion decision and race, or h : \u2424s 0, in model 6.4 .\n.\nthe table contains several small counts. the overall sample size is not small\nn s 74 , but one marginal count collapsing over month of decision equals\n\u017e\nzero, so we might be wary of using the cmh test.\n\nfor h : \u2424- 0 i.e., odds ratio - 1 , the probability of promotion was\nlower for black employees than for white employees. for the margins of the\ncan range\npartial tables in table 6.15, n\nbetween 0 and 4, and n\ncan\nk\nrange between 0 and 10. the sample data are the most extreme possible\n\ncan range between 0 and 2. the total \u00fd n\n\ncan range between 0 and 4, n\n\n11 k\n\n111\n\n112\n\n113\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\na\n\n0\n\ntable 6.15 promotion decisions by race and by month\n\njuly\n\npromotions\n\naugust\n\npromotions\n\nseptember\npromotions\n\nrace\nblack\nwhite\n\nyes\n0\n4\n\nno\n7\n16\n\nyes\n0\n4\n\nno\n7\n13\n\nyes\n0\n2\n\nno\n8\n13\n\nsource: j. gastwirth, statistical reasoning in law and public policy san diego, ca: academic\npress, 1988 , p. 266.\n\n.\n\n\u017e\n\n "}, {"Page_number": 270, "text": "255\nconditional logistic regression and exact distributions\nresult in each case. the observed \u00fd n s 0, and the p-value is the null\nprobability of this outcome. software provides p s 0.026. a two-sided p-\nvalue, based on summing the probabilities of all tables no more likely than\nthe observed table, equals 0.056.\n\n11 k\n\nk\n\n.\n\n\u017e\n\n6.7.7 exact conditional estimation and comparison of odds ratios\nfor model 6.4 of homogeneous association in 2 = 2 = k tables,\nthe\nordinary ml estimator of the odds ratio \u242as exp \u2424 behaves poorly for\nsparse-data asymptotics. the conditional ml estimator maximizes the condi-\ntional likelihood function after reducing the parameter space by conditioning\non sufficient statistics for the other parameters andersen 1970; birch\n.\n1964b .\nity mass function that n s t , . . . , n s t\ntions 3.20 from the separate strata, or\n\nfor all k, the conditional probabil-\nis the product of the func-\n\n4\n\u0004\n, given n\n\u017e\n1\n\niqk qj k\n11 k\n\nfor cell counts n\n\n, n\n\n111\n\ni jk\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n4\n\nk\n\np n s t n\n\u017e\n<\n\n11 k\n\nk\n\n1qk\n\n\u0142\n\nk\n\n, n\n\nq1 k qqk\n\n, n\n\n;\u242a s\n\n.\n\n\u0142\n\nk\n\nn\n\n\u017e\nu\u017e\n\n\u00fd\n\n/\n\n/\u017e\n/\u017e\n\n1qk\n\nn y n\nqqk\nn y t\nq1 k\nn y n\nqqk\n1qk\nn y u\nq1 k\n\nk\n\n1qk\nt\n\nk\nn\n\n1qk\nu\n\ntk\u242a\n\n.\n\nu\n\u242a\n\n/\n\n\u017e\n\n6.19\n\n.\n\n\u02c6\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u02c6\nmh\n\nthe conditional distribution 6.19 propagates one for \u00fd n\n\nthe conditional ml estimator \u242a maximizes 6.19 . like the mantel\u1390haenszel\nestimator \u242a , it has good properties for both standard and sparse-data\nasymptotic cases andersen 1970; breslow 1981 , since the number of param-\neters does not change as k does. it can be slightly more efficient than \u242a ,mh\nexcept when \u242as 1.0, where they are equally efficient, or for matched pairs,\n.\nwhere they are identical breslow 1981 .\n, which is\nused to test h : \u242as \u242a for an arbitrary value. then, a 95% confidence\ninterval for \u242a consists of all \u242a for which the p-value exceeds 0.05. such an\ninterval is guaranteed to have at least the nominal coverage probability gart\n1970; kim and agresti 1995; mehta et al. 1985 . this extends the interval for\na single 2 = 2 table section 3.6.1 . for the promotion discrimination case\ntable 6.15 , \u00fd n s 0, so the lower bound of any confidence interval for \u242a\n\u017e\nshould be 0. for the generalization to several strata of cornfield\u2019s tail-method\n.\ninterval, statxact reports a 95% confidence interval of 0, 1.01 .\n\n11 k\n\n11 k\n\n\u02c6\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n0\n\n0\n\n0\n\nk\n\nk\n\nzelen 1971 presented a small-sample test of homogeneity of the odds\nratios. see agresti 1992 for discussion of this and other small-sample\nmethods for contingency tables.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 271, "text": "256\n\nbuilding and applying logistic regression models\n\ntable 6.16 example for exact conditional logistic regression\n\ncephalexin\n\na\n\n0\n0\n0\n0\n1\n\na\n\nage\n0\n0\n1\n1\n1\n\nlength\na\nof stay\n\ncases of\ndiarrhea\n\n0\n1\n0\n1\n1\n\n0\n5\n3\n47\n5\n\nsample\n\nsize\n385\n233\n789\n1081\n5\n\nasee the text for an explanation of 0 and 1.\nsource: based on study by e. jaffe and v. chang, cornell medical center, reported in the\nmanual for logxact cambridge, ma: cytel software, 1999 , p. 259.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n6.7.8 diarrhea example\nthe final example deals with a larger number of variables. table 6.16 refers\nto 2493 patients having stays in a hospital. the response is whether they\nsuffered an acute form of diarrhea during their stay. the three predictors are\nage 1 for over 50 years old, 0 for under 50 , length of stay in hospital 1 for\nmore than 1 week, 0 for less than 1 week , and exposure to an antibiotic\ncalled cephalexin 1 for yes, 0 for no . we discuss estimation of the effect of\ncephalexin, controlling for age and length of stay, using a model containing\nonly main-effect terms.\n\n.\n.\n\nthe sample size is large, yet relatively few cases of acute diarrhea\noccurred. moreover, all subjects having exposure to cephalexin were also\ndiarrhea cases. such boundary outcomes in which none or all responses fall\nin one category cause infinite ml estimates of some model parameters. an\nml estimate of \u2b01 for the cephalexin effect means that the likelihood\nfunction increases continually as the parameter estimate for cephalexin\nincreases indefinitely.\n\nto study the cephalexin effect, we use an exact distribution, conditioning\non sufficient statistics for the other predictors. although the estimate of the\nlog-odds-ratio parameter for the effect of cephalexin is infinite, it is possible\nto construct a confidence interval by inverting the family of tests for the\nparameter, using the conditional distribution. doing this, a 95% confidence\ninterval is 19, \u2b01 for the odds ratio. assuming that the main-effects model is\nvalid, cephalexin appears to have a strong effect. similarly, p - 0.0001 for\ntesting that the log odds ratio equals zero.\n\nresults must be qualified somewhat because no cephalexin cases occurred\nat the first three combinations of levels of age and length of stay. in fact, the\nfirst three rows of table 6.16 make no contribution to the analysis problem\n6.18 . the data actually provide evidence about the effect of cephalexin only\nfor older subjects having a long stay.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n "}, {"Page_number": 272, "text": "notes\n\n257\n\n0\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n6.7.9 complications from discreteness\nlike fisher\u2019s exact test, exact conditional inference for contingency tables is\nconservative because of discreteness. this is especially true when n is small\nor the data are unbalanced, with most observations falling in a single column\nor row. using mid-p-values or p-values based on a finer partitioning of the\nsample space note 3.9 in tests and related confidence intervals reduces\nconservativeness. for the promotion discrimination data table 6.15 , we\n.\nreported a 95% confidence interval for the common odds ratio of 0, 1.01 .\ninverting exact tests of h : \u242as \u242a with the mid-p-value yields the interval\n\u017e\n0, 0.78 . however, this approach cannot guarantee that the actual coverage\nprobability is bounded below by 0.95.\n\na particular problem occurs when no other set of y* values has the same\nvalue of a given sufficient statistic \u00fd y x as the observed data. in that case\nthe conditional distribution of the sufficient statistic for the parameter of\ninterest is degenerate. the p-value for the exact test then equals 1.0. this\ncommonly happens when at least one explanatory variable x whose effect is\nconditioned out for the inference is continuous, with unequally spaced\nobserved values.\n\nfinally, a limitation of the conditional approach is requiring sufficient\nstatistics for the nuisance parameters. this happens only with glms that use\nthe canonical link. thus, for instance, the conditional approach works for\nlogit models but not probit models.\n\n\u017e\n\n\u0004\n\n4\n\ni j\n\n0\n\ni\n\ni\n\ni\n\nj\n\nnotes\n\nsection 6.1: strategies in model selection\n\n.\u017e\n\n.x\n\n6.1. a bayesian argument motivates the bayesian information criterion bic s g y\n\u017e\nlog n df , an alternative to aic. it takes sample size into account. compared to aic,\nbic gravitates less quickly toward more complex models as n increases. for details and\ncritiques, see raftery 1986 and the february 1999 issue of sociological methods and\nresearch.\n\n\u017e\n\n.\n\n2\n\nw\n\n6.2. tree-structured methods such as cart are alternatives to logistic regression that\nformalize a decision process using a sequential set of questions that branch in different\ndirections depending on a subject\u2019s responses. an example is deciding whether a subject\nwith chest pains may be suffering a heart attack. zhang et al. 1998 surveyed such\nmethods.\n\n\u017e\n\n.\n\nsection 6.2: logistic regression diagnostics\n\n6.3. for logistic regression diagnostics, see copas 1988 , fowlkes 1987 , hosmer and\n.\nlemeshow 2000, chap. 5 , johnson 1985 , landwehr et al. 1984 , and pregibon 1981 .\nseparate diagnostics are useful for checking the adequacy of each component of a glm\n\u017e\nmccullagh and nelder 1989, chap. 12 . for a family g \u242e; \u2425 of link functions indexed\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 273, "text": "258\n\nbuilding and applying logistic regression models\n\nby parameter \u2425, pregibon 1980 showed how to estimate \u2425 giving the link with best fit\n.\nand how to check the adequacy of a given link g \u242e; \u2425 .0\n\n\u017e\n\n.\n\n6.4. amemiya 1981 , efron 1978 , maddala 1983 , and zheng and agresti 2000 and\nreferences therein reviewed r2 measures for binary regression. hosmer and lemeshow\n\u017e\n2000, sec. 5.2.3 discussed classification tables and their limitations. pepe 2000 and\nreferences therein surveyed roc methodology.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\nsection 6.3: inference about conditional associations in 2 = 2 = k tables\n\n\u02c6\n6.5. analogs of \u242a\nmh\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nsummarize differences of proportions or relative risks from several\nstrata greenland and robins 1985 . breslow and day 1980, p. 142 proposed an\nalternative large-sample test of homogeneity of odds ratios. in each partial table let\n\u02c6\n4\u242e have the same marginals as the data observed, yet have odds ratio equal to \u242a .\n\u0004\n\u02c6i jk\nmh\n\u0004\n.\nto \u242e . tarone 1985\ntheir test statistic has the pearson form comparing n\n\u02c6\nshowed that because of the inefficiency of \u242a\none must adjust the breslow\u1390day\nmh\nstatistic for it to have a limiting chi-squared null distribution with df s k y 1. this\nadjustment is usually minor. jones et al. 1989 reviewed and compared several tests of\nhomogeneity in sparse and nonsparse settings. other work on comparing odds ratios and\nestimating a common value include breslow and day 1980, sec. 4.4 , donner and\nhauck 1986 , gart 1970 , and liang and self 1985 . for modeling the odds ratio, see\nbreslow 1976 , breslow and day 1980, sec. 7.5 , and prentice 1976a . breslow\nemphasized retrospective studies, in which the conditional approach is natural since the\noutcome totals are fixed.\n\n.\n.\n\n\u02c6\n\ni jk\n\ni jk\n\n\u017e\n\n.\n\n\u017e\n\n4\n\n\u0004\n\n4\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nsection 6.5: sample size and power considerations\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n6.6. for sample-size determination for comparing proportions, fleiss 1981, sec. 3.2 pro-\nvided tables. see lachin 1977 for the i = j case. chapman and meng 1966 , drost\n.\net al. 1989 , haberman 1974a, pp. 109\u1390112 , harkness and katz 1964 , mitra 1958 ,\nand patnaik 1949 derived theory for asymptotic nonnull behavior of chi-squared\nstatistics; see also section 14.3.5. o\u2018brien\u2019s 1986 simulation results suggested that the\nnoncentral chi-squared approximation for g2 holds well for a wide range of powers.\nread and cressie 1988, pp. 147\u1390148 listed other articles that studied the nonnull\nbehavior of x 2 and g2.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nsection 6.6: probit and complementary log-log models\n\n.\n\n\u017e\n\n.\n6.7. finney 1971 is the standard reference on probit modeling. chambers and cox 1967\nshowed that it is difficult to distinguish between probit and logit models unless n is\nextremely large. ashford and sowden 1970 generalized the probit model for multivari-\nate binary responses; see also lesaffre and molenberghs 1991 and ochi and prentice\n\u017e\n.\n1984 . wedderburn 1976 showed that the log likelihood is concave for probit and\ncomplementary log-log links.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nsection 6.7: conditional logistic regression\n\n.\n\n\u017e\n\n.\n\n6.8. for details about conditional logistic regression, see section 10.2, breslow and day\n\u017e\n.\n1980, chap. 7 , cox 1970 , and hosmer and lemeshow 2000, chap. 5 . liang 1984\nshowed that conditional ml estimators and conditional score tests are asymptotically\nequivalent to their unconditional counterparts under sampling from exponential fami-\nlies. for exact inference using the conditional likelihood, see hirji et al. 1987 , mehta\n.\nand patel 1995 , and the logxact manual cytel software . mehta et al. 2000\ndiscussed monte carlo approximations.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n "}, {"Page_number": 274, "text": "problems\n\nproblems\n\napplications\n\n259\n\n6.1 for the horseshoe crab data, fit a model using weight and width as\npredictors. conduct a a likelihood-ratio test of h : \u2424 s \u2424 s 0, and\n\u017e .b separate tests for the partial effects. why does neither test in part\n\u017e .\nb show evidence of an effect when the test in part a shows strong\nevidence?\n\n\u017e .\n\n\u017e .\n\n0\n\n1\n\n2\n\n6.2 refer to the data for problem 8.13. treating opinion about premarital\nsex as the response variable, use backward elimination to select a\nmodel. interpret.\n\n6.3 refer to table 6.4. fit the stage 3 model denoted there by e*p q g .\n.\nuse parameter estimates to interpret the g effect and the dependence\nof the e effect on p.\n\n\u017e\n\n6.4 discern the reasons that simpson\u2019s paradox occurs for table 6.7.\n\n6.5 refer to problem 2.12.\n\na. fit the model with g and d main effects. using it, estimate the\nag conditional odds ratio. compare to the marginal odds ratio, and\nexplain why they are so different. test its goodness of fit.\n\nb. fit the model of no g effect, given the department. use x 2 to test\nfit. obtain residuals, and interpret the lack of fit. each department\nhas a single nonredundant standardized pearson residual. they\nsatisfy \u00fd r s x , their squares giving six df s 1 components.\n\n\u017e\n\n.\n\n2\n\n6\n2\nis1 i\n\nc. fit the two models excluding department a. again consider lack of\n\nfit, and interpret.\n\n6.6 conduct a residual analysis for the independence model with table\n\n6.11. what type of lack of fit is indicated?\n\n6.7 table 6.17, refers to the effectiveness of immediately injected or\n11 -hour-delayed penicillin in protecting rabbits against lethal injection\n2\nwith \u2424-hemolytic streptococci.\na. let x s delay, y s whether cured, and z s penicillin level. fit\nthe logit model 6.4 . argue that the pattern of 0 cell counts\nsuggests that with no intercept \u2424 s y\u2b01 and \u2424 s \u2b01. what does\nyour software report?\n\n\u02c6z\n1\n\n\u02c6z\n5\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nb. using the logit model, conduct the likelihood-ratio test of xy\n\nconditional independence. interpret.\n\n "}, {"Page_number": 275, "text": "260\n\nbuilding and applying logistic regression models\n\ntable 6.17 data for problem 6.7\n\npenicillin\nlevel\n\nresponse\n\ndelay\n\ncured\n\ndied\n\n1\n8\n\n1\n4\n\n1\n2\n\n1\n\n4\n\nnone\n11 h\n2\nnone\n11 h\n2\nnone\n11 h\n2\nnone\n11 h\n2\nnone\n11 h\n2\n\n0\n0\n3\n0\n6\n2\n5\n6\n2\n5\n\n6\n5\n3\n6\n0\n4\n1\n0\n0\n0\n\n.\nsource: reprinted with permission from mantel 1963 .\n\n\u017e\n\nc. test xy conditional\n\nindependence using the cochran\u1390mantel\u1390\n\nhaenszel test. interpret.\n\nd. estimate the xy conditional odds ratio using i ml with the logit\n\n\u017e .\nmodel, and ii\n\nthe mantel\u1390haenszel estimate. interpret.\n\n\u017e .\n\ne. the small cell counts make large-sample analyses questionnable.\n\nconduct small-sample inference, and interpret.\n\n6.8 refer to table 2.6. use the cmh statistic to test independence of\ndeath penalty verdict and victim\u2019s race, controlling for defendant\u2019s\nrace. show another test of this hypothesis, and compare results.\n\n6.9 treatments a and b were compared on a binary response for 40 pairs\nof subjects matched on relevant covariates. for each pair, treatments\nwere assigned to the subjects randomly. twenty pairs of subjects made\nthe same response for each treatment. six pairs had a success for the\nsubject receiving a and a failure for the subject receiving b, whereas\nthe other 14 pairs had a success for b and a failure for a. use the\ncochran\u1390mantel\u1390haenszel procedure to test\nindependence of re-\nsponse and treatment. in section 10.1 we present an equivalent test,\nmcnemar\u2019s test.\n\n.\n\n\u017e\n\n6.10 refer to section 6.5.1. suppose that \u2432 s 0.7 and \u2432 s 0.6. what\nsample size is needed for the test to have approximate power 0.80,\nwhen \u2423s 0.05, for a h : \u2432 / \u2432 , and b h : \u2432 ) \u2432 ?\n\n\u017e .\n\n\u017e .\n\n1\n\n2\n\na\n\n1\n\n2\n\na\n\n1\n\n2\n\n "}, {"Page_number": 276, "text": "261\nproblems\n6.11 refer to section 6.5.1. suppose that \u2432 s 0.63 and \u2432 s 0.57. when\ntreatment sample sizes are equal, explain why the joint probabilities in\nthe 2 = 2 table are 0.315 and 0.185 in the row for treatment a and\n0.285 and 0.215 in the row for treatment b. for the model of indepen-\ndence, explain why the fitted joint probabilities are 0.30 for success\nand 0.20 for failure, in each row. show that x 2 has noncentrality\nparameter 0.00375n and df s 1. for n s 200 and \u2423s 0.05, find the\npower.\n\n1\n\n2\n\n6.12 in an experiment designed to compare two treatments on a three-cate-\ngory response, a researcher expects the conditional distributions to be\n.\napproximately 0.2, 0.2, 0.6 and 0.3, 0.3, 0.4 .\na. with \u2423s 0.05, find the approximate power using i x , and ii\n\u017e .\nto compare the distributions with 100 observations for each\n\n\u017e .\n\n\u017e\n\n.\n\n\u017e\n\n2\n\ng2\ntreatment. compare results.\n\nb. what sample size is needed for each treatment for the tests in part\n\n\u017e .a to have approximate power 0.90?\n\n6.13 the horseshoe crab width values in table 4.3 have x s 26.3 and\ns s 2.1. if the true relationship were similar to the fitted equation in\nsection 5.1.3, about how large a sample yields p type ii error s 0.10,\nwith \u2423s 0.05, for testing h : \u2424s 0 against h : \u2424) 0?\n\n\u017e\n\n.\n\nx\n\n0\n\na\n\n6.14 refer to problem 5.1. table 6.18 shows output for fitting a probit\nmodel. interpret the parameter estimates a using characteristics of\nthe normal cdf response curve, b finding the estimated rate of change\nin the probability of remission where it equals 0.5, and c finding the\ndifference between the estimated probabilities of remission at the\nupper and lower quartiles of the labeling index, 14 and 28.\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\ntable 6.18 data for problem 6.14\n\nstandard likelihood ratio 95% chi-\n\nparameter\nestimate\nintercept y2.3178\n0.0878\nli\n\nerror\n0.7795\n0.0328\n\nconfidence limits\ny4.0114\ny0.9084\n0.1575\n0.0275\n\nsquare\n\n8.84\n7.19\n\npr ) chisq\n\n0.0029\n0.0073\n\n6.15 use probit models to describe the effects of width and color on the\n\nprobability of a satellite for table 4.3. interpret.\n\n6.16 refer to table 6.14. fit the model having log-log link rather than\n\ncomplementary log-log. test the fit. why does it fit so poorly?\n\n "}, {"Page_number": 277, "text": "262\n\nbuilding and applying logistic regression models\n\n.\n6.17 for the linear logit model with table 3.9 and scores 0, 15, 30 ,\nconduct the exact test of h : \u2424s 0 and find a point and interval\nestimate of \u2424 using the conditional likelihood. interpret.\n\n\u017e\n\n0\n\n6.18 refer to table 6.16. apply conditional logistic regression to the model\n\ndiscussed in section 6.7.8.\na. obtain an exact p-value for testing no c effect against the alterna-\ntive of a positive effect. construct a 95% confidence interval for the\nconditional cd odds ratio.\n\n.\n\n\u017e\n\nb. construct the partial tables relating c to d for the combinations of\nlevels of a, l . note that three tables have no data when c s 1.\nfor the sole partial table having data at both c levels, find a 95%\nexact confidence interval for the odds ratio and find an exact\none-sided p-value. compare to results using the entire data set.\ncomment about the contribution to inference of tables having only\na single positive row total or a single positive column total.\n\nc. obtain the ordinary ml fit of the logistic regression model. to\ninvestigate the sensitivity of the estimated c effect, find the change\nin the estimate and se after adding one observation to the data set,\na case with no diarrhea when c, a, l s 1, 1, 1 .\n.\n\n\u017e\n\n.\n\n\u017e\n\n6.19 consider table 6.19, from a study of nonmetastatic osteosarcoma\n\u017ea. m. goorin, j. clin oncol. 5: 1178\u13901184, 1987, and the manual\nfor logxact . the response is whether the subject achieved a three-year\ndisease-free interval.\na. show that each predictor has a significant effect when used individ-\n\n.\n\nually without the others.\n\nb. try to fit a main-effects logistic regression model containing all\nthree predictors. explain why the ml estimate for the effect of\nlymphocytic infiltration is infinite.\n\ntable 6.19 data for problem 6.19\n\nlymphocytic\ninfiltration\nhigh\n\nlow\n\ngender\nfemale\n\nmale\n\nfemale\n\nmale\n\nosteoblastic\npathology\n\nno\nyes\nno\nyes\nno\nyes\nno\nyes\n\ndisease-free\nno\nyes\n0\n3\n0\n2\n4\n0\n0\n1\n0\n5\n2\n3\n4\n5\n6\n11\n\n.\nsource: logxact 4 for windows cambridge, ma: cytel software, 1999 .\n\n\u017e\n\n "}, {"Page_number": 278, "text": "problems\n\n263\n\nc. using conditional logistic regression,\n\n\u017e .\ni conduct an exact test for\nthe effect of lymphocytic infiltration, controlling for the other\n\u017e .\nvariables; and ii\nfind a 95% confidence interval for the effect.\ninterpret results.\n\n6.20 use the methods discussed in this chapter to select a model for table\n\n5.5.\n\n6.21 logistic regression is applied increasingly to large financial databases,\nsuch as for credit scoring to model the influence of predictors on\nwhether a consumer is creditworthy. the data archive found under the\nindex at www.stat.uni-muenchen.de contains such a data set that in-\ncludes 20 covariates for 1000 observations. build a model for credit-\nworthiness using the predictors running account, duration of credit,\npayment of previous credits, intended use, gender, and marital status.\n\ntheory and methods\n\n6.22 for a sequence of s nested models m , . . . , m , model m is the most\ncomplex. let \u242f denote the difference in residual df between m and\nm .s\na. explain why for j - k, g m m f g m m .\n.\nb. assume model m , so that m also holds when k ) j. for all k ) j,\n\nas n \u2122 \u2b01, p g m m ) \u2439 \u2423 f \u2423. explain why.\n\n2\u017e .x\n\n2\u017e\n\n2\u017e\n\n2\u017e\n\n.\n\n.\n\nw\n\n1\n\n1\n\nk\n\nk\n\ns\n\ns\n\ns\n\nj\n\nj\n\nj\n\n<\n\n<\n\n<\n\n\u242f\n\nj\n\nk\n\n\u017e\n\n.\n\n2\n\nc. gabriel 1966 suggested a simultaneous testing procedure in which,\nfor each pair of models, the critical value for differences between\ng values is \u2439 \u2423 . the final model accepted must be more\ncomplex than any model rejected in a pairwise comparison. since\npart b is true for all j - k, argue that gabriel\u2019s procedure has type\ni error probability no greater than \u2423.\n\n2\u017e .\n\u242f\n\n\u017e .\n\n6.23 prove that the pearson residuals for the linear logit model applied to a\ne2. note that this holds for a\ni\n\ni = 2 contingency table satisfy x 2 s \u00fdi\nbinomial glm with any link.\n\nis1\n\n6.24 refer to logit model 6.4 for a 2 = 2 = k contingency table n\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n.\n\ni jk\n\na. using dummy variables, write the log-likelihood function. identify\nthe sufficient statistics for the various parameters. explain how to\nconduct exact conditional inference about the effect of x, controlling\nfor z.\nb. using a basic result for testing in exponential families, explain why\nuniformly most powerful unbiased tests of conditional xy indepen-\ndence are based on \u00fd n\n\n\u017e\n.\nbirch 1964b; lehmann 1986, sec. 4.8 .\n\nk\n\n11 k\n\n "}, {"Page_number": 279, "text": "4\n\n\u0004\n\n264\nbuilding and applying logistic regression models\n6.25 suppose that \u2432 in a 2 = 2 = 2 table are, by row, 0.15, 0.10 r 0.10,\n0.15 when z s 1 and 0.10, 0.15 r 0.15, 0.10 when z s 2. for testing\nconditional xy independence with logit models having y as a re-\nsponse, explain why the likelihood-ratio test comparing models x q z\nand z is not consistent but the likelihood-ratio test of fit of the xy\nconditional independence model is.\n\ni jk\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n2.\n\ni\n\ni\n\ni\n\ni\n\ni\n\n1\n\n\u0004\n\n4\n\n.\n\n\u017e\n\n.\n\n6.26 refer to section 6.4.1. when y is n \u242e,\u2434 , consider the comparison\nof \u242e , . . . , \u242e based on independent samples at the i categories of x.\nwhen approximately \u242e s \u2423q \u2424x , explain why the t or f test of h :\n0\n\u2424s 0 is more powerful than the one-way anova f test. describe a\npattern for \u242e for which the anova test would be more powerful.\n6.27 for a multinomial distribution, let \u2425s \u00fd b \u2432 , and suppose that \u2432 s\np , let s s \u00fd b p .\n4\n\n\u017e\nf \u242a ) 0,\ni\nlet t s \u00fd b \u2432 , where \u2432 s f \u242a , for the ml estimator \u242a of \u242a.\na. show that var s s \u00fd b \u2432 y \u00fd b \u2432 rn.\n.x\nb. using the delta method, show var t f var \u242a \u00fd b f \u242a .\nc. by computing the information for l \u242a s \u00fd n log f \u242a , show that\n\ni s 1, . . . , i. for sample proportions\n\n.xw\n\u02c6\n\n.2x\n\ni\n.x\nvar \u242a is approximately n\u00fd f \u242a rf \u242a\ny1\n\u017e\n.x\n\nd. asymptotically, show that var n t y \u2425 f var n s y \u2425 . hint:\nshow that var t rvar s is a squared correlation between two\nrandom variables, where with probability \u2432 the first equals b and\n. x\nthe second equals f \u242a rf \u242a .\n\n\u017e\n'\nw\n\n.x w\n\n\u017e\n..\n2\n\n\u017e\n.x\n\n\u02c6\nw\n\n'\nw\n\n\u02c6\n.\n\n\u02c6\n.\n\n\u02c6\n\n\u02c6\n\ni\n\u017e\n\ni\nw\n\n2\ni\n\nx\ni\n\nx\ni\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\nw\n\nw\n\n\u0004\n\n.\n\n2\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nx\u017e\ni\n\ni\n\ni\n\n6.28 a threshold model can also motivate the probit model. for it, there is\nan unobserved continuous response y * such that the observed y s 0\nif y * f \u2436 and y s 1 if y * ) \u2436. suppose that y * s \u242e q \u2440, where\n\u242e s \u2423q \u2424x and where \u2440 are independent from a n 0, \u2434 distri-\nbution. for identifiability one can set \u2434s 1 and the threshold \u2436s 0.\nshow that the probit model holds and explain why \u2424 represents the\nexpected number of standard deviation change in y * for a 1-unit\nincrease in x.\n\ni\n2.\n\ni\n\u017e\n\n\u0004\n\n4\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n0\n\n6.29 consider the choice between two options, such as two product brands.\nlet u denote the utility of outcome y s 0 and u the utility of y s 1.\nfor y s 0 and 1, suppose that u s \u2423 q \u2424 x q \u2440 , using a scale such\nthat \u2440 has some standardized distribution. a subject selects y s 1 if\nu ) u for that subject.\na. if \u2440 and \u2440 are independent n 0, 1 random variables, show that\n\n\u017e\np y s 1 satisfies the probit model.\n\u017e\n\n.\n\n.\n\n1\n\n0\n\n0\n\n1\n\n1\n\ny\n\ny\n\ny\n\ny\n\ny\n\nb. if \u2440 are independent extreme-value random variables, with cdf\nf \u2440 s exp yexp y\u2440 , show that p y s 1 satisfies the logistic\n\u017e\n.\nregression model maddala 1983, p. 60; mcfadden 1974 .\n\n\u017e\n\u017e\n\n.x\n\ny\n.\n\n\u017e\n\n.\n\nw\n\n "}, {"Page_number": 280, "text": "problems\n\n265\n\n\u017e\n\n.\n\u017e\n\n6.30 consider model 6.12 with complementary log-log link.\n\n1\n\n.\n\na. find x at which \u2432 x s .2\nb. show the greatest rate of change of \u2432 x occurs at x s y\u2423r\u2424.\nwhat does \u2432 x equal at that point? give the corresponding result\nfor the model with log-log link, and compare to the logit and probit\nmodels.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n6.31 suppose that log-log model 6.13 holds. explain how to interpret \u2424.\n6.32 let y , i s 1, . . . , n, denote n independent binary random variables.\n\na. derive the log likelihood for the probit model \u233d \u2432 x s \u00fd \u2424 x .\ni j\nb. show that\nthe likelihood equations for the logistic and probit\n\ny1w\n\n.x\n\n\u017e\n\ni\n\ni\n\nj\n\nj\n\nregression models are\n\u00fd i\n\n\u017e\n\ni\n\ny y \u2432 z x s 0,\n\n.\u02c6\n\ni j\n\ni\n\ni\n\nj s 0, . . . , p,\n\nwhere z s 1 for the logistic case and z s \u243e \u00fd \u2424 x r\u2432 1 y \u2432\n.\n\u02c6\ni\nfor the probit case. when the link is not canonical, there is no\nreduction of the data in sufficient statistics.\n\n\u02c6\nj\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni j\n\ni\n\ni\n\ni\n\nj\n\n\u017e\n\n.\n\n\u017e\n\n6.33 sometimes, sample proportions are continuous rather than of the\nbinomial form number of successes r number of trials . each observa-\ntion is any real number between 0 and 1, such as the proportion of a\ntooth surface that is covered with plaque. for independent responses\n4\n\u0004\ny , aitchison and shen 1980 and bartlett 1937 modeled logit y ;\ni\n\u017e\nn \u2424, \u2434 . then y itself is said to have a logistic-normal distribution.\na. expressing a n \u2424, \u2434 variate as \u2424q \u2434z, where z is standard\n\n.x\nnormal, show that y s exp \u2424 q \u2434z r 1 q exp \u2424 q \u2434z .\n\n2.\n\n2.\n\ni\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nw\n\ni\n\ni\n\ni\n\ni\n\ni\n\nb. show that for small \u2434,\n\ny s\n\ni\n\ne \u2424i\n1 q e\n\n\u2424\ni\n\nq\n\ne \u2424i\n1 q e\n\n\u2424\ni\n\n1\n1 q e\n\n\u2424\ni\n\n\u2434z q\n\n\u017e\n\ne \u2424i 1 y e \u2424i\n2 1 q e\n\u017e\n.\n\n\u2424\ni\n\n.\n3\n\n\u2434 z q \u2b48\u2b48\u2b48 .\n\n2\n\n2\n\nc. letting \u242e s e r 1 q e\n\n\u2424i\n\n\u017e\n\ni\n\n\u2424i.\n\n, when \u2434 is close to 0 show that\n\ne y f \u242e ,\n\u017e\n\n.\n\ni\n\ni\n\nvar y f \u242e 1 y \u242e \u2434 .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2\n\n2\n\ni\n\ni\n\ni\n\nd. for independent continuous proportions\n\ny , let \u242e s e y . for a\n4\nglm, it is sensible to use an inverse cdf link for \u242e, but it is unclear\nhow to choose a distribution for y . the approximate moments for\nthe logistic-normal motivate a quasi-likelihood approach wedder-\nburn 1974 with variance function \u00ae \u242e s \u243e \u242e 1 y \u242e for un-\n\n\u017e\n.x2\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\nw\n\n\u0004\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n "}, {"Page_number": 281, "text": "266\n\nbuilding and applying logistic regression models\n\nknown \u243e. explain why this provides similar results as fitting a\nnormal regression model to the sample logits assuming constant\nvariance. the ql approach has the advantage of not requiring\nadjustment of 0 or 1 observations, for which sample logits don\u2019t\nexist.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ne. wedderburn 1974 gave an example with response the proportion\nof a leaf showing a type of blotch. envision an approximation of\nbinomial form based on cutting each leaf into a large number of\nsmall regions of the same size and observing for each region\nwhether it is mostly covered with blotch. explain why this suggests\nthat \u00ae \u242e s \u243e\u242e 1 y \u242e . what violation of the binomial assump-\ntions might make this questionnable? the parametric family of\nbeta distributions has variance function of this form see section\n13.3.1 . barndorff-nielsen and jorgensen 1991 proposed a distri-\n. x\nbution having \u00ae \u242e s \u243e \u242e 1 y \u242e ; see also cox 1996 .\n\n.x3\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nw\n\nw\n\n\u2c91\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n6.34 for independent binomial sampling, construct the log likelihood and\nidentify the sufficient statistics to be conditioned out to perform exact\n.\ninference about \u2424 in model 6.4 .\n\n\u017e\n\n6.35 let \u2432 s \u2432 , . . . , \u2432\n\u02c6\n\n\u017ey1.\n\n\u017ey.\n\n\u02c6\n\n\u017e\n\n\u017eyn ..\n\n\u017eyi .\n\ni\n\n\u02c6\n\n\u02c6\n, where \u2432\n\ndenotes the estimate of\n\u017e\n.e y for binary observation i after fitting the model without that\nobservation. cross-validation declares a model to have good predictive\npower if corr \u2432 , y is high. consider the model logit \u2432 s \u2423 for all\n\u017e\ni. show that \u2432 s y and hence \u2432 s nr n y 1\ny y 1rn y , and\nhence corr \u2432 , y s y1 regardless of how well the model fits. thus,\ncross-validation can be misleading with binary data zheng and agresti\n.\n2000 .\n\n\u017e\n\u02c6\n\u02c6\ni\n\u017ey.\n\n.\ni\n\u017e\n\n\u017eyi .\n\n.xw\n\n\u017ey.\n\n\u02c6\n\n\u02c6\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nw\n\nx\n\ni\n\n "}, {"Page_number": 282, "text": "c h a p t e r 7\n\nlogit models for multinomial\nresponses\n\n\u017e\n\nin chapters 5 and 6 we discussed modeling binary response variables with\nbinomial glms. multicategory responses use multinomial glms. in this\nchapter we generalize logistic regression for multinomial nominal and ordi-\n.nal response variables.\n\nin section 7.1 we present a model for nominal responses that uses a\nseparate binary logit model for each pair of response categories. in section\n7.2 we present a model for ordinal responses that uses logits of cumulative\nresponse probabilities. in section 7.3 we use other link functions for those\ncumulative probabilities. section 7.4 covers alternative ordinal-response mod-\nels.\n\nin section 7.5 we discuss tests of conditional independence with multino-\nmial responses using models and using generalizations of the cochran\u1390\nmantel\u1390haenszel statistic. in the final section we introduce a multinomial\nlogit model for discrete-choice modeling of a subject\u2019s choice from one of\nseveral options when values of predictors may depend on the option.\n\n7.1 nominal responses: baseline-category\nlogit models\n\n.\n\nlet y be a categorical response with j categories. multicategory also called\nlogit models for nominal response variables simultaneously de-\npolytomous\npairs of categories. given a certain choice of j y 1\nscribe log odds for all\nof these, the rest are redundant.\n\n/2\n\n\u017e\n\nj\n\n\u017e\n\n7.1.1 baseline-category logits\nlet \u2432 x s p y s j x at a fixed setting x for explanatory variables, with\n\u00fd \u2432 x s 1. for observations at that setting, we treat the counts at the j\n\u017e .4\ncategories of y as multinomial with probabilities \u2432 x , . . . , \u2432 x .\n\nj\u017e .\n\n\u017e .\n\n.\n\n\u017e\n\n\u0004\n\nj\n\nj\n\n<\n\n\u017e .\n1\n\nj\n\n267\n\n "}, {"Page_number": 283, "text": "268\n\nlogit models for multinomial responses\n\nlogit models pair each response category with a baseline category, often\n\nthe last one or the most common one. the model\n\nlog\n\nj\n\n\u2432 x\u017e .\n\u2432 x\u017e .\n\nj\n\ns \u2423 q \u2424 x,\n\nx\nj\n\nj\n\nj s 1, . . . , j y 1,\n\n7.1\u017e\n\n.\n\nsimultaneously describes the effects of x on these j y 1 logits. the effects\nvary according to the response paired with the baseline. these j y 1 equa-\ntions determine parameters for logits with other pairs of response categories,\nsince\n\nlog\n\na\n\n\u017e .\n\u2432 x\n\u017e .\n\u2432 x\n\nb\n\ns log\n\na\n\n\u017e .\n\u2432 x\n\u017e .\n\u2432 x\n\nj\n\ny log\n\nb\n\n\u017e .\n\u2432 x\n\u017e .\n\u2432 x\n\nj\n\n.\n\nwith categorical predictors, x 2 and g2 goodness-of-fit statistics provide a\nmodel check when data are not sparse. when an explanatory variable is\ncontinuous or the data are sparse, such statistics are still valid for comparing\nnested models differing by relatively few terms haberman 1974a, pp.\n.\n372\u1390373; 1977a .\n\n\u017e\n\n7.1.2 alligator food choice example\ntable 7.1 is from a study of factors influencing the primary food choice of\nalligators. it used 219 alligators captured in four florida lakes. the nominal\nresponse variable is the primary food type, in volume, found in an alligator\u2019s\nstomach. this had five categories: fish, invertebrate, reptile, bird, other. the\ninvertebrates included apple snails, aquatic insects, and crayfish. the reptiles\nwere primarily turtles, although one stomach contained the tags of 23 baby\nalligators released in the lake the previous year! the \u2018\u2018other\u2019\u2019 category\nconsisted of amphibian, mammal, plant material, stones or other debris, or\nno food or dominant type. table 7.1 also classifies the alligators according to\nl s lake of capture hancock, oklawaha, trafford, george , g s gender\nmale, female , and s s size f 2.3 meters long, ) 2.3 meters long .\n.\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e .\n\nbaseline-category logit models can investigate the effects of l, g, and s\non primary food type. table 7.2 contains fit statistics for several models. we\ndenote a model by its predictors: for instance, l q s having additive lake\nand size effects and\nhaving no predictors. the data are sparse, 219\nobservations scattered among 80 cells. thus, g2 is more reliable for compar-\ng s 2.1 and g s\ning models than for testing fit. the statistics g\nw\u017e\nl q s g q l q s s 2.2, each based on df s 4, suggest simplifying by\ncollapsing the table over gender. other analyses, not presented here, show\n.\nthat adding interaction terms including g do not improve the fit significantly.\nthe g2 and x 2 values for the collapsed table indicate that both l and s\nhave effects. table 7.3 exhibits fitted values for model l q s for the\n\n2w\u017e . < \u017e\n\n. < \u017e\n\n.x\n\n.x\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n2\n\n "}, {"Page_number": 284, "text": "nominal responses: baseline-category logit models\n\n269\n\ntable 7.1 primary food choice of alligators\n\noklawaha male\n\nlake\nhancock\n\ntrafford\n\ngeorge\n\nsize\n\u017e\n.\nm\ngender\nf 2.3\nmale\n) 2.3\nfemale f 2.3\n) 2.3\nf 2.3\n) 2.3\nfemale f 2.3\n) 2.3\nf 2.3\nmale\n) 2.3\nfemale f 2.3\n) 2.3\nf 2.3\nmale\n) 2.3\nfemale f 2.3\n) 2.3\n\nprimary food choice\n\ninvertebrate\n\nreptile\n\nbird other\n\n1\n0\n3\n0\n2\n7\n9\n1\n7\n6\n4\n1\n10\n0\n9\n1\n\n0\n0\n2\n1\n0\n6\n1\n0\n1\n6\n1\n0\n0\n0\n1\n0\n\n0\n1\n2\n2\n0\n0\n0\n1\n0\n3\n1\n0\n2\n1\n0\n0\n\n5\n2\n3\n3\n1\n0\n2\n0\n1\n5\n4\n0\n2\n2\n1\n1\n\nfish\n7\n4\n16\n3\n2\n13\n3\n0\n3\n8\n2\n0\n13\n9\n3\n8\n\nsource: data courtesy of clint moore, from an unpublished manuscript by m. f. delaney and c.\nt. moore.\n\ntable 7.2 goodness of fit of baseline-category\nlogit models for table 7.1\n\na\n\nmodel\n.\n\u017e\n.g\n\u017e\n.s\n\u017e\n.l\n\u017e\nl q s\n\u017e\ng q l q s\n\u017e\ncollapsed over g\n\n.\n\n.\n\n\u017e\n\u017e\n\u017e\n\u017e\n\n.\n.s\n.l\nl q s\n\n.\n\n2\n\ng\n\n116.8\n114.7\n101.6\n73.6\n52.5\n50.3\n\n81.4\n66.2\n38.2\n17.1\n\n2\n\nx\n106.5\n101.2\n86.9\n79.6\n58.0\n52.6\n\n73.1\n54.3\n32.7\n15.0\n\ndf\n60\n56\n56\n48\n44\n40\n\n28\n24\n16\n12\n\nag, gender; s, size; l,\ndetails.\n\nlake of capture. see the text for\n\n "}, {"Page_number": 285, "text": "270\n\nlogit models for multinomial responses\n\ntable 7.3 observed and fitted values for study of alligator\u2019s primary food choice\n\nlake\nhancock\n\noklawaha\n\ntrafford\n\ngeorge\n\nsize of alligator\n\n.\n\n\u017e\nmeters\nf 2.3\n\n) 2.3\n\nf 2.3\n\n) 2.3\n\nf 2.3\n\n) 2.3\n\nf 2.3\n\n) 2.3\n\nprimary food choice\n\ninvertebrate\n\nreptile\n\n4\n\u017e\n3.6\n0\n\u017e\n0.4\n11\n\u017e\n12.0\n8\n\u017e\n7.0\n11\n\u017e\n12.4\n7\n\u017e\n5.6\n19\n\u017e\n16.9\n1\n\u017e\n3.1\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n2\n\u017e\n1.9\n1\n\u017e\n1.1\n1\n\u017e\n1.5\n6\n\u017e\n5.5\n2\n\u017e\n2.1\n6\n\u017e\n5.9\n1\n\u017e\n0.5\n0\n\u017e\n0.5\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nbird other\n2\n\u017e\n2.7\n3\n\u017e\n2.3\n0\n\u017e\n0.2\n1\n\u017e\n0.8\n1\n\u017e\n0.9\n3\n\u017e\n3.1\n2\n\u017e\n1.2\n1\n\u017e\n1.8\n\n8\n\u017e\n9.9\n5\n\u017e\n3.1\n3\n\u017e\n1.1\n0\n\u017e\n1.9\n5\n\u017e\n4.2\n5\n\u017e\n5.8\n3\n\u017e\n3.8\n3\n\u017e\n2.2\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nfish\n23\n\u017e\n20.9\n7\n\u017e\n9.1\n5\n\u017e\n5.2\n13\n\u017e\n12.8\n5\n\u017e\n4.4\n89\n\u017e\n8.6\n16\n\u017e\n18.5\n17\n\u017e\n14.5\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\ncollapsed table. absolute values of standardized pearson residuals comparing\nobserved and fitted values exceed 2 in only two of the 40 cells and exceed 3 in\nnone of the cells. the fit seems adequate.\n\nfish was the most common food choice. we now estimate the effects of\nlake and size on the odds that alligators select other primary food types\ninstead of fish. with fish as the baseline category, table 7.4 contains ml\nestimates of effect parameters. these result from models using dummy\nvariables for the first three lakes and for size. the table uses letter subscripts\nto denote the food choice categories. for example, the prediction equation\nfor the log odds of selecting invertebrates instead of fish is\n\nlog \u2432 r\u2432 s y1.55 q 1.46 s y 1.66 z q 0.94 z q 1.12 z ,\n\n\u02c6 \u02c6\ni\n\nf\n\n.\n\n\u017e\n\nh\n\no\n\nt\n\ntable 7.4 estimated parameters in logit model for alligator food choice,\nbased on dummy variable for first size category and each lake except\nlake george\n\na\n\nlake\n\ni\n\nb\n\nf\n\nr\n\n\u017e\n\u017e\n\u017e\n\u017e\n\nlogit\nintercept\nlog \u2432 r\u2432\ny1.55\nlog \u2432 r\u2432\ny3.31 y0.35 0.58\ny2.09 y0.63 0.64\nlog \u2432 r\u2432\nlog \u2432 r\u2432\ny1.90\n0.33 0.45\nase values in parentheses.\ni, invertebrate; r, reptile; b, bird; o, other; f, fish.\n\nsize f 2.3\noklawaha\nhancock\n1.46 0.40 y1.66 0.61\n.\n0.94 0.47\n.\n1.24 1.19\n2.46 1.12\n0.70 0.78 y0.65 1.20\n.\n.\n0.83 0.56\n0.01 0.78\n\n.\n.\n.\n.\n\n\u017e\n\u017e\n\u017e\n\u017e\n\n\u017e\n\u017e\n\u017e\n\u017e\n\n.\n.\n.\n.\n\n\u017e\n\u017e\n\u017e\n\u017e\n\no\n\nb\n\nf\n\nf\n\nf\n\ntrafford\n.\n1.12 0.49\n.\n2.94 1.12\n.\n1.09 0.84\n.\n1.52 0.62\n\n\u017e\n\u017e\n\u017e\n\u017e\n\n.\n.\n.\n.\n\n "}, {"Page_number": 286, "text": "t\n\nh\n\n\u017e\n\n271\n\nnominal responses: baseline-category logit models\nwhere s s 1 for size f 2.3 meters and 0 otherwise, z\nis a dummy variable\nfor lake hancock z s 1 for alligators in that lake and 0 otherwise , and z\no\nand z\nare dummy variables for lakes oklawaha and trafford. size of\nalligator has a noticeable effect. for a given lake, for small alligators the\nestimated odds that primary food choice was invertebrates instead of fish are\nexp 1.46 s 4.3 times the estimated odds for large alligators; the wald 95%\nis exp 1.46 \" 1.96 0.396 s 2.0, 9.3 . the lake effects\nconfidence interval\nindicate that the estimated odds that the primary food choice was inverte-\nbrates instead of fish are relatively higher at lakes trafford and oklawaha\nand relatively lower at lake hancock than they are at lake george.\n\n.x\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\nh\n\nw\n\nthe equations in table 7.4 determine those for other food-choice pairs.\n\n.\nfor instance, for invertebrate, other ,\n\n\u017e\n\nlog \u2432 r\u2432 s log \u2432 r\u2432 y log \u2432 r\u2432\n\u02c6\n\n\u02c6 \u02c6\ni\n\n\u02c6 \u02c6\ni\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\no\n\no\n\nf\n\nf\n\n.\n\ns y1.55 q 1.46 s y 1.66 z q 0.94 z q 1.12 z\n\n\u017e\n.\ny y1.90 q 0.33s q 0.83 z q 0.01 z q 1.52 z\n\nh\n\no\n\n\u017e\n\nt\n\nh\n\no\n\ns 0.35 q 1.13s y 2.48 z q 0.93 z y 0.39 z .\n\nh\n\no\n\nt\n\n.\n\nt\n\n7.1.3 estimating response probabilities\nthe equation that expresses multinomial logit models directly in terms of\n\u017e .4\nresponse probabilities \u2432 x\n\nis\n\n\u0004\n\nj\n\n\u2432 x s\n\n\u017e .\n\nj\n\nexp \u2423 q \u2424x x\n\u017e\njy1\nhs1\n\n1 q \u00fd exp \u2423 q \u2424 x\n\nx\nh\n\n.\n\n\u017e\n\nh\n\nj\n\nj\n\n.\n\n\u017e\n\n7.2\n\n.\n\nj\n\nj\n\nwith \u2423 s 0 and \u2424 s 0. this follows from 7.1 , using the fact that 7.1 also\nholds with j s j by setting \u2423 s 0 and \u2424 s 0. also, the parameters equal\nzero for a baseline category for identifiability reasons; see problem 7.26. the\ndenominator of 7.2 is the same for each j. the numerators for various j\nsum to the denominator, so \u00fd \u2432 x s 1. for j s 2, 7.2 simplifies to the\n\u017e\nformula of type 5.1 used for binary logistic regression.\n\n\u017e .\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\nj\n\nj\n\nj\n\nj\n\nfrom table 7.4 the estimated probability that a large alligator in lake\n\nhancock has invertebrates as the primary food choice is\n\n\u2432 s\n\u02c6i\n\n1 q e\n\ny1 .55y1 .66\n\nq e\n\ney1 .55y1 .66\nq e\n\ny3 .31q1 .24\n\ny2 .09q0 .70\n\nq e\n\ny1 .90q0 .83\n\ns 0.023.\n\nthe estimated probabilities for reptile, bird, other, and fish are 0.072, 0.141,\n0.194, and 0.570.\n\nthis example used qualitative predictors. multinomial logit models can\nalso contain quantitative predictors. in this study, the biologists used the size\ndummy variable to distinguish between adult and subadult alligators. how-\never, the alligators\u2019 actual length was measured and is quantitative. with\nquantitative predictors, it is informative to plot the estimated probabilities.\n\n "}, {"Page_number": 287, "text": "272\n\nlogit models for multinomial responses\n\nfigure 7.1 estimated probabilities for primary food choice.\n\nto illustrate,\nfor alligators at one lake, figure 7.1 plots the estimated\nprobabilities that primary food choice is fish, invertebrate, or other which\ncombines the other, bird, and reptile categories as a function of length. with\nmore than two response categories, the probability for a given category need\n.\nnot continuously increase or decrease problem 7.27 .\n\n\u017e\n\n\u017e\n\n.\n\nj\n\n7.1.4 fitting of baseline-category logit models*\nml fitting of multinomial logit models maximizes the likelihood subject to\nsimultaneously satisfying the j y 1 equations that specify the model.\n\u0004\n\u017e .4\u2432 x\nfor i s 1, . . . , n, let y s y , . . . , y\nrepresent the multinomial trial for\ni, where y s 1 when the response is in category j and y s 0\nsubject\notherwise. thus, \u00fd y s 1. let x s x , . . . , x\ndenote explanatory vari-\ni p\nable values for subject i. let \u2424 s \u2424 , . . . ,\u2424 denote parameters for the\n.x\njth logit.\n\n.x\n\nj p\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\ni j\n\ni1\n\nj1\n\ni1\n\ni j\n\ni j\n\ni j\n\ni\n\ni\n\nj\n\nj\n\nsince \u2432 s 1 y \u2432 q \u2b48\u2b48\u2b48 q\u2432\n\n\u017e\n\n1\n\n.\n\njy1\n\nand y s 1 y y q \u2b48\u2b48\u2b48 qy\n\n\u017e\n\ni j\n\ni1\n\n.\n, the\n\ni, jy1\n\nj\n\ncontribution to the log likelihood by subject i is\n\nlog\n\nj\n\n\u0142\njs1\n\n\u017e\n\u2432 x\n\nj\n\ni\n\n.\n\ny i j\n\njy1\ns y log\u2432 x q 1 y y\n\u00fd\njs1\n\n\u017e\n\n.\n\ni j\n\ni\n\nj\n\n\u017e\n\n/\n\ni j\n\nlog 1 y \u2432 x\n\u017e\n\nj\n\njy1\n\u00fd\njs1\n\n.\n\ni\n\ns y log\n\ni j\n\n.\n\ni\n\n\u2432 x\u017e\nj\njy1\njs1\n\n1 y \u00fd \u2432 x\u017e\n\nj\n\n.\n\ni\n\nq log 1 y \u2432 x\n\u017e\n\nj\n\njy1\n\u00fd\njs1\n\n.\n\ni\n\n.\n\njy1\n\u00fd\njs1\njy1\n\u00fd\njs1\n\n "}, {"Page_number": 288, "text": "nominal responses: baseline-category logit models\n\n273\n\nthus, the baseline-category logits are the natural parameters for the multino-\nmial distribution.\nsubstituting \u2423 q \u2424 x\n1r 1 q \u00fd exp \u2423 q \u2424 x\n\nnow assume n independent observations. in the last expression above,\nterm and \u2432 x s\nw\n\nthe logit\nin the second term, the log likelihood is\n\nin the first\n\nx\nj\n\n\u017e\n\n\u017e\n\n.\n\nj\n\ni\n\ni\n\nj\n\nfor\n.x\ni\n\nx\nj\n\njy1\njs1\n\ny \u2423 q \u2424 x y log 1 q exp \u2423 q \u2424 x\n\n\u017e\n\n\u017e\n\ni j\n\n.\n\ni\n\nx\nj\n\nx\nj\n\nj\n\nj\n\n5\n\n.\n\ni\n\nlog\n\nn\n\nj\n\n\u0142 \u0142 j\nis1\n\njs1\n\n\u2432 x\u017e\n\ny i j\n\n.\n\ni\n\nj\n\n\u00bd\n\nn\n\n\u00fd\nis1\n\ns\n\ns\n\nn\n\njy1\n\u00fd \u00fd\nis1\njs1\njy1\n\u00fd\njs1\n\n\u2423\nj\n\n\u017e\n\njy1\n\u00fd\njs1\n\n/\n\n/\n\ny q \u2424\n\ni j\n\njk\n\np\n\n\u00fd\nks1\n\n\u017e\n\nn\n\n\u00fd\nis1\n\nn\n\n\u00fd\nis1\n\nx\n\nik\n\ny\n\ni j\n\ny log 1 q exp \u2423 q \u2424 x\n\n\u017e\n\nx\nj\n\nj\n\n.\n\ni\n\n.\n\njy1\n\u00fd\njs1\n\nthe sufficient statistic for \u2424 is \u00fd x\ni\nsufficient statistic for \u2423 is \u00fd y s \u00fd x\ni j\nnumber of outcomes in category j.\n\nik\n\njk\n\ni\n\ni\n\nj\n\ny , j s 1, . . . , j y 1, k s 1, . . . , p. the\nfor x s 1; this is the total\n\ni j\n\ny\ni0 i j\n\ni0\n\nthe likelihood equations equate the sufficient statistics to their expected\nvalues. the log likelihood is concave, and the newton\u1390raphson method\nyields the ml parameter estimates. the estimators have large-sample normal\ndistributions. their asymptotic standard errors are square roots of diagonal\nelements of the inverse information matrix.\n\nmost statistical software can fit multinomial logit models, but some can fit\nonly binary logistic regression models. an alternative fitting approach fits\nbinary logit models separately for the j y 1 pairings of responses: model\n.7.1 for j s 1 alone, using only observations in category 1 or j of the\n\u017e\n\u017e\nresponse variable to obtain estimates of \u2423 and \u2424 ; model 7.1 using only\ncategories 2 and j to obtain estimates of \u2423 and \u2424 ;\nin this manner,\n2\nobtaining j y 1 separate fits of logit models. a logit model fitted using data\nfrom only two response categories is the same as a regular logit model fitted\nconditional on classification into one of those categories. for instance, the jth\nbaseline-category logit is a logit of conditional probabilities\n\n.\n\n1\n\n1\n\n2\n\nlog\n\nj\n\n\u2432 x r \u2432 x q \u2432 x\n\u017e .\n\u2432 x r \u2432 x q \u2432 x\n\u017e .\n\n\u017e .\n\u017e .\n\n\u017e .\n\u017e .\n\nj\n\n\u017e\n\u017e\n\nj\n\nj\n\nj\n\nj\n\n.\n.\n\ns log\n\nj\n\n\u017e .\n\u2432 x\n\u017e .\n\u2432 x\n\nj\n\n.\n\nthe separate-fitting estimates differ from the ml estimates for simultane-\nous fitting of the j y 1 logits. they are less efficient, tending to have larger\nstandard errors. however, begg and gray 1984 showed that the efficiency\nloss is minor when the response category having highest prevalence is the\n\n\u017e\n\n.\n\n "}, {"Page_number": 289, "text": "274\n\nlogit models for multinomial responses\n\nh\n\nbaseline. to illustrate this approach, we used the data for the categories\nis log \u2432 r\u2432 s y1.69 q 1.66 s y\ninvertebrate and fish alone. the fit\n1.78 z q 1.05z q 1.22 z , with standard errors 0.43, 0.62, 0.49, 0.52 for\nthe effects. the effects are similar to those from simultaneous fitting with all\nfive response categories\u138fsee the first row of table 7.4. the estimated\nstandard errors are only slightly larger, since 155 of the 219 observations\nwere in the fish or invertebrate categories of food type.\n\n\u02c6 \u02c6i\n\u017e\n\n\u017e\n\n.\n\n.\n\no\n\nt\n\nf\n\ni\n\nx\ni\n\n.\n\n\u017e\n\n7.1.5 multicategory logit model as multivariate glm*\nfor a univariate response variable in the natural exponential family, a glm\nhas form g \u242e s x \u2424 for a link function g, expected response \u242e s e y ,\n.\nvector of values x of p explanatory variables for observation i, and parame-\nter vector \u2424 s \u2424 , . . . , \u2424 . this extends to a multivariate glm for distribu-\ntions in the multivariate exponential family problem 7.24 , such as the\nmultinomial.\nbe a vector response for subject i, with \u242e s e y .\n.\n\u017e\n\nlet y s y , y , . . .\n\n.x\n\n.x\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n1\n\np\n\ni\n\ni\n\ni\n\ni\n\ni1\n\ni2\n\ni\n\ni\n\nlet g be a vector of link functions. the multivariate glm has the form\n\u017e\n\ng \u242e s x \u2424,\n\u017e\n\n.\n\n7.3\n\n.\n\ni\n\ni\n\nwhere row h of the model matrix x for observation i contains values of\nexplanatory variables for y . for details, see fahrmeir and tutz 2001,\n.\nchap. 3 .\nis a multivariate glm. here y s\n..x\n\nis redundant. then, \u242e s \u2432 x , . . . , \u2432 x\n\u017e\n\nthe baseline-category logit model\n\u017e\ny , . . . , y\ni1\nand\n\n, since y\n\ni, jy1\n\njy1\n\n\u017e\n1\n\n.x\n\nih\n\n\u017e\n\n\u017e\n\n.\n\ni j\n\ni\n\ni\n\ni\n\ni\n\ni\n\ng \u242e s log \u242e r 1 y \u242e q \u2b48\u2b48\u2b48 q\u242e\n\u017e\n\n.\n\n\u017e\n\nj\n\ni\n\ni1\n\ni j\n\n\u0004\n\ni , jy1\n\n.\n\n4\n\n.\n\nthe model matrix for observation i is\n\n1\n\nxx\ni\n\n1\n\nx\n\nx i\n\nx s\n\ni\n\n(cid:2)\n\n0\n\n.\n\n\u2b48\u2b48\u2b48\n\n1\n\nx\n\nx i\n\nwith 0 entries in other locations, and \u2424 s \u2423 , \u2424 , . . . , \u2423 , \u2424\n.\n. one can\nalso formulate it for grouped data using sample proportions in the categories.\n\nx\njy1\n\njy1\n\nx\n1\n\n\u017e\n\n1\n\nx\n\n7.2 ordinal responses: cumulative logit models\n\nin section 6.4.1 we showed the benefits of utilizing the ordinality of a\nvariable by focusing inferences on a single parameter. these benefits extend\nto models for ordinal responses. models with terms that reflect ordinal\n\n "}, {"Page_number": 290, "text": "ordinal responses: cumulative logit models\n\n275\n\ncharacteristics such as monotone trend have improved model parsimony and\npower. in this section we introduce the most popular logit model for ordinal\nresponses.\n\n7.2.1 cumulative logits\n\none way to use category ordering forms logits of cumulative probabilities,\n\np y f j x s \u2432 x q \u2b48\u2b48\u2b48 q\u2432 x ,\n\u017e\n\u017e .\n\n\u017e .\n\n.\n\n<\n\n1\n\nj\n\nthe cumulati\u00aee logits are defined as\n\nlogit p y f j x s log\n\n.\n\n\u017e\n\n<\n\ns log\n\np y f j x\n\u017e\n<\n\n.\n1 y p y f j x\n<\n\u2432 x q \u2b48\u2b48\u2b48 q\u2432 x\n\u017e .\n1\n\u2432 x q \u2b48\u2b48\u2b48 q\u2432 x\n\u017e .\njq1\n\n\u017e\n\u017e .\n\u017e .\n\n.\n\nj\n\nj\n\nj s 1, . . . , j.\n\nj s 1, . . . , j y 1.\n\n7.4\u017e\n\n.\n\n,\n\nw\n\na model for logit p y f j\n\neach cumulative logit uses all j response categories.\nalone is an ordinary logit model for a binary\nresponse in which categories 1 to j form one outcome and categories j q 1\nto j form the second. better, models can use all j y 1 cumulative logits in a\nsingle parsimonious model.\n\n.x\n\n\u017e\n\n7.2.2 proportional odds model\na model that simultaneously uses all cumulative logits is\n\nlogit p y f j x s \u2423 q \u2424 x,\n\n\u017e\n\n.\n\nx\n\n<\n\nj\n\nj s 1, . . . , j y 1.\n\n7.5\u017e\n\n.\n\n<\n\n.\n\n\u017e\n\neach cumulative logit has its own intercept. the \u2423 are increasing in j,\nsince p y f j x increases in j for fixed x, and the logit is an increasing\nfunction of this probability.\nthis model has the same effects \u2424 for each logit. for a continuous\npredictor x, figure 7.2 depicts the model when j s 4. for fixed j, the\n\nj\n\n\u0004\n\n4\n\nfigure 7.2 cumulative logit model with effect independent of cutpoint.\n\n "}, {"Page_number": 291, "text": "276\n\nlogit models for multinomial responses\n\nfigure 7.3 category probabilities in cumulative logit model.\n\nresponse curve is a logistic regression curve for a binary response with\noutcomes y f j and y ) j. the response curves for j s 1, 2, and 3 have the\nsame shape. they share exactly the same rate of increase or decrease but are\nhorizontally displaced from each other. for j - k, the curve for p y f k is\nthe curve for p y f j\ntranslated by \u2423 y \u2423 r\u2424 units in the x direction;\nthat is,\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nk\n\nj\n\n.\np y f k x s x s p y f j x s x q \u2423 y \u2423 r\u2424 .\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n<\n\n<\n\nk\n\nj\n\nfigure 7.3 portrays the curves for the category probabilities.\n\nthe cumulative logit model 7.5 satisfies\n\n\u017e\n\n.\n\n<\n\n1\n\n.\n\n\u017e\n\nlogit p y f j x y logit p y f j x\n<\n.\n.\n\np y f j x rp y ) j x\n\u017e\n<\np y f j x rp y ) j x\n\u017e\n<\n\ns log\n\n.\n.\n\n\u017e\n\u017e\n\n\u017e\n\n1\n\n1\n\n<\n\n<\n\n2\n\n2\n\n.\n\n2\n\ns \u2424 x y x\n\n\u017e\n\nx\n\n1\n\n.\n\n.\n\n2\n\nan odds ratio of cumulative probabilities is called a cumulati\u00aee odds ratio.\nthe odds of making response f j at x s x are exp \u2424 x y x\ntimes the\nodds at x s x . the log cumulative odds ratio is proportional to the distance\nbetween x and x . the same proportionality constant applies to each logit.\nbecause of this property, mccullagh 1980 called 7.5 a proportional odds\nmodel.\n\n.x\n\nx\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nw\n\n1\n\n1\n\n2\n\n2\n\n1\n\n2\n\n "}, {"Page_number": 292, "text": "ordinal responses: cumulative logit models\n\n277\n\nfigure 7.4 uniform odds ratios adrbc whenever x y x s 1, for all response cutpoints\nwith proportional odds model.\n\n1\n\n2\n\n2\n\n\u017e\n\nwith a single predictor, the cumulative odds ratio equals e \u2424 whenever\nx y x s 1. figure 7.4 illustrates the constant cumulative odds ratio this\n1\nmodel then implies for all\nj. it shows the j-category response collapsed into\nthe binary outcome f j, ) j and shows the sets of cells that determine\nthe cumulative odds ratio adrbc that takes the same value e \u2424 for each\nsuch collapsing.\nmodel 7.5 constrains the j y 1 response curves to have the same shape.\n.\nthus, its fit is not the same as fitting separate logit models for each j. again\nlet\ni. the\nlikelihood function is\n\nbe binary indicators of the response for subject\n\ny , . . . , y\ni1\n\n.\n\n\u017e\n\n\u017e\n\n.\n\ni j\n\nj\n\nn\n\n\u0142 \u0142\njs1\nis1\n\n.\u2432 x\ni\n\n\u017e\n\nj\n\ny\n\ni j\n\ns\n\ns\n\nj\n\nj\n\nn\n\n\u0142 \u0142\njs1\nis1\n\nn\n\n\u0142 \u0142\njs1\nis1\n\n\u017e\n\n\u017e\n\np y f j x y p y f j y 1 x\n\u017e\n<\n\n\u017e\n\n.\n\n<\n\ni\n\ny\n\ni j\n\n.\n\n.\n\ni\n\nx\n\n\u017e\n\nexp \u2423 q \u2424 x\n\n.\n1 q exp \u2423 q \u2424 x\n\nj\n\u017e\n\ni\nx\n\nj\n\ny\n\n.\n\ni\n\n\u017e\n\nexp \u2423 q \u2424 x\n\n.\n1 q exp \u2423 q \u2424 x\n\njy1\n\u017e\n\ni\nx\n\nx\n\njy1\n\n/\n\n.\n\ni\n\ny\n\ni j\n\n,\n\n7.6\u017e\n\n.\n\nviewed as a function of \u2423 , \u2424 . mccullagh 1980 and walker and duncan\n\u017e\n1967 used fisher scoring algorithms to obtain ml estimates.\n\n.\n\nj\n\n\u017e\u0004\n\n4\n\n.\n\n\u017e\n\n.\n\n7.2.3 latent variable motivation*\na regression model for a continuous variable assumed to underlie y moti-\nvates the common effect \u2424 for different\nj in the proportional odds model\n\u017e\nanderson and philips 1981 . let y * denote this underlying variable. in\nstatistics, such an unobserved variable is called a latent \u00aeariable. suppose that\nit has cdf g y* y \u2429 , where values of y* vary around a location parameter \u2429\nsuch as a mean that depends on x through \u2429 x s \u2424 x. suppose that\n\u017e\ny\u2b01 s \u2423 - \u2423 - \u2b48\u2b48\u2b48 - \u2423 s \u2b01 are cutpoints of the continuous scale such\n\n\u017e .\n\n.\n\n.\n\n.\n\n\u017e\n\nx\n\n0\n\n1\n\nj\n\n "}, {"Page_number": 293, "text": "278\n\nlogit models for multinomial responses\n\nfigure 7.5 ordinal measurement and underlying regression model for a latent variable.\n\nthat the observed response y satisfies\n\ny s j\n\nif \u2423 - y * f \u2423 .\n\njy1\n\nj\n\nthat is, y falls in category j when the latent variable falls in the jth interval\nof values figure 7.5 . then\n\n.\n\n\u017e\n\np y f j x s p y * f \u2423 x s g \u2423 y \u2424 x .\n.\n\u017e\n\n\u017e\n\n.\n\nx\n\n<\n\n<\n\n\u017e\n\n.\n\nj\n\nj\n\n<\n\nx\n\n\u017e\n\n.\n\n.\n\n\u017e\n\ny1\n\nthe appropriate model for y implies that the link gy1, the inverse of the cdf\nfor y *, applies to p y f j x . if y * s \u2424 x q \u2440, where the cdf g of \u2440 is the\nlogistic section 4.2.5 , then g is the logit link and a proportional odds\nmodel results. normality for \u2440 implies a probit link for cumulative probabili-\n.\nties section 7.3.1 .\n\n\u017e\nin this derivation, the same parameters \u2424 occur for the effects on y\nregardless of how the cutpoints \u2423 chop up the scale for the latent variable.\nthe effect parameters are invariant to the choice of categories for y. if a\ncontinuous variable measuring political philosophy has a linear regression\nwith some predictor variables, then the same effect parameters apply to a\ndiscrete version of political philosophy with the categories liberal, moderate,\nconservative or very liberal, slightly liberal, moderate, slightly conservative,\nvery conservative . this feature makes it possible to compare estimates from\nstudies using different response scales.\n\n\u017e\n.\n\n.\n\n\u017e\n\n\u0004\n\n4\n\nj\n\n "}, {"Page_number": 294, "text": "ordinal responses: cumulative logit models\n\n279\nnote that the use of a cdf of form g y* y \u2429 for the latent variable\nresults in linear predictor \u2423 y \u2424x x rather than \u2423 q \u2424x x. when \u2424) 0, as x\nincreases each cumulative logit then decreases, so each cumulative probabil-\nity decreases and relatively less probability mass falls at the low end of the y\nscale. thus, y tends to be larger at higher values of x. with this parameteri-\nzation the sign of \u2424 has the usual meaning. however, most software e.g.,\n.\nsas uses form 7.5 .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nj\n\nj\n\n7.2.4 mental impairment example\ntable 7.5 comes from a study of mental health for a random sample of adult\nresidents of alachua county, florida. it relates mental impairment to two\nexplanatory variables. mental impairment is an ordinal response, with cate-\ngories well, mild symptom formation, moderate symptom formation,\nim-\npaired . the life events index x\nis a composite measure of the number and\nseverity of important life events such as birth of child, new job, divorce, or\ndeath in family that occurred to the subject within the past 3 years. socioeco-\nnomic status\n\nx s ses is measured here as binary 1 s high, 0 s low .\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n1\n\n2\n\ntable 7.5 mental impairment by ses and life events\n\nsubject\n\nmental\n\nimpairment\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\nwell\nwell\nwell\nwell\nwell\nwell\nwell\nwell\nwell\nwell\nwell\nwell\nmild\nmild\nmild\nmild\nmild\nmild\nmild\nmild\n\na0, low; 1, high.\n\nses\nx\n2\n1\n1\n1\n1\n0\n1\n0\n1\n1\n1\n0\n0\n1\n0\n1\n0\n1\n1\n0\n1\n\nlife\nevents\n\na\n\nx\n1\n1\n9\n4\n3\n2\n0\n1\n3\n3\n7\n1\n2\n5\n6\n3\n1\n8\n2\n5\n5\n\nsubject\n\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\nmental\n\nimpairment\nmild\nmild\nmild\nmild\nmoderate\nmoderate\nmoderate\nmoderate\nmoderate\nmoderate\nmoderate\nimpaired\nimpaired\nimpaired\nimpaired\nimpaired\nimpaired\nimpaired\nimpaired\nimpaired\n\nses\nx\n2\n1\n0\n1\n1\n0\n1\n0\n0\n1\n0\n0\n1\n1\n1\n0\n0\n0\n1\n0\n0\n\nlife\nevents\n\na\n\nx\n1\n9\n3\n3\n1\n0\n4\n3\n9\n6\n4\n3\n8\n2\n7\n5\n4\n4\n8\n8\n9\n\n "}, {"Page_number": 295, "text": "280\n\nlogit models for multinomial responses\n\ntable 7.6 output for fitting cumulative logit model to table 7.5\n\nscore test for the proportional odds assumption\n\nchi- square\n\n2.3255\n\ndf\n4\n\npr ) chisq\n\n0.6761\n\nstd\n\nlike. ratio 95%\n\nchi-\n\nparameter\nestimate\nintercept1 y0.2819\n1.2128\nintercept2\n2.2094\nintercept3\ny0.3189\nlife\n1.1112\nses\n\nconf limits\n\nerror\n0.6423 y1.5615\n0.9839\n0.6607 y0.0507\n2.5656\n0.7210\n0.8590\n3.7123\n0.1210 y0.5718 y0.0920\n0.6109 y0.0641\n2.3471\n\nsquare\n\npr > chi sq\n\n0.19\n3.37\n9.39\n6.95\n3.31\n\n0.6607\n0.0664\n0.0022\n0.0084\n0.0689\n\nthe main-effects model of form 7.5 is\n\n\u017e\n\n.\n\nlogit p y f j x s \u2423 q \u2424 x q \u2424 x .\n\n\u017e\n\n.\n\n<\n\n2\n\n2\n\n1 1\n\nj\n\ntable 7.6 shows output. with j s 4 response categories, the model has three\n\u0004\n4\u2423 intercepts. usually, these are not of interest except for computing\nj\nresponse probabilities. the parameter estimates yield estimated logits and\nhence estimates of p y f j , p y ) j , or p y s j . we illustrate for sub-\njects at the mean life events score of x s 4.275 with low ses x s 0 . since\n\u2423 s y0.282, the estimated probability of response well is\n\u02c61\nexp y0.282 y 0.319 4.275\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n1\n\n2\n\n.\n1 q exp y0.282 y 0.319 4.275\n\n\u017e\n\n\u017e\n\n.\n\ns 0.16.\n\np y s 1 s p y f 1 s\n\u02c6\n\u017e\n\n\u02c6\n\u017e\n\n.\n\n.\n\nfigure 7.6 plots p y ) 2 as a function of the life events index, at the two\nlevels of ses.\n\n\u02c6\u017e\n\n.\n\nfigure 7.6 estimated values of p y ) 2 for table 7.5.\n\n\u017e\n\n.\n\n "}, {"Page_number": 296, "text": "\u02c6\n1\n\nordinal responses: cumulative logit models\n\n281\nthe effect estimates \u2424 s y0.319 and \u2424 s 1.111 suggest that the cumu-\nlative probability starting at the well end of the scale decreases as the life\nevents score increases and increases at the higher level of ses. given the life\nevents score, at the high ses level the estimated odds of mental impairment\nbelow any fixed level are e1.111 s 3.0 times the estimated odds at the low\nses level.\n\n\u02c6\n2\n\ndescriptions of effects can compare cumulative probabilities rather than\nuse odds ratios. these can be easier to understand. we describe effects of\nquantitative variables by comparing probabilities at their quartiles. we de-\nscribe effects of qualitative variables by comparing probabilities for different\ncategories. we control for quantitative variables by setting them at their\nmean. we control for qualitative variables by fixing the category, unless there\nare several,\nin which case we can set each at their dummy means. we\nillustrate again with p y s 1 , the well outcome. first, we describe the ses\neffect. at the mean life events of 4.275, p y s 1 s 0.37 at high ses i.e.,\nx s 1 and 0.16 at low ses x s 0 . next, we describe the life events effect.\n2\nthe lower and upper quartiles of the life events score are 2.0 and 6.5. for\nhigh ses, p y s 1 changes from 0.55 to 0.22 between these quartiles; for\nlow ses, it changes from 0.28 to 0.09. note that comparing 0.55 to 0.28 at\nthe lower quartile and 0.22 to 0.09 at the upper quartile provides further\ninformation about the ses effect. the sample effect is substantial for both\npredictors.\n\n\u02c6\u017e\n\n\u02c6\u017e\n\nthe output in table 7.6, taken from sas, also presents a score test of the\nproportional odds property. this tests whether the effects are the same for\neach cumulative logit against the alternative of separate effects. it com-\npares the model with one parameter for x\nto a more\ncomplex model with three parameters for each, allowing different effects\nfor logit p y f 1 ,\nlogit p y f 2 , and logit p y f 3 . here, the score\n.\nstatistic equals 2.33. it has df s 4, since the more complex model has four\nadditional parameters. the more complex model does not fit significantly\nbetter p s 0.68 .\n.\n\nand one for x\nw\n\n.x\n\n.x\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\nw\n\nw\n\n2\n\n1\n\n2\n\n7.2.5 more complex models\nmore complex cumulative logit models are formulated as in ordinary logistic\nregression. they simply require a set of intercept parameters rather than a\nsingle one. in the previous example, for instance, permitting interaction\nyields a model with ml fit\n\nlogit p y f j x s \u2423 y 0.420 x q 0.371 x q 0.181 x x ,\n\n\u02c6\n\u017e\n\n.\n\n<\n\n\u02c6j\n\n1\n\n2\n\n1\n\n2\n\nwhere the coefficient of x x has se s 0.238. the estimated effect of life\nevents on the cumulative logit is y0.420 for the low ses group and\ny0.420 q 0.181 s y0.239 for the high ses group. the impact of life\n\u017e\n\n.\n\n1\n\n2\n\n "}, {"Page_number": 297, "text": "282\n\nlogit models for multinomial responses\n\nj\n\nj\n\nevents seems more severe for the low ses group, but the difference in effects\nis not significant.\n\nmodels in this section used the proportional odds assumption of the same\neffects for different cumulative logits. an advantage is that effects are simple\nto summarize and interpret, requiring only a single parameter for each\npredictor. the models generalize to include separate effects, replacing \u2424 in\n\u017e\n.7.5 by \u2424 . this implies nonparallelism of curves for different logits. how-\never, curves for different cumulative probabilities then cross for some x\nvalues. such models violate the proper order among the cumulative probabil-\nities.\n\neven if such a model fits better over the observed range of x, for reasons\nof parsimony the simple model might be preferable. one case is when effects\n\u02c6\u0004\n4\u2424 with different logits are not substantially different in practical terms.\nthen the significance in a test of proportional odds may reflect primarily a\nlarge value of n. even with smaller n, although effect estimators using the\nsimple model are biased, they may have smaller mse than estimators from a\nmore complex model having many more parameters. so even if a test of\nproportional odds has a small p-value, don\u2019t discard this model automati-\ncally.\n\n\u017e .\n\n\u017e .\n\n.\n\u017e .\n\nif a proportional odds model fits poorly in terms of practical as well as\nstatistical significance, alternative strategies exist. these include 1 trying a\nlink function for which the response curve is nonsymmetric e.g., complemen-\ntary log-log ; 2 adding additional terms, such as interactions, to the linear\npredictor; 3 adding dispersion parameters; 4 permitting separate effects\nfor each logit for some but not all predictors i.e., partial proportional odds;\nand 5 fitting baseline-category logit models and using the ordinality in an\ninformal way in interpreting the associations. for approach 4 , see peterson\nand harrell 1990 , stokes et al. 2000, sec. 15.13 , and criticism by cox\n\u017e\n1995 . in the next section we generalize the cumulative logit model to permit\n\u017e .\nextensions 1 and 3 .\n\n\u017e .\n\u017e\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n7.3 ordinal responses: cumulative link models\ncumulative logit models use the logit link. as in univariate glms, other link\nfunctions are possible. let gy1 denote a link function that is the inverse of\nthe continuous cdf g recall section 4.2.5 . the cumulati\u00aee link model\n\n\u017e\n\n.\n\ny1\n\ng\n\np y f j x s \u2423 q \u2424 x\n\u017e\n\n.\n\nx\n\n<\n\nj\n\n7.7\u017e\n\n.\n\nw\n\ny1\u017e .\n\nlinks the cumulative probabilities to the linear predictor. the logit link\nfunction g u s log ur 1 y u is the inverse of the standard logistic cdf.\nas in the proportional odds model 7.5 , effects of x in 7.7 are assumed\nthe same for each cutpoint, j s 1, . . . , j y 1. in section 7.2.3 we showed that\nthis assumption holds when a linear regression for a latent variable y * has\n\n.x\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 298, "text": "ordinal responses: cumulative link models\n\n283\n\n\u017e\n\n.\n\nstandardized cdf g. model 7.7 results from discrete measurement of y *\nfrom a location-parameter family having cdf g y* y \u2424 x . the parameters\n\u0004\n4\u2423 are category cutpoints on a standardized version of the latent scale. in\nj\nthis sense, cumulative link models are regression models, using a linear\npredictor \u2424x x to describe effects of explanatory variables on crude ordinal\nmeasurement of y *. using y\u2424 rather than q\u2424 in the linear predictor\n\u02c6\nmerely results in change of sign of \u2424. most software e.g., genmod and\nlogistic in sas fits it in q\u2424 form.\n\nx .\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n7.3.1 types of cumulative links\nuse of the standard normal cdf \u233d for g gives the cumulati\u00aee probit model.\nthis generalizes the binary probit model section 6.6 to ordinal responses. it\nis appropriate when the distribution for y * is normal. parameters in probit\nmodels can be interpreted in terms of the latent variable y *. for instance,\nconsider the model \u233d p y f j s \u2423 y \u2424x. from section 7.2.3, since\ny * s \u2424x q \u2440 where \u2440; n 0, 1 has cdf \u233d, \u2424 has the interpretation that a\n1-unit increase in x corresponds to a \u2424 increase in e y * . when \u2440 need not\nbe in standardized form with \u2434s 1, a 1-unit increase in x corresponds to a\n\u2424 standard deviation increase in e y * . cumulative logit models provide fits\nsimilar to those for cumulative probit models, and their parameter interpre-\ntation is simpler.\n\ny1w\n\n.x\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nj\n\nan underlying extreme value distribution for y * implies a model of the\n\nform\n\nlog ylog 1 y p y f j x s \u2423 q \u2424 x .\n\n\u017e\n\n.\n\n\u0004\n\n4\n\nx\n\n<\n\nj\n\nin section 6.6 we introduced this complementary log-log link for binary data.\nthe ordinal model using this link is sometimes called a proportional hazards\nmodel since it results from a generalization of the proportional hazards\nmodel for survival data to handle grouped survival times prentice and\ngloeckler 1978 . it has the property\n\n.\n\n\u017e\n\n<\n\n.\n\np y ) j x s p y ) j x\n\u017e\n<\n\u017e\n\n\u017e\n\n1\n\n.\n\n2\n\nexp \u2424 x yx\n\n\u017e\n\nx\n\nw\n\n1\n\n.x\n\n2\n\n.\n\nwith this link, p y f j approaches 1.0 at a faster rate than it approaches\n0.0. the related log-log link log ylog p y f j\nis appropriate when the\ncomplementary log-log link holds for the categories listed in reverse order.\n\nw \u017e\n\n.x4\n\n.\n\n\u0004\n\n.\n\n\u017e\n\n7.3.2 estimation for cumulative link models\n\u017e\nmccullagh 1980 and thompson and baker 1981 treated cumulative link\nmodels as multivariate glms. mccullagh presented a fisher scoring algo-\nrithm for ml estimation, expressing the likelihood in the form 7.6 using\ncumulative probabilities. mccullagh showed that sufficiently large n guaran-\n.\ntees a unique maximum of the likelihood. burridge 1981 and pratt 1981\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n "}, {"Page_number": 299, "text": "284\n\nlogit models for multinomial responses\n\nshowed that the log likelihood is concave for many cumulative link models,\nincluding the logit, probit, and complementary log-log. iterative algorithms\nusually converge rapidly to the ml estimates.\n\n7.3.3 life table example\ntable 7.7 shows the life-length distribution for u.s. residents in 1981, by race\nand gender. life length uses five ordered categories. the underlying continu-\nous cdf of life length increases slowly at small to moderate ages but increases\nsharply at older ages. this suggests the complementary log-log link. this link\nalso results from assuming that the hazard rate increases exponentially with\n.\nage, which happens for an extreme value distribution the gompertz .\nfor gender g 1 s female; 0 s male , race r 1 s black; 0 s white , and\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\nlife length y, table 7.7 contains fitted distributions for the model\n\nlog ylog 1 y p y f j g s g , r s r s \u2423 q \u2424 g q \u2424 r.\n\n\u017e\n\n.\n\n4\n\n\u0004\n\n<\n\nj\n\n1\n\n2\n\ngoodness-of-fit statistics are irrelevant, since the table contains population\ndistributions. the model describes well the four distributions. its parameter\nvalues are \u2424 s y0.658 and \u2424 s 0.626. the fitted cdf\u2019s satisfy\n\n1\n\np y ) j g s 0, r s r s p y ) j g s 1, r s r\n\u017e\n\n\u017e\n\n<\n\n<\n\n.\n\nexp 0.658\n\n\u017e\n\n.\n\n.\n\n2\n.\n\n.\n\n\u017e\n\ngiven race, the proportion of men living longer than a fixed time equaled the\nproportion for women raised to the exp 0.658 s 1.93 power. given gender,\nthe proportion of blacks living longer than a fixed time equaled the propor-\ntion for whites to the exp 0.626 s 1.87 power. the \u2424 and \u2424 values\nindicate that white men and black women had similar distributions, that\nwhite women tended to have longest lives and black men tended to have\nshortest lives. if the probability of living longer than some fixed time equaled\n\u2432 for white women, that probability was about \u24322 for white men and black\nwomen and \u24324 for black men.\n\n\u017e\n\n.\n\n1\n\n2\n\ntable 7.7 life-length distribution of u.s. residents percent , 1981\n\n) a\n\n(\n\nmales\n\nfemales\n\nwhite\n\u017e\n2.4\n2.4\n\u017e\n3.5\n3.4\n\u017e\n3.8\n4.4\n17.5 16.7\n72.9 73.0\n\n\u017e\n\u017e\n\n.\n.\n.\n.\n.\n\nblack\n\u017e\n4.4\n3.6\n\u017e\n6.4\n7.5\n\u017e\n8.3\n7.7\n25.0 26.1\n55.6 55.4\n\n\u017e\n\u017e\n\n.\n.\n.\n.\n.\n\nwhite\n\u017e\n1.6\n1.2\n\u017e\n1.9\n1.4\n\u017e\n2.2\n2.4\n\u017e\n9.9\n9.6\n84.9 84.9\n\n\u017e\n\n.\n.\n.\n.\n.\n\nblack\n\u017e\n2.3\n2.7\n\u017e\n3.4\n2.9\n\u017e\n4.4\n4.3\n16.3 16.3\n73.7 73.7\n\n\u017e\n\u017e\n\n.\n.\n.\n.\n.\n\nlife length\n0\u139020\n20\u139040\n40\u139050\n50\u139060\nover 65\na\n\nvalues in parentheses are fit of proportional hazards i.e., complementary log-log link model.\nsource: data from statistical abstract of the united states washington, dc: u.s. bureau of the\ncensus, 1984 , p. 69.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n "}, {"Page_number": 300, "text": "ordinal responses: cumulative link models\n\n285\n\n<\n\n2\n\n1\n\n.\n\n.\n\n\u017e\n\nfor all\n\nincorporating dispersion effects*\n\nj or p y f j x g p y f j x\n<\n\n7.3.4\nfor cumulative link models, settings of the explanatory variables are stochas-\ntically ordered on the response: for any pair x and x , either p y f j x f\n\u017e\np y f j x\n\u017e\n<\nj. figure 7.7a\nillustrates for underlying continuous density functions and cdf\u2019s at\ntwo\nsettings of x. when this is violated and such models fit poorly, often it is\nbecause the dispersion also varies with x. for instance, perhaps responses\ntend to concentrate around the same location but more dispersion occurs\nat x than at x . then perhaps p y f j x ) p y f j x\n<\nj but\np y f j x - p y f j x\n\u017e\n<\nfor large j. in other words, at x the responses\nconcentrate more at the extreme categories than at x . figure 7.7b illustrates\nfor underlying continuous distributions.\n\nfor small\n1\n\nfor all\n\n2\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n1\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\n2\n\n<\n\n<\n\n<\n\na cumulative link model that incorporates dispersion effects is\n\ny1\n\ng\n\np y f j x s\n\u017e\n\n.\n\n<\n\nj\n\n\u2423 q \u2424x x\n.\nx\nexp \u2425 x\n\n\u017e\n\n.\n\n7.8\u017e\n\n.\n\n\u017eagain, one can replace q by y to more closely mimic a location\u1390scale\nfamily for an underlying continuous variable. the denominator contains\n\n.\n\nfigure 7.7\nstochastically ordered.\n\n\u017e .\na distribution 1 stochastically higher than distribution 2; b distributions not\n\n\u017e .\n\n "}, {"Page_number": 301, "text": "286\n\nlogit models for multinomial responses\n\n\u017e\n\n.\n\nscale parameters \u2425 that describe the dispersion\u2019s dependence on x. the\nordinary model 7.7 is the special case \u2425 s 0. otherwise, the cumulative\nprobabilities tend to shrink toward each other when \u2425x x ) 0. this creates\nhigher probabilities in the end categories and overall greater dispersion. the\ncumulative probabilities tend to move apart creating less dispersion when\n\u2425x x - 0.\nto illustrate, we use this model to compare two groups on an ordinal\nscale. suppose that x is a dummy variable with x s 1 for the first group.\nwith cumulative logits, model 7.8 is\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\nx s 0,\nlogit p y f j s \u2423 q \u2424 rexp \u2425 ,\n.\n\nlogit p y f j s \u2423 ,\n\u017e\n\n.\n\n\u017e\n\nj\n\n\u017e\n\n.\n\nj\n\nx s 1.\n\nthe case \u2425s 0 is the usual model,\nin which \u2424 is a location shift that\ndetermines a common cumulative log odds ratio for all 2 = 2 collapsings of\nthe 2 = j table. when \u2425/ 0 the difference between the logits for the two\ngroups, and hence the cumulative odds ratio, varies as j does. when \u2425) 0,\nresponses at x s 1 tend to be more disperse than at x s 0. see cox 1995\n.\nand mccullagh 1980 for model fitting and examples.\n\n.\n\n\u017e\n\n\u017e\n\n7.4 alternative models for ordinal responses*\n\nmodels for ordinal responses need not use cumulative probabilities. in this\nsection we discuss alternative logit models and a simpler model that resem-\nbles ordinary regression.\n\n7.4.1 adjacent-categories logits\nthe adjacent-categories logits are\n\nlogit p y s j y s j or\n\n\u017e\n\n<\n\nj q 1 s log\n\n.\n\n\u2432\nj\njq1\n\n\u2432\n\n,\n\nj s 1, . . . , j y 1.\n\n7.9\u017e\n\n.\n\nthese logits are a basic set equivalent to the baseline-category logits. the\nconnections are\n\nlog s log\n\n\u2432\nj\n\u2432\nj\n\n\u2432\nj\njq1\n\n\u2432\n\nq log\n\n\u2432\n\n\u2432\n\njq1\njq2\n\nq \u2b48\u2b48\u2b48 qlog\n\n\u2432\n\njy1\n\u2432\nj\n\n,\n\n\u017e\n\n7.10\n\n.\n\nand\n\nlog\n\n\u2432\nj\njq1\n\n\u2432\n\nj s 1, . . . , j y 1.\n\n,\n\n\u2432\n\njq1\n\u2432\nj\n\n\u2432\nj\n\u2432\nj\n\ns log y log\n/2\n\u017e\n\nj\n\neither set determines logits for all\n\npairs of response categories.\n\n "}, {"Page_number": 302, "text": "alternative models for ordinal responses\n\n287\n\nmodels using adjacent-categories logits can be expressed as baseline-cate-\ngory logit models. for instance, consider the adjacent-categories logit model\n\nlog\n\n\u2432 x\u017e .\nj\n\u2432 x\u017e .\njq1\n\ns \u2423 q \u2424 x,\n\nx\n\nj\n\nj s 1, . . . , j y 1,\n\n\u017e\n\n7.11\n\n.\n\nwith common effect \u2424. from adding j y j\nbaseline-category logit model is\n\n\u017e\n\n.\n\nterms as in 7.10 , the equivalent\n\n\u017e\n\n.\n\nlog\n\nj\n\n\u2432 x\u017e .\n\u2432 x\u017e .\n\nj\n\ns \u2423 q \u2424 j y j x,\n\n\u017e\n\n.\n\nx\n\njy1\n\u00fd k\nksj\n\nj s 1, . . . , j y 1\n\ns \u2423* q \u2424x u ,\n\nj\n\nj\n\nj s 1, . . . , j y 1\n\nj\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nwith u s j y j x. the adjacent-categories logit model corresponds to a\nbaseline-category logit model with adjusted model matrix but also a single\nparameter for each predictor. with some software one can fit model 7.11 by\nfitting the equivalent baseline-category logit model.\n\nthe construction of the adjacent-categories logits recognizes the ordering\nof y categories. to benefit from this in model parsimony requires appropri-\nate specification of the linear predictor. for instance,\nif an explanatory\nvariable has similar effect for each logit, advantages accrue from having a\nj y 1 parameters describing that effect. when\nsingle parameter instead of\nused with this proportional odds form, model 7.11 with adjacent-categories\nlogits fit well in similar situations as model 7.5 with cumulative logits. they\nboth imply stochastically ordered distributions for y at different predictor\nvalues.\n\nthe choice of model should depend less on goodness of fit than on\nwhether one prefers effects to refer to individual response categories, as the\nadjacent-categories logits provide, or instead to groupings of categories using\nthe entire scale or an underlying latent variable, which cumulative logits\nprovide. since effects in cumulative logit models refer to the entire scale,\nthey are usually larger. the ratio of estimate to standard error, however, is\nusually similar for the two model types. an advantage of the cumulative logit\nmodel is the approximate invariance of effect estimates to the choice and\nnumber of response categories. this does not happen with the adjacent-cate-\ngories logits.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\njob satisfaction example\n\n7.4.2\ntable 7.8 refers to the relationship between job satisfaction y and income,\nstratified by gender, for black americans. for simplicity, we use income\nscores 1, 2, 3, 4 . for income x and gender g 1 s females, 0 s males ,\n.\nconsider the model\n\u017e\n\nlog \u2432r\u2432 s \u2423 q \u2424 x q \u2424 g ,\n\nj s 1, 2, 3.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nj\n\n1\n\njq1\n\n2\n\nj\n\n "}, {"Page_number": 303, "text": "288\n\nlogit models for multinomial responses\n\ntable 7.8\n\njob satisfaction and income, controlling for gender\n\njob satisfaction\n\n.\n\nvery\n\ndissatisfied\n\na little\nsatisfied\n\nmoderately\n\nsatisfied\n\nvery\n\nsatisfied\n\nincome\n\u017e\ngender\ndollars\nfemale - 5000\n\nmale\n\n5000\u139015,000\n15,000\u139025,000\n) 25,000\n- 5000\n5000\u139015,000\n15,000\u139025,000\n) 25,000\n\n1\n2\n0\n0\n1\n0\n0\n0\n\n3\n3\n1\n2\n1\n3\n0\n1\n\n11\n17\n8\n4\n2\n5\n7\n9\n\n2\n3\n5\n2\n1\n1\n3\n6\n\nsource:1991, general social survey, national opinion research center.\n\nit describes the odds of being very dissatisfied instead of a little satisfied, a\nlittle instead of moderately satisfied, and moderately instead of very satisfied.\n\nthis model is equivalent to the baseline-category logit model\n\nlog \u2432r\u2432 s \u2423u q \u2424 4 y j x q \u2424 4 y j g ,\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n4\n\n1\n\n2\n\nj\n\nj\n\nj s 1, 2, 3.\n\n4\n\n3\n\n4\n\n2\n\n1\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\u017e\n\n\u02c6\n1\n\n\u02c6\n2\n\n.\n4\n.\n\nthe value of the first predictor in this model is set equal to 3 x in the\nequation for log \u2432 r\u2432 , 2 x in the equation for log \u2432 r\u2432 , and x in the\nequation for log \u2432 r\u2432 . some software e.g., proc catmod in sas; see\ntable a.12 allows one to enter a row of a model matrix for each baseline-\ncategory logit at a given setting of predictors. then, after fitting the\nbaseline-category logit model that constrains the effects to be the same for\neach logit,\nthe estimated regression parameters are the ml estimates\nof parameters for the adjacent-categories logit model. the ml fit gives\n\u2424 s y0.389 se s 0.155 and \u2424 s 0.045 se s 0.314 . for this parameter-\n\u02c6\n1\nization, \u2424 - 0 means the odds of lower job satisfaction decrease as income\nincreases. given gender, the estimated odds of response in the lower of two\nadjacent categories multiplies by exp y0.389 s 0.68 for each category in-\ncrease in income. the model describes 24 logits three for each income =\ngender combination with five parameters. its deviance g s 12.6 with\ndf s 19. this model with a linear trend for the income effect and a lack of\ninteraction between income and gender seems adequate.\nsimilar substantive results occur with a cumulative logit model. its de-\nviance g s 13.3 with df s 19. the income effect is larger \u2424 s y0.51,\nse s 0.20 , since it refers to the entire response scale rather than adjacent\ncategories. however, significance is similar, with \u2424 rse f y2.5 for each\nmodel.\n\n\u02c6\u017e\n1\n\n2\n.\n\n\u02c6\n1\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n2\n\n "}, {"Page_number": 304, "text": "alternative models for ordinal responses\n\n289\n\n7.4.3 continuation-ratio logits\ncontinuation-ratio logits are defined as\n\nlog\n\n\u2432\nj\n\n\u2432 q \u2b48\u2b48\u2b48 q\u2432\nj\n\njq1\n\nj s 1, . . . , j y 1\n\n,\n\n\u017e\n\n7.12\n\n.\n\nor as\n\nlog\n\n\u2432\n\njq1\n\n\u2432 q \u2b48\u2b48\u2b48 q\u2432\nj\n\n1\n\nj s 1, . . . , j y 1.\n\n,\n\n\u017e\n\n7.13\n\n.\n\nthe continuation-ratio logit model form is useful when a sequential mecha-\nnism, such as survival through various age periods, determines the response\noutcome e.g., tutz 1991 . let \u243b s p y s j y g j . with explanatory vari-\nables,\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nj\n\n<\n\n\u243b x s\n\n\u017e .\n\nj\n\n\u2432 x\u017e .\n\nj\n\n\u2432 x q \u2b48\u2b48\u2b48 q\u2432 x\n\u017e .\n\n\u017e .\n\nj\n\nj\n\nj s 1, . . . , j y 1.\n\n,\n\n\u017e\n\n7.14\n\n.\n\nj\n\ni\n\nw\n\n.\n\n\u017e\n\u017e .\nat the ith setting x of x,\n\nthe continuation-ratio logits 7.12 are ordinary logits of these conditional\n\u017e ..x\nprobabilities: namely, log \u243b x r 1 y \u243b x .\nj\nj s 1, . . . , j denote the response\ny ,\ni j\ncounts, with n s \u00fd y . when n s 1, y\nindicates whether the response is\nin category j, as in section 7.1.4. let b n, y; \u243b denote the binomial probabil-\nity of y successes in n trials with parameter \u243b for each trial. by expressing\n\u2b48\u2b48\u2b48\nthe multinomial probability of\n\u017e\n.\n, one can show that the multinomial mass function has\np y\nfactorization\n\n\u017e\nin the form p y\n\ny , . . . , y\ni1\n\ny , . . . , y\ni1\n\n\u017e\nlet\ni\n\n\u017e\np y\n\ni, jy1\n\ny\n\ni2\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\ni j\n\ni j\n\ni1\n\ni1\n\n\u0004\n\n4\n\ni j\n\ni j\n\ni\n\nj\n\n<\n\n<\n\n\u017e\nb n , y ; \u243b x\n\ni1\n\ni\n\n.\n\ni\n\nb n y y , y ; \u243b x\n\u017e\n\ni1\n\ni2\n\n2\n\ni\n\ni\n\nb n y y y \u2b48\u2b48\u2b48 yy\n\ni1\n\ni , jy2\n\n, y\n\ni , jy1\n\n1\n\ni\n\n\u2b48\u2b48\u2b48\n\n.\n\u017e\n; \u243b x\n\njy1\n\n.\n\ni\n\n.\n\n\u017e\n\n7.15\n\n.\n\nj\n\nj\n\ni\n\n.\n\n\u017e\n\nthe full likelihood is the product of multinomial mass functions from the\ndifferent x values. thus, the log likelihood is a sum of terms such that\ndifferent \u243b enter into different terms. when parameters in the model\nspecification for logit \u243b are distinct from those for logit \u243b whenever\nj / k, maximizing each term separately maximizes the full log likelihood.\nthus, separate fitting of models for different continuation-ratio logits gives\nthe same results as simultaneous fitting. the sum of the j y 1 separate g2\nstatistics provides an overall goodness-of-fit statistic pertaining to the simul-\ntaneous fitting of j y 1 models.\n\nbecause these logits refer to a binary response in which one category\ncombines levels of the original scale, separate fitting can use methods for\n.\nbinary logit models. similar remarks apply to continuation-ratio logits 7.13 ,\n\n\u017e\n\n\u017e\n\n.\n\nk\n\n "}, {"Page_number": 305, "text": "290\n\nlogit models for multinomial responses\n\nalthough those logits and the subsequent analysis do not give equivalent\nresults. sometimes, simpler models with the same effects for each logit are\n.\nplausible mccullagh and nelder 1989, p. 164; tutz 1991 .\n\n\u017e\n\n\u017e\n\n7.4.4 developmental toxicity study with pregnant mice\nwe illustrate continuation-ratio logits using table 7.9 from a developmental\ntoxicity study. such experiments with rodents test substances posing potential\n.\ndanger to developing fetuses. diethylene glycol dimethyl ether diegdime ,\nis an industrial solvent used in the manufacture of\none such substance,\nprotective coatings such as lacquer and metal coatings.\n\nthis study administered diegdime in distilled water to pregnant mice.\neach mouse was exposed to one of five concentration levels for 10 days early\nin the pregnancy. the mice exposed to level 0 formed a control group. two\ndays later, the uterine contents of the pregnant mice were examined for\ndefects. each fetus has three possible outcomes nonlive, malformation,\nnormal . the outcomes are ordered, with nonlive the least desirable result.\nwe use continuation-ratio logits to model 1 the probability \u2432 of a nonlive\nfetus, and 2 the conditional probability \u2432 r \u2432 q \u2432 of a malformed fetus,\ngiven that the fetus was live.\n\n\u017e .\n\u017e\n\n\u017e .\n\n.\n\n.\n\n\u017e\n\n1\n\n2\n\n2\n\n3\n\nwe fitted the continuation-ratio logit models\n\nlog\n\n.\n\ni\n\n1\n\n\u017e\n\u2432 x\n.\n\ni\n\n\u2432 x q \u2432 x\n\u017e\n\n\u017e\n\n2\n\n3\n\ns \u2423 q \u2424 x ,\n\n1\n\ni\n\n1\n\n.\n\ni\n\nlog\n\n2\n\n\u017e\n\u2432 x\n\u017e\n\u2432 x\n\n3\n\n.\n.\n\ni\n\ni\n\ns \u2423 q \u2424 x ,\n\n2\n\ni\n\n2\n\n\u0004\n\n4\n\ni\n\n\u017e\n\n\u02c6\n1\n\nusing x scores 0, 62.5, 125, 250, 500 for concentration level. the ml esti-\nmates are \u2424 s 0.0064 se s 0.0004 and \u2424 s 0.0174 se s 0.0012 . in\neach case, the less desirable outcome is more likely as the concentration\nincreases. for instance, given that a fetus was live, the estimated odds that it\nwas malformed rather than normal multiplies by exp 1.74 s 5.7 for every\n100-unit increase in the concentration of diegdime. the likelihood-ratio fit\n\n\u02c6\n2\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ntable 7.9 outcomes for pregnant mice in developmental toxicity study\n\nresponse\n\n.\n\n.\n\nnonlive\n\nmalformation\n\nconcentration\nmgrkg per day\n\u017e\n\u017e\n0 controls\n62.5\n125\n250\n500\na\nbased on results in c. j. price et al., fund. appl. toxicol. 8:115\u1390126 1987 . i thank\nlouise ryan for showing me these data.\n\n15\n17\n22\n38\n144\n\n1\n0\n7\n59\n132\n\n281\n225\n283\n202\n9\n\nnormal\n\n\u017e\n\n.\n\n "}, {"Page_number": 306, "text": "2\n\n2\n\n.\n\n\u017e\n\n\u017e\n\n291\nalternative models for ordinal responses\nstatistics are g2 s 5.78 for j s 1 and g2 s 6.06 for j s 2, each based on\ndf s 3. their sum, g s 11.84 or similarly x s 9.76 , with df s 6, summa-\nrizes the fit.\n\nthis analysis treats pregnancy outcomes for different fetuses as indepen-\ndent, identical observations. in fact, each pregnant mouse had a litter of\nfetuses, and statistical dependence may exist among different fetuses in the\nsame litter. different litters at a given concentration level may also have\ndifferent response probabilities. heterogeneity of various sorts among the\nlitters e.g., due to varying physical characteristics among different pregnant\n.mice would cause these probabilities to vary somewhat. either statistical\ndependence or heterogeneous probabilities violates the binomial assumption\nand causes overdispersion. at a fixed concentration level, the number of\nfetuses in a litter that die may vary among pregnant mice more than if the\ncounts were independent and identical binomial variates. the total g2 shows\nsome evidence of lack of fit p s 0.07 but may reflect overdispersion caused\nby these factors rather than an inappropriate choice of response curve.\n\nto account for overdispersion, we could adjust standard errors using the\nquasi-likelihood approach section 4.7 . this multiplies standard errors by\n2'\nx rdf s 9.76r6 s 1.28. for each logit, strong evidence remains that\n\u2424 ) 0. in chapters 12 and 13 we present other methods that account for the\nclustering of fetuses in litters.\n\n'\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nj\n\n7.4.5 mean response models for ordered response\nwe now present a model that resembles ordinary regression for a continuous\nresponse variable. for scores \u00ae f \u00ae f \u2b48\u2b48\u2b48 f \u00ae , let\n\n1\n\n2\n\nj\n\nm x s \u00ae \u2432 x\n\u017e .\n\n\u00fd j\n\n\u017e .\n\nj\n\nj\n\ndenote the mean response. the model\n\nm x s \u2423q \u2424x x\n\n\u017e .\n\n\u017e\n\n7.16\n\n.\n\n\u017e\n\nassumes a linear relationship between the mean and the explanatory vari-\nables. with j s 2, it is the linear probability model section 4.2.1 . with\nj ) 2, it does not structurally specify the response probabilities but merely\ndescribes the dependence of the mean on x.\n\nassuming independent multinomial sampling at different x , bhapkar\n\u017e\n1968 , grizzle et al. 1969 , and williams and grizzle 1972 presented\nweighted least squares wls fits for mean response models. the wls\napproach, described in section 15.1, applies when all explanatory variables\nare categorical. the ml approach for maximizing the product multinomial\nlikelihood applies for categorical or continuous explanatory variables. haber\n\u017e\n1985 and lipsitz 1992 presented algorithms for ml fitting of a family,\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\ni\n\n "}, {"Page_number": 307, "text": "292\n\nlogit models for multinomial responses\n\nincluding mean response models. this is somewhat complex, since the\nprobabilities in the multinomial likelihood are not direct functions of the\n.\nparameters in 7.16 . specialized software is available see appendix a .\n\n.\n\n\u017e\n\n\u017e\n\njob satisfaction example revisited\n\n7.4.6\nwe illustrate for table 7.8, modeling the mean of y s job satisfaction using\nincome x and gender g 1 s females, 0 s males . for simplicity, we use job\nsatisfaction scores and income scores 1, 2, 3, 4 . the model has ml fit,\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u02c6m s 2.59 q 0.181 x y 0.030 g ,\n\nw\n\n\u017e\n\nwith se s 0.069 for income and 0.145 for gender. given gender, the esti-\nmated increase in mean job satisfaction is about 0.2 response category for\neach category increase of income. although the evidence is strong of a\nx\npositive effect e.g., wald statistic 0.181r0.069 s 6.8, df s 1, p s 0.009 ,\nthe strength of the effect is weak. job satisfaction at the highest income level\nis estimated to average about half a category higher than at the lowest\nincome level, since 3 0.181 s 0.54. similar results occur with the wls\nsolution, for which the estimated income effect of 0.182 has se s 0.068\n.\n\u017e\ntable a.12 shows the use of catmod in sas .\n\n.2\n\nthe deviance for testing the model fit equals 5.1. since means occur at\neight income = gender settings and the model has three parameters, residual\ndf s 5. the fit seems adequate.\n\n\u017e\n\n.\n\n7.4.7 advantages and disadvantages of mean response models\n\ntreating ordinal variables in a quantitative manner is sensible if their\ncategorical nature reflects crude measurement of an inherently continuous\nvariable. mean response models have the advantage of closely resembling\nordinary regression.\nwith j s 2, in section 4.2.1 we noted that linear probability models have\na structural difficulty because of the restriction of probabilities to 0, 1 . a\nsimilar difficulty occurs here, since a linear model can have predicted means\noutside the range of assigned scores. this happens less frequently when j is\nlarge and reasonable dispersion of responses occurs throughout the domain\nof interest for the explanatory variables. the notion of an underlying latent\nvariable makes more sense for an ordinal variable than for a strictly binary\nresponse, so this difficulty has less relevance here.\n\nunlike logit models, mean response models do not uniquely determine cell\nprobabilities. thus, mean response models do not specify structural aspects\nsuch as stochastic orderings. these models do not represent the categorical\nresponse structure as fully as do models for probabilities, and conditions such\nas independence do not occur as special cases. however, they provide\nsimpler descriptions than odds ratios or summaries from cumulative link\n\n\u017e\n\n.\n\n "}, {"Page_number": 308, "text": "testing conditional independence in i = j = k tables\n\n293\n\nmodels. as j increases, they also interface with ordinary regression models.\nfor large j, they are a simple mechanism for approximating results for a\nregression model we would use if we could measure y continuously.\n\n7.5 testing conditional independence in\ni = j = k tables*\n\nin section 6.3.2 we introduced the cochran\u1390mantel\u1390haenszel cmh test of\nconditional independence for 2 = 2 = k tables. this section presents related\ntests with multicategory responses for i = j = k tables. likelihood-ratio\ntests compare the fit of a model specifying xy conditional independence\nwith a model having dependence. alternatively, generalizations of the cmh\nstatistic are score statistics for certain models.\n\n\u017e\n\n.\n\n7.5.1 using multinomial models to test conditional independence\n\u017e\ntreating z as a nominal control factor, we discuss four cases with y, x as\n\u017e\n.\nordinal, ordinal , ordinal, nominal , nominal, ordinal , nominal, nominal .\nfor ordinal y we use cumulative logit models, but other ordinal links yield\nanalogous tests. as we noted in section 6.3.2 when the xy association is\nsimilar in the partial tables, the power benefits from basing a test statistic on\na model of homogeneous association.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n1. y ordinal, x ordinal. let x be ordered scores. the model\nlogit p y f j x s i, z s k s \u2423 q \u2424x q \u2424\n\n\u017e\n\n.\n\ni\n\n<\n\nj\n\ni\n\nz\nk\n\n\u0004\n\n4\n\n\u017e\n\n7.17\n\n.\n\nhas the same linear trend for the x effect in each partial table. for it,\nxy conditional independence is h : \u2424s 0. likelihood-ratio, score, or\nwald statistics for h provide large-sample chi-squared tests with\ndf s 1 that are sensitive to the trend alternative.\n\n0\n\n0\n\n2. y ordinal, x nominal. an alternative to conditional independence that\n\ntreats x as a factor is\n\nlogit p y f j x s i, z s k s \u2423 q \u2424 q \u2424 ,\n\n\u017e\n\n.\n\n<\n\nj\n\ni\n\nz\nk\n\nwith constraint such as \u2424 s 0. for this model, xy conditional inde-\npendence is h : \u2424 s \u2b48\u2b48\u2b48 s \u2424 . large-sample chi-squared tests have\ndf s i y 1.\n\n0\n\n1\n\ni\n\ni\n\n3. y nominal, x ordinal. when y is nominal, analogous tests use\nbaseline-category logit models. the model of xy conditional indepen-\ndence is\n\n<\n\np y s j x s i, z s k\n\u017e\np y s j x s i, z s k\n\u017e\n\n<\n\n.\n.\n\ns \u2423 .\n\njk\n\n\u017e\n\n7.18\n\n.\n\nlog\n\n "}, {"Page_number": 309, "text": "294\n\nlogit models for multinomial responses\n\nfor ordered scores x , a test that is sensitive to the same linear trend\nalternatives in each partial table compares this model to\n\ni\n\n\u0004\n\n4\n\n<\n\np y s j x s i, z s k\n\u017e\np y s j x s i, z s k\n\u017e\n\n<\n\n.\n.\n\nlog\n\ns \u2423 q \u2424 x .\n\nj\n\ni\n\njk\n\nconditional independence is h : \u2424 s \u2b48\u2b48\u2b48 s \u2424 s 0. large-sample\nchi-squared tests have df s j y 1.\n\njy1\n\n0\n\n1\n\n4. y nominal, x nominal. an alternative to xy conditional independence\n\nthat treats x as a factor is\n\n<\n\np y s j x s i, z s k\n\u017e\np y s j x s i, z s k\n\u017e\n\n<\n\n.\n.\n\ns \u2423 q \u2424\ni j\n\njk\n\n\u017e\n\n7.19\n\n.\n\nlog\n\nwith constraint such as \u2424 s 0 for each j. for each j, x and z have\nadditive effects of form \u2423 q \u2424. conditional\nindependence is h :\n0\n\u2424 s \u2b48\u2b48\u2b48 s \u2424 for j s 1, . . . , j y 1. large-sample chi-squared tests\nhave df s i y 1 j y 1 .\n.\n\n.\u017e\n\n1 j\n\n\u017e\n\ni j\n\ni j\n\nk\n\ni\n\n\u017e\n\ntable 7.10 summarizes the four tests. they work well when the model\ndescribes at least a major component of the departure from conditional\nindependence. this does not mean that one must test the fit of the model to\n.\nuse the test see the remarks at the end of section 6.3.2 .\n\noccasionally, the association may change dramatically across the k partial\ntables. when z is ordinal, an alternative by which a log odds ratio changes\nlinearly across levels of z is sometimes of use. for instance, when z s age\nof subject, the association between a risk factor x e.g., level of smoking and\na response y e.g., severity of heart disease may tend to increase with z.\nindependence models\nwhen z is nominal, one can test the conditional\n\n.\n\n\u017e\n\n\u017e\n\n.\n\ntable 7.10 summary of models for testing conditional independence\n\ny-x\nord-ord\n\n-nom\n\nlogit p y f j s \u2423 q \u2424x q \u2424\nlogit p y f j s \u2423 q \u2424 q \u2424\n\ni\n\nj\n\nz\nk\n\nz\nk\n\ni\n\nj\n\nmodel\n.x\n.x\n\nconditional\nindependence\n\n\u2424s 0\n\n\u2424 s \u2b48\u2b48\u2b48 s \u2424\n1\ni\n\nnom-ord\n\nlog\n\n-nom log\n\ns \u2423 q \u2424 x\n\njk\n\nj\n\n\u2424 s \u2b48\u2b48\u2b48 s \u2424 s 0\n\njy1\n\n1\n\ni\n\ns \u2423 q \u2424\ni j\n\njk\n\nall \u2424 s 0\n\ni j\n\n\u017e\n\ni y 1 j y 1\n.\n\n.\u017e\n\n\u017e\n\u017e\n\nw\nw\np y s j\n\u017e\np y s j\n\u017e\np y s j\n\u017e\np y s j\n\u017e\n\n.\n.\n\n.\n.\n\ndf\n1\n\ni y 1\n\nj y 1\n\n "}, {"Page_number": 310, "text": "testing conditional independence in i = j = k tables\n\n295\n\nagainst a more general alternative with separate effect parameters at each\nlevel of z. allowing effects to vary across levels of z, however, results in the\ntest df being multiplied by k, which handicaps power.\n\n\u0004\n\n4\n\n.\n\njob satisfaction example revisited\n\u017e\n\n7.5.2\nwe now revisit the job satisfaction data table 7.8 . table 7.11 summarizes\nthe fit of several models. the model treating income as an ordinal predictor\nuses scores 3, 10, 20, 35 , approximate midpoints of categories in thousands\nof dollars. each likelihood-ratio test compares a given model to the model\ndeleting the income effect, controlling for gender.\n.\ntesting conditional independence with the cumulative logit model 7.17\nyields likelihood-ratio statistic 19.62 y 13.95 s 5.7 with df s 20 y 19 s 1,\nstrong evidence of an effect. models that treat either or both variables as\nnominal do not provide such strong evidence. focusing the test on a linear\ntrend alternative yields a smaller p-value. however, we learn more from\nestimating parameters than from significance tests, as in sections 7.4.2 and\n7.4.6.\n\n\u017e\n\n.\n\n\u017e\n\n7.5.3 generalized cochran\u2013mantel\u1390haenszel tests for\ni = j = k tables\nbirch 1965 , landis et al. 1978 , and mantel and byar 1978 generalized\nthe cmh statistic section 6.3.2 . the tests treat x and y symmetrically, so\nthe three cases correspond to treating both as nominal, both as ordinal, or\none of each. conditional on row and column totals, each stratum has\ni y 1 j y 1 nonredundant cell counts. let\n\u017e\n\n.\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\nn s n\n\u017e\n\nk\n\n, n\n\n12 k\n\n, . . . , n\n\n1, jy1 ,k\n\n, . . . , n\n\n11 k\n\niy1 , jy1 ,k\n\nx\n\n.\n\n.\n\ntable 7.11 summary of model-based likelihood-ratio tests of\nconditional independence for table 7.8\n\nsatisfaction\nordinal\n\nnominal\n\nincome\n\nordinal\nnominal\nnot in model\nordinal\nnominal\nnot in model\n\n2\n\ng fit\n13.95\n10.51\n19.62\n11.74\n7.09\n19.37\n\ndf\n19\n17\n20\n15\n9\n18\n\ntest\n\nstatistic\n\n5.7\n9.1\n\u138f\n7.6\n12.3\n\u138f\n\ndf\n1\n3\n\u138f\n3\n9\n\u138f\n\np-value\n0.017\n0.028\n\n\u138f\n\n0.054\n0.198\n\n\u138f\n\n "}, {"Page_number": 311, "text": "logit models for multinomial responses\n\n296\nlet \u242e s e n\n\u017e\n\nk\n\n.\n\nk\n\n0\n\nunder h : conditional independence, namely\nrn\n\n, . . . , n\n\niy1 ,q,k q, jy1 ,k\n\n1qk q2 k\n\n1qk q1 k\n\n, n\n\nn\n\nn\n\nn\n\n.\n\nx\n\nqqk\n\n.\n\n\u242e s n\n\u017e\n\nk\n\nlet v denote the null covariance matrix of n , where\n\nk\n\nk\n\n\u017e\n\nn\n\ncov n , n\n\n.\nn\ni qk qj k\nn y 1\n\u017e\nqqk\nwith \u2426 s 1 when a s b and \u2426 s 0 otherwise.\n\n\u2426 x n y n x\nii qqk\n2n\nqqk\n\ns\n\niqk\n\nx\ni j k\n\ni jk\n\n\u017e\n\n\u017e\n\n.\n\nx\n\nab\n\nab\n\n\u2426 x n y n x\nj j qqk\nqj k\n.\n\n.\n\nthe most general statistic treats rows and columns as unordered. sum-\n\nming over the k strata, let\n\nn s n ,\n\n\u00fd\n\nk\n\n\u242e s \u242e ,\n\n\u00fd\n\nk\n\nv s v .\n\n\u00fd\n\nk\n\nthe generalized cmh statistic for nominal x and y is\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nx y1\n\nn y \u242e .\n.\n\ncmh s n y \u242e v\n\n.\n7.20\nits large-sample chi-squared distribution has df s i y 1 j y 1 . the df\n.\nvalue equals that for the statistics comparing logit models 7.18 and 7.19 .\nboth statistics are sensitive to detecting a conditional association that is\nsimilar in each stratum. for k s 1 stratum with n observations, cmh s\nw\u017e\nn y 1 rn x , where x is the pearson statistic 3.10 .\n.\nmantel 1963 introduced a generalized statistic for ordinal x and y.\nusing ordered scores u and \u00ae , it is sensitive to a correlation of common\nsign in each stratum. evidence of a positive trend occurs if in each stratum\nt s \u00fd \u00fd u \u00ae n\nexceeds its null expectation. given the marginal totals in\neach stratum, under conditional independence\n\n.\u017e\n\u017e\n\nx\n\u017e\n\ni jk\n\n.\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n2\n\n2\n\nk\n\ni\n\ni\n\ni\n\nj\n\nj\n\nj\n\ne t s\n\u017e\n\n.\n\nk\n\n\u00fd\n\ni\n\nu n\ni\n\niqk\n\n\u00ae n\nj qj k\n\n\u00fd\n\nj\n\nn\nqqk\n\n,\n\nvar t s\n\n.\n\n\u017e\n\nk\n\n1\n\nn y 1\nqqk\n\n\u00fd\n\ni\n\n\u017e\n\nu n y\n\niqk\n\n2\ni\n\niqk\n\ni\n\n\u00fd u n\ni\nn\nqqk\n\n2\n\n.\n\n=\n\n\u00fd j qj k\n\n2\u00ae n y\n\nj\n\n2\n\n.\n\n\u017e\n\n\u00fd \u00ae n\nj qj k\nj\nnqqk\n\n.\n\nw\n\n.x1r2\nthe statistic t y e t r var t\n\u017e\nequals the correlation between x and\n' qqk\ny in stratum k multiplied by n y 1 . to summarize across the k strata,\n\n.x\n\n\u017e\n\nw\n\nk\n\nk\n\nk\n\n "}, {"Page_number": 312, "text": "testing conditional independence in i = j = k tables\n\n297\n\n.\n\n\u017e\n\nmantel 1963 proposed\n\u0004\n\n2m s\n\n\u00fd \u00fd \u00fd u \u00ae n y e \u00fd \u00fd u \u00ae n\n\n\u017e\n\ni\n\nj\n\ni jk\n\nk\n\ni\n\ni\n\nj\n\nj\n\nj\n\n\u00fd var \u00fd \u00fd u \u00ae n\n\n\u017e\n\nk\n\ni\n\ni\n\nj\n\nj\n\ni jk\n\ni\n\n.\n\n.\n\ni jk\n\n2\n\n4\n\n.\n\n\u017e\n\n7.21\n\n.\n\n1\n\n\u017e\n\nthis has an approximate \u24392 null distribution, the same as for testing h :\n0\n\u2424s 0 in ordinal model 7.17 . for k s 1, this is the m statistic 3.15 .\n.\n\nlandis et al. 1978 presented a statistic that has 7.20 and 7.21 as\nspecial cases. his statistic also can treat x as nominal and y as ordinal,\nsummarizing information about how i row means compare to their null\nexpected values, with df s i y 1 see note 7.7 .\n.\n\n2\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n4\n\n\u0004\n\n\u0004\n\njob satisfaction example revisited\n\n7.5.4\ntable 7.12 shows output from conducting generalized cmh tests for table\n7.8. statistics treating a variable as ordinal used scores 3, 10, 20, 35 for\nincome and scores 1, 3, 4, 5 for job satisfaction. table a.12 shows the use of\nproc freq in sas, but with different scores.\n\nthe general association alternative treats x and y as nominal and uses\n\u017e\n7.20 . it is sensitive to any association that is similar in each level of z. the\nrow mean scores differ alternative treats rows as nominal and columns as\nordinal. it is sensitive to variation among the i row mean scores on y, when\nthat variation is similar in each level of z. finally, the nonzero correlation\nalternative treats x and y as ordinal and uses 7.21 . it is sensitive to a\nsimilar linear trend in each level of z. as in the model-based analyses that\ntable 7.11 summarized, the evidence is stronger using the df s 1 ordinal test.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n4\n\n7.5.5 related score tests for multinomial logit models\nthe generalized cmh tests seem to be non-model-based alternatives to\nthose of section 7.5.1 using multinomial\nlogit models. however, a close\nconnection exists between them. for various multinomial logit models, the\ngeneralized cmh tests are score tests.\n\ntable 7.12 output for generalized cochran\u2013mantel\u2013haenszel tests\nwith job satisfaction and income data\n\nsummary statistics for income by satisf\n\ncontrolling for gender\n\ncochran- mantel- haenszel statistics (based on table scores)\n\nstatistic\n\nalternative hypothesis\n\ndf\n\nvalue\n\nprob\n\n1\n2\n3\n\nnonzero correlation\nrow mean scores differ\ngeneral association\n\n1\n3\n9\n\n6.1563\n9.0342\n10.2001\n\n0.0131\n0.0288\n0.3345\n\n "}, {"Page_number": 313, "text": "298\n\nlogit models for multinomial responses\n\n\u017e\n\n.\n\ni j\n\n4\n\n4\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\u0004\n\n.\u017e\n\nthe generalized cmh test 7.20 that treats x and y as nominal is the\nscore test that the i y 1 j y 1 \u2424 parameters in logit model 7.19 equal\n0. the generalized cmh test using m 2 that treats x and y as ordinal is the\nscore test of \u2424s 0 in model 7.17 . for the cumulative logit model, the\n\u017e\nscores in the model as in m , and the \u00ae\n\u0004\n\u0004\n4\nequivalence has the same x\nj\nscores in m 2 are average rank scores. for the adjacent-categories logit\nmodel analog of 7.17 , the \u00ae scores in m are any equally spaced scores.\nwith large samples in each stratum, the generalized cmh tests give\nsimilar results as likelihood-ratio tests comparing the relevant models. an\nadvantage of the model-based approach is providing estimates of effects. an\nadvantage of the generalized cmh tests is maintaining good performance\nunder sparse asymptotics whereby k grows as n does. remarks in section\n6.3.4 apply here also.\n\n.\n\n\u017e\n\n\u0004\n\n4\n\n2\n\n2\n\ni\n\nj\n\n7.5.6 exact tests of conditional independence\nin principle, exact tests of conditional independence can use the generalized\ncmh statistics, generalizing section 6.7.5 for 2 = 2 = k tables. to eliminate\nnuisance parameters, one conditions on row and column totals in each\nstratum. the distribution of counts in each stratum is the multiple hypergeo-\nmetric section 3.5.7 , and this propagates an exact conditional distribution\nfor the statistic of interest. the p-value is the probability of those tables\nhaving the same strata margins as observed but test statistic at least as large\n.\nas observed see birch 1965; kim and agresti 1997; mehta et al. 1988 .\n\n.\n\n\u017e\n\n\u017e\n\n7.6 discrete-choice multinomial logit models*\n\nan important application of multinomial logit models is determining effects\nof explanatory variables on a subject\u2019s choice from a discrete set of\noptions\u138ffor instance, the choice of transportation system to take to work\n\u017e\ndrive, bus, subway, walk, bicycle , housing buy house, buy condominium,\nrent , primary shopping location downtown, mall, catalogs, internet , or\nproduct brand. models for response variables consisting of a discrete set of\nchoices are called discrete-choice models.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n7.6.1 discrete-choice modeling\nin many discrete-choice applications, an explanatory variable takes different\nvalues for different response choices. as predictors of choice of transporta-\ntion system, cost and time to reach destination take different values for each\noption. as a predictor of choice of product brand, price varies according to\nthe option. explanatory variables of this type are characteristics of the choices.\nthey differ from the usual ones, for which values remain constant across the\nchoice set. such variables, characteristics of the chooser,\ninclude income,\neducation, and other demographic characteristics.\n\n "}, {"Page_number": 314, "text": "discrete-choice multinomial logit models\n\n299\n\n\u017e\n\n.\n\nmcfadden 1974 proposed a discrete-choice model for explanatory vari-\nables that are characteristics of the choices. his model also permits the\nchoice set to vary among subjects. for instance, some subjects may not have\nthe subway as an option for travel to work. for subject i and response choice\nj, let x s x\n.x\n\u017e\n, . . . , x\ndenote the values of the p explanatory variables,\nand let x s x , . . . , x\n\u017e\n.\n. conditional on the choice set c for subject i, the\ni1\nmodel for the probability of selecting option j is\n\u017e\n.i j\nx\nexp \u2424 x\n\nexp \u2424x x\n\u017e\nhg c\n\n\u2432 x s\n\n7.22\n\n\u00fd\n\ni j p\n\ni j1\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\ni p\n\nih\n\ni j\n\n.\n\ni\n\ni\n\ni\n\nj\n\ni\n\nfor each pair of choices a and b, this model has the logit form\n\nlog \u2432 x r\u2432 x s \u2424 x y x\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nx\n\nb\n\ni\n\na\n\ni\n\nia\n\n.\n\n.\n\nib\n\n\u017e\n\n7.23\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nconditional on the choice being a or b, a variable\u2019s influence depends on the\ndistance between the subject\u2019s values of that variable for those choices. if the\nvalues are the same, the model asserts that the variable has no influence on\nthe choice between a and b. reflecting this property, mcfadden originally\nreferred to model 7.22 as a conditional logit model.\n\nfrom 7.23 , the odds of choosing a over b do not depend on the other\nalternatives in the choice set or on their values of the explanatory variables.\nluce 1959 called this property independence from irrele\u00aeant alternati\u00aees. it is\nunrealistic in some applications. for instance, for travel options auto and red\nbus, suppose that 80% choose auto, an odds of 4.0. now suppose that the\noptions are auto, red bus, and blue bus. according to 7.23 , the odds are still\n4.0 of choosing auto instead of red bus, but intuitively, we expect them to be\nabout 8.0 10% choosing each bus option , mcfadden 1974 stated: \u2018\u2018appli-\ncation of the model should be limited to situations where the alternatives can\nplausibly be assumed to be distinct and weighed independently in the eyes of\neach decision-maker.\u2019\u2019\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n7.6.2 discrete-choice and multinomial logit models\nmodel 7.22 can also incorporate explanatory variables that are characteris-\ntics of the chooser. this may seem surprising, since 7.22 has a single\nparameter for each explanatory variable; that is, the parameter vector is the\nsame for each pair of choices. however, multinomial logit model 7.2 has\ndiscrete-choice form 7.22 after replacing such an explanatory variable by j\nartificial variables; the jth is the product of the explanatory variable with a\ndummy variable that equals 1 when the response choice is j. for instance, for\na single explanatory variable,\ni. for\nj s 1, . . . , j, let \u2426 equal 1 when k s j and 0 otherwise, and let\n\nlet x denote its value for subject\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\njk\n\nz s \u2426 , . . . , \u2426 ,\u2426 x , . . . , \u2426 x\n\n\u017e\n\nj1\n\ni\n\nj1\n\nj j\n\nj j\n\ni j\n\nx\n\n.\n\n.\n\ni\n\n "}, {"Page_number": 315, "text": "300\nlogit models for multinomial responses\nlet \u2424 s \u2423 , . . . , \u2423 ,\u2424 , . . . , \u2424 . then \u2424 z s \u2423 q \u2424 x , and 7.2 is with\n\u2423 s \u2424 s 0 for identifiability\n\nj\n.\n\n.x\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\ni j\n\n1\n\n1\n\nj\n\nx\n\ni\n\nj\n\nj\n\nj\n\nj\n\n\u2432 x s\u017e\n\n.\n\ni\n\nj\n\ns\n\nexp \u2423 q \u2424 x\n\n\u017e\n\nj\n\nj\n\ni\n\n.\n\nexp \u2423 q \u2424 x q \u2b48\u2b48\u2b48 qexp \u2423 q \u2424 x\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\nj\n\n1\n\ni\n\n1\n\nj\n\n.i j\n\n\u017e\n\nexp \u2424xz\n.\n\nexp \u2424 z q \u2b48\u2b48\u2b48 qexp \u2424 z\n\n\u017e\n\n\u017e\n\nx\n\nx\n\ni1\n\n.\n\n.\n\ni j\n\n.\nthis has form 7.22 .\n\n\u017e\n\nwith this approach, discrete-choice models can contain characteristics of\nthe chooser and the choices. thus, model 7.22 is very general. the ordinary\nmultinomial logit model 7.2 using baseline-category logits is a special case.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n7.6.3 shopping choice example\nmcfadden 1974 used multinomial logit models to describe how residents of\npittsburgh, pennsylvania chose a shopping destination. the five possible\ndestinations were different city zones. one explanatory variable measured\nshopping opportunities, defined to be the retail employment in the zone as a\npercentage of total retail employment in the region. the other explanatory\nvariable was price of the trip, defined from a separate analysis using auto\nin-vehicle time and auto operating cost.\nthe ml estimates of model parameters were y1.06 se s 0.28 for price\n.\nof trip and 0.84 se s 0.23 for shopping opportunity. from 7.23 ,\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\nlog \u2432r\u2432 s y1.06 p y p q 0.84 s y s ,\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u02c6 \u02c6\na\n\n.\n\n\u017e\n\na\n\na\n\nb\n\nb\n\nb\n\nwhere p s price and s s shopping opportunity. not surprisingly, a destina-\ntion is relatively more attractive as the trip price decreases and as the\nshopping opportunity increases. given values of p and s for each destina-\ntion, the sample analog of 7.22 provides estimated probabilities of choosing\neach destination.\n\n\u017e\n\n.\n\nnotes\n\nsection 7.1: nominal responses: baseline-category logit models\n\n7.1. multicategory models derive from latent variable constructions that generalize those for\nbinary responses. one approach uses the principle of selecting the category having\nmaximum utility problem 6.29 . fahrmeir and tutz 2001, chap. 3 gave discussion and\nreferences. baseline-category logit models were developed in bock 1970 , haberman\n\u017e\n.\n1974a, pp. 352\u1390373 , mantel 1966 , nerlove and press 1973 , and theil 1969, 1970 .\nlesaffre and albert 1989 presented regression diagnostics. amemiya 1981 , haber-\nman 1982 , and theil 1970 presented r-squared measures.\n\n.\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 316, "text": "notes\n\n301\n\nsection 7.2: ordinal responses: cumulati\u00a9e logit models\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n7.2. early uses of cumulative logit models include bock and jones 1968 , simon 1974 , snell\n\u017e\n.\n1964 , walker and duncan 1967 , and williams and grizzle 1972 . mccullagh 1980\n.\npopularized the proportional odds case. later articles include agresti and lang 1993a ,\nhastie and tibshirani 1987 , peterson and harrell 1990 , and tutz 1989 . see also\n.\nsection 11.3.3, note 11.3, and section 12.4.1. mccullagh and nelder 1989, sec. 5.6\nsuggested using cumulative totals in forming residuals.\n.\n\n7.3. mccullagh 1980 noted that score tests for model 7.5 are equivalent to nonparametric\ntests using average ranks. for instance, for 2 = j tables assume that logit p y f j s\n\u2423 q \u2424x, with x an indicator. the score test of h : \u2424s 0 is equivalent to a discrete\nversion of the wilcoxon\u1390mann\u1390whitney test. whitehead 1993 gave sample size\nformulas for this case. the sample size n needed for a certain power decreases as j\nincreases: when response categories have equal probabilities, n f 0.75n r 1 y 1rj\n2.\n.\nthus, for large j, n f 0.75n , and 1 y 1rj 2 is a type of efficiency measure of using j\ncategories instead of a continuous response. the efficiency loss is minor with j f 5, but\nmajor in collapsing to j s 2. edwardes 1997 innovatively adapted the test by treating\nthe cutpoints as random. this relates to random effects models of section 12.4.1.\n\n\u017e\n\u017e\n\n.x\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nw\n\n0\n\n2\n\n2\n\nj\n\nj\n\nj\n\nj\n\nsection 7.3: ordinal responses: cumulati\u00a9e link models\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n7.4. aitchison and silvey 1957 and bock and jones 1968, chap. 8 studied cumulative\nprobit models. farewell 1982 generalized the complementary log-log model to allow\nvariation among the sample in the category boundaries for the underlying scale; this\nrelates to random effects models section 12.4 . genter and farewell 1985 introduced\na generalized link function that permits comparison of fits provided by probit, comple-\nmentary log-log, and other links. yee and wild 1996 defined generalized additive\n.\nmodels for nominal and ordinal responses. hamada and wu 1990 and nair 1987\npresented alternatives to model 7.8 for detecting dispersion effects.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n7.5. some authors have considered inference relating generally to stochastic ordering; see,\n.\nfor instance, dardanoni and forcina 1998 and survey articles in a 2002 issue of j.\n.\nstatist. plann. inference vol. 107, nos. 1\u13902 .\n\n\u017e\n\n\u017e\n\nsection 7.4: alternati\u00a9e models for ordinal responses\n\n.\n7.6. the ratio of a pdf to the complement of the cdf is the hazard function section 9.7.3 .\nfor discrete variables,\nthis is the ratio found in continuation-ratio logits. hence,\n.\ncontinuation-ratio logits are sometimes interpreted as log hazards. thompson 1977\nused them in modeling discrete survival-time data. when lengths of time intervals\napproach 0, his model converges to the cox proportional hazards model. other applica-\n.\ntions of continuation-ratio logits include laara and matthews 1985 and tutz 1991 .\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u00a8\u00a8 \u00a8\n\nsection 7.5: testing conditional independence in i = j = k tables\n\n7.7. let b s u m v denote a matrix of constants based on row scores u and column\nfor stratum k, where m denotes the kronecker product. the landis et al.\n\nk\nscores v\nk\n\nk\n\nk\n\nk\n\n "}, {"Page_number": 317, "text": "302\n\nlogit models for multinomial responses\n\n\u017e\n1978 generalized statistic is\n\n.\n\nx\n\ny1\n\ni\n\nk\n\nk\n\nk\n\nk\n\nk\n\nk\n\nk\n\nk\n\n1\n\n1\n\n.\n\nk\n\nk\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nk k\n\n\u00fd\n\n2l s\n\nx\nb v b\nk\n\nb n y \u242e\n\n\u00fd\nb n y \u242e\n\u017e\nand v s \u00ae , . . . , \u00ae\n.\n\n\u00fd\nk\nfor all strata, l s m . when u is an\nwhen u s u , . . . , u\ni y 1 = i matrix i, y1 , where i is an identity matrix of size i y 1 and 1 denotes a\n\u017e\ncolumn vector of i y 1 ones, and v\nis the analogous matrix of size j y 1 = j, l\nsimplifies to 7.20 with df s i y 1 j y 1 . with this u and v s \u00ae , . . . , \u00ae , l sums\n.\u017e\nover the strata information about how i row means compare to their null expected\nvalues, and it has df s i y 1. rank score versions are analogs for ordered categorical\nresponses of strata-adjusted spearman correlation and kruskal\u1390wallis tests. landis et\nal. 1998 and stokes et al. 2000 reviewed cmh methods. koch et al. 1982 reviewed\nrelated methods.\n\n2\n\u017e\n\n2\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2\n\n2\n\n1\n\nk\n\nk\n\nk\n\nk\n\nj\n\nj\n\nsection 7.6: discrete-choice multinomial logit models\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n7.8. mcfadden\u2019s model relates to models proposed by bradley and terry 1952 see section\n10.6 and luce 1959 . see train 1986 for a text treatment. mcfadden 1982 discussed\nhierarchical models having a nesting of choices in a tree-like structure. for other\ndiscussion, see maddala 1983 and small 1987 . models that do not assume indepen-\ndence from irrelevant alternatives result with probit link amemiya 1981 or with the\nlogit link but including random effects brownstone and train 1999 . methods in section\n12.6 for random effects models are useful for fitting such models. these include monte\ncarlo methods for approximating integrals that determine the likelihood function. see\nstern 1997 for a review.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n. \u017e\n\nproblems\n\napplications\n7.1 for table 7.13, let y s belief in life after death, x s gender 1 s\nfemales, 0 s males , and x s race 1 s whites, 0 s blacks . table\n7.14 shows the fit of the model\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n1\n\n2\n\nlog \u2432r\u2432 s \u2423 q \u2424g x q \u2424r x ,\n\n\u017e\n\n.\n\n3\n\n1\n\n2\n\nj\n\nj\n\nj\n\nj\n\nj s 1, 2,\n\nwith se values in parentheses.\n\ntable 7.13 data for problem 7.1\n\nrace\nwhite\n\nblack\n\ngender\nfemale\nmale\nfemale\nmale\n\nyes\n371\n250\n64\n25\n\nbelief in afterlife\n\nundecided\n\n49\n45\n9\n5\n\nno\n74\n71\n15\n13\n\nsource: 1991 general social survey, national opinion\nresearch center.\n\n "}, {"Page_number": 318, "text": "problems\n\n303\n\ntable 7.14 fit of model for problem 7.1\n\nparameter\nintercept\ngender\nrace\n\nbelief categories for logit\n\nyesrno\n0.883 0.243\n0.419 0.171\n0.342 0.237\n\n\u017e\n\u017e\n\u017e\n\n.\n.\n.\n\nundecidedrno\ny0.758 0.361\n.\n.\n0.105 0.246\n.\n0.271 0.354\n\n\u017e\n\u017e\n\u017e\n\n2\n\n1\n\n\u017e\n\na. find the prediction equation for log \u2432 r\u2432 .\n.\nb. using the yes and no response categories, interpret the conditional\n\ngender effect using a 95% confidence interval for an odds ratio.\n\nc. show that for white females, \u2432 s p y s yes s 0.76.\nd. without calculating estimated probabilities, explain why the inter-\ncept estimates indicate that for black males \u2432 ) \u2432 ) \u2432 . use\nthe intercept and gender estimates to show that the same ordering\napplies for black females.\n\n\u02c6 1\n\n\u02c6\u017e\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n.\n\n1\n\n3\n\n2\n\ne. without calculating estimated probabilities, explain why the esti-\nmates in the gender and race rows indicate that \u2432 is highest for\nblack males.\n\nf. for this fit, g2 s 0.9. explain why residual df s 2. deleting the\ngender effect, g2 s 8.0. test whether opinion is independent of\ngender, given race. interpret.\n\n\u02c6 3\n\n7.2 a model fit predicting preference for u.s. president democrat, re-\nis\n\n\u017e\n\u017e\n.\npublican,\nin $10,000\nlog \u2432 r\u2432 s 3.3 y 0.2 x and log \u2432 r\u2432 s 1.0 q 0.3 x.\n\u02c6\na. find the prediction equation for log \u2432 r\u2432 and interpret the\n\n.\n\u017e\n\u02c6\nslope. for what range of x is \u2432 ) \u2432 ?\nd\n\nusing x s annual\n\nindependent\n.\n\nincome\n\n\u02c6r\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nd\n\nd\n\nr\n\nr\n\ni\n\ni\n\nb. find the prediction equation for \u2432 .\u02c6i\nc. plot \u2432 , \u2432 , and \u2432 for x between 0 and 10, and interpret.\n\n\u02c6\n\nd\n\n\u02c6\n\ni\n\n\u02c6\n\nr\n\n7.3 table 7.15 refers to the effect on political party identification of\ngender and race. find a baseline-category logit model that fits well.\n\ntable 7.15 data for problem 7.3\n\ngender\nmale\n\nfemale\n\nrace\nwhite\nblack\nwhite\nblack\n\nparty identification\n\ndemocrat\n\nrepublican\n\nindependent\n\n132\n42\n172\n56\n\n176\n6\n129\n4\n\n127\n12\n130\n15\n\n "}, {"Page_number": 319, "text": "304\n\nlogit models for multinomial responses\n\ninterpret estimated effects on the odds that party identification is\ndemocrat instead of republican.\n\ntable 7.16 data for problem 7.4 a\n\nmales\n\nfemales\n\nlength choice\n\n\u017e\n.\nm\n1.30\n1.32\n1.32\n1.40\n1.42\n1.42\n1.47\n1.47\n1.50\n1.52\n1.63\n1.65\n1.65\n1.65\n1.65\n1.68\n1.70\n1.73\n1.78\n1.78\n\ni\nf\nf\nf\ni\nf\ni\nf\ni\ni\ni\no\no\ni\nf\nf\ni\no\nf\no\n\nlength\n\n\u017e\n.\nm\n2.56\n2.67\n2.72\n2.79\n2.84\n\nchoice\n\no\nf\ni\nf\nf\n\nlength\n\n\u017e\n.\nm\n1.80\n1.85\n1.93\n1.93\n1.98\n2.03\n2.03\n2.31\n2.36\n2.46\n3.25\n3.28\n3.33\n3.56\n3.58\n3.66\n3.68\n3.71\n3.89\n\nchoice\n\nf\nf\ni\nf\ni\nf\nf\nf\nf\nf\no\no\nf\nf\nf\nf\no\nf\nf\n\nlength\n\n\u017e\n.\nm\n1.24\n1.30\n1.45\n1.45\n1.55\n1.60\n1.60\n1.65\n1.78\n1.78\n1.80\n1.88\n2.16\n2.26\n2.31\n2.36\n2.39\n2.41\n2.44\n\nchoice\n\ni\ni\ni\no\ni\ni\ni\nf\ni\no\ni\ni\nf\nf\nf\nf\nf\nf\nf\n\nai, invertebrates; f, fish; o, other.\n\n\u017e\n\n7.4 for 63 alligators caught in lake george, florida, table 7.16 classifies\nprimary food choice as fish, invertebrate, other and shows length in\n.\nmeters. alligators are called subadults if length - 1.83 meters 6 feet\nand adults if length ) 1.83 meters.\na. measuring length as adult, subadult , find a model that adequately\ndescribes effects of gender and length on food choice. interpret the\neffects. for adult females, find the estimated probabilities of the\nfood-choice categories.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\nb. using only observations for which primary food choice was fish or\ninvertebrate,\nfind a model that adequately describes effects of\ngender and binary length. compare parameter estimates and stan-\ndard errors for this separate-fitting approach to those obtained with\nsimultaneous fitting, including the other category.\n\nc. treating length as binary loses information. adapt the model in\npart a to use the continuous measurements. interpret, explaining\nhow the estimated outcome probabilities vary with length. find the\n\n\u017e .\n\n "}, {"Page_number": 320, "text": "problems\n\n305\n\nestimated length at which the invertebrate and other categories are\nequally likely.\n\n.\n\n\u017e\n\n7.5 for recent data from a general social survey, the cumulative logit\nmodel 7.5 with y s political ideology very liberal, slightly liberal,\nmoderate, slightly conservative, very conservative and x s 1 for the\n428 democrats and x s 0 for the 407 republicans has \u2424s 0.975\nse s 0.129 and \u2423 s y2.469. interpret \u2424. find the estimated prob-\n\u017e\nability of a very liberal response for each group.\n\n\u02c61\n\n\u02c6\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\n7.6 refer to problem 7.5. with adjacent-categories logits, \u2424s 0.435. in-\nterpret using odds ratios for adjacent categories and for the very\nliberal, very conservative pair of categories.\n\n\u02c6\n\n.\n\n\u017e\n\n\u017e .\n\n7.7 table 7.17 is an expanded version of a data set analyzed in section\n8.4.2. the response categories are 1 not injured, 2 injured but not\ntransported by emergency medical services, 3 injured and transported\nby emergency medical services but not hospitalized, 4 injured and\nhospitalized but did not die, and 5 injured and died. table 7.18 shows\noutput for a model of form 7.5 , using dummy variables for predictors.\na. why are there four intercepts? explain how they determine the\nestimated response distribution for males in urban areas wearing\nseat belts.\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\n\u017e\n\n.\n\nb. construct a confidence interval for the effect of gender, given\n\nseat-belt use and location. interpret.\n\nc. find the estimated cumulative odds ratio between the response and\nseat-belt use for those in rural locations and for those in urban\nlocations, given gender. based on this, explain how the effect of\nseat-belt use varies by region, and explain how to interpret the\ninteraction estimate, y0.1244.\n\ntable 7.17 data for problem 7.7\n\ngender\nfemale\n\nlocation\nurban\n\nmale\n\nrural\n\nurban\n\nrural\n\nseat belt\n\nno\nyes\nno\nyes\nno\nyes\nno\nyes\n\n1\n\n7,287\n11,587\n3,246\n6,134\n10,381\n10,969\n6,123\n6,693\n\nresponse\n\n2\n175\n126\n73\n94\n136\n83\n141\n74\n\n3\n720\n577\n710\n564\n566\n259\n710\n353\n\n4\n91\n48\n159\n82\n96\n37\n188\n74\n\n5\n10\n8\n31\n17\n14\n1\n45\n12\n\nsource: data courtesy of cristanna cook, medical care development, augusta, maine.\n\n "}, {"Page_number": 321, "text": "306\n\nlogit models for multinomial responses\n\ntable 7.18 output for problem 7.7\n\nparameter\nintercept1\nintercept2\nintercept3\nintercept4\ngender\ngender\nlocation\nlocation\nseatbelt\nseatbelt\nlocation*seatbelt\nlocation*seatbelt\nlocation*seatbelt\nlocation*seatbelt\n\nfemale\nmale\nrural\nurban\nno\nyes\nrural\nrural\nurban\nurban\n\nno\nyes\nno\nyes\n\ndf\n1\n1\n1\n1\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n\nestimate\n3.3074\n3.4818\n5.3494\n7.2563\ny0.5463\n0.0000\ny0.6988\n0.0000\ny0.7602\n0.0000\ny0.1244\n0.0000\n0.0000\n0.0000\n\nstd error\n\n0.0351\n0.0355\n0.0470\n0.0914\n0.0272\n0.0000\n0.0424\n0.0000\n0.0393\n0.0000\n0.0548\n0.0000\n0.0000\n0.0000\n\n7.8 refer to the cumulative logit model for table 7.8.\n\n\u02c6\n1\n\na. compare the estimated income effect \u2424 s y0.510 to the estimate\nafter collapsing the response to three categories by combining\n\u017e .\ncategories\ni very satisfied and moderately satisfied, and ii very\ndissatisfied and a little satisfied. what property of the model does\nthis reflect?\nb. consider \u2424 rse using the full scale to \u2424 rse for the collapsing in\n\u02c6\n1\n. usually, a disadvantage of collapsing multinomial re-\n\npart a i\nsponses is that the significance of effects diminishes.\n\n\u017e \u017e ..\n\n\u017e .\n\n\u02c6\n1\n\nc. check whether an improved model results from permitting interac-\n\ntion between income and gender. interpret.\n\n7.9 table 7.19 refers to a clinical trial for the treatment of small-cell lung\ncancer. patients were randomly assigned to two treatment groups. the\nsequential therapy administered the same combination of chemothera-\npeutic agents in each treatment cycle; the alternating therapy had\nthree different combinations, alternating from cycle to cycle.\n\ntable 7.19 data for problem 7.9\n\nresponse to chemotherapy\n\nprogressive\n\ndisease\n\nno\n\nchange\n\npartial\n\nremission\n\ncomplete\nremission\n\ntherapy\nsequential\n\nalternating\n\ngender\nmale\nfemale\nmale\nfemale\n\n28\n4\n41\n12\n\n45\n12\n44\n7\n\n.\nsource: w. holtbrugge and m. schumacher, appl. statist. 40: 249\u1390259 1991 .\n\n29\n5\n20\n3\n\u017e\n\n26\n2\n20\n1\n\n "}, {"Page_number": 322, "text": "problems\n\n307\n\na. fit a cumulative logit model with main effects for treatment and\n\ngender. interpret.\n\nb. fit the model that also contains an interaction term. interpret.\ndoes it fit better? explain why it is equivalent to using the four\ngender\u1390treatment combinations as levels of a single factor.\n\n7.10 refer to table 7.13. treating belief in an afterlife as ordinal, fit and\n\ninterpret an ordinal model.\n\n.\n\n\u017e\n\n7.11 table 9.7 displays associations among smoking status s , breathing\ntest results b , and age a for workers in certain industrial plants.\ntreat b as a response.\na. specify a baseline-category logit model with additive factor effects\nof s and a. this model has deviance g2 s 25.9. show that df s 4,\nand explain why this model treats all variables as nominal.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nb. treat b as ordinal and s as ordinal in terms of how recently one\n\nwas a smoker, with scores s . consider the model\n\n\u0004\n\n4\n\ni\n\np b s k q 1 s s i, a s j\n\u017e\np b s k s s i, a s j\n.\n\u017e\n\n<\n\n<\n\n.\n\nlog\n\ns \u2423 q \u2424 s q \u2424 a q \u2424 s a\n\n3 i\n\nj\n\n2\n\nj\n\n1 i\n\nk\n\n1\n\nwith a s 0 and a s 1. show that this assumes a linear effect of s\nwith slope \u2424 for age - 40 and \u2424 q \u2424 for age 40\u139059. using\ns s i , \u2424 s 0.115, \u2424 s 0.311, and \u2424 s 0.663 se s 0.164 . in-\n\u02c6\n\u0004\n2\nterpret the interaction.\n\n\u02c6\n3\n\n\u02c6\n1\n\n\u017e\n\n.\n\n4\n\n2\n\n1\n\n1\n\n3\n\ni\n\n\u017e .\n\nc. from part b , for age 40\u139059 show that the estimated odds of\nabnormal rather than borderline breathing for current smokers are\n2.18 times those for former smokers and exp 2 = 0.778 s 4.74\ntimes those for never smokers. explain why the squares of these\nvalues are estimated odds of abnormal rather than normal breath-\ning.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n7.12 the book\u2019s web site www.stat.ufl.edur;aarcdarcda.html has a 7 =\n2 table that refers to subjects who graduated from high school in 1965.\nthey were classified as protestors if they took part in at least one\ndemonstration, protest march, or sit-in, and classified according to\n\u017e .\ntheir party identification in 1982. analyze the data, using response a\nparty identification, b whether a protestor. compare interpretations.\n7.13 for table 7.5, the cumulative probit model has fit \u233d p y f j s \u2423\u02c6j\ny 0.195 x q 0.683 x , with \u2423 s y0.161, \u2423 s 0.746, and \u2423 s 1.339.\nfind the means and standard deviation for the two normal cdf\u2019s that\nprovide the curves for p y ) 2 as a function of x s life events index,\nat the two levels of x s ses. interpret effects.\n\ny1 \u02c6w\n\u017e\n\n\u017e .\n\n\u02c6\u017e\n\n.x\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n.\n\n1\n\n2\n\n1\n\n2\n\n3\n\n1\n\n2\n\n "}, {"Page_number": 323, "text": "308\n\nlogit models for multinomial responses\n\n7.14 analyze table 7.8 with a cumulative probit model. compare interpre-\n\ntations to those in the text with other ordinal models.\n\n7.15 fit a model with complementary log-log link to table 7.20, which\nshows family income distributions by percent for families in the north-\neast u.s. interpret the difference between the income distributions.\n\ntable 7.20 data for problem 7.15\n\nyear\n1960\n1970\n\n0\u13903\n6.5\n4.3\n\n3\u13905\n8.2\n6.0\n\n5\u13907\n11.3\n7.7\n\n\u017e\n\n.\nincome $1000\n10\u139012\n15.6\n10.5\n\n7\u139010\n23.5\n13.2\n\n12\u139015\n12.7\n16.3\n\n15 q\n22.2\n42.1\n\nsource: reproduced with permission from the royal statistical society, london mccullagh\n.\n1980 .\n\n\u017e\n\n7.16 table 7.21 shows results of fitting the mean response model to table\n7.8 using scores 3, 10, 20, 35 for income and 1, 3, 4, 5 for job satisfac-\ntion. interpret the income effect, provide a confidence interval for the\ndifference in mean satisfaction at income levels 35 and 3, controlling\nfor gender, and check the model fit.\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ntable 7.21 results for problem 7.16\n\nsource\n\nresidual\n\ndf\n\n5\n\nchi- square\n\npr > chisq\n\n6.99\n\n0.2211\n\neffect\n\nintercept\ngender\nincome\n\nanalysis of weighted least squares estimates\nchi- square\n\nstd error\n\nestimate\n\nparameter\n\n1\n2\n3\n\n3.8076\ny0.0687\n0.0160\n\n0.1796\n0.1419\n0.0066\n\n449.47\n0.23\n5.97\n\npr > chisq\n\n<.0001\n0.6283\n0.0146\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n7.17 the book\u2019s web site www.stat.ufl.edur;aarcdarcda.html has a 3 =\n4 = 4 table that cross-classifies dumping severity y and operation\n\u017e\nx for four hospitals h . the four operations refer to treatments for\nduodenal ulcer patients and have a natural ordering. dumping severity\ndescribes a possible undesirable side effect of the operation. its three\ncategories are also ordered.\na. table 7.22 shows results of generalized cmh tests. interpret,\nexplaining how one test can be much more significant than the\nothers.\n\n\u017e\n\n.\n\n "}, {"Page_number": 324, "text": "problems\n\ntable 7.22 results for problem 7.17\n\n309\n\nsummary statistics for dumping by operate\n\ncontrolling for hospital\n\nstatistic\n\nalternative hypothesis\n\ndf\n\nvalue\n\n1\n2\n3\n\nnonzero correlation\nrow mean scores differ\ngeneral association\n\n1\n3\n6\n\n6.3404\n6.5901\n10.5983\n\nprob\n\n0.0118\n0.0862\n0.1016\n\nb. let x s i . fit the model\n\n\u0004\n\ni\n\n4\nlogit p y f j h s h, x s i s \u2423 q \u242e q \u2424x .\n\n\u017e\n\n.\n\n<\n\nj\n\nh\n\ni\n\ntest conditional independence of x and y using it, and interpret\n\u02c6\u2424. which generalized cmh test has the same spirit as this?\n\nc. does an improved fit result from allowing the operation effect to\n\nvary by hospital? interpret.\n\nd. find a mean response model that fits well. interpret.\n\n7.18 table 7.23 refers to a study that randomly assigned subjects to a\ncontrol or treatment group. daily during the study, treatment subjects\nate cereal containing psyllium. the study analyzed the effect on ldl\ncholesterol.\na. model the ending cholesterol level as a function of treatment, using\n\nthe beginning level as a covariate. interpret the treatment effect.\n\nb. repeat part a , now treating the beginning level as qualitative.\n\n\u017e .\ncompare results.\n\n\u017e .\n\nc. an alternative to part b uses a generalized cmh test relating\ntreatment to the ending response for partial tables defined by\nbeginning cholesterol level. apply such a test, taking into account\nthe response ordering, to compare treatments. interpret, and com-\n\u017e .\npare to part b .\n\ntable 7.23 data for problem 7.18\n\nending ldl cholesterol level\n\nbeginning f 3.4\n18\n16\n0\n0\n\nf 3.4\n3.4\u13904.1\n4.1\u13904.9\n) 4.9\n\ncontrol\n\n3.4\u13904.1\n\n4.1\u13904.9 ) 4.9\n\n8\n30\n14\n2\n\n0\n13\n28\n15\n\n0\n2\n7\n22\n\nsource: data courtesy of sallee anderson, kellogg co.\n\ntreatment\n\n3.4\u13904.1\n\n4.1\u13904.9 ) 4.9\n\n4\n25\n35\n5\n\n2\n6\n36\n14\n\n0\n0\n6\n12\n\n3.4\n21\n17\n11\n1\n\n "}, {"Page_number": 325, "text": "310\n\nlogit models for multinomial responses\n\n7.19 analyze table 7.5 with each type of model studied in this chapter.\nwrite a report summarizing results and advantages and disadvantages\nof each modeling strategy.\n\n\u017e\n\n7.20 the book\u2019s web site www.stat.ufl.edur;aarcdarcda.html has a 4 =\n4 = 5 table that cross-classifies assessment of cognitive impairment,\n\u017e .\nalzheimer\u2019s disease, and age. analyze these data,\na\nalzheimer\u2019s disease, and b cognitive impairment, as the response\nvariable.\n\ntreating\n\n\u017e .\n\n.\n\n7.21 analyze table 9.5 using logit models that treat a party affiliation, and\n\n\u017e .\n\n\u017e .b ideology, as the response variable.\n\n\u017e\n\n7.22 the book\u2019s web site www.stat.ufl.edur;aarcdarcda.html has a 4 =\n2 = 3 = 3 table that refers to a sample of residents of copenhagen.\nthe variables are type of housing h , degree of contact with other\n\u017e .\nresidents c , feeling of influence on apartment management\ni , and\nsatisfaction with housing conditions s . treating s as the response\nvariable, analyze these data.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n7.23 refer to table 7.17. analyze these data.\n\ntheory and methods\n\n7.24 a multivariate generalization of the exponential dispersion family\n\n\u017e\n4.14 is\n\n.\n\nf y ; \u242a , \u243e s exp y \u242a y b \u242a ra \u243e q c y , \u243e ,\n4\n.\n\u017e\n\n\u0004\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nx\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nwhere \u242a is the natural parameter. show that the multinomial variate y\ni\nj s\ndefined in section 7.1.5 for a single trial with parameters \u2432,\nj\n1, . . . , j y 1 is in the j y 1 -parameter exponential family, with base-\nline-category logits as natural parameters.\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n7.25 cell counts\n\n4\n\n\u0004\n\ny\n\n\u2432 distribution. show that p y s n , i s 1, . . . , i,\n\u0004\ncan be expressed as\n\n\u017e\nin an i = j contingency table have a multinomial n;\nj s 1, . . . , j\n4\n\n4.\n\n\u017e\n\n.\n\n\u0004\n\ni j\n\ni j\n\ni j\n\ni j\n\nnd n!\n\n\u0142 \u0142\n\ni\n\nj\n\ny1\n\n\u017e\n\nn !\ni j\n\n.\n\nexp\n\niy1 jy1\n\u00fd \u00fd\nis1 js1\n\nn log \u2423\u017e\ni j\ni j\n\n.\n\nq n log \u2432 r\u2432 q n log \u2432 r\u2432\n\n\u017e\n\n.\n\n\u017e\n\niq\n\nqj\n\ni j\n\ni j\n\ni j\n\ni j\n\n.\n\niy1\n\u00fd\nis1\n\njy1\n\u00fd\njs1\n\n "}, {"Page_number": 326, "text": "problems\n\n311\nwhere \u2423 s \u2432 \u2432 r\u2432 \u2432 and d is a constant independent of the\ndata. find an alternative expression using local odds ratios \u242a , by\nshowing that\n\ni j\n\ni j\n\ni j\n\n\u0004\n\n4\n\ni j\n\ni j\n\ni j\n\n\u00fd \u00fd\n\nn log \u2423 s\n\ni j\n\ni j\n\n\u00fd \u00fd\n\ns log\u242a , where\ni j\n\ni j\n\ns s\n\ni j\n\n\u00fd \u00fd\nafi bfj\n\nn .\nab\n\ni\n\nj\n\ni\n\nj\n\n7.26 suppose that we express 7.2 as\n\n\u017e\n\n.\n\n\u2432 x s\n\n\u017e .\n\nj\n\n\u017e\n\nexp \u2423 q \u2424x x\nj\u00fd exp \u2423 q \u2424 x\nhs1\n\nj\n\u017e\n\nx\nh\n\n.\n\nh\n\nj\n\n.\n\n.\n\nshow that dividing numerator and denominator by exp \u2423 q \u2424 x\n.\nyields new parameters \u2423* s \u2423 y \u2423 and \u2424* s \u2424 y \u2424 that satisfy\n\u2423 s 0 and \u2424 s 0. thus, without\nloss of generality, \u2423 s 0 and\n\u2424 s 0.\n\nx\nj\n\n\u017e\n\nj\n\nj\n\nj\n\nj\n\nj\n\nj\n\nj\n\nj\n\nj\n\nj\n\nj\n\n7.27 when j s 3, suppose that\n\n\u2432 x s exp \u2423 q \u2424 x r 1 q exp \u2423 q \u2424 x q exp \u2423 q \u2424 x\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n,\n\n.\n\n\u017e\n\nj\n\n1\n\n1\n\n2\n\n2\n\nj\n\nj\n\nj s 1, 2. show that \u2432 x is a decreasing in x if \u2424 ) 0 and \u2424 ) 0,\n\u017e .\nb increasing in x if \u2424 - 0 and \u2424 - 0, and c nonmonotone when\n\u2424 and \u2424 have different signs.\n1\n\n\u017e .\n\n\u017e .\n\n\u017e\n\n.\n\n3\n\n1\n\n2\n\n1\n\n2\n\n2\n\n\u017e\n\n7.28 refer to the log-likelihood function for the baseline-category logit\nmodel section 7.1.4 . denote the sufficient statistics by np s \u00fd y\n.\ni j\nand s s \u00fd x\nj s 1, . . . j y 1, k s 1, . . . , p. let s s s , . . . ,\ni\n. condition on \u00fd y , j s 1, . . . , j. under the null\ns , . . . s , . . .\n1t\nhypothesis that explanatory variables have no effect, show that\n\ny ,\nik\ni j\n.x\n, s\nj t\n\nj\n\u017e\n\nj 1\n\n11\n\njk\n\ni j\n\ni\n\ni\n\ne s s n p m m ,\n\u017e\n.\n\n.\n\n\u017e\n\nvar s s n v m \u233a ,\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\nx\n\nwhere p s p , . . . , p ; m s x , . . . , x\n, where x s \u00fd x rn; \u233a\n\u017e\n\u017e\nx y x r n y 1 ; v\n2\nhas elements\nk \u00ae\ni\u00ae\nhas elements \u00ae s p 1 y p\nand \u00ae s y p p , and m denotes the\n.\nkronecker product zelen 1991 .\n\n, where s s \u00fd x y x\n.\n\n\u017e\n.x\n\ns\nii\n\n2\nk \u00ae\n\n1\n\u017e\n\n.\u017e\n\n.\nx\n\ni\n\u017e\n\nik\n\nik\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\nw\n\ni j\n\n\u00ae\n\n1\n\nk\n\nk\n\nj\n\nt\n\ni\n\ni\n\ni\n\ni\n\nj\n\n7.29 is the proportional odds model a special case of a baseline-category\n\nlogit model? explain why or why not.\n\n7.30 prove factorization 7.15 for the multinomial distribution.\n\n\u017e\n\n.\n\n "}, {"Page_number": 327, "text": "312\nlogit models for multinomial responses\n7.31 show that for the model, logit p y f j s \u2423 q \u2424 x, cumulative prob-\n\n.x\n\n\u017e\n\nw\n\nj\n\nj\n\nabilities may be misordered for some x values.\n\n7.32 for an i = j contingency table with ordinal y and scores\n\nx, consider the model\n\nlogit p y f j x s x s \u2423 q \u2424x .\n\n<\n\n\u017e\n\n.i\n\ni\n\nj\n\n\u0004\n\nx s i\n\ni\n\n4\n\nfor\n\n\u017e\n\n7.24\n\n.\n\nw\n\nw\n\n.x\n\n.x\n\n<\n\n<\n\ni\n\n\u017e\n\n\u017e\n\niq1\n\na. show that logit p y f j x s x\n\ny logit p y f j x s x s \u2424.\nshow that this difference in logits is a log cumulative odds ratio for\nthe 2 = 2 table consisting of rows i and i q 1 and the binary\nresponse having cutpoint following category j. thus, 7.24 is a\nuniform association model in cumulative odds ratios.\n\nb. show that residual df s ij y i y j.\nc. show that independence of x and y is the special case \u2424s 0.\nd. using the same linear predictor but with adjacent-categories logits,\n.\nshow that uniform association applies to the local odds ratios 2.10 .\ne. a generalization of 7.24 replaces \u2424x by unordered parameters\n\u0004\n4\u242e , hence treating x as nominal. for rows a and b, show that the\ni\nlog cumulative odds ratio equals \u242e y \u242e for all j y 1 cutpoints.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u0004\n\n4\n\ni\n\na\n\nb\n\n\u017e\n\n.\n\n7.33 suppose that model 7.24 holds for a 2 = j table with j ) 2, and let\nx y x s 1. explain why local log odds ratios are typically smaller in\n2\nabsolute value than the cumulative log odds ratio \u2424. in fact, on p. 122\nof their first edition, mccullagh and nelder 1989 noted that local\nodds ratios \u242a relate to \u2424 by\n\nw\n.\n\n\u017e\n\n4\n\n\u0004\n\n1\n\n1 j\n\nlog\u242a s \u2424 p y f j q 1 y p y f j y 1 q o \u2424 ,\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n1 j\n\nj s 1, . . . , j y 1,\n\nwhere o \u2424r\u2424\u2122 0 as \u2424\u2122 0.\n\n\u017e\n\n.\n\nx\n\n7.34 a response scale has the categories\n\n\u017e\nstrongly agree, mildly agree,\nmildly disagree, strongly disagree, don\u2019t know . one way to model such\na scale uses a logit model for the probability of a don\u2019t know response\nand uses a separate ordinal model for the ordered categories condi-\ntional on response in one of those categories. explain how to construct\na likelihood to do this simultaneously.\n\n.\n\n7.35 for the cumulative probit model \u233d p y f j s \u2423 y \u2424 x, explain\nwhy a 1-unit increase in x corresponds to a \u2424 standard deviation\nincrease in the expected underlying latent response, controlling for\nother predictors.\n\n\u017e\n\nx\n\ni\n\ni\n\nj\n\ny1w\n\n.x\n\n "}, {"Page_number": 328, "text": "313\nproblems\n7.36 for cumulative link model 7.7 , show that for 1 f j - k f j y 1,\np y f k x s p y f j x* , where x* is obtained by increasing the ith\n\u017e\ncomponent of x by \u2423 y \u2423 r\u2424. interpret.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n<\n\n<\n\nk\n\nj\n\ni\n\n7.37 a cumulative link model for an i = j contingency table with a qualita-\n\ntive predictor is\n\ny1g\n\np y f j s \u2423 q \u242e ,\n\u017e\n\n.\n\ni\n\nj\n\ni s 1, . . . , i, j s 1, . . . , j y 1 .\n\na. show that the residual df s i y 1 j y 2 .\n.\nb. when this model holds, show that independence corresponds to\n\n\u242e s \u2b48\u2b48\u2b48 s \u242e and the test of independence has df s i y 1.\n\n.\u017e\n\n\u017e\n\n1\n\ni\n\nc. when this model holds, show that\n\nordered on y.\n\nthe rows are stochastically\n\n.\n\n\u017e\n\n.\n\u017e\n\n\u017e\n1\n\n7.38 f y s 1 y exp y\u242dy\n\nfor y ) 0 is a negative exponential cdf with\nparameter \u242d, and f y s 1 y exp y\u242ey\n.\nfor y ) 0. show that the\ndifference between the cdf\u2019s on a complementary log-log scale is\nidentical for all y. give implications for categorical data analysis.\n7.39 consider the model link \u243b x s \u2423 q \u2424 x, where \u243b x is 7.14 .\n.\n\n\u017e .x\n\n\u017e .\n\n\u017e\n\n.\n\n\u017e\n\nw\n\n2\n\nj\n\nj\n\na. explain why this model can be fitted separately for j s 1, . . . , j y 1.\nb. for the complementary log-log link, show that this model is equiva-\nlent to one using the same link for cumulative probabilities laara\u00a8\u00a8 \u00a8\n.\nand matthews 1985 .\n\n\u017e\n\nx\nj\n\nj\n\n7.40 why is it not optimal to fit mean response models for ordinal re-\nsponses using ordinary least squares as is done for normal regression?\n\n7.41 when x and y are ordinal, explain how to test conditional indepen-\ntable. hint:\n\ntrend in each partial\n\ndence by allowing a different\ngeneralize model 7.17 by replacing \u2424 by \u2424 .k\n\n\u017e\n\n.\n\nw\n\nx\n\n7.42 a cafe has four entrees: chicken, beef, fish, vegetarian. specify a model\nof form 7.22 for the selection of an entree using x s gender 1 s\nfemale, 0 s male and u s cost of entree, which is a characteristic of\nthe choices. interpret the model parameters.\n\n\u00b4\n\n\u00b4\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u00b4\n\n "}, {"Page_number": 329, "text": "c h a p t e r 8\n\nloglinear models for\ncontingency tables\n\n.\n\nin section 4.3 we introduced loglinear models as generalized linear models\n\u017e\nglms using the log link function with a poisson response. a common use is\nmodeling cell counts in contingency tables. the models specify how the\nexpected count depends on levels of the categorical variables for that cell as\nwell as associations and interactions among those variables. the purpose of\nloglinear modeling is the analysis of association and interaction patterns.\n\nin section 8.1 we introduce loglinear models for two-way contingency\ntables. in sections 8.2 and 8.3 we extend them to three-way tables, and in\nsection 8.4 discuss models for multiway tables. loglinear models are of use\nprimarily when at least two variables are response variables. with a single\ncategorical response,\nit is simpler and more natural to use logit models.\nwhen one variable is treated as a response and the others as explanatory\nvariables, logit models for that response variable are equivalent to certain\nloglinear models. section 8.5 covers this connection. in sections 8.6 and 8.7\nwe discuss ml loglinear model fitting.\n\n8.1 loglinear models for two-way tables\n\nconsider an i = j contingency table that cross-classifies a multinomial sam-\n4\nple of n subjects on two categorical responses. the cell probabilities are \u2432\ni j\nand the expected frequencies are \u242e s n\u2432 . loglinear model formulas use\n\u242e rather than \u2432 , so they also apply with poisson sampling for n s ij\n\u0004\nindependent cell counts y having \u242e s e y . in either case we denote\n4\nthe observed cell counts by n .\n\n4\ni j\n\u0004\n\n.4\n\n\u017e\n\n\u0004\n\n\u0004\n\n4\n\n4\n\n\u0004\n\n\u0004\n\n4\n\n\u0004\n\ni j\n\ni j\n\ni j\n\ni j\n\ni j\n\ni j\n\ni j\n\nindependence model\n\n8.1.1\nunder statistical independence, in section 4.3.6 we noted that the \u242e have\nthe structure\n\n\u0004\n\n4\n\ni j\n\n\u242e s \u242e\u2423 \u2424 .\n\ni j\n\ni\n\nj\n\n314\n\n "}, {"Page_number": 330, "text": "loglinear models for two-way tables\n315\nfor multinomial sampling, for instance, \u242e s n\u2432 \u2432 . denote the row\nvariable by x and the column variable by y. the formula expressing\nindependence is multiplicative. thus, log \u242e has additive form\n\niq qj\n\ni j\n\ni j\n\nlog \u242e s \u242dq \u242dx q \u242dy\n\ni j\n\ni\n\nj\n\n8.1\u017e\n\n.\n\ni\n\nj\n\nfor a row effect \u242dx and a column effect \u242dy. this is the loglinear model\nof independence. as usual, identifiability requires constraints such as \u242dx s\n\u242dy s 0.\nthe ml fitted values are \u242e s n n rn , the estimated expected fre-\nquencies for chi-squared tests of independence. the tests using x 2 and g2\n\u017e\nsection 3.2.1 are also goodness-of-fit tests of this loglinear model.\n\niq qj\n\n\u02c6i j\n\n.\n\n\u0004\n\n4\n\ni\n\nj\n\ninterpretation of parameters\n\n8.1.2\nloglinear models for contingency tables are glms that treat the n cell\ncounts as independent observations of a poisson random component. loglin-\near glms identify the data as the n cell counts rather than the individual\nclassifications of the n subjects. the expected cell counts link to the explana-\ntory terms using the log link. as 8.1 illustrates, of the cross-classified\nvariables, the model does not distinguish between response and explanatory\nvariables. it treats both jointly as responses, modeling \u242e for combinations\nof their levels. to interpret parameters, however, it is helpful to treat the\nvariables asymmetrically.\n\nwe illustrate with the independence model for i = 2 tables. in row i, the\n\n\u017e\n\n.\n\n\u0004\n\n4\n\ni j\n\nlogit equals\n\nlogit p y s 1 x s i s log\n\n.\n\n\u017e\n\n<\n\n.\n.\n\n<\n\n<\n\np y s 1 x s i\n\u017e\np y s 2 x s i\n\u017e\n\u242e\ni1\n\u242e\ni2\n\ni1\n\ns log s log \u242e y log \u242e\n\ni2\n\ni\n\ns \u242dq \u242dx q \u242dy y \u242dq \u242dx q \u242dy s\u242dy y \u242dy.\n\n.\n\n1\n\n.\n\n2\n\n\u017e\n\n\u017e\n\n1\n\n2\n\ni\n\nlogit p y s 1 x s i\n\n\u017e\n\n<\n\nw\n\n.x\n\n<\n\nw\n\n\u017e\n\n\u017e .\n\n.x\n\u017e y\n1\n\nis\nthe final term does not depend on i; that is,\nidentical at each level of x. thus, independence implies a model of form,\nlogit p y s 1 x s i s \u2423. in each row, the odds of response in column 1\nequal exp \u2423 s exp \u242d y \u242d .\ny .\n2\n\nan analogous property holds when j ) 2. differences between two pa-\nrameters for a given variable relate to the log odds of making one response,\nrelative to the other, on that variable. of course, with a single response\nvariable, logit models apply directly and loglinear models are unneeded.\n\n "}, {"Page_number": 331, "text": "316\n\nloglinear models for contingency tables\n\n8.1.3 saturated model\nstatistically dependent variables satisfy a more complex loglinear model,\n\nlog \u242e s \u242dq \u242dx q \u242dy q \u242dx y.\n\nj\n\ni j\n\ni j\n\ni\n\n8.2\u017e\n\n.\n\n\u0004 x y 4\n\ni j\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\ny\nj\n\nx\ni\n\n\u0004 x y 4\n\nx y\ni j\n\u0004 x 4\ni\n\u017e\n\nthe \u242d\nare association terms that reflect deviations from independence.\ni j\nthe right-hand side of 8.2 resembles the formula for cell means in two-way\nanova, allowing interaction. the \u242d\nrepresent interactions between x\ni j\nand y, whereby the effect of one variable on \u242e depends on the level of the\nother. the independence model 8.1 results when all \u242d s 0.\nwith constraints \u242d s \u242d s 0 in 8.1 and 8.2 , \u242d and \u242d are,\ni y 1 categories\nequivalently, coefficients of dummy variables for the first\nj y 1 categories of y. thus, \u242d is the coefficient of the\nof x and the first\nproduct of dummy variables for \u242d and \u242d . since there are i y 1 j y 1\n.\nsuch cross products, \u242d s \u242d s 0, and only\nthese\nparameters are nonredundant. tests of\nindependence analyze whether\ni y 1 j y 1 parameters equal zero, so they have residual df s\n.\u017e\n\u017e\nthese\ni y 1 j y 1 .\n\u017e\n.\n.\u017e\nthe number of parameters in model 8.2 equals 1 q i y 1 q j y 1 q\ni y 1 j y 1 s ij, the number of cells. hence, this model describes per-\n\u017e\nfectly any \u242e ) 0\nsee problem 8.16 . it is the most general model for\ntwo-way contingency tables, the saturated model. for it, direct relationships\nexist between log odds ratios and \u242d . for instance, for 2 = 2 tables,\n\n.\u017e\ni y 1 j y 1 of\n\n\u0004 x y 4\n\n\u0004 y 4\nj\n\n4 \u017e\n\nx y\ni j\n\nx y\ni j\n\nx y\ni j\n\n.\u017e\n\n.\u017e\n\nx\ni\n\ny\nj\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u0004\n\ni j\n\nlog \u242as log\n\n11\n\n\u242e \u242e\n22\n\u242e \u242e\n21\n\n12\n\ns log \u242e q log \u242e y log \u242e y log \u242e\n\n22\n\n12\n\n21\n\n1\n\n1\n\n.\n\ns \u242dq \u242dx q \u242dy q \u242dx y q \u242dq \u242dx q \u242dy q \u242dx y\n22\n\n\u017e\n.\ny \u242dq \u242dx q \u242dy q \u242dx y y \u242dq \u242dx q \u242dy q \u242dx y\n21\n\n.\ns \u242dx y q \u242dx y y \u242dx y y \u242dx y.\n\n\u017e\n\n\u017e\n\n\u017e\n\n11\n\n12\n\n2\n\n2\n\n1\n\n2\n\n2\n\n1\n\n22\n\n12\n\n11\n\n21\n\n.\n\n8.3\u017e\n\n.\n\nthus, \u242d\ni j\n\n\u0004 x y 4\n\ndetermine the association.\n\n.\n\nin practice, unsaturated models are preferable, since their fit smooths the\nsample data and has simpler interpretations. for tables with at least three\nvariables, unsaturated models can include association terms. then, loglinear\n\u017e\nmodels are more commonly used to describe associations through two-factor\n.\nterms than to describe odds through single-factor terms .\n\nlike others in this book, model 8.2 is hierarchical. this means that the\nmodel includes all lower-order terms composed from variables contained in a\nhigher-order model term. when the model contains \u242dx y, it also contains \u242dx\ni\nand \u242dy. a reason for including lower-order terms is that, otherwise, the\nstatistical significance and the interpretation of a higher-order term depends\non how variables are coded. this is undesirable, and with hierarchical models\nthe same results occur no matter how variables are coded.\n\n\u017e\n\n\u017e\n\n.\n\ni j\n\nj\n\ni j\n\n11\n\n "}, {"Page_number": 332, "text": "loglinear models for two-way tables\n\n317\n\nan example of a nonhierarchical model is\n\nlog \u242e s \u242dq \u242dx q \u242dx y.\n\ni j\n\ni j\n\ni\n\ni j\n\nthis model permits association but forces unnatural behavior of expected\nfrequencies, with the pattern depending on constraints used for parameters.\nfor instance, with constraints whereby parameters are zero at the last level,\nlog \u242e s \u242d in every column. nonhierarchical models are rarely sensible in\npractice. using them is analogous to using anova or regression models\nwith interaction terms but without the corresponding main effects.\n\nwhen a model has two-factor terms, interpretations focus on them rather\nthan on the single-factor terms. by analogy with two-way anova with\ntwo-factor interaction,\nit can be misleading to report main effects. the\nestimates of the main-effect terms depend on the coding scheme used for the\nhigher-order effects, and the interpretation also depends on that scheme see\nproblem 8.16 . normally, we restrict our attention to the highest-order terms\nfor a variable, as we illustrate in section 8.2.\n\n.\n\n\u017e\n\nj\n\ni\n\ni j\n\ni j\n\ni j\n\ni j\n\n8.1.4 alternative parameter constraints\nas with the independence model, the parameter constraints for the saturated\nmodel are arbitrary. instead of setting all \u242dx y s \u242dx y s 0, one could set\n\u00fd \u242dx y s \u00fd \u242dx y s 0 for all\ni and j. different software uses different con-\nstraints. what is unique are contrasts such as \u242dx y q \u242dx y y \u242dx y y \u242dx y in\n\u017e\n.8.3 that determine odds ratios.\n\nfor instance, suppose that a log odds ratio equals 2.0 in a 2 = 2 table.\nwith the first set of constraints, 2.0 is the coefficient of a product of a\ndummy variable indicating the first category of x and a dummy variable\nindicating the first category of y. with it, \u242dx y s 2.0 and \u242dx y s \u242dx y s \u242dx y\n22\ns 0. for sum-to-zero constraints, \u242dx y s \u242dx y s 0.5, \u242dx y s \u242dx y s y0.5. for\neither set, the log odds ratio 8.3 equals 2.0. for a set of parameters, an\nadvantage of setting a baseline parameter equal to 0 instead of the sum equal\nto 0 is that some parameters in a set can have infinite estimates.\n\n11\n\n22\n\n12\n\n21\n\n11\n\n12\n\n21\n\n11\n\n22\n\n12\n\n21\n\n\u017e\n\n.\n\n8.1.5 multinomial models for cell probabilities\nconditional on the sum n of the cell counts, poisson loglinear models for\n\u242e become multinomial models for cell probabilities \u2432 s \u242e r \u00fd\u00fd \u242e .\n\u0004\n.4\nto illustrate, for the saturated model,\n\nab\n\n\u017e\n\n\u0004\n\n4\n\ni j\n\ni j\n\ni j\n\n\u2432 s\n\ni j\n\n.\n\u017e\nexp \u242dq \u242dx q \u242dy q \u242dx y\nexp \u242dq \u242d q \u242d q \u242d\n\ni j\n\ni\n\nj\n\n\u017e\n\nx\na\n\ny\nb\n\n\u00fd \u00fd\n\nx y\nab\n\na\n\nb\n\n.\n\n.\n\n8.4\u017e\n\n.\n\n "}, {"Page_number": 333, "text": "318\nloglinear models for contingency tables\nthis representation implies the usual constraints for probabilities, \u2432 g 0\n4\nand \u00fd \u00fd \u2432 s 1. the \u242d intercept parameter cancels in the multinomial\nmodel 8.4 . this parameter relates purely to the total sample size, which is\nrandom in the poisson model but not in the multinomial model.\n\ni\n\u017e\n\n.\n\n\u0004\n\ni j\n\ni j\n\nj\n\n8.2 loglinear models for independence and\ninteraction in three-way tables\n\nin section 2.3 we introduced three-way contingency tables and related\nstructure such as conditional independence and homogeneous association.\nloglinear models for three-way tables describe their independence and\nassociation patterns.\n\n8.2.1 types of independence\na three-way i = j = k cross-classification of response variables x, y, and z\nhas several potential types of independence. we assume a multinomial\ndistribution with cell probabilities \u2432 , and \u00fd \u00fd \u00fd \u2432 s 1.0. the models\ni\n4\nalso apply to poisson sampling with means \u242e .\n\ni jk\n\ni jk\n\n\u0004\n\n4\n\n\u0004\n\nk\n\nj\n\ni jk\n\nthe three variables are mutually independent when\n\n\u2432 s \u2432 \u2432 \u2432\n\niqq qjq qqk\n\ni jk\n\nfor all i, j, and k.\n\n8.5\u017e\n\n.\n\nfor expected frequencies \u242e , mutual independence has loglinear form\n\n\u0004\n\n4\n\ni jk\n\nlog \u242e s \u242dq \u242dx q \u242dy q \u242dz.\n\ni jk\n\nk\n\ni\n\nj\n\nvariable y is jointly independent of x and z when\n\n\u2432 s \u2432 \u2432\n\niqk qjq\n\ni jk\n\nfor all i, j, and k.\n\n8.6\u017e\n\n.\n\n8.7\u017e\n\n.\n\nthis is ordinary two-way independence between y and a variable composed\nof the ik combinations of levels of x and z. the loglinear model is\n\nlog \u242e s \u242dq \u242dx q \u242dy q \u242dz q \u242dx z.\n\nk\n\nik\n\ni jk\n\ni\n\nj\n\n8.8\u017e\n\n.\n\nsimilarly, x could be jointly independent of y and z, or z could be jointly\nindependent of x and y. mutual independence 8.5 implies joint indepen-\ndence of any one variable from the others.\nfrom section 2.3, x and y are conditionally independent, gi\u00aeen z when\nindependence holds for each partial table within which z is fixed. that is, if\n\u2432 s p x s i, y s j z s k , then\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n<\n\ni j < k\n\n\u2432 s \u2432 \u2432\n\niq< k qj < k\n\ni j < k\n\nfor all i, j, and k.\n\n "}, {"Page_number": 334, "text": "loglinear models for three-way tables\n\nfor joint probabilities over the entire table, equivalently\n\n\u2432 s \u2432 \u2432 r\u2432\n\niqk qj k\n\ni jk\n\nqqk\n\nfor all i, j, and k.\n\n319\n\n8.9\u017e\n\n.\n\nconditional independence of x and y, given z, is the loglinear model\n\nlog \u242e s \u242dq \u242dx q \u242dy q \u242dz q \u242dx z q \u242dy z.\n\nk\n\nik\n\ni jk\n\njk\n\ni\n\nj\n\n\u017e\n\n8.10\n\n.\n\nthis is a weaker condition than mutual or joint independence. mutual\nindependence implies that y is jointly independent of x and z, which itself\nimplies that x and y are conditionally independent. table 8.1 summarizes\nthese three types of independence.\n\nin section 2.3.2 we showed that partial associations can be quite different\nfrom marginal associations. for instance, conditional independence does not\nimply marginal independence. conditional independence and marginal inde-\npendence both hold when one of the stronger types of independence studied\nabove applies. figure 8.1 summarizes relationships among the four types of\nindependence.\n\n8.2.2 homogeneous association and three-factor interaction\nloglinear models 8.6 , 8.8 , and 8.10 have three, two, and one pair of\nconditionally independent variables, respectively. in the latter two models,\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ntable 8.1 summary of loglinear independence models\n\nmodel\n\u017e\n.8.6\n\u017e\n.8.8\n\u017e\n8.10\n\n.\n\nprobabilistic\nform for \u2432\ni jk\n\u2432 \u2432 \u2432\n\u2432 \u2432\n\u2432 \u2432 r\u2432\n\niqq qjq qqk\niqk qjq\niqk qj k\n\nqqk\n\nassociation terms\nin loglinear model\n\ninterpretation\n\nnone\nx z\n\u242d\ni k\n\u242d q \u242d\nx z\ny z\njk\ni k\n\nvariables mutually independent\ny independent of x and z\nx and y independent, given z\n\nfigure 8.1 relationships among types of xy independence.\n\n "}, {"Page_number": 335, "text": "320\n\nloglinear models for contingency tables\n\nx y .\nthe doubly subscripted terms such as \u242d\npertain to conditionally depen-\ni j\ndent variables. a model that permits all three pairs to be conditionally\ndependent is\n\n\u017e\n\nlog \u242e s \u242dq \u242dx q \u242dy q \u242dz q \u242dx y q \u242dx z q \u242dy z.\n\nk\n\ni j\n\ni jk\n\nik\n\njk\n\ni\n\nj\n\n\u017e\n\n8.11\n\n.\n\nfrom exponentiating both sides, the cell probabilities have form\n\n\u2432 s \u243a \u243e \u243b .\n\ni jk\n\nik\n\njk\n\ni j\n\n4\n\n\u0004\n\ni jk\n\nno closed-form expression exists for the three components in terms of\n.\nmargins of \u2432 except in certain special cases see note 9.2 .\n\nfor this model, in the next section we show that conditional odds ratios\nbetween any two variables are identical at each category of the third variable.\nthat is, each pair has homogeneous association section 2.3.5 . model 8.11 is\ncalled the loglinear model of homogeneous association or of no three-factor\ninteraction.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nthe general loglinear model for a three-way table is\n\nlog \u242e s \u242dq \u242dx q \u242dy q \u242dz q\u242dx y q \u242dx z q \u242dy z q \u242dx y z.\n\nj\n\nk\n\ni j\n\ni jk\n\ni jk\n\nik\n\njk\n\ni\n\n\u017e\n\n8.12\n\n.\n\ni jk\n\nwith dummy variables, \u242dx y z is the coefficient of the product of the ith\ndummy variable for x,\njth dummy variable for y, and kth dummy variable\nfor z. the total number of nonredundant parameters is\n1 q i y 1 q j y 1 q k y 1 q i y 1\n. \u017e\n\n\u017e\nq j y 1 k y 1 q i y 1\n\n\u017e\nj y 1 k y 1 s ijk ,\n\nj y 1 q i y 1 k y 1\n\n\u017e\n. \u017e\n\n.\n. \u017e\n\n.\n.\n\n\u017e\n\u017e\n\n. \u017e\n\n. \u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\nthe total number of cell counts. this model has as many parameters as\nobservations and is saturated. it describes all possible positive \u242e . each\npair of variables may be conditionally dependent, and an odds ratio for any\npair may vary across categories of the third variable.\n\u017e\n\nsetting certain parameters equal to zero in 8.12 yields the models\nintroduced previously. table 8.2 lists some of these models. to ease referring\nto models, table 8.2 assigns to each model a symbol that lists the highest-order\n\ni jk\n\n.\n\n\u0004\n\n4\n\ntable 8.2 loglinear models for three-dimensional tables\n\ni jk\n\nloglinear model\nlog \u242e s \u242dq \u242d q \u242d q \u242d\nx\ni\nlog \u242e s \u242dq \u242d q \u242d q \u242d q \u242d\nx\ni\nlog \u242e s \u242dq \u242d q \u242d q \u242d q \u242d q \u242d\nx\ni\nlog \u242e s \u242dq \u242d q \u242d q \u242d q \u242d q \u242d q \u242d\nx\ni\nlog \u242e s \u242dq \u242d q \u242d q \u242d q \u242d q \u242d q \u242d q \u242d\nx\ni\n\nx y\ni j\nx y\ni j\nx y\ni j\nx y\ni j\n\nyz\njk\nyz\njk\nyz\njk\n\nz\nk\nz\nk\nz\nk\nz\nk\nz\nk\n\nxz\nik\nxz\nik\n\ny\nj\ny\nj\ny\nj\ny\nj\ny\nj\n\ni jk\n\ni jk\n\ni jk\n\ni jk\n\nx y z\ni jk\n\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\nsymbol\n.\n\nx, y, z\n.\nxy, z\nxy, yz\nxy, yz, xz\nxyz\n\n.\n\n.\n\n.\n\n "}, {"Page_number": 336, "text": "loglinear models for three-way tables\n\n321\n\n\u017e .\nterm s\nfor each variable. for instance, the model 8.10 of conditional\nindependence between x and y has symbol xz, yz , since its highest-order\nterms are \u242dx z and \u242dy z. in the notation we used for logit models in sections\n6.1 and 7.1.2 this stands for x *z q y *z , which is itself shorthand for\nnotation x q y q z q x = z q y = z that has the main effects as well as\ninteractions.\n\nik\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\njk\n\ninterpreting model parameters\n\n8.2.3\ninterpretations of loglinear model parameters use their highest-order terms.\nfor instance, interpretations for model 8.11 use the two-factor terms to\ndescribe conditional odds ratios. at a fixed level k of z, the conditional\ni y 1 j y 1 odds ratios, such as the\nassociation between x and y uses\nlocal odds ratios\n\n.\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u242a s\ni j\u017e k. \u2432\n\n\u2432 \u2432\n\ni jk\ni , jq1 ,k\n\niq1 , jq1 , k\niq1 , j, k\n\n\u2432\n\n,\n\n1 f i f i y 1,\n\n1 f j f j y 1.\n\n\u017e\n\n8.13\n\n.\n\n.\n\n.\n\n.\u017e\n\n.\u017e\n\ni\u017e j.k\n\ni y 1 k y 1 odds ratios \u242a\n\u0004\n\u017e\n\n4\nj y 1 k y 1 odds ratios \u242a\n\u0004\n\n\u017e\ndescribe xz conditional associ-\nsimilarly,\nation, and\ndescribe yz conditional\n\u017ei. jk\nassociation. loglinear models have characterizations using constraints on\nconditional odds ratios. for instance, conditional independence of x and y\nis equivalent to \u242a s 1, i s 1, . . . , i y 1,\nj s 1, . . . , j y 1, k s 1, . . . , k .\n4\nthe two-factor parameters relate directly to the conditional odds ratios.\nyields\n\nto illustrate, substituting 8.11 for model xy, xz, yz into log \u242a\n\ni j\u017e k.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n4\n\n\u0004\n\ni j\u017e k.\n\nlog \u242a s log\n\ni j\u017e k.\n\n\u242e \u242e\n\ni jk\niq1 , jk\n\n\u242e\n\niq1 , jq1 ,k\n\u242e\n1, jq1 , k\n\ns \u242d q \u242d\n\nx y\ni j\n\nx y\niq1 , jq1\n\ny \u242d y \u242d\n\nx y\ni , jq1\n\nx y\niq1 , j\n\n.\n\n\u017e\n\n8.14\n\n.\n\nsince the right-hand side is the same for all k, an absence of three-factor\ninteraction is equivalent to\n\n\u242a s \u242a s \u2b48\u2b48\u2b48 s \u242a\ni j\u017e1.\n\ni j\u017e2.\n\ni j\u017e k .\n\nfor all i and j.\n\nthe same argument for the other conditional odds ratios shows that model\n\u017e\nxy, xz, yz is also equivalent to\n\n.\n\n\u242a s \u242a s \u2b48\u2b48\u2b48 s \u242a\ni\u017e1. k\n\ni\u017e2. k\n\ni\u017e j .k\n\nand to\n\n\u242a s \u242a s \u2b48\u2b48\u2b48 s \u242a\n\u017e1. jk\n\n\u017e2. jk\n\n\u017e i . jk\n\nfor all i and k,\n\nfor all j and k.\n\nany model not having the three-factor interaction term has a homogeneous\nassociation for each pair of variables.\n\n "}, {"Page_number": 337, "text": "322\n\nloglinear models for contingency tables\n\n.\n\n\u017e\n\nwhen x and y have two categories, only one nonredundant \u242dx y parame-\nter occurs. thus, expression 8.14 is simplified depending on the constraints.\nby the same argument as in section 8.1.3 for 2 = 2 tables, the conditional log\nodds ratio simplifies to \u242dx y with dummy-variable constraints setting parame-\nters at the second level of x or y equal to 0.\n.\n\nterm in the general model 8.12 refers to three-factor interac-\ntion. it describes how the odds ratio between two variables changes across\ncategories of the third. we illustrate for 2 = 2 = 2 tables. by direct substitu-\ntion of the general model formula,\n\nthe \u242d\n\nx y z\ni jk\n\n11\n\n\u017e\n\ni j\n\nlog\n\n\u242a\n\n11\u017e1.\n\n\u242a\n\n11\u017e2.\n\ns log\n\n\u017e\n\u017e\n\n\u242e \u242e r \u242e \u242e\n111\n\u242e \u242e r \u242e \u242e\n112\n\n.\n.\n\n\u017e\n\u017e\n\n221\n\n121\n\n222\n\n122\n\n211\n\n212\n\n.\n.\n\n111\n\ns \u242dx y z q \u242dx y z y \u242dx y z y \u242dx y z\n211\n\n\u017e\ny \u242dx y z q \u242dx y z y \u242dx y z y \u242dx y z .\n.\n\n221\n\n121\n\n\u017e\n\n.\n\n112\n\n222\n\n122\n\n212\n\nonly one parameter is nonredundant. for constraints setting the second-cat-\negory parameters equal to 0, this log ratio of odds ratios equals \u242dx y z. when\n\u242dx y z s 0, \u242a s \u242a\n111\n\n, giving homogeneous xy association.\n\n11\u017e2.\n\n11\u017e1.\n\n111\n\n8.2.4 alcohol, cigarette, and marijuana use example\ntable 8.3 refers to a 1992 survey by the wright state university school of\nmedicine and the united health services in dayton, ohio. the survey asked\n2276 students in their final year of high school in a nonurban area near\ndayton, ohio whether they had ever used alcohol, cigarettes, or marijuana.\ndenote the variables in this 2 = 2 = 2 table by a for alcohol use, c for\ncigarette use, and m for marijuana use.\n\nsection 8.7 covers the fitting of loglinear models. for now, we emphasize\ninterpretation. table 8.4 shows fitted values for several loglinear models. the\n\ntable 8.3 alcohol, cigarette, and marijuana use\nfor high school seniors\n\nalcohol\nuse\nyes\n\nno\n\ncigarette\n\nuse\nyes\nno\nyes\nno\n\nmarijuana use\nyes\nno\n538\n911\n456\n44\n43\n3\n2\n279\n\nsource: data courtesy of harry khamis, wright state\nuniversity.\n\n "}, {"Page_number": 338, "text": "loglinear models for three-way tables\n\n323\n\ntable 8.4 fitted values for loglinear models applied to table 8.3\n\nalcohol cigarette marijuana\nuse\nyes\n\nuse\nyes\n\n\u017e\n\nno\n\nno\n\nyes\n\nno\n\nloglinear model\n. \u017e\n\n. \u017e\n\na\n\n. \u017e\n\n. \u017e\n\n.\n\na, c, m ac, m am, cm ac, am, cm acm\n911\n540.0\n740.2\n538\n44\n282.1\n456\n386.7\n3\n90.6\n124.2\n43\n2\n47.3\n64.9\n279\n\n909.24\n438.84\n45.76\n555.16\n4.76\n142.16\n0.24\n179.84\n\n611.2\n837.8\n210.9\n289.1\n19.4\n26.6\n118.5\n162.5\n\n910.4\n538.6\n44.6\n455.4\n3.6\n42.4\n1.4\n279.6\n\nuse\nyes\nno\nyes\nno\nyes\nno\nyes\nno\n\naa, alcohol use; c, cigarette use; m, marijuana use.\n\n\u017e\n\n.\n\nfit for model ac, am, cm is close to the observed data, which are the\nfitted values for the saturated model acm . the other models fit poorly.\n\ntable 8.5 illustrates model association patterns by presenting estimated\nconditional and marginal odds ratios. for example, the entry 1.0 for the ac\nconditional association for the model am, cm of ac conditional indepen-\ndence is the common value of the ac fitted odds ratios at the two levels\nof m,\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n1.0 s\n\n909.24 = 0.24\n45.76 = 4.76\n\ns\n\n438.84 = 179.84\n555.16 = 142.16\n\n.\n\nthe entry 2.7 for the ac marginal association for this model is the odds ratio\nfor the marginal ac fitted table. the odds ratios for the observed data are\n.\nthose reported for the saturated model acm .\n\ntable 8.5 shows that estimated conditional odds ratios equal 1.0 for each\npairwise term not appearing in a model, such as the ac association in model\n\u017e\nam, cm . for that model, the estimated marginal ac odds ratio differs\nfrom 1.0, since conditional independence does not imply marginal indepen-\ndence. some models have conditional associations that are necessarily the\n\n\u017e\n\n.\n\ntable 8.5 estimated odds ratios for loglinear models in table 8.5\n\n.\n\nmodel\n\u017e\na, c, m\n.\n\u017e\nac, m\n\u017e\nam, cm\n\u017e\nac, am, cm\n\u017e\nacm level 1\n\u017e\nacm level 2\n\n.\n.\n\n.\n\nconditional association\ncm\nac\n1.0\n1.0\n1.0\n17.7\n25.1\n1.0\n17.3\n7.8\n13.8\n17.5\n9.7\n7.7\n\nam\n1.0\n1.0\n61.9\n19.8\n24.3\n13.5\n\n.\n\nmarginal association\n\nac\n1.0\n17.7\n2.7\n17.7\n17.7\n\nam\n1.0\n1.0\n61.9\n61.9\n61.9\n\ncm\n1.0\n1.0\n25.1\n25.1\n25.1\n\n "}, {"Page_number": 339, "text": "324\n\nloglinear models for contingency tables\n\n\u017e\n\n.\n\nsame as the corresponding marginal associations. in section 9.1.2 we present\na condition guaranteeing this.\n\nmodel ac, am, cm permits all pairwise associations but maintains\nhomogeneous odds ratios between two variables at each level of the third.\nthe ac fitted conditional odds ratios for this model equal 7.8. one can\ncalculate this odds ratio using the model\u2019s fitted values at either level of m,\nor from 8.14 using exp \u242d q \u242d y \u242d y \u242d .\n\u02c6ac\n.\n21\n\nw\ntable 8.5 shows that estimated odds ratios are very dependent on the\nmodel. this highlights the importance of good model selection. an estimate\nfrom this table is informative only to the extent that its model fits well. in the\nnext section we discuss goodness of fit.\n\n\u02c6ac\n12\n\n\u02c6ac\n22\n\n\u02c6ac\n11\n\n.x\n\n\u017e\n\n\u017e\n\n8.3\n\ninference for loglinear models\n\na good-fitting loglinear model provides a basis for describing and making\ninferences about associations among categorical responses. standard meth-\nods apply for checking fit and making inference about model parameters.\n\n8.3.1 chi-squared goodness-of-fit tests\nas usual, x 2 and g2 test whether a model holds by comparing cell fitted\nvalues to observed counts. here df equals the number of cell counts minus\nthe number of model parameters.\n\nfor the student survey table 8.3 , table 8.6 shows results of testing fit for\nseveral loglinear models. models that lack any association term fit poorly.\nthe model ac, am, cm that has all pairwise associations fits well p s\n0.54 . it is suggested by other criteria also, such as minimizing\n\n.\naic s y2 maximized log likelihood\u138fnumber of parameters in model\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.x\nor equivalently, minimizing g y 2 df .\n\n\u017e\n\nw\n\n2\n\ntable 8.6 goodness-of-fit tests for loglinear models in table 8.4\n\n.\n\nmodel\n\u017e\na, c, m\n.\n\u017e\na, cm\n.\n\u017e\nc, am\n.\n\u017e\nm, ac\n.\n\u017e\nac, am\n.\n\u017e\nac, cm\n\u017e\n.\nam, cm\n\u017e\nac, am, cm\n\u017e\nacm\nap-value for g2 statistic.\n\n.\n\n.\n\n2\n\ng\n\n1286.0\n534.2\n939.6\n843.8\n497.4\n92.0\n187.8\n0.4\n0.0\n\n2\n\nx\n\n1411.4\n505.6\n824.2\n704.9\n443.8\n80.8\n177.6\n0.4\n0.0\n\ndf\n4\n3\n3\n3\n2\n2\n2\n1\n0\n\na\n\np-value\n- 0.001\n- 0.001\n- 0.001\n- 0.001\n- 0.001\n- 0.001\n- 0.001\n0.54\n\u138f\n\n "}, {"Page_number": 340, "text": "inference for loglinear models\n\n325\n\n0\n\n1\n\n0\n\n.\n\n.\n\n.\n\n\u017e\n\n2\u017e\n\n2\u017e\n\ninference about conditional associations\n\n8.3.2\ntests about conditional associations compare loglinear models. the likeli-\nhood-ratio statistic y2 l y l is identical to the difference g m m s\ng m y g m between deviances for models without that term and with\nit. for model xy, xz, yz , consider the hypothesis of xy conditional\nindependence. this is h : \u242d s 0 for the i y 1 j y 1 xy association\nparameters. the test statistic is g xz, yz y g xy, xz, yz , with df s\ni y 1 j y 1 . this has the same purpose as the generalized cmh and\n\u017e\nmodel-based tests for nominal variables presented in section 7.5.\n\n1\u017e\n\nx y\ni j\n\n2\u017e\n\n2\u017e\n\n2\u017e\n\n.\u017e\n\n.\u017e\n\nfor instance, the test of conditional independence between alcohol use\nand cigarette smoking compares model am, cm with the alternative\n\u017e\nac, am, cm . the test statistic is\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n0\n\n1\n\n0\n\n<\n\n2\n\ng\n\n\u017e\n\nam, cm ac, am, cm s 187.8 y 0.4 s 187.4,\n\n. \u017e\n<\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\nwith df s 2 y 1 s 1 p - 0.001 . the statistics comparing ac, cm and\n\u017e\nac, am with ac, am, cm also provide strong evidence of am and cm\nconditional associations. further analyses of table 8.3 use model\n.\n\u017e\nac, am, cm .\n\nwith large sample sizes, statistically significant effects can be weak and\nunimportant. a more relevant concern is whether the associations are strong\nenough to be important. confidence intervals are more useful than tests for\nassessing this. table 8.7 shows output from fitting model ac, am, cm with\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ntable 8.7 output for fitting loglinear model to table 8.3\n\ncriteria for assessing goodness of fit\n\ncriterion\ndeviance\npearson chi- square\n\ndf\n1\n1\n\nvalue\n0.3740\n0.4011\n\nvalue / df\n\n0.3740\n0.4011\n\nstandard\n\nwald\n\nparameter\nintercept\na\nc\nm\na*m\na*c\nc*m\n\n1\n1\n1\n1\n1\n1\n\n1\n1\n1\n\nestimate\n5.6334\n0.4877\ny1.8867\ny5.3090\n2.9860\n2.0545\n2.8479\n\nerror\n0.0597\n0.0758\n0.1627\n0.4752\n0.4647\n0.1741\n0.1638\n\nchi- square\n\npr>chisq\n\n8903.96\n41.44\n134.47\n124.82\n41.29\n139.32\n302.14\n\n<.0001\n<.0001\n<.0001\n<.0001\n<.0001\n<.0001\n<.0001\n\nlr statistics\n\nsource\n\na*m\na*c\nc*m\n\ndf\n1\n1\n1\n\nchi- square\n\npr>chisq\n\n91.64\n187.38\n497.00\n\n<.0001\n<.0001\n<.0001\n\n "}, {"Page_number": 341, "text": "326\n\nloglinear models for contingency tables\n\nw\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u02c6ac\n11\n\nparameters in the last row and in the last column equal to zero, such as by\nusing 1, 0 dummy variables for each classification. consider the conditional\nac odds ratio, assuming model ac, am, cm . table 8.7 reports \u242d s\n2.054, with se s 0.174. for these constraints, this is the estimated condi-\ntional log odds ratio. a 95% wald confidence interval for the true condi-\ntional ac odds ratio is exp 2.054 \" 1.96 0.174 , or 5.5, 11.0 . strong posi-\ntive association exists between cigarette use and alcohol use, both for users\nand nonusers of marijuana.\n\n.\nfor model ac, am, cm ,\n\nthe 95% wald confidence intervals are\n\u017e\n8.0, 49.2 for the am conditional odds ratio and 12.5, 23.8 for the cm\nconditional odds ratio. the intervals are wide, but these associations also are\nstrong. table 8.5 shows that estimated marginal associations are even stronger.\ncontrolling for outcome on one response moderates the association some-\nwhat between the other two.\n\n.x\n\nthe analyses in this section pertain to associations. a different analysis\npertains to comparing single-variable marginal distributions, for instance to\ndetermine if students used cigarettes more than alcohol or marijuana. that\ntype of analysis is presented in section 10.1.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n8.4 loglinear models for higher dimensions\n\nloglinear models for three-way tables are more complex than for two-way\ntables, because of the variety of potential association terms. loglinear models\nfor three-way tables extend readily, however, to multiway tables. as the\nnumber of dimensions increases, some complications arise. one is the in-\ncrease in the number of possible association and interaction terms, making\nmodel selection more difficult. another is the increase in number of cells. in\nsection 9.8 we show that this can cause difficulties with existence of estimates\nand appropriateness of asymptotic theory.\n\n8.4.1 four-way contingency tables\nwe illustrate models for higher dimensions using a four-way table with\nvariables w, x, y, and z. interpretations are simplest when the model has\nno three-factor interaction terms. such models are special cases of\n\nlog \u242e s \u242dq \u242dw q \u242dx q \u242dy q \u242dz\n\nhi jk\n\nh\n\nk\n\nj\n\nq \u242dw x q \u242dw y q \u242dw z q \u242dx y q \u242dx z q \u242dy z,\n\nh k\n\ni j\n\nhi\n\nik\n\njk\n\ni\n\nh j\n\n\u017e\n\ndenoted by wx, wy, wz, xy, xz, yz . each pair of variables is condition-\nally dependent, with the same odds ratios at each combination of categories\nof the other two variables. an absence of a two-factor term implies condi-\ntional independence, given the other two variables.\n\n.\n\n "}, {"Page_number": 342, "text": "loglinear models for higher dimensions\n\n327\n\na variety of models exhibit three-factor interaction. a model could con-\n.\ntain any of wxy, wxz, wyz, or xyz terms. for model wxy, wz, xz, yz ,\neach pair of variables is conditionally dependent, but at each level of z the\nwx association, the wy association, and the xy association may vary across\ncategories of the remaining variable. the conditional association between z\nand another variable is homogeneous. the saturated model contains all the\nthree-factor terms plus a four-factor interaction term.\n\n\u017e\n\n8.4.2 automobile accident example\ntable 8.8 summarizes observations of 68,694 passengers in autos and light\ntrucks involved in accidents in the state of maine in 1991. the table classifies\npassengers by gender g , location of accident l , seat-belt use s , and\ninjury i . table 8.8 reports the sample proportion of passengers who were\ninjured. for each gl combination, the proportion of injuries was about\nhalved for passengers wearing seat belts.\n\n\u017e .\n\ntable 8.9 displays tests of fit for several loglinear models. to investigate\n.\nthe complexity of model needed, we consider models g, i, l, s ,\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\ntable 8.8 loglinear models for injury, seat-belt use, gender, and locationa\n\ninjury\n\n\u017e\n\ngi, gl, gs, il, is, ls\n\n.\n\n\u017e\n\ngender location\n\nfemale\n\nurban\n\nrural\n\nmale\n\nurban\n\nrural\n\nseat\nbelt\n\nno\nyes\nno\nyes\nno\nyes\nno\nyes\n\nno\n\n7,287\n11,587\n3,246\n6,134\n10,381\n10,969\n6,123\n6,693\n\nyes\n\n996\n759\n973\n757\n812\n380\n1,084\n513\n\nno\n\n7,166.4\n11,748.3\n3,353.8\n5,985.5\n10,471.5\n10,837.8\n6,045.3\n6,811.4\n\nyes\n\n993.0\n721.3\n988.8\n781.9\n845.1\n387.6\n1,038.1\n518.2\n\n.\n\nsample\n\nproportion\n\ngls, gi, il, is\nyes\n\nno\n\n7,273.2\n11,632.6\n3,254.7\n6,093.5\n10,358.9\n10,959.2\n6,150.2\n6,697.6\n\n1,009.8\n713.4\n964.3\n797.5\n834.1\n389.8\n1,056.8\n508.4\n\nyes\n\n0.12\n0.06\n0.23\n0.11\n0.07\n0.03\n0.15\n0.07\n\nag, gender; i, injury; l, location; s, seat-belt use.\nsource:data courtesy of cristanna cook, medical care development, augusta, maine.\n\ntable 8.9 goodness-of-fit tests for loglinear models in table 8.8\n\n.\n\nmodel\n\u017e\ng, i, l, s\n\u017e\ngi, gl, gs, il, is, ls\n\u017e\ngil, gis, gls, ils\n.\n\u017e\ngil, gs, is, ls\n.\n\u017e\ngis, gl, il, ls\n\u017e\n.\ngls, gi, il, is\n.\n\u017e\nils, gi, gl, gs\n\n.\n\n.\n\n2\n\ng\n\n2792.8\n23.4\n1.3\n18.6\n22.8\n7.5\n20.6\n\ndf\n11\n5\n1\n4\n4\n4\n4\n\np-value\n- 0.0001\n- 0.001\n0.25\n0.001\n- 0.001\n0.11\n- 0.001\n\n "}, {"Page_number": 343, "text": "328\n\nloglinear models for contingency tables\n\ntable 8.10 estimated conditional odds ratios for models of table 8.8\n\nodds ratio\n\n\u017e\n\ngi\nil\nis\ngl s s no\ns s yes\n\ngs ls urban\nls rural\nls gs female\ngs male\n\ngi, gl, gs, il, is, ls\n\ngls, gi, il, is\n\n.\n\nloglinear model\n\u017e\n\n.\n\n0.58\n2.13\n0.44\n1.23\n1.23\n0.63\n0.63\n1.09\n1.09\n\n0.58\n2.13\n0.44\n1.33\n1.17\n0.66\n0.58\n1.17\n1.03\n\n2\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\u017e\n\n\u017e\ngi, gl, gs, il, is, ls , and gil, gis, gls, ils having all terms of vary-\ning complexity. model g, i, l, s of mutual independence fits very poorly.\nmodel gi, gl, gs, il, is, ls fits much better but still has a lack of fit\np - 0.001 . model gil, gis, gls, ils fits well g s 1.3, df s 1 but is\n\u017e\n\u017e\nto interpret. this suggests studying models more\ncomplex and difficult\ncomplex\nthan gil,\n.\ngis, gls, ils .\n\nthan gi, gl, gs, il, is, ls\n\nsimpler\n\nbut\n\nfirst, however, we analyze model gi, gl, gs, il, is, ls , which focuses\non pairwise associations. table 8.8 displays its fitted values. table 8.10\nreports the model-based estimated conditional odds ratios. one can obtain\nthem directly using the fitted values for partial tables relating two variables at\nany combination of levels of the other two. they also follow directly from\nparameter estimates; for instance, 0.44 s exp \u242d q \u242d y \u242d y \u242d .\n\u02c6is\n.\n21\n\nsince the sample size is large, the estimates of odds ratios are quite\nprecise. for instance, the standard error of the estimated is conditional log\nodds ratio of y0.814 is 0.028. a 95% wald confidence interval for the true\nodds ratio is exp y0.814 \" 1.96 0.028 or 0.42, 0.47 . this model estimates\nthat the odds of injury for passengers wearing seat belts were less than half\nthe odds for passengers not wearing them, at each gender\u1390location combina-\ntion. the fitted odds ratios in table 8.10 also suggest that other factors being\nfixed, injury was more likely in rural than urban accidents and more likely for\nfemales than for males. the estimated odds that males used seat belts were\nonly 0.63 times the estimated odds for females.\n\n\u02c6is\n22\n\n\u02c6is\n12\n\n\u02c6is\n11\n\n.x\n\ninterpretations are more complex for models containing three-factor inter-\naction terms. table 8.9 shows results of adding a single three-factor term to\nmodel gi, gl, gs, il, is, ls . of\nfour possible models,\n\u017e\ngls, gi, il, is appears to fit best. table 8.8 also displays its fit. given the\nlarge sample size, its g2 value suggests that it fits quite well.\n\nthe\n\nfor model gls, gi, il, is , each pair of variables is conditionally depen-\ndent, and at each category of i the association between any two of the others\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nw\n\n "}, {"Page_number": 344, "text": "loglinear models for higher dimensions\n\n329\n\n\u017e\n\nvaries across categories of the remaining variable. for this model,\nit is\ninappropriate to interpret the gl, gs, and ls two-factor terms on their\nown. since i does not occur in a three-factor interaction, the conditional\nodds ratio between i and each variable see the top portion of table 8.10 is\nthe same at each combination of categories of the other two variables.\n\nwhen a model has a three-factor interaction term but no term of higher\norder than that, one can study the interaction by calculating fitted odds ratios\nbetween two variables at each level of the third. one can do this at any levels\nof remaining variables not involved in the interaction. the bottom portion of\ntable 8.10 illustrates this for model gls, gi, il, is . for instance, the\nfitted gs odds ratio of 0.66 for l s urban refers to four fitted values for\nurban accidents, both the four with injury s no and the four with injury s\n\u017e\nyes ; for example, 0.66 s 7273.2 = 10,959.2 r 11,632.6 = 10,358.9 .\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n8.4.3 large samples and statistical versus practical significance\nmodel gls, gi, il, is\nthan gi, gl, gs,\nil, is, ls . the difference in g values of 23.4 y 7.5 s 15.9 has df s 5 y 4\ns 1 p s 0.0001 . table 8.10 indicates, however, that the degree of three-\nfactor interaction is weak. the fitted odds ratio between any two of g, l,\nand s is similar at both levels of the third variable. the significantly better fit\nof model gls, gi, il, is reflects mainly the enormous sample size.\n\nto fit much better\n\nseems\n2\n\nas in any test, a statistically significant effect need not be practically\nimportant. with huge samples, it is crucial to focus on estimation rather than\nhypothesis testing. for instance, a comparison of fitted odds ratios for the\ntwo models\nthe\nsimpler model\n\u017e\ngi, gl, gs, il, is, ls is adequate for most purposes.\n\nin table\n\nsuggests\n\n8.10\n\nthat\n\n.\n\n.\n\n\u017e\n\n8.4.4 dissimilarity index\nfor a table of arbitrary dimension with cell counts n s np\nand fitted\nvalues \u242e s n\u2432 , one can summarize the closeness of a model fit to the data\n.\nby the dissimilarity index gini 1914 ,\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ni\n\ni\n\ni\n\ni\n\n\u02c6\u232c s n y \u242e r2 n s\n\n\u00fd\n\n\u02c6\n\ni\n\ni\n\ni\n\n\u00fd\n\ni\n\np y \u2432 r2 .\n\n\u02c6\n\ni\n\ni\n\nthis index falls between 0 and 1, with smaller values representing a better\nfit. it represents the proportion of sample cases that must move to different\ncells for the model to fit perfectly.\nthe dissimilarity index \u232c estimates a corresponding population index \u232c\ndescribing model lack of fit. the value \u232c s 0 occurs when the model holds\nperfectly. in practice, this is unrealistic for unsaturated models, and \u232c ) 0.\nthe estimator \u232c helps study whether the lack of fit is important in a practical\nsense. when \u232c - 0.02 or 0.03, the sample data follow the model pattern\n\n\u02c6\n\u02c6\n\n\u02c6\n\n "}, {"Page_number": 345, "text": "330\n\nloglinear models for contingency tables\n\n\u02c6\nquite closely, even though the model is not perfect. when \u232c is near 0, \u232c\n.\ntends to overestimate \u232c, substantially so for small n. firth and kuha 2000\nprovided an approximate variance for \u232c and studied ways to reduce its\nestimation bias.\nfor table 8.8, model gi, gl, gs, il, is, ls has \u232c s 0.008, and model\ngls, gi, il, is has \u232c s 0.003. for either model, moving less than 1% of\n\u017e\nthe data yields a perfect\nvalue for\n\u017e\ngi, gl, gs, il, is, ls indicated that it does not truly hold. nevertheless,\nthe small \u232c value suggests that, in practical terms, it fits decently.\n\nfit. the relatively\n\nlarge g 2\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n8.5 loglinear\u1390logit model connection\n\nloglinear models treat categorical response variables symmetrically, focusing\non associations and interactions in their joint distribution. logit models, by\ncontrast, describe how a single categorical response depends on explanatory\nvariables. the model types seem distinct, but connections exist between\nthem. for a loglinear model, forming logits on one response helps to\ninterpret the model. moreover, logit models with categorical explanatory\nvariables have equivalent loglinear models.\n\n8.5.1 using logit models to interpret loglinear models\nto understand implications of a loglinear model formula, it can help to form\n.\na logit on one variable. we illustrate with the loglinear model xy, xz, yz .\nwhen y is binary, its logit is\n\n\u017e\n\nlog\n\n<\n\np y s 1 x s i, z s k\n\u017e\np y s 2 x s i, z s k\n\u017e\n\n<\n\n.\n.\n\ns log\n\n\u242e\n\ni1 k\n\n\u242e\n\ni2 k\n\ns log \u242e y log \u242e\n\ni1 k\n\ni2 k\n\ni\n\nk\n\n1\n\ns \u242dq \u242dx q \u242dy q \u242dz q \u242dx y q \u242dx z q \u242dy z\n1 k\n\n\u017e\n.\ny \u242dq \u242dx q \u242dy q \u242dz q \u242dx y q \u242dx z q \u242dy z\n.\n2 k\ns \u242dy y \u242dy q \u242dx y y \u242dx y q \u242dy z y \u242dy z .\n\u017e\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nik\n\nik\n\ni2\n\ni1\n\n2\n\nk\n\ni\n\n2 k\n\n1 k\n\ni2\n\ni1\n\n1\n\n2\n\nthe first parenthetical term is a constant, not depending on i or k. the\nsecond parenthetical\nterm depends on the category i of x. the third\nparenthetical term depends on the category k of z. this logit has the\nadditive form\n\nlogit p y s 1 x s i, z s k s \u2423q \u2424 q \u2424 .\n\n\u017e\n\n.\n\nx\n\n<\n\nz\nk\n\ni\n\n\u017e\n\n8.15\n\n.\n\nusing the notation summarizing logit models by their predictors, we denote it\nby x q z .\n.\n\n\u017e\n\n "}, {"Page_number": 346, "text": "loglinear\u1390logit model connection\n\n331\n\nin section 5.4.1 we discussed this logit model. when y is binary, the\nloglinear model xy, xz, yz is equivalent to it. the \u242d terms for associa-\ntion among explanatory variables cancel in the difference in logarithms the\nlogit defines. the logit model does not study this association.\n\nx z\nik\n\n.\n\n\u017e\n\n8.5.2 auto accident example revisited\n.\nfor the maine auto accidents table 8.8 , in section 8.4.2 we showed that the\n.\nloglinear model gls, gi, li, is ,\n\n\u017e\n\n\u017e\n\nlog \u242e s \u242dq \u242dg q \u242di q \u242dl q \u242ds q \u242dg i q \u242dg l q \u242dg s\n\ns\n\ng i\n\ng s\n\ng i ll s\n\ng ll\n\ng\n\nq \u242dil q \u242dis q \u242dls q \u242dg ls ,\n\ng ll s\n\ni ll\n\ni\n\ni s\n\nll\n\nll s\n\n.\n\n\u017e .\nfits well. it is natural to treat injury\ni as a response variable and gender\n\u017e\n.\n\u017e\ng , location l , and seat-belt use s as explanatory variables, or perhaps s\nas a response with g and l as explanatory. one can show that this loglinear\nmodel is equivalent to logit model g q l q s ,\n.\n\n\u017e\n\n\u017e\n\n.\n\n<\n\ns\n\n\u017e\n\n\u017e\n\n.\n\n1 s\n\ns\ns\n\nl\nll\n\nlogit p i s 1 g s g , l s ll , s s s s \u2423q \u2424 q \u2424 q \u2424 .\n\n.\n8.16\nfor instance, the seat-belt effects in the two models satisfy \u2424s s \u242dis y \u242dis.\n2 s\nin the logit calculation, all terms in the loglinear model not having the injury\nindex i cancel. fitted values, goodness-of-fit statistics, residual df, and\nstandardized pearson residuals for the logit model are identical to those for\nthe loglinear model.\n\ng\ng\n\n\u02c6is\n11\n\nodds ratios describing effects on i relate to two-factor loglinear parame-\nters and main-effect logit parameters. in the logit model, the log odds ratio\nfor the effect of s on i equals \u2424s y \u2424s. this equals \u242dis q \u242dis y \u242dis y \u242dis\n21\nin the loglinear model. their estimates are the same no matter how software\nsets up constraints. for table 8.8, \u2424 y \u2424 s y0.817 for the logit model,\nand \u242d q \u242d y \u242d y \u242d s y0.817 for the loglinear model.\n\n\u02c6s\n1\n\n\u02c6s\n2\n\nloglinear models are glms that treat the 16 cell counts in table 8.8 as 16\nindependent poisson variates. logit models are glms that treat the table as\nbinomial counts. logit models with i as the response treat the marginal gls\ntable n\nas eight independent binomial\nvariates on that response. although the sampling models differ, the results\nfrom fits of corresponding models are identical.\n\nas fixed and regard n\n\ngqll s\n\ng1 ll s\n\n\u02c6is\n12\n\n\u02c6is\n22\n\n\u02c6is\n21\n\n11\n\n22\n\n12\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n1\n\n2\n\n8.5.3 correspondence between loglinear and logit models\nin the derivation of the logit model x q z see 8.15 from loglinear model\n\u017e\n.\nxy, xz, yz , the \u242d term cancels. it might seem as if the model xy, yz\nomitting this term is also equivalent to that logit model. indeed, forming the\nlogit on y for xy, yz results in the same logit formula. the loglinear\n\n. w\n\nx z\nik\n\n.x\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n "}, {"Page_number": 347, "text": "332\n\nloglinear models for contingency tables\n\ntable 8.11 equivalent loglinear and logit models for a three-way table\nwith binary response variable y\n\nloglinear symbol\n.\n\u017e\ny, xz\n.\n\u017e\nxy, xz\n\u017e\n.\nyz, xz\n\u017e\nxy, yz, xz\n\u017e\nxyz\n\n.\n\n.\n\nlogit model\n\nlogit symbol\n\nx\n\n\u2423\n\u2423q \u2424\ni\n\u2423q \u2424\nz\nk\n\u2423q \u2424 q \u2424\nz\nx\nk\n\u2423q \u2424 q \u2424 q\u2424\nxz\nz\nk\nik\n\nx\n\ni\n\ni\n\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\n.\n\u138f\n.\nx\n.\nz\nx q z\n.\nx *z\n\n.\n\n.\n\n\u017e\n\nmodel that has the same fit as the logit model, however, contains a general\ninteraction term for relationships among the explanatory variables. the logit\nmodel does not assume anything about relationships among explanatory\nvariables, so it allows an arbitrary interaction pattern for them.\n\ntable 8.11 summarizes equivalent logit and loglinear models for three-way\ntables when y is a binary response. each loglinear model contains the xz\nassociation term relating the explanatory variables in the logit models. the\nsimple loglinear model y, xz states that y is jointly independent of both\nx and z, and is equivalent to the logit model having only an intercept.\nthe saturated loglinear model xyz contains the three-factor interaction\nterm. when y is a binary response, this model is equivalent to a logit model\nwith an interaction between the predictors x and z. for instance, the effect\nof x on y depends on z, meaning that the xy odds ratio varies across its\ncategories. that logit model is also saturated.\n\nanalogous correspondences hold when y has several categories, using\nbaseline-category logit models. an advantage of the loglinear approach\nis its generality. it applies when more than one response variable exists.\nthe alcohol\u1390cigarette\u1390marijuana example in section 8.2.4, for instance, used\nloglinear models to study association patterns among three response vari-\nables. loglinear models are most natural when at\ntwo variables\nare response variables. when only one is a response, it is more sensible to\nuse logit models directly.\n\nleast\n\n\u017e\n\n.\n\n\u017e\n\n8.5.4 generalized loglinear model*\nlet n s n , . . . , n\nand \u242e s \u242e , . . . , \u242e denote column vectors of ob-\nserved and expected counts for the n cells of a contingency table, with\nn s \u00fd n . for simplicity we use a single index, but the table may be multidi-\nmensional. loglinear models for positive poisson means have the form\n\n.x\n\n.x\n\n\u017e\n\nn\n\nn\n\n1\n\n1\n\ni\n\ni\n\nlog \u242e s x\u2424\n\n\u017e\n\n8.17\n\n.\n\nfor model matrix x and column vector \u2424 of model parameters.\n\n "}, {"Page_number": 348, "text": "model fitting: likelihood equations and asymptotics\n\n333\nwe illustrate with the independence model, log \u242e s \u242dq \u242dx q \u242dy, for a\n\ni j\n\ni\n\nj\n\n2 = 2 table. with constraints \u242dx s \u242dy s 0, it is\n\n2\n\n2\n\nlog \u242e\n11\nlog \u242e\n12\nlog \u242e\n21\nlog \u242e\n22\n\ns\n\n1\n1\n1\n1\n\n1\n1\n0\n0\n\n1 \u242d\n0 \u242d\nx\n1\n1\ny\u242d\n0\n1\n\n.\n\na generalization of 8.17 allows many additional models. this generalized\n\n\u017e\n\n.\n\nloglinear model is\n\nc log a\u242e s x\u2424\n\n\u017e\n\n.\n\n\u017e\n\n8.18\n\n.\n\n\u017e\n\n.\n\nfor matrices c and a. the ordinary loglinear model 8.17 results when c and\na are identity matrices. other special cases include logit models for binary or\nmulticategory responses.\n\nfor instance, the loglinear model of independence for a 2 = 2 table is\nequivalent to a model by which the logit for y is the same in each row of x\n\u017e\nsee section 8.1.2 . that logit model has form 8.18 : a is a 4 = 4 identity\nmatrix, so a\u242e is the 4 = 1 vector \u242e s \u242e , \u242e , \u242e , \u242e ; the product\n.x\nc log a\u242e forms the logit in row 1 and the logit in row 2 using\n\n11\n\n12\n\n21\n\n22\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nc s\n\n1 y1\n0\n0\n\n0\n0\n1 y1\n\n;\n\n\u017e\n\n.x\n\nthen x s 1, 1 is a 2 = 1 matrix, and \u2424 is a single constant \u2423, so x\u2424 forms\na common value for those two logits.\n\nin chapters 10 and 11 we use the generalized loglinear model for models\noutside the classes of glms studied thus far. an example is modeling\nmarginal distributions of multivariate responses.\n\n8.6 loglinear model fitting: likelihood equations\nand asymptotic distributions*\n\nin discussing the fitting of loglinear models, we first derive sufficient statistics\nand likelihood equations. we then present large-sample normal distributions\nfor ml estimators of model parameters and cell probabilities. we illustrate\nresults with models for three-way tables. for simplicity, derivations use the\npoisson sampling model, which does not require a constraint on parameters\nsuch as the multinomial does.\n\n "}, {"Page_number": 349, "text": "334\n\nloglinear models for contingency tables\n\n8.6.1 minimal sufficient statistics\nfor three-way tables, the joint poisson probability that cell counts y s n\nis\n\ni jk\n\n\u0004\n\n4\n\ni jk\n\ney\u242ei jk\n\u0142 \u0142 \u0142 n !\n\nk\n\ni\n\nj\n\ni jk\n\nn i jk\u242e\ni jk ,\n\nwhere the product refers to all cells of the table. the kernel of the log\nlikelihood is\n\nl \u242e s\n\u017e\n\n.\n\n\u00fd \u00fd \u00fd\n\nn log \u242e y\n\ni jk\n\ni jk\n\n\u00fd \u00fd \u00fd\n\n\u242e .\ni jk\n\n\u017e\n\n8.19\n\n.\n\ni\n\nj\n\nk\n\ni\n\nj\n\nk\n\nfor the general loglinear model 8.12 , this simplifies to\n\n.\nl \u242e s n\u242dq n \u242dx q n\n\u00fd\n\u017e\n\niqq i\n\n\u00fd\n\n.\n\n\u017e\n\n\u242dy q n\n\u00fd\n\nqjq j\n\nqqk\n\n\u242dz\nk\n\nj\n\nk\n\ni\n\nj\n\nj\n\nq\n\nq\n\ni\n\ni\n\n\u00fd \u00fd\n\nn \u242dx y q\ni jq i j\n\n\u00fd \u00fd\n\nn \u242dx z q\niqk\n\nik\n\n\u00fd \u00fd\n\nn \u242dy z\nqj k\njk\n\ni\n\nk\n\nj\n\nk\n\n\u00fd \u00fd \u00fd\n\nn \u242dx y z y\n\ni jk\n\ni jk\n\n\u00fd \u00fd \u00fd\n\n.\nexp \u242dq \u2b48\u2b48\u2b48 q\u242dx y z .\n\n\u017e\n\ni jk\n\n\u017e\n\n8.20\n\n.\n\nk\n\ni\n\nj\n\nk\n\n4\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\ni jk\n\ni jk\n\n\u0004 x y z4\n\nsince the poisson distribution is in the exponential family, coefficients of\n\u0004\nare\nthe parameters are sufficient statistics. for this saturated model, n\ncoefficients of \u242d\n, so there is no reduction of the data. for simpler\nmodels, certain parameters are zero and 8.20 simplifies. for instance, for\nindependence, sufficient statistics are the\nthe model x, y, z of mutual\n\u0004 y 4\n4\ncoefficients in 8.20 of \u242d , \u242d , and \u242d . these are n\n,\nj\nand n\n\n\u0004 z4\nk\n\n\u0004 x 4\n\n, n\n\nqjq\n\niqq\n\n.\n.\n\ntable 8.12 lists minimal sufficient statistics for several loglinear models.\neach one is the coefficient of the highest-order term s in which a variable\nappears. in fact, they are the marginal distributions for terms in the model\nsymbol. simpler models use more condensed sample information. for in-\nstance, whereas x, y, z uses only the single-factor marginal distributions,\n\u017e\nxy, xz, yz uses the two-way marginal tables.\n\n\u017e .\n\nqqk\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n\u0004\n\n4\n\n.\n\ni\n\ntable 8.12 minimal sufficient statistics for\nfitting loglinear models\n\n.\n\nmodel\n\u017e\nx, y, z\n.\n\u017e\nxy, z\n\u017e\nxy, yz\n\u017e\nxy, xz, yz\n\n.\n\nminimal sufficient statistics\n\n\u0004\n\u0004\n\u0004\n\u0004\n\nn\nn\nn\nn\n\n.\n\n4 \u0004\n, n\n4 \u0004\n, n\n4 \u0004\n, n\n4 \u0004\n, n\n\n4 \u0004\n, n\nqjq\n4\nqqk\n4\nqj k\n4 \u0004\n, n\niqk\n\niqq\ni jq\ni jq\ni jq\n\n4\n\nqj k\n\n4\n\nqqk\n\n "}, {"Page_number": 350, "text": "model fitting: likelihood equations and asymptotics\n\n335\n\n8.6.2 likelihood equations for loglinear models\nthe fitted values for a model are solutions to the likelihood equations. we\nderive likelihood equations using general representation 8.17 for a loglinear\nmodel. for a vector of counts n with \u242e s e n , the model is log \u242e s x\u2424, for\nwhich log \u242e s \u00fd x \u2424 for all i.\n\n\u017e .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nextending 8.19 , for poisson sampling the log likelihood is\n\ni\n\n\u017e\n\ni j\n\nj\n\nj\n.\n\nl \u242e s n log \u242e y \u242e\n\u017e\n\n. \u00fd\n\ni\n\ni\n\ni\n\ni\n\ni\n\ns n\n\n\u017e\n\u00fd \u00fd\n\ni\n\nj\n\nx \u2424 y exp\n\n\u00fd\n\ni j\n\nj\n\ni\n\n\u017e\n\n\u00fd\n\nj\n\n/\n\nx \u2424 .\ni j\n\nj\n\n\u017e\n\n8.21\n\n.\n\ni\n\n\u00fd\n/\n\nthe sufficient statistic for \u2424 is its coefficient, \u00fd n x . since\n\ni\n\ni\n\ni j\n\nj\n\n\u2b78\n\n\u2b78\u2424\nj\n\nexp\n\n\u017e\n\n\u00fd\n\nj\n\nx \u2424 s x \u242e ,\n\ni j\n\ni\n\nx \u2424 s x exp\n\ni j\n\nj\n\ni j\n\n\u017e\n\n\u00fd\n\n/\n/\n. s n x y \u242e x ,\n\ni j\n\nj\n\n\u00fd\n\n\u00fd\n\ni j\n\ni j\n\ni\n\ni\n\nj\n\n\u2b78l \u242e\u017e\n\u2b78\u2424\nj\n\ni\n\ni\n\nj s 1,2, . . . , p.\n\nthe likelihood equations equate these derivatives to zero. they have the\nform\n\nxx n s xx\u242e.\n\u02c6\n\n\u017e\n\n8.22\n\n.\n\nthese equations equate the sufficient statistics to their expected values, a\nresult obtained with glm theory in 4.29 . for models considered so far,\nthese sufficient statistics are the marginal tables in the model symbol.\n\u242dx y s \u242dx y z s 0. the log-likelihood derivatives\n\nto illustrate, consider model xz, yz . its log likelihood is 8.20 with\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u2b78l\nx z\n\u2b78\u242d\nik\n\ns n y \u242e\n\niqk\n\niqk\n\nand\n\n\u2b78l\ny z\n\u2b78\u242d\njk\n\ns n y \u242e\n\nqj k\n\nqj k\n\nyield the likelihood equations\n\n\u242e s n\n\u02c6iqk\n\u242e s n\n\u02c6qj k\n\niqk\n\nfor all i and k,\n\nqj k\n\nfor all j and k.\n\n\u017e\n\u017e\n\n8.23\n\n8.24\n\n.\n.\n\nderivatives with respect to lower-order terms yield equations implied by\nthese problem 8.30 . for model xz, yz , the fitted values have the same\nxz and yz marginal totals as the observed data.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n "}, {"Page_number": 351, "text": "336\n\nloglinear models for contingency tables\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n. \u017e\n\n8.6.3 birch\u2019s results for loglinear models\nfor model xz, yz , from 8.23 , 8.24 , and table 8.12, the minimal suffi-\ncient statistics are the ml estimates of the corresponding marginal distribu-\ntions of expected frequencies. equation 8.22 gives the corresponding result\nfor any loglinear model. birch 1963 showed that likelihood equations\nfor loglinear models match minimal sufficient statistics to their expected\nvalues. poisson glm theory implied this result in 4.29 and 4.44 . thus,\nfitted values for loglinear models are smoothed versions of the cell counts\nthat match them in certain marginal distributions but have associations and\ninteractions satisfying the model-implied patterns.\n\nbirch showed that a unique set of fitted values both satisfy the model and\nmatch the data in the minimal sufficient statistics. hence, if we find such a\nsolution, it must be the ml solution. to illustrate, the independence model\nfor a two-way table\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nlog \u242e s \u242dq \u242dx q \u242dy\n\ni j\n\ni\n\nj\n\nhas minimal sufficient statistics n\n\n\u0004\n\n4\n\niq\n\nand n\n\n\u0004\n\nqj\n\n4\n\n. the likelihood equations are\n\n\u242e s n , \u242e s n ,\n\u02c6\nqj\niq\n\niq\n\nqj\n\n\u02c6\n\nfor all i and j.\n\nthe fitted values \u242e s n n rn satisfy these equations and also satisfy the\nmodel. birch\u2019s result implies that they are the ml estimates.\n\niq qj\n\n\u02c6i j\n\n4\n\n\u0004\n\n8.6.4 direct versus iterative calculation of fitted values\nto illustrate how to solve likelihood equations, we continue the analysis of\nmodel xz, yz . from 8.9 , the model satisfies\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u2432 s\n\ni jk\n\n\u2432 \u2432\niqk qj k\n\u2432qqk\n\nfor all i, j, and k.\n\ni jk\n\ni jk\n\nfor poisson sampling, the related formula uses expected frequencies. setting\n\u2432 s \u242e rn,\n. the likelihood equations\nand \u242e s\n8.23 and 8.24 specify that ml estimates satisfy \u242e s n\n\u017e\n. since ml estimates of functions of param-\nn\nqj k\neters are the same functions of the ml estimates of those parameters,\n\nthis is \u242e s \u242e \u242e r\u242e\n.\n\n.\nand thus also \u242e s n\n\niqk qj k\n\nqqk\n\nqqk\n\nqqk\n\nqj k\n\niqk\n\niqk\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\ni jk\n\n\u017e\n\n\u0004\n\n4\n\n\u242e s\n\u02c6i jk\n\n\u02c6\n\u02c6\n\u242e \u242e\niqk qj k\n\u02c6qqk\n\u242e\n\ns\n\nn\n\nn\n\niqk qj k\nn\nqqk\n\n.\n\nthis solution satisfies the model and matches the data in the sufficient\nstatistics. thus, it is the unique ml solution.\n\n "}, {"Page_number": 352, "text": "model fitting: likelihood equations and asymptotics\n\n337\n\ntable 8.13 fitted values for loglinear models in three-way tables\n\nmodel\n\na\n\n\u017e\n\n\u017e\n\n\u017e\n\nx, y, z\n\n.\n\nxy, z\n\n.\n\nxy, xz\n\n.\n\nprobabilistic form\n\u2432 s \u2432 \u2432 \u2432\n\niqq qjq qqk\n\ni jk\n\n\u2432 s \u2432 \u2432\n\ni jq qqk\n\ni jk\n\n\u2432 s\n\ni jk\n\n\u2432 \u2432\n\ni jq iqk\n\u2432\n\niqq\n\u2432 s \u243a \u243e \u243b\nik\njk\nno restriction\n\ni jk\n\ni j\n\nn\n\nn\n\nn\n\ni jk\n\n\u242e s\u02c6\n\u242e s\u02c6\n\u242e s\u02c6\n\ni jk\n\ni jk\n\nfitted value\niqq qjq qqk\n\nn\n\nn\n2n\nni jq qqk\n\nn\nn\n\ni jq iqk\nn\n\niqq\n\n.\n\n\u017e\n\u017e\na\n\n.\n\nxy, xz, yz\nxyz\nformulas for models not listed are obtained by symmetry; for example, for xz, y , \u242e s\u02c6i jk\nn\niqk qjq\n\niterative methods section 8.7\n\u242e s n\n\u02c6i jk\n\nrn.\n\ni jk\n\nn\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n\u02c6i jk\n\n4\n\n4\n\n\u0004\n\ni jk\n\n\u02c6i jk\n\nsimilar reasoning produces \u242e for all except one model in table 8.12.\ntable 8.13 shows formulas. that table also expresses \u2432 in terms of\nmarginal probabilities. these expressions and the likelihood equations deter-\nmine the ml formulas, using the approach just described.\n\nfor models having explicit formulas for \u242e , the estimates are said to be\ndirect. many loglinear models do not have direct estimates. ml estimation\nthen requires iterative methods. of models in tables 8.12 and 8.13, the only\none not having direct estimates is xy, xz, yz . although the two-way\nmarginal tables are its minimal sufficient statistics,\nit is not possible to\n\u0004\nexpress \u2432 directly in terms of \u2432 , \u2432 , and \u2432 . direct estimates\nqj k\ndo not exist for unsaturated models containing all two-factor associations. in\npractice,\nit is not essential to know which models have direct estimates.\niterative methods for models not having direct estimates also apply with\nmodels that have direct estimates. statistical software for loglinear models\nuses such iterative methods for all cases.\n\niqk\n\ni jq\n\ni jk\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n4\n\n\u0004\n\n4\n\n8.6.5 chi-squared goodness-of-fit tests\nmodel goodness-of-fit statistics compare fitted cell counts to sample counts.\nfor poisson glms, in section 4.5.2 we showed that for models with an\nintercept term, the deviance equals the g2 statistic. with a fixed number of\ncells, g2 and x 2 have approximate chi-squared null distributions when\nexpected frequencies are large. the df equal the difference in dimension\nbetween the alternative and null hypotheses. this equals the difference\nbetween the number of parameters in the general case and when the model\nholds.\nwe illustrate with model x, y, z , for multinomial sampling with proba-\nbilities \u2432 . in the general case, the only constraint is \u00fd \u00fd \u00fd \u2432 s 1, so\nthere are ijk y 1 parameters. for model x, y, z , \u2432 s \u2432 \u2432 \u2432\n4\niqq qjq qqk\nare determined by i y 1 of \u2432\nsince \u00fd \u2432 s 1 , j y 1 of \u2432 , and\n4\nk y 1 of \u2432\ndf s ijk y 1 y i y 1 q j y 1 q k y 1 s ijk y i y j y k q 2.\n\nqqk\n.\n\n. \u0004\n.\n\n. thus,\n\nqjq\n\niqq\n\niqq\n\n4 \u017e\n\ni jk\n\ni jk\n\ni jk\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u0004\n\n\u0004\n\n\u0004\n\n4\n\n\u0004\n\n4\n\nk\n\ni\n\ni\n\nj\n\n "}, {"Page_number": 353, "text": "338\n\nloglinear models for contingency tables\n\ntable 8.14 residual degrees of freedom for loglinear\nmodels for three-way tables\n\n.\n\nmodel\n\u017e\nx, y, z\n.\n\u017e\nxy, z\n\u017e\n.\nxz, y\n.\n\u017e\nyz, x\n\u017e\n.\nxy, yz\n.\n\u017e\nxz, yz\n\u017e\n.\nxy, xz\n\u017e\nxy, xz, yz\n\u017e\nxyz\n\n.\n\ndegrees of freedom\nijk y i y j y k q 2\nk y 1 ij y 1\n\u017e\n.\nj y 1 ik y 1\n\u017e\n.\ni y 1 jk y 1\n\u017e\n.\nj i y 1 k y 1\n.\n\u017e\nk i y 1 j y 1\n.\n\u017e\ni j y 1 k y 1\n.\n\u017e\ni y 1 j y 1 k y 1\n\u017e\n.\n.\u017e\n0\n\n.\u017e\n.\u017e\n.\u017e\n.\u017e\n.\u017e\n.\u017e\n.\u017e\n\n.\n\n4\n\n4\n\n\u0004\n\ni jk\n\ni jk\n\nthe same df formula applies for poisson sampling. then, the general case\nhas ijk \u242e parameters. the residual df equal the number of cells in the\ntable minus the number of parameters in the poisson loglinear model for\n\u242e . for instance, model x, y, z has residual df s ijk y 1 q i y 1 q\n\u0004\nj y 1 q k y 1 , reflecting the single intercept parameter \u242d and con-\n\u017e\nstraints such as \u242dx s \u242dy s \u242dz s 0. this equals the number of linearly\nindependent parameters equated to zero in the saturated model to obtain the\ngiven model. table 8.14 shows df formulas for testing three-way loglinear\nmodels.\n\n.x\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nw\n\nk\n\nj\n\ni\n\n8.6.6 covariance matrix of ml parameter estimators\nto present large-sample distributions of ml parameter estimators, we return\nto general expression log \u242e s \u00fd x \u2424, from which we obtained the log-like-\nlihood derivatives\n\n\u017e\n\n.\n\ni j\n\ni\n\nj\n\nj\n\n\u2b78l \u242e\u017e\n\u2b78\u2424\nj\n\n. s n x y \u242e x ,\n\n\u00fd\n\n\u00fd\n\ni j\n\ni j\n\ni\n\ni\n\ni\n\ni\n\nj s 1, 2, . . . , p.\n\nthe hessian matrix of second partial derivatives has elements\n\n\u2b78\u242e\ni\n\u2b78\u2424\nk\n\n.\n\n\u017e\n\n\u2b782l \u242e\n\u2b78\u2424\u2b78\u2424\nk\n\nj\n\ns y x\u00fd i j\n\ni\n\ns y x\n\u00fd\n\ni\n\ni j\n\n\u00bd\n\n\u2b78\n\n\u2b78\u2424\nk\n\nexp\n\n\u017e\n\n\u00fd\n\nh\n\nx \u2424\nh\nih\n\n5\n\n/\n\ns y x x \u242e.\n\n\u00fd\n\nik\n\ni j\n\ni\n\ni\n\nlike logistic regression models, loglinear models are glms using the canoni-\ncal\nlink; thus this matrix does not depend on the observed data. the\n\n "}, {"Page_number": 354, "text": "model fitting: likelihood equations and asymptotics\n\n339\n\ninformation matrix, the negative of this matrix, is\n\niiiii s xx diag \u242e x,\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nwhere diag \u242e has the elements of \u242e on the main diagonal.\n\nfor a fixed number of cells, as n \u2122 \u2b01, the ml estimator \u2424 is asymptoti-\ny1. thus, for poisson\n\ncally normal with mean \u2424 and covariance matrix iiiii\nsampling, the asymptotic covariance matrix\n\n\u02c6\n\ncov \u2424 s x diag \u242e x\n.\n\n\u02c6\n\n\u017e\n\nx\n\n\u017e\n\n.\n\ny1\n\n.\n\n\u017e\n\n8.25\n\n.\n\nsubstituting ml fitted values and then taking square roots of diagonal\nelements yields standard errors for \u2424. this also follows from the general\nexpression 4.28 for glms, as noted in section 4.4.7.\n\n\u02c6\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\na\n\na\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n\u017e\n\n.\n\n.4\n\n8.6.7 connection between multinomial and poisson loglinear models\ni s\nsimilar asymptotic results hold with multinomial sampling. when y ,\ni\n1, . . . , n are independent poisson random variables, the conditional distri-\nbution of y given n s \u00fd y is multinomial with parameters \u2432 s \u242er\n\u017e\n\u00fd \u242e . birch 1963 showed that ml estimates of loglinear model parame-\nters are the same for multinomial sampling as for independent poisson\nsampling. he showed that estimates are also the same for independent\nmultinomial sampling, as long as the model contains a term for the marginal\ndistribution fixed by the sampling design. to illustrate, suppose that at each\ncombination of categories of x and z, an independent multinomial sample\nare fixed. the model must contain \u242d , so the\noccurs on y. then, n\nfitted values satisfy \u242e s n\n\nx z\nik\n\nthat separate inferential theory is unnecessary for multinomial loglinear\nmodels follows from the following argument. express the poisson loglinear\nmodel for \u242e as\n\n\u0004\niqk\n\u02c6iqk\n\niqk\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n.\n\ni\n\ni\n\ni\n\n.\n\nlog \u242e s \u242dq x \u2424,\n\ni\n\ni\n\n\u017e\n\nwhere 1, x\nparameter vector. the poisson log likelihood is\n\nis row i of the model matrix x and \u242d, \u2424\n\ni\n\n\u017e\n\nx.x\n\nis the model\n\nl s l \u242d, \u2424 s n log \u242e y \u242e\n\n. \u00fd\n\n\u00fd\n\n\u017e\n\ni\n\ni\n\ni\n\ns n \u242dq x \u2424 y exp \u242dq x \u2424 s n\u242dq n x \u2424 y \u2436,\n\n.\n\n\u017e\n\n.\n\n\u017e\n\ni\n\ni\n\ni\n\n\u00fd\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u00fd\n\ni\n\n\u00fd\n\ni\n\nwhere \u2436s \u00fd \u242e s \u00fd exp \u242dq x \u2424 . since log \u2436s \u242dq log \u00fd exp x \u2424 , this\n\u017e\nlog likelihood has the form\n\n.\n\n\u017e\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n.x\n\nw\n\nl s l \u2436, \u2424 s\n\n\u017e\n\n.\n\nn x \u2424 y nlog\n\ni\n\ni\n\n\u00fd\n\ni\n\n\u00fd\n\ni\n\nexp x \u2424 q nlog \u2436y \u2436 .\n.\n\n\u017e\n\n.\n\n\u017e\n\ni\n\n\u017e\n\n8.26\n\n.\n\n5\n\n\u00bd\n\n "}, {"Page_number": 355, "text": "340\n\ni\n\ni\n\ni\n\na\n\na\n\na\n\na\n\nw\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.x\n\n\u017e .\n\nloglinear models for contingency tables\nnow \u2432 s \u242er \u00fd \u242e s exp \u242dq x \u2424 r \u00fd exp \u242dq x \u2424 , and exp \u242d can-\ncels in the numerator and denominator. thus, the first term in braces on\nthe right-hand side in 8.26 is \u00fdn log \u2432 , which is the multinomial\nlog\nlikelihood, conditional on the total cell count n. unconditionally, n s \u00fd ni\ni\nhas a poisson distribution with expectation \u00fd \u242e s \u2436, so the second term in\n\u017e\n8.26 is the poisson log likelihood for n. since \u2424 enters only in the first term,\nthe ml estimator \u2424 and its covariance matrix for the poisson log likelihood\nl \u242d, \u2424 are identical to those for the multinomial log likelihood. the poisson\nloglinear model has one more parameter i.e., \u242d than the multinomial log-\nlinear model because of the random sample size. see birch 1963 , lang\n\u017e\n.\n1996c , mccullagh and nelder 1989, p. 211 , and palmgren 1981 for\ndetails.\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ni\n\nfor a multinomial sample, we show in section 14.4.1 that the estimated\n\ncovariance matrix of loglinear parameter estimators is\n\n$\ncov \u2424 s x diag \u242e y \u242e\u242ern x\n\n\u02c6\n\n\u017e\n\n.\n\n\u02c6 \u02c6\n\n\u02c6\n\n\u0004\n\n\u017e\n\n.\n\nx\n\nx\n\ny1\n\n4\n\n.\n\n\u017e\n\n8.27\n\n.\n\nthe intercept \u242d from the poisson model is not relevant, and x for the\nmultinomial model deletes the column of x pertaining to it in the poisson\nmodel.\n\na similar argument applies with several independent multinomial samples.\neach log-likelihood term is a sum of components from different samples, but\nthe poisson log likelihood again decomposes into two parts. one part is a\npoisson log likelihood for the independent sample sizes, and the other part is\n.\nthe sum of the independent multinomial log likelihoods. palmgren 1981\nshowed that conditional on observed marginal totals for explanatory vari-\nables, the asymptotic covariances for estimators of parameters involving the\nresponse are the same as for poisson sampling. for a single multinomial\nsample, palmgren\u2019s result implies that 8.27 is identical to 8.25 with the row\nand column referring to \u242d deleted. birch 1963 and goodman 1970 gave\nrelated results. lang 1996c gave an elegant discussion of connections\nbetween multinomial and poisson models. his results imply that the asymp-\ntotic variance of any linear contrast of estimated log means within a covariate\nlevel is identical for the two models.\n\n.\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n8.6.8 distribution of probability estimators\nfor multinomial sampling, the ml estimates of cell probabilities are \u2432 s\u02c6\n\u242ern. we next give the asymptotic cov \u2432 . lang 1996c showed the asymp-\n\u02c6\ntotic covariance matrix for \u242e for poisson sampling and its connection with\n.\n\u017e\ncov \u2432 .\u02c6\nthe saturated model has \u2432 s p, the sample proportions. under multino-\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nmial sampling, from 3.7 and 3.8 , their covariance matrix is\n\n\u017e\n\n.\n\n\u02c6\n\n\u017e\n\n.\n\ncov p s diag \u2432 y \u2432 \u2432 rn.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nx\n\n\u017e\n\n8.28\n\n.\n\n "}, {"Page_number": 356, "text": "model fitting: likelihood equations and asymptotics\n\n341\n\nwith i independent multinomial samples on a response variable with j\ncategories, \u2432 and p consist of i sets of proportions, each having j y 1\nnonredundant elements. then, cov p is a block diagonal matrix. each of the\nindependent samples has a j y 1 = j y 1 block of form 8.28 , and the\nmatrix contains zeros off the main diagonal of blocks.\n\n\u017e .\n.\n\nnow assume an unsaturated model. using the delta method we show in\nsections 14.2.2 and 14.4.1 that \u2432 has an asymptotic normal distribution\nabout \u2432. the estimated covariance matrix equals\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n$\ncov \u2432 s cov p x x cov p x\n.\n\n$\n\n$\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\nx\n\n\u02c6\n\n\u00bd\n\n$y1\n\nx\n\nx cov p rn.\n\n\u017e\n\n.\n\n5\n\nfor a single multinomial sample, this expression equals\n\n.\n\n\u017e\n\n\u02c6\n\n\u00bd\n\n$\ncov \u2432 s diag \u2432 y \u2432 \u2432 x x diag \u2432 y \u2432 \u2432 x\n.\n\u02c6 \u02c6 5\n\nx diag \u2432 y \u2432 \u2432 rn.\n\n\u02c6 \u02c6\n\n\u02c6 \u02c6\n\n.\u02c6\n\ny1\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nx\n\nx\n\nx\n\nx\n\nx\n\nfor tables with many cells, it is not unusual to have a sample proportion of\n0 in a cell. in this case the ordinary standard error is 0, which is unappealing.\nan advantage of fitting a model is that it typically has a positive fitted\nprobability and standard error.\n\n\u0004\n\n8.6.9 uniqueness of ml estimates\nwhen all n ) 0 , the ml estimates exist and are unique. to show this, for\nsimplicity we use poisson sampling. suppose that the model is parameterized\nso that x has full rank. birch 1963 showed that the likelihood equations are\nsoluble, by noting that the kernel of the poisson log likelihood\n\n\u017e\n\n.\n\n4\n\ni\n\nl \u242e s\n\u017e\n\n.\n\n\u00fd i\n\n\u017e\n\nn log \u242e y \u242e\n\ni\n\ni\n\ni\n\n.\n\ni\n\n.\n\n\u017e\n\nhas individual terms converging to y\u2b01 as log \u242e \u2122 \"\u2b01; thus, the log\nlikelihood is bounded above and attains its maximum at finite values of the\nmodel parameters. it is stationary at this maximum, since it has continuous\nfirst partial derivatives.\n\nbirch showed that the likelihood equations have a unique solution, and\nthe likelihood is maximized at that point. he proved this by showing that the\nmatrix of values y\u2b78 lr\u2b78\u2424 \u2b78\u2424 i.e., the information matrix x diag \u242e x is\nnonsingular and nonnegative definite, and hence positive definite. nonsingu-\nlarity follows from x having full rank and the diagonal matrix having positive\n.x\nelements \u242e . any quadratic form c x diag \u242e xc equals \u00fd \u242e \u00fd x c\n(\n2\ni j\ng 0, so the matrix is also nonnegative definite.\n\n'\nw\n\n) x\n\n4 w\n\n)\n\n(\n\n\u017e\n\n\u0004\n\n4\n\n\u0004\n\nh\n\n2\n\nx\n\nx\n\nx\n\ni\n\ni\n\ni\n\nj\n\nj\n\nj\n\n "}, {"Page_number": 357, "text": "342\n\nloglinear models for contingency tables\n\n8.7 loglinear model fitting: iterative methods\nand their application*\n\nwhen a loglinear model does not have direct estimates, iterative algorithms\nsuch as newton\u1390raphson can solve the likelihood equations. in this section\niterati\u00aee proportional\nwe also present a simpler but more limited method,\nfitting.\n\n8.7.1 newton\u1390raphson method\nin section 4.6.1 we introduced the newton\u1390raphson method. referring to\nnotation there, we identify l \u2424 as the log likelihood for poisson loglinear\nmodels.\n\n\u017e\n\n.\n\nfrom 8.21 , let\n\n\u017e\n\n.\n\nl \u2424 s n\n\u017e\n\n\u017e\n. \u00fd \u00fd\n\ni\n\ni\n\nh\n\nu s\n\nj\n\n.\n\n\u2b78l \u2424\u017e\n\u2b78\u2424\nj\n\n/\n\nx \u2424 y exp\n\n\u00fd\n\nih\n\nh\n\ni\n\n\u017e\n\n\u00fd\n\nh\n\n/\n\nx \u2424 .\nih\n\nh\n\ns n x y \u242e x ,\n\n\u00fd\n\n\u00fd\n\ni j\n\ni j\n\ni\n\ni\n\ni\n\ni\n\nh s\n\njk\n\n.\n\n\u2b782l \u2424\u017e\n\u2b78\u2424\u2b78\u2424\nk\n\nj\n\ns y \u242e x x ,\n\n\u00fd\n\nik\n\ni j\n\ni\n\ni\n\nthen\n\nso that\n\nu\u017et. s\n\nj\n\n\u017e\n\n\u00fd\n\ni\n\nn y \u242e\u017et. x\n.\n\ni\n\ni\n\nand\n\ni j\n\nh\u017et. s y \u242e\u017et. x x .\n\n\u00fd\n\nik\n\njk\n\ni j\n\ni\n\ni\n\nthe tth approximation \u242e\u017et.\n\u017et..\nexp x\u2424 . it generates the next value \u2424\ncontext is\n\nfor \u242e derives\n\u017etq1.\n\n\u02c6\n\n\u017e\n\nthrough \u242e\u017et. s\nfrom \u2424\u017et.\n.\nusing 4.39 , which in this\n\n\u017e\n\n\u017etq1.\n\n\u2424\n\ns \u2424 q x diag \u242e x\n.\n\n\u017et.\n\n\u017et.\n\n\u017e\n\nx\n\ny1\n\nthis in turn produces \u242e\u017etq1., and so on.\n\nalternatively, \u2424\u017etq1. can be expressed as\n\nx\n\nx n y \u242e .\n.\n\u017et.\n\n\u017e\n\n.\n8.29\nlog \u242e q n y \u242e r\u242e . the expression in brackets\n\nwhere r s \u00fd \u242e x\n\u017et.\ni\nis the first term in the taylor series expansion of log n at log \u242e\u017et..\n\ns y h\n\u017e\n\n\u017etq1.\n\n\u017et.\nj\n\n\u017et.\ni\n\n\u017et.\ni\n\n\u017et.\ni\n\n\u2424\n\n\u017et.\n\n\u017et.\n\n.\n\nr\n\n\u017e\n\n.\n\n\u017e\n\ni j\n\n,\n\ni\n\ni\n\ni\n\ny1\n\n "}, {"Page_number": 358, "text": "loglnear model fitting: iterative methods\n\n343\nthe iterative process begins with all \u242e\u017e0. s n , or with an adjustment such\nas \u242e s n q if any n s 0. then 8.29 produces \u2424 , and for t ) 0 the\n\u017e0.\ni\niterations proceed as just described with n . for loglinear models l \u2424 is\nconcave, and \u242e\u017et. and \u2424\u017et. usually converge rapidly to the ml estimates \u242e\u02c6\nand \u2424 as t increases. the h matrix converges to h s yx diag \u242e x. by\n\u02c6\nthe estimated large-sample covariance matrix of \u2424 is yh , a\n\u017e\n.\n8.25 ,\nby-product of the method.\n\n.\n\u02c6\n\u02c6 y1\n\n.\n\u0004\n\n\u02c6\n\n\u02c6\n\n\u017e1.\n\n\u017et.\n\n1\n2\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n4\n\nx\n\ni\n\ni\n\ni\n\ni\n\ni\n\nas we discussed in section 4.6.3 for glms, 8.29 has the iterative\n\n\u017e\n\n.\n\nreweighted least squares form\n\n\u017etq1.\n\n\u2424\n\ns x v x\n\ny1\n\u02c6\nt\n\n\u017e\n\nx\n\ny1\n\n.\n\nx\n\nx v z\n\ny1 \u017et.\n\u02c6\nt\n\n.\n\nhere,\nw\n\u017e\ndiag \u242e\n\n\u017e t.\nz\n\u017et..xy1\n\nhas elements n s log \u242e q n y \u242e r\u242e and v s\n. thus, \u2424\nis the weighted least squares solution for a model\n\n\u017etq1.\n\n\u02c6\nt\n\n\u017e t.\ni\n\n\u017e t.\ni\n\n\u017e t.\ni\n\n\u017e\n\n.\n\ni\n\ni\n\nz\u017et. s x\u2424 q \u2440 ,\n\n\u0004\n\nwhere \u2440 are uncorrelated with variances 1r\u242e . with \u242e s n , \u2424 is\nthe weighted least squares estimate for model log n s x\u2424 q \u2440.\n\n\u017et.4\ni\n\u017e .\n\n\u017e0.\ni\n\n\u017e1.\n\n\u0004\n\n\u0004\n\n4\n\n4\n\ni\n\ni\n\niterative proportional fitting\n\u017e\n\n8.7.2\nthe iterati\u00aee proportional fitting\nipf algorithm is a simple method for\ncalculating \u242e for loglinear models. introduced by deming and stephan\n\u017e\n1940 , it has the following steps:\n\n\u02c6i\n\n.\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n\u017e0.4\ni\n\n1. start with \u242e satisfying a model no more complex than the one being\n\nfitted. for instance, \u242e ' 1.0 are trivially adequate.\n\n4\n\n\u0004\n\n\u017e0.\ni\n\n2. by multiplying by appropriate factors, adjust \u242e successively to match\n\n\u0004\n\n\u017e0.4\ni\n\neach marginal table in the set of minimal sufficient statistics.\n\n3. continue until the maximum difference between the sufficient statistics\n\nand their fitted values is sufficiently close to zero.\n\n\u017e\nwe illustrate using model xy, xz, yz . its minimal sufficient statistics\n\u0004\n. initial estimates must satisfy the model. the\n\nare n\nfirst cycle of the ipf algorithm has three steps:\n\n, and n\n\n, n\n\nqj k\n\niqk\n\ni jq\n\n.\n\n\u0004\n\n4\n\n4\n\n\u0004\n\n4\n\n\u242e s \u242e\n\u017e0.\n\u017e1.\ni jk\ni jk\n\nn\n\u242e\n\ni jq\n\u017e0.\ni jq\n\n, \u242e s \u242e\n\u017e1.\ni jk\n\n\u017e2.\ni jk\n\nn\n\u242e\n\niqk\n\u017e1.\niqk\n\n, \u242e s \u242e\n\u017e2.\ni jk\n\n\u017e3.\ni jk\n\nn\nqj k\n\u017e2.\n\u242e\nqj k\n\n.\n\nsumming both sides of the first expression over k shows that \u242e\u017e1. s n\nfor\nall i and j. after step 1, observed and fitted values match in the xy marginal\ntable. after step 2, all \u242e\u017e2. s n\n, but the xy marginal tables no longer\nmatch. after step 3, all \u242e\u017e3. s n\n, but the xy and xz marginal tables no\n\ni jq\n\ni jq\n\niqk\nqj k\n\niqk\nqj k\n\n "}, {"Page_number": 359, "text": "344\n\nloglinear models for contingency tables\n\n\u017e\n\ni jq\n\n\u017e4.\ni jk\n\n\u017e3. \u017e\ni jk\n\n\u017e3. .\ni jq\n\nlonger match. a new cycle begins by again matching the xy marginal tables,\nusing \u242e s \u242e n r\u242e , and so on.\nat each step, the updated estimates continue to satisfy the model. for\ninstance, step 1 uses the same adjustment factor n r\u242e\nat different\nlevels k of z. thus, xy odds ratios from different levels of z have ratio\nequal to 1, and the homogeneous association pattern continues at each step.\nas the cycles progress, the g2 statistic comparing cell counts to the\nupdated fit is monotone decreasing, and the process must converge fienberg\n1970a; haberman 1974a . the ipf algorithm produces ml estimates because\nit generates a sequence of fitted values converging to a solution that both\nsatisfies the model and matches the sufficient statistics. by birch\u2019s results\n\u017e\nsection 8.6.3 , only one such solution exists, and it is ml.\n\n\u017e0. .\ni jq\n\ni jq\n\nthe ipf method works even for models having direct estimates. then, ipf\nnormally yields ml estimates within one cycle haberman 1974a, p. 197 . we\nillustrate with the model of independence. the minimal sufficient statistics\nare n\n\n. with \u242e ' 1.0 , the first cycle gives\n\n\u0004\nand n\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\niq\n\nqj\n\n\u017e0.\ni j\n\n\u242e s \u242e\n\u017e0.\n\u017e1.\ni j\ni j\n\n\u242e s \u242e\n\u017e1.\n\u017e2.\ni j\ni j\n\ns\n\ns\n\nn\n\u242e\n\niq\n\u017e0.\niq\n\nn\nqj\n\u017e1.\n\u242eqj\n\nn\n\niq\nj\n\n,\n\nn n\niq qj\nn\n\n.\n\nthe ipf algorithm then gives \u242e\u017et. s n n rn for all t ) 2.\n\niq qj\n\n\u02c6i j\n\n8.7.3 comparison of iterative methods\nthe ipf algorithm is simple and easy to implement. it converges to the ml\nfit even when the likelihood is poorly behaved, for instance with zero fitted\ncounts and estimates on the boundary of\nspace. the\nnewton\u1390raphson method is more complex, requiring solving a system of\nequations at each step. newton\u1390raphson is sometimes not feasible when the\nmodel is of high dimensionality\u138ffor instance, when the contingency table\nand parameter vector are huge.\n\nthe parameter\n\nhowever, ipf has disadvantages. it is applicable primarily to models for\nwhich likelihood equations equate observed and fitted counts in marginal\ntables. by contrast, newton\u1390raphson is a general-purpose method that can\nsolve more complex likelihood equations. ipf sometimes converges slowly\ncompared to newton\u1390raphson. unlike newton\u1390raphson, ipf does not\nproduce the model parameter estimates and their estimated covariance\nmatrix as a by-product. fitted values that ipf produces can generate this\nlog \u242e see prob-\ninformation. model parameter estimates are contrasts of\n\u02c6\n.\n.\n\u017e\nlems 8.16 and 8.17 , and substituting fitted values into 8.25 yields cov \u2424 .\n\n4 \u017e\n\n\u02c6i\n\nbecause newton\u1390raphson applies to a wide variety of models and also\nyields standard errors, it is the fitting routine used by most software for\n\n.\n\n\u017e\n\n\u0004\n\n "}, {"Page_number": 360, "text": "loglnear model fitting: iterative methods\n\n345\n\nloglinear models. ipf is increasingly viewed as primarily of historical interest.\nhowever, for some applications the analysis is more transparent using ipf, as\nthe next example illustrates.\n\n8.7.4 contingency table standardization\ntable 8.15 relates education and attitudes toward legalized abortion using a\ngeneral social survey, conducted by the national opinion research center.\nto make patterns of association clearer, smith 1976 standardized the table\nso that all row and column marginal totals equal 100 while maintaining the\nsample odds ratio structure.\n\n\u017e\n\n.\n\nthe ipf routine to standardize with margins of 100 is\n\nand then for t s 1, 3, 5, . . . ,\n\n\u242e\u017e0. s n\n\ni j\n\ni j\n\n\u242e s \u242e\n\u017ety1.\n\u017et.\ni j\ni j\n\n100\n\u017ety1.\n\u242e\niq\n\n, \u242e s \u242e\n\u017et.\ni j\n\n\u017etq1.\ni j\n\n100\n\u017et.\n\u242e\nqj\n\n.\n\n.\n\n\u017e\n\nat the end of each odd-numbered step, all row totals equal 100. at the end\nof each even-numbered step, all column totals equal 100. odds ratios do not\n.\nchange at each odd even step, since all counts in a given row column\nmultiply by the same constant.\n\nthe ipf algorithm converges to the entries in parentheses in table 8.15.\nthe association is clearer in this standardized table. a ridge appears down\nthe main diagonal, with higher levels of education having more favorable\nattitudes about abortion. the other counts fall away smoothly on both sides.\ntable standardization is useful for comparing tables having different\nmarginal structures. mosteller 1968 compared intergenerational occupa-\n\n\u017e\n\n.\n\n\u017e\n\ntable 8.15 marginal standardization of attitudes toward abortion\nby years of schooling\n\nattitude toward legalized abortion\n\ngenerally\ndisapprove\n\nmiddle\nposition\n\ngenerally\napprove\n\n.\n\n209\n\u017e\n49.4\n151\n\u017e\n32.8\n16\n\u017e\n.\n17.8\n\u017e\n.\n100\n\n.\n\n.\n\n101\n\u017e\n32.0\n126\n\u017e\n36.6\n21\n\u017e\n.\n31.3\n\u017e\n.\n100\n\n.\n\n.\n\n237\n\u017e\n18.6\n426\n\u017e\n30.6\n138\n\u017e\n.\n50.9\n\u017e\n.\n100\n\n.\n\ntotal\n\n\u017e\n.\n100\n\n\u017e\n.\n100\n\n\u017e\n.\n100\n\nschooling\nless than high school\n\nhigh school\n\nmore than high school\n\ntotal\n\n.\nsource: smith 1976 .\n\n\u017e\n\n "}, {"Page_number": 361, "text": "346\n\nloglinear models for contingency tables\n\n\u017e\n\n.\n\ntional mobility tables from britain and denmark. yule 1912 compared\nthree hospitals on vaccination and recovery for smallpox patients. a modern\napplication is adjusting sample data to match marginal distributions specified\nby census results.\n\nthe process of table standardization is called raking the table. imrey et al.\n\u017e\n1981 and little and wu 1991 derived the asymptotic covariance matrix for\nraked sample proportions. for sample counts n\n, let\n\u0004\ne denote expected frequencies for the standardized table and e fitted\nvalues in the standardized table. the standardization process corresponds to\nfitting the model\n\nwith \u242e s e n\n\u017e\ni j\u02c6\n4\n\n.4\n\n\u017e\n\n.\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n\u0004\n\n4\n\ni j\n\ni j\n\ni j\n\ni j\n\nlog e r\u242e s \u242dq \u242de q \u242da.\n\n\u017e\n\n.\n\ni j\n\ni j\n\ni\n\nj\n\nthat is, maintaining the odds ratios means that the two-way tables of\ne r\u242e and of e rn\n\u0004\n4\ni j\u02c6\u0004\ni j\n4\nthe fitted values e in the standardized table satisfy\n\nsatisfy independence.\n\n\u02c6\ni j\n\n4\n\n\u0004\n\ni j\n\ni j\n\nlog e y log n s \u242dq \u242d q \u242d .\n\u02c6a\nj\n\n\u02c6e\ni\n\n\u02c6\ni j\n\n\u02c6\n\ni j\n\ni j\n\nthe adjustment term, ylog n , to the log link of the fit is called an offset.\nthe fit corresponds to using log n as a predictor on the right-hand side and\nforcing its coefficient to equal 1.0. standard glm software can fit models\nhaving offsets. to rake a table, one enters as sample data pseudo-values that\nsatisfy independence and have the desired margins, taking log n\nas an\noffset. for sas, see table a.14 . in section 9.7.1 we discuss further the use\nof model offsets.\n\n.\n\n\u017e\n\ni j\n\ni j\n\nnotes\n\nsection 8.2: loglinear models for independence and interaction in three-way tables\n\n\u017e\n\n.\n\n8.1. roy and mitra 1956 discussed types of independence for three-way tables and their\nlarge-sample tests. birch\u2019s 1963 article on ml estimation for loglinear models was part\nof substantial research on loglinear models in the 1960s, much due to l. a. goodman\n\u017e\nsee section 16.4 . haberman 1974a presented an influential theoretical study of\nloglinear models.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\nsection 8.3: inference for loglinear models\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n8.2. goodman 1970, 1971b , haberman 1974a, chap. 5 , lauritzen 1996 , sundberg 1975 ,\nand whittaker 1990, sec. 12.4 discussed families of loglinear models that have direct\nml estimates and interpretations in terms of independence, conditional independence,\nor equiprobability. such models are called decomposable, since expected frequencies\ndecompose into products and ratios of expected marginal sufficient statistics. haberman\nproved conditions under which loglinear models have direct estimates. baglivo et al.\n\u017e\n1992 , forster et al. 1996 , and morgan and blumenstein 1991 discussed exact\ninference.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n "}, {"Page_number": 362, "text": "problems\n\n347\n\n8.3. for methods that allow for misclassification error, see kuha and skinner 1997 and\nkuha et al. 1998 and references therein. for treatment of missing data, see little\n\u017e\n1998 , schafer 1997, chap. 8 , and their references.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nsection 8.7: loglinear model fitting: iterati\u00a9e methods and their application\n\n.\n\n\u017e\n\u017e\n\n.\n\n8.4. deming 1964, chap. vii described early work on ipf by deming and stephan.\ndarroch 1962 used ipf to obtain ml estimates in contingency tables. bishop et al.\n\u017e\n1975 , fienberg 1970a , and speed 1998 presented other applications of ipf. darroch\nand ratcliff 1972 generalized ipf for models in which sufficient statistics are more\ncomplex than marginal tables.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n8.5. for further discussion of table raking, see bishop et al. 1975, pp. 76\u1390102 , fleiss 1981,\n\n.\nchap. 14 , haberman 1979, chap. 9 , hoem 1987 , and little and wu 1991 .\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\nproblems\n\napplications\n\n\u017e .\n\n8.1 the 1988 general social survey compiled by the national opinion\nresearch center asked:\n\u2018\u2018do you support or oppose the following\nmeasures to deal with aids? 1 have the government pay all of the\nhealth care costs of aids patients; 2 develop a government informa-\ntion program to promote safe sex practices, such as the use of con-\n.\ndoms.\u2019\u2019 table 8.16 summarizes opinions about health care costs h\nand the information program i , classified also by the respondent\u2019s\n.\ngender g .\na. fit loglinear models gh, gi , gh, hi , gi, hi , and gh, gi,\n\n\u017e .\n\n. \u017e\n\n. \u017e\n\n\u017e .\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.hi . show that models that lack the hi term fit poorly.\n\n\u017e\n\n\u017e\n\nb. for model gh, gi, hi , show that 95% wald confidence intervals\n.\nequal 0.55, 1.10 for the gh conditional odds ratio and 0.99, 2.55\nfor the gi conditional odds ratio. interpret. is it plausible that\ngender has no effect on opinion for these issues?\n\n.\n\n\u017e\n\n.\n\ntable 8.16 data for problem 8.1\n\ngender\nmale\n\nfemale\n\ninformation\n\nopinion\nsupport\noppose\nsupport\noppose\n\nhealth opinion\n\nsupport\n\noppose\n\n76\n6\n114\n11\n\n160\n25\n181\n48\n\nsource: 1988 general social survey, national opinion research\ncenter.\n\n "}, {"Page_number": 363, "text": "348\n\nloglinear models for contingency tables\n\ntable 8.17 data for problem 8.2 a\n\npresident\n\nbusing\n\n1\n\n2\n\n3\n\n1\n2\n3\n1\n2\n3\n1\n2\n3\n\nhome\n2\n65\n157\n17\n5\n44\n0\n3\n10\n0\n\n1\n41\n71\n1\n2\n3\n1\n0\n0\n0\n\n3\n0\n1\n0\n0\n0\n0\n1\n0\n1\n\na1, yes; 2, no; 3, don\u2019t know.\nsource: 1991 general social survey, national opinion research\ncenter.\n\n.\n\n\u017e\n\n8.2 refer to table 8.17 from the 1991 general social survey. white\nsubjects were asked: b \u2018\u2018do you favor busing of negrorblack and\nwhite school children from one school district to another?\u2019\u2019, p \u2018\u2018if\nyour party nominated a negrorblack for president, would you vote\nfor him if he were qualified for the job?\u2019\u2019, d \u2018\u2018during the last few\nyears, has anyone in your family brought a friend who was a\nnegrorblack home for dinner?\u2019\u2019 the response scale for each item\n\u017e\n.\nwas yes, no, don\u2019t know . fit model bd, bp, dp .\na. using the yes and no categories, estimate the conditional odds ratio\n\n.\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\nfor each pair of variables. interpret.\n\nb. analyze the model\u2019s goodness of fit. interpret.\nc. conduct inference for the bp conditional association using a wald\n\nor likelihood-ratio confidence interval and test. interpret.\n\n8.3 refer to section 8.3.2. explain why software for which parameters sum\nto zero across levels of each index reports \u242d s \u242d s 0.514 and\n\u242d s \u242d s y0.514, with se s 0.044 for each term.\n\u02c6ac\n12\n\n\u02c6ac\n11\n\n\u02c6ac\n21\n\n\u02c6ac\n22\n\n8.4 refer to table 2.6. let d s defendant\u2019s race, v s victims\u2019 race, and\n\np s death penalty verdict. fit the loglinear model dv, dp, pv .\n.\na. using the fitted values, estimate and interpret\n\nthe odds ratio\nbetween d and p at each level of v. note the common odds ratio\nproperty.\n\n\u017e\n\nb. calculate the marginal odds ratio between d and p,\n\n\u017e .\ni using the\nfitted values, and ii using the sample data. why are they equal?\ncontrast the odds ratio with part a . explain why simpson\u2019s para-\ndox occurs.\n\n\u017e .\n\n\u017e .\n\n "}, {"Page_number": 364, "text": "problems\n\n349\n\ntable 8.18 data for problem 8.5\n\nsafety equipment\nin use\nseat belt\n\nnone\n\nwhether\nejected\n\nyes\nno\nyes\nno\n\ninjury\n\nnonfatal\n1,105\n411,111\n4,624\n157,342\n\nfatal\n14\n483\n497\n1,008\n\nsource: florida department of highway safety and motor vehicles.\n\nc. fit the corresponding logit model, treating p as the response. show\nthe correspondence between parameter estimates and fit statistics.\nd. is there a simpler model that fits well? interpret, and show the\n\nlogit\u1390loglinear connection.\n\n8.5 table 8.18 refers to automobile accident records in florida in 1988.\n\na. find a loglinear model that describes the data well. interpret\n\nassociations.\n\nb. treating whether killed as the response, fit an equivalent logit\n\nmodel. interpret the effects.\n\nc. since n is large, goodness-of-fit statistics are large unless the model\nfits very well. calculate the dissimilarity index for the model in part\n\u017e .a , and interpret.\n\n8.6 refer to table 8.19. subjects were asked their opinions about govern-\n\u017e\nment spending on the environment e , health h , assistance to big\n.\ncities c , and law enforcement l .\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ntable 8.19 data for problem 8.6 a\n\nenvironment health enforcement:\n\nlaw\n\n1\n\n2\n\n3\n\n1\n2\n3\n1\n2\n3\n1\n2\n3\n\ncities\n\n2\n2\n42\n18\n0\n13\n9\n1\n1\n1\n0\n\n3\n5\n0\n1\n0\n0\n1\n0\n0\n0\n\n1\n90\n22\n2\n21\n6\n2\n2\n2\n0\n\n3\n3\n1\n1\n2\n0\n1\n0\n0\n0\n\n1\n74\n19\n1\n20\n6\n4\n9\n4\n1\n\n1\n2\n17\n7\n3\n3\n4\n0\n0\n0\n0\n\n1\n62\n11\n2\n11\n1\n1\n3\n1\n1\n\n3\n2\n31\n14\n3\n8\n5\n3\n2\n2\n2\n\n3\n11\n3\n1\n3\n2\n1\n1\n0\n3\n\na1, too little; 2, about right; 3, too much.\nsource: 1989 general social survey, national opinion research center.\n\n "}, {"Page_number": 365, "text": "350\n\nloglinear models for contingency tables\n\ntable 8.20 output for fitting model to table 8.19\n\ncriteria for assessing goodness of fit\n\ncriterion\ndeviance\npearson chi- square\nlog likelihood\n\ndf\n48\n48\n\nvalue\n31.6695\n26.5224\n1284.9404\n\nvalue / df\n\n0.6598\n0.5526\n\nstandard\n\nwald 95%\n\nparameter\ne*h\ne*h\ne*h\ne*h\ne*l\ne*l\ne*l\ne*l\ne*c\ne*c\ne*c\ne*c\nh*c\nh*c\nh*c\nh*c\nh*l\nh*l\nh*l\nh*l\nc*l\nc*l\nc*l\nc*l\n\n1\n1\n2\n2\n1\n1\n2\n2\n1\n1\n2\n2\n1\n1\n2\n2\n1\n1\n2\n2\n1\n1\n2\n2\n\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n\ndf\nestimate\n1\n2.1425\n1\n1.4221\n1\n0.7294\n1\n0.3183\n1 y0.1328\n1\n0.3739\n1 y0.2630\n0.4250\n1\n1.2000\n1\n1.3896\n1\n0.6917\n1\n1\n1.3767\n1 y0.1865\n1\n0.7464\n1 y0.4675\n0.7293\n1\n1\n1.8741\n1.0366\n1\n1.9371\n1\n1.8230\n1\n0.8735\n1\n1\n0.5707\n1.0793\n1\n1\n1.2058\n\nerror\n0.5566\n0.6034\n0.5667\n0.6211\n0.6378\n0.6975\n0.6796\n0.7361\n0.5177\n0.4774\n0.5605\n0.5024\n0.4547\n0.4808\n0.4978\n0.5023\n0.5079\n0.5262\n0.6226\n0.6355\n0.4604\n0.4863\n0.4326\n0.4462\n\nconfidence\n\n1.0515\n0.2394\ny0.3813\ny0.8991\ny1.3829\ny0.9931\ny1.5949\ny1.0178\n0.1854\n0.4540\ny0.4068\n0.3921\ny1.0777\ny0.1959\ny1.4431\ny0.2553\n0.8786\n0.0052\n0.7168\n0.5775\ny0.0289\ny0.3824\n0.2314\n0.3312\n\nlimits\n3.2335\n2.6049\n1.8402\n1.5356\n1.1172\n1.7410\n1.0689\n1.8678\n2.2147\n2.3253\n1.7902\n2.3614\n0.7048\n1.6886\n0.5081\n1.7138\n2.8696\n2.0680\n3.1574\n3.0686\n1.7760\n1.5239\n1.9271\n2.0804\n\nchi-\n\nsquare\n14.81\n5.55\n1.66\n0.26\n0.04\n0.29\n0.15\n0.33\n5.37\n8.47\n1.52\n7.51\n0.17\n2.41\n0.88\n2.11\n13.61\n3.88\n9.68\n8.23\n3.60\n1.38\n6.23\n7.30\n\na. table 8.20 shows some results, including the two-factor estimates,\nfor the homogeneous association model. check the fit, and inter-\npret.\n\nb. all estimates at category 3 of each variable equal 0. report the\nestimated conditional odds ratios using the too much and too little\ncategories for each pair of variables. summarize the associations.\nbased on these results, which term s might you consider dropping\nfrom the model? why?\n\u02c6e h\nc. table 8.21 reports \u242d\neh\n\nwhen parameters sum to zero within rows\nand within columns, and when parameters are zero in the first row\nand first column. show how these yield the estimated eh condi-\ntional odds ratio for the too much and too little categories. com-\npare to part b . construct a confidence interval for that odds ratio.\ninterpret.\n\n\u017e .\n\n\u017e .\n\n\u0004\n\n4\n\n "}, {"Page_number": 366, "text": "problems\n\n351\n\ntable 8.21 parameter estimates for problem 8.6\n\nsum to zero constraints\n\nzero for first level\n\ne\n1\n2\n3\n\n1\n0.509\ny0.065\ny0.445\n\nh\n2\n0.166\ny0.099\ny0.068\n\n3\n\ny0.676\n0.163\n0.513\n\n1\n0\n0\n0\n\nh\n\n2\n0\n\n0.309\n0.720\n\n3\n0\n\n1.413\n2.142\n\n8.7 refer to the loglinear models for table 8.8.\n\na. explain why the fitted odds ratios in table 8.10 for model\n\u017e\ngi, gl, gs, il, is, ls suggest that the most likely accident case\nfor injury is females not wearing seat belts in rural locations.\n\n.\n\nb. fit model gls, gi, il, is . using model parameter estimates,\nshow that the fitted is conditional odds ratio equals 0.44. show that\nfor each injury level, the estimated conditional ls odds ratio is 1.17\nfor g s female and 1.03 for g s male . how can you get these\nusing the model parameter estimates?\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n8.8 consider the following two-stage model for table 8.8. the first stage is\na logit model with s as the response for the three-way gls table. the\nsecond stage is a logit model with these three variables as predictors\nfor i in the four-way table. explain why this composite model\nis\nsensible, fit the models, and interpret results.\n\n8.9 refer to the logit model in problem 5.24. let a s opinion on abor-\n\ntion.\na. give the symbol for the loglinear model that is equivalent to this\n\nlogit model.\n\n.\nb. which logit model corresponds to loglinear model ar, ap, grp ?\nc. state the equivalent loglinear and logit models for which i a is\n\u017e .\nii there are main effects of r\njointly independent of g, r, and p;\n\u017e\n.\non a, but a is conditionally independent of g and p, given r;\niii\nthere is interaction between p and r in their effects on a, and g\nhas main effects.\n\n\u017e .\n\n\u017e\n\n8.10 for a multiway contingency table, when is a logit model more appro-\npriate than a loglinear model? when is a loglinear model more\nappropriate?\n\n8.11 using software, conduct the analyses described in this chapter for the\n\n.\nstudent survey data table 8.3 .\n\n\u017e\n\n "}, {"Page_number": 367, "text": "352\n\nloglinear models for contingency tables\n\n8.12 standardize table 10.6. describe the migration patterns.\n\n\u017e\n\n8.13 the book\u2019s web site www.stat.ufl.edur;aarcdarcda.html has a 2 =\n3 = 2 = 2 table relating responses on frequency of attending religious\nservices, political views, opinion on making birth control available to\nteenagers, and opinion about a man and woman having sexual rela-\ntions before marriage. analyze these data using loglinear models.\n\n.\n\ntheory and methods\n8.14 suppose that \u242e s n\u2432 satisfy the independence model 8.1 .\n.\n\n\u017e\n\n\u0004\n\n4\n\na. show that \u242d y \u242d s log \u2432 r\u2432 .\n.\nqb\nb. show that all \u242d s 0 is equivalent to \u2432 s 1rj for all\n\nqa\n\ny\nb\n\n\u017e\n\n\u0004\n\n4\n\nj.\n\ni j\ny\na\n\ni j\n\nqj\n\ny\nj\n\n8.15 refer to the independence model, \u242e s \u242e\u2423 \u2424. for the corresponding\n\ni j\n\ni\n\nj\n\n.\nloglinear model 8.1 :\na. show that one can constrain \u00fd\u242dx s \u00fd\u242dy s 0 by setting\n\n\u017e\n\ni\n\nj\n\n\u242dx s log \u2423 y log \u2423 i, \u242dy s log \u2424 y log \u2424 j,\n\ni\n\ni\n\n\u017e\n\n\u00fd\n\nh\n\n/\n\nh\n\n\u017e\n\u017e\n\n\u00fd\n\nh\n\n\u00fd\n\nh\n\n/\n/\n\nh\n\nh\n\nj\n\n\u017e\n\n\u00fd\n\nh\n\nj\n\n/\n\nh\n\n\u242ds log \u242eq log \u2423 i q log \u2424 j.\n\nb. show that one can constrain \u242dx s \u242dy s 0 by defining \u242dx s log \u2423\n\ny log \u2423 and \u242dy s log \u2424 y log \u2424 . then, what does \u242dequal?\n\n1\n\n1\n\ni\n\ni\n\n1\n\nj\n\n1\n\nj\n\n8.16 for an i = j table, let \u2429 s log \u242e , and let a dot subscript denote the\nmean for that index e.g., \u2429s \u00fd \u2429 rj . then, let \u242ds \u2429 , \u242d s \u2429y\n\u2429 , \u242dy s \u2429 y \u2429 , and \u242dx y s \u2429 y \u2429y \u2429 q \u2429 .\na. show that log \u242e s \u242dq \u242dx q \u242dy q \u242dx y. hence, any set of positive\n\nx\ni\n\n\u017e\n\n.\n\n. j\n\n. j\n\ni j\n\ni j\n\ni j\n\ni j\n\ni j\n\n. .\n\n. .\n\n. .\n\n. .\n\ni.\n\ni.\n\ni.\n\nj\n\nj\n\n\u0004\n\nj\n4\u242e satisfies the saturated model.\ni j\n\ni j\n\ni\n\ni j\n\nj\n\nj\n\nj\n\ni\n\ni\n\ni\n\ni j\n\ni j\n\n\u242e \u242e r\u242e \u242e , j s 2, . . . , j.\n\nb. show that \u00fd \u242dx s \u00fd \u242dy s \u00fd \u242dx y s \u00fd \u242dx y s 0.\nc. for 2 = 2 tables, show that log \u242as 4\u242dx y.\nd. for 2 = j tables, show that \u242d s \u00fd log \u2423 r2 j, where \u2423 s\ne. alternative constraints have other odds ratio formulas. let \u242ds \u2429 ,11\n\u242dx s \u2429 y \u2429 , \u242dy s \u2429 y \u2429 , and \u242dx y s \u2429 y \u2429 y \u2429 q \u2429 .\nthen, show that the saturated model holds with \u242dx s \u242dy s \u242dx y s\n\u242d s 0 for all i and j, and \u242d s log \u242e \u242e r\u242e \u242e .\n.\n\nx y\n11\n\n11\n\u017e\n\n2 j\n\n11\n\n21\n\n11\n\n11\n\n11\n\n1 j\n\n1 j\n\n1 j\n\n1 j\n\n\u017e\n\n.\n\ni1\n\ni1\n\ni j\n\ni j\n\n1\n\n1\n\ni\n\nj\n\nj\n\nj\n\nj\n\n11\n\ni j\n\n1 j\n\ni1\n\nx y\ni j\n\nx y\ni1\n\n "}, {"Page_number": 368, "text": "353\nproblems\n8.17 suppose that all \u242e ) 0. let \u2429 s log \u242e , and consider model\n\ni jk\nparameters with zero-sum constraints.\n\u017e\na. for the general loglinear model 8.12 , define parameters in the\n\nfashion of problem 8.16 e.g., \u242d s \u2429 y \u2429 y \u2429 q \u2429 .\n.\n\ni jk\n\ni jk\n\n.\n\ni j.\n\ni. .\n\n. j.\n\nx y\ni j\n\n. . .\n\nb. for model xy, xz, yz with a 2 = 2 = 2 table, show that \u242d\nx y\n11\n\n\u017e\n.\n\n1s log \u242a\n\n4\n\n\u017e\n\n.\n\n11\u017e k.\n.\n\nc. for xyz with a 2 = 2 = 2 table, show that\n\n\u017e\n\nx y z\u242d s log \u242a r\u242a\n\n1\n8\n\n11\u017e2.\nthus, \u242dx y z s 0 is equivalent to \u242a s \u242a .\n11\u017e2.\n\n11\u017e1.\n\n11\u017e1.\n\n111\n\ni jk\n\n.\n\n8.18 two balanced coins are flipped, independently. let x s whether the\nfirst flip resulted in a head yes, no , y s whether the second flip\nresulted in a head, and z s whether both flips had the same result.\nusing this example, show that marginal independence for each pair of\nthree variables does not imply that the variables are mutually indepen-\ndent.\n\n\u017e\n\n.\n\n8.19 for three categorical variables x, y, and z:\n\na. when y is jointly independent of x and z, show that x and y are\n\nconditionally independent, given z.\n\nb. prove that mutual independence of x, y, and z implies that x and\n\ny are both marginally and conditionally independent.\n\nc. when x is independent of y and y is independent of z, does it\n\nfollow that x is independent of z? explain.\n\nd. when any pair of variables is conditionally independent, explain\n\nwhy there is no three-factor interaction.\n\n8.20 suppose that x and y are conditionally independent, given z, and x\n\nand z are marginally independent.\na. show that x is jointly independent of y and z.\nb. show x and y are marginally independent.\n\u017e\nc. show that if x and z are conditionally rather than marginally\n\n.\n\nindependent, then x and y are still marginally independent.\n\n8.21 a 2 = 2 = 2 table satisfies \u2432 s \u2432 s \u2432 s , all\n.\n\ni, j, k. give\n\u017e . \u017e\n.\nan example of \u2432 that satisfies model a x, y, z , b xy, z ,\n\u017e . \u017e\nc xy, yz , d xy, xz, yz , and e xyz , but in each case not a\nsimpler model.\n\nqqk\n\u017e . \u017e\n.\n\n. \u017e . \u017e\n\n\u017e . \u017e\n\nqjq\n\niqq\n\ni jk\n\n1\n2\n\n.\n\n\u0004\n\n4\n\n8.22 suppose that model xy, xz, yz holds in a 2 = 2 = 2 table, and the\nlog odds ratio at the two levels of z is\n\ncommon xy conditional\n\n\u017e\n\n.\n\n "}, {"Page_number": 369, "text": "354\n\nloglinear models for contingency tables\n\npositive. if the xz and yz conditional\nlog odds ratios are both\npositive or both negative, show that the xy marginal odds ratio is\nlarger than the xy conditional odds ratio. hence, simpson\u2019s paradox\ncannot occur for the xy association.\n\n8.23 show that the general loglinear model in t dimensions has 2t terms.\ntwo-factor\n\nsingle-factor terms,\n\nw hint: it has an intercept,\nterms, . . . .\n\nx\n\n\u017e\n\nt\n1\n\n/\n\n\u017e\n\nt\n2\n\n/\n\n8.24 each of t responses is binary. for dummy variables\nloglinear model of mutual independence has the form\n\n\u0004\n\nz , . . . , z\n1\n\nt\n\n4\n\n, the\n\nlog \u242e\n\nz , . . . , z\n1\n\nt\n\ns \u242d z q \u2b48\u2b48\u2b48 q\u242d z .\n\nt t\n\n1 1\n\n.\nshow how to express the general loglinear model cox 1972 .\n\n\u017e\n\n8.25 consider a cross-classification of w, x, y, z.\n\na. explain why wxz, wyz is the most general loglinear model for\n\n\u017e\n\n.\n\nwhich x and y are conditionally independent.\n\nb. state the model symbol for which x and y are conditionally\n\nindependent and there is no three-factor interaction.\n\n8.26 for a four-way table with binary response y, give the equivalent\n\nloglinear and logit models that have:\na. main effects of a, b, and c on y.\nb. interaction between a and b in their effects on y, and c has main\n\nc. repeat part a for a nominal response y with a baseline-category\n\n\u017e .\n\neffects.\n\nlogit model.\n\n8.27 for a 3 = 3 table with ordered rows having scores\n\u017e\n.\n8.18\n.\n\n\u0004\nx , identify all\ni\n\u017e .\nterms in the generalized loglinear model\nfor models\na\nlogit p y f j s \u2423 q \u2424x , and b log p y s j rp y s 3 s \u2423 q\n\u017e\n\u017e\n\u2424 x .\ni\n\n\u017e .\n\n.x\n\n.x\n\n\u017e\n\nw\n\nw\n\n4\n\ni\n\nj\n\nj\n\nj\n\n8.28 for the independence model for a two-way table, derive minimal\nsufficient statistics, likelihood equations, fitted values, and residual df.\n8.29 for the loglinear model for an i = j table, log \u242e s \u242dq \u242dx, show\n\ni j\n\ni\n\nthat \u242e s n rj and residual df s i j y 1 .\n.\n\n\u017e\n\n\u02c6i j\n\niq\n\n8.30 write the log likelihood l for model xz, yz . calculate \u2b78lr\u2b78\u242d and\nshow that it implies \u242e s n. show that \u2b78lr\u2b78\u242dx s n y \u242e .\niqq\n\n\u02c6qqq\n\niqq\n\n\u017e\n\n.\n\ni\n\n "}, {"Page_number": 370, "text": "problems\n\n355\n\nsimilarly, differentiate with respect to each parameter to obtain likeli-\nhood equations. show 8.23 and 8.24 imply the other equations, so\nthose equations determine the ml estimates.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n8.31 for model xy, z , derive a minimal sufficient statistics, b likeli-\n\nhood equations, c fitted values, and d residual df for tests of fit.\n\n\u017e .\n\n.\n\u017e .\n\n\u017e .\n\n\u017e .\n\n\u017e\n\n.\n8.32 consider the loglinear model with symbol xz, yz .\n\n\u017e\n\na. for fixed k, show that \u242e equal the fitted values for testing\n\n\u0004\n\n\u02c6i jk\n\n4\n\nindependence between x and y within level k of z.\n\nb. show that the pearson and likelihood-ratio statistics for testing this\nmodel\u2019s fit have form x 2 s \u00fd x 2, where x 2 tests independence\nk\nbetween x and y at level k of z.\n\nk\n\n.\n8.33 verify the df values shown in table 8.14 for models xy, z , xy, yz ,\n\n. \u017e\n\n\u017e\n\n.\nand xy, xz, yz .\n\n\u017e\n\n.\n8.34 verify that loglinear model gls, gi, li, is implies logit model 8.16 .\nshow that the conditional log odds ratio for the effect of s on i equals\n\u2424s y \u2424s in the logit model and \u242dis q \u242dis y \u242dis y\u242dis in the loglinear\nmodel.\n\n11\n\n22\n\n12\n\n21\n\n\u017e\n\n.\n\n\u017e\n\n1\n\n2\n\n8.35 table 8.22 shows fitted values for models for four-way tables that have\n\ndirect estimates.\n.\na. use birch\u2019s results to verify that the entry is correct for w, x, y, z .\n\n\u017e\n\nverify its residual df.\n\n.\nb. motivate the estimate and df formulas for wx, yz , wxy, z ,\n\u017e\nwxy, wz , and wxy, wxz using composite variables and the\ncorresponding results for two-way tables e.g., for wxy, wz , given\nx\nw, z is independent of the composite xy variable .\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nw\n\ntable 8.22 data for problem 8.35 a\n\n2\n\n3\n\n.\n\n.\n\n.\n\nrn\n\nresidual df\n\nexpected frequency estimate\n\nmodel\n\u017e\nw, x, y, z\n.\n\u017e\nwx, y, z\n\u017e\nwx, wy, z\n\u017e\nwx, yz\n\u017e\nwx, wy, xz\n\u017e\nwx, wy, wz\n.\n\u017e\nwxy, z\n.\n\u017e\nwxy, wz\n\u017e\nwxy, wxz\nanumber of levels of w, x, y, z, denoted by h, i, j, k. estimates for other models of each type\nare obtained by symmetry.\n\nhijk y h y i y j y k q 3\nhijk y hi y j y k q 2\nhijk y hi y hj y k q h q 1\nhi y 1 jk y 1\n.\n\u017e\nhijk y hi y hj y ik q h q i\nhijk y hi y hj y hk q 2 h\nhij y 1 k y 1\n.\n\u017e\nh ij y 1 k y 1\n.\n\u017e\nhi j y 1 k y 1\n.\n\nn\nhqqq qiqq qqjq qqqk\nn\nhiqq qqjq qqqk\nn\nhiqq hqjq qqqk\nrn\nhiqq qqj k\nn\nhiqq hqjq qiqk\nn\nhiqq hqjq hqqk\nrn\nhi jq qqqk\nrn\nhi jq hqqk\nrn\nhi jq hiqk\n\nn\nrn\nrn\nrn\nr n\n\u017e\n\nn\nn\nn\nn\nn\nn\nn\nn\nn\n\nhqqq\nhiqq\n\nn\nn\nn\nn\nn\nn\nn\nn\nn\n\nn\nhqqq\n\nhqqq qiqq\n\n.\u017e\n.\u017e\n.\u017e\n\nhqqq\n\n.\n.\n\n.\u017e\n\nn\n\n\u017e\n\n.\n\n.\n\n2\n\n "}, {"Page_number": 371, "text": "356\n\nloglinear models for contingency tables\n\n8.36 a t-dimensional table n\n\n\u0004\n\n4\n\nab . . . t\n\nhas i categories in dimension i.\n\ni\n\na. find minimal sufficient statistics, ml estimates of cell probabilities,\n\nand residual df for the mutual independence model.\n\nb. find the minimal sufficient statistics and residual df for the hierar-\nchical model having all two-factor associations but no three-factor\ninteractions.\n\n8.37 consider loglinear model x, y, z for a 2 = 2 = 2 table.\n\na. express the model in the form log \u242e s x\u2424.\nb. show that the likelihood equations x n s x \u242e equate n\n\u0004\n\nx\n\nx\n\n\u017e\n\n.\n\n\u0004\n\n4\u242e in the one-dimensional margins.\n\u02c6i jk\n\n4\n\ni jk\n\nand\n\n8.38 apply ipf to model a x, yz , and b xz, yz . show that the ml\n\n\u017e . \u017e\n\n.\nestimates result within one cycle.\n\n\u017e . \u017e\n\n\u02c6\n\n.\n\n4\n8.39 given target row totals r ) 0 and column totals c ) 0 :\n\u0004\na. explain how to use ipf to adjust sample proportions p\ni j\n\n\u0004\n\n4\n\n\u0004\n\ni\n\nj\n\nthese totals but maintain the sample odds ratios.\n\n4\n\nto have\n\nb. show how to find cell proportions that have these totals and for\nwhich all local odds ratios equal \u242a) 0. hint: take initial values of\n1.0 in all cells in the first row and in the first column. this\ndetermines all other initial cell entries such that all local odds ratios\nequal \u242a.\n\n.\n\n\u017e\n\nc. explain how cell proportions are determined by the marginal pro-\n\nportions and the local odds ratios.\n\n8.40 refer to birch\u2019s results in section 8.6.3. show that l has individual\nterms converging to y\u2b01 as log \u242e \u2122 \"\u2b01. explain why positive defi-\nniteness of the information matrix implies that the solution of the\nlikelihood equations is unique, with likelihood maximized at that point.\n\ni\n\n "}, {"Page_number": 372, "text": "c h a p t e r 9\n\nbuilding and extending\nloglinear logit models\n\nr\n\nin chapters 5 through 7 we presented logistic regression models, which use\nthe logit link for binomial or multinomial responses. in chapter 8 we\npresented loglinear models for contingency tables, which use the log link for\npoisson cell counts. equivalences between them were discussed in section\n8.5.3. in this chapter we discuss building and extending these models with\ncontingency tables.\n\nin section 9.1 we present graphs that show a model\u2019s association and\nconditional independence patterns. in section 9.2 we discuss selection and\ncomparison of loglinear models. diagnostics for checking models, such as\nresiduals, are presented in section 9.3.\n\nthe loglinear models of chapter 8 treat all variables as nominal. in\nsection 9.4 we present loglinear models of association between ordinal\nvariables. in sections 9.5 and 9.6 we present generalizations that replace\nfixed scores by parameters. in the final section we discuss complications that\noccur with sparse contingency tables.\n\n9.1 association graphs and collapsibility\n\na graphical representation for associations in loglinear models indicates the\npairs of conditionally independent variables. this representation helps reveal\nimplications of models. our presentation derives partly from darroch et al.\n\u017e\n1980 , who used mathematical graph theory to represent certain loglinear\nmodels called graphical models having a conditional independence struc-\nture.\n\n.\n\n.\n\n\u017e\n\n9.1.1 association graphs\nan association graph has a set of vertices, each vertex representing a variable.\nan edge connecting two variables represents a conditional association be-\n357\n\n "}, {"Page_number": 373, "text": "358\n\nbuilding and extending loglinearr logit models\n\n.\nfigure 9.1 association graph for model wx, wy, wz, yz .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\ntween them. for instance, loglinear model wx, wy, wz, yz lacks xy and\nxz terms. it assumes independence between x and y and between x and\nz, conditional on the remaining two variables. figure 9.1 portrays this\nmodel\u2019s association graph. the four variables form the vertices. the four\nedges represent pairwise conditional associations. edges do not connect x\nand y or x and z, the conditionally independent pairs.\n\ntwo loglinear models with the same pairwise associations have the same\nassociation graph. for instance, this association graph is also the one for\nmodel wx, wyz , which adds a three-factor wyz interaction.\n\na path in an association graph is a sequence of edges leading from one\nvariable to another. two variables x and y are said to be separated by a\nsubset of variables if all paths connecting x and y intersect that subset. for\ninstance, in figure 9.1, w separates x and y, since any path connecting x\nand y goes through w. the subset w, z also separates x and y.\na fundamental result states that two variables are conditionally independent\ngiven any subset of variables that separates them kreiner 1987; whittaker\n1990, p. 67 . thus, not only are x and y conditionally independent given w\nand z, but also given w alone. similarly, x and z are conditionally\nindependent given w alone.\n\n.\n\n\u017e\n\n\u0004\n\n4\n\n9.1.2 collapsibility in three-way contingency tables\nin section 2.3.3 we showed that conditional associations in partial tables\nusually differ from marginal associations. under certain collapsibility condi-\ntions, however, they are the same.\n\nfor three-way tables, xy marginal and conditional odds ratios are identical if\neither z and x are conditionally independent or if z and y are conditionally\nindependent.\n\nthe conditions state that the variable treated as the control z is condition-\nally independent of x or y, or both. these conditions occur for loglinear\nmodels xy, yz and xy, xz . thus, the fitted xy odds ratio is identical\nin the partial tables and the marginal table for models with association\ngraphs\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nx\n\ny\n\nz and y\n\nx\n\nz\n\n "}, {"Page_number": 374, "text": "association graphs and collapsibility\n\n359\n\nor even simpler models, but not for the model with graph\n\nx\n\nz\n\ny\n\nin which an edge connects z to both x and y. the proof follows directly\n.\nfrom the formulas for models xy, yz and xy, xz problem 9.26 .\nwe illustrate for the student survey table 8.3 from section 8.2.4, with\na s alcohol use, c s cigarette use, and m s marijuana use. model\n\u017e\n.\nam, cm specifies ac conditional independence, given m. it has associa-\ntion graph\n\n. \u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\na\n\nm\n\nc.\n\nconsider the am association. since c is conditionally independent of a, the\nam fitted conditional odds ratios are the same as the am fitted marginal\nodds ratio collapsed over c. from table 8.5, both equal 61.9. similarly, the\ncm association is collapsible. the ac association is not, because m is\nconditionally dependent with both a and c in model am, cm . thus, a\nand c may be marginally dependent, even though they are conditionally\nindependent. in fact, from table 8.5, the fitted ac marginal odds ratio for\nthis model is 2.7.\n\u017e\n\nfor model ac, am, cm , no pair is conditionally independent. no\ncollapsibility conditions are fulfilled. table 8.5 showed that each pair has\nquite different fitted marginal and conditional associations for this model.\nwhen a model contains all two-factor effects, effects may change after\ncollapsing over any variable.\n\n.\n\n\u017e\n\n.\n\n9.1.3 collapsibility and logit models\nthe collapsibility conditions apply also to logit models. for instance, suppose\nthat a clinical\ntrial studies the association between a binary treatment\nvariable x x s 1, x s 0 and a binary response y, using data from k\n.\n1\ncenters z . the logit model\n\n\u017e\n\n\u017e\n\n.\n\n2\n\nlogit p y s 1 x s i, z s k s \u2423q \u2424x q \u2424\n\n\u017e\n\n.\n\n<\n\ni\n\nz\nk\n\nhas the same treatment effect \u2424 for each center. since this model corre-\nsponds to loglinear model xy, xz, yz , this effect may differ after collaps-\ning the 2 = 2 = k table over centers. the estimated xy conditional odds\n\u02c6\u017e\n.\nratio, exp \u2424 , typically differs from the sample odds ratio in the marginal\n2 = 2 table.\n\n\u017e\n\n.\n\nnext, consider the simpler model that lacks center effects,\nlogit p y s 1 x s i, z s k s \u2423q \u2424x .\n\n\u017e\n\n.\n\n<\n\ni\n\nfor a given treatment, the success probability is identical for each center.\nthe model satisfies a collapsibility condition, because it states that z is\n\n "}, {"Page_number": 375, "text": "360\n\nbuilding and extending loglinearr logit models\n\nconditionally independent of y, given x. this logit model is equivalent to\nloglinear model xy, xz , for which the xy association is collapsible. so,\nwhen center effects are negligible and the simpler model fits nearly as well,\nthe estimated treatment effect is approximately the marginal xy odds ratio.\n\n.\n\n\u017e\n\n9.1.4 collapsibility and association graphs for multiway tables\nbishop et al. 1975, p. 47 provided a parametric collapsibility condition with\nmultiway tables:\n\n.\n\n\u017e\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n4\n\n\u0004\n\n\u0004\n\n.\n\n\u017e\n\nsuppose that a model for a multiway table partitions variables into three\nmutually exclusive subsets, a, b, c, such that b separates a and c. after\ncollapsing the table over the variables in c, parameters relating variables in a and\nparameters relating variables in a to variables in b are unchanged.\nwe illustrate using model wx, wy, wz, yz figure 9.1 . let a s x ,\n4\nb s w , and c s y, z . since the xy and xz terms do not appear, all\nparameters linking set a with set c equal zero, and b separates a and c. if\nwe collapse over y and z, the wx association is unchanged. next, identify\na s y, z , b s w , c s x . then, conditional associations among w, y,\nand z remain the same after collapsing over x.\n\n. \u017e\n\nthis result also implies that when any variable is independent of all other\nvariables, collapsing over it does not affect any other model terms. for\ninstance, associations among w, x, and y in model wx, wy, xy, z are\n.\nthe same as in wx, wy, xy .\n\nwhen set b contains more than one variable, although parameter values\nare unchanged in collapsing over set c, the ml estimates of those parame-\nters may differ slightly. a stronger collapsibility definition also requires that\nthe estimates be identical. this condition of commutativity of fitting and\ncollapsing holds if the model contains the highest-order term relating vari-\n.\nables in b to each other. asmussen and edwards 1983 discussed this\n.\nproperty, which relates to decomposability of tables note 8.2 .\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n9.2 model selection and comparison\n\nstrategies for selecting and comparing loglinear models are similar to those\nfor logistic regression discussed in section 6.1. a model should be complex\nenough to fit well but also relatively simple to interpret, smoothing rather\nthan overfitting the data.\n\n9.2.1 considerations in model selection\nthe potentially useful models are usually a small subset of the possible\nmodels. a study designed to answer certain questions through confirmatory\nanalyses may plan to compare models that differ only by the inclusion of\ncertain terms. also, models should recognize distinctions between response\n\n "}, {"Page_number": 376, "text": "model selection and comparison\n\n361\n\n4\n\n\u0004\n\ngqllq\n\nand explanatory variables. the modeling process should concentrate on\nterms linking responses and terms linking explanatory variables to responses.\nthe model should contain the most general interaction term relating the\nexplanatory variables. from the likelihood equations, this has the effect of\nequating the fitted totals to the sample totals at combinations of their levels.\nthis is natural, since one normally treats such totals as fixed. related to this,\ncertain marginal totals are often fixed by the sampling design. any potential\nmodel should include those totals as sufficient statistics, so likelihood equa-\ntions equate them to the fitted totals.\nconsider table 8.8 with i s automobile injury and s s seat-belt use as\nresponses and g s gender and l s location as explanatory variables. then\nwe treat n\nas fixed at each combination for g and l. for example,\n20,629 women had accidents in urban locations, so the fitted counts should\nhave 20,629 women in urban locations. to ensure this, a loglinear model\nshould contain the gl term, which implies from its likelihood equations that\n\u242e s n\n\u0004\n.\n\u02c6 gqllq\n. thus, the model should be at least as complex as gl, s, i\nand focus on the effects of g and l on s and i as well as the si association.\nshould be fixed.\nwith a single categorical response, relevant loglinear models correspond to\nlogit models for that response. one should then use logit rather than\nloglinear models, when the main focus is describing effects on that response.\nfor exploratory studies, a search among potential models may provide\nclues about associations and interactions. one approach first fits the model\nhaving single-factor terms, then the model having two-factor and single-factor\nterms, then the model having three-factor and lower terms, and so on. fitting\nsuch models often reveals a restricted range of good-fitting models. in\nsection 8.4.2 we used this strategy with the automobile injury data set.\nautomatic search mechanisms among possible models, such as backward\nelimination, may also be useful but should be used with care and skepticism.\nsuch a strategy need not yield a meaningful model.\n\nif s is also explanatory and only i is a response, n\n\ngqllq\n\ngqll s\n\n\u017e\n\n\u0004\n\n4\n\n4\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n9.2.2 model building for the dayton student survey\n.\nin sections 8.2.4 and 8.3.2 we analyzed the use of alcohol a , cigarettes c ,\nand marijuana m by a sample of high school seniors. the study also\nclassified students by gender g and race r . table 9.1 shows the five-di-\nmensional contingency table. in selecting a model, we treat a, c, and m as\nresponses and g and r as explanatory. thus, a model should contain the\ngr term, which forces the gr fitted marginal totals to equal the sample\nmarginal totals\n\ntable 9.2 displays goodness-of-fit tests for several models. because many\ncell counts are small, the chi-squared approximation for g2 may be poor, but\nthis index is useful for comparing models. the first model listed contains only\nthe gr association and assumes conditional independence for the other nine\npairs of associations. it fits horribly, which is no surprise. model 2, with all\ntwo-factor terms, on the other hand, seems to fit well. model 3, containing all\n\n "}, {"Page_number": 377, "text": "362\n\nbuilding and extending loglinearr logit models\n\ntable 9.1 alcohol, cigarette, and marijuana use for high school seniors\n\nmarijuana use\n\nrace s white\n\nrace s other\n\nfemale\n\nmale\n\nfemale\n\nmale\n\nyes\n405\n13\n1\n1\n\nno\n268\n218\n17\n117\n\nyes\n453\n28\n1\n1\n\nno\n228\n201\n17\n133\n\nyes\n23\n2\n0\n0\n\nno\n23\n19\n1\n12\n\nyes\n30\n1\n1\n0\n\nno\n19\n18\n8\n17\n\nalcohol cigarette\n\nuse\nyes\n\nno\n\nuse\nyes\nno\nyes\nno\n\nsource: harry khamis, wright state university.\n\ntable 9.2 goodness-of-fit tests for loglinear models for table 9.1\n\n2\n\na\n\ng\n\nmodel\n1. mutual independence q gr\n2. homogeneous association\n3. all three-factor terms\n4a. 2 \u1390ac\n4b. 2 \u1390am\n4c. 2 \u1390cm\n4d. 2 \u1390ag\n4e. 2 \u1390ar\n4f. 2 \u1390cg\n4g. 2 \u1390cr\n4h. 2 \u1390gm\n4i. 2 \u1390mr\n5. ac, am, cm, ag, ar, gm, gr, mr\n6. ac, am, cm, ag, ar, gm, gr\n7. ac, am, cm, ag, ar, gr\nag, gender; r, race; a, alcohol use; c, cigarette use; m, marijuana use.\n\n1325.1\n15.3\n5.3\n201.2\n107.0\n513.5\n18.7\n20.3\n16.3\n15.8\n25.2\n18.9\n16.7\n19.9\n28.8\n\n\u017e .\n\u017e .\n\u017e .\n\u017e .\n\u017e .\n\u017e .\n\u017e .\n\u017e .\n\u017e .\n\u017e\n\u017e\n\u017e\n\n.\n\n.\n\n.\n\ndf\n25\n16\n6\n17\n17\n17\n17\n17\n17\n17\n17\n17\n18\n19\n20\n\n2\n\n\u017e\n\nthe three-factor interaction terms, also fits well, but the improvement in fit is\nnot great difference in g of 15.3 y 5.3 s 10.0 based on df s 16 y 6 s 10 .\n.\nthus, we consider models without three-factor terms. beginning with model\n2, we eliminate two-factor terms. we use backward elimination, sequentially\ntaking out terms for which the resulting increase in g2 is smallest, when\nrefitting the model.\n\ntable 9.2 shows the start of this process. nine pairwise associations are\ncandidates for removal from model 2 all except gr , shown in models 4a\nthrough 4i. the smallest increase in g2, compared to model 2, occurs in\nremoving the cr term i.e., model 4g . the increase is 15.8 y 15.3 s 0.5,\nwith df s 17 y 16 s 1, so this elimination seems sensible. after removing it,\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n "}, {"Page_number": 378, "text": "model selection and comparison\n\n363\n\n\u017e\n\n2\n\n2\n\n.\n\n\u017e\n\nthe smallest additional increase results from removing the cg term model\n5 , resulting in g s 16.7 with df s 18, and a change in g of 0.9 based on\n.\ndf s 1. removing next\nthe mr term model 6 yields g s 19.9 with\ndf s 19, a change in g2 of 3.2 based on df s 1.\nfurther removals have a more severe effect. for instance, removing the\nag term increases g2 by 5.3, with df s 1, for a p-value of 0.02. one cannot\ntake such p-values literally, since the data suggested these tests, but it seems\nsafest not to drop additional terms. see westfall and wolfinger 1997 and\nwestfall and young 1993 for methods of adjusting p-values to account for\nmultiple tests . model 6, denoted by ac, am, cm, ag, ar, gm, gr , has\nassociation graph\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\nw\n\nx\n\n2\n\nm\n\ng\n\nc\n\na\n\nr\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u017e\n\nevery path between c and g, r involves a variable in a, m . given the\noutcome on alcohol use and marijuana use, the model states that cigarette\nuse is independent of both gender and race. collapsing over the explanatory\nvariables race and gender, the conditional associations between c and a and\nbetween c and m are the same as with the model ac, am, cm fitted in\nsection 8.2.4.\n\nremoving the gm term from this model yields model 7 in table 9.2. its\nassociation graph reveals that a separates g, r from c, m . thus, all\npairwise conditional associations among a, c, and m in model 7 are\nidentical to those in model ac, am, cm , collapsing over g and r. in fact,\nmodel 7 does not fit poorly g s 28.8 with df s 20 considering the large\nsample size. its sample dissimilarity index is \u232c s 0.036. hence, one might\ncollapse over gender and race in studying associations among the primary\nvariables. an advantage of the full five-variable model is that it estimates\neffects of gender and race on these responses, in particular the effects of race\nand gender on alcohol use and the effect of gender on marijuana use.\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n2\n\n<\n\n0\n\n1\n\n0\n\n1\n\n.\n\n2\u017e\n\n9.2.3 loglinear model comparison statistics\nconsider two loglinear models, m and m , with m a special case of m . by\nsections 4.5.4 and 5.4.3, the likelihood-ratio statistic for testing m against\nm is g m m s g m y g m . we used this statistic above in\n2\u017e\ncomparing pairs of models.\n\nlet n denote a column vector of the observed cell counts n . let \u242e and\n4\n\u02c6\n\u242e denote vectors of the fitted values \u242e and \u242e for m and m . the\n1i\ndeviance g m for the simpler model partitions into\ng m s g m q g m m .\n.\n\n\u02c6\n1\n\n9.1\n\n2\u017e\n\n2\u017e\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n0 i\n\n.\n\n.\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n\u0004\n\n4\n\n1\n\n0\n\n0\n\n2\n\n2\n\n2\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n0\n\ni\n\n<\n\n0\n\n1\n\n0\n\n1\n\njust as g m measures the distance of fitted values for m from n,\ng m m measures the distance of fit \u242e from fit \u242e . in this sense,\n\n2\u017e\n\n<\n\n\u02c6\n\n0\n\n\u02c6\n\n1\n\n0\n\n.\n\n2\u017e\n.\n\n1\n\n "}, {"Page_number": 379, "text": "364\n\nbuilding and extending loglinearr logit models\n\ndecomposition 9.1 expresses a certain orthogonality: the distance of n from\n\u02c6\n\u242e equals the distance of n from \u242e plus the distance of \u242e from \u242e .\n0\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n0\n\n1\n\n1\n\n\u017e\n\n.\n\nthe model comparison statistic equals\n\ng m m s 2\n\n2\n\n<\n\n.\n\n\u017e\n\n0\n\n1\n\n\u00fd\n\ni\n\nn log n r\u242e y 2\n\n\u017e\n\n.\n\n\u02c6\n\n0 i\n\ni\n\ni\n\n\u00fd\n\ni\n\nn log n r\u242e\n\u02c6\n\n\u017e\n\ni\n\ni\n\n1i\n\ns 2\n\n\u00fd i\n\nn log \u242e r\u242e .\n.\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n0 i\n\n1i\n\ni\n\n.\n\n9.2\u017e\n\n.\n\nthe two loglinear models have the matrix form 8.17 , or\nand log \u242e s x \u2424 .\n\nlog \u242e s x \u2424\n\n1\n\n1\n\n1\n\n0\n\n0\n\n0\n\n\u017e\n\n.\n\nsince m is simpler than m , one can express log \u242e s x \u2424 s x \u2424u, where\n\u2424u equals \u2424 with 0 elements appended corresponding to the extra parame-\n.\nters in \u2424 but not in \u2424 . then, from 9.2 ,\n\n\u017e\n\n0\n\n1\n\n0\n\n0\n\n0\n\n1\n\n1\n\n1\n\n0\n\n1\n\n0\n\n<\n\nx\n\n1\n\n1\n\n0\n\n2\n\n\u017e\n\n.\n\n\u017e\n\n\u02c6\n\ng m m s 2n log \u242e y log \u242e s 2n x \u2424 y x \u2424\n\u02c6\ns 2\u242e x \u2424 y x \u2424 s 2\u242e log \u242e y log \u242e\n\u02c6\ns 2 \u242e log \u242e r\u242e ,\n.\n\n\u02c6\n\u00fd 1i\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\nx\n1\n\nx\n1\n\nu\n1\n\nu\n1\n\n.\n\n\u017e\n\n\u017e\n\n0 i\n\n1i\n\n0\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\nx\n\n.\n\n0\n\n9.3\u017e\n\n.\n\n0\n\n1\n\n1\n\n\u017e\n\nx\n1\n\n.\n.\n\n\u02c6\n\n\u02c6 1\n.x\n\nw\n4\n\nwhere the replacement of n by \u242e follows from the likelihood equations\nn x s \u242e x for m recall 8.22 . statistic 9.3 has the same form as\nx\n2\u017e\ng m , but with \u242e playing the role of the observed data. note that\n2\u017e\ng m is the special case of g m m with m saturated.\nthe pearson difference x m y x m does not have pearson form.\nit is not even necessarily nonnegative. a more appropriate pearson statistic\nfor comparing models is\n\n.\n1\n2\u017e\n\n1\n\u02c6\n\n2\u017e\n\n2\u017e\n\n1i\n\n.\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n0\n\n0\n\n1\n\n0\n\n1\n\n<\n\nx m m s \u242e y \u242e r\u242e .\n\n2\n\n2\n\n<\n\n\u017e\n\n.\n\n\u00fd\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n0 i\n\n0 i\n\n1i\n\n0\n\n1\n\n9.4\u017e\n\n.\n\ni\n\n0\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2\u017e\n\n2\u017e\n\n\u02c61i\n\n.\nthis has the usual form with \u242e in place of n . statistics 9.3 and 9.4\ndepend on the data only through the fitted values and thus only through\nsufficient statistics for m .1\n\nwhen m holds, g m and g m have asymptotic chi-squared distri-\nbutions, and g m m is asymptotically chi-squared with df equal to the\ndifference between df for m and m . haberman 1977a showed that\ng m m and x m m have the same null large-sample behavior, even\nfor fairly sparse tables. under certain conditions, their difference converges\n.\nin probability to 0 as n increases. when m holds but m does not, g m\n1\nstill has its asymptotic chi-squared distribution, but the other two statistics\ntend to grow unboundedly as n increases.\n\n2\u017e\n\n2\u017e\n\n2\u017e\n\n2\u017e\n\n0\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n0\n\n1\n\n0\n\n1\n\n1\n\n1\n\n0\n\n0\n\n1\n\n0\n\n1\n\n<\n\n<\n\n<\n\n "}, {"Page_number": 380, "text": "model selection and comparison\n\n365\n\n.\n\n.\n\n\u017e\n\n9.2.4 partitioning chi-squared with model comparisons\nequation 9.1 utilizes the property by which a chi-squared statistic with\ndf ) 1 partitions into components. we used such partitionings in tests for\ntrend with ordinal predictors in linear logit or linear probability models\n\u017e\nsection 5.3.5 and with ordinal responses in cumulative logit models section\n.7.2 . more generally, this property applies with a set of nested models to test\na sequence of hypotheses. the separate tests for comparing pairs of models\nare asymptotically independent.\nfor example, a chi-squared decomposition with j y 1 models justifies the\npartitioning of g2 stated in section 3.3.3 for 2 = j tables. for j s 2, . . . , j,\nlet m denote the model that satisfies\n\n\u017e\n\nj\n\n\u242a s \u242e \u242e\ni\n\n\u017e\n\n1i\n\n2, iq1\n\n.\n\nr \u242e\n\u017e\n\n1, iq1\n\n\u242e s 1,\n\n.\n\n2 i\n\ni s 1, . . . ,\n\nj y 1.\n\nj\n\nfor m , the 2 = j table consisting of columns 1 through j satisfies indepen-\ndence. model m is independence in the complete 2 = j table. model m is\n.\na special case of m whenever h ) j. by 9.2 ,\n\n\u017e\n\nh\n\nj\n\nj\n\n<\n\nj\n\nj\n\n2\n\n2\n\n.\n\n\u017e\n\ng m s g m m\ns g m m\ns \u2b48\u2b48\u2b48 s g m m\n\u017e\n\n.\n.\n<\n\njy1\n\njy1\n\n\u017e\n\u017e\n\n2\n\n2\n\nj\n\n<\n\nj\n\n2\n\nq g m\nq g m\n\n2\n\n.\n<\n\njy1\n\n\u017e\n\u017e\nq \u2b48\u2b48\u2b48 qg m m q g m .\n.\n\nq g m\u017e\n<\n\njy2\n\u017e\n\njy1\n\njy2\n\n.\n\u017e\n\nm\n\n.\n\n.\n\n2\n\n2\n\n2\n\n3\n\n2\n\n2\n\n.\n\njy1\n\n<\n\n<\n\nj\n\nj\n\n2\n\n2\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n2\u017e\n\njy1\n\njy1\n\n\u017e\nfrom 9.3 , g m m\nhas the g form with the fitted values for model\nplaying the role of the observed data. substitution of fitted values for\nm\nthe two models into 9.3 shows that g m m\nis identical to g for\ntesting independence in a 2 = 2 table; the first column combines column 1\nthrough j y 1 of the original table, and the second column is column j of the\noriginal table.\n\njy1\n\n2\u017e\n\nwith several preplanned comparisons, simultaneous test procedures lessen\nthe probability of attributing importance to sample effects that simply reflect\nchance variation. these procedures use adjusted significance levels. for a set\nof s tests for nested models, when each test has level 1 y 1 y \u2423 , the\noverall asymptotic p type i error f \u2423 goodman 1969a . for instance,\nsuppose that we test the fit of wxz, wy, xy, zy , compare that model to\n\u017e\nwx, wz, xz, wy, xy, zy , and compare that model\nto wx, wz, xz,\nwy, zy . to ensure overall \u2423s 0.05 for the s s 3 tests, use level 1 y\n0.95 s 0.017 for each.\n\u017e\n\n.1r s\n\n.1r3\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n9.2.5\n\nidentical marginal and conditional tests of independence\n\na test using g m m simplifies dramatically when both models have\ndirect estimates. in that case, the models have independence linkages neces-\n\n0\n\n1\n\n2\u017e\n\n<\n\n.\n\n "}, {"Page_number": 381, "text": "366\n\nbuilding and extending loglinearr logit models\n\n.\n\nsary to ensure collapsibility. a test of conditional independence has the same\nresult as the test of independence applied to the marginal table. sundberg\n\u017e\n1975 proved the following: when two direct models m and m are\nidentical except for a pairwise association term, g m m is identical to\ng2 for testing independence in the marginal table for that pair of variables.\nbishop 1971 and goodman 1970, 1971b have related discussion.\nxy, z tests \u242d s 0 in model xy, z .\n.\nthus, it tests xy conditional independence under the assumption that x\nand y are jointly independent of z. using the two sets of fitted values, from\n\u017e\n.9.3 , it equals\n\nfor instance, g x, y, z\n\n\u017e\n. < \u017e\n\n2w\u017e\n\n.x\n\n2\u017e\n\n0\n.\n\nx y\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n1\n\n0\n\n1\n\n<\n\n\u00fd \u00fd \u00fd\n\n2\n\ni\n\nj\n\nk\n\nn\n\ni jq qqk\n\nn\nn\n\nlog\n\nn\n\nn\n\nn\n\nrn\niqq qjq qqk\n\ni jq qqk\nn\nn\n\nrn\n\n2\n\ns 2\n\n\u00fd \u00fd i jq\n\nn\n\ni\n\nj\n\nlog\n\nni jq\nn\niqq qjq\n\nrn\n\nn\n\n,\n\n2w\u017e\n\n.x\n\nwhich equals g x, y for testing independence in the marginal xy table.\nthis is not surprising. the collapsibility conditions imply that for model\n\u017e\nxy, z , the marginal xy association is the same as the conditional xy\nassociation.\n\n.\n\n9.3 diagnostics for checking models\n\nthe model comparison test using g m m is useful for detecting whether\nan extra term improves a model fit. cell residuals provide a cell-specific\nindication of model lack of fit.\n\n0\n\n1\n\n2\u017e\n\n<\n\n.\n\n9.3.1 residuals for loglinear models\nin section 4.5.5 we noted that residuals for the independence model section\n3.3.1 extend to any poisson glm. for cell\ni in a contingency table with\nobserved count n and fitted value \u242e, the pearson residual is\n\n.\n\n\u017e\n\n\u02c6\n\ni\n\ni\n\ne s\n\ni\n\ni\n\nn y \u242e\u02c6\n\u242e'\u02c6i\n\ni\n\n.\n\n9.5\u017e\n\n.\n\nthese relate to the pearson statistic by \u00fde2 s x 2.\n\ni\n\nlike the pearson residual\n\nthe asymptotic\nare less than 1.0. they average residual df r number of\n.\n\n\u017e\n6.1 for binomial models,\n\n.\n\n\u017e\n\n\u017e\n\nvariances of\n\n\u0004\n\n4\n\ne\n\ni\n\n "}, {"Page_number": 382, "text": "modeling ordinal associations\n\n367\n\ncells . haberman 1973a defined the standardized pearson residual,\n\n\u017e\n\n.\n\n.\n\n\u02c6'\nr s e r 1 y h ,\n\ni\n\ni\n\ni\n\n\u02c6\n\ni\n\n.\n\nwhere the leverage h is a diagonal element of the estimated hat matrix\n\u017e\nsection 4.5.5 . this has an asymptotic standard normal distribution and is\npreferable to the pearson residual. a closed-form expression applies for\nloglinear models having direct estimates haberman 1978, p. 275 . alterna-\n.\ntive residuals use components of the deviance section 4.5.5 .\n\n\u017e\n\n\u017e\n\n.\n\n9.3.2 student survey example revisited\nfor table 9.1 cross-classifying alcohol, cigarette, and marijuana use by\ngender and race, we suggested in section 9.2.2 that the model with all\ntwo-factor associations is plausible. for it, the only large standardized pear-\nson residual equals 3.2, resulting from a fitted value of 3.1 in the cell having\na count of 8. further comparisons suggested that\nthe simpler model\n\u017e\nac, am, cm, ag, ar, gm, gr is adequate. its only large standardized\nresidual equals 3.3, referring to a fitted value of 2.9 in that cell. the number\nof nonwhite males who did not use alcohol or marijuana but who smoked\ncigarettes is somewhat greater than either model predicts. the standardized\npearson residuals do not suggest problems with either model, considering the\nlarge sample size and many cells studied.\n\n.\n\n9.3.3 correspondence between loglinear and logit residuals\n\nin section 8.5 we showed that logit models in contingency tables are equiva-\nlent to certain loglinear models. however, a pearson residual for a logit\nmodel differs from a pearson residual for a loglinear model. the numerators\ncomparing the ith observed and fitted binomial or poisson count are the\nsame, since the model fitted values are the same. however, the logit model\nw\n.x\nuses a fitted binomial standard deviation in the denominator\nsee 6.1 ,\nwhereas the loglinear model uses a fitted poisson standard deviation see\n.x\n\u017e\n9.5 . thus, the logit pearson residual exceeds the loglinear pearson residual\n\u017e\n.9.5 .\n\nonce standardized by dividing by estimated standard errors, the standard-\nized pearson residuals are identical for the two models. this is another\nreason for preferring standardized residuals over ordinary pearson residuals.\n\n\u017e\n\nw\n\n9.4 modeling ordinal associations\n\nthe loglinear models presented so far have a serious limitation\u138fthey treat\nall classifications as nominal. if the order of a variable\u2019s categories changes in\n\n "}, {"Page_number": 383, "text": "368\n\nbuilding and extending loglinearr logit models\n\ntable 9.3 opinions about premarital sex and availability of teenage birth control\n\nteenage birth control\n\na\n\npremarital sex\nalways wrong\n\nalmost always wrong\n\nwrong only sometimes\n\nnot wrong at all\n\nstrongly\ndisagree\n\ndisagree\n\n.\n\n.\n\n81\n.\n\u017e\n1\n42.4\n27.6\n\u017e\n.\n3\n80.9\n24\n\u017e\n16.0\n2.3\n\u017e\n20.8\n18\n\u017e\n30.1\ny2.7\n\u017e\n24.4\n36\n\u017e\n70.6\ny6.1\n\u017e\n33.0\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n68\n\u017e\n51.2\n3.1\n\u017e\n67.6\n26\n\u017e\n19.3\n1.8\n\u017e\n23.1\n41\n\u017e\n36.3\n1.0\n\u017e\n36.1\n57\n\u017e\n85.2\ny4.6\n\u017e\n.\n65.1\n\n.\n\n.\n\n.\n\nagree\n\n60\n\u017e\n86.4\ny4.1\n\u017e\n69.4\n29\n\u017e\n32.5\ny0.8\n\u017e\n31.5\n74\n\u017e\n61.2\n2.2\n\u017e\n65.7\n161\n\u017e\n143.8\n2.4\n\u017e\n157.4\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nstrongly\nagree\n\n38\n\u017e\n.\n67.0\ny4.8\n\u017e\n.\n29.1\n14\n\u017e\n.\n25.2\ny2.8\n\u017e\n.\n17.6\n42\n.\n\u017e\n47.4\ny1.0\n\u017e\n.\n48.8\n157\n\u017e\n111.4\n6.8\n\u017e\n155.5\n\n.\n\n.\n\na1independence model fit; 2standardized pearson residuals for the independence model fit;\n3linear-by-linear association model fit.\nsource: 1991 general social survey, national opinion research center.\n\n.\n\n\u017e\n\n\u017e\n\nany way, the fit is the same. for ordinal classifications, these models ignore\nimportant information.\n\nrefer to table 9.3. subjects were asked their opinion about a man and\nwoman having sexual relations before marriage always wrong, almost always\nwrong, wrong only sometimes, not wrong at all . they were also asked\nwhether methods of birth control should be available to teenagers between\nthe ages of 14 and 16 strongly disagree, disagree, agree, strongly agree . for\nthe loglinear model of independence, denoted by i, g i s 127.6 with\ndf s 9. the model fits poorly. yet, adding the ordinary association term\nmakes it saturated and unhelpful.\n\n2\u017e .\n\ntable 9.3 also contains fitted values and standardized residuals for inde-\npendence. the residuals in the corners stand out. sample counts are much\nlarger than independence predicts where both responses are the most nega-\ntive possible or the most positive possible. by contrast, the counts are much\nsmaller than fitted values where one response is the most positive and the\nother is the most negative. cross-classifications of ordinal variables often\nexhibit their greatest deviations from independence in the corner cells. this\npattern for table 9.3 indicates lack of fit in the form of a positive trend.\n\n.\n\n "}, {"Page_number": 384, "text": "modeling ordinal associations\n\n369\n\nsubjects who are more willing to make birth control available to teenagers\nalso tend to feel more tolerant about premarital sex.\n\nmodels for ordinal variables use association terms that permit trends. the\nmodels are more complex than the independence model, yet unsaturated.\nmodels with association and interaction terms exist in situations in which\nnominal models are saturated. tests with ordinal models have improved\npower for detecting trends.\n\n9.4.1 linear-by-linear association in two-way tables\nfor two-way tables, a simple model for two ordinal variables assigns ordered\nrow scores u f u f \u2b48\u2b48\u2b48 f u and column scores \u00ae f \u00ae f \u2b48\u2b48\u2b48 f \u00ae . the\nmodel is\n\n1\n\n2\n\n1\n\n2\n\nj\n\ni\n\nj\n\nj\n\nj\n\nj\n\ni\n\ni\n\ni\n\ni\n\ni\n\nj\n\ni j\n\ni j\n\n.\n\n\u017e\n\n\u017e\n\n.\u017e\n\nx y\ni j\n\nlog \u242e s \u242dq \u242dx q \u242dy q \u2424u \u00ae ,\n\n9.6\u017e\n.\nwith constraints such as \u242dx s \u242dy s 0. this is the special case of the satu-\nrated model 8.2 in which \u242d s \u2424u \u00ae . it requires only one parameter to\ndescribe association, whereas the saturated model requires\nindependence occurs when \u2424s 0. the term \u2424u \u00ae represents the devia-\ntion of log \u242e from independence. the deviation is linear in the y scores at\na fixed level of x and linear in the x scores at a fixed level of y. in column\nj, for instance, the deviation is a linear function of x, having form slope =\nscore for x , with slope \u2424\u00ae . because of this property, 9.6 is called the\n\u017e\nlinear-by-linear association model abbreviated, l = l . the model has its\ngreatest departures from independence in the corners of the table. birch\n\u017e\n1965 , goodman 1979a , and haberman 1974b introduced special cases.\n\ni y 1 j y 1 .\n.\n\nthe direction and strength of the association depend on \u2424. when \u2424) 0,\ny tends to increase as x increases. expected frequencies are larger than\nexpected under independence in cells where x and y are both high or both\nlow. when \u2424- 0, y tends to decrease as x increases. when the data display\na positive or negative trend, the l = l model usually fits much better than\nthe independence model.\n\nfor the 2 = 2 table using the cells intersecting rows a and c with columns\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\nj\n\nb and d, direct substitution shows that the model has\n\nlog\n\n\u242e \u242e\n\nab\n\ncd\n\n\u242e \u242e\n\nad\n\ncb\n\ns \u2424 u y u\n\n\u017e\n\nc\n\n. \u017e\n\na\n\n\u00ae y \u00ae\nd\n\nb\n\n.\n\n.\n\n\u017e\n\n9.7\n\n.\n\n<\n\n<\n\nand \u00ae y \u00ae s \u2b48\u2b48\u2b48 s \u00ae y \u00ae\n\nthis log odds ratio is stronger as \u2424 increases and for pairs of categories that\nare farther apart. simple interpretations result when u y u s \u2b48\u2b48\u2b48 s u y\n. when u s i and \u00ae s j , for instance,\nu\niy1\nthe local odds ratios 2.10 for adjacent rows and adjacent columns have\ncommon value e . goodman 1979a called this case uniform association.\nfigure 9.2 portrays local odds ratios having uniform value.\n\njy1\n\nj\n.\n\n2\n\u0004\n\n\u017e\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n4\n\n\u2424\n\n1\n\n2\n\n1\n\ni\n\ni\n\nj\n\n "}, {"Page_number": 385, "text": "370\n\nbuilding and extending loglinearr logit models\n\nfigure 9.2 constant odds ratio implied by uniform association model. note: \u2424s the\nconstant log odds ratio for adjacent rows and adjacent columns.\n\n\u017e\n\n.\n\nthe choice of scores affects the interpretation of \u2424. often, the response\nscale discretizes an inherently continuous scale. it is sensible to choose scores\nthat approximate distances between midpoints of categories for the underly-\ning scale, such as we did in measuring alcohol consumption for a linear logit\nmodel in section 3.4.5. it is sometimes useful to standardize the scores,\nsubtracting the mean and dividing by the standard deviation, so\n\n\u00fd\n\u00fd\n\ni\n\niq\n\nu \u2432 s \u00ae \u2432 s 0\nu2\u2432 s \u00ae2\u2432 s 1.\n\n\u00fd\n\u00fd\n\nj qj\n\nj qj\n\niq\n\ni\n\nthen, \u2424 represents the log odds ratios for standard deviation distances in the\nx and y directions. the l = l model tends to fit well when an underlying\ncontinuous distribution is approximately bivariate normal. for standardized\nscores, \u2424 is then comparable to \u2433r 1 y \u2433 , where \u2433 is the underlying\ncorrelation. for weak associations, \u2424f \u2433 see becker 1989b; goodman\n.\n1981a, b, 1985 .\n\n2.\n\u017e\n\n\u017e\n\n9.4.2 corresponding logit model for adjacent responses\na logit formulation of the l = l model treats y as a response and x as\nexplanatory. let \u2432 s p y s j x s i . using logits for adjacent response\n.\ncategories section 7.4.1 ,\ns log\n\ns \u242d y \u242d q \u2424 \u00ae y \u00ae u .\n\nlog\n\n\u2432\n\n\u242e\n\nj < i\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n<\n\njq1\n\nj\n\ni\n\ny\njq1\n\n.\n\ny\nj\n\ni , jq1\n\u242e\ni j\n\njq1 < i\n\u2432\nj < i\n\u0004\n\n\u017e\nfor unit-spaced \u00ae , this simplifies to\njq1 < i\n\u2432\nj < i\n\nlog\n\n\u2432\n\n4\n\nj\n\ns \u2423 q \u2424u\n\nj\n\ni\n\n "}, {"Page_number": 386, "text": "j\n\n371\nmodeling ordinal associations\nwhere \u2423 s \u242dy y \u242dy. the same linear logit effect \u2424 applies simultaneously\nj y 1 pairs of adjacent response categories: the odds y s j q 1\nfor all\ninstead of y s j multiply by e \u2424 for each unit change in x. in using\nequal-interval response scores, we implicitly assume that the effect of x is\nthe same on each of the j y 1 adjacent-categories logits for y.\n\njq1\n.\n\n\u017e\n\nj\n\n9.4.3 likelihood equations and model fitting\nthe poisson log-likelihood l \u242e s \u00fd \u00fd n log \u242e y \u00fd \u00fd \u242e simplifies for\nthe l = l model 9.6 to\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni j\n\ni j\n\ni j\n\ni\n\ni\n\nj\n\nj\n\nl \u242e s n\u242dq n \u242dx q n \u242dy q \u2424\n\u017e\n\n.\n\n\u00fd\n\niq i\n\nqj\n\nj\n\n\u00fd \u00fd\n\nu \u00ae n\n\ni\n\nj\n\ni j\n\n\u00fd\n\nj\n\ny\n\n\u00fd \u00fd\n\n.\nexp \u242dq \u242dx q \u242dy q \u2424u \u00ae .\n\n\u017e\n\ni\n\ni\n\nj\n\nj\n\ni\n\nj\n\ni\n\nj\n\ni\n\ndifferentiating l \u242e with respect to \u242d , \u242d , \u2424 and setting the three partial\nderivatives equal to zero yields likelihood equations\n\ny\nj\n\n\u017e x\ni\n\n\u017e\n\n.\n\n.\n\niq\n\n\u242e s n , i s 1, . . . , i,\n\u02c6\niq\n\u02c6\n\n\u00fd \u00fd\n\nu \u00ae \u242e s\n\ni j\n\ni\n\nj\n\ni\n\nj\n\nqj\n\n\u242e s n , j s 1, . . . , j,\n\u02c6\nqj\n\u00fd \u00fd\n\nu \u00ae n .\n\ni j\n\ni\n\nj\n\ni\n\nj\n\niterative methods such as newton\u1390raphson yield the ml fit.\n\nlet p s n rn and \u2432 s \u242e rn. the third likelihood equation implies\n\n\u02c6\n\ni j\n\n\u02c6\n\ni j\n\ni j\n\ni j\n\nthat\n\n\u00fd \u00fd\n\nu \u00ae \u2432 s\n\n\u02c6\n\ni j\n\ni\n\nj\n\n\u00fd \u00fd\n\nu \u00ae p .\n\ni j\n\ni\n\nj\n\ni\n\nj\n\ni\n\nj\n\nsince marginal distributions and hence marginal means and variances are\nidentical for fitted and observed distributions, the third equation implies the\ncorrelation between the scores for x and y is the same for both distribu-\ntions. the fitted counts display the same positive or negative trend as the\ndata.\nsince u and \u00ae are fixed, the l = l model 9.6 has only one more\n\n.\nparameter \u2424 than the independence model. its residual\n\ni\n\u017e\ndf s ij y 1 q i y 1 q j y 1 q 1 s ij y i y j,\n\n.\n\n\u017e\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nj\n\nunsaturated for all but 2 = 2 tables.\n\n9.4.4 sex opinions example\ntable 9.3 also reports fitted values for the linear-by-linear association model\napplied to table 9.3, using scores 1, 2, 3, 4 for rows and columns. table 9.4\n\n\u0004\n\n4\n\n "}, {"Page_number": 387, "text": "372\n\nbuilding and extending loglinearr logit models\n\ntable 9.4 output for fitting linear-by-linear association model to table 9.3\n\ncriteria for assessing goodness of fit\ncriterion\ndeviance\npearson chi- square\n\n11.5337\n11.5085\n\ndf\n8\n8\n\nvalue\n\nchi-\nsquare pr ) chisq\n\nparameter\nintercept\npremar\npremar\npremar\npremar\nbirth\nbirth\nbirth\nbirth\nlinlin\n\nestimate\n0.4735\n1\n1.7537\n2\n0.1077\n3 y0.0163\n0.0000\n4\n1.8797\n1\n1.4156\n2\n3\n1.1551\n0.0000\n4\n0.2858\n\nsource\n\nlinlin\n\nstandard\n\nlimits\n\nwald 95% conf.\nerror\n0.4339 y0.3769 1.3239\n0.2343\n1.2944 2.2129\n0.1988 y0.2820 0.4974\n0.1264 y0.2641 0.2314\n0.0000 0.0000\n0.0000\n1.3914\n0.2491\n2.3679\n1.0243 1.8068\n0.1996\n0.1291\n0.9021 1.4082\n0.0000 0.0000\n0.0000\n0.0282\n0.2305 0.3412 102.46\n\n1.19\n56.01\n0.29\n0.02\n\n56.94\n50.29\n80.07\n\n.\n\n.\n\nlr statistics\ndf\n1\n\n116.12\n\nchi- square\n\npr ) chisq\n\n).0001\n\n0.2751\n-.0001\n0.5880\n0.8972\n\n.\n\n-.0001\n-.0001\n-.0001\n\n.\n\n-.0001\n\n.\n\n\u017e\n\nx\n\nshows software output. to get this, we added a variable denoted \u2018\u2018linlin\u2019\u2019 to\nthe independence model having values equal to the product of row and\ncolumn number. compared to the independence model, for which g i s\n127.6 with df s 9, the l = l model fits dramatically better g l = l s\n11.5, df s 8 . this is especially noticeable in the corners, where it predicts\nthe greatest departures from independence.\nthe ml estimate \u2424s 0.286 se s 0.028 indicates that subjects having\nmore favorable attitudes about teen birth control also tend to have more\ntolerant attitudes about premarital sex. the estimated local odds ratio is\nexp \u2424 s exp 0.286 s 1.33. a 95% wald confidence interval is exp 0.286 \"\n1.96 = 0.028 , or 1.26, 1.41 . the strength of association seems weak. from\n\u017e\n.9.7 , however, nonlocal odds ratios are stronger. the estimated odds ratio\nfor the four corner cells equals\n\n2\u017e .\n.\n\n\u02c6\u017e\n.\n\n2\u017e\n\n\u02c6\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nw\n\nexp \u2424 u y u\n\n\u02c6\n\n\u017e\n\n4\n\n. \u017e\n\n1\n\n4\n\n\u00ae y \u00ae s exp 0.286 4 y 1 4 y 1 s 13.1.\n\n. \u017e\n\n.\n\n\u017e\n\n.\n\n1\n\nthis also results from the corner fitted values, 80.9 = 155.5 r 29.1 = 33.0\ns 13.1.\n\n\u017e\n\n.\n\n\u017e\n\ntwo sets of scores having the same spacings yield the same \u2424 and the\nsame fit. any other sets of equally spaced scores yield the same fit but an\nappropriately rescaled \u2424. for instance, using row scores 2, 4, 6, 8 with\n\u00ae s j also yields g s 11.5, but \u2424s 0.143 with se s 0.014 both half as\n\u0004\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n\u0004\n\n4\n\n4\n\n2\n\n\u02c6\n\n.\n\nj\n\n "}, {"Page_number": 388, "text": "association models\n\n373\n\n.\n\n4\n\n\u0004\n\n.\n\n\u017e\n\nlarge . for table 9.3, one might regard categories 2 and 3 as farther apart\nthan categories 1 and 2, or categories 3 and 4. scores such as 1, 2, 4, 5 for\nrows and columns recognize this. the l = l model then has g2 s 8.8\ndf s 8 and \u2424s 0.146 se s 0.014 .\n.\n\u017e\n\n\u02c6\n\none need not regard the scores as approximations for distances between\ncategories or as reasonable scalings of ordinal variables in order for the\nmodels to be valid. they simply imply a certain pattern for the odds ratios. if\nthe l = l model fits well with equally spaced row and column scores, the\nuniform local odds ratio describes the association regardless of whether the\nscores are sensible indexes of true distances between categories.\nfor scores u s i with table 9.3, the marginal mean and standard\ndeviation for premarital sex are 2.81 and 1.26. the standardized scores are\ni y 2.81 r1.26 , or y1.44, y0.65, 0.15, 0.95 . the standardized equal-inter-\n\u0004\u017e\nval scores for birth control are y1.65, y0.69, 0.27, 1.23 . for these scores,\n\u2424s 0.374. by solving \u2424s \u2433r 1 y \u2433 for \u2433, \u2433s 0.333. if there is an\n\u02c6\nunderlying bivariate normal distribution, we estimate the correlation to be\n0.333.\n\n\u02c6 \u02c6\n\n\u017e\n\u017e\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n4\n\n\u0004\n\n4\n\n2\n\ni\n\n<\n\n<\n\n2\n\n2\n\n0\n\n0\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.2\n\n9.4.5 directed ordinal test of independence\nfor the linear-by-linear association model, h : independence is h : \u2424s 0.\nthe likelihood-ratio test statistic equals\n\u017e\n2\n\ng i l = l s g i y g l = l .\n.\n\ndesigned to detect positive or negative trends, it has df s 1. for table 9.3,\ng i l = l s 127.6 y 11.5 s 116.1. this has p - 0.0001, extremely strong\n2\u017e\nz s \u2424rse s\nevidence of an association. the wald statistic\n0.286r0.0282 s 102.5 df s 1 also shows strong evidence. the correlation\n\u017e\nstatistic 3.15 presented in section 3.4.1 for testing independence is the score\nstatistic for h : \u2424s 0 in this model. it equals 112.6 df s 1 .\n.\n\nwhen the l = l model holds, the ordinal test using g i l = l is\nasymptotically more powerful than the test using g i . this is true for the\nsame reason given in section 6.4.2 for the linear logit model. the power of a\nchi-squared test increases when df decrease, for fixed noncentrality. when\nthe l = l model holds, the noncentrality is the same for g i l = l and\ng i ; thus g i l = l is more powerful, since its df s 1 compared to\n2\u017e .\ni y 1 j y 1 for g i . the power advantage increases as i and j in-\n\u017e\ncrease, since the noncentrality remains focused on df s 1 for g i l = l\n.\n2\u017e .\nbut df also increases for g i .\n\n2\u017e .\n\n2\u017e .\n\n2\u017e\n\n2\u017e\n\n2\u017e\n\n2\u017e\n\n.\u017e\n\n\u02c6\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n2\n\n2\n\n0\n\n<\n\n<\n\n<\n\n<\n\n9.5 association models*\n\ngeneralizations of the linear-by-linear association model apply to multiway\ntables or treat scores as parameters rather than fixed. the models are called\nassociation models, because they focus on the association structure.\n\n "}, {"Page_number": 389, "text": "374\n\nbuilding and extending loglinearr logit models\n\n9.5.1 row and column effects models\nwe first present a model that treats x as nominal and y as ordinal. it is\nappropriate for two-way tables with ordered columns, using scores \u00ae f \u00ae f\n\u2b48\u2b48\u2b48 f \u00ae . since the rows are unordered, they do not have scores. replacing\nthe ordered values \u2424u in the linear-by-linear term \u2424u \u00ae in model 9.6 by\nunordered parameters \u242e gives\n\ni\n\u0004\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n4\n\n1\n\n2\n\nj\n\ni\n\nj\n\ni\n\nlog \u242e s \u242dq \u242dx q \u242dy q \u242e\u00ae .\n\ni\n\nj\n\ni j\n\ni\n\nj\n\n9.8\u017e\n\n.\n\ni\n\n.\n\n\u017e\n\ny\nj\n\nx\ni\n\nconstraints are needed such as \u242d s \u242d s \u242e s 0. the \u242e are called row\neffects. the model is called the row effects model.\nmodel 9.8 has i y 1 more parameters the \u242e than the independence\n\u0004\nmodel. independence is the special case \u242e s \u2b48\u2b48\u2b48 s \u242e . a corresponding\ncolumn effects model has association term u \u242f. it treats x as ordinal with\n\u0004\nand y as nominal with parameters \u242f . the row effects and\nscores u\ncolumn effects models were developed by goodman 1979a , haberman\n.\n\u017e\n1974b , and simon 1974 .\n\n4.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n4\n\n1\n\ni\n\ni\n\ni\n\ni\n\ni\n\nj\n\nj\n\n9.5.2 logit model for adjacent responses\nwith \u00ae y \u00ae s 1 , the row effects model has adjacent-categories logit\nform\n\njq1\n\n4\n\n\u0004\n\nj\n\np y s j q 1 x s i\n\u017e\np y s j x s i\n.\n\u017e\n\n<\n\n<\n\n.\n\nlog\n\ns \u2423 q \u242e.\n\ni\n\nj\n\n9.9\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\nfor different\n\ni s 1, . . . , i\n\u017e\n4\n\nthe effect in row i is identical for each pair of adjacent responses. plots of\nthese logits against i\nj are parallel. goodman\n\u017e\n1983 referred to model 9.9 as the parallel odds model.\ndifferences among \u242e compare rows with respect to their conditional\ndistributions on y. when \u242e s \u242e , rows h and i have identical conditional\ndistributions. if \u242e ) \u242e , y is stochastically higher in row i than row h.\nthe likelihood equations for the row effects model 9.8 are \u242e s n\n\u242e s n\n\u02c6qj\nqj\n\n, and\n\n\u02c6iq\n\niq\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n,\n\nh\n\nh\n\ni\n\ni\n\ni\n\n\u02c6\u00fd\n\n\u00ae \u242e s \u00ae n ,\n\n\u00fd\n\ni j\n\ni j\n\nj\n\nj\n\nj\n\ni s 1, . . . , i.\n\ni j\n\nj < i\n\n\u02c6\n\n\u02c6\n\nlet \u2432 s \u242e r\u242e and p s n rn . since \u242e s n , the third likelihood\nequation is \u00fd \u00ae \u2432 s \u00fd \u00ae p . for the conditional distribution within each\nrow, the mean column score is the same for the fitted and sample distribu-\ntions. the likelihood equations are solved using iterative methods.\n\n\u02c6\niq\n\u02c6j\nj < i\nj\n\niq\n\niq\n\niq\n\n\u02c6\n\nj < i\n\nj < i\n\ni j\n\nj\n\nj\n\n "}, {"Page_number": 390, "text": "association models\n\n375\n\ntable 9.5 observed frequencies and fitted values for political ideology data\n\nparty affiliation\ndemocrat\n\nindependent\n\nrepublican\n\n1\n2\n\n.\n.\n\nliberal\n143\n\u017e\n102.0\n\u017e\n136.6\n119\n\u017e\n120.2\n\u017e\n123.8\n15\n\u017e\n54.7\n\u017e\n16.6\n\n.\n.\n\n.\n.\n\npolitical ideology\n\na\n\nmoderate\n\nconservative\n\n.\n.\n\n.\n.\n\n156\n\u017e\n161.4\n\u017e\n168.7\n210\n\u017e\n190.1\n\u017e\n200.4\n72\n\u017e\n86.6\n\u017e\n68.9\n\n.\n.\n\n100\n\u017e\n135.6\n\u017e\n.\n93.6\n141\n\u017e\n159.7\n\u017e\n145.8\n127\n\u017e\n.\n72.7\n\u017e\n128.6\n\n.\n\n.\n.\n\n.\n\ntotal\n399\n\n470\n\n214\n\na1independence model; 2 row effects model.\n.\nsource: based on data in r. d. hedlund, public opinion quart. 41: 498\u1390514 1978 .\n\n\u017e\n\n<\n\nj\n\n3\n\n2\n\n1\n\n0\n\n\u0004\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n9.5.3 political ideology example\ntable 9.5 displays the relationship between political ideology and political\nparty affiliation for a sample of voters in a presidential primary in wisconsin.\nthe table shows fitted values for the independence i model and the row\neffects r model with \u00ae s j .\n4\n\n\u017e .\n\ntable 9.6 shows output. goodness-of-fit tests show that independence is\n2\u017e .\ninadequate. adding the row effects parameters much improves the fit g i\ns 105.7, df s 4; g r s 2.8, df s 2 . also, testing h : \u242e s \u242e s \u242e using\ng i r s 102.9 df s 2 shows very strong evidence of an association. in\ntable 9.5, the improved fit is especially noticeable at the ends of the ordinal\nscale, where the model has greatest deviation from independence.\n\n2\u017e\n\n2\u017e\n\n\u02c6\n\nthe output uses dummy variables for the first two categories of each\nclassification. the interaction term equals the product of the score for\nideology and a parameter for party. thus, the row effect estimates satisfy\n\u242e s 0, and the other two estimates contrast the first two parties with\n\u02c6 3\nrepublicans. the estimates are \u242e s y1.213 and \u242e s y0.943. the further\n\u02c6i\n\u242e falls in the negative direction, the greater the tendency for the party i to\nlocate at the liberal end of the ideology scale, relative to republicans. in this\nsample the republicans are much more conservative than the other two\ngroups, and the democrats row 1 are the most liberal. from 9.9 the model\npredicts constant odds ratios for adjacent columns of political ideology. for\ninstance, since \u242e y \u242e s 1.213, the estimated odds that republicans were\nconservative instead of moderate, or moderate instead of liberal, were\nexp 1.213 s 3.36 times the corresponding estimated odds for democrats.\nfigure 9.3 shows the parallelism of the estimated logits for the row effects\nmodel.\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\nthe loglinear model does not distinguish between response and explana-\ntory variables. instead, one could use a cumulative logit model to describe\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n1\n\n2\n\n3\n\n1\n\n "}, {"Page_number": 391, "text": "376\n\nbuilding and extending loglinearr logit models\n\ntable 9.6 output for fitting row effects model to table 9.5\n\ncriteria for assessing goodness of fit\ncriterion\ndeviance\npearson chi- square\n\nvalue\n2.8149\n2.8039\n\ndf\n2\n2\n\nstd\n\nchi-\n\nsquare\n\nlimits\n\nwald 95% conf.\n\nestimate error\n4.8565 0.0858\n3.3230 0.3188\n2.9536 0.3149\n0.0000 0.0000\n\npr )\nparameter\nchisq\n5.0246 3204.02 -.0001\nintercept\n108.63 -.0001\nparty\n3.9479\n87.98 -.0001\nparty\n3.5707\nparty\n0.0000\ny2.0488 0.2216 y2.4831 y1.6145\nideology\ny0.6244 0.1139 y0.8476 y0.4013\nideology\nideology\n0.0000\nscore*party democ y1.2134 0.1304 y1.4690 y0.9577\nscore*party indep y0.9426 0.1260 y1.1896 y0.6956\n0.0000\nscore*party repub\n\ndemoc\nindep\nrepub\n1\n2\n3\n\n86.56 -.0001\n55.95 -.0001\n\n85.50 -.0001\n30.08 -.0001\n\n4.6883\n2.6981\n2.3364\n0.0000\n\n0.0000 0.0000\n\n0.0000 0.0000\n\n0.0000\n\n0.0000\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nlr statistics\n\nsource\nscore*party\n\ndf\n2\n\nchi- square\n\n102.85\n\npr ) chisq\n\n-.0001\n\nfigure 9.3 observed and predicted logits for adjacent response categories.\n\n "}, {"Page_number": 392, "text": "association models\n\n377\n\nthe effects of party affiliation on ideology, or a baseline-category logit model\nto describe linear effects of ideology on party affiliation.\n\n9.5.4 ordinal variables in models for multiway tables\nmultidimensional tables with ordinal responses can use generalizations of\nassociation models. in three dimensions, the rich collection of models in-\ncludes 1 association models that are more parsimonious than the nominal\nmodel xy, xz, yz , and 2 models permitting heterogeneous association\nthat, unlike model xyz , are unsaturated.\n\n\u017e .\n\u017e\n\n\u017e .\n\n.\n\n\u017e\n\n.\n\nmodels for association that are special cases of xy, xz, yz replace \u242d\nassociation terms by structured terms that account for ordinality. for in-\nstance, when both x and y are ordinal, alternatives to \u242dx y are a linear-by-\nlinear term \u2424u \u00ae , a row effects term \u242e\u00ae , or a column effects term u \u242f;\nj\nthese provide a stochastic ordering of conditional distributions within rows\nand within columns, or just within rows, or just within columns. with a\nlinear-by-linear term, the model is\n\ni j\n\ni\n\ni\n\ni\n\nj\n\nj\n\n\u017e\n\n.\n\nlog \u242e s \u242dq \u242dx q \u242dy q \u242dz q \u2424u \u00ae q \u242dx z q \u242dy z.\n\ni\n\nj\n\nik\n\ni jk\n\njk\n\nk\n\ni\n\n\u017e\n\n9.10\n\n.\n\nthe conditional local odds ratios 8.13 then satisfy\n\n.\n\nj\n\n\u017e\n\nlog \u242a s \u2424 u y u\n\n\u017e\n\niq1\n\ni j\u017e k.\n\n. \u017e\n\ni\n\n\u00ae y \u00ae\njq1\nj\n\n.\n\nfor all k.\n\nthe association is the same in different partial tables, with homogeneous\nlinear-by-linear xy association.\n\nwhen the association is heterogeneous, structured terms for ordinal\nvariables make effects simpler to interpret than in the saturated model. for\ninstance, the heterogeneous linear-by-linear xy association model\nlog \u242e s \u242dq \u242dx q \u242dy q \u242dz q \u2424 u \u00ae q \u242dx z q \u242dy z\n\n9.11\n\n\u017e\n\n.\n\nj\n\nk\n\nk\n\ni\n\nj\n\nik\n\ni jk\n\njk\n\ni\n\nallows the xy association to change across levels of z. with unit-spaced\nscores,\n\nlog \u242a s \u2424\n\ni j\u017e k.\n\nk\n\nfor all i and j.\n\nit has uniform association within each level of z, but heterogeneity among\nlevels of z in the strength of association. fitting it corresponds to fitting the\nl = l model 9.6 separately at each level of z.\n\n\u017e\n\n.\n\n9.5.5 air pollution and breathing examples\ntable 9.7 displays associations among smoking status s , breathing test\nresults b , and age a for workers in certain industrial plants in houston,\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 393, "text": "378\n\nbuilding and extending loglinearr logit models\n\ntable 9.7 cross-classification of industrial workers by breathing test results\n\nage\n- 40\n\n40\u139059\n\nsmoking status\nnever smoked\nformer smoker\ncurrent smoker\nnever smoked\nformer smoker\ncurrent smoker\n\nbreathing test results\n\nnormal\n\nborderline\n\nabnormal\n\n577\n192\n682\n164\n145\n245\n\n27\n20\n46\n4\n15\n47\n\n7\n3\n11\n0\n7\n27\n\nsource: from p. 21 of public program analysis by r. n. forthofer and r. g. lehnen. copyright\n\u429a 1981 by lifetime learning publications, belmont, ca 94002, a division of wadsworth, inc.\nreprinted by permission of van nostrand reinhold. all rights reserved.\n\n2\n\n2\n\n2\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u02c6\n1\n\n\u02c6\n2\n\ntexas. the loglinear model sa, sb, ba fits poorly g s 25.9, df s 4 .\n.\nthus, simpler models such as homogeneous linear-by-linear sb association\nare not plausible g s 29.1, df s 7, using equally spaced scores . the\nheterogeneous linear-by-linear sb association model fits much better with\nonly one additional parameter g s 10.8, df s 6 . with integer scores for s\nand b, \u2424 s 0.115 for the younger group and \u2424 s 0.781 for the older group,\nwith se s 0.167 for the difference. the effect of smoking seems much\nstronger for the older group, with estimated local odds ratio of exp 0.781 s\n2.18 compared to exp 0.115 s 1.12 for the younger group. here, it may be\nmore natural to use logit models with b as the response variable problem\n.\n7.11 .\nwhen strata are ordered, roughly a linear trend may exist across strata in\ncertain log odds ratios as table 9.8 illustrates. the data refer to a sample of\ncoal miners, measured on b s breathlessness, w s wheeze, and a s age,\nwhere b and w are response variables. one could use a separate logit model\nto describe effects of age on each response. to study whether the bw\nassociation varies by age, we fit model bw, ab, aw . it has residual\ng2 s 26.7, with df s 8. table 9.8 reports the standardized pearson residuals.\nthey show a decreasing tendency as age increases.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nthis suggests the model\n\nlog \u242e s bw , ab, aw q ki i s j s 1 \u2426,\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni jk\n\n\u017e\n\n9.12\n\n.\n\nwhere i is the indicator function. it amends the homogeneous association\nmodel by adding \u2426 in the cell for \u242e , . . . , 9\u2426 in the cell for \u242e . then, the\nbw log odds ratio changes linearly in the age category. the model fit has\n\u2426s y0.131 se s 0.029 . the estimated bw log odds ratio at level k of age\n\u02c6\nis 3.676 y 0.131k, decreasing from 3.55 to 2.50. the model has residual\ng s 6.80 df s 7 . mccullagh and nelder 1989, sec. 6.6 showed other\nanalyses.\n\n111\n\n119\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n2\n\n "}, {"Page_number": 394, "text": "association, correlation, and correspondence models\n\n379\n\ntable 9.8 coal miners classified by breathlessness, wheeze, and age\n\nbreathlessness\n\nyes\n\nno\n\nwheeze\n\nwheeze\n\nwheeze\n\nwheeze\n\nstd. pearson\na\nage\nresidual\n20\u139024\n0.75\n25\u139029\n2.20\n30\u139034\n2.10\n35\u139039\n1.77\n40\u139044\n1.13\ny0.42\n45\u139049\n50\u139054\n0.81\ny3.65\n55\u139059\ny1.44\n60\u139064\naresidual refers to yes\u1390yes and no\u1390no cells; reverse sign for yes\u1390no and no\u1390yes cells.\n.\nsource: reprinted with permission from ashford and sowden 1970 .\n\nno\n1841\n1654\n1863\n2357\n1778\n1712\n1324\n967\n526\n\nyes\n95\n105\n177\n257\n273\n324\n245\n225\n132\n\nyes\n9\n23\n54\n121\n169\n269\n404\n406\n372\n\nno\n7\n9\n19\n48\n54\n88\n117\n152\n106\n\n\u017e\n\n<\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n2\u017e\n\n9.5.6 other ordinal tests of conditional independence\ntests of conditional independence of ordinal classifications can generalize\ng i l = l . for instance, one can compare the xy conditional indepen-\ndence model xz, yz to the homogeneous linear-by-linear xy association\nmodel 9.10 . it tests \u2424s 0 in that model, with df s 1. this is an alternative\nto the ordinal test of conditional independence in section 7.5.3. like mantel\u2019s\n\u017e\n.\n7.21 ,\nscore statistic\nsince\n\u00fd \u00fd \u00fd u \u00ae n\n.\nis the sufficient statistic for \u2424 in model 9.10 . in fact, the\ni jk\nmantel statistic provides the score test of h : \u2424s 0 in that model.\n\nstatistic uses correlation information,\n\nthis\n\nexact, small-sample tests can use likelihood-ratio, score, or wald statistics\nfor such models. computations require special algorithms agresti et al.\n.\n1990; kim and agresti 1997 .\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n0\n\nk\n\ni\n\ni\n\nj\n\nj\n\n9.6 association models, correlation models, and\ncorrespondence analysis*\n\n.\n\n\u017e\n\nthe linear-by-linear association l = l model is a special case of the row\neffects r model, which has parameter row scores, and the column effects\n\u017e\n.c model, which has parameter column scores. these models are special\ncases of a more general model with row and column parameter scores.\n\n\u017e\n\n.\n\n9.6.1 multiplicative row and column effects model\nreplacing u and \u00ae in the l = l model 9.6 by parameters yields the row\n.\n.\nand column effects rc model goodman 1979a\n\n\u0004\nj\u017e\n\n\u017e\n\n4\n\n\u0004\n\n4\n\ni\n\n\u017e\n\n.\nlog \u242e s \u242dq \u242dx q \u242dy q \u2424\u242e\u242f .\n\ni\n\nj\n\ni j\n\ni\n\nj\n\n\u017e\n\n9.13\n\n.\n\n "}, {"Page_number": 395, "text": "380\n\nbuilding and extending loglinearr logit models\n\n\u017e\n\nidentifiability requires location and scale constraints on \u242e and \u242f . the\nresidual df s i y 2 j y 2 . this model is not loglinear, because the predic-\ntor is a multiplicative rather than linear function of parameters \u242e and \u242f. it\ntreats classifications as nominal; the same fit results from a permutation of\nrows or columns. parameter interpretation is simplest when at least one\nvariable is ordinal, through the local log odds ratios\n\n.\u017e\n\u017e\n\n.\n\n.\n\ni\n\ni\n\nj\n\nj\n\n\u0004\n\n4\n\n\u0004\n\n4\n\nlog\u242a s \u2424 \u242e y \u242e \u242f y \u242f .\n.\n\n. \u017e\n\n\u017e\n\niq1\n\njq1\n\ni j\n\ni\n\nj\n\n.\n\n.\n\n\u017e\n\nalthough it may seem appealing to use parameters instead of arbitrary\nscores, the rc model presents complications that do not occur with loglinear\nmodels. the likelihood may not be concave and may have local maxima.\nindependence is a special case, but it is awkward to test independence using\nthe rc model. haberman 1981 showed that\nthe null distribution of\ng i y g rc is not chi-squared but rather that of the maximum eigen-\nvalue from a wishart matrix.\n\n2\u017e .\n\n2\u017e\n\nwhen one set of parameter scores is fixed, the rc model simplifies to the\nr or c model. goodman 1979a suggested an iterative model-fitting algo-\nrithm that exploits this. a cycle of the algorithm has two steps. first, for\nsome initial guess of \u242f , it estimates the row scores as in the r model. then,\ntreating the estimated row scores from the first step as fixed, it estimates the\ncolumn scores as in the c model. those estimates serve as fixed column\nscores in the first step of the next cycle, for reestimating the row scores in the\nr model. there is no guarantee of convergence to ml estimates, but this\nseems to happen when the model fits well. haberman 1995 provided more\nsophisticated fitting methods for association models.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n4\n\nj\n\ngoodman 1985 expressed the association term in the saturated model in\n\n.\n\n\u017e\n\na form that generalizes the \u2424\u242e\u242f term in the rc model, namely,\n\ni\n\nj\n\nx y\u242d s \u2424 \u242e \u242f\njk\ni j\n\nik\n\nk\n\nm\n\n\u00fd\nks1\n\n\u017e\n\n9.14\n\n.\n\nwhere m s min i y 1, j y 1 . the parameters satisfy constraints such as\n\n\u017e\n\n.\n\n\u00fd\n\ni\n\n\u00fd\n\ni\n\n\u242e \u2432 s \u242f \u2432 s 0\n\njk qj\n\n\u00fd\n\niq\n\nik\n\nj\n\n\u242e2 \u2432 s \u242f2 \u2432 s 1\n\njk qj\n\n\u00fd\n\niq\n\nik\n\nj\n\n\u00fd\n\ni\n\n\u242e \u242e \u2432 s \u242f \u242f \u2432 s 0\n\njh qj\n\n\u00fd\n\niq\n\nih\n\nik\n\njk\n\nj\n\nfor all k,\n\nfor all k,\n\nfor all k / h.\n\n\u017e\n\n9.15\n\n.\n\nwhen \u2424 s 0 for k ) m*, model 9.14 is called the rc m* model. see\nk\u017e\nbecker 1990 for ml model fitting. the rc model 9.13 is the case\nm* s 1.\n\n.\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n "}, {"Page_number": 396, "text": "association, correlation, and correspondence models\n\n381\n\ntable 9.9 cross-classification of mental health status and socioeconomic status\n\n.\n\n\u017e\n\nparents\u2019\nsocioeconomic\nstatus\na high\nb\nc\nd\ne\nf low\n\n\u017e\n\n.\n\nmental health status\n\nmild\n\nsymptom\nformation\n\nmoderate\nsymptom\nformation\n\n94\n94\n105\n141\n97\n71\n\n58\n54\n65\n77\n54\n54\n\nwell\n64\n57\n57\n72\n36\n21\n\nimpaired\n\n46\n40\n60\n94\n78\n71\n\nsource: reprinted with permission from l. srole et al. mental health in the metropolis: the\nmidtown manhattan study, new york: nyu press, 1978 , p. 289.\n\n\u017e\n\n.\n\n2\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n9.6.2 mental health status example\ntable 9.9 describes the relationship between child\u2019s mental impairment and\nparents\u2019 socioeconomic status for a sample of residents of manhattan good-\nman 1979a . the rc model fits well g s 3.6, df s 8 . for scaling 9.15 ,\n.\nthe ml estimates are y1.11, y1.12, y0.37, 0.03, 1.01, 1.82 for the row\nscores, y1.68, y0.14, 0.14, 1.41 for the column scores, and \u2424s 0.17. nearly\nall estimated local log odds ratios are positive, indicating a tendency for\nmental health to be better at higher levels of parents\u2019 ses.\nordinal loglinear models also fit well. for equal-interval scores, g l =\nl s 9.9 df s 14 . the statistic g l = l rc s 6.3 df s 6 tests that\n.\nrow and column scores in the rc model are equal-interval. the parameter\nscores do not provide a significantly better fit. it is sufficient to use a uniform\nlocal odds ratio to describe the table. for unit-spaced scores, \u2424s 0.091\nse s 0.015 , so the fitted local odds ratio is exp 0.091 s 1.09. there is\n\u017e\nstrong evidence of positive association, but the degree of association is rather\nweak, at least locally.\n\n.\n\u02c6\n\n2\u017e\n\n2\u017e\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n<\n\n9.6.3 correlation models\na correlation model for two-way tables has many features in common with\nthe rc model goodman 1985 . in its simplest form, it is\n\n.\n\n\u017e\n\n\u2432 s \u2432 \u2432 1 q \u242d\u242e\u242f ,\n.\n\niq qj\n\n\u017e\n\ni j\n\ni\n\nj\n\n\u017e\n\n9.16\n\n.\n\nwhere \u242e and \u242f are score parameters satisfying\n\n\u0004\n\n4\n\ni\n\n\u0004\n\n4\n\ni\n\n\u00fd\n\n\u242e\u2432 s \u242f\u2432 s 0 and\n\n\u00fd\n\nj qj\n\niq\n\ni\n\n\u00fd\n\n\u242e2\u2432 s \u242f2\u2432 s 1.\n\nj qj\n\n\u00fd\n\niq\n\ni\n\n "}, {"Page_number": 397, "text": "382\n\nbuilding and extending loglinearr logit models\n\nthe parameter \u242d is the correlation between the scores for joint distribution\n.\n\u017e\n9.16 .\n\nthe correlation model\n\nis also called the canonical correlation model,\nbecause ml estimates of the scores maximize the correlation for 9.16 . the\ngeneral canonical correlation model is\n\n\u017e\n\n.\n\n\u2432 s \u2432 \u2432 1 q \u242d \u242e \u242f\n\niq qj\n\nik\n\ni j\n\nk\n\njk\n\n\u017e\n\nm\n\n\u00fd\nks1\n\n/\n\nk\n\n2\n\n1\n\n1\n\n4\n\n\u0004\n\n4\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n4\n\n\u0004\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\nj1\n\ni1\n\nj1\n\ni1\n\njk\n\nj2\n\n\u017e\n\n\u017e\n\ni2\n\ni2\n\nm\n\n.\ni s 1, . . . , i and \u242f ,\n\nwhere 0 f \u242d f \u2b48\u2b48\u2b48 f \u242d f 1 and with constraints such as in 9.15 . the\nj s\nparameter \u242d is the correlation between \u242e ,\nik\n1, . . . , j . the \u242e and \u242f are standardized scores that maximize the\ncorrelation \u242d for the joint distribution; \u242e and \u242f are standardized\nscores that maximize the correlation \u242d , subject to \u242e and \u242e being\nuncorrelated and \u242f and \u242f being uncorrelated, and so on.\nunsaturated models result from replacing m by m* - min i y 1, j y 1 .\n.\n\u017e\ngilula and haberman 1986 and goodman 1985 discussed ml fitting.\nwhen \u242d is close to zero in 9.16 , goodman 1981a, 1985, 1986 noted that\nml estimates of \u242d and the score parameters are similar to those of \u2424 and\nthe score parameters in the rc model. correlation models can also use fixed\nscores instead of parameter scores.\n\nj2\u0004\n\n\u017e\n\u017e\n\n.\n\u017e\n\ngoodman discussed advantages of association models over correlation\nmodels. the correlation model is not defined for all possible combinations of\nscore values because of the constraint 0 f \u2432 f 1, ml fitted values do not\nhave the same marginal totals as the observed data, and the model is not\nsimply generalizable to multiway tables. gilula and haberman 1988 ana-\nlyzed multiway tables with correlation models by treating explanatory vari-\nables as a single variable and response variables as a second variable.\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\ni j\n\n9.6.4 correspondence analysis\ncorrespondence analysis is a graphical way to represent associations in\ntwo-way contingency tables. the rows and columns are represented by points\n.\non a graph, the positions of which indicate associations. goodman 1985, 1986\n4\nnoted that coordinates of the points are reparameterizations of \u242e and \u242f\njk\nin the general canonical correlation model. correspondence analysis uses\nadjusted scores\n\nik\n\n\u017e\n\n\u0004\n\n4\n\n\u0004\n\nx s \u242d \u242e ,\n\nik\n\nik\n\nk\n\ny s \u242d \u242f .\n\njk\n\njk\n\nk\n\nthese are close to zero for dimensions k in which the correlation \u242d is close\nto zero. a correspondence analysis graph uses the first two dimensions,\nplotting x , x\n\nfor each row and y , y\n\nfor each column.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nk\n\ni1\n\ni2\n\nj1\n\nj2\n\n "}, {"Page_number": 398, "text": "association, correlation, and correspondence models\n\n383\n\ntable 9.10 scores from correspondence analysis applied to table 9.9\n\ncolumn score\n\n1\n2\n3\n4\n\ndimension\n\n3\n2\n1\n0.012\n0.023\n0.260\n0.024 y0.019\n0.030\ny0.013 y0.069 y0.002\ny0.236\n0.016\n\n0.019\n\nrow score\n\n1\n2\n3\n4\n5\n6\n\ndimension\n\n2\n\n3\n1\n0.181 y0.018\n0.028\n0.185 y0.011 y0.026\n0.059 y0.021 y0.010\ny0.008\n0.042\n0.011\n0.044 y0.009\ny0.164\ny0.287 y0.061\n0.005\n\nsource: reprinted with permission from the institute of mathematical statistics, based on\n.\ngoodman 1985 .\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\ngoodman 1985, 1986 used table 9.9 to illustrate the similarities of\ncorrespondence analysis to analyses using correlation models and association\nmodels. for the general canonical correlation model, m s min i y 1, j y 1\n.\ns 3. its estimated squared correlations are 0.0260, 0.0014, and 0.0003 . the\nassociation is rather weak. table 9.10 contains estimated row and column\nscores for the correspondence analysis of these three dimensions. both sets\nof scores in the first dimension fall in a monotone increasing pattern, except\nfor a slight discrepancy between the first two row scores. this indicates an\noverall positive association. the scores for the second and third dimension\n\u02c6\nare close to zero, reflecting the relatively small \u242d and \u242d .\n3\n\nfigure 9.4 exhibits the results of the correspondence analysis. the hori-\nzontal axis has estimates for the first dimension, and the vertical axis has\nestimates for the second dimension. six points circles\nrepresent the six\n.\n\u017e\n\u02c6\n\u02c6i1\nrows, with point i giving x , x\n. similarly, four points squares display the\ni2\n.\nestimates\n. both sets of points lie close to the horizontal axis, since\nthe first dimension is more important than the second.\n\n\u02c6\n\u02c6j1\ny , y\nj2\n\n\u02c6\n2\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\nfigure 9.4 graphical display of scores from first two dimensions of correspondence analysis.\nw\nbased on escoufier 1982 ; reprinted with permission.\n\n\u017e\n\n.\n\nx\n\n "}, {"Page_number": 399, "text": "384\n\nbuilding and extending loglinearr logit models\n\n.\n\nrow points that are close together represent rows with similar conditional\ndistributions across the columns. close column points represent columns with\nsimilar conditional distributions across rows. row points close to column\npoints represent combinations that are more likely than expected under\nindependence. figure 9.4 shows a tendency for subjects at the high end of\none scale to be at the high end of the other and for subjects at the low end of\none to be at the low end of the other.\n\ncorrespondence analysis is used mainly as a descriptive tool. goodman\n\u017e\n1986 developed inferential methods for it. for table 9.9, inferential analysis\nreveals that the first dimension, accounting for 94% of the total squared\ncorrelation, is adequate for describing the association. goodman argued for\nchoosing the unsaturated model employing only one dimension and having\ngraphics display fitted scores for that dimension alone. then, correspondence\nanalysis is equivalent to a ml analysis using correlation model 9.16 . the\nestimated scores for that model are y1.09, y1.17, y0.37, 0.05, 1.01, 1.80\n.\nfor the rows and y1.60, y0.19, 0.09, 1.48 for the columns. the model fits\nwell g s 2.75, df s 8 . the quality of fit and the estimated scores are\nsimilar to those we saw in section 9.6.2 for the rc model. more parsimo-\nnious correlation models also fit these data well, such as ones using equally\nspaced scores.\n\nall analyses of table 9.9 have yielded similar conclusions about the\nthat mental health is a natural\n\nassociation. they all neglect, however,\nresponse variable. it may make more sense to use an ordinal logit model.\n\nlike correlation models, a severe limitation of correspondence analysis is\nnontrivial generalization to multiway tables. greenacre 1993 showed dis-\nplays of several pairwise associations in a single plot.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2\n\n9.6.5 model selection and score choice for ordinal variables\nthe past three sections showed several ways to use category orderings in\nmodel building. with allowance for ordinal effects, the variety of potential\nmodels is much greater than standard loglinear models. to choose among\nmodels, one approach uses the standard models for guidance. if a standard\nmodel fits well, simplify by replacing some parameters with structured terms\nfor ordinal classifications.\n\nassociation, correlation, and correspondence analysis models have scores\nfor categories of ordinal variables. parameter interpretations are simplest for\nequally spaced scores. with parameter scores, the resulting ml estimates of\nscores need not be monotone. constrained versions of the models force\nmonotonicity by maximizing the likelihood subject to order restrictions e.g.,\nagresti et al. 1987; ritov and gilula 1991 . disadvantages exist, however, of\ntreating scores as parameters. the model becomes less parsimonious, and\ntests of effects may be less powerful because of a greater df value recall\nsection 6.4.3 . when one variable alone is a response, cumulative link models\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n "}, {"Page_number": 400, "text": "poisson regression for rates\n\n385\n\n\u017e\nsections 7.2 and 7.3 for that response do not require preassigned or\nparameter scores.\n\n.\n\n9.7 poisson regression for rates\n\nloglinear models need not refer to contingency tables. in section 4.3 we\nintroduced poisson regression for modeling counts. when outcomes occur\nover time, space, or some other index of size, it is more relevant to model\ntheir rate of occurrence than their raw number.\n\n9.7.1 analyzing rates using loglinear models with offsets\nwhen a response count n has index equal to t , the sample rate is n rt . its\nexpected value is \u242ert . with an explanatory variable x, a loglinear model for\nthe expected rate has form\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nlog \u242ert s \u2423q \u2424x .\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\n\u017e\n\n9.17\n\n.\n\nthis model has equivalent representation\n\nlog \u242e y log t s \u2423q \u2424x .\n\ni\n\ni\n\ni\n\nas noted in section 8.7.4, the adjustment term, ylog t , to the log link of the\nmean is called an offset. the fit correspond to using log t as a predictor on\ni\nthe right-hand side and forcing its coefficient to equal 1.0.\nfor model 9.17 , the expected response count satisfies\n\n.\n\n\u017e\n\ni\n\n\u242e s t exp \u2423q \u2424x\n\n\u017e\n\ni\n\ni\n\n.\n\ni\n\n.\n\nthe mean is proportional to the index, with proportionality constant depend-\ning on the value of x. the identity link is also sometimes useful. the model is\nthen\n\n\u242ert s \u2423q \u2424x , or \u242e s \u2423t q \u2424x t .\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nthis does not require an offset. it corresponds to an ordinary poisson glm\nusing identity link with t and x t as explanatory variables and no intercept.\nit provides additive, rather than multiplicative, predictor effects. it is less\nuseful with many predictors, as the fitting process may fail because of\nnegative fitted counts at some iteration.\n\ni\n\ni\n\ni\n\n9.7.2 modeling death rates for heart valve operations\nlaird and olivier 1981 analyzed patient survival after heart valve replace-\nment operations. a sample of 109 patients were classified by type of heart\n\n\u017e\n\n.\n\n "}, {"Page_number": 401, "text": "386\n\nbuilding and extending loglinearr logit models\n\ntable 9.11 data on heart valve replacement operations\n\ntype of heart valve\n\nage\n- 55\n\n55 q\n\ndeaths\ntime at risk\ndeath rate\ndeaths\ntime at risk\ndeath rate\n\naortic\n4\n1259\n0.0032\n7\n1417\n0.0049\n\nmitral\n1\n2082\n0.0005\n9\n1647\n0.0055\n\n.\nsource: reprinted with permission, based on data in laird and olivier 1981 .\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nvalve aortic, mitral and by age - 55, g 55 . follow-up observations oc-\ncurred until the patient died or the study ended. operations occurred\nthroughout the study period, and follow-up observations covered lengths of\ntime varying from 3 to 97 months. the response was whether the subject died\nand the follow-up time. for subjects who died, this is the time after the\noperation until death; for the others, it is the time until the study ended or\nthe subject withdrew from it.\n\ntable 9.11 lists the numbers of deaths during the follow-up period, by\nvalve type and age. these counts are the first layer of a three-way contin-\ngency table that classifies valve type, age, and whether died yes, no . the\nsubjects not tabulated in table 9.11 were not observed to die. they are\ncensored, since we know only a lower bound for how long they lived after the\noperation. it is inappropriate to analyze that 2 = 2 = 2 table using binary\nglms for the probability of death, since subjects had differing times at risk;\nit is not sensible to treat a subject who could be observed for 3 months and a\nsubject who could be observed for 97 months as identical trials with the same\nprobability. to use age and valve type as predictors in a model for frequency\nof death, the proper baseline is not the number of subjects but rather the\ntotal time that subjects were at risk. thus, we model the rate of death.\n\nthe time at risk for a subject is their follow-up time of observation. for a\ngiven age and valve type, the total time at risk is the sum of the times at risk\n\u017e\nthose who died and those censored . table 9.11\nfor all subjects in that cell\nlists those total times in months. the sample rate, also shown in that table,\ndivides the number of deaths by total time at risk. for instance, 4 deaths in\n1259 months of observation occurred for younger subjects with aortic valve\nreplacement, so their sample rate is 4r1259 s 0.0032.\nwe now model effects of age and valve type on the rate. let a be a\ndummy variable for age, with a s 0 for the younger age group and a s 1\nfor the older group. let \u00ae be a dummy variable for valve type, with \u00ae s 0 for\naortic and \u00ae s 1 for mitral. let n denote the number of deaths for age a\ni\nand valve type \u00ae , with expected value \u242e for total time at risk t\n. given t\n,\n\n.\n\ni j\n\n1\n\n2\n\n1\n\n2\n\ni j\n\ni j\n\ni j\n\nj\n\n "}, {"Page_number": 402, "text": "poisson regression for rates\n\n387\n\ntable 9.12 fit to table 9.11 for poisson regression models\n\nage\n- 55\n\n55 q\n\nnumber of deaths\ndeath rate\nnumber of deaths\ndeath rate\n\nlog link\n\nidentity link\n\naortic\n2.28\n0.0018\n8.72\n0.0062\n\nmitral\n2.72\n0.0013\n7.28\n0.0044\n\naortic\n3.16\n0.0025\n9.17\n0.0065\n\nmitral\n1.19\n0.0006\n7.48\n0.0046\n\nthe expected rate is \u242e rt . the model\n\ni j\n\ni j\n\nlog \u242e rt s \u2423q \u2424 a q \u2424 \u00ae\n\n\u017e\n\n.\n\n1 i\n\n2 j\n\ni j\n\ni j\n\n\u017e\n\n9.18\n\n.\n\nassumes a lack of interaction in the effects.\n\nmodel fitting uses standard iterative methods, treating n\n\nas indepen-\n\u0004\ndent poisson variates with means \u242e . this is done conditional on t\n. table\n9.12 presents the fitted death counts and estimated rates. the estimated\neffects are\n\n\u0004\n\n4\n\n4\n\ni j\n\ni j\n\ni j\n\n\u0004\n\n4\n\n\u2424 s 1.221\n\u02c6\n1\n\nse s 0.514 ,\n.\n\n\u017e\n\n\u2424 s y0.330\n\u02c6\n2\n\nse s 0.438 .\n.\n\n\u017e\n\n1\n\n1\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n. .\n\nthere is evidence of an age effect. given valve type, the estimated rate for\nthe older age group is exp 1.221 s 3.4 times that for the younger age group.\nthe 95% wald confidence interval for \u2424 of 1.221 \" 1.96 0.514 translates to\n\u017e\n1.2, 9.3 for the true multiplicative effect exp \u2424 . the likelihood-ratio\nconfidence interval is 1.3, 10.4 . the study contains much censored data. of\nthe 109 patients, only 21 died during the study period. both effect estimates\nare imprecise. note, though, that the analysis uses all 109 patients through\ntheir contributions to the times at risk.\nto fitted values \u242e are g s 3.2\n\u0004\nand x 2 s 3.1. the residual df s 1, since the four response counts have three\nparameters. the mild evidence of lack of fit corresponds to evidence of\ninteraction between valve type and age. however, the model without valve-\nfits nearly as well, with g s 3.8 and\ntype effects\nx s 3.8 df s 2 . models omitting age effects fit poorly.\n\u017e\n\ngoodness-of-fit statistics comparing n\n\nw\ni.e., \u2424 s 0 in 9.18\n\n2\nthe corresponding model with identity link\n\n.x\n\n\u02c6\n\n\u017e\n\n.\n\n4\n\n\u0004\n\n4\n\ni j\n\ni j\n\n2\n\n2\n\n2\n\n\u242e s \u2423t q \u2424 a t q \u2424 \u00ae t\n\n1 i\n\ni j\n\n2 j\n\ni j\n\ni j\n\ni j\n\n2\n\nshows a good fit, with g s 1.1 and x s 1.1 df s 1 . table 9.12 shows the\nfit. substantive conclusions are similar. the estimate \u2424 s 0.0040 se s\n0.0014 then represents an estimated difference in death rates between the\nolder and younger age groups for each valve type.\n\n\u02c6\n1\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n2\n\n "}, {"Page_number": 403, "text": "388\n\nbuilding and extending loglinearr logit models\n\n.\n\n\u017e\n\n9.7.3 modeling survival times*\na method for modeling survival times relates to the poisson loglinear model\nfor rates. this method focuses on times until death rather than on numbers\nof deaths. let t denote the time to some event, such as death or such as\nproduct failure in a reliability study. let f t denote the probability density\n\u017e .\nfunction pdf and f t\nthe cdf of t. a connection exists between ml\nestimation using a poisson likelihood for numbers of events and a negative\n.\nexponential likelihood for t aitkin and clayton 1980 .\n\na subject having t s t contributes f t\n\u017e .\n\nto the likelihood. for a subject\nwhose censoring time equals t, we know only that t ) t. thus, this subject\ncontributes p t ) t s 1 y f t . using the indicator w s 1 for death and 0\nfor censoring for subject i, the survival-time likelihood for n independent\nobservations is\n\n\u017e .\n\n\u017e .\n\n\u017e\n\n.\n\n\u017e\n\ni\n\nn\n\n\u0142 i\n\u017e\nt\nis1\n\nf\n\nw\n\ni\n\n.\n\n1 y f t\n\u017e\n\n.\n\ni\n\n1yw\n\ni\n\n.\n\nthe log likelihood equals\n\nw log f\n\ni\n\n\u017e\n\n\u00fd\n\ni\n\nt q 1 y w log 1 y f t\n.\n\u017e\n\n\u017e\n\n.\n\n\u00fd\n\ni\n\ni\n\ni\n\n.\n\ni\n\n.\n\n\u017e\n\n9.19\n\n.\n\nfurther analysis requires a parametric form for f and a model for the\ndependence of its parameters on explanatory variables.\n\nmost survival models focus on the rate at which death occurs rather than\n\non e t . the hazard function\n\n\u017e\n\n.\n\nh t s\n\u017e .\n\nf\n\n\u017e .\nt\n1 y f t\n\u017e .\n\ns lim\n\u2440x0\n\nw\np t - t - t q \u2440 t ) t\n\n<\n\nx\n\n\u2440\n\nrepresents the instantaneous rate of death for subjects who have survived to\ntime t. a simple density for survival modeling is the negative exponential.\nthe pdf is\n\nf\n\nthe cdf is f t s 1 y e\nis\n\n\u017e .\n\ny\u242dt\n\nt s \u242dey\u242dt,\n\u017e .\nfor t ) 0, and e t s \u242d . the hazard function\n\nt ) 0.\n\ny1\n\n\u017e\n\n.\n\nh t s \u242d,\n\u017e .\n\nt ) 0,\n\nconstant for all t.\n\nnow we include explanatory variables x. suppose that the hazard function\n\nfor a negative exponential survival distribution is\nh t ; x s \u242dexp \u2424x x .\n\u017e\n.\n\n.\n\n\u017e\n\n\u017e\n\n9.20\n\n.\n\n "}, {"Page_number": 404, "text": "poisson regression for rates\n\n389\n\ni\n\ni\n\n.\nthat is, the distribution for t has parameter depending on x through 9.20 .\nthe choice of functional form 9.20 for explanatory variable effects ensures\n.\nthe hazard is nonnegative at all x. for instance,\nloglinear model 9.18\n.\ncorresponds to a multiplicative model of type 9.20 for the rate itself.\n\u017e .\n\nnow, consider the log likelihood 9.19 with f t equal to the negative\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\nexponential density with parameter \u242dexp \u2424 x . for subject i, let\n\n\u017e\n\n.\n\u017e x .\n\u242e s t \u242dexp \u2424x x .\n\u017e\n.\n\ni\n\ni\n\nwith this substitution, the log likelihood simplifies to\n\nw log \u242e y \u242e y w log t .\n\n\u00fd\n\n\u00fd\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u00fd\n\ni\n\ni\n\ni\n\nthe first two terms involve \u2424. this part is identical to the log likelihood for\nindependent poisson variates w with expected values \u242e . in this applica-\ntion w are binary rather than poisson, but that is irrelevant to the process\nof maximizing with respect to \u2424. this process is equivalent to maximizing the\nlikelihood for the poisson loglinear model\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ni\n\ni\n\ni\n\nlog \u242e y log t s log \u242dq \u2424x x\n\ni\n\ni\n\ni\n\n4\n\n\u0004\n\n.\n\n\u017e\n\n\u017e .\nwith offset log t\n, using observations w . when we sum terms in the log\ni\nlikelihood for subjects having a common value of x, the observed data are the\n.\nnumbers of deaths \u00fdw at each setting of x, and the offset is the log of \u00fdt\ni\nat each setting.\n\nthe assumption of constant hazard over time is often not sensible. as\nproducts wear out, their failure rate increases. a generalization divides the\ntime scale into disjoint time intervals and assumes constant hazard in each,\nnamely,\n\n\u017e\n\ni\n\nh t ; x s \u242d exp \u2424x x\n\u017e\n\n.\n\n\u017e\n\n.\n\nk\n\n.\n\nfor t in interval k, k s 1, . . . . a separate hazard rate applies to each piece\nof the time scale. consider the contingency table for numbers of deaths, in\nwhich one dimension is a discrete time scale and other dimensions represent\ncategorical explanatory variables. holford 1980 and laird and olivier\n\u017e\n1981 showed that poisson loglinear models and likelihoods for this table are\nequivalent to loglinear hazard models and likelihoods that assume piecewise\nexponential hazards for the survival times.\n\nfor short time intervals, the piecewise exponential approach is essentially\nnonparametric, making no assumption about the dependence of the hazard\non time. this suggests the generalization of model 9.20 that replaces \u242d by\nan unspecified function \u242d t , so that\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e .\nh t ; x s \u242d t exp \u2424x x .\n\u017e\n.\n\n\u017e .\n\n.\n\n\u017e\n\nthis is the cox proportional hazards model. its ratio of hazards\n\nh t ; x rh t ; x s exp \u2424 x y x\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nx\n\n1\n\n2\n\n1\n\n.\n\n2\n\nis the same for all t.\n\n "}, {"Page_number": 405, "text": "390\n\nbuilding and extending loglinearr logit models\n\ntable 9.13 number of deaths from lung cancer\n\nhistology\n\na\n\nfollow-up\ntime\ninterval\n\u017e\nmonths\n0\u13902\n\n.\n\ndisease\nstage:\n\n1\n\n6\u13908\n\n4\u13906\n\n2\u13904\n\n3\n42\n212\n26\n136\n12\n90\n10\n64\n5\n47\n4\n39\n1\n29\navalues in parentheses represent total follow-up.\n.\nsource: reprinted with permission from the biometric society, based on holford 1980 .\n\n9\n\u017e\n157\n2\n\u017e\n139\n9\n\u017e\n126\n10\n\u017e\n102\n1\n\u017e\n88\n3\n\u017e\n82\n1\n\u017e\n76\n\n3\n28\n130\n19\n72\n10\n42\n5\n21\n0\n14\n3\n13\n2\n7\n\n1\n5\n77\n2\n68\n3\n63\n2\n55\n2\n50\n2\n45\n2\n42\n\n1\n1\n21\n1\n17\n1\n14\n1\n12\n0\n10\n1\n8\n0\n6\n\n12 q\n\n10\u139012\n\n8\u139010\n\n\u017e\n\n3\n19\n.\n101\n11\n.\n63\n7\n.\n43\n6\n.\n32\n3\n.\n21\n3\n.\n14\n3\n.\n10\n\ni\n2\n12\n134\n7\n110\n5\n96\n10\n86\n4\n66\n3\n59\n4\n51\n\niii\n2\n1\n22\n1\n18\n3\n14\n1\n10\n0\n8\n0\n8\n2\n6\n\nii\n2\n4\n71\n3\n63\n5\n58\n4\n42\n2\n35\n1\n32\n4\n28\n\n9.7.4 lung cancer survival example*\ntable 9.13 describes survival for 539 males diagnosed with lung cancer. the\nprognostic factors are histology h and stage s of disease. for a piecewise\nexponential hazard approach, the time scale for follow-up t was divided\ninto two-month intervals.\n\nlet \u242e denote the expected number of deaths and t\n\nthe total time at\nrisk for histology i and state of disease j, in follow-up time interval k. the\nmodel\n\ni jk\n\ni jk\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\n2\n\ni jk\n\ni jk\n\n.\n.\n\n\u017e\n\u017e\n\nlog \u242e rt s \u242dq \u242dh q \u242ds q \u242dt\n\n.\n9.21\nhas residual g s 43.9 df s 52 . all models assuming no interaction be-\ntween follow-up time interval and either prognostic factor are proportional\nhazards models, since they have the same effects of histology and stage of\ndisease for each time interval. table 9.14 summarizes results of fitting several\nsuch models. although stage of disease is an important prognostic factor,\nhistology did not contribute significant additional information.\n\n\u017e\n\nk\n\nj\n\nfor model 9.21 , the effects of stage of disease satisfy\n\n\u017e\n\n.\n\n\u242d y \u242d s 0.470\n\u02c6s\n2\n\u242d y \u242d s 1.324\n\u02c6s\n3\n\n\u02c6s\n1\n\u02c6s\n1\n\nse s 0.174 ,\n.\nse s 0.152 .\n.\n\n\u017e\n\u017e\n\n "}, {"Page_number": 406, "text": "empty cells and sparseness in modeling contingency tables\n\n391\n\ntable 9.14 results for poisson regression models\nof proportional hazards form with table 9.13\n\neffects\n\na\n\nt\nt q h\nt q s\nt q s q h\nt q s q h q s = h\nat, time scale for follow-up; h, histology; s, disease stage.\n\n2\n\ng\n170.7\n143.1\n45.8\n43.9\n41.5\n\ndf\n56\n54\n54\n52\n48\n\nfor instance, at a fixed follow-up time for a given histology, the estimated\ndeath rate at the third stage of disease is exp 1.324 s 3.8 times that at the\nfirst stage. adding interaction terms between stage and time does not sig-\nnificantly improve the fit change in g s 14.9, change in df s 12 . the\n.\n\u02c6s\u0004\n4\u242d are very similar for the simpler model without the histology effects.\nj\n\n\u017e\n\n\u017e\n\n.\n\n2\n\n9.7.5 analyzing weighted data*\nthe process of fitting a loglinear model with an offset is also useful in other\n\u0004\napplications. for expected frequencies \u242e and fixed constants t\n, consider a\nmodel\n\n\u0004\n\n4\n\n4\n\ni\n\ni\n\nlog \u242ert s \u2423q \u2424 x q \u2424 x q \u2b48\u2b48\u2b48 .\n\n\u017e\n\n.\n\n2\n\ni2\n\n1\n\ni1\n\ni\n\ni\n\ni\n\n4\n\n\u0004\n\nstandard loglinear models have t s 1 . the general form is useful for the\nanalysis of categorical data with sampling designs more complex than simple\nrandom sampling.\nmany surveys have sampling designs employing stratification andror clus-\ntering. case weights inflate or deflate the influence of each observation\naccording to features of that design. adding the case weights for subjects in a\nparticular cell i provides a total weighted frequency for that cell. the average\ncell weight z is defined to be the total weighted frequency divided by the cell\nloglinear models for the weighted expected\ncount. conditional on\nz \u242e s \u242ert with t s z\ny1\nfrequencies\nexpress the model as a standard\ni\ni\nlog t s ylog z . fitting this model\n\u0004\nloglinear model for log \u242e , with offset\nprovides appropriate parameter estimates and standard errors clogg and\n.\neliason 1987 .\n\n4\nz ,\ni\n4\n\n\u017e\n\n\u0004\n\n\u0004\n\n4\n\n4\n\n\u0004\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n9.8 empty cells and sparseness in modeling\ncontingency tables\n\ncontingency tables having small cell counts are said to be sparse. we end\nthis chapter by discussing effects of sparse tables on model fitting. sparse\n\n "}, {"Page_number": 407, "text": "392\n\nbuilding and extending loglinearr logit models\n\ntables occur when the sample size n is small. they also occur when n is large\nbut so is the number of cells. sparseness is common in tables with many\nvariables. the following discussion refers to a generic contingency table and\nmodel, with cell counts n and expected frequencies \u242e for n observations\nin n cells.\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n9.8.1 empty cells: sampling versus structural zeros\nsparse tables usually contain cells with n s 0. these empty cells are of two\ntypes: sampling zeros and structural zeros. in most cases, even though n s 0,\n\u242e ) 0. it is possible to have observations in the cell, and n ) 0 with\nsufficiently large n. this empty cell is called a sampling zero. the empty cells\nin table 9.1 for the student survey are sampling zeros.\nan empty cell in which observations are impossible is called a structural\nzero. for such cells \u242e s 0 and necessarily \u242e s 0 and n s 0 regardless of n.\nfor a table that cross classifies cancer patients on their gender, race, and\ntype of cancer, some cancers e.g., prostate cancer, ovarian cancer are\ngender specific. thus, certain cells have structural zeros. contingency tables\nwith structural zeros are called incomplete tables.\n\n\u02c6\n\nsampling zeros are part of the data set. a count of 0 is a permissible\noutcome for a poisson or multinomial variate. it contributes to the likelihood\nfunction and model fitting. a structural zero, on the other hand, is not an\nobservation and is not part of the data. sampling zeros are much more\ncommon than structural zeros, and the remaining discussion refers to them.\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\n9.8.2 existence of estimates in loglinear rrrrr logit models\nsampling zeros can affect the existence of finite ml estimates of loglinear\nand logit model parameters. haberman 1973b, 1974a , generalizing work by\nbirch 1963 and fienberg 1970b , studied this. let n denote the vector of\ncell counts and \u242e their expected values. haberman showed results 1 through\n5 for poisson sampling, but by result 6 they apply also to multinomial\nsampling.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n1. the log-likelihood function is a strictly concave function of log \u242e.\n2. if a ml estimate of \u242e exists, it is unique and satisfies the likelihood\nequations xx n s xx\u242e. conversely, if \u242e satisfies the model and also the\nlikelihood equations, it is the ml estimate of \u242e.\n\n\u02c6\n\n\u02c6\n\ni\n\n3. if all n ) 0, ml estimates of loglinear model parameters exist.\n4. suppose that ml parameter estimates exist for a loglinear model that\nequates observed and fitted counts in certain marginal tables. then\nthose marginal tables have uniformly positive counts.\n\n5. if ml estimates exist for a model m, they also exist for any special case\n\nof m.\n\n "}, {"Page_number": 408, "text": "empty cells and sparseness in modeling contingency tables\n\n393\n\n6. for any loglinear model, the ml estimates \u242e are identical for multino-\nmial and independent poisson sampling, and those estimates exist in\nthe same situations.\n\n\u02c6\n\ni\n\ni\n\ni\n\n\u0004\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\nto illustrate, consider the saturated model. by results 2 and 3, when all\nn ) 0, the ml estimate of \u242e is n. by result 4, parameter estimates do not\ni\nexist when any n s 0. model parameter estimates are contrasts of\n4\nlog \u242e ,\u02c6\ni\nand since \u242e s n for the saturated model, the estimates are finite only when\nall n ) 0.\n\n\u02c6\n\nfor unsaturated models, by results 3 and 4 ml estimates exist when all\nn ) 0 and do not exist when any count is zero in the set of sufficient\ni\nmarginal tables. suppose that at least one n s 0 but the sufficient marginal\n.\ncounts are all positive. for hierarchical loglinear models, glonek et al. 1988\nshowed that the positivity of the sufficient counts implies the existence of ml\nestimates if and only if the model is decomposable note 8.2 , which includes\nthe conditional independence models. models having all pairs of variables\nassociated, however, are more complex. for model xy, xz, yz , for in-\nstance, ml estimates exist when only one n s 0 but may not exist when at\nleast two cells are empty. for instance, ml estimates do not exist for table\n\u017e\n9.15, even though all sufficient statistics\nthe two-way marginal totals are\n.\npositive problem 9.47 .\n\nhaberman showed that the supremum of the likelihood function is finite.\nthis motivated him to define extended ml estimators of \u242e. these always\nexist but may equal 0 and, falling on the boundary, need not have the same\nproperties as regular ml estimators see also baker et al. 1985 . a sequence\nof estimates satisfying the model that converges to the extended estimate has\nlog likelihood approaching its supremum. in this extended sense, \u242e s 0 is\nthe ml estimate of \u242e for the saturated model when n s 0, and one can\nhave infinite loglinear parameter estimates.\n\n\u02c6i\n\n.x\n\n\u02c6x y\ninfinite estimates occur among \u242d\ni j\n\nwhen a sufficient marginal count for a factor equals zero, infinite esti-\nmates occur for that term. for instance, when a xy marginal total equals\nzero,\nfor loglinear models such as\n\u017e\nxy, xz, yz , and infinite estimates occur among \u2424 for the effect of x\non y in logit models. sometimes, however, not even infinite estimates exist.\nan example is estimating the log odds ratio when both entries in a row or\ncolumn of a 2 = 2 table equal 0.\n\n\u02c6 x\ni\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\nw\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ni\n\ni\n\ni\n\ntable 9.15 data for which ml estimates do not exist\nfor model xy, xz, yz\n\n(\n\n)\n\na\n\nz:\ny:\n\n1\n\n2\n\nx\n1\n2\nacells containing * may contain any positive numbers.\n\n1\n)\n)\n\n2\n)\n)\n\n1\n0\n)\n\n2\n)\n0\n\n "}, {"Page_number": 409, "text": "\u017e\n\n.\n\n394\n\nbuilding and extending loglinearr logit models\na value of \u2b01 or y\u2b01 for a ml parameter estimate implies that ml fitted\nvalues equal 0 in some cells, and some odds ratio estimates equal \u2b01 or 0. one\npotential indicator is when the iterative fitting process does not converge,\ntypically because an estimate keeps increasing from cycle to cycle. most\nsoftware, however, is fooled after a certain point in the iterative process by\nthe nearly flat likelihood. it reports convergence, but because of the very\nslight curvature of the log likelihood, the estimated standard errors based on\ninverting the information matrix of second partial derivatives are extremely\nlarge and numerically unstable. slight changes in the data then often cause\ndramatic changes in the estimates and their standard errors. a danger with\nsparse data is that one might not realize that a true estimated effect is\ninfinite and, as a consequence, report estimated effects and results of\nstatistical inferences that are invalid and highly unstable.\n\nmany ml analyses are unharmed by empty cells. even when a parameter\nestimate is infinite, this is not fatal to data analysis. the likelihood-ratio\nconfidence interval for the true log odds ratio has one endpoint that is finite.\nfor instance, when n s 0 but other n ) 0 in a 2 = 2 table, log \u242as y\u2b01\nand a confidence interval has form y\u2b01, u for some finite upper bound u.\nwhen the pattern of empty cells forces certain fitted values for a model to\n.\nequal 0, this affects the df for testing model fit haslett 1990 .\n\n\u02c6\n\n11\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\ni j\n\n\u017e\n\n9.8.3 clinical trials example\ntable 9.16 shows results of a clinical trial conducted at five centers. the\npurpose was to compare an active drug to placebo for treating fungal\nlet\ninfections, with a binary\ny s response, x s treatment x s 1 for active drug and x s 0 for placebo ,\n.\nand z s center.\n\nsuccess, failure response. for these data,\n\ncenters 1 and 3 had no successes. thus, the 5 = 2 marginal table relating\nresponse to center, collapsed over treatment, contains zero counts. the last\ntwo columns of table 9.16 show this marginal table. infinite ml estimates\noccur for terms in loglinear or logit models containing the yz association. an\nexample is the logit model\n\n.\n\n\u017e\n\n1\n\n2\n\nlogit p y s 1 x s i, z s k s \u2424x q \u2424 .\n\n\u017e\n\n.\n\n<\n\nz\nk\n\ni\n\n\u0004\n\nz4\nk\n\n\u017e\nwe omit the intercept, so the \u2424 need no constraint; then, these refer to\n.\ncenter effects rather than contrasts between centers and a baseline center.\nthe likelihood function increases continually as \u2424z and \u2424z decrease toward\ny\u2b01; that is, as the logit decreases toward y\u2b01, so the fitted probability of\nsuccess decreases toward the ml estimate of 0 for those centers.\n\nthe counts in the 2 = 2 marginal table relating response to treatment,\nshown in the bottom panel of table 9.16, are all positive. the empty cells in\ntable 9.16 affect the center estimates, but not the treatment estimate, for\nthis logit model. in the limit as the log likelihood increases, the fitted values\nhave a log odds ratio \u2424s 1.55 se s 0.70 . most software reports this, but\n\n\u02c6\n\n\u017e\n\n.\n\n1\n\n3\n\n "}, {"Page_number": 410, "text": "empty cells and sparseness in modeling contingency tables\n\n395\n\ntable 9.16 clinical trial relating treatment to response with xy and\nyz marginal tables\n\na\n\ncenter\n\n1\n\n2\n\n3\n\n4\n\n5\n\ntreatment\nactive drug\nplacebo\nactive drug\nplacebo\nactive drug\nplacebo\nactive drug\nplacebo\nactive drug\nplacebo\n\nresponse\n\nyz marginal\n\nsuccess\n\nfailure\n\nsuccess\n\nfailure\n\n0\n0\n1\n0\n0\n0\n6\n2\n5\n2\n\n5\n9\n12\n10\n7\n5\n3\n6\n9\n12\n\n0\n\n1\n\n0\n\n8\n\n7\n\n14\n\n22\n\n12\n\n9\n\n21\n\nxy\n\nactive drug\nplacebo\n\n12\n4\n\nmarginal\nax, treatment; y, response; z, center.\nsource: data courtesy of diane connell, sandoz pharmaceuticals corporation.\n\n36\n42\n\n\u017e\n\n\u02c6z\n1\n\n\u02c6z\n1\n\n\u02c6z\n3\n\u02c6z\n3\n\ninstead of \u2424 s \u2424 s y\u2b01 reports large numbers with extremely large stan-\ndard errors. for instance, proc genmod in sas reports values of about\ny26 for \u2424 and \u2424 , with standard errors of about 200,000.\nthe treatment estimate \u2424s 1.55 also results from deleting centers 1 and 3\nfrom the analysis. when a center contains responses of only one type, it\nprovides no information about this odds ratio. it does provide information\n.\nabout the size of some other measures, such as the difference of proportions.\nin fact, such tables also make no contribution to standard tests of conditional\n.\nindependence, such as the cochran\u1390mantel\u1390haenszel test section 6.3.2\n.\nand exact test section 6.7.5 .\n\n\u02c6\n\nan alternative strategy in multicenter analyses combines centers of a\nsimilar type. then, if each resulting partial table has responses with both\noutcomes, the inferences use all data. for table 9.16, perhaps centers 1 and\n3 are similar to center 2, since the success rate is very low for that center.\ncombining these three centers and refitting the model to this table and the\ntables for the other two centers yields \u2424s 1.56 se s 0.70 . usually, this\nstrategy produces results similar to deleting the table with no outcomes of a\nparticular type.\n\n\u02c6\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n9.8.4 effect of small samples on x 2 and g 2\nalthough empty cells and sparse tables need not affect parameter estimates\nof interest, they can cause sampling distributions of goodness-of-fit statistics\nto be far from chi-squared. the true sampling distributions converge to\n\n "}, {"Page_number": 411, "text": "i\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\u017e\n\nbuilding and extending loglinearr logit models\n396\nchi-squared as n \u2122 \u2b01, for a fixed number of cells n. the adequacy of the\nchi-squared approximation depends both on n and n.\n\ncochran studied the chi-squared approximation for x 2 in several articles.\nin 1954, he suggested that to test independence with df ) 1, a minimum\nexpected value \u242e f 1 is permissible as long as no more than about 20% of\n\u242e - 5. koehler 1986 , koehler and larntz 1980 , and larntz 1978 showed\nthat x 2 applies with smaller n and more sparse tables than g2. the\ndistribution of g2 is usually poorly approximated by chi-squared when nrn\nis less than 5. depending on the sparseness, p-values based on referring g2\nto a chi-squared distribution can be too large or too small. when most \u242e are\nsmaller than 0.5, treating g2 as chi-squared gives a highly conservative test;\nwhen h is true, reported p-values tend to be much larger than true ones.\nwhen most \u242e are between 0.5 and 4, g2\ntends to be too liberal; the\nreported p-value tends to be too small.\nthe size of nrn that produces adequate approximations for x 2 tends to\ndecrease as n increases koehler and larntz 1980 . however, the approxi-\nmation tends to be poor for sparse tables containing both small and moder-\nately large \u242e haberman 1988 . it is difficult to give a guideline that covers\nall cases. for other discussion, see cressie and read 1989 and lawal\n\u017e\n.\n1984 .\n\n2w\u017e\n\nfor fixed n and n, the chi-squared approximation is better for tests with\nsmaller df. for instance, in testing conditional independence in i = j = k\ntables, g xz, yz\nis closer to\nchi-squared than g xz, yz with df s k i y 1 j y 1 . the ordinal test\nof h : \u2424s 0 with the homogeneous linear-by-linear xy association model\n9.10 has df s 1, and behaves even better.\n\u017e\n\nwith df s i y 1 j y 1\n..\n\nxy, xz, yz\n\n. < \u017e\n2\u017e\n\n.x \u017e\n\n. w\n\n.x\n\n.\u017e\n\n.\u017e\n\n0\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n0\n\ni\n\ni\n\ni\n\n<\n\n<\n\n0\n\n1\n\n0\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2\u017e\n\n2\u017e\n\n9.8.5 model-based tests and sparseness\n.\nfrom 9.3 and 9.4 , the model-based statistics g m m and x m m\n1\ndepend on the data only through the fitted values, and hence only through\nminimal sufficient statistics for the more complex model. these statistics\nhave null distributions converging to chi-squared as the expected values of\nthe minimal sufficient statistics grow. for most loglinear models, these\nsufficient statistics refer to marginal tables. marginal totals are more nearly\nnormally distributed than are single cell counts. thus, g m m and\nx m m converge to their limiting chi-squared distribution more quickly\nthan does g m and x m , which depend also on individual cell counts.\nwhen \u242e are small but the sufficient marginal totals for m are mostly in\nat least the range 5 to 10, the chi-squared approximation is usually adequate\nfor model comparison statistics. haberman 1977a provided theoretical\njustification.\n\n2\u017e\n4\n\n\u02c6i\n\n2\u017e\n\n2\u017e\n\n2\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n\u0004\n\n0\n\n1\n\n0\n\n1\n\n0\n\n0\n\n1\n\n<\n\n<\n\n9.8.6 alternative asymptotics and alternative statistics\nwhen large-sample approximations are inadequate, exact small-sample meth-\nods are an alternative. when they are infeasible,\nit is often possible to\n\n "}, {"Page_number": 412, "text": "empty cells and sparseness in modeling contingency tables\n\n397\n\napproximate exact distributions precisely using monte carlo methods\n\u017ee.g., booth and butler 1999; forster et al. 1996; kim and agresti 1997;\n.\nmehta et al. 1988 .\n\nan alternative approach uses sparse asymptotic approximations that apply\n4\nwhen the number of cells n increases as n increases. for this approach, \u242e\nfixed n, n \u2122 \u2b01 large-sam-\ni\n\u017e\nneed not increase, as they must do in the usual\nple theory. for goodness-of-fit testing of a specified multinomial, koehler\nand larntz 1980 showed that a standardized version of g has an approxi-\nmate normal distribution for very sparse tables. koehler 1986 presented\nlimiting normal distributions for g2 for use in testing models having direct\nml estimates. mccullagh 1986 reviewed ways of handling sparse tables and\npresented an alternative approximation for g . zelterman 1987 gave nor-\nmal approximations for x 2 and proposed an alternative statistic.\n\n2\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n2\n\n.\n\n\u017e\n\n1\n2\n\n9.8.7 adding constants to cells of a contingency table\nempty cells and sparse tables can cause problems with existence of estimates\nfor loglinear model parameters, estimation of odds ratios, performance of\ncomputational algorithms, and asymptotic approximations of chi-squared\nstatistics. however, they need not be problematic. the likelihood can still be\nmaximized, a point estimate of \u2b01 for an effect still usually has a finite lower\nbound for a likelihood-based confidence interval, and one can use small-sam-\nple inferential methods rather than asymptotic ones.\n\none way to obtain finite estimates of all effects and ensure convergence of\nfitting algorithms is to add a small constant to cell counts. some algorithms\nadd\nto each cell, as goodman 1964b, 1970, 1971a recommended for\nsaturated models. an example of the beneficial effect of this for a saturated\nmodel is bias reduction for estimating an odds ratio in a 2 = 2 table gart\n1966; gart and zweiful 1967 . adding\nto each cell before fitting an\nunsaturated model smooths the data too much, however, causing havoc with\nsampling distributions. this operation has too conservative an influence on\nestimated effects and test statistics. the effect is very severe with a large\nnumber of cells.\n\neven for a saturated model, adding\n\nto each cell is not a panacea for all\npurposes. when the ordinary ml estimate of an odds ratio is infinite, the\nestimate after adding\nto each cell is finite, as are the endpoints of any\nconfidence interval. however, it is more sensible to use an upper bound of \u2b01\nfor the odds ratio, since no sample evidence suggests that the odds ratio falls\nbelow any given value.\n\nwhen in doubt about the effect of sparse data, one should perform a\nsensitivity analysis. for example, for each possibly influential observation,\ndelete it or move it to another cell to see how results vary with small\n.\nperturbations to the data. influence diagnostics for glms williams 1987\nare also useful for this purpose. often, some associations are not affected by\nempty cells and give stable results for the various analyses, whereas others\n\n1\n2\n\n1\n2\n\n1\n2\n\n.\n\n\u017e\n\n\u017e\n\n "}, {"Page_number": 413, "text": "398\n\nbuilding and extending loglinearr logit models\n\nthat are affected are highly unstable. use caution in making conclusions\nabout an association if small changes in the data are influential.\n\nlater chapters show ways to smooth data in a less ad hoc manner than\nadding arbitrary constants to cells. these include random effects models\n.\n\u017e\nsection 12.3 and bayesian methods section 15.2 .\n\n\u017e\n\n.\n\nnotes\n\nsection 9.1: association graphs and collapsibility\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u00a8\n\n\u017e\n.\n\n.\n9.1. darroch et al. 1980 defined a class of graphical models that contains the family of\n\u017e\ndecomposable models\nsee note 8.2 . for expositions on graphical models and their\nrelevant independence graphs, which show the conditional independence structure, see\n.\nalso anderson and bockenholt 2000 , edwards 2000 , edwards and kreiner 1983 ,\n.\nkreiner 1998 , lauritzen 1996 , and whittaker 1990 . whittaker 1990, sec. 12.5\nsummarized connections with various definitions of collapsibility.\n.\n\n9.2 for i = j = 2 tables, the collapsibility conditions section 9.1.2 are necessary as well\nas sufficient simpson 1951; whittemore 1978 . for i = j = k tables, ducharme and\nlepage 1986 showed the conditions are necessary and sufficient for the odds ratios to\nremain the same no matter how the levels of z are pooled i.e., no matter how z is\n.\npartially collapsed .\n.\n\ndarroch 1962 defined a perfect table as one for which for all i, j, k,\n\n\u017e\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u00fd\n\ni\n\n\u00fd\n\nk\n\n\u2432 \u2432\n\ni jq iqk\n\u2432\n\niqq\n\ns \u2432 \u2432 ,\nqjq qqk\n\n\u00fd\n\nj\n\n\u2432 \u2432\niqk qj k s \u2432 \u2432 .\niqq qjq\n\u2432qqk\n\n\u2432 \u2432\n\ni jq\n\nqj k\n\u2432\n\nqjq\n\ns \u2432 \u2432 ,\niqq qqk\n\nfor perfect tables, homogeneous association implies that\n\n\u0004\n\n\u2432 s \u2432 \u2432 \u2432 r\u2432 \u2432 \u2432\n\ni jq iqk qj k\n\niqq qjq qqk\n\ni jk\n\n4\n\n.\nand conditional odds ratios are identical to marginal odds ratios. whittemore 1978\nused perfect tables to illustrate that for i = j = k tables with k ) 2, conditional and\nmarginal odds ratios can be identical even when no pair of variables is conditionally\n.\nindependent. see also davis 1986b .\n\n\u017e\n\n\u017e\n\nsuppose that the difference of proportions or relative risk, computed for a binary\nresponse y and predictor x, is the same at every level of z. if z is independent of x\nin the marginal xz table or if z is conditionally independent of y given x, the\nmeasure has the same value in the marginal xy table shapiro 1982 . thus, for\nfactorial designs with the same number of observations at each combination of levels,\n.\nthe difference of proportions and relative risk are collapsible. see also wermuth 1987 .\n\n\u017e\n\n.\n\n\u017e\n\nsection 9.2: model selection and comparison\n\n.\n\n9.3. articles on loglinear model selection include aitkin 1979, 1980 , benedetti and brown\n\u017e\n1978 , brown 1976 , goodman 1970, 1971a , wermuth 1976 , and whittaker and\naitkin 1978 . when a certain model holds, g rdf has an asymptotic mean of 1.\ngoodman 1971a recommended this index for comparing fits. smaller values represent\nbetter fits.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n2\n\n\u017e\n\n.\n\n "}, {"Page_number": 414, "text": "notes\n\n399\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n9.4. kullback et al. 1962 and lancaster 1951 were among the first to partition chi-squared\nstatistics in multiway tables. goodman 1970 and plackett 1962 noted difficulties with\ntheir approaches. when observations have distribution in the natural exponential\nfamily, simon 1973 showed g m m s 2\u00fd \u242e log \u242e r\u242e whenever models are\nlinear in the natural parameters. see lang 1996b for partitionings for more complex\nmodels.\n\n\u02c6\n1i\n.\n\n2\u017e\n\n\u02c6\n\n\u02c6\n\n0 i\n\n1i\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n0\n\n1\n\ni\n\n<\n\nsection 9.4: modeling ordinal associations\n\n\u017e\n\n.\n\n9.5. goodman 1979a stimulated research on loglinear models for ordinal data. his work\nextended haberman 1974b , who expressed the \u242d association term with an expan-\nsion in orthogonal polynomials. for more general ordinal models for multiway tables,\n.\nsee agresti 1984 , becker 1989a , becker and clogg 1989 , and goodman 1986 .\n\nx y\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nsection 9.6: association models, correlation models, and correspondence analysis\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n\u00a8\n.\n\n9.6. early articles on the rc model include goodman 1979a, 1981a, b and andersen\n\u017e\n1980, pp. 210\u1390216 , apparently partly motivated by earlier work of g. rasch see\nandersen 1995 . anderson and bockenholt 2000 , becker 1989a, b, 1990 , becker and\nclogg 1989 , chuang et al. 1985 , and goodman 1985, 1986, 1996 discussed general-\nizations for multiway tables. anderson 1984 discussed a related model. anderson and\nvermunt 2000 showed that rc and related association models arise when observed\nvariables are conditionally independent given a latent variable that is conditionally\nnormal, given the observed variables. their work generalizes results in lauritzen and\nwermuth 1989 and discussion by whittaker of van der heijden et al. 1989 . see also\nde falguerolles et al. 1995 . clogg and shihadeh 1994 surveyed association models\nand related correlation models.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n.\n\n9.7. kendall and stuart 1979, chap. 33 surveyed basic canonical correlation methods for\ncontingency tables. see also williams 1952 , who discussed earlier work by r. a.\nfisher and others. karl pearson often analyzed tables by assuming an underlying\nbivariate normal distribution section 16.1 . for estimating that distribution\u2019s correla-\ntion, see becker 1989b , goodman 1981b , kendall and stuart 1979, chaps. 26 and\n33 , lancaster 1969, chap. x , the pearson 1904 tetrachoric correlation for 2 = 2\ntables, and the lancaster and hamdan 1964 polychoric correlation for i = j tables.\n9.8. correspondence analysis gained popularity in france under the influence of benzecri\u00b4\n\u017e\nsee, e.g., 1973 . goodman 1996 attributed its origins to h. o. hartley, publishing\nunder his original german name hirschfeld, 1935 . greenacre 1993 related it to the\n.\nsingular value decomposition of a matrix. for other discussion, see escoufier 1982 ,\nfriendly 2000, chap. 5 , goodman 1986, 1996, 2000 , michailidis and de leeuw\n.\n\u017e\n.\n1998 , van der heijden and de leeuw 1985 , and van der heijden et al. 1989 .\ngabriel 1971 discussed related work on biplots.\n\n.\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nsection 9.7: poisson regression for rates\n\n.\n\n9.9. another application using offsets is table standardization section 8.7.4 . for analyses\nof rate data, see breslow and day 1987, sec. 4.5 , freeman and holford 1980 , frome\n\u017e\n1983 , and hoem 1987 . articles dealing with grouped survival data, particularly\n.\nloglinear and logit models for survival probabilities,\ninclude aranda-ordaz 1983 ,\nlarson 1984 , prentice and gloeckler 1978 , schluchter and jackson 1989 , stokes et\nal. 2000, chap. 17 , and thompson 1977 . aitkin and clayton 1980 discussed\nexponential survival models and also presented similar models having hazard functions\n\n\u017e\n\u017e\n\n.\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 415, "text": "400\n\nbuilding and extending loglinearr logit models\n\nfor weibull or extreme-value survival distributions. log likelihood 9.19 actually\napplies only for noninformati\u00aee censoring mechanisms. it does not make sense if\nsubjects tend to withdraw from the study because of factors related to it, perhaps\nbecause of health effects related to one of the treatments.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n9.10. lindsey and mersch 1992 showed a clever way to use loglinear models to fit\nexponential family distributions f y;\u242a of form 4.14 with \u243e known. one breaks the\ny y \u232c r2, y q \u232c r2 . counts in those intervals follow\nresponse scale into intervals\na multinomial with probabilities approximated by f y , \u242a \u232c . the log expected count\napproximations are linear in \u242a with an offset.\n\nk\n\u0004 \u017e\n\n.\n.4\n\n\u0004\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n4\n\nk\n\nk\n\nk\n\nk\n\nk\n\nproblems\n\napplications\n\n9.1 use odds ratios in table 8.3 to illustrate the collapsibility conditions.\na. for a, c, m , all conditional odds ratios equal 1.0. explain why all\n\n.\n\n\u017e\n\nreported marginal odds ratios equal 1.0.\n\n\u017e .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u02c6 acq\n\nb. for ac, m , explain why i all conditional odds ratios are the\n\nsame as the marginal odds ratios, and ii all \u242e s n\n\n\u017e .\n\n\u017e .\nc. for am, cm , explain why i\n\nthe ac conditional odds ratios of\n1.0 need not be the same as the ac marginal odds ratio,\nthe\nam and cm conditional odds ratios are the same as the marginal\nodds ratios, and iii all \u242e s n\naqm\n\u017e .\n\nd. for ac, am, cm , explain why i no conditional odds ratios need\nthe fitted\n\n\u017e .\nbe the same as the related marginal odds ratios, and ii\nmarginal odds ratios must equal the sample marginal odds ratios.\n\nand \u242e s n\n\n\u017e .\nii\n\nqc m\n\nqc m\n\n.\n.\n\naqm\n\nacq\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n9.2 table 9.17 summarizes a study with variables age of mother a ,\n\u017e .\nlength of gestation g in days, infant survival\ni , and number of\ncigarettes smoked per day during the prenatal period s . treat g and\ni as response variables and a and s as explanatory.\na. explain why a loglinear model should include the \u242das term.\nb. fit\n\nthe models agis , agi, ais, ags, gis , ag, ai, as,\ngi, gs, is , and as, g, i . identify a subset of models nested\nbetween two of these that may fit well. select one such model.\n\n\u017e\n.\n\n\u017e\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e .\nc. use i\n\nforward selection, and ii backward elimination to build a\nmodel. compare the results of the strategies, and interpret the\nmodels chosen.\n\n\u017e .\n\n\u017e\n\n.\n\n.\n9.3 refer to table 2.13. consider the nested set dvp , dp, vp, dv ,\n\u017e\nvp, dv , p, dv , d, v, p . partition chi-squared to compare the\nfour pairs, ensuring that the overall type i error probability for the four\ncomparisons does not exceed \u2423s 0.10. which model would you select,\nusing a backward comparison starting with dvp ? show that the final\n\n\u0004\u017e\n\n.4\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n "}, {"Page_number": 416, "text": "problems\n\ntable 9.17 data for problem 9.2\n\nage\n- 30\n\n30 q\n\nsmoking\n\n- 5\n\n5 q\n\n- 5\n\n5 q\n\n401\n\ninfant survival\n\ngestation\nf 260\n) 260\nf 260\n) 260\nf 260\n) 260\nf 260\n) 260\n\nno\n50\n24\n9\n6\n41\n14\n4\n1\n\nyes\n315\n4012\n40\n459\n147\n1594\n11\n124\n\u017e\n\n.\nsource: n. wermuth, pp. 279\u1390295 in proc. 9th international biometrics conference, vol. 1 1976 .\nreprinted with permission from the biometric society.\n\nmodel selected depends on the choice of nested set, by repeating the\n.\nanalysis with dp, vp, dv , dp, dv , p, dv , d, v, p .\n\n. \u017e\n\n. \u017e\n\n. \u017e\n\n\u017e\n\n9.4 consider the loglinear model selection for table 6.3.\n\na. why is it not sensible to consider models omitting the \u242dg m term?\nb. using forward selection starting with gm, e, p , show that model\n\n\u017e\ngm, gp, eg, emp seems reasonable.\n\n.\n\n\u017e\n\n.\n\nc. using backward elimination,\n\nshow that gm, gp, emp\n\n\u017e\n\n.\n\nor\n\n\u017e\n\ngm, gp, eg, emp seems reasonable.\n\n.\n\nd. the emp interaction seems vital. to describe it, show that the\neffect of extramarital sex on divorce is greater for subjects who had\nno premarital sex.\n\n.\ne. use residuals to describe the lack of fit of model gm, emp .\n\n\u017e\n\n.\n\n\u017e\n\n9.5 for model ac, am, cm with table 8.3, the standardized pearson\nresidual in each cell equals \"0.63. interpret, and explain why each one\nhas the same absolute value. by contrast, model am, cm has stan-\ndardized pearson residual \"3.70 in each cell where m s yes e.g.,\nq3.70 when a s c s yes and \"12.80 in each cell where m s no\ne.g., q12.80 when a s c s yes . interpret.\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n9.6 refer to table 8.8. conduct a residual analysis with the model of no\n\nthree-factor interaction to describe the nature of the interaction.\n\n9.7 perform a residual analysis for the independence model with table\n3.2. explain why it suggests that the linear-by-linear association model\nmay fit better. fit it, compare to the independence model, and inter-\npret.\n\n "}, {"Page_number": 417, "text": "402\n\nbuilding and extending loglinearr logit models\n\n9.8 refer to problem 9.7.\n\na. using standardized scores, find \u2424. comment on the strength of\n\n\u02c6\n\nassociation.\n\nb. fit a model in which job satisfaction scores are parameters. inter-\npret the estimated scores, and compare the fit to the l = l model.\n\n9.9 refer to table 9.3.\n\na. for the linear-by-linear association model, construct a 95% confi-\ndence interval for the odds ratio using the four corner cells. inter-\npret.\n\nb. fit the column effects model. compare estimated column scores to\nthe equal-interval scores in part a . test that the true column\nscores are equal-interval, given that the model holds. interpret.\nconstruct a 95% confidence interval for the odds ratio using the\n\u017e .\nfour corner cells. compare to part a .\n\n\u017e .\n\n9.10 a weak local association may be substantively important for nonlocal\ncategories. illustrate with the l = l model for table 9.9, showing how\nthe estimated odd ratio for the four corner cells compares to the\nestimated local odds ratio.\n\n9.11 refer to table 7.8. fit the homogeneous linear-by-linear association\nmodel, and interpret. test conditional independence between income\n\u017e\n\u017e .\ni and job satisfaction s , controlling for gender g , using a that\n\u017e\nmodel, and b model\nis, ig, sg . explain why the results are so\ndifferent.\n\n\u017e .\n\n\u017e .\n\n.\n\n.\n\n\u017e\n\n.\n\n9.12 fit the rc model to table 9.3. interpret the estimated scores. does it\n\nfit better than the uniform association model?\n\n9.13 replicate the results in section 9.6 for the correlation and correspon-\n\ndence models with table 9.9.\n\n9.14 one hundred leukemia patients were randomly assigned to two treat-\nments. during the study, 10 subjects on treatment a died and 18\nsubjects on treatment b died. the total time at risk was 170.4 years for\ntreatment a and 147.3 years for treatment b. test whether the two\ntreatments have the same death rates. compare the rates with a\nconfidence interval.\n\n9.15 for table 9.11, fit a model in which death rate depends only on age.\n\ninterpret the age effect.\n\n9.16 consider model 9.18 . what is the effect on the model parameter\nestimates, their standard errors, and the goodness-of-fit statistics when\n\u017e .a the times at risk are doubled, but the numbers of deaths stay the\n\n\u017e\n\n.\n\n "}, {"Page_number": 418, "text": "problems\n\n403\n\n\u017e .\n\nsame; b the times at risk stay the same, but the numbers of deaths\ndouble; and c the times at risk and the numbers of deaths both\ndouble.\n\n\u017e .\n\n9.17 consider table 9.13. explain how one could analyze whether the\n\nhazard depends on time.\n\n\u017e\n\n.\n\n9.18 an article by w. a. ray et al. amer. j. epidemiol. 132: 873\u1390884,\n1992 dealt with motor vehicle accident rates for 16,262 subjects aged\n65\u139084 years, with data on each for up to 4 years. in 17.3 thousand\nyears of observation, the women had 175 accidents in which an injury\noccurred. in 21.4 thousand years, men had 320 injurious accidents.\na. find a 95% confidence interval for the true overall rate of injurious\n\naccidents.\n\nb. using a model, compare the rates for men and women.\n\n\u017e\n\n9.19 a table at the text\u2019s web site www.stat.ufl.edur;aarcdarcda.html\n.\n\u017e\nin millions and the number of\nshows the number of train miles\ncollisions involving british rail passenger trains between 1970 and\n1984. a poisson model assuming a constant log rate \u2423 over the 14-year\nperiod has \u2423s y4.177 se s 0.1325\nand x s 14.8 df s 13 .\n.\n.\ninterpret.\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n2\n\n9.20 table 9.18 lists total attendance in thousands and the total number of\narrests in the 1987\u13901988 season for soccer teams in the second\ndivision of the british football league. let y s number of arrests for a\nteam, and let t s total attendance. explain why the model e y s \u242et\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ntable 9.18 data for problem 9.20\n\nteam\naston villa\nbradford city\nleeds united\nbournemouth\nwest brom\nhudderfield\nmiddlesbro\nbirmingham\nipswich town\nleicester city\nblackburn\ncrystal palace\n\nattendance\n\u017e\n.\nthousands\n\narrests\n\n404\n286\n443\n169\n222\n150\n321\n189\n258\n223\n211\n215\n\u017e\n\n308\n197\n184\n149\n132\n126\n110\n101\n99\n81\n79\n78\n\nteam\nshrewsbury\nswindon town\nsheffield utd.\nstoke city\nbarnsley\nmillwall\nhull city\nmanchester city\nplymouth\nreading\noldham\n\nattendance\n\u017e\n.\nthousands\n\narrests\n\n108\n210\n224\n211\n168\n185\n158\n429\n226\n150\n148\n\n68\n67\n60\n57\n55\n44\n38\n35\n29\n20\n19\n\nsource: the independent london , dec. 21, 1988. thanks to p. m. e. altham for showing me\nthese data.\n\n.\n\n "}, {"Page_number": 419, "text": "404\n\nbuilding and extending loglinearr logit models\n\nmight be plausible. assuming poisson sampling, fit it and interpret.\nplot arrests against attendance, and overlay the prediction equation.\nuse residuals to identify teams that had arrest counts much different\nthan expected.\n\ntable 9.19 data for problem 9.21\n\nperson-years\n\ncoronary deaths\n\nage\n35\u139044\n45\u139054\n55\u139064\n65\u139074\n75\u139084\n\nnonsmokers\n\n18,793\n10,673\n5710\n2585\n1462\n\nsmokers\n52,407\n43,248\n28,612\n12,663\n5317\n\nnonsmokers\n\nsmokers\n\n2\n12\n28\n28\n31\n\n32\n104\n206\n186\n102\n\nsource: r. doll and a. b. hill, natl. cancer inst. monogr. 19: 205\u1390268 1966 . see also n. r.\nbreslow in a celebration of statistics, ed. a. c. atkinson and s. e. fienberg, new york:\n.\nspringer-verlag, 1985 .\n\n\u017e\n\n\u017e\n\n.\n\n9.21 table 9.19 is based on a study with british doctors.\n\na. for each age, find the sample coronary death rates per 1000\nperson-years for nonsmokers and smokers. to compare them, take\ntheir ratio and describe its dependence on age.\n\nb. fit a main-effects model for the log rates having four parameters\nfor age and one for smoking. in discussing lack of fit, show that this\nmodel assumes a constant ratio of nonsmokers\u2019 to smokers\u2019 coro-\nnary death rates over age.\n\n\u017e .\n\nc. from part a , explain why it is sensible to add a quantitative\ninteraction of age and smoking. for this model, show that the log\nratio of coronary death rates changes linearly with age. assign\nscores to age, fit the model, and interpret.\n\n9.22 analyze table 9.9 using ordinal logit models. interpret, and discuss\n\nadvantagesrdisadvantages compared to loglinear analyses.\n\n9.23 refer to problem 8.6. analyze these data, using methods of this\n\nchapter.\n\ntheory and methods\n\n9.24 in a 2 = 2 = k table, the true xy conditional odds ratios are identi-\ncal, but different from the xy marginal odds ratio. is there three-fac-\ntor interaction? is z conditionally independent of x or y ? explain.\n\n "}, {"Page_number": 420, "text": "problems\n\n405\n\n9.25 consider loglinear model wx, xy, yz . explain why w and z are\nindependent given x alone or given y alone or given both x and y.\nwhen are w and y conditionally independent? when are x and z\nconditionally independent?\n\n\u017e\n\n.\n\n9.26 suppose that loglinear model xy, xz holds.\n\n\u017e\n\n.\n\ni jq\n\ni jq\n\na. find \u242e and log \u242e . show the loglinear model for the xy\nmarginal table has the same association parameters as \u242d\nin\ni j\n\u017e\nxy, xz . deduce that odds ratios are the same in the xy marginal\ntable as in the partial tables. using an analogous result for model\n\u017e\nxy, yz , deduce the collapsibility conditions in section 9.1.2.\n\n\u0004 x y 4\n\n.\n\n.\n\nb. calculate log \u242e for model xy, xz, yz , and explain why mar-\n\ni jq\n\nginal associations need not equal conditional associations.\n\n\u017e\n\n.\n\n9.27 for a four-way table, is the wx conditional association the same as the\nwx marginal association for the loglinear model a wx, xyz ? and\n\u017e . \u017e\nb wx, wz, xy, yz ? why?\n\n\u017e . \u017e\n\n.\n\n.\n\n9.28 loglinear model m is a special case of loglinear model m .\n1\n\n0\n\n\u017e\n\na. explain why the fitted values for the two models are identical in the\n\nsufficient marginal distributions for m .0\n4\n\u0004\n\u02c6i\nb. haberman 1974a showed that when \u242e satisfy any model that is a\nspecial case of m , \u00fd \u242e log \u242e s \u00fd \u242e log \u242e. thus, \u242e is the\n\u02c6\n\u02c6\n\u02c6\n0 \u0004\n1i\ni\n4\n\u02c6 1\northogonal projection of \u242e onto the linear manifold of\nlog \u242e\nshow that g m y g m s\n2\u017e\nsatisfying m . using this,\n1\n2\u00fd \u242e log \u242e r\u242e .\n.\n\n2\u017e\n\n\u02c6\n\n\u02c6\n\n0 i\n\n.\n\n\u017e\n\n.\n\n.\n\n0\n\n0\n\n0\n\ni\n\ni\n\ni\n\n\u02c6\n\n1i\n\n\u02c6\n\n0 i\n\n\u02c6\n\n1i\n\ni\n\n9.29 refer to section 9.2.4. show that g m m\n\nequals g for inde-\npendence in the 2 = 2 table comparing columns 1 through j y 1 with\ncolumn j.\n\njy1\n\nj\n\n2\n\n2\u017e\n\n<\n\n.\n\n9.30 for t categorical variables x , . . . , x , explain why:\na. g x , x , . . . , x s g x , x q g x x , x\n\n2\u017e\n\n2\u017e\n\n.\n\n.\n\nt\n\n1\n\n1\n\n2\n\n.\n\n3\n\nt\n\nq \u2b48\u2b48\u2b48 qg x x \u2b48\u2b48\u2b48 x\n\n1\n\n2\n2\u017e\n1\nb. g x \u2b48\u2b48\u2b48 x\nty1\n2\u017e\n1\n\n2\u017e\n\n1\n\nq \u2b48\u2b48\u2b48 qg x x \u2b48\u2b48\u2b48 x\n\nt\n\n2\n\n2\n\n1\n\n2\u017e\n.\n, x .\nty1\n2\u017e\nty1\n\nt\n\n1\n\n1\n\n2\n\nt\n\n2\n\n, x x \u2b48\u2b48\u2b48 x\n\n, x s g x , x q g x x , x x\n\n.\n\n.\n\n.\n\n2\n\n1\n\n2\u017e\nty2\n\n1\nt\n.\nx .\n\nt\n\n9.31 for i = 2 contingency tables, explain why the linear-by-linear associa-\n\n.\ntion model is equivalent to the linear logit model 5.5 .\n\n\u017e\n\n9.32 consider the l = l model 9.6 with \u00ae s j\n\nj\nexplain why \u2424 is halved but \u242e , \u242a , and g are unchanged.\n\n\u02c6\n\nreplaced by \u00ae s 2 j .\n4\n\n\u017e\n\u0004\n\n\u0004\n\n4\n\n\u0004\n\n\u0004\n\n4\n\n2\n\nj\n\n.\n4\n\u02c6i j\n\n\u02c6\ni j\n\n "}, {"Page_number": 421, "text": "406\n\nbuilding and extending loglinearr logit models\nto be positi\u00aeely likelihood-ratio\n9.33 lehmann\ng\nsatisfies\ndependent\n\u017e\n. \u017e\nf x , y whenever x - x and y - y . then, the conditional\nf x , y\ndistribution of y x stochastically increases as x y increases\n\u017e\n.\ngoodman 1981a .\na. for the l = l model, show that the conditional distributions of y\n\n.\nx, y\njoint density\n2\n\n.\n\u017e\n1966\nif\ntheir\n.\n1\n2\n\n. \u017e\nf x , y\n\n\u017e\nf x , y\n\ndefined\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n1\n\n1\n\n2\n\n2\n\n1\n\n2\n\n1\n\n1\n\n2\n\nand of x are stochastically ordered. what is its nature if \u2424) 0?\n\n\u017e\n\n.\n\nb. in row effects model 9.8 , if \u242e ) \u242e , show that the conditional\ndistribution of y is stochastically higher in row i than in row h.\nexplain why \u242e s \u2b48\u2b48\u2b48 s \u242e is equivalent to the equality of the i\nconditional distributions within rows.\n\nh\n\n1\n\ni\n\ni\n\n\u017e\n\n.\n\n9.34 yule 1906 defined a table to be isotropic if an ordering of rows and of\ncolumns exists such that the local log odds ratios are all nonnegative\nw\n.x\nsee also goodman 1981a .\na. show that a table is isotropic if it satisfies\n\n\u017e .\ni\n\n\u017e\n\n\u017e .\nii\n\nthe row effects model, and iii\n\nthe linear-by-linear\nthe rc\n\n\u017e\n\n.\n\nassociation model,\nmodel.\n\nb. explain why a table that is isotropic for a certain ordering is still\n\nisotropic when adjacent rows or columns are combined.\n\n9.35 consider the log likelihood for the linear-by-linear association model.\na. differentiating with respect to \u2424 and evaluating at \u2424s 0 and null\nestimates of parameters, show that the score function is propor-\ntional to\n\n\u00fd \u00fd i\n\nu \u00ae p y p p\niq qj\n\n\u017e\n\ni j\n\nj\n\n.\n\n.\n\ni\n\nj\n\nb. use the delta method to show that its null se is\n\n\u00bd\n\n\u00fd\n\nu p y u p\niq\n\n\u00fd\n\niq\n\n2\ni\n\ni\n\n\u017e\n\n2\n\n.\n\n\u00fd\n\n\u00ae p y\n2\nj qj\n\n\u017e\n\n\u00fd\n\n\u00ae p\nj qj\n\n2\n\n.\n\n5\n\nn\n\n1r2\n\n.\n\nc. construct a score statistic for testing independence. show that it is\nessentially the correlation test 3.15 . hirotsu 1982 discussed a\nfamily of score tests for ordered alternatives.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nw\n\nx\n\n\u017e\n\n9.36 given the parenthetical result in problem 7.33, show that if cumulative\nlogit model 7.24 holds and \u2424 is small, the linear-by-linear association\nmodel should fit well with row scores\nand \u2018\u2018ridit\u2019\u2019 column scores\n\u00ae s p y f j y 1 q p y f j r2 , with its \u2424 parameter about twice\n\u0004\n.\n.\n\u2424 for model 7.24 .\n\n.x\n\nx\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\nw\n\n\u0004\n\n4\n\n4\n\ni\n\nj\n\n<\n\n<\n\n "}, {"Page_number": 422, "text": "problems\n\n407\n\n.\n9.37 consider the row effects model 9.8 .\n\n\u017e\n\na. show that no loss of generality occurs in letting \u242dx s \u242dy s \u242e s 0.\n, and \u00fd \u00ae n ,\nb. show that minimal sufficient statistics are n\n\ni\n\u0004\n, n\ni s 1, . . . , i , and derive the likelihood equations.\n\niq\n\nqj\n\n\u0004\n\n4\n\n4\n\n\u0004\n\n4\n\ni j\n\nj\n\ni\n\nj\n\nj\n\n9.38 show that the column effects model corresponds to a baseline-category\nlogit model for y that is linear in scores for x, with slope depending\non the paired response categories.\n\n.\n9.39 refer to the homogeneous linear-by-linear association model 9.10 .\n\n\u017e\n\na. show that the likelihood equations are, for all i, j, and k,\n\n\u242e s n\n\u02c6\niqk\n\niqk\n\n, \u242e s n\n\nqj k\n\n\u02c6\n\nqj k\n\n,\n\n\u00fd \u00fd\n\nu \u00ae \u242e s\n\ni jq\n\n\u02c6\n\ni\n\nj\n\n\u00fd \u00fd\n\nu \u00ae n\n\ni\n\nj\n\n.\n\ni jq\n\ni\n\nj\n\ni\n\nj\n\n\u017e\n\nb. show that residual df s k i y 1 j y 1 y 1.\nc. when i s j s 2, explain why it is equivalent to xy, xz, yz .\n.\nd. show how the last likelihood equation above changes for heteroge-\nneous linear-by-linear xy association 9.11 . explain why, in each\nstratum, the fitted xy correlation equals the sample correlation.\n\n.\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n9.40 when model xy, xz, yz is inadequate and variables are ordinal,\nuseful models are nested between it and xyz . for ordered scores\nu , \u00ae , and w , consider\n\u0004\n4\nlog \u242e s \u242dq \u242dx q \u242dy q \u242dz q \u242dx y q \u242dx z q \u242dy z q \u2424u \u00ae w .\n\n9.22\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u017e\n\n.\n\nk\n\ni\n\nj\n\nj\n\nk\n\ni j\n\ni\n\nj\n\nk\n\ni jk\n\nik\n\njk\n\ni\n\na. define \u242a s \u242a\n\nr\u242a . for\nunit-spaced scores, show that log \u242a s \u2424. goodman 1979a called\nthis the uniform interaction model.\n\nr\u242a s \u242a\n\nr\u242a s \u242a\n\n\u017eiq1. jk\n\u017e\n\ni\u017e jq1. k\n\ni j\u017e kq1.\n\n\u017ei. jk\n.\n\ni\u017e j.k\n\ni j\u017e k.\n\ni jk\n\ni jk\n\nb. show that log odds ratios for any two variables change linearly\n\nacross levels of the third variable.\n\nc. show that\n\n\u017e\n\nxy, xz, yz plus\n\nthe likelihood equations are those for model\n.\n\n\u00fd \u00fd \u00fd\n\nu \u00ae w \u242e s\n\n\u02c6\n\ni jk\n\nk\n\ni\n\nj\n\n\u00fd \u00fd \u00fd\n\nu \u00ae w n .\n\ni jk\n\nk\n\ni\n\nj\n\ni\n\nj\n\nk\n\ni\n\nj\n\nk\n\n.\nd. explain why model 9.12 is a special case of model 9.22 .\n\n\u017e\n\n.\n\n\u017e\n\n9.41 construct a model having general xz and yz associations, but row\n\u017e .\neffects for the xy association that are a homogeneous, and b\nheterogeneous across levels of z. interpret.\n\n\u017e .\n\n "}, {"Page_number": 423, "text": "408\n\nbuilding and extending loglinearr logit models\n\n9.42 explain why the rc model requires scale constraints for the scores.\nshow the residual df s i y 2 j y 2 . find and interpret the likeli-\nhood equations. explain why the fit is invariant to category orderings.\n\n.\u017e\n\n\u017e\n\n.\n\n.\n9.43 refer to correlation model 9.16 goodman 1985, 1986 .\n\n. \u017e\n\n\u017e\n\nj\n\ni j\n\n.\n\nthis model holds,\n\u017e\nj\n\na. show that \u242d is the correlation between the scores.\nb. if\n\nshow that \u00fd \u242e \u2432 r\u2432 s \u242d\u242f and\n\n\u00fd \u242f \u2432 r\u2432 s \u242d\u242e. interpret.\nc. with \u242dclose to zero, show that log \u2432 has form \u2425 q \u2426 q \u242d\u242e\u242f q\no \u242d , where o \u242dr\u242d\u2122 0 as \u242d\u2122 0. thus, when the association is\n\u017e .\nweak, the correlation model is similar to the linear-by-linear associ-\nation model with \u2424s \u242d and scores u s \u242e and \u00ae s \u242f .\n4\n\n\u017e .\n\nqj\n\niq\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u0004\n\n4\n\n\u0004\n\ni j\n\ni j\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nj\n\nj\n\ni\n\ni\n\nj\n\nj\n\n\u017e\n\n9.44 for the general canonical correlation model, show that \u00fd\u242d2 s\n\u00fd \u00fd \u2432 y \u2432 \u2432 r\u2432 \u2432 . thus, the squared correlations partition\na dependence measure that is the noncentrality 6.8 of x for the\nindependence model with n s 1. goodman 1986 stated other parti-\ntionings.\n\niq qj\n\niq qj\n\n\u017e\n.\n\n.2\n\n.\n\n\u017e\n\nx\n\nw\n\ni j\n\n2\n\nk\n\ni\n\nj\n\n\u0004\n9.45 refer to model 9.18 . given the times at risk t\n\n.\n\n4\n\ni j\n\n, show that sufficient\n\n\u0004\nstatistics are n\n\n4\n\nand n\n\n\u0004\n\n4\n\n.\n\nqj\n\n\u017e\niq\n\n9.46 refer to section 9.7.3. let t s \u00fdt and w s \u00fdw . suppose that\nsurvival times have a negative exponential distribution with parameter\n\u242d.\na. using log likelihood 9.19 , show that \u242ds wrt.\nb. conditional on t, show that w has a poisson distribution with\n\nmean t\u242d. using the poisson likelihood, show that \u242ds wrt.\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\ni\n\ni\n\n9.47 show that ml estimates do not exist for table 9.15. hint: haberman\n1973b, 1974a, p. 398 : if \u242e s c ) 0, then marginal constraints the\n\u017e\nmodel satisfy imply that \u242e s yc.\n\n\u02c6111\n\u02c6 222\n\n.\n\nx\n\nw\n\n9.48 for a loglinear model, explain heuristically why the ml estimate of a\nparameter is infinite when its sufficient statistic takes its maximum or\nminimum possible value, for given values of other sufficient statistics.\n\n "}, {"Page_number": 424, "text": "c h a p t e r 1 0\n\nmodels for matched pairs\n\nwe next introduce methods for comparing categorical responses for two\nsamples when each observation in one sample pairs with an observation in\nthe other. such matched-pairs data commonly occur in studies with repeated\nmeasurement of subjects, such as longitudinal studies that observe subjects\nover time. because of the matching, the responses in the two samples are\nstatistically dependent. this is the first of four chapters on special methods\nfor handling such dependence.\n\ntable 10.1 illustrates matched-pairs data. for a poll of a random sample\nof 1600 voting-age british citizens, 944 indicated approval of the prime\nminister\u2019s performance in office. six months later, of these same 1600 people,\n880 indicated approval. the two cells with identical row and column response\nform the main diagonal of the table. these subjects had the same opinion at\nboth surveys. they compose most of the sample, since relatively few people\nchanged opinion. a strong association exists between opinions six months\napart, the sample odds ratio being 794 = 570 r 150 = 86 s 35.1.\n\nfor matched pairs with a categorical response, a two-way contingency\ntable with the same row and column categories summarizes the data. the\ntable is square. in this chapter we present analyses of square tables. in\nsection 10.1 we describe methods for comparing proportions with a binary\nresponse. in section 10.2 we discuss logistic regression analyses of such data.\nfor multicategory responses, section 10.3 covers nominal and ordinal logit\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ntable 10.1 rating of performance of prime minister\n\nfirst\nsurvey\napprove\ndisapprove\n\ntotal\n\nsecond survey\n\napprove\n\ndisapprove\n\n794\n86\n880\n\n150\n570\n720\n\ntotal\n944\n656\n1600\n\n409\n\n "}, {"Page_number": 425, "text": "410\n\nmodels for matched pairs\n\nmodels for comparing the response distributions. in section 10.4 we intro-\nduce loglinear models for square tables. in sections 10.5 and 10.6 we discuss\ntwo matched-pairs applications for which models for square tables are useful:\nanalyzing agreement between two observers who rate a common set of\nsubjects, and evaluating preferences of treatments based on their pairwise\nevaluation.\n\nsection 10.7 extends the models of sections 10.2 through 10.4 to multiway\ntables that result from matched sets of observations. in chapter 11 we extend\nthem further to incorporate explanatory variables.\n\n10.1 comparing dependent proportions\n\n4\n\nab\n\nab\n\nab\n\nas a sample from a multinomial n; \u2432 distribution. then p\n\nfor each of n matched pairs, let \u2432 denote the probability of outcome a for\nthe first observation and outcome b for the second. let n\ncount the\nnumber of such pairs, with p s n rn the sample proportion. we treat\n\u0004\nis the\nn\nproportion in category a for observation 1, and p\nis the corresponding\nproportion for observation 2. we compare samples by comparing marginal\n. with matched samples, these proportions are\nproportions p\ncorrelated, and methods for independent samples are inappropriate.\nin this section we consider binary outcomes. when \u2432 s \u2432 , then\n\u2432 s \u2432 also, and there is marginal homogeneity. since\n2q\n\nwith p\n\nab\n\u017e\n\naq\n\naq\n\nqa\n\nqa\n\n1q\n\nq1\n\nq2\n\n4.\n\nab\n\nab\n\n\u0004\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u2432 y \u2432 s \u2432 q \u2432 y \u2432 q \u2432 s \u2432 y \u2432 ,\n1q\n\nq1\n\n11\n\n12\n\n11\n\n21\n\n12\n\n21\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nmarginal homogeneity in 2 = 2 tables is equivalent to \u2432 s \u2432 . the table\nthen shows symmetry across the main diagonal.\n\n12\n\n21\n\ninference for dependent proportions\n\n10.1.1\none comparison of the marginal distributions uses \u2426s \u2432 y \u2432 . let\n\nq1\n\n1q\n\nd s p y p s p y p\n\nq1\n\n1q\n\n2q\n\n.\n\nq2\n\n\u017e\nfrom formula 1.3 for multinomial covariances, cov p\nq1\nsimplifies to \u2432 \u2432 y \u2432 \u2432 rn. thus,\np , p q p\n21\n\n11\n\n12\n\n22\n\n12\n\n21\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n, p s cov p q\n\n.\n\n\u017e\n\n1q\n\n11\n\n.\n\n\u017e\n\n'var n d s \u2432 1 y \u2432 q \u2432 1 y \u2432 y 2 \u2432 \u2432 y \u2432 \u2432 .\n.\n21\n\u017e\n10.1\n\n1q\n\n1q\n\nq1\n\nq1\n\n11\n\n22\n\n12\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n11\n.\n\nfor large samples, d has approximately a normal sampling distribution. a\n\nconfidence interval for \u2426s \u2432 y \u2432 is then\n\nq1\n\n1q\n\n.\nd \" z \u2434 d ,\n\n\u02c6\u2423r2\n\n\u017e\n\n "}, {"Page_number": 426, "text": "comparing dependent proportions\n\nwhere\n\n.\n\n\u017e\n\n2\u2434 d s p\n\u02c6\n\u017e\n\n1 y p q p\n\n\u017e\ns p q p y p y p\n\n1q\n.\n\n1q\n\nq1\n\n\u017e\n\n.\n\n\u017e\n\n12\n\n21\n\n12\n\n2\n\n.\n\n21\n\n.\nq1\nrn,\n\n1 y p y 2 p p y p p\n\n\u017e\n\n11\n\n22\n\n12\n\n411\n\n.\n\n21\n\nrn\n\u017e\n\n10.2\n\n.\n\nwith the second formula following after substitution and some algebra.\ninverting the score test of h : \u2426s \u2426 is more complex but provides coverage\nprobabilities closer to the nominal values tango 1998 , as does adding 1 to\n.\neach cell before computing d and \u2434 d .\nthe hypothesis of marginal homogeneity is h : \u2432 s \u2432 i.e., \u2426s 0 .\n.\nthe ratio z s dr\u2434 d or its square is a wald test statistic. under h , an\nalternative estimated variance is\n\n1q\n\nq1\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n0\n\n0\n\n0\n\n0\n\n2\u2434 d s\n\u02c60\n\n\u017e\n\n.\n\np q p\n\n12\n\n21\n\nn q n\n\n12\n\n21\n\ns\n\n.\n\n\u017e\n\n10.3\n\n.\n\nn\nthe score test statistic z s dr\u2434 d simplifies to\n\nn\n\n\u017e\n\n2\n\n\u02c6\n0\n\n0\n\n.\nn y n\nn q n\n\n21\n\n21\n\n12\n\n12\n.\n\nz s\n\n0\n\n\u017e\n\n.\n\n1r2\n\n\u017e\n\n10.4\n\n.\n\n0\n\n\u017e\n\nis a chi-squared statistic with df s 1. the test using it is\n\nthe square of z\n.\ncalled mcnemar\u2019s test mcnemar 1947 .\nthe mcnemar statistic depends only on cases classified in different cate-\ngories for the two observations. the n q n\non the main diagonal are\nirrelevant to inference about whether \u2432 and \u2432 differ. this may seem\nq1\nsurprising, but all cases contribute to inference about how much \u2432 and\n\u2432 differ: for instance, to estimating \u2426 and the standard error.\n\n11\n1q\n\n1q\n\n22\n\nq1\n\n10.1.2 prime minister approval rating example\nfor table 10.1, the sample proportions of approval of the prime minister\u2019s\nperformance are p s 944r1600 s 0.59 for the first survey and p s\n880r1600 s 0.55 for the second. using 10.2 , a 95% confidence interval for\n\u2432 y \u2432 is 0.55 y 0.59 \" 1.96 0.0095 , or y0.06, y0.02 . the approval\nq1\nrating appears to have dropped between 2 and 6%.\n\n1q\n\nq1\n\n1q\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nfor testing marginal homogeneity, the test statistic 10.4 using the null\n\n\u017e\n\n.\n\nvariance is\n\nz s\n\n0\n\n86 y 150\n86 q 150\n.\n\n1r2\n\n\u017e\n\ns y4.17.\n\nit shows strong evidence of a drop in the approval rating.\n\n "}, {"Page_number": 427, "text": "412\n\nmodels for matched pairs\n\n10.1.3\n\nincreased precision with dependent samples\n\nterm of formula 10.1 , based on cov p\n\nthe final\n, reflects the\ndependence between the marginal proportions. by contrast, for independent\nsamples of size n each to estimate binomial probabilities \u2432 and \u2432 , the\ncovariance for the sample proportions is zero, and\n\n, p\n\nq1\n\n1q\n\n1\n\n2\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n'var n difference of sample proportions s \u2432 1 y \u2432 q \u2432 1 y \u2432 .\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n1\n\n2\n\n1\n\n2\n\nx\n\nw\n\n.\n\n\u017e\n\n.\n\n\u017e\n\ndependent samples usually exhibit a positive dependence, with log \u242as\nlog \u2432 \u2432 r\u2432 \u2432 ) 0; that is, \u2432 \u2432 ) \u2432 \u2432 . from 10.1 , positive de-\npendence implies that var d is smaller than when the samples are indepen-\ndent.\n\n11\n\n22\n\n12\n\n11\n\n22\n\n12\n\n21\n\n21\n\na study design using dependent samples can help improve the precision of\nstatistical inferences for within-subject effects. by contrast, standard errors\ntend to be larger, per given number of observations, for between-subject\ngroup comparisons. the improvement is substantial when samples are highly\ncorrelated. to illustrate, table 10.1 with dependent samples of size 1600\neach has a standard error of 0.0095 for d s 0.55 y 0.59. the two observa-\ntions have strong association, the sample odds ratio being 35.1. independent\nsamples of size 1600 each with \u2432 y \u2432 s 0.55 y 0.59 have a standard error\nof 0.0175 for the difference, nearly twice as large.\n\n\u02c6\n\n\u02c6\n\n.\n\n\u017e\n\n1\n\n2\n\n0\n\n\u017e\n\n21\n\n12\n\n10.1.4 small-sample test comparing matched proportions\nthe null hypothesis of marginal homogeneity for binary matched pairs is,\nequivalently, h : \u2432 s \u2432 or \u2432 r \u2432 q \u2432 s 0.5. for small samples,\nan exact test conditions on n* s n q n mosteller 1952 . under h , n\ndistribution, for which e n s n*. the p-value for\nhas a binomial n*,\nthe test is a binomial tail probability.\nfor instance, for table 10.1, consider h : \u2432 - \u2432 , or equivalently,\nh : \u2432 - \u2432 . since n* s 86 q 150 s 236, the reference distribution is\n21\n.\nbin 236,\n. the p-value is the probability of at least 150 successes out of 236\ntrials, which equals 0.00002. the p-value for h : \u2432 / \u2432 doubles this.\n\nq1\n\n1q\n\n12\n\n21\n\n21\n\n21\n\n12\n\n21\n\n21\n\n12\n\n1\n2\n\n1\n2\n\n1\n2\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\na\n\na\n\n0\n\nwhen n* ) 10, the reference binomial distribution is approximately nor-\n. the standardized normal test\n\nn* and variance n*\n\nq1\n\n1q\n\na\n\n\u017e .\u017e .\n1\n1\n2\n2\n\n1\n2\n\nmal with mean\nstatistic equals\n\nz s\n\n1\n2\n\n21\n\nn y n*\n1r2\nn*\u017e\n.\n\n.\u017e\n\n1\n2\n\n1\n2\n\ns\n\n\u017e\n\n21\n\nn y n\nn q n\n\n21\n\n12\n\n12\n.\n\n.\n\n1r2\n\n.\nthis is identical to the mcnemar statistic 10.4 .\n\n\u017e\n\n "}, {"Page_number": 428, "text": "comparing dependent proportions\n\n413\n\n.\n\n\u017e\n\n10.1.5 connection between mcnemar and cochran\u2013mantel\u2013haenszel tests\nan alternative representation of binary responses for n matched pairs\npresents the data in n partial tables, one 2 = 2 table for each pair. it has\ncolumns that are the two possible outcomes for each measurement. row 1\nshows the outcome of the first observation, and row 2 shows the outcome of\nthe second.\n\ntable 10.2 shows the four possible partial tables in this representation.\nfor table 10.1, the full three-way table has 1600 partial tables; 794 look like\nthe one for subject 1 i.e., \u2018\u2018approve\u2019\u2019 at both surveys , 570 who disapproved at\neach survey have tables like the one for subject 2, 86 have tables like the one\nfor subject 3, and 150 have tables like the one for subject 4. the 1600 subjects\nfrom table 10.1 provide 3200 observations in a 2 = 2 = 1600 contingency\ntable. collapsing this table over the 1600 partial tables yields a 2 = 2 table\nwith first row equal to 944, 656 and second row equal to 880, 720 . these\n.\nare the total number of approve, disapprove responses for the two surveys.\nthey form the marginal counts in table 10.1.\n\nfor each subject, suppose that the probability of approval is identical in\nindependence exists between the opinion\neach survey. then, conditional\noutcome and the survey time, controlling for subject. the probability of\napproval is then also the same for each survey in the marginal table collapsed\nover the subjects. but this implies that the true probabilities for table 10.1\nsatisfy marginal homogeneity. thus, a test of conditional independence in the\n2 = 2 = 1600 table provides a test of marginal homogeneity for table 10.1.\nto test conditional independence in this three-way table, one can use the\ncochran\u1390mantel\u1390haenszel cmh statistic 6.6 . the result of that chi-\nsquared statistic is algebraically identical to the squared mcnemar\u2019s statistic,\nnamely n y n\nfor tables of form 10.1 . mcnemar\u2019s test is\na special case of the cmh test applied to the binary responses of n matched\npairs displayed in n partial tables. this connection is not helpful for compu-\ntational purposes, since the mcnemar statistic is simple. but it does suggest\n\n\u017e\nr n q n\n\u017e\n\n.2\n\n21\n\n12\n\n12\n\n21\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\ntable 10.2 representation of four types of matched\npairs contributing to counts in table 10.1\n\nsubject\n\n1\n\n2\n\n3\n\n4\n\nsurvey\nfirst\nsecond\nfirst\nsecond\nfirst\nsecond\nfirst\nsecond\n\nresponse\n\napprove\n\ndisapprove\n\n1\n1\n0\n0\n0\n1\n1\n0\n\n0\n0\n1\n1\n1\n0\n0\n1\n\n "}, {"Page_number": 429, "text": "414\n\nmodels for matched pairs\n\n\u017e\n\nways of handling more complex matched data. with several outcome cate-\ngories or several observations, one can test marginal homogeneity by applying\nthe generalized cmh tests section 7.5 using a single stratum for each\nsubject, with each row representing a particular observation darroch 1981;\n.\nmantel and byar 1978 .\n\ncoming sections refer to the 2 = 2 = n table representation of matched-\npairs data as the subject-specific table. they refer to the 2 = 2 table of form\nof table 10.1 as the population-a\u00aeeraged table, since its margins provide\ndirect estimates of population marginal proportions.\n\n.\n\n\u017e\n\n10.2 conditional logistic regression for binary\nmatched pairs\n\nin section 6.7 we introduced conditional logistic regression for eliminating\nnuisance parameters from an analysis. we now study this for binary\nmatched-pairs data. the models refer to subject-specific tables.\n\n10.2.1 marginal versus conditional models for matched pairs\n.\nthe analyses of section 10.1 occur in the context of models. let y , y1\n2\ndenote the pair of observations for a randomly selected subject, where a \u2018\u20181\u2019\u2019\noutcome denotes category 1 success and \u2018\u20180\u2019\u2019 denotes category 2. the\ndifference \u2426s p y s 1 y p y s 1 between marginal probabilities occurs\n\u017e\nas a parameter in\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n2\n\n1\n\np y s 1 s \u2423q \u2426x ,\n\u017e\n\n.\n10.5\nwhere x s 0 and x s 1; then, p y s 1 s \u2423 and p y s 1 s \u2423q \u2426.\nalternatively, the logit link yields\n\u017e\n\nlogit p y s 1 s \u2423q \u2424x .\n\n10.6\n\n.\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n1\n\n2\n\n1\n\n2\n\nt\n\nt\n\nt\n\nt\n\nthe parameter \u2424 is a log odds ratio with the marginal distributions.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nmodels 10.5 and 10.6 are marginal models: they focus on the marginal\ndistributions of responses for the two observations. for instance, in terms of\n\u017e\nthe population-averaged table, the ml estimate of \u2424 in 10.6 is the log odds\nx\nw\nratio of marginal proportions, \u2424s log p\n. see problem 10.26\nfor its asymptotic variance.\n\np rp\n2q q2\n\nq1\n\n1q\n\n\u02c6\n\np\n\n.\n\nby contrast, the subject-specific table having strata like table 10.2 implic-\ndenote the ith pair of\n\n\u017e\n\n.\nitly allows probabilities to vary by subject. let y , y\ni2\nobservations, i s 1, . . . , n. a model then has the form\nlink p y s 1 s \u2423 q \u2424x .\n\ni1\n\n\u017e\n\n.\n\n\u017e\n\n10.7\n\n.\n\ni\n\nt\n\nit\n\nthis is called a conditional model, since the effect \u2424 is defined conditional on\nthe subject. its estimate describes conditional association for the three-way\ntable stratified by subject. the effect is subject-specific, since it is defined at\n\n "}, {"Page_number": 430, "text": "conditional logistic regression for binary matched pairs\n\n415\n\n.\n\n\u017e\n\n.\nthe subject level. by contrast, the effects in marginal models 10.5 and 10.6\nare population-a\u00aeeraged, since they refer to averaging over the entire popula-\ntion rather than to individual subjects.\n\nfor the identity link, subject-specific and population-averaged effects are\nidentical. for instance, for the conditional model 10.7 with identity link,\n\u2424s p y s 1 y p y s 1 for all i, and averaging this over subjects in the\npopulation equates \u2424 to the \u2426 parameter in model 10.5 . for nonlinear\nlinks, however, the effects differ. for model 10.7 with the logit link, for\ninstance,\n\ni2\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\ni1\n\np y s 1 s exp \u2423 q \u2424x r 1 q exp \u2423 q \u2424x\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\ni\n\nt\n\nit\n\ni\n\n.\n\nt\n\n.\n\nthe average of this for the population does not have the form exp \u2423q\n\u2424x r 1 q exp \u2423q \u2424x\n.\ncorresponding to the marginal logit model 10.6 .\nwe now take a closer look at the conditional model with logit link.\n\n.x\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\nw\n\nt\n\nt\n\n.\n\n\u017e\n\n10.2.2 a logit model with subject-specific probabilities\nmodel 10.7 differs from models in earlier chapters by permitting subjects to\n.\nhave their own probability distributions. cox 1958b, 1970 and rasch 1961\npresented this model with logit link. this model for y , observation t for\nsubject i, is\n\n\u017e\n\n.\n\n\u017e\n\nit\n\nlogit p y s 1 s \u2423 q \u2424x ,\n\n.\n10.8\nwhere x s 0 and x s 1. although permitting subject-specific distributions,\nit assumes a common effect \u2424. for subject i,\n\n\u017e\n\n.\n\n\u017e\n\nit\n\n1\n\n2\n\nt\n\ni\n\np y s 1 s\n\u017e\n\n.\n\ni1\n\n\u017e\n\n.\nexp \u2423\ni\n1 q exp \u2423\n\u017e\n\ni\n\n,\n\n.\n\np y s 1 s\n\u017e\n\n.\n\ni2\n\n\u017e\n\nexp \u2423 q \u2424\n\n.\n1 q exp \u2423 q \u2424\n\ni\n\u017e\n\ni\n\n.\n\n.\n\n<\n\n.\n\n.\n\n\u017e\n\n\u017e\n\nthe parameter \u2424 compares the response distributions. for each subject,\nthe odds of success for observation 2 are exp \u2424 times the odds for observa-\ntion 1.\n\n<\n\u017e\n\ngiven the parameters, with model 10.8 one normally assumes indepen-\ndence of responses for different subjects and for the two observations on the\nsame subject. however, averaged over all subjects, the responses are nonneg-\natively associated. suppose that \u2424 is small compared to \u2423 . a subject with\na large positive \u2423 has high p y s 1 for each t and is likely to have a\nsuccess each time; a subject with a large negative \u2423 has low p y s 1 for\neach t and is likely to have a failure each time. the greater the variability in\n4\u2423 , the greater the overall positive association between responses, successes\n\u0004\ni\n\u017e\nfailures\nfor\nobservation 2. this is true for any \u2424. the positive association reflects the\nshared value of \u2423 for each observation in a pair. no association occurs only\nwhen \u2423 are identical. thus, the model does account for the dependence in\nmatched pairs. fitting it takes into account nonnegative association through\nthe structure of the model.\n\nfor observation 1 tending to occur with successes\n\n\u017e\nfailures\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n4\n\nit\n\nit\n\ni\n\ni\n\ni\n\ni\n\ni\n\n<\n\n<\n\n "}, {"Page_number": 431, "text": "416\n\nmodels for matched pairs\n\n\u0004\n\n4\n\ni\n\nfor this model, the large number of \u2423 causes difficulties with the fitting\n.\nprocess and with the properties of ordinary ml estimators problem 10.24 .\nthe remedy of conditional ml treats them as nuisance parameters and\nmaximizes the likelihood function for a conditional distribution that elimi-\nnates them. a note on terminology: we\u2019ve referred to model 10.8 as a\nconditional model, meaning that its effect \u2424 is subject-specific, conditional\non the subject. the analyses described below for such models are examples of\nconditional logistic regression; but here the term conditional refers to the ml\nanalysis that is performed conditional on sufficient statistics for nuisance\nparameters, to eliminate those parameters from the likelihood.\n\n\u017e\n\n\u017e\n\n.\n\n10.2.3 conditional ml inference for binary matched pairs\nfor model 10.8 , assuming independence of responses for different subjects\nand for the two observations on the same subject, the joint mass function for\n\u0004\u017e\n\n.\n, . . . , y , y\n\n.4\n\nis\n\n\u017e\n\n.\n\n\u017e\n\ny , y\n11\n\n12\n\nn1\n\nn2\n\n\u017e\n\nn\n\n\u0142\nis1\n\n/\n\n\u017e\n\ny\n\ni1\n\n1\n\n\u017e\n\n.i\nexp \u2423\n1 q exp \u2423\n\u017e\n\u017e\n\n.\n\n1 q exp \u2423\n\u017e\n.\n1 q exp \u2423 q \u2424\n\nexp \u2423 q \u2424\n\ni\n\u017e\n\n.\n\ni2\n\ny\n\ni\n\ni\n\n\u017e\n\n/\n\n\u017e\n\ni\n\n=\n\n1yy\n\ni1\n\n/\n\n.\n\n1\n\u017e\n\n1 q exp \u2423 q \u2424\n\ni\n\n/\n\n.\n\n1yy\n\ni2\n\n.\n\nin terms of the data, this is proportional to\n\nexp\n\n\u00fd\n\ni\n\n\u2423 y q y q \u2424\n\n\u017e\n\n.\n\ni2\n\ni1\n\ni\n\n\u017e\n\n\u00fd\n\ni\n\ny\n\ni2\n\n/\n\n.\n\n4\n\n\u0004\n\u0004\n\ni\n\ni\n\ni\n\n4\n\ni1\n\n\u017e\n\ni2\n\nto eliminate \u2423 , we condition on their sufficient statistics, the pairwise\nsuccess totals s s y q y\n. given s s 0, p y s y s 0 s 1, and given\ns s 2, p y s y s 1 s 1. the distribution of y , y\ndepends on \u2424 only\nwhen s s 1; that is, only when outcomes differ for the two responses. given\ny q y s 1, the conditional distribution is\np y s y , y s y\n\u017e\n\ni1\n\u017e\n\ni2\n\ni2\n\ni2\n\ni2\n\n\u017e\n\n.\n\n.\n\n.\n\ni1\n\ni1\n\ni1\n\ni\n\ni\n\ni1\n\n<\n\ni\n\ni2\n\ni2\n\ni1\n\u017e\n\ns s 1\ns p y s y , y s y\n\u017e\n\n.\nexp \u2423\ni\n1 q exp \u2423\n\u017e\n\n/\n\n\u017e\n\ni2\n\ni1\n\ni1\n\n\u017e\n\ni1\n\ny\n\ni2\n\n1\n\n.\n.\n\ns\n\ni\n\n1 q exp \u2423\n.\n\u017e\n.\nexp \u2423\ni\n\u017e\n\n\u017e\n\n.\n\ni\n\n1 q exp \u2423 1 q exp \u2423 q \u2424\n.\ny s 0,\n\ni\n\ni\n\n.\n\n\u017e\n\ns exp \u2424 r 1 q exp \u2424 ,\ns 1r 1 q exp \u2424 ,\n\n.\ny s 1,\n\n\u017e\n\n\u017e\n\n.\n\ni1\n\ni1\n\ny s 0.\n\ni2\n\n.\n1yy\n\ni2\n\ni1\n\ni2\n\n.\n\np y s 1, y s 0 q p y s 0, y s 1\n\u017e\n/\n/\n1yy\n\nexp \u2423 q \u2424\n.\n1 q exp \u2423 q \u2424\n.\n\n\u017e\n\u017e\n\n\u017e\n\n/\n\ni\n\u017e\n\ni2\n\ni1\n\n\u017e\n\n.\n\ni1\n\ny\n\ni\n\ni2\n\n1\n\n1 q exp \u2423 q \u2424\n.\n\u017e\n\n1\n\u017e\nexp \u2423 q \u2424\n.\n1 q exp \u2423 1 q exp \u2423 q \u2424\n.\n\u017e\ny s 1\n\ni\n\u017e\n\n.\n\ni\n\ni\n\ni\n\nq\n\n1\n\u017e\n\ni2\n\n "}, {"Page_number": 432, "text": "conditional logistic regression for binary matched pairs\n\n417\n\n\u0004\n\n4\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni1\n\ni2\n\n12\n\n21\n\n12\n\nab\n\nagain, let n\n\nand \u00fd s s n* s n q n . since n\n\ndenote the counts for the four possible sequences. for\nsubjects having s s 1, \u00fd y s n , the number of subjects having success for\nobservation 1 and failure for observation 2. similarily, for those subjects,\n\u00fd y s n\nis the sum of n* indepen-\ndent, identical bernoulli variates, its conditional distribution is binomial with\nparameter exp \u2424r 1 q exp \u2424 . for testing marginal homogeneity \u2424s 0 ,\n.\n. in summary, the conditional analysis for the logit\nthe parameter equals\nmodel implies that pairs in which y s y\nare irrelevant to inference about\ni1\n\u2424. when this model\nit provides justification for comparing\nmarginal distributions using only the n q n\npairings having outcomes in\ndifferent categories at the two observations.\n\nconditional on s s 1, the joint distribution of the matched pairs is\n\nis realistic,\n\n.x\n\n21\n\n21\n\n12\n\n21\n\n1\n2\n\ni2\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nw\n\ni\n\n\u017e\n\n\u0142\ns s1\ni\n\n1\n\n1 q exp \u2424\n\n\u017e\n\n/\n\n.\n\ny\n\ni1\n\n\u017e\n\n\u017e\n\n.\nexp \u2424\n1 q exp \u2424\n\u017e\n\n.\n\ny\n\ni2\n\ns\n\n/\n\n21\n\nn\n\n\u017e\n\n.\nexp \u2424\n1 q exp \u2424\n\u017e\n.\n\n\u017e\n\n10.9\n\n.\n\nn*\n\nwhere the product refers to all pairs having s s 1. differentiating the log of\nthis conditional likelihood and equating to 0 and solving yields the condi-\ntional ml estimator of \u2424 in model 10.8 . you can check that it and its\nstandard error are\n\n\u017e\n\n.\n\ni\n\n\u02c6\u2424s log n rn\n\n\u017e\n\n21\n\n.\n\n,\n\n12\n\nse s 1rn q 1rn .\n\n21\n\n12\n\n'\n\n\u017e\n\n10.10\n\n.\n\ni\n\ni\n\ni\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n\u017e\n\n\u017e\n\n.\n\n2.\n\n10.2.4 random effects in binary matched-pairs model\nan alternative remedy to handling the huge number of nuisance parameters\nin logit model 10.8 treats \u2423 as random effects. this regards \u2423 as an\nunobserved random sample from a probability distribution, usually assumed\nto be n \u242e, \u2434 with unknown \u242e and \u2434. it eliminates \u2423 by averaging with\nrespect to their distribution, yielding a marginal distribution. the likelihood\nfunction then depends on \u2424 as well as the n \u242e, \u2434 parameters. it has only\nthree parameters and is more manageable. for matched pairs with non-\nnegative sample log odds ratio, this approach also yields \u2424s log n rn21\n.\n12\n\u017e\nneuhaus et al. 1994 . this model is an example of a generalized linear mixed\nmodel, containing both random effects and the fixed effect \u2424. its analysis is\npresented in chapter 12.\n\n2.\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nmodel 10.8 implies that the true odds ratio for each of the n subject-\nspecific partial tables equals exp \u2424 . in section 6.3.5 we presented the\nmantel\u1390haenszel estimate of a common odds ratio for several 2 = 2 tables.\nin fact, that estimator applied to subject-specific tables of the form shown in\ntable 10.2 is algebraically identical to n rn\nfor tables of the form shown\nin table 10.1. recall that partial tables with responses in only one column\ndo not contribute to the cmh test or mantel\u1390haenszel estimate.\nin\nsummary, the mantel\u1390haenszel estimate, the conditional ml estimate, and\n\n21\n\n12\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 433, "text": "418\n\nmodels for matched pairs\n\n\u017e\nwith nonnegative log odds ratio the ml estimate for the random effects\nversion of logit model 10.8 yield exp \u2424 s n rn .\n\n\u02c6\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n21\n\n12\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\ni2\n\ny , y\ni1\n\n10.2.5 logistic regression for matched case\u2013control studies\nthe two observations\nin a matched pair need not refer to the same\nsubject. for instance, case\u1390control studies that match a single control with\neach case yield matched-pairs data. for a binary response y, each case\ny s 1 is matched with a control y s 0 according to criteria that could\n\u017e\naffect the response. subjects in the matched pairs are measured on the\npredictor variable s of interest, x, and the xy association is analyzed.\n\n\u017e .\n\ntable 10.3 illustrates. a case\u1390control study of acute myocardial infarction\n\u017e\n.mi among navajo indians matched 144 victims of mi according to age and\ngender with 144 people free of heart disease. subjects were asked whether\nx s 0, no; x s 1, yes .\n.\nthey had ever been diagnosed as having diabetes\ntable 10.3 has the same form as table 10.1 except that the levels of x rather\nthan the levels of y form the rows and the columns.\n\none can display the data for each matched case\u1390control pair using a\npartial table of the form shown in table 10.2, but reversing the roles of x\nand y. the x values have four possible patterns, shown in table 10.4. there\nare 37 partial tables of type a, since for 37 pairs the case had diabetes and\nthe control did not, 16 partial tables of type b, 9 of type c, and 82 of type d.\n\n\u017e\n\nnow, for subject t in matched pair i, consider the model\n\nlogit p y s 1 s \u2423 q \u2424x .\n\n\u017e\n\n.\n\nit\n\nit\n\ni\n\n\u017e\n\n10.11\n\n.\n\ntable 10.3 previous diagnoses of diabetes for\nmyocardial infarction mi case\u2013control pairs\n\n(\n\n)\n\nmi controls\ndiabetes\nno diabetes\n\ntotal\n\nmi cases\n\ndiabetes\n\nno diabetes\n\n9\n37\n46\n\n16\n82\n98\n\n.\nsource: j. l. coulehan et al., amer. j. public health 76: 412\u1390414 1986 ,\nreprinted with permission from the american public health association.\n\ntotal\n25\n119\n144\n\u017e\n\ntable 10.4 possible case\u2013control pairs for table 10.3\n\ndiabetes\nyes\nno\n\ncase\n\n1\n0\n\na\ncontrol\n\n0\n1\n\nb\ncontrol\n\n1\n0\n\ncase\n\n0\n1\n\ncase\n\n1\n0\n\nc\ncontrol\n\nd\n\ncase control\n\n1\n0\n\n0\n1\n\n0\n1\n\n "}, {"Page_number": 434, "text": "conditional logistic regression for binary matched pairs\n\n419\n\nthe probabilities modeled refer to the distribution of y given x, but the\nretrospective study provides information only about the distribution of x\ngiven y. one can estimate the odds ratio exp \u2424 , however, since it refers to\nthe xy odds ratio, which relates to both conditional distributions sections\n2.2.4, 5.1.4 . even though this study reverses the roles of x and y in terms of\nwhich is fixed and which is random, the conditional ml estimate of exp \u2424 is\nsimply n rn s 37r16 s 2.3.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n21\n\n12\n\n10.2.6 conditional ml for matched pairs with multiple predictors\nwhen the binary response has p predictors for case\u1390control or subject-\nspecific matched pairs, the model generalizes to\n\nlogit p y s 1 s \u2423 q \u2424 x q \u2424 x q \u2b48\u2b48\u2b48 q\u2424 x\n\n.\n\n\u017e\n\ni\n\n1 1it\n\n2\n\n2 it\n\nit\n\n,\n\npit\n\n\u017e\n\n10.12\n\n.\n\np\n\nhit\n\ndenotes the value of predictor h for observation t\n\nwhere x\nin pair i,\nt s 1, 2. typically, one predictor is an explanatory variable of interest, such\nas diabetes status. the others are covariates being controlled, in addition to\nthose already controlled by virtue of using them to form the matched pairs.\nthe conditional ml approach to estimating \u2424 conditions on sufficient\nstatistics for \u2423 to eliminate them from the likelihood.\nand \u2424 s \u2424 , . . . , \u2424 . a generalization of the\n\ni\u017e\nlet x s x\n\n\u017e\nderivation in section 10.2.3 shows that\n\n, . . . , x\n\n.x\n\n.x\n\n1it\n\npit\n\n4\n\n\u0004\n\nit\n\n1\n\np\n\nj\n\ni1\n\np y s 0, y s 1 s s 1 s exp x \u2424\n\u017e\np y s 1, y s 0 s s 1 s exp x \u2424\n\u017e\n\nx\ni2\n\n.\n\n.\n\n\u017e\n\n\u017e\n\ni2\n\ni\n\n<\n\n<\n\n.\n\n.\n\nx\ni1\n\ni2\n\ni1\n\ni\n\nexp x \u2424 q exp x \u2424 ,\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nx\ni2\n\nx\ni1\n\nexp x \u2424 q exp x \u2424 .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nx\ni2\n\nx\ni1\n\n\u017e\n\n10.13\n\n.\n\n\u017e x\ni1\n\n.\n\ni\n\ni1\n\ni1\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\ni2\n\ni2\n\ndividing numerator and denominator by exp x \u2424 shows that\nthe first\nequation has the form of logistic regression with no intercept and with\npredictor values x* s x y x . in fact, one can obtain conditional ml\nestimates for model 10.12 by fitting a logistic regression model to those\npairs alone, using artificial response y* s 1 when y s 0, y s 1 , y* s 0\nwhen y s 1, y s 0 , no intercept, and predictor values x*. this addresses\nthe same likelihood as the conditional\nlikelihood breslow et al. 1978;\n.\nchamberlain 1980 .\nand\nt s 1 refers to the control and t s 2 to the case, then\nx* s x y x . if\ny * s 1 always. since x s 1 represents \u2018\u2018yes\u2019\u2019 for diabetes and x s 0\ni\ny* s 1, x* s y1 for 16 observations,\ny* s 1, x* s 0 for\nrepresents \u2018\u2018no,\u2019\u2019\n9 q 82 s 91 observations, and y* s 1, x* s q1 for 37 observations. the\nlogit model that forces \u2423s 0 has \u2424s 0.84. with a single binary predictor,\n\u02c6\nthe estimate is identical to log n rn\n12\n\nto illustrate, for model 10.11 with table 10.3, let y* s y y y\n\n.\n.\n\n\u02c6\n\n21\n\ni2\n\ni2\n\ni2\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\ni1\n\ni1\n\ni1\n\nit\n\nit\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n "}, {"Page_number": 435, "text": "420\n\nmodels for matched pairs\n\nj\n\ni\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n10.2.7 marginal models and conditional models: extensions\nfor binary matched-pairs data, section 10.1 presented analyses for a marginal\n\u017e\ni.e., population-averaged model, and this section presented analyses for a\n\u017e\nconditional\ni.e., subject-specific model. these models generalize to multino-\n.\nmial responses and to matched sets. for instance, chamberlain 1980\ndiscussed conditional ml for matched pairs on a multinomial response. for\nbinary responses, model 10.12 applies when \u2423 refers to a set of repeated\nmeasurements on subject i. or, it could refer to a matched set that is a\ncluster of subjects, such as children from family i or fetuses from litter i.\n\nwith extensions of the conditional model to matched-set clusters, the\nconditional ml approach is restricted to estimating \u2424 that are within-\ncluster effects, such as occur in case\u1390control and crossover studies. for these,\nthe explanatory variable varies in t\nfor each i. conditional ml cannot\nestimate a between-cluster effect. statistics providing information about such\nan effect use subject totals at different levels of the relevant explanatory\nvariable; however, those totals sum the sufficient statistics for \u2423 , so they\nare themselves fixed and have degenerate distributions after conditioning on\nthe sufficient statistics. an explanatory variable that is constant in t for each\ni cancels out of the conditional likelihood. you can observe this for matched\npairs with 10.13 for any j for which x s x\nall i. for it, at best one can\nstratify by its levels and fit a model estimating within-cluster effects sepa-\nrately at each level. an advantage of using the random effects approach\ninstead of conditional ml with the conditional model\nis that it is not\nrestricted to estimating within-cluster effects.\n\nin the remainder of this chapter we emphasize marginal models for\nmatched pairs with multinomial responses. in the following chapter we deal\nwith marginal model extensions allowing matched sets and explanatory vari-\nables. conditional models using a random effects approach have extra\ncomputational complexities. we mention briefly some multinomial condi-\ntional models in this chapter, but we defer most discussion to chapter 12.\n\nji2\n\nji1\n\n\u017e\n\n.\n\nw\n\nx\n\n\u0004\n\n4\n\ni\n\n10.3 marginal models for square contingency tables\n\nmatched pairs analyses generalize from binary to i ) 2 outcome categories.\na square i = i table n\nshows counts of possible sequences a, b of\noutcomes for y , y . let \u2432 s p y s a, y s b . marginal homogeneity is\np y s a s p y s a for a s 1, . . . , i. marginal models compare p y s\n\u017e\na and p y s a .\n.4\n.4\n\nab\n\nab\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u0004\n\n4\n\n\u0004\n\n\u0004\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\n10.3.1 marginal models for ordinal classifications\nfor ordered categories, marginal model 10.6 for binary matched pairs\nextends using ordinal logits. with cumulative logits,\n\n\u017e\n\n.\n\nlogit p y f j s \u2423 q \u2424x ,\n\n.\n\n\u017e\n\nt s 1, 2,\n\nj s 1, . . . , i y 1,\n\n\u017e\n\n10.14\n\n.\n\nj\n\nt\n\nt\n\n "}, {"Page_number": 436, "text": "1\n\n2\n\n2\n\n1\n\n.\n\n\u017e\n\n.\n\n421\nmarginal models for square contingency tables\nwhere x s 0 and x s 1. this model has proportional odds structure\nsection 7.2.2 . the odds of outcome y f j equal exp \u2424 times the odds of\n\u017e\noutcome y f j. the model implies stochastically ordered marginal distribu-\ntions, with \u2424) 0 meaning that y tends to be higher than y . marginal\nhomogeneity corresponds to \u2424s 0.\n\n\u0004\n\u0004\n\nab\n\u017e\n\nmodel fitting treats y , y as dependent. the ml approach maximizes\nthe multinomial likelihood for \u2432 . this is not simple. since the model\nrefers to marginal probabilities p y s a s \u2432 and p y s b s \u2432 ,\n4\nqb\none cannot substitute the model formula in the kernel \u00fd \u00fd n log \u2432 of\nthe log likelihood, which has joint probabilities. we defer discussion of ml\nmodel fitting of marginal models to section 11.2.5. model 10.14 describes\nthe 2 i y 1 marginal probabilities by i parameters, so df s i y 2 for testing\nfit. alternatively, one can compare margins using summaries such as a\n.\ndifference in means for chosen category scores problem 10.38 .\n\naq\n\nab\n\nab\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n4\n\n4\n\n\u0004\n\na\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\nb\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n10.3.2 premarital and extramarital sex example\nrefer to table 10.5. for a general social survey, subjects gave their opinion\nabout premarital sex a couple having sex before marriage and extramarital\nsex a married person having sex with someone other than the marriage\npartner . the response categories are 1 s always wrong, 2 s almost always\nwrong, 3 s wrong only sometimes, 4 s not wrong at all.\n\u017e\n\nthe sample cumulative marginal proportions are 0.307, 0.389, 0.611 for\npremarital sex and 0.815, 0.918, 0.987 for extramarital sex. this suggests that\nresponses on premarital sex tended to be higher on the ordinal scale than\nthose on extramarital sex. with scores 1, 2, 3, 4 , the mean for premarital sex\nis 2.69, closest to the \u2018\u2018wrong only sometimes\u2019\u2019 score, and the mean response\nfor extramarital sex is 1.28, closest to the \u2018\u2018always wrong\u2019\u2019 score.\nthe cumulative logit model 10.14 has \u2424s 2.51 se s 0.13 . there is\nstrong evidence that population responses are more positive on premarital\nthan on extramarital sex. the fit of the marginal homogeneity model has\ng s 348.1 df s 3 , and the fit of model 10.14 has g s 35.1 df s 2 . the\nordinal model does not fit well, but it fits much better than the marginal\nhomogeneity model. models to be considered in section 10.4.7 fit better yet.\n\n\u02c6\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n2\n\n2\n\ntable 10.5 opinions on premarital sex and extramarital sex\n\npremarital\n\nsex\n1\n2\n3\n4\n\ntotal\n\n1\n144\n33\n84\n126\n\n387\n\nextramarital sex\n3\n0\n2\n6\n25\n\n2\n2\n4\n14\n29\n\n49\n\n33\n\n4\n0\n0\n1\n5\n\n6\n\ntotal\n146\n39\n105\n185\n\n475\n\nsource: 1989 general social survey, national opinion research center.\n\n "}, {"Page_number": 437, "text": "422\n\nmodels for matched pairs\n\n10.3.3 marginal models for nominal classifications\nwith nominal responses, it is not sensible to assume the same effect for each\nlogit. a baseline-category logit model has form\n\nlog p y s j rp y s i s \u2423 q \u2424 x ,\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nj\n\nj\n\nt\n\nt\n\nt\n\nt s 1, 2,\n\nj s 1, . . . , i y 1,\n10.15\n\n\u017e\n\n.\n\n2\n\n1\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n4\n\nwhere x s 0 and x s 1. this model has 2 i y 1 parameters for the\n2 i y 1 marginal probabilities. it is saturated.\n\u017e\nmarginal homogeneity is the special case \u2424 s \u2b48\u2b48\u2b48 s \u2424 s 0. to fit it,\nlipsitz et al. 1990 and madanksy 1963 maximized the multinomial likeli-\n\u0004\nsubject to these constraints. iterative methods produce fitted\nhood for n\nab\n4\n\u02c6 ab\nvalues \u242e . comparing these to n\nusing g or x tests marginal\nhomogeneity, with df s i y 1.\nbhapkar 1966 tested marginal homogeneity by exploiting the asymptotic\nnormality of marginal proportions. let d s p y p , and let dx s\n. it is redundant to include d , since \u00fdd s 0. the sample\n.\n\u017e\nd , . . . , d\n1\ncovariance matrix v of n d has elements\n\naq\na\n\niy1\n\niy1\n\n'\n\nqa\n\n\u02c6\n\nab\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n\u0004\n\na\n\n1\n\n2\n\n2\n\ni\n\n\u017e\n\n\u017e\n\n.\n\nab\n\nb a\n\n\u00ae s y p q p y p y p\n\u02c6ab\naq\n\u00ae s p q p y 2 p y p y p\n\u02c6aa\n' w\n\nqa\n\u017e\n\n\u017e .x\n\naq\n\nqa\n\nqa\n\naa\n\n.\n\nbq\n\nfor a / b,\n\np y p\nqb\n.\n\n.\n\n2\n\n. \u017e\n\naq\n\nnow n d y e d has an asymptotic multivariate normal distribution with\nestimated covariance matrix v. under marginal homogeneity, e d s 0,\nand\n\n\u017e .\n\n\u02c6\n\nw s nd v d\nx \u02c6y1\n\n.\nis asymptotically chi-squared with df s i y 1. this is a wald test for parame-\n.\nters in the analog of model 10.15 using the identity link. stuart 1955\nproposed w s nd v d, which uses the sample null covariance matrix v\n\u02c6\n0\nand is the score test. this has\n\nx \u02c6y1\n0\n\n10.16\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n0\n\n\u017e\n\n\u00ae s y p q p\n\u02c6ab0\n\u00ae s p q p y 2 p .\n\u02c6aa0\n\nqa\n\naq\n\nb a\n\naa\n\nab\n\n.\n\nfor a / b,\n\n.\n\n\u017e\n\nireland et al. 1969 noted that w s w r 1 y w rn . for i s 2, w is\n.\nmcnemar\u2019s statistic, the square of 10.4 .\nthese tests use all i y 1 degrees of freedom available for comparisons of\ni pairs of marginal proportions. with ordered categories, when i is large and\nthe dependence between classifications is strong, ordinal tests with df s 1\n.\n.\ncan be much more powerful agresti 1984, p. 209 .\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n0\n\n0\n\n0\n\n "}, {"Page_number": 438, "text": "symmetry, quasi-symmetry, and quasi-independence\n\n423\n\ntable 10.6 migration from 1980 to 1985, with fit of marginal homogeneity model\n\nresidence\nin 1980\nnortheast\n\nmidwest\n\nsouth\n\nwest\n\ntotal\n\n.\n\nnortheast\n11,607\n\u017e\n.\n11,607\n87\n\u017e\n88.7\n172\n\u017e\n276.5\n63\n\u017e\n92.5\n11,929\n\u017e\n12,064.7\n\n.\n\n.\n\n.\n\nresidence in 1985\n\n.\n\nmidwest\n100\n\u017e\n98.1\n13,677\n\u017e\n.\n13,677\n255\n\u017e\n350.8\n176\n\u017e\n.\n251.3\n14,178\n\u017e\n14,377.1\n\n.\n\n.\n\n.\n\nsouth\n366\n\u017e\n265.7\n515\n\u017e\n.\n379.1\n17,819\n\u017e\n.\n17,819\n286\n\u017e\n.\n269.8\n18,986\n\u017e\n18,733.5\n\n.\n\nwest\n\n.\n\n.\n\n124\n\u017e\n94.0\n302\n\u017e\n323.3\n270\n.\n\u017e\n287.3\n10,192\n\u017e\n.\n10,192\n10,888\n\u017e\n10,805.6\n\ntotal\n12,197\n\u017e\n12,064.7\n14,581\n\u017e\n14,377.1\n18,486\n\u017e\n18,733.5\n10,717\n\u017e\n10,805.6\n55,981\n\n.\n\n.\n\n.\n\n.\n\n.\n\nsource: data based on table 12 of u.s. bureau of the census, current population reports,\nseries p-20, no. 420, geographical mobility: 1985 washington, dc: u.s. government printing\noffice , 1987.\n\n.\n\n\u017e\n\n2\n\n10.3.4 migration example\nfor a sample of u.s. residents, table 10.6 compares region of residence in\n1985 with 1980. relatively few people changed region, 95% of the observa-\ntions falling on the main diagonal. the ml fit of marginal homogeneity,\nshown in table 10.6, gives g s 240.8 df s 3 . statistics using differences in\nsample marginal proportions give similar results. for instance, bhapkar\u2019s\nstatistic 10.16 is w s 236.5 df s 3 .\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nfor\n\nthe\n\nthe\n\nfour\n\nsample marginal proportions\n\nregions were\n\u017e\n.\n0.218, 0.260, 0.330, 0.191 in 1980 and 0.213, 0.253, 0.339, 0.194 in 1985. lit-\ntle change occurred over such a short time period. the large test statistics\nreflect the huge sample size. to estimate the change for a given region, we\napply 10.2 to the collapsed 2 = 2 table that combines the other regions. a\n95% confidence interval for \u2432 y \u2432 is 0.2131 y 0.2179 \" 1.96 0.00054 ,\n.\nor y0.005 \" 0.001. similarly, a 95% confidence interval for \u2432 y \u2432 is\ny0.007 \" 0.001, for \u2432 y \u2432 is 0.009 \" 0.001, and for \u2432 y \u2432 is\n0.003 \" 0.001. although strong evidence of change occurs for all\nfour\nregions, the changes were small.\n\n2q\n4q\n\nq2\nq4\n\nq1\n\n1q\n\nq3\n\n3q\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n10.4 symmetry, quasi-symmetry, and quasi-independence\n\nan alternative analysis of square contingency tables directly models the joint\ndistribution using logit or loglinear models. some models have marginal\nhomogeneity as a special case.\n\n "}, {"Page_number": 439, "text": "424\n\nmodels for matched pairs\n\n4\n\nab\n\n\u0004\n\u2432 s \u2432\nb a\nab\n\nan i = i joint distribution \u2432 satisfies symmetry if\n\n.\n10.17\nunder symmetry, \u2432 s \u00fd \u2432 s \u00fd \u2432 s \u2432 for all a, so marginal homo-\ngeneity occurs. for i s 2, symmetry is equivalent to marginal homogeneity,\nbut for i ) 2, marginal homogeneity can occur without symmetry.\n\nwhenever a / b.\n\naq\n\nqa\n\nb a\n\nab\n\n\u017e\n\nb\n\nb\n\n10.4.1 symmetry as logit and loglinear models\nwhen all \u2432 ) 0, symmetry is a logit and a loglinear model. in logit form, it\nis trivially\n\nab\n\nfor expected frequencies \u242e s n\u2432 , it has the loglinear form\n\n.\n\n\u017e\n\nlog \u2432 r\u2432 s 0\n4\n\nab\n\u0004\n\nb a\n\nab\n\nab\n\nfor all a - b.\n\nab\n\nlog \u242e s \u242dq \u242d q \u242d q \u242d\n\n.\n10.18\nwhere all \u242d s \u242d . both classifications have the same single-factor parame-\nters \u242d , so log \u242e s log \u242e . identifiability requires constraints. a simpler\nexpression is log \u242e s \u242d , with all \u242d s \u242d .\nb a\n4\n\nfor poisson or multinomial cell counts n\n\n, the likelihood equations are\n\nb a\n\nb a\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\n\u017e\n\n\u0004\n\n\u0004\n\n4\n\na\n\na\n\nb\n\nab\n\n\u242e q \u242e s n q n\n\u02c6\n\n\u02c6\n\nb a\n\nab\n\nab\n\nb a\n\nfor all a - b and \u242e s n\n\n\u02c6\n\naa\n\nfor all a.\n\naa\n\nthe main diagonal has perfect fit. the solution that satisfies symmetry is\n\n\u242e s\n\u02c6 ab\n\nn q n\n\nab\n\nb a\n\n2\n\nfor all a, b.\n\nab\n\nb a\n\n.4\n\nbinomial pairs\nthe logit symmetry model has no parameters for the\nwith a - b, so its residual df s i i y 1 r2. equivalently, the\n\u0004\u017e\nn , n\nloglinear symmetry model log \u242e s \u242d \u242d s \u242d for i poisson counts\n/2\n\u242d with a - b and i \u242d , so df s i y i q i i y 1 r2 s\n\u0004\nn\ni i y 1 r2. for testing symmetry, bowker 1948 showed that x simplifies\n\u017e\nto\n\nhas\n.\n\n\u017e\n4\n\n\u017e\n\nb a\n\naa\n\nab\n\nab\n\nab\n\nab\n\nab\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\nw\n\nx\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n2\n\n2\n\n2\n\ni\n\n\u017e\n\ni\n\n/2\n\n2x s\n\n\u017e\n\nn y n\n\u00fd\u00fd n q n\n\nab\n\na-b\n\nab\n\nb a\n\nb a\n\n2\n\n.\n\n.\n\nfor i s 2 this is mcnemar\u2019s statistic, the square of 10.4 . the standardized\npearson residuals equal\n\n\u017e\n\n.\n\nr s n y n\n\n\u017e\n\nab\n\nab\n\n. \u017e\n\nn q n\n\nab\n\n.\n\nb a\n\nb a\n\n1r2\n\n.\n\n "}, {"Page_number": 440, "text": ". they satisfy \u00fd\n\nsymmetry, quasi-symmetry, and quasi-independence\n425\nonly one residual for each pair of categories is nonredundant, since r s\nyr\n\nb a\nthe symmetry model is very simple. except for a few specialized applica-\ntions, such as describing intraobserver agreement for pairs of measurements\nby an observer, it rarely fits well. when the marginal distributions differ\nsubstantially, it fits poorly.\n\nr 2 s x 2.\n\na- b ab\n\nab\n\n10.4.2 quasi-symmetry\none can accommodate marginal heterogeneity by permitting the main-effect\nterms in the symmetry model 10.18 to differ. the resulting loglinear model,\ncalled quasi-symmetry, is\n\n\u017e\n\n.\n\nlog \u242e s \u242dq \u242dx q \u242dy q \u242d ,\n\n.\n10.19\nwhere \u242d s \u242d for all a - b caussinus 1966 . symmetry is the special case\n\u242dx s \u242dy for a s 1, . . . , i, and independence is the special case in which all\n\u242d s 0.\n\nb a\n\nab\n\nab\n\nab\n\n\u017e\n\n\u017e\n\n.\n\na\n\na\n\na\n\nb\n\nab\nthe likelihood equations for quasi-symmetry are\n\n\u242e s n\n\u02c6 aq\n\u242e s n\n\u02c6qb\n\u02c6\n\nb a\n\naq\n\nqb\n\n\u242e q \u242e s n q n\n\u02c6\n\nab\n\nab\n\nfor a f b.\n\nb a\n\n,\n\n,\n\na s 1, . . . , i\nb s 1, . . . , i\n\n\u017e\n\n10.20\n\n.\n\nonly one of the first two sets of equations is needed. the other is redundant,\ngiven the other two. the residual df s i y 1 i y 2 r2. from 10.20 ,\n.\nfor a s 1, . . . , i. otherwise, the likelihood equations do not have a\n\u242e s n\n\u02c6 aa\ndirect\nsuch as\n.\nnewton\u1390raphson and ipf caussinus 1966 .\n\naa\nsolution. they are solved using iterative methods\n\n.\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nthe quasi-symmetry model has multiplicative form\n\na\n\nab\n\nab\n\nb ab\n\nall a - b\n\n\u2432 s \u2423 \u2424 \u2425 , where \u2425 s \u2425\n\n.\n10.21\nand all parameters are positive. the symmetry model is 10.21 with \u2423 s \u2424\na\nfor all a. this equation indicates that a table satisfying quasi-symmetry is the\ncellwise product of a table satisfying independence with one satisfying sym-\nmetry. the association symmetry implies that odds ratios on one side of the\nmain diagonal are identical to corresponding odds ratios on the other side. in\nfact, the model can be defined by properties such as\n\nb a\n\n\u017e\n\n\u017e\n\n.\n\na\n\nab\n\n\u242e \u242e\nii\n\u242e \u242e\ni b\n\na i\n\ns\n\nb a\n\n\u242e \u242e\nii\n\u242e \u242e\nia\n\nbi\n\nfor all a - b\n\n\u017e\n\n10.22\n\n.\n\nor \u242a s \u242a for local odds ratios. goodman 1979a referred to it as the\nsymmetric association model.\n\nb a\n\nab\n\n\u017e\n\n.\n\n "}, {"Page_number": 441, "text": "426\n\nmodels for matched pairs\n\nthe meaning of quasi-symmetry is less obvious than symmetry. however, it\nusually fits much better and has greater scope. one way to interpret its\nparameters relates to subject-specific logit models. for such models having\nadditivity of subject terms and occasion terms, of which model 10.8 is the\nsimplest case, the joint distribution in the corresponding population-averaged\ntable necessarily satisfies quasi-symmetry see darroch 1981; section 13.2.7\nshows this . consider the generalization of baseline-category logit model\n\u017e\n10.15 to a subject-specific model\nlog p y s j rp y s i s \u2423 q \u2424 x ,\n\nj s 1, . . . , i y 1.\n\nt s 1, 2,\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nit\n\nit\n\nj\n\nt\n\nj\n\n\u0004\n\n\u0004\n\n.\n\n\u017e\n\ny\nj\n\nthis has the additive form of 10.8 for each j. the model implies, averaging\nover subjects, that the quasi-symmetry model 10.19 holds for the i = i\npopulation-averaged table with \u2424 s \u242d y \u242d , when one constrains \u242d s\n\u242d s 0. in fact, for the conditional ml analysis that conditions out \u2423 , the\ny\ni\nconditional ml estimates of \u2424 relate to the ordinary ml fit of quasi-sym-\nmetry by \u2424 s \u242d y \u242d conaway 1989 . this provides an interpretation for\n.\nthe main-effect terms in quasi-symmetry.\n\n\u02c6\u0004\nj\n\n\u02c6x\nj\n\n\u02c6y\nj\n\nx 4\nj\n\n4 \u017e\n\n\u02c6\nj\n\nrelated results hold for multiple occasions using a multivariate form\n\u017e\n10.33 of quasi-symmetry e.g., agresti 1997; conaway 1989; darroch 1981;\ntjur 1982; see also section 13.2.7 . in addition, quasi-symmetry contains as a\nspecial case other useful models. these include the ones in sections 10.4.3\nand 10.6.3.\n\nx\ni\n\n\u017e\n\n.\n\n.\n\n\u0004\n\n4\n\n4\n\ni j\n\ni j\n.\n\n\u017e\n\n10.4.3 quasi-independence\nsquare tables usually exhibit positive dependence, manifested by larger\ncounts on the main diagonal than the independence model predicts. condi-\ntional on the event that a matched pair falls off the main diagonal, though,\nthe relationship may have a simple structure.\n\na square contingency table satisfies quasi-independence when the variables\nare independent, given that the row and column outcomes differ. this has\nthe loglinear form\n\nlog \u242e s \u242dq \u242dx q \u242dy q \u2426 i a s b ,\n.\n\n\u017e\n\nb\n\na\n\n\u017e\n\n10.23\n\n.\n\n\u017e .\nwhere i \u2b48\n\na\nis the indicator function,\n\nab\n\ni a s b s\n\u017e\n\n.\n\n\u00bd 0,\n\n1,\n\na s b\na / b.\n\nthis adds a parameter to the independence model for each cell on the main\n4\ndiagonal. the first three terms in 10.23 specify independence, and \u2426\na\npermit \u242e to depart from this pattern and have arbitrary positive values.\nwhen \u2426 ) 0, \u242e is larger than under independence.\n\naa\n\n\u017e\n\n.\n\n\u0004\n\n\u0004\n\n4\n\na\n\naa\n\nthe likelihood equations for quasi-independence are\n\n\u242e s n\n\u02c6\naq\n\n,\n\naq\n\n\u242e s n\n\u02c6\nqa\n\nqa\n\n,\n\n\u242e s n ,\n\u02c6\n\naa\n\naa\n\na s 1, . . . , i.\n\n "}, {"Page_number": 442, "text": "symmetry, quasi-symmetry, and quasi-independence\n\n427\n\na perfect fit occurs on the main diagonal, but independence holds for the\nremaining cells. the model implies that odds ratios equal 1.0 for all rectangu-\nlarly formed 2 = 2 tables in which all cells fall off the main diagonal. one\ncan fit the model using newton\u1390raphson or ipf. the model has i more\nparameters than the independence model, so its residual df s i y 1 y i.\nit applies to tables with i g 3.\n\n.2\n\nquasi-independence is the special case of quasi-symmetry 10.21 in which\n\u0004\n\u2425 for a / b are identical. caussinus 1966, p. 146 showed that they are\nab\nequivalent when i s 3.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n4\n\n2\n\n2\n\n2\n\n\u017e\n\n10.4.4 migration revisited\nwe now return to table 10.6 on migration patterns. not surprisingly, the\nindependence model fits terribly, with g s 125,923 and x s 146,929. the\nmaximum possible value of x is 3n s 167,943; see problem 3.33. the\nsymmetry model is also unpromising. for instance, 124 people moved from\nthe northeast to the west, but only 63 people made the reverse move. the\ndeviance for testing symmetry is g s 243.6 df s 6 .\n.\n\nquasi-independence states that for people who moved, residence in 1985\nis independent of region in 1980. table 10.7 contains its fitted values, for\nwhich g s 69.5 df s 5 . this model fits much better than the indepen-\ndence model, primarily because it forces a perfect fit on the main diagonal,\nwhere most observations occur. however, lack of fit is apparent off that\ndiagonal. many more people moved from the northeast to the south and\nmany fewer moved from the west to the south than quasi-independence\npredicts.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n2\n\n2\n\ntable 10.7 fit of models to table 10.6\n\nresidence\nin 1980\nnortheast\n\nmidwest\n\nsouth\n\nwest\n\ntotal\n\nnortheast\n11,607\n\n.\n\n87\n\u017e\n117.4\n\u017e\n.\n91.2\n172\n\u017e\n133.2\n\u017e\n167.6\n63\n\u017e\n71.4\n\u017e\n63.2\n11,929\n\n.\n.\n\n.\n.\n\na\n\nresidence in 1985\nmidwest\n100\n\u017e\n.\n126.6\n\u017e\n.\n2\n95.8\n13,677\n\n1\n\n.\n.\n\n255\n\u017e\n243.8\n\u017e\n238.3\n176\n\u017e\n.\n130.6\n\u017e\n.\n166.9\n14,178\n\n.\n.\n\nsouth\n366\n\u017e\n312.9\n\u017e\n370.4\n515\n\u017e\n.\n531.1\n\u017e\n.\n501.7\n17,189\n\n286\n\u017e\n.\n323.0\n\u017e\n.\n294.9\n18,986\n\nwest\n124\n.\n\u017e\n150.5\n.\n\u017e\n123.8\n302\n.\n\u017e\n255.5\n\u017e\n.\n311.1\n270\n.\n290.0\n.\n261.1\n10,192\n\n\u017e\n\u017e\n\ntotal\n12,197\n\n14,581\n\n18,486\n\n10,717\n\n10,888\n\n55,981\n\na1quasi-independence fit; 2quasi-symmetry fit; both models giving perfect fit on main diagonal.\n\n "}, {"Page_number": 443, "text": "428\n\n\u0004\n\nmodels for matched pairs\nthe quasi-symmetry model has g2 s 3.0, with df s 3. table 10.7 displays\nits fit, which is much better than with quasi-independence. the lack of\nsymmetry in cell probabilities reflects slight marginal heterogeneity. the\nsubject-specific effects can be described using the model\u2019s parameter esti-\nmates, \u242d y \u242d s y0.672, \u242d y \u242d s y0.623, \u242d y \u242d s 0.122 . for in-\nstance, for a given subject the estimated odds of living in the south instead of\nthe west in 1985 were exp 0.122 s 1.13 times the odds in 1980. we\u2019ll see in\nchapter 12 that such subject-specific effects tend to be stronger than those in\ncorresponding marginal models, especially in tables like this with strong\nassociation.\n\n\u02c6x\n2\n\n\u02c6x\n1\n\n\u02c6x\n3\n\n\u02c6y\n3\n\n\u02c6y\n2\n\n\u02c6y\n1\n\na related application with matched samples is the study of occupational\nmobility. each observation pairs parent\u2019s occupation with child\u2019s occupation\n.\n\u017e\ngoodman 1979b; hout et al. 1987 .\n\n\u017e\n\n.\n\n4\n\n.\n\n\u017e\n\n10.4.5 marginal homogeneity and quasi-symmetry\nmarginal homogeneity is not equivalent to a loglinear model. however,\nquasi-symmetry is a useful model\nfor studying marginal homogeneity.\ncaussinus 1966 showed that symmetry is equivalent to quasi-symmetry and\nmarginal homogeneity holding simultaneously. we have seen that symmetry\nimplies both quasi-symmetry and marginal homogeneity. now we give\ncaussinus\u2019s argument for the converse, that the joint occurrence of quasi-\nsymmetry and marginal homogeneity implies symmetry.\nfrom 10.21 , if quasi-symmetry holds, \u2432 s \u2423 \u2424 \u2425 , where \u2425 s \u2425 )\nb ab\n\nb a\n\nab\n\nab\n\n\u017e\n\n.\n\na\n\n0 for all a - b. equivalently,\n\n\u2432 s \u2433 \u2426 ,\n\na ab\n\nab\n\nwhere \u2433 s \u2423 r\u2424 and \u2426 s \u2424 \u2424 \u2425 also satisfies \u2426 s \u2426 ) 0 for all\na - b. if there is also marginal homogeneity, then\n\nb ab\n\nb a\n\nab\n\nab\n\na\n\na\n\na\n\na\n\n\u2432 s \u2433 \u2426 s \u2433 \u2426 s \u2432 ,\nqj\njq\n\n\u00fd\n\n\u00fd\n\na a j\n\njb\n\nj\n\nb\n\na\n\nor\n\n\u2433 s\nj\n\n\u017e\n\n\u00fd\n\na\n\n\u2433 \u2426\na a j\n\n/\n\n\u017e\n\n\u00fd\n\nb\n\n\u2426 s\njb\n\n/\n\n\u017e\n\n\u00fd\n\na\n\n\u2433 \u2426\na a j\n\n/\n\n\u017e\n\n\u00fd\n\nb\n\n/\n\n\u2426 ,\nb j\n\nj s 1, . . . , i.\n\nj\n\n4\n\n\u0004\n\nthus, each \u2433 is a weighted average of \u2433 , with weights \u2426 r\u00fd \u2426 ) 0,\na s 1, . . . , i . any set \u2433 satisfying this must be identical. otherwise, there\nwould be a \u2433 that is no greater than any \u2433 but smaller than at least one,\nand hence it could not be a positive weighted average of all of them. but\nsince \u2433 are identical, \u2432 s \u2433 \u2426 s \u2433 \u2426 s \u2433 \u2426 s \u2432 , so symmetry\nholds. thus, a table that satisfies both quasi-symmetry and marginal homo-\n\nb ab\n\na ab\n\nb a\n\nb a\n\nab\n\na j\n\nb j\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n\u0004\n\n4\n\na\n\na\n\na\n\na\n\nb\n\nb\n\nj\n\n "}, {"Page_number": 444, "text": "symmetry, quasi-symmetry, and quasi-independence\n\n429\n\ngeneity also satisfies symmetry. since the converse holds,\n\nquasi-symmetry q marginal homogeneity s symmetry.\n\n\u017e\n\n10.24\n\n.\n\n.\n\nit follows that when quasi-symmetry qs holds, marginal homogeneity\nmh is equivalent to symmetry s , which is \u242d s \u242d , a s 1, . . . , i\n\u017e\nin the\nqs model. thus, conditional on quasi-symmetry, testing marginal homogene-\nity is equivalent to testing symmetry. a test of marginal homogeneity com-\npares fit statistics for the symmetry and quasi-symmetry models,\n\n\u0004 x\na\n\n\u017e .\n\ny\na\n\n4\n\n\u017e\n\n.\n\n<\n\n2\n\n2\n\n2\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\ng s qs s g s y g qs ,\n.\n\n.\n10.25\nwith df s i y 1. this is an alternative to approaches using marginal models\ndiscussed in section 10.3.3.\ntable 10.6 on migration from 1980 to 1985 has g s s 243.6 and g qs\n.\ns 3.0. the difference g s qs s 240.6 df s 3 shows extremely strong\nevidence of marginal heterogeneity. results are similar to those quoted in\nsection 10.3.4 for the likelihood-ratio test based on model 10.15 , for which\ng s 240.8, or the wald test, for which w s 236.5 both with df s 3 .\n.\n\n2\u017e .\n\n2\u017e\n\n2\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n2\n\n<\n\n10.4.6 ordinal quasi-symmetry model\nthe loglinear models presented so far for square tables treat classifications\nas nominal. with ordered categories, more parsimonious models are useful.\nlet u f \u2b48\u2b48\u2b48 f u denote ordered scores for both the row and columns. an\nordinal quasi-symmetry model is\n\n1\n\ni\n\nlog \u242e s \u242dq \u242d q \u242d q \u2424u q \u242d ,\n\n.\n10.26\nwhere \u242d s \u242d for all a - b. it is the special case of the quasi-symmetry\nmodel 10.19 in which\n\nb a\n\nab\n\nab\n\nab\n\n\u017e\n\n\u017e\n\n.\n\na\n\nb\n\nb\n\n\u242dy y \u242dx s \u2424u\n\nb\n\nb\n\nb\n\nhas a linear trend. symmetry is the special case \u2424s 0.\n\nthis model has logit representation,\n\nb\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nab\n\nb a\n\nfor a f b.\n\nlog \u2432 r\u2432 s \u2424 u y u\n\n.\n10.27\nthis is the special case of the linear logit model, logit \u2432 s \u2423q \u2424x, with\n\u2423s 0, x s u y u and \u2432 equal to the conditional probability of cell a,b ,\n.\ngiven response sequence a, b or b, a . the greater the value of \u2424 , the\ngreater the difference between \u2432 and \u2432 and hence between the marginal\ndistributions.\n\n\u017e\n<\n\nb a\n\nab\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\na\n\na\n\nb\n\n<\n\n "}, {"Page_number": 445, "text": "430\n\nmodels for matched pairs\n\nthe likelihood equations for ordinal quasi-symmetry are\n\n\u00fd\n\na\n\nu \u242e s u n\n\n\u00fd\n\naq\n\n\u02c6\n\na\n\na\n\na\n\n,\n\naq\n\n\u242e q \u242e s n q n\n\u02c6\n\n\u02c6\n\nb a\n\nab\n\nab\n\n\u00fd\n\nb\n\nb a\n\nu \u242e s u n\n\u02c6\nb qb\n\n\u00fd\n\nb qb\n\nb\n\n,\n\nfor\n\na - b.\n\nthe fitted marginal counts need not equal the observed marginal counts.\nhowever, dividing the first two equations by n shows that they have the same\nmeans.\n\nwhen \u2424/ 0, this model implies stochastically ordered margins. when\n\u2424) 0 \u2424- 0 , responses have a higher mean in the column row distri-\n.\nbution. like the ordinal marginal models section 10.3.1 ,\nthis model\nconcentrates the marginal effect on df s 1. a test of marginal homogeneity\nh : \u2424s 0 uses\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n0\n\nordinal quasi-symmetry q marginal homogeneity s symmetry.\n\nthe likelihood-ratio test statistic compares the deviance for symmetry and\nordinal quasi-symmetry.\n\n\u017e\nas binomial with n q n\n\none can fit this model by fitting 10.27 with logit model software: identify\n\u017e\nn , n\ntrials, and fit a logit model with no\nab\nintercept and predictor x s u y u . one can also fit 10.26 using iterative\nmethods for loglinear models.\n\nb a\n\nb a\n\nab\n\n.\n\n.\n\n\u017e\n\n.\n\na\n\nb\n\n2\n\n\u017e\n\n\u017e\n\n.\n\n10.4.7 premarital and extramarital sex revisited\nfor table 10.5 on attitudes toward premarital and extramarital sex, a cursory\nglance at the data reveals that the symmetry model is inadequate g s 402.2,\ndf s 6 . by comparison, quasi-symmetry fits well g s 1.4, df s 3 . the\n4\nsimpler model of ordinal quasi-symmetry also fits well: with scores 1, 2, 3, 4 ,\ng s 2.1 df s 5 .\n.\nthe ml estimate \u2424s y2.86. from 10.27 , the estimated probability that\noutcome on premarital sex is x categories more positive than the outcome on\nextramarital sex equals exp 2.86 x times the reverse probability. for instance,\nthe estimated probability that premarital sex is judged almost always wrong\nand extramarital sex is always wrong equals exp 2.86 s 17.4 times the\nestimated probability that premarital sex is always wrong and extramarital sex\nis almost always wrong.\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n2\n\n2\n\n10.4.8 other ordinal models for square tables\nfor ordered classifications, when symmetry does not hold, often either\n\u2432 ) \u2432 for all a - b, or \u2432 - \u2432 for all a - b. a generalization of\n\nab\n\nb a\n\nab\n\nb a\n\n "}, {"Page_number": 446, "text": "measuring agreement between observers\n\n431\n\nsymmetry with this property is the logit model\n\nlog \u2432 r\u2432 s \u2436\n\n\u017e\n\n.\n\nb a\n\nab\n\nfor a - b.\n\n\u017e\n\n10.28\n\n.\n\nit implies that for all a - b,\n\np y s a, y s b y - y s p y s b, y s a y ) y .\n.\n\u017e\n\n.\n\n\u017e\n\n<\n\n<\n\ni1\n\ni2\n\ni2\n\ni2\n\ni2\n\ni1\n\ni1\n\ni1\n\nthe pattern of probabilities for cells above the main diagonal is a mirror\nimage of the pattern for cells below it. this property is called conditional\nsymmetry mccullagh 1978 . problem 10.35 shows the corresponding loglin-\near model and its fit. symmetry is the special case \u2436s 0.\n\n.\n\n\u017e\n\nanother model generalizes quasi-independence. let u be ordered scores.\n\n\u0004\n\n4\n\na\n\nthe model\n\nlog \u242e s \u242dq \u242dx q \u242dy q \u2424u u q \u2426 i a s b\n\n\u017e\n\na\n\nb\n\na\n\nab\n\na\n\n.\n\n\u017e\n\n10.29\n\n.\n\npermits linear-by-linear association see 9.6 off the main diagonal. it is a\nspecial case of quasi-symmetry, and quasi-independence is the special case\n\u2424s 0. for equal-interval scores, it implies uniform local association, given\nthat responses differ. goodman 1979a called it quasi-uniform association.\nfor table 10.5 on opinions about premarital and extramarital sex, the\nconditional symmetry model has \u2436s y4.130 se s 0.451 . the estimated\nprobability that extramarital sex is considered more wrong are exp 4.13 s\n62.2 times the estimated probability that premarital sex is considered more\nwrong. the quasi-uniform association model has \u2424s 0.632 se s 0.106 . off\nthe main diagonal, the estimated local odds ratio equals exp 0.632 s 1.88.\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nb\nw\n\n\u017e\n\n.x\n\n10.5 measuring agreement between observers\n\nwe now discuss an application, analyzing agreement between two observers,\nthat uses matched-pairs models. we illustrate with table 10.8. this shows\nratings by two pathologists, labeled a and b, who separately classified 118\nslides regarding the presence and extent of carcinoma of the uterine cervix.\nthe rating scale has the ordered categories 1 negative, 2 atypical squa-\nmous hyperplasia, 3 carcinoma in situ, 4 squamous or invasive carcinoma.\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\nab\n\n10.5.1 agreement: departures from independence\nlet \u2432 denote the probability that observer a classifies a slide in category a\nand observer b classifies it in category b. then \u2432 is the probability that\nthey both choose category a, and \u00fd \u2432 is the total probability of agreement.\nperfect agreement occurs when \u00fd \u2432 s 1.\n\na aa\n\nwith subjective scales, agreement is less than perfect. analyses focus on\ndescribing strength of agreement and detecting patterns of disagreement.\n\na aa\n\naa\n\n "}, {"Page_number": 447, "text": "432\n\nmodels for matched pairs\n\ntable 10.8 diagnoses of carcinoma\n\npathologist a\n\n1\n\n2\n\n3\n\n4\n\n1\n22\n\u017e\n8.5\n5\ny0.5\n\u017e\n0\ny4.1\n\u017e\n0\ny3.3\n\u017e\n27\n\n.\n\n.\n\n.\n\n.\n\na\n\n.\n\npathologist b\n2\n2\ny0.5\n\u017e\n7\n\u017e\n3.2\n2\ny1.2\n\u017e\n1\ny1.3\n\u017e\n12\n\n3\n2\ny5.9\n\u017e\n14\ny0.5\n\u017e\n36\n\u017e\n5.5\n17\n\u017e\n0.3\n69\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\ntotal\n26\n\n26\n\n38\n\n28\n\n.\n\n.\n\n.\n\n4\n0\ny1.8\n\u017e\n0\ny1.8\n\u017e\n0\ny2.3\n\u017e\n10\n\u017e\n5.9\n10\n\n.\n\ntotal\n\n118\navalues in parentheses are standardized pearson residuals for the independence model.\nsource: n. s. holmquist, c. a. mcmahon, and o. d. williams, arch. pathol. 84: 334\u1390345\n\u017e\n1967 ; reprinted with permission from the american medical association. see also landis\n.\nand koch 1977 .\n\n.\n\n\u017e\n\n4\n\n\u0004\n\n4\n\n\u0004\n\nab\n\naq qb\n\nto the values n\n\nevaluations of agreement compare n\n\nagreement and association are distinct facets of the joint distribution. strong\nagreement requires strong association, but strong association can exist with-\nout strong agreement. if observer a consistently rates subjects one category\nhigher than observer b, strength of agreement is poor even though the\nassociation is strong.\nn rn pre-\ndicted under independence. that model is a baseline, showing the agreement\nexpected if no association existed between ratings. normally, it fits poorly if\n.\neven mild agreement exists, but its cell standardized residuals section 3.3.1\nshow patterns of agreement and disagreement. ideally, standardized residu-\nals are large positive on the main diagonal and large negative off that\ndiagonal. the sizes are influenced by sample size n, however, larger values\ntending to occur as n increases.\nthe independence model fits table 10.8 poorly g s 118.0, df s 9 .\n.\nthat table reports the standardized pearson residuals in parentheses. the\nlarge positive residuals on the main diagonal indicate that agreement for\neach category is greater than expected by chance, especially for the first\ncategory. off the main diagonal they are primarily negative. disagreements\noccurred less than expected under independence, although the evidence of\nthis is weaker for categories closer together. the most common disagree-\nments were observer b choosing category 3 and observer a instead choosing\ncategory 2 or 4.\n\n\u017e\n\n\u017e\n\n2\n\n10.5.2 using quasi-independence to analyze agreement\nmore complex models add components that relate to agreement beyond that\nexpected under independence. a useful generalization is quasi-independence\n\n "}, {"Page_number": 448, "text": "measuring agreement between observers\n\n433\n\ntable 10.9 fitted values for carcinoma diagnoses of table 10.8\n\npathologist b\n\na\n\npathologist a\n\n1\n\n2\n\n3\n\n4\n\n1\n22\n.\n\u017e\n22\n\u017e\n.\n22\n5\n\u017e\n2.4\n\u017e\n4.6\n0\n\u017e\n0.8\n\u017e\n0.4\n0\n\u017e\n1.9\n\u017e\n0.0\n\n1\n2\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n2\n2\n\u017e\n0.7\n\u017e\n2.4\n7\n\u017e .\n7\n\u017e .\n7\n\n\u017e\n1.2\n\u017e\n1.6\n1\n\u017e\n3.0\n\u017e\n1.0\n\n.\n.\n\n.\n.\n\n.\n.\n\n3\n2\n\u017e\n3.3\n\u017e\n1.6\n14\n\u017e\n16.6\n\u017e\n14.4\n36\n\u017e\n.\n36\n\u017e\n.\n36\n17\n\u017e\n13.1\n\u017e\n17.0\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n4\n0\n\u017e\n0.0\n\u017e\n0.0\n0\n\u017e\n0.0\n\u017e\n0.0\n0\n\u017e\n0.0\n\u017e\n0.0\n10\n\u017e\n.\n10\n\u017e\n.\n10\n\n.\n.\n\na1quasi-independence model; 2quasi-symmetry model.\n\n2\n\n.\n\n\u017e\n\n\u017e\n10.23 , which adds main-diagonal parameters \u2426 . for table 10.8, this\nmodel has g s 13.2 df s 5 . it fits much better than independence, but\nsome lack of fit remains. table 10.9 shows the fit.\n\nfor two subjects, suppose that each observer classifies one in category a\nand one in category b. the odds that the observers agree rather than\ndisagree on which is in category a and which is in category b equal\n\n.\n\n\u0004\n\n4\n\na\n\n\u2432 \u2432\n\u2436 s\nab \u2432 \u2432\n\naa\n\nbb\n\nab\n\nb a\n\ns\n\n\u242e \u242e\n\naa\n\nbb\n\n\u242e \u242e\n\nab\n\nb a\n\n.\n\n\u017e\n\n10.30\n\n.\n\nas \u2436 increases, the observers are more likely to agree for that pair of\ncategories. under quasi-independence,\n\nab\n\n\u2436 s exp \u2426 q \u2426 .\n.\n\n\u017e\n\nab\n\na\n\nb\n\n\u0004\n\n4\n\na\n\nlarger \u2426 represent stronger agreement. for instance, for table 10.8,\n\u2426 s 0.6 and \u2426 s 1.9, and \u2436 s 12.3. the degree of agreement also seems\n\u02c6\n2\nfairly strong for other pairs of categories.\n\n\u02c6\n23\n\n\u02c6\n3\n\n10.5.3 quasi-symmetry and agreement modeling\nfor table 10.8, the quasi-independence model shows some lack of fit. given\nthat the pathologists disagree, some association remains between ratings. for\nobserver agreement tables, this is common. quasi-symmetry 10.19 often fits\nmuch better, because it permits association. for table 10.8, it has g2 s 1.0\ndf s 2 . table 10.9 displays the fit. it is not unusual for tables to have many\n\u017e\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 449, "text": ".\n\n\u017e\n\nab\n\nab\n\nb a\n\nb a\n\n\u02c6\n\n\u02c6\n\n434\nmodels for matched pairs\nempty cells. when n q n s 0 for any pair such as categories 1 and 4 in\ntable 10.8 , the ml fitted values for quasi-symmetry in those cells must also\nbe zero since one of its likelihood equations is \u242e q \u242e s n q n . one\nshould eliminate those cells from the fitting process to get the proper\nresidual df value.\nunder quasi-symmetry, \u2436 s exp \u242d q \u242d y \u242d y \u242d , where \u242d s\n\u02c6\u242d . for categories 2 and 3 of table 10.8, for instance, \u2436 s 10.7.\n\nb a\nloglinear models directly address the association component of agree-\nment. the quasi-symmetry model also yields information about similarity of\nmarginal distributions. the simpler symmetry model that forces the margins\nto be identical fits table 10.8 poorly g s 39.2, df s 5 . the statistic\ng s qs s 39.2 y 1.0 s 38.2 df s 3 provides strong evidence of marginal\nheterogeneity. in table 10.8, differences in marginal proportions are substan-\ntial in each category but the first. the marginal heterogeneity is one reason\nthat the agreement is not stronger.\n\n\u02c6\nb a\n\u02c6\n23\n\n\u02c6\nbb\n\n\u02c6ab\n\n2\u017e\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\nb a\n\naa\n\nab\n\nab\n\nab\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\nmodels for agreement can take ordering of categories into account.\nconditional on observer disagreement, a tendency usually remains for high\n\u017e\nlow ratings by one observer to occur with relatively high low ratings by the\n.\nother observer see problem 10.41 .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2\n\n<\n\n10.5.4 kappa measure of agreement\nan alternative approach summarizes agreement with a single index. for\nnominal scales, the most popular measure is cohen\u2019s kappa cohen 1960 . it\ncompares the probability of agreement \u00fd \u2432 to that expected if the ratings\nwere independent, \u00fd \u2432 \u2432 , by\n\na aa\n\n\u017e\n\n.\n\na aq qa\n\n\u242cs\n\na aa\n\n\u00fd \u2432 y \u00fd \u2432 \u2432\na aq qa\n1 y \u00fd \u2432 \u2432\na aq qa\n\n.\n\nthe denominator equals the numerator with \u00fd \u2432 replaced by its maximum\npossible value of 1, corresponding to perfect agreement. kappa equals 0\nwhen the agreement merely equals that expected under independence. it\nequals 1.0 when perfect agreement occurs. the stronger the agreement, the\nhigher is \u242c, for given marginal distributions. negative values occur when\nagreement is weaker than expected by chance, but this rarely happens.\n\na aa\n\nfor multinomial sampling, the sample value \u242c has a large-sample normal\n\ndistribution. its estimated asymptotic variance fleiss et al. 1969 is\np q p\naq\n\n2 1 y p\n\u017e\n\naa\n\n.\n\n.\n\n\u017e\n\no\n\ne\n\n.\n\nqa\n\n.\n\n\u02c6\n\n\u017e\n\n2\u2434 \u242c s\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u00bdn\n\n1 p 1 y p\n\u017e\n1 y p\n.\n\no\n\u017e\n\no\n2\n\ne\n\nq\n\no\n\n2 p p y \u00fd p\na\n.\n\n\u017e\n\n3\n\ne\n\n1 y p\n\u017e\n\na\n\n\u00fd \u00fd p\nb\n\u017e\n\nab\n\np q p\nbq\n1 y p\n.e\n4\n\nqa\n\n2\n\n.\n\ny 4p\n\n2\ne\n\n5\n\n,\n\n\u017e\n\n1 y p\n\no\n\n2\n\n.\n\nq\n\n "}, {"Page_number": 450, "text": "435\n\ne\n\na\n\na\n\no\n\naa\n\np\n\naq qa\n\nand p s \u00fd p\n\nmeasuring agreement between observers\nwhere p s \u00fd p\n. it is rarely plausible that agreement\nis no better than expected by chance. thus, rather than testing h : \u242cs 0,\nit is more relevant to estimate strength of agreement by interval estimation\nof \u242c.\nfor table 10.8, p s 0.636 and p s 0.281. sample kappa equals 0.636 y\n0.281 r 1 y 0.281 s 0.493. the difference between observed agreement and\nthat expected under independence is about 50% of the maximum possible\ndifference. the estimated standard error is 0.057, so \u242c apparently falls\nroughly between 0.4 and 0.6, moderately strong agreement.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\no\n\n0\n\ne\n\n10.5.5 weighted kappa: quantifying disagreement\nkappa treats classifications as nominal. when categories are ordered, the\nseriousness of a disagreement depends on the difference between the ratings.\nfor nominal classifications also, some disagreements may be considered\nmore severe than others. the measure weighted kappa spitzer et al.\nsatisfying 0 f w f 1, with all w s 1 and all\n1967 uses weights w\nw s w to describe closeness of agreement. one possibility is w s 1 y\na y b r i y 1 , for which agreement is greater for cells nearer the main\n<\ndiagonal. fleiss and cohen 1973 suggested w s 1 y a y b r i y 1\n.24\n.\nthe weighted agreement is \u00fd \u00fd w \u2432 and weighted kappa is\n\n\u017e\naa \u0004\n\nb a\n\u017e\n\n.2\n\n.4\n\nab\n\nab\n\nab\n\nab\n\nab\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u0004\n\n4\n\n\u0004\n\n<\n\na\n\nb\n\nab\n\nab\n\n\u242c s\nw\n\n\u00fd \u00fd w \u2432 y \u00fd \u00fd w \u2432 \u2432\naq qb\n\nab\n\na\n\nb\n\nab\n\nb\n1 y \u00fd \u00fd w \u2432 \u2432\naq qb\n\nab\n\nab\n\na\n\na\n\nb\n\n.\n\ncontroversy surrounds the utility of kappa and weighted kappa, partly\nbecause their values depend strongly on the marginal distributions. the same\ndiagnostic rating process can yield quite different values, depending on the\nproportions of cases of the various types problem 10.40 . in summarizing a\ncontingency table by a single number, the reduction in information can be\nsevere. it is helpful to construct models providing more detailed investigation\nof the agreement and disagreement structure rather than to depend solely on\na summary index.\n\n\u017e\n\n.\n\n10.5.6 extensions to multiple observers\nwith several observers, ordinary loglinear models are not usually relevant.\ntheir description of agreement and association between two observers is\nconditional on ratings by the others. it is more relevant\nto study this\nmarginally, without conditioning on the other ratings. hence, for r ob-\nservers, modelling simultaneously the pairwise agreement and association\nstructure requires studying the\npairs of two-way marginal distributions\n\u017e\n.\nbecker and agresti 1992 .\n\n/2\n\n\u017e\n\nr\n\n "}, {"Page_number": 451, "text": "436\n\nmodels for matched pairs\n\nother approaches have also been used. for instance, generalizations of\nkappa summarize pairwise agreements or multiple agreements fleiss 1981,\nsec. 13.2; landis and koch 1977 . or, it may make sense to use a mixture\nmodel that assumes latent classes of subjects for whom the observers agree\nand subjects for whom they disagree. such an analysis is shown in section\n13.1.2.\n\n.\n\n\u017e\n\n10.6 bradley\u2013terry model for paired preferences\n\n.\n\n\u017e\n\nsometimes, categorical outcomes result from pairwise evaluations. a com-\nmon example is athletic competitions, when the outcome for a team or player\nconsists of categories win, lose . another example is pairwise comparison\nof product brands, such as two brands of wine of some type. when a wine\ncritic rates i brands of sauvignon blanc, it might be difficult to establish an\noutright ranking, especially if i is large. however, for any given pair, the\ncritic could probably state a preference after tasting them at the same\noccasion. an overall ranking of the wines could then be based on the\npairwise preferences. we present a model for this in this section.\n\n10.6.1 bradley\u2013terry model\nbradley and terry 1952 proposed a logit model for paired evaluations.\nlet \u2338 denote the probability that a is preferred to b. suppose that\n\u2338 q \u2338 s 1 for all pairs; that is, a tie cannot occur. the bradley\u1390terry\nmodel is\n\nb a\n\nab\n\nab\n\n\u017e\n\n.\n\nlog\n\n\u2338\n\n\u2338\n\nab\n\nb a\n\nalternatively,\n\ns \u2424 y \u2424 .\n\na\n\nb\n\n\u017e\n\n10.31\n\n.\n\n\u2338 s exp \u2424 r exp \u2424 q exp \u2424 .\nthus, \u2338 s when \u2424 s \u2424 and \u2338 ) when \u2424 ) \u2424 .\n\nab\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\na\n\na\n\nb\n\n1\n2\n\n1\n2\n\nab\n\nidentifiability requires a constraint such as \u2424 s 0 or \u00fd exp \u2424 s 1.\n.\ni y 1\n.\n\nprobabilities \u2338\n\nfor a - b by\n\n\u02c6\u017e\na\n\u017e\n\n\u017e\u0004\n\n.\n\n4\n\na\n\ni\n\na\n\nb\n\nab\n\na\n\nab\n\nsince the model describes\nparameters, residual df s y i y 1 .\n.\n\n\u017e\n\ni\n\n/2\n\nb\n\n\u017e\n/2\n\ni\n\n\u017e\n\nab\n\nfor a - b,\nab\n\nlet n denote the sample number of evaluations, with a\ntimes and b preferred n s n y n\npreferred n\ntimes. a square\ncontingency table with empty cells on the main diagonal summarizes results.\nwhen the n comparisons are independent with probability \u2338 for each,\nhas a bin n , \u2338 distribution. if evaluations for different pairs are also\nn\nab\nindependent, ordinary methods for logit models apply for fitting the model.\n\nab\n\u017e\n\nb a\n\nab\n\nab\n\nab\n\nab\n\nab\n\n.\n\n "}, {"Page_number": 452, "text": "bradley\u1390terry model for paired preferences\n\n437\n\ntable 10.10 results of 1987 season for american league baseball teams\n\nlosing team\n\na\n\n.\n\n.\n.\n\nmilwaukee detroit toronto new york boston cleveland baltimore\n.\n.\n.\n.\n.\n.\n\nwinning\nteam\n\u017e\nmilwaukee\n7 8.0\n\u017e\ndetroit\n11 7.6\n\u017e\ntoronto\n7 7.1\n\u017e\nnew york\n6 7.0\n\u138f\nboston\n.\n\u017e\ncleveland\n6 5.1\n.\n\u017e\nbaltimore\n1 3.2\navalues in parentheses represent the fit of the bradley\u1390terry model.\n.\nsource: american league red book, 1988 st. louis, mo: sporting news publishing co.\n\n11 10.8\n9 10.5\n12 10.2\n10 10.1\n\u017e\n12 9.8\n\u017e\n6 8.6\n\u138f\n\n\u017e\n9 7.4\n\u017e\n7 7.0\n\u138f\n\u017e\n6 6.3\n\u017e\n6 5.9\n\u017e\n5 4.6\n\u017e\n1 2.8\n\n\u017e\n7 7.6\n\u017e\n5 7.1\n\u017e\n7 6.7\n\u138f\n\u017e\n7 6.0\n\u017e\n6 4.7\n\u017e\n3 2.9\n\n\u017e\n9 9.2\n\u017e\n9 8.8\n\u017e\n8 8.4\n\u017e\n7 8.3\n\u017e\n7 7.9\n\u138f\n\u017e\n7 4.4\n\n\u017e\n7 7.0\n\u138f\n\u017e\n6 6.0\n\u017e\n8 5.9\n\u017e\n2 5.4\n\u017e\n4 4.2\n\u017e\n4 2.5\n\n\u138f\n\u017e\n6 6.0\n\u017e\n4 5.6\n\u017e\n6 5.4\n\u017e\n6 5.0\n\u017e\n4 3.8\n\u017e\n2 2.2\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n\u017e\n\u017e\n\u017e\n\u017e\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n\n\u017e\n\n10.6.2 home team advantage in baseball\ntable 10.10 shows results of the 1987 season for the seven baseball teams in\nthe eastern division of the american league. for instance, of games\nbetween boston and new york, boston won 7 and new york won 6. table\n10.10 shows the population of regular-season games. we regard this as a\nsample estimate of a conceptual distribution representing the long-run per-\nformance of teams as constituted in 1987.\ns 21 indepen-\ndent binomial samples, using an appropriate model matrix and no intercept\ne.g., for sas, see table a.19 . the model fits adequately g s 15.7,\n\u017e\ndf s 15 . table 10.10 contains the fitted values \u242e . table 10.11 displays\nthe sample proportion of games each team won and the model estimates of\n\u2424 setting \u2424 s 0 and exp \u2424 setting \u00fd exp \u2424 s 1 . when boston\n\u02c6\n\u0004\na\nplayed new york, the estimated probability that boston won is\n\nwe fitted the bradley\u1390terry model as a logit model for\n\n\u02c6 ab\n\u02c6\na\n\n/2\n\n.4 w\n\n4 \u017e\n\n\u02c6\n7\n\n\u02c6\na\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nx\n\n\u0004\n\n4\n\n\u0004\n\n7\n\na\n\n2\n\n\u2338 s exp \u2424 exp \u2424 q exp \u2424 s 0.46.\n\u02c6\n\n\u017e\n\n\u02c6\n5\n\n.\n\n\u017e\n\n\u02c6\n5\n\n\u02c6\n4\n\n.\n\n54\n\n.\n\u02c6\na\n\n\u017e\n\u02c6\na\n\nthe standard error of each \u2424 and of each \u2424 y \u2424 is about 0.3, so not\nmuch evidence exists of a difference among the top five teams.\n\n\u02c6\nb\n\ntable 10.11 results of fitting bradley\u2013terry models to baseball data\n\nteam\nmilwaukee\ndetroit\ntoronto\nnew york\nboston\ncleveland\nbaltimore\n\nwinning\npercentage\n\n64.1\n60.2\n56.4\n55.1\n51.3\n39.7\n23.1\n\n.\n\n\u02c6\n\u2424\ni\n\n\u017e\n10.31\n1.58\n1.44\n1.29\n1.25\n1.11\n0.68\n0.00\n\n\u017e\n\n\u02c6\n.\nexp \u2424\ni\n.\n\u017e\n10.31\n0.218\n0.189\n0.164\n0.158\n0.136\n0.089\n0.045\n\n\u017e\n\n\u02c6\n.\nexp \u2424\ni\n.\n\u017e\n10.32\n0.220\n0.190\n0.164\n0.157\n0.137\n0.088\n0.044\n\n "}, {"Page_number": 453, "text": "438\n\nmodels for matched pairs\n\ntable 10.12 wins rrrrr losses by home and away team, 1987\n\naway team\n\nhome team milwaukee detroit toronto new york boston cleveland baltimore\nmilwaukee\ndetroit\ntoronto\nnew york\nboston\ncleveland\nbaltimore\n\n\u138f\n3-3\n2-5\n3-3\n5-1\n2-5\n2-5\n\n4-3\n\u138f\n4-3\n5-1\n2-5\n3-3\n1-5\n\n4-3\n4-3\n2-4\n\u138f\n4-2\n4-3\n2-4\n\n6-1\n6-0\n4-3\n4-3\n\u138f\n4-2\n1-6\n\n4-2\n6-1\n4-2\n4-2\n5-2\n\u138f\n3-4\n\n6-0\n4-3\n6-0\n6-1\n6-0\n2-4\n\u138f\n\n.\nsource: american league red book, 1988 st. louis, mo: sporting news publishing co. .\n\n4-2\n4-2\n\u138f\n2-5\n3-3\n3-4\n1-6\n\u017e\n\nthis model does not recognize which team is the home team. most sports\nhave a home field advantage: a team is more likely to win when it plays at its\nhome city. table 10.12 contains results for the 1987 season according to the\n\u017e\nhome team, away team classification. for instance, when boston was the\nhome team, it beat new york 4 times and lost 2 times; when new york was\nthe home team, it beat boston 4 times and lost 3 times. now for all a / b, let\n\u2338* denote the probability that team a beats team b, when a is the home\nteam. consider logit model\n\nab\n\n.\n\nlog\n\n\u2338*ab\n\n1 y \u2338*ab\n\ns \u2423q \u2424 y \u2424 .\n.\n\n\u017e\n\na\n\nb\n\n\u017e\n\n10.32\n\n.\n\n2\n\nw\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e .\n\n\u017e .x\n\nwhen \u2423) 0, a home field advantage exists. the home team of two evenly\nmatched teams has probability exp \u2423r 1 q exp \u2423 of winning.\nfor table 10.12, model 10.32 describes 42 binomial distributions with 7\nparameters. it has g s 38.6 df s 35 . table 10.11 displays exp \u2424 , which\nare similar to those obtained previously. the estimate of the home-field\nparameter is \u2423s 0.302. for two evenly matched teams, the home team had\nestimated probability 0.575 of winning. when boston played new york, the\nestimated probability of a boston win was 0.54 at boston and 0.39 at new\nyork.\n\n\u02c6\na\n\nmodel 10.32 is a useful generalization of the bradley\u1390terry model\nwhenever an order effect exists. for instance, in pairwise taste evaluations,\nthe product tasted first may have a slight advantage.\n\n\u02c6\n\n.4\n\n\u017e\n\n.\n\n\u017e\n\n\u0004\n\n10.6.3 bradley\u2013terry model and quasi-symmetry\nfienberg and larntz 1976 showed that the bradley\u1390terry model is a logit\nformulation of the quasi-symmetry model 10.19 . for quasi-symmetry, given\nthat an observation is in cell a, b or b, a , the logit of the conditional\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n "}, {"Page_number": 454, "text": "marginal and quasi-symmetry models for matched sets\n\n439\n\nprobability of cell a, b equals\n\n\u017e\n\n.\n\nlog\n\n\u242e\n\nab\n\n\u242e\n\nb a\n\ns \u242dq \u242d q \u242d q \u242d y \u242dq \u242d q \u242d q \u242d\nx y\nb a\n\nx y\nab\n\nx\nb\n\nx\na\n\n\u017e\n\n.\n\n\u017e\n\ny\na\n\ny\nb\n\n.\n\ns \u242dx y \u242dy y \u242dx y \u242dy s \u2424 y \u2424 ,\n\n.\n\na\n\n\u017e\n\na\n\na\n\nb\n\nb\n\nb\n\n\u017e\n\u02c6x\na\n\n.\n\u02c6y\na\n\nwhere \u2424 s \u242d y \u242d . estimates \u242d and \u242d for quasi-symmetry yield \u2424\n\u02c6\na\nfor the bradley\u1390terry model.\n\nx\na\n\ny\na\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\na\n\n4\n\n10.6.4 extensions to ties and ordinal evaluations\nthe bradley\u1390terry model extends to ordinal comparisons, such as the\nevaluation scale much better, slightly better, the same, slightly worse, much\nworse in comparing two products. with cumulative logits and an i-category\nevaluation scale, let y denote the response for a comparison of a with b.\nthe model is\n\nab\n\n\u017e\n\n.\n\nlogit p y f j s \u2423 q \u2424 y \u2424 .\n.\n\n\u017e\n\n.\n\n\u017e\n\na\n\nb\n\nab\n\nj\n\nw\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\nab\n\nsince p y f j s p y ) i y j s 1 y p y f i y j ,\n.\nlogit p y f j s y logit p y f i y j\n\u2423 s y\u2423 .\n\nthe most common ordered preference scale is win, tie,\n\n\u017e\nthat\nfollows\n.x\n. thus, necessarily, \u2423 s y\u2423 .\niyj\nj\nlose . then,\n\nb a\nw\n\nit\n\nb a\n\nb a\n\nab\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nx\n\n1\n\n2\n\n10.7 marginal and quasi-symmetry models for\nmatched sets*\n\nmethods for matched pairs extend to matched sets. here we present mainly\nthe loglinear modeling approach; in chapters 11 and 12 we present exten-\nsions of the marginal and conditional logit modeling approaches.\n\n1\n\n\u017e\n\n10.7.1 marginal homogeneity, complete symmetry, and quasi-symmetry\ndenote the t responses in each matched set. with i\nlet y , y , . . . , y\nt\nresponse categories, a contingency table with i t cells summarizes the possi-\nble outcomes. let i s i , . . . , i\ndenote the cell having y s i , t s 1, . . . , t.\nlet \u2432 s p y s i , t s 1, . . . , t , and let \u242e s n\u2432 . then\n\n.\n.\n\n.\n\n\u017e\n\n\u017e\n\nt\n\n2\n\n1\n\nt\n\nt\n\ni\n\nt\n\nt\n\ni\n\np y s j s \u2432\n\u017e\n\n.\n\nt\n\nq\u2b48 \u2b48 \u2b48qjq\u2b48 \u2b48 \u2b48q\n\ni\n\n,\n\nwhere the j subscript is in position t, and p y s j ,\n.\nmarginal distribution for y .t\n\n\u017e\n\n\u0004\n\nt\n\nj s 1, . . . , i\n\n4\n\nis the\n\n "}, {"Page_number": 455, "text": "440\n\nmodels for matched pairs\n\nthis t-way table satisfies marginal homogeneity if\n\np y s j s p y s j s \u2b48\u2b48\u2b48 s p y s j\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\nfor j s 1, . . . , i.\n\nt\n\n1\n\n2\n\nit satisfies complete symmetry if\n\n\u2432 s \u2432i\n\nj\n\nfor any permutation j s j , . . . , j\n.\n. complete symmetry\nimplies marginal homogeneity, but the converse does not hold except when\nt s i s 2.\n\nof i s i , . . . , i\n\n\u017e\n\n.\n\n\u017e\n\nt\n\nt\n\n1\n\n1\n\ncomplete symmetry is a loglinear model. one representation is\n\nlog \u242e s \u242d\n\ni\n\nab . . . m\n\n,\n\n\u017e\n\nwhere a is the minimum of\ni , . . . , i\n1\nthe maximum. in a three-way table, for instance,\nlog \u242e s \u242d . the number of \u242d\nof selecting t out of i items with replacement, which is\nresidual df s i y\n\n.\n, b is the next smallest, . . . , and m is\nlog \u242e s log \u242e s\nparameters is the number of ways\n. thus,\n\n\u017e\n.\nhaberman 1978, p. 518 .\n\ni q t y 1\n\ni q t y 1\n\nab . . . m\n\n122\n\n212\n\n221\n\n122\n\n\u017e\n\n/\n\nt\n\n\u0004\n\n4\n\nt\n\nt\n\n\u017e\n\nt\n\n/\n\nan i t table satisfies quasi-symmetry if\n\nlog \u242e s \u242d q \u242d q \u2b48\u2b48\u2b48 q\u242d q \u242d\n\ni\n\n1i\n\n1\n\nt i\n\nt\n\n2 i\n\n2\n\nab . . . m\n\n\u017e\n\n10.33\n\n.\n\nab . . . m\n\nwhere \u242d\nis defined as in the complete symmetry model. it has symmet-\nric association and higher-order interaction terms, but permits each single-\nfactor marginal distribution to have its own parameters. identifiability re-\nquires constraints such as \u242d s 0 for each t. one set of main-effect terms is\ni y 1 t y 1 more parameters\nredundant problem 10.31 . this model has\nthan complete symmetry. it is fitted using iterative methods.\n\n.\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nt i\n\nfor ordinal responses, a simpler model with quantitative main effects uses\n\n\u0004\nordered scores u . the ordinal quasi-symmetry model is\nlog \u242e s \u2424 u q \u2424 u q \u2b48\u2b48\u2b48 q\u2424 u q \u242d\n\n4\n\na\n\ni\n\n1\n\ni\n\n1\n\n2\n\ni\n\n2\n\nt i\n\nt\n\nab . . . m\n\nt\n\nwhere one can set \u2424 s 0. complete symmetry is the special case \u2424 s\n\u2b48\u2b48\u2b48 s \u2424 .t\n\nwhen quasi-symmetry 10.33 or ordinal quasi-symmetry holds, marginal\nhomogeneity is equivalent to complete symmetry. marginal heterogeneity\noccurs if quasi-symmetry qs holds but complete symmetry s does not.\nthe statistic\n\n\u017e .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n1\n\ng s qs s g s y g qs\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n2\n\n2\n\n2\n\n<\n\n.\n\n "}, {"Page_number": 456, "text": "marginal and quasi-symmetry models for matched sets\n\n441\n\ntests marginal homogeneity. under complete symmetry, it is asymptotically\nchi-squared with df s i y 1 t y 1 . the corresponding test for the ordinal\nquasi-symmetry model has df s t y 1 .\n.\n\n.\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e .\n\n10.7.2 attitudes toward legalized abortion example\nrefer to table 10.13. subjects indicated whether they support legalized\nabortion in three situations: 1 if the family has a very low income and\ncannot afford any more children, 2 when the woman is not married and\ndoes not want to marry the man, and 3 when the woman wants it for any\nreason. the table also classifies subjects by gender, resulting in a 2 4 table.\nlet \u242e denote the expected frequency for gender g 1 s female; 0 s\nfor the three questions. consider the\n\n.\nmale with response sequence h, i, j\nmodel\n\n\u017e .\n\n\u017e .\n\ng hi j\n\n\u017e\n\n.\n\n\u017e\n\nlog \u242e s \u2424g q \u242d ,\n\ng hi j\n\nabc\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\ni,\n\n222\n\n\u017e\n\u017e\n\n\u017e\n.\n\n\u017e\n.\n\n112\n.\n\n122\n\u017e\n\nwhere the interaction term is \u242d when h, i, j s 1, 1, 1 , \u242d when h, i, j\n.\n111\ns 1, 1, 2 or 1, 2, 1 or 2, 1, 1 , \u242d when h, i, j s 1, 2, 2 or 2, 1, 2 or\n.\nj s 2, 2, 2 . this model implies the same\n\u017e\n.\n2, 2, 1 , and \u242d when h,\ncomplete symmetry pattern of probabilities for each gender. its fit has\ng2 s 39.2 with df s 11.\nadding main-effect terms for the three issues implies the same quasi-sym-\nmetric pattern for each gender. it fits much better, having g2 s 10.2 with\ndf s 9. thus, it seems plausible to assume a symmetric association structure.\nin fact, the loglinear model with only two-factor association terms has fitted\nlog odds ratios of 3.2 for items 1 and 2, 2.6 for items 1 and 3, and 3.3 for\nitems 2 and 3.\none can test marginal homogeneity, given gender, by the likelihood-ratio\nstatistic 39.2 y 10.2 s 29.0, with df s 2. an analysis of the main-effect terms\nin the quasi-symmetry model shows greater support for legalized abortion\nwhen the family has a low income and cannot afford any more children than\nin the other two instances.\n\ntable 10.13 support for legalizing abortion in three situations, by gender\n\na\n\n\u017e\n.\n2, 1, 2\n\n\u017e\n.\n2, 1, 1\n\n\u017e\n.\n1, 1, 1\n342\n440\n\nsequence of responses on the three items\n\u017e\n\u017e\n.\n.\n2, 2, 1\n1, 1, 2\n\ngender\nmale\nfemale\na\nitems are 1 if the family has a very low income and cannot afford anymore children, 2 when\nthe woman is not married and does not want to marry the man, and 3 when the woman wants it\nfor any reason. 1, yes; 2, no.\nsource: data from 1994 general social survey, national opinion research center.\n\n\u017e\n.\n2, 2, 2\n356\n457\n\u017e .\n\n\u017e\n.\n1, 2, 1\n\n\u017e\n.\n1, 2, 2\n\n21\n18\n\n11\n14\n\n32\n47\n\n26\n25\n\n6\n14\n\n19\n22\n\n\u017e .\n\n\u017e .\n\n "}, {"Page_number": 457, "text": "442\n\nmodels for matched pairs\n\n1\n\nt\n\nt\n\nh\n\n1\n\n1\n\nh\n\nh\n\nt\n\n\u017e\n\n\u017e\n\ntable, p y s j , . . . , y s j\n\n10.7.3 types of marginal symmetry\na general type of symmetry for i t tables has marginal homogeneity and\n.\ncomplete symmetry as special cases. for an i\n,\nwhere h is between 1 and t, is a h-dimensional marginal probability, h s 1\ngiving single-variable marginal probabilities. there is hth-order marginal\nsymmetry if for all h-tuples j s j , . . . , j\n.\n, this probability is the same for\neach permutation of j and for all combinations t s t , . . . , t\nof h of the t\nresponses.\nfor h s 1, first-order marginal symmetry is marginal homogeneity. sec-\nond-order marginal symmetry occurs if for all t and u, p y s a, y s b is\nthe same and the equality holds for all pairs of outcomes a, b . in other\nwords, the two-way marginal tables exhibit symmetry, and they are identical.\nt th-order marginal symmetry in an i t table is complete symmetry.\n\nwhen hth-order symmetry holds, ith-order marginal symmetry holds for\nany i - h. for instance, complete symmetry implies second-order marginal\nsymmetry, which itself implies marginal homogeneity. although this hierarchy\nis mathematically attractive, the higher-order symmetries are usually too\nrestrictive to fit well in practice.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nu\n\nh\n\n1\n\nt\n\n10.7.4 marginal models: multiway tables\nin practice, usually the form of the joint distribution is of secondary interest.\nresearch questions pertain instead to the marginal distributions. the\nmarginal models of section 10.3 for matched pairs extend to matched sets.\nfor instance, with ordinal classifications, a cumulative logit model is\n\nlogit p y f j s \u2423 q \u2424 ,\n\n\u017e\n\n.\n\nj\n\nt\n\nt\n\nj s 1, . . . , i y 1,\n\nt s 1, . . . , t .\n\n\u017e\n\n10.34\n\n.\n\nin the next chapter we study marginal models in more general contexts,\nextending the analyses of this chapter to incorporate matched sets and\nexplanatory variables.\n\nnotes\n\nsection 10.1: comparing dependent proportions\n\n\u017e\n\n.\n\n10.1. miettinen 1969 generalized the mcnemar test to case\u1390control sets having several\ncontrols per case. the table 10.2 representation is then useful. each of n matched\n.\nsets forms a stratum of a 2 = 2 = n table with one observation in column 1 the case\n.\nand several observations in column 2 the controls .\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\naltham 1971 and ghosh et al. 2000 presented bayesian analyses for binary\nmatched pairs. copas 1973 , gart 1969 , kenward and jones 1994 , and miettinen\n\u017e\n1969 studied generalizations of matched-pairs designs. with some approaches ghosh\n.\net al. 2000; liang and zeger 1988; suissa and shuster 1991 ,\ninferences about\nmarginal homogeneity also use the main-diagonal observations.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n "}, {"Page_number": 458, "text": "notes\n\n443\n\nsection 10.4: symmetry, quasi-symmetry, and quasi-independence\n\nb\n\na\n\n.\n\nab\n\n.\n.\n\n\u017e\n\u017e\n\n.\n10.2. for other discussion of quasi-symmetry, see darroch 1981 and mccullagh 1982 .\nthe term quasi-independence originated in goodman 1968 . a more general defini-\ntion of it is \u2432 s \u2423 \u2424 for some fixed set of cells. see caussinus 1966 , fienberg\n\u017e\n1970b, 1972 , and goodman 1968 . caussinus used the concept to analyze tables that\ndeleted a certain set of cells from consideration, and goodman used it in earlier\nanalyses of social mobility. altham 1975 used it with triangular tables, for which\nobservations occur only above or only below the main diagonal. stigler 1999, chap.\n.19 summarized early uses, including karl pearson\u2019s handling in 1913 of a triangular\narray. booth and butler 1999 and smith et al. 1996 discussed exact tests for\nsquare-table models.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n10.3. the effect \u2424 in ordinal quasi-symmetry relates to the occasion effect in a subject-\nspecific adjacent-categories-logit model agresti 1993 . conditional symmetry is a\nspecial case of diagonals-parameter symmetry,\n\n\u017e\n\n.\n\nlog \u2432 r\u2432 s \u2436\n\n\u017e\n\n.\n\nb a\n\nab\n\nbya\n\n,\n\na - b.\n\n.\nsee goodman 1979b, 1985 and hout et al. 1987 .\n\n\u017e\n\n.\n\n\u017e\n\n10.4. in some applications a table is a priori symmetric or independent, but one can observe\n.\nonly the pair\nj rather than their order, thus leading to an upper-triangular table.\n.\n\u017e\nsee khamis 1983 for examples and ml fitting of models for such three-way tables\nthat are symmetric within layers.\n\ni,\n\n\u017e\n\nsection 10.5: measuring agreement between obser\u00a9ers\n\n\u017e\n\n10.5. kappa and weighted kappa relate to the intraclass correlation, a measure of interrater\n.\nreliability for interval scales fleiss 1981; fleiss and cohen 1973; kraemer 1979 .\nbanerjee et al. 1999 and fleiss 1981, chap. 13 reviewed kappa and its generaliza-\n.\ntions. see becker and agresti 1992 , goodman 1979b , tanner and young 1985 ,\nand problem 10.41 for examples of modeling agreement with loglinear models.\ndarroch and mccloud 1986 showed that quasi-symmetry has an important role in\nagreement modeling.\n\n.\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nsection 10.6: bradley\u2013terry model for paired preferences\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n10.6. zermelo 1929 proposed a model that is equivalent to the bradley\u1390terry model.\n.\nluce 1959 provided an axiomatic basis for it. mosteller 1951 and thurstone 1927\nproposed an analogous model with probit link. an interesting interview of ralph\nbradley by m. hollander stat. sci. 16: 75\u1390100, 2001 discussed food-tasting applica-\ntions that motivated its development. for extensions, see bradley 1976 . fienberg and\nlarntz 1976 and imrey et al. 1976 related it to quasi-independence. dittrich et al.\n\u017e\n1998 allowed covariates. matthews and morris 1995 gave an application with a\nfactorial design, ties, and allowance for dependence among judgments. bockenholt\nand dillon 1997 modeled dependence with ordinal preferences. david 1988 and\nimrey 1998 surveyed paired preference methods.\n\n\u017e\n.\n\n\u00a8\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 459, "text": "444\n\nmodels for matched pairs\n\ntable 10.14 data for problem 10.1\n\nlet patient die\n\nsuicide\nyes\nno\n\nyes\n1097\n203\n\nno\n90\n435\n\nsource: 1994 general social survey, national opinion re-\nsearch center.\n\nproblems\n\napplications\n\n10.1 table 10.14 shows results when subjects were asked \u2018\u2018do you think a\nperson has the right to end his or her own life if this person has an\nincurable disease?\u2019\u2019 and \u2018\u2018when a person has a disease that cannot be\ncured, do you think doctors should be allowed to end the patient\u2019s life\nby some painless means if the patient and his family request it?\u2019\u2019 the\ntable refers to these variables as \u2018\u2018suicide\u2019\u2019 and \u2018\u2018let patient die.\u2019\u2019\na. compare the marginal proportions using a confidence interval.\nb. perform mcnemar\u2019s test, and interpret.\nc. find the conditional ml estimate of \u2424 for model 10.8 . interpret.\n\n\u017e\n\n.\n\n10.2 refer to table 8.16 and problem 8.1. treat the data as matched pairs\non opinion, stratified by gender. testing independence for the 2 = 2\ntable using entries 6, 160 in row 1 and 11, 181 in row 2 tests\nequality of \u2424 for logit model 10.8 for each gender. explain why.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e .\n\n10.3 a crossover experiment with 100 subjects compares two drugs for\ntreating migraine headaches. the response scale is success 1 or\nfailure 0 . half the study subjects, randomly selected, used drug a\nthe first time they had a headache and drug b the next time. for\nthem, 6 had outcomes 1, 1 for a, b , 25 had outcomes 1, 0 , 10\nhad outcomes 0, 1 , and 9 had outcomes 0, 0 . for the 50 subjects\nwho took the drugs in the reverse order, 10 were 1, 1 for a, b , 20\n.\nwere 1, 0 , 12 were 0, 1 , and 8 were 0, 0 .\na. ignoring treatment order, compare the success probabilities for the\n\n\u017e .\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ntwo drugs. interpret.\n\nb. mcnemar\u2019s test uses only the pairs of outcomes that differ. for\nthis study, table 10.15 shows such data from both treatment\norders. testing independence for this table tests whether success\nrates are identical for the treatments gart 1969 . explain why.\nanalyze these data, and interpret.\n\n\u017e\n\n.\n\n "}, {"Page_number": 460, "text": "problems\n\n445\n\ntable 10.15 data for problem 10.3\n\ntreatment\norder\na, then b\nb, then a\n\ntreatment that is better\nfirst\nsecond\n25\n12\n\n10\n20\n\n.\n\n10.4 a case\u1390control study has 8 pairs of subjects. the cases have colon\ncancer, and the controls are matched with the cases on gender and\nage. a possible explanatory variable is the extent of red meat in a\nsubject\u2019s diet, measured as \u2018\u20181 s high\u2019\u2019 or \u2018\u20180 s low.\u2019\u2019 the case, con-\n.\n\u017e\ntrol observations on this were 1, 1 for 3 pairs, 0, 0 for 1 pair, 1, 0\nfor 3 pairs, and 0, 1 for 1 pair.\na. cross-classify the 8 pairs in terms of diet 1 or 0 for the case\nagainst diet 1 or 0 for the control. call this table a. display the\n2 = 2 = 8 table with eight partial tables relating diet 1 or 0 to\nresponse case or control\n\nfor the 8 pairs. call this table b.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nb. calculate the mcnemar z 2 for table a and the cmh statistic for\n\ntable b. compare.\n\nc. show that the mantel\u1390haenszel estimate of a common odds ratio\n\nfor table b is identical to n rn\n\n12\n\n21\n\nfor table a.\n\nd. for table b with pairs deleted in which the case and the control\nthe cmh statistic and the\n\nhad the same diet,\nmantel\u1390haenszel odds ratio estimate do not change.\n\nshow that\n\ne. this sample size is small for large-sample tests. use the binomial\ndistribution with table a to find the exact p-value for testing\nmarginal homogeneity against the alternative hypothesis of a higher\nincidence of colon cancer for the high-red-meat diet.\n\n10.5 each week variety magazine summarizes reviews of new movies by\ncritics in several cities. each review is categorized as pro, con, or\nmixed, according to whether the overall evaluation is positive, nega-\ntive, or a mixture of the two. table 10.16 summarizes the ratings from\n\ntable 10.16 data for problem 10.5\n\nsiskel\ncon\nmixed\npro\n\ncon\n24\n8\n10\n\nebert\nmixed\n\n8\n13\n9\n\npro\n13\n11\n64\n\nsource: a. agresti and l. winner, chance 10: 10\u139014\n\u017e\n.\nreprinted with permission, copyright 1997 by the\n1997 ,\namerican statistical association.\n\n "}, {"Page_number": 461, "text": "446\n\nmodels for matched pairs\n\napril 1995 through september 1996 for chicago film critics gene\nsiskel and roger ebert.\na. fit the symmetry model, quasi-independence model, and quasi-\n\nsymmetry model. interpret.\n\nb. test marginal homogeneity using models, and interpret.\nc. analyze these data using agreement models andror measures of\n\nagreement.\n\n10.6 refer to table 10.5. fit the ordinal quasi-symmetry model using\nu s 1 and u s 4 and picking u and u that are unequally spaced\n1\nbut represent sensible choices. compare results and interpretations to\nthose in sections 10.3.2 and 10.4.7.\n\n4\n\n2\n\n3\n\n10.7 refer to all four items in table 8.19.\n\na. fit\n\nthe complete symmetry and quasi-symmetry models. test\n\nmarginal homogeneity. interpret.\n\nb. fit the ordinal quasi-symmetry model. test marginal homogeneity.\n\ninterpret the effects.\n\n10.8 table 10.17 shows subjects\u2019 purchase choice of instant decaffeinated\n\ncoffee at two times.\n\na. fit the symmetry model and use residuals to analyze changes.\nb. test marginal homogeneity. show that the small p-value reflects a\ndecrease in the proportion choosing high point and an increase in\nthe proportion choosing sanka, with no evidence of change for the\nother coffees.\n\nc. show that quasi-independence has g s 13.8 df s 11 . interpret,\n\n\u017e\nand suggest other analyses that might be useful.\n\n.\n\n2\n\ntable 10.17 data for problem 10.8\n\nsecond purchase\n\nfirst\npurchase\nhigh point\ntaster\u2019s choice\nsanka\nnescafe\nbrim\n\nhigh\npoint\n\ntaster\u2019s\nchoice\n\n93\n9\n17\n6\n10\n\n17\n46\n11\n4\n4\n\nsanka\n44\n11\n155\n9\n12\n\nnescafe\n\n7\n0\n9\n15\n2\n\n.\nsource: based on data from r. grover and v. srinivasan, j. market. res. 24: 139\u1390153 1987 .\nreprinted with permission from the american marketing association.\n\nbrim\n10\n9\n12\n2\n27\n\u017e\n\n "}, {"Page_number": 462, "text": "problems\n\ntable 10.18 data for problem 10.9\n\nfather\u2019s\nstatus\n\n1\n2\n3\n4\n5\n\ntotal\n\n1\n50\n28\n11\n14\n3\n106\n\nson\u2019s status\n\n2\n45\n174\n78\n150\n42\n489\n\n3\n8\n84\n110\n185\n72\n459\n\n4\n18\n154\n223\n714\n320\n1429\n.\n\n447\n\n5\n\n8\n55\n96\n447\n411\n1017\n\ntotal\n129\n495\n518\n1510\n848\n3500\n\nsource: reprinted with permission from d. v. glass ed , social mobility in britain, glencoe, il:\n.\nfree press 1954 .\n\n\u017e\n\n\u017e\n\n10.9 table 10.18 relates father\u2019s and son\u2019s occupational status for a british\nsample. analyze these data, using models of a symmetry, b quasi-\n\u017e .\nsymmetry, c ordinal quasi-symmetry, d conditional symmetry, e\n\u017e .\nmarginal homogeneity,\nf quasi-independence, and g quasi-uniform\nassociation. interpret using their fit and lack of fit.\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\n10.10 for table 10.18, use kappa to describe agreement. interpret.\n\n10.11 table 10.19 displays multiple sclerosis diagnoses for two neurologists\nwho classified patients in two sites, winnipeg and new orleans. the\n\u017e .\ndiagnostic classes are 1 certain; 2 probable; 3 possible; and 4\ndoubtful, unlikely, or definitely not. for the new orleans patients,\nstudy the agreement using a the independence model and residuals,\n\u017e .\nb more complex models, and c kappa. interpret each.\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\ntable 10.19 data for problem 10.11\n\nwinnipeg neurologist\n\nwinnipeg patients\n\nnew orleans patients\n\nnew orleans\nneurologist\n\n1\n2\n3\n4\n\n1\n38\n33\n10\n3\n\n2\n5\n11\n14\n7\n\n3\n0\n3\n5\n3\n\n4\n1\n0\n6\n10\n\n2\n3\n11\n13\n2\n\n3\n0\n4\n3\n4\n\n4\n0\n0\n4\n14\n\n1\n5\n3\n2\n1\n\u017e\n\nsource: j. r. landis and g. g. koch, biometrics 33: 159\u1390174 1977 . reprinted with permission\nfrom the biometric society.\n\n.\n\n "}, {"Page_number": 463, "text": "448\n\nmodels for matched pairs\n\n10.12 for problem 10.11, construct a model that describes agreement\n\nbetween neurologists for the two sites simultaneously.\n\n10.13 calculate kappa for a 4 = 4 table having n s 5 all i, n\n\ns 15,\ni s 1, 2, 3, n s 15, and n s 0 otherwise. explain why strong asso-\nciation does not imply strong agreement.\n\niq1\n\n41\n\ni j\n\nii\n\ni,\n\n10.14 refer to table 10.8. based on the reported standardized residuals,\nexplain why the linear-by-linear association model 9.6 might fit well.\nfit it and describe the association.\n\n\u017e\n\n.\n\n10.15 in 1990, a sample of psychology graduate students at the university of\nflorida made blind, pairwise preference tests of three cola drinks.\nfor 49 comparisons of coke and pepsi, coke was preferred 29 times.\nfor 47 comparisons of classic coke and pepsi, classic coke was\npreferred 19 times. for 50 comparisons of coke and classic coke,\ncoke was preferred 31 times. comparisons resulting in ties are not\nreported.\na. fit the bradley\u1390terry model, analyze the quality of fit, and rank\nthe drinks. is there sufficient evidence to conclude a preference\nfor one drink?\n\nb. estimate the probability that coke is preferred to pepsi, using the\n\nmodel, and compare to the sample proportion.\n\n10.16 table 10.20 refers to journal citations among four statistics journals\nduring 1987\u13901989. the more often articles in a particular journal are\ncited, the more prestige that journal accrues. for citations involving\npair a and b, view it as a victory for a if it is cited by b and a defeat\nfor a if it cites b. fit the bradley\u1390terry model. interpret the fit, and\ngive a prestige ranking of the journals. for citations involving com-\nmun. stat. and jrss-b, estimate the probability that the commun.\nstat. article cites the jrss-b article.\n\ntable 10.20 data for problem 10.16\n\ncited journal\n\nciting journal\n\nbiometrika\n\ncommun. stat.\n\nbiometrika\ncommun. stat.\njasa\njrss-b\n\n714\n730\n498\n221\n\n33\n425\n68\n17\n\njasa\n320\n813\n1072\n142\n\njrss-b\n\n284\n276\n325\n188\n\nsource: stigler 1994 . reprinted with permission from the institute of mathematical statistics.\n\n\u017e\n\n.\n\n "}, {"Page_number": 464, "text": "problems\n\ntable 10.21 data for problem 10.17\n\n449\n\nwinner\nseles\ngraf\nsabatini\nnavratilova\nsanchez\n\nseles\n\u138f\n3\n0\n3\n0\n\nloser\n\ngraf\n\nsabatini\n\nnavratilova\n\nsanchez\n\n2\n\u138f\n3\n0\n1\n\n1\n6\n\u138f\n2\n2\n\n3\n3\n1\n\u138f\n1\n\n2\n7\n3\n3\n\u138f\n\n10.17 table 10.21 refers to matches for several women tennis players during\n\n1989 and 1990.\na. fit the bradley\u1390terry model. interpret, and rank the players.\nb. estimate the probability of seles beating graf. compare the model\nestimate to the sample proportion. construct a 90% confidence\ninterval for the probability.\n\nc. which pairs of players are significantly different according to a\n\n80% simultaneous bonferroni comparison?\n\n10.18 refer to problem 3.3 on basketball free-throw shooting. analyze\n\nthese data.\n\n10.19 refer to table 2.12 and problem 2.19. using models, describe the\n\nrelationship between husband\u2019s and wife\u2019s sexual fun.\n\n10.20 refer to table 8.19. the two-way table relating responses for the\nenvironment as rows and cities as columns has cell counts, by row,\n108, 179, 157 r 21, 55, 52 r 5, 6, 24 . analyze these data.\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\ntheory and methods\n\n10.21 explain the following analogy: mcnemar\u2019s test is to binary data as the\n\npaired difference t test is to normally distributed data.\n\n10.22 for a 2 = 2 table, derive cov p\n\n.x\n\np\n\n1q\n\n.\nequals 10.1 .\n\n\u017e\n\n\u017e\n\nq1\n\n, p\n\n1q\n\n, and show that var n p y\n.\n\n\u017e\n\n'\nw\n\nq1\n\n10.23 refer to the subject-specific model 10.8 for binary matched pairs.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\na. show that exp \u2424 is a conditional odds ratio between observation\nand outcome. explain the distinction between it and the odds ratio\n.\nexp \u2424 for model 10.6 .\n\n\u017e\n\n\u017e\n\n.\n\n "}, {"Page_number": 465, "text": "450\n\nb. using the conditional distribution\n\nlog n rn\n\n\u017e\n\n21\n\n12\n\n.\n.\n\nmodels for matched pairs\nshow that \u2424s\n\n.\n10.9 ,\n\n\u02c6\n\n\u017e\n\nc. for a random sample of n pairs, explain why\n\ne n rn s\n\u017e\n\n.\n\n21\n\nn1\n\u00fd\nn\nis1\n\n1\n\n.\n1 q exp \u2423 1 q exp \u2423 q \u2424\n.\n\nexp \u2423 q \u2424\n\ni\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\ni\n\ni\n\n\u017e\n\n12\n\n.\n\u2b01, explain why n rn\n\nsimilarily, state e n rn . using their ratio for fixed n and as\n6 \u017e\nexp \u2424 . hint: apply the law of\nn\nlarge numbers due to a. a. markov for independent but not\nidentically distributed random variables, or use chebyshev\u2019s in-\nequality.\n\n21\n\n12\n\n.\n\n.\n\n\u017e\n\np\n\n\u017e\n\n.\n\n12\n\n21\n\nd. show that the mantel\u1390haenszel estimator 6.7 of a common odds\nratio in the 2 = 2 = n form of the data simplifies to exp \u2424 s\nn rn .\n\n\u02c6\ne. use the delta method to show 10.10 for the se of \u2424.\nf. for a table of the form shown in table 10.2, show that the cmh\nstatistic 6.6 is algebraically identical to the mcnemar statistic\nn y n\n\u017e\n\nfor tables of table 10.1 type.\n\n.\nr n q n\n\u017e\n\n\u02c6\u017e\n.\n\n.2\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n21\n\n12\n\n21\n\n12\n\nt\n\nit\n\n\u017e\n\n\u017e\n\n.\n\n10.24 refer to problem 10.23. unlike the conditional ml estimator of \u2424,\nthe unconditional ml estimator is inconsistent andersen 1980, pp.\n244\u1390245; first shown by him in 1973 . show this as follows:\na. assuming independence of responses for different subjects and\ndifferent observations by the same subject, find the log likelihood.\nshow that the likelihood equations are y s \u00fd p y s 1 and\ny s \u00fd p y s 1 .\n.\niq\nb. substituting exp \u2423 r 1 q exp \u2423 q exp \u2423 q \u2424 r 1 q exp \u2423 q\n\u017e\n.x\u2424 in the second likelihood equation, show that \u2423 s y\u2b01 for the\nsubjects with y s 0, \u2423 s \u2b01 for the n subjects with y s 2,\nn\nand \u2423 s y\u2424r2 for the n q n\n\u02c6i\n\nsubjects with y s 1.\n\nc. by breaking \u00fd p y s 1 into components for the sets of subjects\nlike-\n0 q n 1 q n q\n\u017e .\nfor\n21\nexp y\u2424r2 r 1 q exp y\u2424r2 . explain why y s n q n ,\n\u017e\n.\n12\u02c6\nlikelihood equation to show that \u2424s\n\niq\nit\nhaving y s 0, y s 2, and y s 1, show that\niq\niq\nlihood equation is,\n\u02c6\nn\n12\nand solve the first\n2 log n rn\n\n11\n6\u02c6\n.\n. hence, as a result of problem 10.23, \u2424 2 \u2424.\n\nthe first\n\u017e\n11\nq1\n\niq\nt s 1,\n.x\n\u02c6\n\ny s n\nq1\n\n.\n\u02c6i\n\n\u02c6\n21\n\n\u017e .\n\n.x\n\nqt\n\niq\n\niq\n\n\u02c6\n\n22\n\n11\n\n12\n\n22\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\nw\n\nw\n\nw\n\nit\n\np\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n21\n\n12\n\n10.25 consider marginal model 10.6 when y and y are independent and\nconditional model 10.8 when \u2423 are identical. explain why they are\nequivalent.\n\n\u017e\n\n.\n\n4\n\n1\n\n2\n\ni\n\n\u017e\n\n.\n\u0004\n\n6\n "}, {"Page_number": 466, "text": "problems\n10.26 let \u2424 s log p\n\u017e\n\u2424 s log n rn\n\u02c6\nc\nmethod, show that the asymptotic variance of n \u2424 y \u2424 is\n\nrefer to marginal model 10.6 and\n\u017e\n10.8 . using the delta\n\u02c6' \u017e\n\np rp\n2q q2\nto conditional model\n\nq1\n.\n12\n\n\u02c6\nm\n\n1q\n\np\n\n21\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\nm\n\nm\n\n451\n\n\u017e\n\n\u2432 \u2432\n\ny1\n\ny1\n\n.\n\n.\n\n\u017e\n\nq1 q2\n\nq \u2432 \u2432\n\ny 2 \u2432 \u2432 y \u2432 \u2432 r \u2432 \u2432 \u2432 \u2432 .\n.\n1q 2q\nunder the independence condition of the previous problem, \u2424 s \u2424 .\nin that case, show that the asymptotic variances satisfy\n\n1q 2q q1 q2\n\n11\n\n12\n\n21\n\n22\n\n\u017e\n\n.\n\n\u017e\n\nm\n\nc\n\n.\n\n\u017e\n\n\u017e\n\ny1\n\n.m\n\nq \u2432 \u2432\n\n\u02c6'var n \u2424 s \u2432 \u2432\n1q 2q\nf \u2432 \u2432\nq \u2432 \u2432\nq1\n2q\n\u02c6'\ns \u2432 q \u2432 s var n \u2424\u017e\n\n.\ny1\n21\n\n1q q2\n\nq1 q2\n\ny1\n12\n\ny1\n\n\u017e\n\n\u017e\n\n\u017e\n\nc\n\ny1\n\ny1\n\n.\n\n.\n.\n\n\u017e\n\n.\n\n10.27 refer to model 10.12 for a matched-pairs study. for the conditional\n.\nml approach, show that the conditional distribution satisfies 10.13\nand does not depend on \u2424 when s s 0 or 2. show what happens to\n\u2424 in the conditional distribution for a predictor for which x s x\nall i.\n\nji2\n\nji1\n\n\u017e\n\ni\n\nj\n\n10.28 consider model 10.12 for a study with matched sets of t observa-\n\u017e\ntions rather than matched pairs. explain how 10.13 generalizes and\nconstruct the form of the conditional likelihood.\n\n.\n\n\u017e\n\n.\n\n10.29 give an example illustrating that when i ) 2, marginal homogeneity\n\ndoes not imply symmetry.\n\n10.30 derive the likelihood equations and residual df for a symmetry,\n\u017e .\nb quasi-symmetry, c quasi-independence, and d ordinal quasi-\nsymmetry.\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\n10.31 for the quasi-symmetry model 10.19 , let \u242d s \u242d y \u242d . show that\none can express it equivalently as log \u242e s \u242dq \u242d q \u242d* , with \u242d*\ns \u242d* . hence, one needs only one set of main-effect parameters.\n\nx\na\n\ny\na\n\nab\n\nab\n\nab\n\n\u017e\n\n.\n\na\n\na\n\nb a\n\n10.32 show that quasi-symmetry is equivalent caussinus 1966 to\n\u2432 \u2432 \u2432 r \u2432 \u2432 \u2432 s 1 all a, b, and c.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nab\n\nbc\n\nca\n\nb a cb\n\nac\n\n\u017e\n\n.\n\n10.33 derive the covariance matrix 10.16 for the difference vector d.\n\n\u017e\n\n.\n\n "}, {"Page_number": 467, "text": "452\n\nmodels for matched pairs\n\n10.34 construct the loglinear model satisfying both marginal homogeneity\np q\nqb\n\nindependence. show that \u2432 s p q p\n\n\u02c6ab\n\naq\n\nqa\n\n.\u017e\n\n\u017e\n\nand statistical\np\nbq\n\nr4 and residual df s i i y 1 .\n.\n.\n\n\u017e\n\n.\n10.35 consider the conditional symmetry cs model 10.28 .\n\n\u017e\n\n.\n\n\u017e\n\na. show that it has the loglinear representation\n\nlog \u242e s \u242d\n\nab\n\nmin \u017e a, b., max \u017e a, b.\n\nq \u2436i a - b ,\n.\n\n\u017e\n\n.\nwhere i \u2b48 is an indicator see also bishop et al. 1975, pp. 285\u1390286 .\n\n\u017e .\n\n\u017e\n\nb. show that the likelihood equations are\n\n\u242e q \u242e s n q n\n\u02c6\n\n\u02c6\n\nb a\n\nab\n\nab\n\nfor all a f b,\n\nb a\n\n\u242e s\nab\n\n\u02c6\u00fd\u00fd\na-b\n\n\u00fd\u00fd\na-b\n\nn .\nab\n\nc. show that \u2436s log \u00fd\u00fd\n\n.x\n, \u242e s n , a s\n1, . . . , i, \u242e s exp \u2436i a - b n q n r exp \u2436 q 1 for a / b.\n\u017e .\n\u02c6\n\nn r \u00fd\u00fd\na- b\n.x\u017e\n\nw\u017e\n\u017e\n\u02c6\n\na) b\n.\n\nn\nw\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\naa\n\naa\n\nab\n\nab\n\n.\n\n\u017e\n\nw\n\nx\n\nb a\n\nab\n\nab\n\nd. show that the estimated asymptotic variance of \u2436 is\u02c6\n\n\u017e\n\n\u00fd\u00fd\n\na-b\n\nn\n\nab\n\ny1\n\nq\n\n/\n\n\u017e\n\n\u00fd\u00fd\n\na)b\n\nn\n\nab\n\ny1\n\n.\n\n/\n\n\u017e\n\ne. show that residual df s i q 1 i y 2 r2.\nf. show that\n\nsymmetry q marginal homogeneity s\n2\u017e\nsymmetry. explain why g s cs tests marginal homogeneity\ndf s 1 . when the model holds g s cs is more powerful\n\u017e\nasymptotically than g s qs . why?\n\nconditional\n\n2\u017e\n\n2\u017e\n\n.\u017e\n\n.\n\n.\n\n.\n\n.\n\n.\n\n<\n\n<\n\n<\n\n10.36 identify loglinear models that correspond to the logit models, for\n\na - b, log \u2432 r\u2432 s a 0, b \u2436, c \u2423 y \u2423 , and d \u2424 b y a .\n.\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\n.\n\n\u017e\n\n\u017e\n\nab\n\nb a\n\na\n\nb\n\n10.37 a nonmodel-based ordinal measure of marginal heterogeneity is\n\n\u02c6\u232c s\n\n\u00fd\u00fd\n\na-b\n\np y\n\naq qb\n\np\n\n\u00fd\u00fd\n\na)b\n\np\n\np\n\naq qb\n\n.\n\n\u02c6\n\nshow that \u232c estimates \u232c s p y ) y y p y ) y , where y has\ndistribution \u2432\nand y is independent from \u2432 . show that\nmarginal homogeneity implies that \u232c s 0. show that the estimated\n\n.\nqb\n\naq\n\n\u017e\n\n.\n\n\u017e\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n1\n\n2\n\n2\n\n1\n\n1\n\n2\n\n "}, {"Page_number": 468, "text": "problems\n\nasymptotic variance of \u232c is\n\n\u02c6\n\n453\n\n\u00fd \u00fd\n\n\u243e p y\n2\u02c6\nab\n\nab\n\n2\n\n/\n\n\u02c6\n\u243e p\nab\n\n\u00fd \u00fd\n\n\u017e\na\ny f y f\n\u02c6\n\u02c6\nay1,2\n1q\na2\n.\n. \u017e\nagresti 1984, pp. 208\u1390209 .\nqa\n\n\u02c6\na1\n\nn,\n\nab\n\n\u017e\n\nb\n\na\n\nb\nwhere \u243e s f q f\n\u02c6\nby1,1\nand f s p q \u2b48\u2b48\u2b48 qp\n\n\u02c6\nab\n\u02c6\na2\n\n\u02c6\nb1\nq1\n\n\u017e\n\nwith f s p q \u2b48\u2b48\u2b48 qp\n\n.\n\naq\n\n10.38 for ordered scores u , let y s \u00fd u p\n\nand y s \u00fd u p\nthat marginal homogeneity implies that e y s e y and\n.\n\naq\n\u017e\n\n\u017e\n\n.\n\n\u0004\n\n4\n\na\n\na\n\na\n\na\n\n1\n\n2\n\na qa\n\n1\n\n2\n\n. show\n\n\u00fd \u00fd a\n\nu y u\n\n\u017e\n\n2\n\n.\n\nb\n\np y y y y\n\n\u017e\n\nab\n\n1\n\n2\n\n.\n\n2\n\nn.\n\na\n\nb\n\nestimates var y y y . construct a test of marginal homogeneity\n1\n.\n\u017e\nbhapkar 1968 .\n\n\u017e\n\n.\n\n2\n\n10.39 consider the multiplicative model for a square table,\n\n\u2423 \u2423 1 y \u2424 ,\n.\n\n\u00bd \u2423 q \u2424\u2423 1 y \u2423 ,\n\na\n2\na\n\n\u017e\n\n\u017e\n\n.\n\na\n\na\n\nb\n\n\u2432 s\nab\n\na / b\na s b.\n\na. show that the model satisfies\n\u017e\niv quasi-independence.\n\n\u017e\niii quasi-symmetry,\n\nsymmetry,\n\ngeneity,\n\n.\n\n\u017e .\ni\n.\n\n\u017e .\nii marginal homo-\n\nb. show that \u2423 s \u2432 s \u2432 , a s 1, . . . , i.\nc. show that \u2424s cohen\u2019s kappa, and interpret \u242cs 0 and \u242cs 1 for\n\nqa\n\naq\n\na\n\nthis model.\n\n\u017e .\n\n10.40 a 2 = 2 table has a true odds ratio of 10. find the cell probabilities\nfor which a \u2432 s \u2432 s 0.5, b \u2432 s \u2432 s 0.3, and c \u2432 s\n\u2432 s 0.1. find the value of kappa for each. this shows that for a\nq1\ngiven association, kappa depends strongly on the marginal probabili-\nties; see also sprott 2000, p. 59.\n\nq1\n\u017e\n\n\u017e .\n\n\u017e .\n\n1q\n\nq1\n\n1q\n\n1q\n\n.\n\n10.41 a model for agreement on an ordinal response partitions beyond-\nchance agreement into that due to a baseline association and a\n.\nmain-diagonal increment a. agresti, biometrics 44: 539\u1390548, 1988 .\nfor ordered scores u , the model is\n\n\u017e\n\n\u0004\n\n4\n\na\n\nlog \u242e s \u242dq \u242da q \u242db q \u2424u u q \u2426i a s b .\n.\n\n\u017e\n\na\n\nb\n\na\n\nb\n\nab\n\n\u017e\n\n10.35\n\n.\n\na. show that this is a special case of quasi-symmetry and of quasi-\n\n.\nassociation 10.29 .\n\n\u017e\n\n "}, {"Page_number": 469, "text": "454\n\nmodels for matched pairs\nb. for agreement odds 10.30 , show that log \u2436 s u y u \u2424q 2 \u2426.\nfor unit-spaced scores, show the local odds ratios have log \u242a s \u2424\nwhen none of the four cells falls on the main diagonal.\n\n.2\n\nab\n\nab\n\n\u017e\n\n.\n\n\u017e\n\na\n\nb\n\nc. find the likelihood equations and show that \u242e and n\n\nshare\nthe same marginal distributions, correlation, and prevalence of\nexact agreement.\n\nd. for table 10.8 using u s a , show that 10.35 has g s 4.8\ndf s 7 , with \u2426s 0.842 se s 0.427 and \u2424s 1.316 se s 0.420 .\n\u02c6\n\u017e\n.\n\u02c6a, aq1\ninterpret using \u2436\n\n\u0004\n\u017e\n\u017e\nand \u242a for a y b ) 1.\n\n\u02c6\nab\n\n\u02c6\n\nab\n\n.\n\n.\n\n.\n\n\u017e\n\n4\n\na\n\n2\n\n\u0004\n\n\u02c6 ab\n\n4\n\n\u0004\n\n4\n\n10.42 refer to the bradley\u1390terry model.\n\u017e\n\na. show that log \u2338 r\u2338 s log \u2338 r\u2338 q log \u2338 r\u2338 .\n.\nb. with this model, is it possible that a could be preferred to b i.e.,\nand b could be preferred to c, yet c could be pre-\n\nb a\n\nab\n\nca\n\nbc\n\ncb\n\nac\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\nab\n\n\u2338 ) \u2338\nferred to a? explain.\n\nb a\n\n\u0004\n\n4\n\nc. explain why \u2424 are not identifiable without a constraint such as\n\u2424 s 0. hint: show the model holds when \u2424* s \u2424 y c for any\ni\n.c.\n\n\u017e\n\n\u0004\n\n4\n\na\n\na\n\na\n\n.\n10.43 refer to model 10.32 .\n\n\u017e\n\n4\n\n4\n\n\u0004\n\n\u017e\n\nh i\n\nbeats\nw\n\u017e\n\na. construct a more general model having home-team parameters\n\u0004\n\u2424 and away-team parameters \u2424 , such that the probability\nteam i\nthe home team is\nexp \u2424 r exp \u2424 q exp \u2424 , where \u2424 s 0 but \u2424 is unre-\n.\nh i\nstricted.\n\nai\nteam j when i\n.\n\nb. interpret the case \u2424 s \u2424 q c , when i c s 0, and ii c ) 0.\n.\nc. fit the model to table 10.12. compare the fit to model 10.32 .\ncompare \u2424 and \u2424 to describe how teams play at home and\naway.\n\n\u017e .\n\u017e\n\n\u02c6\nh i\n\n\u02c6\nai\n\n\u017e .\n\n.x\n\nis\n\na i\n\nh i\n\nh i\n\nh i\n\na j\n\nai\n\n\u017e\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n10.44 find the log likelihood for the bradley\u1390terry model. from the\nthe minimal sufficient statistics are\n. thus, explain how \u2018\u2018victory totals\u2019\u2019 determine the estimated\n\nkernel, show that given n\n\u0004\nn\nranking.\n\naq\n\n4.\n\nab\n\n\u017e\n\n\u0004\n\n4\n\n10.45 explain how to fit the complete symmetry model in t dimensions.\n\n10.46 prove that if kth-order marginal symmetry holds,\n\nsymmetry holds for any j - k.\n\njth-order marginal\n\n10.47 suppose that quasi-symmetry holds for an i t table. when the table is\ncollapsed over a variable, show that the model holds for the i ty1\ntable with the same main effects.\n\n "}, {"Page_number": 470, "text": "c h a p t e r 1 1\n\nanalyzing repeated categorical\nresponse data\n\nmany studies observe the response variable for each subject repeatedly, at\nseveral times or under various conditions. repeated categorical response\ndata occur commonly in health-related applications, especially in longitudinal\nstudies. for example, a physician might evaluate patients at weekly intervals\nregarding whether a new drug treatment is successful. in some cases explana-\ntory variables may also vary over time. but the repeated responses need not\nrefer to different times. a dental study might measure whether there is decay\nfor each tooth in a subject\u2019s mouth.\n\noften, the responses refer to matched sets, or clusters, of subjects. an\nexample is a survival, nonsurvival response for each fetus in a litter, for a\nsample of pregnant mice exposed to various dosages of a toxin. a multistage\nsample to study factors affecting obesity in children may regard children from\nthe same family as a cluster. observations within a cluster tend to be more\nalike than observations from different clusters. ordinary analyses that ignore\nthis may be badly inappropriate.\n\n.\n\n\u017e\n\nin this chapter we generalize methods of chapter 10, which referred to\nmatched pairs. in section 11.1 we compare marginal distributions in t-way\ntables. the remaining sections extend models to include explanatory vari-\nables. for instance, many studies compare the repeated measurements for\ndifferent groups or treatments. in section 11.2 we use ml methods for fitting\nmarginal models. in section 11.3 we use generalized estimating equations\n\u017e\n.gee , a multivariate version of quasi-likelihood that is computationally\nsimpler than ml. section 11.4 covers technical details about the gee\napproach. in the final section we introduce a transitional approach that\nmodels observations in terms of previous outcomes.\n\n455\n\n "}, {"Page_number": 471, "text": "456\n\nanalyzing repeated categorical response data\n\n11.1 comparing marginal distributions:\nmultiple responses\n\n\u017e\n\nusually, the multivariate dependence among repeated responses is of less\ninterest than their marginal distributions. for instance, in treating a chronic\ncondition such as a phobia with some treatment, the primary goal might be\nto study whether the probability of success increases over the t weeks of a\ntreatment period. the t success probabilities refer to the t first-order\nmarginal distributions. in sections 10.2.1 and 10.3 we compared marginal\ndistributions for matched pairs t s 2 using models that apply directly to\nthe marginal distributions. in this section we extend this approach to t ) 2.\n\n.\n\n\u017e\n\n.\n\n11.1.1 binary marginal models and marginal homogeneity\n\u017e\ndenote t binary responses by y , y , . . . , y . the marginal logit model\n\u017e\n10.6 for matched pairs extends to\n\n.\n\n.\n\nt\n\n1\n\n2\n\nlogit p y s 1 s \u2423q \u2424 ,\n\n\u017e\n\n.\n\nt s 1, . . . , t ,\n\n\u017e\n\n11.1\n\n.\n\nt\n\nt\n\nt\n\nt\n\nwith a constraint such as \u2424 s 0 or \u2423s 0. for a possible sequence of\noutcomes i s i , i , . . . , i where each i s 0 or 1, let\n\n.\n\n\u017e\n\nt\n\n1\n\n2\n\n\u2432 s p y s i , y s i\n\n\u017e\n\n1\n\n2\n\n1\n\ni\n\n, . . . , y s i\n\nt\n\nt\n\n.\n\n.\n\n2\n\nt\n\n1\n\nt\n\n.\n\n\u017e\n\n.\n\nlet \u2432 denote the vector of these probabilities for the possible i. they refer\nto a 2t table that cross-classifies the t responses and describes the joint\ndistribution of y , . . . , y . the sample cell proportions are the ml esti-\nmates of \u2432, and the sample proportion with y s 1 is the ml estimate of\np y s 1 .\n.\n\u017e\nt\n\u017e\nmodel 11.1 is saturated, describing t marginal probabilities by t param-\neters. marginal homogeneity, for which p y s 1 s \u2b48\u2b48\u2b48 s p y s 1 , is the\nspecial case \u2424 s \u2b48\u2b48\u2b48 s \u2424 . even though this case has only one parameter,\nml fitting is not simple. the multinomial likelihood refers to the 2t joint cell\nprobabilities \u2432 rather than the t marginal probabilities p y s 1 . fitting\nmethods are described in section 11.2.5.\n\n.4\n\nlet n denote the sample cell count in cell i. the kernel of the log\nlikelihood l \u2432 is \u00fd n log \u2432 . let l p denote the log likelihood evaluated\np s n rn , the ml fit of model 11.1 . let\nat the sample proportions\n\u017e m h .\n\u02c6\nl \u2432\ndenote the maximized log likelihood assuming marginal homogene-\nity. the likelihood-ratio test of marginal homogeneity lipsitz et al. 1990;\nmadansky 1963 uses\n\n\u017e .\n4\n\ni\u0004\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u0004\n\nt\n\nt\n\n1\n\n1\n\ni\n\ni\n\ni\n\ni\n\ni\n\nt\n\ny2 l \u2432\n\u02c6\n\n\u017e\n\nm h\n\n.\n\ny l p s 2\n\n\u017e\n\n.\n\n\u00fd i\n\nn log p r\u2432\n\u02c6\n\n\u017e\n\ni\n\nm h\ni\n\ni\n\n.\n\n.\n\n\u017e\n\n11.2\n\n.\n\n "}, {"Page_number": 472, "text": "comparing marginal distributions: multiple responses\n\n457\n\ntable 11.1 responses to three drugs in a crossover study\n\ndrug a favorable\n\ndrug a unfavorable\n\nb favorable\n\nb unfavorable\n\nb favorable\n\nb unfavorable\n\nc favorable\nc unfavorable\n\n6\n16\n\n2\n4\n\n2\n4\n\n6\n6\n\n.\nsource: reprinted with permission from the biometric society grizzle et al. 1969 .\n\n\u017e\n\nthe asymptotic null chi-squared distribution has df s t y 1, since the gen-\neral model 11.1 has t y 1 more parameters than marginal homogeneity.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n11.1.2 crossover drug comparison example\ntable 11.1 comes from a crossover study in which each subject used each of\nthree drugs for treatment of a chronic condition at three times. the response\nmeasured the reaction as favorable or unfavorable. the 2 3 table gives the\n\u017e\nfavorable, unfavorable classification for reaction to drug a in the first\ndimension, drug b in the second, and drug c in the third. we assume that\nthe drugs have no carryover effects and that the severity of the condition\nremained stable for each subject throughout the experiment. these assump-\ntions are reasonable for many chronic conditions, such as migraine headache.\n.\nthe sample proportion favorable was 0.61, 0.61, 0.35 for drugs a, b, c .\nthe likelihood-ratio statistic for testing marginal homogeneity is 5.95 df s 2 ,\n.\nfor a p-value of 0.05. for simultaneous confidence intervals comparing pairs\nof treatments with overall error probability no greater than 0.05, the bonfer-\nroni method uses confidence coefficient 1 y 0.05r3 s 0.9833 for each. for\ninstance, from formula 10.1 , the estimate 0.261 s 0.609 y 0.348 of the\ndifference between drugs a and c has an estimated standard error of 0.108.\nthe confidence interval for the true difference is 0.261 \" 2.39 0.108 , or\n\u017e\n0.002, 0.520 . the same interval holds for comparison of drugs b and c.\nthere is some evidence that the proportion of favorable responses is lower\nfor drug c.\n\n\u017e\n\u017e\n\nthe sample size is not large, however, so we view these results with\ncaution. for each pair of drugs, a 2 = 2 table relates the two responses. an\nexact binomial test section 10.4.1 uses its off-diagonal counts. these yield\np-values of 1.0 for comparing drugs a and b and 0.036 for comparing a with\nc and for comparing b with c.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n11.1.3 modeling margins of a multicategory response\nthe binary marginal model 11.1 extends to multinomial responses. with\nbaseline-category logits for i outcome categories, the saturated model is\nj s 1, . . . , i y 1.\n\nlog p y s j rp y s i s \u2424 ,\n\nt s 1, . . . , t ,\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nt\n\nt\n\nt j\n\n\u017e\n\n11.3\n\n.\n\n "}, {"Page_number": 473, "text": "458\nmarginal homogeneity, whereby p y s j s \u2b48\u2b48\u2b48 s p y s j\n\u017e\n1, . . . , i y 1, is the special case in which\n\u2424 s \u2424 s \u2b48\u2b48\u2b48 s \u2424 ,\n\nanalyzing repeated categorical response data\nj s\n\nj s 1, . . . , i y 1.\n\nfor\n\n.\n\n\u017e\n\n.\n\nt\n\n1\n\n1 j\n\n2 j\n\nt j\n\n.\n\n\u017e\n\nthe likelihood-ratio test of marginal homogeneity comparing the two models\nhas form 11.2 and df s t y 1 i y 1 .\n.\n\n.\u017e\n\nfor an ordinal response, an unsaturated model that is more complex than\nmarginal homogeneity focuses on shifts up and down in the t margins. one\nsuch model is\n\n\u017e\n\nlogit p y f j s \u2423 q \u2424 ,\n\n.\n\n\u017e\n\nj\n\nt\n\nt\n\nt s 1, . . . , t ,\n\nj s 1, . . . , i y 1,\n\n\u017e\n\n11.4\n\n.\n\nwith constraint such as \u2424 s 0. marginal homogeneity is the special case\n\u2424 s \u2b48\u2b48\u2b48 s \u2424 . its test has df s t y 1. the \u2423 satisfy \u2423 - . . . - \u2423\niy1\nbecause of the ordering of the cumulative probabilities. these models can be\nfitted using ml methodology presented in section 11.2.5.\n\n\u0004\n\n4\n\nt\n\nt\n\n1\n\n1\n\nj\n\n11.1.4 wald and generalized cmh score tests of marginal homogeneity\nin this chapter we focus on modeling the marginal distributions rather than\nmerely testing marginal homogeneity. however, a variety of tests are avail-\nable besides the likelihood ratio, so we briefly summarize a couple of them.\nlet p t denote the sample proportion in category j for response y , let\n\n\u017e .\n\nt\n\nj\n\np s p t rt ,\n\u00fdj\n\n\u017e .\n\nj\n\nt\n\nd t s p t y p ,\n\n\u017e .\n\n\u017e .\n\nj\n\nj\n\nj\n\nand let d denote the vector of d t , t s 1, . . . , t y 1, j s 1, . . . , i y 1 . let\n\u02c6\nv denote the estimated covariance matrix of n d. bhapkar 1973 proposed\nthe wald statistic\n\n\u017e .\n\n'\n\n\u017e\n\n.\n\n\u0004\n\n4\n\nj\n\nw s nd v d.\n\nx \u02c6y1\n\n\u017e\n\n11.5\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\u017e\n\nfor the general alternative. this generalizes 10.16 and has a large-sample\nchi-squared distribution with df s i y 1 t y 1 .\n.\n\n.\n\u017e\n\n\u017e\n\u017e\n.\n\nother statistics are special cases of the generalized cochran\u1390mantel\u1390\nhaenszel cmh statistic section 7.5.3 . recall that for the binary case\ni s 2 with matched pairs t s 2 , the cmh statistic applies to a three-way\n\u017e\ntable see, e.g., table 10.2 in which each stratum shows the two outcomes\nfor a given subject. a generalization of table 10.2 provides n strata of t = i\ntables. the kth stratum gives the t outcomes for subject k. row t\nin a\nstratum has a 1 in the column that is the outcome for observation t, and 0 in\n.\nall other columns or 0 in every column if that observation is missing .\nprobability distributions for the subject-stratified setup naturally relate to\n\n\u017e\n\n.\n\n.\n\n "}, {"Page_number": 474, "text": "marginal modeling: maximum likelihood approach\n\n459\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nsubject-specific models such as logit model 10.8 , rather than to marginal\nmodels. however, conditional independence in this three-way table given\nsubject corresponds to an exchangeability among variables in the i\ntable\nthat implies marginal homogeneity. a generalized cmh test of conditional\nindependence in the t = i = n table also tests marginal homogeneity using a\nsampling distribution generated under the stronger exchangeability condition\n\u017e\ndarroch 1981 . for an ordinal response with fixed scores, the generalized\ncmh statistic for detecting variability among t means is appropriate.\nwhen i s 2 and t s 2, this cmh approach is equivalent to mcnemar\u2019s\nstatistic. when i s 2 but t ) 2, the generalized cmh statistic treating the t\n.\nresponses as unordered is identical to a statistic cochran 1950 proposed.\nhis statistic, called cochran\u2019s q, has df s t y 1 problem 11.22 .\n.\n\n.\n\n\u017e\n\n\u017e\n\nt\n\n11.2 marginal modeling: maximum likelihood approach\n\nanalyses above compared marginal distributions, but without accounting for\nexplanatory variables. we now include such predictors. in this section we use\nml, but we defer model fitting details to the end of the section.\n\n11.2.1 longitudinal mental depression example\nwe use table 11.2 to illustrate a variety of analyses in this and the next\nchapter. it refers to a longitudinal study comparing a new drug with a\nstandard drug for treatment of subjects suffering mental depression koch\net al. 1977 . subjects were classified into two initial diagnosis groups accord-\ning to whether severity of depression was mild or severe. in each group,\nsubjects were randomly assigned to one of the two drugs. following 1 week, 2\nweeks, and 4 weeks of treatment, each subject\u2019s suffering from mental\ndepression was classified as normal or abnormal.\n\n.\n\n\u017e\n\ntable 11.2 cross-classification of responses on depression\nat three times by diagnosis and treatment\n\nresponse at three times\n\na\n\ndiagnosis treatment nnn nna nan naa ann ana aan aaa\nmild\n\nstandard\nnew drug\nstandard\nnew drug\n\n16\n31\n2\n7\n\n13\n0\n2\n2\n\n9\n6\n8\n5\n\n3\n0\n9\n2\n\n14\n22\n9\n31\n\n4\n2\n15\n5\n\n15\n9\n27\n32\n\n6\n0\n28\n6\n\nsevere\n\nan, normal; a, abnormal.\n.\nsource: reprinted with permission from the biometric society koch et al. 1977 .\n\n\u017e\n\n "}, {"Page_number": 475, "text": "460\n\nanalyzing repeated categorical response data\n\nt\n\n.\n\n\u017e\n\ntable 11.2 shows four groups, the combinations of categories of the two\nexplanatory variables: treatment type and severity of initial diagnosis. since\nthe study observed the binary response depression assessment at t s 3\noccasions, table 11.2 shows a 2 3 table for each group. the three depression\nassessments form a multivariate response variable with three components,\nwith y s 1 for normal and 0 for abnormal. the 12 marginal distributions\nresult from three repeated observations for each of the four groups.\nlet s denote the severity of the initial diagnosis, with s s 1 for severe and\ns s 0 for mild. let d denote the drug, with d s 1 for new and d s 0 for\nstandard. let t denote the time of measurement. koch et al. 1977 noted\nthat if the time metric reflects cumulative drug dosage, a logit scale often has\na linear effect for the logarithm of time. they used scores 0, 1, 2 , the logs to\nbase 2 of the week numbers 1, 2, and 4 , for time.\ntable 11.3 shows sample proportions of normal responses i.e., y s 1 for\nthe 12 marginal distributions. for instance, from table 11.2, the sample\nproportion of normal responses after week 1 for subjects with mild initial\ndiagnosis using the standard drug was 16 q 13 q 9 q 3 r 16 q 13 q 9 q 3\nq 14 q 4 q 15 q 6 s 0.51. the sample proportion of normal responses 1\n\u017e .\nincreased over time for each group; 2 increased at a faster rate for the new\ndrug than the standard, for each fixed initial diagnosis; and 3 was higher for\nthe mild than the severe initial diagnosis,\nfor each treatment at each\noccasion. in such a study the company that developed the new drug would\nhope to show that patients have a significantly higher rate of improvement\nwith it.\n\n\u017e .\n\n\u017e .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\nt\n\nthe marginal logit model\n\nlogit p y s 1 s \u2423q \u2424 s q \u2424 d q \u2424 t\n\n\u017e\n\n.\n\n1\n\n2\n\n3\n\nt\n\n.\n\nhas the main effects of the explanatory variables severity of initial diagnosis\nand drug and of the variable time that specifies the different components\nof the multivariate response. its linear time effect \u2424 is the same for each\ngroup.\n\nthe natural sampling assumption is multinomial for the eight cells in the\nindependently for the four\n\n2 3 cross-classification of the three responses,\n\n\u017e\n\n.\n\n3\n\n\u017e\n\ntable 11.3 sample marginal proportions of normal response for\ndepression data of table 11.2\n\ndiagnosis\nmild\n\nsevere\n\ntreatment\nstandard\nnew drug\nstandard\nnew drug\n\nsample proportion\n\nweek 1\n\nweek 2\n\nweek 4\n\n0.51\n0.53\n0.21\n0.18\n\n0.59\n0.79\n0.28\n0.50\n\n0.68\n0.97\n0.46\n0.83\n\n "}, {"Page_number": 476, "text": "marginal modeling: maximum likelihood approach\n\n461\n\n.\n\n\u017e\ngroups. however, the model refers to 12 marginal probabilities\nfor 2 drug\ntreatments = 2 initial severity diagnoses = 3 time points\nrather than the\n4 = 2 3 s 32 cell probabilities in the product multinomial likelihood function.\nthe three marginal binomial variates for each group are dependent. ml\nestimation requires an iterative routine for maximizing the product multino-\nmial\nlikelihood, subject to the constraint that the marginal probabilities\nsatisfy the model. an algorithm for this is given in section 11.2.5.\n\na check of model fit compares the 32 cell counts in table 11.2 to their ml\nfitted values. since the model describes 12 marginal logits using four parame-\nters, residual df s 8. the deviance g2 s 34.6. the poor fit is not surprising.\nthe model assumes a common rate of improvement \u2424 , but the sample\nshows a higher rate for the new drug.\n\n3\n\na more realistic model permits the time effect to differ by drug,\n\nlogit p y s 1 s \u2423q \u2424 s q \u2424 d q \u2424 t q \u2424 dt.\n\n\u017e\n\n.\n\n3\n\n4\n\n1\n\n2\n\nt\n\n2\n\n2\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n.\n\n\u02c6\n4\n\n\u02c6\n4\n\n\u02c6\n3\n\n\u02c6\n3\n\u017e\n\nits time effect estimate is \u2424 s 0.48 se s 0.12 for the standard drug\nd s 0 and \u2424 q \u2424 s 1.49 se s 0.14 for the new one d s 1 . for the\n\u017e\nnew drug, the slope is \u2424 s 1.01 se s 0.18 higher than for the standard,\ngiving strong evidence of faster improvement. this model fits much better,\nwith g s 4.2 df s 7 . the g decrease of 34.6 y 4.2 s 30.4 compared to\nthe simpler model is the likelihood-ratio test of h : \u2424 s 0, a common time\neffect for each drug.\nthe severity of initial diagnosis estimate is \u2424 s y1.29 se s 0.14 ; for\neach drug\u1390time combination, the estimated odds of a normal response when\nthe initial diagnosis was severe equal exp y1.29 s 0.27 times the estimated\nodds when the initial diagnosis was mild. the estimate \u2424 s y0.06 se s\n0.22 indicates an insignificant difference between the drugs after 1 week for\nwhich t s 0 . at time t, the estimated odds of normal response with the new\ndrug are exp y0.06 q 1.01 t\ntimes the estimated odds for the standard drug,\nfor each initial diagnosis level. in summary, severity of initial diagnosis, drug\ntreatment, and time all have substantial effects on the probability of a normal\nresponse.\n\n.\n\u017e\n\n\u02c6\n2\n\n\u02c6\n1\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n0\n\n4\n\n11.2.2 modeling a repeated multinomial response\nmodels for marginal distributions of a repeated binary response generalize to\nmulticategory responses. at observation t, the marginal response distribution\nhas i y 1 logits. with nominal responses, baseline-category logit models\ndescribe the odds of each outcome relative to a baseline. for ordinal\nresponses, one might use cumulative logit models.\n\nfor a particular marginal logit, a model has the form\n\nlogit\n\nj\n\nt s \u2423 q \u2424x x ,\n\u017e .\n\nj\n\nt\n\nj\n\nj s 1, . . . , i y 1,\n\nt s 1, . . . .\n\n "}, {"Page_number": 477, "text": "462\n\nanalyzing repeated categorical response data\n\nj\n\nw\n\nt s logit p y f j\n\u017e .\n\n.x\n. then, \u2424 may\nfor an ordinal response, perhaps logit\nsimplify to \u2424, in which case the model takes the proportional odds form with\nthe same effects for each logit. some parameters in \u2424 may refer to the\nvariable subscripted by t e.g., time that indexes the repeated measurements.\none can then compare marginal distributions at particular settings of x or\nevaluate effects of x on the response. in either case, checking for interaction\nis crucial. for instance, are the effects of x the same at each t?\n\n\u017e\n\n.\n\n\u017e\n\nt\n\nj\n\n\u017e\n\ninsomnia example\n\n11.2.3\ntable 11.4 shows results of a randomized, double-blind clinical trial compar-\ning an active hypnotic drug with a placebo in patients who have insomnia\nproblems. the response is the patient\u2019s reported time in minutes\nto fall\nasleep after going to bed. patients responded before and following a two-week\ntreatment period. the two treatments, active and placebo, form a binary\nexplanatory variable. the subjects receiving the two treatments were inde-\npendent samples.\n\ntable 11.5 displays\n\nsample marginal distributions\n\nthe four\ntreatment\u1390occasion combinations. from the initial to follow-up occasion,\ntime to falling asleep seems to shift downward for both treatments. the\ndegree of shift seems greater for the active treatment, indicating possible\ninteraction. the response variable is a discrete version of a continuous\nvariable, so by the derivation in section 7.2.3 a cumulative link model is\nnatural. the proportional odds model\n\nfor\n\n.\n\nlogit p y f j s \u2423 q \u2424 t q \u2424 x q \u2424 tx\n\n\u017e\n\n.\n\nj\n\n1\n\n2\n\n3\n\nt\n\n\u017e\n\n11.6\n\n.\n\npermits interaction between t s occasion 0 s initial, 1 s follow-up and\n\n\u017e\n\n.\n\ntable 11.4 time to falling asleep, by treatment and occasion\n\ntreatment\nactive\n\nplacebo\n\ninitial\n- 20\n20\u139030\n30\u139060\n) 60\n- 20\n20\u139030\n30\u139060\n) 60\n\n- 20\n7\n11\n13\n9\n7\n14\n6\n4\n\ntime to falling asleep\nfollow-up\n\n20\u139030\n\n30\u139060\n\n4\n5\n23\n17\n4\n5\n9\n11\n\n1\n2\n3\n13\n2\n1\n18\n14\n\n) 60\n0\n2\n1\n8\n1\n0\n2\n22\n\u017e\n\n.\nsource: from s. f. francom, c.chuang-stein, and j. r. landis, statist. med. 8: 571\u1390582 1989 .\nreprinted with permission from john wiley & sons ltd.\n\n "}, {"Page_number": 478, "text": "marginal modeling: maximum likelihood approach\n\n463\n\ntable 11.5 sample marginal distributions of table 11.4\n\ntreatment\nactive\n\nplacebo\n\noccasion\ninitial\nfollow-up\ninitial\nfollow-up\n\n- 20\n0.101\n0.336\n0.117\n0.258\n\nresponse\n\n20\u139030\n0.168\n0.412\n0.167\n0.242\n\n30\u139060\n0.336\n0.160\n0.292\n0.292\n\n) 60\n0.395\n0.092\n0.425\n0.208\n\n2\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u02c6\n1\n\nx s treatment 0 s placebo, 1 s active , but assumes the same effects for\neach response cutpoint.\nfor ml model fitting, g s 8.0 df s 6 for comparing observed to fitted\ncell counts in modeling the 12 marginal logits using these six parameters. the\nml estimates are \u2424 s 1.074 se s 0.162 , \u2424 s 0.046 se s 0.236 , and\n\u2424 s 0.662 se s 0.244 . this shows evidence of interaction. at the initial\n\u02c6\n3\nobservation, the estimated odds that time to falling asleep for the active\ntreatment is below any fixed level equal exp 0.046 s 1.04 times the esti-\nmated odds for the placebo treatment; at the follow-up observation, the\neffect is exp 0.046 q 0.662 s 2.03. in other words, initially the two groups\nhad similar distributions, but at the follow-up those with the active treatment\ntended to fall asleep more quickly.\n\n\u02c6\n2\n\nfor simpler interpretation, it can be helpful to report sample marginal\nmeans and their differences. with response scores 10, 25, 45, 75 for time to\nfall asleep, the initial means were 50.0 for the active group and 50.3 for the\nplacebo. the difference in means between the initial and follow-up responses\nwas 22.2 for the active group and 13.0 for the placebo. the difference\nbetween these differences of means equals 9.2, with se s 3.0, indicating that\nthe change was significantly greater for the active group.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n11.2.4 comparisons that control for initial response\nfor data such as table 11.4, suppose that the marginal distributions for\ninitial response are identical for the treatment groups. this is true, apart\nfrom sampling error, with random assignment of subjects to the groups.\nsuppose also that conditional on the initial response, the follow-up response\ndistribution is identical for the treatment groups. then, the follow-up marginal\ndistributions are also identical.\n\nif the initial marginal distributions are not identical, however, the differ-\nence between follow-up and initial marginal distributions may differ between\ntreatment groups, even though their conditional distributions for follow-up\nresponse are identical. in such cases, although marginal models can be\nuseful, they may not tell the entire story. it may be more informative\nto construct models that compare the follow-up responses while controlling\nfor the initial response.\n\n "}, {"Page_number": 479, "text": "464\n\nanalyzing repeated categorical response data\n\nlet y denote the follow-up response, for treatment x with initial re-\n\nsponse y . in the model\n\n2\n\n1\n\nlogit p y f j s \u2423 q \u2424 x q \u2424 y ,\n\n\u017e\n\n.\n\n2\n\n1\n\n2\n\n1\n\nj\n\n\u017e\n\n11.7\n\n.\n\n1\n\n\u2424 compares the follow-up distributions for the treatments, controlling for\ninitial observation. this is an analog of an analysis-of-covariance model, with\nordinal rather than continuous response. this cumulative logit model refers\nto a univariate response y rather than marginal distributions of a multi-\nvariate response y , y . it is an example of a transitional model, discussed in\nthe final section of this chapter.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n2\n\n1\n\n2\n\n11.2.5 ml fitting of marginal logit models*\nml fitting of marginal logit models is awkward. for t observations on an\ni-category response, at each setting of predictors the likelihood refers to i t\nmultinomial joint probabilities, but the model applies to t sets of marginal\nmultinomial parameters p y s k , k s 1, . . . , i . the marginal multinomial\n\u017e\nvariates are not independent.\n\nlet \u2432 denote the complete set of multinomial joint probabilities for all\nsettings of predictors. marginal logit models have the generalized loglinear\nmodel form\n\n.\n\n\u0004\n\n4\n\nt\n\nc log a\u2432 s x\u2424\n\n\u017e\n\n.\n\n\u017e\n\n11.8\n\n.\n\nt\n\n\u0004\n\n\u017e\n\nintroduced in section 8.5.4. in the binary case, the matrix a applied to \u2432\nforms the t marginal probabilities p y s 1\n.4\nand their complements at\neach setting of predictors. the matrix c applied to the log marginal probabil-\nities forms the t marginal logits for each setting; each row of c has 1 in the\nposition multiplied by the log numerator probability for a given marginal\nlogit, y1 in the position multiplied by the log denominator probability, and 0\nelsewhere.\nfor instance, for the model of marginal homogeneity in a 2t table with no\ncovariates, \u2424 is a single parameter, denoted by \u2423 in 11.1 . for t s 2, \u2432 has\nfour elements, and this model is\n\n\u017e\n\n.\n\n1 y1\n0\n0\n\n0\n0\n1 y1\n\nlog\n\n1\n0\n1\n0\n\n1\n0\n0\n1\n\n0\n1\n1\n0\n\n\u2432\n\n11\n0\n\u2432\n1\n12\n0 \u2432\n21\n1 \u2432\n22\n\ns\n\n1\n1\n\n\u2423,\n\nw\n\n\u017e\n\n\u017e\n\nwhich sets both logit \u2432 q \u2432 s logit p y s 1 and logit \u2432 q \u2432 s\nlogit p y s 1 equal to \u2423.\n\nthe likelihood function ll \u2432 for a marginal logit model is the product of\nthe multinomial mass functions from the various predictor settings. one\n\n.x\n\n11\n\n12\n\n11\n\n21\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n1\n\n2\n\n.x\n\nw\n\n "}, {"Page_number": 480, "text": "marginal modeling: maximum likelihood approach\n\n465\n\napproach for ml fitting views the model as a set of constraints and uses\nmethods for maximizing a function subject to constraints. in model 11.8 , let\nu denote a full column rank matrix such that the space spanned by the\ncolumns of u is the orthogonal complement of the space spanned by the\ncolumns of x. then, ux x s 0, and the model has the equivalent constraint\nform\n\n\u017e\n\n.\n\nuxc log a\u2432 s 0.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nx\n\nx\n\n\u017e\n\n.\n\n\u017e\n\nfor instance, for marginal homogeneity in a 2 = 2 table with 11.8 as\nexpressed above, u s 1, y1 . then u applied to c log a\u2432 sets the differ-\n.\nence between the row and column marginal logits equal to 0.\n\nthis method of maximizing the likelihood incorporates these model con-\nstraints as well as identifiability constraints, which constrain the response\nprobabilities at each predictor setting to sum to 1. we express this collection\nof model constraints u c log a\u2432 s 0 and identifiability constraints as f \u2432\n.\n\u017e\ns 0. the method introduces lagrange multipliers corresponding to these\nconstraints and solves the lagrangian likelihood equations using a newton\u1390\nraphson algorithm aitchison and silvey 1958; haber 1985 . let \u242a be a\nvector having elements \u2432 and the lagrange multipliers \u242d. the lagrangian\nlikelihood equations have form h \u242a s 0, where\n\n\u017e .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nx\n\nh \u242a s h \u2432 , \u242d s f \u2432 , \u2b78log ll \u2432 r\u2b78\u2432 q \u2b78f \u2432 r\u2b78\u2432 \u242d\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nx\n\nx\n\n.\n\nis a vector with terms involving the contrasts in marginal logits that the model\nspecifies as constraints as well as log-likelihood derivatives.\n\nthe newton\u1390raphson method then is\ny1\n\n\u017etq1.\n\n\u242a\n\ns \u242a y\n\n\u017et.\n\n\u017et.\n\n.\n\n\u2b78h \u242a\u017e\n\u2b78\u242a\n\n\u017e\nh \u242a\n\n\u017et.\n\n.\n\n,\n\nt s 1, . . . .\n\n\u017e\n\n.\n\nthis can be computationally intensive because the derivative matrix inverted\nhas dimensions larger than the number of elements in \u2432. a refinement lang\n1996a; lang and agresti 1994 uses an asymptotic approximation to a\nreparameterized derivative matrix that has a much simpler form, requiring\ninverting only a diagonal matrix and a symmetric positive definite matrix.\n\nthis ml marginal fitting method is available in specialized software\n\u017e\nappendix a mentions an s-plus function . it makes no assumption about the\nmodel that describes the joint distribution \u2432. thus, when the marginal model\nthe ml estimate of \u2424 in 11.8 is consistent regardless of the\nholds,\ndependence structure for that distribution. several alternative fitting ap-\nproaches have been considered. lang and agresti 1994 simultaneously\nfitted a marginal model and an unsaturated loglinear model for \u2432. the\ncomplete model can be specified as a special case of 11.8 and fitted using\nthe constraint approach with lagrange multipliers just described. in standard\nthe marginal and joint model parameters are orthogonal. if the\ncases,\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 481, "text": "466\n\nanalyzing repeated categorical response data\n\n\u017e\n\n.\n\n.\n\n\u017e\n\nmarginal model holds, the ml estimator of the marginal model parameters is\nconsistent even if the model for the joint distribution is incorrect.\n\nfitzmaurice and laird 1993 gave a related ml approach. a one-to-one\ncorrespondence holds between \u2432 and parameters of the saturated loglinear\nmodel. they used a further one-to-one correspondence between the main\neffect and the higher-order parameters of that loglinear model with the\nmarginal probabilities and those same higher-order loglinear parameters.\nmodels were then specified separately for the marginal probabilities and the\nhigher-order conditional\nloglinear parameters. the likelihood is then maxi-\nmized in terms of the two sets of model parameters. again, the two sets of\nparameters are orthogonal, so the ml estimator of marginal model parame-\nters is consistent when the marginal model holds. this mixed parameter\napproach is also available in specialized software kastner et al. 1997; see\n.\nalso appendix a .\n\nyet another ml approach uses a one-to-one correspondence between \u2432\nand parameters that describe the marginal distributions, the bivariate distri-\nbutions, the trivariate distributions, and so on e.g., glonek and mccullagh\n1995; molenberghs and lesaffre 1994 . multivariate logistic models then\napply to the component distributions, although some higher-order effects\nmay be assumed to vanish, for simplicity. glonek 1996 proposed a hybrid of\nthis and the fitzmaurice and laird 1993 approach.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n11.3 marginal modeling: generalized estimating\nequations gee approach\n\n(\n\n)\n\nat each combination of predictor values, ml fitting assumes a multinomial\ndistribution for the i t cell probabilities for the t observations on an\ni-category response. as the number of predictors increases, the number of\nmultinomial probabilities increases dramatically. currently, all the ml ap-\nproaches described above are not practical when t is large or there are many\npredictors, especially when some are continuous. compared to the con-\ntinuous-response case using the multivariate normal, marginal modeling of\nmultivariate categorical responses is also hindered by the lack of a simple\nmultivariate distribution for describing correlations among the t responses.\nfor instance, with t means and a common variance and correlation, the\nmultivariate normal has only t q 2 parameters, compared to the i t y 1\nparameters for the multinomial.\n\nan alternative to ml fitting uses a multivariate generalization of quasi-\nlikelihood section 4.7 . rather than assuming a particular distribution for y,\nthe quasi-likelihood method specifies only the first two moments; it links the\nmean to a linear predictor and also specifies how the variance depends on\nthe mean. the estimates are solutions of estimating equations that are\nlikelihood equations under the further assumption of a distribution in the\n.\nexponential family with that mean and variance wedderburn 1974 .\n\n.\n\n\u017e\n\n\u017e\n\n "}, {"Page_number": 482, "text": "marginal modeling: generalized estimating equations approach\n\n467\n\n2\n\n1\n\n\u017e\n\n.\n\n\u017e\n\n11.3.1 generalized estimating equation methodology: basic ideas\n.\nrepeated measurement provides a multivariate response y , y , . . . , y ,\nwhere t sometimes varies by subject. as in the univariate case, the quasi-\nlikelihood method specifies a model for \u242es e y and specifies a variance\nfunction \u00ae \u242e describing how var y depends on \u242e. now, though, that model\napplies to the marginal distribution for each y . the method also requires a\nworking guess for the correlation structure among y . the estimates are\nsolutions of quasi-likelihood equations called generalized estimating equations.\n.\nthe method is often referred to as the gee method. liang and zeger 1986\nproposed it for marginal modeling with glms. their work built on related\nmaterial in the econometrics literature e.g., gourieroux et al. 1984; hansen\n1982; white 1982 . we outline concepts here and give more details in section\n11.4.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u0004\n\n4\n\nt\n\nt\n\nt\n\nt\n\n2\n\nt\n\n4\n\n\u0004\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\nthe gee approach utilizes an assumed covariance structure for\n\u017e\ny , y , . . . , y , specifying a variance function and a pairwise correlation\n1\npattern, without assuming a particular multivariate distribution. the gee\nestimates of model parameters are valid even if one misspecifies the covari-\nance structure. consistency i.e., estimates converging in probability to the\ntrue parameters depends on the first moment but not the second. specifi-\ncally, suppose that the model is correct in the sense that the chosen link\nfunction and linear predictor truly describe how e y depend on the\nt s 1, . . . , t. then the gee model parameter estimators are\npredictors,\nconsistent.\n\nin practice, a chosen model is never exactly correct. this result is useful,\nhowever, for suggesting that the correlation structure need not adversely\naffect the quality of estimates for whatever model one uses. often, no\na priori information is available about this structure, and the correlation is\nregarded as a nuisance. a simple implementation of the gee method naively\ntreats y as pairwise independent. although parameter estimates are usually\nfine under this naive assumption, standard errors are not. more appropriate\nstandard errors result from an adjustment the gee method makes using the\nempirical dependence the data exhibit. the naive standard errors based on\nthe independence assumption are updated using the information the data\nprovide about the actual dependence structure to yield more appropriate\n\u017e\nrobust standard errors.\n\nas an alternative to estimates that treat y as pairwise independent, the\ngee method can use a working guess about the correlation structure but\nagain empirically adjust the standard error. the exchangeable working corre-\nlation structure treats corr y , y as identical for all s and t. this is more\nflexible and realistic than the naive independence assumption. even more\nrealistic is an unstructured working correlation that permits a separate\ncorrelation for each pair. when t is large, however, this approach suffers\nsome efficiency loss because of the many additional parameters.\n\nin theory, choosing the working correlation wisely can pay benefits of\nimproved efficiency of estimation. however, liang and zeger 1986 noted\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n4\n\ns\n\nt\n\nt\n\nt\n\n "}, {"Page_number": 483, "text": "468\n\nanalyzing repeated categorical response data\n\n.\n\nthat estimators based on independence working correlation can have surpris-\ningly good efficiency when the actual correlation is weak to moderate. one\ncan check the sensitivity to the selection by comparing results for different\nworking correlation assumptions. in our experience, when the correlations\nare modest, all working correlation structures yield similar gee estimates\nand standard errors, as the empirical dependence has a large impact on\n\u017e\nadjusting the naive standard errors.\nif they differed substantially, a more\ncareful study of the correlation structure would be necessary. unless one\nexpects dramatic differences among the correlations, we recommend the\nexchangeable working correlation structure. this recognizes the dependence\nat the cost of only one extra parameter.\n\nthe gee approach is appealing for categorical data because of its\ncomputational simplicity compared to ml. advantages include not requiring\na multivariate distribution and the consistency of estimation even with\nmisspecified correlation structure. however,\nit has limitations. since the\ngee approach does not completely specify the joint distribution, it does not\nhave a likelihood function. likelihood-based methods are not available for\ntesting fit, comparing models, and conducting inference about parameters.\ninstead, inference uses wald statistics constructed with the asymptotic nor-\nmality of the estimators together with their estimated covariance matrix.\nhowever, unless the sample size is quite large, the empirically based standard\nerrors tend to underestimate the true ones e.g., firth 1993b . as estimators,\nthose standard errors can also show more variability than parametric estima-\ntors kauermann and carroll 2001 . boos 1992 and rotnitzky and jewell\n\u017e\n1990 proposed analogs of score tests for effects of predictors, using quasi-\nlog-likelihood, that may be more trustworthy than wald tests. some statisti-\ncians e.g., lindsey 1999 are critical of the gee approach because of the\nlack of likelihood. others do not find this problematic, as they regard gee\nas an estimation method rather than a model.\n\n\u017e\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n11.3.2 longitudinal mental depression example\nfor table 11.2 comparing two treatments for mental depression, ml fitting\nof a logit model with drug = time interaction was used in section 11.2.1. the\ngee analysis provides similar results, regardless of the choice of working\ncorrelation structure. with the exchangeable structure, the gee estimated\nslope on the logit scale for the standard drug is \u2424 s 0.48 se s 0.12 . for\nthe new drug the slope increases by \u2424 s 1.02 se s 0.19 . table 11.6 shows\nresults using the independence working correlations. estimates are the same\nto two decimal places. the initial estimates and standard errors there are\nthose that apply if the repeated responses are truly independent. they equal\nthose obtained by using ordinary logistic regression with 3 = 340 s 1020\nindependent observations rather than treating the data as three dependent\nobservations for each of 340 subjects. the empirical standard errors incorpo-\nrate the sample dependence to adjust the independence-based standard\nerrors.\n\n\u02c6\n3\n\n\u02c6\n4\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n "}, {"Page_number": 484, "text": "marginal modeling: generalized estimating equations approach\n\n469\n\ntable 11.6 output from using gee to fit logit model to table 11.2\n\ninitial parameter estimates\n\ngee parameter estimates\n\nparameter\nestimate\nintercept y0.0280\ny1.3139\ndiagnose\ny0.0596\ndrug\n0.4824\ntime\ndrug)time\n1.0174\n\nstd error\n\n0.1639\n0.1464\n0.2222\n0.1148\n0.1888\n\nempirical std error estimates\n\nparameter\nestimate\nintercept y0.0280\ny1.3139\ndiagnose\ny0.0596\ndrug\n0.4824\ntime\ndrug)time\n1.0174\n\nstd error\n\n0.1742\n0.1460\n0.2285\n0.1199\n0.1877\n\nworking correlation matrix\n\ncol1\n\n1.0000\n0.0000\n0.0000\n\ncol2\n\n0.0000\n1.0000\n0.0000\n\ncol3\n\n0.0000\n0.0000\n1.0000\n\nrow1\nrow2\nrow3\n\nwith exchangeable correlation structure, the estimated common correla-\ntion between pairs of the three responses is y0.003. the successive observa-\ntions apparently have pairwise appearance like independent observations.\nthis is quite unusual for repeated measurement data. for this reason, similar\nresults occur from fitting the model assuming the three observations for a\n\u017e\nsubject actually come from three separate subjects\ni.e., assuming 1020\n.\nindependent observations .\n\n\u017e\n\n.\n\n11.3.3 gee approach for multinomial responses: insomnia example\nliang and zeger 1986 originally specified the gee methodology for model-\ning univariate marginal distributions, such as the binomial and poisson. it\n.\nextends to marginal modeling of multinomial responses. lipsitz et al. 1994\noutlined a gee approach for cumulative logit models with repeated ordinal\nresponses. with this approach, for each pair of outcome categories one\nselects a working correlation matrix for the pairs of repeated observations.\neach multinomial response at a fixed observation uses the i y 1 = i y 1\n.\nmultinomial covariance matrix. section 11.4.4 has details.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nwe illustrate for the insomnia data of table 11.4. in section 11.2.3 we\n\nused ml to fit the marginal model\n\nlogit p y f j s \u2423 q \u2424 t q \u2424 x q \u2424 tx\n\n\u017e\n\n.\n\nj\n\n1\n\n2\n\n3\n\nt\n\nt\n\nfor y s time to fall asleep with treatment x at occasion t. with indepen-\nthe gee estimates are \u2424 s 1.038\ndence working correlation structure,\nse s 0.168 , \u2424 s 0.034 se s 0.238 , and \u2424 s 0.708 se s 0.244 . the\n\u017e\nestimates are similar to the ml estimates, and the substantive conclusions\nare the same. considerable evidence exists that the distribution of time to fall\nasleep decreased more for the treatment group than for the placebo group.\n\n\u02c6\n3\n\n\u02c6\n2\n\n\u02c6\n1\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 485, "text": "470\n\nanalyzing repeated categorical response data\n\n11.4 quasi-likelihood and its gee multivariate\nextension: details*\n\ni\n\n.\n\n\u017e\n\na glm assumes a certain distribution for the response variable. sometimes\nit is unclear how to select it. however, often there is a plausible relationship\nbetween the mean and variance, such as \u00ae \u242e s \u243e\u242e for count data. then,\n.\nan alternative to ml estimation is quasi-likelihood estimation section 4.7 .\nwe next present some details about this method and its gee extension for\nmarginal modeling of multivariate responses.\nwe begin with models for a single response and later discuss marginal\nmodels for a multivariate response. for subject i, i s 1, . . . , n, let y be the\noutcome on y with \u242e s e y and variance function \u00ae \u242e , and let x be the\nvalue of explanatory variable j. for link function g, the linear predictor is\n\u2429 s g \u242e s \u00fd \u2424 x s x \u2424. the quasi-likelihood ql parameter estimates\ni\n\u02c6\u2424 are the solutions of quasi-score equations\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nx\ni\n\ni j\n\ni j\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nj\n\nj\n\nu \u2424 s\n\u017e\n\n.\n\n\u00fd\n\ni\n\n\u017e\n\nx\u2b78\u242e\n\n/\u2b78\u2424\n\ni\n\n\u00ae \u242e\n\u017e\ni\n\n.\n\ny1\n\n\u017e\n\ny y \u242e s 0,\n\n.\n\ni\n\ni\n\n\u017e\n\n11.9\n\n.\n\nwhere \u242e s g\nhood equations 4.22 for glms when we substitute\n\ny1\u017e x\nx \u2424 . these estimating equations are the same as the likeli-\ni\u017e\n\n.\n.\n\ni\n\ns\n\n\u2b78\u242e\ni\n\u2b78\u2424\nj\n\ni\n\n\u2b78\u242e \u2b78\u2429\ni\n\u2b78\u2429 \u2b78\u2424\nj\n\ni\n\ns\n\n\u2b78\u242e\ni\n\u2b78\u2429\ni\n\nx .i j\n\ni\n\ni\n\ni\n\n4\n\n\u0004\n\n\u2c91\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ny\n\nthey are not likelihood equations, however, without the extra assumption\nthat\nhas distribution in the natural exponential family. under that\nassumption, \u00ae \u242e characterizes the distribution within the natural exponen-\ntial family jorgensen 1987 . another motivation for equations 11.9 is that\nwith \u00ae \u242e replaced by known variance \u00ae , they result from the weighted least\nsquares problem of minimizing \u00fd y y \u242e \u00ae\n\n.2 y1\n\nthe likelihood equations 4.22 for a glm depend only on the mean and\nand the link function g, which determines \u2b78\u242er\u2b78\u2429. thus,\nvariance of\nwedderburn 1974 suggested using them as estimating equations for any\nlink and variance function, even if they do not correspond to a particular\nmember of the natural exponential family.\n\ny\ni\n\u017e\n\ni\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n.\n\n\u017e\n\n11.4.1 properties of quasi-likelihood estimators\n.\nin the quasi-likelihood ql method, the quasi-score function u \u2424 in 11.9\nis called an unbiased estimating function; this term refers to any function\nh y; \u2424 of y and \u2424 such that e h y; \u2424 s 0 for all \u2424. the equations 11.9\n\u017e\n.\nthat determine \u2424 are called estimating equations.\n\nw \u017e\n\n.x\n\n\u02c6\n\nthe quasi-likelihood method treats the quasi-score function as the deriva-\ntive of a function called the quasi-log likelihood. this function may not be a\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\nj\n\n "}, {"Page_number": 486, "text": "quasi-likelihood and its gee multivariate extensions: details\n\n471\n\nproper log likelihood function. nonetheless, mccullagh 1983 showed that\nql estimators have properties similar to those of ml estimators. for\ninstance, the ql estimators \u2424 are asymptotically normal with covariance\nmatrix approximated by\n\n\u02c6\n\n\u017e\n\n.\n\nv s\n\n\u017e\n\n\u00fd\n\ni\n\n\u2b78\u242e\ni\n\u2b78\u2424\n\nx\n\n/\n\n\u00ae \u242e\n\u017e\ni\n\n.\n\ny1\n\n\u017e\n\n\u2b78\u242e\ni\n\u2b78\u2424\n\n/\n\ny1\n\n.\n\n\u017e\n\n11.10\n\n.\n\nj\n\nj\n\ni\n\np\n\ni j\n\nw\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u02c6\n\nthis is equivalent to the formula for the large-sample covariance matrix of\n.x\nthe ml estimator in a glm which is estimated by 4.28 .\n\n\u02c6\na key result is that the ql estimator \u2424 is consistent for \u2424 i.e., \u2424\n\n\u017e\nwhen truly \u242e s g \u00fd \u2424 x\n\n.\n\u2424\neven if the variance function is misspecified, as long as the specification is\ncorrect for the link function and linear predictor. that is, assuming that the\nmodel form g \u242e s \u00fd \u2424 x\nis correct, the consistency of \u2424 holds even if the\ntrue variance function is not \u00ae \u242e . we now give a heuristic explanation for\nthis.\nj.\nfrom 11.9 , u \u2424 rn is a vector of sample means. by a law of large numbers,\nit converges in probability to its expected value of 0. the solution \u2424 of the\nquasi-score equations is a continuous function of these sample means, so it\nconverges to \u2424, since \u2424 is the value of \u2424 for which the sum is exactly equal\nto 0. the consistency also follows from general results for unbiased estimat-\n.\ning functions liang and zeger 1995 .\n\n.\n, then from 11.9 , e u \u2424 s 0 for all\n.\n\ny1\u017e\n\n.x\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\ni\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nw\n\ni j\n\ni\n\nj\n\nj\n\nj\n\n11.4.2 sandwich covariance adjustment for variance misspecification\nif one assumes that var y s \u00ae \u242e but the true var y / \u00ae \u242e , then the\nactual asymptotic covariance matrix of the ql estimator \u2424 is not v as given\n.\nin 11.10 . instead, it is diggle et al. 2001; white 1982\n\n.\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ni\n\n\u017e\n\nx\u2b78\u242e\ni\n\u2b78\u2424\n\n/\n\nv\n\n\u00fd\n\ni\n\n\u00ae \u242e\n\u017e\ni\n\n.\n\ny1\n\nvar y\ni\n\n\u017e\n\n.\n\n\u00ae \u242e\n\u017e\ni\n\n.\n\ny1\n\n\u017e\n\n\u2b78\u242e\ni\n\u2b78\u2424\n\n/\n\nv.\n\n\u017e\n\n11.11\n\n.\n\ni\n\ni\n\ni\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\neven though the variances are scalar, we express the matrices in this form to\n.\nmotivate the gee multivariate extension discussed below. matrix 11.11\nsimplifies to v if var y s \u00ae \u242e . in practice, the true variance function is\n\u017e\nunknown. a consistent estimator of 11.11 is a sample analog, replacing \u242e\ni\nby \u242e and var y by\nliang and zeger 1986 . the estimated\ncovariance matrix is valid regardless of whether the variance specification\n\u00ae \u242e is correct. this estimated covariance matrix is called a sandwich\n\u017e\nestimator, because the empirical evidence is sandwiched between the model-\ndriven covariance matrices.\n\n\u017e\ny y \u242e\n\u02c6\n\n.2 \u017e\n\n\u02c6\n\nin summary, even with incorrect specification of the variance function, one\ncan still consistently estimate \u2424 and one can estimate the asymptotic variance\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ni\n\n6\n "}, {"Page_number": 487, "text": "472\n\nanalyzing repeated categorical response data\n\n\u02c6\n\n\u017e\n\n.\n\nof \u2424 by estimating the sandwich adjustment 11.11 . however, some effi-\nciency loss occurs when the variance chosen, \u00ae \u242e , is wildly inaccurate. also,\nthe number of clusters n may need to be large for the sample version of\n\u017e\n11.11 to work well; otherwise, it can be biased downward. of course, a\nmodeling process never gets anything exactly correct. just as the variance\nfunction chosen only approximates the true one hopefully, closely , so is the\nspecification for the mean only approximate.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\ni1\n\n\u017e\n\nit\n\n.x\n\n11.4.3 gee methodology: technical details\nnow we consider the generalized estimating equations gee multivariate\nand \u242e s\ngeneralization of ql. for subject\n\u242e , . . . , \u242e , where \u242e s e y . the number t of responses may vary by\n\u017e\ncluster. let x denote a p = 1 vector of explanatory variable values for y .\nit\nthe notation allows for cases where explanatory variables also vary for the\nrepeated measurements. the linear predictor of the model is \u2429 s g \u242e s\nxx \u2424 for link function g. the model refers to the marginal distribution at\nit\neach t rather than the joint distribution. let x be the t = p matrix of\npredictor values for cluster or subject\n\nlet y s y , . . . , y\n\ni, for which row t is x .it\n\n.\n.x\n\ni,\n\nit\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\ni1\n\nit\n\nit\n\nit\n\nit\n\nit\n\nx\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nwe assume that y has probability mass function of form\n\nit\n\nf y ; \u242a , \u243e s exp\n\u017e\n\n.\n\nit\n\nit\n\n\u00bd\n\n5\ny \u242a y b \u242a \u243eq c y , \u243e .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nit\n\nit\n\nit\n\nit\n\nwhen \u243e is known, this is the natural exponential\nparameter \u242a . from section 4.4.1,\n\u242e s e y s bx \u242a ,\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nit\n\nit\n\nit\n\nit\n\n\u00ae \u242e s var y s by \u242a \u243e.\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nit\n\nit\n\nit\n\nfamily with natural\n\nthe gee method also assumes a working correlation matrix r \u2423 for y ,i\ndepending on parameters \u2423. the exchangeable working correlation has\ncorr y , y s \u2423 for each pair in y . let b \u242a s b \u242a , . . . , b \u242a , and let\nb denote a diagonal matrix with main diagonal elements b \u242a . then the\nworking covariance matrix for y isi\n\n\u017e\niti\ny\u017e .\ni\n\n\u017e .\n\n\u017e \u017e\n\n..\n\n\u017e\n\n.\n\n.\n\ni1\n\ni s\n\nit\n\ni\n\ni\n\ni\n\n\u017e\n\n.\n\nv s b1r2 r \u2423 b1r2\u243e.\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\n\u017e\n\n11.12\n\n.\n\ni\n\ni\n\ni\n\n.\n\n\u017e\n\nnote that v s cov y if r is the true correlation matrix for y .\nnow let \u232c be the diagonal matrix with elements \u2b78\u242a r\u2b78\u2429 on the main\ndiagonal for t s 1, . . . , t . for the canonical link, this is the identity matrix.\n.\nlet d s \u2b78\u242e r\u2b78\u2424 s b \u232c x be a t = p matrix with typical element express-\ning \u2b78\u242e r\u2b78\u2424 in the form \u2b78\u242e r\u2b78\u242a \u2b78\u242a r\u2b78\u2429 \u2b78\u2429 r\u2b78\u2424 . from 11.9 , for\nunivariate glms the quasi-likelihood estimating equations have the form\n\n.\u017e\n\n.\u017e\n\ni\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nit\n\nit\n\nit\n\nit\n\nit\n\nit\n\nit\n\nit\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nj\n\nj\n\n\u00fd i\n\n\u2b78\u242er\u2b78\u2424 \u00ae \u242e\ni\n\n\u017e\n\n.\n\n\u017e\n\nx\n\ni\n\ny1\n\n.\n\ny y \u242e \u2424 s 0,\n\n\u017e\n\n.\n\ni\n\ni\n\n "}, {"Page_number": 488, "text": "quasi-likelihood and its gee multivariate extensions: details\nwhere \u242e s \u242e \u2424 s g\nthe set of generalized estimating equations\n\ny1\u017e x\nx \u2424 . the analog of this in the multivariate case is\ni\n\n473\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\nn\n\nx y1\n\n\u00fd i\nd v\ni\nis1\n\ny y \u242e \u2424 s 0.\n\n\u017e\n\n.\n\ni\n\ni\n\n\u02c6\n\nthe gee estimator \u2424 is the solution of these equations.\nthe naive approach, which sets r \u2423 s i, treats pairs of responses as\nindependent. in that case, 11.12 simplifies to v s b \u243e, and the generalized\nestimating equations simplify to\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\ny1\nd v\ni\n\nx\ni\n\n\u00fd\n\ni\n\ny y \u242e \u2424 s x \u232c b v\n\n\u017e\n\n.\n\n\u00fd\n\nx\ni\n\ni\n\ni\n\ni\n\ni\n\ny1\ni\n\ni\n\ny y \u242e \u2424\n\u017e\n\n.\n\ni\n\ni\n\ns 1r\u243e x \u232c y y \u242e \u2424 s 0,\n\n\u00fd i\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nx\n\ni\n\ni\n\ni\n\ni\n\nw\n\n.x\n\ni\n\ni\n\ni\n\ni\n\nit\n\ni1\n\nx\ni\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\niti\n\nor \u00fd x \u232c y y \u242e \u2424 s 0. the solution \u2424 is then the same as the ordinary\nestimator for a glm with the chosen link function and variance function,\ntreating y , . . . , y\n\nas independent observations.\n\n\u02c6\n\n< tys <\n\nnormally, one selects a working correlation matrix permitting dependence,\nsuch as the exchangeable structure. for time-series data, also popular is the\nautoregressive structure, corr y , y s \u2423 , which treats observations far-\n.\nther apart in time as more weakly correlated. liang and zeger 1986\nsuggested computing the gee estimates by iterating between a modified\nfisher scoring algorithm for solving the generalized estimating equations for\n\u2424 given current estimates of \u2423 and \u243e and using residuals for moment\nestimation of \u2423 and \u243e based on the current estimates of \u2424 . they suggested\nestimates of r \u2423 for a variety of correlation structures. alternative algo-\nrithms simultaneously solve estimating equations for \u2424 and for association\nparameters e.g., liang et al. 1992; see also note 11.8 . gee algorithms need\nnot converge, but often one iteration gives adequate results lipsitz et al.\n.\n1991 .\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\ni s\n\nliang and zeger 1986 showed asymptotic normality and consistency as\n\n\u017e\n\n.\n\nthe number of clusters n increases. under certain regularity conditions,\n\n\u02c6'n \u2424 y \u2424\n.\nhere, generalizing 11.11 , v s lim\n\n\u017e\n\n\u017e\n\n.\n\ng\n\nd\n\n\u017e\n\nn 0, v\ng\n\n.\n\n.\n\nn\u2122\u2b01 g, n\n\nv\n\nwith\n\nv s n\n\ng , n\n\n\u00fd\n\ni\n\nd v d\n\ny1\ni\n\nx\ni\n\ny1\n\ni\n\n\u00fd\n\ni\n\nd v cov y v d\n\n\u017e\n\n.\n\ni\n\ny1\ni\n\ny1\ni\n\nx\ni\n\nd v d\n\ny1\ni\n\nx\ni\n\ny1\n\n.\n\ni\n\ni\n\n\u00fd\n\ni\n\nthe estimated covariance matrix v rn of \u2424 replaces \u2424 with \u2424, \u243e with \u243e,\n\u02c6\n\u2423 with \u2423 , and cov y by y y \u242e \u2424 y y \u242e \u2424 . the purpose of the\n\n\u02c6\ng, n \u02c6\n\u017e\n\n.x\n\u02c6 x\n\n.xw\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\nw\n\ni\n\ni\n\ni\n\ni\n\n\u02c6\n\ni\n\n6\n "}, {"Page_number": 489, "text": "474\n\nanalyzing repeated categorical response data\n\ni\n\ni\n\ni\n\n\u017e\n\nsandwich estimator is to use the data\u2019s empirical evidence about covariation\nto adjust the standard errors in case the true covariance differs substantially\nfrom the working guess.\nwhen the working correlation structure is the true one and cov y s v ,\n.\ni\ni\nthe asymptotic covariance matrix v rn simplifies to \u00fd d v d\n. this is\nthe relevant covariance if we put complete faith in our guess about the\ncorrelation structure.\n\nx y1\ni\n\n\u017e\n.y1\n\ng,n\n\nwith binary data, the correlation may not be the best way to express the\nwithin-cluster association. the marginal probabilities constrain the possible\ncorrelation values, since the range of possible values for e y y s p y s 1,\ny s 1 depends on p y s 1 and p y s 1 . an alternative approach uses\ni s\nthe odds ratio, for instance by modeling the log odds ratios for pairs in a\ncluster as exchangeable. this has the advantage that the association parame-\nters are distinct from the means. see fitzmaurice et al. 1993 and lipsitz\net al. 1991 . carey et al. 1993 suggested an iterative alternating logistic\nregressions algorithm. it alternates between a gee step for the regression\nparameters in the model for the mean and a step for an association model for\nthe log odds ratio. this is useful when the structure of the association is itself\na major focus rather than a nuisance.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\ni s\n\ni s\n\nit\n\nit\n\nit\n\nit\n\n\u017e\n\n.\n\n\u017e\n\nj s 1 if observation t\n\u017e .\n\n11.4.4 gee approach: multinomial responses\n.\nwe now briefly describe the lipsitz et al. 1994 gee approach for marginal\nmodeling with a multinomial response. this is appropriate, for instance, with\ncumulative logit models. let y\nin cluster i has\nj s 1, . . . , i y 1 . let y be the t i y 1 binary indicators for\n.\noutcome j\n.x\nw\ncluster i. then, one selects a t i y 1 = t i y 1 working covariance\n\u017e\nfor each pair of\nmatrix v for y , specifying a pattern for corr y j , y k\nj, k and each pair t, s . the i y 1 = i y 1 block of\n\u017e\n.\n\u017e\noutcome categories\ni y 1 is a multinomial covariance matrix with \u00ae\nj s\n\u017e .\n\u017e\n..\nv for\ny 1 , . . . , y\nit\nit\nit\n.x\np y j s 1 1 y p y j s 1\non the main diagonal and yp y j s\n\u017e\n\u017e\n\u017e .\n\u017e .\n\u017e\nit\nit\n1 p y k s 1 off\n\u017e\n.\nit. the remaining elements of v contain elements\n\u017e\ncov y j , y k . for instance, one possibility is the exchangeable structure,\ncorr y j , y k s \u2433 for all t and s.\n\u017e\nin this approach the generalized estimating equations for \u2424 again have the\n\n\u017e\n\u017e .\n\u017e\nit\n\u017e .\n\u017e .\n\n\u017e\nw\n\u017e\ni\u017e\n\n.\n..\n..\n\n\u017e .\n.w\n\n\u017e .\n\u017e\n\n\u017e\n\u017e\n\n.x\n\n..\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\njk\n\ni s\n\ni s\n\ni s\n\nit\n\nit\n\nit\n\nit\n\nit\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nform\n\nn\n\n.\n\nu \u2424 s d v\n\u00fd i\n\u017e\nis1\n\ni\n\nx y1\n\n\u017e\n\ny y \u242e s 0,\n\n.\n\ni\n\ni\n\ni\n\nwhere \u242e is the vector of probabilities associated with y , dx s \u2b78\u242ex r\u2b78\u2424, and\nthe parameters are evaluated at their current estimates. lipsitz et al. sug-\ngested a fisher scoring algorithm for solving these equations and a method of\nmoments update for estimating \u2433 at each step of the iteration. an\n\n\u0004\n\n4\n\ni\n\ni\n\ni\n\njk\n\n "}, {"Page_number": 490, "text": "quasi-likelihood and its gee multivariate extensions: details\n\n475\n\nempirically adjusted sandwich covariance matrix of \u2424 is again\n\n\u02c6\n\nn\n\n\u00fd\nis1\n\nd v d\n\ny1\ni\n\nx\ni\n\ny1\n\ni\n\nn\n\n\u00fd\nis1\n\nd v cov y v d\n\n\u017e\n\n.\n\ni\n\ny1\ni\n\ny1\ni\n\nx\ni\n\nd v d\n\ny1\ni\n\nx\ni\n\ny1\n\n.\n\ni\n\nn\n\n\u00fd\nis1\n\ni\n\n\u017e\nthis is estimated by substituting \u242e from the model fit and replacing cov y\ni\nby the empirical covariance matrix of y .i\n\n\u02c6 i\n\n.\n\n11.4.5 dealing with missing data\nunfortunately, studies with repeated measurement often have cases for\nwhich at least one response in a cluster is missing. in a longitudinal study, for\ninstance, some subjects may drop out before its conclusion. when data are\nmissing, analyzing the observed data alone as if no data are missing can result\nin biased estimates.\n\nan advantage of the gee method is that different clusters can have\ndifferent numbers of observations. the data input file has a separate line for\neach observation, and for longitudinal studies, computations use those times\nfor which a subject has an observation. however, bias can arise in gee\nestimates unless one can make certain assumptions about why the data are\nmissing.\n\nlet y \u017e o. denote the observed responses, y \u017e m. the missing responses, and y\ntheir union. let m denote a missing data indicator that equals 1 when an\nobservation is missing and 0 otherwise. little and rubin 1987 called the\ndata missing completely at random if m is statistically independent of y; that\nis, the probability that an observation is missing is independent of that\nobservation\u2019s value, although it may depend on the explanatory variables.\nless restrictively, they called the data missing at random if the distribution of\n\u017e\n\u017e o.\nm y equals that of m y\nand not on the missing values.\n\n; that is, missingness depends only on y\n\n\u017e o..\n\nwhen either of these is plausible, with a likelihood-based analysis it is not\nnecessary to model the missingness mechanism. an analysis using only y \u017e o.\nis not systematically biased. the same is true with gee methods when\nestimating equations can be weighted by response probabilities robins et al.\n1995 . otherwise, however, with non-likelihood-based methods such as gee,\nthe missingness process can be ignored only when data are missing com-\npletely at random. kenward et al. 1994 illustrated the breakdown in gee\nestimates when the data are not missing completely at random.\n\noften, missingness depends on the missing values. for instance,\n\nin a\nlongitudinal study measuring pain, perhaps a subject dropped out when the\npain got above some threshhold. then, more complex analyses are needed\nthat model the joint distribution of y and m little 1998 . let f \u2b48 denote a\ngeneric probability mass function, which also depends on explanatory vari-\nables x and parameters. selection models factor the joint distribution of y\n\n\u017e .\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n<\n\n<\n\n "}, {"Page_number": 491, "text": "476\n\nand m as\n\nanalyzing repeated categorical response data\n\nf y, m; x, \u2424, \u243a s f y; x, \u2424 f m y; x, \u243a ,\n.\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n<\n\n.\n\n\u017e\n\n.\nwhere f y; x, \u2424 is the model in the absence of missing values and f m y; x, \u243a\nis the model for the missing-data mechanism. pattern mixture models use the\nalternative factorization,\n\n\u017e\n\n<\n\nf y, m; x, \u242a, \u243e s f y m, x, \u243e f m; x, \u242a ,\n\u017e\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n<\n\n.\n\nwhich conditions the distribution of y on the missing data pattern. the two\nspecifications are equivalent when m is independent of y, with \u2424 s \u243e and\n\u243a s \u242a. for discussion of advantages of each modeling approach and details\n\u017e\non ways of modeling missingness, see little 1998 and references in note\n11.9. see stokes et al.\nfor an example of building\nthe missingness pattern into a model to check whether it is associated with\nthe response or interacts with effects of explanatory variables.\n\n\u017e\n.\n2000, p. 524\n\nanalyses in the presence of much missingness should be made with\ncaution. typically, little is known about the missing data mechanism, and\nassumptions about it cannot be checked. since inferences may not be robust,\na sensitivity study is necessary to check how results depend on specification\nof that mechanism. in the absence of a model for the missingness, one should\nat least compare results of the analysis using all available cases for all clusters\nto the analysis using only clusters having no missing observations. if results\ndiffer substantially, conclusions should be very tentative until the reasons for\nmissingness can be studied.\n\nt\n\n11.5 markov chains: transitional modeling\nwhen y denotes the response at time t, t s 0, 1, 2, . . . , the indexed family of\nrandom variables y , y , y , . . .\nis a stochastic process. the state space of the\nprocess is the set of possible values for y . the value y is the initial state.\nwhen the state space is categorical and observations occur at a discrete set of\ntimes, y has discrete state space and discrete time.\n\n.\n\n\u017e\n\n\u0004\n\n4\n\n0\n\n1\n\n2\n\n0\n\nt\n\nt\n\n11.5.1 transitional models\n\nthe main focus is usually on the dependence of y on the responses\n\u0004\nobserved previously as well as any explanatory variables.\ny , y , . . . , y\n0\nmodels of this type are called transitional models. let f y , . . . , y\ndenote\nignoring, for now, ex-\nthe joint probability mass function of y , . . . , y\nt\n\n. \u017e\n\nty1\n\n\u017e\n\n\u017e\n\n.\n\n4\n\nt\n\n1\n\n0\n\n0\n\nt\n\n "}, {"Page_number": 492, "text": "markov chains: transitional modeling\n\n477\n\nplanatory variables . transitional models use the factorization\n\n.\n\nf y , . . . , y s f y\n\u017e\n\u017e\n\n.\n\nt\n\n0\n\n.\n\n\u017e\nf y\n\n1\n\n0\n\n<\n\ny\n\n0\n\n.\n\n\u017e\nf y\n\n2\n\n<\n\ny , y\n0\n\n1\n\n.\n\n\u017e\n\u2b48\u2b48\u2b48 f y\n\n<\n\nt\n\ny , y , . . . , y\n0\n\n1\n\nty1\n\n.\n\n.\n\nunlike the marginal models in the other sections of this chapter,\nmodeling is conditional on previous responses.\n\nthis\nin this section we introduce discrete-time marko\u00ae chains, a simple stochas-\ntic process having discrete state space. many transitional models have markov\nchain structure for at least part of the model.\n\n11.5.2 first-order markov chains\na marko\u00ae chain is a stochastic process for which, for all t, the conditional\ndistribution of y , given y , . . . , y , is identical to the conditional distribu-\ntion of y\nis conditionally indepen-\ndent of y , . . . , y . knowing the present state of a markov chain, informa-\ntion about past states does not help us predict the future. for markov chains,\n\ngiven y alone. that is, given y , y\n\ntq1\n0\n\ntq1\n\nty1\n\ntq1\n\n0\n\nt\n\nt\n\nt\n\n<\n\n<\n\n<\n\nt\n\nt\n\n1\n\n2\n\n0\n\n1\n\n0\n\n0\n\n.\n\nt\n\nt\n\n\u017e\n\ny\n\ny\n\ny\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n.\n\ntq1\n\nty1\n\n\u017e\nf y\n\n\u017e\nf y\n\ntq1\n\u017e\n\n\u017e\n. . . f y\n\nf y , . . . , y s f y\n\u017e\n\u017e\n\n.\n11.13\na stochastic process is a kth-order marko\u00ae chain if, for all t, the condi-\ntional distribution of y , given y , . . . , y , is identical to the conditional\n.\n. given the states at the previous k\ndistribution of y , given y , . . . , y\ntimes, the future behavior of the chain is independent of past behavior before\nthose k times. our discussion here focuses mainly on ordinary markov chains\nas in 11.13 , which are first order k s 1 .\n.\ndenote the conditional probability p y s j y s i by \u2432 t . the\n, which satisfy \u00fd \u2432 t s 1, are called transition probabilities. the\n\u017e .4\n\u0004\n\u2432 t\nj s 1, . . . , i\n\u017e .\ni = i matrix \u2432 t ,\nis a transition probability\nmatrix. it is called one-step, to distinguish it from the matrix of probabilities\nfor k-step transitions from time t y k to time t.\n\ni s 1, . . . , i,\n\n0\ntykq1\n\n\u017e .\n\n\u017e .\n\nty1\n\nfrom 11.13 , the joint distribution for a markov chain depends only on\none-step transition probabilities and the marginal distribution for the initial\nstate. it also follows that the joint distribution satisfies loglinear model\n\nj < i\n\nj < i\n\nj < i\n\nj < i\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n4\n\n\u0004\n\nt\n\nj\n\n<\n\n\u017e\n\ny y , y y , . . . , y\n\n1 2\n\n0\n\n1\n\n.\ny .\n\nty1 t\n\nfor a sample of realizations of a stochastic process, a contingency table\ndisplays counts of the possible sequences. a test of fit of this loglinear model\nchecks whether the process plausibly satisfies the markov property.\n\nstatistical inference for markov chains uses standard methods of categori-\ncal data analysis. for example, consider ml estimation of transition probabil-\nities. let n t denote the number of transitions from state i at time t y 1 to\nstate j at time t. for fixed t, n t\nform the two-way marginal table for\ndimensions t y 1 and t of an i\n\u017e .\nt subjects\n\n\u017e .4\ncontingency table. for the n\n\n\u0004\ni j\ntq1\n\n\u017e .\n\ni j\n\niq\n\n "}, {"Page_number": 493, "text": "478\nanalyzing repeated categorical response data\nin category i at time t y 1, suppose that n t , j s 1, . . . , i have a multino-\nmial distribution with parameters \u2432 t\ndenote the initial counts.\nsuppose that they also have a multinomial distribution, with parameters\n\u0004\n\u2432 . if subjects behave independently, from 11.13 the likelihood function is\nproportional to\n\n\u0004\n\u017e .4\n. let n\n\n\u017e .\n\u0004\n\nj < i\n\ni0\n\ni0\n\n\u017e\n\n.\n\n4\n\n\u0004\n\n4\n\n4\n\ni j\n\n\u017e\n\ni\n\n\u0142\nis1\n\nn\ni0\u2432\ni0\n\n/\n\n\u00bd\n\ni\n\nt\n\ni\n\n\u0142 \u0142 \u0142\nts1 is1\njs1\n\n\u017e .\n\u2432 t\n\nj < i\n\n\u017e .\nt\n\nn\n\ni j\n\n5\n\n.\n\n\u017e\n\n11.14\n\n.\n\nthe transition probabilities are parameters of it independent multinomial\ndistributions. from anderson and goodman 1957 , the ml estimates are\n\n\u017e\n\n.\n\n\u2432 t s n\n\u02c6j < i\n\n\u017e .\n\ni j\n\nt rn\n\u017e .\n\niq\n\n\u017e .\nt\n\n.\n\n\u017e\n\ndenote the binary response wheeze, no wheeze by y at age t,\n\n11.5.3 respiratory illness example\ntable 11.7 refers to a longitudinal study at harvard of effects of air pollution\non respiratory illness in children. the children were examined annually at\nages 9 through 12 and classified according to the presence or absence of\nwheeze.\nt s\n.\n9, 10, 11, 12. the loglinear model y y , y y , y y\nrepresents a first-\norder markov chain. it fits poorly, with g s 122.9 df s 8 . given the state\n.\nat time t, classification at time t q 1 depends on states at times previous to\ntime t. the model y y y , y y y\nrepresents a second-order markov\nchain, satisfying conditional independence at ages 9 and 12, given states at\nages 10 and 11. this model also fits poorly, with g s 23.9 df s 4 . the\npoor fits may partly reflect subject heterogeneity, since these analyses ignore\npossibly relevant covariates such as parental smoking behavior.\nthe loglinear model y y , y y , y y , y y , y y , y y\nthat per-\n11\nmits association at each pair of ages fits well, with g s 1.5 df s 5 . table\n\u017e\n.\n\n11 12\n\u017e\n\n11 12\n\n10\n2\n\n10\n\n10\n\n10\n\n11\n\n10\n\n10\n\n11\n\n11\n\n12\n\n12\n\n11\n\n10\n\n12\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n9\n\n2\n\n9\n\n2\n\n9\n\n9\n\n9\n\nt\n\ntable 11.7 results of breath test at four agesa\n\ny\n9\n1\n1\n1\n1\n1\n1\n1\n1\n\ny\n10\n1\n1\n1\n1\n2\n2\n2\n2\n\ny\n11\n1\n1\n2\n2\n1\n1\n2\n2\n\ny\n12\n1\n2\n1\n2\n1\n2\n1\n2\n\na 1, wheeze; 2, no wheeze.\n.\nsource: ware et al. 1988 .\n\n\u017e\n\ncount\n\n94\n30\n15\n28\n14\n9\n12\n63\n\ny\n9\n2\n2\n2\n2\n2\n2\n2\n2\n\ny\n10\n1\n1\n1\n1\n2\n2\n2\n2\n\ny\n11\n1\n1\n2\n2\n1\n1\n2\n2\n\ny\n12\n1\n2\n1\n2\n1\n2\n1\n2\n\ncount\n19\n15\n10\n44\n17\n42\n35\n572\n\n "}, {"Page_number": 494, "text": "markov chains: transitional modeling\n\n479\n\ntable 11.8 estimated conditional log odds\nratios for table 11.7\n\nassociation\n\nestimate\n\ny y\n9 10\ny y\n10 11\ny y\n11 12\ny y\n9 11\ny y\n9 12\ny y\n10 12\n\n1.81\n1.65\n1.85\n0.95\n1.05\n1.07\n\nsimpler\nstructure\n\n1.75\n1.75\n1.75\n1.04\n1.04\n1.04\n\n11.8 shows its ml estimates of pairwise conditional log odds ratios. the\nassociation seems similar for pairs of ages 1 year apart, and somewhat weaker\nfor pairs of ages more than 1 year apart. the simpler model in which\n\n\u242dy9y10 s \u242dy10 y11 s \u242dy11y12\ni j\n\ni j\n\ni j\n\nand \u242dy9y11 s \u242dy9y12 s \u242dy10 y12\n\ni j\n\ni j\n\ni j\n\nfits well, with g s 2.3 df s 9 . the estimated log odds ratios are 1.75 in the\nfirst case, and 1.04 in the second.\n\n\u017e\n\n.\n\n2\n\n11.5.4 transitional models with explanatory variables\ntransitional models usually also include explanatory variables x. the joint\nmass function of t sequential responses is then\n\n1\n\n\u017e\n.\nf y , . . . , y ; x\n.\n\ns f y ; x f y\n\u017e\n\n\u017e\n\nt\n\n1\n\n<\n\n\u017e\ny ; x f y\n1\n\n.\n\n3\n\n<\n\n2\n\n\u017e\ny , y ; x \u2b48\u2b48\u2b48 f y\n1\n\n.\n\n2\n\n<\n\nt\n\ny , y , . . . , y\n1\n\n2\n\nty1\n\n.\n; x .\n\nwith binary y, for instance, one might specify a logistic regression model for\neach term in this factorization,\n\n\u017e\nf y\n\nt\n\n<\n\ny , . . . , y\n1\n\n; x\n\nt\n\n.\n\nty1\n\u017e\n\ns\n\nexp y \u2423q \u2424 y q \u2b48\u2b48\u2b48 q\u2424 y q \u2424 x\n.\n1 q exp \u2423q \u2424 y q \u2b48\u2b48\u2b48 q\u2424 y q \u2424 x\n\nty1\nty1\n\nty1\nty1\n\n\u017e\n\n1\n\n1\n\n1\n\n1\n\nt\n\nt\n\nx\n\nx\n\nt\n\ny s 0,1.\n\nt\n\n,\n\n.\n\nhere, the predictor x may take different value for each component. the\nmodel treats previous responses as explanatory variables. it is called a\nregressi\u00aee logistic model bonney 1987 .\n.\n\nthe interpretation and magnitude of \u2424 depends on how many previous\nobservations are in the model. within-cluster effects may diminish markedly\n\n\u02c6\n\n\u017e\n\n "}, {"Page_number": 495, "text": "480\n\nanalyzing repeated categorical response data\n\n\u0004\n\nby conditioning on previous responses. this is an important difference from\nmarginal models, for which the interpretation does not depend on the\nspecification of the dependence structure. in the special case of first-order\nmarkov structure, the coefficients of y , . . . , y\nequal 0 in the model for y\nt\n\u017e\ne.g., azzalini 1994; bonney 1987 . it may help to allow interaction between\nx and y\nt\n\n.\nin their effects on y .\nt\n\nfor a given subject, the product of the conditional mass functions deter-\nmines that subject\u2019s contribution to the likelihood function. one usually\nignores the contribution of the marginal distribution for the first term. that\nis, given the predictor, the model treats repeated transitions by a subject as\nindependent. thus, one can fit the model with ordinary glm software,\n.\ntreating each transition as a separate observation bonney 1986 .\n\nty1\n\nty2\n\n\u017e\n\n.\n\n\u017e\n\n4\n\n1\n\n11.5.5 child\u2019s respiratory illness and maternal smoking\ntable 11.9 is also from the harvard study of air pollution and health. at ages\n7 through 10, children were evaluated annually on the presence of respiratory\nillness. a predictor is maternal smoking at the start of the study, where s s 1\nfor smoking regularly and s s 0 otherwise. let y denote the response at age\n\u017e\nt\n\nt s 7, 8, 9, 10 . we consider the regressive logistic model\n\n.\n\nt\n\nlogit p y s 1 s \u2423q \u2424 s q \u2424 t q \u2424 y\n\n.\n\n\u017e\n\n1\n\n2\n\n3 ty1\n\nt\n\nt s 8, 9, 10.\n\n,\n\neach subject contributes three observations to the model fitting. the data\n.\nset consists of 12 binomials, for the 2 = 3 = 2 combinations of\n.\nty1\nfor instance, for the combination 0, 8, 0 , y s 0 for 237 q 10 q 15 q 4 s\n\ns, t, y\n\n\u017e\n\n.\n\n\u017e\n\n8\n\ntable 11.9 child\u2019s respiratory illness by age and maternal smoking\n\nchild\u2019s respiratory illness\n\nno maternal\n\nsmoking\nage 10\n\nmaternal\nsmoking\nage 10\n\nage 7\nno\n\nyes\n\nage 8\nno\n\nyes\n\nno\n\nyes\n\nage 9\nno\nyes\nno\nyes\nno\nyes\nno\nyes\n\nno\n237\n15\n16\n7\n24\n3\n6\n5\n\nyes\n10\n4\n2\n3\n3\n2\n2\n11\n\nno\n118\n8\n11\n6\n7\n3\n4\n4\n\nyes\n6\n2\n1\n4\n3\n1\n2\n7\n\nsource: data courtesy of james ware.\n\n "}, {"Page_number": 496, "text": "notes\n266 subjects and y s 1 for 16 q 2 q 7 q 3 s 28 subjects. the ml fit is\n\n8\n\n481\n\nlogit p y s 1 s y0.293 q 0.296 s y 0.243t q 2.211 y\n\n\u02c6\n\u017e\n\n.\n\nt\n\nty1\n\n,\n\n\u017e\n\nwith se values 0.846, 0.156, 0.095, 0.158 . not surprisingly, the previous ob-\nservation has a strong effect. given that and the child\u2019s age, there is slight\nevidence of a positive effect of maternal smoking: the likelihood-ratio\nstatistic for h : \u2424 s 0 is 3.55 df s 1, p s 0.06 . the model itself does not\nshow any evidence of lack of fit g s 3.1, df s 8 .\n.\n\n\u017e\n\n\u017e\n\n.\n\n0\n\n1\n\n2\n\n.\n\nnotes\n\nsection 11.1: comparing marginal distributions: multiple responses\n\n\u017e\n\n.\n\n11.1. darroch 1981 surveyed thoroughly the relationships among statistics for testing\nmarginal homogeneity and their connections with generalized cmh analyses. see also\nmantel and byar 1978 and white et al. 1982 . croon et al. 2000 studied a variety\nof hypotheses for longitudinal data in the context of the generalized loglinear model.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nsection 11.2: marginal modeling: maximum likelihood approach\n\n.\n11.2. for other work on ml fitting of marginal models, see bergsma and rudas 2002 ,\n\n\u017e\n\n.\nekholm et al. 2000 , fitzmaurice et al. 1993 , and lang et al. 1999 .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nsection 11.3: marginal modeling: generalized estimating equations approach\n\n\u017e\n\n\u017e\n\n.\n\n11.3. liang et al. 1992 discussed gee methods for categorical primarily binary\n\nre-\nsponses. for multinomial responses, see heagerty and zeger 1996 , lipsitz et al.\n\u017e\n1994 , miller et al. 1993 , and references in agresti and natarajan 2001 . more\ngeneral models with ordinal responses allow for dispersion parameters that also\n.\ndepend on covariates toledano and gatsonis 1996 .\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n11.4. lavange et al. 2001 used gee methods to adjust for clustered sampling in surveys\nand clinical trials. boos 1992 discussed generalized score tests that incorporate\nempirical variance estimates, illustrating with tests for trend and lack of fit in binary\nregression.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n11.5. koch et al. 1977 used weighted least squares wls to fit marginal models to table\n11.2. wls for categorical modeling is described in section 15.1. it has severe\n.\nlimitations e.g., covariates must be categorical and marginal tables cannot be sparse\nbut led naturally to the gee approach.\n\n\u017e\n\n\u017e\n\n.\n\nsection 11.4: quasi-likelihood and its gee multi\u00a9ariate extension: details\n\n\u017e\n\n.\n\n.\n\n11.6. firth 1993b provided a useful overview of quasi-likelihood methods. mccullagh\n\u017e\n1983 showed that under correct specification of the mean and the variance function,\nquasi-likelihood estimators are asymptotically efficient among estimators that are\nlocally linear in y . his result generalizes the gauss\u1390markov theorem, although in an\nasymptotic rather than exact manner. see also heyde 1997 and liang and zeger\n\u017e\n1995 for discussions of unbiased estimating functions and their connections with\n\n.\n\n\u0004\n\n4\n\n\u017e\n\n.\n\ni\n\n "}, {"Page_number": 497, "text": "482\n\nanalyzing repeated categorical response data\n\nasymptotic consistency and efficiency. godambe showed in 1960 that ml estimators\nare optimal solutions with an unbiased estimating function. when quasi-likelihood\nestimators are not ml, cox 1983 and firth 1987 suggested that they still retain\ngood efficiency when the departure from the natural exponential family is at most\nmoderate, such as modest overdispersion relative to such a family.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n11.7. the generalized estimating equations are likelihood equations, and hence the gee\nestimates are also ml, in certain cases. examples are multivariate normal data or\nbinary data when the working covariance is correct fitzmaurice et al. 1993 . results\nabout effects of model misspecification arise in a variety of model-building contexts.\nfor general theory, see gourieroux et al. 1984 , hansen 1982 , liang and zeger\n.\n\u017e\n1995 , and white 1982 .\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n11.8. a gee2 analysis adds estimating equations for the correlation structure prentice and\nzhao 1991 . this has the potential to increase efficiency. a disadvantage is that,\nunlike with ordinary gee, \u2424 is no longer consistent if this part of the model is\nmisspecified. qu et al. 2000 showed how to increase efficiency by representing the\nworking correlation matrix by a linear combination of basis matrices.\n\n\u02c6\n.\n\n\u017e\n\n\u017e\n\n.\n\n11.9. for surveys of ways to handle missing data, see little 1998 , little and rubin 1987,\nchap. 9 , schafer 1997 , and verbeke and molenberghs 2000 . see also baker and\nlaird 1988 , fay 1986 , fitzmaurice et al. 1994 , forster and smith 1998 , fuchs\n\u017e\n1982 , molenberghs and goetghebeur 1997 , molenberghs et al. 1997 , park and\n.\nbrown 1994 , and stokes et al. 2000 .\n\n\u017e\n\u017e\n\n.\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nsection 11.5: marko\u00a9 chains: transitional modeling\n\n11.10. for statistical inference with markov chains, see andersen 1980, sec. 7.7 , anderson\nand goodman 1957 , billingsley 1961 , bishop et al. 1975, chap. 7 , and kalbfleisch\n.\nand lawless 1985 . see conaway 1989 , stiratelli et al. 1984 , and ware et al. 1988\nfor other analyses focusing on the conditional dependence structure.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nproblems\n\napplications\n\n11.1 refer to table 8.3. viewing the table as matched triplets, construct\nthe marginal distribution for each substance. find the sample propor-\ntions of students who used marijuana, alcohol, and cigarettes. test\nthe hypothesis of marginal homogeneity. interpret results.\n\n11.2 refer to table 9.1. fit a marginal model to describe main effects of\nrace, gender, and substance type marijuana, alcohol, cigarettes on\nwhether a subject had used that substance. summarize effects.\n\n\u017e\n\n.\n\n11.3 refer to problem 11.2. further study shows evidence of an interac-\ntion between gender and substance type. using gee with exchange-\nable working correlation, the model fit for the probability \u2432 of using\n\n "}, {"Page_number": 498, "text": "problems\n\na particular substance is\n\n483\n\nlogit \u2432 s y0.57 q 1.93s q 0.86s q 0.38 r\n\n\u017e\n\n.\u02c6\n\n1\n\n2\n\ny 0.20g q 0.37g = s q 0.22g = s ,\n.\n\nwhere r, g, s , s are dummy variables for race 1 s white , gender\n1 s female , and substance type s s 1, s s 0 for alcohol; s s\n\u017e\n0, s s 1 for cigarettes; s s s s 0 for marijuana . show that:\na. the estimated odds a nonwhite male has used marijuana are\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\nexp y0.57 s 0.57.\n\n.\n\n\u017e\n\nb. given gender, the estimated odds a white subject used a given\n\nsubstance are 1.46 times the estimated odds for a black subject.\n\nc. given race, the estimated odds a female has used alcohol are 1.19\ntimes the estimated odds for males; for cigarettes and for mari-\njuana, the estimated odds ratios are 1.02 and 0.82.\n\n.\n\n\u017e\n\nd. given race,\nthe estimated odds a female has used alcohol\n\u017e\n.\ncigarettes are 9.97 2.94 times the estimated odds she has used\nmarijuana.\n\n.\ne. given race, the estimated odds a male has used alcohol cigarettes\nare 6.89 2.36 times the estimated odds he has used marijuana.\ninterpret the interaction.\n\n\u017e\n\n.\n\n\u017e\n\n11.4 refer to table 11.2. analyze the data using the scores 1, 2, 4 for the\nweek number, using ml or gee. interpret estimates and compare\n.\nsubstantive results to those in the text with scores 0, 1, 2 .\n\n\u017e\n\n\u017e\n\n.\n\n11.5 analyze table 11.9 using a marginal\n\nlogit model with age and\nmaternal smoking as predictors. compare interpretations to the\nmarkov model of section 11.5.5.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n11.6 table 11.10 refers to a three-period crossover trial to compare placebo\n\u017e\ntreatment a with a low-dose analgesic treatment b and high-dose\nanalgesic treatment c for relief of primary dysmenorrhea. subjects\nin the study were divided randomly into six groups, the possible\nsequences for administering the treatments. at the end of each\nperiod, each subject rated the treatment as giving no relief 0 or\nsome relief 1 . let y s 1 denote relief for subject i using treat-\nt s a, b, c , where subject i is nested in treatment sequence\nment t\nk k s 1, . . . , 6 . assuming common treatment effects for each se-\nquence, and setting \u2424 s 0, obtain and interpret \u2424 using ml or\n.gee for the model\n\n\u02c6\u0004\nt\n\n\u017e .\n\n\u017e .\n\n4 \u017e\n\ni\u017e k.t\n\n.\n\n.\n\n\u017e\n\n\u017e\n\na\n\nlogit p y s 1 s \u2423 q \u2424 .\n\n\u017e\n\n.\n\ni\u017e k.t\n\nk\n\nt\n\nhow would you order the drugs, taking significance into account?\n\n "}, {"Page_number": 499, "text": "484\n\nanalyzing repeated categorical response data\n\ntable 11.10 data for problem 11.6\n\n.\nresponse pattern for treatments a, b, c\n\ntreatment\nsequence\na b c\na c b\nb a c\nb c a\nc a b\nc b a\n\n000\n0\n2\n0\n0\n3\n1\n\n010\n2\n0\n1\n1\n0\n0\n\n011\n9\n9\n8\n8\n7\n4\n\n100\n0\n1\n1\n1\n0\n0\n\n001\n2\n0\n1\n1\n0\n5\n\u017e\n\n.\nsource: jones and kenward 1987 .\n\n\u017e\n101\n0\n0\n3\n0\n1\n3\n\n110\n1\n0\n0\n0\n2\n1\n\n111\n1\n4\n1\n1\n1\n0\n\n.\n\n11.7 table 11.11 is from a kansas state university survey of 262 pig\nfarmers. for the question \u2018\u2018what are your primary sources of veteri-\nnary information?,\u2019\u2019 the categories were a professional consultant,\n\u017e\nb veterinarian, c state or local extension service, d magazines,\nand e feed companies and reps. farmers sampled were asked to\n.\nselect all relevant categories. the 2 = 2 = 4 table shows the yes, no\ncounts for each of these five sources cross-classified with the farmers\u2019\neducation whether they had at least some college education and size\n.\nof farm number of pigs marketed annually, in thousands .\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n5\n\ntable 11.11 data for problem 11.7\n\nresponse on d\n\na s yes\n\na s no\n\nb s yes\n\nb s no\n\nb s yes\n\nb s no\n\neduc\nno\n\nc s yes c s no c s yes c s no c s yes c s no c s yes c s no\npigs e y n y n y n y n y n y n y n y n\n- 1 y 1\n3\n0\nn 0\n1\u13902 y 2\n4\n0\nn 0\n2\u13905 y 3\n1\nn 1\n0\n) 5 y 2\n2\n0\nn 1\nsome - 1 y 3\n11\nn 0\n0\n1\u13902 y 0\n6\n0\nn 0\n2\u13905 y 0\n3\n0\nn 1\n) 5 y 1\n2\n0\nn 1\n\n5\n1\n7\n7\n0\n0\n4\n3\n1\n0\n4\n1\n0\n1\n6\n0\n0\n2\n6 14\n0\n1\n7 14\n1\n1\n4\n4\n0\n0\n4\n2\n\n0\n1\n0\n0\n0\n3\n0\n6\n0\n0\n0\n1\n0\n0\n0\n10\n\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n\n2\n1\n4\n0\n3\n0\n1\n0\n4\n4\n2\n2\n1\n0\n0\n0\n\n1\n0\n0\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n1\n0\n\n2\n5\n4\n5\n1\n2\n0\n1\n1\n2\n1\n4\n0\n5\n1\n4\n\n1\n4\n1\n0\n2\n0\n0\n0\n0\n4\n0\n2\n0\n0\n0\n1\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\nsource: data courtesy of tom loughin, kansas state university.\n\n "}, {"Page_number": 500, "text": "problems\n\n485\n\na. explain why it is not proper to analyze the data by fitting a\nmultinomial model to the counts in the 2 = 4 = 5 contingency\ntable cross-classifying education by size of farm by the source of\nveterinary information, treating source as the response variable.\n\u017ethis table contains 453 positive responses of sources from the 262\nfarmers.\n\n.\n\nb. for a farmer with education i and size of farm s, let \u2432 is denote\nthe probability of responding \u2018\u2018yes\u2019\u2019 on the jth source. table 11.12\nshows output for using gee with exchangeable working correla-\ntion to estimate parameters in the model lacking an education\neffect,\n\nj\n\n\u017e\n\n.\n\nlogit \u2432 is s \u2423 q \u2424 s,\n\n\u017e\n\n.\n\nj\n\nj\n\nj\n\ns s 1, 2, 3, 4.\n\nexplain how to interpret the working correlation matrix. explain\nwhy the results suggest a strong positive size of farm effect for\nsource a and perhaps a weak negative size effect of similar\nmagnitude for c, d, and e.\n\nc. constraining \u2424 s \u2424 s \u2424 , the ml estimate of the common slope\nis y0.184 se s 0.063 . explain why it is advantageous to fit the\nmarginal model simultaneously for all sources rather than sepa-\nrately to each. agresti and liu 1999 and loughin and scherer\nx\n\u017e\n1998 discussed analyses for data of this form.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\nw\n\n3\n\n4\n\n5\n\ntable 11.12 output for problem 11.7\n\nrow1\nrow2\nrow3\nrow4\nrow5\n\ncol1\n\n1.0000\n0.0997\n0.0997\n0.0997\n0.0997\n\nparameter\nsource\nsource\nsource\nsource\nsource\nsize*source\nsize*source\nsize*source\nsize*source\nsize*source\n\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n\nworking correlation matrix\n\ncol2\n\n0.0997\n1.0000\n0.0997\n0.0997\n0.0997\n\ncol3\n\n0.0997\n0.0997\n1.0000\n0.0997\n0.0997\n\ncol4\n\n0.0997\n0.0997\n0.0997\n1.0000\n0.0997\n\nanalysis of gee parameter estimates\nempirical standard error estimates\n\nestimate\ny4.4994\ny0.8279\ny0.1526\n0.4875\ny0.0808\n1.0812\n0.0792\ny0.1894\ny0.2206\ny0.2387\n\nstd error\n\n0.6457\n0.2809\n0.2744\n0.2698\n0.2738\n0.1979\n0.1105\n0.1121\n0.1081\n0.1126\n\nz\n\ny6.97\ny2.95\ny0.56\n1.81\ny0.30\n5.46\n0.72\ny1.69\ny2.04\ny2.12\n\ncol5\n\n0.0997\n0.0997\n0.0997\n0.0997\n1.0000\n\n<\n\n<\npr> z\n<.0001\n0.0032\n0.5780\n0.0708\n0.7680\n<.0001\n0.4738\n0.0912\n0.0412\n0.0341\n\n "}, {"Page_number": 501, "text": "486\n\nanalyzing repeated categorical response data\n\ntable 11.13 output for problem 11.8\n\nworking correlation matrix\ncol1\ncol3\n\ncol2\n\n1.0000\n0.8173\n0.8173\n\n0.8173\n1.0000\n0.8173\n\n0.8173\n0.8173\n1.0000\n\nrow1\nrow2\nrow3\n\nanalysis of gee parameter estimates\nempirical standard error estimates\n\nestimate\ny0.1253\n0.1493\n0.0520\n0.0000\n0.0034\n\nstd error\n\n0.0676\n0.0297\n0.0270\n0.0000\n0.0878\n\nz\n\ny1.85\n5.02\n1.92\n\n.\n\n0.04\n\n<\n\n<\npr> z\n0.0637\n<.0001\n0.0544\n\n.\n\n0.9688\n\nparameter\nintercept\nquestion 1\nquestion 2\nquestion 3\nfemale\n\n.\n\n\u017e\n\nt\nw\n\n11.8 refer to table 11.13 on attitudes toward legalized abortion. for the\nresponse y 1 s support legalization, 0 s oppose for question t\nt s 1, 2, 3 and for gender g 1 s female, 0 s male , consider the\n\u017e\nmodel logit p y s 1 s \u2423q \u2425g q \u2424 with \u2424 s 0.\na. a gee analysis using unstructured working correlation gives cor-\nrelation estimates 0.826 for questions 1 and 2, 0.797 for 1 and 3,\nand 0.832 for 2 and 3. what does this suggest about a reasonable\nworking correlation structure?\n\n.x\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n3\n\nt\n\nt\n\nb. table 11.13 shows a gee analysis with exchangeable working\n\ncorrelation. interpret effects.\n\n\u017e\n\nc. treating the three responses for each subject as independent\nobservations and performing ordinary logistic regression, \u2424 s\n0.149 se s 0.066 , \u2424 s 0.052 se s 0.066 , and \u2425s 0.004 se\ns 0.054 . give a heuristic explanation of why within-subject stan-\ndard errors are much larger than with gee, yet the between-sub-\nject standard error is smaller.\n\n\u02c6\n1\n\u017e\n\n\u02c6\n2\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\n.\n\n11.9 refer to the air pollution data in table 11.7. using ml or gee, fit\nmarginal logit models that assume a marginal homogeneity, b a\nlinear effect of time, and c no pattern. interpret and compare.\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\n11.10 refer to the clinical trials data in table 12.5, analyzed with random\neffects models in section 12.3.4. use gee methods to analyze them,\ntreating each center as a correlated cluster.\n\n11.11 refer to table 10.5. using gee methods with cumulative logits,\ncompare the two marginal distributions. compare results to those\nusing ml in section 10.3.2.\n\n11.12 refer to the 34 table on government spending in table 8.19. analyze\nthese data with a marginal cumulative logit model. interpret effects.\n\n "}, {"Page_number": 502, "text": "problems\n\n11.13 refer to table 11.4.\n\n487\n\n\u0004\n\n.\n\na. to compare effects while controlling for initial response, fit model\n\u017e\n11.7 , using scores 10, 25, 45, 75 for time to falling asleep. also fit\nthe interaction model, and describe the lack of fit. note that for\nthe first two baseline levels, the active and placebo treatments\nhave similar sample response distributions at the follow-up; at\n.\nhigher baseline levels, the active treatment seems more successful.\n\n\u017e\n\n4\n\nb. fit the interaction model\n\nlogit p y f j s \u2423 q \u2424 x q \u2424 y q \u2424 xy\n\n\u017e\n\n.\n\nj\n\n1\n\n2\n\n1\n\n3\n\n1\n\n2\n\nthat constrains effects \u2424 x q \u2424 y q \u2424 xy\nto follow the pattern\n\u2436, \u2436, \u242dq \u2434, \u242d for the active group and \u2436, \u2436, \u2434, 0 for the placebo\n\u017e\n\u02c6\ngroup. interpret \u242d.\n\n3\n\u017e\n\n.\n\n.\n\n\u0004\n\n4\n\n1\n\n2\n\n1\n\n1\n\n11.14 find a marginal model with another type of logit that\n\nfits the\ninsomnia data of table 11.4 well. interpret parameter estimates, and\ncompare conclusions to those using cumulative logits.\n\n11.15 refer to table 11.9. combine the data for the two levels of maternal\nsmoking. does a first-order markov chain model these data ade-\nquately? find a loglinear model that does fit adequately.\n\n11.16 analyze table 11.9 using a transitional model with two previous\nresponses. does it fit better than the first-order model of section\n11.5.5? interpret.\n\n11.17 analyze table 11.2 using a first-order transitional model. compare\n\ninterpretations to those in this chapter using marginal models.\n\n11.18 table 11.14 is from a longitudinal study of coronary risk factors in\nschoolchildren woolson and clarke 1984 . a sample of children aged\n11\u139013 in 1977 were classified by gender and by relative weight obese,\nnot obese in 1977, 1979, and 1981. analyze these data.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\ntable 11.14 data for problem 11.18\n\nresponses\n\na\n\n7\n8\n\nonn\n\nnon\n\nnno\n\nnoo\n\nnnn\n119\n129\n\ngender\nmale\nfemale\nannn indicates not obese in 1977, 1979, and 1981; nno indicates not obese in 1977 and 1979\nbut obese in 1981; and so on.\nsource: reproduced with permission from the royal statistical society, london woolson and\n.\nclarke 1984 .\n\nooo\n\nono\n\noon\n\n13\n6\n\n11\n7\n\n16\n14\n\n8\n7\n\n3\n9\n\n4\n2\n\n\u017e\n\n "}, {"Page_number": 503, "text": "488\n\nanalyzing repeated categorical response data\n\n11.19 refer to the pig farmer survey of problem 11.7 table 11.11 . analyze\n\n\u017e\n\n.\n\nthese data using marginal models with all the variables.\n\n11.20 refer to the cereal diet and cholesterol study of problem 7.18 table\n\n\u017e\n\n7.23 . analyze these data with marginal models.\n\n.\n\ntheory and methods\n\n11.21 refer to problem 11.1. suppose that we expressed the data with a\n3 = 2 partial table of drug-by-response for each subject, to use a\ngeneralized cmh procedure to test marginal homogeneity. explain\nwhy the 911 q 279 subjects who make the same response for every\ndrug have no effect on the test.\n\n11.22 let y s 1 or 0 for observation t on subject\n\ni s 1, . . . , n,\n1, . . . , t. let y s \u00fd y rn, y s \u00fd y rt, and y s \u00fd \u00fd y rnt.\n\u0004\na. regard y\n\nas fixed. suppose that each way to allocate the y\n\ni,\n\n. .\n\nit\n\nit\n\nit\n\nit\n\ni.\n\nt\n\nt\n\ni\n\ni\n\nt s\n\n.t\n4\niq\n\ni.\n\ni.\n\nw\n\n.\n\n.\n\n\u017e\n\nit\n\u017e\n\ni.\n.\n\niq\n\u017e\n\niq\n\u2018\u2018successes\u2019\u2019 to y\nof the observations is equally likely. show that\ne y s y , var y s y 1 y y , and cov y , y s yy 1 y\n\u017e\n.\n.\nit\ny r t y 1 for t / k. hint: the covariance is the same for any\n.\nx\npair of cells in the same row, and var \u00fd y s 0 since y\nis fixed.\nb. refer to part a . for large n with independent subjects, explain\nwhy y , . . . , y\nis approximately multivariate normal with pair-\n.t\nwise correlation \u2433s y1r t y 1 . conclude that cochran\u2019s q\n.\nstatistic cochran 1950\n\n\u017e .\n.\n\niq\n\nik\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.1\n\nit\n\nit\n\ni.\n\ni.\n\nt\n\nq s\n\n2\n\n\u017e\n\nn t y 1 \u00fd\n.\nis1\n\nnt \u00fd y 1 y y\n\n.t\n\ny y y\n.\n\nt\nts1\n\u017e\n\n\u017e\n\ni.\n\ni.\n\n2\n\n.\n\n. .\n\nw\n\n1\n\n\u017e\n\n.\n\nis approximately chi-squared with df s t y 1 . one way notes\nthat if x , . . . , x is multivariate normal with common mean and\n.\ncommon variance \u2434 and common correlation \u2433 for pairs x , x ,\nthen \u00fd x y x r\u2434 1 y \u2433 is chi-squared with df s t y 1 . see\n.\nbhapkar and somes 1977 for slightly weaker conditions for a\n\u017e . x\nchi-squared limiting distribution for q than those in part a .\n\n.\n.\n\n\u017e\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\nt\n\n2\n\n2\n\n2\n\nk\n\nt\n\nt\n\nc. show that q is unaffected by deleting cases in which y s \u2b48\u2b48\u2b48 s\n\ni1\n\ny .\nit\n\ni\n\n11.23 consider the model \u242e s \u2424, i s 1, . . . , n, assuming that \u00ae \u242e s \u242e.\nsuppose that actually var y s \u242e . using the univariate version of\ngee described in section 11.4, show that u \u2424 s \u00fd y y \u2424 r\u2424 and\n\u2424s y. show that v in 11.10 equals \u2424rn, the actual asymptotic\n\u02c6\nvariance 11.11 simplifies to \u2424 rn, and its consistent estimate is\n\u00fd y y y rn .\n\n\u017e\n.\n2\n\n2\ni\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2\n\n2\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n "}, {"Page_number": 504, "text": "489\nproblems\n11.24 repeat problem 11.23 assuming that \u00ae \u242e s \u2434 when actually\n\n\u017e\n\n.\n\n2\n\ni\n\nvar y s \u242e.\n\n\u017e\n\n.\n\ni\n\ni\n\n11.25 consider the model \u242e s \u2424,\n\ni s 1, . . . , n, for independent poisson\nobservations. for \u2424s y, show that the model-based asymptotic vari-\nance estimate is yrn, whereas the robust estimate of the asymptotic\nvariance is \u00fd y y y rn . which would you expect to be better a if\n\u017e .\nthe poisson model holds, and b if there is severe overdispersion?\n\n\u017e .\n\n\u02c6\n\n\u017e\n\n.\n\n2\n\n2\n\ni\n\ni\n\ni\n\n11.26 show that 11.10 is equivalent to the formula for the large-sample\n\n.\ncovariance of the ml estimator in a glm, estimated by 4.28 .\n\n\u017e\n\n\u017e\n\n.\n\n11.27 a. for a univariate response, how is quasi-likelihood ql inference\n\n.\ndifferent from ml inference? when are they equivalent?\n\n\u017e\n\nb. explain the sense in which gee methodology is a multivariate\n\nversion of ql.\n\nc. summarize the advantages and disadvantages of the ql approach.\nd. describe conditions under which gee parameter estimators are\nconsistent and conditions under which they are not. for conditions\nin which they are consistent, explain why.\n\n11.28 formulate a model using adjacent-categories logits or continuation-\n\nratio logits that is analogous to 11.4 . interpret parameters.\n\n\u017e\n\n.\n\n11.29 refer to the analysis of mean time to falling asleep at the end of\nsection 11.2.3. explain how to calculate se for the difference be-\ntween the difference of means reported there. note that one differ-\nence uses paired samples and the other uses independent samples.\n\n\u017e\n\n.\n\n11.30 what is wrong with this statement?: \u2018\u2018for a first-order markov chain,\n\ny is independent of y\nt\n\nty2\n\n.\u2019\u2019\n\n11.31 suppose that loglinear model y , y , . . . , y\nt\n\n0\n\n1\n\n\u017e\n\nchain?\n\n.\n\nholds. is this a markov\n\n11.32 gamblers a and b have a total of i dollars. they play games of pool\nrepeatedly. each game they each bet $1, and the winner takes the\nother\u2019s dollar. the outcomes of the games are statistically indepen-\ndent, and a has probability \u2432 and b has probability 1 y \u2432 of winning\nany game. play stops when one player has all the money. let yt\ndenote a\u2019s monetary total after t games.\na. show that y is a first-order markov chain.\nb. state the transition probability matrix. for this gambler\u2019s ruin\nproblem, 0 and i are absorbing states. eventually, the chain enters\none of these and stays. the other states are transient.\n\n\u017e\n\n.\n\n\u0004\n\n4\n\nt\n\n "}, {"Page_number": 505, "text": "490\n\nanalyzing repeated categorical response data\n\n11.33 a first-order markov chain has\n\n.\nstationary or time-homogeneous\ntransition probabilities if the one-step transition probability matrices\nare identical, that is, if for all i and j,\n\n\u017e\n\n\u2432 1 s \u2432 2 s \u2b48\u2b48\u2b48 s \u2432 t s \u2432 .\n\n\u017e .\n\n\u017e .\n\n\u017e\n\n.\n\nj < i\n\nj < i\n\nj < i\n\nj < i\n\ni j\n\n\u0004\n\n\u017e .\n\nlet x, y, and z denote the classifications for the i = i = t table\nconsisting of n t , i s 1, . . . , i, j s 1, . . . , i, t s 1, . . . , t .\n4\na. explain why all transition probabilities are stationary if expected\nfrequencies for this table satisfy loglinear model xy, xz . thus,\nthe likelihood-ratio statistic for testing stationary transition proba-\n. x\nbilities equals g for testing fit of model xy, xz .\n\nb. let n s \u00fd n t . under the assumption of stationary transition\nprobabilities, show how the likelihood in 11.14 simplifies, and\nshow that the ml estimators are\n\n2\n\u017e .\n\n. w\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\ni j\n\ni j\n\nt\n\n\u2432 s n rn .\n\u02c6j < i\niq\n\ni j\n\nc. for a markov chain with stationary transition probabilities, let yi jk\ndenote the number of transitions from i to j to k over two\n4\n, argue that the goodness of fit of\nsuccessive steps. for\ni jk\n.\nloglinear model y y , y y\ntests that the chain is first order\n3\n1 2\nagainst the alternative that it is second order anderson and\n.\ngoodman 1957 .\n\ny\n\n\u017e\n\n\u017e\n\n\u0004\n\n2\n\n "}, {"Page_number": 506, "text": "c h a p t e r 1 2\n\nrandom effects: generalized\nlinear mixed models for\ncategorical responses\n\nin chapter 11 we noted that observations often occur in clusters. for\ninstance, cluster i might consist of repeated measurements on subject i or\nobservations for all subjects in family i. observations within a cluster tend to\nbe more alike than observations from different clusters. thus, they are\nusually positively correlated. ordinary analyses that ignore the correlation\nand treat within-cluster observations the same as between-cluster observa-\ntions produce invalid standard errors.\n\nin chapter 11 we focused on modeling the marginal distributions of\nclustered responses, treating the joint dependence structure as a nuisance. in\nthis chapter we present an alternative approach using cluster-level terms\nin the model. these terms take the same value for each observation in\na cluster but different values for different clusters. they are unobserved\nand, when treated as varying randomly among clusters, are called random\neffects. in section 10.2.4 we introduced this approach in a model for matched\npairs. the models have conditional interpretations, referred to as subject-\nspecific when each cluster is a subject. this contrasts with marginal models,\nwhich have population-a\u00aeeraged interpretations.\n\nrandom effects models for normal responses are well established. by\ncontrast, only recently have random effects been used much in models for\ncategorical data. in this chapter we extend generalized linear models to\ninclude random effects. in section 12.1 we introduce this extension, the\ngeneralized linear mixed model. in section 12.2 we discuss an important special\ncase for binary data, the logistic-normal model. several examples are shown in\nsection 12.3. section 12.4 covers extensions for multinomial responses, and\nsection 12.5 covers models with multivariate random effects. in section 12.6\nwe discuss model fitting, assuming normality for the random effects. parts of\n.\nthis chapter are from agresti et al. 2000 .\n\n\u017e\n\n491\n\n "}, {"Page_number": 507, "text": "492\n\nrandom effects: generalized linear mixed models\n\n12.1 random effects modeling of clustered\ncategorical data\n\nparameters that describe a factor\u2019s effects in ordinary linear models are\ncalled fixed effects. they apply to all categories of interest, such as genders,\nage groupings, or treatments. by contrast, random effects usually apply to a\nsample. for a study using a sample of clinics, for example, the model treats\nobservations from a given clinic as a cluster, and it has a random effect for\neach clinic.\n\nglms extend ordinary regression by allowing nonnormal responses and a\nlink function of the mean. the generalized linear mixed model glmm is a\nfurther extension that permits random effects as well as fixed effects in the\nlinear predictor.\n\n\u017e\n\n.\n\nit\n\nin cluster i,\n\n12.1.1 generalized linear mixed model\nt s 1, . . . , t . as in the gee\nlet y denote observation t\nanalyses in chapter 11, the number of observations may vary by cluster. in a\nlongitudinal study, even if clusters have equal size, many of them may have\nmissing observations. let x denote a column vector of values of explanatory\nvariables, for fixed effect model parameters \u2424. let u denote the vector of\nrandom effect values for cluster i. this is common to all observations in the\ncluster. let z denote a column vector of their explanatory variables. often,\nthe random effect is univariate.\nconditional on u , a glmm resembles an ordinary glm. let \u242e s\n\nit\n\nit\n\ni\n\ni\n\ne y u . the linear predictor for a glmm has the form\n\n\u017e\n\n<\n\nit\n\n.\n\ni\n\nit\n\ni\n\ng \u242e s xx \u2424 q zx u\n\u017e\n\n.\n\nit\n\nit\n\nit\n\ni\n\n\u017e\n\n12.1\n\n.\n\n\u017e .\n\n<\n\ni\n\nit\n\nit\n\nit\n\nit\n\nit\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nit\n.\n\n\u017e .\n\n.\ni\n.\n\nfor link function g \u2b48 . the random effect vector u is assumed to have a\nmultivariate normal distribution n 0, \u233a . the covariance matrix \u233a depends\non unknown \u00aeariance components and possibly also correlation parameters.\ndenote var y u s \u243e \u00ae \u242e , where the variance function \u00ae \u2b48 describes\nhow the conditional variance depends on the mean. as in section 4.4, often\n\u243e s 1 or \u243e s \u243er\u243b , where \u243b is a known weight e.g., number of trials\nfor a binomial count and \u243e is an unknown dispersion parameter. condi-\ntional on u , the model treats\nas independent over i and t. as discussed\nin section 10.2.2, the variability among u induces a nonnegative association\namong the responses, for the marginal distribution averaged over the sub-\njects. this is caused by the shared random effect u for each observation in a\ncluster.\n\u017e\n\nin 12.1 , the random effect enters the model on the same scale as the\npredictor terms. this is convenient but also natural for many applications.\nfor instance, random effects sometimes represent heterogeneity caused by\n\ny\n\n.\n\n\u017e\n\n\u0004\n\n4\n\nit\n\nit\n\ni\n\ni\n\ni\n\n "}, {"Page_number": 508, "text": "random effects modeling of clustered categorical data\n\n493\n\nomitting certain explanatory variables. consider the special case with univari-\nate random effect and z s 1. with u replaced by u *\u2434 where u * are\n\u017e\nn 0, 1 , the glmm has the form\n\n.\n\n\u0004\n\n4\n\nit\n\ni\n\ni\n\ni\n\ng \u242e s xx \u2424 q u *\u2434.\n\u017e\n\n.it\n\nit\n\ni\n\n\u0004\n\n4\n\ni\n\nthis has the form of an ordinary glm with unobserved values u * of a\nparticular covariate. thus, random effects models relate to methods of\ndealing with unmeasured predictors and other forms of missing data. the\nrandom effects part of the linear predictor reflects terms that would be in the\nfixed effects part if those explanatory variables had been included. random\neffects also sometimes represent random measurement error in the explana-\ntory variables. if we replace a particular predictor x by x * q \u2440, with x *\nit\nthe true value and \u2440 the measurement error, then \u2440 times the regression\nparameter can be absorbed in the random effects term. related to these\nmotivations, random effects also provide a mechanism for explaining overdis-\n.\npersion in basic models not having those effects breslow and clayton 1993 .\n\n\u017e\n\nit\n\nit\n\ni\n\ni\n\ni\n\n12.1.2 logit glmm for binary matched pairs\nwe illustrate the glmm expression 12.1 using a simple case, that of binary\nmatched pairs. the data form two dependent binomial samples section\n10.1 . cluster i consists of the responses\nfor matched pair i.\nobservation t in cluster i has y s 1 a success or 0 a failure , t s 1, 2.\n\ny , y\ni1\n.\n\n.\nin section 10.2.2 we introduced the model cox 1958b, rasch 1961\n\ni2\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nit\n\n\u017e\n\nlogit p y s 1 s \u2423 q \u2424x\n\n.\n12.2\nwhere x s 0 and x s 1. for it, \u2424 is a cluster-specific log odds ratio. that\nsection treated \u2423 as a fixed effect and eliminated it using conditional ml.\nan equivalent representation of 12.2 is\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nit\n\n1\n\n2\n\nt\n\ni\n\ni\n\nlogit p y s 1 u s \u2423q u ,\n\n<\n\n.\n\n\u017e\n\ni1\n\ni\n\ni\n\n\u017e\n\nlogit p y s 1 u s \u2423q \u2424q u ,\n12.3\n\n.\n\ni2\n\n\u017e\n\ni\n\ni\n\n<\n\n.\n\ni\n\ni\n\ni\n\ni\n\n4\n\n\u017e\n\n.\n\n\u017e\n\nwhere u s \u2423 y \u2423 for some constant \u2423. now, we treat u as a random effect\n\u0004\nindependent from a n 0, \u2434 distribution with \u2434\nfor cluster i, with u\nare independent.\nunknown. conditionally on u , we assume that y and y\nmodel 12.3 is the special case of 12.1 in which \u242e s p y s 1 u , g \u2b48\n\u017e\n\u017e .\nis the logit link, \u2424 s \u2423, \u2424 , x s 1, 0 and x s 1, 1 for all i, and z s 1\nfor all i and t. the univariate random effect adjusts the intercept but does\nnot modify the fixed effect. a glmm with random effect of this form is\ncalled a random intercept model. instead of the usual fixed intercept \u2423, it has\na random intercept \u2423q u .i\n\nx\ni2\n\n2.\n\nx\ni1\n\ni2\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\ni1\n\nit\n\nit\n\nit\n\nx\n\ni\n\ni\n\n<\n\n "}, {"Page_number": 509, "text": "i\n\ni\n\n2\n\n1\n\n2\n\n1\n\n2\n\n1\n\n\u0004\n\ni1\n\n\u017e\n\n\u017e\n\n\u017e\n\ni2\n\n2.\n\nw\nw\n\n.\n.\n\n\u017e\n\u017e\n\n.x4\n\n.x4\n\n\u0004\n\u017e\n\n494\n\nlet y s \u00fd y\n\nrandom effects: generalized linear mixed models\nand y s \u00fd y . marginally, y is binomial with n trials\nand parameter e exp \u2423q u r 1 q exp \u2423q u , and y is binomial with\nparameter e exp \u2423q \u2424q u r 1 q exp \u2423q \u2424q u . the expectations re-\nfer to u, a n 0, \u2434 random variable. the model implies a nonnegative\ncorrelation between y and y , with greater association resulting from\ngreater heterogeneity i.e., larger \u2434 . clusters with a large positive u have a\nrelatively large p y s 1 u\n<\nfor each t, whereas clusters with a large\nnegative u have a relatively small p y s 1 u for each. for this model, y\n\u017e\n1\nand y are independent only if \u2434s 0.\n\n\u017e\na 2 = 2 population-averaged table with success, failure for both the row\nand column categories summarizes the number of observations for which\ny , y s 1, 1 , 1, 0 , 0, 1 , or 0, 0 . let n\n\u0004\n\u017e\ndenote these counts. table\n12.1, analyzed first in section 10.1, is an example. let \u242e denote marginal\nfitted values for model 12.3 . we defer discussion of model fitting until\nsection 12.6. however, model 12.3 is a rare instance in which the fixed\neffect in a random effects model has a closed-form ml estimate,\n\n\u02c6 ab\n\n. \u017e\n\n. \u017e\n\nab\n\ni2\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\ni1\n\n\u0004\n\n4\n\n4\n\nit\n\nit\n\n2\n\ni\n\ni\n\ni\n\ni\n\n<\n\n\u02c6\u2424s log \u242e r\u242e .\n.\n\n\u02c6\n\n21\n\n12\n\n\u017e\n\u02c6\n\u017e\n\n\u0004\n\n4\n\n\u0004\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n12\n\n21\n\n12\n\n22\n\n11\n\nab\n\n\u02c6\n\n\u02c6\n\n21\n.\n\nwhen the sample log odds ratio log n n rn n g 0, then \u242e s n\n4\nab\nand \u2424s log n rn\n.\n. this is the same as the conditional ml estimate\n\u017e\nsection 10.2.3 . neuhaus et al. 1994 showed that this is true for any\n.\nparametric choice of random effects distribution for which the model 12.3\ncan generate n\nas fitted values. lindsay et al. 1991 showed that this\nestimate also results with a nonparametric approach discussed in section\n13.2.4. the model implies that the true log odds ratio for this 2 = 2 table\nis at least 0. when log n n rn n - 0, however, then \u2434s 0 and the\n\u017e\nfitted values \u242e s n\nn rn satisfy independence. then, \u2424 is identical\nto the estimate for the marginal model 10.6 by which \u2424 is the dif-\nference between logits for the two marginal distributions, namely \u2424s\nw\u017e\nlog n\n\nr n n\n.\n\u017e\n\n11\naq qb\n\n\u02c6\n\u02c6\n\n\u02c6 ab\n\n.x\n.\n\n\u02c6\n\nn\n\nab\n\n22\n\n12\n\n21\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n4\n\n\u0004\n\n2q q1\n\n1q q2\n\n12.1.3 ratings of prime minister revisited\n.\nfor table 12.1, the ml fit of model 12.3 , treating u as normal, yields\n\u2424s log 86r150 s y0.556 se s 0.135 , with \u2434s 5.16. this is identical to\n\u02c6\nw\u017e\n1r86 q\nthe conditional ml estimate\n1r150\n\u017e\n. for a given subject, the estimated odds of approval at the second\n\n10.10 , with standard error\n\n\u017e\n.x1r2\n\n.\n.\n\n\u02c6\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u0004\n\n4\n\ni\n\ntable 12.1 rating of performance of prime minister\n\nfirst\nsurvey\napprove\ndisapprove\n\ntotal\n\nsecond survey\n\napprove\n\ndisapprove\n\n794\n86\n880\n\n150\n570\n720\n\ntotal\n944\n656\n1600\n\n "}, {"Page_number": 510, "text": "495\nrandom effects modeling of clustered categorical data\nsurvey equal exp y0.556 s 0.57 times those at the first survey. the large \u2434\u02c6\nreflects the very strong association between the two responses, with sample\nodds ratio 35.1.\n\n.\n\n\u017e\n\n12.1.4 extension: rasch model and item response models\nan extension of the logit matched-pairs model 12.3 allows t ) 2 observa-\ntions in each cluster. the random intercept model then has form\n\n\u017e\n\n.\n\nlogit p y s 1 u s u q \u2424 ,\n\n<\n\n\u017e\n\n.\n\nit\n\nt\n\ni\n\ni\n\n\u017e\n\n12.4\n\n.\n\n\u0004\n\n4\n\n\u017e\n\n2.\n\ni\n\ni\n\ni\n\nt\n\nt\n\nit\n\n4\n\n\u0004\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nwhere u are independent n 0, \u2434 . equivalently, the model can add an\nintercept \u2423 or let e u s \u2423, but then identifiability requires a constraint\nsuch as \u2424 s 0.\n\nthat subject\n\nearly applications of this glmm were in psychometrics. the model\ndescribes responses to a battery of t questions on an exam. the probability\np y s 1 u\n\u017e\n<\ni makes the correct response on question t\ndepends on the overall ability of subject i, characterized by u , and the\neasiness of question t, characterized by \u2424. such models are called item-re-\n.\nsponse models. the logit form 12.4 is called the rasch model rasch 1961 .\nin estimating \u2424 , rasch treated u as fixed effects and used conditional\nml, as outlined in section 10.2.3 for matched pairs. later authors used the\nnormal random effects approach for this model and the model with probit\n.\nlink e.g., bock and aitkin 1981 .\n\n.\n\u0004\n\nthe \u2424 in the rasch model differ from parameters in corresponding\nmarginal models such as 11.1 , since the effects are subject specific. the\nrasch model refers to a t = 2 = n table of observation by outcome by\nsubject, whereas the marginal model refers to the t = 2 observation-by-\noutcome table of the t marginal distributions, collapsed over subjects. for\n.\nobservations s and t for a given subject i with model 12.4 ,\n\u2424 y \u2424 s logit p y s 1 u y logit p y s 1 u\n<\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n4\n\n\u0004\n\n4\n\n,\n\nt\n\nt\n\ni\n\ni\n\n<\n\n\u017e\n\n\u017e\n\ns\n\nt\n\n.\n\ni\n\n.\n\ni\n\ni s\n\nit\n\nwhich is a log odds ratio conditional on the subject. by contrast,\ncorresponding population-averaged effect in marginal model 11.1 is\n\n\u017e\n\u2424 y \u2424 s logit p y s 1 y logit p y s 1 ,\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ns\n\nt\n\nh s\n\nit\n\nthe\n\nwith subject h randomly selected for observation s and subject i randomly\nselected for observation t\n\n\u017e\n.\ni.e., h and i are independent observations .\n\n12.1.5 random effects versus conditional ml approaches\n\u0004\nsuppose that one treated u\nin model 12.4 as fixed effects instead of\nrandom effects. then, consider ordinary ml estimation of \u2424 and u . as n\nincreases, so does the number of parameters, since each subject has a u .i\n\n\u017e\n\n.\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\nt\n\ni\n\ni\n\n "}, {"Page_number": 511, "text": "496\n\nrandom effects: generalized linear mixed models\n\n\u0004\n\n4\n\nt\n\n4\n\n\u02c6\u0004\nt\n\neven though the number of \u2424 does not increase as n does, the ordinary\nml estimators \u2424 are not consistent. this happens in many models when\nthe number of parameters has an order similar to that of the number of\nsubjects. asymptotic optimality properties of ml estimators, such as consis-\ntency, require the number of parameters to be fixed as n increases. for\nmodel 12.4 , ml estimators of \u2424 have bias of order tr t y 1 andersen\n1980, pp. 244\u1390245 . for the matched-pairs model 12.2 , for instance, \u2424\u2122 2 \u2424\n.\nin probability problem 10.24 .\n\n. \u017e\n\n\u02c6\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u0004\n\n4\n\nt\n\ni\n\ni\n\ni\n\ni\n\nt\n\nt\n\nit\n\nit\n\n\u0004\n\n\u0004\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n4\n\n\u0004\n\n4\n\n4\n\n4\n\n.\n\n\u017e\n\n.\n\n\u017e\n\ny\n\nfor this reason, the preferable approach for the fixed effects model is\n\u0004\nby conditioning on their sufficient\nconditional ml. one eliminates u\nstatistics s s \u00fd y , i s 1, . . . , n . in the item response context, these are\nthe numbers of correct responses for each subject. conditional on s , the\ndistribution of\nis independent of u . maximizing the resulting likeli-\nhood then yields consistent estimators of \u2424 . the analysis generalizes the\none in section 10.2.3 for the subject-specific logistic model 10.8 for matched\npairs. see andersen 1980 for details.\n\n4\ni \u0004\n\ncompared with the random effects approach, the conditional ml ap-\nproach has certain advantages. one does not need to assume a parametric\ndistribution for u . it is difficult to check this assumption in the random\neffects approach. conditional ml is also appropriate with retrospective\nsampling. in that case, bias can occur with a random effects approach\n.\nbecause the clusters are not randomly sampled neuhaus and jewell 1990b .\nhowever, the conditional ml approach has severe disadvantages. it is\nrestricted to the canonical\nlink the logit , for which reduced sufficient\nstatistics exist for u . more important, as discussed in section 10.2.7, it is\nrestricted to inference about within-cluster fixed effects. the conditioning\nremoves the source of variability needed for estimating between-cluster\neffects in models with explanatory variables such as those considered next.\nalso, this approach does not provide information about u , such as predic-\ntions of their values and estimates of their variability or of the probabilities\nthey determine. finally, in more general models with covariates, conditional\nml can be less efficient than the random effects approach for estimating the\n.\nfixed effects see note 12.2 .\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ni\n\ni\n\n12.2 binary responses: logistic-normal model\n\nthe item response model 12.4 with random intercept is a special case of an\nimportant class of random effects models for binary data called logistic-\nnormal models. with univariate random effect, the model form is\n\n\u017e\n\n.\n\nlogit p y s 1 u s x \u2424 q u\n\n<\n\n\u017e\n\n.\n\nx\nit\n\nit\n\ni\n\ni\n\n\u017e\n\n12.5\n\n.\n\n\u0004\n\n4\n\nwhere u are independent n 0, \u2434 variates. this is the special case of the\nglmm 12.1 in which g \u2b48 is the logit link and the random effects structure\n\n\u017e .\n\ni\u017e\n\n.\n\n\u017e\n\n2.\n\n "}, {"Page_number": 512, "text": "binary responses: logistic-normal model\n\n497\n\n.\n\n\u017e\n\n\u017e\n\nsimplifies to a random intercept. the logistic-normal model has a long\nhistory, dating at least to cox 1970, prob. 20 in that text\nfor the matched-\n.\npairs model 12.3 and pierce and sands 1975 .\n\nmore generally, the link function in model 12.5 can be an arbitrary\n.\ninverse cdf. for such models, y and y are treated conditionally given u\ni\nas independent but are marginally nonnegatively correlated. let \u233d denote\nthe cdf that is the inverse link function. then, for s / t,\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\ni s\n\nit\n\ncov y , y s e cov y , y u q cov e y u , e y u\n<\n\n\u017e\n\n.\n\n<\n\n<\n\n\u017e\n\n\u017e\n\ni s\n\nit\n\ni s\n\nit\n\ni s\n\ni\n\n.\n\ni\n\nit\n\n.\n\ni\n\ns 0 q cov \u233d x \u2424 q u , \u233d x \u2424 q u\n\n\u017e\n\n.\n\n\u017e\n\nx\ni s\n\ni\n\n.\n\n\u017e\n\n12.6\n\n.\n\n\u017e\n\nx\nit\n\n.\n.\n\ni\n\nthe functions in the last covariance term are both monotone increasing in u ,i\nand hence are nonnegatively correlated. for common predictor value x at\neach t, the joint distribution for the model is exchangeable. this is often\nplausible for clustered data. in longitudinal studies, however, observations\ncloser together in time may tend to be more highly correlated.\n\nusually, the main focus in using a glmm is inference about the fixed\neffects. the random effects part of the model is a mechanism for represent-\ning how the positive correlation occurs between observations within a cluster.\nparameters pertaining to the random effects may themselves be of interest,\nhowever. for instance, the estimate \u2434 of the standard deviation of a random\nintercept may be a useful summary of the degree of heterogeneity of a\npopulation.\n\n\u02c6\n\n.\n\n\u017e\n\ninterpreting heterogeneity in logistic-normal models\n\n12.2.1\nwhen \u2434s 0, the logistic-normal model 12.5 simplifies to the ordinary\nlogistic regression model treating all observations as independent. when\n\u2434) 0, how can we interpret the variability in effects this model implies?\nconsider observation y at setting x of predictors and observation y\nh s\n\nsetting x . their log odds ratio is\n\u2424 q u y u .\nlogit p y s 1 u y logit p y s 1 u s x y x\n.\nwe cannot observe u y u , which has a n 0, 2 \u2434 distribution. however,\n\u017e\n100 1 y \u2423 % of those log odds ratios fall within\n'\n2 \u2434.\n\nx y x\n\n\u2424 \" z\n\n12.7\n\nat\n\n2.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nh s\n\nh s\n\nh s\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nit\n\nit\n\nit\n\nit\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nh\n\nh\n\nh\n\nx\n\nx\n\ni\n\ni\n\ni\n\n<\n\n<\n\nit\n\nh s\n\n\u2423r2\n\nit\n\n\u017e\n\nwhen \u2434s 0, x y x\n\n.\n\u2b18\u2424 is the usual form of log odds ratio for a model\nwithout random effects. when \u2434) 0, x y x\n\u2424 is the log odds ratio for\ntwo observations in the same cluster h s i or with the same random effect\nvalue. suppose that x s x\nfor observations from different clusters. then,\nit\nusing z s 0.674,\nthe middle 50% of the log odds ratios fall within\n\n.x\n\nh s\n\nh s\n\nh s\n\n\u017e\n\n\u017e\n\n.\n\nit\n\n0.25\n\n "}, {"Page_number": 513, "text": "'\n\nrandom effects: generalized linear mixed models\n498\n\"0.674 2 \u2434s \"0.95\u2434. hence, the median odds ratio between the observa-\ntion with higher random effect and the observation with lower random effect\nequals exp 0.95\u2434 . with a single predictor and x y x s 1, the median\nsuch odds ratio equals exp \u2424q 0.95\u2434 . larsen et al. 2000 presented\nrelated interpretations.\n\nh s\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nit\n\n\u017e\n\n12.2.2 connections between conditional models and marginal models\nthe fixed effects parameters \u2424 in glmms have conditional intepretations,\ngiven the random effect. those fixed effects are of two types. first, consider\nan explanatory variable that varies in value among observations in a cluster.\nfor instance, in a crossover study comparing t drugs, for each subject the\ndrug taken varies from observation to observation in that subject\u2019s cluster of\nt observations. for such an explanatory variable, its coefficient in the model\n.\nrefers to the effect on the response of a within-cluster e.g., subject-specific\n1-unit increase of that predictor. the random effect as well as other explana-\ntory variables in the model are constant while that predictor increases by 1.\nthe effect of that explanatory variable is a \u2018\u2018within-cluster\u2019\u2019 or \u2018\u2018within-sub-\nject\u2019\u2019 one.\n\nsecond, consider an explanatory variable with constant value among\nobservations in a cluster. an example is gender when each subject forms a\ncluster. for such an explanatory variable, its coefficient refers to the effect on\nthe response of a \u2018\u2018between-cluster\u2019\u2019 1-unit increase of that predictor. an\nexample is a comparison of females and males using a dummy variable and\nits coefficient. however, this fixed effect in the glmm applies only when the\n.\nrandom effect as well as other explanatory variables in the model\ntakes the\nsame value in both groups: for instance, a male and a female with the same\nvalue for their random effects.\n\nit is in this sense that random effects models are conditional models, as\nboth within- and between-cluster effects apply conditional on the random\neffect value. by contrast, effects in marginal models are averaged over all\nclusters i.e., population averaged , so those effects do not refer to a compari-\nson at a fixed value of a random effect. in fact, a fundamental difference\nbetween the two model types is that when the link function is nonlinear, such\nas the logit, the population-averaged effects of marginal models often are\nsmaller than the cluster-specific effects of glmms.\nthe glmm 12.1 refers to the conditional mean, \u242e s\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\nit\n\nspecifically,\nit\n\n.\n\ni\n\n<\n\n\u017e\n\ne y u . by inverting the link function,\n\ne y u s g\n\u017e\n\n.\n\n<\n\nit\n\ni\n\ny1\n\n\u017e\n\nx \u2424 q z u .\n.\n\nx\nit\n\nx\nit\n\ni\n\nmarginally, averaging over the random effects, the mean is\n\nh\ne y s e e y u s g\n\u017e\n\n\u017e\n\n.\n\n.\n\nit\n\nit\n\ni\n\n<\n\ny1\n\n\u017e\n\nx \u2424 q z u f u ; \u233a du ,\n\n.\n\n\u017e\n\n.\n\nx\nit\n\nx\nit\n\ni\n\ni\n\ni\n\n "}, {"Page_number": 514, "text": "binary responses: logistic-normal model\n\n499\n\n\u017e\n\n.\n\nwhere f u; \u233a is the n 0, \u233a density function for the random effects. for the\nidentity link,\n\n\u017e\n\n.\n\ne y s xx \u2424 q zx u f u ; \u233a du s xx \u2424 .\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\nit\n\nit\n\ni\n\nit\n\nit\n\ni\n\nh\n\nthe marginal model has the same model form and effects \u2424. this is not true\n.\nfor other links. for instance, for the logistic-normal model 12.5 ,\n\n\u017e\n\ne y s e\n\u017e\n\n.\n\nit\n\n\u017e\n\nexp x \u2424 q u\n\n.\n1 q exp x \u2424 q u\n\nx\nit\n\u017e\n\ni\n\nx\nit\n\n.\n\n.\n\ni\n\ni\n\nthis expectation does not have form exp x \u2424 r 1 q exp x \u2424 except when\nu has a degenerate distribution \u2434s 0 .\n.\n\n\u017e x\nit\n\n\u017e x\nit\n\napproximate relationships exist between estimates from the two model\ntypes. in the logistic-normal case with effect \u2424 and small \u2434, zeger et al.\n\u017e\n1988 showed that\n\n.\n\n\u017e\n\n.\n\n.x\n\nw\n\nw\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nx\nit\n\nx\nit\n\n12.8\n\nit\n2xy1r2\n\ne y f exp cx \u2424 r 1 q exp cx \u2424 ,\n\u017e\nwhere c s 1 q 0.6\u2434\n. since the effect in the marginal model multiplies\nthat of the conditional model by about c, it is typically smaller in absolute\nvalue. the discrepancy increases as \u2434 increases. for \u2424 near 0, neuhaus et\nal. 1991 showed that the marginal model effect is approximately \u2424 1 y \u2433 ,\n.\nwhere \u2433s corr y , y\nat \u2424 s 0. again, the discrepancy increases as \u2434\nincreases, since \u2433 increases with \u2434.\nfor table 12.1 on ratings of the prime minister, the ml estimate for\nmodel 12.3 is \u2424s y0.556, with \u2434s 5.16 for variability of u . approxima-\n\u02c6\ntion 12.8 suggests that \u2424s y0.556 with \u2434s 5.16 corresponds to a marginal\n\u02c6\n\u02c6\nw\n.2xy1r2 \u017e\n1 q 0.6 5.16\ny0.556 s y0.135. the actual\nestimate of about\nmarginal estimate is the log odds ratio for the sample marginal distributions,\nequaling\n\n\u02c6\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni s\n\n\u0004\n\n4\n\nit\n\ni\n\nlog 880r720 r 944r656 s y0.163.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n12\n\n21\n\n12\n\nin fact, the marginal effect is much smaller than the conditional effect, but\nthis approximation connecting the two estimates works better for smaller \u2434.\u02c6\nat \u2424s 0, the fit of the model is that of the symmetry model, for which\n\u242e s \u242e s n q n r2. the correlation for that 2 = 2 table equals 0.699,\n\u02c6\nfrom which the conditional estimate of y0.556 suggests a marginal estimate\nof y0.556 1 y 0.699 s y0.167, very close to the actual value of y0.163.\n\n\u02c6\n\nfigure 12.1 illustrates why the marginal effect is smaller than the condi-\ntional effect. for a single explanatory variable x, the figure shows subject-\nspecific curves for p y s 1 u for several subjects when considerable het-\nerogeneity exists. this corresponds to a relatively large \u2434 for random effects.\n\n21\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nit\n\ni\n\n<\n\n "}, {"Page_number": 515, "text": "500\n\nrandom effects: generalized linear mixed models\n\nfigure 12.1 logistic random-intercept model, showing the conditional\ncurves and the marginal population-averaged curve averaging over these.\n\n\u017e\n\n.\n\n\u017e\nsubject-specific\n\n.\n\n<\n\n<\n\ni\n\nit\n\n\u017e\n\n.\n\n\u017e\n\n.\nat any fixed value of x, variability occurs in the conditional means, e y uit\ns p y s 1 u . the average of these is the marginal mean, e y . these\naverages for various x values yield the superimposed curve. it has a shal-\nlower slope. in fact, it does not exactly follow the logistic formula. similar\nremarks apply to other glmms. for the probit link with binary data,\nhowever, the conditional probit model with normal random effect does imply\na marginal model of probit form problem 12.29 . with univariate random\nintercept, the marginal effect equals the conditional effect multiplied by\nw\n1 q \u2434\n\u017e\nzeger et al. 1988 . in section 13.5.1 we explore the condi-\ntional\u1390marginal connection for loglinear glmms.\n\n2xy1r2\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\nit\n\ni\n\n.\n\n\u017e\n\n12.2.3 comments about conditional versus marginal models\nrandom effects models describe conditional subject-specific effects, whereas\nmarginal models describe population-averaged effects. some statisticians\nprefer one of these types, but most feel that both are useful, depending on\nthe application.\n\nthe conditional modeling approach is preferable if one wants to specify a\nmechanism that could generate positive association among clustered observa-\ntions, estimate cluster-specific effects, estimate their variability, or model the\njoint distribution. latent variable constructions used to motivate model forms\n\u017ee.g., the tolerance motivation for binary models of section 6.6.1 and the\nrelated threshold motivation in problem 6.28 and utility motivation in prob-\nlem 6.29 usually apply more naturally at the cluster level than at the\nmarginal level. given a conditional model, one can recover information about\nmarginal distributions. that is, a conditional model implies a marginal model,\n\n.\n\n "}, {"Page_number": 516, "text": "binary responses: logistic-normal model\n\n501\n\n\u017e\n\nbut a marginal model does not itself imply a conditional model although see\n.\nnote 12.10 for an implicit connection .\n\nin many surveys or epidemiological studies, a goal is to compare the\nrelative frequency of occurrence of some outcome for different groups in a\npopulation. then, quantities of primary interest include between-group odds\nratios among marginal probabilities for the different groups. that is, effects\nof interest are between-cluster rather than within-cluster. when marginal\neffects are the main focus, it is usually simpler and may be preferable to\nmodel the margins directly. one can then parameterize the model so that\nregression parameters have a direct marginal interpretation. developing a\nmore detailed model of the joint distribution that generates those margins, as\na random effects model does, provides greater opportunity for misspecifica-\ntion. for instance, with longitudinal data the assumption that observations\nare independent, given the random effect, need not be realistic. with the\nmarginal model approach, we showed in chapter 11 that ml is sometimes\npossible but that the gee approach is computationally simpler and more\nversatile. a drawback of the gee approach is that it does not explicitly\nmodel random effects and therefore does not allow these effects to be\nestimated. in addition, likelihood-based inferences are not possible because\nthe joint distribution of the responses is not specified.\n\nin section 12.2.2 it was noted that conditional effects are usually larger\nthan marginal effects, and increase as variance components increase. usually,\nthough, the significance of an effect e.g., as measured by the ratio of\nestimate to standard error is similar in the two model types. if one effect\nseems more important than another in a conditional model, the same is\nusually true with a marginal model. so the choice of the model is usually not\ncrucial to inferential conclusions.\n\nthis statement requires a caveat, however, since sizes of effects in marginal\nmodels depend on the degree of heterogeneity in conditional models. in\ncomparing effects for two groups or two variables that have quite different\nvariance components, relative sizes of effects will differ for marginal and\nconditional models. from 12.8 , with binary data the attenuation from the\nconditional to the marginal effect will tend to be greater for the group having\nthe larger variance component. for instance, suppose that two groups, one\nyoung in age and the other elderly, both show the same conditional effect in\na crossover study comparing two drugs. if the elderly group has more\nheterogeneity on the response, their marginal effect may be smaller than that\nfor the younger group. the marginal effects differ even though the condi-\ntional effects are the same, because of the greater variance component for\nthe elderly. in such cases, the conditional effect appropriately modeled may\nhave more relevance.\n\nfinally, with either marginal or conditional models, missing data are a\ncommon problem with multivariate responses. unless data are missing at\nrandom, potential bias occurs in ml inference. gee methods usually require\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 517, "text": "502\n\nrandom effects: generalized linear mixed models\n\n\u017e\n\n.\n\nthe stronger condition that data are missing completely at random section\n11.4.5 . thus, modeling missingness or conducting a sensitivity study to\ndiscern its potential effects can be an important component of an analysis.\n\nregardless of the choice of paradigm, it is a challenge for statisticians\neven to explain to practitioners why marginal and conditional effects differ\nwith a nonlinear link function. graphics such as figure 12.1 can help.\nneuhaus 1992 and pendergast et al. 1996 surveyed ways of analyzing\nclustered binary data, including conditional and marginal models. agresti\nand natarajan 2001 surveyed conditional and marginal modeling of clus-\ntered ordinal data.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n12.3 examples of random effects models for\nbinary data\n\nin the next three sections we present a variety of examples of random effects\nmodels. in this section we consider binary responses.\n\n12.3.1 small-area estimation of binomial proportions\nsmall-area estimation refers to estimation of parameters for a large number of\ngeographical areas when each has relatively few observations. for instance,\none might want county-specific estimates of characteristics such as the\nunemployment rate or the proportion of families having health insurance\ncoverage. with a national or statewide survey, some counties may have few\nobservations. then, sample proportions in the counties may poorly estimate\nthe true countywide proportions. random effects models that treat each\ncounty as a cluster can provide improved estimates. in assuming that the true\nproportions vary according to some distribution, the fitting process \u2018\u2018borrows\nfrom the whole\u2019\u2019\u138fit uses data from all the counties to estimate the propor-\ntion in any given one.\nlet \u2432 denote the true proportion in area i, i s 1, . . . , n. these areas may\n4\ny denote independent\nbe all the ones of interest, or only a sample. let\ni\ny , where y , t s 1, . . . , t are inde-\nbin t , \u2432 variates; that is, y s \u00fd\nti\nts1\nit\npendent with p y s 1 s \u2432 and p y s 0 s 1 y \u2432. the sample propor-\n\u017e\ntions p s y rt are ml estimates of \u2432 for the fixed-effects model\n\n\u017e\n4\n\nit\n\u0004\n\n.\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n\u0004\n\n4\n\n4\n\n\u0004\n\nit\n\nit\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nlogit \u2432 s \u2423q \u2424,\n\n\u017e\n\n.i\n\ni\n\ni s 1, . . . , n.\n\nthis model is saturated, having n nonredundant parameters with a con-\nstraint such as \u00fd \u2424 s 0 for the n binomial observations.\n4\n\nfor small t , p have large standard errors. thus, p may display much\nmore variability than \u2432 , especially when \u2432 are similar. then, it is helpful\n\ni\n4 \u0004\n\n.\n\n4\n\n\u0004\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u017e\n\n "}, {"Page_number": 518, "text": "examples of random effects models for binary data\n\n503\n\n\u0004\n\n4\n\nto shrink p toward their overall mean. one can accomplish this with the\nrandom effects model\n\ni\n\nlogit p y s 1 u s \u2423q u ,\n\n<\n\n\u017e\n\n.\n\nit\n\ni\n\ni\n\n\u017e\n\n12.9\n\n.\n\n\u0004\n\n4\n\nwhere u are independent n 0, \u2434 variates. this model is a logit analog of\none-way random effects anova. when \u2434s 0, all \u2432 are identical.\n\ni\n\n\u017e\n\n2.\n\ni\n\nfor this model,\n\n\u2432 s exp \u2423q u r 1 q exp \u2423q u\n\u02c6\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\ni\n\ni\n\n.\n\ni\n\n.\n\ni\n\ni\n\ni\n\ni\n\nit\n\n4\n\n\u0004\n\nthis estimate differs from the sample proportion p . if \u2434s 0, then all\n\u02c6\nu s 0. then, the random effects estimate of each \u2432 is \u00fd \u00fd\ny r \u00fd t ,\n.\n\u02c6i\nti\nts1\ni\nthe overall sample proportion after pooling all n samples. when truly all \u2432\ni\nare equal, this is a much better estimator of that common value than the\nsample proportion from a single sample.\n\nn\nis1\n\ni\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u02c6\n\n\u02c6i\n\ni s 1, . . . , 51, where i s 51 is dc s district of columbia , y\n.\n\ngenerally, the random effects model estimators shrink the separate sam-\nple proportions toward the overall sample proportion. the amount of shrink-\nage decreases as \u2434 increases. the shrinkage also decreases as the t grow;\nas each sample has more data, we put more trust in the separate sample\nproportions. the predicted random effect u is the estimated mean of the\ndistribution of u , given the data see section 12.6.7 . this prediction de-\npends on all the data, not just data from area i. a benefit is potential\nreduction in the mean-squared error of the estimates around the true values.\nwe illustrate model 12.9 with a simulated sample of size 2000 to mimic a\npoll taken before the 1996 u.s. presidential election. for t observations in\nstate i\nis\nbin t , \u2432 , where \u2432 is the actual proportion of votes in state i for bill\ni\nclinton in the 1996 election, conditional on voting for clinton or the\nrepublican candidate, bob dole. here, t is proportional to the state\u2019s\npopulation size, subject to \u00fd t s 2000. table 12.2 shows t , \u2432 , and\np s y rt .\n4\n\u0004\nfor the ml fit of model 12.9 , \u2423s 0.163 and \u2434s 0.29. the predicted\nrandom effect values obtained using proc nlmixed in sas yield the\nproportion estimates \u2432 , also shown in table 12.2. since t are mostly\nsmall and since \u2434 is relatively small, considerable shrinkage of these esti-\nmates occurs from the sample proportions toward the overall proportion\nsupporting clinton, which was 0.548. the \u2432 vary only between 0.468 for\ntx s texas and 0.696 for ny s new york , whereas the sample propor-\ntions vary between 0.111 for idaho and 1.0 for dc . sample proportions\nbased on fewer observations, such as dc, tended to shrink more. although\nthe estimates incorporating random effects are relatively homogeneous, they\ntend to be closer than the sample proportions to the true values.\n\n\u017e\n\u0004\n\u02c6i\n\n4\n.\n\u017e\n\n\u02c6i\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n\u017e\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n4\n\n\u0004\n\n4\n\n\u0004\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n "}, {"Page_number": 519, "text": "504\n\nrandom effects: generalized linear mixed models\n\ntable 12.2 estimates of proportion of vote for clinton, conditional on voting\nfor clinton or dole in 1996 u.s. presidential election\n\na\n\n\u02c6\n\u2432\ni\n\np\ni\n\n\u2432\ni\n\nt\ni\n5\n32\n19\n34\n240\n29\n25\n4\n5\n108\n56\n9\n22\n9\n89\n44\n19\n29\n33\n46\n38\n9\n73\n35\n41\n21\n\nstate\nak\nal\nar\naz\nca\nco\nct\ndc\nde\nfl\nga\nhi\nia\nid\nil\nin\nks\nky\nla\nma\nmd\nme\nmi\nmn\nmo\nms\na\u2432 , true; p , sample; \u2432 , estimate using random effects model.\n\n0.394\n0.463\n0.594\n0.512\n0.572\n0.492\n0.604\n0.903\n0.586\n0.532\n0.494\n0.643\n0.557\n0.391\n0.596\n0.468\n0.400\n0.506\n0.566\n0.686\n0.586\n0.627\n0.573\n0.594\n0.535\n0.472\n\u02c6\n\n0.200\n0.500\n0.526\n0.618\n0.538\n0.586\n0.720\n1.000\n0.400\n0.602\n0.554\n0.556\n0.500\n0.111\n0.539\n0.432\n0.316\n0.448\n0.667\n0.739\n0.474\n0.778\n0.589\n0.571\n0.561\n0.333\n\n0.508\n0.524\n0.537\n0.573\n0.538\n0.558\n0.602\n0.576\n0.527\n0.583\n0.548\n0.543\n0.528\n0.472\n0.540\n0.488\n0.477\n0.506\n0.592\n0.637\n0.511\n0.578\n0.570\n0.554\n0.550\n0.477\n\nstate\nmt\nnc\nnd\nne\nnh\nnj\nnm\nnv\nny\noh\nok\nor\npa\nri\nsc\nsd\ntn\ntx\nut\nva\nvt\nwa\nwi\nwv\nwy\n\nt\ni\n7\n55\n5\n13\n9\n60\n13\n12\n137\n84\n23\n24\n90\n7\n28\n6\n40\n144\n15\n51\n4\n42\n39\n14\n4\n\ni\n\ni\n\ni\n\n\u2432\ni\n\n0.483\n0.475\n0.461\n0.395\n0.567\n0.600\n0.540\n0.506\n0.660\n0.536\n0.456\n0.547\n0.552\n0.689\n0.469\n0.479\n0.513\n0.473\n0.380\n0.489\n0.633\n0.572\n0.559\n0.584\n0.426\n\np\ni\n\n0.429\n0.455\n0.600\n0.462\n0.556\n0.667\n0.462\n0.500\n0.752\n0.488\n0.478\n0.625\n0.567\n0.571\n0.571\n0.667\n0.500\n0.444\n0.333\n0.412\n0.500\n0.619\n0.487\n0.571\n0.250\n\n\u02c6\n\u2432\ni\n\n0.526\n0.494\n0.546\n0.524\n0.543\n0.611\n0.524\n0.533\n0.696\n0.507\n0.520\n0.569\n0.558\n0.545\n0.552\n0.555\n0.522\n0.468\n0.490\n0.473\n0.538\n0.578\n0.517\n0.548\n0.518\n\n12.3.2 modeling repeated binary responses\nin section 12.1.4 we introduced a random effects version of the rasch model\nfor repeated binary measurement. this model extends to incorporate covari-\nates.\n\nwe illustrate using table 10.13, first analyzed in section 10.7.2. the\nsubjects indicated whether they supported legalizing abortion in each of\nthree situations. table 10.13 also classified the subjects by gender. let yit\ni on item t, with y s 1 representing\ndenote the response for subject\nsupport. consider the model\n\u017e\n\n.\n12.10\nwhere x s 1 for females and 0 for males, and where u are independent\n\u017e\nn 0, \u2434 . equivalently, one could place a constraint on \u2424 and allow an\n\nlogit p y s 1 u s u q \u2424 q \u2425x ,\n\ni\n2.\n\n4\n\u0004\n\n.\n\n\u017e\n\n\u017e\n\n\u0004\n\n4\n\nit\n\nit\n\nt\n\ni\n\ni\n\ni\n\ni\n\n<\n\nt\n\n "}, {"Page_number": 520, "text": "examples of random effects models for binary data\n\n505\n\n.\n\nt\n\n4\n\n\u0004\n\n4\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\nintercept \u2423. here, the gender effect \u2425 is assumed the same for each item,\nand the \u2424 refer to the items.\n\n\u02c6\u0004\nt\n\nsince model 12.10 implies nonnegative association among responses on\nthe items, one should use items and scales for which this should occur. for\nopinions about\nit would not be\nappropriate for one question to ask \u2018\u2018do you agree that abortion should be\nlegal when a woman is not married?\u2019\u2019 and another to ask \u2018\u2018do you agree that\nabortion should be illegal during the last three months of pregnancy?\u2019\u2019\n\nlegalized abortion with scale\n\n\u017e\n.\nyes, no ,\n\ntable 12.3 summarizes ml fitting results. the contrasts of \u2424 indicate\ngreater support for legalized abortion with item 1 when the family has a low\nincome and cannot afford any more children than with the other two. there\nis slight evidence of greater support with item 2 when the woman is not\nmarried and does not want to marry the man than with item 3 when the\nwoman wants the abortion for any reason . the fixed effects estimates have\nlog odds ratio interpretations. for a given subject of either gender, for\ninstance, the estimated odds of supporting legalized abortion for item 1 equal\nexp 0.83 s 2.3 times the estimated odds for item 3. since \u2425s 0.01, for each\nitem the estimated probability of supporting legalized abortion is similar for\nfemales and males with similar random effect values.\nfor these data, subjects are highly heterogeneous \u2434s 8.6 . thus, strong\nassociations exist among responses on the three items. this is reflected by\n1595 of the 1850 subjects making the same response on all three items: that\nis, response patterns 0, 0, 0 and 1, 1, 1 . it implies tremendous variability in\nbetween-subject odds ratios. from 12.7 , for different subjects of a given\ngender, the middle 50% of odds ratios comparing items 1 and 3 are estimated\nto vary between about exp 0.83 y 0.95 = 8.6 and exp 0.83 q 0.95 = 8.6 .\n.\n\n\u02c6\n\n\u02c6\n\nfor contingency tables, one can obtain cell fitted values. to do this, one\nmust integrate over the estimated random effects distribution to obtain\nestimated marginal probabilities of any particular sequence of responses. for\nthe ml parameter estimates, the probability of a particular sequence of\nresponses\nfor a given u is the appropriate product of condi-\ntional probabilities, \u0142 p y s y u , since the responses are independent\n\u017e\ngiven u . integrating this product probability with respect to u for the\n\ny , . . . , y\ni1\n\nit\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nit\n\nit\n\nt\n\ni\n\ni\n\n<\n\ni\n\ni\n\ntable 12.3 summary of ml estimates for random effects model 12.10\nand ml and gee estimates for corresponding marginal model\n\n(\n\n)\n\neffect\n\nabortion\n\nparameter\n\u2424 y \u2424\n3\n1\n\u2424 y \u2424\n2\n1\n\u2424 y \u2424\n3\n2\n\n\u2425\n\ngender\n'\nvar u\n\n\u017e\n\ni\n\n.\n\n\u2434\n\nglmm ml\n\nestimate\n\n0.83\n0.54\n0.29\n0.01\n8.6\n\nse\n\n0.16\n0.16\n0.16\n0.48\n0.54\n\nmarginal model ml marginal model gee\nestimate\n\nestimate\n\nse\n\nse\n\n0.148\n0.098\n0.049\n0.005\n\n0.030\n0.027\n0.027\n0.088\n\n0.149\n0.097\n0.052\n0.003\n\n0.030\n0.028\n0.027\n0.088\n\n "}, {"Page_number": 521, "text": "506\n\nrandom effects: generalized linear mixed models\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u02c6\n\n2.\n\n\u017e\nn 0, \u2434 distribution estimates the marginal probability for a given cell\n\u017e\naveraged over subjects . this requires numerical integration methods de-\nscribed in section 12.6. multiplying this marginal probability of a given\nsequence by the sample size for that multinomial gives a fitted value.\n\n.\nnot surprisingly, for these data, the response patterns 0, 0, 0 and 1, 1, 1\nalso have the largest fitted values for the multinomial for each gender. for\ninstance, for females 440 indicated support under all three circumstances\n\u017e\n457 under none of the three , and the fitted value was 436.5 459.3 . overall\nchi-squared statistics comparing the 16 observed and fitted counts are g2 s\n23.2 and x s 27.8 df s 9 . these are not that large considering the very\nlarge sample size and the few parameters \u2424 , \u2424 , \u2424 , \u2425, \u2434 used to describe\nthe 14 multinomial cell probabilities 8 y 1 s 7 for each gender\nin table\n10.13. here, df s 9 since we are modeling 14 multinomial parameters using\nfive glmm parameters.\n\nan extended model allows interaction between gender and item. it has\ndifferent \u2424 for men and women. however, it does not fit better. the\nlikelihood-ratio statistic s 1.0 df s 2 for testing that the extra parameters\nequal 0.\n\nan alternative analysis of these data focuses on the marginal distributions,\ntreating the dependence as a nuisance. a marginal model analog of 12.10 is\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n2\n\n1\n\n2\n\n3\n\nt\n\nlogit p y s 1 s \u2424 q \u2425x.\n\n\u017e\n\n.\n\nt\n\nt\n\n4\n\n.\n\nfor it, table 12.3 also shows gee estimates for the exchangeable working\ncorrelation structure and ml estimates. the marginal model fits well, with\ng2 s 1.1; here, df s 2 since the model describes six marginal probabilities\n\u017e\nthree for each gender using four parameters. these population-averaged\n\u02c6\n\u0004\n\u2424 are much smaller than the subject-specific \u2424 from the glmm. this\nt\nreflects the very large glmm heterogeneity \u2434s 8.6 and the corresponding\nstrong correlations among the three responses. for instance,\nthe gee\nanalysis estimates a common correlation of 0.82 between pairs of responses.\n\u02c6\n4\nalthough the glmm \u2424 are about five to six times the marginal model \u2424 ,\nt\nso are the standard errors. the two approaches provide similar substantive\ninterpretations and conclusions.\n\n\u02c6\nt\n\n\u02c6\nt\n\n\u02c6\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n12.3.3 longitudinal mental depression study revisited\nwe now revisit table 11.2 from a longitudinal study to compare a new drug\nwith a standard for treating subjects suffering mental depression. in section\n11.2.1 we analyzed the data using marginal models. the response y\nfor\nmeasurement t on mental depression equals 1 for normal and 0 for abnor-\nmal. for severity of initial diagnosis s 1 s severe, 0 s mild , drug treatment\nd 1 s new, 0 s standard , and time of measurement t, we used the model\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nt\n\nlogit p y s 1 s \u2423q \u2424 s q \u2424 d q \u2424 t q \u2424 dt\n\n\u017e\n\n.\n\n3\n\n4\n\n1\n\n2\n\nt\n\nto evaluate the marginal distributions.\n\n "}, {"Page_number": 522, "text": "examples of random effects models for binary data\n\n507\n\ntable 12.4 model parameter estimates for marginal and conditional logit\nmodels fitted to table 11.2\n\nml marginal std. gee marginal std. random effects\n\nparameter\ndiagnosis\ndrug\ntime\ndrug = time\n\nestimate\ny1.29\ny0.06\n0.48\n1.01\n\nerror\n0.14\n0.22\n0.12\n0.18\n\nestimate\ny1.31\ny0.06\n0.48\n1.02\n\nerror ml estimate\n0.15\n0.23\n0.12\n0.19\n\ny1.32\ny0.06\n0.48\n1.02\n\nstd.\nerror\n0.15\n0.22\n0.12\n0.19\n\nnow let y denote observation t for subject i. the model\n\nit\n\nlogit p y s 1 u s \u2423q \u2424 s q \u2424 d q \u2424 t q \u2424 dt q u\n\n<\n\n\u017e\n\n.\n\ni\n\nit\n\n1\n\n2\n\n3\n\n4\n\ni\n\n.\n\n\u017e\n\n\u02c6\n4\n\n\u02c6\n3\n\nhas subject-specific rather than population-averaged effects. table 12.4 shows\nthe ml estimates. the time trend estimates are \u2424 s 0.48 for the standard\ndrug and \u2424 q \u2424 s 1.50 for the new one. these are nearly identical to the\nml and gee estimates for the corresponding marginal model, also shown in\nthe table these are discussed in sections 11.2.1 and 11.3.2 . the reason is\nthat the repeated observations do not exhibit much correlation, as the gee\nanalysis observed. here, this is reflected by \u2434s 0.07, showing little hetero-\ngeneity among subjects.\n\n\u02c6\n3\n\n\u02c6\n\nbased on the model fit, integrating over the n 0, 0.07\n\nrandom effects\ndistribution yields marginal fitted values of the possible response sequences.\ncomparing these to the sample counts in table 11.2 indicates a relatively\ngood fit. the model describes the 28 multinomial cell probabilities seven for\nthe trivariate response at each of the four severity\u1390drug combinations using\nsix parameters. the usual fit statistics comparing the observed cell counts to\ntheir fitted values are g s 22.0 and x s 20.8 df s 28 y 6 s 22 .\n.\nthe deviance increases by only 0.001 when one assumes that \u2434s 0. from\nresults to be discussed in section 12.6.6, the p-value for comparing models is\nhalf what one gets by treating the deviance as chi-squared with df s 1, or\np s 0.49. this simpler model, which gives nearly identical effect estimates\nand se values, is adequate. this is also suggested by aic values e.g., proc\nnlmixed in sas reports 1173.9 for the random effects model and 1171.9\nfor the simpler model with \u2434s 0 .\n.\n\n2.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n2\n\n2\n\n12.3.4 modeling heterogeneity among multicenter clinical trials\nmany applications compare two groups on a response for data stratified on a\nthird variable. with binary outcomes, the data form several 2 = 2 contin-\ngency tables. the main focus relates to studying the association in the 2 = 2\ntables and whether and how it varies among the strata.\n\n "}, {"Page_number": 523, "text": "508\n\nrandom effects: generalized linear mixed models\n\nthe strata are sometimes themselves a sample, such as schools or medical\nclinics. a random effects approach is then natural. with a random sampling\nof strata, it enables inferences to extend to the population of strata. the fit of\nthe random effects model provides a simple summary such as an estimated\nmean and standard deviation of log odds ratios for the population of strata.\nin each stratum it also provides a predicted log odds ratio that shrinks the\nsample value toward the mean. this is especially useful when the sample size\nin a stratum is small and the ordinary sample odds ratio has large standard\nerror. even when the strata are not a random sample or not even a sample\nand a random effects approach is not as natural, the model is beneficial for\nthese purposes.\n\nwe illustrate using table 12.5, previously analyzed in section 6.3, showing\nthe results of a clinical trial at eight centers. the purpose was to compare an\nactive drug and a control, for curing an infection. for a subject in center i\nusing treatment t 1 s active drug; 2 s control , let y s 1 denote success.\none possible model is the logistic-normal,\n\n\u017e\n\n.\n\nit\n\nlogit p y s 1 u s \u2423q \u2424r2 q u\n\n<\n\n\u017e\n\nlogit p y s 1 u s \u2423y \u2424r2 q u ,\n\n<\n\n\u017e\n\ni1\n\ni2\n\n.\n\n.\n\ni\n\ni\n\ni\n\ni\n\n\u017e\n\n12.11\n\n.\n\ntable 12.5 clinical trial relating treatment to response for eight centers\n\ncenter\n\ntreatment\n\nsuccess\n\nfailure\n\nresponse\n\nsample\n\nodds ratio\n\nfitted\n\nodds ratio\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\ndrug\ncontrol\ndrug\ncontrol\ndrug\ncontrol\ndrug\ncontrol\ndrug\ncontrol\ndrug\ncontrol\ndrug\ncontrol\ndrug\ncontrol\n\n11\n10\n16\n22\n14\n7\n2\n1\n6\n0\n1\n0\n1\n1\n4\n6\n\n.\nsource: beitler and landis 1985 .\n\n\u017e\n\n25\n27\n4\n10\n5\n12\n14\n16\n11\n12\n10\n10\n4\n8\n2\n1\n\n1.19\n\n1.82\n\n4.80\n\n2.29\n\n\u2b01\n\n\u2b01\n\n2.0\n\n0.33\n\n2.02\n\n2.09\n\n2.19\n\n2.11\n\n2.18\n\n2.12\n\n2.11\n\n2.06\n\n "}, {"Page_number": 524, "text": "examples of random effects models for binary data\n\n509\n\n\u0004\n\n4\n\n\u017e\n\n2.\n\ni\n\nwhere u are independent n 0, \u2434 variates. this model assumes that the\nlog odds ratio \u2424 between treatment and response is constant over centers.\nthe parameter \u2434 summarizes center heterogeneity in the success probabili-\nties.\n\na logistic-normal model permitting treatment-by-center interaction is\n\nlogit p y s 1 u , b s \u2423q \u2424q b r2 q u ,\nlogit p y s 1 u , b s \u2423y \u2424q b r2 q u ,\n\n\u017e\n\u017e\n\n.\n.\n\ni1\n\n\u017e\n\u017e\n\n.\n.\n\ni\n\ni\n\ni\n\ni\n\n<\n\n<\n\ni\n\ni\n\ni2\n\ni\n\ni\n\n\u017e\n\n12.12\n\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nb\n\na\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n2. \u0004\na\n\n\u0004\n4\nwhere u are independent n 0, \u2434 , b are independent n 0, \u2434 , and u\nare independent of b . the log odds ratio equals \u2424q b in center i. these\nvary among centers according to a n \u2424, \u2434 distribution. that is, \u2424 is the\nexpected center-specific log odds ratio between treatment and response, and\n\u2434 describes variability in those log odds ratios. the model parameters are\nb\u017e\n.\n\u2423, \u2424, \u2434 , \u2434 .\n\n2.\nb\n\n2.\nb\n\n\u02c6\n\n\u02c6\n\n\u017e\n.\n\nin table 12.5 the sample success rates vary markedly among centers both\nfor the control and drug treatments, but in all except the last center that rate\nis higher for the drug treatment. in using models with random center and\npossibly random treatment effects, it is preferable to have more than eight\ncenters. it is difficult to get reliable variance component estimates with so\nfew centers. keeping this in mind, we use these data to illustrate the models.\nwith a large number of centers it would also be sensible to allow correlation\nbetween b and u , but we shall not attempt that here. the treatment\nestimates are \u2424s 0.739 se s 0.300 for the model 12.11 of no interaction\nand \u2424s 0.746 se s 0.325 for the model 12.12 permitting interaction.\nconsiderable evidence of a drug effect occurs. with such a small sample,\nhowever, it is unclear whether that effect is weak or moderate.\nthe evidence about association is weaker for the model permitting inter-\naction. the wald statistics are 0.739r0.300 s 6.0 for the no-interaction\nmodel and 0.746r0.325 s 5.3 for the interaction model. the correspond-\ning likelihood-ratio statistics are 6.3 and 4.6 df s 1 . the extra variance\ncomponent in the interaction model pertains to variability in the log odds\nratios. as its estimate \u2434 increases, so does the standard error of the\nestimated treatment effect \u2424 tend to increase. in this example, \u2434 s 0.15 is\nrelatively small and the standard errors of \u2424 are not very different in the two\nmodels. when \u2434 s 0, the standard errors and the model fits are the same.\nto show the effect of larger \u2434 on the standard error of the mean\ntreatment effect estimate \u2424, we alter table 12.5 slightly. we change three\nfailures to successes for drug in center 3 and three successes to failures for\ndrug in center 8. with these changes,\nthe estimated variability of the\ntreatment effects increases from \u2434 s 0.15 to \u2434 s 1.4. the ml estimates of\nthe mean treatment effects are then \u2424s 0.722 se s 0.299 for the no\ninteraction model 12.11 and \u2424s 0.767 se s 0.623 for the interaction\nmodel. the wald statistics are 5.8 and 1.5. the evidence of a treatment\n\n\u02c6b \u02c6\n\n\u02c6b\n\n\u02c6b\n\n\u02c6b\n\n\u02c6\nb\n\n\u02c6\nb\n\n.2\n\n.2\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 525, "text": "510\n\nrandom effects: generalized linear mixed models\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u02c6b\n\neffect is then dramatically weaker for the interaction model 12.12 . not\nsurprisingly, when the treatment effect varies substantially among centers, it\nis more difficult to estimate the mean of that effect.\nfor the actual data in table 12.5, because \u2434 s 0.15 for model 12.12 is\nrelatively small, the model shrinks the sample odds ratios considerably. table\n12.5 shows the sample values and the model predicted values. these are\nbased on predicting the random effects to be explained in section 12.6 , and\nsubstituting them and the ml estimates of fixed effects into the model\nformula to estimate the two response probabilities for each treatment in each\ncenter. the sample odds ratios vary from 0.33 to \u2b01; their random effects\nmodel counterparts computed with proc nlmixed in sas vary only\nbetween 2.0 and 2.2. the smoothed estimates are much less variable and do\nnot have the same ordering as the sample values. for instance, the smoothed\nestimate of 2.2 for center 3 is greater than the estimate of 2.1 for center 6,\neven though the sample value is infinite for the latter. this partly reflects the\ngreater shrinkage that occurs when sample sizes are smaller. when \u2434 s 0,\nmodel 12.12 provides the same fit as model 12.11 , and estimated odds\nratios are identical in each center.\n\n\u02c6b\n\nfor related analyses permitting heterogeneity in odds ratios with several\n\n.\n2 = 2 tables, see liu and pierce 1993 and skene and wakefield 1990 .\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n12.3.5 alternative formulations of random effects models\nthere are other ways to express the models. for instance, an equivalent\nexpression for interaction model 12.12 is\n\n\u017e\n\n.\n\n<\n\ni\n\nt\n\nit\n\nit\n\n\u017e\n\n.\n\n\u017e\n\nlogit p y s 1 u , b s \u2423q \u2424x q b q u ,\n. \u0004\n2.\n\nwhere x is a treatment dummy variable x s 1, x s 0 , u are indepen-\nare independent n 0, \u2434 . here, b y b\n\u0004\ndent n 0,\u2434 , and b\ni1\ni2\n2\ncorresponds to b in parameterization 12.12 , and 2 \u2434 corresponds to \u2434 .\nb\nformulating a random effects model requires care about implications of\nthe model expression and the random effects correlation structure. suppose\nthat one expressed the interaction model 12.12 as\n\n\u0004\nand b\ni2\n\n2.\na\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\ni1\n\n4\n\n4\n\n4\n\nit\n\n1\n\n2\n\n2\n\nt\n\ni\n\ni\n\ni\n\nlogit p y s 1 u , b s \u2423q \u2424q b x q u ,\n\n\u017e\n\n.\n\n<\n\ni\n\nt\n\ni\n\n.\n\ni\n\nit\n\ni\n\n\u017e\n\n12.13\n\n.\n\n\u0004\n\n4\n\n\u017e\n\ni\n\nwith b from n 0, \u2434 . this is inappropriate, since the model then imposes\ngreater variability for the logit with the first treatment than the second, since\nx s 0 and u and b are uncorrelated. also, the model should not depend\n2\non the definition of the dummy variable x . note, however, that if z s x q c\nfor some constant c, then model 12.13 is equivalently\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n4\n\nt\n\nt\n\nt\n\ni\n\ni\n\n\u017e\n2.\nb\n\n\u017e\n\nlogit p y s 1 u , b\n.\ni\ns \u2423q \u2424q b\n. \u017e\n\n\u017e\n\nit\n\ni\n\n<\n\ni\n\nz y c q u s \u2423x q \u2424q b z q \u00ae ,\n\n.\n\n\u017e\n\n.\n\ni\n\nt\n\nt\n\ni\n\ni\n\n "}, {"Page_number": 526, "text": "examples of random effects models for binary data\nwhere \u2423\u2b18 s \u2423y c\u2424 and \u00ae s u y cb . thus, \u00ae , b\nare correlated even if\n\u017e\nu , b\nare not. in fact, expression 12.13 is sensible only with correlated\ni\ni\nrandom effects. it is then equivalent to 12.12 with correlated random\neffects. see agresti and hartzel 2000 for further discussion.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\n511\n\n\u017e\n\n.\n\n12.3.6 capture\u2013recapture modeling to predict population size\ncapture\u1390recapture experiments are a method of using a series of samples to\nestimate the size of a population. such methods have traditionally been used\nto estimate animal abundance in some habitat. at each sampling occasion,\nanimals are captured and marked in some manner. the animals captured for\nany given sample are freed and all animals are candidates for recapture in a\nlater sample. with t sampling occasions, a 2t contingency table displays the\ndata, with scale captured, not captured at each occasion. the count n22 \u2b48\u2b48\u2b48 2\nis missing for the cell corresponding to noncapture at each occasion. if we\nknew this cell count, adding it to the others would yield the population size.\nmodels specified for this 2t table use the 2t y 1 observed counts to fit the\nmodel. the fit refers to those 2t y 1 cells, but extrapolating it yields an\nestimated count in the unobserved cell. adding that to the total of the 2t y 1\nobserved counts yields an estimate of population size.\nto illustrate, suppose that t s 2. we observe n\n11\n\nanimals at both occa-\nsions, n\nat the second but\nnot the first. we do not know the number n\nnot captured either time. if we\nassumed independence in the 2 = 2 table, the prediction n would be the\nvalue giving an odds ratio of 1.0; but n n r n n s 1 implies that\nn s n n rn . this yields a population size prediction sekar and deming\n\u02c622\n12\n1949 of\n\nat the first but not the second occasion, and n\n\n.\n21\u017e\n\n\u02c611\n\n\u02c622\n\n12\n\n21\n\n22\n\n22\n\n12\n\n21\n\n11\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u02c6n s n q n q n q n n rn\n\n11\n\n12\n\n21\n\n11\n\n12\n21\n$\n\ns n n rn\n\n1q q1\n\n11\n\nwith var n s\n\n\u02c6\n\n\u017e\n\n.\n\nn n\n12\n\nn n\n1q q1\n3n11\n\n21\n\n.\n\n.\n\n\u017e\n\nthe assumption of independence is usually unrealistic, however. with addi-\ntional sampling occasions, one can try more complex models.\ntable 12.6, analyzed by cormack 1989 and others, refers to a study\nhaving t s 6 consecutive trapping days for a population of snowshoe hares.\nthe study observed 68 hares. for instance, table 12.6 indicates that 3 hares\nwere observed on the first day but on none of the other days. for simplicity,\nmodels for studies over a brief time period assume that no deaths, births, or\nimmigration into the population occurred during the study period. this is\ncalled a closed population.\n\nmost methods for capture\u1390recapture treat the probability of capture at a\ngiven occasion as identical for each subject e.g., animal . this is usually\n\n\u017e\n\n.\n\n "}, {"Page_number": 527, "text": "512\n\nrandom effects: generalized linear mixed models\n\ntable 12.6 results of capture\u2013recapture of snowshoe hares\n\na\ncapture 3, capture 2, capture 1\n\ncapture capture capture\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n1\n\n1\n\n0\n\n0\n\n1\n\n0\n\n1\n\n0\n\n0\n\n4\n0\n\n5\n0\n\n6\n0\n\n1 0 0\n5\n\u017e\n3.2\n0\n\u017e\n1.1\n0\n\u017e\n0.9\n0\n\u017e\n0.5\n2\n\u017e\n1.5\n1\n\u017e\n0.8\n1\n\u017e\n0.7\n0\n\u017e\n0.5\navalues in parentheses represent the fit of the logistic-normal model.\n.\nsource: a. agresti, biometrics 50: 494\u1390500 1994 .\n\n0 0 0\n\u138f\n\u017e\n24.0\n3\n\u017e\n4.8\n4\n\u017e\n3.9\n1\n\u017e\n1.3\n4\n\u017e\n6.8\n4\n\u017e\n2.3\n2\n\u017e\n1.9\n1\n\u017e\n1.0\n\n0 1 1\n0\n\u017e\n0.9\n0\n\u017e\n0.5\n1\n\u017e\n0.4\n0\n\u017e\n0.3\n1\n\u017e\n0.6\n0\n\u017e\n0.5\n0\n\u017e\n0.4\n0\n\u017e\n0.5\n\n0 0 1\n3\n\u017e\n2.3\n2\n\u017e\n0.8\n2\n\u017e\n0.6\n0\n\u017e\n0.3\n1\n\u017e\n1.1\n0\n\u017e\n0.6\n0\n\u017e\n0.5\n1\n\u017e\n0.4\n\n0 1 0\n6\n\u017e\n5.4\n3\n\u017e\n1.8\n3\n\u017e\n1.5\n0\n\u017e\n0.8\n1\n\u017e\n2.6\n3\n\u017e\n1.3\n1\n\u017e\n1.1\n1\n\u017e\n0.9\n\n1\n\n0\n\n0\n\n1\n\n0\n\n1\n\n1\n\n1\n\n0\n\n1\n\n1\n\n1\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n1 0 1\n1\n\u017e\n0.5\n1\n\u017e\n0.3\n1\n\u017e\n0.2\n0\n\u017e\n0.2\n0\n\u017e\n0.4\n0\n\u017e\n0.3\n0\n\u017e\n0.3\n0\n\u017e\n0.3\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n1 1 0\n0\n\u017e\n1.2\n0\n\u017e\n0.6\n0\n\u017e\n0.5\n0\n\u017e\n0.4\n2\n\u017e\n0.9\n2\n\u017e\n0.7\n1\n\u017e\n0.6\n1\n\u017e\n0.7\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n1 1 1\n0\n\u017e\n0.3\n0\n\u017e\n0.3\n0\n\u017e\n0.2\n0\n\u017e\n0.3\n0\n\u017e\n0.4\n0\n\u017e\n0.4\n0\n\u017e\n0.4\n2\n\u017e\n0.7\n\n.\n\n.\n\n.\n\n.\n\n.\n\nunrealistic. one way to allow heterogeneous capture probabilities uses a logit\ni s 1, . . . , n with n\nmodel having subject random effects. for subject i,\n, where y s 1 denotes capture in sample t\nunknown, let y s y , . . . , y\n.\nand y s 0 denotes noncapture. lacking explanatory variables, one might\nuse the rasch-type model\n\nit\n\n\u017e\n\nx\ni\n\ni1\n\nit\n\nit\n\nlogit p y s 1 u s u q \u2424 ,\n\n<\n\n\u017e\n\n.\n\nit\n\nt\n\ni\n\ni\n\n\u0004\n\n4\n\n\u017e\n\n2.\n\ni\n\nt\n\nw\n\n\u017e\n\nwhere u are independent n 0, \u2434 . the larger the value of \u2424, the greater\nthe capture probability at occasion t. the larger is \u2434, the more heteroge-\nneous are the capture probabilities. when \u2434s 0 this logistic-normal model\nsimplifies to mutual\nfor the 2\ntable.\n\nloglinear model 8.6\n\nindependence i.e.,\n\n.x\n\nas with other random effects models, integrating the random effect from\nthe probability mass function of y u\nyields the likelihood function as\ndiscussed in section 12.6 . one can consider this likelihood function and the\nresulting ml estimates of \u2424 and \u2434 for all possible counts in the unob-\nserved cell. a profile likelihood function views the maximized likelihood as a\nfunction of the unobserved cell count. the ml prediction for that unob-\nserved cell count is the value that maximizes this profile likelihood. lacking\nspecialized software, one can fit the random effects model repeatedly with\nvarious counts in the unobserved cell to determine by trial and error the\ncount that maximizes the likelihood function.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u0004\n\n4\n\nt\n\nt\n\ni\n\ni\n\n<\n\n "}, {"Page_number": 528, "text": "random effects models for multinomial data\n\n513\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u02c6\n\nml fitting of this model to table 12.6 yields a prediction of 24 for the\nunobserved cell count. since the study observed 68 hares, the population size\nestimate is n s 92. for this fit, \u2434s 1.0.\n\n\u02c6\n\n2\u017e .\n1\n\nmethods for obtaining a confidence interval for n include using the\nprofile likelihood function or a nonparametric bootstrap method. with the\nprofile likelihood approach, the interval for the missing cell count consists of\nthe possible counts for that cell such that the g2 fit statistic increases by less\nthan \u2439 \u2423 from its value at the ml estimate. adding the number of subjects\nobserved in the samples to the endpoints of this interval gives the corre-\nsponding interval for n. for the snowshoe hares, a 95% profile-likelihood\nconfidence interval for n is 75, 154 . it is common for n to be nearer the\nlow end of the interval. see coull and agresti 1999 for details.\n\u02c6\n\nthe greater the heterogeneity, as reflected by larger \u2434, n tends to be\ntends to be wider. large \u2434 causes\nlarger and the confidence interval\ndifficulties in estimation, since it results in a relatively flat likelihood surface.\nthis implies imprecise estimates of n. in particular, the upper limit of the\nprofile-likelihood confidence interval for n is essentially infinite when the\nlikelihood function gets sufficiently flat. also, the ml estimator is then often\n\u02c6\nunstable, with small changes in the data yielding large changes in n.\ndifficulties can also arise when probabilities of capture are small. evidence\nof this occurs when most subjects captured appear in only one sample. when\nthis happens or when \u2434 is large, it is unrealistic to expect narrow confidence\nintervals for n.\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\nalternative models are discussed in section 13.1.3. models that ignore\nlikely heterogeneity can give unrealistically narrow confidence intervals for\nn. although traditionally used for animal populations, capture\u1390recapture\napplications also include estimating population size for human populations,\nsuch as estimating population prevalence of injecting drug use and hiv\ninfection. darroch et al. 1993 considered census population estimation, and\nchao et al. 2001 estimated the number of people infected during a hepatitis\noutbreak problem 12.21 . an interesting application is estimating the num-\nber of files on the world wide web relating to some subject by taking\n.\nsamples using several search engines fienberg et al. 1999 .\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n12.4 random effects models for multinomial data\n\nrandom effects models for binary responses extend to multicategory re-\nsponses. for the multicategory models of chapter 7, a multinomial observa-\ntion with i categories is a vector of i y 1 indicators, the jth of which is 1\nwhen the observation falls in category j and 0 otherwise. in section 7.1.5 we\ndefined a multivariate glm by applying a vector of link functions to this\nmultivariate response. adding random effects extends this multivariate glm\nand the glmm 12.1 to a multivariate glmm hartzel et al. 2001b; tutz\nand hennevogl 1996 . this class includes models for nominal and ordinal\nresponses.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n "}, {"Page_number": 529, "text": "514\n\nrandom effects: generalized linear mixed models\n\n12.4.1 cumulative logit model with random intercept\nmodeling is simpler with ordinal than nominal responses, since often the\nsame random effect and the same fixed effect can apply to each logit. with\n.\ncumulative logits, this is the proportional odds structure section 7.2.2 .\ndenote the possible outcomes for y , observation t in cluster i, by 1, 2, . . . , i.\na glmm for the cumulative logits has the form\nlogit p y f j u s \u2423 q x \u2424 q z u ,\n\nj s 1, . . . , i y 1.\n\n12.14\n\n\u017e\n\nit\n\n\u017e\n\n.\n\n<\n\n\u017e\n\nx\nit\n\ni\n\n.\n\ni\n\nx\nit\n\nit\n\nj\n\n\u017e\n\n.\n\nhedeker and gibbons 1994 discussed model fitting, primarily with u as\nmultivariate normal.\n\nfor cumulative logit and probit random intercept models,\n\nthe same\nrelationship exists between their effects and those in marginal models as\npresented in section 12.2.2 for binary-response models. marginal effects tend\nincreasingly so as \u2434 increases. also, the same predictor\nto be smaller,\n\u017e\nstructure as in 12.14 holds with other links for which a common effect for\neach logit is plausible. for instance, hartzel et al. 2001a, b used it with\nadjacent-categories logits.\n\n.\n\n\u017e\n\n.\n\ni\n\ninsomnia study revisited\n\n12.4.2\ntable 11.4 showed results of a clinical trial at two occasions comparing a\ndrug with placebo in treating insomnia patients. in sections 11.2.3 and 11.3.3\nthe data were analyzed with marginal models. for y s time to fall asleep at\noccasion t, the marginal model\n\nt\n\nlogit p y f j s \u2423 q \u2424 t q \u2424 x q \u2424 tx\n\n\u017e\n\n.\n\nj\n\n1\n\n2\n\n3\n\nt\n\npermitted interaction between t s occasion 0 s initial, 1 s follow-up and\nx s treatment 1 s active, 0 s placebo . table 12.7 shows the ml and gee\nestimates.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nnow, let y denote the response for subject i at occasion t. table 12.7 also\n\nit\n\nshows results of fitting the random-intercept model\n\nlogit p y f j u s u q \u2423 q \u2424 t q \u2424 x q \u2424 tx.\n\n<\n\n\u017e\n\n.\n\ni\n\nit\n\n1\n\n2\n\n3\n\ni\n\nj\n\ntable 12.7 fits of cumulative logit models to table 11.4 a\n\nmarginal\n\neffect\ntreatment\noccasion\ntreatment = occasion\navalues in parentheses represent standard errors.\n\n0.046 0.236\n1.074 0.162\n0.662 0.244\n\nml\n\u017e\n\u017e\n\u017e\n\n.\n.\n.\n\nmarginal\n\ngee\n\n0.034 0.238\n1.038 0.168\n0.708 0.244\n\n\u017e\n\u017e\n\u017e\n\n.\n.\n.\n\n.\n\nrandom effects\n\u017e\nglmm ml\n.\n0.058 0.366\n.\n1.602 0.283\n.\n1.081 0.380\n\n\u017e\n\u017e\n\u017e\n\n "}, {"Page_number": 530, "text": "random effects models for multinomial data\n\n515\n\nresults are substantively similar to the marginal model, but estimates and\nstandard errors are about 50% larger. this reflects the relatively large\nheterogeneity \u2434s 1.90 and the resultant strong association between the\nresponses at the two occasions.\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n12.4.3 cluster sampling\nwith surveys that use cluster sampling, standard methods based on simple\nrandom sampling e.g., for a single multinomial sample require adjustment.\nordinary standard errors are too small. the usual chi-squared test statistics\nno longer have chi-squared null distributions, but rather, weighted sums of\nchi-squared. rao and thomas 1988 surveyed ways of adjusting standard\ninferences to take into account complex sampling methods in the analysis and\nmodeling of categorical data.\n\nwhen the sampling scheme randomly samples clusters, one can account\nfor the clustering using cluster random effects. we illustrate using data from\nbrier 1980 , who reported 96 observations taken from 20 neighborhoods the\nclusters on y s satisfaction with home and x s satisfaction with neighbor-\nhood as a whole. each variable was measured with the ordinal scale unsatis-\nfied, satisfied, very satisfied . brier\u2019s analysis adjusted for clustering by\nreducing the pearson statistic for testing independence in the 3 = 3 contin-\ngency table relating x and y from 17.9 to 15.7 df s 4 .\n.\nconsider the model for y , observation t in cluster i,\nlogit p y f j u s u q \u2423 q x \u2424,\n\n12.15\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\nit\n\n\u017e\n\n.\n\n<\n\n\u017e\n\nj\n\nit\n\n.\n\ni\n\nit\n\ni\n\n\u017e\n\n.\n\nwith scores 1, 2, 3 for the satisfaction levels of x . with a n 0, \u2434 distribu-\ntion assumed for u , the ml effect estimate is \u2424s y1.201 se s 0.407 ,\n.\nwith \u2434s 0.92. by contrast, treating the 96 observations as a random sample\ncorresponds to fitting this model with \u2434s 0. it has \u2424s y1.226 se s 0.370 .\n.\na slight reduction in significance results from adjusting for clustering.\n\nit \u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n\u017e\n\ni\n\n\u017e\n\n2.\n\n12.4.4 baseline-category logit models with random effects\nfor nominal response variables, one can formulate a binary model that pairs\neach category with a baseline and fit these models simultaneously while\nallowing separate effects. this requires using a vector of cluster-specific\nrandom effects u , one for each logit. the general form of the baseline-cate-\ngory logit model with random effects is\n\ni j\n\nlog\n\np y s j\n\u017e\np y s i\n\u017e\n\nit\n\nit\n\n.\n.\n\ns \u2423 q x \u2424 q z u ,\n\nx\nit\n\nx\nit\n\ni j\n\nj\n\nj\n\nj s 1, . . . , i y 1.\n\nthe fixed effects \u2424 and the random effects u\ndepend on j, since the\nbaseline category is arbitrary. with nominal responses there is no reason to\nexpect effects to be similar for different j.\n\ni j\n\nj\n\n "}, {"Page_number": 531, "text": "516\n\nrandom effects: generalized linear mixed models\n\ni\n\n\u0004\n\n4\n\n.\n\nx\ni\n\n\u017e x\ni1\n\nx\ni, iy1\n\ncluster i has a vector u s u , . . . , u\n\nof random effects. the usual\napproach treats u as independent multivariate normal variates. we recom-\nmend an unspecified covariance matrix \u233a for u . for instance, it is sensible\nto allow different variances for random effects that apply to different logits.\nwith a common variance, that variance would not be the same as that for\nthe implied random effect for a logit for an arbitrary pair of categories,\nlog p y s j rp y s k . with unspecified covariance the model is struc-\nturally the same regardless of the choice of baseline category. see hartzel\net al. 2001b for an example.\n\n.x\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nw\n\nit\n\nit\n\ni\n\n12.5 multivariate random effects models for\nbinary data\n\nin practice, random effects are often univariate, taking the form of random\nintercepts. however, we\u2019ve seen that nominal responses require multivariate\nrandom effects and that bivariate random effects are helpful for describing\nheterogeneity in multicenter clinical trials. in this section we present other\nexamples in which multivariate random effects are natural.\n\n\u017e\n\n12.5.1 matched pairs with a bivariate binary response\n.\nleo goodman analyzed table 12.8 in several articles e.g., goodman 1974 .\na sample of schoolboys were interviewed twice, several months apart, and\nasked about their self-perceived membership in the \u2018\u2018leading crowd\u2019\u2019 and\nabout whether they sometimes needed to go against their principles to belong\nto that group. thus, there are two binary response variables, which we refer\nto as membership and attitude, measured at two interview times for each\n.\nsubject. table 12.8 labels the categories for attitude as positive, negative ,\nwhere \u2018\u2018positive\u2019\u2019 refers to disagreeing with the statement that one must go\nagainst his principles.\n\n\u017e\n\ntable 12.8 membership and attitude toward the \u2018\u2018leading crowd\u2019\u2019\n\na\n\n.\n\n\u017e\nyes, positive\n\n.m, a for second interview\n\u017e\n\u017e\n.\n\u017e\nno, positive\nyes, negative\n\n\u017e\n.m, a for\nfirst interview\n458\nyes, positive\n171\nyes, negative\n184\nno, positive\nno, negative\n85\nam, membership; a, attitude.\nsource: j. s. coleman, introduction to mathematical sociology london: free press of glencoe,\n1964 , p. 170.\n\n\u017e\n.\nno, negative\n\n140\n182\n75\n97\n\n110\n56\n531\n338\n\n49\n87\n281\n554\n\n.\n\n.\n\n\u017e\n\n "}, {"Page_number": 532, "text": "multivariate random effects models for binary data\n\n517\nbe the response at interview time t on variable \u00ae,\n\nfor subject i, let y\n\nit \u00ae\n\n<\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\ni a\n\ni\u00ae\n\n.4\n\n\u0004\u017e\n\nit \u00ae\n\ni m\n\u017e\n\ni\u00ae\n.\n.\n\nt \u00ae\n\u017e\n\n\u02c6\n1 m\n\n12.16\n\nu , u\ni m\n\ni a\n\u02c6\n2 m\n\nlogit p y s 1 u s \u2424 q u\n\nwhere \u00ae s m for membership and \u00ae s a for attitude. the logit model\n\u017e\n\n.\nis a multivariate form of the rasch-type model 12.4 . it has additive item\nand subject effects for each variable \u00ae. here, u , u\nis a bivariate random\neffect that describes subject heterogeneity for membership, attitude . we\nassume that the\nare independent from a bivariate normal distri-\n.\nbution, n 0, \u233a , with possibly different variances and nonzero correlation.\nthe ml fit yields \u2424 y \u2424 s 0.379 se s 0.075 and \u2424 y \u2424 s\n\u02c6\n1 a\n0.176 se s 0.058 . for both variables, the probability of the first outcome\ncategory is higher at the second interview. for instance, for a given subject\nthe odds of self-perceived membership in the leading crowd at interview 2\nare estimated to be exp 0.379 s 1.46 times the odds at interview 1.\nthe estimated correlation between the random effects is 0.30. their\nestimated standard deviations are \u2434 s 3.1 for u\n\u0004\n4\n.\ni a\nsince these are quite different, the relative sizes of membership and attitude\n\u017e\nrecall the caveat in\neffects differ for marginal and conditional models\nsection 12.2.3 . the marginal effect is attenuated more for membership. for\nthis\nis\nexp 0.379 rexp 0.176 s 1.46r1.19 s 1.22. for the marginal model, the esti-\n\u017e\nmated odds ratios use the marginal distributions of each variable at each time\nw\ne.g., this is 1392r2006 r 1253r2145 s 1.188 for membership , and the\nratio of estimated odds ratios is 1.188r1.133 s 1.05.\n\nand \u2434 s 1.5 for u\n\u0004\n\nconditional model,\n\nestimated odds\n\nratio of\n\nratios\n\n\u02c6\n2 a\n\nthe\n\n\u02c6\n2\n\n\u02c6\n1\n\nintegrating over the estimated random effects distribution yields fitted\nvalues for the 16 possible sequences of responses in table 12.8. the deviance\nof g s 5.5 df s 8 compares the 16 observed counts to their fitted values.\nthe model, which describes 15 multinomial probabilities with seven parame-\nters, fits well. the model constraining the random effects to be uncorrelated\nfits poorly g s 97.5, df s 9 . the model constraining the random effects to\nbe perfectly correlated is equivalent to having a single random effect u for\neach subject. the model is then a rasch-type model with four items that are\nthe combinations of interviews and variables. that model fits very poorly\ng s 655.5, df s 10 . agresti et al. 2000 gave further details.\n\u017e\n\ni m\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\nx\n\n4\n\n2\n\n2\n\n2\n\ni\n\n12.5.2 continuation-ratio logits for clustered ordinal\noutcomes: toxicity study\nfor continuation-ratio logit models with ordinal responses, the logits refer to\nindependent binomial variates section 7.4.3 . thus, binary logit random\neffects models apply to clustered ordinal responses using continuation-ratio\nlogits ten have and uttal 1994 . for observation t in cluster i, let \u243b s\np y s j y g j, u . more generally, this probability could also depend on\n\u017e\nt, but\nthe example below. the\ncontinuation-ratio logits are logit \u243b , j s 1, . . . , i y 1 .\n4\n\n<\nthis generality is not needed for\n\n. \u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u0004\n\ni j\n\ni j\n\nit\n\nit\n\ni j\n\n "}, {"Page_number": 533, "text": "518\n\nrandom effects: generalized linear mixed models\n\ni\n\ni j\n\ni j\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\nbe the number of subjects in cluster i making response j. let\nlet n\ni j\nn s \u00fdi\nn . for a given cluster in a continuation-ratio logit model, treating\njs1\ni j\ni\n\u017e\n.\nas multinomial is equivalent to treating them as a sequential\nn , . . . , n\ni, iy1\ni1\n.\nn , \u243b ,\nset of independent binomial variates, where n\nih\nj s 1, . . . , i y 1.\n\nis bin n y \u00fd\n\nh- j\n\nwe illustrate with a developmental toxicity study conducted under the\nu.s. national toxicology program. this study examined the developmental\neffects of ethylene glycol eg by administering one of\nfour dosages\n0, 0.75, 1.50, 3.00 grkg to pregnant rodents. the four dose groups had\n\u017e\n\u017e\n25, 24, 22, 23 pregnant rodents. the clusters are litters of mice. the three\npossible outcomes deadrresorption, malformation, normal\n.\nfor each fetus\nare ordered, normal being the most desirable result. table 12.9 shows the\ndata. the continuation-ratio logit is natural here since categories are hierar-\nchically related; an animal must survive before a malformation can take\n.\nplace. the following analyses are from coull and agresti 2000 .\n\n.\n\u017e\nfor litter i in dose group d, let logit \u243b\nbe the continuation-ratio logit\nfor the probability of death and logit \u243b\nthe continuation-ratio logit for\ni\u017e d.2\n\u017e\n.\nthe conditional probability of malformation, given survival. the notation i d\nx\nrepresents litter i nested within dose d. let x be the dosage for group d.\nwe account for the litter effect using litter-specific random effects u s\n\u017e\nsampled from n 0, \u233a . this bivariate random effect allows for\nu\ndiffering amounts of overdispersion for the probability of death and for the\nprobability of malformation, given survival. a model also permitting different\nfixed effects for each is\n\ni\u017e d.1\n.\n\n, u\n\ni\u017e d.2\n\ni\u017e d.1\n\ni\u017e d.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\nw\n\nd\n\nd\n\nlogit \u243b s u q \u2423 q \u2424 x .\n\n\u017e\n\n.\n\ni\u017e d. j\n\ni\u017e d. j\n\nj\n\nd\n\nj\n\n\u017e\n\n12.17\n\n.\n\n(\n\ntable 12.9 response counts for 94 litters of mice on number dead,\nnumber malformed, number normal\ndose s 0.00 grkg\n. \u017e\n\u017e\n.\n1, 0, 7 , 0, 0, 14\n. \u017e\n\u017e\n.\n0, 0, 13 , 0, 0, 10\n. \u017e\n\u017e\n.\n0, 1, 15 , 1, 0, 14\n. \u017e\n\u017e\n.\n1, 0, 10 , 0, 0, 12\n. \u017e\n\u017e\n.\n0, 0, 11 , 0, 0, 8\n. \u017e\n\u017e\n.\n1, 0, 6 , 0, 0, 15\n. \u017e\n\u017e\n.\n0, 0, 12 , 0, 0, 12\n. \u017e\n\u017e\n.\n0, 0, 13 , 0, 0, 10\n. \u017e\n\u017e\n.\n0, 0, 10 , 1, 0, 11\n. \u017e\n\u017e\n.\n0, 0, 12 , 0, 0, 13\n. \u017e\n\u017e\n.\n1, 0, 14 , 0, 0, 13\n\u017e\n. \u017e\n.\n0, 0, 13 , 1, 0, 14\n.\n\u017e\n0, 0, 14\n\n)\ndose s 0.75 grkg\n\u017e\n.\n0, 3, 7 , 1, 3, 11\n\u017e\n.\n0, 2, 9 , 0, 0, 12\n\u017e\n.\n0, 1, 11 , 0, 3, 10\n\u017e\n.\n0, 0, 15 , 0, 0, 11\n\u017e\n.\n2, 0, 8 , 0, 1, 10\n\u017e\n.\n0, 0, 10 , 0, 1, 13\n\u017e\n.\n0, 1, 9 , 0, 0, 14\n\u017e\n.\n1, 1, 11 , 0, 1, 9\n.\n\u017e\n0, 1, 10 , 0, 0, 15\n\u017e\n.\n0, 0, 15 , 0, 3, 10\n\u017e\n.\n0, 2, 5 , 0, 1, 11\n\u017e\n.\n0, 1, 6 , 1, 1, 8\n\ndose s 1.50 grkg\n\u017e\n.\n0, 8, 2 , 0, 6, 5\n\u017e\n.\n0, 5, 7 , 0, 11, 2\n\u017e\n.\n1, 6, 3 , 0, 7, 6\n\u017e\n.\n0, 0, 1 , 0, 3, 8\n\u017e\n.\n0, 8, 3 , 0, 2, 12\n\u017e\n.\n0, 1, 12 , 0, 10, 5\n\u017e\n.\n0, 5, 6 , 0, 1, 11\n\u017e\n.\n0, 3, 10 , 0, 0, 13\n.\n\u017e\n0, 6, 1 , 0, 2, 6\n\u017e\n.\n0, 1, 2 , 0, 0, 7\n.\n\u017e\n0, 4, 6 , 0, 0, 12\n\ndose s 3.00 grkg\n. \u017e\n\u017e\n.\n0, 4, 3 , 1, 9, 1\n. \u017e\n\u017e\n.\n0, 4, 8 , 1, 11, 0\n. \u017e\n\u017e\n.\n0, 7, 3 , 0, 9, 1\n. \u017e\n\u017e\n.\n0, 3, 1 , 0, 7, 0\n. \u017e\n\u017e\n.\n0, 1, 3 , 0, 12, 0\n. \u017e\n\u017e\n.\n2, 12, 0 , 0, 11, 3\n. \u017e\n\u017e\n.\n0, 5, 6 , 0, 4, 8\n. \u017e\n\u017e\n.\n0, 5, 7 , 2, 3, 9\n. \u017e\n\u017e\n.\n0, 9, 1 , 0, 0, 9\n. \u017e\n.\n\u017e\n0, 5, 4 , 0, 2, 5\n\u017e\n. \u017e\n.\n1, 3, 9 , 0, 2, 5\n.\n\u017e\n0, 1, 11\n\n. \u017e\n. \u017e\n. \u017e\n. \u017e\n. \u017e\n. \u017e\n. \u017e\n. \u017e\n. \u017e\n. \u017e\n. \u017e\n. \u017e\n\n. \u017e\n. \u017e\n. \u017e\n. \u017e\n. \u017e\n. \u017e\n. \u017e\n. \u017e\n. \u017e\n. \u017e\n. \u017e\n\nsource: study described by c. j. price, c. a. kimmel, r. w. tyl, and m. c. marr, toxicol. appl.\n.\npharmacol. 81: 113\u1390127 1985 .\n\n\u017e\n\n "}, {"Page_number": 534, "text": "multivariate random effects models for binary data\n\n519\n\ntable 12.10 comparisons of log likelihoods for multivariate random\neffects models for developmental toxicity study\n\nmodel\ndose-specific \u233a\n\u233a , common \u2423, \u2424\ncommon \u233a\ncommon \u233a, \u2433s 0\nunivariate \u2434\n\n2\n\ni\n\ni\n\nnumber of\nparameters\n\nchange in\nparameters\n\nchange in\n\nlog likelihood\n\n16\n14\n7\n6\n5\n\n\u138f\n2\n9\n10\n11\n\n\u138f\n28.4\n7.4\n7.4\n16.7\n\ntable 12.10 reports the change in the maximized log likelihood from fitting\nfour special cases of this model:\n\n1. common intercept and slope for the two logits: \u2423 s \u2423 and \u2424 s \u2424\n2. common covariance matrix for the four doses: \u233a s \u233a s \u233a s \u233a\n3. common covariance matrix and uncorrelated random effects\n4. univariate common variance component across dose: u s u\n\n1\n\n2\n\n1\n\n1\n\n2\n\n3\n\n4\n\n2\n\nand\n\ni\u017e d.1\n\ni\u017e d.2\n\n\u2434 s \u2434\nd\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\ntests of the first three special cases against the general model 12.17 can\nuse ordinary likelihood-ratio tests. little seems to be lost by using the simpler\nmodel having uncorrelated random effects with homogeneous covariance\nstructure i.e., the fourth model listed in table 12.10 , as the likelihood-ratio\nstatistic comparing this to model 12.17 equals 2 7.4 s 14.8 df s 10 . the\nmodel provides a separate univariate logistic-normal model for each condi-\ntional binomial outcome, specifying that the proportion of dead pups and the\nproportion of malformed pups given survival are independent, both within\nlitter and marginally.\n\n.\n.\n\nthe univariate model in table 12.10 is the special case of the third model\nlisted in which the variances are common for the two logits and the random\neffects are perfectly correlated. hence, it reduces to a univariate random\neffects model. comparing the univariate model to a multivariate counterpart\ninvolves testing that correlation parameters fall on the boundary. ordinary\nchi-squared asymptotic theory for likelihood-ratio tests applies only when the\nparameter falls in the interior of the parameter space. tests when a null\nmodel has a correlation of 1 or a variance component of 0 are complex and\nbeyond our scope here see section 12.6.6 . however, an informal analysis of\nchange in log likelihoods suggests that the univariate model is inadequate.\nthe ml estimated effects for the separate univariate logistic-normal\nmodel for each conditional binomial outcome are \u2424 s 0.08 se s 0.21 ,\n.\n\u2424 s 1.79 se s 0.22 . for a given cluster, there is no evidence of a dose\n\u02c6\n2\neffect on the death rate, but the estimated odds of malformation, given\nsurvival, multiply by exp 1.79 s 6.0 for every additional grkg of ethylene\n\n\u02c6\n1\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n "}, {"Page_number": 535, "text": "520\n\nrandom effects: generalized linear mixed models\n\nglycol. the variance component estimates suggest a stronger litter effect for\nthe malformation outcome given survival \u2434 s 1.6 than for death \u2434 s 0.5 .\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u02c6\n2\n\n\u02c6\n1\n\n)\n\n(\n\n12.5.3 hierarchical multilevel modeling\nhierarchical data structures, with units grouped at different\nlevels, are\ncommon in education. a statewide study of factors that affect student\nperformance might measure students\u2019 scores on a battery of exams but use a\nmodel that takes into account the student, the school or school district, and\nthe county. just as two observations on the same student might tend to be\nmore alike than observations on different students, so might two students in\nthe same school tend to be more alike than students from different schools.\nstudent, school, and county terms might be treated as random effects,\nwith different ones referring to different le\u00aeels of the model. for instance,\na model might have students at level 1, schools at level 2, and counties at\nlevel 3. glmms for data having a hierarchical grouping of this sort are called\nmultile\u00aeel models. random effects enter the model at each level of the\nhierarchy.\n\nwe illustrate with a two-level model. let \u2432 denote the probability that\nstudent i in school\nj passes test t in a battery of tests. a multilevel model\nwith random effects for student and school and fixed effects for explanatory\nvariables has the form\n\ni\u017e j.t\n\nw\n\nlogit \u2432 s x\n\ni\u017e j.t\n\nx\ni\u017e j.t\n\n\u2424 q u q \u00ae\n\nj\n\ni\u017e j.\n\n.\n\nx\n\nj\n\n\u0004\n\n4\n\n\u00ae\ni\u017e j.\n\nhere, the explanatory variables x might include one that identifies the test in\nthe battery. the random effects u for schools and \u00ae\nfor students within\nschools are independent with different variance components. the level 1\nrandom effects\naccount for variability among students in ability or\nparents\u2019 socioeconomic status or other characteristics not measured by x.\nwhen they have a relatively large variance component, there is a strong\ncorrelation among the test results for students. the level 2 random effects\n\u0004\n4u account for variability among schools due to possibly unmeasured factors\nj\nsuch as per-capita expenditure in the school\u2019s budget.\n\ni\u017e j.\n\nfor examples of the use of multivariate random effects in multilevel\nmodeling, see aitkin et al. 1981 , anderson and aitkin 1985 , gibbons and\n\u017e\nhedeker 1997 , goldstein 1995 , goldstein and rasbash 1996 , and long-\n.\nford 1993 .\n\n.\n.\n\n\u017e\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n12.6 glmm fitting, inference, and prediction\n\nmodel fitting is rather complex for glmms. the main difficulty is that the\nlikelihood function does not have a closed form. numerical methods for\napproximating it can be computationally intensive for models with multivari-\n\n "}, {"Page_number": 536, "text": "glmm fitting, inference, and prediction\n\n521\n\nate random effects. in this section we outline the basic ideas of ml fitting of\n\u017e\ne.g., proc\nglmms. some ml methods are available in software\n.\nnlmixed in sas .\n\n12.6.1 marginal likelihood and maximum likelihood fitting\nthe glmm is a two-stage model. at the first stage, conditional on the\nrandom effects, observations are assumed to follow a glm. that is, observa-\ntion y\nin cluster i has distribution in the exponential family with expected\nvalue \u242e linked to a linear predictor,\nit\n\nit\n\ng \u242e s xx \u2424 q zx u .\n\u017e\n\n.it\n\nit\n\nit\n\ni\n\ni\n\ni\n\nit\n\n4\n\n\u0004\n\n.\n\n\u017e\n\nthen, zx u is a known offset and observations in a cluster are independent.\nat the second stage, the random effects u are assumed independent from a\nn 0, \u233a distribution.\n\nfor a discrete variable, denote the vector of all the observations by y and\nthe vector of all the random effects by u. let f y u; \u2424 denote the condi-\n\u017e\nf u; \u233a denote the normal density\ntional mass function of y, given u. let\nfunction for u. the likelihood function ll \u2424, \u233a; y for a glmm is the\nprobability mass function f y; \u2424, \u233a of y, viewed as a function of \u2424 and \u233a.\nthis mass function refers to the marginal distribution of y after integrating\nout the random effects,\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n<\n\nll \u2424, \u233a ; y s f y; \u2424, \u233a s f y u; \u2424 f u; \u233a du.\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n<\n\nh\n\n\u017e\n\n12.18\n\n.\n\nit is often called a marginal likelihood. for example, the likelihood function\n\u017e\nll \u2424, \u2434 ; y for the logistic-normal model 12.5\n\nabsorbing \u2423 into \u2424 is\n\n. \u017e\n\n\u017e\n\n.\n\n.\n\n2\n\n\u017e\n\n\u0142\n\ni\n\n\u2b01\n\n\u0142h\n\ny\u2b01\n\nt\n\n\u017e\n\nexp x \u2424 q u\n\n.\n1 q exp x \u2424 q u\n\nx\nit\n\u017e\n\ni\n\nx\nit\n\ny\n\nit\n\n.\n\ni\n\n1 q exp x \u2424 q u\n\n\u017e\n\n1\nx\nit\n\n1yy\n\nit\n\n.\n\ni\n\n\u017e\nf u ; \u2434 du .\n\n.\n\n2\n\ni\n\ni\n\n/\n\nthe likelihood function is evaluated numerically and maximized as a function\nof \u2424 and \u233a. many methods have been developed to do this. we next discuss\na few of the most popular.\n\n12.6.2 gauss\u2013hermite quadrature methods\nthe integral determining the likelihood function has dimension that depends\non the random effects structure. when the dimension is small, as in the\none-dimensional integral above for the logistic-normal model 12.5 , standard\nnumerical integration methods can approximate the likelihood function.\n\n\u017e\n\n.\n\n "}, {"Page_number": 537, "text": "522\n\nrandom effects: generalized linear mixed models\n\n\u017e .\n\ngauss\u1390hermite quadrature is a method for approximating the integral of\na function f \u2b48 multiplied by another function having the shape of a normal\ndensity. the approximation is a finite weighted sum that evaluates the\nfunction at certain points. in the univariate normal random effects case,\nthe approximation has the form\n\n\u2b01\n\nh\n\ny\u2b01\n\nf u exp yu\n\u017e\n\n.\n\n\u017e\n\n2\n\n.\n\ndu f c f s\n\u017e\n\nk\n\nq\n\n\u00fd\nks1\n\n.\n\nk\n\n,\n\nk\n\nk\n\n4\n\n4\n\n\u0004\n\ns\n\n\u0004\nwith weights c\nmation improves as q, the number of quadrature points, increases.\n\nand quadrature points\n\nthat are tabulated. the approxi-\n\n\u02c6\n\n\u02c6\n\nthe approximated likelihood can be maximized with standard algorithms\nsuch as newton\u1390raphson, yielding ml estimates \u2424 and \u233a. inverting an\napproximation for the observed information matrix provides standard errors\nfor the ml estimates. for complex models, second partial derivatives for the\nhessian may be computed numerically rather than analytically. adequate\napproximation usually requires larger q for standard errors than for \u2424. we\nrecommend sequentially increasing q until the changes are negligible in both\nthe estimates and standard errors.\n\nan adaptive version of gauss\u1390hermite quadrature e.g., liu and pierce\n1994 centers the quadrature points with respect to the mode of the function\nbeing integrated and scales them according to the estimated curvature at the\nmode. this improves efficiency, dramatically reducing the number of quadra-\nture points needed to approximate the integrals effectively. lesaffre and\nspiessens 2001 showed comparisons and warned against using too few\npoints.\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n12.6.3 monte carlo methods\nmultivariate forms of gauss\u1390hermite quadrature handle multivariate, corre-\nlated random effects. adequate approximation becomes more difficult, how-\never, when the dimension of the integral exceeds roughly 5. then, monte\ncarlo methods are more feasible computationally than numerical integration.\n.\nvarious monte carlo approaches have been studied e.g., mcculloch 1997 ,\nincluding monte carlo in combination with newton\u1390raphson, monte carlo\nin combination with the em algorithm, and simulating the likelihood directly.\nhere, we briefly describe a monte carlo em mcem algorithm.\n\nthe em algorithm is a popular iterative method of finding ml estimates\nwhen data are missing or when filling in some \u2018\u2018missing\u2019\u2019 data simplifies a\nlikelihood dempster et al. 1977\nsee laird 1998 for a useful review . in\neach cycle an e-step takes an expectation over the missing data to approxi-\nmate the likelihood function and an m-step maximizes the likelihood given\nthe working values of the parameter estimates. in glmms, one regards the\nrandom effects u as missing data. then, h y, u; \u2424, \u233a s f y u; \u2424 f u; \u233a\n.\nspecifies the joint distribution of the complete data. the e-step in iteration r\n\n. w\n\n. \u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\nx\n\n<\n\n "}, {"Page_number": 538, "text": "glmm fitting, inference, and prediction\n\n523\n\nof the em algorithm calculates\n\n\u0004\ne log h y, u; \u2424, \u233a y; \u2424 , \u233a\n\n\u017e r .\n\n\u017e\n\n.\n\n<\n\n\u017e r .\n\n4\n\n.\n\n\u017e\n\n<\n\n.\n\n<\n\n<\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nthe expectation is with respect to the distribution of u y with parameter\nvalues equal to \u2424\u017e r . and \u233a\u017e r ., the working estimates for iteration r. the\ndistribution of u y follows from those of y u and u in the glmm via\nbayes\u2019 theorem. the m-step then maximizes the result with respect to \u2424 and\n\u233a to obtain \u2424\u017e rq1. and \u233a\u017e rq1..\n\nthe mcem algorithm approximates the expectation in the e-step using\nmonte carlo methods. possible ways of doing this include using independent\nsimulations from the distribution of u y , at the current estimate of parame-\nters, or using markov chain monte carlo mcmc . for details, including the\nissue of choosing an appropriate monte carlo sample size, see booth and\n.\nhobert 1999 , chan and kuk 1997 , and mcculloch 1994, 1997 .\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n<\n\n12.6.4 penalized quasi-likelihood approximation\nthe gauss\u1390hermite and monte carlo integration methods provide likeli-\nhood approximations such that resulting parameter estimates converge to the\nml estimates as they are applied more finely\ni.e., as the number of\nquadrature points increases for numerical\nintegration and as the monte\ncarlo sample size increases in the mcem method . this contrasts with other\napproximate methods that are simpler but need not yield estimates near the\nml estimates. these methods maximize an analytical approximation of the\nlikelihood function.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\njoint distribution,\n\nrecall that the likelihood function 12.18 results from integrating out the\nrandom effects u from the joint distribution of y and u. using the exponential\nfamily representation of each component of that\nthe\nintegrand of 12.18 is an exponential function of u. one approach approxi-\nmates that function using a second-order taylor series expansion of its\nexponent around a point u at which the first-order term equals 0. that point\nu f e u y . the approximating function for the integrand is then exponen-\n\u02dc\ntial with quadratic exponent in u y u and has the form of a constant\nmultiple of a multivariate normal density. thus, its integral has closed form.\nthis type of integral approximation is called a laplace approximation. the\napproximation for integral 12.18 is then treated as a likelihood and maxi-\nmized with respect to \u2424 and \u233a.\n\n. x\n\n\u02dc\n\n\u02dc\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nw\n\n\u017e\n\n.\n\n<\n\nfor one such method breslow and clayton 1993 , the integral approxima-\n\n\u017e\n\n.\n\ntion yields a function approximating the log likelihood that has the form\n\nq \u2424, y y 1r2 ux \u233ay1 u,\n\u017e\n\u02dc\n\n.\u02dc\n\n.\n\n\u017e\n\n "}, {"Page_number": 539, "text": "524\n\nrandom effects: generalized linear mixed models\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u02dc\n\nwhere q \u2424, y resembles a quasi-log-likelihood function for the glm condi-\ntional on u s u. thus, the approximation results in a penalty for the quasi-\nlog likelihood, with the penalty increasing as elements of u increase in\nabsolute value. this approach is called penalized quasi-likelihood pql . the\ncalculations for maximizing the penalized quasi-likelihood use methods\nfor linear mixed models with a normal response. this treats a linearization of\nthe logit as a working response and entails iterative solution of sets of\nlikelihood-like equations in \u2424 and u. pql methods do not require numerical\nor monte carlo integration and so are simpler than ml methods. they are\ncomputationally feasible for large data sets and models with complex random\neffects structure.\n\n\u02dc\n\nunfortunately, pql methods can perform poorly relative to ml mccul-\nloch 1997 . for instance, for the abortion example in section 12.3.2, the pql\napproximations to the ml estimates obtained using the glimmix macro in\nsas are decent for \u2424 , but the standard errors and the estimate of \u2434 are\nonly about half what they should be e.g., pql gives \u2434s 4.3, compared to\nthe ml estimate of 8.6 . when true variance components are large, ordinar-\nily pql tends to produce variance component estimates with substantial\nnegative bias breslow and lin 1995 . the pql estimators also behave poorly\nwhen the response distribution is far from normal e.g., binary . adjustments\nhave been developed for some cases to lessen the bias e.g., goldstein and\nrasbash 1996 , but where possible we recommend using ml rather than\npql.\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u0004\n\n4\n\nt\n\n.\n\n\u017e\n\n\u017e\n\n12.6.5 bayesian approaches\nanother approach to fitting of glmms is bayesian. with it, the distinction\nbetween fixed and random effects no longer occurs, as every effect has a\nprobability distribution. use of a flat prior distribution yields a posterior that\nis a constant multiple of the likelihood function. then, markov chain monte\ncarlo mcmc methods for approximating intractable posterior distributions\ncan approximate the likelihood function zeger and karim 1991 . for in-\nstance, an approximation for the mode of the posterior distribution approxi-\nmates the ml estimate.\n\na danger is that improper prior distributions have improper posteriors for\nmany models for categorical data natarajan and mcculloch 1995 . in using\nmcmc, one may fail to realize that the posterior is improper. it is safer to\nuse a proper but relatively diffuse prior. however, the posterior mode need\nnot be close to the ml estimate, and markov chains may converge slowly\n\u017e\nnatarajan and mcculloch 1998 . this is currently an active area of research,\nnot just as a way of approximating ml results but also as an approach\npreferred over ml by those who adopt the bayesian paradigm. see, for\ninstance, daniels and gatsonis 1999 for multilevel modeling of geographic\nand temporal trends with clustered longitudinal binary data, which built on\n.\nearlier hierarchical modeling by wong and mason 1985 .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n "}, {"Page_number": 540, "text": "glmm fitting, inference, and prediction\n\n525\n\ninference for model parameters\n\n12.6.6\nafter fitting the model, inference about fixed effects proceeds in the usual\nway. for instance, likelihood-ratio tests can compare nested models. asymp-\ntotics for glmms apply as the number of clusters increases, rather than as\nthe numbers of observations within the clusters increase. similarly, resam-\npling methods such as the bootstrap using a large number of clusters should\nsample clusters rather than individual observations within clusters, to pre-\nserve the within-cluster dependence.\n\u017e\n\ninference about random effects e.g., their variance components is more\ncomplex. for instance, sometimes one model is a special case of another in\nwhich a variance component equals 0. the simpler model then falls on the\nboundary of the parameter space relative to the more complex model, so\nordinary likelihood-based inference does not apply. the asymptotic distribu-\ntion of the likelihood-ratio statistic is known for the most common situation,\ntesting h : \u24342 s 0 against h : \u24342 ) 0 for a model containing a single\nvariance component. the null distribution is an equal mixture of \u2439 i.e.,\ndegenerate at 0 and \u2439 random variables self and liang 1987 . the value\nof 0 occurs when \u2434s 0,\nin which case the maximized likelihoods are\nidentical under h and h . when \u2434) 0 and the observed test statistic\na\nequals t, the p-value for this large-sample test is p \u2439 ) t , half the\np-value that applies for \u24392 asymptotic tests. for testing more than one\nvariance component, the mixture distribution becomes more complex, and it\n.\nis simpler to use a score test lin 1997 .\n\n2 \u017e\n0\n\n\u02c6\n\n\u02c6\n\n2\n1\n\n2\n1\n\n1\n2\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\na\n\n0\n\n0\n\n1\n\n12.6.7 prediction using random effects\nthe use of random effects in a model implies heterogeneity of certain effects\nof interest, such as odds ratios. estimated effects of interest are often then\nlinear combinations of fixed and random effects. for example, in the clinical\ntrial comparing two treatments with random effects for centers section\n12.3.4 , one can predict the probability of success for each treatment in each\ncenter and odds ratios in those centers.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\ngiven the data, the conditional distribution of u y contains the informa-\ntion about the random effects u. a prediction for u is e u y , its posterior\nmean given the data. calculation of e u y itself requires numerical integra-\ntion or monte carlo approximation. the expectation depends on \u2424 and \u233a,\nso in practice one substitutes \u2424 and \u233a in the approximation. the standard\nerror of the predictor of the random effect u is the standard deviation of the\ndistribution of u y . when one substitutes \u2424 and \u233a in e u y , however,\nthe standard error does not account for the sampling variability in those\nestimates. hence, the true standard error tends to be underestimated booth\n.\nand hobert 1998 .\n\ni \u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\nthis approach to prediction using posterior means of random effects\nprovides effect estimates that exhibit shrinkage relative to estimates using\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\ni\n\n<\n\n<\n\n<\n\n<\n\n<\n\n "}, {"Page_number": 541, "text": "526\n\nrandom effects: generalized linear mixed models\n\n.\n\n\u017e\n\nonly data in the specific cluster. in this sense the results are similar to those\nusing an empirical bayes approach ten have and localio 1999 . this adapts\nan ordinary bayesian analysis by using the sample data to estimate parame-\nters of the prior distribution. for a vector of mean parameters, this approach\nyields an estimate of a particular mean that is a weighted average of the\nsample mean and the overall mean of the sample means. thus, it shrinks the\nsample mean toward the overall mean. shrinkage estimators can be far\nsuperior to sample values when the sample size for estimating each parame-\nter is small, when there are many parameters to estimate, or when the true\nparameter values are roughly equal. the empirical bayes paradigm has been\nin use for some time: for instance, for estimating a vector of means or\n.\nbinomial proportions efron and morris 1975 .\n\nalthough random effects models are natural in many applications, further\nwork is needed. work continues on the development of methodology for\nmodel-fitting and inference with complex glmms. in addition, research is\nneeded on model checking and diagnostics. nonetheless, we believe that\nglmms provide a very useful extension of ordinary glms.\n\n\u017e\n\nnotes\n\nsection 12.1: random effects modeling of clustered categorical data\n\n.\n\n\u017e\n\n12.1. for further discussion of the rasch model and ways of estimating its parameters, see\n.\nandersen 1980, sec. 6.4 and fischer and molenaar 1995 . haberman 1977b\nshowed ml estimators can achieve consistency when both n and t grow at suitable\nrates. for multinomial rasch extensions, see andersen 1980, pp. 272\u1390284; 1995 and\nconaway 1989 . early work on random effects models for a categorical response\n.\nincludes anderson and aitkin 1985 , bartholomew 1980 , bock and aitkin 1981 ,\nchamberlain 1980 , gilmour et al. 1985 , pierce and sands 1975 , and stiratelli et\n.\nal. 1984 .\n\n.\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n12.2. in models with covariates, neuhaus and lesperance 1996 noted that conditional ml\nmay lose considerable efficiency compared to the random effects approach when\ncluster sizes are small and covariates have strong positive within-cluster correlation.\nas that correlation approaches q1, the covariate effect resembles a between-cluster\none, which the conditional ml approach cannot estimate. the matched-pairs case\nreferred to in section 12.1.2 in which the conditional ml estimate equals the random\neffects estimate has within-cluster covariate correlation s y1, as depending on the\norder of viewing the observations, x changes from 0 to 1 or from 1 to 0; then, no\nefficiency loss occurs.\n\nt\n\nsection 12.3: examples of random effects models for binary data\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n12.3. for further discussion of modeling capture\u1390recapture data, see bishop et al. 1975,\nchap. 6 , chao et al. 2001 , cormack 1989 , coull and agresti 1999 , darroch et al.\n\u017e\n1993 , fienberg et al. 1999 , and hook and regal 1995 . similarities exist between\nthis problem and the related problem of estimating the binomial\nindex n when\nobserving independent bin n, \u2432 counts with unknown n and \u2432; see aitkin and\nstasinopoulos 1989 and references therein. relatively flat log likelihoods also occur\n.\nwith other models that permit capture heterogeneity burnham and overton 1978 ,\nsuch as a beta-binomial model.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n "}, {"Page_number": 542, "text": "problems\n\n527\n\n\u017e\n\n.\n\n12.4. king 1997 used random effects models as part of a solution for analyzing aggregated\n.\ncategorical data, the problem of ecological\ninference. chambers and steel 2001\ndiscussed early work by leo goodman on this problem and proposed a simpler\nsemiparametric approach.\n\n\u017e\n\nsection 12.4: random effects models for multinomial data\n\n12.5. with the complementary log-log link, the likelihood function has closed form with a\nlog gamma random effects distribution crouchley 1995, farewell 1982, ten have\n.\n1996 .\n\n\u017e\n\n12.6. chen and kuo 2001 discussed nominal responses, including discrete choice models\n\u017e\nsec. 7.6 with random effects. see also brownstone and train 1999 for discrete\nchoice glmms.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nsection 12.5: multi\u00a9ariate random effects models for binary data\n\n\u017e\n\n.\n\n12.7. rabe-hesketh and skrondal 2001 showed that careful attention must be paid to\nparameter identification in models with multivariate random effects. their factor\nmodel contains many multivariate random effects models as special cases.\n.\n\n12.8. for longitudinal bivariate binary responses, ten have and morabia 1999 simultane-\nously modeled bivariate log odds ratios and univariate logits. multivariate responses\nsometimes have both continuous and categorical components. for random effects\nmodeling of such data, see catalano and ryan 1992 and gueorguieva and agresti\n.\n\u017e\n2001 .\n\n\u017e\n\n.\n\n\u017e\n\nsection 12.6: glmm fitting, inference, and prediction\n\n\u017e\n\n12.9. see fahrmeir and tutz 2001, chap. 7 and mcculloch and searle 2001 for more\ndetails on the fitting of glmms. just as the likelihood function for a glmm is an\nintegral, so do likelihood equations have the form of integral equations mcculloch\nand searle 2001, p. 227 . wolfinger and o\u2019connell 1993 described a fitting method\nrelated to pql, also motivated by a laplace approximation.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n12.10. a glmm determines the marginal relationship averaged over random effects\n.\nbetween the mean response and explanatory variables. conversely, heagerty 1999\nnoted that a marginal model for the mean implicitly determines the form of the fixed\n.\nportion of the linear predictor in a conditional model. the conditional glmm 12.1\nhas linear predictor, xx \u2424 q zx u . a more general form \u232c q zx u implies a particu-\nlar marginal model. here, \u232c is a function of the marginal linear predictor and the\nrandom effects distribution. it is implicitly defined by the integral equation that links\nthe marginal and conditional means.\n\n\u017e\n\n\u017e\n\nit\n\nit\n\nit\n\nit\n\nit\n\ni\n\ni\n\nproblems\n\napplications\n\n12.1 refer to the matched-pairs data of table 10.14 and problem 10.1.\n\n\u017e\n\n.\n\na. fit model 12.3 . interpret \u2424. if your software uses numerical\nintegration, report \u2424, \u2434, and their standard errors for 5, 10, 25,\n100, and 200 quadrature points, and comment on convergence.\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n "}, {"Page_number": 543, "text": "528\n\nrandom effects: generalized linear mixed models\n\nb. compare \u2424 and its se for this approach to the conditional ml\n\n\u02c6\n\napproach.\n\n12.2 refer to table 4.8 on the free-throw shooting of shaq oxneal. in\ngame i, suppose that y s number made out of n attempts is a\nbin n ,\u2432 variate and y are independent.\na. fit the model,\n\nlogit \u2432 s \u2423. find and interpret \u2432. does the\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n4\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u02c6\n\ni\n\ni\n\nmodel appear to fit adequately?\n\nb. fit the model,\n\u02c6\n\nare independent\n\u017e\nn 0,\u2434 . use \u2423 and \u2434 to summarize o neal\u2019s free-throw shoot-\ning.\n\nlogit \u2432 s \u2423q u , where u\n\u0004\n\n2.\n\n\u02c6\n\n\u017e\n\n.\n\n4\n\nx\n\ni\n\ni\n\ni\n\nc. explain how the model in part a is a special case of that in part\n\n\u017e .\nb . is there evidence that the one in part b fits better?\n\n\u017e .\n\n\u017e .\n\n12.3 for table 8.3, let y s 1 when subject i used substance t. table\n\nit\n\n12.11 shows output for the logistic-normal model\nlogit p y s 1 u s u q \u2424 .\n\n<\n\n\u017e\n\n.\n\ni\n\nit\n\nt\n\ni\n\ninterpret. illustrate by comparing use of cigarettes and marijuana.\n\ntable 12.11 output for problem 12.3\n\ndescription\nsubjects\nmax obs per subject\nparameters\nquadrature points\nlog likelihood\n\nvalue\n2276\n3\n4\n200\ny3311\n\nparameter\nbeta1\nbeta2\nbeta3\nsigma\n\nestimate\n4.2227\n1.6209\ny0.7751\n3.5496\n\nstd\n\nerror\n0.1824\n0.1207\n0.1061\n0.1627\n\nt value\n23.15\n13.43\ny7.31\n21.82\n\n12.4 how is the focus different for the model in problem 12.3 than for the\nloglinear model ac, am, cm used in section 8.2.4? if \u2434s 0,\nwhich loglinear model has the same fit as the glmm?\n\n\u02c6\n\n\u017e\n\n.\n\n12.5 for the student survey in table 9.1, a analyze using glmms, and\n\u017e .b compare results and interpretations to those with marginal models\nin problem 11.2.\n\n\u017e .\n\n\u017e\n\n.\n\n12.6 fit model 12.10 to the responses on abortion. if your software uses\ngauss\u1390hermite quadrature,\nthe approximate number of\nquadrature points needed for parameter estimates to converge and\nthe number needed for standard error estimates to converge. this\nexample has large \u2434 and requires many points.\n\nreport\n\n\u017e\n\n.\n\n\u02c6\n\n "}, {"Page_number": 544, "text": "problems\n\n529\n\n12.7 for the crossover study in table 11.10 problem 11.6 , fit the model\n\n\u017e\n\n.\n\nlogit p y s 1 u\n<\n\ni\u017e k.t\n\n\u017e\n\n.\n\ni\u017e k.\n\ns \u2423 q \u2424 q u\n\nk\n\nt\n\n,\n\ni\u017e k.\n\n\u017e\n\n12.19\n\n.\n\n\u0004\nwhere u\n\n4\n\ni\u017e k.\n\nare independent n 0, \u2434 . interpret \u2424 and \u2434.\u02c6\n\n2\n\n\u017e\n\n.\n\n\u0004\n\n\u02c6\nt\n\n4\n\n12.8 for problem 12.7, compare estimates of \u2424 y \u2424 and \u2424 y \u2424 and\n\u017e .\nse values to those using a a marginal model problem 11.6 , and b\nconditional logistic regression section 10.2 , treating subject terms in\nmodel 12.19 as fixed effects.\n\n\u017e .\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\nc\n\na\n\na\n\nb\n\n4\n\n\u0004\n\ntk\n\n12.9 for problem 12.7, fit the more general glmm having treatment\neffects \u2424 that vary by sequence. test whether the fit is better. one\ncould also consider period or carryover effects. add two period\neffects to model 12.19 e.g., the first-period-effect parameter adds to\nthe model when t s a and k s 1, 2, t s b and k s 3, 4, and t s c\nand k s 5, 6 . check whether the fit improves. interpret.\n\n. \u017e\n\n\u017e\n\n.\n\n12.10 consider the logistic-normal model 12.10 for the abortion opinion\n\ndata, under the constraint \u2434s 0.\na. explain why the fit is the same as an ordinary logit model treating\nthe three responses for each subject as if they were independent\nresponses for three separate subjects.\n\n\u017e\n\n.\n\n\u017e\n\nb. explain why the model fit is the same as an ordinary loglinear\nmodel gi , gi , gi of mutual independence of responses on the\n1\nthree items\n\n.\n, given g s gender.\n.\n\nc. fit the model. interpret, and explain why \u2424 y \u2424 are quite\n\n\u02c6\nu\ndifferent from those in section 12.3.2 allowing \u2434) 0.\n\n3\ni , i , i\n1\n\n\u02c6\nt\n\n\u017e\n\n\u0004\n\n4\n\n2\n\n2\n\n3\n\n12.11 for table 6.7 on admissions decisions for graduate school applicants,\nlet y s 1 denote a subject in department i of gender g 1 s females,\n0 s males being admitted.\na. for the fixed effects model,\n\nlogit p y s 1 s \u2423q \u2424g q \u2424 ,\n\n.x\n\ni g\n\n.\n\n\u017e\n\n\u017e\n\nw\n\nd\n\ni g\n\ni\n\n\u2424s 0.173 se s 0.112 . interpret.\n\u02c6\n.\n\n\u017e\n\n.\n\nb. the corresponding model 12.12 in which departments are a\n\nnormal random effect has \u2424s 0.163 se s 0.111 . interpret.\n\n\u017e\n\n.\n\nc. the model of form 12.12 allowing the gender effect to vary by\ndepartment has \u2424s 0.176 se s 0.132 , with \u2434 s 0.20. inter-\npret. explain why the standard error of \u2424 is slightly larger than\nwith the other analyses.\n\n\u02c6b\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\u02c6\n.\n\nd. the marginal sample log odds ratio between gender and whether\nadmitted equals y0.07. how could this take different sign from \u2424\n\u02c6\nin these models?\n\n "}, {"Page_number": 545, "text": "530\n\nrandom effects: generalized linear mixed models\n\ne. the sample conditional odds ratios between gender and whether\nadmitted vary between 0 and \u2b01. by contrast, predicted odds ratios\nfor the interaction random effects model do not vary much. ex-\nplain why results can be so different.\n\n12.12 for the clinical trial in table 9.16, let \u2432 s p y s 1 u denote the\n\n\u017e\n\n.\n\n<\n\ni\n\nit\n\n.\n\n\u017e\n\n\u02c6\n\nit\nprobability of success for treatment t in center i.\na. the random intercept model 12.11 has \u2424s 1.52 se s 0.70 and\n\n\u2434s 1.9. interpret.\n\u02c6\nb. from section 9.8.3, the fixed effects analog of this model replac-\ning \u2423q u by \u2423 has \u2423 s \u2423 s y\u2b01, corresponding to \u2432 s \u2432\n\u02c6\n3t\ns 0 for each treatment. by contrast, the random effects model has\n\u2423q u s y3.78 using nlmixed in sas and \u2432 s 0.047 and\n\u02c6\n\u2432 s 0.011 in center 1. explain how this model can have \u2432 ) 0\n\u02c6\nin centers having no successes.\n\n\u02c6\n1\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n11\n\n12\n\n1t\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nit\n\n1\n\n3\n\ni\n\ni\n\n12.13 refer to the subject-specific model in section 12.3.3. verify that the\nestimated difference in time effect slopes between the new and\nstandard drugs for treating depression are a 1.018 se s 0.192 with\nthe glmm approach, and b 1.156 se s 0.222 with conditional\nml.\n\n\u017e .\n\n\u017e .\n\n\u017e\n.\n\n\u017e\n\n.\n\n12.14 for marginal model 10.14 for table 10.5 on premarital and extra-\nmarital sex, table 12.12 shows results of fitting a corresponding\nrandom intercept model. interpret \u2424. compare estimates of and\ninferences about \u2424 to those in section 10.3.2 for the marginal model.\n\n\u02c6\n\n\u017e\n\n.\n\ntable 12.12 output for problem 12.14\n\nsubjects\nmax obs per subject\nparameters\nquadrature points\nlog likelihood\n\n475\n2\n5\n100\ny890.1\n\nparameter\ninter1\ninter2\ninter3\nbeta\nsigma\n\nestimate\ny1.5422\ny0.6682\n0.9273\n4.1342\n2.0757\n\nstd\n\nerror\nt value\n0.1826 y8.45\n0.1578 y4.24\n0.1673\n5.54\n12.54\n0.3296\n0.2487\n8.35\n\n\u017e\n\n12.15 a data set from the 1994 general social survey on subjects\u2019 opinions\n.\non four items the environment, health, law enforcement, education\nrelated to whether they believed government spending on each item\nshould increase, stay the same, or decrease. subjects were also\nlet g s 1 for\nclassified by their gender and race. for subject\nlet r s 1 for whites and 0 otherwise,\nfemales and 0 for males,\n\ni,\n\ni\n\n1i\n\n "}, {"Page_number": 546, "text": "problems\n\n2 i\n\n531\nr s 1 for blacks and 0 otherwise, and r s r s 0 for the other\ncategory of race. let\ni on\n\u017e\nspending item t, where outcomes 1, 2, 3 represent\nincrease, stay the\n.\nsame, decrease .\na. with constraint \u2424 s 0, the random-intercept model\n\ndenote the response for subject\n\ny\n\n2 i\n\n1i\n\n\u017e\n\n.\n\nit\n\n4\nlogit p y f j u\n<\n\n\u017e\n\nit\n\n.\n\ni\n\nj\n\ns \u2423 q \u2424 q \u2424 g q \u2424 r q \u2424 r q u ,\ni\n\u02c6\n\nhas \u2424 s y0.55, \u2424 s y0.60, \u2424 s y0.49, with \u2434s 1.03. these\nestimates are greater than five standard errors in absolute value.\ninterpret.\n\nj s 1, 2,\n\nr1\n\u02c6\n3\n\nt\n\u02c6\n2\n\n\u02c6\n1\n\nr 2\n\n2 i\n\ni1\n\ng\n\ni\n\nb. table 12.13 shows results with a race-by-item interaction. inter-\n\npret.\n\ntable 12.13 results for problem 12.15 a\n\nse\nvariable\n0.391\nintercept-1\n0.051\nintercept-2\n0.088\ngender\n0.397\nrace1-w\n0.452\nrace2-b\n0.539\nitem1-envir\n0.493\nitem2-health\n0.480\nitem3-crime\nrace1)item1\n0.549\nrace1)item2\n0.503\nrace1)item3\n0.491\nrace2)item1\n0.606\nrace2)item2\n0.598\nrace2)item3\n0.560\n.\ncoding 0 for item 4 education and race 3 other .\n\nestimate\n1.065\n1.919\n0.409\ny0.055\n0.434\ny0.357\ny0.319\ny0.585\ny0.170\ny0.387\n0.197\ny0.452\n0.454\ny0.518\n\n\u017e\n\n.\n\n\u017e\n\na\n\n12.16 refer to problem 11.12 for table 8.19 on government spending.\nanalyze these data using a cumulative logit model with random\neffects. interpret. compare results to those with a marginal model\n.\n\u017e\nproblem 11.12 .\n\n12.17 for the insomnia example in section 12.4.2, according to sas the\nmaximized log likelihood equals y593.0, compared to y621.0 for the\nsimpler model forcing \u2434s 0. compare models, using either a likeli-\nhood-ratio test or aic. what do you conclude?\n\n "}, {"Page_number": 547, "text": "532\n\nrandom effects: generalized linear mixed models\n\ntable 12.14 results for problem 12.18\n\nobserver\n\neffect\n\na\nb\nc\nd\ne\nf\n\ngee\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\ny0.451 0.108\n.\ny0.391 0.093\n.\n.\n0.319 0.118\n.\n0.632 0.105\ny0.491 0.098\n.\n.\n1.252 0.161\n\nrandom\neffects\n\ny1.201 0.300\ny0.919 0.299\n0.558 0.301\n1.545 0.313\ny1.379 0.300\n2.907 0.344\n\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\u017e\n\n.\n.\n.\n.\n.\n.\n\n\u017e\n\n.\n\n12.18 landis and koch 1977 showed ratings by seven pathologists who\nseparately classified 118 slides regarding the presence and extent of\ncarcinoma of the uterine cervix, using a five-point ordinal scale.\n\u017etable 13.1 is a collapsing of their table that combines the first two\n.\ncategories and the last three categories. for slide i with rater t,\ntable 12.14 shows results of fitting model\n\nlogit p y f j u s u q \u2423 q \u2424\n\n<\n\n\u017e\n\n.\n\nit\n\nt\n\ni\n\ni\n\nj\n\n\u017e\n\n.\n\n\u017e\n\n2.\n\n\u02c6\ng\n\nto the ordinal table with \u2424 s 0 , assuming that the u are inde-\npendent n 0, \u2434 . it also shows gee estimates, using independence\nworking equations, for the corresponding marginal model. interpret\n\u02c6\u2424 for each model. explain why estimates using the random effects\nf\nmodel, for which \u2434s 3.8, tend to be much larger in absolute value.\ndiscuss the differences in assumptions and interpretations for the two\nmodels.\n\n\u02c6\n\n\u0004\n\n4\n\ni\n\n12.19 refer to section 12.5.1 on boys\u2019 attitudes toward the leading crowd.\ntable 12.15 shows results for a sample of schoolgirls. fit model\n\u017e\n12.16 and interpret. summarize the estimated variability and corre-\nlation of random effects.\n\n.\n\ntable 12.15 data for problem 12.19\n\na\n\n.\n\n\u017e\nyes, positive\n\n.m, a for second interview\n\u017e\n\u017e\n.\n\u017e\nno, positive\nyes, negative\n\n\u017e\n.m, a for\nfirst interview\n484\nyes, positive\n112\nyes, negative\n129\nno, positive\nno, negative\n74\nam, membership; a, attitude.\nsource: j. s. coleman, introduction to mathematical sociology london: free press of glencoe,\n1964 , p. 168.\n\n\u017e\n.\nno, negative\n\n107\n30\n768\n303\n\n32\n46\n321\n536\n\n93\n110\n40\n75\n\n.\n\n.\n\n\u017e\n\n "}, {"Page_number": 548, "text": "problems\n\n533\n\n\u017e\n\n.\n\n12.20 generalize model 12.16 to apply simultaneously to tables 12.8 and\n12.15, using a gender main effect but the same membership effect\nand the same attitude effect for each gender. fit the model. use the\nmaximized log likelihood to compare with a more general model\nhaving different membership effects and different attitude effects for\neach gender. interpret.\n\n12.21 table 12.16 reports results from a study to estimate the number n of\npeople infected during a 1995 hepatitis a outbreak in taiwan. the\n271 observed cases were reported from records based on a serum test\ntaken by the institute of preventive medicine of taiwan p , records\nreported by the national quarantine service q , and records based\non questionnaires administered by epidemiologists e . estimating n\nis difficult, because many subjects had only one capture.\n\u017e .\na. find n if you observed only i p and q,\nii p and e,\n\n\u017e\niii q and\n\n\u017e .\n\n\u017e .\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\ne.\n\n\u02c6\n\n\u017e .\npart b .\n\nb. find n using the model of mutual independence with p, q, and e.\nc. find a 95% profile likelihood interval for n using the model in\n\n\u02c6\n\nd. the random effects model of section 12.3.6 has fit shown in table\n12.16, for which \u2434s 2.9. the log-likelihood is relatively flat, and\nn s 4551 with a 95% profile likelihood interval of 758, \u2b01 coull\n\u02c6\nand agresti 1999 . explain why this model may provide imprecise\nestimates of n. since the interval in part c is much narrower, is it\nnecessarily more reliable?\n\n\u017e .\n\n. \u017e\n\n.\n\n\u017e\n\ntable 12.16 data for problem 12.21\n\np q e\n0 0 0\n0 0 1\n0 1 0\n0 1 1\n1 0 0\n1 0 1\n1 1 0\n1 1 1\n\nobserved\n\ncount\n\n\u138f\n63\n55\n18\n69\n17\n21\n28\n\nlogistic-normal\n\nml fit\n\u017e\n.\n487, \u2b01\n61.0\n58.0\n17.0\n68.0\n20.0\n19.0\n28.0\n\n.\nsource: data from chao et al. 2001 .\n\n\u017e\n\n12.22 analyze the crossover data of table 11.1 using a random effects\napproach. interpret, and compare results to those in section 11.1.2.\n\n "}, {"Page_number": 549, "text": "534\n\nrandom effects: generalized linear mixed models\n\n12.23 the analyses in section 12.3.2 comparing opinions on some topic\nextend to ordinal responses. using an ordinal random effects model,\nanalyze the 4 table in agresti 1993 , found also at the book\u2019s web\nsite, www.stat.ufl.edur; aarcdarcda.html.\n\n\u017e\n\n.\n\n3\n\n12.24 the analyses in section 12.3.4 describing heterogeneity in multicenter\ntrials extend to ordinal responses. using random effects\n\nclinical\n.\nmodels, analyze the 2 = 3 = 8 table in hartzel et al. 2001a .\n\n\u017e\n\n12.25 you are a statistical consultant asked to analyze table 4 in b. efron,\nstatistical science 13: 95\u1390122 1998 , which shows 2 = 2 tables from a\nclinical trial in 41 cities. analyze, and write a report summarizing\nyour analysis.\n\n\u017e\n\n.\n\n12.26 analyze table 11.9 with age and maternal smoking as predictors\nusing a a logistic-normal model, b marginal model, and c transi-\ntional model. explain how the interpretation of the maternal smoking\neffect differs for the three approaches.\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\ntheory and methods\n\n12.27 refer to section 12.3.1. using supplementary information improves\npredictions. let q denote the true proportion of votes for clinton in\nstate i in the 1992 election, conditional on voting for him or bush.\nconsider the model\n\ni\n\nlogit p y s 1 u s logit q q \u2423q u ,\n\n\u017e\n\n.\n\n<\n\n\u017e\n\n.\n\ni\n\nit\n\ni\n\ni\n\ni\n\n4\n\n\u0004\n\n4\n\nq\n\n\u0004\nare known and u\nw\n\nare independent n 0,\u2434 . when\nwhere\n\u2434s 0, show \u2432 s q exp \u2423r 1 y q q q exp \u2423 . compared to q ,\n4\n\u02c6\ni\nexplain how \u2432 then shifts up or down depending on how the overall\ndemocratic vote compares in the current poll to the previous election\ni.e., depending on \u2423 . when also \u2423s 0, show \u2432 s q .\n\u017e\n\n\u017e .x\n\u02c6\n\n\u02c6\n\u02c6i\n\n\u017e .\n\u02c6\n\n2.\n\n.\n\n\u017e\n\n\u0004\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u02c6\n\n\u02c6\n\n\u02c6i\n\ni\n\n12.28 for a binary response, consider the random effects model\n\nlogit p y s 1 u s \u2423q \u2424 q u ,\n\n<\n\n\u017e\n\n.\n\nit\n\ni\n\ni\n\nt s 1, . . . , t ,\n\nwhere u are independent n 0, \u2434 , and the marginal model\n\n\u017e\n\n2.\n\n\u0004\n\n4\n\ni\n\nlogit p y s 1 s \u2423q \u2424* ,\n\n\u017e\n\n.\n\nt\n\nt s 1, . . . , t .\n\nfor identifiability, \u2424 s \u2424* s 0. explain why all \u2424 s 0 implies that\nall \u2424* s 0. is the converse true?\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\n "}, {"Page_number": 550, "text": "problems\n\n535\n\n12.29 the glmm for binary data using probit link function is\n\ny1\n\n\u233d\n\n<\n\nit\n\n.\n\np y s 1 u s x \u2424 q z u ,\n\u017e\n.\n\nx\nit\n\nx\nit\n\n\u017e\n\n.\n\ni\n\ni\n\n.\nwhere \u233d is the n 0, 1 cdf and u has n 0, \u233a pdf, f u ; \u233a .\ni\na. show that the marginal mean is\n\n\u017e\n\n\u017e\n\ni\n\np y s 1 s p z y zx u f xx \u2424 f u ; \u233a du ,\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nit\n\ni\n\nit\n\nt\n\ni\n\ni\n\nh\n\nwhere z is a standard normal variate that is independent of u .i\ndistribution, deduce that\n\nb. since z y z u has a n 0, 1 q z \u233az\nw\n\np y s 1 s x \u2424 1 q z \u233az\n\u017e\n\nx\n\ny1r2\n\n.\n\nit\n\ny1\n\nx\nit\n\nx\nit\n\nx\nit\n\nx\nit\n\n\u233d\n\n.\n\n\u017e\n\n.\n\nit\n\nt\n\ni\n\nhence, the marginal model\nis a probit model with attenuated\neffect. in the univariate random intercept case, show the marginal\neffect equals that from the glmm divided by\n\n'\n1 q \u2434 .\n\n2\n\nw\n\n.x\n\n\u017e\n\n12.30 in the rasch model, logit p y s 1 s \u2423 q \u2424, \u2423 is a fixed effect.\na. assuming independence of responses for different subjects and for\ndifferent observations on the same subject, show that the log\nlikelihood is\n\u00fd \u00fd\n\nlog 1 q exp \u2423 q \u2424 .\n\n\u2423 y q\n\n\u2424 y y\n\n\u00fd \u00fd\n\n\u00fd \u00fd\n\n\u017e\n\n.\n\nit\n\nt\n\nit\n\nit\n\nt\n\nt\n\ni\n\ni\n\ni\n\ni\n\ni\n\nt\n\ni\n\nt\n\ni\n\nt\n\nb. show that the likelihood equations are y s \u00fd p y s 1 and\ni and t. explain why conditioning on\n\nqt\n\n.\n\n\u017e\n\n.\n\nit\n\ni\n\n4\nyields a distribution that does not depend on \u2423 .\n\n\u0004\n\ny s \u00fd p y s 1 for all\niq\n\u0004\ny\niq\n\n\u017e\n\n4\n\nit\n\nt\n\nc. discuss advantages and disadvantages of, instead, treating \u2423 as\n\ni\n\nrandom.\n\n0\n\n\u017e\n\n12.31 consider the matched-pairs random effects model 12.3 . for given\n\u2424 , let \u2426 be such that \u242e s n q \u2426 and \u242e s n y \u2426 satisfies\nlog \u242e r\u242e s \u2424 . suppose \u242e has nonnegative log odds ratio.\n.\n12\nexplain why:\na. this is the fit of the model assuming \u2424s \u2424 .0\nb. the likelihood-ratio statistic for testing h : \u2424s \u2424 in this model\n\n12\n4\n\u02c6\n\n0\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n12\n\n21\n\n21\n\n21\n\n\u0004\n\ni j\n\n0\n\n0\n\n0\n\n0\n\n0\n\n\u017e\n\nequals\n\ni\n\n.\n\n\u017e\n\n2 n log\n\n12\n\nn\n\n12\n\nn q \u2426\n\n12\n\n0\n\nq n log\n\n21\n\nn\n\n21\n\nn y \u2426\n\n21\n\n0\n\n/\n\n.\n\nc. the likelihood-ratio test of h : \u2424s 0 is the test of symmetry.\n\n0\n\n "}, {"Page_number": 551, "text": "536\n\nrandom effects: generalized linear mixed models\n\n12.32 explain why the logistic-normal model is not helpful for capture\u1390\n\nrecapture experiments with only two captures.\n\n12.33 refer to the crossover study in problem 12.7. kenward and jones\n\u017e\n1991 reported results using the ordinal response scale none, moder-\nate, complete for relief. explain how to formulate an ordinal logit\n.\nrandom effects model for these data analogous to model 12.19 .\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n12.34 formulate a model using adjacent-categories logits that is analogous\n\nto model 12.14 for cumulative logits. interpret parameters.\n\n\u017e\n\n.\n\n12.35 for ordinal square i = i tables of counts n\n\nbinary matched-pairs responses y , y\ni2\n\ni1\n\n\u017e\n\n\u0004\n\n4\n\nab\n\n, model 12.3 for\n\n\u017e\n\n.\n\n.\n\nfor subject i extends to\n\nlogit p y f j u s \u2423 q \u2424x q u\n\n<\n\n\u017e\n\n.\n\nit\n\nt\n\ni\n\nj\n\ni\n\n4\n\n\u0004\n\nwith u independent n 0, \u2434 variates and x s 0 and x s 1.\na. explain how to interpret \u2424, and compare to the interpretation of\n\n2.\n\n\u017e\n\n1\n\n2\n\ni\n\n.\n\u2424 in the corresponding marginal model 10.14 .\n\n\u017e\n\n\u017e\n\n.\n\nb. this model implies model 12.3 for each 2 = 2 collapsing that\ncombines categories 1 through j for one outcome and categories\nj q 1 through i for the other. use the form of the conditional ml\n\u017e\nor random effects ml estimator for binary matched pairs to\nexplain why\n\n.\n\n\u017e\n\nlog\n\n\u00fd \u00fd\n\na)j b-j\n\nn\n\nab\n\n/\n\n\u017e\n\n\u00fd \u00fd\n\na-j b)j\n\nn\n\nab\n\n/\n\nis a consistent estimator of \u2424.\n\n.\n\n\u017e\n\nc. treat these i y 1 collapsed 2 = 2 tables naively as if they are\nindependent samples. show that adding the numerators and adding\nthe denominators of the separate estimates of e \u2424 motivates the\nsummary estimator of \u2424,\n\n\u02dc\u2424s log\n\n\u00bd\n\na y b n\n.\n\n\u017e\n\nab\n\n\u00fd\n\na)b\n\nb y a n\n.\n\n\u017e\n\nab\n\n\u00fd\n\nb)a\n\n5\n\n.\n\n\u02dc\n\nexplain why \u2424 is consistent for \u2424 even recognizing the actual\ndependence.\n\n\u017e .\nd. a standard error for \u2424 that treats the collapsed tables in part c\nas independent is inappropriate. treating n\nas a multinomial\nsample, show that an estimated asymptotic variance of \u2424 is agresti\n\n\u02dc \u017e\n\nab\n\n\u0004\n\n4\n\n\u02dc\n\n "}, {"Page_number": 552, "text": "problems\n\n537\n\n.\nand lang 1993a\n\n\u00bd\n\nb y a n\n\n.\n\n2\n\n\u017e\n\n\u00fd\nb)a\n\n\u00bd\n\n\u017e\n\n\u00fd\na)b\n\nq\n\nab\n\nb y a n\n.\n\n\u017e\n\nab\n\n\u00fd\nb)a\n\n2\n\n5\n\na y b n\n\n.\n\n2\n\nab\n\na y b n\n.\n\n\u017e\n\nab\n\n\u00fd\na)b\n\n2\n\n5\n\n.\n\n12.36 summarize advantages and disadvantages of using a glmm ap-\nproach compared to a marginal model approach. describe conditions\nunder which parameter estimators are consistent for a marginal\nmodels using gee, b marginal models using ml, c glmm using\npql, and d glmm using ml.\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\n "}, {"Page_number": 553, "text": "c h a p t e r 1 3\n\nother mixture models for\ncategorical data*\n\n.\n\n.\n\n\u017e\n\nin chapters 10 through 12 we introduced ways of handling correlated\nobservations due to repeated measurement and other forms of clustering.\nthe generalized linear mixed models glmms of chapter 12 assume\nnormal random effects. they describe heterogeneity by replacing the linear\npredictor by a normally distributed mixture of linear predictors. in this\nchapter we present additional models having connections with glmms.\nexcept for one case, these models use nonnormal mixture distributions.\n\nin section 13.1 we present latent class models. these treat a contingency\ntable as a mixture of unobserved tables at categories of a qualitative latent\n\u017e\nunobserved variable. in section 13.2 we discuss a related nonparametric\napproach to fitting glmms that uses an unspecified discrete quantitative\ndistribution for the random effects distribution.\n\nin section 13.3 we model clustered binomial responses using the beta\ndistribution to describe heterogeneity of binomial parameters. the resulting\nbeta-binomial distribution has variance function for which quasi-likelihood\nmethods are also available. in section 13.4 we model count responses using\nthe gamma distribution to describe heterogeneity of poisson parameters. the\nresulting negative binomial regression model corresponds to a poisson glmm\nhaving a log-gamma distributed random effect. it is an alternative to the\nglmm for poisson responses with normal random effects, a model discussed\nin section 13.5.\n\n13.1 latent class models\n\nglmms create a mixture of linear predictor values using a latent variable,\nthe unobserved random effect vector, having a normal distribution. by\ncontrast, latent class models use a mixture distribution that is qualitative\nrather than quantitative. the basic model assumes existence of a latent\n538\n\n "}, {"Page_number": 554, "text": "latent class models\n\n539\n\nfigure 13.1 association graph for latent class model.\n\ncategorical variable such that the observed response variables are condition-\nally independent, given that variable.\n\nfor categorical response variables y , y , . . . , y , the latent class model\nassumes a latent categorical variable z such that for each possible sequence\nof response outcomes\n\nand each category z of z,\n\n\u017e\n\n\u017e\n\n.\n\n.\n\nt\n\n1\n\n2\n\ny , . . . , y\n1\n\nt\n\np y s y , . . . , y s y z s z s p y s y z s z \u2b48\u2b48\u2b48 p y s y z s z .\n\u017e\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n<\n\n<\n\n<\n\nt\n\nt\n\nt\n\nt\n\n1\n\n1\n\n1\n\n1\n\n<\n\nt\n\nt\n\n.\n\n\u017e\n\n\u017e\n\nfigure 13.1 shows the association graph for the model. a latent class model\nsummarizes probabilities of classification p z s z\nin the latent classes as\nwell as conditional probabilities p y s y z s z of outcomes for each y\n.\nt\nwithin each latent class. these are the model parameters. more generally,\nthe latent variable z can be multivariate. the model\nis an analog for\ncategorical responses and latent variables of the factor analysis model for\nmultivariate normal responses.\n\nthe latent class model is sometimes plausible when the observed variables\nare several\nindicators of some concept, such as prejudice, religiosity, or\nopinion about an issue. an example is table 10.13, in which subjects gave\ntheir opinions about whether abortion should be legal in various situations.\nperhaps an underlying latent variable describes one\u2019s basic attitude toward\nlegalized abortion, such that given the value of that latent variable, responses\non the observed variables are conditionally independent. for instance, the\nlatent variable may be a qualitative variable with three categories: one class\nfor those who always oppose legalized abortion regardless of the situation,\none for those who always favor it, and one for those whose response depends\non the situation.\nis ob-\nserved. the t q 1 -dimensional table that cross-classifies it with the latent\nvariable is an unobserved table. denote the number of categories of each yt\nby i and the number of latent classes of z by q. for the observed table, let\n.\n\u2432\n. the model assumes a multinomial distri-\nbution over its i t cells. for a given cell,\n\nthe t-dimensional contingency table cross classifying y , . . . , y\nt\n\ns p y s y , . . . , y s y\n\ny , . . . , y\n1\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\nt\n\nt\n\n1\n\n1\n\n1\n\nt\n\n\u2432\n\ny , . . . , y\n1\n\ns\n\nt\n\nq\n\n\u00fd\nzs1\n\np y s y , . . . , y s y z s z p z s z .\n\u017e\n.\n\n.\n\n\u017e\n\n<\n\nt\n\nt\n\n1\n\n1\n\n "}, {"Page_number": 555, "text": "540\n\nother mixture models for categorical data\n\nthe conditional independence factorization for the latent class model states\nthat\n\n\u2432\n\ny , . . . , y\n1\n\ns\n\nt\n\nq\n\nt\n\n\u00fd \u0142\nts1\nzs1\n\np y s y z s z p z s z .\n\u017e\n.\n\n.\n\n\u017e\n\n<\n\nt\n\nt\n\n\u017e\n\n13.1\n\n.\n\nthis is a nonlinear model for the i t multinomial probabilities.\n\n13.1.1 fitting latent class models\ndenote the counts in the observed table by n\ncells in that table, the kernel of the multinomial log likelihood is\n\ny , . . . , y\n1\n\n\u0004\n\n4\n\nt\n\n. summing over the i\n\nt\n\nlog \u2432\n\ny , . . . , y\n1\n\n.\n\nt\n\nt\n\n\u017e\n\n13.2\n\n.\n\nn\n\n\u00fd y , . . . , y\n.\n\n\u017e\n\n1\n\n\u017e\n\n.\n\nt\n\n2\n\n1\n\nt\n\n4\n\n4\n\n\u0004\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nsubstituting parameters from 13.1 , one can maximize 13.2 with respect to\nthose parameters using newton\u1390raphson haberman 1979, chap. 10 or the\nem algorithm goodman 1974 . it is helpful to note that the latent class\n.\nmodel states that the loglinear model symbolized by y z, y z, . . . , y z\nholds for the unobserved table. the model makes no assumption about the\n\u0004\ny z associations but assumes that the y are mutually independent within\nt\neach category of z.\n\ndescribed shortly. the m maximization\n\n.\nthe em algorithm has two steps in each iteration. the e expectation\nstep in iteration s calculates pseudo-counts n\nfor the unob-\nserved table using n\nand a working conditional distribution for\n\u017e\n<\nz y , . . . , y\nstep treats\nt\n\u0004\n\u017e s.\nas data and applies an algorithm such as iterative reweighted\nn\ny , . . . , y , z\n1\n\u017e\nleast squares or ipf for fitting the model\nthe loglinear model\ni.e.,\n\u017e\n4\ny z, y z, . . . , y z . the fit \u242e\nof that model in the unob-\n1\nserved table then determines the new working conditional distribution of\n\u017e\nz y , . . . , y\nfor the e-step of the next iteration.\nt\nthis allocates the observed data to pseudo-counts in the unobserved cells in\nproportion to this fit, using\n\n\u0004\nto apply to n\n\n\u017e s.\ny , . . . , . . . , y , z\n1\n\n\u017e s.\ny , . . . , y , z\n1\n\ny , . . . , y\n1\n\ny , . . . , y\n1\n\n..\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n4\n\nt\n\n1\n\n2\n\n1\n\nt\n\nt\n\nt\n\nt\n\nt\n\n<\n\n\u017e sq1.\nn\ny , . . . , y , z\n1\n\nt\n\ns n\n\ny , . . . , y\n1\n\nt\n\n\u242e\u017e s.\ny , . . . , y , z\n1\nq\n\nt\n\n\u017e s.\u242e\u00fd y , . . . , y , k\nks1\n\nt\n\n1\n\n.\n\nthese are entries in the unobserved table for iteration s q 1 . they are used\nas pseudo-data for the m-step of iteration s q 1 .\n.\n\neventually, the algorithm converges to fitted values for the unobserved\ntable that provide fitted probabilities that satisfy mutual independence within\neach latent class, and such that the corresponding fitted probabilities in the\nobserved table i.e., added over the latent categories maximize the likelihood\n\u017e\n13.2 . the fitted probabilities in the unobserved table are an estimated joint\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 556, "text": "latent class models\n\n541\n\n\u017e\n\n.\n\n<\n\nt\n\nt\n\n1\n\nt\n\n\u0004\n\n\u017e\n\n.\n\n\u017e\n\n.4\n\nthe latent class model parameters p y s y z s z\n\ndistribution for y , . . . , y , z . one can use them to calculate the ml\nestimates of\nand\np z s z .\n\u017e\n.4\n\u0004\nthe em algorithm is computationally simple and relatively stable. each\niteration increases the likelihood. however, its convergence can be slow. see\nlaird 1998 for a review. the log likelihood for a latent class model may\nhave local maxima. thus, with either the newton\u1390raphson or em algorithm,\nit is advisable to perform the fitting process a few times with different\nstarting guesses for the parameter values. the em algorithm tends to be less\nsensitive to the choice of starting values. thus, some software begins with the\nem algorithm and then switches to the newton\u1390raphson algorithm as it\napproaches the ml estimates to speed the process. as q increases, multiple\nlocal maxima are more likely and the danger increases of a lack of identifia-\nbility.\n\nis a by-product of\n\nstandard errors for model parameter estimates result from inverting the\nmodel\u2019s estimated information matrix. this\nthe\nnewton\u1390raphson algorithm but not the em algorithm. one way to obtain\nstandard errors with it applies a useful formula of louis 1982 for the\nobserved information when using the em algorithm. it equals the expected\nvalue of the observed information for the loglinear model for the unobserved\ntable minus the expected value of the information for the conditional\ndistribution of z given the observed data. baker 1992 and lang 1992 gave\nrelated results.\nchi-squared statistics comparing observed cell counts to fitted values test\nthe model fit. the residual df s i y qt i y 1 y q. this follows since\nmultinomial model 13.1 describes i y 1 multinomial probabilities using\ni y 1 parameters p y s y z s z , y s 1, . . . , i y 1 at each of qt com-\n\u017e\nbinations of z and t, and q y 1 parameters p z s z . often, the nature of\nthe variables suggests a value for q, usually quite small 2 to 4 . otherwise,\nthe usual procedure starts with q s 2; if the fit is inadequate, it increases by\nsteps of 1 as long as the fit shows substantive improvement. specialized\n.\nsoftware exists for such models appendix a .\n\n\u017e\n\u0004\n\n.4\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n4\n\n\u0004\n\nt\n\nt\n\nt\n\nt\n\nt\n\n<\n\n13.1.2 latent class model for rater agreement\ntable 13.1 is an expanded data set of the example in section 10.5. seven\npathologists classified each of 118 slides on the presence or absence of\ncarcinoma in the uterine cervix. for modeling interobserver agreement, the\nconditional\nis often\nplausible. with a blind rating scheme, ratings of a given subject or unit by\ndifferent pathologists are independent. if subjects having true rating in a\ngiven category are relatively homogeneous, then ratings by different patholo-\ngists may be nearly independent within a given true rating class. thus, one\nmight posit a latent class model with q s 2 classes, one for subjects whose\ntrue rating is positive and one for subjects whose true rating is negative. this\n\nindependence assumption of the latent class model\n\n "}, {"Page_number": 557, "text": "542\n\nother mixture models for categorical data\n\ntable 13.1 diagnoses of carcinoma and fits of latent class modelsa\n\npathologist\n\na\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\nb\n0\n0\n1\n1\n1\n1\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\nc\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n\nd\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n1\n1\n\ne\n0\n1\n0\n0\n1\n1\n0\n1\n0\n0\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n\nf\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n1\n0\n1\n\ng\n0\n0\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\ncount\n\n34\n2\n6\n1\n4\n5\n2\n1\n2\n1\n2\n7\n1\n1\n2\n3\n13\n5\n10\n16\n\nq s 1\n1.1\n1.6\n2.2\n2.8\n3.3\n4.2\n1.4\n1.6\n2.8\n3.5\n4.2\n5.3\n1.4\n1.3\n2.0\n0.5\n3.3\n0.9\n1.2\n0.3\n\nfit\nq s 2\n23.0\n6.6\n12.7\n1.7\n3.6\n0.5\n3.0\n0.2\n1.7\n0.3\n0.5\n3.7\n2.6\n0.1\n4.3\n3.1\n11.5\n8.4\n13.5\n9.9\n\nq s 3\n33.8\n2.0\n6.3\n1.5\n3.0\n4.7\n2.1\n0.2\n1.3\n1.6\n2.9\n6.5\n1.4\n0.1\n2.6\n2.0\n9.6\n8.7\n13.6\n12.3\n\na\n\nfits obtained with latent gold statistical innovations, belmont ma . 1, yes; 0, no.\nsource: based on data in landis and koch 1977 , not showing empty cells.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nmodel expresses the 27 joint distribution of the seven ratings as a mixture of\ntwo 27 distributions, one for each true rating class.\n\ntable 13.2 shows results of fitting some latent class models including a\nmixture model studied in section 13.2.4 . because the observed table is\nsparse, the deviance is mainly useful for comparing models. this is an\ninformal comparison, though, since the chi-squared distribution does not\napply for comparing deviances of models with different numbers of latent\nclasses. a model with q classes is a special case of a model with q* ) q\nclasses in which p z s z s 0 for z ) q and hence falls on the boundary of\nthe parameter space. ordinary chi-squared likelihood-ratio tests require\nparameters to fall in the interior of the parameter space i.e., 0 - p z s z\n.\n- 1 for z s 1, . . . , q* .\n.\ntable 13.1 also shows the fitted values for latent class models with\nq s 1, 2, 3, for the cells having positive counts. each empty cell also has a\nfitted value, not shown here . the model with q s 1 latent class is the model\nof mutual independence of the seven ratings. equivalently, it is the loglinear\nmodel y , y , . . . , y . it fits poorly, as one would expect. with q s 2,\nconsiderable evidence remains of lack of fit. for instance, the fitted count for\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n1\n\n2\n\n7\n\n "}, {"Page_number": 558, "text": "latent class models\n\ntable 13.2 likelihood-ratio statistics for latent class models\nfitted to table 13.1\n\na\n\ndeviance g\n\n1\n2\n\nlatent classes\n\nmodel\n\nnumber of\n\n\u017e\nstatistic\n476.8\n62.4\n67.6\n15.3\n27.5\n6.4\n23.7\n.\nmodels fitted with latent gold statistical innovations, belmont, ma .\n\nmutual independence\nlatent class\nrasch mixture\nlatent class\nrasch mixture\nlatent class\nrasch mixture quasi-symmetry\n\n3\n\n4\n\n\u017e\n\n.\n\n\u017e\n\na\n\n2\n\n.\n\n543\n\ndf\n120\n112\n118\n104\n116\n96\n114\n\n2\n\n\u017e\n\n\u017e\n\n.\n\na negative rating by each pathologist is 23.0, compared to an observed count\nof 34. the small g that table 13.2 reports for this model does not imply a\ngood fit; in section 9.8.4 we noted that g2 tends to be highly conservative\nwhen most fitted values are very close to 0. the model with q s 3 seems to\nfit adequately.\nstudying the estimated probability p y s 1 z s z of a carcinoma diag-\nnosis for each pathologist, conditional on a given latent class\nz, helps\nilluminate the nature of these classes. table 13.3 reports these for the\nthree-class model. they suggest that 1 the first latent class refers to cases\nthat all pathologists except occasionally b agree show no carcinoma; 2 the\nthird latent class refers to cases in which a, b, e, and g agree show\ncarcinoma and c and d usually agree; and 3 the second latent class refers\nto cases of strong disagreement, whereby c, d, and f rarely diagnose\ncarcinoma but b, e, and g usually do. the estimated proportions in the\nthree latent classes are p z s 1 s 0.37, p z s 2 s 0.18, and p z s 3 s\n0.45. the model estimates that 18% of the cases fall in the problematic class.\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\n\u02c6\n\u017e\n\n\u02c6\n\u017e\n\n\u02c6\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n.\n\nt\n\n<\n\ntable 13.3 estimated probabilities of diagnosing carcinoma, for\na\nlatent class model and rasch mixture model with three classes\n\nmodel\nlatent\nclass\n\nrasch\n\nmixture\n\nlatent\nclass\n\n1\n2\n3\n1\n2\n3\n\npathologist\n\na\n\n0.057\n0.513\n1.000\n0.022\n0.611\n0.994\n\nb\n\n0.138\n1.00\n0.981\n0.150\n0.923\n0.999\n\nc\n\n0.000\n0.000\n0.858\n0.001\n0.052\n0.853\n\nd\n\n0.000\n0.058\n0.586\n0.000\n0.015\n0.617\n\ne\n\n0.055\n0.751\n1.000\n0.047\n0.774\n0.997\n\nf\n\n0.000\n0.000\n0.476\n0.000\n0.009\n0.483\n\ng\n\n0.000\n0.631\n1.000\n0.022\n0.611\n0.994\n\na\n\n.\nresults obtained with latent gold statistical innovations, belmont, ma .\n\n\u017e\n\n "}, {"Page_number": 559, "text": "544\n\nother mixture models for categorical data\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\na danger with latent variable models, shared by factor analysis for contin-\nuous responses, is the temptation to interpret latent variables too literally. in\nthis example it is tempting to treat latent class 1 latent class 3 as cases truly\nwithout carcinoma with carcinoma . thus, it is tempting to treat a rating of\nno carcinoma a rating of carcinoma given that the subject falls in latent\nclass 1 latent level 3 as necessarily being a correct judgment. one should\nrealize the tentative nature of the latent variable. be careful not to make the\nerror of reification\u138ftreating an abstract construction as if it has actual\n.\nexistence gould 1981 .\nusing model parameter estimates and bayes\u2019 theorem, one can also\nestimate p z s z y s y and p z s z y s y , . . . , y s y\n.\n. if a patholo-\ngist makes a \u2018\u2018yes\u2019\u2019 rating, for instance, what is the estimated probability that\nthe subject is in the latent class for which agreement on a positive rating\nusually occurs? we perform further analysis in section 13.2.5 after studying a\nsimpler model.\n\nespeland and handelman 1989 , uebersax 1993 , uebersax and grove\n\u017e\n1990, 1993 , and yang and becker 1997 presented various latent variable\nmodels for rater agreement and diagnostic accuracy. one could also use\nmethods of chapters 11 and 12, such as a model with a continuous rather\nthan qualitative latent variable. a logistic-normal random intercept model,\nfor instance, yields subject-specific comparisons of p y s 1 for various t.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nt\n\nt\n\n1\n\n1\n\nt\n\nt\n\n<\n\n<\n\nt\n\n\u017e\n\n.\n\n13.1.3 latent class models for capture\u2013recapture\nwe next apply latent class models to capture\u1390recapture modeling for esti-\nmating population size. in section 12.3.6 a logistic-normal glmm was used\nfor this. with t sampling occasions, a 2t contingency table displays the data,\nwith scale captured, not captured at each occasion. a prediction of the\npopulation size equals the prediction for the missing cell count, representing\nsubjects not captured at every occasion, added to the counts in other cells.\n\nthe type of any given subject\n\nwith two classes, the latent class model treats the population as a mixture\nof two types, perhaps determined by genetic or environmental\nfactors.\nhomogeneity of capture probabilities occurs for subjects within each type,\nbut\nis unknown. this model represents\na compromise between the mutual independence model, which assumes a\nsingle latent class and complete homogeneity, and the logistic-normal glmm,\nwhich assumes a continuous mixture of capture probabilities rather than two\nclasses.\nwe illustrate with the t s 6-capture data set on snowshoe hares in table\nindependence predicts that n s 75. its 95%\n12.6. the model of mutual\nprofile-likelihood confidence interval for n is 70, 83 . the latent class model\nwith two classes has n s 85 and a profile-likelihood confidence interval of\n\u017e\n74, 106 . the latent class model with three classes gives similar results. since\nthe logistic-normal glmm in section 12.3.6 gave the interval 75, 154 , these\nseem too short to be trusted. this simple latent class model may not capture\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 560, "text": "nonparametric random effects models\n\n545\n\nall the existing heterogeneity. it is more plausible to assume a continuous\nlatent variable than a discrete one with a couple of classes. we\u2019ll analyze\nthese data further with related models in the next section.\n\n13.2 nonparametric random effects models\n\nin spite of its popularity and attractive features, the normality assumption for\nrandom effects in ordinary glmms can rarely be closely checked. for\ninstance, in studying normal glmms, verbeke and lesaffre 1996 noted\nthat under a normality assumption for random effects, their predicted values\noften appear normally distributed even when the true values are generated\nfrom a highly nonnormal distribution. an obvious concern of this or any\nparametric assumption for the random effects is possibly harmful effects of\nmisspecification. to check sensitivity to this assumption, one can fit glmms\nusing alternative or more general random effects assumptions.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n13.2.1 logit models with unspecified random effects distribution\na nonparametric approach e.g., aitkin 1999 guards against possibly harmful\nmisspecification effects. this uses an unspecified random effects distribution\non a finite set of mass points. the location of the mass points and their\nprobabilities are parameters. the number of mass points can be fixed. when\nthis number is itself unknown, one treats it as fixed in the estimation process\nbut increases it sequentially until the likelihood is maximized. the maximiza-\ntion usually requires relatively few mass points. even allowing a continuous\nmixture distribution, the nonparametric estimate of that distribution takes a\nfinite number of points e.g., lindsay et al. 1991 . in fact, fitting a model\nhaving only two mass points often results in fixed effects estimates quite\nsimilar to those with the full maximization. this approach is useful primarily\nwhen the random effects distribution is not itself of direct interest, since the\nnonparametric estimate of that distribution tends to be poor even for very\nlarge samples.\n\nmodel fitting is actually simpler than for models with normal random\neffects, since the integral that determines the likelihood function simplifies to\na finite sum. in section 13.2.4 we discuss this point with a rasch-type model.\n.\nspecialized software can fit nonparametric mixture models appendix a .\nhowever, this approach also has disadvantages. for instance, with multivari-\nate random effects it cannot provide simple correlation structure as the\nnormal can. standard inference does not apply for comparing models\nwith different numbers of mass points, since one model is on the boundary of\nthe parameter space compared to the other. also, the ml estimate of the\nrandom effects distribution often places some weight at \"\u2b01. although\nthis can be useful with binary data for identifying a subsample for which the\nestimated response probability equals 1 or equals 0 for all observations in a\n\n\u017e\n\n "}, {"Page_number": 561, "text": "546\n\nother mixture models for categorical data\n\ncluster, it is not then possible to describe heterogeneity with an estimated\nvariance component.\n\nto illustrate this approach, we reanalyze table 10.13 on attitudes about\nlegalized abortion. in section 12.3.2 we fitted the logistic-normal model\n.\n\u017e\n12.10 ,\n\nit\n\n\u017e\n\n.\n4\n\nlogit p y s 1 u s u q \u2424 q \u2425x,\n\n.\n13.3\nwith x s gender and parameters \u2424 representing three conditions under\nwhich abortion might be legal. treating u instead nonparametrically, the\nlikelihood maximizes with a two-point mixture distribution. estimated abor-\ntion item effects are \u2424 y \u2424 s 0.83 se s 0.16 , \u2424 y \u2424 s 0.30 se s\n0.16 , and \u2424 y \u2424 s 0.52 se s 0.16 . results are similar to those that\n.\ntable 12.3 shows for the normal random effects approach section 12.3.2 .\n\n\u017e\n.\n\n\u02c6\n3\n\n\u02c6\n2\n\n\u02c6\n2\n\n\u02c6\n3\n\n\u02c6\n1\n\n\u02c6\n1\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u0004\n\nt\n\nt\n\ni\n\ni\n\ni\n\n<\n\ni\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n13.2.2 nonparametric mixing of logistic regression\nfollman and lambert 1989 presented an example with a prespecified\nnumber of mass points. they analyzed the effect of the dosage of a poison on\nthe probability of death of a protozoan of a particular genus. table 13.4\nshows the data. they assumed two unobserved types of that genus.\nlet \u2432 x denote the probability of death at log dose level x for genus\ntype i, i s 1, 2. let \u2433 denote the probability a protozoan belongs to genus\ntype 1. their model specifies\n.\n\n\u2432 x s \u2433\u2432 x q 1 y \u2433 \u2432 x ,\n.\n\u017e\nwith unknown \u2433. the curve for \u2432 x\nhaving the same shapes but different intercepts.\nlogit \u2432 x s y68.4 q 42.1 x with se s 3.8 for \u2424s 42.1 ,\n.\ndeviance g s 24.7 df s 6 . the fit of the mixture model is\n\nis a weighted average of two curves\nthe ordinary logistic regression model is the special case \u2433s 1. its fit,\nw\nis poor, with\n\nlogit \u2432 x s \u2423 q \u2424x ,\n\nwhere\n\n.x\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n1\n\n2\n\n2\n\ni\n\ni\n\n\u2432 x s 0.34\u2432 x q 0.66\u2432 x ,\n.\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n1\n\n2\n\nwith\nlogit \u2432 x s y196.2 q 124.8 x,\n\n\u017e\n\n.\n\n\u02c6\n\n1\n\nlogit \u2432 x s y205.7 q 124.8 x,\n\n\u017e\n\n.\n\n\u02c6\n\n2\n\ntable 13.4 number of protozoa exposed to poison dose and number that died\n\npoison\ndose\n4.7\n4.8\n4.9\n5.0\n\nexposed\n\ndead\n\n55\n49\n60\n55\n\n0\n8\n18\n18\n\npoison\ndose\n5.1\n5.2\n5.3\n5.4\n\nexposed\n\ndead\n\n53\n53\n51\n50\n\n22\n37\n47\n50\n\nsource: follman and lambert 1989 . reprinted with permission from the journal of\namerican statistical association.\n\n\u017e\n\n.\n\nthe\n\n "}, {"Page_number": 562, "text": "nonparametric random effects models\n\n547\n\nfigure 13.2 fit of binary mixture of logistic regressions to table 13.4 model fitted using\n.x\nlatent gold statistical innovations, belmont, ma .\n\n\u017e\n\nw\n\n2\n\n\u017e\n\n.\n\n\u02c6\n\nand se s 25.2 for \u2424s 124.8. figure 13.2 shows the fit. this is much better,\nwith g s 3.4 df s 4 ; that is, double the maximized log-likelihood in-\ncreases by 24.7 y 3.4 s 21.3 by adding two parameters: an additional inter-\ncept and the probability for the mixture. follman and lambert noted that\nwith eight dose levels, at most two mixture points are identifiable for this\nmodel.\n\nthe ordinary glmm assumes a normal mixture of logistic curves. it gives\na deviance reduction of only 1.7 compared to the ordinary logistic model with\n\u2433s 1.\n\nis misspecification a serious problem?\n\n13.2.3\nis it worth the trouble to consider alternatives to the normality assumption\nfor random effects in glmms, whether they be parametric or nonparamet-\nric? not much work exists on investigating misspecification effects. for\nlogistic random intercept models, different assumptions for the random\neffects distribution often provide similar results for estimating the regression\neffects. choosing an incorrect random effects distribution does not tend to\nbias estimators of those effects. the true distribution for the random effects\nbeing skewed can result in some bias for the normal intercept estimator\n\u017e\nneuhaus et al. 1992 . the choice of random effects distribution also usually\nhas little impact on efficiency of estimation.\n\nwhen the true random effects distribution is dramatically far from normal,\nthere can be some efficiency loss for the logistic-normal estimator. this can\n\n.\n\n "}, {"Page_number": 563, "text": "548\n\nother mixture models for categorical data\n\nhappen when the true distribution is a two-point mixture with large variance\ncomponent. b. caffo and i studied this with various models, such as a simple\none-way random effects model. in cluster i, let y be a bernoulli variate\nsatisfying\n\nit\n\nlogit p y s 1 u s \u2423q u ,\n\n<\n\n.\n\n\u017e\n\nit\n\ni\n\ni\n\ni s 1, . . . , n,\n\nt s 1, . . . , t ,\n\n\u017e\n\n13.4\n\n.\n\n<\n\ni\n\ni\n\ni\n\ni\n\ni\n\n2\n\nit\n\n\u0004\n\nx\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nu\n\nwhere var u s \u2434 . simulated samples from this model used various n, t,\n\u2423, and \u2434, and various true distributions for u including normal, uniform,\nexponential, and binary. usually, assuming normality does not hurt when the\ntrue distribution is nonnormal. also, using a nonparametric approach when\nthe true distribution is normal does not result\nin much efficiency loss\nw\nneuhaus and lesperance 1996 noted this for a related model. however,\nwhen the true distribution is a two-point mixture, the normal approach loses\nefficiency in estimating \u242e s p y s 1\nas \u2434 and t increase. for\nexample, when n s t s 30, \u2423s 0, and the mixture has probability 0.5 at\neach point,\nthe\nnormal, nonparametric approach when \u2434s 0.5, 0.06, 0.02 when \u2434s 1.0,\n\u017e\nand 0.04, 0.01 when \u2434s 2.0. differences for estimating \u2423 are less dra-\n\u017e\nmatic.\nthe example from follman and lambert 1989 discussed in section\n13.2.2, which has a covariate but t s 1, illustrates the potential efficiency\nloss with the logistic-normal glmm. the two-point mixture model has\n\u2424s 124.8 with se s 25.2, for which \u2424rse s 4.9. the normal mixture model\n\u02c6\nhas \u2424s 65.5 with se s 19.5, for which \u2424rse s 3.4.\n\nthe expected value of \u242e y \u242e is\n\n\u017e\n0.06, 0.05\n\nfor\n\n\u02c6i\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n.4\n\n. \u017e\n\nour study suggested that the random effects distribution has to be rather\nextremely nonnormal for the normal glmm to suffer in bias or efficiency.\nhowever, heagerty and zeger 2000\nsee also mcculloch 1997 noted that\nother types of misspecification can be more crucial. regarding bias, they\nargued that sensitivity to the random effects assumption is greater for\nestimating regression parameters in random effects models than estimating\ntheir counterparts in corresponding marginal models. they illustrated this\nwith a model violation by which the variance of the random effects depends\non values of covariates. they concluded that between-cluster effects may be\nmore sensitive to correct specification of the random effects distribution than\nwithin-cluster effects. this is an advantage of using marginal models for\nbetween-cluster effects.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n13.2.4 rasch mixture model\nfrom section 12.1.4, for subject i with item t the rasch model for a binary\nresponse is\n\nlogit p y s 1 u s u q \u2424 ,\n\n<\n\n\u017e\n\n.\n\nit\n\nt\n\ni\n\ni\n\nt s 1, . . . , t .\n\n\u017e\n\n13.5\n\n.\n\n "}, {"Page_number": 564, "text": "nonparametric random effects models\n\n549\n\n4\n\n\u0004\n.\nthe glmm treats u\nas normal random effects. lindsay et al. 1991\nstudied this model when u instead can assume only a finite number q of\nvalues. denote the distribution of the latent variable u , which is the same for\nall i, by\n\n\u017e\n\ni\n\ni\n\ni\n\np u s a s \u2433 ,\n\u017e\n\n.k\n\nk\n\nk s 1, . . . , q,\n\nt\n\nk\n\nk\n\nk\n\nk\n\nk\n\n4\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n\u0004\nand \u2433 . for identifiability one can either place a con-\nfor unknown a\nstraint on this distribution, such as \u00fd \u2433 a s 0, or on \u2424 . this model is\ncalled a rasch mixture model.\n\nlike other random effects models, the rasch mixture model is a latent\nvariable model. the random effect u is unobserved, and the t responses are\nassumed conditionally independent at each fixed u value. it differs from the\nordinary latent class model for binary responses having q latent classes\nsection 13.1 , since it assumes structure 13.5 for p y s 1 u whereas\n\u017e\n.\nlatent class model 13.1 assumes no structure for p y s y z s z .\n.\n\nthis model is simpler to fit than glmms with normal random effects\nbecause the glmm\u2019s intractable integral that determines the likelihood\nfunction is replaced by a finite sum. the marginal probability of a sequence\nof responses\n\nis\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\nit\n\nt\n\nt\n\ni\n\ni\n\ni\n\n<\n\n<\n\ny , . . . , y\n1\n\nt\n\n\u2432\n\ny , . . . , y\n1\n\ns\n\nt\n\nq\n\nt exp y a q \u2424\n.\n\u00fd \u0142\n1 q exp a q \u2424\nts1\nks1\n\n\u2433\nk\n\n\u017e\n\n\u017e\n\nk\n\nk\n\nt\n\nt\n\nt\n\n.\n\n.\n\n\u017e\n\n.\n\nt\n\nk\n\n4\n\n\u0004\n\n4\n\nsubstituting this in the multinomial log likelihood 13.2 , ml estimation of\n\u0004\na , \u2433 and \u2424 can proceed using newton\u1390raphson or em algorithms. as\nk\nq increases, the maximized likelihood increases and the fit improves. how-\never, lindsay et al. 1991 showed that with t items, the likelihood no longer\nchanges once q s t q 1 r2. then, the model gives the same fit to the 2\nt\nobserved table as the quasi-symmetry model 10.33 . thus, this simpler latent\nclass model has a symmetric conditional association structure among the\nobserved variables. arminger et al. 2000 extended the rasch mixture model\nto incorporate covariates.\n\n\u017e\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n13.2.5 modeling rater agreement\nfor the ratings of carcinoma by seven pathologists table 13.1 , table 13.2\nalso summarizes the fit of rasch mixture models. here, p y s 1 u\n.\nin\n\u017e\n13.5 denotes the probability of a carcinoma diagnosis for pathologist\nt\nevaluating slide i. with q s 3 i.e., u can take 3 values , it does not fit\nsignificantly more poorly than the latent class model. with t s 7 raters, the\ndiscrete mixture can take at most t q 1 r2 s 4 points. the model with\nq s 4 is equivalently the quasi-symmetry model. it does not seem to fit better\nthan with q s 3.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nit\n\ni\n\ni\n\n<\n\n "}, {"Page_number": 565, "text": "550\n\nother mixture models for categorical data\n\nfigure 13.3 pathologist estimates for rasch mixture model and results of 90% bonferroni\nsimultaneous comparison.\n\nt\n\nt\n\n4\n\n\u02c6\u0004\nt\n\nfigure 13.3 shows \u2424 for the rasch mixture model with q s 3, setting\n\u02c6\u00fd \u2424 s 0. these describe variation among the pathologists\u2019 response distribu-\ntions at each latent level. for a given latent class, for instance, the estimated\nodds of a carcinoma diagnosis for pathologist b are exp 3.52 y 1.48 s 7.7\ntimes the estimated odds for pathologist a. pathologist b tends to make a\ncarcinoma diagnosis most often, and d and f the least. the figure also shows\nresults of a 90% bonferroni comparison of the 21 pairs of pathologists, based\non standard errors of pairwise differences \u2424 y \u2424 .\n\u02c6\ns\n\n\u02c6\nt\n\n\u017e\n\n.\n\nfor pathologist t, conditional on latent level k for a slide,\n\nexp a q \u2424 1 q exp a q \u2424\n\u02c6\nt\n\n\u02c6\nk\n\n\u02c6\nk\n\n\u02c6\nt\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nll\n\nk\n\n.\n\n\u02c6\n1\n\n\u02c6\n2\n\n\u02c6\n3\n\n\u02c6\n1\n\n\u02c6\n3\n\n\u02c6\n2\n\n\u017e\n\u017e\n\n.\n.x\n\nthe estimated odds\n\nestimates the probability of a carcinoma diagnosis. table 13.3 reports these,\nwhich use a s y5.25, a s y1.02, and a s 3.63. they are similar to the\nestimates for the ordinary latent class model but a bit smoother, with fewer\nestimates at the boundary. again, at latent level 1 pathologists tend not to\ndiagnose carcinoma, at level 2 many disagreements occur, and at level 3\npathologists tend to diagnose carcinoma. the estimated latent class propor-\ntions are \u2433 s 0.37, \u2433 s 0.19, and \u2433 s 0.43, with 19% of cases falling in\nthe problematic class.\nmodel 13.5 implies that the association between each y and u has log\nodds ratio a y a\nfor levels k and ll of u. for instance, in the third latent\nclass\nthat a pathologist diagnoses carcinoma are\nw\nexp 3.63 y y5.25 ) 7000 times those in the first latent class. in terms of\nin table 13.3, using pathologist a this is\nthe estimated probabilities\n.x\nexp 0.994r0.006 r 0.022r0.978 . the large a y a\n\u02c6\nsuggest strong associa-\ntion between each pathologist\u2019s rating and the latent variable. this induces\nstrong association between pairs of pathologist ratings. the model-fitted\nodds ratios between pairs of raters vary between about 7 and 400. however,\nthe quite varied \u2424 suggest that substantial marginal heterogeneity exists\namong the seven ratings. this causes heterogeneity in pairwise levels of\nagreement.\nthe mutual independence model is the special case of the rasch mixture\nmodel with q s 1; that is, \u2433 s 1. for table 13.1 the rasch mixture model\nwith q s 3 has only four more parameters than the mutual independence\n\n\u02c6\u0004\nt\n\n\u02c6\nk\n\nw\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n4\n\n\u0004\n\n4\n\n1\n\nt\n\nll\n\n "}, {"Page_number": 566, "text": "nonparametric random effects models\n\n551\ni.e., \u2433 and a , k s 1,2 . yet it fits well and has simple interpreta-\n\u017e\nmodel\ntions. see agresti and lang 1993b for further details and a simpler model\nthat sets a y a s a y a .\n\n.\n\n\u017e\n\n.\n\nk\n\nk\n\n1\n\n2\n\n2\n\n3\n\n2\n\n1\n\nt\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n13.2.6 other models for capture\u2013recapture\nin section 13.1.3 latent-class models were used for capture\u1390recapture experi-\n.\nments. alternatively, one could use the rasch mixture model. model 13.5\nwith two classes gives n s 77 and a 95% profile-likelihood confidence\ninterval of 71, 87 . this seems too short to trust. it is more realistic to allow\na continuous distribution for capture probabilities. model 13.5 treating ui\nas normal rather than binary does this, and in section 12.3.6 we used it for\nthese data.\n\n\u02c6\n\nso, which models might be used other than a parametric random effects\nmodel? one possibility is a loglinear model cormack 1989 . this is a\nmarginal model, applying to probabilities averaged over subjects. let yt\ndenote the binary capture variable for a randomly selected subject at occa-\nsion t, with categories captured, not captured . the simplest model, denoted\nby y , y , . . . , y , assumes that capture events are mutually independent.\nthis is equivalent to the logistic-normal model 13.5 with \u2434s 0 and latent\nclass model 13.1 with q s 1. a more plausible model allows an association\nbetween pairs of capture variables. this is equivalently the loglinear model\ndenoted y y , y y , . . . , y\ny . alternatively, a model with markov struc-\nty1 t\nture such as y y , y y , . . . , y\ny may be useful. usually, insufficient\n3\ndata exists to warrant using very complex loglinear models. for any such\nmodel, its fit for the 2t y 1 observed cells projects to the remaining cell to\npredict the number unobserved at every occasion.\n\n.\nty1 t\n\n1 2\n\n1 3\n\n1 2\n\na connection exists between nonparametric random effects and loglinear\napproaches. in section 13.2.7 we show that assuming model 13.5 but using a\nnonparametric treatment of u implies a loglinear model of quasi-symmetric\nform for the marginal model. the quasi-symmetry model 10.33 itself is not\nuseful for this problem, because any count in the missing cell is consistent\nwith it. the model has an interaction parameter pertaining to that cell alone,\nwhich results in a likelihood equation equating that cell count to its fitted\nvalue. so, information in other cells does not help in the estimation of the\nexpected frequency in that cell. however, special cases of quasi-symmetry\nare useful darroch et al. 1993 . an example is the loglinear model with the\nsame association for each pair of occasions. like the logistic-normal model,\nthis model of exchangeable association has only one more parameter than the\nmutual independence model.\nfor the snowshoe hare data of table 12.6, the model with exchangeable\ntwo-factor association has n s 90.5 and a confidence interval of 75, 125 .\n.\nthis interval and the one of 71, 87 for the rasch mixture model with q s 2\nare substantially narrower than the interval 75, 154 for the logistic-normal\nmodel section 12.3.6 . in capture\u1390recapture experiments, n and the confi-\n\n\u02c6\n\u017e\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2\n\ni\n\n "}, {"Page_number": 567, "text": "552\n\nother mixture models for categorical data\n\n2\n\n2\n\n\u017e\n\ndence interval for n depend strongly on the choice of model. the problem is\ninherently one of prediction. estimating n requires extrapolating from the\nobserved numbers of subjects having 1, 2, . . . , t captures to the number of\nsubjects with 0 captures. standard goodness-of-fit criteria are of limited help.\ntwo models can fit the data well, yet yield quite different estimates for the\nunobserved count. for instance, for the snowshoe hare data, the loglinear\nmodels of mutual independence and of two-factor association both fit the\nobserved cells relatively well g s 58.3, df s 56 for mutual independence\nand g s 32.4, df s 41 for the two-factor model ; however, their n values\nare 75 and 105.\n\n\u02c6\n\nsimpler models usually give narrower confidence intervals for n, through\nthe usual benefits of model parsimony. this is not necessarily good. a narrow\nconfidence interval for n is desirable, but not at the expense of severe\nsacrifice in the actual confidence level. intervals based on a possibly unrealis-\ntic assumption of subject homogeneity may be overly optimistic. simulations\nsuggest that actual coverage probabilities are often well below nominal levels\nwhen even slight model misspecification occurs. allowance for heterogeneity\namong subjects results in wider intervals. severe population heterogeneity\nmakes reaching useful conclusions difficult, as intervals can be very wide\n\u017e\n.\nburnham and overton 1978, coull and agresti 1999 .\n\n.\n\n13.2.7 nonparametric mixtures and quasi-symmetry\n.\na distribution-free approach for u with the rasch form of model 13.5\nimplies the quasi-symmetry loglinear model marginally darroch 1981; tjur\n1982 . we now show this result, to which we alluded in section 10.4.2.\noutcomes y s y , . . . , y\n\nlet y denote the sequence of t responses for subject i. for possible\n\n, where each y s 1 or 0,\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\ni\n\ni\n\n1\n\nt\n\nt\n\np y s y u s\n\u017e\n\n. \u0142\n\n<\n\ni\n\ni\n\nt\n\nt\n\n\u017e\n\nexp u q \u2424\n\n.\n1 q exp u q \u2424\n\n.\nexp u \u00fd y q \u00fd y \u2424\n\u0142 1 q exp u q \u2424\n.\n\ni\n\u017e\n.\n\u017e\n\n\u017e\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\ni\n\ni\n\nt\n\nt\n\ni\n\ns\n\n1\n\u017e\n\n1 q exp u q \u2424\n\nt\n\ni\n\n1yy\n\nt\n\n.\n\ny\n\nt\n\n.\n\nlet f denote the cdf of u . the marginal probability of sequence y for a\nrandomly selected subject is suppressing the subject label\n\ni \u017e\n\n.\n\n\u2432\n\ny , . . . , y\n1\n\nt\n\ns e p y s y u s exp\n\n.\n\n\u017e\n\n<\n\nu\n\n\u017e\n\n\u00fd\n\nt\n\n/\n\nh\n\ny \u2424\nt\nt\n\nexp u \u00fd y\nt\n\n\u017e\n\nt\n\n.\n\n\u0142 1 q exp u q \u2424\n\n\u017e\n\nt\n\nthis probability contributes to the log likelihood, which is 13.2 for a\nmultinomial distribution over the 2t cells for possible y. regardless of the\nchoice for f, the integral is complex. however, it depends on the data only\n\n.\ndf u .\n\n\u017e\n\n.\n\nt\n\n\u017e\n\n.\n\n "}, {"Page_number": 568, "text": "beta-binomial models\n\n553\n\nthrough \u00fd y . a more general model replaces this integral by a separate\nparameter for each value of \u00fd y . this model has form\n\nt\n\nt\n\nt\n\nt\n\nlog\u2432\n\ny , . . . , y\n1\n\nt\n\ns y \u2424 q \u242d\n\n\u00fd\n\nt\n\nt\n\ny q \u2b48\u2b48\u2b48 qy\n1\n\n.\n\nt\n\n\u017e\n\n13.6\n\n.\n\nt\n\nt\n\nt\n\nt\n\n\u0004\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\nthe final term represents a separate parameter at each value of \u00fd y .\nt\n\nthe implied marginal model 13.6 has interaction term that is invariant to\na permutation of the response outcomes y, since each such permutation\nyields the same sum, \u00fd y . thus, it is the loglinear model of quasi-symmetry\n\u017e\n10.33 . no matter what form f takes, the marginal model has the same main\neffect structure, and it has an interaction term that is a special case of the\none in 13.6 . thus, one can consistently estimate \u2424 using the ordinary ml\nestimates for the loglinear model. in fact, tjur 1982 showed that these\nestimates are also the conditional ml estimates, treating u as fixed effects\nand conditioning on their sufficient statistics. the interaction parameters in\nmodel 13.6 result from the dependence in responses among variables, due\n4\nto heterogeneity in u .i\n\nwe illustrate for the opinions about\n\nlegalized abortion analyzed in\nsections 10.7.2 and 12.3.2 and with a nonparametric random effects approach\nin section 13.2.1. for model 13.3 , estimated within-subject comparisons\n\u2424 y \u2424 of items result from fitting a quasi-symmetric loglinear model. let\n.\n\u242e y , y , y denote the expected frequency for gender g making response y\nt s 1, 2, 3, where for item t, y s 1 for approval of legalized\nto item t,\nabortion and 0 for disapproval. the loglinear model is\n\nt\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n1\n\n2\n\n3\n\ng\n\ns\n\nt\n\nt\n\nt\n\ni\n\nlog \u242e y , y , y s \u2424 y q \u2424 y q \u2424 y q \u2425g q \u242d\n\n.\n\n\u017e\n\ng\n\n1\n\n2\n\n3\n\n2\n\n2\n\n1\n\n1\n\n3\n\n3\n\ny qy qy\n1\n\n2\n\n\u017e\n\n13.7\n\n.\n\n.\n\n3\n\nk\n\n2\n\n3\n\n2\n\n1\n\n.\n\n\u017e\n\n\u02c6\n1\n\n\u02c6\n3\n\n\u02c6\n2\n\n\u02c6\n2\n\u017e\n\nfor y q y q y s k, \u242d refers to all cells in which subjects voiced\napproval for k of the three items, k s 0, 1, 2, 3. the ml fit, which has\ng s 10.2 with df s 9, yields \u2424 y \u2424 s 0.521 se s 0.154 , \u2424 y \u2424 s 0.828\nse s 0.160 , and \u2424 y \u2424 s 0.307 se s 0.161 . these are similar to the\n\u017e\nnormal random effects estimates table 12.3 and nonparametric random\neffects estimates in section 13.2.1. they also are the conditional ml esti-\nmates for model 13.3 , treating u as fixed. with this approach or condi-\ntional ml, however, one cannot estimate between-groups effects, such as the\ngender effect in model 13.7 . the \u2425 parameter in model 13.7 refers to\nrelative sample sizes of males and females and is not the same as the gender\n. x\neffect in 13.3 .\n\n\u02c6\n3\n\n\u02c6\n1\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\nw\n\n\u0004\n\n4\n\ni\n\n13.3 beta-binomial models\n\nthe beta-binomial model\nis a parametric mixture model that is another\nalternative to binary glmms with normal random effects. as with other\n\n "}, {"Page_number": 569, "text": "554\n\nother mixture models for categorical data\n\nmixture models that assume a binomial distribution at a fixed parameter\nvalue, the marginal distribution permits more variation than the binomial.\nthus, a model using the beta-binomial is a way to handle overdispersion\noccurring with ordinary binomial models.\n\n13.3.1 beta-binomial distribution\nthe beta-binomial distribution results from a beta distribution mixture of\n.\nbinomials. suppose that a given \u2432, y has a binomial distribution, bin n, \u2432 ,\nand b \u2432 has a beta distribution.\n\n\u017e .\n\n\u017e .\n\n\u017e\n\nthe beta probability density function is\n\nf \u2432; \u2423, \u2424 s\n\u017e\n\n.\n\n\u232b \u2423q \u2424\n.\n\u017e\n\u017e .\n.\n\u232b \u2423 \u232b \u2424\n\n\u017e\n\n\u2423y1\n\n\u2432\n\n1 y \u2432\n\n.\n\n\u017e\n\n\u2424y1\n\n,\n\n0 f \u2432f 1,\n\n\u017e\n\n13.8\n\n.\n\nwith parameters \u2423) 0 and \u2424) 0, for the gamma function \u232b \u2b48 . let\n\n\u017e .\n\n\u242es\n\n\u2423\n\n\u2423q \u2424\n\n, \u242as 1r \u2423q \u2424 .\n.\n\n\u017e\n\nthe beta distribution for \u2432 has mean and variance\n\ne \u2432 s \u242e,\n\u017e\n\n.\n\nvar \u2432 s \u242e 1 y \u242e \u242ar 1 q \u242a .\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nwhen \u2423 and \u2424 exceed 1.0, the distribution is unimodal, with skew to the\nright when \u2423- \u2424, skew to the left with \u2423) \u2424, and symmetry when \u2423s \u2424.\nit simplifies to the uniform distribution when \u2423s \u2424s 1.\n\nmarginally, averaging with respect to the beta distribution for \u2432, y has\n\nthe beta-binomial distribution. its mass function is\n\np y; \u2423, \u2424 s\n\u017e\n\n.\n\n\u017e\n\nn\n\n/y\n\nb \u2423q y, n q \u2424y y\n\u017e\n\n.\n\nb \u2423, \u2424\u017e\n.\n\n,\n\ny s 0, 1, . . . , n.\n\nin terms of \u242e and \u242a, the beta-binomial mass function is\n\np y; \u242e, \u242a s\n\u017e\n\n.\n\n\u0142\n\nyy1\nks0\n\n\u017e\n\n\u017e\n\nn\n\n/y\n\n\u242eq k\u242a \u0142\n\u017e\n\n.\nny1\nks0\n\n\u0142\n\n1 y \u242eq k\u242a\nnyyy1\n\u017e\nks0\n1 q k\u242a\n.\n\n.\n\n.\n\n\u017e\n\n13.9\n\n.\n\nit is easier to understand the nature of this distribution from its moments\nthan from its mass function. the first two moments are\n\ne y s n\u242e,\n\u017e\n\n.\n\nvar y s n\u242e 1 y \u242e 1 q n y 1 \u242ar 1 q \u242a .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 570, "text": "beta-binomial models\n555\nas \u242a\u2122 0 in the beta distribution, var \u2432 \u2122 0 and that distribution converges\nto a degenerate distribution at \u242e. then var y \u2122 n\u242e 1 y \u242e and the beta-\n.\n.\nbinomial distribution converges to the bin n, \u242e .\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n13.3.2 models using the beta-binomial distribution\nmodels using the beta-binomial distribution permit \u242e and hence e y to\ndepend on explanatory variables. the simplest models let \u242a be the same\nunknown constant for all observations. prentice 1986 considered extensions\nwhere it could also depend on covariates. like glms, models can use\nvarious link functions, but the logit is most common. for observation i with\nn trials, assuming that y has a beta-binomial distribution with index n and\ni\nparameters \u242e, \u242a , the model links \u242e to predictors by\n\n.x\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nw\n\nx\n\nw\n\ni\n\ni\n\ni\n\ni\n\nlogit \u242e s \u2423q \u2424x x .\n\n\u017e\n\n.i\n\ni\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nthe beta-binomial is not in the natural exponential family, even for known\n\u242a. articles using beta-binomial models have employed a variety of fitting\nmethods note 13.4 . crowder 1978 discussed the likelihood behavior for an\nanova-type model. hinde and demetrio 1998 obtained the ml fit by\niterating between solving the likelihood equations for the regression parame-\nters \u2424, for fixed \u242a, and solving the likelihood equation for \u242a for fixed \u2424.\n.\neach part can use newton\u1390raphson. mcculloch and searle 2001, p. 61\n\u017e\nshowed the asymptotic covariance matrix of \u242e, \u242a and of \u2423, \u2424 for\nindependent observations from a single beta-binomial distribution.\n\n\u02c6\n.\n\n\u00b4\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\na related but simpler approach for overdispersed binary counts uses\nquasi-likelihood with similar variance function as the beta-binomial. the\nquasi-likelihood variance function is\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u00ae \u242e s n \u242e 1 y \u242e 1 q n y 1 \u2433\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u017e\n\n13.10\n\n.\n\n<\n\n<\n\n.\n\n\u017e\n\nwith \u2433 f 1. although motivated by the beta-binomial model, this variance\nfunction results merely from assuming that \u2432 has a distribution with\nvar \u2432 s \u2433\u242e 1 y \u242e . it also results from assuming a common correlation \u2433\nbetween each pair of the n individual binary random variables that sum to y\naltham 1978 . the ordinary binomial variance results when \u2433s 0. overdis-\n\u017e\npersion occurs when \u2433) 0.\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nfor this quasi-likelihood approach, williams 1982 gave an iterative\nroutine for estimating \u2424 and the overdispersion parameter \u2433. he let \u2433 be\u02c6\nsuch that the resulting pearson x 2 that sums the squared pearson residuals\nfor this variance function equals the residual df for the model. this requires\nan iterative two-step process of 1 solving the quasi-likelihood equations for\n\u2424 for a given \u2433, and then 2 using the updated \u2424, solving for \u2433 in the\nequation that equates x which depends on \u2424 and \u2433 to its df.\n\n\u017e .\n\n\u017e .\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n2\n\n\u02c6\n\u02c6\n\n\u017e\n\n.\n\n "}, {"Page_number": 571, "text": "556\n\nother mixture models for categorical data\n\nan alternative quasi-likelihood approach uses the simpler variance func-\n\ntion\n\ni\n\ni\n\ni\n\ni\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u02c6\n\n\u02c6\n\n2\n.\n\n\u00ae \u242e s \u243en \u242e 1 y \u242e\n\u017e\n\n.\n13.11\nintroduced in section 4.7.3. the ordinary binomial variance has \u243es 1.0 and\noverdispersion has \u243e) 1. with this approach, \u2424 is the same as its ml\nestimate for the ordinary binomial model. commonly, \u243es x rdf, where x\n2\nis the pearson fit statistic for the binomial model finney 1947 . the standard\nerrors for the overdispersion approach multiply those for the binomial model\n\u02c61r2\nby \u243e .\nliang and mccullagh 1993 showed several examples using these two\nvariance functions. a plot of the standardized residuals for the ordinary\nbinomial model against the indices n can provide insight about which is\nmore appropriate. when the residuals show an increasing trend in their\nspread as n increases, the beta-binomial-type variance function may be more\nappropriate. this\nis because when the beta-binomial variance holds,\nthe residuals from an ordinary binomial model have denominator that is\nprogressively too small as n increases. the two quasi-likelihood approaches\nare equivalent when n are identical. only when the indices vary consider-\nably might results differ much. because the variance function \u00ae \u242e s\n\u243en \u242e 1 y \u242e has a structural problem when n s 1 problem 13.33 and has\nless direct motivation, we prefer quasi-likelihood with the beta-binomial\nvariance function.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n13.3.3 teratology overdispersion example revisited\nrefer back to table 4.5 on results of a teratology experiment analyzed by\nliang and mccullagh 1993 and moore and tsiatis 1991 . female rats on\niron-deficient diets were assigned to four groups. group 1 was given only\nplacebo injections. the other groups were given injections of an iron supple-\nment according to various schedules. the rats were made pregnant and then\nsacrificed after 3 weeks. for each fetus in each rat\u2019s litter, the response was\nwhether the fetus was dead. because of unmeasured covariates, it is natural\nto permit the probability of death to vary from litter to litter within a\nparticular treatment group.\n\nlet y denote the number dead out of the n fetuses in litter i. let \u2432\nit\ndenote the probability of death for fetus t in litter i. first, suppose that y isi\na bin n ,\u2432 variate, with\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\nit\n\nlogit \u2432 s \u2423q \u2424 z q \u2424 z q \u2424 z ,\n\n\u017e\n\n4\n\n4 i\n\n.it\n\n2\n\n2 i\n\n3 3i\n\nwhere z s 1 if litter i is in group g and 0 otherwise. this model treats all\nlitters in a group g as having the same probability of death, exp \u2423q \u2424 r\n.\nw\n1 q exp \u2423q \u2424 , where \u2424 s 0. however, it has evidence of overdispersion,\n\n.x\n\ng i\n\n\u017e\n\n\u017e\n\ng\n\ng\n\n1\n\n "}, {"Page_number": 572, "text": "557\n\nglmm\n\nbeta-binomial models\n\ntable 13.5 estimates for several logit models fitted to table 4.5\n\ntype of logit model\n\na\n\nparameter\n\nintercept\ngroup 2\ngroup 3\ngroup 4\noverdispersion\n\n1.144 0.129\n\nbinomial ml\n.\n.\n.\n.\n\n\u017e\n\u017e\n\u017e\n\u017e\nnone\n\n1.212 0.223\n\n.\n1.802 0.362\ny3.322 0.331 y3.370 0.563 y3.322 0.560 y3.322 0.440 y4.515 0.736\n.\ny4.476 0.731 y4.585 1.303 y4.476 1.238 y4.476 0.610 y5.855 1.190\n.\ny4.130 0.476 y4.250 0.848 y4.130 0.806 y4.130 0.576 y5.594 0.919\n.\n\n1.144 0.219\n\n1.144 0.276\n\n.\n.\n.\n.\n\n\u017e\n\u017e\n\u017e\n\u017e\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n\u2433s 0.192\n\u02c6\n\n\u243es 2.86\n\u02c6\n\n\u2433s 0.185\n\u02c6\n\n\u2434s 1.53\n\u02c6\n\ngee\n\u017e\n\u017e\n\u017e\n\u017e\n\n\u017e .\nql 1\n\u017e\n\u017e\n\u017e\n\u017e\n\n\u017e .\nql 2\n\u017e\n\u017e\n\u017e\n\u017e\n\na\nbinomial ml assumes no overdispersion, ql 1 is quasi-likelihood with beta-binomial-type\nvariance, ql 2 is quasi-likelihood with inflated binomial variance; ql 2 and gee indepen-\ndence working equations estimates are the same as binomial ml estimates. values in parenthe-\nses are standard errors.\n\n\u017e .\n\n\u017e .\n\n\u017e\n\n.\n\n\u017e .\n\n2\n\n2\n\n.\n\n\u017e\n\nwith x s 154.7 and g s 173.5 df s 54 . table 13.5 shows ml estimates\nand standard errors.\n\ntable 13.5 also shows results for the two quasi-likelihood approaches.\nestimates and standard errors are qualitatively similar for each. for variance\nfunction \u00ae \u242e s \u243en \u242e 1 y \u242e ,\n.\n.\nthe binomial ml\ni\nstandard errors are multiplied by \u243e s x rdf s\nestimates but\n'154.7r54 s 1.69. for the beta-binomial-type variance function, \u2433s 0.192.\nthis fit treats the variance of y as\n\nthe estimates equal\n\n\u02c61r2\n\n2\n\u02c6\n\n1r2\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\ni\n\ni\n\ni\n\ni\n\nn \u242e 1 y \u242e 1 q 0.192 n y 1 .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\nthis corresponds roughly to a doubling of the variance relative to the\nbinomial with a litter size of 6 and a tripling with n s 11. even with these\nadjustments for overdispersion, table 13.5 shows that strong evidence re-\nmains that the probability of death is substantially lower for each treatment\ngroup than the placebo group.\n\nfigure 13.4 plots the standardized pearson residuals against litter size for\nthe binomial logit model. the apparent increase in their variability as litter\nsize increases suggests that the beta-binomial variance function is plausible.\nthe term \u2433 in that variance function corresponds to \u242ar 1 q \u242a in the\nvariance of the beta-binomial distribution. for that distribution or more\ngenerally, \u2433s 0.192 means that the probabilities of death for litters of a\n'\n0.192 \u242e 1 y \u242e . this\nparticular group have estimated standard deviation\nequals 0.22 when the mean is 0.5 and 0.13 when the mean is 0.1 or 0.9, which\nis considerable heterogeneity. more generally, a model could let \u2433 vary by\ntreatment group or be different for the placebo group than the others. we\nleave this to the reader.\n\n\u02c6\n\nfor comparison, table 13.5 also shows results with the gee approach to\nfitting the logit model, assuming an independence working correlation struc-\nture for observations within a litter. the estimates are the same as the ml\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\n "}, {"Page_number": 573, "text": "558\n\nother mixture models for categorical data\n\nfigure 13.4 standardized pearson residuals for binomial logit model fitted to table 4.5.\n\nestimates for the binomial\nlogit model, but the empirical adjustment in-\ncreases the standard errors. similar results occur with an exchangeable\nworking correlation structure. for it, the estimated within-litter correlation\nbetween the binary responses is 0.185. this is comparable to the value of\n0.192 that yields the quasi-likelihood results with beta-binomial variance\nfunction. the gee standard errors are somewhat different from those with\nthe quasi-likelihood approach. it may be that the sample size is insufficient\nfor the gee sandwich adjustment, which tends to underestimate standard\nerrors unless the number of clusters is quite large. or, this may simply reflect\nthe different variance function for the gee approach.\n\nfinally, table 13.5 also shows results for the glmm that adds a normal\nrandom intercept u for litter i to the binomial logit model. results are also\nsimilar in terms of significance of the treatment groups relative to placebo.\nestimated effects are larger for this logistic-normal model, since they are\nsubject-specific i.e., litter-specific rather than population-averaged.\n\n.\n\n\u017e\n\ni\n\n13.3.4 conjugate mixture models\nthe beta-binomial model is an example of a conjugate mixture model. these\nare models for which the marginal distribution has closed form. the data\nhave a particular distribution, conditional on a parameter, and then the\nparameter has its own distribution such that the marginal distribution has\nclosed form.\n\n "}, {"Page_number": 574, "text": "negative binomial refression\n\n559\n\nsimilarly, in bayesian methods the conjugate prior distribution is a distri-\nbution that when combined with the likelihood, gives a closed form for the\nposterior distribution. for instance, for observations from a binomial distri-\nbution with beta prior distribution for the binomial parameter, the posterior\ndistribution of that parameter is also beta. conjugate models were the\nprimary method of conducting bayesian analysis before the development of\ncomputationally intensive methods, such as markov chain monte carlo, for\nevaluating the integral that determines the posterior distribution.\n\nthe beta-binomial conjugate mixture model applies with totals from\nbinary trials. in the next section we study a conjugate mixture model for\ncount data. it uses a gamma distribution to mix the poisson parameter. a\ndisadvantage of the conjugate mixture approach is the lack of generality and\nflexibility, requiring a different mixture distribution for each type of problem.\nin addition, the extra variability need not enter on the same scale as the\nordinary predictors, and it can be difficult to have multivariate random\neffects structure. lee and nelder 1996 discussed this approach and consid-\nered a variety of hierarchical models of glmm form in which the random\neffect need not be normal.\n\n\u017e\n\n.\n\n13.4 negative binomial regression\n\nthe negative binomial is a conjugate mixture distribution for count data. it is\nuseful when overdispersion occurs with poisson glms.\n\n13.4.1 negative binomial as gamma mixture of poisson distributions\nin section 4.3.3 we noted that a severe limitation of poisson models is that\nthe variance of y must equal the mean. hence, at a fixed mean the variance\ncannot decrease as additional predictors enter the model. count data often\nshow overdispersion, with the variance exceeding the mean. this might\nhappen, for instance, because some relevant explanatory variables are not in\nthe model. a mixture model is a flexible way to account for overdispersion.\nat a fixed setting of the predictors used, given the mean the distribution of y\nis poisson, but the mean itself varies according to some distribution.\n\nsuppose that 1 given \u242d, y has a poisson distribution with mean \u242d, and\n\u017e .\n2 \u242d has a gamma distribution, g k, \u242e . the gamma probability density\nfunction for \u242d is\n\n\u017e .\n\n\u017e\n\n.\n\nf \u242d; k, \u242e s\n\u017e\n\n.\n\nk\n\nkr\u242e\u017e\n.\n.\n\u232b k\u017e\n\nthis gamma distribution has\n.\n\ne \u242d s \u242e,\n\u017e\n\nexp yk\u242dr\u242e \u242d , \u242dg 0.\n\nky1\n\n\u017e\n\n.\n\n\u017e\n\n13.12\n\n.\n\nvar \u242d s \u242e2rk.\n\n\u017e\n\n.\n\n "}, {"Page_number": 575, "text": "560\n\nother mixture models for categorical data\n\nthe parameter k ) 0 describes the shape. the density is skewed to the right,\nbut the degree of skewness decreases as k increases.\n\nmarginally, the gamma mixture of the poisson distributions yields the\n\nnegative binomial distribution for y. its probability mass function is\n\np y; k, \u242e s\n\u017e\n\n.\n\n.\n\n\u232b y q k\n\u017e\n.\n\n\u017e\n\n\u232b k \u232b y q 1 \u242eq k\n\u017e\n\n.\n\nk\n\n\u017e\n\n/\n\nk\n\n\u017e\n\n1 y\n\nk\n\n\u242eq k\n\n/\n\ny\n\n,\n\ny s 0, 1, 2, . . . .\n\n\u017e\n\n13.13\n\n.\n\nthis negative binomial distribution has\n\ne y s \u242e,\n\u017e\n\n.\n\nvar y s \u242eq \u242e2rk .\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e .\n\nthe index ky1 is called the dispersion parameter. as ky1 \u2122 0, the gamma\ndistribution has var \u242d \u2122 0 and it converges to a degenerate distribution at\n\u242e; similarly, the negative binomial distribution then has var y \u2122 \u242e and it\nconverges to the poisson distribution with mean \u242e.\nfor given ky1, the negative binomial is in the natural exponential family.\nthe natural parameter is log \u242er \u242eq k . usually, though, the dispersion\nparameter ky1 is itself unknown. estimating it helps to summarize the extent\nof overdispersion. the greater ky1, the greater the overdispersion compared\nto the ordinary poisson glm. for independent observations, the ml esti-\nmate of \u242e is the sample mean, but ml estimation for ky1 requires iterative\nmethods r. a. fisher showed this in an appendix of a 1953 biometrics article\nby c. bliss . problem 13.40 shows an alternative gamma parameterization\nthat implies a linear rather than quadratic variance function for the negative\nbinomial.\n\n.x\n\n\u017e\n\n\u017e\n\n.\n\nw\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n13.4.2 negative binomial regression modeling\nnegative binomial models for counts permit \u242e to depend on explanatory\nvariables lawless 1987 . such models normally take k\nto be the same for\nall observations. this corresponds to a constant coefficient of variation in the\n'\nvar \u242d re \u242d s 1r k . with the standard devi-\ngamma mixing distribution,\nation increasing as the mean does. most common is the log link, as in poisson\nloglinear models. sometimes the identity link is adequate. one such case is\nwith a single predictor that is a factor.\n\n\u017e .\n\n'\n\ny1\n\nfor k fixed, a negative binomial model is a glm. thus, the likelihood\nequations for the regression parameters \u2424 are special cases of those see\n.x\nfor an ordinary glm with variance function \u00ae \u242e s \u242eq \u242e rk. the\n\u017e\n4.22\nusual\niterative reweighted least squares algorithm applies for ml model\nfitting. when k is unknown, ml fitting can use a newton\u1390raphson routine\non all the parameters simultaneously. or, one can evaluate the profile\nlikelihood for various fixed k lawless 1987 . another approach alternates\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nw\n\n2\n\n "}, {"Page_number": 576, "text": "negative binomial refression\n\n561\n\n\u017e .\n\nbetween 1 using iterative reweighted least squares to solve the equations for\n\u2424, for fixed k, and 2 for fixed \u2424, using newton\u1390raphson to estimate k,\niterating between them until convergence.\n\n\u017e .\n\n\u02c6\n\nthe full log likelihood l \u2424, k; y for a negative binomial model satisfies\n\n\u017e\n\n.\n\n\u2b782l\n\u2b78\u2424 \u2b78k\n\nj\n\ns\n\n\u00fd\n\ni\n\n\u017e\n\ny y \u242e\n\ni\n\ni\n\nk q \u242e g \u242e\n\n.\n\n\u017e\n\n2\n\nx\n\ni\n\nx .\nit\n\n.\n\ni\n\nj\n\n2\n\n\u017e\n\n.\n\nthus, e \u2b78 lr\u2b78\u2424\u2b78k s 0 for each j. similarly, the inverse of the expected\ninformation matrix has 0 elements connecting k with each \u2424. since this is\nthe asymptotic covariance matrix, \u2424 and k are asymptotically independent.\nit follows that standard errors for \u2424 obtained from part 1 of the iterative\n.\nscheme above are correct. cameron and trivedi 1998, p. 72 showed the\nasymptotic covariance matrix. they\nconsidered a\nmoment estimator for ky1 and studied robustness properties of estimators.\nthey noted that \u2424 from this model is consistent if the model for the mean is\ncorrectly specified, even if the true distribution is not negative binomial.\n\n\u017e\nw\nand lawless\n\n\u017e .\n.x\n\u017e\n1987\n\n\u02c6\n\u02c6\n\n\u02c6\n\n\u02c6\n\nj\n\n13.4.3 frequency of knowing homicide victims example\ntable 13.6 summarizes responses of 1308 subjects to the question: within the\npast 12 months, how many people have you known personally that were\nvictims of homicide? the table shows responses by race, for those who\nidentified their race as white or as black. the sample mean for the 159 blacks\nwas 0.522, with a variance of 1.150. the sample mean for the 1149 whites was\n0.092, with a variance of 0.155.\n\na natural first choice for modeling count data is a poisson glm, such as a\nloglinear model with a dummy predictor for race. let y denote the response\nfor subject t of race i. for \u242e s e y , this model is\n\n\u017e\n\n.\n\nit\n\nit\n\nit\n\nlog \u242e s \u2423q \u2424x ,\n\nit\n\nit\n\ntable 13.6 number of victims of murder known in past year, by race,\nwith fit of poisson and negative binomial models\n\ndata\n\nresponse black white\n1070\n60\n14\n4\n0\n0\n1\n\n119\n16\n12\n7\n3\n2\n0\n\n0\n1\n2\n3\n4\n5\n6\n\npoisson glm neg. bin. glm poisson glmm\nblack white\nblack white\n1068.3\n116.7\n1047.7\n94.3\n49.2\n96.7\n24.5\n65.3\n10.1\n8.1\n4.5\n12.9\n2.8\n3.6\n0.1\n2.2\n1.1\n1.9\n0.0\n0.3\n0.5\n1.1\n0.0\n0.0\n0.0\n0.0\n0.7\n0.3\n\nblack white\n1064.9\n122.8\n17.9\n67.5\n12.7\n7.8\n2.9\n4.1\n0.7\n2.4\n0.2\n1.4\n0.9\n0.1\n\nsource: 1990 general social survey, national opinion research center.\n\n "}, {"Page_number": 577, "text": "it\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n1t\n\n2 t\n\n562\nother mixture models for categorical data\nwith x s 1 blacks and x s 0 whites . this model has fit log \u242e s y2.38\nq 1.733 x . the estimated expected responses are exp y2.38 q 1.733 s\n0.522 for blacks and exp y2.38 s 0.092 for whites, the sample means. for\nany link function for this model, the likelihood equations imply that the fitted\nmeans equal the sample means. since \u2424s 1.733 se s 0.147 is the differ-\nence between the log means for blacks and whites, the ratio of sample means\nis exp 1.733 s 5.7 s 0.522r0.092. however, for each race the sample vari-\nance is roughly double the mean. table 13.6 also shows the fit of this model.\nthe evidence of overdispersion is reflected by the higher observed counts at\ny s 0 and at large y values than the poisson glm predicts.\n\n\u02c6\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nit\n\n\u02c6\n\nan alternative is the same model form but assuming a negative binomial\nresponse. a mixture model does seem plausible. due to various demographic\nfactors, heterogeneity probably occurs among subjects of a given race in the\ndistribution of y. for ml fitting, the deviance decreases by 122.2 compared\nto the ordinary poisson glm that is the special case with ky1 s 0. table\n13.6 also shows this model fit. it is dramatically better at y s 0 and 1.\ntable 13.7 shows parameter estimates for the negative binomial and\npoisson glms. for both, \u2424s 1.733 since both models provide fitted means\n\u02c6\nequal to the sample means. however the estimated standard error of \u2424\nincreases from 0.147 for the poisson glm to 0.238 for the negative binomial\nmodel. the wald 95% confidence interval for the ratio of means for blacks\nand whites goes from exp 1.733 \" 1.96 0.147 s 4.2, 7.5 for the poisson\nglm to exp 1.733 \" 1.96 0.238 s 3.5, 9.0 for the negative binomial. in\naccounting for the overdispersion, we obtain results that are not as precise as\nthe more naive model suggests.\nthe negative binomial model has k s 4.94 se s 1.00 . this shows\nstrong evidence that ky1 ) 0, indicating that the negative binomial model is\nmore appropriate than the poisson glm. the estimated variance of y is\n\u242eq \u242e rk s \u242eq 4.94\u242e , which is 0.13 for whites and 1.87 for blacks, much\n\u02c6\ncloser to the sample values than the poisson model provides.\ntable 13.7 also shows results for negative binomial and poisson models\nusing the identity link. the fits \u242e s 0.092 q 0.430 x\nreproduce the sample\nmeans. now \u2424 refers to the difference in means rather than their log ratio.\nthe estimated difference \u2424s 0.430 has se s 0.058 for the poisson model\nand se s 0.109 for the negative binomial. results are more imprecise but\n\n2 \u02c6\n\n\u02c6y1\n\n\u02c6it\n\n.x\n\n.x\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nw\n\nw\n\nit\n\n2\n\ntable 13.7 parameter estimates for models fitted to homicide data\n\nmodels with log link\n\nneg. binom.\n\nglm\n\ny2.38\n1.733\n0.238\n\npoisson\nglm\ny2.38\n1.733\n0.147\n\npoisson\nglmm\ny3.69\n1.897\n0.246\n\nmodels with identity link\npoisson\nneg. binom.\nglm\n0.092\n0.430\n0.058\n\nglm\n0.092\n0.430\n0.109\n\nterm\n\u2423\n\u2424\n\u02c6\u017e\n.\nse \u2424\n\n "}, {"Page_number": 578, "text": "poisson regression with random effects\n\n563\n\nmore realistic with the negative binomial model. for this link also the\nestimated dispersion parameter is k s 4.94.\n\n\u02c6y1\n\n13.5 poisson regression with random effects\n\nthe glmms introduced in chapter 12 referred to categorical responses.\nglmms are also useful for other types of discrete responses, such as counts.\nthis section illustrates with poisson regression modeling of count data.\n\nwe\u2019ve seen that a flexible way to account for overdispersion is with a\nmixture model. in section 13.4 we mixed the poisson using the gamma\ndistribution, yielding the negative binomial marginally. breslow 1984 and\nhinde 1982 suggested the glmm structure 12.1 with the log link and\nnormal random intercept. the model for the mean for observation t\nin\ncluster i is\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nlog e y u s x \u2424 q u ,\n\n<\n\n\u017e\n\n.\n\nx\nit\n\nit\n\ni\n\ni\n\n\u017e\n\n13.14\n\n.\n\n\u0004\n\n4\n\n\u017e\n\n2.\n\ni\n\ni\n\nit\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nwhere u are independent n 0, \u2434 . conditional on u , y has a poisson\ndistribution. marginally, the distribution has variance greater than the mean\nwhenever \u2434) 0.\n\napplications of poisson glmms include the analysis of maps of cancer\nrates in epidemiology breslow and clayton 1993 and modeling variability in\nbacteria counts aitchison and ho 1989 . although links other than the log\nare possible, the identity link and any other link having range only the\npositive real line has a structural problem. with a normal random effect\nwith \u2434) 0, a positive probability exists that the linear predictor is negative,\nbut the poisson mean must be nonnegative.\n\nthe negative binomial model\n\n\u017e\nfor fixed k is a glmm with nonnormal\nrandom effect. with the log link, it results from a loglinear model of form\n\u017e\n13.14 with random intercept, where exp u has a gamma distribution with\nmean 1 and variance ky1. with identity link, negative binomial models\nusually work better than poisson glmms. regardless of the gamma mixture\ndistribution, the resulting marginal mean is nonnegative for the negative\nbinomial.\n\n.\n\n\u017e\n\n.\n\n.\n\ni\n\n13.5.1 marginal model implied by poisson glmm\nthe poisson glmm 13.14 implies a relatively simple marginal model,\naveraging out the random effect. the mean of the marginal distribution is\n\n\u017e\n\n.\n\nit\n\n.\n\nw\ne y s e e y u s e e\n\u017e\n.x\n\n\u017e\n\n.\n\nit\n\ni\n\n<\n\nx \u2424qu\nx\nit\n\ni\n\nx\n\ns e\n\nx \u2424q\u2434 r2\nx\nit\n\n2\n\n.\n\nw\n\nhere e exp u s exp \u2434 r2 because a n 0, \u2434 variate u has moment\ngenerating function e exp tu s exp t \u2434 r2 . so, for the poisson glmm\n\n.\n.x\n\n\u017e 2\n\n2\n\u017e\n\n\u017e\nw\n\n2.\n\n\u017e\n\n.\n\n\u017e\n\n2\n\ni\n\ni\n\ni\n\n "}, {"Page_number": 579, "text": "564\nother mixture models for categorical data\nthe log of the mean conditionally equals xx \u2424 q u and marginally equals\nxx \u2424 q \u24342r2. a loglinear model still applies. the marginal effects of the\nit\nexplanatory variables are the same as the cluster-specific effects. thus, the\nratio of means at two different settings of xx\nis the same conditionally\nit\nand marginally. however, marginally the intercept\nis offset. note that\n.\njensen\u2019s inequality applies, since the link is not linear.\n\n\u017e\n\nit\n\ni\n\nx \u2424qu\nx\nit\n\ni\n\nx\n\nq e\n.\n\n2x \u2424\n\nx\nit\n\ni\n\nu\n\n.\n\u017e\nvar e\ne y 1 .\n.\n\n\u2434\n\n2\n\nthe variance of the marginal distribution is\nw\nvar y s e var y u q var e y u s e e\n.\n\nit\nx \u2424q\u2434 r2\nx\nit\n\n.\nq e\n\ns e\n\n2x \u2424\n\n2\n2 \u2434\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nit\n\nit\n\n2\n\ni\n\ni\n\n<\n\n<\n\nit\n\nw\n\nx\nit\n\n.\n\n2\n\u2434\n\n\u017e u i.\n\n\u017e 2 u i.\n\ne y e s e y q e y\n\u017e\n\u017e\n\u017e u i.x2\n\n\u017e\nhere, var e s e e y e e s e y e\nby evaluating the moment\ngenerating function at t s 2 and t s 1. as in the negative binomial model,\nthe marginal variance is a quadratic function of the marginal mean. it\nexceeds the marginal mean when \u2434) 0. the ordinary poisson model results\nwhen \u2434s 0. when \u2434) 0 the marginal distribution is not poisson, and the\nextent to which the variance exceeds the mean increases as \u2434 increases.\n\n2 \u24342\n\n\u24342\n\n\u017e\n\nit\n\n2\n\nas in binary glmms, y and y are independent given u but are\n\nit\n\ni s\n\ni\n\nmarginally nonnegatively correlated. for t / s,\n\ncov y , y s e cov y , y u q cov e y u , e y u\n<\n\n\u017e\n\n.\n\n<\n\n<\n\n\u017e\n\nit\n\ni s\n\n.\n\ni\n\n.\n\ni\n\ni s\n\nit\n\n.\n\ni\n\ns 0 q cov exp x \u2424 q u , exp x \u2424 q u\n\n.\n\nx\nit\n\ni\n\n.\n\n\u017e\n\n13.15\n\n.\n\n\u017e\n\u017e\n\nit\n\nx\ni s\n\n\u017e\n.\n\ni\n\ni s\n\u017e\n\nthe functions in the last covariance term are both monotone increasing\n.\nfunctions of u , and hence are nonnegatively correlated problem 13.44 .\n\n\u017e\n\ni\n\n13.5.2 frequency of knowing homicide victims example\nwe now return to table 13.6 on responses, classified by race, of the number\nof victims of homicide within the past 12 months that subjects knew person-\nally. models permitting subject heterogeneity are sensible. for the response\ny\nit\n\nfor subject t of race i, the poisson glmm is\n\nlog e y u s \u2423q \u2424x q u ,\n\n<\n\n\u017e\n\n.\n\nit\n\nit\n\nit\n\nit\n\nit\n\nit\n\nit\n\n4\n\n\u017e\n\n\u017e\n\n2.\n\n\u0004\nare independent n 0, \u2434 . the log means vary according to a\nwhere u\nn \u2423, \u2434 distribution for whites and a n \u2423q \u2424, \u2434 distribution for blacks.\n2.\n\u017e\ngiven u , y has a poisson distribution.\ntable 13.6 also shows this model fit, and table 13.7 shows estimates. the\nrandom effects have \u2434s 1.63 se s 0.15 . the deviance decreases by 116.6\ncompared to the poisson glm, indicating a better fit by allowing heterogene-\nity. for subjects at the means of the random effects distributions u s 0 the\nestimated expected responses are exp y3.69 q 1.90 s 0.167 for blacks and\n\n2.\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\nit\n\n "}, {"Page_number": 580, "text": "2\n\n.\n\n\u017e\n\n565\nnotes\nexp y3.69 s 0.025 for whites. the fitted marginal mean is exp \u2423q \u2424x q\n.\u2434 r2 , or 0.63 for blacks and 0.09 for whites. the fitted marginal variances\n\u02c6\nare 0.21 for blacks and 5.78 for whites. these are somewhat larger than the\nsample means and variances, perhaps because the fitted distribution has\nnonnegligible mass above the largest observed response of 6.\n\n\u02c6\n\n\u02c6\n\n\u017e\n\nit\n\n13.5.3 negative binomial models versus poisson glmms\nthe poisson glmm with normal random effects has the advantage, relative\nto the negative binomial glm, of easily permitting multivariate random\neffects and multilevel models. however, the negative binomial has properties\nthat can make interpretation simpler. we\u2019ve seen that the identity link is\nvalid for it, which is useful for simple examples such as the preceding one\nwith a factor predictor. with any link and a factor predictor, its ml fitted\nmeans equal the sample means. this is not the case for the poisson glmm.\nbesides the poisson glmm and the negative binomial model, an alterna-\ntive way of accounting for overdispersion with count data is quasi-likelihood\nwith variance function\n\n\u00ae \u242e s \u243e\u242e ,\n\u017e\n\n.i\n\ni\n\nfor some constant \u243e. this is often adequate for exploratory analyses.\n\nnotes\n\nsection 13.1: latent class models\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n13.1. aitkin et al. 1981 , bartholomew and knott 1999 , clogg 1995 , clogg and goodman\n\u017e\n1984 , goodman 1974 , haberman 1979, chap. 10 , hagenaars 1998 , heinen\n\u017e\n1996 , and lazarsfeld and henry 1968 discussed fitting and intrepretation of latent\nclass and related latent variable models.\n\n.\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n1\n\n.\n\n\u017e\n\n13.2. rudas et al. 1994 proposed a clever mixture method for summarizing goodness of fit.\nfor a model m for a contingency table with true probabilities \u2432, they used the mixture\n\u2432 s 1 y \u2433 \u2432 q \u2433\u2432 , with \u2432 the model-based probabilities and \u2432 unconstrained.\ntheir index of lack of fit is the smallest such \u2433 possible for which this holds. it is the\nfraction of the population that cannot be described by the model. this recognizes that\nany given model does not truly hold but is useful if \u2433 is close to 0. the mixture\nin which both \u2432 and \u2432 correspond to\ncontrasts with the latent class model\nindependence.\n\n2\n\n1\n\n2\n\n1\n\n2\n\nsection 13.2: nonparametric random effects models\n\n.\n\n13.3. for connections between rasch-type models and quasi-symmetry models, see agresti\n\u017e\n1993 , conaway 1989 , darroch 1981 , darroch et al. 1993 , hatzinger 1989 , and\nkelderman 1984 . for the matched-pairs random effects model 12.16 , a nonparamet-\nric or conditional ml treatment of u , u\nimplies a multivariate quasi-symmetry\nmodel agresti 1997 . model 12.16 with correlated normal random effects is a\n\n\u017e\n.\n\n\u017e\n.\n\ni2\n\ni1\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n "}, {"Page_number": 581, "text": "566\n\nother mixture models for categorical data\n\ncontinuous analog to discrete latent class models that goodman 1974 proposed, based\non two associated binary latent variables.\n\n\u017e\n\n.\n\nsection 13.3: beta-binomial models\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n.\n\n13.4. skellam 1948 introduced the beta-binomial distribution and discussed parameter\nestimation. for modeling using this distribution or related quasi-likelihood approaches,\nsee brooks et al. 1997 , crowder 1978 , hinde 1996 , lee and nelder 1996 , liang\nand hanfelt 1994 , liang and mccullagh 1993 , lindsey and altham 1998 , moore\n\u017e\n1986a , moore and tsiatis 1991 , nelder and pregibon 1987 , prentice 1986 , rosner\n\u017e\n1984, 1989 with critique by neuhaus and jewell 1990a , slaton et al. 2000 , and\nwilliams 1975, 1982 . for beta-binomial type variance, ryan 1995 and williams\n\u017e\n1988 showed advantages of the quasi-likelihood approach over ml. often, it helps to\npermit the quasi-likelihood scale parameter \u2433 or the related parameter \u242a in the\nbeta-binomial\n\nto vary among groups.\n\n. w\n\n\u017e\n.\n\n\u017e\n\u017e\n\n.\n.\n\n.x\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nthe beta-binomial generalizes to a dirichlet-multinomial. conditional on the prob-\nabilities, the distribution is multinomial. the probabilities themselves have a dirichlet\ndistribution, which is a generalization of the beta defined on vectors of probabilities\n.\nthat sum to 1. see mosimann 1962 and paul et al. 1989 .\n\n\u017e\n\n.\n\n\u017e\n\n13.5. kupper et al. 1986 and ryan 1992 discussed modeling overdispersion caused by\n.\n\u017e\nlitter effects in developmental toxicity studies. see follman and lambert 1989 ,\nkupper and haseman 1978 , and lefkopoulou et al. 1989 for related material.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nsection 13.4: negati\u00a9e binomial regression\n\n13.6. greenwood and yule 1920 derived the negative binomial as a gamma mixture of\npoissons. johnson et al. 1992 summarized its properties. biggeri 1998 , cameron and\ntrivedi 1998 , hinde and demetrio 1998 , and lawless 1987 discussed modeling\nusing it.\n\n\u017e\n.\n\n\u00b4\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nproblems\n\napplications\n\n3\n\nit is saturated. for each latent class, report\n\n.\n13.1 for the 2 table of opinions about legalized abortion table 10.13\ncollapsed over gender, fit a latent class model with two classes. show\nthat\nthe estimated\nprobability of supporting legalized abortion in each of the three\nsituations. give a tentative interpretation for the classes.\n13.2 analyze table 8.3 using a latent class model with q s 2.\n\n\u017e\n\na. for a subject in the first latent class, estimate the probability of\n\u017e\niv all three,\n\n\u017e\niii cigarettes,\n\n\u017e .\nii alcohol,\n\n\u017e .\n\n.\n\n.\n\nhaving used i marijuana,\nand v none of them.\n\n\u017e .\n\nb. estimate the probability a subject is in the first latent class, given\n\u017e\niv all\n\n\u017e\niii cigarettes,\n\n\u017e .\nii alcohol,\n\n\u017e .\n\n.\n\n.\n\nthey have used i marijuana,\nthree, and v none of them.\n\n\u017e .\n\n "}, {"Page_number": 582, "text": "problems\n\n567\n\n13.3 analyze table 8.19 on government spending using latent class mod-\n\nels.\n\n13.4 for capture\u1390recapture experiments, coull and agresti 1999 used a\nloglinear model with exchangeable association and no higher-order\nterms. explain why the model expected frequencies satisfy\n\n\u017e\n\n.\n\nlog \u242e y , . . . , y s \u242dq \u2424 y q \u2b48\u2b48\u2b48 q\u2424 y\n\n\u017e\n\n.\n\n1\n\n1\n\nt t\n\nt\n\n1\n\nq \u2424 y y q y y q \u2b48\u2b48\u2b48 qy\n\n\u017e\n\n1\n\n3\n\n1\n\n2\n\ny\nty1 t\n\n.\n\n.\n\nshow that the fit of this model to table 12.6 yields n s 90.5 and a\n.\n95% profile-likelihood confidence interval for n of 75, 125 .\n\n\u02c6\n\n\u017e\n\n13.5 use or write software to replicate the analyses of the opinions about\nabortion data in section 13.2 using a nonparametric random effects\nfitting of logit model 13.3 , and b the quasi-symmetry model.\n\n\u017e .\n\n\u017e .\n\n\u017e\n\n.\n\ni\n\n13.6 a data set on pregnancy rates among girls under 18 years of age in 13\nnorth central florida counties has information on a 3-year total for\neach county i on n s number of births and y s number of those for\nwhich mother had age under 18 see j. booth, in statistical modelling:\n.\nlecture notes in statistics, 104, springer, 43\u139052, 1995 .\na. a beta-binomial model states that given \u2432 , y are independent\n.\n\u0004\nbin n , \u2432 variates, and \u2432 are independent from a beta \u2423, \u2424\ndistribution. the ml estimated parameters are \u2423s 9.9 and \u2424s\n240.8 thanks to j. booth for this analysis . use the mean and\nvariance to describe the estimated beta distribution and the esti-\n.\nmated marginal distribution of y as a function of n .\n\n\u02c6\n\n\u02c6\n\n.4\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u017e\n\n.\n\ni\n\n\u017e\n\n.\n\n\u02c6\n\n\u02c6\n\nb. quasi-likelihood using variance function 13.10 for the model\nlogit \u242e s \u2423 has \u2423s y3.18 and \u2433s 0.005. describe the esti-\nmated mean and variance of y .i\nc. quasi-likelihood using variance 13.11 for the model logit \u242e s \u2423\n\u017e\nhas \u2423s y3.35 and \u243es 8.3. describe the estimated mean and\nvariance of y .i\nd. the logistic-normal glmm, logit \u2432 s \u2423q u , yields \u2423s y3.24\n.x\nand \u2434s 0.33. describe the estimated mean of y recall 12.8 .\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\nw\n\ni\n\ni\n\ni\n\n\u02c6\n\ni\n\n13.7 in problem 12.2 about shaq oxneal\u2019s free-throw shooting, the simple\nbinomial model, \u2432 s \u2423, has lack of fit. fit the beta-binomial model,\nor use the quasi-likelihood approach with that variance structure. use\nthe fit to summarize his free-throw shooting, by giving an estimated\nmean and standard deviation for \u2432.i\n\ni\n\n "}, {"Page_number": 583, "text": "568\n\nother mixture models for categorical data\n\n13.8 for the toxicity study of table 12.9, collapsing to a binary response,\n\nconsider linear logit models for the probability a fetus is normal.\na. does the ordinary binomial model show evidence of overdisper-\n\nsion?\n\nb. fit the linear logit model using the quasi-likelihood approach with\n\ninflated binomial variance. how do the standard errors change?\n\nc. fit the linear logit model using quasi-likelihood with beta-binomial\n\nvariance. interpret and compare with previous results.\n\nd. fit the linear logit model using a gee approach with exchange-\nable working correlation among fetuses in the same litter. inter-\npret and compare with previous results, including comparing the\n\u017e .\nestimated gee correlation with the estimate \u2433 from part c .\n\n\u02c6\n\ne. fit the linear logit glmm after adding a litter-specific normal\n\nrandom effect. interpret and compare with previous results.\n\n13.9 extend the various analyses of the teratology data table 4.5 in\n\n\u017e\n\n.\n\nsection 13.3.3 as follows:\na. include a predictor for litter size as well as group . interpret, and\n\ncompare results to those without this predictor.\n.\n\nb. fit a model with beta-binomial variance 13.10 in which \u2433 varies\nby treatment group. use results to motivate a model that allows\noverdispersion only in the placebo group. interpret and compare\nresults to those with common \u2433 for each group.\n\n\u017e\n\n\u017e\n\n.\n\n13.10 table 13.8 reports the results of a study of fish hatching under three\nenvironments. eggs from seven clutches were randomly assigned to\nthree treatments, and the response was whether an egg hatched by\nday 10. the three treatments were 1 carbon dioxide and oxygen\nremoved, 2 carbon dioxide only removed, and 3 neither removed.\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\ntable 13.8 data for problem 13.10\n\ntreatment 1\n\ntreatment 2\n\ntreatment 3\n\nclutch\n\nnumber\nhatched\n\ntotal\n\nnumber\nhatched\n\ntotal\n\nnumber\nhatched\n\ntotal\n\n1\n2\n3\n4\n5\n6\n7\n\n0\n0\n0\n0\n0\n0\n0\n\n6\n13\n10\n16\n32\n7\n21\n\n3\n0\n8\n10\n25\n7\n10\n\n6\n13\n10\n16\n28\n7\n20\n\n0\n0\n6\n9\n23\n5\n4\n\n6\n13\n9\n16\n30\n7\n20\n\nsource: data courtesy of becca hale, zoology department, university of florida.\n\n "}, {"Page_number": 584, "text": "problems\n\n569\n\nit\n\na. let \u2432 denote the probability of hatching for an egg from clutch i\nin treatment t. assuming independent binomial observations, fit\nthe model\n\nlogit \u2432 s \u2424 z q \u2424 z q \u2424 z ,\n\n\u017e\n\n.it\n\n2\n\n2\n\n3\n\n3\n\n1\n\n1\n\nwhere z s 1 for treatment t and 0 otherwise. what does your\nsoftware report for \u2424 , and what should it be? hint: note that\ntreatment 1 has no successes.\n\n\u02c6\n1\n\n.\n\n\u017e\n\nt\n\nb. analyze these data using an approach that allows overdispersion.\ninterpret. indicate whether evidence of overdispersion occurs for\ntreatments 2 and 3.\n\n13.11 for the train accidents in problem 9.19, a negative binomial model\nassuming constant log rate over the 14-year period has estimate\ny4.177 se s 0.153 and estimated dispersion parameter 0.012. in-\nterpret.\n\n.\n\n\u017e\n\n13.12 one question in the 1990 general social survey asked subjects how\nintercourse in the preceding month.\n\nmany times they had sexual\ntable 13.9 shows responses, classified by gender.\na. the sample means were 5.9 for males and 4.3 for females; the\nsample variances were 54.8 and 34.4. the mode for each gender\nwas 0. does an ordinary poisson glm seem appropriate? explain.\nb. the poisson glm with log link and a dummy variable for gender\n1 s males, 0 s females has gender estimate 0.308 se s 0.038 .\n\u017e\n.\nexplain why this implies a ratio of 1.36 for the fitted means. this\nis also the ratio of sample means, since this model has fitted means\nequal to sample means. show that the wald 95% confidence\n.\ninterval for the ratio of means for males and females is 1.26, 1.47 .\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\ntable 13.9 data for problem 13.12\n\nresponse male female response male female response male female\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\n65\n11\n13\n14\n26\n13\n15\n7\n21\n\n128\n17\n23\n16\n19\n17\n17\n3\n15\n\n9\n10\n12\n13\n14\n15\n16\n17\n18\n\n2\n24\n6\n3\n0\n3\n3\n0\n0\n\n2\n13\n10\n3\n1\n10\n1\n1\n1\n\n20\n22\n23\n24\n25\n27\n30\n50\n60\n\n7\n0\n0\n1\n1\n0\n3\n1\n1\n\n6\n1\n1\n0\n3\n1\n1\n0\n0\n\nsource:1990 general social survey, national opinion research center.\n\n "}, {"Page_number": 585, "text": "570\n\nother mixture models for categorical data\n\n\u017e\n\nc. for the negative binomial model, the log likelihood increases by\n248.7 deviance decreases by 497.3 . the estimated difference\nbetween the log means is also 0.308, but now se s 0.127. show\nthat the 95% confidence interval for the ratio of means is 1.06,\n1.75 . compare to the poisson glm, and interpret.\n\n.\n\n.\n\n\u017e\n\nd. the mode for the poisson distribution is the integer part of the\nmean, rather than 0. argue that a possibly more realistic mixture\nmodel assumes for gender i a proportion \u2433 that has a poisson\ndistribution with mean 0 and a proportion 1 y \u2433 that has distribu-\ntion that is a gamma mixture of poissons. explain why the corre-\nsponding marginal distribution for each gender is a mixture of a\ndegenerate distribution at 0 and a negative binomial distribution.\n\ni\n\ni\n\n13.13 refer to problem 13.12. fit the poisson and negative binomial glms\nusing identity link. show that the estimated differences in means\nbetween males and females are identical for the two glms but the\nse values are very different. explain why. use the more appropriate\none to form a confidence interval for the true difference in means.\n\n13.14 for the counts of horseshoe-crab satellites in table 4.3, table 13.10\nshows the results of ml fitting of the negative binomial model using\nwidth as the predictor, with the identity link.\na. state and interpret the prediction equation.\nb. show that at a predicted \u242e, the estimated variance is roughly\n\n\u02c6\n\n\u242eq \u242e2.\n\u02c6\n\u02c6\nc. the corresponding poisson glm has fit \u242es y11.53 q 0.55 x\nse s 0.06 . compare 95% confidence intervals for the slopes for\n\u017e\nthe two models. interpret, and indicate whether overdispersion\nseems to exist relative to the poisson glm.\n\n\u02c6\n\n.\n\ntable 13.10 results for problem 13.14\n\nparameter\nintercept\nwidth\ndispersion\n\nestimate\ny11.1471\n0.5308\n0.9843\n\nstandard\n\nerror\n2.8275\n0.1132\n0.1822\n\nlimits\n\nwald 95% confidence\ny16.6890\ny5.6052\n0.7528\n0.3089\n0.6847\n1.4149\n\nchi-\n\nsquare\n15.54\n21.97\n\n13.15 refer to problem 13.14.\n\na. fit a negative binomial model with log link. interpret. plot the\ncounts against width and indicate which link seems more appropri-\nate.\n\nb. fit a poisson glmm with log link, using width predictor. inter-\n\npret.\n\n "}, {"Page_number": 586, "text": "problems\n\n571\n\nc. compare results for the various models, including those in section\n\n4.3.2 for a poisson glm. indicate your preferred model. justify.\n\n13.16 refer to problems 13.14 and 13.15. using width and qualitative color\nas predictors, fit a a negative binomial glm, and b poisson\nglmm, checking for interaction and interpreting the final model.\n\n\u017e .\n\n\u017e .\n\n13.17 refer to table 13.6. for those with race classified as \u2018\u2018other,\u2019\u2019 the\n.\nsample counts for 0, 1, 2, 3, 4, 5, 6 homicides were 55, 5, 1, 0, 1, 0, 0 .\nfit an appropriate model simultaneously to these data and those for\nwhite and black race categories. interpret by making pairwise com-\nparisons of the three pairs of means.\n\n\u017e\n\n.\n\n\u017e\n\n13.18 use a quasi-likelihood approach to analyze table 13.6 on counts of\n\nmurder victims.\n\n13.19 conduct the analyses of problem 4.6 on defects in the fabrication of\ncomputer chips, but use a negative binomial glm. compare results\nto those for the poisson glm. indicate why results are similar.\n\n13.20 with data at\n\n.\n\n\u017e\n\nthe book\u2019s web site www.stat.ufl.edur;aarcdar\ncda.html , use methods of this chapter to analyze how the countywide\nvote for the reform party candidate pat buchanan in the 2000\npresidential election related to the vote for reform party candidate\nross perot in the 1996 presidential election. note that palm beach\ncounty is an enormous outlier apparently mainly reflecting votes\nintended for al gore but cast for buchanan because of a confusing\nballot . model with and without that observation and compare results.\n\n\u017e\n\n.\n\n13.21 conduct a latent class analysis of the data in espeland and handel-\n\n.\nman 1989 .\n\n\u017e\n\n13.22 refer to the teratology study in liang and hanfelt 1994 . analyze\nthese data using at least two different approaches for overdispersed\nbinary data. compare results and interpret.\n\n\u017e\n\n.\n\n13.23 refer to problem 13.14. using an appropriate subset of width, weight,\ncolor, and spine condition as predictors, find and interpret a reason-\nable model for predicting the number of satellites.\n\ntheory and methods\n\n13.24 derive residual df for a latent class model with q latent classes. when\ni s 2, for q g 2 show one needs t g 4 for the model to be unsatu-\nrated. then, find the maximum value for q when t s 4, 5. for an i 2\ntable, show one needs q - i r 2 i y 1 .\n.\n\n\u017e\n\n2\n\n "}, {"Page_number": 587, "text": "572\n\nother mixture models for categorical data\n\n13.25 express the log likelihood for latent class model 13.1 in terms of the\nmodel parameters. derive likelihood equations goodman 1974,\n.\nhaberman 1979 .\n\n\u017e\n\n\u017e\n\n.\n\n13.26 let \u2338 denote an i = j matrix of cell probabilities for the joint\ni = 1 column\nof probabilities, k s\n\ndistribution of x and y. suppose that there exist\nvectors \u2432\n1, . . . , q, and a set of probabilities \u2433 such that\n\nand j = 1 column vectors \u2432\n\n2 k\n\n1 k\n\n\u0004\n\n4\n\nk\n\n\u2338 s\n\nq\n\n\u00fd k\nks1\n\nx\n\u2433 \u2432 \u2432 .\n2 k\n\n1 k\n\nexplain why this implies that there is a latent variable z such that x\nand y are conditionally independent, given z.\n\n13.27 in section 13.2.2, under the null that the ordinary logistic regression\nmodel holds, explain why it is inappropriate to treat the difference\nbetween the deviances for that model and the mixture of two logistic\nregressions as a chi-squared statistic.\n\n\u017e\n\n.\n\n\u017e\n\n13.28 refer to problem 12.7. let \u242e a, b, c denote the expected frequency\nof outcomes a, b, c for treatments a, b, c under treatment se-\nquence k, where outcome 1 s relief and 0 s nonrelief. with a non-\nparametric random effects approach, show that one can estimate\ntreatment effects in model 12.19 by fitting the quasi-symmetry\nmodel\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\nk\n\nlog \u242e a, b, c s a\u2424 q b\u2424 q c\u2424 q \u242d a, b, c ,\n.\n\n\u017e\n\n.\n\n\u017e\n\nc\n\nk\n\na\n\nb\n\nk\n\nk\n\n.\n\n\u017e\n\nwhere \u242d a, b, c s \u242d a, c, b s \u242d b, a, c s \u242d b, c, a s\n\u242d c, a, b s \u242d c, b, a . fit the model, and show that \u2424 y \u2424 s 1.64\n\u017e\nse s 0.34 , \u2424 y \u2424 s 2.23 se s 0.39 , \u2424 y \u2424 s 0.59 se s\n\u017e\n.\n0.39 . interpret. compare results with problem 12.7 for model 12.19 .\n\n\u017e\nk\n\u02c6\nc\n\nk\u02c6\n\n\u02c6\na\n\n\u02c6\na\n\n\u02c6\nc\n\n\u02c6\nb\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\nb\n\nk\n\nk\n\nk\n\n13.29 show that\n\nthe beta-binomial distribution 13.9 simplifies to the\n\n\u017e\n\n.\n\nbinomial when \u242as 0.\n\n13.30 express the numerator of the beta density in terms of \u242e and \u242a. using\nthis, show that it is a unimodal when \u242a- min \u242e, 1 y \u242e , and b the\nuniform density when \u242es \u242as .2\n\n\u017e .\n\n\u017e .\n\n\u017e\n\n.\n\n1\n\n13.31 suppose that \u2432 s p y s 1 s 1 y p y s 0 , for t s 1, . . . , n , and\n.\nvar y s \u2432 1 y \u2432 ,\n.\n\n\u017e\ncorr y , y s \u2433 for\n\nt / s. show that\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nit\n\nit\n\ni\n\ni\n\nit\n\ni\n\ni\n\nit\n\ni s\n\n "}, {"Page_number": 588, "text": "problems\n\n573\n\ncov y , y s \u2433\u2432 1 y \u2432 , and\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nit\n\ni s\n\ni\n\ni\n\n\u017e\n\nvar\n\n/\ny s n \u2432 1 y \u2432 1 q \u2433 n y 1 .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u00fd it\n\ni\n\ni\n\ni\n\ni\n\nt\n\n13.32 when n s 1, show that the beta-binomial distribution is no different\n\u017e\nfrom the binomial\ni.e., bernoulli . explain why overdispersion cannot\noccur when n s 1.\n\n.\n\ni\n\n13.33 when y is the sum of n binary responses each having mean \u242e, refer\nto the quasi-likelihood approach with \u00ae \u242e s \u243en \u242e 1 y \u242e . explain\nwhy this variance function has a structural problem, with only \u243es 1\nmaking sense when n s 1.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u017e\n\n.\n\n13.34 liang and hanfelt 1994 described a teratology study comparing\ncontrol and treatment groups in which the ml estimate of the\ntreatment effect in a beta-binomial model differs by a factor of 2\ndepending on whether one assumes the same overdispersion parame-\nter for each group. by contrast, with variance function 13.11 , the\nquasi-likelihood estimate of the treatment effect is the same whether\none assumes the same or different \u243e for the two groups. explain why,\nand discuss whether this is an advantage or disadvantage of that\nmethod.\n\n\u017e\n\n.\n\n13.35 consider the logistic-normal model,\n\nlogit \u2432 s \u2423q x \u2424 q u . for\nsmall \u2434, show that it corresponds approximately to a mixture model\nfor which the mixture distribution has var \u2432 s \u242e 1 y \u242e \u2434 .\n2\n\u017e\nhint: see problem 6.33.\n\n.x2\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nx\ni\n\nw\n\ni\n\ni\n\ni\n\ni\n\ni\n\n13.36 altham 1978 introduced the discrete distribution\n\n\u017e\n\n.\n\nf y; \u2432, \u243a s c \u2432, \u243a\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\nn\n\n/y\n\n.\n\n\u2432 1 y \u2432\n\n\u017e\n\ny\n\nnyy\n\n.\n\nexp \u243ay n y y\n\n\u017e\n\n.\n\n,\n\ny s 0,1, . . . , n,\n\n\u017e\n\n.\nfamily. show that\n\nwhere c \u2432, \u243a is a normalizing constant. show that this is in the\nthe binomial occurs when \u243as 0.\nexponential\nwaltham noted that overdispersion occurs when \u243a- 0. corcoran\net al. 2001 and lindsey and altham 1998 used this as the basis of\nan alternative model to the beta-binomial.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nx\n\n13.37 when y , . . . , y\n\nare independent from the negative binomial distri-\n\nbution 13.13 with k fixed, show that \u242es y.\n\n\u017e\n\n.\n\n\u02c6\n\nn\n\n1\n\n "}, {"Page_number": 589, "text": "574\n13.38 using e y s e e y x\n\nother mixture models for categorical data\nand var y s e var y x q\n\u017e\n.x\nvar e y x , derive the mean and variance of the a beta-binomial\ndistribution,\nand b negative binomial distribution.\n\nw\n\u017e .\n\n\u017e .\n\n.x\n\n.x\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nw\n\nw\n\n<\n\n<\n\n<\n\n13.39 suppose that given u, y is poisson with e y u s u\u242e, where \u242e may\ndepend on predictors. suppose that u is a positive random variable\nwith e u s 1 and var u s \u2436. show that e y s \u242e and var y s\n\u242eq \u2436\u242e2. explain how negative binomial glms and poisson glmms\nwith log link can follow as special cases.\n\n\u017e .\n\n\u017e .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n<\n\n13.40 an alternative negative binomial parameterization results from the\n\ngamma density formula,\n\nf \u242d; k, \u242e s\n\u017e\n\n.\n\n.\n\nk \u242ek\u017e\n\u232b k\u242e\u017e\n.\n\nexp yk\u242d \u242d\n\n\u017e\n\n.\n\nk \u242ey1\n\n,\n\n\u242dg 0,\n\nfor which e \u242d s \u242e, var \u242d s \u242erk. show that this gamma mixture of\npoissons yields a negative binomial with\n\n\u017e .\n\n\u017e .\n\ne y s \u242e,\n\u017e\n\n.\n\nvar y s \u242e 1 q k rk .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nfor what limiting value of k does this reduce to the poisson? see\nnelder and lee 1996 for ml model fitting. cameron and trivedi\n\u017e\n1998, p. 75 pointed out that, unlike with quadratic variance, consis-\ntency does not occur for parameter estimators when the model for the\nmean holds but the true distribution is not negative binomial.\n\n.\n\n.\n\nx\n\nw\n\n13.41 the negative binomial distribution is unimodal with a mode at the\ninteger part of \u242e k y 1 rk johnson et al. 1992, pp. 208\u1390209 . show\nthat the mode is 0 when \u242ef 1, and that when \u242e) 1 the mode is still\n0 if k - \u242er \u242ey 1 . this gives greater scope than the poisson, since\nits mode equals the integer part of the mean.\n\n. \u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n13.42 consider the loglinear random effects model\n\nlog e y u s x \u2424 q z u ,\n\n<\n\n\u017e\n\n.\n\nx\nit\n\nx\nit\n\nit\n\ni\n\ni\n\n\u0004\nwhere u\nmarginal loglinear model\n\n4\n\ni\n\nare independent n 0, \u233a . show that this implies the\n\n\u017e\n\n.\n\nlog e y y z \u233az s x \u2424,\n\n\u017e\n\n.it\n\nx\nit\n\nit\n\n1 x\nit\n2\n\n "}, {"Page_number": 590, "text": "problems\n\n575\n\nwith the same fixed effects but with offset term. for the random-in-\ntercept case, indicate the role of \u2434 on the size of the offset. explain\nwhat happens when \u2434s 0.\n\n13.43 in section 13.5.1 and problem 13.42 we saw that for poisson glmms,\nthe marginal effects are the same as the cluster-specific effects. this\ndoes not imply that ml estimates of effects are the same for a\npoisson glmm and a poisson glm. explain why. hint: for the\nglmm, is the marginal distribution poisson?\n\n\u017e\n\n.\n\n13.44 for the poisson glmm 13.14 , use the normal mgf to show that for\n\n\u017e\n\n.\n\nt / s,\n\ncov y , y s exp x q x \u2424 exp \u2434 exp \u2434 y 1\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2\n\n2\n\n\u017e\n\n.\n\nit\n\ni s\n\nx\ni s\n\nx\nit\n\n.\nhence, find corr y , y .\n\n\u017e\n\nit\n\ni s\n\n13.45 consider a poisson glmm using the identity link. relate the marginal\nmean and variance to the conditional mean and variance. explain the\nstructural problem that this model has.\n\n "}, {"Page_number": 591, "text": "c h a p t e r 1 4\n\nasymptotic theory for\nparametric models\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nthis chapter has a more theoretical flavor than others. it presents asymptotic\ntheory for parametric models for categorical data, with emphasis on multino-\nmial models for contingency tables. in section 14.1 we review and extend the\ndelta method. this is used to derive large-sample normal distributions for\nmany statistics. in section 14.2 we apply the delta method to ml estimation\nof parameters in models for contingency tables, later illustrated in section\n14.4 for logit and loglinear models. in section 14.3 we derive asymptotic\ndistributions of cell residuals and the x 2 and g2 goodness-of-fit statistics.\n\n\u00b4\n\nthe results in this chapter have a long history. pearson 1900 derived the\nasymptotic chi-squared distribution of x 2 for testing a specified multinomial\ndistribution. fisher 1922, 1924 showed the adjustment in degrees of free-\ndom when multinomial probabilities are functions of unknown parameters.\ncramer 1946, pp. 424\u1390434 formally proved this result, under the assump-\ntion that ml estimators of the parameters are consistent. rao 1957 proved\nconsistency of the ml estimators under general conditions. he also gave the\nasymptotic distribution of the ml estimators, although the primary emphasis\nof his articles was on proving consistency. birch 1964a proved these results\n.\nunder weaker conditions. andersen 1980 , bishop et al. 1975 , cox 1984 ,\n\u017e\nhaberman 1974a , and watson 1959 provided other proofs or considered\nrelated cases.\n\nas in cramer\u2019s and rao\u2019s proofs, our derivation regards the ml estimator\nas a point in the parameter space where the derivative of the log likelihood\nfunction is zero. birch regarded it as a point at which the likelihood takes\nvalue arbitrarily near its supremum. although his approach is more powerful,\nthe proofs are more complex. we avoid a formal \u2018\u2018theorem\u1390proof\u2019\u2019 style of\nexposition. instead, we show that powerful results follow from simple mathe-\nmatical ideas, such as taylor series expansions.\n\n\u00b4\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n576\n\n "}, {"Page_number": 592, "text": "delta method\n\n14.1 delta method\n\n577\n\nsuppose that a statistic used as an estimator of a parameter has a large-sam-\nple normal distribution. then, in this section we show that many functions of\nthat statistic are also asymptotically normal.\n\n4\n\n\u0004\n\n\u017e\nz , the little o notation o z\nn\n\n14.1.1 o, o rates of convergence\nbig o and little o notation is useful for describing limiting behavior of\nsequences. for real numbers\nrepresents a\nterm that has smaller order than z as n \u2122 \u2b01, in the sense that o z rz \u2122 0\n\u017e\n.\nas n \u2122 \u2b01. for instance, n is o n as n \u2122 \u2b01, since n rn \u2122 0 as n \u2122 \u2b01.\na sequence that is o 1 satisfies o 1 r1 s o 1 \u2122 0; for instance, n\nis\n\u017e .o 1 as n \u2122 \u2b01.\nmagnitude as z , in the sense that o z rz\ninstance, 3rn q 8rn is o n\nthat takes value close to 3 as n increases.\n\nrepresents terms that have the same order of\nis bounded as n \u2122 \u2b01. for\ngives a ratio\n\nas n \u2122 \u2b01; dividing it by n\n\n\u017e\nthe big o notation o z\n\n\u017e y1.\n\nn\n\u017e .\n\ny1r2\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\n'\n\n'\n\ny1\n\n2.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\nn\n\nn\n\nn\n\nn\n\nn\n\nn\n\nn\n\np\n\np\n\np\n\n1\n\n0\n\nn\n\nn\n\nn\n\nn\n\nn\n\nn\n\nn\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\np\n.\nn\nw\n\nfor large n, in the sense that o z rz\n\nsimilar notation applies to sequences of random variables. this notation\nuses a subscript p to indicate that the sequence has probabilistic rather than\n\u017e\ndeterministic behavior. the symbol o z\ndenotes a random variable of\ncon\u00aeerges\nsmaller order than z\nin probability to 0; that is, for any fixed \u2440) 0, p o z rz f \u2440 \u2122 1 as\n.\nn \u2122 \u2b01. the notation o z\n\u017e\nrepresents a random variable such that for every\nx\n\u2440) 0, there is a constant k and an integer n such that p o z rz - k\n) 1 y \u2440 for all n ) n .0\nto illustrate, let y denote the sample mean of n independent observa-\ntions y , . . . , y from a distribution having e y s \u242e. then y y \u242e s\no 1 , since y y \u242e r1 converges in probability to zero as n \u2122 \u2b01 by the law\n\u017e .\np\nof large numbers. by tchebychev\u2019s inequality, the difference between a\nrandom variable and its expected value has the same order of magnitude as\nthe standard deviation of that random variable. since y y \u242e has standard\ndeviation \u2434r n , y y \u242e s o n\ny1r2\n\u017e\n.\n.\np y1r2\nis also o 1 . an example is y y \u242e .\n\u017e\n.\n.\n\u017e\na random variable that is o n\nn\u017e\np\nmultiplication affects the order in the way one expects intuitively problem\ns o 1 .\n\u017e .\n.\n14.1 . for instance,\nif the difference between two random variables is o 1 as n \u2122 \u2b01, slutzky\u2019s\ntheorem states that those random variables have the same limiting distribu-\ntion.\n\n'\nn y y \u242e s n o n\n\u017e\n\u017e\n\ns o n\n\u017e\np\n\u017e .\n\n1r2 y1r2\n\n' \u017e\n\ny1r2\n\n\u017e .\n\n1r2\n\nn\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\nn\n\nn\n\nn\n\nn\n\nn\n\nn\n\nn\n\nn\n\np\n\np\n\np\n\np\n\np\n\ni\n\n14.1.2 delta method for function of random variable\nlet t denote a statistic, the subscript expressing its dependence on the\nsample size n. for large samples, suppose that t is approximately normally\n\nn\n\nn\n\n "}, {"Page_number": 593, "text": "578\nasymptotic theory for parametric models\ndistributed about \u242a, with approximate standard error \u2434r n . more precisely,\nas n \u2122 \u2b01, suppose that the cdf of n t y \u242a converges to a n 0, \u2434 cdf.\nthis limiting behavior is an example of con\u00aeergence in distribution, denoted\n\n' \u017e\n\n'\n\n.\n\n\u017e\n\n.\n\nn\n\n2\n\n'n t y \u242a\n\n\u017e\n\nn\n\n.\n\nd\n\n.\n26\nn 0, \u2434 .\n\n\u017e\n\n\u017e\n\n14.1\n\n.\n\n.\nfor a function g, we now derive the limiting distribution of g t .n\nsuppose that g is at least twice differentiable at \u242a. we use the taylor series\n\u017e .\nin a neighborhood of \u242a. for some \u242a* between t and \u242a,\nexpansion for g t\n\n\u017e\n\ng t s g \u242a q t y \u242a g \u242a q t y \u242a g \u242a* r2\n\u017e .\n\n\u017e\n.\ns g \u242a q t y \u242a g \u242a q o t y \u242a .\n\n.\n.\n\n\u017e\n\u017e\n\n.\n.\n\n\u017e\n\u017e\n\n.\n.\n\n\u017e\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n2\n\n2\n\ny\n\nx\n\nx\n\nsubstituting the random variable t for t, we have\n\nn\n\n.\n\n\u017e\n\n'\nn g t y g \u242a s n t y \u242a g \u242a q n o t y \u242a\n\n\u017e\nn\n's n t y \u242a g \u242a q o n\ny1r2\n\u017e\n\n\u017e\n\u017e\n\n.\n.\n\n\u017e\n\u017e\n\n.\n.\n\n'\n\n'\n\n\u017e\n\n.\n\nn\n\nn\n\nx\n\nx\n\n2\n\n.\n\n\u017e\n\n14.2\n\n.\n\nn\n\np\n\nsince\n\n'\nn o t y \u242a s n o o n\n\u017e\n\n'\n\n2\n\ny1\n\n.\n\n.\n\nn\n\np\n\n\u017e\ny1r2\n\ny1r2\n\np\n\ns o n\n\u017e\n'\n\np\n\n\u017e\n\n.\n\nterm is asymptotically negligible, n g t y g \u242a has\nsince the o n\n' \u017e\nn t y \u242a g \u242a ; that is, g t y g \u242a\n\u017e\n.\nthe same limiting distribution as\nbehaves like the constant multiple g \u242a of t y \u242a . now, t y \u242a is\nx\u017e\n.\n.\napproximately normal with variance \u2434 rn. thus, g t y g \u242a is approxi-\n.\n2\n.x2\nmately normal with variance \u2434 g \u242a rn. more precisely,\n.\n\nx\u017e\n'n g t y g \u242a\n\u017e\n\n\u017e\n26\nn 0, \u2434 g \u242a\n\n14.3\n\n\u017e\n.\n\n2w\n\n\u017e\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\nd\n\n.\n\nn\n\nn\n\nn\n\nn\n\nn\n\nn\n\nn\n\n2\n\nx\n\nx\n\n.x\n\nw\n\n\u017e\n\n.\n2\n\nfigure 3.1 illustrated this result, and in section 3.1.6 it was applied to the\nsample logit.\nresult 14.3 is called the delta method for obtaining asymptotic distribu-\ntions. since \u2434 s \u2434 \u242a and g \u242a usually depends on \u242a, the asymptotic\nvariance is unknown. let \u2434 t and g t denote these terms evaluated at\nthe sample estimator t of \u242a. when g \u2b48 and \u2434s \u2434 \u2b48 are continuous at \u242a,\n\u017e\n\u2434 t g t is a consistent estimator of \u2434 \u242a g \u242a . thus, confidence inter-\nvals and tests use the result that n g t y g \u242a r\u2434 t\n\u017e\n\u017e\nis asymp-\ntotically standard normal. for instance,\n\n. x\u017e\n.x\n\u017e\n\n\u017e\nx\ng t\nn\n\nx\u017e\nx\u017e .\n\n' w\n\nx\u017e\n.\n\n. x\u017e\n\n\u017e .\n\n2\u017e\n\n2\u017e\n\n.\n\n.\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\nn\n\nn\n\nn\n\nn\n\nn\n\nn\n\nn\n\n\u017e\n\u017e\ng t \" z \u2434 t\nn\n\n\u2423r2\n\n.\n\nn\n\n.\n\nx\n\n'\ng t r n\n\u017e\n\n.\n\nn\n\nis a large-sample 100 1 y \u2423 % confidence interval for g \u242a .\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n "}, {"Page_number": 594, "text": "delta method\n\n579\nwhen g \u242a s 0, 14.3 is uninformative because the limiting variance\nequals 0. in that case, n g t y g \u242a s o 1 , and higher-order terms in\n.\nthe taylor series expansion yield the asymptotic distribution see note 14.1 .\n\n.\n' w\n\n\u017e .\n\n.x\n\nx\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nn\n\np\n\n14.1.3 delta method for function of random vector\nthe delta method generalizes to functions of random \u00aeectors. suppose that\nt s t , . . . , t\nis asymptotically multivariate normal with mean \u242a s\nn1\nn\n\u017e\n\u242a , . . . , \u242a\nhas a\n1\nn\nnonzero differential \u243e s \u243e , . . . , \u243e at \u242a, where\n\nand covariance matrix \u233arn. suppose that g t , . . . , t\n\nn n\n\n.x\n\n.x\n\n.x\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\nn\n\n1\n\n1\n\nn\n\n\u243e s\ni\n\n\u2b78g\n\u2b78ti\n\n.\n\nts\u242a\n\nthen,\n\n'n g t y g \u242a\n\u017e\n\n\u017e\n\n.\n\nn\n\nd\n\n.\n\n.\nn 0, \u243e \u233a \u243e .\n\n\u017e\n\nx\n\n\u017e\n\n14.4\n\n.\n\nn\n\n\u017e\n\n.\n\n.\nfor large n, g t has distribution similar to the normal with mean g \u242a and\nvariance \u243ex \u233a \u243ern.\n\u017e\ng t y g \u242a s t y \u242a \u243e q o t y \u242a ,\n.\n.\n\u017e\nn\n2 1r2\n.\ni\n\nthe proof of 14.4 follows from the expansion\n\u017e\n\nwhere z s \u00fd z\ndenotes the length of vector z. for large n, g t y g \u242a\n\u017e .\nbehaves like a linear function of the approximately normal random vector\nt y \u242a . thus, it itself is approximately normal.\n\u017e\n\n\u017e .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nn\n\nn\n\nn\n\nx\n\nn\n\n1\n\nn\n\n.\n.x\n\n14.1.4 asymptotic normality of functions of multinomial counts\nthe delta method for random vectors implies asymptotic normality of many\nfunctions of cell counts in contingency tables. suppose that cell counts\nhave a multinomial distribution with cell probabilities \u2432 s\n\u017e\nn , . . . , n\n1\n\u2432 , . . . , \u2432 . let n s n q \u2b48\u2b48\u2b48 qn , and let p s p , . . . , p\n\u017e\ndenote the\nsample proportions, where p s n rn.\ndenote observation i of the n cross-classified in the contingency table by\ny s y , . . . , y , where y s 1 if it falls in cell\nj, and y s 0 otherwise,\ni s 1, . . . , n. for instance, y s 0, 0, 1, 0,0, . . . , 0 means that observation 6 is\n.\nin the third cell of the table. now, since each observation falls in just one cell,\n\u00fd y s 1 and y y s 0 when j / k. also, p s \u00fd y rn, and\n\n.x\n\ni n\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\ni1\n\nn\n\nn\n\nn\n\ni j\n\ni j\n\n1\n\n1\n\n6\n\ni\n\ni\n\ni\n\nj\n\ni\n\ni j\n\ni j\n\nik\n\nj\n\ni j\n\n.\ne y s p y s 1 s \u2432 s e y 2 ,\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\ni j\n\ni j\n\ni j\n\nj\n\nit follows that\n\ne y y s 0 if j / k.\n\u017e\n\n.\n\nik\n\ni j\n\ne y s \u2432 and cov y s \u233a ,\n\u017e\n\n.\n\n\u017e\n\n.\n\ni s 1, . . . , n,\n\ni\n\ni\n\n6\n "}, {"Page_number": 595, "text": "580\nwhere \u233a s \u2434 with\n\n\u017e\n\n.\n\njk\n\nasymptotic theory for parametric models\n\n\u2434 s var y s e y y e y s \u2432 1 y \u2432 ,\n.\n\u017e\n\n\u2434 s cov y , y s e y y y e y e y s y\u2432\u2432 for j / k.\n\n.\ni j\n\u017e\n\n\u017e\n.\n\n2\ni j\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\ni j\n\nj j\n\n2\n\nj\n\nj\n\ni j\n\nik\n\ni j\n\nik\n\nj\n\nk\n\nik\n\njk\n\ni j\n\n\u017e\n\n.\n\nthe matrix \u233a has form\n\n\u233a s diag \u2432 y \u2432 \u2432x\n\n\u017e\n\n.\n\nwhere diag \u2432 is the diagonal matrix with the elements of \u2432 on the main\ndiagonal.\n\n\u017e\n\n.\n\nsince p is a sample mean of n independent observations, namely\n\np s\n\n\u00fdn yis1 i\n\nn\n\n,\n\ncov p s diag \u2432 y \u2432 \u2432 rn.\n\n.\n14.5\nthis covariance matrix is singular, because of the linear dependence \u00fd p s 1.\nthe multivariate central limit theorem rao 1973, p. 128 implies\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nx\n\ni\n\n'n p y \u2432\n\n\u017e\n\n.\n\nd\n\nn 0, diag \u2432 y \u2432 \u2432 .\n\n\u017e\n\n.\n\nx\n\n\u017e\n\n14.6\n\n.\n\nby the delta method, functions of p having nonzero differential at \u2432 are\nbe a differentiable function, and\n\nalso asymptotically normal. let g t , . . . , t\nlet\n\n\u017e\n\n.\n\nn\n\n1\n\n\u243e s \u2b78gr\u2b78\u2432 ,\n\ni\n\ni\n\ni s 1, . . . , n,\n\ndenote \u2b78gr\u2b78t evaluated at t s \u2432. by the delta method 14.4 ,\n.\nn 0, \u243e diag \u2432 y \u2432 \u2432 \u243e .\n\u017e\n.\n\n'n g p y g \u2432\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nd\n\nx\n\nx\n\ni\n\n\u017e\n\n14.7\n\n.\n\nthe asymptotic variance equals\n\nx\n\n\u243e diag \u2432 \u243e y \u243e \u2432 s \u2432 \u243e y\n\n2\n\nx\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u00fd\n\n2\ni\n\ni\n\n\u017e\n\n\u00fd\n\n.\n2\n\u2432 \u243e .\n\ni\n\ni\n\nin section 3.1.7 we used this formula to derive the large-sample variance of\nthe sample log odds ratio.\n\n14.1.5 delta method for vector function of random vector\nthe delta method generalizes further to a \u00aeector of functions of an asymptot-\nically normal random vector. let g t s g t , . . . , g t\nand let \u2b78gr\u2b78\u242a\n\u017e ..x\n.\ndenote the q = n jacobian matrix for which the entry in row i and column j\n\n\u017e .\n1\n\n\u017e .\n\n\u017e\n\n\u017e\n\nq\n\n6\n6\n "}, {"Page_number": 596, "text": "delta method\nis \u2b78g t r\u2b78t evaluated at t s \u242a. then,\n\n\u017e .\n\ni\n\nj\n\n'n g t y g \u242a\n\u017e\n\n\u017e\n\n.\n\nn\n\nd\n\n.\n\nn 0, \u2b78gr\u2b78\u242a \u233a \u2b78gr\u2b78\u242a\n\n\u017e\n\n.\n\n\u017e\n\n581\n\nx\n\n.\n\n.\n\n\u017e\n\n14.8\n\n.\n\nthe rank of the limiting normal distribution equals the rank of the asymp-\ntotic covariance matrix.\n\nexpression 14.8 is useful for finding large-sample joint distributions. for\ninstance, from 14.6 , 14.7 , and 14.8 , the asymptotic distribution of several\nfunctions of multinomial proportions has covariance matrix of the form\n\n.\n. \u017e\n\n\u017e\n\u017e\n\n.\n\n\u017e\n\n.\n\nasymp. cov n g p y g \u2432 s \u233d diag \u2432 y \u2432 \u2432 \u233d ,\n\n.\n\n\u017e\n\n.\n\nx\n\nx\n\n4\n\n'\n\n\u0004\n\n\u017e\nwhere \u233d is the jacobian \u2b78gr\u2b78\u2432 .\n.\n\n\u017e\n\n.\n\n\u017e\n\njoint asymptotic normality of log odds ratios\n\n14.1.6\nwe illustrate formula 14.8 by finding the joint asymptotic distribution of a\nset of log odds ratios in a contingency table. we use the log scale because\nconvergence to normality is more rapid for it.\nlet g \u2432 s log \u2432 denote the vector of natural logs of cell probabilities,\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\nfor which\n\nthe covariance of the asymptotic distribution of n log p y log \u2432 is\n\n\u017e .\n\n\u017e\n\n.x\n\n\u2b78gr\u2b78\u2432 s diag \u2432\n\n\u017e\n\n.\n\ny1\n\n.\n' w\n\ndiag \u2432\n\n\u017e\n\n.\n\ny1\n\ndiag \u2432 y \u2432 \u2432 diag \u2432 s diag \u2432 y 11\n\ny1\n\ny1\n\nx\n\nx\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nwhere 1 is an n = 1 vector of 1 elements.\n\nfor a q = n matrix of constants c, it follows that\n\n'n c log p y log \u2432\n\u017e\n\n.\n\n\u017e\n\n.\n\nd\n\nn 0, c diag \u2432\n\n\u017e\n\ny1\n\n.\n\nx\n\nc y c11 c .\n\nx\n\nx\n\n\u017e\n\n14.9\n\n.\n\n\u017e .\n\nnow, suppose c log p is a set of sample log odds ratios. then, each row of c\ncontains zeros except for two q1 elements and two y1 elements in the\npositions multiplied by the relevant elements of log p to form the given log\nodds ratio. the second term in the covariance matrix in 14.9 is then zero. if\na particular odds ratio uses the cells numbered h, i, j, and k, the variance of\nthe asymptotic distribution is\n\n\u017e .\n\n\u017e\n\n.\n\nasymp. var n sample log odds ratio s \u2432 q \u2432 q \u2432 q \u2432 .\ny1\nk\n\ny1\nh\n\ny1\ni\n\ny1\nj\n\n\u017e\n\n.\n\n'\n\nwhen two log odds ratios have no cells in common, their asymptotic covari-\nance in the limiting normal distribution equals zero.\n\n6\n6\n "}, {"Page_number": 597, "text": "582\n\nasymptotic theory for parametric models\n\n14.2 asymptotic distributions of estimators of\nmodel parameters and cell probabilities\n\nwe now derive basic results of large-sample model-based inference for\ncontingency tables. the delta method is the key tool. the derivations apply to\na single multinomial distribution. they extend directly to products of multi-\nnomials, when the parameter space stays fixed as the sample size increases.\nin n cells of a contingency\ntable. the asymptotics regard n as fixed and let n s \u00fdn \u2122 \u2b01. we assume\nthat n s np has a multinomial distribution with probabilities \u2432 s\n\u017e\n\u2432 , . . . , \u2432 . the model is\n\nthe observations are counts n s n , . . . , n\n\n.x\n\n.x\n\n\u017e\n\nn\n\n1\n\ni\n\n1\n\nn\n\n\u2432 s \u2432 \u242a ,\n.\n\n\u017e\n\n\u017e .\n\nq\n\n1\n\n\u017e\n\n.x\n\n\u017e .\n\nwhere \u2432 \u242a denotes a function that relates \u2432 to a smaller number of\nparameters \u242a s \u242a , . . . , \u242a .\n.x\n\nand \u2432 s \u2432 , . . . , \u2432 s \u2432 \u242a\n\u017e\n\nas \u242a ranges over its parameter space, \u2432 \u242a ranges over a subset of the\nspace of \u2432 for n probabilities. adding components to \u242a, the model becomes\nmore complex and the space of \u2432 that satisfy the model is larger. we use \u242a\nand \u2432 to denote generic parameter and probability values, and \u242a s\n\u017e\n\u242a , . . . , \u242a\nto denote true values for a\nq0\n10\nparticular application. when the model does not hold, no \u242a exists for which\n\u2432 \u242a s \u2432 ; that is, \u2432 falls outside the subset of \u2432 values that is the range\n\u017e\nof \u2432 \u242a for the space of possible \u242a. we consider this case in section 14.3.5.\nwe first derive the asymptotic distribution of the ml estimator \u242a of \u242a.\nwe use that to derive the asymptotic distribution of the model-based ml\nestimator \u2432 s \u2432 \u242a of \u2432. the approach follows rao 1973, sec. 5e and\nbishop et al. 1975, secs. 14.7 and 14.8 . the assumed regularity conditions\nare:\n\n.\n\u017e .\n\n\u02c6\u017e .\n\n\u02c6\n\n\u02c6\n\nn 0\n\n.x\n\n10\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\ni0\n\n1. \u242a is not on the boundary of the parameter space.\n2. all \u2432 ) 0.\n3. \u2432 \u242a has continuous first-order partial derivatives in a neighborhood\n4. the jacobian matrix \u2b78\u2432r\u2b78\u242a has full rank q at \u242a .0\n\n\u017e .\nof \u242a .0\n\n\u017e\n\n.\n\nthese conditions ensure that \u2432 \u242a is locally smooth and one-to-one at \u242a\n0\nand taylor series expansions exist in neighborhoods around \u242a and \u2432 .\n0\nwhen the jacobian does not have full rank, often it does with reformulation\nof the model using fewer parameters.\n\n0\n\n\u017e .\n\n14.2.1 distribution of model parameter estimator\nthe key to deriving the asymptotic distribution of \u242a is to express \u242a as a\nlinearized function of p. then the delta method applies, using the asymptotic\n\n\u02c6\n\n\u02c6\n\n "}, {"Page_number": 598, "text": "asymptotic distributions of estimators\n\n583\n\nnormality of p. the linearization has two steps, first relating p to \u2432, and then\n\u02c6\n\u02c6\n\u2432 to \u242a.\n\n\u02c6\n\nthe kernel of the multinomial log likelihood is\n\nl \u242a s log \u2432 \u242a s n\n\u017e\n\n.\n\n\u017e\n\n.\n\nn i\n\ni\n\nn\n\n\u0142\nis1\n\nn\n\n\u00fd\nis1\n\n.\np log\u2432 \u242a .\ni\n\n\u017e\n\ni\n\nthe likelihood equations are\n\n.\n\n\u017e\n\u2b78l \u242a\n\u2b78\u242a\nj\n\ns n\n\np\ni\n\u017e\n\u2432 \u242a\n\ni\n\n.\n\n\u00fd\n\ni\n\n.\n\n\u017e\n\u2b78\u2432 \u242a\ni\n\u2b78\u242a\nj\n\ns 0,\n\nj s 1, . . . , q.\n\n\u017e\n\n14.10\n\n.\n\nthese depend on the functional form \u2432 \u242a used in the model. note that\n\n\u017e .\n\n.\n\n\u017e\n\u2b78\u2432 \u242a\ni\n\u2b78\u242a\nj\n\n\u00fd\n\ni\n\ns\n\n\u2b78\n\n\u2b78\u242a\nj\n\n\u2432 \u242a s\n\n\u00fd i\n\n\u017e\n\n.\n\ni\n\n\u2b78\n\n\u2b78\u242a\nj\n\n1 s 0.\n\u017e .\n\n\u017e\n\n14.11\n\n.\n\ni\n\ni\n\n\u02c6\nj\n\n\u017e .\n\nlet \u2b78\u2432r\u2b78\u242a represent \u2b78\u2432 \u242a r\u2b78\u242a evaluated at \u242a. subtracting a common\n.\nterm from both sides of the jth likelihood equation 14.10 ,\nn \u2432 y \u2432 \u2b78\u2432\n\u017e\ni\n\u02c6\n\u2b78\u242a\nj\n\nn p y \u2432 \u2b78\u2432\n\u017e\ni\n\u2b78\u242a\nj\n\n\u00fd\u02c6\n\n14.12\n\n\u00fd\n\ns\n\n\u02c6\n\u2432\ni\n\n\u02c6\n\u2432\ni\n\n\u02c6\n\n\u02c6\n\n.\n\n.\n\n\u017e\n\n.\n\ni0\n\ni0\n\n\u017e\n\n,\n\ni\n\ni\n\ni\n\ni\n\nj\n\n.\nsince the first sum on the right-hand side equals zero from 14.11 .\n\n\u017e\n\nnext we express \u2432 in terms of \u242a using\n\n\u02c6\n\n\u02c6\n\n\u2432 y \u2432 s\n\u02c6\n\ni0\n\ni\n\n\u017e\n\n\u00fd\n\nk\n\n\u242a y \u242a\n\u02c6\nk\n\n\u2b78\u2432\n.\ni\nk 0 \u2b78\u242a\nk\n\nwhere \u2b78\u2432r\u2b78\u242a represents \u2b78\u2432r\u2b78\u242a evaluated at some point \u242a falling be-\ntween \u242a and \u242a . substitution of this into the right-hand side of 14.12 and\ndivision of both sides by n yields, for each j,\n\n\u02c6\n\n\u017e\n\n.\n\nk\n\nk\n\ni\n\ni\n\n0\n\n'\n'n p y \u2432 \u2b78\u2432\ni\n\u02c6\n\u2b78\u242a\nj\n\ni\n\u02c6\n\u2432\ni\n\n.\n\n\u017e\n\ni0\n\n\u00fd\n\ni\n\n\u02c6's\n\u017e\nn \u242a y \u242a\n\n\u00fd\n\nk\n\nk 0\n\nk\n\n\u017e\n\n.\n\n\u00fd\n\ni\n\n1 \u2b78\u2432 \u2b78\u2432\ni\n\u02c6\n\u2432\n\u2b78\u242a\ni\nk\n\ni\n\u02c6\n\u2b78\u242a\nj\n\n/\n\n.\n\n\u017e\n\n14.13\n\n.\n\nsome notation lets us express more simply the dependence of \u242a on p. let\n\n\u02c6\n\na denote the n = q matrix having elements\n\na s \u2432\n\ni j\n\ny1r2\ni0\n\n.\n\ni\n\n\u2b78\u2432 \u242a\u017e\n\u2b78\u242a\nj0\n\n.\n\n "}, {"Page_number": 599, "text": "584\n\nasymptotic theory for parametric models\n\nthe matrix expression for a is\n\n\u017e\n\n.\n\n\u017e\n\ny1r2\n\na s diag \u2432\n\n.\n14.14\ndenotes the jacobian \u2b78\u2432r\u2b78\u242a evaluated at \u242a . as \u242a\nwhere \u2b78\u2432r\u2b78\u242a\n\u02c6\n.\nconverges to \u242a , the term in brackets on the right-hand side of 14.13\nconverges to the element in row j and column k of aa. as \u242a \u2122 \u242a , the set of\nequations 14.13 has the form\n\n0 \u017e\n\n\u2b78\u2432r\u2b78\u242a ,\n.\n\n\u02c6\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n0\n\n0\n\n0\n\n0\n\n0\n\nx\n\nx\na diag \u2432\n\n\u017e\n\n.\n\n0\n\ny1r2\n\n'\nn p y \u2432 s aa n \u242a y \u242a q o 1 .\n\u017e\n\u017e .\n\n'\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\nx\n\n.\n\n0\n\n\u017e\n\n0\n\np\n\nsince the jacobian has full rank at \u242a , axa is nonsingular. thus,\n\n0\n\n\u02c6'\n\u017e\nn \u242a y \u242a s aa\n\n.0\n\n\u017e\n\nx\n\ny1\n\n.\n\nx\na diag \u2432\n\n\u017e\n\n.\n\n0\n\ny1r2\n\n'\nn p y \u2432 q o 1 .\n\u017e\n\u017e .\n\n.\n\n0\n\np\n\n\u017e\n\n14.15\n\n.\n\n.\nnow, the asymptotic distribution of p determines that of \u242a. from 14.6 ,\n' \u017e\nn p y \u2432 is asymptotically normal, with covariance matrix diag \u2432 y\n\u2432 \u2432 . by the delta method, n \u242a y \u242a\nis also asymptotically normal, with\nasymptotic covariance matrix\n\n\u02c6'\n\u017e\n\nx\n0\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nx\n\nw\n\n0\n\n0\n\n0\n\n0\n\n\u02c6\n\ny1\n\n\u017e\n\nx\naa\n\n.\n\nx\na diag \u2432\n\n\u017e\n\n.\n\n0\n\ny1r2\n\n= diag \u2432 y \u2432 \u2432 = diag \u2432\n\n\u017e\n\n.\n\n\u017e\n\nx\n0\n\n0\n\n0\n\ny1r2\n\nx\n\n\u017e\na aa\n\n.\n\ny1\n\n.\n\n.\n\n0\n\n\u017e\nusing 14.11 and 14.14 , the term subtracted in this expression disappears\nbecause\n\n\u017e\n\n.\n\n.\n\n\u2432 diag \u2432\n\n\u017e\n\nx\n0\n\ny1r2\n\n.\n\n0\n\na s \u2432 diag \u2432\n\n\u017e\n\nx\n0\n\n.\n\n0\n\ny1r2\n\ns 1 \u2b78\u2432r\u2b78\u242a s\n\n\u017e\n\n.\n\nx\n\n0\n\n\u2b78\u2432r\u2b78\u242a\n\ny1r2\n\n0\n\n\u017e\n\n.\n\ndiag \u2432\n\u00fd\n\n\u017e\n\u2b78\u2432r\u2b78\u242a s 0 .\n\nx\n\nx\n\n.\n\n0\n\n\u017e\n\ni\n\ni\n\n0\n\n/\n\u02c6' \u017e\nn \u242a y \u242a\n\n.\n\n0\n\nsimplifies to\n\nthus, this asymptotic covariance expression for\n\u017e x .y1\naa\n\n.\n\nin summary, this argument establishes the general result\n\n\u02c6'n \u242a y \u242a\n\n\u017e\n\n.0\n\nd\n\nn 0, aa\n\n\u017e\n\nx\n\ny1\n\n.\n\n.\n\n\u017e\n\n14.16\n\n.\n\nthe asymptotic covariance matrix of \u242a depends on \u2b78\u2432r\u2b78\u242a\nand hence on\n\u02c6\nthe function for modeling \u2432 in terms of \u242a. let a denote a evaluated at the\nml estimate \u242a. the estimated covariance matrix is\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n0\n\n$\ncov \u242a s aa\n\u02c6\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\nx\n\ny1\n\n.\n\nrn.\n\u02c6\n\nthe asymptotic normality and covariance of \u242a follows more simply from\ngeneral results for ml estimators. however, those results require stronger\n\n6\n "}, {"Page_number": 600, "text": "asymptotic distributions of estimators\n\n585\n\nregularity conditions rao 1973, p. 364 than the ones assumed here. suppose\nthat observations are independent from f y; \u242a , some probability mass func-\ntion. the ml estimator \u242a is efficient, in the sense that\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u02c6'n \u242a y \u242a\n\n\u017e\n\n.\n\nd\n\n\u017e\n\nn 0, iiiii\n\ny1\n\n.\n\n,\n\nwhere iiiii is the information matrix for a single observation. the\nelement of iiiii is\n\n\u017e\n\nj, k\n\n.\n\n\u017e\n\nye\n\n2\u2b78 log f y, \u242a\n\n\u017e\n\u2b78\u242a\u2b78\u242a\nk\n\nj\n\n.\n\n/\n\ns e\n\n\u2b78log f y, \u242a\n\n\u017e\n\u2b78\u242a\nj\n\n.\n\n\u2b48\n\n\u2b78log f y, \u242a\n\n\u017e\n\u2b78\u242a\nk\n\n.\n\n.\n\nwhen f is the probability of a single observation having multinomial proba-\nbilities \u2432 \u242a , . . . , \u2432 \u242a , this element of iiiii equals\n\n\u017e .4\n\n\u0004\n\n\u017e .\n1\n\nn\n\nn\n\n\u00fd\nis1\n\n\u017e\n\u2b78log \u2432 \u242a\n\n\u017e\ni\n\u2b78\u242a\nj\n\n.\n\n.\n\n.\n\n.\n\n\u017e\n\u2b78log \u2432 \u242a\n\n\u017e\ni\n\u2b78\u242a\nk\n\n\u2432 \u242a s\n\n.\n\n\u017e\n\ni\n\nn\n\n\u00fd\nis1\n\n.\n\n\u017e\n\u2b78\u2432 \u242a \u2b78\u2432 \u242a\n1\ni\n\u2b78\u242a \u2432 \u242a\u017e\n\n\u017e\ni\n\u2b78\u242a\nj\n\n.\n\nk\n\ni\n\n.\n\n.\n\nx\n\n.\n\n.\n\n\u017e\n\nthis is the j, k element of aa. thus the asymptotic covariance is iiiii s\n\u017e x .y1\naa\n\ny1\n\nfor results of this section to apply, a ml estimator of \u242a must exist and be\na solution of the likelihood equations. this requires the following strong\nidentifiability condition: for every \u2440) 0, there exists a \u2426) 0 such that if\n\u242a y \u242a ) \u2440, then \u2432 \u242a y \u2432 ) \u2426. this condition implies a weaker one\nthat two \u242a values cannot have the same \u2432 value. when strong identifiability\nand the other regularity conditions hold, the probability an ml estimator is a\nroot of the likelihood equations converges to 1 as n \u2122 \u2b01. that estimator has\nthe asymptotic properties given above of a solution of the likelihood equa-\n.\ntions. for proofs, see birch 1964a and rao 1973, pp. 360\u1390362 .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n0\n\n0\n\n14.2.2 asymptotic distribution of cell probability estimators\nthe asymptotic distribution of the model-based estimator \u2432 follows from the\ntaylor-series expansion\n\n\u02c6\n\n\u2432 s \u2432 \u242a s \u2432 \u242a q\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n0\n\n\u2b78\u2432\n\n\u2b78\u242a\n\n0\n\n\u017e\n\n\u242a y \u242a q o n\n\u02c6\n\u017e\n\n.\n\n0\n\np\n\ny1r2\n\n.\n\n.\n\n\u017e\n\n14.17\n\n.\n\nthe size of the remainder term follows from \u242a y \u242a s o n\n\u017e\n\u2432 \u242a s \u2432 , and n \u242a y \u242a\n\u017e\n\n.\n. now\nis asymptotically normal with asymptotic co-\n\n\u02c6'\n\u017e\n\ny1r2\n\n\u02c6\n\n.\n\n.\n\n\u017e\n\n.\n\n0\n\np\n\n0\n\n0\n\n0\n\n6\n "}, {"Page_number": 601, "text": "586\n\nasymptotic theory for parametric models\n\nvariance aa\n\n\u017e x .y1\n\n. by the delta method,\n\n'n \u2432 y \u2432\n\n\u017e\n\n\u02c6\n\nd\n\n.\n\n0\n\nn 0,\n\n\u2b78\u2432\n\n\u2b78\u242a\n\n0\n\ny1\n\n\u017e\n\nx\naa\n\n.\n\nx\n\n\u2b78\u2432\n\n\u2b78\u242a\n\n0\n\n.\n\n\u017e\n\n14.18\n\n.\n\nwhen the model holds with \u242a having q - n y 1 elements, \u2432 s \u2432 \u242a is\nmore efficient than the sample proportion p for estimating \u2432. more gener-\nally, for estimating a smooth function g \u2432 of \u2432, g \u2432 has smaller asymp-\ntotic variance than g p . we next derive this result, discussed in section\n6.4.5. the derivation deletes the nth component from p and \u2432, so their\ncovariance matrices are positive definite problem 14.16 . the nth propor-\ntion is linearly dependent on the first n y 1 since they sum to 1. let \u233a s\n'\ndiag \u2432 y \u2432 \u2432 denote the n y 1 = n y 1 covariance matrix of\nn p.\nthe inverse of \u233a is\n\n\u02c6\u017e .\n\n\u017e .\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\nx\n\ny1\u233a s diag \u2432 q 11\u2432 ,\n\ny1\n\n\u017e\n\n.\n\nx\n\nn\n\n\u017e\n\n14.19\n\n.\n\nwhich can be verified by evaluating \u233a \u233ay1 and showing that it equals the\nidentity matrix.\n, evaluated at \u2432 s \u2432 . by the\n\nlet \u2b78gr\u2b78\u2432 s \u2b78gr\u2b78\u2432 , . . . , \u2b78gr\u2b78\u2432\n\n.x\n\n\u017e\n\n\u017e\n\n.\n\nny1\n\n0\n\n1\n\n0\n\ndelta method,\n\nasymp. var n g p s\n\n\u017e\n\n.\n\n'\n\n\u017e\n\n\u2b78g\n\u2b78\u2432\n\n0\n\nx\n\n/\n\n'\n\n\u017e\n\ncov n p\n\n.\n\n\u2b78g\n\u2b78\u2432\n\n0\n\ns\n\n\u017e\n\n\u2b78g\n\u2b78\u2432\n\n0\n\nx\n\n/\n\n\u233a\n\n\u2b78g\n\u2b78\u2432\n\n0\n\nand\n\nasymp. var n g \u2432 s\n\n.\u02c6\n\n\u017e\n\n'\n\ns\n\n\u017e\n\u017e\n\n\u2b78g\n\u2b78\u2432\n\n\u2b78g\n\u2b78\u2432\n\n0\n\n0\n\nx\n\nx\n\n/\n/\n\n\u02c6\nasymp. cov n \u2432\n\n'\n\n\u017e\n\n.\n\n\u2b78g\n\u2b78\u2432\n\n\u2b78\u2432\n\n\u2b78\u242a\n\n0\n\n\u02c6'\nasymp. cov n \u242a\n\n\u017e\n\n.\n\n0\n\n\u017e\n\nx\n\n/\n\n\u2b78\u2432\n\n\u2b78\u242a\n\n0\n\n\u2b78g\n\u2b78\u2432\n\n.\n\n0\n\nusing 14.11 and 14.19 yields\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nasymp. cov n \u242a s aa s \u2b78\u2432r\u2b78\u242a\n\ny1\n\n\u02c6'\n\n\u017e\n\n.\n\n\u017e\n\nx\n\n.\n\n\u017e\n\nx\n\n.\n\ndiag \u2432\n\n\u017e\n\n.\n\n0\n\n0\n\ny1\n\n\u017e\n\n\u2b78\u2432r\u2b78\u242a\n\n.\n\n0\n\ny1\n\ns \u2b78\u2432r\u2b78\u242a \u233a\n\nx y1\n\n\u017e\n\n.\n\n0\n\n\u2b78\u2432r\u2b78\u242a\n\n\u017e\n\n.\n\n0\n\ny1\n\n.\n\nsince \u233a is positive definite and \u2b78\u2432r\u2b78\u242a\nw\u017e\n\u2b78\u2432r\u2b78\u242a \u233a\n\nhas\nare also positive definite.\n\n\u2b78\u2432r\u2b78\u242a\n\n.x y1\u017e\n\n.xy1\n\n\u017e\n\n.\n\n0\n\n0\n\n0\n\nrank q, \u233a\n\ny1\n\nand\n\n6\n "}, {"Page_number": 602, "text": "distributions of residuals and fit statistics\n\n587\n\nto show that asymp. var n g p g asymp. var n g \u2432 , we show that\n\n\u017e\n\n\u017e .x\n\n.x\n\n'\nw\n\n'\nw\n\n\u017e\n\n\u2b78g\n\u2b78\u2432\n\n0\n\nx\n\n/\n\n\u233a y\n\n\u2b78\u2432\n\n\u2b78\u242a\n\n0\n\n\u017e\n\n\u2b78\u2432\n\n\u2b78\u242a\n\n0\n\nx\n\n/\n\ny1\n\n\u233a\n\ny1\n\n\u017e\n\n\u2b78\u2432\n\n\u2b78\u242a\n\n0\n\n\u2b78\u2432\n\n\u2b78\u242a\n\n0\n\n\u00bd\n\n\u02c6\n\nx\n\n/\n\n5\n\n\u2b78g\n\u2b78\u2432\n\n0\n\ng 0.\n\nbut this quadratic form is identical to\ny y b\u2428 \u233a\n\n\u017e\n\n.\n\nx y1\n\ny y b\u2428\n\n\u017e\n\n.\n\nwhere y s \u233a \u2b78gr\u2b78\u2432 , b s \u2b78\u2432r\u2b78\u242a , and \u2428 s b \u233a b\n\u017e x y1\nresult then follows from the positive definiteness of \u233ay1.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n0\n\n0\n\n.y1\n\nx y1\n\nb \u233a y. the\n\nthis proof is based on one given by altham 1984 . her proof uses\nstandard properties of ml estimators. it applies whenever regularity condi-\ntions hold that guarantee those properties. the proof applies not only to\ncategorical data but to any situation in which a model describes the depen-\ndence of a set of parameters \u2432 on some smaller set \u242a.\n\n\u017e\n\n.\n\n14.3 asymptotic distributions of residuals and\ngoodness-of-fit statistics\n\n\u02c6\n\n\u017e .\n\nwe next study the distribution of pearson x 2 and likelihood-ratio g2\ngoodness-of-fit statistics for the multinomial model \u2432 s \u2432 \u242a . we first\nderive the asymptotic joint distribution of the sample proportions p and\nmodel-based estimator \u2432. this distribution determines large-sample distribu-\ntions of statistics that depend on both p and \u2432. for instance, it determines\nthe asymptotic joint distribution of the pearson residuals, which compare p\nwith \u2432. deriving the large-sample chi-squared distribution for x 2, which is\nthe sum of squared pearson residuals, is then straightforward. we also show\nthat x 2 and g2 are asymptotically equivalent, when the model holds. our\npresentation borrows from bishop et al. 1975, chap 14 , cox 1984 , cramer\u00b4\n.\n\u017e\n1946, pp. 432\u1390433 , and rao 1973, sect. 6b .\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\njoint asymptotic normality of p and \u2432\u02c6\n\n14.3.1\nwe first express the joint dependence of p and \u2432 on p, in order to show the\njoint asymptotic normality of p and \u2432. let\ny1\n\ny1r2\n\n1r2\n\n\u02c6\n\n\u02c6\n\u017e\na aa\n\nx\n\n.\n\nx\na diag \u2432\n\n\u017e\n\n.\n\n0\n\n.\n\n.\nfrom 14.15 and 14.17 ,\n\n\u017e\n\n.\n\n\u017e\n\nd s diag \u2432\n\u017e\n\n.\n\n0\n\n\u2432 y \u2432 s\n\u02c6\n\n0\n\n\u2b78\u2432\n\n\u2b78\u242a\n\n0\n\n\u017e\n\n\u242a y \u242a q o n\u017e\n\u02c6\n\n.\n\n0\n\np\n\ny1r2\n\n.\n\ns d p y \u2432 q o ny1r2\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n0\n\np\n\n "}, {"Page_number": 603, "text": "588\n\ntherefore,\n\nasymptotic theory for parametric models\n\n'\nn\n\n\u017e\n\np y \u2432\n0\n\u2432 y \u2432\n\u02c6\n\n0\n\n/\n\ns\n\n\u017e\n\n/\n\ni\nd\n\n'\nn p y \u2432 q o 1 ,\n\u017e\n\u017e .\n\n.\n\n0\n\np\n\nwhere i is a n = n identity matrix. by the delta method,\n\n'n\n\n\u017e\n\np y\u2432\n0\n\u2432 y \u2432\n\u02c6\n\n0\n\n/\n\nwhere\n\n\u233a* s\n\n\u017e\n\ndiag \u2432 y \u2432 \u2432\nd diag \u2432 y \u2432 \u2432\n\n\u017e\n\u017e\n\n.\n.\n\n0\n\n0\n\nx\n0\nx\n0\n\n0\n\n0\n\nd\n\nn 0, \u233a*\n\n\u017e\n\n.\n\n\u017e\n\n14.20\n\n.\n\n\u017e\n\ndiag \u2432 y \u2432 \u2432 d\nd diag \u2432 y \u2432 \u2432 d\n\n.\n\nx\n0\nx\n0\n\n\u017e\n\n.\n\n0\n\n0\n\n0\n\n0\n\nx\n\nx\n\n/\n\n.\n\n\u017e\n\n14.21\n\n.\n\nthe two matrix blocks on the main diagonal of \u233a* are cov n p and asymp.\ncov n \u2432 , derived previously. the new information here is that asymp.\ncov n p, n \u2432 s diag \u2432 y \u2432 \u2432 d .\n\n'\u017e\n'\n\u017e\n\n.\n'\n\n\u02c6\n\n\u017e\n\n.\n\n.\n\nw\n\nx\n\nx\n\n\u02c6\n\nx\n0\n\n0\n\n0\n\n'\u017e\n\n.\n\n14.3.2 asymptotic distribution of pearson and standardized residuals\nfor cell counts n the pearson statistic is x s \u00fde , where\n\n4\n\n\u0004\n\n2\n\n2\ni\n\ni\n\ne s\n\ni\n\nn y \u242e\n\u02c6\ni\n1r2\n\u02c6\n\u242e\ni\n\ni\n\ns\n\n'\nn p y \u2432\n\u017e\n\u02c6\n\ni\n\ni\n1r2\n\u02c6\n\u2432\ni\n\n.\n\n.\n\nwe next derive the asymptotic distribution of e s e , . . . , e\n, which is a\ndiagnostic measure of lack of fit. for poisson models it is the pearson\nresidual. dividing it by its standard error gives the standardized residual. the\ndistribution of e is also helpful in deriving the distribution of x 2.\n\n.x\n\n\u017e\n\nn\n\nthe residuals e are functions of p and \u2432, which are jointly asymptotically\n\n1\n\n\u02c6\n\n\u017e\n\nnormal from 14.20 . to use the delta method, we calculate\n.\n\n\u2b78e r\u2b78\u2432 s y n p q \u2432 r2\u2432\ny3r2\n\u02c6\ni\n\n'\n\n\u02c6\n\n\u02c6\n\n\u017e\n\ni\n\ni\n\ni\n\n.\n'\n\ni\n\n\u2b78e r\u2b78p s n \u2432\ny1r2\n\u02c6\ni\n\u2b78e r\u2b78p s \u2b78e r\u2b78\u2432 s 0\n\n,\n\n\u02c6\n\nj\n\ni\n\ni\n\nj\n\ni\n\ni\nfor i / j.\n\nthat is,\n\n\u2b78e\n\u2b78p\n\u2b78e\n\u2b78\u2432\u02c6\n\n's n diag \u2432\n\u017e\n.\u02c6\n\ny1r2\n\nand\n\ns y\n\n\u017e\n\n1 '\n.2\n\nevaluated at p s \u2432 and \u2432 s \u2432 , these matrices equal n diag \u2432\n\u017e\ny n diag \u2432\n\u017e\n\nand\n. using 14.21 , 14.22 , and a \u2432 s 0 which follows\n\ny1r2\n.\n\ny1r2\n.\n\n'\n\n\u02c6\n\n\u017e\n\n\u017e\n\nw\n\n0\n\n0\n\n0\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u02c6\n\nn diag p q diag \u2432 diag \u2432\n\u02c6\n'\n1r2\n0\n\n\u017e\n\n.\n\n.\n\n0\n\nx\n\ny3r2\n\n.\n\n.\n\n\u017e\n\n14.22\n\n.\n\n6\n "}, {"Page_number": 604, "text": "distributions of residuals and fit statistics\n\n589\n\nfrom 14.11 , the delta method implies that\n\n\u017e\n\n.x\n\nd\n\n\u017e\nn 0, i y \u2432 \u2432 y a aa\n6e\n\n1r2\n0\n\n1r2\n0\n\n\u017e\n\nx\n\nx\n\ny1\n\n.\n\n.\nx\na .\n\n\u017e\n\n14.23\n\n.\n\n\u017e\n\nthe limiting distribution has form n 0, i y hat , where hat is the hat\nmatrix section 4.5.5 . although asymptotically normal, e behaves less vari-\nably than standard normal random variables. the standardized pearson\nresidual haberman 1973a divides e by its estimated standard error. this\nstatistic, which is asymptotically standard normal, equals\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\nr s\n\ni\n\n.\n1 y \u2432 y \u00fd \u00fd 1r\u2432 \u2b78\u2432r\u2b78\u242a \u2b78\u2432r\u2b78\u242a \u00ae\n\u02c6\n\n\u02c6\nk\n\n\u02c6\nj\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\nk\n\ni\n\ni\n\ni\n\ni\n\nj\n\n/\n\nei\n\n\u017e\n\n,\n\n1r2\n\njk\n\n\u017e\n\n14.24\n\n.\n\njk\n\n.\ndenotes the element in row j and column k of aa\n\nwhere \u00ae\n\u02c6\n. the\n'\n1 y h , where the leverage h for observation i\ndenominator of\n.\nestimates the ith diagonal element of the hat matrix. this simplifies to 3.13\nfor testing independence in two-way tables.\n\n\u02c6x\u02c6 y1\n\n\u02c6\n\n\u02c6\n\nis\n\n\u017e\n\n\u017e\n\nr\n\ni\n\ni\n\ni\n\n14.3.3 asymptotic distribution of pearson statistic\nthe proof that the pearson x 2 statistic has an asymptotic chi-squared\ndistribution uses the following relationship between normal and chi-squared\n.\ndistributions rao 1973, p. 188 :\n\n\u017e\n\nlet x be multivariate normal with mean \u242f and covariance matrix b.\na necessary and sufficient condition for x y \u242f c x y \u242f to have a chi-squared\ndistribution is bcbcb s bcb. the degrees of freedom equal the rank of cb.\n\n.x\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nwhen b is nonsingular, the condition simplifies to cbc s c.\nthe pearson statistic relates to e by x 2 s exe, so we apply this result by\nidentifying x with e, \u242f s 0, c s i, and b s i y \u2432 \u2432 y a aa\n1r2x\n\u017e x .y1 x4\na .\n0\nsince c s i, the condition for x y \u242f c x y \u242f s e e s x to have a\nchi-squared distribution simplifies to bbb s bb. a direct computation using\nax \u24321r2 s 0 shows that b is idempotent, so the condition holds. since e is\nasymptotically multivariate normal, x 2 is asymptotically chi-squared.\nfor symmetric idempotent matrices, the rank equals the trace. the trace\nof i is n; the trace of \u24321r2 \u24321r2x equals the trace of \u24321r2x \u24321r2 s \u00fd\u2432 s 1,\naa s identity\nwhich is 1; the trace of a aa\nmatrix of size q = q, which is q. thus, the rank of b s cb is n y q y 1, and\nthe asymptotic chi-squared distribution has df s n y q y1.\n\na equals the trace of aa\n\n0\n\u017e x .y1\u017e x .\n\n0\n\u017e x .y1 x\n\n1r2\n0\n\u0004 x 4\n\nx.\n\ni0\n\n\u017e\n\n.\n\n2\n\n0\n\n0\n\n0\n\nthis result, due to fisher 1922 , is remarkably simple. when the sample\nsize is large, the distribution of x 2 does not depend on \u2432 or the model\nform. it depends only on the difference between the dimension of \u2432, which\nis n y 1, and the dimension of \u242a. with q s 0 parameters, x 2 is pearson\u2019s\n\n0\n\n\u017e\n\n.\n\n "}, {"Page_number": 605, "text": "590\n\nasymptotic theory for parametric models\n\n\u017e\n\n.\n\n.\n\n\u017e\n1900 statistic 1.15 for testing that multinomial probabilities equal certain\nspecified values, and df s n y 1 as pearson claimed. watson 1959 showed\nthat the same result holds for the asymptotic conditional distribution, given a\nsufficient statistic for nuisance parameters.\n\n\u017e\n\n.\n\n14.3.4 asymptotic distribution of likelihood-ratio statistic\nwhen the model holds, the likelihood-ratio statistic g2\nequivalent to x 2 as n \u2122 \u2b01. to show this, we express\n\nis asymptotically\n\n2g s 2\n\n\u00fd\n\ni\n\nn log s 2 n\n\ni\n\nn\ni\n\u02c6\n\u242e\ni\n\n\u00fd\n\ni\n\np log 1 q\n\ni\n\n\u017e\n\np y \u2432\u02c6\n\ni\n\ni\n\n\u02c6\n\u2432\ni\n\n/\n\nand apply the expansion\n.\n\n\u017e\n\nlog 1 q x s x y x r2 q x r3 y \u2b48\u2b48\u2b48\n\n2\n\n3\n\nfor x - 1.\n\nwe identify x with p y \u2432 r\u2432, which converges in probability to 0 when\n.\n\u02c6\ni\nthe model holds. for large n,\n\n\u02c6\n\n\u017e\n\ni\n\ni\n\n2g s 2 n\n\ns 2 n\n\n\u2432 q p y \u2432\n\u00fd i\n\u02c6\n\u02c6\n\n\u017e\n\ni\n\ni\n\ni\n\n.\n\n\u00fd i\n\n\u017e\n\np y \u2432 y\n\n.\n\n\u02c6\n\ni\n\ni\n\n\u017e\n\n\u017e\n\n1\n\n/2\n\np y \u2432\n\u02c6\n\ni\n\ni\n\ny\n\n\u02c6\n\u2432\ni\np y \u2432\n\u02c6\n\ni\n\ni\n\n\u02c6\n\u2432\ni\n\n\u017e\n\n\u017e\n\n/\n\n1\n2\n\np y \u2432\n\u02c6\n\ni\n\ni\n\n\u02c6\n2\n\u2432\ni\n\n2\n\n.\n\nq \u2b48\u2b48\u2b48\n\n2\n\n.\n\n\u017e\n\nq\n\np y \u2432\n\u02c6\n\ni\n\ni\n\n2\n\n.\n\n\u02c6\n\u2432\ni\n\nq o p y \u2432\n\u02c6\n\n\u017e\n\np\n\ni\n\ni\n\n3\n\n.\n\np y \u2432\n.\u02c6\n\ni\n\ni\n\n2\n\n\u017e\n\ns n\n\n\u00fd\n\ni\n\n\u2432\u02c6i\n.\n\nq 2 no n\n\u017e\n\np\n\ny3r2\n\n.\n\ns x q o n\n\u017e\n\n2\n\np\n\ny1r2\n\n.\n\ns x q o 1 ,\n\u017e .\n\n2\n\np\n\ni\n\nsince \u00fd p y \u2432 s 0 and p y \u2432 s p y \u2432 y \u2432 y \u2432 , both of which\n\u017e\n\u02c6\ni\n\u017e y1r2 .\nare o n\n. thus, when the model holds, the difference between x and\ng2 converges in probability to 0. as a consequence, g2, like x 2, has an\nasymptotic chi-squared distribution with df s n y q y 1.\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2\n\np\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nthe parameter value that maximizes the likelihood is the one that mini-\n\nmizes g2. to show this, we let\n.\n\ng2 \u2432 ; p s 2 n\n\n\u017e\n\n\u00fd i\n\np log p r\u2432 .\n.\n\n\u017e\n\ni\n\ni\n\nthe kernel of the multinomial log likelihood is\n\nl \u242a s n\n\u017e\n\n.\n\ns yn\n\n.\npi\n\u2432 \u242a\u017e\n\ni\n\np log\ni\n\n\u00fd i\n\u017e\np log\u2432 \u242a\n\u00fd\n/2\n\n\u017e\n\n1\n\n\u017e\n\n\u017e\n\n2\n\ni\n\ns y\n\ng \u2432 \u242a ; p q n\n\n.\n\n.\n\n\u00fd i\n\np log p .\ni\n\nq n\n\n\u00fd\n\np log p\ni\ni\n\n.\n\n "}, {"Page_number": 606, "text": "distributions of residuals and fit statistics\n\n591\n\n\u017e\n\u017e\n\n2\n\nthe second term in the last expression does not depend on \u242a, so maximizing\nl \u242a is equivalent to minimizing g with respect to \u242a.\n\n\u017e .\na fundamental result for g2 concerns comparisons of nested models.\nsuppose that model m is a special case of model m . let q and q denote\n4\nthe numbers of parameters in the two models. let \u2432 and \u2432 denote ml\nestimators of cell probabilities for the two models. then\np log \u2432 r\u2432\n\u02c6\n\ng2 m y g2 m s 2 n\n\n0\n\u0004\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n0 i\n\n1i\n\n\u0004\n\n4\n\n\u00fd\n\n0 i\n\n1i\n\n0\n\n1\n\n1\n\n0\n\n1\n\ni\n\n1\n\n1\n\n0\n\n.\n\n.\n\n2\u017e\n\nhas the form of y2 log likelihood ratio for testing that m holds against the\nalternative that m holds. theory for likelihood-ratio tests suggests that\nwhen the simpler model holds, the asymptotic distribution of g m y\ng m is chi-squared with q y q degrees of freedom. for details, see\nbishop et al. 1975, pp. 525\u1390526 , haberman 1974a, p. 108 , and rao 1973,\npp. 418\u1390419 . the statistic x m m defined in 9.4 is a quadratic\napproximation for the g difference. haberman 1977a noted that these\ntests can perform well even for large, sparse tables, as long as q y q is\nsmall compared to the sample size and no expected frequency has larger\norder of magnitude than the others.\n\n.\n2\u017e\n\n\u017e\n.\n\n.\n.\n\n2\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n0\n\n1\n\n2\n\n0\n\n1\n\n2\n\n1\n\n0\n\n<\n\n0\n\n14.3.5 asymptotic noncentral distributions\nresults in this chapter assume that a certain parametric model holds. in\npractice, any unsaturated model almost surely does not hold perfectly, so one\nmight question the scope of these results. this is not problematic if we regard\nmodels merely as convenient approximations for reality. for instance, the\nml estimator \u242a converges to a value \u242a that describes the best fit of the\nchosen model to reality. in this sense, inferences for \u242a give us information\nabout a useful approximation for reality. similarly, model-based inferences\nabout cell probabilities are inconsistent for the true probabilities when the\nmodel does not hold; nevertheless, those inferences are consistent for de-\nscribing a useful smoothing of reality.\n\n\u02c6\n\nfor goodness-of-fit statistics, a relevant distinction exists between limiting\nbehavior when the model holds and when it does not hold. when the model\nholds, we\u2019ve seen x 2 and g2 have a limiting chi-squared distribution, and\nthe difference between them disappears as n increases. when the model\ndoes not hold, x 2 and g2 tend to grow unboundedly as n increases, and\nx y g need not go to zero. one method for obtaining proper limiting\ndistributions considers a sequence of situations \u2432 for which the lack of fit\ndiminishes as n increases. specifically, the model is \u2432 s f \u242a , but in reality\n.\n14.25\n\u017e .\nthe best fit of the model to the population has ith probability equal to f \u242a ,\nbut the true value differs from that by \u2426r n .\nfor this representation, mitra 1958 showed that the pearson x has a\nlimiting noncentral chi-squared distribution, with df s n y q y 1 and non-\n\n\u2432 s f \u242a q \u2426r n .\n\n\u017e .\n\n'\n\n'\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nn\n\nn\n\n2\n\n2\n\n2\n\ni\n\ni\n\n "}, {"Page_number": 607, "text": "592\n\nasymptotic theory for parametric models\n\ncentrality parameter\n\n\u242ds n\n\nn \u2432 y f \u242a\u017e\n\u00fd\n.\nis1\n\ni\nf \u242a\u017e\ni\n\nni\n\n2\n\n.\n\n.\n\n\u02c6\n\ni\n\ni\n\ni\n\n2\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nni\n\nthis has the form of x 2, with the sample values p and \u2432 replaced by\npopulation values \u2432 and f \u242a . similarly,\nthe\nlikelihood-ratio statistic has the form of g2, with the same substitution.\n2\nhaberman 1974a, pp. 109\u1390112 showed that under certain conditions g\nand x 2 have the same limiting distribution; that is, their noncentrality values\nconverge to a common value as n \u2122 \u2b01.\n\nthe noncentrality of\n\n\u017e .\n\n\u2432 s f \u242a q \u2426\n\nrepresentation 14.25 means that for large n, the noncentral chi-squared\napproximation is valid when the model is just barely incorrect. in practice, it\nis often reasonable to adopt 14.25 for fixed, finite n to approximate the\ndistribution of x , even though 14.25 would not be plausible as we obtain\nmore data. the alternative representation\n.\n\n.\n14.26\nin which \u2432 differs from f \u242a by a fixed amount as n \u2122 \u2b01 may seem more\nnatural. in fact, this is more appropriate than 14.25 for proving the test to\n\u017e\nbe consistent\ni.e., for convergence to 1 of the probability of rejecting the\nhypothesis that the model holds . for 14.26 , however, the noncentrality\nparameter \u242dgrows unboundedly as n \u2122 \u2b01, and a proper limiting distribution\ndoes not result for x 2 and g2.\nwhen the model holds, \u2426 s 0 in either representation 14.25 or 14.26 .\n.\nthat is, f \u242a s \u2432 \u242a , \u242ds 0, and the results in sections 14.3.3 and 14.3.4\napply.\n\n\u017e .\n\n\u017e .\n\n\u017e .\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n14.4 asymptotic distributions for\nlogit rrrrr loglinear models\n\n\u02c6\n\nfor loglinear models, formulas in section 8.6 for the asymptotic covariance\nmatrices of \u242a and \u2432 are special cases of ones derived in section 14.2. we\npresent these for the multinomial form of the models, which relates directly\nto that section. then we discuss the connection to poisson loglinear models.\nto constrain probabilities to sum to 1, we express loglinear models for\n\n\u02c6\n\nmultinomial sampling as\n\n.\n14.27\nwhere x is a model matrix and 1 s 1, . . . , 1 . letting x denote row i of x,\n\n\u2432 s exp x\u242a r 1 exp x\u242a\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nx\n\nx\n\ni\n\n\u2432 s \u2432 \u242a s\n\n.\n\n\u017e\n\ni\n\ni\n\nexp x \u242a\u017e\ni\n\n.\n\u00fd exp x \u242a\u017e\nk\n\nk\n\n.\n\n.\n\n "}, {"Page_number": 608, "text": "asymptotic distributions for logit r loglinear models\n\n593\n\n14.4.1 asymptotic covariance matrices\na model affects covariance matrices through the jacobian. since\n\n\u00fd exp x \u242a\nk\n\nk\n\n\u017e\n\n.\n\ns\n\n\u2b78\u2432\ni\n\u2b78\u242a\nj\n\nexp x \u242a\n\n\u017e\n\ni\n\nx y exp x \u242a \u00fd x\n\n\u017e\n\n.\n\nk\n\ni\n\nexp x \u242a\nk\n\n\u017e\n\n.\n\nk j\n\ni j\n\n.\n\u00fd exp x \u242a\u017e\nk\n\nk\n\n2\n\n.\n\ns \u2432 x y \u2432 x \u2432 ,\n\n\u00fd\n\nk j\n\ni j\n\nk\n\ni\n\ni\n\nk\n\nthe matrix of these elements has the form\n.\n\n\u2b78\u2432r\u2b78\u242a s diag \u2432 y \u2432 \u2432 x.\n\n\u017e\n\nx\n\nusing this with 14.14 and 14.16 , the information matrix at \u242a is0\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nx\n\n0\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ny1\n\nxaa s \u2b78\u2432r\u2b78\u242a\n\u017e\n\u017e\n\n\u2b78\u2432r\u2b78\u242a\ns x diag \u2432 y \u2432 \u2432 diag \u2432\ns x diag \u2432 y \u2432 \u2432 x.\n\ndiag \u2432\n.\n.0\n\nx\n0\nx\n0\n\n\u017e\n\n\u017e\n\n0\n\n0\n\n0\n\n0\n\n0\n\nx\n\nx\n\nx\n\n.\n0\ny1\n.\n\ndiag \u2432 y \u2432 \u2432 x\n\n\u017e\n\n.\n\nx\n0\n\n0\n\n0\n\nthus,\n\nfor multinomial\n\nloglinear models, \u242a is asymptotically normally\n\n\u02c6\n\ndistributed with estimated covariance matrix\n\n$\ncov \u242a s x diag \u2432 y \u2432 \u2432 x\n\n\u02c6\n\n\u017e\n\n.\n\n\u02c6 \u02c6\n\n\u02c6\n\n\u0004\n\n\u017e\n\n.\n\nx\n\nx\n\ny1\n\n4\n\nrn.\n\n\u017e\n\n14.28\n\n.\n\nsimilarly, from 14.23 the estimated asymptotic covariance matrix of \u2432 is\u02c6\n\n\u017e\n\n.\n\n$\ncov \u2432 s diag \u2432 y \u2432 \u2432 x x diag \u2432 y \u2432 \u2432 x\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nx\n\nx\n\nx\n\n\u02c6 \u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\ny1\n\n4\n\n\u02c6 \u02c6\n.\u02c6\n\n\u0004\n\u02c6 \u02c6\n\n= x\n\nx diag \u2432 y \u2432 \u2432 n.\n\n\u017e\n\nx\n\nfrom 14.23 , the pearson residuals e are asymptotically normal with\n\n\u017e\n\n.\n\n\u017e .\n\nasymp. cov e s i y \u2432\ns i y \u2432\n\nx\n\nx\n\nx\n\n\u017e\n\ny1\n\n\u2432\n\n\u2432\n\n\u017e\n\u017e\n\n1r2\n0\n1r2\n0\n\n1r2\n0\n1r2\n0\n\n.\n.\n.0\n\ny a aa\n.\nx\na\ny1r2\ny diag \u2432\n.\n\u017e\ny1\nx diag \u2432 y \u2432 \u2432 x\n4\ndiag \u2432 y \u2432 \u2432 diag \u2432\n\u017e\n\nx\n0\n\n\u017e\n\n\u017e\n\n.\n\n0\n\n0\n\nx\n\nx\n0\n\n0\n\n0\n\nx\n\nx\ny1r2\n.\n\n.\n\n0\n\n=\n\n\u0004\n\n=\n\ndiag \u2432 y \u2432 \u2432 x\n\n\u017e\n\n.\n\nx\n0\n\n0\n\n0\n\n14.4.2 connection with poisson loglinear models\nthis book expressed loglinear models in terms of poisson expected cell\nfrequencies \u242e s \u242e , . . . , \u242e , using formulas of the form\n\n.x\n\n\u017e\n\n1\n\nn\n\nlog \u242e s x \u242a .\n\na a\n\n\u017e\n\n14.29\n\n.\n\n "}, {"Page_number": 609, "text": "594\n\nasymptotic theory for parametric models\n\nthe model matrix x and parameter vector \u242a in this formula are slightly\ndifferent from x and \u242a in multinomial model 14.27 . the poisson expression\n\u017e\n.\n14.29 does not have constraints on \u242e. for multinomial model 14.27 ,\n\u00fd \u242e s n is fixed, and \u2432 s \u242ern satisfies\ni\n\na\u017e\n\n.\n\n.\n\n\u017e\n\na\n\ni\n\nlog \u242e s log n\u2432 s x\u242a q log n y log 1 exp x\u242a\n\n\u017e\n\nx\n\n\u017e\n\n.\n\n.\n\n1\n\ns x\u242a q 1\u242e\n\n..x\nwhere \u242es log n y log 1 exp x\u242a . in other words, multinomial model 14.27\nimplies poisson model 14.29 with\nx\n\n\u017e x\n\u017e\nx s 1: x\n\nand \u242a s \u242e, \u242a .\n.\n\nw\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nx\n\nx\n\n.\n\na\n\na\n\nthe columns of x in the multinomial representation must be linearly\nindependent of 1; that is, the parameter \u242e, which relates to the total sample\nsize, does not appear in \u242a. the dimension of \u242a is 1 less than the number of\nparameters reported in this text for poisson loglinear models. for instance,\nfor the saturated model, \u242a has n y 1 elements for the multinomial represen-\ntation, reflecting the sole constraint on \u2432 of \u00fd\u2432 s 1.\n\ni\n\nnotes\n\nsection 14.1: delta method\n\n14.1. for detailed discussion of large-sample theory including the delta method, see bishop\n\n.\net al. 1975, chap. 14 and sen and singer 1993 .\n\n\u017e\n\n.\n\n\u017e\n\nn\n\n14.2. in applying the delta method to a function g of an asymptotically normal random\nvector t , suppose that the first-order, . . . , a y 1 st-order differentials of the function\nare zero at \u242a, but the ath-order differential is nonzero. a generalization of the delta\ng t y g \u242a has limiting distribution involving products of\n\u017e\nmethod implies that n\norder a of components of a normal random vector. when a s 2, the limiting distribu-\ntion is a quadratic form in a multivariate normal vector, which often relates to a\nchi-squared distribution; in the univariate case, it is \u2434 g \u242a r2 times a \u2439 variable\n.\n\u017e\ncasella and berger 2001, p. 244 .\n\n2\u017e y\u017e\n\n\u017e .x\n\nar2w\n\n..\n\n2\n1\n\n\u017e\n\n.\n\n.\n\nn\n\nresampling methods such as the jackknife and the bootstrap are alternative tools\nfor estimating standard errors and obtaining confidence intervals. they can be helpful\nwhen use of the delta method is questionable\u138ffor instance, for small samples, highly\n.\nsparse data, or complex sampling designs. for details, see davison and hinkley 1997 ,\n.\nfay 1985 , parr and tolley 1982 , and simonoff 1986 .\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nsection 14.3: asymptotic distributions of residuals and goodness-of-fit statistics\n\n.\n\n\u017e\n\n14.3. if y is poisson with e y s \u242e, then for large \u242e the delta method implies y\n\nis\napproximately normal with standard deviation . this motivates an alternative good-\nness-of-fit statistic, the freeman\u1390tukey statistic, ft s 4\u00fd y y \u242e . when the\nmodel holds, ft y x is also o 1 as n \u2122 \u2b01. see bishop et al. 1975, p. 514 for\ndetails.\n\n'\u02c6i\n\u017e\n\n'\n\u017e\n\n\u017e .\n\n1r2\n\n1\n2\n\n.\n\n.\n\n2\n\n2\n\np\n\ni\n\n "}, {"Page_number": 610, "text": "problems\n\n595\n\nresults of this chapter do not apply when the number of cells n grows as n \u2122 \u2b01, or\nwhen different expected frequencies grow at different rates. haberman 1988 showed\nthe consistency of x 2 breaks down with non-standard asymptotics.\n\n\u017e\n\n.\n\n14.4. drost et al. 1989 showed noncentral approximations using other sequences of alterna-\n\n\u017e\n\n.\n\n.\ntives than the local and fixed ones 14.25 and 14.26 .\n\n\u017e\n\n.\n\n\u017e\n\nproblems\n\n14.1 explain why:\n\na. if c ) 0, n s o 1 as n \u2122 \u2b01.\nb. if c / 0, cz has the same order as z ; that is, o cz\n\n\u017e .\n\nyc\n\n\u017e\n\nn\n\nn\n\n.\nc. o y o z s o y z\n\nand o cz\n.\n\n\u017e\n\u017e\n\n\u017e\n.\nto o z\nn\n. \u017e\n\u017e\nn\n.\n\u017e\n.\no y z\nn n\n\nn\n\nn\n\nn n\n\n\u017e\nis equivalent to o z\n, o y o z s o y z\n.\n\n.\n.\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nn\n\nn\n\nn\n\nn n\n\n.\n\nn\n\nis equivalent\n\n.\n\n, o y o z s\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nn\n\nn\n\n14.2 if x 2 has an asymptotic chi-squared distribution with fixed df as\n\nn \u2122 \u2b01, then explain why x rn s o 1 .\n\u017e .\n\n2\n\np\n\n14.3 a. use tchebychev\u2019s inequality to show that if e x s \u242e and\n\n\u017e\n\n.\n\nn\n\nn\n\nvar x s \u2434 - \u2b01, then x y \u242e s o \u2434 .\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nn\n\nn\n\np\n\nn\n\n2\nn\n\nn\n\nb. suppose that y , . . . , y are independent with e y s \u242e and\nvar y s \u2434 for i s 1, . . . , n. let y s \u00fd y rn. apply part a\n\u017e .\nto show that y y \u242es o n\n\u017e\n\ny1r2\n\n.\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nn\n\nn\n\n1\n\n2\n\ni\n\ni\n\ni\n\ni\n\nn\n\np\n\n14.4 let y be a poisson random variable with mean \u242e.\n\na. for a constant c ) 0, show that\n\ne log y q c s log \u242eq c y r\u242eq o \u242e\ny2\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n1\n\n.2\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nx .\nhint: note that log y q c s log \u242eq log 1 q y q c y \u242e r\u242e .\nb. cell counts in a 2 = 2 table are independent poisson random\nvariables. use part a to argue that to reduce bias in estimating\nthe log odds ratio, a sensible estimator is the sample log odds ratio\nafter adding\n\nto each cell.\n\n\u017e .\n\n\u017e\n\n.\n\nw\n\n1\n2\n\n14.5 let p denote the sample proportion for n independent bernoulli\n.x1r2\n\nw\ntrials. find the asymptotic distribution of the estimator p 1 y p\nof the standard deviation. what happens when \u2432s 0.5?\n\n\u017e\n\n14.6 suppose that t has a poisson distribution with mean \u242ds n\u242e, for\nfixed \u242e) 0. for large n, show that the distribution of log t is\napproximately normal with mean log \u242d and variance \u242d . hint: by\n\n\u017e .\n\ny1\n\nw\n\nn\n\nn\n\n "}, {"Page_number": 611, "text": "596\n\nthe central\nlarge n.\n\nx\n\nasymptotic theory for parametric models\nlimit theorem, t rn is approximately n \u242e, \u242ern for\n\n\u017e\n\n.\n\nn\n\n14.7 a. refer to problem 14.6. if t is poisson, show t has asymptotic\n\nn\n\n'\n\nn\n\n1\nvariance .4\n\nb. for a binomial sample with n trials and sample proportion p,\n'\np is 1r4n. this transfor-\n.\nshow the asymptotic variance of sin\nmation and the one in part a are \u00aeariance stabilizing, producing\nvariates with asymptotic variances that are the same for all values\nof the parameter. traditionally, these transformations were em-\nployed to make ordinary least squares applicable to count data.\nsee cochran 1940 for discussion and ml analyses.\n\ny1\u017e\n\n\u017e .\n\nw\n\nx\n\n14.8 for a multinomial n, \u2432 distribution, show the correlation between\n. what does this equal when\n\np and p is y \u2432\u2432r 1 y \u2432 1 y \u2432\n\u2432 s 1 y \u2432 and \u2432 s 0 for k / i, j?\n\n.x1r2\n\n.\u017e\n\n\u017e\n\nw\n\ni\n\ni\n\ni\n\ni\n\nj\n\nj\n\nj\n\n\u017e\n\n\u0004\n\n4.\n\nj\n\nk\n\ni\n\n.\n\n14.9 an animal population has n species, with population proportion \u2432\ni\nof species i. simpson\u2019s index of ecological di\u00aeersity simpson 1949 is\ni \u2432 s 1 y \u00fd\u2432 . rao 1982 surveyed diversity measures.\n\u017e\na. two animals are randomly chosen from the population, with\nreplacement. show i \u2432 is the probability they are different\nspecies.\n\n2 w\ni\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nx\n\nb. for proportions p for a random sample, show that the estimated\n\nasymptotic standard error of i p is\n\n\u017e .\n\n\u00bd\n\n2\n\np y\n\n3\ni\n\n\u00fd\n\ni\n\n\u017e\n\n2\n\n/\n\n2\np\ni\n\n\u00fd\n\ni\n\n5\n\nn\n\n1r2\n\n.\n\n\u0004\n\n4\n\ni\n\n14.10 let y be independent poisson random variables. show by the delta\nmethod that the estimated asymptotic variance of \u00fda log y is \u00fda ry .\ni\nwthis formula applies to ml estimators of parameters for the satu-\n\u017e\n.\nrated loglinear model, which are contrasts of log y\n. formula 14.9\nyields the asymptotic covariance structure of such estimators; see lee\n. x\n\u017e\n1977 .\n\n.4\n\n2\ni\n\n\u017e\n\n.\n\n\u017e\n\n\u0004\n\ni\n\ni\n\ni\n\n14.11 assuming two independent binomial samples, derive the asymptotic\n\n.\nstandard error of the log relative risk section 3.1.4 .\n\n\u017e\n\n14.12 refer to problem 3.27. the sample size may need to be quite large\nfor the sampling distribution of \u2425 to be approximately normal,\nespecially if \u2425 is large. the fisher-type transform \u2430s log 1 q \u2425 r\u02c6\n.\n1 y \u2425 agresti 1984, pp. 166\u1390167, 177; o\u2019gorman and woolson\n\u017e\n1988 converges more quickly to normality.\n\n.x \u017e\n\n\u02c6\n.\n\nw\u017e\n\n\u02c6\n\n\u02c6\n\n1\n2\n\n<\n\n<\n\n "}, {"Page_number": 612, "text": "problems\n\n597\n\na. show that the asymptotic variance of \u2430 equals the asymptotic\n\nvariance of \u2425 multiplied by 1 y \u2425\n\n\u017e\n\n2.y2\n\n.\n\n\u02c6\n\n\u02c6\n\nb. explain how to construct a confidence interval for \u2430 and use it to\n\nc. show that \u2430s log crd . for 2 = 2 tables, show that this is half\n\n\u017e\n\n.\n\nobtain one for \u2425.\n1\n2\n\n\u02c6\n\nthe log odds ratio.\n\ni\n\n\u017e\n\n2\u017e .\n\n14.13 let \u243e t s \u00fd t y \u2432 r\u2432 . then \u243e p s x rn, where x is\nthe pearson statistic 1.15 for testing h : \u2432 s \u2432 , i s 1, . . . , n, and\nn\u243e \u2432 is the noncentrality for that test when \u2432 is the true value.\nunder h , why does the delta method not yield an asymptotic normal\ndistribution for \u243e p ? see note 14.2.\n\n2\u017e .\n\n2\u017e .\n\n.2\n.\n\n2\u017e\n\ni0\n\ni0\n\ni0\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n2\n\n2\n\n0\n\n0\n\ni\n\ni\n\n.\n14.14 in an i = j contingency table, let \u242a denote local odds ratio 2.10 ,\n\n\u017e\n\n\u02c6\ni j\n\nand let \u242a denote its sample value.\n\u02c6\na. show that asymp. cov n log \u242a ,\ni j\n'\n\ny1\niq1, jq1\n\n'\n\u017e\n\n'\n\u017e\n\nx\n.\n\n\u2432\n\ni j\n\n\u02c6\nb. show that asymp. cov n log \u242a , n log \u242a\niq1, jq1\nc. when \u242a and \u242a use mutually exclusive sets of cells, show that\n\ny1\niq1, jq1\n\n\u02c6\ni j\n\n.\n\n.\n\ns \u2432\n\n\u02c6\ni j\n\nasymp. cov n log \u242a , n log \u242a s 0.\n\n.\n\n\u02c6\nh k\n\ni j\n\n\u02c6\nd. state the asymptotic distribution of log \u242a .i j\n\n'\n\u017e\n\n'\n\n\u02c6\nh k\u02c6\n\n'\n\u02c6\nn log \u242a\niq1, j\n\n.\n\ns y \u2432 q\n\ny1\niq1, j\n\nw\n\n\u017e\n\n14.15 for loglinear model xy, xz, yz , ml estimates of \u242e and hence\nthe x 2 and g2 statistics are not direct. alternative approaches may\nyield direct analyses. for 2 = 2 = 2 tables, find a statistic for testing\nthe hypothesis of no three-factor interaction, using the delta method\nwith the asymptotic normality of log\u242a , where\n\ni jk\n\n\u02c6\n111\n\n\u0004\n\n4\n\n.\n\n\u02c6\u242a s\n111\n\np\np\n\n111\n\n112\n\n221\n\np rp\np rp\n\n222\n\n121\n\n122\n\np\n211\np\n\n212\n\n.\n\n14.16 refer to section 14.2.2, with \u233a s diag \u2432 y \u2432 \u2432 the covariance\n\n\u017e\n\n.\n\nx\n\nmatrix of n p , . . . , p\n\n1\n\n.\nx\n\n. let\n\nny1\n\n' \u017e\n\nz s\u00bd 0\n\nc\n\ni\n\nwith probability \u2432 ,\ni\nwith probability \u2432\nn\n\ni s 1, . . . , n y 1\n\n\u017e\n\nand let c s c , . . . , c\na. show that e z s c \u2432, e z s c diag \u2432 c, and var z s c \u233ac.\nb. suppose that at least one c / 0, and all \u2432 ) 0. show var z ) 0,\n\nny1\nx\n\n2.\n\n.x\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n1\n\nx\n\nx\n\ni\n\ni\n\nand deduce that \u233a is positive definite.\n\n "}, {"Page_number": 613, "text": "598\n\nasymptotic theory for parametric models\nc. if \u2432 s \u2432 , . . . , \u2432 , so \u233a is n = n, prove that \u233a is not positive\n\n.x\n\n1\n\nn\n\n\u017e\ndefinite.\n\n14.17 consider the model\n\n.\n\n\u017e\n\n22\n\n11\n\nfor a 2 = 2 table, \u2432 s \u242a2, \u2432 s \u2432 s\n\u242a 1 y \u242a , \u2432 s 1 y \u242a , where \u242a is unknown problems 3.31 and\n\u017e\n.2\n.\n10.34 .\na. find the matrix a in 14.14 for this model.\nb. use a to obtain the asymptotic variance of \u242a. as a check, it is\nsimple to find it directly using the inverse of ye\u2b782lr\u2b78\u242a2, where\nl is the log likelihood. for which \u242a value is the variance maxi-\nmized? what is the distribution of \u242a if \u242as 0 or \u242as 1?\n\n\u02c6 \u017e\n\n\u02c6\n\n12\n\n21\n\n\u017e\n\n.\n\n.\n\n\u017e\n\nc. find the asymptotic covariance matrix of n \u2432.\u02c6\nd. find df for testing fit using x 2.\n\n'\n\n14.18 refer to the model for the calf data in section 1.5.6. obtain the\n\nasymptotic variance of \u2432.\u02c6\n\n14.19 justify the use of estimated asymptotic covariance matrices. for\n\ninstance, for large samples, why is a\u2b18a close to a\u2b18a?\n\n\u02c6 \u02c6\n\n14.20 cell counts y are independent poisson random variables, with\n\n\u242e s e y . consider the poisson loglinear model\n\n\u017e\n\n.\n\ni\n\ni\n\n\u0004\n\n4\n\ni\n\nlog \u242e s x \u242a , where \u242e s \u242e , . . . , \u242e .\n.\n\n\u017e\n\na a\n\nn\n\n1\n\nusing arguments similar to those in section 14.2, show that the\n\u02c6\nlarge-sample covariance matrix of \u242a\ncan be estimated by\nw\nx diag \u242e x\n\n, where \u242e is the ml estimator of \u242e.\n\nxy1\n\n.\n\n\u017e\n\na\n\n\u02c6\n\na\n\n\u02c6\n\nx\na\n\n14.21 for a given set of parameter constraints, show that weak identifiabil-\nity conditions hold for the independence loglinear model for a two-way\ntable; that is, when two values for \u242a give the same \u2432, those parame-\nter vectors must be identical.\n\n14.22 use the delta method, with derivatives 14.22 , to derive the asymp-\ntotic covariance matrix in 14.23 for residuals. show that this matrix\nis idempotent.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n14.23 in some situations, x 2 and g2 take very similar values. explain the\n\u017e .\njoint influence on this event of a whether the model holds, b\nwhether the sample size n is large, and c whether the number of\ncells n is large.\n\n\u017e .\n\n\u017e .\n\n "}, {"Page_number": 614, "text": "problems\n\n599\n\n14.24 show x and \u242a in multinomial representation 14.27 for the indepen-\ndence model for an i = j table. by contrast, show x for the\n.\ncorresponding poisson loglinear model 14.29 .\n\n\u017e\n\na\n\n\u017e\n\n.\n\n14.25 using 14.18 and 14.28 , derive the asymptotic cov \u2432 for a multino-\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nmial loglinear model.\n\n$\n\u017e\n\n.\n\n\u02c6\n\n14.26 consider the ml estimator \u2432 s p p\niq qj\n\nof \u2432 for the independence\nmodel, when that model does not hold. show that e p p s\n\u2432 \u2432 n y 1 rn q \u2432 rn. to what does \u2432 converge as n in-\n\u017e\niq qj\ncreases?\n\niq qj\n\n\u02c6i j\n\n\u02c6\n\n.\n\n\u017e\n\n.\n\ni j\n\ni j\n\ni j\n\n14.27 let \u2428 denote a generic measure of association. for k independent\n'\nn \u2428 y \u2428\n.\nk\n\nmultinomial\nn 0, \u2434 as n \u2122 \u2b01. a summary measure is\n26\u017e\nk\n\nsuppose that\n\nsamples of\n\n4\nn ,\nk\n\nsizes\n\n\u02c6\nk\n\n.\n\n\u017e\n\n\u0004\n\nd\n\nk\n\nk\n\n\u2428s\n\n.\n\nk\n\nk\n\n\u017e\n\n\u00fd n r\u2434 \u2428\u02c6\n\u017e\n.\n2 \u02c6\nk\nk\n\u00fd n r\u2434\u02c6\n.\n2\nk\n.x\n\n2\n\nk\n\nk\n\n\u02c6\n\na. show that \u00fd z s v q \u2428 r\u2434 \u2428 , where\n\n\u017e\n\n2\n\n2\nk\n\nk\n\nw\n\nv s\n\n\u00fd\n\nk\n\n\u017e\nn \u2428 y \u2428\n\nk\n\n\u02c6\nk\n\u02c6\n2\n\u2434\nk\n\n2\n\n/\n\n,\n\nz s\n\nk\n\n1r2\n\u02c6\nn \u2428\nk\nk\n\u02c6\n\u2434\nk\n\n, \u2434 \u2428 s\n\n\u02c6 \u017e\n\n.\n\n2\n\n\u017e\n\nn\nk\n\u02c6\n2\n\u2434\nk\n\n\u00fd\n\nk\n\ny1\n\n.\n\n/\n\nb. suppose that n \u2122 \u2b01 with n rn \u2122 \u2433 ) 0, k s 1, . . . , k. state the\nasymptotic chi-squared distribution for each component in the\npartitioning in part a . indicate the hypothesis that each tests.\n\n\u017e .\n\nk\n\nk\n\n "}, {"Page_number": 615, "text": "c h a p t e r 1 5\n\nalternative estimation theory\nfor parametric models\n\n\u017e\n\n.\n\nin this book we have used the maximum likelihood ml approach to\ninference. this is by far the most common approach for categorical data\nanalysis. other paradigms have been used, however. in this chapter we\ndiscuss some of them. these methods have similar asymptotic properties as\nmaximum likelihood, so the large-sample theory of chapter 14 applies also to\nthem.\n\nin section 15.1 we discuss weighted least squares for fitting models for\ncategorical data. this and related quasi-likelihood methods introduced in\nsections 4.7 and 11.4 are sometimes simpler to apply than ml.\n\nthe bayesian paradigm is increasingly popular as computations become\neasier to implement. a full discussion of modern developments with this\napproach is beyond our scope, but in section 15.2 we present bayesian\nmethods of estimating cell probabilities in a contingency table. four other\nmethods of estimation for categorical data are described in the final section.\n\n15.1 weighted least squares for categorical data\n\nweighted least squares wls is an extension of ordinary least squares that\npermits responses to be correlated and to have nonconstant variance. famil-\niarity with the wls method is useful because:\n\n\u017e\n\n.\n\n1. wls computations have a standard form that is simple to apply for a\n\nwide variety of models.\n\n2. algorithms for calculating ml estimates often consist of iterative use of\nwls. an example is the fisher scoring method for generalized linear\n.\nmodels section 4.6.3 .\n\n\u017e\n\n3. when the model holds, wls and ml estimators are asymptotically\n.\nequivalent, both falling in the class of best asymptotically normal ban\n\n\u017e\n\n600\n\n "}, {"Page_number": 616, "text": "weighted least squares for categorical data\n\n601\n\nestimators. for large samples, the estimators are approximately nor-\nmally distributed around the parameter value, and the ratio of their\nvariances converges to 1.\n\ngrizzle, starmer, and koch 1969 popularized wls for categorical data\nanalyses. in honor of them, wls for such analyses is often called the gsk\nmethod. this section summarizes the ingredients of this approach.\n\n\u017e\n\n.\n\n15.1.1 notation and preliminaries for wls approach\nfor a response variable y with j categories, consider multinomial samples of\nsizes n , . . . , n at i levels of an explanatory variable or combinations of\nlevels of several explanatory variables. let \u2432 s \u2432 , . . . , \u2432 , where\n\n\u017e\n\n1\n\ni\n\nx .x\ni\n\nx\n1\n\n\u2432 s \u2432 , \u2432 , . . . , \u2432\n\n\u017e\n\n2 < i\n\n1 < i\n\ni\n\nj < i\n\nx\n\n.\n\nwith\n\n\u2432 s 1\n\nj < i\n\n\u00fd\n\nj\n\ndenotes the conditional distribution of y at level i. let p denote correspond-\ning sample proportions, with v their ij = ij covariance matrix. when the i\nsamples are independent,\n\nv s\n\nv\n1\n\n0\n\n0\n\nv2\n\n.\n\n. .\n\nvi\n' i\nn p is\n\ni\n\nfrom section 14.1.4, the covariance matrix of\n\n\u017e\n\n\u2432 1 y \u2432\n1 < i\ny\u2432 \u2432\n\n.\n\n1 < i\n\n1 < i\n\ny\u2432 \u2432\n\u2432 1 y \u2432\n\n1 < i\n\n\u017e\n\n2 < i\n\n2 < i\n\n2 < i\n\n\u2b48\u2b48\u2b48\n\n\u2b48\u2b48\u2b48\n\n.\n\nn v s\n\ni\n\ni\n\n2 < i\n.\n.\n.\n\n.\n.\n.\n\ny\u2432 \u2432\ny\u2432 \u2432\n\n1 < i\n\nj < i\n\nj < i\n\n2 < i\n.\n.\n.\n\n.\n\n.\n\ny\u2432 \u2432\n\nj < i\n\n1 < i\n\ny\u2432 \u2432\n\nj < i\n\n2 < i\n\n\u2b48\u2b48\u2b48 \u2432 1 y \u2432\n\n\u017e\n\nj < i\n\nj < i\n\neach set of proportions has\n\nlet f be a vector of u f i j y 1 response functions\n\nj y 1 linearly independent elements.\n\u017e\n\n\u017e\n\n.\n\n.\n\nf \u2432 s f \u2432 , . . . , f \u2432 .\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nx\n\nu\n\n1\n\nthe wls approach applies to linear models for f of form\n\nf \u2432 s x\u2424,\n\u017e\n\n.\n\n\u017e\n\n15.1\n\n.\n\n "}, {"Page_number": 617, "text": "602\n\nalternative estimation theory for parametric models\n\nwhere \u2424 is a q = 1 vector of parameters and x is a u = q model matrix of\nknown constants having rank q. from section 8.5.4,\nloglinear and logit\nresponse functions are special cases of f \u2432 s c log a\u2432 for certain matri-\n.\nces c and a.\n\u017e .\n\nlet f p denote the sample response functions. we assume that f has\ncontinuous second-order partial derivatives in an open region containing \u2432.\nthis assumption enables the delta method to determine the large-sample\n\u017e .\n\u017e .\nnormal distribution for f p . the asymptotic covariance matrix of f p\ndepends on the u = ij matrix\n\n\u017e\n\n.\n\n\u017e\n\nq s\n\n.\n\n\u2b78f \u2432\u017e\nk\n\u2b78\u2432\n\nj < i\n\nfor k s 1, . . . , u and all ij combinations\ni, j . linear response models have\nresponse functions of form f \u2432 s a\u2432 for a matrix of known constants a, in\nwhich case q s a. for the generalized loglinear model f \u2432 s c log a\u2432\n.\nrecall sections 8.5.4 and 11.2.5 , q s c diag a\u2432\n\u017e\na. see magnus and\nx\nneudecker 1988 for matrix differential calculus. by the multivariate delta\nmethod section 14.1.5 , the asymptotic covariance matrix of f p is\n\n.xy1\n\n\u017e .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nw\n\nw\n\nv s qvqx.\n\nf\n\n\u02c6let v denote the sample version of v , substituting sample proportions in q\nand v. for subsequent formulas, this matrix must be nonsingular.\n\nf\n\nf\n\ninference using the wls approach to model fitting\n\n15.1.2\nfor the general model 15.1 , the wls estimate of \u2424 is\n\n\u017e\n\n.\n\nb s x v x\n\n\u02c6\n\ny1\nf\n\nx\n\n\u017e\n\ny1\n\n.\n\nx\n\n.\nx v f p .\n\ny1\n\u02c6\nf\n\n\u017e\n\nthis is the \u2424 value that minimizes the quadratic form\nf p y x\u2424 .\n\u017e\n\nf p y x\u2424 v\n\u017e\n\n.\n\n.\n\nx y1\u02c6\nf\n\nthe ordinary least squares estimate, for uncorrelated responses with constant\nvariance, results when v is a constant multiple of the identity matrix.\n\nthe wls estimator has an asymptotic multivariate normal distribution,\n\n\u02c6\nf\n\nwith estimated covariance matrix\n\n$\ncov b s x v x\n\nx y1\u02c6\nf\n\n\u017e .\n\n\u017e\n\ny1\n\n.\n\n.\n\nthe normal distribution improves as the sample size increases and f p is\nmore nearly normally distributed.\n\n\u017e .\n\n "}, {"Page_number": 618, "text": "weighted least squares for categorical data\n\n603\nthe estimate b yields predicted values f s xb for the response functions.\nsince they satisfy the model, these predicted values are smoother than the\nsample response functions f p . when the model holds, f is asymptotically\nbetter than f p as an estimator of f \u2432 section 14.2.2 . the estimated\ncovariance matrix of the predicted values is\n.\n\nv s x x v x\n\u02c6\n\u02c6f\n\ny1\n\u02c6\nf\n\nx\nx .\n\n\u017e .\n\n\u017e .\n\n. \u017e\n\ny1\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\nx\n\nthe test of model goodness of fit uses the residual term\nw s f p y xb v\ny1\n\u02c6\nf\n\n.\nf p y xb s f p v f p y b x v x b,\n\u017e\n\ny1\n\u02c6\nf\n\ny1\n\u02c6\nf\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nx\n\nx\n\nx\n\nx\n\n0\n\n.\n\n\u017e\n\nwhich compares the sample response functions with their model predicted\nvalues. under h : f \u2432 y x\u2424 s 0 that the model holds, w is asymptotically\nchi-squared with df s u y q, the difference between the number of response\nfunctions and the number of model parameters.\n\u017e .f p y f. they are orthogonal to the fit f, so\n\none can more closely check the model fit by studying the residuals,\n\n\u02c6\n\ncov f p s cov f p y f q f s cov f p y f q cov f .\n.\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u00bd\n\n\u02c6\n5\n\nthus, the estimated covariance matrix of the residuals equals\n.\n\ncov f p y cov f s v y v s v y x x v x\n\ny1\n\u02c6\nf\n\n\u02c6\n\u02c6\nf\n\n\u02c6\nf\n\n\u02c6\nf\n\n\u02c6\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\nx\n\ny1\n\nx\nx .\n\ndividing the residuals by their standard errors yields standardized residuals\nhaving large-sample standard normal distributions.\nhypotheses about contrasts and other effects of explanatory variables have\nform h : c\u2424 s 0, where c is a known c = q matrix with c f q, having rank\nc. the estimator cb of c\u2424 is asymptotically normal with mean 0 under h0\nc . the wald statistic\nand with covariance matrix estimated by c x v x\n\nx \u02c6y1 y1\n\n\u017e\n\n.\n\n0\n\nx\n\nf\n\ny1\n\nx\n\nx\n\nx\n\nc\n\n\u017e\n\ny1\u02c6\nf\n\n.\nw s b c c x v x c\n\n.\n15.2\nhas an approximate chi-squared null distribution with df s c. this statistic\nalso equals the difference between residual chi-squared statistics for the\nreduced model implied by h and the full model. for the special case h :\n0\n\u2424 s 0, w s b rvar b has df s 1.\n\ncb\n\n\u017e\n\n\u017e\n\n.\n\n0\n\nx\n\ni\n\nc\n\n2\ni\n\ni\n\n15.1.3 scope of wls versus ml estimation\nthe wls approach requires estimating the multinomial covariance matrix of\nsample responses at each setting of the explanatory variables. it is inapplica-\nble when explanatory variables are continuous, since there may be only one\n\n "}, {"Page_number": 619, "text": "604\n\nalternative estimation theory for parametric models\n\nobservation at each such setting. wls also becomes less appropriate as the\nnumber of explanatory variables increases, since few observations may occur\nat each of the many combinations of settings. by contrast,\nin principle,\ncontinuous explanatory variables or many explanatory settings are not prob-\nlematic to ml.\n\nwhen a certain model holds, with large cell expected frequencies ml and\nwls give similar results. both estimators are in the class of best asymptoti-\ncally normal estimators. however, practical considerations often favor ml\nestimation. for example, zero cell counts often adversely affect the wls\napproach. the sample response functions may then be ill-defined or have a\nsingular estimated covariance matrix.\n\nwls shares with quasi-likelihood the feature that\n\ninferential results\ndepend only on specifying a model for the mean responses and specifying a\n.\nvariance function and covariance structure here, based on the multinomial .\nit does not use the likelihood function for the complete distribution. thus,\ninference uses wald methods.\n\nhistorically, an advantage of the wls approach was computational sim-\nplicity. this is not relevant now that software is available for ml analyses\nand for extensions of wls e.g., quasi-likelihood methods such as gee that\ndo not have some of its disadvantages. thus, wls is now used much less\nfrequently than it was about 25 years ago. nonetheless, it has close connec-\ntions with more sophisticated methods. some algorithms for calculating ml\nestimates iteratively use wls. also, miller et al. 1993 showed that under\ncertain conditions the solution of the first iteration in the gee fitting process\ngives the wls estimate. this equivalence uses initial estimates based directly\non sample values and assumes a saturated association structure that allows a\nseparate correlation parameter for each pair of response categories and each\npair of observations in a cluster. in this sense, gee is an iterated form of\nwls. moreover, in this case, the covariance matrix for the estimates is the\nsame in both approaches.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n15.2 bayesian inference for categorical data\n\nmethodology using the bayesian paradigm has advanced tremendously in the\npast decade. new computational methods make it easier to evaluate poste-\nrior distributions for model parameters. nonetheless, bayesian inference is\nnot as fully developed or commonly used for categorical data analysis as in\nmany other areas of statistics. for multiway contingency table analysis, partly\nthis is because of the plethora of parameters for multinomial models, often\nnecessitating substantial prior specification. bayesian theory and methods are\nbeyond the scope of this book. we present only relatively elementary prob-\nlems in which the bayesian approach applies quite naturally and is sometimes\nmore appealing than ml. we then briefly summarize more complex develop-\nments.\n\n "}, {"Page_number": 620, "text": "bayesian inference for categorical data\n\n605\n\ni\n\ni\n\n\u017e\n\n\u017e\n\n.\n\nthe first applications of bayesian methods to contingency tables involved\nsmoothing cell counts to improve estimation of cell probabilities e.g., good\n1965 . the sample proportions are ordinary ml estimators for the saturated\nmodel. when data are sparse, these can have undesirable features. large\nsparse tables often contain many sampling zeros, for which 0.0 is unappealing\nas a probability estimate. in addition, stein\u2019s results for estimating multivari-\nate normal means suggest that lower total mean-squared error occurs with\nbayes estimators that shrink the sample proportions toward some average\n.\nvalue efron and morris 1975 .\n\nin considering bayesian estimators, we cannot hope to find one that is\nuniformly better than ml. for instance, suppose that a true cell probability\n\u2432 s 0. then the sample proportion p s 0 with probability 1, and the\nsample proportion is better than any other estimator. because parameter\nvalues exist for which the sample proportion is optimal, no other estimator is\nuniformly better over the entire parameter space. here the criterion of\ncomparison is the expected value of a loss function that measures distance\nbetween the estimator and the parameter, such as squared error. in\ndecision-theoretic terms the sample proportion is an admissible estimator,\nfor standard loss functions johnson 1971 . in this sense, the sample mean for\nthe multinomial or multivariate binomial differs from the sample mean for\nthe multivariate normal, which is inadmissible dominated by bayes estima-\ntors when the dimension of the mean vector is at least three ferguson 1967,\n.\np. 170 . meeden et al. 1998 gave related results for decomposable loglinear\nmodels.\n\nanother approach for estimating cell probabilities fits an unsaturated\nmodel. often, though, there is no particular model expected to describe the\ntable well. for i = j cross-classifications of nominal variables, for instance,\nthe independence model rarely fits well. when unsaturated models approxi-\nmate the true relationship poorly, model-based estimators also have undesir-\nable properties. although they smooth the data, the smoothing is too severe\nfor large samples. the model-based estimators are inconsistent, converging\nto values that may be far from the true cell probabilities as n increases.\n\na bayesian approach to estimating cell probabilities compromises between\nsample proportions and model-based estimators. a model still provides part\nof the smoothing mechanism, with the bayes estimators shrinking the sample\nproportions toward a set of proportions satisfying the model.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n15.2.1 bayesian estimation of binomial parameter\nwe illustrate basic ideas with bayesian inference for a binomial parameter.\nlet y denote a bin n, \u2432 variate. since \u2432 falls between 0 and 1, a natural\nw\u017e\nprior density for \u2432 is the beta\n13.8 in section 13.3.1 for some choice of\n\u2423) 0 and \u2424) 0. this satisfies e \u2432 s \u2423r \u2423q \u2424 .\n.\n\u017e\n\n.\n.\n\nin bayesian inference the posterior density of a parameter, given the data,\nis proportional to the product of the prior density with the likelihood\n\n\u017e\n\n.\n\n\u017e\n\nx\n\n "}, {"Page_number": 621, "text": "606\n\nalternative estimation theory for parametric models\n\nfunction. here, the beta prior depends on \u2432 through \u2432\n, and\nthe binomial likelihood has kernel depending on \u2432 through \u2432 1 y \u2432 .\n.nyy\nthus, the posterior density h \u2432 y of \u2432 is proportional to\n\n\u017e\n\n.\n\n<\n\n\u2423y1\u017e\n\n. \u2424y1\n\n1 y \u2432\ny\u017e\n\nh \u2432 y a \u2432 1 y \u2432\n\u017e\n\n.\n\n\u017e\n\ny\n\n<\n\nnyy\n\n.\n\n\u2423y1\n\n\u2432\n\n1 y \u2432\n\n.\n\n\u017e\n\n\u2424y1\n\ns \u2432\n\nyq\u2423y1\n\n1 y \u2432\n\n.\n\n\u017e\n\nnyyq\u2424y1\n\n,\n\nfor 0 f \u2432f 1. the beta is the conjugate prior distribution. the posterior\ndensity is also beta, with parameters \u2423* s y q \u2423 and \u2424* s n y y q \u2424.\nthe mean of the posterior distribution is a bayesian estimator of a\nparameter. this is optimal when a squared-error loss function t y \u2432\n.2\ndescribes the consequence of estimating \u2432 by an estimator t ferguson\n1967, p. 46 . the mean of the beta posterior distribution for \u2432 is\ne \u2432 y s \u2423*r \u2423* q \u2424* s y q \u2423 r n q \u2423q \u2424\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n<\n\ns w yrn q 1 y w \u2423r \u2423q \u2424 ,\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n.\n\n.\n\n\u017e\n\nwhere w s nr n q \u2423q \u2424 . this is a weighted average of the sample propor-\ntion p s yrn and the mean of the prior distribution. for fixed \u2423, \u2424 , the\nweight given the sample increases as n increases. the standard deviation of\nthe posterior distribution describes the accuracy of this estimator. this equals\nthe square root of\n\n\u017e\n\n.\n\nvar \u2432 y s \u2423*\u2424*r \u2423* q \u2424* \u2423* q \u2424* q 1 .\n.\n\n. \u017e\n2\n\n\u017e\n\n.\n\n\u017e\n\n<\n\n'\np 1 y p rn , the ordinary\n\u017e\n\n.\n\nfor large n the standard deviation is roughly\nstandard error for the ml estimator \u2432s p.\n\n\u02c6\n\nthe bayes estimator requires selecting parameters \u2423, \u2424 for the prior\ndistribution. complete ignorance about \u2432 might suggest a uniform prior\ndistribution. this is the beta distribution with \u2423s \u2424s 1. the posterior\ndistribution then has the same shape as the binomial likelihood function. the\nbayes estimator is then\n\n\u017e\n\n.\n\ne \u2432 y s y q 1 r n q 2 .\n\u017e\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n<\n\n1\nthis shrinks the sample proportion slightly toward .2\n\nalternatively, a popular prior with bayesians is the jeffreys prior. this is\nproportional to the square root of the determinant of the fisher information\nmatrix for the parameters of interest, for a single observation. with a single\nparameter \u242a, this is e \u2b78 log f y \u242a r\u2b78\u242a\n. in the binomial case with\n\u242as \u2432 and n s 1, this equals \u2432 1 y \u2432\nand the prior is beta with\n\u2423s \u2424s .5. brown et al. 2001 showed that the posterior generated by this\nprior yields a confidence interval for \u2432 with good performance. it approxi-\nmates the clopper\u1390pearson interval with the mid-p adjustment sections\n\n2.x1r2\n.xy1r2\n\n\u017e\nw\n\n<\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nw\n\n2\n\n "}, {"Page_number": 622, "text": "607\nbayesian inference for categorical data\n1.4.4 and 1.4.5 . for a test of h : \u2432g against h : \u2432- , a bayesian\np-value is the posterior probability that \u2432g . routledge 1994 showed that\nwith the jeffreys prior, this posterior probability approximately equals the\none-sided mid-p-value for the ordinary binomial test.\n\n1\n2\n.\n\n1\n2\n\n1\n2\n\n.\n\n\u017e\n\na\n\n0\n\n15.2.2 dirichlet prior and posterior for multinomial parameters\n.\nthese ideas generalize from the binomial to the multinomial good 1965 .\nhave a multinomial distribution with\nsuppose that cell counts n , . . . , n\nn s \u00fdn and parameters \u2432 s \u2432 , . . . , \u2432 . the multinomial likelihood is\n1\nproportional to\n\n.x\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\nn\n\nn\n\n1\n\ni\n\nn\n\nn i\u2432 .\u0142 i\nis1\n\nfor a prior distribution over potential \u2432 values, the multivariate generaliza-\ntion of the beta is the dirichlet density\n\ng \u2432 s\n\u017e\n\n.\n\n.i\n\u232b \u00fd \u2424\u017e\nn\n\u0142\n. is1\n\u0142 \u232b \u2424\u017e\ni\n\ni\n\n\u2424y1\n\u2432\ni\ni\n\nfor 0 f \u2432 f 1 all i,\n\ni\n\n\u2432 s 1,\n\ni\n\n\u00fd\n\ni\n\nwhere \u2424 ) 0 . for it, e \u2432 s \u2424r \u00fd \u2424 .\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u0004\n\n4\n\ni\n\nj\n\nj\n\ni\n\ni\n\nthe posterior density is also dirichlet, with parameters n q \u2424 . the\n\n\u0004\n\n4\n\ni\n\ni\n\nbayes estimator of \u2432 is\n\ni\n\ne \u2432 n , . . . , n s n q \u2424\n\u017e\n\n.\n\n\u017e\n\n<\n\ni\n\n1\n\nn\n\ni\n\n\u017e\n\n.\n\n/\nn q \u2424 .\n\n\u00fd\n\nj\n\nj\n\n\u017e\n\n15.3\n\n.\n\ni\n\nlet k s \u00fd \u2424 and \u2425 s e \u2432 s \u2424rk. the \u2425 are prior guesses for the cell\nprobabilities. bayes estimator 15.3 equals the weighted average\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u0004\n\n4\n\ni\n\ni\n\ni\n\ni\n\nj\n\nnr n q k p q kr n q k \u2425 .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\ni\n\n\u017e\n\n15.4\n\n.\n\n\u017e\n\n.\n\nj\n\nj\n\ni\n\ni\n\n\u0004\n\nfrom 15.3 the bayes estimator is a sample proportion when the prior\ninformation corresponds to \u00fd \u2424 trials with \u2424 outcomes of type i, i s 1, . . . ,\nn. this interpretation may provide guidance for choosing \u2424 . the jeffreys\nprior sets all \u2424 s 0.5. good referred to k as a flattening constant, since with\nidentical \u2424 15.4 shrinks each sample proportion toward the uniform value\n\u2425 s 1rn. greater flattening occurs as k increases, for fixed n. hierarchical\ni\nmodels treat \u2424 as unknown and specify a second-stage prior for them e.g.,\n.\nalbert and gupta 1982 .\n\n4 \u017e\n\nbayes estimators combine good characteristics of sample proportions and\nmodel-based estimators. like sample proportions and unlike model-based\n\n.\n\n\u017e\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ni\n\ni\n\ni\n\n "}, {"Page_number": 623, "text": "608\n\nalternative estimation theory for parametric models\n\nestimators, they are consistent even when the model does not hold. unless\nthe model holds, the weight given the sample proportion increases to 1.0 as\nthe sample size increases. like model-based estimators and unlike sample\nproportions, the bayes estimators smooth the data. the resulting estimates,\nalthough slightly biased, usually have smaller total mean-squared error than\nthe sample proportions.\n\ni\n\ni\n\ni\n\ni\n\n2\n\n1\n\n2\n\n2\n\n1\n\n1\n\n1\n\n2\n\n1\n\n1\n\n0\n\n1\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n15.2.3 development of bayesian methods for categorical data\nwe now summarize the development of bayesian methods for categorical\ndata since good\u2019s 1965 work on smoothing multinomial proportions.\nleonard and hsu 1994 provided a more detailed review. we begin with\nmethods for two-way contingency tables.\nfor 2 = 2 tables, altham 1969 gave a bayesian analysis comparing\nparameters for two independent binomial samples. she tested h :\u2432 f \u2432\n2\nagainst \u2432 ) \u2432 using independent beta \u2423, \u2424 priors for \u2432 and \u2432 .\n2\naltham showed that the p-value that is the posterior probability that \u2432 f \u2432\n2\ncan equal the one-sided p-value for fisher\u2019s exact test. this happens when\none uses improper prior distributions \u2423 , \u2424 s 1, 0 and \u2423 , \u2424 s 0, 1 .\n.\nthese represent prior belief favoring the null hypothesis, in effect penalizing\nagainst concluding that \u2432 ) \u2432 . that is, fisher\u2019s exact test corresponds to a\nconservative prior distribution.\nif \u2423 s \u2424 s \u2425, i s 1, 2, with 0 f \u2425f 1, altham showed that the bayesian\np-value is smaller than the fisher p-value. the difference between the two is\nno greater than the null probability of the observed data. use of jeffreys\npriors with \u2423 s \u2424 s 0.5 provides a type of continuity correction to fisher\u2019s\nexact test in much the way the mid-p-value does for the frequentist approach.\nhoward 1998 showed that with these priors the posterior probability that\n\u2432 f \u2432 approximates the one-sided p-value for the large-sample z test\nusing pooled variance i.e., the signed square root of the pearson statistic; see\nproblem 3.30 for testing h : \u2432 s \u2432 against h : \u2432 ) \u2432 . howard also\ndiscussed other priors for 2 = 2 tables, including ones that treat \u2432 and \u2432\n2\nas dependent.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\naltham 1971 showed bayesian analyses for binomial proportions from\nmatched-pairs data. for a simple model in which the probability of success is\nthe same for each subject at a given occasion, she again showed that the\nclassical exact p-value section 10.1.4, using the binomial distribution is a\nbayesian p-value for a prior distribution favoring h . for a model similar to\n\u017e\n10.8 in which the probability varies by subject but the occasion effect is\nconstant, she showed that the bayesian evidence against the null is weaker\nas the number of pairs giving the same response at both occasions increases,\nfor fixed values of the numbers of pairs giving different responses at the two\noccasions. this differs from the conditional ml result, which does not\n\n.\n\n.\n\n\u017e\n\n.\n\na\n\n1\n\n2\n\n0\n\n1\n\n2\n\n1\n\n2\n\n1\n\n0\n\ni\n\ni\n\n "}, {"Page_number": 624, "text": "bayesian inference for categorical data\n\n609\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\ndepend on such pairs section 10.2.3 . ghosh et al. 2000 showed related\nresults.\n\nthe bayesian approaches presented so far focused directly on cell proba-\nbilities by using a prior distribution for them. lindley 1964 did this with\ni = j contingency tables. he considered the posterior distribution of con-\ntrasts of log probabilities, such as the log odds ratio. an alternative approach\n\u017e\nlaird 1978; leonard 1975 focused on parameters of the saturated loglinear\nmodel, using normal priors. this is not a conjugate prior, but normal\ndistributions can approximate the posterior. using independent normal\nn 0, \u2434 distributions for the association parameters is a way of inducing\nshrinkage toward the independence model laird 1978 . a hierarchical\napproach puts second-stage priors on the parameters of the prior distribution\n.\n\u017e\nleonard 1975 .\n\n2.\n\nhistorically, a barrier for the bayesian approach has been the difficulty of\ncalculating the posterior distribution when the prior is not conjugate. this is\nless problematic with modern ways of approximating posterior distributions\nby simulating samples from them. these include the importance sampling\ngeneralization of monte carlo simulation zellner and rossi 1984 and\nmarkov chain monte carlo methods such as gibbs sampling gelfand and\nsmith 1990 . zellner and rossi used bayesian methods for logistic regression\nand gelfand and smith considered a class of multinomial models with\ndirichlet prior. zeger and karim 1991 fitted generalized linear mixed\nmodels glmms essentially using a bayesian framework with priors for\nfixed and random effects.\n\nthe focus on distributions for random effects in glmms in articles such\nas zeger and karim 1991 led to the treatment of parameters in glms as\nrandom variables with a fully bayesian approach. dey et al. 2000 edited a\ncollection of articles that provided bayesian analyses for glms. for instance,\nin that volume gelfand and ghosh surveyed the subject, albert and ghosh\nreviewed item response modeling, chib modeled correlated binary data, and\nchen and dey modeled correlated ordinal data.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nbayesian methods are used increasingly in applications. for instance,\nskene and wakefield 1990 modeled multicenter binary response studies\nwith a logit model that allows the treatment\u1390response log odds ratio to vary\namong centers. this gives a bayesian alternative to the glmm analysis\npresented in section 12.3.4. daniels and gatsonis 1999 used multi-level\nglms to analyze geographic and temporal trends with clustered longitudinal\nbinary data. this built on hierarchical modeling ideas introduced by wong\n.\nand mason 1985 . an article by landrum and normand in dey et al. 2000\ngave a case study using bayesian ordinal probit and logit models. chaloner\nand larntz 1989 used a bayesian approach to determining optimal design\nfor experiments using logistic regression. j. albert has suggested bayesian\n.\nmodels for a variety of categorical data analyses. for instance, albert 1997\nmodeled associations in two-way tables and albert and chib 1993 studied\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n "}, {"Page_number": 625, "text": "610\n\nalternative estimation theory for parametric models\n\nbinary regression modeling, focusing on the probit case with extensions to\nordered multinomial responses.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n15.2.4 data-dependent choice of prior distribution\nwith bayesian analyses, careful prior specification is necessary. the use of an\nimproper prior, such as the uniform prior over the entire or positive real line,\nsometimes results in improper posteriors. one may not realize this from the\noutput of software for bayesian fitting. in addition, with simulation methods\nit may not be obvious when convergence has occurred. be suspicious if\nresults are dramatically different from ordinary ml frequentist results.\n\nsome dislike the subjectivity of the bayesian approach inherent in select-\ning a prior distribution. instead of choosing particular parameters for a prior\ndistribution, it is increasingly popular to use a hierarchical approach in which\nthose parameters themselves have a second-stage prior distribution. alterna-\ntively, the empirical bayes approach lets the data suggest parameter values\nfor use in the prior distribution e.g., efron and morris 1975 . this approach\nuses the prior that maximizes the marginal probability of the observed data,\nintegrating out with respect to the prior. laird 1978 did this for the\nloglinear model, estimating \u24342\nin normal priors for association parameters\nby finding the value that maximizes an approximation for the marginal\ndistribution of the cell counts, evaluated at the observed data. a disadvan-\ntage of empirical bayes compared to the hierarchical approach is that it does\nnot take into account the source of variability due to substituting estimates\nfor prior parameters.\n\n.\u00fd\nk s 1 y \u2432\n\nfienberg and holland 1973 proposed analyses for contingency tables\n4\nwith data-dependent priors. for a particular choice of dirichlet means \u2425\ni\n\u017e\nfor the bayes estimator 15.4 , they showed that the minimum total mean-\nsquared error occurs when\n\u017e\n.\n\n.\n15.5\nthe optimal k s k \u2425, \u2432 depends on \u2432, so they used the estimate k \u2425, p\n.\nof k in which the sample proportion p replaces \u2432. as p falls closer to the\nprior guess \u2425, k \u2425, p increases and the prior guess receives more weight in\nthe posterior estimate. they selected the prior pattern \u2425 for the cell\nprobabilities based on the fit of a simple model. for two-way tables, they\nused the independence fit \u2425 s p p\n. the bayes estimator then shrinks\niq qj\nsample proportions toward that fit.\n\n\u2425 y \u2432 .\n\n\u00fdi\n\nas in other inference, bayesian modeling should normally account for any\nordering in the response categories. for instance, in the method just men-\ntioned for smoothing contingency tables, one could shrink toward an ordinal\nmodel.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u0004\n\n\u0004\n\n4\n\n\u0004\n\n4\n\ni j\n\n2\n\n2\n\ni\n\ni\n\ni\n\n "}, {"Page_number": 626, "text": "other methods of estimation\n\n611\n\n15.3 other methods of estimation\n\nin this final section we describe some alternative estimation methods for\ncategorical data. consider estimation of \u2432 or \u242a, assuming a model \u2432 s \u2432 \u242a .\n\u017e .\nlet \u242a denote a generic estimator of \u242a, for which \u2432 s \u2432 \u242a estimates \u2432.\nthe ml estimator \u242a maximizes the likelihood. it also minimizes the deviance\n.\nstatistic g comparing observed and fitted proportions section 14.3.4 .\n\n\u02dc\u017e .\n\n\u02c6\n\n\u02dc\n\n\u02dc\n\n\u017e\n\n2\n\n15.3.2 minimum chi-squared estimators\nother estimators minimize other measures of distance between \u2432 \u242a and p.\nthe value \u242a that minimizes the pearson statistic\n\n\u017e .\n\n\u02dc\n\n2x \u2432 \u242a , p s n\n\n\u017e\n\n.\n\n\u00fd\n\n2\n\n.\n\ni\n\np y \u2432 \u242a\u017e\n.\n\ni\n\u2432 \u242a\u017e\n\ni\n\nis called the minimum chi-squared estimate. it is simpler to calculate the\nestimate that minimizes the modified statistic\n\n2x\n\nmod\n\n\u2432 \u242a , p s n\n\u017e\n\n.\n\n\u00fd\n\np y \u2432 \u242a\u017e\n\ni\n\ni\n\npi\n\n2\n\n.\n\n\u017e\n\n15.6\n\n.\n\nthat replaces the denominator by the sample proportion. this is called the\nminimum modified chi-squared estimate. it is the solution for \u242a to the\nequations\n\n.\n\ni\n\n\u017e\n\u2432 \u242a\np\ni\n\n\u00fd\n\ni\n\n.\n\n\u017e\n\u2b78\u2432 \u242a\ni\n\u2b78\u242a\nj\n\n\u017e\n\n/\n\ns 0,\n\nj s 1, . . . , q.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\nneyman 1949 introduced minimum modified chi-squared estimators. he\nshowed that they and minimum chi-squared estimators are best asymptoti-\ncally normal ban estimators. when the model holds, they are asymptoti-\ncally as n \u2122 \u2b01 equivalent to ml estimators. under the model, different\nestimation methods ml, wls, minimum chi-squared, etc.\nyield nearly\nidentical estimates of parameters when n is large. this happens partly\nbecause the estimators are consistent, converging in probability to \u242a as n\nincreases. when the model does not hold, estimates for different methods\ncan be quite different, even when n is large. the estimators converge to\nvalues for which the model gives the best approximation to reality, and this\napproximation is different when best is defined in terms of minimizing g2\nrather than minimizing x 2 or some other measure.\n\nfor any n, minimum modified chi-squared estimates are sometimes identi-\ncal to wls estimates. the connection refers to an alternative way of\n\n.\n\n "}, {"Page_number": 627, "text": "612\n\nalternative estimation theory for parametric models\n\nspecifying a model, using a set of constraint equations for \u2432,\n\n\u0004\n\n4\ng \u2432 , . . . , \u2432 s 0 .\n\u017e\n\n.\n\nn\n\n1\n\nj\n\nfor instance, for an i = j table, the i y 1 j y 1 constraint equations\n\n.\nlog \u2432 y log \u2432 y log \u2432 q log \u2432\n\n.\u017e\n\n\u017e\n\niq1 , j\n\ni , jq1\n\ni j\n\niq1 , jq1\n\ns 0\n\nspecify the model of independence. the number of constraint equations\nequals the residual df for the model.\n\nneyman 1949 noted that minimum modified chi-squared estimates result\n\n\u017e\n\n.\n\nfrom minimizing\n\n\u017e\n\nn\n\n\u00fd\nis1\n\np y \u2432\n\ni\n\n2\n\n.\ni q\n\npi\n\nnyq\n\u00fd j\njs1\n\n\u242d g \u2432 , . . . , \u2432\nn\n\n1\n\nj\n\n\u017e\n\n.\n\n\u0004\n\n4\n\nj\n\n\u017e\n\nwith respect to \u2432, where the \u242d are lagrange multipliers. when the\nconstraint equations are linear in \u2432, the resulting estimating equations are\nlinear. then bhapkar 1966 showed that these estimators are identical to\n.\nwls estimators. the statistic 15.6 then equals the wls residual statistic\n\u017e\nsection 15.1.2 for testing model fit.\n\nusually, however, constraint equations are nonlinear in \u2432, such as for the\nindependence model. the wls estimator is then the minimum modified\nchi-squared estimator based on a linearized version of the constraints,\n\n.\n\n\u017e\n\n.\n\ng p q \u2432 y p \u2b18\u2b78g \u2432 r\u2b78\u2432 s 0,\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nj\n\nj\n\nwith differential vector evaluated at p.\n\n\u017e\n\n.\n\nberkson 1944, 1955, 1980 was a strong advocate of minimum chi-squared\nmethods. for logistic regression, his minimum logit chi-squared estimators\nminimized a weighted sum of squares between sample logits and linear\npredictions. mantel 1985 criticized such methods, noting that their consis-\ntency requires group sizes to grow large, whereas ml or conditional ml,\n.\nwhen there are many nuisance parameters is consistent however information\n.\ngoes to the limit see also problem 15.14 .\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n15.3.2 minimum discrimination information\nkullback 1959 formulated estimation by minimum discrimination informa-\ntion mdi . the discrimination information for two probability vectors \u2432 and\n\u2425 is\n\n\u017e\n.\n\n.\n\n\u017e\n\ni \u2432 ; \u2425 s \u2432 log \u2432r\u2425 .\n\u017e\n.\n\n.\n\n\u017e\n\ni\n\ni\n\nn\n\n\u00fd i\nis1\n\n\u017e\n\n15.7\n\n.\n\n "}, {"Page_number": 628, "text": "other methods of estimation\n\n613\n\ni\n\n2\n\n1\n\nn\n\n4\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u0004\n.\n\nthis directed measure of distance between \u2432 and \u2425 is nonnegative, equaling\n0 only when \u2432 s \u2425. gokhale and kullback 1978 studied mdi estimates\nthat minimize i \u2432; \u2425 , subject to model constraints, using \u2425 s p for some\nproblems and \u2425 with \u2425 s \u2425 s \u2b48\u2b48\u2b48 s \u2425 s 1rn for others. good 1963\n.\nconducted related work in the area of maximum entropy.\nin some cases with \u2425 s 1rn , the mdi estimator is identical to the ml\nestimator simon 1973 . with \u2425 s p it is not ml, but it has similar asymp-\ntotic properties, being best asymptotically normal ban . then gokhale and\nkullback recommended testing goodness of fit using twice the minimized\n\u017e\n.\n2\nvalue of i \u2432; p . this statistic reverses the roles of p and \u2432 relative to g ,\nmuch as x\nin 15.6 reverses their roles relative to x . both statistics fall\nin the class of power divergence statistics cressie and read 1984; see also\nproblem 3.34 and have similar asymptotic properties. more generally, one\ncould choose any member of the power divergence statistics and define\nestimates to be the values minimizing it. under regularity conditions, they\nare all ban.\n\n2\nmod\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n2\n\n15.3.3 kernel smoothing\nkernel estimation is a smoothing method that estimates a probability density\nor mass function without assuming a parametric distribution. let k denote a\nmatrix containing nonnegative elements and having column sums equal to 1.\nkernel estimates of cell probabilities in a contingency table have form\n\n\u2432 s kp.\n\u02dc\n\n\u017e\n\n15.8\n\n.\n\nfor unordered multinomials with n categories, aitchison and aitken\n\n\u017e\n1976 used\n\n.\n\ni j\n\nk s \u242d,\n\u017e\n\ni s j\n.\n\ns 1 y \u242d r n y 1 ,\n.\n\n\u017e\n\ni / j\n\nfor 1rn f \u242df 1. the resulting kernel estimator of \u2432 has form\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n1 y \u2423 p q \u2423 1rn ,\n.\n.\n\n.\n15.9\nwhere \u2423s n 1 y \u242dr n y 1 . this estimator shrinks the sample proportion\ntoward 1rn, . . . , 1rn . as \u242d decreases from 1 to 1rn, the smoothing\nparameter \u2423 increases from 0 to 1. brown and rundell 1985 proved that\nwhen no \u2432 s 1, \u242d- 1 exists such that the total mean squared error is\nsmaller for this kernel estimator than for the sample proportions. results for\nother shrinkage estimators applied to multivariate means suggest that the\nimprovement for the kernel estimator can be large when n is small and the\ntrue cell probabilities are roughly equal.\n\nbrown and rundell generalized kernel smoothing for multiway contin-\ngency tables that may contain both nominal and ordinal variables. for a\n\n\u017e\n\n.\n\ni\n\n "}, {"Page_number": 629, "text": "614\n\nalternative estimation theory for parametric models\n\nt-way table, let l be a stochastic matrix i.e., row and column sums equal to\n.1 with elements\n\nk\n\n\u017e\n\nll s\n\nk , i j\n\n\u00bd d i, j\n\n\u242d ,\nk\n\u017e\n\nk\n\n. \u017e\n\n1 y \u242d ,\n.\n\nk\n\ni s j\ni / j,\n\nk s 1, . . . , t. they let k in 15.8 be the kronecker product\n\n.\n\n\u017e\nk s l m \u2b48\u2b48\u2b48 m l .\n\nt\n\n1\n\nk\n\nk\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nwhen variable k is ordinal, shrinkage alone is not enough, and it helps to\nborrow information from nearby cells. then d i, j\nis chosen to be smaller\nfor greater distances between categories i and j. if variable k is nominal, the\nnatural choice is d i, j s 1r i y 1 , where i\nis the number of categories\nfor variable k. for fixed \u242d , collapsing the smoothed table gives the same\nresult as smoothing the corresponding collapsing of the original table. with\n\u242d s \u242d, k s 1, . . . , t , brown and rundell described ways of finding \u242d to\n\u0004\nminimize an unbiased estimate of the total mean squared error.\n\ndong and simonoff 1995 and simonoff 1986 described other ap-\nproaches for ordered categories. most such kernels yield probability esti-\nmates of the form\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u0004\n\n4\n\n4\n\nk\n\nk\n\nk\n\nk\n\n\u2432 s 1 y \u2423 p q \u2423= smoother ,\n\u02dci\n\n\u017e\n\n.\n\ni\n\ni\n\nwhere the smoothing is designed to work well when true probabilities in\nnearby cells are similar.\n\n15.3.4 penalized likelihood\ngood and gaskins 1971 introduced the penalized likelihood method for\ndensity estimation. for log likelihood l \u2432 , the estimator maximizes\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nl* \u2432 s l \u2432 y \u2423 \u2432\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nw\n\n\u017e\n\n\u017e\n\n\u017e .\n.\nis a function that provides a roughness penalty. that is, \u2423 \u2432\nwhere \u2423 \u2b48\ndecreases as elements of \u2432 are smoother, in some sense. the penalized\nlikelihood estimator has a bayesian interpretation. with prior density pro-\nportional to exp y\u2423 \u2432 , the posterior density is proportional to the penal-\nized likelihood function. hence, the mode of the posterior distribution equals\nthe penalized likelihood estimator.\n\n.x\n\nsimonoff 1983 applied penalized likelihood to estimating cell probabili-\nties \u2432. like bayesian and kernel methods, it provides estimates that are\nsmoother than the sample proportions. for a single multinomial with ordered\ncategories, simonoff 1983 used penalty function \u2423 \u2432 s \u242d\u00fd\nlog \u2432 y\nlog \u2432\n, which encourages adjacent category estimates to be similar. for\n\nny1 \u017e\nis1\n\n.2\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni\n\niq1\n\n "}, {"Page_number": 630, "text": "615\n\nj\n\ni\n\ni j\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nnotes\ntwo-way contingency tables, simonoff suggested using \u2423 \u2432 s \u242d\u00fd \u00fd log \u242a\n.2\nwith the local odds ratios. this provides shrinkage toward the independence\nestimator. one chooses the smoothing parameter \u242d to minimize an approxi-\nmation for the mean-squared error of the estimator.\n\nin evaluating smoothing methods such as kernel smoothing and penalized\nlikelihood, it is useful to distinguish between large-sample asymptotics with a\nfixed number of cells n and sparse-data asymptotics for which n grows with\nn recall section 6.3.4 . for the former, these smoothing methods and\nbayesian inference behave asymptotically like ordinary ml i.e., the sample\nproportions . they have the same rate of convergence to true probabilities.\nthese methods then improve over ml primarily for small samples, where the\nbenefit of \u2018\u2018borrowing from the whole\u2019\u2019 occurs. for sparse-data asymptotics,\nhowever, smoothing is particularly beneficial. as the dimensions of a table\nincrease, the number of cells grows exponentially and the \u2018\u2018curse of dimen-\nsionality\u2019\u2019 occurs. accurate estimation becomes more difficult, with estima-\ntors converging more slowly to true values. the table then has an increasing\nproportion of empty cells. smoothing can be better than ml even asymptoti-\ncally. for such results, see fienberg and holland 1973 for the dirichlet-\n\u017e\nbased bayes multinomial estimator and simonoff 1983 for penalized likeli-\nhood with the multinomial. simonoff showed that consistency can occur with\nthe latter estimator in the sense that sup \u2432r\u2432 y 1\n\u02c6i\n0 as n and n grow\ni\nand the probabilities themselves approach 0.\n\n.\nfor surveys of smoothing methods, see fahrmeir and tutz 2001, chap. 5 ,\nlloyd 1999, chap. 5 , and simonoff 1996, chap. 6; 1998 . as simonoff\nnoted, all smoothing methods attempt to balance the low bias of under-\nsmoothing with the low variability of oversmoothing. the methods require\ninput from the user about the degree of smoothness, whether it be deter-\nmined by a prior distribution or some type of smoothing parameter.\n\n\u017e\n.\n\nin summary, many methods exist for smoothing categorical data. besides\nthose discussed in this section, there are traditional model-building methods.\nsome of these, such as generalized additive models section 4.8 , are also\nspecifically directed toward smoothing. a particular type of smoothing method\nmay seem most natural for a given application. an advantage of the bayesian\napproach is that its entire formulation seems less ad hoc than some others.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\np\n\ni\n\nnotes\n\nsection 15.1: weighted least squares for categorical data\n\n15.1. applications of wls include fitting mean response models grizzle et al. 1969 and\nmodels for marginal distributions koch et al. 1977 . for general discussion, see\n.\nbhapkar and koch 1968 , imrey et al. 1981 , and koch et al. 1985 .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n6\n "}, {"Page_number": 631, "text": "616\n\nalternative estimation theory for parametric models\n\nsection 15.2: bayesian inference for categorical data\n\n15.2. other literature on bayesian analyses of categorical responses includes fienberg et al.\n\u017e\n\u017e\n1999 , forster and smith 1998 , good 1976 , knuiman and speed 1988 , spiegelhal-\n.\nter and smith 1982 , and walley 1996 .\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nsection 15.3: other methods of estimation\n\n15.3. for further discussion of minimum chi-squared methods, see bhapkar 1966 , koch et\n\n\u017e\n\n.\n\n.\nal. 1985 , neyman 1949 , and rao 1963 .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n15.4. for the use of minimum discrimination information, see gokhale and kullback 1978 ,\n\n\u017e\n\n.\nireland and kullback 1968a, b , ireland et al. 1969 , and ku et al. 1971 .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n15.5. hall and titterington 1987 studied rates of convergence for multinomial kernel\nestimators. they defined one that achieves the optimal rate. ordinary kernel estimators\n.\ntend to be biased toward zero at the boundary of a table. dong and simonoff 1994\ndealt with improving kernel estimates on the boundary of large sparse tables. kernel\nmethods are also useful for discrete regression modeling. for binary response data,\n.\ncopas 1983 used one to display in a nonparametric manner the dependence of\np y s 1 on x.\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nproblems\n\napplications\n\n15.1 consider the mean response model fitted in section 7.4.6. show how\nto use wls for this analysis. identify the number of multinomial\nsamples i, the number of response categories j, the response func-\ntions f, the model matrix x, the parameter vector \u2424, and the\n\u02c6\nestimated covariance matrix v .f\n\n15.2 use wls to conduct the longitudinal analysis of depression in sec-\ntion 11.2.1. using software e.g., sas: proc catmod , obtain\nwls estimates and standard errors and compare to the ml results.\n\n\u017e\n\n.\n\n15.3 refer to problem 15.2. using these data, describe the differences\nbetween a wls and ml, and b wls and gee methods for\nmarginal models with multivariate categorical response data.\n\n\u017e .\n\n\u017e .\n\n15.4 using data from section 1.4.3, obtain a bayesian estimate of the\nproportion of vegetarians. explain how you chose the prior distribu-\ntion. compare results to those with ml.\n\n15.5 refer to table 9.8. consider the model that simultaneously assumes\n\u017e\n9.12 as well as linear logit relationships for the marginal effects of\nage on breathlessness and on wheeze.\n\n.\n\n "}, {"Page_number": 632, "text": "problems\n\n617\na. specify c, a, and x for which this model has form c log a\u2432 s x\u2424.\nb. using software, fit the model and interpret estimates.\n\ntheory and methods\n\n15.6 consider marginal homogeneity for an i = i table.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e .\n\n.\n\u017e .\n\na. letting f \u2432 s a\u2432, explain how i f \u2432 s 0, where a has i y 1\nrows, and ii f \u2432 s x\u2424, where a has 2 i y 1 rows and \u2424 has\ni y 1 elements. in part\n. x\nbhapkar\u2019s test 10.16 .\nw\n\nb. explain how to use wls to test marginal homogeneity. this is\n\nii , show a, \u2432, x, \u2424 when i s 3.\n\u017e .\n\n15.7 for wls with f \u2432 s c log a\u2432 , show that q s c diag a\u2432\n15.8 with wls, show that f p y x\u2424 \u2b18v\n\na.\nw \u017e .\nf p y x\u2424 is minimized by\n\n.xy1\n\n.x\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nx\n\nw\n\nw\n\nx\n\n\u02c6y1\nf\n\n\u2424 s x\u2b18v x\n\n\u02c6y1 y1\nf\n\n.\n\n\u017e\n\nw \u017e .\n\u017e .\nx\u2b18v f p .\n\n\u02c6y1\nf\n\n15.9 the response functions f p have asymptotic covariance matrix v .f\nderive the asymptotic covariance matrix of the wls model parame-\nter estimator b and the predicted values f s xb.\n\n\u02c6\n\n\u017e .\n\n15.10 consider the bayes estimator of a binomial parameter \u2432 using a beta\n\nprior distribution.\na. does any beta prior distribution produce a bayes estimator that\n\ncoincides with the ml estimator?\n\nb. show that the ml estimator is a limit of bayes estimators, for a\n\ncertain sequence of beta prior parameter values.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nc. find an improper prior density one for which its integral is not\nfinite such that the bayes estimator coincides with the ml estima-\ntor. in this sense, the ml estimator is a generalized bayes estima-\ntor.\n\nd. for bayesian inference using loss function w \u242a t y \u242a , the\n.\nbayes estimator of \u242a is the posterior expected value of \u242aw \u242a\ndivided by the posterior expected value of w \u242a ferguson 1967,\np. 47 . with loss function t y \u2432 r \u2432 1 y \u2432 , show the ml\nestimator of \u2432 is a bayes estimator for the uniform prior distribu-\ntion.\n\n. \u017e\n.x\n\n.2\n\n.2\n\n.\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\nw\n\ne. the risk function is the expected loss, treated as a function of \u2432.\nfor the loss function in part d , show the risk function is constant.\n\u017ebayes\u2019 estimators with constant risk are minimax; their maximum\n.\nrisk is no greater than the maximum risk for any other estimator.\nf. show that the jeffreys prior for \u2432 equals the beta density with\n\n\u017e .\n\n\u2423s \u2424s .5.\n\n "}, {"Page_number": 633, "text": "618\n\nalternative estimation theory for parametric models\n\n15.11 for the dirichlet prior for multinomial probabilities, show the poste-\nrior expected value of \u2432 is 15.3 . derive the expression for this\n.\nbayes estimator as a weighted average of p and e \u2432 .\n\n\u017e\n\n.\n\n\u017e\n\ni\n\ni\n\ni\n\n15.12 for bayes estimator 15.4 , show that the total mean squared error is\n\n\u017e\n\nkr n q k\n\n\u017e\n\n.\n\n2\n\n\u00fd\n\n\u017e\n\n\u2432 y \u2425 q nr n q k\n\n.\n\n\u017e\n\n2\n\ni\n\n.\n\n2\n\n\u017e\n\n.\n1 y \u2432 .\n\n\u00fd\n\n2\ni\n\n.\n\ni\n\nshow that 15.5 is the value of k that minimizes this.\n\n\u017e\n\n.\n\n15.13 refer to problem 15.6. for marginal homogeneity, explain why the\nminimum modified chi-squared estimates are identical to wls esti-\nmates.\n\ni\n\ni\n\n.\n\n\u017e\n\n15.14 let y be a bin n , \u2432 variate for group i,\n\ni s 1, . . . , n, with y\n\u0004\n4\nindependent. consider the model that \u2432 s \u2b48\u2b48\u2b48 s \u2432 . denote that\ncommon value by \u2432.\na. show that the ml estimator of \u2432 is p s \u00fd y r \u00fd n .\n.\nb. the minimum chi-squared estimator \u2432 is the value of \u2432 minimiz-\n\n\u017e\n\n.\n\n\u017e\n\nn\n\n1\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n\u02dc\n\ning\n\n\u017e\n\nn\n\n\u00fd\nis1\n\ny rn y \u2432\n\ni\n\n.\ni\n\u2432\n\n\u017e\n\n2\n\nq\n\nn\n\n\u00fd\nis1\n\ny rn y \u2432\n\n.\n\ni\n\ni\n\n1 y \u2432\n\n2\n\n.\n\nthe second term results from comparing 1 y y rn to 1 y \u2432 ,\n.\nthe proportions in the second category. if n s \u2b48\u2b48\u2b48 s n s 1,\nshow that \u2432 minimizes np 1 y \u2432 r\u2432q n 1 y p \u2432r 1 y \u2432 .\n.\nhence show\n\n1\n\u017e\n\n\u02dc\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\nn\n\ni\n\ni\n\n\u2432s p r p q 1 y p\n\u02dc\n\n1r2\n\n1r2\n\n\u017e\n\n1r2\n\n.\n\n.\n\n1\n2\n\nnote the bias toward\n\nin this estimator.\n\nc. argue that as n \u2122 \u2b01 with all n s 1, the ml estimator is consis-\n.\ntent but the minimum chi-squared estimator is not mantel 1985 .\n15.15 refer to problem 15.14. for n s 2 groups with n and n indepen-\ndent observations, find the minimum modified chi-squared estimator\nof \u2432. compare it to the ml estimator.\n\n\u017e\n\n1\n\n2\n\ni\n\n15.16 show that the kernel estimator 15.9 is the same as the bayes\nestimator 15.3 for the dirichlet prior with \u2424 s \u2423nr 1 y \u2423 n .\n4\nusing this result, suggest a way of letting the data determine the\nvalue of \u2423 in the kernel estimator.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u0004\n\ni\n\n\u017e\n\n.\n\n "}, {"Page_number": 634, "text": "c h a p t e r 1 6\n\nhistorical tour of categorical\ndata analysis*\n\n.\n\n\u017e\n\nthis book concludes with an informal historical overview of the evolution of\nmethods for categorical data analysis cda . we have seen that categorical\nscales are pervasive in the social sciences and the biomedical sciences. not\nsurprisingly, the development of glms for categorical responses was fos-\ntered by statisticians having ties to the social sciences or to the biomedical\nsciences.\n\nonly in the last quarter of the twentieth century did these models receive\nthe attention given early in the century to models for continuous data.\nregression models for continuous variables evolved out of francis galton\u2019s\nbreakthroughs in the 1880s. the strong influence of r. a. fisher, g. udny\nyule, and other statisticians on experimentation in agriculture and biological\nsciences ensured widespread adoption of regression and anova modeling\nby the mid-twentieth century. on the other hand, despite influential articles\naround 1900 by karl pearson and yule on association between categorical\nvariables, models for categorical responses received scant attention until the\n1960s.\n\nthe beginnings of cda were often shrouded in controversy. key figures\nin the development of statistical science made groundbreaking contributions,\nbut these statisticians were often in heated disagreement with one another.\n\n16.1 pearson\u2013yule association controversy\n\nmuch of the early development of methods for cda took place in england,\nand it is fitting that we begin our historical tour in london at the beginning\nof the twentieth century. the year 1900 is an apt starting point, since in that\nyear karl pearson introduced his chi-squared statistic x\nand g. udny\nyule presented the odds ratio and related measures of association. before\n\n2.\n\n\u017e\n\n619\n\n "}, {"Page_number": 635, "text": "620\n\nhistorical tour of categorical data analysis\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nthen most work focused on descriptive aspects for relatively simple measures.\nfor instance, goodman and kruskal 1959 noted that the belgian social\nstatistician adolphe quetelet used the relative risk in 1849.\n\nby 1900, karl pearson 1857\u13901936 was already well known in the statistics\ncommunity. he was head of a statistical laboratory at university college in\nlondon. his work the previous decade included developing a family of\nskewed probability distributions called pearson cur\u00aees , obtaining the prod-\nuct-moment estimate of the correlation coefficient and finding its standard\nerror, and extending galton\u2019s work on linear regression. in fact, pearson was\na true renaissance man, writing on a wide variety of topics that included art,\nreligion, philosophy, law, socialism, women\u2019s rights, physics, genetics, eugen-\nics, and evolution. pearson\u2019s motivation for developing the chi-squared test\nincluded testing whether outcomes on a roulette wheel in monte carlo varied\nrandomly, checking the fit to various data sets of normal distributions and\npearson curves, and testing statistical independence in two-way contingency\ntables.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\nmuch of the literature on cda early in the twentieth century consisted of\nvocal debates about appropriate ways to summarize association. pearson\u2019s\napproach assumed that continuous bivariate distributions underlie two-way\ncontingency tables pearson 1904, 1913 . he argued in favor of approximating\na measure, such as the correlation, for the underlying continuum. in 1904,\npearson introduced the term contingency as a \u2018\u2018measure of the total\ndeviation of the classification from independent probability,\u2019\u2019 and he intro-\nduced measures to describe its extent. the tetrachoric correlation is a ml\nestimate of the correlation for a bivariate normal distribution assumed to\nunderlie counts in 2 = 2 tables. it is the correlation value \u2433 in the bivariate\nnormal density that would produce cell probabilities equal to the sample cell\nproportions when that density is collapsed to a 2 = 2 table having the same\nmarginal proportions as the observed table. the mean-square contingency and\nthe contingency coefficient are normalizations of x to the 0, 1 scale.\npearson\u2019s contingency coefficient problem 3.33 for i = j tables standard-\nized x 2 to approximate an underlying correlation.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\ngeorge udny yule 1871\u13901951 , a british contemporary of pearson\u2019s, took\na different approach. having completed pioneering work developing multiple\nregression models and multiple and partial correlation coefficients, yule\nturned his attention between 1900 and 1912 to association in contingency\ntables. he believed that many categorical variables, such as vaccinated, un-\nvaccinated and died, survived , are inherently discrete. yule defined indices\ndirectly using cell counts without assuming an underlying continuum. he\npopularized the odds ratio \u242a which goodman 2000 noted may first have\nbeen proposed by a hungarian statistician, j. korosy and a transformation of\nit to the y1, q1 scale, q s \u242ay 1 r \u242aq 1 , now called yule\u2019s q problem\n2.36 . discussing one of pearson\u2019s measures that assumes underlying normal-\nity, yule argued 1912, p. 612 that \u2018\u2018at best the normal coefficient can only\nbe said to give us in cases like these a hypothetical correlation between\n\n\u017e\n\u02dd \u00a8\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\nw\n\nw\n\nx\n\nx\n\n2\n\n "}, {"Page_number": 636, "text": "pearson\u1390yule association controversy\n\n621\n\nsupposititious variables. the introduction of needless and unverifiable hy-\npotheses does not appear to me a desirable proceeding in scientific work.\u2019\u2019\nyule 1903 also showed the potential discrepancy between marginal and\nconditional associations in contingency tables, later studied by e. h. simpson\n\u017e\n1951 and now called simpson\u2019s paradox.\n\n\u017e\n\n.\n\n.\n\nx\n\n.\n\n\u017e\n\nin the first quarter of the twentieth century, karl pearson was the rarely\nchallenged leader of statistical science in britain. pearson\u2019s strong personality\ndid not take kindly to criticism, and he reacted negatively to yule\u2019s ideas. he\nargued that yule\u2019s own coefficients were unsuitable. for instance, pearson\nclaimed that their values were unstable, since different collapsings of i = j\ntables to 2 = 2 tables could produce quite different values of the measures.\npearson and d. heron 1913 filled more than 150 pages of biometrika, a\njournal he co-founded and edited, with a scathing reply to yule\u2019s criticism. in\na passage critical also of yule\u2019s well-received book an introduction to the\ntheory of statistics, they stated \u2018\u2018if mr. yule\u2019s views are accepted, irreparable\ndamage will be done to the growth of modern statistical theory. . . . yule\u2019s\nxq has never been and never will be used in any work done under his\nw\npearson\u2019s supervision. . . . we regret having to draw attention to the man-\nner in which mr. yule has gone astray at every stage in his treatment of\nassociation, but criticism of his methods has been thrust on us not only by\nmr. yule\u2019s recent attack, but also by the unthinking praise which has been\nbestowed on a text-book which at many points can only lead statistical\nstudents hopelessly astray.\u2019\u2019 pearson and heron attacked yule\u2019s \u2018\u2018half-baked\nnotions\u2019\u2019 and \u2018\u2018specious reasoning\u2019\u2019 and argued that yule would have to\nwithdraw his ideas \u2018\u2018if he wishes to maintain any reputation as a statistician.\u2019\u2019\nin retrospect, pearson and yule both had valid points. some classifica-\ntions, such as most nominal variables, have no apparent underlying continu-\nous distribution. on the other hand, many applications relate naturally to an\nunderlying continuum, and that fact can motivate models and inference e.g.,\nsection 7.2.3 . goodman 1981a, b noted that the ordinal models presented\nin sections 9.4.1 and 9.6.1 provide a sort of reconciliation between yule and\npearson, since yule\u2019s odds ratio characterizes models that fit well when\nunderlying distributions are approximately normal.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\nw\n\nhalf a century after the pearson\u1390yule controversy, leo goodman and\nwilliam kruskal surveyed the development of association measures for\ncontingency tables and made many contributions of their own. their 1979\nbook reprinted four influential articles of theirs from the journal of the\namerican statistical association on this topic. initial development of many\nmeasures occurred in the nineteenth century. their 1959 article contains the\nfollowing quote from m. h. doolittle in 1887, which illustrates the lack of\nprecision in early attempts to quantify the meaning of association even in\n2 = 2 tables: \u2018\u2018having given the number of instances respectively in which\nthings are both thus and so, in which they are thus but not so, in which they\nare so but not thus, and in which they are neither thus nor so, it is required\nto eliminate the general quantitative relativity inhering in the mere thingness\n\n "}, {"Page_number": 637, "text": "622\n\nhistorical tour of categorical data analysis\n\nof the things, and to determine the special quantitative relativity subsisting\nbetween the thusness and the soness of the things.\u2019\u2019 goodman 2000 added\nto the historical survey and proposed a new measure.\n\n\u017e\n\n.\n\n16.2 r. a. fisher\u2019s contributions\n\n2\n\nx\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\u017e\n\npearson\u2019s disagreements with yule were minor compared to his later ones\nwith ronald a. fisher 1890\u13901962 . using a geometric representation, fisher\n\u017e\n1922 introduced degrees of freedom to characterize the family of chi-squared\ndistributions. fisher claimed that for tests of independence in i = j tables,\nx has df s i y 1 j y 1 . by contrast, pearson 1900, 1904 had argued\nthat for any application of x 2, the index that fisher later identified as df\nequals the number of cells minus 1, or ij y 1 for two-way tables. fisher\npointed out, however, that estimating hypothesized cell probabilities using\ni y 1 q\nestimated row and column probabilities resulted in an additional\nj y 1 constraints on the fitted values, thus affecting the distribution of x .\n\u017e\n2\nnot surprisingly, pearson 1922 reacted critically to fisher\u2019s suggestion\nthat his df formula was incorrect. he stated:\n\u2018\u2018i hold that such a view\nw\nfisher\u2019s is entirely erroneous, and that the writer has done no service to the\nscience of statistics by giving it broad-cast circulation in the pages of the\njournal of the royal statistical society. . . . i trust my critic will pardon me for\ncomparing him with don quixote tilting at the windmill; he must either\ndestroy himself, or the whole theory of probable errors, for they are invari-\nably based on using sample values for those of the sampled population\nunknown to us.\u2019\u2019 pearson claimed that using row and column sample propor-\ntions to estimate unknown probabilities had negligible effect on large-sample\ndistributions, although he had realized pearson 1917 that df must be\nadjusted when the cell counts have linear constraints. fisher was unable to\nget his rebuttal published by the royal statistical society, and he ultimately\nresigned his membership.\n\nstatisticians soon realized that fisher was correct, but he maintained\nmuch bitterness over this and other dealings with pearson. in the preface to a\nlater volume of his collected works, he remarked that his 1922 article \u2018\u2018had to\nfind its way to publication past critics who, in the first place, could not\nbelieve that pearson\u2019s work stood in need of correction, and who, if this had\nto be admitted, were sure that they themselves had corrected it.\u2019\u2019 writing\nabout pearson: he stated: \u2018\u2018if peevish intolerance of free opinion in others is\na sign of senility, it is one which he had developed at an early age.\u2019\u2019 in fisher\n\u017e\n1926 , he was able to dig the knife a bit deeper into the pearson family using\n11,688 2 = 2 tables randomly generated assuming independence by karl\npearson\u2019s son, e. s. pearson. fisher showed that the sample mean of x 2 for\nthese tables was 1.00001, much closer to the 1.0 predicted by his formula for\ne x of df s i y 1 j y 1 s 1 than pearson\u2019s ij y 1 s 3. his daughter,\n\u017e\n\n2.\n\n.\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n "}, {"Page_number": 638, "text": "r. a. fisher\u2019s contributions\n\n623\n\n.\n\n\u017e\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\njoan fisher box 1978 , discussed this and other conflicts between fisher and\n\u017e\n\u017e\npearson. hald 1998, pp. 652\u1390663 , plackett\n1983 , and stigler\n1999,\nchap. 19 summarized the chi-squared controversy.\n\nfisher\u2019s preeminent reputation among statisticians today accrues mainly\nfrom his theoretical work introducing concepts such as sufficiency, informa-\ntion, and optimal properties of ml estimators and his methodological\ncontributions to the design of experiments and the analysis of variance.\nalthough not so well known for work in cda, he made other interesting\ncontributions. moreover, he made good use of the methods in his applied\nwork. for instance, fisher was also a famed geneticist. in one article, he used\npearson\u2019s goodness-of-fit test to check mendel\u2019s theories of natural inheri-\n.\ntance and showed that the fit was too good section 1.5.3 .\n\nfisher realized the limitations of large-sample methods for laboratory\nwork, and he was at the forefront of advocating specialized small-sample\nmethods. writing about large-sample methods in the preface to the first\nedition of his classic text statistical methods for research workers, he stated:\nw\n\u2018\u2018 t he traditional machinery of statistical processes is wholly unsuited to the\nneeds of practical research. not only does it take a cannon to shoot a\nsparrow, but it misses the sparrow! the elaborate mechanism built on the\ntheory of infinitely large samples is not accurate enough for simple laboratory\ndata. only by systematically tackling small sample problems on their merits\ndoes it seem possible to apply accurate tests to practical data.\u2019\u2019 fisher was\n.\namong the first to promote the work by w. s. gosset pseudonym \u2018\u2018student\u2019\u2019\non the t distribution. the fifth edition of statistical methods for research\nworkers 1934 introduced fisher\u2019s exact test for 2 = 2 contingency tables. in\nhis 1935 book the design of experiments, fisher described the tea-tasting\nexperiment section 3.5.2 motivated by his experience at an afternoon tea\nbreak while employed at rothamsted experiment station.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\nx\n\n\u017e\n\n\u017e\n\n.\n\n.\n.\n\nthe mid-1930s finally saw some model building for categorical responses.\nchester bliss 1934, 1935 , following up a 1933 report on quantal response\nmethods by j. h. gaddum, popularized the probit model for applications in\ntoxicology with a binary response. bliss introduced the term probit but used\nthe inverse normal cdf with mean 5 rather than 0, in order to avoid negative\nvalues and standard deviation 1. in the appendix of bliss 1935 , fisher\n\u017e\n1935b outlined an algorithm for finding ml estimates of model parameters.\nthat algorithm was a newton\u1390raphson type of method using expected\ninformation, today commonly called fisher scoring section 4.6.2 . stigler\n\u017e\n1986, p. 246 and finney 1971 attributed the first use of inverse normal cdf\ntransformations of proportions to the german physicist gustav fechner in\nhis 1860 book elemente der psychophysik. see finney 1971 and mcculloch\n\u017e\n2000 for other history of the probit method.\n\nthe definition for homogeneous association no interaction in contin-\ngency tables originated in an article by the british statistician maurice\nbartlett 1935 about 2 = 2 = 2 tables. bartlett showed how to find ml\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 639, "text": "624\n\nhistorical tour of categorical data analysis\n\nestimates of cell probabilities satisfying the property of equality of odds ratios\nbetween two variables at each level of the third. he attributed the idea to\nfisher.\n\nin 1940, fisher developed canonical correlation methods for contingency\ntables. he showed how to assign scores to rows and columns of a contingency\ntable to maximize the correlation. his work relates to the later development,\nparticularly in france, of correspondence analysis methods e.g., benzecri\u00b4\n.\n1973 .\n\nr. a. fisher has had the greatest influence on the practice of modern\nstatistical science. the biography by his daughter box 1978 gives a fascinat-\ning account of his impressive contributions to statistics and genetics. fienberg\n\u017e\n1980 summarized his contributions to cda.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nw\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.x\n\n16.3 logistic regression\nbartlett 1937 used log yr 1 y y\nin regression and anova to transform\nobservations y that are continuous proportions problem 6.33 . in a book of\nstatistical tables published in 1938, r. a. fisher and frank yates suggested it\nas a possible transformation of a binomial parameter for analyzing binary\ndata. in 1944, the physician and statistician joseph berkson introduced the\nterm logit for this transformation. berkson showed that the model using the\nlogit fitted similarly to the probit model, and his subsequent work did much\nto popularize logistic regression. in 1951, jerome cornfield, another statisti-\ncian with strong medical ties, used the odds ratio to approximate relative\nrisks in case\u1390control studies. dyke and patterson 1952 apparently first used\nthe logit in models with qualitative predictors.\n\nsir david r. cox introduced many statisticians to logistic regression,\nthrough his 1958 article and 1970 book, the analysis of binary data. about\nthe same time, an article by the danish statistician and mathematician georg\nrasch sparked an enormous literature on item response models. the most\nimportant of these is the logit model with subject and item parameters, now\ncalled the rasch model section 12.1.4 . this work was highly influential in\nthe psychometric community of northern europe especially in denmark, the\nnetherlands, and germany and spurred many generalizations in the educa-\ntional testing community in the united states.\n\nthe extension of logistic regression to multicategory responses received\noccasional attention before 1970 e.g., mantel 1966 but substantial work\nafter about that date. for nominal responses, early work was mainly in\nthe econometrics literature. see bock 1970 , mcfadden 1974 , nerlove\nand press 1973 , and theil 1969, 1970 . in 2000, daniel mcfadden won the\nnobel prize in economics for his work in the 1970s and 1980s on the\ndiscrete-choice model section 7.6 . for cumulative logit models for ordinal\nresponses, see bock and jones 1968 , simon 1974 , snell 1964 , walker and\nduncan 1967 , and williams and grizzle 1972 . the cumulative probit case,\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 640, "text": "multiway contingency tables and loglinear models\n\n625\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\nbased on an underlying normal response, has a longer history; see, for\n.\ninstance, aitchison and silvey 1957 and bock and jones 1968, chap. 8 .\ncumulative logit and probit models received much more attention following\npublication of mccullagh 1980 , which provided a fisher scoring algorithm\nfor ml fitting of all cumulative link models.\n\nthe next major advances with logistic regression dealt with its application\nto case\u1390control studies e.g., breslow 1996; mantel 1973; prentice 1976a;\nprentice and pyke 1979; see also section 5.1.4 and the conditional ml\napproach to model fitting for those studies and others with numerous\nnuisance parameters breslow et al. 1978, with related work in breslow 1976,\n1982; breslow and day 1980; breslow and powers 1978; cox 1970; farewell\n1979; prentice 1976a; prentice and breslow 1978; zelen 1971; see also\nsections 6.7 and 10.2 . the conditional approach was later exploited in\nsmall-sample exact inference hirji et al. 1987; mehta and patel 1995; see\n.\nalso section 6.7 .\n\nnathan mantel, whose name appears in the preceding two paragraphs,\nmade a variety of interesting contributions to cda. although best known for\nthe 1959 mantel\u1390haenszel test and related odds ratio estimator, he also\n.\ndiscussed trend tests 1963 , multinomial logit and loglinear modeling 1966 ,\nlogistic regression for case\u1390control data 1973 , the number of contingency\ntables having fixed margins gail and mantel 1977 , the analysis of square\ncontingency tables mantel and byar 1978 , and problems with minimum\n.\nchi-squared and wald tests 1985, 1987a .\n\nmore recently, attention has focused on fitting logistic models to corre-\nlated responses for clustered data. one strand of this is marginal modeling of\nlongitudinal data diggle et al. 2002; liang and zeger 1986; liang et al.\n1992 . much of this literature focuses on quasi-likelihood methods such as\ngeneralized estimating equations gee . another strand is generalized linear\n.\nmixed models e.g., breslow and clayton 1993 .\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\nperhaps the most far-reaching contribution of the past half century has\nbeen the introduction by british statisticians john nelder and r. w. m.\nwedderburn in 1972 of the concept of generalized linear models. this unifies\nthe logistic and probit regression models for binomial data with loglinear\nmodels for poisson data and with long-established regression and anova\nmodels for normal-response data. interestingly, the algorithm they used to fit\nglms is fisher scoring, which r. a. fisher introduced in 1935 for ml fitting\nof probit models. mcculloch 2000 reviewed the journey from probit models\nto glms and their further generalizations such as quasi-likelihood.\n\n\u017e\n\n.\n\n16.4 multiway contingency tables and\nloglinear models\n\nthe quarter century following the end of world war ii saw the development\nof a theoretical underpinning for models for contingency tables. h. cramer\u00b4\n\n "}, {"Page_number": 641, "text": "626\n\nhistorical tour of categorical data analysis\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n1946 derived general expressions for large-sample distributions of parame-\nter estimators. c. r. rao 1957, 1963 conducted related work.\n\nin 1949, the berkeley-based statistician jerzy neyman, who had already\nperformed fundamental work on hypothesis testing and interval estimation\nmethods with e. s. pearson, introduced the family of\nbest asymptotically\nnormal ban estimators. these have the same optimal large-sample proper-\nties as ml estimators. the ban family includes estimators obtained by\nminimizing chi-squared-type measures comparing observed proportions to\nproportions predicted by the model section 15.3.1 . this type of estimator\nitself includes some weighted least squares wls estimators. the simplicity of\ntheir computation, compared to ml estimators, was an important considera-\ntion before the advent of modern computing. neyman\u2019s 1949 only mention\nof fisher was the suggestion that fisher did not realize that estimators other\nthan ml could be ban, stating that \u2018\u2018the results . . . contradict the assertion\nof r. a. fisher, not a very clear one, that \u2018the maximum likelihood equation\nmay indeed be derived from the conditions that it shall be linear in frequen-\ncies, and efficient for all values of \u242a\u2019.\u2019\u2019 fisher, of course, returned the\ncompliment: for instance, writing 1956 about proposals for an unconditional\ntest for 2 = 2 tables, \u2018\u2018the principles of neyman and pearson\u2019s \u2018theory of\ntesting hypotheses\u2019 are liable to mislead those who follow them into much\nwasted effort.\u2019\u2019\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n.\n\n\u017e\n\u017e\n\nin the early 1950s, william cochran published work dealing with a variety\nof important topics in cda. scottish-born, cochran spent most of his career\nat american universities: iowa state, north carolina state, johns hopkins,\nand harvard. he 1940 modeled poisson and binomial responses with\nvariance-stabilizing transformations. he 1943 recognized and discussed\nways of dealing with overdispersion. he 1950 introduced a generalization\n\u017e\nfor comparing proportions in several\ncochran\u2019s q of mcnemar\u2019s test\nmatched samples. his classic 1954 article is a mixture of new methodology\nand advice for applied statisticians. it gave sample-size guidelines for chi-\nsquared approximations to work well for the x 2 statistic. it also stressed the\nimportance of directing inferences toward narrow e.g., single-degree-of-\nfreedom alternatives and partitioning chi-squared statistics into components.\none instance of this was cochran\u2019s proposed test of conditional indepen-\ndence in several 2 = 2 tables, which was closely related to the mantel and\nhaenszel 1959 test section 6.3.2 . another was a test for a linear trend in\nproportions across quantitatively defined rows of an i = 2 table section\n5.3.5 . see also cochran 1955 . fienberg 1984 reviewed cochran\u2019s contribu-\ntions to cda.\n\nbartlett\u2019s work on interaction structure in 2 = 2 = 2 contingency tables\nhad relatively little impact for 20 years. indeed, in presenting methods for\npartitioning x in 2 = 2 = 2 tables, lancaster 1951 noted that \u2018\u2018doubtless\nlittle use will ever be made of more than a three-dimensional classification.\u2019\u2019\nhowever, in the mid-1950s and early 1960s, bartlett\u2019s work was extended in\nmany ways to multiway tables. see, for instance, darroch 1962 , good\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n2\n\n "}, {"Page_number": 642, "text": "multiway contingency tables and loglinear models\n\n627\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n1963 , goodman 1964b , plackett 1962 , roy and kastenbaum 1956 , and\nroy and mitra 1956 . these articles as well as influential articles by martin\nw. birch 1963, 1964a, b, 1965 were the genesis of research work on\nloglinear models between about 1965 and 1975. birch\u2019s work was part of a\nnever-submitted ph.d. thesis at the university of glasgow. he showed how\nto obtain ml estimates of cell probabilities in three-way tables, under various\nconditions. he showed the equivalence of those ml estimates for poisson\nand multinomial sampling. he and watson 1959 extended theoretical\nresults of cramer and rao on large-sample distributions for contingency\ntable models. mantel 1966 discussed early results and made the loglinear\nmodel formula explicit. a survey article by the french statistician henri\ncaussinus 1966 , based partly on his ph.d. thesis, provides a good glimpse of\nthe state-of-the-art of cda just before this decade of advances. there,\ncaussinus introduced the quasi-symmetry model for square tables.\n\n\u00b4\n\nmuch of the work in the next decades on loglinear and related logit\nmodeling took place at three american universities: the university of chicago,\nharvard university, and the university of north carolina. at chicago, leo\ngoodman wrote a series of groundbreaking articles, dealing with such topics\nas partitionings of chi-squared, models for square tables e.g., quasi-indepen-\ndence , stepwise logit and loglinear model-building procedures, deriving\nasymptotic variances of ml estimates of loglinear parameters, latent class\nmodels, association models, correlation models, and correspondence analysis.\nfor surveys of his early work, see goodman 1968, an r. a. fisher memorial\nlecture, 1970 . for later work, see goodman 1985, 1996, 2000 . goodman\nalso wrote a stream of articles for social science journals that had a substan-\ntial impact on popularizing loglinear and logit methods for applications e.g.,\n.\ngoodman 1969b .\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nover the past 50 years, goodman has been the most prolific contributor to\nthe advancement of cda methodology. the field owes tremendous gratitude\nto his steady and impressive body of work. in addition, some of goodman\u2019s\nstudents at chicago also made fundamental contributions. in 1970, shelby\nhaberman completed a ph.d. dissertation the basis of his 1974a mono-\ngraph making substantial theoretical contributions to loglinear modeling.\namong topics he considered were residual analyses, existence of ml esti-\nmates,\nloglinear models for ordinal variables, and theoretical results for\n\u017e\nmodels such as the rasch model for which the number of parameters grows\nwith the sample size. clifford clogg followed in goodman\u2019s steps by having\ninfluence in the social sciences and in statistics with his work on association\nmodels, demography, models for rates, the census, and various other topics.\nsimultaneously with goodman\u2019s work, related research on ml methods\nfor loglinear-logit models occurred at harvard by students of frederick\n\u017e\nmosteller\nsuch as stephen fienberg and william cochran. much of this\nresearch was inspired by problems arising in analyzing large, multivariate\ndata sets in the national halothane study bishop and mosteller 1969; see\nalso p. 345 of an interview with lincoln moses in statist. sci. 14, 1999 . that\n\n.\n\n\u017e\n\n.\n\n.\n\n "}, {"Page_number": 643, "text": "628\n\nhistorical tour of categorical data analysis\n\npublisher's note:\npermission to reproduce this image\nonline was not granted by the\ncopyright holder. readers are kindly\nasked to refer to the printed version\nof this chapter.\n\nfigure 16.1 four leading figures in the development of categorical data analysis.\n\n "}, {"Page_number": 644, "text": "recent and future? developments\n\n.\n\n\u017e\n\n629\n\n\u017e\n\n.\n\nstudy investigated whether halothane was more likely than other anesthetics\nto cause death due to liver damage. a presidential address by mosteller\n\u017e\n1968 to the american statistical association described early uses of loglin-\near models for smoothing multidimensional discrete data sets. fienberg and\nhis own students advanced this work further. a landmark book in 1975 by\nhim with yvonne bishop and paul holland, discrete multi\u00aeariate analysis, was\nlargely responsible for introducing loglinear models to the general statistical\ncommunity and remains an excellent reference.\n\nresearch at north carolina by gary koch and several students and\nin the biomedical sciences. their\nco-workers has been highly influential\n.\nresearch developed wls methods for categorical data models section 15.1 .\nthe 1969 article by koch with j. grizzle and f. starmer popularized this\napproach. koch and colleagues extended it in later articles to an impressive\nvariety of problems, including problems for which ml methods are awkward\nto use, such as the analysis of repeated categorical measurement data koch\net al. 1977 . in 1966, vasant bhapkar showed that the wls estimator is often\nidentical to neyman\u2019s minimum modified chi-squared estimator.\n\nthe early literature on loglinear models treated all classifications as\nnominal. haberman 1974b and simon 1974 showed how to exploit ordinal-\nity of classifications in loglinear models. this work was extended in several\narticles by leo goodman 1979a, 1981a, b, 1983, 1985, 1986 . the extensions\nincluded association models, which replace ordered scores in loglinear mod-\nels by parameters section 9.5 . goodman 1985, 1986, 1996 also discussed\nrelated correlation models and provided a model-based perspective for the\nclosely related correspondence analysis methods.\n\ncertain loglinear models with conditional independence structure provide\ngraphical models for contingency tables. these relate to the association\ngraphs used in section 9.1. darroch et al. 1980 was the genesis of much of\nthis work.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n16.5 recent and future? developments\n\n(\n\n)\n\nthe most active area of new research in cda in the past decade has been\nthe modeling of clustered data, such as occur in longitudinal studies and\nother forms of repeated measurement. a variety of ways now exist of\nmodeling while accounting for the correlation among responses in the same\ncluster.\n\nas discussed in chapters 11 and 12, ml estimation is difficult for such\nmodels. for complex forms of generalized linear mixed models, for instance,\nit is a challenge to estimate well regression parameters and variance compo-\nnents. integrating out the random effect to obtain the likelihood function\nrequires an approximation such as numerical integration. not surprisingly,\nvarious monte carlo approaches are applied increasingly here. a promising\n\n "}, {"Page_number": 645, "text": "630\n\nhistorical tour of categorical data analysis\n\n.\n\n\u017e\n\napproach is a monte carlo em algorithm that uses a monte carlo approxi-\nmation for the e step booth and hobert 1999 . the monte carlo error can\nbe assessed at each iteration, and one can accurately reproduce the ml\nestimates with sufficiently many iterations.\n\nthe modeling of clustered correlated data is likely to be an active area of\nresearch in coming years. the class of generalized linear mixed models is\ncertain to see substantial work and further generalization. one extension\nis generalized additi\u00aee mixed models. time-series models for categorical\nresponses have so far received relatively little attention. for all such models\nwith correlated responses, model diagnostics are of vital importance and\nneed development. for longitudinal data, missing data are a common prob-\nlem. this area currently has much activity.\n\nanother important recent advance is the development of efficient algo-\nrithms for exact small-sample methods. with such methods, one can guaran-\ntee that the size of a test is no greater than some prespecified level and that\nthe coverage probability for a confidence interval is at least the nominal level.\nthe \u2018\u2018exactness\u2019\u2019 refers only to inference being based on probability distribu-\ntions that do not depend on unknown parameters. there is no unique way to\ndo this, and certain methods can be highly conservative because of discrete-\nness. most literature deals with the conditional approach, which eliminates\nnuisance parameters by conditioning on their sufficient statistics. hence, the\nbasic idea builds on fisher\u2019s exact test. conditional methods are versatile,\napplying to exponential family linear models that use the canonical\nlink\nfunction, such as loglinear models for poisson responses and logit models for\nbinomial responses. many of the computational advances with the exact\nconditional approach occurred in a series of articles by cyrus mehta, nitin\npatel, and colleagues at harvard e.g., mehta and patel 1983 , using the\nnetwork algorithm. see surveys by agresti 1992 , mehta 1994 , mehta and\npatel 1995 , and the statxact and logxact manuals cytel software, cam-\n.\nbridge, ma, founded by mehta and patel .\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nalthough the development of \u2018\u2018exact\u2019\u2019 methods has seen considerable\nprogress, certain analyses are still infeasible and likely to be so for some time\nbecause of the exponential increase in computing time as the table size or\nsample size increases. there are an ever-increasing variety of methods for\naccurate approximation of exact methods. these include simple monte carlo\n\u017e\ne.g., agresti et al. 1979 , monte carlo with importance sampling e.g., booth\nand butler 1999; mehta et al. 1988 , markov chain monte carlo mcmc;\nforster et al. 1996 , saddlepoint approximations pierce and peters 1992,\nstrawderman and wells 1998 , and related work on an approximate condi-\ntioning approach pierce and peters 1999 in which discreteness is not so\nproblematic.\n\nfinally, the development of bayesian approaches to cda is an increas-\ningly active area. the multiplicity of parameters complicates bayesian model-\n.\ning. for early use of bayesian estimation of probabilities, see good 1965\nand lindley 1964 . good\u2019s 1965 article apparently evolved from his work\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n "}, {"Page_number": 646, "text": "recent and future? developments\n\n.\n\n\u017e\n\n631\n\nduring world war ii with alan turing at bletchley park, england, on\nbreaking nazi codes. the development of the bayesian approach for cda is\ndiscussed in section 15.2.3.\n\npredicting the future is always dangerous. however, it is likely that much\nfuture research will focus on computationally intensive methods such as\ngeneralized linear mixed models. another hot topic, largely outside the realm\nof traditional modeling, is the development of algorithmic methods for huge\ndata sets with large numbers of variables. such methods, often referred to as\ndata mining, deal with the handling of complex data structures, with a\npremium on predictive power at the sacrifice of simplicity and interpretability\nof structure. important areas of application include genetics, such as the\nanalysis of discrete dna sequences in the form of very high-dimensional\ncontingency tables, and business applications such as credit scoring and\ntree-structured methods for predicting future behavior of customers.\n\n.\nsources for the historical tour in this chapter include stigler 1986 ,\nstudies in the history of probability and statistics, edited by e. s. pearson and\nm. g. kendall london: griffin, 1970 , and personal conversations over the\nyears with several statisticians, including erling andersen, r. l. anderson,\nhenri caussinus, william cochran, sir david cox, john darroch, leo\ngoodman, gary koch, frederick mosteller, john nelder, c. r. rao, stephen\nstigler, geoffrey watson, and marvin zelen. to readers who have made it\nthis far, i congratulate your perseverance! to develop a more complete\nunderstanding of the historical development of cda, you may want to study\nthe following chronological list of 25 sources. these convey a sense of how\nmethodology has evolved. alternatively, look at some early books on this\ntopic, such as a. e. maxwell\u2019s analysing qualitati\u00aee data new york:\nmethuen, 1961 , r. l. plackett\u2019s the analysis of categorical data london:\ngriffin, 1974 , and the bishop, fienberg, and holland discrete multi\u00aeariate\n.\nanalysis cambridge, ma: mit press 1975 .\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\u017e\n\u017e\n\u017e\n\n.\n\u017e\npearson 1900\n.\nyule 1912\n.\nfisher 1922\n.\nbartlett 1935\n.\nberkson 1944\n.\nneyman 1949\n.\ncochran 1954\n.\ngoodman and kruskal 1954\n.\nroy and mitra 1956\n.\ncox 1958a\n.\nmantel and haenszel 1959\n.\nbirch 1963\n.\nbirch 1964b\n\n\u017e\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\u017e\n\n\u017e\n\u017e\n\u017e\n\n.\ncaussinus 1966\n.\ngoodman 1968\n.\nmosteller 1968\n.\n\u017e\ngrizzle et al. 1969\n.\ngoodman 1970\n.\nhaberman 1974a\n.\nnelder and wedderburn 1972\n.\nmcfadden 1974\n.\ngoodman 1979a\n.\nmccullagh 1980\n.\n\u017e\nliang and zeger 1986\n.\n\u017e\nbreslow and clayton 1993\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n "}, {"Page_number": 647, "text": "a p p e n d i x a\n\nusing computer software to\nanalyze categorical data\n\nin this appendix we discuss statistical software for categorical data analysis,\nwith emphasis on sas. we begin by mentioning major software that can\nperform the analyses discussed in this book. then we illustrate, by chapter,\nsas code for the analyses. information about other packages such as s-plus,\nr, spss, and stata , as well as updated information about sas, is at the web\nsite www.stat.ufl.edur;aarcdarcda.html. section a.2 on sas also lists\nother software for analyses not currently available in sas.\n\n.\n\n.\n\n\u017e\n\n\u017e\n\na.1 software for categorical data analysis\n\n\u017e\n\na.1.1 sas\nsas is general-purpose software for a wide variety of statistical analyses. the\n.\nmain procedures procs\nfor categorical data analyses are freq, gen-\nmod, logistic, nlmixed, and catmod.\n\nproc freq computes measures of association and their estimated\nstandard errors. it also performs generalized cochran\u1390mantel\u1390haenszel\ntests of conditional independence, and exact tests of independence in i = j\ntables.\n\nproc genmod fits generalized linear models. it fits cumulative link\nmodels for ordinal responses. it can perform gee analyses for marginal\nmodels. one can form one\u2019s own variance function and allow scale parame-\nters, making it suitable for quasi-likelihood analyses.\n\nproc logistic gives ml fitting of binary response models, cumulative\nlink models for ordinal responses, and baseline-category logit models for\nnominal responses. it incorporates model selection procedures, regression\ndiagnostic options, and exact conditional\ninference. proc probit also\nconducts ml fitting of binary and cumulative link models as well as quantal\n\n632\n\n "}, {"Page_number": 648, "text": "software for categorical data analysis\n\n633\n\nresponse models that permit a strictly positive probability as the linear\npredictor decreases to y\u2b01.\n\nproc catmod fits baseline-category logit models. it is also useful for\n\nwls fitting of a wide variety of models for categorical data.\n\nproc nlmixed fits generalized linear mixed models glmms . it\n\napproximates the likelihood using adaptive gauss\u1390hermite quadrature.\n\nother programs run on sas that are not specifically supported by the sas\ninstitute. for further details about sas for categorical data analyses, see the\nvery helpful guide by stokes et al. 2000 . also useful are sas publications on\n.\nlogistic regression allison 1999 and graphics friendly 2000 .\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\na.1.2 other software packages\nmost major statistical software has procedures for categorical data analyses.\nfor instance, see spss spss regression models 10.0 by m. j. norusis, spss\ninc., 1999 , stata a handbook of statistical analyses using stata, 2nd ed., by\ns. rabe-hesketh and b. everitt, crc press, boca raton, fl, 2000 , s-plus\n\u017e modern applied statistics with s-plus, 3rd ed., by w. n. venables and b. d.\nripley, springer-verlag, new york, 1999 , and the related free package, r,\nand glim aitkin et al. 1989 . most major software now follows the lead of\nglim and includes a generalized linear models routine. examples are\nproc genmod in sas and the glm function in r and s-plus.\n\nfor certain analyses, specialized software is better than the major pack-\n.\nages. a good example is statxact cytel software, cambridge, massachusetts ,\nwhich provides exact analysis for categorical data methods and some non-\nparametric methods. among its procedures are small-sample confidence\nintervals for differences and ratios of proportions and for odds ratios, and\nfisher\u2019s exact test and its generalizations for i = j tables. it can also conduct\nexact tests of conditional independence and of equality of odds ratios in\n2 = 2 = k tables, and exact confidence intervals for the common odds ratio\nin several 2 = 2 tables. statxact uses monte carlo methods to approximate\nexact p-values and confidence intervals when a data set is too large for exact\ninference to be computationally feasible. its companion, logxact, performs\nexact conditional logistic regression.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\nother examples of specialized software are sudaan for gee-type\nanalyses that handle clustering in survey data research triangle institute,\nresearch triangle park, north carolina , latent gold for latent class\nmodeling statistical innovations, belmont, massachusetts , mln institute\nof education, london and hlm scientific software, chicago for multi-\nlevel models, and pass for power analyses ncss statistical software,\nkaysville, utah . s-plus and r functions are also available from individuals\nor from published work for particular analyses. for instance, statistical\nmodels in s by j. m. chambers and t. j. hastie wadsworth, belmont,\ncalifornia, 1993, p. 227 showed the use of s-plus in quasi-likelihood analyses\nusing the quasi and make.family functions.\n\n.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 649, "text": "634\n\nusing computer software to analyze categorical data\n\ntable a.1 sas code for chi-squared, measures of association,\nand residuals for education\u2013religion data in table 3.2\n\ndata table;\n\ninput degree religion $ count @@;\n\ndatalines;\n1 fund 178\n2 fund 570\n3 fund 138\n\n;\n\n1 mod 138\n2 mod 648\n3 mod 252\n\n1 lib 108\n2 lib 442\n3 lib 252\n\nproc freq order = data; weight count;\n\ntables degree*religion/ chisq expected measures cmh1;\n\nproc genmod order = data; class degree religion;\n\nmodel count = degree religion / dist = poi link = log residuals;\n\na.2 examples of sas code by chapter\n\n\u017e\n\n.\n\nthe examples below show sas code version 8.1 . we focus on basic model\nfitting rather than the great variety of options. the material is organized by\nchapter of presentation. for convenience, data for examples are entered in\nthe form of the contingency table displayed in the text. in practice, one\nwould usually enter data at the subject level. these tables and the full data\nsets are available at www.stat.ufl.edur;aarcdarcda.html.\n\n.\n\nchapters 1\u20133: introduction, two-way contingency tables\ntable a.1 uses sas to analyze table 3.2. the @@ symbol indicates that\neach line of data contains more than one observation. input of a variable as\ncharacters rather than numbers requires an accompanying $ label\nin the\ninput statement. proc freq forms the table with the tables state-\nment, ordering row and column categories alphanumerically. to use instead\nthe order in which the categories appear in the data set e.g., to treat the\nvariable properly in an ordinal analysis , use the order s data option in\nthe proc statement. the weight statement is needed when one enters\nthe cell counts instead of subject-level data. proc freq can conduct\nchi-squared tests of independence chisq option , show its estimated ex-\npected frequencies expected , provide a wide assortment of measures of\nassociation and their standard errors measures , and provide ordinal\nstatistic 3.15 with a \u2018\u2018nonzero correlation\u2019\u2019 test cmh1 . one can also\nperform chi-squared tests using proc genmod using loglinear models\ndiscussed in the chapters 8\u13909 section of this appendix , as shown. its\nresiduals option provides cell residuals. the output labeled \u2018\u2018streschi\u2019\u2019\n.\nis the standardized pearson residual 3.13 .\n\ntable a.2 analyzes table 3.8. with proc freq, for 2 = 2 tables the\nmeasures option in the tables statement provides confidence intervals\n\n\u017e\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n.\n\n "}, {"Page_number": 650, "text": "examples of sas code by chapter\n\n635\n\ntable a.2 sas code for fisher\u2019s exact test and confidence intervals\nfor odds ratio for tea-tasting data in table 3.8\n\ndata fisher;\ninput poured guess count @@;\ndatalines;\n1 1 3\n;\nproc freq;\n\nweight count;\n\n2 2 3\n\n1 2 1\n\n2 1 1\n\ntables poured*guess/ measures riskdiff;\nexact fisher or / alpha = .05;\n\nproc logistic descending; freq count;\n\nmodel guess = poured / clodds = pl;\n\n.\n\n\u017e\n\n\u017e\n\nfor the odds ratio labeled \u2018\u2018case-control\u2019\u2019 on output and the relative risk,\nand the riskdiff option provides intervals for the proportions and their\ndifference. for tables having small cell counts, the exact statement can\nprovide various exact analyses. these include fisher\u2019s exact test and its\ngeneralization for i = j tables, treating variables as nominal, with keyword\nfisher. the or keyword gives the odds ratio and its large-sample confi-\ndence interval 3.2 and the small-sample interval based on 3.20 . other\nexact statement keywords include binomial tests for 1 = 2 tables keyword\nbinomial , exact trend tests for i = 2 tables trend , and exact chi-\n.\nsquared tests chisq and exact correlation tests for i = j tables mhchi .\none can use monte carlo simulation option mc to estimate exact p-values\nwhen the exact calculation is too time consuming. table a.2 also uses proc\nlogistic to get a profile-likelihood confidence interval\nthe\nodds ratio clodds s pl . logistic uses freq to serve the same\npurpose as proc freq uses weight.\n\nfor\n\n.\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nother\nstatxact provides small-sample confidence intervals for a binomial parame-\n.\nter, the difference of proportions, relative risk, and odds ratio. blaker 2000\ngave s-plus functions that provide his confidence interval for a binomial\nparameter.\n\n\u017e\n\n\u017e\n\nchapter 4: models for binary response variables\nproc genmod fits glms. it specifies the response distribution in the\ndist option \u2018\u2018poi\u2019\u2019 for poisson, \u2018\u2018bin\u2019\u2019 for binomial, \u2018\u2018mult\u2019\u2019 for multinomial,\n\u2018\u2018negbin\u2019\u2019 for negative binomial and specifies the link in the link option.\ntable a.3 illustrates for table 4.2. for binomial models with grouped data,\nthe response in the model statements takes the form of the number of\n\u2018\u2018successes\u2019\u2019 divided by the number of cases.\n\n.\n\n "}, {"Page_number": 651, "text": "636\n\nusing computer software to analyze categorical data\n\ntable a.3 sas code for binary glms for snoring data in table 4.2\n\ndata glm;\ninput snoring disease total @@;\ndatalines;\n0 24 1379\n;\nproc genmod; model disease / total = snoring / dist = bin link = identity;\nproc genmod; model disease / total = snoring / dist = bin link = logit;\nproc genmod; model disease / total = snoring / dist = bin link = probit;\n\n5 30 254\n\n4 21 213\n\n2 35 638\n\ntable a.4 sas code for poisson and negative binomial glms for horseshoe\ncrab data in table 4.3\n\ndata crab;\ninput color spine width satell weight;\ndatalines;\n3 3 28.3 8 3.05\n4 3 22.5 0 1.55\n\u2b48\u2b48\u2b48\n3 2 24.5 0 2.00\n;\nproc genmod;\n\nmodel satell = width / dist = poi link = log;\n\nproc genmod;\n\nmodel satell = width / dist = poi link = identity;\n\nproc genmod;\n\nmodel satell = width / dist = negbin link = identity;\n\ntable a.4 uses genmod for count modeling of table 4.3. each observa-\ntion refers to a single crab. using width as the predictor, the first two models\nuse poisson regression. the third model uses the identity link assuming a\nnegative binomial distribution.\n\ntable a.5 uses genmod for the overdispersed data of table 4.5.\na class statement requests dummy variables for the groups. with no\nintercept in the model option noint for the identity link, the estimated\nparameters are the four group probabilities. the estimate state-\nment provides an estimate, confidence interval, and test for a contrast of\nmodel parameters, in this case the difference in probabilities for the first\nand second groups. the second analysis uses the pearson statistic to scale\nstandard errors to adjust for overdispersion. proc logistic can also\nprovide overdispersion modeling of binary responses; see table a.27 in the\nchapter 13 part of this appendix.\n\n\u017e\n\n.\n\nproc gam starting in version 8.2 fits generalized additive models.\n\n.\n\n\u017e\n\n "}, {"Page_number": 652, "text": "examples of sas code by chapter\n\n637\n\ntable a.5 sas code for overdispersion modeling of teratology data in table 4.5\n\ndata moore;\n\ninput litter group n y @@;\n\n2 1 11 4\n\ndatalines;\n1 1 10 1\n\u2b48\u2b48\u2b48\n55 4 14 1\n;\nproc genmod; class group;\n\n56 4 8 0\n\n58 4 17 0\n\n3 1 12 9\n\n4 1 4 4\n\n5 1 10 10\n\nmodel y/n = group / dist = bin link = identity noint;\n\nestimate \u2018pi1- pi2 \u2019 group 1 -1 0 0;\nproc genmod; class group;\n\nmodel y/n = group / dist = bin link = identity noint scale = pearson;\n\nchapters 5 and 6: logistic regression\none can fit logistic regression models using either software for glms or\nspecialized software for logistic regression. proc genmod uses newton-\nraphson, whereas proc logistic uses fisher scoring. both yield ml\nestimates, but se values use observed information in genmod and ex-\npected information in logistic. these are the same for the logit link.\n\ntable a.6 applies genmod and logistic to table 5.2, when \u2018\u2018y\u2019\u2019 out\nof \u2018\u2018n\u2019\u2019 crabs had satellites at a given width level. in genmod, the lrci\noption provides profile likelihood confidence intervals. the alpha s option\ncan specify an error probability other than the default of 0.05. the type3\noption provides likelihood-ratio tests for each parameter.\nin the chapter\n8\u13909 section we discuss the second genmod analysis.\n\n\u017e\n\n.\n\ntable a.6 sas code for modeling grouped crab data in table 5.2\n\ndata crab;\ninput width y n satell; logcases = log(n);\ndatalines;\n22.69 5 14 14\n\u2b48\u2b48\u2b48\n30.41 14 14 72\n;\nproc genmod;\n\nmodel y/n = width / dist = bin link = logit 1rci alpha = .01 type3;\n\nproc logistic;\n\nmodel y/n = width / influence stb;\noutput out = predict p = pi hat lower = lcl upper = ucl;\n\n\u1390\n\nproc print data = predict;\nproc genmod;\n\nmodel satell = width / dist = poi link = log offset = logcases residuals;\n\n "}, {"Page_number": 653, "text": "638\n\nusing computer software to analyze categorical data\n\ntable a.7 sas code for logit modeling of aids data in table 5.5\n\ndata aids;\ninput race $ azt $ y n @@;\ndatalines;\n\nwhite yes 14 107\n\nwhite no 32 113\n\nblack yes 11 63\n\nblack no 12 55\n\n;\nproc genmod; class race azt;\n\nmodel y/n = azt race / dist = bin type3 lrci residuals obstats;\n\nproc logistic; class race azt / param = reference;\n\nmodel y/n = azt race / aggregate scale = none clparm = both clodds = both;\noutput out = predict p = pi hat lower = lower upper = upper;\n\n\u1390\n\nproc print data = predict;\nproc logistic; class race azt (ref = first) / param = ref;\n\nmodel y/n = azt / aggregate = (azt race) scale = none;\n\n.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\nx j\n\nwith proc logistic, logistic regression is the default for binary data.\nlogistic has a built-in check of whether logistic regression ml estimates\nexist. it can detect a complete separation of data points with 0 and 1\noutcomes. logistic can also apply other links, such as the probit. its\ninfluence option provides pearson and deviance residuals and diagnostic\nmeasures pregibon 1981 . the stb option provides standardized estimates\n'\n3r\u2432 section 5.4.7 and note 5.9 . following the model\nby multiplying by s\nstatement, table a.6 requests predicted probabilities and lower and upper\n95% confidence limits for the probabilities.\n\ntable a.7 uses genmod and logistic to fit a logit model with\nqualitative predictors to table 5.5. in genmod, the obstats option\nprovides various \u2018\u2018observation statistics,\u2019\u2019 including predicted values and their\nconfidence limits. the residuals option requests residuals such as the\npearson and standardized pearson residuals\nlabeled \u2018\u2018reschi\u2019\u2019 and\n\u2018\u2018streschi\u2019\u2019 . a class statement requests dummy variables for the factor. by\ndefault, in genmod the parameter estimate for the last level of each factor\nequals 0. in logistic, estimates sum to zero. that is, dummies take the\neffect coding 1, y1 of 1 when in the category and y1 when not, for which\nparameters sum to 0. in the class statement in logistic, the option\nparam s ref requests 1, 0 dummy variables with the last category as the\nreference level. also putting ref s first next to a variable name requests\nits first category as the reference level. the clparm s both and\nclodds s both options provide wald and profile likelihood confidence\nintervals for parameters and odds ratio effects of explanatory variables. with\naggregate scale s none in the model statement, logistic re-\nports pearson and deviance tests of fit; it forms groups by aggregating data\ninto the possible combinations of explanatory variable values, without\noverdispersion adjustments. adding variables in parentheses after aggre-\ngate as in the second use of logistic in table a.7 specifies the\npredictors used for forming the table on which to test fit, even when some\npredictors may have no effect in the model.\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n "}, {"Page_number": 654, "text": "examples of sas code by chapter\n\n639\n\ntable a.8 sas code for logistic regression models with horseshoe\ncrab data in table 4.3\n\ndata crab;\ninput color spine width satell weight;\nif satell>0 then y = 1; if satell = 0 then y = 0;\nif color = 4 then light = 0; if color<4 then light = 1;\ndatalines;\n2 3 28.3 8 3.05\n\u2b48\u2b48\u2b48\n2 2 24.5 0 2.00\n;\nproc genmod descending; class color;\n\nmodel y = width color / dist = bin link = logit lrci type3 obstats;\ncontrast \u2019a- d\u2019 color 1 0 0 -1;\n\nproc genmod descending;\n\nmodel y = width color / dist = bin link = logit;\n\nproc genmod descending;\n\nmodel y = width light / dist = bin link = logit;\n\nproc genmod descending; class color spine;\n\nmodel y = width weight color spine / dist = bin link = logit type3;\n\nproc logistic descending; class color spine / param = ref;\n\nmodel y = width weight color spine / selection = backward lackfit\n\noutroc = classif1;\n\nproc plot data = classif1; plot sensit * lmspec ;\n\n\u1390 \u1390\n\n\u1390\n\n\u1390\n\nw\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.x\n\ntable a.8 shows logistic regression analyses for table 4.3. the models\nrefer to a constructed binary variable y that equals 1 when a horseshoe crab\nhas satellites and 0 otherwise. with binary data entry, genmod and\n.\nlogistic order the levels alphanumerically, forming the logit with 1, 0\nresponses as log p y s 0 rp y s 1 . invoking the procedure with de-\nscending following the proc name reverses the order. the first two\ngenmod statements use both color and width as predictors; color is\nqualitative in the first model by the class statement and quantitative in\nthe second. a contrast statement tests contrasts of parameters, such as\nwhether parameters for two levels of a factor are identical. the statement\nshown contrasts the first and fourth color levels. the third genmod\nstatement uses a dummy variable for color, indicating whether a crab is light\nor dark color s 4 . the fourth genmod statement fits the main effects\nmodel using all the predictors from table 4.3. logistic has options for\nstepwise selection of variables, as the final model statement shows. the\nlackfit option yields the hosmer\u1390lemeshow statistic. using the out-\nroc option, logistic can output a data set for plotting a roc curve.\n\ntable a.9 analyzes table 6.9. the cmh option in proc freq specifies\nthe cmh statistic, the mantel\u1390haenszel estimate of a common odds ratio\nand its confidence interval, and the breslow\u1390day statistic. freq uses the\n\n.\n\n\u017e\n\n.\n\n "}, {"Page_number": 655, "text": "640\n\nusing computer software to analyze categorical data\n\ntable a.9 sas code for cmh analysis of clinical trial data in table 6.9\n\ndata crab;\ninput center $ treat response count @@ ;\ndatalines;\na 1 1 11\n\u2b48\u2b48\u2b48\nh 1 1 4\n;\nproc freq; weight count;\n\na 1 2 25\n\na 2 1 10\n\nh 2 1 6\n\nh 1 2 2\n\na 2 2 27\n\nh 2 2 1\n\ntables center*treat*response/ cmh chisq;\n\ntwo rightmost variables in the tables statement as the rows and columns\nfor each partial table; the chisq option yields chi-square tests of indepen-\ndence for each partial table. for i = 2 tables the trend keyword in the\ntables statement provides the cochran\u1390armitage trend test.\n\nexact conditional logistic regression is available in proc logistic with\nthe exact statement. it provides ordinary and mid-p-values as well as\nconfidence limits for each model parameter and the corresponding odds ratio\nwith the estimate s both option. one can also conduct the exact\nconditional version of the cochran\u1390armitage test using the trend option\nin the exact statement with proc freq. version 9 of sas will include\nasymptotic conditional\nlogistic regression, using a strata statement to\nindicate the stratification parameters to be conditioned out. one can also use\n.\nproc phreg to do this stokes et al. 2000 .\n\nmodels with probit and complementary log-log cloglog links are\navailable with proc genmod, proc logistic, or proc probit.\no\u2019brien 1986 gave a sas macro for computing powers using the noncentral\nchi-squared distribution.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nother\nlogxact provides exact conditional logistic regression and statxact provides\nexact inference about the odds ratio in 2 = 2 = k tables. pass ncss\nstatistical software provides power analyses.\n\n.\n\n\u017e\n\nchapter 7: multinomial response models\n.\nproc logistic fits baseline-category logit models as of version 8.2\nusing the link s glogit option. the final response category is the\ndefault baseline for the logits. exact inference is also available using the\nconditional distribution to eliminate nuisance parameters. proc catmod\nalso fits baseline-category logit models, as table a.10 shows. catmod\ncodes estimates for a factor so that they sum to zero. the pred s prob\nand pred s freq options provide predicted probabilities and fitted val-\nues and their standard errors. the population statement provides the\n\n\u017e\n\n "}, {"Page_number": 656, "text": "examples of sas code by chapter\n\n641\n\ntable a.10 sas code for baseline-category logit models with alligator data\nin table 7.1\n\ndata gator;\ninput lake gender size food count @@;\ndatalines;\n1 1 1 1 7 1 1 1 2 1 1 1 1 3 0 1 1 1 4 0 1 1 1 5 5\n\u2b48\u2b48\u2b48\n4 2 2 1 8 4 2 2 2 1 4 2 2 3 0 4 2 2 4 0 4 2 2 5 1\n;\nproc logistic; freq count; class lake size / param = ref;\n\nmodel food(ref = \u20191 \u2019) = lake size / link = glogit\n\naggregate scale = none;\n\nproc catmod; weight count;\n\npopulation lake size gender;\nmodel food = lake size / pred = freq pred = prob;\n\nvariables that define the predictor settings. for instance, with \u2018\u2018gender\u2019\u2019 in\nthat statement, the model with lake and size effects is fitted to the full table\nalso classified by gender.\nproc genmod can fit the proportional odds version of cumulative\nlogit models using the dist s multinomial and link s clogit\noptions. table a.11 fits it to table 7.5. when the number of response\ncategories exceeds 2, by default proc logistic fits this model. it also\ngives a score test of the proportional odds assumption of identical effect\nparameters for each cutpoint. both procedures use the \u2423 q \u2424x form of the\n.\nmodel. cox 1995 used proc nlin for the more general model 7.8 having\na scale parameter.\nboth genmod and logistic can use other links in cumulative link\nmodels. genmod uses link s cprobit for the cumulative probit model\nand link s ccll for the cumulative complementary log-log model. table\na.11 uses link s probit in logistic to fit a cumulative probit model.\n\n\u017e\n\n\u017e\n\n.\n\nj\n\ntable a.11 sas code for cumulative logit and probit models with mental\nimpairment data in table 7.5\n\ndata impair;\ninput mental ses life;\ndatalines;\n1 1 1\n\u2b48\u2b48\u2b48\n4 0 9\n;\nproc genmod ;\n\nmodel mental = life ses / dist = multinomial link = clogit lrci type3;\n\nproc logistic;\n\nmodel mental = life ses / link = probit;\n\n "}, {"Page_number": 657, "text": "642\n\nusing computer software to analyze categorical data\n\ntable a.12 sas code for adjacent-categories logit and mean response models\nand cmh analysis of job satisfaction data in table 7.8\n\ndata jobsat;\ninput gender income satisf count @@;\ncount2 = count + .01;\ndatalines;\n1 1 1 1 1 1 2 3 1 1 3 11 1 1 4 2\n...\n0 4 1 0 0 4 2 1 0 4 3 9 0 4 4 6\n;\nproc catmod order = data; * ml analysis of adj- cat logit (acl) model;\n\nweight count;\npopulation gender income;\nmodel satisf =\n\n(1 0 0 3 3, 0 1 0 2 2, 0 0 1 1 1,\n1 0 0 6 3, 0 1 0 4 2, 0 0 1 2 1,\n1 0 0 9 3, 0 1 0 6 2, 0 0 1 3 1,\n1 0 0 12 3, 0 1 0 8 2, 0 0 1 4 1,\n1 0 0 3 0, 0 1 0 2 0, 0 0 1 1 0,\n1 0 0 6 0, 0 1 0 4 0, 0 0 1 2 0,\n1 0 0 9 0, 0 1 0 6 0, 0 0 1 3 0,\n1 0 0 12 0, 0 1 0 8 0, 0 0 1 4 0)\n\n/ml pred = freq;\n\nproc catmod order = data; weight count2; * wls analysis of acl model;\nresponse alogits; population gender income; direct gender income;\nmodel satisf = response gender income;\n\n\u1390\n\n\u1390\n\nproc catmod; weight count; * mean response model;\n\npopulation gender income; response mean; direct gender income;\nmodel satisf = gender income / covb;\n\nproc freq; weight count;\n\ntables gender*income*satisf/ cmh scores = table;\n\n.\n\nincome, and gender\n\none can fit adjacent-categories logit models in catmod by fitting\nequivalent baseline-category logit models. table a.12 uses it for table 7.8,\nwhere each line of code in the model statement specifies the predictor values\n\u017e\nfor the three logits. the\nfor the three intercepts,\nincome and gender predictor values are multiplied by 3 for the first logit, 2\nfor the second, and 1 for the third, to make effects comparable in the two\nmodels. proc catmod has options clogits and alogits for fitting\ncumulative logit and adjacent-categories logit models to ordinal responses;\nhowever, those options provide weighted least squares wls rather than\nml fits. a constant must be added to empty cells for wls to run. catmod\ntreats zero counts as structural zeros, so they must be replaced by small\nconstants when they are actually sampling zeros. the direct statements\nidentify predictors treated as quantitative. the second analysis in table a.12\nuses the alogits option. catmod can also fit mean response models\nusing wls, as the third analysis in table a.12 shows.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nwith the cmh option, proc freq provides the generalized cmh tests\nindependence. the statistic for the \u2018\u2018general association\u2019\u2019\n\nof conditional\n\n "}, {"Page_number": 658, "text": "examples of sas code by chapter\n\n643\n\nw\n\n\u017e\n\n.x\n\nw\n\nalternative treats x and y as nominal statistic 7.20 , the statistic for the\n\u2018\u2018row mean scores differ\u2019\u2019 alternative treats x as nominal and y as ordinal,\nand the statistic for the \u2018\u2018nonzero correlation\u2019\u2019 alternative treats x and y as\n.\n\u017e\nordinal statistic 7.21 . table a.12 analyzes table 7.8, using scores 1, 2, 3, 4\nfor each variable.\n\n.x\n\nproc mdc fits multinomial discrete choice models, with logit and probit\nlinks. one can also use proc phreg, which is designed for the cox\nproportional hazards model for survival analysis, because the partial likeli-\nhood for that analysis has the same form as the likelihood for the multino-\n.\nmial model allison 1999, chap. 7; chen and kuo 2001 .\n\n\u017e\n\n\u017e\n\nother\nlogxact provides exact conditional analyses for baseline-category logit mod-\nels. joseph lang\njblang@stat.uiowa.edu has an r function that can fit\nmean response models by ml.\n\n\u017e\n\n.\n\nchapters 8 and 9: loglinear models\ntable a.13 uses genmod to fit model ac, am, cm to table 8.3. table\na.14 uses genmod for table raking of table 8.15. table a.15 uses\ngenmod to fit the linear-by-linear association model 9.6 and the row\neffects model 9.8 to table 9.3 with column scores 1, 2, 4, 5 . the defined\n\n.\n.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\ntable a.13 sas code for fitting loglinear models to drug survey\ndata in table 8.3\n\ndata drugs;\ninput a c m count @@;\ndatalines;\n1 1 1 911\n2 1 1\n3\n;\nproc genmod; class a c m;\n\n1 1 2 538\n2 1 2 43\n\n1 2 1 44\n2 2 1 2\n\n1 2 2 456\n2 2 2 279\n\nmodel count = a c m a*m a*c c*m / dist = poi link = log lrci type3 obstats;\n\ntable a.14 sas code for raking table 8.15\n\n\u1390\n\ndata rake;\ninput school atti count @@;\nlog c = log(count); pseudo = 100 / 3;\ndata lines;\n1 1 209\n\u2b48\u2b48\u2b48\n;\nproc genmod; class school atti;\n\n1 3 237\n\n1 2 101\n\nmodel pseudo = school atti / dist = poi link = log offset = log c obstats;\n\n\u1390\n\n "}, {"Page_number": 659, "text": "644\n\nusing computer software to analyze categorical data\n\ntable a.15 sas code for fitting association models to gss data in table 9.3\n\ndata sex;\ninput premar birth u v count @@; assoc = u*v ;\ndatalines;\n1 1 1 1 38\n\u2b48\u2b48\u2b48\n;\nproc genmod; class premar birth;\n\n1 2 1 2 60\n\n1 3 1 4 68\n\n1 4 1 5 81\n\nmodel count = premar birth assoc / dist = poi link = log;\n\nproc genmod; class premar birth;\n\nmodel count = premar birth premar*v / dist = poi link = log;\n\nvariable \u2018\u2018assoc\u2019\u2019 represents the cross-product of row and column scores,\nwhich has \u2424 parameter as coefficient\nin model 9.6 . table a.6 uses\ngenmod to fit the poisson regression model with log link for the grouped\ndata of table 5.2. it models the total number of satellites at each width level\n\u017e\nvariable \u2018\u2018satell\u2019\u2019 , using the log of the number of cases as offset.\ncorrespondence analysis is available with proc corresp.\n\n.\n\n\u017e\n\n.\n\nother\nprof. joseph lang\njblang@stat.uiowa.edu has r and s-plus functions for\n\u017e\nml fitting of the generalized loglinear model 8.18 . becker 1990 gave a\nfortran program that fits the rc m model.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n.\n\n\u017e\n\n.\n\nchapter 10: models for matched pairs\ntable a.16 analyzes table 10.1. for square tables, the agree option in\nproc freq provides the mcnemar chi-squared statistic for binary matched\n.\npairs, the x test of fit of the symmetry model also called bowker\u2019s test ,\n\n\u017e\n\n2\n\ntable a.16 sas code for mcnemar\u2019s test and comparing proportions\nfor matched samples in table 10.1\n\ndata matched;\ninput first second count @@;\ndatalines;\n1 1 794\n;\nproc freq; weight count;\n\n1 2 150\n\n2 1 86\n\n2 2 570\n\ntables first*second/ agree; exact mcnem;\n\nproc catmod; weight count;\n\nresponse marginals;\nmodel first*second= (1 0 ,\n1 1 ;\n\n "}, {"Page_number": 660, "text": "examples of sas code by chapter\n\n645\n\ntable a.17 sas code for testing marginal homogeneity with migration\ndata in table 10.6\n\ndata migrate;\ninput then $ now $ count m11 m12 m13 m21 m22 m23 m31 m32 m33 m44 m1 m2 m3;\ndatalines;\n\nne\nne\nne\nne\nmw\nmw\nmw\nmw\ns\ns\ns\ns\nw\nw\nw\nw\n\n0\n1\n0\n\n1\n0\n0\n\n0\n0\n0\n0\n1\n0\n0\n\n0\n0\n0\n0\n0\n1\n0\n\ns\nw\nne\nmw\n\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n0\n0\nne 11607\n0\n0\n0\nmw\n100\n0\n0\n366\ns\n1\n0\n0\n124 -1 -1 -1\nw\n0\n0\n0\nne\n87\n0\n0\n0\nmw 13677\n0\n0\n515\n0\n1\n0\n0 -1 -1 -1\n302\n0\n0\n0\n172\n0\n0\n0\n0\n225\n0\n0\n0\ns 17819\n1\n0\n0 -1 -1 -1\nw\n270\n0\n0\n0\n0 -1\n0 -1\nne\n0\n0\n0\nmw\ns\n0 -1\n0 -1\n0 -1\nw 10192 0\n;\n\n0\n0\n0\n0\n0\n0\n0\n0\n63 -1\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0 -1\n0\n\n0\n0\n0\n0\n0\n0 -1\n0\n\n0\n0 -1\n0\n\n176\n286\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n1\n\n0\n\n0\n\n0\n\nproc genmod;\n\nmodel count = m11 m12 m13 m21 m22 m23 m31 m32 m33 m44 m1 m2 m3\n\n/ dist = poi link = identity;\n\nproc catmod;\n\nweight count;\nmodel then*now = response\n\n\u1390\n\nresponse marginals;\n/freq;\n\n\u1390\n\nrepeated time 2;\n\n\u017e\n\nand cohen\u2019s kappa and weighted kappa with se values. the mcnem\nkeyword in the exact statement provides a small-sample binomial version\nof mcnemar\u2019s test. proc catmod can provide the confidence interval for\nthe difference of proportions. the code forms a model for the marginal\nproportions in the first row and the first column, specifying a model matrix in\nthe model statement that has an intercept parameter the first column that\napplies to both proportions and a slope parameter that applies only to the\nsecond; hence the second parameter is the difference between the second\nand first marginal proportions.\n\nproc logistic can conduct conditional logistic regression.\ntable a.17 shows ways of testing marginal homogeneity for table 10.6.\nthe genmod code shows the lipsitz et al. 1990 approach, expressing the\ni expected frequencies in terms of parameters for the i y 1\n.2\n2\ncells in the\nfirst i y 1 rows and i y 1 columns, the cell in the last row and last column,\nand i y 1 marginal totals which are the same for rows and columns . here,\nm11 denotes expected frequency \u242e , m1 denotes \u242e s \u242e , and so on. this\nparameterization uses formulas such as \u242e s \u242e y \u242e y \u242e y \u242e for\nterms in the last column or last row. catmod provides the bhapkar test\n\u017e\n10.16 of marginal homogeneity, as shown.\n\n1q\n1q\n\nq1\n11\n\n11\n\n14\n\n12\n\n13\n\n\u017e\n\n\u017e\n\n.\n\n.\n\n.\n\n.\n\n\u017e\n\n "}, {"Page_number": 661, "text": "646\n\nusing computer software to analyze categorical data\n\ntable a.18 sas code showing square-table analysis of table 10.5\n\ndata sex;\ninput premar extramar symm qi count @@;\nunif = premar*extramar;\ndatalines;\n1 1 1 1 144\n2 1 2 5 33\n3 1 3 5 84\n4 1 4 5 126\n;\nproc genmod; class symm;\n\n1 3 3 5 0\n2 3 6 5 2\n3 3 8 3 6\n4 3 9 5 25\n\n1 2 2 5 2\n2 2 5 2 4\n3 2 6 5 14\n4 2 7 5 29\n\n1 4 4 5 0\n2 4 7 5 0\n3 4 9 5 1\n4 4 10 4 5\n\nmodel count = symm / dist = poi link = log; * symmetry;\n\nproc genmod; class extramar premar symm;\n\nmodel count = symm extramar premar / dist = poi link = log; *qs;\n\nproc genmod; class symm;\n\nmodel count = symm extramar premar / dist = poi link = log; * ordinal qs;\n\nproc genmod; class extramar premar qi;\n\nmodel count = extramar premar qi / dist = poi link = log; * quasi indep;\n\nproc genmod; class extramar premar;\n\nmodel count = extramar premar unif / dist = poi link = log;\n\ndata sex2;\ninput score below above @@; trials = below + above;\ndatalines;\n1 33 2\n;\nproc genmod data = sex2;\n\n1 14 2\n\n2 84 0\n\n2 29 0\n\n1 25 1\n\n3 126 0\n\nmodel above / trials = score / dist = bin link = logit noint;\nproc genmod data = sex2;\nmodel above / trials = /dist = bin link = logit noint;\n\nproc genmod data = sex2;\n\nmodel above / trials = /dist = bin link = logit;\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ntable a.18 shows various square-table analyses of table 10.5. the \u2018\u2018symm\u2019\u2019\nfactor indexes the pairs of cells that have the same association terms in the\nsymmetry and quasi-symmetry models. for instance, \u2018\u2018symm\u2019\u2019 takes the same\nvalue for cells 1, 2 and 2, 1 . including this term as a factor in a model\ninvokes a parameter \u242d satisfying \u242d s \u242d . the first model fits this factor\nalone, providing the symmetry model. the second model looks like the third\n\u017e\nexcept that it identifies \u2018\u2018premar\u2019\u2019 and \u2018\u2018extramar\u2019\u2019 as class variables\nfor\n\u017e\nquasi-symmetry , whereas the third model statement does not\nfor ordinal\nquasi-symmetry . the fourth model fits quasi-independence. the \u2018\u2018qi\u2019\u2019 factor\ninvokes the \u2426 parameters. it takes a separate level for each cell on the main\ndiagonal and a common value for all other cells. the fifth model fits the\n.\nquasi-uniform association model 10.29 .\n\n.\n.\n\n\u017e\n\ni j\n\ni j\n\nji\n\ni\n\nthe bottom of table a.18 fits square-table models as logit models. the\npairs of cell counts n , n , labeled as \u2018\u2018above\u2019\u2019 and \u2018\u2018below\u2019\u2019 with reference\nto the main diagonal, are six sets of binomial counts. the variable defined as\n\u2018\u2018score\u2019\u2019 is the distance u y u s j y i. the first two cases are symmetry\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ni j\n\nji\n\nj\n\ni\n\n "}, {"Page_number": 662, "text": "examples of sas code by chapter\n\n647\n\ntable a.19 sas code for fitting bradley\u2013terry model to table 10.10\n\ndata baseball;\ninput wins games milw detr toro newy bost clev balt;\ndatalines;\n7 13 1 -1 0 0 0 0 0\n\u2b48\u2b48\u2b48\n6 13 0 0 0 0 0 1 -1\n;\nproc genmod;\n\nmodel wins / games = milw detr toro newy bost clev balt /\ndist = bin link = logit noint covb;\n\n\u017e\n\n\u017e\n\n.\nand ordinal quasi-symmetry. neither model contains an intercept noint ,\nand the ordinal model uses \u2018\u2018score\u2019\u2019 as the predictor. the third model allows\n.\nan intercept and is the conditional symmetry model 10.28 .\n\ntable a.19 uses genmod for logit fitting of the bradley\u1390terry model to\ntable 10.10 by forming an artificial explanatory variable for each team. for a\ngiven observation, the variable for team i is 1 if it wins, y1 if it loses, and 0 if\nit is not one of the teams for that match. each observation lists the number\n\u017e\nof wins \u2018\u2018wins\u2019\u2019 for the team with variate-level equal to 1 out of the number\n\u2018\u2018games\u2019\u2019 against the team with variate-level equal to y1. the\n\u017e\nof games\nmodel has these artificial variates, one of which is redundant, as explanatory\nvariables with no intercept term. the covb option provides the estimated\ncovariance matrix of parameter estimators.\n\n.\n\n.\n\nchapter 11: analyzing repeated categorical response data\ntable a.20 uses genmod for the likelihood-ratio test of marginal homo-\ngeneity for table 11.1, where for instance m11p denotes \u242e . the marginal\nhomogeneity model expresses the eight cell expected frequencies in terms of\n\n11q\n\ntable a.20 sas code for testing marginal homogeneity with crossover\nstudy of table 11.1\n\ndata crossover;\ninput a b c count m111 m11p m1p1 mp11 m1pp m222 @@;\ndatalines;\n6\n1\n2\n1\n2\n2\n2\n6\n;\nproc genmod;\n\n0\n1 -1 -1\n1 -1\n0\n0\n\n0\n0\n0\n0\n0\n1\n0 -1 -1\n\n0\n0\n0 -1\n0\n0\n\n1\n-1\n-1\n1\n\n16 -1\n\n2\n2\n2\n2\n\n1\n2\n1\n2\n\n0\n0\n0\n0\n\n1\n1\n2\n2\n\n0\n0\n0\n1\n\n0\n0\n0\n1\n\n0\n1\n1\n0\n\n1\n1\n1\n1\n\n1\n2\n1\n2\n\n4\n4\n6\n\n0\n1\n0\n\n1\n\nmodel\n\ncount = m111\n\nm11p\n\nm1p1\n\nmp11\n\nm1pp\n\nm222 / dist = poi link = identity;\n\nproc catmod;\n\nweight count;\n\nresponse marginals;\n\nmodel\n\na*b*c = response\n\n\u1390\n\n\u1390\n\n/freq;\n\nrepeated drug 3;\n\n "}, {"Page_number": 663, "text": "648\n\nusing computer software to analyze categorical data\n\ntable a.21 sas code for marginal modeling of depression data in table 11.2\n\ndata depress;\ninput case diagnose drug time outcome @@;\ndatalines;\n0\n\n1\n\n0\n\n0\n\n1\n\n1\n\n0\n\n0\n\n1\n\n1\n\n1\n\n0\n\n0\n\n2\n\n1\n\n*\n\noutcome = 1 is normal;\n\n1\n\n\u2b48\u2b48\u2b48\n340\n;\nproc genmod descending;\n\n0 340\n\n1\n\n0\n\n1\n\n1\n\n1\n\n0 340\n\n1\n\n1\n\n2\n\n0\n\nclass case;\n\nmodel; outcome = diagnose drug time drug*time / dist = bin link = logit type3;\nrepeated subject = case / type = exch corrw;\n\nproc n1mixed\n\nqpoints = 200;\n\nparms alpha = -.03 beta1 = -1.3 beta2 = -.06 beta3 = .48 beta4 = 1.02 sigma = .066;\neta = alpha + beta1*diagnose + beta2*drug + beta3*time + beta4*drug*time + u;\np = exp(eta) / (1 + exp(eta));\nmodel outcome ; binary(p);\nrandom u ; normal(0, sigma*sigma)\n\nsubject = case;\n\ntable a.22 sas code for gee and random intercept cumulative logit analysis\nof insomnia data in table 11.4\n\ndata francom;\n\ninput case treat time outcome @@;\n\ndatalines;\n\n1 1 0 1\n\n1 1 1 1\n\n\u2b48\u2b48\u2b48\n239 0 0 4 239 0 1 4\n;\nproc genmod; class case;\n\nmodel outcome = treat time treat*time / dist = multinomial\n\nlink = clogit;\n\nrepeated subject = case / type = indep corrw;\n\nproc n1mixed qpoints = 40;\n\nbounds i2>0; bounds i3>0;\neta1 = i1 + treat*beta1 + time*beta2 + treat*time*beta3+ u;\neta2 = i1 + i2 + treat*beta1 + time*beta2 + treat*time*beta3+ u;\neta3 = i1 + i2 + i3 + treat*beta1 + time*beta2 + treat*time*beta3+ u;\np1 = exp(eta) / (1 + exp(eta1));\np2 = exp(eta2) / (1 + exp(eta2))- exp(eta1) / (1 + exp(eta1));\np3 = exp(eta3) / (1 + exp(eta3))- exp(eta2) / (1 + exp(eta2));\np4 = 1- exp(eta3) / (1 + exp(eta3));\n11 = y1*log(p1) + y2*log(p2) + y3*log(p3) + y4*log(p4);\nmodel y1 ; general(11);\nestimate \u2019interc2 \u2019 i1 + i2; * this is alpha 2 in model, and\n\n\u1390\n\ni1 is alpha 1;\u1390\n\nestimate \u2019interc3 \u2019 i1 + i2 + i3; * this is alpha 3 in model;\n\u1390\nrandom u ; normal(0, sigma*sigma) subject = case;\n\n "}, {"Page_number": 664, "text": "649\n\n.\n\n\u017e\n\n122\n\n111\n\n222\n\n111\n\n1q1\n\n11q\n\nq11\n112\n\n1qq\n11q\n\n1qq\n11q\n\nqq1\n1qq\n\nq1q\n111\n\u017e\n\nexamples of sas code by chapter\n\u242e , \u242e , \u242e , \u242e , \u242e , and \u242e since \u242e s \u242e s \u242e\n.\n. note,\nfor instance, that \u242e s \u242e y \u242e and \u242e s \u242e q \u242e y \u242e y \u242e .\n1q1\ncatmod provides the generalized bhapkar test 11.5 of marginal homo-\ngeneity.\ntable a.21 uses genmod to analyze table 11.2 using gee. possible\nworking correlation structures are type s exch for exchangeable, type\ns ar for autoregressive, type s indep for independence, and type s\nunstr for unstructured. output shows estimates and standard errors under\nthe naive working correlation and based on the sandwich matrix incorporat-\ning the empirical dependence. alternatively, the working association struc-\nture in the binary case can use the log odds ratio e.g., using logor s\nexch for exchangeability . the type 3 option in gee provides score tests\nabout effects. see stokes et al. 2000, sec. 15.11 for the use of gee with\nmissing data.\n\ntable a.22 uses genmod to implement gee for a cumulative logit\nmodel for table 11.4. for multinomial responses, independence is currently\nthe only working correlation structure.\n\n.\n\n\u017e\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nother\njoseph lang\njblang@stat.uiowa.edu has r and s-plus functions for ml\n.\nfitting of marginal models through the generalized loglinear model 11.8 ,\nusing the constraint approach with lagrange multipliers. the program\nmareg kastner et al. 1997 provides gee fitting and ml fitting of\nmarginal models with the fitzmaurice and laird 1993 approach, allowing\nmulticategory responses. see www.stat.uni-muenchen.der;andreasrmaregr\nwinmareg.html.\n\n.\n\n\u017e\n\n\u017e\n\n\u017e\n\n.\n\nchapter 12: random effects: generalized linear mixed models\nproc nlmixed extends glms to glmms by including random effects.\ntable a.23 analyzes the matched pairs model 12.3 . table a.24 analyzes the\nelection data in table 12.2.\n\n\u017e\n\n.\n\ntable a.23 sas code for fitting model 12.3 for matched pairs to table 12.1\n\n(\n\n)\n\ndata matched;\ninput case occasion response count @@;\ndatalines;\n2 0 1 794\n3 0 0 86\n;\nproc n1mixed;\n\n1 1 1 794\n3 1 1 86\n\n2 0 1 150\n4 0 0 570\n\n2 1 0 150\n4 1 0 570\n\neta = alpha + beta*occasion + u; p = exp(eta) / (1 + exp(eta));\nmodel response ; binary(p);\nrandom u ; normal(0, sigma*sigma) subject = case;\nreplicate count;\n\n "}, {"Page_number": 665, "text": "650\n\nusing computer software to analyze categorical data\n\ntable a.24 sas code for glmm analysis of election data in table 12.2\n\ndata vote;\ninput y n;\ncase = n ;\ndatalines;\n\n\u1390 \u1390\n\n1\n\n5\n16 32\n\u2b48\u2b48\u2b48\n1\n\n4\n\n;\nproc n1mixed;\n\neta = alpha + u; p = exp(eta) / (1 + exp(eta));\nmodel y ; binomial(n,p);\nrandom u ; normal (0, sigma*sigma) subject = case;\npredict p out = new;\nproc print data = new;\n\ntable a.25 sas code for glmm modeling of opinions in table 10.13\n\ndata new;\ninput sex poor single any count;\ndatalines;\n1 1 1 1 342\n\u2b48\u2b48\u2b48\n2 0 0 0 457\n;\ndata new; set new;\n\n\u1390 \u1390\n\nsex = sex- 1; case = n ;\nq1 = 1; q2 = 0; resp = poor; output;\nq1 = 0, q2 = 1; resp = single; output;\nq1 = 0; q2 = 0; resp = any; output;\n\ndrop poor single any;\nproc n1mixed qpoints = 50;\n\nparms alpha = 0 beta1 = .8 beta2 = .3 gamma = 0 sigma = 8.6;\neta = alpha + beta1*q1 + beta2*q2 + gamma*sex + u;\np = exp(eta) / (1 + exp(eta));\nmodel resp ; binary(p);\nrandom u ; normal(0, sigma*sigma) subject = case;\nreplicate count;\n\n "}, {"Page_number": 666, "text": "examples of sas code by chapter\n\n651\n\ntable a.26 sas code for glmm for leading crowd data in table 12.8\n\ndata crowd;\ninput mem1 att1 mem2 att2 count;\ndatalines;\n\n1 1 1 1 458\n\n\u2b48\u2b48\u2b48\n\n0 0 0 0 554\n\n;\ndata new; set crowd;\n\n\u1390 \u1390\n\ncase = n ;\nx1m = 1; x1a = 0; x2m = 0; x2a = 0; var = 1; resp = mem1; output;\nx1m = 0; x1a = 1; x2m = 0; x2a = 0; var = 0; resp = att1; output;\nx1m = 0; x1a = 0; x2m = 1; x2a = 0; var = 1; resp = mem2; output;\nx1m = 0; x1a = 0; x2m = 0; x2a = 1; var = 0; resp = att2; output;\ndrop mem1 att1 mem2 att2;\n\nproc n1mixed data = new;\n\neta = beta1m*x1m + beta1a*x1a + beta2m*x2m + beta2a*x2a + um*var +\n\nua*(1- var);\n\np = exp(eta) / (1 + exp(eta));\nmodel resp ; binary(p);\nrandom um ua ; normal([0,0],[s1*s1, cov12, s2*s2]) subject = case;\nreplicate count;\nestimate \u2019mem change\u2019 beta2m- beta1m; estimate \u2019att change\u2019\n\nbeta2a- beta1a;\n\n\u017e\n\n.\n\n\u017e\n\n.\n\ntable a.25 fits model 12.10 to table 10.13. this shows how to set initial\nvalues and set the number of quadrature points for gauss\u1390hermite quadra-\nture e.g., qpoints s . one could let sas fit without initial values but\nthen take that fit as initial values in further runs, increasing qpoints until\nestimates and standard errors converge to the necessary precision.\n\n\u017e\n\u017e\n\ntable a.21 uses nlmixed for table 11.2. table a.22 uses nlmixed\nfor ordinal modeling of table 11.4, defining a general multinomial\nlog\nlikelihood. table a.26 shows a correlated bivariate random effect analysis of\ntable 12.8. agresti et al. 2000 showed nlmixed examples for clustered\ndata, agresti and hartzel 2000 showed code for multicenter trials such as\ntable 12.5, and hartzel et al. 2001a showed code for multicenter trials with\nan ordinal response. the web site for the journal statistical modelling shows\nnlmixed code for an adjacent-categories logit model and a nominal model\nat the data archive for hartzel et al. 2001b . chen and kuo 2001 discussed\nfitting multinomial logit models, including discrete-choice models, with ran-\ndom effects.\n\n.\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n.\n\nother\nmln institute of education, london and hlm scientific software,\nchicago fit multilevel models. mixor is a fortran program for ml\n\n.\n\n\u017e\n\n.\n\n\u017e\n\n "}, {"Page_number": 667, "text": "652\n\nusing computer software to analyze categorical data\n\ntable a.27 sas code for overdispersion analysis of table 4.5\n\ndata moore;\ninput litter group n y @@;\n\nz2 = 0; z3 = 0; z4 = 0;\nif group = 2 then z2 = 1; if group = 3 then z3 = 1; if group = 4\n\nthen z4 = 1;\n\ndatalines;\n1 1 10 1\n\u2b48\u2b48\u2b48\n55 4 14 1\n;\nproc logistic;\n\n2 1 11 4\n\n3 1 12 9\n\n4 1 4 4\n\n56 4 8 0\n\n57 4 6 0\n\n58 4 17 0\n\nmodel y / n = z2 z3 z4 / scale = williams;\n\nproc logistic;\n\nmodel y / n = z2 z3 z4 / scale = pearson;\n\nproc n1mixed qpoints = 200;\n\neta = alpha + beta2*z2 + beta3*z3 + beta4*z4 + u;\np = exp(eta) / (1 + exp(eta));\nmodel y ; binomial(n,p);\nrandom u ; normal(0, sigma*sigma) subject = litter;\n\ntable a.28 sas code for fitting models to murder data in table 13.6\n\ndata new;\ninput white black other response;\ndatalines;\n1070 119 55\n5\n\n0\n1\n\n16\n\n60\n\n\u2b48\u2b48\u2b48\n\n1\n\n0\n\n0\n\n6\n\n;\ndata new; set new; count = white; race = 0; output;\n\ncount = black; race = 1; output; drop white black other;\n\ndata new2; set new; do i = 1 to count; output; end; drop i;\nproc genmod data = new2;\n\nmodel response = race / dist = negbin link = log;\n\nproc genmod data = new2;\n\nmodel response = race / dist = poi link = log scale = pearson;\n\ndata new; set new; case = n ;\nproc n1mixed data = new qpoints = 400;\n\n\u1390 \u1390\n\nparms alpha = -3.7 beta = 1.90 sigma = 1.6;\neta = alpha + beta*race + u; mu = exp(eta);\nmodel response ; poisson(mu);\nrandom u ; normal(0, sigma*sigma) subject = case;\nreplicate count;\n\n "}, {"Page_number": 668, "text": "examples of sas code by chapter\n\n653\n\nfitting of binary and ordinal random effects models available from don\nhedeker www.uic.edur;hedekerrmix.html .\n.\n\n\u017e\n\n\u017e\n\n.\n\nchapter 13: other mixture models for categorical data\nproc logistic provides two overdispersion approaches for binary data.\nthe scale s williams option uses variance function of the beta-bi-\nnomial form 13.10 , and scale s pearson uses the scaled binomial\nvariance 13.11 . table a.27 illustrates for table 4.5. that table also uses\nnlmixed for adding litter random intercepts.\n\nfor table 13.6, table a.28 uses genmod to fit a negative binomial\nmodel and a quasi-likelihood model with scaled poisson variance using the\npearson statistic, and nlmixed to fit a poisson glmm. proc nlmixed\ncan also fit negative binomial models.\n\n\u017e\n\n.\n\n\u017e\n\nother\nlatent gold developed by j. vermunt and j. magidson for statistical\ninnovations, belmont, massachusetts\ncan fit a wide variety of mixture\nincluding latent class models, nonparametric mixtures of logistic\nmodels,\nregression, and some rasch mixture models.\n\n.\n\n "}, {"Page_number": 669, "text": "a p p e n d i x b\n\nchi-squared distribution values\n\ndf\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n25\n30\n40\n50\n60\n70\n80\n90\n100\n\n0.250\n1.32\n2.77\n4.11\n5.39\n6.63\n7.84\n9.04\n10.22\n11.39\n12.55\n13.70\n14.85\n15.98\n17.12\n18.25\n19.37\n20.49\n21.60\n22.72\n23.83\n29.34\n34.80\n45.62\n56.33\n66.98\n77.58\n88.13\n98.65\n109.1\n\nright-tailed probability\n\n0.100\n2.71\n4.61\n6.25\n7.78\n9.24\n10.64\n12.02\n13.36\n14.68\n15.99\n17.28\n18.55\n19.81\n21.06\n22.31\n23.54\n24.77\n25.99\n27.20\n28.41\n34.38\n40.26\n51.80\n63.17\n74.40\n85.53\n96.58\n107.6\n118.5\n\n0.050\n3.84\n5.99\n7.81\n9.49\n11.07\n12.59\n14.07\n15.51\n16.92\n18.31\n19.68\n21.03\n22.36\n23.68\n25.00\n26.30\n27.59\n28.87\n30.14\n31.41\n37.65\n43.77\n55.76\n67.50\n79.08\n90.53\n101.8\n113.1\n124.3\n\n0.025\n5.02\n7.38\n9.35\n11.14\n12.83\n14.45\n16.01\n17.53\n19.02\n20.48\n21.92\n23.34\n24.74\n26.12\n27.49\n28.85\n30.19\n31.53\n32.85\n34.17\n40.65\n46.98\n59.34\n71.42\n83.30\n95.02\n106.6\n118.1\n129.6\n\n0.010\n6.63\n9.21\n11.34\n13.28\n15.09\n16.81\n18.48\n20.09\n21.67\n23.21\n24.72\n26.22\n27.69\n29.14\n30.58\n32.00\n33.41\n34.81\n36.19\n37.57\n44.31\n50.89\n63.69\n76.15\n88.38\n100.4\n112.3\n124.1\n135.8\n\n0.005\n7.88\n10.60\n12.84\n14.86\n16.75\n18.55\n20.28\n21.96\n23.59\n25.19\n26.76\n28.30\n29.82\n31.32\n32.80\n34.27\n35.72\n37.16\n38.58\n40.00\n46.93\n53.67\n66.77\n79.49\n91.95\n104.2\n116.3\n128.3\n140.2\n\n0.001\n10.83\n13.82\n16.27\n18.47\n20.52\n22.46\n24.32\n26.12\n27.88\n29.59\n31.26\n32.91\n34.53\n36.12\n37.70\n39.25\n40.79\n42.31\n43.82\n45.32\n52.62\n59.70\n73.40\n86.66\n99.61\n112.3\n124.8\n137.2\n149.5\n\nsource: calculated using statable, cytel software, cambridge, ma.\n\n654\n\n "}, {"Page_number": 670, "text": "references\n\nadelbasit, k. m., and r. l. plackett. 1983. experimental design for binary data. j. amer. statist.\n\nassoc. 78: 90\u139098.\n\nagresti, a. 1984. analysis of ordinal categorical data. new york: wiley.\nagresti, a. 1992. a survey of exact inference for contingency tables. statist. sci. 7: 131\u1390153.\nagresti, a. 1993. computing conditional maximum likelihood estimates for generalized rasch\nmodels using simple loglinear models with diagonal parameters. scand. j. statist. 20:\n63\u139071.\n\nagresti, a. 1997. a model for repeated measurements of a multivariate binary response.\n\nj. amer. statist. assoc. 92: 315\u1390321.\n\nagresti, a. 1999. on logit confidence intervals for the odds ratio with small samples. biometrics\n\n55: 597\u1390602.\n\nagresti, a. 2001. exact inference for categorical data: recent advances and continuing contro-\n\nversies. statist. medic. 20: 2709\u13902722.\n\nagresti, a., and b. caffo. 2000. simple and effective confidence intervals for proportions and\ndifference of proportions result from adding two successes and two failures. amer. statist.\n54: 280\u1390288.\n\nagresti, a., and b. a. coull. 1998. approximate is better than exact for interval estimation of\n\nbinomial parameters. amer. statist. 52: 119\u1390126.\n\nagresti, a., and j. hartzel. 2000. strategies for comparing treatments on a binary response with\n\nmulti-centre data. statist. medic. 19 8 : 1115\u13901139.\n\n\u017e .\n\nagresti, a., and j. lang. 1993a. a proportional odds model with subject-specific effects for\n\nrepeated ordered categorical responses. biometrika 80: 527\u1390534.\n\nagresti, a., and j. lang. 1993b. quasi-symmetric latent class models, with application to rater\n\nagreement. biometrics 49: 131\u1390139.\n\nagresti, a., and i. liu. 1999. modeling a categorical variable allowing arbitrarily many category\n\nchoices. biometrics 55: 936\u1390943.\n\nagresti, a., and y. min. 2001. on small-sample confidence intervals for parameters in discrete\n\ndistributions. biometrics 57: 963\u1390971.\n\nagresti, a., and r. natarajan. 2001. modeling clustered ordered categorical data: a survey.\n\ninternal. statist. re\u00ae. 69: 345\u1390371.\n\nagresti, a., d. wackerly, and j. boyett. 1979. exact conditional tests for cross-classifications:\n\napproximation of attained significance levels. psychometrika 44: 75\u139084.\n\nagresti, a., c. chuang, and a. kezouh. 1987. order-restricted score parameters in association\n\nmodels for contingency tables. j. amer. statist. assoc. 82: 619\u1390623.\n\n655\n\n "}, {"Page_number": 671, "text": "656\n\nreferences\n\nagresti, a., c. r. mehta, and n. r. patel. 1990. exact inference for contingency tables with\n\nordered categories. j. amer. statist. assoc. 85: 453\u1390458.\n\nagresti, a., j. booth, j. hobert, and b. caffo. 2000. random-effects modeling of categorical\n\nresponse data. sociol. methodol. 30: 27\u139081.\n\naitchison, j., and c. g. g. aitken. 1976. multivariate binary discrimination by the kernel\n\nmethod. biometrika 63: 413\u1390420.\n\naitchison, j., and c. h. cho. 1989. the multivariate poisson-log normal distribution. biometrika\n\n76: 643\u1390653.\n\naitchison, j., and s. m. shen. 1980. logistic-normal distributions: some properties and uses.\n\nbiometrika 67: 261\u1390272.\n\naitchison, j., and s. d. silvey. 1957. the generalization of probit analysis to the case of multiple\n\nresponses. biometrika 44: 131\u1390140.\n\naitchison, j., and s. d. silvey. 1958. maximum likelihood estimation of parameters subject to\n\nrestraints. ann. math. statist. 29: 813\u1390828.\n\naitkin, m. 1979. a simultaneous test procedure for contingency table models. appl. statist. 28:\n\n233\u1390242.\n\naitkin, m. 1980. a note on the selection of log-linear models. biometrics 36: 173\u1390178.\naitkin, m. 1999. a general maximum likelihood analysis of variance components in generalized\n\nlinear models. biometrics 55: 117\u1390128.\n\naitkin, m., and d. clayton. 1980. the fitting of exponential, weibull, and extreme value\n\ndistributions to complex censored survival data using glim. appl. statist. 29: 156\u1390163.\n\naitkin, m., and m. stasinopoulos. 1989. likelihood analysis of a binomial sample size problem.\npp. 399\u1390411 in contributions to probability and statistics: essays in honor of ingram olkin,\ned. l. j. gleser, m. d. perlman, s. j. press, and a. r. sampson. new york: springer-\nverlag.\n\naitkin, m., d. anderson, and j. hinde. 1981. statistical modelling of data on teaching styles.\n\nj. roy. statist. soc. ser. a 144: 419\u1390461.\n\naitkin, m., d. anderson, b. francis, and j. hinde. 1989. statistical modeling in glim. oxford:\n\nclarendon press.\n\nalbert, j. h. 1997. bayesian testing and estimation of association in a two-way contingency table.\n\nj. amer. statist. assoc. 92: 685\u1390693.\n\nalbert, a., and j. a. anderson. 1984. on the existence of maximum likelihood estimates in\n\nlogistic models. biometrika 71: 1\u139010.\n\nalbert, j. h., and s. chib. 1993. bayesian analysis of binary and polychotomous response data.\n\nj. amer. statist. assoc. 88: 669\u1390679.\n\nalbert, j. h., and a. k. gupta. 1982. mixtures of dirichlet distributions and estimation in\n\ncontingency tables. ann. statist. 10: 1261\u13901268.\n\nallison, p. d. 1999. logistic regression using the sas system. cary, nc: sas institute.\naltham, p. m. e. 1969. exact bayesian analysis of a 2 = 2 contingency table and fisher\u2019s \u2018\u2018exact\u2019\u2019\n\nsignificance test. j. roy. statist. soc. ser b 31: 261\u1390269.\n\naltham, p. m. e. 1970. the measurement of association of rows and columns for an r = s\n\ncontingency table. j. roy. statist. soc. ser b 32: 63\u139073.\n\naltham, p. m. e. 1971. the analysis of matched proportions. biometrika 58: 561\u1390576.\naltham, p. m. e. 1975. quasi-independent triangular contingency tables. biometrics 31: 233\u1390238.\naltham, p. m. e. 1978. two generalizations of the binomial distribution. appl. statist. 27:\n\n162\u1390167.\n\naltham, p. m. e. 1984. improving the precision of estimation by fitting a model. j. roy. statist.\n\nsoc. ser b 46: 118\u1390119.\n\namemiya, t. 1981. qualitative response models: a survey. j. econom. literature 19: 1483\u13901536.\n\n "}, {"Page_number": 672, "text": "references\n\n657\n\nandersen, e. b. 1970. asymptotic properties of conditional maximum-likelihood estimators.\n\nj. roy. statist. soc. ser b 32: 283\u1390301.\n\nandersen, e. b. 1980. discrete statistical models with social science applications. amsterdam:\n\nnorth-holland.\n\nandersen, e. b. 1995. polytomous rasch models and their estimation. pp. 272\u1390291\nin rasch models: foundations, recent de\u00aeelopments, and applications, eds. g. fischer and\ni. molenaar. new york: springer-verlag.\n\nanderson, j. a. 1972. separate sample logistic discrimination. biometrika 59: 19\u139035.\nanderson, j. a. 1975. quadratic logistic discrimination. biometrika 62: 149\u1390154.\nanderson, j. a. 1984. regression and ordered categorical variables. j. roy. statist. soc. ser b\n\n46: 1\u139030.\n\nanderson, d. a., and m. aitkin. 1985. variance component models with binary response:\n\ninterviewer variability. j. roy. statist. soc. ser b 47: 203\u1390210.\n\nanderson, c. j., and u. bockenholt. 2000. graphical regression models for polytomous vari-\n\n\u00a8\n\nables. psychometrika 65: 497\u1390509.\n\nanderson, t. w., and l. a. goodman. 1957. statistical inference about markov chains. ann.\n\nmath. statist. 28: 89\u1390110.\n\nanderson, j. a., and p. r. philips. 1981. regression, discrimination, and measurement models\n\nfor ordered categorical variables. appl. statist. 30: 22\u139031.\n\nanderson, c. j., and j. k. vermunt. 2000. log-multiplicative models as latent variable models\n\nfor nominal andror ordinal data. sociol. methodol. 30: 81\u1390121.\n\naranda-ordaz, f. j. 1981. on two families of transformations to additivity for binary response\n\ndata. biometrics 68: 357\u1390363.\n\naranda-ordaz, f. j. 1983. an extension of the proportional hazards model for grouped data.\n\nbiometrics 39: 109\u1390117.\n\narminger, g., c. c. clogg, and t. cheng. 2000. regression analysis of multivariate binary\nresponse variables using rasch-type models and finite mixture methods. sociol. methodol.\n30: 1\u139026.\n\narmitage, p. 1955. tests for linear trends in proportions and frequencies. biometrics 11:\n\n375\u1390386.\n\nashford, j. r., and r. d. sowden. 1970. multivariate probit analysis. biometrics 26: 535\u1390546.\nasmussen, s., and d. edwards. 1983. collapsibility and response variables in contingency tables.\n\nbiometrika 70: 567\u1390578.\n\nazzalini, a. 1994. logistic regression for autocorrelated data with application to repeated\n\nmeasures. biometrika 81: 767\u1390775.\n\nbaglivo, j., d. olivier, and m. pagano. 1992. methods for exact goodness-of-fit tests. j. amer.\n\nstatist. assoc. 87: 464\u1390469.\n\nbaker, s. g. 1992. a simple method for computing the observed information matrix when using\n\nthe em algorithm with categorical data. j. comput. graph. statist. 1: 63\u139076.\n\nbaker, s. g., and n. m. laird. 1988. regression analysis for categorical variables with outcome\n\nsubject to nonignorable nonresponse. j. amer. statist. assoc. 83: 62\u139069.\n\nbaker, r. j., m. r. b. clarke, and p. w. lane. 1985. zero entries in contingency tables. comput.\n\nstatist. data anal. 3: 33\u139045.\n\nbanerjee, c., m. capozzoli, l. mcsweeney, and d. sinha. 1999. beyond kappa: a review of\n\ninterrater agreement measures. canad. j. statist. 27: 3\u139023.\n\nbaptista, j., and m. c. pike. 1977. algorithm as115: exact two-sided confidence limits for the\n\nodds ratio in a 2 = 2 table. appl. statist. 26: 214\u1390220.\n\nbarnard, g. a. 1945. a new test for 2 = 2 tables. nature 156: 177.\nbarnard, g. a. 1947. significance tests for 2 = 2 tables. biometrika 34: 123\u1390138.\n\n "}, {"Page_number": 673, "text": "658\n\nreferences\n\nbarnard, g. a. 1949. statistical inference. j. roy. statist. soc. ser b 11: 115\u1390139.\nbarnard, g. a. 1979. in contradiction to j. berkson\u2019s dispraise: conditional tests can be more\n\nefficient. j. statist. plann. inference 3: 181\u1390188.\n\nbarndorff-nielsen, o. e., and b. jorgensen. 1991. some parametric models on the simplex.\n\n\u00a8\nj. multi\u00aeariate anal. 39: 106\u1390116.\n\nbartholomew, d. j. 1980. factor analysis for categorical data. j. roy. statist. soc. ser b 42:\n\n293\u1390321.\n\nbartholomew, d. j., and m. knott. 1999. latent variable models and factor analysis, 2nd ed.\n\nlondon: edward arnold.\n\nbartlett, m. s. 1935. contingency table interactions. j. roy. statist. soc. suppl. 2: 248\u1390252.\nbartlett, m. s. 1937. some examples of statistical methods of research in agriculture and applied\n\nbiology. j. roy. statist. soc. suppl. 4: 137\u1390183.\n\nbecker, m. 1989a. models for the analysis of association in multivariate contingency tables.\n\nj. amer. statist. assoc. 84: 1014\u13901019.\n\nbecker, m. 1989b. on the bivariate normal distribution and association models for ordinal\n\ncategorical data. statist. probab. lett. 8: 435\u1390440.\n\n\u017e\n\n.\n\nbecker, m. 1990. maximum likelihood estimation of the rc m association model. appl. statist.\n\n39: 152\u1390167.\n\nbecker, m., and a. agresti. 1992. log-linear modelling of pairwise interobserver agreement on a\n\ncategorical scale. statist. medic. 11: 101\u1390114.\n\nbecker, m., and c. c. clogg. 1989. analysis of sets of two-way contingency tables using\n\nassociation models. j. amer. statist. assoc. 84: 142\u1390151.\n\nbedrick, e. j. 1983. chi-squared tests for cross-classified tables of survey data. biometrika 70:\n\n591\u1390595.\n\nbedrick, e. j. 1987. a family of confidence intervals for the ratio of two binomial proportions.\n\nbiometrics 43: 993\u1390998.\n\nbegg, c. b., and r. gray. 1984. calculation of polytomous logistic regression parameters using\n\nindividualized regressions. biometrika 71: 11\u139018.\n\nbeitler, p. j., and j. r. landis. 1985. a mixed-effects model for categorical data. biometrics 41:\n\n991\u13901000.\n\nbenedetti, j. k., and m. b. brown. 1978. strategies for the selection of loglinear models.\n\nbiometrics 34: 680\u1390686.\n\nbenichou, j. 1998. attributable risk. pp. 216\u1390229 in encyclopedia of biostatistics. chichester,\n\nbenzecri, j.-p. 1973. l\u2019analyse des donnees, vol. 1, la taxonomie; vol. 2, l\u2019analyse des\n\n\u00b4\n\nuk: wiley.\n\u00b4\ncorrespondances. paris: dunod.\n\nberger, r., and d. d. boos. 1994. p-values maximized over a confidence set for the nuisance\n\nparameter. j. amer. statist. assoc. 89: 1012\u13901016.\n\nbergsma, w. p., and t. rudas. 2002. marginal models for categorical data. ann. statist. 30:\n\n140\u1390159.\n\nberkson, j. 1938. some difficulties of interpretation encountered in the application of the\n\nchi-square test. j. amer. statist. assoc. 33: 526\u1390536.\n\nberkson, j. 1944. application of the logistic function to bio-assay. j. amer. statist. assoc. 39:\n\n357\u1390365.\n\nberkson, j. 1951. why i prefer logits to probits. biometrics 7: 327\u1390339.\nberkson, j. 1953. a statistically precise and relatively simple method of estimating the bioassay\nwith quantal response, based on the logistic function. j. amer. statist. assoc. 48: 565\u1390599.\nberkson, j. 1955. maximum likelihood and minimum logit \u24392 estimation of the logistic function.\n\nj. amer. statist. assoc. 50: 130\u1390162.\n\n "}, {"Page_number": 674, "text": "references\n\n659\n\nberkson, j. 1978. in dispraise of the exact test. j. statist. plann. inference 2: 27\u139042.\nberkson, j. 1980. minimum chi-square, not maximum likelihood! ann. statist. 8: 457\u1390487.\nberry, g., and p. armitage. 1995. mid-p confidence intervals: a brief review. the statistician 44:\n\n417\u1390423.\n\nbhapkar, v. p. 1966. a note on the equivalence of two test criteria for hypotheses in categorical\n\ndata. j. amer. statist. assoc. 61: 228\u1390235.\n\nbhapkar, v. p. 1968. on the analysis of contingency tables with a quantitative response.\n\nbiometrics 24: 329\u1390338.\n\nbhapkar, v. p. 1973. on the comparison of proportions in matched samples. sankhya ser a 35:\n\n341\u1390356.\n\nbhapkar, v. p. 1989. conditioning on ancillary statistics and loss of information in the presence\n\nof nuisance parameters. j. statist. plann. inference. 21: 139\u1390160.\n\nbhapkar, v. p., and g. g. koch. 1968. on the hypothesis of \u2018\u2018no interaction\u2019\u2019 in multidimen-\n\nsional contingency tables. biometrics 24: 567\u1390594.\n\nbhapkar, v. p., and g. w. somes. 1977. distribution of q when testing equality of matched\n\nproportions. j. amer. statist. assoc. 72: 658\u1390661.\n\nbiggeri, a. 1998. negative binomial distribution. pp. 2962\u13902967 in encyclopedia of biostatistics.\n\nchichester, uk: wiley.\n\nbillingsley, p. 1961. statistical methods in markov chains. ann. math. statist. 32: 12\u139040.\nbirch, m. w. 1963. maximum likelihood in three-way contingency tables. j. roy. statist. soc.\n\nser. b 25: 220\u1390233.\n\nbirch, m. w. 1964a. a new proof of the pearson\u1390fisher theorem. ann. math. statist. 35:\n\n817\u1390824.\n\nbirch, m. w. 1964b. the detection of partial association i: the 2 = 2 case. j. roy. statist. soc.\n\nser. b 26: 313\u1390324.\n\nbirch, m. w. 1965. the detection of partial association ii: the general case. j. roy. statist. soc.\n\nser b 27: 111\u1390124.\n\nbishop, y. m. m. 1971. effects of collapsing multidimensional contingency tables. biometrics 27:\n\n545\u1390562.\n\nbishop, y. m. m., and f. mosteller. 1969. smoothed contingency table analysis. chap. iv-3 in\n\nthe national halothane study. washington, dc: u.s. government printing office.\n\nbishop, y. m. m., s. e. fienberg, and p. w. holland. 1975. discrete multi\u00aeariate analysis.\n\ncambridge, ma: mit press.\n\nblaker, h. 2000. confidence curves and improved exact confidence intervals for discrete\n\ndistributions. canad. j. statist. 28: 783\u1390798.\n\nbliss, c. i. 1934. the method of probits. science 79: 38\u139039.\nbliss, c. i. 1935. the calculation of the dosage\u1390mortality curve. ann. appl. biol. 22: 134\u1390167.\nblyth, c. r. 1972. on simpson\u2019s paradox and the sure-thing principle. j. amer. statist. assoc.\n\n67: 364\u1390366.\n\nblyth, c. r., and h. a. still. 1983 binomial confidence intervals. j. amer. statist. assoc. 78:\n\n108\u1390116.\n\nbock, r. d. 1970. estimating multinomial response relations. pp. 453\u1390479 in contributions to\nstatistics and probability, ed. r. c. bose. chapel hill, nc: university of north carolina\npress.\n\nbock, r. d., and m. aitkin. 1981. marginal maximum likelihood estimation of item parameters:\n\napplication of an em algorithm. psychometrika 46: 443\u1390459.\n\nbock, r. d., and l. v. jones. 1968. the measurement and prediction of judgement and choice.\n\nsan francisco: holden-day.\n\n "}, {"Page_number": 675, "text": "660\n\nreferences\n\n\u00a8\nbockenholt, u., and w. dillon. 1997. modelling within-subject dependencies in ordinal paired\n\ncomparison data. psychometrika 62: 411\u1390434.\n\nbonney, g. e. 1987. logistic regression for dependent binary observations. biometrics 43:\n\n951\u1390973.\n\nboos, d. d. 1992. on generalized score tests. amer. statist. 46: 327\u1390333.\nbooth, j., and r. butler. 1999. an importance sampling algorithm for exact conditional tests in\n\nlog-linear models. biometrika 86: 321\u1390332.\n\nbooth, j. g., and j. p. hobert. 1998. standard errors of prediction in generalized linear mixed\n\nmodels. j. amer. statist. assoc. 93: 262\u1390272.\n\nbooth, j. g., and j. p. hobert. 1999. maximizing generalized linear mixed model likelihoods with\n\nan automated monte carlo em algorithm. j. roy. statist. soc. ser. b 61: 265\u1390285.\n\nbowker, a. h. 1948. a test for symmetry in contingency tables. j. amer. statist. assoc. 43:\n\n572\u1390574.\n\nbox, j. f. 1978. r. a. fisher: the life of a scientist. new york: wiley\nbradley, r. a. 1976. science, statistics, and paired comparisons. biometrics 32: 213\u1390240.\nbradley, r. a., and m. e. terry. 1952. rank analysis of incomplete block designs i. the method\n\nof paired comparisons. biometrika 39: 324\u1390345.\n\nbreslow, n. 1976. regression analysis of the log odds ratio: a method for retrospective studies.\n\nbiometrics 32: 409\u1390416.\n\nbreslow, n. 1981. odds ratio estimators when the data are sparse. biometrika 68: 73\u139084.\nbreslow, n. 1982. covariance adjustment of relative-risk estimates in matched studies. biomet-\n\nrics 38: 661\u1390672.\n\nbreslow, n. 1984. extra-poisson variation in log-linear models. appl. statist. 33: 38\u139044.\nbreslow, n. 1996. statistics in epidemiology: the case\u1390control study. j. amer. statist. assoc. 91:\n\n14\u139028.\n\nbreslow, n., and d. g. clayton. 1993. approximate inference in generalized linear mixed\n\nmodels. j. amer. statist. assoc. 88: 9\u139025.\n\nbreslow, n., and n. e. day. 1980, 1987. statistical methods in cancer research, vol. i, the\nanalysis of case\u1390control studies; vol. ii. the design and analysis of cohort studies. lyon:\niarc.\n\nbreslow, n., and x. lin. 1995. bias correction in generalised linear mixed models with a single\n\ncomponent of dispersion. biometrika 82: 81\u139091.\n\nbreslow, n., and w. powers. 1978. are there two logistic regressions for retrospective studies?\n\nbiometrics 34: 100\u1390105.\n\nbreslow, n., n. day, k. halvorsen, r. prentice, and c. sabai. 1978. estimation of multiple\nrelative risk functions in matched case\u1390control studies. amer. j. epidemiol. 108: 299\u1390307.\nbrier, s. s. 1980. analysis of contingency tables under cluster sampling. biometrika 67: 591\u1390596.\nbrooks, s. p., b. j. t. morgan, m. s. ridout, and s. e. pack. 1997. finite mixture models for\n\nproportions. biometrics 53: 1097\u13901115.\n\nbross, i. d. j. 1958. how to use ridit analysis. biometrics 14: 18\u139038.\nbrown, m. b. 1976. screening effects in multidimensional contingency tables. appl. statist. 25:\n\n37\u139046.\n\nbrown, m. b., and j. k. benedetti. 1977. sampling behavior of tests for correlation in two-way\n\ncontingency tables. j. amer. statist. assoc. 72: 309\u1390315.\n\nbrown, p. j., and p. w. k. rundell. 1985. kernel estimates for categorical data. technometrics\n\n27: 293\u1390299.\n\nbrown, l. d., t. t. cai, and a. das gupta. 2001. interval estimation for a binomial proportion.\n\nstatist. sci. 16: 101\u1390133.\n\n "}, {"Page_number": 676, "text": "references\n\n661\n\nbrownstone, d., and k. f. train. 1999. forecasting new product penetration with flexible\n\nsubstitution patterns. j. econometrics 89: 109\u1390129.\n\nbull, s. b., and a. donner. 1987. the efficiency of multinomial logistic regression compared with\n\nmultiple group discriminant analysis. j. amer. statist. assoc. 82: 1118\u13901122.\n\nburnham, k. p., and d. r. anderson. 1998. model selection and inference: a practical informa-\n\ntion-theoretic approach. new york: springer-verlag.\n\nburnham, k. p. and w. s. overton. 1978. estimation of the size of a closed population when\n\ncapture probabilities vary among animals. biometrika 65: 625\u1390633.\n\nburridge, j. 1981. a note on maximum likelihood estimation for regression models using\n\ngrouped data. j. roy. statist. soc. ser. b 43: 41\u139045.\n\ncameron, a. c., and p. k. trivedi. 1998. regression analysis of count data. cambridge, u.k.:\n\ncambridge university press.\n\ncarey, v., s. l. zeger, and p. diggle. 1993. modelling multivariate binary data with alternating\n\nlogistic regressions. biometrika 80: 517\u1390526.\n\ncarroll, r. j., s. wang, and c. y. wang. 1995. prospective analysis of logistic case\u1390control pairs.\n\nj. amer. statist. assoc. 90: 157\u1390169.\n\ncasella, g., and r. berger. 2001. statistical inference, 2nd ed. pacific grove, ca: wadsworth.\ncatalano, p. j., and l. m. ryan. 1992. bivariate latent variable models for clustered discrete and\n\ncontinuous outcomes. j. amer. statist. assoc. 87: 651\u1390658.\n\ncaussinus, h. 1966. contribution a l\u2019analyse statistique des tableaux de correlation. ann. fac.\n\n\u00b4\n\n`\nsci. uni\u00ae. toulouse 29: 77\u1390182.\n\nchaloner, k., and k. larntz. 1989. optimal bayesian design applied to logistic regression\n\nexperiments. j. statist. plann. inference 21: 191\u1390208.\n\nchamberlain, g. 1980. analysis of covariance with qualitative data. re\u00ae. econ. stud. 47:\n\n225\u1390238.\n\nchambers, e. a., and d. r. cox. 1967. discrimination between alternative binary response\n\nmodels. biometrika 54: 573\u1390578.\n\nchambers, r. l., and d. g. steel. 2001. simple methods for ecological inference in 2 = 2 tables.\n\nj. roy. statist. soc. ser. a 164: 175\u1390192.\n\nchan, i. 1998. exact tests of equivalence and efficacy with non-zero lower bound for comparative\n\nstudies. statist. medic. 17: 1403\u13901413.\n\nchan, j. s. k., and a. y. c. kuk. 1997. maximum likelihood estimation for probit-linear mixed\n\nmodels with correlated random effects. biometrics 53: 86\u139097.\n\nchao, a., p. k. tsay, s.-h. lin, w.-y. shau, and d.-y. chao. 2001. the applications of\n\ncapture\u1390recapture models to epidemiological data. statist. medic. 20: 3123\u13903157.\n\nchapman, d. g., and r. c. meng. 1966. the power of chi-square tests for contingency tables.\n\nj. amer. statist. assoc. 61: 965\u1390975.\n\nchen, z. and l. kuo. 2001. a note on the estimation of the multinomial logit model with\n\nrandom effects. amer. statist. 55: 89\u139095.\n\nchristensen, r. 1997. log\u1390linear models and logistic regression. new york: springer-verlag.\nchuang, c., d. gheva, and c. odoroff. 1985. methods for diagnosing multiplicative-interaction\n\nmodels for two-way contingency tables. commun. statist. ser. a 14: 2057\u13902080.\n\nclogg, c. c. 1995. latent class models. pp. 311\u1390359 in handbook of statistical modeling for the\nsocial and beha\u00aeioral sciences, ed. g. arminger and c. c. clogg. new york: plenum press.\nclogg, c. c., and s. r. eliason. 1987. some common problems in log-linear analysis. sociol.\n\nmethods res. 15: 4\u139044.\n\nclogg, c. c., and l. a. goodman. 1984. latent structure analysis of a set of multidimensional\n\ncontingency tables. j. amer. statist. assoc. 79: 762\u1390771.\n\n "}, {"Page_number": 677, "text": "662\n\nreferences\n\nclogg, c. c., and e. s. shihadeh. 1994. statistical models for ordinal variables. thousand oaks,\n\nca: sage publications.\n\nclopper, c. j., and e. s. pearson. 1934. the use of confidence or fiducial limits illustrated in the\n\ncase of the binomial. biometrika 26: 404\u1390413.\n\ncochran, w. g. 1940. the analysis of variance when experimental errors follow the poisson or\n\nbinomial laws. ann. math. statist. 11: 335\u1390347.\n\ncochran, w. g. 1943. analysis of variance for percentages based on unequal numbers. j. amer.\n\nstatist. assoc. 38: 287\u1390301.\n\ncochran, w. g. 1950. the comparison of percentages in matched samples. biometrika 37:\n\n256\u1390266.\n\ncochran, w. g. 1952. the \u24392 test of goodness-of-fit. ann. math. statist. 23: 315\u1390345.\ncochran, w. g. 1954. some methods of strengthening the common \u24392 tests. biometrics 10:\n\n417\u1390451.\n\ncochran, w. g. 1955. a test of a linear function of the deviations between observed and\n\nexpected numbers. j. amer. statist. assoc. 50: 377\u1390397.\n\ncoe, p. r., and a. c. tamhane. 1993. small sample confidence intervals for the difference, ratio\n\nand odds ratio of two success probabilities. commun. statist. ser. b 22: 925\u1390938.\n\ncohen, j. 1960. a coefficient of agreement for nominal scales. educ. psychol. meas. 20: 37\u139046.\ncohen, j. 1968. weighted kappa: nominal scale agreement with provision for scaled disagree-\n\nment or partial credit. psychol. bull. 70: 213\u1390220.\n\ncohen, a., and h. b. sackrowitz. 1991. tests for independence in contingency tables with\n\nordered alternatives. j. multi\u00aeariate anal. 36: 56\u139067.\n\ncohen, a., and h. b. sackrowitz. 1992. an evaluation of some tests of trend in contingency\n\ntables. j. amer. statist. assoc. 87: 470\u1390475.\n\ncollett, d. 1991. modelling binary data. london: chapman & hall.\nconaway, m. r. 1989. analysis of repeated categorical measurements with conditional likelihood\n\nmethods. j. amer. statist. assoc. 84: 53\u139062.\n\ncook, r. d., and s. weisberg. 1999. applied regression including computing and graphics. new\n\nyork: wiley.\n\ncopas, j. b. 1973. randomization models for the matched and unmatched 2 = 2 tables.\n\nbiometrika 60: 467\u1390476.\n\ncopas, j. b. 1983. plotting p against x. appl. statist. 32: 25\u139031.\ncopas, j. b. 1988. binary regression models for contaminated data. j. roy. statist. soc. ser b 50:\n\n225\u1390265.\n\ncorcoran, c., l. ryan, p. senchaudhuri, c. mehta, n. patel, and g. molenberghs. 2001.\n\nan exact trend test for correlated binary data. biometrics 57: 941\u1390948.\n\ncormack, r. m. 1989. log-linear models for capture\u1390recapture. biometrics 45: 395\u1390413.\ncornfield, j. 1951. a method of estimating comparative rates from clinical data: applications to\n\ncancer of the lung, breast and cervix. j. natl. cancer inst. 11: 1269\u13901275.\n\ncornfield, j. 1956. a statistical problem arising from retrospective studies. in proc. 3rd berkeley\n\nsymposium on mathematics, statistics and probability, ed. j. neyman, 4: 135\u1390148.\n\ncornfield, j. 1962. joint dependence of risk of coronary heart disease on serum cholesterol and\nsystolic blood pressure: a discriminant function analysis. fed. proc. 21, suppl. 11: 58\u139061.\ncoull, b. a., and a. agresti. 1999. the use of mixed logit models to reflect heterogeneity in\n\ncapture\u1390recapture studies. biometrics 55: 294\u1390301.\n\ncoull, b. a., and a. agresti. 2000. random effects modeling of multiple binomial responses\n\nusing the multivariate binomial logit-normal distribution. biometrics 56: 73\u139080.\n\n "}, {"Page_number": 678, "text": "references\n\n663\n\ncox, c. 1984. an elementary introduction to maximum likelihood estimation for multinomial\n\nmodels: birch\u2019s theorem and the delta method. amer. statist. 38: 283\u1390287.\n\ncox, c. 1995. location-scale cumulative odds models for ordinal data: a generalized non-linear\n\nmodel approach. statist. medic. 14: 1191\u13901203.\n\ncox, c. 1996. nonlinear quasi-likelihood models: applications to continuous proportions.\n\ncomput. statist. data anal. 21: 449\u1390461.\n\ncox, d. r. 1958a. the regression analysis of binary sequences. j. roy. statist. soc. ser. b 20:\n\n215\u1390242.\n\ncox, d. r. 1958b. two further applications of a model for binary regression. biometrika 45:\n\n562\u1390565.\n\n.\ncox, d. r. 1970. the analysis of binary data 2nd ed. 1989, by d. r. cox and e. j. snell .\n\n\u017e\n\nlondon: chapman & hall.\n\ncox, d. r. 1972. the analysis of multivariate binary data. appl. statist. 21: 113\u1390120.\ncox, d. r. 1983. some remarks on overdispersion. biometrika 70: 269\u1390274.\ncox, d. r., and d. v. hinkley. 1974. theoretical statistics. london: chapman & hall.\ncramer, h. 1946. mathematical methods of statistics. princeton, nj: princeton university press.\ncressie, n., and t. r. c. read. 1984. multinomial goodness-of-fit tests. j. roy. statist. soc. ser.\n\n\u00b4\n\nb 46: 440\u1390464.\n\ncressie, n., and t. r. c. read. 1989. pearson x 2 and the loglikelihood ratio statistic g2:\n\na comparative review. internat. statist. re\u00ae. 57: 19\u139043.\n\ncroon, m., w. bergsma, and j. hagenaars. 2000. analyzing change in categorical variables by\n\ngeneralized log-linear models. sociol. methods res. 29: 195\u1390229.\n\ncrouchley, r. 1995. a random-effects model for ordered categorical data. j. amer. statist.\n\nassoc. 90: 489\u1390498.\n\ncrowder, m. j. 1978. beta-binomial anova for proportions. appl. statist. 27: 34\u139037.\nd\u2019agostino, r. b., jr. 1998. propensity score methods for bias reduction in the comparison of a\n\ntreatment to a non-randomized control group. statist. medic. 17: 2265\u13902281.\n\ndaniels, m. j., and c. gatsonis. 1999. hierarchical generalized linear models in the analysis of\n\nvariations in health care utilization. j. amer. statist. assoc. 94: 29\u139042.\n\ndardanoni, v., and a. forcina. 1998. a unified approach to likelihood inference on stochastic\n\norderings in a nonparametric context. j. amer. statist. assoc. 93: 1112\u13901123.\n\ndarroch, j. n. 1962. interactions in multi-factor contingency tables. j. roy. statist. soc. ser. b\n\n24: 251\u1390263.\n\ndarroch, j. n. 1981. the mantel\u1390haenszel test and tests of marginal symmetry; fixed-effects\n\nand mixed models for a categorical response. internat. statist. re\u00ae. 49: 285\u1390307.\n\ndarroch, j. n., and p. i. mccloud. 1986. category distinguishability and observer agreement.\n\naustral. j. statist. 28: 371\u1390388.\n\ndarroch, j. n., and d. ratcliff. 1972. generalized iterative scaling for log-linear models. ann.\n\nmath. statist. 43: 1470\u13901480.\n\ndarroch, j. n., s. l. lauritzen, and t. p. speed. 1980. markov fields and log-linear interaction\n\nmodels for contingency tables. ann. statist. 8: 522\u1390539.\n\ndarroch, j. n., s. e. fienberg, g. f. v. glonek, and b. w. junker. 1993. a three-sample\nmultiple-recapture approach to census population estimation with heterogeneous catcha-\nbility. j. amer. statist. assoc. 88: 1137\u13901148.\n\ndas gupta, s., and m. d. perlman. 1974. power of the noncentral f-test: effect of additional\n\nvariates on hotelling\u2019s t 2-test. j. amer. statist. assoc. 69: 174\u1390180.\n\ndavid, h. a. 1988. the method of paired comparisons, 2nd ed. oxford: oxford university press.\ndavis, l. j. 1986a. exact tests for 2 by 2 contingency tables. amer. statist. 40: 139\u1390141.\n\n "}, {"Page_number": 679, "text": "664\n\nreferences\n\ndavis, l. j. 1986b. relationship between strictly collapsible and perfect tables. statist. probab.\n\nlett. 4: 119\u1390122.\n\ndavis, l. j. 1989. intersection union tests for strictly collapsibility in three-dimensional contin-\n\ngency tables. ann. statist. 17: 1693\u13901708.\n\ndavison, a. c., and d. v. hinkley. 1997. bootstrap methods and their application. cambridge,\n\nu.k. cambridge university press.\n\ndawson, r. b., jr. 1954. a simplified expression for the variance of the \u24392-function on a\n\ncontingency table. biometrika 41: 280.\n\nday, n. e., and d. p. byar. 1979. testing hypotheses in case\u1390control studies: equivalence of\n\nmantel\u1390haenszel statistics and logit score tests. biometrics 35: 623\u1390630.\n\nde falguerolles, a., s. jmel, and j. whittaker. 1995. correspondence analysis and association\n\nmodels constrained by a conditional independence graph. psychometrika 60: 161\u1390180.\n\n.\n\n\u017e\n\ndeming, w. e. 1964. statistical adjustment of data reprint of 1943 wiley text . new york:\n\ndover.\n\ndeming, w. e., and f. f. stephan. 1940. on a least squares adjustment of a sampled frequency\n\ntable when the expected marginal totals are known. ann. math. statist. 11: 427\u1390444.\n\ndempster, a. p., n. m. laird, and d. b. rubin. 1977. maximum likelihood from incomplete data\n\nvia the em algorithm. j. roy. statist. soc. ser. b 39: 1\u139038.\n\ndey, d. k., s. k. ghosh, and b. k. mallick editors . 2000. generalized linear models:\n\n\u017e\n\n.\n\na bayesian perspecti\u00aee. new york: marcel dekker.\n\ndiaconis, p., and b. efron. 1985. testing for independence in a two-way table: new interpreta-\n\ntions of the chi-square statistic. ann. statist. 13: 845\u1390874.\n\ndiaconis, p., and b. sturmfels. 1998. algebraic algorithms for sampling from conditional\n\ndistributions. ann. statist. 26: 363\u1390397.\n\ndiggle, p. j., p. heagerty, k.-y. liang, and s. l. zeger. 2002. analysis of longitudinal data,\n\n2nd ed. oxford: clarendon press.\n\ndittrich, r., r. hatzinger, and w. katzenbeisser. 1998. modeling the effect of subject-specific\ncovariates in paired comparison studies with an application to university rankings. appl.\nstatist. 47: 511\u1390525.\n\ndobson, a. j. 2001. an introduction to generalized linear models, 2 nd ed. london: chapman &\n\nhall.\n\ndong, j. 1998. simpson\u2019s paradox. pp. 4108\u13904110 in encyclopedia of biostatistics, vol. 5.\n\nchichester, uk: wiley.\n\ndong, j., and j. s. simonoff. 1994. the construction and properties of boundary kernels for\n\nsmoothing sparse multinomials. j. computat. graph. statist. 3: 57\u139066.\n\ndong, j., and j. s. simonoff. 1995. a geometric combination estimator for d-dimensional ordinal\n\nsparse contingency tables. ann. statist. 23: 1143\u13901159.\n\ndonner, a., and w. w. hauck. 1986. the large-sample efficiency of the mantel\u1390haenszel\n\nestimator in the fixed-strata case. biometrics 42: 537\u1390545.\n\ndoolittle, m. h. 1888. association ratios. bull. philos. soc. washington 10: 83\u139087, 94\u139096.\ndrost, f. c., w. c. m. kallenberg, d. s. moore, and j. oosterhoff. 1989. power approximations\n\nto multinomial tests of fit. j. amer. statist. assoc. 84: 130\u1390141.\n\nducharme, g. r., and y. lepage. 1986. testing collapsibility in contingency tables. j. roy.\n\nstatist. soc. ser b 48: 197\u1390205.\n\ndupont, w. d. 1986. sensitivity of fisher\u2019s exact test to minor perturbations in 2 = 2 contin-\n\ngency tables. statist. medic. 5: 629\u1390635.\n\ndyke, g. v., and h. d. patterson. 1952. analysis of factorial arrangements when the data are\n\nproportions. biometrics 8: 1\u139012.\n\n "}, {"Page_number": 680, "text": "references\n\n665\n\nedwardes, m. d. deb. 1997. univariate random cut-points theory for the analysis of ordered\n\ncategorical data. j. amer. statist. assoc. 92: 1114\u13901123.\n\nedwards, a. w. f. 1963. the measure of association in a 2 = 2 table. j. roy. statist. soc. ser a\n\n126: 109\u1390114.\n\nedwards, d. 2000. introduction to graphical modelling, 2nd ed. new york: springer-verlag.\nedwards, d., and s. kreiner. 1983. the analysis of contingency tables by graphical models.\n\nbiometrika 70: 553\u1390565.\n\nefron, b. 1975. the efficiency of logistic regression compared to normal discriminant analysis.\n\nj. amer. statist. assoc. 70: 892\u1390898.\n\nefron, b. 1978. regression and anova with zero\u1390one data: measures of residual variation.\n\nj. amer. statist. soc. 73: 113\u1390121.\n\nefron, b., and d. v. hinkley. 1978. assessing the accuracy of the maximum likelihood estimator:\n\nobserved versus expected fisher information. biometrika 65: 457\u1390482.\n\nefron, b., and c. morris. 1975. data analysis using stein\u2019s estimator and its generalizations.\n\nj. amer. statist. assoc. 70: 311\u1390319.\n\nekholm, a., j. w. mcdonald, and p. w. f. smith. 2000. association models for a multivariate\n\nbinary response. biometrics 56: 712\u1390718.\n\nescoufier, y. 1982. l\u2019analyse des tableaux de contingence simples et multiples. in proc.\n.\ninternational meeting on the analysis of multidimensional contingency tables rome, 1981 ,\ned. r. coppi. metron 40: 53\u139077.\n\n\u017e\n\nespeland, m. a., and s. l. handelman. 1989. using latent class models to characterize and\n\nassess relative error in discrete measurements. biometrics 45: 587\u1390599.\n\nfahrmeir, l., and g. tutz. 2001. multi\u00aeariate statistical modelling based on generalized linear\n\nmodels, 2nd ed. new york: springer-verlag.\n\nfarewell, v. t. 1979. some results on the estimation of logistic models based on retrospective\n\ndata. biometrika 66: 27\u139032.\n\nfarewell, v. t. 1982. a note on regression analysis of ordinal data with variability of classifica-\n\ntion. biometrika 69: 533\u1390538.\n\nfay, r. 1985. a jackknifed chi-squared test for complex samples. j. amer. statist. assoc. 80:\n\n148\u1390157.\n\nfay, r. 1986. causal models for patterns of nonresponse. j. amer. statist. assoc. 81: 354\u1390365.\nferguson, t. s. 1967. mathematical statistics: a decision theoretic approach. new york:\n\nacademic press.\n\nfienberg, s. e. 1970a. an iterative procedure for estimation in contingency tables. ann. math.\n\nstatist. 41: 907\u1390917.\n\nfienberg, s. e. 1970b. quasi-independence and maximum likelihood estimation in incomplete\n\ncontingency tables. j. amer. statist. soc. 65: 1610\u13901616.\n\nfienberg, s. e. 1972. the analysis of incomplete multi-way contingency tables. biometrics 28:\n\n177\u1390202.\n\nfienberg, s. e. 1980. fisher\u2019s contributions to the analysis of categorical data. pp. 75\u139084 in\nr. a. fisher: an appreciation, ed. s. e. fienberg and d. v. hinkley. berlin: springer-\nverlag.\n\nfienberg, s. e. 1984. the contributions of william cochran to categorical data analysis.\npp. 103\u1390118 in w. g. cochran\u2019s impact on statistics, ed. p. s. r. s. rao and j. sedransk.\nnew york: wiley.\n\nfienberg, s. e., and p. w. holland. 1973. simultaneous estimation of multinomial cell probabili-\n\nties. j. amer. statist. assoc. 68: 683\u1390690.\n\nfienberg, s. e., and k. larntz. 1976. loglinear representation for paired and multiple compari-\n\nson models. biometrika 63: 245\u1390254.\n\n "}, {"Page_number": 681, "text": "666\n\nreferences\n\nfienberg, s. e., m. a. johnson, and b. j. junker. 1999. classical multilevel and bayesian\napproaches to population size estimation using multiple lists. j. roy. statist. soc. ser. a\n162: 383\u1390405.\n\nfinney, d. j. 1947. the estimation from individual records of the relationship between dose and\n\nquantal response. biometrika 34: 320\u1390334.\n\nfinney, d. j. 1971. probit analysis, 3rd ed. cambridge: cambridge university press.\nfirth, d. 1987. on the efficiency of quasi-likelihood estimation. biometrika 74: 233\u1390245.\nfirth, d. 1989. marginal homogeneity and the superposition of latin squares. biometrika 76:\n\n179\u1390182.\n\nfirth, d. 1991. generalized linear models. pp. 55\u139082 in statistical theory and modelling. in\nhonour of sir da\u00aeid cox, frs, d. v. hinkley, n. reid, and e. j. snell, eds. london:\nchapman & hall.\n\nfirth, d. 1993a. bias reduction of maximum likelihood estimates. biometrika 80: 27\u139038.\nfirth, d. 1993b. recent developments in quasi-likelihood methods. proc. isi 49th session,\n\npp. 341\u1390358.\n\nfirth, d., and j. kuha. 2000. on the index of dissimilarity for lack of fit in log linear models.\n\nunpublished manuscript.\n\nfischer, g. h., and i. w. molenaar. 1995. rasch models: foundations, recent de\u00aeelopments, and\n\napplications. new york: springer-verlag.\n\nfisher, r. a. 1922. on the interpretation of chi-square from contingency tables, and the\n\ncalculation of p. j. roy. statist. soc. 85: 87\u139094.\n\nfisher, r. a. 1924. the conditions under which chi-square measures the discrepancy between\n\nobservation and hypothesis. j. roy. statist. soc. 87: 442\u1390450.\n\nfisher, r. a. 1926. bayes\u2019 theorem and the fourfold table. eugenics re\u00ae. 18: 32\u139033.\nfisher, r. a. 1934, 1970. statistical methods for research workers originally published 1925,\n\n\u017e\n\n14th ed., 1970. edinburgh: oliver & boyd.\n\u017e\n\n.\n\n.\n\nfisher, r. a. 1935a. the design of experiments 8th ed., 1966 . edinburgh: oliver & boyd.\nfisher, r. a. 1935b. appendix to article by c. bliss. ann. appl. biol. 22: 164\u1390165.\nfisher, r. a. 1935c. the logic of inductive inference. j. roy. statist. soc. 98: 39\u139082.\nfisher, r. a. 1945. a new test for 2 = 2 tables letter to the editor . nature 156: 388.\nfisher, r. a. 1956. statistical methods for scientific inference. edinburgh: oliver & boyd.\nfisher, r. a., and f. yates. 1938. statistical tables. edinburgh: oliver and boyd.\nfitzmaurice, g. m., and n. m. laird. 1993. a likelihood-based method for analysing longitudinal\n\n\u017e\n\n.\n\nbinary responses. biometrika 80: 141\u1390151.\n\nfitzmaurice, g. m., n. m. laird, and s. lipsitz. 1994. analysing incomplete longitudinal binary\n\nresponses: a likelihood-based approach. biometrics 50: 601\u1390612.\n\nfitzmaurice, g. m., n. m. laird, and a. g. rotnitzky. 1993. regression models for discrete\n\nlongitudinal responses. statist. sci. 8: 284\u1390299.\n\nfitzpatrick, s., and a. scott. 1987. quick simultaneous confidence intervals for multinomial\n\nproportions. j. amer. statist. assoc. 82: 875\u1390878.\n\nfleiss, j. l. 1981. statistical methods for rates and proportions, 2nd ed. new york: wiley.\nfleiss, j. l., and j. cohen. 1973. the equivalence of weighted kappa and the intraclass\n\ncorrelation coefficient as measures of reliability. educ. psychol. meas. 33: 613\u1390619.\n\nfleiss, j. l., j. cohen, and b. s. everitt. 1969. large-sample standard errors of kappa and\n\nweighted kappa. psychol. bull. 72: 323\u1390327.\n\nfollman, d. a., and d. lambert. 1989. generalizing logistic regression by nonparametric mixing.\n\nj. amer. statist. assoc. 84: 295\u1390300.\n\n "}, {"Page_number": 682, "text": "references\n\n667\n\nforster, j. j., and p. w. f. smith. 1998. model-based inference for categorical survey data\n\nsubject to non-ignorable non-response. j. roy. statist. soc. ser b 60: 57\u139070.\n\nforster, j. j., j. w. mcdonald, and p. w. f. smith. 1996. monte carlo exact conditional tests for\n\nlog-linear and logistic models. j. roy. statist. soc. ser b 58: 445\u1390453.\n\nfowlkes, e. b. 1987. some diagnostics for binary logistic regression via smoothing. biometrika\n\n74: 503\u1390515.\n\nfowlkes, e. b., a. e. freeny, and j. landwehr. 1988. evaluating logistic models for large\n\ncontingency tables. j. amer. statist. assoc. 83: 611\u1390622.\n\nfreedman, d., r. pisani, and r. purves. 1978. statistics. new york: w. w. norton.\nfreeman, g. h., and j. h. halton. 1951. note on an exact treatment of contingency, goodness-\n\nof-fit and other problems of significance. biometrika 38: 141\u1390149.\n\nfreeman, d. h., jr. and t. r. holford. 1980. summary rates. biometrics 36: 195\u1390205.\nfreeman, m. f., and j. w. tukey. 1950. transformations related to the angular and the square\n\nroot. ann. math. statist. 21: 607\u1390611.\n\nfreidlin, b., and j. l. gastwirth. 1999. unconditional versions of several tests commonly used in\n\nthe analysis of contingency tables. biometrics 55: 264\u1390267.\n\nfriendly, m. 2000. visualizing categorical data. cary, nc: sas institute.\nfrome, e. l. 1983. the analysis of rates using poisson regression models. biometrics 39:\n\n665\u1390674.\n\nfuchs, c. 1982. maximum likelihood estimation and model selection in contingency tables with\n\nmissing data. j. amer. statist. assoc. 77: 270\u1390278.\n\ngabriel, k. r. 1966. simultaneous test procedures for multiple comparisons on categorical data.\n\nj. amer. statist. assoc. 61: 1081\u13901096.\n\ngabriel, k. r. 1971. the biplot graphic display of matrices with applications to principal\n\ncomponent analysis. biometrika 58: 453\u1390467.\n\ngail, m. h., and j. j. gart. 1973. the determination of sample sizes for use with the exact\n\nconditional test in 2 = 2 comparative trials. biometrics 29: 441\u1390448.\n\ngail, m., and n. mantel. 1977. counting the number of r = c contingency tables with fixed\n\nmargins. j. amer. statist. assoc. 72: 859\u1390862.\n\ngart, j. j. 1966. alternative analyses of contingency tables. j. roy. statist. soc. ser b 28:\n\n164\u1390179.\n\ngart, j. j. 1969. an exact test for comparing matched proportions in crossover designs.\n\nbiometrika 56: 75\u139080.\n\ngart, j. j. 1970. point and interval estimation of the common odds ratio in the combination of\n\n2 = 2 tables with fixed margins. biometrika 57: 471\u1390475.\n\ngart, j. j. 1971. the comparison of proportions: a review of significance tests, confidence\n\nintervals and adjustments for stratification. re\u00ae. internat. statist. re\u00ae. 39: 148\u1390169.\n\ngart, j. j., and j. nam. 1988. approximate interval estimation of the ratio of binomial\n\nparameters: a review and corrections for skewness. biometrics 44: 323\u1390338.\n\ngart, j. j., and j. r. zweiful. 1967. on the bias of various estimators of the logit and its variance\n\nwith applications to quantal bioassay. biometrika 54: 181\u1390187.\n\ngelfand, a. e., and a. f. smith. 1990. sampling-based approaches to calculating marginal\n\ndensities. j. amer. statist. assoc. 85: 398\u1390409.\n\ngenter, f. c., and v. t. farewell. 1985. goodness-of-link testing in ordinal regression models.\n\ncanad. j. statist. 13: 37\u139044.\n\nghosh, b. k. 1979. a comparison of some approximate confidence intervals for the binomial\n\nparameter. j. amer. statist. assoc. 74: 894\u1390900.\n\nghosh, m., m. chen, a. ghosh, and a. agresti. 2000. hierarchical bayesian analysis of binary\n\nmatched pairs data. statist. sin. 10: 647\u1390657.\n\n "}, {"Page_number": 683, "text": "668\n\nreferences\n\ngibbons, r. d., and d. hedeker. 1997. random-effects probit and logistic regression models for\n\nthree-level data. biometrics 53: 1527\u13901537.\n\ngill, j. 2000. generalized linear models: a unified approach. thousand oaks, ca: sage\n\npublications.\n\ngilmour, a. r., r. d. anderson, and a. l. rae. 1985. the analysis of binomial data by a\n\ngeneralized linear mixed model. biometrika 72: 593\u1390599.\n\ngilula, z., and s. haberman. 1986. canonical analysis of contingency tables by maximum\n\nlikelihood. j. amer. statist. assoc. 81: 780\u1390788.\n\ngilula, z., and s. haberman. 1988. the analysis of multivariate contingency tables by restricted\n\ncanonical and restricted association models. j. amer. statist. assoc. 83: 760\u1390771.\n\ngilula, z., and s. haberman. 1998. chi-square, partition of. pp. 622\u1390627 in encyclopedia of\n\nbiostatistics. chichester, uk: wiley.\n\ngleser, l. j., and d. s. moore. 1985. the effect of positive dependence on chi-squared tests for\n\ncategorical data. j. roy. statist. soc. ser b 47: 459\u1390465.\n\nglonek, g. 1996. a class of regression models for multivariate categorical responses. biometrika\n\n83: 15\u139028.\n\nglonek, g. f. v., and p. mccullagh. 1995. multivariate logistic models. j. roy. statist. soc. ser.\n\nb 57: 533\u1390546.\n\nglonek, g., j. n. darroch, and t. p. speed. 1988. on the existence of maximum likelihood\n\nestimators for hierarchical loglinear models. scand. j. statist. 15: 187\u1390193.\n\ngokhale, d. v., and s. kullback. 1978. the information in contingency tables. new york: marcel\n\ndekker.\n\ngoldstein, h. 1995. multile\u00aeel statistical models, 2nd ed. london: edward arnold.\ngoldstein, h., and j. rasbash. 1996. improved approximations for multilevel models with binary\n\nresponses. j. roy. statist. soc. ser a 159: 505\u1390513.\n\ngood, i. j. 1963. maximum entropy for hypothesis formulation, especially for multi-dimensional\n\ncontingency tables. ann. math. statist. 34: 911\u1390934.\n\ngood, i. j. 1965. the estimation of probabilities: an essay on modern bayesian methods.\n\ncambridge, ma: mit press.\n\ngood, i. j. 1976. on the application of symmetric dirichlet distributions and their mixtures to\n\ncontingency tables. ann. statist. 4: 1159\u13901189.\n\ngood, i. j., and r. a. gaskins. 1971. nonparametric roughness penalties for probability\n\ndensities. biometrika 58: 255\u1390277.\n\ngood, i. j., and y. mittal. 1987. the amalgamation and geometry of two-by-two contingency\n\ntables. ann. statist. 15: 694\u1390711.\n\ngood, i. j., t. n. gover, and g. j. mitchell. 1970. exact distributions for \u24392 and for the\nlikelihood-ratio statistic for the equiprobable multinomial distribution. j. amer. statist.\nassoc. 65: 267\u1390283.\n\ngoodman, l. a. 1964a. simultaneous confidence intervals for cross-product ratios in contin-\n\ngency tables. j. roy. statist. soc. ser b 26: 86\u1390102.\n\ngoodman, l. a. 1964b. interactions in multi-dimensional contingency tables. ann. math. statist.\n\n35: 632\u1390646.\n\ngoodman, l. a. 1965. on simultaneous confidence intervals for multinomial proportions.\n\ntechnometrics 7: 247\u1390254.\n\ngoodman, l. a. 1968. the analysis of cross-classified data: independence, quasi-independence,\nand interactions in contingency tables with or without missing entries. j. amer. statist.\nassoc. 63: 1091\u13901131.\n\ngoodman, l. a. 1969a. on partitioning chi-square and detecting partial association in three-way\n\ncontingency tables. j. roy. statist. soc. ser b 31: 486\u1390498.\n\n "}, {"Page_number": 684, "text": "references\n\n669\n\ngoodman, l. a. 1969b. how to ransack social mobility tables and other kinds of cross-classifica-\n\ntion tables. amer. j. sociol. 75: 1\u139040.\n\ngoodman, l. a. 1970. the multivariate analysis of qualitative data: interaction among multiple\n\nclassifications. j. amer. statist. assoc. 65: 226\u1390256.\n\ngoodman, l. a. 1971a. the analysis of multidimensional contingency tables: stepwise proce-\ndures and direct estimation methods for building models for multiple classifications.\ntechnometrics 13: 33\u139061.\n\ngoodman, l. a. 1971b. the partitioning of chi-square, the analysis of marginal contingency\ntables, and the estimation of expected frequencies in multidimensional contingency tables.\nj. amer. statist. assoc. 66: 339\u1390344.\n\ngoodman, l. a. 1973. the analysis of multidimensional contingency tables with some variables\n\nare posterior to others: a modified path analysis approach. biometrika 60: 179\u1390192.\n\ngoodman, l. a. 1974. exploratory latent structure analysis using both identifiable and unidenti-\n\nfiable models. biometrika 61: 215\u1390231.\n\ngoodman, l. a. 1979a. simple models for the analysis of association in cross-classifications\n\nhaving ordered categories. j. amer. statist. assoc. 74: 537\u1390552.\n\ngoodman, l. a. 1979b. multiplicative models for square contingency tables with ordered\n\ncategories. biometrika 66: 413\u1390418.\n\ngoodman, l. a. 1981a. association models and canonical correlation in the analysis of\n\ncross-classifications having ordered categories. j. amer. statist. assoc. 76: 320\u1390334.\n\ngoodman, l. a. 1981b. association models and the bivariate normal for contingency tables with\n\nordered categories. biometrika 68: 347\u1390355.\n\ngoodman, l. a. 1983. the analysis of dependence in cross-classification having ordered\ncategories, using log-linear models for frequencies and log-linear models for odds. biomet-\nrics 39: 149\u1390160.\n\ngoodman, l. a. 1985. the analysis of cross-classified data having ordered andror unordered\ncategories: association models, correlation models, and asymmetry models for contingency\ntables with or without missing entries. ann. statist. 13: 10\u139069.\n\ngoodman, l. a. 1986. some useful extensions of the usual correspondence analysis approach\nand the usual log-linear models approach in the analysis of contingency tables. internat.\nstatist. re\u00ae. 54: 243\u1390309.\n\ngoodman, l. a. 1996. a single general method for the analysis of cross-classified data:\nreconciliation and synthesis of some methods of pearson, yule, and fisher, and also some\nmethods of correspondence analysis and association analysis. j. amer. statist. assoc. 91:\n408\u1390427.\n\ngoodman, l. a. 2000. the analysis of cross-classified data: notes on a century of progress\nin contingency table analysis, and some comments on its prehistory and its future.\npp. 189\u1390231 in statistics for the 21st century, ed. c. r. rao and g. j. szekely. new york:\nmarcel dekker.\n\n\u00b4\n\ngoodman, l. a., and w. h. kruskal. 1979. measures of association for cross classifications. new\nyork: springer-verlag contains articles appearing in j. amer. statist. assoc. in 1954,\n.\n1959, 1963, 1972 .\n\n\u017e\n\ngould, s. j. 1981. the mismeasure of man. new york: w. w. norton.\ngourieroux, c., a. monfort, and a. trognon. 1984. pseudo maximum likelihood methods:\n\ntheory. econometrica 52: 681\u1390700.\n\ngraubard, b. i., and e. l. korn. 1987. choice of column scores for testing independence in\n\nordered 2 = k contingency tables. biometrics 43: 471\u1390476.\n\ngreen, p. j. 1984. iteratively weighted least squares for maximum likelihood estimation and\n\nsome robust and resistant alternatives. j. roy. statist. soc. ser b 46: 149\u1390192.\n\ngreenacre, m. j. 1993. correspondence analysis in practice. new york: academic press.\n\n "}, {"Page_number": 685, "text": "670\n\nreferences\n\ngreenland, s. 1991. on the logical justification of conditional tests for two-by-two contingency\n\ntables. amer. statist. 45: 248\u1390251.\n\ngreenland, s., and j. m. robins. 1985. estimation of a common effect parameter from sparse\n\nfollow-up data. biometrics 41: 55\u139068.\n\ngreenwood, m., and g. u. yule. 1920. an inquiry into the nature of frequency distributions\nrepresentative of multiple happenings with particular reference to the occurrence of\nmultiple attacks of disease or of repeated accidents. j. roy. statist. soc. ser a 83: 255\u1390279.\ngreenwood, p. e., and m. s. nikulin. 1996. a guide to chi-squared testing. new york: wiley.\ngrizzle, j. e., c. f. starmer, and g. g. koch. 1969. analysis of categorical data by linear\n\nmodels. biometrics 25: 489\u1390504.\n\ngross, s. t. 1981. on asymptotic power and efficiency of tests of independence in contingency\n\ntables with ordered classifications. j. amer. statist. assoc. 76: 935\u1390941.\n\ngueorguieva, r., and a. agresti. 2001. a correlated probit model for joint modeling of clustered\n\nbinary and continuous responses. j. amer. statist. assoc. 96: 1102\u13901112.\n\nhaber, m. 1980. a comparison of some continuity corrections for the chi-squared test on 2 = 2\n\ntables. j. amer. statist. assoc. 75: 510\u1390515.\n\nhaber, m. 1982. the continuity correction and statistical testing. internat. statist. re\u00ae. 50:\n\n135\u1390144.\n\nhaber, m. 1985. maximum likelihood methods for linear and log-linear models in categorical\n\ndata. comput. statist. data anal. 3: 1\u139010.\n\nhaber, m. 1986. an exact unconditional test for the 2 = 2 comparative trial. psychol. bull. 99:\n\n129\u1390132.\n\nhaber, m. 1989. do the marginal totals of a 2 = 2 contingency table contain information\n\nregarding the table proportions? commun. statist. ser a 18: 147\u1390156.\n\nhaberman, s. j. 1973a. the analysis of residuals in cross-classification tables. biometrics 29:\n\n205\u1390220.\n\nhaberman, s. j. 1973b. log-linear models for frequency data: sufficient statistics and likelihood\n\nequations. ann. statist. 1: 617\u1390632.\n\nhaberman, s. j. 1974a. the analysis of frequency data. chicago: university of chicago press.\nhaberman, s. j. 1974b. log-linear models for frequency tables with ordered classifications.\n\nbiometrics 36: 589\u1390600.\n\nhaberman, s. j. 1977a. log-linear models and frequency tables with small expected cell counts.\n\nann. statist. 5: 1148\u13901169.\n\nhaberman, s. j. 1977b. maximum likelihood estimation in exponential response models. ann.\n\nstatist. 5: 815\u1390841.\n\nhaberman, s. j. 1978, 1979. analysis of qualitati\u00aee data, vols. 1 and 2. new york: academic\n\npress.\n\nhaberman, s. j. 1981. tests for independence in two-way contingency tables based on canonical\n\ncorrelation and on linear-by-linear interaction. ann. statist. 9: 1178\u13901186.\n\nhaberman, s. j. 1982. the analysis of dispersion of multinomial responses. j. amer. statist.\n\nassoc. 77: 568\u1390580.\n\nhaberman, s. j. 1988. a warning on the use of chi-squared statistics with frequency tables with\n\nsmall expected cell counts. j. amer. statist. assoc. 83: 555\u1390560.\n\nhaberman, s. j. 1995. computation of maximum likelihood estimates in association models.\n\nj. amer. statist. assoc. 90: 1438\u13901446.\n\nhagenaars, j. a. 1998. categorical causal modeling: latent class analysis and directed log-linear\n\nmodels with latent variables. sociol. methods res. 26: 436\u1390486.\n\nhald, a. 1998. a history of mathematical statistics from 1750 to 1930. new york: wiley.\n\n "}, {"Page_number": 686, "text": "references\n\n671\n\nhaldane, j. b. s. 1940. the mean and variance of \u24392, when used as a test of homogeneity, when\n\nexpectations are small. biometrika 31: 346\u1390355.\n\nhaldane, j. b. s. 1956. the estimation and significance of the logarithm of a ratio of frequencies.\n\nann. human genet. 20: 309\u1390311.\n\nhall, p., and d. m. titterington. 1987. on smoothing sparse multinomial data. austral. j. statist.\n\n29: 19\u139037.\n\nhamada, m., and c. f. j. wu. 1990. a critical\n\nlook at accumulation analysis and related\n\nmethods. technometrics 32: 119\u1390130.\n\nhansen, l. p. 1982. large sample properties of generalized-method of moments estimators.\n\neconometrica 50: 1029\u13901054.\n\nharkness, w. l., and l. katz. 1964. comparison of the power functions for the test of\n\nindependence in 2 = 2 contingency tables. ann. math. statist. 35: 1115\u13901127.\n\nharrell f. e., r. m. califf, d. b. pryor, k. l. lee, and r. a. rosati. 1982. evaluating the yield\n\nof medical tests. j. amer. medic. assoc. 247: 2543\u13902546.\n\nhartzel, j., i.-m. liu, and a. agresti. 2001a. describing heterogeneous effects in stratified\nordinal contingency tables, with application to multi-center clinical trials. computat. statist.\ndata anal. 35: 429\u1390449.\n\nhartzel, j., a. agresti, and b. caffo. 2001b. multinomial logit random effects models. statistical\n\nmodelling 1: 81\u1390102.\n\nhaslett, s. 1990. degrees of freedom and parameter estimability in hierarchical models for\n\nsparse complete contingency tables. computat. statist. data anal. 9: 179\u1390195.\n\nhastie, t., and r. tibshirani. 1987. non-parametric logistic and proportional odds regression.\n\nappl. statist. 36: 260\u1390276.\n\nhastie, t., and r. tibshirani. 1990. generalized additi\u00aee models. london: chapman & hall.\nhatzinger, r. 1989. the rasch model, some extensions and their relation to the class of\ngeneralized linear models. statistical modelling: lecture notes in statistics, vol. 57. berlin:\nspringer-verlag.\n\nhauck, w. w. 1979. the large sample variance of the mantel\u1390haenszel estimator of a common\n\nodds ratio. biometrics 35: 817\u1390819.\n\nhauck, w. w. 1983. a note on confidence bands for the logistic response curve. amer. statist.\n\n37: 158\u1390160.\n\nhauck, w. w., and a. donner. 1977. wald\u2019s test as applied to hypotheses in logit analysis.\n\nj. amer. statist. assoc. 72: 851\u1390853.\n\nheagerty, p. j. 1999. marginally specified logistic-normal models for longitudinal binary data.\n\nbiometrics 55: 688\u1390698.\n\nheagerty, p. j., and s. l. zeger. 1996. marginal regression models for clustered ordinal\n\nmeasurements. j. amer. statist. assoc. 91: 1024\u13901036.\n\nheagerty, p. j., and s. l. zeger. 2000. marginalized multilevel models and likelihood inference.\n\nstatist. sci. 15: 1\u139019.\n\nhedeker, d., and r. d. gibbons. 1994. a random-effects ordinal regression model for multilevel\n\nanalysis. biometrics 50: 933\u1390944.\n\nheinen, t. 1996. latent class and discrete latent trait models. thousand oaks, ca: sage\n\npublications.\n\nheyde, c. c. 1997. quasi-likelihood and its application. new york: springer-verlag.\nhinde, j. 1982. compound poisson regression models. pp. 109\u1390121 in glim82: proc. interna-\ntional conference on generalised linear models, ed. r. gilchrist. new york: springer-verlag.\nhinde, j., and c. g. b. demetrio. 1998. overdispersion: models and estimation. comput. statist.\n\n\u00b4\ndata anal. 27: 151\u1390170.\n\n "}, {"Page_number": 687, "text": "672\n\nreferences\n\nhirji, k. f. 1991. a comparison of exact, mid-p, and score tests for matched case-control studies.\n\nbiometrics 47: 487\u1390496.\n\nhirji, k. f., c. r. mehta, and n. r. patel. 1987. computing distributions for exact logistic\n\nregression. j. amer. statist. assoc. 82: 1110\u13901117.\n\nhirotsu, c. 1982. use of cumulative efficient scores for testing ordered alternatives in discrete\n\nmodels. biometrika 69: 567\u1390577.\n\nhirschfeld, h. o. 1935. a connection between correlation and contingency. cambridge philos.\n\nsoc. proc. math. proc. 31: 520\u1390524.\n\n\u017e\n\n.\n\nhodges, j. l., jr. 1958. fitting the logistic by maximum likelihood. biometrics 14: 453\u1390461.\nhoem, j. m. 1987. statistical analysis of a multiplicative model and its application to the\n\nstandardization of vital rates: a review. internat. statist. re\u00ae. 5: 119\u1390152.\n\nholford, t. r. 1980. the analysis of rates and of survivorship using log-linear models. biometrics\n\n36: 299\u1390305.\n\nholt, d., a. j. scott, and p. d. ewings. 1980. chi-squared tests with survey data. j. roy. statist.\n\nsoc. ser. a 143: 303\u1390320.\n\nhook, e. b., and r. r. regal. 1995. capture\u1390recapture methods in epidemiology: methods and\n\nlimitations. epidemiol. re\u00ae. 17: 243\u1390264.\n\nhosmer, d. w., and s. lemeshow. 1980. a goodness-of-fit test for multiple logistic regression\n\nmodel. commun. statist. ser a 9: 1043\u13901069.\n\nhosmer, d. w., and s. lemeshow. 2000. applied logistic regression, 2nd ed. new york: wiley.\nhosmer, d. w., t. hosmer, s. le cessie, and s. lemeshow. 1997. a comparison of goodness-of-fit\n\ntests for the logistic regression model. statist. medic. 16: 965\u1390980.\n\nhout, m., o. d. duncan, and m. e. sobel. 1987. association and heterogeneity: structural\n\nmodels of similarities and differences. sociol. methodol. 17: 145\u1390184.\n\nhoward, j. v. 1998. the 2 = 2 table: a discussion from a bayesian viewpoint. statist. sci. 13:\n\n351\u1390367.\n\nhsieh, f. y. 1989. sample size tables for logistic regression. statist. medic. 8: 795\u1390802.\nhsieh, f. y., d. a. bloch, and m. d. larsen. 1998. a simple method of sample size calculation\n\nfor linear and logistic regression. statist. medic. 17: 1623\u13901634.\n\nhwang, j. t. g., and m. t. wells. 2002. optimality results for mid p-values. to appear.\nhwang, j. t. g., and m.-c. yang. 2001. an optimality theory for mid p-values in 2 = 2\n\ncontingency tables. statist. sin. 11: 807\u1390826.\n\nimrey, p. b. 1998. bradley\u1390terry model. pp. 437\u1390443 in encyclopedia of biostatistics. chichester,\n\nuk: wiley.\n\nimrey, p. b., w. d. johnson, and g. g. koch. 1976. an incomplete contingency table approach\n\nto paired-comparison experiments. j. amer. statist. assoc. 71: 614\u1390623.\n\nimrey, p. b., g. g. koch, and m. e. stokes. 1981. categorical data analysis: some reflections on\nthe log linear model and logistic regression. i: historical and methodological overview.\ninternat. statist. re\u00ae. 49: 265\u1390283.\n\nireland, c. t., and s. kullback. 1968a. minimum discrimination information estimation.\n\nbiometrics 24: 707\u1390713.\n\nireland, c. t., and s. kullback. 1968b. contingency tables with given marginals. biometrika 55:\n\n179\u1390188.\n\nireland, c. t., h. h. ku, and s. kullback. 1969. symmetry and marginal homogeneity of an r = r\n\ncontingency table. j. amer. statist. assoc. 64: 1323\u13901341.\n\nirwin, j. o. 1935. tests of significance for differences between percentages based on small\n\nnumbers. metron 12: 83\u139094.\n\njennison, c., and b. w. turnbull. 2000. group sequential methods with applications to clinical\n\ntrials. london: chapman & hall.\n\n "}, {"Page_number": 688, "text": "references\n\n673\n\njohnson, b. m. 1971. on the admissible estimators for certain fixed sample binomial problems.\n\nann. math. statist. 42: 1579\u13901587.\n\njohnson, w. 1985. influence measures for logistic regression: another point of view. biometrika\n\n72: 59\u139065.\n\njohnson, n. l., s. kotz, and a. w. kemp. 1992. uni\u00aeariate discrete distributions, 2nd ed. new\n\nyork: wiley.\n\njones, b., and m. g. kenward. 1987. modelling binary data from a three-period cross-over trial.\n\nstatist. medic. 6: 555\u1390564.\n\njones, m. p., t. w. o\u2019gorman, j. h. lemke, and r. f. woolson. 1989. a monte carlo\ninvestigation of homogeneity tests of the odds ratio under various sample size considera-\ntions. biometrics 45: 171\u1390181.\n\n\u2c91\njorgensen, b. 1983. maximum likelihood estimation and large-sample inference for generalized\n\nlinear and nonlinear regression models. biometrika 70: 19\u139028.\n\n\u2c91\njorgensen, b. 1987. exponential dispersion models. j. roy. statist. soc. ser. b 49: 127\u1390162.\nkalbfleisch, j. d., and j. f. lawless. 1985. the analysis of panel data under a markov\n\nassumption. j. amer. statist. assoc. 80: 863\u1390871.\n\nkastner, c., a. fieger, and c. heumann. 1997. mareg and winmareg: a tool for marginal\n\nregression models. comput. statist. data anal. 24: 237\u1390241.\n\nkauermann, g., and r. j. carroll, 2001. a note on the efficiency of sandwich covariance matrix\n\nestimation. j. amer. statist. assoc. 96: 1387\u13901397.\n\nkauermann, g., and g. tutz. 2001. testing generalized linear and semiparametric models\n\nagainst smooth alternatives. j. roy. statist. soc. ser. b 63: 147\u1390166.\n\nkelderman, h. 1984. loglinear rasch model tests. psychometrika 49: 223\u1390245.\nkempthorne, o. 1979. in dispraise of the exact test: reactions. j. statist. plann. inference 3:\n\n199\u1390213.\n\nkendall, m. g. 1945. the treatment of ties in rank problems. biometrika 33: 239\u1390251.\nkendall, m., and a. stuart. 1979. the ad\u00aeanced theory of statistics, vol. 2; inference and\n\nrelationship, 4th ed. new york: macmillan.\n\nkenward, m. g., and b. jones. 1991. the analysis of categorical data from cross-over trials using\n\na latent variable model. statist. medic. 10: 1607\u13901619.\n\nkenward, m. g., and b. jones. 1994. the analysis of binary and categorical data from crossover\n\ntrials. statist. methods medic. res. 3: 325\u1390344.\n\nkenward, m. g., e. lesaffre, and g. molenberghs. 1994. an application of maximum likelihood\nand estimating equations to the analysis of ordinal data from a longitudinal study with\ncases missing at random. biometrics 50: 945\u1390953.\n\nkhamis, h. j. 1983. log-linear model analysis of the semi-symmetric intraclass contingency table.\n\ncommun. statist. ser. a 12: 2723\u13902752.\n\nkim, d., and a. agresti. 1995. improved exact inference about conditional association in\n\nthree-way contingency tables. j. amer. statist. assoc. 90: 632\u1390639.\n\nkim, d., and a. agresti. 1997. nearly exact tests of conditional independence and marginal\n\nhomogeneity for sparse contingency tables. comput. statist. data anal. 24: 89\u1390104.\n\nking, g. 1997. a solution to the ecological inference problem. princeton, nj: princeton univer-\n\nsity press.\n\nknuiman, m. w., and t. p. speed. 1988. incorporating prior information into the analysis of\n\ncontingency tables. biometrics 44: 1061\u13901071.\n\nkoch, g. g., and v. p. bhapkar. 1982. chi-square tests. pp. 442\u1390457 in encyclopedia of\n\nstatistical sciences, vol. 1. new york: wiley.\n\n "}, {"Page_number": 689, "text": "674\n\nreferences\n\nkoch, g. g., j. r. landis, j. l. freeman, d. h. freeman, and r. g. lehnen. 1977. a general\nmethodology for the analysis of experiments with repeated measurement of categorical\ndata. biometrics 33: 133\u1390158.\n\nkoch, g. g., i. a. amara, g. w. davis, and d. b. gillings. 1982. a review of some statistical\n\nmethods for covariance analysis of categorical data. biometrics 38: 563\u1390595.\n\nkoch, g. g., p. b. imrey, j. m. singer, s. s. atkinson, and m. e. stokes. 1985. lecture notes for\n\nanalysis of categorical data. montreal: les presses de l\u2019universite de montreal.\n\n\u00b4\n\n\u00b4\n\nkoehler, k. 1986. goodness-of-fit tests for log-linear models in sparse contingency tables.\n\nj. amer. statist. assoc. 81: 483\u1390493.\n\nkoehler, k. 1998. chi-square tests. pp. 608\u1390622 in encyclopedia of biostatistics. chichester, uk:\n\nwiley.\n\nkoehler, k., and k. larntz. 1980. an empirical investigation of goodness-of-fit statistics for\n\nsparse multinomials. j. amer. statist. assoc. 75: 336\u1390344.\n\nkoehler, k., and j. wilson. 1986. chi-square tests for comparing vectors of proportions for\n\nseveral cluster samples. commun. statist. ser. a 15: 2977\u13902990.\n\nkoopman, p. a. r. 1984. confidence limits for the ratio of two binomial proportions. biometrics\n\n40: 513\u1390517.\n\nkraemer, h. c. 1979. ramifications of a population model for \u242c as a coefficient of reliability.\n\npsychometrika 44: 461\u1390472.\n\nkreiner, s. 1987. analysis of multidimensional contingency tables by exact conditional tests:\n\ntechniques and strategies. scand. j. statist. 14: 97\u1390112.\n\nkreiner, s. 1998. interaction models. pp. 2063\u13902068 in encyclopedia of biostatistics. chichester,\n\nuk: wiley.\n\nkruskal, w. h. 1958. ordinal measures of association. j. amer. statist. assoc. 53: 814\u1390861.\nku, h. h., r. n. varner, and s. kullback. 1971. analysis of multidimensional contingency tables.\n\nj. amer. statist. assoc. 66: 55\u139064.\n\nkuha, j., and c. skinner. 1997. categorical data analysis and misclassification. pp. 633\u1390670 in\n\nsur\u00aeey measurement and process quality, ed. l. lyberg et al. new york: wiley.\n\nkuha, j., c. skinner, and j. palmgren. 1998. misclassification error. pp. 2615\u13902621 in encyclope-\n\ndia of biostatistics. chichester, uk: wiley.\n\nkullback, s. 1959. information theory and statistics. new york: wiley.\nkullback, s., m. kupperman, and h. h. ku. 1962. tests for contingency tables and markov\n\nchains. technometrics 4: 573\u1390608.\n\nkupper, l. l., and j. k. haseman. 1978. the use of a correlated binomial model for the analysis\n\nof certain toxicological experiments. biometrics 34: 69\u139076.\n\nkupper, l. l., c. portier, m. d. hogan, and e. yamamoto. 1986. the impact of litter effects on\n\ndose\u1390response modeling in teratology. biometrics 42: 85\u139098.\n\n\u00a8\u00a8 \u00a8\nlaara, e., and j. n. s. matthews. 1985. the equivalence of two models for ordinal data.\n\nbiometrika 72: 206\u1390207.\n\nlachin, j. m. 1977. sample-size determinations for r = c comparative trials. biometrics 33:\n\n315\u1390324.\n\nlaird, n. m. 1978. empirical bayes methods for two-way contingency tables. biometrika 65:\n\n581\u1390590.\n\nlaird, n. m. 1998. em algorithm. pp. 1300\u13901313 in encyclopedia of biostatistics. chichester, uk:\n\nwiley.\n\nlaird, n. m., and d. olivier. 1981. covariance analysis of censored survival data using log-linear\n\nanalysis techniques. j. amer. statist. assoc. 76: 231\u1390240.\n\nlancaster, h. o. 1949. the derivation and partition of \u24392\n\nin certain discrete distributions.\n\nbiometrika 36: 117\u1390129.\n\n "}, {"Page_number": 690, "text": "references\n\n675\n\nlancaster, h. o. 1951. complex contingency tables treated by partition of \u24392. j. roy. statist.\n\nsoc. ser. b 13: 242\u1390249.\n\nlancaster, h. o. 1961. significance tests in discrete distributions. j. amer. statist. assoc. 56:\n\n223\u1390234.\n\nlancaster, h. o. 1969. the chi-squared distribution. new york: wiley.\nlancaster, h. o., and m. a. hamdan. 1964. estimation of the correlation coefficient in\n\ncontingency tables with possible nonmetrical characters. psychometrika 29: 383\u1390391.\n\nlandis, j. r., and g. g. koch. 1977. an application of hierarchical kappa-type statistics in the\n\nassessment of majority agreement among multiple observers. biometrics 33: 363\u1390374.\n\nlandis, j. r., e. r. heyman, and g. g. koch. 1978. average partial association in three-way\ncontingency tables: a review and discussion of alternative tests. internat. statist. re\u00ae. 46:\n237\u1390254.\n\nlandis, j. r., t. j. sharp, s. j. kuritz, and g. g. koch. 1998. mantel-haenszel methods.\n\npp. 2378\u13902691 in encyclopedia of biostatistics. chichester, uk: wiley.\n\nlandwehr, j. m., d. pregibon, and a. c. shoemaker. 1984. graphical methods for assessing\n\nlogistic regression models. j. amer. statist. assoc. 79: 61\u139071.\n\nlang, j. b. 1992. obtaining the observed information matrix for the poisson log linear model\n\nwith incomplete data. biometrika 79: 405\u1390407.\n\nlang, j. b. 1996a. maximum likelihood methods for a generalized class of log-linear models.\n\nann. statist. 24: 726\u1390752.\n\nlang, j. b. 1996b. on the partitioning of goodness-of-fit statistics for multivariate categorical\n\nresponse models. j. amer. statist. assoc. 91: 1017\u13901023.\n\nlang, j. b. 1996c. on the comparison of multinomial and poisson log-linear models. j. roy.\n\nstatist. soc. ser. b 58: 253\u1390266.\n\nlang, j. b., and a. agresti. 1994. simultaneously modeling joint and marginal distributions of\n\nmultivariate categorical responses. j. amer. statist. assoc. 89: 625\u1390632.\n\nlang, j. b., j. w. mcdonald, and p. w. f. smith. 1999. association-marginal modeling of\nmultivariate categorical responses: a maximum likelihood approach. j. amer. statist.\nassoc. 94: 1161\u13901171.\n\n\u00b4\n\n\u00b4\n\nlaplace, p. s. 1812. theorie analytique des probabilites. paris: courcier.\nlarntz, k. 1978. small-sample comparison of exact levels for chi-squared goodness-of-fit statis-\n\ntics. j. amer. statist. assoc. 73: 253\u1390263.\n\nlarsen, k., j. h. petersen, e. budtz-jo\u2c91rgensen, and l. endahl. 2000. interpreting parameters in\n\nthe logistic regression model with random effects. biometrics 56: 909\u1390914.\n\nlarson, m. g. 1984. covariate analysis of competing-risks data with log-linear models. biomet-\n\nrics 40: 459\u1390469.\n\nlauritzen, s. l. 1996. graphical models. new york: oxford university press.\nlauritzen, s. l., and n. wermuth. 1989. graphical models for associations between variables,\n\nsome of which are qualitative and some quantitative. ann. statist. 17: 31\u139057.\n\nlavange, l. m., g. g. koch, and t. a. schwartz. 2001. applying sample survey methods to\n\nclinical trials data. statist. medic. 20: 2609\u13902623.\n\nlawal, h. b. 1984. comparisons of the x 2, y 2, freeman\u1390tukey and williams improved g2 test\n\nstatistics in small samples of one-way multinomials. biometrika 71: 415\u1390418.\n\nlawless, j. f. 1987. negative binomial and mixed poisson regression. canad. j. statist. 15:\n\n209\u1390225.\n\nlazarsfeld, p. f., and n. w. henry. 1968. latent structure analysis. boston: houghton mifflin.\nlee, s. k. 1977. on the asymptotic variances of u terms in loglinear models of multidimensional\n\n\u02c6\n\ncontingency tables. j. amer. statist. assoc. 72: 412\u1390419.\n\n "}, {"Page_number": 691, "text": "676\n\nreferences\n\nlee, y., and j. a. nelder. 1996. hierarchical generalized linear models. j. roy. statist. soc. ser\n\nb 58: 619\u1390678.\n\nlefkopoulou, m., d. moore, and l. ryan. 1989. the analysis of multiple correlated binary\nj. amer. statist. assoc. 84:\n\noutcomes: application to rodent teratology experiments.\n810\u1390815.\n\nlehmann, e. l. 1966. some concepts of dependence. ann. math. statist. 37: 1137\u13901153.\nlehmann, e. l. 1986. testing statistical hypotheses, 2nd ed. new york: wiley.\nleonard, t. 1975. bayesian estimation methods for two-way contingency tables. j. roy. statist.\n\nsoc. ser. b 37: 23\u139037.\n\nleonard, t. and j. s. j. hsu. 1994. the bayesian analysis of categorical data: a selective review.\npp. 283\u1390310 in aspects of uncertainty: a tribute to d. v. lindley. p. r. freeman and\na. f. m. smith, eds. new york: wiley.\n\nlesaffre, e., and a. albert. 1989. multiple-group logistic regression diagnostics. appl. statist. 38:\n\n425\u1390440.\n\nlesaffre, e., and g. molenberghs. 1991. multivariate probit analysis: a neglected procedure in\n\nmedical statistics. statist. medic. 10: 1391\u13901403.\n\nlesaffre, e., and b. spiessens. 2001. on the effect of quadrature points in a logistic random-\n\neffects model: an example. appl. statist. 50: 325\u1390335.\n\nlewis, t., i. w. saunders, and m. westcott. 1984. the moments of the pearson chi-squared\n\nstatistic and the minimum expected value in two-way tables. biometrika 71: 515\u1390522.\n\nliang, k. y. 1984. the asymptotic efficiency of conditional likelihood methods. biometrika 71:\n\n305\u1390313.\n\nliang, k. y., and j. hanfelt. 1994. on the use of the quasi-likelihood method in teratological\n\nexperiments. biometrics 50: 872\u1390880.\n\nliang, k. y., and p. mccullagh. 1993. case studies in binary dispersion. biometrics 49: 623\u1390630.\nliang, k. y., and s. g. self. 1985. tests for homogeneity of odds ratios when the data are\n\nsparse. biometrika 72: 353\u1390358.\n\nliang, k. y., and s. l. zeger. 1986. longitudinal data analysis using generalized linear models.\n\nbiometrika 73: 13\u139022.\n\nliang, k. y., and s. l. zeger. 1988. on the use of concordant pairs in matched case\u1390control\n\nstudies. biometrics 44: 1145\u13901156.\n\nliang, k. y, and s. l. zeger. 1995. inference based on estimating functions in the presence of\n\nnuisance parameters. statist. sci. 10 158\u1390173.\n\nliang, k. y., s. l. zeger, and b. qaqish. 1992. multivariate regression analyses for categorical\n\ndata. j. roy. statist. soc.ser. b 54: 3\u139024.\n\nlin, x. 1997. variance component testing in generalized linear models with random effects.\n\nbiometrika 84: 309\u1390326.\n\nlindley, d. v. 1964. the bayesian analysis of contingency tables. ann. math. statist. 35:\n\n1622\u13901643.\n\nlindsay, b., c. clogg, and j. grego. 1991. semi-parametric estimation in the rasch model and\nincluding a simple latent class model for item\n\nrelated exponential response models,\nanalysis. j. amer. statist. assoc. 86: 96\u1390107.\n\nlindsey, j. k. 1999. models for repeated measurements, 2nd ed. oxford: oxford university press.\nlindsey, j. k., and p. m. e. altham. 1998. analysis of the human sex ratio by using overdisper-\n\nsion models. appl. statist. 47: 149\u1390157.\n\nlindsey, j. k., and g. mersch. 1992. fitting and comparing probability distributions with log\n\nlinear models. comput. statist. data anal. 13: 373\u1390384.\n\nlipsitz, s. 1992. methods for estimating the parameters of a linear model for ordered categorical\n\ndata. biometrics 48: 271\u1390281.\n\n "}, {"Page_number": 692, "text": "references\n\n677\n\nlipsitz, s. r., and g. fitzmaurice. 1996. the score test for independence in r = c contingency\n\ntables with missing data. biometrics 52: 751\u1390762.\n\nlipsitz, s., n. laird, and d. harrington. 1990. finding the design matrix for the marginal\n\nhomogeneity model. biometrika 77: 353\u1390358.\n\nlipsitz, s., n. laird, and d. harrington. 1991. generalized estimating equations for correlated\n\nbinary data: using the odds ratio as a measure of association. biometrika 78: 153\u1390160.\n\nlipsitz, s. r., k. kim, and l. zhao. 1994. analysis of repeated categorical data using generalized\n\nestimating equations. statist. medic. 13: 1149\u13901163.\n\nlittle, r. j. 1989. testing the equality of two independent binomial proportions. amer. statist.\n\n43: 283\u1390288.\n\nlittle, r. j. 1998. missing data. pp. 2622\u13902635 in encyclopedia of biostatistics. chichester, uk:\n\nwiley.\n\nlittle, r. j., and d. b. rubin. 1987. statistical analysis with missing data. new york: wiley.\nlittle, r. j. a., and m.-m. wu. 1991. models for contingency tables with known margins when\n\ntarget and sampled populations differ. j. amer. statist. assoc. 86: 87\u139095.\n\nliu, q., and d. a. pierce. 1993. heterogeneity in mantel\u1390haenszel-type models. biometrika 80:\n\n543\u1390556.\n\nliu, q., and d. a. pierce. 1994. a note on gauss\u1390hermite quadrature. biometrika 81: 624\u1390629.\nlloyd, c. j. 1988a. some issues arising from the analysis of 2 = 2 contingency tables. austral. j.\n\nstatist. 30: 35\u139046.\n\nlloyd, c. j. 1988b. doubling the one-sided p-value in testing independence in 2 = 2 tables\n\nagainst a two-sided alternative. statist. medic. 7: 1297\u13901306.\n\nlloyd, c. j. 1999. statistical analysis of categorical data. new york: wiley.\nlongford, n. t. 1993. random coefficient models. new york: oxford university press.\nloughin, t. m., and p. n. scherer. 1998. testing for association in contingency tables with\n\nmultiple column responses. biometrics 54: 630\u1390637.\n\nlouis, t. a. 1982. finding the observed information matrix when using the em algorithm.\n\nj. roy. statist. soc. ser. b 44: 226\u1390233.\n\nluce, r. d. 1959. indi\u00aeidual choice beha\u00aeior. new york: wiley.\nmadansky, a. 1963. tests of homogeneity for correlated samples. j. amer. statist. assoc. 58:\n\n97\u1390119.\n\nmaddala, g. s. 1983. limited-dependent and qualitati\u00aee variables in econometrics. cambridge:\n\ncambridge university press.\n\nmagnus, j. r., and h. neudecker. 1988. matrix differential calculus with applications in statistics\n\nand econometrics. new york: wiley.\n\nmantel, n. 1963. chi-square tests with one degree of\n\nfreedom: extensions of\n\nthe\n\nmantel\u1390haenszel procedure. j. amer. statist. assoc. 58: 690\u1390700.\n\nmantel, n. 1966. models for complex contingency tables and polychotomous dosage response\n\ncurves. biometrics 22: 83\u139095.\n\nmantel, n. 1973. synthetic retrospective studies and related topics. biometrics 29: 479\u1390486.\nmantel, n. 1985. maximum likelihood vs. minimum chi-square. biometrics 41: 777\u1390781.\nmantel, n. 1987a. understanding wald\u2019s test\n\nfor exponential\n\nfamilies. amer. statist. 41:\n\n147\u1390148.\n\nmantel, n. 1987b. exact tests for 2 = 2 contingency tables letter . amer. statist. 41: 159.\nmantel, n., and d. p. byar. 1978. marginal homogeneity, symmetry and independence.\n\ncommun. statist. ser. a 7: 953\u1390976.\n\nmantel, n., and w. haenszel. 1959. statistical aspects of the analysis of data from retrospective\n\nstudies of disease. j. natl. cancer inst. 22: 719\u1390748.\n\n\u017e\n\n.\n\n "}, {"Page_number": 693, "text": "678\n\nreferences\n\n\u00b4martin andres, a., and silva mato, a. 1994. choosing the optimal unconditional test for\n\n\u00b4\n\ncomparing two independent proportions. comput. statist. data anal. 17: 555\u1390574.\n\nmatthews, j. n. s., and k. p. morris. 1995. an application of bradley\u1390terry-type models to the\n\nmeasurement of pain. appl. statist. 44: 243\u1390255.\n\nmccullagh, p. 1978. a class of parametric models for the analysis of square contingency tables\n\nwith ordered categories. biometrika 65: 413\u1390418.\n\nmccullagh, p. 1980. regression models for ordinal data. j. roy. statist. soc. ser. b 42: 109\u1390142.\nmccullagh, p. 1982. some applications of quasisymmetry. biometrika 69: 303\u1390308.\nmccullagh, p. 1983. quasi-likelihood functions. ann. statist. 11: 59\u139067.\nmccullagh, p. 1986. the conditional distribution of goodness-of-fit statistics for discrete data.\n\nj. amer. statist. assoc. 81: 104\u1390107.\n\nmccullagh, p., and j. a. nelder. 1983; 2nd ed., 1989. generalized linear models. london:\n\nchapman & hall.\n\nmcculloch, c. e. 1994. maximum likelihood variance components estimation for binary data.\n\nj. amer. statist. assoc. 89: 330\u1390335.\n\nmcculloch, c. e. 1997. maximum likelihood algorithms for generalized linear mixed models.\n\nj. amer. statist. assoc. 92: 162\u1390170.\n\nmcculloch, c. e. 2000. generalized linear models. j. amer. statist. assoc. 95: 1320\u13901324.\nmcculloch, c. e., and s. searle. 2001. generalized, linear, and mixed models. new york: wiley.\nmcfadden, d. 1974. conditional logit analysis of qualitative choice behavior. pp. 105\u1390142 in\n\nfrontiers in econometrics, ed. p. zarembka. new york: academic press.\n\nmcfadden, d. 1982. qualitative response models. pp. 1\u139037 in ad\u00aeances in econometrics, ed.\n\nw. hildebrand. cambridge: cambridge university press.\n\nmcnemar, q. 1947. note on the sampling error of the difference between correlated proportions\n\nor percentages. psychometrika 12: 153\u1390157.\n\nmee, r. w. 1984. confidence bounds for the difference between two probabilities\n\n\u017e\n.\nletter .\n\nbiometrics 40: 1175\u13901176.\n\nmeeden, g., c. geyer, j. lang, and e. funo. 1998. the admissibility of the maximum likelihood\nestimator for decomposable log-linear interaction models for contingency tables. commun.\nstatist. ser. a 27: 473\u1390493.\n\nmehta, c. r. 1994. the exact analysis of contingency tables in medical research. statist. methods\n\nmedic. res. 3: 135\u1390156.\n\nmehta, c. r., and n. r. patel. 1983. a network algorithm for performing fisher\u2019s exact test in\n\nr = c contingency tables. j. amer. statist. assoc. 78: 427\u1390434.\n\nmehta, c. r., and n. r. patel. 1995. exact logistic regression: theory and examples. statist.\n\nmedic. 14: 2143\u13902160.\n\nmehta, c. r., and s. j. walsh. 1992. comparison of exact, mid-p, and mantel\u1390haenszel\nconfidence intervals for the common odds ratio across several 2 = 2 contingency tables.\namer. statist. 46: 146\u1390150.\n\nmehta, c. r., n. r. patel, and r. gray. 1985. computing an exact confidence interval for the\ncommon odds ratio in several 2 by 2 contingency tables. j. amer. statist. assoc. 80:\n969\u1390973.\n\nmehta, c. r., n. r. patel, and p. senchaudhuri. 1988. importance sampling for estimating exact\n\nprobabilities in permutational inference. j. amer. statist. assoc. 83: 999\u13901005.\n\nmehta, c. r., n. r. patel, and p. senchaudhuri. 2000. efficient monte carlo methods for\n\nconditional logistic regression. j. amer. statist. assoc. 95: 99\u1390108.\n\nmichailidis, g., and j. de leeuw. 1998. the gifi system of descriptive multivariate analysis.\n\nstatist. sci. 13: 307\u1390336.\n\n "}, {"Page_number": 694, "text": "references\n\n679\n\nmiettinen, o. s. 1969. individual matching with multiple controls in the case of all-or-none\n\nresponses. biometrics 25: 339\u1390355.\n\nmiettinen, o. s., and m. nurminen. 1985. comparative analysis of two rates. statist. medic. 4:\n\n213\u1390226.\n\nmiller, m. e., c. s. davis, and j. r. landis. 1993. the analysis of longitudinal polytomous data:\ngeneralized estimating equations and connections with weighted least squares. biometrics\n49: 1033\u13901044.\n\nminkin, s. 1987. on optimal design for binary data. j. amer. statist. assoc. 82: 1098\u13901103.\nmirkin, b. 2001. eleven ways to look at the chi-squared coefficient for contingency tables. amer.\n\nstatist. 55: 111\u1390120.\n\nmitra, s. k. 1958. on the limiting power function of the frequency chi-square test. ann. statist.\n\n29: 1221\u13901233.\n\nmolenberghs, g., and e. goetghebeur. 1997. simple fitting algorithms for incomplete categorical\n\ndata. j. roy. statist. soc. ser. b 59: 401\u1390414.\n\nmolenberghs, g., and e. lesaffre. 1994. marginal modeling of correlated ordinal data using a\n\nmultivariate plackett distribution. j. amer. statist. assoc. 89: 633\u1390644.\n\nmolenberghs, g., m. g. kenward, and e. lesaffre. 1997. the analysis of longitudinal ordinal\n\ndata with nonrandom drop-out. biometrika 84: 33\u139044.\n\nmoore, d. f. 1986a. asymptotic properties of moment estimates for overdispersed counts and\n\nproportions. biometrika 35: 583\u1390588.\n\nmoore, d. s. 1986b. tests of chi-squared type. pp. 63\u139095 in goodness-of-fit techniques, ed.\n\nr. d\u2019agostino and m. a. stephens. new york: marcel dekker.\n\nmoore, d. f., and a. tsiatis. 1991. robust estimation of the variance in moment methods for\n\nextra-binomial and extra-poisson variation. biometrics 47: 383\u1390401.\n\nmorgan, b. j. t. 1992. analysis of quantal response data. london: chapman & hall.\nmorgan, w. m., and b. a. blumenstein. 1991. exact conditional tests for hierarchical models in\n\nmultidimensional contingency tables. appl. statist. 40: 435\u1390442.\n\nmosimann, j. e. 1962. on the compound multinomial distribution, the multivariate \u2424-distribu-\n\ntion and correlations among proportions. biometrika 49: 65\u139082.\n\nmosteller, f. 1951. remarks on the method of paired comparisons i: the least-squares solution\n\nassuming equal standard deviations and equal correlations. psychometrika 16: 3\u13909.\n\nmosteller, f. 1952. some statistical problems in measuring the subjective response to drugs.\n\nbiometrics 8: 220\u1390226.\n\nmosteller, f. 1968. association and estimation in contingency tables. j. amer. statist. assoc. 63:\n\n1\u139028.\n\nnair, v. n. 1987. chi-squared-type tests for ordered alternatives in contingency tables. j. amer.\n\nstatist. assoc. 82: 283\u1390291.\n\nnatarajan, r., and c. mcculloch. 1995. a note on the existence of the posterior distribution for\n\na class of mixed models for binomial responses. biometrika 82: 639\u1390643.\n\nnatarajan, r., and c. mcculloch. 1998. gibbs sampling with diffuse proper priors: a valid\n\napproach to data-driven inference? j. comput. graph. statist. 7: 267\u1390277.\n\nnelder, j., and d. pregibon. 1987. an extended quasi-likelihood function. biometrika 74:\n\n221\u1390232.\n\nnelder, j., and r. w. m. wedderburn. 1972. generalized linear models. j. roy. statist. soc. ser.\n\na 135: 370\u1390384.\n\nnerlove, m., and s. j. press. 1973. univariate and multivariate log-linear and logistic models.\n\ntechnical report r-1306-edarnih, rand corporation, santa monica, ca.\n\nneuhaus, j. m. 1992. statistical methods for longitudinal and clustered designs with binary\n\nresponses. statist. methods medic. res. 1: 249\u1390273.\n\n "}, {"Page_number": 695, "text": "680\n\nreferences\n\nneuhaus, j. m., and n. p. jewell. 1990a. some comments on rosner\u2019s multiple logistic model for\n\nclustered data. biometrics 46: 523\u1390534.\n\nneuhaus, j. m., and n. p. jewell. 1990b. the effect of retrospective sampling on binary\n\nregression models for clustered data. biometrics 46: 977\u1390990.\n\nneuhaus, j. m., and m. l. lesperance. 1996. estimation efficiency in a binary mixed-effects\n\nmodel setting. biometrika 83: 441\u1390446.\n\nneuhaus, j. m., j. d. kalbfleisch, and w. w. hauck. 1991. a comparison of cluster-specific and\npopulation-averaged approaches for analyzing correlated binary data. internat. statist. re\u00ae.\n59: 25\u139035.\n\nneuhaus, j. m., w. w. hauck, and j. d. kalbfleisch. 1992. the effects of mixture distribution\n\nmisspecification when fitting mixed-effects logistic models. biometrika 79: 755\u1390762.\n\nneuhaus, j. m., j. d. kalbfleisch, and w. w. hauck. 1994. conditions for consistent estimation\n\nin mixed-effects models for binary matched-pairs data. canad. j. statist. 22: 139\u1390148.\n\nnewcombe, r. 1998a. two-sided confidence intervals for the single proportion: comparison of\n\nseven methods. statist. medic. 17: 857\u1390872.\n\nnewcombe, r. 1998b. interval estimation for the difference between independent proportions:\n\ncomparison of eleven methods. statist. medic. 17: 873\u1390890.\n\nnewcombe, r. 2001. logit confidence intervals and the inverse sinh transformation. amer.\n\nstatist. 55: 200\u1390202.\n\nneyman, j. 1935. on the problem of confidence limits. ann. math. statist. 6: 111\u1390116.\nneyman, j. 1949. contributions to the theory of the \u24392 test. pp. 239\u1390273 in proc. first berkeley\nsymposium on mathematical statistics and probability, ed. j. neyman. berkeley, ca:\nuniversity of california press.\n\nnurminen, m. 1986. confidence intervals for the ratio and difference of two binomial propor-\n\ntions. biometrics 42: 675\u1390676.\n\no\u2019brien, p. c. 1988. comparing two samples: extensions of the t, rank-sum, and log-rank tests.\n\nj. amer. statist. assoc. 83: 52\u139061.\n\no\u2019brien, r. g. 1986. using the sas system to perform power analyses for log-linear models.\npp. 778\u1390784 in proc. 11th annual sas users group conference. cary, nc: sas institute.\nochi, y., and r. prentice. 1984. likelihood inference in a correlated probit regression model.\n\nbiometrika 71: 531\u1390543.\n\no\u2019gorman, t. w., and r. f. woolson. 1988. analysis of ordered categorical data using the sas\nsystem. pp. 957\u1390963 in proc. 13th annual sas users group conference. cary, nc: sas\ninstitute.\n\npaik, m. 1985. a graphic representation of a three-way contingency table: simpson\u2019s paradox\n\nand correlation. amer. statist. 39: 53\u139054.\n\npalmgren, j. 1981. the fisher information matrix for log-linear models arguing conditionally in\n\nthe observed explanatory variables. biometrika 68: 563\u1390566.\n\npalmgren, j., and a. ekholm. 1987. exponential family non-linear models for categorical data\n\nwith errors of observation. appl. stochastic models data anal. 3: 111\u1390124.\n\npark, t., and m. b. brown. 1994. models for categorical data with nonignorable nonresponse. j.\n\namer. statist. assoc. 89: 44\u139052.\n\nparr, w. c., and h. d. tolley. 1982. jackknifing in categorical data analysis. austral. j. statist.\n\n24: 67\u139079.\n\nparzen, e. 1997. concrete statistics. pp. 309\u1390332 in statistics of quality. new york: marcel\n\ndekker.\n\npatefield, w. m. 1982. exact tests for trends in ordered contingency tables. appl. statist. ser b\n\n31: 32\u139043.\n\n "}, {"Page_number": 696, "text": "references\n\n681\n\npatnaik, p. b. 1949. the non-central \u24392 and f-distributions and their applications. biometrika\n\n36: 202\u1390232.\n\npaul, s. r., k. y. liang, and s. g. self. 1989. on testing departure from the binomial and\n\nmultinomial assumptions. biometrics 45: 231\u1390236.\n\npearson, e. s. 1947. the choice of a statistical test illustrated on the interpretation of data\n\nclassified in 2 = 2 tables. biometrika 34: 139\u1390167.\n\npearson, k. 1900. on a criterion that a given system of deviations from the probable in the case\nof a correlated system of variables is such that it can be reasonably supposed to have arisen\nfrom random sampling. philos. mag. ser. 5 50: 157\u1390175. reprinted in karl pearson\u2019s early\nstatistical papers, ed. e. s. pearson. cambridge: cambridge university press, 1948.\n\n\u017e\n\n.\n\npearson, k. 1904. mathematical contributions to the theory of evolution xiii: on the theory of\ncontingency and its relation to association and normal correlation. draper\u2019s co. research\nmemoirs, biometric series, no. 1. reprinted in karl pearson\u2019s early papers, ed. e. s.\n.\npearson, cambridge: cambridge university press, 1948.\n\n\u017e\n\npearson, k. 1913. on the probable error of a correlation coefficient as found from a fourfold\n\ntable. biometrika 9: 22\u139027.\n\npearson, k. 1917. on the general theory of multiple contingency with special reference to partial\n\ncontingency. biometrika 11: 145\u1390158.\n\npearson, k. 1922. on the \u24392 test of goodness of fit. biometrika 14: 186\u1390191.\npearson, k., and d. heron. 1913. on theories of association. biometrika 9: 159\u1390315.\npeduzzi, p., j. concato, e. kemper, t. r. holford, and a. r. feinstein. 1996. a simulation study\nof the number of events per variable in logistic regression analysis. j. clin. epidemiol. 49:\n1373\u13901379.\n\npendergast, j. f., s. j. gange, m. a. newton, m. j. lindstrom, m. palta, and m. r. fisher. 1996.\na survey of methods for analyzing clustered binary response data. internat. statist. re\u00ae. 64:\n89\u1390118.\n\npepe, m. s. 2000. receiver operating characteristic methodology. j. amer. statist. assoc. 95:\n\n308\u1390311.\n\npeterson, b., and f. e. harrell, jr. 1990. partial proportional odds models for ordinal response\n\nvariables. appl. statist. 39: 205\u1390217.\n\npierce, d. a., and d. peters. 1992. practical use of higher order asymptotics for multiparameter\n\nexponential families. j. roy. statist. soc. ser. b 54: 701\u1390725.\n\npierce, d. a., and d. peters. 1999. improving on exact tests by approximate conditioning.\n\nbiometrika 86: 265\u1390277.\n\npierce, d. a., and b. r. sands. 1975. extra-bernoulli variation in regression of binary data.\n\ntechnical report 46, statistics deptartment, oregon state university, cornwallis, or.\n\npierce, d. a., and d. w. schafer. 1986. residuals in generalized linear models. j. amer. statist.\n\nassoc. 81: 977\u1390983.\n\nplackett, r. l. 1962. a note on interactions in contingency tables. j. roy. statist. soc. ser. b 24:\n\n162\u1390166.\n\nplackett, r. l. 1964. the continuity correction in 2 = 2 tables. biometrika 51: 327\u1390337.\nplackett, r. l. 1983. karl pearson and the chi-squared test. internat. statist. re\u00ae. 51: 59\u139072.\npodgor, m. j., j. l. gastwirth, and c. r. mehta. 1996. efficiency robust tests of independence in\n\ncontingency tables with ordered classifications. statist. medic. 15: 2095\u13902105.\n\n`\npoisson, s.-d. 1837. recherches sur la probabilite des jugements en matieere criminelle et en matiere\n\nci\u00aeile, precedees des regles generales du calcul des probabilites. paris: bachelier.\n\n`\n\n\u00b4\n\n\u00b4 \u00b4 \u00b4\n\n\u00b4 \u00b4\n\n\u00b4\n\n`\n\npratt, j. w. 1981. concavity of the log likelihood. j. amer. statist. assoc. 76: 103\u1390106.\npregibon, d. 1980. goodness of link tests for generalized linear models. appl. statist. 29: 15\u139024.\npregibon, d. 1981. logistic regression diagnostics. ann. statist. 9: 705\u1390724.\n\n "}, {"Page_number": 697, "text": "682\n\nreferences\n\npregibon, d. 1982. score tests in glim with application. pp. 87\u139097 in lecture notes in statistics,\n14: glim 82, proc. international conference on generalised linear models, ed. r. gilchrist.\nnew york: springer-verlag.\n\nprentice, r. 1976a. use of the logistic model in retrospective studies. biometrics 32: 599\u1390606.\nprentice, r. 1976b. generalization of the probit and logit methods for dose response curves.\n\nbiometrics 32: 761\u1390768.\n\nprentice, r. 1986. binary regression using an extended beta-binomial distribution, with discus-\nsion of correlation induced by covariate measurement errors. j. amer. statist. assoc. 81:\n321\u1390327.\n\nprentice, r., and n. breslow. 1978. retrospective studies and failure time models. biometrika\n\n65: 153\u1390158.\n\nprentice, r., and l. a. gloeckler. 1978. regression analysis of grouped survival data with\n\napplication to breast cancer data. biometrics 34: 57\u139067.\n\nprentice, r., and r. pyke. 1979. logistic disease incidence models and case-control studies.\n\nbiometrika 66: 403\u1390412.\n\nprentice, r., and l. p. zhao. 1991. estimating equations for parameters in means and\n\ncovariances of multivariate discrete and continuous responses. biometrics 47: 825\u1390839.\n\npress, s. j., and s. wilson. 1978. choosing between logistic regression and discriminant analysis.\n\nj. amer. statist. assoc. 73: 699\u1390705.\n\nqu, a., b. g. lindsay, and b. li. 2000. improving generalised estimating equations using\n\nquadratic inference functions. biometrika 87: 823\u1390836.\n\nquine, m. p., and e. seneta. 1987. bortkiewicz\u2019s data and the law of small numbers. internat.\n\nstatist. re\u00ae. 5: 173\u1390181.\n\nrabe-hesketh, s., and a. skrondal. 2001. parameterisation of multivariate random effects\n\nmodels for categorical data. biometrics 57:\u1390.\n\nraftery, a. e. 1986. choosing models for cross-classification. amer. sociol. re\u00ae. 51: 145\u1390146.\nrao, c. r. 1957. maximum likelihood estimation for the multinomial distribution. sankhya 18:\n\n139\u1390148.\n\nrao, c. r. 1963. criteria of estimation in large samples. sankhya 25: 189\u1390206.\nrao, c. r. 1973. linear statistical inference and its applications, 2nd ed. new york: wiley.\nrao, c. r. 1982. diversity: its measurement, decomposition, apportionment, and analysis.\n\nsankhya ser. a 44: 1\u139022.\n\nrao, j. n. k., and a. j. scott. 1987. on simple adjustments to chi-square tests with sample\n\nsurvey data. ann. statist. 15: 385\u1390397.\n\nrao, j. n. k., and d. r. thomas. 1988. the analysis of cross-classified categorical data from\n\ncomplex sample surveys. sociol. methodol. 18: 213\u1390270.\n\nrasch, g. 1961. on general laws and the meaning of measurement in psychology. pp. 321\u1390333 in\nproc. 4th berkeley symposium on mathematics, statistics, and probability, vol. 4, ed.\nj. neyman. berkeley, ca: university of california press.\n\nrayner, j. c. w., and d. j. best. 2001. a contingency table approach to nonparametric testing.\n\nlondon: chapman & hall.\n\nread, t. r. c., and n. a. c. cressie. 1988. goodness-of-fit statistics for discrete multi\u00aeariate\n\ndata. new york: springer-verlag.\n\nrice, w. r. 1988. a new probability model for determining exact p-values for 2 = 2 contingency\n\ntables when comparing binomial proportions. biometrics 44: 1\u139022.\n\nritov, y., and z. gilula. 1991. the order-restricted rc model for ordered contingency tables:\n\nestimation and testing for fit. ann. statist. 19: 2090\u13902101.\n\nrobins, j., n. breslow, and s. greenland. 1986. estimators of the mantel\u1390haenszel variance\n\nconsistent in both sparse data and large-strata limiting models. biometrics 42: 311\u1390323.\n\n "}, {"Page_number": 698, "text": "references\n\n683\n\nrobins, j., a. rotnitzky, and l. p. zhao. 1995. analysis of semiparametric regression models for\nrepeated outcomes in the presence of missing data. j. amer. statist. assoc. 90: 106\u1390121.\nrohmel, j., and u. mansmann. 1999. unconditional non-asymptotic one-sided tests for indepen-\nlies in showing non-inferiority andror\n\n\u00a8\n\ndent binomial proportions when the interest\nsuperiority. biometrical j. 41: 149\u1390170.\n\nrosenbaum, p. r., and d. r. rubin. 1983. the central role of the propensity score in\n\nobservational studies for causal effects. biometrika 70: 41\u139055.\n\nrosner, b. 1984. multivariate methods in ophthalmology with application to other paired-data\n\nsituations. biometrics 40: 1025\u13901035.\n\nrosner, b. 1989. multivariate methods for clustered binary data with more than one level of\n\nnesting. j. amer. statist. assoc. 84: 373\u1390380.\n\nrotnitzky, a., and n. p. jewell. 1990. hypothesis testing of regression parameters in semipara-\n\nmetric generalized linear models for cluster correlated data. biometrika 77: 485\u1390497.\n\nroutledge, r. d. 1992. resolving the conflict over fisher\u2019s exact test. canad. j. statist. 20:\n\n201\u1390209.\n\nroutledge, r. d. 1994. practicing safe statistics with the mid-p*. canad. j. statist. 22: 103\u1390110.\nroy, s. n., and m. a. kastenbaum. 1956. on the hypothesis of no \u2018\u2018interaction\u2019\u2019 in a multiway\n\ncontingency table. ann. math. statist. 27: 749\u1390757.\n\nroy, s. n., and s. k. mitra. 1956. an introduction to some nonparametric generalizations of\n\nanalysis of variance and multivariate analysis. biometrika 43: 361\u1390376.\n\nrudas, t., c. c. clogg, and b. g. lindsay. 1994. a new index of fit based on mixture methods\n\nfor the analysis of contingency tables. j. roy. statist. soc. 56: 623\u1390639.\n\nryan, l. 1992. quantitative risk assessment for developmental toxicity. biometrics 48: 163\u1390174.\nryan, l. 1995. comment on article by liang and zeger. statist. sci. 10: 189\u1390193.\nsamuels, m. l. 1993. simpson\u2019s paradox and related phenomena. j. amer. statist. assoc. 88:\n\n81\u139088.\n\nsantner, t. j., and m. k. snell. 1980. small-sample confidence intervals for p \u1390p and p rp in\n\n1\n\n2\n\n1\n\n2\n\n2 = 2 contingency tables. j. amer. statist. assoc. 75: 386\u1390394.\n\nsantner, t. j., and s. yamagami. 1993. invariant small sample confidence intervals for the\n\ndifference of two success probabilities. commun. statist. ser. b 22: 33\u139059.\n\nschafer, j. l. 1997. analysis of incomplete multi\u00aeariate data. london: chapman & hall.\nschluchter, m. d., and k. l. jackson. 1989. log-linear analysis of censored survival data with\n\npartially observed covariates. j. amer. statist. assoc. 84: 42\u139052.\n\nscott, a., and c. wild. 2001. case\u1390control studies with complex sampling. appl. statist. 50:\n\n389\u1390401.\n\nseeber, g. 1998. poisson regression. pp. 3404\u13903412 in encyclopedia of biostatistics. chichester,\n\nuk: wiley.\n\nsekar, c. c., and w. e. deming. 1949. on a method of estimating birth and death rates and the\n\nextent of registration. j. amer. statist. assoc. 44: 101\u1390115.\n\nself, s. g., and k.-y. liang. 1987. asymptotic properties of maximum likelihood estimators and\nlikelihood ratio tests under nonstandard conditions. j. amer. statist. assoc. 82: 605\u1390610.\nsen, p. k., and j. m. singer. 1993. large sample methods in statistics: an introduction with\n\napplications. london: chapman & hall.\n\nshapiro, s. h. 1982. collapsing contingency tables: a geometric approach. amer. statist. 36:\n\n43\u139046.\n\nshuster, j., and d. downing. 1976. two-way contingency tables for complex sampling schemes.\n\nbiometrika 63: 271\u1390276.\n\nsilvapulle, m. j. 1981. on the existence of maximum likelihood estimators for the binomial\n\nresponse models. j. roy. statist. soc. ser. b 43: 310\u1390313.\n\n "}, {"Page_number": 699, "text": "684\n\nreferences\n\nsimon, g. 1973. additivity of information in exponential family probability laws.\n\n.\n\nj. amer.\n\nstatist. assoc. 68: 478\u1390482.\n\nsimon, g. 1974. alternative analyses for the singly-ordered contingency table. j. amer. statist.\n\nassoc. 69: 971\u1390976.\n\nsimon, g. 1978. efficacies of measures of association for ordinal contingency tables. j. amer.\n\nstatist. assoc. 73: 545\u1390551.\n\nsimonoff, j. 1983. a penalty function approach to smoothing large sparse contingency tables.\n\nann. statist. 11: 208\u1390218.\n\nsimonoff, j. 1986. jackknifing and bootstrapping goodness-of-fit statistics in sparse multinomials.\n\nj. amer. statist. assoc. 81: 1005\u13901111.\n\nsimonoff, j. s. 1996. smoothing methods in statistics. new york: springer-verlag.\nsimonoff, j. s. 1998. three sides of smoothing: categorical data smoothing, nonparametric\n\nregression, and density estimation. internat. statist. re\u00ae. 66: 137\u1390156.\n\nsimpson, e. h. 1949. the measurement of diversity. nature 163: 699.\nsimpson, e. h. 1951. the interpretation of interaction in contingency tables. j. roy. statist. soc.\n\nser. b 13: 238\u1390241.\n\nskellam, j. g. 1948. a probability distribution derived from the binomial distribution by\nregarding the probability of success as variable between the sets of trials. j. roy. statist.\nsoc. ser. b 10: 257\u1390261.\n\nskene, a. m., and j. c. wakefield. 1990. hierarchical models for multicentre binary response\n\nstudies. statist. medic. 9: 919\u1390929.\n\nslaton, t. l., w. w. piegorsch, and s. d. durham. 2000. estimation and testing with overdis-\npersed proportions using the beta-logistic regression model of heckman and willis.\nbiometrics 56: 125\u1390133.\n\nsmall, k. a. 1987. a discrete choice model for ordered alternatives. econometrica 55: 409\u1390424.\nsmith, k. w. 1976. table standardization and table shrinking: aids in the traditional analysis of\n\ncontingency tables. social forces 54: 669\u1390693.\n\nsmith, p. w. f., j. j. forster, and j. w. mcdonald. 1996. monte carlo exact tests for square\n\ncontingency tables. j. roy. statist. soc. ser. a 159: 309\u1390321.\n\nsnell, e. j. 1964. a scaling procedure for ordered categorical data. biometrics 20: 592\u1390607.\nsomers, r. h. 1962. a new asymmetric measure of association for ordinal variables. amer.\n\nsociol. re\u00ae. 27: 799\u1390811.\n\nspeed, t. 1998. iterative proportional fitting. pp. 2116\u13902119 in encyclopedia of biostatistics.\n\nchichester, uk: wiley.\n\nspiegelhalter, d. j., and a. f. m. smith. 1982. bayes factors for linear and log-linear models\n\nwith vague prior information. j. roy. statist. soc. ser. b 44: 377\u1390387.\n\nspitzer, r. l., j. cohen, j. l. fleiss, and j. endicott. 1967. quantification of agreement in\n\npsychiatric diagnosis. arch. gen. psychiatry 17: 83\u139087.\n\nsprott, d. a. 2000. statistical inference in science. new york: springer-verlag.\nstern, s. 1997. simulation-based estimation. j. econ. literature 35: 2006\u13902039.\nsterne, t. e. 1954. some remarks on confidence or fiducial limits. biometrika 41: 275\u1390278.\nstevens, s. s. 1951. mathematics, measurement, and psychophysics. pp. 1\u139049 in handbook of\n\nexperimental psychology, ed. s. s. stevens. new york: wiley.\n\nstevens, w. l. 1950. fiducial limits of the parameter of a discontinuous distribution. biometrika\n\n37: 117\u1390129.\n\nstigler, s. 1986. the history of statistics: the measurement of uncertainty before 1900. cambridge,\n\nma: harvard university press.\n\n "}, {"Page_number": 700, "text": "references\n\n685\n\nstigler, s. 1994. citation patterns in the journals of statistics and probability. statist. sci. 9:\n\n94\u1390108.\n\nstigler, s. 1999. statistics on the table. cambridge, ma: harvard university press.\nstiratelli, r., n. laird, and j. h. ware. 1984. random-effects models for serial observations with\n\nbinary response. biometrics 40: 1025\u13901035.\n\nstokes, m. e., c. s. davis, and g. g. koch. 2000. categorical data analysis using the sas system,\n\n2nd ed. cary, nc: sas institute.\n\nstrawderman, r. l., and m. t. wells. 1998. approximately exact inference for the common odds\n\nratio in several 2 = 2 tables. j. amer. statist. assoc. 93: 1294\u13901307.\n\nstuart, a. 1955. a test for homogeneity of the marginal distributions in a two-way classification.\n\nbiometrika 42: 412\u1390416.\n\nstukel, t. a. 1988. generalized logistic models. j. amer. statist. assoc. 83: 426\u1390431.\nsuissa, s., and j. j. shuster. 1984. are uniformly most powerful unbiased tests really best? amer.\n\nstatist. 38: 204\u1390206.\n\nsuissa, s., and j. j. shuster. 1985. exact unconditional samples sizes for the 2 by 2 binomial trial.\n\nj. roy. statist. soc. ser. a 148: 317\u1390327.\n\nsuissa, s., and j. j. shuster. 1991. the 2 = 2 matched-pairs trial: exact unconditional design and\n\nanalysis. biometrics 47: 361\u1390372.\n\nsundberg, r. 1975. some results about decomposable or markov-type models for multidimen-\nsional contingency tables: distribution of marginals and partitioning of tests. scand. j.\nstatist. 2: 71\u139079.\n\n\u017e\n\n.\n\ntango, t. 1998. equivalence test and confidence interval for the difference in proportions for\n\nthe paired-sample design. statist. medic. 17: 891\u1390908.\n\ntanner, m. a., and m. a. young. 1985. modelling agreement among raters. j. amer. statist.\n\nassoc. 80: 175\u1390180.\n\ntarone, r. e. 1985. on heterogeneity tests based on efficient scores. biometrika 72: 91\u139095.\ntarone, r. e., and j. j. gart. 1980. on the robustness of combined tests for trends in\n\nproportions. j. amer. statist. assoc. 75: 110\u1390116.\n\ntarone, r. e., j. j. gart, and w. w. hauck. 1983. on the asymptotic relative efficiency of\ncertain noniterative estimators of a common relative risk or odds ratio. biometrika 70:\n519\u1390522.\n\u00b4\ntables, and corrections to chi-squared statistics. biometrika 70: 139\u1390144.\n\ntavare, s., and p. m. e. altham. 1983. serial dependence of observations leading to contingency\n\nten have, t. r. 1996. a mixed effects model for multivariate ordinal response data including\n\ncorrelated discrete failure times with ordinal responses. biometrics 52: 473\u1390491.\n\nten have, t. r., and a. r. localio. 1999. empirical bayes estimation of random effects\n\nparameters in mixed effects logistic regression models. biometrics 55: 1022\u13901029.\n\nten have, t. r., and a. morabia. 1999. mixed effects models with bivariate and univariate\nassociation parameters for longitudinal bivariate binary response data. biometrics 55:\n85\u139093.\n\nten have, t. r., and d. h. uttal. 1994. subject-specific and population-averaged continuation\n\nratio logit models for multiple discrete time survival profiles. appl. statist. 43: 371\u1390384.\n\ntheil, h. 1969. a multinomial extension of the linear logit model. internat. econ. re\u00ae. 10:\n\n251\u1390259.\n\ntheil, h. 1970. on the estimation of relationships involving qualitative variables. amer. j.\n\nsociol. 76: 103\u1390154.\n\nthompson, r., and r. j. baker. 1981. composite link functions in generalized linear models.\n\nappl. statist. 30: 125\u1390131.\n\n "}, {"Page_number": 701, "text": "686\n\nreferences\n\nthompson, w. a. 1977. on the treatment of grouped observations in life studies. biometrics 33:\n\n463\u1390470.\n\nthurstone, l. l. 1927. the method of paired comparisons for social values. j. abnormal social\n\npsych. 21: 384\u1390400.\n\ntjur, t. 1982. a connection between rasch\u2019s item analysis model and a multiplicative poisson\n\nmodel. scand. j. statist. 9: 23\u139030.\n\ntocher, k. d. 1950. extension of the neyman\u1390pearson theory of tests to discontinuous variates.\n\nbiometrika 37: 130\u1390144.\n\ntoledano, a., and c. gatsonis. 1996. ordinal regression methodology for roc curves derived\n\nfrom correlated data. statist. medic. 15: 1807\u13901826.\n\ntrain, k. 1986. qualitati\u00aee choice analysis: theory, econometrics, and an application. cambridge,\n\nma: mit press.\n\ntsiatis, a. a. 1980. a note on the goodness-of-fit test for the logistic regression model.\n\nbiometrika 67: 250\u1390251.\n\ntutz, g. 1989. compound regression models for ordered categorical data. biometrical j. 31:\n\n259\u1390272.\n\ntutz, g. 1991. sequential models in categorical regression. comput. statist. data anal. 11:\n\n275\u1390295.\n\ntutz, g., and w. hennevogl. 1996. random effects in ordinal regression models. comput. statist.\n\ndata anal. 22: 537\u1390557.\n\nuebersax, j. s. 1993. statistical modeling of expert ratings on medical treatment appropriate-\n\nness. j. amer. statist. assoc. 88: 421\u1390427.\n\nuebersax, j. s., and w. m. grove. 1990. latent class analysis of diagnostic agreement. statist.\n\nmedic. 9: 559\u1390572.\n\nuebersax, j. s., and w. m. grove. 1993. a latent trait finite mixture model for the analysis of\n\nrating agreement. biometrics 49: 823\u1390835.\n\nvan der heijden, p. g. m., and j. de leeuw. 1985. correspondence analysis: a complement to\n\nlog-linear analysis. psychometrika 50: 429\u1390447.\n\nvan der heijden, p. g. m., a. de falguerolles, and j. de leeuw. 1989. a combined approach to\ncontingency table analysis using correspondence analysis and log-linear analysis. appl.\nstatist. 38: 249\u1390292.\n\nverbeke, g., and e. lesaffre. 1996. a linear mixed-effects model with heterogeneity in the\n\nrandom-effects population. j. amer. statist. assoc. 91: 217\u1390221.\n\nverbeke, g., and g. molenberghs. 2000. linear mixed models for longitudinal data. new york:\n\nspringer-verlag.\n\nwald, a. 1943. tests of statistical hypotheses concerning several parameters when the number of\n\nobservations is large. trans. amer. math. soc. 54: 426\u1390482.\n\nwalker, s. h., and d. b. duncan. 1967. estimation of the probability of an event as a function of\n\nseveral independent variables. biometrika 54: 167\u1390179.\n\nwalley, p. 1996. inferences from multinomial data: learning about a bag of marbles. j. roy.\n\nstatist. soc. ser. b 58: 3\u139034.\n\nwardrop, r. l. 1995. simpson\u2019s paradox and the hot hand in basketball. amer. statist. 49:\n\n24\u139028.\n\nware, j. h., s. lipsitz, and f. e. speizer. 1988. issues in the analysis of repeated categorical\n\noutcomes. statist. medic. 7: 95\u1390107.\n\nwatson, g. s. 1956. missing and \u2018\u2018mixed up\u2019\u2019 frequencies in contingency tables. biometrics 12:\n\n47\u139050.\n\nwatson, g. s. 1959. some recent results in chi-square goodness-of-fit tests. biometrics 15:\n\n440\u1390468.\n\n "}, {"Page_number": 702, "text": "references\n\n687\n\nwedderburn, r. w. m. 1974. quasi-likelihood functions, generalized linear models, and the\n\ngauss\u1390newton method. biometrika 61: 439\u1390447.\n\nwedderburn, r. w. m. 1976. on the existence and uniqueness of the maximum likelihood\n\nestimates for certain generalized linear models. biometrika 63: 27\u139032.\n\nwermuth, n. 1976. model search among multiplicative models. biometrics 32: 253\u1390263.\nwermuth, n. 1987. parametric collapsibility and the lack of moderating effects in contingency\n\ntables with a dichotomous response variable. j. roy. statist. soc. ser. b 49: 353\u1390364.\n\nwestfall, p. h., and r. d. wolfinger. 1997. multiple tests with discrete distributions. amer.\n\nstatist. 51: 3\u13908.\n\nwestfall, p. h., and s. s. young. 1993. resampling-based multiple testing: examples and methods\n\nfor p-value adjustment. new york: wiley.\n\nwhite, h. 1982. maximum likelihood estimation of misspecified models. econometrica 50: 1\u139026.\nwhite, a. a., j. r. landis, and m. m. cooper. 1982. a note on the equivalence of several\n\nmarginal homogeneity test criteria for categorical data. internat. statist. re\u00ae. 50: 27\u139034.\n\nwhitehead, j. 1993. sample size calculations for ordered categorical data. statist. medic. 12:\n\n2257\u13902271.\n\nwhittaker, j. 1990. graphical models in applied multi\u00aeariate statistics. new york: wiley.\nwhittaker, j., and m. aitkin. 1978. a flexible strategy for fitting complex log-linear models.\n\nbiometrics 34: 487\u1390495.\n\nwhittemore, a. s. 1978. collapsibility of multidimensional tables. j. roy. statist. soc. ser. b 40:\n\n328\u1390340.\n\nwhittemore, a. s. 1981. sample size for logistic regression with small response probability.\n\nj. amer. statist. assoc. 76: 27\u139032.\n\nwilks, s. s. 1935. the likelihood test of independence in contingency tables. ann. math. statist.\n\n6: 190\u1390196.\n\nwilks, s. s. 1938. the large-sample distribution of the likelihood ratio for testing composite\n\nhypotheses. ann. math. statist. 9: 60\u139062.\n\nwilliams, d. a. 1975. the analysis of binary responses from toxicological experiments involving\n\nreproduction and teratogenicity. biometrics 31: 949\u1390952.\n\nwilliams, d. a. 1982. extra-binomial variation in logistic linear models. appl. statist. 31:\n\n144\u1390148.\n\nwilliams, d. a. 1987. generalized linear model diagnostics using the deviance and single-case\n\ndeletions. appl. statist. 36: 181\u1390191.\n\nwilliams, d. a. 1988. comments on \u2018\u2018the impact of litter effects on dose\u1390response modeling in\n\nteratology.\u2019\u2019 biometrics 44: 305\u1390308.\n\nwilliams, e. j. 1952. use of scores for the analysis of association in contingency tables.\n\nbiometrika 39: 274\u1390289.\n\nwilliams, o. d., and j. e. grizzle. 1972. analysis for contingency tables having ordered response\n\ncategories. j. amer. statist. assoc. 67: 55\u139063.\n\nwilson, e. b. 1927. probable inference, the law of succession, and statistical inference. j. amer.\n\nstatist. assoc. 22: 209\u1390212.\n\nwolfinger, r., and m. o\u2019connell. 1993. generalized linear mixed models: a pseudo-likelihood\n\napproach. j. statist. comput. simul. 48: 233\u1390243.\n\nwong, g. y., and w. m. mason. 1985. the hierarchical logistic regression model for multilevel\n\nanalysis. j. amer. statist. assoc. 80: 513\u1390524.\n\nwoolf, b. 1955. on estimating the relation between blood group and disease. ann. human\n\ngenet. london 19: 251\u1390253.\n\n\u017e\n\n.\n\nwoolson, r. f., and w. r. clarke. 1984. analysis of categorical incomplete longitudinal data.\n\nj. roy. statist. soc. ser. a 147: 87\u139099.\n\n "}, {"Page_number": 703, "text": "688\n\nreferences\n\nwu, c. f. j. 1985. efficient sequential designs with binary data. j. amer. statist. soc. 80:\n\n974\u1390984.\n\nyang, i., and m. p. becker. 1997. latent variable modeling of diagnostic accuracy. biometrics 53:\n\n948\u1390958.\n\nyates, f. 1934. contingency tables involving small numbers and the \u24392 test. j. roy. statist. soc.\n\nsuppl. 1: 217\u1390235.\n\nyates, f. 1948. the analysis of contingency tables with grouping based on quantitative charac-\n\nters. biometrika 35: 176\u1390181.\n\nyates, f. 1984. tests of significance for 2 = 2 contingency tables. j. roy. statist. soc. ser. a\n\n147: 426\u1390463.\n\nyee, t. w., and c. j. wild. 1996. vector generalized additive models. j. roy. statist. soc. ser. b\n\n58: 481\u1390493.\n\nyerushalmy, j. 1947. statistical problems in assessing methods of medical diagnosis, with special\n\nreference to x-ray techniques. public health rep. 62: 1432\u13901449.\n\nyule, g. u. 1900. on the association of attributes in statistics. philos. trans. roy. soc. london\n\nser. a 194: 257\u1390319.\n\nyule, g. u. 1903. notes on the theory of association of attributes in statistics. biometrika 2:\n\n121\u1390134.\n\nyule, g. u. 1906. on a property which holds good for all groupings of a normal distribution of\nfrequency for two variables, with application to the study of contingency tables for the\ninheritance of unmeasured qualities. proc. roy. soc. ser a 77: 324\u1390336.\n\nyule, g. u. 1912. on the methods of measuring association between two attributes. j. roy.\n\nstatist. soc. 75: 579\u1390642.\n\nzeger, s. l., and m. r. karim. 1991. generalized linear models with random effects: a gibbs\n\nsampling approach. j. amer. statist. assoc. 86: 79\u139086\n\nzeger, s. l., k.-y. liang, and p. s. albert. 1988. models for longitudinal data: a generalized\n\nestimating equation approach. biometrics 44: 1049\u13901060.\n\nzelen, m. 1971. the analysis of several 2 = 2 contingency tables. biometrika 58: 129\u1390137.\nzelen, m. 1991. multinomial response models. comput. statist. data anal. 12: 249\u1390254.\nzellner, a., and p. e. rossi. 1984. bayesian analysis of dichotomous quantal response models.\n\nj. economet. 25: 365\u1390393.\n\nzelterman. d. 1987. goodness-of-fit tests for large sparse multinomial distributions. j. amer.\n\nstatist. soc. 82: 624\u1390629.\n\nzermelo, e. 1929. die berechnung der turnier-ergebnisse als ein maximumproblem der\n\nwahrscheinlichkeitsrechnung. math. z. 29: 436\u1390460.\n\nzhang, h., j. crowley, h. sox, and r. olshen. 1998. tree-structured statistical methods.\n\npp. 4561\u13904573 in encyclopedia of biostatistics. chichester, uk: wiley.\n\nzheng, b., and a. agresti. 2000. summarizing the predictive power of a generalized linear\n\nmodel. statist. medic. 19: 1771\u13901781.\n\nzhu, y., and n. reid. 1994. information, ancillarity, and sufficiency in the presence of nuisance\n\nparameters. canad. j. statist. 22: 111\u1390123.\n\n "}, {"Page_number": 704, "text": "examples index\n\nabortion and education, 345\nabortion opinions, 29, 205\u1390206, 441, 486,\n\n504\u1390506, 553\n\nadmissions into berkeley, 62\u139063\nadmissions into florida, 223\u1390224, 529\nafterlife, belief in, 302\u1390303\naids and azt use, 184\u1390187\naids, measures to deal with, 347\nair pollution and breathing, 377\u1390378\nalcohol, cigarettes, and marijuana use,\n\n322\u1390326, 361\u1390363, 367, 482\u1390483, 528\n\nalcohol consumption and malformation,\n\n89\u139090, 158, 179\u1390180, 182\n\nalcohol and driving, 203\nalligator food choice, 268\u1390274, 304\nalzheimer\u2019s disease and cognitive impairment,\n\n310\n\naspirations by income, 107,\naspirin and heart attacks, 37, 46, 71\u139072\nautomobile collisions and seat belts, 40\u139041,\n\n61, 305\u1390306, 327\u1390329, 331, 349, 361\n\nbaseball complete games, 157\u1390158\nbaseball standings, 437\u1390438\nbeetle mortality, 247\u1390250\nbirth control, teenage, 352\nblood pressure and heart disease, 221\u1390223\nbreast cancer, 38, 105, 107\nbreathing test and smoking, 307, 377\u1390378\nbreathlessness, wheeze, and age, 378\nbuchanan vote in palm beach county,\n\n156\u1390157\n\nbusing and race, 348\n\ncalves and pneumonia, 25\u139026, 34\ncancer of larynx and radiation therapy, 107\ncancer remission, 197\u1390199, 261\ncapture\u1390recapture, hepatitis, 533\n\ncapture\u1390recapture of snowshoe hares,\n\n511\u1390513, 544\u1390545, 551\u1390552\n\ncarcinoma of uterine cervix, 431\u1390435, 532,\n\n541\u1390544, 549\u1390551\n\nchlorophyll inheritance, 29\ncholesterol and cereal, 309\nclaritin, 109\nclinical trials, 230\u1390236, 507\u1390510\ncoffee drinking, 446\ncola drink taste test, 448\ncondoms and adolescents, 202\ncoronary deaths and smoking, 404\ncredit card and income italy , 206\ncrime and race, 63\ncrossover drug trial, 457, 483\u1390484\n\n\u017e\n\n.\n\ndeath penalty and race, 48\u139052, 63, 65, 201\ndepression, mental, 459\u1390461, 468\u1390469,\n\n506\u1390507\n\ndevelopmental toxicity study, 290\u1390291,\n\n517\u1390521\n\ndiabetes, case-control study, 418\u1390419\ndiagnostic tests, 60, 66\ndiarrhea, 255\ndraft position in sports, 207\ndumping severity, 308\u1390309\ndysmenorrhea, 483\u1390484, 572\n\nesophageal cancer, 203\n\nfish egg hatching, 568\u1390569\nfree throws, 105, 160\u1390161\n\ngambler\u2019s ruin, 489\u1390490\ngenetics, 165\ngovernment spending, 349\u1390351, 449, 530\u1390531\ngraduate admissions at florida, 223\u1390224, 529\ngraduate admissions at berkeley, 62\u139063\n\n689\n\n "}, {"Page_number": 705, "text": "690\n\ngraham greene, 28\ngun-related deaths, 61\n\nheart attacks and aspirin use, 37, 46\nheart catheterization and race, 62\nheart disease and blood pressure, 221\u1390223\nheart disease and snoring, 121\u1390123\nheart valve replacement and survival, 385\u1390387\nhepatitis outbreaks, 533\nhome team advantage in baseball, 437\u1390438\nhomicide victims, number, 561\u1390563, 564\u1390565,\n\n571\n\nhorseshoe crab mating, 126\u1390131, 154\u1390155,\n\n159, 168\u1390170, 173\u1390176, 188\u1390192, 212\u1390216,\n570\n\nincome by year, 308\ninfant survival, gestation, smoking, and age,\n\n400\u1390401\n\ninsomnia, 462\u1390464, 469, 487, 514\u1390515, 531\n\nexamples index\n\nmultiple sclerosis and neurologist ratings, 447\nmurder rates in u.s., 62, 63\nmyocardial infarction and aspirin, 37, 46,\n\n71\u139072\n\nmyocardial infarction and diabetes, 418\u1390419\n\nncaa graduation rates, 202\nnervousness and claritin, 109\n\nobesity, occasion and gender, 487\noccupational aspirations, 206\noccupational status, father and son, 447\noral contraceptive use, 200\nosteosarcoma, 262\u1390263\n\npalm beach county vote for buchanan,\n\n156\u1390157\n\nparty identification by race and by gender,\n\n105\u1390106, 303\n\nparty identification and protestors, 307\npathologists ratings of carcinoma, 431\u1390435,\n\njob satisfaction and income, 57\u139059, 87\u139088,\n\n532, 541\u1390544, 549\u1390551\n\n287\u1390288, 295, 297, 308\n\njob satisfaction and race, gender, age, and\n\nlocation, 205\n\njournal citations, 448\n\npenicillin and rabbits, 259\u1390260\npig farmer survey, 484\u1390485\npneumonia infections, 25\u139026, 34\npoison dose for protozoa, 546\u1390547\npolitical ideology and party affiliation, 305,\n\nkyphosis and spinal surgery, 199\u1390200\n\n375\u1390377\n\nlabelling index and remission, 197\u1390199, 261\nlarry bird free throws, 105\nleading crowd, 516\u1390517, 532\nleprosy, 239\nlife table, 284\nlung cancer and chemotherapy, 306,\nlung cancer and smoking, 42, 61, 62, 64\nlung cancer survival, 390\u1390391\n\npregnancy rates, 567\npresidential approval rating, 409\u1390412\npresidential vote, by state, 503\u1390504, 534\npromotion discrimination, 254\u1390255\nprussian army and mule kicks, 30\npsychiatric patients and prescribed drugs,\n\n106\u1390107\n\nreligious fundamentalism and education, 80,\n\n81\u139082\n\nmalformation of infants, 89\u139090, 158, 179\u1390180,\n\nreligious services, frequency of attendance,\n\n182\n\n352\n\nmendel\u2019s theories, 22\u139023\nmental health, and parents ses, 381, 383\u1390384\nmental impairment, life events and ses,\n\nrespiratory illness, age and maternal smoking,\n\n480\u1390481\n\nrespiratory illness in children, 478\u1390479\n\n279\u1390282\n\nmigration, 423, 427\u1390428\nmissing people in london, 202\nmixture for two protozoan genuses, 546\nmotor vehicle accident rates, 403\nmovie reviewers, 445\u1390446\nmulticenter clinical trial, infection cream,\n\n230\u1390235, 508\u1390510\n\nsatisfaction with housing, 310\nsatisfaction with job, 205\nschizophrenia origin, 83\u139084\nseat belts and injury, 40\u139041, 61, 305\u1390306,\n\n327\u1390329, 331, 349, 361\nsex, frequency of, 569\u1390570\nsex opinions, 65, 217\u1390219, 368, 371\u1390373, 421,\n\nmulticenter clinical trial, fungal infections,\n\n430, 431, 530\n\n394\u1390395, 530\n\nsexual intercourse, gender and race, 201\n\n "}, {"Page_number": 706, "text": "examples index\n\n691\n\nshopping choice, 300\nsnowshoe hares, 511\u1390513, 544\u1390545, 551\u1390552\nsoccer and arrests, 403\nsore throat in surgery, 204\nspace shuttle, 199\n.\nstudent survey alcohol, marijuana, cigarettes ,\n\n\u017e\n\n322\u1390326, 361\u1390363, 367, 482\u1390483, 528\n\nteratology studies, 151\u1390153\ntitanic, 61\ntoxicity study, 517\u1390520\ntrain accidents, 403, 569\n\nufos, 106\n\ntea drinker, 92, 100\nteenage birth control, 368, 371\u1390373\ntennis rankings, 449\n\nvegetarianism, 16\u139017, 29\nveterinary information sources, 484\u1390485\nvoting, proportion by state, 503\u1390504, 534\n\n "}, {"Page_number": 707, "text": " "}, {"Page_number": 708, "text": "author index\n\nadelbasit, k. m., 196\nagresti, a., 27, 32, 33, 60, 100, 101, 102, 104,\n111, 156, 227, 255, 258, 266, 298, 301, 379,\n384, 397, 399, 422, 426, 435, 443, 445, 453,\n465, 481, 485, 491, 502, 511, 513, 517, 518,\n526, 527, 533, 536, 551, 552, 565, 567, 596,\n630, 651\n\naitchison, j., 265, 301, 465, 561, 613, 625\naitken, c. g. g., 613\naitkin m., 155, 388, 398, 399, 495, 520, 526,\n\n545, 565, 633\n\nalbert, a., 195, 197\nalbert, j. h., 607, 609\nalbert, p. s., 688\nallison, p. d., 633, 643\naltham, p. m. e., xv, 59, 103, 104, 240, 442,\n\n443, 555, 566, 573, 587, 608\n\namemiya, t., 227, 258, 300\nandersen, e. b., 255, 399, 450, 496, 526, 576,\n\n631\n\nanderson, c. j., 398, 399\nanderson, d. a., 520, 526\nanderson, d. r., 216\nanderson, j. a., 171, 195, 196, 197, 207, 277\nanderson, r. l., 631\nanderson, t. w., 478, 482, 490\naranda-ordaz, f. j., 250, 399\narminger, g., 549\narmitage, p., 104, 181\nashford, j. r., 258, 379\nasmussen, s., 360\nazzalini, a., 480\n\nbaglivo, j., 346\nbaker, r. j., 283\nbaker, s. g., 393, 482, 541\nbanerjee, c., 443\nbaptista, j., 100\nbarnard, g. a., 95, 104, 114\nbarndorff-nielsen, o. e., 266\nbartholomew, d. j., 526, 565\n\nbartlett, m. s., 265, 623\u1390624, 631\nbecker, m., 370, 399, 435, 443, 544, 644\nbedrick, e. j., 77, 103\nbegg, c. b., 273\nbeitler, p. j., 508\nbenedetti, j. k., 102, 398\nbenichou, j., 66\nbenzecri, j. p., 399, 624\nberger, r., 18, 33, 95, 594\nbergsma, w. p., 481\nberkson, j., 80, 104, 166, 197, 612, 624, 631\nberry, g., 104\nberry, s. m., 207\nbest, d. j., 103\nbhapkar, v. p., 27, 103, 104, 291, 422, 453,\n\n\u00b4\n\n488, 612, 615, 616, 629\n\nbickel, p., 63\nbiggeri, a., 566\nbillingsley, p., 482\nbirch, m. w., 255, 263, 295, 298, 336, 339, 340,\n\n341, 346, 369, 392, 576, 585, 627, 631\n\nbishop, y. m. m., 347, 360, 366, 452, 482, 526,\n\n576, 582, 587, 591, 594, 627, 629, 631\n\nblaker, h., 20, 27, 93, 635\nbliss, c., 246, 247, 560, 623\nblyth, c. r., 20, 27, 32, 59\nbock, r. d., 300, 301, 495, 526, 624, 625\n\u00a8\nbockenholt, u., 398, 399, 443\nbonney, g. e., 479, 480\nboos, d. d., 95, 467, 481\nbooth, j., xv, 104, 223, 397, 443, 523, 525, 567,\n\n630\n\nbowker, a. h., 424\nbox, j. f., 23, 92, 623, 624\nbradley, r. a., 302, 436, 443\nbreslow, n., 51, 59, 155, 156, 171, 234, 235,\n\n255, 258, 399, 419, 493, 523, 524, 563, 625,\n631\n\nbrier, s. s., 515\nbrooks, s. p., 566\nbross, i. d. j., 111\n\n693\n\n "}, {"Page_number": 709, "text": "694\n\nbrown, l. d., 15, 27, 33, 606\nbrown, m. b., 102, 398\nbrown, p. j., 613, 614\nbrownstone, d., 302, 527\nbull, s. b., 196\nburnham, k. p., 216, 526, 552\nburridge, j., 283\nbutler, r., 104, 397, 443, 630\nbyar, d. p., 232, 295, 414, 481, 625\n\ncaffo, b., xv, 102, 656\ncameron, a. c., 131, 155, 561, 566, 574\ncarey, v., 474\ncarroll, r. j., 171, 467\ncasella, g., 18, 33, 594\ncatalano, p. j., 527\ncaussinus, h., 425, 427, 428, 443, 451, 627, 631\nchaloner, k., 196, 609\nchamberlain, g., 419, 420, 526\nchambers, e. a., 258\nchambers, j. m., 633\nchambers, r. l., 527\nchan, i., 104\nchan, j. s. k., 523\nchao, a., 513, 526, 533\nchapman, d. g., 258\nchen, z., 527, 643, 651\nchib, s., 609\nchristensen, r., 196\nchuang, c., 399, 462\nclayton d. g., 388, 399, 493, 523, 563, 625,\n\n631\n\nclogg, c. c., 103, 391, 399, 565, 627\nclopper, c. j., 18\ncochran w. g., 27, 80, 88, 163, 181, 232, 239,\n\n396, 459, 488, 596, 626, 627, 631\n\ncoe, p. r., 101\ncohen, a., 103, 104\ncohen, j., 434, 435, 443\ncoleman, j. s., 516, 532\ncollett, d., 196, 204\nconaway, m. r., 426, 482, 526, 565\ncook, r. d., 225\ncopas, j. b., 156, 257, 442, 616\ncorcoran, c., 197, 573\ncormack, r. m., 511, 526, 551\ncornfield, j., 42, 47, 51, 71, 77, 99, 100, 171,\n\n196, 208, 221, 624\n\ncoull, b. a., xv, 27, 32, 33, 513, 518, 526, 533,\n\n552, 567, 655, 662\n\ncox, c., 266, 282, 286, 576, 587, 641\ncox, d. r., 12, 104, 133, 138, 196, 197, 258,\n\n415, 482, 493, 497, 624, 625, 631\n\u00b4\ncramer, h., 112, 576, 587, 625\u1390626\n\nauthor index\n\ncressie, n., 27, 112, 258, 396, 612\ncroon, m., 481\ncrouchley, r., 527\ncrowder, m. j., 555, 566\n\nd\u2019agostino, r. b., jr., 196\ndalal, s. r., 199\ndaniels, m. j., 524, 609\ndardanoni, v., 301\ndarroch, j. n., 347, 357, 398, 414, 426, 443,\n\n459, 481, 513, 526, 551, 552, 565, 626, 629,\n631\n\n\u00b4\n\ndas gupta, s., 237\ndavid, h. a., 443\ndavis, l. j., 59, 93, 398\ndavison, a. c., 156, 594\ndawson, r. b., jr., 103\ndawson, r. j. m., 61\nday, n. e., 51, 171, 232, 235, 258, 399, 625\nde falguerolles, a., 399, 664, 686\nde leeuw, j., 399\ndemetrio, c. g. b., 156, 555, 566\ndeming, w. e., 343, 347, 511\ndempster, a. p., 522\ndey, d. k., 609\ndiaconis, p., 103, 104\ndiggle, p., 471, 625\ndillon, w., 443\ndittrich, r., xv, 443\ndobson, a. j., 155\ndoll, r., 42, 62, 64, 404\ndong, j., xv, 59, 614, 616\ndonner, a., 172, 196, 258\ndoolittle, m. h., 621\ndowning, d., 103\ndrost, f. c., 112, 258, 595\nducharme, g. r., 398\nduncan, d. b., 195, 197, 277, 301, 624\ndupont, w. d., 93\ndyke, g. v., 624\n\nedwardes, m. d., 301\nedwards, a. w. f., 59\nedwards, d., 360, 398\nefron, b., 103, 146, 196, 227, 258, 526, 605,\n\n610\n\nekholm, a., 156, 481\neliason, s. r., 103, 391\nescoufier, y., 383, 399\nespeland, m. a., 544, 571\neveritt, b. s., 633\n\nfahrmeir, l., 155, 300, 615\nfarewell, v. t., 171, 301, 527, 625\n\n "}, {"Page_number": 710, "text": "author index\n\n695\n\nfay, r., 103, 482, 594\nfechner, g., 623\nferguson, t. s., 605, 617\nfienberg, s. e., 344, 347, 392, 438, 443, 513,\n\n526, 610, 615, 616, 626, 627, 629, 631\n\nfinney, d., 151, 258, 556, 623\nfirth, d., 155, 156, 196, 330, 467, 481, 482\nfischer, g. h., 526\nfisher, r. a., 12, 22, 23, 29, 51, 79, 91, 92, 95,\n99, 104, 114, 146, 156, 162, 237, 247, 560,\n576, 589, 622\u1390624, 625, 626, 628, 631\n\nfitzmaurice, g. m., 103, 466, 474, 481, 482,\n\n649\n\nfitzpatrick, s., 35\nfleiss, j. l., 104, 110, 111, 242, 258, 347, 435,\n\n436, 443\n\nfollman, d. a., 546, 547, 548, 566\nforcina, a., 301\nforster, j. j., 104, 346, 397, 482, 616, 630\nforthofer, r. n., 378\nfowlkes, e. b., 199, 226, 257\nfrancom, s., 462\nfreedman, d., 23, 63\nfreeman, d. h. jr., 399\nfreeman, g. h., 97\nfreeman, m. f., 112\nfreidlin, b., 104\nfriendly, m., 59, 399, 633\nfrome, e. l., 155, 399\nfuchs, c., 482\n\ngabriel, k. r., 263, 399\ngaddum, j. h., 623\ngail, m. h., 104, 625\ngart, j. j., 70, 71, 77, 102, 104, 197, 255, 258,\n\n397, 442\n\ngaskins, r. a., 614\ngastwirth, j., 104, 197\ngatsonis, c., xv, 230, 481, 524, 609\ngelfand, a. e., 609\ngenter, f. c., 301\ngeyer, c., 678\nghosh, b. k., 27\nghosh, m., 442, 609\ngibbons, r. d., 520\ngilbert, g. n., 217\ngill, j., 155\ngilmour, a. r., 526\ngilula, z., 83, 382, 384\ngini, c., 329\nglass, p. v., 447\ngleser, l. j., 103\nglonek, g. f. v., 393, 466\ngodambe, v. p., 104, 482\n\ngoetghebeur, e., 482\ngokhale, d. v., 112, 612, 616\ngoldstein, h., 520, 524\ngood, i. j., 24, 60, 104, 605, 607, 608, 612, 614,\n\n616, 626, 630\n\ngoodman l. a., 35, 59, 68, 69, 83, 84, 102,\n\n110, 213, 217, 228, 340, 346, 365, 366, 369,\n370, 374, 379, 380, 381, 382, 383, 384, 397,\n398, 399, 406, 407, 408, 425, 428, 431, 443,\n478, 482, 490, 516, 527, 540, 565, 566, 572,\n621, 622, 627, 628, 629, 631\n\ngould, s. j., 544\ngourieroux, c., 467, 482\ngraubard, b. i., 89, 103\ngray, r., 273\ngreen, p. j., 156\ngreenacre, m. j., 384, 399\ngreene, g., 28\ngreenland, s., 96, 234, 258\ngreenwood, m., 566\ngreenwood, p. e., 27\ngrego, j., 686\ngrizzle, j. e., 291, 301, 457, 601, 615, 624, 629,\n\n631\n\ngross, s. t., 197\ngrove, w. m., 544\ngueorguieva, r., xv, 527, 670\ngupta, a. k., 607\n\nhaber, m., 95, 96, 103, 291, 465\nhaberman, s. j., 69, 81, 83, 113, 195, 224, 258,\n268, 300, 349, 346, 347, 364, 367, 369, 374,\n380, 382, 392, 393, 396, 399, 408, 440, 526,\n540, 565, 572, 576, 589, 591, 592, 595, 627,\n629, 631\n\nhagenaars, j., 565\nhald, a., 623\nhaldane, j. b. s., 70, 103, 196\nhall, p., 616\nhalton, j. h., 97\nhamada, m., 301\nhandelman, s. l., 544, 571\nhanfelt, j., 566, 571\nhansen, l. p., 467, 482\nharkness, w. l., 258\nharrell, f. e., 229, 282, 301\nhartzel, j., 511, 513, 514, 516, 534, 651\nhaslett, s., 394\nhastie, t., 153, 199, 301, 633\nhatzinger, r., 565\nhauck, w. w., 172, 234, 258\nhaynam, g. f., 243\nheagerty, p., 481, 527, 548\nhedeker, d., 520, 653\n\n "}, {"Page_number": 711, "text": "696\n\nauthor index\n\nheinen, t., 565\nhennevogl, w., 513\nhenry, n. w., 565\nheyde, c. c., 156, 481\nhill, a. b., 42, 64, 111, 404\nhinde, j., 155, 156, 555, 563, 566\nhinkley, d., 12, 104, 133, 138, 146, 156, 594\nhirji, k. f., 104, 258, 625\nhirotsu, c., 406\nhirschfeld, h., 399\nhoadley, b., 199\nhobert, j., xv, 523, 525, 630\nhodges, j. l., 197\nhoem, j. m., 347, 399\nholford, t. r., 389, 390, 399\nholland, p. w., 610, 615, 629, 631\nhollander, m., 443\nholt, d., 103\nholtbrugge, 306\nhook, e. b., 526\nhosmer, d. w., 177, 196, 197, 257, 258\nhout, m., 65, 428, 443\nhoward, j. v., 104, 608\nhsieh, f. y., 242, 243\nhsu, j. s. j., 608\nhwang, j. t. g., 104\n\n\u00a8\n\nimrey, p. b., 346, 443, 615\nireland, c. t., 616\nirwin, j., 91\n\njennison, c., 103\njewell, n. p., 467, 496, 566\njohnson, b. m., 605\njohnson, n. l., 566, 574\njohnson, w., 257\njones, b., 442, 484, 536\njones, m. p., 258\n\u2c91\njorgensen, b., 136, 155, 156, 266, 470\n\nkalbfleisch, j. d., 482\nkarim, m. r., 524, 609\nkastenbaum, m. a., 627\nkastner, c., 466, 649\nkatzenbeisser, w., xv, 672\nkauerman, g., 156, 467\nkelderman, h., 565\nkempthorne, o., 96, 104\nkendall m. g., 27, 56, 60, 68, 399, 631\nkenward, m. g., 442, 475, 484, 536\nkhamis, h. j., xv, 332, 443\nkim, d., 104, 255, 298, 379, 397\nking, g., 527\nknott, m., 565\n\nknuiman, m. w., 616\nkoch, g. g., 27, 302, 436, 447, 459, 460, 481,\n532, 601, 615, 616, 629, 631, 670, 673, 674,\n675\n\nkoehler, k., 27, 103, 396, 397\nkoopman, p. a. r., 77\nkorn, e. l., 89, 103\nkraemer, h. c., 443\nkreiner, s., xv, 358, 398\nkruskal, w. h., 59, 60, 68, 69, 102, 110, 621,\n\n631\n\nku, h. h., 616\nkuha, j., 330, 347\nkuk, a. y. c., 523\nkullback, s., 112, 399, 612, 616\nkuo, l., 527, 643, 651\nkupper, l. l., 566\n\n\u00a8\u00a8 \u00a8\nlaara, f., 301, 313\nlachin, j., 258\nlaird, n. m., 385, 386, 389, 466, 482, 522, 541,\n\n609, 610, 649\n\nlambert, d., 546, 547, 548, 566\nlancaster, h., 20, 27, 83, 84, 113, 399, 626\nlandis, j. r., 111, 295, 297, 301, 302, 436, 447,\n\n462, 508, 532\nlandrum, m., 609\nlandwehr, j. m., 226, 257\nlang, j. b., xv, 301, 340, 399, 465, 481, 537,\n\n541, 551, 643, 644, 649, 655, 675, 678\n\nlaplace, p. s., 15\nlarntz, k., 196, 396, 397, 438, 443, 609\nlarsen, k., 498\nlarson, m. g., 399\nlauritzen, s. l., 346, 398, 399\nlavange, l. m., 103, 197, 481\nlawal, h. b., 396\nlawless, j. f., 155, 482, 560, 561, 566\nlazarsfeld, p. f., 565\nlee, e., 198\nlee, s. k., 596\nlee, y., 559, 566, 574\nlefkopoulou, m., 566\nlehmann, e., 67, 104, 263, 406\nlehnen, r. g., 378\nlemeshow, s., 177, 196, 197, 257, 258\nleonard, t., 608, 609\nlesaffre, e., 258, 300, 466, 522, 545\nlesperance, m. l., 526, 548\nliang, k.-y., 104, 258, 442, 467, 469, 471, 473,\n481, 482, 525, 556, 566, 571, 573, 625, 631\n\nlin, x., 524, 525\nlindley, d. v., 609, 630\nlindsay, b., 494, 545, 549\n\n "}, {"Page_number": 712, "text": "author index\n\n697\n\nlindsey, j. k., 400, 467, 566, 573\nlipsitz, s., 103, 291, 422, 456, 469, 473, 474,\n\n481, 645\n\nlittle, r. j., 114, 346, 347, 475, 476, 482\nliu, i., xv, 485, 655, 671\nliu, q., 510, 522\nlloyd, c., 93, 104, 156, 615\nlocalio, a. r., 526\nlongford, n. t., 520\nloughin, t., 484\nlouis, t., 541\nluce, r., 299, 302, 443\n\nmadansky, a., 422, 456\nmaddala, g. s., 258, 264, 302\nmagidson, j., 653\nmagnus, j. r., 602\nmansmann, u., 104\nmantel, n., 87, 93, 104, 171, 197, 209, 230, 231,\n232, 234, 238, 260, 295, 296, 297, 300, 379,\n414, 481, 612, 618, 624, 625, 627, 631\n\nmartin andres, a., 104\nmason, w. m., 524, 609\nmatthews, j. n. s., 301, 313, 443\nmaxwell, a. e., 631\nmcardle, j. j., 202\nmccloud, p. i., 443\nmccullagh, p., 132, 155, 156, 257, 276, 277,\n\n283, 286, 290, 301, 308, 312, 340, 378, 397,\n431, 443, 466, 471, 481, 556, 566, 625, 631\nmcculloch, c. e., 522, 523, 524, 527, 548, 555,\n\n623, 625\n\nmcdonald, j. w., 667, 684\nmcfadden, d., 228, 264, 299, 300, 302, 302,\n\n624, 631\n\nmcnemar, q., 411\nmee, r. w., 77\nmeeden, g., 605\nmehta, c. r., 98, 104, 254, 255, 258, 298, 397,\n\n625, 630\n\nmendel, g., 22\nmendenhall, w. m., 107\nmersch, g., 400\nmichailidis, g., 399\nmiettinen, o. s., 77, 442\nmiller, m. e., 481, 604\nmin, y., 100, 101\nminkin, s., 196\nmirkin, b., 112\nmitra, s. k., 79, 258, 346, 591, 627, 631\nmittal, y., 60\nmolenaar, i. w., 526\nmolenberghs, g., 258, 466, 482\nmoore, d. f., 152, 556, 566\n\nmoore, d. s., 27, 103\nmorabia, a., 527\nmorgan, b. j. t., 196, 207\nmorgan, w. m., 346\nmorris, c., 526, 605, 610\nmosimann, j. e., 566\nmosteller, f., 345, 412, 443, 627, 629, 631\n\nnair, v. n., 103, 301\nnam, j., 77\nnatarajan, r., xv, 481, 502, 524\nnelder, j., 116, 132, 148, 149, 155, 156, 257,\n\n290, 301, 312, 340, 378, 559, 566, 574, 625,\n631\n\nnerlove, m., 300, 624\nneudecker, h., 602\nneuhaus, j. m., 417, 494, 496, 499, 502, 526,\n\n547, 548, 566\n\nnewcombe, r., 27, 109, 110\nneyman, j., 18, 112, 611, 612, 616, 626, 631\nnikulin, m. s., 27\nnormand, s.-l., 609\nnorusis, m. j., 633\nnurminen, m., 77\n\no\u2019brien, p. c., 207\no\u2019brien, r. g., 244, 258, 640\nochi, y., 258\nodoroff, c., 661\no\u2019gorman, t. w., 596\nolivier, d., 385, 386, 389\noverton, w. s., 526, 552\n\npagano m., 61, 657\npaik, m., 59\npalmgren, j., 156, 340\npark, t., 482\nparr, w. c., 594\nparzen, e., 34\npatefield, w. m., 104\npatel, n. r., 98, 258, 625, 630\npatnaik, p. b., 258\npaul, s. r., 566\npearson, e. s., 18, 104, 626, 631\npearson, k., 22, 79, 112, 399, 576, 589, 620,\n\n621, 622, 628, 631\n\npeduzzi, p., 212\npendergast, j. f., xv, 502\npepe, m. s., 258\nperlman, m., 237\npeters, d., 104, 630\npeterson, b., 282, 301\npeto, r., 62\npiccarreta, r., 206\n\n "}, {"Page_number": 713, "text": "698\n\nauthor index\n\npierce, d. a., 104, 143, 156, 497, 502, 522, 526,\n\n630\n\npike, m. c., 100\npiegorsch, w. w., 684\nplackett, r. l., 103, 196, 399, 623, 627, 631\npodgor, m. j., 197\npoisson, s.-d., 7\npratt, j. w., 283\npregibon, d., 143, 156, 197, 225, 257, 258, 566,\n\n638\n\nprentice, r. l., 171, 196, 258, 283, 399, 482,\n\n555, 566, 625\npresnell, b., 156\npress, s. j., 196, 300, 624\npyke, r., 171, 625\n\nqaqish, b., 676\nqu, a., 482\nquetelet, a., 68\nquine, m. p., 29\n\nrabe-hesketh, s., 527, 633\nradelet, m., 48, 65\nraftery, a., 257\nrao, c. r., 10, 12, 576, 582, 585, 587, 589, 591,\n\n596, 616, 626, 631\nrao, j. n. k., 103, 515\nrasbash, j., 520, 524\nrasch, g., 399, 415, 493, 495, 624\nrayner, j. c. w., 103\nread, t. r. c., 27, 112, 258, 396, 612\nregal, r. r., 526\nreid, n., 96\nrice, w. r., 104\nripley, b., 633\nritov, y., 384\nrobins, j., 234, 258, 475\nrohmel, j., 104\nrosenbaum, p. r., 196\nrosner, b., 566\nrossi, p. e., 609\nrotnitzky, a., 467\nroutledge, r. d., 104, 607\nroy, s. n., 79, 346, 627, 631\nrubin, d., 196, 475, 482\nrudas, t., 481, 565\nrundell, p. w. k., 613, 614\nryan, l., 290, 527, 566\n\n\u00a8\n\nsackrowitz, h. b., 103, 104\nsamuels, m. l., 60\nsantner, t. j., 101\nschafer, d. w., 143, 156\nschafer, j. l., 103, 347, 482\n\nschluchter, m. d., 399\nschumacher, m., 306\nscott, a. j., 35, 103, 197\nsearle, s., 527, 555\nseeber, g., 155\nsekar, c. c., 511\nself, s. g., 258, 525\nsen, p. k., 594\nseneta e., 29\nsilvey, s. d., 301, 465, 625\nsinger, j. m., 594\nshapiro, s. h., 398\nshen, s. m., 265\nshihadeh, e. s., 399\nshuster, j. j., 95, 103, 104, 442\nsilva mato, a., 104\nsilvapulle, m. j., 195\nsilvey, s. d., 301, 465, 625\nsimon, g., 197, 301, 374, 399, 612, 624, 629\nsimonoff, j., 594, 614, 615, 616\nsimpson, e. h., 51, 60, 398, 596, 621\nsinger, j. m., 594\nskellam, j. g., 566\nskene, a. m., 502, 609\nskinner, c., 347\nskrondal, a., 527\nslaton, t. l., 566\nsmall, k. a., xv, 302\nsmith a. f. m., 609, 616\nsmith, k. w., 345\nsmith, p. w. f., 443, 482, 616\nsnell, e. j., 196, 301, 624\nsnell, m. k., 101\nsobel, m. e., 672\nsomers, r. h., 68\nsomes, g. w., 488\nspeed, t., 347, 616\nspiegelhalter, d., 616\nspitzer, r. l., 435\nsprott, d. a., 95, 114, 453\nstarmer, c. f., 601, 629\nstasinopoulos, m., 526\nstern, s., 302\nsterne, t. e., 20\nstevens, s. s., 26\nstigler, s., 22, 443, 448, 623, 631\nstill, h. a., 20, 27, 32\nstiratelli, r., 482, 526\nstokes m. e., xv, 282, 302, 399, 476, 482, 633,\n\n640, 649\n\nstrawderman, r. l., 104, 630\nstuart, a., 27, 56, 399, 422\nstukel, t. a., 196, 250\nsturmfels, b., 104\n\n "}, {"Page_number": 714, "text": "author index\n\nsuissa, s., 95, 104, 442\nsundberg, r., 346, 366\n\n\u00b4\n\ntamhane, a. c., 101\ntango, t., 411\ntanner, m. a., 443\ntarone, r. e., 197, 234, 258\ntavare, s., 103\nten have, t. r., xv, 517, 526, 527, 527\ntheil, h., 57, 228, 300, 624\nthomas, d. r., 103, 515\nthompson, r., 283\nthompson, w. a., 399\nthurstone, l. l., 443\ntibshirani, r., 153, 199, 301\ntitterington, d. m., 616\ntjur, t., 426, 552, 553\ntocher, k. d., 94\ntoledano, a., 230, 481\ntolley, h. d., 594\ntrain, k., 302, 527\ntrivedi, p. k., 131, 155, 561, 566, 574\ntsiatis, a. a., 152, 197, 556, 566\ntukey, j., 112\nturing, a., 631\nturnbull, b. w., 103\ntutz, g., 155, 156, 289, 290, 300, 301, 513, 615\n\nuebersax, j. s., 544\nuttal, d. h., 517\n\nvan der heijden, p. g., 399\nvenables, w. n., 633\nverbeke, g., 482, 545\nvermunt, j. k., 399, 653\n\n699\n\nwedderburn, r. w. m., 116, 148, 149, 150,\n\n155, 156, 195, 258, 265, 266, 466, 470, 625,\n631\n\nweisberg, s., 226\nwells, m. t., 104, 630\nwermuth, n., 398, 399, 401\nwestfall, p. h., 214, 360\nwhite, a. a., 481\nwhite, h., 467, 471, 482\nwhitehead, j., 301\nwhittaker, j., 346, 358, 398, 399\nwhittemore, a. s., 243, 398\nwild, c., 103, 197, 301\nwilks, s. s., 12\nwilliams, d. a., 156, 225, 397, 555, 566, 653\nwilliams, e. j., 103, 399\nwilliams, o. d., 291, 301, 624\nwilson, e. b., 16\nwilson, j., 103\nwinner, l., 445\nwolfinger, r. d., 214, 360, 527\nwong, g. y., 524, 609\nwoolf, b., 71\nwoolson, r. f., 487, 596\nwu, c. f. j., 196, 301\nwu, m., 346, 347\n\nyamagami, s., 101\nyang, i., 544\nyang, m., 104\nyates f., 91, 93, 96, 98, 103, 104, 114, 239, 624\nyee, t. w., 301\nyerushalmy, j., 38\nyoung, s. s., 214, 360\nyule, g. u., 44, 53, 59, 68, 110, 346, 406, 566,\n\n620\u1390621, 628, 631\n\nzeger, s. l., 442, 467, 469, 471, 473, 481, 482,\n\n499, 500, 524, 548, 609, 625, 631\n\nwainer, h., 63\nwakefield, j. c., 502, 609\nwald, a., 11, 172\nwalker, s. h., 195, 197, 277, 301, 624\nwalley, p., 616\nwalsh, s. j., 104\nwardrop, r. l., 105\nware, j. h., 478, 480, 482\nwatson, g. s., 79, 103, 576, 590, 627, 631\n\nzelen, m., 255, 625, 631\nzellner, a., 609\nzelterman, d., 397\nzermelo, e., 443\nzhang, h., 257\nzhao, l., 482\nzheng, b., 227, 258, 266\nzhu, y., 96\nzweiful, j. r., 70, 397\n\n "}, {"Page_number": 715, "text": " "}, {"Page_number": 716, "text": "subject index\n\nadjacent categories logit, 286\u1390288, 370\u1390371,\n\n374\u1390376, 642\n\nadjusted residual, see standardized pearson\n\nbinomial distribution, 5\u13906\nadmissible estimator, 605\nconfidence interval for proportion, 15\u139017,\n\nresidual\n\nagreement, 431\u1390436, 443, 453\u1390454, 541\u1390544,\n\n549\u1390551\n\naic, 216\u1390217, 324\nalternating logistic regressions, 474\nancillary statistic, 104\narc sine transformation, 596\narmitage test, see cochran\u1390armitage\n\ntrend test\n\nassociation, see measures of association\nassociation graphs, 357\u1390360, 539\nassociation models, 373\u1390381, 399\nasymptotic covariance matrix, 137\u1390138,\n\n577\u1390581, 594\n\nasymptotic normality, 73\u139077, 577\u1390581\nattributable risk, 66, 110\n\nbackward elimination, 214\u1390216\nban, 611, 626\nbaseline-category logits, 267\u1390274, 300,\n\n310\u1390311, 426, 515, 640\u1390643\n\nbayesian inference, 604\u1390610, 616, 630\u1390631\n\nbinomial parameters, 605\u1390607, 617\ngeneralized linear mixed models, 524, 609\nkernel smoothing, connection, 614\nmultinomial proportions, 607\u1390610, 618\n\nbernoulli distribution, 117\nbeta-binomial distribution, 30, 553\u1390559, 566,\n\n572, 573, 653\n\nbeta distribution, 554, 572, 605\u1390606\nbias, 70, 85, 196, 450, 496, 524, 548, 595, 615\nbic, 257\nbinary data\n\ncorrelated, 409\u1390420, 455\u1390482, 491\u1390527,\n\n538\u1390559\n\ngeneralized linear models, 120\u1390125, 137, 140\nmatched pairs, 409\u1390420\n\n32\u139033, 635\n\nexact inference, 18\u139020\nexponential family, 117, 134\nglm likelihood equations, 137\nlikelihood function, 9\nmatched pairs, 409\u1390420\nmoment generating function, 31\noverdispersion, 8, 30\ntests for proportion, 14\u139015\nvariance stabilizing, 596\n\nbinomial models\ndeviance, 140\nglms, 120\u1390125\nlikelihood equations, 137, 265\noverdispersion, 151\u1390153, 291, 573, 653\n\nbirch\u2019s results, 336\nbootstrap, 75, 156, 525, 531, 594\nbradley\u1390terry model, 436\u1390439, 443, 647\nbreslow\u1390day test, 258\n\ncalibration, 207\ncanonical correlation, 382, 399, 408, 624\ncanonical link, 117, 148\u1390149, 193, 257, 472,\n\n496\n\ncapture\u1390recapture, 511\u1390513, 526, 544\u1390545,\n\n551\u1390552\n\ncart, 257\ncase-control study, 42\u139043, 46\u139047, 59, 233,\n\nand logistic regression, 170\u1390171, 418\u1390420,\n\n625\n\nseveral controls per case, 233, 442\n\ncategorical data analysis, 1\u1390688\ncausal diagram, 217\u1390218\ncensoring, 386, 400\ncentering, 167, 175\nchi-squared distribution\n\ndf, 12, 79, 175, 589\n\n701\n\n "}, {"Page_number": 717, "text": "702\n\nchi-squared distribution continued\n\n\u017e\n\n.\n\nmgf, 35\nmoments, 27\nnoncentral, 237, 258, 408, 591\u1390592, 595, 597\nreproductive property, 82\ntable of percentage points, 654\n\nchi-squared statistics\n\nlikelihood-ratio, see likelihood-ratio\n\nstatistic\n\npartitioning, see partitioning\npearson, see pearson chi-squared statistic\n\nclassification methods, 196, 257, 228\u1390230, 258\nclinical trials, 42, 230\u1390236, 507\u1390510\nclopper\u1390pearson confidence interval, 18\u139020,\n\n33, 606\n\ncluster sampling, 103, 481, 515\nclustered data, 455, 491\u1390527, 556\u1390558\ncochran, w. g., 626\ncochran\u1390armitage trend test, 181\u1390182, 197,\n\n237, 253, 640\n\ncochran\u1390mantel\u1390haenszel test, 231\u1390234, 639\n\nexact test, 254, 298\nand marginal homogeneity, 413, 458\u1390459,\n\n481\n\nand mcnemar test, 413\u1390414\nmatched pairs, 413\nnominal and ordinal cases, 295\u1390298, 302,\n\n379, 642\u1390643\n\nscore test for logit model, 232, 297\u1390298\n\ncochran\u2019s q, 459, 488\ncollapsibility, 358\u1390360, 398\ncomplementary log\u1390log model\n\nbinary response, 248\u1390250, 640\nordinal response, 283\u1390284, 301, 313, 527,\n\n641\n\ncomputer software, see software\nconcentration coefficient, 69\nconcordance index, 229\nconcordant pair, 57\u139059\nconditional distribution, 37, 48\nconditional independence, 52\n\ni = j = k tables, 293\u1390298, 302, 318\u1390319,\n\n325\n\nlogit models, 183\u1390184, 230\u1390234, 263,\n\n293\u1390295, 359\u1390360\n\nversus marginal independence, 53, 365\u1390366\npower and sample size, 244\u1390245\nsmall-sample test, 254, 298\n\nconditional inference, 91\u1390101, 250\u1390257,\n\n416\u1390420, 495\u1390496, 630\n\nconditional logistic regression, 250\u1390258,\n\n414\u1390420, 495\u1390496, 526, 625, 640, 645\n\nconditional logit, 299\nconditional ml, 100, 417, 494\u1390496, 526\n\nsubject index\n\nconditional symmetry, 431, 452\nconfidence intervals\n\nlikelihood-based, 13, 77\u139078\ntail method, 18, 99\nwald, 13\nscore, 15\u139016, 77\n\nconfounding, 47\u139051, 230\nconjugate mixture model, 558\u1390559\nconstraint equations, 612\nconstraints, parameter, 178\u1390179, 317, 352\u1390353\ncontingency coefficient, 112, 620\ncontingency table, 36, 47\u139054\ncontinuation-ratio logit, 289\u1390291, 301,\n\n517\u1390520\n\ncontinuity correction, 27,\ncontinuous proportions, 265\u1390266, 624\ncontrasts, 82, 317, 340, 344, 603, 636, 639\ncorrelation, 87, 226, 296, 634\ncorrelation models, 381\u1390384, 399, 408\ncorrespondence analysis, 382\u1390384, 399, 624,\n\n644\n\u00b4\n\ncramer\u2019s v 2, 112\ncredit scoring, 165, 263, 631\ncross-classification table, see contingency\n\ntable\n\ncrossover study, 444, 457, 483, 498, 501, 572\ncross-product ratio, 44\ncross validation, 266\ncumulant function, 155\ncumulative link models, 282\u1390286, 313\ncumulative logit models, 274\u1390282, 301, 624,\n\n641\n\ndispersion effects, 285\u1390286\nmarginal models, 420\u1390421, 462\u1390463, 469\nproportional odds property, 275\u1390276, 282\nrandom effects, 514\u1390515, 536\nscore test and ranks, 301\n\ncumulative odds ratio, 67, 276\ncumulative probit model, 278, 283, 301, 312,\n\n624\u1390625, 641\n\ndata mining, 219, 631\ndecomposable model, 346, 360\ndegrees of freedom, 12, 79, 175, 589, 622\ndelta method, 73\u139077, 577\u1390581, 594\ndependent proportions, 410\u1390412\ndesign, 196, 609\ndesign matrix, see model matrix\ndeviance, 118\u1390119, 139\u1390142\n\ngrouped vs. ungrouped binary data, 208\nlikelihood-ratio tests, 141\u1390142, 186\u1390187,\n\n363\u1390365\n\nresidual, 142, 220, 638\nr-squared measures, 228\n\n "}, {"Page_number": 718, "text": "subject index\n\n703\n\ndiagnostics, 142\u1390143, 219\u1390230, 257\u1390258,\n\n366\u1390367\n\ndiagonals-parameter symmetry, 443\ndifference of proportions, 43\n\ncollapsibility, 398\ndependent, 410\u1390412, 645\nhomogeneity, 258\nlarge-sample confidence interval, 72, 77, 102,\n\n110, 410\u1390411\n\nsample size determination, 240\u1390242, 258\nsmall-sample confidence interval, 101\nz test and pearson statistic, 111\n\ndirected alternatives, 88\u139090, 236\u1390239, 373\ndirichlet distribution, 607, 610\ndiscordant pair, 57\u139059\ndiscrete choice models, 298\u1390300, 302, 527,\n\n624\n\ndiscreteness and conservatism, 18\u139020, 93\u139094,\n\n257\n\ndiscriminant analysis, 196\ndispersion parameters, 131, 133, 285\u1390286, 560\ndissimilarity index, 329\u1390330\ndiversity index, 596\ndummy variables, 178\u1390179\n\necological inference, 527\neffect modifier, 54\nem algorithm, 522\u1390523, 540\u1390541\nempirical bayes, 526, 610\nempirical logit, 168\nempty cells, 392\nentropy, 57, 613\nestimated expected frequencies, 25, 78, 315\nestimating equations, 470, 481\u1390482\nexact confidence intervals, 18\u139020, 99\u1390101, 255\nexact tests\n\nbinomial parameter, 18, 412\nconditional independence, 254, 298\nfisher, 91\u139097, 253\ni = j tables, 97\u139098, 104\nlogistic regression, 251\u1390257\nmatched pairs, 412\nordinal variables, 114\nstatxact and logxact, 633, 635, 640, 643\ntrend in proportions, 98\nunconditional test, 94\u139096, 104, 114\n\nexpected frequencies, 22, 25,\nexponential dispersion family, 133, 310\nexponential distribution, 313, 388\nexponential family, 116, 133\nextreme-value distribution, 249\u1390250, 264\n\nvariance test, 163\nfisher scoring, 145\u1390149, 156, 247, 623, 625\n\nfisher\u2019s exact test, 91\u139097, 99, 253, 623\n\nand bayes approach, 608\nconservativism, 93\u139094\ncontroversy, 95\u139096, 104\nsoftware, 635\numpu, 104\nversus unconditional test, 95\u139096, 104, 114\n\nfitted values, 121\n\nasymptotic distribution, 194, 341, 585\u1390586,\n\n593\n\nfreeman\u1390tukey chi-squared, 112, 594\n\n<\n\n1\n\n0\n\n.\n\n2\u017e\n\ng2 statistic, see likelihood-ratio statistic\ng m m , 187, 363\ngamma, 58\u139059, 88, 110, 596\u1390597\ngamma distribution, 559\u1390560, 574\ngauss\u1390hermite quadrature, 521\u1390522, 651\n.\ngeneralized estimating equations gee ,\n466\u1390475, 481\u1390482, 501, 557\u1390558, 649\n\n\u017e\n\ngeneralized additive models, 153\u1390155, 156,\n\n301, 630, 636\n\n.\ngeneralized linear mixed model glmm ,\n\n\u017e\n\n417, 492\n\nbayesian approach, 524, 609\nbinary data, 492\u1390527\ncorrelation nonnegative, 497, 564\ncount data, 563\u1390565\nheterogeneity, interpretation, 497\u1390498\nmarginal effects, comparison, 498\u1390502, 535,\n\n563\u1390564\n\nmarginal model, corresponding, 527,\n\n563\u1390564, 574\u1390575\n\nmisspecification, 547\u1390548\nmodel fitting, 520\u1390526, 527\nmultinomial data, 513\u1390516\nsoftware, 649\u1390653\n\n\u017e\n\n.\n\ngeneralized linear model glm , 116\u1390119,\n\n625\n\ncanonical link, 117, 148\u1390149, 193, 257, 472,\n\n496\n\ncovariance matrix, 137\u1390138\nexponential dispersion family, 133\ninference using, 139\u1390143\nlikelihood equations, 135\u1390136, 148\nmodel fitting, 143\u1390149\nmoments, 132\u1390134\nmultivariate, 274\nvariance function, 136\n\ngeneralized loglinear model, 332\u1390333, 464,\n\n481, 602\n\nfisher, r. a., 22\u139023, 622\u1390624, 626, 628\n\ndf argument with pearson, 622\u1390623\n\ngini concentration index, 68\ngoodman, l. a., 627\u1390629\n\n "}, {"Page_number": 719, "text": "704\n\nsubject index\n\ngoodman and kruskal tau and lambda, 68\u139069\ngoodness-of-fit statistics\n\ncontinuous explanatory variables, 176\u1390177,\n\n197\n\ndeviance for glms, 118\u1390119, 139\u1390142\nlikelihood-ratio test, 141\u1390142, 186\u1390187,\n\n363\u1390365\n\nlogistic regression, 174\u1390177, 186\u1390187, 208\nloglinear models, 324\nmixture summary, 565\npearson chi-squared, 22\u139026\nuninformative for ungrouped data, 162\n\ngraphical models, 357\u1390360, 398, 629\ngrouped versus ungrouped data, 140\u1390141, 162,\n\n174\u1390177, 208, 228\n\ngsk method, 601\ngumbel distribution, 249\n\nhat matrix, 143, 225, 589\nhazard function, 301, 388, 399\u1390400\nheterogeneity, 130, 235\u1390236, 291, 377,\n\n492\u1390493, 497, 499\u1390500, 507\u1390510, 538\n\nhierarchical models, 316, 520, 609\nhistory, 619\u1390631\nhomogeneity of odds ratios, 54, 183, 234\u1390236,\n\n255, 258\n\nhomogeneous association, 54, 320, 377, 407,\n\n623\n\nhosmer\u1390lemeshow statistic, 177, 639\nhypergeometric distribution, 91\n\nand binomial, 113\nmoments, 103, 232\nmultiple hypergeometric, 97\nnoncentral, 99\n\nidentity link, 117, 120, 124, 128, 385, 387, 562,\n\n565\n\nincomplete table, 392\nindependence\n\nconditional, see conditional independence\nestimated expected frequencies, 78\nexact test, see fisher\u2019s exact test\nfrom irrelevant alternatives, 299, 302\njoint, 318, 319\nlikelihood-ratio test, 79\nloglinear model, 132, 314\u1390315, 336, 352\nmutual, 318\u1390319, 353, 354\npearson test, 78\u139079\nquasi, 426\u1390428, 432\u1390433, 443\nresiduals, 81, 111\u1390112\nsmoothing using, 85\u139086\ntwo-way table, 38\u139039, 78\u139079, 111\nvariance of proportion estimator, 113\n\nindependent multinomial sampling, 40, 67,\n\n339\u1390340\n\ninfluence diagnostics, 224\u1390226, 638\ninformation matrix, 9\nglm, 138, 145\u1390146\nlogistic regression, 193\nloglinear model, 339\nobserved versus expected, 145\u1390146, 247\n\ninteraction, 210\n\nand odds ratios, 54\nthree-factor, 320\nuniform, 407\n\nisotropy, 406\nitem response models, 495\niterative proportional fitting, 343\u1390345, 347\niterative reweighted least squares, 147, 156,\n\n195, 343\n\njoint independence, 318, 319\n\nkappa, 434\u1390435, 443, 453, 645\nkendall\u2019s tau and tau-b, 60, 68\nkernel smoothing, 613\u1390615, 616\n\n.\n\n\u017e\n\nlambda measure of association , 69\nlaplace approximation, 523\nlatent class models, 538\u1390545, 565, 571\u1390572,\n\n653\n\nlatent variable, 277\u1390278, 399\nld 50, 167\nleverage, 143, 589\nlikelihood function, 9\n\ngeneralized linear model, 133, 135\nmarginal likelihood, 521\n\nlikelihood-ratio statistic, 11\u139012, 24\n\nasymptotic chi-squared distribution,\n\n590\u1390591\n\nand confidence intervals, 13, 16, 17, 78, 638\ndifference of deviances, 141\u1390142, 187,\n\n363\u1390364\n\nindependence, 79\nminimized by ml estimate, 590\u1390591\nmonotone property, 141\nnested models, 363\u1390365\nnoncentrality, 243\nnonnegative, 34, 141\npartitioning, 82\u139084, 363\u1390365, 399, 405\npearson statistic, comparison, 24, 80, 364\nas power divergence statistic, 112\nsparse data, 80, 395\u1390397\n\nlinear-by-linear association, 369\u1390373,\n\n643\u1390644\n\nand bivariate normal, 370, 399\nand correlation model, 408\nheterogeneous, 377\nhomogeneous, 377\u1390379, 407\nscore statistic, 406\n\n "}, {"Page_number": 720, "text": "subject index\n\nlinear logit model, 180\u1390182\n\ndirected inference, 236\u1390237\nefficiency, 197\nexact test, 253\nlikelihood equations, 209\nand trend test, 197, 237\u1390239\n\nlinear predictor, 116\nlinear probability model, 120\u1390121, 291\n\nand trend test, 181\u1390182\n\nlink function, 116, 135\n\ncanonical, 117, 148\u1390149, 193, 257, 472, 496\ncumulative, 282\u1390286, 301\ngoodness of link, 257\u1390258, 301\ninverse cdf, 124\u1390125, 163, 282\n\nlitter effects, 151\u1390153, 291, 556\u1390558, 566\nlocal odds ratio, 55, 312, 369\u1390370\n\nasymptotic covariances, 597\nconditional, 321\u1390322, 377\nexponential family for multinomial,\n\n310\u1390311\n\nlogistic distribution, 125, 162, 197, 246\nlogistic-normal distribution, 265\nlogistic-normal model, 496\u1390513, 516\u1390527\nlogistic regression, 121\u1390125, 165\u1390196\n\ncase-control studies, 170\u1390171, 418\u1390420, 625\ncategorical predictors, 177\u1390186\nconditional, 250\u1390258, 414\u1390420, 495\u1390496,\n\n526, 625, 640, 645\n\nconditional independence, 183\u1390184, 231\ncovariance matrix, 193\u1390194\ndesign, 196, 609\ndiagnostics, 219\u1390230, 257\u1390258\nexistence of ml estimates, 195\u1390196,\n\n394\u1390395\n\nfitting model, 192\u1390196\ngeneralized linear model, 117, 121\u1390125\ngoodness-of-fit, 174\u1390177, 186\u1390187, 197\ninference, 172\u1390177\ninterpretation, 166\u1390171, 191\nlikelihood equations, 192\u1390193\nlinear logit model, see linear logit model\nloglinear models, connection, 315, 330\u1390332,\n\n367, 593\u1390594\n\nmarginal models, 414, 456\u1390476\nmatched pairs, 414\u1390420, 493\u1390496\nmodel-building, 211\u1390225\nmultiple predictors, 182\u1390195\nnonparametric mixture, 546\u1390547, 653\nnormal distribution connection, 171,\n\n207\u1390208\n\nand odds ratio, 124, 166\nperfect discrimination, 195\u1390196\nprobability estimators, 166\u1390167, 191, 194\nrandom effects, 496\u1390513, 516\u1390527\nregressive logistic model, 479\u1390481\n\n705\n\nrepeated binary response, 414\u1390420,\n\n456\u1390476, 496\u1390513, 516\u1390527\n\nrepeated multinomial response, 461\u1390464,\n\n469, 474\u1390475, 513\u1390516\n\nresiduals, 219\u1390223\nsample size determination, 242\u1390243\nsample size and number of predictors, 212\nsoftware, 637\u1390643, 645, 649\u1390651\n\nlogit transform, 75, 117, 624\n\nbias, 196\nconfidence interval, 109\nin logistic regression, 123\nstandard error, 74\u139075\nwald test of proportion, 208\u1390209\n\nloglinear models, 117\u1390118, 314\u1390347,\n\n627\u1390629\n\ncovariance matrix, 138\u1390139, 338, 341, 593,\n\n598\n\nexistence of estimates, 341, 392\u1390395\nfitting, 342\u1390344\nfour dimensions, 326\u1390330, 355\ngeneralized loglinear model, 332\u1390333, 464,\n\n481\n\ngeneralized linear model, 117\u1390118, 125\u1390132\ngoodness of fit, 337\u1390338\nhomogeneous association, 320, 377\nindependence, 232, 314\u1390315, 318\u1390319, 336,\n\n352, 365\u1390366\n\nlikelihood equations, 334\u1390336\nlinear-by-linear association, 369\u1390373,\n\n377\u1390379\n\nlogit models, connection, 315, 330\u1390332, 367,\n\n593\u1390594\n\nordinal variables, 367\u1390377\nparameter definition, 316\u1390317, 352\u1390353\npoisson-multinomial connection, 317\u1390318,\n\n339\u1390340\n\nprobability estimates, 340\u1390341\nrates, 385\u1390391\nsaturated, 316, 380\nselection, 360\u1390366\nsoftware, 643\u1390644\nsquare-tables, 424\u1390431\nthree-factor interaction, 320\n\u017e\nx, y, z type symbols, 320\u1390321\n\n.\n\nlog link, 118, 124, 125, 132, 138, 140, 314, 560,\n\n563\n\nlog-log models, 248\u1390250, 283\nlongitudinal studies, see repeated response\nlowess, 154\n\nmann\u1390whitney statistic, 90, 301, 452\u1390453\nmantel, n., 625\nmantel\u1390haenszel estimator, 234\u1390235, 417,\n\n639\n\n "}, {"Page_number": 721, "text": "706\n\nsubject index\n\nmantel\u1390haenszel test, see\n\ncochran\u1390mantel\u1390haenszel test\n\nmantel score test, 87, 88, 89, 379\nmarginal distribution, 37. see also marginal\n\nmodels\n\nmarginal likelihood, 521\nmarginal homogeneity\n\nbinary matched pairs, 410\u1390413\nand independence, 111\nnominal tests, 422\u1390423, 457\u1390459\nordinal tests, 421, 452\u1390453, 458\nmulti-way table, 439\u1390442, 456\u1390459,\n\n647\u1390649\n\nmarginal models, 414, 420\u1390423, 439\u1390442,\n\n456\u1390476\n\nminimum discrimination information, 112,\n\n612\u1390613, 616\n\nmisclassification error, 347\nmissing data, 103, 347, 463, 475\u1390476, 482\nmixture models, 538\u1390566. see also\n\ngeneralized linear mixed models\n\nml, see maximum likelihood\nmodel-based inference\n\nimproved precision of estimation, 85, 112,\n\n174, 239\u1390240, 264\n\nmodel-based tests, 141\u1390142, 172, 363\u1390365,\n\n396, 399\n\nmodel matrix, 135\nmonotone trends, 88. see also trend tests\nmonte carlo methods, 114, 522\u1390525, 609,\n\nconditional models, comparison,\n\n629\u1390630, 635\n\n498\u1390502\n\ngee approach, 466\u1390475\nml fitting, 464\u1390466, 481\nodds ratio, 451, 494\nsoftware, 644\u1390649\n\nmarginal symmetry, 442\nmarginal table, 48\n\nmulticollinearity, 212\nmultilevel models, 520, 609, 651\nmultinomial distribution, 6\u13907\nbinomial factorization, 289\nexponential family, 310\u1390311\ninference, 21\u139026, 35\nmean, correlation, covariance, 7, 31,\n\nsame association as partial table, 358\u1390360,\n\n579\u1390580, 596\n\n398\n\nmarkov chains, 477\u1390481, 482, 489\u1390490\nmatched pairs, 409\u1390454\n\ncochran\u1390mantel\u1390haenszel approach, 413\ndependent proportions, 410\u1390412\nlogistic models, 414\u1390420, 493\u1390496, 516\u1390517\nmcnemar test, 411\u1390413, 424, 442, 644\u1390645\nodds ratio estimates, 417, 451, 494\nordinal data, 420\u1390421, 429\u1390431, 439, 443,\n\n452\u1390453, 462\u1390464, 536\n\nrandom effects, 417\u1390418, 493\u1390494, 535\n\nmaximum likelihood, 9\n\nconditional, 100, 417, 494\u1390496, 526\ninconsistent estimator, 450\niterative reweighted least squares, 147, 156,\n\n195, 343\n\nlikelihood function, see likelihood function\nversus other methods, 468, 603\u1390605, 612\nmcnemar test, 411\u1390413, 424, 442, 644\u1390645\nmean response model, 291\u1390294\nmeasurement error, 347, 493\nmeasures of association, 43\u139047, 54\u139060, 68\u139069,\n\n620\u1390622\n\nasymptotic normality, 110\ncomparing several values, 599\n\nmendel, 22\u139023, 623\nmid-distribution function, 34\nmid-p-value, 20, 27, 33, 104\nmidranks, 89, 90, 302\nminimum chi-squared, 112, 611\u1390612, 616, 618,\n\n629\n\nand poisson, 8\u13909, 40\nsampling models, 40\u139041, 67\n\nmultinomial logit models, 267\u1390291, 298\u1390300,\n\n302, 624, 640\u1390643, 651\u1390653\n\nmultinomial loglinear model, 317\u1390318,\n\n339\u1390341\n\nmultinomial response models, 267\u1390300,\n\n640\u1390643\n\nmutual independence, 318\u1390319, 353, 354\n\nnational halothane study, 627, 629\nnatural exponential family, 116, 133, 155\nnatural parameter, 133\nnegative binomial\n\ndistribution, 31, 161, 163, 560, 566, 574\nregression model, 131, 560\u1390563, 565, 566,\n\n653\n\nnested models\n\nlikelihood-ratio comparison, 141\u1390142, 187,\n\n363\u1390364\n\nsimultaneous tests, 263\nusing x 2, 364\n\nnewton\u1390raphson, 143\u1390146, 163\u1390164\n\nand fisher scoring, 145, 247\nipf, comparison, 344\u1390345\nlogistic regression, 194\u1390195\nloglinear models, 342\u1390345\n\nneyman, j., 626\nnominal variable, 2\u13903\n\nbaseline-category logit models, 267\u1390274,\n\n300, 310\u1390311, 426, 515, 640\u1390643\n\n "}, {"Page_number": 722, "text": "subject index\n\nnominal variable continued\n\n\u017e\n\n.\n\nmatched pairs, 422\u1390423\nmeasures of association, 55\u139057, 68\u139069\nsquare table models, 425\u1390433, 439\u1390442\nnoncentral chi-squared distribution, 237,\n\n258\n\nasymptotic representation, 591\u1390592, 595\nnoncentrality parameter, 237, 243\u1390245,\n\n408, 597\n\npower and df, 237\u1390239\n\n707\n\nmarginal models, 420\u1390421, 429\u1390430,\n\n440\u1390441, 462\u1390464\n\nmatched pairs, 420\u1390421, 429\u1390431, 439, 443,\n\n452\u1390454, 462\u1390464\n\nmean response model, 291\u1390294\nmeasures of association, 57\u139059, 67, 68\nmultinomial response models, 274\u1390295\nordinal quasi symmetry, 429\u1390430, 440\u1390441,\n\n647\n\nrepeated response, 461\u1390464, 469, 474\u1390475,\n\nnonparametric random effects, 545\u1390553,\n\n514\u1390515, 517\u1390520\n\n565\u1390566, 653\n\nnormal distribution\n\nasymptotic normality, see delta method\nand chi-squared, 82\nand logistic regression, 171, 207\u1390208\nunderlying categorical data, 112, 264, 370,\n\n620\n\no, o rates of convergence, 577, 595\nobservational study, 43\nodds, 44\nodds ratio, 44, 620\n\nbias, 70, 595\ncase-control studies, 46\u139047\nconditional, 51\u139054, 255, 321, 417, 451\nconditional ml estimate, 255, 417\nconfidence interval, 71, 77\u139078, 99\u1390102, 255,\n\n256\n\ncumulative, 67\nexact inference, 99\u1390101, 253, 255\nhomogeneity, in 2 = 2 = k tables, 54, 183,\n\n234\u1390236, 255\n\ni = j tables, 55\u139056, 581, 597\ninvariance properties, 45\u139046, 59\nlocal, see local odds ratio\nmantel\u1390haenszel estimator, 234\u1390235\nmarginal, 451, 494\nmatched pairs, 415\u1390418, 451\nlogistic regression parameters, 124, 166, 171,\n\n179, 183, 331, 415, 497\u1390500\n\nloglinear model parameters, 315, 316, 321,\n\n331, 369\n\nordinal variables, see local odds ratio\nrelation to relative risk, 47, 124, 624\nstandard error, 71, 75\u139077, 581, 597\n\noffset, 385\nordinal variables, 2\u13903\n\ncumulative link models, 282\u1390286\ncumulative logit models, 274\u1390282, 301,\n\n420\u1390421\n\nefficiency, 197, 301\nexact tests, 98, 253\nimproved power, 88\u139090, 236\u1390239, 373\nloglinear models, 367\u1390377, 399\n\nscores, choice of, 88\u139090, 383\u1390384\ntesting independence, 86\u139091, 373\n\noverdispersion, 493\n\nbinomial, 8, 30, 151\u1390153, 291, 555\u1390558, 573,\n\n653\n\nlitter effects, 151\u1390153, 291, 556\u1390558, 566\npoisson, 7\u13908, 130\u1390131, 636\nquasi-likelihood, 151\u1390153, 291, 555\u1390558, 653\n\npaired comparisons, see bradley\u1390terry model\nparallel odds models, 374\u1390375\npartial tables, 48\npartitioning\n\nchi-squared statistic, 82\u139084, 112\u1390113, 365,\n\n399, 405\n\nand combining rows, 112\ni = j tables, 82\u139083\nnested models, 365\ntrend test, 181, 373\n\npattern mixture model, 476\npearson, karl, 619\u1390623, 628\n\narguments with fisher, yule, 79, 619\u1390623\ngoodness of fit, 22\u139024, 79\n\npearson chi-squared statistic, 22\u139026, 79,\n\n111\u1390112\n\nasymptotic chi-squared distribution,\n\n589\u1390590\n\nasymptotic conditional distribution, 103\ncontinuity correction, 103\ndegrees of freedom, 25, 79, 622\nand z for difference of proportions, 111\ngoodness of fit, 22\u139026\nindependence, 78\u139079, 111\u1390112, 622\nand likelihood-ratio, comparison, 24, 80, 364\nminimizing, 112, 611\u1390612, 616, 618, 629\nmoments, 103\nmultinomial parameters, 22\u139026\nnested models, 364\nnoncentral chi-squared distribution, see\nnoncentral chi-squared distribution\n\nscore statistic, 24\nsparse data, 80, 395\u1390397\nwith ungrouped data, 162\nupper bound, 112\n\n "}, {"Page_number": 723, "text": "708\n\nsubject index\n\npearson residual, 81, 142, 588\u1390589, 593\n\n.\n\n\u017e\n\nbinomial glm, 220, 555, 638\npoisson glm, 142, 366, 588\npenalized likelihood, 614\u1390615\npenalized quasi likelihood pql , 523\u1390524\nperfect contingency tables, 398\nperfect discrimination, 195\u1390196\nphi-squared, 112\npoisson distribution, 7\ncomparing means, 31\nexponential family, 117, 134\nmoments, 7, 31\nand multinomial, 8\u13909, 40,\nand negative binomial, 131, 559\u1390560, 566,\n\n574\n\noverdispersion, 7\u13908, 130\u1390131, 636\npoisson sampling, 39\nvariance test, 163\n\npoisson models\n\ncounts, 125\u1390132, 155, 563\u1390565\ndeviance, 140\nloglinear model, 117\u1390118, 125\u1390132,\n\n138\u1390139, 232, 314\u1390347\n\noverdispersion, 130\u1390131, 150\u1390151, 636\nrandom effects, 563\u1390565\nrates, 385\u1390391, 399\u1390400\n\npolytomous logit models, 267\u1390291\npopulation-averaged effects, 414, 495, 499\u1390501\npositive likelihood-ratio dependence, 406\npower\n\ncalculating, 240\u1390245, 640\nincreased, for directed alternatives, 88\u139090,\n\n236\u1390239, 373\n\nand noncentrality, 237\u1390239, 243\u1390245\nand number of ordinal categories, 301\n\npower-divergence statistic, 112, 613\nprediction, 525\u1390526\nprobit model, 124\u1390125, 246\u1390247, 258, 623, 640\n\ndiscrete choice, 302\nlikelihood equations, 265\nnormal parameters, 163, 246, 264\nordinal data, 278, 283, 301, 312, 641\nrandom effects, 535\nthreshold and utility motivations, 264\n\nprofile likelihood confidence interval, 78, 512,\n\n638\n\nasymptotic distribution, 585\u1390588, 593\nbayesian inference, 605\u1390607\nconfidence interval, 15\u139017, 32\u139033, 635\ndependent, 410\u1390412\ndifference, see difference of proportions\nratio, see relative risk\nstandard error, 11, 340\u1390341\n\np-value\n\nmid-p-value, 20, 27, 33, 104\nrandomized, 27, 32\numvu estimator, 162\n\nqualitative variable, 3\u13904\nquantitative variable, 3\u13904\nquasi-association, 431, 453\u1390454\nquasi-independence, 426\u1390428, 432\u1390433, 443\nquasi-likelihood\n\nbinary models, 151\u1390153, 291, 555\u1390558\ncount models, 150\u1390151\nglm, 149\u1390153, 156\n.\nmultivariate gee , 466\u1390475, 481\u1390482, 625\noverdispersion, 150\u1390153, 291, 555\u1390558\n\n\u017e\n\nquasi-symmetry, 425\u1390431, 433\u1390434, 451, 454,\n\n646\u1390647\n\nand bradley\u1390terry model, 438\u1390439\nand marginal homogeneity, 428\u1390430\nmultiway tables, 440\u1390441\nand rasch model, 552\u1390553, 565\n\nraking a table, 345\u1390346, 347, 643\nrandom component of glm, 116, 133\nrandom effects, 417, 492\u1390527\nrandom intercept, 493\nranks, 89, 90, 298, 301, 302\nrasch mixture model, 548\u1390551, 653\nrasch model, 495\u1390496, 517, 526, 535, 565, 624\nrates, 385\u1390391, 399\u1390400\nrc model, 379\u1390381, 399\u1390400\nregressive logistic model, 479\u1390481\nrelative risk, 43\u139044\n\nasymptotic standard error, 73\ncollapsibility, 398\nconfidence interval, 73, 77\nhomogeneity, 258\nin model, 124\nand odds ratio, 47, 624\n\npropensity score, 196\nproportional hazards model, 283\u1390284, 301,\n\n389, 643\n\nrepeated response, 409\u1390517. see also\ngeneralized linear mixed models;\nmarginal models; matched pairs\n\nproportional odds, see cumulative logit\n\nresiduals, 142\u1390143, 156\n\nmodels\n\nproportional reduction in variation, 56\u139057,\n\n67\u139068\nproportions\n\nadmissible estimator, 605\n\nasymptotic distribution, 587\u1390589\nbinomial glms, 219\u1390223\ndeviance, see deviance residual\npearson, see pearson residual\npoisson glms, 143, 366\u1390367\n\n "}, {"Page_number": 724, "text": "subject index\n\nstandardized pearson, see standardized\n\npearson residual\n\nretrospective study, 42\u139043. see also case-\n\ncontrol study\n\nlogistic regression, 170\u1390171\nodds ratio, 46\u139047\n\nridits, 111, 406\nroc curve, 228\u1390230, 258\nrow and column effects model, see rc\n\nmodel\n\nrow effects model, 374\u1390376, 643\u1390644\nr-squared type measure\n\nlogistic regression, 226\u1390228, 258\nnominal association, 56\u139057, 67\u139068\n\nsample size determination, 240\u1390245\nsampling methods, 39\u139043\nsampling zero, 392\nsandwich estimator, 471\u1390474\nsas, 632\u1390643\nsaturated model, 119, 139, 382\n\nlogit models, 178,\nloglinear models, 316, 380\n\nscaled deviance, 140\nscores\n\nchoice of, 88\u139090, 383\u1390384\nefficiency, 197, 301\nin loglinear models, 369\u1390379, 407\nin trend test, 88\u139089, 181\u1390182, 406\n\nscore statistic, 12, 26\u139027\n\nconfidence intervals, 15\u139016, 77\nlogistic regression, 232, 297\u1390298\npearson statistic, 24\nand standardized residuals, 156\ntrend test, 182\n\nselection model, 475\u1390476\nsensitivity, 38, 60, 228\u1390230\nsimpson diversity index, 596\nsimpson\u2019s paradox, 51, 59\u139060, 224, 354, 621\nsmall-area estimation, 502\u1390504\nsmall samples\n\nadding constants to cells, 397\u1390398\nalternative asymptotics, 233, 396\u1390397\nexact inference, 18\u139020, 91\u1390101, 104,\n\n251\u1390257\n\nexistence of estimates, 195\u1390196, 341,\n\n392\u1390395\n\nmodel-based tests, 187, 251\u1390257\nx 2 and g2, 24, 80, 364, 395\u1390397\nzeros, 392\u1390398\n\nsmoothing\n\nbayes, 606\u1390610\ngeneralized additive model, 153\u1390155\nimproved estimation with model, 85, 112,\n\n174, 239\u1390240, 264\n\n709\n\nkernel, 613\u1390615, 616\npenalized likelihood, 614\u1390615\n\nsoftware, 632\u1390653\n\nsas, 632\u1390643\nstatxact and logxact, 633, 635, 640, 643\n\nsomers\u2019 d, 68\nsparse data, 391\u1390398, 187, 250\u1390257, 591\n\nasymptotics, 233, 396\u1390397\n\nspearman\u2019s rho, 90\nspecificity, 38, 60, 228\u1390230\nsquare tables, 409\u1390454\nstandardized table, 345\u1390346\nstandardized parameter estimate, 191\u1390192,\n\n197\n\nstandardized pearson residual, 81, 143, 589\n\nbinomial glms, 220, 638\nand pearson statistic, 112\npoisson glms, 143, 367, 634\nas score statistic, 156\n\nstatxact, 633, 635, 640, 643\nstepwise model-building, 213\u1390216\nstochastic ordering, 33, 67, 301\nstructural zero, 25, 392\nsubject-specific effects, 414\u1390420, 491, 498\u1390500\nsufficient statistics, 148, 250\u1390257, 273, 334,\n\n336\n\nsuppressor variable, 67\nsurvival data, 385\u1390391\nsymmetric association, 425\nsymmetry, 424\u1390425, 644\u1390647\n\ncomplete, 440\nmultiway, 439\u1390442\n\nsystematic component of glm, 116\n\ntetrachoric correlation, 620\nthree-factor interaction, 320\nthreshold model, 264, 277\u1390279\ntolerance distribution, 245\u1390246\ntransformations, 595, 596\ntransition probabilities, 477, 490\ntransitional model, 464, 476\u1390481, 482\ntree-structured methods, 257, 631\ntrend tests, 86\u139090, 103, 296, 373, 379\n\ncochran\u1390armitage for proportions, 90,\n\n181\u1390182, 237\u1390239\nefficiency, 197, 301\nexact, 253\nsoftware, 634, 635\n\nuncertainty coefficient, 57\nuniform association model, 312, 369\u1390370, 377\nuniform interaction model, 407\nuniqueness of ml estimate, 341\nutility, 264\n\n "}, {"Page_number": 725, "text": "710\n\nvariance\n\nasymptotic, see delta method\ncomponents, 492, 525\nin exponential family, 134\nstabilizing, 596, 626\ntest for poisson, 163\nvariance function, 136, 149\u1390150\n\nsubject index\n\nand minimum modified chi-squared, 611,\n\n612\n\nand ml estimation, 146\u1390148, 603\u1390604\n\nwilcoxon test, 90, 301\nwls, see weighted least squares\n\nx 2 statistic, see pearson chi-squared statistic\nx m m , 364\n\n2\u017e\n\n.\n\n<\n\n0\n\n1\n\nwald statistic, 11, 27\n\nand power, 172, 208\u1390209\n\nwald confidence intervals, 13\nadjusted intervals, 33, 102\nweight matrix, 138, 155, 164\nweighted kappa, 435, 443, 645\nweighted observation, 391\nweighted least squares, 481, 600\u1390604,\n\n615, 629\n\nyates continuity correction, 103\nyule, g. u., 620\u1390621, 628\nyule\u2019s q, 68, 110\n\nzero cell count\n\nadding constants, 70\u139071, 397\u1390398\neffects on estimates, 70\u139071, 78, 256\nsampling, 392\nstructural, 25, 392\n\n "}, {"Page_number": 726, "text": "p&s-cp.qxd  12/4/02  10:27 am  page 2\n\nwiley series in probability and statistics\n\nestablished by walter a. shewhart and samuel s. wilks\n\neditors: david j. balding, peter bloomfield, noel a. c. cressie,\nnicholas i. fisher, iain m. johnstone, j. b. kadane, louise m. ryan,\ndavid w. scott, adrian f. m. smith, jozef l. teugels\neditors emeriti: vic barnett, j. stuart hunter, david g. kendall\n\nthe wiley series in probability and statistics is well established and authoritative. it covers\nmany topics of current research interest in both pure and applied statistics and probability\ntheory. written by leading statisticians and institutions, the titles span both state-of-the-art\ndevelopments in the field and classical methods.\n\nreflecting the wide range of current research in statistics, the series encompasses applied,\nmethodological  and  theoretical  statistics,  ranging  from  applications  and  new  techniques\nmade  possible  by  advances  in  computerized  practice  to  rigorous  treatment  of  theoretical\napproaches.\n\nthis series provides essential and invaluable reading for all statisticians, whether in aca-\n\ndemia, industry, government, or research.\n\nabraham and ledolter \u00b7 statistical methods for forecasting\nagresti \u00b7 analysis of ordinal categorical data\nagresti \u00b7 an introduction to categorical data analysis\nagresti \u00b7 categorical data analysis, second edition\nande\uf6f5l \u00b7 mathematics of chance\nanderson \u00b7 an introduction to multivariate statistical analysis, second edition\n*anderson \u00b7 the statistical analysis of time series\nanderson, auquier, hauck, oakes, vandaele, and weisberg \u00b7\n\nstatistical methods for comparative studies\n\nanderson and loynes \u00b7 the teaching of practical statistics\narmitage and david (editors) \u00b7 advances in biometry\narnold, balakrishnan, and nagaraja \u00b7 records\n*arthanari and dodge \u00b7 mathematical programming in statistics\n*bailey \u00b7 the elements of stochastic processes with applications to the natural\n\nbalakrishnan and koutras \u00b7 runs and scans with applications\nbarnett \u00b7 comparative statistical inference, third edition\nbarnett and lewis \u00b7 outliers in statistical data, third edition\nbartoszynski and niewiadomska-bugaj \u00b7 probability and statistical inference\nbasilevsky \u00b7 statistical factor analysis and related methods: theory and\n\nsciences\n\napplications\n\nbasu and rigdon \u00b7 statistical methods for the reliability of repairable systems\nbates and watts \u00b7 nonlinear regression analysis and its applications\nbechhofer, santner, and goldsman \u00b7 design and analysis of experiments for\n\nstatistical selection, screening, and multiple comparisons\n\nbelsley \u00b7 conditioning diagnostics: collinearity and weak data in regression\nbelsley, kuh, and welsch \u00b7 regression diagnostics: identifying influential\n\ndata and sources of collinearity\n\nbendat and piersol \u00b7 random data: analysis and measurement procedures, \n\nthird edition\n\n*now available in a lower priced paperback edition in the wiley classics library.\n\n "}, {"Page_number": 727, "text": "p&s-cp.qxd  12/4/02  10:27 am  page 3\n\nberry, chaloner, and geweke \u00b7 bayesian analysis in statistics and\n\neconometrics: essays in honor of arnold zellner\n\nbernardo and smith \u00b7 bayesian theory\nbhat and miller \u00b7 elements of applied stochastic processes, third edition\nbhattacharya and johnson \u00b7 statistical concepts and methods\nbhattacharya and waymire \u00b7 stochastic processes with applications\nbillingsley \u00b7 convergence of probability measures, second edition\nbillingsley \u00b7 probability and measure, third edition\nbirkes and dodge \u00b7 alternative methods of regression\nblischke and murthy (editors) \u00b7 case studies in reliability and maintenance\nblischke and murthy \u00b7 reliability: modeling, prediction, and optimization\nbloomfield \u00b7 fourier analysis of time series: an introduction, second edition\nbollen \u00b7 structural equations with latent variables\nborovkov \u00b7 ergodicity and stability of stochastic processes\nbouleau \u00b7 numerical methods for stochastic processes\nbox \u00b7 bayesian inference in statistical analysis\nbox \u00b7 r. a. fisher, the life of a scientist\nbox and draper \u00b7 empirical model-building and response surfaces\n*box and draper \u00b7 evolutionary operation: a statistical method for process\n\nimprovement\n\nbox, hunter, and hunter \u00b7 statistics for experimenters: an introduction to\n\ndesign, data analysis, and model building\n\nbox and luce\u00f1o \u00b7 statistical control by monitoring and feedback adjustment\nbrandimarte \u00b7 numerical methods in finance: a matlab-based introduction\nbrown and hollander \u00b7 statistics: a biomedical introduction\nbrunner, domhof, and langer \u00b7 nonparametric analysis of longitudinal data in \n\nfactorial experiments\n\nbucklew \u00b7 large deviation techniques in decision, simulation, and estimation\ncairoli and dalang \u00b7 sequential stochastic optimization\nchan \u00b7 time series: applications to finance\nchatterjee and hadi \u00b7 sensitivity analysis in linear regression\nchatterjee and price \u00b7 regression analysis by example, third edition\nchernick \u00b7 bootstrap methods: a practitioner\u2019s guide\nchernick and friis \u00b7 introductory biostatistics for the health sciences\nchil\u00e8s and delfiner \u00b7 geostatistics: modeling spatial uncertainty\nchow and liu \u00b7 design and analysis of clinical trials: concepts and methodologies\nclarke and disney \u00b7 probability and random processes: a first course with\n\napplications, second edition\n\n*cochran and cox \u00b7 experimental designs, second edition\ncongdon \u00b7 bayesian statistical modelling\nconover \u00b7 practical nonparametric statistics, second edition\ncook \u00b7 regression graphics\ncook and weisberg \u00b7 applied regression including computing and graphics\ncook and weisberg \u00b7 an introduction to regression graphics\ncornell \u00b7 experiments with mixtures, designs, models, and the analysis of mixture\n\ndata, third edition\n\ncover and thomas \u00b7 elements of information theory\ncox \u00b7 a handbook of introductory statistical methods\n*cox \u00b7 planning of experiments\ncressie \u00b7 statistics for spatial data, revised edition\ncs\u00f6rgo\u00b4\u00b4 and horv\u00e1th \u00b7 limit theorems in change point analysis\ndaniel \u00b7 applications of statistics to industrial experimentation\ndaniel \u00b7 biostatistics: a foundation for analysis in the health sciences, sixth edition\n\n*now available in a lower priced paperback edition in the wiley classics library.\n\n "}, {"Page_number": 728, "text": "p&s-cp.qxd  12/4/02  10:27 am  page 4\n\n*daniel \u00b7 fitting equations to data: computer analysis of multifactor data, \n\nsecond edition\n\ndavid \u00b7 order statistics, second edition\n*degroot, fienberg, and kadane \u00b7 statistics and the law\ndel castillo \u00b7 statistical process adjustment for quality control\ndette and studden \u00b7 the theory of canonical moments with applications in\n\nstatistics, probability, and analysis\n\ndey and mukerjee \u00b7 fractional factorial plans\ndillon and goldstein \u00b7 multivariate analysis: methods and applications\ndodge \u00b7 alternative methods of regression\n*dodge and romig \u00b7 sampling inspection tables, second edition\n*doob \u00b7 stochastic processes\ndowdy and wearden \u00b7 statistics for research, second edition\ndraper and smith \u00b7 applied regression analysis, third edition\ndryden and mardia \u00b7 statistical shape analysis\ndudewicz and mishra \u00b7 modern mathematical statistics\ndunn and clark \u00b7 applied statistics: analysis of variance and regression, second\n\nedition\n\ndunn and clark \u00b7 basic statistics: a primer for the biomedical sciences, \n\nthird edition\n\ndupuis and ellis \u00b7 a weak convergence approach to the theory of large deviations\n*elandt-johnson and johnson \u00b7 survival models and data analysis\nethier and kurtz \u00b7 markov processes: characterization and convergence\nevans, hastings, and peacock \u00b7 statistical distributions, third edition\nfeller \u00b7 an introduction to probability theory and its applications, volume i,\n\nthird edition, revised; volume ii, second edition\n\nfisher and van belle \u00b7 biostatistics: a methodology for the health sciences\n*fleiss \u00b7 the design and analysis of clinical experiments\nfleiss \u00b7 statistical methods for rates and proportions, second edition\nfleming and harrington \u00b7 counting processes and survival analysis\nfuller \u00b7 introduction to statistical time series, second edition\nfuller \u00b7 measurement error models\ngallant \u00b7 nonlinear statistical models\nghosh, mukhopadhyay, and sen \u00b7 sequential estimation\ngifi \u00b7 nonlinear multivariate analysis\nglasserman and yao \u00b7 monotone structure in discrete-event systems\ngnanadesikan \u00b7 methods for statistical data analysis of multivariate observations,\n\nsecond edition\n\ngoldstein and lewis \u00b7 assessment: problems, development, and statistical issues\ngreenwood and nikulin \u00b7 a guide to chi-squared testing\ngross and harris \u00b7 fundamentals of queueing theory, third edition\n*hahn and shapiro \u00b7 statistical models in engineering\nhahn and meeker \u00b7 statistical intervals: a guide for practitioners\nhald \u00b7 a history of probability and statistics and their applications before 1750\nhald \u00b7 a history of mathematical statistics from 1750 to 1930\nhampel \u00b7 robust statistics: the approach based on influence functions\nhannan and deistler \u00b7 the statistical theory of linear systems\nheiberger \u00b7 computation for the analysis of designed experiments\nhedayat and sinha \u00b7 design and inference in finite population sampling\nheller \u00b7 macsyma for statisticians\nhinkelman and kempthorne: \u00b7 design and analysis of experiments, volume 1:\n\nintroduction to experimental design\n\nhoaglin, mosteller, and tukey \u00b7 exploratory approach to analysis\n\nof variance\n\n*now available in a lower priced paperback edition in the wiley classics library.\n\n "}, {"Page_number": 729, "text": "p&s-cp.qxd  12/4/02  10:27 am  page 5\n\nhoaglin, mosteller, and tukey \u00b7 exploring data tables, trends and shapes\n*hoaglin, mosteller, and tukey \u00b7 understanding robust and exploratory\n\ndata analysis\n\nhochberg and tamhane \u00b7 multiple comparison procedures\nhocking \u00b7 methods and applications of linear models: regression and the analysis\n\nof variance, second edition\n\nhoel \u00b7 introduction to mathematical statistics, fifth edition\nhogg and klugman \u00b7 loss distributions\nhollander and wolfe \u00b7 nonparametric statistical methods, second edition\nhosmer and lemeshow \u00b7 applied logistic regression, second edition\nhosmer and lemeshow \u00b7 applied survival analysis: regression modeling of\n\ntime to event data\n\nh\u00f8yland and rausand \u00b7 system reliability theory: models and statistical methods\nhuber \u00b7 robust statistics\nhuberty \u00b7 applied discriminant analysis\nhunt and kennedy \u00b7 financial derivatives in theory and practice\nhuskova, beran, and dupac \u00b7 collected works of jaroslav hajek\u2014\n\nwith commentary\n\niman and conover \u00b7 a modern approach to statistics\njackson \u00b7 a user\u2019s guide to principle components\njohn \u00b7 statistical methods in engineering and quality assurance\njohnson \u00b7 multivariate statistical simulation\njohnson and balakrishnan \u00b7 advances in the theory and practice of statistics: a\n\njudge, griffiths, hill, l\u00fctkepohl, and lee \u00b7 the theory and practice of\n\nvolume in honor of samuel kotz\n\neconometrics, second edition\n\njohnson and kotz \u00b7 distributions in statistics\njohnson and kotz (editors) \u00b7 leading personalities in statistical sciences: from the\n\nseventeenth century to the present\n\njohnson, kotz, and balakrishnan \u00b7 continuous univariate distributions,\n\njohnson, kotz, and balakrishnan \u00b7 continuous univariate distributions,\n\nvolume 1, second edition\n\nvolume 2, second edition\n\njohnson, kotz, and balakrishnan \u00b7 discrete multivariate distributions\njohnson, kotz, and kemp \u00b7 univariate discrete distributions, second edition\njurec\uf6f5 kov\u00e1 and sen \u00b7 robust statistical procedures: aymptotics and interrelations\njurek and mason \u00b7 operator-limit distributions in probability theory\nkadane \u00b7 bayesian methods and ethics in a clinical trial design\nkadane and schum \u00b7 a probabilistic analysis of the sacco and vanzetti evidence\nkalbfleisch and prentice \u00b7 the statistical analysis of failure time data, second\n\nedition\n\nanalysis\n\nkass and vos \u00b7 geometrical foundations of asymptotic inference\nkaufman and rousseeuw \u00b7 finding groups in data: an introduction to cluster\n\nkedem and fokianos \u00b7 regression models for time series analysis\nkendall, barden, carne, and le \u00b7 shape and shape theory\nkhuri \u00b7 advanced calculus with applications in statistics, second edition\nkhuri, mathew, and sinha \u00b7 statistical tests for mixed linear models\nklugman, panjer, and willmot \u00b7 loss models: from data to decisions\nklugman, panjer, and willmot \u00b7 solutions manual to accompany loss models: \n\nkotz, balakrishnan, and johnson \u00b7 continuous multivariate distributions,\n\nfrom data to decisions\n\nvolume 1, second edition\n\n*now available in a lower priced paperback edition in the wiley classics library.\n\n "}, {"Page_number": 730, "text": "p&s-cp.qxd  12/4/02  10:27 am  page 6\n\nkotz and johnson (editors) \u00b7 encyclopedia of statistical sciences: volumes 1 to 9\n\nkotz and johnson (editors) \u00b7 encyclopedia of statistical sciences: supplement\n\nkotz, read, and banks (editors) \u00b7 encyclopedia of statistical sciences: update\n\nkotz, read, and banks (editors) \u00b7 encyclopedia of statistical sciences: update\n\nwith index\n\nvolume\n\nvolume 1\n\nvolume 2\n\nkovalenko, kuznetzov, and pegg \u00b7 mathematical theory of reliability of\n\ntime-dependent systems with practical applications\n\nlachin \u00b7 biostatistical methods: the assessment of relative risks\nlad \u00b7 operational subjective statistical methods: a mathematical, philosophical, and\n\nhistorical introduction\n\nlamperti \u00b7 probability: a survey of the mathematical theory, second edition\nlange, ryan, billard, brillinger, conquest, and greenhouse \u00b7\n\ncase studies in biometry\n\nlarson \u00b7 introduction to probability theory and statistical inference, third edition\nlawless \u00b7 statistical models and methods for lifetime data, second edition\nlawson \u00b7 statistical methods in spatial epidemiology\nle \u00b7 applied categorical data analysis\nle \u00b7 applied survival analysis\nlee and wang \u00b7 statistical methods for survival data analysis, third edition\nlepage and billard \u00b7 exploring the limits of bootstrap\nleyland and goldstein (editors) \u00b7 multilevel modelling of health statistics\nliao \u00b7 statistical group comparison\nlindvall \u00b7 lectures on the coupling method\nlinhart and zucchini \u00b7 model selection\nlittle and rubin \u00b7 statistical analysis with missing data, second edition\nlloyd \u00b7 the statistical analysis of categorical data\nmagnus and neudecker \u00b7 matrix differential calculus with applications in\n\nstatistics and econometrics, revised edition\n\nmaller and zhou \u00b7 survival analysis with long term survivors\nmallows \u00b7 design, data, and analysis by some friends of cuthbert daniel\nmann, schafer, and singpurwalla \u00b7 methods for statistical analysis of\n\nreliability and life data\n\nmanton, woodbury, and tolley \u00b7 statistical applications using fuzzy sets\nmardia and jupp \u00b7 directional statistics\nmason, gunst, and hess \u00b7 statistical design and analysis of experiments with\n\napplications to engineering and science, second edition\n\nmcculloch and searle \u00b7 generalized, linear, and mixed models\nmcfadden \u00b7 management of data in clinical trials\nmclachlan \u00b7 discriminant analysis and statistical pattern recognition\nmclachlan and krishnan \u00b7 the em algorithm and extensions\nmclachlan and peel \u00b7 finite mixture models\nmcneil \u00b7 epidemiological research methods\nmeeker and escobar \u00b7 statistical methods for reliability data\nmeerschaert and scheffler \u00b7 limit distributions for sums of independent\n\nrandom vectors: heavy tails in theory and practice\n\n*miller \u00b7 survival analysis, second edition\nmontgomery, peck, and vining \u00b7 introduction to linear regression analysis,\n\nthird edition\n\nrobustness\n\nmorgenthaler and tukey \u00b7 configural polysampling: a route to practical\n\nmuirhead \u00b7 aspects of multivariate statistical theory\n\n*now available in a lower priced paperback edition in the wiley classics library.\n\n "}, {"Page_number": 731, "text": "p&s-cp.qxd  12/4/02  10:27 am  page 7\n\nmurray \u00b7 x-stat 2.0 statistical experimentation, design data analysis, and \n\nnonlinear optimization\n\nmyers and montgomery \u00b7 response surface methodology: process and product\n\noptimization using designed experiments, second edition\n\nmyers, montgomery, and vining \u00b7 generalized linear models. with\n\napplications in engineering and the sciences\n\nnelson \u00b7 accelerated testing, statistical models, test plans, and data analyses\nnelson \u00b7 applied life data analysis\nnewman \u00b7 biostatistical methods in epidemiology\nochi \u00b7 applied probability and stochastic processes in engineering and physical\n\nsciences\n\nokabe, boots, sugihara, and chiu \u00b7 spatial tesselations: concepts and\n\napplications of voronoi diagrams, second edition\n\noliver and smith \u00b7 influence diagrams, belief nets and decision analysis\npankratz \u00b7 forecasting with dynamic regression models\npankratz \u00b7 forecasting with univariate box-jenkins models: concepts and cases\n*parzen \u00b7 modern probability theory and its applications\npe\u00f1a, tiao, and tsay \u00b7 a course in time series analysis\npiantadosi \u00b7 clinical trials: a methodologic perspective\nport \u00b7 theoretical probability for applications\npourahmadi \u00b7 foundations of time series analysis and prediction theory\npress \u00b7 bayesian statistics: principles, models, and applications\npress \u00b7 subjective and objective bayesian statistics, second edition\npress and tanur \u00b7 the subjectivity of scientists and the bayesian approach\npukelsheim \u00b7 optimal experimental design\npuri, vilaplana, and wertz \u00b7 new perspectives in theoretical and applied\n\nstatistics\n\nputerman \u00b7 markov decision processes: discrete stochastic dynamic programming\n*rao \u00b7 linear statistical inference and its applications, second edition\nrencher \u00b7 linear models in statistics\nrencher \u00b7 methods of multivariate analysis, second edition\nrencher \u00b7 multivariate statistical inference with applications\nripley \u00b7 spatial statistics\nripley \u00b7 stochastic simulation\nrobinson \u00b7 practical strategies for experimenting\nrohatgi and saleh \u00b7 an introduction to probability and statistics, second edition\nrolski, schmidli, schmidt, and teugels \u00b7 stochastic processes for insurance\n\nand finance\n\nrosenberger and lachin \u00b7 randomization in clinical trials: theory and practice\nross \u00b7 introduction to probability and statistics for engineers and scientists\nrousseeuw and leroy \u00b7 robust regression and outlier detection\nrubin \u00b7 multiple imputation for nonresponse in surveys\nrubinstein \u00b7 simulation and the monte carlo method\nrubinstein and melamed \u00b7 modern simulation and modeling\nryan \u00b7 modern regression methods\nryan \u00b7 statistical methods for quality improvement, second edition\nsaltelli, chan, and scott (editors) \u00b7 sensitivity analysis\n*scheffe \u00b7 the analysis of variance\nschimek \u00b7 smoothing and regression: approaches, computation, and application\nschott \u00b7 matrix analysis for statistics\nschuss \u00b7 theory and applications of stochastic differential equations\nscott \u00b7 multivariate density estimation: theory, practice, and visualization\n*searle \u00b7 linear models\nsearle \u00b7 linear models for unbalanced data\nsearle \u00b7 matrix algebra useful for statistics\n\n*now available in a lower priced paperback edition in the wiley classics library.\n\n "}, {"Page_number": 732, "text": "p&s-cp.qxd  12/4/02  10:27 am  page 8\n\nsearle, casella, and mcculloch \u00b7 variance components\nsearle and willett \u00b7 matrix algebra for applied economics\nseber and lee \u00b7 linear regression analysis, second edition\nseber \u00b7 multivariate observations\nseber and wild \u00b7 nonlinear regression\nsennott \u00b7 stochastic dynamic programming and the control of queueing systems\n*serfling \u00b7 approximation theorems of mathematical statistics\nshafer and vovk \u00b7 probability and finance: it\u2019s only a game!\nsmall and mcleish \u00b7 hilbert space methods in probability and statistical inference\nsrivastava \u00b7 methods of multivariate statistics\nstapleton \u00b7 linear statistical models\nstaudte and sheather \u00b7 robust estimation and testing\nstoyan, kendall, and mecke \u00b7 stochastic geometry and its applications, second\n\nstoyan and stoyan \u00b7 fractals, random shapes and point fields: methods of\n\nstyan \u00b7 the collected papers of t. w. anderson: 1943\u20131985\nsutton, abrams, jones, sheldon, and song \u00b7 methods for meta-analysis in\n\nedition\n\ngeometrical statistics\n\nmedical research\n\ntanaka \u00b7 time series analysis: nonstationary and noninvertible distribution theory\nthompson \u00b7 empirical model building\nthompson \u00b7 sampling, second edition\nthompson \u00b7 simulation: a modeler\u2019s approach\nthompson and seber \u00b7 adaptive sampling\nthompson, williams, and findlay \u00b7 models for investors in real world markets\ntiao, bisgaard, hill, pe\u00f1a, and stigler (editors) \u00b7 box on quality and\n\ndiscovery: with design, control, and robustness\n\ntierney \u00b7 lisp-stat: an object-oriented environment for statistical computing\n\nand dynamic graphics\n\ntsay \u00b7 analysis of financial time series\nupton and fingleton \u00b7 spatial data analysis by example, volume ii:\n\ncategorical and directional data\n\nvan belle \u00b7 statistical rules of thumb\nvidakovic \u00b7 statistical modeling by wavelets\nweisberg \u00b7 applied linear regression, second edition\nwelsh \u00b7 aspects of statistical inference\nwestfall and young \u00b7 resampling-based multiple testing: examples and\n\nmethods for p-value adjustment\n\nwhittaker \u00b7 graphical models in applied multivariate statistics\nwinker \u00b7 optimization heuristics in economics: applications of threshold accepting\nwonnacott and wonnacott \u00b7 econometrics, second edition\nwooding \u00b7 planning pharmaceutical clinical trials: basic statistical principles\nwoolson and clarke \u00b7 statistical methods for the analysis of biomedical data,\n\nsecond edition\n\noptimization\n\nwu and hamada \u00b7 experiments: planning, analysis, and parameter design\n\nyang \u00b7 the construction theory of denumerable markov processes\n*zellner \u00b7 an introduction to bayesian inference in econometrics\nzhou, obuchowski, and mcclish \u00b7 statistical methods in diagnostic medicine\n\n*now available in a lower priced paperback edition in the wiley classics library.\n\n "}]}
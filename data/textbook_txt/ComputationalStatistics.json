{"Filename": "ComputationalStatistics", "Pages": [{"Page_number": 1, "text": "computational\nstatistics\n\n "}, {"Page_number": 2, "text": "wiley series in computational statistics\nconsulting editors:\npaolo giudici\nuniversity of pavia, italy\ngeof h. givens\ncolorado state university, usa\nbani k. mallick\ntexas a&m university, usa\n\nwiley series in computational statistics is comprised of practical guides and cutting\nedge research books on new developments in computational statistics. it features\nquality authors with a strong applications focus. the texts in the series provide\ndetailed coverage of statistical concepts, methods, and case studies in areas at the\ninterface of statistics, computing, and numerics.\nwith sound motivation and a wealth of practical examples, the books show in\nconcrete terms how to select and to use appropriate ranges of statistical computing\ntechniques in particular fields of study. readers are assumed to have a basic\nunderstanding of introductory terminology.\ntheseriesconcentratesonapplicationsofcomputationalmethodsinstatisticstofields\nof bioformatics, genomics, epidemiology, business, engineering, finance, and applied\nstatistics.\na complete list of titles in this series appears at the end of the volume\n\n "}, {"Page_number": 3, "text": "second edition\n\ncomputational\nstatistics\n\ngeof h. givens and jennifer a. hoeting\ndepartment of statistics, colorado state university, fort collins, co\n\n "}, {"Page_number": 4, "text": "copyright \u00a9 2013 by john wiley & sons, inc. all rights reserved.\n\npublished by john wiley & sons, inc., hoboken, new jersey.\npublished simultaneously in canada.\n\nno part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form\nor by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as\npermitted under section 107 or 108 of the 1976 united states copyright act, without either the prior\nwritten permission of the publisher, or authorization through payment of the appropriate per-copy fee to\nthe copyright clearance center, inc., 222 rosewood drive, danvers, ma 01923, (978) 750-8400, fax\n(978) 750-4470, or on the web at www.copyright.com. requests to the publisher for permission should\nbe addressed to the permissions department, john wiley & sons, inc., 111 river street, hoboken, nj\n07030, (201) 748-6011, fax (201) 748-6008, or online at http://www.wiley.com/go/permission.\n\nlimit of liability/disclaimer of warranty: while the publisher and author have used their best efforts in\npreparing this book, they make no representations or warranties with respect to the accuracy or\ncompleteness of the contents of this book and specifically disclaim any implied warranties of\nmerchantability or fitness for a particular purpose. no warranty may be created or extended by sales\nrepresentatives or written sales materials. the advice and strategies contained herein may not be suitable\nfor your situation. you should consult with a professional where appropriate. neither the publisher nor\nauthor shall be liable for any loss of profit or any other commercial damages, including but not limited to\nspecial, incidental, consequential, or other damages.\n\nfor general information on our other products and services or for technical support, please contact our\ncustomer care department within the united states at (800) 762-2974, outside the united states\nat (317) 572-3993 or fax (317) 572-4002.\n\nwiley also publishes its books in a variety of electronic formats. some content that appears in print may\nnot be available in electronic formats. for more information about wiley products, visit our web site at\nwww.wiley.com.\n\nlibrary of congress cataloging-in-publication data:\n\ngivens, geof h.\np. cm.\n\ncomputational statistics / geof h. givens, jennifer a. hoeting. \u2013 2nd ed.\nincludes index.\nisbn 978-0-470-53331-4 (cloth)\n1. mathematical statistics\u2013data processing.\n(jennifer ann), 1966\u2013\n\ni. hoeting, jennifer a.\n\nii. title.\n\nqa276.4.g58 2013\n519.5\u2013dc23\n\n2012017381\n\nprinted in the united states of america\n\n10 9 8 7 6 5 4 3 2 1\n\n "}, {"Page_number": 5, "text": "to natalie and neil\n\n "}, {"Page_number": 6, "text": "contents\n\npreface\nacknowledgments\n\n1\n\nreview\n1.1 mathematical notation\n1.2\n1.3\n1.4\n1.5\n1.6\n1.7 markov chains\n1.8\n\ncomputing\n\ntaylor\u2019s theorem and mathematical limit theory\nstatistical notation and probability distributions\nlikelihood inference\nbayesian inference\nstatistical limit theory\n\npart i\noptimization\n2 optimization and solving nonlinear equations\n\n2.1\n\nunivariate problems\n2.1.1 newton\u2019s method\n\n2.1.1.1 convergence order\n\n2.1.2 fisher scoring\n2.1.3 secant method\n2.1.4 fixed-point iteration\n\n2.1.4.1 scaling\n\n2.2 multivariate problems\n\n2.2.1 newton\u2019s method and fisher scoring\n\n2.2.1.1\n\niteratively reweighted least squares\n\n2.2.2 newton-like methods\n\n2.2.2.1 ascent algorithms\n2.2.2.2 discrete newton and fixed-point methods\n2.2.2.3 quasi-newton methods\n\nxv\nxvii\n\n1\n1\n2\n4\n9\n11\n13\n14\n17\n\n21\n22\n26\n29\n30\n30\n32\n33\n34\n34\n36\n39\n39\n41\n41\n\nvii\n\n "}, {"Page_number": 7, "text": "3.1\n\n3.2\n3.3\n\n3.4\n\n3.5\n\nviii\n\ncontents\n\n2.2.3 gauss\u2013newton method\n2.2.4 nelder\u2013mead algorithm\n2.2.5 nonlinear gauss\u2013seidel iteration\nproblems\n\n3 combinatorial optimization\n\nhard problems and np-completeness\n3.1.1 examples\n3.1.2 need for heuristics\nlocal search\nsimulated annealing\n3.3.1 practical issues\n\n3.3.1.1 neighborhoods and proposals\n3.3.1.2 cooling schedule and convergence\n\n3.3.2 enhancements\ngenetic algorithms\n3.4.1 definitions and the canonical algorithm\n\n3.4.1.1 basic definitions\n3.4.1.2 selection mechanisms and genetic operators\n3.4.1.3 allele alphabets and genotypic representation\n3.4.1.4\ninitialization, termination, and parameter values\n\n3.4.2.1 fitness\n3.4.2.2 selection mechanisms and updating generations\n3.4.2.3 genetic operators and permutation\n\n3.4.2 variations\n\nchromosomes\n\ninitialization and parameter values\n\n3.4.3\n3.4.4 convergence\ntabu algorithms\n3.5.1 basic definitions\n3.5.2 the tabu list\n3.5.3 aspiration criteria\n3.5.4 diversification\n3.5.5\nintensification\n3.5.6 comprehensive tabu algorithm\nproblems\n\n4\n\nem optimization methods\n4.1 missing data, marginalization, and notation\n4.2\n\nthe em algorithm\n4.2.1 convergence\n4.2.2 usage in exponential families\n\n44\n45\n52\n54\n\n59\n59\n61\n64\n65\n68\n70\n70\n71\n74\n75\n75\n75\n76\n78\n79\n80\n80\n81\n\n82\n84\n84\n85\n86\n87\n88\n89\n90\n91\n92\n\n97\n97\n98\n102\n105\n\n "}, {"Page_number": 8, "text": "4.2.3 variance estimation\n\n4.2.3.1 louis\u2019s method\n4.2.3.2 sem algorithm\n4.2.3.3 bootstrapping\n4.2.3.4 empirical information\n4.2.3.5 numerical differentiation\n\nimproving the e step\n4.3.1.1 monte carlo em\nimproving the m step\n4.3.2.1 ecm algorithm\n4.3.2.2 em gradient algorithm\n\n4.3\n\nem variants\n4.3.1\n\n4.3.2\n\n4.3.3 acceleration methods\n\n4.3.3.1 aitken acceleration\n4.3.3.2 quasi-newton acceleration\n\nproblems\n\npart ii\nintegration and simulation\n5 numerical integration\n\n5.1\n\n5.2\n5.3\n\n5.4\n\nnewton\u2013c\u02c6otes quadrature\n5.1.1 riemann rule\n5.1.2 trapezoidal rule\n5.1.3 simpson\u2019s rule\n5.1.4 general kth-degree rule\nromberg integration\ngaussian quadrature\n5.3.1 orthogonal polynomials\n5.3.2 the gaussian quadrature rule\nfrequently encountered problems\n5.4.1 range of integration\n5.4.2\n5.4.3 multiple integrals\n5.4.4 adaptive quadrature\n5.4.5 software for exact integration\nproblems\n\nintegrands with singularities or other extreme behavior\n\n6\n\nsimulation and monte carlo integration\n6.1\n6.2\n\nintroduction to the monte carlo method\nexact simulation\n\ncontents\n\nix\n106\n106\n108\n110\n110\n111\n111\n111\n111\n112\n113\n116\n118\n118\n119\n121\n\n129\n129\n130\n134\n136\n138\n139\n142\n143\n143\n146\n146\n146\n147\n147\n148\n148\n\n151\n151\n152\n\n "}, {"Page_number": 9, "text": "6.3\n\n6.4\n\n7.2\n\n7.3\n\nx\n\ncontents\n\n6.2.1 generating from standard parametric families\n6.2.2\n6.2.3 rejection sampling\n\ninverse cumulative distribution function\n\n6.2.3.1 squeezed rejection sampling\n6.2.3.2 adaptive rejection sampling\n\napproximate simulation\n6.3.1 sampling importance resampling algorithm\n\n6.3.1.1 adaptive importance, bridge, and path sampling\n\n6.3.2 sequential monte carlo\n\n6.3.2.1 sequential importance sampling for markov processes\n6.3.2.2 general sequential importance sampling\n6.3.2.3 weight degeneracy, rejuvenation, and effective\n\nsample size\n\n6.3.2.4 sequential importance sampling for hidden\n\nmarkov models\n6.3.2.5 particle filters\nvariance reduction techniques\n6.4.1\nimportance sampling\n6.4.2 antithetic sampling\n6.4.3 control variates\n6.4.4 rao\u2013blackwellization\nproblems\n\n7 markov chain monte carlo\n\n7.1 metropolis\u2013hastings algorithm\n\n7.1.1\nindependence chains\n7.1.2 random walk chains\ngibbs sampling\n7.2.1 basic gibbs sampler\n7.2.2 properties of the gibbs sampler\n7.2.3 update ordering\n7.2.4 blocking\n7.2.5 hybrid gibbs sampling\n7.2.6 griddy\u2013gibbs sampler\nimplementation\n7.3.1 ensuring good mixing and convergence\n7.3.1.1 simple graphical diagnostics\n7.3.1.2 burn-in and run length\n7.3.1.3 choice of proposal\n7.3.1.4 reparameterization\n7.3.1.5 comparing chains: effective sample size\n7.3.1.6 number of chains\n\n153\n153\n155\n158\n159\n163\n163\n167\n168\n169\n170\n\n171\n\n175\n179\n180\n180\n186\n189\n193\n195\n\n201\n202\n204\n206\n209\n209\n214\n216\n216\n216\n218\n218\n219\n219\n220\n222\n223\n224\n225\n\n "}, {"Page_number": 10, "text": "7.3.2 practical implementation advice\n7.3.3 using the results\nproblems\n\ncontents\n\nadvanced topics in mcmc\n8.1\n\nadaptive mcmc\n8.1.1 adaptive random walk metropolis-within-gibbs algorithm\n8.1.2 general adaptive metropolis-within-gibbs algorithm\n8.1.3 adaptive metropolis algorithm\nreversible jump mcmc\n8.2.1 rjmcmc for variable selection in regression\nauxiliary variable methods\n8.3.1 simulated tempering\n8.3.2 slice sampler\nother metropolis\u2013hastings algorithms\n8.4.1 hit-and-run algorithm\n8.4.2 multiple-try metropolis\u2013hastings algorithm\n8.4.3 langevin metropolis\u2013hastings algorithm\nperfect sampling\n8.5.1 coupling from the past\n\n8.5.1.1 stochastic monotonicity and sandwiching\n\n8.6 markov chain maximum likelihood\n8.7\n\nexample: mcmc for markov random fields\n8.7.1 gibbs sampling for markov random fields\n8.7.2 auxiliary variable methods for markov random fields\n8.7.3 perfect sampling for markov random fields\nproblems\n\npart iii\nbootstrapping\nbootstrapping\n9.1\n9.2\n\nthe bootstrap principle\nbasic methods\n9.2.1 nonparametric bootstrap\n9.2.2 parametric bootstrap\n9.2.3 bootstrapping regression\n9.2.4 bootstrap bias correction\nbootstrap inference\n9.3.1 percentile method\n\n9.3.1.1\n9.3.2 pivoting\n\njustification for the percentile method\n\n8\n\n9\n\n8.2\n\n8.3\n\n8.4\n\n8.5\n\n9.3\n\nxi\n226\n226\n230\n\n237\n237\n238\n240\n247\n250\n253\n256\n257\n258\n260\n260\n261\n262\n264\n264\n267\n268\n269\n270\n274\n277\n279\n\n287\n287\n288\n288\n289\n290\n291\n292\n292\n293\n294\n\n "}, {"Page_number": 11, "text": "xii\n\ncontents\n\n9.3.2.1 accelerated bias-corrected percentile method, bca\n9.3.2.2 the bootstrap t\n9.3.2.3 empirical variance stabilization\n9.3.2.4 nested bootstrap and prepivoting\n\n9.3.3 hypothesis testing\nreducing monte carlo error\n9.4.1 balanced bootstrap\n9.4.2 antithetic bootstrap\nbootstrapping dependent data\n9.5.1 model-based approach\n9.5.2 block bootstrap\n\n9.5.2.1 nonmoving block bootstrap\n9.5.2.2 moving block bootstrap\n9.5.2.3 blocks-of-blocks bootstrapping\n9.5.2.4 centering and studentizing\n9.5.2.5 block size\n\nbootstrap performance\n9.6.1\nindependent data case\n9.6.2 dependent data case\nother uses of the bootstrap\npermutation tests\nproblems\n\n9.4\n\n9.5\n\n9.6\n\n9.7\n9.8\n\npart iv\ndensity estimation and smoothing\n10 nonparametric density estimation\n\n10.1 measures of performance\n10.2 kernel density estimation\n\n10.2.1 choice of bandwidth\n\n10.2.1.1 cross-validation\n10.2.1.2 plug-in methods\n10.2.1.3 maximal smoothing principle\n\n10.2.2 choice of kernel\n\n10.2.2.1 epanechnikov kernel\n10.2.2.2 canonical kernels and rescalings\n\n10.3 nonkernel methods\n\n10.3.1 logspline\n\n10.4 multivariate methods\n\n10.4.1 the nature of the problem\n\n294\n296\n298\n299\n301\n302\n302\n302\n303\n304\n304\n304\n306\n307\n309\n311\n315\n315\n316\n316\n317\n319\n\n325\n326\n327\n329\n332\n335\n338\n339\n339\n340\n341\n341\n345\n345\n\n "}, {"Page_number": 12, "text": "contents\n\n10.4.2 multivariate kernel estimators\n10.4.3 adaptive kernels and nearest neighbors\n10.4.3.1 nearest neighbor approaches\n10.4.3.2 variable-kernel approaches and transformations\n\n10.4.4 exploratory projection pursuit\nproblems\n\n11\n\nbivariate smoothing\n11.1 predictor\u2013response data\n11.2 linear smoothers\n\n11.2.1 constant-span running mean\n\n11.2.1.1 effect of span\n11.2.1.2 span selection for linear smoothers\n\n11.2.2 running lines and running polynomials\n11.2.3 kernel smoothers\n11.2.4 local regression smoothing\n11.2.5 spline smoothing\n\n11.2.5.1 choice of penalty\n11.3 comparison of linear smoothers\n11.4 nonlinear smoothers\n\n11.4.1 loess\n11.4.2 supersmoother\n\n11.5 confidence bands\n11.6 general bivariate data\n\nproblems\n\n12 multivariate smoothing\n12.1 predictor\u2013response data\n12.1.1 additive models\n12.1.2 generalized additive models\n12.1.3 other methods related to additive models\n\n12.1.3.1 projection pursuit regression\n12.1.3.2 neural networks\n12.1.3.3 alternating conditional expectations\n12.1.3.4 additivity and variance stabilization\n\n12.1.4 tree-based methods\n\n12.1.4.1 recursive partitioning regression trees\n12.1.4.2 tree pruning\n12.1.4.3 classification trees\n12.1.4.4 other issues for tree-based methods\n\nxiii\n346\n348\n349\n350\n353\n359\n\n363\n363\n365\n366\n368\n369\n372\n374\n374\n376\n377\n377\n379\n379\n381\n384\n388\n389\n\n393\n393\n394\n397\n399\n399\n402\n403\n404\n405\n406\n409\n411\n412\n\n "}, {"Page_number": 13, "text": "xiv\n\ncontents\n\n12.2 general multivariate data\n12.2.1 principal curves\n\n12.2.1.1 definition and motivation\n12.2.1.2 estimation\n12.2.1.3 span selection\n\nproblems\n\ndata acknowledgments\nreferences\nindex\n\n413\n413\n413\n415\n416\n416\n\n421\n423\n457\n\n "}, {"Page_number": 14, "text": "preface\n\nthis book covers most topics needed to develop a broad and thorough working knowl-\nedgeofmodern computationalstatistics.weseek to developapracticalunderstanding\nof how and why existing methods work, enabling readers to use modern statistical\nmethods effectively. since many new methods are built from components of exist-\ning techniques, our ultimate goal is to provide scientists with the tools they need to\ncontribute new ideas to the field.\na growing challenge in science is that there is so much of it. while the pursuit\nof important new methods and the honing of existing approaches is a worthy goal,\nthere is also a need to organize and distill the teeming jungle of ideas. we attempt to\ndo that here. our choice of topics reflects our view of what constitutes the core of the\nevolving field of computational statistics, and what will be interesting and useful for\nour readers.\nour use of the adjective modern in the first sentence of this preface is potentially\ntroublesome:thereisnowaythatthisbookcancoverallthelatest,greatesttechniques.\nwe have not even tried. we have instead aimed to provide a reasonably up-to-date\nsurvey of a broad portion of the field, while leaving room for diversions and esoterica.\nthe foundations of optimization and numerical integration are covered in this\nbook. we include these venerable topics because (i) they are cornerstones of frequen-\ntist and bayesian inference; (ii) routine application of available software often fails\nfor hard problems; and (iii) the methods themselves are often secondary components\nof other statistical computing algorithms. some topics we have omitted represent\nimportant areas of past and present research in the field, but their priority here is\nlowered by the availability of high-quality software. for example, the generation of\npseudo-random numbers is a classic topic, but one that we prefer to address by giv-\ning students reliable software. finally, some topics (e.g., principal curves and tabu\nsearch) are included simply because they are interesting and provide very different\nperspectives on familiar problems. perhaps a future researcher may draw ideas from\nsuch topics to design a creative and effective new algorithm.\nin this second edition, we have both updated and broadened our coverage, and\nwenowprovidecomputercode.forexample,wehaveaddednewmcmctopicstore-\nflect continued activity in that popular area. a notable increase in breadth is our inclu-\nsionofmoremethodsrelevantforproblemswherestatisticaldependencyisimportant,\nsuch as block bootstrapping and sequential importance sampling. this second edition\nprovides extensive new support in r. specifically, code for the examples in this book\nis available from the book website www.stat.colostate.edu/computationalstatistics.\nour target audience includes graduate students in statistics and related fields,\nstatisticians, and quantitative empirical scientists in other fields. we hope such readers\nmay use the book when applying standard methods and developing new methods.\n\nxv\n\n "}, {"Page_number": 15, "text": "xvi\n\npreface\nthe level of mathematics expected of the reader does not extend much beyond\ntaylor series and linear algebra. breadth of mathematical training is more helpful\nthan depth. essential review is provided in chapter 1. more advanced readers will find\ngreater mathematical detail in the wide variety of high-quality books available on spe-\ncific topics, many of which are referenced in the text. other readers caring less about\nanalytical details may prefer to focus on our descriptions of algorithms and examples.\nthe expected level of statistics is equivalent to that obtained by a graduate\nstudent in his or her first year of study of the theory of statistics and probability.\nan understanding of maximum likelihood methods, bayesian methods, elementary\nasymptotic theory, markov chains, and linear models is most important. many of\nthese topics are reviewed in chapter 1.\nwith respect to computer programming, we find that good students can learn as\ntheygo.however,aworkingknowledgeofasuitablelanguageallowsimplementation\nof the ideas covered in this book to progress much more quickly. we have chosen\nto forgo any language-specific examples, algorithms, or coding in the text. for those\nwishingtolearnalanguagewhiletheystudythisbook,werecommendthatyouchoose\na high-level, interactive package that permits the flexible design of graphical displays\nandincludessupportingstatisticsandprobabilityfunctions,suchasrandmatlab.1\nthese are the sort of languages often used by researchers during the development of\nnew statistical computing techniques, and they are suitable for implementing all the\nmethods we describe, except in some cases for problems of vast scope or complexity.\nwe use r and recommend it. although lower-level languages such as c++ could\nalso be used, they are more appropriate for professional-grade implementation of\nalgorithms after researchers have refined the methodology.\nthe book is organized into four major parts: optimization (chapters 2, 3, and\n4), integration and simulation (chapters 5, 6, 7, and 8), bootstrapping (chapter 9)\nand density estimation and smoothing (chapters 10, 11, and 12). the chapters are\nwritten to stand independently, so a course can be built by selecting the topics one\nwishes to teach. for a one-semester course, our selection typically weights most\nheavily topics from chapters 2, 3, 6, 7, 9, 10, and 11. with a leisurely pace or more\nthorough coverage, a shorter list of topics could still easily fill a semester course.\nthere is sufficient material here to provide a thorough one-year course of study,\nnotwithstanding any supplemental topics one might wish to teach.\na variety of homework problems are included at the end of each chapter. some\nare straightforward, while others require the student to develop a thorough under-\nstanding of the model/method being used, to carefully (and perhaps cleverly) code a\nsuitabletechnique,andtodevoteconsiderableattentionto theinterpretationofresults.\na few exercises invite open-ended exploration of methods and ideas. we are some-\ntimes asked for solutions to the exercises, but we prefer to sequester them to preserve\nthe challenge for future students and readers.\nthedatasetsdiscussedintheexamplesandexercisesareavailablefromthebook\nwebsite, www.stat.colostate.edu/computationalstatistics. the r code is also provided\nthere. finally, the website includes an errata. responsibility for all errors lies with us.\n\n1r  is  available  for  free  from  www.r-project.org.  information  about  matlab  can  be  found  at\nwww.mathworks.com.\n\n "}, {"Page_number": 16, "text": "acknowledgments\n\nthe course upon which this book is based was developed and taught by us at colorado\nstate university from 1994 onwards. thanks are due to our many students who have\nbeen semiwilling guinea pigs over the years. we also thank our colleagues in the\nstatistics department for their continued support. the late richard tweedie merits\nparticular acknowledgment for his mentoring during the early years of our careers.\nwe owe a great deal of intellectual debt to adrian raftery, who deserves special\nthanks not only for his teaching and advising, but also for his unwavering support and\nhis seemingly inexhaustible supply of good ideas. in addition, we thank our influential\nadvisorsandteachersattheuniversityofwashingtonstatisticsdepartment,including\ndavid madigan, werner stuetzle, and judy zeh. of course, each of our chapters could\nbe expanded into a full-length book, and great scholars have already done so. we owe\nmuch to their efforts, upon which we relied when developing our course and our\nmanuscript.\nportions of the first edition were written at the department of mathematics\nand statistics, university of otago, in dunedin, new zealand, whose faculty we\nthank for graciously hosting us during our sabbatical in 2003. much of our work\non the second edition was undertaken during our sabbatical visit to the australia\ncommonwealth scientific and research organization in 2009\u20132010, sponsored by\ncsiro mathematics, informatics and statistics, and hosted at the longpocket lab-\noratory in indooroopilly, australia. we thank our hosts and colleagues there for their\nsupport.\nour manuscript has been greatly improved through the constructive reviews\nof john bickham, ben bird, kate cowles, jan hannig, alan herlihy, david hunter,\ndevin johnson, michael newton, doug nychka, steve sain, david w. scott, n. scott\nurquhart, haonan wang, darrell whitley, and eight anonymous referees. we also\nthank the sharp-eyed readers listed in the errata for their suggestions and corrections.\nour editor steve quigley and the folks at wiley were supportive and helpful during\nthe publication process. we thank n\u00b4elida pohl for permission to adapt her photograph\nin the cover design of the first edition. we thank melinda stelzer for permission to use\nher painting \u201cchampagne circuit,\u201d 2001, for the cover of the second edition. more\nabout her art can be found at www.facebook.com/geekchicart. we also owe special\nnote of thanks to zube (a.k.a. john dzubera), who kept our own computers running\ndespite our best efforts to the contrary.\nfunding from national science foundation (nsf) career grant #sbr-\n9875508 was a significant source of support for the first author during the preparation\nof the first edition. he also thanks his colleagues and friends in the north slope\nborough, alaska, department of wildlife management for their longtime research\nsupport. the second author gratefully acknowledges the support of star research\n\nxvii\n\n "}, {"Page_number": 17, "text": "acknowledgments\n\nxviii\nassistance agreement cr-829095 awarded to colorado state university by the u.s.\nenvironmental protection agency (epa). the views expressed here are solely those\nof the authors. nsf and epa do not endorse any products or commercial services\nmentioned herein.\nfinally,wethankourparentsforenablingandsupportingoureducationsandfor\nproviding us with the \u201cstubbornness gene\u201d necessary for graduate school, the tenure\ntrack, or book publication\u2014take your pick! the second edition is dedicated to our\nkids, natalie and neil, for continuing to show us what is important and what is not.\n\ngeof h. givens\njennifer a. hoeting\n\n "}, {"Page_number": 18, "text": "chapter 1\nreview\n\nthis chapter reviews notation and background material in mathematics, probability,\nand statistics. readers may wish to skip this chapter and turn directly to chapter 2,\nreturning here only as needed.\n\n1.1 mathematical notation\nwe use boldface to distinguish a vector x = (x1, . . . , xp) or a matrix m from a scalar\nvariable x or a constant m. a vector-valued function f evaluated at x is also boldfaced,\nas in f(x) = (f1(x), . . . , fp(x)). the transpose of m is denoted mt.\nunless otherwise specified, all vectors are considered to be column vectors, so,\nfor example, an n \u00d7 p matrix can be written as m = (x1 . . . xn)t. let i denote an\nidentity matrix, and 1 and 0 denote vectors of ones and zeros, respectively.\na symmetric square matrix m is positive definite if xtmx > 0 for all nonzero\nvectors x. positive definiteness is equivalent to the condition that all eigenvalues of\nm are positive. m is nonnegative definite or positive semidefinite if xtmx \u2265 0 for all\nnonzero vectors x.\nthe derivative of a function f, evaluated at x, is denoted f\u2032(x). when x =\n(x1, . . . , xp), the gradient of f at x is\n\nf\u2032(x) =! df(x)\n\ndx1\n\n, . . . ,\n\ndf(x)\n\ndxp \".\n\nthe hessian matrix for f at x is f\u2032\u2032(x) having (i, j)th element equal to d2f(x)/\n(dxi dxj). the negative hessian has important uses in statistical inference.\nlet j(x) denote the jacobian matrix evaluated at x for the one-to-one mapping\ny = f(x). the (i, j)th element of j(x) is equal to dfi(x)/dxj.\na functional is a real-valued function on a space of functions. for example, if\nt(f) =# 1\n0 f(x) dx, then the functional t maps suitably integrable functions onto the\nreal line.\nthe indicator function 1{a} equals 1 if a is true and 0 otherwise. the real line\nis denoted \u211c, and p-dimensional real space is \u211cp.\n\ncomputational statistics, second edition. geof h. givens and jennifer a. hoeting.\n\u00a9 2013 john wiley & sons, inc. published 2013 by john wiley & sons, inc.\n\n1\n\n "}, {"Page_number": 19, "text": "2\n\nchapter 1 review\n\nf(z) = o(g(z))\n\n1.2 taylor\u2019s theorem and mathematical\nlimit theory\nfirst, we define standard \u201cbig oh\u201d and \u201clittle oh\u201d notation for describing the relative\nordersofconvergenceoffunctions.letthefunctions f and gbedefinedonacommon,\npossibly infinite interval. let z0 be a point in this interval or a boundary point of it\n(i.e., \u2212\u221e or \u221e). we require g(z) /= 0 for all z /= z0 in a neighborhood of z0. then\nwe say\n(1.1)\nif there exists a constant m such that |f(z)| \u2264 m|g(z)| as z \u2192 z0. for example,\n(n + 1)/(3n2) = o(n\u22121), and it is understood that we are considering n \u2192 \u221e. if\nlimz\u2192z0 f(z)/g(z) = 0, then we say\n(1.2)\nfor example, f(x0 + h) \u2212 f(x0) = hf\u2032(x0) + o(h) as h \u2192 0 if f is differentiable at\nx0. the same notation can be used for describing the convergence of a sequence {xn}\nas n \u2192 \u221e, by letting f(n) = xn.\ntaylor\u2019stheoremprovidesapolynomialapproximationtoafunction f.suppose\nf hasfinite(n + 1)thderivativeon(a, b)andcontinuous nth derivativeon[a, b].then\nfor any x0 \u2208 [a, b] distinct from x, the taylor series expansion of f about x0 is\n\nf(z) = o(g(z)).\n\n(1.3)\n\nn$i=0\n\nf(x) =\n\n1\ni! f (i)(x0)(x \u2212 x0)i + rn,\nwhere f (i)(x0) is the ith derivative of f evaluated at x0, and\n1\n(n + 1)! f (n+1)(\u03be)(x \u2212 x0)n+1\n\n(1.4)\nfor some point \u03be in the interval between x and x0. as |x \u2212 x0| \u2192 0, note that rn =\no(|x \u2212 x0|n+1).\nthe multivariate version of taylor\u2019s theorem is analogous. suppose f is a\nreal-valued function of a p-dimensional variable x, possessing continuous partial\nderivatives of all orders up to and including n + 1 with respect to all coordinates, in\nan open convex set containing x and x0 /= x. then\n\nrn =\n\nwhere\n\nf(x) = f(x0) +\n\nd(i)(f;x,y) =\n\np$j1=1\u00b7\u00b7\u00b7\n\nn$i=1\np$ji=1%!\n\n1\ni! d(i)(f;x0,x \u2212 x0) + rn,\n\ndi\n\ndtj1 \u00b7\u00b7\u00b7 dtji\n\nyjk(\n\nf(t)&&&&t=x\" i\u2019k=1\n\n(1.5)\n\n(1.6)\n\n "}, {"Page_number": 20, "text": "1.2 taylor\u2019s theorem and mathematical limit theory\n\n3\n\nand\n\nrn =\n\n1\n(n + 1)! d(n+1)(f; \u03be,x \u2212 x0)\n\n(1.7)\nfor some \u03be on the line segment joining x and x0. as |x \u2212 x0| \u2192 0, note that rn =\no(|x \u2212 x0|n+1).\nthe euler\u2013maclaurin formula is useful in many asymptotic analyses. if f has\n2n continuous derivatives in [0,1], then\n\n) 1\n\n0\n\nf(x) dx =\n\n2\n\nf(0) + f(1)\nn\u22121$i=0\n\n\u2212\n\nb2i(f (2i\u22121)(1) \u2212 f (2i\u22121)(0))\n\n(2i)!\n\nb2nf (2n)(\u03be)\n\n(2n)!\n\n,\n\n\u2212\n\n(1.8)\n\nwhere 0 \u2264 \u03be \u2264 1, f (j) is the jth derivative of f, and bj = bj(0) can be determined\nusing the recursion relation\n\nm$j=0* m + 1\n\nj + bj(z) = (m + 1)zm\n\n(1.9)\n\ninitialized with b0(z) = 1. the proof of this result is based on repeated integrations\nby parts [376].\nfinally, we note that it is sometimes desirable to approximate the derivative of\na function numerically, using finite differences. for example, the ith component of\nthe gradient of f at x can be approximated by\n\ndf(x)\ndxi \u2248\n\nf(x + \u03f5iei) \u2212 f(x \u2212 \u03f5iei)\n\n2\u03f5i\n\n,\n\n(1.10)\n\nwhere \u03f5i is a small number and ei is the unit vector in the ith coordinate direction.\ntypically, one might start with, say, \u03f5i = 0.01 or 0.001 and approximate the desired\nderivative for a sequence of progressively smaller \u03f5i. the approximation will\ngenerally improve until \u03f5i becomes small enough that the calculation is degraded and\neventually dominated by computer roundoff error introduced by subtractive cancel-\nlation. introductory discussion of this approach and a more sophisticated richardson\nextrapolation strategy for obtaining greater precision are provided in [376]. finite\ndifferences can also be used to approximate the second derivative of f at x via\n\ndf(x)\ndxi dxj \u2248\n\n1\n\n4\u03f5i\u03f5j!f(x + \u03f5iei + \u03f5jej) \u2212 f(x + \u03f5iei \u2212 \u03f5jej)\n\n\u2212 f(x \u2212 \u03f5iei + \u03f5jej) + f(x \u2212 \u03f5iei \u2212 \u03f5jej)\"\n\nwith similar sequential precision improvements.\n\n(1.11)\n\n "}, {"Page_number": 21, "text": "4\n\nchapter 1 review\n\n1.3 statistical notation and probability\ndistributions\nwe use capital letters to denote random variables, such as y or x, and lowercase\nletters to represent specific realized values of random variables such as y or x. the\nprobability density function of x is denoted f; the cumulative distribution function\nis f. we use the notation x \u223c f(x) to mean that x is distributed with density f(x).\nfrequently, the dependence of f(x) on one or more parameters also will be denoted\nwith a conditioning bar, as in f(x|\u03b1, \u03b2). because of the diversity of topics covered in\nthis book,wewanttobecarefultodistinguish when f(x|\u03b1)referstoadensityfunction\nas opposed to the evaluation of that density at a point x. when the meaning is unclear\nfrom the context, we will be explicit, for example, by using f(\u00b7|\u03b1) to denote the\nfunction. when it is important to distinguish among several densities, we may adopt\nsubscripts referring to specific random variables, so that the density functions for\nx and y are fx and fy, respectively. we use the same notation for distributions of\ndiscrete random variables and in the bayesian context.\nthe conditional distribution of x given that y equals y (i.e., x|y = y) is de-\nscribed by the density denoted f(x|y), or fx|y(x|y). in this case, we write that x|y has\ndensity f(x|y). for notational simplicity we allow density functions to be implicitly\nspecified by their arguments, so we may use the same symbol, say f, to refer to many\ndistinct functions, as in the equation f(x, y|\u00b5) = f(x|y, \u00b5)f(y|\u00b5). finally, f(x) and\nf(x) are random variables: the evaluations of the density and cumulative distribution\nfunctions, respectively, at the random argument x.\nthe expectation of a random variable is denoted e{x}. unless specifically\nmentioned, the distribution with respect to which an expectation is taken is the dis-\ntribution of x or should be implicit from the context. to denote the probability of\nan event a, we use p[a] = e{1{a}}. the conditional expectation of x|y = y is\ne{x|y}. when y is unknown, e{x|y} is a random variable that depends on y. other\nattributes of the distribution of x and y include var{x}, cov{x, y}, cor{x, y}, and\ncv{x} = var{x}1/2/e{x}. these quantities are the variance of x, the covariance and\ncorrelation of x and y, and the coefficient of variation of x, respectively.\na useful result regarding expectations is jensen\u2019s inequality. let g be a convex\nfunction on a possibly infinite open interval i, so\n\ng(\u03bbx + (1 \u2212 \u03bb)y) \u2264 \u03bbg(x) + (1 \u2212 \u03bb)g(y)\n\n(1.12)\nfor all x, y \u2208 i and all 0 < \u03bb < 1. then jensen\u2019s inequality states that e{g(x)} \u2265\ng(e{x}) for any random variable x having p[x \u2208 i] = 1.\ntables 1.1, 1.2, and 1.3 provide information about many discrete and contin-\nuous distributions used throughout this book. we refer to the following well-known\ncombinatorial constants:\n\nn! = n(n \u2212 1)(n \u2212 2)\u00b7\u00b7\u00b7(3)(2)(1) with 0! = 1,\n* n\nk+ =\n\nk!(n \u2212 k)! ,\n\nn!\n\n(1.13)\n(1.14)\n\n "}, {"Page_number": 22, "text": ".\ns\ne\nl\nb\na\ni\nr\na\nv\nm\no\nd\nn\na\nr\n\ne\nt\ne\nr\nc\ns\ni\nd\n\nf\n\no\n\ns\nn\no\n\ni\nt\n\nu\nb\ni\nr\nt\ns\ni\n\nd\n\ny\nt\ni\nl\ni\n\nb\na\nb\no\nr\np\n\nn\no\nm\nm\no\nc\n\nr\no\n\nf\n\nn\no\ni\nt\np\ni\nr\nc\ns\ne\nd\n\nd\nn\na\nn\no\ni\nt\na\nt\no\nn\n\n1\n.\n1\n\ne\nl\nb\na\nt\n\n)\np\n\u2212\n\n1\n(\np\n=\n\np\n=\n\n}\n\n)\np\n\u2212\n\n1\n(\np\nn\n=\n\np\nn\n=\n\n}\n\nd\nn\na\n\nn\na\ne\n\nm\n\ne\nc\nn\na\ni\nr\na\nv\n\n{\n\n}\n\nx\n\nx\n\n{\ne\n\nr\na\nv\n\n{\n\n}\n\nx\n\nx\n\n{\ne\n\nr\na\nv\n\nd\nn\na\ny\nt\ni\ns\nn\ne\nd\n\ne\nc\na\np\ns\ne\nl\np\nm\na\ns\n\nx\n\u2212\n\n1\n)\np\n\u2212\n\n1\n(\n\nx\n\u2212\nn\n\n)\np\n\u2212\n\n1\n(\n\nx\np\n\n\"\nn\n.\n\nn x\n\n.\n\n2\n\np\n/\n)\np\n\u2212\n\np\n/\n)\np\n\u2212\n\n1\n(\nr\n\n1\n(\nr\n\n=\n\n=\n\n}\n\n{\n\n}\n\nx\n\nx\n\n{\ne\n\nr\na\nv\n\nx\n\n)\np\n\u2212\n\n1\n(\n\nr\n\np\n\n\"\n\n1\n\n\u2212\n\n1\n\nj\n\ni\n\np\np\nn\n\u2212\n=\n\n}\n\n)\ni\np\n\u2212\n\n1\n(\ni\np\nn\n=\n\nj\n\nx\n\n,\ni\n\n}\n\np\nn\n=\n\nx\n\n}\n\n{\n\n{\n\ni\n\nx\n\nx\nv\no\nc\n\n{\ne\n\nr\na\nv\n\n}\nn\n.\n\n.\n\n.\n\ni\n\ni\n\nx\np\n1\n\n=\n\nk i\n\n,\n1\n,\n0\n\n-\n{\n\u2208\n\n\"\nx\n\ni\n\nk\nx\n\nn\n\n.\n\n.\n\n.\n\nd\nn\na\n\n)\nk\nx\n,\n\n1\nx\n\n.\n\n.\n\nr\n\n\u2212\n\nr\n\n+\nx\n\n}\n.\n\n.\n\n.\n\nn\n=\n\n1\n\nx\np\n=\n\nr\no\n\n0\n\n.\n\n!\n,\n1\n,\n0\n\n=\n\n)\nx\n(\nf\n\n=\nx\n\n)\nx\n(\nf\n\n=\nx\n\n!\n\n.\n\n=\n\n,\n1\nx\n(\n\ni\n\nx\n1\n\n!\n,\n1\n,\n0\n\n=\n\n)\nx\n(\nf\n\n=\n\nk i\n\n=\n,\n\nx\n\n)\nx\n(\nf\n\n{\n\u2208\nx\n\nd\nn\na\n\nn\no\ni\nt\na\nt\no\nn\n\ne\nc\na\np\ns\nr\ne\nt\ne\nm\na\nr\na\np\n\n)\np\n(\ni\nl\nl\nu\no\nn\nr\ne\nb\n\n1\n\n\u2264\np\n\u2264\n\n\u223c\nx\n\n0\n\n}\n.\n\n.\n\n.\n\n)\np\n,\nn\n(\nn\ni\nb\n\n1\n\n,\n2\n,\n1\n\n\u2264\np\n\u2264\n\n{\n\u2208\nn\n\n\u223c\nx\n\n0\n\n}\n.\n\n.\n\n.\n\n,\n2\n,\n1\n\n)\np\n,\nn\n(\nl\na\ni\nm\no\nn\ni\nt\nl\nu\nm\n\n{\n\u2208\nn\n)\nk\nd\np\nn\n,\na\n1\n\n.\n\n.\n\n.\n\n,\n1\np\n(\n\n\u223c\n\n=\n\nx\n\np\n\n0\n\n1\n\n=\n\n)\np\n,\nr\n(\nn\ni\nb\ng\ne\nn\np\n\u223c\n\u2264\n,\nx\n\np\n1\n\n\u2264\n\n=\n\nk i\n\ni\n\ni\n\n}\n.\n\n.\n\n.\n\n1\n\n,\n2\n,\n1\n\n\u2264\np\n\u2264\n\n{\n\u2208\nr\n\n0\n\n\u03bb\n=\n\n}\n\nx\n\n{\n\n\u03bb\n=\n\n}\n\nx\n\n{\ne\n\nr\na\nv\n\n}\n\u03bb\n\u2212\n\n{\n\np\nx\ne\n\n}\n.\n\n.\n\n.\n\nx\n\u03bb\n\n!\nx\n\n,\n1\n,\n0\n\n=\n\n)\nx\n(\nf\n\n{\n\u2208\nx\n\n)\n\u03bb\n(\nn\no\ns\ns\ni\no\np\n\n0\n>\n\u03bb\n\n\u223c\nx\n\ni\nl\nl\nu\no\nn\nr\ne\nb\n\ne\nm\na\nn\n\nl\na\ni\nm\no\nn\ni\nb\n\nl\na\ni\nm\no\nn\ni\nt\nl\nu\nm\n\ne\nv\ni\nt\na\ng\ne\nn\n\nl\na\ni\nm\no\nn\ni\nb\n\nn\no\ns\ns\ni\no\np\n\n5\n\n "}, {"Page_number": 23, "text": "d\nn\na\n\nn\na\ne\n\nm\n\ne\nc\nn\na\ni\nr\na\nv\n\n\u03b1\n\n\u03b2\n\u03b1\n\n)\n1\n\n+\n\u03b2\n+\n\u03b1\n(\n2\n)\n\u03b2\n+\n\u03b1\n(\n\n\u03b2\n+\n\u03b1\n\n=\n\n}\n\nx\n\n=\n\n}\n\nx\n\n{\n\n{\ne\n\nr\na\nv\n\nt\nn\ne\nt\ns\ni\nx\ne\nn\no\nn\n\nt\nn\ne\nt\ns\ni\nx\ne\nn\no\nn\n\ns\ni\n\ns\ni\n\n}\n\n\u03bd\n2\n\n\u03bd\n=\n\n=\n\n}\n\n{\n\n}\n\nx\n\nx\n\n{\ne\n\nr\na\nv\n\n{\n\n}\n\nx\n\nx\n\n{\ne\n\nr\na\nv\n\n)\n1\n\n+\n\n)\n1\n\ni\n\nj\n\u03b1\n\u03b1\n\u2212\n+\n\n)\ni\n\u03b1\n\u2212\n\n0\n\u03b1\n(\n20\n\u03b1\n\n0\n\u03b1\n(\ni\n\u03b1\n\n0\n\u03b1\n/\n\u03b1\n=\n\n0\n\u03b1\n(\n20\n\u03b1\n\n=\n\n}\n\ni\n\nx\n\n{\n\n}\n\nx\n\n{\ne\n\nr\na\nv\n\n=\n\n}\n\nj\n\nx\n\n,\ni\n\n{\n\nx\nv\no\nc\n\n2\n\n\u03bb\n/\n1\n\n\u03bb\n/\n1\n\n=\n\n=\n\n}\n\n{\n\n}\n\nx\n\nx\n\n{\ne\n\nr\na\nv\n\n}\nx\n\u03bb\n\u2212\n\n{\n\np\nx\ne\n\u03bb\n=\n\n0\n>\nx\n\n)\nx\n(\nf\n\n2\n\n\u03bb\n/\nr\n\n\u03bb\n/\nr\n\n=\n\n=\n\n}\n\n{\n\n}\n\nx\n\nx\n\n{\ne\n\nr\na\nv\n\n}\nx\n\u03bb\n\u2212\n\n{\n\np\nx\ne\n\n1\n\n)\nr\n(\n\u0001\n\n\u2212\n\nr\nx\nr\n\n\u03bb\n\n=\n\n)\nx\n(\nf\n\n0\n>\nx\n\n)\n\u03bb\n,\nr\n(\na\nm\nm\na\ng\n\n0\n>\n\nr\nd\nn\na\n\n0\n>\n\u03bb\n\n\u223c\nx\n\nl\na\ni\nt\nn\ne\nn\no\np\nx\ne\n\na\nm\nm\na\ng\n\n1\n\n\u2264\n\ni\n\nx\n\u2264\n\n1\n\n\u2212\n\u03b1\n\ni\n\ni\n\nx\n1\n\n=\n\nk i\n\n0\n\n)\ni\n\u03b1\n(\n\u0001\n1\n\nd\nn\na\n\n-\n\nk i\n\n)\n=\nk\nx\n,\n\n)\n0\n\u03b1\n(\n\u0001\n\n-\n\n1\n\n.\n\n.\n\n.\n\n=\n\n=\n\n,\n1\nx\n(\n\ni\n\nx\n1\n\n)\nx\n(\nf\n\n=\n\nk i\n\n=\n,\n\nx\n\n)\nk\n\u03b1\n,\n\n.\n\n.\n\n.\n\n)\n\u03b1\n(\nt\ne\nl\nh\nc\ni\nr\ni\n\nd\n\ni\n\n\u03b1\n1\n\n=\n\nk i\n\n,\n1\n\u03b1\n(\n\n,\n0\n>\n\n=\n\n\u223c\n\n=\n\u03b1\n\ni\n\n\u03b1\n\n0\n\u03b1\n\n)\n\u03bb\n(\np\nx\ne\n\n0\n>\n\u03bb\n\n\u223c\nx\n\nx\n\nt\ne\nl\nh\nc\ni\nr\ni\n\nd\n\n1\n\n\u2212\n\u03b2\n\n)\nx\n\u2212\n\n1\n\n2\n\n0\n\nd\nn\na\ny\nt\ni\ns\nn\ne\nd\n\ne\nc\na\np\ns\ne\nl\np\nm\na\ns\n\nd\nn\na\n\nn\no\ni\nt\na\nt\no\nn\n\ne\nc\na\np\ns\nr\ne\nt\ne\nm\na\nr\na\np\n\n1\n(\n1\n\n\u2212\n\u03b1\nx\n)\n\u03b2\n(\n\u0001\n)\n\u03b1\n(\n1\n\u0001\n\n)\n\u03b2\n+\n\u03b1\n(\n\u0001\n\n\u2264\nx\n\u2264\n\n=\n\n)\nx\n(\nf\n\n0\n\n0\n>\n\u03b2\nd\nn\na\n\n)\n\u03b2\n,\n\u03b1\n(\na\nt\ne\nb\n\n0\n>\n\u03b1\n\n\u223c\nx\n\n\u03b1\n\u2212\nx\n\n\u03b2\n\n1\n\n/\n\n+\n\n1\n\n.\n\n\u03b2\n\u03c0\n\n=\n\n)\nx\n(\nf\n\n\u211c\n\u2208\nx\n\n0\n>\n\u03b2\nd\nn\na\n\n)\n\u03b2\n,\n\u03b1\n(\ny\nh\nc\nu\na\nc\n\n\u211c\n\u2208\n\u03b1\n\n\u223c\nx\n\ne\nm\na\nn\n\na\nt\ne\nb\n\ny\nh\nc\nu\na\nc\n\n)\n2\n/\n1\n,\n2\n/\n\u03bd\n(\na\nm\nm\na\ng\n\n=\n\n0\n>\nx\n\n)\nx\n(\nf\n\n2\u03bd\n\u03c7\n\u223c\nx\n\n0\n>\n\u03bd\n\ne\nr\na\nu\nq\ns\n-\ni\nh\nc\n\n.\ns\ne\nl\nb\na\ni\nr\na\nv\nm\no\nd\nn\na\nr\n\ns\nu\no\nu\nn\n\ni\nt\n\nn\no\nc\n\nf\n\no\n\ns\nn\no\n\ni\nt\n\nu\nb\ni\nr\nt\ns\ni\nd\ny\nt\ni\nl\ni\n\nb\na\nb\no\nr\np\n\nn\no\nm\nm\no\nc\n\ne\nm\no\ns\n\nr\no\n\nf\n\nn\no\ni\nt\np\ni\nr\nc\ns\ne\nd\n\nd\nn\na\nn\no\ni\nt\na\nt\no\nn\n\n2\n.\n1\n\ne\nl\nb\na\nt\n\n6\n\n "}, {"Page_number": 24, "text": "}\n\n2\n\n\u03c3\n+\n\u00b5\n2\n\n{\n\np\nx\ne\n\n\u2212\n\n}\n\n2\n}\n\n2\n/\n\n\u03c3\n2\n\n2\n\np\nx\ne\n\n+\n\u00b5\n2\n\n\u03c3\n+\n\u00b5\n{\n\n{\n\np\nx\ne\n\n=\n\n=\n\n}\n\n{\n\n}\n\nx\n\nx\n\n{\ne\n\nr\na\nv\n\n\u0001\n=\n\n}\n\n\u00b5\n=\n\n}\n\nx\n\n{\n\n{\ne\n\nr\na\nv\n\nx\n\n2\n\n\u00b5\n=\n\n\u03c3\n=\n\n}\n\n{\n\n}\n\nx\n\nx\n\n{\ne\n\nr\na\nv\n\n2\n)\nb\n/\n1\n\n+\n\n1\n(\n\u0001\n\u2212\n\nb\n/\n2\n\na\n\n)\nb\n/\n2\n\n2\n>\n\u03bd\n\nf\ni\n\n2\n1\n/\n\n2\n/\n)\nb\n+\na\n(\n\n2\n)\na\n\u2212\nb\n(\n\n=\n\n=\n\n}\n\n{\n\n}\n\nx\n\nx\n\n{\ne\n\nr\na\nv\n\n1\n>\n\u03bd\n\nf\ni\n\n0\n\n2\n\n\u03bd\n\n+\n\u03bd\n\n=\n\n}\n\nx\n\n{\n\n=\n\n}\n\nx\n\n{\ne\n\nr\na\nv\n\n)\nb\n/\n1\n\nb\n/\n1\n\n+\n\n+\n\na\n\n1\n(\n\u0001\n\n1\n(\n\u0001\n\n=\n\n}\n\nx\n\n=\n\n}\n\nx\n\n{\n\n{\ne\n\nr\na\nv\n\n3\n0\n\n2\n\n\u00b5\n\u2212\n\n\u03c3\n\n}\nx\n{\n\ng\no\nl\n\n}\n\n2\n/\n)\n\u00b5\n\u2212\n\n3\n0\n\n2\n\n2\n/\n)\n1\n\n+\n\u03bd\n(\n\u2212\n\n\u00b5\n\u2212\nx\n\n\u03c3\n\n/\n12\n\u2212\n\n2\n\n)\n\nx\n(\n1\n\n2\n/\n1\n\n|\n\n|\n\n\u0001\n\n2\n/\nk\n\n\u2212\n\u0001\nt\n\u00b5\n\u2212\n\n)\n\u03c0\n2\n(\n\nk\n\n\u211c\n\u2208\n\n/\n12\n\u2212\n\n2\n\np\nx\ne\n\n2\n\n1\n\n\u03c3\n\u03c0\n2\n\u221a\nx\n\n=\n\n)\nx\n(\nf\n\n)\n2\n\n\u03c3\n,\n\np\nx\ne\n\n2\n\n\u03c3\n\u03c0\n2\n\u221a\n=\n\n1\n\nx\n(\n\n\u2212\n\n{\n\np\nx\ne\n\n)\nk\nx\n,\n\n.\n\n.\n\n.\n\n,\n1\nx\n(\n\n=\n\n\u211c\n\u2208\nx\n\n)\nx\n(\nf\n\n=\n\nx\n\n)\nx\n(\nf\n\nk\n\n\u211c\n\u2208\n\n\"\n\n2 \u03bd\n\nx\n\n+\n\n1\n\n!\n\n)\n2\n/\n)\n1\n\n\u03bd\n\u03c0\n\u221a\n)\n2\n/\n\u03bd\n(\n\u0001\n\n+\n\u03bd\n(\n(\n\u0001\n\n1\n\n}\nb\nx\na\n\u2212\n\n{\n\np\nx\ne\n\n1\n\n\u2212\nb\nx\nb\na\n=\n\na\n\u2212\n]\nb\nb\n,\na\n[\n\n=\n\n=\n\n\u211c\n\u2208\nx\n\n)\nx\n(\nf\n\n\u211c\n\u2208\nx\n\n)\nx\n(\nf\n\n0\n>\nx\n\n)\nx\n(\nf\n\n\u2208\nx\n\n\u00b5\n\n0\n>\n\u03c3\nd\nn\na\n\n(\nl\na\nm\nr\no\nn\ng\no\nl\n\n\u211c\n\u2208\n\u00b5\n\n\u223c\nx\n\n\u00b5\n\ne\n)\nt\nk\ni\nn\nfi\ne\nd\n\n,\n\n.\n\n.\n\n)\n\n\u0001\n\n,\n\n\u00b5\n\n(\nk\n\n(\n\nn\n\u223c\n\n.\n\n,\n1\n\u00b5\n\ne\nv\ni\nt\ni\ns\no\np\n\u0001\n\n=\n\u00b5\n\nx\n\n0\n>\n\u03c3\nd\nn\na\n\n)\n2\n\n\u03c3\n,\n\n(\n\n\u00b5\nn\n\n\u211c\n\u2208\n\u00b5\n\n\u223c\nx\n\n\u03bd\nt\n0\n>\n\u03bd\n\n\u223c\nx\n\nb\n<\na\nd\nn\na\n\n)\nb\n,\na\n(\nf\ni\nn\nu\n\n\u211c\n\u2208\nb\n,\na\n\n\u223c\nx\n\n)\nb\n,\na\n(\nl\nl\nu\nb\ni\ne\n\nw\n\n0\n>\nb\n\nd\nn\na\n\n0\n>\na\n\n\u223c\nx\n\nd\nn\na\n\nn\na\ne\n\nm\n\ne\nc\nn\na\ni\nr\na\nv\n\nd\nn\na\ny\nt\ni\ns\nn\ne\nd\n\ne\nc\na\np\ns\ne\nl\np\nm\na\ns\n\nd\nn\na\n\nn\no\ni\nt\na\nt\no\nn\n\ne\nc\na\np\ns\nr\ne\nt\ne\nm\na\nr\na\np\n\n.\ns\ne\nl\nb\na\ni\nr\na\nv\nm\no\nd\nn\na\nr\n\ns\nu\no\nu\nn\n\ni\nt\n\nn\no\nc\n\nf\n\no\n\ns\nn\no\n\ni\nt\n\nu\nb\ni\nr\nt\ns\ni\nd\n\ny\nt\ni\nl\ni\n\nb\na\nb\no\nr\np\nn\no\nm\nm\no\nc\n\ne\nr\no\nm\n\nr\no\nf\n\nn\no\ni\nt\np\ni\nr\nc\ns\ne\nd\n\nd\nn\na\nn\no\ni\nt\na\nt\no\nn\n\n3\n.\n1\ne\nl\nb\na\nt\n\nl\na\nm\nr\no\nn\ng\no\nl\n\ne\nm\na\nn\n\ne\nt\na\ni\nr\na\nv\ni\nt\nl\nu\nm\n\nl\na\nm\nr\no\nn\n\nl\na\nm\nr\no\nn\n\nt\n\ns\n\u2019\nt\nn\ne\nd\nu\nt\ns\n\nm\nr\no\nf\ni\nn\nu\n\nl\nl\nu\nb\ni\ne\n\nw\n\n7\n\n "}, {"Page_number": 25, "text": "8\n\nchapter 1 review\n\nn\n\n*\n\nk1\n\n. . .\n\nki,\n\nkm+ =\nn!\ni=1 ki! where n =\n-m\n\u0001(r) =%(r \u2212 1)!\n# \u221e0\ntr\u22121 exp{\u2212t} dt\n\u0001/1\n20 = 1 \u00d7 3 \u00d7 5 \u00d7 \u00b7\u00b7\u00b7 \u00d7 (2n \u2212 1)\u221a\u03c0/2n\n20 = \u221a\u03c0 and \u0001/n +\n\nm$i=1\nfor r = 1,2, . . .\nfor general r > 0.\n\n1\n\nit is worth knowing that\n\nfor positive integer n.\nexponential family. a k-parameter exponential family density can be expressed as\n\nmany of the distributions commonly used in statistics are members of an\n\n(1.15)\n\n(1.16)\n\nf(x|\u03b3) = c1(x)c2(\u03b3)exp% k$i=1\n\nyi(x)\u03b8i(\u03b3)(\n\n(1.17)\n\nfornonnegativefunctions c1 and c2.thevector \u03b3 denotesthefamiliarparameters,such\nas \u03bb for the poisson density and p for the binomial density. the real-valued \u03b8i(\u03b3) are\nthe natural, or canonical, parameters, which are usually transformations of \u03b3. the\nyi(x) are the sufficient statistics for the canonical parameters. it is straightforward\nto show\n\nand\n\ne{y(x)} =\u03ba\u2032(\u03b8)\n\n(1.18)\n\n(1.19)\nwhere \u03ba(\u03b8) = \u2212log c3(\u03b8), letting c3(\u03b8) denote the reexpression of c2(\u03b3) in terms of\nthe canonical parameters \u03b8 = (\u03b81, . . . , \u03b8k), and y(x) = (y1(x), . . . , yk(x)). these\nresults can be rewritten in terms of the original parameters \u03b3 as\n\nvar{y(x)} =\u03ba\u2032\u2032(\u03b8),\n\nand\n\nlog c2(\u03b3)\n\n(1.20)\n\nd\u03b8i(\u03b3)\nd\u03b3j\n\ne% k$i=1\nyi(x)( = \u2212\n\nd\nd\u03b3j\n\nyi(x)( = \u2212\nlog c2(\u03b3) \u2212 e% k$i=1\n\nj\n\n(1.21)\n\nd2\nd\u03b32\n\nd\u03b8i(\u03b3)\nd\u03b3j\n\nd2\u03b8i(\u03b3)\nd\u03b32\n\nyi(x)( .\n\nvar% k$i=1\nexample 1.1 (poisson) the poisson distribution belongs to the exponential family\nwith c1(x) = 1/x!, c2(\u03bb) = exp{\u2212\u03bb}, y(x) = x, and \u03b8(\u03bb) = log \u03bb. deriving moments\nin terms of \u03b8, we have \u03ba(\u03b8) = exp{\u03b8}, so e{x} = \u03ba\u2032(\u03b8) = exp{\u03b8} = \u03bb and var{x} =\n\u03ba\u2032\u2032(\u03b8) = exp{\u03b8} = \u03bb. the same results may be obtained with (1.20) and (1.21), noting\nthat d\u03b8/d\u03bb = 1/\u03bb. for example, (1.20) gives e{x/\u03bb} = 1.\n!\nit is also important to know how the distribution of a random variable changes\nwhen it is transformed. let x = (x1, . . . , xp) denote a p-dimensional random\n\nj\n\n "}, {"Page_number": 26, "text": "1.4 likelihood inference\n\n9\n\nvariable with continuous density function f. suppose that\n\nu = g(x) = (g1(x), . . . , gp(x)) = (u1, . . . , up),\n\n(1.22)\nwhere g is a one-to-one function mapping the support region of f onto the space of\nall u = g(x) for which x satisfies f(x) > 0. to derive the probability distribution of\nu from that of x, we need to use the jacobian matrix. the density of the transformed\nvariables is\n\n(1.23)\nwhere |j(u)| is the absolute value of the determinant of the jacobian matrix of g\u22121\nevaluated at u, having (i, j)th element dxi/duj, where these derivatives are assumed\nto be continuous over the support region of u.\n\nf(u) = f(g\u22121(u))|j(u)| ,\n\n1.4 likelihood inference\nif x1, . . . ,xn are independent and identically distributed (i.i.d.) each having density\nf(x | \u03b8) that depends on a vector of p unknown parameters \u03b8 = (\u03b81, . . . , \u03b8p), then the\njoint likelihood function is\n\n(1.24)\n\nl(\u03b8) =\n\nf(xi|\u03b8).\n\nn\u2019i=1\nwhen the data are not i.i.d., the joint likelihood is still expressed as the joint density\nf(x1, . . . ,xn|\u03b8) viewed as a function of \u03b8.\nthe observed data, x1, . . . ,xn, might have been realized under many different\nvalues for \u03b8. the parameters for which observing x1, . . . ,xn would be most likely\nconstitute the maximum likelihood estimate of \u03b8. in other words, if \u02c6\u03d1 is the function\nof x1, . . . ,xn that maximizes l(\u03b8), then \u02c6\u03b8 = \u02c6\u03d1(x1, . . . ,xn) is the maximum likeli-\nhood estimator (mle) for \u03b8. mles are invariant to transformation, so the mle of a\ntransformation of \u03b8 equals the transformation of \u02c6\u03b8.\n\nit is typically easier to work with the log likelihood function,\n\nl (\u03b8) = log l(\u03b8),\n\n(1.25)\nwhich has the same maximum as the original likelihood, since log is a strictly mono-\ntonic function. furthermore, any additive constants (involving possibly x1, . . . ,xn\nbut not \u03b8) may be omitted from the log likelihood without changing the location of its\nmaximum or differences between log likelihoods at different \u03b8. note that maximizing\nl(\u03b8) with respect to \u03b8 is equivalent to solving the system of equations\n\nwhere\n\nl\u2032(\u03b8) = 0,\nl\u2032(\u03b8) =! dl(\u03b8)\n\nd\u03b81\n\n, . . . ,\n\n(1.26)\n\ndl(\u03b8)\n\nd\u03b8n \"\n\n "}, {"Page_number": 27, "text": "chapter 1 review\n\n10\nis called the score function. the score function satisfies\n\ne{l\u2032(\u03b8)} = 0,\n\n(1.27)\nwhere the expectation is taken with respect to the distribution of x1, . . . ,xn. some-\ntimes an analytical solution to (1.26) provides the mle; this book describes a variety\nof methods that can be used when the mle cannot be solved for in closed form. it\nis worth noting that there are pathological circumstances where the mle is not a\nsolution of the score equation, or the mle is not unique; see [127] for examples.\nthe mle has a sampling distribution because it depends on the realization\nof the random variables x1, . . . ,xn. the mle may be biased or unbiased for \u03b8, yet\nunder quite general conditions it is asymptotically unbiased as n \u2192 \u221e. the sampling\nvariance of the mle depends on the average curvature of the log likelihood: when the\nlog likelihood is very pointy, the location of the maximum is more precisely known.\nto make this precise, let l\u2032\u2032(\u03b8) denote the p \u00d7 p matrix having (i, j)th element\ngiven by d2l(\u03b8)/(d\u03b8id\u03b8j). the fisher information matrix is defined as\n\n} = \u2212e{l\u2032\u2032(\u03b8)},\n\ni(\u03b8) = e{l\u2032(\u03b8)l\u2032(\u03b8)t\n\n(1.28)\nwhere the expectations are taken with respect to the distribution of x1, . . . ,xn. the\nfinal equality in (1.28) requires mild assumptions, which are satisfied, for example, in\nexponential families. i(\u03b8) may sometimes be called the expected fisher information\nto distinguish it from \u2212l\u2032\u2032(\u03b8), which is the observed fisher information. there are\ntwo reasons why the observed fisher information is quite useful. first, it can be\ncalculated even if the expectations in (1.28) cannot easily be computed. second, it is\na good approximation to i(\u03b8) that improves as n increases.\nunder regularity conditions, the asymptotic variance\u2013covariance matrix of\nlimiting distribution of \u02c6\u03b8 is np(\u03b8\u2217,i(\u03b8\u2217)\u22121). since the true parameter values are\nunknown, i(\u03b8\u2217)\u22121 must be estimated in order to estimate the variance\u2013covariance\nmatrix of the mle. an obvious approach is to use i(\u02c6\u03b8)\u22121. alternatively, it is also\nbe estimated by taking the square root of the diagonal elements of the chosen\nestimate of i(\u03b8\u2217)\u22121. a thorough introduction to maximum likelihood theory and\nthe relative merits of these estimates of i(\u03b8\u2217)\u22121 can be found in [127, 182, 371, 470].\nprofile likelihoods provide an informative way to graph a higher-dimensional\nlikelihood surface, to make inference about some parameters while treating others\nas nuisance parameters, and to facilitate various optimization strategies. the profile\nlikelihood is obtained by constrained maximization of the full likelihood with respect\nto parameters to be ignored. if \u03b8 = (\u00b5, \u03c6), then the profile likelihood for \u03c6 is\n\nthe mle4\u03b8 is i(\u03b8\u2217)\u22121, where \u03b8\u2217 denotes the true value of \u03b8. indeed, as n \u2192 \u221e, the\nreasonable to use \u2212l\u2032\u2032(4\u03b8)\u22121. standard errors for individual parameter mles can\n\nl(\u03c6|\u02c6\u00b5(\u03c6)) = max\n\n\u00b5\n\nl(\u00b5, \u03c6).\n\n(1.29)\n\nthus, for each possible \u03c6, a value of \u00b5 is chosen to maximize l(\u00b5, \u03c6). this optimal\n\u00b5 is a function of \u03c6. the profile likelihood is the function that maps \u03c6 to the value\nof the full likelihood evaluated at \u03c6 and its corresponding optimal \u00b5. note that the \u02c6\u03c6\n\n "}, {"Page_number": 28, "text": "11\nthat maximizes the profile likelihood l(\u03c6| \u02c6\u00b5(\u03c6)) is also the mle for \u03c6 obtained from\nthe full likelihood l(\u00b5, \u03c6). profile likelihood methods are examined in [23].\n\n1.5 bayesian inference\n\n1.5 bayesian inference\nin the bayesian inferential paradigm, probability distributions are associated with\nthe parameters of the likelihood, as if the parameters were random variables. the\nprobability distributions are used to assign subjective relative probabilities to regions\nof parameter space to reflect knowledge (and uncertainty) about the parameters.\nsuppose that x has a distribution parameterized by \u03b8. let f(\u03b8) represent the\ndensity assigned to \u03b8 before observing the data. this is called a prior distribution.\nit may be based on previous data and analyses (e.g., pilot studies), it may represent\na purely subjective personal belief, or it may be chosen in a way intended to have\nlimited influence on final inference.\nbayesian inference is driven by the likelihood, often denoted l(\u03b8|x) in this\ncontext. having established a prior distribution for \u03b8 and subsequently observed data\nyielding a likelihood that is informative about \u03b8, one\u2019s prior beliefs must be updated\nto reflect the information contained in the likelihood. the updating mechanism is\nbayes\u2019 theorem:\n\nf(\u03b8|x) = cf(\u03b8)f(x|\u03b8) = cf(\u03b8)l(\u03b8|x),\n\n(1.30)\nwhere f(\u03b8|x) is the posterior density of \u03b8. the posterior distribution for \u03b8 is used\nfor statistical inference about \u03b8. the constant c equals 1/#f(\u03b8)l(\u03b8|x) d\u03b8 and is often\ndifficult to compute directly, although some inferences do not require c. this book\ndescribes a large variety of methods for enabling bayesian inference, including the\nestimation of c.\nlet \u02dc\u03b8 be the posterior mode, and let \u03b8\u2217 be the true value of \u03b8. the posterior\ndistribution of \u02dc\u03b8 converges to n(\u03b8\u2217,i(\u03b8\u2217)\u22121) as n \u2192 \u221e, under regularity conditions.\nnote that this is the same limiting distribution as for the mle. thus, the posterior\nmode is of particular interest as a consistent estimator of \u03b8. this convergence reflects\nthefundamentalnotionthattheobserveddatashouldoverwhelmanyprioras n \u2192 \u221e.\nbayesian evaluation of hypotheses relies upon the bayes factor. the ratio of\nposterior probabilities of two competing hypotheses or models, h1 and h2, is\n\np[h2]\np[h1] b2,1\n\n(1.31)\nwhere p[hi|x] denotes posterior probability, p[hi] denotes prior probability, and\n(1.32)\n\np[h2|x]\np[h1|x] =\nf(x|h1) = # f(\u03b82|h2)f(x|\u03b82, h2) d\u03b82\nf(x|h2)\n# f(\u03b81|h1)f(x|\u03b81, h1) d\u03b81\n\nwith \u03b8i denoting the parameters corresponding to the ith hypothesis. the quantity\nb2,1 is the bayes factor; it represents the factor by which the prior odds are multiplied\nto produce the posterior odds, given the data. the hypotheses h1 and h2 need not be\n\nb2,1 =\n\n "}, {"Page_number": 29, "text": "chapter 1 review\n\n12\nnested as for likelihood ratio methods. the computation and interpretation of bayes\nfactors is reviewed in [365].\nbayesian interval estimation often relies on a 95% highest posterior density\n(hpd) region. the hpd region for a parameter is the region of shortest total length\ncontaining 95% of the posterior probability for that parameter for which the posterior\ndensity for every point contained in the interval is never lower than the density for\nevery point outside the interval. for unimodal posteriors, the hpd is the narrowest\npossible interval containing 95% of the posterior probability. a more general interval\nfor bayesian inference is a credible interval. the 100(1 \u2212 \u03b1)% credible interval is\nthe region between the \u03b1/2 and 1 \u2212 \u03b1/2 quantiles of the posterior distribution. when\nthe posterior density is symmetric and unimodal, the hpd and the credible interval\nare identical.\na primary benefit of the bayesian approach to inference is the natural manner\nin which resulting credibility intervals and other inferences are interpreted. one may\nspeak of the posterior probability that the parameter is in some range. there is also\na sound philosophical basis for the bayesian paradigm; see [28] for an introduction.\ngelman et al. provide a broad survey of bayesian theory and methods [221].\nthe best prior distributions are those based on prior data. a strategy that is\nalgebraicallyconvenientistoseekconjugacy.aconjugatepriordistributionisonethat\nyields a posterior distribution in the same parametric family as the prior distribution.\nexponential families are the only classes of distributions that have natural conjugate\nprior distributions.\nwhenpriorinformationispoor,itisimportanttoensurethatthechosenpriordis-\ntribution does not strongly influence posterior inferences. a posterior that is strongly\ninfluenced by the prior is said to be highly sensitive to the prior. several strategies are\navailable to reduce sensitivity. the simplest approach is to use a prior whose support\nis dispersed over a much broader region than the parameter region supported by the\ndata, and fairly flat over it. a more formal approach is to use a jeffreys prior [350].\nin the univariate case, the jeffreys prior is f(\u03b8) \u221d i(\u03b8)\u22121/2, where i(\u03b8) is the fisher\ninformation; multivariate extensions are possible. in some cases, the improper prior\nf(\u03b8) \u221d 1 may be considered, but this can lead to improper posteriors (i.e., not inte-\ngrable), and it can be unintentionally informative depending on the parameterization\nof the problem.\nexample 1.2 (normal\u2013normal conjugate bayes model) consider bayesian\ninference based on observations of i.i.d. random variables x1, . . . , xn with density\nxi|\u03b8 \u223c n(\u03b8, \u03c32) where \u03c32 is known. for such a likelihood, a normal prior for \u03b8 is\nconjugate. suppose the prior is \u03b8 \u223c n(\u00b5, \u03c42). the posterior density is\n\nn\u2019i=1\nf(\u03b8|x) \u221d f(\u03b8)\nf(xi|\u03b8)\n\u221d exp2\u2212\n2!(\u03b8 \u2212 \u00b5)2\n1\n\u221d exp%\u2212\n2 !\u03b8 \u2212\n\n1\n\n+,n\n\"3\ni=1(xi \u2212 \u03b8)2\n\u03c42\n\u03c32\n1/\u03c42 + n/\u03c32\"( ,\n1/\u03c42 + n/\u03c32 \"25!\n\u00b5/\u03c42 + (n\u00afx)/\u03c32\n\n1\n\n(1.33)\n\n(1.34)\n\n(1.35)\n\n "}, {"Page_number": 30, "text": "1\n\nand\n\n\u03c42\nn =\n\nn), where\n\n1.6 statistical limit theory\n\n13\nwhere \u00afx is the sample mean. recognizing (1.35) as being in the form of a normal\ndistribution, we conclude that f(\u03b8|x) = n(\u00b5n, \u03c42\n1/\u03c42 + n/\u03c32\n\u03c32\" \u03c42\n\n(1.37)\nhence,aposterior95%credibilityintervalfor \u03b8 is(\u00b5n \u2212 1.96\u03c4n, \u00b5n + 1.96\u03c4n).since\nthe normal distribution is symmetric, this is also the posterior 95% hpd for \u03b8.\nfor fixed \u03c3, consider increasingly large choices for the value of \u03c4. the posterior\nvariance for \u03b8 converges to \u03c32/n as \u03c42 \u2192 \u221e. in other words, the influence of the\nprior on the posterior vanishes as the prior variance increases. next, note that\n\n\u00b5n =! \u00b5\n\n(1.36)\n\n\u03c42 +\n\nn\u00afx\n\nn.\n\n\u03c42\n\u03c32/n = 1.\n\nn\n\nlim\nn\u2192\u221e\n\nthis shows that the posterior variance for \u03b8 and the sampling variance for the mle,\n\u02c6\u03b8 = \u00afx, are asymptotically equal, and the effect of any choice for \u03c4 is washed out with\nincreasing sample size.\nas an alternative to the conjugate prior, consider using the improper prior\nf(\u03b8) \u221d 1. in this case, f(\u03b8|x) = n(\u00afx, \u03c32/n), and a 95% posterior credibility\ninterval corresponds to the standard 95% confidence interval found using frequentist\nmethods.\n!\n\n1.6 statistical limit theory\nalthough this book is mostly concerned with a pragmatic examination of how and\nwhy various methods work, it is useful from time to time to speak more precisely\naboutthelimitingbehavioroftheestimatorsproducedbysomeprocedures.wereview\nbelow some basic convergence concepts used in probability and statistics.\na sequence of random variables, x1, x2, . . ., is said to converge in proba-\nbility to the random variable x if limn\u2192\u221e p[|xn \u2212 x| < \u03f5] = 1 for every \u03f5 > 0.\nthe sequence converges almost surely to x if p[limn\u2192\u221e |xn \u2212 x| < \u03f5] = 1 for\nevery \u03f5 > 0. the variables converge in distribution to the distribution of x if\nlimn\u2192\u221e fxn(x) = fx(x) for all points x at which fx(x) is continuous. the vari-\nable x has property a almost everywhere if p[a] =# 1{a}fx(x) dx = 1.\nsome of the best-known convergence theorems in statistics are the laws of large\nnumbersandthecentrallimittheorem.fori.i.d.sequencesofone-dimensionalrandom\nvariables x1, x2, . . .,let \u00afxn =,n\ni=1 xi/n.theweaklawoflargenumbersstatesthat\n\u00afxn converges in probability to \u00b5 = e{xi} if e{|xi|} < \u221e. the strong law of large\nnumbers states that \u00afxn converges almost surely to \u00b5 if e{|xi|} < \u221e. both results\nhold under the more stringent but easily checked condition that var{xi} = \u03c32 < \u221e.\n\n "}, {"Page_number": 31, "text": "14\n\nchapter 1 review\n\nif \u03b8 is a parameter and tn is a statistic based on x1, . . . , xn, then tn is said to\nbe weakly or strongly consistent for \u03b8 if tn converges in probability or almost surely\n(respectively) to \u03b8. tn is unbiased for \u03b8 if e{tn} = \u03b8; otherwise the bias is e{tn} \u2212 \u03b8.\nif the bias vanishes as n \u2192 \u221e, then tn is asymptotically unbiased.\na simple form of the central limit theorem is as follows. suppose that i.i.d. ran-\ndom variables x1, . . . , xn have mean \u00b5 and finite variance \u03c32, and that e{exp{txi}}\nexists in a neighborhood of t = 0. then the random variable tn = \u221an( \u00afxn \u2212 \u00b5)/\u03c3\nconverges in distribution to a normal random variable with mean zero and variance\none, as n \u2192 \u221e. there are many versions of the central limit theorem for various\nsituations. generally speaking, the assumption of finite variance is critical, but the\nassumptions of independence and identical distributions can be relaxed in certain\nsituations.\n\n1.7 markov chains\nwe offer here a brief introduction to univariate, discrete-time, discrete-state-space\nmarkov chains. we will use markov chains in chapters 7 and 8. a thorough introduc-\ntion to markov chains is given in [556], and higher-level study is provided in [462,\n543].\nconsider a sequence of random variables6x(t)7, t = 0,1, . . ., where each x(t)\nmay equal one of a finite or countably infinite number of possible values, called states.\nthe notation x(t) = j indicates that the process is in state j at time t. the state space,\ns, is the set of possible values of the random variable x(t).\na complete probabilistic specification of x(0), . . . , x(n) would be to write their\njoint distribution as the product of conditional distributions of each random variable\ngiven its history, or\n\np8x(0), . . . , x(n)9 = p8 x(n)&&& x(0), . . . , x(n\u22121)9\n\n\u00d7 p8 x(n\u22121)&&& x(0), . . . , x(n\u22122)9 \u00d7 \u00b7\u00b7\u00b7\n\n\u00d7 p8 x(1)&&& x(0)9 p8x(0)9 .\n\na simplification of (1.38) is possible under the conditional independence assumption\nthat\n\np8 x(t)&&& x(0), . . . , x(t\u22121)9 = p8 x(t)&&& x(t\u22121)9 .\n\nhere the next state observed is only dependent upon the present state. this is the\nmarkov property, sometimes called \u201cone-step memory.\u201d in this case,\n\np8x(0), . . . , x(n)9 = p8 x(n)&&& x(n\u22121)9\n\n\u00d7p8 x(n\u22121)&&& x(n\u22122)9\u00b7\u00b7\u00b7 p8 x(1)&&& x(0)9 p8x(0)9 .\n\n(1.40)\n\n(1.38)\n\n(1.39)\n\n "}, {"Page_number": 32, "text": "1.7 markov chains\n\n15\n\ntable 1.4 san francisco rain data considered in example 1.3.\ndry today\n\nwet today\n\nwet yesterday\ndry yesterday\n\n418\n256\n\n256\n884\n\n(t)\nij be the probability that the observed state changes from state i at time t\nlet p\n\nto state j at time t + 1. the sequence6x(t)7, t = 0,1, . . . is a markov chain if\n\n(t)\np\n\nij = p8 x(t+1)\n= p8 x(t+1)\n\n= j&&& x(0)\n= j&&& x(t)\n\n= x(0), x(1)\n= i9\n\n= x(1), . . . , x(t)\n\n= i9\n\n(1.41)\n\n(t)\nfor all t = 0,1, . . . and x(0), x(1), . . . , x(t\u22121), i, j \u2208 s. the quantity p\nij is called the\none-step transition probability. if none of the one-step transition probabilities change\n(t)\nwith t,thenthechainiscalled time homogeneous,and p\nij = pij.ifanyoftheone-step\ntransition probabilities change with t, then the chain is called time-inhomogeneous.\na markov chain is governed by a transition probability matrix. suppose that\nthe s states ins are, without loss of generality, all integer valued. then p denotes s \u00d7 s\ntransition probability matrix of a time-homogeneous chain, and the (i, j)th element\nof p is pij. each element in p must be between zero and one, and each row of the\nmatrix must sum to one.\n\nexample 1.3 (san francisco weather) let us consider daily precipitation out-\ncomesinsanfrancisco.table1.4givestherainfallstatusfor1814pairsofconsecutive\ndays [488]. the data are taken from the months of november through march, starting\nin november of 1990 and ending in march of 2002. these months are when san\nfrancisco receives over 80% of its precipitation each year, virtually all in the form of\nrain. we consider a binary classification of each day. a day is considered to be wet if\nmore than 0.01 inch of precipitation is recorded and dry otherwise. thus, s has two\nelements: \u201cwet\u201d and \u201cdry.\u201d the random variable corresponding to the state for the tth\nday is x(t).\nassuming time homogeneity, an estimated transition probability matrix for x(t)\nwould be\n\n\u02c6p =:0.620\n\n0.224\n\n0.380\n\n0.775; .\n\n(1.42)\n\nclearly, wet and dry weather states are not independent in san francisco, as a wet\nday is more likely to be followed by a wet day and pairs of dry days are highly\nlikely.\n!\n\n "}, {"Page_number": 33, "text": "16\n\nm > 0 such that p< x(m+n) = i&& x(n) = j= > 0. a markov chain is periodic if it can\n\nchapter 1 review\nthe limiting theory of markov chains is important for many of the methods\ndiscussed in this book. we now review some basic results.\na state to which the chain returns with probability 1 is called a recurrent state.\na state for which the expected time until recurrence is finite is called nonnull. for\nfinite state spaces, recurrent states are nonnull.\na markov chain is irreducible if any state j can be reached from any state i in a\nfinite number of steps for all i and j. in other words, for each i and j there must exist\nvisit certain portions of the state space only at certain regularly spaced intervals. state\nj has period d if the probability of going from state j to state j in n steps is 0 for all\nn not divisible by d. if every state in a markov chain has period 1, then the chain is\ncalled aperiodic. a markov chain is ergodic if it is irreducible, aperiodic, and all its\nstates are nonnull and recurrent.\nlet \u03c0 denote a vector of probabilities that sum to one, with ith element \u03c0i\ndenoting the marginal probability that x(t) = i. then the marginal distribution of\nx(t+1) must be \u03c0tp. any discrete probability distribution \u03c0 such that \u03c0tp = \u03c0t is\ncalled a stationary distribution for p, or for the markov chain having transition proba-\nbility matrix p. if x(t) follows a stationary distribution, then the marginal distributions\nof x(t) and x(t+1) are identical.\n\nif a time-homogeneous markov chain satisfies\n\n\u03c0ipij = \u03c0jpji\n\n(1.43)\nfor all i, j \u2208 s, then \u03c0 is a stationary distribution for the chain, and the chain is called\nreversible because the joint distribution of a sequence of observations is the same\nwhether the chain is run forwards or backwards. equation (1.43) is called the detailed\nbalance condition.\nifamarkovchainwithtransitionprobabilitymatrixpandstationarydistribution\n\u03c0 is irreducible and aperiodic, then \u03c0 is unique and\n\n(1.44)\nwhere \u03c0j is the jth element of \u03c0. the \u03c0j are the solutions of the following set of\nequations:\n\nlim\nn\u2192\u221e\n\n= i9 = \u03c0j,\n\np8 x(t+n)\n\n\u03c0j \u2265 0, $i\u2208s\n\n\u03c0i = 1,\n\n\u03c0ipij for each j \u2208 s.\n\n(1.45)\n\nwe can restate and extend (1.44) as follows. if x(1), x(2), . . . are realizations\nfrom an irreducible and aperiodic markov chain with stationary distribution \u03c0, then\nx(n) converges in distribution to the distribution given by \u03c0, and for any function h,\n\n1\nn\n\nn$t=1\n\nh(x(t)) \u2192 e\u03c0{h(x)}\n\n(1.46)\n\nalmost surely as n \u2192 \u221e, provided e\u03c0{|h(x)|} exists [605]. this is one form of the\nergodic theorem, which is a generalization of the strong law of large numbers.\n\n= j&&& x(t)\nand \u03c0j =$i\u2208s\n\n "}, {"Page_number": 34, "text": "17\nwe have considered here only markov chains for discrete state spaces. in\nchapters 7 and 8 we will apply these ideas to continuous state spaces. the prin-\nciples and results for continuous state spaces and multivariate random variables are\nsimilar to the simple results given here.\n\n1.8 computing\n\n1.8 computing\nif you are new to computer programming, or wishing to learn a new language, there is\nnobettertimetostartthannow.ourpreferredlanguageforteachingandlearningabout\nstatistical computing is r (freely available at www.r-project.org), but we avoid any\nlanguage-specific limitations in this text. most of the methods described in this book\ncan also be easily implemented in other high-level computer languages for mathemat-\nics and statistics such as matlab. programming in java and low-level languages\nsuch as c++ and fortran is also possible. the tradeoff between implementation\nease for high-level languages and computation speed for low-level languages often\nguides this selection. links to these and other useful software packages, including\nlibraries of code for some of the methods described in this book, are available on the\nbook website.\nideally, your computer programming background includes a basic understand-\ning of computer arithmetic: how real numbers and mathematical operations are\nimplemented in the binary world of a computer. we focus on higher-level issues\nin this book, but\nthe most meticulous implementation of the algorithms we\ndescribe can require consideration of the vagaries of computer arithmetic, or use\nof available routines that competently deal with such issues. interested readers may\nrefer to [383].\n\n "}, {"Page_number": 35, "text": "p a r t i\noptimization\n\nin statistics we need to optimize many functions, including likelihood func-\ntions and generalizations thereof, bayesian posterior distributions, entropy,\nand fitness landscapes. these all describe the information content in some\nobserved data. maximizing these functions can drive inference.\n\nhow to maximize a function depends on several criteria including the\nnatureofthefunctionandwhatispractical.youcouldarbitrarilychoosevalues\nto input to your function to eventually find a very good choice, or you can\ndo a more guided search. optimization procedures help guide search efforts,\nsome employing more mathematical theory and others using more heuristic\nprinciples. options include methods that rely on derivatives, derivative-free\napproaches, and heuristic strategies. in the next three chapters, we describe\nsome of the statistical contexts within which optimization problems arise and\na variety of methods for solving them.\n\nin chapter 2 we consider fundamental methods for optimizing smooth\nnonlinear equations. such methods are applicable to continuous-valued func-\ntions, as when finding the maximum likelihood estimate of a continuous func-\ntion. in chapter 3 we consider a variety of strategies for combinatorial opti-\nmization. these algorithms address problems where the functions are discrete\nand usually formidably complex, such as finding the optimal set of predictors\nfrom a large set of potential explanatory variables in multiple regression anal-\nysis. the methods in chapters 2 and 3 originate from mathematics and com-\nputer science but are used widely in statistics. the expectation\u2013maximization\n(em) algorithm in chapter 4 is focused on a problem that is frequently en-\ncountered in statistical inference: how do you maximize a likelihood function\nwhen some of the data are missing? it turns out that this powerful algorithm\ncan be used to solve many other statistical problems as well.\n\ncomputational statistics, second edition. geof h. givens and jennifer a. hoeting.\n\u00a9 2013 john wiley & sons, inc. published 2013 by john wiley & sons, inc.\n\n19\n\n "}, {"Page_number": 36, "text": "chapter 2\noptimization and solving\nnonlinear equations\n\nmaximum likelihood estimation is central to statistical inference. long hours can\nbe invested in learning about the theoretical performance of mles and their analytic\nderivation. faced with a complex likelihood lacking analytic solution, however, many\npeople are unsure how to proceed.\nmost functions cannot be optimized analytically. for example, maximizing\ng(x) = (log x)/(1 + x) with respect to x by setting the derivative equal to zero and\nsolving for x leads to an algebraic impasse because 1 + 1/x \u2212 log x = 0 has no\nanalytic solution. many realistic statistical models induce likelihoods that cannot\nbe optimized analytically\u2014indeed, we would argue that greater realism is strongly\nassociated with reduced ability to find optima analytically.\nstatisticiansfaceotheroptimizationtasks,too,asidefrommaximumlikelihood.\nminimizingriskinabayesiandecisionproblem,solvingnonlinearleastsquaresprob-\nlems, finding highest posterior density intervals for many distributions, and a wide\nvariety of other tasks all involve optimization. such diverse tasks are all versions of\nthe following generic problem: optimize a real-valued function g with respect to its\nargument, a p-dimensional vector x. in this chapter, we will limit consideration to\ng that are smooth and differentiable with respect to x; in chapter 3 we discuss opti-\nmization when g is defined over a discrete domain. there is no meaningful distinction\nbetween maximization and minimization, since maximizing a function is equivalent\ntominimizingitsnegative.asaconvention,wewillgenerallyuselanguagesuggestive\nof seeking a maximum.\nfor maximum likelihood estimation, g is the log likelihood function l, and x is\nthe corresponding parameter vector \u03b8. if \u02c6\u03b8 is an mle, it maximizes the log likelihood.\ntherefore \u02c6\u03b8 is a solution to the score equation\nl\u2032(\u03b8) = 0,\n\n(2.1)\n\nwhere\n\nl\u2032(\u03b8) =! dl(\u03b8)\n\nd\u03b81\n\nand 0 is a column vector of zeros.\n\n, . . . ,\n\ndl(\u03b8)\n\nd\u03b8n \"t\n\ncomputational statistics, second edition. geof h. givens and jennifer a. hoeting.\n\u00a9 2013 john wiley & sons, inc. published 2013 by john wiley & sons, inc.\n\n21\n\n "}, {"Page_number": 37, "text": "22\n\nchapter 2 optimization and solving nonlinear equations\nimmediately, we see that optimization is intimately linked with solving non-\nlinear equations. indeed, one could reinterpret this chapter as an introduction to root\nfinding rather than optimization. finding an mle amounts to finding a root of the\nscore equation. the maximum of g is a solution to g\u2032(x) = 0. (conversely, one may\nalso turn a univariate root-finding exercise into an optimization problem by minimiz-\ning##g\u2032(x)## with respect to x, where g\u2032 is the function whose root is sought.)\nthe solution of g\u2032(x) = 0 is most difficult when this set of equations has a\nsolution that cannot be determined analytically. in this case, the equations will be\nnonlinear. solving linear equations is easy, however there is another class of difficult\noptimization problems where the objective function itself is linear and there are linear\ninequality constraints. such problems can be solved using linear programming tech-\nniques such as the simplex method [133, 198, 247, 497] and interior point methods\n[347, 362, 552]. such methods are not covered in this book.\nfor smooth, nonlinear functions, optima are routinely found using a variety\nof off-the-shelf numerical optimization software. many of these programs are very\neffective, raising the question of whether optimization is a solved problem whose\nstudy here might be a low priority. for example, we have omitted the topic of uniform\nrandom number generation from this book\u2014despite its importance in the statistical\ncomputing literature\u2014because of the widespread use of high-quality prepackaged\nroutines that accomplish the task. why, then, should optimization methods be treated\ndifferently? the difference is that optimization software is confronted with a new\nproblem every time the user presents a new function to be optimized. even the best\noptimization software often initially fails to find the maximum for tricky likelihoods\nand requires tinkering to succeed. therefore, the user must understand enough about\nhow optimization works to tune the procedure successfully.\nwe begin by studying univariate optimization. extensions to multivariate prob-\nlems are described in section 2.2. optimization over discrete spaces is covered\nin chapter 3, and an important special case related to missing data is covered in\nchapter 4.\nuseful references on optimization methods include [153, 198, 247, 475, 486,\n494].\n\n2.1 univariate problems\na simple univariate numerical optimization problem that we will discuss throughout\nthis section is to maximize\n\ng(x) =\n\nlog x\n1 + x\n\n(2.2)\n\nwith respect to x. since no analytic solution can be found, we resort to iterative\nmethods that rely on successive approximation of the solution. graphing g(x) in\nfigure 2.1, we see that the maximum is around 3. therefore it might be reasonable\nto use x(0) = 3.0 as an initial guess, or starting value, for an iterative procedure. an\nupdating equation will be used to produce an improved guess, x(t+1), from the most\n\n "}, {"Page_number": 38, "text": "0.3\n\n0.2\n\n0.1\n\n0\n\n)\nx\n(\ng\n\n1\n\n2\n\n2.1 univariate problems\n\n23\n\n2\n1\n1\n9\n5\n\n.\n\n \n\n3\n\u2248\n*\nx\n\n \n\n4\n\n5\n\n3\nx\n\nfigure 2.1 the maximum of g(x) = (log x)/(1 + x) occurs at x\u2217 \u2248 3.59112, indicated by\nthe vertical line.\nrecent value x(t), for t = 0,1,2, . . . until iterations are stopped. the update may be\nbased on an attempt to find the root of\ng\u2032(x) =\n\n1 + 1/x \u2212 log x\n\n,\n\nor on some other rationale.\nthe bisection method illustrates the main components of iterative root-finding\nprocedures. if g\u2032 is continuous on [a0, b0] and g\u2032(a0)g\u2032(b0) \u2264 0, then the intermediate\nvalue theorem [562] implies that there exists at least one x\u2217 \u2208 [a0, b0] for which\ng\u2032(x\u2217) = 0 and hence x\u2217 is a local optimum of g. to find it, the bisection method\nsystematically shrinks the interval from [a0, b0] to [a1, b1] to [a2, b2] and so on,\nwhere [a0, b0] \u2283 [a1, b1] \u2283 [a2, b2] \u2283 \u00b7\u00b7\u00b7 and so forth.\n\nlet x(0) = (a0 + b0)/2 be the starting value. the updating equations are\n\n(1 + x)2\n\n[at+1, bt+1] =$[at, x(t)]\n\n[x(t), bt]\n\nif g\u2032(at)g\u2032(x(t)) \u2264 0,\nif g\u2032(at)g\u2032(x(t)) > 0\n\n(2.3)\n\nand\n\n1\n2(at+1 + bt+1).\n\n=\n\nx(t+1)\n\n(2.4)\nif g has more than one root in the starting interval, it is easy to see that bisection will\nfind one of them, but will not find the rest.\nexample 2.1 (a simple univariate optimization)\nto find the value of x\nmaximizing (2.2), we might take a0 = 1, b0 = 5, and x(0) = 3. figure 2.2 illustrates\nthe first few steps of the bisection algorithm for this simple function.\n!\n\n "}, {"Page_number": 39, "text": "24\n\nchapter 2 optimization and solving nonlinear equations\n\n0.4\n\n0.2\n\n0\n\n)\nx\n(\n\ng\n\na0\n\n1\n\n2\n\nx*\n\nx(2)\n\nx(0)\n\na1\na2\n\n3\nx\n\nx(1)\n\nb2\n\n4\n\nb0\nb1\n\n5\n\nfigure 2.2 illustration of the bisection method from example 2.1. the top portion of this\ngraph shows g\u2032(x) and its root at x\u2217. the bottom portion shows the first three intervals obtained\nusing the bisection method with (a0, b0) = (1, 5). the tth estimate of the root is at the center\nof the tth interval.\n\nsuppose the true maximum of g(x) with respect to x is achieved at x\u2217. the\nupdating equation of any iterative procedure should be designed to encourage x(t) \u2192\nx\u2217 as t increases. of course there is no guarantee that x(t) will converge to anything,\nlet alone to x\u2217.\nin practice, we cannot allow the procedure to run indefinitely, so we require a\nstopping rule, based on some convergence criteria, to trigger an end to the successive\napproximation. at each iteration, the stopping rule should be checked. when the\nconvergence criteria are met, the new x(t+1) is taken as the solution. there are two\nreasons to stop: if the procedure appears to have achieved satisfactory convergence\nor if it appears unlikely to do so soon.\nit is tempting to monitor convergence by tracking the proximity of g\u2032(x(t+1))\nto zero. however, large changes from x(t) to x(t+1) can occur even when g\u2032(x(t+1)) is\nvery small; therefore a stopping rule based directly on g\u2032(x(t+1)) is not very reliable.\non the other hand, a small change from x(t) to x(t+1) is most frequently associated\nwith g\u2032(x(t+1)) near zero. therefore, we typically assess convergence by monitoring\n\nthe absolute convergence criterion mandates stopping when\n\n##x(t+1) \u2212 x(t)## and use g\u2032(x(t+1)) as a backup check.\n\u2212 x(t)### < \u03f5,\n\n###x(t+1)\n\nwhere \u03f5 is a constant chosen to indicate tolerable imprecision. for bisection, it is easy\nto confirm that\n\n(2.5)\n\nbt \u2212 at = 2\u2212t(b0 \u2212 a0).\n\n(2.6)\n\n "}, {"Page_number": 40, "text": "2.1 univariate problems\n\n25\n\na true error tolerance of##x(t) \u2212 x\u2217## < \u03b4 is achieved when 2\u2212(t+1)(b0 \u2212 a0) < \u03b4,\nwhich occurs once t > log2{(b0 \u2212 a0)/\u03b4} \u2212 1. reducing \u03b4 tenfold therefore requires\nan increase in t of log2 10 \u2248 3.3. hence, three or four iterations are needed to achieve\neach extra decimal place of precision.\nthe relative convergence criterion mandates stopping when iterations have\nreached a point for which\n\n< \u03f5.\n\n(2.7)\n\n##x(t+1) \u2212 x(t)##\n##x(t)##\n\nthis criterion enables the specification of a target precision (e.g., within 1%) without\nworrying about the units of x.\npreference between the absolute and relative convergence criteria depends on\nthe problem at hand. if the scale of x is huge (or tiny) relative to \u03f5, an absolute\nconvergence criterion may stop iterations too reluctantly (or too soon). the relative\nconvergence criterion corrects for the scale of x, but can become unstable if x(t)\nvalues (or the true solution) lie too close to zero. in this latter case, another option is\nto monitor relative convergence by stopping when\n\n##x(t+1) \u2212 x(t)##\n##x(t)## + \u03f5\n\n< \u03f5.\n\nbisection works when g\u2032 is continuous. taking limits on both sides of (2.6)\nimplies limt\u2192\u221e at = limt\u2192\u221e bt; therefore the bisection method converges to some\npoint x(\u221e). the method always ensures that g\u2032(at)g\u2032(bt) \u2264 0; continuity therefore\nimplies that g\u2032(x(\u221e))2 \u2264 0. thus g\u2032(x(\u221e)) must equal zero, which proves that x(\u221e)\nis a root of g. in other words, the bisection method is\u2014in theory\u2014guaranteed to\nconverge to a root in [a0, b0].\nin practice, numerical imprecision in a computer may thwart convergence. for\nmost iterative approximation methods, it is safer to add a small correction to a previ-\nous approximation than to initiate a new approximation from scratch. the bisection\nmethod is more stable numerically when the updated endpoint is calculated as, say,\nat+1 = at + (bt \u2212 at)/2 instead of at+1 = (at + bt)/2. yet, even carefully coded al-\ngorithms can fail, and optimization procedures more sophisticated than bisection can\nfail for all sorts of reasons. it is also worth noting that there are pathological circum-\nstances where the mle is not a solution of the score equation or the mle is not\nunique; see [127] for examples.\ngiven such anomalies, it is important to include stopping rules that flag a failure\nto converge. the simplest such stopping rule is to stop after n iterations, regardless\nof convergence. it may also be wise to stop if one or more convergence measures like\nover several iterations. the solution itself may also cycle unsatisfactorily. it is also\nsensible to stop if the procedure appears to be converging to a point at which g(x) is\ninferior to another value you have already found. this prevents wasted effort when a\nsearch is converging to a known false peak or local maximum. regardless of which\nsuch stopping rules you employ, any indication of poor convergence behavior means\n\n##x(t+1) \u2212 x(t)## or ##x(t+1) \u2212 x(t)##%##x(t)##, or##g\u2032(x(t+1))## either fail to decrease or cycle\n\n "}, {"Page_number": 41, "text": "chapter 2 optimization and solving nonlinear equations\n\n26\nthat x(t+1) must be discarded and the procedure somehow restarted in a manner more\nlikely to yield successful convergence.\nstarting is as important as stopping. in general, a bad starting value can lead to\ndivergence, cycling, discovery of a misleading local maximum or a local minimum,\nor other problems. the outcome depends on g, the starting value, and the optimization\nalgorithm tried. in general, it helps to start quite near the global optimum, as long\nas g is not virtually flat in the neighborhood containing x(0) and x\u2217. methods for\ngenerating a reasonable starting value include graphing, preliminary estimates (e.g.,\nmethod-of-moments estimates), educated guesses, and trial and error. if computing\nspeed limits the total number of iterations that you can afford, it is wise not to invest\nthem all in one long run of the optimization procedure. using a collection of runs from\nmultiple starting values (see the random starts local search method in section 3.2)\ncan be an effective way to gain confidence in your result and to avoid being fooled\nby local optima or stymied by convergence failure.\nthe bisection method is an example of a bracketing method, that is to say, a\nmethod that bounds a root within a sequence of nested intervals of decreasing length.\nbisection is quite a slow approach: it requires a rather large number of iterations to\nachieve a desired precision, relative to other methods discussed below. other brack-\neting methods include the secant bracket [630], which is equally slow after an initial\nperiod of greater efficiency, and the illinois method [348], ridders\u2019s method [537],\nand brent\u2019s method [68], which are faster.\ndespite the relative slowness of bracketing methods, they have one significant\nadvantage over the methods described in the remainder of this chapter. if g\u2032 is contin-\nuous on [a0, b0], a root can be found, regardless of the existence, behavior, or ease of\nderiving g\u2032\u2032. because they avoid worries about g\u2032\u2032 while performing relatively robustly\non most problems, bracketing methods continue to be reasonable alternatives to the\nmethods below that rely on greater smoothness of g.\n\n2.1.1 newton\u2019s method\nan extremely fast root-finding approach is newton\u2019s method. this approach is also\nreferred to as newton\u2013raphson iteration, especially in univariate applications. sup-\npose that g\u2032 is continuously differentiable and that g\u2032\u2032(x\u2217) /= 0. at iteration t, the\napproach approximates g\u2032(x\u2217) by the linear taylor series expansion:\n(2.8)\nsince g\u2032 is approximated by its tangent line at x(t), it seems sensible to approximate\nthe root of g\u2032 by the root of the tangent line. thus, solving for x\u2217 above, we obtain\n\n0 = g\u2032(x\u2217) \u2248 g\u2032(x(t)) + (x\u2217 \u2212 x(t))g\u2032\u2032(x(t)).\n\nx\u2217 = x(t)\n\n\u2212\n\ng\u2032(x(t))\ng\u2032\u2032(x(t)) = x(t)\n\n+ h(t).\n\n(2.9)\n\nthis equation describes an approximation to x\u2217 that depends on the current guess x(t)\nand a refinement h(t). iterating this strategy yields the updating equation for newton\u2019s\nmethod:\n\nx(t+1)\n\n= x(t)\n\n+ h(t),\n\n(2.10)\n\n "}, {"Page_number": 42, "text": "2.1 univariate problems\n\n27\n\n.03\n\n.02\n\n)\nx\n(\n\ng\n\n.01\n\n0\n\nx(0)\n\nx(1)\n\nx(2)\n\nx*\n\n2.6\n\n3.0\n\n3.4\n\n3.8\n\nfigure 2.3 illustration of newton\u2019s method applied in example 2.2. at the first step,\nnewton\u2019s method approximates g\u2032 by its tangent line at x(0), whose root x(1) serves as the next\napproximation of the true root x\u2217. the next step similarly yields x(2), which is already quite\nclose to x\u2217.\n\nx\n\nwhere h(t) = \u2212g\u2032(x(t))%g\u2032\u2032(x(t)). the same update can be motivated by analytically\nsolving for the maximum of the quadratic taylor series approximation to g(x\u2217),\nnamely g(x(t)) + (x\u2217 \u2212 x(t))g\u2032(x(t)) + (x\u2217 \u2212 x(t))2g\u2032\u2032(x(t))/2. when the optimization\nof g corresponds to an mle problem where \u02c6\u03b8 is a solution to l\u2032(\u03b8) = 0, the updating\nequation for newton\u2019s method is\n\n\u03b8(t+1)\n\n= \u03b8(t)\n\n\u2212\n\nl\u2032(\u03b8(t))\nl\u2032\u2032(\u03b8(t)) .\n\n(2.11)\n\nexample 2.2 (a simple univariate optimization, continued) figure 2.3 illus-\ntrates the first several iterations of newton\u2019s method applied to the simple function\nin (2.2).\n\nthe newton increment for this problem is given by\n\n=\n\nh(t)\n\n(x(t) + 1)&1 + 1/x(t) \u2212 log x(t)\u2019\n3 + 4/x(t) + 1/(x(t))2 \u2212 2 log x(t) .\n\n(2.12)\nstarting from x(0) = 3.0, newton\u2019s method quickly finds x(4) \u2248 3.59112. for com-\nparison,thefirstfivedecimalplacesof x\u2217 arenotcorrectlydeterminedbythebisection\nmethod in example 2.1 until iteration 19.\n!\nwhether newton\u2019s method converges depends on the shape of g and the starting\nvalue. figure 2.4 illustrates an example where the method diverges from its starting\nvalue. to better understand what ensures convergence, we must carefully analyze the\nerrors at successive steps.\n\n "}, {"Page_number": 43, "text": "28\n\nchapter 2 optimization and solving nonlinear equations\n\nx(3)\n\nx(1)\n\n)\nx\n(\n\ng\n\nx*\n\nx(0)\n\nx(2)\n\nfigure 2.4 starting from x(0), newton\u2019s method diverges by taking steps that are increas-\ningly distant from the true root, x\u2217.\n\nx\n\nsuppose g\u2032 has two continuous derivatives and g\u2032\u2032(x\u2217) /= 0. since g\u2032\u2032(x\u2217) /= 0\nand g\u2032\u2032 is continuous at x\u2217, there exists a neighborhood of x\u2217 within which g\u2032\u2032(x) /= 0\nfor all x. let us confine interest to this neighborhood, and define \u03f5(t) = x(t) \u2212 x\u2217.\na taylor expansion yields\n0 = g\u2032(x\u2217) = g\u2032(x(t)) + (x\u2217 \u2212 x(t))g\u2032\u2032(x(t)) +\n\n1\n2(x\u2217 \u2212 x(t))2g\u2032\u2032\u2032(q)\n\n(2.13)\n\nfor some q between x(t) and x\u2217. rearranging terms, we find\n\u2212 x\u2217 = (x\u2217 \u2212 x(t))2 g\u2032\u2032\u2032(q)\n2g\u2032\u2032(x(t)) ,\n\n+ h(t)\n\nx(t)\n\n(2.14)\n\nwhere h(t) isthenewtonupdateincrement.sincetheleft-handsideequals x(t+1) \u2212 x\u2217,\nwe conclude\n\n\u03f5(t+1)\n\n(2.15)\nnow, consider a neighborhood of x\u2217, n\u03b4(x\u2217) = [x\u2217 \u2212 \u03b4, x\u2217 + \u03b4], for \u03b4 > 0. let\n(2.16)\n\ng\u2032\u2032\u2032(x1)\n\nc(\u03b4) =\n\nmax\n\n= (\u03f5(t))2 g\u2032\u2032\u2032(q)\n2g\u2032\u2032(x(t)) .\nx1,x2\u2208n\u03b4(x\u2217)####\n2g\u2032\u2032(x2)#### .\n2g\u2032\u2032(x\u2217)####\nc(\u03b4) \u2192####\n\ng\u2032\u2032\u2032(x\u2217)\n\nsince\n\n "}, {"Page_number": 44, "text": "(2.18)\n\n29\nas \u03b4 \u2192 0, it follows that \u03b4c(\u03b4) \u2192 0 as \u03b4 \u2192 0. let us choose \u03b4 such that \u03b4c(\u03b4) < 1. if\nx(t) \u2208 n\u03b4(x\u2217), then (2.15) implies that\n\n2.1 univariate problems\n\n.\n\n(2.17)\n\nsupposethatthestartingvalueisnottoobad,inthesensethat##\u03f5(0)## =##x(0) \u2212 x\u2217## \u2264 \u03b4.\n\nthen (2.17) implies that\n\n###c(\u03b4)\u03f5(t+1)### \u2264(c(\u03b4)\u03f5(t))2\n###\u03f5(t)### \u2264\n\n(c(\u03b4)\u03b4)2t\nc(\u03b4)\n\n,\n\nwhich converges to zero as t \u2192 \u221e. hence x(t) \u2192 x\u2217.\nwe have just proven the following theorem: if g\u2032\u2032\u2032 is continuous and x\u2217 is a\nsimple root of g\u2032, then there exists a neighborhood of x\u2217 for which newton\u2019s method\nconverges to x\u2217 when started from any x(0) in that neighborhood.\nin fact, when g\u2032 is twice continuously differentiable, is convex, and has a root,\nthen newton\u2019s method converges to the root from any starting point. when starting\nfrom somewhere in an interval [a, b], another set of conditions one may check is as\nfollows. if\n1. g\u2032\u2032(x) /= 0 on [a, b],\n2. g\u2032\u2032\u2032(x) does not change sign on [a, b],\n3. g\u2032(a)g\u2032(b) < 0, and\n4. ##g\u2032(a)/g\u2032\u2032(a)## < b \u2212 a and##g\u2032(b)/g\u2032\u2032(b)## < b \u2212 a,\n\nthen newton\u2019s method will converge from any x(0) in the interval. results like these\ncan be found in many introductory numerical analysis books such as [131, 198, 247,\n376]. a convergence theorem with less stringent conditions is provided by [495].\n2.1.1.1 convergence order the speed of a root-finding approach like\nnewton\u2019s method is typically measured by its order of convergence. a method has\nconvergence of order \u03b2 if limt\u2192\u221e \u03f5(t) = 0 and\nt\u2192\u221e##\u03f5(t+1)##\n##\u03f5(t)##\u03b2 = c\n\nfor some constants c /= 0 and \u03b2 > 0. higher orders of convergence are better in\nthe sense that precise approximation of the true solution is more quickly achieved.\nunfortunately,highordersaresometimesachievedattheexpenseofrobustness:some\nslow algorithms are more foolproof than their faster counterparts.\n\n(2.19)\n\nlim\n\nfor newton\u2019s method, (2.15) shows us that\ng\u2032\u2032\u2032(q)\n2g\u2032\u2032(x(t)) .\n\n\u03f5(t+1)\n(\u03f5(t))2 =\n\n(2.20)\n\nif newton\u2019s method converges, then continuity arguments allow us to note that\nthe right-hand side of this equation converges to g\u2032\u2032\u2032(x\u2217)/[2g\u2032\u2032(x\u2217)]. thus, newton\u2019s\n\n "}, {"Page_number": 45, "text": "chapter 2 optimization and solving nonlinear equations\n\n30\nmethod has quadratic convergence (i.e., \u03b2 = 2), and\n\nc =####\n\ng\u2032\u2032\u2032(x\u2217)\n\n2g\u2032\u2032(x\u2217)#### .\n\nquadratic convergence is indeed fast: usually the precision of the solution will double\nwith each iteration.\nfor bisection, the length of the bracketing interval exhibits a property anal-\nogous to linear convergence (\u03b2 = 1), in that it is halved at each iteration and\nlimt\u2192\u221e |\u03f5(t)| = 0 if there is a root in the starting interval. however, the distances\nx(t) \u2212 x\u2217 need not shrink at every iteration, and indeed their ratio is potentially\nunbounded. thus\n\nlim\nt\u2192\u221e\n\n|\u03f5(t+1)|\n|\u03f5(t)|\u03b2\n\nmay not exist for any \u03b2 > 0, and bisection does not formally meet the definition for\ndetermining order of convergence.\nitispossibletouseafoolproofbracketingmethodsuchasbisectiontosafeguard\na faster but less reliable root-finding approach such as newton\u2019s method. instead of\nviewing the bracketing approach as a method to generate steps, view it only as a\nmethod providing an interval within which a root must lie. if newton\u2019s method seeks\na step outside these current bounds, the step must be replaced, curtailed, or (in the\nmultivariate case) redirected. some strategies are mentioned in section 2.2 and in\n[247]. safeguarding can reduce the convergence order of a method.\n\nfisher scoring\n\n2.1.2\nrecall from section 1.4 that i(\u03b8) can be approximated by \u2212l\u2032\u2032(\u03b8). therefore when\nthe optimization of g corresponds to an mle problem, it is reasonable to replace\n\u2212l\u2032\u2032(\u03b8) in the newton update with i(\u03b8). this yields an updating increment of h(t) =\nl\u2032(\u03b8(t))/i(\u03b8(t)) where i(\u03b8(t)) is the expected fisher information evaluated at \u03b8(t). the\nupdating equation is therefore\n\u03b8(t+1)\n\n= \u03b8(t)\nthis approach is called fisher scoring.\nfisher scoring and newton\u2019s method both have the same asymptotic properties,\nbut for individual problems one may be computationally or analytically easier than\nthe other. generally, fisher scoring works better in the beginning to make rapid\nimprovements, while newton\u2019s method works better for refinement near the end.\n\n+ l\u2032(\u03b8(t))i(\u03b8(t))\u22121.\n\n(2.21)\n\n2.1.3 secant method\nthe updating increment for newton\u2019s method in (2.10) relies on the second deriva-\ntive, g\u2032\u2032(x(t)). if calculating this derivative is difficult, it might be replaced by the\n\n "}, {"Page_number": 46, "text": "0.2\n\nx (0)\n\n0.1\n\n)\nx\n(\n\ng\n\n0\n\n2.1 univariate problems\n\n31\n\nx (1)\n\nx (2)\n\nx (3)\n\nx*\n\n1.5\n\n2.0\n\n2.5\n\n3.0\n\n3.5\n\n4.0\n\nfigure2.5 thesecantmethodlocallyapproximates g\u2032 usingthesecantlinebetween x(0) and\nx(1). the corresponding estimated root, x(2), is used with x(1) to generate the next approximation.\n\nx\n\ndiscrete-difference approximation, [g\u2032(x(t)) \u2212 g\u2032(x(t\u22121))]/(x(t) \u2212 x(t\u22121)). the result is\nthe secant method, which has updating equation\n\nx(t+1)\n\n= x(t)\n\n\u2212 g\u2032(x(t))\n\nx(t) \u2212 x(t\u22121)\n\ng\u2032(x(t)) \u2212 g\u2032(x(t\u22121))\n\n(2.22)\n\nfor t \u2265 1. this approach requires two starting points, x(0) and x(1). figure 2.5 illus-\ntrates the first steps of the method for maximizing the simple function introduced in\nexample 2.1.\nunder conditions akin to those for newton\u2019s method, the secant method will\nconverge to the root x\u2217. to find the order of convergence in this case, restrict attention\nto a suitably small interval [a, b], containing x(0), x(1), and x\u2217, on which g\u2032\u2032(x) /= 0\nand g\u2032\u2032\u2032(x) /= 0. letting \u03f5(t+1) = x(t+1) \u2212 x\u2217, it is straightforward to show that\n+,\u03f5(t)\u03f5(t\u22121)-\n\n=*\ng\u2032(x(t)) \u2212 g\u2032(x(t\u22121))+* g\u2032(x(t))/\u03f5(t) \u2212 g\u2032(x(t\u22121))/\u03f5(t\u22121)\nx(t) \u2212 x(t\u22121)\n= a(t)b(t)\u03f5(t)\u03f5(t\u22121),\n\nx(t) \u2212 x(t\u22121)\n\n(2.23)\n\n\u03f5(t+1)\n\nwhere a(t) \u2192 1/g\u2032\u2032(x\u2217) as x(t) \u2192 x\u2217 for continuous g\u2032\u2032.\n\nto deduce a limit for b(t), expand g\u2032 in a taylor series about x\u2217:\ng\u2032(x(t)) \u2248 g\u2032(x\u2217) + (x(t)\n\u2212 x\u2217)2g\u2032\u2032\u2032(x\u2217),\n\n\u2212 x\u2217)g\u2032\u2032(x\u2217) +\n\n1\n2(x(t)\n\n(2.24)\n\n "}, {"Page_number": 47, "text": "chapter 2 optimization and solving nonlinear equations\n\n32\nso\n\ng\u2032(x(t))\n\u03f5(t) \u2248 g\u2032\u2032(x\u2217) +\n\n\u03f5(t)g\u2032\u2032\u2032(x\u2217)\n\n2\n\n.\n\n(2.25)\n\n(2.26)\n\n,\n\nsimilarly, g\u2032(x(t\u22121))/\u03f5(t\u22121) \u2248 g\u2032\u2032(x\u2217) + \u03f5(t\u22121)g\u2032\u2032\u2032(x\u2217)/2. thus,\ng\u2032\u2032\u2032(x\u2217)\n\nb(t)\n\n\u2248 g\u2032\u2032\u2032(x\u2217) \u03f5(t) \u2212 \u03f5(t\u22121)\n\n2(x(t) \u2212 x(t\u22121)) =\n\n2\n\nand a careful examination of the errors shows this approximation to be exact as\nx(t) \u2192 x\u2217. thus,\n\n\u03f5(t+1)\n\n\u2248 d(t)\u03f5(t)\u03f5(t\u22121),\n\n(2.27)\n\nwhere\n\nd(t)\n\n\u2192\n\ng\u2032\u2032\u2032(x\u2217)\n2g\u2032\u2032(x\u2217) = d as t \u2192 \u221e.\n\nto find the order of convergence for the secant method, we must find the \u03b2 for\n\nwhich\n\nlim\nt\u2192\u221e\n\n|\u03f5(t+1)|\n|\u03f5(t)|\u03b2 = c\n\nfor some constant c. suppose that this relationship does indeed hold, and use this\nproportionality expression to replace \u03f5(t\u22121) and \u03f5(t+1) in (2.27), leaving only terms in\n\u03f5(t). then, after some rearrangement of terms, it suffices to find the \u03b2 for which\n\nt\u2192\u221e|\u03f5(t)\nlim\n\n(2.28)\nthe right-hand side of (2.28) is a positive constant. therefore 1 \u2212 \u03b2 + 1/\u03b2 = 0. the\nsolution is \u03b2 = (1 + \u221a5)/2 \u2248 1.62. thus, the secant method has a slower order of\nconvergence than newton\u2019s method.\n\n1\u2212\u03b2+1/\u03b2 =\n\n|\n\nd\n\n.\n\nc1+1/\u03b2\n\nfixed-point iteration\n\n2.1.4\na fixed-point of a function is a point whose evaluation by that function equals itself.\nthe fixed-point strategy for finding roots is to determine a function g for which\ng\u2032(x) = 0 if and only if if g(x) = x. this transforms the problem of finding a root of\ng\u2032 into a problem of finding a fixed point of g. then the simplest way to hunt for a\nfixed point is to use the updating equation x(t+1) = g(x(t)).\nany suitable function g may be tried, but the most obvious choice is g(x) =\ng\u2032(x) + x. this yields the updating equation\n= x(t)\n(2.29)\n\n+ g\u2032(x(t)).\n\nx(t+1)\n\n "}, {"Page_number": 48, "text": "2.1 univariate problems\n\n33\nthe convergence of this algorithm depends on whether g is contractive. to be\n\ncontractive on [a, b], g must satisfy\n1. g(x) \u2208 [a, b] whenever x \u2208 [a, b], and\n2. |g(x1) \u2212 g(x2)| \u2264 \u03bb|x1 \u2212 x2| for all x1, x2 \u2208 [a, b] for some \u03bb \u2208 [0,1).\nthe interval [a, b] may be unbounded. the second requirement is a lipschitz condi-\ntion,and \u03bbiscalledthelipschitzconstant.if giscontractiveon[a, b],thenthereexists\na unique fixed point x\u2217 in this interval, and the fixed-point algorithm will converge to\nit from any starting point in the interval. furthermore, under the conditions above,\n\n|x(t)\n\n\u2212 x\u2217| \u2264\n\n\u03bbt\n\n1 \u2212 \u03bb|x(1)\n\n\u2212 x(0)\n\n|.\n\n(2.30)\n\nproof of a contractive mapping theorem like this can be found in [6, 521].\nnewton\u2019s method and the secant method are special cases of fixed-point iteration.\n\nfixed-point iteration is sometimes called functional iteration. note that both\n\nscaling if fixed-point iteration converges, the order of convergence de-\n2.1.4.1\npends on \u03bb. convergence is not universally assured. in particular, the lipschitz con-\ndition holds if |g\u2032(x)| \u2264 \u03bb < 1 for all x in [a, b]. if g(x) = g\u2032(x) + x, this amounts\nto requiring |g\u2032\u2032(x) + 1| < 1 on [a, b]. when g\u2032\u2032 is bounded and does not change sign\non [a, b], we can rescale nonconvergent problems by choosing g(x) = \u03b1g\u2032(x) + x\nfor \u03b1 /= 0, since \u03b1g\u2032(x) = 0 if and only if g\u2032(x) = 0. to permit convergence, \u03b1 must\nbe chosen to satisfy |\u03b1g\u2032\u2032(x) + 1| < 1 on an interval including the starting value.\nalthough one could carefully calculate a suitable \u03b1, it may be easier just to try a few\nvalues. if the method converges quickly, then the chosen \u03b1 was suitable.\nrescaling is only one of several strategies for adjusting g. in general, the\neffectiveness of fixed-point iteration is highly dependent on the chosen form of g.\nforexample,considerfindingtherootof g\u2032(x) = x + log x.then g(x) = (x + e\u2212x)/2\nconverges quickly, whereas g(x) = e\u2212x converges more slowly and g(x) = \u2212log x\nfails to converge at all.\nexample 2.3 (a simple univariate optimization, continued) figure 2.6 illus-\ntrates the first several steps of the scaled fixed-point algorithm for maximizing the\nfunction g(x) = (log x)/(1 + x) in (2.2) using g(x) = g\u2032(x) + x and \u03b1 = 4. note that\nline segments whose roots determine the next x(t) are parallel, with slopes equal\nto \u22121/\u03b1. for this reason, the method is sometimes called the method of parallel\nchords.\n!\nsuppose an mle is sought for the parameter of a quadratic log likelihood l, or\none that is nearly quadratic near \u02c6\u03b8. then the score function is locally linear, and l\u2032\u2032 is\nroughly a constant, say \u03b3. for quadratic log likelihoods, newton\u2019s method would use\nthe updating equation \u03b8(t+1) = \u03b8(t) \u2212 l\u2032(\u03b8)/\u03b3. if we use scaled fixed-point iteration\nwith \u03b1 = \u22121/\u03b3, we get the same updating equation. since many log likelihoods are\napproximately locally quadratic, scaled fixed-point iteration can be a very effective\ntool. the method is also generally quite stable and easy to code.\n\n "}, {"Page_number": 49, "text": "34\n\nchapter 2 optimization and solving nonlinear equations\n\nx(0)\n\n0.2\n\n0.1\n\n)\nx\n(\n\ng\n\n0\n\nx(1)\n\nx(2)\n\nx(3)\n\nx*\n\n1.5\n\n2.0\n\n2.5\n\n3.0\n\n3.5\n\n4.0\n\nfigure 2.6 first\n(log x)/(1 + x) using g(x) = g\u2032(x) + x and scaling with \u03b1 = 4, as in example 2.3.\n\nthree steps of scaled fixed-point\n\niteration to maximize g(x) =\n\nx\n\np )t.\n\n(t)\n1 , . . . , x(t)\n\n2.2 multivariate problems\nin a multivariate optimization problem we seek the optimum of a real-valued function\ng of a p-dimensional vector x = (x1, . . . , xp)t. at iteration t, denote the estimated\noptimum as x(t) = (x\nmany of the general principles discussed above for the univariate case also\napply for multivariate optimization. algorithms are still iterative. many algorithms\ntake steps based on a local linearization of g\u2032 derived from a taylor series or se-\ncant approximation. convergence criteria are similar in spirit despite slight changes\nin form. to construct convergence criteria, let d(u,v) be a distance measure\ni=1 |ui \u2212 vi| and\ni=1(ui \u2212 vi)2. then absolute and relative convergence criteria can be\n\nfor p-dimensional vectors. two obvious choices are d(u,v) =.p\nd(u,v) =/.p\nor d(x(t+1),x(t))\nd(x(t),0) + \u03f5\n\nd(x(t+1),x(t))\nd(x(t),0) < \u03f5,\n\nformed from the inequalities\n\nd(x(t+1),x(t)) < \u03f5,\n\n< \u03f5.\n\n2.2.1 newton\u2019s method and fisher scoring\nto fashion the newton\u2019s method update, we again approximate g(x\u2217) by the quadratic\ntaylor series expansion\n\ng(x\u2217) = g(x(t)) + (x\u2217 \u2212 x(t))tg\u2032(x(t)) +\n\n1\n2(x\u2217 \u2212 x(t))tg\u2032\u2032(x(t))(x\u2217 \u2212 x(t))\n\n(2.31)\n\n "}, {"Page_number": 50, "text": "2.2 multivariate problems\n\n35\n\nx(0)\na\n\nx(0)\nb\n\nx*\n\nfigure 2.7 application of newton\u2019s method for maximizing a complicated bivariate func-\ntion, as discussed in example 2.4. the surface of the function is indicated by shading and\na and x(0)\ncontours, with light shading corresponding to high values. two runs starting from x(0)\nare shown. these converge to the true maximum and to a local minimum, respectively.\n\nb\n\nand maximize this quadratic function with respect to x\u2217 to find the next iterate. setting\nthe gradient of the right-hand side of (2.31) equal to zero yields\n\ng\u2032(x(t)) + g\u2032\u2032(x(t))(x\u2217 \u2212 x(t)) = 0.\n\n(2.32)\n\nthis provides the update\n\nx(t+1)\n\n= x(t)\n\n\u2212 g\u2032\u2032(x(t))\u22121g\u2032(x(t)).\n\n(2.33)\nalternatively, note that the left-hand side of (2.32) is in fact a linear taylor\nseries approximation to g\u2032(x\u2217), and solving (2.32) amounts to finding the root of this\nlinear approximation. from either viewpoint, the multivariate newton increment is\nh(t) = \u2212g\u2032\u2032(x(t))\u22121g\u2032(x(t)).\nas in the univariate case, in mle problems we may replace the observed\ninformation at \u03b8(t) with i(\u03b8(t)), the expected fisher information at \u03b8(t). this yields\nthe multivariate fisher scoring approach with update given by\n\n\u03b8(t+1)\n\n= \u03b8(t)\n\n+ i(\u03b8(t))\u22121l\u2032(\u03b8(t)).\n\n(2.34)\n\nthis method is asymptotically equivalent to newton\u2019s method.\nexample 2.4 (bivariate optimization problem) figure 2.7 illustrates the appli-\ncation of newton\u2019s method to a complicated bivariate function. the surface of the\nfunction is indicated by shading and contour lines, with high values corresponding to\na and x(0)\nlight shading. the algorithm is started from two different starting points, x(0)\nb .\nfrom x(0)\na , the algorithm converges quickly to the true maximum. note that although\nsteps were taken in an uphill direction, some step lengths were not ideal. from x(0)\nb ,\n\n "}, {"Page_number": 51, "text": "chapter 2 optimization and solving nonlinear equations\n\n36\nwhich lies very close to x(0)\na , the algorithm fails to maximize the function\u2014in fact it\nconverges to a local minimum. one step length in this attempt was so large as to com-\npletely overshoot the uphill portion of the ridge, resulting in a step that was downhill.\nnear the end, the algorithm steps downhill because it has honed in on the wrong root\nof g\u2032. in section 2.2.2, approaches for preventing such problems are discussed. !\niteratively reweighted least squares consider finding the mles\n2.2.1.1\nfor the parameters of a logistic regression model, which is a well-known type of\ngeneralized linear model [446]. in a generalized linear model, response variables\nyi for i = 1, . . . , n are independently distributed according to a distribution pa-\nrameterized by \u03b8i. different types of response variables are modeled with different\ndistributions, but the distribution is always a member of the scaled exponential fam-\nily. this family has the form f(y|\u03b8) = exp{[y\u03b8 \u2212 b(\u03b8)]/a(\u03c6) + c(y, \u03c6)}, where \u03b8 is\ncalled the natural or canonical parameter and \u03c6 is the dispersion parameter. two of the\nmost useful properties of this family are that e{y} = b\u2032(\u03b8) and var{y} = b\u2032\u2032(\u03b8)a(\u03c6)\n(see section 1.3).\nthe distribution of each yi is modeled to depend on a corresponding set of\nobserved covariates, zi. specifically, we assume that some function of e{yi|zi} can\nbe related to zi according to the equation g(e{yi|zi}) = zt\ni \u03b2, where \u03b2 is a vector of\nparameters and g is called the link function.\nthe generalized linear model used for logistic regression is based on the\nbernoulli distribution, which is a member of the exponential family. model the re-\nsponse variables as yi|zi \u223c bernoulli(\u03c0i) independently for i = 1, . . . , n. suppose\nthe observed data consist of a single covariate value zi and a response value yi, for\ni = 1, . . . , n.definethecolumnvectorszi = (1, zi)t and \u03b2 = (\u03b20, \u03b21)t.thenforthe\nith observation, the natural parameter is \u03b8i = log{\u03c0i/(1 \u2212 \u03c0i)}, a(\u03c6) = 1, and b(\u03b8i) =\nlog{1 + exp{\u03b8i}} = log{1 + exp{zt\n\ni \u03b2}} = \u2212log{1 \u2212 \u03c0i}. the log likelihood is\nl(\u03b2) = ytz\u03b2 \u2212 bt1,\n\n(2.35)\nwhere 1 is a column vector of ones, y = (y1 . . . yn)t, b = (b(\u03b81) . . . b(\u03b8n))t, and z\nis the n \u00d7 2 matrix whose ith row is zt\ni .\nconsider using newton\u2019s method to find \u03b2 that maximizes this likelihood. the\nscore function is\n\nl\u2032(\u03b2) = zt(y \u2212 \u03c0),\n\n(2.36)\n\nwhere \u03c0 is a column vector of the bernoulli probabilities \u03c01, . . . , \u03c0n. the hessian is\ngiven by\n\nl\u2032\u2032(\u03b2) =\n\nd\nd\u03b2\n\nd\u03b2\"t\n(zt(y \u2212 \u03c0)) = \u2212! d\u03c0\n\nz = \u2212ztwz,\n\n(2.37)\n\nwhere w is a diagonal matrix with ith diagonal entry equal to \u03c0i(1 \u2212 \u03c0i).\n\n "}, {"Page_number": 52, "text": "newton\u2019s update is therefore\n\n2.2 multivariate problems\n\n37\n\n\u03b2(t+1)\n\n= \u03b2(t)\n= \u03b2(t)\n\n\u2212 l\u2032\u2032(\u03b2(t))\u22121l\u2032(\u03b2(t))\n+(ztw(t)z)\u22121(zt(y \u2212 \u03c0(t))) ,\n\n(2.38)\n(2.39)\n\nwhere \u03c0(t) is the value of \u03c0 corresponding to \u03b2(t), and w(t) is the diagonal weight\nmatrix evaluated at \u03c0(t).\nnotethatthehessiandoesnotdependony.therefore,fisher\u2019sinformationma-\ntrix is equal to the observed information: i(\u03b2) = e{\u2212l\u2032\u2032(\u03b2)} = e{ztwz} = \u2212l\u2032\u2032(\u03b2).\ntherefore, for this example the fisher scoring approach is the same as newton\u2019s\nmethod. for generalized linear models, this will always be true when the link func-\ntion is chosen to make the natural parameter a linear function of the covariates.\nexample 2.5 (human face recognition) we will fit a logistic regression model\nto some data related to testing a human face recognition algorithm. pairs of images\nof faces of 1072 humans were used to train and test an automatic face recognition al-\ngorithm [681]. the experiment used the recognition software to match the first image\nof each person (called a probe) to one of the remaining 2143 images. ideally, a match\nis made to the other image of the same person (called the target). a successful match\nyielded a response of yi = 1 and a match to any other person yielded a response of\nyi = 0. the predictor variable used here is the absolute difference in mean standard-\nized eye region pixel intensity between the probe image and its corresponding target;\nthis is a measure of whether the two images exhibit similar quality in the important\ndistinguishing region around the eyes. large differences in eye region pixel intensity\nwould be expected to impede recognition (i.e., successful matches). for the data de-\nscribed here, there were 775 correct matches and 297 mismatches. the median and\n90th percentile values of the predictor were 0.033 and 0.097, respectively, for image\npairs successfully matched, and 0.060 and 0.161 for unmatched pairs. therefore, the\ndata appear to support the hypothesis that eye region pixel intensity discrepancies\nimpede recognition. these data are available from the website for this book; analyses\nof related datasets are given in [250, 251].\ntoquantifytherelationshipbetweenthesevariables,wewillfitalogisticregres-\nsion model. thus, zi is the absolute difference in eye region intensity for an image pair\nand yi indicates whether the ith probe was successfully matched, for i = 1, . . . ,1072.\nthe likelihood function is composed as in (2.35), and we will apply newton\u2019s\nmethod.\n= (0.95913,0)t, which means \u03c0i =\n775/1072 for all i at iteration 0. table 2.1 shows that the approximation converges\nquickly, with \u03b2(4)\n= (1.73874,\u221213.58840)t. quick convergence is also achieved\nfrom the starting value corresponding to \u03c0i = 0.5 for all i (namely \u03b2(0)\n= 0), which\nis a suggested rule of thumb for fitting logistic regression models with bernoulli data\n[320]. since \u02c6\u03b21 = \u221213.59 is nearly 9 marginal standard deviations below zero, these\ndata strongly support the hypothesis that eye region intensity discrepancies impede\nrecognition.\n!\n\nto start, we may take \u03b2(0)\n\n=&\u03b2\n\n1 \u2019t\n\n(0)\n0 , \u03b2\n\n(0)\n\n "}, {"Page_number": 53, "text": "38\n\nchapter 2 optimization and solving nonlinear equations\n\ntable 2.1 parameter estimates and corresponding variance\u2013\ncovariance matrix estimates are shown for each newton\u2019s method\niteration for fitting a logistic regression model to the face recognition\ndata described in example 2.5.\niteration, t\n\n\u03b2(t)\n\n\u2212l\u2032\u2032(\u03b2(t))\u22121\n\n0\n\n1\n\n2\n\n3\n\n4\n\n! 0.95913\n0.00000\"\n! 1.70694\n\u221214.20059\"\n! 1.73725\n\u221213.56988\"\n! 1.73874\n\u221213.58839\"\n! 1.73874\n\u221213.58840\"\n\n! 0.01067 \u22120.11412\n2.16701\"\n\u22120.11412\n! 0.13312 \u22120.14010\n2.36367\"\n\u22120.14010\n! 0.01347 \u22120.13941\n2.32090\"\n\u22120.13941\n! 0.01349 \u22120.13952\n2.32241\"\n\u22120.13952\n! 0.01349 \u22120.13952\n2.32241\"\n\u22120.13952\n\nthe fisher scoring approach to maximum likelihood estimation for generalized\nlinear models is important for several reasons. first, it is an application of the method\nof iteratively reweighted least squares (irls). let\n= y \u2212 \u03c0(t)\n\n(2.40)\n\ne(t)\n\nand\n\nnow the fisher scoring update can be written as\n\nx(t)\n\n= z\u03b2(t)\n\n+ (w(t))\u22121e(t).\n\n(2.41)\n\n\u03b2(t+1)\n\n+(ztw(t)z)\u22121 zte(t)\n= \u03b2(t)\n=(ztw(t)z)\u22121,ztw(t)z\u03b2(t)\n=(ztw(t)z)\u22121 ztw(t)x(t).\n\n+ ztw(t)(w(t))\u22121e(t)-\n\n(2.42)\nwe call x(t) the working response because it is apparent from (2.42) that \u03b2(t+1) are the\nregression coefficients resulting from the weighted least squares regression of x(t) on\nz with weights corresponding to the diagonal elements of w(t). at each iteration, a\nnew working response and weight vector are calculated, and the update can be fitted\nvia weighted least squares.\nsecond, irls for generalized linear models is a special case of the gauss\u2013\nnewton method for nonlinear least squares problems, which is introduced briefly\nbelow. irls therefore shows the same behavior as gauss\u2013newton; in particular, it\ncan be a slow and unreliable approach to fitting generalized linear models unless the\nmodel fits the data rather well [630].\n\n "}, {"Page_number": 54, "text": "2.2.2 newton-like methods\nsome very effective methods rely on updating equations of the form\n\n2.2 multivariate problems\n\n39\n\nx(t+1)\n\n= x(t)\n\n\u2212 (m(t))\u22121g\u2032(x(t))\n\n(2.43)\nwherem(t) isa p \u00d7 pmatrixapproximatingthehessian,g\u2032\u2032(x(t)).ingeneraloptimiza-\ntion problems, there are several good reasons to consider replacing the hessian by\nsome simpler approximation. first, it may be computationally expensive to evaluate\nthe hessian. second, the steps taken by newton\u2019s method are not necessarily always\nuphill: at each iteration, there is no guarantee that g(x(t+1)) > g(x(t)). a suitable\nm(t) can guarantee ascent. we already know that one possible hessian replacement,\nm(t) = \u2212i(\u03b8(t)), yields the fisher scoring approach. certain other (possibly scaled)\nchoices for m(t) can also yield good performance while limiting computing effort.\n2.2.2.1 ascent algorithms to force uphill steps, one could resort to an ascent\nalgorithm. (another type of ascent algorithm is discussed in chapter 3.) in the present\ncontext, the method of steepest ascent is obtained with the hessian replacement\nm(t) = \u2212i, where i is the identity matrix. since the gradient of g indicates the steep-\nest direction uphill on the surface of g at the point x(t), setting x(t+1) = x(t) + g\u2032(x(t))\namounts to taking a step in the direction of steepest ascent. scaled steps of the form\nx(t+1) = x(t) + \u03b1(t)g\u2032(x(t)) for some \u03b1(t) > 0 can be helpful for controlling conver-\ngence, as will be discussed below.\n\nmany forms of m(t) will yield ascent algorithms with increments\n\n(2.44)\n\nh(t)\n\n= \u2212\u03b1(t)0m(t)1\u22121g\u2032(x(t)).\n+ h(t)) \u2212 g(x(t))\n\nfor any fixed x(t) and negative definite m(t), note that as \u03b1(t) \u2192 0 we have\n\ng(x(t+1)) \u2212 g(x(t)) = g(x(t)\n\n= \u2212\u03b1(t)g\u2032(x(t))t(m(t))\u22121g\u2032(x(t)) + o(\u03b1(t)),\n\n(2.45)\nwhere the second equality follows from the linear taylor expansion g(x(t) + h(t)) =\ng(x(t)) + g\u2032(x(t))th(t) + o(\u03b1(t)). therefore, if \u2212m(t) is positive definite, ascent can\nbe assured by choosing \u03b1(t) sufficiently small, yielding g(x(t+1)) \u2212 g(x(t)) > 0 from\n(2.45) since o(\u03b1(t))/\u03b1(t) \u2192 0 as \u03b1(t) \u2192 0.\ntypically, therefore, an ascent algorithm involves a positive definite matrix\n\u2212m(t) toapproximatethenegativehessian,anda contractionorsteplengthparameter\n\u03b1(t) > 0 whose value can shrink to ensure ascent at each step. for example, start each\nstep with \u03b1(t) = 1. if the original step turns out to be downhill, \u03b1(t) can be halved.\nthis is called backtracking. if the step is still downhill, \u03b1(t) is halved again until\na sufficiently small step is found to be uphill. for fisher scoring, \u2212m(t) = i(\u03b8(t)),\nwhich is positive semidefinite. therefore backtracking with fisher scoring would\navoid stepping downhill.\nexample2.6 (bivariateoptimizationproblem,continued) figure2.8illustrates\nan application of the steepest ascent algorithm to maximize the bivariate function\n\n "}, {"Page_number": 55, "text": "40\n\nchapter 2 optimization and solving nonlinear equations\n\nx(0)\n\nx*\n\nfigure 2.8 applications of two optimization methods for maximizing a complex bivariate\nfunction. the surface of the function is indicated by shading and contours, with light shading\ncorresponding to high values. the two methods start at a point x(0) and find the true maximum,\nx\u2217. the solid line corresponds to the method of steepest ascent (example 2.6). the dashed\nline corresponds to a quasi-newton method with the bfgs update (example 2.7). both algo-\nrithms employed backtracking, with the initial value of \u03b1(t) at each step set to 0.25 and 0.05,\nrespectively.\n\n1\ndiscussed in example 2.4, starting from x(0) and initialized with \u03b1(t) =\n4 at each\nstep. the steps taken by steepest ascent are shown by the solid line. although the\noptimization was successful, it was not fast or efficient. the dashed line illustrates\nanother method, discussed in section 2.2.2.3.\n!\nstep halving is only one approach to backtracking. in general methods that rely\non finding an advantageous step length in the chosen direction are called line search\nmethods. backtracking with a positive definite replacement for the negative hessian\nis not sufficient to ensure convergence of the algorithm, however, even when g is\nbounded above with a unique maximum. it is also necessary to ensure that steps make\na sufficient ascent (i.e., require that g(x(t)) \u2212 g(x(t\u22121)) does not decrease too quickly\nas t increases) and that step directions are not nearly orthogonal to the gradient (i.e.,\navoid following a level contour of g). formal versions of such requirements include\nthe goldstein\u2013armijo and wolfe\u2013powell conditions, under which convergence of\nascent algorithms is guaranteed [14, 270, 514, 669].\nwhen the step direction is not uphill, approaches known as modified newton\nmethods alter the direction sufficiently to find an uphill direction [247]. a quite effec-\ntive variant is the modified cholesky decomposition approach [246]. in essence, when\nthe negative hessian is not positive definite, this strategy replaces it with \u2212\u02dcg\u2032\u2032(x(t)) =\n\u2212g\u2032\u2032(x(t)) + e, where e is a diagonal matrix with nonnegative elements. by crafting e\ncarefully to ensure that \u2212\u02dcg\u2032\u2032(x(t)) is positive definite without deviating unnecessarily\nfrom the original direction \u2212g\u2032\u2032(x(t)), a suitable uphill direction can be derived.\n\n "}, {"Page_number": 56, "text": "2.2 multivariate problems\n\n41\n2.2.2.2 discrete newton and fixed-point methods to avoid calculating\nthe hessian, one could resort to a secant-like method, yielding a discrete newton\nmethod, or rely solely on an initial approximation, yielding a multivariate fixed-point\nmethod.\nmultivariate fixed-point methods use an initial approximation of g\u2032\u2032 throughout\nthe iterative updating. if this approximation is a matrix of constants, so m(t) = m for\nall t, then the updating equation is\nx(t+1)\n\n(2.46)\na reasonable choice for m is g\u2032\u2032(x(0)). notice that if m is diagonal, then this amounts\nto applying the univariate scaled fixed-point algorithm separately to each component\nof g. see section 2.1.4 for more on the relationship between fixed-point iteration and\nnewton\u2019s method when maximizing log likelihoods that are locally quadratic.\nmultivariate discrete newton methods approximate the matrix g\u2032\u2032(x(t)) with a\nmatrix m(t) of finite-difference quotients. let g\u2032i(x) = dg(x)/dxi be the ith element\nof g\u2032(x). let ej denote the p-vector with a 1 in the jth position and zeros elsewhere.\namong the ways one might approximate the (i, j)th element of the hessian using\ndiscrete differences, perhaps the most straightforward is to set the (i, j)th element of\nm(t) to equal\n\n\u2212 m\u22121g\u2032(x(t)).\n\n= x(t)\n\nh\n\n(t)\n\n(t)\nij\n\nm(t)\n\nij =\n\n(2.47)\n\nfor all i, where x\n\ng\u2032i&x(t) + h\n(t)\nij . it is easiest to use h\n\nij ej\u2019 \u2212 g\u2032i&x(t)\u2019\n(t)\nij = h for all (i, j) and t, but this leads\nfor some constants h\nto a convergence order of \u03b2 = 1. alternatively, we can generally obtain an order of\n(t)\n(t)\nconvergence similar to that of the univariate secant method if we set h\nj \u2212\nij = x\n(t)\n(t\u22121)\nj denotes the jth element of x(t). it is important to average\nx\nj\nm(t) with its transpose to ensure symmetry before proceeding with the update of x(t)\ngiven in (2.43).\n2.2.2.3 quasi-newton methods the discrete newton method strategy for\nnumerically approximating the hessian by m(t) is a computationally burdensome\none. at each step, m(t) is wholly updated by calculating a new discrete difference for\neach element. a more efficient approach can be designed, based on the direction of\nthe most recent step. when x(t) is updated to x(t+1) = x(t) + h(t), the opportunity is\npresented to learn about the curvature of g in the direction of h(t) near x(t). then m(t)\ncan be efficiently updated to incorporate this information.\nto do this, we must abandon the componentwise discrete-difference approxi-\nmation to g\u2032\u2032 used in the discrete newton method. however, it is possible to retain a\ntype of secant condition based on differences. specifically, a secant condition holds\nfor m(t+1) if\n\ng\u2032(x(t+1)) \u2212 g\u2032(x(t)) = m(t+1)(x(t+1)\n\n\u2212 x(t)).\n\n(2.48)\n\n "}, {"Page_number": 57, "text": "chapter 2 optimization and solving nonlinear equations\n\n42\nthis condition suggests that we need a method to generate m(t+1) from m(t) in a\nmanner that requires few calculations and satisfies (2.48). this will enable us to gain\ninformation about the curvature of g in the direction of the most recent step. the\nresult is a quasi-newton method, sometimes called a variable metric approach [153,\n247, 486].\nthere is a unique symmetric rank-one method that meets these requirements\n[134]. let z(t) = x(t+1) \u2212 x(t) and y(t) = g\u2032(x(t+1)) \u2212 g\u2032(x(t)). then we can write the\nupdate to m(t) as\n(2.49)\n\nm(t+1)\n\n= m(t)\n\n+ c(t)v(t)(v(t))t,\n\nwhere v(t) = y(t) \u2212 m(t)z(t) and c(t) = 1/[(v(t))tz(t)].\nit is important to monitor the behavior of this update to m(t). if c(t) cannot\nreliably be calculated because the denominator is zero or close to zero, a temporary\nsolution is to take m(t+1) = m(t) for that iteration. we may also wish to backtrack to\nensure ascent. if \u2212m(t) is positive definite and c(t) \u2264 0, then \u2212m(t+1) will be positive\ndefinite. we use the term hereditary positive definiteness to refer to the desirable\nsituation when positive definiteness is guaranteed to be transferred from one iteration\nto the next. if c(t) > 0, then it may be necessary to backtrack by shrinking c(t) toward\nzerountilpositivedefinitenessisachieved.thus,positivedefinitenessisnothereditary\nwith this update. monitoring and backtracking techniques and method performance\nare further explored in [375, 409].\nthere are several symmetric rank-two methods for updating a hessian approx-\nimation while retaining the secant condition. the broyden class [78, 80] of rank-two\nupdates to the hessian approximation has the form\n\nm(t+1)\n\nwhere\n\n\u2212\n\nm(t)z(t)(m(t)z(t))t\n(z(t))tm(t)z(t) +\n\n= m(t)\n+ \u03b4(t)((z(t))tm(t)z(t))d(t)(d(t))t,\n\ny(t)(y(t))t\n(z(t))ty(t)\n\n(2.50)\n\nd(t)\n\ny(t)\n\nm(t)z(t)\n\n=\n\n(z(t))ty(t) \u2212\n\n(z(t))tm(t)z(t) .\n\nthe most popular member of this class is the bfgs update [79, 197, 269, 588], which\nsimply sets \u03b4(t) = 0. an alternative, for which \u03b4(t) = 1, has also been extensively\nstudied [134, 199]. however, the bfgs update is generally accepted as superior to\nthis, based on extensive empirical and theoretical studies. the rank-one update in\n(2.49) has also been shown to perform well and to be an attractive alternative to\nbfgs [120, 375].\nthe bfgs update\u2014indeed, all members of the broyden class\u2014confer heredi-\ntary positive definiteness on \u2212m(t). therefore, backtracking can ensure ascent. how-\never, recall that guaranteed ascent is not equivalent to guaranteed convergence. the\norder of convergence of quasi-newton methods is usually faster than linear but slower\nthan quadratic. the loss of quadratic convergence (compared to a newton\u2019s method)\nis attributable to the replacement of the hessian by an approximation. nevertheless,\nquasi-newton methods are fast and powerful, and are among the most frequently used\n\n "}, {"Page_number": 58, "text": "2.2 multivariate problems\n\n43\nmethods in popular software packages. several authors suggest that the performance\nof (2.49) is superior to that of bfgs [120, 409].\nexample2.7 (bivariateoptimizationproblem,continued) figure2.8illustrates\nan application of quasi-newton optimization with the bfgs update and backtracking\nfor maximizing the bivariate function introduced in example 2.4, starting from x(0)\nand initialized with \u03b1(t) = 0.05 at each step. the steps taken in this example are\nshown by the dashed line. the optimization successfully (and quickly) found x\u2217.\nrecall that the solid line in this figure illustrates the steepest ascent method discussed\nin section 2.2.2.1. both quasi-newton methods and steepest ascent require only first\nderivatives, and backtracking was used for both. the additional computation required\nbyquasi-newtonapproachesisalmostalwaysoutweighedbyitssuperiorconvergence\nperformance, as was seen in this example.\n!\nthere has been a wide variety of research on methods to enhance the perfor-\nmance and stability of quasi-newton methods. perhaps the most important of these\nimprovements involves the calculation of the update for m(t). although (2.50) pro-\nvides a relatively straightforward update equation, its direct application is frequently\nless numerically stable than alternatives. it is far better to update a cholesky decom-\nposition of m(t) as described in [245].\nthe performance of quasi-newton methods can be extremely sensitive to the\nchoice of the starting matrix m(0). the easiest choice is the negative identity matrix,\nbut this is often inadequate when the scales of the components of x(t) differ greatly.\nin mle problems, setting m(0) = \u2212i(\u03b8(0)) is a much better choice, if calculation of\nthe expected fisher information is possible. in any case, it is important to rescale\nany quasi-newton optimization problem so that the elements of x are on comparable\nscales. this should improve performance and prevent the stopping criterion from\neffectively depending only on those variables whose units are largest. frequently, in\npoorly scaled problems, one may find that a quasi-newton algorithm will appear to\n(t)\nconverge to a point for which some x\ni differ from the corresponding elements of the\nstarting point but others remain unchanged.\nin the context of mle and statistical inference, the hessian is critical because\nit provides estimates of standard error and covariance. yet, quasi-newton methods\nrely on the notion that the root-finding problem can be solved efficiently even us-\ning poor approximations to the hessian. further, if stopped at iteration t, the most\nrecent hessian approximation m(t\u22121) is out of date and mislocated at \u03b8(t\u22121) instead\nof at \u03b8(t). for all these reasons, the approximation may be quite bad. it is worth\nthe extra effort, therefore, to compute a more precise approximation after iterations\nhave stopped. details are given in [153]. one approach is to rely on the central dif-\nference approximation, whose (i, j)th element is\n\nl\u2032i(\u03b8(t) + hijej) \u2212 l\u2032i(\u03b8(t) \u2212 hijej)\n\n2hij\n\n,\n\n(2.51)\n\n!l\u2032\u2032&\u03b8(t)\u2019 =\n\nwhere l\u2032i(\u03b8(t)) is the ith component of the score function evaluated at \u03b8(t). in this case,\ndecreasing hij is associated with reduced discretization error but potentially increased\n\n "}, {"Page_number": 59, "text": "chapter 2 optimization and solving nonlinear equations\n\n44\ncomputer roundoff error. one rule of thumb in this case is to take hij = h = \u03b51/3 for\nall i and j, where \u03b5 represents the computer\u2019s floating-point precision [535].\n\n2.2.3 gauss\u2013newton method\nfor mle problems, we have seen how newton\u2019s method approximates the log like-\nlihood function at \u03b8(t) by a quadratic, and then maximizes this quadratic to obtain\nthe update \u03b8(t+1). an alternative approach can be taken in nonlinear least squares\nproblems with observed data (yi,zi) for i = 1, . . . , n, where one seeks to estimate \u03b8\nby maximizing an objective function g(\u03b8) = \u2212.n\ni=1 (yi \u2212 f(zi, \u03b8))2. such objective\nfunctions might be sensibly used, for example, when estimating \u03b8 to fit the model\nyi = f(zi, \u03b8) + \u03f5i\n(2.52)\nfor some nonlinear function f and random error \u03f5i.\nrather than approximate g, the gauss\u2013newton approach approximates f itself\nbyitslineartaylorseriesexpansionabout \u03b8(t).replacing f byitslinearapproximation\nyields a linear least squares problem, which can be solved to derive an update \u03b8(t+1).\n\nspecifically, the nonlinear model in (2.52) can be approximated by\nyi \u2248 f(zi, \u03b8(t)) + (\u03b8 \u2212 \u03b8(t))tf\u2032(zi, \u03b8(t)) + \u03f5i = \u02dcf(zi, \u03b8(t), \u03b8) + \u03f5i,\n\n(2.53)\nwhere for each i, f\u2032(zi, \u03b8(t)) is the column vector of partial derivatives of f(zi, \u03b8(t))\n(t)\nj , for j = 1, . . . , p, evaluated at (zi, \u03b8(t)). a gauss\u2013newton step is\nwith respect to \u03b8\ni=10yi \u2212 \u02dcf(zi, \u03b8(t), \u03b8)12 with respect\nderived from the maximization of \u02dcg(\u03b8) = \u2212.n\nto \u03b8, whereas a newton step is derived from the maximization of a quadratic approx-\nimation to g itself, namely g(\u03b8(t)) + (\u03b8 \u2212 \u03b8(t))tg\u2032(\u03b8(t)) + (\u03b8 \u2212 \u03b8(t))tg\u2032\u2032(\u03b8(t))(\u03b8 \u2212 \u03b8(t)).\n(t)\ni = yi \u2212\nf(zi, \u03b8(t)), and define a(t)\ni = f\u2032(zi, \u03b8(t)). then the approximated problem can be re-\nexpressed as minimizing the squared residuals of the linear regression model\n(2.54)\nand \u03f5i,\n\nwhere x(t) and \u03f5 are column vectors whose ith elements consist of x\nrespectively. similarly, a(t) is a matrix whose ith row is (a(t)\n\ndenote a working response whose observed value is x\n\n= a(t)(\u03b8 \u2212 \u03b8(t)) + \u03f5,\n\nlet x\n\nx(t)\n\n(t)\ni\n\n(t)\ni\n\nthe minimal squared error for fitting (2.54) is achieved when\n\ni )t.\n(\u03b8 \u2212 \u03b8(t)) =((a(t))ta(t))\u22121 (a(t))tx(t).\n+((a(t))ta(t))\u22121 (a(t))tx(t).\n\n= \u03b8(t)\n\nthus, the gauss\u2013newton update for \u03b8(t) is\n\n\u03b8(t+1)\n\n(2.56)\ncompared to newton\u2019s method, the potential advantage of the gauss\u2013newton\nmethod is that it does not require computation of the hessian. it is fast when f is\nnearly linear or when the model fits well. in other situations, particularly when the\n\n(2.55)\n\n "}, {"Page_number": 60, "text": "2.2 multivariate problems\n\n45\n\nworst\n\nbad\n\nc\n\nbest\n\nfigure 2.9 simplex superimposed over contours of the objective function g for p = 2. the\nbest vertex is near the optimum of g. the best face is the triangle side containing c, which is\nits centroid.\n\nresiduals at the true solution are large because the model fits poorly, the method may\nconverge very slowly or not at all\u2014even from good starting values. a variant of the\ngauss\u2013newton method has better convergence behavior in such situations [152].\n\n2.2.4 nelder\u2013mead algorithm\nthe algorithms described thus far all rely on derivatives or approximations thereof. in\nmany cases, derivation of g\u2032 and g\u2032\u2032 is undesirable or infeasible. the nelder\u2013mead al-\ngorithmisoneofaclassofoptimizationmethodsthatrequirenoderivativeinformation\n[482,650].itisaniterativedirectsearchapproachbecauseitdependsonlyontheranks\nof a collection of function evaluations at possible solutions while it tries to nominate\na superior point for the next iteration [369, 385, 515, 675]. chapter 3 describes some\nother direct search methods including genetic algorithms and simulated annealing.\nthe tthiterationofthenelder\u2013meadalgorithmbeginswithacollectionofpoints\nrepresenting possible solutions, that is, approximations to the maximum. these points\ndefine a neighborhood\u2014specifically a simplex\u2014near which search effort is currently\nfocused. an iteration of the algorithm seeks to reshape and resize the neighborhood\nthrough the nomination of a new point to replace the worst point in the collection. ide-\nally, the candidate may be much better than some other points, or even the best so far.\nwhen x is p-dimensional, p + 1 distinct points x1, . . . ,xp+1 define a\np-dimensional simplex, namely the convex hull of vertices x1, . . . ,xp+1. a sim-\nplex is a triangle or a tetrahedron (i.e., a pyramid with triangular base) when p = 2\nand p = 3, respectively. the vertices of the simplex can be ranked from best to worst\naccording to the ranks of g(x1), . . . , g(xp+1); see figure 2.9. when seeking a maxi-\nmum, let xbest correspond to the vertex with the highest objective function value and\nxworst to the lowest. denote the second-worst vertex as xbad. ties can be addressed\nas by [397, 482]. we can also define the best face to be the face opposite xworst. the\nbest face is therefore the hyperplane containing the other points, and its centroid is\n\nthe mean of all the other vertices, namely c = (1/p)0&.p+1\n\ni=1 xi\u2019 \u2212 xworst1.\n\n "}, {"Page_number": 61, "text": "46\n\nchapter 2 optimization and solving nonlinear equations\n\nw\n\nc\n\nreflection\n\nc\n\nw\n\nr\n\nw\n\nr\n\nw\n\nc\n\nr\n\nb\n\nc\n\nc\n\nexpansion\n\nr\n\nw\n\nr\n\nouter contraction\n\ninner contraction\n\nshrinkage\n\nfigure 2.10 the five possible transformations of a simplex. the unshaded triangle is the\ncurrent simplex, the hashed triangle shows the simplex that would be obtained by reflection,\nand the gray triangle represents the simplex adopted at the completion of the respective trans-\nformations. points labeled w, b and r are the worst, best, and reflection vertices, respectively,\nand c is the centroid of the best face.\n\nhaving determined the best, second-worst, and worst vertices, we try to replace\nthe worst vertex with a better one. the algorithm requires that the new point will lie\nupon the ray extending from xworst through c, which we call the search direction.\ntherefore, in this sense, the new vertex location will be moved in the direction of\nbetter alternatives away from the worst vertex. further, selecting a replacement for\nxworst will change the shape and size of the simplex. although this search direction\nmay be promising, the quality of the new vertex will also depend on the distance of\nthe new vertex from xworst. this distance affects simplex size. indeed, the nelder\u2013\nmead algorithm is sometimes referred to as the amoeba method, reflecting the flexible\ntransformation and movement of the simplex as its size and shape change to adapt to\nthe local hills and valleys of the objective function [517].\nthe location of the chosen new vertex is based upon the reflection vertex\nxr defined as xr = c + \u03b1r(c \u2212 xworst) as shown in figure 2.10. reflections require\n\u03b1r > 0, and usually \u03b1r = 1. although xr itself may not be the new vertex, it, c, and\nxworst are used to determine the new point. the following paragraphs describe the as-\nsortment of ways by which the new vertex can be derived, and figure 2.10 illustrates\nthese methods and the resulting simplex transformations.\nconsider first when g(xr) exceeds g(xbad). if g(xr) does not also exceed the\nobjective function values for xbest, then xr is accepted as a new vertex and xworst is\ndiscarded. the updated collection of vertices defines a new simplex (figure 2.10),\nand a new iteration of the algorithm begins. however, if g(xr) > g(xbest) so that the\nreflection vertex is better than the current best, then even greater improvement is\nsought by extending search further in the direction pursued by xr. this leads to an\nattempt at expansion. if xr is worse than xbad, then we try to mitigate this unfortunate\noutcome using a contraction of xr.\nan expansion occurs when g(xr) exceeds g(xbest). an expansion point xe is then\ndefinedasxe = c + \u03b1e(xr \u2212 c),where \u03b1e > max(1, \u03b1r)andusually \u03b1e = 2.thusxe is\n\n "}, {"Page_number": 62, "text": "2.2 multivariate problems\n\n47\na point along the search direction vector beyond xr. since exploration in the direction\nleading to xr yielded such a good point, the hope is that even more improvement\nmight be achieved by going further in that direction. if g(xe) exceeds g(xr), then\nexpansion was successful so xe is accepted as a new vertex, xworst is discarded, and\na new iteration is begun. if g(xe) fails to surpass g(xr), then xr is retained as the\nimproved vertex, xworst is discarded, and a new iteration is begun.\nthus far we have describe a process which leads to acceptance of a new vertex\n(xr or xe) whenever the reflected point is better than xbad. when g(xr) is no greater\nthan g(xbad), then additional search is needed because xr would be the worst vertex\neven though it replaced xworst. the contraction strategy is to identify a final vertex\nsomewhere along the search direction between xworst and xr. when this vertex is\nbetween c and xr, the transformation is called an outer contraction, otherwise it is an\ninner contraction.\nanoutercontractionisconductedwhen g(xbad) \u2265 g(xr) > g(xworst).thevertex\nobtained by an outer contraction is defined as xo = c + \u03b1c(xr \u2212 c) where 0 < \u03b1c < 1\n1\n2. if g(xo) \u2265 g(xr) so that the outer contraction vertex is at least\nand normally \u03b1c =\nas good as the reflection vertex, then xo is chosen to replace xworst. otherwise, we\nare in a situation where xr would be the worst vertex after it replaced xworst. in this\ncase, instead of performing that pointless replacement, a shrink transformation is\nperformed as described later.\nan inner contraction is attempted when g(xr) \u2264 g(xworst), that is, when xr is\nworse than all vertices of the current simplex. in this case an inner contraction point\nis defined as xi = c + \u03b1c(xworst \u2212 c). then if g(xi) > g(xworst), then xi is chosen to\nreplace xworst. otherwise, no reasonable replacement for xworst has been identified.\nagain a shrink transformation is warranted.\nwhen all else fails, the simplex is subjected to a shrink transformation. in this\ncase, all vertices except the best are shrunk toward xbest by transforming the jth\nvertex xj to xsj according to xsj = xbest + \u03b1s(xj \u2212 xbest) where j indexes vertices\nfor j = 1, . . . , p + 1. shrinking has no effect on xbest so this calculation is omitted.\nshrinking after a failed contraction will focus the simplex near the vertex with the\ngreatestobjectivefunctionvalue.inpractice,shrinkinghappensveryrarely.shrinkage\nrequires 0 < \u03b1s < 1 and normally \u03b1s =\nin summary, the nelder\u2013mead algorithm follows the following steps.\n1. initialize. for t = 1 choose starting vertices x(t)\np+1. choose \u03b1r > 0,\n\u03b1e > max{1, \u03b1r}, 0 < \u03b1c < 1, and 0 < \u03b1s < 1. standard values for (\u03b1r, \u03b1e,\n\u03b1c, \u03b1s) are (1,2, 1\n2. sort. among the current set of vertices, identify the ones that yield the highest,\nsecond lowest, and lowest objective function evaluations, namely x(t)\nbest, x(t)\nbad,\nand x(t)\n\n2) [397, 482].\n\n1 , . . . ,x(t)\n\nworst, respectively.\n\n2 , 1\n\n1\n2.\n\n3. orient. compute\n\nc(t)\n\n=\n\n1\n\np\u23a1\u23a3& p+14i=1\n\nx(t)\n\nworst\u23a4\u23a6 .\ni \u2019 \u2212 x(t)\n\n "}, {"Page_number": 63, "text": "48\n\nchapter 2 optimization and solving nonlinear equations\n\n4. reflect. compute x(t)\n\nr = c(t) + \u03b1r(c(t) \u2212 x(t)\nr ) > g(x(t)\n\nand g(x(t)\nbad).\na. if g(x(t)\nbest) \u2265 g(x(t)\nt + 1 and discard x(t)\nr ) > g(x(t)\nb. if g(x(t)\nc. otherwise skip ahead to the contraction step.\n\nbad), then accept x(t)\nworst. go to the stopping step.\n\nbest), then continue to the expansion step.\n\nworst). compare g(x(t)\n\nr ) to g(x(t)\nbest)\n\nr as a new vertex for iteration\n\ne ) to g(x(t)\n\n5. expansion. compute x(t)\n\na. if g(x(t)\n\ndiscard x(t)\n\ne ) > g(x(t)\n\nr \u2212 c(t)). compare g(x(t)\n\nb. otherwise, accept x(t)\ngo to the stopping step.\n\ne = c(t) + \u03b1e(x(t)\nworst. go to the stopping step.\n\nbest).\nr ), then accept xe as a new vertex for iteration t + 1 and\nr as a new vertex for iteration t + 1 and discard x(t)\nworst.\nr ) to g(x(t)\nbad) \u2265 g(x(t)\nr ) >\nworst), then perform an outer contraction. otherwise [i.e., when g(x(t)\ng(x(t)\nworst) \u2265\ng(x(t)\nr )] perform an inner contraction.\na. outer contraction. compute x(t)\n\nbad) and g(x(t)\n\nworst). if g(x(t)\n\n6. contraction. compare g(x(t)\n\no = c(t) + \u03b1c(x(t)\n\nr \u2212 c(t)).\n\ni. if g(x(t)\n\no ) \u2265 g(x(t)\n\ndiscard x(t)\n\nr ), then accept x(t)\n\nworst. go to the stopping step.\n\nii. otherwise, go to the shrinking step.\n\no as a new vertex for iteration t + 1 and\n\nb. inner contraction. compute x(t)\n\ni = c(t) + \u03b1c(x(t)\n\nworst \u2212 c(t)).\n\ni. if g(x(t)\n\ni ) > g(x(t)\nand discard x(t)\n\nworst), then accept x(t)\nworst. go to the stopping step.\n\nii. otherwise, go to the shrinking step.\n\ni as a new vertex for iteration t + 1\n\nj \u2212 x(t)\n\nj /= x(t)\n\nbest). collect x(t)\n\nbest compute x(t)\n\n7. shrinking. for all j = 1, . . . , p + 1 for which x(t)\nsj =\nx(t)\nbest + \u03b1s(x(t)\nbest and these p new vertices to form the sim-\nplex for iteration t + 1. go to the stopping step.\n8. stopping. check convergence criteria. if stopping is not warranted, increment\nt to t + 1 and begin a new iteration by returning to the sort step. otherwise x(t)\nbest\nis taken to be the approximate maximizer of g.\nan easy choice for initialization is to form a starting simplex around an initial\nguess for the optimum, say x0, by using x0 as one vertex and choosing the remaining\nvertices along coordinate axes to make the simplex right-angled at x0.\nsome variations to this standard algorithm have been explored. small changes\nto the decision rules have been considered by [85, 448, 502, 518]. more radical\nalternativesofsuchfurtherconstraintsonthenominationoracceptanceofnewvertices\nare explored by [85, 479, 518, 565, 636, 650]. alternative choices for (\u03b1r, \u03b1e, \u03b1c, \u03b1s)\nare mentioned by [24, 85, 502].\n\n "}, {"Page_number": 64, "text": "2.2 multivariate problems\n\n49\n\n2\n\n1\n\n4\n3\n\nfigure 2.11 initial steps of the nelder\u2013mead algorithm for maximizing a complicated\nbivariate function. the surface of the function is indicated by shading and contours, with light\nshading corresponding to high values. in the left panel, the algorithm is initiated with simplex\n1. an expansion step yields simplex 2. continuing in the right panel, the next two steps are\ninner contractions, yielding simplices 3 and 4.\n\nexample 2.8 (bivariate optimization problem, continued) let us examine an\napplicationofthestandardnelder\u2013meadalgorithmtomaximizethebivariatefunction\ndiscussedinexample2.4.figures2.11and2.12showtheresults.thestartingsimplex\nwasdefinedbyxworst = (3.5,\u22120.5),xbad = (3.25,\u22121.4)andxbest = (3,\u22120.5).after\ninitialization, the algorithm takes a reflection step, finding a new xbest with much\ngreater objection function value. however, this yields a poor search direction because\nreflection and outer contraction would both produce points far down the opposite\nside of the ridge just ascended. instead, two inner contraction steps occur, at which\npoint the best face has changed. the new search direction is now good, and the next\ntwo steps are an expansion and a reflection. the right panel of figure 2.12 shows\nthe remaining progress of the algorithm. although convergence is slow, the correct\nmaximum is eventually found.\n!\ntwo types of convergence criteria are needed for the nelder\u2013mead algorithm.\nfirst, some measure of the (relative) change in vertex locations should be exam-\nined. it is important to note, however, that a lack of change in xbest alone will not\nprovide complete information about search progress because xbest may remain un-\nchanged for several successive iterations. further, it can be more effective to monitor\nthe convergence of, for example, simplex volume rather than any particular point.\nsuch a criterion corresponds to the relative convergence criterion we have considered\npreviously in this chapter and, notably, is important when optimizing discontinuous\nfunctions. second, one should determine whether the values of the objective function\nappear to have converged. performance of variants of the nelder\u2013mead algorithm can\nbe improved using modified stopping rules [600, 636].\nthe nelder\u2013mead method is generally quite good at finding optima, especially\nfor low to moderate dimensions [397, 675]. for high-dimensional problems, its ef-\nfectiveness is more varied, depending on the nature of the problem [85, 493, 600].\ntheoretical analysis of algorithm convergence has been limited to restricted classes\n\n "}, {"Page_number": 65, "text": "50\n\nchapter 2 optimization and solving nonlinear equations\n\n6\n\n5\n\nfigure 2.12 further steps of the nelder\u2013mead algorithm for maximizing a complicated\nbivariate function. the surface of the function is indicated by shading and contours, with light\nshading corresponding to high values. continuing on from figure 2.11, the left panel shows\nthat the next two nelder\u2013mead steps are an expansion (simplex 5) and a reflection (simplex 6).\nin the right panel, further steps are shown as iterations hone in on the maximum.\nof functions or substantively modified versions of the standard algorithm [85, 368,\n397, 518, 636].\nthe nelder\u2013mead approach is quite robust in the sense that it can successfully\nfind optima for a wide range of functions\u2014even discontinuous ones\u2014and from a\nwide range of starting values [397, 493, 675]. moreover, it is robust in the sense\nthat convergence is often not much impeded when the objective function values are\ncontaminated with random noise [24, 154].\ndespite its good performance, the nelder\u2013mead algorithm can perform poorly\nin certain circumstances. surprisingly, it is even possible for the algorithm to converge\nto points that are neither local maxima nor minima. the following example illustrates\none such case.\nexample 2.9 (nelder\u2013mead failure) a failure of the nelder\u2013mead method is\nillustrated by figure 2.13 where the simplex collapses [448].\nthe following bivariate objective function using p = 2 and\nconsider\nx = (x1, x2):\ng(x1, x2) =$\u2212360|x1|2 \u2212 x2 \u2212 x2\n\n(2.57)\nlet the starting values be (0,0), (1,1), and roughly (0.84,\u22120.59). then for this\nsurprisingly simple function, iterations produce simplices whose best vertex never\nchanges despite it being far from any extremum and yet the simplex area converges\nto zero in the manner shown in figure 2.13. as this happens, the search direction\nbecomes orthogonal to g\u2032 so that the improvement at successive iterations converges\nto zero.\n!\nwhen the algorithm stagnates as in example 2.9, restarting with a different\nsimplex can often remedy the problem by setting the algorithm on a different and\n\nif x1 \u2264 0\notherwise.\n\n1 \u2212 x2 \u2212 x2\n2\n\n\u22126x2\n\n2\n\n "}, {"Page_number": 66, "text": "2.2 multivariate problems\n\n51\n\nx*\n\nfigure 2.13 contours of g and successive simplices for example 2.9. the solid dots indi-\ncate the c(t) locations, the hollow circle is xbest for every t, and x\u2217 is the global maximum of g.\n\npossibly more productive ascent path. alternatively, the oriented restart approach\nis specifically designed to reshape the simplex in a manner targeting steepest\nascent [368]. define a p \u00d7 p matrix of simplex directions as v(t) = (x(t)\n2 \u2212 x(t)\n1 ,\nx(t)\n3 \u2212 x(t)\n1 , . . . ,x(t)\n1 ) and a corresponding vector of objective function differ-\np+1) \u2212 g(x(t)\nences as \u03b4(t) = (g(x(t)\n1 ), g(x(t)\n1 ), . . . , g(x(t)\n1 )), where the\n1 = x(t)\np + 1 vertices are all ordered with respect to quality so that x(t)\nbest. then we may\n(t)) = (v(t))\u2212t\u03b4(t). this simplex\ndefine the simplex gradient of simplex s\ngradient is designed to approximate the true gradient of g at x(t)\n1 .\nan oriented restart is triggered when the average vertex improvement is too\nsmall. specifically, let\n\np+1 \u2212 x(t)\n2 ) \u2212 g(x(t)\n\n(t) to be d(s\n\n3 ) \u2212 g(x(t)\n\n\u00afg(t)\n\n=\n\n1\np + 1\n\np+14i=1\n\ng(x(t)\ni )\n\n(2.58)\n\nbe the average objective function value of the vertices of s\nsufficient increase in simplex quality from iteration t to t + 1 be one for which\n2\n\n(t) at iteration t. define a\n\n(2.59)\n\nwhere \u03f5 is chosen to be a small number (e.g., 0.0001).\nwith vertices situated on coordinate axes centered at x(t)\nspecifically, let x(t+1)\n\nin this case, the oriented restart consists of replacing all vertices except xbest\nbest and having reduced lengths.\n\n1\n\n= x(t)\n\n1 and\n\nx(t+1)\n\nj\n\n= x(t)\n\n1 + \u03b2jej\n\n(2.60)\n\n\u00afg(t+1)\n\n\u2212 \u00afg(t) > \u03f5777d(s\n\n(t))777\n\n "}, {"Page_number": 67, "text": "chapter 2 optimization and solving nonlinear equations\n\n52\nfor j = 2, . . . , p + 1 where ej are the p unit vectors along the coordinate axes and\n\u03b2j orients and scales the coordinate-wise steps according to\n(t)\nif sign{d(s\nj\u22121)} /= 0\notherwise.\n\n\u03b2j =$\u2212d(t) sign{d(s\n(t)\nj\u22121) for j = 2, . . . , p + 1 represents the corresponding component of\nin (2.61), d(s\n(t) and the scalar factor d(t) is the minimum oriented length\nthe gradient of s\n\n(t)\nj\u22121)}\n\n(2.61)\n\n0\n\nd(t)\n\n= min\n\n2\u2264j\u2264p+1777x(t)\n\nj 777 .\n1 \u2212 x(t)\n\n(2.62)\n\nthe rationale for an oriented restart is that the new simplex gradient at xbest should\npoint in a direction that approximates the true objective function gradient once the\nsimplex is small enough, provided that the simplex gradient is in the correct orthant.\nin a case like example 2.9, note further that the oriented restart strategy would halt\nthe simplex collapse.\nthe concept of sufficient descent can be generalized for use with a variety of\nnelder\u2013mead variants. instead of adopting a new vertex generated by the standard\nalgorithm, one may require the new vertex to meet an additional, more stringent\ncondition requiring some type of minimum improvement in the simplex [85, 479,\n518, 636].\ndespite rare failures and relatively slow convergence speed, the nelder\u2013mead\nalgorithm is a very good candidate for many optimization problems. another attrac-\ntive feature is that it can be implemented with great numerical efficiency, making it a\nfeasible choice for problems where objective function evaluations are computation-\nally expensive. in virtually every case, only two new objective function values are\ncalculated: xr and one of xe, xo, and xi. shrinking occurs only rarely and in this case\np evaluations of the objective function are required.\nfinally, for statistical applications like maximum likelihood estimation it is\nimportant to find a variance estimate for \u02c6\u03b8. for this purpose, numerical approximation\nof the hessian can be completed after convergence is achieved [482, 493].\n\n2.2.5 nonlinear gauss\u2013seidel iteration\nan important technique that is used frequently for fitting nonlinear statistical models,\nincluding those in chapter 12, is nonlinear gauss\u2013seidel iteration. this technique is\nalternatively referred to as backfitting or cyclic coordinate ascent.\nthe equation g\u2032(x) = 0 is a system of p nonlinear equations in p unknowns. for\nj = 1, . . . , p, gauss\u2013seidel iteration proceeds by viewing the jth component of g\u2032 as\na univariate real function of xj only. any convenient univariate optimization method\ncan be used to solve for the one-dimensional root of g\u2032j(x\n) = 0. all p components\nare cycled through in succession, and at each stage of the cycle the most recent values\nobtained for each coordinate are used. at the end of the cycle, the complete set of\nmost recent values constitutes x(t+1).\n\n(t+1)\n\nj\n\n "}, {"Page_number": 68, "text": "2.2 multivariate problems\n\n53\n\nx*\n\nx(0)\n\nfigure 2.14 application of gauss\u2013seidel iteration for maximizing a complex bivariate\nfunction, as discussed in example 2.10. the surface of the function is indicated by shading\nand contours. from the starting point of x(0), several steps are required to approach the true\nmaximum, x\u2217. each line segment represents a change of a single coordinate in the current\nsolution, so complete steps from x(t) to x(t+1) correspond to pairs of adjacent segments.\n\nthe beauty of this approach lies in the way it simplifies a potentially difficult\nproblem.thesolutionofunivariateroot-findingproblemscreatedbyapplyinggauss\u2013\nseidel iteration is generally easy to automate, since univariate algorithms tend to be\nmore stable and successful than multivariate ones. further, the univariate tasks are\nlikely to be completed so quickly that the total number of computations may be less\nthan would have been required for the multivariate approach. the elegance of this\nstrategy means that it is quite easy to program.\nexample 2.10 (bivariate optimization problem, continued)\nfigure 2.14 il-\nlustrates an application of gauss\u2013seidel iteration for finding the maximum of the\nbivariate function discussed in example 2.4. unlike other graphs in this chapter, each\nline segment represents a change of a single coordinate in the current solution. thus,\nfor example, the x(1) is at the vertex following one horizontal step and one vertical\nstep from x(0). each complete step comprises two univariate steps. a quasi-newton\nmethod was employed for each univariate optimization. note that the very first uni-\nvariate optimization (one horizontal step left from x(0)) actually failed, finding a\nlocal univariate minimum instead of the global univariate maximum. although this is\nnot advised, subsequent gauss\u2013seidel iterations were able to overcome this mistake,\neventually finding the global multivariate maximum.\n!\nthe optimization of continuous multivariate functions is an area of extensive\nresearch, and the references given elsewhere in this chapter include a variety of ap-\nproaches not mentioned here. for example, the trust region approach constrains di-\nrections and lengths of steps. the nonlinear conjugate gradient approach chooses\n\n "}, {"Page_number": 69, "text": "chapter 2 optimization and solving nonlinear equations\n\n54\nsearch directions that deviate from the direction of the gradient with a bias toward\ndirections not previously explored.\n\nproblems\n2.1. the following data are an i.i.d. sample from a cauchy(\u03b8,1) distribution: 1.77, \u22120.23,\n2.76, 3.80, 3.47, 56.75, \u22121.34, 4.24, \u22122.44, 3.29, 3.71, \u22122.40, 4.53, \u22120.07, \u22121.05,\n\u221213.87, \u22122.53, \u22121.75, 0.27, 43.21.\na. graph the log likelihood function. find the mle for \u03b8 using the newton\u2013raphson\nmethod. try all of the following starting points: \u221211, \u22121, 0, 1.5, 4, 4.7, 7, 8, and\n38. discuss your results. is the mean of the data a good starting point?\nb. apply the bisection method with starting points \u22121 and 1. use additional runs\nto illustrate manners in which the bisection method may fail to find the global\nmaximum.\nc. apply fixed-point iterations as in (2.29), starting from \u22121, with scaling choices\nof \u03b1 = 1, 0.64, and 0.25. investigate other choices of starting values and scaling\nfactors.\nd. from starting values of (\u03b8(0), \u03b8(1)) = (\u22122,\u22121), apply the secant method to estimate\n\u03b8. what happens when (\u03b8(0), \u03b8(1)) = (\u22123,3), and for other starting choices?\ne. use this example to compare the speed and stability of the newton\u2013raphson\nmethod, bisection, fixed-point iteration, and the secant method. do your conclu-\nsions change when you apply the methods to a random sample of size 20 from a\nn(\u03b8,1) distribution?\n\n2.2. consider the density f(x) = [1 \u2212 cos{x \u2212 \u03b8}]/2\u03c0 on 0 \u2264 x \u2264 2\u03c0, where \u03b8 is a param-\neter between \u2212\u03c0 and \u03c0. the following i.i.d. data arise from this density: 3.91, 4.85,\n2.28, 4.06, 3.70, 4.04, 5.46, 3.53, 2.28, 1.96, 2.53, 3.88, 2.22, 3.47, 4.82, 2.46, 2.99,\n2.54, 0.52, 2.50. we wish to estimate \u03b8.\na. graph the log likelihood function between \u2212\u03c0 and \u03c0.\nb. find the method-of-moments estimator of \u03b8.\nc. find the mle for \u03b8 using the newton\u2013raphson method, using the result from\n(b) as the starting value. what solutions do you find when you start at \u22122.7\nand 2.7?\nd. repeatpart(c)using200equallyspacedstartingvaluesbetween\u2212\u03c0 and \u03c0.partition\nthe interval between \u2212\u03c0 and \u03c0 into sets of attraction. in other words, divide the set\nof starting values into separate groups, with each group corresponding to a separate\nunique outcome of the optimization (a local mode). discuss your results.\n\ne. find two starting values, as nearly equal as you can, for which the newton\u2013raphson\n\nmethod converges to two different solutions.\n\n2.3. let the survival time t for individuals in a population have density function f and\ncumulative distribution function f. the survivor function is then s(t) = 1 \u2212 f(t). the\nhazard function is h(t) = f(t)/(1 \u2212 f(t)), which measures the instantaneous risk of\ndying at time t given survival to time t. a proportional hazards model posits that\n\n "}, {"Page_number": 70, "text": "2.2 multivariate problems\n\n55\n\ntable 2.2 length of remission (in weeks) for acute leukemia patients in the treat-\nment and control groups of a clinical trial, with parentheses indicating censored\nvalues. for censored cases, patients are known to be in remission at least as long\nas the indicated value.\n\ntreatment\n\ncontrol\n\n(6)\n10\n22\n\n1\n5\n11\n\n6\n(11)\n23\n\n1\n5\n12\n\n6\n13\n(25)\n\n2\n8\n12\n\n6\n16\n(32)\n\n2\n8\n15\n\n7\n(17)\n(32)\n\n3\n8\n17\n\n(9)\n(19)\n(34)\n\n4\n8\n22\n\n(10)\n(20)\n(35)\n\n4\n11\n23\n\nthe hazard function depends on both time and a vector of covariates, x, through the\nmodel\n\nh(t|x) = \u03bb(t)exp8xt\u03b29 ,\n\nwhere \u03b2 is a parameter vector.\n\n\u2212\u221e\n\nif \u0001(t) =: t\n\n\u03bb(u) du, it is easy to show that s(t) = exp8\u2212\u0001(t)exp{xt\u03b2}9\nand f(t) = \u03bb(t)exp8xt\u03b2 \u2212 \u0001(t)exp{xt\u03b2}9.\na. suppose that our data are censored survival times ti for i = 1, . . . , n. at the end\nof the study a patient is either dead (known survival time) or still alive (censored\ntime; known to survive at least to the end of the study). define wi to be 1 if ti is an\nuncensored time and 0 if ti is a censored time. prove that the log likelihood takes\nthe form\n\nn4i=1\n\nwi log; \u03bb(ti)\n\u0001(ti)< ,\n\n(wi log{\u00b5i} \u2212 \u00b5i) +\ni \u03b2}.\n\nn4i=1\nwhere \u00b5i = \u0001(ti)exp{xt\nb. consider a model for the length of remission for acute leukemia patients in a clin-\nical trial. patients were either treated with 6-mercaptopurine (6-mp) or a placebo\n[202]. one year after the start of the study, the length (weeks) of the remission\nperiod for each patient was recorded (see table 2.2). some outcomes were cen-\nsored because remission extended beyond the study period. the goal is to deter-\nmine whether the treatment lengthened time spent in remission. suppose we set\n\u0001(t) = t\u03b1 for \u03b1 > 0, yielding a hazard function proportional to \u03b1t\u03b1\u22121 and a weibull\ndensity: f(t) = \u03b1t\u03b1\u22121 exp8xt\u03b2 \u2212 t\u03b1 exp{xt\u03b2}9. adopt the covariate parameteri-\nzation given by xt\ni \u03b2 = \u03b20 + \u03b4i\u03b21 where \u03b4i is 1 if the ith patient was in the treatment\ngroup and 0 otherwise. code a newton\u2013raphson algorithm and find the mles of\n\u03b1, \u03b20, and \u03b21.\n\nc. use any prepackaged newton\u2013raphson or quasi-newton routine to solve for the\n\nsame mles.\n\nd. estimate standard errors for your mles. are any of your mles highly correlated?\n\nreport the pairwise correlations.\n\ne. use nonlinear gauss\u2013seidel iteration to find the mles. comment on the implemen-\ntation ease of this method compared to the multivariate newton\u2013raphson method.\n\n "}, {"Page_number": 71, "text": "56\n\nchapter 2 optimization and solving nonlinear equations\n\ntable 2.3 counts of flour beetles in all stages of development over 154 days.\n\ndays\nbeetles\n\n0\n2\n\n8\n47\n\n28\n192\n\n41\n256\n\n63\n768\n\n79\n896\n\n97\n1120\n\n117\n896\n\n135\n1184\n\n154\n1024\n\nf. use the discrete newton method to find the mles. comment on the stability of this\n\nmethod.\n\n2.4. a parameter \u03b8 has a gamma(2,1) posterior distribution. find the 95% highest posterior\ndensity interval for \u03b8, that is, the interval containing 95% of the posterior probability for\nwhich the posterior density for every point contained in the interval is never lower than\nthe density for every point outside the interval. since the gamma density is unimodal,\nthe interval is also the narrowest possible interval containing 95% of the posterior\nprobability.\n\n2.5. there were 46 crude oil spills of at least 1000 barrels from tankers in u.s. waters during\n1974\u20131999. the website for this book contains the following data: the number of spills\nin the ith year, ni; the estimated amount of oil shipped through us waters as part of\nus import/export operations in the ith year, adjusted for spillage in international or\nforeign waters, bi1; and the amount of oil shipped through u.s. waters during domestic\nshipments in the ith year, bi2. the data are adapted from [11]. oil shipment amounts\nare measured in billions of barrels (bbbl).\nthe volume of oil shipped is a measure of exposure to spill risk. suppose\nwe use the poisson process assumption given by ni|bi1, bi2 \u223c poisson(\u03bbi) where\n\u03bbi = \u03b11bi1 + \u03b12bi2.theparametersofthismodelare \u03b11 and \u03b12,whichrepresenttherate\nof spill occurrence per bbbl oil shipped during import/export and domestic shipments,\nrespectively.\na. derive the newton\u2013raphson update for finding the mles of \u03b11 and \u03b12.\nb. derive the fisher scoring update for finding the mles of \u03b11 and \u03b12.\nc. implement the newton\u2013raphson and fisher scoring methods for this problem,\nprovide the mles, and compare the implementation ease and performance of the\ntwo methods.\n\nd. estimate standard errors for the mles of \u03b11 and \u03b12.\ne. apply the method of steepest ascent. use step-halving backtracking as necessary.\nf. apply quasi-newton optimization with the hessian approximation update given in\n\n(2.49). compare performance with and without step halving.\n\ng. construct a graph resembling figure 2.8 that compares the paths taken by methods\nused in (a)\u2013(f). choose the plotting region and starting point to best illustrate the\nfeatures of the algorithms\u2019 performance.\n\n2.6. table 2.3 provides counts of a flour beetle (tribolium confusum) population at various\npoints in time [103]. beetles in all stages of development were counted, and the food\nsupply was carefully controlled.\nan elementary model for population growth is the logistic model given by\n\ndn\n\ndt = rn(1 \u2212\n\nn\n\nk) ,\n\n(2.63)\n\n "}, {"Page_number": 72, "text": "2.2 multivariate problems\n\n57\n\nwhere n is population size, t is time, r is a growth rate parameter, and k is a parameter\nthat represents the population carrying capacity of the environment. the solution to\nthis differential equation is given by\nnt = f(t) =\n\n(2.64)\n\nkn0\n\nn0 + (k \u2212 n0)exp{\u2212rt}\n\nwhere nt denotes the population size at time t.\na. fit the logistic growth model to the flour beetle data using the gauss\u2013newton\napproach to minimize the sum of squared errors between model predictions and\nobserved counts.\n\nb. fit the logistic growth model to the flour beetle data using the newton\u2013raphson\napproach to minimize the sum of squared errors between model predictions and\nobserved counts.\n\nc. in many population modeling applications, an assumption of lognormality is\nadopted. the simplest assumption would be that the log nt are independent and\nnormally distributed with mean log f(t) and variance \u03c32. find the mles under\nthis assumption, using both the gauss\u2013newton and the newton\u2013raphson methods.\nprovide standard errors for your parameter estimates, and an estimate of the corre-\nlation between them. comment.\n\n2.7. himmelblau\u2019s function is f(x, y) = (x2 + y \u2212 11)2 + (x + y2 \u2212 7)2. this function has\nfour minima with a local maximum amid them. illustrate how performance of the\nnelder\u2013mead algorithm can differ depending on the choices for \u03b1r, \u03b1e, and \u03b1c, for the\nfollowing tasks.\na. demonstrate effects for finding a minimum of this function.\nb. demonstrateeffectsforfindingthelocalmaximumofthisfunction.howwellwould\n\na derivative-based procedure work in this case? show examples.\n\ncomment on your results.\n\n2.8. recall that for a two-dimensional problem, the nelder\u2013mead algorithm maintains at\neach iteration a set of three possible solutions defining the vertices of a simplex, specif-\nically a triangle. let us consider whether three is a good choice. imagine an algorithm\nfor two-dimensional optimization that maintains four points defining the vertices of\na convex quadrilateral and is similar to nelder\u2013mead in spirit. speculate how such a\nprocedure could proceed. consider sketches like those shown in figure 2.10. what are\nsome of the inherent challenges? there is no correct answer here; the purpose is to\nbrainstorm and see where your ideas lead.\n\n "}, {"Page_number": 73, "text": "chapter 3\ncombinatorial optimization\n\nit is humbling to learn that there are entire classes of optimization problems for which\nmost methods\u2014including those described previously\u2014are utterly useless.\nwe will pose these problems as maximizations except in section 3.3, although\nin nonstatistical contexts minimization is often customary. for statistical applications,\nrecall that maximizing the log likelihood is equivalent to minimizing the negative log\nlikelihood.\nlet us assume that we are seeking the maximum of f(\u03b8) with respect to \u03b8 =\n(\u03b81, . . . , \u03b8p), where \u03b8 \u2208 \u0001 and \u0001 consists of n elements for a finite positive integer\nn. in statistical applications, it is not uncommon for a likelihood function to depend\non configuration parameters that describe the form of a statistical model and for\nwhich there are many discrete choices, as well as a small number of other parameters\nthat could be easily optimized if the best configuration were known. in such cases,\nwe may view f(\u03b8) as the log profile likelihood of a configuration, \u03b8, that is, the\nhighest likelihood attainable using that configuration. section 3.1.1 provides several\nexamples.\neach \u03b8 \u2208 \u0001 is termed a candidate solution. let fmax denote the globally max-\nimum value of f(\u03b8) achievable for \u03b8 \u2208 \u0001, and let the set of global maxima be\nm = {\u03b8 \u2208 \u0001 : f(\u03b8) = fmax}.ifthereareties,mwillcontainmorethanoneelement.\ndespite the finiteness of \u0001, finding an element of m may be very hard if there are\ndistracting local maxima, plateaus, and long paths toward optima in \u0001, and if n is\nextremely large.\n\n3.1 hard problems and np-completeness\nhard optimization problems are generally combinatorial in nature. in such problems,\np items may be combined or sequenced in a very large number of ways, and each\nchoice corresponds to one element in the space of possible solutions. maximization\nrequires a search of this very large space.\nfor example, consider the traveling salesman problem. in this problem, the\nsalesman must visit each of p cities exactly once and return to his point of origin,\nusing the shortest total travel distance. we seek to minimize the total travel distance\nover all possible routes (i.e., maximize the negative distance). if the distance between\ntwo cities does not depend on the direction traveled between them, then there are\n\ncomputational statistics, second edition. geof h. givens and jennifer a. hoeting.\n\u00a9 2013 john wiley & sons, inc. published 2013 by john wiley & sons, inc.\n\n59\n\n "}, {"Page_number": 74, "text": "chapter 3 combinatorial optimization\n\n60\n(p \u2212 1)!/2 possible routes (since the point of origin and direction of travel are arbi-\ntrary). note that any tour corresponds to a permutation of the integers 1, . . . , p, which\nspecifies the sequence in which the cities are visited.\nto consider the difficulty of such problems, it is useful to discuss the number\nof steps required for an algorithm to solve it, where steps are simple operations like\narithmetic,comparisons,andbranching.thenumberofoperationsdepends,ofcourse,\non the size of the problem posed. in general the size of a problem may be specified\nas the number of inputs needed to pose it. the traveling salesman problem is posed\nby specifying p city locations to be sequenced. the difficulty of a particular size-p\nproblem is characterized by the number of operations required to solve it in the worst\ncase using the best known algorithm.\nthe number of operations is only a rough notion, because it varies with imple-\nmentation language and strategy. it is conventional, however, to bound the number\nof operations using the notation o(h(p)). if h(p) is polynomial in p, an algorithm is\nsaid to be polynomial.\nalthough the actual running time on a computer depends on the speed of the\ncomputer, we generally equate the number of operations and the execution time by\nrelying on the simplifying assumption that all basic operations take the same amount\nof time (one unit). then we may make meaningful comparisons of algorithm speeds\neven though the absolute scale is meaningless.\nconsider two problems of size p = 20. suppose that the first problem can be\nsolved in polynomial time [say o(p2) operations], and the solution requires 1 minute\non your office computer. then the size-21 problem could be solved in just a few\nseconds more. the size-25 problem can be solved in 1.57 minutes, size 30 in 2.25\nminutes, and size 50 in 6.25 minutes. suppose the second problem is o(p!) and\nrequires 1 minute for size 20. then it would take 21 minutes for size 21, 12.1 years\n(6,375,600minutes)forsize25,207millionyearsforsize30,and2.4 \u00d7 1040 yearsfor\nsize 50. similarly, if an o(p!) traveling salesman problem of size 20 could be solved\nin 1 minute, it would require far longer than the lifetime of the universe to determine\nthe optimal path for the traveling salesman to make a tour of the 50 u.s. state capitals.\nfurthermore, obtaining a computer that is 1000 times faster would barely reduce the\ndifficulty. the conclusion is stark: some optimization problems are simply too hard.\nthe complexity of a polynomial problem\u2014even for large p and high polynomial\norder\u2014is dwarfed by the complexity of a quite small nonpolynomial problem.\nthe theory of problem complexity is discussed in [214, 497]. for us to dis-\ncuss this issue further, we must make a formal distinction between optimization (i.e.,\nsearch) problems and decision (i.e., recognition) problems. thus far, we have con-\nsidered optimization problems of the form: \u201cfind the value of \u03b8 \u2208 \u0001 that maximizes\nf(\u03b8).\u201d the decision counterpart to this is: \u201cis there a \u03b8 \u2208 \u0001 for which f(\u03b8) > c, for\na fixed number c?\u201d clearly there is a close relationship between these two versions\nof the problem. in principle, we could solve the optimization problem by repeatedly\nsolving the decision problem for strategically chosen values of c.\ndecisionproblemsthatcanbesolvedinpolynomialtime[e.g.,o(pk)operations\nfor p inputs and constant k] are generally considered to be efficiently solvable [214].\nthese problems belong to the class denoted p. once any polynomial\u2013time algorithm\nhas been identified for a problem, the order of the polynomial is often quickly reduced\n\n "}, {"Page_number": 75, "text": "3.1 hard problems and np-completeness\n\n61\nto practical levels [497]. decision problems for which a given solution can be checked\nin polynomial time are called np problems. clearly a problem in p is in np. however,\nthere seem to be many decision problems, like the traveling salesman problem, that\nare much easier to check than they are to solve. in fact, there are many np problems\nfor which no polynomial\u2013time solution has ever been developed. many np problems\nhave been proven to belong to a special class for which a polynomial algorithm found\nto solve one such problem could be used to solve all such problems. this is the class\nof np-complete problems. there are other problems at least as difficult, for which a\npolynomial algorithm\u2014if found\u2014would be known to provide a solution to all np-\ncomplete problems, even though the problem itself is not proven to be np-complete.\nthese are np-hard problems. there are also many combinatorial decision problems\nthat are difficult and probably np-complete or np-hard although they haven\u2019t been\nproven to be in these classes. finally, optimization problems are no easier than their\ndecision counterparts, and we may classify optimization problems using the same\ncategories listed above.\nit has been shown that if there is a polynomial algorithm for any np-complete\nproblem, then there are polynomial algorithms for all np-complete problems. the\nutter failure of scientists to develop a polynomial algorithm for any np-complete\nproblem motivates the popular conjecture that there cannot be any polynomial algo-\nrithm for any np-complete problem. proof (or counterexample) of this conjecture is\none of the great unsolved problems in mathematics.\nthis leads us to the realization that there are optimization problems that are\ninherently too difficult to solve exactly by traditional means. many problems in bioin-\nformatics, experimental design, and nonparametric statistical modeling, for example,\nrequire combinatorial optimization.\n\nexamples\n\n3.1.1\nstatisticians have been slow to realize how frequently combinatorial optimization\nproblems are encountered in mainstream statistical model-fitting efforts. below we\ngive two examples. in general, when fitting a model requires optimal decisions about\nthe inclusion, exclusion, or arrangement of a number of parameters in a set of possible\nparameters, combinatorial optimization problems arise frequently.\nexample3.1 (geneticmapping) geneticdataforindividualsandgroupsofrelated\nindividuals are often analyzed in ways that present highly complex combinatorial\noptimization problems. for example, consider the problem of locating genes on a\nchromosome, known as the genetic mapping problem.\nthe genes, or more generally genetic markers, of interest in a chromosome\ncan be represented as a sequence of symbols. the position of each symbol along the\nchromosome is called its locus. the symbols indicate genes or genetic markers, and\nthe particular content stored at a locus is an allele.\ndiploid species like humans have pairs of chromosomes and hence two alleles\nat any locus. an individual is homozygous at a locus if the two alleles are identical\nat this locus; otherwise the individual is heterozygous. in either case, each parent\ncontributes one allele at each locus of an offspring\u2019s chromosome pair. there are\n\n "}, {"Page_number": 76, "text": "62\n\nchapter 3 combinatorial optimization\n\nparent\u2019s chromosome\n\n0\n\n0\n\n0\n\n0\n\n0\n\nparent\u2019s contribution\n\nto offspring\n\n0\n\n0\n\n0\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\nmeiosis\n\nfigure 3.1 during meiosis, a crossover occurs between the third and fourth loci. the zeros\nand ones indicate the origin of each allele in the contributed chromosome. only one parental\ncontribution is shown, for simplicity.\n\ntwo possible contributions from any parent, because the parent has two alleles at the\ncorresponding locus in his/her chromosome pair. although each parent allele has a\n50% chance of being contributed to the offspring, the contributions from a particular\nparent are not made independently at random. instead, the contribution by a parent\nconsists of a chromosome built during meiosis from segments of each chromosome in\nthe parent\u2019s pair of chromosomes. these segments will contain several loci. when the\nsource of the alleles on the contributed chromosome changes from one chromosome\nof the parent\u2019s pair to the other one, a crossover is said to have occurred. figure 3.1\nillustrates a crossover occurring during meiosis, forming the chromosome contributed\nto the offspring by one parent. this method of contribution means that alleles whose\nloci are closer together on one of the parent\u2019s chromosomes are more likely to appear\ntogether on the chromosome contributed by that parent.\nwhen the alleles at two loci of a parent\u2019s chromosome appear jointly on the\ncontributed chromosome more frequently than would be expected by chance alone,\nthey are said to be linked. when the alleles at two different loci of a parent\u2019s chro-\nmosome do not both appear in the contributed chromosome, a recombination has\noccurred between the loci. the frequency of recombinations determines the degree\nof linkage between two loci: infrequent recombination corresponds to strong linkage.\nthe degree of linkage, or map distance, between two loci corresponds to the expected\nnumber of crossovers between the two loci.\na genetic map of p markers consists of an ordering of their loci and a list of\ndistances or probabilities of recombination between adjacent loci. assign to each\nlocus a label, \u2113, for \u2113 = 1, . . . , p. the ordering component of the map, denoted\n\u03b8 = (\u03b81, . . . , \u03b8p), describes the arrangement of the p locus labels in order of their\npositions along the chromosome, with \u03b8j = \u2113 if the locus labeled \u2113 lies at the jth po-\nsition along the chromosome. thus, \u03b8 is a permutation of the integers 1, . . . , p. the\nother component of a genetic map is a list of distances between adjacent loci. denote\nthe probability of recombination between adjacent loci \u03b8j and \u03b8j+1 as d(\u03b8j, \u03b8j+1).\nthis amounts to the map distance between these loci. figure 3.2 illustrates this\nnotation.\nsuch a map can be estimated by observing the alleles at the p loci for a sample\nof n chromosomes generated during the meiosis from a parent that is heterozygous\nat all p loci. each such chromosome can be represented by a sequence of zeros and\nones, indicating the origin of each allele in the contributed parent. for example, the\nchromosome depicted on the right side of figure 3.1 can be denoted 00011, because\n\n "}, {"Page_number": 77, "text": "position, j\n\u03b8j =\nlocus label,\n\ndistance, d(\u03b8j,\u03b8j+1)\n\n3.1 hard problems and np-completeness\n\n63\n\n\u03b81\n\n1\n3=\n3\n\n\u03b82\n\n2\n1=\n1\n\n\u03b83\n\n3\n4=\n4\n\n\u03b84\n\n4\n2=\n2\n\nd(3,1)\n\nd(1,4)\n\nd(4,2)\n\nfigure 3.2 notation for gene mapping example with p = 4 loci. the loci are labeled in\nboxesattheirpositionsalongthechromosome.thecorrectsequentialorderingoflociisdefined\nby the \u03b8j values. distances between loci are given by d(\u03b8j, \u03b8j+1) for j = 1, . . . ,3.\nthe first three alleles originate from the first chromosome of the parent and the final\ntwo alleles originate from the second chromosome of the parent.\nlettherandomvariable xi,\u03b8j denotetheoriginofthealleleinthelocuslabeled \u03b8j\nforthe ithchromosomegeneratedduringmeiosis.thedatasetconsistsofobservations,\n, of these random variables. thus, a recombination for two adjacent markers\nxi,\u03b8j\n\nindependently in each interval, the probability of a given map is\n\nhas been observed in the ith case if!!xi,\u03b8j \u2212 xi,\u03b8j+1!! = 1, and no recombination has\nbeen observed if!!xi,\u03b8j \u2212 xi,\u03b8j+1!! = 0. if recombination events are assumed to occur\np\u22121\"j=1\nn\"i=1#$1 \u2212 d(\u03b8j, \u03b8j+1)%$1 \u2212!!xi,\u03b8j \u2212 xi,\u03b8j+1!!% + d(\u03b8j, \u03b8j+1)!!xi,\u03b8j \u2212 xi,\u03b8j+1!!&.\n\n(3.1)\ngiven an ordering \u03b8, the mles for the recombination probabilities are easily found\nto be\n\n\u02c6d(\u03b8j, \u03b8j+1) =\n\n1\nn\n\nn\u2019i=1!!xi,\u03b8j \u2212 xi,\u03b8j+1!!.\n\n(3.2)\n\ngiven d(\u03b8j, \u03b8j+1), the number of recombinations between the loci in positions j and\nj + 1 is(n\ni=1 |xi,\u03b8j \u2212 xi,\u03b8j+1|, which has a bin(n, d(\u03b8j, \u03b8j+1)) distribution. we can\ncompute the profile likelihood for \u03b8 by adding the log likelihoods of the p \u2212 1 sets of\nadjacent loci and replacing each d(\u03b8j, \u03b8j+1) by its conditional maximum likelihood\nestimate \u02c6d(\u03b8j, \u03b8j+1). let \u02c6d(\u03b8) compute these maximum likelihood estimates for any\n\u03b8. then the profile likelihood for \u03b8 is\n\nl(\u03b8|\u02c6d(\u03b8)) =\n\np\u22121\u2019j=1\nn)\u02c6d(\u03b8j, \u03b8j+1)log{\u02c6d(\u03b8j, \u03b8j+1)}\n+ (1 \u2212 \u02c6d(\u03b8j, \u03b8j+1))log{1 \u2212 \u02c6d(\u03b8j, \u03b8j+1)}*\np\u22121\u2019j=1\nt(\u03b8j, \u03b8j+1),\n\n=\n\n(3.3)\n\n "}, {"Page_number": 78, "text": "64\n\nchapter 3 combinatorial optimization\n\nwhere t(\u03b8j, \u03b8j+1)isdefinedtobezeroif \u02c6d(\u03b8j, \u03b8j+1)iszeroorone.thenthemaximum\nlikelihood genetic map is obtained by maximizing (3.3) over all permutations \u03b8. note\nthat (3.3) constitutes a sum of terms t(\u03b8j, \u03b8j+1) whose values depend on only two\nloci. suppose that all possible pairs of loci are enumerated, and the value t(i, j) is\ncomputedforevery iand j where1 \u2264 i < j \u2264 p.thereare p(p \u2212 1)/2 suchvaluesof\nt(i, j). the profile log likelihood can then be computed rapidly for any permutation\n\u03b8 by summing the necessary values of t(i, j).\nhowever, finding the maximum likelihood genetic map requires maximizing\nthe profile likelihood by searching over all p!/2 possible permutations. this is a\nvariant of the traveling salesman problem, where each genetic marker corresponds to\na city and the distance between cities i and j is t(i, j). the salesman\u2019s tour may start\nat any city and terminates in the last city visited. a tour and its reverse are equivalent.\nthere are no known algorithms for solving general traveling salesman problems in\npolynomial time.\nfurther details and extensions of this example are considered in [215, 572]. !\nexample 3.2 (variable selection in regression)\nconsider a multiple linear\nregression problem with p potential predictor variables. a fundamental step in\nregression is selection of a suitable model. given a dependent variable y and a set\nof candidate predictors x1, x2, . . . , xp, we must find the best model of the form\nj=1 \u03b2ij xij + \u03f5, where {i1, . . . , is} is a subset of {1, . . . , p} and \u03f5 denotes\na random error. the notion of what model is best may have any of several meanings.\nsuppose that the goal is to use the akaike information criterion (aic) to select\nthe best model [7, 86]. we seek to find the subset of predictors that minimizes the\nfitted model aic,\n\ny = \u03b20 +(s\n\naic = n log{rss/n} + 2(s + 2),\n\n(3.4)\nwhere n is the sample size, s is the number of predictors in the model, and rss\nis the sum of squared residuals. alternatively, suppose that bayesian regression is\nperformed, say with the normal-gamma conjugate class of priors \u03b2 \u223c n(\u00b5, \u03c32v) and\n\u03bd\u03bb/\u03c32 \u223c \u03c72\n\u03bd.inthiscase,onemightseektofindthesubsetofpredictorscorresponding\nto the model that maximizes the posterior model probability [527].\nin either case, the variable selection problem requires an optimization over a\nspace of 2p+1 possible models, since each variable and the intercept may be included\nor omitted. it also requires estimating the best \u03b2ij for each of the 2p+1 possible\nmodels, but this step is easy for any given model. although a search algorithm that is\nmore efficient than exhaustive search has been developed to optimize some classical\nregression model selection criteria, it is practical only for fairly small p [213, 465].\nwe know of no efficient general algorithm to find the global optimum (i.e., the single\nbest model) for either the aic or the bayesian goals.\n!\n\n3.1.2 need for heuristics\nthe existence of such challenging problems requires a new perspective on optimiza-\ntion. it is necessary to abandon algorithms that are guaranteed to find the global\n\n "}, {"Page_number": 79, "text": "65\nmaximum (under suitable conditions) but will never succeed within a practical time\nlimit. instead we turn to algorithms that can find a good local maximum within toler-\nable time.\nsuch algorithms are sometimes called heuristics. they are intended to find a\nglobally competitive candidate solution (i.e., a nearly optimal one), with an explicit\ntrade of global optimality for speed. the two primary features of such heuristics are\n\n3.2 local search\n\n1. iterative improvement of a current candidate solution, and\n2. limitation of the search to a local neighborhood at any particular iteration.\n\nthese two characteristics embody the heuristic strategy of local search, which we\naddress first.\nno single heuristic will work well in all problems. in fact, there is no search\nalgorithm whose performance is better than another when performance is averaged\nover the set of all possible discrete functions [576, 672]. there is clearly a motivation\nto adopt different heuristics for different problems. thus we continue beyond local\nsearch to examine simulated annealing, genetic algorithms, and tabu algorithms.\n\n3.2 local search\nlocal search is a very broad optimization paradigm that arguably encompasses all\nof the techniques described in this chapter. in this section, we introduce some of the\nits simplest, most generic variations such as k-optimization and random starts local\nsearch.\nbasic local search is an iterative procedure that updates a current candidate\nsolution \u03b8(t) at iteration t to \u03b8(t+1). the update is termed a move or a step. one or more\npossible moves are identified from a neighborhood of \u03b8(t), say n(\u03b8(t)). the advantage\nof local search over global (i.e., exhaustive) search is that only a tiny portion of \u0001\nneed be searched at any iteration, and large portions of \u0001 may never be examined.\nthe disadvantage is that the search is likely to terminate at an uncompetitive local\nmaximum.\na neighborhood of the current candidate solution, n(\u03b8(t)), contains candidate\nsolutions that are near \u03b8(t). often, proximity is enforced by limiting the number of\nchanges to the current candidate solution used to generate an alternative. in practice,\nsimple changes to the current candidate solution are usually best, resulting in small\nneighborhoods that are easily searched or sampled. complex alterations are often\ndifficult to conceptualize, complicated to code, and slow to execute. moreover, they\nrarely improve search performance, despite the intuition that larger neighborhoods\nwould be less likely to lead to entrapment at a poor local maximum. if the neighbor-\nhood is defined by allowing as many as k changes to the current candidate solution\nin order to produce the next candidate, then it is a k-neighborhood, and the alteration\nof those features is called a k-change.\nthe definition a neighborhood is intentionally vague to allow flexible usage\nof the term in a wide variety of problems. for the gene mapping problem intro-\nduced in example 3.1, suppose \u03b8(t) is a current ordering of genetic markers. a simple\n\n "}, {"Page_number": 80, "text": "chapter 3 combinatorial optimization\n\n66\nneighborhood might be the set of all orderings that can be obtained by swapping the\nlocations of only two markers on the chromosome whose order is \u03b8(t). in the regres-\nsion model selection problem introduced in example 3.2, a simple neighborhood is\nthe set of models that either add or omit one predictor from \u03b8(t).\na local neighborhood will usually contain several candidate solutions. an ob-\nvious strategy at each iteration is to choose the best among all candidates in the\ncurrent neighborhood. this is the method of steepest ascent. to speed performance,\none might instead select the first randomly chosen neighbor for which the objective\nfunction exceeds its previous value; this is random ascent or next ascent.\nif k-neighborhoods are used for a steepest ascent algorithm, the solution is said\nto be k-optimal. alternatively, any local search algorithm that chooses \u03b8(t+1) uphill\nfrom \u03b8(t) is an ascent algorithm, even if the ascent is not the steepest possible within\nn(\u03b8(t)).\nthe sequential selection of steps that are optimal in small neighborhoods, dis-\nregarding the global problem, is reminiscent of a greedy algorithm. a chess player\nusing a greedy algorithm might look for the best immediate move with total disre-\ngard to its future consequences: perhaps moving a knight to capture a pawn without\nrecognizing that the knight will be captured on the opponent\u2019s next move. wise selec-\ntion of a new candidate solution from a neighborhood of the current candidate must\nbalance the need for a narrow focus enabling quick moves against the need to find a\nglobally competitive solution. to avoid entrapment in poor local maxima, it might be\nreasonable\u2014every once in a while\u2014to eschew some of the best neighbors of \u03b8(t) in\nfavor of a direction whose rewards are only later realized. for example, when \u03b8(t) is a\nlocal maximum, the approach of steepest ascent/mildest descent [306] allows a move\nto the least unfavorable \u03b8(t+1) \u2208 n(\u03b8(t)) (see section 3.5). there are also a variety of\ntechniques in which a candidate neighbor is selected from n(\u03b8(t)) and a random deci-\nsion rule is used to decide whether to adopt it or retain \u03b8(t). these algorithms generate\nmarkov chains {\u03b8(t)} (t = 0,1, . . .) that are closely related to simulated annealing\n(section 3.3) and the methods of chapter 7.\nsearching within the current neighborhood for a k-change steepest ascent move\ncan be difficult when k is greater than 1 or 2 because the size of the neighborhood\nincreases rapidly with k. for larger k, it can be useful to break the k-change up into\nsmaller parts, sequentially selecting the best candidate solutions in smaller neigh-\nborhoods. to promote search diversity, breaking a k-change step into several smaller\nsequential changes can be coupled with the strategy of allowing one or more of\nthe smaller steps to be suboptimal (e.g., random). such variable-depth local search\napproaches permit a potentially better step away from the current candidate solution,\neven though it will not likely be optimal within the k-neighborhood.\nascent algorithms frequently converge to local maxima that are not globally\ncompetitive. one approach to overcoming this problem is the technique of random\nstarts local search. here, a simple ascent algorithm is repeatedly run to termination\nfrom a large number of starting points. the starting points are chosen randomly. the\nsimplest approach is to select starting points independently and uniformly at random\nover \u0001. more sophisticated approaches may employ some type of stratified sampling\nwhere the strata are identified from some pilot runs in an effort to partition \u0001 into\nregions of qualitatively different convergence behavior.\n\n "}, {"Page_number": 81, "text": "3.2 local search\n\n67\n\ntable 3.1 potential predictors of baseball players\u2019 salaries.\n1. batting average\n2. on base pct. (obp)\n3. runs scored\n4. hits\n5. doubles\n6. triples\n7. home runs (hrs)\n8. runs batted in (rbis)\n9. walks\nafree agent, or eligible.\nbarbitration, or eligible.\n\n10. strikeouts (sos)\n11. stolen bases (sbs)\n12. errors\n13. free agencya\n14. arbitrationb\n15. runs per so\n16. hits per so\n17. hrs per so\n18. rbis per so\n\n19. walks per so\n20. obp / errors\n21. runs per error\n22. hits per error\n23. hrs per error\n24. sos \u00d7 errors\n25. sbs \u00d7 obp\n26. sbs \u00d7 runs\n27. sbs \u00d7 hits\n\nit may seem unsatisfying to rely solely on random starts to avoid being fooled\nby a local maximum. in later sections we introduce methods that modify local search\nin ways that provide a reasonable chance of finding a globally competitive candidate\nsolution\u2014possibly the global maximum\u2014on any single run. of course, the strategy\nof using multiple random starts can be overlaid on any of these approaches to provide\nadditional confidence in the best solution found. indeed, we recommend that this is\nalways done when feasible.\nexample 3.3 (baseball salaries) random starts local search can be very effective\nin practice because it is simple to code and fast to execute, allowing time for a large\nnumber of random starts. here, we consider its application to a regression model\nselection problem.\ntable 3.1 lists 27 baseball performance statistics, such as batting percentages\nandnumbersofhomeruns,whichwerecollectedfor337players(nopitchers)in1991.\nplayers\u2019 1992 salaries, in thousands of dollars, may be related to these variables\ncomputed from the previous season. these data, derived from the data in [654],\nmay be downloaded from the website for this book. we use the log of the salary\nvariable as the response variable. the goal is to find the best subset of predictors\nto predict log salary using a linear regression model. assuming that the intercept\nwill be included in any model, there are 227 = 134,217,728 possible models in the\nsearch space.\nfigure 3.3 illustrates the application of a random starts local search algorithm\nto minimize the aic with respect to regression variable selection. the problem can be\nposedasmaximizingthenegativeoftheaic,thuspreservingourpreferenceforuphill\nsearch.neighborhoodswerelimitedto1-changesgeneratedfromthecurrentmodelby\neither adding or deleting one predictor. search was started from 5 randomly selected\nsubsets of predictors (i.e., five starting points), and 14 additional steps were allocated\nto each start. each move was made by steepest ascent. since each steepest ascent step\nrequires searching 27 neighbors, this small example requires 1890 evaluations of the\nobjective function. a comparable limit to objective function evaluations was imposed\non examples of other heuristic techniques that follow in the remainder of this chapter.\n\n "}, {"Page_number": 82, "text": "68\n\nchapter 3 combinatorial optimization\n\n420\n\n400\n\n380\n\n360\n\ni\n\nc\na\n \ne\nv\ni\nt\na\ng\ne\nn\n\n1\n\n16\n\n31\n\n46\ncumulative iterations\n\n61\n\nfigure 3.3 results of random starts local search by steepest ascent for example 3.3, for\n15 iterations from each of five random starts. only aic values between \u2212360 and \u2212420 are\nshown.\n\nfigure 3.3 shows the value of the aic for the best model at each step. table 3.2\nsummarizes the results of the search. the second and third random starts (labeled ls\n(2,3)) led to an optimal aic of \u2212418.95, derived from the model using predictors\n2, 3, 6, 8, 10, 13, 14, 15, 16, 24, 25, and 26. the worst random start was the first,\nwhich led to an aic of \u2212413.04 for a model with 10 predictors. for the sake of\ncomparison, a greedy stepwise method (the step() procedure in s-plus [642])\nchose a model with 12 predictors, yielding an aic of \u2212418.94. the greedy stepwise\nmethod of efroymson [465] chose a model with 9 predictors, yielding an aic of\n\u2212402.16; however, this method is designed to find a good parsimonious model using\na criterion that differs slightly from the aic. with default settings, neither of these\noff-the-shelf algorithms found a model quite as good as the one found with a simple\nrandom starts local search.\n!\n\n3.3 simulated annealing\nsimulated annealing is a popular technique for combinatorial optimization because\nit is generic and easily implemented in its simplest form. also, its limiting behavior\nis well studied. on the other hand, this limiting behavior is not easily realized in\npractice, the speed of convergence can be maddeningly slow, and complex esoteric\ntinkering may be needed to substantially improve the performance. useful reviews of\nsimulated annealing include [75, 641].\nannealing is the process of heating up a solid and then cooling it slowly. when a\nstressedsolidisheated,itsinternalenergyincreasesanditsmoleculesmoverandomly.\nif the solid is then cooled slowly, the thermal energy generally decreases slowly, but\n\n "}, {"Page_number": 83, "text": "3.3 simulated annealing\n\n69\n\ntable 3.2 results of random starts local search model selection for example 3.3. the bullets\nindicate inclusion of the corresponding predictor in each model selected, with model labels\nexplained in the text. in addition, all models in this table included predictors 3, 8, 13 and 14.\n\npredictors selected\n\nmethod\nls (2,3)\ns-plus\nls (5)\nls (4)\nls (1)\nefroy.\n\n1 2 6 7 9 10 12 15 16 18 19 20 21 22 24 25 26 27\n\n\u2022 \u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\naic\n\u2212418.95\n\u2212418.94\n\u2022 \u2212416.15\n\u2212415.52\n\u2212413.04\n\u2212402.16\n\nthere are also random increases governed by boltzmann\u2019s probability. namely, at\ntemperature \u03c4, the probability density of an increase in energy of magnitude \u0001e is\nexp{\u2212\u0001e/k\u03c4} where k is boltzmann\u2019s constant. if the cooling is slow enough and\ndeep enough, the final state is unstressed, where all the molecules are arranged to\nhave minimal potential energy.\nfor consistency with the motivating physical process, we pose optimization as\nminimization in this section, so the minimum of f(\u03b8) is sought over \u03b8 \u2208 \u0001. then it is\npossible to draw an analogy between the physical cooling process and the process of\nsolving a combinatorial minimization problem [130, 378]. for simulated annealing\nalgorithms, \u03b8 corresponds to the state of the material, f(\u03b8) corresponds to its energy\nlevel,andtheoptimalsolutioncorrespondstothe \u03b8 thathasminimumenergy.random\nchanges to the current state (i.e., moves from \u03b8(t) to \u03b8(t+1)) are governed by the boltz-\nmann distribution given above, which depends on a parameter called temperature.\nwhen the temperature is high, acceptance of uphill moves (i.e., moves to a higher\nenergy state) are more likely to be tolerated. this discourages convergence to the\nfirst local minimum that happens to be found, which might be premature if the space\nof candidate solutions has not yet been adequately explored. as search continues,\nthe temperature is lowered. this forces increasingly concentrated search effort near\nthe current local minimum, because few uphill moves will be allowed. if the cooling\nschedule is determined appropriately, the algorithm will hopefully converge to the\nglobal minimum.\nthe simulated annealing algorithm is an iterative procedure started at time\nt = 0 with an initial point \u03b8(0) and a temperature \u03c40. iterations are indexed by t. the\nalgorithm is run in stages, which we index by j = 0,1,2, . . ., and each stage consists\nof several iterations. the length of the jth stage is mj. each iteration proceeds as\nfollows:\n1. select a candidate solution \u03b8\u2217 within the neighborhood of \u03b8(t), say n(\u03b8(t)),\naccording to a proposal density g(t)(\u00b7 | \u03b8(t)).\n2. randomly decide whether to adopt \u03b8\u2217 as the next candidate solution or to keep\nanother copy of the current solution. specifically, let \u03b8(t+1) = \u03b8\u2217 with probabil-\nity equal to min$1,exp)[f(\u03b8(t)) \u2212 f(\u03b8\u2217)]/\u03c4j*%. otherwise, let \u03b8(t+1) = \u03b8(t).\n\n "}, {"Page_number": 84, "text": "70\n\nchapter 3 combinatorial optimization\n\n3. repeat steps 1 and 2 a total of mj times.\n4. increment j. update \u03c4j = \u03b1(\u03c4j\u22121) and mj = \u03b2(mj\u22121). go to step 1.\nif the algorithm is not stopped according to a limit on the total number of iterations\nor a predetermined schedule of \u03c4j and mj, one can monitor an absolute or relative\nconvergence criterion (see chapter 2). often, however, the stopping rule is expressed\nas a minimum temperature. after stopping, the best candidate solution found is the\nestimated minimum.\nthe function \u03b1 should slowly decrease the temperature to zero. the number of\niterations at each temperature (mj) should be large and increasing in j. ideally, the\nfunction \u03b2 should scale the mj exponentially in p, but in practice some compromises\nwill be required in order to obtain tolerable computing speed.\nalthough the new candidate solution is always adopted when it is superior to the\ncurrent solution, note that it has some probability of being adopted even when it is in-\nferior. in this sense, simulated annealing is a stochastic descent algorithm. its random-\nness allows simulated annealing sometimes to escape uncompetitive local minima.\n\n3.3.1 practical issues\n3.3.1.1 neighborhoods and proposals strategies for choosing neighbor-\nhoods can be very problem specific, but the best neighborhoods are usually small\nand easily computed.\nconsider the traveling salesman problem. numbering the cities 1,2, . . . , p, any\ntour \u03b8 can be written as a permutation of these integers. the cities are linked in this\norder, with an additional link between the final city visited and the original city where\nthe tour began. a neighbor of \u03b8 can be generated by removing two nonadjacent links\nand reconnecting the tour. in this case, there is only one way to obtain a valid tour\nthrough reconnection: one of the tour segments must be reversed. for example, the\ntour 143256 is a neighbor of the tour 123456. since two links are altered, the process\nof generating such neighbors is a 2-change, and it yields a 2-neighborhood. any tour\nhas p(p \u2212 3)/2 unique 2-change neighbors distinct from \u03b8 itself. this neighborhood\nis considerably smaller than the (p \u2212 1)!/2 tours in the complete solution space.\nit is critical that the chosen neighborhood structure allows all solutions in \u0001\nto communicate. for \u03b8i and \u03b8j to communicate, it must be possible to find a finite\nsequence of solutions \u03b81, . . . , \u03b8k such that \u03b81 \u2208 n(\u03b8i), \u03b82 \u2208 n(\u03b81), . . ., \u03b8k \u2208 n(\u03b8k\u22121),\nand \u03b8j \u2208 n(\u03b8k). the 2-neighborhoods mentioned above for the traveling salesman\nproblem allow communication between any \u03b8i and \u03b8j.\nthe most common proposal density, g(t)(\u00b7 | \u03b8(t)), is discrete uniform\u2014a candi-\ndate is sampled completely at random from n(\u03b8(t)). this has the advantage of speed\nand simplicity. other, more strategic methods have also been suggested [281, 282,\n659].\nrapid updating of the objective function is an important strategy for speeding\nsimulated annealing runs. in the traveling salesman problem, sampling a 2-neighbor\nat random amounts to selecting two integers from which is derived a permutation\nof the current tour. note also for the traveling salesman problem that f(\u03b8\u2217) can\nbe efficiently calculated for any \u03b8\u2217 in the 2-neighborhood of \u03b8(t) when f(\u03b8(t)) has\n\n "}, {"Page_number": 85, "text": "3.3 simulated annealing\n\n71\nalready been found. in this case, the new tour length equals the old tour length minus\nthe distance for traveling the two broken links, plus the distance for traveling the two\nnew links. the time to compute this does not depend on problem size p.\n3.3.1.2 cooling schedule and convergence the sequence of stage lengths\nand temperatures is called the cooling schedule. ideally, the cooling schedule should\nbe slow.\nthe limiting behavior of simulated annealing follows from markov chain the-\nory, briefly reviewed in chapter 1. simulated annealing can be viewed as producing\na sequence of homogeneous markov chains (one at each temperature) or a single\ninhomogeneous markov chain (with temperature decreasing between transitions).\nalthough these views lead to different approaches to defining limiting behavior, both\nlead to the conclusion that the limiting distribution of draws has support only on the\nset of global minima.\nto understand why cooling should lead to the desired convergence of the\nalgorithm at a global minimum, first consider the temperature to be fixed at \u03c4. sup-\npose further that proposing \u03b8i from n(\u03b8j) has the same probability as proposing \u03b8j\nfrom n(\u03b8i) for any pair of solutions \u03b8i and \u03b8j in \u0001. in this case, the sequence of\n\u03b8(t) generated by simulated annealing is a markov chain with stationary distribution\n\u03c0\u03c4(\u03b8) \u221d exp{\u2212f(\u03b8)/\u03c4}. this means that limt\u2192\u221e p[\u03b8(t) = \u03b8] = \u03c0\u03c4(\u03b8). this approach\nto generating a sequence of random values is called the metropolis algorithm and is\ndiscussed in section 7.1.\nin principle, we would like to run the chain at this fixed temperature long\nenough that the markov chain is approximately in its stationary distribution before\nthe temperature is reduced.\nsuppose there are m global minima and the set of these solutions is m. denote\nthe minimal value of f on \u0001 as fmin. then the stationary distribution of the chain for\na fixed \u03c4 is given by\n\n\u03c0\u03c4(\u03b8i) =\n\nexp{\u2212+f(\u03b8i) \u2212 fmin,- \u03c4}\n\nexp{\u2212+f(\u03b8j) \u2212 fmin,- \u03c4}\n\nm +(j /\u2208 m\n\nfor each \u03b8i \u2208 \u0001.\nand 1 if i \u2208 m. thus\n\nnow, as \u03c4 \u2192 0 from above, the limit of exp{\u2212+f(\u03b8i) \u2212 fmin,- \u03c4} is 0 if i /\u2208 m\n\n(3.5)\n\n(3.6)\n\nlim\n\n\u03c4\u21930 \u03c0\u03c4(\u03b8i) =.1/m if i \u2208 m,\n\notherwise.\n\n0\n\nthe mathematics to make these arguments precise can be found in [67, 641].\nit is also possible to relate the cooling schedule to a bound on the quality of\nthe final solution. if one wishes any iterate to have not more than probability \u03b4 in\nequilibrium of being worse than the global minimum by no more than \u03f5, this can be\nachieved if one cools until \u03c4j \u2264 \u03f5/log{(n \u2212 1)/\u03b4}, where n is the number of points\nin \u0001 [426]. in other words, this \u03c4j ensures that the final markov chain configuration\nwill in equilibrium have p+f(\u03b8(t)) > fmin + \u03f5, < \u03b4.\n\n "}, {"Page_number": 86, "text": "72\n\nchapter 3 combinatorial optimization\nif neighborhoods communicate and the depth of the deepest local (and non-\nglobal) minimum is c, then the cooling schedule given by \u03c4 = c/log{1 + i} guaran-\ntees asymptotic convergence, where i indexes iterations [292]. the depth of a local\nminimum is defined to be the smallest increase in the objective function needed to\nescape from that local minimum into the valley of any other minimum. however,\nmathematical bounds on the number of iterations required to achieve a high prob-\nability of having discovered at least one element of m often exceed the size of \u0001\nitself. in this case, one cannot establish that simulated annealing will find the global\nminimum more quickly than an exhaustive search [33].\nif one wishes the markov chain generated by simulated annealing to be approx-\nimately in its stationary distribution at each temperature before reducing temperature,\nthen the length of the run ideally should be at least quadratic in the size of the solution\nspace [1], which itself is usually exponential in problem size. clearly, much shorter\nstage lengths must be chosen if simulated annealing is to require fewer iterations than\nexhaustive search.\nin practice, many cooling schedules have been tried [641]. recall that the\ntemperature at stage j is \u03c4j = \u03b1(\u03c4j\u22121) and the number of iterations in stage j is\nmj = \u03b2(mj\u22121). one popular approach is to set mj = 1 for all j and reduce the tem-\nperature very slowly according to \u03b1(\u03c4j\u22121) = \u03c4j\u22121/(1 + a\u03c4j\u22121) for a small value of a.\na second option is to set \u03b1(\u03c4j\u22121) = a\u03c4j\u22121 for a < 1 (usually a \u2265 0.9). in this case,\none might increase stage lengths as temperatures decrease. for example, consider\n\u03b2(mj\u22121) = bmj\u22121 for b > 1, or \u03b2(mj\u22121) = b + mj\u22121 for b > 0. a third schedule uses\n\n\u03b1(\u03c4j\u22121) =\n\n\u03c4j\u22121\n\n1 + \u03c4j\u22121 log{1 + r}/(3s\u03c4j\u22121)\n\nwhere s2\n\u03c4j\u22121 is the square of the mean objective function cost at the current temperature\nminus the mean squared cost at the current temperature, and r is some small real\nnumber [1]. using the temperature schedule \u03c4 = c/log{1 + i} mentioned above\nbased on theory is rarely practical because it is too slow and the determination of c\nis difficult, with excessively large guesses for c further slowing the algorithm.\nmost practitioners require lengthy experimentation to find suitable initial\nparameter values (e.g., \u03c40 and m0) and values of the proposed schedules (e.g., a,\nb, and r). while selection of the initial temperature \u03c40 is usually problem dependent,\nsome general guidelines may be given. a useful strategy is to choose a positive \u03c40\n\u03b8j in \u0001. the rationale for this choice is that it provides any point in the parameter\nspace with a reasonable chance of being visited in early iterations of the algorithm.\nsimilarly, choosing mj to be large can produce a more accurate solution, but can result\nin long computing times. as a general rule of thumb, larger decreases in temperature\nrequire longer runs after the decrease. finally, a good deal of evidence suggests that\nrunning simulated annealing long at high temperatures is not very useful. in many\nproblems, the barriers between local minima are sufficiently modest that jumps be-\ntween them are possible even at fairly low temperatures. good cooling schedules\ntherefore decrease the temperature rapidly at first.\n\nvalue so that exp)[f(\u03b8i) \u2212 f(\u03b8j)]/\u03c40* is close to 1 for any pair of solutions \u03b8i and\n\n "}, {"Page_number": 87, "text": "i\n\nc\na\n\n\u2212360\n\n\u2212380\n\n\u2212400\n\n\u2212420\n\n0\n\n3.3 simulated annealing\n\n73\n\n1.0\n\n0.5\n\ne\nr\nu\nt\na\nr\ne\np\nm\ne\nt\n\n0\n\n2000\n\n1000\niteration\n\nfigure 3.4 results of two simulated annealing minimizations of the regression model aic\nfor example 3.4. the temperature for the bottom curve is shown by the dotted line and the\nright axis. only aic values between \u2212360 and \u2212420 are shown.\n\nexample 3.4 (baseball salaries, continued) to implement simulated annealing\nfor variable selection via the aic in the baseball salary regression problem introduced\nin example 3.3, we must establish a neighborhood structure, a proposal distribution,\nand a temperature schedule. the simplest neighborhoods contain 1-change neighbors\ngenerated from the current model by either adding or deleting one predictor. we\nassigned equal probabilities to all candidates in a neighborhood. the cooling schedule\nhad 15 stages, with stage lengths of 60 for the first 5 stages, 120 for the next 5, and\n220 for the final 5. temperatures were decreased according to \u03b1(\u03c4j\u22121) = 0.9\u03c4j\u22121\nafter each stage.\nfigure 3.4 shows the values of the aic for the sequence of candidate solutions\ngenerated by simulated annealing, for two different choices of \u03c40. the bottom curve\ncorresponds to \u03c40 = 1. in this case, simulated annealing became stuck at particular\ncandidate solutions for distinct periods because the low temperatures allowed little\ntolerance for uphill moves. in the particular realization shown, the algorithm quickly\nfound good candidate solutions with low aic values, where it became stuck fre-\nquently. however, in other cases (e.g., with a very multimodal objective function),\nsuch stickiness may result in the algorithm becoming trapped in a region far from\nthe global minimum. a second run with \u03c40 = 6 (top solid line) yielded considerable\nmixing, with many uphill proposals accepted as moves. the temperature schedule\nfor \u03c40 = 1 is shown by the dotted line and the right axis. both runs exhibited greater\nmixing at higher temperatures. when \u03c40 = 1, the best model found was first identified\nin the 1274th step and dominated the simulation after that point. this model achieved\nan aic of \u2212418.95, and matched the best model found using random starts local\nsearch in table 3.2. when \u03c40 = 6, the best model found had an aic of \u2212417.85.\n\n "}, {"Page_number": 88, "text": "chapter 3 combinatorial optimization\n\n74\nthis run was clearly unsuccessful, requiring more iterations, cooler temperatures,\nor both.\n!\n\nenhancements\n\n3.3.2\nthere are many variations on simulated annealing that purport to improve perfor-\nmance. here we list a few ideas in an order roughly corresponding to the steps in the\nbasic algorithm.\nthe simplest way to start simulated annealing is to start once, anywhere. a\nstrategy employing multiple random starts would have the dual advantages of poten-\ntially finding a better candidate solution and allowing confirmation of convergence to\nthe particular optimum found. purely random starts could be replaced by a stratified\nset of starting points chosen by strategic preprocessing to be more likely to lead to\nminima than simple random starts. such strategies must have high payoffs if they are\nto be useful, given simulated annealing\u2019s generally slow convergence. in some cases,\nthe extra iterations dedicated to various random starts may be better spent on a single\nlong run with longer stage sizes and a slower cooling schedule.\nthesolutionspace, \u0001,mayincludeconstraintson \u03b8.forexample,inthegenetic\nmapping problem introduced in example 3.1, \u03b8 must be a permutation of the inte-\ngers 1, . . . , p when there are p markers. when the process for generating neighbors\ncreates solutions that violate these constraints, substantial time may be wasted fixing\ncandidates or repeatedly sampling from n(\u03b8(t)) until a valid candidate is found. an\nalternative is to relax the constraints and introduce a penalty into f that penalizes in-\nvalid solutions. in this manner, the algorithm can be discouraged from visiting invalid\nsolutions without dedicating much time to enforcing constraints.\nin the basic algorithm, the neighborhood definition is static and the proposal\ndistribution is the same at each iteration. sometimes improvements can be obtained\nby adaptively restricting neighborhoods at each iteration. for example, it can be use-\nful to shrink the size of the neighborhood as time increases to avoid many wasteful\ngenerations of distant candidates that are very likely to be rejected at such low tem-\nperatures. in other cases, when a penalty function is used in place of constraints, it\nmay be useful to allow only neighborhoods composed of solutions that reduce or\neliminate constraint violations embodied in the current \u03b8.\nit is handy if f can be evaluated quickly for new candidates. we noted pre-\nviously that neighborhood definitions can sometimes enable this, as in the traveling\nsalesman problem where a 2-neighborhood strategy led to a simple updating for-\nmula for f. simple approximation of f is sometimes made, often in a problem-\nspecific manner. at least one author suggests monitoring recent iterates and in-\ntroducing a penalty term in f that discourages revisiting states like those recently\nvisited [201].\nnext consider the acceptance probability given in step 2 of the canonical sim-\nulated annealing algorithm in section 3.3. the expression exp{[f(\u03b8(t)) \u2212 f(\u03b8\u2217)]/\u03c4j}\nis motivated by the boltzmann distribution from statistical thermodynamics. other\nacceptance probabilities can be used, however. the linear taylor series expansion\nof the boltzmann distribution motivates min)1,1 +$+f(\u03b8(t)) \u2212 f(\u03b8\u2217),-\u03c4j%* as a\npossible acceptance probability [352]. to encourage moderate moves away from\n\n "}, {"Page_number": 89, "text": "3.4 genetic algorithms\n\nmin)1,exp)+c + f(\u03b8(t)) \u2212 f(\u03b8\u2217),- \u03c4j**, where c > 0, has been suggested for cer-\n\n75\nlocal minima while preventing excessive small moves, the acceptance probability\ntain problems [169].\nin general, there is little evidence that the shape of the cooling schedule (lin-\near, polynomial, exponential) matters much, as long as the useful range of temper-\natures is covered, the range is traversed at roughly the same rate, and sufficient\ntime is spent at each temperature (especially the low temperatures) [169]. reheat-\ning strategies that allow sporadic, systematic, or interactive temperature increases\nto prevent getting stuck in a local minimum at low temperatures can be effective\n[169, 256, 378].\nafter simulated annealing is complete, one might take the final result of one\nor more runs and polish these with a descent algorithm. in fact, one could refine\noccasionalacceptedstepsinthesameway,insteadofwaitinguntilsimulatedannealing\nhas terminated.\n\n3.4 genetic algorithms\nannealingisnottheonlynaturalprocesssuccessfullyexploitedasametaphortosolve\noptimization problems. genetic algorithms mimic the process of darwinian natural\nselection. candidate solutions to a maximization problem are envisioned as biological\norganismsrepresentedbytheirgeneticcode.thefitnessofanorganismisanalogousto\nthe quality of a candidate solution. breeding among highly fit organisms provides the\nbestopportunitytopassalongdesirableattributestofuturegenerations,whilebreeding\namong less fit organisms (and rare genetic mutations) ensures population diversity.\nover time, the organisms in the population should evolve to become increasingly fit,\nthereby providing a set of increasingly good candidate solutions to the optimization\nproblem. the pioneering development of genetic algorithms was done by holland\n[333]. other useful references include [17, 138, 200, 262, 464, 531, 533, 661].\nwe revert now to our standard description of optimization as maximization,\nwhere we seek the maximum of f(\u03b8) with respect to \u03b8 \u2208 \u0001. in statistical applications\nof genetic algorithms, f is often a joint log profile likelihood function.\n\n3.4.1 definitions and the canonical algorithm\nin example 3.1 above, some genetics terminology\n3.4.1.1 basic definitions\nwas introduced. here we discuss additional terminology needed to study genetic\nalgorithms.\nin a genetic algorithm, every candidate solution corresponds to an individual, or\norganism, and every organism is completely described by its genetic code. individuals\nare assumed to have one chromosome. a chromosome is a sequence of c symbols,\neach of which consists of a single choice from a predetermined alphabet. the most\nbasic alphabet is the binary alphabet, {0,1}, in which case a chromosome of length\nc = 9 might look like 100110001. the c elements of the chromosome are the genes.\nthe values that might be stored in a gene (i.e., the elements of the alphabet) are alleles.\nthe position of a gene in the chromosome is its locus.\n\n "}, {"Page_number": 90, "text": "76\n\n(t)\np .\n\n(t)\n1 , . . . , \u03b8\n\n(t)\ni depends on the corresponding f(\u03b8\n\nchapter 3 combinatorial optimization\ntheinformationencodedinanindividual\u2019schromosomeisitsgenotype.wewill\nrepresent a chromosome or its genotype as \u03d1. the expression of the genotype in the\norganism itself is its phenotype. for optimization problems, phenotypes are candidate\nsolutions and genotypes are encodings: each genotype, \u03d1, encodes a phenotype, \u03b8,\nusing the chosen allele alphabet.\ngeneticalgorithmsareiterative,withiterationsindexedby t.unlikethemethods\npreviously discussed in this chapter, genetic algorithms track more than one candidate\nsolution simultaneously. let the tth generation consist of a collection of p organisms,\n(t)\n(t)\np . this population of size p at generation t corresponds to a collection of\n1 , . . . , \u03d1\n\u03d1\ncandidate solutions, \u03b8\ndarwinian natural selection favors organisms with high fitness. the fitness of\n(t)\nan organism \u03d1\ni ). a high-quality candidate\nsolution has a high value of the objective function and a high fitness. as generations\nprogress, organisms inherit from their parents bits of genetic code that are associated\nwith high fitness if fit parents are predominantly selected for breeding. an offspring\nis a new organism inserted in the (t + 1)th generation to replace a member of the\ntth generation; the offspring\u2019s chromosome is determined from those of two parent\nchromosomes belonging to the tth generation.\nto illustrate some of these ideas, consider a regression model selection problem\nwith 9 predictors. assume that an intercept will be included in any model. the geno-\ntype of any model can then be written as a chromosome of length 9. for example, the\n(t)\nchromosome \u03d1\ni =100110001isagenotypecorrespondingthephenotypeofamodel\ncontaining only the fitted parameters for the intercept and predictors 1, 4, 5, and 9.\nshare some\ncommon genes. a schema is any subcollection of genes. in this example, the two\nchromosomes share the schema 1*01*****, where * represents a wildcard: the\nallele in that locus is ignored. (these two chromosomes also share the schemata\n**01*****, 1*01*0***, and others.) the significance of schemata is that they encode\nmodest bits of genetic information that may be transferred as a unit from parent to\noffspring. if a schema is associated with a phenotypic feature that induces high values\nof the objective function, then the inheritance of this schema by individuals in future\ngenerations promotes optimization.\n\n(t)\nj = 110100110. notice that \u03d1\n\nanother genotype is \u03d1\n\nand \u03d1\n\n(t)\ni\n\n(t)\nj\n\nselection mechanisms and genetic operators breeding drives\n3.4.1.2\nmost genetic change. the process by which parents are chosen to produce offspring\nis called the selection mechanism. one simple approach is to select one parent with\nprobability proportional to fitness and to select the other parent completely at random.\nanother approach is to select each parent independently with probability propor-\ntional to fitness. section 3.4.2.2 describes some of the most frequently used selection\nmechanisms.\nafter two parents from the tth generation have been selected for breeding, their\nchromosomes are combined in some way that allows schemata from each parent to be\ninherited by their offspring, who become members of generation t + 1. the methods\nfor producing offspring chromosomes from chosen parent chromosomes are genetic\noperators.\n\n "}, {"Page_number": 91, "text": "3.4 genetic algorithms\n\n77\n\ngeneration\n\nt\n011\n010\n101\n000\n\nt\n \nt\na\n \ns\ns\ne\nn\nt\ni\nf\n\n011\n0\n01\n011\n01\n\n1\n\n1\n0\n11\n01\n\n01\n01\n1\n0\n\ngeneration\n\n1+\nt\n011\n110\n111\n001\n\nselection\n\ncrossover\n\nmutation\n\nfigure 3.5 an example of generation production in a genetic algorithm for a population of\nsize p = 4 with chromosomes of length c = 3. crossovers are illustrated by boxing portions\nof some chromosomes. mutation is indicated by an underlined gene in the final column.\n\na fundamental genetic operator is crossover. one of the simplest crossover\nmethods is to select a random position between two adjacent loci and split both parent\nchromosomes at this position. glue the left chromosome segment from one parent to\ntherightsegmentfromtheotherparenttoformanoffspringchromosome.theremain-\ning segments can be combined to form a second offspring or discarded. for example,\nsuppose the two parents are 100110001 and 110100110. if the random split point\nis between the third and fourth loci, then the potential offspring are 100100110 and\n110110001. note that in this example, both offspring inherit the schema 1*01*****.\ncrossover is the key to a genetic algorithm\u2014it allows good features of two candidate\nsolutions to be combined. some more complicated crossover operators are discussed\nin section 3.4.2.3.\nmutation is another important genetic operator. mutation changes an offspring\nchromosome by randomly introducing one or more alleles in loci where those alleles\nare not seen in the corresponding loci of either parent chromosome. for example, if\ncrossover produced 100100110 from the parents mentioned above, subsequent mu-\ntation might yield 101100110. note that the third gene was 0 in both parents and\ntherefore crossover alone was guaranteed to retain the schema **0******. muta-\ntion, however, provides a way to escape this constraint, thereby promoting search\ndiversification and providing a way to escape from local maxima.\nmutationisusuallyappliedafterbreeding. inthe simplest implementation,each\ngene has an independent probability, \u00b5, of mutating, and the new allele is chosen\ncompletely at random from the genetic alphabet. if \u00b5 is too low, many potentially\ngood innovations will be missed; if \u00b5 is too high, the algorithm\u2019s ability to learn over\ntime will be degraded, because excessive random variation will disturb the fitness\nselectivity of parents and the inheritance of desirable schemata.\nto summarize, genetic algorithms proceed by producing generations of in-\ndividuals. the (t + 1)th generation is produced as follows. first the individuals in\ngeneration t are ranked and selected according to fitness. then crossover and muta-\ntion are applied to these selected individuals to produce generation t + 1. figure 3.5\nis a small example of the production of a generation of four individuals with three\nchromosomes per individual and binary chromosome encoding. in generation t, in-\ndividual 110 has the highest fitness among its generation and is chosen twice in the\n\n "}, {"Page_number": 92, "text": "78\n\nchapter 3 combinatorial optimization\n\n400\n\n300\n\n200\n\n100\n\ni\n\nc\na\n \ne\nv\ni\nt\na\ng\ne\nn\n\n0\n\n50\n\ngeneration\n\n100\n\nfigure 3.6 results of a genetic algorithm for example 3.5.\n\nselection stage. in the crossover stage, the selected individuals are paired off so that\neach pair recombines to generate two new individuals. in the mutation stage, a low\nmutation rate is applied. in this example, only one mutation occurs. the completion\nof these steps yields the new generation.\nexample 3.5 (baseball salaries, continued) the results of applying a simple\ngenetic algorithm to the variable selection problem for the baseball data introduced\nin example 3.3 are shown in figure 3.6. one hundred generations of size p = 20\nwere used. binary inclusion\u2013exclusion alleles were used for each possible predictor,\nyielding chromosomes of length c = 27. the starting generation consisted of purely\nrandom individuals. a rank-based fitness function was used; see equation (3.9). one\nparent was selected with probability proportional to this fitness; the other parent\nwas selected independently, purely at random. breeding employed simple crossover.\na 1% mutation rate was randomly applied independently to each locus.\nthe horizontal axis in figure 3.6 corresponds to generation. the aic values\nfor all 20 individuals in each generation are plotted. the best model found included\npredictors 2, 3, 6, 8, 10, 13, 14, 15, 16, 24, 25, and 26, yielding an aic of \u2212418.95.\nthis matches the best model found using random starts local search (table 3.2).\ndarwinian survival of the fittest is clearly illustrated in this figure: the 20 random\nstarting individuals rapidly coalesce into 3 effective subspecies, with the best of these\nquickly overwhelming the rest and slowly improving thereafter. the best model was\nfirst found in generation 60.\n!\n3.4.1.3 allele alphabets and genotypic representation the binary alpha-\nbet for alleles was introduced in the pioneering work of holland [333] and continues\nto be very prevalent in recent research. the theoretical behavior of the algorithm and\nthe relative performance of various genetic operators and other algorithmic variations\nare better understood for binary chromosomes than for other choices.\n\n "}, {"Page_number": 93, "text": "79\nfor many optimization problems, it is possible to construct a binary encod-\ning of solutions. for example, consider the univariate optimization of f(\u03b8) = 100 \u2212\n(\u03b8 \u2212 4)2 on the range \u03b8 \u2208 [1,12.999] = [a1, a2]. suppose that we represent a number\nin [a1, a2] as\n\n3.4 genetic algorithms\n\na1 +/ a2 \u2212 a1\n\n2d \u2212 10decimal(b),\n\n(3.7)\n\nwhere b is a binary number of d digits and the decimal() function converts from base\n2 to base 10. if c decimal places of accuracy are required, then d must be chosen to\nsatisfy\n\n(a2 \u2212 a1)10c \u2264 2d \u2212 1.\n\n(3.8)\nin our example, 14 binary digits are required for accuracy to 3 decimal places, and\nb = 01000000000000 maps to \u03b8 = 4.000 using equation (3.7).\ninsomecases,suchastheregressionmodelselectionproblem,abinary-encoded\nchromosome may be very natural. in others, however, the encoding seems forced,\nas it does above. for f(\u03b8) = 100 \u2212 (\u03b8 \u2212 4)2, the chromosome \u03d1 = 01000000000000\n(\u03b8 = 4.000) is optimal. however, chromosomes that are genetically close to this, such\nas10000000000000(\u03b8 = 7.000)and00000000000000(\u03b8 = 1.000),havephenotypes\nthat are not close to \u03b8 = 4.000. on the other hand, the genotype 00111111111111\nhas phenotype very close to 4.000 even though the genotype is very different than\n01000000000000. chromosomes that are similar in genotype may have very different\nphenotypes. thus, a small mutation may move to a drastically different region of\nsolution space, and a crossover may produce offspring whose phenotypes bear little\nresemblance to either parent. to resolve such difficulties, a different encoding scheme\nor modified genetic operators may be required (see section 3.4.2.3).\nanimportantalternativetobinaryrepresentationarisesinpermutationproblems\nofsize p,likethetravelingsalesmanproblem.insuchcases,anaturalchromosomeisa\npermutationoftheintegers1, . . . , p,forexample, \u03d1 =752631948when p = 9.since\nsuch chromosomes must obey the requirement that each integer appear in exactly one\nlocus, some changes to standard genetic operators will be required. strategies for\ndealing with permutation chromosomes are discussed in section 3.4.2.3.\n\ninitialization, termination, and parameter values genetic algo-\n3.4.1.4\nrithms are usually initialized with a first generation of purely random individuals.\nthe size of the generation, p, affects the speed, convergence behavior, and\nsolution quality of the algorithm. large values of p are to be preferred, if feasible,\nbecause they provide a more diverse genetic pool from which to generate offspring,\nthereby diversifying the search and discouraging premature convergence. for binary\nencoding of chromosomes, one suggestion is to choose p to satisfy c \u2264 p \u2264 2c,\nwhere c is the chromosome length [8]. for permutation chromosomes, the range\n2c \u2264 p \u2264 20c has been suggested [335]. in most real applications, population sizes\nhaverangedbetween10and200[566],althoughareviewofempiricalstudiessuggests\nthat p can often be as small as 30 [531].\n\n "}, {"Page_number": 94, "text": "80\n\nchapter 3 combinatorial optimization\nmutation rates are typically very low, in the neighborhood of 1%. theoretical\nwork and empirical studies have supported a rate of 1/c [464], and another inves-\ntigation suggested that the rate should be nearly proportional to 1/(p\u221ac) [571].\nnevertheless, a fixed rate independent of p and c is a common choice.\nthe termination criterion for a genetic algorithm is frequently just a maximum\nnumber of iterations chosen to limit computing time. one might instead consider\nstopping when the genetic diversity within chromosomes in the current generation is\nsufficiently low [17].\n\n3.4.2 variations\nin this section we survey a number of methodological variations that may offer\nimproved performance. these include alterations to the fitness function, selection\nmechanism, genetic operators, and other aspects of the basic algorithm.\n\nfitness\n\nin a canonical genetic algorithm, the fitness of an organism is\n3.4.2.1\noften taken to be the objective function value of its phenotype, perhaps scaled by the\nmean objective function value in its generation. it is tempting to simply equate the\nobjective function value f(\u03b8) to the fitness because the fittest individual then corre-\nspondstothemaximumlikelihoodsolution.however,directlyequatinganorganism\u2019s\nfitness to the objective function value for its corresponding phenotype is usually naive\nin that other choices yield superior optimization performance. instead, let \u03c6(\u03d1) denote\nthe value of a fitness function that describes the fitness of a chromosome. the fitness\nfunction will depend on the objective function f, but will not equal it. this increased\nflexibility can be exploited to enhance search effectiveness.\na problem seen in some applications of genetic algorithms is excessively fast\nconvergence to a poor local optimum. this can occur when a few of the very best\nindividualsdominatethebreedingandtheiroffspringsaturatesubsequentgenerations.\nin this case, each subsequent generation consists of genetically similar individuals\nthat lack the genetic diversity needed to produce offspring that might typify other,\nmore profitable regions of solution space. this problem is especially troublesome if it\noccurs directly after initialization, when nearly all individuals have very low fitness.\na few chromosomes that are more fit than the rest can then pull the algorithm to\nan unfavorable local maximum. this problem is analogous to entrapment near an\nuncompetitive local maximum, which is also a concern for the other search methods\ndiscussed earlier in this chapter.\nselective pressure must be balanced carefully, however, because genetic algo-\nrithms can be slow to find a very good optimum. it is therefore important to maintain\nfirm selective pressure without allowing a few individuals to cause premature conver-\ngence. to do this, the fitness function can be designed to reduce the impact of large\nvariations in f.\n(t)\ni ) and use only their ranks\n[18, 532, 660]. for example, one could set\n\na common approach is to ignore the values of f(\u03b8\n\n\u03c6(\u03d1\n\n(t)\ni ) =\n\n2ri\n\np(p + 1) ,\n\n(3.9)\n\n "}, {"Page_number": 95, "text": "3.4 genetic algorithms\n\n81\n\n(t)\nwhere ri is the rank of f(\u03b8\ni ) among generation t. this strategy gives the chromosome\ncorresponding to the median quality candidate a selection probability of 1/p, and the\nbest chromosome has probability 2/(p + 1), roughly double that for the median.\nrank-based methods are attractive in that they retain a key feature of any successful\ngenetic algorithm\u2014selectivity based on relative fitness\u2014while discouraging prema-\nture convergence and other difficulties caused by the actual form of f, which can be\nsomewhat arbitrary [660]. some less common fitness function formulations involving\nscaling and transforming f are mentioned in [262].\n\nselection mechanisms and updating generations previously, in\n3.4.2.2\nsection 3.4.1.2, we mentioned only simple approaches to selecting parents on the\nbasis of fitness. selecting parents on the basis of fitness ranks (section 3.4.2.1) is far\nmore common than using selection probabilities proportional to fitness.\nanother common approach is tournament selection [204, 263, 264]. in this\napproach, the set of chromosomes in generation t is randomly partitioned into k dis-\njoint subsets of equal size (perhaps with a few remaining chromosomes temporarily\nignored). the best individual in each group is chosen as a parent. additional random\npartitionings are carried out until sufficient parents have been generated. parents are\nthen paired randomly for breeding. this approach ensures that the best individual\nwill breed p times, the median individual will breed once on average, and the worst\nindividual will not breed at all. the approaches of proportional selection, ranking, and\ntournament selection apply increasing selective pressure, in that order. higher selec-\ntive pressure is generally associated with superior performance, as long as premature\nentrapment in local optima can be avoided [17].\npopulations can be partially updated. the generation gap, g, is a proportion of\nthe generation to be replaced by generated offspring [146]. thus, g = 1 corresponds\nto a canonical genetic algorithm with distinct, nonoverlapping generations. at the\nother extreme, g = 1/p corresponds to incremental updating of the population one\noffspringatatime.inthiscase,asteady-stategeneticalgorithmproducesoneoffspring\nat a time to replace the least fit (or some random relatively unfit) individual [661].\nsuch a process typically exhibits more variance and higher selective pressure than a\nstandard approach.\nwhen g < 1, performance can sometimes be enhanced with a selection mech-\nanism that departs somewhat from the darwinian analogy. for example, an elitist\nstrategy would place an exact copy of the current fittest individual in the next genera-\ntion, thereby ensuring the survival of the best current solution [146]. when g = 1/p,\neachoffspringcouldreplaceachromosomerandomlyselectedfromthosewithbelow-\naverage fitness [5].\ndeterministic selection strategies have been proposed to eliminate sampling\nvariability [19, 464]. we see no compelling need to eliminate the randomness inherent\nin the selection mechanism.\none important consideration when generating or updating a population is\nwhether to allow duplicate individuals in the population. dealing with dupli-\ncate individuals wastes computing resources, and it potentially distorts the par-\nent selection criterion by giving duplicated chromosomes more chances to produce\noffspring [138].\n\n "}, {"Page_number": 96, "text": "chapter 3 combinatorial optimization\n\n82\n3.4.2.3 genetic operators and permutation chromosomes to increase\ngenetic mixing, it is possible to choose more than one crossover point. if two\ncrossover points are chosen, the gene sequence between them can be swapped be-\ntweenparentstocreateoffspring.suchmultipointcrossovercanimproveperformance\n[54, 187].\nmany other approaches for transferring genes from parents to offspring have\nbeen suggested. for example, each offspring gene could be filled with an allele ran-\ndomly selected from the alleles expressed in that position in the parents. in this case,\nthe parental origins of adjacent genes could be independent [4, 622] or correlated\n[602], with strength of correlation controlling the degree to which offspring resemble\na single parent.\nin some problems, a different allele alphabet may be more reasonable. allele\nalphabets with many more than two elements have been investigated [13, 138, 524,\n534]. for some problems, genetic algorithms using a floating-point alphabet have\noutperformed algorithms using the binary alphabet [138, 346, 463]. methods known\nas messy genetic algorithms employ variable-length encoding with genetic operators\nthat adapt to changing length [265\u2013267]. gray coding is another alternative encoding\nthat is particularly useful for real-valued objective functions that have a bounded\nnumber of optima [662].\nwhen a nonbinary allele alphabet is adopted, modifications to other aspects of\nthe genetic algorithm, particularly to the genetic operators, is often necessary and\neven fruitful. nowhere is this more evident than when permutation chromosomes\nare used. recall that section 3.4.1.3 introduced a special chromosome encoding for\npermutation optimization problems. for such problems (like the traveling salesman\nproblem),itisnaturaltowriteachromosomeasapermutationoftheintegers1, . . . , n.\nnew genetic operators are needed then to ensure that each generation contains only\nvalid permutation chromosomes.\nfor example, let p = 9, and consider the crossover operator. from two par-\nent chromosomes 752631948 and 912386754 and a crossover point between the\nsecond and third loci, standard crossover would produce offspring 752386754 and\n912631948.bothoftheseareinvalidpermutationchromosomes,becausebothcontain\nsome duplicate alleles.\na remedy is order crossover [623]. a random collection of loci is chosen, and\nthe order in which the alleles in these loci appear in one parent is imposed on the\nsame alleles in the other parent to produce one offspring. the roles of the parents can\nbe switched to produce a second offspring. this operator attempts to respect relative\npositions of alleles. for example, consider the parents 752631948 and 912386754,\nand suppose that the fourth, sixth, and seventh loci are randomly chosen. in the first\nparent, the alleles in these loci are 6, 1, and 9. we must rearrange the 6, 1, and 9 alleles\nin the second parent to impose this order. the remaining alleles in the second parent\nare **238*754. inserting 6, 1, and 9 in this order yields 612389754 as the offspring.\nreversing the roles of the parents yields a second offspring 352671948.\nmany other crossover operators for permutation chromosomes have been pro-\nposed[135,136,138,268,464,492,587].mostarefocusedonthepositionsofindivid-\nual genes. however, for problems like the traveling salesman problem, such operators\nhave the undesirable tendency to destroy links between cities in the parent tours. the\n\n "}, {"Page_number": 97, "text": "3.4 genetic algorithms\n\n83\n\ntable 3.3 edge tables showing the cities linked to or from each allele in either parent\nfor each of the first three steps of edge recombination crossover. beneath each column\nis the offspring chromosome resulting from each step.\n\ncity\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\nstep 1\n\nlinks\n3, 9, 2\n5, 6, 1, 3\n6, 1, 2, 8\n9, 8, 5\n7, 2, 4\n2, 3, 8, 7\n8, 5, 6\n4, 7, 3, 6\n1, 4\n9********\n\ncity\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\nstep 2\n\nlinks\n3, 2\n5, 6, 1, 3\n6, 1, 2, 8\n8, 5\n7, 2, 4\n2, 3, 8, 7\n8, 5, 6\n4, 7, 3, 6\nused\n94*******\n\nstep 3\n\nlinks\n3, 2\n5, 6, 1, 3\n6, 1, 2, 8\nused\n7, 2\n2, 3, 8, 7\n8, 5, 6\n7, 3, 6\nused\n\ncity\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n945******\n\ndesirability of a candidate solution is a direct function of these links. breaking links\nis effectively an unintentional source of mutation. edge-recombination crossover has\nbeen proposed to produce offspring that contain only links present in at least one\nparent [663, 664].\nwe use the traveling salesman problem to explain edge-recombination\ncrossover. the operator proceeds through the following steps.\n\n1. we first construct an edge table that stores all the links that lead into and out\nof each city in either parent. for our two parents, 752631948 and 912386754,\nthe result is shown in the leftmost portion of table 3.3. note that the number\nof links into and out of each city in either parent will always be at least two\nand no more than four. also, recall that a tour returns to its starting city, so, for\nexample, the first parent justifies listing 7 as a link from 8.\n\n2. to begin creating an offspring, we choose between the initial cities of the two\nparents.inourexample,thechoicesarecities7and9.iftheparents\u2019initialcities\nhave the same number of links, then the choice is made randomly. otherwise,\nchoose the initial city from the parent whose initial city has fewer links. in our\nexample, this yields 9********.\n3. we must now link onward from allele 9. from the leftmost column of the edge\ntable, we find that allele 9 has two links: 1 and 4. we want to chose between\nthese by selecting the city with the fewest links. to do this, we first update\nthe edge table by deleting all references to allele 9, yielding the center portion\nof table 3.3. since cities 1 and 4 both have two remaining links, we choose\nrandomly between 1 and 4. if 4 is the choice, then the offspring is updated to\n94*******.\n4. there are two possible links onward from city 4: cities 5 and 8. updating the\nedge table to produce the rightmost portion of table 3.3, we find that city 5 has\n\n "}, {"Page_number": 98, "text": "84\n\nchapter 3 combinatorial optimization\nthe fewest remaining links. therefore, we choose city 5. the partial offspring\nis now 945******.\n\ncontinuing this process might yield the offspring 945786312 by the following steps:\nselect 7; select 8; select 6; randomly select 3 from the choices of 2 and 3; randomly\nselect 1 from the choices of 1 and 2; select 2.\nnote that in each step a city is chosen among those with the fewest links. if,\ninstead, links were chosen uniformly at random, cities would be more likely to be left\nwithout a continuing edge. since tours are circuital, the preference for a city with few\nlinks does not introduce any sort of bias in offspring generation.\nan alternative edge assembly strategy has been found to be extremely effective\nin some problems [477].\nmutation of permutation chromosomes is not as difficult as crossover. a simple\nmutation operator is to randomly exchange two genes in the chromosome [531].\nalternatively, the elements in a short random segment of the chromosome can be\nrandomly permuted [138].\n\ninitialization and parameter values\n\n3.4.3\nalthough traditionally a genetic algorithm is initiated with a generation of purely\nrandom individuals, heuristic approaches to constructing individuals with good or\ndiverse fitness have been suggested as an improvement on random starts [138, 531].\nequal sizes for subsequent generations are not required. population fitness usu-\nally improves very rapidly during the early generations of a genetic algorithm. in\norder to discourage premature convergence and promote search diversity, it may be\ndesirable to use a somewhat large generation size p for early generations. if p is\nfixed at too large a value, however, the entire algorithm may be too slow for practical\nuse. once the algorithm has made significant progress toward the optimum, important\nimproving moves most often come from high-quality individuals; low-quality indi-\nviduals are increasingly marginalized. therefore, it has been suggested that p may be\ndecreased progressively as iterations continue [677]. however, rank-based selection\nmechanisms are more commonly employed as an effective way to slow convergence.\nit can be also useful to allow a variable mutation rate that is inversely propor-\ntional to the population diversity [531]. this provides a stimulus to promote search\ndiversity as generations become less diverse. several authors suggest other methods\nfor allowing the probabilities of mutation and crossover and other parameters of the\ngenetic algorithm to vary adaptively over time in manners that may encourage search\ndiversity [54, 137, 138, 464].\n\n3.4.4 convergence\ntheconvergencepropertiesofgeneticalgorithmsarebeyondthescopeofthischapter,\nbut several important ideas are worth mentioning.\nmuch of the early analysis about why genetic algorithms work was based on the\nnotion of schemata [262, 333]. such work is based on a canonical genetic algorithm\nwith binary chromosome encoding, selection of each parent with probability propor-\ntional to fitness, simple crossover applied every time parents are paired, and mutation\n\n "}, {"Page_number": 99, "text": "3.5 tabu algorithms\n\n85\nrandomly applied to each gene independently with probability \u00b5. for this setting, the\nschema theorem provides a lower bound on the expected number of instances of a\nschema in generation t + 1, given that it was present in generation t.\nthe schema theorem shows that a short, low-order schema (i.e., one specify-\ning only a few nearby alleles) will enjoy increased expected representation in the\nnext generation if the average fitness of chromosomes containing that schema in the\ngeneration at time t exceeds the average fitness of all chromosomes in the genera-\ntion. a longer and/or more complex schema will require higher relative fitness to\nhave the same expectation. proponents of schema theory argue that convergence to\nglobally competitive candidate solutions can be explained by how genetic algorithms\nsimultaneously juxtapose many short low-order schemata of potentially high fitness,\nthereby promoting propagation of advantageous schemata.\nmore recently, the schema theorem and convergence arguments based upon it\nhave become more controversial. traditional emphasis on the number of instances of\na schema that propagate to the next generation and on the average fitness of chromo-\nsomescontainingthatschemaissomewhatmisguided.whatmattersfarmoreiswhich\nparticular chromosomes containing that schema are propagated. further, the schema\ntheorem overemphasizes the importance of schemata: in fact it applies equally well\nto any arbitrary subsets of \u0001. finally, the notion that genetic algorithms succeed be-\ncause they implicitly simultaneously allocate search effort to many schemata-defined\nregions of \u0001 has been substantially discredited [647]. an authoritative exposition of\nthe mathematical theory of genetic algorithms is given by vose [646]. other helpful\ntreatments include [200, 533].\ngenetic algorithms are not the only optimization strategy that can be motivated\nby analogy to a complex biological system. for example, particle swarm optimization\nalso creates and updates a population of candidate solutions [372, 373, 594]. the lo-\ncations of these solutions within the search space evolve through simple rules that can\nbe viewed as reflecting cooperation and competition between individuals analogous\nto the movement of birds in a flock. over a sequence of iterations, each individual\nadjusts its location (i.e., candidate solution) based on its own flying experience and\nthose of its companions.\n\n3.5 tabu algorithms\na tabu algorithm is a local search algorithm with a set of additional rules that guide\nthe selection of moves in ways that are believed to promote the discovery of a global\nmaximum. the approach employs variable neighborhoods: the rules for identifying\nacceptable steps change at each iteration. detailed studies of tabu methods include\n[254, 255, 257\u2013259].\nin a standard ascent algorithm, entrapment in a globally uncompetitive local\nmaximum is likely, because no downhill moves are allowed. tabu search allows\ndownhill moves when no uphill move can be found in the current neighborhood\n(and possibly in other situations too), thereby potentially escaping entrapment. an\nearly form of tabu search, called steepest ascent/mildest descent, moved to the least\nunfavorable neighbor when there was no uphill move [306].\n\n "}, {"Page_number": 100, "text": "86\n\nchapter 3 combinatorial optimization\n\ntable 3.4 examples of attributes. the left column gives examples in a generic context. the right\ncolumn gives corresponding attributes in the specific context of 2-change neighborhoods in a\nregression model selection problem.\nattribute\na change in the value of \u03b8\n\nmodel selection example\na1: whether the ith predictor is added (or\n\ndeleted) from the model.\n\n(t)\ni\n\n. the attribute\nmay be the value from which the move\nbegan, or the value at which it arrived.\n(t)\nj when\n\na swap in the values of \u03b8\n\n(t)\ni and \u03b8\n\n(t)\ni\n\n\u03b8\n\n(t)\nj .\n\n/= \u03b8\n\na change in the value of f resulting from the\nstep, f(\u03b8(t+1)) \u2212 f(\u03b8(t)).\nthe value g(\u03b8(t+1)) of some other strategically\nchosen function g.\na change in the value of g resulting from the\nstep, g(\u03b8(t+1)) \u2212 g(\u03b8(t)).\n\na2: whether the absent variable is exchanged for\n\nthe variable present in the model.\n\na3: the reduction in aic achieved by the move.\n\na4: the number of predictors in the new model.\n\na5: a change to a different variable selection\ncriterion such as mallows\u2019s cp [435] or the\nadjusted r2 [483].\n\nif a downhill step is chosen, care must be taken to ensure that the next step (or a\nfuture one) does not simply reverse the downhill move. such cycling would eliminate\nthe potential long-term benefit of the downhill move. to prevent such cycling, certain\nmoves are temporarily forbidden, or made tabu, based on the recent history of the\nalgorithm.\nthere are four general types of rules added to local search by tabu search\nmethods. the first is to make certain potential moves temporarily tabu. the others\ninvolve aspiration to better solutions, intensification of search in promising areas of\nsolutionspace,anddiversificationofsearchcandidatestopromotebroaderexploration\nof the solution space. these terms will be defined after we discuss tabus.\n\n3.5.1 basic definitions\ntabu search is an iterative algorithm initiated at time t = 0 with a candidate solution\n\u03b8(0). at the tth iteration, a new candidate solution is selected from a neighborhood\nof \u03b8(t). this candidate becomes \u03b8(t+1). let h(t) denote the history of the algorithm\nthrough time t. it suffices for h(t) to be a selective history, remembering only certain\nmatters necessary for the future operation of the algorithm.\nunlike simple local search, a tabu algorithm generates a neighborhood of\nthe current candidate solution that depends on the search history; denote this by\nn(\u03b8(t), h(t)). furthermore, the identification of the preferred \u03b8(t+1) in n(\u03b8(t), h(t))\nmay depend not only on f but also on the search history. thus, we may assess neigh-\nbors using an augmented objective function, fh(t).\na single step from \u03b8(t) to \u03b8(t+1) can be characterized by many attributes.\nattributes will be used to describe moves or types of moves that will be forbid-\nden, encouraged, or discouraged in future iterations of the algorithm. examples of\nattributes are given in the left column of table 3.4. such attributes are not unique to\ntabu search; indeed, they can be used to characterize moves from any local search.\n\n "}, {"Page_number": 101, "text": "3.5 tabu algorithms\n\n87\nhowever, tabu search explicitly adapts the current neighborhood according to the\nattributes of recent moves.\nthe attributes in table 3.4 can be illustrated by considering a regression model\n(t)\nselection problem. suppose \u03b8\ni = 1 if the ith predictor is included in the model\nat time t, and 0 otherwise. suppose that 2-change neighborhoods consist of all\nmodels to which two variables separately have each been added or deleted from the\ncurrent model. the right column of table 3.4 gives one example of each generic\nattribute listed, in the context of these 2-change neighborhoods in the regression\nmodel selection problem from example 3.2. these examples are labeled a1 through\na5. many other effective attributes can be identified from the context of specific\noptimization problems.\ndenote the ath attribute as aa. note that the complement (i.e., negation) of an\n(t)\nattribute is also an attribute, so if aa corresponds to swapping the values of \u03b8\ni and\n(t+1)\n, then aa corresponds to not making that swap.\n\u03b8\nj\nas the algorithm progresses, the attributes of the tth move will vary with t, and\nthe quality of the candidate solution will also vary. future moves can be guided by\nthe history of past moves, their objective function values, and their attributes. the\nrecency of an attribute is the number of steps that have passed since a move most\n\nrecently had that attribute. let r$aa, h(t)% = 0 if the ath attribute is expressed in the\nmove yielding \u03b8(t), let r$aa, h(t)% = 1 if it is most recently expressed in the move\n\nyielding \u03b8(t\u22121), and so forth.\n\n3.5.2 the tabu list\nwhen considering a move from \u03b8(t), we compute the increase in the objective function\nachieved for each neighbor of \u03b8(t). ordinarily, the neighbor that provides the greatest\nincrease would be adopted as \u03b8(t+1). this corresponds to the steepest ascent.\nsuppose, however, that no neighbor of \u03b8(t) yields an increased objective func-\ntion. then \u03b8(t+1) is ordinarily chosen to be the neighbor that provides the smallest\ndecrease. this is the mildest descent.\nif only these two rules were used for search, the algorithm would quickly\nbecometrappedandconvergetoalocalmaximum. afteronemoveofmildestdescent,\nthe next move would return to the hilltop just departed. cycling would ensue.\nto avoid such cycling, a tabu list of temporarily forbidden moves is incorpo-\nrated in the algorithm. each time a move with attribute aa is taken, aa is put on\naa is removed from the tabu list. thus, moves with attributes on the tabu list are\neffectively excluded from the current neighborhood. the modified neighborhood is\ndenoted\n\na tabu list for \u03c4 iterations. when r$aa, h(t)% first equals \u03c4, the tabu expires and\n\nn(\u03b8(t), h(t)) =#\u03b8 : \u03b8 \u2208 n(\u03b8(t)) and no attribute of \u03b8 is currently tabu& .\n\n(3.10)\n\nthis prevents undoing the change for \u03c4 iterations, thereby discouraging cycling. by\nthe time that the tabu has expired, enough other aspects of the candidate solution\n\n "}, {"Page_number": 102, "text": "chapter 3 combinatorial optimization\n\n88\nshould have changed that reversing the move may no longer be counterproductive.\nnote that the tabu list is a list of attributes, not moves, so a single tabu attribute may\nforbid entire classes of moves.\nthe tabu tenure, \u03c4, is the number of iterations over which an attribute is tabu.\nthis can be a fixed number or it may vary, systematically or randomly, perhaps based\non features of the attribute. for a given problem, a well-chosen tabu tenure will\nbe long enough to prevent cycling and short enough to prevent the deterioration of\ncandidate solution quality that occurs when too many moves are forbidden. fixed\ntabu tenures between 7 and 20, or between 0.5\u221ap and 2\u221ap, where p is the size of\nthe problem, have been suggested for various problem types [257]. tabu tenures that\nvary dynamically seem more effective in many problems [259]. also, it will often be\nimportant to use different tenures for different attributes. if an attribute contributes\ntabu restrictions for a wide variety of moves, the corresponding tabu tenure should\nbe short to ensure that future choices are not limited.\n\nexample 3.6 (genetic mapping, continued) we illustrate some uses of tabus,\nusing the gene mapping problem introduced in example 3.1.\nfirst, consider monitoring the swap attribute. suppose that aa is the swap\nattribute corresponding to exchanging two particular loci along the chromosome.\nwhen a move aa is taken, it is counterproductive to immediately undo the swap, so\naa is placed on the tabu list. search progresses only among moves that do not reverse\nrecent swaps. such a tabu promotes search diversity by avoiding quick returns to\nrecently searched areas.\nsecond,considertheattributeidentifyingthelocuslabel \u03b8j forwhich \u02c6d(\u03b8j, \u03b8j+1)\nis smallest in the new move. in other words, this attribute identifies the two loci in\nthe new chromosome that are nearest each other. if the complement of this attribute\nis put on the tabu list, any move to a chromosome for which other loci are closer will\nbe forbidden moves for \u03c4 iterations. such a tabu promotes search intensity among\ngenetic maps for which loci \u03b8j and \u03b8j are closest.\nsometimes, it may be reasonable to place the attribute itself, rather than its\ncomplement, on the tabu list. for example, let h(\u03b8) compute the mean \u02c6d(\u03b8j, \u03b8j+1)\nbetween adjacent loci in a chromosome ordered by \u03b8. let aa be the attribute indicat-\ning excessive change of the mean conditional mle map distance, so aa equals 1 if\n!!h(\u03b8(t+1)) \u2212 h(\u03b8(t))!! > c and 0 otherwise, for some fixed threshold c. if a move with\nmean change greater than c is taken, we may place aa itself on the tabu list for \u03c4\niterations. this prevents any other drastic mean changes for a period of time, allowing\nbetter exploration of the newly entered region of solution space before moving\nfar away.\n!\n\n3.5.3 aspiration criteria\nsometimes, choosing not to move to a nearby candidate solution because the move is\ncurrently tabu can be a poor decision. in these cases, we need a mechanism to override\nthe tabu list. such a mechanism is called an aspiration criterion.\n\n "}, {"Page_number": 103, "text": "3.5 tabu algorithms\n\n89\na simple and popular aspiration criterion is to permit a tabu move if it provides\na higher value of the objective function than has been found in any iteration so far.\nclearly it makes no sense to overlook the best solution found so far, even if it is\ncurrently tabu. one can easily envision scenarios where this aspiration criterion is\nuseful. for example, suppose that a swap of two components of \u03b8 is on the tabu\nlist and the candidate solutions at each iteration recently have drifted away from the\nregion of solution space being explored when the tabu began. the search will now\nbe in a new region of solution space where it is quite possible that reversing the tabu\nswap would lead to a drastic increase in the objective function.\nanother interesting option is aspiration by influence. a move or attribute is\ninfluential if it is associated with a large change in the value of the objective function.\nthere are many ways to make this idea concrete [257]. to avoid unnecessary detail\nabout numerous specific possibilities, let us simply denote the influence of the ath\nthere are a lot of neighboring moves that cause only small incremental changes to the\nvalue of the objective function, while there are a few moves that cause major shifts.\nknowing the attributes of such moves can help guide search. aspiration by influence\noverrides the tabu on reversing a low-influence move if a high-influence move is\nmade prior to the reversal. the rationale for this is that the recent high-influence step\nmay have moved the search to a new region of the solution space where further local\nexploration is useful. the reversal of the low-influence move will probably not induce\ncycling, since the intervening high-influence move likely shifted scrutiny to a portion\nof solution space more distant than what could be reached by the low-influence\nreversal.\naspiration criteria can also be used to encourage moves that are not tabu. for\nexample, when low-influence moves appear to provide only negligible improvement\nin the objective function, they can be downweighted and high-influence moves can\nbe given preference. there are several ways to do this; one approach is to incorporate\nin fh(t) either a penalty or an incentive term that depends on the relative influence of\ncandidate moves.\n\nattribute as i$aa, h(t)% for a move yielding \u03b8(t). in many combinatorial problems,\n\n3.5.4 diversification\nan important component of any search is to ensure that the search is broad enough.\nrules based on how often attributes are observed during search can be used to increase\nthe diversity of candidate solutions examined during tabu search.\nthe frequency of an attribute records the number of moves that manifested that\nattribute since the search began. let c(aa, h(t)) represent the count of occurrences\nof the ath attribute thus far. then f(aa, h(t)) represents a frequency function that can\nbe used to penalize moves that are repeated too frequently. the most direct definition\nis f(aa, h(t)) = c(aa, h(t))/t, but the denominator may be replaced by the sum,\nthe maximum, or the average of the counts of occurrences of various attributes.\nsupposethefrequencyofeachattributeisrecorded,eitherovertheentirehistory\nor over the most recent \u03c8 moves. note that this frequency may be one of two types,\ndepending on the attribute considered. if the attribute corresponds to some feature\n\n "}, {"Page_number": 104, "text": "chapter 3 combinatorial optimization\n\n90\nof \u03b8(t), then the frequency measures how often that feature is seen in candidate solu-\ntions considered during search. such frequencies are termed residence frequencies.\nif, alternatively, the attribute corresponds to some change induced by moving from\none candidate solution to another, then the frequency is a transition frequency. for\nexample, in the regression model selection problem introduced in example 3.2, the\nattribute noting the inclusion of the predictor xi in the model would have a residence\nfrequency. the attribute that signaled when a move reduced the aic would have a\ntransition frequency.\nif attribute aa has a high residence frequency and the history of the most recent\n\u03c8 moves covers nearly optimal regions of solution space, this may suggest that aa\nis associated with high-quality solutions. on the other hand, if the recent history\nreflects the search getting stuck in a low-quality region of solution space, then a high\nresidence frequency may suggest that the attribute is associated with bad solutions.\nusually, \u03c8 > \u03c4 is an intermediate or long-term memory parameter that allows the\naccumulation of additional historical information to diversify future search.\nifattribute aa hasahightransitionfrequency,thisattributemaybewhathasbeen\ntermed a crack filler. such an attribute may be frequently visited during the search\nin order to fine-tune good solutions but rarely offers fundamental improvement or\nchange [257]. in this case, the attribute has low influence.\na direct approach employing frequency to increase search diversification is to\nincorporate a penalty or incentive function in fh(t). the choice\n\nfh(t)(\u03b8(t+1)) =. f(\u03b8(t+1))\n\nf(\u03b8(t+1)) \u2212 cf$aa, h(t)%\n\nif f(\u03b8(t+1)) \u2265 f(\u03b8(t)),\nif f(\u03b8(t+1)) < f(\u03b8(t))\n\n(3.11)\n\nwith c > 0 has been suggested [566]. if all nontabu moves are downhill, then this\napproach discourages moves that have the high-frequency attribute aa. an analogous\nstrategy can be crafted to diversify the selection of uphill moves.\ninstead of incorporating a penalty or incentive in the objective function, it is\npossible to employ a notion of graduated tabu status, where an attribute may be only\npartially tabu. one way to create a tabu status that varies by degrees is to invoke\nprobabilistic tabu decisions: an attribute can be assigned a probability of being tabu,\nwhere the probability is adjusted according to various factors, including the tabu\ntenure [257].\n\nintensification\n\n3.5.5\nin some searches it may be useful to intensify the search effort in particular areas of\nsolution space. frequencies can also be used to guide such intensification. suppose\nthat the frequencies of attributes are tabulated over the most recent \u03c5 moves, and a\ncorresponding record of objective function values is kept. by examining these data,\nkey attributes shared by good candidate solutions can be identified. then moves that\nretain such features can be rewarded and moves that remove such features can be\npenalized through fh(t). the time span \u03c5 > \u03c4 parameterizes the length of a long-term\nmemory to enable search intensification in promising areas of solution space.\n\n "}, {"Page_number": 105, "text": "by fh(t).\n\n3.5 tabu algorithms\n\n91\n\n3.5.6 comprehensive tabu algorithm\nbelow we summarize a fairly general tabu algorithm that incorporates many of the\nfeatures described above. after initialization and identification of a list of problem-\nspecific attributes, the algorithm proceeds as follows:\n\n1. determineanaugmentedobjectivefunction fh(t) thatdependson f andperhaps\n\non\na. frequency-based penalties or incentives to promote diversification, and/or\nb. frequency-based penalties or incentives to promote intensification.\n2. identify neighbors of \u03b8(t), namely the members of n(\u03b8(t)).\n3. rank the neighbors in decreasing order of improvement, as evaluated\n\n4. select the highest ranking neighbor.\n5. is this neighbor currently on the tabu list? if not, go to step 8.\n6. does this neighbor pass an aspiration criterion? if so, go to step 8.\n7. if all neighbors of \u03b8(t) have been considered and none have been adopted as\n\u03b8(t+1), then stop. otherwise, select the next most high-ranking neighbor and go\nto step 5.\n\n8. adopt this solution as \u03b8(t+1).\n9. update the tabu list by creating new tabus based on the current move and by\n\ndeleting tabus whose tenures have expired.\n\n10. has a stopping criterion been met? if so, stop. otherwise, increment t and go\n\nto step 1.\n\nit is sensible to stop when a maximum number of iterations has been reached, and\nthen to take the best candidate solution yet found as the final result. search effort can\nbe split among a collection of random starts rather than devoting all resources to one\nrun from a single start. by casting tabu search in a markov chain framework, it is\npossible to obtain results on the limiting convergence of the approach [191].\n\nexample 3.7 (baseball salaries, continued) a simple tabu search was applied to\nthe variable selection problem for regression modeling of the baseball data introduced\nin example 3.3. only attributes signaling the presence or absence of each predictor\nwere monitored. moves that would reverse the inclusion or removal of a predictor\nwere made tabu for \u03c4 = 5 moves, and the algorithm was run for 75 moves from a\nrandom start. the aspiration criterion permitted an otherwise tabu move if it yielded\nan objective function value above the best previously seen.\nfigure 3.7 shows the values of the aic for the sequence of candidate solutions\ngenerated by this tabu search. the aic was quickly improved, and an optimum value\nof \u2212418.95, derived from the model using predictors 2, 3, 6, 8, 10, 13, 14, 15, 16, 24,\n25, and 26, was found on two occasions: iterations 29 and 43. this solution matches\nthe best model found using random starts local search (table 3.2).\n!\n\n "}, {"Page_number": 106, "text": "92\n\nchapter 3 combinatorial optimization\n\ni\n\nc\na\n \ne\nv\ni\nt\na\ng\ne\nn\n\n420\n\n400\n\n380\n\n360\n\n0\n\n20\n\n40\niteration\n\n60\n\nfigure 3.7 results of tabu search for example 3.7. only aic values between \u2212360 and\n\u2212420 are shown.\n\nproblems\nthe baseball data introduced in section 3.3 are available from the website for this book.\nproblems 3.1\u20133.4 explore the implications of various algorithm configurations. treat these\nproblems in the spirit of experiments, trying to identify settings where interesting differences\ncan be observed. increase the run lengths from those used above to suit the speed of your\ncomputer,andlimitthetotalnumberofobjectivefunctionevaluationsineveryrun(effectively\nthe search effort) to a fixed number so that different algorithms and configurations can be\ncompared fairly. summarize your comparisons and conclusions. supplement your comments\nwith graphs to illustrate key points.\n3.1.\n\nimplement a random starts local search algorithm for minimizing the aic for the\nbaseball salary regression problem. model your algorithm after example 3.3.\na. change the move strategy from steepest descent to immediate adoption of the first\n\nrandomly selected downhill neighbor.\n\nb. change the algorithm to employ 2-neighborhoods, and compare the results with\n\nthose of previous runs.\n\n3.2.\n\nimplement a tabu algorithm for minimizing the aic for the baseball salary regression\nproblem. model your algorithm after example 3.7.\na. compare the effect of using different tabu tenures.\nb. monitor changes in aic from one move to the next. define a new attribute that\nsignalswhentheaicchangeexceedssomevalue.allowthisattributetobeincluded\non the tabu list, to promote search diversity.\n\nc. implement aspiration by influence, overriding the tabu of reversing a low-influence\nmove if a high-influence move is made prior to the reversal. measure influence with\nchanges in r2.\n\n "}, {"Page_number": 107, "text": "3.3.\n\n3.4.\n\n3.5 tabu algorithms\n\n93\n\nimplement simulated annealing for minimizing the aic for the baseball salary regres-\nsion problem. model your algorithm on example 3.4.\na. compare the effects of different cooling schedules (different temperatures and dif-\n\nferent durations at each temperature).\n\nb. compare the effect of a proposal distribution that is discrete uniform over\n2-neighborhoods versus one that is discrete uniform over 3-neighborhoods.\n\nimplementageneticalgorithmforminimizingtheaicforthebaseballsalaryregression\nproblem. model your algorithm on example 3.5.\na. compare the effects of using different mutation rates.\nb. compare the effects of using different generation sizes.\nc. instead of the selection mechanism used in example 3.5, try the following three\n\nmechanisms:\ni. independent selection of one parent with probability proportional to fitness and\n\nthe other completely at random\n\nii. independent selection of each parent with probability proportional to fitness\niii. tournament selection with p/5 strata, and/or another number of strata that you\n\nprefer\n\nto implement some of these approaches, you may need to scale the fitness function.\nfor example, consider the scaled fitness functions \u03c0 given by\n\n\u03c6(\u03d1\n\u03c6(\u03d1\n\nor\n\n(t)\n\n(t)\n\ni ) = af(\u03b8\ni ) = f(\u03b8\n\ni ) + b,\ni ) \u2212 (f \u2212 zs),\n\n(t)\n\n(t)\n\n(3.12)\n(3.13)\n\n(t)\n\n\u03c6(\u03d1\n\n(t)\ni )v,\n\ni ) = f(\u03b8\n\n(3.14)\nwhere a and b are chosen so that the mean fitness equals the mean objective function\nvalueandthemaximumfitnessisauser-chosen c timesgreaterthanthemeanfitness,\nf isthemeanand sisthestandarddeviationoftheunscaledobjectivefunctionvalues\nin the current generation, z is a number generally chosen between 1 and 3, and v\nis a number slightly larger than 1. some scalings can sometimes produce negative\nvalues for \u03d1\n\n. in such situations, we may apply the transformation\ni ) + d(t) > 0,\n\ni ) + d(t)\n\n\u03c6new(\u03d1\n\n(t)\n\n(t)\n\n(3.15)\n\n(t)\ni\n\nif \u03c6(\u03d1\notherwise,\n\n(t)\n\ni ) =1 \u03c6(\u03d1\n\n0\n\nwhere d(t) is the absolute value of the fitness of the worst chromosome in generation\nt, in the last k generations for some k, or in all preceding generations. each of these\nscaling approaches has the capacity to dampen the variation in f, thereby retaining\nwithin-generation diversity and increasing the potential to find the global optimum.\ncompare and comment on the results for your chosen methods.\n\nd. apply a steady-state genetic algorithm, with the generation gap g = 1/p. compare\n\nwith the canonical option of distinct, nonoverlapping generations.\n\n "}, {"Page_number": 108, "text": "94\n\nchapter 3 combinatorial optimization\n\ns\nu\nc\no\nl\n\n12\n\n6\n\n1\n\n1\n\n12\n\n6\n\n1\n\ns\nu\nc\no\nl\n\n100\n\n1\n\n50\n\nindividual\n\n50\n\nindividual\n\n100\n\nfigure 3.8 chromosomes for problem 3.5. simulated data on 12 loci are available for\n100 individuals. for each locus, the source chromosome from the heterozygous parent is\nencoded in black or white, analogously to figure 3.1 in example 3.1. the left panel shows\nthe data arranged according to the true locus ordering, whereas the right panel shows the\ndata arranged by locus label as they would be recorded during data collection.\n\ne. implementthefollowingcrossoverapproach,termed uniform crossover[622]:each\nlocus in the offspring is filled with an allele independently selected at random from\nthe alleles expressed in that position in the parents.\n\n3.5. consider the genetic mapping example introduced in example 3.1. figure 3.8 shows\nsome data for 100 simulated data sequences for a chromosome of length 12. the left\npanel of this figure shows the data under the true genetic map ordering, and the right\npanel shows the actual data, with the ordering unknown to the analyst. the data are\navailable from the website for this book.\na. apply a random starts local search approach to estimate the genetic map (i.e., the\nordering and the genetic distances). let neighborhoods consist of 20 orderings\nthat differ from the current ordering by randomly swapping the placement of two\nalleles. move to the best candidate in the neighborhood, thereby taking a random\ndescent step. begin with a small number of starts of limited length, to gauge the\ncomputational difficulty of the problem; then report the best results you obtained\nwithin reasonable limits on the computational burden. comment on your results,\nthe performance of the algorithm, and ideas for improved search. [hint: note that\nthe orderings (\u03b8j1 , \u03b8j2 , . . . , \u03b8j12) and (\u03b8j12 , \u03b8j11 , . . . , \u03b8j1) represent identical chromo-\nsomes read from either end.]\nb. apply an algorithm for random starts local search via steepest descent to estimate\nthe genetic map. comment on your results and the performance of the algorithm.\nthis problem is computationally demanding and may require a fast computer.\n\n3.6. consider the genetic mapping data described in problem 3.5.\n\na. apply a genetic algorithm to estimate the genetic map (i.e., the ordering and the\ngenetic distances). use the order crossover method. begin with a small run to\ngauge the computational difficulty of the problem, then report your results for a run\n\n "}, {"Page_number": 109, "text": "3.5 tabu algorithms\n\n95\n\nusing reasonable limits on the computational burden. comment on your results, the\nperformance of the algorithm, and ideas for improved search.\n\nb. compare the speed of fitness improvements achieved with the order crossover and\n\nthe edge-recombination crossover strategies.\n\nc. attempt any other heuristic search method for these data. describe your implemen-\n\ntation, its speed, and the results.\n\n3.7. the website for this book also includes a second synthetic dataset for a genetic mapping\nproblem. for these data, there are 30 chromosomes. attempt one or more heuristic\nsearchmethodsforthesedata.describeyourimplementation,the results, andthenature\nof any problems you encounter. the true ordering used to simulate the data is also given\nfor this dataset. although the true ordering may not be the mle, how close is your best\nordering to the true ordering? how much larger is this problem than the one examined\nin the previous problem?\n\n3.8. thirteen chemical measurements were carried out on each of 178 wines from three\nregions of italy [53]. these data are available from the website for this book. using\none or more heuristic search methods from this chapter, partition the wines into three\ngroups for which the total of the within-group sum of squares is minimal. comment on\nyour work and the results. this is a search problem of size 3p where p = 178. if you\nhave access to standard cluster analysis routines, check your results using a standard\nmethod like that of hartigan and wong [317].\n\n "}, {"Page_number": 110, "text": "chapter 4\nem optimization methods\n\nthe expectation\u2013maximization (em) algorithm is an iterative optimization strategy\nmotivated by a notion of missingness and by consideration of the conditional distribu-\ntion of what is missing given what is observed. the strategy\u2019s statistical foundations\nand effectiveness in a variety of statistical problems were shown in a seminal paper\nby dempster, laird, and rubin [150]. other references on em and related methods\ninclude [409, 413, 449, 456, 625]. the popularity of the em algorithm stems from\nhow simple it can be to implement and how reliably it can find the global optimum\nthrough stable, uphill steps.\nin a frequentist setting, we may conceive of observed data generated from\nrandom variables x along with missing or unobserved data from random variables z.\nwe envision complete data generated from y = (x,z). given observed data x, we\nwish to maximize a likelihood l(\u03b8|x). often it will be difficult to work with this\nlikelihoodandeasiertoworkwiththedensitiesof y|\u03b8 and z|(x, \u03b8).theemalgorithm\nsidesteps direct consideration of l(\u03b8|x) by working with these easier densities.\nin a bayesian application, interest often focuses on estimating the mode of a\nposterior distribution f(\u03b8|x). again, optimization can sometimes be simplified by\nconsideration of unobserved random variables \u03c8 in addition to the parameters of\ninterest, \u03b8.\nthe missing data may not truly be missing: they may be only a conceptual ploy\nthat simplifies the problem. in this case, z is often referred to as latent. it may seem\ncounterintuitive that optimization sometimes can be simplified by introducing this\nnew element into the problem. however, examples in this chapter and its references\nillustrate the potential benefit of this approach. in some cases, the analyst must draw\nupon his or her creativity and cleverness to invent effective latent variables; in other\ncases, there is a natural choice.\n\n4.1 missing data, marginalization, and notation\nwhether z is considered latent or missing, it may be viewed as having been re-\nmoved from the complete y through the application of some many-to-fewer map-\nping, x = m(y). let fx(x|\u03b8) and fy(y|\u03b8) denote the densities of the observed\ndata and the complete data, respectively. the latent- or missing-data assumption\n\ncomputational statistics, second edition. geof h. givens and jennifer a. hoeting.\n\u00a9 2013 john wiley & sons, inc. published 2013 by john wiley & sons, inc.\n\n97\n\n "}, {"Page_number": 111, "text": "chapter 4 em optimization methods\n\n98\namounts to a marginalization model in which we observe x having density fx(x|\u03b8) =\n!{y:m(y)=x} fy(y|\u03b8) dy. note that the conditional density of the missing data given the\nobserved data is fz|x(z|x, \u03b8) = fy(y|\u03b8)/fx(x|\u03b8).\nin bayesian applications focusing on the posterior density for parameters of\ninterest, \u03b8, there are two manners in which we may consider the posterior to represent\na marginalization of a broader problem. first, it may be sensible to view the likelihood\nl(\u03b8|x)asamarginalizationofthecomplete-datalikelihood l(\u03b8|y) = l(\u03b8|x,z).inthis\ncase the missing data are z, and we use the same sort of notation as above. second, we\nmay consider there to be missing parameters \u03c8, whose inclusion simplifies bayesian\ncalculations even though \u03c8 is of no interest itself. fortunately, under the bayesian\nparadigm there is no practical distinction between these two cases. since z and \u03c8 are\nboth missing random quantities, it matters little whether we use notation that suggests\nthe missing variables to be unobserved data or parameters. in cases where we adopt\nthe frequentist notation, the reader may replace the likelihood and z by the posterior\nand \u03c8, respectively, to consider the bayesian point of view.\nin the literature about em, it is traditional to adopt notation that reverses the\nroles of x and y compared to our usage. we diverge from tradition, using x = x to\nrepresent observed data as everywhere else in this book.\n\n4.2 the em algorithm\nthe em algorithm iteratively seeks to maximize l(\u03b8|x) with respect to \u03b8. let \u03b8(t)\ndenote the estimated maximizer at iteration t, for t = 0,1, . . .. define q(\u03b8|\u03b8(t)) to be\nthe expectation of the joint log likelihood for the complete data, conditional on the\nobserved data x = x. namely,\n\nq(\u03b8|\u03b8(t)) = e\"log l(\u03b8|y) ## x, \u03b8(t)$\n= e\"log fy(y|\u03b8) ## x, \u03b8(t)$\n=% &log fy(y|\u03b8)\u2019fz|x(z|x, \u03b8(t)) dz,\n\n(4.1)\n(4.2)\n(4.3)\n\nwhere (4.3) emphasizes that z is the only random part of y once we are given x = x.\nem is initiated from \u03b8(0) then alternates between two steps: e for expectation\nand m for maximization. the algorithm is summarized as:\n1. e step: compute q(\u03b8|\u03b8(t)).\n2. m step: maximize q(\u03b8|\u03b8(t)) with respect to \u03b8. set \u03b8(t+1) equal to the maximizer\n3. return to the e step unless a stopping criterion has been met.\n\nof q.\n\nstopping criteria for optimization problems are discussed in chapter 2. in the\npresent case, such criteria are usually built upon (\u03b8(t+1) \u2212 \u03b8(t))t(\u03b8(t+1) \u2212 \u03b8(t)) or\n##q(\u03b8(t+1)|\u03b8(t)) \u2212 q(\u03b8(t)|\u03b8(t))##.\n\n "}, {"Page_number": 112, "text": "4.2 the em algorithm 99\nexample 4.1 (simple exponential density) to understand the em notation, con-\nsider a trivial example where y1, y2 \u223c i.i.d. exp(\u03b8). suppose y1 = 5 is observed but\nthe value y2 is missing. the complete-data log likelihood function is log l(\u03b8|y) =\nlog fy(y|\u03b8) = 2 log{\u03b8} \u2212 \u03b8y1 \u2212 \u03b8y2.takingtheconditionalexpectation oflog l(\u03b8|y)\nyields q(\u03b8|\u03b8(t)) = 2 log{\u03b8} \u2212 5\u03b8 \u2212 \u03b8/\u03b8(t), since e{y2|y1, \u03b8(t)} = e{y2|\u03b8(t)} = 1/\u03b8(t)\nfollows from independence. the maximizer of q(\u03b8|\u03b8(t)) with respect to \u03b8 is easily\nfound to be the root of 2/\u03b8 \u2212 5 \u2212 1/\u03b8(t) = 0. solving for \u03b8 provides the updating\nequation \u03b8(t+1) = 2\u03b8(t)/(5\u03b8(t) + 1). note here that the e step and m step do not need\nto be rederived at each iteration: iterative application of the updating formula starting\nfrom some initial value provides estimates that converge to \u02c6\u03b8 = 0.2.\nthis example is not realistic. the maximum likelihood estimate of \u03b8 from the\nobserved data can be determined from elementary analytic methods without reliance\non any fancy numerical optimization strategy like em. more importantly, we will\nlearn that taking the required expectation is tricker in real applications, because one\nneeds to know the conditional distribution of the complete data given the missing\ndata.\n!\nexample 4.2 (peppered moths)\nthe peppered moth, biston betularia, presents\na fascinating story of evolution and industrial pollution [276]. the coloring of these\nmoths is believed to be determined by a single gene with three possible alleles, which\nwe denote c, i, and t. of these, c is dominant to i, and t is recessive to i. thus the\ngenotypes cc, ci, and ct result in the carbonaria phenotype, which exhibits solid\nblack coloring. the genotype tt results in the typica phenotype, which exhibits light-\ncolored patterned wings. the genotypes ii and it produce an intermediate phenotype\ncalled insularia, which varies widely in appearance but is generally mottled with\nintermediate color. thus, there are six possible genotypes, but only three phenotypes\nare measurable in field work.\nin the united kingdom and north america, the carbonaria phenotype nearly\nreplaced the paler phenotypes in areas affected by coal-fired industries. this change\nin allele frequencies in the population is cited as an instance where we may observe\nmicroevolution occurring on a human time scale. the theory (supported by experi-\nments) is that \u201cdifferential predation by birds on moths that are variously conspicuous\nagainst backgrounds of different reflectance\u201d (p. 88) induces selectivity that favors the\ncarbonaria phenotype in times and regions where sooty, polluted conditions reduce\nthe reflectance of the surface of tree bark on which the moths rest [276]. not surpris-\ningly, when improved environmental standards reduced pollution, the prevalence of\nthe lighter-colored phenotypes increased and that of carbonaria plummeted.\nthus, it is of interest to monitor the allele frequencies of c, i, and t over time\nto provide insight on microevolutionary processes. further, trends in these frequen-\ncies also provide an interesting biological marker to monitor air quality. within a\nsufficiently short time period, an approximate model for allele frequencies can be\nbuilt from the hardy\u2013weinberg principle that each genotype frequency in a popula-\ntion in hardy\u2013weinberg equilibrium should equal the product of the corresponding\nallele frequencies, or double that amount when the two alleles differ (to account for\nuncertainty in the parental source) [15, 316]. thus, if the allele frequencies in the\npopulation are pc, pi, and pt, then the genotype frequencies should be p2\nc, 2pcpi,\n\n "}, {"Page_number": 113, "text": "chapter 4 em optimization methods\n\n100\n2pcpt, p2\ni , 2pipt, and p2\nt for genotypes cc, ci, ct, ii, it, and tt, respectively.\nnote that pc + pi + pt = 1.\nsupposewecapture nmoths,ofwhichthereare nc, ni,and nt ofthecarbonaria,\ninsularia, and typica phenotypes, respectively. thus, n = nc + ni + nt. since each\nmoth has two alleles in the gene in question, there are 2n total alleles in the sample.\nif we knew the genotype of each moth rather than merely its phenotype, we could\ngenerate genotype counts ncc, nci, nct, nii, nit, and ntt, from which allele frequen-\ncies could easily be tabulated. for example, each moth with genotype ci contributes\none c allele and one i allele, whereas a ii moth contributes two i alleles. such allele\ncounts would immediately provide estimates of pc, pi, and pt. it is far less clear how\nto estimate the allele frequencies from the phenotype counts alone.\nin the em notation, the observed data are x = (nc, ni, nt) and the complete\ndata are y = (ncc, nci, nct, nii, nit, ntt). the mapping from the complete data to the\nobserved data is x = m(y) = (ncc + nci + nct, nii + nit, ntt). we wish to estimate\nthe allele probabilities, pc, pi, and pt. since pt = 1 \u2212 pc \u2212 pi, the parameter vector\nfor this problem is p = (pc, pi), but for notational brevity we often refer to pt in what\nfollows.\n\nthe complete data log likelihood function is multinomial:\n\nlog fy(y|p) = ncc log{p2\n+ nii log{p2\n+ log(\n\nc} + nci log{2pcpi} + nct log{2pcpt}\ni } + nit log{2pipt} + ntt log{p2\nt}\nncc nci nct nii nit ntt) .\n\n(4.4)\nthecompletedataarenotallobserved.lety = (ncc, nci, nct, nii, nit, ntt),since\nwe know ntt = ntt but the other frequencies are not directly observed. to calculate\n(t)\n(t)\nq(p|p(t)), notice that conditional on nc and a parameter vector p(t) = (p\ni ),\nc , p\nthe latent counts for the three carbonaria genotypes have a three-cell multinomial\nc+2,\ndistribution with count parameter nc and cell probabilities proportional to*p\n(t)\n\n(t)\nt . a similar result holds for the two insularia cells. thus the\n\n(t)\ni\n\nn\n\n(p\n\n(t)\n, and 2p\nc p\n\n(t)\n2p\nc p\nexpected values of the first five random parts of (4.4) are\n(t)\nc )2\nnc(p\n(t)\n(t)\n(t)\nc )2 + 2p\ni + 2p\nc p\n(t)\n(t)\n2ncp\ni\nc p\n(t)\n(t)\n(t)\nc )2 + 2p\ni + 2p\nc p\n(t)\n(t)\n2ncp\nt\nc p\n(t)\n(t)\n(t)\nc )2 + 2p\ni + 2p\nc p\n(t)\ni )2\nni(p\n(t)\n(t)\n(t)\ni )2 + 2p\ni p\nt\n(t)\n(t)\n2nip\nt\ni p\n(t)\n(t)\ni )2 + 2p\ni p\n\ne{ncc|nc, ni, nt,p(t)\ne{nci|nc, ni, nt,p(t)\ne{nct|nc, ni, nt,p(t)\ne{nii|nc, ni, nt,p(t)\ne{nit|nc, ni, nt,p(t)\n\n(t)\ncc =\n} = n\n(t)\nci =\n(t)\nct =\n(t)\nii =\n(t)\nit =\n\n} = n\n\n} = n\n\n} = n\n\n} = n\n\n(p\n\n(p\n\n(p\n\n(p\n\n(t)\nt\n\n,\n\n.\n\n,\n\n,\n\n,\n\n(t)\nc p\n\n(t)\nt\n\n(t)\nc p\n\n(t)\nt\n\n(t)\nc p\n\n(t)\nt\n\n(4.5)\n\n(4.6)\n\n(4.7)\n\n(4.8)\n\n(4.9)\n\n "}, {"Page_number": 114, "text": "4.2 the em algorithm 101\nfinally, we know ntt = nt, where nt is observed. the multinomial coefficient in the\nlikelihood has a conditional expectation, say k(nc, ni, nt,p(t)), that does not depend\non p. thus, we have found\n(t)\n(t)\ncc log{p2\nq(p|p(t)) = n\nci log{2pcpi}\n(t)\n(t)\nii log{p2\nct log{2pcpt} + n\n+ n\ni }\n(t)\nt} + k(nc, ni, nt,p(t)).\nit log{2pipt} + ntt log{p2\n+ n\n\nrecalling pt = 1 \u2212 pc \u2212 pi and differentiating with respect to pc and pi yields\n\nc} + n\n\n(4.10)\n\ndq(p|p(t))\ndq(p|p(t))\n\ndpc\n\ndpi\n\n2n\n\n2n\n\n(t)\nct\n\n(t)\n(t)\ncc + n\nci + n\npc\n(t)\n(t)\n(t)\nii + n\nit + n\nci\npi\n\n\u2212\n\n\u2212\n\n=\n\n=\n\n2n\n\n2n\n\n(t)\nit\n\n(t)\n(t)\nct + n\ntt + n\n1 \u2212 pc \u2212 pi\n(t)\n(t)\n(t)\ntt + n\nct + n\nit\n1 \u2212 pc \u2212 pi\n\n.\n\n,\n\n(4.11)\n\n(4.12)\n\nsetting these derivatives equal to zero and solving for pc and pi completes the m step,\nyielding\n\n=\n\n=\n\n(t+1)\nc\np\n(t+1)\ni\np\n(t+1)\nt\np\n\n,\n\n2n\n\n2n\n\n2n\n\n(t)\nct\n\n(t)\n(t)\nci + n\ncc + n\n2n\n(t)\n(t)\n(t)\nit + n\nii + n\nci\n2n\n(t)\n(t)\ntt + n\nct + n\n2n\n\n,\n(t)\nit\n\n(4.13)\n\n(4.14)\n\n,\n\n=\n\n(t)\nci + n\n\n(t)\ncc + n\n\n(4.15)\nwhere the final expression is derived from the constraint that the probabilities sum to\none. if the tth latent counts were true, the number of carbonaria alleles in the sample\n(t)\nwould be 2n\nct. there are 2n total alleles in the sample. thus, the em\nupdate consists of setting the elements of p(t+1) equal to the phenotypic frequencies\nthat would result from the tth latent genotype counts.\nsuppose the observed phenotype counts are nc = 85, ni = 196, and nt = 341.\ntable 4.1 shows how the em algorithm converges to the mles, roughly \u02c6pc =\n0.07084, \u02c6pi = 0.18874, and \u02c6pt = 0.74043. finding a precise estimate of \u02c6pi is slower\nthan for \u02c6pc, since the likelihood is flatter along the pi coordinate.\nthe last three columns of table 4.1 show convergence diagnostics. a relative\nconvergence criterion,\n\nr(t)\n\n= \u2225p(t) \u2212 p(t\u22121)\u2225\n\u2225p(t\u22121)\u2225\n\n,\n\n(4.16)\n\nsummarizes the total amount of relative change in p(t) from one iteration to\n(t)\nthe next, where \u2225z\u2225 = (ztz)1/2. for illustrative purposes, we also include d\nc =\n(t)\n. these ratios quickly con-\nc \u2212 \u02c6pc)/(p\n(p\nverge to constants, confirming that the em rate of convergence is linear as defined\nby (2.19).\n!\n\n(t\u22121)\nc \u2212 \u02c6pc) and the analogous quantity d\n\n(t)\ni\n\n "}, {"Page_number": 115, "text": "102\n\nchapter 4 em optimization methods\n\ntable 4.1 em results for peppered moth example. the diagnostic quantities r(t), d(t)c , and d(t)i are\ndefined in the text.\n(t)\nc\n\n(t)\ni\np\n\nr(t)\n\n(t)\nc\n\n(t)\ni\n\nd\n\nd\n\np\n\n0.333333\n0.081994\n0.071249\n0.070852\n0.070837\n0.070837\n0.070837\n0.070837\n0.070837\n\n0.333333\n0.237406\n0.197870\n0.190360\n0.189023\n0.188787\n0.188745\n0.188738\n0.188737\n\n5.7 \u00d7 10\u22121\n1.6 \u00d7 10\u22121\n3.6 \u00d7 10\u22122\n6.6 \u00d7 10\u22123\n1.2 \u00d7 10\u22123\n2.1 \u00d7 10\u22124\n3.6 \u00d7 10\u22125\n6.4 \u00d7 10\u22126\n\n0.0425\n0.0369\n0.0367\n0.0367\n0.0367\n0.0367\n0.0367\n0.0367\n\n0.337\n0.188\n0.178\n0.176\n0.176\n0.176\n0.176\n0.176\n\nt\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\nexample 4.3 (bayesian posterior mode) consider a bayesian problem with like-\nlihood l(\u03b8|x), prior f(\u03b8), and missing data or parameters z. to find the posterior\nmode, the e step requires\n\nq(\u03b8|\u03b8(t)) = e{log{l(\u03b8|y)f(\u03b8)k(y)}|x, \u03b8(t)\n\n}\n\n= e{log l(\u03b8|y)|x, \u03b8(t)\n\n} + log f(\u03b8) + e{log k(y)|x, \u03b8(t)\n\n(4.17)\nwhere the final term in (4.17) is a normalizing constant that can be ignored because\nq is to be maximized with respect to \u03b8. this function q is obtained by simply adding\nthe log prior to the q function that would be used in a maximum likelihood setting.\nunfortunately, the addition of the log prior often makes it more difficult to maximize\nq during the m step. section 4.3.2 describes a variety of methods for facilitating the\nm step in difficult situations.\n!\n\n},\n\n4.2.1 convergence\nto investigate the convergence properties of the em algorithm, we begin by showing\nthat each maximization step increases the observed-data log likelihood, l(\u03b8|x). first,\nnote that the log of the observed-data density can be reexpressed as\nlog fx(x|\u03b8) = log fy(y|\u03b8) \u2212 log fz|x(z|x, \u03b8).\n(4.18)\n\ntherefore,\n\ne{log fx(x|\u03b8)|x, \u03b8(t)\n\n} = e{log fy(y|\u03b8)|x, \u03b8(t)\n\n} \u2212 e{log fz|x(z|x, \u03b8)|x, \u03b8(t)\n\n},\n\nwhere the expectations are taken with respect to the distribution of z|(x, \u03b8(t)). thus,\n(4.19)\n\nwhere\n\nlog fx(x|\u03b8) = q(\u03b8|\u03b8(t)) \u2212 h(\u03b8|\u03b8(t)),\nh(\u03b8|\u03b8(t)) = e\"log fz|x(z|x, \u03b8)##x, \u03b8(t)$ .\n\n(4.20)\n\n "}, {"Page_number": 116, "text": "4.2 the em algorithm 103\nthe importance of (4.19) becomes apparent after we show that h(\u03b8|\u03b8(t)) is\nh(\u03b8(t)\n\nmaximized with respect to \u03b8 when \u03b8 = \u03b8(t). to see this, write\n\n|\u03b8(t)) \u2212 h(\u03b8|\u03b8(t))\n\n= e\"log fz|x(z|x, \u03b8(t)) \u2212 log fz|x(z|x, \u03b8)###x, \u03b8(t)$\n= % \u2212log, fz|x(z|x, \u03b8)\nfz|x(z|x, \u03b8(t))- fz|x(z|x, \u03b8(t)) dz\n\u2265 \u2212 log% fz|x(z|x, \u03b8) dz\n= 0.\n\n(4.21)\nequation (4.21) follows from an application of jensen\u2019s inequality, since \u2212log u is\nstrictly convex in u.\nthus, any \u03b8 /= \u03b8(t) makes h(\u03b8|\u03b8(t)) smaller than h(\u03b8(t)|\u03b8(t)). in particular, if we\nchoose \u03b8(t+1) to maximize q(\u03b8|\u03b8(t)) with respect to \u03b8, then\nlog fx(x|\u03b8(t+1)) \u2212 log fx(x|\u03b8(t)) \u2265 0,\n(4.22)\nsince q increases and h decreases, with strict inequality when q(\u03b8(t+1)|\u03b8(t)) >\nq(\u03b8(t)|\u03b8(t)).\nchoosing \u03b8(t+1) at each iteration to maximize q(\u03b8|\u03b8(t)) with respect to \u03b8 con-\nstitutes the standard em algorithm. if instead we simply select any \u03b8(t+1) for which\nq(\u03b8(t+1)|\u03b8(t)) > q(\u03b8(t)|\u03b8(t)), then the resulting algorithm is called generalized em, or\ngem. in either case, a step that increases q increases the log likelihood. conditions\nunder which this guaranteed ascent ensures convergence to an mle are explored in\n[60, 676].\nhaving established this result, we next consider the order of convergence for\nthe method. the em algorithm defines a mapping \u03b8(t+1) = \u0001(\u03b8(t)) where the function\n\u0001(\u03b8) = (\u00011(\u03b8), . . . , \u0001p(\u03b8)) and \u03b8 = (\u03b81, . . . , \u03b8p). when em converges, it converges\nto a fixed point of this mapping, so \u02c6\u03b8 = \u0001(\u02c6\u03b8). let \u0001\u2032(\u03b8) denote the jacobian matrix\nwhose (i, j)th element is d\u0001i(\u03b8)/d\u03b8j. taylor series expansion of \u0001 yields\n(4.23)\nsince \u03b8(t+1) \u2212 \u02c6\u03b8 = \u0001(\u03b8(t)) \u2212 \u0001(\u02c6\u03b8). comparing this result with (2.19), we see that the\nem algorithm has linear convergence when p = 1. for p > 1, convergence is still\nlinear provided that the observed information, \u2212l\u2032\u2032(\u02c6\u03b8|x), is positive definite. more\nprecise details regarding convergence are given in [150, 449, 452, 455].\n\n\u2212 \u02c6\u03b8 \u2248 \u0001\u2032(\u03b8(t))(\u03b8(t)\n\nthe global rate of em convergence is defined as\n\u2225\u03b8(t+1) \u2212 \u02c6\u03b8\u2225\n\u2225\u03b8(t) \u2212 \u02c6\u03b8\u2225\n\n(4.24)\nit can be shown that \u03c1 equals the largest eigenvalue of \u0001\u2032(\u02c6\u03b8) when \u2212l\u2032\u2032(\u02c6\u03b8|x) is posi-\ntive definite. in sections 4.2.3.1 and 4.2.3.2 we will examine how \u0001\u2032(\u02c6\u03b8) is a matrix\nof the fractions of missing information. therefore, \u03c1 effectively serves as a scalar\n\n\u03c1 = lim\nt\u2192\u221e\n\n\u2212 \u02c6\u03b8),\n\n\u03b8(t+1)\n\n.\n\n "}, {"Page_number": 117, "text": "104\n\nchapter 4 em optimization methods\n\n)\nx\n\n|\n\n\u03b8\n(\nl\n\nl(\u03b8 |x)\n\ng(\u03b8 |\u03b8(t+1))\n\ng(\u03b8|\u03b8(t))\n\n\u03b8 (t)\n\n\u03b8 (t+1)\n\n\u03b8 (t+2)\n\nfigure4.1 one-dimensionalillustrationofemalgorithmasaminorizationoroptimization\ntransfer strategy.\n\n\u03b8\n\nsummary of the overall proportion of missing information. conceptually, the propor-\ntion of missing information equals one minus the ratio of the observed information\nto the information that would be contained in the complete data. thus, em suffers\nslower convergence when the proportion of missing information is larger. the linear\nconvergence of em can be extremely slow compared to the quadratic convergence of,\nsay, newton\u2019s method, particularly when the fraction of missing information is large.\nhowever, the ease of implementation and the stable ascent of em are often very at-\ntractivedespiteitsslowconvergence.section4.3.3discussesmethodsforaccelerating\nem convergence.\n\nto further understand how em works, note from (4.21) that\n\n|x) \u2212 q(\u03b8(t)\n\n|\u03b8(t)) = g(\u03b8|\u03b8(t)).\n\nl(\u03b8|x) \u2265 q(\u03b8|\u03b8(t)) + l(\u03b8(t)\n\n(4.25)\nsince the last two terms in g(\u03b8|\u03b8(t)) are independent of \u03b8, the functions q and g are\nmaximized at the same \u03b8. further, g is tangent to l at \u03b8(t) and lies everywhere below l.\nwe say that g is a minorizing function for l. the em strategy transfers optimization\nfrom l to the surrogate function g (effectively to q), which is more convenient to\nmaximize. the maximizer of g provides an increase in l. this idea is illustrated in\nfigure 4.1. each e step amounts to forming the minorizing function g, and each\nm step amounts to maximizing it to provide an uphill step.\ntemporarily replacing l by a minorizing function is an example of a more\ngeneral strategy known as optimization transfer. links to the em algorithm and other\nstatistical applications of optimization transfer are surveyed in [410]. in mathematical\napplications where it is standard to pose optimizations as minimizations, one typically\nrefers to majorization, as one could achieve by majorizing the negative log likelihood\nusing \u2212g(\u03b8|\u03b8(t)).\n\n "}, {"Page_number": 118, "text": "4.2 the em algorithm 105\n\n4.2.2 usage in exponential families\nwhen the complete data are modeled to have a distribution in the exponential family,\nthe density of the data can be written as f(y|\u03b8) = c1(y)c2(\u03b8)exp{\u03b8ts(y)}, where \u03b8 is\na vector of natural parameters and s(y) is a vector of sufficient statistics. in this case,\nthe e step finds\n\nq(\u03b8|\u03b8(t)) = k + log c2(\u03b8) +% \u03b8ts(y)fz|x(z|x, \u03b8(t)) dz,\n\n(4.26)\n\nwhere k is a quantity that does not depend on \u03b8. to carry out the m step, set the\ngradient of q(\u03b8|\u03b8(t)) with respect to \u03b8 equal to zero. this yields\nc2(\u03b8) =% s(y)fz|x(z|x, \u03b8(t)) dz\n\u2212c\u20322(\u03b8)\n\nafter rearranging terms and adopting the obvious notational shortcut to vectorize\nthe integral of a vector. it is straightforward to show that c\u20322(\u03b8) = \u2212c2(\u03b8)e{s(y)|\u03b8}.\ntherefore, (4.27) implies that the m step is completed by setting \u03b8(t+1) equal to the \u03b8\nthat solves\n\n(4.27)\n\ne{s(y)|\u03b8} =% s(y)fz|x(z|x, \u03b8(t)) dz.\n\n(4.28)\naside from replacing \u03b8(t) with \u03b8(t+1), the form of q(\u03b8|\u03b8(t)) is unchanged for the next\ne step, and the next m step solves the same optimization problem. therefore, the em\nalgorithm for exponential families consists of:\n\n1. e step: compute the expected values of the sufficient statistics for the complete\ndata, given the observed data and using the current parameter guesses, \u03b8(t). let\n\ns(t) = e{s(y)|x, \u03b8(t)} =! s(y)fz|x(z|x, \u03b8(t)) dz.\n2. m step: set \u03b8(t+1) to the value that makes the unconditional expectation of the\nsufficient statistics for the complete data equal to s(t). in other words, \u03b8(t+1)\nsolves e{s(y)|\u03b8)} = s(t).\n\n3. return to the e step unless a convergence criterion has been met.\n\nexample 4.4 (peppered moths, continued) the complete data in example 4.2\narisefromamultinomialdistribution,whichisintheexponentialfamily.thesufficient\nstatistics are, say, the first five genotype counts (with the sixth derived from the\nconstraint that the counts total n), and the natural parameters are the corresponding\nlog probabilities seen in (4.4). the first three conditional expectations for the e step\n(t)\n(t)\nct, borrowing notation from (4.5)\u2013(4.9) and\nare s\nct = n\nindexing the components of s(t) in the obvious way. the unconditional expectations\nof the first three sufficient statistics are np2\nc, 2npcpi, and 2npcpt. equating these\nthree expressions with the conditional expectations given above and solving for pc\nconstitutes the m step for pc. summing the three equations gives np2\nc + 2npcpi +\n(t)\n(t)\n2npcpt = n\nct,whichreducestotheupdategivenin(4.13).emupdates\ncc + n\n\n(t)\n(t)\nci, and s\nci = n\n\n(t)\n(t)\ncc, s\ncc = n\n\n(t)\nci + n\n\n "}, {"Page_number": 119, "text": "chapter 4 em optimization methods\n\n106\nfor pi and pt arefoundanalogously,onnotingtheconstraintthatthethreeprobabilities\nsum to 1.\n!\n\n4.2.3 variance estimation\ninamaximumlikelihoodsetting,theemalgorithmisusedtofindanmlebutdoesnot\nautomatically produce an estimate of the covariance matrix of the mles. typically,\nwe would use the asymptotic normality of the mles to justify seeking an estimate of\nthe fisher information matrix. one way to estimate the covariance matrix, therefore,\nis to compute the observed information, \u2212l\u2032\u2032(\u02c6\u03b8|x), where l\u2032\u2032 is the hessian matrix of\nsecond derivatives of log l(\u03b8|x).\nin a bayesian setting, an estimate of the posterior covariance matrix for \u03b8 can\nbe motivated by noting the asymptotic normality of the posterior [221]. this requires\nthe hessian of the log posterior density.\nin some cases, the hessian may be computed analytically. in other cases, the\nhessian may be difficult to derive or code. in these instances, a variety of other\nmethods are available to simplify the estimation of the covariance matrix.\nof the options described below, the sem (supplemented em) algorithm is easy\nto implement while generally providing fast, reliable results. even easier is boot-\nstrapping, although for very complex problems the computational burden of nested\nlooping may be prohibitive. these two approaches are recommended, yet the other\nalternatives can also be useful in some settings.\n\nlouis\u2019s method taking second partial derivatives of (4.19) and\n\n4.2.3.1\nnegating both sides yields\n\n\u2212l\u2032\u2032(\u03b8|x) = \u2212 q\u2032\u2032(\u03b8|\u03c9)##\u03c9=\u03b8 + h\u2032\u2032(\u03b8|\u03c9)##\u03c9=\u03b8 ,\n\nwhere the primes on q\u2032\u2032 and h\u2032\u2032 denote derivatives with respect to the first argument,\nnamely \u03b8.\n\nequation (4.29) can be rewritten as\n\n(4.29)\n\n\u02c6ix(\u03b8) = \u02c6iy(\u03b8) \u2212 \u02c6iz|x(\u03b8),\n\n(4.30)\nwhere \u02c6ix(\u03b8) = \u2212l\u2032\u2032(\u03b8|x) is the observed information, and \u02c6iy(\u03b8) and \u02c6iz|x(\u03b8) will be\ncalled the complete information and the missing information, respectively. inter-\nchanging integration and differentiation (when possible), we have\n\n\u02c6iy(\u03b8) = \u2212q\u2032\u2032(\u03b8|\u03c9)##\u03c9=\u03b8 = \u2212e{l\u2032\u2032(\u03b8|y)|x, \u03b8},\n\nwhichisreminiscentofthefisherinformationdefinedin(1.28).thismotivatescalling\n\u02c6iy(\u03b8) the complete information. a similar argument holds for \u2212h\u2032\u2032. equation (4.30),\nstating that the observed information equals the complete information minus the\nmissing information, is a result termed the missing-information principle [424, 673].\n\n(4.31)\n\n "}, {"Page_number": 120, "text": "4.2 the em algorithm 107\nthemissing-informationprinciplecanbeusedtoobtainanestimatedcovariance\n\nmatrix for \u02c6\u03b8. it can be shown that\n\n\u02c6iz|x(\u03b8) = var. d log fz|x(z|x, \u03b8)\n\nd\u03b8\n\n/\n\n(4.32)\n\nwhere the variance is taken with respect to fz|x. further, since the expected score is\nzero at \u02c6\u03b8,\n\n\u02c6iz|x(\u02c6\u03b8) =% sz|x(\u02c6\u03b8)sz|x(\u02c6\u03b8)tfz|x(z|x, \u02c6\u03b8) dz,\n\n(4.33)\n\nwhere\n\nsz|x(\u03b8) =\n\nd log fz|x(z|x, \u03b8)\n\nd\u03b8\n\n.\n\nthe missing-information principle enables us to express \u02c6ix(\u03b8) in terms of the\ncomplete-data likelihood and the conditional density of the missing data given the\nobserved data, while avoiding calculations involving the presumably complicated\nmarginal likelihood of the observed data. this approach can be easier to derive and\ncodeinsomeinstances,butitisnotalwayssignificantlysimplerthandirectcalculation\nof \u2212l\u2032\u2032(\u02c6\u03b8|x).\nif \u02c6iy(\u03b8) or \u02c6iz|x(\u03b8) is difficult to compute analytically, it may be estimated via\nthe monte carlo method (see chapter 6). for example, the simplest monte carlo\nestimate of \u02c6iy(\u03b8) is\n\n1\nm\n\nm0i=1 \u2212\n\nd2 log fy(yi|\u03b8)\n\nd\u03b8 \u00b7 d\u03b8\n\n,\n\n(4.34)\n\nwhere for i = 1, . . . , m, the yi = (x,zi) are simulated complete datasets consisting of\ntheobserveddataandi.i.d.imputedmissing-datavalueszi drawnfrom fz|x.similarly,\na simple monte carlo estimate of \u02c6iz|x(\u03b8) is the sample variance of the values of\n\n\u2212&d log fz|x(zi|x, \u03b8)\u20191d\u03b8\n\nobtained from such a collection of zi.\nexample 4.5 (censored exponential data)\nsuppose we attempt to observed\ncomplete data under the model y1, . . . , yn \u223c i.i.d. exp(\u03bb), but some cases are right-\ncensored. thus, the observed data are x = (x1, . . . ,xn) where xi = (min(yi, ci), \u03b4i),\nthe ci are the censoring levels, and \u03b4i = 1 if yi \u2264 ci and \u03b4i = 0 otherwise.\n\n "}, {"Page_number": 121, "text": "108\n\nthus,\n\nchapter 4 em optimization methods\n\nthe complete-data log likelihood is l(\u03bb|y1, . . . , yn) = nlog \u03bb \u2212 \u03bb2n\n\nq(\u03bb|\u03bb(t)) = e{l(\u03bb|y1, . . . , yn)|x, \u03bb(t)\n\n= nlog \u03bb \u2212 \u03bb\n= nlog \u03bb \u2212 \u03bb\n= nlog \u03bb \u2212 \u03bb\n\n}\n\n}\nn0i=1\ne{yi|xi, \u03bb(t)\nn0i=13yi\u03b4i +4ci +\nn0i=1&yi\u03b4i + ci(1 \u2212 \u03b4i)\u2019 \u2212\n\n1\n\n\u03bb(t)5(1 \u2212 \u03b4i)6\n\nc\u03bb\n\u03bb(t) ,\n\ni=1 yi.\n(4.35)\n\n(4.36)\n\n(4.37)\n\nc\n\u03bb2 ,\n\n(4.39)\n\nwhere c =2n\ni=1(1 \u2212 \u03b4i) denotes the number of censored cases. note that (4.36)\nfollows from the memoryless property of the exponential distribution. therefore,\n\u2212q\u2032\u2032(\u03bb|\u03bb(t)) = n/\u03bb2.\nthe unobserved outcome for a censored case, zi, has density fzi|x(zi|x, \u03bb) =\n\u03bbexp{\u2212\u03bb(zi \u2212 ci)}1{zi>ci}. calculating \u02c6iz|x(\u03bb) as in (4.32), we find\n(zi \u2212 ci).\n\nd log fz|x(z|x, \u03bb)\n\n(4.38)\n\nd\u03bb\n\n= c/\u03bb \u2212 0{i: \u03b4i=0}\nthe variance of this expression with respect to fzi|x is\nvar{zi \u2212 ci} =\n\n\u02c6iz|x(\u03bb) = 0{i: \u03b4i=0}\nsince zi \u2212 ci has an exp(\u03bb) distribution.\nthus, applying louis\u2019s method,\n\u02c6ix(\u03bb) =\n\nu\n\u03bb2 ,\n\nn\n\u03bb2 \u2212\n\nc\n\u03bb2 =\n\n(4.40)\nwhere u =2n\ni=1 \u03b4i denotes the number of uncensored cases. for this elementary\nexample, it is easy to confirm by direct analysis that \u2212l\u2032\u2032(\u03bb|x) = u/\u03bb2.\n!\nsem algorithm recall that \u0001 denotes the em mapping, having fixed\n4.2.3.2\npoint \u02c6\u03b8 andjacobianmatrix \u0001\u2032(\u03b8)with(i, j)thelementequaling d\u0001i(\u03b8)/d\u03b8j.dempster\net al. [150] show that\n\n\u0001\u2032(\u02c6\u03b8)t\n\n= \u02c6iz|x(\u02c6\u03b8)\u02c6iy(\u02c6\u03b8)\u22121\n\nin the terminology of (4.30).\n\nif we reexpress the missing information principle in (4.30) as\n\n\u02c6ix(\u02c6\u03b8) =&i \u2212 \u02c6iz|x(\u02c6\u03b8)\u02c6iy(\u02c6\u03b8)\u22121\u2019\u02c6iy(\u02c6\u03b8),\n\n(4.41)\n\n(4.42)\n\n "}, {"Page_number": 122, "text": "4.2 the em algorithm 109\nwhere i is an identity matrix, and substitute (4.41) into (4.42), then inverting \u02c6ix(\u02c6\u03b8)\nprovides the estimate\n\n7var{\u02c6\u03b8} = \u02c6iy(\u02c6\u03b8)\u221218i + \u0001\u2032(\u02c6\u03b8)t[i \u2212 \u0001\u2032(\u02c6\u03b8)t]\u221219 .\n\n(4.43)\nthis result is appealing in that it expresses the desired covariance matrix as the\ncomplete-data covariance matrix plus an incremental matrix that takes account of\nthe uncertainty attributable to the missing data. when coupled with the following\nnumerical differentiation strategy to estimate the increment, meng and rubin have\ntermed this approach the supplemented em (sem) algorithm [453]. since numerical\nimprecisions in the differentiation approach affect only the estimated increment, es-\ntimation of the covariance matrix is typically more stable than the generic numerical\ndifferentiation approach described in section 4.2.3.5.\nestimation of \u0001\u2032(\u02c6\u03b8) proceeds as follows. the first step of sem is to run the em\nalgorithm to convergence, finding the maximizer \u02c6\u03b8. the second step is to restart the\nalgorithm from, say, \u03b8(0). although one may restart from the original starting point,\nit is preferable to choose \u03b8(0) to be closer to \u02c6\u03b8.\nhaving thus initialized sem, we begin sem iterations for t = 0,1,2, . . .. the\n(t + 1)thsemiterationbeginsbytakingastandardestepandmsteptoproduce \u03b8(t+1)\nj , \u02c6\u03b8j+1, . . . , \u02c6\u03b8p)\nfrom \u03b8(t). next, for j = 1, . . . , p, define \u03b8(t)(j) = (\u02c6\u03b81, . . . , \u02c6\u03b8j\u22121, \u03b8\n(t)\nand\n\n(t)\nij =\nr\n\n\u0001i(\u03b8(t)(j)) \u2212 \u02c6\u03b8i\n\n(t)\n\nj \u2212 \u02c6\u03b8j\n\n\u03b8\n\n(4.44)\n\nnotice that the (i, j)th element of \u0001\u2032(\u02c6\u03b8) equals limt\u2192\u221e r\n\nfor i = 1, . . . , p,recallingthat \u0001(\u02c6\u03b8) = \u02c6\u03b8.thisendsonesemiteration.the \u0001i(\u03b8(t)(j))\nvalues are the estimates produced by applying one em cycle to \u03b8(t)(j) for j =\n1, . . . , p.\n(t)\nij . we may consider\n(t)\nij values\neach element of this matrix to be precisely estimated when the sequence of r\nstabilizes for t \u2265 t\u2217ij. note that different numbers of iterations may be needed for\nprecise estimation of different elements of \u0001\u2032(\u02c6\u03b8). when all elements have stabilized,\nsem iterations stop and the resulting estimate of \u0001\u2032(\u02c6\u03b8) is used to determine7var{\u02c6\u03b8} as\ngiven in (4.43).\nnumerical imprecision can cause the resulting covariance matrix to be slightly\nasymmetric. such asymmetry can be used to diagnose whether the original em pro-\ncedure was run to sufficient precision and to assess how many digits are trustworthy\nin entries of the estimated covariance matrix. difficulties also arise if i \u2212 \u0001\u2032(\u02c6\u03b8)t is\nnot positive semidefinite or cannot be inverted numerically; see [453]. it has been\nsuggested that transforming \u03b8 to achieve an approximately normal likelihood can\nlead to faster convergence and increased accuracy of the final solution.\nexample 4.6 (peppered moths, continued) the results from example 4.2 can\nbe supplemented using the approach of meng and rubin [453]. stable, precise\n(0)\nc = 0.07 and\nresults are obtained within a few sem iterations, starting from p\n\n "}, {"Page_number": 123, "text": "p\n\nchapter 4 em optimization methods\n\n110\n(t)\ni = 0.19. standard errors for \u02c6pc, \u02c6pi, and \u02c6pt are 0.0074, 0.0119, and 0.0132,\nrespectively. pairwise correlations are cor{\u02c6pc, \u02c6pi} = \u22120.14, cor{\u02c6pc, \u02c6pt} = \u22120.44,\nand cor{\u02c6pi, \u02c6pt} = \u22120.83. here, sem was used to obtain results for \u02c6pc and \u02c6pi, and\nelementary relationships among variances, covariances, and correlations were used\nto extend these results for \u02c6pt since the estimated probabilities sum to 1.\n!\nit may seem inefficient not to begin sem iterations until em iterations have\nceased. an alternative would be to attempt to estimate the components of \u0001\u2032(\u02c6\u03b8) as\nem iterations progress, using\n\n(t\u22121)\n\u0001i(\u03b8\n1\n\n\u02dcr\n\n(t)\nij =\n\n, . . . , \u03b8\n\n(t)\n(t\u22121)\n(t\u22121)\nj+1 , . . . , \u03b8(t\u22121)\nj\u22121 , \u03b8\nj , \u03b8\n(t\u22121)\n(t)\nj \u2212 \u03b8\n\u03b8\n\np\n\nj\n\n) \u2212 \u0001i(\u03b8(t\u22121))\n\n.\n\n(4.45)\n\nhowever, meng and rubin [453] argue that this approach will not require fewer\niterations overall, that the extra steps required to find \u02c6\u03b8 first can be offset by starting\nsem closer to \u02c6\u03b8, and that the alternative is numerically less stable. jamshidian and\njennrich survey a variety of methods for numerically differentiating \u0001 or l\u2032 itself,\nincluding some they consider superior to sem [345].\n4.2.3.3 bootstrapping thorough discussion of bootstrapping is given in chap-\nter 9. in its simplest implementation, bootstrapping to obtain an estimated covariance\nmatrix for em would proceed as follows for i.i.d. observed data x1, . . . ,xn:\n1. calculate \u02c6\u03b8em using a suitable em approach applied to x1, . . . ,xn. let j = 1\nand set \u02c6\u03b8j = \u02c6\u03b8em.\n2. increment j. sample pseudo-data x\u22171, . . . ,x\u2217n completely at random from\nx1, . . . ,xn with replacement.\n3. calculate \u02c6\u03b8j byapplyingthesameemapproachtothepseudo-datax\u22171, . . . ,x\u2217n.\n4. stop if j is large enough; otherwise return to step 2.\n\nformostproblems,afewthousanditerationswillsuffice.attheendoftheprocess,we\nhave generated a collection of parameter estimates, \u02c6\u03b81, . . . , \u02c6\u03b8b, where b denotes the\ntotal number of iterations used. then the sample variance of these b estimates is the\nestimated variance of \u02c6\u03b8. conveniently, other aspects of the sampling distribution of \u02c6\u03b8,\nsuch as correlations and quantiles, can be estimated using the corresponding sample\nestimates based on \u02c6\u03b81, . . . , \u02c6\u03b8b. note that bootstrapping embeds the em loop in a\nsecond loop of b iterations. this nested looping can be computationally burdensome\nwhen the solution of each em problem is slow because of a high proportion of missing\ndata or high dimensionality.\n\nempirical information when the data are i.i.d., note that the score\n\n4.2.3.4\nfunction is the sum of individual scores for each observation:\n\nd log fx(x|\u03b8)\n\nd\u03b8\n\n= l\u2032(\u03b8|x) =\n\nl\u2032(\u03b8|xi),\n\nn0i=1\n\n(4.46)\n\n "}, {"Page_number": 124, "text": "111\nwherewewritetheobserveddatasetas x = (x1, . . . ,xn).sincethefisherinformation\nmatrix is defined to be the variance of the score function, this suggests estimating\nthe information using the sample variance of the individual scores. the empirical\ninformation is defined as\n\n4.3 em variants\n\nl\u2032(\u03b8|xi)l\u2032(\u03b8|xi)t\n\n\u2212\n\n1\nn2 l\u2032(\u03b8|x)l\u2032(\u03b8|x)t.\n\n(4.47)\n\n1\nn\n\nn0i=1\n\nthis estimate has been discussed in the em context in [450, 530]. the appeal of this\napproach is that all the terms in (4.47) are by-products of the m step: no additional\nanalysis is required. to see this, note that \u03b8(t) maximizes q(\u03b8|\u03b8(t)) \u2212 l(\u03b8|x) with\nrespect to \u03b8. therefore, taking derivatives with respect to \u03b8,\n\n(4.48)\n\nq\u2032(\u03b8|\u03b8(t))###\u03b8=\u03b8(t) = l\u2032(\u03b8|x)###\u03b8=\u03b8(t) .\n\nsince q\u2032 is ordinarily calculated at each m step, the individual terms in (4.47) are\navailable.\n4.2.3.5 numerical differentiation to estimate the hessian, consider comput-\ning the numerical derivative of l\u2032 at \u02c6\u03b8, one coordinate at a time, using (1.10). the first\nrow of the estimated hessian can be obtained by adding a small perturbation to the\nfirst coordinate of \u02c6\u03b8, then computing the ratio of the difference between l\u2032(\u03b8) at \u03b8 = \u02c6\u03b8\nand at the perturbed value, relative to the magnitude of the perturbation. the remain-\ning rows of the hessian are approximated similarly. if a perturbation is too small,\nestimated partial derivatives may be inaccurate due to roundoff error; if a perturba-\ntion is too big, the estimates may also be inaccurate. such numerical differentiation\ncan be tricky to automate, especially when the components of \u02c6\u03b8 have different scales.\nmore sophisticated numerical differentiation strategies are surveyed in [345].\n\n4.3 em variants\n\nimproving the e step\n\n4.3.1\nthe e step requires finding the expected log likelihood of the complete data con-\nditional on the observed data. we have denoted this expectation as q(\u03b8|\u03b8(t)). when\nthis expectation is difficult to compute analytically, it can be approximated via monte\ncarlo (see chapter 6).\n4.3.1.1 monte carlo em wei and tanner [656] propose that the tth e step can\nbe replaced with the following two steps:\n1 , . . . ,z(t)\n\nis a\nvector of all the missing values needed to complete the observed dataset, so\nyj = (x,zj) denotes a completed dataset where the missing values have been\nreplaced by zj.\n\nm(t) i.i.d. from fz|x(z|x, \u03b8(t)). each z(t)\n\n1. draw missing datasets z(t)\n\nj\n\n "}, {"Page_number": 125, "text": "112\n\nchapter 4 em optimization methods\n\nj |\u03b8).\n\nj=1 log fy(y(t)\n\n2. calculate \u02c6q(t+1)(\u03b8|\u03b8(t)) =*1/m(t)+2m(t)\nthen \u02c6q(t+1)(\u03b8|\u03b8(t)) is a monte carlo estimate of q(\u03b8|\u03b8(t)). the m step is modified to\nmaximize \u02c6q(t+1)(\u03b8|\u03b8(t)).\nthe recommended strategy is to let m(t) be small during early em iterations and\nto increase m(t) as iterations progress to reduce the monte carlo variability introduced\nin \u02c6q. nevertheless, this monte carlo em algorithm (mcem) will not converge in\nthe same sense as ordinary em. as iterations proceed, values of \u03b8(t) will eventually\nbounce around the true maximum, with a precision that depends on m(t). discussion\nof the asymptotic convergence properties of mcem is provided in [102]. a stochastic\nalternative to mcem is discussed in [149].\nexample 4.7 (censored exponential data, continued)\nin example 4.5, it was\ni=1 yi given\ntheobserveddata.theresult,givenin(4.37),canbemaximizedtoprovidetheordinary\nem update,\n\neasy to compute the conditional expectation of l(\u03bb|y) = nlog \u03bb \u2212 \u03bb2n\n\n\u03bb(t+1)\n\n=\n\nn\n\n2n\ni=1 xi + c/\u03bb(t) .\n\napplication of mcem is also easy. in this case,\n\u02c6q(t+1)(\u03bb|\u03bb(t)) = nlog \u03bb \u2212\n\n(4.49)\n\n(4.50)\n\nyt\nj 1,\n\n\u03bb\nm(t)\n\nm(t)\n\n0j=1\n\nwhere 1 is a vector of ones and yj is the jth completed dataset comprising the uncen-\nsored data and simulated data zj = (zj1, . . . , zjc) with zjk \u2212 ck \u223c i.i.d. exp(\u03bb(t))\nfor k = 1, . . . , c to replace the censored values. setting \u02c6q\u2032(\u03bb|\u03bb(t)) = 0 and solving\nfor \u03bb yields\n(4.51)\n\n\u03bb(t+1)\n\nas the mcem update.\nthe website for this book provides n = 30 observations, including c = 17\ncensored observations. figure 4.2 compares the performance of mcem and ordinary\nem for estimating \u03bb with these data. both methods easily find the mle \u02c6\u03bb = 0.2185.\nfor mcem, we used m(t) = 51+\u230at/10\u230b, where \u230az\u230b denotes the integer part of z. fifty\niterations were used altogether. both algorithms were initiated from \u03bb(0) = 0.5042,\nwhich is the mean of all 30 data values disregarding censoring.\n!\n\nn\nj 1/m(t)\n\n=\n\n2m(t)\nj=1 yt\n\nimproving the m step\n\n4.3.2\none of the appeals of the em algorithm is that the derivation and maximization of\nq(\u03b8|\u03b8(t)) is often simpler than incomplete-data maximum likelihood calculations,\nsince q(\u03b8|\u03b8(t)) relates to the complete-data likelihood. in some cases, however, the\nm step cannot be carried out easily even though the e step yielding q(\u03b8|\u03b8(t)) is\n\n "}, {"Page_number": 126, "text": "5\n\n.\n\n0\n\n4\n\n.\n\n0\n\n3\n\n.\n\n0\n\n)\nt\n(\n\u03bb\n\n2\n\n.\n\n0\n\n0\n\n10\n\n4.3 em variants\n\n113\n\n30\n\n40\n\n50\n\n20\niteration, t\n\nfigure 4.2 comparison of iterations for em (solid) and mcem (dotted) for the censored\nexponential data discussed in example 4.7.\n\nstraightforward. several strategies have been proposed to facilitate the m step in such\ncases.\n\necm algorithm meng and rubin\u2019s ecm algorithm replaces the m step\n4.3.2.1\nwith a series of computationally simpler conditional maximization (cm) steps [454].\neach conditional maximization is designed to be a simple optimization problem that\nconstrains \u03b8 to a particular subspace and permits either an analytical solution or a\nvery elementary numerical solution.\nwe call the collection of simpler cm steps after the tth e step a cm cycle.\nthus, the tth iteration of ecm is composed of the tth e step and the tth cm cycle.\nlet s denote the total number of cm steps in each cm cycle. for s = 1, . . . , s, the\nsth cm step in the tth cycle requires the maximization of q(\u03b8|\u03b8(t)) subject to (or\nconditional on) a constraint, say\n(4.52)\nwhere \u03b8(t+(s\u22121)/s) isthemaximizerfoundinthe(s \u2212 1)thcmstepofthecurrentcycle.\nwhen the entire cycle of s steps of cm has been completed, we set \u03b8(t+1) = \u03b8(t+s/s)\nand proceed to the e step for the (t + 1)th iteration.\nclearly any ecm is a gem algorithm (section 4.2.1), since each cm step\nincreases q. in order for ecm to be convergent, we need to ensure that each cm\ncycle permits search in any direction for a maximizer of q(\u03b8|\u03b8(t)), so that ecm\neffectively maximizes over the original parameter space for \u03b8 and not over some\nsubspace. precise conditions are discussed in [452, 454]; extensions of this method\ninclude [415, 456].\nthe art of constructing an effective ecm algorithm lies in choosing the con-\nstraintscleverly.usually,itisnaturaltopartition \u03b8 into s subvectors, \u03b8 = (\u03b81, . . . , \u03b8s).\n\ngs(\u03b8) = gs(\u03b8(t+(s\u22121)/s))\n\n "}, {"Page_number": 127, "text": "chapter 4 em optimization methods\n\n114\nthen in the sth cm step, one might seek to maximize q with respect to \u03b8s while hold-\ning all other components of \u03b8 fixed. this amounts to the constraint induced by the\nfunction gs(\u03b8) = (\u03b81, . . . , \u03b8s\u22121, \u03b8s+1, . . . , \u03b8s). a maximization strategy of this type\nhas previously been termed iterated conditional modes [36]. if the conditional maxi-\nmizations are obtained by finding the roots of score functions, the cm cycle can also\nbe viewed as a gauss\u2013seidel iteration (see section 2.2.5).\nalternatively, the sth cm step might seek to maximize q with respect to all\nother components of \u03b8 while holding \u03b8s fixed. in this case, gs(\u03b8) = \u03b8s. additional\nsystems of constraints can be imagined, depending on the particular problem context.\na variant of ecm inserts an e step between each pair of cm steps, thereby updating\nq at every stage of the cm cycle.\nexample 4.8 (multivariate regression with missing values) a particularly il-\nluminating example given by meng and rubin [454] involves multivariate regression\nwith missing values. let u1, . . . ,un be n independent d-dimensional vectors ob-\nserved from the d-variate normal model given by\n\nui \u223c nd*\u00b5i, \u0001+\n\n(4.53)\nfor ui = (ui1, . . . , uid) and \u00b5i = vi\u03b2, where the vi are known d \u00d7 p design ma-\ntrices, \u03b2 is a vector of p unknown parameters, and \u0001 is a d \u00d7 d unknown variance\u2013\ncovariance matrix. there are many cases where \u0001 has some meaningful structure, but\nwe consider \u0001 to be unstructured for simplicity. suppose that some elements of some\nui are missing.\nbegin by reordering the elements of ui, \u00b5i, and the rows of vi so that for each\ni, the observed components of ui are first and any missing components are last. for\neach ui, denote by \u03b2i and \u0001i the corresponding reorganizations of the parameters.\nthus, \u03b2i and \u0001i are completely determined by \u03b2, \u0001, and the pattern of missing data:\nthey do not represent an expansion of the parameter space.\nthis notational reorganization allows us to write ui = (uobs,i,umiss,i), \u00b5i =\n(\u00b5obs,i, \u00b5miss,i), and\n\ncross,i \u0001miss,i) .\n\u0001i =( \u0001obs,i \u0001cross,i\n\n\u0001t\n\n(4.54)\n\nthe full set of observed data can be denoted uobs = (uobs,1, . . . ,uobs,n).\n\nthe observed-data log likelihood function is\n\n1\n2\n\nn0i=1\n\n1\n2\n\nn0i=1\n\nlog|\u0001obs,i| \u2212\n\n(uobs,i \u2212 \u00b5obs,i)t\u0001\u22121\n\nobs,i(uobs,i \u2212 \u00b5obs,i)\nl(\u03b2, \u0001|uobs) = \u2212\nuptoanadditiveconstant.thislikelihoodisquitetedioustoworkwithortomaximize.\nnote, however, that the complete-data sufficient statistics are given by2n\ni=1 uij for\nj = 1, . . . , d and2n\ni=1 uijuik for j, k = 1, . . . , d.thus,theestepamountstofinding\nthe expected values of these sufficient statistics conditional on the observed data and\ncurrent parameter values \u03b2(t) and \u0001(t).\n\n "}, {"Page_number": 128, "text": "now for j = 1, . . . , d\nuij#####\ne: n0i=1\nij =: \u03b1\n\n(t)\nij\nuij\n\n(t)\n\na\n\nwhere\n\n4.3 em variants\n\n115\n\nuobs, \u03b2(t), \u0001(t); =\n\n(t)\na\nij ,\n\nn0i=1\n\nif uij is missing,\nif uij = uij is observed,\n\n(4.55)\n\n(4.56)\n\nb\n\n0\n\n(t)\n\n(t)\nijk\n\n(t)\nijk),\n\nand \u03b3\n\n(4.58)\n\n(4.57)\n\nwhere\n\n(t)\ni , \u0001\n\n(t)\n(a\nij a\n\nn0i=1\n\n(t)\nik + b\n\nif uij and uik are both missing,\notherwise,\n(t)\ni , \u0001\n\n(t)\ni }. similarly, for j, k = 1, . . . , d,\nuobs, \u03b2(t), \u0001(t); =\n\n(t)\nij = e{uij|uobs,i, \u03b2\nand \u03b1\nuijuik#####\ne: n0i=1\nijk =: \u03b3\n(t)\n(t)\nijk = cov{uij, uik|uobs,i, \u03b2\ni }.\n(t)\n(t)\nij and \u03b3\nijk is fairly straightforward. the\nfortunately, the derivation of the \u03b1\n(t)\n(t)\nconditional distribution of umiss,i|(uobs,i, \u03b2\ni ) is\ni , \u0001\n(t)\nobs,i), \u0001obs,i \u2212 \u0001cross,i\u0001\u22121\nmiss,i(uobs,i \u2212 \u00b5\n(t)\n(t)\nthe values for \u03b1\nij and \u03b3\nijk can be read from the mean vector and variance\u2013covariance\nmatrix of this distribution, respectively. knowing these, q(\u03b2, \u0001|\u03b2(t), \u0001(t)) can be\nformed following (4.26).\nhaving thus achieved the e step, we turn now to the m step. the high dimen-\nsionality of the parameter space and the complexity of the observed-data likelihood\nrenders difficult any direct implementation of the m step, whether by direct maxi-\nmization or by reference to the exponential family setup. however, implementing an\necm strategy is straightforward using s = 2 conditional maximization steps in each\ncm cycle.\ntreating \u03b2 and \u0001 separately allows easy constrained optimizations of q. first,\nif we impose the constraint that \u0001 = \u0001(t), then we can maximize the constrained\nversion of q(\u03b2, \u0001|\u03b2(t), \u0001(t)) with respect to \u03b2 by using the weighted least squares\nestimate\n\nmiss,i\u0001tcross,i9 .\n\n(t)\nmiss,i + \u0001cross,i\u0001\u22121\n\nn8\u00b5\n\n\u03b2(t+1/2)\n\n=( n0i=1\n\nvt\ni (\u0001\n\n(t)\n\ni )\u22121vi)\u22121( n0i=1\n\nvt\ni (\u0001\n\ni ) ,\ni )\u22121a(t)\n(t)\n\n(4.59)\n\n "}, {"Page_number": 129, "text": "chapter 4 em optimization methods\n\n116\nwhere a(t)\n(t)\nid )t and \u0001\nis treated as a known variance\u2013covariance\nmatrix. this ensures that q(\u03b2(t+1/2), \u0001(t)|\u03b2(t), \u0001(t)) \u2265 q(\u03b2(t), \u0001(t)|\u03b2(t), \u0001(t)). this\nconstitutes the first of two cm steps.\n\nthe second cm step follows from the fact that setting \u0001(t+2/2) equal to\n\n(t)\ni1 , . . . , a\n\ni = (a\n\n(t)\ni\n\nn\n\nn0i=1\n\n(ui \u2212 vi\u03b2(t+1/2))(ui \u2212 vi\u03b2(t+1/2))t####\n\ne:1\nmaximizes q(\u03b2, \u0001|\u03b2(t), \u0001(t)) with respect to \u0001 subject to the constraint that \u03b2 =\n(t)\n\u03b2(t+1/2), because this amounts to plugging in \u03b1\nijk values where necessary\nand computing the sample covariance matrix of the completed data. this update\nguarantees\n\nuobs, \u03b2(t+1/2), \u0001(t);\n\n(t)\nij and \u03b3\n\n(4.60)\n\nq(\u03b2(t+1/2), \u0001(t+2/2)\n\n|\u03b2(t), \u0001(t)) \u2265q(\u03b2(t+1/2), \u0001(t)\n\n|\u03b2(t), \u0001(t))\n\n|\u03b2(t), \u0001(t)).\n\n\u2265q(\u03b2(t), \u0001(t)\n\n(4.61)\ntogether, the two cm steps yield (\u03b2(t+1), \u0001(t+1)) = (\u03b2(t+1/2), \u0001(t+2/2)) and ensure\nan increase in the q function.\nthe e step and the cm cycle described here can each be implemented using\nfamiliar closed-form analytic results; no numerical integration or maximization is\nrequired. after updating the parameters with the cm cycle described above, we return\nto another e step, and so forth. in summary, ecm alternates between (i) creating\nupdated complete datasets and (ii) sequentially estimating \u03b2 and \u0001 in turn by fixing\nthe other at its current value and using the current completed-data component.\n!\n\nem gradient algorithm if maximization cannot be accomplished\n4.3.2.2\nanalytically, then one might consider carrying out each m step using an iterative\nnumerical optimization approach like those discussed in chapter 2. this would yield\nan algorithm that had nested iterative loops. the ecm algorithm inserts s conditional\nmaximization steps within each iteration of the em algorithm, also yielding nested\niteration.\nto avoid the computational burden of nested looping, lange proposed replac-\ning the m step with a single step of newton\u2019s method, thereby approximating the\nmaximum without actually solving for it exactly [407]. the m step is replaced with\nthe update given by\n\n\u03b8(t+1)\n\n= \u03b8(t)\n= \u03b8(t)\n\n\u2212 q\u2032\u2032(\u03b8|\u03b8(t))\u22121###\u03b8=\u03b8(t) q\u2032(\u03b8|\u03b8(t))###\u03b8=\u03b8(t)\n\u2212 q\u2032\u2032(\u03b8|\u03b8(t))\u22121###\u03b8=\u03b8(t) l\u2032(\u03b8(t)\n\n(4.62)\n(4.63)\nwhere l\u2032(\u03b8(t)|x) is the evaluation of the score function at the current iterate. note that\n(4.63) follows from the observation in section 4.2.3.4 that \u03b8(t) maximizes q(\u03b8|\u03b8(t)) \u2212\nl(\u03b8|x). this em gradient algorithm has the same rate of convergence to \u02c6\u03b8 as the full\nem algorithm. lange discusses conditions under which ascent can be ensured, and\n\n|x),\n\n "}, {"Page_number": 130, "text": "4\n\n.\n\n0\n\n3\n\n.\n\n0\n\ni\n\np\n\n2\n\n.\n\n0\n\n1\n\n.\n\n0\n\n4.3 em variants\n\n117\n\n\u03b8 (0)\n\nem\nem gradient\naitken accel. em\nquasi-newton em\n\n0.05\n\n0.15\n\n0.25\n\n0.35\n\npc\n\nfigure 4.3 steps taken by the em gradient algorithm (long dashes). ordinary em steps\nare shown with the solid line. steps from two methods from later sections (aitken and quasi-\nnewton acceleration) are also shown, as indicated in the key. the observed-data log likelihood\nisshownwiththegrayscale,withlightshadingcorrespondingtohighlikelihood.allalgorithms\nwere started from pc = pi = 1\n3.\nscalings of the update increment to speed convergence [407]. in particular, when y\nhas an exponential family distribution with canonical parameter \u03b8, ascent is ensured\nand the method matches that of titterington [634]. in other cases, the step can be\nscaled down to ensure ascent (as discussed in section 2.2.2.1), but inflating steps\nspeeds convergence. for problems with a high proportion of missing information,\nlange suggests considering doubling the step length [407].\nexample 4.9 (peppered moths, continued) continuing example 4.2, we apply\nthe em gradient algorithm to these data. it is straightforward to show\n(t)\n(t)\n(t)\n2n\nct + n\ntt + n\nit\n(1 \u2212 pc \u2212 pi)2 ,\n(t)\n(t)\n(t)\n2n\ntt + n\nct + n\nit\n(1 \u2212 pc \u2212 pi)2 ,\n\n(t)\n(t)\nci + n\ncc + n\np2\nc\n(t)\n(t)\n(t)\nii + n\nit + n\nci\np2\ni\n\nd2q(p|p(t))\nd2q(p|p(t))\n\n(4.64)\n\n(4.65)\n\n= \u2212\n\n= \u2212\n\ndp2\nc\n\ndp2\ni\n\n(t)\nct\n\n2n\n\n2n\n\n\u2212\n\n\u2212\n\nand\n\nd2q(p|p(t))\ndpcdpi = \u2212\n\n(t)\n(t)\n(t)\n2n\ntt + n\nct + n\nit\n(1 \u2212 pc \u2212 pi)2 .\n\n(4.66)\n\nfigure 4.3 shows the steps taken by the resulting em gradient algorithm, starting\n1\n3. step halving was implemented to ensure ascent. the first\nfrom pc = pi = pt =\n\n "}, {"Page_number": 131, "text": "chapter 4 em optimization methods\n\n118\nstep heads somewhat in the wrong direction, but in subsequent iterations the gradient\nsteps progress quite directly uphill. the ordinary em steps are shown for comparison\nin this figure.\n!\n\n4.3.3 acceleration methods\nthe slow convergence of the em algorithm is a notable drawback. several techniques\nhavebeensuggestedforusingtherelativelysimpleanalyticsetupfromemtomotivate\nparticular forms for newton-like steps. in addition to the two approaches described\nbelow, approaches that cleverly expand the parameter space in manners that speed\nconvergence without affecting the marginal inference about \u03b8 are topics of recent\ninterest [421, 456].\n(t+1)\n4.3.3.1 aitken acceleration let \u03b8\nem be the next iterate obtained by the stan-\ndard em algorithm from \u03b8(t). recall that the newton update to maximize the log\nlikelihood would be\n\n\u03b8(t+1)\n\n= \u03b8(t)\n\n\u2212 l\u2032\u2032(\u03b8(t)\n\n|x)\u22121l\u2032(\u03b8(t)\n\n(4.67)\nthe em framework suggests a replacement for l\u2032(\u03b8(t)|x). in section 4.2.3.4 we noted\nthat l\u2032(\u03b8(t)|x) = q\u2032(\u03b8|\u03b8(t))##\u03b8=\u03b8(t). expanding q\u2032 around \u03b8(t), evaluated at \u03b8\n(t+1)\nem , yields\n(t+1)\nem \u2212 \u03b8(t)),\n\n(4.68)\n\n|x).\n\nq\u2032(\u03b8|\u03b8(t))###\u03b8=\u03b8\n\nwhere \u02c6iy(\u03b8(t)) is defined in (4.31). since \u03b8\n\u03b8, the left-hand side of (4.68) equals zero. therefore\n(t+1)\nem \u2212 \u03b8(t)).\n\n(t+1)\n\nem \u2248 q\u2032(\u03b8|\u03b8(t))###\u03b8=\u03b8(t) \u2212 \u02c6iy(\u03b8(t))(\u03b8\nq\u2032(\u03b8|\u03b8(t))###\u03b8=\u03b8(t) \u2248 \u02c6iy(\u03b8(t))(\u03b8\n|x)\u22121\u02c6iy(\u03b8(t))(\u03b8\n\n\u2212 l\u2032\u2032(\u03b8(t)\n\nthus, from (4.67) we arrive at\n= \u03b8(t)\n\n\u03b8(t+1)\n\n(t+1)\nem \u2212 \u03b8(t)).\n\n(4.70)\nthis update\u2014relying on the approximation in (4.69)\u2014is an example of a general\nstrategy known as aitken acceleration and was proposed for em by louis [424].\naitken acceleration of em is precisely the same as applying the newton\u2013raphson\nmethod to find a zero of \u0001(\u03b8) \u2212 \u03b8, where \u0001 is the mapping defined by the ordinary\nem algorithm producing \u03b8(t+1) = \u0001(\u03b8(t)) [343].\nexample 4.10 (peppered moths, continued) this acceleration approach can be\napplied to example 4.2. obtaining l\u2032\u2032 is analytically more tedious than the simpler\nderivations employed for other em approaches to this problem. figure 4.3 shows the\naitken accelerated steps, which converge quickly to the solution. the procedure was\nstarted from pc = pi = pt =\n!\n\n1\n3, and step halving was used to ensure ascent.\n\n(t+1)\nem maximizes q(\u03b8|\u03b8(t)) with respect to\n\n(4.69)\n\n "}, {"Page_number": 132, "text": "4.3 em variants\n\n119\naitken acceleration is sometimes criticized for potential numerical instabilities\nand convergence failures [153, 344]. further, when l\u2032\u2032(\u03b8|x) is difficult to compute, this\napproach cannot be applied without overcoming the difficulty [20, 345, 450].\nsection 4.2.1 noted that the em algorithm converges at a linear rate that de-\npends on the fraction of missing information. the updating increment given in (4.70)\nis,looselyspeaking,scaledbytheratioofthecompleteinformationtotheobservedin-\nformation. thus, when a greater proportion of the information is missing, the nominal\nem steps are inflated more.\nnewton\u2019s method converges quadratically, but (4.69) only becomes a precise\napproximation as \u03b8(t) nears \u02c6\u03b8. therefore, we should only expect this acceleration\napproach to enhance convergence only as preliminary iterations hone \u03b8 sufficiently.\nthe acceleration should not be employed without having taken some initial iterations\nof ordinary em so that (4.69) holds.\n4.3.3.2 quasi-newton acceleration the quasi-newton optimization method\ndiscussed in section 2.2.2.3 produces updates according to\n|x)\n\n(4.71)\nfor maximizing l(\u03b8|x) with respect to \u03b8, where m(t) is an approximation to l\u2032\u2032(\u03b8(t)|x).\nwithin the em framework, one can decompose l\u2032\u2032(\u03b8(t)|x) into a part computed during\nem and a remainder. by taking two derivatives of (4.19), we obtain\n\n\u2212 (m(t))\u22121l\u2032(\u03b8(t)\n\n= \u03b8(t)\n\n\u03b8(t+1)\n\nl\u2032\u2032(\u03b8(t)\n\n|x) = q\u2032\u2032(\u03b8|\u03b8(t))###\u03b8=\u03b8(t) \u2212 h\u2032\u2032(\u03b8|\u03b8(t))###\u03b8=\u03b8(t)\n\nat iteration t. the remainder is the last term in (4.72); suppose we approximate it by\nb(t). then by using\n\n(4.72)\n\n(4.73)\n\nm(t)\n\n= q\u2032\u2032(\u03b8|\u03b8(t))###\u03b8=\u03b8(t) \u2212 b(t)\n\nin (4.71) we obtain a quasi-newton em acceleration.\na key feature of the approach is how b(t) approximates h\u2032\u2032(\u03b8(t)|\u03b8(t)). the idea\nis to start with b(0) = 0 and gradually accumulate information about h\u2032\u2032 as iterations\nprogress. the information is accumulated using a sequence of secant conditions, as\nis done in ordinary quasi-newton approaches (section 2.2.2.3).\n\nspecifically, we can require that b(t) satisfy the secant condition\n\nwhere\n\nand\n\nb(t+1)a(t)\n\n= b(t),\n\na(t)\n\n= \u03b8(t+1)\n\n\u2212 \u03b8(t)\n\nb(t)\n\n= h\u2032(\u03b8|\u03b8(t+1))###\u03b8=\u03b8(t+1) \u2212 h\u2032(\u03b8|\u03b8(t+1))###\u03b8=\u03b8(t) .\n\n(4.74)\n\n(4.75)\n\n(4.76)\n\n "}, {"Page_number": 133, "text": "chapter 4 em optimization methods\n\n120\nrecalling the update (2.49), we can satisfy the secant condition by setting\n\nb(t+1)\n\n= b(t)\n\n+ c(t)v(t)(v(t))t,\n\n(4.77)\n\nb(t)\n\nsmallest positive integer that makes m(t) negative definite.\n\nwhere v(t) = b(t) \u2212 b(t)a(t) and c(t) = 1/[(v(t))ta(t)].\nlange proposed this quasi-newton em algorithm, along with several sug-\ngested strategies for improving its performance [408]. first, he suggested starting\nwith b(0) = 0. note that this implies that the first increment will equal the em gra-\ndient increment. indeed, the em gradient approach is exact newton\u2013raphson for\nmaximizing q(\u03b8|\u03b8(t)), whereas the approach described here evolves into approximate\nnewton\u2013raphson for maximizing l(\u03b8|x).\nsecond, davidon\u2019s [134] update is troublesome if (v(t))ta(t) = 0 or is small\ncompared to \u2225v(t)\u2225 \u00b7 \u2225a(t)\u2225. in such cases, we may simply set b(t+1) = b(t).\nthird, there is no guarantee that m(t) = q\u2032\u2032(\u03b8|\u03b8(t))##\u03b8=\u03b8(t) \u2212 b(t) will be negative\ndefinite, which would ensure ascent at the tth step. therefore, we may scale b(t)\nand use m(t) = q\u2032\u2032(\u03b8|\u03b8(t))##\u03b8=\u03b8(t) \u2212 \u03b1(t)b(t) where, for example, \u03b1(t) = 2\u2212m for the\n\n= h\u2032(\u03b8|\u03b8(t+1))###\u03b8=\u03b8(t+1) \u2212 h\u2032(\u03b8|\u03b8(t+1))###\u03b8=\u03b8(t)\n= 0 \u2212 h\u2032(\u03b8|\u03b8(t+1))###\u03b8=\u03b8(t)\n= q\u2032(\u03b8|\u03b8(t))###\u03b8=\u03b8(t) \u2212 q\u2032(\u03b8|\u03b8(t+1))###\u03b8=\u03b8(t) .\n\nfinally, note that b(t) may be expressed entirely in terms of q\u2032 functions since\n(4.78)\n(4.79)\n(4.80)\nequation (4.79) follows from (4.19) and the fact that l(\u03b8|x) \u2212 q(\u03b8|\u03b8(t)) has its min-\nimum at \u03b8 = \u03b8(t). the derivative at this minimum must be zero, forcing l\u2032(\u03b8(t)|x) =\nq\u2032(\u03b8|\u03b8(t))##\u03b8=\u03b8(t), which allows (4.80).\nexample 4.11 (peppered moths, continued) we can apply quasi-newton\nacceleration to example 4.2, using the expressions for q\u2032\u2032 given in (4.64)\u2013(4.66)\n1\nand obtaining b(t) from (4.80). the procedure was started from pc = pi = pt =\n3\nand b(0) = 0, with step halving to ensure ascent.\ntheresultsareshowninfigure4.3.notethatb(0) = 0meansthatthefirstquasi-\nnewton em step will match the first em gradient step. the second quasi-newton em\nstep completely overshoots the ridge of highest likelihood, resulting in a step that is\njust barely uphill. in general, the quasi-newton em procedure behaves like other\nquasi-newton methods: there can be a tendency to step beyond the solution or to\nconverge to a local maximum rather than a local minimum. with suitable safeguards,\nthe procedure is fast and effective in this example.\n!\nthe quasi-newton em requires the inversion of m(t) at step t. lange et al. de-\nscribe a quasi-newton approach based on the approximation of \u2212l\u2032\u2032(\u03b8(t)|x) by some\nm(t) that relies on an inverse-secant update [409, 410]. in addition to avoiding compu-\ntationallyburdensomematrixinversions,suchupdatesto \u03b8(t) andb(t) canbeexpressed\nentirely in terms of l\u2032(\u03b8(t)|x) and ordinary em increments when the m step is solvable.\n\n "}, {"Page_number": 134, "text": "4.3 em variants\n\n121\n\ntable 4.2 frequencies of respondents reporting numbers of risky sexual encounters;\nsee problem 4.2.\nencounters, i\nfrequency, ni\n\n4\n109\n\n0\n379\n\n1\n299\n\n3\n145\n\n2\n222\n\n5\n95\n\n6\n73\n\n7\n59\n\n8\n45\n\nencounters, i\nfrequency, ni\n\n9\n30\n\n10\n24\n\n11\n12\n\n12\n4\n\n13\n2\n\n14\n0\n\n15\n1\n\n16\n1\n\njamshidian and jennrich elaborate on inverse-secant updating and discuss the\nmore complex bfgs approach [344]. these authors also provide a useful survey of\na variety of em acceleration approaches and a comparison of effectiveness. some\nof their approaches converge faster on examples than does the approach described\nabove. in a related paper, they present a conjugate gradient acceleration of em [343].\n\nproblems\n4.1. recall the peppered moth analysis introduced in example 4.2. in the field, it is quite\ndifficult to distinguish the insularia and typica phenotypes due to variations in wing\ncolor and mottle. in addition to the 622 moths mentioned in the example, suppose the\nsample collected by the researchers actually included nu = 578 more moths that were\nknown to be insularia or typical but whose exact phenotypes could not be determined.\na. derive the em algorithm for maximum likelihood estimation of pc, pi, and pi for\nb. apply the algorithm to find the mles.\nc. estimate the standard errors and pairwise correlations for \u02c6pc, \u02c6pi, and \u02c6pi using the\nd. estimate the standard errors and pairwise correlations for \u02c6pc, \u02c6pi, and \u02c6pi by boot-\ne. implement the em gradient algorithm for these data. experiment with step halving\n\nthis modified problem having observed data nc, ni, nt, and nu as given above.\n\nsem algorithm.\n\nstrapping.\n\nto ensure ascent and with other step scalings that may speed convergence.\n\nf. implement aitken accelerated em for these data. use step halving.\ng. implement quasi-newton em for these data. compare performance with and with-\n\nout step halving.\n\nh. comparetheeffectivenessandefficiencyofthestandardemalgorithmandthethree\nvariants in (e), (f), and (g). use step halving to ensure ascent with the three variants.\nbase your comparison on a variety of starting points. create a graph analogous to\nfigure 4.3.\n\n4.2. epidemiologists are interested in studying the sexual behavior of individuals at risk\nfor hiv infection. suppose 1500 gay men were surveyed and each was asked how\nmany risky sexual encounters he had in the previous 30 days. let ni denote the number\nof respondents reporting i encounters, for i = 1, . . . ,16. table 4.2 summarizes the\nresponses.\n\n "}, {"Page_number": 135, "text": "122\n\nchapter 4 em optimization methods\n\nthese data are poorly fitted by a poisson model. it is more realistic to assume\nthat the respondents comprise three groups. first, there is a group of people who,\nfor whatever reason, report zero risky encounters even if this is not true. suppose a\nrespondent has probability \u03b1 of belonging to this group.\nwith probability \u03b2, a respondent belongs to a second group representing typical\nbehavior. such people respond truthfully, and their numbers of risky encounters are\nassumed to follow a poisson(\u00b5) distribution.\nfinally, with probability 1 \u2212 \u03b1 \u2212 \u03b2, a respondent belongs to a high-risk group.\nsuch people respond truthfully, and their numbers of risky encounters are assumed to\nfollow a poisson(\u03bb) distribution.\nthe parameters in the model are \u03b1, \u03b2, \u00b5, and \u03bb. at the tth iteration of em, we\nuse \u03b8(t) = (\u03b1(t), \u03b2(t), \u00b5(t), \u03bb(t)) to denote the current parameter values. the likelihood of\nthe observed data is given by\n\nl(\u03b8|n0, . . . , n16) \u221d\n\n16<i=03 \u03c0i(\u03b8)\ni! 6ni\n\n,\n\n(4.81)\n\nwhere\n\n\u03c0i(\u03b8) = \u03b11{i=0} + \u03b2\u00b5i exp{\u2212\u00b5} + (1 \u2212 \u03b1 \u2212 \u03b2)\u03bbi exp{\u2212\u03bb}\n\n(4.82)\n\ndefine\n\ni=0 ni = 1500.\n\nfor i = 1, . . . ,16.\nthe observed data are n0, . . . , n16. the complete data may be construed to be\nnz,0, nt,0, . . . , nt,16, and np,0, . . . , np,16, where nk,i denotes the number of respondents in\ngroup k reporting i risky encounters and k = z, t, and p correspond to the zero, typical,\nandpromiscuousgroups,respectively.thus, n0 = nz,0 + nt,0 + np,0 and ni = nt,i + np,i\nfor i = 1, . . . ,16. let n =216\nz0(\u03b8) =\nti(\u03b8) =\npi(\u03b8) =\n\n\u03b1\n\u03c00(\u03b8) ,\n\u03b2\u00b5i exp{\u2212\u00b5}\n(1 \u2212 \u03b1 \u2212 \u03b2)\u03bbi exp{\u2212\u03bb}\n\n(4.84)\n\n(4.85)\n\n(4.83)\n\n\u03c0i(\u03b8)\n\n\u03c0i(\u03b8)\n\n,\n\nfor i = 0, . . . ,16. these correspond to probabilities that respondents with i risky\nencounters belong to the various groups.\na. show that the em algorithm provides the following updates:\n\n\u03b1(t+1) =\n\u03b2(t+1) =\n\nn0z0(\u03b8(t))\n\n,\n\nn\nniti(\u03b8(t))\n\nn\n\n160i=0\n\n(4.86)\n\n(4.87)\n\n,\n\n "}, {"Page_number": 136, "text": "4.3 em variants\n\n123\n\n(4.88)\n\n(4.89)\n\n\u00b5(t+1) = 216\ni=0 i niti(\u03b8(t))\n216\ni=0 niti(\u03b8(t)) ,\n\u03bb(t+1) = 216\ni=0 i nipi(\u03b8(t))\n216\ni=0 nipi(\u03b8(t)) .\n\nb. estimate the parameters of the model, using the observed data.\nc. estimate the standard errors and pairwise correlations of your parameter estimates,\n\nusing any available method.\n\n4.3. the website for this book contains 50 trivariate data points drawn from the n3(\u00b5, \u0001)\ndistribution. some data points have missing values in one or more coordinates. only\n27 of the 50 observations are complete.\na. derive the em algorithm for joint maximum likelihood estimation of \u00b5 and \u0001. it\nis easiest to recall that the multivariate normal density is in the exponential family.\nb. determine the mles from a suitable starting point. investigate the performance of\n\nthe algorithm, and comment on your results.\n\nc. consider bayesian inference for \u00b5 when\n1\n1.2\n0.6 0.5 0.5\n1.2\n\n0.6\n\n\u0001 =\u239b\u239d\n\n0.5 3.0\u239e\u23a0\n\nis known. assume independent priors for the three elements of \u00b5. specifically, let\nthe jth prior be\n\nf(\u00b5j) =\n\nexp{\u2212(\u00b5j \u2212 \u03b1j)/\u03b2j}\n\u03b2j&1 + exp{\u2212(\u00b5j \u2212 \u03b1j)/\u03b2j}\u20192 ,\n\nwhere (\u03b11, \u03b12, \u03b13) = (2,4,6) and \u03b2j = 2 for j = 1,2,3. comment on difficul-\nties that would be faced in implementing a standard em algorithm for estimating\nthe posterior mode for \u00b5. implement a gradient em algorithm, and evaluate its\nperformance.\nd. suppose that \u0001 is unknown in part (c) and that an improper uniform prior is adopted,\nthat is, f(\u0001) \u221d 1 for all positive definite \u0001. discuss ideas for how to estimate the\nposterior mode for \u00b5 and \u0001.\n\n4.4. suppose we observe lifetimes for 14 gear couplings in certain mining equipment, as\ngiven in table 4.3 (in years). some of these data are right censored because the equip-\nmentwasreplacedbeforethegearcouplingfailed.thecensoreddataareinparentheses;\nthe actual lifetimes for these components may be viewed as missing.\nmodel these data with the weibull distribution, having density function f(x) =\nabxb\u22121 exp{\u2212axb} for x > 0 and parameters a and b. recall that problem 2.3 in\nchapter 2 provides more details about such models. construct an em algorithm to\nestimate a and b. since the q function involves expectations that are analytically un-\navailable, adopt the mcem strategy where necessary. also, optimization of q cannot\nbe completed analytically. therefore, incorporate the ecm strategy of conditionally\nmaximizing with respect to each parameter separately, applying a one-dimensional\n\n "}, {"Page_number": 137, "text": "124\n\nchapter 4 em optimization methods\n\ntable 4.3 fourteen lifetimes for mining equipment gear couplings,\nin years.\nright-censored values are in parenthesis. in these cases, we know only that the\nlifetime was at least as long as the given value.\n\n(6.94)\n10.24\n\n5.50\n4.56\n\n4.54\n9.42\n\n2.14\n(4.55)\n\n(3.65)\n(4.15)\n\n(3.40)\n5.64\n\n(4.38)\n(10.23)\n\nnewton-like optimizer where needed. past observations suggest (a, b) = (0.003,2.5)\nmay be a suitable starting point. discuss the convergence properties of the procedure\nyou develop, and the results you obtain. what are the advantages and disadvantages of\nyour technique compared to direct maximization of the observed-data likelihood using,\nsay, a two-dimensional quasi-newton approach?\n\n4.5. a hidden markov model (hmm) can be used to describe the joint probability of a\nsequence of unobserved (hidden) discrete-state variables, h = (h0, . . . , hn), and a\nsequence of corresponding observed variables o = (o0, . . . , on) for which oi is de-\npendent on hi for each i. we say that hi emits oi; consideration here is limited to\ndiscrete emission variables. let the state spaces for elements of h and o be h and e,\nrespectively.\nlet o\u2264j and o>j denote the portions of o with indices not exceeding j and\nexceeding j, respectively, and define the analogous partial sequences for h. under an\nhmm, the hi have the markov property\n\np[hi|h\u2264i\u22121, o0] = p[hi|hi\u22121]\n\nand the emissions are conditionally independent, so\n\n(4.90)\n\np[oi|h,o\u2264i\u22121,o>i] = p[oi|hi].\n\n(4.91)\ntime-homogeneous transitions of the hidden states are governed by transition prob-\nabilities p(h, h\u2217) = p[hi+1 = h\u2217|hi = h] for h, h\u2217 \u2208 h. the distribution for h0 is\nparameterized by \u03c0(h) = p[h0 = h] for h \u2208 h. finally, define emission probabilities\ne(h, o) = p[oi = o|hi = h]for h \u2208 hand o \u2208 e.thentheparameterset \u03b8 = (\u03c0,p,e)\ncompletely parameterizes the model, where \u03c0 is a vector of initial-state probabilities,\np is a matrix of transition probabilities, and e is a matrix of emission probabilities.\n\nfor an observed sequence o, define the forward variables to be\n\n\u03b1(i, h) = p[o\u2264i = o\u2264i, hi = h]\n\nand the backward variables to be\n\n(4.92)\n\n\u03b2(i, h) = p[o>i = o>i|hi = h]\n\n(4.93)\nfor i = 1, . . . , nandeach h \u2208 h.ournotationsuppressesthedependenceoftheforward\nand backward variables on \u03b8. note that\n(4.94)\n\n\u03c0(h)e(h, o0)\u03b2(0, h).\n\np[o = o|\u03b8] =0h\u2208h\n\n\u03b1(n, h) =0h\u2208h\n\nthe forward and backward variables are also useful for computing the probability that\nstate h occurred at the ith position of the sequence given o = o according to p[hi =\nh|o = o, \u03b8] =2h\u2208h \u03b1(i, h)\u03b2(i, h)/p[o = o|\u03b8], and expectations of functions of the\nstates with respect to these probabilities.\n\n "}, {"Page_number": 138, "text": "4.3 em variants\n\n125\n\na. show that the following algorithms can be used to calculate \u03b1(i, h) and \u03b2(i, h). the\n\n\u03b1(i, h\u2217)p(h\u2217, h)e(h, oi+1).\n\nforward algorithm is\n\u2022 initialize \u03b1(0, h) = \u03c0(h)e(h, o0).\n\u2022 for i = 0, . . . , n \u2212 1, let \u03b1(i + 1, h) =2h\u2217\u2208h\nthe backward algorithm is\n\u2022 initialize \u03b2(n, h) = 1.\n\u2022 for i = n, . . . ,1, let \u03b2(i \u2212 1, h) =2h\u2217\u2208h\nthese algorithms provide very efficient methods for finding p[o = o|\u03b8] and other\nuseful probabilities, compared to naively summing over all possible sequences of\nstates.\nb. let n(h) denote the number of times h0 = h, let n(h, h\u2217) denote the number of\ntransitions from h to h\u2217, and let n(h, o) denote the number of emissions of o when\nthe underlying state is h. prove that these random variables have the following\nexpectations:\n\np(h, h\u2217)e(h\u2217, oi)\u03b2(h, i).\n\n\u03b1(0, h)\u03b2(0, h)\ne{n(h)} =\np[o = o|\u03b8] ,\nn\u221210i=0\ne{n(h, h\u2217)} =\ne{n(h, o)} = 0i: oi=o\n\np[o = o|\u03b8]\n\n\u03b1(i, h)p(h, h\u2217)e(h\u2217, oi+1)\u03b2(i + 1, h\u2217)\n\u03b1(i, h)\u03b2(i, h)\np[o = o|\u03b8] .\n\n,\n\n(4.95)\n\n(4.96)\n\n(4.97)\n\nc. the baum\u2013welch algorithm efficiently estimates the parameters of an hmm [25].\nfitting these models has proven extremely useful in diverse applications including\nstatistical genetics, signal processing and speech recognition, problems involving\nenvironmental time series, and bayesian graphical networks [172, 236, 361, 392,\n523]. starting from some initial values \u03b8(0), the baum\u2013welch algorithm proceeds\nvia iterative application of the following update formulas:\n\n\u03c0(h)(t+1) =\np(h, h\u2217)(t+1) =\ne(h, o)(t+1) =\n\n,\n\ne{n(h)|\u03b8(t)}\n2h\u2217\u2208h\ne{n(h\u2217)|\u03b8(t)}\ne{n(h, h\u2217)|\u03b8(t)}\n2h\u2217\u2217\u2208h\ne{n(h, h\u2217\u2217)|\u03b8(t)}\ne{n(h, o)|\u03b8(t)}\n2o\u2217\u2208e\ne{n(h, o\u2217)|\u03b8(t)}\n\n.\n\n,\n\n(4.98)\n\n(4.99)\n\n(4.100)\n\nprove that the baum\u2013welch algorithm is an em algorithm. it is useful to begin by\nnoting that the complete data likelihood is given by\n\n<h\u2208h\n\n\u03c0(h)n(h)<h\u2208h<o\u2208e\n\ne(h, o)n(h,o)<h\u2208h<h\u2217\u2208h\n\np(h, h\u2217)n(h,h\u2217).\n\n(4.101)\n\nd. consider the following scenario. in flip\u2019s left pocket is a penny; in his right pocket\nis a dime. on a fair toss, the probability of showing a head is p for the penny\n\n "}, {"Page_number": 139, "text": "126\n\nchapter 4 em optimization methods\n\nand d for the dime. flip randomly chooses a coin to begin, tosses it, and reports the\noutcome(headsortails)withoutrevealingwhichcoinwastossed.then,flipdecides\nwhether to use the same coin for the next toss, or to switch to the other coin. he\nswitches coins with probability s, and retains the same coin with probability 1 \u2212 s.\nthe outcome of the second toss is reported, again not revealing the coin used. this\nprocess is continued for a total of 200 coin tosses. the resulting sequence of heads\nand tails is available from the website for this book. use the baum\u2013welch algorithm\nto estimate p, d, and s.\n\ne. only for students seeking extra challenge: derive the baum\u2013welch algorithm for\nthe case when the dataset consists of m independent observation sequences arising\nfromahmm.simulatesuchdata,followingthecoinexampleabove.(youmaywish\ntomimicthesingle-sequencedata,whichweresimulatedusing p = 0.25, d = 0.85,\nand s = 0.1.) code the baum\u2013welch algorithm, and test it on your simulated data.\nin addition to considering multiple sequences, hmms and the baum\u2013welch algorithm\ncan be generalized for estimation based on more general emission variables and emis-\nsion and transition probabilities that have more complex parameterizations, including\ntime inhomogeneity.\n\n "}, {"Page_number": 140, "text": "p a r t ii\nintegration and\nsimulation\n\nstatisticians attempt to infer what is and what could be. to do this, we\noften rely on what is expected. in statistical contexts, expectations are usually\nexpressed as integrals with respect to probability distributions.\n\nthe value of an integral can be derived analytically or numerically. since\nan analytic solution is usually impossible for all but the simplest statistical\nproblems, a numerical approximation is often used.\n\nnumerical quadrature approximates the integral by systematically parti-\ntioning the region of integration into smaller parts, applying a simple approx-\nimation for each part, and then combining the results. we begin this portion\nof the book with coverage of quadrature techniques.\n\nthe monte carlo method attacks the problem by simulating random\nrealizations and then averaging these to approximate the theoretical average.\nwe describe these methods and explore a variety of strategies for improving\ntheir performance. markov chain monte carlo is a particularly important\nsimulation technique, and we devote two chapters to such methods.\n\nalthough we initially pose monte carlo methods as integration tools,\nit becomes increasingly apparent in these chapters that such probabilistic\nmethods have broad utility for simulating random variates irrespective of how\nthose simulations will be used. our examples and exercises illustrate a variety\nof simulation applications.\n\ncomputational statistics, second edition. geof h. givens and jennifer a. hoeting.\n\u00a9 2013 john wiley & sons, inc. published 2013 by john wiley & sons, inc.\n\n127\n\n "}, {"Page_number": 141, "text": "chapter 5\nnumerical integration\n\nconsider a one-dimensional integral of the form! b\n\na f(x) dx. the value of the integral\ncan be derived analytically for only a few functions f. for the rest, numerical approx-\nimations of the integral are often useful. approximation methods are well known by\nboth numerical analysts [139, 353, 376, 516] and statisticians [409, 630].\napproximation of integrals is frequently required for bayesian inference since\na posterior distribution may not belong to a familiar distributional family. integral\napproximation is also useful in some maximum likelihood inference problems when\nthe likelihood itself is a function of one or more integrals. an example of this occurs\nwhen fitting generalized linear mixed models, as discussed in example 5.1.\na f(x) dx, partition the interval [a, b] into\nn subintervals, [xi, xi+1] for i = 0, . . . , n \u2212 1, with x0 = a and xn = b. then\n! b\na f(x) dx =\"n\u22121\nf(x) dx. this composite rule breaks the whole integral into\nmany smaller parts, but postpones the question of how to approximate any single part.\nthe approximation of a single part will be made using a simple rule. within the\ninterval [xi, xi+1], insert m + 1 nodes, x\u2217ij for j = 0, . . . , m. figure 5.1 illustrates the\nrelationships among the interval [a, b], the subintervals, and the nodes. in general,\nnumerical integration methods require neither equal spacing of subintervals or nodes\nnor equal numbers of nodes within each subinterval.\na simple rule will rely upon the approximation\n\nto initiate an approximation of ! b\n\ni=0 ! xi+1\n\nxi\n\n# xi+1\n\nxi\n\nf(x) dx \u2248\n\nm$j=0\n\naijf%x\u2217ij&\n\n(5.1)\n\nfor some set of constants aij. the overall integral is then approximated by the com-\nposite rule that sums (5.1) over all subintervals.\n\n5.1 newton\u2013c \u02c6otes quadrature\na simple and flexible class of integration methods consists of the newton\u2013c\u02c6otes\nrules. in this case, the nodes are equally spaced in [xi, xi+1], and the same number\nof nodes is used in each subinterval. the newton\u2013c\u02c6otes approach replaces the true\nintegrand with a polynomial approximation on each subinterval. the constants aij are\n\ncomputational statistics, second edition. geof h. givens and jennifer a. hoeting.\n\u00a9 2013 john wiley & sons, inc. published 2013 by john wiley & sons, inc.\n\n129\n\n "}, {"Page_number": 142, "text": "130\n\nchapter 5 numerical integration\n\n)\nx\n(\nf\n\na = x0\n\nx*\n11 x*\n\n12\n\netc.\nx2 = x*\n\n1m\n\nx1 = x*\n\n10\n\netc.\n\nb = xn\n\nfigure 5.1 to integrate f between a and b, the interval is partitioned into n subintervals,\n[xi, xi+1], each of which is further partitioned using m + 1 nodes, x\u2217i0, . . . , x\u2217im. note that when\nm = 0, the subinterval [xi, xi+1] contains only one interior node, x\u2217i0 = xi.\nj=0 aijf%x\u2217ij& equals the integral of an interpolating polynomial\nselected so that\"m\non [xi, xi+1] that matches the value of f at the nodes within this subinterval. the\nremainder of this section reviews common newton\u2013c\u02c6otes rules.\n\n5.1.1 riemann rule\nconsider the case when m = 0. suppose we define x\u2217i0 = xi, and ai0 = xi+1 \u2212 x\u2217i0.\nthe simple riemann rule amounts to approximating f on each subinterval by a\nconstant function, f (xi), whose value matches that of f at one point on the interval.\nin other words,\n\n# xi+1\n\nxi\n\nf(x) dx \u2248# xi+1\n\nxi\n\nf (xi) dx = (xi+1 \u2212 xi) f (xi) .\n\n(5.2)\n\nthe composite rule sums n such terms to provide an approximation to the integral\nover [a, b].\nsuppose the xi are equally spaced so that each subinterval has the same length\nh = (b \u2212 a)/n. then we may write xi = a + ih, and the composite rule is\n\n# xi+1\n\nxi\n\nf(x) dx \u2248 h\n\nn\u22121$i=0\n\nf(a + ih) = \u2019r(n).\n\n(5.3)\n\nas figure 5.2 shows, this corresponds to the riemann integral studied in ele-\nmentary calculus. furthermore, there is nothing special about the left endpoints of\nthe subintervals: we easily could have replaced f (xi) with f (xi+1) in (5.2).\n\n "}, {"Page_number": 143, "text": "5.1 newton\u2013c \u02c6otes quadrature\n\n131\n\n)\nx\n(\nf\n\nriemann\nm 0=\n\ntrapezoidal\n\nm 1=\n\nsimpson\u2019s\nm 2=\n\ni0\n\nxi = x*\nxi+1 = x*\nfigure 5.2 approximation (dashed) to f (solid) provided on the subinterval [xi, xi+1], for\nthe riemann, trapezoidal, and simpson\u2019s rules.\n\nxi+1 = x*\n\nxi = x*\n\nxi = x*\n\nxi+1\n\nx*\n\ni0\n\ni0\n\ni1\n\ni2\n\ni1\n\nthe approximation given by (5.3) converges to the true value of the integral\nas n \u2192 \u221e, by definition of the riemann integral of an integrable function. if f is\na polynomial of zero degree (i.e., a constant function), then f is constant on each\nsubinterval, so the riemann rule is exact.\nwhen applying the composite riemann rule, it makes sense to calculate a\n\nsequence of approximations, say \u2019r(nk), for an increasing sequence of numbers of\nsubintervals, nk, as k = 1,2, . . .. then, convergence of\u2019r(nk) can be monitored using\nan absolute or relative convergence criterion as discussed in chapter 2. it is particu-\nlarly efficient to use nk+1 = 2nk so that half the subinterval endpoints at the next step\ncorrespond to the old endpoints from the previous step. this avoids calculations of f\nthat are effectively redundant.\nexample 5.1 (alzheimer\u2019s disease) data from 22 patients with alzheimer\u2019s dis-\nease, an ailment characterized by progressive mental deterioration, are shown in\ntable 5.1. in each of five consecutive months, patients were asked to recall words\nfrom a standard list given previously. the number of words recalled by each patient\nwas recorded. the patients in table 5.1 were receiving an experimental treatment\nwith lecithin, a dietary supplement. it is of interest to investigate whether memory\nimproved over time. the data for these patients (and 25 control cases) are available\nfrom the website for this book and are discussed further in [155].\nconsider fitting these data with a very simple generalized linear mixed model\n[69, 670]. let yij denote the number of words recalled by the ith individual in the\njth month, for i = 1, . . . ,22 and j = 1, . . . ,5. suppose yij|\u03bbij have independent\npoisson(\u03bbij) distributions, where the mean and variance of yij is \u03bbij. let xij = (1 j)t\nbe a covariate vector: aside from an intercept term, only the month is used as a\n\n "}, {"Page_number": 144, "text": "132\n\nchapter 5 numerical integration\n\ntable 5.1 words recalled on five consecutive monthly tests for 22 alzheimer\u2019s patients receiving\nlecithin.\n\nmonth\n1\n2\n3\n4\n5\n\nmonth\n1\n2\n3\n4\n5\n\n1\n9\n12\n16\n17\n18\n\n12\n1\n3\n2\n4\n5\n\n2\n6\n7\n10\n15\n16\n\n13\n6\n7\n7\n9\n10\n\n3\n13\n18\n14\n21\n21\n\n14\n0\n3\n3\n4\n6\n\n4\n9\n10\n12\n14\n15\n\n15\n18\n18\n19\n22\n22\n\npatient\n6\n11\n11\n12\n14\n16\n\npatient\n17\n10\n14\n16\n17\n19\n\n5\n6\n7\n8\n9\n12\n\n16\n15\n15\n15\n18\n19\n\n7\n7\n10\n11\n12\n14\n\n18\n6\n6\n7\n9\n10\n\n8\n8\n18\n19\n19\n22\n\n19\n9\n9\n13\n16\n20\n\n9\n3\n3\n3\n7\n8\n\n20\n4\n3\n4\n7\n9\n\n10\n4\n10\n11\n17\n18\n\n21\n4\n13\n13\n16\n19\n\n11\n11\n10\n10\n15\n16\n\n22\n10\n11\n13\n17\n21\n\npredictor. let \u03b2 = (\u03b20 \u03b21)t be a parameter vector corresponding to x. then we model\nthe mean of yij as\n(5.4)\nwhere the \u03b3i are independent n(0, \u03c32\n\u03b3) random effects. this model allows separate\nshifts in \u03bbij on the log scale for each patient, reflecting the assumption that there may\nbe substantial between-patient variation in counts. this is reasonable, for example, if\nthe baseline conditions of patients prior to the start of treatment varied.\n\n\u03bbij = exp{xt\n\nij\u03b2 + \u03b3i},\n\nwith this model, the likelihood is\n\n5(j=1\n\n\u03b3)\n\n\u03b3|y& =\n\nl%\u03b2, \u03c32\n\n22(i=1# )\u03c6(\u03b3i;0, \u03c32\n22(i=1\nli%\u03b2, \u03c32\n\u03b3|y&,\nwhere f(yij|\u03bbij) is the poisson density, \u03c6(\u03b3i;0, \u03c32\nwith mean zero and variance \u03c32\nthe log likelihood is therefore\n\n=\n\nf(yij|\u03bbij)*d\u03b3i\n\n(5.5)\n\n\u03b3) is the normal density function\n\u03b3, and y is a vector of all the observed response values.\n\nl%\u03b2, \u03c32\n\n\u03b3|y& =\n\n22$i=1\n\nli%\u03b2, \u03c32\n\u03b3|y&,\n\n(5.6)\n\nwhere li denotes the contribution to the log likelihood made by data from the ith\npatient.\n\n "}, {"Page_number": 145, "text": "4\n\u2212\n0\n1\n\n \nf\no\n\n \ns\nt\ni\nn\nu\nn\ni\n \n)\n7\n\n \n\n.\n\n5\n(\n \nf\no\n\n \n\nd\nn\na\nr\ng\ne\nt\nn\ni\n\n2\n\n1\n\n0\n\n\u22120.05\n\n5.1 newton\u2013c \u02c6otes quadrature\n\n133\n\n0.05\n\n0\n\u03b3 1\n\nfigure 5.3 example 5.1 seeks to integrate this function, which arises from a generalized\nlinear mixed model for data on patients treated for alzheimer\u2019s disease.\n\nto maximize the log likelihood, we must differentiate l with respect to each\nparameter and solve the corresponding score equations. this will require a numerical\nroot-finding procedure, since the solution cannot be determined analytically. in this\nexample, we look only at one small portion of this overall process: the evaluation of\ndli/d\u03b2k for particular given values of the parameters and for a single i and k. this\nevaluation would be repeated for the parameter values tried at each iteration of a\nroot-finding procedure.\nlet i = 1 and k = 1. the partial derivative with respect to the parameter for\nmonthly rate of change is dl1/d\u03b21 = (dl1/d\u03b21)+l1, where l1 is implicitly defined\n\nin (5.5). further,\n\nd\n\nd\n\n\u03b3)\n\nd\u03b21# )\u03c6(\u03b31;0, \u03c32\ndl1\nd\u03b21 =\nd\u03b21)\u03c6(\u03b31;0, \u03c32\n=#\n\u03b3)\u23a1\u23a3\n5$j=1\n=# \u03c6(\u03b31;0, \u03c32\n\n\u03b3)\n\n5(j=1\nf(y1j|\u03bb1j)* d\u03b31\n5(j=1\nf(y1j|\u03bb1j)* d\u03b31\nj(y1j \u2212 \u03bb1j)\u23a4\u23a6\n5(j=1\n\nf(y1j|\u03bb1j) d\u03b31,\n\n(5.7)\n\nwhere \u03bb1j = exp{\u03b20 + j\u03b21 + \u03b31}. the last equality in (5.7) follows from standard\nanalysis of generalized linear models [446].\nsuppose, at the very first step of optimization, we start with initial values\n\u03b2 = (1.804, 0.165)and \u03c32\n\u03b3 = 0.0152.thesestartingvalues werechosenusing simple\nexploratory analyses. using these values for \u03b2 and \u03c32\n\u03b3, the integral we seek in (5.7)\nhas the integrand shown in figure 5.3. the desired range of integration is the entire\n\n "}, {"Page_number": 146, "text": "134\n\nchapter 5 numerical integration\n\ntable 5.2 estimates of the integral in (5.7) using the riemann rule with various\nnumbers of subintervals. all estimates are multiplied by a factor of 105. errors\nfor use in a relative convergence criterion are given in the final column.\nsubintervals\n\nrelative error\n\nestimate\n\n2\n4\n8\n16\n32\n64\n128\n256\n512\n1024\n\n3.49388458186769\n1.88761005959780\n1.72890354401971\n1.72889046749119\n1.72889038608621\n1.72889026784032\n1.72889018400995\n1.72889013551548\n1.72889010959701\n1.72889009621830\n\n\u22120.46\n\u22120.084\n\u22120.0000076\n\u22120.000000047\n\u22120.000000068\n\u22120.000000048\n\u22120.000000028\n\u22120.000000015\n\u22120.0000000077\n\nreal line, whereas we have thus far only discussed integration over a closed inter-\nval. transformations can be used to obtain an equivalent interval over a finite range\n(see section 5.4.1), but to keep things simple here we will integrate over the range\n[\u22120.07,0.085],withinwhichnearlyallofthenonnegligiblevaluesoftheintegrandlie.\ntable 5.2 shows the results of a series of riemann approximations, along with\nrunning relative errors. the relative errors measure the change in the new estimated\nvalue of the integral as a proportion of the previous estimate. an iterative approx-\nimation strategy could be stopped when the magnitude of these errors falls below\nsome predetermined tolerance threshold. since the integral is quite small, a relative\nconvergence criterion is more intuitive than an absolute criterion.\n!\n\n5.1.2 trapezoidal rule\nalthoughthesimpleriemannruleisexactif f isconstanton[a, b],itcanbequiteslow\nto converge to adequate precision in general. an obvious improvement would be to\nreplace the piecewise constant approximation by a piecewise mth-degree polynomial\napproximation. we begin by introducing a class of polynomials that can be used for\nsuchapproximations.thispermitstheriemannruletobecastasthesimplestmember\nof a family of integration rules having increased precision as m increases. this family\nalso includes the trapezoidal rule and simpson\u2019s rule (section 5.1.3).\n\nlet the fundamental polynomials be\n\npij(x) =\n\nm(k=0,k /= j\nx \u2212 x\u2217ik\nx\u2217ij \u2212 x\u2217ik\nj=0 f%x\u2217ij& pij(x) is an mth-degree\nfor j = 0, . . . , m. then the function pi(x) =\"m\npolynomial that interpolates f at all the nodes x\u2217i0, . . . , x\u2217im in [xi, xi+1]. figure 5.2\nshows such interpolating polynomials for m = 0, 1, and 2.\n\n(5.8)\n\n "}, {"Page_number": 147, "text": "these polynomials are the basis for the simple approximation\n\n5.1 newton\u2013c \u02c6otes quadrature\n\n135\n\n# xi+1\n\nxi\n\nxi\n\nf(x) dx \u2248# xi+1\nm$j=0\nm$j=0\n\n=\n\n=\n\npi(x) dx\n\nxi\n\nf%x\u2217ij&# xi+1\naijf%x\u2217ij&\n\npij(x) dx\n\n(5.9)\n\n(5.10)\n\n(5.11)\n\nxi\n\nfor aij =! xi+1\npij(x) dx. this approximation replaces integration of an arbitrary\nfunction f with polynomial integration. the resulting composite rule is! b\na f(x) dx \u2248\nj=0 aijf%x\u2217ij& when there are m nodes on each subinterval.\ni=0 \"m\n\"n\u22121\nletting m = 1 with x\u2217i0 = xi and x\u2217i1 = xi+1 yields the trapezoidal rule. in this\n\ncase,\n\npi0(x) =\n\nx \u2212 xi+1\nxi \u2212 xi+1\n\nand pi1(x) =\n\nx \u2212 xi\nxi+1 \u2212 xi\n\n.\n\nintegrating these polynomials yields ai0 = ai1 = (xi+1 \u2212 xi) /2. therefore, the\ntrapezoidal rule amounts to\n\n# b\n\na\n\nf(x) dx \u2248\n\nn\u22121$i=00 xi+1 \u2212 xi\n\n2\n\n1%f (xi) + f (xi+1)&.\n\n(5.12)\n\nwhen [a, b] is partitioned into n subintervals of equal length h = (b \u2212 a)/n, then the\ntrapezoidal rule estimate is\n\n# b\n\na\n\nf(x) dx \u2248\n\nh\n\n2 f(a) + h\n\nn\u22121$i=1\n\nf(a + ih) +\n\nh\n\n2 f(b) = \u2019t(n).\n\n(5.13)\n\nvalue equals that of f at two points. therefore, when f itself is a line on [a, b], \u2019t(n)\n\nthe name of this approximation arises because the area under f in each subin-\nterval is approximated by the area of a trapezoid, as shown in figure 5.2. note that\nf is approximated in any subinterval by a first-degree polynomial (i.e., a line) whose\nis exact.\nexample 5.2 (alzheimer\u2019s disease, continued) for small numbers of subinter-\nvals, applying the trapezoidal rule to the integral from example 5.1 yields similar\nresults to those from the riemann rule because the integrand is nearly zero at the end-\npoints of the integration range. for large numbers of subintervals, the approximation\nis somewhat better. the results are shown in table 5.3.\n!\n\n "}, {"Page_number": 148, "text": "136\n\nchapter 5 numerical integration\n\ntable 5.3 estimates of the integral in (5.7) using the trapezoidal rule with\nvarious numbers of subintervals. all estimates are multiplied by a factor of 105.\nerrors for use in a relative convergence criterion are given in the final column.\nsubintervals\n\nrelative error\n\nestimate\n\n2\n4\n8\n16\n32\n64\n128\n256\n512\n1024\n\n3.49387751694744\n1.88760652713768\n1.72890177778965\n1.72888958437616\n1.72888994452869\n1.72889004706156\n1.72889007362057\n1.72889008032079\n1.72889008199967\n1.72889008241962\n\n\u22120.46\n\u22120.084\n\u22120.0000071\n0.00000021\n0.000000059\n0.000000015\n0.0000000039\n0.00000000097\n0.00000000024\n\nsuppose f has two continuous derivatives. problem 5.1 asks you to show that\n(5.14)\n\npi(x) = f(xi) + f\u2032(xi)(x \u2212 xi) +\nsubtracting the taylor expansion of f about xi from (5.14) yields\n\n1\n2 f\u2032\u2032(xi)(xi+1 \u2212 xi)(x \u2212 xi) + o(n\u22123).\n\npi(x) \u2212 f(x) =\n\n(5.15)\nand integrating (5.15) over [xi, xi+1] shows the approximation error of the trapezoidal\nrule on the ith subinterval to be h3f\u2032\u2032(xi)/12 + o(n\u22124). thus\n\n1\n2 f\u2032\u2032(xi)(x \u2212 xi)(x \u2212 xi+1) + o(n\u22123),\n\n(5.16)\n\n(5.17)\n\n\u2019t(n) \u2212# b\n\na\n\nf(x) dx =\n\n+ o(n\u22124)*\n\nn$i=1) h3f\u2032\u2032(xi)\n12\nnh3f\u2032\u2032(\u03be)\n+ o(n\u22123)\n(b \u2212 a)3f\u2032\u2032(\u03be)\n\n12\n\n=\n=\n\n12n2\n\n+ o(n\u22123)\n\n(5.18)\nfor some \u03be \u2208 [a, b] by the mean value theorem for integrals. hence the leading term\nof the overall error is o(n\u22122).\n5.1.3 simpson\u2019s rule\nletting m = 2, x\u2217i0 = xi, x\u2217i1 = (xi + xi+1) /2, and x\u2217i2 = xi+1 in (5.8), we obtain\nsimpson\u2019s rule. problem 5.2 asks you to show that ai0 = ai2 = (xi+1 \u2212 xi) /6 and\nai1 = 2 (ai0 + ai2). this yields the approximation\n\n# xi+1\n\nxi\n\nf(x) dx \u2248\n\nxi+1 \u2212 xi\n\n6\n\n)f (xi) + 4f0 xi + xi+1\n\n2\n\n1 + f (xi+1)*\n\n(5.19)\n\n "}, {"Page_number": 149, "text": "5.1 newton\u2013c \u02c6otes quadrature\n\n137\n\ntable 5.4 estimates of the integral in (5.7) using simpson\u2019s rule with various\nnumbers of subintervals (and two nodes per subinterval). all estimates are\nmultiplied by a factor of 105. errors for use in a relative convergence criterion\nare given in the final column.\nsubintervals\n\nrelative error\n\nestimate\n\n2\n4\n8\n16\n32\n64\n128\n256\n512\n1024\n\n1.35218286386776\n1.67600019467364\n1.72888551990500\n1.72889006457954\n1.72889008123918\n1.72889008247358\n1.72889008255419\n1.72889008255929\n1.72889008255961\n1.72889008255963\n\n0.24\n0.032\n0.0000026\n0.0000000096\n0.00000000071\n0.000000000047\n0.0000000000029\n0.00000000000018\n0.000000000000014\n\nforthe(i + 1)thsubinterval.figure5.2showshowsimpson\u2019sruleprovidesaquadratic\napproximation to f on each subinterval.\nsuppose the interval [a, b] has been partitioned into n subintervals of equal\nlength h = (b \u2212 a)/n, where n is even. to apply simpson\u2019s rule, we need an interior\nnode in each [xi, xi+1]. since n is even, we may adjoin pairs of adjacent subintervals,\nwith the shared endpoint serving as the interior node of the larger interval. this\nprovides n/2 subintervals of length 2h, for which\n\n(5.20)\n\n# b\n\na\n\nf(x) dx \u2248\n\nh\n3\n\nn/2$i=1%f (x2i\u22122) + 4f (x2i\u22121) + f (x2i)& =\u2019s% n\n2& .\n\nexample 5.3 (alzheimer\u2019s disease, continued) table 5.4 shows the results of\napplying simpson\u2019s rule to the integral from example 5.1. one endpoint and an\ninterior node were evaluated on each subinterval. thus, for a fixed number of subin-\ntervals, simpson\u2019s rule requires twice as many evaluations of f as the riemann or the\ntrapezoidal rule. following this example, we show that the precision of simpson\u2019s\nrule more than compensates for the increased evaluations. from another perspective,\nif the number of evaluations of f is fixed at n for each method, we would expect\nsimpson\u2019s rule to outperform the previous approaches, if n is large enough.\n!\nif f is quadratic on [a, b], then it is quadratic on each subinterval. simpson\u2019s\nrule approximates f on each subinterval by a second-degree polynomial that matches\nf at three points; therefore the polynomial is exactly f. thus, simpson\u2019s rule exactly\nintegrates quadratic f.\nsuppose f is smooth\u2014but not polynomial\u2014and we have n subintervals\n[xi, xi+1] of equal length 2h. to assess the degree of approximation in simpson\u2019s\nrule, we begin with consideration on a single subinterval, and denote the simple\nsubinterval as \u2019si(n) = (h/3)2f (xi) + 4f (xi + h) +\nsimpson\u2019s\nf (xi + 2h)3. denote the true value of the integral on that subinterval as ii.\n\nrule on that\n\n "}, {"Page_number": 150, "text": "138\n\nchapter 5 numerical integration\n\nwe use the taylor series expansion of f about xi, evaluated at x = xi + h and\n\nx = xi + 2h, to replace terms in\u2019si(n). combining terms, this yields\n4\n3 h3f\u2032\u2032 (xi)\n100\n360 h5f\u2032\u2032\u2032\u2032 (xi) + \u00b7\u00b7\u00b7 .\n\n\u2019si(n) = 2hf (xi) + 2h2f\u2032 (xi) +\n\n2\n3 h4f\u2032\u2032\u2032 (xi) +\n(5.21)\n+\nf(t) dt. this function has the useful properties that\nf (xi) = 0, f (xi + 2h) = ii, and f\u2032(x) = f(x). taylor series expansion of f about\nxi, evaluated at x = xi + 2h, yields\n\nnow let f(x) =! x\n\nxi\n\nii = 2hf (xi) + 2h2f\u2032 (xi) +\n\n2\n3 h4f\u2032\u2032\u2032 (xi) +\n\n+\n\n4\n3 h3f\u2032\u2032 (xi)\n32\n120 h5f\u2032\u2032\u2032\u2032 (xi) + \u00b7\u00b7\u00b7 .\n\n(5.22)\n\nsubintervals that partition [a, b], the error is therefore the sum of such errors, namely\n\nsubtracting (5.22) from (5.21) yields \u2019si(n) \u2212 ii = h5 f\u2032\u2032\u2032\u2032 (xi)+90 + \u00b7\u00b7\u00b7 =\no4n\u221255. this is the error for simpson\u2019s rule on a single subinterval. over the n\no4n\u221245. note that simpson\u2019s rule therefore exactly integrates cubic functions, too.\n\n5.1.4 general kth-degree rule\nthe preceding discussion raises the general question about how to determine a\nnewton\u2013c\u02c6otes rule that is exact for polynomials of degree k. this would require\nconstants c0, . . . , ck that satisfy\n\n# b\n\na\n\nf(x) dx = c0f(a) + c1f0a +\ni(b \u2212 a)\n\n+ cif0a +\n\nb \u2212 a\n\nk 1 + \u00b7\u00b7\u00b7\n1 + \u00b7\u00b7\u00b7 + ckf(b)\n\nk\n\n(5.23)\n\nfor any polynomial f. of course, one could follow the derivations shown above\nfor m = k, but there is another simple approach. if a method is to be exact for all\npolynomials of degree k, then it must be exact for particular\u2014and easily integrated\u2014\nchoices like 1, x, x2, . . . , xk. thus, we may set up a system of k equations in k\nunknowns as follows:\n\na\n\n# b\n# b\n\na\n\n1 dx = b \u2212 a = c0 + \u00b7\u00b7\u00b7 + ck,\nx dx =\n\n2\n\nb2 \u2212 a2\n= c0a + c10a +\n...\n\nb \u2212 a\n\nk 1 + \u00b7\u00b7\u00b7 + ckb,\n\n# b\n\na\n\nxk dx = etc.\n\n "}, {"Page_number": 151, "text": "139\nall that remains is to solve for the ci to derive the algorithm. this approach is some-\ntimes called the method of undetermined coefficients.\n\n5.2 romberg integration\n\n5.2 romberg integration\nin general, low-degree newton\u2013c\u02c6otes methods are slow to converge. however, there\nisaveryefficientmechanismtoimproveuponasequenceoftrapezoidalruleestimates.\nlet \u2019t(n) denote the trapezoidal rule estimate of! b\na f(x) dx using n subintervals of\nequal length h = (b \u2212 a)/n, as given in (5.13). without loss of generality, suppose\na = 0 and b = 1. then\n\u2019t(1) =\n\u2019t(2) =\n\u2019t(4) =\n\n1\n2 f(0) +\n1\n4 f(0) +\n1\n8 f(0) +\n\nand so forth. noting that\n\n453 +\n\n1\n8 f(1),\n\n(5.24)\n\nand so forth suggests the general recursion relationship\n\nthe euler\u2013maclaurin formula (1.8) can be used to show that\n\n1\n\n1\n\n1\n1\n\n1\n1\n\nh\n2\n\n1\n2 f(1),\n25 +\n2 f41\n1\n1\n4 f(1),\n42f41\n45 + f41\n25 + f43\n1\n25,\n2 f41\n2\u2019t(1) +\n\u2019t(2) =\n45 + f43\n42f41\n453,\n\u2019t(4) =\n2\u2019t(2) +\nf0a +0i \u2212\n21 h1 .\nn$i=1\n\u2019t(2n) =\n2\u2019t(n) +\n\u2019t(n) =# b\n\u2019t(2n) =# b\n4\u2019t(2n) \u2212\u2019t(n)\n\nf(x) dx + o%n\u22124& ,\n\n+ o%n\u22124& .\n\n+ o%n\u22124&\n\nf(x) dx + c1h2\n\nf(x) dx +\n\n=# b\n\nc1\n4 h2\n\n3\n\na\n\na\n\na\n\nfor some constant c1, and hence\n\ntherefore,\n\n(5.25)\n\n(5.26)\n\n(5.27)\n\n(5.28)\n\n(5.29)\n\nso the h2 error terms in (5.27) and (5.28) cancel. with this simple adjustment we have\nmade a striking improvement in the estimate. in fact, the estimate given in (5.29) turns\nout to be simpson\u2019s rule with subintervals of width h/2. moreover, this strategy can\nbe iterated for even greater gains.\n\n "}, {"Page_number": 152, "text": "140\n\nchapter 5 numerical integration\n\nof estimates like\n\nbeginbydefining\u2019t i,0 = \u2019t42i5for i = 0, . . . , m.thendefineatriangulararray\n\n\u2019t 0,0\n\u2019t 1,0\n\u2019t 2,0\n\u2019t 3,0\n\u2019t 4,0\n...\n\n\u2019t 1,1\n\u2019t 2,1\n\u2019t 3,1\n\u2019t 4,1\n...\n\n\u2019t 2,2\n\u2019t 3,2\n\u2019t 4,2\n...\n\n\u2019t 3,3\n\u2019t 4,3\n...\n\n\u2019t 4,4\n...\n\n...\n\nusing the relationship\n\n(5.30)\n\n4j\u2019t i,j\u22121 \u2212\u2019t i\u22121,j\u22121\n\n4j \u2212 1\n\n\u2019t i,j =\nfor j = 1, . . . , iand i = 1, . . . , m.notethat(5.30)canbereexpressedtocalculate\u2019t i,j\nby adding an increment equal to 1/(4j \u2212 1) times the difference \u2019t i,j\u22121 \u2212\u2019t i\u22121,j\u22121\nto the estimate given by \u2019t i,j\u22121.\na f(x) dx = o42\u22122mj5 for j \u2264 m [121, 376]. this is\nthe array have error \u2019t m,j \u2212! b\n\nsuch fast convergence that very small m will often suffice.\nis increased. to do this, consider the quotient\n\nit is important to check that the romberg calculations do not deteriorate as m\n\nif f has 2m continuous derivatives on [a, b], then the entries in the mth row of\n\n.\n\n(5.31)\n\nqij = \u2019t i,j \u2212\u2019t i\u22121,j\n\u2019t i+1,j \u2212\u2019t i,j\n\nthe error in \u2019t i,j is attributable partially to the approximation strategy itself and par-\nqij values will become erratic. the columns of the triangular array of \u2019t i,j can be\n\ntially to numerical imprecision introduced by computer roundoff. as long as the\nformer source dominates, the qij values should approach 4j+1 as i increases. how-\never, when computer roundoff error is substantial relative to approximation error, the\nexamined to determine the largest j for which the quotients appear to approach 4j+1\nbefore deteriorating. no further column should be used to calculate an update via\n(5.30). the following example illustrates the approach.\nexample 5.4 (alzheimer\u2019s disease, continued) table 5.5 shows the results of\napplying romberg integration to the integral from example 5.1. the right columns of\nthis table are used to diagnose the stability of the romberg calculations. the top por-\ngiven in table 5.3. after some initial steps, the quotients in the top portion of the table\nconverge nicely to 4. therefore, it is safe and advisable to apply (5.30) to generate a\nsecond column of the triangular array. it is safe because the convergence of the quo-\ntients to 4 implies that computer roundoff error has not yet become a dominant source\n\ntion of the table corresponds to j = 0, and the \u2019t i,j are the trapezoidal rule estimates\n\n "}, {"Page_number": 153, "text": "table 5.5 estimates of the integral in (5.7) using romberg integration. all estimates and differences\nare multiplied by a factor of 105. the final two columns provide performance evaluation measures\ndiscussed in the text.\n\n5.2 romberg integration\n\n141\n\ni\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\nj\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n\nsubintervals\n\n2\n4\n8\n16\n32\n64\n128\n256\n512\n1024\n\n2\n4\n8\n16\n32\n64\n128\n256\n512\n1024\n\n2\n4\n8\n16\n32\n64\n128\n256\n512\n1024\n\n\u2019t i,0\n\n3.49387751694744\n1.88760652713768\n1.72890177778965\n1.72888958437616\n1.72888994452869\n1.72889004706156\n1.72889007362057\n1.72889008032079\n1.72889008199967\n1.72889008241962\n\n1.35218286386776\n1.67600019467364\n1.72888551990500\n1.72889006457954\n1.72889008123918\n1.72889008247358\n1.72889008255420\n1.72889008255929\n1.72889008255961\n\n1.69758801672736\n1.73241120825375\n1.72889036755784\n1.72889008234983\n1.72889008255587\n1.72889008255957\n1.72889008255963\n1.72889008255963\n\n\u2019t i,j \u2212\u2019t i\u22121,j\n\u22121.60627098980976\n\u22120.15870474934803\n\u22120.00001219341349\n0.00000036015254\n0.00000010253287\n0.00000002655901\n0.00000000670022\n0.00000000167888\n0.00000000041996\n\n0.32381733080589\n0.05288532523136\n0.00000454467454\n0.00000001665964\n0.00000000123439\n0.00000000008062\n0.00000000000510\n0.00000000000032\n\nqij\n\n10.12\n13015.61\n\u221233.86\n3.51\n3.86\n3.96\n3.99\n4.00\n\n6.12\n11636.77\n272.80\n13.50\n15.31\n15.82\n16.14\n\n0.03482319152639\n\u22120.00352084069591\n\u22120.00000028520802\n0.00000000020604\n0.00000000000370\n0.00000000000006\n<0.00000000000001\n\n\u22129.89\n12344.82\n\u22121384.21\n55.66\n59.38\n\u221220.44\n\nof error. it is advisable because incrementing one of the current integral estimates by\none-third of the corresponding difference would yield a noticeably different updated\nestimate.\nthe second column of the triangular array is shown in the middle portion of\ntable 5.5. the quotients in this portion also appear reasonable, so the third column is\ncalculated and shown in the bottom portion of the table. the values of qi2 approach\n64, allowing more tolerance for larger j. at i = 10, computer roundoff error appears\n\n "}, {"Page_number": 154, "text": "chapter 5 numerical integration\n\n142\nto dominate approximation error because the quotient departs from near 64. however,\nnote that incrementing the integral estimate by 1\n63 of the difference at this point would\nhave a negligible impact on the updated estimate itself. had we proceeded one more\nstepwiththereasoningthatthegrowingamountofroundofferrorwillcauselittleharm\natthispoint,wewouldhavefoundthattheestimatewasnotimprovedandtheresulting\nquotients clearly indicated that no further extrapolations should be considered.\nthus,wemaytake\u2019t 9,2 = 1.72889008255963 \u00d7 10\u22125 tobetheestimatedvalue\nof the integral. in this example, we calculated the triangular array one column at a\ntime, for m = 10. however, in implementation it makes more sense to generate the\narray one row at a time. in this case, we would have stopped after i = 9, obtaining a\nprecise estimate with fewer subintervals\u2014and fewer evaluations of f\u2014than in any\nof the previous examples.\n!\nthe romberg strategy can be applied to other newton\u2013c\u02c6otes integration rules.\na f(x) dx using n subintervals\n\nfor example, if\u2019s(n) is the simpson\u2019s rule estimate of! b\n\nof equal length, then the analogous result to (5.29) is\n\n16\u2019s(2n) \u2212\u2019s(n)\n\n15\n\n=# b\n\na\n\nf(x) dx + o%n\u22126&.\n\n(5.32)\n\nromberg integration is a form of a more general strategy called richardson\n\nextrapolation [325, 516].\n\n5.3 gaussian quadrature\nallthenewton\u2013c\u02c6otesrulesdiscussedabovearebasedonsubintervalsofequallength.\nthe estimated integral is a sum of weighted evaluations of the integrand on a regular\ngrid of points. for a fixed number of subintervals and nodes, only the weights may\nbe flexibly chosen; we have limited attention to choices of weights that yield exact\nintegration of polynomials. using m + 1 nodes per subinterval allowed mth-degree\npolynomials to be integrated exactly.\nan important question is the amount of improvement that can be achieved if the\nconstraint of evenly spaced nodes and subintervals is removed. by allowing both the\nweights and the nodes to be freely chosen, we have twice as many parameters to use in\nthe approximation of f. if we consider that the value of an integral is predominantly\ndetermined by regions where the magnitude of the integrand is large, then it makes\nsensetoputmorenodesinsuchregions.withasuitablyflexiblechoiceof m + 1nodes,\nx0, . . . , xm,andcorrespondingweights, a0, . . . , am,exactintegrationof2(m + 1)th-\ndegree polynomials can be obtained using! b\nthis approach, called gaussian quadrature, can be extremely effective for inte-\ngrals like! b\na xkw(x) dx < \u221e\nfor all k \u2265 0. these requirements are reminiscent of density function with finite\nmoments. indeed, it is often useful to think of w as a density, in which case inte-\ngrals like expected values and bayesian posterior normalizing constants are natural\n\ni=0 aif (xi).\na f(x)w(x) dx where w is a nonnegative function and! b\n\na f(x) dx =\"m\n\n "}, {"Page_number": 155, "text": "143\ncandidates for gaussian quadrature. the method is more generally applicable, how-\na f\u2217(x)w(x) dx.\nthebestnodelocationsturnouttobetherootsofasetoforthogonalpolynomials\n\never, by defining f\u2217(x) = f(x)/w(x) and applying the method to! b\n\n5.3 gaussian quadrature\n\nthat is determined by w.\n\n5.3.1 orthogonal polynomials\nsome background on orthogonal polynomials is needed to develop gaussian quadra-\nture methods [2, 139, 395, 620]. let pk(x) denote a generic polynomial of degree\nk. for convenience in what follows, assume that the leading coefficient of pk(x) is\npositive.\nif! b\na f(x)2w(x) dx < \u221e, then the function f is said to be square-integrable\n2\nwith respect to w on [a, b]. in this case we will write f \u2208 l\nw,[a,b]. for any f and g\n2\nin l\nw,[a,b], their inner product with respect to w on [a, b] is defined to be\n\n\u27e8f, g\u27e9w,[a,b] =# b\n\nf(x)g(x)w(x) dx.\n\n(5.33)\nif \u27e8f, g\u27e9w,[a,b] = 0, then f and g are said to be orthogonal with respect to w on [a, b].\nif also f and g are scaled so that \u27e8f, f\u27e9w,[a,b] = \u27e8g, g\u27e9w,[a,b] = 1, then f and g are\northonormal with respect to w on [a, b].\ngivenany wthatisnonnegativeon[a, b],thereexistsasequenceofpolynomials\n{pk(x)}\u221ek=0 that are orthogonal with respect to w on [a, b]. this sequence is not unique\nwithoutsomeformofstandardizationbecause\u27e8f, g\u27e9w,[a,b] = 0implies\u27e8cf, g\u27e9w,[a,b] =\n0foranyconstant c.thecanonicalstandardizationforasetoforthogonalpolynomials\ndepends on w and will be discussed later; a common choice is to set the leading\ncoefficient of pk(x) equal to 1. for use in gaussian quadrature, the range of integration\nis also customarily transformed from [a, b] to a range [a\u2217, b\u2217] whose choice depends\non w.\na set of standardized, orthogonal polynomials can be summarized by a recur-\nrence relation\n\na\n\npk(x) = (\u03b1k + x\u03b2k) pk\u22121(x) \u2212 \u03b3kpk\u22122(x)\nfor appropriate choices of \u03b1k, \u03b2k, and \u03b3k that vary with k and w.\nthe roots of any polynomial in such a standardized set are all in (a\u2217, b\u2217).\nthese roots will serve as nodes for gaussian quadrature. table 5.6 lists several sets of\northogonalpolynomials,theirstandardizations,andtheircorrespondencestocommon\ndensity functions.\n\n(5.34)\n\n5.3.2 the gaussian quadrature rule\nstandardized orthogonal polynomials like (5.34) are important because they deter-\nmine both the weights and the nodes for a gaussian quadrature rule based on a chosen\nw. let {pk(x)}\u221ek=0 be a sequence of orthonormal polynomials with respect to w on\n[a, b] for a function w that meets the conditions previously discussed. denote the\n\n "}, {"Page_number": 156, "text": "144\n\nchapter 5 numerical integration\n\ntable 5.6 orthogonal polynomials, their standardizations, their correspondence to common\ndensity functions, and the terms used for their recursive generation. the leading coefficient of\na polynomial is denoted ck. in some cases, variants of standard definitions are chosen for best\ncorrespondence with familiar densities.\n\nw(x)\n\n(1 \u2212 x)p\u2212qxq\u22121\n1\n\nexp{\u2212x}\n\nxr exp{\u2212x}\n\nexp{\u2212x2/2}\n\nstandardization\n\n(a\u2217, b\u2217)\nck = 1\n(0,1)\npk(1) = 1\n(0,1)\n\nck = (\u22121)k/k!\n\n(0,\u221e)\n\nck = (\u22121)k/k!\n\n(0,\u221e)\nck = 1\n(\u2212\u221e,\u221e)\n\n\u03b1k\n\u03b2k\n\u03b3k\n\nsee [2, 516]\n\n(1 \u2212 2k)/k\n(4k \u2212 2)/k\n(k \u2212 1)/k\n(2k \u2212 1)/k\n\u22121/k\n(k \u2212 1)/k\n(2k \u2212 1 + r)/k\n\u22121/k\n(k \u2212 1 + r)/k\n0\n1\nk \u2212 1\n\nname\n(density)\njacobia\n(beta)\nlegendrea\n(uniform)\n\nlaguerre\n(exponential)\n\nlaguerreb\n(gamma)\n\nhermitec\n(normal)\n\nashifted.\nbgeneralized.\ncalternative form.\n\nroots of pm+1(x) by a < x0 < \u00b7\u00b7\u00b7 < xm < b. then there exist weights a0, . . . , am\nsuch that:\n1. ai > 0 for i = 0, . . . , m.\n2. ai = \u2212cm+2+2cm+1pm+2 (xi) p\u2032m+1 (xi)3, where ck is the leading coefficient\nof pk(x).\n3. ! b\na f(x)w(x) dx =\"m\ni=0 aif (xi) whenever f is a polynomial of degree not\nexceeding 2m + 1. in other words, the method is exact for the expectation of\nany such polynomial with respect to w.\n4. if f is 2(m + 1) times continuously differentiable, then\n\nf(x)w(x) dx \u2212\n\naif (xi) =\n\nm$i=0\n\nf (2m+2)(\u03be)\n(2m + 2)!c2\nm+1\n\n(5.35)\n\n# b\nfor some \u03be \u2208 (a, b).\n\na\n\nthe proof of this result may be found in [139].\n\n "}, {"Page_number": 157, "text": "5.3 gaussian quadrature\n\n145\n\nestimate\n\ntable 5.7 estimates of the integral in (5.7) using gauss\u2013hermite quadrature\nwith various numbers of nodes. all estimates are multiplied by a factor of 105.\nerrors for use in a relative convergence criterion are given in the final column.\nnodes\n2\n3\n4\n5\n6\n7\n8\n\n1.72893306163335\n1.72889399083898\n1.72889068827101\n1.72889070910131\n1.72889070914313\n1.72889070914166\n1.72889070914167\n\n\u22120.000023\n\u22120.0000019\n0.000000012\n0.000000000024\n\u22120.00000000000085\n\u22120.0000000000000071\n\nrelative error\n\nalthough this result and table 5.6 provide the means by which the nodes and\nweights for an (m + 1)-point gaussian quadrature rule can be calculated, one should\nbe hesitant to derive these directly, due to potential numerical imprecision. numeri-\ncally stable calculations of these quantities can be obtained from publicly available\nsoftware [228, 489]. alternatively, one can draw the nodes and weights from pub-\nlished tables like those in [2, 387]. lists of other published tables are given in [139,\n630].\nof the choices in table 5.6, gauss\u2013hermite quadrature is particularly useful\nbecause it enables integration over the entire real line. the prominence of normal\ndistributions in statistical practice and limiting theory means that many integrals\nresemble the product of a smooth function and a normal density; the usefulness of\ngauss\u2013hermite quadrature in bayesian applications is demonstrated in [478].\nexample 5.5 (alzheimer\u2019s disease, continued) table 5.7 shows the results of\napplyinggauss\u2013hermitequadraturetoestimatetheintegralfromexample5.1.using\nthe hermite polynomials in this case is particularly appealing because the integrand\nfrom example 5.1 really should be integrated over the entire real line, rather than the\ninterval (\u22120.07,0.085). convergence was extremely fast: with 8 nodes we obtained\na relative error half the magnitude of that achieved by simpson\u2019s rule with 1024\nnodes. the estimate in table 5.7 differs from previous examples because the range\nof integration differs. applying gauss\u2013legendre quadrature to estimate the integral\novertheinterval(\u22120.07,0.085)yieldsanestimateof1.72889008255962\u00d710\u22125 using\n26 nodes.\n!\ngaussian quadrature is quite different from the newton\u2013c\u02c6otes rules discussed\npreviously. whereas the latter rely on potentially enormous numbers of nodes to\nachieve sufficient precision, gaussian quadrature is often very precise with a re-\nmarkably small number of nodes. however, for gaussian quadrature the nodes for\nan m-point rule are not usually shared by an (m + k)-point rule for k \u2265 1. recall\nthe strategy discussed for newton\u2013c\u02c6otes rules where the number of subintervals is\nsequentially doubled so that half the new nodes correspond to old nodes. this is not\n\n "}, {"Page_number": 158, "text": "chapter 5 numerical integration\n\n146\neffective for gaussian quadrature because each increase in the number of nodes will\nrequire a separate effort to generate the nodes and the weights.\n\n5.4 frequently encountered problems\nthissectionbrieflyaddressesstrategiestotrywhenyouarefacedwithaproblemmore\ncomplex than a one-dimensional integral of a smooth function with no singularities\non a finite range.\n\n5.4.1 range of integration\nintegrals over infinite ranges can be transformed to a finite range. some useful trans-\nformations include 1/x, exp{x}/(1 + exp{x}), exp{\u2212x}, and x/(1 + x). any cumu-\nlative distribution function is a potential basis for transformation, too. for example,\nthe exponential cumulative distribution function transforms the positive half line to\nthe unit interval. cumulative distribution functions for real-valued random variables\ntransform doubly infinite ranges to the unit interval. of course, transformations to\nremove an infinite range may introduce other types of problems such as singularities.\nthus, among the options available, it is important to choose a good transformation.\nroughly speaking, a good choice is one that produces an integrand that is as nearly\nconstant as can be managed.\ninfinite ranges can be dealt with in other ways, too. example 5.5 illustrates\nthe use of gauss\u2013hermite quadrature to integrate over the real line. alternatively,\nwhen the integrand vanishes near the extremes of the integration range, integra-\ntion can be truncated with a controllable amount of error. truncation was used in\nexample 5.1.\nfurther discussion of transformations and strategies for selecting a suitable one\nare given in [139, 630]\n\nintegrands with singularities or other extreme behavior\n\n5.4.2\nseveral strategies can be employed to eliminate or control the effect of singularities\nthat would otherwise impair the performance of an integration rule.\ntransformation is one approach. for example, consider ! 1\n0 (exp{x}/\u221ax) dx,\nwhich has a singularity at 0. the integral is easily fixed using the transformation\nu = \u221ax, yielding 2! 1\n0 exp{u2} du.\nthe integral! 1\n0 x999 exp{x} dx has no singularity on [0,1] but is very difficult to\nestimate directly with a newton\u2013c\u02c6otes approach. transformation is helpful in such\ncases, too. letting u = x1000 yields! e0 exp{u1/1000} du, whose integrand is nearly\nconstant on [0, e]. the transformed integral is much easier to estimate reliably.\nanother approach is to subtract out the singularity. for example, consider inte-\ngrating! \u03c0/2\n\u2212\u03c0/2 log{sin2 x} dx, which has a singularity at 0. by adding and subtracting\n\n "}, {"Page_number": 159, "text": "5.4 frequently encountered problems\n\n147\n\naway the square of the log singularity at zero, we obtain! \u03c0/2\n\u2212\u03c0/2 log{(sin2 x)/x2} dx +\n! \u03c0/2\n\u2212\u03c0/2 log{x2} dx. the first term is then suitable for quadrature, and elementary\nmethods can be used to derive that the second term equals 2\u03c0[log(\u03c0/2) \u2212 1].\nrefer to [139, 516, 630] for more detailed discussions of how to formulate an\nappropriate strategy to address singularities.\n\n5.4.3 multiple integrals\nthe most obvious extensions of univariate quadrature techniques to multiple inte-\ngrals are product formulas. this entails, for example, writing! b\na! d\nc f(x, y) dy dx as\na g(x) dx where g(x) =! d\n! b\nc f(x, y) dy.valuesof g(x)couldbeobtainedviaunivariate\nquadrature approximations to! d\nc f(x, y) dy for a grid of x values. univariate quadra-\nture could then be completed for g. using n subintervals in each univariate quadrature\nwould require np evaluations of f, where p is the dimension of the integral. thus,\nthis approach is not feasible for large p. even for small p, care must be taken to avoid\nthe accumulation of a large number of small errors, since each exterior integral de-\npends on the values obtained for each interior integral at a set of points. also, product\nformulas can only be implemented directly for regions of integration that have simple\ngeometry, such as hyperrectangles.\nto cope with higher dimensions and general multivariate regions, one may\ndevelop specialized grids over the region of integration, search for one or more di-\nmensions that can be integrated analytically to reduce the complexity of the problem,\nor turn to multivariate adaptive quadrature techniques. multivariate methods are dis-\ncussed in more detail in [139, 290, 516, 619].\nmontecarlomethodsdiscussedinchapters6and7canbeemployedtoestimate\nintegrals over high-dimensional regions efficiently. for estimating a one-dimensional\nintegral based on n points, a monte carlo estimate will typically have a convergence\nrate of o(n\u22121/2), whereas the quadrature methods discussed in this chapter converge\nat o(n\u22122) or faster. in higher dimensions, however, the story changes. quadrature ap-\nproaches are then much more difficult to implement and slower to converge, whereas\nmonte carlo approaches generally retain their implementation ease and their conver-\ngence performance. accordingly, monte carlo approaches are generally preferred for\nhigh-dimensional integration.\n\n5.4.4 adaptive quadrature\ntheprincipleofadaptivequadratureistochoosesubintervallengthsbasedonthelocal\nbehavior of the integrand. for example, one may recursively subdivide those existing\nsubintervals where the integral estimate has not yet stabilized. this can be a very\neffective approach if the bad behavior of the integrand is confined to a small portion\nof the region of integration. it also suggests a way to reduce the effort expended for\nmultiple integrals because much of the integration region may be adequately covered\nby a very coarse grid of subintervals. a variety of ideas is covered in [121, 376, 630].\n\n "}, {"Page_number": 160, "text": "148\n\nchapter 5 numerical integration\n\n5.4.5 software for exact integration\nthis chapter has focused on integrals that do not have analytic solutions. for most of\nus, there is a class of integrals that have analytic solutions that are so complex as to\nbe beyond our skills, patience, or cleverness to derive. numerical approximation will\nwork for such integrals, but so will symbolic integration tools. software packages\nsuch as mathematica [671] and maple [384] allow the user to type integrands in a\nsyntax resembling many computer languages. the software interprets these algebraic\nexpressions. with deft use of commands for integrating and manipulating terms, the\nuser can derive exact expressions for analytic integrals. the software does the algebra.\nsuch software is particularly helpful for difficult indefinite integrals.\n\nproblems\n5.1. for the trapezoidal rule, express pi(x) as\n\nf(xi) + (x \u2212 xi) f(xi+1) \u2212 f(xi)\nxi+1 \u2212 xi\n\n.\n\nexpand f in taylor series about xi and evaluate this at x = xi+1. use the resulting\nexpression in order to prove (5.14).\n5.2. following the approach in (5.8)\u2013(5.11), derive aij for j = 0,1,2 for simpson\u2019s rule.\n5.3. suppose the data (x1, . . . , x7) = (6.52, 8.32, 0.31, 2.82, 9.96, 0.14, 9.64) are observed.\nconsider bayesian estimation of \u00b5 based on a n(\u00b5,32/7) likelihood for the minimally\nsufficient \u00afx | \u00b5, and a cauchy(5,2) prior.\na. using a numerical integration method of your choice, show that the proportional-\nity constant is roughly 7.84654. (in other words, find k such that! k \u00d7 (prior) \u00d7\n(likelihood) d\u00b5 = 1.)\nb. usingthevalue7.84654from(a),determinetheposteriorprobabilitythat2 \u2264 \u00b5 \u2264 8\nusing the riemann, trapezoidal, and simpson\u2019s rules over the range of integration\n[implementing simpson\u2019s rule as in (5.20) by pairing adjacent subintervals]. com-\npute the estimates until relative convergence within 0.0001 is achieved for the slow-\nest method. table the results. how close are your estimates to the correct answer of\n0.99605?\nc. find the posterior probability that \u00b5 \u2265 3 in the following two ways. since the range\nof integration is infinite, use the transformation u = exp{\u00b5}/(1 + exp{\u00b5}). first,\nignore the singularity at 1 and find the value of the integral using one or more\nquadrature methods. second, fix the singularity at 1 using one or more appropriate\nstrategies, and find the value of the integral. compare your results. how close are\nthe estimates to the correct answer of 0.99086?\nd. use the transformation u = 1/\u00b5, and obtain a good estimate for the integral in\n\npart (c).\n\n5.4. let x \u223c unif[1, a] and y = (a \u2212 1)/x, for a > 1. compute e{y} = log a using\nromberg\u2019s algorithm for m = 6. table the resulting triangular array. comment on\nyour results.\n\n "}, {"Page_number": 161, "text": "5.4 frequently encountered problems\n\n149\n\ntable 5.8 nodes and weights for 10-point gauss\u2013\nlegendre quadrature on the range [\u22121, 1].\nai\n\n\u00b1xi\n\n0.148874338981631\n0.433395394129247\n0.679409568299024\n0.865063366688985\n0.973906528517172\n\n0.295524224714753\n0.269266719309996\n0.219086362515982\n0.149451394150581\n0.066671344308688\n\n5.5. the gaussian quadrature rule having w(x) = 1 for integrals on [\u22121,1] (cf. table 5.6)\nis called gauss\u2013legendre quadrature because it relies on the legendre polynomials.\nthe nodes and weights for the 10-point gauss\u2013legendre rule are given in table 5.8.\na. plot the weights versus the nodes.\nb. find the area under the curve y = x2 between \u22121 and 1. compare this with the\n\nexact answer and comment on the precision of this quadrature technique.\n\n5.6. suppose 10 i.i.d. observations result in \u00afx = 47. let the likelihood for \u00b5 correspond to\nthe model \u00afx | \u00b5 \u223c n(\u00b5,50/10), and the prior for (\u00b5 \u2212 50)/8 be student\u2019s t with 1\ndegree of freedom.\na. show that the five-point gauss\u2013hermite quadrature rule relies on the hermite poly-\nnomial h5(x) = c(x5 \u2212 10x3 + 15x).\nb. show that the normalization of h5(x) [namely, \u27e8h5(x), h5(x)\u27e9 = 1] requires c =\n1/6120\u221a2\u03c0. you may wish to recall that a standard normal distribution has odd\nmoments equal to zero and rth moments equal to r!/[(r/2)!2r/2] when r is even.\nc. using your favorite root finder, estimate the nodes of the five-point gauss\u2013hermite\nquadrature rule. (recall that finding a root of f is equivalent to finding a local\nminimum of |f|.) plot h5(x) from \u22123 to 3 and indicate the roots.\nknowing that the normalizing constant for h6(x) is 1/6720\u221a2\u03c0.\n\ne. using the nodes and weights found above for five-point gauss\u2013hermite integration,\nestimate the posterior variance of \u00b5. (remember to account for the normalizing\nconstant in the posterior before taking posterior expectations.)\n\nd. find the quadrature weights. plot the weights versus the nodes. you may appreciate\n\n "}, {"Page_number": 162, "text": "chapter 6\nsimulation and monte carlo\nintegration\n\nthis chapter addresses the simulation of random draws x1, . . . ,xn from a\ntarget distribution f. the most frequent use of such draws is to perform monte\ncarlo integration, which is the statistical estimation of the value of an integral using\nevaluations of an integrand at a set of points drawn randomly from a distribution with\nsupport over the range of integration [461].\nestimation of integrals via monte carlo simulation can be useful in a wide\nvariety of settings. in bayesian analyses, posterior moments can be written in the\nform of an integral but typically cannot be evaluated analytically. posterior probabil-\nities can also be written as the expectation of an indicator function with respect to\nthe posterior. the calculation of risk in bayesian decision theory relies on integra-\ntion. integration is also an important component in frequentist likelihood analyses.\nfor example, marginalization of a joint density relies upon integration. example 5.1\nillustrates an integration problem arising from the maximum likelihood fit of a gen-\neralized linear mixed model. a variety of other integration problems are discussed\nhere and in chapter 7.\naside from its application to monte carlo integration, simulation of random\ndraws from a target density f is important in many other contexts. indeed, chapter 7\nis devoted to a specific strategy for monte carlo integration called markov chain\nmonte carlo. bootstrap methods, stochastic search algorithms, and a wide variety of\nother statistical tools also rely on generation of random deviates.\nfurther details about the topics discussed in this chapter can be found in [106,\n158, 190, 374, 383, 417, 432, 469, 539, 555, 557].\n\n6.1 introduction to the monte carlo method\nmany quantities of interest in inferential statistical analyses can be expressed as the\nexpectation of a function of a random variable, say e{h(x)}. let f denote the density\nof x, and \u00b5 denote the expectation of h(x) with respect to f. when an i.i.d. random\nsample x1, . . . ,xn is obtained from f, we can approximate \u00b5 by a sample average:\n(6.1)\n\nh(xi) \u2192\" h(x)f(x) dx = \u00b5\n\n\u02c6\u00b5mc =\n\n1\nn\n\nn!i=1\n\ncomputational statistics, second edition. geof h. givens and jennifer a. hoeting.\n\u00a9 2013 john wiley & sons, inc. published 2013 by john wiley & sons, inc.\n\n151\n\n "}, {"Page_number": 163, "text": "chapter 6 simulation and monte carlo integration\n\n152\nas n \u2192 \u221e, by the strong law of large numbers (see section 1.6). further, let v(x) =\n[h(x) \u2212 \u00b5]2, and assume that h(x)2 has finite expectation under f. then the sampling\nvariance of \u02c6\u00b5mc is \u03c32/n = e{v(x)/n}, where the expectation is taken with respect\nto f. a similar monte carlo approach can be used to estimate \u03c32 by\n\n#var{ \u02c6\u00b5mc} =\n\n1\nn \u2212 1\n\nn!i=1$h(xi) \u2212 \u02c6\u00b5mc%2\n\n.\n\n(6.2)\n\nwhen \u03c32 exists,thecentrallimittheoremimpliesthat \u02c6\u00b5mc hasanapproximatenormal\ndistribution for large n, so approximate confidence bounds and statistical inference\nfor \u00b5 follow. generally, it is straightforward to extend (6.1), (6.2), and most of the\nmethods in this chapter to cases when the quantity of interest is multivariate, so it\nsuffices hereafter to consider \u00b5 to be scalar.\nmonte carlo integration provides slow o(n\u22121/2) convergence. with n nodes,\nthe quadrature methods described in chapter 5 offer convergence of order o(n\u22122) or\nbetter. there are several reasons why monte carlo integration is nonetheless a very\npowerful tool.\nmost importantly, quadrature methods are difficult to extend to multidimen-\nsional problems because general p-dimensional space is so vast. straightforward\nproduct rules creating quadrature grids of size np quickly succumb to the curse of\ndimensionality (discussed in section 10.4.1), becoming harder to implement and\nslower to converge. monte carlo integration samples randomly from f over the\np-dimensional support region of f, but does not attempt any systematic exploration\nof this region. thus, implementation of monte carlo integration is less hampered by\nhigh dimensionality than is quadrature. however, when p is large, a very large sample\nsize may still be required to obtain an acceptable standard error for \u02c6\u00b5mc. quadra-\nture methods also perform best when h is smooth, even when p = 1. in contrast,\nthe monte carlo integration approach ignores smoothness. further comparisons are\noffered in [190].\nmonte carlo integration replaces the systematic grid of quadrature nodes with a\nset of points chosen randomly from a probability distribution. the first step, therefore,\nis to study how to generate such draws. this topic is addressed in sections 6.2 and\n6.3. methods for improving upon the standard estimator given in equation (6.1) are\ndescribed in section 6.4.\n\n6.2 exact simulation\nmontecarlointegrationmotivatesourfocusonsimulationofrandomvariablesthatdo\nnot follow a familiar parametric distribution. we refer to the desired sampling density\nf as the target distribution. when the target distribution comes from a standard\nparametric family, abundant software exists to easily generate random deviates. at\nsome level, all of this code relies on the generation of standard uniform random\ndeviates. given the deterministic nature of the computer, such draws are not really\nrandom, but a good generator will produce a sequence of values that are statistically\n\n "}, {"Page_number": 164, "text": "6.2 exact simulation\n\n153\nindistinguishablefromindependentstandarduniformvariates.generationofstandard\nuniformrandomdeviatesisaclassicproblemstudiedin[195,227,383,538,539,557].\nrather than rehash the theory of uniform random number generation, we focus\non the practical quandary faced by those with good software: what should be done\nwhen the target density is not one easily sampled using the software? for example,\nnearly all bayesian posterior distributions are not members of standard parametric\nfamilies. posteriors obtained when using conjugate priors in exponential families are\nexceptions.\nthere can be additional difficulties beyond the absence of an obvious method\nto sample f. in many cases\u2014especially in bayesian analyses\u2014the target density\nmay be known only up to a multiplicative proportionality constant. in such cases, f\ncannot be sampled and can only be evaluated up to that constant. fortunately, there\nare a variety of simulation approaches that still work in this setting.\nfinally, it may be possible to evaluate f, but computationally expensive. if each\ncomputationof f(x)requiresanoptimization,anintegration,orothertime-consuming\ncomputations, we may seek simulation strategies that avoid direct evaluation of f as\nmuch as possible.\nsimulation methods can be categorized by whether they are exact or approxi-\nmate. the exact methods discussed in this section provide samples whose sampling\ndistribution is exactly f. later, in section 6.3, we introduce methods producing sam-\nples from a distribution that approximates f.\n\n6.2.1 generating from standard parametric families\nbefore discussing sampling from difficult target distributions, we survey some strate-\ngies for producing random variates from familiar distributions using uniform random\nvariates. we omit justifications for these approaches, which are given in the refer-\nences cited above. table 6.1 summarizes a variety of approaches. although the tabled\napproaches are not necessarily state of the art, they illustrate some of the underlying\nprinciples exploited by sophisticated generators.\n\ninverse cumulative distribution function\n\n6.2.2\nthe methods for the cauchy and exponential distributions in table 6.1 are justi-\nfied by the inverse cumulative distribution function or probability integral trans-\nform approach. for any continuous distribution function f, if u \u223c unif(0,1), then\nx = f\u22121(u) = inf{x : f(x) \u2265 u} has a cumulative distribution function equal to f.\nif f\u22121 is available for the target density, then this strategy is probably the\nsimplestoption.if f\u22121 isnotavailablebut f iseitheravailableoreasilyapproximated,\nthenacrudeapproachcanbebuiltuponlinearinterpolation.usingagridof x1, . . . , xm\nspanning the region of support of f, calculate or approximate ui = f(xi) at each grid\npoint. then, draw u \u223c unif(0,1) and linearly interpolate between the two nearest\ngrid points for which ui \u2264 u \u2264 uj according to\n\nx =\n\nuj \u2212 u\nuj \u2212 ui\n\nxi +\n\nu \u2212 ui\nuj \u2212 ui\n\nxj.\n\n(6.3)\n\n "}, {"Page_number": 165, "text": "1\n5\n4\n\ntable 6.1 some methods for generating a random variable x from familiar distributions.\ndistribution\nuniform\nnormal(\u00b5, \u03c32) and\nlognormal(\u00b5, \u03c32)\nmultivariate n(\u00b5, \u0001)\ncauchy(\u03b1, \u03b2)\nexponential(\u03bb)\npoisson(\u03bb)\ngamma(r, \u03bb)\nchi-square (df = k)\nstudent\u2019s t (df = k) and fk,m\ndistribution\nbeta(a, b)\nbernoulli(p) and\nbinomial(n, p)\n\nmethod\nsee [195, 227, 383, 538, 539, 557]. for x \u223c unif(a, b); draw u \u223c unif(0,1); then let x = a + (b \u2212 a)u.\ndraw u1, u2 \u223c i.i.d. unif(0,1); then x1 = \u00b5 + \u03c3&\u22122 log u1 cos{2\u03c0u2} and x2 = \u00b5 + \u03c3&\u22122 log u1 sin{2\u03c0u2} are\nindependent n(\u00b5, \u03c32). if x \u223c n(\u00b5, \u03c32) then exp{x} \u223c lognormal(\u00b5, \u03c32).\ngenerate standard multivariate normal vector, y, coordinatewise; then x = \u0001\u22121/2y + \u00b5.\n2)}.\ndraw u \u223c unif(0,1); then x = \u03b1 + \u03b2 tan{\u03c0(u \u2212 1\ndraw u \u223c unif(0,1); then x = \u2212(log u)/\u03bb.\ndraw u1, u2, . . .\u223ci.i.d. unif(0,1); then x= j\u22121, where j is the lowest index for which\u2019j\nsee example 6.1, references, or for integer r, x = \u2212(1/\u03bb)(r\ndraw y1, . . . , yk \u223c i.i.d. n(0,1), then x =(k\ni=1 y2\ndraw y \u223c n(0,1), z \u223c \u03c72\nf distribution.\ndraw y \u223c gamma(a,1) and z \u223c gamma(b,1) independently; then x = y/(y + z).\ndraw u \u223c unif(0,1); then x = 1{u<p} is bernoulli(p). the sum of n independent bernoulli(p) draws has a binomial(n, p)\ndistribution.\ndraw u1, . . . , ur \u223ci.i.d. unif(0,1); then x=(r\npartition [0,1] into k segments so the ith segment has length pi. draw u \u223c unif(0,1); then let x equal the index of the segment\ninto which u falls. tally such draws for multinomial(n,(p1, . . . , pk)).\ndraw independent yi \u223c gamma(\u03b1i,1) for i = 1, . . . , k; then xt=)y1/(k\n\ni=1\u230a(log ui)/log{1\u2212p}\u230b, and \u230a\u00b7\u230b means greatest integer.\ni=1 yi*.\n\nm independently, then x = y/\u221az/k has the t distribution and f = (z/k)/(w/m) has the\n\ni=1 ui < e\u2212\u03bb.\ni=1 log ui for u1, . . . , ur \u223c i.i.d. unif(0,1).\n\nnegative binomial(r, p)\nmultinomial(1,(p1, . . . , pk))\n\ni=1 yi, . . . , yk/(k\n\ni ; or draw x \u223c gamma(k/2, 1\n2).\n\ndirichlet(\u03b11, . . . , \u03b1k)\n\nk, w \u223c \u03c72\n\n "}, {"Page_number": 166, "text": "6.2 exact simulation\n\n155\nalthough this approach is not exact, we include it in this section because the degree of\napproximation is deterministic and can be reduced to any desired level by increasing\nm sufficiently. compared to the alternatives, this simulation method is not appealing\nbecause it requires a complete approximation to f regardless of the desired sample\nsize, it does not generalize to multiple dimensions, and it is less efficient than other\napproaches.\n\n6.2.3 rejection sampling\nif f(x) can be calculated, at least up to a proportionality constant, then we can use\nrejection sampling to obtain a random draw from exactly the target distribution. this\nstrategy relies on sampling candidates from an easier distribution and then correcting\nthe sampling probability through random rejection of some candidates.\nlet g denote another density from which we know how to sample and for which\nwe can easily calculate g(x). let e(\u00b7) denote an envelope, having the property e(x) =\ng(x)/\u03b1 \u2265 f(x) for all x for which f(x) > 0 for a given constant \u03b1 \u2264 1. rejection\nsampling proceeds as follows:\n1. sample y \u223c g.\n2. sample u \u223c unif(0,1).\n3. reject y if u > f(y)/e(y). in this case, do not record the value of y as an\n4. otherwise, keep the value of y. set x = y, and consider x to be an element\nof the target random sample. return to step 1 until you have accumulated a\nsample of the desired size.\n\nelement in the target random sample. instead, return to step 1.\n\nthe draws kept using this algorithm constitute an i.i.d. sample from the target density\nf; there is no approximation involved. to see this, note that the probability that a kept\ndraw falls at or below a value y is\n\nf(y)\n\nf(y)\n\np[x \u2264 y] = p+y \u2264 y,,,,u \u2264\ne(y)-\n= p+y \u2264 y and u \u2264\ne(y)-. p+u \u2264\ndu g(z) dz/\" \u221e\n\u2212\u221e\" f(z)/e(z)\n= \" y\n=\" y\n\n0\nf(z) dz,\n\ne(y)-\n\u2212\u221e\" f(z)/e(z)\n\nf(y)\n\n\u2212\u221e\n\n0\n\ndu g(z) dz\n\n(6.4)\n\n(6.5)\n\nwhich is the desired probability. thus, the sampling distribution is exact, and \u03b1 can\nbe interpreted as the expected proportion of candidates that are accepted. hence \u03b1 is\na measure of the efficiency of the algorithm. we may continue the rejection sampling\nprocedureuntilityieldsexactlythedesirednumberofsampledpoints,butthisrequires\na random total number of iterations that will depend on the proportion of rejections.\n\n "}, {"Page_number": 167, "text": "156\n\nchapter 6 simulation and monte carlo integration\n\ne(y)\n\n0\n\nt\nc\ne\nj\ne\nr\n\np\ne\ne\nk\n\ne(x)\n\nf(x)\n\nfigure 6.1 illustration of rejection sampling for a target distribution f using a rejection\nsampling envelope e.\n\ny\n\nrecall the rejection rule in step 3 for determining the fate of a candidate draw,\ny = y. sampling u \u223c unif(0,1) and obeying this rule is equivalent to sampling\nu|y \u223c unif(0, e(y)) and keeping the value y if u < f(y). consider figure 6.1. sup-\npose the value y falls at the point indicated by the vertical bar. then imagine sampling\nu|y = y uniformly along the vertical bar. the rejection rule eliminates this y draw\nwith probability proportional to the length of the bar above f(y) relative to the overall\nbarlength.therefore,onecanviewrejectionsamplingassamplinguniformlyfromthe\ntwo-dimensional region under the curve e and then throwing away any draws falling\nabove f and below e. since sampling from f is equivalent to sampling uniformly\nfrom the two-dimensional region under the curve labeled f(x) and then ignoring the\nvertical coordinate, rejection sampling provides draws exactly from f.\nthe shaded region in figure 6.1 above f and below e indicates the waste. the\ndraw y = y is very likely to be rejected when e(y) is far larger than f(y). envelopes\nthat exceed f everywhere by at most a slim margin produce fewer wasted (i.e.,\nrejected) draws and correspond to \u03b1 values near 1.\nsuppose now that the target distribution f is only known up to a proportionality\nconstant c. that is, suppose we are only able to compute easily q(x) = f(x)/c, where\nc is unknown. such densities arise, for example, in bayesian inference when f is a\nposterior distribution known to equal the product of the prior and the likelihood scaled\nby some normalizing constant. fortunately, rejection sampling can be applied in such\ncases. we find an envelope e such that e(x) \u2265 q(x) for all x for which q(x) > 0. a\ndraw y = yisrejectedwhen u > q(y)/e(y).thesamplingprobabilityremainscorrect\nbecausetheunknownconstant c cancelsoutinthenumeratoranddenominatorof(6.4)\nwhen f is replaced by q. the proportion of kept draws is \u03b1/c.\nmultivariate targets can also be sampled using rejection sampling, provided that\na suitable multivariate envelope can be constructed. the rejection sampling algorithm\nis conceptually unchanged.\n\n "}, {"Page_number": 168, "text": "6.2 exact simulation\n\n157\nto produce an envelope we must know enough about the target to bound it.\nthis may require optimization or a clever approximation to f or q in order to ensure\nthat e can be constructed to exceed the target everywhere. note that when the target is\ncontinuous and log-concave, it is unimodal. if we select x1 and x2 on opposite sides\nof that mode, then the function obtained by connecting the line segments that are\ntangent to log f or log q at x1 and x2 yields a piecewise exponential envelope with\nexponential tails. deriving this envelope does not require knowing the maximum of\nthe target density; it merely requires checking that x1 and x2 lie on opposite sides of\nit. the adaptive rejection sampling method described in section 6.2.3.2 exploits this\nidea to generate good envelopes.\nto summarize, good rejection sampling envelopes have three properties: they\nare easily constructed or confirmed to exceed the target everywhere, they are easy to\nsample, and they generate few rejected draws.\nexample 6.1 (gamma deviates)\nthe problem of generating a\ngamma(r,1) random variable when r \u2265 1. when y is generated according to the\ndensity\n\nconsider\n\nt(y)r\u22121t\u2032(y)exp{\u2212t(y)}\n\n1\n\nf(y) =\n\n(6.6)\n\u0001(r)\n3,and b = 1/\u221a9a,then x = t(y)\nfor t(y) = a(1 + by)3 for\u22121/b < y < \u221e, a = r \u2212\nwill have a gamma(r,1) distribution [443]. marsaglia and tsang describe how to use\nthisfactinarejectionsamplingframework[444].adopt(6.6)asthetargetdistribution\nbecause transforming draws from f gives the desired gamma draws.\nsimplifying f and ignoring the normalizing constant, we wish to generate from\nthedensitythatisproportionalto q(y) = exp{alog{t(y)/a} \u2212 t(y) + a}.conveniently,\nq fits snugly under the function e(y) = exp{\u2212y2/2}, which is the unscaled stan-\ndard normal density. therefore, rejection sampling amounts to sampling a standard\nnormal random variable, z, and a standard uniform random variable, u, then setting\nx = t(z) if\n\nu \u2264\n\nq(z)\n\ne(z) = exp0 z2\n\n2 + alog1 t(z)\n\na 2 \u2212 t(z) + a3\n\n(6.7)\n\nand t(z) > 0.otherwise,thedrawisrejectedandtheprocessbegunanew.anaccepted\ndraw has density gamma(r,1). draws from gamma(r,1) can be rescaled to obtain\ndraws from gamma(r, \u03bb).\nin a simulation when r = 4, over 99% of candidate draws are accepted and a\nplot of e(y) and q(y) against y shows that the two curves are nearly superimposed.\neven in the worst case (r = 1), the envelope is excellent, with less than 5% waste.!\nexample 6.2 (sampling a bayesian posterior)\nsuppose 10 independent obser-\nvations (8,3,4,3,1,7,2,6,2,7) are observed from the model xi|\u03bb \u223c poisson(\u03bb).\na lognormal prior distribution for \u03bb is assumed: log \u03bb \u223c n(log4,0.52). denote the\nlikelihood as l(\u03bb|x) and the prior as f(\u03bb). we know that \u02c6\u03bb = \u00afx = 4.3 maximizes\nl(\u03bb|x) with respect to \u03bb; therefore the unnormalized posterior, q(\u03bb|x) = f(\u03bb)l(\u03bb|x)\n\n "}, {"Page_number": 169, "text": "158\n\nchapter 6 simulation and monte carlo integration\n\n \n\n \n\n1\n1\n0\n1\n\u00d7\ny\nt\ni\ns\nn\ne\nd\nd\ne\nz\ni\nl\na\nm\nr\no\nn\nn\nu\n\n \n\n3\n\n2\n\n1\n\n0\n\n0\n\n10\n\u03bb\n\n20\n\nfigure 6.2 unnormalized target (dotted) and envelope (solid) for rejection sampling in\nexample 6.2.\nis bounded above by e(\u03bb) = f(\u03bb)l(4.3|x). figure 6.2 shows q and e. note that the\nprior is proportional to e. thus, rejection sampling begins by sampling \u03bbi from\nthe lognormal prior and ui from a standard uniform distribution. then \u03bbi is kept\nif ui < q(\u03bbi|x)/e(\u03bbi) = l(\u03bbi|x)/l(4.3|x). otherwise, \u03bbi is rejected and the process\nis begun anew. any kept \u03bbi is a draw from the posterior. although not efficient\u2014only\nabout 30% of candidate draws are kept\u2014this approach is easy and exact.\n!\nsqueezed rejection sampling ordinary rejection sampling requires\n6.2.3.1\none evaluation of f for every candidate draw y. in cases where evaluating f is\ncomputationally expensive but rejection sampling is otherwise appealing, improved\nsimulation speed is achieved by squeezed rejection sampling [383, 441, 442].\nthis strategy preempts the evaluation of f in some instances by employing a\nnonnegative squeezing function, s. for s to be a suitable squeezing function, s(x) must\nnot exceed f(x) anywhere on the support of f. an envelope, e, is also used; as with\nordinary rejection sampling, e(x) = g(x)/\u03b1 \u2265 f(x) on the support of f.\nthe algorithm proceeds as follows:\n1. sample y \u223c g.\n2. sample u \u223c unif(0,1).\n3. if u \u2264 s(y)/e(y), keep the value of y. set x = y and consider x to be an\nelement in the target random sample. then go to step 6.\n4. otherwise, determine whether u \u2264 f(y)/e(y). if this inequality holds, keep\nthe value of y, setting x = y. consider x to be an element in the target random\nsample; then go to step 6.\n5. if y has not yet been kept, reject it as an element in the target random sample.\n6. return to step 1 until you have accumulated a sample of the desired size.\n\n "}, {"Page_number": 170, "text": "6.2 exact simulation\n\n159\n\ne(y)\n\n0\n\nt\nc\ne\nj\ne\nr\n\nr\ne\nt\na\nl\np\ne\ne\nk\n\nt\ns\nr\ni\n\n \n\nf\np\ne\ne\nk\n\ne(x)\n\nf(x)\n\ns(x)\n\ny\n\nfigure 6.3 illustration of squeezed rejection sampling for a target distribution, f, using\nenvelope e and squeezing function s. keep first and keep later correspond to steps 3 and 4 of\nthe algorithm, respectively.\n\nnote that when y = y, this candidate draw is kept with overall probability f(y)/e(y),\nand rejected with probability [e(y) \u2212 f(y)]/e(y). these are the same probabilities as\nwith simple rejection sampling. step 3 allows a decision to keep y to be made on\nthe basis of an evaluation of s, rather than of f. when s nestles up just underneath f\neverywhere, we achieve the largest decrease in the number of evaluations of f.\nfigure 6.3 illustrates the procedure. when a candidate y = y is sampled, the\nalgorithm proceeds in a manner equivalent to sampling a unif(0, e(y)) random vari-\nable. if this uniform variate falls below s(y), the candidate is kept immediately. the\nlighter shaded region indicates where candidates are immediately kept. if the can-\ndidate is not immediately kept, then a second test must be employed to determine\nwhether the uniform variate falls under f(y) or not. finally, the darker shaded region\nindicates where candidates are ultimately rejected.\nas with rejection sampling, the proportion of candidate draws kept is \u03b1. the\nsqueezed rejection sampling can also be carried out when the target is known\nonlyupto aproportionalityconstant.inthiscase, theenvelopeandsqueezingfunction\nsandwich the unnormalized target. the method is still exact, and the same efficiency\nconsiderations apply.\n\nproportion of iterations in which evaluation of f is avoided is4 s(x) dx54 e(x) dx.\n\ngeneralizations for sampling multivariate targets are straightforward.\n\n6.2.3.2 adaptive rejection sampling clearly the most challenging aspect of\nthe rejection sampling strategy is the construction of a suitable envelope. gilks and\nwild proposed an automatic envelope generation strategy for squeezed rejection sam-\npling for a continuous, differentiable, log-concave density on a connected region of\nsupport [244].\n\n "}, {"Page_number": 171, "text": "160\n\nchapter 6 simulation and monte carlo integration\n\ne*\n5(x)\n\n(x)\n\ns*\n5(x)\n\nx1\n\nx2\n\nx3\n\nx4\n\nx5\n\nfigure 6.4 piecewise linear outer and inner hulls for \u2113(x) = log f(x) used in adaptive\nrejection sampling when k = 5.\n\nthe approach is termed adaptive rejection sampling because the envelope and\nsqueezing function are iteratively refined concurrently with the generation of sample\ndraws. the amount of waste and the frequency with which f must be evaluated both\nshrink as iterations increase.\nlet \u2113(x) = log f(x), and assume f(x) > 0 on a (possibly infinite) interval of\nthe real line. let f be log-concave in the sense that \u2113(a) \u2212 2\u2113(b) + \u2113(c) < 0 for\nany three points in the support region of f for which a < b < c. under the additional\nassumptionsthat f iscontinuousanddifferentiable,notethat \u2113\u2032(x)existsanddecreases\nmonotonically with increasing x, but may have discontinuities.\nthealgorithmisinitiatedbyevaluating \u2113and \u2113\u2032 at k points, x1 < x2 < \u00b7\u00b7\u00b7 < xk.\nlet tk = {x1, . . . , xk}. if the support of f extends to \u2212\u221e, choose x1 such that\n\u2113\u2032(x1) > 0. similarly, if the support of f extends to\u221e, choose xk such that \u2113\u2032(xk) < 0.\ndefine the rejection envelope on tk to be the exponential of the piecewise linear\nupper hull of \u2113 formed by the tangents to \u2113 at each point in tk. if we denote the upper\nhull of \u2113 as e\u2217k, then the rejection envelope is ek(x) = exp{e\u2217k(x)}. to understand the\nconcept of an upper hull, consider figure 6.4. this figure shows \u2113 with a solid line\nand illustrates the case when k = 5. the dashed line shows the piecewise upper hull,\ne\u2217. it is tangent to \u2113 at each xi, and the concavity of \u2113 ensures that e\u2217k lies completely\nabove \u2113 everywhere else. one can show that the tangents at xi and xi+1 intersect at\n(6.8)\n\n\u2113(xi+1) \u2212 \u2113(xi) \u2212 xi+1\u2113\u2032(xi+1) + xi\u2113\u2032(xi)\n\nzi =\n\n\u2113\u2032(xi) \u2212 \u2113\u2032(xi+1)\n\nfor i = 1, . . . , k \u2212 1. therefore,\n\ne\u2217k(x) = \u2113(xi) + (x \u2212 xi)\u2113\u2032(xi)\n\nfor x \u2208 [zi\u22121, zi]\n\n(6.9)\n\n "}, {"Page_number": 172, "text": "6.2 exact simulation\n\n161\n\nx1\n\nx2\n\nx3\n\nx4\n\nx5\n\nfigure 6.5 envelopes and squeezing function for adaptive rejection sampling. the target\ndensity is the smooth, nearly bell-shaped curve. the first method discussed in the text, using\nthe derivative of \u2113, produces the envelope shown as the upper boundary of the lighter shaded\nregion. this corresponds to equation (6.9) and figure 6.4. later in the text, a derivative-\nfree method is presented. that envelope is the upper bound of the darker shaded region and\ncorrespondsto(6.11)andfigure6.6.thesqueezingfunctionforbothapproachesisgivenbythe\ndotted curve.\nand i = 1, . . . , k, with z0 and zk defined, respectively, to equal the (possibly infinite)\nlower and upper bounds of the support region for f. figure 6.5 shows the envelope\nek exponentiated to the original scale.\ndefinethesqueezingfunctionon tk tobetheexponentialofthepiecewiselinear\nlower hull of \u2113 formed by the chords between adjacent points in tk. this lower hull\nis given by\n\nxi+1 \u2212 xi\n\ns\u2217k(x) =\n\n(xi+1 \u2212 x)\u2113(xi) + (x \u2212 xi)\u2113(xi+1)\n\nfor x \u2208 [xi, xi+1]\n\n(6.10)\nand i = 1, . . . , k \u2212 1. when x < x1 or x > xk, let s\u2217k(x) = \u2212\u221e. thus the squeezing\nfunction is sk(x) = exp{s\u2217k(x)}. figure 6.4 shows a piecewise linear lower hull, s\u2217k(x),\nwhen k = 5. figure 6.5 shows the squeezing function sk on the original scale.\nfigures 6.4 and 6.5 illustrate several important features of the approach. both\ntherejectionenvelopeandthesqueezingfunctionarepiecewiseexponentialfunctions.\nthe envelope has exponential tails that lie above the tails of f. the squeezing function\nhas bounded support.\nadaptive rejection sampling is initialized by choosing a modest k and a corre-\nsponding suitable grid tk. the first iteration of the algorithm proceeds as for squeezed\nrejection sampling, using ek and sk as the envelope and squeezing function, respec-\ntively. when a candidate draw is accepted, it may be accepted without evaluating \u2113\nand \u2113\u2032 at the candidate if the squeezing criterion was met. however, it may also be\n\n "}, {"Page_number": 173, "text": "chapter 6 simulation and monte carlo integration\n\n162\naccepted at the second stage, where evaluation of \u2113 and \u2113\u2032 at the candidate is required.\nwhen a candidate is accepted at this second stage, the accepted point is added to\nthe set tk, creating tk+1. updated functions ek+1 and sk+1 are also calculated. then\niterations continue. when a candidate draw is rejected, no update to tk, ek, or sk is\nmade. further, we see now that a new point that matches any existing member of tk\nprovides no meaningful update to tk, ek, or sk.\ncandidate draws are taken from the density obtained by scaling the piecewise\nexponential envelope ek so that it integrates to 1. since each accepted draw is made\nusing a rejection sampling approach, the draws are an i.i.d. sample precisely from f.\nif f is known only up to a multiplicative constant, the adaptive rejection sampling\napproach may still be used, since the proportionality constant merely shifts \u2113, e\u2217k,\nand s\u2217k.\ngilks and co-authors have developed a similar approach that does not require\nevaluation of \u2113\u2032 [237, 240]. we retain the assumptions that f is log-concave with a\nconnected support region, along with the basic notation and setup for the tangent-\nbased approach above.\nfor the set of points tk, define li(\u00b7) to be the straight line function connecting\n(xi, \u2113(xi)) and (xi+1, \u2113(xi+1)) for i = 1, . . . , k \u2212 1. define\n\ne\u2217k(x) =\u23a7\u23aa\u23a8\u23aa\u23a9\n\nmin{li\u22121(x), li+1(x)}\nl1(x)\nlk\u22121(x)\n\nfor x \u2208 [xi, xi+1],\nfor x < x1,\nfor x > xk,\n\n(6.11)\n\nwith the convention that l0(x) = lk(x) = \u221e. then e\u2217k is a piecewise linear upper\nhull for \u2113 because the concavity of \u2113 ensures that li(x) lies below \u2113(x) on (xi, xi+1)\nand above \u2113(x) when x < xi or x > xi+1. the rejection sampling envelope is then\nek(x) = exp{e\u2217k(x)}.\nthe squeezing function remains as in (6.10). iterations of the derivative-free\nadaptive rejection sampling algorithm proceed analogously to the previous approach,\nwith tk, the envelope, and the squeezing function updated each time a new point is\nkept.\nfigure 6.6 illustrates the derivative-free adaptive rejection sampling algorithm\nfor the same target shown in figure 6.4. the envelope is not as efficient as when \u2113\u2032 is\nused. figure 6.5 shows the envelope on the original scale. the lost efficiency is seen\non this scale, too.\nregardless of the method used to construct ek, notice that one would prefer\nthe tk grid to be most dense in regions where f(x) is largest, near the mode of f.\nfortunately, this will happen automatically, since such points are most likely to be\nkept in subsequent iterations and included in updates to tk. grid points too far in the\ntails of f, such as x5, are not very helpful.\nsoftware for the tangent-based approach is available in [238]. the derivative-\nfree approach has been popularized by its use in the winbugs software for carrying\nout markov chain monte carlo algorithms to facilitate bayesian analyses [241, 243,\n610]. adaptive rejection sampling can also be extended to densities that are not log-\nconcave, for example, by applying markov chain monte carlo methods like those in\nchapter 7 to further correct the sampling probabilities. one strategy is given in [240].\n\n "}, {"Page_number": 174, "text": "6.3 approximate simulation\n\n163\n\ne*\n5(x)\n\n(x)\n\ns*\n5(x)\n\nx1\n\nx2\n\nx3\n\nx4\n\nx5\n\nfigure6.6 piecewiselinearouterandinnerhullsfor \u2113(x) = log f(x)usedinderivative-free\nadaptive rejection sampling when k = 5.\n\n6.3 approximate simulation\nalthough the methods described above have the appealing feature that they are exact,\nthere are many cases when an approximate method is easier or perhaps the only\nfeasible choice. despite how it might sound, approximation is not a critical flaw\nof these methods because the degree of approximation can be controlled by user-\nspecified parameters in the algorithms. the simulation methods in this section are\nall based to some extent on the sampling importance resampling principle, which we\ndiscuss first.\n\n6.3.1 sampling importance resampling algorithm\nthe sampling importance resampling (sir) algorithm simulates realizations approx-\nimately from some target distribution. sir is based upon the notion of importance\nsampling, discussed in detail in section 6.4.1. briefly, importance sampling proceeds\nby drawing a sample from an importance sampling function, g. informally, we will\ncall g an envelope. each point in the sample is weighted to correct the sampling prob-\nabilities so that the weighted sample can be related to a target density f. for example,\nthe weighted sample can be used to estimate expectations under f.\nhaving graphed some univariate targets and envelopes in the early part of this\nchapter to illustrate basic concepts, we shift now to multivariate notation to emphasize\nthe full generality of techniques. thus, x = (x1, . . . , xp) denotes a random vector\nwith density f(x), and g(x) denotes the density corresponding to a multivariate enve-\nlope for f.\n\n "}, {"Page_number": 175, "text": "164\n\nchapter 6 simulation and monte carlo integration\nfor the target density f, the weights used to correct sampling probabilities are\n\ncalled the standardized importance weights and are defined as\n\nw(xi) =\n\nf(xi)/g(xi)\n(m\ni=1 f(xi)/g(xi)\n\n(6.12)\n\nfor a collection of values x1, . . . ,xm drawn i.i.d. from an envelope g. although not\nnecessary for general importance sampling, it is useful to standardize the weights as\nin (6.12) so they sum to 1. when f = cq for some unknown proportionality constant\nc, the unknown c cancels in the numerator and denominator of (6.12).\nwe may view importance sampling as approximating f by the discrete distribu-\ntion having mass w(xi) on each observed point xi for i = 1, . . . , m. rubin proposed\nsampling from this distribution to provide an approximate sample from f [559, 560].\nthe sir algorithm therefore proceeds as follows:\n1. sample candidates y1, . . . ,ym i.i.d. from g.\n2. calculate the standardized importance weights, w(y1), . . . , w(ym).\n3. resample x1, . . . ,xn from y1, . . . ,ym with replacement with probabilities\n\nw(y1), . . . , w(ym).\na random variable x drawn with the sir algorithm has distribution that con-\nverges to f as m \u2192 \u221e. to see this, define w\u2217(y) = f(y)/g(y), let y1, . . . ,ym \u223c\ni.i.d. g, and consider a set a. then\np[x \u2208 a|y1, . . . ,ym] =\n\nw\u2217(yi).\n\n(6.13)\n\nthe strong law of large numbers gives\n\n1\nm\n\nm!i=1\n\nas m \u2192 \u221e. further,\n\nas m \u2192 \u221e. hence,\n\nm!i=1\n\n1{yi\u2208a}w\u2217(yi)/ m!i=1\n1{yi\u2208a}w\u2217(yi) \u2192 e:1{yi\u2208a}w\u2217(yi); =\"a\nw\u2217(yi) \u2192 e:w\u2217(yi); = 1\nw\u2217(y)g(y) dy =\"a\np[x \u2208 a|y1, . . . ,ym] \u2192\"a\np[x \u2208 a] = e{p[x \u2208 a|y1, . . . ,ym]} \u2192\"a\n\nm!i=1\n\n1\nm\n\nas m \u2192 \u221e. finally, we note that\n\nw\u2217(y)g(y) dy\n\n(6.14)\n\n(6.15)\n\nf(y) dy\n\n(6.16)\n\nf(y) dy\n\n(6.17)\n\nby lebesgue\u2019s dominated convergence theorem [49, 595]. the proof is similar when\nthe target and envelope are known only up to a constant [555].\n\n "}, {"Page_number": 176, "text": "6.3 approximate simulation\n\n165\nalthough both sir and rejection sampling rely on the ratio of target to en-\nvelope, they differ in an important way. rejection sampling is perfect, in the sense\nthat the distribution of a generated draw is exactly f, but it requires a random num-\nber of draws to obtain a sample of size n. in contrast, the sir algorithm uses a\npre-determined number of draws to generate an n-sample but permits a random\ndegree of approximation to f in the distribution of the sampled points.\nwhen conducting sir, it is important to consider the relative sizes of the initial\nsample and the resample. these sample sizes are m and n, respectively. in principle,\nwe require n/m \u2192 0 for distributional convergence of the sample. in the context\nof asymptotic analysis of monte carlo estimates based on sir, where n \u2192 \u221e, this\ncondition means that m \u2192 \u221e even faster than n \u2192 \u221e. for fixed n, distributional\nconvergence of the sample occurs as m \u2192 \u221e, therefore in practice one obviously\nwants to initiate sir with the largest possible m. however, one faces the competing\ndesire to choose n as large as possible to increase the inferential precision. the maxi-\nmum tolerable ratio n/m depends on the quality of the envelope. we have sometimes\n1\nfound n/m \u2264\n10 tolerable so long as the resulting resample does not contain too many\nreplicates of any initial draw.\nthe sir algorithm can be sensitive to the choice of g. first, the support of g\nmust include the entire support of f if a reweighted sample from g is to approximate\na sample from f. further, g should have heavier tails than f, or more generally g\nshould be chosen to ensure that f(x)/g(x) never grows too large. if g(x) is nearly\nzero anywhere where f(x) is positive, then a draw from this region will happen only\nextremely rarely, but when it does it will receive a huge weight.\nwhen this problem arises, the sir algorithm exhibits the symptom that one or\na few standardized importance weights are enormous compared to the other weights,\nand the secondary sample consists nearly entirely of replicated values of one or a\nfew initial draws. when the problem is not too severe, taking the secondary resample\nwithout replacement has been suggested [220]. this is asymptotically equivalent to\nsampling with replacement, but has the practical advantage that it prevents exces-\nsive duplication. the disadvantage is that it introduces some additional distributional\napproximation in the final sample. when the distribution of weights is found to be\nhighly skewed, it is probably wiser to switch to a different envelope or a different\nsampling strategy altogether.\nsince sir generates x1, . . . ,xn approximately i.i.d. from f, one may proceed\nwith monte carlo integration such as estimating the expectation of h(x) by \u02c6\u00b5sir =\n(n\ni=1 h(xi)/n as in (6.1). however, in section 6.4 we will introduce superior ways\nto use the initial weighted importance sample, along with other powerful methods to\nimprove monte carlo estimation of integrals.\n\nexample 6.3 (slash distribution) the random variable y has a slash distribution\nif y = x/u where x \u223c n(0,1) and u \u223c unif(0,1) independently. consider using\nthe slash distribution as a sir envelope to generate standard normal variates, and\nconversely using the normal distribution as a sir envelope to generate slash variates.\nsince it is easy to simulate from both densities using standard methods, sir is not\nneeded in either case, but examining the results is instructive.\n\n "}, {"Page_number": 177, "text": "166\n\nchapter 6 simulation and monte carlo integration\n\n0.4\n\n0.2\n\ny\nt\ni\ns\nn\ne\nd\n\n \nl\na\nm\nr\no\nn\n\n0\n\n\u22124\n\n0.2\n\n0.1\n\ny\nt\ni\ns\nn\ne\nd\nh\ns\na\nl\ns\n\n \n\n4\n\n0\n\n\u22127\n\n0\nx\n\n0\ny\n\n7\n\nfigure 6.7 the left panel shows a histogram of approximate draws from a standard normal\ndensity obtained via sir with a slash distribution envelope. the right panel shows a histogram\nof approximate draws from a slash density obtained via sir using a normal envelope. the solid\nlines show the target densities.\n\nthe slash density function is\n\ny2\u221a2\u03c0\n\n1 \u2212 exp{\u2212y2/2}\n1\n2\u221a2\u03c0\n\n,\n\n,\n\ny /= 0,\ny = 0.\n\nf(y) =\u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9\n\nthis density has very heavy tails. therefore, it is a fine importance sampling function\nfor generating draws from a standard normal distribution using sir. the left panel of\nfigure 6.7 illustrates the results when m = 100,000 and n = 5000. the true normal\ndensity is superimposed for comparison.\non the other hand, the normal density is not a suitable importance sampling\nfunction for sir use when generating draws from the slash distribution because the\nenvelope\u2019s tails are far lighter than the target\u2019s. the right panel of figure 6.7 (where,\nagain, m = 100,000 and n = 5000) illustrates the problems that arise. although the\ntails of the slash density assign appreciable probability as far as 10 units from the\norigin, no candidate draws from the normal density exceeded 5 units from the origin.\ntherefore, beyond these limits, the simulated tails of the target have been completely\ntruncated. further, the most extreme candidate draws generated have far less density\nunder the normal envelope than they do under the slash target, so their importance\nratios are extremely high. this leads to abundant resampling of these points in the\ntails. indeed, 528 of the 5000 values selected by sir are replicates of the three lowest\nunique values in the histogram.\n!\nexample 6.4 (bayesian inference) suppose that we seek a sample from the pos-\nterior distribution from a bayesian analysis. such a sample could be used to provide\nmonte carlo estimates of posterior moments, probabilities, or highest posterior den-\nsity intervals, for example. let f(\u03b8) denote the prior, and l(\u03b8|x) the likelihood, so\nthe posterior is f(\u03b8|x) = cf(\u03b8)l(\u03b8|x) for some constant c that may be difficult to\ndetermine. if the prior does not seriously restrict the parameter region favored by\n\n "}, {"Page_number": 178, "text": "6.3 approximate simulation\n\n167\nthe data via the likelihood function, then perhaps the prior can serve as a useful\nimportance sampling function. sample \u03b81, . . . , \u03b8m i.i.d. from f(\u03b8). since the target\ndensity is the posterior, the ith unstandardized weight equals l(\u03b8i|x). thus the sir\nalgorithm has a very simple form: sample from the prior, weight by the likelihood,\nand resample.\nfor instance, recall example 6.2. in this case, importance sampling could be-\ngin by drawing \u03bb1, . . . , \u03bbm \u223c i.i.d. lognormal(log4,0.52). the importance weights\nwould be proportional to l(\u03bbi|x). resampling from \u03bb1, . . . , \u03bbm with replacement\nwith these weights yields an approximate sample from the posterior.\n!\n6.3.1.1 adaptive importance, bridge, and path sampling in some circum-\nstances, one initially may be able to specify only a very poor importance sampling\nenvelope. this may occur, for example, when the target density has support nearly\nlimited to a lower-dimensional space or surface due to strong dependencies between\nvariables not well understood by the analyst. in other situations, one may wish to\nconduct importance sampling for a variety of related problems, but no single enve-\nlope may be suitable for all the target densities of interest. in situations like this, it is\npossible to adapt the importance sampling envelope.\none collection of ideas for envelope improvement is termed adaptive impor-\ntance sampling. an initial sample of size m1 is taken from an initial envelope e1.\nthis sample is weighted (and possibly resampled) to obtain an initial estimate of\nquantities of interest or an initial view of f itself. based on the information obtained,\nthe envelope is improved, yielding e2. further importance sampling and envelope\nimprovement steps are taken as needed. when such steps are terminated, it is most\nefficient to use the draws from all the steps, along with their weights, to formulate\nsuitable inference. alternatively, one can conduct quick envelope refinement during\nseveral initial steps, withholding the majority of simulation effort to the final stage\nand limiting inference to this final sample for simplicity.\nin parametric adaptive importance sampling, the envelope is typically assumed\nto belong to some family of densities indexed by a low-dimensional parameter. the\nbest choice for the parameter is estimated at each iteration, and the importance sam-\npling steps are iterated until estimates of this indexing parameter stabilize [189, 381,\n490, 491, 606]. in nonparametric adaptive importance sampling, the envelope is often\nassumed to be a mixture distribution, such as is generated with the kernel density\nestimation approach in chapter 10. importance sampling steps are alternated with\nenvelope updating steps, adding, deleting, or modifying mixture components. exam-\nples include [252, 657, 658, 680]. although potentially useful in some circumstances,\nthese approaches are overshadowed by markov chain monte carlo methods like those\ndescribed in chapter 7, because the latter are usually simpler and at least as effective.\nasecondcollectionofideasforenvelopeimprovementisrelevantwhenasingle\nenvelope is inadequate for the consideration of several densities. in bayesian statistics\nand certain marginal likelihood and missing data problems, one is often interested\nin estimating a ratio of normalizing constants for a pair of densities. for example, if\nfi(\u03b8|x) = ciqi(\u03b8|x)isthe ithposteriordensityfor \u03b8 (for i = 1,2)undertwocompeting\nmodels, where qi is known but ci is unknown, then r = c2/c1 is the posterior odds\nfor model 1 compared to model 2. the bayes factor is the ratio of r to the prior odds.\n\n "}, {"Page_number": 179, "text": "168\n\nchapter 6 simulation and monte carlo integration\n\nsinceitisoftendifficulttofindgoodimportancesamplingenvelopesforboth f1\nand f2, one standard importance sampling approach is to use only a single envelope\nto estimate r. for example, in the convenient case when the support of f2 contains that\nof f1 and we are able to use f2 as the envelope, r = e{q1(\u03b8|x)/q2(\u03b8|x)}. however,\nwhen f1 and f2 differ greatly, such a strategy will perform poorly because no single\nenvelope can be sufficiently informative about both c1 and c2. the strategy of bridge\nsampling employs an unnormalized density, qbridge, that is, in some sense, between\nq1 and q2 [457]. then noting that\n\nr =\n\nef2{qbridge(\u03b8|x)/q2(\u03b8|x)}\nef1{qbridge(\u03b8|x)/q1(\u03b8|x)}\n\n,\n\n(6.18)\n\nwe may employ importance sampling to estimate the numerator and the denominator,\nthus halving the difficulty of each task, since qbridge is nearer to each qi than the\nqi are to each other. these ideas have been extensively studied for bayesian model\nchoice [437].\nin principle, the idea of bridging can be extended by iterating the strategy\nemployed in (6.18) with a nested sequence of intermediate densities between q1 and\nq2. each neighboring pair of densities in the sequence between q1 and q2 would be\nclose enough to enable reliable estimation of the corresponding ratio of normalizing\nconstants, and from those ratios one could estimate r. in practice, it turns out that the\nlimit of such a strategy amounts to a very simple algorithm termed path sampling.\ndetails are given in [222].\n\n6.3.2 sequential monte carlo\nwhen the target density f becomes high dimensional, sir is increasingly inefficient\nand can be difficult to implement. specifying a very good high-dimensional envelope\nthat closely approximates the target with sufficiently heavy tails but little waste can\nbe challenging. sequential monte carlo methods address the problem by splitting the\nhigh-dimensional task into a sequence of simpler steps, each of which updates the\nprevious one.\nsuppose that x1:t = (x1, . . . , xt) represents a discrete time stochastic process\nwith xt being the observation at time t and x1:t representing the entire history of\nthe sequence thus far. the xt may be multidimensional, but for simplicity we adopt\nscalar notation here when possible. write the density of x1:t as ft. suppose that we\nwish to estimate at time t the expected value of h(x1:t) with respect to ft using an\nimportance sampling strategy.\na direct application of the sir approach from section 6.3.1 would be to draw\na sample of x1:t sequences from an envelope gt and then calculate the importance\nweighted average of this sample of h(x1:t) values. however, this overlooks a key\naspect of the problem. as t increases, x1:t and the expected value of h(x1:t) evolve.\nat time t it would be better to update previous inferences than to act as if we had no\nprevious information. indeed, it would be very inefficient to start the sir approach\nfrom scratch at each time t. instead we will develop a strategy that will enable us\nto append the simulated xt to the x1:t\u22121 previously simulated and to adjust the\n\n "}, {"Page_number": 180, "text": "6.3 approximate simulation\n\n169\nprevious importance weights in order to estimate the expected value of h(x1:t). such\nan approach is called sequential importance sampling [419].\nthe attractiveness of such incremental updating is particularly apparent when\nsequential estimation must be completed in real time. for example, when applied to\nobject tracking, sequential importance sampling can be used to estimate the current\nlocation of an object (e.g., a missile) from noisy observations made by remote sensors\n(e.g., radar). at any time, the true location is estimated from the current observation\nand the sequence of past observations (i.e., the estimated past trajectory). as track-\ning continues at each time increment, the estimation must keep up with the rapidly\nincoming data. a wide variety of sequential estimation techniques have been applied\nto tackle this tracking problem [95, 239, 287, 447].\napplicationsofsequentialimportancesamplingareextremelydiverse,spanning\nsciences from physics to molecular biology, and generic statistical problems from\nbayesian inference to the analysis of sparse contingency tables [108, 109, 167, 419].\nsequential importance sampling for markov processes let us\n6.3.2.1\nbegin with the simplifying assumption that x1:t is a markov process. in this case, xt\ndepends only on xt\u22121 rather than the whole history x1:t\u22121. then the target density\nft(x1:t) may be expressed as\n\nft(x1:t) = f1(x1)f2(x2|x1:1)f3(x3|x1:2)\u00b7\u00b7\u00b7 ft(xt|x1:t\u22121)\n\n= f1(x1)f2(x2|x1)f3(x3|x2)\u00b7\u00b7\u00b7 ft(xt|xt\u22121).\n\n(6.19)\n\nsuppose that we adopt the same markov form for the envelope, namely\n\ngt(x1:t) = g1(x1)g2(x2|x1)g3(x3|x2)\u00b7\u00b7\u00b7 gt(xt|xt\u22121).\n\n(6.20)\nusing the ordinary nonsequential sir algorithm (section 6.3.1), at time t one\nwould sample from gt(x1:t) and reweight each x1:t value by wt = ft(x1:t)/gt(x1:t).\nusing (6.19) and (6.20), we see that wt = u1u2 \u00b7\u00b7\u00b7 ut, where u1 = f1(x1)/g1(x1) and\nui = fi(xi|xi\u22121)/gi(xi|xi\u22121) for i = 2, . . . , t.\ngiven x1:t\u22121 and wt\u22121, we can take advantage of the markov property by\nsampling only the next component, namely xt, appending the value to x1:t\u22121, and\nadjusting wt\u22121 using the multiplicative factor ut. specifically, when the target and\nenvelope distributions are markov, a sequential importance sampling approach for\nobtaining at each time a sequence x1:t and corresponding weight wt is given in the\nsteps below. a sample of n such points and their weights can be used to approximate\nft(x1:t) and hence the expected value of h(x1:t). the algorithm is:\n1. sample x1 \u223c g1. let w1 = u1 = f1(x1)/g1(x1). set t = 2.\n2. sample xt|xt\u22121 \u223c gt(xt|xt\u22121).\n3. append xt to x1:t\u22121, obtaining x1:t.\n4. let ut = ft(xt|xt\u22121)/gt(xt|xt\u22121).\n5. let wt = wt\u22121ut. at the current time, wt is the importance weight for x1:t.\n6. increment t and return to step 2.\n\n "}, {"Page_number": 181, "text": "1\n\ni=1 w\n\ni=1 w\n\n1:t)/(n\n\nchapter 6 simulation and monte carlo integration\n\n4(xt \u2212 xt\u22121)23 .\n\nthis sample of size n, the weighted average(n\n\n170\nfor obtaining an independent sample of n draws x(i)\n1:t, i = 1, . . . , n, the above algo-\nrithm can be carried out by treating the n sequences one at a time or as a batch. using\n(i)\n(i)\nt h(x(i)\nt serves as the\nestimate of eft h(x1:t).\nit is not necessary to standardize the weights at the end of each cycle above, al-\nthoughwhenweseektoestimate eft h(x1:t)thenormalizationisnatural.section6.4.1\ndiscusses weight normalization in a more general context. section 6.3.2.5 describes\na generalization of sequential importance sampling that involves resampling with\nnormalized weights between cycles.\nexample 6.5 (simple markov process) let xt|xt\u22121 \u223c ft denote a markov pro-\ncess where\n(6.21)\n\nft(xt|xt\u22121) \u221d |cos{xt \u2212 xt\u22121}|exp0\u2212\n1:t with importance\nfor i = 1, . . . , n. suppose we wish to estimate \u03c3t the standard deviation\n\nfor each t we wish to obtain an importance-weighted sample of x(i)\n(i)\nweights w\nt\nof xt using the weighted estimator\n1\n(i)\ni=1(w\nt )2\n(i)\n.\ni=1 w\nt\n(i)\nt\u22121 \u223c n(x\n|x\n(i)\nt \u2212 x\n\n<\u03c3t ==\n1 \u2212(n\nt /(n\nt \u221d ,,,cos1x\n\nthus to update the (t \u2212 1)th sample, the x\n(i)\n(i)\nt = w\nt\u22121u\n\nt\u221212,,,exp0\u2212)x\nt\u22121,1.52*\n\u03c6)x\n(i)\nt are drawn, appended to the cor-\n1:t, with the respective weights updated accord-\n. we may then estimate \u03c3t using (6.22). at t = 100 we find\n<\u03c3t = 13.4. by comparison, when xt|xt\u22121 \u223c n(xt\u22121,22), then no sequential impor-\ntance sampling is needed and the analogous<\u03c3t = 9.9. thus, the cosine term in (6.21)\n\nwhere \u03c6(z; a, b) denotes the normal density for z with mean a and variance b.\nresponding past sequences to form x(i)\ning to w\n\ncontributes additional variance to the distribution ft.\nthis example is sufficiently simple that other approaches are possible, but our\nsequential importance sampling approach is very straightforward. however, as t in-\ncreases, the weights develop an undesirable degeneracy discussed in section 6.3.2.3.\nimplementing the ideas there makes this example more effective.\n!\n6.3.2.2 general sequential importance sampling the task of obtaining an\napproximate sample from ft(x1:t) was greatly simplified in section 6.3.2.1 because\n\nusing sequential importance sampling, at stage t we can begin by sampling\n(i)\nt\u22121,1.52). due to the markov property, the\n\nfrom a normal envelope x\nweight updates are\n\nt \u2212<\u00b5t)2>1/2\n\nwhere<\u00b5t =(n\n\nt\u22121*2543\n\n(i)\nt \u2212 x\n\nn!i=1\n\n(6.22)\n\ni=1 w\n\n(i)\nt (x\n\n(i)\nt x\n\n(i)\nu\n\n; x\n\n(i)\nt\n\n(i)\nt\n\n(i)\nt\n\n(i)\n\n(i)\n\n(i)\n\n(i)\n\n(i)\n\nw\n\n "}, {"Page_number": 182, "text": "(6.25)\n\n(6.26)\n\n171\nof the markov assumption. suppose now that we want to achieve the same goal in a\ncase where the markov property does not apply. then the target density is\n\n6.3 approximate simulation\n\nft(x1:t) = f1(x1)f2(x2|x1:1)f3(x3|x1:2)\u00b7\u00b7\u00b7 ft(xt|x1:t\u22121),\n\n(6.23)\nnoting x1:1 = x1. similarly, let us drop the markov property of the envelope,\npermitting\n(6.24)\n\ngt(x1:t) = g1(x1)g2(x2|x1:1)g3(x3|x1:2)\u00b7\u00b7\u00b7 gt(xt|x1:t\u22121).\n\nthe importance weights then take the form\n\nf1(x1)f2(x2|x1:1)f3(x3|x1:2)\u00b7\u00b7\u00b7 ft(xt|x1:t\u22121)\ng1(x1)g2(x2|x1:1)g3(x3|x1:2)\u00b7\u00b7\u00b7 ft(xt|x1:t\u22121)\nand the recursive updating expression for the importance weights is\n\nwt(x1:t) =\n\nwt(x1:t) = wt\u22121(x1:t\u22121) ft(xt|x1:t\u22121)\ngt(xt|x1:t\u22121)\n\nfor t > 1. example 6.6 presents an application of the approach described here for\nnon-markov sequences. first, however, we consider a potential problem with the\nsequential weights.\n6.3.2.3 weight degeneracy, rejuvenation, and effective sample size as\nthe importance weights are updated at each time, it is increasingly likely that the\nmajority of the total weight will be concentrated in only a few sample sequences. the\nreason for this is that each component of a sequence must be reasonably consistent\nwith each corresponding conditional density ft(xt|x1:t\u22121) as time progresses. each\ninstance when an unusual (i.e., unlikely) new component is appended to a sample\nsequence, this proportionally reduces the weight for that entire sequence. eventually\nfew\u2014if any\u2014sequences have avoided such pitfalls. we say that the weights are\nincreasingly degenerate as they become concentrated on only a few sequences x1:t.\ndegenerating weights degrade estimation performance.\nto identify such problems, the effective sample size can be used to measure\nthe efficiency of using the envelope g with target f [386, 418]. this assesses the\ndegeneracy of the weights.\ndegeneracy of the weights is related to their variability: as weights are in-\ncreasingly concentrated on a few samples with the remaining weights near zero, the\nvariance of the weights will increase. a useful measure of variability in the weights\nis the (squared) coefficient of variation (cv) given by\n\ncv2\n\ne:w(x) \u2212 e{w(x)};2\n{w(x)} =\n(e{w(x)})2\ne{w(x) \u2212 n\u22121}2\n=\nn\u22122\n2.\n= e{nw(x) \u2212 1}\n\n(6.27)\n\n "}, {"Page_number": 183, "text": "<cv2\n\n{w(x)} =\n\nchapter 6 simulation and monte carlo integration\n\nin this case, the coefficient of variation in (6.27) can be estimated as\n\n172\nnext we will identify how this quantity can be incorporated into a measure of weight\ndegeneracy.\nthe effective sample size can be interpreted as indicating that the n weighted\nsamples used in an importance sampling estimate are worth a certain number of\nunweighted i.i.d. samples drawn exactly from f. suppose that n samples have nor-\nmalized importance weights w(x(i)) for i = 1, . . . , n. if z of these weights are zero\nand the remaining n \u2212 z are equal to 1/(n \u2212 z), then estimation effectively relies on\nthe n \u2212 z points with nonzero weights.\nn!i=1\n[nw(x(i)) \u2212 1]2\nd(x(i))2\u23a4\u23a6\nn\u23a1\u23a3!s0\n+!s1\nn \u2212 z \u2212 1d2>\nn=z + (n \u2212 z)c n\nn \u2212 z \u2212 1\n\n(6.28)\nwhere d(x(i)) = nw(x(i)) \u2212 1 and the samples are partitioned based on whether the\nweight is 0 or 1 using s0 = {i : w(x(i)) = 0} and s1 = {i : w(x(i)) = 1/(n \u2212 z)}.\ntherefore n \u2212 z = n/(1 + <cv2\n{w(x)}). moreover, by the nature of the weights we\nare considering, it is intuitive that the effective sample size should be n \u2212 z since that\nis the number of nonzero weights. thus we can measure the effective sample size as\n(6.29)\n\nd(x(i))2\n\n1\nn\n1\n\n=\n\n=\n\n=\n\n1\n\nn\n\nn\n\n.\n\n\u02c6n(g, f) =\n\nlarger effective sample sizes are better, while smaller ones indicate increasing de-\ngeneracy. the notation \u02c6n(g, f) is used to stress that the effective sample size is a\nmeasure of the quality of g as an envelope for f. in the case where the weights have\nnot been standardized, the equivalent expression is\n\n{w(x)}\n\n1 + <cv2\n\n(6.30)\napplying (6.29) to calculate \u02c6n(g, f) from the standardized weights is straight-\n\n<n(g, f) =\n\nn\n\n.\n\nforward since\n\n1 +#var{w\u2217(x)}\nn!i=1enw(x(i)) \u2212 1f2\nn!i=1+w(x(i))2\n\u2212\nn!i=1\n\u2212 1\n\nw(x(i))2\n\nn\n\n2w(x(i))\n\n<cv2\n\n{w(x)} =\n\n1\nn\n\n= n\n\n= n\n\n1\n\nn2-\n\n+\n\n(6.31)\n\n "}, {"Page_number": 184, "text": "6.3 approximate simulation\n\n173\n\nx1\n\nx2\n\nx3\n\nx4\n\nx5\n\nf1(x1)\n\nf2(x2|x1:1)\n\nf3(x3|x1:2)\n\nf4(x4|x1:3)\n\nf5(x5|x1:4)\n\nfigure 6.8 illustration of sequential importance sampling. time progresses from left to\nright. samples are indicated by solid lines. densities are shaded boxes. points are shown with\ncircles; the area of the circle is an indication of the sequence\u2019s weight at that time. the dashed\nline indicates an instance of weight regeneration through sequence resampling. see the text for\na detailed discussion.\n\n(6.32)\n\nso\n\n\u02c6n(g, f) =\n\n1\n\n(n\ni=1 w(x(i))2 .\n\nin the case of sequential importance sampling, we may monitor\n\n\u02c6n t(g, f) to\nassess importance weight degeneracy. typically, the effective sample size will de-\ncrease as time progresses. at time t when the effective sample size falls below some\nthreshold, the collection of sequences x(i)\n1:t\u22121, i = 1, . . . , n, should be rejuvenated.\nthe simplest rejuvenation approach is to resample the sequences with replacement\n(i)\nwith probabilities equal to the wt(x\n1:t) and then reset all weights to 1/n. inclusion\nof this multinomial resampling step is sometimes called sequential importance sam-\npling with resampling [421] and is closely related to the notion of particle filters which\nfollows in section 6.3.2.5. we illustrate the approach in several examples below. a va-\nriety of more sophisticated approaches for reducing degeneration and implementing\nrejuvenation are cited in sections 6.3.2.4 and 6.3.2.5.\nfigure 6.8 illustrates the concepts discussed in the last two subsections. time\nincreases from left to right. the five shaded boxes and corresponding axes represent\nsome hypothetical univariate conditional densities (like histograms turned sideways)\nfor sampling the xt|x1:t\u22121. for each t, let gt be a uniform density over the range for\n(i)\nxt covered by the shaded boxes. starting at t = 1 there are points x\n1 for i \u2208 {1,2,3},\nrepresented by the three small circles on the leftmost edge of figure 6.8. their current\nweights are represented by the sizes of the circles; initially the weights are equal\nbecause g1 is uniform. to initiate a cycle of the algorithm, the three points receive\nweights proportional to f1(x1)/g1(x1). the weighted points are shown just touching\n\n "}, {"Page_number": 185, "text": "chapter 6 simulation and monte carlo integration\n\n174\nthe x1 axis. the point in the dominant region of f1 receives proportionally more\nweight than the other two, so its circle has grown larger. the effect is dampened in the\nfigure because the weights are rescaled for visual presentation to prevent the circles\nfrom growing too large to fit in the graph or too small to see.\nat time t = 2, new uniform draws are appended to the current points, thereby\nforming sequences of length 2. the lines between the first two density graphs indicate\nwhich x2 is appended to which x1. at this point, the sequence reaching the middle x2\npoint has the largest weight because its x1 point had high weight at the first stage and\nits x2 sits in a region of high density according to f2(x2|x1). the bottom path also\nhad increased weight because its x2 landed in the same high-density region for t = 2.\nagain, g2 has not influenced the weights because it is uniform.\nat the next time step, the sequences have grown by appending sampled x3\nvalues, and they have been reweighted according to f3(x3|x1:2). at this point, sup-\npose that the weights are sufficiently degenerate to produce an effective sample size\n<n(g3, f3) small enough to trigger regeneration of the weights. this event is indicated\nby the dashed vertical line. to fix the problem, we resample three sequences with\nreplacement from the current ones with probabilities proportional to their respective\nweights. in the figure, two sequences will now progress from the middle x3, one from\nthe bottom x3, and none from the top x3. the latter sequence becomes extinct, to be\nreplaced by two sequences having the same past up to t = 3 but different futures be-\nyond that. finally, the right portion of the figure progresses as before, with additional\nsamples at each t and corresponding adjustments to the weights.\n\nexample 6.6 (high-dimensional distribution) although the role of t has been\nemphasized here as indexing a sequence of random variables xt for t = 1,2, . . . ,\nsequential importance sampling can also be used as a strategy for the generic prob-\nlem of sampling from a p-dimensional target distribution fp for fixed p. we can\naccumulate an importance weighted sample from this high-dimensional distribution\none dimension at a time by sampling from the univariate conditional densities as sug-\ngested by (6.23) and (6.24). after p steps we obtain the x(i)\n1:p and their corresponding\nweights needed to approximate a sample from fp.\nfor example, consider a sequence of probability densities given by ft(x1:t) =\nkt exp{\u2212||x1:t||3/3} for some constants kt, where || \u00b7 || is the euclidean norm and\nt = 1, . . . , p. here x1:p is the random variable having density fp. although ft(x1:t)\nis a smooth unimodal density with tails somewhat lighter than the normal distribution,\nthere is no easy method for sampling from it directly. note also that the sequence of\nrandom variables x1:t is not markov because for each t the density of xt|x1:t\u22121\ndepends on all prior components of x1:t. thus this example requires the general\nsequential importance sampling approach described in section 6.3.2.2.\nlet us adopt a standard normal distribution for the envelope. the tth conditional\ndensity can be expressed as ft(xt|x1:t\u22121) = ft(x1:t)/ft\u22121(x1:t\u22121). the sequential im-\nportance sampling strategy is given by:\n(1)\n1. let t = 1. sample n points x\ni.i.d. from a standard normal\n1 , . . . , x\n1 ,,3/2;5\u03c6gx\n(i)\n(i)\n\n(n)\n1\ndistribution. calculate the initial weights as w\n\n1 = exp: \u2212,,x\n\n1 h\n\n(i)\n\n "}, {"Page_number": 186, "text": "175\nwhere \u03c6 is the standard normal density function. the constants kt vanish when\nthe weights are standardized and are therefore ignored.\n\n6.3 approximate simulation\n\n(i)\nt\n\nthese to the x(i)\n\n2. for t > 1 sample n points x\n\nfrom a standard normal distribution. append\n1:t\u22121, yielding the x(i)\n1:t.\n\n(i)\nt ).\n\n|x(i)\n1:t\u22121)/\u03c6(x\n\n(i)\n(i)\nt = ft(x\nt\n(i)\nt and standardize the new weights.\n\n3. calculate the weight adjustment factors u\n(i)\n(i)\n4. set w\nt = w\nt\u22121u\n5. calculate the effective sample size \u02c6n t(\u03c6, ft). let \u03b1 control the tolerance for\n1:t with\n1:t in favor of\n\ndegeneracy. if \u02c6n t(\u03c6, ft) < \u03b1n then resample the current sample of x(i)\n(i)\nt and discard the x(i)\nreplacement with probabilities equal to the w\nthis new sample. in this case, reset all the weights to 1/n.\n6. increment t and return to step 2 until t = p.\nnote that the resampling strategy in step 5 attempts to rejuvenate the sample peri-\nodically to reduce degeneracy; hence the algorithm is called sequential importance\nsampling with resampling.\nas an example, set p = 50, n = 5000, and \u03b1 = 0.2. over the 50 stages, step 5\nwas triggered 8 times. the median effective sample size during the process was\nabout 2000. suppose we simply drew 5000 points from a 50-dimensional standard\nnormal envelope and importance reweighted these points all at once. this one-step\napproach yielded an effective sample size of about 1.13. increasing the sample size\nto n = 250,000 only increases the effective sample size to 1.95 for the one-step\napproach. high-dimensional space is too vast to stumble across many p-dimensional\ndraws with high density using the one-step approach: good points must be generated\nsequentially one dimension at a time.\n!\n\n6.3.2.4\nsequential importance sampling for hidden markov models\nanotherclassofdistributionstowhichsequentialimportancesamplingcanbeapplied\neffectively is generated by hidden markov models. consider a markov sequence of\nunobservable variables x0, x1, x2, . . . indexed by time t. suppose that these vari-\nables represent the state of some markov process, so the distribution of xt|x1:t\u22121\ndepends only on xt\u22121. although these states are unobservable, suppose that there is\nalso an observable sequence of random variables y0, y1, y2, . . . where yt is dependent\non the process state at the same time, namely xt. thus we have the model\n\nyt \u223c py(yt|xt)\n\nand\n\nxt \u223c px(xt|xt\u22121)\n\nfor t = 1,2, . . . and px and py are density functions. this is called a hidden markov\nprocess.\nwe wish to use the observed y1:t as data to estimate the states x1:t of the hidden\nmarkov process. in the importance sampling framework ft(x1:t|y1:t) is the target\ndistribution.\nnote that there is a recursive relationship between ft and ft\u22121. specifically,\n(6.33)\n\nft(x1:t|y1:t) = ft(x1:t\u22121|y1:t\u22121)px(xt|xt\u22121)py(yt|xt).\n\n "}, {"Page_number": 187, "text": "ut =\n\n(6.34)\n\nft(x1:t|y1:t)\n\nchapter 6 simulation and monte carlo integration\n\n176\nsuppose that at time t we adopt the envelope gt(xt|x1:t\u22121) = px(xt|xt\u22121). then the\nmultiplicative update for the importance weights can be expressed as\nft(x1:t\u22121|y1:t\u22121)px(xt|xt\u22121) = py(yt|xt).\nthe final equality results from the substitution of (6.33) into (6.34).\n\nthisframeworkcanberecastinbayesianterms.inthiscase,x1:t areconsidered\nparameters. the prior distribution at time t is px(x0)\u2019t\ni=1 px(xi|xi\u22121). the likelihood\nis obtained from the observed data density, equaling\u2019t\ni=0 py(yi|xi). the posterior\nft(x1:t|y1:t) is proportional to the product of the prior and the likelihood, as obtained\nrecursively from (6.33). thus the importance weight update at time t is the likelihood\nobtained from the new data yt at time t. the sequential factorization given here is akin\ntoiteratingexample6.4inwhichwesampledfromthepriordistributionandweighted\nby the likelihood. a similar strategy is described by [113], where the procedure is\ngeneralized to sample dimensions in batches.\nexample 6.7 (terrain navigation)\nan airplane flying over uneven terrain can\nuse information about the ground elevation beneath it to estimate its current location.\nas the plane follows its flight path, sequential elevation measurements are taken.\nsimultaneously, an inertial navigation system provides an estimated travel direction\nand distance. at each time point the previously estimated location of the plane is\nupdated using both types of new information. interest in such problems arises in, for\nexample, military applications where the approach could serve as an alternative or\nbackup to global satellite systems. details on the terrain navigation problem are given\nby [30, 31, 287].\nlet the two-dimensional variable xt = (x1t, x2t) denote the true location of\nthe plane at time t, and let dt denote the measured drift, or shift in the plane location\nduring the time increment as measured by the inertial navigation system. the key\ndata for terrain navigation come from a map database that contains (or interpolates)\nthe true elevation m(xt) at any location xt.\n\nour hidden markov model for terrain navigation is\n\nand\n\nyt = m(xt) + \u03b4t\n\nxt = xt\u22121 + dt + \u03f5t\n\n(6.35)\nwhere \u03f5t and \u03b4t are independent random error processes representing the error in drift\nand elevation measurement, respectively, and yt is the observed elevation. we treat\ndt as a known term in the location process rather than a measurement and allow any\nmeasurement error to be subsumed into \u03f5t.\nfigure 6.9 shows a topographic map of a region in colorado. light shading\ncorrespondstohighergroundelevationandtheunitsaremeters.letussupposethatthe\nplaneisfollowingacirculararcspecifiedby101angles \u03b8t (for t = 0, . . . ,100)equally\nspaced between \u03c0/2 and 0, with the true location at time t being xt = (cos \u03b8t,sin \u03b8t)\nand the true drift dt being the difference between the locations at times t and t \u2212 1.\nlet us assume that measurement error in the elevation process can be modeled as\n\u03b4t \u223c n(0, \u03c32) where we assume \u03c3 = 75 here.\n\n "}, {"Page_number": 188, "text": "6.3 approximate simulation\n\n177\n\n0\n0\n0\n\n,\n\n0\n3\n\n0\n0\n0\n\n,\n\n0\n2\n\n0\n0\n0\n\n,\n\n0\n1\n\n0\n\nh\nt\nr\no\nn\n\u2013\nh\nt\nu\no\ns\n\n0\n\n10,000\n\n20,000\n\n30,000\n\neast\u2013west\n\nfigure 6.9 results of example 6.7 showing an image of ground elevations for a region of\ncolorado, where lighter shades correspond to higher elevations. the dashed line is the true,\nunknown, flight path and the solid line is the estimated path. the two are nearly the same.\n\n0\n\n0\n\nx2t\n\nsuppose that random error in location \u03f5t has a distribution characterized by\n\nt zt wherert =i\u2212x1t\n\n\u2212x2t \u2212x1tjandzt \u223c n2i0, q2i1\n\nk2jjwherewe\n\u03f5t = rt\n1\n2. this distribution gt(\u03f5t) effectively constitutes the impor-\ntake q = 400 and k =\ntance sampling envelope gt(xt|xt\u22121). this complicated specification is more simply\ndescribedbysayingthat \u03f5t hasabivariatenormaldistributionwithstandarddeviations\nq and kq, rotated so that the major axis of density contours is parallel to the tangent of\nthe flight path at the current location. a standard bivariate normal distribution would\nbe an alternative choice, but ours simulates the situation where uncertainty about the\ndistance flown during the time increment is greater than uncertainty about deviations\northogonal to the direction of flight.\n, . . . ,x(100)\n,\nalthough this number would be much greater in a real application. to initiate the\nmodel we sample from a bivariate normal distribution centered at x0 with standard\ndeviationsof50.inreallife,onecouldimaginethattheinitializationpointcorresponds\nto the departure airport or to some position update provided by occasional detection\nstations along the flight path, which provide highly accurate location data allowing\nthe current location to be \u201creinitialized.\u201d\nthe sequential importance sampling algorithm for this problem proceeds as\nfollows:\n1. initialize at t = 0. draw n starting points x(i)\n2. receive observed elevation data yt.\n\nin this example, we maintain n = 100 sampled trajectories x(1)\n\n0 for i = 1, . . . , n.\n\nt\n\nt\n\n "}, {"Page_number": 189, "text": "178\n\nchapter 6 simulation and monte carlo integration\n\n(i)\nt\n\nt = \u03c6(yt; m(x(i)\n(i)\n3. calculate the weight update factor u\n\nt ), \u03c32) where \u03c6(\u00b7; a, b2) is\nthe value of the normal density with mean a and standard deviation b evaluated\nat its argument.\n(i)\nt\u22121 = 1/n.\n\n4. update the weights according to w\n\nnormalize the weights so they sum to 1.\n\n(i)\n(i)\nt = w\nt\u22121u\n\n5. the current estimate of the true location is \u02c6xt =(n\n6. check for weight degeneracy by calculating the effective sample size, namely\n\u02c6n(gt, ft) = 1/(n\nt )2. if \u02c6n(gt, ft) < \u03b1n, then rejuvenate the sample ac-\n(i)\ni=1(w\ncording to the substeps below. here \u03b1 is a threshold that governs the lowest\ntolerable ratio of effective sample size to actual sample size. we used \u03b1 = 0.3.\nif rejuvenation is not needed, proceed straight to step 7.\nt,new, . . . ,x(n)\na. resample x(1)\nt,new from x(1)\nt with replacement with prob-\n(1)\n.\nt\n\n(n)\n, . . . , w\nt\n\n, . . . ,x(n)\n\n. if t = 0, then w\ni=1 w\n\n(i)\nt x(i)\n\n.\n\nt\n\nt\n\nb. replace the current sample of x(i)\n\nt with the new draws x(i)\n\nt,new. in other words,\n\nabilities w\nt = x(i)\nt,new.\n\nset x(i)\n\nc. reset the weights to w\n\n(i)\nt = 1/n for all i.\n(i)\n7. sample a set of location errors \u03f5\nt+1 \u223c gt+1(\u03f5).\n8. advance the set of locations according to x(i)\n9. increment t and return to step 2.\n\nt+1 = x(i)\n\nt + dt+1 + \u03f5\n\n(i)\nt+1.\n\n(i)\nt\n\nin this algorithm, each sequence x(i)\n\nnote that this algorithm incorporates the resampling option to reduce weight\ndegeneracy.\n1:t represents one possible path for the air-\n. fig-\nplane, and these complete paths have corresponding importance weights w\nure 6.9 shows the result. the dashed line is the true flight path and the solid line is\nthe estimated path calculated as in step 5 above, using the elevation data yt.\nthe results show very good tracking of the true locations. of course, the result\nis dependent on n and the magnitudes of noise in the state and observation processes.\nperformance here also benefits from the fact that colorado terrain is very distinctive\nwith large variation in elevations. in flat areas the procedure is less effective. although\nthe colorado rocky mountains are rarely flat, there are long ridges and valleys that\nhave relatively constant elevation, thereby tempting the algorithm to pursue false\ndirections. the algorithm will also struggle when the terrain exhibits localized topo-\nlogical features (e.g., hilltops) that resemble each other and are repeated across a\nregion of the map. in that case, some x\nalthough estimation performance is good in our example, maintenance of a\nlarge effective sample size was poor. a majority of the iterations included a rejuve-\nnation step. the data for this example are available from the book website.\n!\n\n(i)\nt may jump to the wrong hilltop.\n\nthe sequential importance sampling technique evolved from a variety of meth-\nods for sequential imputation and monte carlo analysis of dynamic systems [34, 386,\n\n "}, {"Page_number": 190, "text": "179\n416, 418, 430]. considerable research has focused on methods to improve or adapt\ngt and to slow weight degeneracy [90, 95, 168, 239, 260, 386, 417, 419, 679].\n\n6.3 approximate simulation\n\n6.3.2.5 particle filters aswehavediscussedabove,sequentialimportancesam-\npling is often of little help unless resampling steps are inserted in the algorithm at\nleast occasionally to slow weight degeneracy that is signaled by diminishing effective\nsample sizes. particle filters are sequential monte carlo strategies that emphasize\nthe need for preventing degeneracy [90, 167, 239, 274, 379]. they have been de-\nveloped primarily within the framework of hidden markov models as described in\nsection 6.3.2.4. at time t, the current sample of sequences is viewed as a collection of\nweighted particles. particles with high weights enjoy increasing influence in monte\ncarlo estimates, while underweighted particles are filtered away as t increases.\nparticlefilterscanbeseenasgeneralizationsofsequentialimportancesampling,\nor as specific strategies for sequential importance sampling with resampling. the\ndistinct names of these approaches hide their methodological similarity. as noted\npreviously, sequential importance sampling can be supplemented by a resampling\nstep where the x1:t are resampled with replacement with probability proportional to\ntheir current weights and then the weights are reset to 1/n. in the simplest case, this\nwould be triggered when the effective sample size diminished too much. adopting a\nparticlefiltermindset,onewouldinsteadresampleateach t.analgorithmdescribedas\na particle filter is often characterized by a stronger focus on resampling or adjustment\nof the new draws between or within cycles to prevent degeneracy.\nresampling alone does not prevent degeneracy. although low-weight se-\nquences are likely to be discarded, high-weight sequences are merely replicated rather\nthan diversified. particle filters favor tactics like perturbing or smoothing samples at\neach t. for example, with a particle filter one might supplement the resampling step by\nmoving samples according to a markov chain transition kernel with appropriate sta-\ntionary distribution [239]. alternatively, one could smooth the resampling step via a\nweightedsmoothedbootstrap[175],forexample,byreplacingthesimplemultinomial\nresampling with sampling from an importance-weighted mixture of smooth densities\ncentered at some or all the current particles [252, 273, 611, 658]. another strategy\nwould be to employ an adaptive form of bridge sampling (see section 6.3.1.1) to facil-\nitate the sequential sampling steps [260]. the references in section 6.3.2.4 regarding\nimprovements to sequential importance sampling are also applicable here.\nthe simplest particle filter is the bootstrap filter [274]. this approach relies\non a simple multinomial importance-weighted resample at each stage, rather than\nwaiting for the weights to degenerate excessively. in other words, sequences are\nresampled with replacement with probability proportional to their current weights,\nthentheweightsareresetto1/n.thesequentialimportancesamplingstrategywehave\npreviously described would wait until resampling was triggered by a low effective\nsample size before conducting such a resample.\n\nexample 6.8 (terrain navigation, continued)\nthe bootstrap filter is easy to\nimplement in the terrain navigation example. specifically, we always resample the\ncurrent collection of paths at each t.\n\n "}, {"Page_number": 191, "text": "180\n\nchapter 6 simulation and monte carlo integration\nthus, we replace step 6 in example 6.7 with\n\n6. regardless of the value of <nt(gt, ft), carry out the following substeps:\n\nwhere those substeps are (a), (b), and (c) listed under step 6 in that example.\nthe estimated flight path is qualitatively indistinguishable from the estimate in\nfigure 6.9.\n!\n\n6.4 variance reduction techniques\n\nthe simple monte carlo estimator of4 h(x)f(x) dx is\n\n\u02c6\u00b5mc =\n\nh(xi)\n\n1\nn\n\nn!i=1\n\nwhere the variables x1, . . . ,xn are randomly sampled from f. this approach is\nintuitively appealing, and we have thus far focused on methods to generate draws\nfrom f. in some situations, however, better monte carlo estimators can be derived.\nthese approaches are still based on the principle of averaging monte carlo draws,\nbut they employ clever sampling strategies and different forms of estimators to yield\nintegral estimates with lower variance than the simplest monte carlo approach.\n\nimportance sampling\n\n6.4.1\nsuppose we wish to estimate the probability that a die roll will yield a one. if we roll\nthe die n times, we would expect to see about n/6 ones, and our point estimate\nof the true probability would be the proportion of ones in the sample. the variance\nof this estimator is 5/36n if the die is fair. to achieve an estimate with a coefficient\nof variation of, say, 5%, one should expect to have to roll the die 2000 times.\nto reduce the number of rolls required, consider biasing the die by replacing the\nfaces bearing 2 and 3 with additional 1 faces. this increases the probability of rolling\na one to 0.5, but we are no longer sampling from the target distribution provided by\na fair die. to correct for this, we should weight each roll of a one by 1/3. in other\nwords, let yi = 1/3 if the roll is a one and yi = 0 otherwise. then the expectation of\nthe sample mean of the yi is 1/6, and the variance of the sample mean is 1/36n. to\nachieve a coefficient of variation of 5% for this estimator, one expects to need only\n400 rolls.\nthis improved accuracy is achieved by causing the event of interest to occur\nmore frequently than it would in the naive monte carlo sampling framework, thereby\nenabling more precise estimation of it. using importance sampling terminology, the\ndie-rolling example is successful because an importance sampling distribution (cor-\nresponding to rolling the die with three ones) is used to oversample a portion of the\nstate space that receives lower probability under the target distribution (for the out-\ncome of a fair die). an importance weighting corrects for this bias and can provide an\nimproved estimator. for very rare events, extremely large reductions in monte carlo\nvariance are possible.\n\n "}, {"Page_number": 192, "text": "181\nthe importance sampling approach is based upon the principle that the expec-\n\n6.4 variance reduction techniques\n\ntation of h(x) with respect to its density f can be written in the alternative form\n\nor even\n\ng(x) g(x) dx\n\n\u00b5 =\" h(x)f(x) dx =\" h(x) f(x)\n\u00b5 = 4 h(x)f(x) dx\n4 f(x) dx = 4 h(x)[f(x)/g(x)]g(x) dx\n4 [f(x)/g(x)]g(x) dx\n\n(6.36)\n\n(6.37)\n\n,\n\nwhere g is another density function, called the importance sampling function or\nenvelope.\nequation (6.36) suggests that a monte carlo approach to estimating e{h(x)}\nis to draw x1, . . . ,xn i.i.d. from g and use the estimator\n\n\u02c6\u00b5\u2217is =\n\n1\nn\n\nn!i=1\n\nh(xi)w\u2217(xi),\n\n(6.38)\n\nwhere w\u2217(xi) = f(xi)/g(xi) are unstandardized weights, also called importance\nratios. for this strategy to be convenient, it must be easy to sample from g and to\nevaluate f, even when it is not easy to sample from f.\nequation (6.37) suggests drawing x1, . . . ,xn i.i.d. from g and using the\nestimator\n\nn!i=1\n\n(6.39)\n\nh(xi)w(xi),\n\nwhere w(xi) = w\u2217(xi)/(n\n\n\u02c6\u00b5is =\ni=1 w\u2217(xi) are standardized weights. this second ap-\nproach is particularly important in that it can be used when f is known only up\nto a proportionality constant, as is frequently the case when f is a posterior density\nin a bayesian analyses.\nboth estimators converge by the same argument applied to the simple monte\ncarlo estimator given in (6.1), as long as the support of the envelope includes all of\nthe support of f. in order for the estimators to avoid excess variability, it is important\nthat f(x)/g(x) be bounded and that g have heavier tails than f. if this requirement is\nnot met, then some standardized importance weights will be huge. a rare draw from\ng with much higher density under f than under g will receive huge weight and inflate\nthe variance of the estimator.\nnaturally, g(x) often will be larger than f(x) when x \u223c g, yet it is easy to\nshow that e{f(x)/g(x)} = 1. therefore, if f(x)/g(x) is to have mean 1, this ratio\nmust sometimes be quite large to counterbalance the predominance of values between\n0 and 1. thus, the variance of f(x)/g(x) will tend to be large. hence, we should\nexpect the variance of h(x)f(x)/g(x) to be large, too. for an importance sampling\nestimate of \u00b5 to have low variance, therefore, we should choose the function g so that\nf(x)/g(x) is large only when h(x) is very small. for example, when h is an indicator\nfunction that equals 1 only for a very rare event, we can choose g to sample in a\nway that makes that event occur much more frequently, at the expense of failing to\n\n "}, {"Page_number": 193, "text": "chapter 6 simulation and monte carlo integration\n\n182\nsample adequately uninteresting outcomes for which h(x) = 0. this strategy works\nvery well in cases where estimation of a small probability is of interest, such as in\nestimation of statistical power, probabilities of failure or exceedance, and likelihoods\nover combinatorial spaces like those that arise frequently with genetic data.\nalso recall that the effective sample size given in (6.29) can be used to mea-\nsure the efficiency of an importance sampling strategy using envelope g. it can be\ninterpreted as indicating that the n weighted samples used in an importance sampling\nestimate are worth \u02c6n(g, f) unweighted i.i.d. samples drawn exactly from f and used\nin a simple monte carlo estimate [386, 417]. in this sense, it is an excellent way to\nassess the quality of the envelope g. section 6.3.2.2 provides further detail.\nthe choice between using the unstandardized and the standardized weights\ndepends on several considerations. first consider the estimator \u02c6\u00b5\u2217is defined in (6.38)\nusingtheunstandardizedweights. let t(x) = h(x)w\u2217(x).whenx1, . . . ,xn aredrawn\ni.i.d. from g, let \u00afw\u2217 and \u00aft denote averages of the w\u2217(xi) and t(xi), respectively. note\ne{ \u00afw\u2217} = e{w\u2217(x)} = 1. now,\n\ne{ \u02c6\u00b5\u2217is} =\n\n1\nn\n\ne{t(xi)} = \u00b5\n\nn!i=1\n\nvar{t(xi)} =\n\n1\nn\n\nvar{t(x)}.\n\n(6.40)\n\n(6.41)\n\nand\n\nvar{ \u02c6\u00b5\u2217is} =\n\n1\nn2\n\nn!i=1\n\n1\nn\n\n= \u00b5 \u2212\n\nthus \u02c6\u00b5\u2217is is unbiased, and an estimator of its monte carlo standard error is the sample\nstandard deviation of t(x1), . . . , t(xn) divided by n.\nnow consider the estimator \u02c6\u00b5is defined in (6.39) that employs importance\nweight standardization. note that \u02c6\u00b5is = \u00aft/ \u00afw\u2217. taylor series approximations yield\n+ \u00b7\u00b7\u00b7]2\n= e1\u00aft \u2212 (\u00aft \u2212 \u00b5)( \u00afw\u2217 \u2212 1) \u2212 \u00b5( \u00afw\u2217 \u2212 1) + \u00aft( \u00afw\u2217 \u2212 1)2\n+ \u00b7\u00b7\u00b72\nn2d .\nvar{w\u2217(x)} + oc 1\n\ne{ \u02c6\u00b5is} = e1\u00aft [1 \u2212 ( \u00afw\u2217 \u2212 1) + ( \u00afw\u2217 \u2212 1)2\n\ncov{t(x), w\u2217(x)} +\n\nthus, standardizing the importance weights introduces a slight bias in the estimator\n\u02c6\u00b5is.thebiascanbeestimatedbyreplacingthevarianceandcovariancetermsin(6.42)\nwith sample estimates obtained from the monte carlo draws; see also example 6.12.\n\nthe variance of \u02c6\u00b5is is similarly found to be\nvar{ \u02c6\u00b5is} =\n\nn$var{t(x)} + \u00b52 var{w\u2217(x)} \u2212 2\u00b5cov{t(x), w\u2217(x)}%\nn2d .\n+ oc 1\n\n(6.43)\nagain, a variance estimate for \u02c6\u00b5is can be computed by replacing the variances and\ncovariances in (6.43) with sample estimates obtained from the monte carlo draws.\n\n(6.42)\n\n\u00b5\nn\n\n1\n\n "}, {"Page_number": 194, "text": "183\nfinally, consider the mean squared errors (mse) of \u02c6\u00b5\u2217is and \u02c6\u00b5is. combining\n\n6.4 variance reduction techniques\n\nthe bias and variance estimates derived above, we find\n\nmse{ \u02c6\u00b5is} \u2212 mse{ \u02c6\u00b5\u2217is}\n\n1\n\nn2d .\nn)\u00b52 var{w\u2217(x)} \u2212 2\u00b5cov{t(x), w\u2217(x)}* + oc 1\n\n(6.44)\nassuming without loss of generality that \u00b5 > 0, the leading terms in (6.44) suggest\nthat the approximate difference in mean squared errors is negative when\n\n=\n\ncv{w\u2217(x)}\n2 cv{t(x)}\n\n,\n\ncor{t(x), w\u2217(x)} >\n\n(6.45)\nwhere cv{\u00b7} denotes a coefficient of variation. this condition can be checked\nusing sample-based estimators as discussed above. thus, using the standardized\nweights should provide a better estimator when w\u2217(x) and h(x)w\u2217(x) are strongly\ncorrelated. in addition to these considerations, a major advantage to using the\nstandardized weights is that it does not require knowing the proportionality constant\nfor f. hesterberg warns that using the standardized weights can be inferior to using\nthe raw weights in many settings, especially when estimating small probabilities, and\nrecommends consideration of an improved importance sampling strategy we describe\nbelow in example 6.12 [326]. casella and robert also discuss a variety of uses of the\nimportance weights [100].\nusing the standardized weights is reminiscent of the sir algorithm (sec-\ntion 6.3.1), and it is sensible to compare the estimation properties of \u02c6\u00b5is with those\nof the sample mean of the sir draws. suppose that an initial sample y1, . . . ,ym\nwith corresponding weights w(y1), . . . , w(ym) is resampled to provide n sir draws\nx1, . . . ,xn, where n < m. let\n\n\u02c6\u00b5sir =\n\n1\nn\n\nh(xi)\n\nn!i=1\n\ndenote the sir estimate of \u00b5.\n\u02c6\u00b5is ordinarily should be preferred over \u02c6\u00b5sir. to see this, note\n\nwhen interest is limited to estimation of \u00b5, the importance sampling estimator\n\ne{ \u02c6\u00b5sir} = e{h(xi)} = e{e{h(xi)|y1, . . . ,ym}} = e0(m\ni=1 w\u2217(yi) 3 = e{ \u02c6\u00b5is}.\ni=1 h(yi)w\u2217(yi)\n(m\n\ntherefore the sir estimator has the same bias as \u02c6\u00b5is. however, the variance of\n\n\u02c6\u00b5sir is\n\nvar{ \u02c6\u00b5sir} = e:var{ \u02c6\u00b5sir|y1, . . . ,ym}; + var:e{ \u02c6\u00b5sir|y1, . . . ,ym};\n= e:var{ \u02c6\u00b5sir|y1, . . . ,ym}; + var0(m\ni=1 w\u2217(yi) 3\ni=1 h(yi)w\u2217(yi)\n(m\n\u2265 var{ \u02c6\u00b5is}.\n\nthus the sir estimator provides convenience at the expense of precision.\n\n(6.46)\n\n "}, {"Page_number": 195, "text": "184\n\nchapter 6 simulation and monte carlo integration\n\na\n\nb\n\nfigure 6.10 network connecting a and b described in example 6.9.\n\nan attractive feature of any importance sampling method is the possibility of\nreusing the simulations. the same sampled points and weights can be used to compute\na variety of monte carlo integral estimates of different quantities. the weights can be\nchanged to reflect an alternative importance sampling envelope, to assess or improve\nperformance of the estimator itself. the weights can also be changed to reflect an\nalternative target distribution, thereby estimating the expectation of h(x) with respect\nto a different density.\nfor example, in a bayesian analysis, one can efficiently update estimates based\non a revised posterior distribution in order to carry out bayesian sensitivity analysis or\nsequentiallytoupdatepreviousresultsviabayes\u2019theoreminlightofnewinformation.\nsuch updates can be carried out by multiplying each existing weight w(xi) by an\nadjustment factor. for example, if f is a posterior distribution for x using prior p1,\nthen weights equal to w(xi)p2(xi)/p1(xi) for i = 1, . . . , n can be used with the\nexisting sample to provide inference from the posterior distribution using prior p2.\nexample 6.9 (network failure probability) many systems can be represented\nby connected graphs like figure 6.10. these graphs are composed of nodes (circles)\nand edges (line segments). a signal sent from a to b must follow a path along any\navailable edges. imperfect network reliability means that the signal may fail to be\ntransmitted correctly between any pair of connected nodes\u2014in other words, some\nedges may be broken. in order for the signal to successfully reach b, a connected path\nfrom a to b must exist. for example, figure 6.11 shows a degraded network where\nonly a few routes remain from a to b. if the lowest horizontal edge in this figure were\nbroken, the network would fail.\nnetwork graphs can be used to model many systems. naturally, such a network\ncan model transmission of diverse types of signals such as analog voice transmission,\nelectromagnetic digital signals, and optical transmission of digital data. the model\nmay also be more conceptual, with each edge representing different machines or\npeople whose participation may be needed to achieve some outcome. usually, an\nimportant quantity of interest is the probability of network failure given specific\nprobabilities for the failure of each edge.\n\n "}, {"Page_number": 196, "text": "6.4 variance reduction techniques\n\n185\n\na\n\nb\n\nfigure 6.11 network connecting a and b described in example 6.9, with some edges\nbroken.\n\nconsider the simplest case, where each edge is assumed to fail independently\nwith the same probability, p. in many applications p is quite small. bit error rates for\nmany types of signal transmission can range from 10\u221210 to 10\u22123 or even lower [608].\nlet x denote a network, summarizing random outcomes for each edge: intact\nor failed. the network considered in our example has 20 potential edges, so x =\n(x1, . . . , x20). let b(x) denote the number of broken edges in x. the network in\nfigure6.10has b(x) = 0;thenetworkinfigure6.11has b(x) = 10.let h(x)indicate\nnetwork failure, so h(x) = 1 if a is not connected to b, and h(x) = 0 if a and b are\nconnected. the probability of network failure, then, is \u00b5 = e{h(x)}. computing \u00b5\nfor a network of any realistic size can be a very difficult combinatorial problem.\nthe naive monte carlo estimate of \u00b5 is obtained by drawing x1, . . . ,xn inde-\npendently and uniformly at random from the set of all possible network configurations\nwhose edges fail independently with probability p. the estimator is computed as\n\n\u02c6\u00b5mc =\n\n1\nn\n\nn!i=1\n\nh(xi).\n\n(6.47)\n\nnotice that this estimator has variance \u00b5(1 \u2212 \u00b5)/n. for n = 100,000 and p = 0.05,\nsimulation yields \u02c6\u00b5mc = 2.00 \u00d7 10\u22125 with a monte carlo standard error of about\n1.41 \u00d7 10\u22125.\nthe problem with \u02c6\u00b5mc is that h(x) is very rarely 1 unless p is unrealistically\nlarge. thus, a huge number of networks may need to be simulated in order to estimate\n\u00b5 with sufficient precision. instead, we can use importance sampling to focus on\nsimulation of x for which h(x) = 1, compensating for this bias through the assign-\nment of importance weights. the calculations that follow adopt this strategy, using\nthe nonstandardized importance weights as in (6.38).\nsuppose we simulate x\u22171, . . . ,x\u2217n by generating network configurations formed\nby breaking edges in figure 6.10, assuming independent edge failure with probability\np\u2217 > p. the importance weight for x\u2217i can be written as\n\nw\u2217(x\u2217i ) =c 1 \u2212 p\n\n1 \u2212 p\u2217d20+ p(1 \u2212 p\u2217)\n\np\u2217(1 \u2212 p)-b(x\u2217i )\n\n,\n\n(6.48)\n\n "}, {"Page_number": 197, "text": "chapter 6 simulation and monte carlo integration\n\n186\nand the importance sampling estimator of \u00b5 is\n\n\u02c6\u00b5\u2217is =\n\n1\nn\n\nn!i=1\n\nh(x\u2217i )w\u2217(x\u2217i ).\n\n(6.49)\n\nlet c denote the set of all possible network configurations, and let f denote the\n\nsubset of configurations for which a and b are not connected. then\n\n(6.50)\n(6.51)\n\n(6.52)\n\n(6.53)\n\n(6.54)\n\n(6.55)\n\n1\nn\n1\n\nvar{ \u02c6\u00b5\u2217is} =\n=\n\n=\n\nvar{h(x\u2217i )w\u2217(x\u2217i )}\nn)e1[h(x\u2217i )w\u2217(x\u2217i )]22 \u2212$e:h(x\u2217i )w\u2217(x\u2217i );%2*\nni!x\u2208f)w\u2217(x)pb(x)(1 \u2212 p)20\u2212b(x)* \u2212 \u00b52j .\n\n1\n\nnow, for a network derived from figure 6.10, failure only occurs when b(x) \u2265 4.\ntherefore,\n\n1 \u2212 p\u2217d20+ p(1 \u2212 p\u2217)\np\u2217(1 \u2212 p)-4\n\n.\n\nwhen p\u2217 = 0.25 and p = 0.05, we find w\u2217(x) \u2264 0.07. in this case,\n\u2212 \u00b52>\npb(x)(1 \u2212 p)20\u2212b(x)\n\u2212 \u00b52>\nh(x)pb(x)(1 \u2212 p)20\u2212b(x)\n\nvar{ \u02c6\u00b5\u2217is} \u2264\n\n1\n\nw\u2217(x) \u2264c 1 \u2212 p\nn=0.07!x\u2208f\nn=0.07!x\u2208c\n0.07\u00b5 \u2212 \u00b52\n\n=\n\n1\n\n.\n\nn\n\n=\n\n(6.56)\nthus var{ \u02c6\u00b5\u2217is} is substantially smaller than var{ \u02c6\u00b5mc}. under the approximation\nthat c\u00b5 \u2212 \u00b52 \u2248 c\u00b5 for small \u00b5 and relatively larger c, we see that var{ \u02c6\u00b5mc}/\nvar{ \u02c6\u00b5\u2217is} \u2248 14.\nwith the naive simulation strategy using p = 0.05, only 2 of 100,000 simulated\nnetworks failed. however, using the importance sampling strategy with p\u2217 = 0.2\nyielded 491 failing networks, producing an estimate of \u02c6\u00b5\u2217is = 1.02 \u00d7 10\u22125 with a\nmonte carlo standard error of 1.57 \u00d7 10\u22126.\nrelatedmontecarlovariancereductiontechniquesfornetworkreliabilityprob-\nlems are discussed in [432].\n!\n\n6.4.2 antithetic sampling\na second approach to variance reduction for monte carlo integration relies on finding\ntwo identically distributed unbiased estimators, say \u02c6\u00b51 and \u02c6\u00b52, that are negatively\n\n "}, {"Page_number": 198, "text": "6.4 variance reduction techniques\n\n187\ncorrelated. averaging these estimators will be superior to using either estimator alone\nwith double the sample size, since the estimator\n\u02c6\u00b51 + \u02c6\u00b52\n\n(6.57)\n\n\u02c6\u00b5as =\n\n2\n\nhas variance equal to\n1\n\nvar{ \u02c6\u00b5as} =\n\n4gvar{ \u02c6\u00b51} + var{ \u02c6\u00b52}h +\n\n1\n2cov{ \u02c6\u00b51, \u02c6\u00b52} =\n\n(1 + \u03c1)\u03c32\n\n2n\n\n,\n\n(6.58)\n\nwhere \u03c1 is the correlation between the two estimators and \u03c32/n is the variance of\neither estimator using a sample of size n. such pairs of estimators can be generated\nusing the antithetic sampling approach [304, 555].\ngiven an initial estimator, \u02c6\u00b51, the question is how to construct a second, identi-\ncallydistributedestimator \u02c6\u00b52 thatisnegativelycorrelatedwith \u02c6\u00b51.inmanysituations,\nthere is a convenient way to create such estimators while reusing one simulation sam-\nple of size n rather than generating a second sample from scratch. to describe the\nstrategy, we must first introduce some notation. let x denote a set of i.i.d. random\nvariables,{x1, . . . ,xn}. suppose \u02c6\u00b51(x) =(n\ni=1 h1(xi)/n, where h1 is a real-valued\nfunction of m arguments, so h1(xi) = h1(xi1, . . . , xim). assume e{h1(xi)} = \u00b5.\nlet \u02c6\u00b52(x) =(n\ni=1 h2(xi)/n be a second estimator, with the analogous assumptions\nabout h2.\nwe will now prove that if h1 and h2 are both increasing in each argument (or\nboth decreasing), then cov{h1(xi), h2(xi)} is positive. from this result, we will be\nable to determine requirements for h1 and h2 that ensure that cor{ \u02c6\u00b51, \u02c6\u00b52} is negative.\nthe proof proceeds via induction. suppose the above hypotheses hold and\nm = 1. then\n\n[h1(x) \u2212 h1(y)][h2(x) \u2212 h2(y)] \u2265 0\n\n(6.59)\nfor any random variables x and y. hence, the expectation of the left-hand side of\n(6.59) is also nonnegative. therefore, when x and y are independent and identically\ndistributed, this nonnegative expectation implies\n\ncov{h1(xi), h2(xi)} \u2265 0.\n\n(6.60)\nnow, suppose that the desired result holds when xi is a random vector of length\nm \u2212 1, and consider the case when xi = (xi1, . . . , xim). then, by hypothesis, the\nrandom variable\n(6.61)\n\ncov{ h1(xi), h2(xi)| xim} \u2265 0.\n\ntaking the expectation of this inequality gives\n\n0 \u2264 e:e{ h1(xi)h2(xi)| xim}; \u2212 e:e{ h1(xi)| xim} e{ h2(xi)| xim};\n\u2264 e{h1(xi)h2(xi)} \u2212 e:e{ h1(xi)| xim};e:e{ h2(xi)| xim};\n= cov{h1(xi), h2(xi)} ,\n\n(6.62)\n\n "}, {"Page_number": 199, "text": "chapter 6 simulation and monte carlo integration\n\nthe fact that each e: hj(xi),, xim; for j = 1,2 is a function of the single random\n\n188\nwhere the substitution of terms in the product on the right side of (6.62) follows from\nargument xim, for which the result (6.60) applies.\nthus, we have proven by induction that h1(xi) and h2(xi) will be positively\ncorrelated in these circumstances; it follows that \u02c6\u00b51 and \u02c6\u00b52 will also be positively\ncorrelated. we leave it to the reader to show the following key implication: if h1 and\nh2 are functions of m random variables u1, . . . , um, and if each function is mono-\ntone in each argument, then cov{h1(u1, . . . , um), h2(1 \u2212 u1, . . . ,1 \u2212 um)} \u2264 0.\nthis result follows simply from our previous proof after redefining h1 and h2 to create\ntwo functions increasing in their arguments that satisfy the previous hypotheses. see\nproblem 6.5.\nnow the antithetic sampling strategy becomes apparent. the monte carlo in-\ntegral estimate \u02c6\u00b51(x) can be written as\n\n1\nn\n\nn!i=1\n\nh1)f\u22121\n\nm (uim)* ,\n\n\u02c6\u00b51(x) =\n\n1 (ui1), . . . , f\u22121\n\n(6.63)\nwhere fj is the cumulative distribution function of each xij (j = 1, . . . , m) and the\nuij are independent unif(0,1) random variables. since fj is a cumulative distribu-\ntion function, its inverse is nondecreasing. therefore, h1(f\u22121\nm (uim))\nis monotone in each uij\nfor j = 1, . . . , m whenever h1\nis monotone in\nits arguments. moreover, if uij \u223c unif(0,1), then 1 \u2212 uij \u223c unif(0,1). hence,\nh2(1 \u2212 ui1, . . . ,1 \u2212 uim) = h1(f\u22121\nm (1 \u2212 uim)) is monotone in\neach argument and has the same distribution as h1(f\u22121\nm (uim)).\ntherefore,\n\n1 (1 \u2212 ui1), . . . , f\u22121\n\n1 (ui1), . . . , f\u22121\n\n\u02c6\u00b52(x) =\n\n1\nn\n\nn!i=1\n\nh1)f\u22121\n\n1 (1 \u2212 ui1), . . . , f\u22121\n\nis a second estimator of \u00b5 having the same distribution as \u02c6\u00b51(x). our analysis above\nallows us to conclude that\n\ncov{ \u02c6\u00b51(x), \u02c6\u00b52(x)} \u2264 0.\n\n(6.65)\n\u02c6\u00b5as = ( \u02c6\u00b51 + \u02c6\u00b52)/2 will have smaller variance than \u02c6\u00b51\ntherefore, the estimator\nwould have with a sample of size 2n. equation (6.58) quantifies the amount of im-\nprovement. we accomplish this improvement while generating only a single set of n\nrandom numbers, with the other n derived from the antithetic principle.\nexample 6.10 (normal expectation)\nsuppose x has a standard normal distri-\nbution and we wish to estimate \u00b5 = e{h(x)} where h(x) = x/(2x \u2212 1). a standard\nmonte carlo estimator can be computed as the sample mean of n = 100,000 val-\nues of h(xi) where x1, . . . , xn \u223c i.i.d. n(0,1). an antithetic estimator can be con-\nstructed using the first n = 50,000 draws. the antithetic variate for xi is simply\n[h(xi) + h(\u2212xi)]5100,000. in\n\u2212xi, so the antithetic estimator is \u02c6\u00b5as =(50,000\nthe simulation,#cor{h(xi), h(\u2212xi)} = \u22120.95, so the antithetic approach is profitable.\nthe standard approach yielded \u02c6\u00b5mc = 1.4993 with a monte carlo standard error of\n\ni=1\n\n1 (ui1), . . . , f\u22121\nm (1 \u2212 uim)*\n\n(6.64)\n\n "}, {"Page_number": 200, "text": "6.4 variance reduction techniques\n\n189\n0.0016, whereas the antithetic approach gave \u02c6\u00b5as = 1.4992 with a standard error\nof 0.0003 [estimated via (6.58) using the sample variance and correlation]. further\nsimulation confirms a more than fourfold reduction in standard error for the antithetic\napproach.\n!\nexample6.11 (networkfailureprobability,continued) recallingexample6.9,\nletthe ithsimulatednetwork,xi,bedeterminedbystandarduniformrandomvariables\nui1, . . . , uim, where m = 20. the jth edge in the ith simulated network is broken if\nuij < p. now h(xi) = h(ui1, . . . , uim) equals 1 if a and b are not connected, and\n0 if they are connected. note that h is nondecreasing in each uij; therefore the anti-\nthetic approach will be profitable. since xi is obtained by breaking the jth edge when\nuij < pfor j = 1, . . . , m,theantitheticnetworkdraw,sayx\u2217i ,isobtainedbybreaking\nthe jth edge when uij > 1 \u2212 p, for the same set of uij used to generate xi. the neg-\ni=1gh(xi) + h(x\u2217i )h\native correlation induced by this strategy will ensure that 1\nis a superior estimator to 1\n!\n\n2n(n\n\ni=1 h(xi).\n\n2n(2n\n\n6.4.3 control variates\nthe control variate strategy improves estimation of an unknown integral by relat-\ning the estimate to some correlated estimator of an integral whose value is known.\nsuppose we wish to estimate the unknown quantity \u00b5 = e{h(x)} and we know\nof a related quantity, \u03b8 = e{c(y)}, whose value can be determined analytically.\nlet (x1,y1), . . . ,(xn,yn) denote pairs of random variables observed indepen-\ndently as simulation outcomes, so cov{xi,xj} = cov{yi,yj} = cov{xi,yj} = 0\nwhen i /= j. the simple monte carlo estimators are \u02c6\u00b5mc = (1/n)(n\ni=1 h(xi) and\n\u02c6\u03b8mc = (1/n)(n\ni=1 c(yi).ofcourse, \u02c6\u03b8mc isunnecessary,since \u03b8 canbefoundanalyti-\ncally.however,notethat \u02c6\u00b5mc willbecorrelatedwith \u02c6\u03b8mc whencor{h(xi), c(yi)} /= 0.\nfor example, if the correlation is positive, an unusually high outcome for \u02c6\u03b8mc should\ntend to be associated with an unusually high outcome for \u02c6\u00b5mc. if comparison of \u02c6\u03b8mc\nwith \u03b8 suggests such an outcome, then we should adjust \u02c6\u00b5mc downward accordingly.\nthe opposite adjustment should be made when the correlation is negative.\n\nthis reasoning suggests the control variate estimator\n\u02c6\u00b5cv = \u02c6\u00b5mc + \u03bb(\u02c6\u03b8mc \u2212 \u03b8),\n\nwhere \u03bb is a parameter to be chosen by the user. it is straightforward to show that\n\nminimizing this quantity with respect to \u03bb shows that the minimal variance,\n\nvar{ \u02c6\u00b5cv} = var{ \u02c6\u00b5mc} + \u03bb2 var{\u02c6\u03b8mc} + 2\u03bbcov{ \u02c6\u00b5mc, \u02c6\u03b8mc}.\n\u03bb gvar{ \u02c6\u00b5cv}h = var{ \u02c6\u00b5mc} \u2212gcov{ \u02c6\u00b5mc, \u02c6\u03b8mc}h2\nvar{\u02c6\u03b8mc}\n\nmin\n\n,\n\nis obtained when\n\n(6.66)\n\n(6.67)\n\n(6.68)\n\n(6.69)\n\n\u03bb = \u2212cov{ \u02c6\u00b5mc, \u02c6\u03b8mc}\nvar{\u02c6\u03b8mc}\n\n.\n\n "}, {"Page_number": 201, "text": "chapter 6 simulation and monte carlo integration\n\n190\nthis optimal \u03bb depends on unknown moments of h(xi) and c(yi), but these can be\nestimated using the sample (x1,y1), . . . ,(xn,yn). specifically, using\n\nand\n\n(6.70)\n\n#var{\u02c6\u03b8mc} =\n#cov{ \u02c6\u00b5mc, \u02c6\u03b8mc} =\n\n[c(yi) \u2212 \u00afc]2\nn(n \u2212 1)\n\nn!i=1\nn!i=1 eh(xi) \u2212 \u00afhfec(yi) \u2212 \u00afcf\n\u02c6\u03bb, where \u00afc = (1/n)(n\n\nn(n \u2212 1)\n\n(6.71)\ni=1 c(yi) and \u00afh =\nin (6.69) provides an estimator\n(1/n)(n\ni=1 h(yi). further, plugging such sample variance and covariance estimates\ninto the right-hand side of (6.68) provides a variance estimate for \u02c6\u00b5cv.\nin practice, \u02c6\u00b5mc and \u02c6\u03b8mc often depend on the same random variables, so xi =\nyi. also, it is possible to use more than one control variate. in this case, we may write\nj=1 \u03bbj(\u02c6\u03b8mc,j \u2212 \u03b8j) when using m control variates.\nthe estimator as \u02c6\u00b5cv = \u02c6\u00b5mc +(m\nequation (6.68) shows that the proportional reduction in variance obtained by\nusing \u02c6\u00b5cv instead of \u02c6\u00b5mc is equal to the square of the correlation between \u02c6\u00b5mc and\n\u02c6\u03b8mc. if this result sounds familiar, you have astutely noted a parallel with simple\nlinear regression. consider the regression model e{h(xi)|yi = yi} = \u03b20 + \u03b21c(yi)\nwith the usual regression assumptions and estimators. then \u02c6\u03bb = \u2212 \u02c6\u03b21 and \u02c6\u00b5mc +\n\u02c6\u03bb(\u02c6\u03b8mc \u2212 \u03b8) = \u02c6\u03b20 + \u02c6\u03b21\u03b8. in other words, the control variate estimator is the fitted\nvalue on the regression line at the mean value of the predictor (i.e., at \u03b8), and the\nstandard error of this control variate estimator is the standard error for the fitted\nvalue from the regression. thus, linear regression software may be used to obtain\nthe control variate estimator and a corresponding confidence interval. when more\nthan one control variate is used, multiple linear regression can be used to obtain \u02c6\u03bbi\n(i = 1, . . . , m) and \u02c6\u00b5cv [555].\nproblem 6.5 asks you to show that the antithetic approach to variance reduction\ncan be viewed as a special case of the control variate method.\nexample 6.12 (control variate for importance sampling) hesterberg suggests\nusing a control variate estimator to improve importance sampling [326]. recall that\nimportance sampling is built upon the idea of sampling from an envelope that induces\na correlation between h(x)w\u2217(x) and w\u2217(x). further, we know e{w\u2217(x)} = 1. thus,\nthe situation is well suited for using the control variate \u00afw\u2217 =(n\ni=1 w\u2217(xi)/n. if the\naverage weight exceeds 1, then the average value of h(x)w\u2217(x) is also probably\nunusually high, in which case \u02c6\u00b5is probably differs from its expectation, \u00b5. thus, the\nimportance sampling control variate estimator is\n\n\u02c6\u00b5iscv = \u02c6\u00b5\u2217is + \u03bb( \u00afw\u2217 \u2212 1).\n\n(6.72)\nthe value for \u03bb and the standard error of \u02c6\u00b5iscv can be estimated from a regression of\nh(x)w\u2217(x) on w\u2217(x) as previously described. like \u02c6\u00b5is, which uses the standardized\nweights, the estimator \u02c6\u00b5iscv has bias of order o(1/n), but will often have lower mean\n\n "}, {"Page_number": 202, "text": "6.4 variance reduction techniques\n\n191\nsquared error than the importance sampling estimator with unstandardized weights\n\u02c6\u00b5\u2217is given in (6.38).\n!\nexample 6.13 (option pricing) a call option is a financial instrument that gives\nits holder the right\u2014but not the obligation\u2014to buy a specified amount of a financial\nasset, on or by a specified maturity date, for a specified price. for a european call\noption, the option can be exercised only at the maturity date. the strike price is the\nprice at which the transaction is completed if the option is exercised. let s(t) denote\nthe price of the underlying financial asset (say, a stock) at time t. denote the strike\nprice by k, and let t denote the maturity date. when time t arrives, the holder of\nthe call option will not wish to exercise his option if k > s(t) because he can obtain\nthe stock more cheaply on the open market. however, the option will be valuable if\nk < s(t) because he can buy the stock at the low price k and immediately sell it at\nthe higher market price s(t). it is of interest to determine how much the buyer of this\ncall option should pay at time t = 0 for this option with strike price k at maturity\ndate t.\nthe nobel prize\u2013winning model introduced by black, scholes, and merton in\n1973 provides a popular approach to determining the fair price of an option using a\nstochastic differential equation [52, 459]. further background on option pricing and\nthe stochastic calculus of finance includes [184, 406, 586, 665].\nthe fair price of an option is the amount to be paid at time t = 0 that would\nexactly counterbalance the expected payoff at maturity. we\u2019ll consider the simplest\ncase: a european call option on a stock that pays no dividends. the fair price of\nsuch an option can be determined analytically under the black\u2013scholes model, but\nestimation of the fair price via monte carlo is an instructive starting point. according\nto the black\u2013scholes model, the value of the stock at day t can be simulated as\n\ns(t)\n\n= s(0) expkcr \u2212\n\n\u03c32\n\n2 d t\n\n365 + \u03c3zl t\n\n365m ,\n\n(6.73)\n\nwhere r is the risk-free rate of return (typically the return rate of the u.s. treasury\nbill that matures on day (t \u2212 1), \u03c3 is the stock\u2019s volatility [an annualized estimate\nof the standard deviation of log:s(t+1)/s(t); under a lognormal price model], and z\nis a standard normal deviate. if we knew that the price of the stock at day t would\nequal s(t), then the fair price of the call option would be\n\nc = exp0\u2212\n\nrt\n\n3653max:0, s(t)\n\n\u2212 k;,\n\n(6.74)\n\ndiscounting the payoff to present value. since s(t) is unknown to the buyer of the call,\nthe fair price to pay at t = 0 is the expected value of the discounted payoff, namely\ne{c}. thus, a monte carlo estimate of the fair price to pay at t = 0 is\n\n\u00afc =\n\n1\nn\n\nci,\n\nn!i=1\n\n(6.75)\n\n "}, {"Page_number": 203, "text": "chapter 6 simulation and monte carlo integration\n\n192\nwhere the ci are simulated from (6.73) and (6.74) for i = 1, . . . , n using an i.i.d.\nsample of standard normal deviates, z1, . . . , zn.\nsince the true fair price, e{c}, can be computed analytically in this instance,\nthere is no need to apply the monte carlo approach. however, a special type of\neuropean call option, called an asian, path-dependent, or average-price option, has\na payoff based on the average price of the underlying stock throughout the holding\nperiod.suchoptionsareattractiveforconsumersofenergiesandcommoditiesbecause\nthey tend to be exposed to average prices over time. since the averaging process\nreducesvolatility,asianoptionsalsotendtobecheaperthanstandardoptions.control\nvariate and many other variance reduction approaches for the monte carlo pricing of\noptions like these are examined in [59].\nto simulate the fair price of an asian call option, simulation of stock value at\nmaturity is carried out by applying (6.73) sequentially t times, each time advancing\nthe stock price one day and recording the simulated closing price for that day, so\n\ns(t+1)\n\n= s(t) exp0 r \u2212 \u03c32/2\n\n365 +\n\n\u03c3z(t)\n\n\u221a3653\n\n(6.76)\n\nfor a sequence of standard normal deviates, {z(t)}, where t = 0, . . . , t \u2212 1. the dis-\ncounted payoff at day t of the asian call option on a stock with current price s(0) is\ndefined as\n\na = exp0\u2212\n\nrt\n\n3653max{0, \u00afs \u2212 k},\n\n(6.77)\n\nwhere \u00afs =(t\nt=1 s(t)/t and the s(t) for t = 1, . . . , t are the random variables rep-\nresenting future stock prices at the averaging times. the fair price to pay at t = 0 is\ne{a}, but in this case there is no known analytic solution for it. denote the standard\nmonte carlo estimator for the fair price of an asian call option as\n\n\u02c6\u00b5mc = \u00afa =\n\n1\nn\n\nn!i=1\n\nai,\n\n(6.78)\n\nwhere the ai are simulated independently as described above.\nif \u00afs is replaced in (6.77) by the geometric average of the price of the underlying\nstock throughout the holding period, an analytic solution for e{a} can be found [370].\nthe fair price is then\n\nc3\u03c32\n\n\u03b8 = s(0)\u0001(c1)exp0\u2212tcr +\n\u2212k\u0001(c1 \u2212 c2)exp0\u2212\n\n6 d 1 \u2212 1/n\n730 3\n3653 ,\n\nrt\n\n(6.79)\n\n "}, {"Page_number": 204, "text": "where\n\n6.4 variance reduction techniques\n\n193\n\n730dcr \u2212\n\n\u03c32\n\n2 d +\n\nc3\u03c32t\n\n1095 c1 +\n\n1\n\n2nd- ,\n\n1\n\nc2+log0 s(0)\n1095c1 +\n\nc1 =\nc2 = \u03c3+ c3t\nc3 = 1 + 1/n,\n\nk 3 +c c3t\n2nd-1/2\n\n1\n\n,\n\nwhere \u0001 is the standard normal cumulative distribution function, and n is the number\nof prices in the average. alternatively, one could estimate the fair price of an asian\ncall option with geometric averaging using the same sort of monte carlo strategy\ndescribed above. denote this monte carlo estimator as \u02c6\u03b8mc.\nthe estimator \u02c6\u03b8mc makes an excellent control variate for estimation of \u00b5. let\n\u02c6\u00b5cv = \u02c6\u00b5mc + \u03bb(\u02c6\u03b8mc \u2212 \u03b8). since we expect the fair price of the two asian options\n(arithmetic and geometric mean pricing) to be very highly correlated, a reasonable\ninitial guess is to use \u03bb = \u22121.\nconsider a european call option with payoff based on the arithmetic mean\nprice during the holding period. suppose that the underlying stock has current price\ns(0) = 100, strike price k = 102, and volatility \u03c3 = 0.3. suppose there are n = 50\ndays to maturity, so simulation of the maturity price requires 50 iterations of (6.76).\nassume the risk-free rate of return is r = 0.05. then the analogous geometric mean\nprice option has a fair price of 1.83. simulations show that the true fair value of the\narithmetic mean price option is roughly \u00b5 = 1.876. using n = 100,000 simulations,\nwe can estimate \u00b5 using either \u02c6\u00b5mc or \u02c6\u00b5cv, and both estimators tend to give answers\nin the vicinity of \u00b5. but what is of interest is the standard error of estimates of \u00b5. we\nreplicated the entire monte carlo estimation process 100 times, obtaining 100 values\nfor \u02c6\u00b5mc and for \u02c6\u00b5cv. the sample standard deviation of the values obtained for \u02c6\u00b5mc\nwas 0.0107, whereas that of the \u02c6\u00b5cv values was 0.000295. thus, the control variate\napproach provided an estimator with 36 times smaller standard error.\nfinally, consider estimating \u03bb from the simulations using (6.69). repeating\nthe same experiment as above, the typical correlation between \u02c6\u00b5mc and \u02c6\u03b8mc was\n0.9999. the mean of \u02c6\u03bb was \u22121.0217 with sample standard deviation 0.0001. using\nthe \u02c6\u03bb found in each simulation to produce each \u02c6\u00b5cv yielded a set of 100 \u02c6\u00b5cv values\nwhose standard deviation was 0.000168. this represents a 63-fold improvement in\nthe standard error over \u02c6\u00b5mc.\n!\n\n6.4.4 rao\u2013blackwellization\nwe have been considering the estimation of \u00b5 = e{h(x)} using a random sample\nx1, . . . ,xn drawn from f. suppose that each xi = (xi1,xi2) and that the conditional\nexpectation e{h(xi)|xi2} can be solved for analytically. to motivate an alternate esti-\nmatorto \u02c6\u00b5mc,wemayusethefactthat e{h(xi)} = e{e{h(xi)|xi2}},wheretheouter\n\n "}, {"Page_number": 205, "text": "chapter 6 simulation and monte carlo integration\n\n194\nexpectation is taken with respect to the distribution of xi2. the rao\u2013blackwellized\nestimator can be defined as\n\n\u02c6\u00b5rb =\n\n1\nn\n\nn!i=1\n\ne{h(xi)|xi2}\n\n(6.80)\n\nand has the same mean as the ordinary monte carlo estimator \u02c6\u00b5mc. notice that\n\nvar{ \u02c6\u00b5mc} =\n\n1\nn\n\nvar{e{h(xi)|xi2}} +\n\n1\nn\n\ne{var{h(xi)|xi2}} \u2265 var{ \u02c6\u00b5rb}\n\n(6.81)\n\nfollows from the conditional variance formula. thus, \u02c6\u00b5rb is superior to \u02c6\u00b5mc in terms\nofmeansquarederror.thisconditioningprocessisoftencalledrao\u2013blackwellization\ndue to its use of the rao\u2013blackwell theorem, which states that one can reduce the\nvariance of an unbiased estimator by conditioning it on the sufficient statistics [96].\nfurther study of rao\u2013blackwellization for monte carlo methods is given in [99, 216,\n507, 542, 543].\nexample 6.14 (rao\u2013blackwellization of rejection sampling) a generic ap-\nproach that rao\u2013blackwellizes rejection sampling is described by casella and robert\n[99]. in ordinary rejection sampling, candidates y1, . . . , ym are generated sequen-\ntially, and some are rejected. the uniform random variables u1, . . . , um provide\nthe rejection decisions, with yi being rejected if ui > w\u2217(yi), where w\u2217(yi) =\nf(yi)/e(yi). rejection sampling stops at a random time m with the acceptance of the\nnth draw, yielding x1, . . . , xn. the ordinary monte carlo estimator of \u00b5 = e{h(x)}\ncan then be reexpressed as\n\n\u02c6\u00b5mc =\n\n1\nn\n\nm!i=1\n\nh(yi)1{ui\u2264w\u2217(yi)},\n\n(6.82)\n\nwhich presents the intriguing possibility that \u02c6\u00b5mc somehow can be improved by using\nall the candidate yi draws (suitably weighted), rather than merely the accepted draws.\n\nrao\u2013blackwellization of (6.82) yields the estimator\n\n\u02c6\u00b5rb =\n\n1\nn\n\nm!i=1\n\nh(yi)ti(y),\n\n(6.83)\n\nwhere the ti(y) are random quantities that depend on y = (y1, . . . , ym) and m\naccording to\n\nti(y) = e:1{ui\u2264w\u2217(yi)}|m, y1, . . . , ym;\n= p[ui < w\u2217(yi)|m, y1, . . . , ym].\n\n(6.84)\n\n "}, {"Page_number": 206, "text": "195\nnow tm(y) = 1 since the final candidate was accepted. for previous candidates, the\nprobability in (6.84) can be found by averaging over permutations of subsets of the\nrealized sample [99]. we obtain\n\n6.4 variance reduction techniques\n\n,\n\nti(y) =\n\nw\u2217(yi)(a\u2208ai\u2019j\u2208a w\u2217(yj)\u2019j /\u2208 a[1 \u2212 w\u2217(yj)]\n(b\u2208b\u2019j\u2208b w\u2217(yj)\u2019j /\u2208 b[1 \u2212 w\u2217(yj)]\n\n(6.85)\nwhereai is the set of all subsets of{1, . . . , i \u2212 1, i + 1, . . . , m \u2212 1} containing n \u2212 2\nelements, and b is the set of all subsets of {1, . . . , m \u2212 1} containing n \u2212 1 elements.\ncasella and robert [99] offer a recursion formula for computing the ti(y), but it is\ndifficult to implement unless n is fairly small.\nnotice that the conditioning variables used here are statistically sufficient since\nthe conditional distribution of u1, . . . , um does not depend on f. both \u02c6\u00b5rb and \u02c6\u00b5mc\nare unbiased; thus, the rao\u2013blackwell theorem implies that \u02c6\u00b5rb will have smaller\nvariance than \u02c6\u00b5mc.\n!\n\nproblems\n6.1. consider the integral sought in example 5.1, equation (5.7), for the parameter values\ngiven there. find a simple rejection sampling envelope that will produce extremely few\nrejections when used to generate draws from the density proportional to that integrand.\n6.2. consider the piecewise exponential envelopes for adaptive rejection sampling of the\nstandardnormaldensity,whichislog-concave.forthetangent-basedenvelope,suppose\nyou are limited to an even number of nodes at \u00b1c1, . . . ,\u00b1cn. for the envelope that does\nnot require tangent information, suppose you are limited to an odd number of nodes at\n0,\u00b1d1, . . . ,\u00b1dn. the problems below will require optimization using strategies like\nthose in chapter 2.\na. for n = 1,2,3,4,5, find the optimal placement of nodes for the tangent-based\nb. for n = 1,2,3,4,5, find the optimal placement of nodes for the tangent-free\nc. plot these collections of envelopes; also plot rejection sampling waste against num-\n\nenvelope.\n\nenvelope.\n\nber of nodes for both envelopes. comment on your results.\n\n6.3. consider finding \u03c32 = e{x2} when x has the density that is proportional to q(x) =\n\nexp{\u2212|x|3/3}.\na. estimate \u03c32 using importance sampling with standardized weights.\nb. repeat the estimation using rejection sampling.\nc. philippe and robert describe an alternative to importance-weighted averaging that\nemploys a riemann sum strategy with random nodes [506, 507]. when draws\nx1, . . . , xn originate from f, an estimator of e{h(x)} is\n(x[i+1] \u2212 x[i])h(x[i])f(x[i]),\n\n(6.86)\n\nn\u22121!i=1\n\n "}, {"Page_number": 207, "text": "196\n\nchapter 6 simulation and monte carlo integration\n\ns\nr\ne\nt\ns\na\ns\ni\nd\n\n \nf\no\n\n \nr\ne\nb\nm\nu\nn\n\n6\n\n4\n\n2\n\n0\n\n1850\n\nfigure 6.12 number of coal-mining disasters per year between 1851 and 1962.\n\n1900\nyear\n\n1950\n\nwhere x[1] \u2264 \u00b7\u00b7\u00b7 \u2264 x[n] is the ordered sample associated with x1, . . . , xn. this\nestimator has faster convergence than the simple monte carlo estimator. when\nf = cq and the normalization constant c is not known, then\n\n(n\u22121\ni=1 (x[i+1] \u2212 x[i])h(x[i])q(x[i])\n(n\u22121\ni=1 (x[i+1] \u2212 x[i])q(x[i])\n\n(6.87)\n\nestimates e{h(x)}, noting that the denominator estimates 1/c. use this strategy to\nestimate \u03c32, applying it post hoc to the output obtained in part (b).\nd. carry out a replicated simulation experiment to compare the performance of the\n\ntwo estimators in parts (b) and (c). discuss your results.\n\n6.4. figure 6.12 shows some data on the number of coal-mining disasters per year between\n1851 and 1962, available from the website for this book. these data originally appeared\nin [434] and were corrected in [349]. the form of the data we consider is given in [91].\nother analyses of these data include [445, 525].\nthe rate of accidents per year appears to decrease around 1900, so we consider\na change-point model for these data. let j = 1 in 1851, and index each year thereafter,\nso j = 112 in 1962. let xj be the number of accidents in year j, with x1, . . . , x\u03b8 \u223c\ni.i.d. poisson(\u03bb1) and x\u03b8+1, . . . , x112 \u223c i.i.d. poisson(\u03bb2). thus the change-point oc-\ncurs after the \u03b8th year in the series, where \u03b8 \u2208 {1, . . . ,111}. this model has parameters\n\u03b8, \u03bb1, and \u03bb2. below are three sets of priors for a bayesian analysis of this model. in\neach case, consider sampling from the priors as the first step of applying the sir algo-\nrithm for simulating from the posterior for the model parameters. of primary interest\nis inference about \u03b8.\n\n "}, {"Page_number": 208, "text": "6.4 variance reduction techniques\n\n197\na. assume a discrete uniform prior for \u03b8 on {1,2, . . . ,111}, and priors \u03bbi|ai \u223c\ngamma(3, ai) and ai \u223c gamma(10,10) independently for i = 1,2. using the sir\napproach, estimate the posterior mean for \u03b8, and provide a histogram and a credible\ninterval for \u03b8. provide similar information for estimating \u03bb1 and \u03bb2. make a scatter-\nplot of \u03bb1 against \u03bb2 for the initial sir sample, highlighting the points resampled at\nthe second stage of sir. also report your initial and resampling sample sizes, the\nnumber of unique points and highest observed frequency in your resample, and a\nmeasure of the effective sample size for importance sampling in this case. discuss\nyour results.\nb. assume that \u03bb2 = \u03b1\u03bb1. use the same discrete uniform prior for \u03b8 and \u03bb1|a \u223c\ngamma(3, a), a \u223c gamma(10,10), and log \u03b1 \u223c unif(log1/8,log2). provide the\nsame results listed in part (a), and discuss your results.\nc. markov chain monte carlo approaches (see chapter 7) are often applied in the\nanalysis of these data. a set of priors that resembles the improper diffuse priors used\ninsomesuchanalysesis: \u03b8 havingthediscreteuniformprior, \u03bbi|ai \u223c gamma(3, ai),\nand ai \u223c unif(0,100) independently for i = 1,2. provide the same result listed in\npart(a),anddiscussyourresults,includingreasonswhythisanalysisismoredifficult\nthan the previous two.\n6.5. prove the following results.\n\na. if h1 and h2 are functions of m random variables u1, . . . , um, and if each function\n\nis monotone in each argument, then\n\ncov{h1(u1, . . . , um), h2(1 \u2212 u1, . . . ,1 \u2212 um)} \u2264 0.\n\nb. let \u02c6\u00b51(x) estimate a quantity of interest, \u00b5, and let \u02c6\u00b52(y) be constructed from\nrealizations y1, . . . ,yn chosen to be antithetic to x1, . . . ,xn. assume that both\nestimators are unbiased for \u00b5 and are negatively correlated. find a control variate\nfor \u02c6\u00b51, say z, with mean zero, for which the control variate estimator \u02c6\u00b5cv =\n\u02c6\u00b51(x) + \u03bbz corresponds to the antithetic estimator based on \u02c6\u00b51 and \u02c6\u00b52 when the\noptimal \u03bb is used. include your derivation of the optimal \u03bb.\n\n6.6. consider testing the hypotheses h0 : \u03bb = 2 versus ha: \u03bb > 2 using 25 observations\nfrom a poisson(\u03bb) model. rote application of the central limit theorem would suggest\nrejecting h0 at \u03b1 = 0.05 when z \u2265 1.645, where z = ( \u00afx \u2212 2)/\u221a2/25.\na. estimate the size of this test (i.e., the type i error rate) using five monte carlo\napproaches: standard, antithetic, importance sampling with unstandardized and\nstandardized weights, and importance sampling with a control variate as in\nexample 6.12. provide a confidence interval for each estimate. discuss the\nrelative merits of each variance reduction technique, and compare the importance\nsampling strategies with each other. for the importance sampling approaches,\nuse a poisson envelope with mean equal to the h0 rejection threshold, namely\n\u03bb = 2.4653.\nb. draw the power curve for this test for \u03bb \u2208 [2.2,4], using the same five techniques.\nprovide pointwise confidence bands in each case. discuss the relative merits of each\ntechnique in this setting. compare the performances of the importance sampling\nstrategies with their performance in part (a).\n\n "}, {"Page_number": 209, "text": "198\n\nchapter 6 simulation and monte carlo integration\n\n6.7. consider pricing a european call option on an underlying stock with current price\ns(0) = 50, strike price k = 52, and volatility \u03c3 = 0.5. suppose that there are n = 30\ndays to maturity and that the risk-free rate of return is r = 0.05.\na. confirm that the fair price for this option is 2.10 when the payoff is based on s(30)\n\n[i.e., a standard option with payoff as in (6.74)].\n\nb. consider the analogous asian option (same s(0), k, \u03c3, n, and r) with payoff based\non the arithmetic mean stock price during the holding period, as in (6.77). using\nsimple monte carlo, estimate the fair price for this option.\n\nc. improve upon the estimate in (b) using the control variate strategy described in\n\nexample 6.13.\n\npart (b).\n\nd. try an antithetic approach to estimate the fair price for the option described in\n\ne. using simulation and/or analysis, compare the sampling distributions of the esti-\n\nmators in (b), (c), and (d).\n\n6.8. considerthemodelgivenby x \u223c lognormal(0,1)andlog y = 9 + 3log x + \u03f5,where\n\u03f5 \u223c n(0,1). we wish to estimate e{y/x}. compare the performance of the standard\nmonte carlo estimator and the rao\u2013blackwellized estimator.\n6.9. consider a bug starting at the origin of an infinite lattice l and walking one unit north,\nsouth, east, or west at each discrete time t. the bug cannot stay still at any step. let\nx1:t denote the sequence of coordinates (i.e., the path) of the bug up to time t, say\n{xi = (vi, wi) : i = 1, . . . , t} with x0 = (0,0). let the probability distribution for the\nbug\u2019s path through time t be denoted ft(x1:t) = f1(x1)f2(x2|x1), . . . , ft(xt|x1:t\u22121).\ndefine dt(x1:t) to be the manhattan distance of the bug from the origin at time\nt, namely dt(x1:t) = |vt| + |wt|. let rt(v, w) denote the number of times the bug has\nvisited the lattice point (v, w) up to and including time t. thus rt(xt) counts the number\nof visits to the current location.\nthe bug\u2019s path is random, but the probabilities associated with moving to the\nadjacentlocationsattime t arenotequal.thebugpreferstostayclosetohome(i.e.,near\nthe origin), but has an aversion to revisiting its previous locations. these preferences\nare expressed by the path distribution ft(x1:t) \u221d exp{\u2212(dt(xt) + rt(xt)/2)}.\na. suppose we are interested in the marginal distributions of dt(xt) and mt(x1:t) =\nmax(v,w)\u2208l{rt(v, w)} where the latter quantity is the greatest frequency with which\nany lattice point has been visited. use sequential importance sampling to simulate\nfrom the marginal distributions of dt and mt at time t = 30. let the proposal distri-\nbution or envelope gt(xt|x1:t\u22121) be uniform over the four lattice points surrounding\nxt\u22121. estimate the mean and standard deviation of d30(x30) and m30(x1:30).\nb. let gt(xt|x1:t\u22121)beproportionalto ft(x1:t)ifxt isadjacenttoxt\u22121 andzerootherwise.\nrepeat part (a) using this choice for gt and discuss any problems encountered. in\nparticular, consider the situation when the bug occupies an attractive location but\narrived there via an implausible path.\n\nc. a self-avoiding walk (saw) is similar to the bug\u2019s behavior above except that\nthe bug will never revisit a site it has previously occupied. simulation of saws\nhas been important, for example, in the study of long-chain polymers [303, 394,\n553]. let ft(x1:t) be the uniform distribution on all saws of length t. show that by\nusing gt(xt|x1:t\u22121) = ft(xt|x1:t\u22121), the sequential update specified by wt = wt\u22121ut is\n\n "}, {"Page_number": 210, "text": "6.4 variance reduction techniques\n\n199\ngiven by ut = ct\u22121 where ct\u22121 is the number of unvisited neighboring lattice points\nadjacent to xt\u22121 at time t \u2212 1. estimate the mean and standard deviation of d30(x30)\nand m30(x1:30). discuss the possibility that the bug becomes entrapped.\nd. finally, try applying the simplistic method of generating saws by simulating\npaths disregarding the self-avoidance requirement and then eliminating any self-\nintersecting paths post hoc. compare the efficiency of this method to the approach\nin part (c) and how it depends on the total number of steps taken (i.e., t \u226b 30).\n\n "}, {"Page_number": 211, "text": "chapter 7\nmarkov chain monte carlo\n\n(t)\n1 , . . . , x(t)\n\nwhen a target density f can be evaluated but not easily sampled, the methods from\nchapter 6 can be applied to obtain an approximate or exact sample. the primary use\nof such a sample is to estimate the expectation of a function of x \u223c f(x). the markov\nchain monte carlo (mcmc) methods introduced in this chapter can also be used to\ngenerate a draw from a distribution that approximates f, but they are more properly\nviewed as methods for generating a sample from which expectations of functions of\nx can reliably be estimated. mcmc methods are distinguished from the simulation\ntechniques in chapter 6 by their iterative nature and the ease with which they can be\ncustomized to very diverse and difficult problems. viewed as an integration method,\nmcmc has several advantages over the approaches in chapter 5: increasing problem\ndimensionality usually does not slow convergence or make implementation more\ncomplex.\na quick review of discrete-state-space markov chain theory is provided in\n\nsection1.7.letthesequence!x(t)\"denoteamarkovchainfor t = 0,1,2, . . .,where\nx(t) =#x\np$ and the state space is either continuous or discrete. for the\n\ntypes of markov chains introduced in this chapter, the distribution of x(t) converges\nto the limiting stationary distribution of the chain when the chain is irreducible and\naperiodic. the mcmc sampling strategy is to construct an irreducible, aperiodic\nmarkovchainforwhichthestationarydistributionequalsthetargetdistribution f.for\nsufficiently large t, a realization x(t) from this chain will have approximate marginal\ndistribution f. a very popular application of mcmc methods is to facilitate bayesian\ninferencewhere f isabayesianposteriordistributionforparametersx;ashortreview\nof bayesian inference is given in section 1.5.\nthe art of mcmc lies in the construction of a suitable chain. a wide variety of\nalgorithms has been proposed. the dilemma lies in how to determine the degree of\ndistributional approximation that is inherent in realizations from the chain as well as\nestimators derived from these realizations. this question arises because the distribu-\ntion of x(t) may differ substantially from f when t is too small (note that t is always\nlimited in computer simulations), and because the x(t) are serially dependent.\nmcmctheoryandapplicationsareareasofactiveresearchinterest.ourempha-\nsis here is on introducing some basic mcmc algorithms that are easy to implement\nand broadly applicable. in chapter 8, we address several more sophisticated mcmc\ntechniques. some comprehensive expositions of mcmc and helpful tutorials include\n[70, 97, 106, 111, 543, 633]\n\ncomputational statistics, second edition. geof h. givens and jennifer a. hoeting.\n\u00a9 2013 john wiley & sons, inc. published 2013 by john wiley & sons, inc.\n\n201\n\n "}, {"Page_number": 212, "text": "202\n\nchapter 7 markov chain monte carlo\n\n7.1 metropolis\u2013hastings algorithm\na very general method for constructing a markov chain is the metropolis\u2013hastings\nalgorithm [324, 460]. the method begins at t = 0 with the selection of x(0) = x(0)\ndrawn at random from some starting distribution g, with the requirement that\nf%x(0)& > 0. given x(t) = x(t), the algorithm generates x(t+1) as follows:\n1. sample a candidate value x\u2217 from a proposal distribution g%\u00b7|x(t)&.\n2. compute the metropolis\u2013hastings ratio r%x(t),x\u2217&, where\n\nf (v) g(u|v)\nf (u) g (v|u) .\nnote that r%x(t),x\u2217& is always defined, because the proposal x\u2217 = x\u2217 can\nonly occur if f%x(t)& > 0 and g%x\u2217|x(t)& > 0.\n\n3. sample a value for x(t+1) according to the following:\n\nr(u,v) =\n\n(7.1)\n\nx(t+1)\n\n=\u2019x\u2217 with probability min!r%x(t),x\u2217& ,1\" ,\n\notherwise.\n\nx(t)\n\n(7.2)\n\n4. increment t and return to step 1.\nwe will call the tth iteration the process that generates x(t) = x(t). when the proposal\ndistribution is symmetric so that g%x(t)|x\u2217& = g%x\u2217|x(t)&, the method is known as the\nmetropolis algorithm [460].\nclearly, a chain constructed via the metropolis\u2013hastings algorithm is markov\nsince x(t+1) is only dependent on x(t). whether the chain is irreducible and aperiodic\ndepends on the choice of proposal distribution; the user must check these conditions\ndiligently for any implementation. if this check confirms irreducibility and aperiod-\nicity, then the chain generated by the metropolis\u2013hastings algorithm has a unique\nlimiting stationary distribution. this result would seem to follow from equation\n(1.44). however, we are now considering both continuous- and discrete-state-space\nmarkov chains. nevertheless, irreducibility and aperiodicity remain sufficient condi-\ntionsforconvergenceofmetropolis\u2013hastingschains.additionaltheoryisprovidedin\n[462, 543].\ntofindtheuniquestationarydistributionofanirreducibleaperiodicmetropolis\u2013\nhastings chain, suppose x(t) \u223c f(x), and consider two points in the state space of the\nchain, say x1 and x2, for which f(x1) > 0 and f(x2) > 0. without loss of generality,\nlabel these points in the manner such that f(x2)g(x1|x2) \u2265 f(x1)g(x2|x1).\nit follows that the unconditional joint density of x(t) = x1 and x(t+1) = x2 is\nf(x1)g(x2|x1), because if x(t) = x1 and x\u2217 = x2, then r(x1,x2) \u2265 1 so x(t) = x2.\nthe unconditional joint density of x(t) = x2 and x(t+1) = x1 is\n\nf(x2)g(x1|x2) f(x1)g(x2|x1)\nf(x2)g(x1|x2) ,\n\n(7.3)\n\n "}, {"Page_number": 213, "text": "7.1 metropolis\u2013hastings algorithm 203\nbecause we need to start with x(t) = x2, to propose x\u2217 = x1, and then to set x(t+1)\nequal to x\u2217 with probability r(x1,x2). note that (7.3) reduces to f(x1)g(x2|x1),\nwhich matches the joint density of x(t) = x1 and x(t+1) = x2. therefore the joint\ndistribution of x(t) and x(t+1) is symmetric. hence x(t) and x(t+1) have the same\nmarginal distributions. thus the marginal distribution of x(t+1) is f, and f must be\nthe stationary distribution of the chain.\nrecall from equation (1.46) that we can approximate the expectation of a func-\ntion of a random variable by averaging realizations from the stationary distribution of\na metropolis\u2013hastings chain. the distribution of realizations from the metropolis\u2013\nhastings chain approximates the stationary distribution of the chain as t progresses;\n\ni=1 h%x(i)&. some of the useful quantities that can be\ntherefore e{h(x)} \u2248 (1/n)(n\nestimated this way include means e{h(x)}, variances e![h(x) \u2212 e{h(x)}]2\", and\ntail probabilities e!1{h(x)\u2264q}\" for constant q, where 1{a} = 1 if a is true and 0\n\notherwise. using the density estimation methods of chapter 10, estimates of f itself\ncan also be obtained. due to the limiting properties of the markov chain, estimates\nof all these quantities based on sample averages are strongly consistent. note that the\nsequence x(0),x(1), . . . will likely include multiple copies of some points in the state\nspace. this occurs when x(t+1) retains the previous value x(t) rather than jumping to\nthe proposed value x\u2217. it is important to include these copies in the chain and in any\nsample averages since the frequencies of sampled points are used to correct for the\nfact that the proposal density differs from the target density.\nin some applications persistent dependence of the chain on its starting value\ncan seriously degrade its performance. therefore it may be sensible to omit some of\nthe initial realizations of the chain when computing a sample average. this is called\nthe burn-in period and is an essential component of mcmc applications. as with\noptimization algorithms, it is also a good idea to run mcmc procedures like the\nmetropolis\u2013hastings algorithm from multiple starting points to check for consistent\nresults. see section 7.3 for implementation advice about burn-in, number of chains,\nstarting values, and other aspects of mcmc implementation.\nspecific features of good proposal distributions can greatly enhance the per-\nformance of the metropolis\u2013hastings algorithm. a well-chosen proposal distribution\nproduces candidate values that cover the support of the stationary distribution in a\nreasonable number of iterations and, similarly, produces candidate values that are not\nacceptedorrejectedtoofrequently[111].bothofthesefactorsarerelatedtothespread\nof the proposal distribution. if the proposal distribution is too diffuse relative to the\ntarget distribution, the candidate values will be rejected frequently and thus the chain\nwill require many iterations to adequately explore the space of the target distribution.\nif the proposal distribution is too focused (e.g., has too small a variance), then the\nchain will remain in one small region of the target distribution for many iterations\nwhile other regions of the target distribution will not be adequately explored. thus\na proposal distribution whose spread is either too small or too large can produce a\nchain that requires many iterations to adequately sample the regions supported by the\ntarget distribution. section 7.3.1 further discusses this and related issues.\nbelow we introduce several metropolis\u2013hastings variants obtained by using\ndifferent classes of proposal distributions.\n\n "}, {"Page_number": 214, "text": "204\n\nchapter 7 markov chain monte carlo\n\nindependence chains\n\n7.1.1\nsuppose that the proposal distribution for the metropolis\u2013hastings algorithm is cho-\ndence chain, where each candidate value is drawn independently of the past. in this\ncase, the metropolis\u2013hastings ratio is\n\nsen such that g%x\u2217|x(t)& = g(x\u2217) for some fixed density g. this yields an indepen-\n\nr%x(t),x\u2217& =\n\nf (x\u2217) g%x(t)&\nf%x(t)&g%x\u2217& .\n\n(7.4)\n\nw\u2217 = f (x\u2217) /g(x\u2217) and w(t) = f%x(t)& /g%x(t)&, then r%x(t),x\u2217& = w\u2217/w(t). this\n\nthe resulting markov chain is irreducible and aperiodic if g (x) > 0 whenever\nf (x) > 0.\nnoticethatthemetropolis\u2013hastingsratioin(7.4)canbereexpressedastheratio\nof importance ratios (see section 6.4.1) where f is the target and g is the envelope: if\nreexpression indicates that when w(t) is much larger than typical w\u2217 values, then\nthe chain will tend to get stuck for long periods at the current value. therefore, the\ncriteria discussed in section 6.3.1 for choosing importance sampling envelopes are\nalso relevant here for choosing proposal distributions: the proposal distribution g\nshould resemble the target distribution f, but should cover f in the tails.\nexample7.1 (bayesianinference) mcmcmethodslikethemetropolis\u2013hastings\nalgorithm are particularly popular tools for bayesian inference, where some data\ny are observed with likelihood function l(\u03b8|y) for parameters \u03b8 which have prior\ndistribution p(\u03b8). bayesian inference is based on the posterior distribution p(\u03b8|y) =\nc p(\u03b8)l(\u03b8|y), where c is an unknown constant. the difficulty of computing c and\nother features of the posterior prevents most direct inferential strategies. however,\nif we can obtain a sample from a markov chain whose stationary distribution is\nthe target posterior, this sample may be used to estimate posterior moments, tail\nprobabilities, and many other useful quantities, including the posterior density itself.\nmcmc methods typically allow easy generation of such a sample in the bayesian\ncontext.\naverysimplestrategyistousethepriorasaproposaldistributioninanindepen-\ndence chain. in our metropolis\u2013hastings notation, f(\u03b8) = p(\u03b8|y) and g(\u03b8\u2217) = p(\u03b8\u2217).\nconveniently, this means\n\nr%\u03b8(t), \u03b8\u2217& =\n\nl( \u03b8\u2217))y)\nl%\u03b8(t)))y& .\n\n(7.5)\n\nin other words, we propose from the prior, and the metropolis\u2013hastings ratio equals\nthe likelihood ratio. by definition, the support of the prior covers the support of the\ntarget posterior, so the stationary distribution of this chain is the desired posterior.\nthere are more specialized mcmc algorithms to sample various types of posteriors\nin more efficient manners, but this is perhaps the simplest generic approach.\n!\n\n "}, {"Page_number": 215, "text": "7.1 metropolis\u2013hastings algorithm 205\n\ny\nt\ni\ns\nn\ne\nd\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\nfigure 7.1 histogram of 100 observations simulated from the mixture distribution (7.6)\nin example 7.2.\n\ny\n\n\u03b4n%7,0.52& + (1 \u2212 \u03b4)n%10,0.52&.\n\nexample 7.2 (mixture distribution) suppose we have observed data y1, y2, . . . ,\ny100 sampled independently and identically distributed from the mixture distribution\n(7.6)\nfigure 7.1 shows a histogram of the data, which are available from the website for\nthis book. mixture densities are common in real-life applications where, for example,\nthe data may come from more than one population. we will use mcmc techniques\nto construct a chain whose stationary distribution equals the posterior density of \u03b4\nassuming a unif(0,1) prior distribution for \u03b4. the data were generated with \u03b4 = 0.7,\nso we should find that the posterior density is concentrated in this area.\ninthisexample,wetrytwodifferentindependencechains.inthefirstcaseweuse\na beta(1,1) density as the proposal density, and in the second case we use a beta(2,10)\ndensity. the first proposal distribution is equivalent to a unif(0,1) distribution, while\nthe second is skewed right with mean approximately equal to 0.167. in this second\ncase values of \u03b4 near 0.7 are unlikely to be generated from the proposal distribution.\nfigure7.2showsthesamplepathsfor10,000iterationsofbothchains.asample\npath is a plot of the chain realizations \u03b4(t) against the iteration number t. this plot\nis useful for investigating the behavior of the markov chain and is discussed further\nin section 7.3.1. the top panel of figure 7.2 corresponds to the chain generated\nusing the beta(1,1) proposal density. this panel shows a markov chain that moves\nquickly away from its starting value and seems easily able to sample values from\nall portions of the parameter space supported by the posterior for \u03b4. such behavior\nis called good mixing. the lower panel corresponds to the chain using a beta(2,10)\nproposal density. the resulting chain moves slowly from its starting value and does\na poor job of exploring the region of posterior support (i.e., poor mixing). this chain\nhas clearly not converged to its stationary distribution since drift is still apparent. of\n\n "}, {"Page_number": 216, "text": "206\n\nchapter 7 markov chain monte carlo\n\n)\nt\n(\n\u03b4\n\n)\nt\n(\n\u03b4\n\n0.8\n\n0.6\n\n0.4\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n\n0\n\n2500\n\n5000\nt\n\n7500\n\n10000\n\n2500\n\n5000\nt\n\n7500\n\n10000\n\nfigure 7.2 sample paths for \u03b4 from independence chains with proposal densities beta(1,1)\n(top) and beta(2,10) (bottom) considered in example 7.2.\n\ncourse, the long-run behavior of the chain will in principle allow estimation of aspects\nof the posterior distribution for \u03b4 since the posterior is still the limiting distribution of\nthe chain. yet, chain behavior like that shown in the bottom panel of figure 7.2 does\nnot inspire confidence: the chain seems nonstationary, only a few unique values of\n\u03b4(t) were accepted, and the starting value does not appear to have washed out. a plot\nlike the lower plot in figure 7.2 should make the mcmc user reconsider the proposal\ndensity and other aspects of the mcmc implementation.\nfigure 7.3 shows histograms of the realizations from the chains, after the first\n200 iterations have been omitted to reduce the effect of the starting value (see the\ndiscussion of burn-in periods in section 7.3.1.2). the top and bottom panels again\ncorrespond to the beta(1,1) and beta(2,10) proposal distributions, respectively. this\nplot shows that the chain with the beta(1,1) proposal density produced a sample for\n\u03b4 whose mean well approximates the true value (and posterior mean) of \u03b4 = 0.7.\non the other hand, the chain with the beta(2,10) proposal density would not yield\nreliable estimates for the posterior or the true value of \u03b4 based on the first 10,000\niterations.\n!\n\n7.1.2 random walk chains\na random walk chain is another type of markov chain produced via a simple variant\nof the metropolis\u2013hastings algorithm. let x\u2217 be generated by drawing \u03f5 \u223c h(\u03f5) for\nsome density h and then setting x\u2217 = x(t) + \u03f5. this yields a random walk chain.\nin this case, g%x\u2217|x(t)& = h%x\u2217 \u2212 x(t)&. common choices for h include a uniform\ndistribution over a ball centered at the origin, a scaled standard normal distribution,\n\n "}, {"Page_number": 217, "text": "7.1 metropolis\u2013hastings algorithm 207\n\n0.5\n\n0.6\n\n0.7\n\u03b4(t)\n\n0.8\n\n0.9\n\ny\nc\nn\ne\nu\nq\ne\nr\nf\n\ny\nc\nn\ne\nu\nq\ne\nr\nf\n\n2000\n1500\n1000\n500\n0\n\n2000\n1500\n1000\n500\n0\n\n0.5\n\n0.6\n\n0.8\n\n0.9\n\n0.7\n\u03b4(t)\n\nfigure 7.3 histograms of \u03b4(t) for iterations 201\u201310,000 of independence chains with pro-\nposal densities beta(1,1) (top) and beta(2,10) (bottom) considered in example 7.2.\n\nand a scaled student\u2019s t distribution. if the support region of f is connected and h is\npositive in a neighborhood of 0, the resulting chain is irreducible and aperiodic [543].\nfigure 7.4 illustrates how a random walk chain might progress in a two-\ndimensional problem. the figure shows a contour plot of a two-dimensional target\ndistribution (dotted lines) along with the first steps of a random walk mcmc pro-\ncedure. the sample path is shown by the solid line connecting successive values in\nthe chain (dots). the chain starts at x(0). the second candidate value is accepted to\nyield x(1). the circles around x(0) and x(1) show the proposal densities, where h is\na uniform distribution over a disk centered at the origin. in a random walk chain,\nthe proposal density at iteration t + 1 is centered around x(t). some candidate values\nare rejected. for example, the 13th candidate value, denoted by \u25e6, is not accepted,\nso x(13) = x(12). note how the chain frequently moves up the contours of the target\ndistribution, while also allowing some downhill moves. the move from x(15) to x(16)\nis one instance where the chain moves downhill.\nexample 7.3 (mixture distribution, continued) as a continuation of exam-\nple 7.2, consider using a random walk chain to learn about the posterior for \u03b4 under\na unif(0,1) prior. suppose we generate proposals by adding a unif(\u2212a, a) random\nincrement to the current \u03b4(t). clearly it is likely that some proposals will be generated\noutside the interval [0,1] during the progression of the chain. an inelegant approach\nis to note that the posterior is zero for any \u03b4 /\u2208 [0,1], thereby forbidding steps to such\npoints. an approach that usually is better involves reparameterizing the problem.\nlet u = logit{\u03b4} = log!\u03b4/(1 \u2212 \u03b4)\". we may now run a random walk chain on u,\ngenerating a proposal by adding, say, a unif(\u2212b, b) random increment to u(t).\n\n "}, {"Page_number": 218, "text": "208\n\nchapter 7 markov chain monte carlo\n\nx(16)\nx(15)\n\nx(12)\n\nx(0)\n\nx(1)\n\nfigure 7.4 hypothetical random walk chain for sampling a two-dimensional target distri-\nbution (dotted contours) using proposed increments sampled uniformly from a disk centered\nat the current value. see text for details.\n\nthere are two ways to view the reparameterization. first, we may run the\nchain in \u03b4-space. in this case, the proposal density g(\u00b7|u(t)) must be transformed into a\nproposal density in \u03b4-space, taking account of the jacobian. the metropolis\u2013hastings\nratio for a proposed value \u03b4\u2217 is then\n\n,\n\n(7.7)\n\nf (\u03b4\u2217) g%logit!\u03b4(t)\"))logit!\u03b4\u2217\"&|j(\u03b4(t))|\nf%\u03b4(t)&g%logit!\u03b4\u2217\"))logit!\u03b4(t)\"&|j(\u03b4\u2217)|\n\nwhere, for example, |j(\u03b4(t))| is the absolute value of the (determinant of the) jacobian\nfor the transformation from \u03b4 to u, evaluated at \u03b4(t). the second option is to run\nthe chain in u-space. in this case, the target density for \u03b4 must be transformed into\na density for u, where \u03b4 = logit\u22121{u} = exp{u}/(1 + exp{u}). for u\u2217 = u\u2217, this\nyields the metropolis\u2013hasting ratio\n\nf%logit\u22121{u\u2217}&|j(u\u2217)|g% u(t))) u\u2217&\nf%logit\u22121!u(t)\"&|j(u(t))|g% u\u2217| u(t)& .\n\n(7.8)\nsince |j(u\u2217)| = 1/|j(\u03b4\u2217)|, we can see that these two viewpoints produce equivalent\nchains. examples 7.10 and 8.1 demonstrate the change-of-variables method within\nthe metropolis\u2013hastings algorithm.\nthe random walk chain run with uniform increments in a reparameterized space\nmayhavequitedifferentpropertiesthanonegeneratedfromuniformincrementsinthe\noriginal space. reparameterization is a useful approach to improving the performance\nof mcmc methods and is discussed further in section 7.3.1.4.\nfigure 7.5 shows sample paths for \u03b4 from two random walk chains run in\nu-space. the top panel corresponds to a chain generated by drawing \u03f5 \u223c unif(\u22121,1),\nsetting u\u2217 = u(t) + \u03f5, and then using (7.8) to compute the metropolis\u2013hastings ratio.\n\n "}, {"Page_number": 219, "text": "7.2 gibbs sampling\n\n209\n\n2500\n\n5000\nt\n\n7500\n\n10000\n\n0.8\n\n0.6\n\n0.4\n\n0.8\n\n0.6\n\n0.4\n\n)\nt\n(\n\u03b4\n\n)\nt\n(\n\u03b4\n\n0\n\n0\n\n2500\n\n5000\nt\n\n7500\n\n10000\n\nfigure 7.5 sample paths for \u03b4 from random walk chains in example 7.3, run in u-space\nwith b = 1 (top) and b = 0.01 (bottom).\nthe top panel in figure 7.5 shows a markov chain that moves quickly away from\nits starting value and seems easily able to sample values from all portions of the\nparameter space supported by the posterior for \u03b4. the lower panel corresponds to\nthe chain using \u03f5 \u223c unif(\u22120.01,0.01), which yields very poor mixing. the resulting\nchain moves slowly from its starting value and takes very small steps in \u03b4-space at\neach iteration.\n!\n\n7.2 gibbs sampling\nthus far we have treated x(t) with little regard to its dimensionality. the gibbs\nsampler is specifically adapted for multidimensional target distributions. the goal is\nto construct a markov chain whose stationary distribution\u2014or some marginalization\nthereof\u2014equals the target distribution f. the gibbs sampler does this by sequentially\nsamplingfromunivariateconditionaldistributions,whichareoftenavailableinclosed\nform.\n\n7.2.1 basic gibbs sampler\n\nrecallx =%x1, . . . , xp&t,anddenotex-i = (x1, . . . , xi\u22121, xi+1, . . . , xp)t.sup-\nposethattheunivariateconditionaldensityof xi|x-i = x-i,denoted f ( xi|x-i),iseas-\nily sampled for i = 1, . . . , p. a general gibbs sampling procedure can be described\nas follows:\n1. select starting values x(0), and set t = 0.\n\n "}, {"Page_number": 220, "text": "210\n\nchapter 7 markov chain monte carlo\n\n2. generate, in turn,\n(t+1)\n1\n(t+1)\n2\nx\n\nx\n\n))\u00b7 \u223c f#x1))x\n))\u00b7 \u223c f#x2))x\n...\np\u22121))\u00b7 \u223c f#xp\u22121))x\n))\u00b7 \u223c f#xp))x\n\np\n\n(t+1)\nx\nx(t+1)\n\np$ ,\n(t)\n2 , . . . , x(t)\np$ ,\n(t)\n(t+1)\n3 , . . . , x(t)\n1\n\n, x\n\n(t+1)\n1\n(t+1)\n1\n\n(t+1)\n2\n, x\n(t+1)\n2\n, x\n\np$ ,\n(t+1)\np\u22122 , x(t)\n, . . . , x\np\u22121$ ,\n(t+1)\n\n, . . . , x\n\n(7.9)\n\nwhere |\u00b7 denotes conditioning on the most recent updates to all other elements\nof x.\n\n3. increment t and go to step 2.\n\nthecompletionofstep2forallcomponents of x iscalled a cycle.severalmethodsfor\nimproving and generalizing the basic gibbs sampler are discussed in sections 7.2.3\u2013\n7.2.6. in subsequent discussion of the gibbs sampler, we frequently refer to the\nterm x(t)\n\u2212i, which represents all the components of x, except for xi, at their current\nvalues, so\n\nx(t)\n\n\u2212i =#x\n\n(t+1)\n1\n\n, . . . , x\n\n(t+1)\ni\u22121 , x\n\np$.\n(t)\ni+1, . . . , x(t)\n\nexample 7.4 (stream ecology) stream insects called benthic invertebrates are an\neffective indicator for monitoring stream ecology because their relatively stationary\nsubstrate habitation provides constant exposure to contamination and easy sampling\nof large numbers of individuals. imagine that at many sites along a stream, insects are\ncollected and classified into several categories based on some ecologically significant\ncriterion. at a particular site, let y1, . . . , yc denote the counts of insects in each of c\ndifferent classes.\nthe probability that an insect is classified in each category varies randomly\nfrom site to site, as does the total number collected at each site. for a given site, let\np1, . . . , pc denote the class probabilities, and let n denote the random total number\nof insects collected. suppose, further, that the p1, . . . , pc depend on a set of site-\nspecific features summarized by parameters \u03b11, . . . , \u03b1c, respectively. let n depend\non a site-specific parameter, \u03bb.\nsuppose two competing statistics, t1(y1, . . . , yc) and t2(y1, . . . , yc), are used\ntomonitorstreamsfornegativeenvironmentalevents.analarmistriggeredifthevalue\nof t1 or t2 exceeds some threshold. to compare the performance of these statistics\nacross multiple sites within the same stream or across different types of streams,\na monte carlo simulation experiment is designed. the experiment is designed by\nchoosing a collection of parameter sets (\u03bb, \u03b11, . . . , \u03b1c) that are believed to encompass\nthe range of sampling effort and characteristics of sites and streams likely to be\nmonitored. each parameter set corresponds to a hypothetical sampling effort at a\nsimulated site.\n\n "}, {"Page_number": 221, "text": "7.2 gibbs sampling\n\n211\n\nlet c = 3. for a given simulated site, we can establish the model:\n\n(y1, y2, y3)|%n = n, p1 = p1, p2 = p2, p3 = p3& \u223c multinomial(n; p1, p2, p3) ,\n\n(p1, p2, p3) \u223c dirichlet(\u03b11, \u03b12, \u03b13) ,\n\nn \u223c poisson(\u03bb),\n\nwhere n isviewedasrandombecauseitvariesacrosssites.thismodelisoverspecified\nbecause we require y1 + y2 + y3 = n and p1 + p2 + p3 = 1. therefore, we can\nwrite the state of the model as x = (y1, y2, p1, p2, n), where the remaining variables\ncan be determined analytically for any value of x. cassella and george offer a related\nmodel for the hatching of insect eggs [97]. more sophisticated models of stream\necology data are given in [351].\nto complete the simulation experiment, it is necessary to sample from the\nmarginal distribution of (y1, y2, y3) so that the performance of the statistics t1 and\nt2 may be compared for a simulated site of the current type. having repeated this\nprocess over the designed set of simulated sites, comparative conclusions about t1\nand t2 can be drawn.\nit is impossible to get a closed-form expression for the marginal distribution\nof (y1, y2, y3) given the parameters \u03bb, \u03b11, \u03b12, and \u03b13. the most succinct way to\nsummarize the gibbs sampling scheme for this problem is\n\n(y1, y2, y3)|\u00b7 \u223c multinomial(n; p1, p2, p3) ,\n(p1, p2, p3)|\u00b7 \u223c dirichlet(y1 + \u03b11, y2 + \u03b12, n \u2212 y1 \u2212 y2 + \u03b13) ,\nn \u2212 y1 \u2212 y2|\u00b7 \u223c poisson(\u03bb(1 \u2212 p1 \u2212 p2)) ,\n\n(7.10)\n\nwhere |\u00b7 denotes that the distribution is conditional on the variables remaining from\nthe complete set of variables {n, y1, y2, y3, p1, p2, p3}. problem 7.4 asks you to\nderive these distributions.\nat first glance, (7.10) does not seem to resemble the univariate sampling strat-\negy inherent in a gibbs sampler. it is straightforward to show that (7.10) amounts\nto the following sampling scheme based on univariate conditional distributions of\ncomponents of x:\n\n(t+1)\n1\n\ny\n\n))\u00b7 \u223c bin*n(t)\n))\u00b7 \u223c bin*n(t)\n2 )))))\u00b7 \u223c beta#y\n)))))\u00b7 \u223c beta#y\n\np\n\n(t+1)\n2\ny\n(t+1)\n1\n(t)\n1 \u2212 p\n(t+1)\n2\np\n(t+1)\n1 \u2212 p\n1\n\n2 + ,\n(t)\n1\np\n(t)\n1 \u2212 p\n1 + ,\n(t)\n2\np\n1 \u2212 p\n\u2212 y\n\n(t+1)\n1\n\n(t)\n\n(t)\n2 ,\n\n\u2212 y\n\n(t+1)\n1\n\n,\n\n\u2212 y\n(t+1)\n+ \u03b11, n(t)\n1\n+ \u03b12, n(t)\n\n(t+1)\n2\n\n(t+1)\n2\n\n\u2212 y\n\n(t+1)\n1\n\n\u2212 y\n\n\u2212 y\n\n(t+1)\n2\n\n+ \u03b13$ ,\n+ \u03b13$ ,\n\n "}, {"Page_number": 222, "text": "chapter 7 markov chain monte carlo\n\n212\nand\n\n\u2212 y\n\n\u2212 y\n\n(t+1)\n1\n\n(t+1)\n2\n\n(t+1)\n2\n\n(t+1)\n1\n\nn(t+1)\n\n&$ .\n\n))\u00b7 \u223c poisson#\u03bb%1 \u2212 p\n\n\u2212 p\nin section 7.2.4 an alternative gibbs approach uses (7.10) directly.\n!\nexample 7.5 (bayesian inference, continued)\nthe gibbs sampler is particu-\nlarly useful for bayesian applications when the goal is to make inference based\non the posterior distribution of multiple parameters. recall example 7.1 where the\nparameter vector \u03b8 has prior distribution p(\u03b8) and likelihood function l(\u03b8|y) aris-\ning from observed data y. bayesian inference is based on the posterior distribution\np(\u03b8|y) = c p(\u03b8)l(\u03b8|y), where c is an unknown constant. when the requisite univari-\nate conditional densities are easily sampled, the gibbs sampler can be applied and\ndoes not require evaluation of the constant c =, p(\u03b8)l(\u03b8|y) d\u03b8. in this case the ith\nstep in a cycle of the gibbs sampler at iteration t is given by draws from\n\n(t+1)\n\n\u03b8\ni\n\n(t)\n\n))#\u03b8\n\u2212i,y$ \u223c p#\u03b8i))\u03b8\n\n(t)\n\n\u2212i,y$\n\nwhere p is the univariate conditional posterior of \u03b8i given the remaining parameters\nand the data.\n!\nexample 7.6 (fur seal pup capture\u2013recapture study)\nby the late 1800s fur\nseals in new zealand were nearly brought to extinction by polynesian and european\nhunters.inrecentyearstheabundanceoffursealsinnewzealandhasbeenincreasing.\nthis increase has been of great interest to scientists, and these animals have been\nstudied extensively [61, 62, 405].\nour goal is to estimate the number of pups in a fur seal colony using a capture\u2013\nrecaptureapproach[585].insuchstudies,separaterepeatedeffortsaremadetocounta\npopulationofunknownsize.inourcase,thepopulationtobecountedisthepopulation\nof pups. no single census attempt is likely to provide a complete enumeration of the\npopulation, nor is it even necessary to try to capture most of the individuals. the\nindividuals captured during each census are released with a marker indicating their\ncapture. a capture of a marked individual during any subsequent census is termed\na recapture. population size can be estimated on the basis of the history of capture\nand recapture data. high recapture rates suggest that the true population size does not\ngreatly exceed the total number of unique individuals ever captured.\nlet n be the unknown population size to be estimated using i census attempts\nwe assume that the population is closed during the period of the sampling, which\nmeans that deaths, births, and migrations are inconsequential during this period. the\ntotal number of distinct animals captured during the study is denoted by r.\nwe consider a model with separate, unknown capture probabilities for each\ncatchable on any one capture occasion, but capture probabilities may change over\n\nyielding total numbers of captures (including recaptures) equaling c =%c1, . . . , ci&.\n\ncensus effort, \u03b1 =%\u03b11, . . . , \u03b1i&. this model assumes that all animals are equally\n\n "}, {"Page_number": 223, "text": "7.2 gibbs sampling\n\n213\n\ntable 7.1 fur seal data for seven census efforts in one season.\n\nnumber captured\nnumber newly caught\n\nci\nmi\n\n1\n30\n30\n\n2\n22\n8\n\ncensus attempt, i\n3\n29\n17\n\n4\n26\n7\n\n5\n31\n9\n\n6\n32\n8\n\n7\n35\n5\n\ntime. the likelihood for this model is\n\nl(n, \u03b1|c, r) \u221d\n\nn!\n(n \u2212 r)!\n\ni-i=1\n\ni (1 \u2212 \u03b1i)n\u2212ci .\n\u03b1ci\n\n(7.11)\n\nthis model is sometimes called the m(t) model [55].\nin a capture\u2013recapture study conducted on the otago peninsula on the south\nisland of new zealand, fur seal pups were marked and released during i = 7 census\nattempts during one season. it is reasonable to assume the population of pups was\nclosed during the study period. table 7.1 shows the number of pups captured (ci) and\nthe number of these captures corresponding to pups never previously caught (mi),\nfor census attempts i = 1, . . . ,7. a total of r =(7\ni=1 mi = 84 unique fur seals were\nobserved during the sampling period.\nfor estimation, one might adopt a bayesian framework where n and \u03b1 are\nassumed to be a priori independent with the following priors. for the unknown popu-\nlation size we use an improper uniform prior f(n) \u221d 1. for the capture probabilities,\nwe use\n(7.12)\n1\nfor i = 1, . . . ,7, and we assume these are a priori independent. if \u03b81 = \u03b82 =\n2, this\ncorresponds to the jeffreys prior. the combination of a uniform prior for n and\na jeffreys prior for \u03b1i is recommended when i > 5 [653]. this leads to a proper\nposterior distribution for the parameters when i > 2 and there is at least one recapture\n(ci \u2212 mi > 1). a gibbs sampler can then be constructed by simulating from the\nconditional posterior distributions\n\nf(\u03b1i|\u03b81, \u03b82) = beta(\u03b81, \u03b82)\n\n(7.13)\n\nn(t+1)\n\n\u2212 84))\u00b7 \u223c negbin*85,1 \u2212\n))\u00b7 \u223c beta.ci +\n\n(t+1)\n\ni\n\n1\n2 , n(t+1)\n\n7-i=1#1 \u2212 \u03b1\n\n(t)\n\ni $+ ,\n2/\n\n1\n\n\u03b1\n\n\u2212 ci +\n\n(7.14)\nfor i = 1, . . . ,7. here |\u00b7 denotes conditioning on the parameters among {n, \u03b1, \u03b81, \u03b82}\naswellasthedataintable7.1,andnegbindenotesthenegativebinomialdistribution.\nthe results below are based on a chain of 100,000 iterations with the first\n50,000iterationsdiscardedforburn-in.diagnostics(seeexample7.10)donotindicate\nany problems with convergence. to investigate whether the model produces sensible\nresults, one can compute the mean capture probability for each iteration and compare\n\n "}, {"Page_number": 224, "text": "214\n\nchapter 7 markov chain monte carlo\n\n)\nt\n(\n\n\u03b1\u2013\n\n0.40\n\n0.35\n\n0.30\n\n0.25\n\n84\n\n86\n\n88\n\n90\n\n92\n\n94\n\nfigure 7.6 split boxplots of \u00af\u03b1(t) against n(t) for the seal pup example.\n\n98\n\n100\n\n102\n\n104\n\n106\n\n96\nn (t)\n\n1\n\n(t)\ni\n\ni=1 \u03b1\n\n7(7\n\nit to the corresponding simulated population size. figure 7.6 shows split boxplots of\n\u00af\u03b1(t) =\nfrom (7.13)foreachpopulationsize n(t) from (7.14).as expected,\nthe population size increases as the mean probability of capture decreases. figure 7.7\nshows a histogram of the realizations of n(t) upon which posterior inference about\nn is based. the posterior mean of n is 89.5 with a 95% highest posterior density\n(hpd) interval of (84, 94). (a 95% hpd interval for n is the region of shortest length\ncontaining 95% of the posterior probability for n for which the posterior density\nfor every point contained in the interval is never lower than the density for every\npoint outside the interval. see section 7.3.3 for computational details for hpds using\nmcmc.) for comparison, the maximum likelihood estimate for n is 88.5 and a 95%\nnonparametric bootstrap confidence interval is (85.5, 97.3).\nthelikelihoodgivenin(7.11)isjustoneofthemanyformsofcapture\u2013recapture\nmodelsthatcouldhavebeenconsidered.forexample,amodelwithacommoncapture\nprobability may be more appropriate. other parameterizations of the problem might\nalso be investigated to improve mcmc convergence and mixing, which is strongly\ndependent on the parameterization and updating of (\u03b81, \u03b82). we consider these further\nin examples 7.7 and 7.10.\n!\n\n7.2.2 properties of the gibbs sampler\nclearly the chain produced by a gibbs sampler is markov. under rather mild condi-\ntions, geman and geman [226] showed that the stationary distribution of the gibbs\n(t)\nsampler chain is f. it also follows that the limiting marginal distribution of x\ni equals\nthe univariate marginalization of the target distribution along the ith coordinate. as\n\n "}, {"Page_number": 225, "text": ")\nr\n,\n\nc\n\n|\n\nn\n(\nf!\n \n\nd\ne\nt\na\nm\n\ni\nt\ns\ne\n\n0.15\n\n0.10\n\n0.05\n\n0\n\n85\n\n90\n\n7.2 gibbs sampling\n\n215\n\n100\n\n105\n\n95\nn\n\nfigure 7.7 estimated marginal posterior probabilities for n for the seal pup example.\n\n, . . . , x\n\n, . . . , x\n\n(t+1)\n1\n\n(t+1)\n1\n\n(t+1)\n1\n\n(t+1)\ni\u22121 , x\n\n(t)\ni , . . . , x(t)\n\nwith the metropolis\u2013hastings algorithm, we can use realizations from the chain to\nestimate the expectation of any function of x.\nit is possible to relate the gibbs sampler to the metropolis\u2013hastings algo-\nrithm, allowing for a proposal distribution in the metropolis\u2013hastings algorithm\nthat varies over time. each gibbs cycle consists of p metropolis\u2013hastings steps.\nto see this, note that the ith gibbs step in a cycle effectively proposes the candi-\n\n(t+1)\ni\u22121 , x\u2217i , x\n, . . . , x\n(t)\n(t+1)\ni , . . . , x(t)\ni\u22121 , x\n\np$ given the current state of\n(t)\ni+1, . . . , x(t)\np$. thus, the ith univariate gibbs update\ncan be viewed as a metropolis\u2013hastings step drawing\np \u223c gi#\u00b7)))x\np $ ,\n(t)\n(t+1)\ni , . . . , x(t)\n1\np$ =\u2019 f%x\u2217i))x(t)-i&\n\ndate vector x\u2217 =#x\nthe chain#x\nx\u2217)))x\ngi#x\u2217))x\n\nit is easy to show that in this case the metropolis\u2013hastings ratio equals 1, which\nmeans that the candidate is always accepted.\nthe gibbs sampler should not be applied when the dimensionality of x changes\n(e.g., when moving between models with different numbers of parameters at each\niteration of the gibbs sampler). section 8.2 gives methods for constructing a suitable\nmarkov chain with the correct stationary distribution in this case.\nthe \u201cgibbs sampler\u201d is actually a generic name for a rich family of very adapt-\nable algorithms. in the following subsections we describe various strategies that have\nbeen developed to improve the performance of the general algorithm described above.\n\nif x\u2217-i = x(t)-i ,\notherwise.\n\n(t)\ni , . . . , x(t)\n\n(t+1)\ni\u22121 , x\n\n(t+1)\ni\u22121 , x\n\nwhere\n\n(7.15)\n\n(t+1)\n1\n\n, . . . , x\n\n, . . . , x\n\n0\n\n!\n!\n!\n "}, {"Page_number": 226, "text": "216\n\nchapter 7 markov chain monte carlo\n\n7.2.3 update ordering\nthe ordering of updates made to the components of x in the basic gibbs sampler\n(7.9) can change from one cycle to the next. this is called random scan gibbs sam-\npling [417]. randomly ordering each cycle can be effective when parameters are\nhighly correlated. for example, roberts and sahu [546] give asymptotic results for a\nmultilevel mixed model for which a random scan gibbs sampling approach can yield\nfaster convergence rates than the deterministic update ordering given in (7.9). in prac-\ntice without specialized knowledge for a particular model, we recommend trying both\ndeterministic and random scan gibbs sampling when parameters are highly correlated\nfrom one iterations to the next.\n\n7.2.4 blocking\nanother modification to the gibbs sampler is called blocking or grouping. in the\ngibbs algorithm it is not necessary to treat each element of x individually. in the\nbasic gibbs sampler (7.9) with p = 4, for example, it would be allowable for each\ncycle to proceed with the following sequence of updates:\n(t)\n\n(t+1)\n2\n\nx\n\n, x\n\n(t+1)\n1\nx\n(t+1)\n3\n(t+1)\n4\n\nx\n\n(t)\n2 , x\n\n(t)\n3 , x\n(t+1)\n1\n(t+1)\n1\n\n(t)\n, x\n(t+1)\n2\n, x\n\n4 $ ,\n4 $ ,\n\n(t+1)\n3\n, x\n\n)))\u00b7 \u223c f#x1)))x\n)))\u00b7 \u223c f#x2, x3)))x\n)))\u00b7 \u223c f#x4)))x\n\n$ .\n\nin example 7.4, we saw that the stream ecology parameters were naturally\ngrouped into a conditionally multinomial set of parameters, a conditionally dirichlet\nset of parameters, and a single conditionally poisson element (7.10). it would be\nconvenient and correct to cycle through these blocks, sequentially sampling from\nmultivariate instead of univariate conditional distributions in the multinomial and\ndirichlet cases.\nblocking is typically useful when elements of x are correlated, with the algo-\nrithm constructed so that more correlated elements are sampled together in one block.\nroberts and sahu compare convergence rates for various blocking and update order-\ning strategies [546]. the structured markov chain monte carlo method of sargent\net al. offers a systematic approach to blocking that is directly motivated by the model\nstructure [569]. this method has been shown to offer faster convergence for problems\nwith a large number of parameters, such as bayesian analyses for longitudinal and\ngeostatistical data [110, 124].\n\n7.2.5 hybrid gibbs sampling\nfor many problems the conditional distributions for one or more elements of x are\nnot available in closed form. in this case, a hybrid mcmc algorithm can be developed\nwhere at a given step in the gibbs sampler, the metropolis\u2013hastings algorithm is used\nto sample from the appropriate conditional distribution. for example, for p = 5, a\nhybrid mcmc algorithm might proceed with the following sequence of updates:\n\n "}, {"Page_number": 227, "text": "chain because this conditional distribution is not available in closed form.\n\n(t+1)\n1. update x\n1\n\ndistribution is available in closed form.\n(t)\n4 , x\n\n(t+1)\n2\n\n, x\n\n, x\n\n2. update#x\n\n)))#x\n\n(t)\n2 , x\n(t+1)\n3\n\n(t)\n3 , x\n\n$)))#x\n\ncause this joint conditional distribution is difficult to sample from or is not\navailable in closed form. here, blocking x2 and x3 might be recommended\nbecause these elements are highly correlated.\n\n7.2 gibbs sampling\n\n217\n\n(t)\n4 , x\n(t+1)\n1\n\n(t)\n\n(t)\n\n5 $ with a gibbs step because this conditional\n5 $withametropolis\u2013hastingstepbe-\n5 $ with a step from a random walk\n$ with a gibbs step.\n\n(t+1)\n4\n, x\n\n(t+1)\n3\n, x\n(t+1)\n3\n, x\n\n(t)\n\n, x\n\n3. update x\n\n4. update x\n\n(t+1)\n1\n(t+1)\n1\n\n(t+1)\n2\n, x\n(t+1)\n2\n, x\n\n(t+1)\n4\n(t+1)\n5\n\n)))#x\n)))#x\n\nfor both theoretical and practical reasons, only one metropolis\u2013hastings step is per-\nformed at each step in the hybrid gibbs sampler. indeed, it has been proven that the\nbasicgibbssamplerinsection7.2.1isequivalenttothecompositionof pmetropolis\u2013\nhastings algorithms with acceptance probabilities equal to 1 [543]. the term \u201chybrid\ngibbs\u201d is rather generic terminology that is used to describe many different algo-\nrithms (see section 8.4 for more examples). the example shown in steps 1\u20134 above\nis more precisely described as a\u201chybrid gibbs sampler with metropolis steps within\ngibbs,\u201d which is sometimes abbreviated as \u201cmetropolis-within-gibbs,\u201d and was first\nproposed by [472].\nexample7.7 (fursealpupcapture\u2013recapturestudy,continued) example7.6\ndescribed the m(t) model in (7.11) for capture\u2013recapture studies. for this model a\ncommon practice is to assume a beta prior distribution for the capture probabilities\nand a noninformative jeffreys prior for n, so f(n) \u221d 1/n. for some datasets, pre-\nvious analyses have shown sensitivity to the values selected for \u03b81 and \u03b82 in (7.12)\n[230]. to mitigate this sensitivity, we consider an alternative setup with a joint distri-\nbution for (\u03b81, \u03b82), namely f(\u03b81, \u03b82) \u221d exp{\u2212(\u03b81 + \u03b82)/1000} with (\u03b81, \u03b82) assumed\nto be a priori independent of the remaining parameters. a gibbs sampler can then be\nconstructed by simulating from the conditional posterior distributions\n\n7-i=1\n(1 \u2212 \u03b1i)$,\nn \u2212 84|\u00b7 \u223c negbin#84,1 \u2212\n\u03b1i|\u00b7 \u223c beta(ci + \u03b81, n \u2212 ci + \u03b82)\n\u0001(\u03b81)\u0001(\u03b82)17 7-i=1\ni (1 \u2212 \u03b1i)\u03b82 exp2\u2212\n\u03b81, \u03b82|\u00b7 \u223c k0 \u0001(\u03b81 + \u03b82)\n\u03b1\u03b81\n\nfor i = 1, . . . ,7,\n1000 3 ,\n\u03b81 + \u03b82\n\n(7.16)\n\n(7.17)\n\n(7.18)\n\nwhere |\u00b7 denotes conditioning on the remaining parameters from {n, \u03b1, \u03b81, \u03b82} as well\nas the data in table 7.1 and k is an unknown constant. note that (7.18) is not easy to\nsample. this suggests using a hybrid gibbs sampler with a metropolis\u2013hastings step\nfor (7.18). thus the gibbs sampler in (7.13)\u2013(7.14) becomes a hybrid gibbs sampler\nin (7.16)\u2013(7.18) when a prior distribution is used for \u03b81 and \u03b82 instead of selecting\nvalues for these parameters.\n!\n\n "}, {"Page_number": 228, "text": "218\n\nchapter 7 markov chain monte carlo\n\nk\n\n(t+1)\n\nj = f%zj|x(t)\n\n7.2.6 griddy\u2013gibbs sampler\nhybrid methods such as embedding metropolis\u2013hastings steps within a gibbs algo-\nrithm are one way to construct a gibbs-like chain when not all the univariate condi-\ntionals are easily sampled. other strategies, evolved from techniques in chapter 6,\ncan be used to sample difficult univariate conditionals.\none such method is the griddy\u2013gibbs sampler [541, 624]. suppose that it is dif-\nficult to sample from the univariate conditional density for xk|x\u2212k for a particular k.\nto implement a griddy\u2013gibbs step, select some grid points z1, . . . , zn over the range\n\u2212k& for j = 1, . . . , n. using these weights\n(t)\nof support of f(\u00b7|x\u2212k). let w\nand the corresponding grid, one can approximate the density function f(\u00b7|x\u2212k) or,\n))x(t)\nequivalently, its inverse cumulative distribution function. generate x\n\u2212k from\nthis approximation, and proceed with the remainder of the mcmc algorithm. the ap-\nproximation to the kth univariate conditional can be refined as iterations proceed. the\n))x(t)\n(t+1)\nsimplestapproachfortheapproximationandsamplingstepistodraw x\n\u2212k from\n(t)\n1 , . . . , w(t)\nthediscretedistributionon z1, . . . , zn withprobabilitiesproportionalto w\nn ,\nusing the inverse cumulative distribution function method (section 6.2.2). a piece-\nwise linear cumulative distribution function could be generated from an approximat-\ning density function that is piecewise constant between the midpoints of any two\nadjacent grid values with a density height set to ensure that the total probability on\nthe segment containing zi is proportional to w\n. other approaches could be based on\nthe density estimation ideas presented in chapter 10.\nif the approximation to f(\u00b7|x\u2212k) is updated from time to time by improving the\ngrid, then the chain is not time homogeneous. in this case, reference to convergence\nresults for metropolis\u2013hastings or gibbs chains is not sufficient to guarantee that\na griddy\u2013gibbs chain has a limiting stationary distribution equal to f. one way to\nensure time homogeneity is to resist making any improvements to the approximat-\ning univariate distribution as iterations progress. in this case, however, the limiting\ndistribution of the chain is still not correct because it relies on an approximation to\nf(\u00b7|x\u2212k) rather than the true density. this can be corrected by reverting to a hybrid\nmetropolis-within-gibbs framework where the variable generated from the approxi-\nmation to f(\u00b7|x\u2212k) is viewed as a proposal, which is then randomly kept or discarded\nbased on the metropolis\u2013hastings ratio. tanner discusses a wide variety of potential\nenhancements to the basic griddy\u2013gibbs strategy [624].\n\n(t)\ni\n\nk\n\n7.3 implementation\nthe goal of an mcmc analysis is to estimate features of the target distribution f. the\nreliabilityofsuchestimatesdependsontheextenttowhichsampleaveragescomputed\nusing realizations of the chain correspond to their expectation under the limiting\nstationary distribution of the chain. aside from griddy\u2013gibbs, all of the mcmc\nmethods described above have the correct limiting stationary distribution. in practice,\nhowever, it is necessary to determine when the chain has run sufficiently long so that it\nisreasonabletobelievethattheoutputadequatelyrepresentsthetargetdistributionand\n\n "}, {"Page_number": 229, "text": "7.3 implementation\n\n219\ncan be used reliably for estimation. unfortunately, mcmc methods can sometimes be\nquite slow to converge, requiring extremely long runs, especially if the dimensionality\nofx islarge.further,itisofteneasytobemisledwhenusingmcmcalgorithmoutput\nto judge whether convergence has approximately been obtained.\nin this section, we examine questions about the long-run behavior of the chain.\nhas the chain run long enough? is the first portion of the chain highly influenced\nby the starting value? should the chain be run from several different starting values?\nhas the chain traversed all portions of the region of support of f? are the sampled\nvalues approximate draws from f? how shall the chain output be used to produce\nestimates and assess their precision? useful reviews of mcmc diagnostic methods\ninclude [76, 125, 364, 458, 543, 544]. we end with some practical advice for coding\nmcmc algorithms.\n\nensuring good mixing and convergence\n\n7.3.1\nit is important to consider how efficiently an mcmc algorithm provides useful infor-\nmation about a problem of interest. efficiency can take on several meanings in this\ncontext, but here we will focus on how quickly the chain forgets its starting value and\nhow quickly the chain fully explores the support of the target distribution. a related\nconcern is how far apart in a sequence observations need to be before they can be\nconsidered to be approximately independent. these qualities can be described as the\nmixing properties of the chain.\nwe must also be concerned whether the chain has approximately reached its\nstationary distribution. there is substantial overlap between the goals of diagnosing\nconvergence to the stationary distribution and investigating the mixing properties\nof the chain. many of the same diagnostics can be used to investigate both mixing\nand convergence. in addition, no diagnostic is fail-safe; some methods can suggest\nthat a chain has approximately converged when it has not. for these reasons, we\ncombine the discussion of mixing and convergence in the following subsections, and\nwe recommend that a variety of diagnostic techniques be used.\n\nsimple graphical diagnostics after programming and running the\n7.3.1.1\nmcmc algorithm from multiple starting points, users should perform various\ndiagnostics to investigate the properties of the mcmc algorithm for the particular\nproblem. three simple diagnostics are discussed below.\na sample path is a plot of the iteration number t versus the realizations of x(t).\nsample paths are sometimes called trace or history plots. if a chain is mixing poorly,\nit will remain at or near the same value for many iterations, as in the lower panel in\nfigure 7.2. a chain that is mixing well will quickly move away from its starting value\nand the sample path will wiggle about vigorously in the region supported by f.\nthe cumulative sum (cusum) diagnostic assesses the convergence of an es-\ntimator of a one-dimensional parameter \u03b8 = e{h(x)} [678]. for n realizations of\nthe chain after discarding some initial iterates, the estimator is given by \u02c6\u03b8n =\ni=14h%x(i)& \u2212 \u02c6\u03b8n5 ver-\n(1/n)(n\nsus t. if the final estimator will be computed using only the iterations of the chain that\nremain after removing some burn-in values (see section 7.3.1.2), then the estimator\n\nj=1 h%x(j)&. the cusum diagnostic is a plot of (t\n\n "}, {"Page_number": 230, "text": "220\n\nchapter 7 markov chain monte carlo\n\n1.0\n\n0.5\n\n0\n\n1.0\n\n0.5\n\n0\n\nf\nc\na\n\nf\nc\na\n\n0\n\n0\n\n10\n\n10\n\n20\nlag\n\n20\nlag\n\n30\n\n30\n\n40\n\n40\n\nfigure 7.8 autocorrelation function plots for independence chain of example 7.2 with\nproposal densities beta(1,1) (top) and beta(2,10) (bottom).\n\nand cusum plot should be based only on the values to be used in the final estima-\ntor. yu and mykland [678] suggest that cusum plots that are very wiggly and have\nsmaller excursions from 0 indicate that the chain is mixing well. plots that have large\nexcursions from 0 and are smoother suggest slower mixing speeds. the cusum plot\nshares one drawback with many other convergence diagnostics: for a multimodal\ndistribution where the chain is stuck in one of the modes, the cusum plot may appear\nto indicate good performance when, in fact, the chain is not performing well.\nan autocorrelation plot summarizes the correlation in the sequence of x(t) at\ndifferent iteration lags. the autocorrelation at lag i is the correlation between iterates\nthat are i iterations apart [212]. a chain that has poor mixing properties will exhibit\nslowdecayoftheautocorrelationasthelagbetweeniterationsincreases.forproblems\nwith more than one parameter it may also be of use to consider cross-correlations\nbetween parameters that might be related, since high cross-correlations may also\nindicate poor mixing of the chain.\nexample 7.8 (mixture distribution, continued) figure 7.8 shows autocorrela-\ntion function (acf) plots for the independence chain described in example 7.2. in the\ntop panel, the more appropriate proposal distribution yields a chain for which the\nautocorrelations decrease rather quickly. in the lower panel, the bad proposal distri-\nbution yields a chain for which autocorrelations are very high, with a correlation of\n0.92 for observations that are 40 iterations apart. this panel clearly indicates poor\nmixing.\n!\n7.3.1.2 burn-in and run length key considerations in the diagnosis of con-\nvergence are the burn-in period and run length. recall that it is only in the limit that\n\n "}, {"Page_number": 231, "text": "7.3 implementation\n\n221\nan mcmc algorithm yields x(t) \u223c f. for any implementation, the iterates will not\nhave exactly the correct marginal distribution, and the dependence on the initial point\n(or distribution) from which the chain was started may remain strong. to reduce the\nseverity of this problem, the first d values from the chain are typically discarded as\na burn-in period.\na commonly used approach for the determination of an appropriate burn-in\nperiod and run length is that of gelman and rubin [221, 224]. this method is based\non a statistic motivated by an analysis of variance (anova): the burn-in period or\nmcmc run-length should be increased if a between-chain variance is considerably\nlarger than the within-chain variance. the variances are estimated based on the results\nof j runs of the mcmc algorithm to create separate, equal-length chains (j \u2265 2) with\nstarting values dispersed over the support of the target density.\nlet l denote the length of each chain after discarding d burn-in iterates. sup-\npose that the variable (e.g., parameter) of interest is x, and its value at the tth iteration\nof the jth chain is x\nare\ndiscarded and the l values x\n\n(t)\nj . thus, for the jth chain, the d values x\nare retained. let\n\n(0)\nj , . . . , x\nj\n\n(d+l\u22121)\n\n(d\u22121)\n\n(d)\nj\n\n, . . . , x\nj\n\n\u00afxj =\n\n1\nl\n\nd+l\u221216t=d\n\n(t)\nx\nj\n\nand\n\n\u00afx\u00b7 =\n\n\u00afxj,\n\n1\nj\n\nj6j=1\n\nand define the between-chain variance as\n\nb =\n\nl\n\nj \u2212 1\n\nj6j=1%\u00afxj \u2212 \u00afx\u00b7&2\n\n.\n\nnext define\n\ns2\nj =\n\n1\nl \u2212 1\n\nd+l\u221216t=d #x\n\n(t)\n\nj \u2212 \u00afxj$2\n\nto be the within-chain variance for the jth chain. then let\n\nw =\n\n1\nj\n\ns2\n\nj\n\nj6j=1\n\nrepresent the mean of the j within-chain estimated variances. finally, let\n\n[(l \u2212 1)/l]w + (1/l)b\n\nw\n\n.\n\nr =\n\n(7.19)\n\n(7.20)\n\n(7.21)\n\n(7.22)\n\nif all the chains are stationary, then both the numerator and the denominator should\nestimatethemarginalvarianceof x.if,however,therearenotabledifferencesbetween\nthe chains, then the numerator will exceed the denominator.\n\n "}, {"Page_number": 232, "text": "222\n\nchapter 7 markov chain monte carlo\n\nin theory, \u221ar \u2192 1 as l \u2192 \u221e. in practice, the numerator in (7.22) is slightly\ntoo large and the denominator is slightly too small. an adjusted estimator is given by\n\n\u02c6r =\n\nj + 1\n\nj\n\nr \u2212\n\nl \u2212 1\n\njl\n\n.\n\nsome authors suggest that \u221a \u02c6r < 1.1 indicates that the burn-in and chain length are\nsufficient [544]. another useful convergence diagnostic is a plot of the values of \u02c6r\nversus the number of iterations. when \u02c6r has not stabilized near 1, this suggests lack\nof convergence. if the chosen burn-in period did not yield an acceptable result, then d\nshould be increased, l should be increased, or preferably both. a conservative choice\nis to use one-half of the iterations for burn-in. the performance of this diagnostic is\n(t)\nimproved if the iterates x\nj are transformed so that their distribution is approximately\nnormal. alternatively, a reparameterization of the model could be undertaken and the\nchain rerun.\nthere are several potential difficulties with this approach. selecting suitable\nstarting values in cases of multimodal f may be difficult, and the procedure will not\nwork if all of the chains become stuck in the same subregion or mode. due to its uni-\ndimensionality, the method may also give a misleading impression of convergence for\nmultidimensional target distributions. enhancements of the gelman\u2013rubin statistic\nare described in [71, 224], including an improved estimate of r in (7.22) that accounts\nfor variability in unknown parameters. in practice, these improvements lead to very\nsimilar results. an extension for multidimensional target distributions is given in [71].\nraftery and lewis [526] proposed a very different quantitative strategy for\nestimatingrunlengthandburn-inperiod.someresearchersadvocatenoburn-in[231].\n7.3.1.3 choice of proposal as illustrated in example 7.2, mixing is strongly\naffected by features of the proposal distribution, especially its spread. further, advice\non desirable features of a proposal distribution depends on the type of mcmc algo-\nrithm employed.\nfor a general metropolis\u2013hastings chain such as an independence chain, it\nseems intuitively clear that we wish the proposal distribution g to approximate the\ntargetdistribution f verywell,whichinturnsuggeststhataveryhighrateofaccepting\nproposals is desirable. although we would like g to resemble f, the tail behavior of g\nis more important than its resemblance to f in regions of high density. in particular, if\nf/g is bounded, the convergence of the markov chain to its stationary distribution is\nfasteroverall[543].thus,itiswisertoaimforaproposaldistributionthatissomewhat\nmore diffuse than f.\nin practice, the variance of the proposal distribution can be selected through an\ninformal iterative process. start a chain, and monitor the proportion of proposals that\nhave been accepted; then adjust the spread of the proposal distribution accordingly.\nafter some predetermined acceptance rate is achieved, restart the chain using the\nappropriately scaled proposal distribution. for a metropolis algorithm with normal\ntarget and proposal distributions, it has been suggested that an acceptance rate of\nbetween 25 and 50% should be preferred, with the best choice being about 44% for\none-dimensional problems and decreasing to about 23.4% for higher-dimensional\n\n "}, {"Page_number": 233, "text": "7.3 implementation\n\n223\nproblems [545, 549]. to apply such rules, care are must be taken to ensure that the\ntarget and proposal distributions are roughly normally distributed or at least simple,\nunimodal distributions. if, for example, the target distribution is multimodal, the chain\nmay get stuck in one mode without adequate exploration of the other portions of the\nparameter space. in this case the acceptance rate may very high, but the probability\nof jumping from one mode to another may be low. this suggests one difficult issue\nwith most mcmc methods; it is useful to have as much knowledge as possible about\nthe target distribution, even though that distribution is typically unknown.\nmethodsforadaptivemarkovchainmontecarlo(section8.1)tunetheproposal\ndistribution in the metropolis algorithm during the mcmc algorithm. these methods\nhave the advantage that they are automatic and, in some implementations, do not\nrequire the user to stop, tune, and restart the algorithm on multiple occasions.\n7.3.1.4 reparameterization modelreparameterizationcanprovidesubstantial\nimprovements in the mixing behavior of mcmc algorithms. for a gibbs sampler,\nperformance is enhanced when components of x are as independent as possible.\nreparameterization is the primary strategy for reducing dependence. for example,\nif f is a bivariate normal distribution with very strong positive correlation, both\nunivariate conditionals will allow only small steps away from x(t) = x(t) along one\naxis. therefore, the gibbs sampler will explore f very slowly. however, suppose\ny = (x1 + x2,x1 \u2212 x2). this transformation yields one univariate conditional on\nthe axis of maximal variation in x and the second on an orthogonal axis. if we view\nthe support of f as cigar shaped, then the univariate conditionals for y allow one\nstep along the length of the cigar, followed by one across its width. therefore, the\nparameterization inherent in y makes it far easier to move from one point supported\nby the target distribution to any other point in a single move (or a few moves).\ndifferent models require different reparameterization strategies. for example,\nif there are continuous covariates in a linear model, it is useful to center and scale the\ncovariates to reduce correlations between the parameters in the model. for bayesian\ntreatment of linear models with random effects, hierarchical centering can be used to\naccelerate mcmc convergence [218, 219]. the term hierarchical centering comes\nfrom the idea that the parameters are centered as opposed to centering the covariates.\nhierarchical centering involves reexpressing a linear model into another form that\nproduces different conditional distributions for the gibbs sampler.\nexample 7.9 (hierarchical centered random effects model) for example, con-\nsider a study of pollutant levels where it is known that tests performed at different\nlaboratories have different levels of measurement error. let yij be the pollutant level\nof the jth sample that was tested at the ith laboratory. we might consider a simple\nrandom effects model\n\n(7.23)\nwhere i = 1, . . . , i and j = 1, . . . , ni. in the bayesian paradigm, we might assume\n\u00b5 \u223c n(\u00b50, \u03c32\n\u03f5). the hierarchical centered form\nof (7.23) is a simple reparameterization of the model with yij = \u03b3i + \u03f5ij where\n\n\u03b1), and \u03f5ij \u223c n(0, \u03c32\n\n\u00b5), \u03b1i \u223c n(0, \u03c32\n\nyij = \u00b5 + \u03b1i + \u03f5ij\n\n "}, {"Page_number": 234, "text": "chapter 7 markov chain monte carlo\n\n224\n\u03b3i = \u00b5 + \u03b1i and \u03b3i|\u00b5 \u223c n(\u00b5, \u03c32\n\u03b1). thus \u03b3 is centered about \u00b5. hierarchical cen-\ntering usually produces better behaved mcmc chains when \u03c32\n\u03f5 is not a lot larger\nthan \u03c32\n\u03b1, which is likely when random effects are deemed useful for modeling a given\ndataset. while this is a simple example, hierarchical centering has been shown to\nproduce more efficient mcmc algorithms for more complex linear model problems\nsuch as generalized linear mixed models. however, the advantages of hierarchical\ncentering can depend on the problem at hand and should be implemented on a case-\nby-case basis [77, 219]. see problems 7.7 and 7.8 for another example of hierarchical\ncentering.\n!\nunfortunately, reparameterization approaches are typically adapted for specific\nmodels, so it is difficult to provide generic advice. another way to improve mixing\nand accelerate convergence of mcmc algorithms is to augment the problem us-\ning so-called auxiliary variables; see chapter 8. a variety of reparameterization and\nacceleration techniques are described in [106, 225, 242, 543].\n7.3.1.5 comparing chains: effective sample size if mcmc realizations\nare highly correlated, then the information gained from each iteration of the mcmc\nalgorithmwillbemuchlessthansuggestedbytherunlength.thereducedinformation\nisequivalenttothatcontainedinasmalleri.i.d.samplewhosesizeiscalledtheeffective\nsample size. the difference between the total number of samples and the effective\nsample size indicates the efficiency lost when correlated samples from the markov\nchain have been used to estimate a quantity of interest instead of an independent and\nidentically distributed sample with the same variance as the observed sample [543].\nto estimate the effective sample size, the first step is to compute the estimated\nautocorrelation time, a summary measure of the autocorrelation between realizations\nand their rate of decay. the autocorrelation time is given by\n\n\u03c4 = 1 + 2 \u221e6k=1\n\n\u03c1(k),\n\n(7.24)\n\nwhere \u03c1(k) is the autocorrelation between realizations that are k iterations apart (e.g.,\nthe correlation between x(t) and x(t+k) for t = 1, . . . , l). accurate estimation of \u03c1(k)\npresentsitsownchallenges,butacommonapproachistotruncatethesummationwhen\n\u02c6\u03c1(k) < 0.1 [110]. then the effective sample size for an mcmc run with l iterations\nafter burn-in can be estimated using l/\u02c6\u03c4.\neffective sample size can be used to compare the efficiency of competing\nmcmc samplers for a given problem. for a fixed number of iterations, an mcmc\nalgorithm with a larger effective sample size is likely to converge more quickly. for\nexample, we may be interested in the gains achieved from blocking in a gibbs sam-\npler. if the blocked gibbs sampler has a much higher effective sample size than the\nunblocked version, this suggests that the blocking has improved the efficiency of the\nmcmcalgorithm.effectivesamplesizecanalsobeusedforasinglechain.forexam-\nple, consider a bayesian model with two parameters (\u03b1, \u03b2) and an mcmc algorithm\nrun for 10,000 iterations after burn-in. an effective sample size of, say, 9500 iterations\nfor \u03b1 suggests low correlations between iterations. in contrast, if the results indicated\n\n "}, {"Page_number": 235, "text": "225\nan effective sample size of 500 iterations for \u03b2, this would suggest that convergence\nfor \u03b2 is highly suspect.\n\n7.3 implementation\n\n7.3.1.6 number of chains one of the most difficult problems to diagnose\nis whether or not the chain has become stuck in one or more modes of the target\ndistribution. in this case, all convergence diagnostics may indicate that the chain has\nconverged, though the chain does not fully represent the target distribution. a partial\nsolution to this problem is to run multiple chains from diverse starting values and\nthen compare the within- and between-chain behavior. a formal approach for doing\nthis is described in section 7.3.1.2.\nthe general notion of running multiple chains to study between-chain perfor-\nmance is surprisingly contentious. one of the most vigorous debates during the early\nstatistical development of mcmc methods centered around whether it was more im-\nportant to invest limited computing time in lengthening the run of a single chain, or\nin running several shorter chains from diverse starting points to check performance\n[224, 233, 458]. the motivation for trying multiple runs is the hope that all interest-\ning features (e.g., modes) of the target distribution will be explored by at least one\nchain, and that the failure of individual chains to find such features or to wash out\nthe influence of their starting values can be detected, in which case chains must be\nlengthened or the problem reparameterized to encourage better mixing.\narguments for one long chain include the following. many short runs are more\ninformative than one long run only when they indicate poor convergence behavior. in\nthis case, the simulated values from the many short chains remain unusable. second,\nthe effectiveness of using many short runs to diagnose poor convergence is mainly\nlimited to unrealistically simple problems or problems where the features of f are\nalready well understood. third, splitting computing effort into many short runs may\nyield an indication of poor convergence that would not have occurred if the total\ncomputing effort had been devoted to one longer run.\nwe do not find the single-chain arguments entirely convincing from a practical\npoint of view. starting a number of shorter chains from diverse starting points is an\nessential component of thorough debugging of computer code. some primary fea-\ntures of f (e.g., multimodality, highly constrained support region) are often broadly\nknown\u2014evenincomplexrealisticproblems\u2014notwithstandinguncertaintyaboutspe-\ncific details of these features. results from diverse starts can also provide information\naboutkeyfeaturesof f,whichinturnhelpsdeterminewhetherthemcmcmethodand\nproblem parameterization are suitable. poor convergence of several short chains can\nhelp determine what aspects of chain performance will be most important to monitor\nwhen a longer run is made. finally, cpu cycles are more abundant and less expensive\nthan they were a decade ago. we can have diverse short runs and one longer run.\nexploratory work can be carried out using several shorter chains started from various\npoints covering the believed support of f. diagnosis of chain behavior can be made\nusing a variety of informal and formal techniques, using the techniques described in\nthis chapter. after building confidence that the implementation is a promising one, it\nis advisable to run one final very long run from a good starting point to calculate and\npublish results.\n\n "}, {"Page_number": 236, "text": "226\n\nchapter 7 markov chain monte carlo\n\n7.3.2 practical implementation advice\nthe discussion above raises the question of what values should be used for the number\nof chains, the number of iterations for burn-in, and the length of the chain after\nburn-in. most authors are reluctant to recommend generic values because appropriate\nchoices are highly dependent on the problem at hand and the rate and efficiency with\nwhich the chain explores the region supported by f. similarly, the choices are limited\nby how much computing time is available. published analyses have used burn-ins\nfrom zero to tens of thousands and chain lengths from the thousands to the millions.\ndiagnostics usually rely on at least three, and typically more, multiple chains. as\ncomputing power continues to grow, so too will the scope and intensity of mcmc\nefforts.\nin summary, we reiterate our advice from section 7.3.1.6 here, which in turn\nechoes [126]. first, create multiple trial runs of the chain from diverse starting values.\nnext, carry out a suite of diagnostic procedures like those discussed above to ensure\nthat the chain appears to be well mixing and has approximately converged to the\nstationary distribution. then, restart the chain for a final long run using a new seed to\ninitiate the sampling. a popular, though conservative, choice for burn-in is to throw\nout the first half of the mcmc iterations as the burn-in. when each mcmc iteration\nis computationally expensive, users typically select much shorter burn-in lengths that\nconserve more iterations for inference.\nfor learning about mcmc methods and chain behavior, nothing beats pro-\ngramming these algorithms from scratch. for easier implementation, various software\npackages have been developed to automate the development of mcmc algorithms\nand the related diagnostics. the most comprehensive software to date is the bugs\n(bayesian inference using gibbs sampling) software family with developments for\nseveral platforms [610]. a popular application mode is to use bugs within the r sta-\ntistical package [626]. packages in r like coda [511] and boa [607] allow users to\neasily construct the relevant convergence diagnostics. most of this software is freely\navailable via the internet.\n\nthe first\n\n7.3.3 using the results\nwe describe here some of the common summaries of mcmc algorithm output and\ncontinue the fur seal pup example for further illustration.\nif {x(t)} represents a\nis marginalization.\ntopic to consider\n(t)\np-dimensional markov chain, then {x\ni } is a markov chain whose limiting distri-\nbution is the ith marginal of f. if you are focused only on a property of this marginal,\ndiscard the rest of the simulation and analyze the realizations of x\n. further, note\nthat it is not necessary to run a chain for every quantity of interest. post hoc inference\nabout any quantity can be obtained from the realizations of x(t) generated by the\nchain. in particular, the probability for any event can be estimated by the frequency\nof that event in the chain.\nstandard one-number summary statistics such as means and variances are com-\nmonly desired (see section 7.1). the most commonly used estimator is based on an\n\n(t)\ni\n\n "}, {"Page_number": 237, "text": "227\nempirical average. discard the burn-in; then calculate the desired statistic by taking\n\n7.3 implementation\n\n1\nl\n\nd+l\u221216t=d\n\nh#x(t)$\n\n(7.25)\n\nas the estimator of e{h(x)}, where l denotes the length of each chain after discard-\ning d burn-in iterates. this estimator is consistent even though the x(t) are serially\ncorrelated. there are asymptotic arguments in favor of using no burn-in (so d = 1)\n[231]. however, since a finite number of iterations is used to compute the estimator\nin (7.25), most researchers employ a burn-in to reduce the influence of the initial\niterates sampled from a distribution that may be far from the target distribution. we\nrecommend using a burn-in period.\notherestimatorshavebeendeveloped.theriemannsumestimatorin(6.86)has\nbeen shown to have faster convergence than the standard estimator given above. other\nvariancereductiontechniquesdiscussedinsection6.4,suchasrao\u2013blackwellization,\ncan also be used to reduce the monte carlo variability of estimators based on the chain\noutput [507].\nthe monte carlo, or simulation, standard error of an estimator is also of in-\nterest. this is an estimate of the variability in the estimator if the mcmc algorithm\nwere to be run repeatedly. the naive estimate of the standard error for an estimator\nlike (7.25) is the sample standard deviation of the l realizations after burn-in divided\nby \u221al. however, mcmc realizations are typically positively correlated, so this can\nunderestimate the standard error. an obvious correction is to compute the standard er-\nror based on a systematic subsample of, say, every kth iterate after burn-in. however,\nthis approach is inefficient [429]. a simple estimator of the standard error is the batch\nmethod [92, 324]. separate the l iterates into batches with b consecutive iterations\nin each batch. compute the mean of each batch. then the estimated standard error\nis the standard deviation of these means divided by the square root of the number of\nbatches. a recommended batch size is b = \u230al1/a\u230b where a = 2 or 3 and \u230az\u230b denotes\nthe largest integer less than z [355]. other strategies to estimate monte carlo standard\nerrors are surveyed in [196, 233, 609]. the monte carlo standard error can be used\nto assess the between-simulation variability. it has been suggested that, after deter-\nmining that the chain has good mixing and convergence behavior, you should run the\nchain until the monte carlo simulation error is less than 5% of the standard deviation\nfor all parameters of interest [610].\nquantile estimates and other interval estimates are also commonly desired.\nestimates of various quantiles such as the median or the fifth percentile of h(x) can\nbe computed using the corresponding percentile of the realizations of the chain. this\nis simply implementing (7.25) for tail probabilities and inverting the relationship to\nfind the quantile.\nfor bayesian analyses, computation of the highest posterior density (hpd)\ninterval is often of interest (see section 1.5). for a unimodal and symmetric poste-\nrior distribution, the (1 \u2212 \u03b1)% hpd interval is given by the (\u03b1/2)th and (1 \u2212 \u03b1/2)th\npercentiles of the iterates. for a unimodal posterior distribution, an mcmc ap-\nproximation of the hpd interval can be computed as follows. for the parameter\nof interest, sort the mcmc realizations after burn-in, x(d), . . . , x(d+l\u22121) to obtain\n\n "}, {"Page_number": 238, "text": "chapter 7 markov chain monte carlo\n\nfor j = 1,2, . . . ,(l \u2212 1) \u2212 \u230a(1 \u2212 \u03b1)(l \u2212 1)\u230b\n\n228\nx(1) \u2264 x(2) \u00b7\u00b7\u00b7 \u2264 x(l\u22121). compute the 100(1 \u2212 \u03b1)% credible intervals\nij =%x(j), x(j+\u230a(1\u2212\u03b1)(l\u22121)\u230b)&\nwhere \u230az\u230b represents the largest integer not greater than z. the 100(1 \u2212 \u03b1)% hpd\ninterval is the interval ij\u22c6 with the shortest interval width among all credible intervals\n[107]. more sophisticated alternatives for hpd computation for multimodal posterior\ndensities and other complexities are given in [106].\nsimple graphical summaries of mcmc output should not be overlooked.\nhistograms of the realizations of h(x(t)) for any h of interest are standard prac-\ntice. alternatively, one can apply one of the density estimation techniques from\nchapter 10 to summarize the collection of values. it is also common practice to\ninvestigate pairwise scatterplots and other descriptive plots to explore or illustrate\nkey features of f.\nexample 7.10 (fur seal pup capture\u2013recapture study, continued) recall the\nfur seal pup capture\u2013recapture study in example 7.6 that led to the gibbs sampler\nsummarized in (7.13) and (7.14). a hybrid gibbs sampler for this problem is consid-\nered in example 7.7. when applied to the fur seal data these mcmc algorithms have\nvery different performance. we will consider these two variations to demonstrate the\nmcmc diagnostics described above.\nforthebasicgibbssamplerinexample7.6,thesamplepathandautocorrelation\nplots do not indicate any lack of convergence (figure 7.9). based on five runs of\n100,000 iterations each with a burn-in of 50,000, the gelman\u2013rubin statistic for n is\nequal to 0.999995, which suggests the n(t) chain is roughly stationary. the effective\nsample size was 45,206 samples (iterations). similarly, for the hybrid sampler in\nexample 7.7 there is no evidence of lack of convergence for n, so we will not consider\nthis parameter further.\nin contrast to the speedy convergence for n, mcmc convergence behavior for\nthe capture probability parameters (\u03b11, . . . , \u03b17) varies with the form of the model and\ngibbs sampling strategy. for the uniform/jeffreys prior combination and basic gibbs\nsampler in example 7.6, the gelman\u2013rubin statistic for the capture probabilities are\nall close to one, and the capture probabilities exhibit little correlation between mcmc\nsamples(e.g.,lowerrightpaneloffigure7.9).thissuggeststhatthechainsareroughly\nstationary. however, as we will show below, the alternative prior distributions and the\nhybrid gibbs sampler described in example 7.7 lead to less satisfactory mcmc\nconvergence behavior.\ntoimplementthehybridgibbssamplerforexample7.7,ametropolis\u2013hastings\nstep is required to sample (\u03b81, \u03b82) in (7.18). note that the prior distribution for these\nparameters restricts (\u03b81, \u03b82) to be larger than 0. such a constraint can impede mcmc\nperformance, particularly if there is high posterior density near the boundary. there-\nfore we consider using a random walk to update these parameters, but to improve\nperformance we transform (\u03b81, \u03b82) to u = (u1, u2) = (log \u03b81,log \u03b82). this permits\na random walk step on (\u2212\u221e,\u221e) to update u effectively. specifically, proposal val-\nues u\u2217 can be generated by drawing \u03f5 \u223c n(0,0.0852i) where i is the 2 \u00d7 2 identity\nmatrix and then setting u\u2217 = u(t) + \u03f5. we select a standard deviation of 0.085 to get\n\n "}, {"Page_number": 239, "text": "n\n\nf\nc\na\n\n100\n\n95\n\n90\n\n85\n\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n\n49,000\n\n49,500\n\nt\n\n50,000\n\n7.3 implementation\n\n229\n\n49,000\n\n49,500\n\nt\n\n50,000\n\n1\n\u03b1\n\nf\nc\na\n\n0.5\n\n0.4\n\n0.3\n\n0.2\n\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n\n0\n\n20\n\n10\nlag between iterations\n\n30\n\n40\n\n0\n\n20\n\n10\nlag between iterations\n\n30\n\n40\n\nfigure 7.9 output from the basic gibbs sampler from example 7.6. top row: sample paths\nfor last 1000 iterations of n (left) and \u03b11 (right). bottom row: autocorrelation plots after burn-in\nfor n (left) and \u03b11 (right).\n\nan acceptance rate of about 23% for the u updates. recalling (7.8) in example 7.3,\nit is necessary to transform (7.17) and (7.18) to reflect the change of variables. thus,\n(7.17) becomes\n\n(7.26)\n\n(7.27)\n\nfor i = 1, . . . ,7,\n\nand (7.18) becomes\n\n\u03b1i|\u00b7 \u223c beta(ci + exp{u1}, n \u2212 ci + exp{u2})\n\u0001(exp{u1})\u0001({exp{u2})17\nu1, u2|\u00b7 \u223cku exp{u1 + u2}0 \u0001(exp{u1} + exp{u2})\n(1 \u2212 \u03b1i)exp{u2} exp2\u2212\n\nexp{u1}\n\n\u00d7\n\n\u03b1\ni\n\n7-i=1\n\nexp{u1} + exp{u2}\n\n1000\n\n3 ,\n\nwhere ku isanunknownconstant.thismethodoftransformingtheparameterspacevia\na change-of-variables method within the metropolis\u2013hastings algorithm is useful for\nproblems with constrained parameter spaces. the idea is to transform the constrained\nparameters so that the mcmc updates can be made on \u211c. see [329] for a more\ncomplex example.\nwe implement the hybrid sampler running a chain of 100,000 iterations with\nthe first 50,000 iterations discarded for burn-in. the gelman\u2013rubin statistics for the\nparameters are all very close to 1, however, the autocorrelation plots indicate high\ncorrelation between iterations (left panel in figure 7.10). for example, using the al-\nternative prior distributions and the hybrid gibbs sampler produces correlations of\n\n "}, {"Page_number": 240, "text": "230\n\nchapter 7 markov chain monte carlo\n\nf\nc\na\n\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n\n0\n\n)\nt\n(\n\n1\nu\n\n7.0\n\n6.5\n\n6.0\n\n5.5\n\n5.0\n\n40\n\n20\n\n10\nlag between iterations\n\n30\n\n6.0\n\n5.5\nu (t)\n2\n\nfigure 7.10 for the hybrid gibbs sampler from example 7.7: autocorrelation function\nplot for p1 (left panel) and sample path for u (right panel) for final 5000 iterations in the seal\npup example.\n\n(t)\n1 and u\n\n0.6 between mcmc samples that are 40 iterations apart as compared to correlations\nnear 0 at lags of 2 when using the uniform/jeffreys prior combination and basic gibbs\nsampler shown in figure 7.9. similarly, the effective sample size for the hybrid gibbs\nalgorithm of 1127 can be compared to the much larger effective sample size of 45,206\nin the basic gibbs sampler discussed above. the right panel of figure 7.10 shows\n(t)\nthe bivariate sample path of u\n2 for the hybrid sampler. this plot indicates\na high correlation between the parameters. these results suggest a lack of conver-\ngence of the mcmc algorithm or at least poor behavior of the chain for the hybrid\nalgorithm. in spite of these indications of lack of convergence, the prior distributions\nfrom example 7.7 produce very similar results to those from the uniform/jeffreys\nprior combination for example 7.6. the posterior mean of n is 90 with a 95% hpd\ninterval of (85, 95). however, the chain does not mix as well as the simpler model,\nso we prefer the uniform/jeffreys prior for the seal pup data. a hybrid gibbs sampler\ncan be quite effective for many problems, but for these data the alternative prior de-\nscribed in example 7.7 is not appropriate and the hybrid algorithm does not remedy\nthe problem.\n!\n\nproblems\n7.1. the goal of this problem is to investigate the role of the proposal distribution in a\nmetropolis\u2013hastings algorithm designed to simulate from the posterior distribution of\na parameter \u03b4. in part (a), you are asked to simulate data from a distribution with \u03b4\nknown. for parts (b)\u2013(d), assume \u03b4 is unknown with a unif(0,1) prior distribution for\n\u03b4. for parts (b)\u2013(d), provide an appropriate plot and a table summarizing the output of\nthe algorithm. to facilitate comparisons, use the same number of iterations, random\nseed, starting values, and burn-in period for all implementations of the algorithm.\na. simulate 200 realizations from the mixture distribution in equation (7.6) with \u03b4 =\nb. implement an independence chain mcmc procedure to simulate from the posterior\n\n0.7. draw a histogram of these data.\n\ndistribution of \u03b4, using your data from part (a).\n\n "}, {"Page_number": 241, "text": "7.3 implementation\n\n231\n\nc. implement a random walk chain with \u03b4\u2217 = \u03b4(t) + \u03f5 with \u03f5 \u223cunif(\u22121,1).\nd. reparameterize the problem letting u = log{\u03b4/(1 \u2212 \u03b4)} and u\u2217 = u(t) + \u03f5. imple-\ne. compare the estimates and convergence behavior of the three algorithms.\n\nment a random walk chain in u-space as in equation (7.8).\n\n7.2. simulating from the mixture distribution in equation (7.6) is straightforward [see\npart (a) of problem 7.1]. however, using the metropolis\u2013hastings algorithm to simu-\nlate realizations from this distribution is useful for exploring the role of the proposal\ndistribution.\na. implement a metropolis\u2013hastings algorithm to simulate from equation (7.6) with\n\u03b4 = 0.7, using n(x(t),0.012) as the proposal distribution. for each of three starting\nvalues, x(0) = 0,7, and 15, run the chain for 10,000 iterations. plot the sample path\nof the output from each chain. if only one of the sample paths was available, what\nwould you conclude about the chain? for each of the simulations, create a histogram\nof the realizations with the true density superimposed on the histogram. based on\nyour output from all three chains, what can you say about the behavior of the chain?\nb. now change the proposal distribution to improve the convergence properties of the\n\nchain. using the new proposal distribution, repeat part (a).\n\nfor i = 1, . . . , n,7\u03c0 = (4/n)(n\n\n7.3. consider a disk d of radius 1 inscribed within a square of perimeter 8 centered at the\norigin. then the ratio of the area of the disk to that of the square is \u03c0/4. let f represent\nthe uniform distribution on the square. then for a sample of points (xi, yi) \u223c f(x, y)\ni=1 1{(xi,yi)\u2208d} is an estimator of \u03c0 (where 1{a} is 1 if a\nis true and 0 otherwise).\nconsider the following strategy for estimating \u03c0. start with (x(0), y(0)) = (0,0).\nthereafter, generate candidates as follows. first, generate \u03f5(t)\nx \u223cunif(\u2212h, h) and\ny \u223cunif(\u2212h, h). if (x(t) + \u03f5(t)\nx and\ny ) falls outside the square, regenerate \u03f5(t)\n\u03f5(t)\ny untilthesteptakenremainswithinthesquare.let(x(t+1), y(t+1)) = (x(t) + \u03f5(t)\n\u03f5(t)\nx , y(t) +\ny ). increment t. this generates a sample of points over the square. when t = n, stop\n\u03f5(t)\nand calculate7\u03c0 as given above.\na. implement this method for h = 1 and n = 20,000. compute7\u03c0. what is the effect\n\nb. explain why this method is flawed. using the same method to generate candidates,\ndevelop the correct approach by referring to the metropolis\u2013hastings ratio. prove\nthat your sampling approach has a stationary distribution that is uniform on the\nsquare.\n\nof increasing n? what is the effect of increasing and decreasing h? comment.\n\nx , y(t) + \u03f5(t)\n\nand h. comment.\n\nc. implement your approach from part (b) and calculate7\u03c0. experiment again with n\n\n7.4. derive the conditional distributions in equation (7.10) and the univariate conditional\n\ndistributions below equation (7.10).\n\n7.5. a clinical trial was conducted to determine whether a hormone treatment benefits\nwomen who were treated previously for breast cancer. each subject entered the clinical\ntrial when she had a recurrence. she was then treated by irradiation and assigned to\neitherahormonetherapygrouporacontrolgroup.theobservationofinterestisthetime\nuntil a second recurrence, which may be assumed to follow an exponential distribution\nwith parameter \u03c4\u03b8 (hormone therapy group) or \u03b8 (control group). many of the women\n\n "}, {"Page_number": 242, "text": "232\n\nchapter 7 markov chain monte carlo\n\ntable 7.2 breast cancer data.\n\nrecurrence\ntimes\n\ncensoring\ntimes\n\nhormone treated\n\ncontrol\n\n2\n13\n33\n\n10\n18\n23\n31\n40\n48\n55\n\n4\n14\n34\n\n14\n19\n24\n31\n41\n49\n56\n\n6\n18\n43\n\n14\n20\n29\n31\n42\n51\n\n9\n23\n\n16\n20\n29\n33\n42\n53\n\n9\n31\n\n17\n21\n30\n35\n44\n54\n\n9\n32\n\n18\n21\n30\n37\n46\n54\n\n1\n25\n\n1\n10\n17\n24\n29\n40\n47\n\n4\n35\n\n1\n11\n19\n25\n29\n41\n50\n\n6\n35\n\n3\n13\n20\n26\n32\n44\n50\n\n7\n39\n\n4\n14\n22\n26\n35\n45\n51\n\n13\n\n24\n\n5\n14\n24\n26\n38\n47\n\n8\n15\n24\n28\n39\n47\n\ndid not have a second recurrence before the clinical trial was concluded, so that their\nrecurrence times are censored.\nin table 7.2, a censoring time m means that a woman was observed for m\nmonths and did not have a recurrence during that time period, so that her recurrence\ntime is known to exceed m months. for example, 15 women who received the hormone\ntreatment suffered recurrences, and the total of their recurrence times is 280 months.\ni ) be the data for the ith person in the hormone group, where\nis a recurrence time and 0 if a censored time. the\n\ni = (xh\nis the time and \u03b4h\n\ni , \u03b4h\ni equals 1 if xh\n\nxh\ndata for the control group can be written similarly.\ni\n\nlet yh\n\ni\n\nthe likelihood is then\n\nl(\u03b8, \u03c4|y) \u221d \u03b8%( \u03b4c\n\ni +( \u03b4h\n\ni&\u03c4%( \u03b4h\n\ni& exp8\u2212\u03b86 xc\n\ni 9 .\ni \u2212 \u03c4\u03b86 xh\n\nyou\u2019ve been hired by the drug company to analyze their data. they want to know\nif the hormone treatment works, so the task is to find the marginal posterior distribution\nof \u03c4 using the gibbs sampler. in a bayesian analysis of these data, use the conjugate\nprior\n\nf(\u03b8, \u03c4) \u221d \u03b8a\u03c4b exp{\u2212c\u03b8 \u2212 d\u03b8\u03c4}.\n\nphysicians who have worked extensively with this hormone treatment have indicated\nthat reasonable values for the hyperparameters are (a, b, c, d) = (3, 1, 60, 120).\na. summarize and plot the data as appropriate.\nb. derive the conditional distributions necessary to implement the gibbs sampler.\nc. program and run your gibbs sampler. use a suite of convergence diagnostics to\n\nevaluate the convergence and mixing of your sampler. interpret the diagnostics.\n\nd. compute summary statistics of the estimated joint posterior distribution, includ-\ning marginal means, standard deviations, and 95% probability intervals for each\nparameter. make a table of these results.\n\ne. create a graph which shows the prior and estimated posterior distribution for \u03c4\n\nsuperimposed on the same scale.\n\n "}, {"Page_number": 243, "text": "7.3 implementation\n\n233\n\nf. interpret your results for the drug company. specifically, what does your estimate\nof \u03c4 mean for the clinical trial? are the recurrence times for the hormone group\nsignificantly different from those for the control group?\n\ng. a common criticism of bayesian analyses is that the results are highly dependent\non the priors. investigate this issue by repeating your gibbs sampler for values of\nthe hyperparameters that are half and double the original hyperparameter values.\nprovide a table of summary statistics to compare your results. this is called a\nsensitivity analysis. based on your results, what recommendations do you have\nfor the drug company regarding the sensitivity of your results to hyperparameter\nvalues?\n\n7.6. problem 6.4 introduces data on coal-mining disasters from 1851 to 1962. for these\n\ndata, assume the model\n\nxj \u223c2poisson(\u03bb1),\n\npoisson(\u03bb2),\n\nj = 1, . . . , \u03b8,\nj = \u03b8 + 1, . . . ,112.\n\n(7.28)\n\nassume \u03bbi|\u03b1 \u223c gamma(3, \u03b1) for i = 1,2, where \u03b1 \u223c gamma(10,10), and assume \u03b8\nfollows a discrete uniform distribution over {1, . . . ,111}. the goal of this problem is\nto estimate the posterior distribution of the model parameters via a gibbs sampler.\na. derive the conditional distributions necessary to carry out gibbs sampling for the\n\nchange-point model.\n\nb. implement the gibbs sampler. use a suite of convergence diagnostics to evaluate\n\nthe convergence and mixing of your sampler.\n\nc. construct density histograms and a table of summary statistics for the approximate\nposterior distributions of \u03b8, \u03bb1, and \u03bb2. are symmetric hpd intervals appropriate\nfor all of these parameters?\n\nd. interpret the results in the context of the problem.\n\n7.7. consider a hierarchical nested model\n\nyijk = \u00b5 + \u03b1i + \u03b2j(i) + \u03f5ijk,\n\n(7.29)\n\nwhere i = 1, . . . , i, j = 1, . . . , ji, and k = 1, . . . , k. after averaging over k for each\ni and j, we can rewrite the model (7.29) as\n\nyij = \u00b5 + \u03b1i + \u03b2j(i) + \u03f5ij,\n\ni = 1, . . . , i, j = 1, . . . , ji,\n\u03b1), \u03b2j(i) \u223c n(0, \u03c32\n\n(7.30)\n\nk=1 yijk/k. assume that \u03b1i \u223c n(0, \u03c32\n\n\u03f5), where each set of parameters is independent a priori. assume that \u03c32\n\nwhere yij =(k\n\u03b2), and \u03f5ij \u223c\nn(0, \u03c32\n\u03b2, and\n\u03f5 are known. to carry out bayesian inference for this model, assume an improper flat\n\u03c32\nprior for \u00b5, so f(\u00b5) \u221d 1. we consider two forms of the gibbs sampler for this problem\n[546]:\na. let n =(i ji , y\u00b7\u00b7 =(ij yij/n, and yi\u00b7 =(j yij/ji hereafter. show that at itera-\n\ntion t, the conditional distributions necessary to carry out gibbs sampling for this\n\n\u03b1, \u03c32\n\n "}, {"Page_number": 244, "text": "234\n\nchapter 7 markov chain monte carlo\n\nmodel are given by\n\n\u03f5\n\n1\n\n1\n\n1\n\n\u03c32\n\nji\u03b1\n\n\u03c32\n\n(t)\n\u03b2\nj(i),\n\n(t+1)\n\u03b1\ni\n\n(t)\ni \u2212\n\n,y& \u223cn#y\u00b7\u00b7 \u2212\n\u00b5(t+1)))%\u03b1(t), \u03b2(t)\n,y& \u223cn# jiv1\n))%\u00b5(t+1), \u03b2(t)\nj(i) ))%\u00b5(t+1), \u03b1(t+1),y& \u223cn. v2\n\u03b1/\u22121\n\nv1 =. ji\n\nn6i\n\u03f5 #yi\u00b7 \u2212 \u00b5(t+1) \u2212\n\u03f5 %yij \u2212 \u00b5(t+1) \u2212 \u03b1\nand v2 =. 1\n\n\u03f5 +\n\u03c32\n\nn$,\nn6j(i)\nj(i)$, v1$,\nji6j\n& , v2/ ,\n\u03b2/\u22121\n\n\u03f5 +\n\u03c32\n\n1\n\u03c32\n\n1\n\u03c32\n\n(t+1)\n\n(t+1)\n\n\u03c32\n\n(t)\n\n\u03b2\n\n.\n\ni\n\n\u03b2\n\nwhere\n\nb. the convergence rate for a gibbs sampler can sometimes be improved via repa-\nrameterization. for this model, the model can be reparameterized via hierarchical\ncentering (section 7.3.1.4). for example, let yij follow (7.30), but now let \u03b7ij =\n\u00b5 + \u03b1i + \u03b2j(i) and \u03f5ij \u223c n%0, \u03c32\n\u03f5&. then let \u03b3i = \u00b5 + \u03b1i with \u03b7ij|\u03b3i \u223c n%\u03b3i, \u03c32\n\u03b2&\nand \u03b3i|\u00b5 \u223c n%\u00b5, \u03c32\n\u03f5 are known, and assume a\nflat prior for \u00b5. show that the conditional distributions necessary to carry out gibbs\nsampling for this model are given by\n\n\u03b2, and \u03c32\n\n\u03b1, \u03c32\n\n(t)\n\u03b3\ni ,\n\n\u03b1&. as above, assume \u03c32\n\u00b5(t+1)))%\u03b3(t), \u03b7(t),y& \u223cn#1\ni6i\n))%\u00b5(t+1), \u03b7(t),y& \u223cn#v3# 1\n\u03b26j\n))%\u00b5(t+1), \u03b3(t+1),y& \u223cn.v2. yij\n\u03b1/\u22121\n\nv3 =. ji\n\n\u03f5 +\n\u03c32\n\n\u03b2 +\n\u03c32\n\n1\n\u03c32\n\n\u03c32\n\n.\n\n(t+1)\n\n\u03b3\n\ni\n\n(t+1)\n\u03b7\nij\n\nwhere\n\n1\ni\n\n\u03c32\n\n\u03b1$,\n\n(t)\nij +\n\u03b7\n(t+1)\n\u03c32\n\n\u00b5(t+1)\n\u03c32\n\n\u03b1 $, v3$,\n\u03b2 / , v2/ ,\n\n\u03b3\n\ni\n\n7.8.\n\nin problem 7.7, you were asked to derive gibbs samplers under two model parameter-\nizations. the goal of this problem is to compare the performance of the samplers.\nthe website for this book provides a dataset on the moisture content in the\nmanufacture of pigment paste [58]. batches of the pigment were produced, and the\nmoisture content of each batch was tested analytically. consider data from 15 randomly\nselected batches of pigment. for each batch, two independent samples were randomly\nselected and each of these samples was measured twice. for the analyses below, let\n\u03b1 = 86, \u03c32\n\u03c32\nimplement the two gibbs samplers described below. to facilitate comparisons\nbetween the samplers, use the same number of iterations, random seed, starting values,\nand burn-in period for both implementations.\na. analyze these data by applying the gibbs sampler from part (a) of problem 7.7.\nimplementthesamplerinblocks.forexample, \u03b1 = (\u03b11, . . . , \u03b115)isoneblockwhere\n\n\u03b2 = 58, and \u03c32\n\n\u03f5 = 1.\n\n "}, {"Page_number": 245, "text": "7.3 implementation\n\n235\n\nallparameterscanbeupdatedsimultaneouslybecausetheirconditionaldistributions\nare independent. update the blocks using a deterministic order within each cycle.\nfor example, generate \u00b5(0), \u03b1(0), \u03b2(0) in sequence, followed by \u00b5(1), \u03b1(1), \u03b2(1), and\nso on.\n\nb. analyze these data by applying the gibbs sampler from part (b) of problem 7.7.\nimplement the sampler and update the blocks using a deterministic order within\neach cycle, updating \u00b5(0), \u03b3(0), \u03b7(0) in sequence, followed by \u00b5(1), \u03b3(1), \u03b7(1), and\nso on.\n\nc. compare performance of the two algorithms by constructing the following diag-\n\nnostics for each of the above implementations.\ni. afterdeletingtheburn-initerations,computethepairwisecorrelationsbetween\n\nall parameters.\n\nii. select several of the parameters in each implementation, and construct an\n\nautocorrelation plot for each parameter.\n\niii. compare the effective sample size for several parameters for the two imple-\n\nmentations of the algorithm.\n\nyou may also wish to explore other diagnostics to facilitate these comparisons. for\nthis problem, do you recommend the standard or the reparameterized model?\n\n7.9. example 7.10 describes the random walk implementation of the hybrid gibbs sam-\npler for the fur seal pup capture\u2013recapture study. derive the conditional distributions\nrequired for the gibbs sampler, equations (7.26) and (7.27).\n\n "}, {"Page_number": 246, "text": "chapter 8\nadvanced topics in mcmc\n\nthe theory and practice of markov chain monte carlo continues to advance at a\nrapid pace. two particularly notable innovations are the dimension shifting reversible\njump mcmc method and approaches for adapting proposal distributions while the\nalgorithm is running. also, applications for bayesian inference continue to be of\nbroad interest. in this chapter we survey a variety of higher level mcmc methods\nand explore some of the possible uses of mcmc to solve challenging statistical\nproblems.\nsections 8.1\u20138.5 introduce a wide variety of advanced mcmc topics, includ-\ning adaptive, reversible jump, and auxiliary variable mcmc, additional metropolis\u2013\nhasting methods, and perfect sampling methods. in section 8.6 we discuss an ap-\nplication of mcmc to maximum likelihood estimation. we conclude the chapter in\nsection 8.7 with an example where several of these methods are applied to facilitate\nbayesian inference for spatial or image data.\n\n8.1 adaptive mcmc\none challenge with mcmc algorithms is that they often require tuning to improve\nconvergence behavior. for example, in a metropolis\u2013hastings algorithm with a nor-\nmally distributed proposal distribution, some trial and error is usually required to tune\nthe variance of the proposal distribution to achieve an optimal acceptance rate (see\nsection 7.3.1.3). tuning the proposal distribution becomes even more challenging\nwhen the number of parameters is large. adaptive mcmc (amcmc) algorithms\nallow for automatic tuning of the proposal distribution as iterations progress.\nmarkov chain monte carlo algorithms that are adaptive have been considered\nfor some time but formidable theory was typically required to prove stationarity of\nthe resulting markov chains. more recently, the development of simplified criteria for\nconfirming theoretical convergence of proposed algorithms has led to an explosion of\nnew adaptive mcmc algorithms [12, 16, 550]. before describing these algorithms it\nis imperative to stress that care must be taken when developing and applying adaptive\nalgorithmstoensurethatthechainproducedbythealgorithmhasthecorrectstationary\ndistribution. without such care, the adaptive algorithm will not produce a markov\nchain because the entire path up to present time will be required to determine the\npresent state. another risk of adaptive algorithms is that they may depend too heavily\n\ncomputational statistics, second edition. geof h. givens and jennifer a. hoeting.\n\u00a9 2013 john wiley & sons, inc. published 2013 by john wiley & sons, inc.\n\n237\n\n "}, {"Page_number": 247, "text": "chapter 8 advanced topics in mcmc\n\n238\non previous iterations, thus impeding the algorithm from fully exploring the state\nspace. the best adaptive mcmc algorithms solve these problems by progressively\nreducing the amount of tuning as the number of iterations increases.\nan mcmc algorithm with adaptive proposals is ergodic with respect to the\ntarget stationary distribution if it satisfies two conditions: diminishing adaptation and\nbounded convergence. informally, diminishing (or vanishing) adaptation says that\nas t \u2192 \u221e, the parameters in the proposal distribution will depend less and less on\nearlier states of the chain. the diminishing adaptation condition can be met either\nby modifying the parameters in the proposal distribution by smaller amounts or by\nmaking the adaptations less frequently as t increases.the bounded convergence (con-\ntainment) condition considers the time until near convergence. let d(t) denote the\ntotal variation distance between the stationary distribution of the transition kernel\nemployed by the amcmc algorithm at time t and the target stationary distribu-\ntion. (the total variation distance can be informally described as the largest possible\ndistance between two probability distributions.) let m(t)(\u03f5) be the smallest t such\nthat d(t) < \u03f5. the bounded convergence condition states that the stochastic process\nm(t)(\u03f5) is bounded in probability for any \u03f5 > 0. the technical specifications of the\ndiminishing adaptation and the bounded convergence conditions are beyond the scope\nof this book; see [550] for further discussion. however, in practice these conditions\nlead to simpler, verifiable conditions that are sufficient to guarantee ergodicity of the\nresultingchainwithrespecttothetargetstationarydistributionandareeasiertocheck.\nwe describe these conditions for applications of specific amcmc algorithms in the\nsections below.\n\nunivariate conditional density for the ith element of x =!x1, . . . , xp\" is not avail-\n\n8.1.1 adaptive random walk metropolis-within-gibbs\nalgorithm\nthe method discussed in this section is a special case of the algorithm in section 8.1.3,\nbut we prefer to begin here at a simpler level. consider a gibbs sampler where the\nable in closed form. in this case, we might use a random walk metropolis algorithm\nto simulate draws from the ith univariate conditional density (section 7.1.2). the\ngoal of the amcmc algorithm is to tune the variance of the proposal distribution\nso that the acceptance rate is optimal (i.e., the variance is neither too large nor too\nsmall). while many variants of the adaptive metropolis-within-gibbs algorithm are\npossible, we first consider an adaptive normal random walk metropolis\u2013hastings\nalgorithm [551].\nin the algorithm below, the adaptation step is performed only at specific times,\nfor example, iterations t \u2208 {50,100,150, . . .}. we denote these as batch times tb\nwhere b = 0,1, . . ., the proposal variance is first tuned at iteration t1 = 50 and the\nnext at iteration t2 = 100. the proposal distribution variance \u03c32\nb will be changed at\nthese times. performing the adaptation step every 50 iterations is a common choice;\nother updating intervals are reasonable depending on the total number of mcmc\niterations for a particular problem and the mixing performance of the chain.\nwe present the adaptive random walk metropolis-within-gibbs algorithm as if\nthe parameters were arranged so that the univariate conditional density for the first\n\n "}, {"Page_number": 248, "text": "8.1 adaptive mcmc\n\n239\nelement of x is not available in closed form. an adaptive metropolis-within-gibbs\nupdate is used for this element. we assume that the univariate conditional densities\nfor the remaining elements of x permit standard gibbs updates. the adaptive random\nwalk metropolis-within-gibbs proceeds as follows.\n1. initialization: select starting values x(0) = x(0) and set t = 0. select a batching\n(t)\n2. metropolis-within-gibbsupdate:update x\n1 usingarandomwalkupdateusing\n\nschedule {tb} for b = 0,1,2, . . . and set a batch index b = 0. let \u03c32\nthe following steps:\na. generate x\u22171 by drawing \u03f5 \u223c n(0, \u03c32\nb. compute the metropolis\u2013hasting ratio\n1 , x\u22171$ =\n\nb) and then set x\u22171 = x\n\n(t)\n1 + \u03f5.\n\n0 = 1.\n\n(8.1)\n\n(t)\n\n(t)\n\nr#x\n\nf!x\u22171\"\n1 $ .\nf#x\nx\u22171 with probability min(r#x\n\naccording to the following:\n\n(t+1)\n1\n\nc. sample a value for x\n\n3. gibbs updates: since closed-form univariate conditional densities are available\n\n1 , x\u22171$ ,1) ,\n\nx\n\nx\n\n, x\n\n(t)\n\n(t)\n1\nx\n\n(t+1)\n1\n\notherwise.\n\n=\u23a7\u23a8\u23a9\n**\u00b7 \u223c f#x2**x\n**\u00b7 \u223c f#x3**x\n...\np\u22121**\u00b7 \u223c f#xp\u22121**x\n**\u00b7 \u223c f#xp**x\n4. adaptation step: when t = tb+1,\na. update the variance of the proposal distribution\n\nfor i = 2, . . . , p, use gibbs updates as follows:\ngenerate, in turn,\np$ ,\n(t+1)\n(t)\n3 , . . . , x(t)\n2\np$ ,\n(t+1)\n(t)\n(t+1)\n4 . . . , x(t)\n3\n2\nx\n, x\np$ ,\n(t+1)\np\u22122 , x(t)\n, . . . , x\np\u22121$ ,\n(t+1)\n\n(t+1)\n2\n, x\n(t+1)\n2\n, x\n\n(t+1)\n1\n(t+1)\n1\n\n(t+1)\nx\nx(t+1)\n\n(t+1)\n1\n(t+1)\n1\n\n, . . . , x\n\n, x\n\np\n\nwhere |\u00b7 denotes conditioning on the most recent updates to all other elements\nof x.\n\nlog(\u03c3b+1) = log(\u03c3b) \u00b1 \u03b4(b + 1),\n\nwheretheadaptationfactor \u03b4(b + 1)isaddedwhenthemetropolis\u2013hastings\nacceptance rate in step 2(c) is smaller than 0.44 for the iterations in the pre-\nvious batch and subtracted otherwise. a common choice for the adaptation\n\n "}, {"Page_number": 249, "text": "240\n\nchapter 8 advanced topics in mcmc\n\nfactor is \u03b4(b + 1) = min(0.01,1/\u221atb), where 0.01 is an arbitrary constant\nthat initially limits the magnitude of adaptation.\n\nb. increment the batch index b = b + 1.\n5. increment t and return to step 2.\nin the adaptation step, the variance of the proposal distribution is usually tuned\nwith the goal of obtaining a proposal acceptance rate of about 0.44 (so, 44% of pro-\nposals are accepted) when x1 is univariate. this rate has been shown to be optimal for\nunivariate normally distributed target and proposal distributions (see section 7.3.1.3).\nas for any amcmc implementation, we need to check the convergence crite-\nria. for the metropolis-within-gibbs algorithm, the diminishing adaptation condition\nis satisfied when \u03b4(b) \u2192 0 as b \u2192 \u221e. the bounded convergence condition is satis-\nfied if log(\u03c3b) \u2208 [\u2212m, m] where m < \u221e is some finite bound. less stringent\u2014but\nperhaps less intuitive\u2014requirements also satisfy the bounded convergence condition;\nsee [554].\nthe adaptive algorithm above can be generalized to other random walk distri-\nbutions. generally, in step 2(a), generate x\u22171 by drawing \u03f5 \u223c h(\u03f5) for some density h.\nwith this change, note that in step 2(b), the metropolis\u2013hastings ratio will need to\nbe adapted to include the proposal distribution as in (7.1) if h is not symmetric.\nthe adaptive metropolis-within-gibbs algorithm is particularly useful when\nthere are many parameters, each with its own variance to be tuned. for example,\namcmc methods have been used successfully for genetic data where the number\nof parameters can grow very quickly [637]. in that case, the algorithm above will\nneed to be modified so that each element of x will have its own adaptation step and\nadaptive variance. we demonstrate a similar situation in example 8.1. alternatively,\nthe proposal variance can be adapted jointly by accounting for the var{x}. this is\ndiscussed further in section 8.1.3.\n\n8.1.2 general adaptive metropolis-within-gibbs algorithm\nthe adaptive random walk metropolis-within-gibbs algorithm is a modification of\nthe random walk algorithm (section 7.1.2). other adaptive forms of the metropolis-\nwithin-gibbs algorithm can be applied as long as the diminishing adaptation and the\nbounded convergence conditions are met. in the example below we develop one such\nalgorithm for a realistic example.\n\nexample 8.1 (whale population dynamics)\npopulation dynamics models de-\nscribe changes in animal abundance over time. natural mortality, reproduction, and\nhuman-based removals (e.g., catch) usually drive abundance trends. another impor-\ntant concept in many such models is carrying capacity, which represents the num-\nber of animals that can be sustained in equilibrium with the amount of resources\navailable within the limited range inhabited by the population. as animal abundance\nincreases toward (and potentially beyond) carrying capacity, there is greater compe-\ntition for limited resources, which reduces net population growth or even reverses\nit when abundance exceeds carrying capacity. this dependence of the population\n\n "}, {"Page_number": 250, "text": "241\ngrowth rate on how near the current abundance is to carrying capacity is called density\ndependence.\n\na simple discrete-time density-dependent population dynamics model is\n\n8.1 adaptive mcmc\n\nny+1 = ny \u2212 cy + rny+1 \u2212, ny\n\nk-2.\n\n(8.2)\n\nwhere ny and cy, respectively, represent abundance and catch in year y, r denotes\nthe intrinsic growth rate, which encompasses both productivity and natural mortality,\nand k represents carrying capacity. this model is known as the pella\u2013tomlinson\nmodel [504].\nin application, this model should not be taken too literally. abundances may\nbe rounded to integers, but allowing fractional animals is also reasonable. carrying\ncapacity can be considered an abstraction, allowing the model to exhibit density-\ndependent dynamics rather than imposing an abrupt and absolute ceiling for abun-\ndance or allowing unlimited growth. our implementation of (8.2) assumes that\nabundance is measured on the first day of the year and whales are harvested on the\nlast day.\nconsider estimating the parameters in model (8.2) when one knows cy in every\nyearandhasobservedestimatesof ny foratleastsomeyearsoverthemodelingperiod.\nwhen the population is believed to have been in equilibrium before the modeling\nperiod, it is natural to assume n0 = k, and we do so hereafter. in this case, the model\ncontains two parameters: k and r.\nlet the observed estimates of ny be denoted /ny. for whales, abundance sur-\nveys are logistically challenging and usually require considerable expenditures of\ntime, effort, and money, so /ny may be obtained only rarely. thus, in this example\nbased on artificial data there are only six observed abundance estimates, denoted\n/n = {/n1, . . ./n6}.\nthe website for this book provides catch data for 101 years, along with survey\nabundance estimates /ny for y \u2208 {14,21,63,93,100}. each abundance estimate in-\ncludes an estimated coefficient of variation/\u03c8y. conditionally on the/\u03c8y, let us assume\nthat each abundance estimate /ny is lognormally distributed as follows:\n(8.3)\nwhere/\u03c32\ny}. figure 8.1 shows the available data and the estimated pop-\nulation trajectory using the maximum a posteriori estimate of r and k discussed later.\nfor the purposes of this example, let us assume that/\u03c8y = \u03c8 for all y and incorporate\n\u03c8 as a third parameter in the model. the overall likelihood is written l(k, r, \u03c8|/n).\n\nthis setup conceals a serious challenge for analysis that is induced by two\naspects of the data. first, the catches were huge in a brief early period, causing\nsevere population decline, with subsequent small catches allowing substantial re-\ncovery. second, most of the available abundance estimates correspond either to the\nnear present or to years that are near the population nadir that occurred many years\nago. together, these facts require that any population trajectory reasonably consistent\nwith the observed data must \u201cthread the needle\u201d by using parameter values for which\nthe trajectory passes through appropriately small abundances in the distant past and\n\nlog{/ny} \u223c n(log{ny},/\u03c32\n\ny = log{1 +/\u03c82\n\ny)\n\n "}, {"Page_number": 251, "text": "242\n\nchapter 8 advanced topics in mcmc\n\ne\nc\nn\na\nd\nn\nu\nb\na\nd\ne\nt\na\nm\n\n \n\ni\nt\ns\ne\n\n12000\n\n9000\n\n6000\n\n3000\n\n0\n\n1\n\n20\n\n40\n\n60\n\n80\n\n100\n\nyear\n\nfigure 8.1 six abundance estimates and the maximum a posteriori population trajectory\nestimate for the whale dynamics model in example 8.1.\n\nrecovers to observed levels in the present. this situation forces a strong nonlinear\ndependence between k and r: for any k only a very narrow range of r values can\nproduce acceptable population trajectories, especially when k is at the lower end of\nits feasible values.\nwe adopt a bayesian approach for estimating the model parameters using the\nindependent priors\n\nk \u223c unif(7000, 100000),\nr \u223c unif(0.001, 0.1),\n\u03c8/2 \u223c beta(2, 10).\n\nthese choices are based on research for other whale species and basic biological\nlimitations such as gestation and reproduction limits. denote the resultant joint prior\ndistribution as p(k, r, \u03c8).\nfor posterior inference we will use a hybrid gibbs approach (section 7.2.5)\nsince the univariate conditional distributions are not available in closed form. we\nupdate each parameter using a metropolis\u2013hastings update.\nlet gibbs cycles be indexed by t. the proposals for each parameter at cycle\nt + 1 are taken to be random markov steps from their previous values. specifically,\n\nk\u2217 = k(t)\nr\u2217 = r(t)\n\u03c8\u2217 = \u03c8(t)\n\n(t+1)\n+ \u03f5\nk ,\n+ \u03f5(t+1)\n,\n(t+1)\n+ \u03f5\n\n\u03c8\n\n.\n\nr\n\nthe proposal distributions for the parameters, which we will denote gk, gr, and\ng\u03c8, are determined by the conditional distributions of \u03f5\n\n(t+1)\n\nk *** k(t), \u03f5(t+1)\n\nr\n\n**r(t) , and\n\n "}, {"Page_number": 252, "text": "8.1 adaptive mcmc\n\n243\n\n(8.4)\n(8.5)\n(8.6)\n\nwhere the support regions of these densities are\n\n(t+1)\n\u03f5\n\u03c8\nwe use\n\n**\u03c8(t). we denote those distributions as g\u03f5k, g\u03f5r, and g\u03f5\u03c8. for this example,\n\n(t+1)\n\nk\n\n(t+1)\n\n\u03c8\n\n\u2208 sk$ ,\n\u2208 s\u03c8$ ,\n\nr\n\nr\n\n\u03c8\n\n\u03c8\n\n(t+1)\n\n(t+1)\n\n(t+1)\n\n(t+1)\n\nk *** k(t)$ \u221d \u03c6#\u03f5\ng\u03f5k# \u03f5\nk ;0,200$ i#\u03f5\n*** r(t)$ \u221d i#\u03f5(t+1)\ng\u03f5r# \u03f5(t+1)\n\u2208 sr$ ,\n*** \u03c8(t)$ \u221d \u03c6#\u03f5\ng\u03f5\u03c8# \u03f5\n;0,0.1$ i#\u03f5\nk \u2264 100000) ,\n(t+1)\n: 7000 \u2264 k(t)\n+ \u03f5\n: max{0.001, r(t)\n+ \u03f5(t+1)\n\u2212 0.03} \u2264 r(t)\n\u2264 2) .\n(t+1)\n: 0 < \u03c8(t)\n+ \u03f5\n\n\u03c8\n\nr\n\nr\n\n\u03c8\n\nk\n\n(t+1)\n\n(t+1)\n\n\u2264 min{0.1, r(t)\n\nsk =(\u03f5\n+ 0.03}) ,\nsr =(\u03f5(t+1)\ns\u03c8 =(\u03f5\nlastly, i(z \u2208 z) = 1 if z \u2208 z and zero otherwise, and \u03c6(z; a, b) represents the normal\ndistributiondensityfor z withmean aandstandarddeviation b.notethat g\u03f5r issimply\na uniform distribution over sr.\nthese proposal distributions (8.4)\u2013(8.6) are sufficient to specify gk, gr and g\u03c8.\nnote that proposals are not symmetric in the sense that the probability density for\nproposing \u03b8\u2217 from \u03b8(t) is not the same as for proposing \u03b8(t) from \u03b8\u2217 for \u03b8 \u2208 {k, r, \u03c8}.\nthis fact holds because in each case the truncation of the distribution of the pro-\nposed increment depends on the previous parameter value. hence when calculat-\ning transition probabilities one cannot ignore the direction of transition. moreover,\nthe setup described above is not a random walk because the markov increments in\n(8.4)\u2013(8.6) are not independent of the parameter values at the previous time.\nbefore we consider an adaptive mcmc approach, let us review a standard\nimplementation. at iteration t, a nonadaptive mcmc algorithm to sample from the\nposterior is given by:\n\n1. define g\u03f5k, g\u03f5r, and g\u03f5\u03c8 as (8.4), (8.5), and (8.6). this step, which doesn\u2019t de-\npend on t, is included here nevertheless because when we switch to an adaptive\nmethod these definitions will change at each iteration.\n\n2. sample from the increment distributions. this requires sampling \u03f5\n\n\u03f5(t+1)\n\nr\n\n(t+1)\n\n\u03c8\n\n***r(t), and \u03f5\n\n***\u03c8(t) from the distributions specified in step 1.\n\n3. generate k(t+1) as follows:\na. propose k\u2217 = k(t) + \u03f5\nb. calculate\n\n(t+1)\nk .\n\n(t+1)\n\nk ***k(t),\n\nrk =\n\nl#k\u2217, r(t), \u03c8(t)***/n$p#k\u2217, r(t), \u03c8(t)$g\u03f5k#\u03f5\nl#k(t), r(t), \u03c8(t)***/n$p#k(t), r(t), \u03c8(t)$g\u03f5k# \u2212 \u03f5\n\nk ***k(t)$\nk ***k\u2217$ .\n\n(t+1)\n\n(t+1)\n\n "}, {"Page_number": 253, "text": "244\n\nchapter 8 advanced topics in mcmc\n\nc. set k(t+1) = k\u2217 with probability equal to min{1, rk}. otherwise k(t+1) =\n\nk(t).\n\n4. generate r(t+1) as follows:\na. propose r\u2217 = r(t) + \u03f5(t+1)\nb. calculate\n\nr\n\n.\n\nrr =\n\nl#k(t+1), r\u2217, \u03c8(t)***/n$p#k(t+1), r\u2217, \u03c8(t)$g\u03f5r#\u03f5(t+1)\nl#k(t+1), r(t), \u03c8(t)***/n$p#k(t+1), r(t), \u03c8(t)$g\u03f5r# \u2212 \u03f5\n\nr\n\nr\n\n***r(t)$\n***r\u2217$ .\n\nc. set r(t+1) = r\u2217 with probability equal to min{1, rr}. otherwise r(t+1) = r(t).\n\n(t+1)\n\n5. generate \u03c8(t+1) as follows:\na. propose \u03c8\u2217 = \u03c8(t) + \u03f5\nb. calculate\n\n\u03c8\n\n(t+1)\n\n.\n\nr\u03c8 =\n\nl#k(t+1), r(t+1), \u03c8\u2217***/n$p#k(t+1), r(t+1), \u03c8\u2217$g\u03f5\u03c8#\u03f5\nl#k(t+1), r(t+1), \u03c8(t)***/n$p#k(t+1), r(t+1), \u03c8(t)$g\u03f5\u03c8# \u2212 \u03f5\n\nc. set \u03c8(t+1) = \u03c8\u2217 with probability equal\n\n\u03c8\n\n(t+1)\n\n(t+1)\n\n\u03c8\n\n***\u03c8(t)$\n***\u03c8\u2217$ .\n\nto min{1, r\u03c8}. otherwise\n\n\u03c8(t+1) = \u03c8(t).\n\n6. increment t and return to step 1.\n\napplying this algorithm for a chain length of 45,000 with a burn-in of 10,000 shows\nthat the mixing properties of this chain are poor. for example, after burn-in the pro-\nposal acceptance rates are 81, 27, and 68% for k, r, and \u03c8, respectively. it would be\nmore desirable to achieve acceptance rates near 44% (see section 7.3.1.3).\nnow we try to improve mcmc performance using an adaptive approach. re-\ndefine the proposal distributions from (8.4) \u2013 (8.6) as follows:\n\n(t+1)\nk ;0,200\u03b4\n(t+1)\n\u2208 s\n;0,0.1\u03b4\n\n(t+1)\n\nr\n\nr\n\nr\n\n\u03c8\n\n\u03f5r\n\n(t+1)\n\n(t+1)\n\ng(t+1)\ng(t+1)\ng(t+1)\n\n\u03f5k #\u03f5\n\u03f5\u03c8 #\u03f5\n=(\u03f5(t+1)\n\nk ***k(t)$ \u221d \u03c6#\u03f5\n***r(t)$ \u221d i#\u03f5(t+1)\n#\u03f5(t+1)\n***\u03c8(t)$ \u221d \u03c6#\u03f5\n: max(0.001, r(t)\n\u2264 min(0.1, r(t)\n\n\u03c8\n\nr\n\n\u03c8\n\nk\n\n(t+1)\n\n(t+1)\n\n(t+1)\n\n(t+1)\n\nk $i#\u03f5\n$,\n$i#\u03f5\n) \u2264 r(t)\n\u2212 0.03\u03b4(t+1)\n)) .\n\n\u03c8\n\nr\n\n+ 0.03\u03b4(t+1)\n\nr\n\n(t+1)\ns\n\nr\n\n\u2208 sk$,\n\u2208 s\u03c8$,\n+ \u03f5(t+1)\n\nr\n\n(8.7)\n(8.8)\n(8.9)\n\n(t+1)\nk , \u03b4(t+1)\n\nhere, \u03b4\nare adaptation factors that vary as t increases. thus,\nthese equations allow the standard deviations of the normally distributed increments\nand the range of the uniformly distributed increment to decrease or increase over time.\n\n, and \u03b4\n\n(t+1)\n\n\u03c8\n\nr\n\nwhere\n\n "}, {"Page_number": 254, "text": "8.1 adaptive mcmc\n\n245\n\nr\n\nr\n\nr\n\nr\n\n\u03c8\n\n\u03c8\n\n\u03c8\n\n\u03c8\n\nu\n\nu\n\n(t)\n\n(t)\n\n(t+1)\n\n(t+1)\n\n(t+1)\n\n(t+1)\n\n(8.11)\n\n(8.10)\n\n, and u\n\nevery 1500th iteration. the\n\nfor this example, we adjust \u03b4\n\nlog#\u03b4\nlog#\u03b4(t+1)\nlog#\u03b4\n\n(t+1)\nk(t + 1)1/3 ,\nu(t+1)\n(t + 1)1/3 ,\n(t + 1)1/3 ,\n\n, and \u03b4\nexpressions for rescaling the adaptation factors are\n\n(t+1)\nk , \u03b4(t+1)\nk $ = log#\u03b4\nk$ +\n$ = log#\u03b4(t)\nr $ +\n\u03c8$ +\n$ = log#\u03b4\n(t+1)\n(8.12)\n(t+1)\nk , u(t+1)\nwhere u\nare explained below. thus by controlling {uk, ur, u\u03c8}\nwe control how the proposal distributions adapt.\neach adaptation depends on an acceptance rate, that is, the percentage of it-\nerations within a specified period for which the proposal is accepted. at a specific\nadaptation step ta we set u\n= \u22121 if the acceptance rate for \u03b8 during the previous\n= 1 otherwise, separately for the three\n1500 iterations was less than 44% and u\nparameters indexed as \u03b8 \u2208 {k, r, \u03c8}. thus before generating the (ta + 1)th proposals\nwe observe the separate acceptance rates arising from steps 3c, 4c, and 5c. above dur-\ning {t : ta \u2212 1500 < t \u2264 ta}. then u\nare individually set to\nreflect the algorithm performance for each of the parameters during that time period.\nthe u values may have different signs at any adaptation step, so the multiplicative\nfactors \u03b4\nwill increase and decrease separately as simulations\nprogress.\nusingthisapproach,theadaptivemcmcalgorithmforthisexamplewillfollow\nthe same six steps as above for advancing from t to t + 1 except that step 1 is replaced\nwith\n1. if t \u2208 {1500, 3000, . . .42000}, then\na. calculate the acceptance rates in the most recent 1500 iterations for each\n\n, u(ta+1)\n\n, \u03b4(ta+1)\n\n, and u\n\n, and \u03b4\n\n(ta+1)\n\n(ta+1)\n\n(ta+1)\n\n(ta+1)\n\n(ta+1)\n\n(ta+1)\n\nk\n\nk\n\n\u03c8\n\n\u03c8\n\n\u03b8\n\n\u03b8\n\nr\n\nr\n\nparameter separately and determine u\n\n(t+1)\nk , u(t+1)\nas in (8.10)\u2013(8.12).\n\nr\n\n, and u\n\n(t+1)\n\n\u03c8\n\n.\n\nr\n\n\u03c8\n\n\u03f5r\n\n\u03f5r\n\n\u03f5\u03c8\n\n\u03f5\u03c8\n\n\u03f5k\n\n\u03f5k\n\n, and \u03b4\n(t+1)\n, and g\n\nremain unchanged from the previous\n\n(t+1)\nbe updated as in (8.7)\u2013(8.9).\n(t+1)\n\n(t+1)\nk , \u03b4(t+1)\nb. update \u03b4\n(t+1)\n(t+1)\nc. let g\n, g\n, and g\n(t+1)\n(t+1)\n, g\notherwise, g\niteration.\nthe diminishing adaptation condition holds since u(t+1)/(t + 1)1/3 \u2192 0 in\n(8.10)\u2013(8.12). the bounded convergence condition holds because these adaptations\nare restricted to a finite interval. actually, we did not state this explicitly in our pre-\nsentation of the approach because in our example the adaptations settled down nicely\nwithout the need to impose bounds.\nfigure 8.2 shows how the acceptance rates for each parameter change as itera-\ntions progress. in this figure, we see that the initial proposal distributions for k and \u03c8\nare too concentrated, yielding insufficient exploration of the posterior distribution and\n\n "}, {"Page_number": 255, "text": "246\n\nk\n\n \n,\ne\nt\na\nr\n\n \nt\np\ne\nc\nc\na\n\n)\nk\nt\n(\n\u03b4\n0\n0\n2\n\nchapter 8 advanced topics in mcmc\n\n0.65\n\n0.44\n\n0.25\n\n500\n\n375\n\n250\n\nt\n\nr\n \n,\ne\nt\na\nr\n\n \nt\np\ne\nc\nc\na\n\n)\nt\nr\n(\n\u03b4\n3\n0\n\n.\n\n0\n\n0.65\n\n0.44\n\n0.25\n\n0.030\n\n0.025\n\n0.020\n\n0.015\n\nt\n\n\u03c8\n,\ne\nt\na\nr\n\n \nt\np\ne\nc\nc\na\n\n0.65\n\n0.44\n\n0.25\n\n0.20\n\n)\nt\n\u03c8\n(\n\u03b4\n1\n\n.\n\n0\n\n0.15\n\n0.10\n\nt\n\nt\n\nt\n\nt\n\nfigure8.2 trendsinacceptancerate(toprow)andtheadaptingproposaldispersionparam-\neters (bottom row) for the whale dynamics model in example 8.1. the range of the horizontal\naxes is 0\u201345,000 and adaptations are made every 1500 iterations.\n\n(t)\nk , \u03b4(t)\n\nr , and \u03b4\n\n(t)\n\u03c8 are in the correct directions.\n\nacceptance rates that are too high. in contrast, the original proposal distribution for r\nis too broad, yielding acceptance rates that are too low. for all three parameters, as\niterations progress the proposal distributions are adjusted to provide acceptance rates\n(t)\nk , \u03b4(t)\nnear 0.44. the evolution of these proposal distributions by means of adjusting \u03b4\nr ,\n(t)\n\u03c8 is not monotonic and is subject to some monte carlo variation. indeed, the\nand \u03b4\n(t)\n(t)\nk , u(t)\n\u03c8 change sign occasionally\u2014but not necessarily simultaneously\u2014as\nr , and u\nu\nacceptance rates vary randomly for each block of iterations between adaptation steps.\nnevertheless, the trends in \u03b4\ntable8.1comparessomeresultsfromthenonadaptiveandadaptiveapproaches.\nthe results in table 8.1 are compiled over only the last 7500 iterations of the sim-\nulation. for each of the three parameters, the table provides the lag 10 correlation\nwithin each chain. for k and r, these correlations are extremely high because\u2014as\nnoted above\u2014the version of the model we use here forces k and r to lie within a very\nnarrow nonlinear band of the joint parameter space. this makes it very difficult for the\nchain using independent steps for k and r to travel along this narrow high posterior\nprobability ridge. therefore it is difficult for univariate sample paths of k and r to\ntravel quickly around the full extent of their marginal posterior distributions. the table\nalso reports the average squared jumping distance (asjd) for each parameter. the\nasjd is the sum of the squared distances between proposed values and chosen values\nweighted by the acceptance probabilities. the adaptive method provided increased\nasjds and decreased lag correlations when acceptance rates were initially too high,\nbut decreased asjds and increased lag correlations when acceptances were initially\ntoo rare.\n!\n\n "}, {"Page_number": 256, "text": "table 8.1 comparison of mixing behavior for standard and adaptive metropolis within gibbs\nchains for example 8.1. the \u2018baseline\u2019 column shows the acceptance rate for the nonadaptive\napproach. the asjd columns report average squared jumping distance, as discussed in the text.\n\n8.1 adaptive mcmc\n\n247\n\nbaseline\n\nlag 10 correlation\n\nasjd\n\nk\nr\n\u03c8\n\nadaptive\n\nparameter\n\n0.82\n0.74\n0.50\n\n81%\n27%\n68%\n\nnonadaptive\n\nnonadaptive\n\naccept. rate\n\nadaptive\n39,500\n1.44 \u00d7 10\u22125\n4.20 \u00d7 10\u22123\nwhile the adaptive metropolis-within-gibbs algorithm is simple to understand\nand to apply, it ignores the correlations between the parameters. more sophisticated\nadaptive algorithms can have better convergence properties. the adaptive metropolis\nalgorithm incorporates this correlation into the adaptation.\n\n18,000\n1.97 \u00d7 10\u22125\n2.75 \u00d7 10\u22123\n\n0.76\n0.81\n0.27\n\n8.1.3 adaptive metropolis algorithm\ninthischapterandthepreviouschapter,wehavestressedthatagoodproposaldistribu-\ntion produces candidate values that cover the support of the stationary distribution in\na reasonable number of iterations and produces candidate values that are not accepted\nor rejected too frequently. the goal of the adaptive metropolis algorithm is to estimate\nthe variance of the proposal distribution during the algorithm, adapting it in pursuit\nof the the optimal acceptance rate. in particular, the adaptive metropolis algorithm is\na one-step random walk metropolis algorithm (section 7.1.2) with a normal proposal\ndistribution whose variance is calibrated using previous iterations of the chain.\nconsider a normal random walk update for a p-dimensional x via the metropo-\nlis algorithm [12]. at each iteration of the chain, a candidate value x\u2217 is sampled\nfrom a proposal distribution n(x(t), \u03bb\u0001(t)). the goal is to adapt the covariance matrix\n\u0001(t) of the proposal distribution during the algorithm. for a d-dimensional spherical\nmultivariate normal target distribution where \u0001\u03c0 is the true covariance matrix of the\ntarget distribution, the proposal distribution (2.382/p)\u0001\u03c0 has been shown to be op-\ntimal with a corresponding acceptance rate of 44% when p = 1, which decreases to\n23% as p increases [223]. thus, in one version of the adaptive metropolis algorithm,\n\u03bb is set to (2.382/p) [288]. since \u0001\u03c0 is unknown, it is estimated based on previous\niterations of the chain. an adaptation parameter \u03b3(t) is used to blend \u0001(t) and \u0001(t+1) in\nsuch a way that the diminishing adaptation condition will be upheld. a parameter \u00b5(t)\nis also introduced and estimated adaptively. this is used to estimate the covariance\nmatrix \u0001(t), since var{x} = e0[x \u2212 \u00b5][x \u2212 \u00b5]t1.\nthis adaptive metropolis algorithm begins at t = 0 with the selection of x(0) =\nx(0) drawn at random from some starting distribution g, with the requirement that\nf!x(0)\" > 0 where f is the target distribution. similarly, initialize \u00b5(0) and \u0001(0);\ncommon choices are \u00b5(0) = 0 and \u0001(0) = i. given x(t) = x(t), \u00b5(t), and \u0001(t), the\nalgorithm generates x(t+1) as follows:\n1. sample a candidate value x\u2217 from the proposal distribution n(x(t), \u03bb\u0001(t)),\n\nwhere \u03bb is set to (2.382/p) for the basic implementation of the algorithm.\n\n "}, {"Page_number": 257, "text": "248\n\nchapter 8 advanced topics in mcmc\n\n2. select the value for x(t+1) according to\n\nx(t+1)\n\n=2x\u2217 with probability min0r!x(t),x\u2217\" ,11 ,\nwhere r!x(t),x\u2217\" is the metropolis-hastings ratio given in (7.1).\n\n3. adaptation step: update the proposal distribution variance in two steps:\n\notherwise,\n\nx(t)\n\n\u00b5(t+1)\n\u0001(t+1)\n\n= \u00b5(t)\n= \u0001(t)\n\n\u2212 \u0001(t)4 .\n\n\u2212 \u00b5(t)$t\n\n4. increment t and return to step 1.\n\n+ \u03b3(t+1)#x(t+1)\n+ \u03b3(t+1)3#x(t+1)\n\n\u2212 \u00b5(t)$ ,\n\u2212 \u00b5(t)$#x(t+1)\nhere \u03b3(t+1) is an adaptation parameter with values chosen by the user. for\nexample, \u03b3(t+1) = 1/(t + 1) is a reasonable choice.\nthe updating formula for \u0001(t) is constructed so that it is computationally quick\ntocalculateandsothattheadaptationdiminishesasthenumberofiterationsincreases.\nto uphold the diminishing adaptation condition, it is required that limt\u2192\u221e \u03b3(t) = 0.\nthe additional condition that5\u221et=0 \u03b3(t) = \u221e allows the sequence \u0001(t) to move an\ninfinite distance from its initial value [16].\nadapting both the mean and the variance of the proposal distribution in the\nadaptation step may be overkill, but it can have some advantages [12]. specifically,\nthe strategy can result in a more conservative sampler in the sense the sampler may\nresist large moves to poor regions of the parameter space.\nseveral enhancements to the adaptive metropolis algorithm may improve per-\nformance. it may make sense to adapt \u03bb as well as adapt \u0001(t) during the algorithm.\nin this enhancement \u03bb is replaced by \u03bb(t), and then \u03bb(t) and \u0001(t) are updated inde-\npendently. specifically, in step 3 of the adaptive metropolis algorithm \u03bb(t+1) is also\nupdated using\n\nlog#\u03bb(t+1)$ = log#\u03bb(t)$ + \u03b3(t+1)#r#x(t),x\u2217$ \u2212 a$ ,\n\n(8.13)\n\nwhere a denotes the target acceptance rate (e.g., 0.234 for higher-dimensional\nproblems).\none drawback with the adaptive metropolis algorithm above is that all compo-\nnentsareacceptedorrejectedsimultaneously.thisisnotefficientwhentheproblemis\nhigh dimensional (i.e., large p). an alternative is to develop a componentwise hybrid\ngibbs adaptive metropolis algorithm where each component is given its own scaling\nparameter \u03bb(t+1) and is accepted/rejected separately in step 2. in this case, the constant\na in (8.13) is usually set to a higher value, for example, a = 0.44, since now com-\nponents are updated individually (see section 7.3.1.3). in addition, the components\ncould be updated in random order; see random scan gibbs sampling in section 7.2.3.\n\n "}, {"Page_number": 258, "text": "8.1 adaptive mcmc\n\nanother variation is to carry out\n\n249\nthe adaptation in batches instead of\nevery iteration. with the batching strategy, the adaptation (step 3 in the adap-\ntive metropolis algorithm) is only implemented at predetermined times {tb} for\nb = 0,1,2, . . .. for example, the adaptation step could occur at a fixed interval,\nsuch as t \u2208 {50,100,150, . . .}. alternatively, the batching scheduling could be de-\nsigned with an increasing number of iterations between adaptations, for example,\nt \u2208 {50,150,300,500, . . .}. a batch approach was used in example 8.1.\nthe batchwise adaptive metropolis algorithm is described below.\n1. initialization: select starting values x(0) = x(0) and set t = 0. select a batching\nschedule {tb} for b = 0,1,2, . . . with t0 = 0. set a batch index b = 0. select\nstarting values for the adaptation parameters \u00b5(0) and \u0001(0); commonly used\nstarting values are \u00b5(0) = 0 and \u0001(0) = i.\n2. sample candidate x\u2217 from the proposal distribution n!x(t), \u03bb\u0001(b)\".\n=2x\u2217 with probability min0r!x(t),x\u2217\" ,11 ,\n\n3. select a value for x(t+1) according to\n\notherwise,\n\nx(t+1)\n\nx(t)\n\nwhere r!x(t),x\u2217\" is given in (7.1).\n4. when t = tb+1, perform the adaptation steps:\n\na. update the proposal distribution:\n\n\u00b5(b+1)\n\n\u0001(b+1)\n\n= \u00b5(b)\n= \u0001(b)\n\n+\n\n+\n\n1\n\ntb+1 \u2212 tb\n\n1\n\ntb+16j=tb+1#x(j)\n\n\u2212 \u00b5(b)$ ,\n\ntb+1 \u2212 tb\n\ntb+16j=tb+13#x(j)\nb. increment the batch index b = b + 1.\n\n\u00d7\n\n5. increment t and return to step 2.\n\n\u2212 \u00b5(b)$#x(j)\n\n\u2212 \u00b5(b)$t\n\n\u2212 \u0001(b)4 .\n\nnote that in this algorithm, the diminishing adaptation condition is upheld when\nlimb\u2192\u221e(tb+1 \u2212 tb) = \u221e. it has also been suggested that adaptation times could be\nselected randomly. for example, one could carry out the adaptation with probability\np(t) where limt\u2192\u221e p(t) = 0 ensures diminishing adaptation [550].\nthe amcmc algorithms described here all share the property that they are\ntime inhomogeneous, that is, the proposal distributions change over time. other time-\ninhomogeneous mcmc algorithms are described in section 8.4.\na wide variety of other amcmc algorithms have been suggested. this area\nwill likely continue to develop rapidly [12, 16, 288, 551].\n\n "}, {"Page_number": 259, "text": "250\n\nchapter 8 advanced topics in mcmc\n\n8.2 reversible jump mcmc\ninchapter7weconsideredmcmcmethodsforsimulatingx(t) for t = 1,2, . . . from\na markov chain with stationary distribution f. the methods described in chapter 7\nrequired that the dimensionality of x(t) (i.e., of its state space) and the interpretation\nof the elements of x(t) do not change with t. in many applications, it may be of in-\nterest to develop a chain that allows for changes in the dimension of the parameter\nspace from one iteration to the next. green\u2019s reversible jump markov chain monte\ncarlo (rjmcmc) method permits transdimensional markov chain monte carlo sim-\nulation [278]. we discuss this approach below in the context of bayesian model un-\ncertainty. the full generality of rjmcmc is described in many of the references\ncited here.\nconsider constructing a markov chain to explore a space of candidate models,\neach of which might be used to fit observed data y. let m1, . . . ,mk denote a\ncountable collection of models under consideration. a parameter vector \u03b8m denotes\nthe parameters in the mth model. different models may have different numbers of\nparameters, so we let pm denote the number of parameters in the mth model. in the\nbayesian paradigm, we may envision random variables x = (m, \u03b8m) which together\nindex the model and parameterize inference for that model. we may assign prior\ndistributionstotheseparameters,thenseektosimulatefromtheirposteriordistribution\nm(t)$.\nusing an mcmc method for which the tth random draw is x(t) =#m(t), \u03b8\n(t)\nm(t), which denotes the parameters drawn for the model indexed by m(t), has\nthus, the goal of rjmcmc is to generate samples with joint posterior density\n\nhere \u03b8\ndimension p\nf (m, \u03b8m|y). this posterior arises from bayes\u2019 theorem via\n\nm(t) that can vary with t.\n\n(t)\n\nf (m, \u03b8m|y) \u221d f (y|m, \u03b8m) f (\u03b8m|m) f(m),\n\n(8.14)\nwhere f (y|m, \u03b8m) denotes the density of the observed data under the mth model\nand its parameters, f (\u03b8m|m) denotes the prior density for the parameters in the mth\nmodel, and f(m) denotes the prior density of the mth model. a prior weight of f(m)\nis assigned to the mth model so5k\n\nthe posterior factorization\n\nm=1 f(m) = 1.\n\nf (m, \u03b8m|y) = f (m|y) f (\u03b8m|m,y)\n\n(8.15)\nsuggests two important types of inference. first, f (m|y) can be interpreted as the\nposterior probability for the mth model, normalized over all models under con-\nsideration. second, f (\u03b8m|m,y) is the posterior density of the parameters in the\nmth model.\nrjmcmc enables the construction of an appropriate markov chain for x that\njumps between models with parameter spaces of different dimensions. like simpler\nmcmc methods, rjmcmc proceeds with the generation of a proposed step from the\ncurrent value x(t) to x\u2217, and then a decision whether to accept the proposal or to keep\nanother copy of x(t). the stationary distribution for our chain will be the posterior in\n\n "}, {"Page_number": 260, "text": "8.2 reversible jump mcmc\n\n251\n\n(8.15) if the chain is constructed so that\n\nf(m1, \u03b8m1|y)a(m2, \u03b8m2|m1, \u03b8m1 ,y) = f(m2, \u03b8m2|y)a(m1, \u03b8m1|m2, \u03b8m2 ,y)\n\nfor all m1 and m2, where a(x2|x1,y) denotes the density for the chain moving to\nstate x2 = (m2, \u03b8m2) at time t + 1, given that it was in state x1 = (m1, \u03b8m1) at time t.\nchains that meet this detailed balance condition are termed reversible because the\ndirection of time does not matter in the dynamics of the chain.\nthe key to the rjmcmc algorithm is the introduction of auxiliary random\nvariables at times t and t + 1 with dimensions chosen so that the augmented variables\n(namely x and the auxiliary variables) at times t and t + 1 have equal dimensions. we\ncan then construct a markov transition for the augmented variable at time t that main-\ntains dimensionality. this dimension-matching strategy enables the time-reversibility\ncondition to be met by using a suitable acceptance probability, thereby ensuring that\nthe markov chain converges to the joint posterior for x. details of the limiting theory\nfor these chains are given in [278, 279].\nto understand dimension matching, it is simplest to begin by considering how\none might propose parameters \u03b82 corresponding to a proposed move from a model\nm1 with p1 parameters to a model m2 with p2 parameters when p2 > p1. a simple\napproach is to generate \u03b82 from an invertible deterministic function of both \u03b81 and\nan independent random component u1. we can write \u03b82 = q1,2(\u03b81,u1). proposing\nparameters for the reverse move can be carried out via the inverse transformation,\n(\u03b81,u1) = q\u22121\n1,2(\u03b82) = q2,1(\u03b82). note that q2,1 is an entirely deterministic way to\npropose \u03b81 from a given \u03b82.\nnow generalize this idea to generate an augmented candidate parameter vector\n(\u03b8\u2217m\u2217 and auxiliary variables u\u2217), given a proposed move to m\u2217 from the current\nmodel, m(t). we can apply an invertible deterministic function qt,\u2217 to \u03b8(t) and some\nauxiliary random variables u to generate\n\nwhen pm\u2217 = p\n\n(\u03b8\u2217m\u2217 ,u\u2217) = qt,\u2217(\u03b8(t),u),\n\n(8.16)\nwhere u is generated from proposal density h(\u00b7|m(t), \u03b8(t), m\u2217). the auxiliary variables\nu\u2217 and u are used so that qt,\u2217 maintains dimensionality during the markov chain\ntransition at time t, but are discarded subsequently.\nm(t), the approach in (8.16) allows familiar proposal strategies.\nfor example, a random walk could be obtained using (\u03b8\u2217m\u2217 ,u\u2217) = (\u03b8(t) + u,u) with\nu \u223c n(0, \u03c32i) having dimension p\nm(t). alternatively, a metropolis\u2013hastings chain\ncan be constructed by using \u03b8\u2217m\u2217 = qt,\u2217(u) when pu = pm\u2217, for an appropriate func-\ntional form of qt,\u2217 and suitable u. no u\u2217 would be required to equalize dimensions.\nm(t) < pm\u2217, the u can be used to expand parameter dimensionality; u\u2217 may\nwhen p\nor may not be necessary to equalize dimensions, depending on the strategy employed.\nm(t) > pm\u2217, both u and u\u2217 may be unnecessary: for example, the simplest\nwhen p\ndimension reduction is deterministically to reassign some elements of \u03b8(t) to u\u2217 and\nretain the rest for \u03b8\u2217m\u2217. in all these cases, the reverse proposal is again obtained from\nthe inverse of qt,\u2217.\n\n "}, {"Page_number": 261, "text": "252\n\nchapter 8 advanced topics in mcmc\n\n(t)\n\nas follows:\n\nassume that the chain is currently visiting model m(t), so the chain is in the state\n\nm(t)$. the next iteration of the rjmcmc algorithm can be summarized\nx(t) =#m(t), \u03b8\n1. sample a candidate model m\u2217**m(t) from a proposal density with conditional\ndensity g(\u00b7|m(t)). the candidate model requires parameters \u03b8m\u2217 of dimen-\nsion pm\u2217.\nm(t) , m\u2217\" from a\n2. given m\u2217 = m\u2217, generate an augmenting variable u|!m(t), \u03b8\n(t)\nproposal distribution with density h(\u00b7|m(t), \u03b8\nm(t) , m\u2217). let\n(\u03b8\u2217m\u2217 ,u\u2217) = qt,\u2217#\u03b8\nm(t) ,u$,\n(t)\nm(t) ,u$ to (\u03b8\u2217m\u2217 ,u\u2217) and the aux-\nwhere qt,\u2217 is an invertible mapping from#\u03b8\niliary variables have dimensions satisfying pm(t) + pu = pm\u2217 + pu\u2217.\n3. for a proposed model, m\u2217 = m\u2217, and the corresponding proposed parameter\nvalues \u03b8\u2217m\u2217, compute the metropolis\u2013hastings ratio given by\n\n(t)\n\n(t)\n\n(8.17)\n\n(8.18)\n\nwhere j(t) is the jacobian matrix described in section 1.1,\n\n(t)\n\n(t)\n\nf!m\u2217, \u03b8\u2217m\u2217**y\"g!m(t)**m\u2217\"h!u\u2217**m\u2217, \u03b8\u2217m\u2217 , m(t)\"\nm(t) , m\u2217\" |j(t)| ,\nm(t)**y\"g!m\u2217**m(t)\"h!u**m(t), \u03b8\nf!m(t), \u03b8\n****(\u03b8,u)=!\u03b8\nj(t) =\nm(t) ,u\" .\n\ndqt,\u2217f (\u03b8,u)\nd (\u03b8,u)\n\n(t)\n\n(t)\n\n4. discard u and u\u2217. return to step 1.\n\naccept the move to the model m\u2217 with probability equal to the minimum\nof 1 and the expression in (8.17). if the proposal is accepted, set x(t+1) =\n(m\u2217, \u03b8\u2217m\u2217). otherwise, reject the candidate draw and set x(t+1) = x(t).\nthe last term in (8.17) is the absolute value of the determinant of the jacobian matrix\narising from the change of variables from#\u03b8\nm(t) ,u$ to!\u03b8\u2217m\u2217 ,u\u2217\". if p\nm(t) = pm\u2217,\nthen (8.17) simplifies to the standard metropolis\u2013hastings ratio (7.1). note that it is\nimplicitly assumed here that the transformation qt,\u2217 is differentiable.\nexample 8.2 (jumping between two simple models) an elementary example\nillustrates some of the details described above [278]. consider a problem with k = 2\npossible models: the model m1 has a one-dimensional parameter space \u03b81 = \u03b1, and\nthe model m2 has a two-dimensional parameter space \u03b82 = (\u03b2, \u03b3). thus p1 = 1 and\np2 = 2. let m1 = 1 and m2 = 2.\nif the chain is currently in state (1, \u03b81) and the model m2 is proposed, then a\nrandom variable u \u223c h(u|1, \u03b81,2) is generated from some proposal density h. let\n\u03b2 = \u03b1 \u2212 u and \u03b3 = \u03b1 + u, so q1,2 (\u03b1, u) = (\u03b1 \u2212 u, \u03b1 + u) and\n\n**dq1,2(\u03b1, u)/d(\u03b1, u)** = 2.\n\n "}, {"Page_number": 262, "text": "253\nif the chain is currently in state (2, \u03b82) and m1 is proposed, then (\u03b1, u) =\n\n8.2 reversible jump mcmc\n\nq2,1(\u03b2, \u03b3) =# \u03b2+\u03b32 , \u03b2\u2212\u03b32 $ is the inverse mapping. therefore\n\n**dq2,1(\u03b2, \u03b3)/d(\u03b2, \u03b3)** =\n\n1\n2 ,\n\nand u\u2217 is not required to match dimensions. this transition is entirely deterministic,\nso we replace h(u\u2217|2, \u03b82,1) in (8.17) with 1.\nthus for a proposed move from m1to m2, the metropolis\u2013hasting ratio (8.17)\nis equal to\n\nf (2, \u03b2, \u03b3|y) g (1|2)\n\nf (1, \u03b1|y) g (2|1) h(u|1, \u03b81,2) \u00d7 2.\n\n(8.19)\nthe metropolis\u2013hastings ratio equals the reciprocal of (8.19) for a proposed move\nfrom m2 to m1.\n!\nthere are several significant challenges to implementing rjmcmc. since the\nnumber of dimensions can be enormous, it can be critical to select an appropriate pro-\nposal distribution h and to construct efficient moves between the different dimensions\nof the model space. another challenge is the diagnosis of convergence for rjmcmc\nalgorithms. research in these areas is ongoing [72\u201374, 427, 604].\nrjmcmc is a very general method, and reversible jump methods have been\ndeveloped for a myriad of application areas, including model selection and parameter\nestimationforlinearregression[148],variableandlinkselectionforgeneralizedlinear\nmodels [487], selection of the number of components in a mixture distribution [74,\n536, 570], knot selection and other applications in nonparametric regression [48, 162,\n334], and model determination for graphical models [147, 248]. there are many other\nareas for potential application of rjmcmc. genetic mapping was an early area of\nexploration for rjmcmc implementation; [122, 645, 648] there are claims that up\nto 20% of citations related to rjmcmc involve genetics applications [603].\nrjmcmc unifies earlier mcmc methods to compare models with different\nnumbers of parameters. for example, earlier methods for bayesian model selection\nand model averaging for linear regression analysis, such as stochastic search variable\nselection [229] and mcmc model composition [527], can be shown to be special\ncases of rjmcmc [119].\n\n8.2.1 rjmcmc for variable selection in regression\nconsider a multiple linear regression problem with p potential predictor variables\nin addition to the intercept. a fundamental problem in regression is selection of a\nsuitable model. let mk denote model k, which is defined by its inclusion of the i1th\nthrough idth predictors, where the indices {i1, . . . , id} are a subset of {1, . . . , p}. for\nthe consideration of all possible subsets of p predictor variables, there are therefore\nk = 2p models under consideration. using standard regression notation, let y denote\nthe vector of n independent responses. for any model mk, arrange the corresponding\npredictors in a design matrix xmk =!1 xi1 \u00b7\u00b7\u00b7xid\", where xij is the n vector of\n\n "}, {"Page_number": 263, "text": "chapter 8 advanced topics in mcmc\n\n254\nobservations of the ijth predictor. the predictor data are assumed fixed. we seek the\nbest ordinary least squares model of the form\n\n1\np\n0\n\ny = xmk \u03b2mk + \u03f5\n\n(8.20)\namong all mk, where \u03b2mk is a parameter vector corresponding to the design matrix\nfor mk and the error variance is \u03c32. in the remainder of this section, conditioning on\nthe predictor data is assumed.\nthe notion of what model is best may have any of several meanings. in exam-\nple 3.2 the goal was to use the akaike information criterion (aic) to select the best\nmodel [7, 86]. here, we adopt a bayesian approach for variable selection with priors\non the regression coefficients and \u03c32, and with the prior for the coefficients depending\non \u03c32. the immediate goal is to select the most promising subset of predictor vari-\nables, but we will also show how the output of an rjmcmc algorithm can be used to\nestimate a whole array of quantities of interest such as posterior model probabilities,\nthe posterior distribution of the parameters of each model under consideration, and\nmodel-averaged estimates of various quantities of interest.\nin our rjmcmc implementation, based on [119, 527], each iteration begins\nat a model m(t), which is described by a specific subset of predictor variables. to\nadvance one iteration, the model m\u2217 is proposed from among those models having\neither one predictor variable more or one predictor variable fewer than the current\n\nif m\u2217 has one more or one fewer predictor than m(t),\notherwise.\n\nmodel. thus the model proposal distribution is given by g!\u00b7|m(t)\", where\ng#m\u2217|m(t)$ =\u23a7\u23a8\u23a9\ngiven a proposed model m\u2217 = m\u2217, step 2 of the rjmcmc algorithm requires us to\nsampleu**!m(t), \u03b2\nm(t) , m\u2217\" \u223c h! \u00b7 |m(t), \u03b2\nm(t) , m\u2217\".asimplifyingapproachistoletu\nbecome the next value for the parameter vector, in which case we may set the proposal\ndistribution hequaltotheposteriorfor \u03b2m|(m,y),namely f(\u03b2m|m,y).forappropriate\nconjugate priors, \u03b2\u2217m\u2217|(m\u2217,y) has a noncentral t distribution [58]. we draw u from\nm(t) ,u\" =!\u03b2\u2217m\u2217 ,u\u2217\",\nm(t). thus qt,\u2217 =!\u03b2\nthis proposal and set \u03b2\u2217m\u2217 = u and u\u2217 = \u03b2\nyielding a jacobian of 1. since g!m(t)|m\u2217\" = g!m\u2217|m(t)\" = 1/p, the metropolis\u2013\nhastings ratio in (8.17) can be written as\nf#y***m\u2217, \u03b2\u2217m\u2217$f#\u03b2\u2217m\u2217***m\u2217$f#m\u2217$f#\u03b2\nm(t)***m(t),y$\nm(t)***m(t)$f#m(t)$f#\u03b2\u2217m\u2217***m\u2217,y$\nf#y***m(t), \u03b2\n\n(8.21)\nafter simplification. here f (y|m\u2217) is the marginal likelihood, and f (m\u2217) is the prior\ndensity, for the model m\u2217. observe that this ratio does not depend on \u03b2\u2217m\u2217 or \u03b2m(t).\ntherefore, when implementing this approach with conjugate priors, one can treat\nthe proposal and acceptance of \u03b2 as a purely conceptual construct useful for placing\nthe algorithm in the rjmcmc framework. in other words, we don\u2019t need to sim-\nulate \u03b2(t)**m(t), because f (\u03b2|m,y) is available in closed form. the posterior model\nprobabilities and f (\u03b2|m,y) fully determine the joint posterior.\n\nf (y|m\u2217) f (m\u2217)\nf!y|m(t)\" f!m(t)\"\n\nm(t)$f#\u03b2\n\n=\n\n(t)\n\n(t)\n\n(t)\n\n(t)\n\n(t)\n\n(t)\n\n(t)\n\n "}, {"Page_number": 264, "text": "8.2 reversible jump mcmc\n\n255\n\ntable 8.2 rjmcmc model selection results for baseball example: the five models\nwith the highest posterior model probability (pmp). the bullets indicate inclusion of\nthe corresponding predictor in the given model, using the predictor indices given in\ntable 3.2.\n\npmp\n0.22\n0.08\n0.05\n0.04\n0.03\n\n4\n\n8\n\n13\n\n14\n\n24\n\n3\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\npredictors\n10\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\nafter running the rjmcmc algorithm, inference about many quantities of in-\nterest is possible. for example, from (8.15) the posterior model probabilities f(mk|y)\ncan be approximated by the ratio of the number of times the chain visited the kth\nmodel to the number of iterations of the chain. these estimated posterior model prob-\nabilities can be used to select models. in addition, the output from the rjmcmc\nalgorithm can be used to implement bayesian model averaging. for example, if \u00b5 is\nsome quantity of interest such as a future observable, the utility of a course of action,\nor an effect size, then the posterior distribution of \u00b5 given the data is given by\n\nf (\u00b5|y) =\n\nf (\u00b5|mk,y) f (mk|y) .\n\n(8.22)\n\nk6k=1\n\nthis is the average of the posterior distribution for \u00b5 under each model, weighted by\nthe posterior model probability. it has been shown that taking account of uncertainty\nabout the form of the model can protect against underestimation of uncertainty [331].\nexample 8.3 (baseball salaries, continued)\nrecall example 3.3, where we\nsought the best subset among 27 possible predictors to use in linear regression mod-\neling of baseball players\u2019 salaries. previously, the objective was to find the best subset\nas measured by the minimal aic value. here, we seek the best subset as measured\nby the model with the highest posterior model probability.\nwe adopt a uniform prior over model space, assigning f(mk) = 2\u2212p for each\nmodel. for the remaining parameters, we use a normal-gamma conjugate class of pri-\nors with \u03b2mk|mk \u223c n(\u03b1mk , \u03c32vmk) and \u03bd\u03bb/\u03c32 \u223c \u03c72\n\u03bd. for this construction, f (y|mk)\nin (8.21) can be shown to be the noncentral t density (problem 8.1). for the baseball\ndata, the hyperparameters are set as follows. first, let \u03bd = 2.58 and \u03bb = 0.28. next,\n\u03b1mk =!\u02c6\u03b20,0, . . . ,0\" is a vector of length pmk whose first element equals the least\nsquares estimated intercept from the full model. finally, vmk is a diagonal matrix with\nentries#s2y, c2/s2\ni is the sample\nvariance of the ith predictor, and c = 2.58. additional details are given in [527].\nwe ran 200,000 rjmcmc iterations. table 8.2 shows the five models with the\nhighest estimated posterior model probabilities. if the goal is to select the best model,\nthen the model with the predictors 3, 8, 10, 13, and 14 should be chosen, where the\npredictor indices correspond to those in table 3.2.\n\np$, where s2y is the sample variance of y, s2\n\n1, . . . , c2/s2\n\n "}, {"Page_number": 265, "text": "256\n\nchapter 8 advanced topics in mcmc\n\np(\u03b2i /= 0|y)\n\ntable 8.3 rjmcmc results for baseball example:\nthe estimated posterior effect probabilities p(\u02c7i /= 0|y)\nexceeding 0.10. the predictor\nindices and labels\ncorrespond to those given in table 3.2.\nindex\n13\n14\n8\n10\n3\n4\n25\n24\n9\n\npredictor\nfa\narb\nrbis\nsos\nruns\nhits\nsbs\u00d7obp\nsos\u00d7errors\nwalks\n\n1.00\n1.00\n0.97\n0.78\n0.55\n0.52\n0.13\n0.12\n0.11\n\nthe posterior effect probabilities, p(\u03b2i /= 0|y), for those predictors with prob-\nabilities greater than 0.10 are given in table 8.3. each entry is a weighted average of\nan indicator variable that equals 1 only when the coefficient is in the model, where the\nweights correspond to the posterior model probabilities as in equation (8.22). these\nresults indicate that free agency, arbitration status, and the number of runs batted in\nare strongly associated with baseball players\u2019 salaries.\nother quantities of interest based on variants of equation (8.22) can be com-\nputed, such as the model-averaged posterior expectation and variance for each regres-\nsion parameter, or various posterior salary predictions.\n!\nalternativeapproachestotransdimensionalmarkov chainsimulationhavebeen\nproposed. one method is based on the construction of a continuous-time markov\nbirth-and-death process [89, 613]. in this approach, the parameters are modeled via\na point process. a general form of rjmcmc has been proposed that unifies many of\nthe existing methods for assessing uncertainty about the dimension of the parameter\nspace [261]. continued useful development in these areas is likely [318, 428, 603].\none area of promise is to combine rjmcmc and amcmc.\n\n8.3 auxiliary variable methods\nan important area of development in mcmc methods concerns auxiliary variable\nstrategies. in many cases, such as bayesian spatial lattice models, standard mcmc\nmethods can take too long to mix properly to be of practical use. in such cases, one\npotentialremedyistoaugmentthestatespaceofthevariableofinterest.thisapproach\ncan lead to chains that mix faster and require less tuning than the standard mcmc\nmethods described in chapter 7.\ncontinuing with the notation introduced in chapter 7, we let x denote a random\nvariable on whose state space we will simulate a markov chain, usually for the\npurpose of estimating the expectation of a function of x \u223c f(x). in bayesian ap-\nplications, it is important to remember that the random variables x(t) simulated in\n\n "}, {"Page_number": 266, "text": "8.3 auxiliary variable methods\n\n257\nan mcmc procedure are typically parameter vectors whose posterior distribution\nis of primary interest. consider a target distribution f which can be evaluated but\nnot easily sampled. to construct an auxiliary variable algorithm, the state space of\nx is augmented by the state space of a vector of auxiliary variables, u. then one\nconstructs a markov chain over the joint state space of (x,u) having stationary dis-\ntribution (x,u) \u223c f(x,u) that marginalizes to the target f(x). when simulation has\nbeen completed, inference is based only on the marginal distribution of x. for exam-\nple, a monte carlo estimator of \u00b5 =7 h(x)f(x) dx is \u02c6\u00b5 = (1/n)5n\nt=1 h(x(t)) where\n(x(t),u(t)) are simulated in the augmented chain, but the u(t) are discarded.\nauxiliary variable mcmc methods were introduced in the statistical physics\nliterature [174, 621]. besag and green noted the potential usefulness of this strategy,\nand a variety of refinements have subsequently been developed [41, 132, 328]. aug-\nmentingthevariablesofinteresttosolvechallengingstatisticalproblemsiseffectivein\nother areas, such as the em algorithm described in chapter 4 and the reversible jump\nalgorithms described in section 8.2. the links between em and auxiliary variable\nmethods for mcmc algorithms are further explored in [640].\nbelow we describe simulated tempering as an example of an auxiliary variable\nstrategy. another important example is slice sampling, which is discussed in the\nnext subsection. in section 8.7.2 we present another application of auxiliary variable\nmethods for the analysis of spatial or image data.\n\n8.3.1 simulated tempering\ninproblemswithhighnumbersofdimensions,multimodality,orslowmcmcmixing,\nextremely long chains may be required to obtain reliable estimates of the quantities of\ninterest. the approachof simulated tempering provides apotentialremedy [235,438].\nsimulations are based on a sequence of unnormalized densities fi for i = 1, . . . , m,\non a common sample space. these densities are viewed as ranging from cold (i = 1)\nto hot (i = m). typically only the cold density is desired for inference, with the other\ndensities being exploited to improve mixing. indeed, the warmer densities should be\ndesigned so that mcmc mixing is faster for them than it is for f1.\nconsider the augmented variable (x, i) where the temperature i is now viewed\nas random with prior i \u223c p(i). from a starting value, (x(0), i(0)), we may construct a\nmetropolis\u2013hastings sampler in the augmented space as follows:\n1. use a metropolis\u2013hastings or gibbs update to draw x(t+1)**i(t) from a chain\n2. generate i\u2217 from a proposal density, g!\u00b7|i(t)\". a simple option is\ng#i\u2217**i(t)$ =\u23a7\u23aa\u23a8\u23aa\u23a9\nif !i(t), i\u2217\" = (1,2) or!i(t), i\u2217\" = (m, m \u2212 1),\nif **i\u2217 \u2212 i(t)** = 1 and i(t) \u2208 {2, . . . , m \u2212 1},\nratio to be rst!i(t), i\u2217,x(t+1)\" , where\nrst(u,v,z) =\n\n3. accept or reject the candidate i\u2217 as follows. define the metropolis\u2013hastings\n\nwith stationary distribution fi(t).\n\notherwise.\n\n(8.23)\n\nfv(z)p(v)g(u|v)\nfu(z)p(u)g(v|u) ,\n\n1\n1\n2\n0\n\n "}, {"Page_number": 267, "text": "258\n\nchapter 8 advanced topics in mcmc\n\nand accept i(t+1) = i\u2217 with probability min0rst!i(t), i\u2217,x(t+1)\",11. other-\nwise, keep another copy of the current state, setting i(t+1) = i(t).\n\n4. return to step 1.\n\nthe simplest way to estimate an expectation under the cold distribution is to average\nrealizations generated from it, throwing away realizations generated from other fi.\nto use more of the data, note that a state (x, i) drawn from the stationary distribution\nof the augmented chain has density proportional to fi(x)p(i). therefore, importance\nweights w\u2217(x) = \u02dcf(x)/[fi(x)p(i)] can be used to estimate expectations with respect\nto a target density \u02dcf; see chapter 6.\nthe prior p is set by the user and ideally should be chosen so that the m tem-\npering distributions (i.e., the m states for i) are visited roughly equally. in order for\nall the tempering distributions to be visited in a tolerable running time, m must be\nfairly small. on the other hand, each pair of adjacent tempering distributions must\nhave sufficient overlap for the augmented chain to move easily between them. this\nrequires a large m. to balance these competing concerns, choices for m that provide\nacceptance rates roughly in the range suggested in section 7.3.1.3 are recommended.\nimprovements, extensions, and related techniques are discussed in [232, 235, 339,\n417, 480]. relationships between simulated tempering and other mcmc and impor-\ntance sampling methods are explored in [433, 682].\nsimulated tempering is reminiscent of the simulated annealing optimization\nalgorithm described in chapter 3. suppose we run simulated tempering on the state\nspace for \u03b8. let l(\u03b8) and q(\u03b8) be a likelihood and prior for \u03b8, respectively. if we let\nfi(\u03b8) = exp{(1/\u03c4i)log{q(\u03b8)l(\u03b8)}} for \u03c4i = i and i = 1,2, . . ., then i = 1 makes the\ncold distribution match the posterior for \u03b8, and i > 1 generates heated distributions\nthat are increasingly flattened to improve mixing. equation (8.23) then evokes step 2\nofthesimulatedannealingalgorithmdescribedinsection3.3tominimizethenegative\nlog posterior. we have previously noted that simulated annealing produces a time-\ninhomogeneous markov chain in its quest to find an optimum (section 3.3.1.2). the\noutput of simulated tempering is also a markov chain, but simulated tempering does\nnot systematically cool in the same sense as simulated annealing. the two procedures\nshare the idea of using warmer distributions to facilitate exploration of the state space.\n\n8.3.2 slice sampler\nan important auxiliary variable mcmc technique is called the slice sampler [132,\n328, 481]. consider mcmc for a univariate variable x \u223c f(x), and suppose that it is\nimpossible to sample directly from f. introducing any univariate auxiliary variable u\nwould allow us to consider a target density for (x, u) \u223c f(x, u). writing f(x, u) =\nf(x)f(u|x) suggests an auxiliary variable gibbs sampling strategy that alternates\nbetween updates for x and u [328]. the trick is to choose a u variable that speeds\nmcmc mixing for x. at iteration t + 1 of the slice sampler we alternately generate\nx(t+1) and u(t+1) according to\n\nu(t+1)**x(t)\nx(t+1)**u(t+1)\n\n\u223c unif#0, f#x(t)$$,\n\u223c unif(x : f(x) \u2265 u(t+1)) .\n\n(8.24)\n(8.25)\n\n "}, {"Page_number": 268, "text": "8.3 auxiliary variable methods\n\n259\n\nx(t)\n\nf x(t)\n\n0\n\nu(t+1)\n\n0\n\nfigure 8.3 two steps of a univariate slice sampler for target distribution f.\n\nx : f (x)\n\nu> (t+1)\n\nfigure 8.3 illustrates the approach. at iteration t + 1, the algorithm starts at\nx(t) shown in the upper panel. then u(t+1) is drawn from unif!0, f!x(t)\"\". in\nthe top panel this corresponds to sampling along the vertical shaded strip. now\nx(t+1)**!u(t+1) = u(t+1)\" is drawn uniformly from the set of x values for which\nf(x) \u2265 u(t+1). in the lower panel this corresponds to sampling along the horizon-\ntal shaded strip.\nwhile simulating from (8.25) is straightforward for this example, in other set-\ntings the set0x : f(x) \u2265 u(t+1)1 may be more complicated. in particular, sampling\nx(t+1)**!u(t+1) = u(t+1)\" in (8.25) can be challenging if f is not invertible. one ap-\n\nproach to implementing equation (8.25) is to adopt a rejection sampling approach;\nsee section 6.2.3.\nexample 8.4 (moving between distant modes) when the target distribution is\nmultimodal, one advantage of a slice sampler becomes more apparent. figure 8.4\nshows a univariate multimodal target distribution. if a standard metropolis\u2013hastings\nalgorithm is used to generate samples from the target distribution, then the algorithm\nmay find one mode of the distribution, but it may take many iterations to find the other\nmode unless the proposal distribution is very well tuned. even if it finds both modes, it\nwill almost never jump from one to the other. this problem will be exacerbated when\nthe number of dimensions increases. in contrast, consider a slice sampler constructed\nto sample from the density shown in figure 8.4. the horizontal shaded areas indicate\nsampler will have about a 50% chance of switching modes each iteration. therefore\nthe slice sampler will mix much better with many fewer iterations required.\n!\nslice samplers have been shown to have attractive theoretical properties [467,\n543] but can be challenging to implement in practice [481, 543]. the basic slice\n\nthe set defined in (8.25) from which x(t+1)**u(t+1) is uniformly drawn. hence the slice\n\n "}, {"Page_number": 269, "text": "260\n\nchapter 8 advanced topics in mcmc\n\n)\nx\n(\nf\n\nu(t+1)\n\nfigure 8.4 the slice sampler for this multimodal target distribution draws x(t+1)**u(t+1)\n\nuniformly from the set indicated by the two horizontal shaded strips.\n\nx\n\nsampler described above can be generalized to include multiple auxiliary variables\nu1, . . . , uk and to accommodate multidimensional x [132, 328, 467, 543]. it is also\npossible to construct a slice sampler such that the algorithm is guaranteed to sample\nfrom the stationary distribution of the markov chain [98, 466]. this is a variant of\nperfect sampling, which is discussed in section 8.5.\n\n8.4 other metropolis\u2013hastings algorithms\n\ng(t)!\u00b7|x(t)\". such methods can be very effective, but their convergence properties are\n\n8.4.1 hit-and-run algorithm\nthe metropolis\u2013hastings algorithm presented in section 7.1 is time homogeneous in\nthe sense that the proposal distribution does not change as t increases. it is possible\nto construct mcmc approaches that rely on time-varying proposal distributions,\ngenerallymoredifficulttoascertainduetothetimeinhomogeneity[462].theadaptive\nmcmc algorithms of section 8.1 are examples of time-inhomogeneous algorithms.\none such strategy that resembles a random walk chain is known as the hit-and-\nrun algorithm [105]. in this approach, the proposed move away from x(t) is generated\nin two stages: by choosing a direction to move and then a distance to move in the\nchosen direction. after initialization at x(0), the chain proceeds from t = 0 with the\nfollowing steps.\n1. draw a random direction \u03c1(t) \u223c h(\u03c1), where h is a density defined over the\n2. find the set of all real numbers \u03bb for which x(t) + \u03bb\u03c1(t) is in the state space of\n\nsurface of the unit p-sphere.\n\nx. denote this set of signed lengths as \u0001(t).\n\n "}, {"Page_number": 270, "text": "8.4 other metropolis\u2013hastings algorithms\n\n261\n\n(t)\n\n\u03bb !\u03bb|x(t), \u03c1(t)\",wheretheden-\n3. drawarandomsignedlength \u03bb(t)|!x(t), \u03c1(t)\" \u223c g\n\u03bb !\u03bb|x(t), \u03c1(t)\" = g(t)!x(t) + \u03bb\u03c1(t)\" is defined over \u0001(t). the proposal dis-\nsity g\ntribution may differ from one iteration to the next only through a dependence\non \u0001(t).\n4. for the proposal x\u2217 = x(t) + \u03bb(t)\u03c1(t), compute the metropolis\u2013hastings ratio\n\n(t)\n\nr#x(t),x\u2217$ =\n\nf (x\u2217) g(t)!x(t)\"\nf!x(t)\" g(t) (x\u2217) .\n\n=2x\u2217 with probability min0r!x(t),x\u2217\" ,11 ,\n\notherwise.\n\nx(t)\n\n5. set\n\nx(t+1)\n\n6. increment t and go to step 1.\n\nthe above algorithm is one variant of several general hit-and-run approaches [105].\nthe direction distribution h is frequently taken to be uniform over the surface\nof the unit sphere. in p dimensions, a random variable may be drawn from this\ndistribution by sampling a p-dimensional standard normal variable y \u223c n(0,i) and\nmaking the transformation \u03c1 = y/\u221ayty.\nthe performance of this approach has been compared with that of other simple\nmcmc methods [104]. it has been noted that the hit-and-run algorithm can offer\nparticular advantage when the state space of x is sharply constrained [29], thereby\nmaking it difficult to explore all regions of the space effectively with other methods.\nthe choice of h has a strong effect on the performance and convergence rate of the\nalgorithm, with the best choice often depending on the shape of f and the geometry\nof the state space (including constraints and the chosen units for the coordinates of\nx) [366].\n\n8.4.2 multiple-try metropolis\u2013hastings algorithm\nif a metropolis\u2013hastings algorithm is not successful in some problem, it is probably\nbecause the chain is slow to converge or trapped in a local mode of f. to overcome\nsuch difficulties, it may pay to expand the region of likely proposals characterized by\ng(\u00b7|x(t)). however, this strategy often leads to very small metropolis\u2013hastings ratios\nand therefore to poor mixing. liu, liang, and wong proposed an alternative strategy\nknown as multiple-try metropolis\u2013hastings sampling for effectively expanding the\nproposal region to improve performance without impeding mixing [420].\nthe approach is to generate a larger number of candidates, thereby improving\nexploration of f near x(t). one of these proposals is then selected in a manner that\nensures that the chain retains the correct limiting stationary distribution. we still use\na proposal distribution g, along with optional nonnegative weights \u03bb(x(t),x\u2217), where\nthe symmetric function \u03bb is discussed further below. to ensure the correct limiting\n\nstationary distribution, it is necessary to require that g!x\u2217**x(t)\" > 0 if and only if\ng!x(t)**x\u2217\" > 0, and that \u03bb!x(t),x\u2217\" > 0 whenever g!x\u2217**x(t)\" > 0.\n\n "}, {"Page_number": 271, "text": "262\n\nchapter 8 advanced topics in mcmc\n\nlet x(0) denote the starting value, and define\n\n(8.26)\n\nw(u,v) = f(v)g(u|v)\u03bb(u,v).\nthen, for t = 0,1, . . ., the algorithm proceeds as follows:\n1. sample k proposals x\u22171, . . . ,x\u2217k i.i.d. from g(\u00b7|x(t)).\n2. randomlyselectasingleproposalx\u2217j fromthesetofproposals,withprobability\n3. given x\u2217j = x\u2217j, sample k \u2212 1 random variables x\u2217\u22171 , . . . ,x\u2217\u2217k\u22121 i.i.d. from the\n4. calculate the generalized metropolis\u2013hastings ratio\n\nproportional to w(x(t),x\u2217j) for j = 1, . . . , k.\nproposal density g(\u00b7|x\u2217j). set x\u2217\u2217k = x(t).\n\nrg =\n\nw(x\u2217j ,x\u2217\u2217i ).\n\nw(x(t),x\u2217i )9 k6i=1\nwith probability min{rg,1},\notherwise.\n\nk6i=1\n=2x\u2217j\n\n5. set\n\nx(t+1)\n\nx(t)\n6. increment t and go to step 1.\n\n(8.27)\n\n(8.28)\n\nit is straightforward to show that this algorithm yields a reversible markov chain with\nlimiting stationary distribution equal to f. the efficiency of this approach depends\non k, the shape of f, and the spread of g relative to f. it has been suggested that\nan acceptance rate of 40\u201350% be a target [420]. in practice, using the multiple-try\nmetropolis\u2013hastings algorithm to select from one of many proposals at each iteration\ncanleadtochains withlowerserialcorrelation. thisleadstobettermixingin thesense\nthat larger steps can be made to find other local modes or to promote movement in\ncertain advantageous directions when we are unable to encourage such steps through\nother means.\nthe weighting function \u03bb can be used to further encourage certain types of pro-\nposals. the simplest choice is \u03bb(x(t),x\u2217) = 1. an \u201corientational-biased\u201d method with\n\u03bb(x(t),x\u2217) =0:g(x\u2217|x(t)) + g(x(t)|x\u2217); /21\u22121 was suggested in [203]. another inter-\nesting choice is \u03bb(x(t),x\u2217) =:g(x\u2217|x(t))g(x(t)|x\u2217);\u2212\u03b1, defined on the region where\ng(x\u2217|x(t)) > 0. when \u03b1 = 1, the weight w(x(t),x\u2217) corresponds to the importance\nweight f (x\u2217) /g!x\u2217|x(t)\" assigned to x\u2217 when attempting to sample from f using g\nand the importance sampling envelope (see section 6.4.1).\n\nlangevin metropolis\u2013hastings algorithm\n\n8.4.3\nin section 7.1.2 we discussed random walk chains, a type of markov chain produced\nby a simple variant of the metropolis\u2013hastings algorithm. a more sophisticated ver-\nsion, namely a random walk with drift, can be generated using the proposal\n\nx\u2217 = x(t)\n\n+ d(t)\n\n+ \u03c3\u03f5(t),\n\n(8.29)\n\n "}, {"Page_number": 272, "text": "8.4 other metropolis\u2013hastings algorithms\n\n263\n\n(8.30)\n\nwhere\n\nd(t)\n\n=, \u03c32\n\n2 - \u2202 log f(x)\n\n\u2202x\n\n****x=x(t)\n\nand \u03f5(t) is a p-dimensional standard normal random variable. the scalar \u03c3 is a tuning\nparameterwhosefixedvalueischosenbytheusertocontrolthemagnitudeofproposed\nsteps. the standard metropolis\u2013hastings ratio is used to decide whether to accept this\nproposal, using\n\ng(x\u2217|x(t)) \u221d exp<\u2212\n\n1\n\n2\u03c32!x\u2217 \u2212 x(t)\n\n\u2212 d(t)\"t!x\u2217 \u2212 x(t)\n\n\u2212 d(t)\"= .\n\n(8.31)\n\ntheoretical results indicate that the parameter \u03c3 should be selected so that the ac-\nceptance rate for the metropolis\u2013hastings ratio computed using (8.31) should be\n0.574 [548].\nthe proposal distribution for this method is motivated by a stochastic differ-\nential equation that produces a diffusion (i.e., a continuous-time stochastic process)\nwith f as its stationary distribution [283, 508]. to ensure that the discretization\nof this process given by the discrete-time markov chain described here shares the\ncorrect stationary distribution, besag overlaid the metropolis\u2013hastings acceptance\nstrategy [37].\ntherequirementtoknowthegradientofthetarget(8.30)isnotasburdensomeas\nit may seem. any unknown multiplicative constant in f drops out when the derivative\nis taken. also, when exact derivatives are difficult to obtain, they can be replaced with\nnumerical approximations.\nunlike a random walk, this algorithm introduces a drift that favors pro-\nposals that move toward modes of the target distribution. ordinary metropolis\u2013\nhastingsalgorithms\u2014includingtherandomwalkchainandtheindependencechain\u2014\ngenerally are driven by proposals that are made independently of the shape of f,\nthereby being easy to implement but sometimes slow to approach stationarity or ad-\nequately explore the support region of f. when performance of a generic algorithm\nis poor, problem-specific metropolis\u2013hastings algorithms are frequently employed\nwith specialized proposal distributions crafted in ways that are believed to exploit fea-\ntures of the target. langevin metropolis\u2013hastings algorithms also provide proposal\ndistributions motivated by the shape of f, but the self-targeting is done generically\nthrough the use of the gradient. these methods can provide better exploration of the\ntarget distribution and faster convergence.\nin some applications, the update given by (8.29) can yield markov chains that\nfail to approach convergence in runs of reasonable length, and fail to explore more\nthan one mode of f. stramer and tweedie [618] generalize (8.29) somewhat with\ndifferent drift and scaling terms that yield improved performance. further study of\nlangevin methods is given in [547, 548, 617, 618].\n\n "}, {"Page_number": 273, "text": "264\n\nchapter 8 advanced topics in mcmc\n\n8.5 perfect sampling\nmcmc is useful because at the tth iteration it generates a random draw x(t) whose\ndistribution approximates the target distribution f as t \u2192 \u221e. since run lengths are\nfinite in practice, much of the discussion in chapter 7 pertained to assessing when\ntheapproximationbecomessufficientlygood.forexample,section7.3presentsmeth-\nods to determine the run length and the number of iterations to discard (i.e., the\nburn-in). however, these convergence diagnostics all have various drawbacks. perfect\nsampling algorithms avoid all these concerns by generating a chain that has exactly\nreached the stationary distribution. this sounds wonderful, but there are challenges in\nimplementation.\n\n8.5.1 coupling from the past\npropp and wilson introduced a perfect sampling mcmc algorithm called coupling\nfrom the past (cftp) [520]. other expositions of the cftp algorithm include [96,\n165, 519]. the website maintained by wilson surveys much of the early literature on\ncftp and related methods [667].\ncftp is often motivated by saying that the chain is started at t = \u2212\u221e and run\nforward to time t = 0. while this is true, convergence does not suddenly occur in\nthe step from t = \u22121 to t = 0, and you are not required to somehow set t = \u2212\u221e on\nyour computer. instead, we will identify a window of time from t = \u03c4 < 0 to t = 0\nfor which whatever happens before \u03c4 is irrelevant, and the infinitely long progression\nof the chain prior to \u03c4 means that the chain is in its stationary distribution by time 0.\nwhile this strategy might sound reasonable at the outset, in practice it is im-\npossible to know what state the chain is in at time \u03c4. therefore, we must consider\nmultiple chains: in fact, one chain started in every possible state at time \u03c4. each chain\ncan be run forward from t = \u03c4 to t = 0. because of the markov nature of these chains,\nthe chain outcomes at time \u03c4 + 1 depend only on their status at time \u03c4. therefore,\nthis collection of chains completely represents every possible chain that could have\nbeen run from infinitely long ago in the past.\nthe next problem is that we no longer have a single chain, and it seems that\nchain states at time 0 will differ. to remedy this multiplicity, we rely on the idea of\ncoupling. two chains on the same state space with the same transition probabilities\nhave coupled (or coalesced) at time t if they share the same state at time t. at this\npoint, the two chains will have identical probabilistic properties, due to the markov\nproperty and the equal transition probabilities. a third such chain could couple with\nthese two at time t or any time thereafter. thus, to eliminate the multiple chains\nintroduced above, we use an algorithm that ensures that once chains have coupled,\nthey follow the same sample path thereafter. further, we insist that all chains must\nhave coalesced by time 0. this algorithm will therefore yield one chain from t = 0\nonwards which is in the desired stationary distribution.\nto simplify presentation, we assume that x is unidimensional and has finite\nstate space with k states. neither assumption is necessary for cftp strategies more\ngeneral than the one we describe below.\n\n "}, {"Page_number": 274, "text": "265\nwe consider an ergodic markov chain with a deterministic transition rule q that\nupdates the current state of the chain, x(t), as a function of some random variable\nu(t+1). thus,\n\n8.5 perfect sampling\n\nx(t+1)\n\n= q#x(t), u(t+1)$ .\n\n(8.32)\n\nk\n\n(0)\n\n, . . . , x\n\n(\u22121)\n1\n(\u22121)\n\nfor example, a metropolis\u2013hastings proposal from a distribution with cumulative\ndistribution function f can be generated using q(x, u) = f\u22121(u), and a random walk\nproposalcanbegeneratedusing q(x, u) = x + u.in(8.32)weusedaunivariate u(t+1),\nbut,moregenerally,chaintransitionsmaybegovernedbyamultivariatevectoru(t+1).\nwe adopt the general case hereafter.\ncftp starts one chain from each state in the state space at some time \u03c4 < 0\nand transitions each chain forward using proposals generated by q. proposals are\naccepted using the standard metropolis\u2013hastings ratio. the goal is to find a starting\ntime \u03c4 such that the chains have all coalesced by time t = 0 when run forwards in time\nfrom t = \u03c4. this approach provides x(0), which is a draw from the desired stationary\ndistribution f.\nthe algorithm to find \u03c4 and thereby produce the desired chain is as follows.\n(t)\nlet x\nk be the random state at time t of the markov chain started in state k, with\nk = 1, . . . , k.\n1. let \u03c4 = \u22121. generate u(0). start a chain in each state of the state space at\n(\u22121)\ntime \u22121, namely x\nk , and run each chain forward to time 0 via the\n,u(0)$ for k = 1, . . . , k. if all k chains are in the same\nk = q#x\nupdate x\nstate at time 0, then the chains have coalesced and x(0) is a draw from f; the\nalgorithm stops.\n2. if the chains have not coalesced, then let \u03c4 = \u22122. generate u(\u22121). start a chain\nin each state of the state space at time \u22122, and run each chain forward to\n,u(\u22121)$. next, you must reuse the u(0)\n(\u22121)\ntime 0. to do this, let x\n,u(0)$. if all k chains are in the same\n(0)\ngenerated in step 1, so x\nstate at time 0, then the chains have coalesced and x(0) is a draw from f; the\nalgorithm stops.\n3. if the chains have not coalesced, move the starting time back to time \u03c4 = \u22123\nand update as above. we continue restarting the chains one step further back\nin time and running them forward to time 0 until we start at a \u03c4 for which all\nk chains have coalesced by time t = 0. at this point the algorithm stops. in\nevery attempt, it is imperative that the random updating variables be reused.\nspecifically, when starting the chains at time \u03c4, you must reuse the previously\ndrawnrandomnumberupdatesu(\u03c4+1),u(\u03c4+2) . . . ,u(0).alsonotethatthesame\nu(t) vector is used to update all k chains at the tth iteration.\npropp and wilson show that the value of x(0) returned from the cftp algorithm\nfor a suitable q is a realization of a random variable distributed according to the\nstationary distribution of the markov chain and that this coalescent value will be\n\nk = q#x\nk = q#x\n\n(\u22122)\n(\u22121)\n\nk\n\nk\n\n "}, {"Page_number": 275, "text": "266\n\nchapter 8 advanced topics in mcmc\n\niteration 1\n\niteration 2\n\niteration 3\n\ns3\n\ns2\n\ns1\n\n0\n\ns3\n\ns2\n\ns1\n\ns3\n\ns2\n\ns1\n\n\u22123\n\n\u22122\n\n\u22121\n\n0\n\n\u22123\n\n\u22122\n\n\u22121\n\n0\n\n\u22123\n\n\u2212 2\n\n\u22121\nt\n\nfigure 8.5 example of perfect sampling sample paths. see example 8.5 for details.\n\nt\n\nt\n\nproduced in finite time [520]. even if all chains coalesce before time 0, you must use\nx(0) as the perfect sampling draw; otherwise sampling bias is introduced.\nobtaining the perfect sampling draw x(0) from f is not sufficient for most uses.\ntypicallywedesireani.i.d. n-samplefrom f,eitherforsimulationortouseinamonte\ncarlo estimate of some expectation, \u00b5 =7 h(x)f(x) dx. a perfect i.i.d. sample from\nf can be obtained by running the cftp algorithm n times to generate n individual\nvalues for x(0). if you only want to ensure that the algorithm is, indeed, sampling\nfrom f, but independence is not required, you can run cftp once and continue to\nrun the chain forward from its state at time t = 0. while the first option is probably\npreferable, the second option may be more reasonable in practice, especially for cases\nwhere the cftp algorithm requires many iterations before coalescence is achieved.\nthese are only the two simplest strategies available for using the output of a perfect\nsampling algorithm; see also [474] and the references in [667].\nexample 8.5 (sample paths in a small state space)\nin the example shown in\nfigure 8.5, there are three possible states, s1, s2, s3. at iteration 1, a sample path\nis started from each of the three states at time \u03c4 = \u22121. a random update u(0) is\nk = q!sk, u(0)\" for k = 1,2,3. the paths have not all coalesced at\nselected, and x\ntime t = 0, so the algorithm moves to iteration 2. in iteration 2, the algorithm begins\nat time \u03c4 = \u22122. the transition rule for the moves from t = \u22122 to t = \u22121 is based on a\nnewly sampled update variable, u(\u22121). the transition rule for the moves from t = \u22121\nto t = 0 relies on the same u(0) value obtained previously in iteration 1. the paths\nhave not all coalesced at time t = 0, so the algorithm moves to iteration 3. here, the\nprevious draws for u(0) and u(\u22121) are reused and a new u(\u22122) is selected. in iteration\n3, all three sample paths visit state s2 at time t = 0, thus the paths have coalesced,\nand x(0) = s2 is a draw from the stationary distribution f.\n!\nseveral finer details of cftp implementation merit mention. first, note that\ncftp requires reuse of previously generated variables u(t) and the shared use of the\nsame u(t) realization to update all chains at time t. if the u(t) are not reused, the\nsamples will be biased. propp and wilson show an example where the regeneration\nof the u(t) at each time biases the chain toward the extreme states in an ordered state\nspace [520]. the reuse and sharing of past u(t) ensures that all chains coalesce by\nt = 0 when started at any time \u03c4\u2032 \u2264 \u03c4, where \u03c4 is the starting time chosen by cftp.\nmoreover, this practice ensures that the coalescent state at time 0 is the same for all\n\n(0)\n\n "}, {"Page_number": 276, "text": "8.5 perfect sampling\n\n267\nsuch chains in a given run, which enables the proof that cftp produces an exact draw\nfrom f.\nsecond, cftp introduces a dependence between the \u03c4 and x(0) it chooses.\ntherefore, bias can be induced if a cftp run is stopped prematurely before the\ncoupling time has been determined. suppose a cftp algorithm is run for a long\ntime during which coupling does not occur. if the computer crashes or an impa-\ntient user stops and then restarts the algorithm to find a coupling time, this will\ngenerally bias the sample toward those states in which coupling is easier. an alter-\nnative perfect sampling method known as fill\u2019s algorithm was designed to avoid this\nproblem [193].\nthird,ourdescriptionofthecftpalgorithmusesthesequenceofstartingtimes\n\u03c4 = \u22121,\u22122, . . . for successive cftp iterations. for many problems, this will be in-\nefficient. it may be more efficient to use the sequence \u03c4 = \u22121,\u22122,\u22124,\u22128,\u221216, . . .,\nwhich minimizes the worst case number of simulation steps required and nearly min-\nimizes the expected number of required steps [520].\nfinally, it may seem that this coupling strategy should work if the chains were\nrun forwards from time t = 0 instead of backwards; but that is not the case. to under-\nstand why, consider a markov chain for which some state x\u2032 has a unique predecessor.\nit is impossible for x\u2032 to occur at the random time of first coalescence. if x\u2032 occurred,\nthe chain must have already coalesced at the previous time, since all chains must\nhave been in the predecessor state. therefore the marginal distribution of the chain\nat the first coalescence time must assign zero probability to x\u2032 and hence cannot be\nthe stationary distribution. although this forward coupling approach fails, there is a\nclever way to modify the cftp construct to produce a perfect sampling algorithm for\na markov chain that only runs forward in time [666].\n\nstochastic monotonicity and sandwiching whenapplyingcftpto\n8.5.1.1\nachainwithavastfinitestatespaceoraninfinite(e.g.,continuous)statespace,itcanbe\nchallenging to monitor whether sample paths started from all possible elements in the\nstate space have coalesced by time zero. however, if the state space can be ordered\nin some way such that the deterministic transition rule q preserves the state space\nordering, then only the sample paths started from the minimum state and maximum\nstate in the ordering need to be monitored.\nlet x,y \u2208 s denote any two possible states of a markov chain exploring a\npossibly huge state space s. formally, s is said to admit the natural componentwise\npartial ordering, x \u2264 y, if xi \u2264 yi for i = 1, . . . , n and x,y \u2208 s. the transition rule\nq is monotone with respect to this partial ordering if q (x,u) \u2264 q (y,u) for all u\nwhen x \u2264 y. now, if there exist a minimum and a maximum element of the state\nspace s, so xmin \u2264 x \u2264 xmax for all x \u2208 s and the transition rule q is monotone, then\nan mcmc procedure that uses this q preserves the ordering of the states at each\ntime step. therefore, cftp using a monotone transition rule can be carried out by\nsimulating only two chains: one started at xmin and the other at xmax. sample paths\nfor chains started at all other states will be sandwiched between the paths started in\nthe maximum and minimum states. when the sample paths started in the minimum\nand maximum states have coalesced at time zero, coalescence of all the intermediate\nchains is also guaranteed. therefore, cftp samples from the stationary distribution\n\n "}, {"Page_number": 277, "text": "chapter 8 advanced topics in mcmc\n\n268\nat t = 0. many problems satisfy these monotonicity properties; one example is given\nin section 8.7.3.\nin problems where this form of monotonicity isn\u2019t possible, other related strate-\ngies can be devised [468, 473, 666]. considerable effort has been focused on devel-\noping methods to apply perfect sampling for specific problem classes, such as perfect\nmetropolis\u2013hastings independence chains [123], perfect slice samplers [466], and\nperfect sampling algorithms for bayesian model selection [337, 575].\nperfect sampling is currently an area of active research, and many extensions\nof the ideas presented here have been proposed. while this idea is quite promising,\nperfect sampling algorithms have not been widely implemented for problems of real-\nistic size. challenges in implementation and long coalescence times have sometimes\ndiscouraged large-scale realistic applications. nonetheless, the attractive properties\nof perfect sampling algorithms and continued research in this area will likely motivate\nnew and innovative mcmc algorithms for practical problems.\n\n8.6 markov chain maximum likelihood\nwe have presented markov chain monte carlo in the context of monte carlo integra-\ntion, with many bayesian examples. however, mcmc techniques can also be useful\nfor maximum likelihood estimation, particularly in exponential families [234, 505].\nconsider data generated from an exponential family model x \u223c f(\u00b7|\u03b8) where\n\nf(x|\u03b8) = c1(x)c2(\u03b8)exp{\u03b8ts(x)}.\n\n(8.33)\nhere \u03b8 = (\u03b81, . . . , \u03b8p) and s(x) = (s1(x), . . . , sp(x)) are vectors of canonical param-\neters and sufficient statistics, respectively. for many problems, c2(\u03b8) cannot be deter-\nmined analytically, so the likelihood cannot be directly maximized.\nsuppose that we generate x(1), . . . ,x(n) from an mcmc approach having\nf(\u00b7|\u03c8) as the stationary density, where \u03c8 is any particular choice for \u03b8 and f(\u00b7|\u03c8) is\nin the same exponential family as the data density. then it is easy to show that\n\nc2(\u03b8)\u22121\n\n= c2(\u03c8)\u22121> exp{(\u03b8 \u2212 \u03c8)ts(x)}f(x|\u03c8) dx.\nalthough the mcmc draws are dependent and not exactly from f(\u00b7|\u03c8),\n\n\u02c6k(\u03b8) =\n\n1\nn\n\nn6t=1\n\nexp((\u03b8 \u2212 \u03c8)ts(x(t))) \u2192\n\nc2(\u03c8)\nc2(\u03b8)\n\nas n \u2192 \u221e by the strong law of large numbers (1.46). therefore, a monte carlo\nestimator of the log likelihood given data x is\n(8.36)\nup to an additive constant. the maximizer of \u02c6l(\u03b8|x) converges to the maximizer of\nthe true log likelihood as n \u2192 \u221e. therefore, we take the monte carlo maximum\nlikelihood estimate of \u03b8 to be the maximizer of (8.36), which we denote \u02c6\u03b8\u03c8.\n\n\u02c6l(\u03b8|x) = \u03b8ts(x) \u2212 log \u02c6k(\u03b8),\n\n(8.34)\n\n(8.35)\n\n "}, {"Page_number": 278, "text": "8.7 example: mcmc for markov random fields\n\n269\n\nhence we can approximate the mle \u02c6\u03b8 using simulations from f(\u00b7|\u03c8) generated\nvia mcmc. of course, the quality of \u02c6\u03b8\u03c8 will depend greatly on \u03c8. analogously to\nimportance sampling, \u03c8 = \u02c6\u03b8 is best. in practice, however, we must choose one or\nmore values wisely, perhaps through adaptation or empirical estimation [234].\n\n8.7 example: mcmc for markov random fields\nwe offer here an introduction to bayesian analysis of markov random field models\nwith emphasis on the analysis of spatial or image data. this topic provides interesting\nexamples of many of the methods discussed in this chapter.\na markov random field specifies a probability distribution for spatially refer-\nenced random variables. markov random fields are quite general and can be used\nfor many lattice-type structures such as regular rectangular, hexagonal, and irregular\ngrid structures [128, 635]. there are a number of complex issues with markov ran-\ndom field construction that we do not attempt to resolve here. besag has published a\nnumber of key papers on markov random fields for spatial statistics and image anal-\nysis, including his seminal 1974 paper [35, 36, 40\u201343]. additional comprehensive\ncoverage of markov random fields is given in [128, 377, 412, 668].\nfor simplicity, we focus here on the application of markov random fields to\na regular rectangular lattice. for example, we might overlay a rectangular grid on a\nmap or image and label each pixel or cell in the lattice. the value for the ith pixel\nin the lattice is denoted by xi for i = 1, . . . , n, where n is finite. we will focus on\nbinary random fields where xi can take on only two values, 0 and 1, for i = 1, ..., n.\nit is generally straightforward to extend methods to the case where xi is continuous\nor takes on more than two discrete values [128].\nlet x\u03b4i define the set of x values for the pixels that are near pixel i. the pixels\nthat define \u03b4i are called the neighborhood of pixel i. the pixel xi is not in \u03b4i. a proper\nneighborhood definition must meet the condition that if pixel i is a neighbor of pixel j\nthen pixel j is a neighbor of pixel i. in a rectangular lattice, a first-order neighborhood\nis the set of pixels that are vertically and horizontally adjacent to the pixel of interest\n(see figure 8.6). a second-order neighborhood also includes the pixels diagonally\nadjacent from the pixel of interest.\nimaginethatthevalue xi forthe ithpixelisarealizationofarandomvariable xi.\na locally dependent markov random field specifies that the distribution of xi given\nthe remaining pixels, x\u2212i, is dependent only on the neighboring pixels. therefore,\nfor x\u2212i = x\u2212i,\n\nf (xi|x\u2212i) = f!xi|x\u03b4i\"\n\n(8.37)\nfor i = 1, . . . , n. assuming each pixel has a nonzero probability of equaling 0 or 1\nmeans that the so-called positivity condition is satisfied: that the minimal state space\nof x equals the cartesian product of the state spaces of its components. the positivity\ncondition ensures that the conditional distributions considered later in this section are\nwell defined.\n\n "}, {"Page_number": 279, "text": "270\n\nchapter 8 advanced topics in mcmc\n\nfirst order\n\nsecond order\n\ni\n\ni\n\nfigure 8.6 shaded pixels indicate a first-order and a second-order neighborhood of pixel\ni for a rectangular lattice.\n\nthe hammersley\u2013clifford theorem shows that the conditional distributions in\n(8.37)togetherspecifythejointdistributionofxuptoanormalizingconstant[35].for\nour discrete binary state space, this normalizing constant is the sum of f (x) over all\nx in the state space. this sum is not usually available by direct calculation, because\nthe number of terms is enormous. even for an unrealistically small 40 \u00d7 40 pixel\nimage where the pixels take on binary values, there are 21600 = 4.4 \u00d7 10481 terms in\nthe summation. bayesian mcmc methods provide a monte carlo basis for inference\nabout images, despite such difficulties. we describe below several approaches for\nmcmc analysis of markov random field models.\n\n8.7.1 gibbs sampling for markov random fields\nwe begin by adopting a bayesian model for analysis of a binary markov random\nfield. in the introduction to markov random fields above, we used xi to denote the\nvalue of the ith pixel. here we let xi denote the unknown true value of the ith pixel,\nwhere xi is treated as a random variable in the bayesian paradigm. let yi denote the\nobserved value for the ith pixel. thus x is a parameter vector and y is the data. in an\nimage analysis application, y is the degraded image and x is the unknown true image.\nin a spatial statistics application of mapping of plant or animal species distributions,\nyi = 0 might indicate that the species was not observed in pixel i during the sampling\nperiod and xi might denote the true (unobserved) presence or absence of the species\nin pixel i.\nthree assumptions are fundamental to the formulation of this model. first, we\nassume that observations are mutually independent given true pixel values. so the\njoint conditional density of y given x = x is\n\nf(y1, . . . , yn|x1, . . . , xn) =\n\nf(yi|xi),\n\nn?i=1\n\n(8.38)\n\n "}, {"Page_number": 280, "text": "8.7 example: mcmc for markov random fields\n\n271\nwhere f(yi|xi) is the density of the observed data in pixel i given the true value. thus,\nviewed as a function of x, (8.38) is the likelihood function. second, we adopt a locally\ndependent markov random field (8.37) to model the true image. finally, we assume\nthe positivity condition, defined above.\nthe parameters of the model are x1, . . . , xn, and the goal of the analysis is to\nestimate these true values. to do this we adopt a gibbs sampling approach. assume\nthe prior x \u223c f(x) for the parameters. the goal in the gibbs sampler, then, is to\nobtain a sample from the posterior density of x,\n(8.39)\n\nf (x|y) \u221d f (y|x) f(x).\n\none class of prior densities for x is given by\n\nf(x) \u221d exp\u23a7\u23a8\u23a9\u2212\n\nn6i\u223cj\n\n\u03c6(xi \u2212 xj)\u23ab\u23ac\u23ad\n\n,\n\n(8.40)\n\ni\n\nx\n\n(t+1)\n\nwhere i \u223c j denotes all pairs such that pixel i is a neighbor of pixel j, and \u03c6 is some\nfunction that is symmetric about 0 with \u03c6(z) increasing as |z| increases. equation\n(8.40) is called a pairwise difference prior. adopting this prior distribution based on\npairwise interactions simplifies computations but may not be realistic. extensions to\nallow for higher-order interactions have been proposed [635].\nthe gibbs sampler requires the derivation of the univariate conditional distri-\nbutions whose form follows from (8.37) to (8.39). the gibbs update at iteration t is\ntherefore\n\n\u2212i,y$ .\n\n***!x(t)\n\u2212i,y\" \u223c f#xi|x(t)\n\n(8.41)\na common strategy is to update each xi in turn, but it can be more computationally\nefficient to update the pixels in independent blocks. the blocks are determined by\nthe neighborhoods defined for a particular problem [40]. other approaches to block\nupdating for markov random field models are given in [382, 563].\nexample 8.6 (utah serviceberry distribution map) an important problem in\necology is the mapping of species distributions over a landscape [286, 584]. these\nmapshaveavarietyofuses,rangingfromlocalland-useplanningaimedatminimizing\nhuman development impacts on rare species to worldwide climate modeling. here we\nconsiderthedistributionofadeciduoussmalltreeorshrubcalledtheutahserviceberry\namelanchier utahensis in the state of colorado [414].\nwe consider only the westernmost region of colorado (west of approximately\n104\u25e6w longitude), a region that includes the rocky mountains. we binned the\npresence\u2013absence information into pixels that are approximately 8 \u00d7 8 km. this\ngrid consists of a lattice of 46 \u00d7 54 pixels, giving a total of n = 2484 pixels. the left\npanel in figure 8.7 shows presence and absence, where each black pixel indicates\nthat the species was observed in that pixel.\nin typical applications of this model, the true image is not available. however,\nknowing the true image allows us to investigate various aspects of modeling binary\nspatially referenced data in what follows. therefore, for purposes of illustration, we\n\n "}, {"Page_number": 281, "text": "272\n\nchapter 8 advanced topics in mcmc\n\nfigure 8.7 distribution of utah serviceberry in western colorado. the left panel is the\ntrue species distribution, and the right panel is the observed species distribution used in\nexample 8.6. black pixels indicate presence.\n\nwillusethesepixelwisepresence\u2013absencedataasatrueimageandconsiderestimating\nthis truth from a degraded version of the image. a degraded image is shown in the\nright panel of figure 8.7. we seek a map that reconstructs the true distribution of\nthis species using this degraded image, which is treated as the observed data y. the\nobserved data were generated by randomly selecting 30% of the pixels and switching\ntheircolors.sucherrorsmightariseinsatelliteimagesorothererror-proneapproaches\nto species mapping.\nlet xi = 1 indicate that the species is truly present in pixel i. in a species map-\nping problem such as this one, such simple coding may not be completely appropriate.\nfor example, a species may be present only in a portion of pixel i, or several sites\nmay be included in one pixel, and thus we might consider modeling the proportion\nof sites in each pixel where the species was observed to be present. for simplicity,\nwe assume that this application of markov random fields is more akin to an image\nanalysis problem where xi = 1 indicates that the pixel is black.\n\nwe consider the simple likelihood function arising from the data density\n\nf(y|x) \u221d exp2\u03b1\n\n1{yi=xi}c\n\n(8.42)\n\nn6i=1\n\nfor xi \u2208 {0,1}. the parameter \u03b1 can be specified as a user-selected constant or esti-\nmated by adopting a prior for it. we adopt the former approach here, setting \u03b1 = 1.\n\nwe assume the pairwise difference prior density for x given by\n\nf(x) \u221d exp\u23a7\u23a8\u23a9\n\n\u03b2\n\n1\n\nn6i\u223cj\n\n{xi=xj}\u23ab\u23ac\u23ad\n\n(8.43)\n\nfor x \u2208 s = {0,1}46\u00d754. we consider a first-order neighborhood, so summation over\ni \u223c j in (8.43) indicates summation over the horizontally and vertically adjacent\npixels of pixel i, for i = 1, . . . , n. equation (8.43) introduces the hyperparame-\nter \u03b2, which can be assigned a hyperprior or specified as a constant. usually \u03b2 is\n\n "}, {"Page_number": 282, "text": "8.7 example: mcmc for markov random fields\n\n273\n\nfigure 8.8 estimated posterior mean of x for the gibbs sampler analysis in example 8.6.\n\ni\n\n(t)\n\n(t)\n\n{x\n\n(t+1)\n\n(8.44)\n\nj =0} \u2212 1\n{x\n\nj =1}$)$\u22121\n\n\u2212i,y$\n= 1**x(t)\n\nrestricted to be positive to encourage clustering of similar-colored pixels. here we set\n\u03b2 = 0.8. sensitivity analysis to determine the effect of chosen values for \u03b1 and \u03b2 is\nrecommended.\nassuming(8.42)and(8.43),theunivariateconditionaldistributionfor xi|x\u2212i,y\nis bernoulli. thus during the (t + 1)th cycle of the gibbs sampler, the ith pixel is set\nto 1 with probability\np#x\n=#1 + exp(\u03b1#1{yi=0} \u2212 1{yi=1}$ + \u03b26i\u223cj#1\nfor i = 1, . . . , n. recall that\n\u2212i =!x\n\nso neighbors are always assigned their most recent values as soon as they become\navailable within the gibbs cycle.\nfigure 8.8 gives the posterior mean probability of presence for the utah ser-\nviceberry in western colorado as estimated using the gibbs sampler described above.\nfigure 8.9 shows that the mean posterior estimates from the gibbs sampler success-\nfully discriminate between true presence and absence. indeed, if pixels with posterior\nmean of 0.5 or larger are converted to black and pixels with posterior mean smaller\nthan 0.5 are converted to white, then 86% of the pixels are labeled correctly.\n!\nthe model used in example 8.6 is elementary, ignoring many of the important\nissues that may arise in the analysis of such spatial lattice data. for example, when\nthe pixels are created by binning spatially referenced data, it is unclear how to code\nthe observed response for pixel i if the species was observed to be present in some\nportions of it and not in other portions.\n\np\",\n(t)\ni+1, . . . , x(t)\n\n(t+1)\ni\u22121 , x\n\n(t+1)\n1\n\n, . . . , x\n\nx(t)\n\n "}, {"Page_number": 283, "text": "274\n\nchapter 8 advanced topics in mcmc\n\n)\n1\n=\n\ni\n\nx\n(\np\nd\ne\nt\na\nm\n\ni\nt\ns\ne\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n\nxi\n\n0=\n\ntrue absence\n\nxi\n\n1=\n\ntrue presence\n\nfigure 8.9 boxplots of posterior mean estimates of p[xi = 1] for example 8.6. averaging\npixel-specific sample paths from the gibbs sampler provides an estimate of p[xi = 1] for each\ni. the boxplots show these estimates split into two groups corresponding to pixels where the\nserviceberry was truly present and pixels where it wasn\u2019t.\n\na model that addresses this problem uses a latent binary spatial process over\nthe region of interest [128, 217]. let \u03bb(s) denote a binary process over the image\nregion, where s denotes coordinates. then the proportion of pixel i that is occupied\nby the species of interest is given by\n\n1\n\n|ai|>s within pixel i\n\npi =\n\n1{\u03bb(s)=1} ds,\n\n(8.45)\nwhere |ai| denotes the area of pixel i. the yi|xi are assumed to be conditionally\nindependent bernoulli trials with probability of detecting presence given by pi, so\np[yi = 1|xi = 1] = pi. this formalization allows for direct modeling when pixels\nmay contain a number of sites that were sampled. a more complex form of this model\nis described in [217]. we may also wish to incorporate covariate data to improve our\nestimates of species distributions. for example, the bernoulli trials may be modeled\nas having parameters pi for which\n\nlog< pi\n\n1 \u2212 pi= = wt\n\n(8.46)\nwhere wi is a vector of covariates for the ith pixel, \u03b2 is the vector of coefficients\nassociated with the covariates, and \u03b3i is a spatially dependent random effect. these\nmodels are popular in the field of spatial epidemiology; see [44, 45, 411, 503].\n\ni \u03b2 + \u03b3i,\n\n8.7.2 auxiliary variable methods for markov random fields\nalthough convenient, the gibbs sampler implemented as described in section 8.7.1\ncan have poor convergence properties. in section 8.3 we introduced the idea of\n\n!\n "}, {"Page_number": 284, "text": "8.7 example: mcmc for markov random fields\n\n275\n\nfigure 8.10 illustration of the swendsen\u2013wang algorithm.\n\nincorporating auxiliary variables to improve convergence and mixing of markov\nchain algorithms. for binary markov random field models, the improvement can be\nprofound.\none notable auxiliary variable technique is the swendsen\u2013wang algorithm\n[174, 621]. as applied to binary markov random fields, this approach creates a\ncoarser version of an image by clustering neighboring pixels that are colored simi-\nlarly. each cluster is then updated with an appropriate metropolis\u2013hastings step. this\ncoarsening of the image allows for faster exploration of the parameter space in some\napplications [328].\nin the swendsen\u2013wang algorithm, clusters are created via the introduction of\nbond variables, uij, for each adjacency i \u223c j in the image. clusters consist of bonded\npixels. adjacent like-colored pixels may or may not be bonded, depending on uij.\nlet uij = 1 indicate that pixels i and j are bonded, and uij = 0 indicate that they\nare not bonded. the bond variables uij are assumed to be conditionally independent\ngiven x = x. let u denote the vector of all the uij.\nloosely speaking, the swendsen\u2013wang algorithm alternates between growing\nclusters and coloring them. figure 8.10 shows one cycle of the algorithm applied to\na 4 \u00d7 4 pixel image. the left panel in figure 8.10 shows the current image and the\nset of all possible bonds for a 4 \u00d7 4 image. the middle panel shows the bonds that\nwere generated at the start of the next iteration of the swendsen\u2013wang algorithm. we\nwill see below that bonds between like-colored pixels are generated with probability\n1 \u2212 exp{\u2212\u03b2}, so like-colored neighbors are not forced to be bonded. connected sets\nof bonded pixels form clusters. we\u2019ve drawn boxes around the five clusters in the\nmiddle panel of figure 8.10. this shows the coarsening of the image allowed by the\nswendsen\u2013wang algorithm. at the end of each iteration, the color of each cluster is\nupdated: clusters are randomly recolored in a way that depends upon the posterior\ndistribution for the image. the updating produces the new image in the right panel in\nfigure 8.10. the observed data y are not shown here.\nformally, the swendsen\u2013wang algorithm is a special case of a gibbs sampler\nthat alternates between updates to x|u and u|x. it proceeds as follows:\n1. draw independent bond variables\n\n(t+1)\n\nij\n\nu\n\n*** x(t)\n\n\u223c unif,0,exp<\u03b210x\n\n(t)\ni =x\n\n(t)\n\nj 1=-\n\n "}, {"Page_number": 285, "text": "276\n\nchapter 8 advanced topics in mcmc\n\n(t+1)\n\nij\n\nij\n\n(t+1)\n\ncan exceed 1 only if x\n> 1 with probability 1 \u2212 exp{\u2212\u03b2}. when u\n\nfor all i \u223c j adjacencies. note that u\nin this case u\ndeclare the ith and jth pixels to be bonded for iteration t + 1.\n2. sample x(t+1)**u(t+1) \u223c f! \u00b7 |u(t+1)\", where\nn6i=1\n1(0\u2264u\n\nf#x|u(t+1)$ \u221dexp(\u03b1\n\u00d7?i\u223cj\n\n1{yi=xi})\nij \u2264exp(\u03b21{xi=xj})).\n\n(t+1)\n\n(t)\ni = x\n(t+1)\n\n(t)\nj , and\n> 1, we\n\nij\n\n(8.47)\n\nnote that (8.47) forces the color of each cluster to be updated as a single unit.\n\n3. increment t and return to step 1.\n\nthusforoursimplemodel,pixelpairswiththesamecolorarebondedwithprobability\n1 \u2212 exp{\u2212\u03b2}.thebondvariablesdefineclustersofpixels,witheachclusterconsisting\nof a set of pixels that are interlinked with at least one bond. each cluster is updated\nindependently with all pixels in the cluster taking on the same color. updates in (8.47)\nare implemented by simulating from a bernoulli distribution where the probability of\ncoloring a cluster of pixels, c, black is\n\n(8.48)\n\nexp0\u03b15i\u2208c 1{yi=1}1\n\nexp0\u03b15i\u2208c 1{yi=0}1 + exp0\u03b15i\u2208c 1{yi=1}1 .\n\nthe local dependence structure of the markov random field is decoupled from the\ncoloring decision given in (8.48), thereby potentially enabling faster mixing.\nexample 8.7 (utah serviceberry distributions, continued)\nto compare the\nperformance of the gibbs sampler and the swendsen\u2013wang algorithm, we return\nto example 8.6. for this problem the likelihood has a dominant influence on the\nposterior. thus to highlight the differences between the algorithms, we set \u03b1 = 0 to\nunderstand what sort of mixing can be enabled by the swendsen\u2013wang algorithm.\nin figure 8.11, both algorithms were started in the same image in iteration 1, and\nthe three subsequent iterations are shown. the swendsen\u2013wang algorithm produces\nimages that vary greatly over iterations, while the gibbs sampler produces images\nthat are quite similar. in the swendsen\u2013wang iterations, large clusters of pixels switch\ncolors abruptly, thereby providing faster mixing.\nwhen the likelihood is included, there are fewer advantages to the swendsen\u2013\nwang algorithm when analyzing the data from example 8.6. for the chosen \u03b1 and \u03b2,\nclusters grow large and change less frequently than in figure 8.11. in this application,\nsequential images from a swendsen\u2013wang algorithm look quite similar to those for a\ngibbs sampler, and the differences between results produced by the swendsen\u2013wang\nalgorithm and the gibbs sampler are small.\n!\nexploiting a property called decoupling, the swendsen\u2013wang algorithm grows\nclusters without regard to the likelihood, conditional on x(t). the likelihood and\n\n "}, {"Page_number": 286, "text": "8.7 example: mcmc for markov random fields\n\n277\n\niteration 2\n\niteration 3\n\niteration 4\n\ngibbs\n\niteration 1\nswendsen\u2013wang\n\niteration 1\n\niteration 2\n\niteration 3\n\niteration 4\n\nfigure 8.11 comparison between gibbs sampling and the swendsen\u2013wang algorithm\nsimulating a markov random field. iteration 1 is the same for both algorithms. see example 8.7\nfor details.\n\nimage prior terms are separated in steps 1 and 2 of the algorithm. this feature is\nappealing because it can improve mixing rates in mcmc algorithms. unless \u03b1 and \u03b2\nare carefully chosen, however, decoupling may not be helpful. if clusters tend to grow\nlarge but change color very infrequently, the sample path will consist of rare drastic\nimage changes. this constitutes poor mixing. further, when the posterior distribution\nis highly multimodal, both the gibbs sampler and the swendsen\u2013wang algorithm can\nmiss potential modes if the chain is not run long enough. a partial decoupling method\nhas been proposed to address such problems, and offers some potential advantages\nfor challenging imaging problems [327, 328].\n\n8.7.3 perfect sampling for markov random fields\nimplementing standard perfect sampling for a binary image problem would require\nmonitoring sample paths that start from all possible images. this is clearly impossible\neven in a binary image problem of moderate size. in section 8.5.1.1 we introduced the\nideaofstochasticmonotonicitytocopewithlargestatespaces.wecanapplythisstrat-\negytoimplementperfectsamplingforthebayesiananalysisofmarkovrandomfields.\nto exploit the stochastic monotonicity strategy, the states must be partially\nordered, so x \u2264 y if xi \u2264 yi for i = 1, . . . , n and for x,y \u2208 s. in the binary image\nproblem, such an ordering is straightforward. if s = {0,1}n, define x \u2264 y if yi = 1\nwhenever xi = 1 for all i = 1, . . . , n. if the deterministic transition rule q maintains\nthis partial ordering of states, then only the sample paths that start from all-black and\nall-white images need to be monitored for coalescence.\nexample 8.8 (sandwiching binary images)\nfigure 8.12 shows five iterations\nof a gibbs sampler cftp algorithm for a 4 \u00d7 4 binary image with order-preserving\n\n "}, {"Page_number": 287, "text": "278\n\nchapter 8 advanced topics in mcmc\n\nt =!\n\n\u2013 1000\n\nt = \u2212 400\n\nt = \u2212 399\n\nt = \u2212 398\n\nt = 0\n\nfigure 8.12 sequence of images from a perfect sampling algorithm for a binary image\nproblem. see example 8.8 for details.\n\ni\n\n(\u22121000)\n\npixelwiseupdates.thesamplepathinthetoprowstartsatiteration \u03c4 = \u22121000,where\nthe image is all black. in other words, x\n= 1 for i = 1, . . . ,16. the sample path\ninthebottomrowstartsatallwhite.thepathstartingfromallblackistheupperbound\nand the path starting from all white is the lower bound used for sandwiching.\nafter some initial iterations, we examine the paths around t = \u2212400. in the\nlower sample path, the circled pixel at iteration t = \u2212400 changed from white to\nblack at t = \u2212399. monotonicity requires that this pixel change to black in the upper\npath too. this requirement is implemented directly via the monotone update function\nq. note, however, that changes from white to black in the upper image do not compel\nthe same change in the lower image; see, for example, the pixel to the right of the\ncircled pixel.\nchanges from black to white in the upper image compel the same change in the\nlower image. for example, the circled pixel in the upper sample path at t = \u2212399 has\nchanged from black to white at t = \u2212398, thereby forcing the corresponding pixel in\nthe lower sample path to change to white. a pixel change from black to white in the\nlower image does not necessitate a like change to the upper image.\nexamination of the pixels in these sequences of images shows that pixelwise\nimage ordering is maintained over the simulation. at iteration t = 0, the two sample\npaths have coalesced. therefore a chain started at any image at \u03c4 = \u22121000 must also\nhave coalesced to the same image by iteration t = 0. the image shown at t = 0 is a\nrealization from the stationary distribution of the chain.\n!\nexample 8.9 (utah serviceberry distribution, continued)\nthe setup for the\ncftp algorithm for the species distribution mapping problem closely follows the\ndevelopment of the gibbs sampler described in example 8.6. to update the ith pixel\nat iteration t + 1, generate u(t+1) from unif(0,1). then the update is given by\n\n(t+1)\n\ni\n\nx\n\n= q#x(t)\n= 21\n\n\u2212i, u(t+1)$\nif u(t+1) < p#x\n\notherwise,\n\n0\n\n(t+1)\n\ni\n\n= 1***x(t)\n\u2212i,y$ ,\n\n(8.49)\n\n!\n!\n!\n!\n "}, {"Page_number": 288, "text": "8.7 example: mcmc for markov random fields\n\n279\n\ni\n\n(t+1)\n\n= 1***x(t)\n\nwhere pd x\n\u2212i,ye is given in (8.44). such updates maintain the partial\nordering of the state space. therefore, the cftp algorithm can be implemented\nby starting at two initial images: all black and all white. these images are mon-\nitored, and the cftp algorithm proceeds until they coalesce by iteration t = 0.\nthe cftp algorithm has been implemented for similar binary image problems in\n[165, 166].\n!\n\nproblems\n8.1. one approach to bayesian variable selection for linear regression models is described\nin section 8.2.1 and further examined in example 8.3. for a bayesian analysis for the\nmodel in equation (8.20), we might adopt the normal\u2013gamma conjugate class of priors\n\u03b2|mk \u223c n(\u03b1mk , \u03c32vmk) and \u03bd\u03bb/\u03c32 \u223c \u03c72\n\u03bd. show that the marginal density of y|mk is\ngiven by\n\n\u0001((\u03bd + n)/2)(\u03bd\u03bb)\u03bd/2\n\u03c0n/2\u0001(\u03bd/2)|i + xmkvmkxt\n\u00d7d\u03bb\u03bd +!y \u2212 xmk \u03b1mk\"t!i + xmkvmkxt\n\nmk|1/2\n\nmk\"\u22121!y \u2212 xmk \u03b1mk\"e\u2212(\u03bd+n)/2\n\n,\n\nwhere xmk is the design matrix, \u03b1mk is the mean vector, and vmk is the covariance\nmatrix for \u03b2mk for the model mk.\n\n8.2. consider the cftp algorithm described in section 8.5.\n\na. constructanexamplewithafinitestatespacetowhichboththemetropolis\u2013hastings\nalgorithmandthecftpalgorithmcanbeappliedtosimulatefromsomemultivariate\nstationary distribution f. for your example, define both the metropolis\u2013hastings\nratio (7.1) and the deterministic transition rule (8.32), and show how these quantities\nare related.\n\nb. construct an example with a state space with two elements so that the cftp algo-\nrithm can be applied to simulate from some stationary distribution f. define two\ndeterministic transition rules of the form in (8.32). one transition rule, q1, should\nallow for coalescence in one iteration, and the other transition rule, q2, should be\ndefined so that coalescence is impossible. which assumption of the cftp algorithm\nis violated for q2?\nc. constructanexamplewithastatespacewithtwoelementsthatshowswhythecftp\nalgorithm cannot be started at \u03c4 = 0 and run forward to coalescence. this should\nillustrate the argument mentioned in the discussion after example 8.5 (page 266).\n8.3. suppose we desire a draw from the marginal distribution of x that is determined by the\n\nassumptions that \u03b8 \u223c beta(\u03b1, \u03b2) and x|\u03b8 \u223c bin(n, \u03b8) [96].\na. show that \u03b8|x \u223c beta(\u03b1 + x, \u03b2 + n \u2212 x).\nb. what is the marginal expected value of x?\nc. implementagibbssamplertoobtainajointsampleof(\u03b8, x),using x(0) = 0, \u03b1 = 10,\n\n\u03b2 = 5, and n = 10.\n\n "}, {"Page_number": 289, "text": "280\n\nchapter 8 advanced topics in mcmc\n\nd. let u(t+1) and v (t+1) be independent unif(0,1) random variables. then the transition\n\nrule from x(t) = x(t) to x(t+1) can be written as\n\nx(t+1) = q(x(t), u(t+1), v (t+1))\nbin!v (t+1); n, f\u22121\n= f\u22121\n\nbeta!u(t+1); \u03b1 + x(t), \u03b2 + n \u2212 x(t)\"\" ,\n\nd\n\n(p; \u00b51, \u00b52) is the inverse cumulative distribution function of the distri-\nwhere f\u22121\nbution d with parameters \u00b51 and \u00b52, evaluated at p. implement the cftp algorithm\nfrom section 8.5.1, using the transition rule given in (8.50), to draw a perfect sam-\nple for this problem. decrement \u03c4 by one unit each time the sample paths do not\ncoalesce by time 0. run the function 100 times to produce 100 draws from the\nstationary distribution for \u03b1 = 10, \u03b2 = 5, and n = 10. make a histogram of the 100\nstarting times (the finishing times are all t = 0, by construction). make a histogram\nof the 100 realizations of x(0). discuss your results.\ne. run the function from part (d) several times for \u03b1 = 1.001, \u03b2 = 1, and n = 10.\npick a run where the chains were required to start at \u03c4 = \u221215 or earlier. graph the\nsample paths (from each of the 11 starting values) from their starting time to t = 0,\nconnecting sequential states with lines. the goal is to observe the coalescence as in\nthe right panel in figure 8.5. comment on any interesting features of your graph.\nf. run the algorithm from part (d) several times. for each run, collect a perfect chain\nof length 20 (i.e., once you have achieved coalescence, don\u2019t stop the algorithm\nat t = 0, but continue the chain from t = 0 through t = 19). pick one such chain\nhaving x(0) = 0, and graph its sample path for t = 0, . . . ,19. next, run the gibbs\nsamplerfrompart(c)through t = 19startingwith x(0) = 0.superimposethesample\npath of this chain on your existing graph, using a dashed line.\ni. is t = 2 sufficient burn-in for the gibbs sampler? why or why not?\nii. of the two chains (cftp conditional on x(0) = 0 and gibbs starting from\nx(0) = 0), which should produce subsequent variates x(t) for t = 1,2, . . . whose\ndistribution more closely resembles the target? why does this conditional cftp\nchain fail to produce a perfect sample?\n\n8.4. consider the one-dimensional black-and-white image represented by a vector of zeros\n\nand ones. the data (observed image) are\n\n10101111010000101000010110101001101\n\nfor the 35 pixels y = (y1, . . . , y35). suppose the posterior density for the true image\nx is given by\n\n\u03b21{xi=xj}c ,\n\nwhere\n\nf(x|y) \u221d exp2 356i=1\n\n\u03b1(xi, yi)cexp26i\u223cj\n\u03b1(xi, yi) =<log{2/3} if xi = yi,\nlog{1/3} if xi /= yi.\ndrawn according to uij|x \u223c unif!0,exp0\u03b21{xi=xj}1\".\n\nconsider the swendsen\u2013wang algorithm for this problem where the bond variable is\n\n "}, {"Page_number": 290, "text": "8.7 example: mcmc for markov random fields\n\n281\n\n0\n4\n\n0\n3\n\ne\nm\nt\n\ni\n\n0\n2\n\n0\n1\n\n0\n\n0\n\n10\n\n20\n\npixel\n\n30\n\nfigure 8.13 forty gibbs sampler iterates for problem 8.4, with \u03b2 = 1.\na. implement the swendsen\u2013wang algorithm described above with \u03b2 = 1. create a\nchain of length 40, starting from the initial image x(0) equal to the observed data.\nnote that the entire sequence of images can be displayed in a two-dimensional\ngraph as shown in figure 8.13. this figure was created using a gibbs sampler. using\nyour output from your implementation of the swendsen\u2013wang algorithm, create a\ngraph analogous to figure 8.13 for your swendsen\u2013wang iterations. comment on\nthe differences between your graph and figure 8.13.\nb. investigate the effect of \u03b2 by repeating part (a) for \u03b2 = 0.5 and \u03b2 = 2. comment\non the differences between your graphs and the results in part (a).\nc. investigate the effect of the starting value by repeating part (a) for three different\nstarting values: first with x(0) = (0, . . . ,0), second with x(0) = (1, . . . ,1), and third\nwith x(0)\ni = 1 for i = 18, . . . ,35. compare the results\nof these trials with the results from part (a).\nd. what would be a good way to produce a single best image to represent your estimate\n\ni = 0 for i = 1, . . . ,17 and x(0)\n\nof the truth?\n\n8.5. data corresponding to the true image and observed images given in figure 8.14 are\navailable on the website for this book. the true image is a binary 20 \u00d7 20 pixel image\nwith prior density given by\n\nf!xi|x\u03b4i\" = n,\u00afx\u03b4i ,\n\n\u03c32\n\n\u03bdi-\n\nfor i = 1, . . . , n, where \u03bdi is the number of neighbors in the neighborhood \u03b4i of xi and\n\u00afx\u03b4i is the mean value of the neighbors of the ith pixel. this density promotes local\ndependence. the observed image is a gray scale degraded version of the true image\nwith noise that can be modeled via a normal distribution. suppose the likelihood is\n\n "}, {"Page_number": 291, "text": "282\n\nchapter 8 advanced topics in mcmc\n\n0\n2\n\n5\n1\n\n0\n1\n\n5\n\n0\n\n0\n2\n\n5\n1\n\n0\n1\n\n5\n\n0\n\n0\n\n5\n\n10\n\n15\n\n20\n\n0\n\n5\n\n10\n\n15\n\n20\n\nfigure 8.14 images for problem 8.5. the left panel is the true image, and the right panel\nis an observed image.\n\ngiven by\n\nf (yi|xi) = n!xi, \u03c32\"\n\nfor i = 1, . . . , n.\na. prove that univariate conditional posterior distribution used for gibbs sampling for\nthis problem is given by\n\nf (xi|x\u2212i,y) = n, 1\n\n\u03bdi + 1 yi +\n\n\u03bdi\n\n\u03bdi + 1 \u00afx\u03b4i ,\n\n\u03c32\n\n\u03bdi + 1- .\n\nb. starting with the initial image x(0) equal to the observed data image, and using \u03c3 = 5\nand a second-order neighborhood, use the gibbs sampler (with no burn-in period\nor subsampling) to generate a collection of 100 images from the posterior. do not\ncount an image as new until an update has been proposed for each of its pixels\n(i.e., a full cycle). record the data necessary to make the following plots: the data\nimage, the first image sampled from the posterior distribution (x(1)), the last image\nsampled from the posterior distribution (x(100)), and the mean image.\n\nhints:\n\u2022 dealing with the edges is tricky because the neighborhood size varies. you may\nfind it convenient to create a matrix with 22 rows and 22 columns consisting of\nthe observed data surrounded on each of the four sides by a row or column of\nzeros. if you use this strategy, be sure that this margin area does not affect the\nanalysis.\n\u2022 plotx(t) attheendofeachfullcyclesothatyoucanbetterunderstandthebehavior\nof your chain.\nc. fill in the rest of a 2 \u00d7 3 factorial design with runs analogous to (b), crossing the\nfollowing factors and levels:\n\u2022 neighborhood structure chosen to be (i) first-order neighborhoods or (ii) second-\norder neighborhoods.\n\n "}, {"Page_number": 292, "text": "8.7 example: mcmc for markov random fields\n\n283\n\u2022 pixelwise error chosen to have variability given by (i) \u03c3 =2, (ii) \u03c3 =5, or\n(iii) \u03c3 =15.\nprovide plots and detailed comments comparing the results from each design point\nin this experiment.\nd. repeat a run analogous to part (b) once more, but this time using the initial starting\nimage x(0) equal to 57.5 (the true posterior mean pixel color) everywhere, for \u03c3 = 5\nand a first-order neighborhood. discuss your results and their implications for the\nbehavior of the chain.\n\n "}, {"Page_number": 293, "text": "p a r t iii\nbootstrapping\n\nin the previous four chapters, we explored how to estimate expectations of\nrandom variables. a mean is never enough however. ideally we would like to\nknow the entire probability distribution of the variable.\n\nbootstrapping is a computational intensive method that allows re-\nsearchers to simulate the distribution of a statistic. the idea is to repeatedly\nresample the observed data, each time producing an empirical distribution\nfunction from the resampled data. for each resampled data set\u2014or equiva-\nlently each empirical distribution function\u2014a new value of the statistic can\nbe computed, and the collection of these values provides an estimate of the\nsampling distribution of the statistic of interest. in this manner, the method\nallows you to \u201cpull yourself up by your bootstraps\u201d (an old idiom, popularized\nin america, that means to improve your situation without outside help). boot-\nstrapping is nonparametric by nature, and there is a certain appeal to letting\nthe data speak so freely.\n\nbootstrapping was first developed for independent and identically dis-\ntributed data, but this assumption can be relaxed so that bootstrap estimates\nfrom dependent data such as regression residuals or time series data is pos-\nsible. we will explore bootstrapping methods in both the independent and\ndependent cases, along with approaches for improving performance using\nmore complex variations.\n\ncomputational statistics, second edition. geof h. givens and jennifer a. hoeting.\n\u00a9 2013 john wiley & sons, inc. published 2013 by john wiley & sons, inc.\n\n285\n\n "}, {"Page_number": 294, "text": "chapter 9\nbootstrapping\n\ni=1 xi/n.\n\n9.1 the bootstrap principle\nlet \u03b8 = t(f) be an interesting feature of a distribution function, f, expressed\nas a functional of f. for example, t(f) =! z df(z) is the mean of the distri-\nbution. let x1, . . . ,xn be data observed as a realization of the random variables\nx1, . . . ,xn \u223c i.i.d. f. in this chapter, we use x \u223c f to denote that x is distributed\nwith density function f having corresponding cumulative distribution function f. let\nx = {x1, . . . ,xn} denote the entire dataset.\nif\"f is the empirical distribution function of the observed data, then an estimate\nof \u03b8 is\"\u03b8 = t(\"f). for example, when \u03b8 is a univariate population mean, the estimator\nis the sample mean,\"\u03b8 =! z d\"f(z) =#n\nstatistical inference questions are usually posed in terms of t(\"f) or some\nr(x, f), a statistical function of the data and their unknown distribution function\nf. for example, a general test statistic might be r(x, f) =$t(\"f) \u2212 t(f)%&s(\"f),\nwhere s is a functional that estimates the standard deviation of t(\"f).\nthedistributionoftherandomvariable r(x, f)maybeintractableoraltogether\nunknown. this distribution also may depend on the unknown distribution f. the\nbootstrap provides an approximation to the distribution of r(x, f) derived from the\nempirical distribution function of the observed data (itself an estimate of f) [175,\n177]. several thorough reviews of bootstrap methods have been published since its\nintroduction [142, 181, 183].\nlet x\u2217 denote a bootstrap sample of pseudo-data, which we will call a pseudo-\ndataset. the elements of x\u2217 = {x\u22171, . . . ,x\u2217n} are i.i.d. random variables with dis-\ntribution \"f. the bootstrap strategy is to examine the distribution of r(x\u2217,\"f), that\nis, the random variable formed by applying r to x\u2217. in some special cases it is\npossible to derive or estimate the distribution of r(x\u2217,\"f) through analytical means\n(see example 9.1 and problems 9.1 and 9.2). however, the usual approach is via\nsimulation, as described in section 9.2.1.\nexample 9.1 (simple illustration) suppose n = 3 univariate data points, namely\n{x1, x2, x3} = {1,2,6}, are observed as an i.i.d. sample from a distribution f that has\nmean \u03b8. at each observed data value,\"f places mass 1\n3. suppose the estimator to be\nbootstrapped is the sample mean\"\u03b8, which we may write as t(\"f) or r(x, f), where\n\nr does not depend on f in this case.\n\ncomputational statistics, second edition. geof h. givens and jennifer a. hoeting.\n\u00a9 2013 john wiley & sons, inc. published 2013 by john wiley & sons, inc.\n\n287\n\n "}, {"Page_number": 295, "text": "288\n\nchapter 9 bootstrapping\n\ntable 9.1 possible bootstrap pseudo-datasets from {1, 2, 6} (ignoring\norder), the resulting values of!\u0002\u2217 = t(!f\u2217), the probability of each outcome\nin the bootstrapping experiment (p\u2217\"!\u0002\u2217#), and the observed relative frequency\n\nin 1000 bootstrap iterations.\n\nx\u2217\n1 1 1\n1 1 2\n1 2 2\n2 2 2\n1 1 6\n1 2 6\n2 2 6\n1 6 6\n2 6 6\n6 6 6\n\n\"\u03b8\u2217\n\n3/3\n4/3\n5/3\n6/3\n8/3\n9/3\n10/3\n13/3\n14/3\n18/3\n\np\u2217$\"\u03b8\u2217%\n\n1/27\n3/27\n3/27\n1/27\n3/27\n6/27\n3/27\n3/27\n3/27\n1/27\n\nobserved\nfrequency\n36/1000\n101/1000\n123/1000\n25/1000\n104/1000\n227/1000\n131/1000\n111/1000\n102/1000\n40/1000\n\nlet x\u2217 = {x\u22171, x\u22172, x\u22173} consist of elements drawn i.i.d. from \"f. there are\n33 = 27 possible outcomes for x\u2217. let\"f\u2217 denote the empirical distribution function\nof such a sample, with corresponding estimate\"\u03b8\u2217 = t(\"f\u2217). since\"\u03b8\u2217 does not depend\non the ordering of the data, it has only 10 distinct possible outcomes. table 9.1 lists\nthese.\nin table 9.1, p\u2217$\"\u03b8\u2217% represents the probability distribution for\"\u03b8\u2217 with respect\nto the bootstrap experiment of drawing x\u2217 conditional on the original observations.\nto distinguish this distribution from f, we will use an asterisk when referring to such\nconditional probabilities or moments, as when writing p\u2217$\"\u03b8\u2217 \u2264\nthe bootstrap principle is to equate the distributions of r(x, f) and r(x\u2217,\"f).\nin this example, that means we base inference on the distribution of\"\u03b8\u2217. this distri-\nbution is summarized in the columns of table 9.1 labeled\"\u03b8\u2217 and p\u2217$\"\u03b8\u2217%. so, for\nusing quantiles of the distribution of\"\u03b8\u2217. the point estimate is still calculated from\nthe observed data as\"\u03b8 =\n\n27 (roughly 93%) confidence interval for \u03b8 is (4\n\nexample, a simple bootstrap 25\n\n3 , 14\n3 )\n!\n\n3% =\n\n8\n27.\n\n9\n3.\n\n6\n\n9.2 basic methods\n\n9.2.1 nonparametric bootstrap\nfor realistic sample sizes the number of potential bootstrap pseudo-datasets is very\nlarge, so complete enumeration of the possibilities is not practical. instead, b in-\ndependent random bootstrap pseudo-datasets are drawn from the empirical distribu-\n\ntion function of the observed data, namely\"f. denote these x\u2217i = {x\u2217i1, . . . ,x\u2217in} for\ni = 1, . . . , b. the empirical distribution of the r(x\u2217i ,\"f) for i = 1, . . . , b is used\nto approximate the distribution of r(x, f), allowing inference. the simulation error\n\n "}, {"Page_number": 296, "text": "9.2 basic methods\n\n289\nintroduced by avoiding complete enumeration of all possible pseudo-datasets can be\nmade arbitrarily small by increasing b. using the bootstrap frees the analyst from\nmaking parametric assumptions to carry out inference, provides answers to problems\nfor which analytic solutions are impossible, and can yield more accurate answers than\ngiven by routine application of standard parametric theory.\nexample 9.2 (simple illustration, continued) continuing with the dataset in\nexample 9.1, recall that the empirical distribution function of the observed data,\"f,\nplaces mass 1\n3 on 1, 2, and 6. a nonparametric bootstrap would generate x\u2217i by\nsampling x\u2217i1, x\u2217i2, and x\u2217i3 i.i.d. from\"f. in other words, draw the x\u2217ij with replace-\nment from {1, 2, 6} with equal probability. each bootstrap pseudo-dataset yields a\ncorresponding estimate\"\u03b8\u2217. table 9.1 shows the observed relative frequencies of the\npossible values for\"\u03b8\u2217 resulting from b = 1000 randomly drawn pseudo-datasets,x\u2217i .\nthese relative frequencies approximate p\u2217$\"\u03b8\u2217%. the bootstrap principle asserts that\np\u2217$\"\u03b8\u2217% in turn approximates the sampling distribution of\"\u03b8.\ncan be completely enumerated and the p\u2217$\"\u03b8\u2217% exactly derived. therefore there is\n\nfor this simple illustration, the space of all possible bootstrap pseudo-datasets\nno need to resort to simulation. in realistic applications, however, the sample size is\ntoo large to enumerate the bootstrap sample space. thus, in real applications (e.g.,\nsection 9.2.3), only a small proportion of possible pseudo-datasets will ever be drawn,\noften yielding only a subset of possible values for the estimator.\n!\na fundamental requirement of bootstrapping is that the data to be resampled\nmust have originated as an i.i.d. sample. if the sample is not i.i.d., the distributional\nuser must carefully consider the relationship between the stochastic mechanism gen-\nerating the observed data and the bootstrap resampling strategy employed. methods\nfor bootstrapping with dependent data are described in section 9.5.\n\napproximation of r(x, f) by r(x\u2217,\"f) will not hold. section 9.2.3 illustrates that the\n\n9.2.2 parametric bootstrap\nthe ordinary nonparametric bootstrap described above generates each pseudo-dataset\nx\u2217 by drawing x\u22171, . . . ,x\u2217n i.i.d. from\"f. when the data are modeled to originate from\na parametric distribution, so x1, . . . ,xn \u223c i.i.d. f(x, \u03b8), another estimate of f may\nbe employed. suppose that the observed data are used to estimate \u03b8 by\"\u03b8. then each\nparametric bootstrap pseudo-dataset x\u2217 can be generated by drawing x\u22171, . . . ,x\u2217n \u223c\ni.i.d. f(x,\"\u03b8). when the model is known or believed to be a good representation\nof reality, the parametric bootstrap can be a powerful tool, allowing inference in\notherwise intractable situations and producing confidence intervals that are much\nmore accurate than those produced by standard asymptotic theory.\nin some cases, however, the model upon which bootstrapping is based is almost\nan afterthought. for example, a deterministic biological population model might pre-\ndict changes in population abundance over time, based on biological parameters and\ninitial population size. suppose animals are counted at various times using various\nmethodologies. the observed counts are compared with the model predictions to find\n\n "}, {"Page_number": 297, "text": "chapter 9 bootstrapping\n\n290\nmodel parameter values that yield a good fit. one might fashion a second model\nasserting that the observations are, say, lognormally distributed with mean equal to\nthe prediction from the biological model and with a predetermined coefficient of\nvariation. this provides a convenient\u2014if weakly justified\u2014link between the param-\neters and the observations. a parametric bootstrap from the second model can then\nbe applied by drawing bootstrap pseudo-datasets from this lognormal distribution.\nin this case, the sampling distribution of the observed data can hardly be viewed as\narising from the lognormal model.\nsuch an analysis, relying on an ad hoc error model, should be a last resort. it\nis tempting to use a convenient but inappropriate model. if the model is not a good\nfit to the mechanism generating the data, the parametric bootstrap can lead inference\nbadly astray. there are occasions, however, when few other inferential tools seem\nfeasible.\n\n9.2.3 bootstrapping regression\nconsider the ordinary multiple regression model, yi = xt\ni \u03b2 + \u03f5i, for i = 1, . . . , n,\nwhere the \u03f5i are assumed to be i.i.d. mean zero random variables with constant vari-\nance. here, xi and \u03b2 are p-vectors of predictors and parameters, respectively. a naive\nbootstrapping mistake would be to resample from the collection of response values a\nnew pseudo-response, say y\u2217i , for each observed xi, thereby generating a new regres-\nsiondataset.thenabootstrapparametervectorestimate,\"\u03b2\u2217,wouldbecalculatedfrom\nthese pseudo-data. after repeating the sampling and estimation steps many times, the\nempirical distribution of\"\u03b2\u2217 would be used for inference about \u03b2. the mistake is that\nthe yi | xi are not i.i.d.\u2014they have different conditional means. therefore, it is not\nappropriate to generate bootstrap regression datasets in the manner described.\nwe must ask what variables are i.i.d. in order to determine a correct bootstrap-\nping approach. the \u03f5i are i.i.d. given the model. thus a more appropriate strategy\nwould be to bootstrap the residuals as follows.\nstart by fitting the regression model to the observed data and obtaining the fitted\nthe set of fitted residuals, completely at random with replacement. (note that the \u02c6\u03f5i\nare actually not independent, though they are usually roughly so.) create a bootstrap\n\nresponses\"yi and residuals \u02c6\u03f5i. sample a bootstrap set of residuals, {\u02c6\u03f5\u22171, . . . , \u02c6\u03f5\u2217n}, from\nset of pseudo-responses, y\u2217i =\"yi + \u02c6\u03f5\u2217i , for i = 1, . . . , n. regress y\u2217 on x to obtain a\nbootstrap parameter estimate\"\u03b2\u2217. repeat this process many times to build an empirical\ndistribution for\"\u03b2\u2217 that can be used for inference.\n\nthis approach is most appropriate for designed experiments or other data where\nthe xi values are fixed in advance. the strategy of bootstrapping residuals is at the\ncoreofsimplebootstrappingmethodsforothermodelssuchasautoregressivemodels,\nnonparametric regression, and generalized linear models.\nbootstrapping the residuals is reliant on the chosen model providing an appro-\npriate fit to the observed data, and on the assumption that the residuals have constant\nvariance. without confidence that these conditions hold, a different bootstrapping\nmethod is probably more appropriate.\n\n "}, {"Page_number": 298, "text": "9.2 basic methods\n\n291\n\ntable 9.2 copper\u2013nickel alloy data for illustrating methods of obtaining a bootstrap confidence\ninterval for \u02c71/\u02c70.\n\nxi\nyi\n\nxi\nyi\n\n0.01\n127.6\n\n1.44\n92.3\n\n0.48\n124.0\n\n0.71\n113.1\n\n0.71\n110.8\n\n1.96\n83.7\n\n0.95\n103.9\n\n0.01\n128.0\n\n1.19\n101.5\n\n1.44\n91.4\n\n0.01\n130.1\n\n1.96\n86.2\n\n0.48\n122.0\n\nsuppose that the data arose from an observational study, where both response\nand predictors are measured from a collection of individuals selected at random.\nin this case, the data pairs zi = (xi, yi) can be viewed as values observed for i.i.d.\nrandomvariableszi = (xi, yi)drawnfromajointresponse\u2013predictordistribution.to\nbootstrap, sample z\u22171, . . . ,z\u2217n completely at random with replacement from the set of\nobserved data pairs, {z1, . . . ,zn}. apply the regression model to the resulting pseudo-\ndataset to obtain a bootstrap parameter estimate\"\u03b2\u2217. repeat these steps many times,\nthen proceed to inference as in the first approach. this approach of bootstrapping the\ncases is sometimes called the paired bootstrap.\nif you have doubts about the adequacy of the regression model, the constancy\nof the residual variance, or other regression assumptions, the paired bootstrap will be\nless sensitive to violations in the assumptions than will bootstrapping the residuals.\nthe paired bootstrap sampling more directly mirrors the original data generation\nmechanism in cases where the predictors are not considered fixed.\nthere are other, more complex methods for bootstrapping regression problems\n[142, 179, 183, 330].\nexample 9.3 (copper\u2013nickel alloy)\ntable 9.2 gives 13 measurements of cor-\nrosion loss (yi) in copper\u2013nickel alloys, each with a specific iron content (xi) [170].\nof interest is the change in corrosion loss in the alloys as the iron content increases,\nrelative to the corrosion loss when there is no iron. thus, consider the estimation of\n\u03b8 = \u03b21/\u03b20 in a simple linear regression.\nletting zi = (xi, yi) for i = 1, . . . ,13, suppose we adopt the paired boot-\nstrapping approach. the observed data yield the estimate\"\u03b8 =\"\u03b21/\"\u03b20 = \u22120.185. for\ni = 2, . . . ,10,000, we draw a bootstrap dataset {z\u22171, . . . ,z\u221713} by resampling 13 data\npairs from the set {z1, . . . ,z13} completely at random with replacement. figure 9.1\nshowsahistogramoftheestimatesobtainedfromregressionsofthebootstrapdatasets.\nthe histogram summarizes the sampling variability of\"\u03b8 as an estimator of \u03b8.\n!\n9.2.4 bootstrap bias correction\na particularly interesting choice for bootstrap analysis when t(f) = \u03b8 is the quantity\nr(x, f) = t(\"f) \u2212 t(f). this represents the bias of t(\"f) =\"\u03b8, and it has mean equal\nto e{\"\u03b8} \u2212 \u03b8. the bootstrap estimate of the bias is#b\nregression data introduced in example 9.3, the mean value of\"\u03b8\u2217 \u2212\"\u03b8 among the\n\ni=1(\"\u03b8\u2217i \u2212\"\u03b8)/b = \u03b8\u2217 \u2212\"\u03b8.\n\nexample 9.4 (copper\u2013nickel alloy, continued)\n\nfor the copper\u2013nickel alloy\n\n "}, {"Page_number": 299, "text": "292\n\nchapter 9 bootstrapping\n\n1000\n\ny\nc\nn\ne\nu\nq\ne\nr\nf\n\n500\n\n0\n\n\u20130.21\n\n\u20130.19\n\n\u20130.17\n\nfigure 9.1 histogram of 10,000 bootstrap estimates of \u03b21/\u03b20 from the nonparametric\npaired bootstrap analysis with the copper\u2013nickel alloy data.\n\nbootstrap estimates of \u03b8 = \u03b2 1/\u03b2 0\n\nbootstrap pseudo-datasets is \u22120.00125, indicating a small degree of negative bias.\nthus, the bias-corrected bootstrap estimate of \u03b21/\u03b20 is \u22120.18507 \u2212 (\u22120.00125) =\n\u22120.184. the bias estimate can naturally be incorporated into confidence interval\nestimates via the nested bootstrap of section 9.3.2.4.\n!\nan improved bias estimate requires only a little additional effort. let \"f\u2217j\ndenote the empirical distribution of the jth bootstrap pseudo-dataset, and define\nf\u2217(x) =#b\nj=1\"f\u2217j (x)/b. then \u03b8\u2217 \u2212 t(f\u2217) is a better estimate of bias. compare this\nstrategy with bootstrap bagging, discussed in section 9.7. study of the merits of these\nand other bias corrections has shown that \u03b8\u2217 \u2212 t(f\u2217) has superior performance and\nconvergence rate [183].\n\n9.3 bootstrap inference\n\n9.3.1 percentile method\nthe simplest method for drawing inference about a univariate parameter \u03b8 using boot-\nstrap simulations is to construct a confidence interval using the percentile method.\nthis amounts to reading percentiles off the histogram of\"\u03b8\u2217 values produced by boot-\nstrapping. it has been the approach implicit in the preceding discussion.\nexample 9.5 (copper\u2013nickel alloy, continued) returning to the estimation of\n\u03b8 = \u03b21/\u03b20 for the copper\u2013nickel alloy regression data introduced in example 9.3,\nrecall that figure 9.1 summarizes the sampling variability of\"\u03b8 as an estimator of \u03b8.\n\n "}, {"Page_number": 300, "text": "9.3 bootstrap inference\n\n293\na bootstrap 1 \u2212 \u03b1 confidence interval based on the percentile method could be con-\nstructed by finding the [(1 \u2212 \u03b1/2)100]th and [(\u03b1/2)100]th empirical percentiles in\nthe histogram. the 95% confidence interval for \u03b21/\u03b20 using the simple bootstrap\npercentile method is (\u22120.205, \u22120.174).\n!\nconducting a hypothesis test is closely related to estimating a confidence inter-\nval. the simplest approach for bootstrap hypothesis testing is to base the p-value on\na bootstrap confidence interval. specifically, consider a null hypothesis expressed in\nterms of a parameter whose estimate can be bootstrapped. if the (1 \u2212 \u03b1)100% boot-\nstrap confidence interval for the parameter does not cover the null value, then the\nnull hypothesis is rejected with a p-value no greater than \u03b1. the confidence interval\nitself may be obtained from the percentile method or one of the superior approaches\ndiscussed later.\nusing a bootstrap confidence interval to conduct a hypothesis test often sac-\nrifices statistical power. greater power is possible if the bootstrap simulations are\ncarried out using a sampling distribution that is consistent with the null hypothe-\nsis [589]. use of the null hypothesis sampling distribution of a test statistic is a\nfundamental tenet of hypothesis testing. unfortunately, there will usually be many\ndifferent bootstrap sampling strategies that are consistent with a given null hypothe-\nses, with each imposing various extra restrictions in addition to those imposed by\nthe null hypothesis. these different sampling models will yield hypothesis tests\nof different quality. more empirical and theoretical research is needed to develop\nbootstrap hypothesis testing methods, particularly methods for appropriate bootstrap\nsampling under the null hypothesis. strategies for specific situations are illustrated\nby [142, 183].\nalthoughsimple,thepercentilemethodisprone to biasandinaccuratecoverage\nprobabilities. the bootstrap works better when \u03b8 is essentially a location parameter.\nthis is particularly important when using the percentile method. to ensure best boot-\nstrap performance, the bootstrapped statistic should be approximately pivotal: its\ndistribution should not depend on the true value of \u03b8. since a variance-stabilizing\nprovides a good pivot. section 9.3.2 discusses several approaches that rely on pivoting\nto improve bootstrap performance.\n\ntransformation g naturally renders the variance of g(\"\u03b8) independent of \u03b8, it frequently\n\njustification for the percentile method the percentile method can\n9.3.1.1\nbejustifiedbyaconsiderationofacontinuous,strictlyincreasingtransformation \u03c6 and\na distribution function h that is continuous and symmetric [i.e., h(z) = 1 \u2212 h(\u2212z)],\nwith the property that\n\np\u2019h\u03b1/2 \u2264 \u03c6(\"\u03b8) \u2212 \u03c6(\u03b8) \u2264 h1\u2212\u03b1/2( = 1 \u2212 \u03b1,\n\n(9.1)\n\nwhere h\u03b1 isthe \u03b1quantileof h.forinstance,if \u03c6 isanormalizing,variance-stabilizing\ntransformation, then h is the standard normal distribution. in principle, when f\nis continuous we may transform any random variable x \u223c f to have any desired\ndistribution g, using the monotone transformation g\u22121(f(x)). there is therefore\n\n "}, {"Page_number": 301, "text": "chapter 9 bootstrapping\n\n294\nnothing special about normalization. in fact, the remarkable aspect of the percentile\napproach is that we are never actually required to specify explicitly \u03c6 or h.\n\napplying the bootstrap principle to (9.1), we have\n\n1 \u2212 \u03b1 \u2248 p\u2217\u2019h\u03b1/2 \u2264 \u03c6(\"\u03b8\u2217) \u2212 \u03c6(\"\u03b8) \u2264 h1\u2212\u03b1/2(\n\n= p\u2217\u2019h\u03b1/2 + \u03c6(\"\u03b8) \u2264 \u03c6(\"\u03b8\u2217) \u2264 h1\u2212\u03b1/2 + \u03c6(\"\u03b8)(\n= p\u2217\u2019\u03c6\u22121)h\u03b1/2 + \u03c6(\"\u03b8)* \u2264\"\u03b8\u2217 \u2264 \u03c6\u22121)h1\u2212\u03b1/2 + \u03c6(\"\u03b8)*( .\n\n(9.2)\nsince the bootstrap distribution is observed by us, its percentiles are known quantities\n(aside from monte carlo variability which can be made arbitrarily small by increasing\nthe number of pseudo-datasets, b). let \u03be\u03b1 denote the \u03b1 quantile of the empirical\ndistributionof\"\u03b8\u2217.then \u03c6\u22121+h\u03b1/2 + \u03c6(\"\u03b8), \u2248 \u03be\u03b1/2 and \u03c6\u22121+h1\u2212\u03b1/2 + \u03c6(\"\u03b8), \u2248 \u03be1\u2212\u03b1/2.\nnext, the original probability statement (9.1) from which we hope to build a\nconfidence interval is reexpressed to isolate \u03b8. exploiting symmetry by noting that\nh\u03b1/2 = \u2212h1\u2212\u03b1/2 yields\n\np\u2019\u03c6\u22121)h\u03b1/2 + \u03c6(\"\u03b8)* \u2264 \u03b8 \u2264 \u03c6\u22121)h1\u2212\u03b1/2 + \u03c6(\"\u03b8)*( = 1 \u2212 \u03b1.\n\n(9.3)\nthe confidence limits in this equation happily coincide with the limits in (9.2), for\nwhich we already have estimates \u03be\u03b1/2 and \u03be1\u2212\u03b1/2. hence we may simply read off the\nquantiles for\"\u03b8\u2217 from the bootstrap distribution and use these as the confidence limits\nfor \u03b8. note that the percentile method is transformation respecting in the sense that\nthe percentile method confidence interval for a monotone transformation of \u03b8 is the\nsame as the transformation of the interval for \u03b8 itself [183].\n\n9.3.2 pivoting\n9.3.2.1 accelerated bias-corrected percentile method, bca the acceler-\nated bias-corrected percentile method, bca, usually offers substantial improvement\nover the simple percentile approach [163, 178]. for the basic percentile method to\nvariance that does not depend on \u03b8. bca augments \u03c6 with two parameters to better\nmeet these conditions, thereby ensuring an approximate pivot.\nassume there exists a monotonically increasing function \u03c6 and constants a and\nb such that\n\nwork well, it is necessary for the transformed estimator \u03c6(\"\u03b8) to be unbiased with\n\n\u03c6(\"\u03b8) \u2212 \u03c6(\u03b8)\n(9.4)\n1 + a\u03c6(\u03b8) + b\nhas a n(0,1) distribution, with 1 + a\u03c6(\u03b8) > 0. note that if a = b = 0, this transfor-\nmation leads us back to the simple percentile method.\n\nu =\n\nby the bootstrap principle,\n\nu\u2217 =\n\n\u03c6(\"\u03b8\u2217) \u2212 \u03c6(\"\u03b8)\n1 + a\u03c6(\"\u03b8) + b\n\n(9.5)\n\n "}, {"Page_number": 302, "text": "295\nhas approximately a standard normal distribution. for any quantile of a standard\nnormal distribution, say z\u03b1,\n\n9.3 bootstrap inference\n\n(9.6)\n\n(9.7)\n\n\u03b1 \u2248 p\u2217[u\u2217 \u2264 z\u03b1]\n= p\u2217\u2019\"\u03b8\u2217 \u2264 \u03c6\u22121)\u03c6(\"\u03b8) + (z\u03b1 \u2212 b)$1 + a\u03c6(\"\u03b8)%*( .\n\nfrom the bootstrap distribution. therefore\n\nhowever, the \u03b1 quantile of the empirical distribution of\"\u03b8\u2217, denoted \u03be\u03b1, is observable\n\n\u03c6\u22121)\u03c6(\"\u03b8) + (z\u03b1 \u2212 b)$1 + a\u03c6(\"\u03b8)%* \u2248 \u03be\u03b1.\n\nin order to use (9.7), consider u itself:\n\n1 \u2212 \u03b1 = p[u > z\u03b1]\n\n= p\u2019\u03b8 < \u03c6\u22121)\u03c6(\"\u03b8) + u(a, b, \u03b1)$1 + a\u03c6(\"\u03b8)%*(\n\n(9.8)\nwhere u(a, b, \u03b1) = (b \u2212 z\u03b1)/[1 \u2212 a(b \u2212 z\u03b1)]. notice the similarity between (9.6) and\n(9.8). if we can find a \u03b2 such that u(a, b, \u03b1) = z\u03b2 \u2212 b, then the bootstrap principle\ncan be applied to conclude that \u03b8 < \u03be\u03b2 will approximate a 1 \u2212 \u03b1 upper confidence\nlimit. a straightforward inversion of this requirement yields\n\n\u03b2 = \u0001(b + u(a, b, \u03b1)) = \u0001-b +\n\nb + z1\u2212\u03b1\n\n1 \u2212 a(b + z1\u2212\u03b1). ,\n\n(9.9)\n\na\n\nfor\n\ntwo-sided\n\nwhere \u0001 is the standard normal cumulative distribution function and the last equality\nfollows from symmetry. thus, if we knew a suitable a and b, then to find a 1 \u2212 \u03b1\nupper confidence limit we would first compute \u03b2 and then find the \u03b2th quantile of the\nempirical distribution of\"\u03b8\u2217, namely \u03be\u03b2, using the bootstrap pseudo-datasets.\nyields\n1 \u2212 \u03b1 confidence\np$\u03be\u03b21 \u2264 \u03b8 \u2264 \u03be\u03b22% \u2248 1 \u2212 \u03b1, where\n\u03b21 = \u0001-b +\n\u03b22 = \u0001-b +\n\n1 \u2212 a(b + z\u03b1/2). ,\n1 \u2212 a(b + z1\u2212\u03b1/2). ,\n\nb + z1\u2212\u03b1/2\n\nb + z\u03b1/2\n\napproach\n\ninterval,\n\n(9.10)\n\n(9.11)\n\nthis\n\nand \u03be\u03b21 and \u03be\u03b22 are the corresponding quantiles from the bootstrapped values of\"\u03b8\u2217.\n\nas with the percentile method, the beauty of the above justification for bca is\nthat explicit specification of the transformation \u03c6 is not necessary. further, since the\nbca approach merely corrects the percentile levels determining the confidence inter-\nval endpoints to be read from the bootstrap distribution, it shares the transformation-\nrespecting property of the simple percentile method.\n\n "}, {"Page_number": 303, "text": ",\n\n(9.12)\n\n296\n\nchapter 9 bootstrapping\nthe remaining question is the choice of a and b. the simplest nonparametric\n\nchoices are b = \u0001\u22121)\"f\u2217+\"\u03b8,* and\ni01 n/i=1\nn/i=1\n\u03c8i = \u02c6\u03b8(\u00b7) \u2212 \u02c6\u03b8(-i)\n\nwhere\n\na =\n\n\u03c83\n\n1\n6\n\n\u03c82\n\ni23/2\n\n1\n\n\u03c8i = lim\n\u03f5\u21920\n\ni=1 \u02c6\u03b8(-i). a related alternative is to let\n\n(9.13)\nwith \u02c6\u03b8(-i) denoting the statistic computed omitting the ith observation, and \u02c6\u03b8(\u00b7) =\n(1/n)#n\n\n\u03f5)t+(1 \u2212 \u03f5)\"f + \u03f5\u03b4i, \u2212 t+\"f,* ,\n\n(9.14)\nwhere \u03b4i represents the distribution function that steps from zero to one at the obser-\nvation xi (i.e., unit mass on xi). the \u03c8i in (9.14) can be approximated using finite\ndifferences. the motivation for these quantities and additional alternatives for a and\nb are described by [589].\nexample 9.6 (copper\u2013nickel alloy, continued) continuing the copper\u2013nickel\nalloy regression problem introduced in example 9.3, we have a = 0.0486 [using\n(9.13)] and b = 0.00802. the adjusted quantiles are therefore \u03b21 = 0.038 and \u03b22 =\n0.986. the main effect of bca was therefore to shift the confidence interval slightly\nto the right. the resulting interval is (\u22120.203, \u22120.172).\n!\nthe bootstrap t another approximate pivot that is quite easy to imple-\n9.3.2.2\nmentisprovidedbythe bootstrap t method,alsocalledthe studentized bootstrap[176,\n\n183]. suppose \u03b8 = t(f) is to be estimated using\"\u03b8 = t(\"f), with v(\"f) estimating the\nvariance of\"\u03b8. then it is reasonable to hope that r(x, f) = [t(\"f) \u2212 t(f)]/3v(\"f)\nwill be roughly pivotal. bootstrapping r(x, f) yields a collection of r(x\u2217,\"f).\ndenote by \"g and \"g\u2217 the distributions of r(x, f) and r(x\u2217,\"f), respectively.\nby definition, a 1 \u2212 \u03b1 confidence interval for \u03b8 is obtained from the relation\np$\u03be\u03b1/2(\"g) \u2264 r(x, f) \u2264 \u03be1\u2212\u03b1/2(\"g)%\n= p4\"\u03b8 \u22123v(\"f)\u03be1\u2212\u03b1/2(\"g) \u2264 \u03b8 \u2264\"\u03b8 \u22123v(\"f)\u03be\u03b1/2(\"g)5\n\n= 1 \u2212 \u03b1,\n\nwhere \u03be\u03b1(\"g) is the \u03b1 quantile of \"g. these quantiles are unknown because f (and\nhence\"g) is unknown. however, the bootstrap principle implies that the distributions\n\"g and \"g\u2217 should be roughly equal, so \u03be\u03b1(\"g) \u2248 \u03be\u03b1(\"g\u2217) for any \u03b1. thus, a bootstrap\n\nconfidence interval can be constructed as\n\n-t(\"f) \u22123v(\"f)\u03be1\u2212\u03b1/2(\"g\u2217), t(\"f) \u22123v(\"f)\u03be\u03b1/2(\"g\u2217). ,\n\n(9.15)\n\n "}, {"Page_number": 304, "text": "9.3 bootstrap inference\n\n297\n\ny\nc\nn\ne\nu\nq\ne\nr\nf\n\n750\n\n500\n\n250\n\n0\n\n\u201310\n\n\u20135\n\n0\n\n5\n\n10\n\nwith the copper\u2013nickel alloy data.\n\nthousand bootstrap pseudo-datasets are needed for adequate precision.\nexample 9.7 (copper\u2013nickel alloy, continued) continuing the copper\u2013nickel\n\nfigure 9.2 histogram of 10,000 values of r(x\u2217,\"f) from a studentized bootstrap analysis\nwhere the percentiles of \"g\u2217 are taken from the histogram of bootstrap values of\nr(x\u2217,\"f). since these are percentiles in the tail of the distribution, at least several\nalloyregressionproblemintroducedinexample9.3,anestimator v(\"f)ofthevariance\nof\"\u03b21/\"\u03b20 based on the delta method is\n\"\u03b202216var{\"\u03b21}\n1\"\u03b21\n1 + 6var{\"\u03b20}\n0 \u2212\n\"\u03b22\n\"\u03b22\n\nwhere the estimated variances and covariance can be obtained from basic regression\nresults. carrying out the bootstrap t method then yields the histogram shown in fig-\n\n2 6cov{\"\u03b20,\"\u03b21}\n\"\u03b20\"\u03b21\n\nure 9.2, which corresponds to \"g\u2217. the 0.025 and 0.975 quantiles of \"g\u2217 are \u22125.77\nand 4.44, respectively, and3v(\"f) = 0.00273. thus, the 95% bootstrap t confidence\ninterval is (\u22120.197, \u22120.169).\nthis method requires an estimator of the variance of\"\u03b8, namely v(\"f). if no such\nestimator is readily available, a delta method approximation may be used [142].\nthe bootstrap t usually provides confidence interval coverage rates that closely\napproximate the nominal confidence level. confidence intervals from the bootstrap\nt are most reliable when t(\"f) is approximately a location statistic in the sense that\na constant shift in all the data values will induce the same shift in t(\"f). they are\n\nalso more reliable for variance-stabilized estimators. coverage rates for bootstrap\n\n2 ,\n\n(9.16)\n\n!\n\n "}, {"Page_number": 305, "text": "chapter 9 bootstrapping\n\n298\nt intervals can be sensitive to the presence of outliers in the dataset and should be\nused with caution in such cases. the bootstrap t does not share the transformation-\nrespecting property of the percentile-based methods above.\n\nof the jth bootstrap pseudo-dataset.\n\nempirical variance stabilization a variance-stabilizing transforma-\n9.3.2.3\ntion is often the basis for a good pivot. a variance-stabilizing transformation of the\nestimator\"\u03b8 is one for which the sampling variance of the transformed estimator does\nnot depend on \u03b8. usually a variance-stabilizing transformation of the statistic to be\nbootstrapped is unknown, but it can be estimated using the bootstrap.\nstart by drawing b1 bootstrap pseudo-datasets x\u2217j for j = 1, . . . , b1. calculate\n\"\u03b8\u2217j for each bootstrap pseudo-dataset, and let\"f\u2217j be the empirical distribution function\nfor each x\u2217j , next draw b2 bootstrap pseudo-datasets x\u2217\u2217j1 , . . . ,x\u2217\u2217jb2 from\"f\u2217j .\nfor each j, let\"\u03b8 \u2217\u2217jk denote the parameter estimate from the kth subsample, and let \u03b8 \u2217\u2217j\nbe the mean of the\"\u03b8 \u2217\u2217jk . then\n\"s(\"\u03b8\u2217j) =\n\nb2/k=1)\"\u03b8\u2217\u2217jk \u2212 \u03b8 \u2217\u2217j *2\nis an estimate of the standard error of\"\u03b8 given \u03b8 =\"\u03b8\u2217j.\n\nfit a curve to the set of points {\"\u03b8\u2217j ,\"s(\"\u03b8\u2217j)}, j = 1, . . . , b1. for a flexible, non-\nparametric fit, chapter 11 reviews many suitable approaches. the fitted curve is an\nestimate of the relationship between the standard error of the estimator and \u03b8. we\nseek a variance-stabilizing transformation to neutralize this relationship.\nrecall that if z is a random variable with mean \u03b8 and standard deviation s(\u03b8),\nthen taylor series expansion (i.e., the delta method) yields var{g(z)} \u2248 g\u2032(\u03b8)2s2(\u03b8).\nfor the variance of g(z) to be constant, we require\n\n1\nb2 \u2212 1\n\n(9.17)\n\ng(z) =7 z\n\na\n\n1\ns(u) du,\n\n(9.18)\n\nwhere aisanyconvenientconstantforwhich1/s(u)iscontinuouson[a, z].therefore,\nbootstrapdatabyapplying(9.18)tothefittedcurvefromthepreviousstep.theintegral\ndenote the result.\nnow that an approximate variance-stabilizing transformation has been esti-\nmated, the bootstrap t may be carried out on the transformed scale. draw b3 new\n\nan approximately variance-stabilizing transformation for\"\u03b8 may be obtained from our\ncan be approximated using a numerical integration technique from chapter 5. let\"g(\u03b8)\nbootstrap pseudo-datasets from\"f, and apply the bootstrap t method to find an inter-\nval for\"g(\u03b8). note, however, that the standard error of\"g(\u03b8) is roughly constant, so we\ncan use r(x\u2217,\"f) =\"g(\u02c6\u03b8\u2217) \u2212\"g(\u02c6\u03b8) for computing the bootstrap t confidence interval.\nby applying the transformation\"g\u22121.\n\nfinally, the endpoints of the resulting interval can be converted back to the scale of \u03b8\n\n "}, {"Page_number": 306, "text": "9.3 bootstrap inference\n\np\u2019f\u22121\n\n299\nthe strategy of drawing iterated bootstrap pseudo-datasets from each original\npseudo-dataset sample can be quite useful in a variety of settings. in fact, it is the\nbasis for the confidence interval approach described below.\n9.3.2.4 nested bootstrap and prepivoting another style of pivoting is pro-\nvided by the nested bootstrap [26, 27]. this approach is sometimes also called the\niterated or double bootstrap.\nconsider constructing a confidence interval or conducting a hypothesis test\nbased on a test statistic r0(x, f), given observed data values x1, . . . ,xn from the\nmodel x1, . . . ,xn \u223c i.i.d. f. let f0(q, f) = p[r0(x, f) \u2264 q]. the notation for f0\nmakes explicit the dependence of the distribution of r0 on the distribution of the data\nused in r0. then a two-sided confidence interval can be fashioned after the statement\n(9.19)\n\n0 (\u03b1/2, f) \u2264 r0(x, f) \u2264 f\u22121\nand a hypothesis test based on the statement\np\u2019r0(x, f) \u2264 f\u22121\n\n0 (1 \u2212 \u03b1/2, f)( = 1 \u2212 \u03b1,\n0 (1 \u2212 \u03b1, f)( = 1 \u2212 \u03b1.\n\nthe ordinary nonparametric bootstrap.\n\n(9.20)\nof course, these probability statements depend on the quantiles of f0, which\nare not known. in the estimation case, f is not known; for hypothesis testing, the null\nvalue for f is hypothesized. in both cases, the distribution of r0 is not known. we\ncan use the bootstrap to approximate f0 and its quantiles.\nthe bootstrap begins by drawing b bootstrap pseudo-datasets, x\u22171 , . . . ,x\u2217b,\nfrom the empirical distribution\"f. for the jth bootstrap pseudo-dataset, compute the\nstatistic r0(x\u2217j ,\"f). let \"f0(q,\"f) = (1/b)#b\nj=1 18r0(x\u2217j ,\"f)\u2264q9, where 1{a} = 1 if\na is true and zero otherwise. thus \"f0 estimates p\u2217[r0(x\u2217,\"f) \u2264 q], which itself\nestimates p$r0(x, f) \u2264 q% = f0(q, f) according to the bootstrap principle. thus,\nthe upper limit of the confidence interval would be estimated as\"f\u22121\n0 (1 \u2212 \u03b1/2,\"f), or\nwe would reject the null hypothesis if r0({x1, . . . ,xn}, f) >\"f\u22121\n0 (1 \u2212 \u03b1,\"f). this is\nnote, however, that a confidence interval constructed in this manner will not\nhave coverage probability exactly equal to 1 \u2212 \u03b1, because \"f0 is only a bootstrap\napproximation to the distribution of r0(x, f). similarly, the size of the hypothesis\ntest is p\u2019r0(x, f) >\"f\u22121\nnot knowing the distribution f0 also deprives us of a perfect pivot: the random\nvariable r1(x, f) = f0 (r0(x, f), f) has a standard uniform distribution indepen-\ndent of f. the bootstrap principle asserts the approximation of f0 by\"f0, and hence\nthe approximation of r1(x, f) by\"r1(x, f) =\"f0+r0(x, f),\"f,. this allows boot-\nstrap inference based on a comparison of \"r1(x, f) to the quantiles of a uniform\nhowever, we could instead proceed by acknowledging that \"r1(x, f) \u223c f1,\nfor some nonuniform distribution f1. let f1(q, f) = p[\"r1(x, f) \u2264 q]. then the\ncorrect size test rejects the null hypothesis if \"r1 > f\u22121\n1 (1 \u2212 \u03b1, f). a confidence\n\ndistribution. for hypothesis testing, this amounts to accepting or rejecting the null\nhypothesis based on the bootstrap p-value.\n\n0 (1 \u2212 \u03b1,\"f)( /= \u03b1, since f0(q, f) /=\"f0(q,\"f).\n\n "}, {"Page_number": 307, "text": "chapter 9 bootstrapping\n\n300\ninterval with the correct coverage probability is motivated by the statement\n\nsample from the original data with replacement.\n\nfrom two sources: (1) the observed data were random observations from f, and (2)\n\np\u2019f\u22121\n1 (1 \u2212 \u03b1/2, f)( = 1 \u2212 \u03b1. as before, f1 is un-\n1 (\u03b1/2, f) \u2264\"r1(x, f) \u2264 f\u22121\nknown but may be approximated using the bootstrap. now the randomness\"r1 comes\ngiven the observed data (and hence \"f), \"r1 is calculated from random resamplings\nfrom \"f. to capture both sources of randomness, we use the following nested boot-\nstrapping algorithm:\n1. generate bootstrap pseudo-datasets x\u22171 , . . . ,x\u2217b0, each as an i.i.d. random\n2. compute r0(x\u2217j ,\"f) for j = 1, . . . , b0.\n3. for j = 1, . . . , b0:\na. let \"fj denote the empirical distribution function of x\u2217j . draw b1 iterated\nbootstrap pseudo-datasets, x\u2217\u2217j1 , . . . ,x\u2217\u2217jb1, each as an i.i.d. random sample\nfrom\"fj.\nb. compute the r0(x\u2217\u2217jk ,\"fj) for k = 1, . . . , b1.\n\"r1(x\u2217j ,\"f) =\"f0(r0(x\u2217j ,\"f),\"f)\n\nc. compute\n\n(9.21)\n\n1\nb1\n\n=\n\nb1/k=1\n\n18r0(x\u2217\u2217jk ,\"fj)\u2264r0(x\u2217j ,\"f)9.\n\ninterval or hypothesis test.\n\nexample 9.8 (copper\u2013nickel alloy, continued)\n\nsteps1and2capturethefirstsourceofrandomnessbyapplyingthebootstrapprinciple\n\n4. denote the empirical distribution function of the resulting sample of\"r1(x\u2217j ,\"f)\nas\"f1.\n5. use \"r1({x1, . . . ,xn}, f) and the quantiles of \"f1 to construct the confidence\nto approximate f by\"f. step 3 captures the second source of randomness introduced\nin\"r1 when r0 is bootstrapped conditional on\"f.\nproblem introduced in example 9.3, let r0({x1, . . . ,x13}, f) =\"\u03b21/\"\u03b20 \u2212 \u03b21/\u03b20.\nfigure 9.3 shows a histogram of \"r1 values obtained by the nested bootstrap with\nb0 = b1 = 300. this distribution shows that \"f1 differs noticeably from uniform.\nindeed, the nested bootstrap gave 0.025 and 0.975 quantiles of \"r1 as 0.0316 and\n0.990, respectively. the 3.16% and 99.0% percentiles of r0(x\u2217,\"f) are then found\nand used to construct a confidence interval for \u03b21/\u03b20, namely (\u22120.197, \u22120.168). !\nwith its nested looping, the double bootstrap can be much slower than\nother pivoting methods: in this case nine times more bootstrap draws were used\nthan for the preceding methods. there are reweighting methods such as bootstrap\n\nreturning to the regression\n\n "}, {"Page_number": 308, "text": "40\n\n20\n\ny\nc\nn\ne\nu\nq\ne\nr\nf\n\n0\n\n0\n\n9.3 bootstrap inference\n\n301\n\n0.5\n\n1.0\n\nthe copper\u2013nickel alloy data.\n\nfigure 9.3 histogram of 300 values of\"r1(x\u2217,\"f) from a nested bootstrap analysis with\n\n1\n\nrecycling that allow reuse of the initial sample, thereby reducing the computational\nburden [141, 484].\n\n9.3.3 hypothesis testing\nthe preceding discussion about bootstrap construction of confidence intervals is\nrelevant for hypothesis testing, too. a hypothesized parameter value outside a\n(1 \u2212 \u03b1)100% confidence interval can be rejected at a p-value of \u03b1. hall and wilson\noffersomeadditionaladvicetoimprovethestatisticalpowerandaccuracyofbootstrap\nhypothesis tests [302].\nfirst, bootstrap resampling should be done in a manner that reflects the null\nhypothesis. to understand what this means, consider a null hypothesis about a uni-\nvariate parameter \u03b8 with null value \u03b80. let the test statistic be r(x, f) = \u02c6\u03b8 \u2212 \u03b80.\nthe null hypothesis would be rejected in favor of a simple two-sided alternative when\n|\u02c6\u03b8 \u2212 \u03b80| is large compared to a reference distribution. to generate the reference distri-\nbution, it may be tempting to resample values r(x\u2217, f) = \u02c6\u03b8\u2217 \u2212 \u03b80 via the bootstrap.\nhowever, if the null is false, this statistic does not have the correct reference distri-\nbution. if \u03b80 is far from the true value of \u03b8, then |\u02c6\u03b8 \u2212 \u03b80| will not seem unusually\nlarge compared to the bootstrap distribution of |\u02c6\u03b8\u2217 \u2212 \u03b80|. a better approach is to use\nvalues of r(x\u2217,\"f) = \u02c6\u03b8\u2217 \u2212 \u02c6\u03b8 to generate a bootstrap estimate of the null distribution\nof r(x, f). when \u03b80 is far from the true value of \u03b8, the bootstrap values of |\u02c6\u03b8\u2217 \u2212 \u02c6\u03b8|\nwill seem quite small compared to |\u02c6\u03b8 \u2212 \u03b80|. thus, comparing \u02c6\u03b8 \u2212 \u03b80 to the bootstrap\ndistribution of \u02c6\u03b8\u2217 \u2212 \u02c6\u03b8 yields greater statistical power.\n\n "}, {"Page_number": 309, "text": "302\n\nchapter 9 bootstrapping\nsecond, we should reemphasize the importance of using a suitable pivot. it is\noften best to base the hypothesis test on the bootstrap distribution of (\u02c6\u03b8\u2217 \u2212 \u02c6\u03b8)/\u02c6\u03c3\u2217,\nwhere \u02c6\u03c3\u2217 is the value of a good estimator of the standard deviation of \u02c6\u03b8\u2217 computed\nfrom a bootstrap pseudo-dataset. this pivoting approach is usually superior to basing\nthe test on the bootstrap distribution of (\u02c6\u03b8\u2217 \u2212 \u02c6\u03b8)/\u02c6\u03c3, (\u02c6\u03b8\u2217 \u2212 \u03b80)/\u02c6\u03c3, \u02c6\u03b8\u2217 \u2212 \u02c6\u03b8, or \u02c6\u03b8\u2217 \u2212 \u03b80,\nwhere \u02c6\u03c3 estimates the standard deviation of \u02c6\u03b8 from the original dataset.\n\n9.4 reducing monte carlo error\n\n9.4.1 balanced bootstrap\nconsider a bootstrap bias correction of the sample mean. the bias correction should\nequal zero because x is unbiased for the true mean \u00b5. now, r(x, f) = x \u2212 \u00b5, and\nthe corresponding bootstrap values are r(x\u2217j ,\"f) = x\u2217j \u2212 x for j = 1, . . . , b. even\nthough x is unbiased, random selection of pseudo-datasets is unlikely to produce a\nset of r(x\u2217,\"f) values whose mean is exactly zero. the ordinary bootstrap exhibits\nunnecessary monte carlo variation in this case.\nhowever, if each data value occurs in the combined collection of bootstrap\npseudo-datasets with the same relative frequency as it does in the observed data,\nthen the bootstrap bias estimate (1/b)#b\nj=1 r(x\u2217j ,\"f) must equal zero. by balanc-\ning the bootstrap data in this manner, a source of potential monte carlo error is\neliminated.\nthe simplest way to achieve this balance is to concatenate b copies of the\nobserveddatavalues,randomlypermutethisseries,andthenreadoff bblocksofsize n\nsequentially. the jth block becomes x\u2217j . this is the balanced bootstrap\u2014sometimes\ncalled the permutation bootstrap [143]. more elaborate balancing algorithms have\nbeen proposed [253], but other methods of reducing monte carlo error may be easier\nor more effective [183].\n\n9.4.2 antithetic bootstrap\nfor a sample of univariate data, x1, . . . , xn, denote the ordered data as x(1), . . . , x(n),\nwhere x(i) is the value of the ith order statistic (i.e., the ith smallest data value). let\n\u03c0(i) = n \u2212 i + 1 be a permutation operator that reverses the order statistics. then\nfor each bootstrap dataset x\u2217 = {x\u22171, . . . , x\u2217n}, let x\u2217\u2217 = {x\u2217\u22171 , . . . , x\u2217\u2217n } denote the\ndataset obtained by substituting x(\u03c0(i)) for every instance of x(i) in x\u2217. thus, for\nexample, if x\u2217 has an unrepresentative predominance of the larger observed data\nvalues, then the smaller observed data values will predominate in x\u2217\u2217.\nusing this strategy, each bootstrap draw provides two estimators: r(x\u2217,\"f) and\nr(x\u2217\u2217,\"f). these two estimators will often be negatively correlated. for example,\nif r is a statistic that is monotone in the sample mean, then negative correlation is\nlikely [409].\n\n "}, {"Page_number": 310, "text": "9.5 bootstrapping dependent data\n\n303\n\nerty that it estimates the quantity of interest with variance\n\nlet ra(x\u2217,\"f) =\n\n1\n\n2+r(x\u2217,\"f) + r(x\u2217\u2217,\"f),. then ra has the desirable prop-\n\nvar{ra(x\u2217,\"f)} =\n\n1\n\n4+var{r(x\u2217,\"f)} + var{r(x\u2217\u2217,\"f)}\n+ 2 cov{r(x\u2217,\"f), r(x\u2217\u2217,\"f)},\n\u2264 var{r(x\u2217,\"f)}\n\n(9.22)\n\nif the covariance is negative.\npermit an antithetic bootstrap strategy [294].\n\nthere are clever ways of establishing orderings of multivariate data, too, to\n\n9.5 bootstrapping dependent data\na critical requirement for the validity of the above methods is that it must be reason-\nable to assume that the bootstrapped quantities are i.i.d. with dependent data, these\napproaches will produce a bootstrap distribution\"f\u2217 that does not mimic f because\nit fails to capture the covariance structure inherent in f.\nassume that data x1, . . . ,xn comprise a partial realization from a station-\nary time series of random variables x1, . . . ,xn, . . . with the finite dimensional\njoint distribution function of the random variables {x1, . . . ,xn} denoted f. for\na time series (x1, . . . ,xn, . . .), stationarity implies that the joint distribution of\n{xt,xt+1, . . . ,xt+k} does not depend on t for any k \u2265 0. we also assume that\nthe process is weakly dependent in the sense that {xt : t \u2264 \u03c4} is independent of\n{xt : t \u2265 \u03c4 + k} in the limit as k \u2192 \u221e for any \u03c4. let x = (x1, . . . ,xn) denote the\ntime series we wish to bootstrap, and hereafter we denote series with (\u00b7) and unordered\nsets with {\u00b7}.\nsince the elements of x are dependent, it is inappropriate to apply the ordinary\nbootstrap for i.i.d. data. this is obvious since fx1,...,xn /= :n\ni=1 fxi under depen-\ndence. as a specific example, consider bootstrapping \u00afx with mean \u00b5. in the case\nof dependent data, nvar{ \u00afx \u2212 \u00b5} equals var{x1} plus many covariance terms. how-\never nvar\u2217{ \u00afx\u2217 \u2212 \u00afx} \u2192 var{x1} as n \u2192 \u221e where var\u2217 represents the variance with\nrespect to the distribution \"f. thus the covariance terms would be lost in the i.i.d.\nbootstrap. also see example 9.9. hence, applying the i.i.d. bootstrap to dependent\ndata cannot even ensure consistency [601].\nseveral bootstrap methods have been developed for dependent data. bootstrap\ntheory and methods for dependent data are more complex than for the i.i.d. case, but\nthe heuristic of resampling the data to generate values of t(\"f\u2217) for approximating the\nsamplingdistributionof t(\"f)isthesame.comprehensivediscussionofbootstrapping\n\nmethods for dependent data is given by [402]. a wide variety of methods have been\nintroduced by [81, 93, 94, 396, 425, 498, 512, 513, 529, 590, 591].\n\n "}, {"Page_number": 311, "text": "304\n\nchapter 9 bootstrapping\n\n9.5.1 model-based approach\nperhaps the simplest context for bootstrapping dependent data is when a time se-\nries is known to be generated from a specific model such as the first-order station-\nary autoregressive process, that is, the ar(1) model. this model is specified by\nthe relation\n\nxt = \u03b1xt\u22121 + \u03f5t\n\n(9.23)\n\nwhere |\u03b1| < 1 and the \u03f5t are i.i.d. random variables with mean zero and constant\nvariance.ifthedataareknowntofolloworcanbeassumedtofollowanar(1)process,\nthen a method akin to bootstrapping the residuals for linear regression (section 9.2.3)\ncan be applied.\nspecifically,afterusingastandardmethodtoestimate \u03b1(see,e.g.,[129]),define\n\nthe estimated innovations to be\"et = xt \u2212\"\u03b1xt\u22121 for t = 2, . . . , n, and let \u00afe be the\nmean of these. the\"et can be recentered to have mean zero by defining\"\u03f5t =\"et \u2212 \u00afe.\nbootstrap iterations should then resample n + 1 values from the set {\"\u03f52, . . . ,\"\u03f5n} with\nreplacement with equal probabilities to yield a set of pseudo-innovations{\u03f5\u22170, . . . , \u03f5\u2217n}.\ngiven the model (and |\"\u03b1| < 1), a pseudo-data series can be reconstructed using x\u22170 =\n\u03f5\u22170 and x\u2217t =\"\u03b1x\u2217t\u22121 + \u03f5\u2217t for t = 1, . . . , n.\nwhengeneratedinthisway,thepseudo-dataseriesisnotstationary.oneremedy\nis to sample a larger number of pseudo-innovations and to start generating the data\nseries \u201cearlier,\u201d that is, from x\u2217k for k much less than 0. the first portion of the\ngenerated series (t = k, . . . ,0) can then be discarded as a burn-in period [402]. as\nwith any model-based bootstrap procedure, good performance for this approach is\ndependent on the model being appropriate.\n\n9.5.2 block bootstrap\nmost often, a model-based approach should not be applied, so a more general method\nis needed. many of the most common approaches to bootstrapping with dependent\ndata rely on notions of blocking the data in order to preserve the covariance structure\nwithin each block even though that structure is lost between blocks once they are\nresampled. we begin by introducing the nonmoving and moving block bootstraps.\nit is important to note that our initial presentation of these methods omits several\nrefinements like additional blocking, centering and studentizing that help ensure the\nbest possible performance. we introduce those topics in sections 9.5.2.3 and 9.5.2.4.\n9.5.2.1 nonmoving block bootstrap consider estimating an unknown quan-\ntion of the data. a bootstrap resampling approach will be used to estimate the sam-\n\ntity \u03b8 = t(f) using the statistic\"\u03b8 = t(\"f) where\"f is the empirical distribution func-\npling distribution of\"\u03b8 by obtaining a collection of bootstrap pseudo-estimates\"\u03b8\u2217i\nfor i = 1, . . . , m. each\"\u03b8\u2217i\nis computed as t(\"f\u2217i ) where \"f\u2217i denotes the empirical\ndistribution function of a pseudo-dataset x\u2217i . these x\u2217i must be generated in a man-\nner that respects the correlation structure in the stochastic process that produced the\n\n "}, {"Page_number": 312, "text": "9.5 bootstrapping dependent data\n\n305\n\n)\n\n%\n\n(\n \ne\ng\nn\na\nh\nc\np\nd\ng\n\n \n\n6\n\n3\n\n0\n\n\u20133\n\n1870\n\n1880\n\n1890\nyear\n\n1900\n\n1910\n\nfigure 9.4 time series of mean changes in gross domestic product (gdp) for 16 industri-\nalized countries for 1871\u20131910. the horizontal line is the overall mean, \u00afx = 2.6875.\noriginal data x. a simple approximate method that attempts to achieve this goal is\nthe nonmoving block bootstrap [93].\nconsider splitting x = (x1, . . . ,xn) into b nonoverlapping blocks of length\nl, where for simplicity hereafter we assume lb = n. denote these blocks as bi =\n(x(i\u22121)l+1, . . . ,xil)for i = 1, . . . , b.thesimplestnonmovingblockbootstrap begins\nby sampling b\u22171 , . . . ,b\u2217b independently from {b1, . . . ,bb} with replacement. these\nblocks are then concatenated to form a pseudo-dataset x\u2217 = (b\u22171 , . . . ,b\u2217b ). replicat-\ning this process b times yields a collection of bootstrap pseudo-datasets denoted x\u2217i\nfor i = 1, . . . , b. each bootstrap pseudo-value\"\u03b8\u2217i is computed from a corresponding\nx\u2217i and the distribution of\"\u03b8 is approximated by the distribution of these b pseudo-\nvalues. although this bootstrap procedure is simple, we will discuss shortly why it is\nnot the best way to proceed.\nfirst, however, let us consider a simple example. suppose n = 9, l = 3,\nb = 3, and x = (x1, . . . , x9) = (1,2,3,4,5,6,7,8,9). the blocks would be b1 =\n(1,2,3),b2 = (4,5,6),andb3 = (7,8,9).independentlysamplingtheseblockswith\nreplacement and reassembling the result might yield x\u2217 = (4,5,6,1,2,3,7,8,9).\nthe order within blocks must be retained, but the order in which the blocks are re-\nassembled doesn\u2019t matter because x is stationary. another possible bootstrap sample\nis x\u2217 = (1,2,3,1,2,3,4,5,6).\nexample 9.9 (industrialized countries gdp) the website for this book contains\ndata on the average percent change in gross domestic product (gdp) for 16 indus-\ntrialized countries for the n = 40 years from 1871 to 1910, derived from [431]. the\ndata are shown in figure 9.4.\n\n "}, {"Page_number": 313, "text": "306\n\nchapter 9 bootstrapping\n\ny\nc\nn\ne\nu\nq\ne\nr\nf\n\n1500\n\n1000\n\n500\n\n0\n\n2.2\n\n2.5\n\n2.8\nx*\n\ni\n\n3.1\n\n3.4\n\n(9.24)\n\nfigure 9.5 histogram of b = 10,000 bootstrap estimates \u00afx\u2217i from example 9.9.\nlet\"\u03b8 = \u00afx estimate the mean gdp change over the period. the variance of this\n\nestimator is\n\nvar{ \u00afx} =\n\n1\n\nn1var{xi} + 2\n\nn\u22121/i=1-1 \u2212\n\ni\n\nn.cov{x1, x1+i}2 .\n\nlet b = 5 and l = 8. figure 9.5 shows a histogram of b = 10,000 bootstrap\nestimates \u00afx\u2217i\nfor i = 1, . . . , b using the nonmoving block bootstrap. the sample\nstandard deviation of these values is 0.196.\nbecause most of the dominant covariance terms in equation (9.24) are negative,\nthe sample standard deviation generated by the i.i.d. approach will be larger than the\none from the block bootstrap approach. in this example, the i.i.d. approach (which\ncorresponds to l = 1, b = 40) yields 0.332.\n!\n9.5.2.2 moving block bootstrap thenonmovingblockbootstrapusessequen-\ntial disjoint blocks that partition x. this choice is inferior to the more general strategy\nemployed by the moving block bootstrap [396]. with this approach, all blocks of l\nadjacent xt are considered, regardless of whether the blocks overlap. thus we de-\nfine bi = (xi, . . . ,xi+l\u22121) for i = 1, . . . , n \u2212 l + 1. resample these blocks indepen-\ndently with replacement, obtaining b\u22171 , . . . ,b\u2217b where again we make the convenient\nassumption that n = lb. after arranging the b\u2217i end to end in order to assemble x\u2217, a\npseudo-estimate\"\u03b8 \u2217 = t(\"f\u2217) is produced. replicating this process b times provides\na bootstrap sample of\"\u03b8 \u2217i values for i = 1, . . . , b. for the case where x = (1, . . . ,9),\na possible bootstrap series x\u2217 is (1,2,3,2,3,4,6,7,8), formed from the two over-\nlapping blocks (1,2,3) and (2,3,4) and the additional block (6,7,8).\n\n "}, {"Page_number": 314, "text": "9.5 bootstrapping dependent data\n\n307\nexample 9.10 (industrialized countries gdp, continued)\nfor the previous\ngdp dataset, the moving blocks bootstrap with l = 8 yields an estimated standard\ndeviation of 0.188. for comparison, the moving and nonmoving bootstrap applica-\ntions were replicated 20,000 times to assess the expected performance of the two\nprocedures. the medians (and standard deviations) of the bootstrap estimates of the\nstandard deviation were 0.187 (0.00125) and 0.196 (0.00131) for the nonmoving\nand moving block approaches, respectively. in principle, the moving block bootstrap\nshould outperform its nonmoving block counterpart; see section 9.6.2.\n!\n\n9.5.2.3 blocks-of-blocks bootstrapping above we have sidestepped a key\nissue for the block bootstrap. our example using \u00afx is not sufficiently general be-\ncause the distribution of the sample mean depends only on the univariate marginal\ndistribution of xt. for dependent data problems, many important parameters of\ninterest pertain to the covariance structure inherent in the joint distribution of\nseveral xt.\nnotice that the serial correlation in x will (usually) be broken in x\u2217 at each\npoint where adjacent resampled blocks meet as they are assembled to construct x\u2217.\nif the parameter \u03b8 = t(f) is related to a p-dimensional distribution, a naive mov-\ning or nonmoving block bootstrap will not replicate the targeted covariance struc-\nture because the pseudo-dataset will resemble white noise more than the original\nseries did.\nfor example, consider the lag 2 autocovariance \u03c12 = e{(xt \u2212 ext)(xt+2 \u2212\next)}. this depends on the distribution function of the trivariate random variable\n(xt, xt+1, xt+2). an appropriate block bootstrapping technique would ensure that\neach pseudo-estimate \u03c1\u22172 is estimated only from such triples. this would eliminate\nthe instances in x\u2217 where x\u2217t and x\u2217t+2 are not lag 2 adjacent to each other in the\noriginal data. without such a strategy, there would be as many as b \u2212 1 inappropriate\ncontributions to \u03c1\u22172.\nthe remedy is the blocks-of-blocks bootstrap. let yj = (xj, . . . , xj+p\u22121) for\nj = 1, . . . , n \u2212 p + 1.theseyj nowconstituteanewseriesof p-dimensionalrandom\nvariables to which a block bootstrap may be applied. furthermore, the sequence\ny = {yt} is stationary and we may now reexpress \u03b8 and\"\u03b8 as ty(fy) and ty(\"fy),\nrespectively. here fy is the distribution function of y and ty is a reexpression of\nt that enables the functional to be written in terms of \"fy so that the estimator is\ncalculated using y rather than x.\nfor a nonmoving block bootstrap, then, y = (y1, . . . , yn\u2212p+1) is partitioned\ninto b adjacentblocksoflength l.denotetheseblocksasb\u20321, . . . ,b\u2032b.theseblocksare\nresampled with replacement, and appended end-to-end to form a pseudo-dataset y\u2217.\neachy\u2217 yieldsapseudo-estimate\"\u03b8\u2217 = ty(\"f\u2217y),where\"f\u2217y istheempiricaldistribution\nfunction of y\u2217.\nfor example, let n = 13, b = 4, l = 3, p = 2, and x = (1,2, . . . ,13). then\n\ny =111\n\n22 ,12\n\n32 ,\u00b7\u00b7\u00b7112\n\n1322 .\n\n "}, {"Page_number": 315, "text": "chapter 9 bootstrapping\n\n308\nfor the nonmoving blocks-of-blocks approach, the four nonoverlapping blocks of\nblocks would be\n\nb\u20321 =;11\n\n22 ,12\n\n32 ,13\n\n42< , . . . ,b\u20324 =;110\n\n112 ,111\n\n122 ,112\n\n132< .\n\none potential blocks-of-blocks nonmoving bootstrap dataset would be\n\ny\u2217 =;17\n\n82 ,18\n\n92 ,1 9\n\n102 ,11\n22 ,12\n11\n22 ,12\n\n32 ,13\n42 ,\n42 ,110\n32 ,13\n\n112 ,111\n\n122 ,112\n\n132< .\n\nthe blocks-of-blocks approach for the moving block bootstrap proceeds anal-\nogously. in this case, there are n \u2212 p + 1 blocks of size p. these blocks overlap,\nso adjacent blocks look like (xt, . . . , xt+p\u22121) and (xt+1, . . . , xt+p). in the above\nexample, the first two of 10 blocks of blocks would be\n\nb\u20321 =;11\n\n22 ,12\n\n32 ,13\n42<\n\none potential pseudo-dataset would be\n\nb\u20322 =;12\n\n32 ,13\n\n42 ,14\n\n52< .\n\ny\u2217 =;17\n\n82 ,18\n\n92 ,1 9\n\n92 ,1 9\n102 ,18\n13\n42 ,14\n\n112 ,\n102 ,110\n52 ,15\n62 ,11\n\n22 ,12\n\n32 ,13\n\n42< .\n\nthe blocks-of-blocks strategy is implicit in the rest of our block bootstrap dis-\ncussion. however, there are situations where vectorizing the data to work with the yt\nor reexpressing t as ty is difficult or awkward. when these challenges become too\ngreat an impediment, a pragmatic solution is to adopt the naive approach correspond-\ning to p = 1.\nexample 9.11 (tree rings)\nthe website for this book provides a dataset related\nto tree rings for the long-lived bristlecone pine pinus longaeva at campito mountain\nin california. raw basal area growth increments are shown in figure 9.6 for one\nparticular tree with rings corresponding to the n = 452 years from 1532 to 1983. the\ntime series considered below has been detrended and standardized [277].\n\n "}, {"Page_number": 316, "text": "9.5 bootstrapping dependent data\n\n309\n\ns\nt\nn\ne\nm\ne\nr\nc\nn\ni\n \na\ne\nr\na\n\n \nl\na\ns\na\nb\nw\na\nr\n\n \n\n0\n0\n8\n\n0\n0\n6\n\n0\n0\n4\n\n0\n0\n2\n\n0\n\n1600\n\n1700\n\n1800\n\n1900\n\nfigure 9.6 raw bristlecone pine basal area growth increments for the years 1532\u20131983\ndiscussed in example 9.11.\n\nyear\n\nconsider estimating the standard error of the lag 2 autocorrelation of the basal\narea increments, that is, the correlation between xt and xt+2. the sample lag 2\nautocorrelation is\"r = 0.183. to apply the blocks-of-blocks method we must use\np = 3 so that each small block includes both xt and xt+2 for t = 1, . . . ,450.\nthus x yields 450 triples yt = (xt, xt+1, xt+2) and the vectorized series is\ny = (y1, . . . ,y450). from these 450 blocks, we may resample blocks of blocks. let\neach of these blocks of blocks be comprised of 25 of the small blocks. the lag 2\ncorrelation can be estimated as\n\n450/t=1\n\n(yt,1 \u2212 m)(yt,3 \u2212 m)= 452/t=1\n\n(xt \u2212 m)2\n\n\"r =\n\nwhere yt,j is the jth element in yt and m is the mean of x1, . . . , xn. the denominator\nand m are expressed here in terms of x for brevity, but they can be reexpressed in\nterms of y so that\"r = ty(\"fy).\napplying the moving blocks-of-blocks bootstrap by resampling the yt and\nassembling a pseudo-dataset y\u2217i yields a bootstrap estimate\"r\u2217i for each i = 1, . . . , b.\nthe standard deviation of the resulting r\u2217i , that is, the estimated standard error of\"r,\nis 0.51. a bootstrap bias estimate is \u22120.008 (see section 9.2.4).\n!\n9.5.2.4 centering and studentizing the moving and nonmoving block boot-\nstrap methods yield different bootstrap distributions for\"\u03b8\u2217n. to see this, consider when\n\u03b8 = ext and\"\u03b8 = \u00afx. for the nonmoving block bootstrap, assume that n = lb and note\n\nthat the blocks b\u2217i are i.i.d., each with probability 1/b. let e\u2217 represent expectation\n\n "}, {"Page_number": 317, "text": "chapter 9 bootstrapping\n\n310\nwith respect to the block bootstrap distribution. then\n\n1\nl\n\nx\u2217j\u23ab\u23ac\u23ad\ne\u2217(\"\u03b8\u2217) = e\u2217\u23a7\u23a8\u23a9\nl/j=1\nx\u2217(i\u22121)l+j\u239e\u23a0\nb/i=1\u239b\u239d\nl/j=1\n\n1\nl\n\n=\n\n1\nb\nn \u00afx\n=\nlb\n= \u00afx.\n\n(9.25)\n\n(9.26)\n\nhowever, for the moving block bootstrap it can be shown that\n\ne\u2217(\"\u03b8\u2217) = e\u2217\u23a7\u23a8\u23a9\n\n1\n\n1\nl\n\nx\u2217j\u23ab\u23ac\u23ad\nl/j=1\nn\u2212l+1/i=1 \u239b\u239d\nn \u2212 l + 1\nn \u2212 l + 1;n \u00afx \u2212\n\n1\n\n=\n\n=\n\nx\u2217i+j\u22121\u239e\u23a0\n(l \u2212 i)(xi + xn\u2212i+1)< .\n\n1\nl\n1\nl\n\nl/j=1\nl\u22121/i=1\n\nerror converges to zero at a slow rate that is unnecessary given the approach described\nin the next paragraph.\ngiven in (9.26). this alternative centering could present a significant new hurdle for\n\nthe second term in the braces in (9.26) accounts for the fact that observations within l\npositions of either end of the series occur in fewer blocks and hence contribute fewer\nterms to the double sum above. in other words, the moving block bootstrap exhibits\nedge effects. note, however, that the mean squared difference between bootstrap\nmeans is o(l/n2) so the difference vanishes as n \u2192 \u221e.\nthere is an important implication of the fact that\"\u03b8\u2217 is unbiased for the nonmov-\ning block bootstrap but biased for the moving block approach. suppose we intend\nto apply the moving block bootstrap to a pivoted quantity such as\"\u03b8 \u2212 \u03b8. one would\nnaturally consider the bootstrap version\"\u03b8\u2217 \u2212\"\u03b8. however, e\u2217{\"\u03b8\u2217 \u2212\"\u03b8} /= 0, and this\nthe improvement is to center using\"\u03b8\u2217 \u2212 e\u2217\"\u03b8\u2217. for the sample mean, e\u2217\"\u03b8\u2217 is\napplying the moving blocks bootstrap to a more general statistic\"\u03b8 = t(\"f) because\nthe calculation of e\u2217\"\u03b8\u2217 can be challenging. fortunately, it can be shown that under\nsuitable conditions it suffices to apply the pivoting approach \u03b8( \u00afx\u2217) \u2212 \u03b8(e\u2217 \u00afx\u2217) when\nbootstrapping any statistic that can be expressed as\"\u03b8 =\"\u03b8( \u00afx) if\"\u03b8 is a smooth function\n[140, 275, 398]. this is called the smooth function model, which is a common context\ninwhichtostudyandsummarizeasymptoticperformanceofblockbootstrapmethods.\nstudentizing the statistic by scaling it with its estimated standard deviation suf-\nfers from an analogous problem. recognizing the smooth function result above, let us\nmakethesimplifyingassumptionthat\"\u03b8 = \u00afxandlimitconsiderationtothenonmoving\n\n "}, {"Page_number": 318, "text": "9.5 bootstrapping dependent data\n\n311\n\nbootstrap. a natural studentization would be seem to be ( \u00afx\u2217 \u2212 e\u2217 \u00afx\u2217)/(s\u2217/\u221an)\nwhere s\u2217 is the standard deviation of the bootstrap data x\u2217. however, s\u2217 is not a\ngood approximation to var\u2217{ \u00afx\u2217 \u2212 e\u2217 \u00afx\u2217} [296, 312]. the improvements\n\n\u02dcs2\n\u2217,1 =\n\nand\n\nl/j=1\n\nl/k=1\n\n(xil+j \u2212 \u00afx)(xil+k \u2212 \u00afx)\n\n\u02dcs2\n\u2217,2 =\n\n1\nn\n\nl/j=1\n\nl/k=1\n\n(xil+j \u2212 \u00afx\u2217)(xil+k \u2212 \u00afx\u2217)\n\n1\nn\n\nb/i=1\nb\u22121/i=0\n\n(9.27)\n\n(9.28)\n\nare suggested alternatives [275, 312, 399]. either is adequate.\nanother way to correct for edge effects is the circular block bootstrap [512].\nthis approach extends the observed time series by defining \u201cnew\u201d observations\nx(new)\nn+i = xi for 1 \u2264 i \u2264 b \u2212 1, which are concatenated to the end of the original\nseries. then overlapping blocks are formed from the \u201cwrapped\u201d series in the same\nmannerasforthemovingblocksbootstrap.theseblocksareresampledindependently\nwith replacement with equal probabilities. since each xi (1 \u2264 i \u2264 n) in the original\nx now occurs exactly n times in the extended collection of blocks, the edge effect is\neliminated.\nthe stationary block bootstrap tackles the same edge effect issue by using\nblocks of random lengths [513]. the block starting points are chosen i.i.d. over\n{1, . . . , n}. the ending points are drawn according to the geometric distribution given\nby p[endpoint = j] = p(1 \u2212 p)j\u22121. thus block lengths are random with a condi-\ntional mean of 1/p. the choice of p is a challenging question; however, simulations\nshow that stationary block bootstrap results are far less sensitive to the choice of p\nthan is the moving blocks bootstrap to the choice of l [513]. theoretically, it suffices\nthat p \u2192 0 and np \u2192 \u221e as n \u2192 \u221e. from a practical point of view, p = 1/ l can be\nrecommended. the term stationary block bootstrap is used to describe this method\nbecause it produces a stationary time series, whereas the moving and nonmoving\nblock bootstraps do not.\n9.5.2.5 block size performanceofablockbootstraptechniquedependsonblock\nlength, l.when l = 1,themethodcorrespondstothei.i.d.bootstrapandallcorrelation\nstructure is lost. for very large l, the autocorrelation is mostly retained but there will\nbe few blocks from which to sample. asymptotic results indicate that, for the block\nbootstrap, block length should increase as the length of the time series increases if\nthe method is to produce consistent estimators of moments, correct coverage proba-\nbilities for confidence intervals, and appropriate error rates for hypothesis tests (see\nsection 9.6.2). several approaches for choosing block length in practice have been\nsuggested. we limit discussion here to two methods relevant for the moving block\nbootstrap.\na reasonable basis for choosing block length is to consider the mse of the\nbootstrap estimator. in this chapter, we have considered \u03b8 = t(f) as an interesting\nfeature of a distribution f, and\"\u03b8 as an estimator of this quantity. the statistic\"\u03b8 will\n\n "}, {"Page_number": 319, "text": "chapter 9 bootstrapping\n\nknown f, such as var{\"\u03b8} or mse{\"\u03b8}. the bootstrap is used to estimate such quantities.\n\n312\nhave certain properties (features of its sampling distribution) that depend on the un-\nyet, the bootstrap estimator itself has its own bias, variance, and mean squared error\nthat again depend on f. these serve as criteria to evaluate the performance of the\nblock bootstrap and consequently to compare different choices for block length.\nthe mse of a bootstrap estimator can be estimated by bootstrapping the boot-\nstrap estimator. although neither of the methods discussed below implements a nest-\ning strategy as explicit as the one described in section 9.3.2.4, they both adopt the\nheuristic of multilevel resampling for estimation of the optimal block length, denoted\nlopt. an alternative approach is explored by [83].\n\nsubsampling plus bootstrapping the approach described here is based on\n\nan estimate of the mean squared error of a block bootstrap estimate when\"\u03b8 is the\nmean or a smooth function thereof [297]. define \u03c6b = bias{\"\u03b8} = e(\"\u03b8 \u2212 \u03b8) and \u03c6v =\nvar{\"\u03b8} = e{\"\u03b82} \u2212 (e\"\u03b8)2. let\"\u03c6b = bias\u2217{\"\u03b8\u2217} and\"\u03c6v = var\u2217{\"\u03b8\u2217} be block bootstrap\nestimatesof \u03c6b and \u03c6v.forexample,underthesmoothfunctionmodelwith \u00b5denoting\ni=1(h( \u00afx\u2217i ) \u2212 h( \u00afx))/b where \u00afx\u2217i is the mean\nthe true mean and \u03b8 = h(\u00b5),\"\u03c6b =#b\nofthe ithpseudo-datasetand h isthesmoothfunction.notethateach \u03c6j for j \u2208 {b, v}\ndepends on l, so we may write these quantities as \u03c6j(l). under suitable conditions,\none can show that\nvar{\"\u03c6j(l)} =\nbias{\"\u03c6j(l)} =\n\n(9.30)\n\n(9.29)\n\nc1l\n\nand therefore\n\n(9.31)\nfor j \u2208 {b, v}, although c1 and c2 depend on j. differentiating this last expression and\nsolving for the l that minimizes the mse, we find\n\nmse{\"\u03c6(l)j} =\n\nn3 +\n\nc1l\nn3 +\n\n1\n\nn2l2. ,\n\nc2\n\nn3 + o- l\nn3. ,\nnl + o- 1\nnl. ,\nn2l2 + o- l\n\nc2\n2\n\nlopt \u223c12c2\n\nc1 21/3\n\n2\n\nn1/3.\n\n(9.32)\n\nwhere the symbol \u223c is defined by the relation an \u223c bn if limn\u2192\u221e an/bn = 1. for\nsimplicity in the rest of this section, let us focus on bias estimation, letting \u03c6 = \u03c6b.\nwe will note later that the same result holds for variance estimation.\nthe goal is to derive the block length that minimizes mse{\"\u03c6(l)} with respect\nto l. we will do this by estimating mse{\"\u03c6(l)} for several candidate values of l and\nbootstrap to obtain\"\u03c6(l0). next, consider a smaller sub-dataset of size m < n for which\nwe can obtain an analogous estimate,\"\u03c6m(l\u2032) for some l\u2032. the estimate of mse{\"\u03c6(l\u2032)}\nwill depend on a collection of these\"\u03c6m(l\u2032) and the original\"\u03c6(l0).\n\nselect the best. begin by choosing a pilot block size l0 and performing the usual block\n\n "}, {"Page_number": 320, "text": "1\n\n(m)\ni\n\nlet x\n\nn \u2212 m + 1\n\n9.5 bootstrapping dependent data\n\nn\u2212m+1/i=1 )\"\u03c6i,m(l\u2032) \u2212\"\u03c6(l0)*2\n\n313\n(m)\ni = (xi, . . . , xi+m\u22121) denote a subsequence of x of length m, for\nusing b iterations and\ni = 1, . . . , n \u2212 m + 1. applying the block bootstrap to x\ntrial block length l\u2032 produces a point estimate of \u03c6, denoted\"\u03c6i,m(l\u2032), for each i. for\nj=1(h( \u00afx\u2217i,j) \u2212 h( \u00afxi))/b where \u00afxi is the mean of\nthe bias example above,\"\u03c6i,m =#b\nand \u00afx\u2217i,j is the mean of the jth bootstrap pseudo-dataset generated from x\n(m)\n(m)\nx\ni\ni\nfor j = 1, . . . , b. then an estimate of the mean squared error of the block bootstrap\nestimator\"\u03c6m(l\u2032) based on the subset of size m is\n!msem{\"\u03c6m(l\u2032)} =\nrecalling that\"\u03c6(l0) is the estimate obtained by bootstrapping the full dataset using a\nlet\"l\nopt minimize !msem{\"\u03c6(l\u2032)} with respect to l\u2032. this minimum may be found\n(m)\nopt estimates the best block\nby trying a sequence of l\u2032 and selecting the best. then\"l\nsize for a series of length m. since the real data series is length n and since optimal\n(m)\nblock size is known to be of order n1/3, we must scale up\"l\nopt accordingly to yield\n(n)\nopt = (n/m)1/3\"l\n\"lopt =\"l\nthe procedure described here applies when \u03c6 is the bias or variance functional.\nfor estimating a distribution function, an analogous approach leads to an appropriate\nscaling factor of\"lopt = (n/m)1/4\"l\ngood choices for m and l0 are unclear. choices like m \u2248 0.25n and l0 \u2248 0.05n\nhave produced reasonable simulation results in several examples [297, 402]. it is\nimportant that the pilot value l0 is plausible, but the effect of l0 can potentially be\nreduced through iteration. specifically, after applying the procedure with an initial\npilot value l0, the result may be iteratively refined by replacing the previous pilot\n\npilot block length l0.\n\n(9.33)\n\n(m)\nopt .\n\n(m)\nopt .\n\n(m)\n\n,\n\n(n)\nopt and repeating the process.\n\njackknifing plus bootstrapping an empirical plug-in approach has been\nsuggested as an alternative to the above method [404]. here, an application of the\njackknife-after-bootstrap approach [180, 401] is applied to estimate properties of the\nbootstrap estimator.\nequation (9.32) identifies the optimal rate at which block size should grow with\nincreasing sample size, namely proportionally to n1/3. however, a concrete choice\nfor lopt cannot be made without determination of c1 and c2.\n\nrecall the expressions for mse{\"\u03c6(l)} and lopt in equations (9.31) and (9.32).\n\nrearranging terms in equations (9.29) and (9.30) yields\n\nvalue with the current estimate\"l\n\nc1 \u223c n3l\u22121var8\"\u03c6(l)9,\nc2 \u223c nl bias8\"\u03c6(l)9.\n\n(9.34)\n(9.35)\n\nthus if var{\"\u03c6(l)} and bias{\"\u03c6(l)} can be approximated by convenient estimators\"v and\n\"b, then we can estimate c1, c2, and hence mse{\"\u03c6(l)}. moreover, equation (9.32) can\n\nbe applied to estimate lopt.\n\n "}, {"Page_number": 321, "text": "314\n\nchapter 9 bootstrapping\n\n(9.36)\n\nthe estimator\n\n\"b = 2(\"\u03c6(l\u2032) \u2212\"\u03c6(2l\u2032))\n\nthe crux of this strategy is crafting the estimators\"b and\"v. one can show that\nisconsistentforbias{\"\u03c6(l\u2032)}undersuitableconditionswhere l\u2032 is achosenblocklength.\nthe choice of l\u2032 determines the accuracy of the estimator\"b.\ncalculation of \"v relies on a jackknife-after-bootstrap strategy [180]. applied\nwithin the blocks-of-blocks context, this approach deletes an adjacent set of blocks\nand resamples the remainder. from this resample, \"\u03c6 is calculated. repeating this\nprocess sequentially as the set of deleted blocks progresses from one end of x to the\nother, one block at a time, yields the complete set of\"\u03c6 bootstrap pseudo-values whose\nvariance may be calculated and scaled up to estimate var{\"\u03c6}.\nthe details are as follows. when the moving block bootstrap is applied to a\ndata sequence (x1, . . . , xn), there are n \u2212 l + 1 blocks b1, . . . , bn\u2212l+1 available for\nresampling. the blocks are bj = (xj, . . . , xj+l\u22121) for j = 1, . . . , n \u2212 l + 1. sup-\npose that we delete d adjacent blocks from this set of blocks. there are n \u2212 l \u2212 d + 2\npossible ways to do this, deleting (bi, . . . , bi+d\u22121) for i = 1, . . . , n \u2212 l \u2212 d + 2.\nthe ith such deletion leads to the ith reduced dataset of blocks, called a block-deleted\ndataset. by performing a moving block bootstrap with block length l\u2032 on the ith\nblock-deleted dataset, the ith block-deleted value\"\u03c6i can be computed via\"\u03c6i = \u03c6(\"f\u2217i )\nwhere\"f\u2217i is the empirical distribution function of the sample from this moving block\nbootstrap of the ith block-deleted dataset.\nhowever,the n \u2212 l \u2212 d + 2separateblock-deletedbootstrapsconsideredabove\ncan be carried out without explicitly conducting the block deletion steps. for each i in\nturn,thecollectionoforiginalbootstrappseudo-datasetscanbesearchedtoidentifyall\nx\u2217 in which none of the ith set of deleted blocks are present. then this subcollection\nof the original bootstrap pseudo-datasets can be used to calculate\"\u03c6i. an appropriate\nvariance estimator based on the block-deleted data is\n\n\"v =\n\u02dc\u03c6i =\n\nd\n\nn \u2212 l \u2212 d + 1\n\nn\u2212l\u2212d+2/i=1\n\n(\u02dc\u03c6i \u2212\"\u03c6)2\nn \u2212 l \u2212 d + 2\n(n \u2212 l + 1)\"\u03c6 \u2212 (n \u2212 l \u2212 d \u2212 1)\"\u03c6i\n\nd\n\n(9.37)\n\n(9.38)\n\nfinally, lopt can be found using equation (9.32). in this manner, the computational\neffortassociatedwithrepeatedresamplingisreplacedbyincreasedcodingcomplexity\nneeded to keep track of (or search for) the appropriate pseudo-datasets for each i.\n\nand\"\u03c6 is the estimate of \u03c6 resulting from the original application of the bootstrap.\nnote that the choice of d will strongly affect the performance of\"v as an estimator of\nvar{\"\u03c6(lopt)}.\nunder suitable conditions,\"b is consistent for bias{\"\u03c6} and \"v is consistent for\nvar{\"\u03c6} when d \u2192 \u221e and d/n \u2192 0 as n \u2192 \u221e [404]. yet a key part of this method\n\nwhere\n\n "}, {"Page_number": 322, "text": "9.6 bootstrap performance\n\n315\nremains to be specified: the choices for d and l0. the values of l0 = n1/5 and d =\nn1/3l2/3 are suggested on the basis of heuristic arguments and simulation [401, 403,\n404]. an iterative strategy to refine l0 is also possible.\nthese results pertain to cases when estimating the best block length for boot-\nstrap estimation of bias or variance. analogous arguments can be used to address\nthe situation when \u03c6 represents a quantile. in this case, assuming studentization,\nlopt \u223c (c2\n2/c1)1/4n1/4 and suggested starting values are l0 = n1/6 and d = 0.1n1/3l2/3\n[404].\n\n9.6 bootstrap performance\n\nindependent data case\n\n9.6.1\nall the bootstrap methods described in this chapter rely on the principle that the boot-\nstrap distribution should approximate the true distribution for a quantity of interest.\nstandardparametricapproachessuchasa t-testandthecomparisonofaloglikelihood\nratio to a \u03c72 distribution also rely on distributional approximation.\nwe have already discussed one situation where the i.i.d. bootstrap approxima-\ntion fails: for dependent data. the bootstrap also fails for estimation of extremes.\nfor example, bootstrapping the sample maximum can be catastrophic; see [142] for\ndetails. finally, the bootstrap can fail for heavy-tailed distributions. in these circum-\nstances, the bootstrap samples outliers too frequently.\nthere is a substantial asymptotic theory for the consistency and rate of con-\nvergence of bootstrap methods, thereby formalizing the degree of approximation it\nprovides. these results are mostly beyond the scope of this book, but we mention a\nfew main ideas below.\nfirst, the i.i.d. bootstrap is consistent under suitable conditions [142]. specif-\nically, consider a suitable space of distribution functions containing f, and let\nnf denote a neighborhood of f into which \"f eventually falls with probabil-\nity 1. if the distribution of a standardized r(x, g) is uniformly weakly conver-\ngent when the elements of x are drawn from any g \u2208 nf, and if the map-\nping from g to the corresponding limiting distribution of r is continuous, then\np\u2217\u2019hhp[r(x\u2217,\"f) \u2264 q] \u2212 p[r(x, f) \u2264 q]hh > \u03f5( \u2192 0forany \u03f5andany qas n \u2192 \u221e.\nedgeworth expansions can be used to assess the rate of convergence [295].\nsuppose that r(x, f) is standardized and asymptotically pivotal, when r(x, f) is\nasymptotically normally distributed. then the usual rate of convergence for the boot-\nstrap is given by p\u2217[r(x\u2217,\"f) \u2264 q] \u2212 p[r(x, f) \u2264 q] = op(n\u22121). without pivot-\ning, the rate is typically only op(n\u22121/2). in other words, coverage probabilities for\nconfidenceintervalsareo(n\u22121/2)accurateforthebasic,unpivotedpercentilemethod,\nbut o(n\u22121) accurate for bca and the bootstrap t. the improvement offered by the\nnested bootstrap depends on the accuracy of the original interval and the type of\ninterval. in general, nested bootstrapping can reduce the rate of convergence of cov-\nerage probabilities by an additional multiple of n\u22121/2 or n\u22121. most common infer-\nential problems are covered by these convergence results, including estimation of\n\n "}, {"Page_number": 323, "text": "chapter 9 bootstrapping\n\n316\nsmooth functions of sample moments and solutions to smooth maximum likelihood\nproblems.\nit is important to note that asymptotic approaches such as the normal approx-\nimation via the central limit theorem are o(n\u22121/2) accurate. this illustrates the\nbenefit of standardization when applying bootstrap methods because the conver-\ngence rate for the bootstrap in that case is superior to what can be achieved using\nordinary asymptotic methods. accessible discussions of the increases in conver-\ngence rates provided by bca, the nested bootstrap, and other bootstrap improve-\nments are given in [142, 183]. more advanced theoretical discussion is also available\n[47, 295, 589].\n\n9.6.2 dependent data case\nunder suitable conditions, the dependent data bootstrap methods discussed here are\nalso consistent. the convergence performance of these methods depends on whether\nblock length l is the correct order (e.g., l \u221d n1/3 for bias and variance estimation). in\ngeneral, performance of block bootstrap methods when incorporating studentization\nis superior to what is achieved by normal approximation via the central limit theorem,\nbut not as good as the performance of the bootstrap for i.i.d. data.\nnot all dependent data bootstrap methods are equally effective. the moving\nblockbootstrapissuperiortothenonmovingblockapproachintermsofmeansquared\nerror. suppose that bootstrapping is focused on estimating the bias or variance of an\nunderlying estimator. then the asymptotic mean squared error (amse) is 1.52/3 \u2248\n31% larger for the nonmoving blocks bootstrap than for the moving blocks method\nwhen the asymptotically optimal block sizes are used for each approach [297, 400].\nthe difference is attributable to the contribution of variances to amse; the bias terms\nfor the two methods are the same. both amses converge to zero at the same rate.\nmore sophisticated bootstrapping methods for dependent data can offer better\nasymptotic performance but are considerably more cumbersome and sometimes lim-\nitedtoapplicationsthatarelessgeneralthanthosethatcanbeaddressedwithoneofthe\nblock methods described above. the tapered block bootstrap seeks to reduce the bias\nin variance estimation by down-weighting observations near the edges of blocks [498,\n499]. the sieve bootstrap aims to approximate the data generation process by initially\nfitting an autoregressive process. recentered residuals are then resampled and used to\ngenerate bootstrap datasetsx\u2217 from the fitted model via a recursion method for which\nthe impact of initializing the process is washed away as iterations increase [81, 82,\n393]. the dependent wild bootstrap shares the superior asymptotic properties of the\ntapered block bootstrap and can be extended to irregularly spaced time series [590].\n\n9.7 other uses of the bootstrap\n\nby viewing x\u2217 as a random sample from a distribution\"f with known parameter\"\u03b8,\n\nthe bootstrap principle can be seen as a tool used to approximate the likelihood func-\ntion itself. bootstrap likelihood [141] is one such approach, which has connections\n\n "}, {"Page_number": 324, "text": "9.8 permutation tests\n\n317\nto empirical likelihood methods. by ascribing random weights to likelihood compo-\nnents, a bayesian bootstrap can be developed [558]. a generalization of this is the\nweighted likelihood bootstrap, which is a powerful tool for approximating likelihood\nsurfaces in some difficult circumstances [485].\nthe bootstrap is generally used for assessing the statistical accuracy and preci-\nsion of an estimator. bootstrap aggregating, or bagging, uses the bootstrap to improve\nthe estimator itself [63]. suppose that the bootstrapped quantity, r(x, f), depends\non f only through \u03b8. thus, the bootstrap values of r(x, \u03b8) are r(x\u2217,\"\u03b8). in some\ncases, \u03b8 is the result of a model-fitting exercise where the form of the model is un-\ncertain or unstable. for example, classification and regression trees, neural nets, and\nlinear regression subset selection are all based on models whose form may change\nsubstantially with small changes to the data.\nin these cases, a dominant source of variability in predictions or estimates\nj ,\nj is the parameter estimate arising from the jth bootstrap pseudo-dataset.\nsince each bootstrap pseudo-dataset represents a perturbed version of the original\ndata, the models fit to each pseudo-dataset can vary substantially in form. thus \u03b8 \u2217\nprovides a sort of model averaging that can reduce mean squared estimation error in\nmodel-averaging philosophy is provided in [331].\na related strategy is the bootstrap umbrella of model parameters, or bumping\napproach [632]. for problems suitable for bagging, notice that the bagged average is\nnot always an estimate from a model of the same class as those being fit to the data.\nfor example, the average of classification trees is not a classification tree. bumping\navoids this problem.\nsuppose that h(\u03b8,x) is some objective function relevant to estimation in the\nsense that high values of h correspond to \u03b8 that are very consistent with x. for\nexample, h could be the log likelihood function. the bumping strategy generates\nj = argmax\u03b8 h(\u03b8,x\u2217j). the original dataset is included\nbootstrap pseudo-values via\"\u03b8\u2217\namong the bootstrap pseudo-datasets, and the final estimate of \u03b8 is taken to be the\n\"\u03b8j that maximizes h(\u03b8,x) with respect to \u03b8. thus, bumping is really a method for\nsearching through a space of models (or parameterizations thereof) for a model that\nyields a good estimator.\n\nmay be the model form. bagging consists of replacing\"\u03b8 with \u03b8 \u2217 = (1/b)#b\nj=1\"\u03b8 \u2217\nwhere\"\u03b8 \u2217\ncases where perturbing the data can cause significant changes to\"\u03b8. a review of the\n\n9.8 permutation tests\nthereareotherimportanttechniquesasidefromthebootstrapthatsharetheunderlying\nstrategy of basing inference on \u201cexperiments\u201d within the observed dataset. perhaps\nthe most important of these is the classic permutation test that dates back to the era of\nfisher [194] and pitman [509, 510]. comprehensive introductions to this field include\n[173, 271, 439]. the basic approach is most easily explained through a hypothetical\nexample.\nexample 9.12 (comparison of independent group means) consider a medical\nexperiment where rats are randomly assigned to treatment and control groups. the\n\n "}, {"Page_number": 325, "text": "chapter 9 bootstrapping\n\n318\noutcome xi is then measured for the ith rat. under the null hypothesis, the outcome\ndoes not depend on whether a rat was labeled as treatment or control. under the\nalternative hypothesis, outcomes tend to be larger for rats labeled as treatment.\na test statistic t measures the difference in outcomes observed for the two\ngroups. for example, t might be the difference between group mean outcomes,\nhaving value t1 for the observed dataset.\nunder the null hypothesis, the individual labels \u201ctreatment\u201d and \u201ccontrol\u201d are\nmeaningless because they have no influence on the outcome. since they are mean-\ningless, the labels could be randomly shuffled among rats without changing the joint\nnull distribution of the data. shuffling the labels creates a new dataset: although one\ninstance of each original outcome is still seen, the outcomes appear to have arisen\nfrom a different assignment of treatment and control. each of these permuted datasets\nis as likely to have been observed as the actual dataset, since the experiment relied on\nrandom assignment.\nlet t2 be the value of the test statistic computed from the dataset with this\nfirst permutation of labels. suppose all m possible permutations (or a large number\nof randomly chosen permutations) of the labels are examined, thereby obtaining\nt2, . . . , tm.\nunderthenullhypothesis, t2, . . . , tm weregeneratedfromthesamedistribution\nthat yielded t1. therefore, t1 can be compared to the empirical quantiles of t1, . . . , tm\nto test a hypothesis or construct confidence limits.\n!\nto pose this strategy more formally, suppose that we observe a value t for a\ntest statistic t having density f under the null hypothesis. suppose large values of\nt indicate that the null hypothesis is false. monte carlo hypothesis testing proceeds\nby generating a random sample of m \u2212 1 values of t drawn from f. if the observed\nvalue t is the kth largest among all m values, then the null hypothesis is rejected at\na significance level of k/m. if the distribution of the test statistic is highly discrete,\nthen ties found when ranking t can be dealt with naturally by reporting a range of\np-values. barnard [22] posed the approach in this manner; interesting extensions are\noffered in [38, 39].\nthere are a variety of approaches for sampling from the null distribution of the\ntest statistic. the permutation approach described in example 9.12 works because\n\u201ctreatment\u201d and \u201ccontrol\u201d are meaningless labels assigned completely at random and\nindependentofoutcome,underthenullhypothesis.thissimplepermutationapproach\ncan be broadened for application to a variety of more complicated situations. in all\ncases, the permutation test relies heavily on the condition of exchangeability. the\ndata are exchangeable if the probability of any particular joint outcome is the same\nregardless of the order in which the observations are considered.\nthere are two advantages to the permutation test over the bootstrap. first, if\nthe basis for permuting the data is random assignment, then the resulting p-value\nis exact (if all possible permutations are considered). for such experiments, the\napproach is usually called a randomization test. in contrast, standard parametric ap-\nproaches and the bootstrap are founded on asymptotic theory that is relevant for large\nsample sizes. second, permutation tests are often more powerful than their boot-\nstrap counterparts. however, the permutation test is a specialized tool for making\n\n "}, {"Page_number": 326, "text": "9.8 permutation tests\n\n319\n\ntable 9.3 forty years of fishery data: numbers of recruits (r) and spawners (s).\n\nr\n68\n77\n299\n220\n142\n287\n276\n115\n64\n206\n\ns\n56\n62\n445\n279\n138\n428\n319\n102\n51\n289\n\nr\n222\n205\n233\n228\n188\n132\n285\n188\n224\n121\n\ns\n351\n282\n310\n266\n256\n144\n447\n186\n389\n113\n\nr\n311\n166\n248\n161\n226\n67\n201\n267\n121\n301\n\ns\n412\n176\n313\n162\n368\n54\n214\n429\n115\n407\n\nr\n244\n222\n195\n203\n210\n275\n286\n275\n304\n214\n\ns\n265\n301\n234\n229\n270\n478\n419\n490\n430\n235\n\na comparison between distributions, whereas a bootstrap tests hypotheses about pa-\nrameters, thereby requiring less stringent assumptions and providing greater flex-\nibility. the bootstrap can also provide a reliable confidence interval and standard\nerror, beyond the mere p-value given by the permutation test. the standard deviation\nobserved in the permutation distribution is not a reliable standard error estimate. ad-\nditional guidance on choosing between a permutation test and a bootstrap is offered in\n[183, 271, 272].\n\nproblems\n\n9.1. let x1, . . . , xn \u223c i.i.d. bernoulli(\u03b8). define r(x, f) = x \u2212 \u03b8 and r\u2217 = r(x\u2217,\"f),\nwhere x\u2217 is a bootstrap pseudo-dataset and\"f is the empirical distribution of the data.\nderive the exact e\u2217{r\u2217} and var\u2217{r\u2217} analytically.\n9.2. suppose \u03b8 = g(\u00b5), where g is a smooth function and \u00b5 is the mean of the distribution\nfrom which the data arise. consider bootstrapping r(x, f) = g(x) \u2212 g(\u00b5).\na. show that e\u2217{x\u2217} = x and var\u2217{x\u2217} =\"\u00b52/n, where\"\u00b5k =#n\nb. use taylor series to show that\n\ni=1 (xi \u2212 x)k.\n\nand\n\n6n2 + \u00b7\u00b7\u00b7\n\n2n +\n\ng\u2032\u2032(x)\"\u00b52\ne\u2217{r(x\u2217,\"f)} =\ng\u2032(x)2\"\u00b52\n\n\u2212\n\nn\n\ng\u2032\u2032\u2032(x)\"\u00b53\n4n2 -\"\u00b52 \u2212\"\u00b54\n\ng\u2032\u2032(x)2\n\nvar\u2217{r(x\u2217,\"f)} =\n\nn. + \u00b7\u00b7\u00b7 .\n\n9.3. justify the choice of b for bca given in section 9.3.2.1.\n9.4. table 9.3 contains 40 annual counts of the numbers of recruits and spawners in a salmon\npopulation. the units are thousands of fish. recruits are fish that enter the catchable\npopulation. spawners are fish that are laying eggs. spawners die after laying eggs.\n\n "}, {"Page_number": 327, "text": "320\n\nchapter 9 bootstrapping\n\ntable 9.4 survival times (days) for patients with two types of terminal cancer\n\nstomach\n\nbreast\n\n25\n146\n\n24\n1581\n\n42\n340\n\n40\n1804\n\n45\n396\n\n719\n3460\n\n46\n412\n\n727\n3808\n\n51\n876\n\n791\n\n103\n1112\n\n124\n\n1166\n\n1235\n\nthe classic beverton\u2013holt model for the relationship between spawners and\n\nrecruits is\n\nr =\n\n1\n\n\u03b21 + \u03b22/s\n\n,\n\n\u03b21 \u2265 0 and \u03b22 \u2265 0,\n\nwhere r and s are the numbers of recruits and spawners, respectively [46]. this model\nmay be fit using linear regression with the transformed variables 1/r and 1/s.\nconsider the problem of maintaining a sustainable fishery. the total population\nabundancewillonlystabilizeif r = s.thetotalpopulationwilldeclineiffewerrecruits\nare produced than the number of spawners who died producing them. if too many\nrecruits are produced, the population will also decline eventually because there is not\nenough food for them all. thus, only some middle level of recruits can be sustained\nindefinitely in a stable population. this stable population level is the point where the\n45\u25e6 line intersects the curve relating r and s.\na. fit the beverton\u2013holt model, and find a point estimate for the stable population\nlevel where r = s. use the bootstrap to obtain a corresponding 95% confidence\ninterval and a standard error for your estimate, from two methods: bootstrapping\nthe residuals and bootstrapping the cases. histogram each bootstrap distribution,\nand comment on the differences in your results.\n\nb. provide a bias-corrected estimate and a corresponding standard error for the\n\nc. use the nested bootstrap with prepivoting to find a 95% confidence interval for the\n\ncorrected estimator.\n\nstabilization point.\n\n9.5. patients with advanced terminal cancer of the stomach and breast were treated with\nascorbate in an attempt to prolong survival [87]. table 9.4 shows survival times (days).\nwork with the data on the log scale.\na. use the bootstrap t and bca methods to construct 95% confidence intervals for the\n\nb. use a permutation test to examine the hypothesis that there is no difference in mean\n\nmean survival time of each group.\n\nsurvival times between groups.\n\nc. having computed a reliable confidence interval in (a), let us explore some possible\nmissteps. construct a 95% confidence interval for the mean breast cancer survival\ntime by applying the simple bootstrap to the logged data and exponentiating the\nresulting interval boundaries. construct another such confidence interval by apply-\ning the simple bootstrap to the data on the original scale. compare with (a).\n\n "}, {"Page_number": 328, "text": "9.8 permutation tests\n\n321\n\n9.6. the national earthquake information center has provided data on the number of earth-\nquakes per year exceeding magnitude 7.0 for the years from 1900 to 1998 [341]. these\ndata are available from the website for this book. difference the data so that the number\nfor each year represents the change since the previous year.\na. determine a suitable block length for bootstrapping in this problem.\nb. estimate the 90th percentile of the annual change. estimate the standard error of\n\nthis estimate using the moving block bootstrap.\n\nc. apply the model-based approach of section 9.5.1, assuming an ar(1) model, to\n\nestimate the standard error from part (b).\n\nd. estimate the lag-1 autocorrelation of the annual change. find the bootstrap bias and\nstandard error of this estimate using the moving block bootstrap with an appropriate\nblocks-of-blocks strategy.\n\n9.7. use the problem of estimating the mean of a standard cauchy distribution to illustrate\nhow the bootstrap can fail for heavy-tailed distributions. use the problem of estimating\n\u03b8 for the unif(0, \u03b8) distribution to illustrate how the bootstrap can fail for extremes.\n\n9.8. perform a simulation experiment on an artificial problem of your design, to compare\nthe accuracy of coverage probabilities and the widths of 95% bootstrap confidence\nintervals constructed using the percentile method, the bca method, and the bootstrap\nt. discuss your findings.\n\n9.9. conduct an experiment in the same spirit as the previous question to study block\n\nbootstrapping for dependent data, investigating the following topics.\na. compare the performance of the moving and nonmoving block bootstraps.\nb. compare the performance of the moving block bootstrap for different block lengths\n\nl, including one choice estimated to be optimal.\n\nc. compare the performance of the moving block bootstrap with and without\n\nstudentization.\n\n "}, {"Page_number": 329, "text": "chapter 10\nnonparametric density\nestimation\n\nthischapterconcernsestimationofadensityfunction f usingobservationsofrandom\nvariables x1, . . . ,xn sampled independently from f. initially, this chapter focuses on\nunivariate density estimation. section 10.4 introduces some methods for estimating\na multivariate density function.\nin exploratory data analysis, an estimate of the density function can be used to\nassessmultimodality,skew,tailbehavior,andsoforth.forinference,densityestimates\nare useful for decision making, classification, and summarizing bayesian posteriors.\ndensity estimation is also a useful presentational tool since it provides a simple,\nattractive summary of a distribution. finally, density estimation can serve as a tool\nin other computational methods, including some simulation algorithms and markov\nchain monte carlo approaches. comprehensive monographs on density estimation\ninclude [581, 598, 651].\nthe parametric solution to a density estimation problem begins by assuming a\nparametric model, x1, . . . ,xn \u223c i.i.d. fx|\u03b8, where \u03b8 is a very low-dimensional para-\nmeter vector. parameter estimates!\u03b8 are found using some estimation paradigm, such\nas maximum likelihood, bayesian, or method-of-moments estimation. the resulting\ndensity estimate at x is fx|\u03b8(x|!\u03b8). the danger with this approach lies at the start:\nrelying on an incorrect model fx|\u03b8 can lead to serious inferential errors, regardless\nof the estimation strategy used to generate!\u03b8 from the chosen model.\nin this chapter, we focus on nonparametric approaches to density estimation\nthat assume very little about the form of f. these approaches use predominantly\nlocal information to estimate f at a point x. more precise viewpoints on what makes\nan estimator nonparametric are offered in [581, 628].\none familiar nonparametric density estimator is a histogram, which is a piece-\nwise constant density estimator. histograms are produced automatically by most soft-\nware packages and are used so routinely that one rarely considers their underlying\ncomplexity. optimal choice of the locations, widths, and number of bins is based on\nsophisticated theoretical analysis.\nanother elementary density estimator can be motivated by considering how\ndensity functions assign probability to intervals. if we observe a data point xi = xi,\nwe assume that f assigns some density not only at xi but also in a region around xi, if\nf is smooth enough. therefore, to estimate f from x1, . . . , xn \u223c i.i.d. f, it makes\ncomputational statistics, second edition. geof h. givens and jennifer a. hoeting.\n\u00a9 2013 john wiley & sons, inc. published 2013 by john wiley & sons, inc.\n\n325\n\n "}, {"Page_number": 330, "text": "1\n2hn\n\nchapter 10 nonparametric density estimation\n\n326\nsense to accumulate localized probability density contributions in regions around\neach xi.\nspecifically, to estimate the density at a point x, suppose we consider a region\nof width dx = 2h, centered at x, where h is some fixed number. then the proportion\nof the observations that fall in the interval \u03b3 = [x \u2212 h, x + h] gives an indication\nof the density at x. more precisely, we may take !f(x)dx = (1/n)\"n\ni=1 1{|x\u2212xi|<h},\nthat is,\n\n\u02c6f(x) =\n\n1{|x\u2212xi|<h},\n\nlet n\u03b3(h, n) =\"n\n\nn#i=1\nwhere 1{a} = 1 if a is true, and 0 otherwise.\ni=1 1{|x\u2212xi|<h} denote the number of sample points falling in\ntheinterval \u03b3.then n\u03b3 isabin(n, p(\u03b3))randomvariable,where p(\u03b3) =$ x+h\nx\u2212h f(t) dt.\nthus e{n\u03b3 /n} = p(\u03b3) and var{n\u03b3 /n} = p(\u03b3)(1 \u2212 p(\u03b3))/n. clearly nh must in-\ncrease as n\u03b3 increases in order for (10.1) to provide a reasonable estimator, yet\nwe can be more precise about separate requirements for n and h. the proportion\nof points falling in the interval \u03b3 estimates the probability assigned to \u03b3 by f. in\norder to approximate the density at x, we must shrink \u03b3 by letting h \u2192 0. then\nlimh\u21920 e{ \u02c6f(x)} = limh\u21920[p(\u03b3)/(2h)] = f(x). simultaneously, however, we want\nto increase the total sample size since var{ \u02c6f(x)} \u2192 0 as n \u2192 \u221e. thus, a funda-\nmental requirement for the pointwise consistency of the estimator \u02c6f in (10.1) is that\nnh \u2192 \u221e and h \u2192 0 as n \u2192 \u221e. we will see later that these requirements hold in far\ngreater generality.\n\n(10.1)\n\n10.1 measures of performance\nto better understand what makes a density estimator perform well, we must first\nconsider how to assess the quality of a density estimator. let \u02c6f denote an estimator\nof f that is based on some fixed number h that controls how localized the probability\ndensity contributions used to construct \u02c6f should be. a small h will indicate that \u02c6f(x)\nshould depend more heavily on data points observed near x, whereas a larger h will\nindicate that distant data should be weighted nearly equally to observations near x.\nto evaluate \u02c6f as an estimator of f over the entire range of support, one could\nuse the integrated squared error (ise),\n\nise(h) =% \u221e\n\n\u2212\u221e& \u02c6f(x) \u2212 f(x)\u20192\n\n(10.2)\nnote that ise(h) is a function of the observed data, through its dependence on \u02c6f(x).\nthus it summarizes the performance of \u02c6f conditional on the observed sample. if we\nwant to discuss the generic properties of an estimator without reference to a particular\nobserved sample, it seems more sensible to further average ise(h) over all samples\nthat might be observed. the mean integrated squared error (mise) is\n\ndx.\n\nmise(h) = e{ise(h)},\n\n(10.3)\n\n "}, {"Page_number": 331, "text": "where\n\n327\nwhere the expectation is taken with respect to the distribution f. thus mise(h) may\nbe viewed as the average value of a global measure of error [namely ise(h)] with\nrespect to the sampling density. moreover, with an interchange of expectation and\nintegration,\n\n10.2 kernel density estimation\n\nmise(h) =% mseh( \u02c6f(x)) dx,\n\n(10.4)\n\nmseh( \u02c6f(x)) = e() \u02c6f(x) \u2212 f(x)*2+ = var, \u02c6f(x)- +&bias, \u02c6f(x)-\u20192\n(10.5)\nand bias{ \u02c6f(x)} = e{ \u02c6f(x)} \u2212 f(x). equation (10.4) suggests that mise(h) can also\nbe viewed as accumulating the local mean squared error at every x.\nfor multivariate density estimation, ise(h) and mise(h) are defined analo-\ngously. specifically, ise(h) =$ [ \u02c6f(x) \u2212 f(x)]2dx and mise(h) = e{ise(h)}.\nmise(h) and ise(h) both measure the quality of the estimator \u02c6f, and each can\nbe used to develop criteria for selecting a good value for h. preference between these\ntwo measures is a topic of some debate [284, 299, 357]. the distinction is essentially\none between the statistical concepts of loss and risk. using ise(h) is conceptually\nappealing because it assesses the estimator\u2019s performance with the observed data.\nhowever, focusing on mise(h) is an effective way to approximate ise-based eval-\nuation while reflecting the sensible goal of seeking optimal performance on average\nover many data sets. we will encounter both measures in the following sections.\nalthough we limit attention to performance criteria based on squared error for\nthe sake of simplicity and familiarity, squared error is not the only reasonable option.\nfor example, there are several potentially appealing reasons to replace integrated\nsquared error with the l1 norm$ | \u02c6f(x) \u2212 f(x)| dx, and mise(h) with the corre-\nsponding expectation. notably among these, the l1 norm is unchanged under any\nmonotone continuous change of scale. this dimensionless character of l1 makes it a\nsort of universal measure of how near \u02c6f is to f. devroye and gy\u00a8orfi study the theory\nof density estimation using l1 and present other advantages of this approach [159,\n160]. in principle, the optimality of an estimator depends on the metric by which per-\nformance is assessed, so the adoption of different metrics may favor different types\nof estimators. in practice, however, many other factors generally affect the quality of\na density estimator more than the metric that might have been used to motivate it.\n\n10.2 kernel density estimation\nthedensityestimatorgiveninequation(10.1)weightsallpointswithin hof xequally.\na univariate kernel density estimator allows a more flexible weighting scheme, fitting\n\n\u02c6f(x) =\n\n1\nnh\n\nk. x \u2212 xi\nh / ,\n\nn#i=1\n\n(10.6)\n\nwhere k is a kernel function and h is a fixed number usually termed the bandwidth.\n\n "}, {"Page_number": 332, "text": "328\n\nchapter 10 nonparametric density estimation\n\ny\nt\ni\ns\nn\ne\nd\n\nx1\n\nx2 x3\n\nx4\n\nfigure 10.1 normal kernel density estimate (solid) and kernel contributions (dotted) for\nthesample x1, . . . , x4.thekerneldensityestimateatany xisthesumofthekernelcontributions\ncentered at each xi.\n\na kernel function assigns weights to the contributions given by each xi to the\nkernel density estimator \u02c6f(x), depending on the proximity of xi to x. typically, kernel\nfunctions are positive everywhere and symmetric about zero. k is usually a density,\nsuch as a normal or student\u2019s t density. other popular choices include the triweight\nand epanechnikov kernels (see section 10.2.2), which don\u2019t correspond to familiar\n1\ndensities. note that the univariate uniform kernel, namely k(z) =\n21{|z|<1}, yields\nthe estimator given by (10.1). constraining k so that$ z2k(z) dz = 1 allows h to\nplay the role of the scale parameter of the density k, but is not required.\nfigure 10.1 illustrates how a kernel density estimate is constructed from a\nsample of four univariate observations, x1, . . . , x4. centered at each observed data\npoint is a scaled kernel: in this case, a normal density function divided by 4. these\ncontributions are shown with the dotted lines. summing the contributions yields the\nestimate \u02c6f shown with the solid line.\nthe estimator in (10.6) is more precisely termed a fixed-bandwidth kernel den-\nsity estimator because h is constant. the value chosen for the bandwidth exerts a\nstrong influence on the estimator \u02c6f. if h is too small, the density estimator will tend\nto assign probability density too locally near observed data, resulting in a very wiggly\nestimated density function with many false modes. when h is too large, the density\nestimator will spread probability density contributions too diffusely. averaging over\nneighborhoods that are too large smooths away important features of f.\nnotice that computing a kernel density estimate at every observed sample point\nbased on a sample of size n requires n(n \u2212 1) evaluations of k. thus, the compu-\ntational burden of calculating \u02c6f grows quickly with n. however, for most practical\npurposes such as graphing the density, the estimate need not be computed at each\nxi. a practical strategy is to calculate \u02c6f(x) over a grid of values for x, then linearly\ninterpolate between grid points. a grid of a few hundred values is usually sufficient\nto provide a graph of \u02c6f that appears smooth. an even faster, approximate method of\n\n "}, {"Page_number": 333, "text": "10.2 kernel density estimation\n\n329\n\ny\nt\ni\ns\nn\ne\nd\n\n0.2\n\n0.1\n\n0\n\n0\n\n5\n\n10\n\n15\n\nfigure 10.2 histogram of 100 data points drawn from the bimodal distribution in exam-\nple 10.1, and three normal kernel density estimates. the estimates correspond to bandwidths\nh = 1.875 (dashed line), h = 0.625 (heavy), and h = 0.3 (solid).\n\nx\n\ncalculating the kernel density estimate relies on binning the data and rounding each\nvalue to the nearest bin center [315]. then, the kernel need only be evaluated at each\nnonempty bin center, with density contributions weighted by bin counts. drastic re-\nductions in computing time can thereby be obtained in situations where n is so large\nas to prevent calculating individual contributions to \u02c6f centered at every xi.\n\n10.2.1 choice of bandwidth\nthe bandwidth parameter controls the smoothness of the density estimate. recall\nfrom (10.4) and (10.5) that mise(h) equals the integrated mean squared error. this\nemphasizes that the bandwidth determines the trade-off between the bias and variance\n\u02c6f. such a trade-off is a pervasive theme in nearly all kinds of model selection,\nof\nincluding regression, density estimation, and smoothing (see chapters 11 and 12).\na small bandwidth produces a density estimator with wiggles indicative of high\nvariability caused by undersmoothing. a large bandwidth causes important features\nof f to be smoothed away, thereby causing bias.\nexample 10.1 (bimodal density) the effect of bandwidth is shown in figure 10.2.\nthis histogram shows a sample of 100 points from an equally weighted mixture of\nn(4,12) and n(9,22) densities. three density estimates that use a standard normal\nkernel are superimposed, with h = 1.875 (dashed), h = 0.625 (heavy), and h = 0.3\n(solid). the bandwidth h = 1.875 is clearly too large because it leads to an over-\nsmooth density estimate that fails to reveal the bimodality of f. on the other hand,\nh = 0.3 is too small a bandwidth, leading to undersmoothing. the density estimate is\ntoo wiggly, exhibiting many false modes. the bandwidth h = 0.625 is adequate,\n\n "}, {"Page_number": 334, "text": "chapter 10 nonparametric density estimation\n\n330\ncorrectly representing the main features of f while suppressing most effects of\nsampling variability.\n!\nin the following subsections we discuss a variety of ways to choose h. when\ndensity estimation is used mainly for exploratory data analysis, span choice based on\nvisual inspection is defensible, and the trial-and-error process leading to your choice\nmight itself provide useful insight into the stability of features observed in the density\nestimate. in practice, one may simply try a sequence of values for h, and choose a\nvalue that is large enough to avoid the threshold where smaller bandwidths cause\nfeatures of the density estimate to become unstable or the density estimate exhibits\nobvious wiggles so localized that they are unlikely to represent modes of f. although\nthe density estimate is sensitive to the choice of bandwidth, we stress that there is no\nsingle correct choice in any application. indeed, bandwidths within 10\u201320% of each\nother will often produce qualitatively similar results.\nthere are situations when a more formal bandwidth selection procedure might\nbe desired: for use in automatic algorithms, for use by a novice data analyst, or for\nuse when a greater degree of objectivity or formalism is desired. a comprehensive\nreview of approaches is given in [360]; other good reviews include [32, 88, 359, 500,\n581, 592, 598].\nto understand bandwidth selection, it is necessary to further analyze mise(h).\nsuppose that k is a symmetric, continuous probability density function with mean\nzero and variance 0 < \u03c32\nk < \u221e. let r(g) denote a measure of the roughness of a\ngiven function g, defined by\n\nr(g) =% g2(z) dz.\n\n(10.7)\nassume hereafter that r(k) < \u221e and that f is sufficiently smooth. in this section,\nthis means that f must have two bounded continuous derivatives and r(f\u2032\u2032) < \u221e;\nhigher-order smooth derivatives are required for some methods discussed later. recall\nthat\nmise(h) =% mseh( \u02c6f(x)) dx =% var, \u02c6f(x)- +&bias, \u02c6f(x)-\u20192\nwe further analyze this expression, allowing h \u2192 0 and nh \u2192 \u221e as n \u2192 \u221e.\n\nto compute the bias term in (10.8), note that\n\n(10.8)\n\ndx.\n\ne{ \u02c6f(x)} =\n\n1\n\nh / f(u) du\n\nh% k. x \u2212 u\n=% k(t)f(x \u2212 ht) dt\n\n(10.9)\n\nby applying a change of variable. next, substituting the taylor series expansion\n\nf(x \u2212 ht) = f(x) \u2212 htf\u2032(x) +\n\n1\n2 h2t2f\u2032\u2032(x) + o(h2)\n\n(10.10)\n\n "}, {"Page_number": 335, "text": "kr(f\u2032\u2032) + o(h4).\n\n(10.13)\n\nin (10.9) and noting that k is symmetric about zero leads to\n\n10.2 kernel density estimation\n\n331\n\n1\n2 h2\u03c32\n\ne{ \u02c6f(x)} = f(x) +\n\n(10.11)\nwhere o(h2) is a quantity that converges to zero faster than h2 does as h \u2192 0. thus,\n(10.12)\n\nkf\u2032\u2032(x) + o(h2),\n\nk[f\u2032\u2032(x)]2\nand integrating the square of this quantity over x gives\n\n+ o(h4),\n\n1\n4 h4\u03c34\n\n=\n\nto compute the variance term in (10.8), a similar strategy is employed:\nvar{ \u02c6f(x)} =\n=\n\n1\nn\n1\n\n1\n\nh\n\nh /+12\n\nk. x \u2212 xi\nn2f(x) + o(1)32\n\n1\n\nh\n\n1\n\ndx =\n\n1\n4 h4\u03c34\n\n&bias, \u02c6f(x)-\u20192\n% &bias, \u02c6f(x)-\u20192\nvar(1\nk. x \u2212 xi\nh /+\nn0e(1\nnh% k(t)2f(x \u2212 ht) dt \u2212\nnh% k(t)22f(x) + o(1)3 dt \u2212\nf(x)r(k) + o. 1\nnh/ .\nnh + o. 1\nnh/ .\n% var{ \u02c6f(x)} dx =\nnh + h4/ ,\nmise(h) = amise(h) + o. 1\nkr(f\u2032\u2032)\n4\n\nr(k)\nnh +\n\nr(k)\n\n1\nnh\n\nh4\u03c34\n\n=\n=\n\namise(h) =\n\n(10.17)\nis termed the asymptotic mean integrated squared error. if nh \u2192 \u221e and h \u2192 0\nas n \u2192 \u221e, then mise(h) \u2192 0, confirming our intuition from the uniform kernel\nestimator discussed in the chapter introduction. the error term in (10.16) can be\nshown to equal o(n\u22121 + h5) with a more delicate analysis of the squared bias as in\n[580], but it is the amise that interests us most.\ntominimizeamise(h)withrespectto h,wemustset hatanintermediatevalue\nthat avoids excessive bias and excessive variability in \u02c6f. minimizing amise(h) with\n\n(10.14)\n\n(10.15)\n\n(10.16)\n\nthus,\n\nwhere\n\nintegrating this quantity over x gives\n\n "}, {"Page_number": 336, "text": "chapter 10 nonparametric density estimation\n\n332\nrespect to h shows that exactly balancing the orders of the bias and variance terms in\n(10.17) is best. the optimal bandwidth is\n\nh =. r(k)\n\nkr(f\u2032\u2032)/1/5\n\nn\u03c34\n\n,\n\n(10.18)\n\nbut this result is not immediately helpful since it depends on the unknown density f.\nnote that optimal bandwidths have h = o(n\u22121/5), in which case mise =\no(n\u22124/5). this result reveals how quickly the bandwidth should shrink with increas-\ning sample size, but it says little about what bandwidth would be appropriate for\ndensity estimation with a given dataset. a variety of automated bandwidth selection\nstrategies have been proposed; see the following subsections. their relative perfor-\nmance in real applications varies with the nature of f and the observed data. there\nis no universally best way to proceed.\nmany bandwidth selection methods rely on optimizing or finding the root of\na function of h\u2014for example, minimizing an approximation to amise(h). in these\ncases, a search may be conducted over a logarithmic-spaced grid of 50 or more\nvalues, linearly interpolating between grid points. when there are multiple roots or\nlocal minima, a grid search permits a better understanding of the bandwidth selection\nproblem than would an automated optimization or root-finding algorithm.\n10.2.1.1 cross-validation many bandwidth selection strategies begin by relat-\ning htosomemeasureofthequalityof \u02c6f asanestimatorof f.thequalityisquantified\nby some q(h), whose estimate, \u02c6q(h), is optimized to find h.\nif \u02c6q(h) evaluates the quality of \u02c6f based on how well it fits the observed data in\nsome sense, then the observed data are being used twice: once to calculate \u02c6f from the\ndata and a second time to evaluate the quality of \u02c6f as an estimator of f. such double\nuse of the data provides an overoptimistic view of the quality of the estimator. when\nmisled in this way, the chosen estimator tends to be overfitted (i.e., undersmoothed),\nwith too many wiggles or false modes.\ncross-validation provides a remedy to this problem. to evaluate the quality of\n\u02c6f at the ith data point, the model is fitted using all the data except the ith point. let\n\n\u02c6f\u2212i(xi) =\n\n1\n\nh(n \u2212 1)#j /= i\n\nk. xi \u2212 xj\n\nh\n\n/\n\n(10.19)\n\ndenote the estimated density at xi using a kernel density estimator with all the obser-\nvations except xi. choosing \u02c6q to be a function of the \u02c6f\u2212i(xi) separates the tasks of\nfitting and evaluating \u02c6f to select h.\nalthough cross-validation enjoys great success as a span selection strategy for\nscatterplot smoothing (see chapter 11), it is not always effective for bandwidth se-\nlection in density estimation. the h estimated by cross-validation approaches can\nbe highly sensitive to sampling variability. despite the persistence of these meth-\nods in general practice and in some software, a sophisticated plug-in method like\nthe sheather\u2013jones approach (section 10.2.1.2) is a much more reliable choice.\n\n "}, {"Page_number": 337, "text": "333\nnevertheless, cross-validation methods introduce some ideas that are useful in a\nvariety of contexts.\none easy cross-validation option is to let \u02c6q(h) be the pseudo-likelihood (pl)\n\n10.2 kernel density estimation\n\npl(h) =\n\nn4i=1\n\n\u02c6f\u2212i(xi),\n\n(10.20)\n\nas proposed in [171, 289]. the bandwidth is chosen to maximize the pseudo-\nlikelihood. although simple and intuitively appealing, this approach frequently pro-\nduces kernel density estimates that are too wiggly and too sensitive to outliers [582].\nthe theoretical limiting performance of kernel density estimators with span chosen\nto minimize pl(h) is also poor: in many cases the estimator is not consistent [578].\nanother approach is motivated by reexpressing the integrated squared error as\n\nise(h) =% \u02c6f 2(x) dx \u2212 2e{ \u02c6f(x)} +% f(x)2 dx\n\n(10.21)\nthe final term in this expression is constant, and the middle term can be estimated by\n\n= r( \u02c6f) \u2212 2e{ \u02c6f(x)} + r(f).\n\ni=1 \u02c6f\u2212i(xi). thus, minimizing\n\n(2/n)\"n\n\nucv(h) = r( \u02c6f) \u2212\n\n\u02c6f\u2212i(xi)\n\n(10.22)\n\n2\nn\n\nn#i=1\n\nwith respect to h should provide a good bandwidth [56, 561]. ucv(h) is called\ntheunbiasedcross-validationcriterionbecause e{ucv(h) + r(f)} = mise(h).the\napproach is also called least squares cross-validation because choosing h to minimize\nucv(h) minimizes the integrated squared error between \u02c6f and f.\nif analytic evaluation of r( \u02c6f) is not possible, the best way to evaluate (10.22) is\nprobably to use a different kernel that permits an analytic simplification. for a normal\nkernel \u03c6 it can be shown that\nucv(h) =\n\nr(\u03c6)\nnh\n\n1\n\n+\n\nn(n \u2212 1)h\n\nn#i=1#j /= i0\n\n1\n\n(8\u03c0)1/4 \u03c61/2. xi \u2212 xj\n\nh\n\n/ \u2212 2\u03c6. xi \u2212 xj\n\nh\n\n/1 , (10.23)\n\nfollowing the steps outlined in problem 10.3. this expression can be computed effi-\nciently without numerical approximation.\nalthough the bandwidth identified by minimizing ucv(h) with respect to h is\nasymptotically as good as the best possible bandwidth [293, 614], its convergence\nto the optimum is extremely slow [298, 583]. in practical settings, using unbiased\ncross-validation is risky because the resulting bandwidth tends to exhibit a strong\ndependence on the observed data. in other words, when applied to different datasets\ndrawn from the same distribution, unbiased cross-validation can yield very different\nanswers. its performance is erratic in application, and undersmoothing is frequently\nseen.\n\n "}, {"Page_number": 338, "text": "334\n\nchapter 10 nonparametric density estimation\n\ny\nc\nn\ne\nu\nq\ne\nr\nf\n \ne\nv\ni\nt\na\nl\ne\nr\n\n0.015\n\n0.010\n\n0.005\n\n0\n\n1000\n\n1200\n\n1400\n\nhours since midnight, april 5\n\nfigure 10.3 histogram of the times of 121 bowhead whale calf sightings during the 2001\nspring migration discussed in example 10.2. the date of each sighting is expressed as the\nnumber of hours since midnight, april 5, when the first adult whale was sighted.\n\nthehighsamplingvariabilityofunbiasedcross-validationismainlyattributable\nto the fact that the target performance criterion, q(h) = ise(h), is itself random,\nunlike mise(h). scott and terrell have proposed a biased cross-validation criterion\n[bcv(h)] that seeks to minimize an estimate of amise(h) [583]. in practice, this\napproach is generally outperformed by the best plug-in methods (section 10.2.1.2)\nand can yield excessively large bandwidths and oversmooth density estimates.\nexample 10.2 (whale migration)\nfigure 10.3 shows a histogram of times of\n121 bowhead whale calf sightings during the spring 2001 visual census conducted\nat the ice edge near point barrow, alaska. this census is the central component of\ninternational efforts to manage this endangered whale population while allowing a\nsmall traditional subsistence hunt by coastal inupiat communities [156, 249, 528].\nthe timing of the northeasterly spring migration is surprisingly regular, and\nit is important to characterize the migration pattern for planning of future scientific\nefforts to study these animals. there is speculation that the migration may occur in\nloosely defined pulses. if so, this is important to discover because it may lead to new\ninsights about bowhead whale biology and stock structure.\nfigure 10.4 shows the results of kernel density estimates for these data using\nthe normal kernel. three different cross-validation criteria were used to select h.\nmaximizing cross-validated pl(h) with respect to h yields h = 9.75 and the density\nestimate shown with the dashed curve. this density estimate is barely adequate,\nexhibiting likely false modes in several regions. the result from minimizing ucv(h)\nwith respect to h is even worse in this application, giving h = 5.08 and the density\nestimate shown with the dotted curve. this bandwidth is clearly too small. finally,\nminimizing bcv(h) with respect to h yields h = 26.52 and the density estimate\nshown with the solid line. clearly the best of the three options, this density estimate\n\n "}, {"Page_number": 339, "text": "10.2 kernel density estimation\n\n335\n\ny\nc\nn\ne\nu\nq\ne\nr\nf\n \ne\nv\ni\nt\na\nl\ne\nr\n\n0.015\n\n0.010\n\n0.005\n\n0\n\n1000\n\n1200\n\n1400\n\nhours since midnight, april 5\n\nfigure 10.4 kernel density estimates for the whale calf migration data in example 10.2 us-\ningnormalkernelsandbandwidthschosenbythreedifferentcross-validationcriteria.theband-\nwidths are 9.75 using pl(h) (dashed), 5.08 using ucv(h) (dotted), and 26.52 using bcv(h)\n(solid).\nemphasizes only the most prominent features of the data distribution but may seem\noversmooth. perhaps a bandwidth between 10 and 26 would be preferable.\n!\n10.2.1.2 plug-in methods plug-in methods apply a pilot bandwidth to estimate\none or more important features of f. the bandwidth for estimating f itself is then\nestimated at a second stage using a criterion that depends on the estimated features.\nthe best plug-in methods have proven to be very effective in diverse applications and\naremorepopularthancross-validationapproaches.however,loaderoffersarguments\nagainst the uncritical rejection of cross-validation approaches [422].\nfor unidimensional kernel density estimation, recall that the bandwidth that\nminimizes amise is given by\n\nh =. r(k)\n\nkr(f\u2032\u2032)/1/5\n\nn\u03c34\n\n,\n\n(10.24)\n\nwhere \u03c32\nk is the variance of k, viewing k as a density. at first glance, (10.24) seems\nunhelpful because the optimal bandwidth depends on the unknown density f through\nthe roughness of its second derivative. a variety of methods have been proposed to\nestimate r(f\u2032\u2032).\nsilverman suggests an elementary approach: replacing f by a normal density\nwith variance set to match the sample variance [598]. this amounts to estimating\nr(f\u2032\u2032) by r(\u03c6\u2032\u2032)/\u02c6\u03c35, where \u03c6 is the standard normal density function. silverman\u2019s\nrule of thumb therefore gives\n\nh =. 4\n\n3n/1/5\n\n\u02c6\u03c3.\n\n(10.25)\n\n "}, {"Page_number": 340, "text": "chapter 10 nonparametric density estimation\n\n336\nif f is multimodal, the ratio of r(f\u2032\u2032) to \u02c6\u03c3 may be larger than it would be for nor-\nmally distributed data. this would result in oversmoothing. a better bandwidth can\nbe obtained by considering the interquartile range (iqr), which is a more robust\nmeasure of spread than is \u02c6\u03c3. thus, silverman suggests replacing \u02c6\u03c3 in (10.25) by\n\u02dc\u03c3 = min{\u02c6\u03c3,iqr/(\u0001\u22121(0.75) \u2212 \u0001\u22121(0.25))} \u2248 min{\u02c6\u03c3,iqr/1.35}, where \u0001 is the\nstandard normal cumulative distribution function. although simple, this approach\ncannot be recommended for general use because it has a strong tendency to over-\nsmooth. silverman\u2019s rule of thumb is valuable, however, as a method for producing\napproximate bandwidths effective for pilot estimation of quantities used in sophisti-\ncated plug-in methods.\nempirical estimation of r(f\u2032\u2032) in (10.24) is a better option than silverman\u2019s\nrule of thumb. the kernel-based estimator is\n\n\u02c6f\u2032\u2032(x) =\n\n=\n\nnh0\n\nd2\n\ndx25 1\nn#i=1\n\n1\nnh3\n0\n\nh0 /6\nl. x \u2212 xi\nn#i=1\nl\u2032\u2032. x \u2212 xi\nh0 / ,\n\n(10.26)\n\nwhere h0 is the bandwidth and l is a sufficiently differentiable kernel used to estimate\nf\u2032\u2032. estimation of r(f\u2032\u2032) follows from (10.26).\nit is important to recognize, however, that the best bandwidth for estimating f\nwilldifferfromthebestbandwidthforestimating f\u2032\u2032 or r(f\u2032\u2032).thisisbecausevar{ \u02c6f\u2032\u2032}\ncontributes a proportionally greater share to the mean squared error for estimating\nf\u2032\u2032 than var{ \u02c6f} does for estimating f. therefore, a larger bandwidth is required for\nestimating f\u2032\u2032. we therefore anticipate h0 > h.\nsuppose we use bandwidth h0 with kernel l to estimate r(f\u2032\u2032), and bandwidth\nh with kernel k to estimate f. then the asymptotic mean squared error for estimation\nof r(f\u2032\u2032) using kernel l is minimized when h0 \u221d n\u22121/7. to determine how h0 should\nbe related to h, recall that optimal bandwidths for estimating f have h \u221d n\u22121/5.\nsolving this expression for n and replacing n in the equation h0 \u221d n\u22121/7, one can\nshow that\n\nh0 = c1(r(f\u2032\u2032), r(f\u2032\u2032\u2032))c2(l)h5/7,\n\n(10.27)\n\nwhere c1 and c2 are functionals that depend on derivatives of f and on the kernel l,\nrespectively. equation (10.27) still depends on the unknown f, but the quality of the\nestimate of r(f\u2032\u2032) produced using h0 and l is not excessively deteriorated if h0 is set\nusing relatively simple estimates to find c1 and c2. in fact, we may estimate c1 and\nc2 using a bandwidth chosen by silverman\u2019s rule of thumb.\nthe result is a two-stage process for finding the bandwidth, known as the\nsheather\u2013jones method [359, 593]. at the first stage, a simple rule of thumb is used\nto calculate the bandwidth h0. this bandwidth is used to estimate r(f\u2032\u2032), which is the\nonly unknown in expression (10.24) for the optimal bandwidth. then the bandwidth\nh is computed via (10.24) and is used to produce the final kernel density estimate.\n\n "}, {"Page_number": 341, "text": "337\nfor univariate kernel density estimation with pilot kernel l = \u03c6, the sheather\u2013\n\njones bandwidth is the value of h that solves the equation\n\n10.2 kernel density estimation\n\n.\n\nr(k)\n\n\u02c6r \u02c6\u03b1(h)(f\u2032\u2032)/1/5\n\nn\u03c34\n\nk\n\n\u2212 h = 0,\n\n(10.28)\n\nwhere\n\n\u03c6(4). xi \u2212 xj\n\n\u03b1\n\na\n\n\u03c6(4). xi \u2212 xj\n\u03c6(6). xi \u2212 xj\n\nb\n\n,\n\n1\n\n\u02c6r \u02c6\u03b1(h)(f\u2032\u2032) =\n\nn(n \u2212 1)\u03b15\n\nn#i=1\nn#j=1\n\u02c6rb(f\u2032\u2032\u2032) 81/7\n\u02c6\u03b1(h) =76\u221a2h5 \u02c6ra(f\u2032\u2032)\nn#i=1\nn#j=1\n\u02c6ra(f\u2032\u2032) =\nn#i=1\nn#j=1\n\u02c6rb(f\u2032\u2032\u2032) =\n\n1\n\n1\n\nn(n \u2212 1)a5\nn(n \u2212 1)b7\n0.920(iqr)\n0.912(iqr)\n\nn1/7\n\n,\n\n,\n\na =\nb =\n\nn1/9\n\n/ ,\n\n/ ,\n/ ,\n\nwhere \u03c6(i) is the ith derivative of the normal density function, and iqr is the in-\nterquartile range of the data. the solution to (10.28) can be found using grid search\nor a root-finding technique from chapter 2, such as newton\u2019s method.\nthe sheather\u2013jones method generally performs extremely well [359, 360, 501,\n592]. there are a variety of other good methods based on carefully chosen approxi-\nmations to mise(h) or its minimizer [88, 300, 301, 358, 500]. in each case, careful\npilot estimation of various quantities plays a critical role in ensuring that the final\nbandwidth performs well. some of these approaches give bandwidths that asymptot-\nically converge more quickly to the optimal bandwidth than does the sheather\u2013jones\nmethod; all can be useful options in some circumstances. however, none of these offer\nsubstantially easier practical implementation or broadly better performance than the\nsheather\u2013jones approach.\nexample 10.3 (whale migration, continued) figure 10.5 illustrates the use of\nsilverman\u2019s rule of thumb and the sheather\u2013jones method on the bowhead whale\nmigration data introduced in example 10.2. the bandwidth given by the sheather\u2013\njones approach is 10.22, yielding the density estimate shown with the solid line. this\nbandwidth seems a bit too narrow, yielding a density estimate that is too wiggly.\nsilverman\u2019s rule of thumb gives a bandwidth of 32.96, larger than the bandwidth\ngiven by any previous method. the resulting density estimate is probably too smooth,\nhiding important features of the distribution.\n!\n\n "}, {"Page_number": 342, "text": "338\n\nchapter 10 nonparametric density estimation\n\n0.015\n\ny\nc\nn\ne\nu\nq\ne\nr\nf\n \ne\nv\ni\nt\na\nl\ne\nr\n\n0.010\n\n0.005\n\n0\n\n1000\n\n1200\n\n1400\n\nhours since midnight, april 5\n\nfigure 10.5 kernel density estimates for the whale calf migration data using normal ker-\nnels with bandwidths chosen by three different criteria. the bandwidths are 10.22 using the\nsheather\u2013jones approach (solid), 32.96 using silverman\u2019s rule of thumb (dashed), and 35.60\nusing the maximal smoothing span of terrell (dotted).\n\n10.2.1.3 maximal smoothing principle recallagainthattheminimalamise\nis obtained when\n\nh =. r(k)\n\nkr(f\u2032\u2032)/1/5\n\nn\u03c34\n\n,\n\n(10.29)\n\nbut f is unknown. silverman\u2019s rule of thumb replaces r(f\u2032\u2032) by r(\u03c6\u2032\u2032). the sheather\u2013\njonesmethodestimates r(f\u2032\u2032).terrell\u2019smaximalsmoothingapproachreplaces r(f\u2032\u2032)\nwith the most conservative (i.e., smallest) possible value [627].\nspecifically, terrell considered the collection of all h that would minimize\n(10.29) for various f and recommended that the largest such bandwidth be chosen.\nin other words, the right-hand side of (10.29) should be maximized with respect to\nf. this will bias bandwidth selection against undersmoothing. since r(f\u2032\u2032) vanishes\nas the variance of f shrinks, the maximization is carried out subject to the constraint\nthat the variance of f matches the sample variance \u02c6\u03c32.\nconstrained maximization of (10.29) with respect to f is an exercise in the\ncalculus of variations. the f that maximizes (10.29) is a polynomial. substituting its\nroughness for r(f\u2032\u2032) in (10.29) yields\n\nh = 3. r(k)\n\n35n /1/5\n\n(10.30)\nas the chosen bandwidth. table 10.1 provides the values of r(k) for some common\nkernels.\nterrell proposed the maximal smoothing principle to motivate this choice of\nbandwidth. when interpreting a density estimate, the analyst\u2019s eye is naturally drawn\n\n\u02c6\u03c3\n\n "}, {"Page_number": 343, "text": "10.2 kernel density estimation\n\n339\n\n1\n2\n\n\u03b4(k)\n\nk(z)\n\n(1/(2\u221a\u03c0))1/5\n\n151/5\n241/5\n351/5\n\n& 9\n2\u20191/5\n\ntable 10.1 some kernel choices and related quantities discussed in the text. the kernels are listed\nin increasing order of roughness, r(k). k(z) should be multiplied by 1{|z|<1} in all cases except the\nnormal kernel, which has positive support over the entire real line. re is the asymptotic relative\nefficiency, as described in section 10.2.2.1.\nname\nnormal\nuniform\nepanechnikov\ntriangle\nbiweight\ntriweight\n\nr(k)\n1/(2\u221a\u03c0)\n1\n2\n3\n5\n2\n3\n5\n7\n350\n429\n\nre\n1.051\n1.076\n1.000\n1.014\n1.006\n1.013\n\nexp{\u2212z2/2}/\u221a2\u03c0\n& 3\n4\u2019(1 \u2212 z2)\n1 \u2212 |z|\n& 15\n16\u2019(1 \u2212 z2)2\n& 35\n32\u2019(1 \u2212 z2)3\n\nto modes. further, modes usually have important scientific implications. therefore,\nthe bandwidth should be selected to discourage false modes, producing an estimate\nthat shows modes only where the data indisputably require them.\nthe maximal smoothing approach is appealing because it is quick and simple\nto calculate. in practice, the resulting kernel density estimate is often too smooth. we\nwouldbereluctanttouseamaximalsmoothingbandwidthwhenthedensityestimateis\nbeingusedforinference.forexploratoryanalyses,themaximalsmoothingbandwidth\ncan be quite helpful, allowing the analyst to focus on the dominant features of the\ndensity without being misled by highly variable indications of possibly false modes.\nexample 10.4 (whale migration, continued)\nthe dotted line in figure 10.5\nshows the density estimate obtained using the maximal smoothing bandwidth of\n35.60. even larger than silverman\u2019s bandwidth, this choice appears too large for\nthe whale data. generally, both silverman\u2019s rule of thumb and terrell\u2019s maximal\nsmoothing principle tend to produce oversmoothed density estimates.\n!\n\n& 9450\n143\u20191/5\n\n10.2.2 choice of kernel\nkernel density estimation requires specification of two components: the kernel and\nthe bandwidth. it turns out that the shape of the kernel has much less influence on the\nresults than does the bandwidth. table 10.1 lists a few choices for kernel functions.\nepanechnikov kernel suppose k is limited to bounded, symmetric\n10.2.2.1\ndensitieswithfinitemomentsandvarianceequalto1.thenepanechnikovshowedthat\nminimizing amise with respect to k amounts to minimizing r(k) with respect to\nk subject to these constraints [186]. the solution to this variational calculus problem\n\u221a5 k\u2217(z/\u221a5), where k\u2217 is the epanechnikov kernel\nis the kernel assigning density 1\n(10.31)\nthis is a symmetric quadratic function, centered at zero, where its mode is reached,\nand decreasing to zero at the limits of its support.\n\nk\u2217(z) =5 3\n\nif |z| < 1,\notherwise.\n\n4(1 \u2212 z2)\n0\n\n "}, {"Page_number": 344, "text": "340\n\nchapter 10 nonparametric density estimation\nfrom (10.17) and (10.18) we see that the minimal amise for a kernel density\nestimator with positive kernel k is 5\n4[\u03c3kr(k)/n]4/5r(f\u2032\u2032)1/5. switching to a k\nthat doubles \u03c3kr(k) therefore requires doubling n to maintain the same minimal\namise. thus, \u03c3k2 r(k2)/(\u03c3k1 r(k1)) measures the asymptotic relative efficiency of\nk2 compared to k1. the relative efficiencies of a variety of kernels compared to the\nepanechnikov kernel are given in table 10.1. notice that the relative efficiencies are\nall quite close to 1, reinforcing the point that kernel choice is fairly unimportant.\n10.2.2.2 canonical kernels and rescalings unfortunately, a particular value\nof h corresponds to a different amount of smoothing depending on which kernel is\nbeing used. for example, h = 1 corresponds to a kernel standard deviation nine times\nlarger for the normal kernel than for the triweight kernel.\nlet hk and hl denote the bandwidths that minimize amise(h) when using\nsymmetric kernel densities k and l, respectively, which have mean zero and finite\npositive variance. then from (10.29) it is clear that\n\nhk\nhl =\n\n\u03b4(k)\n\u03b4(l) ,\n\n(10.32)\n\nwhereforanykernelwehave \u03b4(k) =&r(k)/\u03c34\n\nk\u20191/5.thus,tochangefrombandwidth\nh for kernel k to a bandwidth that gives an equivalent amount of smoothing for kernel\nl, use the bandwidth h\u03b4(l)/\u03b4(k). table 10.1 lists values for \u03b4(k) for some common\nkernels.\nsuppose further that we rescale each kernel shape in table 10.1 so that h =\n1 corresponds to a bandwidth of \u03b4(k). the kernel density estimator can then be\nwritten as\n\nwhere\n\n\u02c6f x(x) =\n\n1\nn\n\nkh\u03b4(k)(z) =\n\nkh\u03b4(k)(x \u2212 xi),\n\nn#i=1\nh\u03b4(k) k. z\n\n1\n\nh\u03b4(k)/ ,\n\nand k represents one of the original kernel shapes and scalings shown in table 10.1.\nscaling kernels in this way provides a canonical kernel k\u03b4(k) of each shape [440]. a\nkey benefit of this viewpoint is that a single value of h can be used interchangeably\nfor each canonical kernel without affecting the amount of smoothing in the density\nestimate.\n\nnote that\n\namise(h) = c(k\u03b4(k)). 1\n\nnh +\n\nh4r(f\u2032\u2032)\n\n4\n\n/\n\n(10.33)\n\nfor an estimator using a canonical kernel with bandwidth h [i.e., a kernel from\ntable 10.1 with bandwidth h\u03b4(k)] and with c(k\u03b4(k)) = (\u03c3kr(k))4/5. this means\nthat the balance between variance and squared bias determined by the factor\n\n "}, {"Page_number": 345, "text": "10.3 nonkernel methods\n\n341\n\nuniform\n\nepanechnikov\n\ntriangle\n\nnormal\n\nbiweight\n\ntriweight\n\nfigure 10.6 kernel density estimates for the data in example 10.1 using the canonical\nversion of each of the six kernels from table 10.1, with h = 0.69 (dotted).\n(nh)\u22121 + h4r(f\u2032\u2032)/4 is no longer confounded with the chosen kernel. it also means\nthat the contributions made by the kernel to the variance and squared bias terms in\namise(h) are equal. it follows that the optimal kernel shape does not depend on\nthe bandwidth: the epanechnikov kernel shape is optimal for any desired degree of\nsmoothing [440].\nexample 10.5 (bimodal density, continued) figure 10.6 shows kernel density\nestimates for the data from example 10.1, which originated from the equally weighted\nmixture of n(4,12) and n(9,22) densities. all the bandwidths were set at 0.69 for\nthe canonical kernels of each shape, that being the sheather\u2013jones bandwidth for the\nnormal kernel. the uniform kernel produces a noticeably rougher result due to its\ndiscontinuity. the epanechnikov and uniform kernels provide a slight (false) sug-\ngestion that the lower mode contains two small local modes. aside from these small\ndifferences, the results for all the kernels are qualitatively the same. this example\nillustrates that even quite different kernels can be scaled to produce such similar\nresults that the choice of kernel is unimportant.\n!\n\n10.3 nonkernel methods\n\nlogspline\n\n10.3.1\na cubic spline is a piecewise cubic function that is everywhere twice continuously\ndifferentiable but whose third derivative may be discontinuous at a finite number of\nprespecified knots. one may view a cubic spline as a function created from cubic\n\n "}, {"Page_number": 346, "text": "chapter 10 nonparametric density estimation\n\n342\npolynomials on each between-knot interval by pasting them together twice continu-\nously differentiably at the knots. kooperberg and stone\u2019s logspline density estimation\napproach estimates the log of f by a cubic spline of a certain form [389, 615].\nthismethodprovidesunivariatedensityestimationonaninterval(l, u),where\neach endpoint may be infinite. suppose there are m \u2265 3 knots, tj for j = 1, . . . , m,\nwith l < t1 < t2 < \u00b7\u00b7\u00b7 < tm < u. knot selection will be discussed later.\nlet s denote the m-dimensional space consisting of cubic splines with knots at\nt1, . . . , tm and that are linear on (l, t1] and [tm , u). let a basis for s be denoted by\nthe functions {1, b1, . . . , bm\u22121}. there are numerical advantages to certain types\nof bases; books on splines and the other references in this section provide additional\ndetail [144, 577]. it is possible to choose the basis functions so that on (l, t1] the\nfunction b1 is linear with a negative slope and all other bi are constant, and so that on\n[tm , u) the function bm\u22121 is linear with a positive slope and all other bi are constant.\n(10.34)\n\nnow consider modeling f with a parameterized density, fx|\u03b8, defined by\n\nlog fx|\u03b8(x|\u03b8) = \u03b81b1(x) + \u00b7\u00b7\u00b7 + \u03b8m\u22121bm\u22121(x) \u2212 c(\u03b8),\nexp{c(\u03b8)} =% u\n\nexp{\u03b81b1(x) + \u00b7\u00b7\u00b7 + \u03b8m\u22121bm\u22121(x)} dx\n\n(10.35)\nand \u03b8 = (\u03b81, . . . , \u03b8m\u22121). for this to be a reasonable model for a density, we\nrequire c(\u03b8) to be finite, which is ensured if (i) l > \u2212\u221e or \u03b81 < 0 and (ii) u < \u221e\nor \u03b8m\u22121 < 0. under this model, the log likelihood of \u03b8 is\n\nwhere\n\nl\n\nl(\u03b8|x1, . . . , xn) =\n\nlog fx|\u03b8(xi|\u03b8),\n\n(10.36)\n\nn#i=1\n\ngiven observed data values x1, . . . , xn. as long as the knots are positioned so that each\ninterval contains sufficiently many observations for estimation, maximizing (10.36)\nsubject to the constraint that c(\u03b8) is finite provides the maximum likelihood estimate,\n\u02c6\u03b8. this estimate is unique because l(\u03b8|x1, . . . , xn) is concave. having estimated the\nmodel parameters, take\n(10.37)\n\n\u02c6f(x) = fx|\u03b8(x|\u02c6\u03b8)\n\nas the maximum likelihood logspline density estimate of f(x).\nthe maximum likelihood estimation of \u03b8 is conditional on the number of knots\nand their placement. kooperberg and stone suggest an automated strategy for place-\nment of a prespecified number of knots [390]. their strategy places knots at the small-\nest and largest observed data point and at other positions symmetrically distributed\nabout the median but not equally spaced.\nto place a prespecified number of knots, let x(i) denote the ith order statis-\ntic of the data, for i = 1, . . . , n, so x(1) is the minimum observed value. define an\napproximate quantile function\n\nq. i \u2212 1\nn \u2212 1/ = x(i)\n\n "}, {"Page_number": 347, "text": "10.3 nonkernel methods\n\n343\nfor 1 \u2264 i \u2264 n, where the value of q is obtained by linear interpolation for\nnoninteger i.\nthe m knots will be placed at x(1), at x(n), and at the positions of the order\nstatistics indexed by q(r2), . . . , q(rm\u22121) for a sequence of numbers 0 < r2 < r3 <\n\u00b7\u00b7\u00b7 < rm\u22121 < 1.\nwhen (l, u) = (\u2212\u221e,\u221e), placement of the interior knots is governed by the\nfollowing constraint on between-knot distances:\nn(ri+1 \u2212 ri) = 4 \u00b7 max{4 \u2212 \u03f5,1} \u00b7 max{4 \u2212 2\u03f5,1}\u00b7\u00b7\u00b7max{4 \u2212 (i \u2212 1)\u03f5,1}\n\n1\nfor 1 \u2264 i \u2264 m/2, where r1 = 0 and \u03f5 is chosen to satisfy r(m+1)/2 =\n2 if m is odd, or\nrm/2 + rm/2+1 = 1 if m is even. the remaining knots are placed to maintain quantile\nsymmetry, so that\n(10.38)\n\nrm+1\u2212i \u2212 rm\u2212i = ri+1 \u2212 ri\n\nfor m/2 \u2264 i < m \u2212 1, where rm = 1.\nwhen (l, u) is not a doubly infinite interval, similar knot placement rules have\nbeen suggested. in particular, if (l, u) is a interval of finite length, then r2, . . . , rm\u22121\nare chosen equally spaced, so ri = (i \u2212 1)/(m \u2212 1).\nthe preceding paragraphs assumed that m, the number of knots, was prespec-\nified. a variety of methods for choosing m are possible, but methods for choosing\nthe number of knots have evolved to the point where a complete description of the\nrecommended strategy is beyond the scope of our discussion. roughly, the process is\nas follows. begin by placing a small number of knots in the positions given above. the\nsuggested minimum number is the first integer exceeding min{2.5n1/5, n/4, n\u2217,25},\nwhere n\u2217 is the number of distinct data points. additional knots are then added to\nthe existing set, one at time. at each iteration, a single knot is added in a position\nthat gives the largest value of the rao statistic for testing that the model without that\nknot suffices [391, 615]. without examining significance levels, this process contin-\nues until the total number of knots reaches min{4n1/5, n/4, n\u2217,30}, or until no new\ncandidate knots can be placed due to constraints on the positions or nearness of knots.\nnext, single knots are sequentially deleted. the deletion of a single knot cor-\nresponds to the removal of one basis function. let \u02c6\u03b8 = (\u02c6\u03b81, . . . , \u02c6\u03b8m\u22121) denote the\nmaximum likelihood estimate of the parameters of the current model. then the wald\nstatistic for testing the significance of the contribution of the ith basis function is\n\u02c6\u03b8i/se{\u02c6\u03b8i}, where se{\u02c6\u03b8i} is the square root of the ith diagonal entry of \u2212l\u2032\u2032(\u02c6\u03b8)\u22121, the\ninverted observed information matrix [391, 615]. the knot whose removal would\nyield the smallest wald statistic value is deleted. sequential deletion is continued\nuntil only about three knots remain.\nsequential knot addition followed by sequential knot deletion generates a se-\nquence of s models, with varying numbers of knots. denote the number of knots in\nthe sth model by ms, for s = 1, . . . , s. to choose the best model in the sequence, let\n(10.39)\nmeasure the quality of the sth model having corresponding mle parameter\n\u02c6\u03b8s. the quantity bic(s) is a bayes information criterion for model\nvector\n\nbic(s) = \u22122l(\u02c6\u03b8s|x1, . . . , xn) + (ms \u2212 1)log n\n\n "}, {"Page_number": 348, "text": "344\n\nchapter 10 nonparametric density estimation\n\ny\nc\nn\ne\nu\nq\ne\nr\nf\n \ne\nv\ni\nt\na\nl\ne\nr\n\n0.015\n\n0.010\n\n0.005\n\n0\n\n1000\n\n1200\n\n1400\n\nhours since midnight, april 5\n\nfigure 10.7 logspline density estimate (solid line) from bowhead whale calf migration\ndata in example 10.6. below the histogram are dots indicating where knots were used (solid)\nand where knots were considered but rejected (hollow). two other logspline density estimates\nfor other knot choices are shown with the dotted and dashed lines; see the text for details.\n\ncomparison [365, 579]; other measures of model quality can also be motivated.\nselection of the model with the minimal value of bic(s) among those in the model\nsequence provides the chosen number of knots.\nadditional details of the knot selection process are given in [391, 615]. soft-\nware to carry out logspline density estimation in the r language is available in [388].\nstepwise addition and deletion of knots is a greedy search strategy that is not guar-\nanteed to find the best collection of knots. other search strategies are also effective,\nincluding mcmc strategies [305, 615].\nthe logspline approach is one of several effective methods for density estima-\ntion based on spline approximation; another is given in [285].\nexample 10.6 (whale migration, continued)\nfigure 10.7 shows the logspline\ndensity estimate (solid line) for the whale calf migration data from example 10.2.\nusing the procedure outlined above, a model with seven knots was selected. the\nlocations of these seven knots are shown with the solid dots in the figure. during\ninitial knot placement, stepwise knot addition, and stepwise knot deletion, four other\nknots were considered at various stages but not used in the model finally selected\naccording to the bic criterion. these discarded knots are shown with hollow dots\nin the figure. the degree of smoothness seen in figure 10.7 is typical of logspline\nestimates since splines are piecewise cubic and twice continuously differentiable.\nestimation of local modes can sometimes be a problem if the knots are insuf-\nficient in number or poorly placed. the other lines in figure 10.7 show the logspline\ndensity estimates with two other choices for the knots. the very poor estimate (dashed\nline) was obtained using 6 knots. the other estimate (dotted line) was obtained using\nall 11 knots shown in the figure with either hollow or solid dots.\n!\n\n "}, {"Page_number": 349, "text": "10.4 multivariate methods\n\n345\n\n10.4 multivariate methods\nmultivariate density estimation of a density function f is based on i.i.d. random\nvariables sampled from f. a p-dimensional variable is denoted xi = (xi1, . . . , xip).\n10.4.1 the nature of the problem\nmultivariate density estimation is a significantly different task than univariate density\nestimation. it is very difficult to visualize any resulting density estimate when the\nregion of support spans more than two or three dimensions. as an exploratory data\nanalysis tool, multivariate density estimation therefore has diminishing usefulness\nunless some dimension-reduction strategy is employed. however, multivariate den-\nsity estimation is a useful component in many more elaborate statistical computing\nalgorithms, where visualization of the estimate is not required.\nmultivariate density estimation is also hindered by the curse of dimension-\nality. high-dimensional space is very different than 1, 2, or 3-dimensional space.\nloosely speaking, high-dimensional space is vast, and points lying in such a space\nhave very few near neighbors. to illustrate, scott defined the tail region of a standard\np-dimensional normal density to comprise all points at which the probability density\nis less than one percent of the density at the mode [581]. while only 0.2% of the\nprobability density falls in this tail region when p = 1, more than half the density\nfalls in it when p = 10, and 98% falls in it when p = 20.\nthe curse of dimensionality has important implications for density estimation.\nforexample,considerakerneldensityestimatorbasedonarandomsampleof npoints\nwhose distribution is p-dimensional standard normal. below we mention several\nways to construct such an estimator; our choice here is the so-called product kernel\napproach with normal kernels sharing a common bandwidth, but it is not necessary\nto understand this technique yet to follow our argument. define the optimal relative\nroot mean squared error (orrmse) at the origin to be\n\norrmse(p, n) = 9minh,mseh& \u02c6f(0)\u2019-\n\nf(0)\n\n,\n\nwhere \u02c6f estimates f from a sample of n points using the best possible bandwidth.\nthis quantity measures the quality of the multivariate density estimator at the true\nmode. when p = 1 and n = 30, orrmse(1,30) = 0.1701. table 10.2 shows the\nsample sizes required to achieve as low a value of orrmse(p, n) for different values\nof p. the sample sizes are shown to about three significant digits. a different band-\nwidth minimizes orrmse(p, n) for each different p and n, so the table entries were\ncomputed by fixing p and searching over n, with each trial value of n requiring an\noptimization over h. this table confirms that desirable sample sizes grow very rapidly\nwith p. in practice, things are not as hopeless as table 10.2 might suggest. adequate\nestimates can be sometimes obtained with a variety of techniques, especially those\nthat attempt to simplify the problem via dimension reduction.\n\n "}, {"Page_number": 350, "text": "346\n\nchapter 10 nonparametric density estimation\n\ntable 10.2 sample sizes required to match the optimal rel-\native root mean squared error at the origin achieved for one-\ndimensional data when n = 30. these results pertain to estima-\ntion of a p-variate normal density using a normal product kernel\ndensity estimator with bandwidths that minimize the relative\nroot mean squared error at the origin in each instance.\n\np\n1\n2\n3\n5\n10\n15\n30\n\nn\n30\n180\n806\n17,400\n112,000,000\n2,190,000,000,000\n806,000,000,000,000,000,000,000,000\n\n10.4.2 multivariate kernel estimators\nthe most literal generalization of the univariate kernel density estimator in (10.6)\nto the case of p-dimensional density estimation is the general multivariate kernel\nestimator\n\n1\nn|h|\n\nn#i=1\n\n\u02c6f(x) =\n\nk)h\u22121(x \u2212 xi)* ,\n\n(10.40)\nwhere h is a p \u00d7 p nonsingular matrix of constants, whose absolute determinant is\ndenoted |h|. the function k is a real-valued multivariate kernel function for which\n$ k(z) dz = 1,$ zk(z) dz = 0, and$ zztk(z) dz = ip, where ip is the p \u00d7 p iden-\ntity matrix.\nthis estimator is quite a bit more flexible than usually is required. it allows both\na p-dimensional kernel of arbitrary shape and an arbitrary linear rotation and scaling\nvia h. it can be quite inconvenient to try to specify the large number of bandwidth\nparameters contained in h and to specify a kernel shape over p-dimensional space. it\nis more practical to resort to more specialized forms of h and k that have far fewer\nparameters.\nthe product kernelapproachprovidesagreatdealofsimplification.thedensity\nestimator is\n\n\u02c6f(x) =\n\n(10.41)\nwhere k(z) is a univariate kernel function, x = (x1, . . . , xp), xi = (xi1, . . . , xip),\nand the hj are fixed bandwidths for each coordinate, j = 1, . . . , p.\nanother simplifying approach would be to allow k to be a radially symmetric,\nunimodal density function in p dimensions, and to set\n\nhj /\nk. xj \u2212 xij\n\nn#i=1\n\np4j=1\n\n1\nhj\n\n1\nn\n\n\u02c6f(x) =\n\n1\nnhp\n\nk.x \u2212 xi\nh / .\n\nn#i=1\n\n(10.42)\n\n "}, {"Page_number": 351, "text": "10.4 multivariate methods\n\n347\n\nif ztz \u2264 1,\notherwise\n\n(10.43)\n\nin this case, the multivariate epanechnikov kernel shape\n(1 \u2212 ztz)\n\n(p + 2)\u0001(1 + p/2)\n0\n\n2\u03c0p/2\n\nk(z) =\u23a7\u23a8\u23a9\n\nis optimal with respect to the asymptotic mean integrated squared error. however, as\nwith univariate kernel density estimation, many other kernels provide nearly equiva-\nlent results.\nthe single fixed bandwidth in (10.42) means that probability contributions\nassociated with each observed data point will diffuse in all directions equally. when\nthe data have very different variability in different directions, or when the data nearly\nlie on a lower-dimensional manifold, treating all dimensions as if they were on the\nsamescalecanleadtopoorestimates.fukunaga[211]suggestedlinearlytransforming\nthe data so that they have identity covariance matrix, then estimating the density of\nthe transformed data using (10.42) with a radially symmetric kernel, and then back-\ntransforming to obtain the final estimate. to carry out the transformation, compute\nan eigenvalue\u2013eigenvector decomposition of the sample covariance matrix so !\u0001 =\np\u0001pt, where \u0001 is a p \u00d7 p diagonal matrix with the eigenvalues in descending order\nand p is an orthonormal p \u00d7 p matrix whose columns consist of the eigenvectors\ncorresponding to the eigenvalues in \u0001. let x be the sample mean. then setting\nzi = \u0001\u22121/2pt(xi \u2212 x) for i = 1, . . . , n provides the transformed data. this process\nis commonly called whitening or sphering the data. using the kernel density estimator\nin (10.42) on the transformed data is equivalent to using the density estimator\n\n|!\u0001|\u22121/2\n\nnhp\n\nk7(x \u2212 xi)t!\u0001\u22121(x \u2212 xi)\n\nh\n\nn#i=1\n\n8\n\n(10.44)\n\non the original data, for a symmetric kernel k.\nwithin the range of complexity presented by the choices above, the product\nkernel approach in (10.41) is usually preferred to (10.42) and (10.44), in view of\nits performance and flexibility. using a product kernel also simplifies the numerical\ncalculation and scaling of kernels.\nas with the univariate case, it is possible to derive an expression for the asymp-\ntotic mean integrated squared error for a product kernel density estimator. the min-\nimizing bandwidths h1, . . . , hp are the solutions to a set of p nonlinear equations.\nthe optimal hi are all o(n\u22121/(p+4)), and amise(h1, . . . , hp) = o(n\u22121/(p+4)) for\nthese optimal hi. bandwidth selection for product kernel density estimators and other\nmultivariate approaches is far less well studied than in the univariate case.\nperhaps the simplest approach to bandwidth selection in this case is to assume\nthat f is normal, thereby simplifying the minimization of amise(h1, . . . , hp) with\nrespect to h1, . . . , hp. this provides a bandwidth selection rationale akin to silver-\nman\u2019s rule of thumb in the univariate case. the resulting bandwidths for the normal\nproduct kernel approach are\n\nhi =.\n\n4\n\nn(p + 2)/1/(p+4)\n\n\u02c6\u03c3i\n\n(10.45)\n\n "}, {"Page_number": 352, "text": "chapter 10 nonparametric density estimation\n\n348\nfor i = 1, . . . , p, where \u02c6\u03c3 is an estimate of the standard deviation along the ith\ncoordinate. as with the univariate case, using a robust scale estimator can improve\nperformance. when nonnormal kernels are being used, the bandwidth for the nor-\nmal kernel can be rescaled using (10.32) and table 10.1 to provide an analogous\nbandwidth for the chosen kernel.\nterrell\u2019s maximal smoothing principle can also be applied to p-dimensional\nproblems. suppose we apply the general kernel density estimator given by (10.40)\nwith a kernel function that is a density with identity covariance matrix. then the\nmaximal smoothing principle indicates choosing a bandwidth matrix h that satisfies\n\nhht\n\n16n(p + 2)\u0001((p + 8)/2)>2/(p+4)\n==(p + 8)(p+6)/2\u03c0p/2r(k)\n\n!\u0001,\n\n(10.46)\n\nwhere !\u0001 is the sample covariance matrix. one could apply this result to find the\n\nmaximal smoothing bandwidths for a normal product kernel, then rescale the coor-\ndinatewise bandwidths using (10.32) and table 10.1 if another product kernel shape\nwas desired.\ncross-validation methods can also be generalized to the multivariate case, as\ncan some other automatic bandwidth selection procedures. however, the overall per-\nformance of such methods in general p-dimensional problems is not thoroughly\ndocumented.\n\n10.4.3 adaptive kernels and nearest neighbors\nwith ordinary fixed-kernel density estimation, the shape of k and the bandwidth are\nfixed. these determine an unchanging notion of proximity. weighted contributions\nfrom nearby xi determine \u02c6f(x), where the weights are based on the proximity of xi\nto x. for example, with a uniform kernel, the estimate is based on variable numbers\nof observations within a sliding window of fixed shape.\nit is worthwhile to consider the opposite viewpoint: allowing regions to vary\nin size, but requiring them (in some sense) to have a fixed number of observations\nfalling in them. then regions of larger size correspond to areas of low density, and\nregions of small size correspond to areas of high density.\nit turns out that estimators derived from this principle can be written as kernel\nestimators with a changing bandwidth that adapts to the local density of observed data\npoints. such approaches are variously termed adaptive kernel estimators, variable-\nbandwidthkernelestimators,orvariable-kernelestimators.threeparticularstrategies\nare reviewed below.\nthe motivation for adaptive methods is that a fixed bandwidth may not be\nequally suitable everywhere. in regions where data are sparse, wider bandwidths can\nhelp prevent excessive local sensitivity to outliers. conversely, where data are abun-\ndant, narrower bandwidths can prevent bias introduced by oversmoothing. consider\nagain the kernel density estimate of bowhead whale calf migration times given in\nfigure 10.5 using the fixed sheather\u2013jones bandwidth. for migration times below\n1200 and above 1270 hours, the estimate exhibits a number of modes, yet it is unclear\nhow many of these are true and how many are artifacts of sampling variability. it is not\n\n "}, {"Page_number": 353, "text": "10.4 multivariate methods\n\n349\npossible to increase the bandwidth sufficiently to smooth away some of the small local\nmodes in the tails without also smoothing away the prominent bimodality between\n1200 and 1270. only local changes to the bandwidth will permit such improvements.\nin theory, when p = 1 there is little to recommend adaptive methods over sim-\npler approaches, but in practice some adaptive methods have been demonstrated to be\nquite effective in some examples. for moderate or large p, theoretical analysis sug-\ngests that the performance of adaptive methods can be excellent compared to standard\nkernel estimators, but the practical performance of adaptive approaches in such cases\nis not thoroughly understood. some performance comparisons for adaptive methods\ncan be found in [356, 581, 628].\n10.4.3.1 nearest neighbor approaches the kth nearest neighbor density\nestimator,\n\n\u02c6f(x) =\n\nk\n\nnvpdk(x)p\n\n,\n\n(10.47)\n\nwas the first approach to explicitly adopt a variable-bandwidth viewpoint [423]. for\nthis estimator, dk(x) is the euclidean distance from x to the kth nearest observed\ndata point, and vp is the volume of the unit sphere in p dimensions, where p is the\ndimensionality of the data. since vp = \u03c0p/2/ \u0001(p/2 + 1), note that dk(x) is the only\nrandomquantityin(10.47),asitdependsonx1, . . . ,xn.conceptually,the kthnearest\nneighbor estimate of the density at x is k/n divided by the volume of the smallest\nsphere centered at x that contains k of the n observed data values. the number of\nnearest neighbors, k, plays a role analogous to that of bandwidth: large values of k\nyield smooth estimates, and small values of k yield wiggly estimates.\nthe estimator (10.47) may be viewed as a kernel estimator with a bandwidth\nthat varies with x and a kernel equal to the density function that is uniform on the\nunit sphere in p dimensions. for an arbitrary kernel, the nearest neighbor estimator\ncan be written as\n\n\u02c6f(x) =\n\n1\n\nndk(x)p\n\nk.x \u2212 xi\ndk(x) / .\n\nn#i=1\n\n(10.48)\n\nif dk(x)isreplacedwithanarbitraryfunction hk(x),whichmaynotexplicitlyrepresent\ndistance, the name balloon estimator has been suggested because the bandwidth\ninflates or deflates through a function that depends on x [628]. the nearest neighbor\nestimator is asymptotically of this type: for example, using dk(x) as the bandwidth for\nthe uniform-kernel nearest neighbor estimator is asymptotically equivalent to using\na balloon estimator bandwidth of hk(x) = [k/(nvpf(x))]1/p, since\n\nk\n\nnvpdk(x)p \u2192 f(x)\n\nas n \u2192 \u221e, k \u2192 \u221e, and k/n \u2192 0.\nnearest neighbor and balloon estimators exhibit a number of surprising\nattributes. first, choosing k to be a density does not ensure that \u02c6f is a density; for\ninstance, the estimator in (10.47) does not have a finite integral. second, when p = 1\nand k is a density with zero mean and unit variance, choosing hk(x) = k/[2nf(x)]\n\n "}, {"Page_number": 354, "text": "chapter 10 nonparametric density estimation\n\n350\ndoes not offer any asymptotic improvement relative to a standard kernel estimator,\nregardlessofthechoiceof k [581].finally,onecanshowthatthepointwiseasymptotic\nmean squared error of a univariate balloon estimator is minimized when\n\nhk(x) = h(x) =. f(x)r(k)\n\nnf\u2032\u2032(x) /1/5\n\n.\n\neven with this optimal pointwise adaptive bandwidth, however, the asymptotic\nefficiency of univariate balloon estimators does not greatly exceed that of ordinary\nfixed-bandwidth kernel estimators when f is roughly symmetric and unimodal [628].\nthus, nearest neighbor and balloon estimators seem a poor choice when p = 1.\non the other hand, for multivariate data, balloon estimators offer much more\npromise. the asymptotic efficiency of the balloon estimator can greatly surpass that\nof a standard multivariate kernel estimator, even for fairly small p and symmetric,\nunimodal data [628]. if we further generalize (10.48) to\n\n\u02c6f(x) =\n\n1\n\nn|h(x)|\n\nn#i=1\n\nk)h(x)\u22121(x \u2212 xi)* ,\n\n(10.49)\n\nwhere h(x) is a bandwidth matrix that varies with x, then we have effectively allowed\nthe shape of kernel contributions to vary with x. when h(x) = hk(x)i, the general\nform reverts to the balloon estimator. further, setting hk(x) = dk(x) yields the nearest\nneighbor estimator in (10.48). more general choices for h(x) are mentioned in [628].\n10.4.3.2 variable-kernel approaches and transformations a variable-\nkernel or sample point adaptive estimator can be written as\n\n\u02c6f(x) =\n\n1\nn\n\nn#i=1\n\n1\nhp\ni\n\nhi / ,\nk.x \u2212 xi\n\n(10.50)\n\nwhere k is a multivariate kernel and hi is a bandwidth individualized for the kernel\ncontribution centered at xi [66]. for example, hi might be set equal to the distance\nfrom xi to the kth nearest other observed data point, so hi = dk(xi). a more general\nvariable kernel estimator with bandwidth matrix hi that depends on the ith sampled\npoint is also possible [cf. equation (10.49)], but we focus on the simpler form here.\nthe variable-kernel estimator in (10.50) is a mixture of kernels with identical\nshape but different scales, centered at each observation. letting the bandwidth vary as\na function of xi rather than of x guarantees that \u02c6f is a density whenever k is a density.\noptimal bandwidths for variable-kernel approaches depend on f. pilot estima-\ntion of f can be used to guide bandwidth adaptation. consider the following general\nstrategy:\n\n1. construct a pilot estimator\n\n\u02dcf(x) that is strictly positive for all observed xi.\npilot estimation might employ, for example, a normal product kernel den-\n\u02dcf is based\nsity estimator with bandwidth chosen according to (10.45). if\n\u02dcf(x)\non an estimator that may equal or approach zero at some xi, then let\nequal the estimated density whenever the estimate exceeds \u03f5, and let \u02dcf(x) = \u03f5\n\n "}, {"Page_number": 355, "text": "10.4 multivariate methods\n\n351\n\n.15\n\n.10\n.05\n\ny\nt\ni\ns\nn\ne\nd\n \ne\nu\nr\nt\n\n0\n\u221210 \u22125\n\n0\nx1\n\ny\nt\ni\ns\nn\ne\nd\nd\ne\nt\na\nm\n\n \n\ni\nt\ns\ne\n\n.15\n\n.10\n.05\n\n0\n\u221210 \u22125\n\n5\n\n10\n\n0\nx1\n\ny\nt\ni\ns\nn\ne\nd\nd\ne\nt\na\nm\n\n \n\ni\nt\ns\ne\n\n.15\n\n.10\n.05\n\n0\n\u221210 \u22125\n\n5\n\n10\n\n5\n\n10\n\n0\nx1\n\nfigure 10.8 results from example 10.7. from left to right, the three panels show the\nbivariate density value along the one-dimensional slice for which x2 = 0 for: the true bivariate\nt distribution with two degrees of freedom, the bivariate estimate using a fixed-bandwidth\nproduct kernel approach, and the bivariate estimate using abramson\u2019s adaptive approach as\ndescribed in the text.\n\notherwise. the choice of an arbitrary small constant \u03f5 > 0 improves perfor-\nmance by providing an upper bound for adaptively chosen bandwidths.\n2. let the adaptive bandwidth be hi = h/ \u02dcf(xi)\u03b1, for a sensitivity parameter\n0 \u2264 \u03b1 \u2264 1. the parameter h assumes the role of a bandwidth parameter that\ncan be adjusted to control the overall smoothness of the final estimate.\n3. apply the variable-kernel estimator in (10.50) with the bandwidths hi found in\n\nstep 2 to produce the final estimate.\nthe parameter \u03b1 affects the degree of local adaptation by controlling how\nquickly the bandwidth changes in response to suspected changes in f. asymptotic\n1\narguments and practical experience support setting \u03b1 =\n2, which yields the approach\nof abramson [3]. several investigators have found this approach to perform well in\npractice [598, 674].\nan alternative proposal is \u03b1 = 1/p, which yields an approach that is asymptoti-\ncally equivalent to the adaptive kernel estimator of breiman, meisel, and purcell [66].\nthis choice ensures that the number of observed data points captured by the scaled\nkernel will be roughly equal everywhere [598]. in their algorithm, these authors used\na nearest neighbor approach for \u02dcf and set hi = hdk(xi) for a smoothness parameter\nh that may depend on k.\nexample 10.7 (bivariate t distribution) to illustrate the potential benefit of\nadaptive approaches, consider estimating the bivariate t distribution (with two degrees\nof freedom) from a sample of size n = 500. in the nonadaptive approach, we use a\nnormal product kernel with individual bandwidths chosen using the sheather\u2013jones\napproach. as an adaptive alternative, we use abramson\u2019s variable-kernel approach\n1\n(\u03b1 =\n2) with a normal product kernel, the pilot estimate taken to be the result of\nthe nonadaptive approach, \u03f5 = 0.005, and h set equal to the mean of the coordinate-\nwise bandwidths used in the nonadaptive approach times the geometric mean of the\n\u02dcf(xi)1/2.\nthe left panel of figure 10.8 shows the true values of the bivariate t distribution\nwith two degrees of freedom, f, along the line x2 = 0. in other words, this shows a\n\n "}, {"Page_number": 356, "text": "chapter 10 nonparametric density estimation\n\n352\nslice from the true density. the center panel of figure 10.8 shows the result of the\nnonadaptive approach. the tails of the estimate exhibit undesirable wiggliness caused\nby an inappropriately narrow bandwidth in the tail regions, where a few outliers\nfall. the right panel of figure 10.8 shows the result from abramson\u2019s approach.\nbandwidths are substantially wider in the tail areas, thereby producing smoother\nestimates in these regions than were obtained from the fixed-bandwidth approach.\nabramson\u2019s method also uses much narrower bandwidths near the estimated mode.\nthere is a slight indication of this for our random sample, but the effect can sometimes\nbe pronounced.\n!\nhaving discussed the variable-kernel approach, emphasizing its application in\nhigher dimensions, we next consider a related approach primarily used for univariate\ndata.thismethodillustratesthepotentialadvantageofdatatransformationfordensity\nestimation.\nwand, marron, and ruppert noted that conducting fixed-bandwidth kernel den-\nsity estimation on data that have been nonlinearly transformed is equivalent to using\na variable-bandwidth kernel estimator on the original data [652]. the transformation\ninduces separate bandwidths hi at each data point.\n\nsuppose univariate data x1, . . . , xn are observed from a density fx. let\n\ny = t\u03bb(x) =\n\n\u03c3xt\u2217\u03bb(x)\n\u03c3t\u2217\u03bb(x)\n\n(10.51)\n\ndenote a transformation, where t\u2217\u03bb is a monotonic increasing mapping of the support\nof f to the real line parameterized by \u03bb, and \u03c32\nt\u2217\u03bb(x) are the variances of x and\ny = t\u2217\u03bb(x), respectively. then t\u03bb is a scale-preserving transformation that maps the\nrandom variable x \u223c fx to y having density\n\nx and \u03c32\n\n\u03bb (y)???? .\n\n\u03bb (y))????\n\nd\ndy\n\nt\u22121\n\ng\u03bb(y) = fx(t\u22121\n\n(10.52)\nfor example, if x is a standard normal random variable and t\u2217\u03bb(x) = exp{x}, then\ny has the same variance as x. however, a window of fixed width 0.3 on the y scale\ncentered at any value y has variable width when back-transformed to the x scale: the\nwidth is roughly 2.76 when x = \u22121 but only 0.24 when x = 1. in practice, sample\nstandard deviations or robust measures of spread may be used in t\u03bb to preserve scale.\nsuppose we transform the data using t\u03bb to obtain y1, . . . , yn, then construct a\nfixed-bandwidth kernel density estimate for these transformed data, and then back-\ntransform the resulting estimate to the original scale to produce an estimate of fx.\nfrom (10.18) we know that the bandwidth that minimizes amise(h) for a kernel\nestimate of g\u03bb is\n\nh\u03bb =. r(k)\n\nkr(g\u2032\u2032\u03bb)/1/5\n\nn\u03c34\n\n(10.53)\n\nfor a given choice of \u03bb.\n\n "}, {"Page_number": 357, "text": "353\nsince h\u03bb depends on the unknown density g\u03bb, a plug-in method is suggested to\nestimate r(g\u2032\u2032\u03bb) by \u02c6r(g\u2032\u2032\u03bb) = r(\u02c6g\u2032\u2032\u03bb), where \u02c6g is a kernel estimator using pilot bandwidth\nh0. wand, marron, and ruppert suggest using a normal kernel with silverman\u2019s rule\nof thumb to determine h0, thereby yielding the estimator\n\n10.4 multivariate methods\n\n\u02c6r(g\u2032\u2032\u03bb) =\n\n1\nn2h5\n\n0##i /= j\n\n\u03c6(4). yi \u2212 yj\nh0 / ,\n\n(10.54)\n\nwhere h0 = \u221a2\u02c6\u03c3x[84\u221a\u03c0/(5n2)]1/13 and \u03c6(4) is the fourth derivative of the standard\nnormal density [652]. since t\u03bb is scale-preserving, the sample standard deviation of\nx1, . . . , xn, say \u02c6\u03c3x, provides an estimate of the standard deviation of y to use in the\nexpression for h0. related derivative estimation ideas are discussed in [298, 581].\n\nthe familiar box\u2013cox transformation [57],\n\nt\u03bb(x) =5(x\u03bb \u2212 1)/\u03bb if \u03bb /= 0,\nif \u03bb = 0,\n\nlog x\n\n(10.55)\n\nis among the parameterized transformation families available for (10.51). when any\ngood transformation will suffice, or in multivariate settings, it can be useful to rely\nupon the notion that the transformation should make the data more nearly symmetric\nand unimodal because fixed-bandwidth kernel density estimation is known to perform\nwell in this case.\nthis transformation approach to variable-kernel density estimation can work\nwell for univariate skewed unimodal densities. extensions to multivariate data are\nchallenging, and applications to multimodal data can result in poor estimates. with-\nout all the formalism outlined above, data analysts routinely transform variables to\nconvenient scales using functions such as the log, often retaining this transforma-\ntion thereafter for displaying results and even making inferences. when inferences\non the original scale are preferred, one could pursue a transformation strategy based\non graphical or quantitative assessments of the symmetry and unimodality achieved,\nrather than optimizing the transformation within a class of functions as described\nabove.\n\nexploratory projection pursuit\n\n10.4.4\nexploratory projection pursuit focuses on discovering low-dimensional structure in\na high-dimensional density. the final density estimate is constructed by modifying a\nstandard multivariate normal distribution to reflect the structure found. the approach\ndescribed below follows friedman [206], which extends previous work [210, 338].\nin this subsection, reference will be made to a variety of density functions\nwith assorted arguments. for notational clarity, we therefore assign a subscript to the\ndensity function to identify the random variable whose density is being discussed.\nletthedataconsistof nobservationsof p-dimensionalvariables,x1, . . . ,xn \u223c\ni.i.d. fx. before beginning exploratory projection pursuit, the data are transformed\nto have mean 0 and variance\u2013covariance matrix ip. this is accomplished using the\nwhitening or sphering transformation described in section 10.4.2. let fz denote the\n\n "}, {"Page_number": 358, "text": "chapter 10 nonparametric density estimation\n\n354\ndensity corresponding to the transformed variables, z1, . . . ,zn. both fz and fx are\nunknown. to estimate fx it suffices to estimate fz and then reverse the transformation\nto obtain an estimate of fx. thus our primary concern will be the estimation of fz.\nseveral steps in the process rely on another density estimation technique, based\non legendre polynomial expansion. the legendre polynomials are a sequence of\northogonal polynomials on [\u22121,1] defined by p0(u) = 1, p1(u) = u, and pj(u) =\n2(2j \u2212 1)upj\u22121(u) \u2212 (j \u2212 1)pj\u22122(u)3@ j for j \u2265 2, having the property that the l2\nnorm$ 1\n\u22121 p2\nj (u) du = 2/(2j + 1) for all j [2, 568]. these polynomials can be used\nas a basis for representing functions on [\u22121,1]. in particular, we can represent a\nunivariate density f that has support only on [\u22121,1] by its legendre polynomial\nexpansion\n\nwhere\n\naj =\n\nf(x) =\n\najpj(x),\n\n\u221e#j=0\n2j + 1\n2 e{pj(x)}\n\n(10.56)\n\n(10.57)\n\nand the expectation in (10.57) is taken with respect\nto f. equation (10.57)\ncan be confirmed by noting the orthogonality and l2 norm of the pj. if we\nobserve x1, . . . , xn \u223c i.i.d. f, then (1/n)\"n\ni=1 pj(xi) is an estimator of e{pj(x)}.\ntherefore\n2j + 1\nn#i=1\n2n\n\nmaybeusedasestimatesofthecoefficientsinthelegendreexpansionof f.truncating\nthe sum in (10.56) after j + 1 terms suggests the estimator\n\n(10.58)\n\npj(xi)\n\n\u02c6aj =\n\n\u02c6f(x) =\n\nj#j=0\n\n\u02c6ajpj(x).\n\n(10.59)\n\nhaving described this legendre expansion approach, we can now move on to study\nexploratory projection pursuit.\nthefirststepofexploratoryprojectionpursuitis aprojection step.if yi = \u03b1tzi,\nthen we say that yi is the one-dimensional projection of zi in the direction \u03b1. the goal\nof the first step is to project the multivariate observations onto the one-dimensional\nline for which the distribution of the projected data has the most structure.\nthe degree of structure in the projected data is measured as the amount of\ndeparture from normality. let u(y) = 2\u0001(y) \u2212 1, where \u0001 is the standard normal\ncumulative distribution function. if y \u223c n(0,1), then u(y) \u223c unif(\u22121,1). to mea-\nsure the structure in the distribution of y it suffices to measure the degree to which\nthe density of u(y) differs from unif(\u22121,1).\n\n "}, {"Page_number": 359, "text": "define a structure index as\n\ns(\u03b1) =% 1\n\n\u22121afu(u) \u2212\n\n1\n\n2b2\n\n10.4 multivariate methods\n\n355\n\ndu = r(fu) \u2212\n\n1\n2 ,\n\n(10.60)\n\nwhere fu is the probability density function of u(\u03b1tz) when z \u223c fz. when s(\u03b1) is\nlarge, a large amount of nonnormal structure is present in the projected data. when\ns(\u03b1) is nearly zero, the projected data are nearly normal. note that s(\u03b1) depends on\nfu, which must be estimated.\nto estimate s(\u03b1) from the observed data, use the legendre expansion for fu to\nreexpress r(fu) in (10.60) as\n\nr(fu) =\n\n2j + 1\n2\n\n\u221e#j=0\n\n2e{pj(u)}32\n\n,\n\n(10.61)\n\nn\n\n\u2212\n\n1\n2\n\nn#i=1\n\nj#j=0\n\nwhere the expectations are taken with respect to fu. since u(\u03b1tz1), . . . , u(\u03b1tzn)\nrepresent draws from fu, the expectations in (10.61) can be estimated by sample\nmoments. if we also truncate the sum in (10.61) at j + 1 terms, we obtain\n\n2 71\n2j + 1\n\npj(2\u0001(\u03b1tzi) \u2212 1)82\n\n(10.62)\n\n1z denote the univariate marginal density of \u02c6\u03b1t\n\n\u02c6s(\u03b1) =\nas an estimator of s(\u03b1).\nthus, to estimate the projection direction yielding the greatest nonnormal struc-\nture, we maximize \u02c6s(\u03b1) with respect to \u03b1, subject to the constraint that \u03b1t\u03b1 = 1.\ndenote the resulting direction by \u02c6\u03b11. although \u02c6\u03b11 is estimated from the data, we treat\nit as a fixed quantity when discussing distributions of projections of random vectors\n1z when\nonto it. for example, let f \u02c6\u03b1t\nz \u223c fz, treating z as random and \u02c6\u03b11 as fixed.\nthe second step of exploratory projection pursuit is a structure removal step.\nthe goal is to apply a transformation to z1, . . . ,zn which makes the density of the\nprojection of fz on \u02c6\u03b11 a standard normal density, while leaving the distribution of a\nprojectionalonganyorthogonaldirectionunchanged.todothis,leta1 beanorthonor-\nmalmatrixwithfirstrowequalto \u02c6\u03b1t\n1.also,forobservationsfromarandomvectorv =\n(v1, . . . , vp),definethevectortransformationt(v) =&\u0001\u22121(fv1(v1)), v2, . . . , vp\u2019,\nwhere fv1 isthecumulativedistributionfunctionofthefirstelementofv.thenletting\n(10.63)\nfor i = 1, . . . , n would achieve the desired transformation. the transformation in\n(10.63) cannot be used directly to achieve the structure removal goal because it de-\n1z. to get around\npends on the cumulative distribution function corresponding to f \u02c6\u03b1t\nthisproblem,simplyreplacethecumulativedistributionfunctionwiththecorrespond-\ning empirical distribution function of \u02c6\u03b1t\n1zn. an alternative replacement\nis suggested in [340].\n\nz(1)\ni = at\n\n1z1, . . . , \u02c6\u03b1t\n\n1t(a1zi)\n\n "}, {"Page_number": 360, "text": "356\n\nchapter 10 nonparametric density estimation\n\ni\n\nthe z(1)\n\nfor i = 1, . . . , n may be viewed as a new dataset consisting of the\nobserved values of random variables z(1)\nn whose unknown distribution fz(1)\ndepends on fz. there is an important relationship between the conditionals deter-\nmined by fz(1) and fz given a projection onto \u02c6\u03b11. specifically, the conditional dis-\ntribution of z(1)\n1z(1)\nequals the conditional distribution of zi given \u02c6\u03b1t\n1zi\nbecause the structure removal step creating z(1)\nleaves all coordinates of zi except\nthe first unchanged. therefore\n\n1 , . . . ,z(1)\n\ngiven \u02c6\u03b1t\n\ni\n\ni\n\ni\n\nfz(1)(z) =\n\nfz(z)\u03c6( \u02c6\u03b1t\n1z)\n1z) .\n1z( \u02c6\u03b1t\nf \u02c6\u03b1t\n\n(10.64)\n\n1 , . . . ,z(1)\n\nequation (10.64) provides no immediate way to estimate fz, but iterating the entire\nprocess described above will eventually prove fruitful.\nsuppose a second projection step is conducted. a new direction to project\nthe working variables z(1)\nn is sought to isolate the greatest amount of one-\ndimensionalstructure.findingthisdirectionrequiresthecalculationofanewstructure\nindex based on the transformed sample z(1)\n1 , . . . ,z(1)\nn , leading to the estimation of \u02c6\u03b12\nas the projection direction revealing greatest structure.\ntaking a second structure removal step requires the reapplication of equa-\ntion (10.63) with a suitable matrix a2, yielding new working variables z(2)\n1 , . . . ,z(2)\nn .\niterating the same conditional distribution argument as expressed in (10.64)\nallows us to write the density from which the new working data arise as\n\n(10.65)\n\nfz(2)(z) = fz(z)\n\n1z)\u03c6( \u02c6\u03b1t\n2z)\n2z(1)( \u02c6\u03b1t\n1z)f \u02c6\u03b1t\n\n\u03c6( \u02c6\u03b1t\n1z( \u02c6\u03b1t\nf \u02c6\u03b1t\n2z(1) when z(1) \u223c fz(1).\n\n2z) ,\n\n2z(1) is the marginal density of \u02c6\u03b1t\n\nwhere f \u02c6\u03b1t\nsuppose the projection and structure removal steps are iterated several addi-\ntional times. at some point, the identification and removal of structure will lead to\nnew variables whose distribution has little or no remaining structure. in other words,\ntheir distribution will be approximately normal along any possible univariate projec-\ntion. at this point, iterations are stopped. suppose that a total of m iterations were\ntaken. then (10.65) extends to give\n\nfz(m)(z) = fz(z)\n\nm4m=1\nmz(m\u22121) is the marginal density of \u02c6\u03b1t\n\n\u03c6( \u02c6\u03b1t\nmz)\nmz(m\u22121)( \u02c6\u03b1t\nf \u02c6\u03b1t\nmz(m\u22121) when z(m\u22121) \u223c fz(m\u22121), and\nnow, equation (10.66) can be used to estimate fz because\u2014having eliminated\ni \u2014we may set fz(m)\n\nwhere f \u02c6\u03b1t\nz(0) \u223c fz.\nall structure from the distribution of our working variables z(m)\n\n(10.66)\n\nmz) ,\n\n "}, {"Page_number": 361, "text": "357\nequal to a p-dimensional multivariate normal density, denoted \u03c6p. solving for fz\ngives\n\n10.4 multivariate methods\n\nfz(z) = \u03c6p(z)\n\nm4m=1\n\nmz(m\u22121)( \u02c6\u03b1t\nmz)\nf \u02c6\u03b1t\n\u03c6( \u02c6\u03b1t\nmz)\n\n.\n\n(10.67)\n\nmz(m\u22121), these\ncan be estimated using the legendre approximation strategy. note that if u(m\u22121) =\n2\u0001( \u02c6\u03b1t\n\nalthough this equation still depends on the unknown densities f \u02c6\u03b1t\nmz(m\u22121)) \u2212 1 for z(m\u22121) \u223c fz(m\u22121), then\n\n.\n\n(10.68)\n\n. after substituting\n\n(10.69)\n\n(10.70)\n\nn\n\n)\n\nf \u02c6\u03b1t\n\nfu(m\u22121)(u) =\n\n(m\u22121)\npj(u\ni\nn\n\n\u02c6f u(m\u22121)(u) =\n, . . . , u(m\u22121)\n\nmz(m\u22121)&\u0001\u22121 ((u + 1)/2)\u2019\n2\u03c6&\u0001\u22121 ((u + 1)/2)\u2019\n2 pj(u)\nderived from z(m\u22121)\n\nuse the legendre expansion of fu(m\u22121) and sample moments to estimate\n6\n\nj#j=052j + 1\nn#i=1\n(m\u22121)\n, . . . ,z(m\u22121)\nfrom u\n1\n1\n\u02c6f u(m\u22121) for fu(m\u22121) in (10.68) and isolating f \u02c6\u03b1t\nmz(m\u22121), we obtain\nmz) \u2212 1\u2019\u03c6( \u02c6\u03b1t\nmz) \u2212 1\u2019 \u00afpjm\u23ab\u23ac\u23ad\n) \u2212 1*\n\nmz) = 2 \u02c6f u(m\u22121)&2\u0001( \u02c6\u03b1t\nm4m=1\u23a7\u23a8\u23a9\nj#j=0\n(2j + 1)pj&2\u0001( \u02c6\u03b1t\npj)2\u0001( \u02c6\u03b1t\nn#i=1\n\nthus, from (10.67) the estimate for fz(z) is\n\u02c6f(z) = \u03c6p(z)\n\nmz(m\u22121)( \u02c6\u03b1t\n\n\u00afpjm =\n\nmz(m\u22121)\n\nwhere the\n\nmz).\n\n\u02c6f \u02c6\u03b1t\n\n1\nn\n\nn\n\ni\n\n,\n\n(10.71)\n\n(10.72)\n\nare estimated using the working variables accumulated during the structure removal\nprocess, and z(0)\ni = zi. reversing the sphering by applying the change of variable\nx = p\u00011/2z + \u00afx to \u02c6f z provides the estimate \u02c6f x.\nthe estimate \u02c6f z is most strongly influenced by the central portion of the data\nbecause the transformation u compresses information about the tails of fz into the\nextreme portions of the interval [\u22121,1]. low-degree legendre polynomial expansion\nhas only limited capacity to capture substantial features of fu in these narrow margins\nof the interval. furthermore, the structure index driving the choice of each \u02c6\u03b1m will not\nassign high structure to directions for which only the tail behavior of the projection\nis nonnormal. therefore, exploratory projection pursuit should be viewed foremost\nas a way to extract key low-dimensional features of the density which are exhibited\nby the bulk of the data, and to reconstruct a density estimate that reflects these key\nfeatures.\n\n "}, {"Page_number": 362, "text": "358\n\nchapter 10 nonparametric density estimation\n\n2\n\n0\n\n\u22122\n\n.50\n\n.25\n\n2\nz\n\ny\nt\ni\ns\nn\ne\nd\n\n\u22122\n\n0\n\nz 1\n\n2\n\n2\n\n0\n\n\u22122\n\n.50\n\n.25\n\n)\n1\n(\n\n2\n\nz\n\ny\nt\ni\ns\nn\ne\nd\n\n\u22122\n\n0\nz (1)\n1\n\n2\n\n2\n\n0\n\n\u22124\n\n\u22122\n\n0\n\n2\n\n0\n\n\u22124\n\n\u22122\n\n\u02c6\u03b1 t\n1 zi\n\n0\n\u02c6\u03b1 t\n2 z(1)\n\ni\n\nfigure10.9 firsttwoprojectionandstructureremovalstepsforexample10.8,asdescribed\nin the text.\n\nexample 10.8 (bivariate rotation) to illustrate exploratory projection pursuit,\nwe will attempt to reconstruct the density of some bivariate data. let w = (w1, w2),\nwhere w1 \u223c gamma(4,2) and w2 \u223c n(0,1) independently. then e{w} = (2,0)\nand var{w} = i. use\n\nr =7\u22120.581 \u22120.814\n0.5818\n\n\u22120.814\n\nto rotate w to produce data via x = rw. let fx denote the density of x, which\nwe will try to estimate from a sample of n = 500 points drawn from fx. since\nvar{x} = rrt = i, the whitening transformation is nearly just a translation (aside\nfrom the fact that the theoretical and sample variance\u2013covariance matrices differ\nslightly).\nthe whitened data values, z1, . . . ,z500, are plotted in the top left panel of\nfigure 10.9. the underlying gamma structure is detectable in this graph as an abruptly\ndeclining frequency of points in the top right region of the plot: z and x are rotated\nabout 135 degrees counterclockwise with respect to w.\nthe direction \u02c6\u03b11 that reveals the most univariate projected structure is shown\nwith the line in the top left panel of figure 10.9. clearly this direction corresponds\nroughly to the original gamma-distributed coordinate. the bottom left panel of fig-\nure 10.9 shows a histogram of the zi values projected onto \u02c6\u03b11, revealing a rather\nnonnormal distribution. the curve superimposed on this histogram is the univariate\n\n "}, {"Page_number": 363, "text": "10.4 multivariate methods\n\n359\n\n2\n\n0\n\n2\n\n.\n\n0\n\n)\nz\n(\n\nz\n\n\u02c6f\n\n1\n\n.\n\n0\n\n0\n\n\u22122\n\n0\n\nz\n1\n\n2\n\n2\n\n\u2212\n\nz\n\n2\n\nfigure 10.10 exploratory projection pursuit density estimate \u02c6f z for example 10.8.\n\n1 , . . . ,z(1)\n\ndensity estimate for \u02c6\u03b1t\n1z obtained using the legendre expansion strategy. throughout\nthis example, the number of legendre polynomials was set to j + 1 = 4.\nremoving the structure revealed by the projection on \u02c6\u03b11 yields new working\ndatavalues,z(1)\n500,graphedinthetoprightpaneloffigure10.9.theprojection\ndirection, \u02c6\u03b12, showing the most nonnormal structure is again shown with a line. the\nbottomrightpanelshowsahistogramof \u02c6\u03b1t\n2z(1) valuesandthecorrespondinglegendre\ndensity estimate.\nat this point, there is little need to proceed with additional projection and\nstructure removal steps: the working data are already nearly multivariate normal.\nemploying (10.71) to reconstruct an estimate of fz yields the density estimate shown\nin figure 10.10. the rotated gamma\u2013normal structure is clearly seen in the figure,\nwith the heavier gamma tail extending leftward and the abrupt tail terminating on\nthe right. the final step in application would be to reexpress this result in terms of a\ndensity for x rather than z.\n!\n\nproblems\n10.1. sanders et al. provide a comprehensive dataset of infrared emissions and other charac-\nteristics of objects beyond our galaxy [567]. these data are available from the website\nfor our book. let x denote the log of the variable labeled f12, which is the total\n12-\u00b5m-band flux measurement on each object.\na. fit a normal kernel density estimate for x, using bandwidths derived from\nthe ucv(h) criterion, silverman\u2019s rule of thumb, the sheather\u2013jones approach,\nterrell\u2019s maximal smoothing principle, and any other approaches you wish.\ncomment on the apparent suitability of each bandwidth for these data.\n\nb. fit kernel density estimates for x using uniform, normal, epanechnikov, and tri-\nweight kernels, each with bandwidth equivalent to the sheather\u2013jones bandwidth\nfor a normal kernel. comment.\n\nc. fit a nearest neighbor density estimate for x as in (10.48) with the uniform and\nnormal kernels. next fit an abramson adaptive estimate for x using a normal\n\n "}, {"Page_number": 364, "text": "360\n\nchapter 10 nonparametric density estimation\n\nkernel and setting h equal to the sheather\u2013jones bandwidth for a fixed-bandwidth\nestimator times the geometric mean of the \u02dcf x(xi)1/2 values.\n\ne. let\n\nfor estimating the density of x.\n\nd. if code for logspline density estimation is available, experiment with this approach\n\u02c6f x denote the normal kernel density estimate for x computed using the\nsheather\u2013jones bandwidth. note the ratio of this bandwidth to the bandwidth given\nby silverman\u2019s rule of thumb. transform the data back to the original scale (i.e.,\nz = exp{x}), and fit a normal kernel density estimate \u02c6f z, using a bandwidth equal\nto silverman\u2019s rule of thumb scaled down by the ratio noted previously. (this is an\ninstance where the robust scale measure is far superior to the sample standard devi-\nation.) next, transform \u02c6f x back to the original scale using the change-of-variable\nformula for densities, and compare the two resulting estimates of density for z\non the region between 0 and 8. experiment further to investigate the relationship\nbetween density estimation and nonlinear scale transformations. comment.\n\n10.2. thisproblemcontinuesusingtheinfrareddataonextragalacticobjectsandthevariable\nx (the log of the 12-\u00b5m-band flux measurement) from problem 10.1. the dataset also\nincludes f100 data: the total 100-\u00b5m-band flux measurements for each object. denote\nthe log of this variable by y. construct bivariate density estimates for the joint density\nof x and y using the following approaches.\na. use a standard bivariate normal kernel with bandwidth matrix hi2. describe how\n\nyou chose h.\n\nb. useabivariatenormalkernelwithbandwidthmatrixhchosenbyterrell\u2019smaximal\nsmoothing principle. find a constant c for which the bandwidth matrix ch provides\na superior density estimate.\n\nc. use a normal product kernel with the bandwidth for each coordinate chosen using\n\nthe sheather\u2013jones approach.\n\nd. use a nearest neighbor estimator (10.48) with a normal kernel. describe how you\n\nchose k.\n\ne. use an abramson adaptive estimator with the normal product kernel and band-\n\nwidths chosen in the same manner as example 10.7.\n\n10.3. starting from equation (10.22), derive a simplification for ucv(h) when k(z) =\n\n\u03c6(z) = exp{\u2212z2/2}/\u221a2\u03c0, pursuing the following steps:\na. show that\nn#i=1 % k2) x \u2212 xi\nh * dx\nn(n \u2212 1)h2\n\nucv(h) =\n\n1\nn2h2\n\n+\n\n1\n\nn#i=1 #j /= i% k) x \u2212 xi\nh * k) x \u2212 xj\n*\nk) xi \u2212 xj\nn#i=1 #j /= i\n\nh\n\nh * dx\n\n2\n\n\u2212\n\nn(n \u2212 1)h\n= a + b + c,\n\nwhere a, b, and c denote to the three terms given above.\n\n "}, {"Page_number": 365, "text": "10.4 multivariate methods\n\n361\n\nb. show that\n\nc. show that\n\n1\n\n2nh\u221a\u03c0\n\n.\n\na =\n\n1\n\nb =\n\n2n(n \u2212 1)h\u221a\u03c0\nd. finish by showing (10.23).\n\nn#i=1 #j /= i\n\nexp( \u22121\n\n4h2 (xi \u2212 xj)2+ .\n\n(10.73)\n\n10.4. replicate the first four rows of table 10.2. assume \u02c6f is a product kernel estimator.\nyou may find it helpful to begin with the expression mseh( \u02c6f(x)) = var{ \u02c6f(x)} +\n&bias, \u02c6f(x)-\u20192, and to use the result\n\n\u03c32\u03c42\n\n\u03c32 + \u03c42/\n\u03c6(x; \u00b5, \u03c32)\u03c6(x; \u03bd, \u03c42) = \u03c6.x; \u00b5\u03c42 + \u03bd\u03c32\n\u03c32 + \u03c42\nf2\u03c0(\u03c32 + \u03c42)\n\n\u00d77exp,\u2212(\u00b5 \u2212 \u03bd)2/[2(\u03c32 + \u03c42)]-\n\n,\n\n8 ,\n\nwhere \u03c6(x; \u03b1, \u03b22) denotes a univariate normal density function with mean \u03b1 and\nvariance \u03b22.\n\n10.5. availablefromthewebsiteforthisbookaresomemanifolddataexhibitingsomestrong\nstructure. specifically, these four-dimensional data come from a mixture distribution,\nwith a low weighting of a density that lies nearly on a three-dimensional manifold and\na high weighting of a heavy-tailed density that fills four-dimensional space.\na. estimate the direction of the least normal univariate projection of these data. use a\nsequence of graphs to guess a nonnormal projection direction, or follow the method\ndescribed for the projection step of exploratory projection pursuit.\n\nb. estimate the univariate density of the data projected in the direction found in\n\npart (a), using any means you wish.\n\nc. use the ideas in this chapter to estimate and/or describe the density of these data\n\nvia any productive means. discuss the difficulties you encounter.\n\n "}, {"Page_number": 366, "text": "p a r t iv\ndensity estimation\nand smoothing\n\nthere are three concepts that connect the remaining chapters in this book.\nfirst, the methods are generally nonparametric. the lack of a formal sta-\ntistical model introduces computing tasks beyond straightforward parameter\nestimation.\n\nsecond, the methods are generally intended for description rather than\nformal inference. we may wish to describe the probability distribution of a\nrandom variable or estimate the relationship between several random vari-\nables.\n\nthe most interesting questions in statistics ask how one thing depends\non another. the paragon of all statistical strategies for addressing this question\nis the concept of regression (with all its forms, generalizations, and analogs),\nwhich describes how the conditional distribution of some variables depends\non the value of other variables.\n\nthestandardregressionapproachisparametric:oneassumesanexplicit,\nparameterized functional relationship between variables and then estimates\nthe parameters using the data. this philosophy embraces the rigid assumption\nof a prespecified form for the regression function and in exchange enjoys the\npotential benefits of simplicity. typically, all the data contribute to parameter\nestimation and hence to the global fit. the opposite trade-off is possible,\nhowever. we can reject the parametric assumptions in order to express the\nrelationshipmoreflexibly,buttheestimatedrelationshipcanbemorecomplex.\ngenerally, we will call these approaches smoothing, and this brings us\nto the third theme of the chapters ahead: the methods are usually based on the\nconcept of local averaging. within small neighborhoods of predictor space\n\ncomputational statistics, second edition. geof h. givens and jennifer a. hoeting.\n\u00a9 2013 john wiley & sons, inc. published 2013 by john wiley & sons, inc.\n\n323\n\n "}, {"Page_number": 367, "text": "part iv density estimation and smoothing\n\n324\na summary statistic (e.g., the mean) of the values of the response variable(s)\nwithin that neighborhood is used to describe the relationship. we will see that\nthelocalaveragingconceptisalsoimplicitinourchapterondensityestimation\nthat begins this part of the book. nonparametric density estimation is useful\nbecause for most real-world problems an appropriate parametric form for the\ndensity is either unknown or doesn\u2019t exist.\n\nthus, the primary focus of the remaining chapters is on nonparametric\nmethods to describe and estimate densities or relationships using the philos-\nophy of local averaging. along the way, we detour into some related topics\nthat extend these concepts in interesting alternative directions.\n\n "}, {"Page_number": 368, "text": "chapter 11\nbivariate smoothing\n\nconsiderthebivariatedatashowninfigure11.1.ifasked,virtuallyanyonecoulddraw\na smooth curve that fits the data well, yet most would find it surprisingly difficult to\ndescribe precisely how they had done it. we focus here on a variety of methods for\nthis task, called scatterplot smoothing.\neffective smoothing methods for bivariate data are usually much simpler than\nfor higher-dimensional problems; therefore we initially limit consideration to the\ncase of n bivariate data points (xi, yi), i = 1, . . . , n. chapter 12 covers smoothing\nmultivariate data.\nthe goal of smoothing is different for predictor\u2013response data than for general\nbivariate data. with predictor\u2013response data, the random response variable y is as-\nsumed to be a function (probably stochastic) of the value of a predictor variable x. for\nexample, a model commonly assumed for predictor\u2013response data is yi = s (xi) + \u03f5i,\nwhere the \u03f5i are zero-mean stochastic noise and s is a smooth function. in this case,\nthe conditional distribution of y|x describes how y depends on x = x. one sensible\nsmooth curve through the data would connect the conditional means of y|x for the\nrange of predictor values observed.\nin contrast to predictor\u2013response data, general bivariate data have the charac-\nteristic that neither x or y is distinguished as the response. in this case, it is sensible\nto summarize the joint distribution of (x, y). one smooth curve that would capture\na primary aspect of the relationship between x and y would correspond to the ridge\ntop of their joint density; there are other reasonable choices, too. estimating such re-\nlationships can be considerably more challenging than smoothing predictor\u2013response\ndata; see sections 11.6 and 12.2.1.\ndetailed discussion of smoothing techniques includes [101, 188, 308, 309, 314,\n322, 573, 599, 642, 651].\n\n11.1 predictor\u2013response data\nsuppose that e{y|x} = s(x) for a smooth function s. because smoothing predictor\u2013\nresponse data usually focuses on estimation of the conditional mean function s,\nsmoothing is often called nonparametric regression.\nfor a given point x, let \u02c6s(x) be an estimator of s(x). what estimator is best?\none natural approach is to assess the quality of \u02c6s(x) as an estimator of s(x) at x using\nthe mean squared error (of estimation) at x, namely mse(\u02c6s(x)) = e{[\u02c6s(x) \u2212 s(x)]2},\ncomputational statistics, second edition. geof h. givens and jennifer a. hoeting.\n\u00a9 2013 john wiley & sons, inc. published 2013 by john wiley & sons, inc.\n\n363\n\n "}, {"Page_number": 369, "text": "364\n\nchapter 11 bivariate smoothing\n\n6\n\n3\n\n0\n\n\u22123\n\n\u22126\n\ne\ns\nn\no\np\ns\ne\nr\n\n\u22123\n\n\u22122\n\n\u22121\n\n0\npredictor\n\n1\n\n2\n\n3\n\n+ var{\u02c6s(x)} ,\n\nfigure 11.1 predictor\u2013response data. a smooth curve sketched through these data would\nlikely exhibit several peaks and troughs.\nwhere the expectation is taken with respect to the joint distribution of the responses.\nby adding and subtracting e{\u02c6s(x)|x} inside the squared term in this expression, it is\nstraightforward to obtain the familiar result that\nmse(\u02c6s(x)) = (bias{\u02c6s(x)})2\n(11.1)\n\nwhere bias{\u02c6s(x)} = e{\u02c6s(x)} \u2212 s(x).\nalthough we motivate smoothing by considering estimation of conditional\nmeans under squared error loss, alternative viewpoints are reasonable. for exam-\nple, using absolute error loss shifts focus to the median{y|x}. thus, smoothing may\nbe seen more generally as an attempt to describe how the center of the distribution of\ny|x varies with x, for some notion of what constitutes the center.\nthe smoother \u02c6s(x) is usually based not only on the observed data (xi, yi), for\ni = 1, . . . , n, but also on a user-specified smoothing parameter \u03bb, whose value is\nchosen to control the overall behavior of the smoother. thus, we often write \u02c6s\u03bb and\nmse\u03bb(\u02c6s\u03bb(x)) hereafter.\nconsider prediction of the response at a new point x\u2217, using the smoother \u02c6s\u03bb.\nwe introduced mse\u03bb(\u02c6s\u03bb(x\u2217)) to assess the quality of \u02c6s\u03bb(x\u2217) as an estimator of the true\nconditional mean, s(x\u2217) = e{y|x = x\u2217}. now, to assess the quality of the smoother\nas a predictor of a single response at x = x\u2217, we use the mean squared prediction\nerror (mspe) at x\u2217, namely\n\nmspe\u03bb(\u02c6s\u03bb(x\u2217)) = e!(y \u2212 \u02c6s\u03bb(x\u2217))2\"\" x = x\u2217#\n= var{y|x = x\u2217} + mse\u03bb(\u02c6s\u03bb(x\u2217)).\n\n(11.2)\nmore should be required of \u02c6s\u03bb beyond good prediction at a single x\u2217. if \u02c6s\u03bb is a\ngood smoother, it should limit mspe\u03bb(\u02c6s\u03bb(x)) over a range of x. for the observed\ndataset, a good global measure of the quality of \u02c6s\u03bb = (\u02c6s\u03bb(x1), . . . , \u02c6s\u03bb(xn)) would\nbe mspe\u03bb(\u02c6s\u03bb) = (1/n)$n\ni=1 mspe\u03bb(\u02c6s\u03bb(xi)), namely the average mean squared\n\n "}, {"Page_number": 370, "text": "11.2 linear smoothers\n\n365\nprediction error. there are other good global measures of the quality of a smooth, but\nin many cases the choice is asymptotically unimportant in the sense that they provide\nequivalent asymptotic guidance about optimal smoothing [313].\nhaving discussed theoretical measures of performance of smoothers, we now\nturn our focus to practical methods for constructing smoothers that perform well. for\npredictor\u2013response data, it\u2019s difficult to resist the notion that a smoother should sum-\nmarize the conditional distribution of yi given xi = xi by some measure of location\nlike the conditional mean, even if the model yi = s (xi) + \u03f5i is not assumed explicitly.\nin fact, regardless of the type of data, nearly all smoothers rely on the concept of local\naveraging. the yi whose corresponding xi are near x should be averaged in some way\nto glean information about the appropriate value of the smooth at x.\n\na generic local-averaging smoother can be written as\n\n\u02c6s(x) = ave{yi|xi \u2208 n(x)}\n\n(11.3)\nfor some generalized average function \u201cave\u201d and some neighborhood of x, say n(x).\ndifferent smoothers result from different choices for the averaging function (e.g.,\nmean, weighted mean, median, or m-estimate) and the neighborhood (e.g., the nearest\nfew neighboring points, or all points within some distance). in general, the form of\nn(x) may vary with x so that different neighborhood sizes or shapes may be used in\ndifferent regions of the dataset.\nthe most important characteristic of a neighborhood is its span, which is repre-\nsented by the smoothing parameter \u03bb. in a general sense, the span of a neighborhood\nmeasures its inclusiveness: neighborhoods with small span are strongly local, in-\ncluding only very nearby points, whereas neighborhoods with large span have wider\nmembership.therearemanywaystomeasureaneighborhood\u2019sinclusiveness,includ-\ning its size (number of points), span (proportion of sample points that are members),\nbandwidth (physical length or volume of the neighborhood), and other concepts dis-\ncussed later. we use \u03bb to denote whichever concept is most natural for each smoother.\nthe smoothing parameter controls the wiggliness of \u02c6s\u03bb. smoothers with small\nspans tend to reproduce local patterns very well but draw little information from more\ndistant data. a smoother that ignores distant data containing useful information about\nthe local response will have higher variability than could otherwise be achieved. in\ncontrast, smoothers with large spans draw lots of information from distant data when\nmaking local predictions. when these data are of questionable relevance, potential\nbias is introduced. adjusting \u03bb controls this trade-off between bias and variance.\nbelowweintroducesomestrategiesforconstructinglocal-averagingsmoothers.\nthis chapter focuses on smoothing methods for predictor\u2013response data, but\nsection 11.6 briefly addresses issues regarding smoothing general bivariate data,\nwhich are further considered in chapter 12.\n\n11.2 linear smoothers\nanimportantclassofsmoothersarethelinearsmoothers.forsuchsmoothers,thepre-\ndiction at any point x is a linear combination of the response values. linear smoothers\nare faster to compute and easier to analyze than nonlinear smoothers.\n\n "}, {"Page_number": 371, "text": "366\n\nchapter 11 bivariate smoothing\nfrequently, it suffices to consider estimation of the smooth at only the observed\nxi points.foravectorofpredictorvalues,x = (x1 . . . xn)t,denotethevectorofcorre-\nsponding response variables as y = (y1 . . . yn)t, and define \u02c6s = (\u02c6s (x1) . . . \u02c6s (xn))t.\nthen a linear smoother can be expressed as \u02c6s = sy for an n \u00d7 n smoothing matrix\ns whose entries do not depend on y. a variety of linear smoothers are introduced\nbelow.\n\n11.2.1 constant-span running mean\na very simple smoother takes the sample mean of k nearby points:\n\n\u02c6sk(xi) = %{j: xj\u2208n(xi)}\n\nyj\nk\n\n.\n\n(11.4)\n\ninsisting on odd k, we define n(xi) as xi itself, the (k \u2212 1)/2 points whose predictor\nvalues are nearest below xi, and the (k \u2212 1)/2 points whose predictor values are\nnearest above xi. this n(xi) is termed the symmetric nearest neighborhood, and the\nsmoother is sometimes called a moving average.\nwithout loss of generality, assume hereafter that the data pairs have been sorted\nso that the xi are in increasing order. then the constant-span running-mean smoother\ncan be written as\n\n.\n\nk\n\nk\n\nyi+(k+1)/2\n\nyi\u2212(k\u22121)/2\n\n\u02c6sk(xi+1) = \u02c6sk(xi) \u2212\n\nk \u2212 1\n2 , n().\n\nk \u2212 1\n2 ,1( \u2264 j \u2264 min\u2019i +\n\n\u02c6sk(xi) = mean&yj for max\u2019i \u2212\n(11.5)\nfor the purposes of graphing or prediction, one can compute \u02c6s at each of the xi and\ninterpolate linearly in between. note that by stepping through i in order, we can\nefficiently compute \u02c6sk at xi+1 with the recursive update\n+\n\n(11.6)\nthisavoidsrecalculatingthemeanateachpoint.ananalogousupdateholdsforpoints\nwhose predictor values lie near the edges of the data.\nthe constant-span running-mean smoother is a linear smoother. the middle\n\nk 0 . . . 0(. an important\nrows of the smoothing matrix s resemble\u20190 . . . 0 1\ndetail in most smoothing problems is how to compute \u02c6sk(xi) near the edges of the\ndata. for example, x1 does not have (k \u2212 1)/2 neighbors to its left. some adjustment\nmust be made to the top and bottom (k \u2212 1)/2 rows of s. three possible choices (e.g.,\nfor k = 5) are to shrink symmetric neighborhoods by using\n\u00b7\u00b7\u00b7 0\n\u00b7\u00b7\u00b7 0\n\u00b7\u00b7\u00b7 0\n\u00b7\u00b7\u00b7 0\n...\n\nk . . . 1\n\n1\n1\n3\n1\n5\n0\n...\n\n0\n0\n1\n5\n1\n5\n...\n\n0\n0\n1\n5\n1\n5\n...\n\n0\n0\n0\n1\n5\n...\n\n0\n1\n3\n1\n5\n1\n5\n...\n\n0\n1\n3\n1\n5\n1\n5\n...\n\n(11.7)\n\ns =\n\n;\n\n\u239e\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0\n\n\u239b\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d\n\n "}, {"Page_number": 372, "text": "6\n\n3\n\n0\n\n\u22123\n\n\u22126\n\ne\ns\nn\no\np\ns\ne\nr\n\n\u22123\n\n\u22122\n\n\u22121\n\n11.2 linear smoothers\n\n367\n\n1\n\n2\n\n3\n\n0\npredictor\n\nfigure11.2 resultsfromaconstant-spanrunning-meansmootherwith k = 13(solidline),\ncompared with the true underlying curve (dotted line).\n\nto truncate neighborhoods by using\n1\n3\n1\n4\n1\n5\n1\n5\n...\n\n1\n3\n1\n4\n1\n5\n0\n...\n\ns =\n\n\u239b\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d\n\n1\n3\n1\n4\n1\n5\n1\n5\n...\n\n0\n1\n4\n1\n5\n1\n5\n...\n\n0\n0\n1\n5\n1\n5\n...\n\n0\n0\n0\n1\n5\n...\n\n\u00b7\u00b7\u00b7 0\n\u00b7\u00b7\u00b7 0\n\u00b7\u00b7\u00b7 0\n\u00b7\u00b7\u00b7 0\n...\n\n\u239e\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0\n\nor\u2014in the case of circular data only\u2014to wrap neighborhoods by using\n\n;\n\n(11.8)\n\n1\n5\n1\n5\n1\n5\n0\n...\n\n1\n5\n1\n5\n1\n5\n1\n5\n...\n\n1\n5\n1\n5\n1\n5\n1\n5\n...\n\n0\n1\n5\n1\n5\n1\n5\n...\n\n0\n0\n1\n5\n1\n5\n...\n\n0\n0\n0\n1\n5\n...\n\n\u00b7\u00b7\u00b7\n0\n0\n0\n...\n\n0\n\u00b7\u00b7\u00b7\n0\n0\n...\n\n1\n5\n0\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n\n1\n5\n1\n5\n0\n0\n...\n\ns =\n\n\u239b\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d\n\n\u239e\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0\n\n.\n\n(11.9)\n\nthe truncation option is usually preferred, and is implicit in (11.5). since k is intended\nto be a rather small fraction of n, the overall picture presented by the smooth is not\ngreatly affected by the treatment of the edges, but regardless of how this detail is\naddressed, readers should be aware of the reduced reliability of \u02c6s at the edges of the\ndata.\nexample 11.1 (easy data)\nfigure 11.2 shows a constant-span running-mean\nsmoothofthedataintroducedatthestartofthischapter,whichareeasytosmoothwell\n\n "}, {"Page_number": 373, "text": "chapter 11 bivariate smoothing\n\n368\nusing a variety of methods we will discuss. these data are n = 200 equally spaced\npoints from the model yi = s (xi) + \u03f5i, where the errors are mean-zero i.i.d. normal\nnoise with a standard deviation of 1.5. the data are available from the website for this\nbook. the true relationship, s(x) = x3 sin{(x + 3.4)/2}, is shown with a dotted line;\nthe estimate \u02c6sk(x) is shown with the solid line. we used a smoothing matrix equivalent\nto (11.8) for k = 13. the result is not visually appealing: perhaps this emphasizes the\nsurprising sophistication of whatever methods people employ when they sketch in a\nsmooth curve by hand.\n!\neffect of span a natural smoothing parameter for the constant-span\n11.2.1.1\nrunning-mean smoother is \u03bb = k. as for all smoothers, this parameter controls wig-\ngliness, here by directly controlling the number of data points contained in any neigh-\nborhood. for sorted data and an interior point xi whose neighborhood is not affected\nby the data edges, the span-k running-mean smoother given by (11.5) has\n\nmsek(\u02c6sk(xi)) = e01s (xi) \u2212\n\nwhere, recall, s (xi) = e{y|x = xi}. it is straightforward to reexpress this as\n\nmsek(\u02c6sk(xi)) =4bias{\u02c6sk(xi)}52\n\nwhere\n\n1\nk2\n\n+\n\nbias{\u02c6sk(xi)} = s (xi) \u2212\n\nyj223,\n\n1\nk\n\ni+(k\u22121)/2%j=i\u2212(k\u22121)/2\ni+(k\u22121)/2%j=i\u2212(k\u22121)/2\ni+(k\u22121)/2%j=i\u2212(k\u22121)/2\n\n1\nk\n\ns (xj).\n\nvar{y|x = xj},\n\nto understand how the mean squared prediction error depends on the smoothing span,\nwe can use (11.11) and make the simplifying assumption that var{y|x = xj} = \u03c32\nfor all xj \u2208 n(xi). then\n\nmspek(\u02c6sk(xi)) = var{y|x = xi} + msek(\u02c6sk(xi))\n+4bias{\u02c6sk(xi)}52\n\n=11 +\n\nk2 \u03c32\n\n1\n\n.\n\ntherefore, as the neighborhood size k is increased, the variance term in (11.13) de-\ncreases,butthebiastermwilltypicallyincreasebecause s (xi)willnotlikelybesimilar\nto s(xj) for distant j. likewise, if k is decreased, the variance term will increase, but\nthe bias term will usually be smaller.\nexample 11.2 (easy data, continued) figure 11.3 illustrates how k influences\n\u02c6sk. in this graph, k = 3 leads to a result that is far too wiggly. in contrast, k = 43\nleads to a result that is quite smooth but systematically biased. the bias arises when\na neighborhood is so wide that the response values at the fringes of the neighborhood\n\n(11.10)\n\n(11.11)\n\n(11.12)\n\n(11.13)\n\n "}, {"Page_number": 374, "text": "6\n\n3\n\n0\n\n\u22123\n\n\u22126\n\ne\ns\nn\no\np\ns\ne\nr\n\n\u22123\n\n\u22122\n\n\u22121\n\n11.2 linear smoothers\n\n369\n\n1\n\n2\n\n3\n\n0\npredictor\n\nfigure 11.3 results from a constant-span running-mean smoother with k = 3 (wigglier\nsolid line) and k = 43 (smoother solid line). the underlying true curve is shown with a\ndotted line.\n\nare not representative of the response at the middle. this tends to erode peaks, fill in\ntroughs, and flatten trends near the edges of the range of the predictor.\n!\nspan selection for linear smoothers the best choice for k clearly\n11.2.1.2\nmust balance a trade-off between bias and variance. for small k, the estimated\ncurve will be wiggly but exhibit more fidelity to the data. for large k, the estimated\ncurve will be smooth but exhibit substantial bias in some regions. for all smoothers,\nthe role of the smoothing parameter is to control this trade-off between bias and\nvariance.\nan expression for mspek(\u02c6sk) can be obtained by averaging values from (11.13)\nover all xi, but this expression cannot be minimized to choose k because it depends\non unknown expected values. furthermore, it may be more reasonable to choose the\nspan that is best for the observed data, rather than the span that is best on average\nfor datasets that might have been observed but weren\u2019t. therefore, we might consider\nchoosing the k that minimizes the residual mean squared error (rss)\n\nn\n\n=\n\n1\nn\n\nrssk(\u02c6sk)\n\nn%i=1\n3 = mspek(\u02c6sk) \u2212\n\nhowever,\n\ne0rssk(\u02c6sk)\n\nn\n\n(yi \u2212 \u02c6sk(xi))2 .\n\n1\n\nn%i /= j\n\ncov{yi, \u02c6sk(xj)}.\n\n(11.14)\n\n(11.15)\n\nfor constant-span running means, cov{yi, \u02c6sk(xj)} = var{y|x = xj}/k for interior xj.\ntherefore, rssk(\u02c6sk)/n is a downward-biased estimator of mspek(\u02c6sk).\n\n "}, {"Page_number": 375, "text": "370\n\nchapter 11 bivariate smoothing\n\n)\nk\n\u02c6s\n(\nk\ns\ns\nr\nv\nc\n\n10\n\n20\n\n30\n\n40\n\n50\n\nfigure 11.4 plot of cvrssk(\u02c6sk) versus k for the constant-span running-mean smoother\napplied to the data in figure 11.1. good choices for k range between about 11 and 23. the\nsmaller values in this range would be especially good at bias reduction, whereas the larger ones\nwould produce smoother fits.\n\nk\n\nto eliminate the correlation between yi and \u02c6sk(xi), we may omit the ith point\nwhen calculating the smooth at xi. this process is known as cross-validation [616];\nit is used only for assessing the performance of the smooth, not for fitting the\nsmooth itself. denote by \u02c6s\n(xi) the value of the smooth at xi when it is fitted using\nthe dataset that omits the ith data pair. a better (indeed, pessimistic) estimator of\nmspek(\u02c6sk) is\n\n(-i)\n\nk\n\ncvrssk(\u02c6sk)\n\nn\n\n1\nn\n\n=\n\nn%i=1\u2019yi \u2212 \u02c6s\n\n(-i)\n\nk\n\n(xi)(2\n\n,\n\n(11.16)\n\nwhere cvrssk(\u02c6sk) is called the cross-validated residual sum of squares. typically,\ncvrssk(\u02c6sk) is plotted against k.\nexample 11.3 (easy data, continued) figure 11.4 shows a plot of cvrssk(\u02c6sk)\nagainst k for smoothing the data introduced in example 11.1. this plot usually shows\na steep increase in cvrssk(\u02c6sk) for small k due to increasing variance, and a grad-\nual increase in cvrssk(\u02c6sk) for large k due to increasing bias. the region of best\nperformance is where the curve is lowest; this region is often quite broad and rather\nflat. in this example, good choices of k range between 11 and 23, with k = 13 being\noptimal. minimizing cvrssk(\u02c6sk) with respect to k often produces a final smooth\nthat is somewhat too wiggly. undersmoothing can be reduced by choosing a larger k\nwithin the low cvrssk(\u02c6sk) valley in the cross-validation plot, which corresponds to\ngood performance. in this example, k = 23 would be worth trying.\n!\n\n "}, {"Page_number": 376, "text": "11.2 linear smoothers\n\n371\nthis approach of leave-one-out cross-validation is time consuming, even for\nlinear smoothers, since it seems to require computing n separate smooths of slightly\ndifferent datasets. two shortcuts are worth mentioning.\nfirst, consider a linear smoother with smoothing matrix s. the proper fit at xi\nwhen the ith data pair is omitted from the dataset is a somewhat imprecise concept,\neven for a constant-span running-mean smoother, because smooths are typically cal-\nculated only at the xi values in the dataset. should smooths be fitted at the two data\npoints adjacent to the omitted xi, with linear interpolation used in between, or should\nsome other approach be tried? the most unambiguous way to proceed is to define\n\n(-i)\n\u02c6s\n\nk\n\n(xi) =\n\nyjsij\n\n1 \u2212 sii\n\n,\n\nn%j=1\n\nj /= i\n\n(11.17)\n\nwhere sij is the (i, j)th element of s. in other words, the ith row of s is altered by\nreplacing the (i, j)th element of s with zero and rescaling the remainder of the row so\nit sums to 1. in this case, to compute cvrssk(\u02c6sk) there is no need to actually delete\nthe ith observation and recompute the smooth for each i. following from (11.17), it\ncan be shown that for linear smoothers, (11.16) can be reexpressed as\n\ncvrssk(\u02c6sk)\n\nn\n\n1\nn\n\n=\n\n1 \u2212 sii 22\nn%i=11 yi \u2212 \u02c6sk(xi)\n\n.\n\n(11.18)\n\nthis approach is analogous to the well-known shortcut for calculating deleted resid-\nuals in linear regression [483] and is further justified in [322].\nsecond, one may wish to reduce the number of cross-validation computations\nby generating fewer partial datasets, each with a greater number of points omitted.\nfor example, one could randomly partition the observed dataset into 10 portions, then\nleave out one portion at a time. the cross-validated residual sum of squares would\nthen be accumulated from the residuals of the points omitted in each portion. this\napproach tends to overestimate the true prediction error, while leaving-one-out is less\nbiased but more variable; 5- or 10-fold cross-validation (i.e., 5\u201310 portions) has been\nrecommended [323].\nwe mentioned above that different smoothers employ different smoothing pa-\nrameters to control wiggliness. so far, we have focused on the number (k) or fraction\n(k/n) of nearest neighbors. another reasonable choice, n(x) = {xi : |xi \u2212 x| < h},\nuses the positive real-valued distance h as a smoothing parameter. there are also\nschemes for weighting points based on their proximity to x, in which case the smooth-\ning parameter may relate to these weights. usually, the number of points in a neigh-\nborhoodissmallerneartheboundariesofthedata, meaningthatany fixedspanchosen\nby cross-validation or another method may provide a poorer fit near the boundaries\nthan in the middle of the data. the span may also be allowed to vary locally. for such\nalternative parameterizations of neighborhoods, plotting the cross-validated residual\nsum of squares and drawing conclusions about the bias\u2013variance trade-off proceed in\na fashion analogous to the preceding discussion.\n\n "}, {"Page_number": 377, "text": "372\n\nchapter 11 bivariate smoothing\ncross-validated span selection is not limited to the constant-span running-mean\nsmoother. the same strategy is effective for most other smoothers discussed in this\nchapter. the trade-off between bias and variance is a fundamental principle in many\nareas of statistics: it arose previously for density estimation (chapter 10), and it is\ncertainly a major consideration for all types of smoothing.\nthere are a wide variety of other methods for choosing the span for a scatterplot\nsmoother, resulting in different bias\u2013variance trade-offs [309, 310, 314, 322, 323].\none straightforward approach is to replace cvrss with another criterion such as cp,\naic, or bic [323]. two other popular alternatives are generalized cross-validation\n(gcvrss) and plug-in methods [311, 564, 599]. in generalized cross-validation,\n(11.16) is replaced by\n\ngcvrssk(\u02c6sk) =\n\nrssk(\u02c6sk)\n(1 \u2212 tr{s}/n)2 ,\n\n(11.19)\n\nwhere tr{s} denotes the sum of the diagonal elements of s. for equally spaced xi,\ncvrss and gcvrss give similar results. when the data are not equally spaced, span\nselection based on gcvrss is less affected by observations that exert strong influ-\nence on the fit. notwithstanding this potential benefit of generalized cross-validation,\nreliance on gcvrss often results in significant undersmoothing. plug-in methods\ngenerally derive an expression for the expected mean squared prediction error or some\nother fitting criterion, whose theoretical minimum is found to depend on the type of\nsmoother, the wiggliness of the true curve, and the conditional variance of y|x. a pre-\nliminary smooth is completed using a span chosen informally (or by cross-validation).\nthen this smooth is used to estimate the unknown quantities in the expression for the\noptimal span, and the result is used in a final smooth.\nit is tempting to select the span selection method that yields the picture most\npleasingtoyoureye.thatisfine,butitisworthwhileadmittingupfrontthatscatterplot\nsmoothing is often an exercise in descriptive\u2014not inferential\u2014statistics, so selecting\nyour favorite span from trial and error or a simple plot of cvrss is as reasonable\nas the opportunistic favoring of any technical method. since spans chosen by cross-\nvalidation vary with the random dataset observed and sometimes undersmooth, it is\nimportant for practitioners to develop their own expertise based on hands-on analysis\nand experience.\n\n11.2.2 running lines and running polynomials\nthe constant-span running-mean smoother exhibits visually unappealing wiggliness\nfor any reasonable k. it also can have strong bias at the edges because it fails to\nrecognize the local trend in the data. the running-line smoother can mitigate both\nproblems.\nconsider fitting a linear regression model to the k data points in n(xi). then\nthe least squares linear regression prediction at x is\n\n\u2113i(x) = y i + \u02c6\u03b2i(x \u2212 \u00afxi),\n\n(11.20)\n\n "}, {"Page_number": 378, "text": "6\n\n3\n\n0\n\n\u22123\n\n\u22126\n\ne\ns\nn\no\np\ns\ne\nr\n\n\u22123\n\n\u22122\n\n\u22121\n\n11.2 linear smoothers\n\n373\n\n1\n\n2\n\n3\n\n0\npredictor\n\ni xi5\u22121xt\n\nfigure 11.5 plot of the running-line smooth for k = 23 (solid line) and the true underlying\ncurve (dotted line).\nwhere y i, \u00afxi,and \u02c6\u03b2i arethemeanresponse,themeanpredictor,andtheestimatedslope\nof the regression line, respectively, for the data in n(xi). the running-line smooth at\nxi is \u02c6sk(xi) = \u2113i(xi).\nlet xi = (1 xi), where 1 is a column of ones and xi is the column vector of\npredictor data in n(xi); and let yi be the corresponding column vector of response\ndata. then note that \u2113i(xi)\u2014and hence the smooth at xi\u2014is obtained by multiplying\nyi by one row of hi = xi4xt\ni . (usually hi is called the ith hat matrix.)\ntherefore, this smoother is linear, with a banded smoothing matrix s whose nonzero\nentries are drawn from an appropriate row of each hi. computing the smooth directly\nfromsisnotveryefficient.fordataorderedby xi,itisfastertosequentiallyupdatethe\nsufficient statistics for regression, analogously to the approach discussed for running\nmeans.\nexample 11.4 (easy data, continued) figure 11.5 shows a running-line smooth\nofthedataintroducedinexample11.1,forthespan k = 23chosenbycross-validation.\ntheedgeeffectsaremuchsmallerandthesmoothislessjaggedthanwiththeconstant-\nspan running-mean smoother. since the true curve is usually well approximated by a\nlineevenforfairlywideneighborhoods, k maybeincreasedfromtheoptimalvaluefor\nthe constant-span running-mean smoother. this reduces variance without seriously\nincreasing bias.\n!\nnothing in this discussion limits the local fitting to simple linear regression.\na running polynomial smoother could be produced by setting \u02c6sk(xi) to the value at\nxi of a least squares polynomial regression fit to the data in n(xi). such smoothers\nare sometimes called local regression smoothers (see section 11.2.4). odd-order\npolynomials are preferred [192, 599]. since smooth functions are roughly locally\n\n "}, {"Page_number": 379, "text": "chapter 11 bivariate smoothing\n\n374\nlinear, higher-order local polynomial regression often offers scant advantage over the\nsimpler linear fits unless the true curve has very sharp wiggles.\n\nyi\n\n(11.21)\n\nn%i=1\n\n\u02c6sh(x) =\n\n11.2.3 kernel smoothers\nforthesmoothersmentionedsofar,thereisadiscontinuouschangetothefiteachtime\nthe neighborhood membership changes. therefore, they tend to fit well statistically\nbut exhibit visually unappealing jitters or wiggles.\none approach to increasing smoothness is to redefine the neighborhood so that\npoints only gradually gain or lose membership in it. let k be a symmetric kernel\ncentered at 0. a kernel is essentially a weighting function\u2014in this case it weights\nneighborhood membership. one reasonable kernel choice would be the standard\nnormal density, k(z) = (1/\u221a2\u03c0)exp{\u2212z2/2}. then let\nk((x \u2212 xi)/ h)\n$n\nj=1 k((x \u2212 xj)/ h) ,\n\nwhere the smoothing parameter h is called the bandwidth. notice that for many\ncommon kernels such as the normal kernel, all data points are used to calculate the\nsmooth at each point, but very distant data points receive very little weight. proximity\nincreasesapoint\u2019sinfluenceonthelocalfit;inthissensetheconceptoflocalaveraging\nremains. a large bandwidth yields a quite smooth result because the weightings of the\ndata points change little across the range of the smooth. a small bandwidth ensures\na much greater dominance of nearby points, thus producing more wiggles.\nthe choice of smoothing kernel is much less important than the choice of\nbandwidth. a similar smooth will be produced from diverse kernel shapes. although\nkernels need not be densities, a smooth, symmetric, nonnegative function with tails\ntending continuously toward zero is generally best in practice. thus, there are few\nreasons to look beyond a normal kernel, despite a variety of asymptotic arguments\nsupporting more exotic choices.\nkernel smoothers are clearly linear smoothers. however, the computation of\nthe smooth cannot be sequentially updated in the manner of the previous efficient\napproachesbecausetheweightsforallpointschangeeachtime xchanges.fastfourier\ntransform methods are helpful in the special case of equally spaced data [307, 596].\nfurther background on kernel smoothing is given by [573, 581, 599, 651].\nexample 11.5 (easy data, continued) figure 11.6 shows a kernel smooth of the\ndata introduced in example 11.1, using a normal kernel with h = 0.16 chosen by\ncross-validation. since neighborhood entries and exits are gradual, the result exhibits\ncharacteristically rounded features. however, note that the kernel smoother does not\neliminate systematic bias at the edges, as the running-line smoother does.\n!\n\nlocal regression smoothing\n\n11.2.4\nrunning polynomial smoothers and kernel smoothers share some important links\n[10, 308, 599]. suppose that the data originated from a random design, so they are a\n\n "}, {"Page_number": 380, "text": "6\n\n3\n\n0\n\n\u22123\n\n\u22126\n\ne\ns\nn\no\np\ns\ne\nr\n\n\u22123\n\n\u22122\n\n\u22121\n\n11.2 linear smoothers\n\n375\n\n1\n\n2\n\n3\n\n0\npredictor\n\nfigure 11.6 plot of kernel smooth using a normal kernel with h = 0.16 chosen by cross-\nvalidation (solid line) and the true underlying curve (dotted line).\n\nrandom sample from the model (xi, yi) \u223c i.i.d. f(x, y). (a nonrandom design would\nhave prespecified the xi values.) we may write\n\ns (x) = e{y|x} =6 yf(y|x) dy =6 y\n\n(11.22)\nwhere, marginally, x \u223c f(x). using the kernel density estimation approach described\nin chapter 10 [and a product kernel for estimating f(x, y)], we may estimate\n\nf(x, y)\nf(x) dy,\n\n1\n\nnhxhy\n\nn%i=1\n\nkx1 x \u2212 xi\n\nhx 2ky1 y \u2212 yi\nhy 2\n\n7f(x, y) =\n\n(11.23)\n\nand\n\n1\nnhx\n\nn%i=1\n\nkx1 x \u2212 xi\nhx 2\n\n(11.24)\n\nfor suitable kernels kx and ky and corresponding bandwidths hx and hy. the\n\n7f(x) =\nnadaraya\u2013watson estimator [476, 655] of s(x) is obtained by substituting 7f(x, y)\nand 7f(x) in (11.22), yielding\n7shx(x) =\n\nkx((x \u2212 xi)/hx)\n$n\ni=1 kx((x \u2212 xi)/hx) .\n\nnote that this matches the form of a kernel smoother [see (11.21)].\n\nn%i=1\n\n(11.25)\n\nyi\n\n "}, {"Page_number": 381, "text": "376\n\nchapter 11 bivariate smoothing\nit is easy to show that the nadaraya\u2013watson estimator minimizes\n\n(yi \u2212 \u03b20)2kx1 x \u2212 xi\nhx 2\n\nn%i=1\n\n(11.26)\n\nwithrespectto \u03b20.thisisaleastsquaresproblemthatlocallyapproximates s (x)witha\nconstant. naturally, this locally constant model could be replaced with a local higher-\norder polynomial model. fitting a local polynomial using weighted regression, with\nweights set according to some kernel function, yields a locally weighted regression\nsmooth, often simply called a local regression smooth [118, 192, 651]. the pth-order\nlocal polynomial regression smoother minimizes the weighted least squares criterion\n\nn%i=18yi \u2212 \u03b20 \u2212 \u03b21(x \u2212 xi) \u2212 \u00b7\u00b7\u00b7 \u2212 \u03b2p(x \u2212 xi)p92\n\nkx1 x \u2212 xi\nhx 2\n\nand can be fitted using a weighted polynomial regression at each x, with weights\ndetermined by the kernel kx according to the proximity to x. this is still a linear\nsmoother, with a smoothing matrix composed of one row from the hat matrix used in\neach weighted polynomial regression.\nthe least squares criterion may be replaced by other choices. see section 11.4.1\nfor an extension of this technique that relies on a robust fitting method.\n\n(11.27)\n\n11.2.5 spline smoothing\nperhaps you have found the graphs of smooths presented so far in this chapter to be\nsomewhat unsatisfying visually because they are more wiggly than you would have\ndrawn by hand. they exhibit small-scale variations that your eye easily attributes to\nrandom noise rather than to signal. then smoothing splines may better suit your taste.\nassume that the data have been sorted in increasing order of the predictor, so\nx1 is the smallest predictor value and xn is the largest. define\n\nq\u03bb(\u02c6s) =\n\n(yi \u2212 \u02c6s (xi))2\n\nn%i=1\n\n+ \u03bb6 xn\n\nx1\n\n\u02c6s\u2032\u2032(x)2 dx,\n\n(11.28)\n\nwhere \u02c6s\u2032\u2032(x) is the second derivative of \u02c6s (x). then the summation constitutes a penalty\nfor misfit, and the integral is a penalty for wiggliness. the parameter \u03bb controls the\nrelative weighting of these two penalties.\nit is an exercise in the calculus of variations to minimize q\u03bb(\u02c6s) over all twice\ndifferentiablefunctions \u02c6sforfixed \u03bb.theresultisacubicsmoothingspline, \u02c6s\u03bb(x).this\nfunction is a cubic polynomial in each interval [xi, xi+1] for i = 1, . . . , n \u2212 1, with\nthese polynomial pieces pasted together twice continuously differentiably at each xi.\nalthough usually inadvisable in practice, smoothing splines can be defined on ranges\nextending beyond the edges of the data. in this case, the extrapolative portions of the\nsmooth are linear.\nit turns out that cubic splines are linear smoothers, so \u02c6s\u03bb = sy. this result is\npresented clearly in [322], and efficient computation methods are covered in [144,\n597]. other useful references about smoothing splines include [164, 188, 280, 649].\n\n "}, {"Page_number": 382, "text": "11.3 comparison of linear smoothers\n\n377\n\n6\n\n3\n\n0\n\n\u22123\n\n\u22126\n\ne\ns\nn\no\np\ns\ne\nr\n\n\u22123\n\n\u22122\n\n\u22121\n\n0\npredictor\n\n1\n\n2\n\n3\n\nfigure 11.7 plot of a cubic smoothing spline using \u03bb = 0.066 chosen by cross-validation\n(solid line) and the true underlying curve (dotted line).\n\nthe ith row of s consists of weights si1, . . . , sin, whose relationship to xi is\nsketched in figure 11.8 (discussed in section 11.3). such weights are reminiscent of\nkernel smoothing with a kernel that is not always positive, but in this case the kernel\ndoes not retain the same shape when centered at different points.\nexample 11.6 (easy data, continued) figure 11.7 shows a spline smooth of the\ndata introduced in example 11.1, using \u03bb = 0.066 chosen by cross-validation. the\nresult is a curve very similar to what you might have sketched by hand.\n!\n\n11.2.5.1 choice of penalty smoothing splines depend on a smoothing param-\neter \u03bb that relates to neighborhood size less directly than for smoothers we have\ndiscussed previously. we have already noted that \u03bb controls the bias\u2013variance trade-\noff, with large values of \u03bb favoring variance reduction and small values favoring bias\nreduction. as \u03bb \u2192 \u221e, \u02c6s\u03bb approaches the least squares line. when \u03bb = 0, \u02c6s\u03bb is an\ninterpolating spline that simply connects the data points.\nsince smoothing splines are linear smoothers, the span selection methods dis-\ncussed in section 11.2.1 still apply. calculating cvrss\u03bb(\u02c6s\u03bb) via (11.18) requires\nthe sii, which can be calculated efficiently using the method in [496]. calculating\ngcvrss\u03bb(\u02c6s\u03bb) requires tr{s}, which also can be calculated efficiently [145].\n\n11.3 comparison of linear smoothers\nalthough the smoothers described so far may seem very different, they all rely on\nthe principal of local averaging. the fit of each depends on a smoothing matrix s,\n\n "}, {"Page_number": 383, "text": "378\n\nchapter 11 bivariate smoothing\n\ncsrm\nrl\nlwr\nk\nss\n\nt\nh\ng\ni\ne\n\nw\n\n0\n\npredictor\n\nfigure 11.8 equivalent kernels for five different linear smoothing methods for which\ntr{s} = 7. the methods are constant-span running mean (csrm) with symmetric neighbor-\nhoods, running lines (rl) with symmetric neighborhoods, locally weighted regression (lwr),\ngaussian kernel smoothing (k), and cubic smoothing spline (ss). the smoothing weights for\nan interior point (indicated by the vertical line) correspond to the 36th row of s. the entire\ncollection of 105 values of xi is shown by hashes on the horizontal axis: they are equally\nspaced on each side, but twice as dense on the right.\n\nwhose rows determine the weights used in a local average of the response values.\ncomparison of a typical row of s for different smoothers is a useful approach to\nunderstanding the differences between techniques.\nof course, the weights in a typical row of s depend on the smoothing parameter.\nin general, values of \u03bb that favor greater smoothing will produce rows of s with a\nmore diffuse allocation of weight, rather than high weights concentrated in just a\nfew entries. therefore, to enable a fair comparison, it is necessary to find a common\nlink between the diverse smoothing parameters used by different techniques. the\ncommon basis for comparison is the number of equivalent degrees of freedom of the\nsmooth, which can most simply be defined as df = tr{s} for linear smoothers. several\nalternative definitions and extensions for nonlinear smoothers are discussed in [322].\nfor fixed degrees of freedom, the entries in a row of s are functions of the xi,\ntheirspacing,andtheirproximitytotheedgeofthedata.iftheweightsinarowofsare\nplotted against the predictor values, we may view the result as an equivalent kernel,\nwhose weights are analogous to the explicit weighting used in a kernel smoother.\nfigure 11.8 compares the equivalent kernel for various smoothers with 7 degrees of\nfreedom. the kernel is shown for the 36th of 105 ordered predictor values, of which\n35areequallyspacedtotheleftand69areequallyspacedtwiceasdenselytotheright.\nnotice that these kernels can be skewed, depending on the spacing of the xi. also,\nkernels need not be everywhere positive. in figure 11.8, the equivalent kernel for the\nsmoothing spline assigns negative weight in some regions. although not shown in\n\n "}, {"Page_number": 384, "text": "379\nthis figure, the shape of the kernels is markedly different for a point near the edge of\nthe data. for such a point, weights generally increase toward the edge and decrease\naway from it.\n\n11.4 nonlinear smoothers\n\n11.4 nonlinear smoothers\nnonlinear smoothers can be much slower to calculate, and in ordinary cases they\noffer little improvement over simpler approaches. however, the simpler methods can\nexhibit very poor performance for some types of data. the loess smoother provides\nimproved robustness to outliers that would introduce substantial noise in an ordinary\nsmoother. we also examine the supersmoother, which allows the smoothing span to\nvary as best suits the local needs of the smoother. such smoothing can be very helpful\nwhen var{y|x} varies with x.\n11.4.1\nthe loess smoother is a widely used method with good robustness properties [116,\n117]. it is essentially a weighted running-line smoother, except that each local line\nis fitted using a robust method rather than least squares. as a result, the smoother is\nnonlinear.\nloess is fitted iteratively; let t index the iteration number. to start at t = 0, we\nlet dk(xi) denote the distance from xi to its kth nearest neighbor, where k (or k/n) is\na smoothing parameter. the kernel used for the local weighting around point xi is\n\nloess\n\nwhere\n\nki(x) = k1 x \u2212 xi\ndk(xi)2 ,\n\nk(z) =:(1 \u2212 |z|3)3\n\n0\n\nfor |z| \u2264 1,\notherwise\n\n(11.29)\n\n(11.30)\n\nis the tricube kernel.\niteration t are found by minimizing the weighted sum of squares\n\nthe estimated parameters of the locally weighted regression for the ith point at\n\nn%j=1\u2019yj \u2212\u2019\u03b2\n\n(t)\n0,i + \u03b2\n\n(t)\n\n1,ixj((2\n\nki(xj).\n\n(11.31)\n\nwe denote these estimates as \u02c6\u03b2\n(t)\nm,i for m = 0,1 and i = 1, . . . , n. linear\u2014rather than\npolynomial\u2014regression is recommended, but the extension to polynomials would\nrequire only a straightforward change to (11.31). note that the fitted value for the\nresponse variable given by the local regression is \u02c6y\n(t)\n1,ixi. this completes\niteration t.\n\n0,i + \u02c6\u03b2\n\ni = \u02c6\u03b2\n\n(t)\n\n(t)\n\n "}, {"Page_number": 385, "text": "380\n\nchapter 11 bivariate smoothing\n\n6\n\n3\n\n0\n\n\u22123\n\n\u22126\n\ne\ns\nn\no\np\ns\ne\nr\n\n\u22123\n\n\u22122\n\n\u22121\n\n0\npredictor\n\n1\n\n2\n\n3\n\nfigure 11.9 plot of a loess smooth using k = 30 chosen by cross-validation (solid line)\nand the true underlying curve (dotted line).\n\nto prepare for the next iteration, observations are assigned new weights\nbased on the size of their residual, in order to downweight apparent outliers. if\ni = yi \u2212 \u02c6y\n(t)\ne\n\n(t)\ni\n\n, then define robustness weights as\n(t)\ne\ni\n\n(11.32)\n\n\u239e\u23a0 ,\n\nr\ni\n\n(t+1)\n\n= b\u239b\u239d\nb(z) =:(1 \u2212 z2)2\n\n0\n\n6 \u00d7 median \"\"\"e\n\n(t)\n\ni \"\"\"\n\nwhere b(z) is the biweight kernel given by\n\ni\n\n(t+1)\n\n(11.33)\n\nki(xj), and new locally\n. the\n\nfor |z| \u2264 1,\notherwise.\nthen the weights ki(xj) in (11.31) are replaced with r\nweighted fits are obtained. the resulting estimates for each i provide \u02c6y\nprocess stops after t = 3 by default [116, 117].\nexample 11.7 (easy data, continued) figure 11.9 shows a loess smooth of the\ndataintroducedinexample11.1,using k = 30chosenbycross-validation.theresults\nare very similar to the running-line smooth.\nfigure 11.10 shows the effect of outliers. the dotted line in each panel is\nthe original smooth for loess and running lines; the solid lines are the result when\nthree additional data points at (1, \u22128) are inserted in the dataset. the spans for each\nsmoother were left unchanged. loess was so robust to these outliers that the two\ncurves are nearly superimposed. the running-line smoother shows greater sensitivity\nto the outliers.\n!\n\n(t+1)\n\ni\n\n "}, {"Page_number": 386, "text": "11.4 nonlinear smoothers\n\n381\n\n6\n\n3\n\n0\n\n\u22123\n\n\u22126\n\ne\ns\nn\no\np\ns\ne\nr\n\n\u22123\n\n6\n\n3\n\n0\n\n\u22123\n\n\u22126\n\ne\ns\nn\no\np\ns\ne\nr\n\n3\n\n\u22123\n\n0\n\npredictor\n\n0\n\npredictor\n\n3\n\nfigure 11.10 plot of running-line smooths (left) using k = 23, and loess smooths (right)\nusing k = 30. in each panel, the dotted line is the smooth of the original data, and the solid line\nis the smooth after inserting in the dataset three new outlier points at (1,\u22128).\n\n11.4.2 supersmoother\nall of the previous methods employ a fixed span. there are cases, however, where a\nvariable span may be more appropriate.\nexample 11.8 (difficult data) consider the curve and data shown in figure 11.11.\nthese data are available from the website for this book. suppose that the true condi-\ntional mean function for these data is given by the curve; thus the goal of smoothing\nis to estimate the curve using the observed data. the curve is very wiggly on the right\nside of the figure, but these wiggles could reasonably be detected by a smoother with\na suitably small span because the variability in the data is very low. on the left, the\ncurve is very smooth, but the data have much larger variance. therefore, a large span\nwould be needed in this region to smooth the noisy data adequately. thus a small\nspan is needed to minimize bias in one region, and a large span is needed to control\nvariance in another region. the supersmoother [205, 208] was designed for this sort\nof problem.\n!\nthe supersmoothing approach begins with the calculation of m different\nsmooths, which we denote \u02c6s1(x), . . . , \u02c6sm(x), using different spans, say h1, . . . , hm.\nfor m = 3, spans of h1 = 0.05n, h2 = 0.2n, and h3 = 0.5n are recommended. each\nsmooth should be computed over the full range of the data. for simplicity, use the\nrunning-line smoother to generate the \u02c6sj(x) for j = 1,2, and 3. figure 11.12 shows\nthe three smooths.\n\n "}, {"Page_number": 387, "text": "382\n\nchapter 11 bivariate smoothing\n\ne\ns\nn\no\np\ns\ne\nr\n\n1\n\n0\n\n\u22121\n\n0\n\n0.25\n\n0.50\n\n0.75\n\n1\n\nfigure 11.11 these bivariate data with nonconstant variance and wiggles with changing\nfrequencyandamplitudewouldbefittedterriblybymostfixed-spansmoothers.thetrue e{y|x}\nis shown with the solid line.\n\npredictor\n\ne\ns\nn\no\np\ns\ne\nr\n\n1\n\n0\n\n\u22121\n\n0\n\n0.25\n\n0.50\npredictor\n\n0.75\n\n1\n\nfigure 11.12 the three preliminary fixed-span smooths employed by the supersmoother.\nthe spans are 0.05n (dashed), 0.2n (dotted), and 0.5n (solid). the data points have been faded\nto enable a clearer view of the smooths.\n\n "}, {"Page_number": 388, "text": "11.4 nonlinear smoothers\n\n383\n\n)\ni\n\nx\n\n,\nj\nh\n(\n\u02c6p\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0\n\n0\n\n0.25\n\n0.50\n\n0.75\n\n1\n\nfigure 11.13 the \u02c6p(hj, xi) for j = 1 (dashed), 2 (dotted), and 3 (solid). for each j, the\ncurve is a smooth of the absolute cross-validated residuals.\n\npredictor\n\n(i)\n\n(i)\n\n(i)\n\nj (xi)5( ,\n\nnext, define p(hj, x) to be a measure of performance of the jth smooth at\npoint x, for j = 1, . . . , m. ideally, we would like to assess the performance at point\nj (xi)5\"\"x = xi#, where g is a symmetric function that\nxi according to e!g4y \u2212 \u02c6s\n(i)\npenalizes large deviations, and \u02c6s\nj (xi) is the jth smooth at xi estimated using the\ncross-validation dataset that omits xi. this expected value is of course unknown, so,\nfollowing the local-averaging paradigm, we estimate it as\n\u02c6p(hj, xi) = \u02c6s\u2217\u2019g4yi \u2212 \u02c6s\n\n(11.34)\nwhere \u02c6s\u2217 is some fixed-span smoother. for the implementation suggested in [205],\n\u02c6s\u2217 = \u02c6s2 and g(z) = |z|. figure 11.13 shows the smoothed absolute cross-validated\nresiduals\"\"yi \u2212 \u02c6s\nj (xi)\"\" for the three different smooths. the curves in this figure rep-\nresent \u02c6p(hj, xi) for j = 1,2, and 3. the data used in each smooth originate as resid-\nuals from alternative smooths using spans of 0.05n (dashed), 0.2n (dotted), and 0.5n\n(solid), but each set of absolute residuals is smoothed with a span of 0.2n to generate\nthe curves shown.\nateach xi,theperformancesofthethreesmoothscanbeassessedusing \u02c6p(hj, xi)\nfor j = 1,2, and 3. denote by \u02c6hi the best of these spans at xi, that is, the particular\nspan among h1, h2, and h3 that provides the lowest \u02c6p(hj, xi). figure 11.14 plots \u02c6hi\nagainst xi for our example. the best span can vary abruptly even for adjacent xi, so\nnext the data in figure 11.14 are passed through a fixed-span smoother (say, \u02c6s2) to\nestimate the optimal span as a function of x. denote this smooth as \u02c6h(x). figure 11.14\nalso shows \u02c6h(x).\nnow we have the original data and a notion of the best span to use for any given\nx: namely \u02c6h(x). what remains is to create a final, overall smooth. among several\nstrategies that might be employed at this point, [205] recommends setting \u02c6s(xi) equal\n\n!\n "}, {"Page_number": 389, "text": "384\n\nchapter 11 bivariate smoothing\n\ni\n\n\u02c6h\n\n0.5\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0\n\n0\n\n0.25\n\n0.50\n\n0.75\n\n1\n\npredictor\n\nfigure 11.14 estimate of the optimal span as a function of x. the points correspond to\n(xi, \u02c6hi). a smooth of these points, namely \u02c6h(x), is shown with the curve.\nto a linear interpolation between \u02c6sh\u2212(xi)(xi) and \u02c6sh+(xi)(xi), where among the m fixed\nspans tried, h\u2212(xi) is the largest span less than \u02c6h(xi) and h+(xi) is the smallest span\nthat exceeds \u02c6h(xi). thus,\n\nh+(xi) \u2212 \u02c6h(xi)\nh+(xi) \u2212 h\u2212(xi) \u02c6sh\u2212(xi)(xi).\n\n\u02c6h(xi) \u2212 h\u2212(xi)\nh+(xi) \u2212 h\u2212(xi) \u02c6sh+(xi)(xi) +\n\n\u02c6s (xi) =\nfigure11.15showsthefinalresult.thesupersmootheradjustedthespanwisely,\nbased on the local variability of the data. in comparison, the spline smooth shown in\nthis figure undersmoothed the left side and oversmoothed the right side, for a fixed \u03bb\nchosen by cross-validation.\nalthough the supersmoother is a nonlinear smoother, it is very fast compared\nto most other nonlinear smoothers, including loess.\n\n(11.35)\n\n11.5 confidence bands\nproducing reliable confidence bands for smooths is not straightforward. intuitively,\nwhat is desired is an image that portrays the range and variety of smooth curves\nthat might plausibly be obtained from data like what we observed. bootstrapping\n(chapter 9) provides a method for avoiding parametric assumptions, but it does not\nhelp clarify exactly what sort of region should be graphed.\nconsider first the notion of a pointwise confidence band. bootstrapping the\nresiduals would proceed as follows. let e denote the vector of residuals [so e =\n(i \u2212 s)y for a linear smoother]. sample the elements of e with replacement to gen-\nerate bootstrapped residuals e\u2217. add these to the fitted values to obtain bootstrapped\nresponses y\u2217 =7y + e\u2217. smooth y\u2217 over x to generate a bootstrapped fitted smooth,\n\n "}, {"Page_number": 390, "text": "11.5 confidence bands\n\n385\n\ne\ns\nn\no\np\ns\ne\nr\n\n1\n\n0\n\n\u22121\n\n0\n\n0.25\n\n0.50\n\n0.75\n\n1\n\nfigure 11.15 supersmoother fit (solid line). a spline smoother fit (with \u03bb chosen by cross-\nvalidation) is also shown (dotted line).\n\npredictor\n\n\u02c6s\u2217.startanewandrepeatthebootstrappingmanytimes.then,foreach xinthedataset,\na bootstrap confidence interval for \u02c6s (x) can be generated using the percentile method\n(section 9.3.1) by deleting the few largest and smallest bootstrap fits at that point. if\nthe upper bounds of these pointwise confidence intervals are connected for each x,\nthe result is a band lying above \u02c6s (x). a plot showing this upper band along with the\ncorresponding lower band provides a visually appealing confidence region.\nalthough this approach is appealing, it can be quite misleading. first, the confi-\ndencebandsarecomposedofpointwiseconfidenceintervalswithnoadjustmentmade\nfor simultaneous inference. to correct the joint coverage probability to 95%, each of\nthe individual intervals would need to represent much more than 95% confidence.\nthe result would be a substantial widening of the pointwise bands.\nsecond, the pointwise confidence bands are not informative about features\nshared by all smooths supported by the data. for example, all the smooths could\nhave an important bend at the same point, but the pointwise confidence region would\nnot necessarily enforce this. it would be possible to sketch smooth curves lying\nentirely within the pointwise region that do not have such a bend, or even ones that\nhave a reverse bend at that point. similarly, suppose that all the smooths show gener-\nally the same curved shape and a linear fit is significantly inferior. if the confidence\nbands are wide or if the curve is not too severe, it will be possible to sketch a linear\nfit that lies entirely within the bands. in this case, the pointwise bands fail to convey\nan important aspect of the inference: that a linear fit should be rejected.\nexample 11.9 (confidence bands)\nthe shortcomings of pointwise confidence\nbands are illustrated in figure 11.16, for some data for which the true conditional\nmean function is e{y|x} = x2. the smoothing span for a running-line smoother\nwas chosen by cross-validation, and a pointwise 95% confidence band is shown by\n\n "}, {"Page_number": 391, "text": "386\n\nchapter 11 bivariate smoothing\n\n1\n\n0\n\ne\ns\nn\no\np\ns\ne\nr\n\n1\n\u2212\n\n\u22120.50\n\n\u22120.25\n\n0\n\npredictor\n\n0.25\n\n0.50\n\nfigure 11.16 running-line smooth of some data for which e{y|x} = x2, with span chosen\nbycross-validation.theshadedregionrepresentsapointwise95%confidencebandasdescribed\nin the text. note that the line y = 0 is contained entirely within the band. the dotted lines show\nthe result of the expansion approach described in example 11.9.\nthe shaded region. note that the band widens appropriately near the edges of the\ndata to reflect the increased uncertainty of smoothing in these regions with fewer\nneighboring observations. unfortunately, the null model e{y|x} = 0 lies entirely\nwithin the pointwise confidence band. in example 11.11 we introduce an alternative\nmethod that does not support the null model.\n!\nthe failure of pointwise confidence bands to capture the correct joint coverage\nprobability can be remedied in several ways. one straightforward approach begins\nby writing the ordinary pointwise confidence bands as (\u02c6s (x) \u2212 \u02c6l(x), \u02c6s (x) + \u02c6u(x)),\nwhere \u02c6l(x)and \u02c6u(x)denotehowfarthelowerandupperpointwiseconfidencebounds\ndeviate from \u02c6s (x) at the point x. then the confidence bands can be expanded outwards\nbyfindingthesmallest \u03c9 forwhichthebandsgivenby(\u02c6s (x) \u2212 \u03c9 \u02c6l(x), \u02c6s (x) + \u03c9 \u02c6u(x))\ncontain at\nleast (1 \u2212 \u03b1)100% of the bootstrap curves in their entirety, where\n(1 \u2212 \u03b1)100% is the desired confidence level. alternatively, \u02c6s (x) can be replaced\nby the pointwise median bootstrap curve; for hypothesis testing use the pointwise\nmedian null band. a second, rough approach is simply to additively shift the point-\nwise bounds outwards until the desired percentage of bootstrap curves are entirely\ncontained within.\nexample 11.10 (confidence bands, continued) applying this approach to ex-\nample 11.9, we find \u03c9 = 1.61. the resulting bands are shown as dotted lines in\nfigure 11.16. this method improves the joint coverage probability.\n!\nthe failure of pointwise confidence bands to represent accurately the shape\nof the bootstrap confidence set cannot be blamed on the pointwise nature of the\n\n "}, {"Page_number": 392, "text": "11.5 confidence bands\n\n387\n\n1\n\n0\n\n\u22121\n\ne\ns\nn\no\np\ns\ne\nr\n\n\u22120.50\n\n\u22120.25\n\n0\n\n0.25\n\n0.50\n\nfigure 11.17 twenty bootstrap smooths of the data in figure 11.16 for which v \u2217 was\nwithin the central 95% region of its bootstrap distribution; see example 11.11.\n\npredictor\n\nbands; rather it is caused by the attempt to reduce a n-dimensional confidence set to a\ntwo-dimensional picture. even if a band with correct joint coverage were used, the\nsame problems would remain. for that reason, it may be more reasonable to super-\nimpose a number of smooth curves that are known to belong to the joint confidence\nset, rather than trying to plot the boundaries of the set itself. with this in mind, we\nnow describe a second bootstrapping approach suitable for linear smoothers.\nassume the response variable has constant variance. among the estimators of\nthis variance, hastie and tibshirani [322] recommend\n\n\u02c6\u03c32\n\n=\n\nrss\u03bb(\u02c6s\u03bb)\n\nn \u2212 2tr{s} + tr{sst}\n\n,\n\n(11.36)\n\nwhere \u02c6s\u03bb and rss\u03bb represent the estimated smooth using span \u03bb and the resulting\nresidual squared error. the quantity\n\n(11.37)\nis approximately pivotal, so its distribution is roughly independent of the true un-\nderlying curve. bootstrap the residuals as above, each time computing the vector of\nbootstrap fits, \u02c6s\u2217, and the corresponding value\n\nv = (\u02c6s\u03bb \u2212 s)t4\u02c6\u03c32sst5\u22121(\u02c6s\u03bb \u2212 s)\n\nv\u2217 =4\u02c6s\u2217\u03bb \u2212 \u02c6s\u03bb5t4\u02c6\u03c3\u22172sst5\u221214\u02c6s\u2217\u03bb \u2212 \u02c6s\u03bb5.\n\n(11.38)\nusethecollectionof v\u2217 valuestoconstructtheempiricaldistributionof v\u2217.eliminate\nthose bootstrap fits whose v\u2217 values are in the upper tail of this empirical distribution.\ngraph the remaining smooths\u2014or a subset of them\u2014superimposed. this provides a\nuseful picture of the uncertainty of a smooth.\n\n "}, {"Page_number": 393, "text": "388\n\nchapter 11 bivariate smoothing\n\n2\n\nx\n\n2\n\nx\n\nx1\n\nx1\n\nfigure 11.18 the left panel shows data scattered around the time-parameterized curve\ngiven by (x(\u03c4), y(\u03c4)) = ((1 \u2212 cos \u03c4)cos \u03c4, (1 \u2212 cos \u03c4)sin \u03c4) for \u03c4 \u2208 [0,3\u03c0/2], which is shown\nby the solid line. the dotted line shows the result of a fifth-order polynomial regression of x2\non x1, and the dashed line shows the result of a fifth-order polynomial regression of x1 on\nx2. the right panel shows a principal curve smooth (solid) for these data, along with the true\ncurve (dotted). these are nearly superimposed.\n\nexample 11.11 (confidence bands, continued) applying the above method to\nthe data described in example 11.9 with a running-line smoother yields figure 11.17.\nroughly the same pointwise spread is indicated by this figure as is shown in the\npointwise bands of figure 11.16, but figure 11.17 confirms that the smooth is curved\nlike y = x2. in fact, from 1000 bootstrap iterations, only three smooths resembled\nfunctions with a nonpositive second derivative. thus, this bootstrap approach strongly\nrejects the null relationship y = 0, while the pointwise confidence bands fail to rule\nit out.\n!\na variety of other bootstrapping and parametric approaches for assessing the\n\nuncertainty of results from smoothers are given in [192, 309, 322, 436].\n\n11.6 general bivariate data\nfor general bivariate data, there is no clear distinction of predictor and response vari-\nables, even though the two variables may exhibit a strong relationship. it is therefore\nmore reasonable to label the variables as x1 and x2. as an example of such data,\nconsider the two variables whose scatterplot is shown in figure 11.18. for this ex-\nample, the curve to be estimated corresponds to a curvilinear ridgetop of the joint\ndistribution of x1 and x2.\narbitrary designation of one variable as predictor and the other as response\nis counterproductive for such problems. for example, the left panel of figure 11.18\nshows the two fits obtained by ordinary fifth-order polynomial regression. each of\ntheselinesisfittedbyminimizingasetofresidualsthatmeasurethedistancesbetween\npoints and the fitted curve, parallel to the axis of response. in one case x1 was treated\n\n "}, {"Page_number": 394, "text": "11.6 general bivariate data\n\n389\nas the response, and in the other case x2 was. very different answers result, and in\nthis case they are both terrible approximations to the true relationship.\nthe right panel of figure 11.18 shows another curve fit to these data. here, the\ncurve was chosen to minimize the orthogonal distances between the points and the\ncurve, without designation of either variable as the response. this approach corre-\nsponds to the local-averaging notion that points in any local neighborhood should fall\nnear the curve. one approach to formalizing this notion is presented in section 12.2.1,\nwhere we discuss the principal curves method for smoothing general p-dimensional\ndata when there is no clear distinction of predictor and response. setting p = 2 ac-\ncommodates the bivariate case shown here.\n\nproblems\n11.1. generate 100 random points from the following model: x \u223c unif(0, \u03c0) and y =\ng(x) + \u03f5 with independent \u03f5|x \u223c n(0, g(x)2/64), where g(x) = 1 + sin{x2}/x2.\nsmooth your data with a constant-span (symmetric nearest neighbor) running-mean\nsmoother. select a span of 2k + 1 for 1 \u2264 k \u2264 11 chosen by cross-validation. does a\nrunning-median smooth with the same span seem very different?\n11.2. use the data from problem 11.1 to investigate kernel smoothers as described below:\na. smooth the data using a normal kernel smoother. select the optimal standard\n\ndeviation of the kernel using cross-validation.\nb. define the symmetric triangle distribution as\n\nc. let\n\nthe standard deviation of this distribution is a/\u221a6. smooth the data using a sym-\nmetric triangle kernel smoother. use cross-validation to search the same set of\nstandard deviations used in the first case, and select the optimum.\n\nif |x \u2212 \u00b5| > h,\nif \u00b5 \u2212 h \u2264 x < \u00b5,\nif \u00b5 \u2264 x \u2264 \u00b5 + h.\n\n0\n(x \u2212 \u00b5 + h)/a2\n(\u00b5 + h \u2212 x)/a2\n\nf(x; \u00b5, h) =\u23a7\u23aa\u23a8\u23aa\u23a9\nf(x; \u00b5, h) = c41 + cos{2\u03c0zlog{|z| + 1}}5exp& \u2212\n\n1\n\n2 z2),\n\nwhere z = (x \u2212 \u00b5)/ h and c is a constant. plot this density function. the standard\ndeviation of this density is about 0.90h. smooth the data using a kernel smoother\nwith this kernel. use cross-validation to search the same set of standard deviations\nused previously, and select the optimum.\n\nd. compare the smooths produced using the three kernels. compare their cvrss\nvalues at the optimal spans. compare the optimal spans themselves. for kernel\nsmoothers, what can be said about the relative importance of the kernel and the\nspan?\n\n11.3. use the data from problem 11.1 to investigate running lines and running polynomial\n\nsmoothers as described below:\n\n "}, {"Page_number": 395, "text": "390\n\nchapter 11 bivariate smoothing\n\na. smooth the data using a running-line smoother with symmetric nearest neighbors.\n\nselect a span of 2k + 1 for 1 \u2264 k \u2264 11 chosen by cross-validation.\nb. repeat this process for running local polynomial smoothers of degree 3 and 5;\neach time choose the optimal span using cross-validation over a suitable range\nfor k. (hints: you may need to orthogonalize polynomial terms; also reduce the\npolynomial degree as necessary for large spans near the edges of the data.)\n\nc. comment on the quality and characteristics of the three smooths (local linear,\n\nd. does there appear to be a relationship between polynomial degree and optimal\n\ncubic, and quintic).\n\nspan?\n\ne. comment on the three plots of cvrss.\n\n11.4. the book website provides data on the temperature\u2013pressure profile of the martian\natmosphere, as measured by the mars global surveyor spacecraft in 2003 using a\nradio occultation technique [638]. temperatures generally cool with increasing plan-\netocentric radius (altitude).\na. smooth temperature as a function of radius using a smoothing spline, loess, and at\n\nleast one other technique. justify the choice of span for each procedure.\n\nb. the dataset also includes standard errors for the temperature measurements. apply\nreasonable weighting schemes to produce weighted smooths using each smoother\nconsidered in part (a). compare these results with the previous results. discuss.\n\nc. construct confidence bands for your smooths. discuss.\nd. these data originate from seven separate orbits of the spacecraft. these orbits pass\nover somewhat different regions of mars. a more complete dataset including orbit\nnumber, atmospheric pressure, longitude, latitude, and other variables is available\nin the file mars-all.dat at the book website. introductory students may smooth\nsome other interesting pairs of variables. advanced students may seek to improve\nthe previous analyses, for example, by adjusting for orbit number or longitude and\nlatitude. such an analysis might include both parametric and nonparametric model\ncomponents.\n\n11.5. reproduce figure 11.8. (hint: the kernel for a spline smoother can be reverse-\nengineered from the fit produced by any software package, using a suitable vector\nof response data.)\na. create a graph analogous to figure 11.8 for smoothing at the second smallest\n\npredictor value. compare this with the first graph.\n\nb. graphically compare the equivalent kernels for cubic smoothing splines for differ-\n\nent xi and \u03bb.\n\n11.6. figure 11.19 shows the pressure difference between two sensors on a steel plate\nexposed to a powerful air blast [342]. there are 161 observations during a period\njust before and after the blast. the noise in figure 11.19 is attributable to inadequate\ntemporalresolutionandtoerrorinthesensorsandrecordingequipment;theunderlying\nphysical shock waves that generate these data are smooth. these data are available\nfrom the website for this book.\na. construct a running-line smooth of these data, with span chosen by eye.\n\n "}, {"Page_number": 396, "text": "2.5\n\n2.0\n\n1.5\n\n1.0\n\n0.5\n\n0\n\n)\n|\na\np\nk\n|\n(\n \ne\nd\nu\nt\ni\nn\ng\na\nm\n\n \ne\nr\nu\ns\ns\ne\nr\np\n\n\u22120.01\n\n0\n\n11.6 general bivariate data\n\n391\n\n0.02\n\n0.03\n\n0.01\ntime (s)\n\nfigure 11.19 data on air blast pressure difference for problem 11.6.\n\nb. makeaplotofcvrssk(\u02c6sk)versus k for k \u2208 {3,5,7,11,15,20,30,50}.comment.\nc. produce the most appealing smooth you can for these data, using any smoother\n\nand span you wish. why do you like it?\n\nd. comment on difficulties of smoothing and span selection for these data.\n\n11.7. using the data from problem 11.6 and your favorite linear smoothing method for\nthese data, construct confidence bands for the smooth using each method described\nin section 11.5. discuss. (using a spline smoother is particularly interesting.)\n\n "}, {"Page_number": 397, "text": "chapter 12\nmultivariate smoothing\n\n12.1 predictor\u2013response data\nmultivariate predictor\u2013response smoothing methods fit smooth surfaces to obser-\nvations (xi, yi), where xi is a vector of p predictors and yi is the corresponding\nresponse value. the y1, . . . , yn values are viewed as observations of the random vari-\nables y1, . . . , yn, where the distribution of yi depends on the ith vector of predictor\nvariables.\nmany of the bivariate smoothing methods discussed in chapter 11 can be gen-\neralized to the case of several predictors. running lines can be replaced by running\nplanes. univariate kernels can be replaced by multivariate kernels. one generalization\nof spline smoothing is thin plate splines [280, 451]. in addition to the significant com-\nplexities of actually implementing some of these approaches, there is a fundamental\nchange in the nature of the smoothing problem when using more than one predictor.\nthe curse of dimensionality is that high-dimensional space is vast, and points\nhave few near neighbors. this same problem was discussed in section 10.4.1 as it\nappliedtomultivariatedensityestimation.consideraunitspherein pdimensionswith\nvolume \u03c0p/2/ \u0001(p/2 + 1). suppose that several p-dimensional predictor points are\ndistributed uniformly within the ball of radius 4. in one dimension, 25% of predictors\nare expected within the unit ball; hence unit balls might be reasonable neighborhoods\nfor smoothing. table 12.1 shows that this proportion vanishes rapidly as p increases.\nin order to retain 25% of points in a neighborhood when the full set of points lies in\na ball of radius 4, the neighborhood ball would need to have radius 3.73 if p = 20.\nthus, the concept of local neighborhoods is effectively lost.\nthe effectiveness of\nthe curse of dimensionality raises concerns about\nsmoothers for multivariate data. effective local averaging will require a large number\nofpointsineachneighborhood,buttofindsuchpoints,theneighborhoodsmuststretch\nover most of predictor space. a variety of effective multivariate surface smoothing\nmethods are described in [322, 323, 573].\nthere is a rich set of smoothing methods developed for geostatistics and spatial\nstatistics that are suitable for two and three dimensions. in particular, kriging methods\noffer a more principled foundation for inference than many of the generic smoothers\nconsidered here. we do not consider such methods further, but refer readers to books\non spatial statistics such as [128, 291].\n\ncomputational statistics, second edition. geof h. givens and jennifer a. hoeting.\n\u00a9 2013 john wiley & sons, inc. published 2013 by john wiley & sons, inc.\n\n393\n\n "}, {"Page_number": 398, "text": "394\n\nchapter 12 multivariate smoothing\n\ntable 12.1 ratio of volume of unit\nsphere in p dimensions to volume of\nsphere with radius 4.\n\np\n1\n2\n3\n4\n5\n10\n20\n100\n\nratio\n0.25\n0.063\n0.016\n0.0039\n0.00098\n9.5 \u00d7 10\u22127\n9.1 \u00d7 10\u221213\n6.2 \u00d7 10\u221261\n\n12.1.1 additive models\nsimple linear regression is based on the model e{y|x} = \u03b20 + \u03b21x. nonparamet-\nric smoothing of bivariate predictor\u2013response data generalizes this to e{y|x} = s(x)\nfor a smooth function s. now we seek to extend the analogy to the case with p\npredictors. multiple regression uses the model e{y|x} = \u03b20 +!p\nk=1 \u03b2kxk where\nx = (x1, . . . , xp)t. the generalization for smoothing is the additive model\n\ne{y|x} = \u03b1 +\n\np\"k=1\n\nsk(xk),\n\n(12.1)\n\nwhere sk is a smooth function of the kth predictor variable. thus, the overall model\nis composed of univariate effects whose influence on the mean response is additive.\n\nfitting such a model relies on the relationship\n\nsk(xk) = e#y \u2212 \u03b1 \u2212\"j /= k\n\nsj(xj)$$$x%,\n\n(12.2)\n\nwhere xk is the kth component in x. suppose that we wished to estimate sk at x\u2217k,\nand that many replicate values of the kth predictor were observed at exactly this x\u2217k.\nsuppose further that all the sj (j /= k) were known except sk. then the expected\nvalue on the right side of (12.2) could be estimated as the mean of the values of\nyi \u2212 \u03b1 \u2212!j /= k sj(xij) corresponding to indices i for which the ith observation of\nthe kth predictor satisfies xik = x\u2217k. for actual data, however, there will likely be no\nsuch replicates. this problem can be overcome by smoothing: the average is taken\nover points whose kth coordinate is in a neighborhood of x\u2217k. a second problem\u2014\nthat none of the sj are actually known\u2014is overcome by iteratively cycling through\nsmoothing steps based on isolations like (12.2) updating sk using the best current\nguesses for all sj for j /= k.\ntheiterativestrategyiscalledthebackfittingalgorithm.lety = (y1, . . . , yn)t,\nand for each k, let \u02c6s(t)\nk denote the vector of estimated values of sk(xik) at iteration t for\n\n "}, {"Page_number": 399, "text": "12.1 predictor\u2013response data\n\n395\ni = 1, . . . , n. these n-vectors of estimated smooths at each observation are updated\nas follows:\n1. let \u02c6\u03b1 be the n-vector (y , . . . , y)t. some other generalized average of the re-\nsponse values may replace the sample mean y. let t = 0, where t indexes the\niteration number.\n2. let \u02c6s(0)\nrepresent initial guesses for coordinatewise smooths evaluated at the\nk = ( \u02c6\u03b2kx1k, . . . , \u02c6\u03b2kxnk)t\nobserved data. a reasonable initial guess is to let \u02c6s(0)\nfor k = 1, . . . , p, where the \u02c6\u03b2k are the linear regression coefficients found when\ny is regressed on the predictors.\n3. for k = 1, . . . , p in turn, let\n\nk\n\nwhere\n\n\u02c6s(t+1)\n\nk\n\n= smoothk (rk) ,\n\u2212\"j>k\n\n\u02c6s(t+1)\n\nj\n\nrk = y \u2212 \u03b1 \u2212\"j<k\n\n(12.3)\n\n\u02c6s(t)\n\nj\n\n(12.4)\n\nand smoothk(rk) denotes the vector obtained by smoothing the elements of\nrk against the kth coordinate values of predictors, namely x1k, . . . , xnk, and\nevaluating the smooth at each xik. the smoothing technique used for the kth\nsmooth may vary with k.\n\n4. increment t and go to step 3.\n\nthe algorithm can be stopped when none of the \u02c6s(t)\nk \u2019( p\"k=1&\u02c6s(t)\n\u2212 \u02c6s(t)\n\nk \u2019t&\u02c6s(t+1)\n\u2212 \u02c6s(t)\n\np\"k=1&\u02c6s(t+1)\n\nk\n\nk\n\nk \u2019t\u02c6s(t)\n\nk\n\nk change very much\u2014perhaps when\n\nis very small.\nto understand why this algorithm works, recall the gauss\u2013seidel algorithm for\nsolving a linear system of the form az = b for z given a matrix a and a vector b of\nknown constants (see section 2.2.5). the gauss\u2013seidel procedure is initialized with\nstarting value z0. then each component of z is solved for in turn, given the current\nvalues for the other components. this process is iterated until convergence.\nsuppose only linear smoothers are used to fit an additive model, and let sk be\nthe n \u00d7 n smoothing matrix for the kth component smoother. then the backfitting\nalgorithm solves the set of equations given by \u02c6sk = sk&y \u2212!j /= k \u02c6sj\u2019. writing this\nset of equations in matrix form yields\n\n\u239b\u239c\u239c\u239c\u239c\u239d\n\ns1\ni\n...\n\ns1\ni\ns2\ns2\n...\n...\nsp sp sp\n\n. . . s1\n. . . s2\n...\ni\n\n. . .\n\n\u239e\u239f\u239f\u239f\u239f\u23a0\n\n\u239b\u239c\u239c\u239c\u239c\u239d\n\n\u02c6s1\n\u02c6s2\n...\n\u02c6sp\n\n\u239e\u239f\u239f\u239f\u239f\u23a0\n\n=\n\ns1y\ns2y\n...\nspy\n\n\u239b\u239c\u239c\u239c\u239c\u239c\u239d\n\n\u239e\u239f\u239f\u239f\u239f\u239f\u23a0\n\n,\n\n(12.5)\n\n "}, {"Page_number": 400, "text": "chapter 12 multivariate smoothing\n\n396\nwhich is of the form az = b where z = (\u02c6s1, \u02c6s2, . . . , \u02c6sp)t = \u02c6s. note that b = \u0001y\nwhere \u0001isablock-diagonalmatrixwiththeindividualsk matricesalongthediagonal.\nsince the backfitting algorithm sequentially updates each vector \u02c6sk as a single block, it\nis more formally a block gauss\u2013seidel algorithm. the iterative backfitting algorithm\nis preferred because it is faster than the direct approach of inverting a.\nwe now turn to the question of convergence of the backfitting algorithm and the\nuniqueness of the solution. here, it helps to revisit the analogy to multiple regression.\ni so d =/x1, . . . ,xn0t.\nlet d denote the n \u00d7 p design matrix whose ith row is xt\nconsider solving the multiple regression normal equations dtd\u03b2 = dty for \u03b2. the\nelements of \u03b2 are not uniquely determined if any predictors are linearly dependent,\nor equivalently, if the columns of dtd are linearly dependent. in that case, there\nwould exist a vector \u03b3 such that dtd\u03b3 = 0. thus, if \u02c6\u03b2 were a solution to the normal\nequations, \u02c6\u03b2 + c\u03b3 would also be a solution for any c.\nanalogously, the backfitting estimating equations a\u02c6s = \u0001y will not have a\nunique solution if there exists any \u03b3 such that a\u03b3 = 0. let ik be the space spanned\nby vectors that pass through the kth smoother unchanged. if these spaces are linearly\ndependent, then there exist \u03b3 k \u2208 ik such that!p\nk=1 \u03b3 k = 0. in this case, a\u03b3 = 0,\nwhere \u03b3 = (\u03b31, \u03b32, . . . , \u03b3 p)t, and therefore there is not a unique solution (see\nproblem 12.1).\namorecompletediscussionoftheseissuesisprovidedbyhastieandtibshirani\n[322], from which the following result is derived. let the p smoothers be linear and\neach sk be symmetric with eigenvalues in [0,1]. then a\u03b3 = 0 if and only if there\nexist linearly dependent \u03b3 k \u2208 ik that pass through the kth smoother unchanged. in\nthis case, there are many solutions to a\u02c6s = \u0001y, and backfitting converges to one\nof them, depending on the starting values. otherwise, backfitting converges to the\nunique solution.\ntheflexibilityoftheadditivemodelisfurtherenhancedbyallowingtheadditive\ncomponents of the model to be multivariate and by allowing different smoothing\nmethods for different components. for example, suppose there are seven predictors,\nx1, . . . , x7,where x1 isadiscretevariablewithlevels1, . . . , c.thenanadditivemodel\nto estimate e{y|x} might be fitted by backfitting:\n\n\u02c6\u03b1 +\n\nc\u22121\"i=1\n\n\u02c6\u03b4i1{x1=i} + \u02c6s(x2) + \u02c6p(x3) + \u02c6t(x4, x5) + \u02c6f(x6, x7),\n\n(12.6)\n\nwhere the \u02c6\u03b4i permit a separate additive effect for each level of x1, \u02c6s(x2) is a spline\nsmooth over x2, \u02c6p(x3) is a cubic polynomial regression on x3, \u02c6t(x4, x5) is a recur-\nsively partitioned regression tree from section 12.1.4, and \u02c6f(x6, x7) is a bivariate\nkernel smooth. grouping several predictors in this way provides coarser blocks in the\nblockwise implementation of the gauss\u2013seidel algorithm.\nexample 12.1 (norwegian paper) we consider some data from a paper plant in\nhalden, norway [9]. the response is a measure of imperfections in the paper, and\nthere are two predictors. (our y, x1, and x2 correspond to 16 \u2212 y5, x1, and x3,\nrespectively, in the author\u2019s original notation.) the left panel of figure 12.1 shows\n\n "}, {"Page_number": 401, "text": "12.1 predictor\u2013response data\n\n397\n\ny\n\n4\n\n3\n\n2\n\n1\n\n\u22122 \u22121\n\n0\n\nx1\n\n1\n\n1\n\n0\n\nx 2\n\n\u22121\n\ny\n\n4\n\n3\n\n2\n\n1\n\n\u22122 \u22121\n\nx1\n\n1\n\n0\n\nx 2\n\n\u22121\n\n0\n\n1\n\nfigure 12.1 linear (left) and additive (right) models fitted to the norwegian paper data in\nexample 12.1.\n\nthe response surface fitted by an ordinary linear model with no interaction. the right\npanel shows an additive model fitted to the same data. the estimated \u02c6sk are shown\nin figure 12.2. clearly x1 has a nonlinear effect on the response; in this sense the\nadditive model is an improvement over the linear regression fit.\n!\n\n12.1.2 generalized additive models\nlinearregressionmodelscanbegeneralizedinseveralways.above,wehavereplaced\nlinear predictors with smooth nonlinear functions. a different way to generalize linear\nregression is in the direction of generalized linear models [446].\nsuppose that y|x has a distribution in the exponential family. let \u00b5 = e{y|x}.\na generalized linear model assumes that some function of \u00b5 is a linear function\nof the predictors. in other words, the model is g(\u00b5) = \u03b1 +!p\nk=1 \u03b2kxk, where g\nis the link function. for example, the identity link g(\u00b5) = \u00b5 is used to model\na gaussian distributed response, g(\u00b5) = log \u00b5 is used for log-linear models, and\ng(\u00b5) = log{\u00b5/(1 \u2212 \u00b5)} is one link used to model bernoulli data.\ngeneralized additive models (gams) extend the additive models of sec-\ntion 12.1.1 in a manner analogous to how generalized linear models extend linear\nmodels. for response data in an exponential family, a link function g is chosen, and\nthe model is\n\ng(\u00b5) = \u03b1 +\n\np\"k=1\n\nsk(xk),\n\n(12.7)\n\nwhere sk is a smooth function of the kth predictor. the right-hand side of (12.7) is\ndenoted \u03b7 and is called the additive predictor. gams provide the scope and diversity\nofgeneralizedlinearmodels,withtheadditionalflexibilityofnonlinearsmootheffects\nin the additive predictor.\nfor generalized linear models, estimation of \u00b5 = e{y|x} proceeds via iter-\natively reweighted least squares. roughly, the algorithm proceeds by alternating\nbetween (i) constructing adjusted response values and corresponding weights, and\n(ii) fitting a weighted linear regression of the adjusted response on the predictors.\nthese steps are repeated until the fit has converged.\nspecifically,wedescribedinsection2.2.1.1howtheiterativelyreweightedleast\nsquaresapproachforfittinggeneralizedlinearmodelsinexponentialfamiliesisinfact\n\n "}, {"Page_number": 402, "text": "398\n\nchapter 12 multivariate smoothing\n\n)\n1\nx\n(\n1\ns\u02c6\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n)\n2\nx\n(\n2\ns\u02c6\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n\u22121\n\n0\n\n1\n\n\u22122\n\n\u22121\n\n0\n\n1\n\nx1\n\nx2\n\nfigure 12.2 the smooths \u02c6sk(xk) fitted with an additive model for the norwegian paper\ndata in example 12.1. the points are partial residuals as given on the right-hand side of (12.3),\nnamely, \u02c6sk(xik) plus the overall residual from the final smooth.\n\nthe fisher scoring approach. the fisher scoring method is ultimately motivated by a\nlinearization of the score function that yields an updating equation for estimating the\nparameters. the update is achieved by weighted linear regression. adjusted responses\nand weights are defined as in (2.41). the updated parameter vector consists of the\ncoefficients resulting from a weighted linear least squares regression for the adjusted\nresponses.\nfor fitting a gam, weighted linear regression is replaced by weighted smooth-\ning. the resulting procedure, called local scoring, is described below. first, let \u00b5i be\nthe mean response for observation i, so \u00b5i = e{yi|xi} = g\u22121(\u03b7i), where \u03b7i is called\nthe ith value of the additive predictor; and let v(\u00b5i) be the variance function, namely,\nvar{yi|xi} expressed as a function of \u00b5i. the algorithm proceeds as follows:\n1. initialize the algorithm at t = 0. set \u02c6\u03b1(0) = g(\u00afy) and \u02c6s\n(0)\nk (\u00b7) = 0 for k =\n(0)\ni = \u02c6\u03b1(0) +\n1, . . . , p. this also initializes the additive predictor values \u02c6\u03b7\n(0)\n(0)\n!p\ni = g\u22121(\u02c6\u03b7\nk=1 \u02c6s\n) corresponding to each\nk (xik) and the fitted values \u02c6\u00b5\nobservation.\n2. for i = 1, . . . , n, construct adjusted response values\ni 2\u22121\ni \u20191 d\u00b5\nd\u03b7$$$$\u03b7=\u02c6\u03b7\n3. for i = 1, . . . , n, construct the corresponding weights\ni 22&v& \u02c6\u00b5\ni \u2019\u2019\u22121\n(t)\n\ni +&yi \u2212 \u02c6\u00b5\n(t)\n= \u02c6\u03b7\n=1 d\u00b5\nd\u03b7$$$$\u03b7=\u02c6\u03b7\n\n4. use a weighted version of the backfitting algorithm from section 12.1.1 to es-\n. in this step, a weighted additive model\n\ntimate new additive predictors, \u02c6s\n\n(t+1)\nw\ni\n\n(t+1)\nz\ni\n\n(12.8)\n\n(12.9)\n\n(t+1)\n\n(0)\ni\n\n(t)\n\n(t)\n\n(t)\n\n.\n\n.\n\nk\n\n "}, {"Page_number": 403, "text": "12.1 predictor\u2013response data\n\n399\n\nof the form (12.7) is fitted to the adjusted response values z\n(t+1)\nw\ni\nscribed further below, also allows calculation of new \u02c6\u03b7\n\nwith weights\n(xik) for i = 1, . . . , n and k = 1, . . . , p. this step, de-\n\n, yielding \u02c6s\n\nand \u02c6\u00b5\n\n(t+1)\n\n(t+1)\n\n(t+1)\n\n.\n\nk\n\ni\n\n(t+1)\n\ni\n\ni\n\n5. compute a convergence criterion such as\n\np\"k=1\n\nn\"i=1&\u02c6s\n\n(t+1)\n\nk\n\n(t)\n(xik) \u2212 \u02c6s\n\nk (xik)\u20192( p\"k=1\n\nn\"i=1&\u02c6s\n\n(t)\n\nk (xik)\u20192\n\nand stop when it is small. otherwise, go to step 2.\n\n,\n\n(12.10)\n\ni\n\n(t+1)\n\nto revert to a standard generalized linear model, the only necessary change would be\nto replace the smoothing in step 4 with weighted least squares.\nthe fitting of a weighted additive model in step 4 requires weighted smooth-\ning methods. for linear smoothers, one way to introduce weights is to multiply the\nelements in the ith column of s by w\nfor each i, and then standardize each row\nso it sums to 1. there are other, more natural approaches to weighting some linear\nsmoothers (e.g., splines) and nonlinear smoothers. further details about weighted\nsmooths and local scoring are provided in [322, 574].\nas with additive models, the linear predictor in gams need not consist solely\nof univariate smooths of the same type. the ideas in section 12.1.1 regarding more\ngeneral and flexible model building apply here too.\nexample 12.2 (drug abuse) the website for this book provides data on 575 pa-\ntients receiving residential treatment for drug abuse [336]. the response variable is\nbinary, with y = 1 for a patient who remained drug-free for one year and y = 0 oth-\nerwise. we will examine two predictors: number of prior drug treatments (x1) and age\nof patient (x2). a simple generalized additive model is given by yi|xi \u223c bernoulli(\u03c0i)\nwith\n\nlog3 \u03c0i\n\n1 \u2212 \u03c0i4 = \u03b1 + \u03b21s1(xi1) + \u03b22s2(xi2).\n\n(12.11)\n\nspline smoothing was used in step 4 of the fitting algorithm. figure 12.3 shows the\nfitted response surface graphed on the probability scale. figure 12.4 shows the fitted\nsmooths \u02c6sk on the logit scale. the raw response data are shown by hash marks along\nthe bottom (yi = 0) and top (yi = 1) of each panel.\n!\n12.1.3 other methods related to additive models\ngeneralizedadditivemodelsarenottheonlywaytoextendtheadditivemodel.several\nother methods transform the predictors or response in an effort to provide a more\neffective model for the data. we describe four such approaches below.\n12.1.3.1 projection pursuit regression additive models produce fits com-\nposed of p additive surfaces, each of which has a nonlinear profile along one\n\n "}, {"Page_number": 404, "text": "400\n\nchapter 12 multivariate smoothing\n\n6\n\n.\n\n0\n\n]\n\ne\n\ne\n\nr\n\nf\n\n \n\ng\n\nu\n\nr\n\nd\n\n[\n\np\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n5\n\n0\n\n4\n\n0\n\nx\n2\n\n3\n\n0\n\n2\n\n0\n\n4 0\n\n0\n\n1 0\n\n3 0\n\n2 0\n\nx 1\n\nfigure 12.3 the fit of a generalized additive model to the drug abuse data described in\nexample 12.2. the vertical axis corresponds to the predicted probability of remaining drug\nfree for one year.\n\ncoordinate axis while staying constant in orthogonal directions. this aids interpreta-\ntion of the model because each nonlinear smooth reflects the additive effect of one\npredictor. however, it also limits the ability to fit more general surfaces and interac-\ntion effects that are not additively attributable to a single predictor. projection pursuit\nregression eliminates this constraint by allowing effects to be smooth functions of\nunivariate linear projections of the predictors [209, 380].\n\nspecifically, these models take the form\n\ne{y|x} = \u03b1 +\n\nsk(at\n\nk x),\n\nm\"k=1\n\n(12.12)\n\nwhere each term at\nk x is a one-dimensional projection of the predictor vector x =\n(x1, . . . , xp)t. thus each sk has a profile determined by sk along ak and is constant\nin all orthogonal directions. in the projection pursuit approach, the smooths sk and\nthe projection vectors ak are estimated for k = 1, . . . , m to obtain the optimal fit. for\nsufficientlylarge m,theexpressionin(12.12)canapproximateanarbitrarycontinuous\nfunction of the predictors [161, 380].\nto fit such a model, the number of projections, m, must be chosen. when\nm > 1, the model contains several smooth functions of different linear combinations\nat\nk x. the results may therefore be very difficult to interpret, notwithstanding the\nmodel\u2019s usefulness for prediction. choosing m is a model selection problem akin to\nchoosing the terms in a multiple regression model, and analogous reasoning should\napply. one approach would be to fit a model with small m first, then repeatedly add\nthe most effective next term and refit. a sequence of models is thus produced, until\nno further additional term substantially improves the fit.\nfor a given m, fitting (12.12) can be carried out using the following algorithm:\n1. begin with m = 0 by setting \u02c6\u03b1 = \u00afy.\n\n "}, {"Page_number": 405, "text": "12.1 predictor\u2013response data\n\n401\n\n)\n1\nx\n(\n1\n\u02c6s\n\n2\n\n1\n\n0\n\n\u22121\n\n)\n2\nx\n(\n2\n\u02c6s\n\n2\n\n1\n\n0\n\n\u22121\n\n0\n\n20\n\n40\n\n20\n\n30\n\n40\n\n50\n\nx1\n\nx2\n\nfigure 12.4 the smooth functions \u02c6sk fitted with a generalized additive model for the drug-\nabuse data in example 12.2. the raw response data are shown via the hash marks along the\nbottom (yi = 0) and top (yi = 1) of each panel at the locations observed for the corresponding\npredictor variable.\n\n2. increment m. define the current working residual for observation i as\n\nr\n\n(m)\ni = yi \u2212 \u02c6\u03b1 \u2212\n\n(12.13)\nfor i = 1, . . . , n, where the summation vanishes if m = 1. these current resid-\nuals will be used to fit the mth projection.\n\n\u02c6sk(at\n\nk xi)\n\n3. for any p-vector a and smoother sm, define the goodness-of-fit measure\n\nm\u22121\"k=1\n\n(m)\n\nq(a) = 1 \u2212!n\n\ni \u2212 \u02c6sm(atxi)\u20192\ni=1&r\ni \u20192\ni=1&r\n!n\nam and \u02c6sm. if m = m, stop; otherwise go to step 2.\n\n(m)\n\n4. for a chosen type of smoother, maximize q(a) with respect to a. this provides\n\n.\n\n(12.14)\n\nexample 12.3 (norwegian paper, continued) we return to the norwegian paper\ndata of example 12.1. figure 12.5 shows the response surface fitted with projection\npursuit regression for m = 2. a supersmoother (section 11.4.2) was used for each\nprojection. the fitted surface exhibits some interaction between the predictors that is\nnot captured by either model shown in figure 12.1. an additive model was not wholly\nappropriate for these predictors. the heavy lines in figure 12.5 show the two linear\ndirections onto which the bivariate predictor data were projected. the first projection\ndirection, labeled at\n1x, is far from being parallel to either coordinate axis. this allows\na better fit of the interaction between the two predictors. the second projection very\nnearly contributes an additive effect of x1. to further understand the fitted surface, we\ncan examine the individual \u02c6sk, which are shown in figure 12.6. these effects along\nthe selected directions provide a more general fit than either the regression or the\nadditive model.\n!\n\n "}, {"Page_number": 406, "text": "402\n\nchapter 12 multivariate smoothing\n\n4\n\n3\n\ny\n\n2\n\n1\n\n\u22122\n\n\u22121\n\nat\n1 x\n\n0\n\nx1\n\n1\n\nat\n2 x\n\n1\n\n\u2212\n\n1\n\n0\n\nx\n\n2\n\nfigure 12.5 projection pursuit regression surface fitted to the norwegian paper data for\nm = 2, as described in example 12.3.\n\ninadditiontopredictor\u2013responsesmoothing,theideasofprojectionpursuithave\nbeen applied in many other areas, including smoothing for multivariate response data\n[9]anddensityestimation[205].anotherapproach,knownasmultivariateadaptivere-\ngression splines (mars), has links to projection pursuit regression, spline smoothing\n(section 11.2.5), and regression trees (section 12.1.4.1) [207]. mars may perform\nvery well for some datasets, but recent simulations have shown less promising results\nfor high-dimensional data [21].\n12.1.3.2 neural networks neural networks are a nonlinear modeling method\nfor either continuous or discrete responses, producing a regression or classification\nmodel [50, 51, 323, 540]. for a continuous response y and predictors x, one type of\nneural network model, called the feed-forward network, can be written as\n\ng(y) = \u03b20 +\n\n\u03b2mf(\u03b1t\n\nmx + \u03b3m),\n\n(12.15)\n\nm\"m=1\n\nwhere \u03b20, \u03b2m, \u03b1m, and \u03b3m for m = 1, . . . , m are estimated from the data. we can\nthink of the f(\u03b1t\nmx + \u03b3m) for m = 1, . . . , m as being analogous to a set of basis\nfunctions for the predictor space. these f(\u03b1t\nmx + \u03b3m), whose values are not directly\nobserved, constitute a hidden layer, in neural net terminology. usually the analyst\nchooses m in advance, but data-driven selection is also possible. in (12.15), the\nform of the activation function, f, is usually chosen to be logistic, namely f(z) =\n1/[1 + exp{\u2212z}].weuse gasalinkfunction.parametersareestimatedbyminimizing\nthe squared error, typically via gradient-based optimization.\nneural networks are related to projection pursuit regression where sk in (12.12)\nis replaced by a parametric function f in (12.15), such as the logistic function. many\nenhancements to the simple neural network model given above are possible, such as\nthe inclusion of an additional hidden layer using a different activation function, say\n\n "}, {"Page_number": 407, "text": "12.1 predictor\u2013response data\n\n403\n\n2\n\n0\n\n\u22122\n\n)\nx\nt1\na\n(\n1\n\u02c6s\n\n)\nx\nt2\na\n(\n2\n\u02c6s\n\n2\n\n0\n\n\u22122\n\n\u22122\n\n\u22121\n\n0\n\n1\n\n\u22122\n\n\u22121\n\n0\n\n1\n\nat\n1 x\n\nat\n2 x\n\nfigure 12.6 the smooth functions \u02c6sk fitted with a projection pursuit regression model for\nthe norwegian paper data. the current residuals, namely the component-fitted smooth plus the\noverall residual, shown as dots, are plotted against each projection at\n\nk x for k = 1,2.\n\nh. this layer is composed of evaluations of h at a number of linear combinations of\nthe f(\u03b1t\nmx + \u03b3m) for m = 1, . . . , m, roughly serving as a basis for the first hidden\nlayer. neural networks are very popular in some fields, and a large number of software\npackages for fitting these models are available.\n12.1.3.3 alternating conditional expectations the alternating conditional\nexpectations (ace) procedure fits models of the form\n\ne{g(y)|x} = \u03b1 +\n\np\"k=1\n\nsk(xk)\n\n(12.16)\n\nwhere g is a smooth function of the response [64]. unlike most other methods in this\nchapter, ace treats the predictors as observations of a random variable x, and model\nfitting is driven by consideration of the joint distribution of y and x. specifically, the\nidea of ace is to estimate g and sk for k = 1, . . . , p such that the magnitude of the\ncorrelation between g(y) and!p\nk=1 sk(xk) is maximized subject to the constraint that\nvar{g(y)} = 1. the constant \u03b1 does not affect this correlation, so it can be ignored.\nto fit the ace model, the following iterative algorithm can be used:\n1. initialize the algorithm by letting t = 0 and \u02c6g(0)(yi) = (yi \u2212 \u00afy)/\u02c6\u03c3y, where \u02c6\u03c3y\nis the sample standard deviation of the yi values.\n2. generate updated estimates of the additive predictor functions \u02c6s\nfor k =\n1, . . . , p by fitting an additive model with the \u02c6g(t)(yi) values as the response\nand the \u02c6s\n(xik) values as the predictors. the backfitting algorithm from\nsection 12.1.1 can be used to fit this model.\n\n(t+1)\n\n(t+1)\n\nk\n\nk\n\n3. estimate \u02c6g(t+1) by smoothing the values of!p\nk=1 \u02c6s\n\nresponse) over the yi (treated as the predictor).\n\n(t+1)\n\nk\n\n(xik) (treated as the\n\n "}, {"Page_number": 408, "text": "404\n\nchapter 12 multivariate smoothing\n\nk\n\nk\n\n(t+1)\n\n(t+1)\n\nconvergence criterion, stop. otherwise, increment t and go to step 2.\n\n4. rescale \u02c6g(t+1) by dividing by the sample standard deviation of the \u02c6g(t+1)(yi)\nvalues. this step is necessary because otherwise setting both \u02c6g(t+1) and\n!p\nk=1 \u02c6s\nto zero functions trivially provides zero residuals regardless of\nthe data.\n(xik)62 hasconvergedaccordingtoarelative\ni=15 \u02c6g(t+1)(yi) \u2212!p\n5. if!n\nk=1 \u02c6s\nmaximizing the correlation between!p\nk=1 sk(xk) and g(y) is equivalent to\nk=1 sk(xk)]28 with respect to g and {sk} subject to the\nminimizing e7[g(y) \u2212!p\nconstraint that var{g(y)} = 1. for p = 1, this objective is symmetric in x and y: if\nthe two variables are interchanged, the result remains the same, up to a constant.\nace provides no fitted model component that directly links e{y|x} to the\npredictors, which impedes prediction. ace is therefore quite different than the other\npredictor\u2013response smoothers we have discussed because it abandons the notion of\nestimating the regression function; instead it provides a correlational analysis. conse-\nquently, ace can produce surprising results, especially when there is low correlation\nbetween variables. such problems, and the convergence properties of the fitting al-\ngorithm, are discussed in [64, 84, 322].\n12.1.3.4 additivity and variance stabilization another additive model vari-\nant relying on transformation of the response is additivity and variance stabilization\n(avas) [631]. the model is the same as (12.16) except that g is constrained to be\nstrictly monotone with\n\n(12.17)\n\nsk(xk): = c\n\nvar9g(y)$$$$$\np\"k=1\n\nfor some constant c.\nto fit the model, the following iterative algorithm can be used:\n1. initialize the algorithm by letting t = 0 and \u02c6g(0)(yi) = (yi \u2212 \u00afy)/\u02c6\u03c3y, where \u02c6\u03c3y\nis the sample standard deviation of the yi values.\n2. initialize the predictor functions by fitting an additive model to the \u02c6g(0)(yi) and\n\n(0)\npredictor data, yielding \u02c6s\nk\n\nfor k = 1, . . . , p, as done for ace.\n(t)\n3. denotethecurrentmeanresponsefunctionas \u02c6\u00b5(t) =!p\nk=1 \u02c6s\nk (xk).toestimate\nthe variance-stabilizing transformation, we must first estimate the conditional\nvariance function of \u02c6g(t)(y) given \u02c6\u00b5(t) = u. this function, \u02c6v (t)(u), is estimated\nby smoothing the current log squared residuals against u and exponentiating\nthe result.\n4. given \u02c6v (t)(u), compute the corresponding variance-stabilizing transformation\n\n\u03c8(t)(z) =; z0 \u02c6v (t)(u)\u22121/2 du. this integration can be carried out using a numer-\n5. update and standardize the response transformation by defining \u02c6g(t+1)(y) =\n<\u03c8(t)/\u02c6g(t)(y)0 \u2212 \u00af\u03c8(t)=> \u02c6\u03c3\u03c8(t), where \u00af\u03c8(t) and \u02c6\u03c3\u03c8(t) denote the sample mean and\nstandard deviation of the \u03c8(t)/\u02c6g(t)(yi)0 values.\n\nical technique from chapter 5.\n\n "}, {"Page_number": 409, "text": "405\n6. update the predictor functions by fitting an additive model to the \u02c6g(t+1)(yi) and\n\n12.1 predictor\u2013response data\n\npredictor data, yielding \u02c6s\n\n(t+1)\ni=15 \u02c6g(t+1)(yi) \u2212!p\nk=1 \u02c6s\n\n7. if!n\n\nk\n\nfor k = 1, . . . , p, as done for ace.\n(xik)62 hasconvergedaccordingtoarelative\n(t+1)\n\nk\n\nconvergence criterion, stop. otherwise, increment t and go to step 3.\nunlike ace, the avas procedure is well suited for predictor\u2013response regres-\nsion problems. further details of this method are given by [322, 631].\nboth ace and avas can be used to suggest parametric transformations for\nstandard multiple regression modeling. in particular, plotting the ace or avas trans-\nformed predictor versus the untransformed predictor can sometimes suggest a simple\npiecewise linear or other transformation for standard regression modeling [157, 332].\n\n12.1.4 tree-based methods\ntree-based methods recursively partition the predictor space into subregions associ-\nated with increasingly homogeneous values of the response variable. an important\nappeal of such methods is that the fit is often very easy to describe and interpret. for\nreasons discussed shortly, the summary of the fit is called a tree.\nthe most familiar tree-based method for statisticians is the classification and\nregression tree (cart) method described by breiman, friedman, olshen, and stone\n[65].bothproprietaryandopen-sourcecodesoftwaretocarryouttree-basedmodeling\nare widely available [115, 228, 612, 629, 643]. while implementation details vary,\nall of these methods are fundamentally based on the idea of recursive partitioning.\n\na tree can be summarized by two sets of information:\n\u2022 the answers to a series of binary (yes\u2013no) questions, each of which is based on\nthe value of a single predictor\n\u2022 a set of values used to predict the response variable on the basis of answers to\nthese questions\n\nan example will clarify the nature of a tree.\nexample12.4 (streammonitoring) variousorganismscalledmacroinvertebrates\nlive in the bed of a stream, called the substrate. to monitor stream health, ecologists\nuse a measure called the index of biotic integrity (ibi), which quantifies the stream\u2019s\nability to support and maintain a natural biological community. an ibi allows for\nmeaningful measurement of the effect of anthropogenic and other potential stressors\non streams [363]. in this example, we consider predicting a macroinvertebrate ibi\nfrom two predictors, human population density and rock size in the substrate. the\nfirst predictor is the human population density (persons per square kilometer) in the\nstream\u2019s watershed. to improve graphical presentation, the log of population density\nis used in the analysis below, but the same tree would be selected were the untrans-\nformed predictor to be used. the second predictor is the estimated geometric mean\nof the diameter of rocks collected at the sample location in the substrate, measured in\nmillimeters and transformed logarithmically. these data, considered further in prob-\nlem 12.5, were collected by the environmental protection agency as part of a study\n\n "}, {"Page_number": 410, "text": "406\n\nchapter 12 multivariate smoothing\n\nrock size <!\n\n!0.4\n\nrock size <!\n\n!  \u22121.96\n\npopulation < 2.3\n\n1\n\nibi=20\n\npopulation <! 3.1\n\n4\n\nibi=68\n\n5\n\nibi=54\n\n2\n\nibi=45\n\n3\n\nibi=32\n\nfigure 12.7 tree fit to predict ibi for example 12.4. the root node is the top node of the\ntree, the parent nodes are the other nodes indicated by the \u2022 symbol, and the terminal nodes\nare n1, . . . ,n5. follow the left branch from a parent node when the indicated criterion is true,\nand the right branch when it is false.\nof 353 sites in the mid-atlantic highlands region of the eastern united states from\n1993 to 1998 [185].\nfigure 12.7 shows a typical tree. four binary questions are represented by splits\nin the tree. each split is based on the value of one of the predictors. the left branch\nof a split is taken when the answer is yes, so that the condition labeling that split is\nmet. for example, the top split indicates that the left portion of the tree is for those\nobservations with rock size values of 0.4 or less (sand and smaller). each position in\nthe tree where a split is made is a parent node. the topmost parent node is called the\nroot node. all parent nodes except the root node are internal nodes. at the bottom of\nthe tree the data have been classified into five terminal nodes based on the decisions\nmade at the parent nodes. associated with each terminal node is the mean value of\nthe ibi for all observations in that node. we would use this value as the prediction for\nany observation whose predictors lead to this node. for example, we predict ibi = 20\nfor any observation that would be classified in n1.\n!\n12.1.4.1 recursive partitioning regression trees suppose initially that the\nresponse variable is continuous. then tree-based smoothing is often called recur-\nsive partitioning regression. section 12.1.4.3 discusses prediction of categorical\nresponses.\nconsider predictor\u2013response data where xi is a vector of p predictors associated\nwith a response yi, for i = 1, . . . , n. for simplicity, assume that all p predictors are\ncontinuous. let q denote the number of terminal nodes in the tree to be fitted.\ntree-based predictions are piecewise constant. if the predictor values for the ith\nobservation place it in the jth terminal node, then the ith predicted response equals a\nconstant, \u02c6aj. thus, the tree-based smooth is\n\n\u02c6s(xi) =\n\nq\"j=1\n\n\u02c6aj1{xi\u2208nj}.\n\n(12.18)\n\n!\n!\n!\n!\n!\nn\nn\nn\nn\nn\n "}, {"Page_number": 411, "text": "12.1 predictor\u2013response data\n\n407\nthis model is fitted using a partitioning process that adaptively partitions predictor\nspace into hyperrectangles, each corresponding to one terminal node. once the par-\ntitioning is complete, \u02c6aj is set equal to, say, the mean response value of cases falling\nin the jth terminal node.\nnotice that this framework implies that there are a large number of possible\ntrees whenever n and/or p are not trivially small. any terminal node might be split\nto create a larger tree. the two branches from any parent node might be collapsed to\nconvert the parent node to a terminal node, forming a subtree of the original tree. any\nbranch itself might be replaced by one based on a different predictor variable and/or\ncriterion. the partitioning process used to fit a tree is described next.\nin the simplest case, suppose q = 2. then we seek to split \u211cp into two hyper-\nrectangles using one axis-parallel boundary. the choice can be characterized by a\nsplit coordinate, c \u2208 {1, . . . , p}, and a split point or threshold, t \u2208 \u211c. the two termi-\nnal nodes are then n1 = {xi : xic < t} and n2 = {xi : xic \u2265 t}. denote the sets of\nindices of the observations falling in the two nodes as s1 and s2, respectively. using\nnode-specific sample averages yields the fit\n\n\u02c6s(xi) = 1{i\u2208s1}\"j\u2208s1\n\nyj\n\nn1 + 1{i\u2208s2}\"j\u2208s2\n\nyj\nn2\n\n,\n\n(12.19)\n\nwhere nj is the number of observations falling in the jth terminal node.\nfor continuous predictors and ordered discrete predictors, defining a split in this\nmanner is straightforward. treatment of an unordered categorical variable is different.\nsuppose each observation of this variable may take on one of several categories.\nthe set of all such categories must be partitioned into two subsets. fortunately, we\nmay avoid considering all possible partitions. first, order the categories in order\nof the average response within each category. then, treat these ordered categories\nas if they were observations of an ordered discrete predictor. this strategy permits\noptimal splits [65]. there are also natural ways to deal with observations having some\nmissingpredictorvalues.finally,selectingtransformationsofthepredictorsisusually\nnot a problem: tree-based models are invariant to monotone transformations of the\npredictors because the split point is determined in terms of the rank of predictor, in\nmost software packages.\ntofindthebesttreewith q = 2 terminalnodes,weseektominimizetheresidual\nsquared error,\n\n(12.20)\n\n(yi \u2212 \u02c6aj)2\n\nq\"j=1\"i\u2208sj\nrss(c, t) =\nwith respect to c and t, where \u02c6aj =!i\u2208sj\nyi/nj. note that the sj are defined using\nthe values of c and t, and that rss(c, t) changes only when memberships change in\nthe sets sj. minimizing (12.20) is therefore a combinatorial optimization problem.\nfor each coordinate, we need to try at most n \u2212 1 splits, and fewer if there are tied\npredictor values in the coordinate. therefore, the minimal rss(c, t) can be found by\nsearching at most p(n \u2212 1) trees. exhaustive search to find the best tree is feasible\nwhen q = 2.\n\n "}, {"Page_number": 412, "text": "408\n\nchapter 12 multivariate smoothing\n\n1\n\n3\n\n5\n\n6\n\n4\n\n2\n\n \n\ny\nt\ni\ns\nn\ne\nd\nn\no\ni\nt\na\nl\nu\np\no\np\n\n0\n\n2\n\n\u22122\n\n\u22121\n\n0\n\n4\n\n2\n\n3\n\n1\nrock size\n\nfigure 12.8 partitioning of predictor space (rock size and population density variables)\nfor predicting ibi as discussed in examples 12.4 and 12.5.\n\nnow suppose q = 3. a first split coordinate and split point partition\u211cp into two\nhyperrectangles. one of these hyperrectangles is then partitioned into two portions\nusing a second split coordinate and split point, applied only within this hyperrect-\nangle. the result is three terminal nodes. there are at most p(n \u2212 1) choices for\nthe first split. for making the second split on any coordinate different from the one\nused for the first split, there are at most p(n \u2212 1) choices for each possible first split\nchosen. for a second split on the same coordinate as the first split, there are at most\np(n \u2212 2) choices. carrying this logic on for larger q, we see that there are about\n(n \u2212 1)(n \u2212 2)\u00b7\u00b7\u00b7(n \u2212 q + 1)pq\u22121 trees to be searched. this enormous number\ndefies exhaustive search.\ninstead, a greedy search algorithm is applied (see section 3.2). each split is\ntreated sequentially. the best single split is chosen to split the root node. for each\nchild node, a separate split is chosen to split it optimally. note that the q terminal\nnodes obtained in this way will usually not minimize the residual squared error over\nthe set of all possible trees having q terminal nodes.\nexample 12.5 (stream monitoring, continued)\nto understand how terminal\nnodes in a tree correspond to hyperrectangles in predictor space, recall the stream\nmonitoring data introduced in example 12.4. another representation of the tree in\nfigure 12.7 is given in figure 12.8. this plot shows the partitioning of the predictor\nspace determined by values of the rock size and population density variables. each\ncircle is centered at an xi observation (i = 1, . . . , n). the area of each circle reflects\nthe magnitude of the ibi value for that observation, with larger circles corresponding\nto larger ibi values. the rectangular regions labeled n1, . . . ,n5 in this graph cor-\nrespond to the terminal nodes shown in figure 12.7. the first split (on the rock size\ncoordinate at the threshold t = 0.4) is shown by the vertical line in the middle of the\n\nn\nn\nn\nn\nn\n "}, {"Page_number": 413, "text": "12.1 predictor\u2013response data\n\n409\n\n0\n\n1\n\n2\n\n4\n\n3\n\npopulation density\n\n5\n\n0\n\n\u22121\n\n\u22122\n\n6\n\n7\n\ntree model predictions for\n\nibi as discussed in\n\n0\n7\n\n0\n5\n\ni\n\nb\n\ni\n\n0\n3\n\n0\n1\n\n4\n\n3\n\n2\n1\nrock size\nfigure 12.9 piecewise constant\nexample 12.5.\n\nplot. subsequent splits partition only portions of the predictor space. for example, the\nregion corresponding to rock size exceeding 0.4 is next split into two nodes, n4 and\nn5, based on the value of the population density variable. note that sequential split-\nting has drawbacks: an apparently natural division of the data based on whether the\npopulation density exceeds about 2.5 is represented by two slightly mismatched splits\nbecause a previous split occurred at 0.4 on the rock size variable. the uncertainty of\nthe tree structure is discussed further in section 12.1.4.4.\nthe piecewise constant model for this fitted tree is shown in figure 12.9, with\nthe ibi on the vertical axis. to best display the surface, the axes have been reversed\ncompared to figure 12.8.\n!\ntree pruning for a given q, greedy search can be used to fit a tree\n12.1.4.2\nmodel. note that q is, in essence, a smoothing parameter. large values of q retain\nhigh fidelity to the observed data but provide trees with high potential variation in\npredictions. such an elaborate model may also sacrifice interpretability. low values\nof q provide less predictive variability because there are only a few terminal nodes,\nbut predictive bias may be introduced if responses are not homogeneous within each\nterminal node. we now discuss how to choose q.\na naive approach for choosing q is to continue splitting terminal nodes until\nno additional split gives a sufficient reduction in the overall residual sum of squares.\nthis approach may miss important structure in the data because subsequent splits\nmay be quite valuable even if the current split offers little or no improvement. for\nexample, consider the saddle-shaped response surface obtained when x1 and x2 are\nindependent predictors distributed uniformly over [\u22121,1] and y = x1x2. then no\nsingle split on either predictor variable will be of much benefit, but any first split\nenables two subsequent splits that will greatly reduce the residual sum of squares.\na more effective strategy for choosing q begins by growing the tree, splitting\neach terminal node until it has no more than some prespecified minimal number of\nobservations in it or its residual squared error does not exceed some prespecified\n\n "}, {"Page_number": 414, "text": "chapter 12 multivariate smoothing\n\n410\npercentage of the squared error for the root node. the number of terminal nodes in\nthis full tree may greatly exceed q. then, terminal nodes are sequentially recombined\nfrom the bottom up a way that doesn\u2019t greatly inflate the residual sum of squares.\none implementation of this strategy is called cost\u2013complexity pruning [65, 540]. the\nfinal tree is a subtree of the full tree, selected according to a criterion that balances a\npenalty for prediction error and a penalty for tree complexity.\nlet the full tree be denoted by t0, and let t denote some subtree of t0 that can\nbe obtained by pruning everything below some parent nodes in t0. let q(t) denote\nthe number of terminal nodes in the tree t. the cost\u2013complexity criterion is given by\n(12.21)\nwhere r(t) is the residual sum of squares or some other measure of prediction error\nfor tree t, and \u03b1 is a user-supplied parameter that penalizes tree complexity. for a\ngiven \u03b1, the optimal tree is the subtree of t0 that minimizes r\u03b1(t). when \u03b1 = 0, the\nfull tree, t0, will be selected as optimal. when \u03b1 = \u221e, the tree with only the root\nnode will be selected. if t0 has q(t0) terminal nodes, then there are at most q(t0)\nsubtrees that can be obtained by choosing different values of \u03b1.\nthe best approach for selecting the value for the parameter \u03b1 in (12.21) relies\non cross-validation. the dataset is partitioned into v separate portions of equal size,\nwhere v is typically between 3 and 10. for a finite sequence of \u03b1 values, the algorithm\nproceeds as follows:\n\nr\u03b1(t) = r(t) + \u03b1q(t),\n\n1. remove one of the v parts of the dataset. this subset is called the validation\n\n2. find the optimal subtree for each value of \u03b1 in the sequence using the remaining\n\nset.\nv \u2212 1 parts of the data.\n3. for each optimal subtree, predict the validation-set responses, and compute the\ncross-validated sum of squared error based on these validation-set predictions.\nrepeat this process for all v parts of the data. for each \u03b1, compute the total cross-\nvalidated sum of squares over all v data partitions. the value of \u03b1 that minimizes the\ncross-validated sum of squares is selected; call it \u02c6\u03b1. having estimated the best value\nfor the complexity parameter, we may now prune the full tree for all the data back to\nthe subtree determined by \u02c6\u03b1.\nefficient algorithms for finding the optimal tree for a sequence of \u03b1 values (see\nstep 2 above) are available [65, 540]. indeed, the set of optimal trees for a sequence\nof \u03b1 values is nested, with smaller trees corresponding to larger values of \u03b1, and\nall members in the sequence can be visited by sequential recombination of terminal\nnodes from the bottom up. various enhancements of this cross-validation strategy\nhave been proposed, including a variant of the above approach that chooses the sim-\nplest tree among those trees that nearly achieve the minimum cross-validated sum of\nsquares [629].\nexample 12.6 (stream monitoring, continued)\nlet us return to the stream\necology example introduced in example 12.4. a full tree for these data was obtained\nby splitting until every terminal node has fewer than 10 observations in it or has\nresidual squared error less 1% of the residual squared error for the root node. this\n\n "}, {"Page_number": 415, "text": "12.1 predictor\u2013response data\n\n411\n\n35,000\n\n2300\n\n\u03b1\n1000\n\n610\n\n240\n\n0\n0\n0\n\n,\n\n0\n1\n/\n\ns\ns\nr\nv\nc\n\n1\n2\n\n0\n2\n\n9\n1\n\n8\n1\n\n7\n1\n\n1\n\n10\n\n20\n\n30\n\n40\n\n50\n\nnumber of terminal nodes\n\nfigure 12.10 cross-validation residual sum of squares versus node size for example 12.6.\nthe top horizonal axis shows the cost\u2013complexity parameter \u03b1.\n\nprocess produced a full tree with 53 terminal nodes. figure 12.10 shows the total\ncross-validated residual squared error as a function of the number of terminal nodes.\nthis plot was produced using 10-fold cross-validation (v = 10). the full tree can be\npruned from the bottom up, recombining the least beneficial terminal nodes, until the\nminimal value of r\u03b1(t) is reached. note that the correspondence between values of\n\u03b1 and tree sizes means that we need only consider a limited collection of \u03b1 values,\nand it is therefore more straightforward to plot r\u03b1(t) against q(t) instead of plotting\nagainst \u03b1. the minimal cross-validated sum of squares is achieved for a tree with five\nterminal nodes; indeed, this is the tree shown in figure 12.7.\nfor this example, the selection of the optimal \u03b1, and thus the final tree, varies\nwith different random partitions of the data. the optimal tree typically has between\n3 and 13 terminal nodes. this uncertainty emphasizes the potential structural in-\nstability of tree-based models, particularly for datasets where the signal is not\nstrong.\n!\n12.1.4.3 classification trees digressing briefly from this chapter\u2019s focus\non smoothing, it is worthwhile here to quickly summarize tree-based methods for\ncategorical response variables.\nrecursive partitioning models for predicting a categorical response variable\nare typically called classification trees [65, 540]. let each response observation yi\ntake on one of m categories. let \u02c6pjm denote the proportion of observations in the\nterminal node nj that are of class m (for m = 1, . . . , m). loosely speaking, all the\nobservations in nj are predicted to equal the class that makes up the majority in\nthat node. such prediction by majority vote within terminal nodes can be modified in\ntwo ways. first, votes can be weighted to reflect prior information about the overall\nprevalence of each class. this permits predictions to be biased toward predominant\nclasses. second, votes can be weighted to reflect different losses for different types\n\n "}, {"Page_number": 416, "text": "chapter 12 multivariate smoothing\n\n412\nof misclassification [629]. for example, if classes correspond to medical diagnoses,\nsome false positive or false negative diagnoses may be grave errors, while other\nmistakes may have only minor consequences.\nconstruction of a classification tree relies on partitioning the predictor space\nusing a greedy strategy similar to the one used for recursive partitioning regression.\nfor a regression tree split, the split coordinate c and split point t are selected by\nminimizing the total residual sum of squares within the left and right children nodes.\nfor classification trees, a different measure of error is required. the residual squared\nerror is replaced by a measure of node impurity.\ntherearevariousapproachestomeasuringnodeimpurity,butmostarebasedon\nthe following principle. the impurity of node j should be small when observations\nin that node are concentrated within one class, and the impurity should be large\nwhen they are distributed uniformly over all m classes. two popular measures of\nimpurity are the entropy, given for node j by!m\nm=1 \u02c6pjm log \u02c6pjm, and the gini index,\ngiven by!\u2113 /= m \u02c6pjl \u02c6pjm. these approaches are more effective than simply counting\nmisclassifications because a split may drastically improve the purity in a node without\nchanging any classifications. this occurs, for example, when majority votes on both\nsides of a split have the same outcome as the unsplit vote, but the margin of victory\nin one of the subregions is much narrower than in the other.\ncost\u2013complexitytreepruningcanproceedusingthesamestrategiesdescribedin\nsection12.1.4.2.theentropyortheginiindexcanbeusedforthecostmeasure r(t)in\n(12.21). alternatively, one may let r(t) equal a (possibly weighted) misclassification\nrate to guide pruning.\n12.1.4.4 other issues for tree-based methods tree-based methods offer\nseveral advantages over other, more traditional modeling approaches. first, tree mod-\nels fit interactions between predictors and other nonadditive behavior without re-\nquiring formal specification of the form of the interaction by the user. second, there\nare natural ways to use data with some missing predictor values, both when fit-\nting the model and when using it to make predictions. some strategies are surveyed\nin [64, 540].\none disadvantage is that trees can be unstable. therefore, care must be taken\nnot to overinterpret particular splits. for example, if the two smallest ibi values in\nn1 in figure 12.8 are increased somewhat, then this node is omitted when a new\ntree is constructed on the revised data. new data can often cause quite different splits\nto be chosen even if predictions remain relatively unchanged. for example, from\nfigure 12.8 it is easy to imagine that slightly different data could have led to the root\nnode splitting on population density at the split point 2.5, rather than on rock size at\nthe point 0.4. trees can also be unstable in that building the full tree to a different\nsize before pruning can cause a different optimal tree to be chosen after pruning.\nanother concern is that assessment of uncertainty can be somewhat challeng-\ning. there is no simple way to summarize a confidence region for the tree structure\nitself. confidence intervals for tree predictions can be obtained using the bootstrap\n(chapter 9).\ntree-based methods are popular in computer science, particular for classifi-\ncation [522, 540]. bayesian alternatives to tree methods have also been proposed\n\n "}, {"Page_number": 417, "text": "413\n[112, 151]. medical applications of tree-based methods are particularly popular, per-\nhaps because the binary decision tree is simple to explain and to apply as a tool in\ndisease diagnosis [65, 114].\n\n12.2 general multivariate data\n\n12.2 general multivariate data\nfinally, we consider high-dimensional data that lie near a low-dimensional manifold\nsuch as a curve or surface. for such data, there may be no clear conceptual separation\nof the variables into predictors and responses. nevertheless, we may be interested in\nestimating smooth relationships among variables. in this section we describe one ap-\nproach, called principal curves, for smoothing multivariate data. alternative methods\nfor discovering relationships among variables, such as association rules and cluster\nanalysis, are discussed in [323].\n\n12.2.1 principal curves\na principal curveisaspecialtypeofone-dimensionalnonparametricsummaryofa p-\ndimensional general multivariate dataset. loosely speaking, each point on a principal\ncurve is the average of all data that project onto that point on the curve. we began\nmotivatingprincipalcurvesinsection11.6.thedatainfigure11.18werenotsuitable\nfor predictor\u2013response smoothing, yet adapting the concept of smoothing to general\nmultivariate data allowed the very good fit shown in the right panel of figure 11.18.\nwe now describe more precisely the notion of a principal curve and its estimation\n[321]. related software includes [319, 367, 644].\n12.2.1.1 definition and motivation general multivariate data may lie near a\nconnected, one-dimensional curve snaking through \u211cp. it is this curve we want to\nestimate. we adopt a time\u2013speed parameterization of curves below to accommodate\nthe most general case.\nwe can write a one-dimensional curve in \u211cp as f(\u03c4) = (f1(\u03c4), . . . , fp(\u03c4)) for \u03c4\nbetween \u03c40 and \u03c41. here, \u03c4 can be used to indicate distance along the one-dimensional\n\u03c40 \u2225f\u2032(\u03c4)\u2225 d\u03c4, where\n\ncurve in p-dimensional space. the arc length of a curve f is; \u03c41\nd\u03c4 a2\n+ \u00b7\u00b7\u00b7 +@ dfp(\u03c4)\n\n\u2225f\u2032(\u03c4)\u2225 =?@ df1(\u03c4)\nd\u03c4 a2\n\nif \u2225f\u2032(\u03c4)\u2225 = 1 for all \u03c4 \u2208 [\u03c40, \u03c41], then the arc length between any two points \u03c4a\nand \u03c4b along the curve is |\u03c4a \u2212 \u03c4b|. in this case, f is said to have the unit-speed\nparameterization. it is often helpful to imagine a bug walking forward along the\ncurve at a speed of 1 or backward with at a speed of \u22121 (the designation of forward\nand backward is arbitrary). then the amount of time it takes the bug to walk between\ntwo points corresponds to the arc length, and the positive or negative sign corresponds\nto the direction taken. any smooth curve with \u2225f\u2032(\u03c4)\u2225 > 0 for all \u03c4 \u2208 [\u03c40, \u03c41] can be\nreparameterized to unit speed. if the coordinate functions of a unit-speed curve are\nsmooth, then f itself is smooth.\n\n.\n\n "}, {"Page_number": 418, "text": "414\n\nchapter 12 multivariate smoothing\n\n2\nx\n\n1\n\n0\n\n\u22121\n\n2\nx\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n\u22121\n\n0\n\n\u22122\n\n\u22121\n\n0\n\nx1\n\nx1\n\nfigure 12.11 two panels illustrating the definition of a principal curve and its estimation.\nin the left panel, the curve f is intersected by an axis that is orthogonal to f at a particular \u03c4\u2217.\na conditional density curve is sketched over this axis; if f is a principal curve, then the mean\nof this conditional density must equal f(\u03c4\u2217). in the right panel, a neighborhood around \u03c4\u2217 is\nsketched. within these boundaries, all points project onto f near \u03c4\u2217. the sample mean of these\npoints should be a good approximation to the true mean of the conditional density in the left\npanel.\n\n\u03c4f(x) = sup\n\nthe types of curves we are interested in estimating are smooth, nonintersecting,\ncurves that aren\u2019t too wiggly. specifically, let us assume that f is a smooth unit-speed\ncurve in \u211cp parameterized over the closed interval [\u03c40, \u03c41] such that f(t) /= f(r) when\nr /= t for all r, t \u2208 [\u03c40, \u03c41], and f has finite length inside any closed ball in \u211cp.\nfor any point x \u2208 \u211cp, define the projection index function as \u03c4f(x) : \u211cp \u2192 \u211c1\naccording to\n(12.22)\n\n\u03c4 #\u03c4 : \u2225x \u2212 f(\u03c4)\u2225 = inf\n\nr \u2225x \u2212 f(r)\u2225% .\n\nthus \u03c4f(x) is the largest value of \u03c4 for which f(\u03c4) is closest to x. points with simi-\nlar projection indices project orthogonally onto a small portion of the curve f. the\nprojection index will later be used to define neighborhoods.\nsuppose that x is a random vector in \u211cp, having a probability density with\nfinite second moment. unlike in previous sections, we cannot distinguish variables\nas predictors and response.\nwe define f to be a principal curve if f(\u03c4\u2217) = e{x|\u03c4f(x) = \u03c4\u2217} for all \u03c4\u2217 \u2208\n[\u03c40, \u03c41]. this requirement is sometimes termed self-consistency. figure 12.11 illus-\ntrates this notion that the distribution of points orthogonal to the curve at some \u03c4 must\nhave mean equal to the curve itself at that point. in the left panel, a distribution is\nsketched along an axis that is orthogonal to f at one \u03c4\u2217. the mean of this density is\nf(\u03c4\u2217). note that for ellipsoid distributions, the principal component lines are principal\ncurves. principal components are reviewed in [471].\nprincipal curves are motivated by the concept of local averaging: the principal\ncurve connects the means of points in local neighborhoods. for predictor\u2013response\nsmooths,neighborhoodsaredefinedalongthepredictorcoordinateaxes.forprincipal\ncurves, neighborhoods are defined along the curve itself. points that project nearby\n\n "}, {"Page_number": 419, "text": "415\non the curve are in the same neighborhood. the right panel of figure 12.11 illustrates\nthe notion of a local neighborhood along the curve.\n\n12.2 general multivariate data\n\nestimation an iterative algorithm can be used to estimate a principal\n12.2.1.2\ncurvefromasampleof p-dimensionaldata,x1, . . . ,xn.thealgorithmisinitializedat\niteration t = 0bychoosingasimplestartingcurve \u02c6f(0)(\u03c4)andsetting \u03c4(0)(x) = \u03c4\u02c6f(0)(x)\nfrom (12.22). one reasonable choice would be to set \u02c6f(0)(\u03c4) = \u00afx + a\u03c4, where a is the\nfirst linear principal component estimated from the data. the algorithm proceeds as\nfollows:\n1. smooth the kth coordinate of the data. specifically, for k = 1, . . . , p, smooth\nxik against \u03c4(t)(xi)usingastandardbivariatepredictor\u2013responsesmootherwith\nspan h(t). the projection of the points xi onto \u02c6f(t) for i = 1, . . . , n provides\nthe predictors \u03c4(t)(xi). the responses are the xik. the result is \u02c6f(t+1), which\nservesasanestimateof e{x|\u03c4(t)(x)}.thisimplementsthescatterplotsmoothing\nstrategy of locally averaging the collection of points that nearly project onto\nthe same point on the principal curve.\n2. interpolate between the \u02c6f(t+1)(xi) for i = 1, . . . , n, and compute \u03c4\u02c6f(t+1)(xi) as\nthe distances along \u02c6f(t+1). note that some xi may project onto a quite different\nsegment than they did at the previous iteration.\n3. let \u03c4(t+1)(x) equal \u03c4\u02c6f(t+1)(x) transformed to unit speed. this amounts to rescal-\ning the \u03c4\u02c6f(t+1)(xi) so that each equals the total distance traveled along the polyg-\nonal curve to reach it.\n4. evaluate the convergence of \u02c6f(t+1), and stop if possible; otherwise, increment\nt and return to step 1. a relative convergence criterion could be constructed\nbased on the total error,!n\n\nthe result of this algorithm is a piecewise linear polygonal curve that serves as\nthe estimate of the principal curve.\nthe concept of principal curves can be generalized for multivariate responses.\nfor this purpose, principal surfaces are defined analogously to the above. the surface\nis parameterized by a vector \u03c4, and data points are projected onto the surface. points\nthat project anywhere on the surface near \u03c4\u2217 dominate in the local smooth at \u03c4\u2217.\n\ni=1 \u2225xi \u2212 \u02c6f(t+1)(\u03c4(t+1)(xi))\u2225.\n\nexample 12.7 (principal curve for bivariate data) figure 12.12 illustrates sev-\neral steps during the iterative process of fitting a principal curve. the sequence of\npanels should be read across the page from top left to bottom right. in the first panel,\nthe data are plotted. the solid line shaped like a square letter c is \u02c6f(0). each data point\nis connected to \u02c6f(0) by a line showing its orthogonal projection. as a bug walks along\n\u02c6f(0)(\u03c4) from the top right to the bottom right, \u03c4(0)(x) increases from 0 to about 7.\nthe second and third panels show each coordinate of the data plotted against the\nprojection index, \u03c4(0)(x). these coordinatewise smooths correspond to step 1 of the\nestimation algorithm. a smoothing spline was used in each panel, and the resulting\noverall estimate, \u02c6f(1), is shown in the fourth panel. the fifth panel shows \u02c6f(2). the\nsixth panel gives the final result when convergence was achieved.\n!\n\n "}, {"Page_number": 420, "text": "416\n\nchapter 12 multivariate smoothing\n\n\u22122\n\n\u22121\nx1\n\n0\n\n2\nx\n\n1\n\n0\n\n\u22121\n\n1\n\n2\nx\n\n0\n\n\u22121\n\n0\n\n1\nx\n\n\u22121\n\n\u22122\n\n0\n\n1\n\n2\nx\n\n0\n\n\u22121\n\n2\nx\n\n1\n\n0\n\n\u22121\n\n3\n\n\u03c4 (0)(x)\n\n6\n\n0\n\n3\n\n\u03c4 (0)(x)\n\n6\n\n1\n\n2\nx\n\n0\n\n\u22121\n\n\u22122\n\n\u22121\n\n0\n\n\u22122\n\n\u22121\n\n0\n\n\u22122\n\n\u22121\n\n0\n\nx1\n\nx1\n\nx1\n\nfigure 12.12 these panels illustrate the progression of the iterative fit of a principal curve.\nsee example 12.7 for details.\n\nspan selection theprincipalcurvealgorithmdependsontheselection\n12.2.1.3\nof a span h(t) at each iteration. since the smoothing is done coordinatewise, different\nspans could be used for each coordinate at each iteration, but in practice it is more\nsensible to standardize the data before analysis and then use a common h(t).\nnevertheless, the selection of h(t) from one iteration to the next remains an\nissue. the obvious solution is to select h(t) via cross-validation at each iteration.\nsurprisingly, this doesn\u2019t work well. pervasive undersmoothing arises because the\nerrors in the coordinate functions are autocorrelated. instead, h(t) = h can be chosen\nsensibly and remain unchanged until convergence is achieved. then, one additional\niteration of step 1 can be done with a span chosen by cross-validation.\nthis span selection approach is troubling because the initial span choice clearly\ncan affect the shape of the curve to which the algorithm converges. when the span\nis then cross-validated after convergence, it is too late to correct \u02c6f for such an error.\nnevertheless, the algorithm seems to work well on a variety of examples where\nordinary smoothing techniques would fail catastrophically.\n\nproblems\n12.1. for a defined as in (12.5), smoothing matrices sk, and n-vectors \u03b3 k for k = 1, . . . , p,\nlet ik be the space spanned by vectors that pass through sk unchanged (i.e., vectors\nv satisfying skv = v). prove that a\u03b3 = 0 (where \u03b3 = (\u03b31 \u03b32 . . . \u03b3 p)t) if and only if\n\u03b3 k \u2208 ik for all k and!p\n\nk=1 \u03b3 k = 0.\n\n "}, {"Page_number": 421, "text": "12.2 general multivariate data\n\n417\n\ntable 12.2 potential predictors of body fat. predic-\ntors 4\u201313 are circumference measurements given in\ncentimeters.\n\n1. age (years)\n2. weight (pounds)\n3. height (inches)\n4. neck\n5. chest\n6. abdomen\n7. hip\n\n8. thigh\n9. knee\n10. ankle\n11. extended biceps\n12. forearm\n13. wrist\n\n12.2. accurate measurement of body fat can be expensive and time consuming. good mod-\nels to predict body fat accurately using standard measurements are still useful in many\ncontexts. a study was conducted to predict body fat using 13 simple body measure-\nments on 251 men. for each subject, the percentage of body fat as measured using\nan underwater weighing technique, age, weight, height, and 10 body circumference\nmeasurements were recorded (table 12.2). further details on this study are available\nin [331, 354]. these data are available from the website for this book. the goal of this\nproblem is to compare and contrast several multivariate smoothing methods applied\nto these data.\na. using a smoother of your own choosing, develop a backfitting algorithm to fit an\nadditive model to these data as described in section 12.1.1. compare the results of\nthe additive model with those from a multiple regression model.\n\nb. useanyavailablesoftwaretoestimatemodelsforthesedata,usingfivemethods:(1)\nthe standard multiple linear regression (mlr) model, (2) an additive model (am),\n(3) projection pursuit regression (ppr), (4) the alternating conditional expecta-\ntions (ace) procedure, and (5) the additivity and variance stabilization (avas)\napproach.\ni. for mlr, am, ace, and avas, plot the kth estimated coordinate smooth\nagainst the observed values of the kth predictor for k = 1, . . . ,13. in other\nwords,graphthevaluesof \u02c6sk(xik)versusxik for i = 1, . . . ,251asinfigure12.2.\nfor ppr, imitate figure 12.6 by plotting each component smooth against the\nprojection coordinate. for all methods, include the observed data points in\nan appropriate way in each graph. comment on any differences between the\nmethods.\n\nii. carry out leave-one-out cross-validation analyses where the ith cross-validated\nresidual is computed as the difference between the ith observed response and\nthe ithpredictedresponseobtainedwhenthemodelisfittedomittingthe ithdata\npoint from the dataset. use these results to compare the predictive performance\nof mlr, am, and ppr using a cross-validated residual sum of squares similar\nto (11.16).\n\n12.3. for the body fat data of problem 12.2, compare the performance of at least three\ndifferentsmoothersusedwithinanadditivemodeloftheformgivenin(12.3).compare\nthe leave-one-out cross-validation mean squared prediction error for the different\nsmoothers. is one smoother superior to another in the additive model?\n\n "}, {"Page_number": 422, "text": "418\n\nchapter 12 multivariate smoothing\n\n12.4. example 2.5 describes a generalized linear model for data derived from testing an\nalgorithm for human face recognition. the data are available from the book website.\nthe response variable is binary, with yi = 1 if two images of the same person were\ncorrectly matched and yi = 0 otherwise. there are three predictor variables. the first\nis the absolute difference in eye region mean pixel intensity between the two images\nof the ith person. the second is the absolute difference in nose\u2013cheek region mean\npixel intensity between the two images. the third predictor compares pixel intensity\nvariability between the two images. for each image of the ith person, the median\nabsolute deviation (a robust spread measure) of pixel intensity is computed in two\nimage areas: the forehead and nose\u2013cheek regions. the third predictor is the between-\nimage ratio of these within-image ratios. fit a generalized additive model to these\ndata. plot your results and interpret. compare your results with the fit of an ordinary\nlogistic regression model.\n\n12.5. consider a larger set of stream monitoring predictors of the index of biotic integrity\nfor macroinvertebrates considered in example 12.4. the 21 predictors, described in\nmore detail in the website for this book, can be grouped into four categories:\n\nsite chemistry measures: acid-neutralizing capacity, chloride, specific conductance,\n\ntotal nitrogen, ph, total phosphorus, sulfate\n\nsite habitat measures: substrate diameter, percent fast water, canopy density above\n\nmidchannel, channel slope\n\nsite geographic measures: elevation, longitude, latitude, mean slope above site\nwatershed measures: watershed area above site; human population density; percent-\n\nages of agricultural, mining, forest, and urban land cover\n\na. construct a regression tree to predict the ibi.\nb. compare the performance of several strategies for tree pruning. compare the\n10-fold cross-validated mean squared prediction errors for the final trees selected\nby each strategy.\n\nc. the variables are categorized above into four groups. create a regression tree\nusing only the variables from each group in turn. compare the 10-fold cross-\nvalidated mean squared prediction errors for the final trees selected for each group\nof predictors.\n\n12.6. discuss how the combinatorial optimization methods from chapter 3 might be used\n\nto improve tree-based methods.\n\nbut f is not a principal curve for x.\n\n12.7. find an example for which x = f(\u03c4) + \u03f5, where \u03f5 is a random vector with mean zero,\n12.8. the website for this book provides some artificial data suitable for fitting a principal\ncurve. there are 50 observations of a bivariate variable, and each coordinate has been\nstandardized. denote these data as x1, . . . ,x50.\na. plot the data. let \u02c6f(0) correspond to the segment of the line through the origin with\nslope 1 onto which the data project. superimpose this line on the graph. imitating\nthe top left panel of figure 12.12, show how the data project onto \u02c6f(0).\nb. compute \u03c4(0)(xi) for each data point xi. transform to unit speed. hint: show why\nthe transformation atxi works, where a = (\u221a2/2,\u221a2/2)t.\n\n "}, {"Page_number": 423, "text": "12.2 general multivariate data\n\n419\n\nc. for each coordinate of the data in turn, plot the data values for that coordinate (i.e.,\nthe xik values for i = 1, . . . ,50 and k = 1 or k = 2) against the projection index\nvalues, \u03c4(0)(xi). smooth the points in each plot, and superimpose the smooth on\neach graph. this mimics the center and right top panels of figure 12.12.\nd. superimpose \u02c6f(1) over a scatterplot of the data, as in the bottom left panel of\n\nfigure 12.12.\n\ne. advanced readers may consider automating and extending these steps to produce\nan iterative algorithm whose iterates converges to the estimated principal curve.\nsome related software for fitting principal curves is available as packages for r\n(www.r-project.org).\n\n "}, {"Page_number": 424, "text": "data acknowledgments\n\nthe datasets used in the examples and exercises discussed in this book are available\nfrom the website www.colostate.edu/computationalstatistics/. many of these were\ncollected by scientific researchers in diverse fields. the remaining data are owned by\nus or have been simulated for instructional purposes. further details about ownership\nof some datasets are given below.\nwe thank richard barker, department of statistics, university of otago, new\nzealand, for the fur seal pup data used in section 7.6 and for hosting us during a great\nsabbatical.\nwe thank ross beveridge and bruce draper, department of computer science,\ncolorado state university, for the face recognition data used in example 2.5, and for\nthe opportunity to collaborate with them on this interesting project.\nwethankgordonreese,departmentoffisheryandwildlifebiology,colorado\nstate university, for his assistance in extracting the utah serviceberry data used in\nchapter 8.\nwe thank alan herlihy, department of fisheries and wildlife, oregon state\nuniversity, for providing the stream monitoring data used in example 12.4 and for\nhis assistance with the interpretation of the results. these data and the data used in\nproblem 12.5 were produced by the u.s. environmental protection agency through\nits environmental monitoring and assessment program (emap) [185, 639].\nthe leukemia data in problem 2.3 are used with permission, and are taken from\n[202]. copyright american society of hematology, 1963.\nthe oil spill data in problem 2.5 are derived from data given in [11] and are\nused with permission from elsevier. copyright elsevier 2000.\nthe alzheimer\u2019s data used throughout chapter 5 are reprinted with permission\nfrom [155]. copyright crc press, boca raton, florida, 2002.\nthe pigment moisture data in problem 7.8 are reprinted from [58] with permis-\nsion of john wiley & sons, inc. copyright john wiley & sons, inc. 1978.\nthe copper\u2013nickel alloy data used throughout chapter 9 are reprinted from\n[170] with permission of john wiley & sons, inc. copyright john wiley & sons, inc.\n1966.\n\ncomputational statistics, second edition. geof h. givens and jennifer a. hoeting.\n\u00a9 2013 john wiley & sons, inc. published 2013 by john wiley & sons, inc.\n\n421\n\n "}, {"Page_number": 425, "text": "422\n\ndata acknowledgments\nthe air blast data in problem 11.6 are reprinted from [342] with permission\nfrom elsevier. copyright elsevier 2001.\nthenorwegianpaperdatainchapter12arereprintedfrom[9],withpermission\nfrom elsevier. copyright elsevier 1996.\nthe remaining real datasets are in the public domain and/or are acknowledged\nat the locations in the text where we first use the data. we thank all these authors and\nresearchers.\n\n "}, {"Page_number": 426, "text": "references\n\n1. e. h. l. aarts and p. j. m. van laarhoven. statistical cooling: a general approach to\ncombinatorial optimization problems. philips journal of research, 40:193\u2013226, 1985.\n2. m. abramowitz and i. a. stegun, editors. handbook of mathematical functions. national\nbureau of standards applied mathematics series, no. 55. u.s. government printing\noffice, washington, dc, 1964.\n\n3. i. s. abramson. on bandwidth variation in kernel estimates\u2014a square root law. annals\n\nof statistics, 10:1217\u20131223, 1982.\n\n4. d. ackley. a connectionist machine for genetic hillclimbing. kluwer, boston, 1987.\n5. d. h. ackley. an empirical study of bit vector function optimization. in l. davis, ed-\nitor. genetic algorithms and simulated annealing. morgan kauffman, los altos, ca,\n1987.\n\n6. r. p. agarwal, m. meehan, and d. o\u2019regan. fixed point theory and applications.\n\ncambridge university press, cambridge, 2001.\n\n7. h. akaike. information theory and an extension of the maximum likelihood principle. in\nb. n. petrox and f. caski, editors. proceedings of the second international symposium\non information theory. akedemia kiaodo, budapest, 1973.\n\n8. j. t. alander. on optimal population size of genetic algorithms. in proceedings\nof compeuro 92, pages 65\u201370. ieee computer society press. the hague, the\nnetherlands, 1992.\n\n9. m. aldrin. moderate projection pursuit regression for multivariate response data. com-\n\nputational statistics and data analysis, 21:501\u2013531, 1996.\n\n10. n. s. altman. an introduction to kernel and nearest-neighbor nonparametric regression.\n\nthe american statistican, 46:175\u2013185, 1992.\n\n11. c. m. anderson and r. p. labelle. update of comparative occurrence rates for offshore\n\noil spills. spill science and technology bulletin, 6:303\u2013321, 2000.\n\n12. c. andrieu and j. thoms. a tutorial on adaptive mcmc. statistics and computing,\n\n18(4):343\u2013373, 2008.\n\n13. j. antonisse. a new interpretation of schema notation that overturns the binary encoding\nconstraint. in j. d. schaffer, editor. proceedings of the 3rd international conference on\ngenetic algorithms. morgan kaufmann, los altos, ca, 1989.\n\n14. l.armijo.minimizationoffunctionshavinglipschitz-continuousfirstpartialderivatives.\n\npacific journal of mathematics, 16:1\u20133, 1966.\n\n15. k. arms and p. s. camp. biology, 4th ed. saunders college publishing, fort worth, tx,\n\n1995.\n\ncomputational statistics, second edition. geof h. givens and jennifer a. hoeting.\n\u00a9 2013 john wiley & sons, inc. published 2013 by john wiley & sons, inc.\n\n423\n\n "}, {"Page_number": 427, "text": "424\n\nreferences\n\nyork, 1996.\n\n16. y. atchad\u00b4e, g. fort, e. moulines, and p. priouret. adaptive markov chain monte carlo:\ntheory and methods. in bayesian time series models. cambridge university press.\ncambridge, uk, 2011.\n\n17. t. b\u00a8ack. evolutionary algorithms in theory and practice. oxford university press, new\n\n18. j. e. baker. adaptive selection methods for genetic algorithms. in j. j. grefenstette,\neditor. proceedings of an international conference on genetic algorithms and their\napplications. lawrence erlbaum associates, hillsdale, nj, 1985.\n\n19. j. e. baker. reducing bias and inefficiency in the selection algorithm. in j. j. grefenstette,\neditor. proceedings of the 2nd international conference on genetic algorithms and their\napplications. lawrence erlbaum associates, hillsdale, nj, 1987.\n\n20. s. g. baker. a simple method for computing the observed information matrix when\nusing the em algorithm with categorical data. journal of computational and graphical\nstatistics, 1:63\u201376, 1992.\n\n21. d. l. banks, r. t. olszewski, and r. maxion. comparing methods for multivariate\nnonparametric regression. communications in statistics\u2014simulation and computation,\n32:541\u2013571, 2003.\n\n22. g. a. barnard. discussion of paper by m. s. bartlett. journal of the royal statistical\n\nsociety, series b, 25:294, 1963.\n\n23. o. e. barndorff-nielsen and d. r. cox. inference and asymptotics. chapman & hall,\n\nlondon, 1994.\n\n24. r. r. barton and j. s. ivey, jr. nelder-mead simplex modifications for simulation opti-\n\nmization. management science, 42:954\u2013973, 1996.\n\n25. l. e. baum, t. petrie, g. soules, and n. weiss. a maximization technique occurring in the\nstatistical analysis of probabilistic functions of markov chains. annals of mathematical\nstatistics, 41:164\u2013171, 1970.\n\n26. r. beran. prepivoting to reduce level error of confidence sets. biometrika, 74:457\u2013468,\n\n1987.\n\nnew york, 1980.\n\n27. r. beran. prepivoting test statistics: a bootstrap view of asymptotic refinements. journal\n\nof the american statistical association, 83:687\u2013697, 1988.\n\n28. j. o. berger. statistical decision theory: foundations, concepts, and methods. springer,\n\n29. j. o. berger and m.-h. chen. predicting retirement patterns: prediction for a multi-\nnomial distribution with constrained parameter space. the statistician, 42(4):427\u2013443,\n1993.\n\n30. n. bergman. posterior cram\u00b4er-rao bounds for sequential estimation. in a. doucet,\nn. de freitas, and n. gordon, editors. sequential monte carlo methods in practice.\nspringer, new york, 2001.\n\n31. n. bergman, l. ljung, and f. gustafsson. terrain navigation using bayesian statistics.\n\nieee control systems, 19:33\u201340, 1999.\n\n32. a. berlinet and l. devroye. a comparison of kernel density estimates. publications de\n\nl\u2019institute de statistique de l\u2019universit\u00b4e de paris, 38:3\u201359, 1994.\n\n33. d. bertsimas and j. tsitsiklis. simulated annealing. statistical science, 8:10\u201315, 1993.\n34. c. berzuini, n. g. best, w. r. gilks, and c. larizza. dynamic conditional independence\nmodels and markov chain monte carlo methods. journal of the american statistical\nassociation, 87:493\u2013500, 1997.\n\n "}, {"Page_number": 428, "text": "references\n\n425\n\n35. j.besag.spatialinteractionandthestatisticalanalysisoflatticesystems(withdiscussion).\n\njournal of the royal statistical society, series b, 36:192\u2013236, 1974.\n\n36. j. besag. on the statistical analysis of dirty pictures (with discussion). journal of the\n\nroyal statistical society, series b, 48:259\u2013302, 1986.\n\n37. j.besag.commenton\u201crepresentationsofknowledgeincomplexsystems\u201dbygrenander\n\nand miller. journal of the royal statistical society, series b, 56:591\u2013592, 1994.\n\n38. j. besag and p. clifford. generalized monte carlo significance tests. biometrika, 76:633\u2013\n\n39. j. besag and p. clifford. sequential monte carlo p-values. biometrika, 78:301\u2013304,\n\n642, 1989.\n\n1991.\n\n40. j. besag, p. green, d. higdon, and k. mengersen. bayesian computation and stochastic\n\nsystems (with discussion). statistical science, 10:3\u201366, 1995.\n\n41. j. besag and p. j. green. spatial statistics and bayesian computation. journal of the royal\n\nstatistical society, series b, 55(1):25\u201337, 1993.\n\n42. j. besag and c. kooperberg. on conditional and intrinsic autoregressions. biometrika,\n\n82:733\u2013746, 1995.\n\n43. j. besag, j. york, and a. molli\u00b4e. bayesian image restoration, with two applications in\nspatial statistics (with discussion). annals of the institute of statistical mathematics,\n43:1\u201359, 1991.\n\n44. n.best,s.cockings,j.bennett,j.wakefield,andp.elliott.ecologicalregressionanalysis\nof environmental benzene exposure and childhood leukaemia: sensitivity to data inaccu-\nracies, geographical scale and ecological bias. journal of the royal statistical society,\nseries a, 164(1):155\u2013174, 2001.\n\n45. n. g. best, r. a. arnold, a. thomas, l. a. waller, and e. m. conlon. bayesian models\nfor spatially correlated disease and exposure data. in j. o. berger, j. m. bernardo, a. p.\ndawid, d. v. lindley, and a. f. m. smith, editors. bayesian statistics 6, pages 131\u2013156.\noxford university press, oxford, 1999.\n\n46. r.j.h.bevertonands.j.holt.onthedynamicsofexploitedfishpopulations,volume19\nof fisheries investment series 2.ukministryofagricultureandfisheries,london,1957.\n47. p. j. bickel and d. a. freedman. some asymptotics for the bootstrap. annals of statistics,\n\n9:1196\u20131217, 1981.\n\n48. c. biller. adaptive bayesian regression splines in semiparametric generalized linear\n\nmodels. journal of computational and graphical statistics, 9(1):122\u2013140, 2000.\n\n49. p. billingsley. probability and measure, 3rd ed. wiley, new york, 1995.\n50. c.m.bishop. neural networks for pattern recognition.oxforduniversitypress.oxford,\n\n51. c. m. bishop, editor. neural networks and machine learning. springer, 1998.\n52. f. black and m. scholes. the pricing of options and corporate liabilities. journal of\n\npolitical economy, 81:635\u2013654, 1973.\n\n53. c. l. blake and c. j. merz. uci repository of machine learning databases, univer-\nsity of california, irvine, dept. of information and computer sciences. available from\nhttp://www.ics.uci.edu/\u223cmlearn/mlrepository.html, 1998.\nrithms and simulated annealing. morgan kauffman, los altos, ca, 1987.\n\n54. l. b. booker. improving search in genetic algorithms. in l. davis, editor. genetic algo-\n\n55. d.l.borchers,s.t.buckland,andw.zucchini.estimatinganimalabundance.springer,\n\nlondon, 2002.\n\nuk, 1995.\n\n "}, {"Page_number": 429, "text": "426\n\nreferences\n\nyork, 1978.\n\n56. a. bowman. an alternative method of cross-validation for the smoothing of density\n\nestimates. biometrika, 71:353\u2013360, 1984.\n\n57. g.e.p.boxandd.r.cox.ananalysisoftransformations. journaloftheroyalstatistical\n\nsociety, series b, 26:211\u2013246, 1964.\n\n58. g. e. p. box, w. g. hunter, and j. s. hunter. statistics for experimenters. wiley, new\n\n59. p. boyle, m. broadie, and p. glasserman. monte carlo methods for security pricing.\n\njournal of economic dynamics and control, 21:1267\u20131321, 1997.\n\n60. r. a. boyles. on the convergence of the em algorithm. journal of the royal statistical\n\nsociety, series b, 45:47\u201350, 1983.\n\n61. c. j. a. bradshaw, r. j. barker, r. g. harcourt, and l. s. davis. estimating survival and\ncapture probability of fur seal pups using multistate mark\u2013recapture models. journal of\nmammalogy, 84(1):65\u201380, 2003.\n\n62. c. j. a. bradshaw, c. lalas, and c. m. thompson. cluster of colonies in an expanding\npopulation of new zealand fur seals (arctocephalus fosteri). journal of zoology, 250:\n41\u201351, 2000.\n\n63. l. breiman. bagging predictors. machine learning, 24:123\u2013140, 1996.\n64. l. breiman and j. h. friedman. estimating optimal transformations for multiple regres-\nsion and correlation (with discussion). journal of the american statistical association,\n80:580\u2013619, 1985.\n\n65. l. breiman, j. h. friedman, r. a. olshen, and c. j. stone. classification and regression\n\ntrees. wadsworth. boca raton, fl, 1984.\n\n66. l. breiman, w. meisel, and e. purcell. variable kernel estimates of multivariate densities.\n\ntechnometrics, 19:135\u2013144, 1977.\n\n67. p. br\u00b4emaud. markov chains: gibbs fields, monte carlo simulation, and queues.\n\nspringer, new york, 1999.\n\ncliffs, nj, 1973.\n\n68. r. p. brent. algorithms for minimization without derivatives. prentice-hall, englewood\n\n69. n. e. breslow and d. g. clayton. approximate inference in generalized linear mixed\n\nmodels. journal of the american statistical association, 88:9\u201325, 1993.\n\n70. s. p. brooks. markov chain monte carlo method and its application. the statistician,\n\n47:69\u2013100, 1998.\n\n71. s. p. brooks and a. gelman. general methods for monitoring convergene of it-\nerative simulations. journal of computational and graphical statistics, 7:434\u2013455,\n1998.\n\n72. s. p. brooks and p. giudici. markov chain monte carlo convergence assessment via\ntwo-way analysis of variance. journal of computational and graphical statistics, 9(2):\n266\u2013285, 2000.\n\n73. s. p. brooks, p. giudici, and a. philippe. nonparametric convergence assessment for\nmcmc model selection. journal of computational and graphical statistics, 12(1):1\u201322,\n2003.\n\n74. s. p. brooks, p. giudici, and g. o. roberts. efficient construction of reversible jump\nmarkov chain monte carlo proposal distributions. journal of the royal statistical society,\nseries b, 65(1):3\u201339, 2003.\n\n75. s.p.brooksandb.j.t.morgan.optimizationusingsimulatedannealing. the statistican,\n\n44:241\u2013257, 1995.\n\n "}, {"Page_number": 430, "text": "references\n\n427\n\n76. s. p. brooks and g. o. roberts. assessing convergence of markov chain monte carlo\n\nalgorithms. statistics and computing, 8:319\u2013335, 1999.\n\n77. w. j. browne, f. steele, m. golalizadeh, and m. j. green. the use of simple reparameter-\nizations to improve the efficiency of markov chain monte carlo estimation for multilevel\nmodels with applications to discrete time survival models. journal of the royal statistical\nsociety: series a (statistics in society), 172(3):579\u2013598, 2009.\n\n78. c. g. broyden. quasi-newton methods and their application to function minimization.\n\nmathematics of computation, 21:368\u2013381, 1967.\n\n79. c. g. broyden. the convergence of a class of double-rank minimization algorithms.\n\njournal of the institute of mathematics and its applications, 6:76\u201390, 1970.\n\n80. c. g. broyden. quasi-newton methods. in w. murray, editor. numerical methods for\n\nunconstrained optimization, pages 87\u2013106. academic, new york, 1972.\n81. p. b\u00a8uhlmann. sieve bootstrap for time series. bernoulli, 3:123\u2013148, 2007.\n82. p.b\u00a8uhlmann.sievebootstrapforsmoothingnonstationarytimeseries.annalsofstatistics,\n\n26:48\u201383, 2008.\n\n83. p. b\u00a8uhlmann and h. r. k\u00a8unsch. block length selection in the bootstrap for time series.\n\ncomputational statistics and data analysis, 31:295\u2013310, 2009.\n\n84. a. buja. remarks on functional canonical variates, alternating least squares methods, and\n85. \u00b4a. b\u00a8urmen, j. puhan, and t. tuma. grid restrained nelder\u2013mead algorithm. computa-\n\nace. annals of statistics, 18:1032\u20131069, 1989.\n\ntional optimization and applications, 34:359\u2013375, 2006.\n\n86. k. p. burnham and d. r. anderson. model selection and inference: a practical infor-\n\nmation theoretic approach, 2nd ed. springer, new york, 2002.\n\n87. e. cameron and l. pauling. supplemental ascorbate in the supportive treatment of cancer:\nre-evaluation of prolongation of survival times in terminal human cancer. proceedings\nof the national academy of sciences of the usa, 75(9):4538\u20134542, 1978.\n\n88. r.cao,a.cuevas,andw.gonz\u00b4alez-mantiega.acomparativestudyofseveralsmoothing\nmethods in density estimation. computational statistics and data analysis, 17:153\u2013176,\n1994.\n\n89. o. cape, c. p. rober, and t. ryden. reversible jump, birth-and-death and more general\ncontinuous time markov chain monte carlo sampler. journal of the royal statistical\nsociety, series b, 65(3):679\u2013700, 2003.\n\n90. o. capp\u00b4e, s. j. godsill, and e. moulines. an overview of existing methods and recent\n\nadvances in sequential monte carlo. procedings of the ieee, 95:899\u2013924, 2007.\n\n91. b. p. carlin, a. e. gelfand, and a. f. m. smith. hierarchical bayesian analysis of change-\n\npoint problems. applied statistics, 41:389\u2013405, 1992.\n\n92. b. p. carlin and t. a. louis. bayes and empirical bayes methods for data analysis.\n\nchapman & hall, london, 1996.\n\n93. e. carlstein. the use of subseries methods for estimating the variance of a general statistic\n\nfrom a stationary time series. annals of statistics, 14:1171\u20131179, 1986.\n\n94. e. carlstein, k.-a. do, p. hall, t. hesterberg, and h. r. k\u00a8unsch. matched-block bootstrap\n\nfor dependent data. bernoulli, 4:305\u2013328, 1998.\n\n95. j. carpenter, p. clifford, and p. fernhead. improved particle filter for nonlinear problems.\n\niee proceedings radar, sonar, & navigation, 146:2\u20137, 1999.\n\n96. g. casella and r. l. berger. statistical inference, 2nd ed. brooks/cole, pacific grove,\n\nca, 2001.\n\n "}, {"Page_number": 431, "text": "428\n\nreferences\n\n97. g. casella and e. i. george. explaining the gibbs sampler. the american statistican,\n\n46(3):167\u2013174, 1992.\n\n98. g. casella, k. l. mengersen, c. p. robert, and d. m. titterington. perfect samplers for\nmixturesofdistributions.journaloftheroyalstatisticalsociety,seriesb,64(4):777\u2013790,\n2002.\n\n99. g.casellaandc.robert.rao\u2013blackwellizationofsamplingschemes.biometrika,83:81\u2013\n\n94, 1996.\n\nyork, 1992.\n\n1928.\n\n100. g. casella and c. p. robert. post-processing accept\u2013reject samples: recycling and rescal-\n\ning. journal of computational and graphical statistics, 7:139\u2013157, 1998.\n\n101. j. m. chambers and t. j. hastie, editors. statistical models in s. chapman & hall, new\n\n102. k. s. chan and j. ledholter. monte carlo em estimation for time series models involving\n\ncounts. journal of the american statistical association, 90:242\u2013252, 1995.\n\n103. r. n. chapman. the quantitative analysis of environmental factors. ecology, 9:111\u2013122,\n\n104. m.-h. chen and b. w. schmeiser. performance of the gibbs, hit-and-run, and\nmetropolis samplers. journal of computational and graphical statistics, 2:251\u2013272,\n1993.\n\n105. m.-h. chen and b. w. schmeiser. general hit-and-run monte carlo sampling for evalu-\n\nating multidimensional integrals. operations research letters, 19:161\u2013169, 1996.\n\n106. m.-h. chen, q.-m. shao, and j. g. ibrahim. monte carlo methods in bayesian compu-\n\ntation. springer, new york, 2000.\n\n107. m. h. chen and q. m. shao. monte carlo estimation of bayesian credible and hpd\n\nintervals. journal of computational and graphical statistics, 8(1):69\u201392, 1999.\n\n108. y. chen, p. diaconis, s. p. holmes, and j. s. liu. sequential monte carlo methods for\nstatistical analysis of tables. journal of the american statistical association, 100:109\u2013\n120, 2005.\n\n109. y. chen, i. h. dinwoodie, and s. sullivant. sequential importance sampling for multiway\n\ntables. annals of statistics, 34:523\u2013545, 2006.\n\n110. s. chib and b. p. carlin. on mcmc sampling in hierarchical longitudinal models. statis-\n\n111. s. chib and e. greenberg. understanding the metropolis\u2013hastings algorithm. the amer-\n\ntics and computing, 9(1):17\u201326, 1999.\n\nican statistican, 49(4):327\u2013335, 1995.\n\n112. h. a. chipman, e. i. george, and r. e. mcculloch. bayesian cart model search (with\n\ndiscussion). journal of the american statistical association, 93:935\u2013960, 1998.\n\n113. n. chopin. a sequential particle filter method for static models. biometrika, 89:539\u2013551,\n\n114. a. ciampi, c.-h. chang, s. hogg, and s. mckinney. recursive partitioning: a verstatile\nmethod for exploratory data analysis in biostatistics. in i. b. macneil and g. j. umphrey,\neditors. biostatistics, pages 23\u201350. reidel, dordrecht, netherlands, 1987.\n\n115. l. a. clark and d. pregiborn. tree-based models. in j. m. chambers and t. hastie,\n\neditors. statistical models in s, pages 377\u2013419. duxbury, new york, 1991.\n\n116. w. s. cleveland. robust locally weighted regression and smoothing scatter plots. journal\n\nof the american statistical association, 74:829\u2013836, 1979.\n\n117. w. s. cleveland, e. grosse, and w. m. shyu. local regression models. in j. m. chambers\n\nand t. j. hastie, editors. statistical models in s. chapman & hall, new york, 1992.\n\n2002.\n\n "}, {"Page_number": 432, "text": "references\n\n429\n\n118. w. s. cleveland and c. loader. smoothing by local regression: principles and methods\n(with discussion). in w. h. h\u00a8ardle and m. g. schimek, editors. statistical theory and\ncomputational aspects of smoothing. springer, new york, 1996.\n\n119. m. clyde. discussion of \u201cbayesian model averaging: a tutorial\u201d by hoeting, madigan,\n\nraftery and volinsky. statistical science, 14(4):382\u2013417, 1999.\n\n120. a. r. conn, n. i. m. gould, and p. l. toint. convergence of quasi-newton matrices\ngenerated by the symmetric rank one update. mathematical programming, 50:177\u2013195,\n1991.\n\n121. s. d. conte and c. de boor. elementary numerical analysis: an algorithmic approach.\n\nmcgraw-hill, new york, 1980.\n\n122. j. corander and m. j. sillanpaa. a unified approach to joint modeling of multiple quanti-\ntative and qualitative traits in gene mapping. journal of theoretical biology, 218(4):435\u2013\n446, 2002.\n\n123. j. n. corcoran and r. l. tweedie. perfect sampling from independent metropolis\u2013\nhastings chains. journal of statistical planning and inference, 104(2):297\u2013314,\n2002.\n\n124. m. k. cowles. efficient model-fitting and model-comparison for high-dimensional\nbayesian geostatistical models. journal of statistical planning and inference, 112:221\u2013\n239, 2003.\n\n125. m. k. cowles and b. p. carlin. markov chain monte carlo convergence diagnostics: a\ncomparative review. journal of the american statistical association, 91(434):883\u2013904,\n1996.\n\n126. m. k. cowles, g. o. roberts, and j. s. rosenthal. possible biases induced by mcmc\nconvergence diagnostics. journal of statistical computing and simulation, 64(1):87\u2013104,\n1999.\n\n127. d. r. cox and d. v. hinkley. theoretical statistics. chapman & hall, london, 1974.\n128. n. a. c. cressie. statistics for spatial data. wiley, new york, 1993.\n129. j. d. cryer and k. chan. time series analysis: with applications in r. springer, new\n\n130. v. c\u02c7erny. a thermodynamical approach to the travelling salesman problem: an effi-\ncient simulation algorithm. journal of optimization theory and applications, 45:41\u201355,\n1985.\n131. g. dahlquist and \u02daa. bj\u00a8orck, translated by n. anderson. numerical methods. prentice-\n\nhall, englewood cliffs, nj, 1974.\n\n132. p. damien, j. wakefield, and s. walker. gibbs sampling for bayesian non-conjugate and\nhierarchical models by using auxiliary variables. journal of the royal statistical society,\nseries b, 61:331\u2013344, 1999.\n\n133. g. b. dantzig. linear programming and extensions. princeton university press, prince-\n\nyork, 2008.\n\nton, nj, 1963.\n\n134. w. c. davidon. variable metric methods for minimization. aec research and develop-\n\nment report anl-5990, argonne national laboratory. argonne, il, 1959.\n\n135. l. davis. applying adaptive algorithms to epistatic domains. in proceedings of the 9th\n\njoint conference on artificial intelligence, pages 162\u2013164, 1985.\n\n136. l. davis. job shop scheduling with genetic algorithms. in proceedings of the 1st in-\nternational conference on genetic algorithms and their applications, pages 136\u2013140,\n1985.\n\n "}, {"Page_number": 433, "text": "430\n\nreferences\n\n1991.\n\n1984.\n\n130, 1992.\n\n137. l. davis. adapting operator probabilities in genetic algorithms. in j. d. schaffer, ed-\nitor. proceedings of the 3rd international conference on genetic algorithms. morgan\nkaufmann, san mateo, ca, 1989.\n\n138. l. davis, editor. handbook of genetic algorithms. van nostrand reinhold, new york,\n\n139. p. j. davis and p. rabinowitz. methods of numerical integration. academic, new york,\n\n140. a. c. davison and p. hall. on studentizing and blocking methods for implementing the\n\nbootstrap with dependent data. australian journal of statistics, 35:215\u2013224, 1993.\n\n141. a. c. davison, d. hinkley, and b. j. worton. bootstrap likelihoods. biometrika, 79:113\u2013\n\n142. a. c. davison and d. v. hinkley. bootstrap methods and their applications. cambridge\n\n143. a. c. davison, d. v. hinkley, and e. schechtman. efficient bootstrap simulation.\n\nuniversity press, cambridge, 1997.\n\nbiometrika, 73:555\u2013566, 1986.\n\n144. c. de boor. a practical guide to splines. springer, new york, 1978.\n145. f. r. de hoog and m. f. hutchinson. an efficient method for calculating smoothing\nsplines using orthogonal transformations. numerische mathematik, 50:311\u2013319, 1987.\n146. k. a. dejong. an analysis of the behavior of a class of genetic adaptive systems.\n\nph.d. thesis, university of michigan, 1975.\n\n147. p. dellaportas and j. j. forster. markov chain monte carlo model determination for\n\nhierarchical and graphical log-linear models. biometrika, 86(3):615\u2013633, 1999.\n\n148. p. dellaportas, j. j. forster, and i. ntzoufras. on bayesian model and variable selection\n\nusing mcmc. statistics and computing, 12(2):27\u201336, 2002.\n\n149. b. delyon, m. lavielle, and e. moulines. convergence of a stochastic approximation\n\nversion of the em algorithm. annals of statistics, 27:94\u2013128, 1999.\n\n150. a. p. dempster, n. laird, and d. b. rubin. maximum likelihood from incomplete data\nvia the em algorithm. journal of the royal statistical society, series b, 39:1\u201338, 1977.\n151. d. g. t. denison, b. k. mallick, and a. f. m. smith. a bayesian cart algorithm.\n\nbiometrika, 85:363\u2013377, 1998.\n\n152. j. e. dennis, jr., d. m. gay, and r. e. welsch. an adaptive nonlinear least-squares\n\nalgorithm. acm transactions on mathematical software, 7:369\u2013383, 1981.\n\n153. j. e. dennis, jr. and r. b. schnabel. numerical methods for unconstrained optimization\n\nand nonlinear equations. prentice-hall, englewood cliffs, nj, 1983.\n\n154. j. e. dennis, jr. and d. j. woods. optimization on microcomputers: the nelder\u2013mead\nsimplex algorithm. in a. wouk, editor. new computing environments, pages 116\u2013122.\nsiam, philadelphia, 1987.\n\n155. g. der and b. s. everitt. a handbook of statistical analyses using sas, 2nd ed. chapman\n\n& hall/crc, boca raton, fl, 2002.\n\n156. e. h. dereksd\u00b4ottir and k. g. magn\u00b4usson. a strike limit algorithm based on adaptive\nkalman filtering with application to aboriginal whaling of bowhead whales. journal of\ncetacean research and management, 5:29\u201338, 2003.\n\n157. r. d. deveaux. finding transformations for regression using the ace algorithm. socio-\n\nlogical methods & research, 18(2\u20133):327\u2013359, 1989.\n\n158. l. devroye. non-uniform random variate generation. springer, new york, 1986.\n159. l. devroye. a course in density estimation. birkh\u00a8auser, boston, 1987.\n\n "}, {"Page_number": 434, "text": "48, 2001.\n\nyork, 1985.\n\n431\n160. l. devroye and l. gy\u00a8orfi. nonparametric density estimation: the l1 view. wiley, new\n\nreferences\n\n161. p. diaconis and m. shahshahani. on non-linear functions of linear combinations. siam\n\njournal of scientific and statistical computing, 5:175\u2013191, 1984.\n\n162. r. dias and d. gamerman. a bayesian approach to hybrid splines non-parametric re-\n\ngression. journal of statistical computation and simulation, 72(4):285\u2013297, 2002.\n\n163. t. j. diciccio and b. efron. bootstrap confidence intervals (with discussion). statistical\n\nscience, 11:189\u2013228, 1996.\n\n164. p. dierckx. curve and surface fitting with splines. clarendon, new york, 1993.\n165. x. k. dimakos. a guide to exact simulation. international statistical review, 69(1):27\u2013\n\n166. p. djuric, y. huang, and t. ghirmai. perfect sampling: a review and applications to signal\n\nprocessing. ieee transaction on signal processing, 50(2):345\u2013356, 2002.\n\n167. a. doucet, n. de freitas, and n. gordon. sequential monte carlo methods in practice.\n\nspringer, new york, 2001.\n\n168. a. doucet, s. godsill, and c. andrieu. on sequential monte carlo sampling methods for\n\nbayesian filtering. statistics and computing, 10:197\u2013208, 2000.\n\n169. k. a. dowsland. simulated annealing. in c. r. reeves, editor. modern heuristic tech-\n\nniques for combinatorial problems. wiley, new york, 1993.\n\n170. n. r. draper and h. smith. applied regression analysis. wiley, new york, 1966.\n171. r. p. w. duin. on the choice of smoothing parameter for parzen estimators of probability\n\ndensity functions. ieee transactions on computing, c-25:1175\u20131179, 1976.\n\n172. r. durbin, s. eddy, a. krogh, and g. mitchison. biological sequence analysis: proba-\nbilistic models of proteins and nucleic acids. cambridge university press, cambridge,\n1998.\n\n173. e. s. edgington. randomization tests, 3rd ed. marcel dekker, new york, 1995.\n174. r. g. edwards and a. d. sokal. generalization of the fortuin\u2013kasteleyn\u2013swendsen\u2013\nwang representation and monte carlo algorithm. physical review d, 38(6):2009\u20132012,\n1988.\n\n175. b. efron. bootstrap methods: another look at the jackknife. annals of statistics, 7:1\u201326,\n\n176. b.efron.nonparametricstandarderrorsandconfidenceintervals(withdiscussion).cana-\n\ndian journal of statistics, 9:139\u2013172, 1981.\n\n177. b. efron. the jackknife, the bootstrap, and other resampling plans. number 38 in\ncbms\u2013nsf regional conference series in applied mathematics. siam, philadelphia,\n1982.\n\n178. b. efron. better bootstrap confidence intervals (with discussion). journal of the american\n\nstatistical association, 82:171\u2013200, 1987.\n\n179. b. efron. computer-intensive methods in statistical regression. siam review, 30:421\u2013\n\n180. b. efron. jackknife-after-bootstrap standard errors and influence functions (with discus-\n\nsion). journal of the royal statistical society, series b, 54:83\u2013111, 1992.\n\n181. b.efronandg.gong.aleisurelylookatthebootstrap,thejackknife,andcross-validation.\n\nthe american statistican, 37:36\u201348, 1983.\n\n182. b. efron and d. v. hinkley. assessing the accuracy of the maximum likelihood estimator:\n\nobserved versus expected fisher information. biometrika, 65:457\u2013482, 1978.\n\n449, 1988.\n\n1979.\n\n "}, {"Page_number": 435, "text": "432\n\nreferences\n\nyork, 1993.\n\n1999.\n\n183. b. efron and r. j. tibshirani. an introduction to the bootstrap. chapman & hall, new\n\n184. r. j. elliott and p. e. kopp. mathematics of financial markets. springer, new york,\n\n185. environmental monitoring and assessment program, mid-atlantic highlands streams\nassessment, epa-903-r-00-015, us environmental protection agency, national health\nand environmental effects research laboratory, western ecology division, corvallis,\nor, 2000.\n\n186. v. a. epanechnikov. non-parametric estimation of a multivariate probability density.\n\ntheory of probability and its applications, 14:153\u2013158, 1969.\n\n187. l. j. eshelman, r. a. caruana, and j. d. schaffer. biases in the crossover landscape.\nin j. d. schaffer, editor. proceedings of the 3rd international conference on genetic\nalgorithms. morgan kaufmann, los altos, ca, 1989.\n\n188. r. l. eubank. spline smoothing and nonparametric regression. marcel dekker, new\n\n189. m. evans. adaptive importance sampling and chaining. contemporary mathematics, 115\n\n(statistical multiple integration):137\u2013143, 1991.\n\n190. m. evans and t. swartz. approximating integrals via monte carlo and deterministic\n\nmethods. oxford university press, oxford, 2000.\n\n191. u. faigle and w. kern. some convergence results for probabilistic tabu search. orsa\n\njournal on computing, 4:32\u201337, 1992.\n\n192. j. fan and i. gijbels. local polynomial modelling and its applications. chapman & hall,\n\nnew york, 1996.\n\n193. j. a. fill. an interruptible algorithm for perfect sampling via markov chains. annals of\n\napplied probability, 8(1):131\u2013162, 1998.\n\n194. r. a. fisher. design of experiments. hafner, new york, 1935.\n195. g. s. fishman. monte carlo. springer, new york, 1996.\n196. j. m. flegal, m. haran, and g. l. jones. markov chain monte carlo: can we trust the\n\nthird significant figure. statistical science, 23(2):250\u2013260, 2008.\n\n197. r. fletcher. a new approach to variable metric algorithms. computer journal, 13:317\u2013\n\nyork, 1988.\n\n198. r. fletcher. practical methods of optimization, 2nd ed. wiley, chichester, uk, 1987.\n199. r. fletcher and m. j. d. powell. a rapidly convergent descent method for minimization.\n\ncomputer journal, 6:163\u2013168, 1963.\n\n200. d. b. fogel. evolutionary computation: toward a new philosophy of machine intelli-\n\ngence, 2nd ed. ieee press, piscataway, nj, 2000.\n\n201. b. l. fox. simulated annealing: folklore, facts, and directions. in h. niederreiter and p. j.\nshiue, editors. monte carlo and quasi-monte-carlo methods in scientific computing.\nspringer, new york, 1995.\n\n202. e. j. freireich, e. gehan, e. frei iii, l. r. schroeder, i. j. wolman, r. anabari, e. o.\nburgert, s. d. mills, d. pinkel, o. s. selawry, j. h. moon, b. r. gendel, c. l. spurr,\nr. storrs, f. haurani, b. hoogstraten, and s. lee. the effect of 6-mercaptopurine on\nthe duraction of steriod-induced remissions in acute leukemia: a model for evaluation fo\nother potentially useful therapy. blood, 21(6):699\u2013716, june 1963.\n\n203. d. frenkel and b. smit. understanding molecular simulation. academic, new york,\n\n322, 1970.\n\n1996.\n\n "}, {"Page_number": 436, "text": "references\n\n433\n\n204. h. freund and r. wolter. evolution of bit strings ii: a simple model of co-evolution.\n\n205. j.h.friedman.avariablespansmoother.technicalreport5,dept.ofstatistics,stanford\n\n206. j. h. friedman. exploratory projection pursuit. journal of the american statistical asso-\n\n207. j.h.friedman.multivariateadditiveregressionsplines(withdiscussion).annalsofstatis-\n\ncomplex systems, 7:25\u201342, 1993.\n\nuniversity, palo alto, ca, 1984.\n\nciation, 82:249\u2013266, 1987.\n\ntics, 19(1):1\u2013141, 1991.\n\n208. j.h.friedman andw.steutzle. smoothingof scatterplots.technicalreportorion-003,\n\ndept. of statistics, stanford university, palo alto, ca, 1982.\n\n209. j. h. friedman and w. stuetzle. projection pursuit regression. journal of the american\n\nstatistical association, 76:817\u2013823, 1981.\n\n210. j. h. friedman, w. stuetzle, and a. schroeder. projection pursuit density estimation.\n\njournal of the american statistical association, 79:599\u2013608, 1984.\n\n211. k. fukunaga. introduction to statistical pattern recognition. academic, newyork, 1972.\n212. w. a. fuller. introduction to statistical time series. wiley, new york, 1976.\n213. g. m. furnival and r. w. wilson, jr. regressions by leaps and bounds. technometrics,\n\n16:499\u2013511, 1974.\n\n214. m. r. garey and d. s. johnson. computers and intractability: a guide to the theory of\n\nnp-completeness. freeman, san francisco, 1979.\n\n215. c. gaspin and t. schiex. genetic algorithms for genetic mapping. in j.-k. hao, e. lutton,\ne. ronald, m. schoenauer, and d. snyers, editors. artificial evolution 1997, pages 145\u2013\n156. springer, new york, 1997.\n\n216. a. gelfand and a. f. m. smith. sampling based approaches to calculating marginal\n\ndensities. journal of the american statistical association, 85:398\u2013409, 1990.\n\n217. a. e. gelfand, j. a. silander, jr., s. wu, a. latimer, p. o. lewis, a. g. rebelo,\nand m. holder. explaining species distribution patterns through hierarachical modeling.\nbayesian analysis, 1(1):41\u201392, 2006.\n\n218. a. e. gelfand, s. k. sahu, and b. p. carlin. efficient parametrisations for normal linear\n\nmixed models. biometrika, 82(3):479\u2013488, 1995.\n\n219. a. e. gelfand, s. k. sahu, and b. p. carlin. efficient parametrizations for generalized\nlinear mixed models, (with discussion). in bayesian statistics 5. oxford university\npress. oxford, uk, 1996.\n\n220. a. gelman. iterative and non-iterative simulation algorithms. computing science and\n\n221. a. gelman, j. b. carlin, h. s. stern, and d. b. rubin. bayesian data analysis, 2nd ed.\n\nstatistics, 24:433\u2013438, 1992.\n\nchapman & hall, london, 2004.\n\n222. a. gelman and x.-l. meng. simulating normalizing constants: from importance sam-\n\npling to bridge sampling to path sampling. statistical science, 13:163\u2013185, 1998.\n\n223. a. gelman, g. roberts, and w. gilks. efficient metropolis jumping rules. bayesian statis-\n\ntics, 5:599\u2013608, 1996.\n\n224. a. gelman and d. b. rubin. inference from iterative simulation using multiple sequences\n\n(with discussion). statistical science, 7:457\u2013511, 1992.\n\n225. a. gelman, d. a. van dyk, z. huang, and w. j. boscardin. using redundant parame-\nterizations to fit hierarchical models. journal of computational and graphical statistics,\n17(1):95\u2013122, 2008.\n\n "}, {"Page_number": 437, "text": "434\n\nreferences\n\nyork, 1998.\n\n226. s. geman and d. geman. stochastic relaxation, gibbs distributions, and the bayesian\nrestoration of images. ieee transactions on pattern analysis and machine intelligence,\npami-6(6):721\u2013741, 1984.\n\n227. j. e. gentle. random number generation and monte carlo methods. springer, new\n\n228. r. gentleman and r. ihaka. the comprehensive r archive network. available at\n\nhttp://lib.stat.cmu.edu/r/cran/, 2003.\n\n229.  e. i. george and r. e. mcculloch. variable selection via gibbs sampling. journal of the\n\namerican statistical association, 88:881\u2013889, 1993.\n\nbiometrika, 79(4):677\u2013683, 1992.\n\n230. e. i. george and c. p. robert. capture\u2013recapture estimation via gibbs sampling.\n231.  c. j. geyer. burn-in is unneccessary. available at http://www.stat.umn.edu/\u223ccharlie/\n232. c. j. geyer. markov chain monte carlo maximum likelihood. in e. keramigas, editor.\ncomputing science and statistics: the 23rd symposium on the interface. interface foun-\ndation, fairfax station, va, 1991.\n\nmcmc/burn.html.\n\n233. c. j. geyer. practical markov chain monte carlo (with discussion). statistical science,\n\n7:473\u2013511, 1992.\n\n234. c. j. geyer and e. a. thompson. constrained monte carlo maximum likelihood for\n\ndependent data. journal of the royal statistical society, series b, 54:657\u2013699, 1992.\n\n235. c. j. geyer and e. a. thompson. annealing markov chain monte carlo with applications\nto ancestral inference. journal of the american statistical association, 90:909\u2013920, 1995.\n236. z. ghahramani. an introduction to hidden markov models and bayesian networks. in-\n\nternational journal of pattern recognition and artificial intelligence, 15:9\u201342, 2001.\n\n237. w. r. gilks. derivative-free adaptive rejection sampling for gibbs sampling. in j. m.\nbernardo, j. o. berger, a. p. dawid, and a. f. m. smith, editors. bayesian statistics 4.\noxford, clarendon. oxford, uk, 1992.\n\n238. w. r. gilks. adaptive rejection sampling, mrc biostatistics unit, software from\nthe bsu. available at http://www.mrc-bsu.cam.ac.uk/bsusite/research/software.shtml,\n2004.\n\n239. w. r. gilks and c. berzuini. following a moving target\u2014monte carlo inference for\ndynamic bayesian systems. journal of the royal statistical society, series b, 63:127\u2013\n146, 2001.\n\n240. w. r. gilks, n. g. best, and k. k. c. tan. adaptive rejection metropolis sampling within\n\ngibbs sampling. applied statistics, 44:455\u2013472, 1995.\n\n241. w. r. gilks, s. richardson, and d. j. spiegelhalter. markov chain monte carlo methods\n\nin practice. chapman & hall/crc, london, 1996.\n\n242. w. r. gilks and g. o. roberts. strategies for improving mcmc. in w. r. gilks,\ns. richardson, and d. j. spiegelhalter, editors. markov chain monte carlo in practice,\npages 89\u2013114. chapman & hall/crc, london, 1996.\n\n243. w. r. gilks, a. thomas, and d. j. spiegelhalter. a language and program for complex\n\nbayesian modeling. the statistician, 43:169\u2013178, 1994.\n\n244. w. r. gilks and p. wild. adaptive rejection sampling for gibbs sampling. applied statis-\n\ntics, 41:337\u2013348, 1992.\n\n245. p. e. gill, g. h. golub, w. murray, and m. a. saunders. methods for modifying matrix\n\nfactorizations. mathematics of computation, 28:505\u2013535, 1974.\n\n "}, {"Page_number": 438, "text": "references\n\n435\n\n246. p.e.gillandw.murray.newton-typemethodsforunconstrainedandlinearlyconstrained\n\noptimization. mathematical programming, 28:311\u2013350, 1974.\n\n247. p. e. gill, w. murray, and m. wright. practical optimization. academic, london, 1981.\n248. p. giudici and p. j. green. decomposable graphical gaussian model determination.\n\nbiometrika, 86(4):785\u2013801, 1999.\n\n249. g. h. givens. empirical estimation of safe aboriginal whaling limits for bowhead whales.\n\njournal of cetacean research and management, 5:39\u201344, 2003.\n\n250. g. h. givens, j. r. beveridge, b. a. draper, p. grother, and p. j. phillips. how features\nof the human face affect recognition: a statistical comparison of three face recogni-\ntion algorithms. ieee conference on computer vision and pattern recognition, pages\n381\u2013388, june 2004.\n\n251. g. h. givens, j. r. beveridge, b. a. draper, and d. bolme. a statistical assessment of\nsubject factors in the pca recognition of human faces. in ieee conference on computer\nvision and pattern recognition. december 2003.\n\n252. g. h. givens and a. e. raftery. local adaptive importance sampling for multivariate den-\nsities with strong nonlinear relationships. journal of the american statistical association,\n91:132\u2013141, 1996.\n\n253. j. r. gleason. algorithms for balanced bootstrap simulations. the american statistican,\n\n42:263\u2013266, 1988.\n\n254. f. glover. tabu search, part i. orsa journal on computing, 1:190\u2013206, 1989.\n255. f. glover. tabu search, part ii. orsa journal on computing, 2:4\u201332, 1990.\n256. f. glover and h. j. greenberg. new approaches for heuristic search: a bilateral link with\n\nartificial intelligence. european journal of operational research, 39:119\u2013130, 1989.\n\n257. f. glover and m. laguna. tabu search. in c. r. reeves, editor. modern heuristic tech-\n\nniques for combinatorial problems. wiley, new york, 1993.\n258. f. glover and m. laguna. tabu search. kluwer, boston, 1997.\n259. f.glover,e.taillard,andd.dewerra.auser\u2019sguidetotabusearch. annals of operations\n\nresearch, 41:3\u201328, 1993.\n\n260. s. godsill and t. clapp. improvement strategies for monte carlo particle filters. in\na. doucet, n. de freitas, and n. gordon, editors. sequential monte carlo methods in\npractice, pages 139\u2013158. springer, new york, 2001.\n\n261. s. j. godsill. on the relationship between markov chain monte carlo methods for\nmodel uncertainty. journal of computational and graphical statistics, 10(2):230\u2013248,\n2001.\n\n262. d. e. goldberg. genetic algorithms in search, optimization, and machine learning.\n\naddison-wesley, reading, ma, 1989.\n\n263. d. e. goldberg. a note on boltzmann tournament selection for genetic algorithms and\n\npopulation-oriented simulated annealing. complex systems, 4:445\u2013460, 1990.\n\n264. d. e. goldberg and k. deb. a comparative analysis of selection schemes used in ge-\nnetic algorithms. in g. rawlins, editor. foundations of genetic algorithms and classifier\nsystems. morgan kaufmann, san mateo, ca, 1991.\n\n265. d. e. goldberg, k. deb, and b. korb. messy genetic algorithms revisited: studies in\n\nmixed size and scale. complex systems, 4:415\u2013444, 1990.\n\n266. d. e. goldberg, k. deb, and b. korb. don\u2019t worry, be messy. in r. k. belew and l. b.\nbooker, editors. proceedings of the 4th international conference on genetic algorithms.\nmorgan kaufmann, san mateo, ca, 1991.\n\n "}, {"Page_number": 439, "text": "436\n\nreferences\n\n267. d. e. goldberg, b. korb, and k. deb. messy genetic algorithms: motivation, analysis,\n\nand first results. complex systems, 3:493\u2013530, 1989.\n\n268. d. e. goldberg and r. lingle. alleles, loci, and the travelling salesman problem. in j. j.\ngrefenstette, editor. proceedings of an international conference on genetic algorithms\nand their applications, pages 154\u2013159. lawrence erlbaum associates, hillsdale, nj,\n1985.\n\n269. d. goldfarb. a family of variable metric methods derived by variational means. mathe-\n\nmatics of computation, 24:23\u201326, 1970.\n\n270. a. a. goldstein. on steepest descent. siam journal on control and optimization, 3:147\u2013\n\n271. p. i. good. permutation tests: a practical guide to resampling methods for testing\n\nhypotheses, 2nd ed. springer, new york, 2000.\n\n272. p. i. good. resampling methods: a practical guide to data analysis, 2nd ed. birkh\u00a8auser,\n\n151, 1965.\n\nboston, 2001.\n\n273. n. j. gordon. a hybrid bootstrap filter for target tracking in clutter. ieee transactions\n\non aerospace and electronic systems, 33:353\u2013358, 1997.\n\n274. n. j. gordon, d. j. salmon, and a. f. m. smith. a novel approach to nonlinear/non-\ngaussian bayesian state estimation. ieee proceedings in radar and signal processing,\n140:107\u2013113, 1993.\n\n275. f. g\u00a8otze and h. r. k\u00a8unsch. second-order correctness of the blockwise bootstrap for\n\nstationary observations. annals of statistics, 24:1914\u20131933, 1996.\n\n276. b. s. grant and l. l. wiseman. recent history of melanism in american peppered moths.\n\njournal of heredity, 93:86\u201390, 2002.\n\n277. d. graybill. campito mountain data set. igbp pages/world data center for paleoclimatol-\nogy data contribution series 1983-ca533.rwl. noaa/ncdc paleoclimatology program,\nboulder, co, 1983.\n\n278. p. j. green. reversible jump markov chain monte carlo computation and bayesian model\n\ndetermination. biometrika, 82:711\u2013732, 1995.\n\n279. p. j. green. trans-dimensional markov chain monte carlo. in p. j. green, n. l. hjort,\nand s. richardson, editors. highly structured stochastic systems, pages 179\u2013198. oxford\nuniversity press, oxford, 2003.\n\n280. p. j. green and b. w. silverman. nonparametric regression and generalized linear\n\nmodels. chapman & hall, new york, 1994.\n\n281. j. w. greene and k. j. supowit. simulated annealing without rejected moves. in proceed-\n\nings of the ieee international conference on computer design, 1984.\n\n282. j. w. greene and k. j. supowit. simulated annealing without rejected moves. ieee\n\ntransactions on computer-aided design, cad-5:221\u2013228, 1986.\n\n283. u. grenander and m. miller. representations of knowledge in complex systems (with\n\ndiscussion). journal of the royal statistical society, series b, 56:549\u2013603, 1994.\n\n284. b. grund, p. hall, and j. s. marron. loss and risk in smoothing parameter selection.\n\njournal of nonparametric statistics, 4:107\u2013132, 1994.\n\n285. c. gu. smoothing spline density estimation: a dimensionless automatic algorithm. jour-\n\nnal of the american statistical association, 88:495\u2013504, 1993.\n\n286. a. guisan, t. c. edwards, jr., and t. hastie. generalized linear and generalized addi-\ntive models in studies of speices distributions: setting the scene. ecological modelling,\n157:89\u2013100, 2002.\n\n "}, {"Page_number": 440, "text": "references\n\n437\n\n287. f. gustafsson, f. gunnarsson, n. bergman, u. forssell, j. jansson, r. karlsson, and p-j.\nnordlund. particle filters for positioning, navigation, and tracking. ieee transactions on\nsignal processing, 50:425\u2013437, 2002.\n\n288. h. haario, e. saksman, and j. tamminen. an adaptive metropolis algorithm. bernoulli,\n\n7(2):223\u2013242, 2001.\n\n289. j. d. f. habbema, j. hermans, and k. van der broek. a stepwise discriminant analysis\nprogramusingdensityestimation.ing.bruckman,editor.compstat1974,proceedings\nin computational statistics. physica, vienna, 1974.\n\n290. s. haber. numerical evaluation of multiple integrals. siam review, 12:481\u2013526,\n\n291. r. p. haining. spatial data analysis: theory and practice. cambridge university press,\n\n292. b. hajek. cooling schedules for optimal annealing. mathematics of operations research,\n\n1970.\n\ncambridge, 2003.\n\n13:311\u2013329, 1988.\n\n293. p. hall. large sample optimality of least squares cross-validation in density estimation.\n\nannals of statistics, 11:1156\u20131174, 1983.\n\n294. p. hall. antithetic resampling for the bootstrap. biometrika, 76:713\u2013724, 1989.\n295. p. hall. the bootstrap and edgeworth expansion. springer, new york, 1992.\n296. p.hallandj.l.horowitz.bootstrapcriticalvaluesfortestsbasedongeneralized-method-\n\nof-moments estimators. econometrica, 64:891\u2013916, 1996.\n\n297. p. hall, j. l. horowitz, and b.-y. jing. on blocking rules for the bootstrap with dependent\n\ndata. biometrika, 82:561\u2013574, 1995.\n\n298. p. hall and j. s. marron. extent to which least squares cross-validation minimises inte-\ngrated squared error in nonparametric density estimation. probability theory and related\nfields, 74:567\u2013581, 1987.\n\n299. p. hall and j. s. marron. lower bounds for bandwidth selection in density estimation.\n\nprobability theory and related fields, 90:149\u2013173, 1991.\n\n300. p. hall, j. s. marron, and b. u. park. smoothed cross-validation. probability theory and\n\nrelated fields, 92:1\u201320, 1992.\n\n301. p. hall, s. j. sheather, m. c. jones, and j. s. marron. on optimal data-based bandwidth\n\nselection in kernel density estimation. biometrika, 78:263\u2013269, 1991.\n\n302. p. hall and s. r. wilson. two guidelines for bootstrap hypothesis testing. biometrics,\n\n47:757\u2013762, 1991.\n\n303. j. hammersley and k. morton. poor man\u2019s monte carlo. journal of the royal statistical\n\nsociety, series b, 16:23\u201328, 1954.\n\n304. j. m. hammersley and k. w. morton. a new monte carlo technique: antithetic variates.\n\nproceedings of the cambridge philosophical society, 52:449\u2013475, 1956.\n\n305. m. h. hansen and c. kooperberg. spline adaptation in extended linear models (with\n\ndiscussion). statistical science, 17:2\u201351, 2002.\n\n306. p. hansen and b. jaumard. algorithms for the maximum satisfiability problem. comput-\n\n307. w. h\u00a8ardle. resistant smoothing using the fast fourier transform. applied statistics,\n\n308. w. h\u00a8ardle. applied nonparametric regression. cambridge university press, cambridge,\n\ning, 44:279\u2013303, 1990.\n\n36:104\u2013111, 1986.\n\n1990.\n\n309. w. h\u00a8ardle. smoothing techniques: with implementation in s. springer, new york, 1991.\n\n "}, {"Page_number": 441, "text": "438\n\nreferences\n\n310. w. h\u00a8ardle, p. hall, and j. s. marron. how far are automatically chosen regression smooth-\ning parametersfromtheir optimum?(withdiscussion). journal of the american statistical\nassociation, 83:86\u201399, 1988.\n\n311. w. h\u00a8ardle, p. hall, and j. s. marron. regression smoothing parameters that are not far\nfrom their optimum. journal of the american statistical association, 87:227\u2013233, 1992.\n312. w. h\u00a8ardle, j. horowitz, and j-p. kreiss. bootstrap methods for time series. international\n\nstatistical review, 71:435\u2013459, 2003.\n\n313. w.h\u00a8ardleandj.s.marron.randomapproximationstoanerrorcriterionofnonparametric\n\nstatistics. journal of multivariate analysis, 20:91\u2013113, 1986.\n\n314. w. h\u00a8ardle and m. g. schimek, editors. statistical theory and computational aspects of\n\n315. w. h\u00a8ardle and d. scott. smoothing by weighted averaging using rounded points. com-\n\nsmoothing. physica, heidelberg, 1996.\n\nputational statistics, 7:97\u2013128, 1992.\n\n316. g. h. hardy. mendelian proportions in a mixed population. science, 28:49\u201350, 1908.\n317. j. a. hartigan and m. a. wong. a k-means clustering algorithm. applied statistics,\n\n28:100\u2013108, 1979.\n\n318. d. i. hastie and p. j. green. model choice using reversible jump markov chain monte\n\ncarlo. statistica neerlandica, 66(3):309\u2013338, 2012.\n\n319.  t. j. hastie. principal curve library for s. available at http://lib.stat.cmu.edu/, 2004.\n320. t. j. hastie and d. pregibon. generalized linear models. in j. m. chambers and t. j.\n\nhastie, editors. statistical models in s. chapman & hall, new york, 1993.\n\n321. t. j. hastie and w. steutzle. principal curves. journal of the american statistical asso-\n\n322. t. j. hastie and r. j. tibshirani. generalized additive models. chapman & hall, new\n\nciation, 84:502\u2013516, 1989.\n\nyork, 1990.\n\n323. t. j. hastie, r. j. tibshirani, and j. friedman. the elements of statistical learning: data\n\nmining, inference, and prediction. springer, new york, 2001.\n\n324. w. k. hastings. monte carlo sampling methods using markov chains and their applica-\n\ntions. biometrika, 57:97\u2013109, 1970.\n\n325. p. henrici. elements of numerical analysis. wiley, new york, 1964.\n326. t. hesterberg. weighted average importance sampling and defensive mixture distribu-\n\ntions. technometrics, 37:185\u2013194, 1995.\n\n327. d. higdon. comment on \u201cspatial statistics and bayesian computation,\u201d by besag and\n\ngreen. journal of the royal statistical society, series b, 55(1):78, 1993.\n\n328. d. m. higdon. auxiliary variable methods for markov chain monte carlo with applica-\n\ntions. journal of the american statistical association, 93:585\u2013595, 1998.\n\n329. m. d. higgs and j. a. hoeting. a clipped latent variable model for spatially correlated\nordered categorical data. computational statistics & data analysis, 54(8):1999\u20132011,\n2010.\n\n330. j. s. u. hjorth. computer intensive statistical methods: validation, model selection, and\n\nbootstrap. chapman & hall, new york, 1994.\n\n331. j. a. hoeting, d. madigan, a. e. raftery, and c. t. volinsky. bayesian model averaging:\n\na tutorial (with discussion). statistical science, 14:382\u2013417, 1999.\n\n332. j. a. hoeting, a. e. raftery, and d. madigan. bayesian variable and transformation selec-\ntion in linear regression. journal of computational and graphical statistics, 11(3):485\u2013\n507, 2002.\n\n "}, {"Page_number": 442, "text": "references\n\n439\n\n2011.\n\n333. j. h. holland. adaptation in natural and artificial systems. university of michigan press,\n\nann arbor, 1975.\n\n334. c. c. holmes and b. k. mallick. generalized nonlinear modeling with multivariate free-\nknot regression splines. journal of the american statistical association, 98(462):352\u2013\n368, 2003.\n\n335. a. homaifar, s. guan, and g. e. liepins. schema analysis of the traveling salesman\n\nproblem using genetic algorithms. complex systems, 6:533\u2013552, 1992.\n\n336. d. w. hosmer and s. lemeshow. applied logistic regression. wiley, new york, 2000.\n337. y. f. huang and p. m. djuric. variable selection by perfect sampling. eurasip journal\n\non applied signal processing, 1:38\u201345, 2002.\n\n338. p. j. huber. projection pursuit. annals of statistics, 13:435\u2013475, 1985.\n339. k. hukushima and k. nemoto. exchange monte carlo method and application to spin\n\nglass simulations. journal of the physical society of japan, 64:1604\u20131608, 1996.\n\n340. j. n. hwang, s. r. lay, and a. lippman. nonparametric multivariate density estima-\ntion: a comparative study. ieee transactions on signal processing, 42:2795\u20132810,\n1994.\n\n341.  r. j. hyndman. time series data library. available at http://robjhyndman.com/tsdl,\n\n342. a.c.jacinto,r.d.ambrosini,andr.f.danesi.experimentalandcomputationalanalysis\nof plates under air blast loading. international journal of impact engineering, 25:927\u2013\n947, 2001.\n\n343. m. jamshidian and r. i. jennrich. conjugate gradient acceleration of the em algorithm.\n\njournal of the american statistical association, 88:221\u2013228, 1993.\n\n344. m. jamshidian and r. i. jennrich. acceleration of the em algorithm by using quasi-\nnewton methods. journal of the royal statistical society, series b, 59:569\u2013587, 1997.\n345. m. jamshidian and r. i. jennrich. standard errors for em estimation. journal of the royal\n\nstatistical society, series b, 62:257\u2013270, 2000.\n\n346. c. z. janikow and z. michalewicz. an experimental comparison of binary and floating\npoint representations in genetic algorithms. in r. k. belew and l. b. booker, editors. pro-\nceedings of the 4th international conference on genetic algorithms. morgan kaufmann,\nsan mateo, ca, 1991.\n\n347. b. jansen. interior point techniques in optimization: complementarity, sensitivity and\n\nalgorithms. kluwer, boston, 1997.\n\n348. p. jarratt. a review of methods for solving nonlinear algebraic equations in one variable.\nin p. rabinowitz, editor. numerical methods for nonlinear algebraic equations. gordon\nand breach, london, 1970.\n\n349. r. g. jarrett. a note on the intervals between coal-mining disasters. biometrika, 66:191\u2013\n\n350. h. jeffreys. theory of probability, 3rd ed. oxford university press, new york, 1961.\n351. d. s. johnson. bayesian analysis of state-space models for discrete response compo-\n\nsitions. ph.d. thesis, colorado state university, 2003.\n\n352. d. s. johnson, c. r. aragon, l. a. mcgeoch, and c. schevon. optimization by simulated\nannealing: an experimental evaluation; part i, graph partitioning. operations research,\n37:865\u2013892, 1989.\n\n353. l. w. johnson and r. d. riess. numerical analysis. addison-wesley, reading, ma,\n\n193, 1979.\n\n1982.\n\n "}, {"Page_number": 443, "text": "440\n\nreferences\n\n354. r. w. johnson. fitting percentage of body fat to simple body measurements. journal of\n\nstatistics education, 4(1):265\u2013266, 1996.\n\n355. g. l. jones, m. haran, b. s. caffo, and r. neath. fixed-width output analysis for markov\nchainmontecarlo.journaloftheamericanstatisticalassociation,101(476):1537\u20131547,\n2006.\n\n356. m. c. jones. variable kernel density estimates. australian journal of statistics, 32:361\u2013\n\n357. m. c. jones. the roles of ise and mise in density estimation. statistics and probability\n\n358. m. c. jones, j. s. marron, and b. u. park. a simple root n bandwidth selector. annals of\n\n371, 1990.\n\nletters, 12:51\u201356, 1991.\n\nstatistics, 19:1919\u20131932, 1991.\n\n359. m. c. jones, j. s. marron, and s. j. sheather. a brief survey of bandwidth selection for\ndensity estimation. journal of the american statistical association, 91:401\u2013407, 1996.\n360. m. c. jones, j. s. marron, and s. j. sheather. progress in data-based bandwidth selection\n\nfor kernel density estimation. computational statistics, 11:337\u2013381, 1996.\n\n361. b. h. juang and l. r. rabiner. hidden markov models for speech recognition. techno-\n\n362. n.karmarkar.anewpolynomial-timealgorithmforlinearprogramming.combinatorica,\n\n363. j.r.karrandd.r.dudley.ecologicalperspectivesonwaterqualitygoals.environmental\n\nmetrics, 33:251\u2013272, 1991.\n\n4:373\u2013395, 1984.\n\nmanagement, 5(1):55\u201368, 1981.\n\n364. r. e. kass, b. p. carlin, a. gelman, and r. m. neal. markov chain monte carlo in\n\npractice: a roundtable discussion. american statistican, 52:93\u2013100, 1998.\n\n365. r. e. kass and a. e. raftery. bayes factors. journal of the american statistical associ-\n\nation, 90:773\u2013795, 1995.\n\nrun sampling. operations research, 46:84\u201395, 1998.\n\n366. d. e. kaufman and r. l. smith. direction choice for accelerated convergence in hit-and-\n367.  b.  k\u00b4egl.  principal  curve  webpage.  available  at  http://www.iro.umontreal.ca/\u223ckegl/\n368. c. t. kelley. detection and remediation of stagnation in the nelder-mead algorithm using\n\nresearch/pcurves/.\n\na sufficient decrease condition. siam journal on optimization, 10:43\u201355, 1999.\n\n369. c. t. kelley. iterative methods for optimization. society for industrial and applied math-\n\nematics, philadelphia, pa, 1999.\n\n370. a. g. z. kemna and a. c. f. vorst. a pricing method for options based on average asset\n\nvalues. journal of banking and finance, 14:113\u2013129, 1990.\n\n371. m.kendallanda.stuart.theadvancedtheoryofstatistics,volume1,4thed.macmillan,\n\nnew york, 1977.\n\n372. j. kennedy and r. eberhart. particle swarm optimization. in neural networks, 1995.\nproceedings,ieeeinternationalconferenceon,vol.4,pages1942\u20131948,vol.4.nov/dec\n1995.\n\n373. j. kennedy and r. c. roberts. swarm intelligence. morgan kaufmann, san francisco,\n\n374. w. j. kennedy, jr., and j. e. gentle. statistical computing. marcel dekker, new york,\n\n375. h. f. khalfan, r. h. byrd, and r. b. schnabel. a theoretical and experimental study of\n\nthe symmetric rank-one update. siam journal of optimization, 3:1\u201324, 1993.\n\n2001.\n\n1980.\n\n "}, {"Page_number": 444, "text": "references\n\n441\n\n1997.\n\n376. d. r. kincaid and e. w. cheney. numerical analysis. wadsworth, belmont, ca, 1991.\n377. r. kindermann and j. l. snell. markov random fields and their applications, volume\n\n1 of contemporary mathematics. american mathematical society, providence, 1980.\n\n378. s. kirkpatrick, c. d. gellat, and m. p. vecchi. optimization by simulated annealing.\n\nscience, 220:671\u2013680, 1983.\n\n379. g. kitagawa. monte carlo filter and smoother for non-gaussian nonlinear state space\n\nmodels. journal of computational and graphical statistics, 5:1\u201325, 1996.\n\n380. s. klinke and j. grassmann. projection pursuit regression. in m. g. schimek, editor.\nsmoothing and regression: approaches, computation, and application, pages 277\u2013327.\nwiley, new york, 2000.\n\n381. t. kloek and h. k. van dijk. bayesian estimates of equation system parameters: an\n\napplication of integration by monte carlo. econometrica, 46:1\u201320, 1978.\n\n382. l. knorr-held and h. rue. on block updating in markov random field models for disease\n\nmapping. scandinavian journal of statistics, 29(4):567\u2013614, 2002.\n\n383. d. knuth. the art of computer programming 2: seminumerical algorithms, 3rd ed.\n\naddison-wesley, reading, ma, 1997.\n\n384. m. kofler. maple: an introduction and reference. addison-wesley, reading, ma,\n\n385. t.g.kolda,r.m.lewis,andv.torczon.optimizationbydirectsearch:newperspectives\n\non some classical and modern methods. siam review, 45:385\u2013482, 2003.\n\n386. a. kong, j. s. liu, and w. h. wong. sequential imputations and bayesian missing data\n\nproblems. journal of the american statistical association, 89:278\u2013288, 1994.\n\n387. a. s. konrod. nodes and weights of quadrature formulas. consultants bureau enter-\n\nprises, new york, 1966.\n\npolspline, 2004.\n\n388.  c. kooperberg. polspline. available at http://cran.r-project.org/src/contrib/descriptions/\n\n389. c. kooperberg and c. j. stone. logspline density estimation. computational statistics\n\nand data analysis, 12:327\u2013347, 1991.\n\n390. c. kooperberg and c. j. stone. logspline density estimation for censored data. journal\n\nof computational and graphical statistics, 1:301\u2013328, 1992.\n\n391. c. kooperberg, c. j. stone, and y. k. truong. hazard regression. journal of the american\n\nstatistical association, 90:78\u201394, 1995.\n\n2001.\n\n392. t. koski. hidden markov models of bioinformatics. kluwer, dordrecht, netherlands,\n393. j.-p. kreiss. bootstrap procedures for ar(\u221e)-processes. in k.-h. j\u00a8ockel, g. rothe, and\nw. sendler, editors. bootstrapping and related techniques, pages 107\u2013113. springer,\nberlin, 1992.\n\n394. k. kremer and k. binder. monte carlo simulation of lattice models for macromolecules.\n\ncomputer physics reports, 7:259\u2013310, 1988.\n\n395. v.i.krylov,translatedbya.h.stroud. approximate calculation of integrals.macmillan,\n\nnew york, 1962.\n\n396. h. r. k\u00a8unsch. the jackknife and the bootstrap for general stationary observations. annals\n\nof statistics, 17:1217\u20131241, 1989.\n\n397. j. c. lagarias, j. a. reeds, m. h. wright, and p. e. wright. convergence properties of\nthe nelder-mead simplex algorithm in low dimensions. siam journal of optimization,\n9:112\u2013147, 1998.\n\n "}, {"Page_number": 445, "text": "442\n\nreferences\n\n398. s. n. lahiri. edgeworth correction by \u201cmoving block\u201d bootstrap for stationary and non-\nstationary data. in r. lepage and l. billard, editors. exploring the limits of the bootstrap,\npages 183\u2013214. wiley, new york, 1992.\n\n399. s. n. lahiri. on edgeworth expansion and moving block bootstrap for studentized m-\nestimators in multiple linear regression models. journal of multivariate analysis, 56:42\u2013\n59, 1996.\n\n400. s. n. lahiri. theoretical comparisons of block bootstrap methods. annals of statistics,\n\n27:386\u2013404, 1999.\n\n401. s. n. lahiri. on the jackknife-after-bootstrap method for dependent data and its consis-\n\ntency properties. econometric theory, 18:79\u201398, 2002.\n\n402. s. n. lahiri. resampling methods for dependent data. springer, new york, 2003.\n403. s. n. lahiri. consistency of the jackknife-after-bootstrap variance estimator for block\nbootstrap quantiles of a studentized statistic. annals of statistics, 33:2475\u20132506, 2005.\n404. s. n. lahiri, k. furukawa, and y.-d. lee. a nonparametric plug-in rule for selecting\noptimal block lengths for block bootstrap methods. statistical methodology, 4:292\u2013321,\n2007.\n\n405. c.lalasandb.murphy.increaseintheabundanceofnewzealandfursealsatthecatlins,\nsouth island, new zealand. journal of the royal society of new zealand, 28:287\u2013294,\n1998.\n\n406. d. lamberton and b. lapeyre. introduction to stochastic calculus applied to finance.\n\nchapman & hall, london, 1996.\n\n407. k. lange. a gradient algorithm locally equivalent to the em algorithm. journal of the\n\nroyal statistical society, series b, 57:425\u2013437, 1995.\n\n408. k. lange. a quasi-newton acceleration of the em algorithm. statistica sinica, 5:1\u201318,\n\n409. k. lange. numerical analysis for statisticians. springer, new york, 1999.\n410. k. lange, d. r. hunter, and i. yang. optimization transfer using surrogate objective\nfunctions (with discussion). journal of computational and graphical statistics, 9:1\u201359,\n2000.\n\n411. a. b. lawson. statistical methods in spatial epidemiology. wiley, new york, 2001.\n412. s. z. li. markov random field modeling in image analysis. springer, tokyo, 2001.\n413. r. j. a. little and d. b. rubin. statistical analysis with missing data, 2nd ed. wiley,\n\nhoboken, nj, 2002.\n\n414. e. l. little, jr. atlas of united states trees, minor western hardwoods, volume 3 of\n\nmiscellaneous publication 1314. u.s. department of agriculture, 1976.\n\n415. c. liu and d. b. rubin. the ecme algorithm: a simple extension of em and ecm with\n\nfaster monotone convergence. biometrika, 81:633\u2013648, 1994.\n\n416. j. s. liu. nonparametric hierarchical bayes via sequential imputations. annals of statis-\n\ntics, 24:910\u2013930, 1996.\n\n417. j. s. liu. monte carlo strategies in scientific computing. springer, new york, 2001.\n418. j. s. liu and r. chen. blind deconvolution via sequential imputations. journal of the\n\namerican statistical association, 90:567\u2013576, 1995.\n\n419. j. s. liu and r. chen. sequential monte carlo methods for dynamical systems. journal\n\nof the american statistical association, 93:1032\u20131044, 1998.\n\n420. j. s. liu, f. liang, and w. h. wong. the multiple-try method and local optimization in\nmetropolis sampling. journal of the american statistical association, 95:121\u2013134, 2000.\n\n1995.\n\n "}, {"Page_number": 446, "text": "references\n\n443\n\n1999.\n\n421. j. s. liu, d. b. rubin, and y. wu. parameter expansion to accelerate em: the px-em\n\nalgorithm. biometrika, 85:755\u2013770, 1998.\n\n422. c. r. loader. bandwidth selection: classical or plug-in? annals of statistics, 27:415\u2013438,\n\n423. p. o. loftsgaarden and c. p. quesenberry. a nonparametric estimate of a multivariate\n\nprobability density function. annals of mathematical statistics, 28:1049\u20131051, 1965.\n\n424. t. a. louis. finding the observed information matrix when using the em algorithm.\n\njournal of the royal statistical society, series b, 44:226\u2013233, 1982.\n\n425. r. y. lui and k. singh. moving blocks jackknife and bootstrap capture weak dependence.\ninr.lepageandl.billard,editors.exploringthelimitsofthebootstrap,pages225\u2013248.\nwiley, new york, 1992.\n\n426. m. lundy and a. mees. convergence of an annealing algorithm. mathematical program-\n\nming, 34:111\u2013124, 1986.\n\n427. d. j. lunn, n. best, and j. c. whittaker. generic reversible jump mcmc using graphical\n\nmodels. statistics and computing, 19(4):395\u2013408, 2009.\n\n428. d. j. lunn, n. best, and j. c. whittaker. generic reversible jump mcmc using graphical\n\nmodels. statistics and computing, 19(4):395\u2013408, 2009.\n\n429. s. n. maceachern and l. m. berliner. subsampling the gibbs sampler. american statis-\n\ntican, 48(3):188\u2013190, 1994.\n\n430. s. n. maceachern, m. clyde, and j. s. liu. sequential importance sampling for nonpara-\nmetric bayes models: the next generation. canadian journal of statistics, 27:251\u2013267,\n1999.\n\n431. a. maddison. dynamic forces in capitalist development: a long-run comparative\n\nview. oxford university press, new york, 1991.\n\n432. n. madras. lecture notes on monte carlo methods. american mathematical society,\n\nprovidence, ri, 2002.\n\n433. n. madras and m. piccioni. importance sampling for families of distributions. annals of\n\n434. b. a. maguire, e. s. pearson, and a. h. a. wynn. the time intervals between industrial\n\napplied probability, 9:1202\u20131225, 1999.\n\naccidents. biometrika, 39:168\u2013180, 1952.\n\n435. c. l. mallows. some comments on cp. technometrics, 15:661\u2013675, 1973.\n436. e. mammen. resampling methods for nonparametric regression. in m. g. schimek, edi-\ntor. smoothing and regression: approaches, computation, and application. wiley, new\nyork, 2000.\n\n437. j.-m. marin and c. robert. importance sampling methods for bayesian discrimination\nbetween embedded models. in frontiers of statistical decision making and bayesian\nanalysis, pages 513\u2013527. springer, new york, 2010.\n\n438. e. marinari and g. parisi. simulated tempering: a new monte carlo scheme. europhysics\n\n439. j. s. maritz. distribution free statistical methods, 2nd ed. chapman & hall, london,\n\nletters, 19:451\u2013458, 1992.\n\n1996.\n\n440. j. s. marron and d. nolan. canonical kernels for density estimation. statistics and prob-\n\nability letters, 7:195\u2013199, 1988.\n\n441. g. marsaglia. random variables and computers. in transactions of the third prague con-\nference on information theory, statistical decision functions and random processes.\nczechoslovak academy of sciences, prague, 1964.\n\n "}, {"Page_number": 447, "text": "444\n\nreferences\n\n442. g. marsaglia. the squeeze method for generating gamma variates. computers and math-\n\nematics with applications, 3:321\u2013325, 1977.\n\n443. g. marsaglia. the exact-approximation method for generating random variables in a\n\ncomputer. journal of the american statistical association, 79:218\u2013221, 1984.\n\n444. g. marsaglia and w. w. tsang. a simple method for generating gamma variables. acm\n\ntransactions on mathematical software, 26:363\u2013372, 2000.\n\n445. w. l. martinez and a. r. martinez. computational statistics handbook with matlab.\n\nchapman & hall/crc, boca raton, fl, 2002.\n\n446. p. mccullagh and j. a. nelder. generalized linear models. chapman & hall, new york,\n\n447. s. mcginnity and g. w. irwin. multiple model bootstrap filter for maneuvering target\ntracking. ieee transactions on aerospace and electronic systems, 36:1006\u20131012, 2000.\n448. k. i. m. mckinnon. convergence of the nelder-mead simplex method to a nonstationary\n\npoint. siam journal on optimization, 9:148\u2013158, 1998.\n\n449. g. j. mclachlan and t. krishnan. the em algorithm and extensions. wiley, new york,\n\n1989.\n\n1997.\n\n450. i. meilijson. a fast improvement to the em algorithm on its own terms. journal of the\n\nroyal statistical society, series b, 51:127\u2013138, 1989.\n\n451. j.meinguet.multivariateinterpolationatarbitrarypointsmadesimple.journal of applied\n\nmathematics and physics, 30:292\u2013304, 1979.\n\n452. x.-l. meng. on the rate of convergence of the ecm algorithm. annals of statistics,\n\n22:326\u2013339, 1994.\n\n453. x.-l. meng and d. b. rubin. using em to obtain asymptotic variance\u2013covariance ma-\ntrices: the sem algorithm. journal of the american statistical association, 86:899\u2013909,\n1991.\n\n454. x.-l. meng and d. b. rubin. maximum likelihood estimation via the ecm algorithm:\n\na general framework. biometrika, 80:267\u2013278, 1993.\n\n455. x.-l. meng and d. b. rubin. on the global and componentwise rates of convergence of\n\nthe em algorithm. linear algebra and its applications, 199:413\u2013425, 1994.\n\n456. x.-l. meng and d. van dyk. the em algorithm\u2014an old folk-song sung to a fast new\n\ntune. journal of the royal statistical society, series b, 59:511\u2013567, 1997.\n\n457. x.-l. meng and w. h. wong. simulating ratios of normalizing constants via a simple\n\nidentity: a theoretical exploration. statistica sinica, 6:831\u2013860, 1996.\n\n458. k. l. mengersen, c. p. robert, and c. guihenneuc-jouyaux. mcmc convergence diag-\nnostics: a \u201creviewwww\u201d (with discussion). in j. o. berger, j. m. bernardo, a. p. dawid,\nd. v. lindley, and a. f. m. smith, editors. bayesian statistics 6, pages 415\u2013440. oxford\nuniversity press, oxford, 1999.\n\n459. r. c. merton. theory of rational option pricing. bell journal of economics and manage-\n\nment science, 4:141\u2013183, 1973.\n\n460. n. metropolis, a. w. rosenbluth, m. n. rosenbluth, a. h. teller, and e. teller. equation\nof state calculation by fast computing machines. journal of chemical physics, 21:1087\u2013\n1091, 1953.\n\n461. n. metropolis and s. ulam. the monte carlo method. journal of the american statistical\n\nassociation, 44:335\u2013341, 1949.\n\n462. s. p. meyn and r. l. tweedie. markov chains and stochastic stability. springer, new\n\nyork, 1993.\n\n "}, {"Page_number": 448, "text": "references\n\n445\n\n2001.\n\n463. z. michalewicz. genetic algorithms + data structures = evolution programs. springer,\n\n464. z. michalewicz and d. b. fogel. how to solve it: modern heuristics. springer, new\n\n465. a. j. miller. subset selection in regression, 2nd ed. chapman & hall/crc, boca raton,\n\nnew york, 1992.\n\nyork, 2000.\n\nfl, 2002.\n\n466. a. mira, j. m\u00f8ller, and g. o. roberts. perfect slice samplers. journal of the royal sta-\n\ntistical society, series b, 63(3):593\u2013606, 2001.\n\n467. a. mira and l. tierney. efficiency and convergence properties of slice samplers. scandi-\n\nnavian journal of statistics, 29(1):1\u201312, 2002.\n\n468. j. m\u00f8ller. perfect simulation of conditionally specified models. journal of the royal\n\nstatistical society, series b, 61(1):251\u2013264, 1999.\n\n469. j. f. monahan. numerical methods of statistics. cambridge university press, cambridge,\n\n470. a. m. mood, f. a. graybill, and d. c. boes. introduction to the theory of statistics, 3rd\n\ned. mcgraw-hill, new york, 1974.\n\n471. r. j. muirhead. aspects of multivariate statistical theory. wiley, new york, 1982.\n472. p. m\u00a8uller. a generic approach to posterior integration and gibbs sampling. tech-\nnical report, statistics department, technical report 91-09. purdue university,\n1991.\n\n473. d.j.murdochandp.j.green.exactsamplingfromacontinuousstatespace.scandinavian\n\njournal of statistics, 25(3):483\u2013502, 1998.\n\n474. d. j. murdoch and j. s. rosenthal. efficient use of exact samples. statistics and comput-\n\n475. w. murray, editor. numerical methods for unconstrained optimization. academic, new\n\n476. e. a. nadaraya. on estimating regression. theory of probability and its applications,\n\ning, 10:237\u2013243, 2000.\n\nyork, 1972.\n\n10:186\u2013190, 1964.\n\n477. y. nagata and s. kobayashi. edge assembly crossover: a high-power genetic algorithm\nforthetravelingsalesmanproblem.int.b\u00a8ack,editor.proceedingsofthe7thinternational\nconference on genetic algorithms. morgan kaufmann, los altos, ca, 1997.\n\n478. j. c. naylor and a. f. m. smith. applications of a method for the efficient computation\n\nof posterior distributions. applied statistics, 31:214\u2013225, 1982.\n\n479. l. nazareth and p. tseng. gilding the lily: a variant of the nelder-mead algorithm based\non golden-section search. computational optimization and applications, 22:133\u2013144,\n2002.\n\n480. r. neal. sampling from multimodal distributions using tempered transitions. statistics\n\nand computing, 6:353\u2013366, 1996.\n\n481. r. m. neal. slice sampling. annals of statistics, 31(3):705\u2013767, 1999.\n482. j. a. nelder and r. mead. a simplex method for function minimization. computer jour-\n\nnal, 7:308\u2013313, 1965.\n\nmodels. irwin, chicago, 1996.\n\n483. j. neter, m. h. kutner, c. j. nachtsheim, and w. wasserman. applied linear statistical\n\n484. m. a. newton and c. j. geyer. bootstrap recycling: a monte carlo alternative to\nthe nested bootstrap. journal of the american statistical association, 89:905\u2013912,\n1994.\n\n "}, {"Page_number": 449, "text": "446\n\nreferences\n\n485. m. a. newton and a. e. raftery. approximate bayesian inference with the weighted\nlikelihood bootstrap (with discussion). journal of the royal statistical society, series b,\n56:3\u201348, 1994.\n\n486. j. nocedal and s. j. wright. numerical optimization. springer, new york, 1999.\n487. i. ntzoufras, p. dellaportas, and j. j. forster. bayesian variable and link determination for\ngeneralised linear models. journal of statistical planning and inference, 111(1\u20132):165\u2013\n180, 2003.\n\n488. j. null. golden gate weather services, climate of san francisco. available at\n\nhttp://ggweather.com/sf/climate.html.\n\n489.  numerical recipes home page. available at http://www.nr.com, 2003.\n490. m. s. oh and j. o. berger. adaptive importance sampling in monte carlo integration.\n\njournal of statistical computation and simulation, 41:143\u2013168, 1992.\n\n491. m. s. oh and j. o. berger. integration of multimodal functions by monte carlo\nimportance sampling. journal of the american statistical association, 88:450\u2013456,\n1993.\n\n492. i. oliver, d. smith, and j. r. holland. a study of permutation crossover operators on the\ntraveling salesman problem. in j. j. grefenstette, editor. proceedings of the 2nd interna-\ntional conference on genetic algorithms, pages 224\u2013230. lawrence erlbaum associates,\nhillsdale, nj, 1987.\n\n493. d. m. olsson and l. s. nelson. the nelder\u2013mead simplex procedure for function mini-\n\nmization. technometrics, 17:45\u201351, 1975.\n\n494. j. m. ortega, w. c. rheinboldt, and j. m. orrega. iterative solution of nonlinear equa-\n\ntions in several variables. siam, philadelphia, 2000.\n\n495. a. m. ostrowski. solution of equations and systems of equations, 2nd ed. academic,\n\nnew york, 1966.\n\n496. f. o\u2019sullivan. discussion of \u201csome aspects of the spline smoothing approach to nonpara-\nmetric regression curve fitting\u201d by silverman. journal of the royal statistical society,\nseries b, 47:39\u201340, 1985.\n\n497. c. h. papadimitriou and k. steiglitz. combinatorial optimization: algorithms and com-\n\nplexity. prentice-hall, englewood cliffs, nj, 1982.\n\n498. e. paparoditis and d. n. politis. tapered block bootstrap. biometrika, 88:1105\u20131119,\n\n499. e. paparoditis and d. n. politis. the tapered block bootstrap for general statistics from\n\nstationary sequences. econometrics journal, 5:131\u2013148, 2002.\n\n500. b. u. park and j. s. marron. comparison of data-driven bandwidth selectors. journal of\n\nthe american statistical association, 85:66\u201372, 1990.\n\n501. b. u. park and b. a. turlach. practical performance of several data driven bandwidth\n\nselectors. computational statistics, 7:251\u2013270, 1992.\n\n502. j. m. parkinson and d. hutchinson. an investigation of into the efficiency of variants\non the simplex method. in f. a. lootsma, editor. numerical methods for nonlinear\noptimization, pages 115\u2013135. academic, new york, ny, 1972.\n\n503. c. pascutto, j. c. wakefield, n. g. best, s. richardson, l. bernardinelli, a. staines, and\np. elliott. statistical issues in the analysis of disease mapping data. statistics in medicine,\n19:2493\u20132519, 2000.\n\n504. j. j. pella and p. k. tomlinson. a generalized stock production model. inter-american\n\ntropical tuna commission bulletin, 13:419\u2013496, 1969.\n\n2001.\n\n "}, {"Page_number": 450, "text": "references\n\n447\n\n505. a. penttinen. modelling interaction in spatial point patterns: parameter estimation by\n\nthe maximum likelihood method. ph.d. thesis, university of jyv\u00a8askyl\u00a8a, 1984.\n\n506. a. philippe. processing simulation output by riemann sums. journal of statistical com-\n\nputation and simulation, 59:295\u2013314, 1997.\n\n507. a. philippe and c. p. robert. riemann sums for mcmc estimation and convergence\n\nmonitoring. statistics and computing, 11:103\u2013115, 2001.\n\n508. d. b. phillips and a. f. m. smith. bayesian model comparison via jump diffusions. in\ns. t. richardson w. r. gilks and d. j. spiegelhalter, editors. markov chain monte carlo\nin practice, pages 215\u2013240. chapman & hall/crc, london, 1996.\n\n509. e. j. g. pitman. significance tests which may be applied to samples from any population.\n\nroyal statistical society supplement, 4:119\u2013130, 225\u2013232, 1937.\n\n510. e. j. g. pitman. significance tests which may be applied to samples from any population.\n\npart iii. the analysis of variance test. biometrika, 29:322\u2013335, 1938.\n\n511. m. plummer, n. best, k. cowles, and k. vines. coda: convergence diagnosis and output\n\nanalysis for mcmc. r news, 6(1):7\u201311, 2006.\n\n512. d. n. politis and j. p. romano. a circular block\u2014resampling procedure for stationary\ndata. in r. lepage and l. billard, editors. exploring the limits of the bootstrap, pages\n263\u2013270. wiley, new york, 1992.\n\n513. d. n. politis and j. p. romano. the stationary bootstrap. journal of the american statis-\n\ntical association, 89:1303\u20131313, 1994.\n\n514. m. j. d. powell. a view of unconstrained optimization. in l. c. w. dixon, editor. opti-\n\nmization in action, pages 53\u201372. academic, london, 1976.\n\n515. m. j. d. powell. direct search algorithms for optimization calculations. acta numerica,\n\n516. g. pozrikidis. numerical computation in science and engineering. oxford university\n\n7:287\u2013336, 1998.\n\npress, new york, 1998.\n\n517. w. h. press, s. a. teukolsky, w. t. vetterling, and b. p. flannery. numerical recipes:\n\nthe art of scientific computing. cambridge university press, cambridge, uk, 2007.\n\n518. c. j. price, i. d. coope, and d. byatt. a convergent variant of the nelder\u2013mead algorithm.\n\njournal of optimization theory and applications, 113:5\u201319, 2002.\n\n519. j. propp and d. wilson. coupling from the past: a user\u2019s guide. in d. aldous and j. propp,\neditors. microsurveys in discrete probability, volume 41 of dimacs series in discrete\nmathematicsandtheoreticalcomputerscience,pages181\u2013192.americanmathematical\nsociety. princeton, nj, 1998.\n\n520. j. g. propp and d. b. wilson. exact sampling with coupled markov chains and applica-\n\ntions to statistical mechanics. random structures and algorithms, 9:223\u2013252, 1996.\n\n521. m. h. protter and c. b. morrey. a first course in real analysis. springer, new york,\n\n1977.\n\nca, 1993.\n\n205, 1991.\n\n522. j. r. quinlan. c4.5 : programs for machine learning. morgan kaufmann, san mateo,\n\n523. l.r.rabinerandb.h.juang.anintroductiontohiddenmarkovmodels. ieeeacoustics,\n\nspeech, and signal processing magazine, 3:4\u201316, 1986.\n\n524. n.j.radcliffe.equivalenceclassanalysisofgeneticalgorithms.complexsystems,5:183\u2013\n\n525. a. e. raftery and v. e. akman. bayesian analysis of a poisson process with a change\n\npoint. biometrika, 73:85\u201389, 1986.\n\n "}, {"Page_number": 451, "text": "448\n\nreferences\n\n526. a. e. raftery and s. m. lewis. how many iterations in the gibbs sampler? in j. m.\nbernardo, j. o. berger, a. p. dawid, and a. f. m. smith, editors. bayesian statistics 4,\npages 763\u2013773. oxford university press, oxford, 1992.\n\n527. a. e. raftery, d. madigan, and j. a. hoeting. bayesian model averaging for linear re-\n\ngression models. journal of the american statistical association, 92:179\u2013191, 1997.\n\n528. a. e. raftery and j. e. zeh. estimating bowhead whale, balaena mysticetus, popula-\ntion size and rate of increase from the 1993 census. journal of the american statistical\nassociation, 93:451\u2013463, 1998.\n\n529. m. b. rajarshi. bootstrap in markov sequences based on estimates of transition density.\n\nannals of the institute of statistical mathematics, 42:253\u2013268, 1990.\n\n530. r. a. redner and h. f. walker. mixture densities, maximum likelihood and the em\n\nalgorithm. siam review, 26:195\u2013239, 1984.\n\n531. c. r. reeves. genetic algorithms. in c. r. reeves, editor. modern heuristic techniques\n\nfor combinatorial problems. wiley, new york, 1993.\n\n532. c. r. reeves. a genetic algorithm for flowshop sequencing. computers and operations\n\n533. c. r. reeves and j. e. rowe. genetic algorithms\u2014principles and perspectives. kluwer,\n\nresearch, 22(1):5\u201313, 1995.\n\nnorwell, ma, 2003.\n\n534. c. r. reeves and n. c. steele. a genetic algorithm approach to designing neural network\narchitecture. in proceedings of the 8th international conference on systems engineering.\n1991.\n\n535. j. r. rice. numerical methods, software, and analysis. mcgraw-hill, new york, 1983.\n536. s. richardson and p. j. green. on bayesian analysis of mixtures with an unknown num-\nber of components (with discussion). journal of the royal statistical society, series b,\n59:731\u2013792, 1997. correction, p. 661, 1998.\n\n537. c. j. f. ridders. 3-point iterations derived from exponential curve fitting. ieee transac-\n\ntions on circuits and systems, 26:669\u2013670, 1979.\n\n538. b. ripley. computer generation of random variables. international statistical review,\n\n539. b. ripley. stochastic simulation. wiley, new york, 1987.\n540. b. d. ripley. pattern recognition and neural networks. cambridge university press,\n\n51:301\u2013319, 1983.\n\n1996.\n\n541. c. ritter and m. a. tanner. facilitating the gibbs sampler: the gibbs stopper and the\ngriddy-gibbs sampler. journal of the american statistical association, 87(419):861\u2013868,\n1992.\n\n542. c. p. robert. discretization and mcmc convergence assessment, volume 135 of lecture\n\nnotes in statistics. springer, new york, 1998.\n\n543. c. p. robert and g. casella. monte carlo statistical methods, 2nd ed. springer, new\n\n544. c. p. robert and g. casella. convergence monitoring and adaptation for mcmc algo-\nrithms. introducing monte carlo methods with r, pages 237\u2013268. springer new york,\n2010.\n\n545. g. o. roberts, a. gelman, and w. r. gilks. weak convergence and optimal scaling or\n\nrandom walk metropolis algorithms. annals of probability, 7(1):110\u2013120, 1997.\n\n546. g. o. roberts and s. k. sahu. updating schemes, correlation structure, blocking and\nparameterization for the gibbs sampler. journal of the royal statistical society, series b,\n59(2):291\u2013317, 1997.\n\nyork, 2004.\n\n "}, {"Page_number": 452, "text": "references\n\n449\n\n547. g. o. roberts and r. l. tweedie. exponential convergence of langevin diffusions and\n\ntheir discrete approximations. bernoulli, 2:344\u2013364, 1996.\n\n548. g. o.roberts and j. s.rosenthal.optimal scaling ofdiscrete approximations to langevin\ndiffusions. journal of the royal statistical society: series b (statistical methodology),\n60(1):255\u2013268, 1998.\n\n549. g. o. roberts and j. s. rosenthal. optimal scaling for various metropolis-hastings al-\n\ngorithms. statistical science, 16(4):351\u2013367, 2001.\n\n550. g. o. roberts and j. s. rosenthal. coupling and ergodicity of adaptive markov chain\n\nmonte carlo algorithms. journal of applied probability, 44(2):458, 2007.\n\n551. g. o. roberts and j. s. rosenthal. examples of adaptive mcmc. journal of computa-\n\ntional and graphical statistics, 18(2):349\u2013367, 2009.\n\n552. c. roos, t. terlaky, and j. p. vial. theory and algorithms for linear optimization: an\n\ninterior point approach. wiley, chichester, uk, 1997.\n\n553. m. rosenbluth and a. rosenbluth. monte carlo calculation of the average extension of\n\nmolecular chains. journal of chemical physics, 23:356\u2013359, 1955.\n\n554. jeffrey s. rosenthal. optimal proposal distributions and adaptive mcmc. in handbook\n\nof markov chain monte carlo methods. chapman & hall/crc, hoboken, nj, 2011.\n\n555. s. m. ross. simulation, 2nd ed. academic, san diego, ca, 1997.\n556. s. m. ross. introduction to probability models, 7th ed. academic, 2000.\n557. r. y. rubenstein. simulation and the monte carlo method. wiley, new york, 1981.\n558. d. b. rubin. the bayesian bootstrap. annals of statistics, 9:130\u2013134, 1981.\n559. d. b. rubin. a noniterative sampling/importance resampling alternative to the data aug-\nmentation algorithm for creating a few imputations when fractions of missing information\nare modest: the sir algorihm. discussion of m. a. tanner and w. h. wong. journal of\nthe american statistical association, 82:543\u2013546, 1987.\n\n560. d.b.rubin.usingthesiralgorithmtosimulateposteriordistributions.inj.m.bernardo,\nm. h. degroot, d. v. lindley, and a. f. smith, editors. bayesian statistics 3, pages 395\u2013\n402. clarendon, oxford, 1988.\n\n561. m. rudemo. empirical choice of histograms and kernel density estimators. scandinavian\n\njournal of statistics, 9:65\u201378, 1982.\n\n562. w. rudin. principles of mathematical analysis, 3rd ed. mcgraw-hill, new york, 1976.\n563. h. rue. fast sampling of gaussian markov random fields. journal of the royal statistical\n\nsociety, series b, 63:325\u2013338, 2001.\n\n564. d. ruppert, s. j. sheather, and m. p. wand. an effective bandwidth selector for local\nleast squares regression. journal of the american statistical association, 90:1257\u20131270,\n1995.\n\n565. a. s. rykov. simplex algorithms for unconstrained minimization. problems of control\n\nand information theory, 12:195\u2013208, 1983.\n\n566. s. m. sait and h. youssef. iterative computer algorithms with applications to engineer-\ning: solving combinatorial optimization problems. ieee computer society press, los\nalamitos, ca, 1999.\n\n567. d. b. sanders, j. m. mazzarella, d. c. kim, j. a. surace, and b. t. soifer. the iras\n\nrevised bright galaxy sample (rgbs). astronomical journal, 126:1607\u20131664, 2003.\n\n568. g. sansone. orthogonal functions. interscience publishers, new york, 1959.\n569. d. j. sargent, j. s. hodges, and b. p. carlin. structured markov chain monte carlo.\n\njournal of computational and graphical statistics, 9(2):217\u2013234, 2000.\n\n "}, {"Page_number": 453, "text": "450\n\nreferences\n\n570. l. scaccia and p. j. green. bayesian growth curves using normal mixtures with non-\nparametric weights. journal of computational and graphical statistics, 12(2):308\u2013331,\n2003.\n\n571. j. d. schaffer, r. a. caruana, l. j. eshelman, and r. das. a study of control parameters\naffecting online performance of genetic algorithms for function optimization. in j. d.\nschaffer, editor. proceedings of the 3rd international conference on genetic algorithms.\nmorgan kaufmann, los altos, ca, 1989.\n\n572. t. schiex and c. gaspin. carthagene: constructing and joining maximum likeli-\nhood genetic maps. in t. gaasterland, p. d. karp, k. karplus, c. ouzounis, c. sander,\nand a. valencia, editors. proceedings of the 5th international conference on intelligent\nsystems for molecular biology, pages 258\u2013267. menlo park, ca, 1997. association for\nartificial intelligence (aaai).\n\n573. m. g. schimek, editor. smoothing and regression: approaches, computation, and ap-\n\nplication. wiley, new york, 2000.\n\n574. m. g. schimek and b. a. turlach. additive and generalized additive models. in m. g.\nschimek, editor. smoothing and regression: approaches, computation, and application,\npages 277\u2013327. wiley, new york, 2000.\n\n575. u. schneider and j. n. corcoran. perfect simulation for bayesian model selection in a\nlinear regression model. journal of statistical planning and inference, 126(1):153\u2013171,\n2004.\n\n576. c. schumacher, d. whitley, and m. vose. the no free lunch and problem description\nlength.in genetic and evolutionary computation conference, gecco-2001,pages565\u2013\n570. morgan kaufmann, san mateo, ca, 2001.\n\n577. l. l. schumaker. spline functions: basic theory. wiley, new york, 1993.\n578. e. f. schuster and g. g. gregory. on the nonconsistency of maximum likelihood density\nestimators. in w. g. eddy, editor. proceedings of the thirteenth interface of computer\nscience and statistics, pages 295\u2013298. springer, new york, 1981.\n\n579. g. schwartz. estimating the dimension of a model. annals of statistics, 6:497\u2013511, 1978.\n580. d. w. scott. average shifted histograms: effective nonparametric estimators in several\n\ndimensions. annals of statistics, 13:1024\u20131040, 1985.\n\n581. d. w. scott. multivariate density estimation: theory, practice, and visualization. wiley,\n\nnew york, 1992.\n\n582. d. w. scott and l. e. factor. monte carlo study of three data-based nonparametric density\n\nestimators. journal of the american statistical association, 76:9\u201315, 1981.\n\n583. d. w. scott and g. r. terrell. biased and unbiased cross-validation in density estimation.\n\njournal of the american statistical association, 82:1131\u20131146, 1987.\n\n584. j. m. scott, p. j. heglund, m. l. morrison, j. b. haufler, m. g. raphael, w. q. wall, and\nf. b. samson, editors. predicting species occurrences\u2014issues of accuracy and scale.\nisland press, washington, dc, 2002.\n\n585. g. a. f. seber. the estimation of animal abundance and related parameters, 2nd ed.\n\ncharles griffin, london, 1982.\n\n586. r. seydel. tools for computational finance. springer, berlin, 2002.\n587. k. shahookar and p. mazumder. vlsi cell placement techniques. acm computing\n\nsurveys, 23:143\u2013220, 1991.\n\n588. d. f. shanno. conditioning of quasi-newton methods for function minimization. math-\n\nematics of computation, 24:647\u2013657, 1970.\n\n "}, {"Page_number": 454, "text": "references\n\n451\n\n589. j. shao and d. tu. the jackknife and bootstrap. springer, new york, 1995.\n590. x. shao. the dependent wild bootstrap. journal of the american statistical association,\n\n105:218\u2013235, 2010.\n\n591. x. shao. extended tapered block bootstrap. statistica sinica, 20:807\u2013821, 2010.\n592. s. j. sheather. the performance of six popular bandwidth selection methods on some real\n\ndata sets. computational statistics, 7:225\u2013250, 1992.\n\n593. s. j. sheather and m. c. jones. a reliable data-based bandwidth selection method for\nkernel density estimation. journal of the royal statistical society, series b, 53:683\u2013690,\n1991.\n\n594. y.shiandr.eberhart.amodifiedparticleswarmoptimizer.inevolutionarycomputation\nproceedings, 1998. ieee world congress on computational intelligence, the 1998 ieee\ninternational conference on, pages 69\u201373. may 1998.\n\n595. g. r. shorack. probability for statisticians. springer, new york, 2000.\n596. b. w. silverman. kernel density estimation using the fast fourier transform. applied\n\nstatistics, 31:93\u201399, 1982.\n\n597. b. w. silverman. some aspects of the spline smoothing approach to non-parametric\nregression curve fitting (with discussion). journal of the royal statistical society, series\nb, 47:1\u201352, 1985.\n\n598. b. w. silverman. density estimation for statistics and data analysis. chapman & hall,\n\n599. j. s. simonoff. smoothing methods in statistics. springer, new york, 1996.\n600. s. singer and s. singer. efficient implementation of the nelder\u2013mead search algorithm.\n\napplied numerical analysis and computational mathematics, 1:524\u2013534, 2004.\n\n601. k. singh. on the asymptotic accuracy of efron\u2019s bootstrap. annals of statistics, 9:1187\u2013\n\nlondon, 1986.\n\n1195, 1981.\n\n602. d. j. sirag and p. t. weisser. towards a unified thermodynamic genetic operator. in\nj. j. grefenstette, editor. proceedings of the 2nd international conference on genetic\nalgorithms and their applications. lawrence erlbaum associates, hillsdale, nj, 1987.\n603. s. a. sisson. transdimensional markov chains. journal of the american statistical asso-\n\nciation, 100(471):1077\u20131089, 2005.\n\n604. s. a. sisson. transdimensional markov chains: a decade of progress and future\nperspectives. journal of the american statistical association, 100(471):1077\u20131090,\n2005.\n\n605. a.f.m.smithandg.o.roberts.bayesiancomputationviathegibbssamplerandrelated\nmarkov chain monte carlo methods (with discussion). journal of the royal statistical\nsociety, series b, 55:3\u201323, 1993.\n\n606. a. f. m. smith, a. m. skene, j. e. h. shaw, and j. c. naylor. progress with numeri-\ncal and graphical methods for practical bayesian statistics. the statistician, 36:75\u201382,\n1987.\n\n607. b. j. smith. boa: an r package for mcmc output convergence assessment and posterior\n\ninference. journal of statistical software, 21(11):1\u201337, 2007.\n\n608. p. j. smith, m. shafi, and h. gao. quick simulation: a review of importance sampling\ntechniques in communications systems. ieee journal on selected areas in communica-\ntions, 15:597\u2013613, 1997.\n\n609. d. sorenson and d. gianola. likelihood, bayesian and mcmc methods in quantitative\n\ngenetics. springer, new york, 2002.\n\n "}, {"Page_number": 455, "text": "452\n\nreferences\n\n610. d. spiegelhalter, d. thomas, n. best, and d. lunn. winbugs user manual, version\n1.4. mrc biostatistics unit, institute of public health, cambridge, 2003. available at\nhttp://www.mrc-bsu.cam.ac.uk/bugs.\n\n611. p. stavropoulos and d. m. titterington. improved particle filters and smoothing. in\na. doucet, n. de freitas, and n. gordon, editors. sequential monte carlo methods in\npractice, pages 295\u2013317. springer, new york, 2001.\n\n612.  d. steinberg. salford systems. available at http://www.salford-systems.com, 2003.\n613. m. stephens. bayesian analysis of mixture models with an unknown number of\ncomponents\u2014an alternative to reversible jump methods. annals of statistics, 28(1):40\u2013\n74, 2000.\n\n614. c.j.stone.anasymptoticallyoptimalwindowselectionruleforkerneldensityestimation.\n\nannals of statistics, 12:1285\u20131297, 1984.\n\n615. c. j. stone, m. hansen, c. kooperberg, and y. k. truong. polynomial splines and\ntheir tensor products in extended linear modeling (with discussion). annals of statistics,\n25:1371\u20131470, 1997.\n\n616. m. stone. cross-validatory choice and assessment of statistical predictions. journal of\n\nthe royal statistical society, series b, 36:111\u2013147, 1974.\n\n617. o. stramer and r. l. tweedie. langevin-type models i: diffusions with given stationary\ndistributions, and their discretizations. methodology and computing in applied proba-\nbility, 1:283\u2013306, 1999.\n\n618. o. stramer and r. l. tweedie. langevin-type models ii: self-targeting candidates for\nmcmc algorithms. methodology and computing in applied probability, 1:307\u2013328,\n1999.\n\n619. a. h. stroud. approximate calculation of multiple integrals. prentice-hall, englewood\n\ncliffs, nj, 1971.\n\ncliffs, nj, 1966.\n\n620. a. h. stroud and d. secrest. gaussian quadrature formulas. prentice-hall, englewood\n\n621. r. h. swendsen and j.-s. wang. nonuniversal critical dynamics in monte carlo simula-\n\ntions. physical review letters, 58(2):86\u201388, 1987.\n\n622. g. syswerda. uniform crossover in genetic algorithms. in j. d. schaffer, editor. pro-\nceedings of the 3rd international conference on genetic algorithms, pages 2\u20139. morgan\nkaufmann, los altos, ca, 1989.\n\n623. g. syswerda. schedule optimization using genetic algorithms. in l. davis, editor. hand-\nbook of genetic algorithms, pages 332\u2013349. van nostrand reinhold, new york, 1991.\n624. m. a. tanner. tools for statistical inference: methods for the exploration of posterior\n\ndistributions and likelihood functions, 2nd ed. springer, new york, 1993.\n\n625. m. a. tanner. tools for statistical inference: methods for the exploration of posterior\n\ndistributions and likelihood functions, 3rd ed. springer, new york, 1996.\n\n626. r development core team. r: a language and environment for statistical computing.\n\nr foundation for statistical computing, vienna, austria, 2012.\n\n627. g. r. terrell. the maximal smoothing principle in density estimation. journal of the\n\namerican statistical association, 85:470\u2013477, 1990.\n\n628. g. r. terrell and d. w. scott. variable kernel density estimation. annals of statistics,\n\n20:1236\u20131265, 1992.\n\n629. t. therneau and b. atkinson. an introduction to recursive partitioning using the rpart\nroutines. technical report, mayo clinic, 1997. available at http://lib.stat.cmu.edu, 1997.\n\n "}, {"Page_number": 456, "text": "references\n\n453\n\n630. r. a. thisted. elements of statistical computing: numerical computation. chapman &\n\nhall, new york, 1988.\n\n631. r. tibshirani. estimating optimal transformations for regression via additivity and vari-\nance stabilization. journal of the american statistical association, 82:559\u2013568, 1988.\n632. r. tibshirani and k. knight. model search by bootstrap \u201cbumping.\u201d journal of compu-\n\ntational and graphical statistics, 8:671\u2013686, 1999.\n\n633. l. tierney. markov chains for exploring posterior distributions (with discussion). annals\n\nof statistics, 22:1701\u20131786, 1994.\n\n634. d. m. titterington. recursive parameter estimation using incomplete data. journal of the\n\nroyal statistical society, series b, 46:257\u2013267, 1984.\n\n635. h. tjelmeland and j. besag. markov random fields with higher-order interactions. scan-\n\ndinavian journal of statistics, 25:415\u2013433, 1998.\n\n636. p. tseng. fortified-descent simplicial search method: a general approach. siam journal\n\non optimization, 10:269\u2013288, 2000.\n\n637. e. turro, a. lewin, a. rose, m. j. dallman, and s. richardson. mmbgx: a method\nfor estimating expression at the isoform level and detecting differential splicing using\nwhole-transcript affymetrix arrays. nucleic acids research, 38(1):e4\u2013e4, 2010.\n\n638. g. l. tyler, g. balmino, d. p. hinson, w. l. sjogren, d. e. smith, r. woo, j. w.\narmstrong, f. m. flasar, r. a. simpson, s. asmar, a. anabtawi, and p. priest. mars\nglobal surveyor radio science data products. data can be obtained at http://www-\nstar.stanford.edu/projects/mgs/public.html, 2004.\n\n639. u.s. environmental protection agency, environmental monitoring and assessment\n\nprogram (emap). available at http://www.epa.gov/emap.\n\n640. d. a. van dyk and x.-l. meng. the art of data augmentation (with discussion). journal\n\nof computational and graphical statistics, 10(1):1\u2013111, 2001.\n\n641. p. j. m. van laarhoven and e. h. l. aarts. simulated annealing: theory and applications.\n\nkluwer, boston, 1987.\n\nyork, 1994.\n\n642. w. n. venables and b. d. ripley. modern applied statistics with s-plus. springer, new\n\n643. w. n. venables and b. d. ripley. modern applied statistics with s-plus, 3rd ed. springer,\n\n644.  j.  j.  verbeek.  principal  curve  webpage.  available  at  http://carol.wins.uva.nl/\n\n645. c. vogl and s. xu. qtl analysis in arbitrary pedigrees with incomplete marker informa-\n\n646. m. d. vose. the simple genetic algorithm: foundations and theory. mit press,\n\n647. m. d. vose. form invariance and implicit parallelism. evolutionary computation, 9:355\u2013\n\ncambridge, ma, 1999.\n\n370, 2001.\n\n648. r. waagepetersen and d. sorensen. a tutorial on reversible jump mcmc with a view\ntoward applications in qtl-mapping. international statistical review, 69(1):49\u201361,\n2001.\n\n649. g. wahba. spline models for observational data. siam, philadelphia, 1990.\n650. f. h. walters, l. r. parker, s. l. morgan, and s. n. deming. sequential simplex\n\noptimization. crc press, boca raton, fl, 1991.\n\n651. m. p. wand and m. c. jones. kernel smoothing. chapman & hall, new york, 1995.\n\nnew york, 2002.\n\u223cjverbeek/pc/index en.html.\ntion. heredity, 89(5):339\u2013345, 2002.\n\n "}, {"Page_number": 457, "text": "454\n\nreferences\n\n652. m. p. wand, j. s. marron, and d. ruppert. transformations in density estimation. journal\n\nof the american statistical association, 86:343\u2013353, 1991.\n\n653. x. wang, c. z. he, and d. sun. bayesian population estimation for small sample capture\u2013\nrecapture data using noninformative priors. journal of statistical planning and inference,\n137(4):1099\u20131118, 2007.\n\n654. m. r. watnik. pay for play: are baseball salaries based on performance? journal of\n\nstatistics education, 6(2), 1998.\n\n655. g. s. watson. smooth regression analysis. sankhy\u00afa, series a, 26:359\u2013372, 1964.\n656. g. c. g. wei and m. a. tanner. a monte carlo implementation of the em algorithm\nand the poor man\u2019s data augmentation algorithms. journal of the american statistical\nassociation, 85:699\u2013704, 1990.\n\n657. m. west. modelling with mixtures. in j. m. bernardo, m. h. degroot, and d. v. lindley,\n\neditors. bayesian statistics 2, pages 503\u2013524. oxford university press, oxford, 1992.\n\n658. m. west. approximating posterior distributions by mixtures. journal of the royal statis-\n\ntical society, series b, 55:409\u2013422, 1993.\n\n659. s. r. white. concepts of scale in simulated annealing. in proceedings of the ieee inter-\n\nnational conference on computer design. 1984.\n\n660. d. whitley. the genitor algorithm and selection pressure: shy rank-based allocation\nof reproductive trials is best. in j. d. schaffer, editor. proceedings of the 3rd international\nconference on genetic algorithms. morgan kaufmann, los altos, ca, 1989.\n\n661. d. whitley. a genetic algorithm tutorial. statistics and computing, 4:65\u201385, 1994.\n662. d. whitley. an overview of evolutionary algorithms. journal of information and software\n\ntechnology, 43:817\u2013831, 2001.\n\n663. d.whitley,t.starkweather,andd.fuquay.schedulingproblemsandtravelingsalesman:\nthe genetic edge recombination operator. in j. d. schaffer, editor. proceedings of the 3rd\ninternational conference on genetic algorithms, pages 133\u2013140. morgan kaufmann,\nlos altos, ca, 1989.\n\n664. d. whitley, t. starkweather, and d. shaner. the traveling salesman and sequence\nscheduling: quality solutions using genetic edge recombination. in l. davis, editor.\nhandbook of genetic algorithms, pages 350\u2013372. von nostrand reinhold, new york,\n1991.\n\n665. p. wilmott, j. dewynne, and s. howison. option pricing: mathmatical models and\n\ncomputation. oxford financial press, oxford, 1997.\n\n666. d. b. wilson. how to couple from the past using a read-once source of randomness.\n\nrandom structures and algorithms, 16(1):85\u2013113, 2000.\n\n667. d. b. wilson. web site for perfectly random sampling with markov chains. available at\n\nhttp://dbwilson.com/exact, august 2002.\n\n668. g. winkler. image analysis, random fields and markov chain monte carlo methods,\n\n2nd ed. springer, berlin, 2003.\n\n669. p. wolfe. convergence conditions for ascent methods. siam review, 11:226\u2013235, 1969.\n670. r. wolfinger and m. o\u2019connell. generalized linear models: a pseudo-likelihood ap-\n\nproach. journal of computational and graphical statistics, 48:233\u2013243, 1993.\n\n671. s. wolfram. mathematica: a system for doing mathematics by computer. addison-\n\nwesley, redwood city, ca, 1988.\n\n672. d. h. wolpert and w. g. macready. no free lunch theorems for search. technical report\n\nsfi-tr-95-02-010, santa fe institute, nm, 1995.\n\n "}, {"Page_number": 458, "text": "references\n\n455\n\n673. m. a. woodbury. discussion of \u201cthe analysis of incomplete data\u201d by hartley and hock-\n\ning. biometrics, 27:808\u2013813, 1971.\n\n674. b. j. worton. optimal smoothing parameters for multivariate fixed and adaptive kernel\n\nmethods. journal of statistical computation and simulation, 32:45\u201357, 1989.\n\n675. m.h.wright.directsearchmethods:oncescorned,nowrespectable.ind.f.griffithsand\ng.a.watson,editors. numerical analysis 1995, proc. 1995 dundee bienneal conference\nin numerical analysis, pages 191\u2013208. harlow, u.k., addison-wesley longman, 1996.\n676. c. f. j. wu. on the convergence properties of the em algorithm. annals of statistics,\n\n11:95\u2013103, 1983.\n\n677. h. youssef, s. m. sait, k. nassar, and m. s. t. benton. performance driven standard-cell\nplacement using genetic algorithm. in glsvlsi\u201995: fifth great lakes symposium on\nvlsi. 1995.\n\n678. b. yu and p. mykland. looking at markov samplers through cusum plots: a simple\n\ndiagnostic idea. statistics and computing, 8:275\u2013286, 1998.\n\n679. j.l.zhangandj.s.liu.anewsequentialimportancesamplingmethodanditsapplication\nto the two-dimensional hydrophobic-hydrophilic model. journal of chemical physics,\n117:3492\u20133498, 2002.\n\n680. p. zhang. nonparametric importance sampling. journal of the american statistical as-\n\nsociation, 91:1245\u20131253, 1996.\n\n681. w.zhao,a.krishnaswamy,r.chellappa,d.l.swets,andj.weng.discriminantanalysis\nof principal components for face recognition. in h. wechsler, p. j. phillips, v. bruce, f. f.\nsoulie, and t. s. huang, editors. face recognition: from theory to applications, pages\n73\u201385. springer, berlin, 1998.\n\n682. z. zheng. on swapping and simulated tempering algorithms. stochastic processes and\n\ntheir applications, 104:131\u2013154, 2003.\n\n "}, {"Page_number": 459, "text": "index\n\nbootstrap, bca, 294\n\nabsolute convergence criterion, 24, 34\naccelerated bias-corrected bootstrap, see\naccelerated em methods, 118\u2013121\nace, see alternating conditional\nexpectations, 403\nactivation function, 402\nadaptation factor, 239, 240\nadaptive importance sampling, 167\nadaptive kernel, 348\nadaptive mcmc, 237\u2013249\n\nfor adaptive mcmc, 244, 247, 248\n\nacceptance rate, 238, 240, 245\nadaptation factor, 240, 244, 247, 248\nadaptive metropolis algorithm, 247\u2013249\nbatching, 238, 248\u2013249\nbounded convergence, 238, 240, 245\ndiminishing adaptation, 238, 240, 245,\nmetropolis-within-gibbs, 240\u2013247\nrandom walk metropolis-within-gibbs,\n\n247\u2013249\n\n238\u2013240\n\nadaptive quadrature, 147\nadaptive rejection sampling, 159\u2013162\nadditive model, 394\u2013397\nadditive predictor, 397\nadditivity and variance stabilization,\n404\u2013405\naic, see akaike information criterion, 64\naids, 121\u2013123\nair blast pressure, 390\u2013391\naitken acceleration\nakaike information criterion, 64\nallele, 61\nalmost everywhere, 13\n\nfor em algorithm, 118\u2013119\n\n403\u2013404\n140\u2013142, 145\nsquared error, 331\nalgorithm, 46\n\nalmost sure convergence, 13\nalternating conditional expectations,\nalzheimer\u2019s disease, 131\u2013135, 137,\namise, see asymptotic mean integrated\namoeba method, see nelder\u2013mead\nannealing, 68\nantithetic bootstrap, 302\u2013303\nantithetic sampling, 186\u2013189\naperiodic markov chain, 16, 202\nascent algorithm, 39\u201340, 66\n\nbacktracking, 39\u201340\nrandom ascent, 66\nsteepest ascent, 39, 66\nstep length, 39\nasian option, 192\naspiration criteria, 88\u201389\nasymptotic mean integrated squared error,\n331\u2013332, 339, 347\nasymptotically unbiased estimator, 14\nauxiliary variable methods, 251, 256\u2013260,\n\n274\u2013277\n\nfor markov random fields, 274\u2013277\n\navas, see additivity and variance\naverage squared jumping distance,\n\nstabilization, 404\n246\n\nbackfitting, 52\u201353, 394\nbacktracking, 39\u201340, 42\nbagging, 317\nbalanced bootstrap, 302\nballoon estimator, 349\n\ncomputational statistics, second edition. geof h. givens and jennifer a. hoeting.\n\u00a9 2013 john wiley & sons, inc. published 2013 by john wiley & sons, inc.\n\n457\n\n "}, {"Page_number": 460, "text": "458\n\nindex\n\nbandwidth, 327, 329\u2013339, 347\u2013351\n\nfor smoothing, see smoothing, bandwidth,\noptimal, 332, 335\n\n374\n\n255\u2013256\n\nfor adaptive mcmc, 238, 248\u2013249\n\nbaseball salaries, 67\u201368, 73\u201374, 78, 91\u201394,\nbatch method, 227\nbatch times\nbaum\u2013welch algorithm, 124\u2013126\nbayes factor, 11\nbayes information criterion, 343\nbayes\u2019 theorem, 11\nbayesian bootstrap, 317\nbayesian estimation, 11\u201313, 98, 102, 106,\n148, 151, 157\u2013158, 166\u2013167, 176,\n184, 204, 212\u2013214, 217, 228\u2013230,\n240\u2013247, 254, 269\u2013279, 325\nbca, 294\u2013296, 315\nbcv, see biased cross-validation, 334\nbenthic invertebrates, 210\u2013212\nbernoulli distribution\n\ndefinition, 5\nsimulation, 154\nbeta distribution\ndefinition, 6\nsimulation, 154\nbeverton\u2013holt model, 320\nbfgs update, 42\nbias, 291\u2013292, 330\nbias\u2013variance trade-off, 328\u2013329, 340\nbiased cross-validation, 334\nbic, see bayes information criterion,\n\u201cbig oh\u201d notation, 2\nbinomial coefficient, 4\nbinomial distribution\n\n343\n\ndefinition, 5\nsimulation, 154\n\nbisection method, 23\u201325, 30\nbiweight kernel, 339, 380\nblack\u2013scholes model, 191\nblock bootstrap, 304\u2013315\n\nblock size, 311\u2013315\ncentering, 309\u2013311\ncircular, 311\ndependent wild, 316\nmoving, 306\u2013307\nnonmoving, 304\u2013306\nsieve, 316\n\nstationary, 311\nstudentizing, 309\u2013311\ntapered, 316\nfor block bootstrap, 311\u2013315\n\nblock size\nblocks-of-blocks bootstrap, 307\u2013309\nboa software, 226\nbody fat, 416\u2013417\nbond variable, 275\nbootstrap, 287\u2013317\n\naggregating, see bagging, 317\nantithetic, 302\u2013303\nasymptotics, 315\u2013316\nbagging, 292, 317\nbalanced, 302\nbayesian, 317\nbca, 294\u2013296, 315\nbias correction, 291\u2013292, 302\nblock, 304\u2013315\nblocks of blocks, 307\u2013309\nbumping, 317\ncentering, 309\u2013311\ncircular block, 311\nconfidence interval, 292\u2013301, 315\nconsistency, 315\ndependent wild, 316\nfor ar() models, 304\nfor dependent data, 303\u2013316\nfor em algorithm, 106, 110\nfor independent data, 288\u2013303, 315\u2013316\nfor regression, 290\u2013291\nfor smoothers, 384\u2013388\nhypothesis testing, 301\u2013302\ninferential methods, 292\u2013302\nlikelihood, 316\nmoving block, 306\u2013307\nnested, 292, 299\u2013301, 315\nnonmoving block, 304\u2013306\npaired, 291\nparametric, 289\u2013290\npercentile method, 292\u2013294, 385\npermutation, 302\npivot, 293\npivoting, 294\u2013302, 309\u2013311\npseudo-data, 287\nregression\n\ncases, 291\nresiduals, 290\n\nsieve, 316\nstationary block, 311\n\n "}, {"Page_number": 461, "text": "studentized, see bootstrap, t, 296,\n309\u2013311\nt, 296\u2013298, 315\ntapered block, 316\ntransformation-respecting, 294, 295\numbrella of model parameters, see\nvariance reduction, 302\u2013303\nvariance-stabilizing transformation, 293,\nweighted likelihood, 317\n\nbumping, 317\n\n298\u2013299\n\nfor adaptive mcmc, 238, 240, 245\n\nbootstrap filter, 179\u2013180\nbounded convergence\nbowhead whales, 334\u2013335, 337, 339, 344\nbox\u2013cox transformation, 353\nbracketing methods, 26\nbreast cancer, 231\u2013233, 320\u2013321\nbridge sampling, 167\u2013168\nbugs software, 226\nbumping, 317\nburn-in, 220\u2013222, 226\ncall option, 191\ncancer, 320\u2013321\ncapture\u2013recapture, 212\u2013214, 217, 228\u2013230\ncarrying capacity, 240\ncart, see tree-based methods, 405\ncauchy distribution\ndefinition, 6\nsimulation, 154\n\ncensored data, 54\u201356, 107\u2013108, 112,\n123\u2013124, 231\u2013233\ncentral limit theorem, 14\ncftp, see coupling from the past, 264\nchi-square distribution\n\ndefinition, 6\nsimulation, 154\n\n320\u2013321\n\ncircular block bootstrap, 311\nclinical trial, 55, 231\u2013233, 317\u2013318,\ncoal-mining disasters, 196\u2013197, 233\ncoda software, 226\ncolorado topography, 176\ncombinatorial optimization, 59\u201392\ncandidate solution, 59\ngenetic algorithm, 75\u201385\nglobally competitive solution, 65\nlocal search, 65\u201368\nparticle swarm, 85\n\nindex\n\n459\n\nproblem complexity, 59\u201361\nsimulated annealing, 68\u201375\nsteepest ascent, 66\ntabu algorithm, 85\u201391\ntraveling salesman problem, 59, 64, 70,\n\n79, 82\u201384\ncomplexity, 59\u201361\ncomposite rule, 129\nconfidence bands, 384\u2013388\nconfidence interval\nbootstrap, 292\u2013301, 315\nconjugate prior distribution, 12\nconsistent estimator, 14\nconstant-span running-mean smoother,\ncontainment, see bounded convergence for\ncontraction\ncontractive mapping, 32\u201333\ncontrol variates, 189\u2013193\n\n366\u2013372\nadaptive mcmc, 238\n\nfor nelder\u2013mead method, 47\n\nimprovement to importance sampling,\n\n190\u2013191\n\nconvergence almost surely, 13\nconvergence criterion\n\nabsolute, 131\nrelative, 131\n\nconvergence in probability, 13\nconvergence order, 2\nconvex function, 4\ncooling schedule, 71\u201372, 75\ncopper\u2013nickel alloy, 291\u2013293, 296\u2013297, 300\ncost\u2013complexity pruning, 410\ncoupling from the past, 264\u2013268, 277\u2013279\ncredible interval, 12\ncross-validation, 332\u2013335, 369\u2013372, 377\n\nfor markov random fields, 277\u2013279\n\nfor smoothing, see smoothing,\ncross-validation, 369\nfor tree-based methods, 410\n\ncrossover, 62\ncubic smoothing spline, 341, 376\ncurse of dimensionality, 152, 345, 393\ncurve fitting, see smoothing, 363\ncusum diagnostic, 219\ncvrss, see residual sum of squares,\ncycle, 210\ncyclic coordinate ascent, see gauss\u2013seidel\n\ncross-validated, 370\n\niteration, 53\n\n "}, {"Page_number": 462, "text": "460\n\nindex\n\n331\u2013332, 339, 347\n\ndarwinian natural selection, 75\ndecoupling, 276\ndegeneracy, 171\u2013173, 179\ndelta method, 297\u2013298\ndensity dependence, 241\ndensity estimation, 325\u2013359, 372\nadaptive kernel, 348\nasymptotic mean integrated squared error,\nballoon, 349\nbandwidth, 327, 329\u2013339, 347\u2013351\nbias\u2013variance trade-off, 328\u2013329, 340\nbiased cross-validation, 334\nchoice of kernel, 339\u2013341\ncross-validation, 332\u2013335\nexploratory projection pursuit, 353\u2013359\nintegrated squared error, 326\u2013327, 333\nkernel, 327\u2013341, 346\u2013348, 375\nlogspline, 341\u2013345\nmaximal smoothing principle, 338\u2013339,\nmean integrated squared error, 326\u2013327,\nmean squared error, 327\nmultivariate, 345\u2013359\nnearest neighbor, 349\u2013350\nplug-in methods, 335\u2013337, 353\nproduct kernel, 375\npseudo-likelihood, 333, 334\nsheather\u2013jones method, 336\u2013337\nsilverman\u2019s rule of thumb, 335\u2013336, 347,\ntransformation, 352\u2013353\nunbiased cross-validation, 333\u2013334,\nunivariate, 325\u2013345\nvariable-kernel, 348, 350\u2013353\n\n348\n330\u2013332\n\n360\u2013361\n\n353\n\ndependent wild bootstrap, 316\nderivative-free method, 45\ndetailed balance, 16, 251\ndifferentiation\nnumerical, 3, 110\u2013111\ndiminishing adaptation\nfor adaptive mcmc, 238, 240, 245,\n\ndirichlet distribution\n\n247\u2013249\ndefinition, 6\nsimulation, 154\n\ndiscrete newton methods, 41\ndistributions\n\nbernoulli, 5\nbeta, 6\nbinomial, 5\ncauchy, 6\nchi-square, 6\ndirichlet, 6\nexponential, 6\ngamma, 6\nlognormal, 7\nmultinomial, 5\nmultivariate normal, 7\nnegative binomial, 5\nnormal, 7\npoisson, 5\nstudent\u2019s t, 7\nuniform, 7\nweibull, 7\n\ndouble bootstrap, see nested bootstrap, 299\ndrug abuse, 399\nearthquakes, 321\necm algorithm, 113\u2013116\nedge-recombination crossover, 83\u201384\neffective sample size\n\nfor importance sampling, 182\nfor markov chain monte carlo, 224\u2013225\nfor sequential importance sampling,\n\n171\u2013173\n\nem algorithm, 97\u2013121, 257\n\nacceleration methods, 118\u2013121\naitken acceleration, 118\u2013119\nascent property, 103\nbootstrapping, 106, 110\nconditional maximization, 113\u2013116\nconvergence, 102\u2013104\ne step, 98, 105, 111\u2013112\necm, 113\u2013116\nempirical information, 110\u2013111\nfor exponential families, 105\u2013106\ngeneralized, 103, 113\ngradient em, 116\u2013118\nlatent data, 97\nlouis\u2019s method, 106\u2013108\nm step, 98, 105, 112\u2013118\nmcem, 111\u2013112\nmissing data, 97\u201398\nmissing information principle, 106, 108\nmonte carlo, 111\u2013112\nnumerical differentiation of l\u2032(\u03b8), 111\nq function, 98, 102, 106\n\n "}, {"Page_number": 463, "text": "quasi-newton acceleration, 119\u2013121\nsem, 106, 108\u2013110\nsupplemented, 106, 108\u2013110\nvariance estimates, 106\u2013111\n\nempirical information, 111\nenvelope\n\n181\u2013182\n\nfor importance sampling, 163, 165,\nfor rejection sampling, 155\u2013157\nfor sequential importance sampling,\n\n171\n\nepanechnikov kernel, 339\u2013340\nequivalent degrees of freedom, 378\nequivalent kernels, 377\u2013379\nergodic markov chain, 16, 265\nergodic theorem, 16\neuler\u2013maclaurin formula, 3, 139\neuropean option, 191\nevolution, see peppered moths, 99\nexpanding pointwise confidence bands,\n386\nexpansion\nexpectation\u2013maximization algorithm, see\nexpected fisher information, 10\nexploratory projection pursuit, 353\u2013359\nexponential distribution\n\nfor nelder\u2013mead method, 46\n\nem algorithm, 97\n\ndefinition, 6\nsimulation, 154\nem algorithm, 105\u2013106\n\nexponential family, 8, 36, 397\n\nface recognition, 37, 417\u2013418\nfeed-forward neural network, 402\u2013403\nfinite differences, 3\nfisher information, 10\n\nexpected, 10\nobserved, 10\n\nfisher scoring, 30, 34\u201339, 398\nfitness, 76, 80\u201381\nfixed-point iteration, 32\u201333, 41\n\nconvergence, 32\u201333\nscaled, 33\n\nflour beetle, 56\u201357\nfunctional, 1, 287\nfunctional iteration, see fixed-point iteration,\nfundamental polynomials, 134\nfur seal pups, 212\u2013214, 217, 228\u2013230\n\n33\n\nindex\n\n461\n\ngam, see generalized additive model, 397\ngamma distribution\n\ndefinition, 6\nsimulation, 154, 157\n\napproximation error, 144\n\ngamma function, 8\ngauss\u2013hermite quadrature, 145\ngauss\u2013legendre quadrature, 145, 149\ngauss\u2013newton method, 38, 44\u201345\ngauss\u2013seidel iteration, 52\u201353, 114, 394\ngaussian quadrature, 142\u2013146\ngdp, 305\u2013307\ngear couplings, 123\u2013124\ngelman\u2013rubin statistic, 221\u2013222\ngem algorithm, see em algorithm,\ngeneralized additive model, 397\u2013399\n\ngeneralized, 103\nadditive predictor, 397\nlocal scoring, 398\nsmoothing, 372, see smoothing,\ngeneralized em, see em algorithm,\ngeneralized linear mixed model, 131\u2013134\ngeneralized linear model, 36\u201337, 397,\n\ngeneralized cross-validation, 377\ngeneralized, 103\n\ngeneralized cross-validation\n\n399\n\ngenetic algorithm, 75\u201385\n\nlink function, 397\nallele, 75, 78\u201379, 82\nbinary encoding, 79\nchromosome, 75\nconvergence, 84\u201385\ncrossover, 76\u201377, 82\u201384\nedge recombination, 83\u201384\nfitness, 76, 80\u201381\ngeneration, 76, 79, 81, 84\ngenetic operators, 76\u201377, 82\u201384\ngenotype, 76\nlocus, 75\nmutation, 77, 80, 84\noffspring, 76\norder crossover, 82\nparent, 76\npermutation chromosome, 79, 82\u201384\nphenotype, 76\nscaled fitness, 93\nschema, 76, 84\u201385\nselection mechanism, 76, 80\u201381\nsteady-state, 81\n\n "}, {"Page_number": 464, "text": "462\n\nindex\n\ngenetic algorithm (continued)\n\ntournament selection, 81\nuniform crossover, 94\ngenetic map distance, 62\ngenetic mapping, 61\u201364, 88, 94\u201395\ngibbs sampling, 209\u2013218, 238, 257,\n\nem, 116\n\ncarlo, diagnostics, 218\n\n270\u2013274\nblocked, 216\ncycle, 210\ndiagnostics, see markov chain monte\nfor markov random fields, 270\u2013274\ngriddy, 218\nhybrid, 216\u2013217, 242\nrandom scan, 216, 248\nrelationship to metropolis\u2013hastings, 215\nreparameterization, 223\u2013224\nglobally competitive solution, 65\ngradient, 1\ngradient em, see em algorithm, gradient\ngreedy algorithm, 66, 68, 344, 408, 409, 412\ngriddy\u2013gibbs sampling, 218\nhat matrix, 373, 376\nhermite polynomials, 144\nhessian matrix, 1\nhidden markov model, 124\u2013126, 175, 176\nhierarchical centering, 223, 233\u2013235\nhighest posterior density region, 12\nhimmelblau\u2019s function, 57\nhit-and-run algorithm, 260\u2013261\nhiv, 121\u2013123\nhormone treatment, 231\u2013233\nhpd region, 12\nhuman face recognition, 37, 417\u2013418\nhybrid markov chain monte carlo, 216\u2013217\ni.i.d., 9\nimage analysis, 269\u2013279\nimportance ratio, 181, 204, 258\nimportance sampling, 163\u2013168, 180\u2013186\n\nresampling, 183\n\nadaptive, 167\nchoice of envelope, 181\u2013182\ncompared with sampling importance\ncontrol variate improvement, 190\u2013191\neffective sample size, 182\nenvelope, 163, 165, 181\nimportance ratio, 181, 204\n\n181\u2013183\n181\u2013183, 258\n181\u2013182\n\nsequential, 169\u2013179\nstandardized importance weights, 164,\nunstandardized importance weights,\nimportance sampling function, 163, 165,\nimproper prior distribution, 12\nindependence chain, 204\u2013206\nindustrialized countries, 305\u2013307\ninfrared emissions, 359\u2013360\ninner product, 143\nintegrated squared error, 326\u2013327, 333\nintegration\n\nmonte carlo, see monte carlo\nnumerical, see numerical integration, 129\n\nintegration, 151\n\nmethod, 153\u2013155\nsquares, 36\n\ninternal node, 406\ninterpolating polynomials, 130, 134\ninverse cumulative distribution function\nirls, see iteratively reweighted least\nirreducible markov chain, 16, 202\nise, see integrated squared error, 326\niterated bootstrap, see nested bootstrap, 299\niterated conditional modes, 114\niteratively reweighted least squares, 36\u201338,\n\n397\n\njacobi polynomials, 144\njacobian matrix, 1, 9\njeffreys prior, 12, 213, 217\njensen\u2019s inequality, 4, 103\nk-change, 65\nkernel, 327\n\nadaptive, 348\nasymptotic relative efficiency, 340\nbiweight, 339, 380\ncanonical, 340\u2013341\nepanechnikov, 339\u2013340\nnormal, 339\nproduct, 345, 346\nrescaling, 340\u2013341\ntriangle, 339\ntricube, 379\ntriweight, 339\nuniform, 328, 339\nvariable, 348, 350\u2013353\n\n "}, {"Page_number": 465, "text": "kernel density estimation, see density\nestimation, kernel, 327, 375\nkernel smoother, 374, 378\nknot\n\nlogspline, 341\n\nlaguerre polynomials, 144\nlangevin metropolis\u2013hastings, 262\u2013263\nlatent data, 97\nleast squares cross-validation, 333\nleave-one-out, see cross-validation, 369\nlegendre polynomials, 144, 354, 357\nlikelihood function, 9\nprofile, 10\u201311, 59, 63\nline search methods, 40\nlinear regression, see regression, 64\nlink function, 36, 397\nlinkage, 62\nlipschitz condition, 33\n\u201clittle oh\u201d notation, 2\nlocal averaging, 365\nlocal regression smoother, 373\u2013376\nlocal scoring, 398\nlocal search, 65\u201368\nrandom starts, 66\ntabu algorithm, 85\u201391\nvariable-depth, 66\n\nlocal regression smoother, 374\n\nlocally dependent markov random field, 269\nlocally weighted regression smoother, see\nlocus, 61\nloess smoother, 379\u2013381\nlogistic growth model, 56\u201357\nlogistic regression, 36\u201338\nlognormal distribution\n\ndefinition, 7\nsimulation, 154\n\nlogspline density estimation, 341\u2013345\nlouis\u2019s method, 106\u2013108\nmacroinvertebrates, 405\u2013406, 408\u2013411, 418\nmajorization, 104\nmap distance, 62\nmaple software, 148\nmark\u2013recapture, 212\u2013214, 217, 228\u2013230\nmarkov chain, 14\u201317, 71\n\naperiodic, 16, 202\nconvergence, 201\ncoupling, 264\ndetailed balance, 16, 251\n\nindex\n\n463\n\nergodic, 16, 265\nirreducible, 16, 202\nnonnull state, 16\nrecurrent state, 16\nreversible, 16, 251\nstate space, 14\nstates, 14\nstationary distribution, 16, 202\u2013203\ntime-homogeneous, 15\nmarkov chain monte carlo, 201\u2013230,\n237\u2013279, 325\nacceptance rate, 222, 238, 240, 245\nadaptive, 237\u2013249\nadaptive metropolis algorithm, 247\u2013249\nauxiliary variable methods, 251, 256\u2013260,\nbatch method, 227\nbayesian estimation, 204, 212\u2013214, 217,\nburn-in, 220\u2013222\nconvergence, 201, 218\u2013224\ncoupling from the past, 264\u2013268, 277\u2013279\ncusum diagnostic, 219\ndiagnostics, see burn-in, convergence,\n\n274\u2013277\n\n228\u2013230\n\nmixing, number of chains, run\nlength, 218\n\neffective sample size, 224\u2013225\nfor markov random fields, 269\u2013279\ngelman\u2013rubin statistic, 221\u2013222\ngibbs sampling, 209\u2013218, 270\u2013274\nhierarchical centering, 223, 233\u2013235\nhit-and-run algorithm, 260\u2013261\nhybrid strategies, 216\u2013217, 242\nimage analysis, 269\u2013279\nindependence chain, 204\u2013206\nlangevin metropolis\u2013hastings, 262\u2013263\nmaximum likelihood, 268\u2013269\nmetropolis\u2013hastings, 71, 202\u2013209\nmetropolis\u2013hastings ratio, 202, 204, 261\nmetropolis-within-gibbs, 217, 238, 240\nmixing, 205, 219\u2013224, 259, 276\u2013277\nmonte carlo standard error, 227\nmultiple-try metropolis\u2013hastings,\nnumber of chains, 225\u2013226\nperfect sampling, 260, 264\u2013268, 277\u2013279\nproposal distribution, 202\u2013203, 218,\nrandom walk chain, 206\u2013209, 228\nreparameterization, 207\u2013208, 223\u2013224\n\n261\u2013262\n\n222\u2013223\n\n "}, {"Page_number": 466, "text": "464\n\nindex\n\nmarkov chain monte carlo (continued)\n\nreversible jump, 250\u2013256\nrun length, 226\nsample path, 205, 207, 219\nsimulated tempering, 257\u2013258\nslice sampling, 258\u2013260\nsoftware, 226\nstructured markov chain monte carlo,\nswendsen\u2013wang algorithm, 274\u2013277\ntarget distribution, 201, 257\nmarkov process, 169\u2013170\nmarkov random field, 269\u2013279\n\n216\n\nauxiliary variable methods, 274\u2013277\ngibbs sampling, 270\u2013274\nlocally dependent, 269\nperfect sampling, 277\u2013279\n\nmars, 402\nmartian atmosphere, 390\nmathematica software, 148\nmatlab language, xvi, 17\nmaximal smoothing principle, 338\u2013339, 348\nmaximum likelihood, 9\u201310, 21\u201322, 25, 27,\n\n33, 36, 64, 97, 106, 114, 133,\n268\u2013269, 342\nalgorithm, 111\n\nmcem algorithm, see monte carlo em\nmcmc, see markov chain monte carlo, 201\nmean integrated squared error, 326\u2013327,\nmean squared error, 183, 327\n\n330\u2013332\n\nmetropolis\u2013hastings algorithm, 71,\n\nof estimation, 363\nof prediction, 364\n202\u2013209, 257\nacceptance rate, 222\ndiagnostics, see markov chain monte\nmultiple-try, 261\u2013262\nrelationship to gibbs sampling, 215\nreparameterization, 207\u2013208, 223\u2013224\nmetropolis\u2013hastings ratio, 202, 204, 252,\n\ncarlo, diagnostics, 218\n\n257, 261\ngeneralized, 262\nminorization, 104\nmise, see mean integrated squared error,\nmissing data, 97\u201398\nmissing information principle, 106, 108\nmixing, 205, 219\u2013224, 259, 276\u2013277\n\n326\n\nmixture distribution, 205\u2013209, 220\nmle, 9\nmodified newton method, 40\nmonte carlo em algorithm, 111\u2013112\nmonte carlo integration, 107, 151\u2013152,\n\n180\u2013195, 203, 226\u2013227\nantithetic sampling, 186\u2013189\ncontrol variates, 189\u2013193\nimportance sampling, 180\u2013186\nmarkov chain monte carlo, see markov\n\nchain monte carlo, 201, 203,\n226\u2013227\n\nrao\u2013blackwellization, 193\u2013195\nriemann sum improvement, 195\u2013196\nvariance reduction, 180\u2013195\nmonte carlo maximum likelihood, 268\nmoving average, 366\nmoving block bootstrap, 306\u2013307\nmse, see mean squared error, 327\nmspe, see mean squared error, of\nprediction, 364\nmultinomial coefficient, 8\nmultinomial distribution\n\ndefinition, 5\nsimulation, 154\n\nmultiple integrals, 147\nmultiple-try metropolis\u2013hastings algorithm,\nmultivariate adaptive regression splines, 402\nmultivariate normal distribution\n\n261\u2013262\n\ndefinition, 7\nsimulation, 154\n\nnadaraya\u2013watson estimator, 375\u2013376\nnatural selection, 75\nnavigation, see terrain navigation, 176\nnearest neighbor density estimation,\nnegative binomial distribution\n\n349\u2013350\ndefinition, 5\nsimulation, 154\nneighborhood\nfor local search, 65\nfor nelder\u2013mead method, 45\nfor simulated annealing, 70, 74\nfor smoothing, 365\nfor tabu algorithm, 86\nk-change, 65\n\nnelder\u2013mead algorithm, 45\u201352\nnested bootstrap, 292, 299\u2013301\n\n "}, {"Page_number": 467, "text": "network failure, 184\u2013186, 189\nneural network, 402\u2013403\nnewton\u2019s method, 26\u201329, 34\u201337, 39, 118\n\nconvergence, 27\u201330\ndiscrete, 41\nmodified, 40\n\nnewton\u2013c\u02c6otes quadrature, 129\u2013139\nnewton-like methods, 39\u201344\nnode\n\nbacktracking, 39\u201340, 42\nfor numerical integration, 129\nfor tree-based methods, 406\n\noptimization, 21\n\n287\ndensity estimation, 325\n363\n\nnonmoving block bootstrap, 304\u2013306\nnonlinear equations, solving, see\nnonlinear least squares, 44\nnonnull state, 16\nnonparametric bootstrap, see bootstrap,\nnonparametric density estimation, see\nnonparametric regression, see smoothing,\nnormal distribution\ndefinition, 7\nsimulation, 154\nnormal kernel, 339\nnormalizing constant, 11, 156, 167, 204\nnorwegian paper, 396\u2013397, 401\nnotation, 1, 4\nnp problem, 61\nnp-complete problem, 61\nnp-hard problem, 61\nnumerical differentiation, 3, 110\u2013111\nnumerical integration, 129\u2013148, 152, 298\n\napproximation error, 144\n\nnth-degree rule, 138\u2013139\nadaptive quadrature, 147\ncomposite rule, 129\ngauss\u2013hermite quadrature, 145\ngauss\u2013legendre quadrature, 145, 149\ngaussian quadrature, 142\u2013146\nmethod of undetermined coefficients,\nmultiple integrals, 147\nnewton\u2013c\u02c6otes quadrature, 129\u2013139\nnode, 129\nover infinite range, 146\nproduct formulas, 147\nriemann rule, 130\u2013134\n\n138\u2013139\n\nindex\n\n465\n\napproximation error, 140\n\nromberg integration, 139\u2013142\nsimple rule, 129\nsimpson\u2019s rule, 136\u2013138\n\nadjoining subintervals, 137\napproximation error, 138\n\nsingularities, 146\nsoftware, 148\ntransformations, 146\ntrapezoidal rule, 134\u2013136\napproximation error, 136\n\no() notation, 2\no() notation, 2\nobserved fisher information, 10\noil spills, 56\noptimization, 21\u201354\n\nabsolute convergence criterion, 24, 34\nascent algorithm, 39\u201340\nbackfitting, 52\u201353, 394\nbacktracking, 39\u201340, 42\nbfgs, 42\nbisection, 23\u201325, 30\nbracketing methods, 26\ncombinatorial, see combinatorial\noptimization, 59\nconvergence criteria, 24\u201325, 34, 49\nderivative-free, 45\ndiscrete newton methods, 41\nem algorithm, see em algorithm, 97\nfisher scoring, 30, 34\u201339\nfixed-point iteration, 32\u201333, 41\ngauss\u2013newton, 38, 44\u201345\ngauss\u2013seidel iteration, 52\u201353, 114, 394\niterated conditional modes, 114\niteratively reweighted least squares,\n36\u201338, 397\nmajorization, 104\nminorization, 104\nmultivariate, 34\u201354\nnelder\u2013mead algorithm, 45\u201352\nnewton\u2019s method, 26\u201329, 34\u201337, 39, 118\nnewton-like methods, 39\u201344\norder of convergence, 29\u201330\nquasi-newton, 41\u201344, 119\nrelative convergence criterion, 25, 34\nscaled fixed-point iteration, 33\nsecant method, 30\u201332\nstarting value, 22, 26\nsteepest ascent, 39\n\n "}, {"Page_number": 468, "text": "466\n\nindex\n\noptimization (continued)\n\nstopping rule, 24\u201326\nunivariate, 22\u201333\n\noptimization transfer, 104\noption pricing, 191\u2013193, 197\u2013198\norder crossover, 82\norder of convergence, 2\northogonal polynomials, 143\northonormal polynomials, 143\npaper manufacture, 396\u2013397, 401\nparallel chords, method of, see fixed-point\niteration, 33\nparametric bootstrap, see bootstrap,\nparametric, 289\nparent node, 406\nparticle filter, 179\u2013180\nparticle swarm, 85\npath sampling, 167\u2013168\npeppered moths, 99\u2013101, 105\u2013106,\n109\u2013110, 117\u2013118, 120\u2013121\npercentile method, 292\u2013294, 385\nperfect sampling, 260, 264\u2013268, 277\u2013279\n\nbootstrap, 302\n\nfor markov random fields, 277\u2013279\nsandwiching, 267\u2013268, 277\u2013279\npermutation bootstrap, see balanced\npermutation test, 317\u2013319\npigment moisture content, 234\u2013235\npivotal quantity, 293\u2013301, 387\nplug-in methods, 335\u2013337, 353\npointwise confidence band, 384\npoisson distribution\ndefinition, 5\nsimulation, 154\n\npolynomial algorithm, 60\npolynomials\n\nfundamental, 134\nhermite, 144\ninterpolating, 134\njacobi, 144\nlaguerre, 144\nlegendre, 144, 354, 357\northogonal, 143\northonormal, 143\ncarrying capacity, 240\ndensity dependence, 241\n\npopulation dynamics, 240\u2013247\n\npopulation modeling, 56\u201357, 240\u2013247, 289,\n\n319\u2013320\n\npositive definite matrix, 1\npositive semidefinite matrix, 1\npositivity, 269\nposterior distribution, 11\npredictor, 363\npredictor\u2013response data, 363\npressure of air blast, 390\u2013391\nprincipal curves smoother, 389, 413\u2013416\n\nprojection index, 414\nsoftware, 413\nspan selection, 416\nprincipal surfaces smoother, 415\nprior distribution, 11\n\nconjugate, 12\nimproper, 12\njeffreys, 12\n\nprobability integral transform, 153\nproblem complexity, 59\u201361\nproduct formulas, 147\nproduct kernel, 345, 346, 375\nprofile likelihood, 10\u201311, 59, 63\nprojection index, 414\nprojection pursuit density estimation, see\nexploratory projection pursuit, 353\nprojection pursuit regression, 399\u2013402\nproposal distribution\n218, 222\u2013223\n\nfor markov chain monte carlo, 202\u2013203,\nfor simulated annealing, 69, 70, 74\n\npruning, 409\u2013411\npseudo-data, 287\npseudo-likelihood, 333, 334\nquadrature, see numerical integration,\nquasi-newton acceleration\nquasi-newton methods, 41\u201344, 119\n\nfor em algorithm, 119\u2013121\nbfgs, 42\n\n129\n\nr language, xvi, 17, 226, 344\nrandom ascent, 66\nrandom starts local search, 66\nrandom walk chain, 206\u2013209\nrandomization test, 318\nrao\u2013blackwellization, 193\u2013195, 227\nimprovement of rejection sampling,\n\n194\u2013195\nrecombination, 62\nrecurrent state, 16\n\n "}, {"Page_number": 469, "text": "recursive partitioning regression, see\nreflection\nregression\n\ntree-based methods, 405\nfor nelder\u2013mead method, 46\nbootstrapping, 290\u2013291\n\ncross-validated, 370\n\nmethods, 405\n87, 91, 253\u2013256\n\ncases, 291\npaired, 291\nresiduals, 290\nlogistic, 36\u201338\nrecursive partitioning, see tree-based\nvariable selection, 64, 67\u201368, 73\u201374, 78,\nwith missing data, 114\u2013116\nrejection sampling, 155\u2013162\nadaptive, 159\u2013162\nenvelope, 155\u2013157\nrao\u2013blackwellization improvement,\nsqueezed, 158\u2013159\n\n194\u2013195\nrejuvenation, 173\nrelative convergence criterion, 25, 34\nresidual sum of squares, 369\nresponse, 363\nreversible jump methods, 250\u2013256\nreversible markov chain, 16, 251\nrichardson extrapolation, 142\nriemann rule, 130\u2013134\nrjmcmc, see markov chain monte carlo,\nromberg integration, 139\u2013142\nroot node, 406\nroughness, 330\nrss, see residual sum of squares, 370\nrunning-line smoother, 372\u2013374\nrunning-polynomial smoother, 373\ns-plus language, 68\nsalmon population, 319\u2013320\nsample path, 205, 207, 219\nsample point adaptive estimator, 350\nsampling importance resampling, 163\u2013167\n\nreversible jump, 250\napproximation error, 140\n\nadaptive, 167\nchoice of envelope, 165\ncompared with importance sampling, 183\nstandardized importance weights, 164\nscatterplot smoothing, see smoothing, 363\n\nindex\n\n467\n\nscore equation, 21\nscore function, 10\nsecant condition, 41, 119\nsecant method, 30\u201332\nconvergence, 31\u201332\nself-avoiding walk, 198\nself-consistency, 414\nsem algorithm, 106, 108\u2013110\nsensitivity analysis, 184, 233\nsequential importance sampling, 169\u2013179\n\ndegeneracy, 171\u2013173\neffective sample size, 171\u2013173\nenvelope, 171\nfor markov processes, 169\u2013170\nrejuvenation, 173\nwith resampling, 173, 175, 179\nsequential monte carlo, 168\u2013180\nsexual histories, 121\u2013123\nsheather\u2013jones method, 336\u2013337\nshrink transformation\nfor nelder\u2013mead method, 47\nsieve block bootstrap, 316\nsilverman\u2019s rule of thumb, 335\u2013336, 347,\nsimple rule for integration, 129\nsimplex\nfor nelder\u2013mead method, 45\nsimpson\u2019s rule, 136\u2013138\nadjoining subintervals, 137\napproximation error, 138\nromberg improvement of, 142\nsimulated annealing, 68\u201375, 258\nas a markov chain, 71\nconstrained solution space, 74\nconvergence, 71\u201372\ncooling schedule, 71\u201372, 75\nneighborhoods, 70, 74\nproposal distribution, 69, 70, 74\ntemperature, 69\n\n353\n\nsimulated tempering, 257\u2013258\nsimulation, 151\u2013180\n\nadaptive importance sampling, 167\nadaptive rejection sampling, 159\u2013162\napproximate, 163\u2013180\nbridge sampling, 167\u2013168\nexact, 152\u2013162\nimportance sampling, 163\u2013168\ninverse cumulative distribution function,\npath sampling, 167\u2013168\n\n153\u2013155\n\n "}, {"Page_number": 470, "text": "468\n\nindex\n\nsimulation (continued)\n\n163\u2013167\n\nrejection sampling, 155\u2013162\nsampling importance resampling,\nsequential monte carlo, 168\u2013180\nsqueezed rejection sampling, 158\u2013159\nstandard distributions, 153\nstandard parametric distributions, 154\ntarget distribution, 152\nuniform distribution, 153\n\n163\n\n404\u2013405\n403\u2013404\n\nsir, see sampling importance resampling,\nslash distribution, 165\u2013166\nslice sampling, 258\u2013260\nsmooth function model, 310\nsmoothing, 298, 363\u2013416\nadditive model, 394\u2013397\nadditivity and variance stabilization,\nalternating conditional expectations,\nbandwidth, 365, 374\nconfidence bands, 384\u2013388\nconstant-span running mean, 366\u2013372\ncross-validation, 369\u2013372, 377\nequivalent degrees of freedom, 378\nequivalent kernels, 377\u2013379\nexpanding confidence bands, 386\ngeneralized additive model, 397\u2013399\nlocal scoring, 398\ngeneralized cross-validation, 372, 377\nkernel, 374, 376, 378\nlinear, 365\u2013379\nlocal averaging, 365\nlocal regression, 373\u2013376\nlocally weighted regression, see\nloess, 379\u2013381\nmatrix, 366, 373\nmean squared estimation error, 363\nmean squared prediction error, 364\nnadaraya\u2013watson estimator, 375\u2013376\nneighborhood, 365\nnonlinear, 379\u2013384\nprincipal curves, 389, 413\u2013416\n\nsmoothing, local regression, 374\n\nprojection index, 414\nspan selection, 416\nprincipal surfaces, 415\nprojection pursuit regression, 399\u2013402\nrunning lines, 372\u2013374\n\nsoftware, 17\n\nrunning polynomial, 373\nspan, 365, 368\u2013372, 377\nsplines, 376\u2013377\nsupersmoother, 381\u2013384\nvariable-span, 381\u2013384\ndensity estimation, 344\nfor markov chain monte carlo, 162,\nnumerical integration, 148\nprincipal curves, 413\ntree-based methods, 405\nvariable selection, 68\n\n226\n\nspan, see smoothing, span, 365\nsphering, 347, 353\nspline smoother, 376\u2013377\nsplit coordinate, 407\nsplit point, 407\nsquare-integrable, 143\nsqueezed rejection sampling, 158\u2013159\nsqueezing function, 158\nstandardized importance weights, 164\nstate space, 14\nstates, 14\nstationary block bootstrap, 311\nstationary distribution, 16, 202\u2013203\nsteady-state genetic algorithm, 81\nsteepest ascent, 39, 66\nstep length\n\nsteepest ascent/mildest descent, 66\ndefinition, 39\nfor backtracking, 39\u201340, 42\n\n418\n\nstochastic monotonicity, 267\u2013268, 277\u2013279\nstomach cancer, 320\u2013321\nstream ecology, 210\u2013212\nstream monitoring, 405\u2013406, 408\u2013411,\nstrong law of large numbers, 13, 152\nstructure index, 355\nstructured markov chain monte carlo, 216\nstudent\u2019s t distribution\n\ndefinition, 7\nsimulation, 154\nstudentized bootstrap, see bootstrap, t, 296\nsubinterval, 129\nsubtree, 407\nsufficient descent, 52\nsupersmoother, 381\u2013384\nsupplemented em algorithm, see sem\n\nalgorithm, 108\n\n "}, {"Page_number": 471, "text": "decoupling, 276\n\nsurvival analysis, 54\u201356, 123\u2013124\nswendsen\u2013wang algorithm, 274\u2013277\nsymmetric nearest neighborhood, 366\nt distribution, see student\u2019s t distribution,\ntabu algorithm, 85\u201391\n\n154\n\naspiration by influence, 89\naspiration criteria, 88\u201389\ndiversification, 89\u201390\nfrequency, 89\nintensification, 90\nmove attributes, 86, 88\nrecency, 87\ntabu list, 87\u201388\ntabu tenure, 88\n\ntapered block bootstrap, 316\ntarget distribution, 152, 201, 257\ntaylor series, 2\u20133\n\ndelta method, 297\u2013298\nfor gauss\u2013newton method, 44\nfor newton\u2019s method, 26, 28, 34\nfor simpson\u2019s rule, 138\nfor trapezoidal rule, 136\ntaylor\u2019s theorem, 2\u20133\nterrain navigation, 176\u2013180\ntime\u2013speed parameterization of curves, 413\ntime-homogeneous markov chain, 15\ntournament selection, 81\ntracking, see terrain navigation, 169\ntransformation of random variables, 8\u20139\ntrapezoidal rule, 134\u2013136\napproximation error, 136\nromberg improvement of, 139\n\ntraveling salesman problem, 59, 64, 70, 79,\ntree rings, 308\u2013309\ntree-based methods, 405\u2013413\n\n82\u201384\n\nclassification, 411\u2013412\nmodel uncertainty, 412\nnode\n\ninternal, 406\nparent, 406\nroot, 406\npruning, 409\u2013411\nsoftware, 405\nsplit, 406\nsplit coordinate, 407\n\nindex\n\n469\n\nsplit point, 407\nsubtree, 407\ntree, 405\n\ntriangle kernel, 339\ntricube kernel, 379\ntriweight kernel, 339\nucv, see unbiased cross-validation, 333\nunbiased cross-validation, 333\u2013334,\nunbiased estimator, 14\nundetermined coefficients, method of,\nuniform distribution\n\n360\u2013361\n\n138\u2013139\ndefinition, 7\nsimulation, 153, 154\n\nadaptation, 238\n\nuniform kernel, 339\nunit-speed parameterization, 413\nutah serviceberry, 271\u2013273, 276, 278\u2013279\nvanishing adaptation, see diminishing\nvariable kernel, 348\nvariable selection, 64, 67\u201368, 73\u201374, 78, 87,\n91, 253\u2013256\nvariable-depth local search, 66\nvariable-kernel density estimator, 350\u2013353\nvariable-metric method, 42\nvariable-span smoother, 381\u2013384\nvariance reduction, 180\u2013195\nantithetic sampling, 186\u2013189\ncontrol variates, 189\u2013193\nfor bootstrap, 302\u2013303\nimportance sampling, 180\u2013186\nrao\u2013blackwellization, 193\u2013195\nriemann sum improvement, 195\u2013196\nvariance-stabilizing transformation, 293,\n\n298\u2013299\n\ndefinition, 7\n\nweak law of large numbers, 13\nwebsite for this book, xvi\nweibull distribution\nweighted likelihood bootstrap, 317\nwhale migration, 334\u2013335, 337, 339, 344\nwhale population dynamics, 240\u2013247\nwhitening, 347, 353\nwinbugs software, 162\nwine chemistry, 95\n\n "}, {"Page_number": 472, "text": "wiley series in computational statistics\n\nbelsley and kontoghiorghes \u2022 handbook of computational econometrics\nbiegler, biros, ghattas, heinkenschloss, keyes, mallick, tenorio,\nvan bloemen waanders, willcox, and marzouk \u2022 large-scale inverse\nproblems and quantification of uncertainty\nbillard and diday \u2022 symbolic data analysis: conceptual statistics and data mining\nbolstad \u2022 understanding computational bayesian statistics\nborgelt, steinbrecher, and kruse \u2022 graphical models: representations for\nlearning, reasoning and data mining, second edition\ndehmer and basak \u2022 statistical and machine learning approaches for network analysis\ndunne \u2022 a statistical approach to neural networks for pattern recognition\nliang, liu, and carroll \u2022 advanced markov chain monte carlo methods: learning\nfrom past samples\nntzoufras \u2022 bayesian modeling using winbugs\ntuffery \u2022 data mining and statistics for decision making\n\ncomputational statistics, second edition. geof h. givens and jennifer a. hoeting.\n\u00a9 2013 john wiley & sons, inc. published 2013 by john wiley & sons, inc.\n\n "}]}
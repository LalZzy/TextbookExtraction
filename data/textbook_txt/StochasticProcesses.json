{"Filename": "StochasticProcesses", "Pages": [{"Page_number": 1, "text": " "}, {"Page_number": 2, "text": "this page intentionally left blank\n\n "}, {"Page_number": 3, "text": "stochastic processes\n\nthis comprehensive guide to stochastic processes gives a complete overview of the\ntheory and addresses the most important applications. pitched at a level accessible\nto beginning graduate students and researchers from applied disciplines, it is both\na course book and a rich resource for individual readers. subjects covered include\nbrownian motion, stochastic calculus, stochastic differential equations, markov pro-\ncesses, weak convergence of processes, and semigroup theory. applications include\nthe black\u2013scholes formula for the pricing of derivatives in financial mathematics, the\nkalman\u2013bucy filter used in the us space program, and also theoretical applications\nto partial differential equations and analysis. short, readable chapters aim for clarity\nrather than for full generality. more than 350 exercises are included to help readers put\ntheir new-found knowledge to the test and to prepare them for tackling the research\nliterature.\n\nrichard f. bass is board of trustees distinguished professor in the department\nof mathematics at the university of connecticut.\n\n "}, {"Page_number": 4, "text": "c a m b r i d g e s e r i e s i n s t a t i s t i c a l a n d\n\np r o b a b i l i s t i c m a t h e m a t i c s\n\neditorial board\n\nz. ghahramani (department of engineering, university of cambridge)\n\nr. gill (mathematical insitute, leiden university)\n\nf. p. kelly (department of pure mathematics and mathematical statistics,\n\nuniversity of cambridge)\n\nb. d. ripley (department of statistics, university of oxford)\ns. ross (department of industrial and systems engineering,\n\nuniversity of southern california)\n\nm. stein (department of statistics, university of chicago)\n\nthis series of high-quality upper-division textbooks and expository monographs covers all\naspects of stochastic applicable mathematics. the topics range from pure and applied statistics\nto probability theory, operations research, optimization, and mathematical programming. the\nbooks contain clear presentations of new developments in the field and also of the state of\nthe art in classical methods. while emphasizing rigorous treatment of theoretical methods, the\nbooks also contain applications and discussions of new techniques made possible by advances\nin computational practice.\n\na complete list of books in the series can be found at http://www.cambridge.org/statistics.\nrecent titles include the following:\n\n11. statistical models, by a. c. davison\n12. semiparametric regression, by david ruppert, m. p. wand and r. j. carroll\n13. exercises in probability, by lo\u00a8\u0131c chaumont and marc yor\n14. statistical analysis of stochastic processes in time, by j. k. lindsey\n15. measure theory and filtering, by lakhdar aggoun and robert elliott\n16. essentials of statistical inference, by g. a. young and r. l. smith\n17. elements of distribution theory, by thomas a. severini\n18. statistical mechanics of disordered systems, by anton bovier\n19. the coordinate-free approach to linear models, by michael j. wichura\n20. random graph dynamics, by rick durrett\n21. networks, by peter whittle\n22. saddlepoint approximations with applications, by ronald w. butler\n23. applied asymptotics, by a. r. brazzale, a. c. davison and n. reid\n24. random networks for communication, by massimo franceschetti and ronald meester\n25. design of comparative experiments, by r. a. bailey\n26. symmetry studies, by marlos a. g. viana\n27. model selection and model averaging, by gerda claeskens and nils lid hjort\n28. bayesian nonparametrics, edited by nils lid hjort et al.\n29. from finite sample to asymptotic methods in statistics, by pranab k. sen,\n\njulio m. singer and antonio c. pedrosa de lima\n\n30. brownian motion, by peter m\u00a8orters and yuval peres\n31. probability, by rick durrett\n33. stochastic processes, by richard f. bass\n34. structured regression for categorical data, by gerhard tutz\n\n "}, {"Page_number": 5, "text": "stochastic processes\n\nrichard f. bass\nuniversity of connecticut\n\n "}, {"Page_number": 6, "text": "c a m b r i d g e u n i v e r s i t y p r e s s\n\ncambridge, new york, melbourne, madrid, cape town,\n\nsingapore, s\u02dcao paulo, delhi, tokyo, mexico city\n\ncambridge university press\n\nthe edinburgh building, cambridge cb2 8ru, uk\n\npublished in the united states of america by cambridge university press, new york\n\ninformation on this title: www.cambridge.org/9781107008007\n\nwww.cambridge.org\n\nc(cid:2) r. f. bass 2011\n\nthis publication is in copyright. subject to statutory exception\nand to the provisions of relevant collective licensing agreements,\nno reproduction of any part may take place without the written\n\npermission of cambridge university press.\n\nfirst published 2011\n\nprinted in the united kingdom at the university press, cambridge\n\na catalogue record for this publication is available from the british library\n\nlibrary of congress cataloguing in publication data\n\nbass, richard f.\n\nstochastic processes / richard f. bass.\n\np.\n\ncm. \u2013 (cambridge series in statistical and probabilistic mathematics ; 33)\n\nincludes index.\n\nisbn 978-1-107-00800-7 (hardback)\n\n1. stochastic analysis.\n\ni. title.\n\nqa274.2.b375\n(cid:3)\n32 \u2013 dc23\n519.2\n\n2011\n\n2011023024\n\nisbn 978-1-107-00800-7 hardback\n\ncambridge university press has no responsibility for the persistence or\naccuracy of urls for external or third-party internet websites referred to\n\nin this publication, and does not guarantee that any content on such\n\nwebsites is, or will remain, accurate or appropriate.\n\n "}, {"Page_number": 7, "text": "to meredith, as always\n\n "}, {"Page_number": 8, "text": " "}, {"Page_number": 9, "text": "contents\n\npreface\nfrequently used notation\n\n1\n1.1\n1.2\n\n2\n2.1\n\n3\n3.1\n3.2\n3.3\n3.4\n3.5\n3.6\n\nbasic notions\nprocesses and \u03c3 -fields\nlaws and state spaces\n\nbrownian motion\ndefinition and basic properties\n\nmartingales\ndefinition and examples\ndoob\u2019s inequalities\nstopping times\nthe optional stopping theorem\nconvergence and regularity\nsome applications of martingales\n\nmarkov properties of brownian motion\n\n4\n4.1 markov properties\n4.2\n\napplications\n\n5\n\nthe poisson process\n\nconstruction of brownian motion\n\n6\n6.1 wiener\u2019s construction\n6.2 martingale methods\n\n7\n\n8\n\npath properties of brownian motion\n\nthe continuity of paths\n\nvii\n\npage xiii\nxv\n\n1\n1\n3\n\n6\n6\n\n13\n13\n14\n15\n17\n17\n20\n\n25\n25\n27\n\n32\n\n36\n36\n39\n\n43\n\n49\n\n "}, {"Page_number": 10, "text": "viii\n\ncontents\n\n9\n9.1\n9.2\n9.3\n9.4\n\ncontinuous semimartingales\ndefinitions\nsquare integrable martingales\nquadratic variation\nthe doob\u2013meyer decomposition\n\nstochastic integrals\n\n10\n10.1 construction\n10.2\n\nextensions\n\n11\n\nit\u02c6o\u2019s formula\n\nsome applications of it\u02c6o\u2019s formula\nl\u00b4evy\u2019s theorem\ntime changes of martingales\n\n12\n12.1\n12.2\n12.3 quadratic variation\n12.4 martingale representation\n12.5\n12.6\n\nthe burkholder\u2013davis\u2013gundy inequalities\nstratonovich integrals\n\nthe girsanov theorem\nthe brownian motion case\n\n13\n13.1\n13.2 an example\n\nlocal times\n\n14\n14.1 basic properties\n14.2\n14.3 occupation times\n\njoint continuity of local times\n\nskorokhod embedding\npreliminaries\n\n15\n15.1\n15.2 construction of the embedding\n15.3\n\nembedding random walks\n\nthe general theory of processes\npredictable and optional processes\n\nthe debut and section theorems\nprojection theorems\n\n16\n16.1\n16.2 hitting times\n16.3\n16.4\n16.5 more on predictability\n16.6 dual projection theorems\n16.7\n16.8\n\nthe doob\u2013meyer decomposition\ntwo inequalities\n\n54\n54\n55\n57\n58\n\n64\n64\n69\n\n71\n\n77\n77\n78\n79\n79\n82\n84\n\n89\n89\n92\n\n94\n94\n96\n97\n\n100\n100\n105\n108\n\n111\n111\n115\n117\n119\n120\n122\n124\n126\n\n "}, {"Page_number": 11, "text": "contents\n\nix\n\nprocesses with jumps\n\n17\n17.1 decomposition of martingales\n17.2\n17.3\n17.4\n17.5\n17.6\n17.7\n\nstochastic integrals\nit\u02c6o\u2019s formula\nthe reduction theorem\nsemimartingales\nexponential of a semimartingale\nthe girsanov theorem\n\n18\n\npoisson point processes\n\nframework for markov processes\nintroduction\n\n19\n19.1\n19.2 definition of a markov process\n19.3\n19.4 an example\n19.5\n\ntransition probabilities\n\nthe canonical process and shift operators\n\n20\n20.1\n20.2\n20.3\n\nmarkov properties\nenlarging the filtration\nthe markov property\nstrong markov property\n\napplications of the markov properties\n\n21\n21.1 recurrence and transience\n21.2 additive functionals\n21.3 continuity\n21.4 harmonic functions\n\ntransformations of markov processes\n\n22\n22.1 killed processes\n22.2 conditioned processes\n22.3\n22.4\n\ntime change\nlast exit decompositions\n\n23\n23.1\n23.2\n\noptimal stopping\nexcessive functions\nsolving the optimal stopping problem\n\nstochastic differential equations\npathwise solutions of sdes\n\n24\n24.1\n24.2 one-dimensional sdes\n24.3\n\nexamples of sdes\n\n130\n130\n133\n135\n139\n141\n143\n144\n\n147\n\n152\n152\n153\n154\n156\n158\n\n160\n160\n162\n164\n\n167\n167\n169\n170\n171\n\n177\n177\n178\n180\n181\n\n184\n184\n187\n\n192\n192\n196\n198\n\n "}, {"Page_number": 12, "text": "x\n\ncontents\n\n25\n\n26\n\n27\n\nweak solutions of sdes\n\nthe ray\u2013knight theorems\n\nbrownian excursions\n\nfinancial mathematics\nfinance models\n\n28\n28.1\n28.2 black\u2013scholes formula\n28.3\n28.4\n\nthe fundamental theorem of finance\nstochastic control\n\nfiltering\nthe basic model\nthe innovation process\n\n29\n29.1\n29.2\n29.3 representation of f z-martingales\n29.4\n29.5\n29.6 kalman\u2013bucy filter\n\nthe filtering equation\nlinear models\n\nconvergence of probability measures\nthe portmanteau theorem\nthe prohorov theorem\n\n30\n30.1\n30.2\n30.3 metrics for weak convergence\n\n31\n\nskorokhod representation\n\nthe space c[0, 1]\ntightness\n\n32\n32.1\n32.2 a construction of brownian motion\n\ngaussian processes\n\n33\n33.1 reproducing kernel hilbert spaces\n33.2 continuous gaussian processes\n\n34\nthe space d[0, 1]\n34.1 metrics for d[0, 1]\n34.2 compactness and completeness\n34.3\n\nthe aldous criterion\n\napplications of weak convergence\n\n35\n35.1 donsker invariance principle\n35.2 brownian bridge\n35.3\n\nempirical processes\n\n204\n\n209\n\n214\n\n218\n218\n220\n223\n226\n\n229\n229\n230\n231\n232\n234\n234\n\n237\n237\n239\n241\n\n244\n\n247\n247\n248\n\n251\n251\n254\n\n259\n259\n262\n264\n\n269\n269\n273\n275\n\n "}, {"Page_number": 13, "text": "contents\n\nxi\n\nsemigroups\n\n36\n36.1 constructing the process\n36.2\n\nexamples\n\ninfinitesimal generators\nsemigroup properties\nthe hille\u2013yosida theorem\n\n37\n37.1\n37.2\n37.3 nondivergence form elliptic operators\n37.4 generators of l\u00b4evy processes\n\ndirichlet forms\nframework\n\n38\n38.1\n38.2 construction of the semigroup\n38.3 divergence form elliptic operators\n\nmarkov processes and sdes\n\n39\n39.1 markov properties\n39.2\n39.3 martingale problems\n\nsdes and pdes\n\nsolving partial differential equations\n40\n40.1\npoisson\u2019s equation\n40.2 dirichlet problem\n40.3 cauchy problem\n40.4\n\nschr\u00a8odinger operators\n\none-dimensional diffusions\n\n41\n41.1 regularity\n41.2\n41.3\n41.4\n41.5\n41.6\n\nscale functions\nspeed measures\nthe uniqueness theorem\ntime change\nexamples\n\nl\u00b4evy processes\nexamples\n\n42\n42.1\n42.2 construction of l\u00b4evy processes\n42.3 representation of l\u00b4evy processes\n\nappendices\na\na.1\na.2\na.3\na.4\n\nbasic probability\nfirst notions\nindependence\nconvergence\nuniform integrability\n\n279\n279\n283\n\n286\n286\n292\n296\n297\n\n302\n303\n304\n307\n\n312\n312\n314\n315\n\n319\n319\n320\n321\n323\n\n326\n326\n327\n329\n333\n334\n336\n\n339\n339\n340\n344\n\n348\n348\n353\n355\n356\n\n "}, {"Page_number": 14, "text": "xii\n\ncontents\n\noptional stopping\ndoob\u2019s inequalities\n\nconditional expectation\nstopping times\n\na.5\na.6\na.7 martingales\na.8\na.9\na.10 martingale convergence theorem\na.11 strong law of large numbers\na.12 weak convergence\na.13 characteristic functions\na.14 uniqueness and characteristic functions\na.15 the central limit theorem\na.16 gaussian random variables\n\nb\nb.1\nb.2\n\nc\n\nd\n\nsome results from analysis\nthe monotone class theorem\nthe schwartz class\n\nregular conditional probabilities\n\nkolmogorov extension theorem\n\nreferences\nindex\n\n357\n359\n359\n360\n361\n362\n364\n367\n370\n372\n372\n374\n\n378\n378\n379\n\n380\n\n382\n\n385\n387\n\n "}, {"Page_number": 15, "text": "preface\n\nwhy study stochastic processes? this branch of probability theory offers sophisticated theo-\nrems and proofs, such as the existence of brownian motion, the doob\u2013meyer decomposition,\nand the kolmogorov continuity criterion. at the same time stochastic processes also have\nfar-reaching applications: the explosive growth in options and derivatives in financial mar-\nkets throughout the world derives from the black\u2013scholes formula, while nasa relies on\nthe kalman\u2013bucy method to filter signals from satellites and probes sent into outer space.\n\na graduate student taking a year-long course in probability theory first learns about\nsequences of random variables and topics such as laws of large numbers, central limit\ntheorems, and discrete time martingales. in the second half of the course, the student will\nthen turn to stochastic processes, which is the subject of this text. topics covered here are\nbrownian motion, stochastic integrals, stochastic differential equations, markov processes,\nthe black\u2013scholes formula of financial mathematics, the kalman\u2013bucy filter, as well as\nmany more.\n\nthe 42 chapters of this book can be grouped into seven parts. the first part consists\nof chapters 1\u20138, where some of the basic processes and ideas are introduced, including\nbrownian motion. the next group of chapters, chapters 9\u201315, introduce the theory of\nstochastic calculus, including stochastic integrals and it\u02c6o\u2019s formula. chapters 16\u201318 explore\njump processes. this requires a study of the foundations of stochastic processes, which\nis also known as the general theory of processes. next we take up markov processes in\nchapters 19\u201323. a formidable obstacle to the study of markov processes is the notation, and\ni have attempted to make this as accessible as possible. chapters 24\u201329 involve stochastic\ndifferential equations. two very important applications, to financial mathematics and to\nfiltering, appear in chapters 28 and 29, respectively. probability measures on metric spaces\nand the weak convergence of random variables taking values in a metric space prove to\nbe relevant to the study of stochastic processes. these and related topics are treated in\nchapters 30\u201335. we then return to markov processes, namely, their construction and some\nimportant examples, in chapters 36\u201342. tools used in the construction include infinitesimal\ngenerators, dirichlet forms, and solutions to stochastic differential equations, while two\nimportant examples that we consider are diffusions on the real line and l\u00b4evy processes.\n\nthe prerequisites to this book are a sound knowledge of basic measure theory and a\ncourse in the classical aspects of probability. the probability topics needed are provided\n(with proofs) in an appendix.\n\nthere is far too much material in this book to cover in a single semester, and even too\nmuch for a full year. i recommend that as a minimum the following chapters be studied:\nchapters 1\u20135, chapters 9\u201313, chapters 19\u201321, and chapter 24. if possible, include either\n\nxiii\n\n "}, {"Page_number": 16, "text": "xiv\n\npreface\n\nchapter 28 or chapter 29. in chapter 11, the statement and corollaries of it\u02c6o\u2019s formula are\nvery important, but the proof of it\u02c6o\u2019s formula may be omitted.\n\ni would like to thank the many students who patiently sat through my lectures, pointed out\nerrors, and made suggestions. i especially would like to thank my colleague sasha teplyaev\nwho taught a course from a preliminary version of this book and made a great number of\nuseful suggestions.\n\n "}, {"Page_number": 17, "text": "frequently used notation\n\nhere are some notational conventions we will use. we use the letter c, either with or without\nsubscripts, to denote a finite positive constant whose exact value is unimportant and which\nmay change from line to line. we use b(x, r) to denote the open euclidean ball centered at\nx with radius r. a \u2227 b is the minimum of a and b, while a \u2228 b is the maximum of a and b.\n+ = x \u2228 0 and x\n\u2212 = (\u2212x) \u2228 0. the symbol \u2203 is used in a few formulas and means \u201cthere\nx\nexists.\u201d q, q+, n, and z denote the rationals, the positive rationals, the natural numbers,\nand the integers, respectively. if c is a matrix, ct is the transpose of c.\n\nfor a set a, we use ac for the complement of a. if a is a subset of a topological space, a,\na0, and \u2202a denote the closure, interior, and boundary of a, respectively.\ngiven a topological space s, we use c(s ) for the space of continuous functions on s,\nwhere we use the supremum norm. if s is a domain in rd, ck (s ) refers to the set of\ncontinuous functions with domain s whose partial derivatives up to order k are continuous.\n\u221e\nc\n\nfunctions are those that are infinitely differentiable.\n\nwe will on a few occasions use the fourier transform, which we define by\n\n(cid:3)\n\n(cid:2)f (u) =\n\neiu\u00b7x f (x) dx\n\nfor f integrable. this agrees with the convention in rudin (1987).\nxt\u2212 = lims<t,s\u2192t xs and \u0001xt = xt \u2212 xt\u2212.\n\nif x is a stochastic process whose paths are right continuous with left limits, then\n\nxv\n\n "}, {"Page_number": 18, "text": " "}, {"Page_number": 19, "text": "1\n\nbasic notions\n\nin a first course on probability one typically works with a sequence of random variables\nx1, x2, . . . for stochastic processes, instead of indexing the random variables by the pos-\nitive integers, we index them by t \u2208 [0,\u221e) and we think of xt as being the value at\ntime t. the random variable could be the location of a particle on the real line, the strength\nof a signal, the price of a stock, and many other possibilities as well.\nwe will also work with increasing families of \u03c3 -fields {f t}, known as filtrations. the\n\u03c3 -field ft is supposed to represent what we know up to time t.\n\n1.1 processes and \u03c3 -fields\n\nlet (\u0001,f , p) be a probability space. a real-valued stochastic process (or simply a process)\nis a map x from [0,\u221e) \u00d7 \u0001 to the reals. we write xt = xt (\u03c9) = x (t, \u03c9). we will impose\nstronger measurability conditions shortly, but for now we require that the random variables\nxt be measurable with respect to f for each t \u2265 0.\na collection of \u03c3 -fields ft such that ft \u2282 f for each t and fs \u2282 ft if s \u2264 t is called a\nfiltration. define ft+ = \u2229\u03b5>0ft+\u03b5. a filtration is right continuous if ft+ = ft for all t \u2265 0.\nthe \u03c3 -field ft+ is supposed to represent what one knows if one looks ahead an infinites-\nimal amount. most of the filtrations we will come across will be right continuous, but see\nexercise 1.1.\n\na null set n is one that has outer probability 0. this means that\n\ninf{p(a) : n \u2282 a, a \u2208 f} = 0.\n\na filtration is complete if each ft contains every null set. a filtration that is right continuous\nand complete is said to satisfy the usual conditions.\ngiven a filtration {ft}, whether or not it satisfies the usual conditions, we define f\u221e to be\nthe \u03c3 -field generated by \u222at\u22650ft, that is, the smallest \u03c3 -field containing \u222at\u22650ft, and we write\n\n(cid:4)\n\nt\u22650\n\nf\u221e =\n\nft .\n\nrecall that the arbitrary intersection of \u03c3 -fields is a \u03c3 -field, but the union of even two \u03c3 -fields\nneed not be a \u03c3 -field.\nwe say that a stochastic process x is adapted to a filtration {ft} if xt is ft measurable\nfor each t. often one starts with a stochastic process x and wants to define a filtration with\nrespect to which x is adapted.\n\n1\n\n "}, {"Page_number": 20, "text": "2\n\nbasic notions\n\nthe simplest way to do this is to let ft be the \u03c3 -field generated by the random variables\n{xs, s \u2264 t}. more often one wants to have a slightly larger filtration than the one generated\nby x .\n\nwe define the minimal augmented filtration generated by x to be the smallest filtration that\nis right continuous and complete and with respect to which the process x is adapted. for each\nt, ft is in general strictly larger than the smallest \u03c3 -field with respect to which {xs : s \u2264 t} is\nmeasurable because of the inclusion of the null sets. it is important to include the null sets;\nsee exercise 1.5. there is no widely accepted name for what we call the minimal augmented\nfiltration; i like this nomenclature because it is descriptive and sufficiently different from\n\u201cfiltration generated by x \u201d to avoid confusion.\nsteps. first, let {f 00\n\n} be the smallest filtration with respect to which x is adapted, that is,\n\nthe minimal augmented filtration generated by the process xt can be constructed in three\n\nt\n\nf 00\n\nt\n\n= \u03c3 (xs; s \u2264 t ).\n\n(1.1)\n\nlet p\u2217\n\nbe the outer probability corresponding to p: for a \u2282 \u0001,\np\u2217(a) = inf{p(b) : b \u2208 f , a \u2282 b}.\n\nlet n be the collection of null sets, so that n = {a \u2282 \u0001 : p\u2217(a) = 0}. the second step is\nto let f 0\n\nt be the smallest \u03c3 -field containing f 00\n= \u03c3 (f 00\n\nf 0\n\nand n , or\n\u222a n ).\n\nt\n\n(1.2)\n\nt\n\nt\n\nthe third step is to let\n\nft = \u2229\u03b5>0f 0\nt+\u03b5.\n\nt\n\n} as the filtration generated by x .\n\ntwo stochastic processes x andy are said to be indistinguishable if p(xt\n\n(1.3)\nexercise 1.2 asks you to check that {ft} is the minimal augmented filtration generated by x .\nwe will refer to {f 00\n(cid:16)= yt for some t \u2265\n0) = 0. x and y are versions of each other if for each t \u2265 0, we have p(xt\n(cid:16)= yt ) = 0. an\nexample of two processes that are versions of each other but are not indistinguishable is to\nlet \u0001 = [0, 1], f the borel \u03c3 -field on [0, 1], p lebesgue measure on [0, 1], x (t, \u03c9) = 0\nfor all t and \u03c9, and y (t, \u03c9) equal to 1 if t = \u03c9 and 0 otherwise. note that the functions\nt \u2192 x (t, \u03c9) are continuous for each \u03c9, but the functions t \u2192 y (t, \u03c9) are not continuous\nfor any \u03c9.\nif x is a stochastic process, the functions t \u2192 x (t, \u03c9) are called the paths or trajectories\nof x . there will be one path for each \u03c9. if the paths of x are continuous functions, except\nfor a set of \u03c9\u2019s in a null set, then x is called a continuous process, or is said to be continuous.\nwe similarly define right continuous process, left continuous process, etc.\na function f (t ) is right continuous with left limits if limh>0,h\u21930 f (t + h) = f (t ) for all\nt and limh<0,h\u21910 f (t + h) exists for all t > 0. almost all our stochastic processes will have\nthe property that except for a null set of \u03c9\u2019s the function t \u2192 x (t, \u03c9) is right continuous\nand has left limits. one often sees cadlag to refer to paths that are right continuous with left\nlimits; this abbreviates the french \u201ccontinue `a droite, limite `a gauche.\u201d\n\n "}, {"Page_number": 21, "text": "1.2 laws and state spaces\n\n3\n\n1.2 laws and state spaces\n\nlet s be a topological space. the borel \u03c3 -field on s is defined to be the \u03c3 -field generated\nby the open sets of s. a function f : s \u2192 r is borel measurable if f\n\u22121(g) is in the borel\n\u03c3 -field of s whenever g is an open subset of r. a random variable y: \u0001 \u2192 s is measurable\nwith respect to a \u03c3 -field f of subsets of \u0001 if {\u03c9 \u2208 \u0001 : y (\u03c9) \u2208 a} is in f whenever a is in\nthe borel \u03c3 -field on s.\na stochastic process taking values in a topological space s is a map x : [0,\u221e)\u00d7 \u0001 \u2192 s,\nwhere for each t, the random variable xt is measurable with respect to f.\nrecall that if we have a probability space (\u0001,f , p) and y: \u0001 \u2192 r is a random variable,\nthen the law of y is the probability measure py on the borel subsets of r defined by\npy (a) = p(y \u2208 a). similarly, if y: \u0001 \u2192 rd is a d-dimensional random vector, then the law\nof y is the probability measure py on the borel subsets of rd defined by py (a) = p(y \u2208 a).\nwe extend this definition to random variables y taking values in a topological space s. in\nthis case py is a probability measure on the borel subsets of s with the same definition:\npy (a) = p(y \u2208 a). in particular, if y and z are two random variables with the same state\nspace s, then y and z will have the same law if p(y \u2208 a) = p(z \u2208 a) for all borel subsets\na of s.\nthe relevance of the preceding paragraph to stochastic processes is this. suppose x and\ny are stochastic processes with continuous paths. let s = c[0,\u221e) be the collection of\nreal-valued continuous functions on [0,\u221e) together with the usual metric defined in terms\nof the supremum norm:\n\nd ( f , g) = sup\n0\u2264t\n\n| f (t ) \u2212 g(t )|.\n\n(strictly speaking, we should write c([0,\u221e)), but we follow the usual convention and drop\nthe outside parentheses.) let the random variable x taking values in s be defined by setting\nx (\u03c9) to be the continuous function t \u2192 x (t, \u03c9), and define y similarly. more precisely,\nx : \u0001 \u2192 s with\n\nx (\u03c9)(t ) = x (t, \u03c9),\n\nt \u2265 0.\n\nthen x and y are random variables taking values in the metric space s, and saying that x\nand y have the same law means that p(x \u2208 a) = p(y \u2208 a) for all borel subsets a of s.\nwhen this happens, we also say that the stochastic processes x and y have the same law.\ntwo stochastic processes x and y have the same finite-dimensional distributions if for\nevery n \u2265 1 and every t1 < \u00b7\u00b7\u00b7 < tn, the laws of (xt1\nmost often the topological spaces we will consider will also be metric spaces, but there\nwill be a few occasions when we want to consider topological spaces that are not metric\nspaces. suppose s = r[0,\u221e). we furnish s with the product topology. s can be identified\nwith the collection of real-valued functions on [0,\u221e), but the topology is not given by the\nsupremum norm nor by any other metric. we use f for elements of s, where f (t ) is the tth\ncoordinate of f . we call a subset a of s a cylindrical set if there exist n \u2265 1, non-negative\nreals t1, t2, . . . , tn, and a borel subset b of rn such that\n\n) are equal.\n\n) and (yt1\n\n, . . . , xtn\n\n, . . . , ytn\n\na = { f \u2208 s : ( f (t1), . . . , f (tn)) \u2208 b}.\n\n "}, {"Page_number": 22, "text": "4\nthe appropriate \u03c3 -field to use on s is the one generated by the collection of cylindrical\nsets.\n\nbasic notions\n\nwe want to generalize this notion slightly by allowing more general index sets and by\n\nallowing for the possibility of considering only a subset of the product space.\ndefinition 1.1 let u be a topological space, t an arbitrary index set, and b a subset of u t ,\nthe collection of functions from t into u. we say a set c is a cylindrical subset of b if there\nexist n \u2265 1, t1, . . . , tn \u2208 t , and a borel subset a of rn such that\nc = { f \u2208 b : ( f (t1), . . . , f (tn)) \u2208 a}.\n\n1.1\n\n1.2\n\n1.3\n\n1.4\n\nthis exercise gives an example where {f 00\n\u0001 = {a, b}, let f be the collection of all subsets of \u0001, and let p({a}) = p({b}) = 1\n\n} defined by (1.1) is not right continuous. let\n\nt\n\n2 . define\n\nexercises\n\n\u23a7\u23aa\u23a8\u23aa\u23a90,\n\nt \u2264 1;\nt > 1 and \u03c9 = a;\nt > 1 and \u03c9 = b.\n\nxt (\u03c9) =\n\n0,\nt \u2212 1,\n= \u03c3 (xs; s \u2264 t ) and show {f 00\n\nt\n\nt\n\nt\n\n, f 0\n\n} is not right continuous.\n\ncalculate f 00\nif x is a stochastic process, let f 00\nt , and ft be defined by (1.1), (1.2), and (1.3), respectively.\nshow that {ft} is the minimal augmented filtration generated by x .\nlet {ft} be a filtration satisfying the usual conditions and let b[0, t] be the borel \u03c3 -field on\n[0, t]. a real-valued stochastic process x is progressively measurable if for each t \u2265 0, the\nmap (s, \u03c9) \u2192 x (s, \u03c9) from [0, t] \u00d7 \u0001 to r is measurable with respect to the product \u03c3 -field\nb[0, t] \u00d7 ft.\n\u221e(cid:9)\n(1) if x is adapted to {ft} and we define\n\n(\u03c9) =\n\nx (n)\nt\n\nk=0\n\nxk/2n (\u03c9)1[k/2n,(k+1)/2n )(t ),\n\nshow that x (n) is progressively measurable for each n \u2265 1.\n(2) use (1) to show that if x is adapted to {ft} and has left continuous paths, then x is\nprogressively measurable.\n\u221e(cid:9)\n(3) if x is adapted to {ft} and we define\n\n(\u03c9) =\n\ny (n)\nt\n\nk=0\n\nx(k+1)/2n (\u03c9)1[k/2n,(k+1)/2n )(t ),\n\nshow that for each t \u2265 0, the map (s, \u03c9) \u2192 y (n)(s, \u03c9) from [0, t] \u00d7 \u0001 to r is measurable with\nrespect to b[0, t] \u00d7 ft+2\u2212n.\n(4) show that if x is adapted to {ft} and has right continuous paths, then x is progressively\nmeasurable.\nlet s = r[0,1], the set of functions from [0, 1] to r, and let f be the \u03c3 -field generated by the\ncylindrical sets. the purpose of this exercise is to show that the elements of f depend on only\ncountably many coordinates.\n\n "}, {"Page_number": 23, "text": "1.6\n\n1.7\n\n5\nlet s0 = {(x1, x2, . . .)}, the set of sequences taking values in r. let f0 be the \u03c3 -field\ngenerated by the cylindrical subsets of rn, where n = {1, 2, . . .}.\nshow that b \u2208 f if and only if there exist t1, t2, . . . in [0, 1] and a set c \u2208 f0 such that\n\nnotes\n\nb = { f \u2208 s : ( f (t1 ), f (t2 ), . . .) \u2208 c}.\n\n1.5 null sets are sometimes important! let s and f be as in exercise 1.4. show that d /\u2208 f, where\n\nd = { f \u2208 s :\n\nf is a continuous function on [0, 1]}.\n\nsuppose x is a stochastic process, {ft} its minimal augmented filtration, and f\u221e = \u2228t\u22650ft.\nsuppose with probability one, the paths of x are right continuous with left limits. let xt\u2212 =\nlims<t,s\u2192t xs, the left-hand limit at time t, and \u0001xt = xt \u2212 xt\u2212, the size of the jump at time t. if\n\na = {\u2203 t \u2265 0 : \u0001xt > 1},\n\nprove a \u2208 f\u221e.\nsuppose x is a stochastic process, {ft} is the minimal augmented filtration for x , and f\u221e =\n\u2228t\u22650ft. if the paths of x are right continuous with left limits with probability one, show that\nthe event\n\na = {x has continuous paths}\n\nis in f\u221e.\n\nnotes\n\nthe older literature sometimes uses the notion of a separable stochastic process, but this is\nrarely seen nowadays. for much more on measurability, see chapter 16. for the complete\nstory on the foundations of stochastic processes, see dellacherie and meyer (1978).\n\n "}, {"Page_number": 24, "text": "2\n\nbrownian motion\n\nbrownian motion is by far the most important stochastic process. it is the archetype of\ngaussian processes, of continuous time martingales, and of markov processes. it is basic to\nthe study of stochastic differential equations, financial mathematics, and filtering, to name\nonly a few of its applications.\n\nin this chapter we define brownian motion and consider some of its elementary aspects.\nlater chapters will take up the construction of brownian motion and properties of brownian\nmotion paths.\n\n2.1 definition and basic properties\n\nlet (\u0001,f , p) be a probability space and let {ft} be a filtration, not necessarily satisfying\nthe usual conditions.\ndefinition 2.1 wt = wt (\u03c9) is a one-dimensional brownian motion with respect to {ft} and\nthe probability measure p, started at 0, if\n(1) wt is ft measurable for each t \u2265 0.\n(2) w0 = 0, a.s.\n(3) wt \u2212 ws is a normal random variable with mean 0 and variance t \u2212 s whenever s < t.\n(4) wt \u2212 ws is independent of fs whenever s < t.\n(5) wt has continuous paths.\n\nif instead of (2) we have w0 = x, we say we have a brownian motion started at x. defini-\ntion 2.1(4) is referred to as the independent increments property of brownian motion. the\nfact that wt \u2212 ws has the same law as wt\u2212s, which follows from definition 2.1(3), is called\nthe stationary increments property. when no filtration is specified, we assume the filtration\nis the filtration generated by w , i.e., ft = \u03c3 (ws; s \u2264 t ). sometimes a one-dimensional\nbrownian motion started at 0 is called a standard brownian motion.\nfigure 2.1 is a simulation of a typical brownian motion path.\nwe define d-dimensional brownian motion with respect to a filtration {ft} and started at\nx = (x1, . . . , xd ) to be (w (1)\n), where the w (i) are each one-dimensional brow-\nnian motions with respect to {ft} started at xi, respectively, and w (1), . . . , w (n) are all\nindependent.\nthe law of a brownian motion is called wiener measure. more precisely, given a\nbrownian motion w , we can view it as a random variable taking values in c[0,\u221e), the\nspace of real-valued continuous functions on [0,\u221e). the law of w is the measure pw on\n\n, . . . , w (d )\n\nt\n\nt\n\n6\n\n "}, {"Page_number": 25, "text": "2.1 definition and basic properties\n\n7\n\n2\n\n1.5\n\n1\n\n0.5\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nfigure 2.1 simulation of a typical brownian motion path.\n\nc[0,\u221e) defined by pw (a) = p(w \u2208 a) for all borel subsets a of c[0,\u221e). the measure\npw is wiener measure.\n\nthere are a number of transformations one can perform on a brownian motion that yield\na new brownian motion. the first one is called the scaling property of brownian motion, or\nsimply scaling.\nproposition 2.2 if w is a brownian motion started at 0, a > 0, and yt = awt/a2, then yt is a\nbrownian motion started at 0.\nproof we use gt = ft/a2 for the filtration for y . clearly yt has continuous paths, y0 = 0,\na.s., and yt is gt measurable. if s < t,\n\nis independent of fs/a2, hence is independent of gs. finally, if s < t, and if s < t, then yt \u2212ys\nwill be a normal random variable with mean zero and\n\nyt \u2212 ys = a(wt/a2 \u2212 ws/a2 )\n(cid:10)\n\n(cid:11)\n\nvar (yt \u2212 ys) = a2var (wt/a2 \u2212 ws/a2 ) = a2\n\nt\na2\n\n\u2212 s\na2\n\n= t \u2212 s.\n\nthis suffices to give our result.\n\nfor some other transformations, see exercises 2.3 and 2.5.\nrecall what it means for a finite collection of random variables to be jointly normal; see\n(a.29). a stochastic process x is gaussian or jointly normal if all its finite-dimensional\ndistributions are jointly normal, that is, if for each n \u2265 1 and t1 < \u00b7\u00b7\u00b7 < tn, the collection of\nrandom variables xt1\n\n, . . . , xtn is a jointly normal collection.\n\n "}, {"Page_number": 26, "text": "8\n\nbrownian motion\n\nproposition 2.3 if w is a brownian motion, then w is a gaussian process.\nproof suppose w is a brownian motion and let 0 = t0 < t1 < \u00b7\u00b7\u00b7 < tn. define\n\n\u2212 wti\u22121\nzi = wti\n\u221a\nti \u2212 ti\u22121\n\n,\n\ni = 1, 2, . . . , n.\n\nby definition 2.1(4), zi is independent of fti\u22121, and hence independent of z1, . . . , z j\u22121. by\ndefinition 2.1(3), zi is a mean-zero random variable with variance one. we can write\n\n= j(cid:9)\n\ni=1\n\nwt j\n\n(ti \u2212 ti\u22121)1/2zi,\n\nj = 1, . . . , n,\n\nand so (wt1\nprocess.\n\n, . . . , wtn\n\n) is jointly normal. it follows that brownian motion is a gaussian\n\nsince the law of a finite collection of jointly normal random variables is determined by\ntheir means and covariances, let\u2019s calculate the covariance of ws and wt when w is a brownian\nmotion. if s \u2264 t, then\n\nt \u2212 s = var (wt \u2212 ws) = var wt + var ws \u2212 2 cov (ws, wt )\n\n= t + s \u2212 2 cov (ws, wt )\n\nfrom definition 2.1(2) and (3). hence cov (ws, wt ) = s if s \u2264 t. this is frequently written as\n(2.1)\n\ncov (ws, wt ) = s \u2227 t.\n\nwe have the following converse.\n\ntheorem 2.4 if w is a process such that all the finite-dimensional distributions are jointly\nnormal, e ws = 0 for all s, cov (ws, wt ) = s when s \u2264 t, and the paths of wt are continuous,\nthen w is a brownian motion.\nproof for ft we take the filtration generated by w . if we take s = t, then var wt =\ncov (wt , wt ) = t. in particular, var w0 = 0, and since e w0 = 0, then w0 = 0, a.s. we have\n\nvar (wt \u2212 ws) = var wt \u2212 2 cov (ws, wt ) + var wt\n\n= t \u2212 2s + s = t \u2212 s.\n\nwe have thus established all the parts of definition 2.1 except for the independence of wt\u2212ws\nfrom fs.\nif r \u2264 s < t, then\n\ncov (wt \u2212 ws, wr ) = cov (wt , wr ) \u2212 cov (ws, wr ) = r \u2212 r = 0,\n\nand so wt \u2212 ws is independent of wr by proposition a.55. this shows that wt \u2212 ws is\nindependent of fs.\nwe now look at two results that are more technical. these should only be skimmed on the\nfirst reading of the book: read the statements, but not the proofs. the first result says that\nif w is a brownian motion with respect to the filtration generated by w , then it is also a\nbrownian motion with respect to the minimal augmented filtration.\n\n "}, {"Page_number": 27, "text": "(2.2)\n\n2.1 definition and basic properties\n\n9\n}, where f 00\n=\n\u222a n ), and ft = \u2229\u03b5>0f 0\nt+\u03b5.\n\nt\n\nt\n\nproposition 2.5 let wt be a brownian motion with respect to {f 00\n\u03c3 (ws; s \u2264 t ). let n be the collection of null sets, f 0\n= \u03c3 (f 00\n(1) w is a brownian motion with respect to the filtration {ft}.\n(2) ft = f 0\nproof\nbounded function on r, a \u2208 f 00\nrespect to {f 00\n\n}, the independent increments property shows that\ne [ f (wt \u2212 ws); a] = e [ f (wt \u2212 ws)] p(a).\n\nfor each t.\n\ns\n\nt\n\nt\n\nt\n\nt\n\n(1) the only property we need to check is definition 2.1(4). if f is a continuous\n, and s < t, then because w is a brownian motion with\n\nif a is such that a\\b and b\\a are null sets for some b \u2208 f 00\n, it is easy to see that (2.2)\ncontinues to hold. by linearity, it also holds if a is a finite disjoint union of such sets. if c1\nis the collection of subsets of f 0\ns that are finite disjoint unions of such sets, then c1 is an\ns . let m1 be the collection of subsets of f 0\nalgebra of subsets of f 0\ns for which (2.2) holds. it\nis readily checked that m1 is a monotone class. by the monotone class theorem (theorem\nb.2), m1 is equal to the smallest \u03c3 -field containing c1, which is f 0\ns . therefore (2.2) holds\nfor all a \u2208 f 0\ns .\nnow suppose a \u2208 fs = f 0\nreplaced by s + \u03b5 and t replaced by t + \u03b5, we have\n\ns+. then for each \u03b5 > 0, a \u2208 f 0\n\ns+\u03b5, and so using (2.2) with s\n\ns\n\ne [ f (wt+\u03b5 \u2212 ws+\u03b5 ); a] = e [ f (wt+\u03b5 \u2212 ws+\u03b5 )] p(a).\n\n(2.3)\nletting \u03b5 \u2192 0 and using the facts that f is bounded and continuous and w has continuous\npaths, the dominated convergence theorem implies that\n\ne [ f (wt \u2212 ws); a] = e [ f (wt \u2212 ws)] p(a).\n\n(2.4)\nthis equation holds whenever f is continuous and a \u2208 fs. by a limit argument, (2.4) holds\nis the indicator of a borel subset of r. that says that wt \u2212 ws and fs are\nwhenever f\nindependent.\n(2) fix t and choose t0 > t. let m2 be the collection of subsets of f 00\nt0 whose conditional\nt0 and e [1a | ft]\nexpectation with respect to ft is f 0\nt measurable. let c2 be the collection of events a for which there exist n \u2265 1, 0 \u2264 s0 <\nis f 0\ns1 < \u00b7\u00b7\u00b7 < sn \u2264 t0 with t equal to one of the si, and borel subsets b1, . . . , bn of r such\nthat\n\nt measurable, that is, a \u2208 m2 if a \u2208 f 00\n\na = (ws1\n\n\u2212 ws0\n\n\u2208 b1, . . . , wsn\n\n\u2212 wsn\u22121\n\n\u2208 bn).\n\nsuppose a is of this form, and suppose t = si. then by the independence result that we\nproved in (1),\n\ne [1a | ft] = 1(ws1\n\n\u2212ws0\n\u00d7 p(wsi+1\n\n\u2208b1,...,wsi\n\u2212 wsi\n\n\u2212wsi\u22121\n\n\u2208bi )\n\u2208 bi+1, . . . , wsn\n\n\u2212 wsn\u22121\n\n\u2208 bn),\n\nwhich is f 0\nsubsets of f 00\nmonotone class theorem, m2 equals f 00\nnon-negative and f 00\n\nt measurable. thus c2 \u2282 m2. finite unions of sets in c2 form an algebra of\n. it is easy to check that m2 is a monotone class, so by the\n. by linearity and taking monotone limits, if y is\n\nt measurable, then e [y | ft] is f 0\n\nthat generate f 00\n\nt measurable.\n\nt0\n\nt\n\nt\n\n "}, {"Page_number": 28, "text": "10\n\nbrownian motion\n\nt0 such that 1a = y , a.s. then e [y | ft] is f 0\n\nto finish, suppose a \u2208 ft. then since t < t0, we see that a \u2208 f 0\nexists y \u2208 f 00\nall the null sets, 1a = e [1a | ft] is also f 0\nt measurable, or a \u2208 f 0\nthe final item we consider in this chapter is a subtle one. the question is this: if w and\n(cid:3)\nare both brownian motions, do they have all the same properties? to illustrate this issue,\nw\nlet\u2019s revisit the example of chapter 1 where \u0001 = [0, 1], f is the borel \u03c3 -field on [0, 1],\np is lebesgue measure on [0, 1], x (t, \u03c9) = 0 for all t and \u03c9, and y (t, \u03c9) is 1 if t = \u03c9\nand 0 otherwise. for each t, p(xt = yt ) = 1, so x and y have the same finite-dimensional\ndistributions. however, if\n\nt measurable. since f 0\nt . this proves (2).\n\nt0. by exercise 2.7, there\nt contains\n\na = { f : f is not a continuous function on [0, 1]},\n\n(cid:3)\n\nto rephrase our question, is it true that p(w \u2208 a) = p(w\n\nthen (x \u2208 a) is a null set but (y \u2208 a) is not. even though x and y have the same\nfinite-dimensional distributions, x has continuous paths but y does not.\n(cid:3) \u2208 a) for every borel subset\na of c[0,\u221e)? we know w and w\nhave the same finite-dimensional distributions because\neach is jointly normal with zero means and cov (ws, wt ) = s \u2227 t = cov (w\n). the fact\nthat the answer to our question is yes then comes from the following theorem. we look at\nc[0, t0] instead of c[0,\u221e) for the sake of simplicity.\ntheorem 2.6 let t0 > 0 and let x , y be random variables taking values in c[0, t0] which\nhave the same finite-dimensional distributions. then the laws of x and y are equal.\nproof let m be the collection of borel subsets a of c[0, t0] for which p(x \u2208 a) equals\np(y \u2208 a). we will show that m is a monotone class and then use the monotone class\ntheorem to show that m is equal to the borel \u03c3 -field on c[0, t0].\nfirst, let c be the collection of all cylindrical subsets of c[0, t0] (defined by defini-\ntion 1.1). since the finite-dimensional distributions of x and y are equal, then m contains\nc. it is easy to check that c is an algebra of subsets of c[0, t0]. if a1 \u2283 a2 \u2283 \u00b7\u00b7\u00b7 are elements\nof m, then\n\n, w\nt\n\n(cid:3)\ns\n\n(cid:3)\n\np(x \u2208 \u2229nan) = lim\n\np(x \u2208 an) = lim\n\np(y \u2208 an) = p(y \u2208 \u2229nan)\n\nn\n\nn\n\nsince p is a finite measure. therefore \u2229nan \u2208 m. a very similar argument shows that if\na1 \u2282 a2 \u2282 \u00b7\u00b7\u00b7 are elements of m, then \u222anan \u2208 m. therefore m is a monotone class. by\nthe monotone class theorem, m contains the smallest \u03c3 -field containing c. we will show\nthat m contains all the open sets; then m will contain the smallest \u03c3 -field containing the\nopen sets, and we will be done.\nsince c[0, t0] is separable, every open set is the countable union of open balls. because\nm is a \u03c3 -field, it suffices to show that m contains the open balls in c[0, t0], that is, all sets\nof the form\n\nb( f0, r) = { f \u2208 c[0, t0] : sup\n0\u2264t\u2264t0\nwhere r > 0 and f0 \u2208 c[0, t0]. for each m and n,\n\n| f (t ) \u2212 f0(t )| < r}\n\n{ f \u2208 c[0, t0] :\n\n| f (k/2n) \u2212 f0(k/2n)| \u2264 r \u2212 (1/m)}\n\nsup\n0\u2264k\u22642nt0\n\n "}, {"Page_number": 29, "text": "exercises\n\n11\n\nis a set in c, and so is in m. as n \u2192 \u221e, these sets decrease to\n\ndm = { f \u2208 c[0, t0] : sup\n0\u2264t\u2264t0\nsince all the functions we are considering are continuous. finally, dm increases to b( f0, r)\nas m \u2192 \u221e, so b( f0, r) is in m as desired.\n\n| f (t ) \u2212 f0(t )| \u2264 r \u2212 (1/m)},\n\n2.1\n\n2.2\n\n2.3\n\nexercises\n\nsuppose w is a brownian motion on [0, 1]. let\n\nyt = w1\u2212t \u2212 w1.\n\nshow that yt is a brownian motion on [0, 1].\n\nthis exercise shows that the projection of a d-dimensional brownian motion onto a hyperplane\n(cid:12)\nyields a one-dimensional brownian motion. suppose (w (1)\n) is a d-dimensional\nbrownian motion started from 0 and \u03bb1, . . . , \u03bbd \u2208 r with\n= 1. show that xt =\n\nd\ni=1\nis a one-dimensional brownian motion started from 0.\n\n, . . . , w (d )\n\nt\n\u03bb2\ni\n\nt\n\n\u03bbiw (i)\n\nd\ni=1\n\nt\n\nthis exercise shows that rotating a brownian motion about the origin yields another brownian\nmotion. let w be a d-dimensional brownian motion started at 0 and let a be a d \u00d7 d orthogonal\nmatrix, that is, a\n\n\u22121 = at. show that yt = awt is again a d-dimensional brownian motion.\n\n(cid:12)\n\n\u03bb2\ni\n\nd\ni=1\n\n(cid:12)\n\n, . . . , x d\nt\n\n2.4 here is a converse to exercise 2.2: roughly speaking, if all the projections of a d-dimensional\nprocess x onto hyperplanes are one-dimensional brownian motions, then x is a d-dimensional\nbrownian motion.\n(cid:12)\nsuppose (x 1\n) is a d-dimensional continuous process, i.e., one taking values in\nrd. let {ft} be the minimal augmented filtration generated by x . suppose that whenever\nt\n(cid:12)\n= 1, then\n\u03bb1, . . . , \u03bbd \u2208 r with\nd\ni=1\nis a one-dimensional brownian motion\nstarted at 0 with respect to the filtration {ft}.\n(cid:11)\n)1/2 and let \u03bb j = u j/(cid:21)u(cid:21). calculate\n(1) if u = (u1, . . . , ud ), let (cid:21)u(cid:21) = (\nd(cid:9)\nu2\nj\n= e exp\n\ni(cid:21)u(cid:21) d(cid:9)\nj=1\nthe joint characteristic function of xt.\n(cid:11)\n(2) if t0 < t1 < \u00b7\u00b7\u00b7 < tn, use independence and (1) to calculate\n\n\u03bb jx j\nt\n\nu jx j\nt\n\ne exp\n\n\u03bbix i\nt\n\n(cid:10)\n\n(cid:11)\n\n(cid:10)\n\n(cid:10)\n\nj=1\n\ni\n\n,\n\nn\u22121(cid:9)\n\nd(cid:9)\n\ne exp\n\ni\n\nuk\nj\n\n(x j\ntk+1\n\n\u2212 x j\n\ntk\n\n)\n\n.\n\nk=0\n\n(some care is needed with the filtrations. if we only know that y \u03bb =(cid:12)\n\n(3) prove that (x 1\nt\n\u03bbix i is a brownian\nmotion with respect to the filtration generated by y \u03bb for each \u03bb = (\u03bb1, . . . , \u03bbd ), the assertion is\nnot true. see revuz and yor (1999), exercise i.1.19.)\n\n) is a d-dimensional brownian motion started from 0.\n\n, . . . , x d\nt\n\nj=1\n\ni\n\n2.5\n\nlet wt be a brownian motion and suppose\n\n(2.5)\nlet zt = tw1/t if t > 0 and set z0 = 0. (this is called time inversion.) show that z is a brownian\nmotion. (we will see later that the assumption (2.5) is superfluous; see theorem 7.2.)\n\nlim\n\na.s.\n\nt\u2192\u221e wt /t = 0,\n\n "}, {"Page_number": 30, "text": "12\n\n2.6\n\n2.7\n\n2.8\n\nlet x and y be two independent brownian motions started at 0 and let t0 > 0. let\n\nbrownian motion\n\n(cid:13)\n\nzt =\n\nxt ,\nxt0\n\n+ yt\u2212t0\n\n,\n\nt \u2264 t0,\nt > t0.\n\nt\n\nand f 0\n\nprove that z is also a brownian motion.\nlet f 00\nt measurable and y = z, a.s.\nz such that z is f 00\nlet f 00\nis defined by a \u0001 b = (a\\b) \u222a (b\\a). prove that\n\nand f 0\n\nt\n\nt be defined as in (1.1) and (1.2). prove that if x is f 0\n\nt measurable, there exists\n\nt be defined as in (1.1) and (1.2). the symmetric difference of two sets a and b\n\nf 0\n\nt\n\n= {a \u2282 \u0001 : a \u0001 b \u2208 n for some b \u2208 f 00\n\n}.\n\nt\n\nnotes\n\nbrownian motion is named for robert brown, a botanist who observed the erratic motion\nof colloidal particles in suspension in the 1820s. brownian motion was used by bachelier\nin 1900 in his phd thesis to model stock prices and was the subject of an important paper\nby einstein in 1905. the rigorous mathematical foundations for brownian motion were first\ngiven by wiener in 1923.\n\n "}, {"Page_number": 31, "text": "3\n\nmartingales\n\nalthough discrete-time martingales are useful in a first course on probability, they are nowhere\nnear as useful as continuous-time martingales are in the study of stochastic processes.\nthe whole theory of stochastic integrals and stochastic differential equations is based on\nmartingales indexed by times t \u2208 [0,\u221e). after giving the definition and some examples, we\nextend doob\u2019s inequalities, the optional stopping theorem, and the martingale convergence\ntheorem to continuous-time martingales. we then derive some estimates for brownian motion\nusing martingale techniques.\n\n3.1 definition and examples\n\nwe define continuous-time martingales. let {f t} be a filtration, not necessarily satisfying\nthe usual conditions.\ndefinition 3.1 mt is a continuous-time martingale with respect to the filtration {ft} and the\nprobability measure p if\n(1) e|mt| < \u221e for each t;\n(2) mt is ft measurable for each t;\n(3) e [mt | fs] = ms, a.s., if s < t.\npart (2) of the definition can be rephrased as saying mt is adapted to ft. if in part (3) \u201c=\u201d\nis replaced by \u201c\u2265,\u201d then mt is a submartingale, and if it is replaced by \u201c\u2264,\u201d then we have a\nsupermartingale.\ntaking expectations in definition 3.1(3), we see that if s < t, then e ms \u2264 e mt is m is\na submartingale and e ms \u2265 e mt if m is a supermartingale. thus submartingales tend to\nincrease, on average, and supermartingales tend to decrease, on average.\n\nthere are many martingales associated with brownian motion. here are three examples.\nexample 3.2 let mt = wt, where wt is a brownian motion. then mt is a martingale. to\nverify definition 3.1(3), we write\n\ne [mt | fs] = ms + e [wt \u2212 ws | fs] = ms + e [wt \u2212 ws] = ms,\n\nusing the independent\ne [wt \u2212 ws] = 0.\n\nincrements property of brownian motion and the fact\n\nthat\n\n13\n\n "}, {"Page_number": 32, "text": "14\nexample 3.3 let mt = w 2\nwe write\n\nt\n\nmartingales\n\n\u2212t, where wt is a brownian motion. to show mt is a martingale,\n\ne [mt | fs] = e [(wt \u2212 ws + ws)2 | fs] \u2212 t\n\ns\n\n= w 2\n= w 2\n= w 2\n= w 2\n\ns\n\ns\n\ns\n\n+ e [(wt \u2212 ws)2 | fs] + 2e [ws(wt \u2212 ws) | fs] \u2212 t\n+ e [(wt \u2212 ws)2] + 2wse [wt \u2212 ws | fs] \u2212 t\n+ e [(wt \u2212 ws)2] + 2wse [wt \u2212 ws] \u2212 t\n+ (t \u2212 s) \u2212 t = ms.\n\nwe used the facts that ws is fs measurable and that wt \u2212 ws is independent of fs.\nexample 3.4 again let wt be a brownian motion, let a \u2208 r, and let mt = eawt\u2212a2t/2. since\nwt \u2212 ws is normal with mean zero and variance t \u2212 s, we know e ea(wt\u2212ws ) = ea2 (t\u2212s)/2; see\n(a.6). then\n\ne [mt | fs] = e\n= e\n= e\n\n\u2212a2t/2eaws e [ea(wt\u2212ws ) | fs]\n\u2212a2t/2eaws e [ea(wt\u2212ws )]\n\u2212a2t/2eawsea2 (t\u2212s)/2 = ms.\n\nwe give one more example of a martingale, although not one derived from brownian\n\nmotion.\nexample 3.5 recall that given a filtration {ft}, each ft is contained in f, where (\u0001,f , p)\nis our probability space. let x be an integrable f measurable random variable, and let\nmt = e [x | ft]. then\n\ne [mt | fs] = e [e [x | ft] | fs] = e [x | fs] = ms,\n\nand m is a martingale.\n\n3.2 doob\u2019s inequalities\n\nwe derive the analogs of doob\u2019s inequalities in the stochastic process context.\n\ntheorem 3.6 suppose mt is a martingale or non-negative submartingale with paths that\nare right continuous with left limits. then\n\n(1)\n\n(2) if 1 < p < \u221e, then\n\np(sup\ns\u2264t\n\n|ms| \u2265 \u03bb) \u2264 e|mt|/\u03bb.\n\n(cid:10)\n\n(cid:11)p\n\n|ms|]p \u2264\n\ne [sup\ns\u2264t\n\np\np \u2212 1\n\ne|mt|p.\n\nproof we will do the case where mt is a martingale, the submartingale case being nearly\nidentical. let dn = {kt/2n : 0 \u2264 k \u2264 2n}. if we set n (n)\n= fkt/2n, it is\nclear that {n (n)\n|ms| > \u03bb}.\n\n} is a discrete-time martingale with respect to {g (n)\n\n= mkt/2n and g (n)\n\n}. let\n\nk\n\nk\n\nk\n\nk\n\nan = { sup\ns\u2264t,s\u2208dn\n\n "}, {"Page_number": 33, "text": "3.3 stopping times\n\n15\n\nby doob\u2019s inequality for discrete-time martingales (see theorem a.32),\n\np(an) = p(max\nk\u22642n\n\n|n (n)\n\nk\n\n| > \u03bb) \u2264 e|n (n)\n2n |\n\n\u03bb\n\n= e|mt|\n\n\u03bb\n\n.\n\nnote that the an are increasing, and since mt is right continuous,\n\n\u222anan = {sup\ns\u2264t\n\n|ms| > \u03bb}.\n\nthen\n\nn\u2192\u221e p(an) \u2264 e|mt|/\u03bb.\nif we apply this with \u03bb replaced by \u03bb \u2212 \u03b5 and let \u03b5 \u2192 0, we obtain (1).\n\n|ms| > \u03bb) = p(\u222anan) = lim\n\np(sup\ns\u2264t\n\ntheorem a.33),\n\nthe proof of (2) is similar. by doob\u2019s inequality for discrete-time martingales (see\n\n(cid:10)\n\n(cid:10)\n\n(cid:11)p\n\n(cid:11)p\n|ms|p by the right continuity of m, (2) follows by\n\ne|mt|p.\n\n2n |p =\n\np\np \u2212 1\n\ne|n (n)\n\nk\n\n|n (n)\n\n|p] \u2264\n\np\np \u2212 1\ne [sup\nk\u22642n\n|p increases to sups\u2264t\n\nsince supk\u22642n |n (n)\nfatou\u2019s lemma.\n\nk\n\n3.3 stopping times\n\ngiven a stochastic process x , we define xt (\u03c9) to be equal to x (t (\u03c9), \u03c9); that is, for\n\nthroughout this section we suppose we have a filtration {ft} satisfying the usual conditions.\ndefinition 3.7 a random variable t : \u0001 \u2192 [0,\u221e] is a stopping time if for all t, (t < t ) \u2208\nft. we say t is a finite stopping time if t < \u221e, a.s. we say t is a bounded stopping time if\nthere exists k \u2208 [0,\u221e) such that t \u2264 k, a.s.\nnote that t can take the value infinity. stopping times are also known as optional times.\neach \u03c9 we evaluate t = t (\u03c9) and then look at x (\u00b7, \u03c9) at this time.\nproposition 3.8 suppose ft satisfies the usual conditions. then\n(1) t is a stopping time if and only if (t \u2264 t ) \u2208 ft for all t.\n(2) if t = t, a.s., then t is a stopping time.\n(3) if s and t are stopping times, then so are s \u2228 t and s \u2227 t .\n(4) if tn, n = 1, 2, . . . , are stopping times with t1 \u2264 t2 \u2264 \u00b7\u00b7\u00b7 , then so is supn tn.\n(5) if tn, n = 1, 2, . . . , are stopping times with t1 \u2265 t2 \u2265 \u00b7\u00b7\u00b7 , then so is inf n tn.\n(6) if s \u2265 0 and s is a stopping time, then so is s + s.\n\nproof we will just prove part of (1), leaving the rest as exercise 3.4. note (t \u2264 t ) =\n\u2229n\u2265n (t < t + 1/n) \u2208 ft+1/n for each n. thus (t \u2264 t ) \u2208 \u2229nft+1/n \u2282 ft+ = ft.\nfor a borel measurable set a, let\n\nta = inf{t > 0 : xt \u2208 a}.\n\n(3.1)\n\n "}, {"Page_number": 34, "text": "16\n\nmartingales\n\nproposition 3.9 suppose ft satisfies the usual conditions and xt has continuous paths.\n(1) if a is open, then ta is a stopping time.\n(2) if a is closed, then ta is a stopping time.\n\n(1) (ta < t ) = \u2229q\u2208q+,q<t (xq \u2208 a), where q+ denotes the set of non-negative\nproof\nrationals. since (xq \u2208 a) \u2208 fq \u2282 ft, then (ta < t ) \u2208 ft.\n(2) let an = {x : dist(x, a) < 1/n}, the set of points within a distance 1/n from a.\neach an is open and thus by (1), tan is a stopping time. moreover, the an decrease, so the\ntan increase. let t = supn tan, a stopping time by proposition 3.8(4). since a \u2282 an, then\nta \u2265 tan, so ta \u2265 t . because x has continuous paths, on (t < \u221e), xt = limn xtan . if\nn \u2265 m, then x (tan\n) \u2208 an \u2282 am. therefore xt \u2208 am for each m. since a = \u2229mam, then\nxt \u2208 a. therefore ta \u2264 t , and hence t = ta.\n\nit is true that under the hypotheses of the preceding proposition, ta is a stopping time for\n\nevery borel set a, but that is much harder to prove; see section 16.2.\nt is a finite stopping time, that is, t < \u221e, a.s., define\n\nis often useful\n\nit\n\nto be able to approximate stopping times from the right. if\n\ntn(\u03c9) = (k + 1)/2n\n\nif k/2n \u2264 t (\u03c9) < (k + 1)/2n.\n\n(3.2)\n\nexercise 3.5 asks you to prove that the tn are stopping times decreasing to t .\n\ndefine\n\nft = {a \u2208 f : for each t > 0, a \u2229 (t \u2264 t ) \u2208 ft}.\n\n(3.3)\nthis definition of ft , which is supposed to be the collection of events that are \u201cknown\u201d by\ntime t , is not very intuitive. but it turns out that this definition works well in applications.\nexercise 3.6 gives an equivalent definition that is more appealing but not as useful.\nproposition 3.10 suppose {ft} is a filtration satisfying the usual conditions.\n(1) ft is a \u03c3 -field.\n(2) if s \u2264 t , then fs \u2282 ft .\n(3) if ft+ = \u2229\u03b5>0ft+\u03b5, then ft+ = ft .\n(4) if xt has right-continuous paths, then xt is ft measurable.\nif a \u2208 ft , then ac \u2229 (t \u2264 t ) = (t \u2264 t ) \\ [a \u2229 (t \u2264 t )] \u2208 ft, so ac \u2208 ft . the rest\nproof\nof the proof of (1) is easy.\nsuppose a \u2208 fs and s \u2264 t . then a \u2229 (t \u2264 t ) = [a \u2229 (s \u2264 t )] \u2229 (t \u2264 t ). we have\na \u2229 (s \u2264 t ) \u2208 ft because a \u2208 fs, while (t \u2264 t ) \u2208 ft because t is a stopping time.\ntherefore a \u2229 (t \u2264 t ) \u2208 ft, which proves (2).\nfor (3), if a \u2208 ft+, then a \u2208 ft+\u03b5 for every \u03b5, and so a \u2229 (t + \u03b5 \u2264 t ) \u2208 ft for all t.\nhence a \u2229 (t \u2264 t \u2212 \u03b5) \u2208 ft for all t, or equivalently a \u2229 (t \u2264 t ) \u2208 ft+\u03b5 for all t. this is\ntrue for all \u03b5, so a \u2229 (t \u2264 t ) \u2208 ft+ = ft. this says a \u2208 ft .\n(4) define tn by (3.2). note\n\n\u2208 b) \u2229 (tn = k/2n) = (xk/2n \u2208 b) \u2229 (tn = k/2n) \u2208 fk/2n .\n\n(xtn\n\nsince tn only takes values in {k/2n : k \u2265 0}, we conclude (xtn\n(xtn\n\n\u2208 b) \u2208 ftn\n\n\u2282 ft+1/2n.\n\n\u2208 b) \u2229 (tn \u2264 t ) \u2208 ft and so\n\n "}, {"Page_number": 35, "text": "3.5 convergence and regularity\n\n17\n\nhence xtn is ft+1/2n measurable. if n \u2265 m, then xtn is measurable with respect to ft+1/2n \u2282\nft+1/2m. since xtn\n\u2192 xt , then xt is ft+1/2m measurable for each m. therefore xt is\nmeasurable with respect to ft+ = ft .\n\n3.4 the optional stopping theorem\n\nwe will need doob\u2019s optional stopping theorem for continuous-time martingales. an example\nto keep in mind is mt = wt\u2227t0, where w is a brownian motion and t0 is some fixed time.\nexercise 3.12 is a version of the optional stopping time with slightly weaker hypotheses that\nis often useful.\ntheorem 3.11 let {ft} be a filtration satisfying the usual conditions. if mt is a martingale\n< \u221e, and\nor non-negative submartingale whose paths are right continuous, supt\u22650\nt is a finite stopping time, then e mt \u2265 e m0.\nproof we do the submartingale case, the martingale case being very similar. by doob\u2019s\ninequality (theorem 3.6(1)),\n\ne m 2\nt\n\ns ] \u2264 4e m 2\nm 2\n\ne [sup\ns\u2264t\nt ] < \u221e by fatou\u2019s lemma.\n\n.\n\nt\n\nletting t \u2192 \u221e, we have e [supt\u22650 m 2\nlet us first suppose that t < k, a.s., for some real number k. define tn by (3.2). let\n= mk/2n, g (n)\n= fk/2n, and sn = 2ntn. by doob\u2019s optional stopping theorem applied to\nn (n)\nthe submartingale n (n)\n\n, we have\n\nk\n\nk\n\nk\n\ne m0 = e n (n)\n\n0\n\n\u2264 e n (n)\n\nsn\n\n.\n\n= e mtn\n\u2192 e mt .\n\n\u2192 mt , a.s. the random variables |mtn\n\n| are bounded by\nsince m is right continuous, mtn\n1 + supt\u22650 m 2\nwe apply the above to the stopping time t\u2227k to get e mt\u2227k \u2265 e m0. the random variables\nmt\u2227k are bounded by 1 + supt\u22650 m 2\nt , so by dominated convergence, we get e mt \u2265 e m0\nwhen we let k \u2192 \u221e.\n\nt , so by dominated convergence, e mtn\n\n3.5 convergence and regularity\n\nwe present the continuous-time version of doob\u2019s martingale convergence theorem. we will\nsee that not only do we get limits as t \u2192 \u221e, but also a regularity result.\nlet dn = {k/2n : k \u2265 0}, d = \u222andn.\ntheorem 3.12 let {mt : t \u2208 d} be either a martingale, a submartingale, or a supermartin-\ngale with respect to {ft : t \u2208 d} and suppose supt\u2208d e|mt| < \u221e. then\n(1) limt\u2192\u221e mt exists, a.s.\n(2) with probability one mt has left and right limits along d.\nthe second conclusion says that except for a null set, if t0 \u2208 [0,\u221e), then both limt\u2208d,t\u2191t0 mt\nand limt\u2208d,t\u2193t0 mt exist and are finite. the null set does not depend on t0.\nproof martingales are also submartingales and if mt is a supermartingale, then \u2212mt is a\nsubmartingale, so we may without loss of generality restrict our attention to submartingales.\n\n "}, {"Page_number": 36, "text": "18\n\nmartingales\n\nby doob\u2019s inequality (theorem 3.6(1)),\n\np( sup\n\nt\u2208dn,t\u2264n\n\n|mt| > \u03bb) \u2264 1\n\ne|mn|.\n\n\u03bb\n\nletting n \u2192 \u221e and using fatou\u2019s lemma,\n\np(sup\nt\u2208d\n\n|mt| > \u03bb) \u2264 1\n\ne|mt|.\n\nsup\n\nt\n\n\u03bb\n\nthis is true for all \u03bb, so with probability one, {|mt| : t \u2208 d} is a bounded set.\ntherefore the only way either (1) or (2) can fail is that if for some pair of rationals a < b the\nnumber of upcrossings of [a, b] by {mt : t \u2208 d} is infinite. recall that we define upcrossings\nas follows.\ngiven an interval [a, b] and a submartingale m, if s1 = inf{t : mt \u2264 a}, ti = inf{t > si :\nmt \u2265 b}, and si+1 = inf{t > ti : mt \u2264 a}, then the number of upcrossings up to time u is\nsup{k : tk \u2264 u}.\ndoob\u2019s upcrossing lemma (theorem a.34) tells us that if vn is the number of upcrossings\nby {mt : t \u2208 dn \u2229 [0, n]}, then\n\ne vn \u2264 e|mn|\nb \u2212 a\n\n.\n\nletting n \u2192 \u221e and using fatou\u2019s lemma, the number of upcrossings of [a, b] by {mt : t \u2208 d}\nhas finite expectation, hence is finite, a.s. if na,b is the null set where the number of upcrossings\nof [a, b] by {mt : t \u2208 d} is infinite and n = \u222aa<b,a,b\u2208q+na,b, where q+ is the collection of\nnon-negative rationals, then p(n ) = 0. if \u03c9 /\u2208 n, then (1) and (2) hold.\n\nas a corollary we have\ncorollary 3.13 let {ft} be a filtration satisfying the usual conditions, and let mt be a\nmartingale with respect to {ft}. then m has a version that is also a martingale and that in\naddition has paths that are right continuous with left limits.\nproof let d be as in the above proof. for each integer n \u2265 1, e|mt| \u2264 e|mn| < \u221e\nfor t \u2264 n since |mt| is a submartingale by the conditional expectation form of jensen\u2019s\ninequality (proposition a.21). therefore mt\u2227n has left and right limits when taking limits\nalong t \u2208 d. since n is arbitrary, mt has left and right limits when taking limits along t \u2208 d,\nexcept for a set of \u03c9\u2019s that form a null set. let\n\n(cid:14)mt =\nit is clear that (cid:14)m has paths that are right continuous with left limits. since ft+ = ft and (cid:14)mt\nis ft+ measurable, then (cid:14)mt is ft measurable.\n\nlet n be fixed. we will show {mt; t \u2264 n} is a uniformly integrable family of random\nvariables; see section a.4. let \u03b5 > 0. since mn is integrable, there exists \u03b4 such that\nif p(a) < \u03b4, then e [|mn|; a] < \u03b5. if l is large enough, p(|mt| > l) \u2264 e|mt|/l \u2264\ne|mn|/l < \u03b4. then\n\nu\u2208d,u>t,u\u2192t\n\nmu.\n\nlim\n\ne [|mt|;|mt| > l] \u2264 e [|mn|;|mt| > l] < \u03b5,\n\nsince |mt| is a submartingale and (|mt| > l) \u2208 ft. uniform integrability is proved.\n\n "}, {"Page_number": 37, "text": "3.5 convergence and regularity\n\n19\n\nnow let t < n. if b \u2208 ft,\n\ne [(cid:14)mt; b] =\n\nlim\n\nu\u2208d,u>t,u\u2192t\n\ne [mu; b] = e [mt; b].\n\nhere we used the vitali convergence theorem (theorem a.19) and the fact that mt is a\n\nmartingale. since (cid:14)mt is ft measurable, this proves that (cid:14)mt = mt, a.s. since n was arbitrary,\ncontinuous with left limits. that (cid:14)mt is a martingale is easy.\n\nwe have this for all t. we thus have found a version of m that has paths that are right\n\nthe following technical result will be used several times in this book. a function f is\nincreasing if s < t implies f (s) \u2264 f (t ). a process at has increasing paths if the function\nt \u2192 at (\u03c9) is increasing for almost every \u03c9.\nproposition 3.14 suppose {ft} is a filtration satisfying the usual conditions and suppose at\nis an adapted process with paths that are increasing, are right continuous with left limits,\nand a\u221e = limt\u2192\u221e at exists, a.s. suppose x is a non-negative integrable random variable,\nand mt is a version of the martingale e [x | ft] which has paths that are right continuous\nwith left limits. suppose e [x a\u221e] < \u221e. then\nx das = e\n\n(cid:3) \u221e\n\n(3.4)\n\ne\n\nproof first suppose x and a are bounded. let n > 1 and write e\n\n0 x das as\n\n(cid:15) \u221e\n\n0\n\n0\n\nms das.\n\ne [x (ak/2n \u2212 a(k\u22121)/2n )].\n\n(cid:3) \u221e\n\u221e(cid:9)\n(cid:16) \u221e(cid:9)\ne [x | fk/2n](ak/2n \u2212 a(k\u22121)/2n )\n(cid:3) \u221e\n\n(cid:3) \u221e\n\nk=1\n\nk=1\n\n(cid:17)\n\n.\n\ne\n\ngiven s and n, define sn to be that value of k/2n such that (k \u2212 1)/2n < s \u2264 k/2n. we then\nhave\n\n(3.5)\n\u2192 ms. since\nfor any value of s, sn\u2193 s as n \u2192 \u221e, and since m has right-continuous paths, msn\nx is bounded, so is m. by dominated convergence, the right-hand side of (3.5) converges to\n\nmsn das.\n\n0\n\n0\n\ne\n\nx das = e\n(cid:3) \u221e\n\nthis completes the proof when x and a are bounded. we apply this to x \u2227 n and a \u2227 n, let\nn \u2192 \u221e, and use monotone convergence for the general case.\n\n0\n\ne\n\nms das.\n\nthe only reason we assume x is non-negative is so that the integrals make sense. the\n\nequation (3.4) can be rewritten as\n\nx das = e\n\ne [x | fs] das.\n\n(3.6)\n\n(cid:3) \u221e\n\ne\n\n0\n\n(cid:3) \u221e\n\n0\n\nconditioning the kth summand on fk/2n, this is equal to\n\n "}, {"Page_number": 38, "text": "20\n\nwe also have\n\n(cid:3)\n\ne\n\n0\n\nmartingales\n\n(cid:3)\n\n0\n\nt\n\nx das = e\n\nt\n\ne [x | fs] das\n\n(3.7)\n\nfor each t. this follows either by following the above proof or by applying proposition 3.14\nto as\u2227t.\n\nthe following estimates are very useful.\n\n3.6 some applications of martingales\n\nproposition 3.15 if wt is a brownian motion, then\n\u2212\u03bb2/2t ,\n\nws \u2265 \u03bb) \u2264 e\n\np(sup\ns\u2264t\n\n\u03bb > 0,\n\n(3.8)\n\nand\n\n|ws| \u2265 \u03bb) \u2264 2e\n\n\u2212\u03bb2/2t ,\n\n(3.9)\nproof for any a the process {eawt} is a submartingale. to see this, since x \u2192 eax is convex,\nthe conditional expectation form of jensen\u2019s inequality (proposition a.21) implies\n\np(sup\ns\u2264t\n\n\u03bb > 0.\n\ne [eawt | fs] \u2265 eae [wt|fs] = eaws .\n\nby doob\u2019s inequality (theorem 3.6(1)),\n\nws \u2265 \u03bb) = p(sup\ns\u2264t\n\neaws \u2265 ea\u03bb) \u2264 e eawt\nea\u03bb\n\np(sup\ns\u2264t\n\n(3.10)\nsince e eay = ea2var y/2 if y is gaussian with mean 0 by (a.6), it follows that the right side\n\u2212a\u03bbea2t/2. if we now set a = \u03bb/t, we obtain (3.8). inequality (3.9)\nof (3.10) is bounded by e\nfollows by applying (3.8) to w and to \u2212w and adding.\n\n.\n\nlet us use martingales to calculate some probabilities. let us suppose a, b > 0 and set\nt = inf{t > 0 : wt = \u2212a or wt = b}, the first time brownian motion exits the interval\n[\u2212a, b]. by proposition 3.9, t is a stopping time.\n\nwe have\n\nproposition 3.16 let w be a brownian motion, let t = inf{t > 0 : wt /\u2208 [\u2212a, b]}, and let\na, b > 0. then\n\np(wt = \u2212a) = b\na + b\n\n,\n\np(wt = b) = a\na + b\n\n,\n\n(3.11)\n\nand\n\ne t = ab.\n\n(3.12)\n\u2212 t is a martingale with w0 = 0, it is easy to check that for each u,\nproof since w 2\n= e [u\u2227 t ].\nt\nw 2\nt\u2227u\nas u \u2192 \u221e, the right-hand side tends to e t by monotone convergence. |wu\u2227t|2 is bounded\n\n\u2212 (t \u2227 u) is also a martingale. applying theorem 3.11, we see that e w 2\nu\u2227t\n\n "}, {"Page_number": 39, "text": "3.6 some applications of martingales\n\nby (a + b)2, so by dominated convergence the left-hand side tends to e w 2\nu \u2192 \u221e. therefore\n\nt\n\ne t = e w 2\n\n.\n\nt\n\n21\n\u2264 (a + b)2 as\n\n(3.13)\n\nin particular, e t < \u221e, so we know t < \u221e, a.s.\n\nwe use that t is finite, a.s., to conclude that p(wt \u2208 {\u2212a, b}) = 1, or\n\n1 = p(wt = \u2212a) + p(wt = b).\n\n(3.14)\nsince wt is a martingale, then so is wt\u2227u for each u, and therefore e wu\u2227t = 0. letting\nu \u2192 \u221e and using dominated convergence (noting |wu\u2227t| is bounded by a + b), we have\ne wt = 0, or\n\n(3.15)\nwe get (3.11) by solving (3.14) and (3.15) for the unknowns p(wt = \u2212a) and p(wt = b).\n\n0 = (\u2212a)p(wt = \u2212a) + bp(wt = b).\n\nwe get (3.12) by( 3.13), writing\n\ne t = e w 2\n\nt\n\n= (\u2212a)2p(wt = \u2212a) + b2p(wt = b),\n\nand substituting the values from (3.11).\n\nin proving proposition 3.16, we used the fact that wt\u2227t is a martingale and p(t < \u221e) = 1.\n\nthe same proof shows\ncorollary 3.17 suppose mt is a martingale with continuous paths and with m0 = 0, a.s.,\nt = inf{t \u2265 0 : mt /\u2208 [\u2212a, b]}, and t < \u221e, a.s. then\n\np(mt = \u2212a) = b\na + b\n\n,\n\np(mt = b) = a\na + b\n\n.\n\nwe can also use martingales to get more subtle results. suppose r > 0. since erwt\u2212r2t/2 is\n\na martingale, as above\n\nthe exponent is bounded by rb if r > 0, so we can let t \u2192 \u221e and use dominated convergence\nto get\n\ne erwt\u2227t\u2212r2 (t\u2227t )/2 = 1.\n\ne erwt \u2212r2t /2 = 1.\n\nthis can be written as\n\n\u2212rae [e\ne\n\n\u2212r2t /2; wt = \u2212a] + erbe [e\n\n\u2212r2t /2; wt = b] = 1.\n\nsince e\n\n\u2212rwt\u2212r2t/2 is also a martingale, similar reasoning gives us\n\n\u2212rbe [e\n\n\u2212r2t /2; wt = b] = 1.\n\nerae [e\n\n\u2212r2t /2; wt = \u2212a] + e\n(cid:17)\n(cid:16)\n(cid:16)\n\nwe can solve those two equations to obtain\n\u2212r2t /2; wt = \u2212a\ne\n(cid:17)\n\u2212r2t /2; wt = b\ne\n\nand\n\ne\n\ne\n\n=\n\nerb \u2212 e\n\n\u2212rb\n\ner(a+b) \u2212 e\u2212r(a+b)\nera \u2212 e\n\n\u2212ra\n\ner(a+b) \u2212 e\u2212r(a+b)\n\n.\n\n=\n\n(3.16)\n\n(3.17)\n\n "}, {"Page_number": 40, "text": "martingales\n\n22\nthe left-hand sides of (3.16) and (3.17) are the laplace transforms of the quantities p(t \u2208\ndt; wt = \u2212a)/dt and p(t \u2208 dt; wt = b)/dt, respectively, and finding the inverse laplace\ntransforms of the right-hand sides of (3.16) and (3.17) gives us formulas for p(t \u2208 dt; wt =\n\u2212a)/dt and p(t \u2208 dt; wt = b)/dt. if we add the two formulas, we get an expression for\np(t \u2208 dt )/dt, and integrating over t from 0 to t0 gives an expression for p(t \u2264 t0).\n\nwe sketch how to invert the laplace transform and leave the detailed calculations and\njustification for inverting a laplace transform term by term to the interested reader. see also\nkaratzas and shreve (1991), section 2.8. the right-hand side of (3.16) is equal to\n\n\u2212ra \u2212 e\n\u2212ra\u22122rb\ne\n1 \u2212 e\u22122r(a+b)\n\u221e(cid:9)\n\n(1 \u2212 x)\u22121 =\n(cid:17)\n\nn=0\n\n.\n\nxn\n\nsince e\n\n\u22122r(a+b) < 1, we can use\n\nto expand the denominator as a power series; if we set \u03bb = r2/2, then\n\n\u221a\n\n\u221a\n2\u03bba\u22122n\n\n2\u03bbb \u2212 e\n\n\u2212(2n+1)\n\n\u221a\n\n\u221a\n2\u03bba\u2212(2n+2)\n\n(cid:11)\n\n2\u03bbb\n\n.\n\n(3.18)\n\n(cid:16)\n\ne\n\n(cid:10)\n\n\u221e(cid:9)\n\u2212\u03bbt; wt = \u2212a\ne\n=\n\u2212(2n+1)\ne\n\nn=0\n\nwe then use the fact that the laplace transform of\n\n\u221a\nk\n\u03c0t3\n\n2\n\n\u2212k2/4t\ne\n\n\u221a\n\u2212k\n\u03bb to find the inverse laplace transform of the right-hand side of (3.18) by inverting\nsimilarly (see exercises 3.15 and 3.16), if b > 0, w is a brownian motion, and s =\n\nis e\nterm by term.\ninf{t > 0 : wt = b}, then e e\n\n2\u03bbb. inverting the laplace transform,\n\n\u2212\u03bbs = e\n\n\u2212\u221a\n\np(s \u2208 dt ) = b\u221a\n\n2\u03c0t3\n\n\u2212b2/2t ,\ne\n\nt \u2265 0.\n\n(3.19)\n\n3.1\n\nif w is a brownian motion, show that\n\n\u2212 3\n\nw 3\nt\n\nexercises\n\n(cid:3)\n\nt\n\n0\n\nws ds\n\nis a martingale.\nsuppose {ft} is a filtration satisfying the usual conditions. show that if mt is a submartingale\nand e mt = e m0 for all t, then m is a martingale.\nlet x be a submartingale. show that supt\u22650\nprove all parts of proposition 3.8.\n\ne|xt| < \u221e if and only if supt\u22650\n\n< \u221e.\n\ne x\nt\n\n+\n\n3.2\n\n3.3\n\n3.4\n\n "}, {"Page_number": 41, "text": "3.5\n\n3.6\n\n3.7\n\n3.8\n\n3.9\n\nexercises\n\n23\n\nif tn is defined by (3.2), show tn is a stopping time for each n and tn \u2193 t .\nthis exercise gives an alternate definition of ft which is more appealing, but not as useful.\nsuppose that {ft} satisfies the usual conditions. show that ft is equal to the \u03c3 -field generated\nby the collection of random variables yt such that y is a bounded process with paths that are\nright continuous with left limits and y is adapted to the filtration {ft}.\nsuppose {ft} is a filtration satisfying the usual conditions. show that if t is a stopping time,\nthen t is ft measurable.\nsuppose {ft} is a filtration satisfying the usual conditions and t is a stopping time. show that\nif s is a ft measurable random variable with s \u2265 t , then s is a stopping time.\nthis exercise demonstrates that the conclusion of corollary 3.13 cannot be extended to sub-\nmartingales. find a filtration {ft} satisfying the usual conditions and a submartingale x with\nrespect to {ft} such that x does not have a version with paths that are right continuous with left\nlimits.\n\n3.10 suppose {ft} is a filtration satisfying the usual conditions. show that if s and t are stopping\n\ntimes and x is a bounded f\u221e measurable random variable, then\ne [e [x | fs] | ft ] = e [x | fs\u2227t ].\n\nhint: let yt = e [x | ft] and zt = yt\u2227s. show the left-hand side is equal to ys\u2227t .\n\n3.11 a martingale or submartingale mt is uniformly integrable if the family{mt : t \u2265 0} is a uniformly\nintegrable family of random variables. show that if mt is a uniformly integrable martingale with\npaths that are right continuous with left limits, then {mt; t a finite stopping time} is a uniformly\nintegrable family of random variables. show this also holds if mt is a non-negative submartingale\nwith paths that are right continuous with left limits.\n\n3.12 this exercise weakens the conditions on the optional stopping theorem. show that if mt is a\nuniformly integrable martingale that is right continuous with left limits and t is a finite stopping\ntime, then e mt = e m0.\n\n3.13 let w be a brownian motion and let t be a stopping time with e t < \u221e. prove that e wt = 0\n= e t . this is not an easy application of the optional stopping theorem because we\n\nand e w 2\nt\ndo not know that wt\u2227t is necessarily a uniformly integrable martingale.\n\n) is a d-dimensional brownian motion. show that if i (cid:16)= j, then\n\n3.14 suppose that (w 1\nt\n\n, . . . , w d\nt\n\nt w j\nw i\nt\n\nis a martingale.\n\n3.15 let wt be a brownian motion, b > 0, and t = inf{t > 0 : wt = b}. show t < \u221e, a.s. show\n\ne t = \u221e.\n\nhint: take a limit in (3.11).\n\n3.16 suppose w is a brownian motion and b > 0. if s = inf{t > 0 : wt = b}, show that the laplace\n\ntransform of the density of s is given by\n\n\u2212\u03bbs = e\n\ne e\n\n\u2212\u221a\n\n2\u03bbb.\n\n3.17 let wt be a brownian motion. show that if \u03b1 > 1/2, then\n\nlim\nt\u2192\u221e\n\nwt\nt \u03b1\n\n= 0,\n\na.s.\n\n "}, {"Page_number": 42, "text": "24\n\nmartingales\n\nhint: let \u03b10 \u2208 (1/2, \u03b1), estimate\n\np(\n\nsup\n\n2n\u2264s\u22642n+1\n\n|ws| \u2265 (2n )\u03b10 )\n\nusing (3.9), and then use the borel\u2013cantelli lemma.\n\n3.18 let wt be a one-dimensional brownian motion and \u03b1 \u2208 (0, 1/2]. prove that\n\n|wt|\nt \u03b1\n\nlim sup\nt\u2192\u221e\n\n> 0,\n\na.s.\n\n3.19 if w is a brownian motion and b is a constant, then the process xt = wt + bt is a brownian\n\nmotion with drift. prove that if b > 0, then\n\nt\u2192\u221e xt = \u221e,\n\nlim\n\na.s.\n\n "}, {"Page_number": 43, "text": "4\n\nmarkov properties of brownian motion\n\nin later chapters we will discuss extensively the markov property and strong markov property.\nthe brownian motion case is much simpler, and we do that now.\n\n4.1 markov properties\n\nlet us begin with the markov property.\ntheorem 4.1 let {f t} be a filtration, not necessarily satisfying the usual conditions, and\nlet w be a brownian motion with respect to {ft}. if u is a fixed time, then yt = wt+u \u2212 wu is\na brownian motion independent of fu.\nproof let gt = ft+u. it is clear that y has continuous paths, is zero at time 0, and is\nadapted to {gt}. since yt \u2212 ys = wt+u \u2212 ws+u, then yt \u2212 ys is a mean zero normal random\nvariable with variance (t + u) \u2212 (s + u) = t \u2212 s that is independent of fs+u = gs.\nthe strong markov property is the markov property extended by replacing fixed times u\n\nby finite stopping times.\ntheorem 4.2 let {ft} be a filtration, not necessarily satisfying the usual conditions, and let\nw be a brownian motion adapted to {ft}. if t is a finite stopping time, then yt = wt+t \u2212wt\nis a brownian motion independent of ft .\nproof we will first show that whenever m \u2265 1, t1 < \u00b7\u00b7\u00b7 < tm, f is a bounded continuous\nfunction on rm, and a \u2208 ft , then\n\ne [ f (yt1\n\n, . . . , ytm\n\n); a] = e [ f (wt1\n\n, . . . , wtm\n\n)] p(a).\n\nonce we have done this, we will then show how (4.1) implies our theorem.\n\nto prove (4.1), define tn by (3.2). we have\n\u2212 wtn\n\ne [ f (wtn+t1\n\n\u2212 wtn\n\n); a]\n\n, . . . , wtn+tm\n\u2212 wtn\n\ne [ f (wtn+t1\n\n, . . . , wtn+tm\n\n\u2212 wtn\n\n); a, tn = k/2n]\n\n(4.1)\n\n(4.2)\n\ne [ f (wt1+k/2n \u2212 wk/2n , . . . , wtm+k/2n \u2212 wk/2n ); a, tn = k/2n].\n\n\u221e(cid:9)\n\u221e(cid:9)\n\nk=1\n\nk=1\n\n=\n\n=\n\nfollowing the usual practice in probability that \u201c,\u201d means \u201cand,\u201d we use the notation\n\u201ce [\u00b7\u00b7\u00b7 ; a, tn = k/2n]\u201d as an abbreviation for \u201ce [\u00b7\u00b7\u00b7 ; a \u2229 (tn = k/2n)].\u201d since a \u2208 ft ,\n\n25\n\n "}, {"Page_number": 44, "text": "26\n\nmarkov properties of brownian motion\n\nthen a \u2229 (tn = k/2n) = a \u2229 ((t < k/2n) \\ (t < (k \u2212 1)/2n)) \u2208 fk/2n. we use the\nindependent increments property of brownian motion and the fact that wt \u2212 ws has the same\nlaw as wt\u2212s to see that the sum in the last line of (4.2) is equal to\n\n\u221e(cid:9)\n\nk=1\n\ne [ f (wt1+k/2n \u2212 wk/2n , . . . , wtm+k/2n \u2212 wk/2n )] p(a, tn = k/2n)\n\u221e(cid:9)\n=\n= e [ f (wt1\n\n)] p(a, tn = k/2n)\n\n, . . . , wtm\n\n, . . . , wtm\n\ne [ f (wt1\n\n)] p(a),\n\nk=1\n\nwhich is the right-hand side of (4.1). thus\n\n\u2212 wtn\n\n, . . . wtn+tm\n\ne [ f (wtn+t1\n\n(4.3)\nnow let n \u2192 \u221e. by the right continuity of the paths of w, the boundedness and continuity\nof f , and the dominated convergence theorem, the left-hand side of (4.3) converges to the\nleft-hand side of (4.1).\n\n, . . . wtm\n\n)] p(a).\n\n\u2212 wtn\n\n); a] = e [ f (wt1\n\nif we take a = \u0001 in (4.1), we obtain\n, . . . , ytm\n\ne [ f (yt1\n\n)] = e [ f (wt1\n\n, . . . , wtm\n\n)]\n\nwhenever m \u2265 1, t1, . . . , tm \u2208 [0,\u221e), and f\nis a bounded continuous function on rm.\nthis implies that the finite-dimensional distributions of y and w are the same. since y has\ncontinuous paths, y is a brownian motion.\nnext take a \u2208 ft . by using a limit argument, (4.1) holds whenever f is the indicator of\na borel subset b of rd, or in other words,\n\np(y \u2208 b, a) = p(y \u2208 b)p(a)\n\n(4.4)\nwhenever b is a cylindrical set. let m be the collection of all borel subsets b of c[0,\u221e)\nfor which (4.4) holds. let c be the collection of all cylindrical subsets of c[0,\u221e). then we\nobserve that m is a monotone class containing c and c is an algebra of subsets of c[0,\u221e)\ngenerating the borel \u03c3 -field of c[0,\u221e). by the monotone class theorem (theorem b.2),\nm is equal to the borel \u03c3 -field on c[0,\u221e), and since (4.4) holds for all sets b \u2208 m, this\nestablishes the independence of y and ft .\nin the future, we will not put in the details for the arguments using the monotone class\n\ntheorem.\nobserve that what is needed for the above proof to work is not that w be a brownian\nmotion, but that the process w have right continuous paths and that wt \u2212 ws be independent\nof fs and have the same distribution as wt\u2212s. we therefore have the following corollary.\ncorollary 4.3 let {ft} be a filtration, not necessarily satisfying the usual conditions, and\nlet x be a process adapted to {ft}. suppose x has paths that are right continuous with left\nlimits and suppose xt \u2212 xs is independent of fs and has the same law as xt\u2212s whenever s < t.\nif t is a finite stopping time, then yt = xt+t \u2212 xt is a process that is independent of ft and\nx and y have the same law.\n\n "}, {"Page_number": 45, "text": "4.2 applications\n\n27\n\n2b\u2212x\n\nb\n\nx\n\nfigure 4.1 the reflection principle.\n\n4.2 applications\n\nthe first application is known as the reflection principle and allows us to get control of the\nmaximum of a brownian motion. the idea is the following. suppose that wt is a brownian\nmotion and for some path, the brownian motion goes above a level b before time t but that\nat time t the value of wt is less than x, where x < b. we could take the graph of this path\nand reflect it across the horizontal line at level b the first time the path crosses the level b\n(figure 4.1). this will give us a new path that ends up above 2b\u2212x. thus there is a one-to-one\ncorrespondence between paths where the maximum up to time t is above b and wt is below\nx and the paths where wt is above 2b \u2212 x.\nmore precisely, we have the following.\n\ntheorem 4.4 let wt be a brownian motion, b > 0, t = inf{t : wt \u2265 b}, and x < b.\nthen\n\nws \u2265 b, wt < x) = p(wt > 2b \u2212 x).\n\np(sup\ns\u2264t\n\nproof let tn be defined by (3.2). we first show that\n\np(tn \u2264 t, wt \u2212 wtn\n\n< x \u2212 b) = p(tn \u2264 t, wt \u2212 wtn\n\n> b \u2212 x).\n\n(4.5)\n\n(4.6)\n\n "}, {"Page_number": 46, "text": "= [2nt](cid:9)\n= [2nt](cid:9)\n\nk=0\n\nk=0\n\n[2nt](cid:9)\n\nk=0\n\n28\n\nmarkov properties of brownian motion\n\nwriting [x] for the integer part of x, the left-hand side of (4.6) is equal to\n\n[2nt](cid:9)\n\nk=0\n\np(tn = k/2n, wt \u2212 wtn\n\n< x \u2212 b)\n\np(tn = k/2n, wt \u2212 wk/2n < x \u2212 b)\n\np(tn = k/2n)p(wt \u2212 wtn\n\n< x \u2212 b),\n\nusing the independent increments property of brownian motion and the fact that we have\n(tn = k/2n) \u2208 fk/2n. using the symmetry of the normal distribution, that is, that wt \u2212 ws\nand ws \u2212 wt have the same law, this is the same as\n\np(tn = k/2n)p(wt \u2212 wtn\n\n> b \u2212 x),\n\nand reversing the steps above, this equals the right-hand side of (4.6).\nsince w has continuous paths, wt = b, so (t = t ) \u2282 (wt = b). because wt is a normal\nrandom variable, then p(t = t ) = 0. also, p(wt \u2212 wt = b \u2212 x) and p(wt \u2212 wt = x \u2212 b)\nare both zero. if we now let n \u2192 \u221e in (4.6), we obtain\n\np(t \u2264 t, wt \u2212 wt < x \u2212 b) = p(t \u2264 t, wt \u2212 wt > b \u2212 x).\n\nsince wt = b, this is the same as\n\np(t \u2264 t, wt < x) = p(t \u2264 t, wt > 2b \u2212 x).\n\n(4.7)\n\nby the definition of t and the continuity of the paths of w, the left-hand side is equal to the\nleft-hand side of (4.5). if wt > 2b \u2212 x, then automatically t \u2264 t, so the right-hand side of\n(4.7) is equal to the right-hand side of (4.5).\n\nour second application will be useful when studying local time in chapter 14.\nproposition 4.5 let wt be a brownian motion with respect to a filtration {ft} satisfying the\nusual conditions. let t be a finite stopping time and s > 0. if a < b, then\n\nproof\n\nif a \u2208 ft , let k > 0 and write\n\n.\n\n2\u03c0s\n\np(wt+s \u2208 [a, b] | ft ) \u2264 |b \u2212 a|\u221a\n\u221e(cid:9)\np(wt+s \u2208 [a, b], a)\n\u221e(cid:9)\n\nj=\u2212\u221e\n\n=\n\np(wt+s \u2208 [a, b], a, j/k \u2264 wt < ( j + 1)/k)\n\n\u2264\n\nj=\u2212\u221e\n\np(wt+s \u2212 wt \u2208 [a \u2212 ( j + 1)/k, b \u2212 j/k],\na, j/k \u2264 wt \u2264 ( j + 1)/k).\n\n "}, {"Page_number": 47, "text": "using the fact that wt+s \u2212 wt is a brownian motion independent of ft , this is less than or\nequal to\n\nexercises\n\n29\n\np(ws \u2208 [a \u2212 ( j + 1)/k, b \u2212 j/k]) p(a, j/k \u2264 wt \u2264 ( j + 1)/k)\n\n\u221e(cid:9)\n\nj=\u2212\u221e\n\np(a, j/k \u2264 wt \u2264 ( j + 1)/k)\n\n\u221e(cid:9)\n\nb \u2212 a + 1/k\n\n\u2264\n\nj=\u2212\u221e\n\u2264 1\u221a\n2\u03c0\n\n\u221a\n1\u221a\ns\n2\u03c0\nb \u2212 a + 1/k\n\u221a\ns\n\np(a).\n\nwe used here the formula for the density of a normal random variable with mean zero and\nvariance s. this is true for all k, so letting k \u2192 \u221e yields our result.\n\nif w is a brownian motion, let st = sups\u2264t ws. find the density for st.\n\nexercises\n\n4.1\n\n4.2 with w and s as in exercise 4.1, find the joint density of (st , wt ).\n\n4.3\n\nlet w be a brownian motion started at a > 0 and let t0 be the first time w hits 0. find the law\nof supt\u2264t0 wt.\n(0,\u221e)}, then\n\n4.4 use the reflection principle to prove that if w is a brownian motion and t = inf{t > 0 : wt \u2208\n\np(t = 0) = 1.\n\nin other words, brownian motion enters the interval (0,\u221e) immediately. by symmetry it enters\nthe interval (\u2212\u221e, 0) immediately. conclude that brownian motion hits 0 infinitely often in\nevery time interval [0, t].\nlet wt be a brownian motion and {ft} be the minimal augmented filtration generated by w . let\n\nt = inf{t > 0 : wt = sup\n0\u2264s\u22641\nshow that t is not a stopping time with respect to {ft}.\nlet w and s be as in exercise 4.1.\n\nws}.\n\n(1) let 0 < s < t < u and let a < b with b \u2212 a \u2264 1. show that there exists a constant c,\n\ndepending on s, t, and u, but not a or b, such that\n\np(ss \u2208 [a, b], sup\nt\u2264r\u2264u\n\nwr \u2208 [a, b]) \u2264 c(b \u2212 a)2.\n\n(2) show that the path of a brownian motion does not take on the same value as a\nlocal maximum twice. that is, if s and t are times when w has a local maximum, then\nws (cid:16)= wt , a.s.\nlet vt be the number of upcrossings of [0, 1] by a brownian motion w up to time t. this means\nwe let s1 = 0, ti = inf{t > si : wt \u2265 1}, and si+1 = inf{t > ti : wt \u2264 0} for i = 1, 2, . . . ,\nand we set vt = sup{k : tk \u2264 t}. show that vt \u2192 \u221e, a.s., as t \u2192 \u221e.\n\n4.5\n\n4.6\n\n4.7\n\n "}, {"Page_number": 48, "text": "30\n\nmarkov properties of brownian motion\n\n4.8\n\nlet w be a brownian motion. the zero set of brownian motion is the random set\n\nz(\u03c9) = {t \u2208 [0, 1] : wt (\u03c9) = 0}.\n\n(1) show that z(\u03c9) is a closed set for each \u03c9.\n(2) show that with probability one, every point of z(\u03c9) is a limit point of z(\u03c9). conclude\n\nthat z(\u03c9) is an uncountable set.\n\n4.9\n\nlet w be a one-dimensional brownian motion and \u03b4 > 0.\n\n(1) prove that there exists \u03b3 such that if t \u2264 \u03b3 , then\n\np(0 \u2264 wt \u2264 \u03b4/2) \u2265 1/4\n\nand\n\np(\u2212\u03b4/2 \u2264 wt \u2264 0) \u2265 1/4.\n\n(2) prove there exists \u03b3 such that\n\n|ws| > \u03b4/2) \u2264 1/8.\n\np(sup\ns\u2264\u03b3\n\n(3) prove that if m \u2265 1, then\n\np(\n\nsup\n\nm\u03b3\u2264s\u2264(m+1)\u03b3\n\n|ws \u2212 wm\u03b3| \u2264 \u03b4/2, wm\u03b3 \u2208 [0, \u03b4/2],|w(m+1)\u03b3| \u2264 \u03b4/2 | fm\u03b3 )\n\u2265 1\n\n|ws \u2212 wm\u03b3| \u2264 \u03b4/2, wm\u03b3 \u2208 [0, \u03b4/2])\n\nsup\n\np(\n\n8\n\nm\u03b3\u2264s\u2264(m+1)\u03b3\n\nand the same with wm\u03b3 \u2208 [\u2212\u03b4/2, 0] in place of wm\u03b3 \u2208 [0, \u03b4/2]. conclude that\n\np(\n\nsup\n\nm\u03b3\u2264s\u2264(m+1)\u03b3\n\n|ws \u2212 wm\u03b3| \u2264 \u03b4/2,|wm\u03b3| \u2264 \u03b4/2,|w(m+1)\u03b3| \u2264 \u03b4/2 | fm\u03b3 )\n\u2265 1\n\n|ws \u2212 wm\u03b3| \u2264 \u03b4/2,|wm\u03b3| \u2264 \u03b4/2).\n\nsup\n\np(\n\n8\n\nm\u03b3\u2264s\u2264(m+1)\u03b3\n\n(4) use induction to prove that if t0 > 0, there exists c1 > 0 such that\n\n|ws| \u2264 \u03b4) > c1.\n\np(sup\ns\u2264t0\n\n(5) prove that if w is a d-dimensional brownian motion, t0 > 0, and \u03b4 > 0, there exists c2\n\nsuch that\n\n|ws| \u2264 \u03b4) > c2.\n\np(sup\ns\u2264t0\n\n4.10 the p-variation of a function f on the interval [0, 1] is defined by\n\nv p( f ) = sup\n\n| f (ti+1 ) \u2212 f (ti )|p : n \u2265 1, 0 = t0 < t1,\u00b7\u00b7\u00b7 < tn = 1\n\n(cid:18) n\u22121(cid:9)\n\ni=0\n\n(cid:19)\n\n;\n\nthe supremum is over all partitions p of [0, 1]. in this exercise we will prove that if p < 2 and\nw is a brownian motion, then v p(w ) = \u221e, a.s.\n\n(1) let xi be an i.i.d. sequence of random variables with finite mean. use the strong law of\n\nlarge numbers to prove that if k > e x1, then\n\n(cid:10) n(cid:9)\n\n(cid:11)\n\n\u2192 0\n\nas n \u2192 \u221e.\n\nxi > kn\n\np\n\ni=1\n\n "}, {"Page_number": 49, "text": "exercises\n\n(2) if p < 2, take r \u2208 (p, 2), and let \u03b5n = n\nsi+1 = inf{t > si : |wt \u2212 wsi\n| > \u03b5n}. set xi = \u03b5\u22122\nwith finite mean.\n\nn\n\n(3) use (1) to show that\n\np(sn > 1) = p\n\nxi > \u03b5\u22122\n\nn\n\n\u2192 0\n\n(cid:10) n(cid:9)\n\ni=1\n\n31\n\u22121/r. let s0 = 0 and for i \u2265 0, set\n(si \u2212 si\u22121 ). prove that the xi are i.i.d.\n(cid:11)\n\nas n \u2192 \u221e.\n(4) using the partition {s0, s1, . . . , sn}, show that v p(w ) \u2265 n\u03b5 p\n(5) conclude v p(w ) = \u221e, a.s.\n\nn on the event (sn \u2264 1).\n\n "}, {"Page_number": 50, "text": "5\n\nthe poisson process\n\nat the opposite extreme from brownian motion is the poisson process. this is a process\nthat only changes value by means of jumps, and even then, the jumps are nicely spaced. the\npoisson process is the prototype of a pure jump process, and later we will see that it is the\nbuilding block for an important class of stochastic processes known as l\u00b4evy processes.\ndefinition 5.1 let {f t} be a filtration, not necessarily satisfying the usual conditions. a\npoisson process with parameter \u03bb > 0 is a stochastic process x satisfying the following\nproperties:\n\n(1) x0 = 0, a.s.\n(2) the paths of xt are right continuous with left limits.\n(3) if s < t, then xt \u2212 xs is a poisson random variable with parameter \u03bb(t \u2212 s).\n(4) if s < t, then xt \u2212 xs is independent of fs.\ndefine xt\u2212 = lims\u2192t,s<t xs, the left-hand limit at time t, and \u0001xt = xt \u2212 xt\u2212, the size of\nthe jump at time t. we say a function f is increasing if s < t implies f (s) \u2264 f (t ). we use\n\u201cstrictly increasing\u201d when s < t implies f (s) < f (t ). we have the following proposition.\n\nproposition 5.2 let x be a poisson process. with probability one, the paths of xt are\nincreasing and are constant except for jumps of size 1. there are only finitely many jumps in\neach finite time interval.\nproof for any fixed s < t, we have that xt \u2212 xs has the distribution of a poisson random\nvariable with parameter \u03bb(t \u2212 s), hence is non-negative, a.s.; let ns,t be the null set of \u03c9\u2019s\nwhere xt (\u03c9) < xs(\u03c9). the set of pairs (s, t ) with s and t rational is countable, and so\nn = \u222as,t\u2208q+ns,t is also a null set, where we write q+ for the non-negative rationals. for\n\u03c9 /\u2208 n, xt \u2265 xs whenever s < t are rational. in view of the right continuity of the paths of\nx , this shows the paths of x are increasing with probability one.\n\nsimilarly, since poisson random variables only take values in the non-negative integers, xt\nis a non-negative integer, a.s. using this fact for every t rational shows that with probability\none, xt takes values only in the non-negative integers when t is rational, and the right\ncontinuity of the paths implies this is also the case for all t. since the paths have left limits,\nthere can only be finitely many jumps in finite time.\n\nit remains to prove that \u0001xt is either 0 or 1 for all t. let t0 > 0. if there were a jump of\nsize 2 or larger at some time t strictly less than t0, then for each n sufficiently large there\n\n32\n\n "}, {"Page_number": 51, "text": "the poisson process\n\n33\n\nexists 0 \u2264 kn \u2264 2n such that x(kn+1)t0/2n \u2212 xknt0/2n \u2265 2. therefore\n\n(5.1)\n\np(\u2203 s < t0 : \u0001xs \u2265 2) \u2264 p(\u2203 k \u2264 2n : x(k+1)t0/2n \u2212 xkt0/2n \u2265 2)\n\n1 \u2212 e\n\np(x(k+1)t0/2n \u2212 xkt0/2n \u2265 2)\n\n\u2264 2n sup\nk\u22642n\n= 2np(xt0/2n \u2265 2n)\n(cid:10)\n(cid:11)\n\u2264 2n(1 \u2212 p(xt0/2n = 0) \u2212 p(xt0/2n = 1))\n= 2n\n\u2212\u03bbt0/2n\nwe used property 5.1(3) for the two equalities. by l\u2019h\u02c6opital\u2019s rule, (1 \u2212 e\n\u2212x)/x \u2192 0\n\u2212x \u2212 xe\nas x \u2192 0. we apply this with x = \u03bbt0/2n, and see that the last line of (5.1) tends to 0 as\nn \u2192 \u221e. since the left-hand side of (5.1) does not depend on n, it must be 0. this holds for\neach t0.\n\n\u2212\u03bbt0/2n \u2212 (\u03bbt0/2n)e\n\nanother characterization of the poisson process is as follows. let t1 = inf{t : \u0001xt = 1},\nthe time of the first jump. define ti+1 = inf{t > ti : \u0001xt = 1}, so that ti is the time of the\nith jump.\nproposition 5.3 the random variables t1, t2 \u2212 t1, . . . , ti+1 \u2212 ti, . . . are independent expo-\nnential random variables with parameter \u03bb.\n\n.\n\nin view of corollary 4.3 it suffices to show that t1 is an exponential random variable\nproof\nwith parameter \u03bb. if t1 > t, then the first jump has not occurred by time t, so xt is still zero.\nhence\n\np(t1 > t ) = p(xt = 0) = e\n\n\u2212\u03bbt ,\n\nusing the fact that xt is a poisson random variable with parameter \u03bbt.\n\nwe can reverse the characterization in proposition 5.3 to construct a poisson process. we\n\ndo one step of the construction, leaving the rest as exercise 5.4.\n\nlet u1, u2, . . . be independent exponential random variables with parameter \u03bb and let\n\ntj =(cid:12)\n\nj\ni=1 ui. define\n\nxt (\u03c9) = k\n\nif tk (\u03c9) \u2264 t < tk+1(\u03c9).\n\n(5.2)\n\nan examination of the densities shows that an exponential random variable has a gamma\ndistribution with parameters \u03bb and r = 1, so by proposition a.49, tj is a gamma random\nvariable with parameters \u03bb and j. thus\n\nperforming the integration by parts repeatedly shows that\n\np(xt < k) = p(tk > t ) =\n\n\u03bbe\n\n\u2212\u03bbx(\u03bbx)k\u22121\n\u0001(k)\n\ndx.\n\n(cid:3) \u221e\np(xt < k) = k\u22121(cid:9)\n\nt\n\n\u2212\u03bbt\ne\n\ni=0\n\n(\u03bbt )i\n\ni!\n\n,\n\nand so xt is a poisson random variable with parameter \u03bbt.\n\nwe will use the following proposition later.\n\n "}, {"Page_number": 52, "text": "34\n\nthe poisson process\n\nproposition 5.4 let {ft} be a filtration satisfying the usual conditions. suppose x0 = 0,\na.s., x has paths that are right continuous with left limits, xt \u2212 xs is independent of fs if\ns < t, and xt \u2212 xs has the same law as xt\u2212s whenever s < t. if the paths of x are piecewise\nconstant, increasing, all the jumps of x are of size 1, and x is not identically 0, then x is a\npoisson process.\nproof let t0 = 0 and ti+1 = inf{t > ti : \u0001xt = 1}, i = 1, 2, . . . we will show that if\nwe set ui = ti \u2212 ti\u22121, then the ui are i.i.d. exponential random variables and then appeal to\nexercise 5.4.\n\nby corollary 4.3, the ui are independent and have the same law. hence it suffices to show\n\nu1 is an exponential random variable. we observe\n\np(u1 > s + t ) = p(xs+t = 0) = p(xs+t \u2212 xs = 0, xs = 0)\n\n= p(xt+s \u2212 xs = 0)p(xs = 0) = p(xt = 0)p(xs = 0)\n= p(u1 > t )p(u1 > s).\n\nsetting f (t ) = p(u1 > t ), we thus have f (t + s) = f (t ) f (s). since f (t ) is decreasing\nand 0 < f (t ) < 1, we conclude p(u1 > t ) = f (t ) = e\n\u2212\u03bbt for some \u03bb > 0, or u1 is an\nexponential random variable.\n\n5.1\n\n5.2\n\n5.3\n\n5.4\n\n5.5\n\n5.6\n\nsuppose pt is a poisson process and we write xt = pt\u2212. is p1 \u2212 x1\u2212t a poisson process on [0, 1]?\nwhy or why not?\n\nexercises\n\nlet p be a poisson process with parameter \u03bb. show that\n\n(cid:20)(cid:20)(cid:20) = 0,\n\n\u2212 \u03bbt\n\n(cid:20)(cid:20)(cid:20) pnt\n\nn\n\nn\u2192\u221e sup\nlim\nt\u22641\n\na.s.\n\nshow that if p(1) and p(2) are independent poisson processes with parameters \u03bb1 and \u03bb2,\nrespectively, then p(1)\n\nis a poisson process with parameter \u03bb1 + \u03bb2.\n\n+ p(2)\n\nt\n\nt\n\nif x is defined by (5.2), show that x is a poisson process.\nlet xt be a stochastic process and let {f 00\na poisson process with respect to the filtration {f 00\nrespect to the minimal augmented filtration generated by x .\n\nt\n\nt\n\n} be the filtration generated by x . suppose x is\n}. show that x is a poisson process with\n\nhint: imitate the proof of proposition 2.5.\n\nsuppose pt is a poisson process and f and g are non-negative bounded deterministic functions\nf (s) dps\nwith compact support. find necessary and sufficient conditions on f and g so that\nand\n\n(cid:15) \u221e\nhint: first show that the characteristic function of f =(cid:15) \u221e\n\n0 g(s) dps are independent.\n\nf (s) dps is\n\n0\n\n(cid:15) \u221e\n\n(cid:10)(cid:3) \u221e\n\n0\n\n0\n\n(cid:11)\n\ne eiuf = exp\n\n(eiu f (s) \u2212 1) ds\n\n.\n\n "}, {"Page_number": 53, "text": "exercises\n\n35\n\n5.7 we will talk about weak convergence in general metric spaces in chapters 30\u201335. this ex-\nercise is concerned with the weak convergence of real-valued random variables as defined in\nsection a.12.\nsuppose for each n, pn is a poisson random variable with parameter \u03bbn and \u03bbn \u2192 \u221e as\nn \u2192 \u221e. prove that\n\npn \u2212 \u03bbn\u221a\n\n\u03bbn\n\nconverges weakly to a normal random variable with mean zero and variance one.\n\nhint: imitate the proof of theorem a.51.\n\n "}, {"Page_number": 54, "text": "6\n\nconstruction of brownian motion\n\nthere are several ways of constructing brownian motion, none of them easy. here we give\ntwo constructions. the first is the one that wiener used, which is based on fourier series.\nthe second uses martingale techniques. a method due to l\u00b4evy can be found in bass (1995);\nsee also exercises 6.4 and 6.5. we will see several other constructions in later chapters.\n\n6.1 wiener\u2019s construction\n\nfor any of the constructions of brownian motion, the main step is to construct wt for\nt \u2208 [0, 1]. once we have done this, we get brownian motion for all t rather easily. more\nspecifically, suppose we have a brownian motion y (0) started at 0 on the time interval [0, 1].\n= 0 for each i, and now\ntake independent copies y (1), y (2), . . . , each on [0, 1]. we have y (i)\n+y (1)\n0\nto get brownian motion started at 0, define wt to be equal to y (0)\nt\u22121\nif 1 < t \u2264 2, and more generally\n\nif t \u2264 1, equal to y (0)\n\n1\n\nt\n\n(cid:11)\n\n(cid:10) [t]\u22121(cid:9)\n\ni=0\n\nwt =\n\ny (i)\n1\n\n+ y [t]\nt\u2212[t]\n\nif t \u2265 1, where [t] is the largest integer less than or equal to t. this will give brownian motion\nstarted at 0 on the time interval [0,\u221e).\n\ntherefore the crux of the problem is to construct brownian motion on [0, 1]. because we\nare working with fourier series, it is more convenient to look at brownian motion on [0, \u03c0];\nwe can just disregard times between 1 and \u03c0 when we are done.\n\nthroughout this chapter we make the supposition that we can find a countable sequence\nz1, z2, . . . of independent and identically distributed mean zero normal random variables\nwith variance one that are f measurable, where (\u0001,f , p) is our probability space. this is\nan extremely mild condition.\ntheorem 6.1 there exists a process {wt; 0 \u2264 t \u2264 1} that is brownian motion.\n\nif we fix t \u2208 [0, \u03c0] and compute the fourier series for the function f (s) = s \u2227 t, it\n\nproof\nis an exercise in calculus to get the fourier coefficients. we end up with\n\ns \u2227 t = st\n\n\u03c0\n\n+ 2\n\u03c0\n\nsin ks sin kt\n\nk=1\n\nk2\n\n.\n\n(6.1)\n\n\u221e(cid:9)\n\n36\n\n "}, {"Page_number": 55, "text": "6.1 wiener\u2019s construction\n\n37\n\nthis suggests letting z0, z1, . . . be i.i.d. normal random variables with mean 0 and variance\n1 and setting\n\nzk.\n\n(6.2)\n\n(cid:10)(cid:21)\n\n\u221e(cid:9)\n\nk=1\n\n(cid:11)\n\n2\n\u03c0\n\nsin kt\n\nk\n\nwt = t\u221a\n\n\u03c0\n\nz0 +\n\u221e(cid:9)\n\nassuming there is no problem with convergence, we see that wt has mean zero, since each\nof the zi does, and that\n\ne [wswt] = st\n\n+\n\nsin ks sin kt\n\n= s \u2227 t\n\n2\n\u03c0\n\n(cid:12)\nas required. we used the independence of the zi here to show that e [ziz j] = 0 if i (cid:16)= j.\nm\nk=1\n\nwe argue that there is in fact no difficulty with the convergence. note that\n\nk=1\n\nk2\n\n\u03c0\n\n(6.3)\n\nsin2 kt\n\nk2\n\nincreases as m increases to a finite limit. therefore\n\n(cid:16)(cid:10) n(cid:9)\n\ne\n\nzk\n\nsin kt\n\nk\n\nk=m\n\n(cid:11)2(cid:17)\n\n= n(cid:9)\n\nk=m\n\nsin2 kt\n\nk2\n\n\u2192 0\n\nin l2 as m, n \u2192 \u221e. this means that the sum on the right of (6.2) is a cauchy sequence in\nl2. by the completeness of l2, the sum on the right of (6.2) converges in l2. a use of the\ncauchy\u2013schwarz inequality allows us to justify the formula for the expectation of wswt.\n\nif we let\n\nt = t\u221a\nw j\n\n\u03c0\n\n(cid:10)(cid:21)\n\nz0 + j(cid:9)\n\nk=1\n\n(cid:11)\n\n2\n\u03c0\n\nsin kt\n\nk\n\nzk,\n\n, . . . , w j\ntm\n\nthen (w j\n) is a jointly normal collection of random variables for each j whenever\nt1, . . . , tn \u2208 [0, \u03c0]. by remark a.56, it follows that (wt1\nt1\n) is a jointly normal\ncollection of random variables. therefore wt is a gaussian process. since each wt has mean\nzero and cov (ws, wt ) = s \u2227 t, then wt has the correct finite-dimensional distributions to be\na brownian motion.\n\n, . . . , wtm\n\nthe only part remaining to the construction is to show that wt as constructed above has\ncontinuous paths, for we can then use theorem 2.4. in what follows, pay attention to where\nthe absolute values are placed. if one is cavalier about placing them, one will very likely run\ninto trouble.\n\ndefine\n\nsm(t ) = 2m\u22121(cid:9)\n(cid:21)\n\nk=m\n\nz0 +\n\n\u03c0\n\nsin kt\n\nk\n\nzk\n\n\u221e(cid:9)\n\nn=0\n\n2\n\u03c0\n\ns2n (t ).\n\nand let tm = sup0\u2264t\u2264\u03c0 |sm(t )|. we write\nwt = t\u221a\n\nwe will show\n\ne t 2\nm\n\n\u2264 c\nm1/2\n\n.\n\n(6.4)\n\n "}, {"Page_number": 56, "text": "38\n\nconstruction of brownian motion\n\nonce we have this, then by the fubini theorem and then jensen\u2019s inequality,\n\n\u221e(cid:9)\n\n\u221e(cid:9)\n\n\u221e(cid:9)\n\n(cid:10)\n\n(cid:11)1/2\n\n< \u221e.\n\ne t2n \u2264\n\ne\n\nn=0\n\nn=0\n\nn=0\n\ne [t 2\n2n]\n\n(cid:12)\u221e\nn=0 t2n < \u221e, a.s., and by the weierstrass m-test (see, e.g., rudin, 1976), we\ntherefore\nn=0 s2n (t ) converges uniformly in t. since each s2n (t ) is a\nhave that with probability 1,\ncontinuous function of t, we see that the uniform limit is also continuous and we are done.\nj,k aka j for ak complex valued, we\n\nwe therefore have to prove (6.4). using |(cid:12)\n(cid:20)(cid:20)(cid:20)2\n\nhave\n\nt 2\nm\n\nt2n =\n(cid:12)\u221e\n(cid:20)(cid:20)(cid:20) 2m\u22121(cid:9)\n(cid:20)(cid:20)(cid:20) 2m\u22121(cid:9)\n\nk=m\n\nj,k=m\n\n\u2264 sup\n0\u2264t\u2264\u03c0\n\u2264 sup\n0\u2264t\u2264\u03c0\n\nk\n\nk=m\n\n1\nk2 z2\n\n\u2264 2m\u22121(cid:9)\n\u2264 2m\u22121(cid:9)\n1\nk2 z2\n2m\u22121(cid:9)\n\nk=m\n\nk\n\n=\n\nzk\n\n\u2212i jt\n\nz jzk\n\nk ak|2 =(cid:12)\n(cid:20)(cid:20)(cid:20)\n(cid:20)(cid:20)(cid:20) m\u22121(cid:9)\n2m\u2212(cid:14)\u22121(cid:9)\n(cid:20)(cid:20)(cid:20) 2m\u2212(cid:14)\u22121(cid:9)\n(cid:9)\n\nj=m\n\nj=m\n\n(cid:14)=1\n\n1\n\nj( j + (cid:14))\n(cid:9)\n\n(cid:14)=1\n\n+ 2\n\neikt\nk\n\neikte\njk\n\n+ 2 sup\nm\u22121(cid:9)\n0\u2264t\u2264\u03c0\n\n+ 2\n\n2m\u22121(cid:9)\n\nk=m\n\n1\nk2\n\n\u2264 c\nm\n\n.\n\n(cid:20)(cid:20)(cid:20)\n\n(6.5)\n\nei(cid:14)t\nj( j + (cid:14))\n\nz jz j+(cid:14)\n\n(cid:20)(cid:20)(cid:20).\n\nz jz j+(cid:14)\n\nin the third inequality we wrote\n\n(cid:12)\nand then set (cid:14) = k \u2212 j. write i for the first sum on the last line of (6.5) and j(cid:14) for\n\nm\u2264 j=k\u22642m\u22121\n\nm\u2264 j<k\u22642m\u22121\n\nj,k=m\n\n2m\u2212(cid:14)\u22121\nj=m\n\n1\n\nj( j+(cid:14)) z jz j+(cid:14). the expectation of i is equal to\n\n,\n\nwe next look at the expectation of the j(cid:14). since the zi are mean zero and independent,\ne [zi1zi2zi3zi4] is zero unless either all four subscripts are equal or else two subscripts are\nequal and the other two subscripts are also equal. by jensen\u2019s inequality,\n\ne|j(cid:14)| \u2264\n\n=\n\n(cid:17)2(cid:11)1/2\n\nj( j + (cid:14))\n\nz jz j+(cid:14)\n\n1\n\ne\n\nj=m\n\n(cid:10)\n(cid:16) 2m\u2212(cid:14)\u22121(cid:9)\n(cid:10) 2m\u2212(cid:14)\u22121(cid:9)\n(cid:10)(cid:9) z jz j+(cid:14)\n\nj=m\n\n1\n\nj2( j + (cid:14))2\n\nj( j + (cid:14))\n\n(cid:11)1/2\n(cid:11)2\n\nthe last equality follows by multiplying out\n\n.\n\n(6.6)\n\n "}, {"Page_number": 57, "text": "6.2 martingale methods\n\n39\nand noting that expectations of the cross-product terms are zero. since j \u2265 m in the last\nline of (6.6) and there are at most m terms in the sum, the last line of (6.6) is bounded by\n(cm/m4)1/2 = cm\n\n\u22123/2. therefore\n\nm\u22121(cid:9)\n\ne\n\n(cid:14)=1\n\n|j(cid:14)| \u2264 c/m1/2.\n\nsubstituting in (6.5) completes the proof of (6.4).\n\nby proposition 2.5, the brownian motion that we constructed is a brownian motion with\n\nrespect to the minimal augmented filtration.\n\n6.2 martingale methods\n\nhere, we use martingale methods to take care of the continuity of the paths. we proceed\nas in the previous section to construct {wt; 0 \u2264 t \u2264 \u03c0}, where wt is a gaussian process\nwith e wt = 0 and cov (ws, wt ) = s \u2227 t, and we need to show that w has a version with\ncontinuous paths. we show that w is a martingale, and so has a version with paths that are\nright continuous with left limits. we use doob\u2019s inequalities to control the oscillation of w\nover short time intervals, and then use the borel\u2013cantelli lemma to show continuity.\ntheorem 6.2 if {wt; t \u2264 1} is a gaussian process with e wt = 0 for all t \u2264 1 and\ncov (ws, wt ) = s \u2227 t for all s, t \u2264 1, then there is a version of w that is a brownian motion\non [0, 1].\n\nproof as in the proof of theorem 6.1, we need to show that w has a version with continuous\npaths. since cov (wt \u2212 ws, wr ) = r \u2212 r = 0 if r \u2264 s < t, we see by proposition a.55 that\nwt \u2212 ws is independent of f 00\n\n= \u03c3 (wr; r \u2264 s). then\n\ns\n\ne [wt \u2212 ws | f 00\n\ns ] = e [wt \u2212 ws] = 0,\n\nt\n\n(cid:3)\n\n(cid:3)\nt\n(cid:3)\n\nso wt is a martingale. by theorem 3.12, with probability one, w has left and right limits\nalong d, the dyadic rationals. let w\n= limu>t,u\u2208d,u\u2192t wu. since e (wu \u2212 wt )2 = u\u2212 t \u2192 0\nas u \u2192 t, then w\n= wt, a.s., or w\nis a version of w with paths that are right continuous\nwith left limits. we now drop the primes. set wt = w1 if t \u2265 1.\n\u2212 wt0 is also a martingale, and by jensen\u2019s inequality for\nfor any t0 \u2208 [0, 1], wt+t0\nconditional expectations (proposition a.21), |wt+t0\n|4 is a submartingale. using doob\u2019s\n\u2212 wt0\ninequalities (theorem 3.6), if \u03bb > 0 and t0, \u03b4 \u2208 [0, 1],\nsup\n\n| \u2265 \u03bb) = p(\n\n|4 \u2265 \u03bb4)\n\n|wt \u2212 wt0\n\nsup\n\np(\n\nt0\u2264t\u2264t0+\u03b4\n\n|wt \u2212 wt0\n|4\n\nt0\u2264t\u2264t0+\u03b4\ne|wt0+\u03b4 \u2212 wt0\n\n.\n\n\u03bb4\n\n\u2264 c\n\nsince wt0+\u03b4 \u2212 wt0 is a mean zero normal random variable with variance \u03b4 if t0 + \u03b4 \u2264 1, we\nhave\n\np(\n\nsup\n\nt0\u2264t\u2264t0+\u03b4\n\n|wt \u2212 wt0\n\n| \u2265 \u03bb) \u2264 c\n\n\u03b42\n\n\u03bb4\n\n.\n\n(6.7)\n\n "}, {"Page_number": 58, "text": "40\n\nlet\n\nfrom (6.7) with \u03b4 = 2\n\nconstruction of brownian motion\n\nan = {\u2203 k \u2264 2n :\n\u2212n+1 and \u03bb = 2\np(an) \u2264 2n max\nk\u22642n\n\u22122n\n\u2264 c2n2\n2\u2212n/2\n\np(\n\nsup\n\nk/2n\u2264t\u2264(k+2)/2n\n\u2212n/8,\n\nsup\n\nk/2n\u2264t\u2264(k+2)/2n\n= c2\n\n\u2212n/2,\n\n|wt \u2212 wk/2n| > 2\n\n\u2212n/8}.\n\n|wt \u2212 wk/2n| > 2\n\n\u2212n/8)\n\nwhich is summable. by the borel\u2013cantelli lemma, p(an i.o.) = 0. (the event (an i.o.) is the\nevent where \u03c9 is in infinitely many of the an.)\non \u03c9) such that if n \u2265 n, then \u03c9 /\u2208 an. given \u03b5 > 0, take n \u2265 n such that 2\n|t \u2212 s| \u2264 2\n\nexcept for a set of \u03c9\u2019s in a null set, there exists a positive integer n (which will depend\n\u2212n/8 < \u03b5/2. if\n\u2212n with s, t \u2208 [0, 1], then s, t \u2208 [k/2n, (k + 2)/2n] for some k \u2264 2n. since \u03c9 /\u2208 an,\n\n|wt \u2212 ws| \u2264 |wt \u2212 wk/2n| + |ws \u2212 wk/2n| \u2264 2 \u00b7 2\n\n\u2212n/8 < \u03b5.\n\nthis proves the continuity of wt.\n\nlet (cid:22) f , g(cid:23) =(cid:15)\n\nthere is nothing special about the trigonometric polynomials in this second construction.\n1\n0 f (r)g(r) dr be the inner product for the hilbert space l2[0, 1]; we consider\nonly real-valued functions for simplicity. let {\u03d5n} be a complete orthonormal system for\nl2[0, 1]: we have (cid:22)\u03d5m, \u03d5n(cid:23) = 0 if m (cid:16)= n, (cid:22)\u03d5n, \u03d5n(cid:23) = 1 for each n, and f = 0, a.e., if\n(cid:22) f , \u03d5n(cid:23) = 0 for all n. one property of a complete orthonormal system is parseval\u2019s identity,\nwhich says that\n\n\u221e(cid:9)\n\nn=1\n\n\u221e(cid:9)\n\nn=1\n\n(cid:22) f , f (cid:23) =\n\n|(cid:22) f , \u03d5n(cid:23)|2;\n\nsee folland (1999). if we replace f by g and then by f + g and use\n2 [(cid:22) f + g, f + g(cid:23) \u2212 (cid:22) f , f (cid:23) \u2212 (cid:22)g, g(cid:23)],\n\n(cid:22) f , g(cid:23) = 1\n\nwe obtain\n\nnow let\n\n(cid:22) f , g(cid:23) =\n\n(cid:22) f , \u03d5n(cid:23)(cid:22)g, \u03d5n(cid:23).\n\n(cid:3)\n\n0\n\nt\n\n\u03d5n(r) dr.\n\nan(t ) = (cid:22)1[0,t], \u03d5n(cid:23) =\n\u221e(cid:9)\n\nwt =\n\nif z1, z2, . . . are independent mean zero normal random variables with variance one, let\n\nan(t )zk.\n\nn=1\n\n(6.8)\n\n "}, {"Page_number": 59, "text": "exercises\n\n41\n\nassuming there is no difficulty with the convergence, we have\n\ncov (ws, wt ) =\n\n(cid:22)1[0,s], \u03d5n(cid:23)(cid:22)1[0,t], \u03d5n(cid:23)\n\n\u221e(cid:9)\n\u221e(cid:9)\nn=1\n= (cid:22)1[0,s], 1[0,t](cid:23) = s \u2227 t.\n\nan(s)an(t ) =\n\nn=1\n\nexercise 6.2 asks you to verify that the process w defined by (6.8) is a mean zero gaussian\nprocess on [0, 1] with the same covariances as a brownian motion.\n\nexercises\n\nlet z0, z1, z2, . . . be a sequence of independent identically distributed mean zero normal random\nvariables with variance one. define\n\n(cid:10)(cid:21)\n\n\u221e(cid:9)\n\nk=1\n\n(cid:11)\n\nxt = t2\n\u221a\n\u03c0\n2\n\nz0 +\n\n2\n\u03c0\n\ncos kt\n\nk2\n\nzk.\n\n(6.9)\n\n(1) show that the convergence in (6.9) is absolute and uniform over t \u2208 [0, 1].\n(2) show that xt is a gaussian process.\n(3) if wt is a brownian motion and\nyt =\n\nt \u2208 [0, 1],\n\nwr dr,\n\n(cid:3)\n\nt\n\n0\n\nshow that x and y have the same finite-dimensional distributions. show that x and y have\nthe same law when viewed as random variables taking values in c[0, 1]. (the process x is\nsometimes known as integrated brownian motion.)\n\n(4) find cov (xs, xt ).\nlet {\u03d5n} be a complete orthonormal system for l2[0, 1]. show that the sum (6.8) converges in\nl2 and give the details of the proof that the resulting process w is a mean zero gaussian process\nwith cov (ws, wt ) = s \u2227 t if s, t \u2208 [0, 1].\nlet d = {k/2n : n \u2265 1, k = 0, 1, . . . , 2n} be the dyadic rationals. suppose the collection of\nrandom variables {vt : t \u2208 d} is jointly normal, each vt has mean zero, and cov (vs, vt ) = s\u2227 t.\n(1) prove that the paths of v are uniformly continuous over t \u2208 d.\n(2) if we define wt = lims\u2208d,s\u2192t vs, prove that w is a brownian motion.\nin this and the next exercise we give the haar function construction of brownian motion. let\n\u03d500 = 1 on [0, 1] and for i = 1, 2, . . ., and 1 \u2264 j \u2264 2i\u22121, set\n\n\u23a7\u23aa\u23a8\u23aa\u23a92(i\u22121)/2,\n\n\u22122(i\u22121)/2,\n0,\n\n\u03d5i j (x) =\n\n(2 j \u2212 2)/2i \u2264 x < (2 j \u2212 1)/2i,\n(2 j \u2212 1)/2i \u2264 x < 2 j/2i,\notherwise.\n\nit is a well-known and easily proved result from analysis (see, e.g., bass (1995), section i.2)\nthat the collection {\u03d5i j} is a complete orthonormal system for l2[0, 1].\nfor each i, j, define\n\n(cid:3)\n\n\u03c8i j (t ) =\n\nt\n\n0\n\n\u03d5i j (s) ds,\n\n6.1\n\n6.2\n\n6.3\n\n6.4\n\n "}, {"Page_number": 60, "text": "42\n\nconstruction of brownian motion\n\nfor each i and j, let yi j be independent mean zero normal random variables with variance one,\nand let\n\nvi(t ) = 2i\u22121(cid:9)\n\nj=1\n\nyi j\u03d5i j (t )\n\nfor i \u2265 1. set v0 = y00\u03d500.\n(1) fix i \u2265 1. prove that each \u03c8i j is bounded by 2(\u2212i\u22121)/2. prove that the sets {t : \u03c8i j (t ) > 0},\nj = 1, . . . , 2i\u22121, are disjoint.\n(2) fix i \u2265 1. write\n\n\u22122 ) \u2264 p(\u2203 j \u2264 2i\u22121 : |yi j|2(\u2212i\u22121)/2 > i\n\np(\u2203 t \u2208 [0, 1] : |vi(t )| > i\n\u221e(cid:9)\n\nuse proposition a.52 to estimate this, and conclude that\n|vi(t )| > i\n\u221e(cid:9)\n\np( sup\n0\u2264t\u22641\n\ni=1\n\n\u22122 ) < \u221e.\n\n6.5\n\nthis is a continuation of exercise 6.4. with \u03d5i j, \u03c8i j, yi j, and vi as in that problem, let\n\n\u22122 ),\n\n(6.10)\n\n(1) prove that w is a jointly normal gaussian process with mean zero and cov (ws, wt ) = s\u2227t.\n|vi(t )| converges uniformly\n(2) use (6.10) and the borel\u2013cantelli lemma to show that\n\nn\ni=1\n\nover [0, 1]. conclude that w is a brownian motion.\n\nwt =\n\nvi(t ).\n\ni=0\n\n(cid:12)\n\n "}, {"Page_number": 61, "text": "7\n\npath properties of brownian motion\n\nthe paths of brownian motion are continuous, but we will see that they are not differentiable.\nhow continuous are they? we will see that the paths satisfy what is known as a h\u00a8older\ncontinuity condition. a precise description of the oscillatory behavior of brownian motion\nwill be given by the law of the iterated logarithm.\na function f : [0, 1] \u2192 r is said to be h\u00a8older continuous of order \u03b1 if there exists a\n\nconstant m such that\n\n| f (t ) \u2212 f (s)| \u2264 m|t \u2212 s|\u03b1,\n\ns, t \u2208 [0, 1].\n\n(7.1)\n\nwe show that the paths of brownian motion are h\u00a8older continuous of order \u03b1 if \u03b1 < 1\n2 .\n(they are also not h\u00a8older continuous of order \u03b1 if \u03b1 \u2265 1\n2 ; we will see this from the law of\nthe iterated logarithm.)\n\ntheorem 7.1 if \u03b1 < 1\non [0, 1].\n\n2 , the paths of brownian motion are h\u00a8older continuous of order \u03b1\n\nstep 1. first we apply the borel\u2013cantelli lemma to a certain sequence of sets. let w\n\nproof\nbe a brownian motion and set\n\nan = {\u2203 k \u2264 2n \u2212 1 :\n\nsup\n\nk/2n\u2264t\u2264(k+1)/2n\n\n|wt \u2212 wk/2n| > 2\n\n\u2212n\u03b1}.\n\np(an) \u2264 2n sup\nk\u22642n\n\nsince wt+k/2n \u2212 wk/2n is a brownian motion,\np( sup\nt\u22641/2n\n|wt| > 2\n\n\u2264 2np( sup\nt\u22641/2n\n\u2264 2 \u00b7 2n exp(\u22122\nhere we used proposition 3.15. since \u03b1 < 1\nof (7.2) is less than\n\n|wt+k/2n \u2212 wk/2n| > 2\n\n\u2212n\u03b1 )\n\n\u2212n\u03b1 )\n\u22122n\u03b1/2(2\n2 , then 2n(1\u22122\u03b1) > 2n for n large, and the last line\n\n\u2212n)).\n\n(7.2)\n\n(cid:12)\n\n2n+1 exp(\u22122n(1\u22122\u03b1)/2) \u2264 2n+1e\n\u2212n\n\np(an) < \u221e, and p(an i.o.) = 0 by the borel\u2013cantelli lemma.\n\nif n is large. hence\nstep 2. next we show that this implies the h\u00a8older continuity. for almost every \u03c9\nthere exists n (depending on \u03c9) such that if n \u2265 n, then \u03c9 /\u2208 an. let s \u2264 t be two points\n\u2212(n+1) for some n \u2265 n and k is the largest integer such that\nin [0, 1]. if 2\n\n\u2212(n+2) \u2264 t \u2212 s \u2264 2\n\n43\n\n "}, {"Page_number": 62, "text": "44\nk/2n+2 \u2264 s, then\n\npath properties of brownian motion\n\n|wt \u2212 ws| \u2264 |wt \u2212 wt\u2227((k+1)/2n+2 )| + |wt\u2227((k+1)/2n+2 ) \u2212 wk/2n+2|\n\n+ |ws \u2212 wk/2n+2|\n\u2212n\u03b1 \u2264 3 \u00b7 4\u03b1|t \u2212 s|\u03b1.\n\n\u2264 3 \u00b7 2\n\nwe know |wt (\u03c9)| is bounded on [0, 1] since the paths are continuous; let k (depending\n\non \u03c9) be the bound. if |t \u2212 s| \u2265 2\n\n\u2212(n+1), then\n\n|wt \u2212 ws| \u2264 2k \u2264 (2k )(2n+1)|t \u2212 s| \u2264 (2k )(2n+1)|t \u2212 s|\u03b1.\n\nthus, no matter whether |t \u2212 s| is small or large, there exists l (depending on \u03c9) such that\n|wt (\u03c9) \u2212 ws(\u03c9)| \u2264 l|t \u2212 s|\u03b1 for all s, t \u2208 [0, 1].\n\none of the most beautiful theorems in probability theory is the law of the iterated logarithm\n\n(lil). it describes precisely how brownian motion oscillates.\n\ntheorem 7.2 let w be a brownian motion. we have\n= 1,\n\n|wt|\n2t log log t\n\nlim sup\nt\u2192\u221e\n\na.s.\n\nand\n\nlim sup\n\nt\u21920\n\n|wt|\n\n2t log log(1/t )\n\n= 1,\n\na.s.\n\n(cid:22)\n(cid:22)\n\nproof the second assertion follows from the first by time inversion; see exercise 2.5. thus\nwe only need to prove the first assertion.\nlarger than 1 but close enough to 1 so that (1 + \u03b5)2/q > 1. let\n\nproof of upper bound: we use the borel\u2013cantelli lemma. let \u03b5 > 0 and then choose q\n\n|ws| > (1 + \u03b5)\n\n2qn\u22121 log log qn\u22121).\n\nan = (sup\ns\u2264qn\n(cid:10)\n(cid:10)\n\nby proposition 3.15,\n\np(an) \u2264 2 exp\n= 2 exp\n\n\u2212 (1 + \u03b5)22qn\u22121 log log qn\u22121\n\u2212 (1 + \u03b5)2\n\n2qn\n(log(n \u2212 1) + log log q)\n(cid:12)\n\nq\n\n(cid:11)\n\n=\n\nc\n\n(n \u2212 1)(1+\u03b5)2/q\n\n,\n\nwhere we are using our convention that the letter c denotes a constant whose exact value is\nunimportant. this is summable in n, so\nby the borel\u2013cantelli lemma, p(an i.o.) = 0. hence, except for a null set, there exists\nn = n (\u03c9) such that \u03c9 /\u2208 an if n \u2265 n (\u03c9). if t \u2265 qn , then for some n \u2265 n + 1 we have\nqn\u22121 \u2264 t \u2264 qn, and\n\np(an) < \u221e.\n\n(cid:22)\n2qn\u22121 log log qn\u22121 \u2264 (1 + \u03b5)\n\n(cid:22)\n\n2t log log t.\n\n|wt| \u2264 sup\ns\u2264qn\n\n|ws| \u2264 (1 + \u03b5)\n\n(cid:22)\n\n(cid:11)\n\n "}, {"Page_number": 63, "text": "therefore\n\npath properties of brownian motion\n\n(cid:22)\n\n|wt|\n2t log log t\n\nlim sup\nt\u2192\u221e\n\n\u2264 1 + \u03b5,\n\na.s.\n\n45\n\n(7.3)\n\nsince \u03b5 > 0 is arbitrary, the upper bound is proved.\n\nproof of lower bound: we start with the second half of the borel\u2013cantelli lemma. let\n\n\u03b5 > 0 and then take q > 1 very large so that\n\n\u221a\n\nand 2/\n\nq < \u03b5/2. this is possible because (1 \u2212 \u03b5)2(1 + \u03b5) = (1 \u2212 \u03b52)(1 \u2212 \u03b5) < 1. let\n\nbn = (wqn+1 \u2212 wqn > (1 \u2212 \u03b5)\n\n2qn+1 log log qn+1).\n\n1 \u2212 q\u22121\n\n(1 \u2212 \u03b5)2(1 + \u03b5)\n(cid:22)\n\n< 1\n\nsince brownian motion has independent increments, the events bn are independent. let\n\nthen z is a mean zero normal random variable with variance one. by proposition a.52, we\nsee that\n\np(bn) = p(z > (1 \u2212 \u03b5)\n\n(cid:10)\n\n\u2265 exp\n= c exp\n\n.\n\n(cid:22)\nz = wqn+1 \u2212 wqn\nqn+1 \u2212 qn\n(cid:22)\n\n2qn+1 log log qn+1/\n\n(cid:22)\n\u2212 (1 \u2212 \u03b5)2(1 + \u03b5)2qn+1 log log qn+1\n(cid:10)\n\u2212 (1 \u2212 \u03b5)2(1 + \u03b5)\n(cid:9)\n\n2(qn+1 \u2212 qn)\n\n1 \u2212 q\u22121\n\n1\n\n(n + 1)(1\u2212\u03b5)2 (1+\u03b5)/(1\u2212q\u22121 )\n\nn\n\n(cid:11)\nqn+1 \u2212 qn)\n\n(cid:11)\n\nlog(n + 1) + log log q\n\n= \u221e.\n\nfor n large. hence (cid:9)\n\np(bn) \u2265 c\n\nby the borel\u2013cantelli lemma, with probability one, \u03c9 is in infinitely many bn. conse-\n\nquently, with probability one, infinitely often\nwqn+1 \u2212 wqn > (1 \u2212 \u03b5)\n\n2qn+1 log log qn+1.\n\n(7.4)\n\nthe inequality (7.4) is not exactly what we want, as we want a lower bound for wqn+1, but\nwe can derive the desired lower bound by using the upper bound we proved in step 1. we\nknow from (7.3) that for n large enough,\n2qn log log qn \u2264 2\u221a\nq\n\n2qn+1 log log qn+1 <\n\n2qn+1 log log qn+1.\n\n|wqn| \u2264 2\n\n(cid:22)\n\n(cid:22)\n\n(cid:22)\n\n\u03b5\n2\n\nthus infinitely often\n\nwqn+1 > (1 \u2212 3\u03b5/2)\n\n2qn+1 log log qn+1.\n\n(cid:22)\n\n(cid:22)\n\n "}, {"Page_number": 64, "text": "46\n\nthis proves\n\npath properties of brownian motion\n\n(cid:22)\n\nlim sup\nn\u2192\u221e\n\nwqn+1\n\n2qn+1 log log qn+1\n\n\u2265 1 \u2212 3\u03b5\n2\n\n,\n\na.s.\n\nsince \u03b5 is arbitrary, the lower bound follows.\n\nthe law of the iterated logarithm show that the paths of wt are not differentiable at time 0,\na.s. applying this to ws+t \u2212 wt, we see that for each t, w is not differentiable at time t, a.s.\nbut the null set nt might depend on t, and it is even conceivable that \u222at\u2208[0,1]nt is not a null\nset. we have the following stronger result, which says that except for a set of \u03c9\u2019s that form a\nnull set, t \u2192 wt (\u03c9) is a function that does not have a derivative at any time t \u2208 [0, 1].\ntheorem 7.3 with probability one, the paths of brownian motion are nowhere differentiable.\n\nproof note that if z is a normal random variable with mean 0 and variance 1, then\n\n(cid:3)\n\np(|z| \u2264 r) = 1\u221a\n2\u03c0\n\n\u2212r\n\nr\n\n\u2212x2/2 dx \u2264 2r.\ne\n\n(7.5)\n\nlet m, h > 0 and let\n\nam,h = {\u2203s \u2208 [0, 1] : |wt \u2212 ws| \u2264 m|t \u2212 s| if |t \u2212 s| \u2264 h},\nbn = {\u2203k \u2264 2n : |wk/n \u2212 w(k\u22121)/n| \u2264 4m/n,\n\n|w(k+1)/n \u2212 wk/n| \u2264 4m/n,|w(k+2)/n \u2212 w(k+1)/n| \u2264 4m/n}.\n\nwe check that am,h \u2282 bn if n \u2265 2/h. to see this, if \u03c9 \u2208 am,h, there exists an s such that\n|wt \u2212ws| \u2264 m|t \u2212 s| if |t \u2212 s| \u2264 2/n; let k/n be the largest multiple of 1/n less than or equal\nto s. then\n\n|(k + 2)/n \u2212 s| \u2264 2/n\n\nand\n\n|(k + 1)/n \u2212 s| \u2264 2/n,\n\nand therefore\n\n|w(k+2)/n \u2212 w(k+1)/n| \u2264 |w(k+2)/n \u2212 ws| + |ws \u2212 w(k+1)/n|\n\n\u2264 2m/n + 2m/n < 4m/n.\n\nsimilarly |w(k+1)/n \u2212 wk/n| and |wk/n \u2212 w(k\u22121)/n| are less than 4m/n.\n\nusing the independent increments property, the stationary increments property, and (7.5),\n\np(bn) \u2264 2n sup\nk\u22642n\n\n\u2264 2np(|w1/n| < 4m/n,|w2/n \u2212 w1/n| < 4m/n,\n= 2np(|w1/n| < 4m/n)p(|w2/n \u2212 w1/n| < 4m/n)\n\np(|wk/n \u2212 w(k\u22121)/n| < 4m/n,|w(k+1)/n \u2212 wk/n| < 4m/n,\n|w(k+2)/n \u2212 w(k+1)/n| < 4m/n)\n|w3/n \u2212 w2/n| < 4m/n)\n\u00d7 p(|w3/n \u2212 w2/n| < 4m/n)\n(cid:11)3\n\n= 2n(p(|w1/n| < 4m/n))3\n\u2264 cn\n\n(cid:10)\n\n,\n\n4m\u221a\nn\n\n "}, {"Page_number": 65, "text": "exercises\n\n47\n\nwhich tends to 0 as n \u2192 \u221e. hence for each m and h,\n\nthis implies that the probability that there exists s \u2264 1 such that\n\np(am,h) \u2264 lim sup\nn\u2192\u221e\n\np(bn) = 0.\n\n|ws+h \u2212 ws|\n\n|h|\n\n\u2264 m\n\nlim sup\n\nh\u21920\n\nis zero. since m is arbitrary, this proves the theorem.\n\nexercises\n\n7.1 here you are asked to find a more precise description of the modulus of continuity of brownian\n\npaths. prove that\n\nlim\n\u03b4\u21920\n\nsup\n\ns,t\u2208[0,1],0<|t\u2212s|<\u03b4\n\nhint: imitate the proof of theorem 7.1.\n\n(cid:22)\n|wt \u2212 ws|\n\u03b4 log(1/\u03b4)\n\n< \u221e,\n\na.s.\n\nthe following is part of what is known as chung\u2019s law of the iterated logarithm. we will see in\nsection 40.3 that there exists c1 such that\n\nfor t/\u03bb2 sufficiently large. prove that\n\nlim inf\nt\u2192\u221e\n\n\u2212\u03c0 2t/8\u03bb2\n\np(sup\ns\u2264t\n\n|ws| \u2264 \u03bb) \u2264 c1e\n(cid:22)\n\n|ws|\nsups\u2264t\nt/ log log t\n\n< \u221e,\n\na.s.\n\nlet wt be a one-dimensional brownian motion. we will see in section 40.3 that there exists c2\nsuch that\n\nif t/\u03bb2 is sufficiently large. prove that\n\nlim inf\nt\u2192\u221e\n\np(sup\ns\u2264t\n\n\u2212\u03c0 2t/8\u03bb2\n\n|ws| \u2264 \u03bb) \u2265 c2e\n(cid:22)\n(cid:22)\n\n|ws|\nsups\u2264t\nt/ log log t\n\n|ws|\nsups\u2264t\nt/ log log t\n\n= c,\n\n> 0,\n\na.s.\n\nthis is the other half of chung\u2019s law of the iterated logarithm. in fact,\n\nlim inf\nt\u2192\u221e\n\na.s.\n\n(7.6)\n\nidentify c and prove (7.6).\na function f is h\u00a8older continuous of order \u03b1 at a point t if there exists c such that| f (u)\u2212 f (t )| \u2264\nc|u \u2212 t|\u03b1 for all u. suppose \u03b1 > 1/2 and wt is a brownian motion. show that the event\n\na = {\u2203 t \u2208 [0, 1] : w is h\u00a8older continuous of order \u03b1 at t )\n\nhas probability 0.\n\nhint: imitate the proof of nowhere differentiability, but use more than three time intervals.\n\n7.2\n\n7.3\n\n7.4\n\n "}, {"Page_number": 66, "text": "48\n\n7.5\n\npath properties of brownian motion\n\nlet w be a one-dimensional brownian motion and let mt = sups\u2264t ws (with no absolute value\nsigns). prove that if \u03b5 > 0, then\n\nlim inf\nt\u2192\u221e\n\nmt\n\n\u221a\nt/(log t )1+\u03b5\n\n> 0,\n\na.s.\n\n7.6\n\nthis is a complement to exercise 4.10. prove that if p > 2 and w is a brownian motion, then\nthe p-variation of w , defined in exercise 4.10, is finite, a.s.\n\nhint: use the fact that the paths of brownian motion are h\u00a8older continuous of order \u03b1\n\n7.7\n\nif \u03b1 < 1/2.\nlet w be a brownian motion and let z be the zero set: z = {t \u2208 [0, 1] : wt = 0}.\n(1) show there exists a constant c not depending on x or \u03b4 such that\n|ws| \u2265 |x|) \u2264 ce\np(\u2203s \u2264 \u03b4 : ws = \u2212x) \u2264 p(sup\ns\u2264\u03b4\n(cid:10)\n(cid:11)\n\ndepending on s or t such that\n\n\u2212x2/2\u03b4.\n\n(cid:21)\n\n(2) use the markov property of brownian motion to show that there exists a constant c not\n\n7.8 given a borel measurable subset a of [0, 1], define\n\n1 \u2227\n\nt \u2212 s\ns\n\n.\n\np(z \u2229 [s, t] (cid:16)= \u2205) \u2264 c\n(cid:18) \u221e(cid:9)\n\n(cid:16)\n\nh\u03b3 (a) = lim sup\n\u03b4\u21920\n\ninf\n\ni=1\n\n[bi \u2212 ai]\u03b3 : a \u2282 \u222a\u221e\n\ni=1[ai, bi], sup\ni\n\n(cid:19)(cid:17)\n\n.\n\n|bi \u2212 ai| \u2264 \u03b4\n\nin other words, cover a by the union of intervals [ai, bi] and define the analog of lebesgue\nmeasure. the differences are that we look at |bi \u2212 ai|\u03b3 but do not require that \u03b3 be one, and we\nrequire that none of the intervals be longer than \u03b4. the quantity h\u03b3 (a) is called the hausdorff\nmeasure of a with respect to the function x\u03b3 . the hausdorff dimension of a set a is defined to\nbe\n\ninf{\u03b3 : h\u03b3 (a) > 0} = sup{\u03b3 : h\u03b3 (a) = \u221e}.\n\n(for subsets of rd, we replace the intervals [ai, bi] by balls of radius ri.) as a warm-up to this\nexercise, prove that the hausdorff dimension of the standard cantor set in [0, 1] is log 2/ log 3.\nthe purpose of this exercise is to show that if w is a brownian motion and z = {t \u2208 [0, 1] :\nwt = 0} is the zero set, then the hausdorff dimension of z is no more than 1/2.\n(1) for each n, let cn be the collection of intervals [i/2n, (i + 1)/2n] contained in [0, 1] that\nintersect z. (cn is random.) if #cn is the cardinality of cn, use exercise 7.7 to show\n\ne [ #cn] \u2264 2n\u22121(cid:9)\n\ni=0\n\np(z \u2229 [i/2n, (i + 1)/2n] (cid:16)= \u2205) \u2264 c2n/2.\n(cid:9)\n\n|2\n\n\u2212n|\u03b3 = 2\n\n\u2212n\u03b3 #cn.\n\n[i/2n,(i+1)/2n]\u2208cn\n\n(2) write\n\nuse the chebyshev inequality and (1) to conclude that the hausdorff dimension of z is less than\nor equal to 1/2, a.s. (we will show that it is at least 1/2 in exercise 14.10.)\n\n "}, {"Page_number": 67, "text": "8\n\nthe continuity of paths\n\nit is often important to know whether a stochastic path has continuous paths. an important\nsufficient condition is the kolmogorov continuity criterion. this criterion is also useful in\nshowing the continuity of a family of random variables x a in the variable a, where a is a\nparameter other than time. kolmogorov\u2019s continuity criterion is part (2) of theorem 8.1.\nlet dn = {k/2n : k \u2264 2n} and let d = \u222andn. the set d is known as the set of dyadic\nrationals in [0, 1]. we will use\n\n\u221e(cid:9)\n\n(cid:3) \u221e\n\n\u22122 \u2264 1 +\ni\n\n\u22122 dx = 2.\nx\n\n1\n\ni=1\n\n\u22122 is actually equal to \u03c0 2/6.)\n\n(cid:12)\u221e\n(in fact by a standard exercise using parseval\u2019s identity in the theory of fourier series,\ni=1 i\nwe will be considering at first a real-valued process {xt : t \u2208 d}. to show continuity by\nconsidering xt \u2212 xs for all pairs (s, t ) doesn\u2019t work \u2013 there are too many pairs. kolmogorov\u2019s\nproof circumvents this problem by considering only a restricted collection of pairs. to bound\nx15/32 \u2212 x11/32, for example, we compare x15/32 to x7/16, compare x7/16 to x3/8, and compare\nx3/8 to x1/4, and we also compare x11/32 to x5/16 and compare x5/16 to x1/4. the advantage\nof this complicated way of matching pairs is that each comparison, say, for example x3/8 to\nx1/4, is used for a great many of the possible pairs (s, t ).\n\nthe proof of theorem 8.1 has three main steps. step 1 is to reduce the problem to proving\nthe bound (8.3). the second step is to set up the comparisons that we need, and the third is\nto obtain estimates on all the comparisons.\ntheorem 8.1 suppose {xt : t \u2208 d} is a real-valued process and there exist c1, \u03b5, and p > 0\nsuch that\n\nthen the following hold.\n\n(1) there exists c2 depending only on c1, p, and \u03b5 such that for m > 0,\n\ns, t \u2208 d.\n\ne [|xt \u2212 xs|p] \u2264 c1|t \u2212 s|1+\u03b5,\n(cid:10)\n\n(cid:11)\n\n|xt \u2212 xs|\n|t \u2212 s|\u03b5/4p\n\np\n\nsup\n\ns,t\u2208d,s(cid:16)=t\n\n\u2265 m\n\n\u2264 c1/m p.\n\n(2) with probability one, xt is uniformly continuous on d.\nproof\n\nan =(cid:23)|xt \u2212 xs| \u2265 \u03bbn for some s, t \u2208 d with |t \u2212 s| \u2264 2\n\nstep 1. let \u03bbn = m2\n\n\u2212(n+1)\u03b5/4p and\n\n\u2212n\n\n(8.1)\n\n(8.2)\n\n(cid:24)\n\n.\n\n49\n\n "}, {"Page_number": 68, "text": "50\n\nthe continuity of paths\n\nrecall our convention that the letter c denotes unimportant constants which can change from\nline to line. we will show\n\np(an) \u2264 c2\n\n\u2212n\u03b5/4m\n\n\u2212p.\n\n(8.3)\nthis implies (1) and (2) as follows. if |xt \u2212 xt| \u2265 m|t \u2212 s|\u03b5/4p for some s, t \u2208 d with s (cid:16)= t,\n\u2212n, and then an holds. the event on the left-hand\nchoose n such that 2\nside of (8.2) is contained in \u222anan, and using (8.3) shows that\n\u2212n\u03b5/4 = cm\n2\n\n\u2212(n+1) < |t \u2212 s| \u2264 2\n\np(\u222anan) \u2264 cm\n\n\u221e(cid:9)\n\n\u2212p,\n\n\u2212p\n\nn=1\n\nwhich implies (1). let\n\nbm = { sup\ns,t\u2208d,s(cid:16)=t\n\n|xt \u2212 xt|/|t \u2212 s|\u03b5/4p \u2265 m}.\n\nm=1bm ) = 0. thus except for\nnote bm decreases as m increases and from (1) we have p(\u2229\u221e\nan event of probability zero, each \u03c9 is in bc\nm for some m (where m depends on \u03c9), and this\nimplies (2). thus we must show (8.3).\n\u2212 j that is closest to t (if there are two\nstep 2. define a( j, t ) to be the integer multiple of 2\ndifferent multiples that are equally close, we use some convention to break the tie). if t \u2208 dm,\nthen a(m, t ) = t. if |t \u2212 s| \u2264 2\nnow if s, t \u2208 dm and m \u2265 n, we use the triangle inequality to write\n\n\u2212n, then |a(n, t ) \u2212 a(n, s)| \u2264 2\n\n\u2212n+2.\n\n|xt \u2212 xs| = |xa(m,t ) \u2212 xa(m,s)|\n\u2264 |xa(n,t ) \u2212 xa(n,s)|\n\n+ |xa(n+1,t ) \u2212 xa(n,t )| + \u00b7\u00b7\u00b7 + |xa(m,t ) \u2212 xa(m\u22121,t )|\n+ |xa(n+1,s) \u2212 xa(n,s)| + \u00b7\u00b7\u00b7 + |xa(m,s) \u2212 xa(m\u22121,s)|.\n\n(8.4)\n\nif |xa(n,t ) \u2212 xa(n,s)| < \u03bbn/2 and for each i\n\nand the same with t replaced by s, then by (8.4)\n\n\u03bbn\n\n8(i + 1)2\n\n|xa(n+i+1,t ) \u2212 xa(n+i,t )| <\n\u221e(cid:9)\n\n|xt \u2212 xs| <\n\n+ 2\n\n\u03bbn\n2\n\n\u03bbn\n\n8(i + 1)2\n\n\u2264 \u03bbn.\n\ni=0\n\nhence if |xt \u2212 xs| \u2265 \u03bbn for some s, t \u2208 dm, then at least one of the events e, fi, or gi, i \u2265 0,\nmust hold, where\n\ne = {|xa(n,t ) \u2212 xa(n,s)| \u2265 \u03bbn/2 for some s, t \u2208 dn with |s \u2212 t| \u2264 2\nfi = {|xa(n+i+1,t ) \u2212 xa(n+i,t )| \u2265 \u03bbn/8(i + 1)2 for some t},\ngi = {|xa(n+i+1,s) \u2212 xa(n+i,s)| \u2265 \u03bbn/8(i + 1)2 for some s}.\n\n\u2212n},\n\n "}, {"Page_number": 69, "text": "the continuity of paths\n\n51\n\nstep 3. for the event e to hold, we must have |xr \u2212 xq| \u2265 \u03bbn/2 for some q, r \u2208 dn with\n|q \u2212 r| \u2264 2\n\u2212n+2. there are at most c2n such pairs (q, r), so the probability of e is bounded,\nusing chebyshev\u2019s inequality and (8.1), by\n\n(c2n)\n\nsup\n\nq\u2208dn,r\u2208dn+1,|r\u2212q|\u22642\u2212n+2\n\np(|xr \u2212 xq| \u2265 \u03bbn/2)\n\n(\u03bbn/2)p\n\n\u2264 c2n supq\u2208dn,r\u2208dn+1,|r\u2212q|\u22642\u2212n+2 e [|xr \u2212 xq|p]\n\u2264 c2n\n(2\n\u03bbp\nn\n\u2212n\u03b5\n\u2264 c2\n\u03bbp\nn\n\n\u2212n+2)1+\u03b5\n\n.\n\nfor fi to hold, that is, for |xa(n+i+1,t ) \u2212 xa(n+i,t )| to be greater than \u03bbn/8(i + 1)2 for some t,\nwe must have |xr\u2212 xq| \u2265 \u03bbn/8(i+ 1)2 for some r \u2208 dn+i, q \u2208 dn+i+1 with |r\u2212 q| \u2264 2\n\u2212n\u2212i+2.\nthere are at most c2n+i such pairs, and so the probability of fi is bounded by\n\n(cid:10)\n\n(cid:11)\n\n(c2n+i)\n\nsup\n\nr\u2208dn+i,q\u2208dn+i+1,|r\u2212q|\u22642\u2212n\u2212i+2\n\u2264 c\n\n2n+i2(\u2212n\u2212i+2)(1+\u03b5)(8(i + 1)2)p\n\n\u03bbp\nn\n\np\n\n|xr \u2212 xq| \u2265\n\n\u03bbn\n\n8(i + 1)2\n\n\u2264 c2\n\n\u2212i\u03b5/2\n\n.\n\n\u2212n\u03b52\n\u03bbp\nn\n\n\u221e(cid:9)\n\ni=0\n\nhere we used the fact that 2\nbut not i. we have the same bound for gi. therefore\n\n\u2212i\u03b5 (i + 1)2p \u2264 c2\n\n\u2212i\u03b5/2 for some constant c depending on p and \u03b5\n\np(\u222ai(fi \u222a gi) \u222a e ) \u2264\n\nletting m \u2192 \u221e we have\n\n\u2212i\u03b5/2\n\nc2\n\n\u2212n\u03b5/22\n\u03bbp\nn\n\n+ c2\n\n\u2212n\u03b5/2\n\u03bbp\nn\n\n\u2264 c2\n\n\u2212n\u03b5/2\u03bb\u2212p\n\n.\n\nn\n\np(an) \u2264 c2\n\n\u2212n\u03b5/2\u03bb\u2212p\n\nn\n\n= c2\n\n\u2212n\u03b5/4m\n\n\u2212p\n\nas required.\n\nthe proof of theorem 8.1 is an example of what is known as a metric entropy or chaining\n\nargument.\n\nin the above, the only place we relied on the fact that we were using real-valued processes\nwas in using the triangle inequality. therefore with only slight changes in notation, we have\nthe following theorem.\n\n "}, {"Page_number": 70, "text": "52\ntheorem 8.2 suppose x takes values in some metric space s with metric ds and there exist\nc1, \u03b5, and p > 0 such that\n\nthe continuity of paths\n\nthen the following hold.\n\n(1) there exists c2 depending only on c1, p, and \u03b5 such that for m > 0,\n\ne [ds (xs, xt )p] \u2264 c1|t \u2212 s|1+\u03b5,\n(cid:11)\n(cid:10)\n\np\n\nsup\n\ns,t\u2208d,s(cid:16)=t\n\nds (xs, xt )\n|t \u2212 s|\u03b5/2p\n\n\u2265 m\n\n\u2264 c1/m p.\n\ns, t \u2208 d.\n\n(8.5)\n\n(2) with probability one, xt is uniformly continuous on d.\nremark 8.3 theorem 8.2 holds for random variables indexed by time, but the analogous\nresult holds for the continuity in a of random variables x a indexed by some parameter a\nrunning through d. we may also let the parameter a run instead through the dyadic rationals\nin [b1, b2] for any b1 < b2.\n\nthe proof of the following corollary is an adaptation of the proof of theorem 8.1 and is\n\nleft as exercise 8.1.\ncorollary 8.4 suppose there exist c1, \u03b5, n, and p > 0 such that if n \u2264 n,\n\nthen there exists c2 depending on c1, \u03b5, and p but not n such that for m > 0 and n \u2264 n we\nhave\n\ne [ds (xs, xt )p] \u2264 c|t \u2212 s|1+\u03b5,\n(cid:10)\n\n(cid:11)\n\np\n\nsup\n\ns,t\u2208dn,s(cid:16)=t\n\nds (xs, xt )\n|t \u2212 s|\u03b5/2p\n\n\u2265 m\n\ns, t \u2208 dn.\n\n\u2212p.\n\n< c2m\n\nrecall the definition of h\u00a8older continuity from (7.1).\n\nproposition 8.5 if \u03b1 < 1/2, then the paths of a one-dimensional brownian motion {wt; 0 \u2264\nt \u2264 1} are h\u00a8older continuous of order \u03b1 with probability one.\nproof by the stationary increments property and scaling,\n\ne|wt \u2212 ws|p = e|wt\u2212s|p = |t \u2212 s|p/2e|w1|p.\n\nif \u03b1 < 1/2, choose p large enough so that ((p/2) \u2212 1)/p > \u03b1 and then take \u03b5 = (p/2) \u2212 1.\n(here \u03b5 is large!) take \u03b3 sufficiently small that (\u03b5/p) \u2212 \u03b3 > \u03b1. then by exercise 8.2 the\npaths of wt are h\u00a8older continuous of order \u03b1, with probability one, provided we restrict\nt to d. but the paths of brownian motion are continuous, so we see that we have h\u00a8older\ncontinuity of order \u03b1 when t \u2208 [0, 1].\n\n8.1\n\n8.2\n\nprove corollary 8.4.\n\nexercises\n\nif the hypothesis of theorem 8.1 holds and \u03b3 < \u03b5/p, show that there exists c2 depending only\non c1, \u03b5, \u03b3 , and p such that for m > 0\n\n(cid:10)\n\n(cid:11)\n\np\n\nsup\n\ns,t\u2208d,s(cid:16)=t\n\nds (xs, xt )\n|t \u2212 s|(\u03b5/p)\u2212\u03b3\n\n\u2265 m\n\n\u2264 cm\n\n\u2212p.\n\n "}, {"Page_number": 71, "text": "8.3\n\n8.4\n\n8.5\n\n8.6\n\nexercises\n\n53\n\nsuppose x is a real-valued process and there exist constants c1, c2 such that\ns, t \u2208 [0, 1].\n\np(|xt \u2212 xs| > \u03bb) \u2264 c1e\n\n\u2212c2\u03bb log4 (1/|t\u2212s|),\n\nprove that with probability one, x has a version which is uniformly continuous on the dyadic\nrationals in [0, 1].\nsuppose (xt , t \u2208 [0, 1]) is a mean zero gaussian process and there exist c and \u03b5 such that\n\ns, t \u2208 [0, 1].\nprove that there is a version of x that has continuous paths on [0, 1].\n\nvar (xt \u2212 xs ) \u2264 c|t \u2212 s|\u03b5,\n\nlet x be as in exercise 8.4. for what values \u03b1 will x have paths that are h\u00a8older continuous of\norder \u03b1? (\u03b1 will depend on \u03b5.)\nlet {xs,t : s, t \u2208 [0, 1]} be a collection of random variables. suppose there exist c, p, and \u03b5 > 0\nsuch that\n\ne|xs(cid:3),t(cid:3) \u2212 xs,t|p \u2264 c(|t\n\n(cid:3) \u2212 t| + |s\n\n(cid:3) \u2212 s|)2+\u03b5.\n\nprove that with probability one, the map (s, t ) \u2192 xs,t (\u03c9) is uniformly continuous on d \u00d7 d =\n{(s, t ); s, t \u2208 d}.\n\n "}, {"Page_number": 72, "text": "9\n\ncontinuous semimartingales\n\nroughly speaking, a semimartingale is the sum of a martingale and a process whose paths\nare of bounded variation. in this chapter we consider semimartingales whose paths are con-\ntinuous. we will give definitions, and then investigate in more detail the class of martingales\nthat are square integrable. finally we present a proof of the doob\u2013meyer decomposition for\ncontinuous supermartingales. the doob\u2013meyer decomposition used to be considered a very\nhard theorem, but at least in the continuous case, an elementary proof is possible. for a proof\nfor the general case, see chapter 16.\n\n(cid:11)\nlet {f t} be a filtration satisfying the usual conditions and let\n\n(cid:10)(cid:25)\n\n(cid:4)\n\n9.1 definitions\n\nf\u221e =\n\nft = \u03c3\n\nt\u22650\n\nft\n\n.\n\nt\u22650\n\nwe say a process x has increasing paths or that x is an increasing process if the functions\nt \u2192 xt (\u03c9) are increasing with probability one. throughout this book saying f is \u201cincreasing\u201d\nmeans that s < t implies f (s) \u2264 f (t ), while saying f is \u201cstrictly increasing\u201d means that\ns < t implies f (s) < f (t ). a process x with paths of bounded variation is just what one\nwould expect: with probability one, the functions t \u2192 xt (\u03c9) are of bounded variation. we\nsay x has paths locally of bounded variation if there exist stopping times rn \u2192 \u221e such that\nthe process xt\u2227rn has paths of bounded variation for each n.\nwe turn to martingales. a martingale m is a uniformly integrable martingale if the family\nof random variables {mt} is uniformly integrable. a process x is a local martingale if there\nexist stopping times rn \u2192 \u221e such that m n\n= xt\u2227rn is a uniformly integrable martingale for\neach n. a martingale whose paths are continuous is called a continuous martingale and we\nsimilarly define a right-continuous martingale.\na semimartingale is a process x of the form xt = mt + at, where mt is a local martingale\nand at is a process whose paths are locally of bounded variation. as a consequence of\nthe doob\u2013meyer decomposition we will see that submartingales and supermartingales are\nsemimartingales.\n\nt\n\nas an example, a brownian motion wt is a martingale and is a local martingale (let rn\nbe identically equal to n), but is not a uniformly integrable martingale. we will define what\nit means to be a square integrable martingale in the next section; brownian motion is not a\nsquare integrable martingale.\n\n54\n\n "}, {"Page_number": 73, "text": "9.2 square integrable martingales\n\n55\n\n9.2 square integrable martingales\n\ndefinition 9.1 a martingale is a square integrable martingale if there exists af\u221e measurable\nrandom variable m\u221e such that e m 2\u221e < \u221e and mt = e [m\u221e | ft] for all t.\nan example of a square integrable martingale would be mt = wt\u2227t0, where wt is a brownian\nmotion and t0 is a fixed time; in this case m\u221e = wt0.\nproposition 9.2 let {ft} be a filtration satisfying the usual conditions and m a right con-\ntinuous process. the following are equivalent:\n< \u221e.\nt ] < \u221e.\n\n(1) mt is a square integrable martingale.\ne m 2\n(2) m is a martingale with supt\u22650\nt\n(3) m is a martingale with e [supt\u22650 m 2\n\nproof to show (1) implies (2), suppose m is a square integrable martingale. then by\njensen\u2019s inequality for conditional expectations (proposition a.21),\n\n= e [(e [m\u221e | ft])2] \u2264 e [e [m 2\u221e | ft] ] = e m 2\u221e.\n\ne m 2\nt\n\nto show (2) implies (3), for each n,\n\nt ] \u2264 4e m 2\nm 2\n\nn\n\ne [ sup\n0\u2264t\u2264n\n\nby doob\u2019s inequalities. that (2) implies (3) follows by letting n \u2192 \u221e and using fatou\u2019s\nlemma.\n\nnow suppose (3) holds, and we will show (1) holds. since e m 2\n\nn is uniformly bounded in\nn, the martingale convergence theorem (theorem a.35) implies that mn converges almost\nsurely and in l2. let us call the limit m\u221e; we have e m 2\u221e < \u221e by the l2 convergence. since\ne m 2\nn is uniformly bounded, then mn is a uniformly integrable martingale, and by proposition\na.37, mn = e [m\u221e | fn]. if n \u2212 1 \u2264 t \u2264 n, we have\n\nmt = e [mn | ft] = e [ e [m\u221e | fn] | ft] = e [m\u221e | ft],\n\nas required.\n\nfor the remainder of this section all our martingales will have paths that are right continuous\n\nwith left limits.\nproposition 9.3 if m is a square integrable martingale and s \u2264 t are finite stopping times,\nthen e [mt | fs] = ms.\nproof let a \u2208 fs and define u (\u03c9) = s(\u03c9)1a(\u03c9) + t (\u03c9)1ac (\u03c9). thus u is equal\nto s if \u03c9 \u2208 a and otherwise is equal to t . since a \u2208 fs \u2282 ft , then we have\n(u \u2264 t ) = [(s \u2264 t ) \u2229 a] \u222a [(t \u2264 t ) \u2229 ac] is in ft, and therefore u is a stopping\ntime. by proposition 3.11,\n\ne m0 = e mu = e [ms; a] + e [mt; ac]\n\nand\n\ne m0 = e mt = e [mt; a] + e [mt; ac].\n\n "}, {"Page_number": 74, "text": "56\nthese two equations imply that e [ms; a] = e [mt; a], which is what we needed to\nprove.\n\ncontinuous semimartingales\n\nby exercise 3.11, the conclusion is valid if m is a uniformly integrable martingale.\nas an immediate corollary we have\n\ncorollary 9.4 suppose m is a square integrable martingale and t is a stopping time. then\nxt = mt\u2227t is a martingale with respect to {ft\u2227t}.\nthe proof of the following proposition is similar to that of proposition 9.3. it may be\n\nviewed as a converse of the optional stopping theorem.\nproposition 9.5 suppose {ft} is a filtration satisfying the usual conditions and m is a\nprocess that is adapted to {ft} such that mt is integrable for each t. if e mt = 0 for every\nbounded stopping time t , then mt is a martingale.\nproof suppose s < t and a \u2208 fs. define t to be equal to s if \u03c9 \u2208 a and equal to t if \u03c9 /\u2208 a.\nas in the proof of proposition 9.3, but even more simply, t is a stopping time, so\n\n0 = e mt = e [ms; a] + e [mt; ac].\n\nthe fixed time t is a stopping time, hence\n\n0 = e mt = e[mt; a] + e [mt; ac].\n\ncomparing, e [mt; a] = e [ms; a], which proves m is a martingale.\nproposition 9.6 suppose mt is a square integrable martingale. then\n| fs].\n\ne [(mt \u2212 ms )2 | fs] = e [m 2\n\n\u2212 m 2\n\nt\n\ns\n\n(9.1)\n\nproof by proposition 9.3\n\ne [(mt \u2212 ms )2 | fs] = e [m 2\n= e [m 2\n= e [m 2\n\nt\n\nt\n\nt\n\n| fs] \u2212 2mse [mt | fs] + m 2\n| fs] \u2212 m 2\n| fs]\n\u2212 m 2\n\ns\n\ns\n\ns\n\nand we are done.\n\nif we take expectations in (9.1), we obtain\n\ne [(mt \u2212 ms )2] = e m 2\n\n\u2212 e m 2\n\n.\n\ns\n\nt\n\n(9.2)\ntheorem 9.7 suppose m0 = 0, mt is a continuous local martingale, and the paths of mt are\nlocally of bounded variation. then m is identically 0, a.s., that is, p(mt = 0 for all t ) = 1.\nproof using the definition of local martingale, it suffices to suppose m is a continuous\nuniformly integrable martingale. let t0 be fixed and let at denote the total variation of the\npaths of m up to time t. if tn = inf{t\n= mtn\u2227t\u2227t0. using\nproposition 9.3 and the remark following it, we see that m n is also a continuous martingale\nwith paths of bounded variation, and if m n is identically zero, then letting n \u2192 \u221e and\nt0 \u2192 \u221e, we obtain our result. therefore it suffices to suppose the total variation of mt is\nbounded by n, a.s. in particular, mt is bounded by n.\n\n: at \u2265 n}, we look at m n\n\nt\n\n "}, {"Page_number": 75, "text": "let n \u2265 1 and set\n\n9.3 quadratic variation\n\n57\n\nnote vn \u2264 2n, a.s., and vn \u2192 0, a.s., as n \u2192 \u221e by the uniform continuity of the paths of\nm on [0, t0]. by dominated convergence, e vn \u2192 0 as n \u2192 \u221e. we write\n\ne m 2\nt0\n\nk=0\n\n(m 2\n\nkt0/2n )\n\n= e\n\n(cid:17)\n(cid:17)\n\n(k+1)t0/2n \u2212 m 2\n\nvn = sup\n|m(k+1)t0/2n \u2212 mkt0/2n|.\nk\u22642n\u22121\n(cid:16) 2n\u22121(cid:9)\n(cid:16) 2n\u22121(cid:9)\n(m(k+1)t0/2n \u2212 mkt0/2n )2\n(cid:16)\n2n\u22121(cid:9)\n\u2264 e\nvn\nk=0\n\u2264 n e vn.\ns ] = 0. hence m is identically 0 up to time t0.\n\n|m(k+1)t0/2n \u2212 mkt0/2n|\n\n= e\n\n(cid:17)\n\nk=0\n\nthe second equality follows by (9.2). since n is arbitrary and e vn \u2192 0, then e m 2\ndoob\u2019s inequalities, e [sups\u2264t0 m 2\n\nt0\n\n= 0. by\n\n9.3 quadratic variation\n\n0\n\n\u2212 (cid:22)m(cid:23)\n= 0.\n\nt is a martingale, where (cid:22)m(cid:23)\n\ndefinition 9.8 a continuous square integrable martingale mt has quadratic variation (cid:22)m(cid:23)\n(sometimes written (cid:22)m, m(cid:23)\nt\nt) if m 2\nt is a continuous\nadapted increasing process with (cid:22)m(cid:23)\nt\nin the case where w is a brownian motion, t0 is fixed, and mt = wt\u2227t0 the quadratic\nvariation of m is just (cid:22)m(cid:23)\n= t \u2227 t0 by example 3.3. brownian motion itself does not fit\nperfectly into the framework of stochastic integration because it is not a square integrable\nmartingale, although it is a martingale; we will be dealing with this point several times in\nwhat follows.\n\nwe will show existence and uniqueness of (cid:22)m(cid:23)\n\nt by means of the doob\u2013meyer decompo-\nsition, theorem 9.12, below. however we defer the proof of the doob\u2013meyer decomposition\nuntil the next section. a process z is of class d if {zt : t a finite stopping time} is a\nuniformly integrable family of random variables.\n\nt\n\ntheorem 9.9 let mt be a continuous square integrable martingale. there exists a continuous\nadapted increasing process (cid:22)m(cid:23)\n\u2212\n(cid:22)m(cid:23)\n\u2212 at is a martingale, then\n\nt is a martingale.\nif at is a continuous adapted increasing process such that m 2\n(cid:16)= (cid:22)m(cid:23)\nt\n\n= 0 and with increasing paths such that m 2\n\nt for some t ) = 0.\n\nt with (cid:22)m(cid:23)\n\np(at\n\n0\n\nt\n\nproof by jensen\u2019s inequality for conditional expectations,\n\ne [m 2\nt\n\n| fs] \u2265 (e [mt | fs])2 = m 2\n\ns\n\nt is a submartingale. since m\u221e is square integrable, given \u03b5 there exists \u03b4\n\nif s < t, and so m 2\nsuch that e [m 2\u221e; a] < \u03b5 if p(a) < \u03b4. since m 2\n> k ) \u2264 e m 2\n\np(m 2\nt\n\nt\n\nt is a submartingale, if k > e m 2\u221e/\u03b4, then\n/k \u2264 e m 2\u221e/k < \u03b4,\n\n "}, {"Page_number": 76, "text": "58\n\ncontinuous semimartingales\n\nand consequently\n\ne [m 2\nt\n\n; m 2\n\nt\n\n> k] \u2264 e [m 2\u221e; m 2\n\nt\n\n> k] < \u03b5.\n\n(9.3)\n\n(9.4)\n\nby exercise 3.11, m 2\n(theorem 9.12) to \u2212m 2\nt\nhas increasing paths. we then set (cid:22)m(cid:23)\nt\npart of the doob\u2013meyer decomposition.\n\nt , we write \u2212m 2\n\nis of class d. applying the doob\u2013meyer decomposition\n= nt \u2212 bt, where nt is a martingale and bt\n= bt. the uniqueness follows from the uniqueness\n\nt\n\nin view of proposition 9.3 and the definition of (cid:22)m(cid:23), we have\n) | fs]\n\ne [(mt \u2212 ms )2 \u2212 ((cid:22)m(cid:23)\n\n\u2212 (cid:22)m(cid:23)\n\n= e [m 2\n\nt\n\n\u2212 m 2\n\ns\n\nt\n\n\u2212 ((cid:22)m(cid:23)\n\nt\n\ns\n\n\u2212 (cid:22)m(cid:23)\n\ns\n\n) | fs] = 0\n\nif s and t are finite stopping times and m is a continuous square integrable martingale.\n\nif m and n are two square integrable martingales, we define (cid:22)m, n(cid:23)\n\nt by\n\n(cid:22)m, n(cid:23)\n\nt\n\n= 1\n\n2 [(cid:22)m + n(cid:23)\n\nt\n\n\u2212 (cid:22)m(cid:23)\n\nt\n\n\u2212 (cid:22)n(cid:23)\nt].\n\nthis is sometimes called the covariation of m and n.\n\nan alternative representation of (cid:22)m(cid:23)\n\nt is the following. a proof could be given now, but it\n\nis a bit messy. after we have it\u02c6o\u2019s formula this will be easier.\ntheorem 9.10 let m be a square integrable martingale and let t0 > 0. then (cid:22)m(cid:23)\nlimit in probability of\n\nt is the\n\n[2nt0](cid:9)\n\nk=0\n\n(m(k+1)/2n \u2212 mk/2n )2,\n\nwhere [2nt0] is the largest integer less than or equal to 2nt0.\n\n9.4 the doob\u2013meyer decomposition\n\nin this section we give a proof of the doob\u2013meyer decomposition for continuous super-\nmartingales. first we need the following inequality, which has many other uses as well.\n\nproposition 9.11 suppose a1 and a2 are two increasing adapted continuous processes\nstarting at zero with ai\u221e = limt\u2192\u221e ai\n< \u221e, a.s., i = 1, 2, and suppose there exists a\npositive real k such that for all t,\ne [ai\u221e \u2212 ai\n\n(9.5)\nt . suppose there exists a non-negative random variable v with e v 2 < \u221e\n\n| ft] \u2264 k,\n\ni = 1, 2.\n\na.s.,\n\nt\n\nt\n\n\u2212 a2\nlet bt = a1\nsuch that for all t,\n\nt\n\nthen\n\n|e [b\u221e \u2212 bt | ft]| \u2264 e [v | ft],\n\na.s.\n\n\u221a\n\n\u2264 8e v 2 + 8\n\n2k (e v 2)1/2.\n\ne sup\nt\u22650\n\nb2\nt\n\nproof we start by showing\n\ne (ai\u221e)2 \u2264 2k2,\n\ni = 1, 2.\n\n(9.6)\n\n(9.7)\n\n(9.8)\n\n "}, {"Page_number": 77, "text": "9.4 the doob\u2013meyer decomposition\n\n59\n\nfirst suppose ai\u221e is bounded by a positive real number l. note that we have\ne ai\u221e = e [e [ai\u221e \u2212 ai\n\n| f0] ] \u2264 k. a simple calculation shows that\n\n0\n\n0\n\nwe then have, using proposition 3.14,\ne (ai\u221e)2 = 2e\n= 2e\n= 2e\n\u2264 2ke\n\u2265 l} and ai,l\n\n+ a2\n\n0\n\n0\n\nif we let tl = inf{t : a1\nby ai,l\nt\nproves (9.8).\n\nwe next write\n\nand hence\n\n= 2ke ai\u221e \u2264 2k2.\n\nt\n\n) dai\nt\n| ft] dai\n\nt\n\n.\n\nt\n\nt\n\nt\n\n0\n\n0\n\n) dai\nt\n\n) dai\nt\n\n(ai\u221e \u2212 ai\n\n(cid:3) \u221e\n(ai\u221e)2 = 2\n(cid:3) \u221e\n(cid:3) \u221e\n(ai\u221e \u2212 ai\n(cid:3) \u221e\n(e [ai\u221e | ft] \u2212 ai\n(cid:3) \u221e\ne [ai\u221e \u2212 ai\ndai\nt\nt = ai\n(cid:3) \u221e\nb2\u221e = 2\n(cid:3) \u221e\n(cid:3) \u221e\ne b2\u221e = 2e\n(cid:3) \u221e\n\u2264 e\n\n(b\u221e \u2212 bt ) dbt ,\n\n0\n\n0\n\n0\n\nt\n\nt\n\n+ a2\n= e\n= e [v (a1\u221e + a2\u221e)].\n\nv d (a1\nt\n\n0\n\nt\n\n)\n\ne [b\u221e \u2212 bt | ft] dbt\ne [v | ft] d (a1\n+ a2\n\n)\n\nt\n\nt\u2227tl, then (9.5) still holds if we replace ai\n. we obtain e (ai,l\u221e )2 \u2264 2k2, and then letting l \u2192 \u221e and using fatou\u2019s lemma\nt\n\nt\n\nthe bound (9.8) takes care of the integrability concerns. by the cauchy\u2013schwarz inequality\nwe obtain\n\ne b2\u221e \u2264 (e [(a1\u221e + a2\u221e)2])1/2(e v 2)1/2 \u2264 2\n\n\u221a\n2k (e v 2)1/2.\n\nnow let mt = e [b\u221e | ft], nt = e [v | ft], where we take the right\u2013continuous versions\n(see corollary 3.13), and let xt = mt \u2212 bt. we have\n\n|xt| = |e [b\u221e \u2212 bt | ft]| \u2264 nt ,\n\nand using doob\u2019s inequalities,\n\ne sup\nt\u22650\n\nx 2\nt\n\nalso by doob\u2019s inequalities,\n\n\u2264 e sup\nt\u22650\n\nn 2\nt\n\n\u2264 4e n 2\u221e = 4e v 2.\n\nsince supt\u22650\n\n|bt| \u2264 supt\u22650\n\ne sup\nt\u22650\n\nm 2\nt\n|xt| + supt\u22650\n\n\u2264 4e m 2\u221e = 4e b2\u221e.\n|mt|, our result follows.\n\n "}, {"Page_number": 78, "text": "60\n\ncontinuous semimartingales\n\nwe now prove the doob\u2013meyer decomposition for continuous supermartingales. in view\n\nof the proof of proposition a.30, we would like to let\n| fs\n\nat =\n\ne\n\nt\n\n(cid:3)\n\n0\n\n(cid:16)\n\ndzs\nds\n\n(cid:17)\n\nds,\n\nbut this doesn\u2019t make sense. we instead define an approximation ah\nt converges to what we want as h \u2192 0.\nah\ntheorem 9.12 suppose zt is a continuous adapted supermartingale of class d. then there\nexists an increasing adapted continuous process at with paths locally of bounded variation\nstarted at 0 and a continuous local martingale mt such that\n\nt by (9.9) and show that\n\nzt = mt \u2212 at .\n\n(cid:3)\n\n(cid:3)\nand a\n\nif m\n\nare two other such processes with zt = m\n\n\u2212 a\nt, then mt = m\n(cid:3)\n\nt and at = a\n(cid:3)\n(cid:3)\nt\n\n(cid:3)\nt\n\nfor all t, a.s.\nproof let us prove the second assertion first. let sn be the first time that |mt| + |m\nexceeds n. if\n\n|\n\n(cid:3)\nt\n\n,\n\n(cid:3)\nt\n\n(cid:3)\nt\n\n(cid:3)\nt\u2227sn\n\n\u2212 a\n\n\u2212 m\n\n= at\u2227sn\n\nzt = mt \u2212 at = m\n\u2212 a\n(cid:3)\nthen mt\u2227sn\nt\u2227sn is a martingale whose paths are locally of bounded\nt\u2227sn , a.s. since this is true for all n, then mt = m\n= m\n(cid:3)\n(cid:3)\nvariation. by theorem 9.7, mt\u2227sn\nt .\n= zt\u2227tn .\nnow let us prove the existence of m and a. let tn = inf{t : |zt| \u2265 n}\u2227n and zn\n\u2212 an\n= m n\nt for\nagreeing with\n, respectively, for t \u2264 tn1. hence given t, we can choose n large enough so that\nt . clearly this gives the desired decomposition.\n\nby exercise 9.2, zn is a supermartingale. if we prove the decomposition zn\nt\neach n, then by the uniqueness assertion, if n1 < n2, we have an1\nt\nan2\nt and m n2\nt \u2264 tn and then define mt = m n\nt\nthus we may suppose that zt is bounded by some n and that zt is constant for t \u2265 n.\nlet v\u03b4 = sup|t\u2212s|\u2264\u03b4 |zt \u2212 zs|. since z has continuous paths,\n|zt \u2212 zs|,\n\nt , at = an\n\nand m n1\nt\n\nv\u03b4 =\n\nsup\n\nt\n\nt\n\ns,t\u2208q+,|t\u2212s|\u2264\u03b4\n\nand therefore v\u03b4 is measurable with respect to f\u221e. since the paths of z are uniformly\ncontinuous, v\u03b4 \u2192 0, a.s., as \u03b4 \u2192 0, and since |v\u03b4| \u2264 2n, we have by dominated convergence\nthat e v 2\n\n\u03b4 \u2192 0 as \u03b4 \u2192 0.\n\nwe define\n\nah\nt\n\n= 1\nh\n\nt\n\n(zs \u2212 e [zs+h | fs]) ds.\n\n(9.9)\nat this point we do not know even that e [zs+h | fs] has any nice measurability prop-\nerties (it is not a martingale, for example); let us assume that it has a version that\nhas continuous paths, is adapted, and is jointly measurable in t and \u03c9, and prove this\n\n0\n\n(cid:3)\n\n "}, {"Page_number": 79, "text": "9.4 the doob\u2013meyer decomposition\n\n61\n\nfact a bit later on. because z is a supermartingale, ah is increasing. we have (note\nexercise 9.6)\n\ne[ah\u221e \u2212 ah\n\nt\n\n| ft] = 1\nh\n= 1\nh\n= 1\nh\n= 1\nh\n= e\n\n(cid:17)\n\ne [zs \u2212 zs+h | fs] ds | ft\n(cid:17)\n\ne\n\nt\n\nt\n\n(cid:16)(cid:3) \u221e\n(cid:3) \u221e\n(cid:16)(cid:3) \u221e\ne [zs \u2212 zs+h | ft] ds\n(cid:16)(cid:3)\n(cid:16)(cid:3)\n\n(cid:3) \u221e\n(cid:17)\nzs ds \u2212\nt+h\nt+h\n(cid:17)\nzs ds | ft\nzt+uh du | ft\n(cid:16)(cid:3)\n\nt\n1\n\n.\n\n0\n\nt\n\ne\n\ne\n\nzs ds | ft\n\n(cid:20)(cid:20)(cid:20)e\n\n0\n\n\u2264 e [vh | ft].\n\u2212 ak\n\nsince z is bounded by n, it follows that ah satisfies (9.5). if k < h, then\n\n|e [(ah\u221e \u2212 ah\n\nt\n\n) \u2212 (ak\u221e \u2212 ak\n\nt\n\n) | ft]| =\n\n1\n\n(zt+uh \u2212 zt+uk ) du | ft\n\n(cid:17)(cid:20)(cid:20)(cid:20)\n\n)2 \u2192 0 as k, h \u2192 0. this shows\nnow apply proposition 9.11 to see that e supt\u22650\nthat whenever hn decreases to 0, then ahn is a cauchy sequence in a normed linear space,\nwhere the norm is given by\n\n(ah\nt\n\nt\n\n(cid:21)x (cid:21) = (e sup\nt\u22650\n\n|xt|2)1/2,\n\n(9.10)\n\nwhich is complete by exercise 9.5. therefore there exists a limit a. since\n\n\u2212 at )2 \u2192 0\n\ne sup\nt\u22650\n\n(ah\nt\n\nas h \u2192 0, there exists a subsequence hn \u2192 0 such that supt\u22650\nproves that at is continuous and increasing.\n\nt \u2212 at )2 \u2192 0, a.s., which\n(ahn\n\nwe calculate\n\n(cid:17)\n\nt\n\n(cid:16)(cid:3)\ne [a\u221e \u2212 at | ft] = lim\ne [ah\u221e \u2212 ah\nh\u21920\n(cid:16)(cid:3)\n= lim\nh\u21920\n= e\n= zt .\n\nzt du | ft\n\ne\n\n0\n\n1\n\n0\n\n1\n\n| ft]\n(cid:17)\n\nzt+uh du | ft\n\ntherefore\n\nzt = e [a\u221e | ft] \u2212 at ,\n\nwhich is the decomposition of z into a martingale minus an increasing process.\n\n "}, {"Page_number": 80, "text": "62\n\ncontinuous semimartingales\n\nt\n\nt\n\n(\u03c9) =\n\n\u221e(cid:9)\n\nfix h. it remains to show that there is a version of e [zs+h | fs] that is a continuous\nk/2n \u2264 t < (k+1)/2n. take the right-continuous version(cid:14)y k,n\njointly measurable adapted process. define yt = zt+h and define y n\nto be equal to yk/2n if\nof the martingale e [yk/2n | ft]\n1[k/2n,(k+1)/2n )(t )(cid:14)y k,n\n(see corollary 3.13) and let(cid:14)y n\n| ft], a.s., for all t. moreover,(cid:14)y n\nnote that(cid:14)y n\n\u2212(cid:14)y m\n\nis jointly measurable in t and \u03c9. now for n > m,\n| \u2264 sup\nt\u22650\n\n(9.11) converges to 0 almost surely. hence along the appropriate subsequence,(cid:14)y n\nuniformly. if we call the limit(cid:14)y , we see that(cid:14)yt is right continuous, adapted, and jointly\n\nwe have already seen that there exists a subsequence such that the right-hand side of\nt converges\n\nis right continuous, so we see that it\n\ne [v2\u2212m | ft].\n\n= e [y n\n\n|(cid:14)y n\n\n(9.11)\n\nsup\nt\u22650\n\n(\u03c9).\n\nk=0\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nmeasurable. if k/2n \u2264 t \u2264 (k + 1)/2n, then |y n\n\u2212 y n\n\n|(cid:14)y n\n\nt\n\n\u2212(cid:14)y n\nk/2n| = |e [y n\n|(cid:14)y n\n\u2212(cid:14)y n\n\nt\n\nby the triangle inequality,\n\nt\n\nk/2n| \u2264 v2\u2212n, so\n\n\u2212 y n\nk/2n | ft]| \u2264 e [v2\u2212n | ft].\njump of (cid:14)y n\n\nt\n\n| \u2264 2 sup\nt\u22650\n\ne [v2\u2212n | ft]\ne [v2\u2212n | ft], and we conclude the limit(cid:14)y has continuous paths. finally, y n\nif k/2n \u2264 s, t \u2264 (k + 1)/2n. therefore the largest\nfers from yt by at most v2\u2212n, so we see by passing to the limit that (cid:14)yt is a version of\n2 supt\u22650\ne [zt+h | ft].\n\nis bounded by\nt dif-\n\ns\n\nt\n\n9.1\n\n9.2\n\n9.3\n\n9.4\n\n9.5\n\n9.6\n\nexercises\n\nlet wt be a brownian motion started at 1 and t0 = inf{t > 0 : wt = 0}. is mt = wt\u2227t0 a\nsquare integrable martingale? a locally square integrable martingale? a uniformly integrable\nmartingale? a martingale? a local martingale? a semimartingale?\n\n|mt| is integrable,\nprove that if m is a submartingale such that the paths of m are continuous, supt\nand s \u2264 t are finite stopping times, then e [mt | fs] \u2265 ms. note that the last part of the proof\nof proposition 9.3 breaks down here.\nsuppose mt is a local martingale with continuous paths. show that if n > 0, tn = inf{t :\n|mt| \u2265 n}, and m n\nsuppose w 1\nt and w 2\nshow (cid:22)m 1, m 2(cid:23)t = 0.\nshow that the norm defined in (9.10) is complete.\n(cid:16)(cid:3) \u221e\n\n= mt\u2227tn , then m n is a uniformly integrable martingale.\nt are two independent brownian motions, t0 > 0, and m i\nt\n\nlet zt be a bounded supermartingale with continuous paths that is constant from some time t0\non. show that for each t\n\n= wt\u2227t0, i = 1, 2.\n\n(cid:3) \u221e\n\n(cid:17)\n\nt\n\ne [zs \u2212 zs+h | fs] ds | ft\n\n=\n\ne [zs \u2212 zs+h | ft] ds,\n\na.s.\n\ne\n\nt\n\nt\n\n "}, {"Page_number": 81, "text": "63\n9.7 we mentioned that one can prove the existence of (cid:22)m(cid:23) without using the doob\u2013meyer theorem.\nhere is how that argument starts. let m be a bounded continuous martingale and for each n,\ndefine\n\nnotes\n\nin(t ) = [t2n](cid:9)\n\ni=0\n\n(m(i+1)/2n \u2212 mi/2n )2.\n\nhere [x] is the integer part of x. prove that for each t > 0, e|in(t ) \u2212 im(t )|2 \u2192 0 as n, m \u2192 \u221e.\none can then define (cid:22)m(cid:23)t as the l2 limit of in(t ).\nhint: if n > m, note that\n\nm(i+1)/2m \u2212 mi/2m = 2n\u2212m (i+1)\u22121(cid:9)\n\nj=2n\u2212mi\n\n(m( j+1)/2n \u2212 m j/2n ).\n\nnotes\n\nthe first proof of the doob\u2013meyer decomposition was by meyer in the early 1960s and was\na major breakthrough. there are now a number of alternate proofs. the proof we give here\nfor continuous supermartingales is new.\n\n "}, {"Page_number": 82, "text": "10\n\nstochastic integrals\n\nthis chapter is devoted to the construction of stochastic integrals, primarily with respect to\nt\n0 hs dws, where w\ncontinuous square integrable martingales. the motivating example is\nis a brownian motion and h is an adapted process satisfying certain conditions. we cannot\ndefine this integral as a lebesgue\u2013stieltjes integral because the paths of brownian motion\nare nowhere differentiable (theorem 7.3).\n\n(cid:15)\n\none way to visualize a stochastic integral is to think of dws as \u201cwhite noise,\u201d on a radio\nand hs as the volume control which increases or decreases the white noise by a factor. for\nanother model, if ws is supposed to represent a stock price at time s (of course, stock prices\ncan\u2019t be negative, while brownian motion can!) and hs is the number of shares held at time\ns, then the stochastic integral represents the net profit.\n\n10.1 construction\n\nlet mt be a continuous square integrable martingale with respect to a filtration {f t} satisfying\nthe usual conditions, and suppose ht is an adapted process. under appropriate additional\nassumptions on h , we want to define\n\n(cid:3)\n\nnt =\n\nt\n\nhs dms,\n\n(10.1)\n\n0\nthe stochastic integral of h with respect to m.\nwe impose two conditions on the integrand ht, a measurability one and an integrability\none. first we define the predictable \u03c3 -field p on [0,\u221e) \u00d7 \u0001. this is the smallest \u03c3 -field\nof subsets of [0,\u221e) \u00d7 \u0001 with respect to which all left continuous, bounded, and adapted\nprocesses are measurable. in symbols,\n\np = \u03c3 (x : x is left continuous, bounded, and adapted to {ft}).\n\nthis can be rephrased by saying p is the \u03c3 -field on [0,\u221e) \u00d7 \u0001 generated by the collection\nof all sets of the form\n\n{(t, \u03c9 \u2208 [0,\u221e) \u00d7 \u0001 : xt (\u03c9) > a},\n\nwhere a \u2208 r and x is a bounded, adapted, left continuous process. we require h : [0,\u221e)\u00d7\n\u0001 \u2192 r to be measurable with respect to p. when this happens, we say h is predictable.\nthe integrability is easier to state: we require\n\n(cid:3) \u221e\n\ne\n\n0\n\ns d(cid:22)m(cid:23)\nh 2\n\ns\n\n< \u221e.\n\n64\n\n(10.2)\n\n "}, {"Page_number": 83, "text": "10.1 construction\n\n65\n\n(cid:15)\n\nt\n0 hs dms in three steps:\n\nobserve that h will meet both requirements if h is bounded, adapted, and has continuous\npaths.\nwe define\nstep 1. when hs(\u03c9) = k (\u03c9)1(a,b](s), where k is bounded and fa measurable.\nstep 2. when hs is a sum of processes of the form in step 1.\nstep 3. when h is predictable and satisfies (10.2).\n= t \u2227 t0,\nif mt = wt\u2227t0, where w is a brownian motion and t0 is a fixed time, then (cid:22)m(cid:23)\nand it might help the reader to work through the proofs in this special case. even in this\nsituation, all the elements of the general construction are present.\n\nt\n\nform xt (\u03c9) =(cid:12)\n\nwe will need the following easy lemma.\n\nlemma 10.1 the predictable \u03c3 -field p is generated by the collection c of processes of the\ni=1 ki(\u03c9)1(ai,bi](t ), where for each i, ki is a bounded fai measurable random\nif x \u2208 c, then x is bounded, adapted, and left continuous, hence x is a predictable\n\nvariable.\n\nn\n\non the other hand, if y is a bounded, adapted, left-continuous process, we can approximate\n\nproof\nprocess. thus c \u2282 p.\n\ny by the processes\n\ny n\nt\n\n(\u03c9) = n2n(cid:9)\n\ni=0\n\nyi/2n (\u03c9)1(i/2n,(i+1)/2n](t ).\n\neach such y n is in c. therefore the \u03c3 -field generated by c contains p.\nproposition 10.2 suppose h is as in step 1 above. then\nnt = k (mt\u2227b \u2212 mt\u2227a)\n\nis a continuous martingale,\ne n 2\u221e = e\n\nand\n\n(cid:3) \u221e\n\n0\n\n(cid:22)n(cid:23)\n\nt\n\n=\n\n(cid:3)\n\n0\n\nk21(a,b](s) d(cid:22)m(cid:23)\n\ns\n\n= e [k2((cid:22)m(cid:23)\n\nb\n\n\u2212 (cid:22)m(cid:23)\n\n)],\n\na\n\nt\n\nk21(a,b](s) d(cid:22)m(cid:23)\n\ns\n\n.\n\nproof the continuity of the paths of n is clear. set n\u221e = k (mb\u2212 ma). since k is bounded\nand m is square integrable, e n 2\u221e < \u221e. we will show nt = e [n\u221e | ft], which will prove\nthat nt is a martingale.\nif t \u2265 b, then since k, mb, and ma are ft measurable,\n\ne [n\u221e | ft] = k (mb \u2212 ma) = nt .\n\nif a \u2264 t \u2264 b, k is ft measurable, and\n\ne [k (mb \u2212 ma) | ft] = ke [mb \u2212 ma | ft] = k (mt \u2212 ma) = nt .\n\nin particular, na = e [n\u221e | fa] = 0. finally, if t \u2264 a,\n\ne [n\u221e | ft] = e [e [n\u221e | fa] | ft] = 0 = nt .\n\n "}, {"Page_number": 84, "text": "66\n\nstochastic integrals\nfor e n 2\u221e, we have by( 9.2) with s = a and t = b,\n\ne n 2\u221e = e [k2(mb \u2212 ma)2] = e [k2e [(mb \u2212 ma)2 | fa] ]\n\u2212 (cid:22)m(cid:23)\n\n| fa] ] = e [k2((cid:22)m(cid:23)\n\n\u2212 (cid:22)m(cid:23)\n\na\n\nb\n\n)].\n\na\n\nto verify the formula for (cid:22)n(cid:23)\n\n= e [k2e [(cid:22)m(cid:23)\nt, let\n\nb\n\nl\u221e = k2(mb \u2212 ma)2 \u2212 k2((cid:22)m(cid:23)\nlt = k2(mb\u2227t \u2212 ma\u2227t )2 \u2212 k2((cid:22)m(cid:23)\n\n\u2212 (cid:22)m(cid:23)\nb\u2227t\n\nb\n\n),\n\na\n\n\u2212 (cid:22)m(cid:23)\n\na\u2227t\n\n.\n\nthen\n\nlt = n 2\n\nt\n\n\u2212\n\nt\n\nk21(a,b](s) d(cid:22)m(cid:23)\n\ns\n\n,\n\n(cid:3)\n\n0\n\nand we must show that lt is a martingale. to do this, it suffices to show lt = e [l\u221e | ft].\nif t \u2265 b, then l\u221e is ft measurable, so e [l\u221e | ft] = l\u221e = lt. if a \u2264 t \u2264 b, then\n\ne [l\u221e | ft] = k2e [(mb \u2212 ma)2 \u2212 ((cid:22)m(cid:23)\n\u2212 ((cid:22)m(cid:23)\n= k2e [m 2\n= k2e [m 2\n\u2212 ((cid:22)m(cid:23)\n= k2e [(mt \u2212 ma)2 \u2212 ((cid:22)m(cid:23)\n= lt ,\n\n\u2212 m 2\n\u2212 m 2\n\nb\n\na\n\nb\n\na\n\nt\n\nt\n\n\u2212 (cid:22)m(cid:23)\n\nb\n\na\n\n\u2212 (cid:22)m(cid:23)\n\u2212 (cid:22)m(cid:23)\n\u2212 (cid:22)m(cid:23)\n\na\n\nt\n\na\n\n) | ft]\n) | ft]\n) | ft]\n) | ft]\n\na\n\nusing (9.1) and (9.3) with the stopping times there being fixed positive real numbers. in\nparticular, e [l\u221e | fa] = la = 0. finally, if t \u2264 a,\n\ne [l\u221e | ft] = e [e [l\u221e | fa] | ft] = 0 = la\n\nas required.\n\nnext suppose\n\nhs(\u03c9) = j(cid:9)\n\nnt = j(cid:9)\n\nj=1\n\n(10.3)\nwhere each k j is fa j measurable and bounded. we may rewrite h so that the intervals (a j, b j]\nsatisfy a1 < b1 \u2264 a2 < b2 \u2264 \u00b7\u00b7\u00b7 \u2264 aj < bj . for example, if hs = k11(a1,b1] + k21(a2,b2] with\na1 < a2 < b1 < b2, we may rewrite hs as\n\nk j1(a j ,b j](s),\n\nj=1\n\nk11(a1,a2] + (k1 + k2)1(a2,b1] + k21(b1,b2].\n\ndefine\n\nk j (mt\u2227b j\n\n\u2212 mt\u2227a j\n\n).\n\n(10.4)\n\nwe need to check that rewriting hs so that a1 < b1 \u2264 a2 < \u00b7\u00b7\u00b7 < bj does not affect the value\nof nt, but this is routine.\n\n "}, {"Page_number": 85, "text": "proposition 10.3 with h as in (10.3) and n defined by (10.4), nt is a continuous martingale,\n\n10.1 construction\n\n67\n\nand\n\nproof by linearity, nt is a continuous martingale. we have\n\ne n 2\u221e = e\n\n(cid:3) \u221e\ne n 2\u221e = e\n(cid:3)\n\n0\n\n=\n\nt\n\n(cid:22)n(cid:23)\n(cid:16)(cid:9)\n\n(mb j\n\nh 2\nj\n\n(cid:16)(cid:9)\n\nj\n\n+ 2e\n\ni< j\n\ns d(cid:22)m(cid:23)\nh 2\n\n,\n\ns\n\nt\n\n0\n\n.\n\ns\n\ns d(cid:22)m(cid:23)\nh 2\n(cid:17)\n\n\u2212 ma j\n\n)2\n\nhih j (mbi\n\n\u2212 mai\n\n)(mb j\n\n\u2212 ma j\n\n(10.5)\n\n(10.6)\n\n(cid:17)\n\n)\n\n.\n\nthe cross terms vanish, because when i < j and we condition on fa j , we have\n\ne [hih j (mbi\n\n\u2212 mai\n\n)e [(mb j\n\n\u2212 ma j\n\n) | fa j ] ] = 0.\n\nfor the terms in the first sum in (10.6), by (9.3)\n)2] = e [h 2\n= e [h 2\n= e [h 2\n\n\u2212 ma j\n\ne [h 2\nj\n\n(mb j\n\nj\n\nj\n\ne [(mb j\ne [(cid:22)m(cid:23)\n([(cid:22)m(cid:23)\n\nb j\n\nb j\n\n\u2212 ma j\n\u2212 (cid:22)m(cid:23)\n\u2212 (cid:22)m(cid:23)\n\n)2 | fa j ] ]\n| fa j ] ]\n\na j\na j ])].\n\nj\n\n(cid:3) \u221e\n\n0\n\ne n 2\u221e = e\n(cid:3) \u221e\n\ntherefore\n\nthe argument for (cid:22)n(cid:23)\n\nt is similar.\n\nabove such that\n\n0\nto see that this can be done, define\n\ne\n\n\u2212 hs)2 d(cid:22)m(cid:23)\n(cid:3) \u221e\n\ns\n\n(h n\ns\n\n(cid:10)\n\n\u2192 0.\n\n(cid:11)1/2\n\nt d(cid:22)m(cid:23)\ny 2\n\nt\n\n0\n\n(cid:21)y(cid:21)2 =\n\ne\n\nnow suppose hs is predictable and (10.2) holds. choose h n\n\ns of the form given in (10.3)\n\ns d(cid:22)m(cid:23)\nh 2\n\n.\n\ns\n\n(10.7)\n\nfor y predictable. then (cid:21)y(cid:21)2 is an l2 norm on functions on [0,\u221e) \u00d7 \u0001, so by lemma 10.1\nwe can approximate h in this norm by processes of the form given in (10.3). (when h is\ns equal to hk/2n if k/2n < s \u2264 (k+1)/2n\nbounded, adapted, and has continuous paths, taking h n\n(cid:10)(cid:3) \u221e\nfor s < n and h n\ns\n(cid:3) \u221e\n\nby doob\u2019s inequalities we have\n\u2212 h m\n\n= 0 if s \u2265 n will work.)\n\n(cid:11)2(cid:17)\n\n(cid:10)(cid:3)\n\n(cid:11)2\n\n) dms\n\nsup\nt\u22650\n\n(h n\ns\n\n(h n\ns\n\n(cid:16)\n\ne\n\n0\n\n0\n\ns\n\nt\n\ns\n\n\u2212 h m\n\u2212 h m\n\ns\n\n) dms\n)2 d(cid:22)m(cid:23)\n\ns\n\n\u2192 0.\n\n\u2264 4e\n= 4e\n\n(h n\ns\n\n0\n\n "}, {"Page_number": 86, "text": "68\n\nthe norm\n\n|nt \u2212(cid:15)\n\nstochastic integrals\n\n(cid:21)y(cid:21)\u221e = (e [sup\n\nt\n\n|yt|2])1/2\n\n(10.8)\n\n(cid:11)2 = e\n(cid:17)\n(cid:17)(cid:20)(cid:20)(cid:20) \u2264 e\n(cid:10)\n\n\u2264\n\ne\n\n0\n\n(cid:16)(cid:3)\n\n0\n\n(cid:20)(cid:20)(cid:20)(cid:3)\n(cid:10)(cid:3)\n\n0\n\nt\n\n0\n\n(cid:3)\n\n0\n\n(cid:3)\n\nis complete; this was shown in exercise 9.5. thus there exists a process nt such that\nsupt\u22650\n\ns dms| \u2192 0 in l2.\nt\n(cid:10)(cid:3)\n0 h n\ns are two sequences converging in the (cid:21) \u00b7 (cid:21)2 norm to h , then\n\u2192 0,\n\ns and h n\n\n\u2212 h n\n\n\u2212 h n\n\nif h n\n\n(cid:3)\n\ne\n\nt\n\nt\n\n) dms\n\n(h n\ns\n\n(h n\ns\n\ns\n\ns\n\nor the limit is independent of which sequence h n we choose.\nit is easy to see, because of the l2 convergence, that nt is a martingale: if a \u2208 fs, then\n\nt\n\nr dmr; a\nh n\n\n= e\n\ns\n\nr dmr; a\nh n\n\n0\n\n(cid:16)(cid:3)\n\ne\n\n0\n\nby proposition 10.3. now use that\n\n(cid:16)(cid:3)\n\n(cid:20)(cid:20)(cid:20)e\n\nt\n\nr dmr \u2212 nt; a\nh n\n\n0\n\ns\n\n)2 d(cid:22)m(cid:23)\n(cid:17)\n\n(cid:20)(cid:20)(cid:20)\n\nr dmr \u2212 nt\nh n\nr dmr \u2212 nt\nh n\n\nt\n\n(cid:11)2(cid:11)1/2 \u2192 0\n\nand similarly with t replaced by s.\n\nsimilar arguments using the l2 convergence show that\ns d(cid:22)m(cid:23)\nh 2\n\n= e\n\ne n 2\nt\n\nt\n\ns\n\n,\n\n(10.9)\n\nand\n\n|nt \u2212(cid:15)\n\n(cid:22)n(cid:23)\n\n=\n\nt\n\ns d(cid:22)m(cid:23)\nh 2\n\nt\n\n(10.10)\ns dms| \u2192 0 in l2, there exists a subsequence {nk} such that the\n\n0\n\ns\n\n.\n\nbecause supt\u22650\nconvergence takes place almost surely, that is\n\nt\n0 h n\n\n(cid:20)(cid:20)(cid:20)(cid:3)\n\nsup\nt\u22650\n\nt\n\ns dms \u2212 nt\nh nk\n\n0\n\n(cid:20)(cid:20)(cid:20) \u2192 0,\n\na.s.\n\nt\n0 h n\ns dms has continuous paths, with probability one, nt has continuous paths.\nt\n0 hs dms and call nt the stochastic integral of h with respect to m.\n\nwe summarize our construction as follows.\ntheorem 10.4 suppose the filtration {ft} satisfies the usual conditions and mt is a square\nintegrable martingale with continuous paths. suppose h is of the form\n\n(cid:15)\nwe write nt =(cid:15)\n\nsince each\n\nj(cid:9)\n\nk j (\u03c9)1(a j ,b j](s),\n\ni=1\n\n(10.11)\n\n "}, {"Page_number": 87, "text": "where each k j is bounded and fa j measurable. in this case define\n\n10.2 extensions\n\n69\n\nif h is predictable and\n\nchoose h n of the form given in (10.11) with e\n\n\u2192 0, and define\n\ns\n\nto be the limit with respect to the norm (10.8) of\nmartingale,\n\nt\n0 h n\n\ns dms. then nt is a continuous\n\n(cid:3)\n\nt\n\n0\n\nhs dms = j(cid:9)\n(cid:3) \u221e\n\nj=1\n\nk j (mt\u2227b j\n\n\u2212 mt\u2227a j\n\n).\n\ne\n\n0\n\n(cid:15) \u221e\ns d(cid:22)m(cid:23)\nh 2\n(cid:3)\n\n0\n\nt\n\n< \u221e,\n\ns\n\n(h n\ns\n\nhs dms\n\n0\n\n\u2212 hs)2 d(cid:22)m(cid:23)\n(cid:15)\n\nnt =\n\n(cid:3) \u221e\ne n 2\u221e = e\n(cid:3)\n\n0\n\ns d(cid:22)m(cid:23)\nh 2\n\n,\n\ns\n\n(cid:22)n(cid:23)\n\nt\n\n=\n\nt\n\ns d(cid:22)m(cid:23)\nh 2\n\n.\n\ns\n\n0\n\nand\n\nmoreover the definition of nt is independent of the particular choice of the h n.\n\n10.2 extensions\n\n(cid:3) \u221e\n\nthere are some extensions of the definition that are fairly routine.\nextension 1. if\n\nt\n\ns\n\n0\n\na.s.,\n\n(cid:19)\n\n< \u221e,\n\ntn = inf\n\nbut without the expectation being finite, let\n\ns d(cid:22)m(cid:23)\nh 2\n(cid:15)\n= mt\u2227tn is a square integrable martingale with (cid:22)m\n(cid:3)(cid:23)\n\ns d(cid:22)m(cid:23)\nh 2\n(cid:3)\n(cid:18)\n(cid:15)\n\u2264 n.\n0 hs dms\u2227tn if t \u2264 tn . if t \u2264 tk \u2264 tn , we need to\n0 hs d(cid:22)m(cid:23)\nt\u2227tn , so that our definition is consistent. this is part\n\n(cid:3)\nm\nt\ndefine\ncheck that\nof exercise 10.2.\nextension 2. if mt is a continuous local martingale (see section 9.1 for the definition), let\nsn = inf{t : |mt| \u2265 n}. by exercise 9.3, mt\u2227sn will be a uniformly integrable martingale,\nand in fact, since mt\u2227sn is bounded, it is square integrable. for t \u2264 sn we set\n\n(cid:15)\n0 hs d(cid:22)m(cid:23)\n\nt\n0 hs dms to be the quantity\n\n> n\n= (cid:22)m(cid:23)\n\n=(cid:15)\n\ns d(cid:22)m\n\nt\u2227tn , so\n\nt\n0 h 2\n\nt\u2227tk\n\n(cid:15)\n\n(cid:3)(cid:23)\n\nt :\n\n.\n\n0\n\ns\n\nt\n\nt\n\nt\n\nt\n\nt\n\n(cid:3)\n\n(cid:3)\n\nand (cid:22)m(cid:23)\n\nt\n\n= (cid:22)m(cid:23)\n\nt\u2227sn\n\n. again there is consistency to check, which is also part of exercise 10.2.\n\nt\n\nhs dms =\n\nt\n\nhs dms\u2227sn\n\n0\n\n0\n\n "}, {"Page_number": 88, "text": "70\n(cid:15) \u221e\nextension 3. suppose that xt = mt + at is a semimartingale with continuous paths, so\nthat m is a local martingale and a is a process with paths locally of bounded variation. if\n0 h 2\n\n|hs||das| < \u221e, we define\n\n+(cid:15) \u221e\n\nstochastic integrals\n\ns d(cid:22)m(cid:23)\n\n0\n\ns\n\n(cid:3)\n\n(cid:3)\n\n(cid:3)\n\nt\n\nhs dxs =\n\nt\n\nhs dms +\n\nt\n\nhs das,\n\nwhere the first integral on the right is a stochastic integral and the second is a lebesgue\u2013\nstieltjes integral.\n\n0\n\n0\n\n0\n\nfor a semimartingale, we define\n\n(cid:22)x (cid:23)\n\nt\n\n= (cid:22)m(cid:23)\n\n.\n\nt\n\n(10.12)\n\ngiven two semimartingales x and y we define (cid:22)x , y(cid:23)\n\u2212 (cid:22)x (cid:23)\n\n(cid:22)x , y(cid:23)\n\n2 [(cid:22)x + y(cid:23)\n\n= 1\n\nt\n\nt\n\nt\n\nt by:\n\n\u2212 (cid:22)y(cid:23)\nt].\n\n10.1 prove (10.5) in proposition 10.3.\n\nexercises\n\n(cid:3) \u221e\n\n10.2 check the consistency of the first two extensions of the definition of stochastic integrals.\n\n10.3 show that if m is a continuous square integrable martingale, and t a finite stopping time, then\n\n1[0,t ] dms = mt .\n\n10.4 show that if nt = (cid:15)\n(cid:15) \u221e\nt\n0 hs dms where m is a continuous square integrable martingale, h is\nt\n0 h 2\n0 ks dns, where k is predictable and\ns d(cid:22)n(cid:23)s < \u221e, then\n\ns d(cid:22)m(cid:23)s < \u221e, and lt = (cid:15)\n\npredictable, and e\ne\n\n(cid:15) \u221e\n\n0 k2\n\n0\n\n10.5 show that if m, h , and n are as in exercise 10.4, then (cid:22)m, n(cid:23)t =(cid:15)\n\nhint: derive a formula for (cid:22)n + m(cid:23)t from the fact that\n\nhsks dms.\n\n0\n\nlt =\n\nt\n\n0 hs d(cid:22)m(cid:23)s.\n\nnt + mt =\n\nt\n\n(1 + hs ) dms.\n\n0\n\nand nt =(cid:15)\n\nt\n0 hs dms. show that\n\n10.6 suppose that m and l are square integrable martingales, h is predictable and satisfies (10.2),\n\n(cid:22)n, l(cid:23)t =\n\nt\n\nhs d(cid:22)m, l(cid:23)s.\n\n0\n\n(10.13)\n\nsometimes the stochastic integral of h with respect to m is defined to be the square integrable\nmartingale n for which (10.13) holds for all square integrable martingales l.\n\n10.7 show that if m and n are square integrable martingales with continuous paths, then\n\n(cid:22)m, n(cid:23)t \u2264 ((cid:22)m(cid:23)t )1/2((cid:22)n(cid:23)t )1/2.\n\nhint: imitate an appropriate proof of the cauchy\u2013schwarz inequality. this result is a special\n\ncase of the inequality of kunita\u2013watanabe.\n\n(cid:3)\n\nt\n\n(cid:3)\n\n(cid:3)\n\n "}, {"Page_number": 89, "text": "11\n\nit\u02c6o\u2019s formula\n\nthe most important result in the theory of stochastic integration is it\u02c6o\u2019s formula. this is also\nknown as the change of variables formula.\nlet ck be the functions that are k times continuously differentiable and ck\nck such that the function and its ith-order derivatives are bounded for i \u2264 k.\ntheorem 11.1 let xt be a semimartingale with continuous paths and suppose f \u2208 c2. then\nfor almost every \u03c9\n\nb those functions\n\n(cid:3)\n\n(cid:3)\n\nt\n\n(cid:3)(cid:3)(xs) d(cid:22)x (cid:23)\n\n,\n\ns\n\nt \u2265 0.\n\n(11.1)\n\nf (xt ) = f (x0) +\n\nt\n\n(cid:3)(xs) dxs + 1\n\nf\nstep 1 will be to reduce to the case when f \u2208 c3\n\nf\n\n2\n\n0\n\n0\n\nb and x has appropriate boundedness\nconditions. step 2 is a use of taylor\u2019s formula; see (11.2). step 3 shows that each term\nconverges to the appropriate quantity, and step 4 removes the restriction that f be in c3\nb .\n\nstep 1. if xt = mt + at is the decomposition of x into a local martingale m and a\nproof\nprocess a that has paths locally of bounded variation, let vt be the total variation of a up to\n\ntime t: vt =(cid:15)\n\n|das|. let\n\nt\n0\n\ntn = inf{t : |mt| > n or (cid:22)m(cid:23)\n\nt\n\n> n or vt > n}.\n\nby the continuity of paths, tn \u2192 \u221e, a.s., as n \u2192 \u221e, so for almost every \u03c9 and for each\nt, t \u2227 tn = t for n large enough. since it\u02c6o\u2019s formula is a path-by-path result, it suffices to\nprove it\u02c6o\u2019s formula for xt\u2227tn for each n, or what amounts to the same thing, we may take n\narbitrary and assume mt, (cid:22)m(cid:23)\nt, at, and vt are all bounded by n. in this case, xt is bounded\nby 2n.\noutside of [\u22122n, 2n] without affecting\nthe validity of it\u02c6o\u2019s formula. therefore we will also assume f \u2208 c2 with compact support.\nlet us temporarily assume in addition that f\nexists and is continuous; we will remove this\nlast assumption later on.\n\nsince x is bounded, we may modify f , f\n\n, and f\n\n(cid:3)(cid:3)(cid:3)\n\n(cid:3)(cid:3)\n\n(cid:3)\n\nlet t0 > 0, \u03b5 > 0, s0 = 0, and define\n\nsi+1 = si+1(\u03b5) = inf{t > si :|mt \u2212 msi\n\n| > \u03b5 or (cid:22)m(cid:23)\n\n\u2212 (cid:22)m(cid:23)\n\nor vt \u2212 vsi\n\nt\n\n> \u03b5} \u2227 t0.\n\n> \u03b5\n\nsi\n\nnote si = t0 for i sufficiently large (how large depends on \u03c9) by the continuity of the\npaths.\n\n71\n\n "}, {"Page_number": 90, "text": "[ f (xsi+1\n\n\u221e(cid:9)\n\u221e(cid:9)\n(cid:3)(xsi\n\u221e(cid:9)\n\ni=0\n\nf\n\ni=0\n+\n\nri,\n\ni=0\n\n(cid:3)\n\ne\n\nt0\n\n0\n\n\u2212 f\n\n(cid:3)(xs)) dms\n(cid:3)\n\nt0\n\n0\n\nstep 2. the key idea to proving it\u02c6o\u2019s formula is taylor\u2019s theorem. we write\n\n72\n\n) \u2212 f (x0) =\n\nf (xt0\n\n=\n\nit\u02c6o\u2019s formula\n\n) \u2212 f (xsi\n\n)]\n\n)(xsi+1\n\n\u2212 xsi\n\n) + 1\n\n2\n\n(11.2)\n\n\u221e(cid:9)\n\ni=0\n\n(cid:3)(cid:3)(xsi\n\nf\n\n)(xsi+1\n\n\u2212 xsi\n\n)2\n\nwhere ri is the remainder term. we have |ri| \u2264 c(cid:21) f\n\nstep 3. let us first look at the terms with f\n(cid:3)\nand xs, we see that h \u03b5\n\u2212 f\n(cid:3)(xs)| dvs \u2192 0 boundedly, hence\n\nby the continuity of f\n|h \u03b5\nin particular,\n\n(cid:3)(cid:3)(cid:3)(cid:21)\u221e|xsi+1\nin them. let h \u03b5\ns\n\n|3.\n\u2212 xsi\n= f\n(cid:3)(xsi\n\n(cid:3)\n\n(cid:15)\n\nt0\n0\n\ns converges boundedly and pointwise to f\n\n) if si \u2264 s < si+1.\n(cid:3)(xs).\n\n(cid:3)(xs)| dvs \u2192 0.\n(cid:3)\n\n|h \u03b5\n\n\u2212 f\n\ns\n\n(cid:11)2 = e\n\nt0\n\n|h \u03b5\n\ns\n\n0\n\n\u2192 0\n\ns\n\n\u2212 f\n(cid:3)(xs)|2 d(cid:22)m(cid:23)\n(cid:3)\n\n(cid:3)(xs) (dms + das),\n\nt0\n\nf\n\n0\n\n)(asi+1\n\n\u2212 asi\n\n\u2212 msi\n(cid:3)\n\n) + (asi+1\n) is bounded in absolute value by\n| \u2264 \u03b5(cid:21) f\n(cid:3)(cid:3)(cid:21)\u221en,\n\ndvs \u2264 \u03b5(cid:21) f\n\nt0\n\n\u2212 asi\n\n)2.\n\nalso,\n\n(cid:10)(cid:3)\n\ne\n\n(h \u03b5\ns\nas \u03b5 \u2192 0. we then have\n\n0\n\n(cid:9)\n\ni\n\ns\n\nt0\n\n(cid:3)\n\n(cid:3)(xsi\n\nf\n\n)(xsi+1\n\n\u2212 xsi\n\n) =\n\n(dms + das) \u2192\n\nh \u03b5\ns\n\nnote\n\nwhich leads to the f\n\nterm in it\u02c6o\u2019s formula.\n\ni\n\nf\n\n)(asi+1\n\n(cid:3)(cid:3)(xsi\n\n(cid:12)\n\n(xsi+1\ni f\n\n(cid:3)(cid:3)\nnext let us look at the f\nterms. we can write\n\u2212 xsi\n)2 = (msi+1\n\u2212 msi\n)2 + 2(msi+1\n(cid:9)\n\u2212 asi\n\u2212 msi\n(cid:3)(cid:3)(xsi\n)(msi+1\n)(asi+1\n\u03b5(cid:21) f\n(cid:3)(cid:3)(cid:21)\u221e|asi+1\n\u2212 asi\n(cid:9)\n\u2212 asi\n(cid:9)\n= (cid:22)m(cid:23)\n(cid:9)\n\nterm. we thus need to show that\n\u2212 msi\n\n\u2212 (cid:22)m(cid:23)\n\n\u2212 msi\n\n)((cid:22)m(cid:23)\n\n)[(msi+1\n\n(cid:3)(cid:3)(xsi\n\n(cid:3)(cid:3)(xsi\n\n(cid:3)(cid:3)(xsi\n\n)(msi+1\n\nsi+1\n\n1\n2\n\nsi\n\nf\n\nf\n\ni\n\ni\n\nt\n\nby an argument very similar to the one for the f\n\nand since (cid:22)x (cid:23)\ncorrect f\n\n(cid:3)(cid:3)\n\n(cid:3)(cid:3)(cid:21)\u221e\n(cid:3)\n\ni\n\n0\n\n(cid:3)\n\n(cid:3)\n\nterms,\nt0\n\n) \u2192 1\n\n2\n\n0\n\n(cid:12)\nwhich goes to 0 as \u03b5 \u2192 0; this follows from the definition of si. similarly the expression\n\n0\n\n)2 also goes to 0. therefore we need to show\n(cid:3)(cid:3)(xs) d(cid:22)x (cid:23)\n\n)2 \u2192\n\nt0\n\nf\n\nf\n\n.\n\ns\n\n(cid:3)(cid:3)(xs) d(cid:22)m(cid:23)\n\nf\n\n,\n\ns\n\n(11.3)\n\nt for semimartingales (see (10.12)), the right-hand side of (11.3) is the\n\n)2 \u2212 ((cid:22)m(cid:23)\n\n\u2212 (cid:22)m(cid:23)\n\nsi\n\n)] \u2192 0\n\nsi+1\n\n(11.4)\n\n "}, {"Page_number": 91, "text": "as \u03b5 \u2192 0.\n\nwe will show\n\nwhere\n\nwe have\n\nif i < j, then\n\nit\u02c6o\u2019s formula\n\n(cid:10) \u221e(cid:9)\n\ne\n\nbi\n\ni=0\n\n(cid:11)2 \u2192 0,\n\n73\n\n(11.5)\n\nbi = f\n\n(cid:3)(cid:3)(xsi\n\n)[(msi+1\n\nsi+1\n\n)2 \u2212 ((cid:22)m(cid:23)\n\u2212 msi\n(cid:9)\n(cid:9)\n\n+ 2\n\nb2\ni\n\n(cid:11)2 = e\n\n\u2212 (cid:22)m(cid:23)\n\n)].\n\nsi\n\nbib j.\n\ni\n\ni< j\n\n(cid:10)(cid:9)\n\ne\n\nbi\n\ni\n\ne [bib j] = e [bie [b j | f si+1] ].\n\nby (9.2) and the fact that si+1 \u2264 s j, we see that\n\u2212 ms j\n\ne [b j | fsi+1] = f\n\n)e [(ms j+1\n\n(cid:3)(cid:3)(xs j\n\ntherefore to prove (11.5) it remains to show e\n\nso the cross-products vanish.\n(cid:9)\ninequality (x + y)2 \u2264 2x2 + 2y2. since f\n(cid:3)(cid:3)(cid:21)2\u221e\n+ 2(cid:21) f\n\n(cid:9)\n\n\u2264 2(cid:21) f\n\nb2\ni\n\ne\n\n(cid:3)(cid:3)\n\ni\n\ni\n\n(cid:3)(cid:3)(cid:21)2\u221e\n\n(cid:9)\n\ne [(msi+1\n\nis bounded,\n\n\u2212 msi\ne [((cid:22)m(cid:23)\n\nsi+1\n\n)2 \u2212 ((cid:22)m(cid:23)\n(cid:12)\n\ni b2\ni\n\n\u2212 (cid:22)m(cid:23)\n\ns j\n\n) | fsi+1] = 0,\n\ns j+1\n\n\u2192 0 as \u03b5 \u2192 0. we use the easy\n\n)4]\n\u2212 (cid:22)m(cid:23)\n\n)2].\n\nsi\n\n(11.6)\n\ni\n\n(cid:9)\n\nthe first sum on the right-hand side of (11.6) is bounded by\n)2] = 2\u03b52(cid:21) f\n\u2264 8\u03b52(cid:21) f\n\n\u2212 msi\n\n2\u03b52(cid:21) f\n\ne [(msi+1\n\n(cid:3)(cid:3)(cid:21)2\u221e\n\ni\n\n(cid:3)(cid:3)(cid:21)2\u221ee [m 2\n(cid:3)(cid:3)(cid:21)2\u221en 2.\n\nt0\n\n\u2212 m 2\n0 ]\n\nthe second sum on the right-hand side of (11.6) is bounded by\n(cid:3)(cid:3)(cid:21)2\u221ee(cid:22)m(cid:23)\n\ne [((cid:22)m(cid:23)\n\n\u2212 (cid:22)m(cid:23)\n\n2\u03b5(cid:21) f\n\n(cid:3)(cid:3)(cid:21)2\u221e\n\nsi+1\n\n\u2264 2\u03b5(cid:21) f\n\n(cid:3)(cid:3)(cid:21)2\u221en.\n\nt0\n\n(cid:9)\n\ni\n\nsi] \u2264 2\u03b5(cid:21) f\n(cid:12)\n\ni b2\ni\n\nboth of these tend to 0 as \u03b5 \u2192 0. therefore e\nfor the f\n\nterm is complete.\n\n(cid:3)(cid:3)\n\n\u2192 0, and the proof of the convergence\n\n "}, {"Page_number": 92, "text": "(cid:12)\n\ni\n\n\u2212xsi\n\n)2\n\n(xsi+1\n\n74\n\nit\u02c6o\u2019s formula\n\nthe final terms to examine are the remainder terms. we have shown that e\n\nremains bounded as \u03b5 \u2192 0. since\n\n|ri| \u2264 c\u03b5(cid:21) f\n\n(cid:3)(cid:3)(cid:3)(cid:21)\u221e(xsi+1\n\n\u2212 xsi\n\n)2,\n\n|ri| \u2192 0 as \u03b5 \u2192 0.\n\ni\n\nstep 4. to finish up, we remove the assumption that f \u2208 c3. (we still assume that f \u2208 c2\n(cid:3)(cid:3)\n, and f\nm\n, respectively. apply it\u02c6o\u2019s formula with fm and then let\n(cid:3)\nm terms\n\nwe see e\nwith compact support.) take a sequence { fm} of c3 functions such that\nconverge uniformly to f , f\nm \u2192 \u221e. the terms fm(xt ) and fm(x0) clearly converge to f (xt ) and f (x0). the f\nconverge because\n\n, and f\n\nfm, f\n\n(cid:3)\nm\n\n(cid:3)(cid:3)\n\n(cid:3)\n\n(xs) \u2212 f\n\n(cid:3)\nm\n\n( f\n\n(cid:3)(xs)) dms\n\nt0\n\n| f\n\n(cid:3)\nm\n\n(xs) \u2212 f (xs)|2 d(cid:22)m(cid:23)\n\ns\n\n\u2192 0\n\n(cid:12)\n\nt0\n\n(cid:10)(cid:3)\n(cid:20)(cid:20)(cid:20)(cid:3)\n\ne\n\n0\n\n0\n\ne\n\nand\n\nt0\n\n(xs) \u2212 f\n\n(cid:3)\nm\n\n( f\n\n(cid:3)(xs)) das\n\nt0\n\n| f\n\n(cid:3)\nm\n\n(xs) \u2212 f\n\n(cid:3)(xs)| dvs \u2192 0\n\nas m \u2192 \u221e. the f\n(cid:3)(cid:3)\nm terms converge by dominated convergence. this shows that (11.1) holds\nfor each t0, except for a null set nt0 depending on t0. let n = \u222at\u2208q+nt, where q+ denotes the\nnon-negative rationals. if \u03c9 /\u2208 n, then (11.1) holds for every t0 rational. each term in (11.1)\nis continuous, a.s. (with a null set n\n,( 11.1)\nholds for all t0.\n\nindependent of t0). therefore if \u03c9 /\u2208 n \u222a n\n\n(cid:3)\n\n(cid:3)\n\nthere is a multivariate version of it\u02c6o\u2019s formula, which is proved in a very similar way:\n\ntheorem 11.2 suppose x 1\nt\nand f is a c2 function on rd. then with probability one,\n\n, . . . , x d\n\nt are continuous semimartingales, xt = (x 1\n\nt\n\n(cid:11)2 = e\n(cid:20)(cid:20)(cid:20) \u2264 e\n\n(cid:3)\n(cid:3)\n\n0\n\n0\n\n(cid:3)\nd(cid:9)\nd(cid:9)\n\ni=1\n\n0\n\nt\n\nt\n\n0\n\ni, j=1\n\nf (xt ) = f (x0) +\n(cid:3)\n\n+ 1\n\n2\n\n\u2202 f\n\u2202xi\n\n(xs) dx i\ns\n\n\u2202 2 f\n\u2202xi\u2202x j\n\n(xs) d(cid:22)x i, x j(cid:23)\n\ns\n\n, . . . , x d\nt\n\n),\n\n(11.7)\n\nfor all t \u2265 0.\n\nis very useful.\n\nthe following is known as the integration by parts formula or it\u02c6o\u2019s product formula, and\n\ncorollary 11.3 if x and y are semimartingales with continuous paths, then\n\nxtyt = x0y0 +\n\nt\n\nys dxs + (cid:22)x , y(cid:23)\n\nt\n\n.\n\nproof by it\u02c6o\u2019s formula,\n\n= x 2\n\n0\n\nx 2\nt\n\n+ 2\n\nt\n\nxs dxs + (cid:22)x (cid:23)\n\n.\n\nt\n\n0\n\n(cid:3)\n\n0\n\n(cid:3)\n\nt\n\n0\n\nxs dys +\n(cid:3)\n\n "}, {"Page_number": 93, "text": "75\nthe analogous formula holds when x is replaced by y and when x is replaced by x + y .\nwe then use\n\nexercises\n\nxtyt = 1\nsubstituting the formulas for x 2\nt , y 2\ndoing some algebra yields our result.\n\n2 [(xt + yt )2 \u2212 x 2\nt , and (xt + yt )2 that we obtained from it\u02c6o\u2019s formula and\n\n\u2212 y 2\nt ];\n\nt\n\n11.1 suppose wt is a brownian motion and a \u2208 r. show that the amount of time brownian motion\n\nexercises\n\nspends at the point a is zero, i.e., that(cid:3)\n\nt\n\n1{a}(ws ) ds = 0,\n\na.s.\n\n0\n\nfor all t > 0.\n\n11.2 let a < b and let fa,b be the c1 function such that fa,b(0) = f\n\n(0) = 0 and\n\n(cid:3)\na,b\n\n(cid:3)\na,b\n\nf\n\nx\n\n0\n\n1[a,b](y) dy,\n\nx \u2208 r.\n\nin other words, fa,b is the function whose second derivative is 1[a,b], except that the second\nderivative is not defined at a and b. show it\u02c6o\u2019s formula holds for fa,b:\n\n(cid:3)\n\nfa,b(wt ) =\n\n(cid:3)\na,b\n\n(ws ) dws + 1\n\n2\n\nt\n\n0\n\n1[a,b](ws ) ds.\n\n(cid:15)\n\nt\n0\n\n(ws )k ds for\n\n11.3 if wt is a brownian motion, a > 0, and t = inf{t > 0 : |wt| = a}, calculate e\n\neach non-negative integer k. also calculate\n\n(cid:3)\n\nt\n\ne\n\n0\n\n1[b1,b2](ws ) ds\n\nif [b1, b2] \u2282 [\u2212a, a].\n\n11.4 let w be a brownian motion, let t0 < t1 < \u00b7\u00b7\u00b7 < tn = 1, and let\n)2 \u2212 (ti \u2212 ti\u22121 ).\n\n\u2212 wti\u22121\n\nshow there exists a constant c1 not depending on {t0, . . . , tn} such that\n\nbi = (wti\n(cid:10) n(cid:9)\n\ne\n\nbi\n\ni=1\n\n(cid:11)2 \u2264 c1 max\n\n1\u2264i\u2264n\n\n|ti \u2212 ti\u22121|.\n\n11.5 use exercise 11.4 and the borel\u2013cantelli lemma to prove that if w is a brownian motion, then\n\n2n(cid:9)\n\nlim\nn\u2192\u221e\n\nk=1\n\n(wk/2n \u2212 w(k\u22121)/2n )2 = 1,\n\na.s.\n\n(cid:3)\n\n(x) =\n(cid:3)\n\nt\n\nf\n\n0\n\n "}, {"Page_number": 94, "text": "76\n\nit\u02c6o\u2019s formula\n\n11.6 in our proof of it\u02c6o\u2019s formula, the use of stopping times simplifies the proof considerably. this\nexercise considers a proof of it\u02c6o\u2019s formula using fixed times. suppose m is a bounded continuous\nmartingale, a is a continuous process whose paths have total variation bounded by n > 0, a.s.,\nand xt = mt + at.\n(1) writing [x] for the integer part of x, prove that for each t,\n\n[2nt]+1(cid:9)\n\ni=1\n\n(x(i+1)/2n \u2212 xi/2n )2\n\nconverges in probability to (cid:22)x (cid:23)t.\n[2nt]+1(cid:9)\n(2) prove that if f is a c2 function whose second derivative is bounded, then\n\nf\n\ni=1\n\nconverges in probability to\n\n(cid:3)(cid:3)(xi/2n )(x(i+1)/2n \u2212 xi/2n )2\n(cid:3)\n\n(cid:3)(cid:3)(xs ) d(cid:22)xs(cid:23).\n\nt\n\nf\n\n0\n\nsince the increments of m and a are not uniformly bounded by something small, this is much\nharder than the proof of theorem 11.1 given in this chapter.\n\n11.7 here is an alternate way to prove it\u02c6o\u2019s formula.\n\n(1) suppose x = m + a, where m and a are as in exercise 11.6. write\n\n\u2212 x 2\n\n0\n\nx 2\nt\n\n= [t2n]\u22121(cid:9)\n= [t2n]\u22121(cid:9)\n\ni=0\n\ni=0\n\n(i+1)/2n \u2212 x 2\n(x 2\n\ni/2n )\n\n2xi/2n (x(i+1)/2n \u2212 xi/2n ) + [t2n]\u22121(cid:9)\n\n(x(i+1)/2n \u2212 xi/2n )2.\n\ni=0\n\nuse exercise 11.6 to show that it\u02c6o\u2019s formula holds when f (x) = x2.\n(2) derive the it\u02c6o product formula. then use induction to show that it\u02c6o\u2019s formula holds when\nf (x) = xn, n a positive integer.\n(3) given f \u2208 c2, find polynomials pm such that pm, p\n(cid:3), f\n(cid:3)(cid:3)\n,\nrespectively, on a compact interval as m \u2192 \u221e. apply it\u02c6o\u2019s formula for pm and show that one\ncan take limits to derive it\u02c6o\u2019s formula for f .\n\n(cid:3)(cid:3)\nm converge uniformly to f , f\n\n, p\n\n(cid:3)\nm\n\n "}, {"Page_number": 95, "text": "12\n\nsome applications of it\u02c6o\u2019s formula\n\nwe will be using it\u02c6o\u2019s formula throughout the book. in this chapter we give some applications,\neach of which will turn out to be quite useful.\n\n12.1 l\u00b4evy\u2019s theorem\n\nt to be equal to (cid:22)m(cid:23)\n\nthe following is known as l\u00b4evy\u2019s theorem. recall that if m is a local martingale with\ncontinuous paths and tn = inf{t : |mt| \u2265 n}, we defined (cid:22)m(cid:23)\nt\u2227tn if\nt \u2264 tn ; see section 10.2. moreover, by exercise 9.3, mt\u2227n is a square integrable martingale\nfor each n.\ntheorem 12.1 let mt be a continuous local martingale with respect to a filtration {f t}\nsatisfying the usual conditions such that m0 = 0 and (cid:22)m(cid:23)\n= t. then mt is a brownian\nmotion with respect to {ft}.\n= ft+t0. it is routine to check that nt is a\nproof fix t0 and let nt = mt+t0\n= t. note f(cid:3)\nmartingale with respect to f(cid:3)\n0 will not be the trivial \u03c3 -field in\ngeneral. we see that\n\u2212 e m 2\n\nt\n\nt\n\nt0\n\nif f is a function mapping the reals to the complex numbers, we may still use it\u02c6o\u2019s formula:\njust apply it\u02c6o\u2019s formula to the real and imaginary parts of f . doing this for f (x) = eiux,\nwhere u and x are real, we have\n\ne n 2\nt\n\nt\n\n\u2212 mt0, f(cid:3)\nt and that (cid:22)n(cid:23)\n= e m 2\nt+t0\n(cid:3)\n\neiunt = 1 + iu\n(cid:3)\nif we take tk = inf{t : |nt| \u2265 k}, then\n\nt\n\neiuns dns \u2212 u2\n2\n\n0\n\neiunt\u2227tk = 1 + iu\n\nt\u2227tk\n\n0\n\neiuns dns \u2212 u2\n2\n\n0\n\ntake a \u2208 f(cid:3)\nmartingale, so this term will have 0 expectation. then let k \u2192 \u221e, and we are left with\n\n0, multiply (12.2) by 1a, and take expectations. the stochastic integral is a\n\neiuns ds.\n\n(12.2)\n\neiuns ds.\n\n(12.1)\n\n= t < \u221e.\n(cid:3)\n(cid:3)\n\n0\n\nt\n\nt\u2227tk\n\ne [eiunt; a] = p(a) \u2212 u2\n2\n\nt\n\ne [eiuns; a] ds.\n\n(12.3)\n\n(cid:3)\n\n0\n\nwe used the fubini theorem here. (the reason we introduced the stopping time tk is that\nnt\u2227tk is a square integrable martingale, and hence the stochastic integral is a martingale. we\nmight run into integrability problems if we worked with (12.1) instead of (12.2).)\n\n77\n\n "}, {"Page_number": 96, "text": "78\n\nsome applications of it\u02c6o\u2019s formula\n\nwrite j (t ) = e [eiunt; a], so we have\n\n(cid:3)\n\n(12.4)\n\nt\n\nj (t ) = p(a) \u2212 u2\n2\n(cid:3)(t ) = \u2212 u2\n\n0\n\nj (s) ds.\n\nsince j is bounded, (12.4) shows that j is continuous. since j is continuous, using (12.4)\n\n2 j (t ) with j (0) = p(a). the only\n\nagain shows that j is differentiable. hence j\nsolution to this ordinary differential equation is\n\nj (t ) = p(a)e\nif we set a = \u0001, this tells us that e eiunt = e\n\ncharacteristic functions (theorem a.48), mt+t0\nwith variance t. equation (12.5) also tells us that\n\n\u2212u2t/2.\n(12.5)\n\u2212u2t/2, and by the uniqueness theorem for\n\u2212 mt0 is a mean zero normal random variable\nfunction with compact support. the fourier transform (cid:2)f (u)\n\ne [eiunt; a] = e [eiunt ]p(a)\n\n(12.6)\n\n\u221e\n\n0. let f be a c\n\nwhen a \u2208 f(cid:3)\nwill be in the schwartz class; see section b.2. replacing u by \u2212u in (12.6), multiplying the\n\nresulting equation by(cid:2)f (u), and integrating over u \u2208 r, we have\n(cid:3) (cid:2)f (u)e [e\n\n(cid:3) (cid:2)f (u)e [e\n\n\u2212iunt; a] du =\n\n\u2212iunt ] du p(a).\n\nusing the fubini theorem and the fourier inversion theorem, and dividing by a constant, we\nconclude\n\nsince (cid:2)f is in the schwartz class, integrability is not a problem when applying the fubini\n\ne [ f (nt ); a] = e [ f (nt )]p(a).\n\ntheorem. a limit argument shows that this equation holds with f equal to 1b, where b is a\nborel subset of r, hence\n\np(mt+t0\n\n\u2212 mt0\n\n\u2208 b, a) = p(mt+t0\n\n\u2212 mt0\n\n\u2208 b) p(a).\n\nthis shows the independence of mt+t0\nprocess starting at 0 with mt+t0\nt independent of ft0, and therefore m is a brownian motion.\n\n\u2212 mt0 and ft0. we thus see that mt is a continuous\n\u2212mt0 being a mean zero normal random variable with variance\n\n12.2 time changes of martingales\n\nthe next theorem says that most continuous martingales arise from brownian motion via a\ntime change. that is, the paths are the same, but the rate at which one moves along the paths\nvaries. in fact, it is possible to show that all continuous martingales arise from a time change\nof a brownian motion that is possibly stopped at a random time.\ntheorem 12.2 suppose mt is a continuous local martingale, m0 = 0, (cid:22)m(cid:23)\nincreasing, and limt\u2192\u221e (cid:22)m(cid:23)\n\n= \u221e, a.s. let\n\nt is strictly\n\nt\n\n\u2265 t}.\nthen wt = m\u03c4 (t ) is a brownian motion with respect to f(cid:3)\n\n\u03c4 (t ) = inf{u : (cid:22)m(cid:23)\n\nu\n\n= f\u03c4 (t ).\n\nt\n\n "}, {"Page_number": 97, "text": "12.4 martingale representation\n\n79\n\nproof let us first suppose that w 2\nt\ne [wt | f(cid:3)\n\nis integrable. we have by proposition 9.3 that\n\ns] = e [m\u03c4 (t ) | f\u03c4 (s)] = m\u03c4 (s) = ws,\n\nis a continuous martingale. similarly, w 2\nt\n\nor wt\nl\u00b4evy\u2019s theorem, theorem 12.1. removing the assumption that w 2\nt\nexercise 12.1.\n\nis a martingale. now apply\nis integrable is left as\n\n\u2212 t\n\n12.3 quadratic variation\n\nit\u02c6o\u2019s formula allows us to prove theorem 9.10 fairly simply.\nproof of theorem 9.10 if tk = inf{t : |mt| \u2265 k}, we will show that\n\n[t02n](cid:9)\n\nk=0\n\n(mtk\u2227(k+1)/2n \u2212 mtk\u2227k/2n )2\n\nconverges to (cid:22)m(cid:23)\nmay assume m is bounded by k.\nf(cid:3)\nprocess n, we obtain\n\nt0\u2227tk . since tk \u2192 \u221e as k \u2192 \u221e, this will prove the proposition. thus we\nif s > 0 and we let nt = ms+t \u2212 ms, then nt is a martingale with respect to the filtration\n= fs+t and we can check that (cid:22)n(cid:23)\n(cid:3)\ns. by it\u02c6o\u2019s formula applied to the\n\n= (cid:22)m(cid:23)\n\n\u2212 (cid:22)m(cid:23)\n\nt+s\n\nt\n\nt\n\nt\n\n(mr+s \u2212 ms) dmr + ((cid:22)m(cid:23)\nt+s\n\n\u2212 (cid:22)m(cid:23)\n\n).\n\ns\n\napplying this with t = 1/2n and s = k/2n and summing, we see that\n\n(mt+s \u2212 ms)2 = 2\n[t02n](cid:9)\n\n0\n\n(cid:3)\n\nt0\n\nr dmr + r,\nln\n\n(12.7)\n\nwhere ln\nr\n\nnote\n\nt\n\nk=0\n\n(m(k+1)/2n \u2212 mk/2n )2 \u2212 (cid:22)m(cid:23)\n\n= 2\n= mr \u2212 mk/2n for k/2n \u2264 r < (k + 1)/2n and\nr = (cid:22)m(cid:23)([t02n]+1)/2n \u2212 (cid:22)m(cid:23)\n(cid:3)\n\n(cid:3)\n\n0\n\n(cid:10)\n\n.\n\nt0\n\n(cid:11)2 = 4e\n\ne\n\n2\n\nln\nr dmr\n\nt0\n\n0\n\nt0\n\n(ln\nr\n= e m 2\n\n0\n\n.\n\n)2 d(cid:22)m(cid:23)\n\u2264 k2 is finite, and ln\n\nr\n\n(12.8)\n\n)2 is bounded by 4k2, e(cid:22)m(cid:23)\n\nthe integrand (ln\nr tends to 0 as\nn \u2192 \u221e. by dominated convergence, the right-hand side of (12.8) tends to 0 as n \u2192 \u221e. as\nr\nfor the remainder term, r goes to 0 by the continuity of the paths of (cid:22)m(cid:23)\nt. the reason we\nonly have convergence in probability rather than in l2 is due to the stopping time argument\ninvolving tk.\n\nt\n\nt\n\n12.4 martingale representation\n\nthe next theorem says that every martingale adapted to the filtration of a brownian mo-\ntion can be expressed as a stochastic integral with respect to the brownian motion. this\n\n "}, {"Page_number": 98, "text": "80\n\nsome applications of it\u02c6o\u2019s formula\n\nused to be a rather arcane result that was of interest only to probabilists specializing in\nmartingales. but then it turned out that this theorem is the basis for showing the complete-\nness of the market in the theory of financial mathematics; see chapter 28. the martingale\nrepresentation theorem is also key to the innovations approach to stochastic filtering; see\nchapter 29.\ntheorem 12.3 let ft be the minimal augmented filtration generated by a one-dimensional\nbrownian motion wt, let t0 > 0, and let y be ft0 measurable with e y 2 < \u221e. there exists a\npredictable process hs with e\n\ns ds < \u221e such that\n\n(cid:15)\n\nt0\n0 h 2\n\n(cid:3)\n\ny = e y +\n\nt0\n\n0\n\nhs dws,\n\na.s.\n\n(12.9)\n\nthe proof consists of showing (12.9) holds for successively larger classes of random\nvariables. step 1 of the proof shows that the equation holds for random variables of the form\neiu(wt\u2212ws ) and step 2 shows that (12.9) holds for products of such random variables. in step\n3, it is shown that if the equation holds for a set of random variables, it holds for the closure\nof that set with respect to the l2 norm.\n\nstep 1. let xt = iuwt + u2t/2. note (cid:22)x (cid:23)\n\nt\n\n= (iu)2(cid:22)w(cid:23)\n\nt. by it\u02c6o\u2019s formula applied with\n\nproof\nf (x) = ex,\n\n(cid:3)\n\neiuwt+u2t/2 = 1 +\n= 1 +\n\nt\n\nt\n\nexr d (iuwr \u2212 u2r/2) + 1\niueiuwr+u2r/2 dwr.\n\n2\n\nt\n\n(\u2212u2)exr dr\n\n0\n\n(cid:3)\n(cid:3)\n\n0\n\n0\n\ntherefore\n\neiuwt = e\n\n\u2212u2t/2 +\n\n(cid:3)\n\nt\n\n0\n\niueiuwr+u2r/2\u2212u2t/2 dwr.\n\n(12.10)\n\nthe integrand in the stochastic integral in (12.10) is eiuwr times a deterministic function,\nhence is predictable. therefore (12.9) holds when y = eiuwt and moreover, the support of h\nin this case is contained in [0, t], that is, hr = 0 if r /\u2208 [0, t]. similarly, (12.9) holds when\ny = eiu(wt\u2212ws ), and in this case the support of the corresponding h is [s, t].\n\nbe more precise, if yi = e yi +(cid:15)\n\nstep 2. suppose now that y1 and y2 are two random variables for which (12.9) holds with\nthe supports of the corresponding h1 and h2 overlapping by at most finitely many points. to\n0 hi(s) dws, i = 1, 2, then we suppose that, with probability\none, h1(s)h2(s) = 0 except for finitely many points s. this implies\n(cid:3)\n\nt0\n\nt0\n\nh1(s)h2(s) ds = 0.\n\n0\n\n "}, {"Page_number": 99, "text": "12.4 martingale representation\n\n81\n\nt\n\n0 hi(s) dws, i = 1, 2. note zi(0) = e yi and zi(t0) = yi. then by the\n\n(cid:3)\n\nt0\n\nt0\n\nz1(s) dz2(s) +\nz1(s)h2(s) dws +\n\n0\n\nt0\n\n(cid:3)\nz2(s) dz1(s) + (cid:22)z1, z2(cid:23)\n\nt0\n\nt0\n\nz2(s)h1(s) dws\n\n0\n\nlet zi(t ) = e yi +(cid:15)\n\nproduct formula (corollary 11.3),\ny1y2 = (e y1)(e y2) +\n= (e y1)(e y2) +\n+\n= (e y1)(e y2) +\n\n(cid:3)\n\nt0\n\n0\n\nh1(s)h2(s) ds\n\n(cid:3)\n(cid:3)\n(cid:3)\n\n0\n\n0\n\nt0\n\n0\n\nks dws,\n\n(12.11)\nwhere ks = z1(s)h2(s)+zs(s)h1(s), and so the support of ks is contained in the union of the\nsupports of h1(s) and h2(s). taking an expectation in (12.11), e [y1y2] = (e y1)(e y2). thus\n(12.9) holds for y1y2. using induction, (12.9) will hold for the product of n random variables\nyi, i = 1, . . . , n, provided the supports of any two of the corresponding hi overlap by at most\nfinitely many values of s. combining this with step 1, we see that if s1 < s2 < \u00b7\u00b7\u00b7 < sn+1 \u2264 t0,\nthen the random variables of the form\ny = exp\n\nn(cid:9)\n\n(12.12)\n\n(cid:11)\n\n(cid:10)\n\n)\n\ni\n\n\u2212 ws j\n\nu j (ws j+1\n\nj=1\n\nsatisfy (12.9).\n\neach u j by \u2212u j, multiply by (cid:2)f (u1, . . . , un), the fourier transform of a c\n\nstep 3. we claim that random variables of the form (12.12) generate \u03c3 (ws; s \u2264 t0). to see\nthis, we proceed as in the last paragraph of the proof of theorem 12.1, namely, we replace\nfunction f with\ncompact support, integrate over (u1, . . . , un) \u2208 rn, use the fubini theorem and the fourier\ninversion theorem, and we obtain random variables of the form\n\n\u221e\n\n\u2212 ws1\n\nf (ws2\n\n, . . . , wsn+1\n\n\u2212 wsn\n\n)\n\n\u221e\n\n(cid:3)\n\nfor f in c\nwith compact support. by a limit argument, such random variables generate\n\u03c3 (ws; s \u2264 t0). we will prove that whenever yn satisfies (12.9) and yn \u2192 y in l2, then y\nsatisfies (12.9). by exercise 2.7 and proposition 2.5, this will prove our theorem.\nsuppose each yn satisfies (12.9) with integrand hn(s) and suppose yn \u2192 y in l2. then\ne yn \u2192 e y , and yn \u2212 e yn converges in l2 to y \u2212 e y . since\n\n0\n\nt0\n\ne\n\n(cid:15)\n(hn(s) \u2212 hm(s))2 ds = e ((yn \u2212 e yn) \u2212 (ym \u2212 e ym))2 \u2192 0,\n\nthe sequence hn is a cauchy sequence with respect to the norm (cid:21)x (cid:21) = (e\ns ds)1/2,\nwhich is an l2 norm and hence complete. therefore there exists hs (which is predictable\n(hn(s)\u2212 hs)2 ds \u2192\nbecause each hn(s) is predictable) such that e\n0. hence\n\n(cid:15)\nsince yn \u2212 e yn converges in l2 to y \u2212 e y , it follows that y \u2212 e y =(cid:15)\n\ns ds < \u221e and e\n(cid:3)\n\n(cid:15)\n(cid:11)2 = e\n\n(hn(s) \u2212 hs)2 ds \u2192 0.\n\n(yn \u2212 e yn) \u2212\n\nt0\n0 h 2\n\nt0\n0 x 2\n\nhs dws\n\n(cid:10)\n\n(cid:3)\n\nt0\n0 hs dws, a.s.\n\nt0\n0\n\ne\n\nt0\n\nt0\n\n0\n\n0\n\n "}, {"Page_number": 100, "text": "82\n\nsome applications of it\u02c6o\u2019s formula\n\ncorollary 12.4 suppose mt is a right-continuous square integrable martingale with respect\n(cid:15)\nto the minimal augmented filtration {ft} generated by a one-dimensional brownian mo-\ntion and suppose m0 = 0. let t0 > 0. then there exists a predictable process hs with\n\ne\n\nt0\n0 h 2\n\n(cid:3)\ns ds < \u221e such that with probability one\n\nt\n\nmt =\n\nhs dws\n\n0\n\nfor all t \u2264 t0.\nproof since mt is a martingale, e [mt0\ne m0 = 0. by theorem 12.3, there exists a predictable process h with e\nsuch that mt0\n\ntaking conditional expectations with respect to ft, we obtain mt =(cid:15)\n\n=(cid:15)\n\nt\n0 hs dws.\n\nt\n0 hs dws. this holds\nalmost surely for each t. thus except for a null set of \u03c9\u2019s, it holds for all t rational. since mt\nis right continuous, it holds for all t.\n\n| f0] = m0, and taking expectations, e mt0\n\n=\ns ds < \u221e\n\nt0\n0 h 2\n\n(cid:15)\n\ncorollary 12.5 if mt is a square integrable martingale with respect to the minimal aug-\nmented filtration of a one-dimensional brownian motion w , then mt has a version with\ncontinuous paths.\n\nproof by corollary 3.13, m has a version with right continuous paths. by corollary 12.4,\nm can be written as a stochastic integral with respect to w . but such stochastic integrals\nhave continuous paths by theorem 10.4.\n\n(cid:3)\n\nit is important for the martingale representation theorem that mt be a martingale with\nrespect to the minimal augmented filtration of w and not a larger filtration. for example,\nlet (x , y ) be a two-dimensional brownian motion and let {ft} be the minimal augmented\nfiltration generated by (x , y ). we show that we cannot write y1 as a stochastic integral with\nrespect to xt. if it were possible to do so, since y1 has mean zero, we would have\n\n0\n\n1\n\nhs dxs.\n\ntaking conditional expectations, yt =(cid:15)\ns such that y = e y +(cid:12)\n\nt\n0 hs ds by exercise 10.5.\nbut if (x , y ) is two-dimensional brownian motion, then x and y are independent, and so\n(cid:22)x , y(cid:23)\n= 0 by exercise 9.4, a contradiction. (however, it is true, by a proof similar to that\nof theorem 12.3, if {ft} is the minimal augmented filtration of a d-dimensional brownian\nmotion (w 1, . . . , w d ) and y is square integrable and ft0 measurable, then there exist suitable\nprocesses h i\n\ny1 =\n0 hs dxs. then (cid:22)x , y(cid:23)\n(cid:15)\n\n=(cid:15)\n\ns dw i\ns .)\n\nd\ni=1\n\nt0\n0 h i\n\nt\n\nt\n\nt\n\n12.5 the burkholder\u2013davis\u2013gundy inequalities\n\nnext we turn to a pair of basic inequalities, those of burkholder, davis, and gundy. in both\nof the following theorems, the constant depends on p, the exponent. as stated and proved\nbelow, we require p \u2265 2 for theorems 12.6 and 12.7; in fact, the two theorems are true (with\na different proof) as long as p > 0; see bass (1995), pp. 62\u20134, or exercise 12.12. the proof\nwe present here is a nice application of it\u02c6o\u2019s formula.\n\n "}, {"Page_number": 101, "text": "12.5 the burkholder\u2013davis\u2013gundy inequalities\n\n83\n\ndefine\n\n\u2217\nm\nt\n\n= sup\ns\u2264t\n\n|ms|.\n\ntheorem 12.6 let mt be a continuous local martingale with m0 = 0, a.s., and suppose\n2 \u2264 p < \u221e. there exists a constant c1 depending on p such that for any finite stopping\ntime t ,\n\n\u2217\ne (m\nt\n\n)p \u2264 c1e(cid:22)m(cid:23)p/2\n\n.\n\nt\n\n\u2217\nt\n\ne|m\n\nproof there is nothing to prove if the left-hand side is zero, so we may assume it is positive.\nt is bounded by a positive constant k. note for p \u2265 2 the function x \u2192 |x|p\n\u2217\nfirst suppose m\nis c2. by doob\u2019s inequalities and then it\u02c6o\u2019s formula (and the fact that |ms| \u2265 0), we have\n(cid:3)\n|p \u2264 ce|mt|p\n(cid:3)\n= ce\n\u2264 ce\n= ce [(m\n\np(p \u2212 1)|ms|p\u22122 d(cid:22)m(cid:23)\n\np|ms|p\u22121 dms + 1\n\n\u2217\n(m\nt\n)p\u22122(cid:22)m(cid:23)\n\u2217\nt\n\n)p\u22122 d(cid:22)m(cid:23)\n\n2 ce\n\n(cid:3)\n\nt ].\n\nt\n\nt\n\nt\n\n0\n\n0\n\n0\n\ns\n\ns\n\n(recall our convention about constants and the letter c.) using h\u00a8older\u2019s inequality with\nexponents p/(p \u2212 2) and p/2, we obtain\n\n)p \u2264 c(e (m\n\u2217\ne (m\nt\n)p)(p\u22122)/p) and then taking both sides to the power p/2 gives\n\u2217\ndividing both sides by (e (m\nt\nour result.\nwe then apply the above to t \u2227 uk, where uk = inf{t : |mt| \u2265 k}, let k \u2192 \u221e, and use\n\np (e ((cid:22)m(cid:23) p\n\n) 2\np .\n\n)p)\n\n\u2217\nt\n\np\u22122\n\n2\nt\n\nfatou\u2019s lemma.\ntheorem 12.7 let mt be a continuous local martingale with m0 = 0, a.s., and suppose\n2 \u2264 p < \u221e. there exists a constant c2 depending on p such that for any finite stopping\ntime t ,\n\nproof as in the previous theorem, we may assume the left-hand side is positive. set r = p/2.\nt are bounded by a positive constant k. let nt = mt\u2227t , so\nlet us first suppose (cid:22)m(cid:23)\n\u2217\nthat (cid:22)n(cid:23)\u221e = (cid:22)m(cid:23)\n\nt , and let at = (cid:22)m(cid:23)r\u22121\n\nt and m\n\n(cid:3) \u221e\n\ne(cid:22)m(cid:23)p/2\n\nt\n\n\u2217\nt\n\n)p.\n\n\u2264 c2e (m\n(cid:3) \u221e\n\nt\u2227t . using integration by parts,\nas d(cid:22)n(cid:23)\n\ns\n\n(cid:22)n(cid:23)\n\ns das = (cid:22)n(cid:23)\u221ea\u221e \u2212\n= (cid:22)n(cid:23)r\u221e \u2212 1\n(cid:3) \u221e\nr\n\n0\n\n(cid:22)n(cid:23)r\u221e.\n\n(cid:22)n(cid:23)\u221e das = (cid:22)n(cid:23)r\u221e,\n\n0\n\n0\n\nsince\n\n "}, {"Page_number": 102, "text": "84\n\nwe then have\n\nusing propositions 3.14 and 9.6,\n\ne(cid:22)m(cid:23)r\n\nt\n\nsome applications of it\u02c6o\u2019s formula\n\n(cid:3) \u221e\n\n((cid:22)n(cid:23)\u221e \u2212 (cid:22)n(cid:23)\n\n) das.\n\ns\n\n(cid:3) \u221e\n\n0\n\n0\n\n(cid:22)n(cid:23)r\u221e = r\n(cid:3) \u221e\n(cid:3) \u221e\n(cid:3) \u221e\n(cid:3) \u221e\n\n= e(cid:22)n(cid:23)r\u221e = re\n= re\n= re\n= re\n\u2264 ce\n= ce [(n\n\u2217\n\u221e)2a\u221e]\n)2(cid:22)m(cid:23)r\u22121\n= ce [(m\n\u2217\nt\n\ne [(n\n\nt\n\n0\n\n0\n\n0\n\n].\n\n) das\n\ns\n\n0\n\n((cid:22)n(cid:23)\u221e \u2212 (cid:22)n(cid:23)\n(e [(cid:22)n(cid:23)\u221e | fs] \u2212 (cid:22)n(cid:23)\ne [(cid:22)n(cid:23)\u221e \u2212 (cid:22)n(cid:23)\ne [n 2\u221e \u2212 n 2\n\n| fs] das\n\n) das\n| fs] das\n\ns\n\ns\n\ns\n\n\u221e)2 | fs] das\n\u2217\n\nwe use h\u00a8older\u2019s inequality with exponents r and r/(r \u2212 1), divide both sides by the quantity\n(e(cid:22)m(cid:23)r\n\n)(r\u22121)/r, and then take both sides to the rth power. we then get\n\nt\n\ne(cid:22)m(cid:23)r\n\nt\n\n\u2264 ce (m\n\n\u2217\nt\n\n)2r,\n\nwhich is what we wanted.\nplace of t , where vk = inf{t : (cid:22)m(cid:23)\n\nto remove the restriction that (cid:22)m(cid:23) and m\n\u2217\nt\n\n+ m\n\nt\n\nare bounded, we apply the above to t \u2227 vk in\n\n\u2217\n\u2265 k}, let k \u2192 \u221e, and use fatou\u2019s lemma.\n\nfor stochastic differential geometry and also many other purposes, the stratonovich integral\nis more convenient than the it\u02c6o integral. if x and y are continuous semimartingales, the\nstratonovich integral, denoted\n\nt\n\n12.6 stratonovich integrals\n\n(cid:15)\n0 xs \u25e6 dys, is defined by\nxs dys + 1\nxs \u25e6 dys =\n\n(cid:3)\n\nt\n\n0\n\n(cid:3)\n\nt\n\n0\n\n(cid:22)x , y(cid:23)\n\nt\n\n.\n\n2\n\nboth the beauty and the difficulty of it\u02c6o\u2019s formula are due to the quadratic variation term.\n\nthe change of variables formula for the stratonovich integral avoids this.\ntheorem 12.8 suppose f \u2208 c3 and x is a continuous semimartingale. then\n\nt\n\n(cid:3)(xs) \u25e6 dxs.\n\nf\n\n0\n\nf (xt ) = f (x0) +\n(cid:3)\n\nproof by it\u02c6o\u2019s formula applied to the function f and the definition of the stratonovich\nintegral, it suffices to show that\n(cid:22) f\n\n(cid:3)(x ), x (cid:23)\n\n(cid:3)(cid:3)(xs)d(cid:22)x (cid:23)\n\n(12.13)\n\n=\n\nf\n\n.\n\nt\n\ns\n\nt\n\n(cid:3)\n\n0\n\n "}, {"Page_number": 103, "text": "applying it\u02c6o\u2019s formula to the function f\nt\n\n(cid:3)(xt ) = f\n\n(cid:3)(x0) +\n\nf\n\nfrom which (12.13) follows.\n\n0\n\n(cid:3)\n\n(cid:3)\n\nexercises\n\n, which is in c2,\n(cid:3)(cid:3)(xs) dxs + 1\n\nf\n\n2\n\n(cid:3)\n\n0\n\n85\n\nt\n\n(cid:3)(cid:3)(cid:3)(xs) d(cid:22)x (cid:23)\n\nf\n\n,\n\ns\n\nif x and y are continuous semimartingales and we apply the change of variables formula\n\n(cid:3)\nwith f (x) = x2 to x + y and x \u2212 y , we obtain\n(xt + yt )2 = (x0 + y0)2 + 2\n(cid:3)\n\nand\n\n(xt \u2212 yt )2 = (x0 \u2212 y0)2 + 2\n\n0\n\nt\n\n0\n\nt\n\n(xs + ys) \u25e6 d (xs + ys)\n\n(xs \u2212 ys) \u25e6 d (xs \u2212 ys).\n(cid:3)\n\n(cid:3)\n\n0\n\ntaking the difference and then dividing by 4, we have the product formula for stratonovich\nintegrals\n\n(cid:15)\nxtyt = x0y0 +\n\nt\n\nxs \u25e6 dys +\n\nt\n\nys \u25e6 dxs.\n\n0\n\n(12.14)\n\n(cid:15)\n\nthe stratonovich integral\n\nhs \u25e6 dxs can be represented as a limit of riemann sums.\n\nproposition 12.9 suppose h and x are continuous semimartingales and t0 > 0. then\n0 hs \u25e6 dxs is the limit in probability as n \u2192 \u221e of\n\nt\n\nhkt0/2n + h(k+1)t0/2n\n\n2n\u22121(cid:9)\nproof we write the sum as(cid:9)\n(cid:9)\nhkt0/2n (x(k+1)t0/2n \u2212 xkt0/2n )\n+ 1\n\nk=0\n\n2\n\n(x(k+1)t0/2n \u2212 xkt0/2n ).\n\n(cid:15)\n\n2\n\nt\n\n(h(k+1)t0/2n \u2212 hkt0/2n )(x(k+1)t0/2n \u2212 xkt0/2n ).\n\n0 hs dxs while by exercise 12.10 the second sum tends to 1\n\n2\n\n(cid:22)h, x (cid:23)\nt.\n\nthe first sum tends to\nthis proves the proposition.\n\n12.1 show that wt and w 2\nt\n\n12.2.\n\nexercises\n\n\u2212 t are local martingales, where w is defined in the statement of theorem\n\n12.2 suppose {ft} is a filtration satisfying the usual conditions, x is a brownian motion with respect\nto {ft}, and t is a finite stopping time with respect to this same filtration. let y be another\nbrownian motion that is independent of {ft} and define\n\n(cid:13)\nxt ,\nxt + yt\u2212t ,\n\nzt =\n\nt < t\nt \u2265 t.\n\nshow that z is a brownian motion (although not necessarily with respect to {ft}).\n\n "}, {"Page_number": 104, "text": "some applications of it\u02c6o\u2019s formula\n\n86\n12.3 suppose mt is a continuous local martingale with respect to a filtration {ft} satisfying the usual\nconditions, t is a stopping time with respect to {ft}, and (cid:22)m(cid:23)t = t \u2227 t . prove that mt\u2227t has\nthe same law as a brownian motion stopped at time t .\n12.4 here is a multidimensional version of l\u00b4evy\u2019s theorem. let {ft} be a filtration satisfying the\nusual conditions. suppose (m 1\n) is a d-dimensional process such that each component\nt is a continuous martingale with respect to {ft} with (cid:22)m i(cid:23)t = t. suppose that (cid:22)m i, m j(cid:23)t = 0\nt\nm i\nif i (cid:16)= j. prove that (m 1\n12.5 let {ft} be a filtration satisfying the usual conditions. let at be a strictly increasing continuous\n) is a d-dimensional\nis a continuous martingale with respect to {ft} and\n) is a time change of\n\nprocess adapted to {ft} with limt\u2192\u221e at = \u221e, a.s. suppose (m 1\nprocess such that each component m i\n(cid:22)m i(cid:23)t = at. suppose that (cid:22)m i, m j(cid:23)t = 0 if i (cid:16)= j. prove that (m 1\nt\nd-dimensional brownian motion.\n\n) is a d-dimensional brownian motion.\n\n, . . . , m d\nt\n\n, . . . , m d\nt\n\n, . . . , m d\nt\n\n, . . . , m d\nt\n\ngaussian process.\n\n12.6 suppose m is a continuous local martingale such that (cid:22)m(cid:23)t is deterministic. prove that m is a\n12.7 suppose m is a continuous local martingale with m0 = 0, a.s. show that there exists a brownian\n\nmotion w , an increasing process \u03c4t, and a stopping time t such that mt = w\u03c4t\u2227t for all t.\n\nt\n\nt\n\nt\n\n\u2217\u221e < \u221e) and ((cid:22)m(cid:23)\u221e < \u221e)\n\n12.8 let mt be a continuous local martingale. show that the events (m\n\ndiffer by at most a null set.\n\n12.9 let mt be a continuous local martingale. prove that\n\n|mt| > x,(cid:22)m(cid:23)\u221e < y) \u2264 2e\n\n\u2212x2/2y.\n\np(sup\nt\u22650\n\n12.10 suppose x and y are continuous semimartingales and t0 > 0. prove that\n\n2n\u22121(cid:9)\n\nk=0\n\n(x(k+1)t0/2n \u2212 xkt0/2n )(y(k+1)t0/2n \u2212 ykt0/2n )\n\n12.11 let p > 0. suppose x and y are non-negative random variables, \u03b2 > 1, \u03b4 \u2208 (0, 1), and\n\nconverges to (cid:22)x , y(cid:23)t0 in probability.\n\u03b5 \u2208 (0, \u03b2\u2212p/2) such that\n\np(x > \u03b2\u03bb, y < \u03b4\u03bb) \u2264 \u03b5p(x \u2265 \u03bb)\n\nfor all \u03bb > 0. this inequality is known as a good-\u03bb inequality. prove that there exists a constant\nc (depending on \u03b2, \u03b4, \u03b5, and p but not x or y ) such that\ne x p \u2264 ce y p.\n\nhint: first assume x is bounded. write\n\np(x /\u03b2 > \u03bb) = p(x > \u03b2\u03bb, y < \u03b4\u03bb) + p(y \u2265 \u03b4\u03bb)\n\n\u2264 \u03b5p(x \u2265 \u03bb) + p(y/\u03b4 \u2265 \u03bb).\n\nmultiply by p\u03bbp\u22121, integrate over \u03bb, and use the fact that \u03b5 < \u03b2\u2212p/2.\n\n "}, {"Page_number": 105, "text": "exercises\n\n87\n\n12.12 use exercise 12.11 to prove that the burkholder\u2013davis\u2013gundy inequalities hold for all p > 0.\nhint: use time change to reduce to the case of a brownian motion w . if t is a stopping time\n\nand u = inf{t : w\n\n> \u03bb}, write\n\n\u2217\nt\n\n\u2217\np(w\nt\n\n> \u03b2\u03bb, t 1/2 < \u03b4\u03bb) = p(w\n\u2264 p(\n\n\u2217\nt\n\n> \u03b2\u03bb, t < \u03b42\u03bb2, u < \u221e)\nsup\n\n|wt \u2212 wu| > (\u03b2 \u2212 1)\u03bb, u < \u221e).\n\nu\u2264t\u2264u+\u03b42\u03bb2\n\ncondition on fu , use theorem 4.2, and notice that p(u < \u221e) = p(w\n\n\u2217\nt\n\n> \u03bb).\n\n12.13 define the h 1 norm of a martingale by\n\n(cid:21)m(cid:21)h 1 = e [sup\nt\u22650\n\n|mt| ].\n\nprove that this is a norm. does there exist a uniformly integrable continuous martingale that is\nnot in h 1?\n\n12.14 let w be a brownian motion and let t be a stopping time. prove that if e t 1/2 < \u221e, then\n\ne wt = 0.\n12.15 suppose w = (w 1, . . . , w d ) is a d-dimensional brownian motion started at 0, and let {ft} be\n(cid:15)\nthe minimal augmented filtration of w . suppose y is a f1 measurable random variable with\nmean zero and finite variance. prove there exist predictable processes h 1, . . . , h d such that\ne\n\n)2 ds < \u221e for each i and\n\n1\n0\n\n(h i\ns\n\n(cid:3)\n\ny = d(cid:9)\n\ni=1\n\n1\n\nwt+h \u2212 wt\n(cid:3)\n\n1\n\n0\n\nh i\ns dw i\ns\n\n.\n\n(cid:3)\n\nt+h\n\nhs dws\n\nt\n\nt\n\n0\n\n1|ws|\u03b1 ds\n\n12.16 suppose w is a brownian motion and h is adapted, bounded, and right continuous. let t \u2265 0.\n\nshow\n\nconverges in probability to ht.\n\n12.17 let w be a brownian motion and \u03b1 > 0. show that\n\nis infinite almost surely if \u03b1 \u2265 1 but finite almost surely if \u03b1 < 1.\n\n12.18 here is a useful inequality. suppose a is an increasing process with a0 = 0, a.s., and suppose\n\nthere exists a non-negative random variable b such that for each t,\n\ne [a\u221e \u2212 at | ft] \u2264 e [b | ft],\n\na.s.\n\nprove that for each integer p \u2265 1, there exists a constant cp depending only on p such that\n\ne ap\u221e \u2264 cpe bp.\n(cid:3) \u221e\n\na\u221e = p!\n\n(a\u221e \u2212 at ) dat ,\n\nhint: write\n\n0\ntake expectations, and use proposition 3.14.\n\n "}, {"Page_number": 106, "text": "88\n12.19 let w be a one-dimensional brownian motion with filtration {ft} and let f (r, s) be a determin-\n\nsome applications of it\u02c6o\u2019s formula\n\nistic function. define the multiple stochastic integral by\n\n(cid:3)\n\n(cid:10)(cid:3)\n\nt\n\ns\n\n(cid:11)\n\n(cid:3)\n\n(cid:3)\n\nt\n\ns\n\nf (r, s) dwr dws =\n\nf (r, s) dwr\n\ndws,\n\n0\n\n0\n\nprovided\n\n0\n\n0\n\n(cid:3)\n\n(cid:3)\n\nt\n\ns\n\n0\n\n0\n\n(cid:3)\n\n=\n\nm f\nt\n\nt\n\n\u00b7\u00b7\u00b7\n\nf (r, s)2 dr ds < \u221e,\n(cid:3)\n\nrm\u22121\n\n\u00b7\u00b7\u00b7 dwrm\nf dwr1\nt ] = 0 for all t.\n\n,\n\nand similarly for higher-order multiple stochastic integrals.\n\n(1) if f : rm \u2192 r and g : rn \u2192 r are bounded and deterministic, n (cid:16)= m,\n\n0\nt m g\nt is defined similarly, show that e [m f\n\n0\n\nand m g\n\n(2) show that the collection of random variables\n\n{m f\n\n1 : f has domain rm for some m and is bounded and deterministic}\n\nis dense in the set of mean zero f1 measurable random variables with respect to the l2(p) norm.\n\n "}, {"Page_number": 107, "text": "13\n\nthe girsanov theorem\n\nwe look at what happens to a brownian motion when we change p to another probability\nmeasure q. this may seem strange, but there are many applications of this, including to\nfinancial mathematics and to filtering; see chapters 28 and 29. another application we will\ngive (at the end of this chapter in section 13.2) is to determine the probability a brownian\nmotion ws crosses a line a + bs before time t.\n\n0\n\nt\n\n= 1 +\n\n13.1 the brownian motion case\n\nwe start with an observation. suppose yt is a continuous local martingale with y0 = 0 and\nlet zt = eyt\u2212(cid:22)y(cid:23)t /2. applying it\u02c6o\u2019s formula to xt = yt \u2212 1\n(cid:22)y(cid:23)\n\nt with the function ex yields\n+ 1\n\nzt = eyt\u2212(cid:22)y(cid:23)t /2 = 1 +\n\n(cid:10)\nys \u2212 1\n\nexsd(cid:22)y(cid:23)\n\n(cid:11)\n(cid:22)y(cid:23)\n\n(cid:3)\n\n(cid:3)\n\nexsd\n\n2\n\nt\n\nt\n\ns\n\n2\n\ns\n\n2\n\n0\n\n(cid:3)\n\nzs dys.\n\n(13.1)\nthis can be abbreviated by dzt = zt dyt. zt is called the exponential of the martingale y ,\nand since z is the stochastic integral with respect to a local martingale, it is itself a local\nmartingale.\n\n0\n\nbefore stating the girsanov theorem, we need two technical lemmas.\n\nlemma 13.1 suppose y is a continuous local martingale with y0 = 0 and zt = eyt\u2212(cid:22)y(cid:23)t /2. if\n(cid:22)y(cid:23)\nt is a bounded random variable for each t, then e|zt|p < \u221e for each p > 1 and each t.\nproof let us first suppose y is bounded in absolute value by n. since zt \u2265 0, we have by\nthe cauchy\u2013schwarz inequality\n\n(cid:16)\n\ne z p\n\nt = e epyt\u2212p(cid:22)y(cid:23)t /2\n(cid:10)\n= e\n\u2264\n\nepyt\u2212p2(cid:22)y(cid:23)t e(p2\u2212(p/2))(cid:22)y(cid:23)t\ne e2pyt\u22122p2(cid:22)y(cid:23)t\n\n(cid:11)1/2(cid:10)\n\ne e(2p2\u2212p)(cid:22)y(cid:23)t\n\n(13.2)\n\n(cid:11)1/2\n\n.\n\n(cid:17)\n\nby the exact same calculation as in (13.1) but with y replaced by 2py , we see e2pyt\u22122p2(cid:22)y(cid:23)t\nis a stochastic integral of a bounded integrand with respect to a bounded martingale, and\nhence is a martingale. this shows that the first factor on the last line of (13.2) is 1. by our\nassumption that (cid:22)y(cid:23)\nt is bounded, the second factor on this line is finite and does not depend\non n.\n\n89\n\n "}, {"Page_number": 108, "text": "90\n\nthe girsanov theorem\n\nif y is not bounded, let tn = inf{s : |ys| \u2265 n}, apply the above argument to yt\u2227tn , and let\nn \u2192 \u221e.\n\nthe second lemma is the following.\nlemma 13.2 suppose at is a continuous increasing process adapted to a filtration {f t}\nsatisfying the usual conditions. let x be a bounded random variable, h a bounded adapted\nprocess, s < t, and b \u2208 fs. then\n\n(cid:17)\n\n= e\n\nt\n\ne [x | fr] hr dar; b\n\n.\n\n(cid:3)\nproof by linearity, it suffices to suppose x and h are non-negative. let a\nr\n(cid:3)\nh\nr\n\n= hr+s, and f(cid:3)\n\nr\n\n= ar+s,\n\nr\n0 h\nx dcr = e\n\n(cid:3)\n(cid:3)\ns, and so we must show\nr1b da\nt\u2212s\ne [x | f(cid:3)\n\nr] dcr.\n\nt\n\ne\n\n(cid:16)(cid:3)\n(cid:17)\n= fr+s. let cr =(cid:15)\n\nx hr dar; b\n(cid:3)\n\ns\n\nt\u2212s\n\ne\n\n0\n\n(cid:16)(cid:3)\n\ns\n\n(cid:3)\n\n0\n\nthis follows by proposition 3.14.\n\nlet mt be a non-negative continuous martingale with m0 = 1, a.s. define a new probability\nmeasure q by q(a) = e [mt; a] if a \u2208 ft. note q is a probability measure because\nq(\u0001) = e mt = e m0 = 1. q is well-defined because if a \u2208 fs \u2282 ft, then since m is a\nmartingale, we have e [mt; a] = e [ms; a].\n\na more general version of the girsanov theorem is possible (see exercise 13.5), but the\n\ngirsanov theorem is most frequently used with brownian motion.\n\ntheorem 13.3 suppose wt is a brownian motion with respect to p, h is bounded and\npredictable,\n\n(cid:10)(cid:3)\n\n(cid:3)\n\n(cid:11)\n\nmt = exp\n\nt\n\nhr dwr \u2212 1\n\n2\n\n0\n\nt\n\n0\n\nh 2\n\nr dr\n\n,\n\nt\n0 hr dr is a brownian motion with respect to q.\n\nand\n\nq(b) = e p[mt; b]\n\nthen wt \u2212(cid:15)\nproof we prove the theorem by showing wt \u2212(cid:15)\ntheorem (theorem 12.1). we first show wt \u2212(cid:15)\n(13.1) with yt =(cid:15)\n(cid:3)\n\n0 hr dwr and zt = mt,\n\nt\n\nif b \u2208 ft .\n\nt\n0 hr dr satisfies the hypotheses of l\u00b4evy\u2019s\nt\n0 hr dr is a martingale with respect to q. by\n\n(13.3)\n\n(13.4)\n\nmt = 1 +\n\nt\n\n(cid:22)m, w(cid:23)\n\nt\n\n=\n\n0\n\n(cid:3)\n\nmrhr dwr.\n\nt\n\nby exercise 10.5,\n\nwe want to show that if b \u2208 fs, then\n\n(cid:3)\n\n(cid:16)\nwt \u2212\n\ne q\n\n(cid:17)\n\nmrhr dr.\n\n0\n\n= e q\n\n(cid:16)\nws \u2212\n\n(cid:3)\n\n(cid:17)\n\n.\n\ns\n\nhr dr; b\n\n0\n\n(13.5)\n\n(13.6)\n\nt\n\nhr dr; b\n\n0\n\n "}, {"Page_number": 109, "text": "91\n\n(13.7)\n\n(13.8)\n\n(13.9)\n\nif b \u2208 fs, then using the definition of q and the product formula (corollary 11.3),\n\n13.1 the brownian motion case\n\nand\n\nand\n\ne q[wt; b] = e p[mtwt; b]\n\n= e p\n\nt\n\n0\n\n(cid:16)(cid:3)\n\n(cid:17)\nmr dwr; b\n+ e p[(cid:22)m, w(cid:23)\n; b]\n(cid:17)\nmr dwr; b\n; b].\n\n(cid:16)(cid:3)\n\nt\n\ns\n\ns\n\n= e p\n+ e p[(cid:22)m, w(cid:23)\n\n0\n\n(cid:16)(cid:3)\n\n(cid:17)\n\n+ ep\n\nt\n\nwr dmr; b\n\n0\n\n(cid:16)(cid:3)\n\n0\n\n(cid:17)\n\ns\n\nwr dmr; b\n\n+ ep\n\ne q[ws; b] = e p[msws; b]\n\ncombining (13.7), (13.8), (13.9), and (13.10), we see that (13.6) will follow if we show\n\nsince h is bounded, (cid:22)(cid:15) \u00b7\n\n\u2264 ct. by lemma 13.1, mt is a martingale and e|mt|p <\n\u221e for each t and each p \u2265 1. since stochastic integrals with respect to martingales are\nmartingales,\n\nt\n\n0 hr dwr(cid:23)\n(cid:16)(cid:3)\n(cid:16)(cid:3)\n\ne p\n\n0\n\nt\n\nt\n\ne p\n\n0\n\n(cid:17)\nmr dwr; b\n(cid:17)\n\nwr dmr; b\n\n= e p\n\n(cid:16)(cid:3)\n(cid:16)(cid:3)\n\ns\n\n0\n\n(cid:17)\nmr dwr; b\n(cid:17)\n\nt\n\ns\n\ns\n\n0\n\n= e p\n\nwr dmr; b\n(cid:16)(cid:3)\n(cid:16)(cid:3)\n(cid:17)\n= e p\ne [mt | fr]hr dr; b\n; b],\n\n; b] = e q\n(cid:17)\n\n\u2212 (cid:22)m, w(cid:23)\n\nhr dr; b\n\ns\n\nt\n\ns\n\n.\n\n(13.10)\n\nhr dr; b\n\n.\n\n(13.11)\n\n(cid:17)\n\n(cid:17)\n\nt\n\n(cid:16)(cid:3)\nmthr dr; b\n= e p\n\ns\n\nt\n\ns\n\nmrhr dr; b\n\n(cid:17)\n\nt\n\nt\n\ns\n\nmt\n\ne q\n\n(cid:16)(cid:3)\n\nhr dr; b\n\nusing lemma 13.2 and (13.5), we have\n\ne p[(cid:22)m, w(cid:23)\n(cid:17)\n\n\u2212 (cid:22)m, w(cid:23)\n(cid:3)\n(cid:16)\n(cid:16)(cid:3)\na similar proof shows that (wt \u2212(cid:15)\nthe quadratic variation of wt \u2212(cid:15)\nprocess wt \u2212(cid:15)\n\n= e p\n= e p\n= e p[(cid:22)m, w(cid:23)\n\nwhich proves (13.11).\n\n0\n\ns\n\nt\n\nt\n\nmotion under q.\n\nt\n\n0 hr dr has continuous paths, by l\u00b4evy\u2019s theorem, wt \u2212(cid:15)\n\n0 hr dr)2\u2212t is a martingale with respect to q, and hence\nt\n0 hr dr under q is still t (or see exercise 13.2). since the\nt\n0 hr dr is a brownian\n\nt\n\nthe assumption that h be bounded can be weakened, but in practice it is more common\n\nto use a stopping time argument; for an example, see the proof of theorem 29.3.\n\n "}, {"Page_number": 110, "text": "92\n\nthe girsanov theorem\n\n13.2 an example\n\nlet us give an example of the use of the girsanov theorem, namely, to compute the probability\nthat brownian motion crosses a line a + bt by time t0, a > 0. we want to find an exact\nexpression for p(\u2203t \u2264 t0 : wt = a + bt ), where w is a brownian motion.\nlet wt be a brownian motion under p. define q on ft0 by\nby the girsanov theorem, under q,(cid:14)wt = wt + bt is a brownian motion, and wt = (cid:14)wt \u2212 bt.\n\u2212bwt\u2212b2t/2.\nlet a = (sups\u2264t0 ws \u2265 a). if we set s = inf{t > 0 : wt = a}, then a = (s \u2264 t0) and\na \u2208 fs\u2227t0. we write\n\ndq/dp = mt = e\n\np(\u2203t \u2264 t0 : wt = a + bt ) = p(\u2203t \u2264 t0 : wt \u2212 bt = a)\n\n(13.12)\n\nwt is a brownian motion under p while(cid:14)wt is a brownian motion under q. therefore the last\n\n= p(sup\ns\u2264t0\n\n(ws \u2212 bs) \u2265 a).\n\nline of (13.12) is equal to\n\nthis in turn is equal to\n\n((cid:14)ws \u2212 bs) \u2265 a).\n\nq(sup\ns\u2264t0\n\nws \u2265 a) = q(a).\n\nq(sup\ns\u2264t0\n\nto evaluate q(a), note ms = e\n\n\u2212ab\u2212b2s/2 and by (3.19) with b replaced by a,\np(s \u2208 ds) = a\u221a\n\n\u2212a2/2s.\ne\n\n2\u03c0s3\n\nnow we use optional stopping to obtain\n\np(\u2203t \u2264 t0 : wt = a + bt ) = q(a) = e p[mt0\n; a]\n; s \u2264 t0]\n= e p[ms\u2227t0\n(cid:3)\n= e p[ms; s \u2264 t0]\n=\na\u221a\n\u2212ab\u2212b2s/2\ne\n2\u03c0s3\n\nt0\n\n0\n\n(13.13)\n\n\u2212a2/2s ds.\ne\n\nexercises\n\n13.1 whether a filtration satisfies the usual conditions depends on the class of null sets and hence the\nprobability measure involved matters. suppose {ft} satisfies the usual conditions with respect\nto p, h is a bounded predictable process, w a brownian motion with respect to p, m defined\nby (13.3), and q defined by (13.4). if t0 > 0 and a \u2208 \u03c3 (ws; s \u2264 t0 ), show p(a) = 0 if and only\nif q(a) = 0.\n\n "}, {"Page_number": 111, "text": "exercises\n\n93\n\n13.2 theorem 9.10 allows us to avoid some calculations in the last paragraph of the proof of theorem\n13.3. suppose x is a continuous semimartingale under p and q is a probability measure\nequivalent to p. that is, a set is a null set for p if and only if it is a null set for q. show x is a\nsemimartingale under q and the quadratic variation of x under p equals the quadratic variation\nof x under q.\n\n13.3 letw = (w 1, . . . , w d ) be a d-dimensional brownian motion with minimal augmented filtration\n\n(cid:3)\nd(cid:9)\n{ft} and let h1, . . . , hd be bounded predictable processes. let\n\n(cid:10) d(cid:9)\n\n(cid:3)\n\nt\n\nt\n\n(cid:11)\n\nmt = exp\n\ndefine a probability measure q by setting q(a) = e p[mt; a] if a \u2208 ft. let (cid:14)w i\n(cid:15)\n0 hi(s) ds for each i. prove that (cid:14)w = ((cid:14)w 1, . . . ,(cid:14)w d ) is a d-dimensional brownian motion\n\n= w i\n\ni=1\n\ni=1\n\n\u2212\n\nhi(s) dw i\ns\n\n0\n\n2\n\n0\n\n.\n\nt\n\nt\n\nt\n\n\u2212 1\n\n|hi(s)|2 ds\n\nunder q.\n\n: [0, t0] \u2192 rd be a\n\n13.4 let wt be a d-dimensional brownian motion and let \u03b4, t0 > 0. let f\n\ncontinuous function. prove that there exists a constant c such that\n\n|ws \u2212 f (s)| < \u03b4) > c.\n\np(sup\ns\u2264t0\n\nthis is known as the support theorem for brownian motion.\n\nhint: first assume that f has a bounded derivative. use exercise 4.9 and the girsanov theorem.\n\n13.5 here is a more general form of the girsanov theorem. suppose lt is a bounded continuous\nmartingale under p, mt = elt\u2212(cid:22)l(cid:23)t /2, and q is a probability measure defined by q(a) =\n; a] if a \u2208 ft0. suppose {ft} is a filtration satisfying the usual conditions with respect to\ne p[mt0\nboth p and q. show that if x is a martingale under p, then xt \u2212(cid:22)x , l(cid:23)t is a martingale under q.\n\n "}, {"Page_number": 112, "text": "14\n\nlocal times\n\nlet wt be a one-dimensional brownian motion. although the lebesgue measure of the\nrandom set {t : wt = 0} is 0, a.s., nevertheless there is an increasing continuous process\nwhich grows only when the brownian motion is at 0. this increasing process is known as\nlocal time at 0. we want to derive some of its properties.\n\n14.1 basic properties\n\nlet w be a brownian motion. by jensen\u2019s inequality for conditional expectations (proposition\na.21), |wt| is a submartingale, and by the doob\u2013meyer decomposition (theorem 9.12), it\ncan be written as a martingale plus an increasing process. since wt is itself a martingale, the\nincreasing process grows only at times when the brownian motion is at 0.\nsition of |wt|. we define\n\nrather than appealing to the doob\u2013meyer decomposition, we give the explicit decompo-\n\n\u23a7\u23aa\u23a8\u23aa\u23a91,\n\n0,\n\u22121,\n\nx > 0;\nx = 0;\nx < 0.\n\nsgn (x) =\n\n(cid:3)\n\ntheorem 14.1 let wt be a one-dimensional brownian motion.\n\n(1) there exists a non-negative increasing continuous adapted process l0\n\nt such that\n\n|wt| =\n\nt\n\nsgn (ws) dws + l0\n\n(14.1)\nt increases only when w is at 0. more precisely, if ws(\u03c9) (cid:16)= 0 for r \u2264 s \u2264 t, then\n\n(2) l0\n(\u03c9) = l0\nl0\nt is called the local time at 0. the equation (14.1) is called the tanaka formula.\n\n(\u03c9).\n\n0\n\nt\n\nt\n\n.\n\nl0\nr\n\nproof define\n\nf\u03b5 (x) =\n\n(cid:13)\nx2/2\u03b5,\n|x| \u2212 (\u03b5/2),\n\n|x| < \u03b5;\n|x| \u2265 \u03b5.\n\nthe function f\u03b5 is an approximation to the function | \u00b7 |, and note that f\u03b5 (0) = f\n\u03b5 (x) = \u03b5\u221211[\u2212\u03b5,\u03b5](x), except at x = \u00b1\u03b5.\n(cid:3)(cid:3)\nf\n\n(cid:3)\n\u03b5 (0), while\n\n94\n\n "}, {"Page_number": 113, "text": "14.1 basic properties\n\n95\n\n(cid:3)\n\n(cid:3)\n\nwe apply the extension of it\u02c6o\u2019s formula given in exercise 11.2 to f\u03b5 (wt ) and obtain\n\nt\n\nt\n\nt\n\nt\n\n0\n\n0\n\n0\n\n2\n\n0\n\nf\n\nf\n\n(cid:3)\n\n(cid:3)\n\n(14.2)\n\ne sup\nt\u2264t0\n\nf\u03b5 (wt ) =\n\n\u03b5 (ws) dws + 1\n(cid:3)\n\nsgn (ws) dws\n\n\u03b5 (ws) dws \u2212\n(cid:3)\n\n(cid:20)(cid:20)(cid:20)2 \u2192 0,\n\n(cid:3)(cid:3)\n\u03b5 (ws) ds.\n\u03b5 (x) \u2192 sgn (x) boundedly. by\n(cid:3)\n\nf\nas we let \u03b5 \u2192 0, we see that f\u03b5 (x) \u2192 |x| uniformly, and f\ndoob\u2019s inequalities, if t0 > 0,\n\n| f\u03b5 (wt ) \u2212 |wt|| \u2192 0, a.s. therefore there exists an increasing process l0\n\n(cid:20)(cid:20)(cid:20)(cid:3)\n(cid:20)(cid:20)(cid:20) 1\nwhile supt\u2264t0\nsubsequence \u03b5n \u2192 0 such that\n(cid:15)\nhence for almost every \u03c9 there is convergence uniformly over t in finite intervals, so l0\nt is\n0 1[\u2212\u03b5n,\u03b5n](ws) ds increases only for those times t where |wt| \u2264 \u03b5n,\ncontinuous in t. since 1\n2\u03b5n\nthen l0\n\nt increases only on the set of times when wt = 0.\n\n1[\u2212\u03b5n,\u03b5n](ws) ds \u2212 l0\n\n(cid:20)(cid:20)(cid:20) \u2192 0,\n\nin the tanaka formula, the stochastic integral term is a martingale, say nt. note (cid:22)n(cid:23)t = t,\nsince sgn (x)2 = 1 unless x = 0, and we have seen that brownian motion spends 0 time at\n0 (exercise 11.1). hence we have exhibited reflecting brownian motion, namely |wt|, as the\nsum of another brownian motion, nt, and a continuous process that increases only when w\nis at zero.\n\nt and a\n\n(14.3)\n\nsup\nt\u2264t0\n\n2\u03b5n\n\na.s.\n\n0\n\nt\n\nt\n\nt\n\nlet mt denote sups\u2264t ws. note we do not have an absolute value here. the following, due\n\nto l\u00b4evy, is often useful.\ntheorem 14.2 the two-dimensional processes (|w|, l0) and (m \u2212 w, m ) have the same\nlaw.\nproof let vt = \u2212nt in the tanaka formula, so that\n|wt| = \u2212vt + l0\n\n(14.4)\n\n.\n\nlet st = sups\u2264t vs. we will show st = l0\nmotion, and hence (m \u2212 w, m ) is equal in law to (s \u2212 v, s) = (|w|, l0).\nfrom (14.4), vt = l0\n\u2212 |wt|, or vt \u2264 l0\nincreases only when wt = 0 and at those times\n\nt . this will prove the result, since v is a brownian\nt for all t, hence st \u2264 l0\n\nt , since l0 is increasing. l0\nt\n\nt\n\nt\n\n= vt + |wt| = vt \u2264 st .\n\nl0\nt\n\ngiven two increasing functions with f \u2264 g, if f (t ) = g(t ) at those times when f increases,\na little thought shows that f and g are equal for all t. hence l0\nt\n\n= st for all t.\n\njust as we defined l0\n\nt via the tanaka formula, we can construct local time at the level a by\n\nthe formula\n\n(cid:3)\n\n|wt \u2212 a| \u2212 |w0 \u2212 a| =\n\nt\n\nsgn (ws \u2212 a) dws + la\n\nt\n\n,\n\n0\n\n(14.5)\n\n "}, {"Page_number": 114, "text": "96\n\nlocal times\n\nand the same proof as above shows that la\n\nt is the limit in l2 of\n\n(cid:3)\n\n1\n2\u03b5\n\nt\n\n0\n\n1[a\u2212\u03b5,a+\u03b5](ws)ds.\n\n14.2 joint continuity of local times\n\nnext we will prove that la\n\nt can be taken to be jointly continuous in both t and a.\n\ntheorem 14.3 let w be a one-dimensional brownian motion and let la\n\nof w at level a. for each a \u2208 r there exists a version(cid:14)la\n(cid:14)la\n=(cid:15)\nwe will obtain an estimate on(cid:14)n a\n\nt be the local time\nt so that with probability one,\nt is jointly continuous in t and a.\nrecall that two processes x and y are versions of each other if for each t, xt = yt, a.s.\nwe will use the kolmogorov continuity criterion, corollary 8.2, together with remark 8.3.\n0 sgn (ws \u2212 a) dws, by means of the\n\nt of la\n\nt\n\nt\n\nburkholder\u2013davis\u2013gundy inequalities.\n\nproof let m > 0 be arbitrary. it suffices to show the joint continuity for times less than or\nequal to m and for |a| \u2264 m. let\n\n\u2212(cid:14)n b\nt , where(cid:14)n a\n(cid:3)\n\nt\n\nm\u2227t\n\n=\n\nn a\nt\n\n0\n\nsgn (ws \u2212 a) dws.\n\nsince |wt \u2212a| is uniformly continuous in t and a for |t| \u2264 m,|a| \u2264 m, by the tanaka formula\n(14.5) it suffices to establish the same fact for n a\nt .\n\nlet t be a stopping time bounded by m and a < b. since (n a\nt\n\n\u2212 n b\n\n)2 \u2212 (cid:22)n a \u2212 n b(cid:23)\n\nt\n\nt is a\n\nmartingale,\n\n(cid:26)\n\ne\n\n((n a\nm\n\n\u2212 n b\n\nm\n\n(cid:27)\n\n)\u2212(n a\n= e\n\nt\n\n\u2212 n b\n\nt\n\nt\n\n(cid:16)(cid:3)\n(cid:16)(cid:3)\n(cid:16)(cid:3)\n(cid:16)(cid:3)\n\n= 4e\n\u2264 4e\n= 4e\n\n(cid:17)\n\nm\n\nm\n\n(cid:17)\n\n))2|ft\n(sgn (ws \u2212 a) \u2212 sgn (ws \u2212 b))2 ds|ft\n1[a,b](ws) ds|ft\nm+t\n1[a,b](ws) ds|ft\n1[a,b](ws+t ) ds|ft\n(cid:3)\n(cid:17)\n\n(cid:17)\n(cid:17)\n\n;\n\nm\n\nt\n\nt\n\n0\n\nm\n\n(cid:16)(cid:3)\n\nrecall exercise 11.1. from proposition 4.5 we deduce\n\n\u2264\n\nm\n\ne\n\n0\n\ne\n\n1[a,b](ws+t ) ds|ft\n(cid:26)\n\n((n a\nm\n\n\u2212 n b\n\nm\n\n) \u2212 (n a\n\nt\n\n0\n\n\u2212 n b\n\nt\n\nds \u2264 c(b \u2212 a).\n\ns\n\nc(b \u2212 a)\u221a\n(cid:27) \u2264 c|b \u2212 a|,\n\n))2|ft\n\ne [(cid:22)n a \u2212 n b(cid:23)\n\n\u2212 (cid:22)n a \u2212 n b(cid:23)\n\nt\n\nm\n\n| ft ] \u2264 c|b \u2212 a|.\n\nthus\n\nand so by (9.3)\n\n "}, {"Page_number": 115, "text": "if we write at = (cid:22)n a \u2212 n b(cid:23)\n\nt, then we have by proposition 3.14\n\n14.3 occupation times\n\n(cid:3)\n(cid:16)(cid:3)\n(cid:16)(cid:3)\n\n0\n\ne a2\nm\n\nm\n\nm\n\n= 2e\n= 2e\n= 2e\n\u2264 c|b \u2212 a|e\n\n(am \u2212 at ) dat\n(cid:17)\n(e [am | ft] \u2212 at ) dat\n(cid:3)\ne [am \u2212 at | ft] dat\ndat \u2264 c|b \u2212 a|2.\n\nm\n\nm\n\n0\n\n0\n\n97\n\n(cid:17)\n\napplying the burkholder\u2013davis\u2013gundy inequalities,\n\n0\n\n|n a\n\nt\n\n\u2212 n b\n\nt\n\n|4] \u2264 c|b \u2212 a|2.\n\ne [sup\nt\u2264m\n\n(14.6)\n\n| f (t )\u2212 g(t )|, we see n a\n\nby the kolmogorov continuity criterion applied on the banach space of continuous functions\nwith the metric d ( f , g) = supt\u2264m\nt is continuous as a function of a for\na in the dyadic rationals in [\u2212m, m], uniformly over t \u2264 m. therefore lt\na is continuous over\na in the dyadic rationals in [\u2212m, m], uniformly for t \u2264 m. also, (14.5) and (14.6) imply\n\n(cid:28)|a \u2212 b| \u2227 1\nnote that if we define(cid:14)la\nrationals, then (14.7) implies that(cid:14)la\ndyadic rationals and t \u2264 m implies the joint continuity of(cid:14)la\n\n|la\n\u2212 lb\n(14.7)\nt where the limit is as bn \u2192 a and bn is in the dyadic\n= la\nt over a in the\n\nt , a.s. the uniform continuity of la\n\ne [sup\nt\u2264m\n= lim lbn\n\n|4] \u2264 c\n\n(cid:29)2.\n\nt\n\nt\n\nt\n\nt\n\nt .\n\n14.3 occupation times\n\nif we integrate local times over a set, we obtain occupation times. more precisely, we have\nthe following.\n\ntheorem 14.4 let wt be a brownian motion and ly\ntake ly\n\nt to be jointly continuous in t and y. if f is non-negative and borel measurable,\n\nt the local time at the level y, where we\n\n(cid:3)\n\n(cid:3)\n\nf (y)ly\n\nt dy =\n\nt\n\n0\n\nf (ws) ds,\n\na.s.\n\n(14.8)\n\nwith the null set independent of f and t.\n\nproof suppose we prove the above equality for each c2 function f with compact support\nand denote the null set by n f . taking a countable collection { fi} of non-negative c2 functions\nwith compact support that are dense in the set of non-negative continuous functions on r\nwith compact support and letting n = \u222ain fi, then if \u03c9 /\u2208 n we have the above equality for\nall fi. by taking limits, we have (14.8) for all bounded and continuous f . a further limiting\nprocedure implies our result.\n\n "}, {"Page_number": 116, "text": "(cid:15)\n\nf (y)ly\n\nt dy\n\n98\n\nlocal times\n\n(cid:3)\n\nsuppose f is bounded and c2 with compact support. notice that the process\n\nis increasing and continuous. define\n\ng(x) =\n(14.9)\n(cid:3)(cid:3) = f . if we take the tanaka formula (14.5), replace a by\nby exercise 14.1, g is c2 with 1\n2 g\ny, multiply by f (y), and integrate over r with respect to y, we see that\n\nf (y)|x \u2212 y| dy.\n(cid:3)\n\ng(wt ) \u2212 g(w0) = martingale +\n\nf (y)ly\n\nt dy.\n\nt\n\n(cid:3)(cid:3)(ws) ds\ng\n\nt\n\n0\n\n(cid:3)\n\n0\n\nt\n\nf (ws) ds.\n\n0\n\nusing it\u02c6o\u2019s formula,\n\nthus\n\n(cid:3)\ng(wt ) \u2212 g(w0) = martingale + 1\n= martingale +\n\n2\n\n(cid:3)\n\n(cid:3)\n\nf (y)ly\n\nt dy \u2212\n\nt\n\n0\n\nt\n\n0\n\nf (ws) ds\n\nis a continuous martingale with paths locally of bounded variation, hence by theorem 9.7 it\nis identically 0.\n\n14.1 suppose f is c2 with compact support and\ng(x) =\n\nf (y)|x \u2212 y| dy.\n\nexercises\n\n(cid:3)\n\nshow that g is c2 and g\n\n(cid:3)(cid:3) = 2 f .\n(cid:3)\n\n14.2 let ly\n\nt be the jointly continuous local times of a brownian motion w . show\n\n1\n2\u03b5\n\nt\n\n1[y\u2212\u03b5,y+\u03b5](ws ) ds \u2192 ly\n\nt\n\n,\n\na.s.\n\n0\n\nshow the null set can be taken to be independent of y. thus there is no need to take a subsequence\n\u03b5n to get almost sure convergence to ly\nt .\n\n14.3 let w be a brownian motion and fix t. show that the function x \u2192 (cid:15)\n\ncontinuous, a.s., but that the function x \u2192 1(\u2212\u221e,x](wt ) is not continuous.\n\n0 1(\u2212\u221e,x](ws ) ds is\n14.4 let {ft} be a filtration satisfying the usual conditions. suppose wt is a brownian motion and\nxt = wt + at, where xt \u2265 0 for all t, a.s., and at is an increasing continuous adapted process\nsuch that a increases only at those times when xt = 0. suppose also that x\n(cid:3)\nt, where\n(cid:3)\n(cid:3)\nt is an increasing continuous adapted process that increases only\nx\n= xt and at = a\nt\nwhen x\nt\n\n\u2265 0 for all t, a.s., and a\n= 0. show that x\n(cid:3)\n\n= wt + a\n\n(cid:3)\n\n(cid:3)\n\nt\n\nt\n\nt\n\nt, a.s., for all t \u2265 0.\n(cid:3)\nt the local time at 0. since l0\nt . show that the support of dl0\nt\n\nt is increasing, for each \u03c9 there\nis equal to {t : wt = 0}.\n\n14.5 let w be a brownian motion and l0\nis a lebesgue\u2013stieltjes measure dl0\n\n "}, {"Page_number": 117, "text": "exercises\n\n99\n\nsince theorem 14.1(2) states that l0\nto show is that with probability one, if wu(\u03c9) = 0 and t < u < v, then l0\n\nt does not increase when wt is not equal to 0, what you need\n\n(\u03c9).\n\n(\u03c9) > l0\nt\n\nv\n\n14.6 use tanaka\u2019s formula to show that if ly\nt\n\na \u2264 x \u2264 y \u2264 b, and t = inf{t > 0 : wt /\u2208 [a, b]}, then\n\nis the local time of brownian motion at level y,\n\ne xly\nt\n\n= 2(x \u2212 a)(b \u2212 y)\n\n.\n\nb \u2212 a\n\nt is the local time of a brownian motion at 0, show that l0\n\n14.7 if l0\n14.8 let w be a brownian motion with local times ly\n\n\u2217\nt . set l\nt\n\n= supy ly\n\nat has the same law as\n\nt . let p > 0. prove that there\n\n\u221a\n\nal0\nt .\n\nexist constants c1, c2 such that if t is any finite stopping time,\n\u2264 c2e t p/2.\n\nc1e t p/2 \u2264 el\n\n\u2217\nt\n\nthe constants c1, c2 can depend on p, but not on t .\n\nhint: use exercise 12.11.\n\n14.9 this exercise defines the local time of a continuous martingale. if m is a continuous martingale,\nt is a submartingale and so equals a martingale plus an increasing process. the increasing\n\nthen m 2\nprocess l0\n\nt is called the local time of m at 0.\n(1) prove the analog of tanaka\u2019s formula.\n(2) define the local time la\n(3) prove that\n\n(cid:3)\n\n(cid:3)\n\n0\n\nt\n\nf (ms ) d(cid:22)m(cid:23)s =\n\nla\nt f (a) da,\n\nr\n\na.s.\n\nt of m at a. prove that la\n\nt is jointly continuous in t and a.\n\nif f is non-negative and measurable.\n\n14.10 this exercise is a complement to exercise 7.8. let w be a brownian motion and let us define\nz = {t \u2208 [0, 1] : wt = 0}, the zero set. let \u03b5 \u2208 (0, 1/2) and let \u03b4 > 0. fix \u03c9 and let {bi} be any\ncountable covering of z(\u03c9) by closed intervals such that the interiors of the bi\u2019s are pairwise\ndisjoint and the length of each bi is less than or equal to \u03b4. we write bi = [ai, bi].\nlet \u03b5 > 0. since l0 has the same law of the maximum of brownian motion, there exists a c\n\n(depending on \u03c9) such that\n\nfor each 0 \u2264 s \u2264 t \u2264 0. write(cid:9)\n\n|bi \u2212 ai| 1\n\n2\n\ni\n\n\u2212 \u03b5\n\n2\n\n|bi \u2212 ai| 1\n\n2\n\n\u2212 \u03b5\n\n2\n\n\u2212 l0\n\ns\n\nl0\nt\n\n2\n\n\u2264 c(t \u2212 s) 1\n(cid:9)\n(cid:9)\n\nc\n\nc\n\ni\n\n\u2212\u03b5 \u2265 \u03b4\u2212\u03b5/2\n\u2265 \u03b4\u2212\u03b5/2\n= \u03b4\u2212\u03b5/2\n\nc\n\nc\n\n\u2212 l0\n\nai\n\n)\n\n(l0\nbi\n\ni\n\n\u2212 l0\n0].\n\n[l0\n1\n\nshow that this implies that the hausdorff dimension of z is at least 1/2.\n\n "}, {"Page_number": 118, "text": "15\n\nskorokhod embedding\n\nsuppose y is a random variable with mean zero and finite variance. skorokhod proved the\nremarkable fact that if w is a brownian motion, there exists a stopping time t such that\nwt has the same law as y . without any restrictions on t , there is a trivial solution (see\nexercise 15.1), so one wants to require that e t < \u221e. skorokhod\u2019s construction required\nan additional random variable that is independent of the brownian motion, but since that\ntime there have been 15 or 20 other constructions, most of which don\u2019t require the extra\nrandomization, that is, t is a stopping time for the minimal augmented filtration generated\nby w .\n\nalthough conceptually some constructions are easier than others, none is easy from the\npoint of view of technical details. we will give a construction that doesn\u2019t have any optimality\nproperties, but is a nice example of stochastic calculus. then we will use this to prove an\nembedding for random walks.\n\na function f : r \u2192 r is a lipschitz function if there exists a constant k such that\n\n15.1 preliminaries\n\n| f (y) \u2212 f (x)| \u2264 k|y \u2212 x|,\n\nx, y \u2208 r.\n\n(15.1)\n\nby the mean value theorem, if f has a bounded derivative, then f is a lipschitz function.\n\nwe will need the following well-known theorem from the theory of ordinary differential\n\nequations.\ntheorem 15.1 suppose f : [0,\u221e) \u00d7 r \u2192 r is a bounded function and there exists a\npositive real k such that\n\n|f (t, x) \u2212 f (t, y)| \u2264 k|x \u2212 y|\n\nfor all t \u2265 0 and all x, y \u2208 r. let y0 \u2208 r, define the function y0 by y0(t ) = y0 for all t \u2265 0,\nand define the function yi inductively by\nyi+1(t ) = y0 +\n\nf (s, yi(s)) ds,\n\nt \u2265 0.\n\n(15.2)\n\n(cid:3)\n\nt\n\nthen the functions yi converge uniformly on bounded intervals to a function y that satisfies\n\ny(t ) = y0 +\n\nt\n\nf (s, y(s)) ds.\n\n(15.3)\n\n0\n\n(cid:3)\n\n0\n\n100\n\n "}, {"Page_number": 119, "text": "for any s such that f (s, y(s)) is continuous at s, y satisfies\n\n15.1 preliminaries\n\n= f (s, y(s)).\n\ndy\nds\n\nthe solution to (15.3) is unique.\n\n101\n\n(15.4)\n\nthis inductive procedure for obtaining the solution to (15.4) is known as picard iteration.\n\nproof note each yi(t ) is bounded in absolute value by |y0| + t sup|f|. let gi(t ) =\nsups\u2264t\n\n|yi+1(s) \u2212 yi(s)|. if s \u2264 t, then\n|yi+1(s) \u2212 yi(s)| =\n\u2264\n\n(cid:20)(cid:20)(cid:20)\n\ns\n\n[f (r, yi(r)) \u2212 f (r, yi\u22121(r))] dr\n|f (r, yi(r)) \u2212 f (r, yi\u22121(r))| dr\n|yi(r) \u2212 yi\u22121(r)| dr\n\nt\n\n(cid:20)(cid:20)(cid:20)(cid:3)\n(cid:3)\n(cid:3)\n(cid:3)\n\n0\n\n0\nt\n\n\u2264 k\n\u2264 k\n\n0\n\nt\n\n0\n\ngi\u22121(r) dr.\n\n(cid:3)\n\ntaking the supremum over s \u2264 t, we have\ngi(t ) \u2264 k\n\nt\n\n0\n\n(cid:15)\n\ngi\u22121(r) dr.\n\n(cid:15)\nfix t0. now g1(t ) is bounded for t \u2264 t0, say by l. then g2(t ) \u2264 k\n\nt \u2264 t0, and then g3(t ) \u2264 k\nby induction gi(t ) \u2264 ki\u22121lti\u22121/(i \u2212 1)! we conclude\n\n(klr) dr = k2lt2/2 and g4(t ) \u2264 k\n\n(cid:15)\n(cid:12)\u221e\n0 l dr = klt for each\n(k2lr2/2) dr = k3lt3/3!\ni=1 gi(t0) < \u221e.\n|yn(s) \u2212 ym(s)| \u2264 n\u22121(cid:9)\n\nthen\n\nsup\ns\u2264t0\n\ngi(t0),\n\nt\n0\n\nt\n0\n\nt\n\nwhich tends to zero as m and n tend to infinity. by the completeness of the space c[0, t0],\nthere exists a continuous function y such that sups\u2264t0\n\nf is continuous in the x variable, so taking the limit in (15.2) shows that y solves (15.3).\nif f is continuous at a particular value of s, then (15.4) holds by the fundamental theorem of\ncalculus.\nto prove uniqueness, suppose x and y are solutions to (15.4) and let us set g(t ) =\n\ni=m\n|yn(s) \u2212 y(s)| \u2192 0 as n \u2192 \u221e.\n\n|x(s) \u2212 y(s)|. if s \u2264 t, then\n\nsups\u2264t\n\n(cid:3)\n\n0\n\ns\n\n(cid:3)\n(cid:3)\n\n0\n\n0\n\n|x(s) \u2212 y(s)| \u2264\n\u2264 k\n\u2264 k\n\n|f (r, x(r)) \u2212 f (r, y(r))| dr\n|x(r) \u2212 y(r)| dr\n\nt\n\nt\n\ng(r) dr.\n\n "}, {"Page_number": 120, "text": "102\ntaking the supremum over s \u2264 t, we obtain\ng(t ) \u2264 k\n\n(cid:3)\n\nt\n\nskorokhod embedding\n\n(cid:15)\nfor t \u2264 t0, we have|x(t )| and|y(t )| bounded by a constant, say l, so g(t ) is bounded for t \u2264 t0.\nwe then have g(t ) \u2264 k\n0 klr dr = k2lt2/2.\niterating, we have g(t ) \u2264 kitil/i! for each i, and hence g(t ) = 0. this is true for each t,\nhence x(s) = y(s) for all s \u2264 t0.\n\n(cid:15)\n0 l dr = klt for each t \u2264 t0 and then g(t ) \u2264 k\n\ng(r) dr.\n\n0\n\nt\n\nt\n\nif the random variable y that we are considering is equal to 0, a.s., we can just let our\nstopping time t equal 0, a.s., and then wt = 0 = y if w is a brownian motion. in the\nremainder of this section and the next we assume e y = 0, e y 2 < \u221e, but that y is not\nidentically zero.\n\ndefine\n\nps(y) = 1\u221a\n2\u03c0s\n\n\u2212y2/2s,\ne\n\n(cid:3)\nthe density of a mean zero normal random variable with variance s. use p\ns\nderivative of ps with respect to x.\nlemma 15.2 suppose w is a brownian motion and g : r \u2192 r such that e [g(w1)2] < \u221e.\nfor 0 < s < 1, let\n\n(x) to denote the\n\n(cid:3)\n\nand\n\nwe have\n\nand\n\nu2t/2\n\na(s, x) = \u2212\n(cid:3)\n\nb(s, x) =\n\n(cid:3)\n1\u2212s\np\n\n(z \u2212 x)g(z) dz\n\np1\u2212s(z \u2212 x)g(z) dz.\n(cid:3)\n\n1\n\na(s, ws) dws,\n\na.s.\n\n0\n\ng(w1) = e g(w1) +\n\ne [g(w1) | f s] = b(s, ws),\n\n(15.5)\n\n(15.6)\n\n(15.7)\n\n(15.8)\n\nproof we will first prove (15.7), and we will first look at the case when g(x) = eiux.\n\nby it\u02c6o\u2019s formula with the function f (x) = ex applied to the semimartingale xt = iuwt +\n\n(cid:3)\n\neiuwt+u2t/2 = 1 +\n\n0\n\n= 1 + iu\n\na.s.\n\n(cid:3)\n\nt\n\n(\u2212u2)exs ds\n\n0\n\nt\n\nt\n\n(cid:3)\nexs d (iuws + u2s/2) + 1\neiuws+u2s/2 dws,\n(cid:3)\n\n2\n\n0\n\niueiuwseu2 (s\u22121)/2 dws.\n\n1\n\n0\n\nso\n\neiuw1 = e\n\n\u2212u2/2 +\n\n "}, {"Page_number": 121, "text": "15.1 preliminaries\n\n103\n\nwe need to check that\n\nusing integration by parts,\na(s, x) = \u2212\n= iu\n\n(cid:3)\n(cid:3)\n\niueiuxeu2 (s\u22121)/2 = a(s, x).\n(cid:3)\n\n(z \u2212 x)g(z) dz =\n\n(cid:3)\n1\u2212s\np\n1\u221a\n2\u03c0 (1 \u2212 s)\n\n\u2212(z\u2212x)2/2(1\u2212s)eiuz dz.\ne\n\np1\u2212s(z \u2212 x)g\n\n(cid:3)(z) dz\n\nthis is iu times the characteristic function of a normal random variable with mean x and\nvariance 1 \u2212 s, and so by (a.25) equals\n(cid:3)\n\nas desired. we therefore have\n\n\u2212u2 (1\u2212s)/2,\n\niueiuxe\n\n(cid:3)\n\neiuw1 = e eiuw1 \u2212\n\n1\n\n(cid:3)\n1\u2212s\np\n\n(z \u2212 ws)eiuz dz dws.\n\n(15.9)\nnow suppose g is in the schwartz class (see section b.2), replace u by \u2212u in (15.9),\n\nmultiply by the fourier transform of g, and integrate over u \u2208 r. we then obtain\n\n0\n\n(cid:3)\n(2\u03c0 )\u22121g(w1) = (2\u03c0 )\u22121e g(w1)\n\n(cid:3) (cid:3)\n\nwhere(cid:2)g is the fourier transform of g. using the fubini theorem (check that there is no\n\n0\n\n1\n\n(cid:3)\n1\u2212s\np\n\n(z \u2212 ws)e\n\n\u2212\n\ntrouble with the stochastic integral; see exercise 15.2) and the inversion formula for fourier\ntransforms, the triple integral on the right-hand side of (15.10) is equal to\n\n\u2212iuz(cid:2)g(u) dz dws du,\n\n(15.10)\n\n\u2212 (2\u03c0 )\u22121\n\n(cid:3)\n1\u2212s\np\n\n(z \u2212 ws)g(z) dz dws,\n\n(15.11)\n\n(cid:3)\n\n(cid:3)\n\n1\n\n0\n\nwhich gives us (15.7) when g is the schwartz class. a limit argument gives us (15.7) for all\ng that we are interested in.\n\nto prove (15.8) we again start with the case g(x) = eiux. we have\n\ne [eiuw1 | fs] = eiuws e [eiu(w1\u2212ws ) | fs] = eiuws e [eiu(w1\u2212ws )]\n\n= eiuwse\n\n\u2212u2 (1\u2212s)/2,\n\nusing the independent increments property of brownian motion and (a.25). on the other\nhand, the definition of b(s, x) shows that when g(x) = eiux, b(s, x) is the characteristic\nfunction of a normal random variable with mean x and variance 1 \u2212 s, so\n\nb(s, x) = eiuxe\n\n\u2212u2 (1\u2212s)/2.\n\nreplacing x by ws proves (15.8) in the case g(x) = eiux. we extend this to general g in the\nsame way as in the proof of (15.7).\n\n "}, {"Page_number": 122, "text": "(15.12)\n\n(15.13)\n\n104\n\nskorokhod embedding\n\nnext, we want to find a reasonable function g such that g(w1) is equal in law to y , where\nagain w is a brownian motion. let fy (x) = p(y \u2264 x), the distribution function of y and let\n\u0001(x) = p(w1 \u2264 x). then\n\np(\u0001(w1) \u2264 x) = p(w1 \u2264 \u0001\u22121(x)) = \u0001(\u0001\u22121(x)) = x\nfor x \u2208 [0, 1], so \u0001(w1) is a uniform random variable on [0, 1]. define\n\ng(x) = f\n\u22121\nwe use the right-continuous version of f\ny\n\n\u22121\ny\n\n(\u0001(x)).\n\u22121\nif f\ny\n\np(g(w1) \u2264 x) = p(\u0001(w1) \u2264 fy (x)) = fy (x),\n\nis not continuous. then\n\nor y is equal in law to g(w1) as desired. note g is an increasing function.\n\nwe will need the following estimates.\n\nproposition 15.3 let g be defined by (15.12) and define a and b by (15.5) and (15.6).\n(1) for each l > 0 and s0 < 1, a is continuously differentiable on [0, s0] \u00d7 [\u2212l, l]. also,\nfor each l > 0 and s0 < 1, a is bounded below by a positive constant on [0, s0] \u00d7 [\u2212l, l].\n(2) for each l > 0 and s0 < 1, b is continuously differentiable on [0, s0] \u00d7 [\u2212l, l].\n(3) for each s \u2208 [0, s0], the function x \u2192 b(s, x) is strictly increasing. for each fixed s,\nlet b(s, x) be the inverse of b(s, x) (so that b(s, b(s, x)) = x and b(s, b(s, x)) = x). for each\nl > 0 and s0 < 1, b is continuously differentiable on [0, s0] \u00d7 [\u2212l, l].\nproof to start, we observe that for every r > 0,\n\nif m is a non-negative integer, then by the cauchy\u2013schwarz inequality\n\nsince |z|m \u2264 m!e\n(cid:3)\n|z|\nand the fact that e y 2 < \u221e,\n\n|z|mer|z|\n\ne er|w1| \u2264 e erw1 + e e\n\n\u2212rw1 < \u221e.\n\n(cid:16)\n\n(cid:3)\n\u2212z2/2|g(z)| dz \u2264 m!\ne\n(cid:10)\n= m!e\n(cid:10)\n\u2264 m!\n\u2264 m!\n\n\u2212z2/2|g(z)| dz\ne\n\ne(r+1)|z|\n(cid:11)1/2\ne(r+1)|w1||g(w1)|]\n(cid:11)1/2\ne e2(r+1)|w1|\ne e2(r+1)|w1|\n\n(e|g(w1)|2)1/2\n(e y 2)1/2 < \u221e.\n\n|z \u2212 x|\n(1 \u2212 s)3/2 e\n\u2264 c|z \u2212 x|e\n\u2264 c(|z| + l)e\n\u2264 c|z|ec\n(cid:3)|z|\n\n\u2212(z\u2212x)2/2(1\u2212s)\n\n\u2212x2/2(1\u2212s)ezx/2(1\u2212s)e\n|z|l/2(1\u2212s0 )e\n\u2212z2/2\n\u2212z2/2 + cec\n(cid:3)|z|\n\u2212z2/2.\n(cid:3)\ne\ne\n\n\u2212z2/2(1\u2212s)\n\nc|z|ec\n(cid:3)|z|\n\n\u2212z2/2|g(z)| dz +\ne\n\n(cid:3)|z|\n\n\u2212z2/2|g(z)| dz,\ne\n\ncec\n\nwe now turn to (1).\n|p\n(cid:3)\n1\u2212s\n\n(z \u2212 x)| \u2264 c\n\ntherefore\n\n(cid:3)\n\n|a(s, x)| \u2264\n\nwhich is bounded by (15.13). this gives an upper bound for a.\n\n "}, {"Page_number": 123, "text": "15.2 construction of the embedding\n\n105\n\nby the mean value theorem,\n\n(z \u2212 (x + h))| \u2264 c|h|(1 + |z|2 + l2)e\n\n\u2212(z\u2212x)2/2(1\u2212s)\n\n|p\n(cid:3)\n1\u2212s\n\n(z \u2212 x) \u2212 p\n(cid:3)\n1\u2212s\nif s \u2264 s0, |x| \u2264 l, and |h| \u2264 1, so\n(z \u2212 x) \u2212 p\n(cid:3)\n1\u2212s\n\n(cid:20)(cid:20)(cid:20) 1\n\n(cid:3)\n1\u2212s\n(p\n\nh\n\n(cid:20)(cid:20)(cid:20) \u2264 c(1 + |z|2)ec\n\n(cid:3)|z|\n\n\u2212z2/2.\ne\n\n(z \u2212 (x + h))\n(cid:3)\n\nin view of (15.13), we can use dominated convergence to conclude that\n\n\u2202a\n\u2202x\n\n(s, x) =\n\n(z \u2212 x)g(z) dz\nand that |\u2202a(s, x)/\u2202x| is bounded above on [0, s0] \u00d7 [\u2212l, l].\nby a similar argument we obtain that |\u2202a(s, x)/\u2202s| is also bounded above on [0, s0] \u00d7\n[\u2212l, l]. the same argument shows that the second partial derivatives of a are bounded, and\nhence the first partial derivatives are continuous.\n\n(cid:3)(cid:3)\n1\u2212s\np\n\nusing integration by parts,\n\na(s, x) =\n\np1\u2212s(z \u2212 x) dg(z),\n\n(cid:3)\n\nwhere the integral is a lebesgue\u2013stieltjes integral; recall that g is an increasing function.\nsince we are working under the assumption that y is not identically zero, then g is not\nidentically zero, which implies that a is bounded below for s \u2264 s0 and |x| \u2264 l.\n\nthe proof of (2) is quite similar. to prove (3), as above, we can use a dominated convergence\n\nargument to prove\n\n\u2202b(s, x)\n\n= a(s, x).\n\n\u2202x\n\nsince a(s, x) > 0 for each x and for each s < s0, we conclude that x \u2192 b(s, x) is\nstrictly increasing. the estimates for b follow from the implicit function theorem applied to\nf (s, x, y) = 0, where f (s, x, y) = b(s, x) \u2212 y.\n\n15.2 construction of the embedding\n\ntheorem 15.4 suppose y is a random variable with e y = 0 and e y 2 < \u221e. there exists a\nbrownian motion n and a stopping time t with respect to the minimal augmented filtration\nof n such that nt is equal in law to y . moreover e t = e y 2.\nproof the idea is to define m by (15.14) below and do a time change so that nt = m1 =\ng(w1). to show that t is a stopping time relative to the minimal augmented filtration for\nn, we set up an ordinary differential equation that the time change solves and use picard\niteration to show that the solution can be obtained in a constructive way.\nthe case where y is identically zero is trivial for we take t = 0, so we suppose y is\nnot identically zero. let wt be a brownian motion and let {ft} be its minimal augmented\nfiltration. define the function g by (15.12) and define a and b for s < 1 by( 15.5) and (15.6).\ndefine a(s, x) = 1 and b(s, x) = x if s \u2265 1.\n\n "}, {"Page_number": 124, "text": "106\n\nnow let\n\nand hence\n\nnote (cid:22)m(cid:23)\nt\n(15.7). let\n\n0\n\n(cid:3)\n\na(s, ws) dws,\n\n(15.14)\n\nt\n\na(s, ws)2 ds.\n\n\u2192 \u221e, a.s., as t \u2192 \u221e. since e y = 0, then e g(w1) = 0, so m1 = g(w1) by\n\nskorokhod embedding\n\n(cid:3)\n\nt\n\nmt =\n\n(cid:22)m(cid:23)\n\nt\n\n=\n\n0\n\n\u03c4t = inf{s : (cid:22)m(cid:23)\n\ns\n\n\u2265 t},\n\nthe inverse of (cid:22)m(cid:23). by theorem 12.2, if we set nt = m\u03c4t , then n is a brownian motion. let\n{gt} be the minimal augmented filtration generated by n.\nwe let t = (cid:22)m(cid:23)\n\n1. then\n\nnt = n(cid:22)m(cid:23)1\n\n= m\u03c4(cid:22)m(cid:23)\n\n1\n\n= m1 = g(w1),\n\nand nt has the same law as y .\n\nfor the integrability of t we have\n= e m 2\n\ne t = e(cid:22)m(cid:23)\n\n= e [g(w1)2] = e y 2 = var y < \u221e.\n\n1\n\n1\n\n(15.15)\nit remains to show that t is a stopping time with respect to {gt}. since t = lims\u21911 (cid:22)m(cid:23)\ns,\ns is a stopping time with respect to {gt} for each s < 1. fix k. we\nit suffices to show that (cid:22)m(cid:23)\nwill show\n(\u03c4t \u2264 s, sup\ns\u2264t\nletting k \u2192 \u221e will then show ((cid:22)m(cid:23)\nsince \u03c4 is the inverse of (cid:22)m(cid:23), then\n=\n\n|ns| \u2264 k ) \u2208 gt ,\n\u2265 t ) = (\u03c4t \u2264 s) \u2208 gt for s < 1.\n\n(15.16)\n\ns < 1.\n\n=\n\n1\n\ns\n\nd\u03c4t\ndt\n\n1\nd(cid:22)m(cid:23)\u03c4t\n\n/d\u03c4t\n\na(\u03c4t , w\u03c4t\n\n)2\n\nwith \u03c40 = 0, a.s. with b(s, x) being the inverse of b(s, x) in the x variable,\n\nms = e [m1 | fs] = e [g(w1) | fs] = b(s, ws),\n\nor\n\ntherefore\n\nws = b(s, ms),\n\ns < 1.\n\n= b(\u03c4t , m\u03c4t\n\n) = b(\u03c4t , nt )\n\nw\u03c4t\n\non the event (\u03c4t \u2264 s) if s < 1. thus \u03c4t solves the equation\n\nor\n\n\u03c4t =\n\n=\n\nd\u03c4t\ndt\n\n1\n\n,\n\n\u03c40 = 0,\n\na(\u03c4t , b(\u03c4t , nt ))2\n\n(cid:3)\n\nt\n\n1\n\na(\u03c4u, b(\u03c4u, nu))2 du.\n\n0\n\n "}, {"Page_number": 125, "text": "15.2 construction of the embedding\n\n107\n\nfix s and t and choose s0 \u2208 (s, 1). let sk = inf{t : |nt| \u2265 k} and let n k\n\nt\n\n= nt\u2227sk . define\n\n\u0001(q, r) =\n\n1\n\n(a(r, b(r, n k\nq\n\n(\u03c9)))2\n\nif r \u2264 s0. observe that \u0001 depends on \u03c9. define \u0001(q, r) = 1 for r \u2265 1 and define \u0001(q, r) by\nlinear interpolation for r \u2208 (s0, 1). note that by proposition 15.3, \u0001 is continuous, bounded,\nand there exists k > 0 such that\n\n(cid:3))| \u2264 k|r \u2212 r\n\n(cid:3)|,\n\nr \u2208 r, q \u2208 [0,\u221e).\n\n|\u0001(q, r) \u2212 \u0001(q, r\n\n\u03c4t solves the equation\n\n\u03c4t =\n\nwe solve the differential equation\n\n(cid:3)\n(cid:3)\n\n0\n\nt\n\n\u0001(u, \u03c4u) du.\n\ny(t ) =\n\nt\n\n0\n\n\u0001(u, y(u)) du\n\n(15.17)\n\nand the function y1(t ) =(cid:15)\n\nt\n0\n\nusing theorem 15.1. the function y0(t ) in the statement of theorem 15.1 is identically zero,\n\u0001(u, y0(u)) du (which depends on \u03c9 because \u0001 does) will be gt\nmeasurable, and by induction, the functions yi(t ) will be gt measurable. therefore the limit,\ny(t ), will be gt measurable. since |n k\n(\u03c9)| \u2264 k for all q and we are only interested in the\nsolution to (15.17) for y(t ) \u2264 s, then \u03c4t = y(t ) as long as \u03c4t \u2264 s; therefore (15.16) holds and\nthe proof is complete.\n\nq\n\nin the above theorem, we started with a brownian motion w , constructed a new brownian\nmotion n, and then defined our stopping time t in terms of n. we can actually start with a\nbrownian motion w and define a stopping time that is a stopping time with respect to the\nminimal augmented filtration of w .\ncorollary 15.5 letw be a brownian motion and let{ft} be the minimal augmented filtration\nfor w . let y be a random variable with e y = 0 and var y < \u221e. there exists a stopping\ntime v with respect to {ft} such that wv has the same law as y .\nproof we sketch the proof and ask you to give the details in exercise 15.3. define\n\nand solve the equation\n\n\u0001(q, r) =\n\n1\n\n(a(r, b(r, wq(\u03c9))))2\n\n= \u0001(t, \u03c4 t ),\n\n\u03c40 = 0\n\nd\u03c4 t\ndt\n\nby picard iteration. the proof of theorem 15.4 shows that the solution \u03c4 t will satisfy\n(\u03c4 t \u2264 s) \u2208 ft for every t as long as s < 1. let a be the inverse of \u03c4 , and define v = lims\u21911 as.\nthen v will be the desired stopping time.\n\n "}, {"Page_number": 126, "text": "108\n\nskorokhod embedding\n\n15.3 embedding random walks\n\nlet us give an application of skorokhod embedding to show that we can find a brownian\nmotion that is relatively close to a random walk. suppose y1, y2, . . . is an i.i.d. sequence of\nreal-valued random variables with mean zero and variance one. given a brownian motion\nwt we can find a stopping time t1 such that wt1 has the same distribution as y1. we use\nthe strong markov property at time t1 and find a stopping time t2 for wt1+t \u2212 wt1 so that\n\u2212 wt1 has the same distribution as y2 and is independent of ft1. we continue. we see\nwt1+t2\nthat the ti are i.i.d. and by theorem 15.4, e ti = e y 2\nk\ni=1 ti. then for each\n\n= 1. let uk =(cid:12)\n\ni\n\nn\ni=1 yi has the same distribution as wun.\n\nn, sn =(cid:12)\n\ntheorem 15.6\n\n|wui\n\n\u2212 wi|/\n\n\u221a\nn\n\nsup\ni\u2264n\n\ntends to 0 in probability as n \u2192 \u221e.\nproof we will show that for each \u03b5 > 0\n\n\u2212 wk| > \u03b5\n\n\u221a\n\nn) \u2264 \u03b5.\n\n(15.18)\n\n|wuk\n\nlim sup\nn\u2192\u221e\n\np(sup\nk\u2264n\n\nsince the paths of brownian motion are continuous, we can find \u03b4 \u2264 1 small such that\n\np(\n\nsup\n\ns,t\u22642,|t\u2212s|\u2264\u03b4\n\n|wt \u2212 ws| > \u03b5) < \u03b5/2.\n\n|wt \u2212 ws| > \u03b5\n\n\u221a\n\n(15.19)\nthe strong law of large numbers (theorem a.38) says that un/n \u2192 e t1 = 1, a.s., and in\n\ns,t\u22642n,|t\u2212s|\u2264\u03b4n\n\nn) < \u03b5/2.\n\nsup\n\np(\n\nfact, by proposition a.39, we even have\n\nmaxk\u2264n |uk \u2212 k|\n\n\u2192 0,\n\na.s.\n\n(15.20)\n\nby scaling,\n\ntherefore\n\nn\n\u221a\n\n|wuk\np(max\nk\u2264n\n(cid:10)\n\u2264 p(max\nk\u2264n\n\u2264 p\n\nmax\nk\u2264n\n\nn)\n\n\u2212 wk| > \u03b5\n|uk \u2212 k| > \u03b4n) + p(\n|uk \u2212 k|\n+ \u03b5\n2\n\n(cid:11)\n\n> \u03b4\n\nn\n\n.\n\n|wt \u2212 ws| > \u03b5\n\n\u221a\n\nn)\n\nsup\n\ns,t\u22642n,|t\u2212s|\u2264\u03b4n\n\nby (15.20) this will be less than \u03b5 if we take n sufficiently large.\n\nexercises\n\n15.1 without some supplemental conditions on t , the problem of skorokhod embedding is trivial.\nsuppose w is a brownian motion with respect to a filtration {ft} satisfying the usual conditions.\nsuppose y is a finite random variable and suppose h is a real-valued function such that h(w1 )\nhas the same law as y .\n\n "}, {"Page_number": 127, "text": "exercises\n\n109\n\n(1) show that if t = inf{t > 1 : wt = h(w1 )}, then wt and y have the same law.\n(2) give an example of a mean zero random variable y with finite variance such that if t is\ndefined as in (1), then e t = \u221e.\n\n15.2 show that the triple integral on the right-hand side of (15.10) is equal to the expression in\n\n(15.11).\n\n15.3 a sketch was given for the proof of corollary 15.5. provide a detailed proof.\n15.4 here is another approach to proving corollary 15.5. let y , n, t , and {gt} be as in the proof of\ntheorem 15.4.\n(1) show that there is a random variable u that is measurable with respect to \u03c3 (ns : 0 \u2264 s <\n\u221e) such that u = t , a.s.\n(2) show there is a borel measurable map h : c[0,\u221e) \u2192 [0,\u221e) such that u = h (n ).\n(3) if w is a brownian motion, define v = h (w ). show v is a stopping time with respect to\n\nthe minimal augmented filtration generated by w such that wv has the same law as y .\n\n15.5 suppose p \u2208 (0, 1/2) and y is a random variable such that p(y = 1) = p(y = \u22121) = p and\np(y = 0) = 1 \u2212 2p. let w be a brownian motion. let sx = inf{t > 0 : wt = x} and let\nt = inf{t > sx \u2227 s\u2212x : wt \u2208 {\u22121, 0, 1}}. determine x such that wt and y have the same law.\n15.6 suppose y is a mean zero random variable and there exists a real number k > 0 such that\n|y| \u2264 k, a.s. let w be a brownian motion and let t be a stopping time with e t < \u221e such\nthat wt and y have the same law. (we do not necessarily assume that t was constructed by the\nmethod of section 15.2.) let sk = inf{t : |wt| \u2265 k}. prove that t \u2264 sk, a.s.\n\n15.7 let yi be a sequence of i.i.d. random variables with p(yi = 1) = p(yi = \u22121) = 1\n\n2 , and let\nn\ni=1 yi. sn is called a simple symmetric random walk. let t1, t2, . . . and u1, u2, . . . be\n\nsn =(cid:12)\n\nas in section 15.3.\n\n(1) prove that e t p\n1\n(2) prove that if \u03b5 > 0,\n\n< \u221e for all p \u2265 1.\n\nsupk\u2264n\n\n|uk \u2212 k|\n\nn(1/2)+\u03b5\n\nlim\nn\u2192\u221e\n\n= 0,\n\na.s.\n\nhint: use doob\u2019s inequalities to estimate\n\n|uk \u2212 k| \u2265 \u03b4n(1/2)+\u03b5 ).\n\np(sup\nk\u2264n\n\n(3) show that\n\nsup\ni\u2264n\ntends to zero in probability as n \u2192 \u221e.\n\n|wui\n\n\u2212 wi|/n(1/4)+(\u03b5/2)\n\n15.8 let sn, ti, and ui be as in exercise 15.7. prove that\n\u2212 wi|\n\nsupi\u2264n\n\nlim\nn\u2192\u221e\n\n|wui\n\u221a\nn\n\n= 0,\n\na.s.\n\n15.9 let sn be a simple symmetric random walk; see exercise 15.7. let y be a bounded symmetric\nrandom variable that takes values only in z. (y being symmetric means that y and \u2212y have the\nsame law.) does there necessarily exist a stopping time n such that sn and y have the same\nlaw? why or why not?\n\n "}, {"Page_number": 128, "text": "110\n\nskorokhod embedding\n\nnotes\n\nthe survey article ob\u0142\u00b4oj (2004) summarizes many different methods of skorokhod em-\nbedding. the embedding presented here is from bass (1983); see also stroock (2003),\npp. 213\u201317.\n\n "}, {"Page_number": 129, "text": "16\n\nthe general theory of processes\n\nthe name \u201cgeneral theory of processes\u201d refers to the foundations of stochastic processes.\nspecific topics include measurability issues and classifications of stopping times. this chapter\nis fairly technical and abstract and should only be skimmed on the first reading of this book:\nread the definitions and statements of theorems, propositions, and lemmas, but not the\nproofs.\n\nthe two main results we discuss are the measurability of hitting times, and the\n\ndoob\u2013meyer decomposition of submartingales, theorem 16.29.\n\n16.1 predictable and optional processes\n\nsuppose (\u0001,f , p) is a probability space. the outer probability p\u2217\nby\n\nassociated with p is given\n\np\u2217(a) = inf{p(b) : a \u2282 b, b \u2208 f}.\n\n(16.1)\na set a is a p-null set if p\u2217(a) = 0. we suppose throughout this chapter that {ft} is a\nfiltration satisfying the usual conditions; recall from chapter 1 that this means that each ft\ncontains all the p-null sets and that \u2229\u03b5>0ft+\u03b5 = ft for each t. let \u03c0 : [0,\u221e) \u00d7 \u0001 \u2192 \u0001 be\ndefined by\n\n\u03c0 (t, \u03c9) = \u03c9.\n\n(16.2)\nwe define the predictable \u03c3 -field p to be the \u03c3 -field on [0,\u221e) \u00d7 \u0001 generated by the\ncollection of all bounded left continuous processes adapted to ft. that is, p is the \u03c3 -field\non [0,\u221e) \u00d7 \u0001 generated by the collection of all sets of the form\n{(t, \u03c9) \u2208 [0,\u221e) \u00d7 \u0001 : xt (\u03c9) > a},\n\nwhere a \u2208 r and x is a bounded, adapted, left-continuous process. the optional \u03c3 -field o\nis the \u03c3 -field on [0,\u221e) \u00d7 \u0001 generated by the collection of all bounded right-continuous\nprocesses adapted to ft. the word for predictable in french is \u201cpr\u00b4evisible.\u201d the older\nliterature uses \u201cwell measurable\u201d in place of the word \u201coptional.\u201d\nif s and t are random variables taking values in [0,\u221e], let [s, t ) = {(t, \u03c9) \u2208 [0,\u221e)\u00d7\u0001 :\ns(\u03c9) \u2264 t < t (\u03c9)}, and define (s, t ], (s, t ), etc. similarly. with this notation, [t, t ], the\ngraph of t , is equal to {(t, \u03c9) \u2208 [0,\u221e) \u00d7 \u0001 : t (\u03c9) = t < \u221e}. note that [t, t ] is a subset\nof [0,\u221e) \u00d7 \u0001, so \u03c0 ([t, t ]) = (t < \u221e).\n\n111\n\n "}, {"Page_number": 130, "text": "112\n\nthe general theory of processes\n\nexists a sequence of stopping times tn such that for all \u03c9\n\nrecall that a stopping time can take the value \u221e. a stopping time t is predictable if there\n(1) t1(\u03c9) \u2264 t2(\u03c9) \u2264 \u00b7\u00b7\u00b7 ,\n(2) limn\u2192\u221e tn(\u03c9) = t (\u03c9), and\n(3) if t (\u03c9) > 0, then tn(\u03c9) < t (\u03c9) for each n.\n\nin this case, the stopping times tn predict t or announce t . if t is a stopping time satisfying\n(1)\u2013(3) above and s = t , a.s., then we call s a predictable stopping time as well. a stopping\ntime t is totally inaccessible if p(t = s < \u221e) = 0 for every predictable stopping time s.\nfor an example of a predictable stopping time, let wt be a brownian motion started at 0\nand let t = inf{t > 0 : wt = 1}. the stopping time t is predicted by the stopping times\ntn = inf{t > 0 : wt = 1 \u2212 (1/n)}.\nfor an example of a totally inaccessible stopping time, let pt be a poisson process with\nparameter 1 and let t = inf{t : pt = 1}, the first time the poisson process jumps. since\npt has independent increments, pt \u2212 t is a martingale, just as in example 3.2. by( a.8),\ne [(pt \u2212 t )2] < \u221e. if s is a bounded predictable stopping time, by the optional stopping\ntheorem, e ps = e s. if sn are stopping times predicting s, then by monotone convergence\n\ne ps\u2212 = lim\n\nn\u2192\u221e e psn\n\n= lim\nn\u2192\u221e e sn = e s.\n\ntherefore e [ps \u2212 ps\u2212] = 0, and since pt is an increasing process, this says that p does not\njump at time s. applying this to s \u2227 m and letting m \u2192 \u221e, we see that p does not jump at\nany predictable time s, whether or not s is bounded. therefore p(t = s < \u221e) = 0, so t is\ntotally inaccessible.\n\nthe proof of the following proposition is reminiscent of that of the vitali covering theorem\n\nfrom measure theory.\n\nproposition 16.1 let t be a stopping time. there exist predictable stopping times s1, s2, . . .\nand a totally inaccessible stopping time u such that [t, t ] = [u, u ] \u222a (\u222a\u221e\nproof let\n\ni=1[si, si]).\n\na1 = sup{p(s = t < \u221e) : s is a predictable stopping time}\n\nand choose s1 to be a predictable stopping time such that p(s1 = t < \u221e) \u2265 1\ns1, . . . , sn, let\n\n2 a1. given\n\nan+1 = sup{p(s = t < \u221e, s (cid:16)= s1, . . . , s (cid:16)= sn)) :\n\ns is a predictable stopping time}\n\nand choose sn+1 such that p(sn+1 = t < \u221e, sn+1 (cid:16)= s1, . . . , sn+1 (cid:16)= sn) \u2265 1\n\n2 an+1.\n\nif this procedure stops after n steps, set u (\u03c9) equal to t (\u03c9) if t (\u03c9) is not equal to any\nof s1(\u03c9), . . . , sn(\u03c9) and equal to infinity otherwise. it is easy to check that u is a stopping\ntime that is totally inaccessible.\n\nthe other alternative is that this procedure continues indefinitely. in this case define\n\n(cid:13)\n\nu (\u03c9) =\n\nt (\u03c9), t (\u03c9) (cid:16)= s1(\u03c9), s2(\u03c9), . . . ,\n\u221e,\n\notherwise.\n\n "}, {"Page_number": 131, "text": "16.1 predictable and optional processes\n\n113\n\nthere is no problem checking that u is a stopping time, but we need to show that u is\ntotally inaccessible. since probabilities are bounded by one, we have an \u2192 0. if there exists\na predictable stopping time s such that b = p(s = u < \u221e) > 0, then b > 2an for some\nn, and in our construction we would have then chosen s in place of the sn we did choose.\ntherefore such a stopping time s cannot exist.\nproposition 16.2 (1) the optional \u03c3 -field o is generated by the collection of sets\n\n{[s, t ) : s, t stopping times}.\n\n(2) o is generated by the collection of sets of the form [a, b)\u00d7c, where a < b and c \u2208 fa.\n(3) the predictable \u03c3 -field p is generated by the collection of sets\n\n{(s, t ] : s, t stopping times}.\n\n(4) p is generated by the collection of sets\n\n{[s, t ) : s, t predictable stopping times}.\n\n(5) p is generated by the collection of sets of the form [b, c) \u00d7 c, where a < b < c and\n\nc \u2208 fa.\n(1) since 1[s,t ) is a bounded right-continuous process that is adapted to {ft}, sets\nproof\nof the form [s, t ) are optional. now suppose x is a bounded adapted process with right-\ncontinuous paths. let \u03b5 > 0, let u0 = 0, a.s., and let\nui+1 = inf{t > ui : |xt \u2212 xui\n\n| > \u03b5},\n\ni \u2265 0.\n\n(16.3)\n\nsince x has right-continuous paths,\n\n(u1 < t ) = \u2229q\u2208q+,q<t{|xq \u2212 x0| > \u03b5},\n\nwhere q+ denotes the positive rationals, and it follows that u1 is a stopping time. similarly\nui is a stopping time for each i; exercise 16.4 asks you to prove this. if we set\n\n(\u03c9) =\n\nx \u03b5\nt\n\nxui\n\n(\u03c9)1[ui (\u03c9),ui+1 (\u03c9))(t ),\n\nt\n\nthen supt\n\n|xt \u2212 x \u03b5\n\nto do that, it suffices to show that processes of the form\n\n| \u2264 \u03b5. therefore it suffices to show that each process x \u03b5 is measurable\n\nwith respect to the \u03c3 -field (cid:2)o generated by the collection of sets of the form [s, t ).\nwhere a \u2208 fui, are measurable with respect to (cid:2)o. if we set s(\u03c9) equal to ui(\u03c9) if \u03c9 \u2208 a\n\nand equal to \u221e otherwise and we set t (\u03c9) equal to ui+1(\u03c9) if \u03c9 \u2208 a and \u221e otherwise, then\nyt (\u03c9) = 1[s(\u03c9),t (\u03c9)).\n(2) if c \u2208 fa, then 1c (\u03c9)1[a,b)(t ) is a bounded right-continuous adapted process, so it is\noptional. by (1), every bounded right-continuous adapted process can be approximated by\nlinear combinations of processes of the form 1[s,t ). now 1[s,t ) = 1[s,\u221e) \u2212 1[t,\u221e), and 1[s,\u221e)\n\nyt (\u03c9) = 1a(\u03c9)1[ui (\u03c9),ui+1 (\u03c9))(t ),\n\n\u221e(cid:9)\n\ni=0\n\n "}, {"Page_number": 132, "text": "n2n\u22121(cid:9)\n\nk=0\n\n(cid:13)\n\n114\n\nthe general theory of processes\n\nis the limit of 1[sn,\u221e), where sn = k/2n if (k \u2212 1)/2n \u2264 s < k/2n, and we can similarly\napproximate 1[t,\u221e). note\n\n1[sn (\u03c9),\u221e)(t ) =\n\n1((k\u22121)/2n\u2264s(\u03c9)<k/2n )1[k/2n,\u221e)(t ).\n\n\u221e(cid:9)\n\nk=1\n\nsince ((k \u2212 1)/2n \u2264 s(\u03c9) < k/2n) \u2208 fk/2n and 1[k/2n,\u221e)(t ) is the limit of 1[k/2n,m)(t ) as\nm \u2192 \u221e, we see that every bounded right-continuous adapted process is measurable with\nrespect to the \u03c3 -field generated by processes of the form 1a(\u03c9)1[a,b)(t ), where a is fa\nmeasurable.\n\nfor (3), 1(s,t ] is left continuous, bounded, and adapted, hence predictable. any left-\n\ncontinuous adapted bounded process can be approximated by processes of the form\n\nxk/2n (\u03c9)1(k/2n,(k+1)/2n](t ),\n\nwhich in turn can be approximated by linear combinations of processes of the form y =\n1a(\u03c9)1(a,b](t ), where a is fa measurable. such a process y is of the form 1(s,t ] if we define\ns and t by\n\n(cid:13)\n\ns(\u03c9) =\n\n\u03c9 \u2208 a,\na,\n\u221e, \u03c9 /\u2208 a,\n\nt (\u03c9) =\n\n\u03c9 \u2208 a,\nb,\n\u221e, \u03c9 /\u2208 a.\n\nstopping times sn = s + 1\n\nto prove (4), note that s + 1\n\u2212 1\nn for n > k). we have\n(s, t ] = \u222ak{\u2229m[s + 1\n\nk\n\n, t + 1\n\nm\n\n)}.\n\nk\n\nk is always a predictable stopping time (predicted by the\n\non the other hand, if s and t are predictable and are predicted by sequences sn and tm,\nrespectively, then\n\n[s, t ) = \u2229n{\u222am(sn, tm]}.\n\n(4) now follows by using (3).\n\n(5) as long as a + (1/n) < b, the processes 1c (\u03c9)1(b\u2212(1/n),c\u2212(1/n)](t ) are left continuous,\nbounded, and adapted, hence predictable. the process 1c (\u03c9)1[b,c)(t ) is the limit of these\nprocesses as n \u2192 \u221e, so is predictable. on the other hand, if xt is a bounded adapted\nleft-continuous process, it can be approximated by\n\nn2n\u22121(cid:9)\n\nk=1\n\nx(k\u22121)/2n (\u03c9)1(k/2n,(k+1)/2n](t ).\n\neach summand can be approximated by linear combinations of processes of the form\n1c (\u03c9)1(b,c](t ), where c \u2208 fa and a < b < c. finally, 1c (\u03c9)1(b,c](t ) is the limit of\n1c (\u03c9)1[b+(1/n),c+(1/n))(t ) as n \u2192 \u221e.\n\na consequence of proposition 16.2(1) and (4) is that p \u2282 o.\n\n "}, {"Page_number": 133, "text": "16.2 hitting times\n\n115\n\n16.2 hitting times\n\nlet s be a separable metric space. suppose {ft} is a filtration satisfying the usual conditions\nand x is a stochastic process taking values s whose paths are right continuous and such that\nthe jump times are totally inaccessible. saying the jump times are totally inaccessible means\nthat if t is a predictable stopping time, then xt\u2212 = xt , a.s., where xt\u2212 = lims<t,s\u2192t xs.\n\nif b is a borel subset of a metric space s and x is a s-valued process, let\n\nand\n\nub = inf{t \u2265 0 : xt \u2208 b}\n\ntb = inf{t > 0 : xt \u2208 b}.\n\ntb is known as the first hitting time of b and ub as the first entry time of b.\n\nproposition 16.3 (1) if a is an open set, then ta and ua are stopping times.\n\n(2) if a is a compact set, then ta and ua are stopping times.\n\nproof\n\n(1) since the paths of xt are right continuous and a is open, for each t,\n\n(ta < t ) = \u222aq\u2208q+,q<t (xt \u2208 a) \u2208 ft ,\n\nwhere q+ denotes the non-negative rationals. thus ta is a stopping time. since\n\n(ua < t ) = (ta < t ) \u222a (x0 \u2208 a) \u2208 ft ,\n\n(16.4)\n\nthen ua is also a stopping time.\n(2) now suppose a is compact and let an = {x \u2208 s : d (x, a) < 1/n}. each set an is\nopen, hence tan is a stopping time for each n. the tan increase; let t be the limit. if we show\nt = ta, a.s., this will prove ta is a stopping time.\nsince a \u2282 an, then tan\n\u2264 ta for each n. therefore t \u2264 ta. on the other hand, if\n(\u03c9) = t (\u03c9) for all n sufficiently\nn > m, then xtan\nlarge, in which case xt (\u03c9) \u2208 am, or else tan\n(\u03c9) < t (\u03c9) for all n. in the latter case,\nxt (\u03c9) = limn\u2192\u221e xtan\n(\u03c9) \u2208 am except for \u03c9\u2019s in a null set since the jump times of x are\ntotally inaccessible. in either case, xt \u2208 am. this is true for all m, so xt \u2208 \u2229mam = a, and\ntherefore ta \u2264 t .\n\n\u2208 an \u2282 am, the closure of am. either tan\n\nwe conclude ta is a stopping time. to prove ua is a stopping time, we argue using (16.4)\n\nas above.\n\nfor the proof of the following, which uses choquet\u2019s capacity theorem, we refer the reader\n\nto blumenthal and getoor (1968), section i.10. fix t and define\n\nrt (a) = {\u03c9 : xs(\u03c9) \u2208 a for some s \u2208 [0, t]} = (ua \u2264 t ).\n\n(16.5)\ntheorem 16.4 if a is a borel subset of s, then rt (a) \u2208 ft and there exists an increasing\nsequence of compact sets kn contained in a such that p(rt (kn)) \u2191 p(rt (a)).\nsince (ua \u2264 t ) = rt (a), we have the following as an immediate corollary.\n\n "}, {"Page_number": 134, "text": "116\n\nthe general theory of processes\n\ntheorem 16.5 for all borel sets a, ua is a stopping time.\n\nhere is the main theorem of this section.\ntheorem 16.6 suppose {ft} is a filtration satisfying the usual conditions and x is a right\ncontinuous process whose jump times are totally inaccessible. if b is a borel subset of s,\nthen tb is a stopping time.\n\n= xt+\u03b4 and u \u03b4\n\nif we let y \u03b4\nt\n\nproof\nstopping time with respect to the filtration {f \u03b4\nis a stopping time with respect to the filtration {ft}. since (1/m) + u 1/m\nstopping time with respect to {ft}.\n\n}, where f \u03b4\n\nb is a\n= ft+\u03b4. it follows that \u03b4 + u \u03b4\n\u2193 tb, then tb is a\nb\n\nb\n\nb\n\nt\n\nt\n\nt\n\n= inf{t \u2265 0 : y \u03b4\n\n\u2208 b}, then by the above, u \u03b4\n\nwe now show that the hitting times of borel sets can be approximated by the hitting times\n\nof compact sets.\n\nproposition 16.7 there exists an increasing sequence of compact sets kn contained in b\nsuch that ukn\n\n\u2193 ub on (ub < \u221e), p-a.s.\n\nn\n\n\u222a \u00b7\u00b7\u00b7 \u222a lqn\n\nn contained in b\n)) \u2191 p(rt (b)). let q j be an enumeration of the non-negative rationals. let\nn . then the kn are compact, form an increasing sequence, and are all\n\u2265 ub for all n, then s \u2265 ub. if\n\nproof for each t we can find an increasing sequence of compact sets lt\nwith p(rt (lt\nkn = lq1\nn\ncontained in b. thus ukn decreases, say to s, and since ukn\nwe prove s \u2264 ub, p-a.s., then s = ub, and we have our result.\nprove p(ub < q j < s) = 0 for all\nrq j\nenough, hence in rq j\ntherefore p(ub < q j < s) = 0.\n\nif ub < s, there exists a rational q j with ub < q j < s. hence it suffices to\n(b). since\nn ) for all n large\n(\u03c9) \u2264 q j < ub or s \u2264 q j.\n\n(b), a.s., then except for a null set, \u03c9 will be in rq j\n\n(kn) if n is large enough. then ukn\n\nthen \u03c9 \u2208 rq j\n\nn ) \u2191 rq j\n(lq j\n\nj. if ub < q j,\n\n(lq j\n\ntheorem 16.8 there exists an increasing sequence of compacts kn contained in b such that\ntkn\n\n\u2193 tb.\n\n= xt+\u03b4 and u \u03b4\n. let kn = l1\n\nproof let y \u03b4\nt\ny 1/m\n, for each m there exist compact sets lm\n\u2193 u 1/m\nt\nu 1/m\nlm\nn\ncontained in b, and u 1/m\nkn\nwe write\n\n= inf{t \u2265 0 : yt \u2208 b}. applying the above proposition to\nn , increasing in n and contained in b, such that\nn. then kn is an increasing sequence of compact sets\n\u2193 tb.\n\n. also, for each n, 1/m+ u 1/m\n\n\u2193 tkn and 1/m+ u 1/m\n\n\u222a \u00b7\u00b7\u00b7 \u222a ln\n\n\u2193 u 1/m\n\nkn\n\nb\n\nb\n\nb\n\nb\n\nn\n\ntb = lim\n= lim\n\nm\n\nn\n\n(1/m + u 1/m\nlim\nm\n\n(1/m + u 1/m\n\nkn\n\nb\n\n) = lim\n\nm\n\nlim\nn\n) = lim\n\nn\n\n(1/m + u 1/m\ntkn\n\nkn\n\n.\n\n)\n\nsince 1/m + u 1/m\njustified. since tkn is decreasing, this completes the proof.\n\nkn\n\nis decreasing in both m and n, the change in the order of taking limits is\n\n "}, {"Page_number": 135, "text": "16.3 the debut and section theorems\n\n117\n\n16.3 the debut and section theorems\n\nif e \u2282 [0,\u221e) \u00d7 \u0001, let de = inf{t \u2265 0 : (t, \u03c9) \u2208 e}, the debut of e. an important\ngeneralization of theorem 16.6 is the following, known as the debut theorem.\ntheorem 16.9 if e \u2208 o, then de is a stopping time.\nthe proof of this theorem is beyond the scope of this book, and we refer the reader to\n\ndellacherie and meyer (1978) for a proof.\n\nusing theorem 16.9, we can weaken the assumptions on x in theorem 16.6.\n\ntheorem 16.10 if x is an optional process taking values in s and b is a borel subset of s,\nthen ub and tb are stopping times.\nproof since b is a borel subset of s and x is an optional process, then 1b(xt ) is also an\noptional process. ub is then the debut of the set e = {(s, \u03c9) : 1b(xs(\u03c9)) = 1}, and therefore\nis a stopping time.\n\nto prove that tb is a stopping time, we argue exactly as in the proof of theorem 16.6.\n\nremark 16.11 in the theory of markov processes, the notion of completion of a \u03c3 -field is a\nbit different. however it is still the case that the hitting times of borel sets by right continuous\nprocesses are stopping times. see remark 20.4.\n\nthe optional section theorem is the following.\n\ntheorem 16.12 if e is an optional set and \u03b5 > 0, there exists a stopping time t such that\n[t, t ] \u2282 e and p(\u03c0 (e )) \u2264 p(t < \u221e) + \u03b5.\n\nthe statement of the predictable section theorem is very similar.\n\ntheorem 16.13 if e is a predictable set and \u03b5 > 0, there exists a predictable stopping time\nt such that [t, t ] \u2282 e and p(\u03c0 (e )) \u2264 p(t < \u221e) + \u03b5.\n\nagain we refer to dellacherie and meyer (1978) for proofs. we note that proposition\n16.7 is a precursor of the optional section theorem. to see this, let a be a borel set\nand let e = {(t, \u03c9) : xt \u2208 a}. then de = ua. if the process is right continuous, then\n\u2208 kn \u2282 a, where the kn are as in proposition 16.7, and the graphs of the ukn are\nxukn\ncontained in e.\n\nhere is a corollary of theorems 16.12 and 16.13.\n\ncorollary 16.14 (1) if x and y are optional processes such that p(xt = yt ) = 1 for every\nfinite stopping time t , then x and y are indistinguishable: p(xt = yt for all t ) = 1.\n(2) if x and y are predictable processes with p(xt = yt ) = 1 for every finite predictable\n\nstopping time t , then x and y are indistinguishable.\nproof we prove (1), the proof of (2) being similar. let f = {(t, \u03c9) : xt (\u03c9) (cid:16)= yt (\u03c9)}. then\nf is an optional set, and if p(\u03c0 (f )) > 0, there exists a stopping time u with [u, u ] \u2282 f\n\n "}, {"Page_number": 136, "text": "118\nand p(u < \u221e) > 0. by looking at t = u \u2227 n for sufficiently large n, we obtain a\ncontradiction.\n\nthe general theory of processes\n\nanother application of the section theorems is the following.\n\nproposition 16.15 suppose [t, t ] is a predictable set. then t is a predictable stopping\ntime.\n\nproof since t is the debut of [t, t ], then t is a stopping time. by the predictable section\ntheorem, theorem 16.13, for each n there exists a predictable stopping time sn such that\n[sn, sn] \u2282 [t, t ] and\n\np(\u03c0 ([sn, sn])) \u2265 p(\u03c0 ([t, t ])) \u2212 2\n\n\u2212n.\n\nsaying [sn, sn] \u2282 [t, t ] implies that for each \u03c9, either sn(\u03c9) = t (\u03c9) or else sn(\u03c9) = \u221e.\nthe set of \u03c9\u2019s for which t (\u03c9) < \u221e but sn(\u03c9) = \u221e has probability at most 2\nlet qn = s1\u2227\u00b7\u00b7\u00b7\u2227 sn. then the qn\u2019s are predictable stopping times by exercise 16.1, they\ndecrease, [qn, qn] \u2282 [t, t ], and p(\u03c0 ([qn, qn])) \u2265 p(\u03c0 ([t, t ]))\u22122\n\u2212n+1. let q = limn qn.\nif q(\u03c9) < \u221e, then qn(\u03c9) < \u221e for all n sufficiently large (how large depends on \u03c9); since\nqn(\u03c9) is either equal to t (\u03c9) or to \u221e, qn(\u03c9) = q(\u03c9) for all n sufficiently large, and hence\nq(\u03c9) = t (\u03c9). if t (\u03c9) < \u221e, then except for a set of \u03c9\u2019s of probability zero, qn(\u03c9) = t (\u03c9)\nfor n sufficiently large. therefore q = t , a.s.\n\nchoose rnm predicting qn as m \u2192 \u221e. choose mn large enough such that\n\n\u2212n.\n\nand p(rnmn\n\n< n, qn = \u221e) < 2\n\n\u2212n.\n\np(rnmn\n\n+ 2\nlet un = n \u2227 rnmn\nr jm j\nun(\u03c9) \u2264 r jm j\n\n\u2212n < qn < \u221e) < 2\n\u2212n\n\u2227 rn+1,mn+1\n\n(\u03c9) < q(\u03c9).\n\n\u2227 \u00b7\u00b7\u00b7 . fix n for the moment. if 0 < q(\u03c9) < \u221e, then\n(\u03c9) < q j (\u03c9) = q(\u03c9) for all j sufficiently large. choosing j > n sufficiently large,\nthe un increase; let t be the limit. by the borel\u2013cantelli lemma, if q(\u03c9) < \u221e, then\n(\u03c9) \u2265 qn(\u03c9) \u2212 2\n\u2212n for all n sufficiently large, except for a set of \u03c9\u2019s of\n\u2212n+1 for n sufficiently large, and we conclude\nif q(\u03c9) = \u221e, then qn(\u03c9) = \u221e for all n. by the borel\u2013cantelli lemma, except for a set\n\u2265 n for n sufficiently large. hence un(\u03c9) = n for n sufficiently\nof probability zero, rnmn\nlarge, so un(\u03c9) < q(\u03c9) and un(\u03c9) \u2191 q(\u03c9). thus q is predictable and t = q, a.s. (we\nleave consideration of those \u03c9 for which q(\u03c9) = 0 to the reader.)\n\nrnmn\nprobability zero. therefore un(\u03c9) \u2265 q(\u03c9) \u2212 2\nthat un(\u03c9) \u2191 q(\u03c9), except for a set of \u03c9\u2019s of probability zero.\n\n\u2212n = q(\u03c9) \u2212 2\n\nproposition 16.16 let xt be a predictable process with paths that are right continuous with\nleft limits. if a \u2208 r and t = inf{t > 0 : xt \u2265 a}, then t is a predictable stopping time.\nproof the set a = {(t, \u03c9) : xt (\u03c9) \u2265 a} is a predictable set. since xt is right continuous,\n[t,\u221e) = a \u222a (t,\u221e) \u2208 p by proposition 16.2, and so [t, t ] = [t,\u221e) \\ (t,\u221e) \u2208 p.\nnow apply proposition 16.15.\n\n "}, {"Page_number": 137, "text": "16.4 projection theorems\n\n119\n\n(16.6)\n\n(16.7)\n\nlet b[0,\u221e) be the borel \u03c3 -field on [0,\u221e), let f\u221e = \u2228t\u22650ft, and let h be the product\n\u03c3 -field\n\n16.4 projection theorems\n\nh = b[0,\u221e) \u00d7 f\u221e.\n\nthe following is the optional projection theorem.\ntheorem 16.17 let x be a bounded process that is h measurable. there exists a unique\noptional process ox such that\n\noxt 1(t <\u221e) = e [xt 1(t <\u221e) | ft ]\n\nfor all stopping times t , including those taking infinite values. if x \u2265 0, then ox \u2265 0.\n\nox is called the optional projection of x . if x is already optional, then by the uniqueness\n\nresult, corollary 16.14, ox = x .\n\nif we take our stopping time t in (16.7) equal to a fixed time t, we have\n\noxt = e [xt | ft],\n\na.s.\n\n(16.8)\n\nthis observation is sometimes useful when x is not an adapted process and one wants a\nversion of e [xt | ft] that is jointly measurable in t and \u03c9.\nif (16.7) holds, then taking expectations shows that\n\ne [oxt; t < \u221e] = e [xt; t < \u221e]\n\n(16.9)\n\n(cid:13)\nfor all stopping times t . conversely, suppose (16.9) holds for all stopping times t . if s is a\nstopping time and a \u2208 fs, let sa be defined by\ns(\u03c9) \u03c9 \u2208 a;\n\u221e \u03c9 /\u2208 a.\n\nsa(\u03c9) =\n\n(16.10)\n\nthen (16.9) with t replaced by sa implies that\n\ne [oxs1(s<\u221e); a] = e [xs1(s<\u221e); a].\n\nsince oxs1(s<\u221e) is fs measurable, this implies (16.7) holds for the stopping time s. conse-\nquently (16.7) holding for all stopping times t is equivalent to (16.9) holding for all stopping\ntimes t .\n\nproof of theorem 16.17 the uniqueness is immediate from corollary 16.14. we look at\nexistence. if xt (\u03c9) = 1f (\u03c9)1[a,b)(t ) where f \u2208 f\u221e, we set oxt equal to e [1f | ft]1[a,b)(t ),\nwhere we use corollary 3.13 to take the right continuous version of the martingale e [1f | ft].\nwe check:\n\ne [oxt; t < \u221e] = e [e [1f | ft ]1[a,b)(t ); t < \u221e]\n\n= e [1f 1[a,b)(t ); t < \u221e]\n= e [xt; t < \u221e]\n\n "}, {"Page_number": 138, "text": "120\n\nthe general theory of processes\n\nsince (t < \u221e) and 1[a,b)(t ) are both ft measurable. we then use linearity and limits to\ndefine ox for bounded measurable x . the positivity of ox when x \u2265 0 is clear from the\nconstruction.\n\nalmost the same proof gives\n\ntheorem 16.18 let x be a bounded measurable process. there exists a unique predictable\nprocess px , called the predictable projection of x , such that\n\ne [pxt; t < \u221e] = e [xt; t < \u221e]\nfor every predictable stopping time t . if x \u2265 0, then px \u2265 0.\nproof uniqueness is as before. if xt = 1f (\u03c9)1(a,b](t ), we let pxt = 1(a,b](t )zt\u2212(\u03c9), where\nzt\u2212 denotes the left-hand limit of zt at time t and zt is the right-continuous version of the\nmartingale e [1f | ft]. we use linearity and limits to define px for bounded measurable x .\nthe positivity of px when x \u2265 0 is clear.\n\n16.5 more on predictability\n\nif u is a random time, i.e., a f\u221e measurable map from \u0001 to [0,\u221e], define\n\nn=1\n\nfu\u2212 = \u03c3{xu : x is bounded and predictable}.\n\nlemma 16.19 suppose t is a predictable stopping time predicted by stopping times tn.\n\nif x is left continuous, adapted, and bounded, then xt = lim xtm and xtm\n\nthen ft\u2212 =(cid:30)\u221e\nftn.\n(cid:30)\nftn, so xt \u2208 (cid:30)\n(cid:30)\n\u2282\nproof\nftn. an argument using the monotone class theorem shows ft\u2212 \u2282\nftn.\non the other hand, suppose a \u2208 ftn for some n. define x = 1(un,\u221e), where un = tn if\n\u03c9 \u2208 a and \u221e otherwise. since tn < t on (t > 0), then xt = 1a. (we leave consideration of\nwhat happens on the event (t = 0) to the reader.) x is predictable since it is left continuous,\nadapted, and bounded, so a is ft\u2212 measurable. therefore ftn\n\u2282 ft\u2212 for all n, and we\nconclude\n\n\u2208 ftm\n\n\u2282 ft\u2212.\n\n(cid:30)\n\nftn\n\nn\n\nn\n\nn\n\nn\n\ncorollary 16.20 suppose t is a predictable stopping time. if m is a uniformly integrable\nmartingale with right-continuous paths, then\n\ne [mt | ft\u2212] = mt\u2212.\n\nif xt = mt\u2212, then x is left continuous, hence predictable, so mt\u2212 = xt is ft\u2212\nproof\nmeasurable by the definition of ft\u2212 and a limit argument. suppose the sequence tn predicts\n\u2282 ftn, and by optional stopping (see exercise 3.12),\nt . if a \u2208 ftm and n > m, then a \u2208 ftm\n\nas n \u2192 \u221e. since ft\u2212 =(cid:30)\n\ne [mt; a] = e [mtn\n\n; a] \u2192 e [mt\u2212; a]\n\nftm, we have e [mt; a] = e [mt\u2212; a] for all a \u2208 ft\u2212. now\n\nm\n\nuse the definition of conditional expectation.\n\ncorollary 16.21 let s be a predictable stopping time, m a square integrable martingale,\nand nt = \u0001ms1(t\u2265s). then nt is a square integrable martingale.\n\n "}, {"Page_number": 139, "text": "121\n16.5 more on predictability\nproof since |nt| \u2264 2 sups\u22650\n|ms|, n is square integrable. we will show n is a mar-\ntingale by showing e nt = 0 for all bounded stopping times t , and then appealing to\nproposition 9.5.\nif t is a bounded stopping time, then (t \u2265 s) \u2208 fs\u2212; to see this, if sm is a sequence of\nstopping times predicting s, then (t \u2265 s) = \u2229m(t \u2265 sm) \u2208 \u2228mfsm. using corollary 16.20,\n\ne nt = e \u0001ms1(t\u2265s) = e [ms; t \u2265 s] \u2212 e [ms\u2212; t \u2265 s] = 0,\n\nand we are done.\n\nwe now show that every stopping time for brownian motion is predictable.\nproposition 16.22 let {ft} be the minimal augmented filtration of a brownian motion. if t\nis a stopping time with respect to {ft}, then t is a predictable stopping time.\nproof let t be a stopping time for brownian motion. let g be a continuous strictly\nincreasing function from [0,\u221e] to [0, 1], e.g., g(s) = (2/\u03c0 ) arctan s. let mt be the right-\ncontinuous modification of the martingale e [g(t ) | ft]. the property of brownian motion\nthat is key here is that every martingale adapted to the filtration of a brownian motion is\ncontinuous; see corollary 12.5. hence mt can be taken to be continuous.\nlet vt = mt \u2212 g(t \u2227 t ). then vt has continuous paths and since g(t \u2227 t ) increases with\n\nt, v is a supermartingale. we have\n\nso v is non-negative. clearly vt = 0. if s is the first time that vt is 0, then s \u2264 t . also,\n\nvt = e [g(t ) \u2212 g(t \u2227 t ) | ft],\n\n0 = e vs = e [g(t ) \u2212 g(t \u2227 s)],\n\nso s \u2265 t.\n\nwe let tn = inf{t : vt = 1/n}. by the continuity of v , it is clear that each tn is strictly less\n\nthan t if t > 0 and the tn increase up to t . hence t is predictable.\n\nnow let us suppose that at is a right-continuous adapted process whose paths are increas-\ning. we call such a process an increasing process. \u0001at denotes the jump of a at time t, that\nis, \u0001at = at \u2212 at\u2212.\nproposition 16.23 suppose at is an increasing process such that\n(1) \u0001at = 0 whenever t is a totally inaccessible stopping time, and\n(2) \u0001at is ft\u2212 measurable whenever t is a predictable stopping time.\nthen a is predictable.\nproof let umi be the ith time |\u0001at| \u2208 (2\n\u2212m+1]. the umi are predictable stopping times\nby exercise 16.5. we decompose each umi as in proposition 16.1. since a does not jump at\ntotally inaccessible times, none of the umi has a totally inaccessible part.\n\n\u2212m, 2\n\nwe do this for each m and i and obtain a countable collection of predictable stopping\ntimes, the union of whose graphs contains all the jump times of a. we order them in some\nway as r1, r2, . . . define t1 = r1, define t2 by setting t2(\u03c9) = r2(\u03c9) if r2(\u03c9) (cid:16)= r1(\u03c9)\nand infinity otherwise. set tn(\u03c9) = rn(\u03c9) if rn(\u03c9) (cid:16)= r1(\u03c9), . . . , rn\u22121(\u03c9) and tn(\u03c9) = \u221e\notherwise. we thus get a sequence of predictable stopping times tn with disjoint graphs and\n\n "}, {"Page_number": 140, "text": "+(cid:12)\n\nthe general theory of processes\n\n122\n\u222an[tn, tn] includes all the jumps of a, except for the set of \u03c9\u2019s of probability zero. the tn\nare predictable stopping times by exercise 16.6.\nsince a jumps only at the predictable stopping times tn, we see that we can write at =\n)1[tn,\u221e), where ac is a continuous increasing process. by hypothesis, \u0001atn\nac\nis ftn\u2212 measurable. therefore the proof will be complete once we show (\u0001atn\nt\n)1[tn,\u221e) is a\npredictable process.\nit therefore suffices to show that the process yt = 1b(\u03c9)1[t,\u221e)(t ) is predictable if t is\na predictable stopping time and b \u2208 ft\u2212. since yt = 1[tb,\u221e)(t ), where tb is equal to t if\n\u03c9 \u2208 b and equal to infinity otherwise, the predictability of y follows by exercise 16.3.\n\n(\u0001atn\n\ni\n\n16.6 dual projection theorems\n\n0\n\n(cid:3) \u221e\n\n1b(t, \u03c9) dat (\u03c9).\n\nin this section at is a right-continuous increasing process with a0 = 0, a.s. we do not\nnecessarily assume that at is adapted, only that a is measurable with respect to h defined by\n(16.6). define \u03bca on elements of h by\n\u03bca(b) = e\n\n(cid:15) \u221e\n0 xt dat if x is bounded and h measurable. note that if x = 0,\nwe define \u03bca(x ) by e\nthen \u03bca(x ) = 0.\ntheorem 16.24 suppose \u03bc is a bounded positive measure on h such that \u03bc(x ) = 0\nwhenever x = 0. then there exists a unique right-continuous increasing process a with\na0 = 0, a.s., such that \u03bc = \u03bca.\nproof first, uniqueness. if \u03bc = \u03bca = \u03bcb, let t > 0 and let c be the set of \u03c9\u2019s where\nat (\u03c9) > bt (\u03c9)+\u03b5. then \u03bca([0, t]\u00d7c) \u2265 \u03bcb([0, t]\u00d7c)+\u03b5p(c), which implies p(c) = 0.\nsince \u03b5 is arbitrary, then at = bt, a.s. since a and b are right continuous, we conclude a = b.\nto prove existence, for each rational q, define \u03bdq(c) = \u03bc([0, q] \u00d7 c). clearly \u03bdq is\n\nabsolutely continuous with respect to p. let(cid:14)aq be the radon\u2013nikodym derivative of \u03bdq with\nrespect to p. since \u03bc is positive,(cid:14)a is increasing in q. let at = lim supq\u2192t,q>t\n(cid:14)aq. it is easy to\n\ncheck that \u03bca = \u03bc.\ntheorem 16.25 suppose a is right continuous, a0 = 0, a.s., and \u03bca(x ) = \u03bca(ox ) for\nevery bounded h measurable process x . then at is optional.\nproof since at is right continuous, we need only show that at is adapted. fix t and let y be\na bounded f\u221e measurable random variable,\n\nz = y \u2212 e [y | ft],\n\nand xs(\u03c9) = 1[0,t](s)z(\u03c9). if t is a stopping time, then (t \u2264 t ) \u2208 ft, and so by the\ndefinitions of x and z,\n\ne [oxt; t < \u221e] = e [xt; t < \u221e] = e [z; t \u2264 t] = 0.\n\nthis implies ox = 0 by the definition of ox . hence\n\n(cid:16)(cid:3) \u221e\n\n(cid:17)\n\ne [atz] = e\n\nxs das\n\n0\n\n= \u03bca(x ) = \u03bca(ox ) = 0.\n\n "}, {"Page_number": 141, "text": "16.6 dual projection theorems\n\n123\n\nthus e [aty ] = e [at e [y | ft] ]. we write\n\ne [aty ] = e [at e [y | ft] ] = e [e [(at e [y | ft]) | ft] ]\n\n= e [e [at | ft]e [y | ft] ] = e [e [(y e [at | ft]) | ft] ]\n= e [y e [at | ft] ].\n\nhence e [aty ] = e [y e [at | ft] ] for all bounded y , or at = e [at | ft], a.s., which says\nthat at is ft measurable.\ntheorem 16.26 if \u03bca(x ) = \u03bca(px ) for all bounded x , then a is predictable and can be\ntaken to be right continuous.\n\nproof by hypothesis, together with exercise 16.8,\n\n\u03bca(ox ) = \u03bca(p(ox )) = \u03bca(px ) = \u03bca(x ).\n\nby theorem 16.25, at is right continuous and optional. we need to show that a does not\njump at totally inaccessible times and that \u0001at is ft\u2212 measurable at predictable times t ;\nwe then use proposition 16.23.\nlet t be a totally inaccessible stopping time and let b = (\u0001at > 0). set tb equal to t on\nb and equal to infinity otherwise. it is easy to check that tb is also totally inaccessible. let\nx = 1[tb,tb]. if u is a predictable stopping time, e [xu; u < \u221e] = p(tb = u < \u221e) = 0.\nby the definition of predictable projection, px = 0. hence\n\ne [\u0001at; \u0001at > 0] = e [\u0001atb] = \u03bca(x ) = \u03bca(px ) = 0.\n\nnow suppose t is a predictable stopping time. let y be a bounded h measurable random\n\nvariable, set\n\nz = y \u2212 e [y | ft\u2212],\n\nand x = z1[t,t ]. let s be any predictable stopping time. then if w = 1[s,s], w =\nlimn\u2192\u221e 1[s,s+(1/n)) is a predictable process by proposition 16.2(4). by the definition of ft\u2212,\nwt is ft\u2212 measurable. this is the same as saying (s = t < \u221e) \u2208 ft\u2212. therefore\n\nthis implies px = 0, and then\n\ne [xs; s < \u221e] = e [z; s = t < \u221e] = 0.\n\n0 = \u03bca(px ) = \u03bca(x ) = e [z\u0001at ].\n\nsimilarly to the proof of theorem 16.25,\n\ne [\u0001aty ] = e [\u0001at e [y | ft\u2212] ]\n= e [e [\u0001at | ft\u2212]e [y | ft\u2212] ]\n= e [y e [\u0001at | ft\u2212] ].\n\nsince this holds for all y , then \u0001at = e [\u0001at | ft\u2212] is ft\u2212 measurable.\nwe now define the dual optional projection and the dual predictable projection of an\nincreasing process. given a right-continuous increasing, not necessarily adapted process at\nwith a0 = 0, a.s., define \u03bco by\n\n\u03bco(x ) = \u03bca(ox )\n\n(16.11)\n\n "}, {"Page_number": 142, "text": "124\n\nthe general theory of processes\n\nfor bounded h measurable x . exercise 16.11 asks you to prove that \u03bco is a measure. clearly\n\u03bco(ox ) = \u03bca(o(ox )) = \u03bca(ox ) = \u03bco(x ). by theorem 16.17, we see that ox \u2265 0 if x \u2265 0,\nhence \u03bco is a positive measure. if x = 0, then ox = 0, so \u03bco(x ) = \u03bca(ox ) = 0. therefore\nby theorems 16.24 and 16.25, \u03bco corresponds to an optional increasing process ao, called\nthe dual optional projection of a.\nthe dual optional projection is used in excursion theory. more commonly used is the dual\npredictable projection, which is defined in a very similar way. define \u03bcp(x ) = \u03bca(px ), and\n\nlet ap be the predictable increasing process associated with \u03bcp. we often denote ap by(cid:14)a and\nproposition 16.27 let at be an adapted increasing process with a0 = 0, a.s. then at \u2212(cid:14)at\nis a martingale.\nproof let s < t, let b \u2208 fs, define\n\u03c9 \u2208 b,\ns,\n\u221e, \u03c9 /\u2208 b,\n\ncall it the compensator of a. the reason for this terminology is the following proposition.\n\n(cid:13)\n\u03c9 \u2208 b,\nt,\n\u221e, \u03c9 /\u2208 b.\n\nt (\u03c9) =\n\n(cid:13)\n\nand\n\ns(\u03c9) =\nlet x = 1(s,t ]. then\n\ne [at \u2212 as; b] = \u03bca(x ) = \u03bca(px ) = \u03bcap (x ) = e [ap\n\nt \u2212 ap\n\ns\n\n; b],\n\nwhich does it.\n\n16.7 the doob\u2013meyer decomposition\n\nproposition 16.28 if m is a predictable uniformly integrable martingale with paths that are\nright continuous with left limits, then m is continuous.\nproof let \u03b5 > 0 and let t = inf{t : |\u0001mt| > \u03b5}. t is a predictable stopping time by\nexercise 16.2. by corollary 16.20, e [mt | ft\u2212] = mt\u2212. by the definition of ft\u2212 and a\nlimit argument, mt is ft\u2212 measurable, and thus e [mt | ft\u2212] = mt . hence mt = mt\u2212\nat all predictable stopping times, and in particular at time t . but \u03b5 is arbitrary, so m has no\njumps.\n\nwe say a process x is of class d if the family {xt : t a stopping time} is uniformly\nintegrable. the doob\u2013meyer decomposition is the following. if zt is a supermartingale,\nthen \u2212zt is a submartingale, and it is a matter only of convenience whether we state the\ndoob\u2013meyer decomposition in terms of submartingales or supermartingales.\n\ntheorem 16.29 suppose zt is a submartingale of class d with paths that are right continuous\nwith left limits and such that z0 = 0, a.s. then zt = mt+at, where mt is a uniformly integrable\nright-continuous martingale with m0 = 0, a.s., and at is a predictable increasing process\nwith a0 = 0, a.s. the decomposition is unique.\n\nthe existence is the hard part. we define a measure \u03bc by \u03bc((s, t ]) = e[zt \u2212 zs] for\nstopping times s \u2264 t , and then let a be the increasing process such that \u03bca(x ) = \u03bc(px ).\nproof we start with uniqueness. if zt = mt + at = nt + bt, then mt \u2212 nt = bt \u2212 at, and so\nmt \u2212 nt is a predictable uniformly integrable martingale. by proposition 16.28, mt \u2212 nt is\n\n "}, {"Page_number": 143, "text": "16.7 the doob\u2013meyer decomposition\n\n125\na continuous martingale. since mt \u2212 nt = bt \u2212 at, then mt \u2212 nt is a continuous martingale\nwhose paths are of bounded variation on each finite time interval, hence mt \u2212 nt = 0 by\ntheorem 9.7. this proves uniqueness.\nwe turn to existence. by the martingale convergence theorem (theorem 3.12), z\u221e =\nlimt\u2192\u221e zt exists, a.s. by fatou\u2019s lemma, e|z\u221e| < \u221e.\nlet i denote the collection of finite unions of subsets of [0,\u221e) \u00d7 \u0001 of the form (s, t ],\nwhere s \u2264 t are stopping times. define \u03bc((s, t ]) = e [zt \u2212zs]. since z is a submartingale,\nthen \u03bc is non-negative. we note that i is an algebra and that \u03bc is finitely additive on i.\nif k = (s1, t1] \u222a \u00b7\u00b7\u00b7 \u222a (sn, tn] with s1 \u2264 t1 \u2264 s2 \u2264 \u00b7\u00b7\u00b7 \u2264 tn, set k = [s1, t1] \u222a \u00b7\u00b7\u00b7 \u222a\n(cid:13)\nif h = (s, t ] and \u03b5 > 0, let\nsn(\u03c9) =\n\n[sn, tn].\n\ns(\u03c9) + (1/n), s(\u03c9) + (1/n) < t (\u03c9),\n\u221e,\n(cid:13)\n\notherwise,\n\nt (\u03c9), s(\u03c9) + (1/n) < t (\u03c9),\n\u221e,\n\notherwise.\n\ntn(\u03c9) =\n\nand\n\nthen [sn, tn] \u2282 (s, t ] and sn \u2193 s, tn \u2193 t . since z is right continuous and of class d, then\n\u03bc(sn, tn] = e [ztn\n\u2212 zsn] \u2192 e [zt \u2212 zs] = \u03bc(h ). thus if n is sufficiently large and we\ntake k = (sn, tn], then k \u2282 h and \u03bc(k ) > \u03bc(h ) \u2212 \u03b5.\nwe now prove that \u03bc is countably additive on i. suppose hn \u2208 i with hn \u2193 \u2205. we need\nto show that \u03bc(hn) \u2193 0.\nlet \u03b5 > 0 and choose kn \u2208 i such that kn \u2282 hn with \u03bc(kn) > \u03bc(hn) \u2212 \u03b5/2n. let\nln = k1 \u2229 \u00b7\u00b7\u00b7 \u2229 kn. then for each n we have \u03bc(hn) \u2264 \u03bc(ln) + \u03b5. since ln \u2282 kn \u2282 hn, we\nhave ln \u2193 \u2205.\nlet dln be the debut of ln. the stopping times dln increase; let r be the limit. let\nfn = fn(\u03c9) = {t : (t, \u03c9) \u2208 ln}. this is a closed subset of [0,\u221e), and dln\n(\u03c9) \u2208 fn \u2282 fm\n(\u03c9) < \u221e. if r(\u03c9) < \u221e, then r(\u03c9) \u2208 fm for each m, which\nwhenever n \u2265 m and dln\ncontradicts \u2229mlm = \u2205. therefore r = \u221e. since z is of class d, then zdln converges almost\nsurely and in l1 to z\u221e. thus \u03bc(ln) \u2264 e [z\u221e \u2212 zdln ] \u2192 0. hence lim sup \u03bc(hn) < \u03b5, and\nsince \u03b5 is arbitrary, \u03bc(hn) \u2192 0.\nthis proves that \u03bc is countably additive on i. by the carath\u00b4eodory extension theorem, \u03bc\ndefine(cid:14)\u03bc(x ) = \u03bc(px ). then(cid:14)\u03bc(px ) = \u03bc(p(px )) = \u03bc(px ) =(cid:14)\u03bc(x ), and so there exists\nmay be extended to a measure on p.\na predictable right-continuous increasing process at such that(cid:14)\u03bc = \u03bca. since\ne a\u221e = \u03bca(1(0,\u221e)) = \u03bc(p1(0,\u221e)) = \u03bc(1(0,\u221e)) = e [z\u221e \u2212 z0] < \u221e,\n\na\u221e is integrable, and since at is an increasing process, the collection of random variables\n{at} is uniformly integrable.\nif s is any stopping time, then by proposition 16.2, (s,\u221e) is a predictable set, hence\np1(s,\u221e) = 1(s,\u221e). we thus have\n\ne [a\u221e \u2212 as] =(cid:14)\u03bc((s,\u221e)) = \u03bc(p1(s,\u221e)) = \u03bc(1(s,\u221e)) = e [z\u221e \u2212 zs].\n\n "}, {"Page_number": 144, "text": "126\n\nthe general theory of processes\n\nletting t > 0 and b \u2208 ft, define s = t if \u03c9 \u2208 b and equal to infinity otherwise. then\n\ne [a\u221e \u2212 at; b] = e [a\u221e \u2212 as] = e [z\u221e \u2212 zs] = e [z\u221e \u2212 zt; b],\n\nor mt = zt \u2212 at is a martingale. proposition a.17 tells us that m is a uniformly integrable\nmartingale.\na process x is of class dl if there exist stopping times vn \u2192 \u221e such that xt\u2227vn is of\nclass d for each n. it is clear that there is a version of the doob\u2013meyer decomposition for\nsubmartingales of class dl.\n\nproposition 16.30 the process a is continuous if and only if e ztn\nand tn < t on (t > 0).\n\n\u2192 e zt whenever tn \u2191 t\n\nproof let t be a predictable stopping time predicted by the sequence tn. since we know\ne [a\u221e \u2212 atn] = e [z\u221e \u2212 ztn], then taking limits,\n\ne [a\u221e \u2212 at\u2212; t < \u221e] = e [z\u221e \u2212 zt\u2212; t < \u221e],\n\nusing the fact that z is of class d. also e [a\u221e \u2212 at ] = e [z\u221e \u2212 zt ]. thus e [at \u2212 at\u2212] =\ne [zt \u2212 zt\u2212]. then e [at \u2212 at\u2212] = 0 if and only if e zt = e zt\u2212.\ncorollary 16.31 let s be a totally inaccessible stopping time, y a non-negative bounded\n\nrandom variable that is fs measurable, and at = y 1(t\u2265s). let(cid:14)a be the compensator of a.\nthen(cid:14)a has continuous paths.\nproof let t be a stopping time and let tn be stopping times increasing to t . if we have\np(t = s) = 0, then limn\u2192\u221e atn\n= at , a.s., since a jumps only at time s. if p(t = s) >\n0, then [t, t ] cannot contain the graph of a predictable stopping time since s is totally\na.s. by proposition 16.30,(cid:14)a is continuous.\ninaccessible. therefore we cannot have tn < t for all n with positive probability, hence\ntn(\u03c9) = t (\u03c9) for all n sufficiently large (depending on \u03c9). thus again limn\u2192\u221e atn\n= at ,\n\n16.8 two inequalities\n\nproposition 16.32 suppose zt = mt \u2212 at, where mt is a uniformly integrable martingale\nand at is an increasing predictable process with a0 = 0, a.s. suppose z is bounded, that is,\nthere exists k > 0 such that p(|zt| > k for some t ) = 0. if p is any positive integer,\n\ne ap\u221e < \u221e.\n\nproof let \u03bb > 0 and let m = 4k. let t = inf{t : at \u2265 \u03bb}. because at\u2212 \u2264 \u03bb,\n(cid:17)\n\np(a\u221e \u2265 \u03bb + m ) = p(a\u221e \u2265 \u03bb + m, t < \u221e)\n\na\u221e \u2212 at\u2212\n\n(cid:16)\ne [a\u221e \u2212 at\u2212; t < \u221e].\n\n\u2264 p(a\u221e \u2212 at\u2212 \u2265 m, t < \u221e)\n\u2264 e\n\u2264 1\nm\n\nm\n\n; a\u221e \u2212 at\u2212 \u2265 m, t < \u221e\n\n "}, {"Page_number": 145, "text": "16.8 two inequalities\n\nwe will show\n\ne [a\u221e \u2212 at\u2212; t < \u221e] \u2264 1\n\np(t < \u221e),\n\n2\n\n1\nm\n\nwhich, since p(t < \u221e) = p(a\u221e \u2265 \u03bb), implies\np(a\u221e \u2265 \u03bb + m ) \u2264 1\n\np(a\u221e \u2265 \u03bb).\n\ntaking \u03bb = km in (16.13) yields\n\np(a\u221e \u2265 (k + 1)m ) \u2264 1\n\np(a\u221e \u2265 km ).\n\n2\n\n2\n\nsince p(a\u221e \u2265 m ) \u2264 1, induction tells us\n\np(a\u221e \u2265 km ) \u2264 1\n2k\u22121\n\n,\n\n127\n\n(16.12)\n\n(16.13)\n\nwhich implies our conclusion.\ntherefore we need to prove (16.12). t is a predictable stopping time by proposition 16.16.\nlet tn be stopping times with tn \u2191 t and tn < t on (t > 0). let n be fixed for the moment\nand let n > 0. if j > n,\n\ne [a\u221e \u2212 atj\n\n; tn < n] = e [e [a\u221e \u2212 atj\n= \u2212e [e [z\u221e \u2212 ztj\n\u2264 2kp(tn < n )\n\n| ftj ]; tn < n]\n| ftj ]; tn < n]\n\nsince zt + at is a martingale, (tn < n ) \u2208 ftn\nj \u2192 \u221e and using fatou\u2019s lemma, we get\n\n\u2282 ftj , and |z| is bounded by k. letting\n\ne [a\u221e \u2212 at\u2212; tn < n] \u2264 2kp(tn < n ).\n\nletting n \u2192 \u221e, by fatou\u2019s lemma again,\n\ne [a\u221e \u2212 at\u2212; t < n] \u2264 2kp(t \u2264 n ).\n\nfinally, letting n \u2192 \u221e, by monotone convergence,\n\ne [a\u221e \u2212 at\u2212; t < \u221e] \u2264 2kp(t < \u221e).\n\nby our choice of m, this gives (16.12).\n\nfor use in the reduction theorem in chapter 17, we will need a variation of the preceding\n\nproposition.\n\nproposition 16.33 let u be a stopping time, y a non-negative integrable random variable\nthat is fu measurable. let nt be the right-continuous version of e [y | ft]. suppose there\nexists k > 0 such that nt \u2264 k if t < u . let zt = y 1(t\u2265u ), which is an increasing process,\nand let at be its compensator. if p is a positive integer, then e ap\u221e < \u221e.\nproof as in the proof of proposition 16.32, it suffices to show\ne [a\u221e \u2212 at\u2212; t < \u221e] \u2264 kp(t < \u221e),\n\n(16.14)\nwhere \u03bb > 0 and t = inf{t : at \u2265 \u03bb}. since a is a predictable process, then t is a predictable\nstopping time by proposition 16.16. let tn be stopping times predicting t .\n\n "}, {"Page_number": 146, "text": "128\n\nthe general theory of processes\n\nlet n, n \u2265 1. if j > n, then (tn < n ) \u2208 ftn\n\n\u2282 ftj and\n; tn < n] = e [z\u221e \u2212 ztj\n\n; tn < n].\n\ne [a\u221e \u2212 atj\n\nwe observe that z\u221e \u2212 ztj\n(tj < u ). therefore\n\n= 0 on the event (tj \u2265 u ), while z\u221e \u2212 ztj\n\n(16.15)\n= y on the event\n\ne [z\u221e \u2212 ztj\n\n; tn < n] = e [y; tj < u, tn < n]\n; tj < u, tn < n]\n\n= e [e [y | ftj ]; tj < u, tn < n]\n= e [ntj\n\u2264 kp(tj < u, tn < n )\n\u2264 kp(tn < n ).\n\nwith this and (16.15), we can now proceed as in the proof of proposition 16.32 to obtain\n(16.14).\n\nexercises\n\n16.1 show that if s1, . . . , sn are predictable stopping times, then so are s1\u2227\u00b7\u00b7\u00b7\u2227 sn and s1\u2228\u00b7\u00b7\u00b7\u2228 sn.\n16.2 if at is a predictable process with paths that are right continuous with left limits and a > 0,\n\nshow t = inf{t > 0 : \u0001at > a} is a predictable stopping time.\n\n16.3 show that if t is a predictable stopping time, b \u2208 ft\u2212, and tb(\u03c9) is defined to be equal to\n\nt (\u03c9) if \u03c9 \u2208 b and equal to \u221e otherwise, then tb is a predictable stopping time.\n\n16.4 let x be a bounded adapted right-continuous process, let \u03b5 > 0, let u0 = 0, a.s., and define ui\n\nby (16.3) for i \u2265 1. show each ui is a stopping time.\n\n16.5 let a be a predictable increasing process and let sk be the kth time a jumps more than \u03b5. thus\ns0 = 0, a.s., and sk+1 = inf{t > sk : \u0001at > \u03b5}. show each sk is a predictable stopping time.\n\n16.6 show that the stopping times tn defined in the proof of proposition 16.23 are predictable.\n16.7 show that if pt is a poisson process, then (pp)t = pt\u2212.\n16.8 show that if x is bounded and measurable with respect to the product \u03c3 -field b[0,\u221e) \u00d7 f\u221e,\n\nthen p(ox ) = px .\n\n16.9 suppose t is a totally inaccessible stopping time. show that if x = 1[t,t ], then px = 0.\n16.10 if p is a poisson process with parameter \u03bb, determine po\n\nt and pp\nt .\n\n16.11 show that \u03bco defined in (16.11) is a measure.\n\n16.12 let xt be a continuous process and suppose there exists k > 0 such that for all t,\n\ne [|x\u221e \u2212 xt||ft] \u2264 k,\n\na.s.\n\nlet x\n\n\u2217\u221e = supt\u22650\n\n|xt|. prove that there exists a depending only on k such that\n\n\u221e < \u221e.\n\u2217\n\ne eax\n\n "}, {"Page_number": 147, "text": "notes\n\n129\n\nthis is sometimes called the john\u2013nirenberg inequality after the inequality of the same name\nin analysis.\n\nhint: imitate the proof of proposition 16.32. this exercise is somewhat easier than the proof\n\nof that proposition because x has continuous paths.\n\n16.13 a martingale m is said to be in the space bmo if\n\ne [m 2\u221e \u2212 m 2\n\nt\n\n| ft] < \u221e,\n\na.s.\n\nsup\nt\u22650\n\n\u2217\nlet m\nt\n\n= sups\u2264t\n\n|ms|. show that if m is in bmo, then there exists a > 0 such that\n\n\u221e < \u221e.\n\u2217\n\ne eam\n\nthe name bmo comes from the \u201cbounded mean oscillation\u201d spaces of harmonic analysis.\n\nhint: use exercise 16.12.\n\nnotes\n\na progressively measurable set is one whose indicator is a progressively measurable process,\nwhich is defined in exercise 1.3. in fact, the debut of a progressively measurable set is a\nstopping time; see dellacherie and meyer (1978).\n\nan elementary proof of the general doob\u2013meyer theorem along the lines of the proof\n\ngiven in chapter 9 can be found in bass (1996).\n\nsee dellacherie and meyer (1978) for more on the general theory of processes.\n\n "}, {"Page_number": 148, "text": "17\n\nprocesses with jumps\n\nin this chapter we investigate the stochastic calculus for processes which may have jumps\nas well as a continuous component. if x is not a continuous process, it is no longer true\nthat xt\u2227tn is a bounded process when tn = inf{t : |xt| \u2265 n}, since there could be a large\njump at time tn . we investigate stochastic integrals with respect to square integrable (not\nnecessarily continuous) martingales, it\u02c6o\u2019s formula, and the girsanov transformation. we\nprove the reduction theorem that allows us to look at semimartingales that are not necessarily\nbounded.\n\nsince i encouraged you to skim chapter 16 on the first reading of this book, it is only fair\nthat i tell you the facts that we will need from that chapter. we will need the doob\u2013meyer\ndecomposition (theorem 16.29), proposition 16.1, corollaries 16.21 and 16.31, and the two\ninequalities in propositions 16.32 and 16.33.\n\n17.1 decomposition of martingales\n\nwe assume throughout this chapter that {f t} is a filtration satisfying the usual conditions.\nthis means that each ft contains every p-null set and \u2229\u03b5>0ft+\u03b5 = ft for each t.\nlet us begin by recalling a few definitions and facts. the predictable \u03c3 -field is the \u03c3 -field\nof subsets of [0,\u221e) \u00d7 \u0001 generated by the collection of bounded, left-continuous processes\nthat are adapted to {ft}; see section 10.1. a stopping time t is predictable and predicted\nby the sequence of stopping times tn if tn \u2191 t , and tn < t on the event (t > 0). a\nstopping time t is totally inaccessible if p(t = s) = 0 for every predictable stopping time\ns. the graph of a stopping time t is [t, t ] = {(t, \u03c9) : t = t (\u03c9) < \u221e}; see section 16.1.\nif xt is a process that is right continuous with left limits, we set xt\u2212 = lims\u2192t,s<t xs and\n\u0001xt = xt \u2212 xt\u2212. thus \u0001xt is the size of the jump of xt at time t.\nsuppose at is a bounded increasing process whose paths are right continuous with left\nlimits. recall that a function f is increasing if s < t implies f (s) \u2264 f (t ). then trivially at\nis a submartingale, and by the doob\u2013meyer decomposition, theorem 16.28, there exists a\n\npredictable increasing process(cid:14)at such that at\u2212(cid:14)at is a martingale. we call(cid:14)at the compensator\nlinearity to define(cid:14)at as(cid:14)bt \u2212(cid:14)ct. we can even extend the notion of compensator to the case\n\nif at = bt \u2212 ct is the difference of two increasing processes bt and ct, then we can use\n\nof at.\n\nwhere at is complex valued and has paths that are locally of bounded variation by looking at\nthe real and imaginary parts.\n\n130\n\n "}, {"Page_number": 149, "text": "17.1 decomposition of martingales\n\n131\n\nwe will use the following lemma.\n\nlemma 17.1 if at = bt \u2212 ct, where bt and ct are increasing right-continuous processes\nwith b0 = c0 = 0, a.s., and in addition b and c are bounded, then\n\nproof by proposition 16.32, e(cid:14)b2\u221e < \u221e and e(cid:14)c2\u221e < \u221e, and so\n\ne sup\nt\u22650\n\n(cid:14)a2\n\nt\n\n(cid:14)b2\n\nt\n\n\u2264 e [2 sup\nt\u22650\n\n+ 2 sup\nt\u22650\n\nt\n\n< \u221e.\n\n(cid:14)a2\n(cid:14)c2\nt ] \u2264 2e(cid:14)b2\u221e + 2e(cid:14)c2\u221e < \u221e.\n\ne sup\nt\u22650\n\nwe are done.\n\na key result is the following orthogonality lemma.\n\na.s.,(cid:14)at is the compensator of a, and mt = at \u2212(cid:14)at. suppose nt is a right continuous square\n\nlemma 17.2 suppose at is a bounded increasing right-continuous process with a0 = 0,\nintegrable martingale such that (\u0001nt )(\u0001mt ) = 0 for all t. then e m\u221en\u221e = 0.\nproof by lemma 17.1, m is square integrable. suppose\nh (s, \u03c9) = k (\u03c9)1(a,b](s)\n\nwith k being fa measurable. since mt is of bounded variation, we have (this is a lebesgue\u2013\nstieltjes integral here)\n\n0\n\ne\n\nhs dms = e [k (mb \u2212 ma)] = e [ke [mb \u2212 ma | fa] ] = 0.\n\n(cid:15) \u221e\n(cid:15) \u221e\nwe saw in lemma 10.1 that linear combinations of such h \u2019s generate the predictable \u03c3 -field.\n(cid:15) \u221e\n0 hs dms = 0 if hs is a predictable process such\nthus by linearity and taking limits, e\n|hs||dms| < \u221e. in particular, since ns\u2212 is left continuous and hence predictable,\n(cid:3) \u221e\nthat e\n(cid:20)(cid:20)(cid:20) \u2264 e\n0 ns\u2212 dms = 0, provided we check integrability:\ne\n\ne\n\n|ns\u2212||dms|\n\n0\n\n(cid:3) \u221e\n\n0\n\n= e [(sup\n\nr\n\n(sup\n\n|nr|)|dms|\n\n|nr|) (a\u221e +(cid:14)a\u221e)] < \u221e\n(cid:15) \u221e\n0 ns dms = 0. on the other hand, using\n\nr\n\n(cid:20)(cid:20)(cid:20)(cid:3) \u221e\n(cid:15) \u221e\n\n0\n\nby the cauchy\u2013schwarz inequality.\n\nby hypothesis, e\n\n0\nproposition 3.14, we see\n\n\u0001ns dms = 0, so e\n\n(cid:3) \u221e\n\n(cid:3) \u221e\n\ne m\u221en\u221e = e\n\nn\u221e dms = e\n\nns dms = 0.\n\nthe proof is complete.\n\n0\n\n0\n\n "}, {"Page_number": 150, "text": "132\n\nprocesses with jumps\n\nif we apply the above to nt\u2227t , we have e m\u221ent = 0. if we then condition on ft ,\n\ne [mt nt ] = e [nt e [m\u221e | ft ] ] = e [nt m\u221e] = 0.\n\n(17.1)\n\nthe reason for the name \u201corthogonality lemma\u201d is that by (17.1) and proposition 9.5,\nt (which we will define soon, and is defined\n\nmtnt is a martingale. this implies that (cid:22)m, n(cid:23)\nsimilarly to the case of continuous martingales) is identically equal to 0.\nlet mt be a square integrable martingale with paths that are right continuous and left\nlimits, so that e m 2\u221e < \u221e. for each i \u2208 z, let ti1 = inf{t : |\u0001mt| \u2208 [2i, 2i+1)}, ti2 =\ninf{t > ti1 : |\u0001mt| \u2208 [2i, 2i+1)}, and so on; i can be both positive and negative. since mt\nis right continuous with left limits, for each i, ti j \u2192 \u221e as j \u2192 \u221e. we conclude that mt\nhas at most countably many jumps. next we decompose each ti j into predictable and totally\ninaccessible parts by proposition 16.1. we relabel the jump times as s1, s2, . . . so that each\nsk is either predictable or totally inaccessible, the graphs of the sk are disjoint, m has a jump\nat each time sk and only at these times, and |\u0001msk\n| is bounded for each k; of the proof of\n\u2264 sk2 if k1 \u2264 k2, and in general it would not be\nproposition 16.23. we do not assume that sk1\npossible to arrange this.\n\nif si is a totally inaccessible stopping time, let\n\n(17.2)\n\nai(t ) = \u0001msi1(t\u2265si )\nmi(t ) = ai(t ) \u2212(cid:14)ai(t ),\n\nand\n\nwhere(cid:14)ai is the compensator of ai. ai(t ) is the process that is 0 up to time si and then jumps\nan amount \u0001msi; thereafter it is constant. by corollary 16.31,(cid:14)a is continuous. if si is a\n\n(17.3)\n\npredictable stopping time, let\n\n(17.4)\nby corollary 16.21, mi is a martingale. note that in either case, m \u2212 mi has no jump at\ntime si.\n\nmi(t ) = \u0001msi1(t\u2265si ).\n\ntheorem 17.3 suppose m is a square integrable martingale and we define mi as in (17.3)\nand (17.4).\n\n(cid:12)\u221e\n(1) each mi is square integrable.\ni=1 mi(\u221e) converges in l2.\n(2)\n(3) if m c\nt\n\n= mt \u2212(cid:12)\u221e\n\nhas continuous paths.\n\ni=1 mi(t ), then m c is square integrable and we can find a version that\n\n(4) for each i and each stopping time t , e [m c\n\nt mi(t )] = 0.\n\n(1) if si is a totally inaccessible stopping time and we let bt = (\u0001msi\n\n(2) let vn(t ) = (cid:12)\n\n1(t\u2265si ) and\n1(t\u2265si ), then (1) follows by lemma 17.1. if si is predictable, (1) follows by\n\nproof\nct = (\u0001msi\n)\u2212\ncorollary 16.21.\nn\ni=1 mi(t ). by the orthogonality lemma (lemma 17.2),\ne [mi(\u221e)m j (\u221e)] = 0 if i (cid:16)= j and e [mi(\u221e)(m\u221e \u2212 vn(\u221e)] = 0 if i \u2264 n. we thus\n\n)+\n\n "}, {"Page_number": 151, "text": "17.2 stochastic integrals\n\n133\n\nhave\n\nn(cid:9)\n\ni=1\n\ntherefore the series e\n\ne mi(\u221e)2 = e vn(\u221e)2\n\n(cid:16)\n(cid:16)\n\u2264 e\nm\u221e \u2212 vn(\u221e)\n= e\nm\u221e \u2212 vn(\u221e) + vn(\u221e)\n= e m 2\u221e < \u221e.\n(cid:16)\nn(cid:9)\n\n(cid:17)2 + e vn(\u221e)2\n(cid:17)2\n(cid:17)2 = n(cid:9)\n\n(cid:12)\ni=1 mi(\u221e)2 converges. if n > m,\nmi(\u221e)\n\nn\n\ni=m+1\n\ni=m+1\n\ne [(vn(\u221e) \u2212 vm(\u221e)]2 = e\n\ne mi(\u221e)2.\n\nthis tends to 0 as n, m \u2192 \u221e, so vn(\u221e) is a cauchy sequence in l2, and hence converges.\n(3) from (2), doob\u2019s inequalities, and the completeness of l2, the random variables\nsupt\u22650[mt \u2212 vn(t )] converge in l2 as n \u2192 \u221e. let m c\n= limn\u2192\u221e[mt \u2212 vn(t )]. there is a\nsequence nk such that\n| \u2192 0,\n\n(t )) \u2212 m c\n\n|(mt \u2212 vnk\n\na.s.\n\nt\n\nt\n\nsup\nt\u22650\n\nwe conclude that the paths of m c\nt are right continuous with left limits. by the construction\nof the mi, m \u2212 vnk has jumps only at times si for i > nk. we therefore see that m c has no\njumps, i.e., it is continuous.\n\n(4) by the orthogonality lemma and (17.1),\n\nif t is a stopping time and i \u2264 n. letting n tend to infinity proves (4).\n\ne [mi(t )(mt \u2212 vn(t )] = 0\n\n17.2 stochastic integrals\n\nif mt is a square integrable martingale, then m 2\nis a submartingale by jensen\u2019s inequality\nt\nfor conditional expectations. just as in the case of continuous martingales, we can use the\ndoob\u2013meyer decomposition (this time, we use theorem 16.29 instead of theorem 9.12) to\nfind a predictable increasing process starting at 0, denoted (cid:22)m(cid:23)\nt is a\nmartingale.\n\nt, such that m 2\nt\n\n\u2212 (cid:22)m(cid:23)\n\nlet us define\n\n(17.5)\n\nhere m c is the continuous part of the martingale m as defined in theorem 17.3. as an\n= 0 and\nexample, if mt = pt \u2212 t, where pt is a poisson process with parameter 1, then m c\n\nt\n\n(cid:9)\n\n+\n\nt\n\n[m]t = (cid:22)m c(cid:23)\n(cid:9)\n\ns\u2264t\n\n|\u0001ms|2.\n(cid:9)\n\n[m]t =\n\n=\n\n\u0001p2\ns\n\n\u0001ps = pt ,\n\ns\u2264t\n\ns\u2264t\n\nbecause all the jumps of pt are of size one. in this case (cid:22)m(cid:23)\n17.4 below.\n\nt\n\n= t; this follows from proposition\n\n "}, {"Page_number": 152, "text": "t, but the process [m]t is the one\n\n134\n\nprocesses with jumps\n\nin defining stochastic integrals, one could work with (cid:22)m(cid:23)\n\nthat shows up naturally in many formulas, such as the product formula.\n\nproposition 17.4 m 2\nt\n\n\u2212 [m]t is a martingale.\n\nproof by the orthogonality lemma and (17.1) it is easy to see that\n\n(cid:9)\n(cid:11)\n\ni\n\n.\n\nt\n\n(cid:22)mi(cid:23)\n(cid:10)\n\n(cid:22)m c(cid:23)\n(cid:9)\n(t ) \u2212(cid:12)\n\ns\u2264t\n\n(cid:22)m(cid:23)\n\nt\n\n+\n\nt\n\n= (cid:22)m c(cid:23)\n(cid:9)\n(cid:12)\n\ns\u2264t\n\nt is a martingale, we need only show [m]t \u2212 (cid:22)m(cid:23)\n[m]t \u2212 (cid:22)m(cid:23)\n+\n(cid:12)\n\n|\u0001ms|2\n|\u0001mi(s)|2 is a martingale.\n\n(cid:22)mi(cid:23)\n\n=\n\n+\n\n\u2212\n\nt\n\nt\n\ni\n\ns\u2264t\n\ni\n\nt is a martingale. since\n\n(cid:9)\n\n(cid:11)\n\n(cid:22)mi(cid:23)\n\nt\n\nsince m 2\nt\n\n\u2212 (cid:22)m(cid:23)\n\nit suffices to show that\n\nby exercise 17.1\n\nmi(t )2 = 2\n\nt\n\nmi(s\u2212) dmi(s) +\n\n|\u0001mi(s)|2,\n\n,\n\n(17.6)\n\n(cid:10)\n\nt\n\n(cid:22)m c(cid:23)\n\u2212(cid:12)\n(cid:3)\n\nt\n\ni\n\n0\n\n(17.7)\n\nwhere each ki is bounded and fai measurable, define the stochastic integral by\n\nwhere the first term on the right-hand side is a lebesgue\u2013stieltjes integral. if we approximate\nthis integral by a riemann sum and use the fact that mi is a martingale, we see that the first\n|\u0001mi(s)|2 is a martingale.\nterm on the right in (17.6) is a martingale. thus m 2\ni\nsince m 2\ni\n\nt is a martingale, summing over i completes the proof.\n\ns\u2264t\n\n(t ) \u2212 (cid:22)mi(cid:23)\nif hs is of the form\n\ni=1\n\nki(\u03c9)1(ai,bi](s),\n\nhs(\u03c9) = n(cid:9)\n(cid:3)\nhs dms = n(cid:9)\n(cid:15) \u221e\n\u2212 [n]t is a martingale.\n(cid:3) \u221e\ns d[m]s < \u221e, approximate h by integrands h n\n0 h 2\n\nki[mbi\u2227t \u2212 mai\u2227t].\n\ni=1\n\n0\n\nt\n\nnt =\n\nvery similar proofs to those in chapter 10 show that the left-hand side will be a martingale\nand (with [\u00b7] instead of (cid:22)\u00b7(cid:23)), n 2\nif h is p measurable and e\n\nt\n\ns of the\n\nform (17.7) so that\n\n(hs \u2212 h n\n\n)2 d[m]s \u2192 0\n\ns\n\ne\n\n0\n\nt as the stochastic integral of h n with respect to mt. by almost the same proof\nand define n n\nt\nas that of theorem 10.4, the martingales n n\n0 hs dms\nthe stochastic integral of h with respect to m. a subsequence of the n n converges uniformly\nover t \u2265 0, a.s., and therefore the limit has paths that are right continuous with left limits.\nthe same arguments as those of theorem 10.4 apply to prove that the stochastic integral is\na martingale and\n\nt converge in l2. we call the limit nt =(cid:15)\n(cid:3)\n\n[n]t =\n\nt\n\n0\n\nh 2\n\ns d[m]s.\n\n "}, {"Page_number": 153, "text": "a consequence of this last equation is that\n\n(cid:10)(cid:3)\n\n17.3 it\u02c6o\u2019s formula\n\n(cid:11)2 = e\n\n(cid:3)\n\n0\n\ne\n\nt\n\n0\n\nhs dms\n\nt\n\nh 2\n\ns d[m]s.\n\n135\n\n(17.8)\n\n17.3 it\u02c6o\u2019s formula\n\nwe will first prove it\u02c6o\u2019s formula for a special case, namely, we suppose xt = mt + at, where\nmt is a square integrable martingale and at is a process of bounded variation whose total\nvariation is integrable. the extension to semimartingales without the integrability conditions\nwill be done later in the chapter (in section 17.5) and is easy. define (cid:22)x c(cid:23)\ntheorem 17.5 suppose xt = mt + at, where mt is a square integrable martingale and at is\na process with paths of bounded variation whose total variation is integrable. suppose f is\nc2 on r with bounded first and second derivatives. then\n(cid:3)(xs\u2212) dxs + 1\n[ f (xs) \u2212 f (xs\u2212) \u2212 f\n\n(cid:9)\nf (xt ) = f (x0) +\n\nt to be (cid:22)m c(cid:23)\nt.\n\n(cid:3)(cid:3)(xs\u2212) d(cid:22)x c(cid:23)\n\n(17.9)\n\n(cid:3)\n\n(cid:3)\n\n+\n\nf\n\nf\n\n0\n\n2\n\n0\n\ns\n\nt\n\nt\n\n(cid:3)(xs\u2212)\u0001xs].\n(cid:3)\n\ns\u2264t\n\n(cid:3)\n\nproof the proof will be given in several steps. set\n\ns(t ) =\n\nand\n\nt\n\nf\n\n0\n\nq(t ) = 1\n\n(cid:3)(xs\u2212) dxs,\n(cid:9)\n[ f (xs) \u2212 f (xs\u2212) \u2212 f\n\n2\n\nj (t ) =\n\ns\u2264t\n\n(cid:3)(cid:3)(xs\u2212) d(cid:22)x c(cid:23)\n\n,\n\ns\n\nt\n\nf\n\n0\n\n(cid:3)(xs\u2212)\u0001xs].\n\nwe use these letters as mnemonics for \u201cstochastic integral term,\u201d \u201cquadratic variation term,\u201d\nand \u201cjump term,\u201d respectively.\nstep 1. suppose xt has a single jump at time t which is either a predictable stopping time\nor a totally inaccessible stopping time and there exists n > 0 such that |\u0001mt|+|\u0001at| \u2264 n\na.s.\n\na predictable stopping time, replace mt by mt \u2212 \u0001mt 1(t\u2265t ) and at by at + \u0001mt 1(t\u2265t ), and\nagain we may assume m is continuous.\n\nif t is totally inaccessible, let ct = \u0001mt 1(t\u2265t ) and let(cid:14)ct be the compensator. if we replace\nmt by mt \u2212 ct +(cid:14)ct and at by at + ct \u2212(cid:2)ct, we may assume that mt is continuous. if t is\nlet bt = \u0001xt 1(t\u2265t ). set (cid:2)xt = xt \u2212 bt and(cid:2)at = at \u2212 bt. then (cid:2)xt = mt +(cid:2)at and (cid:2)xt is\na continuous process that agrees with xt up to but not including time t . we have(cid:2)xs\u2212 = (cid:2)xs\nand \u0001(cid:2)xs = 0 if s \u2264 t . by theorem 11.1\n(cid:3)\n(cid:3)\n(cid:3)(cid:3)((cid:2)xs) d(cid:22)m(cid:23)\n(cid:3)((cid:2)xs) d(cid:2)xs + 1\n(cid:3)\n(cid:3)\n(cid:3)((cid:2)xs\u2212) d(cid:2)xs + 1\n(cid:3)(cid:3)((cid:2)xs\u2212) d(cid:22)(cid:14)x c(cid:23)\n(cid:3)((cid:2)xs\u2212)\u0001(cid:2)xs],\n[ f ((cid:2)xs) \u2212 f ((cid:2)xs\u2212) \u2212 f\n\nf ((cid:2)xt ) = f ((cid:2)x0) +\n= f ((cid:2)x0) +\n(cid:9)\n\n+\n\nf\n\nf\n\nf\n\nf\n\n0\n\n2\n\n0\n\n0\n\n2\n\n0\n\ns\n\ns\n\nt\n\nt\n\nt\n\nt\n\ns\u2264t\n\n "}, {"Page_number": 154, "text": "136\n\nprocesses with jumps\n\nsince the sum on the last line is zero. for t < t ,(cid:2)xt agrees with xt. at time t , f (xt ) has a\njump of size f (xt ) \u2212 f (xt\u2212). the integral with respect to(cid:2)x , s(t ), will jump f\n\n(cid:3)(xt\u2212)\u0001xt ,\nq(t ) does not jump at all, and j (t ) jumps f (xt ) \u2212 f (xt\u2212) \u2212 f\n(cid:3)(xt\u2212)\u0001xt . therefore both\nsides of (17.9) jump the same amount at time t , and hence in this case we have (17.9)\nholding for t \u2264 t .\nstep 2. suppose there exist times t1 < t2 < \u00b7\u00b7\u00b7 with tn \u2192 \u221e, each ti is either a totally\ninaccessible stopping time or a predictable stopping time, for each i, there exists ni > 0 such\nthat |\u0001mti\n| are bounded by ni, and xt is continuous except at the times t1, t2, . . .\nlet t0 = 0.\n(cid:3)\n\n(cid:3)\nt similarly, and apply step 1 to\n\nfix i for the moment. define x\nat time ti + t. we have for ti \u2264 t \u2264 ti+1\nt\n\n| and |\u0001ati\n\nx\n\n(cid:3)\n\nt\n\nt\n\nf\n\nti\n\n(cid:3)\n\n) +\n\nf (xt ) = f (xti\n+\n\n= x(t\u2212ti )+, define a\n(cid:3)\n(cid:3)\nt and m\n(cid:9)\n(cid:3)(xs\u2212) dxs + 1\n[ f (xs) \u2212 f (xs\u2212) \u2212 f\n(cid:3)\n(cid:9)\nf (xti+1\u2227t ) = f (xti\u2227t ) +\n\n(cid:3)(xs\u2212) dxs + 1\n[ f (xs) \u2212 f (xs\u2212) \u2212 f\n\nti+1\u2227t\nti\u2227t\n\nti<s\u2264t\n\n+\n\nti\n\nf\n\nf\n\n2\n\n2\n\n(cid:3)(xs\u2212)\u0001xs].\n(cid:3)\n\nf\n\nti+1\u2227t\nti\u2227t\n(cid:3)(xs\u2212)\u0001xs].\n\n(cid:3)(cid:3)(xs\u2212) d(cid:22)x c(cid:23)\n\ns\n\nti\u2227t<s\u2264ti+1\u2227t\n\n(cid:3)(cid:3)(xs\u2212) d(cid:22)x c(cid:23)\n\ns\n\nthus for any t we have\n\nsumming over i, we have( 17.9) for each t.\n\nac\nt\n\ni=1\n\n\u0001asi1(t\u2265si ).\n\n|+|\u0001asi\n\nsince at is of bounded variation, then ac will be finite and continuous. define\n\nstep 3. we now do the general case. as in the paragraphs preceding theorem 17.3, we can\nfind stopping times s1, s2, . . . such that each jump of x occurs at one of the times si and so\nthat for each i, there exists ni > 0 such that |\u0001msi\n| \u2264 ni. moreover each si is either\na predictable stopping time or a totally inaccessible stopping time. let m be decomposed\n\u221e(cid:9)\ninto m c and mi as in theorem 17.3 and let\n= at \u2212\n+ n(cid:9)\n+ n(cid:9)\n=(cid:12)\n\n+ an\nt . we already know that m n converges uniformly over t \u2265 0 to m in\n1(t\u2265si ) and let bt = supn bn\nn\n(\u0001asi\ni=1\nt ,\nt , then the fact that a has paths of bounded variation implies that with probability\n\n= m n\nand let x n\nt\nl2. if we let bn\nct = supn cn\nt\n\n=(cid:12)\n\n1(t\u2265si ) and cn\nt\n\n\u0001asi1(t\u2265si ),\n\n= m c\n\n= ac\n\n(\u0001asi\n\nmi(t )\n\nm n\nt\n\nand\n\nn\ni=1\n\n)\u2212\n\n)+\n\ni=1\n\ni=1\n\nan\nt\n\nt\n\nt\n\nt\n\n "}, {"Page_number": 155, "text": "\u2192 bt and cn\n\none, bn\nt\nconvergence in total variation norm:\n\nt\n\n17.3 it\u02c6o\u2019s formula\n\n137\n\u2192 ct uniformly over t \u2265 0 and at = bt \u2212 ct. in particular, we have\n\n(cid:3) \u221e\n\ne\n\n0\n\n|d (an\n\nt\n\n) \u2212 at )| \u2192 0.\n\nwe define sn(t ), qn(t ), and j n(t ) analogously to s(t ), q(t ), and j (t ), respectively. by\n\napplying step 2 to x n, we have\n\n) = f (x n\n\n0\n\nf (x n\nt\n\n) + sn(t ) + qn(t ) + j n(t ),\n\nand we need to show convergence of each term. we now examine the various terms.\n\n) \u2192 f (xt ) and f (x0) \u2192\n\nuniformly in t, x n\n\nt\n\nt converges to xt in probability, that is,\n\u2212 xt| > \u03b5) \u2192 0\n(cid:3)\n< \u221e, by dominated convergence\n\u2192\n\n|x n\n(cid:15)\np(sup\nt\u22650\n0 d(cid:22)m c(cid:23)\ns\u2212) d(cid:22)m c(cid:23)\n\n(cid:3)\n\nf\n\ns\n\ns\n\nt\n\nt\n\nas n \u2192 \u221e for each \u03b5 > 0. since\n(cid:3)(cid:3)(x n\n\nf\n\nt\n\ns\n\n0\n\n0\n\n(cid:3)(cid:3)(xs\u2212) d(cid:22)m c(cid:23)\nin probability. therefore qn(t ) \u2192 q(t ) in probability. also, f (x n\n(cid:3)\nf (x0), both in probability.\nwe now show sn(t ) \u2192 s(t ). write\n\u2212\n(cid:16)(cid:3)\n\n(cid:3)(xs\u2212) das\n\u2212\n(cid:3)(x n\n\ns\u2212) dan\ns\n=\n\n(cid:16)(cid:3)\n\n(cid:3)(x n\n\n(cid:3)(x n\n\n(cid:3)\n\n(cid:3)\n\n(cid:3)\n\nf\n\nf\n\nf\n\n0\n\n0\n\n0\n\nt\n\nt\n\nt\n\nt\n\nt\n\n(cid:17)\n\n(cid:17)\n\ns\u2212) dan\ns\n(cid:3)(x n\n\nf\ns\u2212) das \u2212\n\nf\n\n0\n\nt\n\ns\u2212) das\n(cid:3)(xs\u2212) das\n\nf\n\nt\n\n0\n\n= i n\n\n1\n\nwe see that\n\n|dan\nas n \u2192 \u221e, while by dominated convergence, |i n\n\n(cid:3)(cid:21)\u221e\n\n|i n\n\n1\n\n0\n\nt\n\n2\n\nwe next look at the stochastic integral part of sn(t ).\n(cid:3)(xs\u2212) dms\n\u2212\n\ns\u2212) dm n\ns\n=\n\n(cid:3)(x n\n\nf\n\nf\n\n0\n\nt\n\nt\n\n(cid:3)(x n\n\ns\n\n\u2212 das| \u2192 0\n| also tends to 0.\n(cid:3)\n\n(cid:3)\n\n(cid:17)\n\ns\u2212) dm n\ns\n(cid:3)(xs\u2212) dm n\n\nf\n\ns\n\n0\n\nt\n\nt\n\nf\n\u2212\n\n(cid:3)\n(cid:3)(xs\u2212) dm n\n\ns\n\n(cid:3)(xs\u2212) dms\n\nt\n\nf\n\n0\n\n(cid:17)\n\n+\n+ i n\n\n2\n\n0\n.\n\n(cid:3)\n\n| \u2264 (cid:21) f\n(cid:3)\n(cid:16)(cid:3)\n\n(cid:16)(cid:3)\n\n\u2212\n\nf\n\n0\n\n0\n\nt\n\n+\n+ i n\n\n4\n\n0\n.\n\n= i n\n\n3\n\n "}, {"Page_number": 156, "text": "138\n\nprocesses with jumps\n\nthe l2 norm of i n\n3 is bounded by\n| f\n\ns\u2212) \u2212 f\n\n(cid:3)(x n\n\ne\n\nt\n\n(cid:3)\n\n0\n\n(cid:3)\n\n(cid:3)(xs\u2212)|2 d[m n]s \u2264 e\n\u221e(cid:9)\n\n(cid:3)\n\nt\n\n0\n\n=\n\ni n\n4\n\n(cid:3)(xs\u2212)\n\nf\n\n0\n\ndmi(s),\n\ni=n+1\n\nwhich goes to zero by dominated convergence. also\n\nt\n\n| f\n\n(cid:3)(x n\n\ns\u2212) \u2212 f\n\n(cid:3)(xs\u2212)|2 d[m]s,\n\nso using the orthogonality lemma (lemma 17.2), the l2 norm of i n\n\n4 is less than\n\n\u221e(cid:9)\n\ni=n+1\n\n\u221e(cid:9)\n\ni=n+1\n\n(cid:21) f\n\n(cid:3)(cid:21)2\u221e\n\ne [mi]\u221e \u2264 (cid:21) f\n\n(cid:3)(cid:21)2\u221e\n\ne mi(\u221e)2,\n\n[ f (x n\ns\n\ns\u2264t\n\u2212\n\n(cid:3)(xs\u2212)\u0001xs]\n\nj (t ) \u2212 j n(t ) =\n\nwhich goes to zero as n \u2192 \u221e.\nfinally, we look at the convergence of j n. the idea here is to break both j (t ) and j n(t )\ninto two parts, the jumps that might be relatively large (jumps at times si for i \u2264 n where n\n(cid:9)\nwill be chosen appropriately) and the remaining jumps. let n > 1 be chosen later.\n(cid:9)\n[ f (xs) \u2212 f (xs\u2212) \u2212 f\n(cid:9)\n(cid:9)\n(cid:9)\n(cid:9)\n(cid:9)\n\ns\u2212) \u2212 f\n) \u2212 f (x n\n) \u2212 f (xsi\u2212) \u2212 f\n) \u2212 f (x n\nsi\u2212) \u2212 f\n) \u2212 f (xsi\u2212) \u2212 f\n) \u2212 f (x n\nsi\u2212) \u2212 f\n) \u2212 f (xsi\u2212) \u2212 f\n\nsi\u2212)\u0001x n\nsi ]\n(cid:3)(xsi\u2212)\u0001xsi]\n(cid:3)(x n\n\ns\u2212)\u0001x n\ns ]\n(cid:3)(xsi\u2212)\u0001xsi]\n(cid:3)(x n\n\nsi\u2212)\u0001x n\nsi ]\n(cid:3)(xsi\u2212)\u0001xsi]\n\n{i>n:si\u2264t}\n\u2212\n\n{i:si\u2264t}\n\u2212\n\n{i>n:si\u2264t}\n\n[ f (x n\nsi\n\n[ f (x n\nsi\n\n(cid:3)(x n\n\n[ f (xsi\n\n[ f (xsi\n\n{i:si\u2264t}\n\n(cid:18)\n\n=\n\n=\n\n+\n\ns\u2264t\n\n{i\u2264n,si\u2264t}\n\n= i n\n\n5\n\n\u2212 i n,n\n\n6\n\n[ f (xsi\n\u2212 [ f (x n\n+ i n,n\n\nsi\n\n.\n\n7\n\n(cid:19)\n\n) \u2212 f (x n\n\nsi\u2212) \u2212 f\n\n(cid:3)(x n\n\nsi\u2212)\u0001x n\nsi ]\n\nby the fact that m and a are right continuous with left limits, |\u0001msi\n1/2 if i is large enough (depending on \u03c9), and then |\u0001xsi\n| \u2264 1, and also\n|2 + 2|\u0001asi\n|2\n|.\n|2 + |\u0001asi\n\n|2 \u2264 2|\u0001msi\n\u2264 2|\u0001msi\n\n|\u0001xsi\n\n| \u2264 1/2 and |\u0001asi\n\n| \u2264\n\n "}, {"Page_number": 157, "text": "17.4 the reduction theorem\n\n139\n\n(cid:9)\n(cid:9)\n\ni>n,si\u2264t\n\nwe have\n\n|i n\n\n5\n\n| \u2264 (cid:21) f\n\n(cid:3)(cid:3)(cid:21)\u221e\n\n(\u0001xsi\n\n)2\n\nand\n\n(cid:12)\u221e\n\n|\u0001msi\nsince\nn large such that\n\ni=1\n\n(\u0001xsi\n\n)2.\n\n|i n,n\n\n6\n\n| \u2264 (cid:21) f\n\n(cid:3)(cid:3)(cid:21)\u221e\n\n(cid:12)\u221e\n\n|2 \u2264 [m]\u221e < \u221e and\np(|i n\n\n5\n\ni=1\n| + |i n,n\n\n6\n\nn\u2265i>n,si\u2264t\n|\u0001asi\n| > \u03b5) < \u03b5.\n\n| < \u221e, then given \u03b5 > 0, we can choose\n\ntends to zero in probability as n \u2192 \u221e, since x n\nonce we choose n, we then see that i n,n\nconverges in probability to xt uniformly over t \u2265 0. we conclude that j n(t ) converges to\nj (t ) in probability as n \u2192 \u221e.\nthis completes the proof.\n\n7\n\nt\n\n17.4 the reduction theorem\n\nlet m be a process adapted to {ft}. if there exist stopping times tn increasing to \u221e such\nthat each process mt\u2227tn is a uniformly integrable martingale, we say m is a local martingale.\nif each mt\u2227tn is a square integrable martingale, we say m is a locally square integrable\nmartingale. we say a stopping time t reduces a process m if mt\u2227t is a uniformly integrable\nmartingale.\n\nproof\nreduce m + n.\n\nlemma 17.6 (1) the sum of two local martingales is a local martingale.\n(2) if s and t both reduce m, then so does s \u2228 t .\n(3) if there exist times tn \u2192 \u221e such that mt\u2227tn is a local martingale for each n, then m\nis a local martingale.\n(1) if the sequence sn reduces m and the sequence tn reduces n, then sn \u2227 tn will\n(2) mt\u2227(s\u2228t ) is bounded in absolute value by |mt\u2227t| + |mt\u2227s|. both {|mt\u2227t|} and {|mt\u2227s|}\nare uniformly integrable families of random variables. now use proposition a.17.\n= snm\u2227 tn. renumber\nthe stopping times into a single sequence r1, r2, . . . and let hk = r1\u2228\u00b7\u00b7\u00b7\u2228rk. note hk \u2191 \u221e.\nto show that hk reduces m, we need to show that ri reduces m and use (2). but ri = s\n(cid:3)\nnm\nfor some m, n, so mt\u2227ri\nlet m be a local martingale with m0 = 0. we say that a stopping time t strongly reduces\nm if t reduces m and the martingale e [|mt| | fs] is bounded on [0, t ), that is, there exists\nk > 0 such that\n\n= mt\u2227snm\u2227tn is a uniformly integrable martingale.\n\n(3) let snm be a family of stopping times reducing mt\u2227tn and let s\n\n(cid:3)\nnm\n\ne [|mt| | fs] \u2264 k,\n\na.s.\n\nsup\n0\u2264s<t\n\nlemma 17.7 (1) if t strongly reduces m and s \u2264 t , then s strongly reduces m.\n(2) if s and t strongly reduce m, then so does s \u2228 t .\n(3) if y\u221e is integrable, then e [e [y\u221e | ft ] | fs] = e [y\u221e | fs\u2227t ].\n\n "}, {"Page_number": 158, "text": "140\n\nprocesses with jumps\n\n(1) note e [|ms| | fs] \u2264 e [|mt| | fs] by jensen\u2019s inequality, hence s strongly\nproof\nreduces m.\n(2) it suffices to show that e [|ms\u2228t| | ft] is bounded for t < t , since by symmetry the\nsame will hold for t < s. for t < t this expression is bounded by\ne [|mt| | ft] + e [|ms|1(s>t ) | ft].\n\nthe first term is bounded since t strongly reduces m. for the second term, if t < t ,\n\n1(t<t )e [|ms|1(s>t ) | ft] = e [|ms|1(s>t )1(t<t ) | ft]\n\n\u2264 e [|ms|1(t<s) | ft]\n= e [|ms| | ft]1(t<s),\n\nwhich in turn is bounded since s strongly reduces m.\n\n(3) this is exercise 3.10.\n\nlemma 17.8 if m is a local martingale with m0 = 0, then there exist stopping times tn \u2191 \u221e\nthat strongly reduce m.\nproof let rn \u2191 \u221e be a sequence reducing m. let\nsnm = rn \u2227 inf{t : e [|mrn\n\n| | ft] \u2265 m}.\n\narrange the stopping times snm into a single sequence {un} and let tn = u1 \u2228 \u00b7\u00b7\u00b7 \u2228 un. in\nview of the preceding lemmas, we need to show ui strongly reduces m, which will follow if\nsnm does for each n and m.\n| | ft], where we take a version whose paths are right continuous with\nlet yt = e [|mrn\nleft limits. y is bounded by m on [0, snm). by jensen\u2019s inequality for conditional expectations\nand lemma 17.7\n\ne [|msnm\n\n|1(t<snm ) | ft] \u2264 e [|e [|mrn\n= e [ e [|mrn\n= e [|mrn\n= ysnm\u2227t1(t<snm )\n= yt1(t<snm ) \u2264 m.\n\n| | fsnm]|1(t<snm ) | ft]\n|1(t<snm ) | fsnm] | ft]\n\n|1(t<snm ) | fsnm\u2227t]\n\nwe are done.\n\nour main theorem of this section is the following.\n\ntheorem 17.9 suppose m is a local martingale. then there exist stopping times tn \u2191 \u221e\nsuch that mt\u2227tn\nt , where each u n is a square integrable martingale and each v n\nis a martingale whose paths are of bounded variation and such that the total variation of the\npaths of vn is integrable. moreover, ut = ut and vt = vt for t \u2265 t .\n\n= u n\n\n+ v n\n\nt\n\nthe last sentence of the statement of the theorem says that u and v are both constant from\n\ntime t on.\n\nit suffices to prove that if m is a local martingale with m0 = 0 and t strongly\nproof\nreduces m, then mt\u2227t can be written as u +v with u and v of the described form. thus we\n\n "}, {"Page_number": 159, "text": "17.5 semimartingales\n\n141\n\nmay assume mt = mt for t \u2265 t , |mt| is integrable, and e [|mt| | ft] is bounded, say by\nlet at = mt 1(t\u2265t ) = mt1(t\u2265t ), let(cid:14)a be the compensator of a, let v = a \u2212(cid:14)a, and let\nu = m \u2212 a +(cid:14)a. then v is a martingale of bounded variation. we compute the expectation\nk, on [0, t ).\nof(cid:14)a is bounded by\n\nof the total variation of v . let bt = m\n\u2212\nt 1(t\u2265t ). then the expectation of\nthe total variation of a is bounded by e|mt| < \u221e and the expectation of the total variation\n\nt 1(t\u2265t ) and ct = m\n+\n\ne(cid:14)b\u221e + e(cid:14)c\u221e = e b\u221e + e c\u221e \u2264 e|mt| < \u221e.\n\nwe need to show u is square integrable. note\n\n|mt \u2212 at| = |mt|1(t<t ) = |e [m\u221e | ft]|1(t<t )\n\n= |e [e [m\u221e | ft\u2228t] | ft]|1(t<t ) = |e [mt\u2228t | ft]|1(t<t )\n= |e [mt | ft]|1(t<t ) \u2264 e [|mt| | ft]1(t<t ) \u2264 k.\n\ntherefore it suffices to show(cid:14)a is square integrable.\n16.33, e(cid:14)b2\u221e < \u221e. similarly, e(cid:14)c2\u221e < \u221e. since a = b \u2212 c, then(cid:14)a =(cid:14)b \u2212(cid:14)c, and it follows\n\n| ft] is bounded by k on [0, t ), and by proposition\n\n+\nour hypotheses imply that e [m\nt\n\n(cid:14)at is square integrable.\n\nthat supt\u22650\n\n17.5 semimartingales\n\nwe define a semimartingale to be a process of the form xt = x0 + mt + at, where x0 is finite,\na.s., and is f0 measurable, mt is a local martingale, and at is a process whose paths have\nbounded variation on [0, t] for each t.\n\nif mt is a local martingale, let tn be a sequence of stopping times as in theorem 17.9. we\n\nset m c\n\nt\u2227tn\n\n= (u n)c\n\nt for each n and\n\n[m]t\u2227tn\n\n= (cid:22)m c(cid:23)\n\nt\u2227tn\n\n+\n\n\u0001m 2\ns\n\n.\n\n(cid:9)\n\ns\u2264t\u2227tn\n\nt\n\nt\n\n.\n\n+\n\n\u0001x 2\ns\n\nt and define\n\n= (cid:22)m c(cid:23)\n\n[x ]t = (cid:22)x c(cid:23)\n\nit is easy to see that these definitions are independent of how we decompose m into u n + v n\nand of which sequence of stopping times tn strongly reducing m we choose. we define\n(cid:22)x c(cid:23)\n\n(cid:9)\n(cid:15)\nwe say an adapted process h is locally bounded if there exist stopping times sn \u2191 \u221e and\nconstants kn such that on [0, sn] the process h is bounded by kn. if xt is a semimartingale and\n0 hs dxs as follows. let xt = x0+mt+at.\nh is a locally bounded predictable process, define\nif rn = tn \u2227 sn, where the tn are as in theorem 17.9 and the sn are the stopping times used\nin the definition of locally bounded, set\nhs dms to be the stochastic integral as defined\nin section 17.2. define\nhs das to be the usual lebesgue\u2013stieltjes integral. define\nthe stochastic integral with respect to x as the sum of these two. since rn \u2191 \u221e, this\nt\ndefines\n0 hs dxs for all t. one needs to check that the definition does not depend on the\ndecomposition of x into m and a nor on the choice of stopping times rn.\n\nt\u2227rn\n0\n\nt\u2227rn\n0\n\n(cid:15)\n\n(cid:15)\n\n(cid:15)\n\ns\u2264t\n\nt\n\nwe now state the general it\u02c6o formula.\n\n "}, {"Page_number": 160, "text": "142\n\nprocesses with jumps\n\ntheorem 17.10 suppose x is a semimartingale and f is c2. then\n\n(cid:3)\n\n(cid:3)\n\nt\n\n(cid:3)(cid:3)(xs\u2212) d(cid:22)x c(cid:23)\n\nf\n\ns\n\n0\n\n(cid:3)(xs\u2212)\u0001xs].\n\n(cid:9)\nf (xt ) = f (x0) +\n\n+\n\ns\u2264t\n\nt\n\n2\n\n0\n\nf\n\n(cid:3)(xs\u2212) dxs + 1\n[ f (xs) \u2212 f (xs\u2212) \u2212 f\n(cid:15)\n\n:\n\nt\n0\n\n|das| \u2265 n}, let rn = tn \u2227 sn, and let x n\n\nproof first suppose f has bounded first and second derivatives. let tn be stopping times\n=\nstrongly reducing mt, let sn = inf{t\n\u2212 \u0001arn. since the total variation of at is bounded on [0, rn), it follows that x n is\nxt\u2227rn\na semimartingale which is the sum of a square integrable martingale and a process whose\ntotal variation is integrable. we apply theorem 17.5 to this process. x n\nt agrees with xt on\n[0, rn). as in the proof of theorem 17.5, by looking at the jump at time rn, both sides of\nit\u02c6o\u2019s formula jump the same amount at time rn, and so it\u02c6o\u2019s formula holds for xt on [0, rn].\nif we now only assume that f is c2, we approximate f by a sequence fm of functions that are\nc2 and whose first and second derivatives are bounded, and then let m \u2192 \u221e; we leave the\ndetails to the reader. thus it\u02c6o\u2019s formula holds for t in the interval [0, rn] and for f without\nthe assumption of bounded derivatives. finally, we observe that rn \u2192 \u221e, so except for a\nnull set, it\u02c6o\u2019s formula holds for each t.\n\nt\n\nthe proof of the following corollary is similar to the proof of it\u02c6o\u2019s formula.\n\ncorollary 17.11 if xt = (x 1\ncomponent is a semimartingale, and f is a c2 function on rd, then\n\n, . . . , x d\nt\n\nt\n\n) is a process taking values in rd such that each\n\nt\n\n0\n\ni=1\n\n\u2202 f\n\u2202xi\n\n(xs\u2212) dx i\ns\n\n(cid:3)\nd(cid:9)\nd(cid:9)\nf (xs) \u2212 f (xs\u2212) \u2212 d(cid:9)\n\n\u2202 2 f\n\u2202xi\u2202x j\n\ni, j=1\n\nt\n\n(cid:16)\n\n(xs\u2212) d(cid:22)(x i)c, (x j )c(cid:23)\n\ns\n\n(cid:17)\n\n\u2202 f\n\u2202xi\n\ni=1\n\n(xs\u2212)\u0001x i\ns\n\n.\n\nf (xt ) = f (x0) +\n(cid:3)\n(cid:9)\n\n+ 1\n\n2\n\n0\n\n+\n\ns\u2264t\n\nif x and y are real-valued semimartingales, define\n\n([x + y ]t \u2212 [x ]t \u2212 [y ]t ).\n\n(17.10)\n\nthe following corollary is the product formula for semimartingales with jumps.\n\ncorollary 17.12 if x and y are semimartingales of the above form,\n\n2\n\n[x , y ]t = 1\n(cid:3)\n\n0\n\n(cid:3)\n\n0\n\nxtyt = x0y0 +\n\nt\n\nxs\u2212 dys +\n\nt\n\nys\u2212 dxs + [x , y ]t .\n\nproof apply theorem 17.10 with f (x) = x2. since in this case\n(cid:3)(xs\u2212)\u0001xs = \u0001x 2\n\nf (xs) \u2212 f (xs\u2212) \u2212 f\n\n,\n\ns\n\n "}, {"Page_number": 161, "text": "(cid:3)\n\n17.6 exponential of a semimartingale\n\nwe obtain\n\nxs\u2212 dxs + [x ]t .\napplying (17.11) with x replaced by y and by x + y and using\n\n= x 2\n\n+ 2\n\nx 2\nt\n\n0\n\n0\n\nt\n\nxtyt = 1\n\n2 [(xt + yt )2 \u2212 x 2\n\nt\n\n\u2212 y 2\nt ]\n\n143\n\n(17.11)\n\ngives our result.\n\na(t ) =(cid:12)\n\ns\u2264t\n\n\u0001a(s).\n\n17.6 exponential of a semimartingale\n\na function with finite total variation is purely discontinuous if it has no continuous part, i.e.,\n\n(cid:10)\n\n(cid:11) (cid:31)\n\ntheorem 17.13 let xt be a semimartingale. define\n\nzt = z0 exp\n \n\nxt \u2212 1\n(cid:22)x c(cid:23)\n(1+ \u0001xs)e\n(cid:3)\nthen zt is a semimartingale,\npaths are purely discontinuous, and zt satisfies\nzt = z0 +\n\n0\u2264s\u2264t\n\n2\n\nt\n\n(1 + \u0001xs)e\n\n\u2212\u0001xs .\n\n0\u2264s\u2264t\n\u2212\u0001xs is a process of bounded variation whose\n\n(17.12)\n\nt\n\nzs\u2212 dxs.\n\n(17.13)\n\n0\n\nproof since the product of finitely many functions of bounded variation which are purely\ndiscontinuous will give a function of the same type and in each finite interval there are only\nfinitely many jumps of xt of size larger in absolute value than 1/2, it suffices to consider\n\n=\n\n(cid:3)\n\nv\nt\n\n(1 + \u0001xs)e\n\n\u2212\u0001xs1(|\u0001xs|\u22641/2).\n\nnote\n\n(cid:3)\n\n=\n\nlog v\nt\n\n(log(1 + \u0001xs) \u2212 \u0001xs)1(|\u0001xs|\u22641/2),\n\n(cid:12)\n\nt\n\n(cid:3)\n\n(cid:3)\n\n< \u221e. exercise 17.4 tells\nwe apply the multivariate version of it\u02c6o\u2019s formula (corollary 17.11). let f (x, y) = exy\n\n) is a purely discontinuous process, and consequently v is also.\n\n\u0001x 2\ns\n\ns\u2264t\n\n= exp(log v\n\nwhich is bounded in absolute value by a constant times\nus that v\nt\nand let zt = f (kt , vt ) where kt = xt \u2212 1\n(cid:9)\nzs\u2212dks +\n+\n\nzt \u2212 z0 =\n\n(cid:3)\n(cid:22)x c(cid:23)\n\nt. we obtain\neks\u2212 dvs + 1\n\n(cid:3)\n\n[zs \u2212 zs\u2212 \u2212 zs\u2212\u0001ks \u2212 e\n\nzs\u2212 d(cid:22)kc(cid:23)\n\u2212ks\u2212 \u0001vs]\n\n(cid:3)\n\n2\n\n0\n\n0\n\n2\n\n0\n\nt\n\nt\n\nt\n\nt\n\n(cid:31)\n(cid:9)\n\n0\u2264s\u2264t\n\ns\u2264t\n\nwe have\n\ni1 =\n\n= i1 + i2 + i3 + i4.\n\ns\u2264t\n\n(cid:3)\n\nt\n\nzs\u2212 dxs \u2212 1\n\n2\n\n0\n\n(cid:3)\n\n0\n\nt\n\nzs\u2212 d(cid:22)x c(cid:23)\n\ns\n\n.\n\n "}, {"Page_number": 162, "text": "144\n\nsince vt is purely discontinuous,\n\nsince kc = x c,\n\nprocesses with jumps\n\ni2 =\n\neks\u2212 \u0001vs.\n\nt\n\ni3 = 1\n\nzs\u2212 d(cid:22)x c(cid:23)\n(cid:9)\nnote that zs = zs\u2212(1 + \u0001xs) and zs\u2212\u0001ks = zs\u2212\u0001xs, so\neks\u2212 \u0001vs.\n\ni4 = \u2212\n\n.\n\n2\n\n0\n\ns\n\n(cid:9)\n(cid:3)\n\ns\u2264t\n\ns\u2264t\n\nsumming yields (17.13).\n\nthe solution z of (17.13) is called the exponential of the semimartingale x .\n\n17.7 the girsanov theorem\n\nlet p and q be two equivalent probability measures, that is, p and q are mutually absolutely\ncontinuous. let m\u221e be the radon\u2013nikodym derivative of q with respect to p and let\nmt = e [m\u221e | ft]. the martingale mt is uniformly integrable since m\u221e \u2208 l1(p). once a\nnon-negative martingale hits zero, it is easy to see that it must be zero from then on; this is\nexercise 17.5. since q and p are equivalent, then m\u221e > 0, a.s., and so mt never equals zero,\na.s. observe that mt is the radon\u2013nikodym derivative of q with respect to p on ft .\nlet lt be the local martingale defined by\nlt =\n\ndms,\n\n(cid:3)\n\nt\n\n1\nms\u2212\n\n0\n\nso that\n\ndmt = mt\u2212 dlt ,\n\nor m is the exponential of l.\ntheorem 17.14 suppose x is a local martingale with respect to p. then xt \u2212 dt is a local\nmartingale with respect to q, where\n\n(cid:3)\n\n(cid:3)\n\ndt =\n\nd[x , m]s =\n\nt\n\n0\n\n1\nms\n\nt\n\n0\n\nms\u2212\nms\n\nd[x , l]s.\n\nnote that in the formula for d, we are using a lebesgue\u2013stieltjes integral.\nproof exercise 17.6 tells us that it suffices to show that mt (xt \u2212 dt ) is a local martingale\nwith respect to p. by corollary 17.12,\n\nd (m (x \u2212 d))t = (x \u2212 d)t\u2212 dmt + mt\u2212 dxt \u2212 mt\u2212 ddt\n\n+ d[m, x \u2212 d]t .\n\n "}, {"Page_number": 163, "text": "(cid:9)\n\ns\u2264t\n\n(cid:3)\n(cid:3)\n\nthe first two terms on the right are local martingales with respect to p. since d is of bounded\nvariation, the continuous part of d is zero, hence\n\nexercises\n\n145\n\n[m, d]t =\n\n\u0001ms\u0001ds =\n\n\u0001ms dds.\n\n(cid:3)\n\nt\n\n0\n\n(cid:3)\n\nthus\n\nmt (xt \u2212 dt ) = local martingale + [m, x ]t \u2212\n\nt\n\nms dds.\n\nusing the definition of d shows that mt (xt \u2212 dt ) is a local martingale.\n\n0\n\n17.1 suppose a(t ) is a deterministic right-continuous nondecreasing function of t with a(0) = 0.\n\nprove the following formulas:\n\nexercises\n\nand\n\na(t )2 =\na(t )2 =\n= 2\n\nt\n\n0\n\nt\n\n(cid:3)\n\n0\n\n[(a(t ) \u2212 a(s)) + (a(t ) \u2212 a(s\u2212))] da(s),\n(cid:9)\n(2a(s\u2212) + \u0001a(s)) da(s)\na(s\u2212) da(s) +\n\n(\u0001a(s))2.\n\nt\n\n(17.14)\n\n(17.15)\n\n0\n\ns\n\nhint: first do the case where a has only finitely many discontinuities.\n\n17.2 if at is an increasing process and(cid:14)at is its compensator, show that(cid:14)a jumps only when a does.\nt , j \u2208 z, be independent poisson processes with parameter \u03bb j. suppose \u03bb j = \u03bb\u2212 j for each\n\n17.3 let p j\n\nj (cid:16)= 0. suppose \u03bb j decreases as j increases for j \u2265 1. let\n\n(cid:9)\n\nj\u2208z\n\nxt =\n\np j\nt\n\n.\n\ndetermine reasonable conditions on the sequence \u03bb j so that x is a semimartingale. a local\nmartingale. a martingale. a locally square integrable martingale.\n\n17.4 show that if f (t ) is a purely discontinuous function, then e f (t ) is also.\n17.5 suppose m is a non-negative right-continuous martingale and t = inf{t > 0 : mt = 0}. show\n\nthat mt = 0 on (t > t ).\n\n17.6 suppose p and q are two equivalent probability measures, m\u221e is the radon\u2013nikodym derivative\nof q with respect to p, and mt = e [m\u221e | ft]. show that yt is a local martingale with respect\nto q if and only if ytmt is a local martingale with respect to p.\n\n17.7 suppose xt is an increasing process with paths that are right continuous with left limits, x0 = 0,\na.s., x is purely discontinuous, and all jumps are of size +1 only. suppose xt \u2212t is a martingale.\nprove that x is a poisson process.\n\nhint: imitate the proof of theorem 12.1. when using it\u02c6o\u2019s formula, it is important to use the\n\nfact that \u0001xt is always 0 or 1.\n\n "}, {"Page_number": 164, "text": "146\n17.8 suppose xt is an increasing process with paths that are right continuous with left limits, x0 = 0,\na.s., x is purely discontinuous, and all jumps are of size +1 only. suppose limt\u2192\u221e xt = \u221e, a.s.\nprove that x is a time change of a poisson process.\n\nprocesses with jumps\n\n17.9 suppose pt is a poisson process with parameter \u03bb, {ft} is the minimal augmented filtration for\np, and mt = pt \u2212 \u03bbt. suppose y is a f1 measurable random variable with finite mean and\nvariance. prove that there exists a predictable process h such that\n\ny = e y +\n\n1\n\nhs dms.\n\n(cid:3)\n\n0\n\n(cid:3)\n\n17.10 let p1 and p2 be two independent poisson processes with the same parameter. let xt = p1\n\n\u2212 p2\nand let {ft} be the minimal augmented filtration for x . find a bounded mean zero random\nvariable y that is f1 measurable which does not satisfy\n\nt\n\nt\n\ny =\n\nfor any predictable process h .\n\n1\n\n0\n\nhs dxs\n\n "}, {"Page_number": 165, "text": "18\n\npoisson point processes\n\npoisson point processes are random measures that are related to poisson processes. we will\nuse them when we study l\u00b4evy processes in chapter 42. poisson point processes are also\nuseful in the study of excursions, even excursions of a continuous process such as brownian\nmotion (see chapter 27), and they arise when studying stochastic differential equations with\njumps.\nlet s be a metric space, g the collection of borel subsets of s, and \u03bb a measure on (s,g ).\n\ndefinition 18.1 we say a map\n\nn : \u0001 \u00d7 [0,\u221e) \u00d7 g \u2192 {0, 1, 2, . . .}\n\n(writing nt (a) for n (\u03c9, t, a)) is a poisson point process if\n(1) for each borel subset a of s with \u03bb(a) < \u221e, the process nt (a) is a poisson process\nwith parameter \u03bb(a), and\n(2) for each t and \u03c9, n (t,\u00b7) is a measure on g.\na model to keep in mind is where s = r and \u03bb is a lebesgue measure. for each \u03c9 there\nis a collection of points {(s, z)} (where the collection depends on \u03c9). the number of points\nin this collection with s \u2264 t and z in a subset a is nt (a)(\u03c9). since \u03bb(r) = \u221e, there are\ninfinitely many points in every time interval.\na consequence of the definition is that since \u03bb(\u2205) = 0, then nt (\u2205) is a poisson process\nwith parameter 0; in other words, nt (\u2205) is identically zero.\n\nour main result is that nt (a) and nt (b) are independent if a and b are disjoint.\ntheorem 18.2 let {ft} be a filtration satisfying the usual conditions. let s be a metric\nspace furnished with a positive measure \u03bb. suppose that nt (a) is a poisson point process\nwith respect to the measure \u03bb. if a1, . . . , an are pairwise disjoint measurable subsets of\ns with \u03bb(ak ) < \u221e for k = 1, . . . , n, then the processes nt (a1), . . . , nt (an) are mutually\nindependent.\nproof we first make the observation that because n (t,\u00b7) is a measure and the a1, a2, . . . , an\nk=1ak ) is a poisson process with finite parameter. a\nare disjoint, then\npoisson process has jumps of size one only, hence no two of the nt (ak ) have jumps at the\nsame time.\nto prove the theorem, it suffices to let 0 = r0 < r1 < \u00b7\u00b7\u00b7 < rm and show that the random\n\n(cid:12)\nk=1 nt (ak ) = nt (\u222an\n\nn\n\nvariables\n\n{nr j\n\n(ak ) \u2212 nr j\u22121\n\n(ak ) : 1 \u2264 j \u2264 m, 1 \u2264 k \u2264 n}\n\n147\n\n "}, {"Page_number": 166, "text": "148\n\npoisson point processes\n\nare independent. since for each j and each k, nr j\nsuffices to show that for each j \u2264 m, the random variables\n\n(ak ) \u2212 nr j\u22121\n\n(ak ) is independent of fr j\u22121, it\n\n{nr j\n\n(ak ) \u2212 nr j\u22121\n\n(ak ) : 1 \u2264 k \u2264 n}\n\nare independent. we will do the case j = m = 1 and write r for r j for simplicity; the case\nwhen j, m > 1 differs only in notation.\nwe will prove this using induction. we start with the case n = 2 and show the independence\nof nr(a1) and nr(a2). each nt (ak ) is a poisson process, and so nt (ak ) has moments of all\norders. let u1, u2 \u2208 r and set\n\n\u03c6k = \u03bb(ak )(eiuk \u2212 1),\n\nk = 1, 2.\n\nlet\n\nwe see that m k\n\n= eiuk nt (ak )\u2212t\u03c6k .\n\nm k\nt\n\nt is a martingale because e eiuk nt (ak ) = et\u03c6k , and therefore\ne [eiu(nt (ak )\u2212ns (ak )))\u2212(t\u2212s)\u03c6k | fs]\n| fs] = m k\ns\n\u2212(t\u2212s)\u03c6k e [eiu(nt (ak )\u2212ns (ak ))] = m k\n= m k\ns e\n\ne [m k\nt\n\ns\n\n,\n\nusing the independence and stationarity of the increments of a poisson process.\n\nsince we have argued that no two of the nt (ak ) jump at the same time, the same is true for\nt and so [m j, m k]t = 0 if j (cid:16)= k. by the product formula (corollary 17.12) and it\u02c6o\u2019s\n\neiukns\u2212 (ak )\u2212s\u03c6k dns(ak )\n\neiukns\u2212 (ak )\u2212s\u03c6k [eiuk \u0001ns (ak ) \u2212 1]\n\nt\n\nt\n\n0\n\nm k\nt\n\n(cid:3)\n\n(cid:3)\n(cid:3)\neiuk ns\u2212 (ak )\u2212s\u03c6k [eiuk \u0001ns (ak ) \u2212 1 \u2212 iuk\u0001ns(ak )]\n\nthe m k\nformula (theorem 17.10)\n(cid:9)\n= 1 \u2212 \u03c6k\n+\n\neiukns\u2212 (ak )\u2212s\u03c6k ds + iuk\n(cid:9)\n\u2212(cid:14)bk\nwhose paths are locally of bounded variation, and(cid:14)bk\n\n+ bk\n\u2212 1 is of the form bk\n\n= 1 \u2212(cid:14)bk\n\neiukns\u2212 (ak )\u2212s\u03c6k ds +\n\nwe see therefore that m k\nt\n\ns\u2264t\n= 1 \u2212 \u03c6k\n\ns\u2264t\n\n.\n\n0\n\n0\n\nt\n\nt\n\nt\n\nt\n\nlet m k\nt\n\n= m k\nt\u2227r\n\n\u2212 1. since the m k\n\nlemma (lemma 17.2), e m 1\u221em 2\u221e = 0, which translates to\n= 1.\n(cid:16)\n\nthis implies\n\ne m 1\n\nr m 2\nr\n\n(cid:16)\n\n(cid:17)\n\nei(u1nr (a1 )+u2nr (a2 ))\n\ne\n\n= er\u03c61er\u03c62 = e\n\n(cid:17)\n\n(cid:16)\n\n(cid:17)\n\neiu1nr (a1 )\n\ne\n\neiu2nr (a2 )\n\n.\n\nt , where bk\nt is the compensator of bk\nt .\n\nt is a complex-valued process\n\nt do not jump at the same time, by the orthogonality\n\nsince this holds for all u1, u2, then nr(a1) and nr(a2) are independent. we conclude that the\nprocesses nt (a1) and nt (a2) are independent.\n\n "}, {"Page_number": 167, "text": "poisson point processes\n\n149\n\nto handle the case n = 3, we first show that m 1\n\nt m 2\n\nt is a martingale. we write\n\ne [m 1\n\n| fs]\nt m 2\nt\n\u2212(t\u2212s)(\u03c61+\u03c62 )e [ei(u1 (nt (a1 )\u2212ns (a1 ))+u2 (nt (a2 )\u2212ns (a2 ))) | fs]\n= m 1\ns m 2\ns e\n= m 1\n\u2212(t\u2212s)(\u03c61+\u03c62 )e [ei(u1 (nt (a1 )\u2212ns (a1 ))+u2 (nt (a2 )\u2212ns (a2 )))]\ns m 2\ns e\n= m 1\ns m 2\n,\ns\n\nusing the fact that nt (a1) and nt (a2) are independent of each other and each have stationary\nand independent increments.\n\n= eiu3nt (a3 )\u2212t\u03c63 has no jumps in common with m 1\n\nt or m 2\n\nt . therefore if\n\nnote that m 3\nt\n= m 3\nt\u2227r, then\n\nm 3\nt\n\nand as before this leads to\n\ne [m 3\u221e(m 1\u221em 2\u221e)] = 0,\n\ne [m 3\nr\n\n(m 1\n\nr m 2\nr\n\n)] = 1.\n\nas above this implies that nr(a1), nr(a2), and nr(a3) are independent. to prove the general\ninduction step is similar.\n\nwe will also need the following corollary.\ncorollary 18.3 let ft and nt (ak ) be as in theorem 18.2. suppose yt is a process with paths\nthat are right continuous with left limits such that yt \u2212 ys is independent of fs whenever\ns < t and yt \u2212 ys has the same law as yt\u2212s for each s < t. suppose moreover that y has no\njumps in common with any of the nt (ak ). then the processes nt (a1), . . . , nt (an), and yt are\nindependent.\nproof the law of y0 is the same as that of yt \u2212 yt, so y0 = 0, a.s. by the fact that y has\nstationary and independent increments,\n\ne eiuys+t = e eiuys e eiu(ys+t\u2212ys ) = e eiuys e eiuyt ,\n\nwhich implies that the characteristic function of y is of the form e eiuyt = et\u03c8 (u) for some\nfunction \u03c8 (u).\n\nwe fix u \u2208 r and define\n\n= eiuyt\u2212t\u03c8 (u).\n\nmy\nt\n\nas in the proof of theorem 18.2, we see that my\nt\ncommon with any of the m k\n\n= my\n\nt , if my\n\nt\n\nt\u2227r, we see by lemma 17.2 that\n\nis a martingale. since my has no jumps in\n\ne [my\u221e(m 1\u221e \u00b7\u00b7\u00b7 m n\u221e)] = 1,\n\nor\n\ne [my\n\nr m 1\nr\n\n\u00b7\u00b7\u00b7 m n\n\nr ] = 1.\n\nthis leads as above to the independence of y from all the nt (ak )\u2019s.\n\nwe now turn to stochastic integrals with respect to poisson point processes. in the same\nway that a nondecreasing function on the reals gives rise to a measure, so nt (a) gives rise\n\n "}, {"Page_number": 168, "text": "150\nto a random measure \u03bc(dt, dz) on the product \u03c3 -field b[0,\u221e) \u00d7 g, where b[0,\u221e) is the\nborel \u03c3 -field on [0,\u221e); \u03bc is determined by\n\npoisson point processes\n\n\u03bc([0, t] \u00d7 a)(\u03c9) = nt (a)(\u03c9).\n\nsuppose h (\u03c9, s, z) is of the form\n\ndefine a nonrandom measure \u03bd on b[0,\u221e) \u00d7 g by \u03bd([0, t] \u00d7 a) = t\u03bb(a) for a \u2208 g. if\n\u03bb(a) < \u221e, then \u03bc([0, t] \u00d7 a) \u2212 \u03bd([0, t] \u00d7 a) is the same as a poisson process minus its\nmean, hence is locally a square integrable martingale.\nwe can define a stochastic integral with respect to the random measure \u03bc \u2212 \u03bd as follows.\n\nh (\u03c9, s, z) = n(cid:9)\n(18.1)\nwhere for each i the random variable ki is bounded and fai measurable and ai \u2208 g with\n(cid:3)\n(cid:3)\n\u03bb(ai) < \u221e. for such h we define\n= n(cid:9)\n\nki(\u03c9)1(ai,bi](s)1ai\n\nnt =\n\n(18.2)\n\n(z),\n\ni=1\n\n0\n\nt\n\nh (\u03c9, s, z) d (\u03bc \u2212 \u03bd )(ds, dz)\nki(\u03bc \u2212 \u03bd )(((ai, bi] \u2229 [0, t]) \u00d7 ai).\n(cid:3)\n\n(cid:3)\n\ni=1\n\nlet us assume without loss of generality that the ai are disjoint. it is not hard to see (exercise\n18.3) that nt is a martingale, that n c = 0, and that\n\n[n]t =\n\nt\n\nh (\u03c9, s, z)2 \u03bc(ds, dz).\n\n(18.3)\n\nsince (cid:22)n(cid:23)\nproposition 16.30 that (cid:22)n(cid:23)\n\nt must be predictable and all the jumps of n are totally inaccessible, it follows from\n\nt is a martingale, we conclude\n\nt is continuous. since [n]t \u2212 (cid:22)n(cid:23)\n(cid:22)n(cid:23)\n\nh (\u03c9, s, z)2 \u03bd(ds, dz).\n\n=\n\nt\n\nt\n\n(cid:3)\n\n0\n\n(cid:3)\n\n0\n\n(18.4)\n\nsuppose h (s, z) is a predictable process in the following sense: h is measurable with\n\nrespect to the \u03c3 -field generated by all processes of the form (18.1). suppose also that\n\n(cid:3) \u221e\n\n(cid:3)\n\ne\n\ns\n\n0\n\nh (s, z)2 \u03bd(ds, dz) < \u221e.\n\n=(cid:15)\n\n(cid:15) \u221e\n\n(cid:15)\n\n0\n\ns h 2 d\u03bd )1/2. the corresponding n n\nt\n\ntake processes h n of the form (18.1) converging to h in the space l2 with norm\n0 h n(s, z) d (\u03bc \u2212 \u03bd ) are easily seen to be a\n(e\ncauchy sequence in l2, and the limit nt we call the stochastic integral of h with respect to\n\u03bc\u2212 \u03bd. as in the continuous case, we may prove that e n 2\n(cid:3)\nt, and it follows\nfrom this, (18.3), and (18.4) that\n\n= e [n]t = e(cid:22)n(cid:23)\n(cid:3)\n\n(cid:3)\n\n(cid:3)\n\nt\n\nt\n\nh (s, z)2 \u03bc(ds, dz),\n\nh (s, z)2 \u03bd(ds, dz).\n\n(18.5)\n\n(cid:22)n(cid:23)\n\nt\n\n=\n\nt\n\ns\n\n0\n\n[n]t =\n\nt\n\ns\n\n0\n\none may think of the stochastic integral as follows: if \u03bc gives unit mass to a point at time t\nwith value z, then nt jumps at this time t and the size of the jump is h (t, z).\n\n "}, {"Page_number": 169, "text": "exercises\n\n151\n\nexercises\n\n18.1 suppose {ft} is a filtration satisfying the usual conditions and p1\n\nt and p2\nt are poisson processes\nwith respect to {ft} with parameters \u03bb1, \u03bb2, respectively. suppose p1\n+ p2\nis a poisson process\nwith parameter \u03bb1 + \u03bb2. prove that p1 and p2 are independent processes.\n\n18.2 suppose {ft} is a filtration satisfying the usual conditions, pt is a poisson process with respect\nto {ft}, and wt is a brownian motion with respect to {ft}. show that if wt + pt has stationary\nand independent increments, then p and w are independent processes.\n\n18.3 if h is as in (18.1) and n is defined by (18.2), show that n is a martingale, n c = 0, and [n]t is\n\nt\n\nt\n\ngiven by( 18.3).\n\n18.4 suppose {as, 0 < s < \u221e} is a collection of subsets of s such that \u03bb(as ) \u2192 \u221e as s \u2192 \u221e.\nshow that nt (as )/\u03bb(as ) converges to t uniformly over finite intervals, where the convergence\nis in probability.\n\n18.5 suppose {as, 0 < s < \u221e} is a collection of subsets of s such that ar \u2282 as if r \u2264 s and\n\n\u03bb(as ) \u2192 \u221e as s \u2192 \u221e. show that for each t,\n\n(cid:20)(cid:20)(cid:20) nu(as )\n\n\u03bb(as )\n\n(cid:20)(cid:20)(cid:20)\n\n\u2212 u\n\ntends to zero almost surely as s \u2192 \u221e.\n\nsup\nu\u2264t\n\n18.7 let p j\n\nhas \u03bb as the corresponding measure.\n\n18.6 let s be a metric space and \u03bb a \u03c3 -finite measure on s. construct a poisson point process which\n\nt , j = 1, 2, . . . be independent poisson processes with parameter \u03b2 j. let xt =(cid:12)\u221e\n\nj=1 a jp j\nt ,\nwhere a j is a sequence such that xt is finite, a.s. for a \u2282 r \\ {0}, define nt (a) to be the number\nof times before time t that x has a jump whose size is in a:\n1a(xs \u2212 xs\u2212 ).\n\nnt (a) =\n\n(cid:9)\n\ns\u2264t\n\nprove that nt is a poisson point process and determine \u03bb.\n\n "}, {"Page_number": 170, "text": "19\n\nframework for markov processes\n\nit is not uncommon for a markov process to be defined as a sextuple (\u0001,f ,ft , xt , \u03b8t , px),\nand for additional notation (e.g., \u03b6 , \u0001,s, pt , r\u03bb, etc.) to be introduced rather rapidly. this\ncan be intimidating for the beginner. we will explain all this notation in as gentle a manner\nas possible. we will consider a markov process to be a pair (xt , px) (rather than a sextuple),\nwhere xt is a single stochastic process and {px} is a family of probability measures, one\nprobability measure px corresponding to each element x of the state space.\n\n19.1 introduction\n\nthe idea that a markov process consists of one process and many probabilities is one that takes\nsome getting used to. to explain this, let us first look at an example. suppose x1, x2, . . . is a\nmarkov chain with stationary transition probabilities with k states: 1, 2, . . . , k. everything\nwe want to know about x can be determined if we know p(i, j) = p(x1 = j | x0 = i) for\neach i and j and \u03bc(i) = p(x0 = i) for each i. we sometimes think of having a different\nmarkov chain for every choice of starting distribution \u03bc = (\u03bc(1), . . . , \u03bc(k )). but instead\nlet us define a new probability space by taking \u0001(cid:3)\nto be the collection of all sequences\n\u03c9 = (\u03c90, \u03c91, . . .) such that each \u03c9n takes one of the values 1, . . . , k. define xn(\u03c9) = \u03c9n.\ndefine fn to be the \u03c3 -field generated by x0, . . . , xn; this is the same as the \u03c3 -field generated\nby sets of the form {\u03c9 : \u03c90 = a0, . . . , \u03c9n = an}, where a0, . . . , an \u2208 {1, 2, . . . , k}. for each\nx = 1, 2, . . . , k, define a probability measure px on \u0001(cid:3)\n\nby\n\npx(x0 = x0, x1 = x1, . . . xn = xn)\n\n(19.1)\n\n= 1{x}(x0)p(x0, x1)\u00b7\u00b7\u00b7 p(xn\u22121, xn).\n\nwith an arbitrary probability distribution \u03bc if we define p\u03bc(a) =(cid:12)\n\nwe have k different probability measures, one for each of x = 1, 2, . . . , k, and we can start\npi(a)\u03bc(i). we have\nlost no information by this redefinition, and it turns out this works much better when doing\ntechnical details.\nthe value of x0(\u03c9) = \u03c90 can be any of 1, 2, . . . , k; the notion of starting at x is captured\nby px, not by x0. the probability measure px is concentrated on those \u03c9\u2019s for which \u03c90 = x\nand px gives no mass to any other \u03c9.\n\nk\ni=1\n\nlet us now look at brownian motion, and see how this framework plays out there. let p\nbe a probability measure and let wt be a one-dimensional brownian motion with respect to\n= x + wt is a one-dimensional brownian motion started at x. let\np started at 0. then w x\n\u0001(cid:3) = c[0,\u221e) be the set of continuous functions from [0,\u221e) to r, so that each element\nt\n\n152\n\n "}, {"Page_number": 171, "text": "153\nis a continuous function. (we do not require that \u03c9(0) = 0 or that \u03c9(0) take any\n\n19.2 definition of a markov process\n\n\u03c9 in \u0001(cid:3)\nparticular value of x.) define\n\nxt (\u03c9) = \u03c9(t ).\n\n(19.2)\nthis will be our process. let f be the \u03c3 -field on \u0001(cid:3) = c[0,\u221e) generated by the cylindrical\nsubsets of c[0,\u221e); see definition 1.1. now define px to be the law of w x. this means that\npx is the probability measure on (\u0001(cid:3),f ) defined by\n\npx(x \u2208 a) = p(w x \u2208 a),\n\nx \u2208 r, a \u2208 f .\n\n(19.3)\nthe probability measure px is determined by the fact that if n \u2265 1, t1 \u2264 \u00b7\u00b7\u00b7 \u2264 tn, and\nb1, . . . , bn are borel subsets of r, then\n\np(xt1\n\n\u2208 b1, . . . , xtn\n\n\u2208 bn) = p(w x\n\nt1\n\n\u2208 b1, . . . , w x\n\ntn\n\n\u2208 bn).\n\nwe call the pair (xt , px), x \u2208 r, t \u2265 0, a brownian motion.\n\n19.2 definition of a markov process\n\nwe want to allow our markov processes to take values in spaces other than the euclidean\nones. for now, we take our state space s to be a separable metric space, furnished with the\nborel \u03c3 -field. for the beginner, just think of r in place of s.\nto define a markov process, we start with a measurable space (\u0001,f ) and suppose we\nhave a filtration {ft} (not necessarily satisfying the usual conditions).\ndefinition 19.1 a markov process (xt , px) is a stochastic process\n\nx : [0,\u221e) \u00d7 \u0001 \u2192 s\n\nand a family of probability measures {px : x \u2208 s} on (\u0001,f ) satisfying the following.\n(1) for each t, xt is ft measurable.\n(2) for each t and each borel subset a of s, the map x \u2192 px(xt \u2208 a) is borel measurable.\n(3) for each s, t \u2265 0, each borel subset a of s, and each x \u2208 s, we have\n\npx(xs+t \u2208 a | fs) = pxs (xt \u2208 a),\n\npx \u2212 a.s.\n\n(19.4)\n\nsome explanation is definitely in order. let\n\n\u03d5(x) = px(xt \u2208 a),\n\n(19.5)\nso that \u03d5 is a function mapping s to r. part of the definition of filtration given in chapter 1\nis that each ft \u2282 f. since we are requiring xt to be ft measurable, that means (xt \u2208 a) is\nin f and it makes sense to talk about px(xt \u2208 a). definition 19.1(2) says that the function\n\u03d5 is borel measurable. this is a very mild assumption, and will be satisfied in the examples\nwe look at.\nthe expression pxs (xt \u2208 a) on the right-hand side of (19.4) is a random variable and its\nvalue at \u03c9 \u2208 \u0001 is defined to be \u03d5(xs(\u03c9)), with \u03d5 given by (19.5). note that the randomness\nin pxs (xt \u2208 a) is thus all due to the xs term and not the xt term. definition 19.1(3) can be\nrephrased as saying that for each s, t, each a, and each x, there is a set ns,t,x,a \u2282 \u0001 that is a\nnull set with respect to px and for \u03c9 /\u2208 ns,t,x,a, the conditional expectation px(xs+t \u2208 a | fs)\nis equal to \u03d5(xs).\n\n "}, {"Page_number": 172, "text": "154\n\nframework for markov processes\n\nwe have now explained all the terms in the sextuple (\u0001,f ,ft , xt , \u03b8t , px) except for \u03b8t.\nthese are called shift operators and are maps from \u0001 \u2192 \u0001 such that xs \u25e6 \u03b8t = xs+t. we\ndefer the precise meaning of the \u03b8t and the rationale for them until section 19.5, where they\nwill appear in a natural way.\n\nin the remainder of the section and in section 19.3 we define some of the additional\nnotation commonly used for markov processes. the first one is almost self-explanatory. we\nuse e x for expectation with respect to px. as with pxs (xt \u2208 a), the notation e xs f (xt ), where\nf is bounded and borel measurable, is to be taken to mean \u03c8 (xs) with \u03c8 (y) = e y f (xt ).\nif we want to talk about our markov process started with distribution \u03bc, we define\n\np\u03bc(b) =\n\npx(b) \u03bc(dx),\n\nand similarly for e \u03bc; here \u03bc is a probability on s.\n\n19.3 transition probabilities\n\nif b is the borel \u03c3 -field on a metric space s, a kernel q(x, a) on s is a map from s\u00d7b \u2192 r\nsatisfying the following.\n\n(1) for each x \u2208 s, q(x,\u00b7) is a measure on (s,b).\n(2) for each a \u2208 b, the function x \u2192 q(x, a) is borel measurable.\nthe definition of markov transition probabilities (or simply transition probabilities) is the\n\nfollowing.\ndefinition 19.2 a collection of kernels {pt (x, a); t \u2265 0} are markov transition probabilities\nfor a markov process (xt , px) if\n(1) pt (x,s ) = 1 for each t \u2265 0 and each x \u2208 s.\n(2) for each x \u2208 s, each borel subset a of s, and each s, t \u2265 0,\n\npt (y, a)ps(x, dy).\n(3) for each x \u2208 s, each borel subset a of s, and each t \u2265 0,\n\ns\n\npt+s(x, a) =\n\npt (x, a) = px(xt \u2208 a).\n\n(19.6)\n\n(19.7)\n\ndefinition 19.2(3) can be rephrased as saying that for each x, the measures pt (x, dy) and\n\npx(xt \u2208 dy) are the same. we define\n\npt f (x) =\n\nf (y)pt (x, dy)\n\n(19.8)\n\nwhen f : s \u2192 r is borel measurable and either bounded or non-negative.\nlemma 19.3 suppose pt are markov transition probabilities. if f is borel measurable and\neither non-negative or bounded, then pt f is non-negative (respectively, bounded) and borel\nmeasurable and\n\npt f (x) = e x f (xt ),\n\nx \u2208 s.\n\n(19.9)\n\n(cid:3)\n\n(cid:3)\n\n(cid:3)\n\n "}, {"Page_number": 173, "text": "19.3 transition probabilities\n\n155\n\nproof using (19.7) and definition 19.1(2), the borel measurability and (19.9) hold when\nf is the indicator of a set a. by linearity they hold for simple functions, and then using\nmonotone convergence they hold for non-negative functions. using linearity again, we have\nmeasurability and (19.9) holding for f bounded and borel measurable. the non-negativity\n(respectively, the boundedness) of f follows from (19.9).\n\nthe equations (19.6) are known as the chapman\u2013kolmogorov equations. they can be\n\nrephrased in terms of equality of measures: for each x\n\nps+t (x, dz) =\n\npt (y, dz)ps(x, dy).\n\n(19.10)\n\nmultiplying (19.10) by a bounded borel measurable function f (z) and integrating gives\n\n(cid:3)\n(cid:3)\n\ny\u2208s\n\nps+t f (x) =\n\npt f (y)ps(x, dy).\n\nthe right-hand side is the same as ps(pt f )(x), so we have\nps+t f (x) = pspt f (x),\n\ni.e., the functions ps+t f and pspt f are the same. the equation (19.12) is known as the\nsemigroup property.\non s. we can then rephrase (19.12) simply as\n\nby lemma 19.3, pt is a linear operator on the space of bounded borel measurable functions\n\nps+t = pspt .\n\noperators satisfying (19.13) are called a semigroup, and are much studied in functional\nanalysis. we will show in chapter 36 how to construct the markov process corresponding to\na given semigroup pt. more about semigroups can also be found in chapter 37.\n\none more observation about semigroups: if we take expectations in (19.4), we obtain\n\n(cid:16)\npxs (xt \u2208 a)\n\n(cid:17)\n\n.\n\npx(xs+t \u2208 a) = e x\n\nthe left-hand side is ps+t1a(x) and the right-hand side is\n\ne x[pt1a(xs)] = pspt1a(x),\n(cid:3) \u221e\n\nand so (19.4) encodes the semigroup property.\n\nthe resolvent or \u03bb-potential of a semigroup pt is defined by\n\u03bb \u2265 0,\n\n\u2212\u03bbtpt f (x) dt,\ne\n\nr\u03bb f (x) =\n\nx \u2208 s.\n\nthis can be recognized as the laplace transform of pt. by lemma 19.3 and the fubini\ntheorem, we see that\n\n0\n\n(cid:3) \u221e\n\nr\u03bb f (x) = e x\n\n\u2212\u03bbt f (xt ) dt.\ne\n\nresolvents are useful because they are typically easier to work with than semigroups.\n\nwhen practitioners of stochastic calculus tire of a martingale, they \u201cstop\u201d it. markov\nprocess theorists are a harsher lot and they \u201ckill\u201d their processes. to be precise, attach an\n\n0\n\n(19.11)\n\n(19.12)\n\n(19.13)\n\n "}, {"Page_number": 174, "text": "156\n\nframework for markov processes\n\nisolated point \u0001 to s. thus one looks at (cid:2)s = s \u222a \u0001, and the topology on (cid:2)s is the one\nare extended to(cid:2)s by defining them to be 0 at \u0001. at some random time \u03b6 the markov process\n\ngenerated by the open sets of s and {\u0001}. \u0001 is called the cemetery point. all functions on s\nis killed, which means that xt = \u0001 for all t \u2265 \u03b6 . the time \u03b6 is called the lifetime of the\nmarkov process.\n\n19.4 an example\n\nlet us give an example, that of brownian motion, of course. let xt and px be defined by\n(19.2) and (19.3). define ft = \u03c3 (xr; r \u2264 t ). clearly definition 19.1(1) holds. observe that\nsince, under p, wt is a mean zero normal random variable with variance t,\n\npx(xt \u2208 a) = p(w x\n= 1\u221a\n2\u03c0t\n\nt\n\na\n\n(cid:3)\n\u2208 a) = p(x + wt \u2208 a)\n\n\u2212(y\u2212x)2/2t dy.\ne\n\n(19.14)\n\nby dominated convergence, x \u2192 px(xt \u2208 a) is continuous, therefore measurable. this\nproves definition 19.1(2). it remains to prove definition 19.1(3), which is the following\nproposition.\n\nproposition 19.4 let w be a brownian motion as defined by definition 2.1, let w x\nt\nand let (xt , px) be defined by (19.2) and (19.3). if f is bounded and borel measurable,\n\ne x[ f (xt+s) | fs] = e xs f (xt ),\n\npx-a.s.\n\n(19.15)\n\n= x+wt,\n\nproof we will first prove\n\ne x[ f (xt+s) | fs] = e xs f (xt )\n\n(19.16)\nwhen f (x) = eiux. using independent increments and the fact that wt+s \u2212 ws has the same\nlaw as wt, we see that under each px, xt+s \u2212 xs is independent of fs and has the same law as\na mean zero normal random variable with variance t. we conclude that\n\nsee (a.25). we then write\n\n(cid:16)\n\ne x\n\non the other hand, for any y,\n\n\u2212u2t/2;\n\ne xeiu(xt+s\u2212xs ) = e\n(cid:16)\n(cid:17)\n(cid:16)\n\neiuxt+s|fs\n\n(cid:17)\neiu(xt+s\u2212xs )|fs\neiu(xt+s\u2212xs )\neiuxs\n\neiuxs\n\n(cid:17)\n\n= e x\n= e x\n= e\n\n\u2212u2t/2eiuxs .\n\ne yeiuxt = e eiuw y\n\nt = e eiuwt eiuy = e\n\n\u2212u2t/2eiuy.\n\nreplacing y by xs proves (19.16) for this f .\n\nwith compact support and let(cid:2)f be the fourier transform of f .\nin (19.16) we replace u by \u2212u, multiply both sides by(cid:2)f (u), and integrate over u \u2208 r. using\n\nnow suppose that f \u2208 c\n\n\u221e\n\n "}, {"Page_number": 175, "text": "the fourier inversion formula, we then have\n\n19.4 an example\n\n(cid:16)(cid:3)\n(cid:3)\n\n\u2212iuxt+s(cid:2)f (u) du | fs\n\u2212iuxt(cid:2)f (u) du\n\ne\n\ne\n\ne x[ f (xt+s) | fs] = (2\u03c0 )\u22121e x\n= (2\u03c0 )\u22121e xs\n= e xs f (xt ).\n\n157\n\n(cid:17)\n\nwith compact support implies (cid:2)f is in the schwartz class; see\n\nwe used the fubini theorem several times to interchange expectation and integration; this\nis justified because f in c\nsection b.2. this proves the proposition for f\nin c\nargument gives it for all bounded and measurable f .\n\nwith compact support, and a limit\n\n\u221e\n\n\u221e\n\nthe same proof works for d-dimensional brownian motion.\n\nset\n\n(cid:3)\n\na\n\n\u2212(y\u2212x)2/2t dy.\ne\n\npt (x, a) = px(xt \u2208 a) = p(wt + x \u2208 a) = 1\u221a\n2\u03c0t\n\n(19.17)\nclearly for each x and t, pt (x,\u00b7) is a measure with total mass 1. as we mentioned earlier, the\nfunction x \u2192 pt (x, a) is continuous, hence borel measurable. we will show the chapman\u2013\nkolmogorov equations. these follow from the next proposition.\nproposition 19.5 if s, t > 0 and x, z \u2208 r, then\n1\u221a\n\u2212(y\u2212x)2/2t\ne\n2\u03c0t\n1\u221a\n2\u03c0 (s + t )\n\n\u2212(z\u2212x)2/2(s+t ).\ne\n\n\u2212(z\u2212y)2/2s dy\ne\n\n1\u221a\n2\u03c0s\n\ny\u2208r\n=\n\n(19.18)\n\n(cid:3)\n\nproof this is a well-known property of the gaussian density, but we can derive (19.18)\nfrom proposition 19.4. let f be continuous with compact support. taking expectations in\n(19.15),\n\ne x f (xt+s) = e x[e xs f (xt )],\n\nor\n\npt+s f (x) = pspt f (x).\n\n(cid:3)\n\nusing lemma 19.3 and (19.17),\n(cid:3)\n1\u221a\n\u2212(z\u2212x)2/2(s+t ) dx\n2\u03c0 (s + t )\ne\n1\u221a\n1\u221a\nf (x)\n2\u03c0s\n2\u03c0t\n\nf (x)\n=\n\n\u2212(y\u2212x)2/2t\ne\n\n(cid:3)\n\n\u2212(z\u2212y)2/2s dy dx.\ne\n\nsince this holds for all continuous f with compact support, (19.18) holds for almost every\nx. since both sides of (19.18) are continuous in x, then (19.18) holds for all x.\n\n "}, {"Page_number": 176, "text": "158\n\nframework for markov processes\n\n19.5 the canonical process and shift operators\n\nsuppose we have a markov process (xt , px) where ft = \u03c3 (xs; s \u2264 t ). suppose for the\nmoment that xt has continuous paths. for this to even make sense, it is necessary that the\nset {t \u2192 xt is not continuous} to be in f, and then we require this event to be px-null for\n\neach x. define(cid:14)\u0001 to be the set of continuous functions on [0,\u221e). if(cid:14)\u03c9 \u2208(cid:14)\u0001, set(cid:14)xt =(cid:14)\u03c9(t ).\n(cid:14)ft. finally define(cid:14)px on ((cid:14)\u0001,(cid:14)f\u221e) by(cid:14)px((cid:14)x \u2208 \u00b7) =\ndefine (cid:14)ft = \u03c3 ((cid:14)xs; s \u2264 t ) and (cid:14)f\u221e = \u2228t\u22650\npx(x \u2208 \u00b7). thus(cid:14)px is specified uniquely by\n\u2208 a1, . . . ,(cid:14)xtn\n(or gain) by looking at the markov process ((cid:14)xt ,(cid:14)px), which is called the canonical process.\n\nfor n \u2265 1, a1, . . . , an borel subsets of s, and t1 < \u00b7\u00b7\u00b7 < tn. clearly there is so far no loss\n\n\u2208 an) = px(xt1\n\n\u2208 a1, . . . , xtn\n\n(cid:14)px((cid:14)xt1\n\n\u2208 an)\n\nlet us now suppose we are working with the canonical process, and we drop the tildes\neverywhere. we define the shift operators \u03b8t : \u0001 \u2192 \u0001 as follows. \u03b8t (\u03c9) will be an element\nof \u0001 and therefore is a continuous function from [0,\u221e) to s. define\n\n\u03b8t (\u03c9)(s) = \u03c9(t + s).\n\nthen\n\nxs \u25e6 \u03b8t (\u03c9) = xs(\u03b8t (\u03c9)) = \u03b8t (\u03c9)(s) = \u03c9(t + s) = xt+s(\u03c9).\n\nthe shift operator \u03b8t takes the path of x and chops off and discards the part of the path before\ntime t.\n\nwe will use expressions like f (xs) \u25e6 \u03b8t. if we apply this to \u03c9 \u2208 \u0001, then\n\n( f (xs) \u25e6 \u03b8t )(\u03c9) = f (xs(\u03b8t (\u03c9))) = f (xs+t (\u03c9)),\n\nor f (xs) \u25e6 \u03b8t = f (xs+t ).\n\nwe can follow exactly the above procedure, except we start with(cid:14)\u0001 being the collection of\n\nif the paths of x are not continuous, but instead only right continuous with left limits,\n\nfunctions from [0,\u221e) to s that are right continuous with left limits.\n\neven if we are not in this canonical setup, from now on we will suppose there exist shift\n\noperators mapping \u0001 into itself so that\n\nxs \u25e6 \u03b8t = xs+t .\n\nexercises\n\nand at =(cid:15)\n\n19.1 suppose (xt , px ) is a brownian motion and st = sups\u2264t xs. show that ((xt , st ), px ) is a markov\n\nprocess and determine the transition probabilities.\n\n19.2 suppose (xt , px ) is a brownian motion, f a non-negative, bounded, borel measurable function,\n\nt\n0 f (xs ) ds. show that ((xt , at ), px ) is a markov process.\n\n19.3 suppose pt is a poisson process with parameter \u03bb. let \u0001(cid:3)\n\nwhich are right continuous and which have left limits, let f be the \u03c3 -field on \u0001(cid:3)\nthe cylindrical subsets of \u0001(cid:3)\nis a markov process and determine the transition probabilities.\n\nbe the collection of functions on [0,\u221e)\ngenerated by\n= x + pt, and let px be the law of x + p. show that (xt , px )\n\n, let px\nt\n\n "}, {"Page_number": 177, "text": "159\n19.4 suppose m is a measure on the borel subsets b of a metric space s. suppose for each t > 0 there\npt (x, y) m(dy) = 1\n\nexist jointly measurable non-negative functions pt : s \u00d7s \u2192 r such that\nfor each x and t and define\n\nnotes\n\n(cid:15)\n\n(cid:3)\n\nshow that the kernels pt satisfy the chapman\u2013kolmogorov equations if and only if\n\n(cid:3)\n\npt (x, a) =\n\npt (x, y) m(dy).\n\na\n\nps(x, y)pt (y, z) m(dy) = ps+t (x, z)\n\nfor every s, t \u2265 0, every x \u2208 s, and m-almost every z.\n\n19.5 the ornstein\u2013uhlenbeck process y started at x is a continuous gaussian process with e yt =\n\n\u2212t/2x and covariance\n\ne\n\ncov (ys, yt ) = e\n\n\u2212(s+t )/2(es\u2227t \u2212 1).\n\nif x is the canonical process and px is the law of an ornstein\u2013uhlenbeck process started at x,\nshow that (xt , px ) is a markov process and determine the transition probabilities.\n\nfor more, see blumenthal and getoor (1968).\n\nnotes\n\n "}, {"Page_number": 178, "text": "20\n\nmarkov properties\n\nwe want to accomplish three things in this chapter. first, we want to talk about what it means\nin the markov process context for a filtration to satisfy the usual conditions. this is now\nmore complicated than in chapter 1 because we have more than one probability measure.\nsecond, we want to extend the markov property to expressions that are more complicated\nthan e x[ f (xs+t ) | f s]. third, we want to look at the strong markov property, which means\nwe look at expressions like e x[ f (xt+t ) | ft ], where t is a stopping time.\nthroughout this chapter we assume that x has paths that are right continuous with left\n\nlimits. to be more precise, if\n\nn = {\u03c9 : the function t \u2192 xt (\u03c9) is not right continuous with left limits},\n\nthen we assume n \u2208 f and n is px-null for every x \u2208 s.\n\n20.1 enlarging the filtration\n\nlet us first introduce some notation. define\n\nf 00\n\n= \u03c3 (xs; s \u2264 t ),\n\nt \u2265 0.\n\nt\n\n(20.1)\nthis is the smallest \u03c3 -field with respect to which each xs is measurable for s \u2264 t. we let\nf 0\nt be the completion of f 00\n, but we need to be careful what we mean by completion here,\nbecause we have more than one probability measure present. let n be the collection of sets\nthat are px-null for every x \u2208 s. thus n \u2208 n if (px)\u2217(n ) = 0 for each x \u2208 s, where (px)\u2217\nis the outer probability corresponding to px. the outer probability (px)\u2217\n\nis defined by\n\nt\n\n(px)\u2217(s) = inf{px(b) : a \u2282 b, b \u2208 f}.\n\nlet\n\nfinally, let\n\nf 0\n\nt\n\n= \u03c3 (f 00\n\nt\n\n\u222a n ).\n\n(20.2)\n\nft = f 0\n\nt+ = \u2229\u03b5>0f 0\nt+\u03b5.\n\n(20.3)\nwe call {ft} the minimal augmented filtration generated by x . ultimately, we will work\nonly with {ft}, but we need the other two filtrations at intermediate stages. the reason for\nworrying about which filtrations to use is that {f 00\n} is too small to include many interesting\nsets (such as those arising in the law of the iterated logarithm, for example), while if the\nfiltration is too large, the markov property will not hold for that filtration.\n\nt\n\n160\n\n "}, {"Page_number": 179, "text": "20.1 enlarging the filtration\n\n161\n\nthe filtration matters when defining a markov process; see definition 19.1(3). we will\nassume throughout this section that (xt , px) is a markov process with respect to the filtration\n{f 00\n}, that is,\n\nt\n\npx(xs+t \u2208 a | f 00\n\n) = pxs (xt \u2208 a),\n\ns\n\npx-a.s.\n\n(20.4)\n\nwhenever a is a borel subset of s and s, t \u2265 0.\n\nwe will also make the following assumption, which will be needed here and also in\n\nsection 20.3.\nassumption 20.1 suppose pt f is continuous on s whenever f is bounded and continuous\non s.\n\nmarkov processes satisfying assumption 20.1 are called feller processes or weak feller\nis bounded and borel measurable, then the\n\nis continuous whenever f\n\nprocesses. if pt f\nmarkov process is said to be a strong feller process.\nin (20.4) by f 0\nt .\n\nwe show that we can replace f 00\n\nt\n\nproposition 20.2 let (xt , px) be a markov process and suppose that (20.4) holds. if a is a\nborel subset of s, x \u2208 s, and s, t \u2265 0, then\n\npx(xs+t \u2208 a | f 0\n\n) = pxs (xt \u2208 a),\n\npx-a.s.\nproof since the right-hand side is a function of xs and hence f 0\n(cid:17)\nshow that if b \u2208 f 0\n\ns , then\n\ns\n\n(cid:16)\npxs (xt \u2208 a); b\n\npx(xs+t \u2208 a, b) = e x\n\n(20.5)\n\ns measurable, we need to\n\n.\n\n(20.6)\n\ns by (20.4). it holds for sets b \u2208 n , the class of null sets, since both\nthis holds for b \u2208 f 00\nsides are 0. therefore it holds for sets b such that there exists b1 \u2208 f 00\ns with b\u0001b1 being\na null set. by linearity it holds for finite disjoint unions of sets of the form just described.\nthe class of such finite disjoint unions is a monotone class that generates f 0\ns , and our result\nfollows by the monotone class theorem, theorem b.2.\n\nthe next step is to go from f 0\n\ns to fs.\n\nproposition 20.3 let (xt , px) be a markov process and suppose that (20.4) holds. if as-\nsumption 20.1 holds and f is a bounded borel measurable function, then\n\ne x[ f (xs+t ) | fs] = e xs f (xt ),\nit will turn out (see proposition 20.7 below) that f 0\n\npx-a.s.\n\n(20.7)\n\ns is equal to fs, but we do not know\n\nthis yet.\n\nproof we start with (20.5). by linearity, we have\n\ne x[ f (xs+t ) | f 0\n\ns ] = e xs f (xt ),\n\npx-a.s.,\n\n(20.8)\n\nwhen f is a simple random variable, then by monotone convergence when f is non-negative,\nand then by linearity again, when f is bounded and borel measurable. in particular, we have\nthis when f is bounded and continuous.\n\n "}, {"Page_number": 180, "text": "162\n\nmarkov properties\n\nif b \u2208 fs = f 0\ns + \u03b5, if f is bounded and continuous,\n\ns+, then b \u2208 f 0\n\ns+\u03b5 for every \u03b5 > 0. hence by (20.8) with s replaced by\n\ne x[ f (xs+t+\u03b5 ); b] = e x\n\n.\n\n(20.9)\n\n(cid:16)\ne xs+\u03b5 f (xt ); b\n\n(cid:17)\n\nthe right-hand side is equal to\n\nsince pt f\nconverges to\n\nis continuous and xt has paths that are right continuous with left limits, this\n\ne x[pt f (xs+\u03b5 ); b];\n(cid:16)\ne xs f (xt ); b\n\n(cid:17)\n\ne x[pt f (xs); b] = e x\n\nby dominated convergence. the left-hand side of (20.9) converges, using dominated conver-\ngence, the continuity of f , and the fact that x has paths that are right continuous with left\nlimits, to\n\nwe therefore have\n\ne x[ f (xs+t ); b].\n(cid:16)\n\ne x[ f (xs+t ); b] = e x\n\ne xs f (xt ); b\n\n(cid:17)\n\n.\n\n(20.10)\n\na limit argument shows this holds whenever f is bounded and measurable. since b is an\narbitrary event in fs, that completes the proof.\nremark 20.4 in chapter 16, we discussed the fact that the first time a right continuous\nprocess whose jump times are totally inaccessible hits a borel set is a stopping time, provided\nthe filtration satisfies the usual conditions. even though the notion of completion of a filtration\nis a bit different in the context of markov processes, the result is still true. see blumenthal\nand getoor (1968).\n\n20.2 the markov property\n\nwe start with the markov property given by proposition 20.3:\n\ne x[ f (xs+t ) | fs] = e xs[ f (xt )],\n\npx-a.s.\n\nsince f (xs+t ) = f (xt ) \u25e6 \u03b8s, if we write y for the random variable f (xt ), we have\n\ne x[y \u25e6 \u03b8s | fs] = e xsy,\n\npx-a.s.\n\n(20.11)\n\n(20.12)\n\nwe wish to generalize this to other random variables y .\n\nproposition 20.5 let (xt , px) be a markov process and suppose (20.11) holds. suppose\ni=1 fi(xti\u2212s), where the fi are bounded, borel measurable, and s \u2264 t1 \u2264 \u00b7\u00b7\u00b7 \u2264 tn. then\n\nn\n\ny = \n\n(20.12) holds.\n\n "}, {"Page_number": 181, "text": "20.2 the markov property\n\nlet v = \n\n163\nproof we will prove this by induction on n. the case n = 1 is( 20.11), so we suppose the\nequality holds for n and prove it for n + 1.\n(cid:17)\n\n) and h(y) = e yv . by the induction hypothesis,\n(cid:16) n+1(cid:31)\n(cid:17)\n\nn+1\nj=2 f j (xt j\u2212t1\n\ne x\n\nf j (xt j\n\nj=1\n\n)|fs\n\n= e x\n\n(cid:16)\n(cid:16)\ne x[v \u25e6 \u03b8t1\n\n)|fs\n\n(cid:17)\n|ft1] f1(xt1\n)|fs\n\n(e xt1v ) f1(xt1\n)|fs].\n\n= e x\n= e x[(h f1)(xt1\n(cid:16)\n\n\u2212sv ) f1(xt1\u2212s)]\n\n(cid:17)\n\ne y[v \u25e6 \u03b8t1\u2212s|ft1\u2212s] f1(xt1\u2212s)\n\n= e y\n= e y[(v \u25e6 \u03b8t1\u2212s) f1(xt1\u2212s)].\n\nby (20.11) this is e xs[(h f1)(xt1\u2212s)]. for any y,\ne y[(h f1)(xt1\u2212s)] = e y[(e xt1\n\nif we replace v by its definition, replace y by xs, and use the definition of \u03b8t1\u2212s, we get the\ndesired equality for n + 1 and hence the induction step.\nwe now come to the general version of the markov property. as usual, f\u221e = \u2228t\u22650ft.\nthe expression y \u25e6 \u03b8t for general y may seem puzzling at first. we will give some examples\nwhen we get to applications of the strong markov property in chapter 21.\n\ntheorem 20.6 let (xt , px) be a markov process and suppose (20.11) holds. suppose y is\nbounded and measurable with respect to f\u221e. then\ne x[y \u25e6 \u03b8s | fs] = e xsy,\nif in proposition 20.5 we take f j (x) = 1a j\n\n(x) for borel measurable a j, we have\n\npx-a.s.\n\n(20.13)\n\nproof\n\ne x[1b \u25e6 \u03b8s | fs] = e xs1b\n\n(20.14)\nwhen b = {\u03c9 : \u03c9(t1) \u2208 a1, . . . , \u03c9(tn) \u2208 an}. it is easy to see that the set of b\u2019s for which\n(20.14) holds is a monotone class. by an argument using the monotone class theorem, (20.14)\nholds for all b that are measurable with respect to f\u221e. taking linear combinations, (20.13)\nholds for y \u2019s that are simple random variables. using monotone convergence, (20.13) holds\nfor non-negative y \u2019s, and then by linearity for bounded y \u2019s.\nproposition 20.7 let (xt , px) be a markov process with respect to {ft}. let f 0\ndefined by (20.2) and (20.3). then ft = f 0\n\nt and ft be\n), where t1 < \u00b7\u00b7\u00b7 < tn \u2264 s and\n0 \u2264 u1 < \u00b7\u00b7\u00b7 < um and the f j and g j are bounded borel measurable functions. then by\nproposition 20.5,\n\nproof let y1 =  \n\n) and y2 =  \n\nfor each t \u2265 0.\nm\nj=1 g j (xu j\n\nn\ni=1 fi(xti\n\nt\n\ne x[(y1)(y2 \u25e6 \u03b8s) | fs] = y1e xsy2.\n\nsince e xsy2 is a function of xs, then (y1)(e xsy2) is f 0\ns measurable. using a monotone class\nargument, we conclude that if y is bounded and f\u221e measurable, then e x[y | fs] is f 0\ns\n\n "}, {"Page_number": 182, "text": "164\n\nmarkov properties\n\nmeasurable. now apply this to y = 1a for a \u2208 fs to obtain that 1a = e x[1a | fs] is f 0\nmeasurable.\n\ns\n\nthe following is known as the blumenthal 0\u20131 law.\nproposition 20.8 let (xt , px) be a markov process with respect to {ft}. if a \u2208 f0, then for\neach x, px(a) is equal to 0 or 1.\nproof suppose a \u2208 f0. under px, x0 = x, a.s., and then\n\npx(a) = e x01a = e x[1a \u25e6 \u03b80 | f0] = 1a \u25e6 \u03b80 = 1a \u2208 {0, 1},\n\npx-a.s.\n\nsince 1a \u25e6 \u03b80 is f0 measurable. our result follows because px(a) is a real number and not\nrandom.\n\ngiven a stopping time t , recall that the \u03c3 -field of events known up to time t is defined to be\n\nft =(cid:23)\n\n20.3 strong markov property\n\na \u2208 f\u221e : a \u2229 (t \u2264 t ) \u2208 ft for all t > 0\n\n.\n\n(cid:24)\n\nwe define \u03b8t by \u03b8t (\u03c9)(t ) = \u03c9(t (\u03c9) + t ). thus, for example, xt \u25e6 \u03b8t (\u03c9) = xt (\u03c9)+t (\u03c9) and\nxt (\u03c9) = xt (\u03c9)(\u03c9).\n\nnow we can state the strong markov property. the notation and definition are admittedly\n\na bit opaque at this stage \u2013 be patient until we reach the examples in the next chapter.\ntheorem 20.9 suppose (xt , px) is a markov process with respect to {ft}, that assumption\n20.1 holds, and that t is finite stopping time. if y is bounded and measurable with respect\nto f\u221e, then\n\ne x[y \u25e6 \u03b8t|ft ] = e xt y,\n\npx-a.s.\n\nproof following the proofs of section 20.2, it is enough to prove\n\ne x[ f (xt+t )|ft ] = e xt f (xt )\n\n(20.15)\n\nfor f bounded. we can obtain this by a limit argument if we have (20.15) for f bounded and\ncontinuous. define tn to be equal to (k + 1)/2n on the event (k/2n \u2264 t < (k + 1)/2n).\nif a \u2208 ft , then a \u2208 ftn. therefore a \u2229 (tn = k/2n) \u2208 fk/2n and we have by the markov\nproperty, theorem 20.6,\n\ne x[ f (xtn+t ); a, tn = k/2n] = e x[ f (xt+k/2n ); a, t = k/2n]\n= e x[e xk/2n f (xt ); a, tn = k/2n]\n= e x[e xtn f (xt ); a, tn = k/2n].\n\n "}, {"Page_number": 183, "text": "then\n\ne x[ f (xtn+t ); a] =\n\n20.3 strong markov property\n\n\u221e(cid:9)\n\u221e(cid:9)\n=\n= e x[e xtn f (xt ); a].\n\nk=1\n\nk=1\n\ne x[ f (xtn+t ); a, tn = k/2n]\n(cid:26)\n\ne xtn f (xt ); a, tn = k/2n\n\ne x\n\n165\n\n(cid:27)\n\nnow let n \u2192 \u221e. e x[ f (xtn+t ); a] \u2192 e x[ f (xt+t ); a)] by dominated convergence and\nthe continuity of f and the right continuity of xt. on the other hand, using the continuity of\npt f , e xtn f (xt ) = pt f (xtn\n\n) \u2192 pt f (xt ) = e xt f (xt ). therefore\ne x[ f (xt+t ); a] = e x[e xt f (xt ); a]\n\nfor all a \u2208 ft , and hence (20.15) holds.\nrecall that we are restricting our attention to markov processes whose paths are right\ncontinuous with left limits. if we have a markov process (xt , px) whose paths are right\ncontinuous with left limits, which has shift operators {\u03b8t}, and which satisfies the conclusion\nof theorem 20.9, whether or not assumption 20.1 holds, then we say that (xt , px) is a strong\n\u2192 xt ,\nmarkov process. a strong markov process is said to be quasi-left continuous if xtn\na.s., on {t < \u221e} whenever tn are stopping times increasing up to t . unlike in the definition\nof predictable stopping times given in chapter 16, we are not requiring the tn to be strictly less\nthan t . a hunt process is a strong markov process that is quasi-left continuous. quasi-left\ncontinuity does not imply left continuity; consider the poisson process.\n\nproposition 20.10 if (xt , px) is a strong markov process and assumption 20.1 holds, then\nxt is quasi-left continuous.\nproof first suppose t is bounded, tn increases to t , y = limn\u2192\u221e xtn, and f and g are\nbounded and continuous. if tn = t for some n, then limn\u2192\u221e g(xtn+t ) = g(xt+t ), and if\ntn < t for all n, then limn\u2192\u221e g(xtn+t ) = g(x(t+t )\u2212), where xs\u2212 is the left-hand limit at time\ns. in either case,\n\nn\u2192\u221e g(xtn+t ) = g(xt ).\n\nlim\n\nlim\nt\u21920\n\nthen\n\ne x[ f (y )g(xt )] = lim\nt\u21920\n= lim\nt\u21920\n= lim\nt\u21920\n\n)g(xtn+t )]\n\nn\u2192\u221e e x[ f (xtn\nlim\nn\u2192\u221e e x[ f (xtn\nlim\ne x[ f (y )ptg(y )] = e x[ f (y )g(y )].\n\n)ptg(xtn\n\n)]\n\nby a limit argument we have\n\n(20.16)\nfor all bounded measurable functions h on s \u00d7 s. now take h(x, y) to be zero if x = y and\none otherwise. the right-hand side of (20.16) is 0, so the left-hand side is also.\n\ne x[h(y, xt )] = e x[h(y, y )]\n\n "}, {"Page_number": 184, "text": "166\n\nmarkov properties\n\nif t is not bounded, apply the argument in the preceding paragraph to the stopping time\n\nt \u2227 m, where m is a positive real, and then let m \u2192 \u221e.\n\nexercises\n\n20.1 suppose that s is a locally compact separable metric space and c0 is the set of continuous\nfunctions on s that vanish at infinity. to say a continuous function f vanishes at infinity means\nthat given \u03b5 > 0 there exists a compact set k such that | f (x)| < \u03b5 if x /\u2208 k. show that if\nassumption 20.1 is replaced by the assumptions that pt f \u2208 c0 whenever f \u2208 c0 and pt f \u2192 f\nuniformly as t \u2192 0 whenever f \u2208 c0, then the conclusion of theorem 20.9 still holds.\n\n20.2 suppose (xt , px ) is a markov process with respect to a filtration {ft}. suppose that et \u2282 ft\nfor each t and that xt is et measurable for each t. show that (xt , px ) is a markov process with\nrespect to the filtration {et}.\n\n20.3 give an example of a markov process that is not a strong markov process.\n\nhint: let the state space be [0,\u221e) and starting from x \u2208 (0,\u221e), let x move deterministically\nat constant speed to the right. starting at 0, let x wait an exponential length of time, and then\nbegin moving at constant speed to the right.\n\n20.4 let (xt , px ) be brownian motion and let {ft} be the minimal augmented filtration. suppose\nb \u2208 \u2228t\u22650ft and for some s > 0 is of the form 1b = 1a \u25e6 \u03b8s. show that if b is a px-null set for\nsome x, then it is a px-null set for every x.\n\n20.5 let pt be transition probabilities for a poisson process with parameter \u03bb. these are defined in\n\nexercise 19.3. show that assumption 20.1 holds.\n\n20.6 suppose (xt , px ) is a markov process with transition probabilities pt, f\n\nis a bounded borel\nmeasurable function, t0 > 0, and we define mt = pt0\u2212t f (xt ) for t \u2264 t0. show that (mt , t \u2264 t0 )\nis a px-martingale for each x.\n\n20.7 use the blumenthal 0\u20131 law to show that if w is a one-dimensional brownian motion and\n\nt = inf{t > 0 : wt > 0} is the first time brownian motion hits (0,\u221e), then p(t = 0) = 1.\n\n20.8 let a be a borel subset of a metric space s. let ta = inf{t : xt \u2208 a}, where (xt , px ) is a strong\n\nmarkov process. show that px(ta = 0) is either 0 or 1 for each x.\n\n20.9 let (xt , px ) be a strong markov process and let a be a borel subset of s. we define ar by setting\nar = {x : px(ta = 0) = 1}, where ta is the first hitting time of a. thus ar is the set of points\nthat are regular for a. prove that for each x,\npx(xta\n\n\u2208 a \u222a ar ) = 1.\n\n "}, {"Page_number": 185, "text": "21\n\napplications of the markov properties\n\nat = (cid:15)\n\nwe give some applications of the markov property and the strong markov property. in the\nfirst application, we show that d-dimensional brownian motion is transient if d \u2265 3. next\nwe consider estimates on additive functionals. (an example of an additive functional is\nis a non-negative function on the state space of the markov\nprocess x .) third is a sufficient criterion for a markov process to have continuous paths.\nfinally, we discuss harmonic functions and show how to solve the classical dirichlet problem\nof analysis and partial differential equations.\n\nt\n0 f (xs) ds, where f\n\n21.1 recurrence and transience\n\nlet wt = (w1(t ), . . . , wd (t )) be a d-dimensional brownian motion started at 0 with d \u2265 3\n= x+wt be brownian motion started at x. let h(y) = |y|2\u2212d. a direct calculation\nand let w x\nt\nof derivatives shows that\n\n\u0001h(x) = d(cid:9)\n\ni=1\n\n(x) = 0,\n\n\u2202 2h\n\u2202x2\ni\n\nx (cid:16)= 0.\n\n)1/2 = yi|y|\nt equals 0 if i (cid:16)= j and we saw in section\n\n(noting that\n\nd\n\n(y2\n1\n\n\u2202\n\u2202yi\n\n|y| = \u2202\n\u2202yi\n\n+ \u00b7\u00b7\u00b7 + y2\nhelps with the calculation.) by exercise 9.4, (cid:22)wi, wj(cid:23)\n9.3 that it equals t if i = j. suppose r < |x| < r, and let\n| \u2264 r or |w x\n(cid:3)\n\ns is finite, a.s., because |w x\ntheorem 7.2. by it\u02c6o\u2019s formula,\n\nt\n\n| \u2265 r}.\nd(cid:9)\n\nt\u2227s\n\ns = inf{t : |w x\n| \u2265 |w1(t )| \u2212 |x| and w1(t ) exits [\u22122r, 2r] in finite time by\n\nt\n\nt\n\nh(w x\nt\u2227s\n\n) + martingale + 1\n\n) = h(w x\n= h(x) + martingale.\n\n0\n\n2\n\n\u2202 2h\n\u2202x2\ni\n\n(w x\ns\n\n) ds\n\n0\n\ni=1\n\ntherefore h(wt\u2227s ) \u2212 h(x) is a martingale started at 0. the function h is equal to r2\u2212d on\n\u2202b(0, r), the boundary of b(0, r), and equal to r2\u2212d on \u2202b(0, r), the boundary of b(0, r).\n\n167\n\n "}, {"Page_number": 186, "text": "168\n\napplications of the markov properties\n\nby corollary 3.17, we deduce\n\n) \u2212 h(x) hits r2\u2212d \u2212 |x|2\u2212d before r2\u2212d \u2212 |x|2\u2212d )\n\np(w x\n\nt hits b(0, r) before b(0, r))\n= p(h(w x\n= |x|2\u2212d \u2212 r2\u2212d\nr2\u2212d \u2212 r2\u2212d\n\n.\n\nt\n\n(cid:10)\nif we let r \u2192 \u221e and recall that 2 \u2212 d < 0, we see that\nt ever hits \u2202b(0, r)) =\n\np(w x\n\n(cid:11)d\u22122\n\n.\n\nr|x|\n\n(21.1)\n\nwe want to use the strong markov property to go from (21.1) to\n\n|w x\n\nt\n\n| = \u221e.\n\nlim\nt\u2192\u221e\n\n(there are other ways besides the strong markov property of showing this.) the first step in\ndoing this is to convert to the markov process notation. let (xt , px) be a brownian motion.\nwhat we have shown is that\n\npx(xt ever hits \u2202b(0, r)) =\n\n.\n\n(21.2)\n\n(cid:11)d\u22122\n\n(cid:10)\n\nr|x|\n\nlet m > 0 and let\n\ns1 = inf{t : |xt| \u2265 2m},\nt1 = inf{t > s1 : |xt| \u2264 m},\ns2 = inf{t > t1 : |xt| \u2265 2m},\nt2 = inf{t > s2 : |xt| \u2264 m},\n\nand so on. another way of writing this is to define\n\ns = inf{t > 0 : |xt| \u2265 2m},\nand then to let s1 = s, and for each i \u2265 1,\nti = si + t \u25e6 \u03b8si\n\n,\n\nt = inf{t > 0 : |xt| \u2264 m},\n\nsi+1 = ti + s \u25e6 \u03b8ti\n\n.\n\nlet us explain what is going on. given a path \u03c9, which is a continuous function from [0,\u221e)\nto rd, t \u25e6 \u03b8si means to proceed along the path until time si, disregard this piece, and then\nsee how long it takes after time si to first enter b(0, m ). if we add the quantity si to t \u25e6 \u03b8si,\nwe then get the amount of time for xt to first enter b(0, m ) after time si. thus ti with the\nshift notation is the same as inf{t > si : xt \u2208 b(0, m )}. the shift notation interpretation of\nsi+1 is similar.\nnow we can apply the strong markov property. since ti+1 = si+1 + t \u25e6 \u03b8si+1, we can write\n\npx(ti+1 < \u221e) = px(si+1 < \u221e, t \u25e6 \u03b8si+1\n\n(cid:16)\n< \u221e)\n(cid:16)\n< \u221e | fsi+1\npx(t \u25e6 \u03b8si+1\npxsi+1 (t < \u221e); si+1 < \u221e\n\n(cid:17)\n); si+1 < \u221e\n\n.\n\n(cid:17)\n\n= e x\n= e x\n\n "}, {"Page_number": 187, "text": "at time si+1, we have |xsi+1\n\n21.2 additive functionals\n\n| = 2m, and by (21.1)\n\npxsi+1 (t < \u221e) = ( 1\n\n2\n\n)d\u22122.\n\n169\n\ntherefore\n\npx(ti+1 < \u221e) \u2264 22\u2212dpx(si+1 < \u221e) \u2264 22\u2212dpx(ti < \u221e).\n\nthe last inequality is simply the fact that si+1 \u2265 ti. since px(t1 < \u221e) \u2264 1, induction tells\nus that\n\npx(ti < \u221e) \u2264 2(2\u2212d )(i\u22121) \u2192 0\n\nas i \u2192 \u221e. hence px(ti < \u221e for all i) = 0. since ti increases as i increases, for almost all\n\u03c9, ti will be infinite for i sufficiently large (how large will depend on \u03c9). hence xt returns to\nb(0, m ) for a last time, a.s. since m is arbitrary, this proves that xt tends to \u221e as t \u2192 \u221e.\n\nwe have thus proved\n\nproposition 21.1 if (xt , px) is a d-dimensional brownian motion and d \u2265 3, then |xt| \u2192 \u221e\nas t \u2192 \u221e with px-probability one for each x.\n\nlet d be a closed subset of s, let f : d \u2192 [0,\u221e), let s = \u03c4d, and let\n\n21.2 additive functionals\n\n(cid:3)\n\ns\n\na = sup\nx\u2208d\n\ne x\n\nf (xs) ds,\nwhere \u03c4d = inf{t > 0 : xt /\u2208 d} is the first time x exits d.\n(cid:11)\nproposition 21.2 if a < \u221e, then\n\n(cid:10)(cid:3)\n\n0\n\ns\n\nf (xs)ds \u2265 2ka\n\n\u2264 2\n\n\u2212k.\n\n(21.3)\n\npx\n\nsup\nx\u2208d\n\n0\n\nproof let bt = (cid:15)\n\nt\u2227s\n0\n\nthis is rather remarkable: as soon as one gets a bound on the expectation, although it\nmust be uniform in x, one gets exponential tails for the distribution. a use of chebyshev\u2019s\ninequality would only give the bound (2k)\u22121.\n\nf (xs) ds. this is a special case of what is known as an additive\nfunctional; see section 22.3. let u1 = inf{t : bt \u2265 2a}, and let ui+1 = ui + u1 \u25e6 \u03b8ui. to\nexplain this formula, composing \u03c9 with \u03b8ui means we disregard the path before time ui.\nthus u1 \u25e6 \u03b8ui is the length of time after time ui until bt has increased an amount 2a over its\nvalue at ui. therefore ui + u1 \u25e6 \u03b8ui is the (i + 1)st time b has increased by 2a. the event\npx(bs \u2265 2ka) is bounded by\n\npx(uk \u2264 s) = px(uk\u22121 \u2264 s, u1 \u25e6 \u03b8uk\u22121\n\n= e x\n= e x\n\npx(u1 \u25e6 \u03b8uk\u22121\n\u2264 s \u25e6 \u03b8uk\u22121\npxuk\u22121 (u1 \u2264 s); uk\u22121 \u2264 s\n\n.\n\n\u2264 s \u25e6 \u03b8uk\u22121\n(cid:27)\n|fuk\u22121\n\n)\n\n); uk\u22121 \u2264 s\n\n(cid:27)\n\n(cid:26)\n(cid:26)\n\n "}, {"Page_number": 188, "text": "applications of the markov properties\n\n170\n\nif uk\u22121 \u2264 s, then xuk\u22121\n\n\u2208 d. if y \u2208 d,\n\n(cid:10)(cid:3)\n\n(cid:15)\n\n(cid:11)\n\n\u2264 e y\n\npy(u1 \u2264 s) \u2264 py\n\ns\n\nf (xs)ds \u2265 2a\n\n0\n\ns\n0 f (xs)ds\n2a\n\n\u2264 1\n\n2\n\nby chebyshev\u2019s inequality. then\n\npx(uk \u2264 s) \u2264 1\n\n2\n\npx(uk\u22121 \u2264 s)\n\nand (21.3) follows by induction.\n\nwe give another proof of proposition 4.5.\n\nproposition 21.3 let w be a one-dimensional brownian motion. if t is a finite stopping\ntime and a < b, then\n\np(wt+t \u2208 [a, b] | ft ) \u2264 b \u2212 a\u221a\n\n,\n\na.s.\n\n2\u03c0t\n\nproof let (xt , px) be a one-dimensional brownian motion. if y \u2208 r, then\n\n(cid:3)\n\npy(xt \u2208 [a, b]) = p0(xt \u2208 [a \u2212 y, b \u2212 y])\n\n= 1\u221a\n2\u03c0t\n\na\u2212y\n\nb\u2212y\n\n\u2212z2/2t dz \u2264 b \u2212 a\u221a\n\ne\n\n2\u03c0t\n\n(21.4)\n\n.\n\nby the strong markov property,\n\np(wt+t \u2208 [a, b] | ft ) = p0(xt+t \u2208 [a, b] | ft ) = e 0[1[a,b](xt ) \u25e6 \u03b8t | ft ]\n\n= e xt [1[a,b](xt )] = pxt (xt \u2208 [a, b]).\n\nnow use (21.4) with y replaced by xt .\n\n21.3 continuity\n\nlet us now come up with a criterion for a markov process to have continuous paths. we\nassume we have a strong markov process (xt , px) whose paths are right continuous with left\nlimits. let d (\u00b7,\u00b7) be the metric for the state space s.\nlemma 21.4 let (xt , px) be a strong markov process with state space s. for all x \u2208 s and\nall \u03bb \u2265 0,\n\npx(sup\ns\u2264t\n\nd (xs, x) \u2265 \u03bb) \u2264 2 sup\ns\u2264t\n\nsup\ny\u2208s\n\npy(d (xs, x0) \u2265 \u03bb/2).\n\nnote that the left-hand side has the supremum inside while the right-hand side has the\n\nsuprema outside the probability.\n\nproof let us use the notation\n\nf (t, \u03bb) = sup\ns\u2264t\n\nsup\ny\u2208s\n\npy(d (xs, x0) \u2265 \u03bb).\n\n(21.5)\n\n "}, {"Page_number": 189, "text": "21.4 harmonic functions\n\n171\n\nwrite s = inf{t : d (xt , x0) \u2265 \u03bb}. then by the strong markov property,\n\nd (xs, x) \u2265 \u03bb) \u2264 px(d (xt , x) \u2265 \u03bb/2) + px(s < t, d (xt , x0) \u2264 \u03bb/2)\n\npx(sup\ns\u2264t\n\n(cid:16)\npxs (d (xt\u2212s, x0) \u2265 \u03bb/2)\n\n(cid:17)\n\n\u2264 f (t, \u03bb/2) + e x\n\u2264 2f (t, \u03bb/2);\n\n(21.6)\n\nsee exercise 21.2.\n\nproposition 21.5 let (xt , px) be a strong markov process. with f (t, \u03bb) defined as in (21.5),\nsuppose\n\nf (t, \u03bb)\n\n\u2192 0\n\n(21.7)\nas t \u2192 0 for each \u03bb > 0. then xt has continuous paths with px-probability one for each x.\nfor x a brownian motion, f (t, \u03bb) \u2264 2e\n\u2212\u03bb2/8t by proposition 3.15, and hence f (t, \u03bb)/t \u2192 0\nas t \u2192 0. thus brownian motion satisfies (21.7). on the other hand, (21.7) is not satisfied\nfor the poisson process; see exercise 21.3.\n\nt\n\nproof suppose \u03bb, t0 > 0 and x has a jump of size larger than 4\u03bb at some time before t0\nwith positive probability, that is,\n\nwhere xt\u2212 = lims\u2191t,s<t xs. then for each n there exists k \u2264 [t02n] + 1 such that\n\n[x] is the largest integer less than or equal to x. therefore there exists k \u2264 [t02n]+ 1 such that\n\nd (xt\u2212, xt ) \u2265 4\u03bb) > 0,\n\npx(sup\nt\u2264t0\n\nsup\n\ns,t\u2208[k/2n,(k+1)/2n]\n\nd (xs, xt ) \u2265 4\u03bb;\n\nsup\n\ns\u2208[k/2n,(k+1)/2n]\n\nd (xs, xk/2n ) \u2265 2\u03bb.\n\nbut by lemma 21.4\n\npx(\u2203k \u2264 [t02n] + 1 :\n\nsup\n\n\u2264 ([t02n] + 1) sup\n\u2264 2([t02n] + 1)f (2\n\nk/2n\u2264s\u2264(k+1)/2n\npy( sup\ns\u22642\u2212n\n\u2212n, \u03bb)\n\ny\n\nd (xs, xk/2n ) \u2265 2\u03bb)\nd (xs, x0) \u2265 2\u03bb)\n\nfor every n. in the first inequality we used the markov property at time k/2n and the fact\nthat there are at most [t02n] + 1 intervals. letting n \u2192 \u221e, we see the probability of a jump\nof size larger than 4\u03bb before time t0 must be zero. since \u03bb and t0 are arbitrary, the paths of\nx are continuous.\n\n21.4 harmonic functions\n\nsuppose (xt , px) is a continuous markov process satisfying the strong markov property, and\nfor each x, the sets of paths are right continuous with left limits with px-probability one. let\n\n "}, {"Page_number": 190, "text": "172\n\napplications of the markov properties\n\nd be an open subset of s, and suppose that \u03c4d < \u221e, a.s., with respect to each px, where\n\u03c4d = inf{t : xt /\u2208 d} is the time of the first exit from d. let f be a bounded measurable\nfunction on \u2202d, the boundary of d.\n\nproposition 21.6 define\n\nh(x) = e x f (x\u03c4d\n\n)\n\n= fs\u2227\u03c4d. then for each x, h(xt\u2227\u03c4d\n\nand f(cid:3)\nfiltration {f(cid:3)\n\ns\n\n}.\n\nt\n\n) is a martingale under px with respect to the\n\nproof let s < t. consider a path \u03c9 starting at x and continuing until it exits d at time\n\u03c4d(\u03c9). if we have u \u2264 \u03c4d and we cut off the first u time units of the path, we have a path going\nfrom xu(\u03c9) and proceeding until it exits d. but note that the point at which it exits will not\n\u25e6 \u03b8u = x\u03c4d\nbe changed by cutting off a piece from the beginning of the path. therefore x\u03c4d\nif u \u2264 \u03c4d. using this,\n\ne x[h(xt\u2227\u03c4d\n\n) | fs\u2227\u03c4d] = e x\n\n(cid:17)\n| ft\u2227\u03c4d] | fs\u2227\u03c4d]\n\n(cid:16)\n\ne xt\u2227\u03c4d f (x\u03c4d\n= e x[e x[ f (x\u03c4d\n= e x[ f (x\u03c4d\n= e x[ f (x\u03c4d\n= e xs\u2227\u03c4d f (x\u03c4d\n\n) | fs\u2227\u03c4d\n) \u25e6 \u03b8t\u2227\u03c4d\n) | fs\u2227\u03c4d]\n) \u25e6 \u03b8s\u2227\u03c4d\n) = h(xs\u2227\u03c4d\n\n| fs\u2227\u03c4d]\n\n),\n\nas required.\n\nthis becomes particularly interesting in the case when xt is a d-dimensional brownian\nmotion. suppose d is a bounded domain (i.e., a bounded open subset) in rd. there exists\nm such that d \u2282 b(0, m ). we know x 1\nt , the first component of xt is a one-dimensional\nt will exit [\u2212m, m] in finite time, no matter\nbrownian motion, and by theorem 7.2, x 1\nwhat x 1\n0 is. therefore the time for xt to exit d will be finite almost surely with respect to\neach px. take x \u2208 d and take \u03b4 smaller than the distance from x to the boundary of d.\nif s = inf{t : |xt \u2212 x| \u2265 \u03b4}, the first time x leaves the ball of radius \u03b4 about x, then by\nproposition 21.6 and optional stopping, we have\n\nh(x) = e xh(xs ).\n\n(21.8)\n\nby exercise 2.3 we know that d-dimensional brownian motion is rotationally invariant. we\nconclude from this that the location where a brownian motion hits the boundary of a ball of\nradius \u03b4 about the starting point must have a uniform distribution. hence xs will be uniformly\ndistributed on \u2202b(x, \u03b4). thus (21.8) can be rewritten as\n\n(cid:3)\n\nh(x) =\n\nh(y) \u03c3x,\u03b4 (dy),\n\n\u2202b(x,\u03b4)\n\nwhere \u03c3x,\u03b4 is a surface measure on \u2202b(x, \u03b4) normalized to have total mass one. this holds\nfor every \u03b4 small enough, and since h is bounded (because f is), it can be shown that h is c2\n\n "}, {"Page_number": 191, "text": "in d and is harmonic there:\n\n21.4 harmonic functions\n\n173\n\n\u0001h(x) = d(cid:9)\n\n(x) = 0;\n\n\u2202 2h\n\u2202x2\ni\n\ni=1\n\nthe proof is not obvious \u2013 see bass (1995), section ii.1.\n\nwe can use proposition 21.6 to give a solution to the dirichlet problem. in the dirichlet\nproblem one is given a domain in rd and a continuous function f on the boundary of d.\none wants to find a continuous function h that is harmonic inside d, that is, \u0001h(x) = 0 for\nx \u2208 d, and that agrees with f on \u2202d. there are domains for which one cannot solve the\ndirichlet problem, but a solution can be found provided the domain is moderately nice. we\nexplain how to solve the dirichlet problem probabilistically; the class of domains where one\ncan do this is the same as the class where one can solve the dirichlet problem analytically.\nlet us say that a point x is regular for a borel subset a if px(ta = 0) = 1, where\nta = inf{t > 0 : xt \u2208 a}. thus a point x is regular for a set a if starting at x the brownian\nmotion enters a immediately. for example, a consequence of theorem 7.2 is that the point\n0 is regular for the set a = (0,\u221e) when we have a one-dimensional brownian motion.\ntheorem 21.7 suppose d is a bounded open domain in rd and f is a function on \u2202d that\nis continuous on \u2202d. let (xt , px) be a d-dimensional brownian motion and \u03c4d = inf{t :\nxt \u2208 dc}. if each point of \u2202d is regular for dc, then h(x) = e x f (x\u03c4d\n) is a solution to the\ndirichlet problem.\n\nthe regularity condition says that starting at any point of \u2202d, brownian motion enters\ndc immediately. uniqueness of the solution to the dirichlet problem is easy, and we do not\naddress this here.\n\nproof we have already seen in proposition 21.6 and the remarks immediately following\nthe proof of that proposition that h is harmonic in d. this implies that h is continuous in d.\nthus we only need to show that h agrees with f on \u2202d.\nour first step is to fix t and \u03b5 and to show that the set\n\n{x : px(\u03c4d \u2264 t ) > 1 \u2212 \u03b5}\n\nis an open set. let s < t, define \u03d5s(x) = px(\u03c4d \u2264 t \u2212 s), and let\n\nws(x) = px(xu \u2208 dc for some u \u2208 [s, t]).\n\nby the markov property at time s,\n\nws(x) = e xpxs (xu \u2208 dc for some u \u2208 [0, t \u2212 s]) = e x[pxs (\u03c4d \u2264 t \u2212 s)]\n\n= e x\u03d5s(xs) = (2\u03c0s)\u2212d/2\n\n\u03d5s(y)e\n\n\u2212|x\u2212y|2/2s dy.\n\n(cid:3)\n\nby dominated convergence, the last integral is a continuous function of x. if\n\nw0(x) = px(xu \u2208 dc for some u \u2208 [0, t]),\n\nthen ws(x) \u2191 w0(x), so {x : w0(x) > 1 \u2212 \u03b5} = \u222as\u2208(0,t ){x : ws(x) > 1 \u2212 \u03b5} is open.\nlet z \u2208 \u2202d. let \u03b5 > 0 and choose \u03b7 such that | f (w) \u2212 f (z)| < \u03b5 if |w \u2212 z| < \u03b7 and\nw \u2208 \u2202d. pick t small so that p0(sups\u2264t\n|xs| > \u03b7/2) < \u03b5; this is possible because brownian\n\n "}, {"Page_number": 192, "text": "applications of the markov properties\n\n174\nmotion has continuous paths. because z \u2208 \u2202d and every point of \u2202d is regular for dc,\npz(\u03c4d \u2264 t ) = 1. finally choose \u03b4 < (\u03b7/2) \u2227 \u03b5 so that if |w \u2212 z| < \u03b4 and w \u2208 d, then\npw(\u03c4d \u2264 t ) > 1 \u2212 \u03b5.\n\nnow if |w \u2212 z| < \u03b4 and w \u2208 d, then\n\npw(|x\u03c4d\n\n\u2212 z| < \u03b7) \u2265 pw(\u03c4d \u2264 t, sup\ns\u2264t\n\n|xs \u2212 w| \u2264 \u03b7/2)\n|xs| > \u03b7/2)\n\n\u2265 pw(\u03c4d \u2264 t ) \u2212 p0(sup\ns\u2264t\n\u2265 (1 \u2212 \u03b5) \u2212 \u03b5.\n\nthe set \u2202d is a bounded and closed subset of rd, hence compact, and since f is continuous\non \u2202d, there exists m such that | f | is bounded by m. if |w \u2212 z| < \u03b4 and w \u2208 d,\n\n|h(w) \u2212 f (z)| = |e w f (x\u03c4d\n\u2264 |e w[ f (x\u03c4d\n+ 2mpw(|x\u03c4d\n\u2264 \u03b5pw(|x\u03c4d\n\n) \u2212 f (z)|\n\u2212 z| < \u03b7] \u2212 f (z)pw(|x\u03c4d\n);|x\u03c4d\n\u2212 z| < \u03b7) + 4m\u03b5 \u2264 (1 + 4m )\u03b5.\n\n\u2212 z| \u2265 \u03b7)\n\n\u2212 z| < \u03b7)|\n\n) \u2212 f (z)| < \u03b5 if |x\u03c4d\n\nwe used the fact that | f (x\u03c4d\nthat h(w) \u2192 f (z) as w \u2192 z inside d.\n\nlet us give a sufficient condition for a point to be regular for a domain d. let(cid:14)va =\n}. the vertex of(cid:14)va is the origin. a cone v in\nrd is a translation and rotation of(cid:14)va for some a.\n\n{(x1, . . . , xd ) : x1 > 0, (x2\n\n\u2212 z| < \u03b7. since \u03b5 is arbitrary, this proves\n\n+ \u00b7\u00b7\u00b7 + x2\n\n) < a2x2\n1\n\n2\n\nd\n\nthe following is known as the poincar\u00b4e cone condition.\n\nproposition 21.8 suppose there exists a cone v with vertex y \u2208 \u2202d such that v \u2229 b(y, r) \u2282\ndc for some r > 0. then y is regular for dc.\n\nproof by translation and rotation of the coordinates, we may suppose y = 0 and v =(cid:14)va\n\nfor some a. then for each t,\n\np0(\u03c4d \u2264 t ) \u2265 p0(xt \u2208 dc) \u2265 p0(xt \u2208 v \u2229 b(0, r))\n\n\u2265 p0(xt \u2208 v ) \u2212 p0(xt /\u2208 b(0, r)).\n\nby scaling, the last term is p0(x1 \u2208 v ) \u2212 p0(x1 /\u2208 b(0, r/\n\n\u221a\nt )), which converges to\n\n(cid:3)\n\np0(x1 \u2208 v ) = (2\u03c0 )\u2212d/2\n\n\u2212|z|2/2 dz > 0\ne\n\nas t \u2192 0. observe p0(\u03c4d \u2264 t ) converges to p0(\u03c4d = 0). by the blumenthal 0\u20131 law\n(proposition 20.8), p0(\u03c4d = 0) = 1.\n\nv\n\ncontinue to suppose (xt , px) is a d-dimensional brownian motion and d is a bounded\n\ndomain, but now we suppose d \u2265 3. define\n\n(cid:3) \u221e\n\nu (x, a) = e x\n\n1a(xs) ds,\n\n0\n\nx \u2208 d.\n\n "}, {"Page_number": 193, "text": "this is the same as the \u03bb-resolvent of 1a with \u03bb = 0. we write\n\n21.4 harmonic functions\n\n175\n\nu (x, a) = e x\n\n(cid:3) \u221e\n(cid:3) \u221e\n1a(xs), ds\n(cid:3)\n(cid:3) \u221e\npx(xs \u2208 a) ds\n(cid:3) \u221e\n(cid:3)\n\n(2\u03c0s)d/2 e\n\n1\n\na\n\n0\n\n0\n\n0\n\n=\n\n=\n\n=\n\n\u2212|y\u2212x|2/2s dy ds\n\n1\n\n(2\u03c0s)d/2 e\n\n\u2212|y\u2212x|2/2s ds dy.\n\na\n\n0\n\nsome calculus shows that the inside integral is equal to c|x \u2212 y|2\u2212d. if we denote c|x \u2212 y|2\u2212d\nby u(x, y), we then have that\n\n(cid:3)\n\nu (x, a) =\n\nu(x, y) dy.\n\na\n\n(21.9)\n\nthe expression u(x, y) is called the newtonian potential density. note that u(x, y) is a\nfunction only of |x \u2212 y|, it blows up as |x \u2212 y| \u2192 0, and tends to 0 as |x \u2212 y| \u2192 \u221e.\nif x is in the interior of d, then u(x,\u00b7) will be bounded on \u2202d. define hx(z) = e zu(x, x\u03c4d\n);\nwe saw above that hx is harmonic. now define gd(x, y) = u(x, y) \u2212 hx(y); this function of\ntwo variables is called the green\u2019s function or green function for d with pole at x. this is\na well-known object in analysis \u2013 let us give a probabilistic interpretation. since u(x, y) is\nsymmetric in x and y, if a \u2282 d we have\n\ngd(x, y) dx =\n\nu(x, y) dx \u2212\n\ne yu(x, x\u03c4d\n\n) dx\n\n(21.10)\n\n(cid:3)\n\na\n\n(cid:3)\n\na\n\nusing the strong markov property and then a change of variables,\n\n(cid:16)\n\n(cid:3) \u221e\n\ne y\n\ne x\u03c4d\n\n1a(xs) ds\n\n0\n\n(cid:3)\n(cid:16)\n\na\n\nu(x, x\u03c4d\n\n) dx\n\n(cid:3) \u221e\n\ne x\u03c4d\n\n0\n\n1a(xs) ds\n\n1a(xs) \u25e6 \u03b8\u03c4d ds | f\u03c4d\n\n(cid:17)\n(cid:17)(cid:17)\n\n.\n\n(cid:3)\n\na\n\n= e y\n\n= e y\n\n(cid:3) \u221e\n(cid:3) \u221e\n(cid:17)\n\n0\n\n0\n\n= e y\n\ne y\n\n1a(xs) ds \u2212 e y\n1a(xs) ds \u2212 e y\n(cid:16)(cid:3) \u221e\n(cid:16)\n(cid:3) \u221e\n(cid:3) \u221e\n(cid:3) \u221e\n(cid:3) \u03c4d\n\n\u03c4d\n\n0\n\n0\n\n0\n\n= e y\n\n= e y\n\n= e y\n\n1a(xs) \u25e6 \u03b8\u03c4d ds\n\n1a(x\u03c4d+s) ds\n\n1a(xs) ds.\n\n(cid:3)\n\na\n\ngd(x, y) dx = e y\n\n1a(xs) ds.\n\n0\n\nsubstituting this in (21.10) we have\n\nfor this reason gd is sometimes called the occupation time density for d.\n\n "}, {"Page_number": 194, "text": "176\n\napplications of the markov properties\n\nexercises\n\n21.1 suppose d = 2, (xt , px ) is a two-dimensional brownian motion, and r > 0. imitate the argument\nof proposition 21.1 but with h(x) = log(|x|) to show that px(xt hits b(0, r)) = 1 when |x| > r.\nthen use the strong markov property to show that there are times ti \u2192 \u221e with xti\n\u2208 b(0, r).\nthat is, two-dimensional brownian motion is neighborhood recurrent.\n\n21.2 in the proof of lemma 21.4, justify each inequality in (21.6).\n\n21.3 let (xt , px ) be a poisson process with parameter a and let f be defined by (21.5). show\n\nf (t, 1/2)/t does not converge to 0 as t \u2192 0.\n\n21.4 suppose d \u2265 3, (xt , px ) is a d-dimensional brownian motion, and\n\n(cid:3) \u221e\n\nu f (x) = e x\n\nf (xs ) ds.\n\n0\n\nshow that if f is bounded and measurable with compact support, then u f is continuous and\n|u f (x)| \u2192 0 as |x| \u2192 \u221e. show that if f \u2208 c2 with compact support, then u f is c2. show\nthat 1\n2\n\n\u0001u f = \u2212 f .\n\n21.5 let wt be a brownian motion and f a continuous function. prove that if f (wt ) is a submartingale,\n\nthen f must be convex.\n\n21.6 prove the maximum principle for harmonic functions. this says that if h is harmonic in a bounded\n\ndomain d, then\n\n|h(x)| \u2264 sup\nx\u2208\u2202d\n\nsup\nx\u2208d\n\n|h(x)|.\n\n21.7 if w is a d-dimensional brownian motion started at 0, find e t , where t is the first time w exits\n\nthe ball of radius r centered at the origin.\nhint: use the fact that |wt|2 \u2212 dt is a martingale.\n\n21.8 let f\n\n: r \u2192 r be a bounded function with | f (x) \u2212 f (y)| \u2264 |x \u2212 y| for all x, y \u2208 r. let\nd\u03b5 = {(x, y) \u2208 r2 : f (x) < y < f (x) + \u03b5} for \u03b5 \u2208 (0, 1). let (xt , px ) be a two-dimensional\nbrownian motion and let \u03c4\u03b5 = inf{t : xt /\u2208 d\u03b5}. prove that there exists a constant c not depending\non \u03b5 such that e 0\u03c4\u03b5 \u2264 c\u03b52.\n\nhint: by exercise 21.7 the expected time for two-dimensional brownian motion to leave\na ball of radius 2\u03b5 is less than c\u03b52. then use the strong markov property repeatedly at the\ntimes si, where si is the first time after time si\u22121 that brownian motion has moved at least 2\u03b5\nfrom xsi\u22121.\n\n "}, {"Page_number": 195, "text": "22\n\ntransformations of markov processes\n\nthere are a number of interesting transformations that make new markov processes out of\nold. we will look at four: killing, conditioning, changing time, and stopping at a last exit\ntime. these are only a few of the possible transformations.\n\n22.1 killed processes\n\none sometimes wants to consider a markov process up until a stopping time \u03b6 , called the\nlifetime of the process. we affix to our state space s an isolated point \u0001, called the cemetery\nstate, and the topology on s\u0001 = s \u222a {\u0001} is the one generated by the collection of open sets\n\nof s together with the set {\u0001}. we define the killed process(cid:2)x by\n\n(cid:13)\n\n(cid:2)xt =\n\nxt ,\n\u0001,\n\nt < \u03b6;\nt \u2265 \u03b6 ,\n\n(22.1)\n\nand we say we kill the process x at time \u03b6 . every function f on s is defined to be 0 at \u0001.\none example of this situation would be to let \u03b6 = \u03c4d, where d is a subset of s and\n\u03c4d = inf{t > 0 : xt /\u2208 d}, the first exit from the set d. another common occurrence is to\n(cid:15)\nlet \u03b6 = s, where s is a random variable with an exponential distribution with parameter \u03bb,\ni.e., p(s > t ) = e\n\u2212\u03bbt, such that s is independent of x . a third possibility would be to let\n0 f (xs) ds \u2265 1}, where f is a non-negative function. the crucial property of \u03b6\n\u03b6 = inf{t :\nis that it be a terminal time:\nproposition 22.1 if (xt , px) is a strong markov process and (22.2) holds, then ((cid:2)xt , px)\n\n\u03b6 = s + \u03b6 \u25e6 \u03b8s\n\nif s < \u03b6 .\n\n(22.2)\n\nt\n\nsatisfies the markov and strong markov properties.\n\nproof as in section 20.2, we need to show\n\nif a \u2208 ft ,\n\n(cid:2)xt f ((cid:2)xt ),\n\ne x[ f ((cid:2)xt ) \u25e6 \u03b8t|ft ] = e\ne x[ f ((cid:2)xt ) \u25e6 \u03b8t; a] = e x[ f (xt+t ); a, t + t < \u03b6 ].\n\npx-a.s.\n\n177\n\n "}, {"Page_number": 196, "text": "178\n\ntransformations of markov processes\n\non the other hand,\n\n(cid:2)xt f ((cid:2)xt ) = e xt [ f (xt ); t < \u03b6 ]1(t <\u03b6 )\n\ne\n\n= e x[ f (xt ) \u25e6 \u03b8t; t \u25e6 \u03b8t < \u03b6 \u25e6 \u03b8t|ft ]1(t <\u03b6 )\n= e x[ f (xt+t ); t + t \u25e6 \u03b8t < t + \u03b6 \u25e6 \u03b8t , t < \u03b6|ft ]\n= e x[ f (xt+t ); t + t < \u03b6|ft ],\n\nsince t + t \u25e6 \u03b8t = t + t and t + \u03b6 \u25e6 \u03b8t = \u03b6 on (t < \u03b6 ). hence\n\n(cid:2)xt f ((cid:2)xt ); a] = e x[ f (xt+t ); t + t < \u03b6 , a],\n\ne x[e\n\nas required.\n\n22.2 conditioned processes\n\nanother type of transformation of a markov process is by conditioning, also known as doob\u2019s\nh-path transform. to motivate this, let d be a domain in rd and let xt be a brownian motion\nkilled on exiting the domain. one would like to give a precise meaning to the intuitive notion\nof brownian motion conditioned to exit the domain at a certain point. let h be a positive\nharmonic function in d (i.e., h is c2 in d, and \u0001h = 0 there) and suppose that h is 0\neverywhere on the boundary of d except at one point z. the poisson kernel for the ball or\nfor the half-space gives examples of such harmonic functions. then, heuristically, we have\nby the markov property at time t,\n\npx(xt \u2208 dy|x\u03c4d\n\n= z) = px(xt \u2208 dy, x\u03c4d\n= z)\n= z)\n= px(xt \u2208 dy)py(x\u03c4d\n= z)\n\npx(x\u03c4d\n\npx(x\u03c4d\n\n= z)\n\n.\n\nif p0(t, x, dy) represents the probability that brownian motion started at x and killed on\nleaving d is in dy at time t, we then expect that the analogous probability for brownian\nmotion conditioned to exit d at z ought to be h(y)p0(t, x, dy)/h(x). we now make this\nprecise.\nlet us look at a strong markov process x . we say a function h is invariant with respect\nto x if pth(x) = h(x) for all t and x, where pt is the semigroup associated with x . if h is\ninvariant, by the markov property,\n\ne x[h(xt ) | fs] = e x[h(xt\u2212s) \u25e6 \u03b8s | fs] = e xsh(xt\u2212s)\n\n= pt\u2212sh(xs) = h(xs),\n\nand so for each x, h(xt ) is a martingale with respect to px. conversely, if h(xt ) is a martingale\nwith respect to px for all x,\n\npth(x) = e xh(xt ) = h(x)\n\nby the definition of martingale, and so h is invariant. in the case of brownian motion killed\non leaving a domain, the invariant functions are thus the harmonic ones.\n\n "}, {"Page_number": 197, "text": "22.2 conditioned processes\n\n179\n\nnow let h be a non-negative invariant function for a strong markov process x . letting\nmt = h(xt )/h(x0), mt is a non-negative continuous martingale with m0 = 1, px-a.s., as long\nas h(x) > 0.\n\nwe define the h-path transform of the markov process x by setting\n\n(a) = e x[mt; a],\n\npx\nh\n\na \u2208 ft .\n\n(22.3)\n\nsince m0 = 1, px\nless to where it is small. note the similarity to the girsanov theorem.\n\n(\u0001) = 1. observe that px\n\nh\n\nh gives more mass to paths where h(xt ) is big and\n\nwe have the following.\n\nproposition 22.2 suppose (xt , px) is a strong markov process and that h is non-negative\nand invariant. then (xt , px\nh\nproof suppose a \u2208 fs and h(x) (cid:16)= 0. (we leave consideration of the case where h(x) = 0\nto the reader.) then\n\n) forms a strong markov process.\n\ne x\n\nh[ f (xt+s); a] = e x[ f (xt+s)h(xt+s); a]\n= e x[e xs[ f (xt )h(xt )]; a]\n= e x\n\n(cid:17)\nh(x)\ne xs[ f (xt )h(xt )]h(xs); a\n\nh(x)\n\n(cid:16)\n\n1\n\n(cid:27)\nby the markov property for x . this is equal to\n/h(x) = ex\nh [ f (xt )]h(xs); a\n\ne xs\n\n(cid:26)\n\ne x\n\nh(xs)\n\nh f (xt ); a].\n\nh[e xs\n\nthe markov property follows from this. the strong markov property is proved in almost\nidentical fashion.\n\nlet us consider an example. let (xt , px) be a brownian motion on the non-negative axis\nkilled on first hitting 0. this is the same as a brownian motion killed on exiting (0,\u221e). this\nwill be a strong markov process. since the second derivative of the function h(x) = x is 0,\nthen h is harmonic on (0,\u221e), and so is invariant for the killed brownian motion. let us\nnow condition using the function h to get brownian motion conditioned to hit infinity before\nhitting zero.\nto identify the resulting process, we argue as follows. fix x and let t\u03b5 = inf{t > 0 :\nh with respect to px on ft\u2227t\u03b5 is\nxt < \u03b5}. the radon\u2013nikodym derivative of the law of px\nmt\u2227t\u03b5\n\n= h(xt\u2227t\u03b5\n\n)/h(x). we can rewrite mt\u2227t\u03b5 as\n\u2212 log x) = exp\n= exp(log xt\u2227t\u03b5\n\nmt\u2227t\u03b5\n\n(cid:3)\n\n(cid:10)\n\n(cid:11)2\n\n(cid:11)\n\nds\n\n,\n\nt\u2227t\u03b5\n\ndxs \u2212 1\n\n2\n\n1\nxs\n\n0\n\nt\u2227t\u03b5\n\n0\n\n1\nxs\n\nusing it\u02c6o\u2019s formula. by the girsanov theorem, under px\nh,\nt\u2227t\u03b5\n\nwt\u2227t\u03b5\n\n= xt\u2227t\u03b5\n\n\u2212\n\n1\nxs\n\nds\n\n0\n\n(cid:10)(cid:3)\n(cid:3)\n\n "}, {"Page_number": 198, "text": "transformations of markov processes\n\n180\nis a martingale. by exercise 13.2, its quadratic variation is t \u2227 t\u03b5, and so by exercise 12.3,\nwt\u2227t\u03b5 is a brownian motion stopped at time t\u03b5. we have\nt\u2227t\u03b5\n\n(cid:3)\n\nor x satisfies the stochastic differential equation\n\n0\n\n= x + wt\u2227t\u03b5\n\n+\n\nxt\u2227t\u03b5\n\n1\nxs\n\nds,\n\ndxt = dwt + 1\nxt\n\ndt\n\nfor t \u2264 t\u03b5. we will see later (section 24.3) that this is the stochastic differential equation\ndefining the bessel process of order 3. the same argument shows that brownian motion\nkilled on exiting (0, a) and then conditioned to hit a before 0 is also a bessel process of\norder 3 up until the time of first hitting a.\n\n22.3 time change\n\nat = as + at\u2212s \u25e6 \u03b8s\n\nan additive functional is an increasing adapted process with a0 = 0, a.s., such that\n\n(22.4)\n\nif s < t. the simplest examples are what are known as classical additive functionals:\n\n(cid:3)\n\n(cid:3)\n\nt\n0 f (xr ) dr, where f is a non-negative measurable function. we have\nf (xr ) dr \u25e6 \u03b8s = at\u2212s \u25e6 \u03b8 .\n\nf (xr ) dr =\n\nat \u2212 as =\n\nt\u2212s\n\nt\n\ns\n\n0\n\nif we have the uniform limit of additive functionals, we again get an additive functional, and\nthus, for example, the local times lx\nt of a one-dimensional brownian motion are also additive\nfunctionals.\n\ngiven a markov process x and an additive functional a, let\n\nbt = inf{u : au > t}\n\nat =(cid:15)\n\nand\n\nt\n\n(cid:3)\n\nis a time change of x .\n\n= fbt . thus x\n\nlet f(cid:3)\nproposition 22.3 let (xt , px) be a strong markov process and at an additive functional.\nwith b defined as above, (x\nt\nproof we verify the strong markov property. let f(cid:3)\n= fbt . then if t is a stopping time\nfor f(cid:3)\n\n, px) is also a strong markov process.\n\n(cid:3)\n\nt\n\nt , we have\n\ne x[ f (x\n\n(cid:3)\nt+t\n\n) | f(cid:3)\n\nt ] = e x[ f (x (bt+t )) | fbt ].\n\nbt can be seen to be a stopping time with respect to {ft} and bt+t = bt \u25e6 \u03b8bt where the \u03b8t\nare the shift operators, so this is\n\nthis suffices to show that x\n\n) = e xe x\n\ne xe x (bt ) f (xbt\n(cid:3)\nt is a strong markov process.\n\n(cid:3)\nt f (x\nt\n\n(cid:3)\n\n).\n\n(cid:3)\n\nx\nt\n\n= xbt\n\n.\n\n "}, {"Page_number": 199, "text": "22.4 last exit decompositions\n\n181\n\n22.4 last exit decompositions\n\nlet a be a borel set, and let l be the last visit to a:\n\nl = sup{s : xs \u2208 a}.\n\nwe define l to be 0 if x never hits a. the random time l is not a stopping time, but we can\nnevertheless kill the process x at time l. it turns out the resulting process y is the process\nx conditioned by the function h(x) = px(ta < \u221e). the intuitive meaning of this is that y\nis x conditioned to hit the set a.\nlet t = inf{t : xt \u2208 a}, and set\n\n(cid:13)\n\nyt =\n\nxt ,\n\u0001,\n\nt < l,\nt \u2265 l.\n\nlet ht = \u03c3 (ys; s \u2264 t ).\nproposition 22.4 if (xt , px) is a strong markov process, then (yt , px) is a markov process\nwith respect to {ht}.\nproof\n\nif b \u2282 s (so that \u0001 /\u2208 b), then\n\n(yt \u2208 b) = (xt \u2208 b, l > t ) = (xt \u2208 b, t \u25e6 \u03b8t < \u221e),\n\nsince l, the last time that x is in a, will be larger than t if and only if x hits a at some time\nafter time t. we conclude that the function x \u2192 px(yt \u2208 b) is borel measurable. since\n\npx(yt = \u0001) = px(l \u2264 t ) = 1 \u2212 px(l > t ) = 1 \u2212 px(t \u25e6 \u03b8t < \u221e),\n\nthen the function x \u2192 px(yt = \u0001) is also borel measurable.\nwe need to show that if c \u2208 hs,\n\ne x[ f (yt ); c] = e x[qt\u2212s f (ys); c],\n\n(22.5)\n\nwhere f is bounded and measurable, h(x) = px(l > 0), and\n\nqtg(x) = 1\nh(x)\nwhen h(x) (cid:16)= 0. (set qtg(x) = 0 if h(x) = 0.)\nit suffices to show (22.5) when c = (yr1\n\u2208 b1, . . . , yrn\nthe b1, . . . , bn are borel sets. if we set\ncs = (xr1\n\n\u2208 b1, . . . , xrn\n\npt (gh)(x)\n\n\u2208 bn),\n\nthen cs \u2208 fs, c \u2229 (l > s) = cs \u2229 (l > s), and c \u2229 (l > t ) = cs \u2229 (l > t ).\nwe start with\n\ne x[ f (yt ); c] = e x[ f (xt ); c, l > t] = e x[ f (xt ); cs, l > t]\n\n= e x[ f (xt ); cs, l \u25e6 \u03b8t > 0].\n\nconditioning on ft, this is equal to\n\ne x[ f (xt )pxt (l > 0); cs] = e x[ f (xt )h(xt ); cs].\n\n\u2208 bn) for r1 \u2264 \u00b7\u00b7\u00b7 \u2264 rn \u2264 s and\n\n "}, {"Page_number": 200, "text": "182\n\ntransformations of markov processes\n\nconditioning on fs, this in turn is equal to\n\ne x[pt\u2212s( f h)(xt\u2212s); cs] = e x[h(xs)qt\u2212s f (xs); cs]\n\n(22.6)\n\n= e x[pxs (l > 0)qt\u2212s f (xs); cs]\n= e x[qt\u2212s f (xs); cs, l \u25e6 \u03b8s > 0],\n\nwhere we used the markov property for the last equality. continuing, we have that the last\nline of (22.6) is equal to\n\ne x[qt\u2212s f (xs); cs, l > s] = e x[qt\u2212s f (xs); c, l > s]\n\n= e x[qt\u2212s f (ys); c],\n\nas desired.\n\nwe can also look at xl+t, where l is as above. this new process is again a strong markov\nprocess, and this time is the process x conditioned by the function h(x) = px(ta = \u221e). the\nintuitive meaning of this is that xl+t is x conditioned never to hit a. since we are looking\nat the process after the last visit to a, this is entirely plausible. for a proof of the markov\nproperty of xl+t, see meyer et al. (1972).\n\nexercises\n\nand m a positive finite measure on r. show that at =(cid:15)\n\n22.1 let (xt , px ) be a one-dimensional brownian motion, lx\n\nt the local time of brownian motion at x,\n22.2 we consider the space-time process. let vt = v0 + t. the process vt is simply the process that\nincreases deterministically at unit speed. thus vt can represent time. if (xt , px ) is a markov\nprocess, show that ((xt , vt ), p(x,v) ) is also a markov process. is ((xt , vt ), p(x,v) ) necessarily a\nstrong markov process if (xt , px ) is a strong markov process?\nfor some applications, one lets vt = v0 \u2212 t, and one thinks of time running backwards.\nspace-time processes are useful when considering parabolic partial differential equations.\n\nlx\nt m(dx) is an additive functional.\n\n22.3 suppose (xt , px ) is a strong markov process and f\n\n(xt , px ). write qx for px\nf g is a non-negative invariant function for (xt , px ) and that qx\ng\n\nis a non-negative invariant function for\nf . suppose g is a non-negative invariant function for (xt , qx ). show that\n\n= px\nf g.\n\n22.4 suppose a and b are additive functionals for a markov process and a and b have continuous\n\npaths. prove that if e xat = e xbt for all x and t, then\n\npx(at (cid:16)= bt for some t \u2265 0) = 0\n\nfor all x.\nhint: show at \u2212 bt is a martingale.\n\n\u221e for each x. show\n\n22.5 suppose a and b are additive functionals with continuous paths and suppose e xa\u221e = e xb\u221e <\n\npx(at (cid:16)= bt for some t \u2265 0) = 0\n\nfor each x.\n\nhint: if f (x) = e xa\u221e, then\n\ne x[a\u221e | ft] \u2212 at = e xt a\u221e = f (xt ),\n\nand similarly with b in place of a. then a \u2212 b is a px martingale for each x.\n\n "}, {"Page_number": 201, "text": "notes\n\n183\n\n22.6 let a be an additive functional with continuous paths. suppose there exists k > 0 such that\n\ne xa\u221e \u2264 k for each x. prove that there exists a constant c depending only on k such that\n\ne eca\u221e < \u221e,\n\nx \u2208 s.\n\n22.7 here is an argument that the law of a brownian motion conditioned to have a maximum at a\ncertain level is a bessel process of order 3.\nlet w be a one-dimensional brownian motion killed on hitting 0. let st = sups\u2264t ws be\nthe maximum. by exercise 19.1, x = (w, s) is a markov process. determine the law of\nx for t \u2264 l, where l is the last time x hits the diagonal. to define l more precisely, let\nd = {(w, s) : w = s, w > 0} and l = sup{t \u2265 0 : xt \u2208 d}. l is finite, a.s., because w will hit 0\nin finite time with probability one.\n\nnotes\n\nmarkov processes are in some sense supposed to have the property that the past and the\nfuture are independent given the present. from this point of view, one might hope that a\nmarkov process run backwards is again a markov process. this is, more or less, the case;\nsee chung and walsh (1969) or rogers and williams (2000a).\n\n "}, {"Page_number": 202, "text": "23\n\noptimal stopping\n\na nice application of markov process theory is optimal stopping. suppose we have a reward\nfunction g \u2265 0 and we want to find the stopping time t that maximizes the value of e xg(xt )\nand we also want to find the value of this expectation. this is the optimal stopping problem.\nan important example of an optimal stopping problem is pricing the american put. (see\nchapter 28 for more on options.) a european put is an option to sell a share of stock at a\nfixed price k at a certain time t0. if at time t0 the price st0 of the stock is lower than k, one\ncan make a profit by buying a share of stock on the stock exchange for st0 dollars, exercising\nthe put (which means selling a share of stock for k dollars), and taking home a profit of\nk \u2212 st0. if the price of the stock is above k at time t0, it would be silly to exercise the put,\nand thus the put is worthless. an american put is almost the same, but one has the option to\nsell a share of stock at price k at any time before time t0. an american put is more valuable\nthan a european put because if one exercises the option early, that is, sells the share of stock\nbefore time t0, then one can put the money in a risk-free asset such as a bond or in the bank\nand earn interest on the money. when should one exercise an american put to maximize the\nexpected return? one cannot look into the future, so the time should be a stopping time. the\nstopping time should depend on the stock price, the exercise price, and also the time until\ntime t0. thus one is in the optimal stopping context with xt = (t, st ), where st is the stock\nprice, and one wants to find a stopping time t that maximizes a certain reward function.\n\n23.1 excessive functions\n\na solution to the optimal stopping problem can be given in the markov case through the use\nof excessive functions. a non-negative function f is excessive for a markov process x if\npt f (x) \u2264 f (x) for all t and x and pt f (x) increases up to f (x) pointwise as t \u2192 0. here pt is\nthe semigroup associated with the markov process x . if g \u2265 0, define\n\n(cid:3) \u221e\n\n(23.1)\nwhen g \u2265 0, u g is excessive. to see this, using the semigroup property and a change of\nvariables,\n\ng(xs) ds.\n\n0\n\n0\n\n(cid:3) \u221e\n\n(cid:3) \u221e\n\nps+tg(x) ds\n\n0\n\nu g(x) =\n\npsg(x) ds = e x\n(cid:10)(cid:3) \u221e\n(cid:11)\n(cid:3) \u221e\npt f (x) = pt\n=\n\npsg(x) ds\n\n=\n\npsg(x) ds.\n\n0\n\nt\n\n184\n\n "}, {"Page_number": 203, "text": "23.1 excessive functions\n\n185\nthis is certainly less than the integral from 0 to \u221e, hence is less than f (x), and pt f (x)\nincreases up to f (x) by monotone convergence. (it is possible that f is infinite for some or\nall x.)\n\nthe theory of excessive functions is an important part of markov process theory and we\nrefer the reader to blumenthal and getoor (1968), a book which has inspired a generation of\nmarkov process theorists.\nwe have the following.\n\nlemma 23.1 if f is excessive, there exist functions gn \u2265 0 such that u gn increases up to f ,\nwhere u gn is defined by (23.1).\nproof let gn = n( f \u2212 p1/n f ). since f is excessive, then gn \u2265 0. we have\n\n(cid:3) \u221e\n\n(cid:3) \u221e\n(cid:3)\n\n0\n\n1/n\n\nu gn = n\n= n\n\nps f ds,\n\n0\n\nps f ds \u2212 n\n\nps+(1/n) f ds\n\n0\n\nwhich is less than f and increases to f .\n\nnext we have\n\nproposition 23.2 (1) if f is excessive, t is a finite stopping time, and h(x) = e x f (xt ),\nthen h is excessive.\n\n(2) if f is excessive and t is a finite stopping time, then f (x) \u2265 e x f (xt ).\n(3) if f is excessive, then f (xt ) is a supermartingale\n\nproof\n\n(1) first suppose f = u g for some non-negative function g. then\n\nh(x) = e xu g(xt ) = e xe xt\n\ng(xs) ds\n\n(23.2)\n\n(cid:3) \u221e\n(cid:3) \u221e\n\n0\n\n(cid:3) \u221e\n\n0\n\n= e x\n\nby the strong markov property and a change of variables. the same argument shows that\n\npth(x) = e xh(xt ) = e xe xt\n\n(cid:15) \u221e\nt g(xs) ds = h(x) and increases up to h(x) as t \u2193 0.\n\ng(xs) ds = e x\n\nt+t\n\nt\n\ng(xs) ds.\n\nthis is less than e x\n\nnow let f be excessive but not necessarily of the form u g. in the paragraph above, replace\n\ng by the gn that were defined in lemma 23.1 to conclude\n\npth(x) = lim\n\nn\u2192\u221e ptu gn(x) \u2264 lim\n\nn\u2192\u221e u gn(x) = h(x).\n\nthat pth increases up to h is proved similarly; there is no difficulty interchanging the limit\nas n tends to infinity and the limit as t tends to 0 since ptu gn increases both as n increases\nand as t decreases.\n\ng(xs+t ) ds = e x\n(cid:3) \u221e\n\ng(xs) ds\n\nt\n\n(cid:3) \u221e\n\n "}, {"Page_number": 204, "text": "186\n\noptimal stopping\n\n(2) as in the proof of (1), it suffices to consider the case where f = u g and then take\n\nlimits. by (23.2),\n\ne xu g(xt ) = e x\n\n(3) by the markov property,\n\n(cid:3) \u221e\n\n(cid:3) \u221e\n\ng(xs) ds \u2264 e x\n\ng(xs) ds = u g(x).\n\nt\n\n0\n\ne x[ f (xt ) | f s] = e xs f (xt\u2212s) = pt\u2212s f (xs) \u2264 f (xs).\n\nthe proof is complete.\n\nby proposition 23.2, f (xt ) is a supermartingale and therefore has left and right limits\nalong the dyadic rationals. we could take a version of f (xt ) that is right continuous, but\nthere is the danger that doing so would result in a version of x that is not right continuous\nwith left limits. we want to have x fixed and then conclude that f (xt ) is right continuous\nwith left limits without needing to take a version.\n\nproposition 23.3 let (xt , px) be a strong markov process. if f is excessive, then for each\nx, f (xt ) is right continuous with left limits px almost surely.\n\nfor a proof, we refer the reader to blumenthal and getoor (1968), theorem ii.2.12 or to\n\nexercise 23.8.\ngiven a function g, the function g is an excessive majorant for g if g is excessive and\ng \u2265 g pointwise. g is the least excessive majorant for g if (1) g is an excessive majorant,\n\nand (2) if(cid:14)g is any other excessive majorant, then g \u2264 (cid:14)g pointwise.\n\nit turns out, which we will prove below, that an optimal stopping time is to stop the first\ntime xt leaves the set where g(x) < g(x). therefore it is important to be able to calculate\nthe least excessive majorant of a function.\nhere is one method of constructing the least excessive majorant. we say a function\nf : s \u2192 r is lower semicontinuous if {x : f (x) > a} is an open set for every real number\na. see exercise 23.9 for information about lower semicontinuous functions.\n\nproposition 23.4 suppose that g is non-negative, bounded, and continuous and that as-\nsumption 20.1 holds. let g0 = g, let tn = {k/2n : 0 \u2264 k \u2264 n2n}, and define\n\ngn(x) = max\nt\u2208tn\n\nptgn\u22121(x)\n\nfor n = 1, 2, . . . then gn(x) increases pointwise to g(x), the least excessive majorant of g.\nproof since gn(x) \u2265 p0gn\u22121(x) = e xgn\u22121(x0) = gn\u22121(x), the sequence gn(x) is increasing.\ncall the limit h (x).\n\nwe first show h is lower semicontinuous. if gn\u22121 is bounded and continuous, then ptgn\u22121\nis bounded and continuous for each t by assumption 20.1. since the maximum of a finite\nnumber of continuous functions is continuous, then gn is bounded and continuous. by an\ninduction argument, each gn is continuous. by exercise 23.9, h is lower semicontinuous.\n\nwe next show that h is excessive. if t \u2208 tm and n \u2265 m, then\n\nh (x) \u2265 gn(x) \u2265 ptgn\u22121(x) = e xgn\u22121(xt ).\n\n "}, {"Page_number": 205, "text": "23.2 solving the optimal stopping problem\n\n187\nletting n tend to infinity, h (x) \u2265 e xh (xt ) if t \u2208 tm for some m. now take tk \u2208 \u222amtm with\ntk \u2192 t. since h is lower semicontinuous, then using exercise 23.9 and fatou\u2019s lemma,\n\nh (x) \u2265 lim inf\n\nk\u2192\u221e e xh (xtk\n\n) \u2265 e x[lim inf\n\nk\u2192\u221e h (xtk\n\n)] \u2265 e xh (xt ).\n\nif a \u2208 r, let ea = {y : h (y) > a}, which is open. if a < h (x), then\n\npth (x) = e xh (xt ) \u2265 apx(xt \u2208 ea) \u2192 a\nas t \u2192 0. therefore lim inf t\u21920 pth (x) \u2265 a for all a < h (x), hence\n\npth (x) \u2265 h (x),\n\nlim inf\nt\u21920\n\nsuppose now that f is excessive and f \u2265 g pointwise. if f \u2265 gn\u22121,\n\nand we conclude pth (x) \u2192 h (x) as t \u2192 0. thus h is excessive.\nthen\nf (x) \u2265 ptf (x) \u2265 ptgn\u22121(x) for every t \u2208 tn, hence f (x) \u2265 gn(x). by an induction argument,\nf (x)\u2265 gn(x) for all n, hence f (x)\u2265 h (x). therefore h is the least excessive majorant of g.\nin one case, at least, finding the least excessive majorant is easy. suppose we have a one-\ndimensional brownian motion killed on leaving an interval [a, b] and a non-negative function\ng defined on [a, b]. then the least excessive majorant is the smallest concave function g that\nis larger than or equal to g everywhere. to see this, if g is the smallest concave function, by\njensen\u2019s inequality\n\nptg(x) = e xg(xt ) \u2264 g(e xxt ) \u2264 g(x).\n\nbecause g is concave, it is continuous, and so ptg(x) = e xg(xt ) \u2192 g(x) as t \u2192 0.\n\ntherefore g is excessive. if(cid:14)g is another excessive function larger than g and a \u2264 c < x <\nd \u2264 b, we have (cid:14)g(x) \u2265 e x(cid:14)g(xs ), where s is the first time the process leaves [c, d] by\n\nproposition 23.2(1). since x is equal to a brownian motion up to time s, we know the exact\ndistribution of xs; see proposition 3.16. therefore\nd \u2212 c\n\n(cid:14)g(x) \u2265 e x(cid:14)g(xs ) = d \u2212 x\nrearranging this inequality shows that(cid:14)g is concave. recall that the minimum of two concave\nfunctions is concave, so g \u2227(cid:14)g is a concave function larger than g that is less than or equal\nto g. but g is the smallest concave function larger than or equal to g, hence g = g \u2227(cid:14)g, or\ng \u2264 (cid:14)g. thus g is the least excessive majorant of g.\n\n(cid:14)g(c) + x \u2212 c\n\n(cid:14)g(d ).\n\nd \u2212 c\n\n23.2 solving the optimal stopping problem\n\nnow let us turn to proving that an optimal stopping time can be given in terms of the least\nexcessive majorant. for simplicity we will suppose that g is non-negative, continuous, and\nbounded. we will assume that our markov process and g are such that a least excessive\n\u2217\nmajorant g exists. let g\n\nbe the optimal reward:\n\u2217(x) = sup{e xg(xt ) : t a stopping time}.\ng\n\nlet d = {x : g(x) < g(x)}, the continuation region and let \u03c4d = inf{t : xt /\u2208 d}.\n\u2217\ntheorem 23.5 let (xt , px) be a strong markov process and g, g\n\u03c4d < \u221e, px-a.s., then g\n\n\u2217(x) = g(x) = e xg(x\u03c4d\n\n).\n\n, g, and d as above. if\n\n "}, {"Page_number": 206, "text": "188\n\noptimal stopping\n\nin other words, an optimal stopping time is to stop the first time the process hits {x :\n\ng(x) = g(x)}.\nproof let d\u03b5 = {x : g(x) < g(x) \u2212 \u03b5}, and write \u03c4\u03b5 for \u03c4d\u03b5 . let h\u03b5 (x) = e x[g(x\u03c4\u03b5\nwhich is excessive by proposition 23.2(2).\nthird step is to prove that g(x) = g\nstep 1. let \u03b5 > 0. we claim\n\nthe first step of the proof is to prove (23.3) below. second, we prove g(x) \u2264 g\n\n\u2217(x) and the fourth that g\n\n)],\n\u2217(x). the\n\n\u2217(x) = e xg(x\u03c4d\n\n).\n\ng(x) \u2264 h\u03b5 (x) + \u03b5,\n\nx \u2208 d.\n\n(23.3)\n\nto prove this, we suppose not, that is, we let\nb = sup\nx\u2208d\n\n(g(x) \u2212 h\u03b5 (x))\n\nand suppose b > \u03b5. choose \u03b7 < \u03b5, and then choose x0 such that\n\n(23.4)\nsince h\u03b5 + b is an excessive majorant of g by the definition of b, and g is the least excessive\nmajorant, then\n\ng(x0) \u2212 h\u03b5 (x0) \u2265 b \u2212 \u03b7.\n\ng(x0) \u2264 h\u03b5 (x0) + b.\n\nfrom (23.4) and (23.5) we conclude\n\ng(x0) \u2264 g(x0) + \u03b7.\n\nby the blumenthal 0\u20131 law (proposition 20.8), either \u03c4\u03b5 is strictly positive with px0\n\nprobability one or else zero with px0 probability one. in the first case, for each t > 0,\n\ng(x0) + \u03b7 \u2265 g(x0)\n\n\u2265 e x[g(xt\u2227\u03c4\u03b5\n\u2265 e x0[g(xt ) + \u03b5; \u03c4\u03b5 > t].\n\n)]\n\nthe first inequality is (23.6), the second is due to g being excessive, and the third because\ng > g + \u03b5 up until the time \u03c4\u03b5. if we let t \u2192 0 and use the fact that g is continuous, we get\ng(x0) + \u03b7 \u2265 g(x0) + \u03b5, a contradiction to the way we chose \u03b7.\n\nin the second case, where \u03c4\u03b5 = 0 with px0-probability one, we have\n\nh\u03b5 (x0) = e x0g(x\u03c4\u03b5\n\n) = e x0g(x0) = g(x0) \u2265 g(x0) \u2265 h\u03b5 (x0) + b \u2212 \u03b7,\n\nin either case we reach a contradiction, so (23.3) must hold.\n\na contradiction since we chose \u03b7 < b.\nstep 2. a conclusion we reach from (23.3) is that h\u03b5 + \u03b5 is an excessive majorant of g.\ntherefore\n\n(23.5)\n\n(23.6)\n\n(23.7)\n\ng(x) \u2264 h\u03b5 (x) + \u03b5\n= e x[g(x\u03c4\u03b5\n\u2264 e x[g(x\u03c4\u03b5\n\u2264 g\n\n\u2217(x) + 2\u03b5.\n\n)] + \u03b5\n) + \u03b5] + \u03b5\n\n "}, {"Page_number": 207, "text": "exercises\n\n189\n\n) + \u03b5 = g(x\u03c4\u03b5\n\nthe first inequality holds because g is the least excessive majorant, the second inequality\n\u2217\nbecause g(x\u03c4\u03b5\n) by the definition of \u03c4\u03b5, and the third by the definition of g\n.\nsince \u03b5 is arbitrary, we see that g(x) \u2264 g\nstep 3. for any stopping time t , because g is excessive and majorizes g,\n\n\u2217(x).\n\ng(x) \u2265 e xg(xt ) \u2265 e xg(xt ).\n\ntaking the supremum over all stopping times t , g(x) \u2265 g\n\u2217(x), and therefore g(x) = g\nstep 4. because \u03c4d is finite almost surely, the continuity of g tells us that e xg(x\u03c4\u03b5\ne xg(x\u03c4d\n\n) as \u03b5 \u2192 0. by the definition of g\n\u2217\n\n, we know that e xg(x\u03c4\u03b5\n\n) \u2264 g\n\n\u2217(x).\n\n\u2217(x).\n) \u2192\n\non the other hand, by the definitions of \u03c4\u03b5 and h\u03b5,\n\ne xg(x\u03c4\u03b5\n\n) = e xg(x\u03c4\u03b5\n\n) \u2212 \u03b5 = h\u03b5 (x) \u2212 \u03b5.\n\nby the first inequality in (23.7), the right-hand side is greater than or equal to g(x) \u2212 2\u03b5 =\n\u2217(x) \u2212 2\u03b5. letting \u03b5 \u2192 0 we obtain\ng\n\ne xg(x\u03c4d\n\n) \u2265 g\n\n\u2217(x)\n\nas desired.\n\nthe following two corollaries are useful in applications.\n\ncorollary 23.6 suppose there exists a borel set a such that h is an excessive majorant of g,\nwhere h(x) = e xg(x\u03c4a\nproof let g be the least excessive majorant of g. then h(x) \u2265 g(x). however,\n\n) and \u03c4a = inf{t : xt /\u2208 a}. then g\n\n\u2217(x) = h(x).\n\nh(x) = e xg(x\u03c4a\n\n) \u2264 sup\n\nt\n\ne xg(xt ) = g\n\n\u2217(x) = g(x)\n\nby theorem 23.5.\n\n). if h \u2265 g, then h = g\n\u2217\n\ncorollary 23.7 suppose g is continuous and g, the least excessive majorant of g, is lower\nsemicontinuous. let d be the continuation region, suppose \u03c4d < \u221e, a.s., and let h(x) =\ne xg(x\u03c4d\nproof note d = {x : g(x) < g(x)} = \u222aa<b[(g(x) < a) \u2229 (g(x) > b)], where the union is\nover all pairs of real numbers a < b. since g is lower semicontinuous and g is continuous,\n), a.s. since g \u2264 g, we see that\nthen d is open. this implies x\u03c4d\n\n.\n\n/\u2208 d, and so g(x\u03c4d\nh(x) = e xg(x\u03c4d\n\n) \u2265 g(x\u03c4d\n) = e xg(x\u03c4d\n\n).\n\nsince g is excessive, then h is also excessive by proposition 23.2. therefore h is an excessive\nmajorant of g and we can apply corollary 23.6.\n\n23.1 show that if f is excessive, then 1 \u2212 e\nlook at bounded excessive functions.\n\n23.2 show that if f and g are excessive, then f \u2227 g is excessive.\n\nexercises\n\u2212 f is excessive. thus, for some purposes it is enough to\n\n "}, {"Page_number": 208, "text": "190\n23.3 let at be an additive functional (defined in (22.4)) and let f (x) = e xa\u221e. show that f\n\noptimal stopping\n\nis\n\nexcessive.\n\n23.4 let f be an excessive function for a strong markov process (xt , px ). let \u03b5 > 0 and s1 = inf{t :\n) is a supermartingale with\n\n| f (xt ) \u2212 f (x0 )| \u2265 \u03b5}. let si+1 = si + s1 \u25e6 \u03b8si. prove that f (xsi\nrespect to the \u03c3 -fields fsi and with respect to px for each x.\n\n23.5 for each n, let an\n\nt be an additive functional with continuous paths and suppose that fn(x) = e xan\u221e\nis finite for every x. suppose at is a continuous additive functional with f (x) = e xa\u221e also\nfinite for each x. suppose fn converges to f uniformly. prove that for each x, with px-probability\none, an\n\nt converges to at, uniformly over t \u2265 0.\n\nhint: use proposition 9.11.\n\n23.6 suppose f is bounded and excessive, \u03bb \u2265 0, and a = {y : f (y) \u2264 \u03bb}. prove that if x \u2208 ar (i.e.,\n\nx is regular for a), then f (x) \u2264 \u03bb.\nhint: use the optional section theorem (theorem 16.12) to find stopping times tm whose\ngraphs are contained in {(t, \u03c9) : t \u2264 1/m, f (xt ) \u2264 \u03bb} with px-probability at least 1 \u2212 (1/m).\nif the gn are as in lemma 23.1, write\nu gn(x) = e x\n\u2264 e x\n\ntm\n\ntm\n\n)\n\n0\n\n)\n\ngn(xs ) ds + e xu gn(xtm\ngn(xs ) ds + e x f (xtm\ngn(xs ) ds + \u03bb + (cid:21) f (cid:21)\u221e/m.\n\n(cid:3)\n(cid:3)\n(cid:3)\n\n\u2264 e x\n\ntm\n\n0\n\n0\n\nlet m \u2192 \u221e, then n \u2192 \u221e.\n\n23.7 suppose f is bounded and excessive, \u03bb \u2265 0, and b = {y : f (y) \u2265 \u03bb}. prove that if x \u2208 br, then\n\nf (x) \u2264 \u03bb.\nhint: use the optional stopping theorem as in exercise 23.6 to find stopping times rm\n\nanalogous to the tm. write\n\nand then let m \u2192 \u221e.\n\nf (x) \u2265 e x f (xrm\n\n) \u2265 \u03bb \u2212 (cid:21) f (cid:21)\u221e/m,\n\n23.8 (1) suppose f is bounded and excessive, x \u2208 s, \u03b5 > 0, and c = {y : | f (y) \u2212 f (x)| \u2265 \u03b5}. use\nexercises 23.6 and 23.7 to show that if z \u2208 cr, then | f (z) \u2212 f (x)| \u2265 \u03b5.\n(2) let f , \u03b5, and x be as in (1) and set s = inf{t > 0 : | f (xt ) \u2212 f (x)| \u2265 \u03b5}. use exercise\n20.9 to show that | f (xs ) \u2212 f (x)| \u2265 \u03b5 with px-probability one.\n(3) let f , \u03b5, x, and s be as in (2). define s = 0 and si+1 = si + s \u25e6 \u03b8si. by exercise 23.4,\n) is a positive supermartingale. use corollary a.36 to show si \u2192 \u221e, px-a.s. deduce that\nf (xsi\nwith px-probability one, f (xt ) has paths that are right continuous with left limits.\n\n(4) use exercise 23.1 to show that if f is excessive but not necessarily bounded, then f (xt )\n\nhas paths that are right continuous with left limits.\n\n23.9 (1) show that every continuous function is lower semicontinuous.\n\n(2) show that if f is lower semicontinuous and x \u2208 s, then\n\nf (y) \u2265 f (x).\n\nlim inf\ny\u2192x\n\n(3) show that if fn is a sequence of continuous functions increasing to f , then f is lower\n\nsemicontinuous.\n\n "}, {"Page_number": 209, "text": "191\n23.10 suppose g is non-negative, bounded, and continuous, and assumption 20.1 holds. let g0 = g\nand define gn(x) = supt\u22650 ptgn\u22121(x) for n \u2265 1. prove that gn increases to the least excessive\nmajorant of g.\n\nnotes\n\nnotes\n\nsee \u00f8ksendal (2003) for further information on optimal stopping.\n\nexercise 23.3 shows that e xa\u221e is an excessive function if a is an additive functional. to\na large extent the converse is true: given an excessive function f and some mild conditions,\nf (x) = e xa\u221e for all x. the proof is a\nthere exists an additive functional a such that\nmodification of the doob\u2013meyer decomposition of f (xt ) that takes into account the fact\nthere is a family of probabilities instead of just one; see blumenthal and getoor (1968).\n\nthe optimal stopping problem involving american puts has a theoretical solution: look\nat the least excessive majorant for a certain reward function. the reward function is not just\n(k \u2212 s)+\nbecause the interest earned on the money obtained after the sale of a share of\nstock needs to be taken into account. moreover, the excessive functions here are relative to\nthe space-time process (st , t ), not those relative to st. finding a satisfactory solution to this\noptimal stopping problem is still open and is important.\n\n "}, {"Page_number": 210, "text": "24\n\nstochastic differential equations\n\nstochastic differential equations are used in modeling a wide variety of physical and economic\nsituations, and are one of the main reasons for the interest in stochastic integrals.\n\nwe consider stochastic differential equations (sdes) of the form\n\ndxt = \u03c3 (xt ) dwt + b(xt ) dt,\n\nwhere \u03c3 and b are real-valued functions and w is a one-dimensional brownian motion. we\nalso consider multidimensional analogs of this equation. if x represents the position of a\nparticle, the \u03c3 (xt ) dwt term says that the particle x diffuses like a multiple of brownian\nmotion, but how strong the diffusivity is depends on the location of the particle. the b(xt ) dt\nterm represents a push in one direction or another, the size of the push depending on the\nlocation of the particle.\n\nlet wt be a one-dimensional brownian motion with respect to a filtration {f t} satisfying the\nusual conditions; see chapter 1. we want to consider sdes of the form\n\n24.1 pathwise solutions of sdes\n\n(24.1)\n\n(24.2)\n\nthis means that xt satisfies the equation\n\ndxt = \u03c3 (xt ) dwt + b(xt ) dt,\n(cid:3)\n\n(cid:3)\n\nxt = x0 +\n\nt\n\n\u03c3 (xs) dws +\n\nt\n\n0\n\nb(xs) ds,\n\n0\n\nx0 = x0.\n\nt \u2265 0.\n\n(cid:15)\n\nhere \u03c3 and b are borel measurable functions, the first integral in (24.2) is a stochastic integral\nwith respect to the brownian motion wt, and (24.2) holds almost surely, that is, we can find\n\u03c3 (xs) dws such that for almost all \u03c9,( 24.2) holds for all t. in order to be able\nversions of\nto define the stochastic integral, we require that any solution xt to (24.2) be adapted to the\nfiltration {ft}. if x satisfies (24.2), then x will automatically have continuous paths. we\nwant to consider existence and uniqueness of solutions to the equation (24.2).\n\nt\n0\n\ndefinition 24.1 a stochastic process x will be a pathwise solution to (24.1) if x is adapted\nto the filtration {ft} and (24.2) holds almost surely, where the null set does not depend on t.\nwe say the solution to (24.1) is pathwise unique if whenever x\nt for some t \u2265 0) = 0.\n(cid:3)\n\n(cid:3)\nt is another solution, then\n\n(cid:16)= x\n\n(24.3)\n\np(xt\n\nsometimes pathwise uniqueness is used for a slightly stronger concept: one can let w be\na brownian motion with respect to each of two filtrations {ft} and {f(cid:3)\n}, which are possibly\n\nt\n\n192\n\n "}, {"Page_number": 211, "text": "24.1 pathwise solutions of sdes\n\n193\n}. one then requires (24.3) to hold. we won\u2019t\ndifferent, and one can let x\nneed to use this modification of the definition, and in any case our proof of uniqueness will\nbe equally valid in this situation.\n\nt be adapted to {f(cid:3)\n(cid:3)\n\nt\n\nthe function \u03c3 in (24.1) is called the diffusion coefficient and the function b is called the\ndrift coefficient. \u03c3 tells us the intensity of the noise at a point, and b tells us if there is a push\nin any direction at a given point.\n\nwe will suppose that \u03c3 and b are lipschitz functions: there exists a constant c such that\n\n|\u03c3 (x) \u2212 \u03c3 (y)| \u2264 c|x \u2212 y|,\n\n|b(x) \u2212 b(y)| \u2264 c|x \u2212 y|.\n\n(24.4)\n\nwe also suppose for now that \u03c3 and b are bounded.\n\n(cid:3)\n\n(cid:3)\n\nt\n\ntheorem 24.2 suppose \u03c3 and b are bounded lipschitz functions. then there exists a path-\nwise solution to (24.1) and this solution is pathwise unique.\nproof existence. let x0(t ) = x0 for all t and define xi(t ) recursively by\n\nxi+1(t ) = x0 +\n\nt\n\n\u03c3 (xi(s)) dws +\n\nr\n\n0\n\n0\n\ne sup\nr\u2264t\n\nb(xi(s)) ds.\n\n(cid:16)\n(cid:3)\n(cid:16)\n\nachieve the theorem.\n\n[\u03c3 (xi(s)) \u2212 \u03c3 (xi\u22121(s))] dws\n\n(24.5)\nnote that x0(t ) is trivially adapted to {ft}, and an induction argument shows that xi is adapted\nto {ft} for each i.\nfix t0. we will show existence (and uniqueness) up to time t0; since t0 is arbitrary, this will\nsince (x + y)2 \u2264 2x2 + 2y2, then\n|xi+1(r) \u2212 xi(r)|2 = e\n+\n\u2264 2e\n+ 2e\n\n(cid:10)(cid:3)\n(cid:11)2(cid:17)\nsup\nr\u2264t\n(cid:10)(cid:3)\n(cid:11)2(cid:17)\n[b(xi(s)) \u2212 b(xi\u22121(s))] ds\nr\n(cid:10)(cid:3)\n(cid:11)2(cid:17)\n(cid:16)\n[\u03c3 (xi(s)) \u2212 \u03c3 (xi\u22121(s))] dws\n[b(xi(s)) \u2212 b(xi\u22121(s))] ds\n(cid:3)\n(cid:3)\n\nby doob\u2019s inequalities (theorem 3.6) and the fact that \u03c3 is a lipschitz function, the first\n\n[\u03c3 (xi(s)) \u2212 \u03c3 (xi\u22121(s))] dws\n\nterm after the inequality is bounded by\n\n(cid:16)(cid:10)(cid:3)\n\n(cid:11)2(cid:17)\n\nsup\nr\u2264t\n\nsup\nr\u2264t\n\nt\n\n0\n\n8e\n\n.\n\n0\n\n0\n\n0\n\n0\n\n0\n\nr\n\nr\n\nt\n\n[\u03c3 (xi(s)) \u2212 \u03c3 (xi\u22121(s))]2 ds\n|xi(s) \u2212 xi\u22121(s)|2 ds.\n\nby the cauchy\u2013schwarz inequality, the fact that t \u2264 t0, and the fact that b is a lipschitz\nfunction, the second term is bounded by\n|b(xi(s)) \u2212 b(xi\u22121(s))| ds\n\n(cid:10)(cid:3)\n\n2e\n\nt\n\nt\n\n0\n\n0\n\n0\n\nt\n\n|b(xi(s)) \u2212 b(xi\u22121(s))|2 ds\n|xi(s) \u2212 xi\u22121(s)|2 ds.\n\n= 8e\n\u2264 ce\n(cid:3)\n\n(cid:11)2 \u2264 2t0e\n(cid:3)\n\n\u2264 ce\n\nt\n\n0\n\n "}, {"Page_number": 212, "text": "194\n\ntherefore\n\ne sup\nr\u2264t\nlet gi(t ) = e supr\u2264t\nfor t \u2264 t0 and\n\nstochastic differential equations\n\n(cid:3)\n\n|xi+1(r) \u2212 xi(r)|2 \u2264 ce\n\nt\n\n|xi(s) \u2212 xi\u22121(s)|2 ds.\n\n0\n\n(24.6)\n\n|xi(r)\u2212xi\u22121(r)|2. thus provided we choose a big enough, g1(t ) \u2264 a\n\n(cid:3)\n\nt \u2264 t0.\n\nt\n\n0\n\ngi(s) ds,\n\ngi+1(t ) \u2264 a\n(cid:3)\n(clearly |xi+1(t ) \u2212 xi(t )|2 \u2264 supr\u2264t\n|xi+1(r) \u2212 xi(r)|2.) thus\ng1(s) ds \u2264 a\ng2(t ) \u2264 a\n(cid:3)\n(cid:3)\n\n(cid:3)\n\n0\n\n0\n\nt\n\nt\n\na ds = a2t,\n\ng3(t ) \u2264 a\n\nt\n\ng2(s) ds \u2264 a\n\nt\n\na2s ds = a3t2/2,\n\nand continuing by induction,\n\n0\n\n0\n\ngi(t ) \u2264 aiti\u22121/(i \u2212 1)!\n\nexercise 24.1 asks you to show that if we define\n(cid:21)y(cid:21)t = (e sup\nr\u2264t\n\n(24.7)\nwhen y is a stochastic process, then (cid:21)y(cid:21)t is a norm and the corresponding metric is complete.\nhence\n\n|yr|2)1/2\n\n|xn(s) \u2212 xm(s)|2)1/2 = (cid:21)xn \u2212 xm(cid:21)t0\n\n(e sup\nr\u2264t0\n\n\u2264 n\u22121(cid:9)\n\u2264 n\u22121(cid:9)\n\ni=m\n\n(cid:21)xi+1 \u2212 xi(cid:21)t0\n\n(gi(t0))1/2\n\n(cid:12)\ncan be made small by taking m, n large. (we use the ratio test to show that the sum\n/(i \u2212 1)!)1/2 converges.) we have e x0(t )2 < \u221e. by the completeness of (cid:21) \u00b7 (cid:21)t0\n(aiti\u22121\n|xn(s) \u2212 xs|2 \u2192 0 as n \u2192 \u221e. this implies there exists\nthere exists xt such that e sups\u2264t0\na subsequence {n j} such that sups\u2264t0\n(s) \u2212 xs|2 \u2192 0 almost surely; since each xn j is\ncontinuous in t, then xt is also. taking a limit in (24.5) as n \u2192 \u221e shows xt satisfies (24.2).\n\n|xn j\n\ni=m\n\n0\n\nuniqueness. suppose xt and x\n\n(cid:3)\nt are two solutions to (24.2). let\ng(t ) = e sup\nr\u2264t\n\n|xr \u2212 x\n\n|2.\n\n(cid:3)\nr\n\n "}, {"Page_number": 213, "text": "24.1 pathwise solutions of sdes\n\n|xr \u2212 x\n\n(cid:3)\nr\n\ne sup\nr\u2264t\n\nvery similarly to the existence proof, e supr\u2264t\nand\n\n(cid:16)\n\nsup\nr\u2264t\n\n(cid:16)\n|2 \u2264 2e\n(cid:3)\n+ 2e\n\u2264 ce\n\nr\n\n,\n\n195\n|xr|2 < \u221e, the same with x replaced by x\n(cid:10)(cid:3)\n(cid:3)\n(cid:10)(cid:3)\n[\u03c3 (xs) \u2212 \u03c3 (x\n[b(xs) \u2212 b(x\n|2 ds.\n\n)] dws\n\n)] ds\n\n(cid:3)\ns\n\n(cid:3)\ns\n\n0\n\nr\n\n(cid:11)2(cid:17)\n(cid:11)2(cid:17)\n(cid:15)\n\nt\n\n0\n\nsup\nr\u2264t\n|xs \u2212 x\n(cid:15)\ntherefore there exists a > 0 such that g(t ) is bounded by a and g(t ) \u2264 a\n0 a2s ds = a3t2/2, etc. thus we have\ng(t ) \u2264 aiti\u22121/(i \u2212 1)! for all i, which is only possible if g(t ) = 0. this implies that\nxt = x\n\n(cid:15)\n0 a ds = a2t, g(t ) \u2264 a\n\nt for all t \u2264 t0, except for a null set.\n(cid:3)\n\nthen g(t ) \u2264 a\n\nt\n0 g(s) ds.\n\n(cid:3)\ns\n\n0\n\nt\n\nt\n\nwe also want to consider the sde (24.1) when \u03c3 and b are lipschitz functions, but not\nnecessarily bounded. note |\u03c3 (x)| \u2264 |\u03c3 (0)| + c|x|, so that |\u03c3 (x)| is less than or equal to\nc(1 + |x|), and the same for b.\ntheorem 24.3 suppose \u03c3 and b are lipschitz functions, but not necessarily bounded. then\nthere exists a pathwise solution to (24.1) and this solution is pathwise unique.\n\nproof let \u03c3n and bn be bounded lipschitz functions that agree with \u03c3 and b, respectively,\non [\u2212n, n]. let xn be the unique pathwise solution to (24.1) with \u03c3 and b replaced by \u03c3n and\nbn, respectively. let tn = inf{t : |xn(t )| \u2265 n}. we claim xn(t ) = xm(t ) if t \u2264 tn \u2227 tm; to\nprove this, let g(t ) = e sups\u2264t\u2227tn\u2227tm\n|xn(s) \u2212 xm(s)|2, and proceed as in the uniqueness part\nof the proof of theorem 24.2. we then have existence and uniqueness of the sde for t \u2264 tn\nfor each n.\n\nto complete the proof, it suffices to show tn \u2192 \u221e. let\n|xn(s)|2.\n\nthen\n\nhn(t ) = e sup\n(cid:10)(cid:3)\ns\u2264t\u2227tn\n(cid:3)\n\nt\n\n\u03c3n(xn(s)) dws\n(cid:3)\n\u03c3n(xn(s))2 ds + ct0e\n\nt\n\n|xn(s)|2 ds\n\n0\n\n(cid:11)2 + ce\n(cid:3)\n\n(cid:3)\n\nt\n\n0\n\nbn(xn(s))2 ds\n\nt\n\nbn(xn(s))2 ds\n\n0\n\n0\nt\n\nhn(t ) \u2264 c|x0|2 + ce\n\u2264 c|x0|2 + ce\n(cid:3)\n\u2264 c|x0|2 + c + ce\n\u2264 c + c\n\n0\n\nt\n\nhn(s) ds,\n\n0\n\nusing estimates very similar to those of the proof of theorem 24.2. by exercise 24.2,\nhn(t ) \u2264 cect if t \u2264 t0. note the constant c can be chosen to be independent of n. then\n\np(tn < t0) = p(sup\ns\u2264t0\n\n|xn(s)|2\n|xn(s)| \u2265 n) \u2264 e sups\u2264t0\nn2\n\n\u2264 hn(t0)\nn2\n\n\u2192 0\n\nas n \u2192 \u221e. since t0 is arbitrary, tn \u2192 \u221e, a.s.\n\n "}, {"Page_number": 214, "text": "196\n\nstochastic differential equations\n\nalthough we considered one-dimensional sdes for simplicity, the same arguments apply\n\nwhen we have higher-dimensional sdes. let\n\nw = (w 1, . . . , w d )\n\nbe a d-dimensional brownian motion, let \u03c3i j (x) be bounded lipschitz functions for i =\n1, . . . , n and j = 1, . . . , d, and let bi(x) be bounded lipschitz functions for i = 1, . . . , n.\nconsider the system of equations\n\ndx i\nt\n\n\u03c3i j (xt ) dw j\n\nt + bi(xt ) dt,\n\ni = 1, . . . , n.\n\n(24.8)\n\n= d(cid:9)\n\nj=1\n\nthis is frequently written in matrix form\n\ndxt = \u03c3 (xt ) dwt + b(xt ) dt\n\n(24.9)\nwhere we view x = (x 1, . . . , x n) as a n \u00d7 1 matrix, b = (b1, . . . , bn) as a n \u00d7 1 matrix-\nvalued function, w as a d \u00d7 1 matrix, and \u03c3 as a n \u00d7 d matrix-valued function. we have\nexistence and uniqueness to the system (24.8). exercise 24.5 asks you to prove this in the\ncase when n = d, although there is nothing at all special about requiring n = d.\n\n24.2 one-dimensional sdes\n\nalthough our proof of pathwise existence and uniqueness was for sdes in one dimension,\nas is pointed out in exercise 24.5, almost the same proof works in higher dimensions. in\nthis section we look at a pathwise uniqueness result that is valid only for sdes on r. the\nequation we look at is the same as the one in the last section, namely,\n\n(cid:3)\n\nxt = x0 +\n\nt\n\n\u03c3 (xs) dws +\n\nt\n\n0\n\nb(xs) ds.\n\n(24.10)\n\ntheorem 24.4 suppose b is bounded and lipschitz. suppose there exists a continuous\nfunction \u03c1 : [0,\u221e) \u2192 [0,\u221e) such that \u03c1(0) = 0,\n\n(cid:3)\n\n0\n\n(cid:3) \u03b5\n\n\u03c1\u22122(u) du = \u221e\n\n(24.11)\n\nfor all \u03b5 > 0, and \u03c3 is bounded and satisfies\n\n|\u03c3 (x) \u2212 \u03c3 (y)| \u2264 \u03c1(|x \u2212 y|)\n\n0\n\nfor all x and y. then the solution to (24.10) is pathwise unique.\n\nfor an example, let b(x) = 0 for all x, and let \u03c3 be h\u00a8older continuous of order \u03b1, that is,\nthere exists c such that |\u03c3 (x) \u2212 \u03c3 (y)| \u2264 c|x \u2212 y|\u03b1. then we take \u03c1(x) = x\u03b1, and the integral\ncondition in the theorem is satisfied if and only if \u03b1 \u2265 1/2. if (24.11) holds for all \u03b5 > 0,\nwe say the yamada\u2013watanabe condition holds.\n\ninstead of proving this theorem right away and then essentially repeating the proof to give\na comparison theorem, we will state and prove a comparison theorem (theorem 24.5) and\nthen obtain theorem 24.4 as a corollary of theorem 24.5.\n\nwe only prove the uniqueness of the solution to (24.10) here. the existence is a conse-\n\nquence of some measure-theoretic magic; see revuz and yor (1999), theorem ix.1.7.\n\n "}, {"Page_number": 215, "text": "24.2 one-dimensional sdes\n\n197\n\ntheorem 24.5 suppose \u03c3 satisfies the conditions in theorem 24.4. suppose xt satisfies\n(24.10) with b a lipschitz function. suppose yt is a continuous semimartingale satisfying\n\n(cid:3)\n\n0\n\n(cid:3)\n\n0\n\nyt \u2265 y0 +\n\nt\n\n\u03c3 (ys) dws +\n\nt\n\nb(ys) ds,\n\nwhere b is a borel measurable function and b(z) \u2265 b(z) for all z. if y0 \u2265 x, a.s., then yt \u2265 xt\nalmost surely for all t.\nproof let an \u2193 0 be selected so that(cid:3)\n\nan\u22121\n\n(\u03c1(u))\u22122 du = n.\n\nan\n\n(cid:15)\n\n(cid:15)\n\n(cid:3)\n\n(cid:15)\n\n\u03c1(x)\u22122 dx increases to in-\nthis can be done inductively. choose a0 arbitrarily. since\nfinity as r \u2192 0, we can choose a1 such that\n\u03c1(x)\u22122 dx = 1; in a similar man-\nner we choose a2, a3, . . .. let hn be continuous, supported in (an, an\u22121), 0 \u2264 hn(u) \u2264\nhn(u) du = 1 for each n. the idea here is to start with the function\n2/n\u03c12(u), and\n(1 + \u03b5)1(an,an\u22121 )(u)/(n\u03c1(u)2) for some small \u03b5, and then modify this near an and an\u22121 to get\na function that is continuous, is supported in (an, an\u22121), and integrates to 1. let fn be such\nthat fn(0) = f\n\nan\u22121\nan\n\na0\na1\n\na0\nr\n\n(cid:3)(cid:3)\nn\n\n(cid:3)\nn\n\n(0) = 0 and f\n(cid:3)\nn\n\n(cid:3)\n= hn. note\n(cid:3)(cid:3)\nn\n\nu\n\n(u) =\n\nf\nf\n(u) \u2264 1 and f\n\n0\n\n(cid:3)\nn\n\n(cid:3)\nn\n\n(u) \u2265 0, so 0 \u2264 f\n\n(cid:3)\nn\n\nand f\neach u \u2265 0.\n\nsince x \u2264 y, then fn(x \u2212 y) = 0, and we have by it\u02c6o\u2019s formula\n\nu\n\n0\n\nhn(s) ds \u2264 1\n\n(s) ds =\n(u) = 1 if u \u2265 an\u22121. hence fn(u) \u2191 u as n \u2192 \u221e for\n(cid:3)\n\nfn(xt \u2212 yt ) = martingale +\n\nt\n\n(cid:3)\nn\n\nf\n\n(xs \u2212 ys)[b(xs) \u2212 b(ys)] ds\n\n(24.12)\n\n0\n\n(xs \u2212 ys)[\u03c3 (xs) \u2212 \u03c3 (ys)]2 ds.\n\nwe take expectations of both sides. the martingale term has 0 expectation. the final term\non the right-hand side is bounded in expectation by\n\nt\n\n1\n2\n\n2\n\ne\n\nn(\u03c1|xs \u2212 ys|)2\nby the assumptions on \u03c3 and the bound on f\nexpectation of the second term on the right of (24.12) is bounded above by\n\n(\u03c1|xs \u2212 ys|)2 ds \u2264 t\nn\n= hn, and so goes to 0 as n \u2192 \u221e. the\n(cid:3)(cid:3)\n(cid:3)\nn\n\n(cid:3)\n\n0\n\n(xs \u2212 ys)[b(ys) \u2212 b(ys)] ds\n\n(cid:3)\nn\n\nt\n\nf\n\n0\n\ne\n\nt\n\n(cid:3)\nn\n\n0\n\nf\n\u2264 ce\n= ce\n\nt\n\n(cid:3)\n(xs \u2212 ys)[b(xs) \u2212 b(ys)] ds + e\n(cid:3)\n(1[0,\u221e)(xs \u2212 ys))|xs \u2212 ys| ds\n(xs \u2212 ys)+\n\nds.\n\n0\n\nt\n\n0\n\n(cid:3)\n\n+ 1\n\n2\n\nt\n\n(cid:3)(cid:3)\nn\n\nf\n\n0\n\n(cid:3)\n\n "}, {"Page_number": 216, "text": "198\n\nletting n \u2192 \u221e,\n\nstochastic differential equations\n\ne (xt \u2212 yt )+ \u2264 c\n\nif we set g(t ) = e (xt \u2212 yt )+\n\n, we have\n\ng(t ) \u2264 c\n\nt\n\ne (xs \u2212 ys)+\n\nds.\n\nt\n\ng(s) ds,\n\n(cid:3)\n(cid:3)\n\n0\n\nand by exercise 24.2 we conclude g(t ) = 0 for each t. using the continuity of the paths of\nxt and yt completes the proof.\n\n0\n\nwe now prove theorem 24.4.\n\nproof of theorem 24.4 suppose x and x\n24.5 with y = x\n(cid:3)\nx\n\nand b = b, we have xt \u2264 x\n(cid:3)\n\n\u2264 xt for all t, which completes the proof.\n\nare two solutions to (24.10). then by theorem\n(cid:3)\nt for all t. applying this argument with x and\n\nreversed yields x\nt\n\n(cid:3)\n\n(cid:3)\n\n24.3 examples of sdes\n\nornstein\u2013uhlenbeck process\nthe ornstein\u2013uhlenbeck process is the solution to the sde\n\ndxt = dwt \u2212 xt\n2\n\ndt,\n\nx0 = x.\n\n(24.13)\n\nthe existence and uniqueness follow by theorem 24.3. note that the drift coefficient is not\nbounded, so theorem 24.2 is not sufficient. the process behaves like a brownian motion,\nwith a drift that pushes the process towards the origin; the farther the process gets from the\norigin, the stronger the push.\n\nthe equation (24.13) can be solved explicitly. rearranging, multiplying by et/2, and using\n\nthe product rule,\n\nso\n\nor\n\nd[et/2xt] = et/2 dxt + et/2 xt\n(cid:3)\n2\n\net/2xt = x0 +\n\nes/2 dws,\n\ndt = et/2 dwt ,\n\nt\n\n(cid:3)\n\n0\n\nxt = e\n\n\u2212t/2x + e\n\n\u2212t/2\n\nt\n\nes/2 dws.\n\n(24.14)\nwe used here the fact that the martingale part of the semimartingale zt = et/2 is zero, and\ntherefore (cid:22)z, w(cid:23)\n= 0. by exercise 24.6, xt is a gaussian process and the distribution of xt\n(es/2)2 ds =\nis that of a normal random variable with mean e\n1 \u2212 e\n0 es/2 dws and vt = ylog(t+1), then yt is a mean-zero continuous gaussian\n\nif we let yt =(cid:15)\n\n\u2212t\n\u2212t/2x and variance equal to e\n\n\u2212t.\n\n(cid:15)\n\nt\n0\n\n0\n\nt\n\nt\n\nprocess with independent increments, and hence so is vt. since\n\n(cid:3)\n\nvar (vu \u2212 vt ) =\n\nlog(u+1)\n\nlog(t+1)\n\nes ds = u \u2212 t,\n\n "}, {"Page_number": 217, "text": "24.3 examples of sdes\n\n199\n\nthen vt is a brownian motion. hence\nxt = e\n\n\u2212t/2x + e\n\n\u2212t/2v (et \u2212 1).\n\nthis representation of an ornstein\u2013uhlenbeck process in terms of a brownian motion is\nuseful for, among other things, calculating the exit probabilities of a square root boundary.\n\nlinear equations\nwe consider the linear equation\n\ndxt = axt dwt + bxt dt,\n\nx0 = x0,\n\n(24.15)\n\nwhere a and b are constants. one place this comes up is in models of stock prices in financial\nmathematics; see chapter 28. we have pathwise existence and uniqueness by theorem 24.3;\nhere both the diffusion and drift coefficients are unbounded.\n\nwe will give a candidate for the solution, and verify that it solves (24.15). by the pathwise\n\nuniqueness, this will then be the only solution. our candidate is\n\nxt = x0eawt+(b\u2212a2/2)t .\n\nto verify that this is a solution, we use it\u02c6o\u2019s formula with the process awt + (b\u2212 a2/2)t and\nthe function ex:\n\n(cid:3)\n(cid:3)\n\n(cid:3)\n(cid:3)\nxt = x0 +\n(cid:3)\n+ 1\n(cid:3)\n= x0 +\n= x0 +\n\n2\n\n0\n\n0\nt\n\n0\n\n0\n\nt\n\neaws+(b\u2212a2/2)sa dws +\neaws+(b\u2212a2/2)sa2 ds\neaws+(b\u2212a2/2)sa dws +\naxs dws +\n\n(cid:3)\n\nbxs ds.\n\nt\n\nt\n\nt\n\n0\n\nt\n\neaws+(b\u2212a2/2)s(b \u2212 a2/2) ds\n\n0\n\nt\n\n0\n\neaws+(b\u2212a2/2)sb ds\n\nlet us summarize our discussion.\n\nproposition 24.6 the unique pathwise solution to\n\nis\n\ndxt = axt dwt + bxt dt\n\nxt = x0eawt+(b\u2212a2/2)t .\n\nif we write zt = awt + bt, then (24.15) becomes\n\ndxt = xt dzt ,\n\nx0 = x0.\n\n(24.16)\n\nthe equation (24.16) makes sense for arbitrary continuous semimartingales z, and by using\nit\u02c6o\u2019s formula as above, one can see that a solution is xt = x0ezt\u2212(cid:22)z(cid:23)t /2.\n\n "}, {"Page_number": 218, "text": "200\n\nstochastic differential equations\n\nbessel processes\nwe consider bessel processes and the squares of bessel processes. the reason for the name\nis that these processes turn out to be markov processes and the infinitesimal generator of the\nsemigroup (see chapter 37) is related to bessel\u2019s equation, a type of differential equation.\n\na bessel process of order \u03bd \u2265 2 is defined to be a solution of the sde\n\ndxt = dwt + \u03bd \u2212 1\n\nx0 = x.\n\n(24.17)\nbessel processes of order 0 \u2264 \u03bd < 2 can also be defined using (24.17), but only up until the\nfirst time the process x reaches 0; some extra information needs to be given as to what the\nprocess does at 0. the square of a bessel process of order \u03bd \u2265 0 is defined to be the solution\nto the sde\n\n2xt\n\ndt,\n\ndyt = 2\n\n|yt| dwt + \u03bd dt,\n\ny0 = y.\n\n(24.18)\n\n(cid:22)\n\nthere is no difficulty defining the square of a bessel process for 0 \u2264 \u03bd < 2.\nby theorem 24.4 we have pathwise uniqueness for the solution to (24.18), because\n||y|1/2 \u2212 |x|1/2|\u2264 | y \u2212 x|1/2, and we can thus take \u03c1(u) = 2u1/2 in theorem 24.4. the\nsolution to (24.18) when \u03bd = 0 and y = 0 is clearly yt = 0 for all t. by theorem 24.5 with\nb(x) = \u03bd and b(x) = 0, we see that the solution to (24.18) is greater than or equal to 0 for\nall t. we may thus omit the absolute value in (24.18) and rewrite it as\n\ndyt = 2\n\n\u221a\nyt dwt + \u03bd dt,\n\ny0 = y.\n\n(24.19)\n\nyt solves (24.17) for t up until the first time y reaches 0; the function\n\nif we apply it\u02c6o\u2019s formula to the solution yt of (24.19) with the function\n\n\u221a\nxt = \u221a\n\u221a\nx, we see that\nx is twice\ncontinuously differentiable as long as we stay away from 0. we will see shortly that the square\nof a bessel process started away from 0 never hits 0 if and only if \u03bd \u2265 2.\nusing it\u02c6o\u2019s formula with a d-dimensional process wt and the function |x|2 shows that the\nsquare of the modulus of a d-dimensional brownian motion is the square of a bessel process\nof order d; this is exercise 24.7.\n\nbessel processes have the same scaling properties as brownian motion. that is, if xt is a\nbessel process of order \u03bd started at x, then axa\u22122t is a bessel process of order \u03bd started at ax.\nin fact, from (24.17),\n\nd (axa\u22122t ) = a dwa\u22122t + a2\n\n\u03bd \u2212 1\n2axa\u22122t\n\n\u22122t ),\n\nd (a\n\nand the assertion follows from the uniqueness of the solution to (24.17) and the fact that\naw (a\n\n\u22122t ) is again a brownian motion.\n\nbessel processes are useful for comparison purposes, and so the following is worthwhile.\nproposition 24.7 suppose yt is the square of a bessel process of order \u03bd. suppose y0 = y.\nthe following hold with probability one.\n(1) if \u03bd > 2 and y > 0, yt never hits 0.\n(2) if \u03bd = 2 and y > 0, yt hits every neighborhood of 0, but never hits the point 0.\n(3) if 0 < \u03bd < 2, yt hits 0.\n(4) if \u03bd = 0, then yt hits 0. if started at 0, then yt remains at 0 forever.\n\n "}, {"Page_number": 219, "text": "exercises\n\n201\nwhen we say that yt hits 0, we consider only times t > 0. we define t0 = inf{t > 0 : yt = 0}\nand say that yt hits 0 if t0 < \u221e.\nproof we prove (2). an application of it\u02c6o\u2019s formula with the process being the square of\na bessel process of order 2 and the function being log x shows that log yt is a martingale up\nuntil the first hitting time of 0; cf. exercise 21.1. the quadratic variation of log yt is\nds\nfor t less than the hitting time of 0. suppose 0 < a < y < b.\n\u22122t \u2192 \u221e as t \u2192 \u221e.\nsince log yt is a martingale, it is a time change of brownian motion, and brownian motion\nleaves [log a, log b] with probability one, a contradiction.\n\nwe claim that yt leaves the interval [a, b], a.s. if not, (cid:22)log y(cid:23)\n\n\u22122\nt\n0 y\ns\n\n\u2265 b\n\n(cid:15)\n\nt\n\nthen by corollary 3.17,\n\np(yt hits a before b) = log b \u2212 log y\nlog b \u2212 log a\n\n(24.20)\nletting b \u2192 \u221e, we see that p(yt hits a) = 1, and since a is arbitrary, yt hits every neighbor-\nhood of 0. if in (24.20) we hold b fixed instead and let a \u2192 0, we see p(yt hits 0 before b) = 0;\nsince b is arbitrary, this proves that yt never hits the point 0.\nparts (1), (3), and (4) are similar, but instead of log|x| we use |x|(2\u2212\u03bd )/2. the details are\n\n.\n\nleft as exercise 24.8.\n\nexercises\n\n(cid:3)\n\nt\n\n(cid:3)\n\ng(t ) \u2264 a + b\n(cid:16)\n\n(cid:3)\n\nt\n\n24.1 show that (cid:21) \u00b7 (cid:21)t defined by (24.7) gives rise to a complete normed linear space.\n24.2 suppose g(t ) is non-negative and bounded on each finite subinterval of [0,\u221e). suppose there\n\nexist constants a and b such that\n\n(24.21)\nfor each t \u2265 0. prove that g(t ) \u2264 aebt for all t \u2265 0. this result is known as gronwall\u2019s lemma.\n\ng(s) ds\n\n0\n\nhint: write\n\ng(t ) \u2264 a + b\n\na + b\n\ns\n\ng(r) dr\n\nds,\n\nuse (24.21) to substitute for g(r), and iterate.\n\n0\n\n0\n\n(cid:17)\n\n24.3 the starting point in (24.1) can be random. suppose y is a random variable that is measurable\nwith respect to f0, y is square integrable, and \u03c3 and b are bounded and lipschitz. prove pathwise\nexistence and uniqueness for the equation\n\nxt = y +\n\nt\n\n\u03c3 (xs ) dws +\n\n0\n\nt\n\n0\n\nb(xs ) ds.\n\n24.4 the functions \u03c3 and b in (24.1) can depend on time as well as space. suppose \u03c3 : [0,\u221e)\u00d7 r \u2192\nr, b : [0,\u221e)\u00d7 r \u2192 r are bounded and uniformly lipschitz in the second variable: there exists\nc independent of s such that |\u03c3 (s, x) \u2212 \u03c3 (s, y)| \u2264 c|x \u2212 y| and similarly for b. prove pathwise\nexistence and uniqueness for the equation\n\n(cid:3)\n\n(cid:3)\n\n(cid:3)\n\n(cid:3)\n\nxt = x0 +\n\nt\n\n\u03c3 (s, xs ) dws +\n\n0\n\nt\n\n0\n\nb(s, xs ) ds.\n\n "}, {"Page_number": 220, "text": "202\n\nstochastic differential equations\n\n24.5 here is a multidimensional analog of (24.1). suppose the functions \u03c3i j\n\nt\n\nt\n\n(cid:3)\n\n) satisfies\n\n, . . . , x )d )\n\n: rd \u2192 r, 1 \u2264\n: rd \u2192 r, i = 1, . . . , d, are bounded and\n), and\n\ni, j \u2264 d, are bounded and lipschitz, and bi\nlipschitz, w j are independent one-dimensional brownian motions, x0 = (x(1)\nxt = (x (1)\nd(cid:9)\n(cid:15) \u221e\n(cid:3) \u221e\n\nfor i = 1, . . . , d. prove pathwise existence and uniqueness for this system of equations.\n\nf (t ) dwt is a mean zero gaussian random variable, the same with f replaced by g, and\n\n(cid:15) \u221e\n0 g(t )2 dt < \u221e. show that\n\nf (t )2 dt < \u221e and\n\n(cid:10)(cid:3) \u221e\n\n\u03c3i j (xs ) dw j\ns\n\n(cid:3) \u221e\n\n, . . . , x(d )\n0\n\n(cid:15) \u221e\n\n= x(i)\n\nbi(xs ) ds\n\n(24.22)\n\nx (i)\nt\n\n(cid:11)\n\n(cid:3)\n\nj=1\n\n+\n\n+\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\nt\n\nt\n\n24.6 suppose f and g map [0,\u221e) \u2192 r with\n\ncov\n\n0\n\nf (t ) dwt ,\n\n0\n\ng(t ) dwt\n\nf (t )g(t ) dt.\n\n0\n\n=\n\nhint: approximate f and g by piecewise constant deterministic functions.\n\n24.7 show that if wt is a d-dimensional brownian motion, then |wt|2 is the square of a bessel process\n\nof order d.\n\n24.8 prove (1), (3), and (4) of proposition 24.7.\n24.9 let x be the solution to dxt = \u03c3 (xt ) dwt + b(xt ) dt, where w is a one-dimensional brownian\nmotion, \u03c3 and b are lipschitz continuous real-valued functions, and |\u03c3 (x)| \u2264 c(1 + |x|) and\n|b(x)| \u2264 c(1 + |x|). let t0 > 0. prove that if p \u2265 2, then\n\n|xs|p] \u2264 c(1 + |x0|p).\n\ne [sup\ns\u2264t0\n\n24.10 let w be a one-dimensional brownian motion and let x x\ndxt = \u03c3 (xt ) dwt + b(xt ) dt,\nfunctions and that \u03c3 and b and all their derivatives are bounded. show\nis continuous in x with probability one. show that the map is\n\nt be the solution to\n\nx0 = x.\n\n\u221e\n\nsuppose \u03c3 and b are c\nthat for each t the map x \u2192 x x\ndifferentiable in x.\n\nt\n\n24.11 suppose a(t ) and b(t ) are deterministic functions of t. find an explicit solution to the one-\n\ndimensional sde\n\ndxt = a(t ) dwt + b(t ) dt,\n\nx0 = x.\n\nnotes\n\nif one wants to have a stochastic differential equation with jumps, besides a brownian motion,\none integrates with respect to a poisson point process, which is defined in chapter 18. using\nthe notation of that chapter, one considers the stochastic differential equation\n\ndxt = \u03c3 (xt\u2212) dwt + b(xt\u2212) dt\n\n(cid:3)\n\n+\n\nf (xt\u2212, z) (\u03bc(dt, dz) \u2212 \u03bd(dt, dz)),\n\nx0 = x0,\n\ns\n\n "}, {"Page_number": 221, "text": "which means that we want a solution to\n\n(cid:3)\n(cid:3)\n\n0\n\nt\n\nt\n\ns\n\n0\n\n(cid:3)\nxt = x0 +\n\n+\n\nnotes\n\n(cid:3)\n\nt\n\n\u03c3 (xs\u2212) dws +\nf (xs\u2212, z) (\u03bc(ds, dz) \u2212 \u03bd(ds, dz)).\n\nb(xs\u2212) ds\n\n0\n\n203\n\nthere is pathwise existence and uniqueness to this sde provided f satisfies a suitable\nlipschitz-like condition; see skorokhod (1965).\n\n "}, {"Page_number": 222, "text": "25\n\nweak solutions of sdes\n\nin chapter 24 we considered sdes of the form\n\ndxt = \u03c3 (xt ) dwt + b(xt ) dt,\n\n(25.1)\n\nwhere w is a brownian motion and \u03c3 and b are lipschitz functions, or in one dimension,\nwhere \u03c3 has a modulus of continuity satisfying an integral condition. when the coefficients\n\u03c3 and b fail to be sufficiently smooth, it is sometimes the case that (25.1) may not have a\npathwise solution at all, or it may not be unique. we define another notion of existence and\nuniqueness that is useful.\n\ndefinition 25.1 a weak solution (x , w, p) to (25.1) exists if there exists a probability\nmeasure p and a pair of processes (xt , wt ) such that wt is a brownian motion under p\nand (25.1) holds. there is weak uniqueness holding for (25.1) if whenever (x , w, p) and\n(cid:3), p(cid:3)) are two weak solutions, then the joint law of (x , w ) under p and the joint law\n(cid:3), w\n(x\n(cid:3), w\nof (x\nare equal. when this happens, we also say that the solution to (25.1) is\nunique in law.\n\n(cid:3)) under p(cid:3)\n\nlet us discuss the relationship between weak solutions and pathwise solutions. if the\nsolution to (25.1) is pathwise unique, then weak uniqueness holds. for a proof of this result\nunder very general hypotheses, see revuz and yor (1999), theorem ix.1.7. in the case that\n\u03c3 and b are lipschitz functions, the proof is much simpler.\nproposition 25.2 suppose \u03c3 and b are bounded lipschitz functions and x0 \u2208 rd. then weak\nuniqueness holds for (25.1).\n\nproof for notational simplicity we consider the case of dimension one. suppose (x , w, p)\nand (x\n\n(cid:3), p(cid:3)) are two weak solutions to (25.1). let x0(t ) = x0 and define xi+1(t ) by\n\n(cid:3), w\n\n(cid:3)\n\n(cid:3)\n\nxi+1(t ) = x0 +\n\nt\n\n\u03c3 (xi(s)) dws +\n\n0\n\nt\n\n0\n\nb(xi(s)) ds.\n\n(25.2)\n\nwe saw by the proof of theorem 24.2 that the limit of the xi exists, uniformly over finite\ntime intervals, and solves (25.1), and the solution is pathwise unique. since x also solves\n(25.1), we conclude that xi converges (uniformly over finite time intervals) to x , a.s., with\n(cid:3)\nrespect to p. similarly, if we let x\n0\n(t ) = x0 +\n\n(cid:3)\n(cid:3)\n(t ) = x0 and define x\n(cid:3)\ni+1\n+\nt\n\n(s)) ds,\n\n(25.3)\n\n(t ) by\n\n(cid:3)\n(s)) dw\ns\n\n(cid:3)\ni+1\nx\n\n\u03c3 (x\ni\n\nb(x\ni\n\n(cid:3)\n\n(cid:3)\n\nt\n\nthen x\n\n(cid:3)\ni converges, uniformly over finite time intervals, to x\n\n(cid:3)\n\n.\n\n0\n\n0\n\n204\n\n "}, {"Page_number": 223, "text": "weak solutions of sdes\n\n205\n\nnow since w is a brownian motion under p and w\n, then\n(cid:3)\nthe law of (x0, w ) under p equals the law of (x\n, w\n. by( 25.2) and (25.3), the\n0\n(cid:3)\n(cid:3)) under p(cid:3)\nlaw of (x1, w ) under p equals the law of (x\n, and iterating, the law of (xi, w )\n, w\n1\n(cid:3)) under p(cid:3)\n. passing to the limit, the law of (x , w ) under p\nunder p equals the law of (x\ni\nequals the law of (x\n\n(cid:3)\nis a brownian motion under p(cid:3)\n(cid:3)) under p(cid:3)\n\n(cid:3)) under p(cid:3)\n\n(cid:3), w\n\n, w\n\n.\n\n(cid:3)\n\nwe now give an example to show that weak uniqueness might hold even if pathwise\nuniqueness does not. let \u03c3 (x) be equal to 1 if x \u2265 0 and \u22121 otherwise. we take b to be\nidentically 0. we consider solutions to\nxt =\n\n\u03c3 (xs) dws.\n\n(25.4)\n\n(cid:3)\n\nt\n\n0\n\n12.1), xt is a brownian motion. given a brownian motion xt and letting wt =(cid:15)\nthen again by l\u00b4evy\u2019s theorem, wt is a brownian motion and xt =(cid:15)\n\nweak uniqueness holds since ifw is a brownian motion under p, then xt must be a martingale,\nand the quadratic variation of x is d(cid:22)x (cid:23)\n= \u03c3 (xt )2 dt = dt; by l\u00b4evy\u2019s theorem (theorem\n1\n\u03c3 (xs ) dxs,\n\u03c3 (xs) dws; thus weak\non the other hand, pathwise uniqueness does not hold. to see this, let yt = \u2212xt. we have\n\nsolutions exist.\n\nt\n0\n\nt\n0\n\nt\n\n(cid:3)\n\nyt =\n\nt\n\n\u03c3 (ys) dws \u2212 2\n\nt\n\n0\n\n0\n\n1{0}(xs) dws.\n\n(25.5)\n\n(cid:3)\n\n(cid:15)\n\nt\n\n0 1{0}(xs) ds; this is 0 almost surely\nthe second term on the right has quadratic variation 4\nbecause we showed in exercise 11.1 that the amount of time brownian motion spends at 0\nhas lebesgue measure 0. therefore y is another pathwise solution to (25.4).\n\nthis example is not satisfying because one would like \u03c3 to be positive and even continuous\nif possible. such examples exist, however. for each \u03b2 < 1/2, barlow (1982) has constructed\nfunctions \u03c3 that are h\u00a8older continuous of order \u03b2 and bounded above and below by positive\nconstants and for which\n\ndxt = \u03c3 (xt ) dwt ,\n\nx0 = x0,\n\n(25.6)\n\nhas a unique weak solution but no pathwise solution exists.\n\nlet us show how the technique of time change can be used to study weak uniqueness. we\n\nconsider the sde (25.6).\nproposition 25.3 if \u03c3 is borel measurable and there exist c2 > c1 > 0 such that c1 \u2264\n\u03c3 (x) \u2264 c2 for all x, then weak existence and weak uniqueness hold for (25.6).\nproof we consider only uniqueness, leaving existence as exercise 25.1. suppose (x , w, p)\n(cid:3), p(cid:3)) are two weak solutions. then xt is a martingale, and as in section 12.2, if\nand (x\nwe set\n\n(cid:3), w\n\nthen mt = x\u03c4t is a brownian motion under p. define a\nm under p is that of a brownian motion, as is that of m\n\nanalogously. the law of\n\n\u03c4t = inf{s : as \u2265 t},\n(cid:3), \u03c4 (cid:3), and m\n(cid:3)\n(cid:3)\nunder p(cid:3)\n.\n\n(cid:3)\n\n0\n\nat =\n\nt\n\n\u03c3 (xs)2 ds,\n\n "}, {"Page_number": 224, "text": "206\n\nnow let\n\n(cid:3)\n\nt\n\n0\n\nbt =\n\n\u03c1t = inf{s : bs \u2265 t}.\n\n(25.7)\n\nsince mt is a brownian motion and \u03c3 is bounded above and below by positive constants,\nthen bt is continuous, strictly increasing, and increases to infinity as t \u2192 \u221e, and the same\nis therefore true of \u03c1t. by a change of variables,\n\nweak solutions of sdes\n\n1\n\n\u03c3 (ms)2 ds,\n(cid:3)\n(cid:3) \u03c4t\n\n1\n\u03c3 (x\u03c4s\n1\n\n0\n\nt\n\n\u03c3 (xu)2\n\n0\n\n(cid:3) \u03c4t\n\nbt =\n=\n\n1\n\n)2 ds =\n\u03c3 (xu)2 du = \u03c4t .\n\n0\n\n\u03c3 (xu)2 dau\n\ntherefore m\u03c1t\n\nthe law of m under p equals the law of m\n\n= x\u03c4 (\u03c1t ) = xt. we have the analogous formulas with primes.\nsince both are brownian motions, so\n(cid:3)) under p(cid:3)\n, and consequently\n. since xt = m\u03c1t and similarly\n. finally, since\n, the joint law of (x , w ) under p equals the joint law\n\nby (25.7) the law of (m, b) under p equals the law of (m\nthe law of (m, \u03c1 ) under p equals the law of (m\nfor x\n\n, we conclude the law of x under p equals the law of x\nt\n(cid:3), w\n0\n\n1\n\u03c3 (xs ) dxs and similarly for w\n(cid:3)) under p(cid:3)\n\nunder p(cid:3)\n(cid:3), b\n(cid:3), \u03c1(cid:3)) under p(cid:3)\n\nwt =(cid:15)\n\nunder p(cid:3)\n\nof (x\n\n.\n\n(cid:3)\n\n(cid:3)\n\n(cid:3)\n\n(cid:3)\n\nwe point out that in the above proof it is essential that one can reconstruct x from m in a\n\nmeasurable way.\n\nwe now use the girsanov theorem to prove weak uniqueness for (25.1).\n\nproposition 25.4 suppose \u03c3 and b are measurable and bounded above and \u03c3 is bounded\nbelow by a positive constant. then weak existence and uniqueness holds for (25.1).\n\n0\n\n(cid:10)\n\n(cid:3)\n\n(cid:11)\n\n0\n\nproof we prove the weak uniqueness, leaving it as exercise 25.2 to prove existence. define\n(cid:10)\n{f t} to be the minimal augmented filtration generated by x ,\nb\n\u03c3\n\n(xs) dws \u2212 1\n\n(cid:11)2\n\n(cid:11)\n\n(xs)\n\nb\n\u03c3\n\nds\n\n,\n\n2\n\nt\n\nt\n\n(cid:3)\n(cid:10)\nunder q, the process(cid:14)wt = wt +(cid:15)\n\nmt = exp\n\n\u2212\n\nand q the probability measure defined by q(a) = e p[mt; a] if a \u2208 ft. by theorem 13.3,\n\nt\n0\n\n(b/\u03c3 )(xs) ds is a brownian motion, and\ndwt + b\n\n= \u03c3 (xt ) d(cid:14)wt .\n\n(xt ) dt\n\n(cid:3)\n\n\u03c3\n\n, q(cid:3)\n\ndxt = \u03c3 (xt )\n(cid:3)\n\n, and (cid:14)w\nanalogously. by proposition 25.3 the law of (x ,(cid:14)w ) under q is\n(cid:3),(cid:14)w\ndefine m\n. let n \u2265 1, t1 < \u00b7\u00b7\u00b7 < tn, and let a1, . . . , an be borel\n(cid:3)) under q(cid:3)\n(cid:3)\n(cid:10)(cid:3)\n(cid:3)\nequal to the law of (x\n\u2208 an} and define b\nsubsets of r. set b = {xt1\n\u2208 a1, . . . , xtn\n(cid:3)\n(cid:3)\n(cid:10)(cid:3)\n(cid:3)\n(cid:11)2\n(cid:10)\n(xs) dws + 1\ndq =\ndp\n(xs) d(cid:14)ws \u2212 1\ndq\n\np(b) =\n=\n\nanalogously. we have\n\n(cid:10)\n(cid:11)\n\n(cid:11)2\n\n(cid:11)\n\n(xs)\n\n(xs)\n\n(cid:3)\n\ndq.\n\nexp\n\nexp\n\ndq\n\nb\n\u03c3\n\nb\n\u03c3\n\nds\n\nds\n\nb\n\nb\n\n0\n\n2\n\n0\n\nt\n\nt\n\nt\n\nt\n\nb\n\u03c3\n\nb\n\n0\n\nb\n\u03c3\n\n2\n\n0\n\n "}, {"Page_number": 225, "text": "exercises\n\n(cid:3)) and the fact that the law of (x ,(cid:14)w ) under q is the\n\n(cid:3),(cid:14)w\n. defining yt = xt \u2212(cid:15)\n\nusing the analogous formula for p(cid:3)(b\n(cid:3)); thus the finite-dimensional\nsame as that of (x\ndistributions of x under p and of x\nare\ncontinuous processes, we conclude from theorem 2.6 that the law of x under p equals the\n, the joint law of (x , y )\nlaw of x\n1\nunder p equals the joint law of (x\n\u03c3 (xs ) dys and similarly for\n, so we obtain our conclusion.\nw\n\nt\n0 b(xs) ds and similarly for y\n(cid:3)) under p(cid:3)\n\n. finally, wt =(cid:15)\n\n, we see that p(b) = p(cid:3)(b\n\nare the same. since both x and x\n\n(cid:3)) under q(cid:3)\n\nunder p(cid:3)\n\nunder p(cid:3)\n\n(cid:3), y\n\n207\n\nt\n0\n\n(cid:3)\n\n(cid:3)\n\n(cid:3)\n\n(cid:3)\n\n(cid:3)\n\nthe procedure of using the girsanov theorem to get rid of the drift also works in higher\ndimensions. however the time change procedure of proposition 25.3 is not nearly as useful\nin higher dimensions as in one dimension. the question of weak uniqueness for the system\nof equations in exercise 24.5 is quite an interesting one; see bass (1997) and stroock and\nvaradhan (1977).\n\n25.1 show weak existence holds under the hypotheses of proposition 25.3.\n\n25.2 show weak existence holds under the hypotheses of proposition 25.4.\n\nexercises\n\n25.3 here is an example of an sde where weak uniqueness does not hold. suppose w is a one-\n\ndimensional brownian motion and \u03b1 \u2208 (0, 1\n\n). let \u03c3 (x) = 1 \u2227 |x|\u03b1. find two solutions to\n\nthat are not equal in law.\n\nhint: one is the solution that is identically zero. the other can be constructed by time changing\n\na brownian motion by the inverse of the increasing process\n(1 \u2227 |xs|2\u03b1 )\u22121 ds.\n\n2\n\nt\n\n2\n\ndxt = \u03c3 (xt ) dwt\n(cid:3)\n\n0\n\n25.4 (1) suppose as and bs are bounded predictable processes with as bounded below by a positive\nconstant. let w be a one-dimensional brownian motion. suppose y is a one-dimensional\nsemimartingale such that\n\ndyt = atyt dwt + bt dt,\n\ny0 = 0.\n\nprove that if t0 > 0 and \u03b5 > 0, there exists a constant c > 0 depending only on t0, \u03b5, and the\nbounds on as and bs such that\n\n|ys| < \u03b5) > c.\n\np(sup\ns\u2264t0\n\n(2) now let w be d-dimensional brownian motion, let x \u2208 rd, and let \u03c3 be a d \u00d7 d matrix-\nvalued function that is bounded and such that \u03c3 \u03c3 t (x) is positive definite, uniformly in x. that\nis, there exists \u0001 > 0 such that for all x,\n\nd(cid:9)\n\ni, j=1\n\nd(cid:9)\n\ni=1\n\nyiy j (\u03c3 (x)\u03c3 t (x))i j \u2265 \u0001\n\ny2\ni\n\n,\n\n(y1, . . . , yd ) \u2208 rd .\n\nlet b be a d \u00d7 1 matrix-valued function that is bounded. let x be the solution to\n\ndxt = \u03c3 (xt ) dwt + b(xt ) dt,\n\nx0 = x.\n\n "}, {"Page_number": 226, "text": "208\n\nweak solutions of sdes\n\nuse it\u02c6o\u2019s formula to find an equivalent expression for |xt \u2212 x|2. then use (1) to prove that if\nt0 > 0 and \u03b5 > 0, there exists a constant c > 0 not depending on x such that\n\n|xs \u2212 x| < \u03b5) > c.\n\npx(sup\ns\u2264t0\n\n25.5 this is the support theorem for solutions to sdes. let x , x, \u03b5, and t0 be as in (2) of exercise\n25.4. suppose \u03c8 : [0, t0] \u2192 rd is a continuous function with \u03c8 (0) = x. use the girsanov\ntheorem to prove that there exists c > 0 such that\n\n|xs \u2212 \u03c8 (s)| < \u03b5) > c.\n\npx(sup\ns\u2264t0\n\n25.6 suppose weak uniqueness holds for the one-dimensional stochastic differential equation\n\ndxt = \u03c3 (xt ) dwt ,\n\nx0 = x,\n\n(25.8)\n(cid:3)\n\nwhere w is a one-dimensional brownian motion. suppose also that there exists a process x\n(cid:3)\n) dwt.\nthat is adapted to the minimal augmented filtration of w with x\n0\nprove that pathwise uniqueness holds for (25.8).\n(cid:3) = f (w ).\n(cid:3)(cid:3), w ) and\n\nhint: show there exists a measurable map f from c[0,\u221e) \u2192 c[0,\u221e) such that x\n(cid:3)(cid:3)\nis another solution to (25.8), then weak uniqueness shows that the laws of (x\n(cid:3), w ) are equal, hence x\n\n(cid:3)(cid:3) = f (w ) = x\n\n= x and dx\n\n= \u03c3 (x\n\nif x\n(x\n\n.\n\n(cid:3)\n\n(cid:3)\n\n(cid:3)\n\nt\n\nt\n\n "}, {"Page_number": 227, "text": "26\n\nthe ray\u2013knight theorems\n\nthe local time of brownian motion, lx\nt , is parameterized by space and time: x and t. ray\nand knight independently discovered that at certain stopping times t , the process x \u2192 lx\nt\nis a markov process.\n\ntimes that work are (1) the first time local time at 0 reaches a level r; (2) an exponential\nrandom variable t that is independent of the brownian motion; and (3) the first time t\nthat brownian motion reaches the level one. we will prove the version of the ray\u2013knight\ntheorems in the last case. we will show that if w is a brownian motion with local times lx\nt\nand\n\nt = inf{t > 0 : wt = 1},\n\nthen the process l1\u2212x\nindexed by x has the same law as the square of a bessel process of\norder 2. we will see in chapter 39 that the square of a bessel process is a markov process.\n\nt\n\nwe will use the following lemma.\n\nlemma 26.1 suppose x ( j)\n\n(cid:10)\n\nt\n\n\u2212\n\n1\n\n0\n\n(cid:3)\n(cid:11)\n, j = 1, 2, are two continuous processes such that\n\n(cid:10)\n\n(cid:11)\n\n(cid:3)\n\n= e exp\n\n\u2212\n\n1\n\n0\n\nf (s)x (2)\n\ns\n\nds\n\ne exp\n\nf (s)x (1)\n\ns\n\nds\n\nt\n\n1\n0\n\n; 0 \u2264 t \u2264 1}, j = 1, 2, are equal.\n\nwhenever f is a non-negative continuous function with support in (0, 1). then the laws of\n{x ( j)\n(cid:15)\n(cid:15)\nproof let \u03d5 be a non-negative continuous function with support in [0, 1] such that\nnow let t1, . . . , tn \u2208 (0, 1), a1, . . . , an > 0, and set f\u03b5 (x) = (cid:12)\n\u03d5(x) dx = 1, and let \u03d5\u03b5 (x) = \u03b5\u22121\u03d5(x/\u03b5), so that the sequence {\u03d5\u03b5} is an approxima-\ntion to the identity. if g is a continuous function and t (cid:16)= 0, then\ng(s)\u03d5\u03b5 (s \u2212 t ) ds \u2192 g(t ).\ni=1 ai\u03d5\u03b5 (x \u2212 ti). using the\nhypothesis and letting \u03b5 \u2192 0, we obtain\n(cid:11)\n\u2212 n(cid:9)\n\n\u2212 n(cid:9)\n\n(cid:10)\n\n(cid:10)\n\n(cid:11)\n\nn\n\n.\n\n= e exp\n\ne exp\n\naix (1)\n\nti\n\ni=1\n\naix (2)\n\nti\n\ni=1\n\nthe left-hand side is the joint laplace transform of (x (1)\n) and the right-hand side\nis the same for x (2). by the uniqueness of the laplace transform, the finite-dimensional\ndistributions of x (1) and x (2) are equal. both processes have continuous paths, and the\nconclusion now follows from theorem 2.6.\n\n, . . . , x (1)\n\ntn\n\nt1\n\n209\n\n "}, {"Page_number": 228, "text": "210\n\nthe ray\u2013knight theorems\n\nlet bt be a brownian motion, not necessarily the same as wt, and let zt be the non-negative\n\nsolution to\n\n\u221a\n\ndzt = 2\n\nzt dbt + 2 dt,\n\nz0 = 0, 0 \u2264 t \u2264 1.\n\n(26.1)\n\nthe solution to this equation is unique by theorem 24.4, and zt is the square of a bessel\nprocess of order 2.\ntheorem 26.2 the processes {l1\u2212x\nproof let f \u2265 0 be a continuous function whose support [a, b] is a subset of (0, 1). let f\nbe the solution to\n\n; 0 \u2264 x \u2264 1} and {zx; 0 \u2264 x \u2264 1} have the same law.\n\nt\n\nf\n\n(cid:3)(cid:3)(x) = 2f (x) f (x),\n(cid:10)\n\n(cid:3)\n\nf (1) = 1, f\n(cid:3)\n\nsee exercise 26.1. define g(x) = f (1 \u2212 x) and g(x) = f (1 \u2212 x), so that g\n(cid:3)(0) = 0, and g(0) = 1. we will show\ng\nf (x)l1\u2212x\n\ne exp\n\n(cid:10)\n\n(cid:11)\n\n(cid:11)\n\nf (t )zt dt\n\n\u2212\n\n\u2212\n\n,\n\n1\n\n1\n\n(cid:3)(cid:3) = 2gg,\n\n(26.2)\n\nt dx\n\n(cid:3)(1) = 0;\n\nand then apply lemma 26.1.\n\nthe left-hand side of (26.2) is equal to\nf (1 \u2212 x)lx\n\ne exp\n\n\u2212\n\n1\n\nt dx\n\n(cid:10)\n\n0\n\n(cid:3)\n\n0\n\n= e exp\n(cid:11)\n\n= e exp\n= e exp\n\u2212(cid:15)\n\nt\n0 g(ws ) ds.\n\n0\n\n(cid:3)\n(cid:3)\n\n(cid:10)\n(cid:10)\n\n\u2212\n\n\u2212\n\n1\n\nt\n\n0\n\n0\n\n(cid:11)\n(cid:11)\n\ng(x)lx\n\nt dx\n\ng(xs) ds\n\n,\n\nmt = g(wt )e\n\n\u2212(cid:15)\n0 g(ws ) ds dt + g\n\nt\n\n\u2212(cid:15)\n\nt\n\n0 g(ws ) ds dwt\n\n(cid:3)(wt )e\n\nwhere the last equality follows from the occupation time formula (theorem 14.4). let\n\nby it\u02c6o\u2019s formula and the product formula,\n\ndmt = \u2212g(wt )g(wt )e\n\u2212(cid:15)\n\nt\n\n+ 1\n2 g\n= g\n(cid:3)(wt )e\n\n\u2212(cid:15)\n(cid:3)(cid:3)(wt )e\n\n0 g(ws ) ds dt\n0 g(ws ) ds dwt ,\n\nt\n\n(cid:3)(cid:3) \u2212 gg = 0. therefore mt is a martingale. since g is bounded on (\u2212\u221e, 1], then\n\n2 g\n\nsince 1\nmt\u2227t is bounded and we then have\n\n1 = g(0) = e m0 = e mt = e g(1)e\n\nt\n0 g(ws ) ds,\n\n\u2212(cid:15)\n\nso\n\ne exp\n\n\u2212\n\nt\n\n0\n\ng(ws) ds\n\n(cid:10)\n\n(cid:3)\n(cid:10)\n\n(cid:11)\n(cid:3)\n\nnow look at the right-hand side of (26.2). let\n(cid:3)(t )\nf\n2f (t )\n\nnt = 1\nf (t )\n\nexp\n\nzt\n\n\u2212\n\nf (s)zs ds\n\n.\n\n0\n\n= 1\ng(1)\n\n.\n\nt\n\n(cid:11)\n\n(26.3)\n\n "}, {"Page_number": 229, "text": "the ray\u2013knight theorems\n\n211\n\nlet\n\nso using (26.1),\n\nyt = zt\n\n(cid:3)(t )\nf\n2f (t )\n\n,\n\ndyt = zt\n\n2f (t )f\n\n(cid:3)(cid:3)(t ) \u2212 2f\n4f (t )2\n\n(cid:3)(t )2\n\nif\n\n\u221a\nzt dbt + 2\n\n(cid:3)(t )\nf\n2f (t )\n\n(cid:3)(t )\nf\n2f (t )\n\ndt.\n\ndt + 2\n(cid:3)\n\nt\n\nf (s)zs ds,\n\n0\n\nthen the martingale part of x is\n\nand hence\n\n0\n\nxt = yt \u2212\n(cid:3)\n\nt\n\n\u221a\nzs dbs,\n\n(cid:3)(s)\nf\nf (s)\n\n(cid:10)\n\n(cid:11)2\n\nd(cid:22)x (cid:23)\n\nt\n\n=\n\n(cid:3)(t )\nf\nf (t )\n\nby it\u02c6o\u2019s formula and the product formula and using f\n\n(cid:3)(t )\nf (t )2 ext dt + 1\ndnt = \u2212 f\nf (t )\n\u221a\n(cid:3)(t )\n+ f\nzt dbt + f\nf (t )\n(cid:3)(t )2\n1\nf\nf (t )2 zt dt\nf (t )\n\u221a\n\next\n(cid:3)(t )\nf (t )\n\next\n\n2\n\n+ 1\n(cid:3)(t )\nf (t )\n\n= f\n\nzt dbt .\n\n(cid:18)\n\nzt dt.\n(cid:3)(cid:3) = 2f f ,\n(cid:3)(cid:3)(t )\n(cid:19)\ndt \u2212 zt\nf\nzt\n2f (t )\ndt \u2212 f (t )zt dt\n\n(cid:3)(t )2\nf\n2f (t )2 dt\n\n(cid:3)\n\n(cid:10)\n\nobserve that f is continuous and positive on [0, 1], hence bounded below on [0, 1] by a\nis bounded above on [0, 1]. we see that nt\u22271 is a martingale. then\npositive constant. also f\ne n0 = 1/f (0) = 1/g(1), while\ne n1 = 1\n\u2212(cid:15)\nf (1)\n= e e\n(cid:3)\n(cid:10)\n\n(cid:3)(1)\nf\n2f (1)\n\nf (s)zs ds\n\ntherefore\n\n1\n0 f (s)zs ds.\n\ne exp\n\n(cid:11)\n\n(cid:11)\n\n(cid:3)\n\n\u2212\n\nz1\n\n1\n\n0\n\ne exp\n\n1\n\n\u2212\n\nf (s)zs ds\n\n0\n\n= e n1 = e n0 = 1\ng(1)\n\n.\n\ncombining with (26.3), we conclude the two sides of (26.2) are equal.\n\n "}, {"Page_number": 230, "text": "212\n\nthe ray\u2013knight theorems\n\nyou may wonder how the function f was arrived at. exercises 26.2 and 26.3 may shed\n\nsome light on this.\n\nexercises\n\n26.1\n\nsuppose f is a non-negative continuous function whose support [a, b] is a subset of (0, 1).\n(cid:3)(cid:3)(x) = 2f (x) f (x),\nshow that there is a unique solution to the ordinary differential equation f\nf (1) = 1, f\nis zero there, and hence is of the form f (x) = ax+ b\nfor some a and b for x \u2265 b. since f\n\n(cid:3)(1) = 0, that f is everywhere positive, and f is bounded on [0,\u221e).\n\nhint: since f is zero in (b,\u221e), then f\n\n(cid:3)(1) = 0, conclude that a is 0.\n\n(cid:3)(cid:3)\n\n26.2 suppose xt is a solution to the one-dimensional sde\n\ndxt = \u03c3 (xt ) dwt + b(xt ) dt.\n\nsuppose \u03c3 and b are bounded and continuous and f is a bounded and continuous function. what\nordinary differential equation must h (x) satisfy (in terms of \u03c3, b, and f ) in order that\n\n(cid:15)\nmt = h (xt )e\n\nt\n0 f (xs ) ds\n\nbe a martingale?\n\n26.3 suppose xt is a solution to the one-dimensional sde\n\ndxt = \u03c3 (xt ) dwt + b(xt ) dt.\n\nsuppose \u03c3 and b are bounded and continuous and f is a bounded and continuous function. what\npartial differential equation must k (x, t )) satisfy (in terms of \u03c3, b, and f ) in order that\n\n(cid:15)\nnt = k (xt , t )e\n\nt\n0 f (s)xs ds\n\nbe a martingale?\n\n26.4 let w be a brownian motion and ly\n\ntime t are not a markov process. that is, let t > 0 be fixed and show that (ly\nt\nmarkov process in the variable y.\n\nt the local times at level y. prove that local times at a fixed\n, y \u2265 0) is not a\n\n26.5 let s be the first time two-dimensional brownian motion exits the unit ball and let \u03c8 (\u03bb) =\nt and t = inf{t >\nt in terms of \u03c8, i.e., write p(y \u2264 \u03bb) in\n\np0(s > \u03bb). if w is a one-dimensional brownian motion with local times lx\n0 : wt = 1}, find the distribution of y = sup0\u2264x\u22641 lx\nterms of the function \u03c8.\n\nthe law of the process x \u2192 lx\n\n26.6 suppose x \u2208 (0, 1). with w and t as in exercise 26.5, find the distribution of lx\nt .\n26.7 let w be a one-dimensional brownian motion with local times lx\n\n= r}.\n, x \u2265 0} is the same as the law of {xx, x \u2265 0} started at r, where x is the\n(1) the law of {lx\ntr\n, x \u2265 0} is also the same as the law of {xx, x \u2265 0} started at r, where x is\n(2) the law of {l\n\u2212x\ntr\n(3) the processes {lx\n\n, x \u2265 0} are independent of each other.\n\nthe square of a bessel process of order 0.\n, x \u2265 0} and {l\n\u2212x\ntr\n\nt . let tr = inf{t > 0 : l0\n\nsquare of a bessel process of order 0.\n\ncan be described as follows:\n\ntr\n\ntr\n\nt\n\n "}, {"Page_number": 231, "text": "notes\n\n213\n\nthis is proved in revuz and yor (1999), section xi.2, or for a challenge, try to prove (1) for\n, find the distribution\n\nyourself using the techniques of this chapter. using this description of lx\ntr\nof l\n\n= supx lx\n\n.\n\n\u2217\ntr\n\ntr\n\nnotes\n\nthere are several other proofs of the ray\u2013knight theorems. one by walsh (rogers and\nwilliams, 2000b; walsh, 1978) uses excursion theory. in the next chapter we will indicate\nsome ideas used in that proof.\n\n "}, {"Page_number": 232, "text": "27\n\nbrownian excursions\n\nthe paths of a brownian motion wt are continuous, so the zero set z(\u03c9) = {t : wt (\u03c9) = 0}\nis a closed set. the complement of z(\u03c9) is an open subset of the reals, hence is the countable\nunion of disjoint open intervals. if (a, b) is one of those intervals (depending on \u03c9, of course),\nthen {wt (\u03c9) : a \u2264 t \u2264 b} is a continuous function of t that is zero at t = a and t = b but is\nnever 0 for any t \u2208 (a, b). we call this piece of the path of wt (\u03c9) an excursion.\nto be more formal, let e be the collection of continuous functions f with domain [0,\u221e)\nsuch that the following hold: there exists a positive real \u03c3 f such that f (0) = 0, f (\u03c3 f ) = 0,\nf (t ) (cid:16)= 0 if t \u2208 (0, \u03c3 f ), and f (t ) = 0 if t > \u03c3 f . we make e into a metric space by furnishing\nit with the supremum norm. given a borel subset a of e, we say that the brownian motion\nw has had an excursion in a by time t if there exists a time u and a function f \u2208 a such that\nu + \u03c3 f \u2264 t and wu+s(\u03c9) = f (s) for all s \u2264 \u03c3 f . let kt (a) be the number of excursions of w\nin a by time t. let l0\n\nt be brownian local time at 0, and let\n\u2265 r}\n\ntr = inf{t > 0 : l0\n\nt\n\n(27.1)\n\nbe the inverse of brownian local time at zero.\n\nset\n\nnr(a) = ktr\n\n(a).\n\nalthough nt (a) might be identically infinite for some sets a, it will be finite for others. for\nexample, let \u03b4 > 0 and suppose that every function in a has a supremum greater than \u03b4. the\ncontinuity of the paths of w implies that nt (a) is finite for every t.\n\nthe main result of this section is the following.\ntheorem 27.1 nt (\u00b7) is a poisson point process.\n\nif nt (b) is not infinite, then it has right-continuous paths that increase at most 1\nproof\nat any given time. the main step will be to show that nt (b) has stationary increments and\nnt (b) \u2212 ns(b) is independent of the \u03c3 -field generated by the random variables\n\n{nr(a) : r \u2264 s, a a borel subset of e}.\n\n214\n\n "}, {"Page_number": 233, "text": "215\nif r1 \u2264 \u00b7\u00b7\u00b7 \u2264 rn \u2264 s < t, k \u2265 0, j1, . . . , jn \u2265 0, and b and a1, . . . , an are borel subsets of\ne, then\n\nbrownian excursions\n\np(nt (b) \u2212 ns(b) = k; nr1\n\n(cid:26)\n= p(ktt\n= e\n\n(b) \u2212 kts\npwts (ktt\u2212s\n\n(a1) = j1, . . . nrn\n(b) = k); ktr1\n\n(b) = k; ktr1\n(b) \u2212 kt0\n\n(an) = jn)\n(a1) = j1, . . . , ktrn\n\n(a1) = j1, . . . , ktrn\n\n(an) = jn)\n\n(an) = jn\n\n(27.2)\n\n(cid:27)\n\n,\n\nwhere we used the strong markov property at time ts. since ts is the first time that local time\nof brownian motion at 0 exceeds s and l0\nt increases only when w is at 0, then at time ts the\nprocess w is at 0, so wts\n\n= 0. therefore the last expression in (27.2) equals\n(an) = jn),\n\n(b) \u2212 k0(b) = k)p(ktr1\n\n(a1) = j1, . . . , ktrn\n\np0(ktt\u2212s\n\nwhich can be rewritten as\n\np0(nt\u2212s(b) \u2212 n0(b) = k)p(nr1\n\n(a1) = j1, . . . , nrn\n\n(an) = jn).\n\nthis shows that the law of nt (b) \u2212 ns(b) is the same as the law of nt\u2212s(b) \u2212 n0(b) and is\nindependent of \u03c3 (nr(a) : r \u2264 s, a \u2282 e ), which is what we wanted.\nobserve that nt (b) is constant except for jumps of size one. by proposition 5.4, nt (b) is\n\na poisson process. it is clear that nt (b) is a measure in b, which completes the proof.\n\nlet m(a) = e 0n1(a). the measure a is called the excursion measure. we can say a few\n\nthings about m.\n\nproposition 27.2 if\n\na = { f \u2208 e : sup\n\n| f (t )| > a},\n\nt\n\nthen m(a) = 1/a.\nproof let u = inf{t : |wt| = a} and v = inf{t > u : wt = 0}. since |wt| \u2212 l0\nmartingale by theorem 14.1, then e 0|wt\u2227u| = e 0l0\nconvergence on the left and monotone convergence on the right,\n\nis a\nt\u2227u . letting t \u2192 \u221e and using dominated\n\nt\n\n= e 0|wu| = a.\n\ne l0\nu\n\nset r = inf{r : nr(a) = 1}. because nr(a) is a poisson process, then r is an exponential\nrandom variable with parameter e n1(a) = m(a). it therefore suffices to show e 0r = a;\nsee (a.9).\nwe have r = inf{r : ktr\n(a) = 1}, and because k can only increase at times when wt = 0,\nthen\n\nnow kt (a) will first equal one when t = v . but because local time at 0 does not increase\nwhen w is not at 0, l0\nv\n\nu . therefore\n\n= l0\n\nr = inf{l0\n\nt : kt (a) = 1}.\n\ne 0r = e 0l0\n\nv\n\n= e 0l0\n\nu\n\n= a.\n\nwe conclude that m(a) = 1/a.\n\n "}, {"Page_number": 234, "text": "216\n\nbrownian excursions\n\nby symmetry, if b = { f \u2208 e : supt f (t ) > a}, then m(b) = 1/(2a). one can say\n\nmore about m. consider those excursions whose maximum is some fixed value b. starting\nat any point other than 0, the excursion can be viewed as a brownian motion killed at 0\nand conditioned to have maximum b. such a path can be decomposed into the part before\nthe maximum, which is a brownian motion conditioned to hit b before 0, and the part after\nthe maximum, which is brownian motion conditioned to hit 0 before b. the former can\nbe shown to have the same law as a three-dimensional bessel process, up until it hits the\nlevel b (see the example in section 22.2), and the latter the same law as b \u2212 xt, where xt is\nalso a three-dimensional bessel process up until it hits the level b. moreover, the part of the\npath before the maximum can be taken to be independent of the part of the path after the\nmaximum. see rogers and williams (2000b) for details.\nlet us briefly revisit the ray\u2013knight theorems and indicate how brownian excursions\ncan be used to obtain information about local times at different levels. fix r and let tr =\ninf{t > 0 : l0\n\u2265 r}. if x > 0 and y1, . . . , yn < 0, then the local time at x is a function of the\nexcursions from 0 that hit x and the local times at y1, . . . , yn are functions of the excursions\nthat go below zero. since the set of excursions that take positive values and those that take\nnegative values are independent, then lx\n. to find the\ndistribution of lx\ntr, there are a poisson number of excursions that reach the level x. each\nexcursion that reaches x contributes an amount to the local time at x that is an exponential\nrandom variable; see exercise 27.1. after proving some additional independence, namely,\nthat the amount each excursion contributes to local time at x is independent of the amount any\nother excursion contributes and that the amount contributed by an excursion is independent\nof the number of excursions reaching x, we see that lx\ntr should have the same distribution as\na poisson number of independent exponential random variables.\n\ntr should be independent of ly1\ntr\n\n, . . . , lyn\ntr\n\nt\n\n27.1 let w be a brownian motion, x > 0, and t = inf{t > 0 : wt = x}. if lx\n\nt is the local time at x,\nt is an exponential random variable. determine the parameter of\n\nshow that the distribution of lx\nthis exponential random variable.\n\nexercises\n\n27.2 let w be a one-dimensional brownian motion. this exercise asks you to prove that the nor-\nmalized number of downcrossings by time t converges to local time at 0. if a > 0, let s0 = 0,\nt0 = inf{t : wt = a}, and for i \u2265 1,\n\nsi = inf{t > ti\u22121 : wt = 0},\n\nti = inf{t > si : wt = a}.\n\nthen dt (a), the number of downcrossings up to time t, is defined to be sup{k : sk \u2264 t}. prove\nthat there exists a constant c such that\n\nadt (a) = cl0\n\nt\n\n,\n\nlim\na\u21920\n\na.s.,\n\nwhere l0\n\nt is local time at 0 of w . determine c.\n\nhint: use exercise 18.5.\n\n27.3 let (xt , px ) be a brownian motion.\n\n(1) use the reflection principle to find\n\np0(xs > \u2212a for all s \u2264 r).\n\n "}, {"Page_number": 235, "text": "this is the same as pa(t0 > r), where t0 is the first time the brownian motion hits 0.\n\nnotes\n\n217\n\n(2) let\n\nand\n\nprove that\n\na(a, r) = { f \u2208 e : sup f (t ) > a, \u03c3 f > r},\nb(r) = { f \u2208 e : \u03c3 f > r, sup f (t ) > 0},\n\nc(a) = { f \u2208 e : sup f (t ) > a}.\n\nm(b(r)) = lim\na\u21920\n\nm(a(a, r)) = lim\na\u21920\n\n[m(c(a)) \u00d7 pa(t0 > r)]\n\nand use this and (1) to compute m(b(r)). by symmetry, m({ f \u2208 e : \u03c3 f > r}) will be twice the\nvalue of m(b(r)).\n\n27.4 let w be a brownian motion. let et (r) be the number of excursions of length larger than r that\nhave been completed by time t. an excursion of length larger than r means that \u03c3 f > r. show\nthat there exists a constant c such that\n\u221a\nret (r) = cl0\n\na.s.\n\n,\n\nt\n\nlim\nr\u21920\n\ndetermine c.\none interesting point here is that this shows that l0\nz(\u03c9) = {t : wt (\u03c9) = 0}.\nt\n\nis determined entirely by the zero set\n\n27.5 let \u03b4 > 0 and a\u03b4 = { f \u2208 e : supt\n\n| f (t )| > \u03b4}. let s1 = inf{t : kt (a\u03b4 ) = 1} and s2 = inf{t >\ns1 : kt (a\u03b4 ) = 2}. thus s1 and s2 are the times the first and second excursions in a\u03b4 have been\ncompleted. let y1(t ) be the excursion completed at time s1 and define y2(t ) similarly. to be\nmore precise, if r1 = sup{t < s1 : wt = 0}, then y1(s) = wr1+s if s \u2264 s1 \u2212 r1 and y1(s) is\nequal to 0 for all s \u2265 s1 \u2212 r1.\nprove that y1 and y2 are independent.\nhint: use the strong markov property at time s1.\n\nnotes\n\nbesides its use in the ray\u2013knight theorems (rogers and williams, 2000b), excursion theory\nis useful in many other contexts. see rogers and williams (2000b) for applications to\nskorokhod embedding and to the arc sine law.\n\n "}, {"Page_number": 236, "text": "28\n\nfinancial mathematics\n\na european call option is the option to buy a share of stock at a given price at some particular\ntime in the future. for example, i might buy a call option to purchase one share of company x\nfor $40 three months from today. when the three months is up, i check the price of company\nx. if, say, it is $35, then my option is worthless, because why would i buy a share for $40\nusing the option when i could buy it on the open market for $35? but if three months from\nnow, the share price is, say, $45, then i can exercise my option, which means i buy a share\nfor $40, and i can then turn around immediately and sell that share for $45 and make a profit\nof $5. thus, today, there is a potential for a profit if i have a call option, and so i should pay\nsomething to purchase that option. a significant part of financial mathematics is devoted to\nthe question of what is the fair price i should pay for a call option.\n\noptions originated in the commodities market, where farmers wanted to hedge their risks.\nsince then many types of options have been developed (options are also known as derivatives),\nand the amount of money invested in options has for the past several years exceeded the\namount of money invested in stocks.\n\nin 1973 black and scholes, using the reasonable principle that you can\u2019t get something\nfor nothing, came up with a convincing formula for the price of an option. this chapter gives\ntwo derivations of the black\u2013scholes formula, proves the fundamental theorem of finance,\nand finishes by considering a stochastic control problem. the black\u2013scholes formula is a\nbeautiful example of applied stochastic processes.\n\n28.1 finance models\n\nlet wt be a brownian motion. we assume that st is the price of a stock or other risky security.\nif we have $2,000 and we buy 100 shares in a stock that sells for $20 per share and it goes\nup $2, or if we buy 10 shares in a stock selling for $200 per share that goes up $20, we are\nequally happy; it is the percentage increase that matters. with this in mind, we assume that\nst satisfies\n\ndst = \u03bcst dt + \u03c3 st dwt .\n\n(28.1)\nthis is plausible, since then dst /st = \u03bc dt + \u03c3 dwt, that is, we are assuming the relative\nchange in price is a multiple of brownian motion with drift. the quantity \u03bc is known as the\nmean rate of return and \u03c3 is called the volatility. the solution to this sde is\n\nst = s0e\u03c3wt+(\u03bc\u2212(\u03c3 2/2))t\n\n(28.2)\n\nby proposition 24.6.\n\n218\n\n "}, {"Page_number": 237, "text": "we also assume the existence of a bond with price bt, which is assumed to be riskless,\n\n28.1 finance models\n\n219\n\nand the equation for bt is\n\nwhich implies\n\ndbt = rbt dt,\n\nbt = b0ert .\n\n(cid:12)\n\nsuppose at time t one buys a shares of stock. the cost is ast. if one sells the shares at\ntime t + h, one receives ast+h, and the net gain is a(st+h \u2212 st ). one can also sell short, i.e.,\nlet a be negative. the formula for the gain is the same.\n\n\u2212 sti\n\n). this is the same as the stochastic integral\n\nsuppose at time ti one holds ai shares, up until time ti+1. the total net gain over the whole\nn\u22121\nt\ni=0 ai(sti+1\nperiod t0 to tn is\n0 at dst if at\nequals ai when ti \u2264 t < ti+1.\n(cid:15)\none should allow ai to depend on the entire past f ti. idealizing, one allows continuous\ntrading, and if as is the number of shares held at time s, the net gain through trading the stock\nt\nt\nis\n0 bs dbs when trading bonds if bs is the number of\n0 as dss. one has a similar net gain of\nbonds held at time s.\nalthough at can depend on the entire past ft, one does not want to let at depend on\nthe future. this helps explain why the class of predictable integrands is the appropriate one\nto use.\n\n(cid:15)\n\n(cid:15)\n\nthe pair (a, b) is called a trading strategy. set\n\nthe amount of wealth one has at time t. the strategy is self-financing if\n\nvt = atst + btbt ,\n(cid:3)\n(cid:3)\n\nvt = v0 +\n\nt\n\nas dss +\n\nt\n\nbs dbs\n\n(28.3)\n\n(28.4)\n\n0\n\n0\n\nfor all t. the first integral represents the net gain from trading in the stock, the second integral\nthe net gain from trading in the bond, and (28.4) says that one\u2019s wealth at time t is equal to\nwhat one starts with plus what one has realized through trading in the stock and bond. we\nassume throughout that there are no transaction costs (i.e., no brokerage fees).\n\nwhat is the option worth? at time te, if ste\n\na european call gives the buyer the option of buying a share of the stock at a fixed time\nte at price k. the time te is called the exercise time. after time te, the option has expired\nand is worthless.\n\u2264 k, the option is worth nothing, for who\n> k, one can\nwould pay k dollars for a share of stock when it sells for ste dollars? if ste\nuse the option to buy a share of the stock at price k and immediately sell it at price ste , to\n\u2212 k )+\nmake a profit of ste\n. an important\nquestion is: how much should the option sell for? what is a fair price for the option at\ntime 0?\n\n\u2212 k. thus the value of the option at time te is (ste\n\nthere are a myriad of types of options. the american call is almost the same as the\neuropean call, except that one is allowed to buy a share of the stock at price k at any time in\nthe interval [0, te]. the european put gives the buyer the option to sell a share of the stock at\nprice k at time te, while the american put gives the buyer the option to sell a share at price\nk anytime before time te.\n\n "}, {"Page_number": 238, "text": "(cid:15)\n\n220\n\nfinancial mathematics\n\n28.2 black\u2013scholes formula\n\nin 1973 black and scholes came up with their formula for the price of a european call. we\nwill give two derivations of this formula.\n\nderivation 1. first of all, the interest rate r on the bond may be considered to be the same as\nthe rate of inflation. thus the value of the option (ste\n\nin today\u2019s dollars is\n\n\u2212 k )+\n\nc = e\n\n\u2212rte (ste\n\n\u2212 k )+.\n\n(28.5)\n\nin this first derivation we work in today\u2019s dollars. therefore the present-day value of the stock\nis pt = e\n\n\u2212rtst. note p0 = s0 and the present-day value of our option at time te is then\n\nc = e\n\n\u2212rte (ste\n\n\u2212 k )+ = (pte\n\n\u2212 e\n\n\u2212rte k )+.\n\n(28.6)\n\nby the product formula,\n\n\u2212rtst dt\n\n\u2212rt dst \u2212 re\n\u2212rt \u03c3 st dwt + e\n\ndpt = e\n= e\n= \u03c3 pt dwt + (\u03bc \u2212 r)pt dt.\n\n\u2212rt \u03bcst dt \u2212 re\n\n\u2212rtst dt\n\nthe solution to this stochastic differential equation (see proposition 24.6) is\n\npt = p0e\u03c3wt+(\u03bc\u2212r\u2212\u03c3 2/2)t .\n\nalso, the net gain or loss in present-day dollars when holding as shares of stock at time s is\nt\n0 as dps.\ndefine q on fte by\nunder q,(cid:14)wt = wt + \u03bc\u2212r\n\n\u03c3 t is a brownian motion by the girsanov theorem.\n\ndq/dp = mte\n\n= exp\n\n(cid:11)\n\n(cid:10)\n\nwte\n\nte\n\n.\n\n\u03c3\n\n\u2212 \u03bc \u2212 r\n(cid:10)\n\n2\u03c3 2\n\n\u2212 (\u03bc \u2212 r)2\n(cid:11)\n\ndwt + \u03bc \u2212 r\n\n\u03c3\n\ndt\n\n= \u03c3 pt d(cid:14)wt .\n\ndpt = \u03c3 pt dwt + (\u03bc \u2212 r)pt dt = \u03c3 pt\n\nnow\n\nis\n\ntherefore under q, pt is a martingale since stochastic integrals with respect to martingales\nare martingales. the solution to the sde\n\ndpt = \u03c3 ptd(cid:14)wt\npt = p0e\u03c3(cid:14)wt\u2212(\u03c3 2/2)t ,\nso pt and(cid:14)wt have the same filtration.\n(28.7)\nc is fte measurable. by the martingale representation theorem (theorem 12.3), there\n(cid:3)\nexists an adapted process as such that\nte\n\n(cid:3)\n\nte\n\nas d(cid:14)ws = e qc +\n\nc = e qc +\n\nds dps,\n\n0\n\nwhere ds = as/(\u03c3 ps).\n\n0\n\n "}, {"Page_number": 239, "text": "28.2 black\u2013scholes formula\n\n221\n\ntherefore, if one follows the trading strategy of buying and selling the stock st, where one\nholds ds shares of stock at time s, one can obtain c \u2212 e qc dollars at time te. or, starting\nwith e qc dollars and buying and selling stock, one can get the identical output as c, almost\nsurely. a standard assumption in finance is that of no arbitrage, which means you cannot\nmake a profit without taking some risk. to avoid riskless profits, c must sell for e qc.\n\n(cid:3)\n\nto explain this in more detail, suppose you could sell the european call for c\n(cid:3) > e qc, you could sell a call for c\n\ndollars.\nif c\ndollars, use the money and invest in the trading\n(cid:3) + c \u2212 e qc worth\nstrategy of holding ds shares of stock at time s, and at time te have c\nof stocks and options. the buyer of the option decides whether to exercise the option, and it\n(cid:3) \u2212 e qc\ncosts you c dollars to meet that obligation. with probability one, you have gained c\n(cid:3) < e qc, simply reverse the roles of buying and selling. the\ndollars, a riskless profit. if c\nonly way to avoid making a riskless profit is if c\nto find e qc, using (28.6) and (28.7) we write\n\u2212\u03c3 2te /2 \u2212 e\n(s0e\u03c3 y\u2212\u03c3 2te /2 \u2212 e\n\ne qc = e q[(s0e\u03c3(cid:14)wte\n(cid:3)\n\n(cid:3) = e qc.\n\n\u2212y2/2te dy,\ne\n\n\u2212rte k )+\n\n\u2212rte k )+\n\n= 1\u221a\n\n(28.8)\n\n]\n\n(cid:3)\n\n2\u03c0te\n\nwhich is the black\u2013scholes formula. one can, if one wishes, perform some calculations to\nfind alternate expressions for the right-hand side.\n\nit is noteworthy that \u03bc does not appear in (28.8)! you and i might have different opinions\nas to what \u03bc, the mean rate of return, is equal to, but we should agree on the price of the call.\nthis was a shock to economists when this was first discovered. the value of \u03c3 , the volatility,\ndoes enter into the formula.\n\nuntil we evaluated e qc in (28.8), the actual form of c was unimportant. for any type\nof option expiring at time te, derivation 1 tells us that its price at time zero should be its\nexpectation under q.\nderivation 2. in this approach, which is the one used by black and scholes, we use the actual\nvalues of the securities, not the present-day values. let vt be the value of the option at time t\nand assume\n\nvt = f (st , te \u2212 t )\n\n(28.9)\n\u2212 k )+\n.\nrecall the multivariate version of it\u02c6o\u2019s formula (theorem 11.2). we apply this with d = 2\n\nfor all t, where f is some function that is sufficiently smooth. we also want vte\nand xt = (st , te \u2212 t ). from( 28.1),\n\n= (ste\n\n(cid:22)s(cid:23)\n\nt\n\n= \u03c3 2s2\n\nt dt,\n\n(cid:22)te \u2212 t(cid:23)\n(cid:22)s, te \u2212 t(cid:23)\n\n= 0 since te \u2212 t is of bounded variation and hence has no martingale part, and\n= 0. also, d (te \u2212 t ) = \u2212dt. then\n\nt\n\nt\n\n(cid:3)\n\nvt \u2212 v0 = f (st , te \u2212 t ) \u2212 f (s0, te )\n(cid:3)\nfx(su, te \u2212 u) dsu \u2212\n\n=\n\n0\n\nt\n\n(cid:3)\n\n0\n\n+ 1\n\n2\n\nt\n\nu fxx(su, te \u2212 u) du.\n\n\u03c3 2s2\n\n0\n\nt\n\nft (su, te \u2212 u) du\n\n(28.10)\n\n "}, {"Page_number": 240, "text": "(28.13)\n\n(28.14)\n\n(28.15)\n\n222\n\nfinancial mathematics\n\nhere fx is the partial derivative with respect to x, the first variable, fxx is the second partial\nderivative with respect to x, and ft is the partial derivative with respect to t, the second\nvariable. on the other hand,\n\n(cid:3)\n\n(cid:3)\n\nvt \u2212 v0 =\n\nt\n\nau dsu +\n\nt\n\nbu dbu.\n\n(28.11)\n\nby (28.3) and (28.9),\n\n0\n\n0\n\nbt = vt \u2212 atst\n\nbt\n\n= f (st , te t \u2212 t ) \u2212 atst\n\nbt\n\n.\n\n(28.12)\n\nalso, recall bt = b0ert. comparing (28.10) with (28.11), we must therefore have\n\nat = fx(st , te \u2212 t )\n\nand\n\n\u2212 ft (st , te \u2212 t ) + 1\n\n2\n\nt fxx(st , te \u2212 t ) = btb0rert .\n\n\u03c3 2s2\n\nsubstituting for bt using (28.12),\n\nr[ f (st , te \u2212 t ) \u2212 st fx(st , te \u2212 t )]\n\n= \u2212 ft (st , te \u2212 t ) + 1\n\nt fxx(st , te \u2212 t )\n\n\u03c3 2s2\n\n2\n\nfor almost all t and all st. since st is a continuous process, (28.15) leads to the parabolic\npartial differential equation (pde)\n\nft = 1\n\n\u03c3 2x2 fxx + rx fx \u2212 r f ,\n\n2\n\n(x, s) \u2208 (0,\u221e) \u00d7 [0, te ),\n\nand\n\nf (x, 0) = (x \u2212 k )+.\n\nsolving this equation for f , f (x, te ) tells us what v0 should be, i.e., the cost of setting up the\nequivalent portfolio. this partial differential equation can be solved and the solution is the\nblack\u2013scholes formula. equation (28.13) shows what the trading strategy should be.\n\nlet us now briefly discuss american calls. recall that these are ones where the holder\ncan buy the security at price k at any time up to time te. since the holder of an american\ncall can always wait up to time te, which is equivalent to having a european call, the value\nof an american call should always be at least as large as the value of the corresponding\neuropean call.\n\nsuppose one exercises an american call early. if ste\n\n> k and one exercised early, at time\n\u2212 k ).\nte one has one share of stock, for which one paid k, and one has a profit of (ste\nhowever, because one purchased the stock before time te, one lost the interest ker(te\u2212t ) that\nwould have accrued by waiting to exercise the option. (we are supposing r \u2265 0.) thus in this\ncase it would have been better to wait until time te to exercise the option.\n< k, exercising the option early would mean that one has lost\non the other hand, if ste\n|ste\n\u2212 k|, whereas for the european option, one would have not exercised at all, and lost\nnothing (other than the price of the option).\n\nin either case, exercising early gains nothing, hence the price of an american call should\n\nbe the same as that of a european call.\n\n "}, {"Page_number": 241, "text": "28.3 the fundamental theorem of finance\n\n223\n\none can equally well price the european put, the option to sell a share of stock at price\nk at time te, by either derivation 1 or derivation 2 of the black\u2013scholes formula. however\nthis analysis breaks down for american puts (sell a share of stock anytime up to time te),\nbecause in this case one gains by selling early: one can earn interest on the money received.\n\n28.3 the fundamental theorem of finance\n\nin the preceding section, we showed there was a probability measure q under which pt was a\nmartingale. this is true very generally. let st be the price of a security in present-day dollars.\nwe will suppose st is a continuous semimartingale, and can be written st = mt + at.\n(cid:15)\n\nthe nflvr condition (\u201cno free lunch with vanishing risk\u201d) is that one cannot find fixed\n|hn(s)||das| +\nn d(cid:22)m(cid:23)\n\npositive real numbers t0, \u03b5, b > 0, and predictable processes hn with\nt0\n0 h 2\n\n< \u221e, a.s., for each n such that\n\n(cid:15)\n\nt0\n0\n\ns\n\n(cid:3)\n\nfor all n and\n\n0\n\np\n\nt0\n\nhn(s) dss > \u2212 1\n(cid:10)(cid:3)\nn\n\nt0\n\na.s.,\n\n,\n\n(cid:11)\n\nhn(s) dss > b\n\n> \u03b5.\n\n0\n\nhere t0, b, \u03b5 do not depend on n. the condition says that one can with positive probability\n\u03b5 make a profit of b and with a loss no larger than 1/n. q is an equivalent martingale measure\nif q is a probability measure, q is equivalent to p (which means they have the same null\nsets), and st is a local martingale under q.\n\ntheorem 28.1 if st is a continuous semimartingale and the nflvr condition holds, then\nthere exists an equivalent martingale measure q.\nproof let us prove first of all that dat is absolutely continuous with respect to d(cid:22)m(cid:23)\nt. we\nsuppose not and obtain a contradiction. consider the measures \u03bca and \u03bc(cid:22)m(cid:23) on the predictable\n\u03c3 -field defined by\n\n(cid:3) \u221e\n\n\u03bca(d) = e\n\n1d dat ,\n\n0\n\n\u03bc(cid:22)m(cid:23)(d) = e\n\n1d d(cid:22)m(cid:23)\n\nt\n\n.\n\n(28.16)\n\n(cid:3) \u221e\n\n0\n\n(cid:3)\n\n(cid:3)\n\nbt =(cid:15)\n\nand c\n(bt \u2212 (bt \u2227 ct )) d (b\n(cid:3)\nt\n\n=(cid:15)\n) and ct =(cid:15)\n\nsince a is of bounded variation and continuous, it is a predictable process, and we can\nwrite at = bt \u2212 ct, where b and c are continuous increasing processes and where \u03bcb\nand \u03bcc are mutually singular measures on the predictable \u03c3 -field; we define \u03bcb and \u03bcc\n\u2212 c\nanalogously to (28.16). to give a few more details on how to do this, we write at = b\n(cid:3)\nt ,\nwhere b\nare continuous increasing processes, we find non-negative predictable\n(cid:3)\n(cid:3)\n), and then let\nprocesses bt and ct such that b\n+ c\nt\nt\n). we leave it to\nthe reader to check that b and c are the desired processes. since \u03bcb and \u03bcc are mutually\nsingular, there exists a set e in the predictable \u03c3 -field such that \u03bcb(d) = \u03bcb(d \u2229 e ) and\n\u03bcc (d) = \u03bcc (d \u2229 ec) for all sets d in the predictable \u03c3 -field.\nif \u03bca is not absolutely continuous with respect to \u03bc(cid:22)m(cid:23), then at least one of \u03bcb and \u03bcc is not\nabsolutely continuous. we assume that \u03bcb is not, for otherwise we can look at \u2212st instead of\nt0\nst. therefore there exists a predictable set f and a fixed time t0 such that\n0 1f dbs is almost\n\n+ c\n(cid:3)\n(cid:3)\n(cid:3)\nt\n) and c\n0 ct d (b\n+ c\n(ct \u2212 (bt \u2227 ct )) d (b\nt\nt\nt\n(cid:3)\n(cid:3)\nt\nt\n\n=(cid:15)\n\n(cid:3)\nt\n0 bt d (b\nt\n\n+ c\n\n(cid:15)\n\nt\n0\n\nt\n0\n\n(cid:3)\nt\n\n(cid:3)\nt\n\n "}, {"Page_number": 242, "text": "(cid:3)\n\n0\n\n(cid:19)\n\n(cid:15)\n\n224\n\nfinancial mathematics\n\n(cid:3)\n\n0\n\n= 0. we\nsurely non-negative, is strictly positive with positive probability, and\ncan replace f by f \u2229 e and so assume that f \u2282 e, and hence \u03bcc (f ) = \u03bcc (f \u2229 ec) = 0.\nthen\n\n0 1f d(cid:22)m(cid:23)\n\nt0\n\ns\n\n(cid:3)\n\nt0\n\n1f dss =\n\nt0\n\n0\n\n(cid:15)\n1f dms +\n\nt0\n\n1f dbs +\n\nt0\n\n1f dcs.\n\n= 0. the integral with respect to\nthe stochastic integral term is 0 because\ncs is zero because \u03bcc (f ) = 0. we then have the nflvr condition violated with hn = 1f\nfor all n. hence absolute continuity is established, and by the radon\u2013nikodym theorem,\n\n(1f )2 d(cid:22)m(cid:23)\n\nt0\n0\n\ns\n\nat =(cid:15)\n\nt\n\n0 hs d(cid:22)m(cid:23)\n\n(cid:15)\n\n(cid:3)\n\n0\n\n1\n\nt\u22121 1(x>0)\nlet us investigate case (1) and show that it cannot happen. choose a fixed time t0 such\n\n1|x| dx and f2(t ) =\n\nt\u22121\n\n\u2227 u \u2227 t0.\n\n(28.17)\n\ns\n\n(cid:19)\n\nour next goal is to show\n\ns for some predictable process hs.\n\n(cid:3)\n< \u221e for all t. let\n\n(cid:18)\ns d(cid:22)m(cid:23)\nt\n0 h2\nu = inf\nt :\n(cid:15)\n(cid:15)\non (u < \u221e) there are two possibilities:\n< \u221e if t < u but\n(for a real variable analog, consider the two functions f1(t ) = (cid:15)\nu\n0 h2\ns d(cid:22)m(cid:23)\n< \u221e but\n(cid:15)\nh2\n\ns d(cid:22)m(cid:23)\n= \u221e, and\n= \u221e for all \u03b5.\n\ns d(cid:22)m(cid:23)\nh2\n\n= \u221e\n\nu+\u03b5\nu\n\n(cid:15)\n(cid:15)\n\n.\n\n0\n\ns\n\ns\n\ns\n\ns\n\nt\n\ns\n\n(1)\n(2)\n\ns d(cid:22)m(cid:23)\nt\n0 h2\ns d(cid:22)m(cid:23)\nu\n0 h2\nx dx at t = 0.)\nthat p(u < t0) > 0. let\n\nr1 = r1(n) = inf\n\nt\n\ns d(cid:22)m(cid:23)\nh2\n\ns\n\n\u2265 n4\n\nwe suppose\n\np(r1(n) < u \u2227 t0) > 0\n\ninf\nn\n\nand obtain a contradiction. let ht = ht1[0,r1]/n4. then\nn4 d(cid:22)m(cid:23)\n\nhs das =\n\nh2\ns\n\nr1\n\nt0\n\n0\n\n\u2265 1\n\ns\n\n(cid:3)\n\non (r1 < u < t0). on the other hand,\n\n(cid:20)(cid:20)(cid:20)(cid:3)\n\n(cid:10)\n(cid:20)(cid:20)(cid:20)(cid:3)\n\ne\n\nsup\nt\u2264t0\n\nt\n\n0\n\nhs dms\n\n(cid:10)\n\n(cid:20)(cid:20)(cid:20) >\n\nt\n\n0\n\nhs dms\n\nsup\nt\u2264t0\n\n1\nn\n\nby doob\u2019s inequalities. therefore\n\np\n\nlet\n\nr2 = r2(n) = inf\n\nt0\n\ns d(cid:22)m(cid:23)\nh 2\n\n(cid:20)(cid:20)(cid:20)(cid:15)\n\nn\u22122\n\nt\n0 hs dms\n\nn4\n\ns\n\nn8 n4 = 4\n\u2264 4\n(cid:20)(cid:20)(cid:20)2\n(cid:20)(cid:20)(cid:20) \u2265 1/n\n(cid:19)\n\n\u2264 4/n4\nn\u22122\n\nhs dms\n\n0\n\n= 4\nn2\n\n.\n\n(cid:18)\n\n(cid:3)\n\nt :\n\n0\n\n0\n\n(cid:3)\n(cid:20)(cid:20)(cid:20)(cid:11)2 \u2264 4e\n(cid:11)\n\n(cid:3)\n\n0\n\n\u2264 e supt\u2264t0\n(cid:18)\n\n(cid:20)(cid:20)(cid:20)(cid:3)\n\nt :\n\nt\n\n "}, {"Page_number": 243, "text": "28.3 the fundamental theorem of finance\n\n225\n\nand let(cid:14)ht = ht1[0,r2]. we then have\n(cid:14)hs dss =\n\nt0\n\nalmost surely, and\n\n0\n\nt0\n\n(cid:10)(cid:3)\n\np\n\n0\n\n(cid:11)\n\n(cid:14)hs dss \u2265 1\n\n(cid:3)\np(r2 < r1) \u2264 p(r2 \u2264 t0) \u2264 4/n2,\n\n(cid:3)\n\n(cid:3)\n\nr2\n\n(cid:14)hs dms +\n(cid:3)\n\nr2\n\n(cid:14)hs das\n\nr2\n\n0\n\n0\n\n\u2265 \u2212 1\nn\n\n+\n\n0\n\nh2\ns\n\nn4 d(cid:22)m(cid:23)\n\n\u2265 \u22121/n\n\ns\n\n.\n\n2\n\n\u2265 p(r1 < u < t0) \u2212 p(r2 < r1)\n\u2265 p(r1 < u < t0) \u2212 4\nn2\n(cid:15)\nu+1\nu+\u03b4n\n< \u221e, a.s.,\n\ns d(cid:22)m(cid:23)\nh2\n(cid:3)\n\n(cid:20)(cid:20)(cid:20) \u2265 n or\n\nhs dms\n\nt\n\ns d(cid:22)m(cid:23)\nh2\n\ns\n\n0\n\n(cid:19)\n\n.\n\n\u2265 n\n(cid:11)\n\n(cid:3)\n\nvn\n\n0\n\nt\n\n0\n\n(cid:10)\n\n(cid:3)\n\"\n\n0\n\nt\n\nwe do this for each n, and thus obtain a contradiction to the nflvr condition, so (28.17)\ncannot hold.\n\u2265 n4 with positive probability, let\nht = ht1[u+\u03b4,u+1]/n4, and proceed as above. we leave the details as exercise 28.3.\n\ncase (2) is similar: choose \u03b4n such that\n\ns\n\n(cid:15)\n\nfor each t. consequently the quantity\n\n|(cid:15)\n\nsups\u2264t\n\nwe thus have\n\ns\n\n0 hr dmr| is also finite. let\n\ns\n\nt\n0 h2\n\ns d(cid:22)m(cid:23)\n(cid:18)\n\nvn = inf\n\nt :\n\n(cid:20)(cid:20)(cid:20)(cid:3)\n\nwe conclude vn \u2191 \u221e.\ndefine q on fvn by\n\ndq/dp = exp\n!\n\n(cid:3) \u00b7\n\nvn\n\n\u2212\n\n2\n\nhs dms \u2212 1\n(cid:3)\n\nthe exponent is bounded, so q is well defined. under q, if t \u2264 vn, then\nhs d(cid:22)ms(cid:23) = mt + at\n\n= mt +\n\nhs dms, m\n\nmt \u2212\n\n\u2212\n\nt\n\ns d(cid:22)m(cid:23)\nh2\n\n.\n\ns\n\n0\n\n0\n\nis a martingale by the girsanov theorem (exercise 13.5). therefore st = mt + at is a local\nmartingale.\nfinally, e\n\ns d(cid:22)m(cid:23)s is never zero nor infinite, so q is equivalent to p.\nt\n0 h2\n\n0 hs dms\u2212 1\n\n\u2212(cid:15)\n\n(cid:15)\n\n2\n\nt\n\nlet us give two examples to clarify the proof. let c be the standard cantor set and let g(t )\nbe the cantor function. suppose st = wt + g(t ), where w is a brownian motion. we then\n0 hs dg(s) = 1.\nlet ht = 1c (t ). since the cantor function increases only on the cantor set,\ns ds = 0. but this is the quadratic\nsince the cantor set has lebesgue measure 0, then\nvariation of\n\n1\n0 hs dws, so this stochastic integral is also 0. it follows that\n\n1\n0 h 2\n\n(cid:15)\n\n(cid:15)\n\n1\n\n(cid:3)\n\n(cid:3)\n\n(cid:3)\n\n(cid:15)\n\n1\n\nhs dss =\n\n1\n\nhs dws +\n\n1\n\nhs dg(s) = 1,\n\n0\n\n0\n\n0\n\n "}, {"Page_number": 244, "text": "226\n\nfinancial mathematics\n\nwhich says that with the trading strategy h we make a profit of 1 almost surely, that\nis, without any risk. therefore the nflvr condition is violated. this example indicates why\nwe must have dat absolutely continuous with respect to d(cid:22)m(cid:23)\nt.\nt\n0 hs ds with hs bounded. let\n\nsuppose now that w is a brownian motion and st = wt +(cid:15)\nand define q on f1 by dq/dp = m1. by the girsanov theorem, st = wt +(cid:15)\n\nt\n0 hs ds is a\nmartingale under q. this example shows that if the radon\u2013nikodym derivative of dat with\nrespect to d(cid:22)m(cid:23)\n\nt is not too bad, we can apply the girsanov theorem.\n\nmt = e\n\n0 hs dws\u2212 1\n\n\u2212(cid:15)\n\ns ds,\n\nt\n0 h 2\n\n(cid:15)\n\n2\n\nt\n\n28.4 stochastic control\n\nthe theory of stochastic control, which includes a study of the hamilton\u2013jacobi\u2013bellman\n(hjb) equation and requires some knowledge of partial differential equations, is beyond the\nscope of this book. however, we can consider one simple useful example. suppose we have\navailable to us a stock which satisfies the sde\n\ndst = \u03c3 st dwt + \u03bcst dt,\n\nwhere wt is a brownian motion, and a risk-free asset which satisfies the equation\n\ndbt = rbt dt.\n\nwe want to put a proportion u of our wealth zt into the stock and the remainder into the\nrisk-free asset. we will restrict 0 \u2264 u \u2264 1, so that we do not borrow nor have short selling.\nalso, we take \u03bc > r, for if the mean rate of return on the stock is less than the risk-free rate,\nwe simply put all our money in the risk-free asset. how do we choose u in order to maximize\nour return?\n\nfirst of all, what do we mean by maximizing our return? typically one chooses ahead\nof time a deterministic function u , called the utility function, and one wants to maximize\ne u (zt0\n) at some fixed time t0. usually utility functions are taken to be increasing and\nconcave. the function is chosen to be increasing because more money is considered better. it\nis chosen concave because one assumes that twice the amount of money will give increased\npleasure, but not twice as much pleasure.\nlet us work out the optimal control problem when u (x) = xp for some p \u2208 (0, 1). if\nzt (depending on u) is our wealth, we have zt = st + bt and st = uzt, bt = (1 \u2212 u)zt.\nwe will allow u to depend on t and \u03c9, but our answer will turn out to be deterministic and\nindependent of t, i.e., u is a constant.\n\nwe have seen (proposition 24.6) that\n\nand (cid:22)s(cid:23)\n\nt\n\n= \u03c3 2s2\n\nt dt and that the equation for bt has the solution\n\nst = s0e\u03c3wt\u2212\u03c3 2t/2+\u03bct\n\nbt = b0ert .\n\n "}, {"Page_number": 245, "text": "therefore neither st nor bt can ever be 0 or negative, and so zt > 0 for all t. applying it\u02c6o\u2019s\nformula to z p\n\nexercises\n\n227\n\nt\n\nt\n\nt\n\nt\n\nd(cid:22)z(cid:23)\n\nt, we have\n2 p(p \u2212 1)z p\u22122\n\n= (cid:22)s(cid:23)\ndzt + 1\nt \u03bcst dt + pz p\u22121\n\u03c3 st dwt + pz p\u22121\n\u03c3 2s2\nt dt\nt \u03bc dt + p(1 \u2212 u)rz p\nt dt\n\nt and noting that (cid:22)z(cid:23)\nt = pz p\u22121\ndz p\n= pz p\u22121\n2 p(p \u2212 1)z p\u22122\n+ 1\nt \u03c3 dwt + puz p\n= puz p\n2 p(p \u2212 1)z p\n+ 1\n(cid:3)\n\nt \u03c3 2u2 dt.\n\nt\n\nt\n\nt\n\nrbt dt\n\ne z p\nt0\n\n= e z p\n\n0\n\n+ pe\n\nt0\n\nt [u\u03bc + (1 \u2212 u)r + 1\nz p\n\n2\n\n(p \u2212 1)\u03c3 2u2] dt.\n\n0\n\ntherefore\n\nthis will be largest if the expression\n\nf (u) = u\u03bc + (1 \u2212 u)r + 1\n\n2\n\n(p \u2212 1)\u03c3 2u2\n\nis largest, which by elementary calculus is largest when\n\nu = \u03bc \u2212 r\n(1 \u2212 p)\u03c3 2\n\n.\n\n28.1 let\n\n\u0001(x) = 1\u221a\n2\u03c0\n\n\u2212y2/2 dy,\n\nx\n\n\u2212\u221e e\n\nexercises\n\n(cid:3)\n\nthe cumulative normal distribution function. rewrite the black\u2013scholes formula for the value\nof a european call in terms of \u0001. this is the way the black\u2013scholes formula is written in finance\nbooks.\n\n28.2 a european put that gives one the option to sell a share of stock at price k at time te has value\n\nat time te. find the present-day value of the european put at time 0.\n\n(k \u2212 ste\n\n)+\n\n28.3 carry out the details of the proof of theorem 28.1 for case 2.\n28.4 if the utility function in section 28.4 is u (x) = log x instead of u (x) = xp, what is the optimal\n\nchoice for u?\n\nsn = (cid:12)\n\n28.5 let a, b > 0, let yi be i.i.d. random variables that take only the values b and \u2212a, and let\ni=1 yi. show that if p(y1 = b) > 0 and p(y1 = \u2212a) > 0, there exists a probability\nmeasure q equivalent to p under which sn is a martingale. describe the radon\u2013nikodym\nderivative of q with respect to p.\n\nn\n\n28.6 suppose the interest rate r is equal to 0 and an option v has payoff\n\nat time te. what is the price of v at time 0?\n\nss\n\nsup\ns\u2264te\n\n "}, {"Page_number": 246, "text": "financial mathematics\n\nwhat is the price of u at time 0?\n\n228\n28.7 suppose the interest rate r is equal to 0. let u be the option that pays off \u2212 inf s\u2264te ss at time te.\nif v is as in exercise 28.6, then u + v is the option that pays off the maximum of the stock\nprice minus the minimum of the stock price, in other words, \u201cbuy low, sell high.\u201d naturally\nsuch an option would be expensive. it is remarkable that there exists a trading strategy that can\nduplicate this payoff, even though the times when the maximum and minimum occur are not\nstopping times.\n\n "}, {"Page_number": 247, "text": "29\n\nfiltering\n\nstochastic filtering is a nice example of nontrivial interesting mathematics that is extremely\nuseful. for example, it has been used extensively in nasa\u2019s space program.\n\nthe method we use is called the innovations approach to filtering, and uses l\u00b4evy\u2019s theorem,\n\nthe martingale representation theorem, and other results from stochastic calculus.\n\nwe will start with a fairly general model, except for simplicity we will assume our\nobservation process is one-dimensional. the extension to the d-dimensional case is mostly\nroutine. later on we will look at a specific model, the linear model, where one can give fairly\nexplicit solutions to the filtering equation for real-life problems.\n\n29.1 the basic model\n\nwe start with a probability space (\u0001,f , p), together with a filtration {ft} satisfying the\nusual conditions. in filtering theory, there are a number of filtrations present, and we will\nneed to be careful about which ones are which.\nwe have a signal process xt taking values in a complete separable metric space s and we\nlet {f x\n} be the minimal augmented filtration generated by x . we have a function f mapping\ns to the reals, we suppose e| f (xt )|2 < \u221e for all t, and we suppose that there exists a\nprocess as adapted to the filtration {f x\n\n} such that\n\nt\n\nt\n\n(cid:3)\n\nmt = f (xt ) \u2212 f (x0) \u2212\n\nt\n\nas ds\n\n0\n\nt\n\nt\n\nt\n\nis a martingale with respect to the filtration {f x\nlet wt be a one-dimensional brownian motion with respect to the filtration {f x\n(cid:3)\nreal-valued process adapted to {f x\n}, and suppose\nzt = wt +\n\n}. next we discuss the observation process.\n}, let ht be a\n\n(29.1)\nthe process zt is called the observation process and is what we observe. let {f z\n} be the\nfiltration generated by the process z. in practice one does not necessarily want to assume that\n{f z\n} is right continuous, but let us assume that it is for simplicity. requiring the filtration to\nbe complete is not a serious issue.\nfor an example, suppose that dxt = \u03c3 (xt ) dw t + b(xt ) dt as in chapter 24, where w t is\na d-dimensional brownian motion and \u03c3 and b are matrix valued, and suppose f \u2208 c2(rd )\nis bounded or has linear growth. then it\u02c6o\u2019s formula shows that such an f will satisfy our\n\nhs ds.\n\n0\n\nt\n\nt\n\nt\n\n229\n\n "}, {"Page_number": 248, "text": "230\n\nfiltering\n\nassumptions. in this case hs in (29.1) is of the form g(xs) for a particular function g; see\nsection 39.3.\nthe goal of filtering is to get the best estimate of f (xt ) from the observations {zt}. we\nwant to find the best estimate for f (xt ) in the following sense. we want to minimize the mean\nsquare error e| f (xt )\u2212 y|2 over all random variables y that are f z\nt measurable, i.e., over all\nrandom variables that can be determined by the observations up to time t. the rationale is\nthat since f z\nis the information we have observed up to time t, we want our estimate to be\nf z\nt measurable, and among all random variables that are f z\nt measurable, we want the one\nclosest to f (xt ) in l2 norm, which means we minimize the mean square error.\nlemma 29.1 the best mean square error estimate of f (xt ) over the class of f z\nrandom variables is\n\nt measurable\n\nt\n\ny = e [ f (xt ) | f z\nt ].\n\nproof by our assumptions on f , the random variable v = f (xt ) is in l2(p). let y be\nthe best mean square estimator. the collection m of l2 random variables which are f z\nt\nmeasurable is a linear subspace of l2, and the element of a hilbert space that minimizes the\ndistance from v to this subspace m is the projection onto m. therefore y is the projection\nof v onto m. hence v \u2212 y is orthogonal (in the l2 sense) to every element of m. in\nparticular, if e \u2208 f z\nt ,\n\ne [(v \u2212 y )1e] = 0,\n\nwhich implies e [v; e] = e [y; e]. this holds for every e \u2208 f z\nhence y = e [v | f z\nt ].\n\nt and y is f z\ngiven any process ht that is {ft} adapted, we use the notation(cid:2)ht = e [ht | f z\n(cid:2)hs ds, and you might wonder about the joint measurability of (cid:2)h\nin \u03c9 and t, since (cid:2)ht is only defined almost surely for each t. the way to deal with this is to\nlet(cid:2)ht be the optional projection of h with respect to the optional \u03c3 -field generated by {f z\n\nlook at expressions like\n\nt measurable,\n\nt ]. we will\n\n(cid:15)\n\n};\n\nt\n0\n\nt\n\nsee (16.8) in chapter 16.\n\n(cid:3)\n\n29.2 the innovation process\n\nwe next define the innovation process\n\n(cid:2)hs ds.\n(following our convention on notation,(cid:2)hs = e [hs | f z\nmeasurable, we cannot determine it from (29.2) because it contains the unknown(cid:2)hs on the\ns ].) note that although nt is f z\n\nnt = zt \u2212\n\n(29.2)\n\n0\n\nt\n\nt\n\nright-hand side.\nproposition 29.2 nt is a brownian motion with respect to the filtration {f z\nproof we will show that nt\nis a continuous martingale with respect to the filtration\n{f z\n} whose quadratic variation is t, and then our result follows from l\u00b4evy\u2019s theorem\n= t from\n(theorem 12.1). that nt is continuous is obvious, and (cid:22)n(cid:23)\nthe definitions of z and w . thus we need to show that n is a martingale with respect to {f z\n}.\n\n= (cid:22)w(cid:23)\n\n= (cid:22)z(cid:23)\n\n}.\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\n "}, {"Page_number": 249, "text": "231\n\n(29.3)\n\n(29.4)\n\n(29.5)\n\nif r \u2265 s, we have\n\ne [(cid:2)hr | f z\n\nthen using exercise 29.1,\n\ne [nt \u2212 ns | f z\n\n29.3 representation of f z-martingales\n\ns ] = e [e [hr | f z\n\nt\n\ns ] = e [hr | f z\nr ] | f z\ns ].\n(cid:3)\ne [(cid:2)hr | f z\n(cid:3)\ne [hr \u2212(cid:2)hr | f z\ns ] | f z\ns ] = 0,\n\ns ] dr\n\ns\n\ns\n\nt\n\ns ] = e [zt \u2212 zs | f z\ns ] \u2212\n= e [wt \u2212 ws | f z\ns ] +\n= e [e [wt \u2212 ws | f x\n\ns ] dr\n\nsince f z\n\ns\n\n\u2282 f x\ns .\n\n29.3 representation of f z-martingales\n\nin this section we prove that if yt is a martingale with respect to {f z\n}, then y can be\nrepresented as a stochastic integral with respect to n. this is not an immediate consequence\nof theorem 12.3 because we do not know that nt generates {f z\n}; the filtration generated by\nn could conceivably be strictly smaller than the one generated by z.\ntheorem 29.3 suppose yt is a square integrable martingale with respect to {f z\nthe predictable \u03c3 -field defined on [0,\u221e) \u00d7 \u0001 in terms of {f z\nis p z measurable and with e\n\n}. let p z be\n}. then there exists hs which\n\n(cid:15) \u221e\n\nt\n\nt\n\nt\n\nt\n\n0 h 2\n\ns ds < \u221e such that\nyt = y0 +\n\nhs dns\n\nt\n\n(cid:3)\n\n0\n\nto clarify, p z is the \u03c3 -field generated by all bounded left-continuous processes that are\n\nfor all t.\n\nt\n\n}.\n\nadapted to {f z\n(cid:10)\nproof first let us treat the case where\nq on f z\n= mt, where\n\u2212\nmt = exp\n\nt by dq/dp |f z\n\nt\n\n|(cid:2)hs|2 ds, and yt are each bounded. define\n(cid:3)\n\n(cid:11)\n|(cid:2)hs|2 ds\n\n.\n\nt\n\n0\n\n(cid:15)\n(cid:3)\n\nt\n0\n\nt\n0\n\n(cid:15)\n(cid:2)hs dns,\n(cid:2)hs dns \u2212 1\n(cid:3)\n(cid:2)hs ds\n\n2\n\nt\n\nt\n\n0\n\nthen by the girsanov theorem (theorem 13.3)\nzt = nt +\nis a martingale under q with respect to {f z\n}. since (cid:22)z(cid:23)t = (cid:22)n(cid:23)t = (cid:22)w(cid:23)t = t, then z is a\nbrownian motion under q with respect to {f z\n}.\ne q[(cid:14)yt; a] = e p[mt (m\nt yt. if a \u2208 f z\ns , then a \u2208 f x\n\u22121\ns and\ns ys); a] = e q[(cid:14)ys; a].\nt yt ); a] = e p[yt; a] = e p[ys; a]\n\u22121\n\u22121\n\nlet(cid:14)yt = m\n\n= e p[ms(m\n\n0\n\nt\n\nt\n\n "}, {"Page_number": 250, "text": "}. by the martingale representation\n(cid:3)\n\n(cid:2)htkt dt.\n\n= \u2212mt\n\nt\n\n(cid:2)hsks ds,\n\nt\n\nt\n\nt\n\nt\n\n0\n\n0\n\n0\n\nks\n\n232\n\n(cid:3)\n\nfiltering\n\nks dns +\n\nby the product formula,\n\n(cid:14)yt =(cid:14)y0 +\n\ntherefore(cid:14)yt is a martingale under q with respect to {f z\ntheorem (theorem 12.3) there exists ks \u2208 p z such that\n\non the other hand, dmt = \u2212mt\n\n(cid:2)hs ds.\n(cid:14)yt. we have d(cid:22)m, y(cid:23)\n(cid:3)\n\n(cid:3)\nks dzs =(cid:14)y0 +\n(cid:2)ht dnt and yt = mt\n(cid:3)\n(cid:3)\nms d(cid:14)ys + (cid:22)m,(cid:14)y(cid:23)\n(cid:14)ys dms +\n(cid:3)\n(cid:3)\n(cid:2)hsms ds \u2212\n(cid:2)hs dns +\n(cid:14)ysms\n(cid:2)hs.\nwhich is of the desired form if we set hs = ksms \u2212(cid:14)ysms\n|(cid:2)hs|2 ds + |yt| \u2265 k\n\nyt = m0\n= y0 \u2212\n\n(cid:2)hs dns\n\n(cid:14)y0 +\n(cid:3)\n\nin the general case, let\n\nksms dns +\n\ntk = inf\n\n0\n\n(cid:19)\n\n.\n\n(cid:20)(cid:20)(cid:20)(cid:3)\n\n(cid:18)\n\n(cid:3)\n\nks\n\nt :\n\n0\n\n0\n\n0\n\n0\n\n0\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nms\n\n0\n\n0\n\nwe apply the above argument to yt\u2227tk and use exercise 29.3 to get\n\n(cid:20)(cid:20)(cid:20) +\n(cid:3)\n\n= y0 +\n\nyt\u2227tk\n\nt\n\nh k\n\ns dns,\n\n0\n\nwhere h k\ns\nyt is square integrable, ytk\n\nis predictable with respect to the \u03c3 -fields {f z\nt\u2227tk\n\n} and is 0 from time tk on. since\n\n\u2192 y\u221e almost surely and in l2(p) as k \u2192 \u221e, and\n\n(cid:16)(cid:3) \u221e\n\ne\n\n0\n\n(cid:17)\n|2 ds\n\n|h k\n\ns\n\n\u2212 h l\n\ns\n\n= e [|ytk\n\n\u2212 ytl\n\n|2] \u2192 0\n\nas k, l \u2192 \u221e. using the completeness of l2, there exists hs such that e\n|hs \u2212 h k\nand e\nand that (29.5) holds.\n\ns ds < \u221e\n|2 ds \u2192 0 as k \u2192 \u221e. it is routine to check that hs is p z measurable\n\n0 h 2\n\n0\n\ns\n\n(cid:15) \u221e\n\n(cid:15) \u221e\n\n29.4 the filtering equation\n\nlemma 29.4 if yt \u2212(cid:15)\n\nmartingale with respect to {f z\n\n}.\n\nt\n\nwe now derive the general filtering equation. first we need a lemma.\n\nt\n\n0 hs ds is a martingale with respect to {f x\n\nt\n\n}, then(cid:2)yt \u2212(cid:15)\n\nt\n0\n\n(cid:2)hs ds is a\n\n "}, {"Page_number": 251, "text": "29.4 the filtering equation\n\nproof since f z\n\n(cid:3)\n\n\u2282 f x\ns ,\n\n(cid:16)(cid:2)yt \u2212(cid:2)ys \u2212\n\ns\n\ne\n\ns\n\n= e\n\n= e\n\n= e\n\nt\n\ns\n\n(cid:17)\n(cid:2)hr dr | f z\n(cid:16)\n(cid:3)\n(cid:16)\n(cid:17)\nt ] \u2212 e [ys | f z\ne [yt | f z\ns ] \u2212\n(cid:3)\n(cid:16)\n(cid:16)\nhr dr | f z\nyt \u2212 ys \u2212\nyt \u2212 ys \u2212\n\nhr dr | f x\n\ne\n\ns\n\ns\n\nt\n\nt\n\ns\n\ns\n\n(cid:3)\n\n(cid:17)\n\nr ] dr | f z\n\ns\n\nt\n\ns\n\ne [hr | f z\n(cid:17)\n\n= 0.\n\n| f z\n\ns\n\n233\n\n(cid:17)\n\nthe first equality is proved in a fashion similar to the one you were asked to prove in\nexercise 29.1.\n\nt\n\nt\n\n0\n\n0\n\n(cid:3)\n\nproof by lemma 29.4,\n\nhere is the filtering equation.\n\n(cid:2)ft =(cid:2)f0 +\n\nand write fs for f (xs). suppose (cid:22)m, w(cid:23)\n\ntheorem 29.5 let mt = f (xt ) \u2212 f (x0) \u2212(cid:15)\n=(cid:15)\n(cid:3)\n(cid:2)as ds +\nlt =(cid:2)ft \u2212(cid:2)f0 \u2212\n(cid:3)\n(cid:3)\n(cid:3)\n\nis a martingale with respect to {f z\n(cid:3)\n(cid:3)\n\nby the product formula\n\nlt =\n\n(cid:3)\n(cid:3)\n\nfs dzs +\nfs dns +\n\nftzt =\n=\n= f x -martingale +\n\nzs dfs +\n(cid:3)\nfshs ds +\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\n}\n\nt\n\nt\n\n0 as ds be a martingale with respect to {f x\n(#fshs \u2212(cid:2)fs\n(cid:2)hs +(cid:2)ds) dns.\nt\n0 ds ds. then\nt\n(cid:3)\n(cid:2)as ds\n\nt\n\n(29.6)\n\n(29.7)\n\n} and by theorem 29.3, there exists hs such that\n\n0\n\nhs dns.\n\nds ds\nzs dms +\n\nt\n\n(cid:3)\n\nt\n\nzsas ds +\n\n0\n\n(29.8)\n\n(cid:3)\n\nt\n\n0\n\nds ds\n\n[fshs + zsas + ds] ds.\nby lemma 29.4 and the obvious fact that z is adapted to {f z\n},\n\nt\n\nt\n\n0\n\n(cid:3)\n(#fshs + zs\n(cid:2)ftzt =#ftzt = f z-martingale +\n(cid:3)\n(cid:2)ftzt =\nzs d(cid:2)fs +\n(cid:3)\n(cid:2)hs + zs\n[(cid:2)fs\n\n= f z-martingale +\n\n(cid:2)fs dzs +\n\nhs ds\n\n(cid:3)\n\n(cid:3)\n\n0\n\n0\n\n0\n\n0\n\nt\n\nt\n\nt\n\nt\n\nagain using the product formula,\n\n0\n\n(cid:2)as + hs] ds.\n\n(cid:2)as +(cid:2)ds) ds.\n\n "}, {"Page_number": 252, "text": "234\n\ntherefore\n\n(cid:3)\n\n(#fshs + zs\n\nt\n\nfiltering\n\n(cid:2)as +(cid:2)ds \u2212(cid:2)fs\n\n(cid:2)hs \u2212 zs\n\n(cid:2)as \u2212 hs) ds\n\n0\n\nis a continuous f z-martingale that has paths that are locally of bounded variation and which\nis zero at time zero, hence is identically zero by theorem 9.7. hence with probability one,\n\n(cid:2)hs +(cid:2)ds for almost every s. substituting this in (29.8) and combining with\n\nhs = #fshs \u2212(cid:2)fs\n\n(29.7) gives our result.\n\n29.5 linear models\n\nthe filtering equation (29.6) is difficult to apply in most cases. however, in the linear model,\nwe can get a much simpler representation. to define the linear model in d dimensions, let xt\nsolve\n\n(29.9)\nwhere w t is a d-dimensional brownian motion and a(t ) and b(t ) are deterministic d \u00d7 d\nmatrices that are continuous in t. let\n\ndxt = a(t ) dw t + b(t )xt dt,\n\ndzt = dwt + c(t )xt dt,\n\n(29.10)\nwhere c is a deterministic d \u00d7 d matrix-valued function that is continuous in t and wt is a\nd-dimensional brownian motion independent of w and x .\n\nwhy is this model useful? suppose xt is two-dimensional with x (1)\n\nbeing the position of\na particle and x (2)\nits velocity. suppose the position and the velocity have some randomness\nand that our observations of the position and velocity are noisy. this fits into the model\n(29.9)\u2013(29.10) if we take\na(t ) =\n\n$\n\n$\n\n%\n\n%\n\n$\n\n%\n\n.\n\nt\n\nt\n\n1 0\n0 1\n\n0 1\n0 0\n\nc1\n0\n\n0\nc2\n\nfor another example, suppose a particle has a fixed unknown velocity and the position is\nbe the position and velocity and let a(t )\n\nand x (2)\n\nt\n\nobserved, but obscured by noise. let x (1)\nbe the zero matrix,\n\n, c(t ) =\n%\n$\n\n, b(t ) =\n%\n$\n\nt\n\nb(t ) =\n\n0 1\n0 0\n\n, c(t ) =\n\n1 0\n0 0\n\n.\n\nthe solution of the filtering problem modeled by (29.9)\u2013(29.10) is known as the kalman\u2013\nbucy filter. for simplicity we will consider the special case where the dimension d is 1 and\na, b, c are constant in t; the general case is done in exactly the same way, but the notation\nbecomes much more complicated (see kallianpur, 1980). we will further assume e x0 and\nvar x0 are known.\n\nlet\n\nthe conditional variance of xt given f z\nt .\n\n29.6 kalman\u2013bucy filter\n\nvt =&\n\nt \u2212 ((cid:2)xt )2,\n\nx 2\n\n "}, {"Page_number": 253, "text": "theorem 29.6 vt solves the deterministic ordinary differential equation\n\n29.6 kalman\u2013bucy filter\n\nin particular, vt is deterministic.(cid:2)xt solves\n\ndvt\ndt\n\n= 1 + 2bvt \u2212 c2v 2\n\nd(cid:2)xt = cvt dzt + (b \u2212 cvt )(cid:2)xt dt,\n\nt\n\n,\n\nv0 = var x0\n\n(cid:2)x0 = e x0.\n\n235\n\n(29.11)\n\n(29.12)\n\nthe equation (29.11) is an example of what is known as a riccati equation. we get a\nsimilar equation when d > 1 or when a, b, and c depend on t, but in general one cannot\nsolve the riccati equation explicitly. however, when d = 1 and a, b, c do not depend on t,\none can solve (29.11) by separation of variables. write\n\ndv\n\n1 + 2bv \u2212 c2v 2\n\n= dt,\n\nwhen d = 1 (and even if a, b, and c depend on time), we can solve (29.12). let\n\nand integrate both sides.\ngt = b \u2212 cvt so that we have\n\nor by the product formula\n\nand hence\n\n(cid:2)xt dt,\n\nd(cid:2)xt = cvt dzt + gt\n(cid:16)\n(cid:17)\n0 gr dr(cid:2)xt\n\u2212(cid:15)\n\u2212(cid:15)\n(cid:2)xt = e x0 +\n(cid:15)\ns gr drcvs dzs.\n\n= e\n(cid:3)\n\n0 gr drcvt dzt ,\n\ne\n\ne\n\nt\n\nt\n\nt\n\nt\n\nd\n\n0\n\n(cid:3)(cid:3)(xs) + bxs f\n\n(cid:3)(xs)] ds.\n\nt\n\n0\n\n[ 1\n2 f\n\n(cf. the solution of (24.15).)\n(cid:3)\nproof of theorem 29.6 by it\u02c6o\u2019s formula, if f \u2208 c2,\nf (xt ) \u2212 f (x0) = f x -martingale +\nby the filtering equation applied with f (x) = x,\n\n(cid:3)\n(cid:3)\n(cid:2)xs ds + c\n(cid:2)xt = e x0 + b\nby exercises 29.4(2) and 29.5(3), &\n&\nt = 2(cid:2)xtvt .\nt \u2212(cid:2)xt\n(cid:3)\nwith the filtering equation applied with f (x) = x2 and (29.14),\n(1 + 2b&x 2\n(cid:3)\n(1 + 2b&x 2\n\n&\nt = e x 2\nx 2\n= e x 2\n\n) ds + c\n) ds + 2c\n\n+ c\n+ c\n\n(cid:3)\n(cid:3)\n\nx 3\n\nx 2\n\n0\n\n0\n\n0\n\n0\n\n0\n\ns\n\nt\n\nt\n\nt\n\nt\n\nt\n\ns\n\n0\n\n0\n\nvs dns.\n\n&x 2\n(&x 3\n\u2212(cid:2)xs\n(cid:2)xs dns.\n\nvs\n\ns\n\ns\n\nt\n\n0\n\n(29.13)\n\n(29.14)\n\n) dns\n\n "}, {"Page_number": 254, "text": "236\n\ntherefore\n\ndvt = d (&\n\nt \u2212 ((cid:2)xt )2)\n(cid:2)xt dnt + (1 + 2b\n\nx 2\n= 2cvt\n= (1 + 2bvt \u2212 c2v 2\n\n) dt.\n\nt\n\nfiltering\n\n&\nt dt ) \u2212 2(cid:2)xt (cvt dnt + b(cid:2)xt dt ) \u2212 c2v 2\n\nx 2\n\nt dt\n\n(29.15)\n\nthis shows that vt solves the deterministic ordinary differential equation (29.15). this\nequation has a unique solution (cf. theorem 15.1), so vt is deterministic. we obtain (29.12)\nfrom (29.2), (29.10), and (29.13).\n\nexercises\n\n29.1 justify the first equality in (29.4).\n29.2 show that if mt is a martingale with respect to {f x\n\nt\n\n}, then (cid:2)mt is a martingale with respect\n\n29.3 suppose w is a brownian motion and {ft} is its minimal augmented filtration. let t be a\nbounded stopping time with respect to {ft}. suppose y is a ft measurable random variable\ns ds < \u221e such\nwith e y 2 < \u221e. show that there exists a predictable process hs with e\n\nt\n0 h 2\n\n(cid:15)\n\nto {f z\n\n}.\n\nt\n\nthat y = e y +(cid:15)\n\nt\n0 hs dws, a.s.\n\n29.4 (1) show that the solution to (29.9) is a gaussian process.\n\n(2) show that the solutions (xt , zt ) to (29.9)\u2013(29.10) form a gaussian process.\n29.5 (1) show that if x is a normal random variable with mean \u03bc and variance \u03c3 2, then\n\ne x 3 = \u03bc(\u03bc2 + 3\u03c3 2 ).\n\n(2) show that if x , y1, . . . , yn are jointly normal random variables, then\n\ne [x 3 | y1, . . . , yn] = e [x | y1, . . . , yn](e [x | y1, . . . , yn]2\n\n+ 3var [x | y1, . . . , yn]),\n\nwhere\n\n(3) show that\n\nwhere\n\nvar [x | y1, . . . , yn] = e [(x \u2212 e [x | y1, . . . , yn])2 | y1, . . . , yn].\n\n&\n\nx 3\nt\n\n=(cid:2)xt (((cid:2)xt )2 + 3var (xt | f z\nt ] =&\n) = e [(xt \u2212(cid:2)xt )2 | f z\n\nx 2\nt\n\nt\n\n)),\n\n\u2212 ((cid:2)xt )2.\n\nvar (xt | f z\n\nt\n\nfor more on filtering, see kallianpur (1980) and \u00f8ksendal (2003).\n\nnotes\n\n "}, {"Page_number": 255, "text": "30\n\nconvergence of probability measures\n\nsuppose we have a sequence of probabilities on a metric space s and we want to define\nwhat it means for the sequence to converge weakly. alternately, we may have a sequence of\nrandom variables and want to say what it means for the random variables to converge weakly.\nwe will apply the results we obtain here in later chapters to the case where s is a function\nspace such as c[0, 1] and obtain theorems on the convergence of stochastic processes.\nfor now our state space is assumed to be an arbitrary metric space, although we will\nsoon add additional assumptions on s. we use the borel \u03c3 -field on s, which is the \u03c3 -field\ngenerated by the open sets in s. we write a0, a, and \u2202a for the interior, closure, and boundary\nof a, respectively.\n\n30.1 the portmanteau theorem\n\n(cid:15)\n\nclearly the definition of weak convergence of real-valued random variables in terms of dis-\ntribution functions (see section a.12) has no obvious analog. the appropriate generalization\nis the following; cf. proposition a.41.\ndefinition 30.1 a sequence of probabilities {pn} on a metric space s furnished with the\nf dp for every bounded\nborel \u03c3 -field is said to converge weakly to p if\nand continuous function f on s. a sequence of random variables {xn} taking values in s\nconverges weakly to a random variable x taking values in s if e f (xn) \u2192 e f (x ) whenever\nf is a bounded and continuous function.\n\nf dpn \u2192 (cid:15)\n\nborel subset a of s, then e f (xn) =(cid:15)\n\nsaying xn converges weakly to x is the same as saying that the laws of xn converge weakly\nto the law of x . to see this, if pn is the law of xn, that is, pn(a) = p(xn \u2208 a) for each\nf dp. (this holds when f is\nan indicator by the definition of the law of xn and x , then for simple functions by linearity,\nthen for non-negative measurable functions by monotone convergence, and then for arbitrary\nbounded and borel measurable f by linearity.)\n\nf dpn and e f (x ) =(cid:15)\n\nwhat might cause a bit of confusion is that weak convergence in probability is not the\nsame as weak convergence in functional analysis, but rather is equivalent to what is known as\nweak-\u2217 convergence in functional analysis. feel free to skip the remainder of this paragraph\nis its dual, then xn \u2208 b\n\u2217\nwhere we explain this. recall that if b is a banach space and b\nconverges weakly to x \u2208 b if f (xn) \u2192 f (x) for all f \u2208 b\n. fn \u2208 b\n\u2217\n\u2217\nconverges with respect\nto the weak-\u2217 topology to f \u2208 b\nif fn(x) \u2192 f (x) for all x \u2208 b. by the riesz representation\ntheorem, there is a one-to-one correspondence between positive bounded linear functionals\non b = c(x ), the continuous functions on x , where x is compact, and the set m of finite\n\n\u2217\n\n237\n\n "}, {"Page_number": 256, "text": "convergence of probability measures\n\n238\n(cid:15)\ncan be identified with m, and measures pn with mass\nmeasures on x . when b = c(x ), b\n\u2217\n1 in m converge to p \u2208 m with respect to the weak-\u2217 topology if pn(g) \u2192 p(g) for every\ng \u2208 b = c(x ). interpreting pn(g) as\n\ng dpn shows the connection.\n\nreturning to weak convergence in the probability sense, the following theorem, known as\n\nthe portmanteau theorem, gives some other characterizations. for this chapter we let\n\nf\u03b4 = {x : d (x, f ) < \u03b4}\n\n(30.1)\n\nfor closed sets f, the set of points within \u03b4 of f, where d (x, f ) = inf{d (x, y) : y \u2208 f}.\ntheorem 30.2 suppose {pn, n = 1, 2, . . .} and p are probabilities on a metric space. the\nfollowing are equivalent.\n\n(1) pn converges weakly to p.\npn(f ) \u2264 p(f ) for all closed sets f.\n(2) lim supn\n(3) lim inf n pn(g) \u2265 p(g) for all open sets g.\n(4) limn pn(a) = p(a) for all borel sets a such that p(\u2202a) = 0.\n\nproof the equivalence of (2) and (3) is easy because if f is closed, then g = f c is open\nand pn(g) = 1 \u2212 pn(f ).\n\nto see that (2) and (3) imply (4), suppose p(\u2202a) = 0. then\n\nlim sup\n\nn\n\npn(a) \u2264 lim sup\n\npn(a) \u2264 p(a)\n\nn\n\n= p(a0) \u2264 lim inf pn(a0) \u2264 lim inf pn(a).\n\nnext, let us show (4) implies (2). let f be closed. if y \u2208 \u2202f\u03b4, then d (y, f ) = \u03b4. the sets\n\u2202f\u03b4 are disjoint for different \u03b4. at most countably many of them can have positive p-measure,\nhence there exists a sequence \u03b4k \u2193 0 such that p(\u2202f\u03b4k\npn(f\u03b4k\n\n) = 0 for each k. then\n) = p(f\u03b4k\n\npn(f ) \u2264 lim sup\n\n) = p(f\u03b4k\n\nlim sup\n\n)\n\nn\n\nn\n\n) \u2193 p(f ) as \u03b4k \u2192 0, this gives (2).\n\nfor each k. since p(f\u03b4k\nwe show now that (1) implies (2). suppose f is closed. let \u03b5 > 0. take \u03b4 > 0 small\nenough so that p(f\u03b4 ) \u2212 p(f ) < \u03b5. then take f continuous, to be equal to 1 on f, to have\nsupport in f\u03b4, and to be bounded between 0 and 1. for example, f (x) = 1\u2212 (1\u2227 \u03b4\u22121d (x, f ))\nwould do. then\n\npn(f ) \u2264 lim sup\n\nlim sup\n\nn\n\nf dp\n\n(cid:3)\n\n(cid:3)\nf dpn =\n\u2264 p(f\u03b4 ) \u2264 p(f ) + \u03b5.\n(cid:3)\n\n(cid:3)\n\nn\n\nf dpn \u2264\n\nsince this is true for all \u03b5, (2) follows.\n\nfinally, let us show (2) implies (1). let f be bounded and continuous. if we show\n\n(30.2)\nfor every such f , then applying this inequality to both f and \u2212 f will give (1). by adding a\nsufficiently large positive constant to f and then multiplying by a suitable constant, without\n\nlim sup\n\nf dp,\n\nn\n\n "}, {"Page_number": 257, "text": "loss of generality we may assume f\nfi = {x : f (x) \u2265 i/k}, which is closed.\n\n30.2 the prohorov theorem\n\n239\n\nis bounded and takes values in (0, 1). we define\n\n(cid:10)\n\ni \u2212 1\nk\n\n\u2264 f (x) <\n\ni\nk\n\ni\nk\n\npn\n\n(cid:11)\n\n(cid:3)\n\nf dpn \u2264 k(cid:9)\n= k(cid:9)\n= k\u22121(cid:9)\n\ni=1\n\ni=1\n\n[pn(fi\u22121) \u2212 pn(fi)]\ni\npn(fi) \u2212 k(cid:9)\nk\ni + 1\nk(cid:9)\nk\ni=0\n\u2264 1\n+ 1\nk\nk\n\npn(fi).\n\ni=1\n\ni\nn\n\npn(fi)\n\np(fi).\n\n(cid:3)\n\ni=1\n\ni=1\n\nk(cid:9)\nf dp \u2265 1\nk\nk(cid:9)\nk(cid:9)\n\ni=1\n\n+ 1\nk\n+ 1\nk\n\ni=1\n\nf dpn \u2264 1\nk\n\u2264 1\nk\n\n(cid:3)\n\nlim sup\n\npn(fi)\n\nn\n\np(fi) \u2264 1\nk\n\n+\n\nf dp.\n\nsimilarly,\n\nthen\n\n(cid:3)\n\nlim sup\n\nn\n\nsince k is arbitrary, this gives (30.2).\nif xn \u2192 x, pn = \u03b4xn, and p = \u03b4x, it is easy to see pn converges weakly to p. letting\na = {x} shows that one cannot, in general, have limn pn(f ) = p(f ) for all closed sets f.\n\n30.2 the prohorov theorem\n\nit turns out there is a simple condition that ensures that a sequence of probability measures\nhas a weakly convergent subsequence.\ndefinition 30.3 a sequence of probabilities pn on a metric space s is tight if for every \u03b5\nthere exists a compact set k (depending on \u03b5) such that supn\n\npn(kc) \u2264 \u03b5.\n\nthe important result here is prohorov\u2019s theorem.\n\ntheorem 30.4 if a sequence of probability measures on a metric space s is tight, there is a\nsubsequence that converges weakly to a probability measure on s.\nproof suppose first that the metric space s is compact. then c(s ), the collection of\ncontinuous functions on s, is a separable metric space when furnished with the supremum\nnorm; this is exercise 30.1. let { fi} be a countable collection of non-negative elements of\nc(s ) whose linear span is dense in c(s ). for each i,\nfi dpn is a bounded sequence, so we\n\n(cid:15)\n\n "}, {"Page_number": 258, "text": "(cid:15)\n\n240\n\nconvergence of probability measures\n\nsuch that\n\nfi dpn(cid:3) \u2192(cid:15)\n\nhave a convergent subsequence. by a diagonalization procedure, we can find a subsequence\n(cid:3)\nfi dpn(cid:3) converges for all i. by the term \u201cdiagonalization procedure\u201d we are\nn\nreferring to the well-known method of proof of the ascoli\u2013arzel`a theorem; see any book\nrepresentation theorem (rudin, 1987), there exists a measure p such that l f = (cid:15)\non real analysis for a detailed explanation. call the limit l fi. clearly 0 \u2264 l fi \u2264 (cid:21) fi(cid:21)\u221e,\n(cid:15)\nl is linear, and so we can extend l to a bounded linear functional on s. by the riesz\n(cid:15)\nf dpn(cid:3) \u2192(cid:15)\nf dp.\nfi dp for all fi, it is not hard to see, since each pn(cid:3) has total mass 1, that\nsince\nand continuous, so 1 = pn(cid:3) (s ) =(cid:15)\nf dp for all f \u2208 c(s ). therefore pn(cid:3) converges weakly to p. since l f \u2265 0 if\nf \u2265 0, then p is a positive measure. the function that is identically equal to 1 is bounded\nnext suppose that s is a borel subset of a compact metric space s(cid:3)\n. extend each pn,\ninitially defined on s, to s(cid:3)\nby setting pn(s(cid:3) \\ s ) = 0. by the first paragraph of the proof,\nthere is a subsequence pn(cid:3) that converges weakly to a probability p on s(cid:3)\n(the definition of\nweak convergence here is relative to the topology on s(cid:3)\n). since the pn are tight, there exist\ncompact subsets km of s such that pn(km) \u2265 1\u2212 1/m for all n. the km will also be compact\nrelative to the topology on s(cid:3)\n\n1 dpn(cid:3) \u2192(cid:15)\n\n1 dp, or p(s ) = 1.\n\n, so by theorem 30.2,\np(km) \u2265 lim sup\nn(cid:3)\nsince \u222amkm \u2282 s, we conclude p(s ) = 1.\nif g is open in s, then g = h \u2229 s for some h open in s(cid:3)\n\npn(cid:3) (km) \u2265 1 \u2212 1/m.\n\n. then\n\nn(cid:3)\n\nn(cid:3)\n\nlim inf\n\npn(cid:3) (g) = lim inf\n\npn(cid:3) (h ) \u2265 p(h ) = p(h \u2229 s ) = p(g).\nthus by theorem 30.2, pn(cid:3) converges weakly to p relative to the topology on s.\nnow let s be an arbitrary metric space. since all the pn\u2019s are supported on \u222amkm, we can\nreplace s by \u222amkm, or we may as well assume that s is \u03c3 -compact, and hence separable. it\nremains to embed the separable metric space s into a compact metric space s(cid:3)\n. if d is the\nmetric on s, d \u2227 1 will also be an equivalent metric, that is, one that generates the same\ncollection of open sets, so we may assume d is bounded by 1. now s can be embedded in\ns(cid:3) = [0, 1]n as follows. we define a metric on s(cid:3)\n\nby\n\n(cid:3)(a, b) =\n\nd\n\n\u2212i(|ai \u2212 bi| \u2227 1),\n2\n\na = (a1, a2, . . .), b = (b1, b2, . . .).\n\n(30.3)\n\n\u221e(cid:9)\n\ni=1\n\nbeing the product of compact spaces, s(cid:3)\nof s, let i : s \u2192 [0, 1]n be defined by\n\nis itself compact. if {z j} is a countable dense subset\n\ni (x) = (d (x, z1), d (x, z2), . . .).\n\nwe leave it to the reader to check that i is a one-to-one continuous open map of s to a subset\nof s(cid:3)\n. since s is \u03c3 -compact, and the continuous image of compact sets is compact, then\ni (s ) is a borel set.\n\nclearly, prohorov\u2019s theorem is easily modified to handle the case of finite measures\n\non s.\n\n "}, {"Page_number": 259, "text": "30.3 metrics for weak convergence\n\n241\n\n30.3 metrics for weak convergence\n\nsince we have defined a notion of convergence of probability measures, one might wonder\nif one can make the set of probability measures m on s into a metric space so that weak\nconvergence is equivalent to convergence in m. this is indeed possible and in fact there are\na number of metrics on the space of probability measures that work. we will focus on the\nprohorov metric.\ndefinition 30.5 if p and q are probability measures on a separable metric space s, define\n(30.4)\n\ndm(p, q) = inf{\u03b5 : p(f ) \u2264 q(f\u03b5 ) + \u03b5 for all f closed}.\n\nit is not immediately obvious that dm is even a metric, so the first task is to show that it is.\n\nproposition 30.6 dm is a metric on m.\nproof we start with symmetry, that is, that dm(q, p) = dm(p, q). let \u03b1 be any real\nnumber larger than dm(p, q). if h is closed, then h\u03b1 = {x : d (x, h ) < \u03b1} is open and\nk = s \\ h\u03b1 is closed. note that h \u2282 s \u2212 k\u03b1, where k\u03b1 = {x : d (x, k ) < \u03b1}, because\nif x \u2208 h , then d (x, k ) \u2265 \u03b1, so x /\u2208 k\u03b1 and hence x \u2208 s \\ k\u03b1. since k is closed, by the\ndefinition of dm(p, q),\n\np(h\u03b1 ) = 1 \u2212 p(k ) \u2265 1 \u2212 q(k\u03b1 ) \u2212 \u03b1 = q(s \\ k\u03b1 ) \u2212 \u03b1 \u2265 q(h ) \u2212 \u03b1,\n\nor q(h ) \u2264 p(h\u03b1 ) + \u03b1. since h was an arbitrary closed set, dm(q, p) \u2264 \u03b1, and it follows\nthat dm(q, p) \u2264 dm(p, q). reversing the roles of p and q shows symmetry.\nclearly dm(p, q) \u2265 0. if dm(p, q) = 0, then p(f ) = q(f ) = 0 for all closed sets\nf. since the collection of closed sets generates the borel \u03c3 -field, it is not hard to see that\np(a) = q(a) for all borel subsets a, and hence p = q.\nfinally we prove the triangle inequality. suppose p, q, r \u2208 m. if \u03b1 is any real larger than\ndm(p, q) and \u03b2 any real larger than dm(q, r), then for any \u03b5 > 0 and any closed set f\n\np(f ) \u2264 q(f\u03b1 ) + \u03b1 \u2264 q(f\u03b1 ) + \u03b1\n\n\u2264 r((f\u03b1 )\u03b2 ) + \u03b1 + \u03b2\n\u2264 r(f\u03b1+\u03b2+\u03b5 ) + (\u03b1 + \u03b2 + \u03b5).\n\ntherefore dm(p, r) \u2264 \u03b1 + \u03b2 + \u03b5, and since \u03b5 is arbitrary, the triangle inequality follows.\nnow we show that weak convergence is equivalent to convergence in the topology gen-\n[0, 1] is an example of a nonseparable metric\n\nerated by dm, at least if s is separable. (l\n\u221e\nspace.)\nproposition 30.7 supposes is a separable metric space. a sequence of probability measures\npn on s converges weakly to a probability p if and only if dm(pn, p) \u2192 0.\nproof we first suppose dm(pn, p) \u2192 0 and show that pn converges weakly to p. separa-\nbility is not used in this part of the proof. suppose f is closed and set \u03b5n = dm(pn, p)+ 1/n.\nsince pn(f ) \u2264 p(f\u03b5n\n\n) + \u03b5n, then\nlim sup\n\nn\n\npn(f ) \u2264 lim sup\n\np(f\u03b5n\n\n) = p(f ),\n\nn\n\nand we now apply theorem 30.2(2).\n\n "}, {"Page_number": 260, "text": "242\n\nconvergence of probability measures\n\nwe now suppose pn converges weakly to p. let \u03b5 > 0. cover s with countably many\nballs {bi} of diameter less than \u03b5/2 (separability is used here) and let a1 = b1, a2 = b2 \\ b1,\na3 = b3 \\ (b1 \u222a b2), a4 = b4 \\ (b1 \u222a b2 \u222a b3), and so on. hence the an form a collection of\ndisjoint sets which cover s and each an has diameter less than \u03b5/2. choose n large enough so\nthat p(\u222an\n)\u03b5/2\nsuch that i1, . . . , i j \u2264 n. that is, we look at all finite unions of a1, . . . , an , and then take the\n(\u03b5/2)-enlargements. the collection g is finite. this fact and theorem 30.2(3) imply that we\ncan find n0 such that p(g) \u2264 pn(g) + \u03b5/2 if n \u2265 n0 and g \u2208 g.\nsuppose f is closed. let g = (\u222a{ai : i \u2264 n, ai \u2229 f (cid:16)= \u2205})\u03b5/2. then g \u2208 g and if n \u2265 n0\n\ni=1ai) > 1\u2212 \u03b5/2. let g be the collection of open sets of the form (ai1\n\n\u222a\u00b7\u00b7\u00b7\u222a ai j\n\np(f ) \u2264 p(g) + p(\u222a\u221e\n\ni=n+1ai) \u2264 p(g) + \u03b5/2\n\n\u2264 pn(g) + \u03b5 \u2264 pn(f\u03b5 ) + \u03b5.\n\nin the last inequality we used the definition of g and the fact that the ai have diameters less\nthan \u03b5/2. this shows dm(p, pn) \u2264 \u03b5 if n \u2265 n0, which in turn implies dm(p, pn) \u2192 0.\n\n30.1 if s is a metric space, then it is well known that c(s ), the collection of continuous functions\n\nwith the metric\n\nexercises\n\nd ( f , g) = sup\nx\u2208s\n\n| f (x) \u2212 g(x)|\n\nis a metric space. show that if s is compact, then c(s ) is separable.\n\n30.2 suppose xn converges weakly to x and the random variables zn are such that d (xn, zn )\nconverges to 0 in probability. prove that zn converges weakly to x . this is known as slutsky\u2019s\ntheorem.\nhint: start with p(zn \u2208 f ) \u2264 p(xn \u2208 f\u03b4 ) + p(d (xn, zn ) \u2265 \u03b4).\n\n30.3 suppose xn take values in a normed linear space and converge weakly to x . suppose cn are\n\nscalars converging to c. show cnxn converges weakly to cx .\n\n30.4 give an example of a sequence pn converging weakly to p and a function f that is continuous\n\nbut not bounded such that\n\nf dpn does not converge to\n\nf dp.\n\n30.5 give an example of a sequence pn converging weakly to p and a function f that is bounded but\n\nnot continuous such that\n\nf dpn does not converge to\n\nf dp.\n\n(cid:15)\n\n(cid:15)\n\n(cid:15)\n\n(cid:15)\n\n30.6 show that if xn converges weakly to x and yn converges in probability to 0, then xnyn converges\n\nin probability to 0.\n\npn(a) =(cid:15)\n\n30.7 this exercise considers a sequence of probability measures that have densities. suppose s is\nfurnished with the borel \u03c3 -field and \u03bc is a measure on s. suppose that fn : s \u2192 [0,\u221e) and\n: s \u2192 [0,\u221e) are measurable functions, each of whose integral over s is one, and define\nf\n(1) show that if fn \u2192 f , \u03bc-a.e., then pn converges weakly to p.\n(2) give an example where pn and p are as above, pn converges weakly to p, but fn does not\n\na fn(x) \u03bc(dx) for each n and p(a) =(cid:15)\n\na f (x) \u03bc(dx).\n\nconverge almost everywhere to f .\n\n "}, {"Page_number": 261, "text": "notes\n\n243\n\n30.8 give an example of continuous processes xn and x such that all the finite-dimensional distri-\nbutions of xn converge weakly to the corresponding finite-dimensional distributions of x , but\nwhere xn does not converge weakly to x with respect to the topology of c[0, 1].\n\n30.9 suppose x is a random variable taking values in a complete separable metric space. if \u03b5 > 0,\n\nshow there exists a compact set k such that p(x /\u2208 k ) < \u03b5.\nhint: for each n choose closed balls {bni, i = 1, . . . , nn} such that\n\nthen k = \u2229\u221e\nn=1\n\np(x /\u2208 \u222ann\n\ni=1bni ) < \u03b5/2n+1.\n\u222ann\ni=1 bni is totally bounded, hence compact.\n\n30.10 suppose xn converges weakly to x and the metric space s is complete and separable. prove that\n\nthe sequence {xn} is tight.\n\n30.11 let l be the collection of continuous functions on s such that\n\n(1) supx\u2208s | f (x)| \u2264 1.\n(2) | f (x) \u2212 f (y)| \u2264 d (x, y) for all x, y \u2208 s.\n\n(cid:20)(cid:20)(cid:20)(cid:3)\n\n(cid:3)\n\n(cid:20)(cid:20)(cid:20).\n\nf dq\n\nf dp \u2212\n\ndl(p, q) = sup\nf \u2208l\n\ndefine\n\nshow that dl is a metric on the collection of probability measures on the borel \u03c3 -field of s. prove\nthat a sequence of probability measures pn converges weakly to p if and only if dl(pn, p) \u2192 0.\n\n30.12 suppose s is a separable metric space. show that m is separable.\n\nfor more information, see billingsley (1968) and ethier and kurtz (1986).\n\nnotes\n\n "}, {"Page_number": 262, "text": "31\n\nskorokhod representation\n\nsuppose s is a complete separable metric space furnished with the borel \u03c3 -field. we are\ngoing to show that if xn are random variables taking values in s converging weakly to a\nrandom variable x , then we can find another probability space and other random variables\n(cid:3)\nequals the law\nx\nn\nof x , and x\n\n(cid:3)\nn equals the law of xn for each n, the law of x\nalmost surely.\n\nsuch that the law of x\n(cid:3)\n(cid:3)\nn converges to x\n\n(cid:3)\n\n(cid:3)\n\n, x\nlet \u0001(cid:3) = [0, 1], f(cid:3)\n\nthe borel \u03c3 -field on [0, 1], and p(cid:3)\n\nlebesgue measure. we first\n\n(cid:3)\n\nunder p(cid:3)\n\nis equal to p.\n\nto s such that the law of x\n\nprove\ntheorem 31.1 let p be a probability measure on s. then there exists a random variable x\nmapping \u0001(cid:3)\nproof for each k \u2265 1, let {aki} be a countable disjoint covering of s by borel sets of\ndiameter less than 1/k, such that p(\u2202aki) = 0, and {aki} is a refinement of {ak\u22121,i}. we\ncan construct these families inductively. to start, cover s with countably many balls of\nradius less than 1. since for each x0, p({x : |x \u2212 x0| = r}) can be nonzero for at most\ncountably many values of r, we can arrange matters so that the p-measure of the boundary\nof these balls is 0. we order the balls b1, b2, . . . , and then let a11 = b1, a12 = b2 \\ b1,\na13 = b3 \\ (b1 \u222a b2), and so on. to construct {a2i}, we first find a similar covering of s by\n} with sets\nsets {a\n(cid:3)\nin {a1 j}.\n2i\n\n} of diameter less than 1/2, and then take all intersections of sets in {a\n(cid:3)\n2i\n\nwe inductively define closed subintervals of [0, 1] by choosing i11 to have left endpoint\nat 0 and length equal to p(a11), then i12 to have left endpoint equal to the right endpoint of\ni11 and length equal to p(a12), and so forth. we then decompose i11 into subintervals {i21} in\nan analogous way so that the lengths of the subintervals match the probabilities of the a2i\u2019s\ncontained in a11. we then subdivide i12, and so on. we observe that {iki} is a refinement of\n{ik\u22121,i} for all k \u2265 2 and p(cid:3)(iki) = p(aki) for all k and i.\npick a point xki \u2208 aki for each k and i. we define x k by setting x k (\u03c9(cid:3)) equal to xki if\n\u03c9(cid:3) \u2208 iki. (the set of endpoints of the iki is countable, hence has lebesgue measure 0, and it\ndoesn\u2019t matter how we define x k at those points.) for each \u03c9(cid:3)\nexcept those that are endpoints\nof some iki, if n \u2265 m, then x n(\u03c9(cid:3)) and x m(\u03c9(cid:3)) are in the same ami for some i. since the\ndiameter of ami is less than 1/m, we see that d (x n(\u03c9(cid:3)), x m(\u03c9(cid:3))) \u2264 1/m. that is, x n(\u03c9(cid:3))\nis a cauchy sequence. the space s is complete, so we can define x (\u03c9(cid:3)) to be the limit of\nthe x n(\u03c9(cid:3)). the collection of endpoints of the imi is countable, so the limit exists for almost\nevery \u03c9(cid:3)\n\n.\n\n244\n\n "}, {"Page_number": 263, "text": "it remains to show that the law of x under p(cid:3)\n\n245\nis p. let f be a closed set, let fk =\n(cid:9)\n{x : d (x, f ) < 1/k}, and let jk = {i : aki \u2229 f (cid:16)= \u2205}. we have\n\nskorokhod representation\n\n(cid:9)\np(cid:3)(x k \u2208 f ) \u2264 p(cid:3)(x k \u2208 \u222ai\u2208jk aki) \u2264\n\n(cid:9)\n\ni\u2208jk\n\np(cid:3)(x k \u2208 aki)\n\n=\n\np(cid:3)(iki) =\n\np(aki) \u2264 p(fk ).\n\ni\u2208jk\n\ni\u2208jk\n\nwe used the fact that each aki has diameter less than 1/k. hence\n\nlim sup\n\np(cid:3)(x k \u2208 f ) \u2264 p(f ).\n\nk\n\ntherefore the laws of x k under p(cid:3)\n\u2264 1/k, so x k converges to x , a.s., with respect to p(cid:3)\ne (cid:3)\nx under p(cid:3)\n\nconverge weakly to p. but we know d (x k (\u03c9(cid:3)), x (\u03c9(cid:3)))\nis continuous and bounded,\nf (x ) by dominated convergence, so x k \u2192 x weakly. therefore the law of\n\nf (x k ) \u2192 e (cid:3)\n\nis equal to p.\n\n. if f\n\nto s with law p such that xn \u2192 x , a.s.\n\nwe did not need the fact that the aki were continuity sets, i.e., that the probability of the\nboundary of aki is zero, but this will be used in the next theorem, which is known as the\nskorokhod representation.\ntheorem 31.2 suppose pn are probability measures on s converging weakly to p. then\nto s with laws pn and a random variable x\nthere exist random variables xn mapping \u0001(cid:3)\nmapping \u0001(cid:3)\n\u0001(cid:3)\n\nequivalently, if x\nto s with laws equal to x\nn and x , respectively, such that xn \u2192 x , a.s.\n(cid:3)\n(cid:9)\n\nproof let the aki be as in the proof of the previous theorem, and for each pn define\nintervals i n\nn \u2019s.\nlet kkn = {i : p(aki) > pn(aki)} and kc\n\nn as was done above, and let xn be the limit of the x k\n\n= {i : p(aki) \u2264 pn(aki)}. since\n\nweakly, there exist random variables xn and x mapping\n\nki and random variables x k\n\n(cid:3)\nn converges to x\n\nkn\n\n(cid:3)\n\n[p(aki) \u2212 pn(aki)] = 1 \u2212 1 = 0,\n\ni\n\n(cid:9)\n\nkc\nkn\n\n[p(aki) \u2212 pn(aki)] = \u2212\n\n(cid:9)\n[p(aki) \u2212 pn(aki)].\n\nwe have\n\nhence (cid:9)\n\ni\n\n|p(cid:3)(iki) \u2212 p(cid:3)(i n\n\nki\n\n)| =\n\n=\n\n(cid:9)\n\nkc\nkn\n\n(31.1)\n\n[p(aki) \u2212 pn(aki)]\n\ni\n\nkkn\n\n(cid:9)\n(cid:9)\n|p(aki) \u2212 pn(aki)|\n[p(aki) \u2212 pn(aki)] \u2212\n(cid:9)\n(cid:9)\n[p(aki) \u2212 pn(aki)]\n[p(aki) \u2212 pn(aki)]\n\nkkn\n\nkkn\n\n+.\n\n= 2\n= 2\n\neach term in the sum on the last line goes to 0 as n \u2192 \u221e by theorem 30.2 because the\naki are p-continuity sets, that is, p(\u2202aki) = 0; also each term is dominated by p(aki), and\n\ni\n\n "}, {"Page_number": 264, "text": "(cid:12)\n\n246\n\ni\n\ngoes to 0.\n\np(aki) = 1. therefore by dominated convergence the sum on the last line of (31.1)\n\nskorokhod representation\n\n(cid:9)\n\ni\u2208j\n\nfix k and j and let \u03b1, \u03b1n be the left-hand endpoints of ik j, i n\n\nk j, respectively. then (31.1)\n\nallows us to use dominated convergence to conclude that\np(cid:3)(i n\n\n\u03b1 =\n\np(cid:3)(iki) = lim\nn\u2192\u221e\n\nki\n\n) = lim\n\nn\u2192\u221e \u03b1n,\n\nwhere j consists of those i such that iki is to the left of ik j; note that for i \u2208 j we have that i n\nk j, then i \u2208 j. similarly the right-hand\nis to the left of i n\nendpoint of i n\n\nk j converges to the right-hand endpoint of ik j.\n\nk j and conversely, if i n\n\nki is to the left of i n\n\nki\n\nis in the interior of ik j, then it will be in the interior of i n\n\nk j for all sufficiently large n.\n\nif \u03c9(cid:3)\n\n(cid:9)\n\ni\u2208j\n\nthis means that for n sufficiently large,\n\nd (x (\u03c9(cid:3)), xn(\u03c9(cid:3)) \u2264 2/k.\n\nthis implies our result.\n\nexercises\n\n31.1 suppose f\n\nis bounded, xn converges to x weakly, and also that p(x \u2208 d f ) = 0, where\n\nd f = {x : f is not continuous at x}. show that f (xn ) converges weakly to f (x ).\n\n31.2 suppose a sequence {xn} is uniformly integrable and xn converges to x weakly. show e xn \u2192\n\ne x .\n\n31.3 give an example of a sequence of random variables xn converging weakly to x and where each\n\nxn is integrable, but x is not integrable.\n\n31.4 suppose xn converges weakly to x and each xn is non-negative. prove that\n\ne x \u2264 lim inf\n\nn\u2192\u221e e xn.\n\n31.5 suppose xn converges weakly to x and each xn has the property that with probability one,\n\n(this might arise, for example, if each xn is of the form xn(t ) = (cid:15)\n\n|xn(t ) \u2212 xn(s)| \u2264 |t \u2212 s|,\n\ns, t \u2264 1.\n\nt\n0 yn(s) ds and each yn is\n\nbounded by 1.) prove that x has this same property, that is, with probability one,\n\n|x (t ) \u2212 x (s)| \u2264 |t \u2212 s|,\n\ns, t \u2264 1.\n\n31.6 here is a way to prove one direction of lebesgue\u2019s theorem on riemann integrable functions.\n(1) for each n \u2265 1 and each i \u2264 n, let xin be a point in [(i\u22121)/n, i/n). let pn be the probability\nmeasure that assigns mass 1/n to each point xin, i = 1, 2, . . . , n. show that pn converges weakly\n(cid:15)\nto p, where p is a lebesgue measure on [0, 1].\n\n(2) suppose f is a bounded function which is continuous at almost every point of [0, 1]. show\n\n(cid:15)\n\n(cid:15)\n\nthat\n\nf dp. note that\n\nf dpn is a riemann sum approximation to\n\nf dpn \u2192(cid:15)\n\n1\n0 f (x) dx.\n\n "}, {"Page_number": 265, "text": "32\n\nthe space c[0, 1]\n\nwe examine weak convergence for the space c[0, 1], the set of continuous real-valued\nfunctions on [0, 1]. we give a criterion for the laws of a sequence of continuous stochastic\nprocesses to be tight. we apply these results to show that a simple symmetric random walk\nconverges weakly to a brownian motion, which in particular gives another construction of\nbrownian motion.\n\nlet c[0, 1] be the collection of continuous real-valued functions from [0, 1] into r. we make\nc[0, 1] into a metric space by defining\n\n32.1 tightness\n\nd ( f , g) = sup\nt\u2208[0,1]\n\n| f (t ) \u2212 g(t )|,\n\nand it is well known that c[0, 1] is separable and complete. we recall the ascoli\u2013arzel`a\ntheorem: if a family f of functions on a compact set is equicontinuous and uniformly\nbounded at one point, then every subsequence in f has a further subsequence in f that\nconverges. rephrased another way, if the family f is equicontinuous and uniformly bounded\nat one point, then the closure of f is compact. we furnish c[0, 1] with the borel \u03c3 -field.\ngiven a continuous function f on [0, 1], we define \u03c9 f , the modulus of continuity of f , by\n\n\u03c9 f (\u03b4) =\n\nsup\n\ns,t\u2208[0,1],|t\u2212s|<\u03b4\n\n| f (t ) \u2212 f (s)|.\n\nwe have the following criterion for a sequence of continuous processes to be tight.\n\ntheorem 32.1 suppose the xn are continuous real-valued processes. suppose for each \u03b5\nand \u03b7 > 0 there exist n0, a, and \u03b4 (depending on \u03b5 and \u03b7) such that if n \u2265 n0, then\n\n(\u03b4) \u2265 \u03b5) \u2264 \u03b7\n\np(\u03c9xn\n\nand\n\np(|xn(0)| \u2265 a) \u2264 \u03b7.\n\n(32.1)\n\n(32.2)\n\nthen the xn are tight.\n\n(\u03b4) \u2265 \u03b5) \u2192 0 as \u03b4 \u2192 0\nproof since each xi is a continuous process, then for each i, p(\u03c9xi\nby dominated convergence. hence, given \u03b5 and \u03b7 we can, by taking \u03b4 smaller if necessary,\nassume that (32.1) holds for all n.\n\n247\n\n "}, {"Page_number": 266, "text": "248\n\nchoose \u03b5m = \u03b7m = 2\n\nthe space c[0, 1]\n\n\u2212m and consider the \u03b4m and am so that\n\u2212m\n\n(\u03b4m) \u2265 2\n\n\u2212m) \u2264 2\n\np(\u03c9xn\n\nsup\nn\n\nand\n\nlet\n\nkm0\n\np(|xn(0)| \u2265 am) \u2264 2\n\n\u2212m.\n\nsup\nn\n\n= { f \u2208 c[0, 1] :\n\nsup\n\n| f (t ) \u2212 f (s)| \u2264 2\ns,t\u2208[0,1],|t\u2212s|\u2264\u03b4m\n| f (0)| \u2264 am0\n}.\n\n\u2212m for all m \u2265 m0,\n\neach km0 is an equicontinuous family, and by the ascoli\u2013arzel\u00b4a theorem, each km0 is a\ncompact subset of c[0, 1]. we have\n\np(xn /\u2208 km0\n\n) \u2264 p(|xn(0)| \u2265 am0\n\n) +\n\n(\u03b4m) \u2265 \u03b5m)\n\np(\u03c9xn\n\n\u221e(cid:9)\n\n\u221e(cid:9)\n\nm=m0\n\nm=m0\n\u2212m = 3 \u00b7 2\n2\n\n\u2212m0 .\n\n\u2264 2\n\n\u2212m0 +\n\nthis proves tightness.\n\nwe have given one criterion for a process to have continuous paths, namely, theorem 8.1.\n\nin the case of markov processes, we have given another: theorem 21.5.\n\n32.2 a construction of brownian motion\n\nwe will now use the results of section 32.1 to give a construction of brownian motion, quite\ndifferent from that of chapter 6.\nlet yi be i.i.d. random variables with p(yi = 1) = p(yi = \u22121) = 1\nis a simple symmetric random walk. let zn(t ) = snt /\nt by linear interpolation for other t. that is, if k/n \u2264 t \u2264 (k + 1)/n, then\nzn\n\nn\ni=1 yi\nn for t a multiple of 1/n and define\n\n\u221a\n\n2 . then sn =(cid:12)\n\n= (k + 1) \u2212 nt\n\n\u221a\nn\n\nsk + nt \u2212 k\u221a\n\nn\n\nzn\nt\n\nsk+1.\n\n(32.3)\n\nthe zn are continuous processes. let pn be the law of zn, which will be a probability measure\non c[0, 1].\ntheorem 32.2 the sequence pn converges weakly to a probability measure p\u221e on c[0, 1],\nand p\u221e is the law of a brownian motion.\n\nproof the main step is to prove that the pn are tight. we then show that any subsequential\nlimit point is a wiener measure, that is, the law of a brownian motion. we can then appeal\nto theorem 31.1 to obtain the process x , which will be a brownian motion.\n\na computation shows that\n\ne s4\nn\n\n= n(cid:9)\n\ni=1\n\n(cid:9)\n\ni(cid:16)= j\n\n+\n\ne y 4\ni\n\n(e y 2\ni\n\n)(e y 2\nj\n\n) \u2264 cn2,\n\n(32.4)\n\n "}, {"Page_number": 267, "text": "since e yi and e y 3\nterms.\n\nif s and t are multiples of 1/n, then\n\n32.2 a construction of brownian motion\n\n249\n\ni are both 0, the yi\u2019s are independent, and the second sum has n(n\u22121) \u2264 n2\n\n(cid:10) nt(cid:9)\n\n(cid:11)4 = 1\ne|zt \u2212 zs|4 = 1\nn2\nn2 n2|t \u2212 s|2 \u2264 c|t \u2212 s|2.\n\u2264 c\n\ni=ns+1\n\nn2\n\nyi\n\ne\n\ne\n\n(cid:11)4\n\n(cid:10) nt\u2212ns(cid:9)\n\nyi\n\ni=1\n\n(32.5)\n\nif we tried to get by with only the second moment, we would only end up with c|t \u2212 s|, which\nis not good enough for theorem 8.1.\nat this point we would like to apply theorem 32.1, but we have the technical nuisance\nthat s and t might not be multiples of 1/n. if |t \u2212 s| \u2264 2/n, then by the construction of zn\nhave |zn(t ) \u2212 zn(s)| \u2264 c|t \u2212 s|\u221a\nusing linear interpolation and the fact that the yi\u2019s are bounded by one in absolute value, we\n\nn and then\n\ne|zn(t ) \u2212 zn(s)|4 \u2264 c|t \u2212 s|4n2 \u2264 c|t \u2212 s|2.\n\n(32.6)\n\nbe the largest multiple of 1/n less than or equal to s and t\n\n(cid:3)\n\nthe\n\nsuppose |t \u2212 s| > 2/n. let s\n(cid:3)\nlargest multiple of 1/n larger than or equal to t. using (32.5) and (32.6),\ne|zn(t ) \u2212 zn(s)|4 \u2264 ce|zn(t ) \u2212 zn(t\n(cid:3)|2 + c|t\n\n(cid:3))|4 + ce|zn(t\n(cid:3) \u2212 s\n\n(cid:3)) \u2212 zn(s\n\n(cid:3)|2 + c|s\n\n(cid:3) \u2212 s|2\n\n(cid:3))|4 + e|zn(s\n\n(cid:3)) \u2212 zn(s)|4\n\nsince |t \u2212 t\napply theorems 8.1 and 32.1 to obtain the tightness.\n\n(cid:3) \u2212 s\n\n(cid:3)|, |t\n\n(cid:3)| are all less than c|t \u2212 s|. note zn(0) = 0 for all n. we now\n\n\u221a\n\nn by at most 1/\n\nany subsequential limit point is a probability measure on c[0, 1], so to show that the\nlimit is a brownian motion, it is enough by theorem 2.6 to show that the finite-dimensional\n\u221a\ndistributions under the limit law p\u221e agree with those of brownian motion. fix t. then zn(t )\n\u221a\ndiffers from s[nt]/\nn, where [nt] is the largest integer less than or equal to\nnt. by the central limit theorem (theorem a.51), s[nt]/\n[nt] converges weakly (with respect\n\u221a\nto the topology of r) to a mean zero normal random variable with variance one. by exercise\n30.3, s[nt]/\nn converges weakly to a mean zero normal random variable with variance t,\nand by exercise 30.2, zn(t ) converges weakly to a mean zero normal random variable with\nvariance t. this shows that the one-dimensional distributions of zn converge weakly to the\none-dimensional distributions of a brownian motion. we leave the analogous argument for\nthe higher-dimensional distributions to the reader.\n\none can also use doob\u2019s inequalities to obtain the necessary tightness estimate. if s and t\n\nare multiples of 1/n, we have\n\n|sk \u2212 sns| > \u03bb\n\np( max\nns\u2264k\u2264nt\n\n\u221a\nn) \u2264 c\n\u2264 c\n\ne|snt \u2212 sns|4\n\u03bb4n2\n|t \u2212 s|2\n\n.\n\n\u03bb4\n\n(32.7)\n\n\u2264 c|t \u2212 t\n\u2264 c|t \u2212 s|2,\n(cid:3)|, and |s \u2212 s\n\n "}, {"Page_number": 268, "text": "250\n\nthe space c[0, 1]\n\n32.1 the support of a measure \u03bb is the smallest closed set f such that \u03bb(f c ) = 0. let p be a wiener\nmeasure on c[0, 1], i.e., the law of a brownian motion on [0, 1]. use exercise 13.4 to prove that\nthe support of p is all of c[0, 1].\n\nexercises\n\n32.2 let (s, d ) be a complete separable metric space and let r be a subset of s. then (r, d ) is also\na metric space. if xn converges weakly to x with respect to the topology of (s, d ) and each xn\nand x take values in r, does xn converge weakly to x with respect to the topology of (r, d )?\ndoes the answer change if r is a closed subset of s?\nif xn and x take values in r and xn converges weakly to x with respect to the topology of\n(r, d ), does xn converge weakly to x with respect to the topology of (s, d )? what if r is a\nclosed subset of s?\n\n32.3 give a proof of theorem 32.2 using (32.7) in place of theorem 8.1.\n\n32.4 suppose (x , w, p) is a weak solution to\n\ndxt = \u03c3 (xt ) dwt + b(xt ) dt,\n\nx0 = x,\n\n(32.8)\n\nwhere w is a one-dimensional brownian motion and \u03c3 and b are bounded and continuous, but\nwe do not assume that \u03c3 is bounded below by a positive constant. suppose the solution to (32.8)\nis unique in law.\n\nsuppose \u03c3n and bn are lipschitz functions which are uniformly bounded and which converge\n\nuniformly to \u03c3 and b, respectively. let xt (n) be the unique pathwise solution to\n\ndyt = \u03c3n(yt ) dwt + bn(yt ) dt,\n\ny0 = x;\n\nthe probability measure here is p. prove that x (n) converges weakly to x with respect to c[0, 1].\n32.5 let w be a d-dimensional brownian motion and let {xt , t \u2208 [0, 1]} be the solution to (24.22). if\n\nx \u2208 rd, prove that the support of px is all of c[0, 1].\n\n "}, {"Page_number": 269, "text": "33\n\ngaussian processes\n\na gaussian process is a stochastic process where each of the finite-dimensional distributions\nis jointly normal. we will primarily, but not exclusively, be concerned with gaussian processes\nthat have continuous paths. for much of what we consider, it is not essential that the index\nset of times be [0,\u221e), and can in fact be almost any set. we will thus consider {xt : t \u2208 t}\nfor some index set t , and where for every finite subset s of t , the collection {xs : s \u2208 s} is\njointly normal.\n\n33.1 reproducing kernel hilbert spaces\n\nwe define the covariance function \u0001 by\n\n\u0001(s, t ) = e [(xs \u2212 e xs)(xt \u2212 e xt )],\n\ns, t \u2208 t.\n\n(33.1)\n\nfor our purposes, having a non-zero mean just complicates formulas without adding anything\ninteresting, so in this chapter we will assume e xt = 0 for all t \u2208 t , and (33.1) becomes\n\n\u0001(s, t ) = e [xsxt],\n\ns, t \u2208 t.\n\n(33.2)\n\nwe first show how \u0001 can be used to construct a hilbert space called the reproducing kernel\nhilbert space (rkhs).\nwhen we write \u0001(s,\u00b7), we mean that we fix an element s \u2208 t and then consider the\nfunction g : t \u2192 r defined by g(t ) = \u0001(s, t ) for t \u2208 t . let k be the collection of finite\nlinear combinations of the functions \u0001(s,\u00b7), s \u2208 t . thus each element of k has the form\n\nm(cid:9)\n\na j\u0001(s j,\u00b7),\n\n(cid:12)\nwhere m \u2265 1, the a j\u2019s are real, and each s j, j = 1, . . . , m, is an element of t . if f =\n\nj=1 a j\u0001(s j,\u00b7) and g =(cid:12)\n\nj=1\n\nm\n\nn\n\nk=1 bk\u0001(tk,\u00b7), define\nn(cid:9)\n(cid:22) f , g(cid:23)\n\n= m(cid:9)\n\nrkh s\n\nj=1\n\nk=1\n\na jbk\u0001(s j, tk ).\n\nwe define h to be the closure of k with respect to the norm induced by the inner product\n(cid:22)\u00b7,\u00b7(cid:23)\nthe reproducing property holds, and that h is a hilbert space.\n\nwe need to show that this bilinear form is indeed an inner product, that what is known as\n\nrkh s.\n\n251\n\n "}, {"Page_number": 270, "text": "252\n\nwe start with the reproducing property. if f = (cid:12)\n\ngaussian processes\n\nproperty applied to f is the formula\n\nm\n\nj=1 a j\u0001(s j,\u00b7), then the reproducing\n\n(33.3)\n\na j\u0001(s j, t ) = f (t ).\n\n(cid:22) f , \u0001(t,\u00b7)(cid:23)\n\n= f (t ).\n\nrkh s\n\nrkh s\n\nj=1\n\nthis follows from\n\n(cid:22) f , \u0001(t,\u00b7)(cid:23)\n\nto show that (cid:22)\u00b7,\u00b7(cid:23)\n\n= m(cid:9)\nby taking limits, (33.3) holds for all f \u2208 h.\n(cid:9)\nf =\na jak\u0001(s j, sk ) = m(cid:9)\n= m(cid:9)\nm(cid:9)\n(cid:11)2 \u2265 0.\n(cid:10) m(cid:9)\n\na j\u0001(s j,\u00b7) \u2208 k,\n\nj=1\n= e\n\n(cid:22) f , f (cid:23)\n\nthen\n\nk=1\n\nrkh s\n\nrkh s is an inner product, notice that when\n\nj,k=1\n\na jxs j\n\nj=1\n\na jake [xs j xsk ]\n\nthe cauchy\u2013schwarz inequality holds for (cid:22)\u00b7,\u00b7(cid:23)\nschwarz inequality applies), and so if (cid:22) f , f (cid:23)\n\nrkh s\n\nrkh s (the standard proof of the cauchy\u2013\n= 0, then\n\n| f (t )|2 = (cid:22) f , \u0001(t,\u00b7)(cid:23)2\n\nrkh s\n\n\u2264 (cid:22) f , f (cid:23)\n\nrkh s\n\n(cid:22)\u0001(t,\u00b7), \u0001(t,\u00b7)(cid:23)\n\nrkh s\n\n= 0,\n\nand thus f is zero.\n\nif fn is a cauchy sequence with respect to the norm\n(cid:21)g(cid:21)rkh s = (cid:22)g, g(cid:23)1/2\n\nrkh s\n\n,\n\nthen\n\n| fn(t ) \u2212 fm(t )|2 = (cid:22) fn \u2212 fm, \u0001(t,\u00b7)(cid:23)2\n\u2264 (cid:22) fn \u2212 fm, fn \u2212 fm(cid:23)\n\nrkh s\n\nrkh s\n\n(cid:22)\u0001(t,\u00b7), \u0001(t,\u00b7)(cid:23)\n\n,\n\nrkh s\n\nwhich tends to 0 as n, m \u2192 \u221e. thus fn converges pointwise. this is enough to prove h is\ncomplete; this is exercise 33.1.\n\nwe summarize.\n\nproposition 33.1 h with the inner product (cid:22)\u00b7,\u00b7(cid:23)\nand t \u2208 t , then\n\nrkh s is a hilbert space. moreover, if f \u2208 h\n= f (t ).\n\n(cid:22) f , \u0001(t,\u00b7)(cid:23)\n\nrkh s\n\nwe consider another hilbert space m, the closure of the linear span of {xt : t \u2208 t} with\nrespect to l2(p). we define\n\n(cid:22)y, z(cid:23)m = e [y z]\n\n "}, {"Page_number": 271, "text": "! m(cid:9)\n\nj=1\n\nn(cid:9)\n\nk=1\n\n\"\n\nn(cid:9)\n\n= m(cid:9)\n\nj=1\n\nk=1\n\n253\nif y and z are both finite linear combinations of the xt\u2019s. thus if m, n \u2265 1, a j, bk \u2208 r, we set\n\n33.1 reproducing kernel hilbert spaces\n\na jxs j\n\n,\n\nbkxtk\n\nm\n\na jbke [xs j xtk ],\n\n(33.4)\n\nand we let m be the closure of the collection of random variables of the form\nm\nj=1 a jxs j\n(cid:12)\nwith respect to (cid:22)\u00b7,\u00b7(cid:23)m. since \u0001(s j, tk ) = e [xs j xtk ], from (33.4) we see that h and m\nj=1 a j\u0001(s j,\u00b7) and\nare isomorphic, where we have a one-to-one correspondence between\nm\nj=1 a jxs j\nlet {en} be a complete orthonormal system for h. let yn be the element of m corre-\nsponding to en. then\n\nm\n\n.\n\n(cid:12)\n\n(cid:12)\n\ne [ynym] = (cid:22)yn, ym(cid:23)m = (cid:22)en, em(cid:23)\n\n= \u03b4nm,\n\nrkh s\n\nwhere \u03b4nm is 0 if n (cid:16)= m and 1 if n = m. this implies that the yn are independent normal\nrandom variables with mean zero and variance one; see proposition a.55. (recall that we\nare assuming that all the xt\u2019s have mean zero.)\nsince \u0001(s,\u00b7) is an element of h, we can write\n\n\u221e(cid:9)\n\n\u221e(cid:9)\n\nn=1\n\nen(s)en(\u00b7).\n\n\u0001(s,\u00b7) =\n\n(cid:22)\u0001(s,\u00b7), en(cid:23)\nrkh s en(\u00b7) =\n\u221e(cid:9)\nusing the correspondence between h and m, we have\n\nn=1\n\nxs =\n\nen(s)yn,\n\nn=1\n\nwhere the yn are i.i.d. standard normal variables. this is known as the karhunen\u2013lo`eve\nexpansion of a gaussian process.\n\nexample 33.2 let\u2019s see what this expansion is in the case of brownian motion. if we define\n\n(cid:22) f , g(cid:23)\n\n=\n\n1\n\n(cid:3)(r)g\n\n(cid:3)(r) dr\n\n(33.5)\nfor f and g whose first derivatives are in l2([0, 1]) and such that f (0) = g(0) = 0, then\nbecause \u0001(s, t ) = s \u2227 t,\n\ncm\n\nf\n\n0\n\n(cid:22)\u0001(s,\u00b7), \u0001(t,\u00b7)(cid:23)\n\ncm\n\n1[0,s)(r)1[0,t )(r) dr = s \u2227 t\n\n(cid:3)\n\n(cid:3)\n=\n= \u0001(s, t ),\n\n1\n\n0\n\nand we see that we have identified the reproducing kernel hilbert space for brownian motion\non [0, 1]. the notation (cid:22)\u00b7,\u00b7(cid:23)\ncm is used because the hilbert space with this inner product is\ncalled the cameron\u2013martin space, a space that has many connections with brownian motion.\nif en(s) = \u221a\n2 sin(n\u03c0s)/n\u03c0, then the sequence {en} is a complete orthonormal sequence\nfor the cameron\u2013martin space. the karhunen\u2013lo`eve expansion is equivalent to the formula\n(6.2) that we used in our first construction of brownian motion.\n\n "}, {"Page_number": 272, "text": "254\n\ngaussian processes\n\n33.2 continuous gaussian processes\n\nwe now turn to the construction of gaussian processes with continuous paths. suppose we\nhave an index set t and a non-negative definite kernel \u0001(\u00b7,\u00b7). saying \u0001 is non-negative\ndefinite means that for each n and each t1, . . . , tn \u2208 t , the matrix whose (i, j) entry is\n\u0001(ti, t j ) is a non-negative definite matrix. we define a metric on t by defining\n\nd (s, t ) = (var (xt \u2212 xs))1/2.\n\nactually, d is a pseudo-metric because d (s, t ) = 0 does not necessarily imply t = s. an\n\u03b5-ball is a set of the form {t \u2208 t : d (t, t0) < \u03b5} for some t0. let n (\u03b5) be the minimum\nnumber of \u03b5-balls needed to cover t .\ntheorem 33.3 let \u0001 : t \u00d7 t \u2192 r be continuous with respect to the pseudo-metric d,\nsymmetric, and non-negative definite. if for some \u03b2 < 1 and some constant c we have\n\nlog n (\u03b5) \u2264 c\u03b5\u2212\u03b2 ,\n\n\u03b5 \u2208 (0, 1),\n\n(33.6)\n\nthen there exists a continuous gaussian process {xt : t \u2208 t} with covariance kernel \u0001.\n\none can in fact be more precise than (33.6) and give an integral condition that n (x) must\n\nsatisfy for x small.\n\nbefore proving theorem 33.3, let us look at a number of examples.\n\nexample 33.4 in the case of brownian motion, var (xt \u2212 xs) = |t \u2212 s|, so that d (s, t ) =\n|s \u2212 t|1/2. if t is the interval [0, 1], then the set of intervals of length \u03b52 and centers\nk\u03b52/4, k = 0, 1, . . . , 4/\u03b52, is a collection of \u03b5-balls covering [0, 1]. therefore n (\u03b5) \u2264 c/\u03b52,\nimplying log n (\u03b5) \u2264 c log(1/\u03b5), which satisfies (33.6). this and theorem 2.4 gives a\nconstruction of brownian motion.\nexample 33.5 we look at fractional brownian motion. let h \u2208 (0, 2). h is known as the\nhurst index, where h = 1 corresponds to brownian motion. define\n\n\u0001(s, t ) = |s|h + |t|h \u2212 |s \u2212 t|h .\n\nthis leads to d (s, t ) = c|t \u2212 s|h/2. open intervals of length \u03b52/h are \u03b5-balls, and it takes\nc\u03b5\u22122/h of them to cover [0, 1]. therefore again n (\u03b5) \u2264 c log(1/\u03b5), and (33.6) applies. one\nuse of fractional brownian motion is to model stock prices where there is more or less\nmemory of the past than a brownian motion has.\n\nexample 33.6 here is our first example of a gaussian process where t is not a subset of\n[0,\u221e). we construct a brownian sheet, x (t1, t2), where the points (t1, t2) \u2208 [0, 1]2. more\ngenerally we can consider x (t ), where t \u2208 [0, 1]d. this is no harder, but for simplicity of\nnotation we consider only the case d = 2. if s = (s1, s2) and t = (t1, t2), define\n\n\u0001(s, t ) = (s1 \u2227 t1)(s2 \u2227 t2).\n\none motivation for this formula is to identify the point (t1, t2) with the rectangle rt whose\nlower left corner is at the origin and whose upper right corner is at (t1, t2). then the covariance\nof xs and xt is the area of rs \u2229 rt.\n\n "}, {"Page_number": 273, "text": "33.2 continuous gaussian processes\n\n255\n\nsome simple geometry shows that if we put \u03b5-balls centered at the points (c1 j\u03b52, c1k\u03b52)\nfor an appropriate c1 and with j, k \u2264 c2\u03b5\u22122, we cover t . therefore n (\u03b5) \u2264 c\u03b5\u22124, and so\nlog n (\u03b5) \u2264 c log(1/\u03b5).\nexample 33.7 we can generalize the last example. for every borel subset a of [0, 1]d, let\nxa be a gaussian random variable. we want the covariance of xa and xb to be the lebesgue\nmeasure of a \u2229 b. this is known as a set-indexed process. if we let t be the collection of\nall borel subsets of [0, 1]d, one cannot get a continuous gaussian process. in order to get a\ncontinuous process x one must restrict t to be a subcollection of sets whose boundaries are\nsufficiently smooth; see dudley (1973).\n\nexample 33.8 our last example has a more complicated index set. let w be a one-\ndimensional brownian motion. if f \u2208 l2[0, 1], define\n\n(cid:3)\n\n0\n\n1\n\nf (s) dws.\n\nx f =\n(cid:3)\n\n(cid:15)\nd ( f , g)2 =\n\n1\n0 f (s)g(s) ds. it follows that\n( f (s) \u2212 g(s))2 ds.\n\n1\n\n(cid:15)\n\n1\n0 f (s)2 ds\n\nby exercise 24.6, x f is a gaussian random variable with mean 0 and variance\nand the covariance of x f and xg is\n\n0\n\nthe process x f is known as a gaussian field.\nfor what subsets t of l2([0, 1]) can one define a process x f that has continuous paths with\nrespect to d? this means that the map f \u2192 x f (\u03c9) is continuous for almost all \u03c9, where we\nuse the pseudo-metric d to define open sets in t . it turns out t = { f \u2208 l2([0, 1]) : (cid:21) f (cid:21)2 \u2264 1}\nis too large to obtain a continuous gaussian process, but, for example, t = { f \u2208 c2([0, 1]) :\n(cid:21) f (cid:21)\u221e \u2264 1,(cid:21) f\n\n(cid:3)(cid:3)(cid:21)\u221e \u2264 1} is small enough to apply theorem 33.3.\n\n(cid:3)(cid:21)\u221e \u2264 1,(cid:21) f\n\nwe now proceed to the proof of theorem 33.3.\n\nproof of theorem 33.3 since t can be covered by finitely many \u03b5-balls for each \u03b5, it follows\nthat if a(\u03b5) is the collection of centers for the cover by \u03b5-balls, then a = \u222a\u221e\n\u2212n) is\nn=1\na countable dense subset of t . we first label the elements of a by t1, t2, . . . for each n,\nwe construct the law of (xt1\n). we then use the kolmogorov extension theorem to\nconstruct the law of {xt : t \u2208 a}. next we prove that t \u2192 xt is uniformly continuous on a,\nalmost surely. finally we define xt for all t \u2208 t by continuity.\n). let n be fixed, and let b be an n\u00d7n matrix whose\nstep 1. we construct the law of (xt1\n(i, j) entry is \u0001(ti, t j ). the matrix b is symmetric, and non-negative definite by hypothesis.\nlet y1, . . . , yn be independent normal random variables with mean zero and variance one. if\nwe let c be the non-negative definite square root of b and\n\n, . . . , xtn\n\n, . . . , xtn\n\na(2\n\n(viewed as vectors), or equivalently,\n\nxti\n\nx = cy\n\n= n(cid:9)\n\nj=1\n\nci jyj,\n\n "}, {"Page_number": 274, "text": "256\n\ngaussian processes\n\n, . . . , xtn\n\n, . . . , xtn\n\na simple calculation shows that e [xtk xtm] = bkm = \u0001(tk, tm). the xt j \u2019s are jointly normal\nand this gives the first step of the construction.\nstep 2. we apply the kolmogorov extension theorem. let pn be the law of (xt1\n). it\nis easy to see the consistency property holds for the pn, so by the kolmogorov extension\ntheorem, there exists a probability p on rn such that if we define xt (\u03c9) by \u03c9(t ) for t \u2208 a,\nthe law of (xt1\nstep 3. we show that except for a null set of probability zero, the map t \u2192 xt (\u03c9) is uniformly\ncontinuous on a.\nt \u2208 a, let t j be the element of a(2\nwe will fix j in a moment, and write\n+ (xtj+1\n\nto prove the uniform continuity, we proceed similarly to theorem 8.1. for each point\n\u2212 j ) closest to t, with some convention for breaking ties.\n\n) is pn for each n.\n\n) + (xtj+2\n\nwhere the sum is finite because t \u2208 a. let \u03bb > 0. if |xt \u2212 xs| > \u03bb for some s, t \u2208 a with\n\u2212\nd (s, t ) < 2\n\n, then \u03c9 is in one or more of the following events:\n\nxt = xtj\n\n) + \u00b7\u00b7\u00b7 ,\n\n\u2212 xtj+1\n\n\u2212 xtj\n\n| > \u03bb/2 for some sj , tj \u2208 a(2\n\n\u2212j ) with d (sj , tj ) \u2264 3 \u00b7 2\n\n\u2212j};\n\n| >\n\n\u2212 xt j\n\n|xt j+1\nwith d (t j, t j+1) < 3 \u00b7 2\n\n\u03bb\n\n8 j2 for some t j \u2208 a(2\n\n\u2212 j+1\n\n\u2212 j ), t j+1 \u2208 a(2\n\n\u2212( j+1))\n\n(a) the event\n\nej = {|xtj\n\n(b) the event\nfj =\n\n\u2212 xsj\n(cid:18)\n\nfor some j \u2265 j;\n(c) the event\ng j =\n\n(cid:18)\n\n| >\n\n\u2212 xs j\n\n|xs j+1\nwith d (s j, s j+1) < 3 \u00b7 2\n\n\u03bb\n\n8 j2 for some s j \u2208 a(2\n\n\u2212 j+1\n\n\u2212 j ), s j+1 \u2208 a(2\n\n\u2212( j+1))\n\nfor some j \u2265 j.\n(cid:10)\nfirst we bound the probability of ej . there are n (2\nat most exp(2c(2j )\u03b2 ) pairs (sj , tj ). if d (tj , sj ) < 3 \u00b7 2\n| > \u03bb/2) \u2264 2 exp\n\np(|xsj\n\n\u2212 xtj\n\n\u2212j ) elements of a(2\n\u2212j , then\n\u2212 (\u03bb/2)2\n2 \u00b7 3 \u00b7 2\u2212j\n\n(cid:11)\n\n.\n\n\u2212j ), so there are\n\ntherefore the probability of ej is bounded by\n\np(e j ) \u2264 ec2\u03b2j e\n\n\u2212c\u03bb22j .\n\nsince \u03b2 < 1, this can be made as small as we like by taking j large enough.\n\nfor any t j and t j+1 with d (t j, t j+1) < 3 \u00b7 2\n\n\u2212 j+1,\n\np(|xt j\n\n\u2212 xt j+1\n\n| > \u03bb/(8 j2)) \u2264 2 exp\n\n(cid:10) \u03bb2/64 j4\n\n6 \u00b7 2\u2212 j+1\n\n(cid:11)\n\n.\n\n(cid:19)\n\n(cid:19)\n\n "}, {"Page_number": 275, "text": "there are less than ec2\u03b2 j points in a(2\npairs. thus the probability of fj is bounded by\n\n257\n\u2212( j+1)), so less than ec2\u03b2 j\n\nexercises\n\n\u2212 j ) and ec2\u03b2 ( j+1) points in a(2\n(cid:12)\u221e\np(fj ) \u2264 cec2\u03b2 j e\n\n\u2212c\u03bb22 j / j4 .\n\nsince \u03b2 < 1, this is summable in j, and\ntake j large enough. we handle the bound for g j similarly.\n\nj=j\n\np(fj ) can be made as small as we like if we\n\nthus, given \u03b5, we have\n\np(\n\nsup\n\ns,t\u2208a,d (s,t )<2\u2212j\n\n|xt \u2212 xs| > \u03bb) \u2264 \u03b5\n\nif we take j large enough, where j depends on \u03b5 and \u03bb. this suffices to prove the uniform\ncontinuity.\nstep 4. we use continuity to complete the proof. define xt = lims\u2208a,s\u2192t xs. the limit exists\nand will be a continuous function of t by virtue of the uniform continuity. by remark a.56,\nxt will have the desired covariance function.\n\nwe have been considering gaussian processes taking values in r, but it is also of interest\nto look at brownian motion taking values in a hilbert space or a banach space. there are\nthree steps to constructing such a process:\n\n(1) constructing gaussian measures on banach (or hilbert) spaces;\n(2) getting a suitable estimate on (cid:21)xt \u2212 xs(cid:21);\n(3) constructing a brownian motion.\nof these three steps, the third follows along the lines we used for real-valued processes.\nsteps (1) and (2) require considerable work, and we refer the reader to bogachev (1998) or\nkuo (1975). a measure \u03bc on a banach space is called gaussian if \u03bc \u25e6 l\n\u22121 is a gaussian\nmeasure on r for every linear functional l on the banach space.\n\n33.1 finish the proof that h as defined in section 33.1 is complete.\n33.2 show that if in example 33.8 we let\n\nexercises\n\nt = { f \u2208 c1([0, 1]);(cid:21) f (cid:21)\u221e \u2264 1,(cid:21) f\n\n(cid:3)(cid:21)\u221e \u2264 1},\n\nthen n (\u03b5) is bounded above by c1\u03b5\u22121 and bounded below by c2\u03b5\u22121.\n\n33.3 suppose x i and y i are two sequences of brownian motions with all of the brownian motions\n\nindependent of each other. let\n\n(s,t ) = 1\u221a\nzn\n\nn\n\nn(cid:9)\n\ni=1\n\nx i\ns y i\nt\n\n.\n\nprove that zn converges weakly with respect to the topology of c([0, 1]2 ) as n \u2192 \u221e to a\nbrownian sheet.\n\n "}, {"Page_number": 276, "text": "258\n\ngaussian processes\n\n33.4 let x be a brownian bridge. (this will be studied further in section 35.2.) this means that x\n\nis a mean zero gaussian process with\n\ncov (xs, xt ) = s \u2227 t \u2212 st,\nidentify the reproducing kernel hilbert space for x .\n\n0 \u2264 s, t \u2264 1.\n\n33.5 let x be the ornstein\u2013uhlenbeck process started at 0. this was defined in exercise 19.5.\n\nidentify the reproducing kernel hilbert space for x .\n\n "}, {"Page_number": 277, "text": "34\n\nthe space d[0, 1]\n\nwe define the space d[0, 1] to be the collection of real-valued functions on [0, 1] which\nare right continuous with left limits. we will introduce a topology on d = d[0, 1], the\nskorokhod topology, which makes d into a complete separable metric space. we will give\na criterion for a subset of d to be compact, which will lead to some criteria for a family of\nprobability measures on d to be tight.\n\n34.1 metrics for d[0, 1]\n\nwe write f (t\u2212) for lims<t,s\u2192t f (s). we will need the following observation. if f is in d and\n\u03b5 > 0, let t0 = 0, and for i > 0 let ti+1 = inf{t > ti : | f (t ) \u2212 f (ti)| > \u03b5} \u2227 1. because f is\nright continuous with left limits, then from some i on, ti must be equal to 1.\n\nour first try at a metric, \u03c1, makes d into a separable metric space, but one that is not\ncomplete. let\u2019s start with \u03c1 anyway, since we need it on the way to the metric d we end up\nwith.\nand such that \u03bb(0) = 0, \u03bb(1) = 1. define\n\nlet \u0001 be the set of functions \u03bb from [0, 1] to [0, 1] that are continuous, strictly increasing,\n\n\u03c1( f , g) = inf{\u03b5 > 0 : \u2203\u03bb \u2208 \u0001 such that sup\nt\u2208[0,1]\n\n|\u03bb(t ) \u2212 t| < \u03b5,\n\n| f (t ) \u2212 g(\u03bb(t ))| < \u03b5}.\n\nsup\nt\u2208[0,1]\n\nsince the function \u03bb(t ) = t is in \u0001, then \u03c1( f , g) is finite if f , g \u2208 d. clearly \u03c1( f , g) \u2265 0.\nif \u03c1( f , g) = 0, then either f (t ) = g(t ) or else f (t ) = g(t\u2212) for each t; since elements of d\nare right continuous with left limits, it follows that f = g. if \u03bb \u2208 \u0001, then so is \u03bb\u22121 and we\nhave, setting s = \u03bb\u22121(t ) and noting both s and t range over [0, 1],\n|s \u2212 \u03bb(s)|\n\n|\u03bb\u22121(t ) \u2212 t| = sup\ns\u2208[0,1]\n\nsup\nt\u2208[0,1]\n\nand\n\nsup\nt\u2208[0,1]\n\n| f (\u03bb\u22121(t )) \u2212 g(t )| = sup\ns\u2208[0,1]\n\n| f (s) \u2212 g(\u03bb(s))|,\nand we conclude \u03c1( f , g) = \u03c1(g, f ). the triangle inequality follows from\n|\u03bb2(s) \u2212 s|\n\n|\u03bb2 \u25e6 \u03bb1(t ) \u2212 t| \u2264 sup\nt\u2208[0,1]\n\n|\u03bb1(t ) \u2212 t| + sup\ns\u2208[0,1]\n\nsup\nt\u2208[0,1]\n\n259\n\n "}, {"Page_number": 278, "text": "260\n\nand\n\nthe space d[0, 1]\n\nsup\nt\u2208[0,1]\n\n| f (t ) \u2212 h(\u03bb2 \u25e6 \u03bb1(t ))| \u2264 sup\nt\u2208[0,1]\n+ sup\ns\u2208[0,1]\n\n| f (t ) \u2212 g(\u03bb1(t ))|\n|g(s) \u2212 h(\u03bb2(s))|.\n\nlook at the set of f in d for which there exists an integer k such that f is constant and\nequal to a rational on each interval [(i\u22121)/k, i/k). it is not hard to check (exercise 34.1) that\nthe collection of such f \u2019s is dense in d with respect to \u03c1, which shows (d, \u03c1 ) is separable.\nthe space d with the metric \u03c1 is not, however, complete; see exercise 34.2. we therefore\n\nintroduce a slightly different metric d. define\n\n(cid:20)(cid:20)(cid:20) log\n\n(cid:20)(cid:20)(cid:20)\n\n\u03bb(t ) \u2212 \u03bb(s)\n\nt \u2212 s\n\n(cid:21)\u03bb(cid:21) = sup\n\ns(cid:16)=t,s,t\u2208[0,1]\n\nand let\n\nd ( f , g) = inf{\u03b5 > 0 : \u2203\u03bb \u2208 \u0001 such that (cid:21)\u03bb(cid:21) \u2264 \u03b5, sup\nt\u2208[0,1]\n\n| f (t ) \u2212 g(\u03bb(t ))| \u2264 \u03b5}.\n\nnote (cid:21)\u03bb\u22121(cid:21) = (cid:21)\u03bb(cid:21) and (cid:21)\u03bb2 \u25e6 \u03bb1(cid:21) \u2264 (cid:21)\u03bb1(cid:21) + (cid:21)\u03bb2(cid:21). the symmetry of d and the triangle\ninequality follow easily from this, and we conclude d is a metric.\n\nlemma 34.1 there exists \u03b50 such that\n\n\u03c1( f , g) \u2264 2d ( f , g)\n\nif d ( f , g) < \u03b50.\n(it turns out \u03b50 = 1/4 will do.)\nproof since log(1 + 2x)/(2x) \u2192 1 as x \u2192 0, we have\n\nlog(1 \u2212 2\u03b5) < \u2212\u03b5 < \u03b5 < log(1 + 2\u03b5)\n\nif \u03b5 is small enough. suppose d ( f , g) < \u03b5 and \u03bb is the element of \u0001 such that d ( f , g) <\n(cid:21)\u03bb(cid:21) < \u03b5 and supt\u2208[0,1]\n\n| f (t ) \u2212 g(\u03bb(t ))| < \u03b5. since \u03bb(0) = 0, we have\n< \u03b5 < log(1 + 2\u03b5),\nlog(1 \u2212 2\u03b5) < \u2212\u03b5 < log\n\n\u03bb(t )\n\nor\n\n1 \u2212 2\u03b5 <\n\nwhich implies |\u03bb(t ) \u2212 t| < 2\u03b5, and hence \u03c1( f , g) \u2264 2d ( f , g).\n\nt\n\n< 1 + 2\u03b5,\n\n\u03bb(t )\n\nt\n\n(34.1)\n\n(34.2)\n\nwe define the analog \u03be f of the modulus of continuity for a function in d as follows. define\n\u03b8 f [a, b) = sups,t\u2208[a,b) | f (t ) \u2212 f (s)| and\n\n\u03be f (\u03b4) = inf{ max\n1\u2264i\u2264n\nsuch that ti \u2212 ti\u22121 > \u03b4 for all i \u2264 n}.\n\n\u03b8 f [ti\u22121, ti) : \u2203n \u2265 1, 0 = t0 < t1 < \u00b7\u00b7\u00b7 < tn = 1\n\nobserve that if f \u2208 d, then \u03be f (\u03b4) \u2193 0 as \u03b4 \u2193 0.\n\n "}, {"Page_number": 279, "text": "34.1 metrics for d[0, 1]\n\n261\n\nlemma 34.2 suppose \u03b4 < 1/4. let f \u2208 d. if \u03c1( f , g) \u2264 \u03b42, then d ( f , g) \u2264 4\u03b4 + \u03be f (\u03b4).\nproof choose ti\u2019s such that ti \u2212 ti\u22121 > \u03b4 and \u03b8 f [ti\u22121, ti) < \u03be f (\u03b4) + \u03b4 for each i. pick \u03bc \u2208 \u0001\n|\u03bc(t )\u2212t| < \u03b42. then supt\n| f (\u03bc\u22121(t ))\u2212g(t )| <\nsuch that supt\n\u03b42. set \u03bb(ti) = \u03bc(ti) and let \u03bb be linear in between. since \u03bc\u22121(\u03bb(ti)) = ti for all i, then t and\n\u03bc\u22121 \u25e6 \u03bb(t ) always lie in the same subinterval [ti\u22121, ti). consequently\n\n| f (t )\u2212g(\u03bc(t ))| < \u03b42 and supt\n\n| f (t ) \u2212 g(\u03bb(t ))| \u2264 | f (t ) \u2212 f (\u03bc\u22121(\u03bb(t )))| + | f (\u03bc\u22121(\u03bb(t ))) \u2212 g(\u03bb(t ))|\n\n\u2264 \u03b8 f (\u03b4) + \u03b42\n\u2264 \u03be f (\u03b4) + \u03b4 + \u03b42 < \u03be f (\u03b4) + 4\u03b4.\n\nwe have\n\n|\u03bb(ti) \u2212 \u03bb(ti\u22121) \u2212 (ti \u2212 ti\u22121)| = |\u03bc(ti) \u2212 \u03bc(ti\u22121) \u2212 (ti \u2212 ti\u22121)|\n\n\u2264 2\u03b42 < 2\u03b4(ti \u2212 ti\u22121).\n\nsince \u03bb is defined by linear interpolation,\n\n|\u03bb(t ) \u2212 \u03bb(s)) \u2212 (t \u2212 s)| \u2264 2\u03b4|t \u2212 s|,\n\ns, t \u2208 [0, 1],\n\nwhich leads to\n\nor\n\nlog(1 \u2212 2\u03b4) \u2264 log\n\nsince \u03b4 < 1\n\n4 , we have (cid:21)\u03bb(cid:21) \u2264 4\u03b4.\n\n(cid:20)(cid:20)(cid:20) \u03bb(t ) \u2212 \u03bb(s)\n\n(cid:20)(cid:20)(cid:20) \u2264 2\u03b4,\n(cid:10) \u03bb(t ) \u2212 \u03bb(s)\n(cid:11)\n\nt \u2212 s\n\n\u2212 1\n\nt \u2212 s\n\n\u2264 log(1 + 2\u03b4).\n\nproposition 34.3 the metrics d and \u03c1 are equivalent, i.e., they generate the same topology.\n\nin particular, (d, d ) is separable.\n\nproof let b\u03c1 ( f , r) denote the ball with center f and radius r with respect to the metric \u03c1\nand define bd ( f , r) analogously. let \u03b5 > 0 and let f \u2208 d. if d ( f , g) < \u03b5/2 and \u03b5 is small\nenough, then \u03c1( f , g) \u2264 2d ( f , g) < \u03b5, and so bd ( f , \u03b5/2) \u2282 b\u03c1 ( f , \u03b5).\nto go the other direction, what we must show is that given f and \u03b5, there exists \u03b4 such\nthat b\u03c1 ( f , \u03b4) \u2282 bd ( f , \u03b5). \u03b4 may depend on f ; in fact, it has to in general, for otherwise a\ncauchy sequence with respect to d would be a cauchy sequence with respect to \u03c1, and vice\nversa. choose \u03b4 small enough that 4\u03b41/2 + \u03be f (\u03b41/2) < \u03b5. by lemma 34.2, if \u03c1( f , g) < \u03b4,\nthen d ( f , g) < \u03b5, which is what we want.\nfinally, suppose g is open with respect to the topology generated by \u03c1. for each f \u2208 g,\nlet r f be chosen so that b\u03c1 ( f , r f ) \u2282 g. hence g = \u222a f \u2208gb\u03c1 ( f , r f ). let s f be chosen so that\nbd ( f , s f ) \u2282 b\u03c1 ( f , r f ). then \u222a f \u2208gbd ( f , s f ) \u2282 g, and in fact the sets are equal because if\nf \u2208 g, then f \u2208 bd ( f , s f ). since g can be written as the union of balls which are open with\nrespect to d, then g is open with respect to d. the same argument with d and \u03c1 interchanged\nshows that a set that is open with respect to d is open with respect to \u03c1.\n\n "}, {"Page_number": 280, "text": "262\n\nthe space d[0, 1]\n\n34.2 compactness and completeness\n\nwe now show completeness for (d, d ).\n\ntheorem 34.4 the space d with the metric d is complete.\n\nproof let fn be a cauchy sequence with respect to the metric d. if we can find a subsequence\nn j such that fn j converges, say, to f , then it is standard that the whole sequence converges to\nf . choose n j such that d ( fn j\n| fn j\n\n\u2212 j. for each j there exists \u03bb j such that\n(\u03bb j (t ))| \u2264 2\n\n, fn j+1\n) < 2\n(t ) \u2212 fn j+1\n\n(cid:21)\u03bb j(cid:21) \u2264 2\n\n\u2212 j,\n\n\u2212 j.\n\nsup\n\nt\n\nas in (34.1) and (34.2),\n\nthen\n\n|\u03bb j (t ) \u2212 t| \u2264 2\n\n\u2212 j+1.\n\nsup\n\nt\n\n|\u03bbn+m+1 \u25e6 \u03bbm+n \u25e6 \u00b7\u00b7\u00b7 \u25e6 \u03bbn(t ) \u2212 \u03bbn+m \u25e6 \u00b7\u00b7\u00b7 \u25e6 \u03bbn(t )|\n= sup\n\u2264 2\n\n|\u03bbn+m+1(s) \u2212 s|\n\ns\n\u2212(n+m)\n\nfor each n. hence for each n, the sequence \u03bbm+n \u25e6 \u00b7\u00b7\u00b7 \u25e6 \u03bbn (indexed by m) is a cauchy\nsequence of functions on [0, 1] with respect to the supremum norm on [0, 1]. let \u03bdn be the\nlimit. clearly \u03bdn(0) = 0, \u03bdn(1) = 1, \u03bdn is continuous, and nondecreasing. we also have\n\n(cid:20)(cid:20)(cid:20) log\n\n(cid:20)(cid:20)(cid:20)\n\n\u03bbn+m \u25e6 \u00b7\u00b7\u00b7 \u25e6 \u03bbn(t ) \u2212 \u03bbn+m \u25e6 \u00b7\u00b7\u00b7 \u25e6 \u03bbn(s)\nt \u2212 s\n\u2264 (cid:21)\u03bbn+m \u25e6 \u00b7\u00b7\u00b7 \u25e6 \u03bbn(cid:21)\n\u2264 (cid:21)\u03bbn+m(cid:21) + \u00b7\u00b7\u00b7 + (cid:21)\u03bbn(cid:21)\n\u2264 1\n2n\u22121\n\n.\n\nif we then let m \u2192 \u221e, we obtain(cid:20)(cid:20)(cid:20) log\n\n(cid:20)(cid:20)(cid:20) \u2264 1\n\n,\n\n2n\u22121\n\n\u03bdn(t ) \u2212 \u03bdn(s)\n\nt \u2212 s\n\nwhich implies \u03bdn \u2208 \u0001 with (cid:21)\u03bdn(cid:21) \u2264 21\u2212n.\n\nwe see that \u03bdn = \u03bdn+1 \u25e6 \u03bbn. consequently\n\nsup\n\n| fn j\n\u25e6 \u03bd\u22121\ntherefore fn j\nf be the limit. since\n\nt\n\nj\n\n(\u03bd\u22121\n\nj\n\n(t )) \u2212 fn j+1\n\n(\u03bd\u22121\nj+1\n\n(t ))| = sup\n\n| fn j\n\n(s) \u2212 fn j+1\n\n(\u03bb j (s))| \u2264 2\n\n\u2212 j.\n\ns\n\nis a cauchy sequence on [0, 1] with respect to the supremum norm. let\n\n| fn j\n(\u03bd\u22121\nsup\n, f ) \u2192 0.\nand (cid:21)\u03bd j(cid:21) \u2192 0 as j \u2192 \u221e, then d ( fn j\n\nt\n\nj\n\n(t )) \u2212 f (t )| \u2192 0\n\n "}, {"Page_number": 281, "text": "263\nwe next show that if fn \u2192 f with respect to d and f \u2208 c[0, 1], the convergence is in fact\n\n34.2 compactness and completeness\n\n| fn(t ) \u2212 f (t )| \u2192 0.\n\nuniform.\nproposition 34.5 suppose fn \u2192 f in the topology of d[0, 1] with respect to d and f \u2208\nc[0, 1]. then supt\u2208[0,1]\nis uniformly continuous on [0, 1], there exists \u03b4 such that\nproof let \u03b5 > 0. since f\n| f (t ) \u2212 f (s)| < \u03b5/2 if |t \u2212 s| < \u03b4. for n sufficiently large there exists \u03bbn \u2208 \u0001 such that\n|\u03bbn(t ) \u2212 t| < \u03b4. therefore | f (\u03bbn(t )) \u2212 f (t )| < \u03b5/2,\nsupt\nand so | fn(t ) \u2212 f (t )| < \u03b5.\nwe turn to compactness.\n\n| fn(t ) \u2212 f (\u03bbn(t ))| < \u03b5/2 and supt\n\ntheorem 34.6 a set a has compact closure in d[0, 1] if\n| f (t )| < \u221e\n\nsup\n\nsup\nf \u2208a\n\nt\n\nand\n\n\u03be f (\u03b4) = 0.\n\nlim\n\u03b4\u21920\n\nsup\nf \u2208a\n\nthe converse of this theorem is also true, but we won\u2019t need this. see billingsley (1968) or\nexercise 34.9.\n\nproof a complete and totally bounded set in a metric space is compact, and d[0, 1] is a\ncomplete metric space. hence it suffices to show that a is totally bounded: for each \u03b5 > 0\nthere exist finitely many balls of radius \u03b5 that cover a.\nlet \u03b7 > 0 and choose k large such that 1/k < \u03b7 and \u03be f (1/k) < \u03b7 for each f \u2208 a.\nlet m = sup f \u2208a supt\n| f (t )| and let h = {\u2212m + j/k : j \u2264 2km}, so that h is an \u03b7-net\nfor [\u2212m, m]. let b be the set of functions f \u2208 d[0, 1] that are constant on each interval\n[(i \u2212 1)/k, i/k) and that take values only in the set h . in particular, f (1) \u2208 h .\nwe first prove that b is a 2\u03b7-net for a with respect to \u03c1. if f \u2208 a, there exist t0, . . . , tn such\nthat t0 = 0, tn = 1, ti \u2212 ti\u22121 > 1/k for each i, and \u03b8 f [ti\u22121, ti) < \u03b7 for each i. note we must\nhave n \u2264 k. for each i choose integers ji such that ji/k \u2264 ti < ( ji + 1)/k. the ji are distinct\nsince the ti are at least 1/k apart. define \u03bb so that \u03bb( ji/k) = ti and \u03bb is linear on each interval\n[ ji/k, ji+1/k]. choose g \u2208 b such that |g(m/k) \u2212 f (\u03bb(m/k))| < \u03b7 for each m \u2264 k. observe\nthat each [m/k, (m + 1)/k) lies inside some interval of the form [ ji/k, ji+1/k). since \u03bb is\nincreasing, [\u03bb(m/k), \u03bb((m + 1)/k)) is contained in [\u03bb( ji/k), \u03bb( ji+1/k)) = [ti, ti+1). the\nfunction f does not vary more than \u03b7 over each interval [ti, ti+1), so f (\u03bb(t )) does not vary\nmore than \u03b7 over each interval [m/k, (m + 1)/k). g is constant on each such interval, and\nhence\n\n|g(t ) \u2212 f (\u03bb(t ))| < 2\u03b7.\n\nsup\n\nt\n\nwe have\n\n|\u03bb( ji/k) \u2212 ji/k| = |ti \u2212 ji/k| < 1/k < \u03b7\n\n|\u03bb(t ) \u2212 t| < \u03b7. thus \u03c1( f , g) < 2\u03b7. we have\nfor each i. by the piecewise linearity of \u03bb, supt\nproved that given f \u2208 a, there exists g \u2208 b such that \u03c1( f , g) < 2\u03b7, or b is a 2\u03b7-net for a\nwith respect to \u03c1.\n\n "}, {"Page_number": 282, "text": "264\n\nthe space d[0, 1]\n\nnow let \u03b5 > 0 and choose \u03b4 > 0 small so that 4\u03b4 + \u03be f (\u03b4) < \u03b5 for each f \u2208 a. set\n\u03b7 = \u03b42/4. choose b as above to be a 2\u03b7-net for a with respect to \u03c1. by lemma 34.2, if\n\u03c1( f , g) < 2\u03b7 < \u03b42, then d ( f , g) \u2264 4\u03b4 + \u03be f (\u03b4) < \u03b5. therefore b is an \u03b5-net for a with\nrespect to d.\n\nthe following corollary is proved exactly similarly to theorem 32.1.\n\ncorollary 34.7 suppose xn are processes whose paths are right continuous with left limits.\nsuppose for each \u03b5 and \u03b7 there exists n0, r, and \u03b4 such that\n\n(\u03b4) \u2265 \u03b5) \u2264 \u03b7\n\np(\u03bexn\n\nand\n\n|xn(t )| \u2265 r) \u2264 \u03b7.\n\np( sup\nt\u2208[0,1]\n\nthen the xn are tight with respect to the topology of d[0, 1].\n\n34.3 the aldous criterion\n\na very useful criterion for tightness is the following one due to aldous (1978).\ntheorem 34.8 let {xn} be a sequence in d[0, 1]. suppose\np(|xn(t )| \u2265 r) = 0\n\nlim\nr\u2192\u221e sup\n\nn\n\nfor each t \u2208 [0, 1] and that whenever \u03c4n are stopping times for xn and \u03b4n \u2192 0 are reals,\n\n|xn(\u03c4n + \u03b4n) \u2212 xn(\u03c4n)|\n\nconverges to 0 in probability as n \u2192 \u221e.\nproof we will set xn(t ) = xn(1) for t \u2208 [1, 2] to simplify notation. the proof of this\ntheorem comprises four steps.\nstep 1. we claim that (34.4) implies the following: given \u03b5 there exist n0 and \u03b4 such that\n\np(|xn(\u03c4n + s) \u2212 xn(\u03c4n)| \u2265 \u03b5) \u2264 \u03b5\n\n= snk gives a contradiction to (34.4).\n\n(34.5)\nfor each n \u2265 n0, s \u2264 2\u03b4, and \u03c4n a stopping time for xn. for if not, we choose an increasing\n\u2264 1/k for which (34.5) does not hold. taking\nsubsequence nk, stopping times \u03c4nk , and snk\n\u03b4nk\nstep 2. let \u03b5 > 0, fix n \u2265 n0, and let t \u2264 u \u2264 1 be two stopping times for xn. we will prove\n(34.6)\n\np(u \u2264 t + \u03b4,|xn(u ) \u2212 xn(t )| \u2265 2\u03b5) \u2264 16\u03b5.\n\nto prove this, we start by letting \u03bb be lebesgue measure. if\n\nat = {(\u03c9, s) \u2208 \u0001 \u00d7 [0, 2\u03b4] : |xn(t + s) \u2212 xn(t )| \u2265 \u03b5},\n\nthen for each s \u2264 2\u03b4 we have p(\u03c9 : (\u03c9, s) \u2208 at ) \u2264 \u03b5 by (34.5) with \u03c4n replaced by t .\nwriting p \u00d7 \u03bb for the product measure, we then have\np \u00d7 \u03bb(at ) \u2264 2\u03b4\u03b5.\n\n(34.7)\n\n(34.3)\n\n(34.4)\n\n "}, {"Page_number": 283, "text": "set bt (\u03c9) = {s : (\u03c9, s) \u2208 at} and ct = {\u03c9 : \u03bb(bt (\u03c9)) \u2265 1\n\n34.3 the aldous criterion\n\n265\n\u03b4}. from( 34.7) and the\n\n4\n\nfubini theorem,\n\nso\n\n(cid:3)\n\n\u03bb(bt (\u03c9)) p(d\u03c9) \u2264 2\u03b4\u03b5,\n\np(ct ) \u2264 8\u03b5.\n\n\u03b4,\n\n\u03b4.\n\n4\n\n4\n\nwe similarly define bu and cu , and obtain p(ct \u222a cu ) \u2264 16\u03b5.\n\nif \u03c9 /\u2208 ct \u222a cu , then \u03bb(bt (\u03c9)) \u2264 1\n\n\u03b4 and \u03bb(bu (\u03c9)) \u2264 1\n\n4\n\n\u03b4. suppose u \u2264 t + \u03b4. then\n\n4\n\n\u03bb{t \u2208 [t, t + 2\u03b4] : |xn(t ) \u2212 xn(t )| \u2265 \u03b5} \u2264 1\n\nand\n\n\u03bb{t \u2208 [u, u + \u03b4] : |xn(t ) \u2212 xn(u )| \u2265 \u03b5} \u2264 1\n\nhence there exists t \u2208 [t, t + 2\u03b4] \u2229 [u, u + \u03b4] such that |xn(t ) \u2212 xn(t )| < \u03b5 and\n|xn(t ) \u2212 xn(u )| < \u03b5; this implies |xn(u ) \u2212 xn(t )| < 2\u03b5, which proves (34.6).\nstep 3. we obtain a bound on \u03bexn. let tn0 = 0 and\n\ntn,i+1 = inf{t > tni : |xn(t ) \u2212 xn(tni)| \u2265 2\u03b5} \u2227 2.\n\nnote we have |xn(tn,i+1) \u2212 xn(tni)| \u2265 2\u03b5 if tni < 2. we choose n0, \u03b4 as in step 1. by step 2\nwith t = tni and u = tn,i+1,\n\np(tn,i+1 \u2212 tni < \u03b4, tni < 2) \u2264 16\u03b5.\n\n(34.8)\nlet k = [2/\u03b4] + 1 and apply (34.5) with \u03b5 replaced by \u03b5/k to see that there exist n1 \u2265 n0\nand \u03b6 \u2264 \u03b4 \u2227 \u03b5 such that if n \u2265 n1, s \u2264 2\u03b6 , and \u03c4n is a stopping time, then\n\n(34.9)\n\n(34.10)\n\n(34.11)\n\np(|xn(\u03c4n + s) \u2212 xn(\u03c4n)| > \u03b5/k ) \u2264 \u03b5/k.\n\nby (34.6) with t = tni and u = tn,i+1 and \u03b4 replaced by \u03b6 ,\np(tn,i+1 \u2264 tni + \u03b6 ) \u2264 16\u03b5/k\n\nfor each i and hence\n\np(\u2203i \u2264 k : tn,i+1 \u2264 tni + \u03b6 ) \u2264 16\u03b5.\n\nwe have\n\ne [tni \u2212 tn,i\u22121; tnk < 1] \u2265 \u03b4p(tni \u2212 tn,i\u22121 \u2265 \u03b4, tnk < 1)\n\n\u2265 \u03b4[p(tnk < 1) \u2212 p(tni \u2212 tn,i\u22121 < \u03b4, tnk < 1)]\n\u2265 \u03b4[p(tnk < 1) \u2212 16\u03b5],\n\nwhere we used (34.8) in the last step. summing over i from 1 to k,\n\np(tnk < 1) \u2265 e [tnk; tnk < 1] = k(cid:9)\n\ne [tni \u2212 tn,i\u22121; tnk < 1]\n\u2265 k\u03b4[p(tnk < 1) \u2212 16\u03b5] \u2265 2[p(tnk < 1) \u2212 16\u03b5],\n\ni=1\n\n "}, {"Page_number": 284, "text": "the space d[0, 1]\n\n266\nor p(tnk < 1) \u2264 32\u03b5. hence except for an event of probability at most 32\u03b5, we have\n\u03bexn\n|xn(t )|. let \u03b5 > 0 and choose \u03b4 and n0 as\nstep 4. the last step is to obtain a bound on supt\nin step 1. define\n\n(\u03b6 ) \u2264 4\u03b5.\n\ndrn = {(\u03c9, s) \u2208 \u0001 \u00d7 [0, 1] : |xn(s)(\u03c9)| > r}\n\nfor r > 0. the measurability of drn with respect to the product \u03c3 -field f \u00d7 b[0, 1], where\nb[0, 1] is the borel \u03c3 -field on [0, 1], follows by the fact that xn is right continuous with left\nlimits. let\n\ng(r, s) = sup\n\np(|xn(s)| > r).\n\nn\n\nby (34.3), g(r, s) \u2192 0 as r \u2192 \u221e for each s. pick r large so that\n\nthen\n\n(cid:3)\n\n1drn\n\n\u03bb({s : g(r, s) > \u03b5\u03b4}) < \u03b5\u03b4.\n(cid:13)\n\n(\u03c9, s) p(d\u03c9) = p(|xn(s)| > r) \u2264\n\n1, g(r, s) > \u03b5\u03b4,\n\u03b5\u03b4,\n\notherwise.\n\nif ern(\u03c9) = {s : (\u03c9, s) \u2208 drn} and frn = {\u03c9 : \u03bb(ern) > \u03b4/4}, we have\n\n\u03b4p(frn) =\n\n1\n4\n\n\u03b4 p(d\u03c9) \u2264\n\n1\n4\n\nfrn\n\n(\u03c9, s) \u03bb(ds) p(d\u03c9) \u2264 2\u03b5\u03b4,\n\n1\n\n0\n\n1drn\n\np \u00d7 \u03bb(drn) < 2\u03b5\u03b4.\n\n(cid:3) (cid:3)\n\nintegrating over s \u2208 [0, 1],\n(cid:3)\n\nso p(frn) \u2264 8\u03b5.\n\ndefine t = inf{t : |xn(t )| \u2265 r + 2\u03b5} \u2227 2 and define at , bt , and ct as in step 2. we have\n\np(ct \u222a frn) \u2264 16\u03b5.\n\nif \u03c9 /\u2208 ct \u222a frn and t < 2, then \u03bb(ern(\u03c9)) \u2264 \u03b4/4. hence there exists t \u2208 [t, t + 2\u03b4] such\nthat |xn(t )| \u2264 r and |xn(t ) \u2212 xn(t )| \u2264 \u03b5. therefore |xn(t )| \u2264 r + \u03b5, which contradicts\nthe definition of t . we conclude that t must equal 2 on the complement of ct \u222a frn, or in\n|xn(t )| \u2264 r + 2\u03b5,\nother words, except for an event of probability at most 16\u03b5, we have supt\nprovided, of course, that n \u2265 n0.\n\nan application of corollary 34.7 completes the proof.\n\naldous\u2019s criterion is particularly well suited for strong markov processes.\n\nproposition 34.9 suppose xn is a sequence of real-valued strong markov processes and\nthere exists c, p, and \u03b3 > 0 such that\n\ne x|xn(t ) \u2212 xn(0)|p \u2264 ct \u03b3 ,\n\nx \u2208 r,\n\nt \u2208 [0, 1].\n\n(34.12)\nthen for each x \u2208 r, the sequence of px-laws of {xn} is tight with respect to the space\nd[0, 1].\n\n "}, {"Page_number": 285, "text": "unlike the kolmogorov continuity criterion, we do not require \u03b3 > 1.\n\nexercises\n\n267\n\nproof fix x. for each t,\n\npx(|xn(t )| \u2265 r + |x|) \u2264 px(|xn(t ) \u2212 xn(0)| \u2265 r)\n\n\u2264 e x|xn(t ) \u2212 xn(0)|p\n\u2264 ct \u03b3\nrp\n\nrp\n\n,\n\nwhich tends to 0 as r \u2192 \u221e. we used chebyshev\u2019s inequality here.\n\nsuppose \u03c4n are stopping times for xn and \u03b4n \u2192 0. by the strong markov property, for each\n\n\u03b5 > 0\n\npx(|xn(\u03c4n + \u03b4n) \u2212 xn(\u03c4n)| > \u03b5) \u2264 e x|xn(\u03c4n + \u03b4n) \u2212 xn(\u03c4n)|p\n\n(cid:26)\n= \u03b5\u2212pe x\n\u2264 c\u03b5\u2212p\u03b4\u03b3\nwhich tends to 0 as n \u2192 \u221e. now apply theorem 34.8.\n\ne xn (\u03c4n )|xn(\u03b4n) \u2212 xn(0)|\u03b3\n\n\u03b5 p\n\n,\n\nn\n\n(cid:27)\n\nexercises\n\n34.1 show that the space d with the metric \u03c1 is separable.\n34.2 let fn = 1[1/2,1/2+1/n). show that this is a cauchy sequence with respect to \u03c1, but does not\n\nconverge to an element of d. show { fn} is not a cauchy sequence with respect to d.\n\n34.3 show that (with respect to the topology on d) the subset c[0, 1] of d is nowhere dense.\n34.4 consider d with the metric dsup( f , g) = supt\u2208[0,1]\n\n| f (t ) \u2212 g(t )|. show that d is not separable\n\nwith respect to the metric dsup.\n\n34.5 suppose p and p(cid:3)\n\nd[0, 1]. in other words, all the finite-dimensional distributions agree. prove that p = p(cid:3)\nd[0, 1].\n\nare measures supported on d[0, 1] that agree on all cylindrical subsets of\non\n\n34.6 show that the following are continuous functions on the space d[0, 1].\n\n(2) f (x) =(cid:15)\n\n(1) f (x) = supt\u22641 x(t ).\n1\n0 x(t ) dt.\n(3) f (x) = supt\u22641\n\n(x(t ) \u2212 x(t\u2212)).\n\n34.7 let p be a poisson process with parameter \u03bb. prove that\n\npnt \u2212 n\u03bbt\u221a\n\nn\u03bb\n\nconverges weakly with respect to the topology of d[0, 1] as n \u2192 \u221e to a brownian motion.\n\n34.8 suppose xn converges weakly to x with respect to the topology of c[0, 1]. prove that xn\n\nconverges weakly to x with respect to the topology of d[0, 1].\n\n "}, {"Page_number": 286, "text": "268\n\nthe space d[0, 1]\n\n34.9 this is the converse to theorem 34.6. let a be an index set, and suppose the collection of\n\nfunctions { f\u03b1, f \u2208 a} is precompact in d[0, 1], i.e., its closure is compact.\n\n(1) prove sup\u03b1\u2208a sup0\u2264t\u22641\n(2) prove\n\n| f (t )| < \u221e.\n\n(\u03b4) = 0.\n\nlim\n\u03b4\u21920\n\nsup\n\u03b1\u2208a\n\n\u03be f\u03b1\n\nnotes\n\nsee billingsley (1968) for more information.\n\n "}, {"Page_number": 287, "text": "35\n\napplications of weak convergence\n\nin chapter 32 we showed how weak convergence of stochastic processes could be used to\ngive another construction of brownian motion by showing that a simple symmetric random\nwalk converges to a brownian motion. in the first section of this chapter, we show that the\nsum of independent, identically distributed mean zero random variables with variance one\nalso converges to a brownian motion, which is known as the donsker invariance principle.\nwe then consider a brownian bridge, which is a brownian motion conditioned to return\nto zero at time one. we prove in section 35.3 that a brownian bridge is the limit process for\na sequence of normalized empirical processes.\n\nsn =(cid:12)\n\n35.1 donsker invariance principle\n\n\u221a\n\nn\ni=1 yi, and zn(t ) is defined to be equal to snt /\n\nsuppose the yi are i.i.d. real-valued random variables with mean zero and variance one,\nn if nt is an integer and defined by\nlinear interpolation for other values of t. the donsker invariance principle says that the zn\nconverge weakly with respect to the space c[0, 1] to a brownian motion. this is a bit more\ndelicate than in section 32.2 because here our yi only have second moments.\n\nthe statement of the donsker invariance principle is the following.\n\ntheorem 35.1 let the yi and zn be as above. then zn converges weakly to the law of\nbrownian motion on [0, 1] with respect to the metric of c[0, 1].\n\nbefore we prove this, we give an application and explain the name \u201cinvariance principle.\u201d\n\nan example of how the donsker invariance principle can be used is the following.\ncorollary 35.2 let m = sups\u22641 ws and mn = sups\u22641 zn(s), where w is a brownian motion.\nthen mn converges weakly to m.\n\nproof let g be a bounded and continuous function on the reals and define a function f on\nc[0, 1] by\n\nf ( f ) = g(sup\ns\u22641\n| f2(s)\u2212 f1(s)| and therefore f : c[0, 1] \u2192 r\nnotice | sups\u22641 f2(s)\u2212 sups\u22641 f1(s)| \u2264 sups\u22641\nis bounded and continuous. since zn converges weakly to w with respect to the topology on\nc[0, 1], then e f (zn) \u2192 e f (w ). this is equivalent to e g(mn) \u2192 e g(m ). because g is\nan arbitrary bounded continuous function on the reals, we conclude mn \u2192 m weakly.\n\nf (s)).\n\n269\n\n "}, {"Page_number": 288, "text": "270\n\napplications of weak convergence\n\nthis corollary says that the distribution of maxi\u2264n si/\n\n\u221a\nn converges to the supremum of\na brownian motion. we can actually use this to derive the distribution of the maximum of\na brownian motion: first determine the distribution of the maximum of sn when the yi\u2019s\nare particularly simple, such as when they are a simple symmetric random walk. (that is,\n2 .) then take the limit as n \u2192 \u221e. in the case of a simple\np(yi = 1) = p(yi = \u22121) = 1\nsymmetric random walk, we can find the distribution of the maximum using the reflection\nprinciple, and there are no technical difficulties with the proof, unlike using the reflection\nprinciple with brownian motion.\n|wt|2 dt. here the dis-\ntribution of i can be found by an eigenvalue argument (kuo, 1975), and this is then an\napproximation to the distribution of in.\n\nanother useful example is where in = (cid:15)\n\n|zn(t )|2 dt and i = (cid:15)\n\n1\n0\n\n1\n0\n\nif f\n\nis a continuous function from c[0, 1] to r, an argument similar to the proof of\ncorollary 35.2 shows that f (zn) converges weakly to f (w ). we get the same limit process,\nregardless of the distribution of the yi\u2019s, provided only that they are i.i.d. with mean zero\nand variance one. this is where the name \u201cinvariance principle\u201d comes from \u2013 the limit is\ninvariant with respect to changing the distribution of the yi\u2019s.\n\nlemma 35.3 suppose we have a sequence yi of i.i.d. random variables with mean zero and\n\nvariance one and sn =(cid:12)\n\nn\ni=1 yi. suppose \u03bb > 4. then\n\nn) \u2264 4\n\np(|sn| \u2265 \u03bb\n\n\u221a\nn/2).\n\n\u221a\n\n|si| \u2265 \u03bb\n\u221a\n\np(max\ni\u2264n\nproof let n = min{i : |si| \u2265 \u03bb\ntime and (n = i) is in the \u03c3 -field generated by y1, . . . , yi. we have\n\nn}, the first time si is bigger than \u03bb\n\u221a\n\n3\n\n\u221a\nn) \u2264 p(|sn| \u2265 \u03bb\n\u2264 p(|sn| \u2265 \u03bb\n\n\u221a\n\u221a\n\n+ n\u22121(cid:9)\n\nn/2) + p(n < n,|sn| < \u03bb\nn/2)\n\n\u221a\n\n|si| \u2265 \u03bb\n\np(max\ni\u2264n\n\np(n = i,|sn| < \u03bb\nn/2).\ni=1\n\u221a\n\u221a\nif n = i with i < n and |sn| < \u03bb\nn/2, then |sn \u2212 si| \u2265 \u03bb\nn/2, and moreover the event\n{|sn \u2212 si| \u2265 \u03bb\nn/2} is in the \u03c3 -field generated by yi+1, . . . , yn, and hence is independent\nof the event {n = i}. using chebyshev\u2019s inequality, the sum on the last line of (35.1) is\nn\u22121(cid:9)\nbounded by\n\n\u221a\n\n\u221a\n\ne|sn \u2212 si|2\n\np(n = i)p(|sn \u2212 si| \u2265 \u03bb\n\np(n = i)\n\n\u221a\nn. n is a stopping\n\nn/2)\n\n(35.1)\n\ni=1\n\nn/2) \u2264 n\u22121(cid:9)\n= n\u22121(cid:9)\n\ni=1\n\n\u03bb2n/4\nn \u2212 i\n\u03bb2n/4\n\u221a\n\nn),\n\np(n = i)\n\ni=1\n\u2264 1\np(n < i)\n\u2264 1\np(max\ni\u2264n\n\n4\n\n4\n\n|si| \u2265 \u03bb\n\nsince \u03bb > 4. therefore\n\np(|si| \u2265 \u03bb\n\np(max\ni\u2264n\n\n\u221a\n\nn) \u2264 p(|sn| \u2265 \u03bb\n\n\u221a\nn/2) + 1\n\n4\n\np(max\ni\u2264n\n\n|si| \u2265 \u03bb\n\n\u221a\nn).\n\n "}, {"Page_number": 289, "text": "35.1 donsker invariance principle\n\n271\n\nsubtracting the second term on the right from both sides and multiplying by 4/3 proves the\nlemma.\n\nnote that the central limit theorem tells us that for any \u03b2 > 0\n\u221a\nn) \u2192 p(|z| \u2265 \u03b2 ) \u2264 e\n\np(|sn| \u2265 \u03b2\n\n\u2212\u03b22/2,\n\nwhere z is a mean zero normal random variable with variance one, and hence for n large\n(depending on \u03b2),\n\n(35.2)\nlemma 35.4 for each \u03b5, \u03b7 > 0, there exist n0 and \u03b4 such that if n \u2265 n0 and s \u2208 [0, 1 \u2212 \u03b4],\nthen\n\np(|sn| \u2265 \u03b2\n\n\u221a\nn) \u2264 2e\n\n\u2212\u03b22/2.\n\n|zn(t ) \u2212 zn(s)| > \u03b5) \u2264 \u03b7\u03b4.\n\np( sup\ns\u2264t\u2264s+\u03b4\n\n\u2212\u03b52/128\u03b4 \u2264 \u03b4\u03b7/2. then choose j0\n\nproof let \u03b5, \u03b7 > 0, and choose \u03b4 small enough that 2e\nlarge enough so that, using (35.2),\n|s j| >\n\n\u2264 2e\n\n\u221a\n\u221a\n\np\n\n\u03b5\n\n\u2212\u03b52/128\u03b4 \u2264 \u03b4\u03b7/2\n\n(cid:11)\n\n(cid:10)\n\nj\n\u03b4\n\n8\n\nif j \u2265 j0. finally, choose n0 \u2265 j0/\u03b4 + 2, so that if n \u2265 n0, then [n\u03b4] + 2 \u2265 j0 and\nn\u03b4 \u2265 ([n\u03b4] + 2)/2, where [x] is the largest integer less than or equal to x.\nlet n \u2265 n0 and set j = [n\u03b4] + 2. suppose there exists s such that for some t \u2208 [s, s + \u03b4]\nwe have |zn(t ) \u2212 zn(s)| > \u03b5. then there exists j \u2264 n such that for some i between j and\nj + j we have |si \u2212 s j| \u2265 \u03b5\n\np( sup\ns\u2264t\u2264s+\u03b4\n\n(cid:10)\n\n\u221a\nn/2. therefore n \u2265 j/2\u03b4 and by lemma 35.3\n\u221a\n(cid:11)\nn\u03b5/2)\n\u221a\n\u221a\nj \u03b5\n\u03b4\n\n|zn(t ) \u2212 zn(s)| > \u03b5) \u2264 p( max\nj\u2264i\u2264 j+j\n(cid:10)\n(cid:10)\n\n|si \u2212 s j| >\n|si \u2212 s j| >\nmax\nj\u2264i\u2264 j+j\n\u221a\n4\n(cid:11)\n|s j+j \u2212 s j| >\n\u221a\nj \u03b5\n\u03b4\n|sj| >\n\n\u2264 p\n\n\u2264 4\n\n(cid:11)\n\np\n\n8\n\n3\n\n\u221a\n\u221a\nj \u03b5\n\u03b4\n\n8\n\n\u2264 4\np\n\u2264 \u03b4\u03b7.\n\n3\n\nthe proof is complete.\nlemma 35.5 for each \u03b5, \u03b7 > 0 there exist n0 and \u03b4 such that if n \u2265 n0,\n\n(\u03b4) \u2265 \u03b5) \u2264 2\u03b7.\n\np(\u03c9zn\n\nproof we will take \u03b4 = 1/k for some large k. if |t \u2212 s| \u2264 1/k, then either both s, t are\nin the same interval [(i \u2212 1)/k, i/k] or they are in adjoining intervals. thus they both lie in\nsome interval of the form [(i \u2212 2)/k, i/k]. since\n\n|zn(t ) \u2212 zn(s)| \u2264 |zn(t ) \u2212 zn((i \u2212 2)/k )| + |zn(s) \u2212 zn((i \u2212 2)/k )|,\n\n "}, {"Page_number": 290, "text": "272\nthen using lemma 35.4 with \u03b4 = 2/k\n\napplications of weak convergence\n\np(\u2203s, t \u2208 [0, 1] : |zn(t ) \u2212 zn(s)| \u2265 \u03b5,|t \u2212 s| < \u03b4)\n\nsup\n\n\u2264 p(\u2203i \u2264 k :\n(i\u22122)/k\u2264s\u2264i/k\n\u2264 k sup\nsup\n\u2264 k\u03b7(2/k ) = 2\u03b7,\n\n(i\u22122)/k\u2264s\u2264i/k\n\np(\n\ni\n\n|zn(s) \u2212 z(i\u22122)/k| \u2265 \u03b5/2)\n\n|zn(s) \u2212 z(i\u22122)/k| \u2265 \u03b5/2)\n\nwhich proves the lemma.\n\nwe can now prove the donsker invariance principle.\n\nproof of theorem 35.1 by lemma 35.5, theorem 32.1, and the fact that zn(0) = 0 for all n,\nthe laws of the zn are tight. therefore by prohorov\u2019s theorem (theorem 30.4), every sub-\nsequence has a further subsequence which converges weakly with respect to the topology\non c[0, 1]. we therefore only need to show that every subsequential limit point of the zn\nwith respect to weak convergence is a brownian motion. since our processes lie in c[0, 1],\nthe paths of any subsequential limit point are continuous, so it suffices by theorem 2.6 to\nshow that the finite-dimensional distributions of zn converge weakly to the corresponding\nfinite-dimensional distributions of a brownian motion w . we will show the one-dimensional\ndistributions converge, and leave the analogous argument for the higher-dimensional distri-\nbutions to the reader.\n\nwe have\n\n|yi|/\n\np(max\ni\u2264n\n\n\u221a\nn \u2265 \u03b5) \u2264 np(|y1| \u2265 \u221a\n\nn\u03b5) \u2264 np(|y1|2/\u03b52 \u2265 n).\n\n(35.3)\n\nfor any integrable non-negative random variable x ,\n\nnp(x \u2265 n) = e [n; x \u2265 n] \u2264 e [x ; x \u2265 n],\n\nwhich tends to zero by dominated convergence. therefore\n\u221a\nn \u2265 \u03b5) \u2192 0.\n\n|yi|/\n\np(max\ni\u2264n\n\nfix t \u2208 [0, 1]. by the central limit theorem, s[nt]/\n\n\u221a\n\u221a\n[nt] converges weakly on r to a mean\nzero normal random variable with variance one, and by exercise 30.3, we see that s[nt]/\nn\nconverges weakly to a mean zero normal random variable with variance t. from the preceding\nparagraph we conclude that for each t, |zn(t ) \u2212 s[nt]/\nn| converges to zero in probability.\n\u221a\nby exercise 30.2, zn(t ) has the same weak limit as s[nt]/\nn, namely, a mean zero normal\nrandom variable with variance t, which is the distribution of wt.\n\n\u221a\n\n(35.4)\n\nthere is an elegant proof of the donsker invariance principle using skorokhod embedding.\nunlike the proof above, however, this second proof does not extend to random variables taking\nvalues in rd.\n\nby theorem 15.6 we can find a brownian motion w and a random walk sn such that\n\n|si \u2212 wi|\u221a\n\nn\n\nsup\ni\u2264n\n\n\u2192 0\n\n "}, {"Page_number": 291, "text": "in probability. by the continuity of paths of w ,\n\n35.2 brownian bridge\n\n273\n\np(\n\nsup\n\n|t\u2212s|\u22641/n,s,t\u22641\n\n|wt \u2212 ws| > \u03b5) \u2192 0.\n\n\u221a\n\nif we let w n(t ) = wnt /\nprobability as n \u2192 \u221e and also, because wn is again a brownian motion,\n\nn, we then have that supi\u2264n\n\n|zn(i/n) \u2212 wn(i/n)| tends to zero in\n\nwe conclude that\n\np(\n\nsup\n\n|t\u2212s|\u22641/n,s,t\u22641\n\n|wn(t ) \u2212 wn(s)| > \u03b5) \u2192 0.\n\n|zn(t ) \u2212 wn(t )| \u2192 0.\n\nsup\nt\u22641\n\nrelies on skorokhod embedding.\n\nthe law of wn is that of a brownian motion and does not depend on n. by exercise 30.2 we\nobtain that zn converges weakly to the law of a brownian motion.\n\nif the above proof seems too simple, remember that we used theorem 15.6, which in turn\n\nn; these are the normalized\npartial sums without the linear interpolation. rather than being continuous and piecewise\n\none might ask about the weak convergence of(cid:14)zn(t ) = s[nt]/\nlinear like the zn(t ), the(cid:14)zn(t ) are piecewise constant and have jumps.\nproposition 35.6 suppose the yi are independent with mean zero and variance one. the(cid:14)zn\n\n\u221a\n\nconverge weakly with respect to the topology of d[0, 1] to brownian motion.\n\nproof the zn converge weakly with respect to the topology of c[0, 1] to a brownian\nmotion. by the skorokhod representation (theorem 31.2), we can find a probability space\n(cid:3)\nand random variables z\nn having the same law as zn that converge almost surely with respect\n(cid:3)\nto the supremum norm. therefore the z\nn converge almost surely with respect to the metric of\nd[0, 1], and hence the zn converge weakly to a brownian motion with respect to the topology\nof d[0, 1]. if we show that supt\u22641\nresult will follow by exercise 30.2.\n\u221a\nn\u03b5 in absolute value. but by (35.4), the probability of this tends to zero as n \u2192 \u221e.\n\n|zn(t ) \u2212(cid:14)zn(t )| converges to zero in probability, then our\nnow zn(t ) and(cid:14)zn(t ) will differ by more than \u03b5 for some t only if some yi is larger than\n\na brownian bridge w 0\nt\n\n35.2 brownian bridge\n\nis the process defined by\n= wt \u2212 tw1,\n\nw 0\nt\n\n0 \u2264 t \u2264 1,\n\nwhere w is a brownian motion. w 0 has continuous paths, is jointly normal, is zero at time\n0 and at time 1, has mean zero, and we calculate its covariance by\n\ncov (w 0\ns\n\n, w 0\nt\n\n) = cov (ws, wt ) \u2212 s cov (w1, wt ) \u2212 t cov (ws, w1) + stvar (w1)\n= s \u2227 t \u2212 st,\n\nrecalling (2.1).\n\n "}, {"Page_number": 292, "text": "274\n\napplications of weak convergence\n\na brownian bridge can be characterized as a brownian motion conditioned to be zero at\ntime 1. to make this precise, let w be a brownian motion started at zero under p, and for a\na borel subset of c[0, 1], define\n\np\u03b5 (a) = p(w \u2208 a | |w1| \u2264 \u03b5);\n\ncf. (a.13). set p0(a) = p(w 0 \u2208 a), the law of w 0.\nproposition 35.7 p\u03b5 converges weakly to p0 with respect to the topology of c[0, 1] as \u03b5 \u2192 0.\nproof since w is a jointly normal process and\n\ncov (wt \u2212 tw1, w1) = cov (wt , w1) \u2212 tvar (w1) = 0,\n= wt \u2212 tw1 and the random variable w1 are independent by proposition\nthen the process w 0\na.55. let f be any closed subset of c[0, 1] and let f\u03b4 = {g \u2208 c[0, 1] : d (g, f ) < \u03b4}, where\nt\nd (g, f ) = inf{d (g, f ) : f \u2208 f} and d here is the supremum norm. note supt\u22641\n| \u2264 \u03b5\non the event {|w1| \u2264 \u03b5}. if \u03b4 > \u03b5,\n\n|wt\u2212w 0\n\nt\n\np\u03b5 (f ) = p(w \u2208 f | |w1| \u2264 \u03b5) \u2264 p(w 0 \u2208 f\u03b4 | |w1| \u2264 \u03b5)\n\n= p(w 0 \u2208 f\u03b4 ) = p0(f\u03b4 ).\n\nthus lim sup\u03b5\u21920\nlim sup p\u03b5 (f ) \u2264 p0(f ). an application of theorem 30.2 completes the proof.\n\np\u03b5 (f ) \u2264 p0(f\u03b4 ). since f is closed, p0(f\u03b4 ) \u2192 p0(f ) as \u03b4 \u2192 0, so\n\nwe show that a brownian bridge can also be represented as the solution x of the stochastic\n\ndifferential equation\n\ndxt = dwt \u2212 xt\n1 \u2212 t\n\ndt,\n\nx0 = 0,\n\n(35.5)\n\nwhere w is a brownian motion. this is plausible: x behaves much like a brownian motion\nuntil t is close to 1, when there is a strong push toward the origin. the existence and\nuniqueness theory of chapter 24 shows uniqueness and existence for the solution of (35.5)\nfor s \u2264 t for any t < 1; see exercise 24.4. we can solve (35.5) explicitly. we have\n\ndwt = dxt + xt\n1 \u2212 t\n\nor\n\n(cid:17)\n\n,\n\n(cid:16)\n\nxt\n1 \u2212 t\n\ndt = (1 \u2212 t ) d\n(cid:3)\n\nt\n\n0\n\ndws\n1 \u2212 s\n\n.\n\nxt = (1 \u2212 t )\n(cid:3)\n\n(1 \u2212 t )2\n\nt\n\n(1 \u2212 s)\u22122 ds = t \u2212 t2,\n\nthus xt is a continuous gaussian process with mean zero. the variance of xt is\n\n0\n\nthe same as the variance of a brownian bridge. a similar calculation shows that the covariance\nof xt and xs is the same as the covariance of wt \u2212 tw1 and ws \u2212 sw1; see exercise 24.6.\nhence the finite-dimensional distributions of xt and a brownian bridge are the same. we\nnow appeal to theorem 2.6.\n\n "}, {"Page_number": 293, "text": "35.3 empirical processes\n\n275\n\n35.3 empirical processes\n\nin this section we will consider empirical processes, which are useful in statistics in estimating\ndistribution functions. let xi, i = 1, . . . , n, be i.i.d. random variables that are uniformly\ndistributed on the interval [0, 1]. define the empirical process\n\nn(cid:9)\n\nfn(t ) = 1\nn\n\n1[0,t](xi).\n\ni=1\n\n(35.6)\n\nthe glivenko\u2013cantelli theorem (theorem a.40) says that\n\nour goal in this section is to obtain the corresponding weak limit theorem. let\n\na.s.\n\nsup\nt\u2208[0,1]\n\n|fn(t ) \u2212 t| \u2192 0,\nn(cid:9)\n\nn(fn(t ) \u2212 t ) = 1\u221a\nn\n\n=1\n\nzn(t ) = \u221a\n\n(1[0,t](xi) \u2212 t ).\n\n(35.7)\n\nwe will show that zn converges weakly with respect to d[0, 1] to a brownian bridge.\n\nlet\n\n(\u03b4) =\n\n\u03c9zn\n\nsup\n\ns,t\u2208[0,1],|t\u2212s|<\u03b4\n\n|zn(t ) \u2212 zn(s)|.\n\nthe paths of zn are not continuous: they have a jump of size 1/n at every time xi. thus \u03c9zn\ndoes not tend to zero as \u03b4 \u2192 0. nevertheless we can get reasonable estimates on \u03c9zn\n(\u03b4).\nwe need an elementary lemma on binomial random variables, the proof of which is\n\n(\u03b4)\n\nexercise 35.1.\n\nlemma 35.8 suppose sn is a binomial random variable with parameters n and p. then\nthere exists a constant c not depending on n or p such that\ne|sn \u2212 e sn|4 \u2264 cnp + cn2 p2\n\n(35.8)\n\nand\n\nproposition 35.9 let \u03b5, \u03b7 > 0. there exists \u03b4 and n0 such that if n \u2265 n0, then\n\n(35.9)\n\ne|sn|4 \u2264 cnp + cn4 p4.\n\n(\u03b4) > \u03b5) \u2264 \u03b7.\n\np(\u03c9zn\n\nthe idea of the proof is to use corollary 8.4 to estimate zn(t )\u2212 zn(s) when |t \u2212 s| is small\n\nand use estimates on binomials when |t \u2212 s| is large.\nproof let \u03b5, \u03b7 > 0. we will choose n0, \u03b4 later. assuming that they have been chosen,\nsuppose n \u2265 n0 and choose k such that n \u2264 2k < 2n. if t \u2208 [0, 1], let t (k) be the largest\n\u2212k less than or equal to t and similarly define s(k). let dk = {i/2k : 0 \u2264 i \u2264 2k}.\nmultiple of 2\nwe will show there exists \u03b4 > 0 such that\n\np(\n\nsup\n\ns,t\u2208dk ,|t\u2212s|<2\u03b4\n\n|zn(t ) \u2212 zn(s)| > \u03b5/3) < \u03b7/3\n\n(35.10)\n\n "}, {"Page_number": 294, "text": "276\n\nand\n\napplications of weak convergence\n\n|zn(s) \u2212 zn(s(k))| > \u03b5/3) < \u03b7/3.\n\np( sup\ns\u2208[0,1]\n\n(35.11)\nstep 1. we first prove (35.10) by using corollary 8.4. suppose s, t \u2208 dk with |t \u2212 s| < 2\u03b4.\nthen either s = t, in which case zn(t ) \u2212 zn(s) = 0, or else |t \u2212 s| \u2265 2\n\u2212k \u2265 1/(2n). take\np = t \u2212 s and note that 1(s,t](xi) is a bernoulli random variable with parameter p. using\n(35.7) and lemma 35.8,\n\n(cid:10)\ne|zn(t ) \u2212 zn(s)|4 \u2264 c\nn2\n= c\n\n(cid:11)\n(np + n2 p2)\n+ p2\np\nn\n\n\u2264 c|t \u2212 s|2,\n\nwhere in the last line we used 1/n \u2264 2|t \u2212 s|. by corollary 8.4,\n\np(\n\nsup\n\ns,t\u2208dk ,|t\u2212s|<2\u03b4\n\n|zn(t ) \u2212 zn(s)| > \u03b5/3) \u2264 p\n\n(cid:10)\n\u2264 c(\u03b5/\u03b41/8)\u22124 = c\u03b41/2/\u03b54.\n\ns,t\u2208dk ,|t\u2212s|<2\u03b4\n\nsup\n\n|zn(t ) \u2212 zn(s)|\n\n|t \u2212 s|1/8\n\n(cid:11)\n\n> c\n\n\u03b5\n\n\u03b41/8\n\nwe choose \u03b4 small enough so that the last term is less than \u03b7/3.\nstep 2. we now prove (35.11). let\n\ntn(t ) = n(cid:9)\n\ni=1\n\n1[0,t](xi).\n\n(cid:11)\n\nobserve that tn(t ) is nondecreasing in t. if there exists s \u2208 [0, 1] such that tn(s)\u2212tn(s(k)) >\n\u221a\n\u221a\nn/3, then there exists j \u2264 2k\u22121 such that tn(( j+1)/2k )\u2212tn( j/2k ) > \u03b5\nn/3. therefore,\n\n\u03b5\nusing (35.9),\n\n(cid:10)\n\np\n\ntn(s) \u2212 tn(s(k))\n\n\u221a\n\n\u221a\n\nn\n\n> \u03b5/3\n\nsup\ns\u2208[0,1]\n\u2264 p(\u2203 j \u2264 2k \u2212 1 : tn(( j + 1)/2k ) \u2212 tn( j/2k ) > \u03b5\n\u2264 2k sup\nn/3)\nj\u22642k\u22121\n\u2264 c2k sup j\n\u2264 c2k n2\n\np(tn(( j + 1)/2k ) \u2212 tn( j/2k ) > \u03b5\ne|tn(( j + 1)/2k ) \u2212 tn( j/2k )|4\n\u2212k + (n2\n\u03b54n2\n\n\u2212k )4\n\n\u03b54n2\n\n\u221a\n\n.\n\nn/3)\n\nsince n2\n\n\u2212k \u2264 2, the last line is less than or equal to\n\nc2kn2\n\n\u2212k/\u03b54n2 = c1/\u03b54n.\n\nwe choose n0 > 1/\u03b4 large enough so that if n \u2265 n0, then c1/\u03b54n is less than \u03b7/3.\n\nalso,\n\ne [tn(s) \u2212 tn(s(k)] \u2264 n(s \u2212 s(k)) \u2264 n2\n\n\u2212k \u2264 2\n\n "}, {"Page_number": 295, "text": "\u221a\n\nwill be less than \u03b5\nsince\n\n35.3 empirical processes\n\n277\nn/3 if n \u2265 36/\u03b52 and we choose n0 larger if necessary so that n0 > 36/\u03b52.\nzn(t ) \u2212 zn(s) = tn(t ) \u2212 tn(s)\n\n\u2212 e [tn(t ) \u2212 tn(s)]\n\n,\n\n\u221a\nn\n\n\u221a\nn\n\n(35.11) follows.\nstep 3. now that we have (35.10) and (35.11), we write\n\n|zn(t ) \u2212 zn(s)| \u2264 |zn(t ) \u2212 zn(t (k))| + |zn(t (k)) \u2212 zn(s(k))| + |zn(s(k)) \u2212 zn(s)|.\n\n\u2212k \u2264 \u03b4 + 1/n. provided n \u2265 n0 > 1/\u03b4, combining\n\nif |t \u2212 s| < \u03b4, then |t (k) \u2212 s(k)| \u2264 \u03b4 + 2\n(35.10) and (35.11) gives\n\np(\n\nsup\n\ns,t\u2208[0,1],|t\u2212s|<\u03b4\n\nas required.\n\n|zn(t ) \u2212 zn(s)| > \u03b5) < \u03b7\n\ntheorem 35.10 the zn converge weakly to a brownian bridge with respect to the topology\nof d[0, 1].\nproof we smooth zn to get a continuous process vn. set zn(t ) = zn(1) for t \u2208 [1, 2] and\nset\n\nwe have\n\n|vn(t2) \u2212 vn(t1)| \u2264 n\n\u2264 n\n\n\u22121\nn\n\nzn(u + t ) du.\n\n0\n\n|zn(t2 + u) \u2212 zn(t1 + u)| du\n\n(|t2 \u2212 t1|) du = \u03c9zn\n\n(|t2 \u2212 t1|).\n\n\u03c9zn\n\n(cid:3)\n\nvn(t ) = n\n(cid:3)\n(cid:3)\n\n0\n\n\u22121\nn\n\n\u22121\nn\n\n0\n\n(cid:3)\n\n\u22121\nn\n\nnote also that by (35.8) with p = t \u2212 s and using jensen\u2019s inequality with the measure\nn1[0,n\u22121](u) du,\n\ne|vn(0)|4 \u2264 n\n\ne|zn(u)|4 du \u2264 c.\n\nhence\n\n0\n\np(|vn(0)| \u2265 a) \u2264 e|vn(0)|4\n\na4\n\n\u2264 c\na4\n\n.\n\ntherefore by theorem 8.1, the vn are tight with respect to weak convergence on c[0, 1]. if\nthe vn j converges weakly (with respect to c[0, 1]), by the skorokhod representation we may\n(cid:3)\nfind v\nn j will also converge\nalmost surely in the space d[0, 1]. this proves that the vn are tight in d[0, 1] by exercise\n30.10.\n\n(cid:3)\nn j with the same law as vn j that converge almost surely. then the v\n\ngiven \u03b5 and \u03b7, choose \u03b4 and n0 such that p(\u03c9zn\n\n(\u03b4) > \u03b5) < \u03b7 if n \u2265 n0. we have\n\n(cid:3)\n\n|vn(t ) \u2212 zn(t )| \u2264 n\n\n\u22121\nn\n\n0\n\n|zn(u + t ) \u2212 zn(t )| du \u2264 \u03c9zn\n\n\u22121).\n\n(n\n\n "}, {"Page_number": 296, "text": "278\n\nif n is large enough so that n\n\napplications of weak convergence\n\u22121 < \u03b4, then\n\n|vn(t ) \u2212 zn(t )| > \u03b5) \u2264 p(\u03c9zn\n\n\u22121) > \u03b5) \u2264 p(\u03c9zn\n(n\n\n(\u03b4) > \u03b5) < \u03b7.\n\np(sup\n\nt\n\ntherefore vn \u2212 zn converges to 0 in probability, and by exercise 30.2 the subsequential limit\npoints of vn are the same as those of zn.\n\nit remains to show that any subsequential limit point of the zn is a brownian bridge.\ntheorem for multinomials (see\n\nlimit\n\nthis follows from the multidimensional central\nremark a.57) and is left as exercise 35.2.\n\n35.1 prove lemma 35.8.\n\nexercises\n\n35.2 prove that the finite-dimensional distributions of zn in theorem 35.10 converge to those of a\n\nbrownian bridge.\n\nis a brownian bridge, prove that yt = w 0\n\n35.3 if w 0\nt\n35.4 let t0 < 1. the sde (35.5) has a unique solution when x0 = 0 is replaced by x0 = x. let px\nbe the law of the solution when x0 = x and let zt be the canonical process. show that (zt , px )\nis not a markov process.\n\n1\u2212t is also a brownian bridge.\n\n35.5 let nt (a) be a poisson point process with respect to the measure space (s, m) and let as, s > 0,\n\nbe an increasing sequence of subsets of s with m(as ) \u2192 \u221e as s \u2192 \u221e. does\n\nnt (as ) \u2212 m(as )\n\n\u221a\nm(as )\n\nconverge weakly with respect to d[0, 1] as s \u2192 \u221e? what is the limit?\n\nthis can be applied to get central limit theorems for the number of downcrossings of a\n\nbrownian motion, for example.\n\n35.6 this exercise asks you to prove that the poisson process conditioned to be equal to n at time 1\nhas the same law as n times the empirical process. here is the precise statement. suppose pt is\na poisson process with parameter \u03bb > 0. let q be the law of {pt , t \u2208 [0, 1]} conditioned so that\np1 = n. thus q is a probability on d[0, 1] with\n\nq(p \u2208 a) = p(p \u2208 a | p1 = n).\n\nsince (p1 = n) is an event with positive probability, there is no difficulty defining these\nconditional probabilities. prove that q is also the law of {nfn(t ), t \u2208 [0, 1]}, where fn is defined\nin section 35.3.\n\n "}, {"Page_number": 297, "text": "36\n\nsemigroups\n\nin this chapter we suppose we have a semigroup of positive contraction operators {pt},\nand we show how to construct a markov process x corresponding to this semigroup. in\nchapters 37 and 38, we will show how such semigroups might arise.\nwe suppose that we have a state space s that is a separable locally compact metric space\ns. let c0 be the set of continuous functions on s that vanish at infinity. recall that f \u2208 c0\nif f is continuous, and given \u03b5, there exists a compact set k depending on \u03b5 and f such that\n\n| f (x)| < \u03b5,\n\nx /\u2208 k.\n\nwe use the usual supremum norm on c0. we assume we have a semigroup {pt} of positive\ncontractions mapping c0 to c0. more precisely, we assume\nassumption 36.1 there exists a family {pt}, t \u2265 0, of operators on c0 such that\n\n(1) if f \u2208 c0, then\n\npt (ps f )(x) = pt+s f (x),\n\nx \u2208 s,\n\ns, t \u2265 0.\n\n(2) if f (x) \u2265 0 for all x and if t \u2265 0, then pt f (x) \u2265 0 for all x.\n(3) for all t, (cid:21)pt f (cid:21) \u2264 (cid:21) f (cid:21).\n(4) if f \u2208 c0, then pt f \u2192 f uniformly as t \u2192 0.\nour goal in this section is to construct a process x corresponding to the semigroup pt.\n\nthe steps we use are the following.\n\n(1) we temporarily assume each pt maps the function 1 into itself. we define xt for t in\n\nthe dyadic rationals and define px using the kolmogorov extension theorem.\n\n(2) we verify a preliminary version of the markov property.\n(3) we use the regularity theorem for supermartingales to show that x has left and right\n\nlimits along the dyadic rationals, and then define xt for all t.\n\n(4) we verify that our process (xt , px) corresponds to the semigroup pt.\n(5) we remove the assumption that pt1 = 1.\n\n36.1 constructing the process\n\nlet us assume the following for now. we will remove this assumption at the end of this\nsection.\nassumption 36.2 pt1(x) = 1 for all x and all t \u2265 0.\n\n279\n\n "}, {"Page_number": 298, "text": "280\n\nsemigroups\n\nwe now begin the construction of (xt , px).\nstep 1. let dn = {k/2n : k \u2265 0} and let d = \u222andn, the dyadic rationals. let \u0001 be the set of\nfunctions from d to s. define\n\nxt (\u03c9) = \u03c9(t ),\n\nt \u2208 d, \u03c9 \u2208 \u0001.\n\nwe let f be the \u03c3 -field on \u0001 generated by the collection of cylindrical subsets of \u0001.\n\nby the riesz representation theorem (see rudin (1987)), for each t > 0 there exists a\n\nmeasure pt (x, dy) such that\n\npt f (x) =\n\n(cid:3)\n\nf (y) pt (x, dy),\n\nf \u2208 c0.\n\n(36.1)\n\n(the riesz representation theorem is most often phrased for continuous functions on compact\nspaces; since we are working with c0, we can let the state space satisfy slightly weaker\nhypotheses; see folland (1999), p. 223.) we can use (36.1) to define pt f for all bounded\nborel measurable functions f . since pt maps c0 to c0, and continuous functions are borel\nmeasurable, a limit argument shows that pt f is borel measurable whenever f is bounded\nand borel measurable.\nour main task in this step is to define px. d is countable and we fix a labeling d =\n{t1, t2, . . .}. let en = {t1, . . . , tn}. let s1 \u2264 \u00b7\u00b7\u00b7 \u2264 sn be the ordering of en according to the\nusual ordering of the reals, so that s1 is the smallest element of the set {t1, . . . , tn}, s2 is the\nnext smallest, and so on. define\n\n(cid:3)\n\n(cid:3)\n\npx\nn\n\n(xs1\n=\n\n\u2208 a1, . . . , xsn\n\u00b7\u00b7\u00b7\nps1\n\n\u2208 an)\n(x, dx1)ps2\u2212s1\n\nan\n\na1\n\n(x1, dx2)\u00b7\u00b7\u00b7 psn\u2212sn\u22121\n\n(xn\u22121, dxn)\n\n(36.2)\n\nfor a1, . . . , an borel subsets of s. the px\nn are consistent in the sense of appendix d. the\nkey to checking this is to observe that if s1, . . . , sn is the ordering of en and we temporarily\nwrite s1, . . . , si, s, si+1, . . . , sn for the ordering of en+1, then\n(xi\u22121, dx)psi+1\u2212s(x, dxi) = psi+1\u2212si\n\n(xi\u22121, dxi)\n\nps\u2212si\n\n(cid:3)\n\ns\n\nby the semigroup property; cf. (19.10).\n\nby the kolmogorov extension theorem (theorem d.1), for each x there exists a probability\n\npx such that\n\n\u2208 a1, . . . , xtn\n\n\u2208 an) = px\n\npx(xt1\n\n(xt1\nfor each n whenever a1, . . . , an are borel subsets of s.\nif e x is the expectation corresponding to px,( 36.2) can be rewritten as\n\nn\n\n\u2208 a1, . . . , xtn\n\n\u2208 an)\n\n(cid:3)\n(cid:3)\n)\u00b7\u00b7\u00b7 fn(xsn\ne x[ f1(xs1\n\u00b7\u00b7\u00b7\n\u00d7 psn\u2212sn\u22121\n\n=\n\nf1(x1)\u00b7\u00b7\u00b7 fn(xn)ps1\n(xn\u22121, dxn)\n\n)]\n\n(36.3)\n\n(x, dx1)ps2\u2212s1\n\n(x1, dx2)\u00b7\u00b7\u00b7\n\n "}, {"Page_number": 299, "text": "(cid:3)\n\na\n\n36.1 constructing the process\n\n281\n\nwhen fi = 1ai for each i. to see this, by linearity we have (36.3) when the functions fi are\nsimple functions; by a limit argument we have (36.3) when the fi are borel measurable and\nnon-negative, and by linearity, (36.3) holds when the fi are bounded and borel measurable.\n\nby (36.2) we have\n\npx(xt \u2208 a) = e 1a(xt ) =\n\npt (x, dy) = pt1a(x).\n\nusing linearity and a limit argument, we have e x f (xt ) = pt f (x) when f is bounded and\nborel measurable.\nproposition 36.3 if f is bounded and borel measurable, s, t > 0, and x \u2208 s, then\n\n(cid:16)\n\n(cid:17)\n\ne x\n\ne xt f (xs)\n\n(36.4)\nproof the proof of (36.4) is mainly a matter of sorting out notation. let \u03d5(x) = e x f (xs) =\nps f (x). hence e xt f (xs) = \u03d5(xt ) = ps f (xt ). then the left-hand side is e x(ps f )(xt ) =\npt (ps f )(x). the right-hand side of (36.4) is ps+t f (x), and so the two sides agree by the\nsemigroup property.\nstep 2. we so far only have xt constructed for t \u2208 d. to extend the definition to all t, we\nwant to let xt = limu>t,u\u2208d,u\u2192t xu. but before we can make that definition, we need to know\nthat the limits exist. we will use the regularity of supermartingales to show this, so we need\nto look at conditional expectations. let\n\n= e x f (xs+t ).\n\nf(cid:3)\n\ns\n\n= \u03c3 (xr; r \u2264 s, r \u2208 d).\n\nproposition 36.4 if s < t with s, t \u2208 d and f is bounded and borel measurable, then\n\ne x[ f (xt ) | f(cid:3)\n\ns] = e xs f (xt\u2212s),\n\n(36.5)\nproof take n \u2265 1, r1 \u2264 r2 \u2264 \u00b7\u00b7\u00b7 \u2264 rn \u2264 s with each r j in d, and a1, . . . , an borel subsets\nof s. it suffices to show that\n)\u00b7\u00b7\u00b7 1an\n(xr1\n(xrn\n\u2208 a1, . . . , xrn\n\ns. the right-hand side of (36.6) is equal\n\n)\u00b7\u00b7\u00b7 1an\n\n(36.6)\n\n(xrn\n\n)],\n\npx-a.s.\n\ne x[ f (xt )1a1\nsince the events (xr1\nto\n\n)].\n\n(xr1\n\n(xr1\n\n(xrn\n\n)\u00b7\u00b7\u00b7 1an\n\n)] = e x[(e xs f (xt\u2212s))1a1\n\u2208 an) generate f(cid:3)\n(cid:3)\n(cid:3)\n\u00b7\u00b7\u00b7\n(x, dx1)\u00b7\u00b7\u00b7 prn\u2212rn\u22121\n\u00d7 pr1\n(cid:3) \u221e\n\npt\u2212s f (y)1a1\n\n)] =\n\ne x[pt\u2212s f (xs)1a1\n\n)\u00b7\u00b7\u00b7 1an\n\n(xr1\n\n(xrn\n\nfrom (36.3)\n\ne x[pt\u2212s f (xs)1a1\n\nbut pt\u2212s f (y) = (cid:15)\n\nobtain the left-hand side of (36.6).\n\nstep 3. we define r\u03bb, the resolvent or \u03bb-resolvent of pt, by\n\u2212\u03bbtpt f (x) dt.\ne\n\nr\u03bb f (x) =\n\n0\n\n(36.7)\n\n(xn)\n\n(36.8)\n\n(x1)\u00b7\u00b7\u00b7 1an\n\n(xn\u22121, xn)ps\u2212rn\n\n(xn, dy).\n\nf (z)pt\u2212s(y, dz). substituting this in (36.8) and using (36.3) again, we\n\n(36.9)\n\n "}, {"Page_number": 300, "text": "282\n\nsemigroups\n\nlemma 36.5 if f \u2265 0 is bounded and borel measurable and x \u2208 s, then mt = e\nt \u2208 d, is a supermartingale with respect to the filtration {f(cid:3)\nmeasure px.\nproof what we need to show is that if s < t \u2208 d, then\n\n\u2212\u03bbtr\u03bb f (xt ),\n; t \u2208 d} and the probability\n\nt\n\n\u2212\u03bbtr\u03bb f (xt ) | f(cid:3)\nby proposition 36.3 the left-hand side is\n\ne x[e\n\ns] \u2264 e\n\n\u2212\u03bbsr\u03bb f (xs),\n\npx-a.s.\n\nso what we need to show is that\n\n\u2212\u03bbt e xsr\u03bb f (xt\u2212s),\ne\n\ne yr\u03bb f (xt\u2212s) \u2264 e\u03bb(t\u2212s)r\u03bb f (y)\n\n(36.10)\n\nfor all y. the left-hand side of (36.10) is\npt\u2212sr\u03bb f (y) =\n\u2212\u03bbrpt\u2212spr f (y) dr\ne\n=\n\u2212\u03bbrpr+t\u2212s f (y) dr\ne\n= e\u03bb(t\u2212s)\n\u2212\u03bbrpr f (y) dr\ne\n\u2264 e\u03bb(t\u2212s)\n\u2212\u03bbrpr f (y) dr\ne\n= e\u03bb(t\u2212s)r\u03bb f (y).\n\n(cid:3) \u221e\n(cid:3) \u221e\n\nt\u2212s\n\n0\n\n0\n\nthe first equality is the fubini theorem, the second the semigroup property, and the third\nequality comes from a change of variables.\n\nthat e\n\nnext, if f is non-negative and bounded, by theorem 3.12 with p replaced by px, we see\n\u2212\u03bbtr\u03bb f (xt ) has left and right limits along t \u2208 d. therefore the same is true for r\u03bb f (xt ).\nby assumption 36.1 and dominated convergence, we have that if f \u2208 c0,\n\n\u03bbr\u03bb f (x) \u2212 f (x) =\n=\n\n\u2212\u03bbt (pt f (x) \u2212 f (x)) dt\ne\n\u2212t (pt/\u03bb f (x) \u2212 f (x)) dt\ne\n\ntends to zero uniformly in x as \u03bb \u2192 0. take a countable dense subset { fi} of c0 and look at\njr j fi(xt ) for all positive integers j. since jr j fi(xt ) has left and right limits along d, a.s.,\nletting j \u2192 \u221e, we see that fi(xt ) does also. we conclude that xt has left and right limits\nalong d.\nnow define xt = limu>t,u\u2208d,u\u2192t xu. then xu is right continuous with left limits. we check\n\nthat\n\npx(xt1\n\n\u2208 a1, . . . , xtn\n\n\u2208 an) =\n\n\u00b7\u00b7\u00b7\n\n(x, dx1)\u00b7\u00b7\u00b7 ptn\u2212tn\u22121\n\npt1\n\n(xn\u22121, dxn).\n\n(cid:3)\n\n(cid:3)\n\na1\n\nan\n\n(cid:3) \u221e\n(cid:3) \u221e\n\n0\n\n(cid:3) \u221e\n(cid:3) \u221e\n\n0\n\n0\n\n "}, {"Page_number": 301, "text": "to see this, we know this holds when the ti are in d. by linearity and a limit argument, we\nconclude\n\n(cid:3)\n\n(cid:3)\n\n36.2 examples\n\n283\n\ne x[ f1(xt1\n\n)\u00b7\u00b7\u00b7 fn(xtn\n\n)] =\n\n\u00b7\u00b7\u00b7\n\nf (x1)\u00b7\u00b7\u00b7 f (xn)pt1\n\n(x, dx1)\u00b7\u00b7\u00b7 ptn\u2212tn\u22121\n\n(xn\u22121, dxn)\n\n(36.11)\n\nwhen the fi are bounded and continuous. using a limit argument, we know (36.11) holds\nwhen the ti are arbitrary non-negative real numbers. using a limit argument again, (36.11)\nholds for all bounded and measurable f , in particular, when fi = 1ai.\nstep 4. it remains to show that (xt , px) satisfies definition 19.1 and that pt is the semigroup\nof this process. let f 00\n= \u03c3 (xs; s \u2264 t ). then we have already shown that (xt , px) is a\nmarkov process with respect to the filtration {f 00\npx(xs+t \u2208 a | f 00\n\n}, except for showing that\n\n) = pxs (xt \u2208 a).\n\nt\n\nt\n\ns\n\nhowever, this can be proved almost identically to the way we proved proposition 36.4.\nstep 5. sometimes the semigroup is a contraction semigroup and satisfies assumption 36.1\nbut not assumption 36.2. in this case the pt (x, a) are called sub-markov transition probability\nkernels. the missing probability is due to the process being killed, and we can handle this\nsituation as follows. let s\u0001 = s \u222a {\u0001}, where we introduce an isolated point {\u0001}. the\ntopology on s\u0001 is the one generated by the open sets on s together with the set {\u0001}. given\na function f on s, we extend it to s\u0001 by setting f (\u0001) = 0. we replace pt (x, a) by pt (x, a),\nwhere\n\nx \u2208 s, a \u2282 s,\nx \u2208 s,\n\n(36.12)\n\n\u23a7\u23aa\u23a8\u23aa\u23a9pt (x, a) = pt (x, a),\n\npt (x,{\u0001}) = 1 \u2212 pt (x,s ),\npt (\u0001,{\u0001}) = 1.\n\none can go through the above construction with pt and obtain a strong markov process xt\nwhose state space is s\u0001. it is not hard to show that starting at \u0001, the process stays at \u0001\nforever; see exercise 36.1.\nwe remark that by the results of chapter 20 and also exercise 20.1, we can expand the\n} to {ft}, where {ft} is right continuous and each ft contains all the sets\nfiltration from {f 00\nthat are null with respect to each px. in addition, the strong markov property will hold for\n(xt , px).\n\nt\n\n36.2 examples\n\nexample 36.6 our first example is a brownian motion. let\n\u2212|x\u2212y|2/2t ,\n\np(t, x, y) = (2\u03c0t )d/2e\n\nand set\n\nwe know\n\n(cid:3)\n\npt (x, a) =\n\np(t, x, y) dy.\n\na\n\np(t, x, z)p(s, z, y) dz = p(t + s, x, y)\n\n(cid:3)\n\n "}, {"Page_number": 302, "text": "284\n\nsemigroups\n\nby proposition 19.5, and so pt satisfies the semigroup property. we showed in section 19.4\nthat assumption 36.1 is satisfied, except for the fact that pt maps c0 to c0; this is exercise\n36.2. therefore we have a strong markov process associated with pt. by proposition 21.5,\nthe paths of the strong markov process can be taken to be continuous. this gives yet another\nconstruction of a brownian motion.\n\nexample 36.7 we now use the machinery we have developed in this chapter to construct\nthe poisson process. define transition probabilities by\n\n\u221e(cid:9)\n\npt (x, a) = e\n\n\u2212\u03bbt\n\nk=0\n\u221e(cid:9)\nwhere \u03bb is some fixed parameter. if p(t, k) = e\n\npt f (x) =\n\nk=0\n\n\u221e(cid:9)\n\nj=0\n\nthus\n\nps(pt f )(x) =\n\nthis is equal to\n\n1a(x + k),\n\n(\u03bbt )k\nk!\n\u2212\u03bbt (\u03bbt )k/k!, then\nf (x + k)p(t, k).\n\u221e(cid:9)\n\n\u221e(cid:9)\n\nj=0\n\nk=0\n\np(t, m \u2212 k)p(s, k),\n\npt f (x + j)p(s, j) =\nm(cid:9)\n\u221e(cid:9)\n\nf (x + m)\n\nk=0\n\nm=0\n\n\u221e(cid:9)\n\nm=0\n\nf (x + m)p(s + t, m) = ps+t f (x).\n\nwhich by exercise 36.3 is equal to\n\nf (x + j + k)p(t, k)p(s, j).\n\n(36.13)\n\n(36.14)\n\n(36.15)\n\ntherefore the semigroup property holds.\n\nwe therefore have a strong markov process x whose paths are right continuous with left\nlimits. we want to show that the process xt under the probability measure p0 is a poisson\nprocess. that p0(x0 = 0) = 1 is obvious. we need to show that definition 5.1(3) and (4)\nhold. for the former,\n\np0(xt \u2212 xs = k) =\n\n=\n\np(xt = j + k, xs = j)\n\np(s, j)p(t \u2212 s, k) = p(t \u2212 s, k),\n\n(36.16)\n\n(36.17)\n\n\u221e(cid:9)\n\u221e(cid:9)\n\nj=0\n\nj=0\n\nas desired. for definition 5.1(4), suppose r1 \u2264 r2 \u2264 \u00b7\u00b7\u00b7 \u2264 rn \u2264 s < t, a1, . . . , an are\nintegers, and let a = (xr1\n\n= an). we will be done if we show\n\n= a1, . . . , xrn\n\np0(xt \u2212 xs = k, a) = p0(xt \u2212 xs = k)p0(a).\n\n(36.18)\n\n "}, {"Page_number": 303, "text": "285\n\nthe left-hand side of (36.18) is equal to\np0(xt = j + k, xs = j, a) =\n\n\u221e(cid:9)\n\nj=0\n\n=\n\n=\n\nnotes\n\nj=0\n\n\u221e(cid:9)\n\u221e(cid:9)\n\u221e(cid:9)\n\u221e(cid:9)\n\nj=0\n\nj=0\n\ne 0[p0(xt = j + k | fs); xs = j, a]\n(cid:17)\n(cid:16)\npxs (xt\u2212s = j + k); xs = j, a\n(cid:17)\n(cid:16)\np j (xt\u2212s = j + k); xs = j, a\n\ne 0\n\ne 0\n\ne 0[p(t \u2212 s, k); xs = j, a]\n\n=\n= p(t \u2212 s, k)p0(a).\n\nj=0\n\ntogether with (36.16) this proves (36.18).\n\nexercises\n\n36.1 suppose pt is a family of sub-markov transition probabilities and we define pt by (36.12). show\nthat pt is a family of markov transition probabilities. show that p\u0001(xt (cid:16)= \u0001 for some t > 0) = 0,\ni.e., starting at \u0001, the process stays there forever.\n\n36.2 show that if pt (x, a) is defined by (19.17), and pt f (x) =(cid:15)\n\nf (y) pt (x, dy), then pt maps c0 into\n\nc0.\n\n36.3 show that (36.14) equals (36.15).\n\n36.4 show that pt defined by (36.13) satisfies all the parts of assumption 36.1.\n36.5 suppose {\u03bct , t \u2265 0} is a tight family of probability measures on the real line. suppose there\nexists a function \u03c8 : r \u2192 c such that the fourier transforms of the \u03bct have the following form:\n\n(cid:3)\n\neiux \u03bct (dx) = et\u03c8 (u),\n\nt \u2265 0, u \u2208 r.\n\n(1) prove that \u03bct converges weakly to \u03bc0 as t \u2192 0. note that \u03bc0 is the same as point\nmass at 0.\n\n(2) define the operators pt by\n\npt f (x) =\n\nf (x \u2212 y) \u03bct (dy).\n\n(cid:3)\n\nprove that the pt form a strongly continuous semigroup of contraction operators mapping c0\ninto c0. conclude that there exists a strong markov process whose semigroup is given by\nthe pt.\nthis semigroup is called a convolution semigroup because \u03bct+s = \u03bct \u2217 \u03bcs, in the sense of\nconvolution of measures. we will see later that these are associated with l\u00b4evy processes.\n\nsee blumenthal and getoor (1968) for further information.\n\nnotes\n\n "}, {"Page_number": 304, "text": "37\n\ninfinitesimal generators\n\noften a markov process is specified in terms of its behavior at each point, and one wants\nto form a global picture of the process. this means one is given the infinitesimal generator,\nwhich is a linear operator that is an unbounded operator in general, and one wants to come\nup with the semigroup for the markov process.\n\nwe will begin by looking further at semigroups and resolvents, and then define the\ninfinitesimal generator of a semigroup. we will prove the hille\u2013yosida theorem, which is the\nprimary tool for constructing semigroups from infinitesimal generators. then we will look\nat two important examples: elliptic operators in nondivergence form and l\u00b4evy processes.\n\n37.1 semigroup properties\n\nlet s be a locally compact separable metric space. we will take b to be a separable banach\nspace of real-valued functions on s. for the most part, we will take b to be the continuous\nfunctions on s that vanish at infinity (with the supremum norm), although another common\nexample is to let b be the set of functions on s that are in l2 with respect to some measure.\nwe use (cid:21) \u00b7 (cid:21) for the norm on b.\n\nfor the duration of this chapter we will make the following assumption.\nassumption 37.1 suppose that pt, t \u2265 0, are operators acting on b such that\n(1) the pt are contractions: (cid:21)pt f (cid:21) \u2264 (cid:21) f (cid:21) for all t \u2265 0 and all f \u2208 b,\n(2) the pt form a semigroup: pspt = pt+s for all s, t \u2265 0, and\n(3) the pt are strongly continuous: if f \u2208 b, then pt f \u2192 f as t \u2192 0.\nnote that the semigroup property implies in particular that ps and pt commute. for a bounded\noperator a on b, (cid:21)a(cid:21) = sup{(cid:21)a f (cid:21) : (cid:21) f (cid:21) \u2264 1}, so saying pt is a contraction is the same as\nsaying (cid:21)pt(cid:21) \u2264 1.\n\ndefine the resolvent or \u03bb-resolvent operator of a semigroup pt by\n\n(cid:3) \u221e\n\nr\u03bb f (x) =\n\n\u2212\u03bbtpt f (x) dt.\ne\n\nthe resolvent equation is\n\n0\n\nr\u03bb \u2212 r\u03bc = (\u03bc \u2212 \u03bb)r\u03bbr\u03bc.\n\nwe show that the semigroup property implies the resolvent equation.\n\n286\n\n(37.1)\n\n(37.2)\n\n "}, {"Page_number": 305, "text": "37.1 semigroup properties\n\n287\n\nproposition 37.2 the resolvent equation (37.2) holds.\n\nproof we write\n\nr\u03bb(r\u03bc f )(x) =\n=\n\n=\n\n=\n\n=\n\n=\n\n0\n\n0\n\n(cid:3) \u221e\n(cid:3) \u221e\n(cid:3) \u221e\n(cid:3) \u221e\n(cid:3) \u221e\n(cid:3) \u221e\n\n0\n\n0\n\n0\n\n0\n\n0\n\n(cid:3) \u221e\n(cid:3) \u221e\n(cid:3) \u221e\n\n\u2212\u03bbtpt (r\u03bc f )(x) dt\ne\n\u2212\u03bbt\ne\n\n\u2212\u03bbt\ne\n\n\u2212\u03bbte\u03bct\ne\n\n(cid:3)\n\u2212(\u03bb\u2212\u03bc)te\ne\n1 \u2212 e\n\u2212(\u03bb\u2212\u03bc)s\n\n\u2212\u03bcspt (ps f )(x) ds dt\ne\n\u2212\u03bcspt+s f (x) ds dt\ne\n\u2212\u03bcsps f (x) ds dt\ne\n\u2212\u03bcsps f (x) dt ds\n\u2212\u03bcsps f (x) ds\n\u2212\u03bbsps f (x) ds \u2212\ne\n[r\u03bb f (x) \u2212 r\u03bc f (x)].\n\n(cid:16)(cid:3) \u221e\n\n(cid:3) \u221e\n\n\u03bb \u2212 \u03bc\n\ne\n\n0\n\n0\n\n0\n\ns\n\nt\n\n0\n\n= 1\n\n\u03bc \u2212 \u03bb\n\n= 1\n\n\u03bc \u2212 \u03bb\n\n(cid:17)\n\u2212\u03bcsps f (x) ds\ne\n\nthe second equality uses exercise 37.2, the fourth a change of variables, and the fifth the\nfubini theorem.\n\nwe have the following corollary to proposition 37.2.\n\n\u221e(cid:9)\ncorollary 37.3 if \u03bc, \u03bb > 0 and |\u03bc \u2212 \u03bb| < \u03bb, then\n\nr\u03bc f = r\u03bb f +\n\n(\u03bb \u2212 \u03bc)iri+1\n\n\u03bb\n\nf .\n\n(37.3)\n\ni=1\n\nhere r2\n\n\u03bb f = r\u03bb(r\u03bb f ), and similarly for ri\n\n\u03bb f .\n\nproof by proposition 37.2, we have\n\nr\u03bc f = r\u03bb f + (\u03bb \u2212 \u03bc)r\u03bbr\u03bc f .\n\n(37.4)\n\nif we substitute for r\u03bc f in the last term on the right-hand side of (37.4), we have\n\nr\u03bc f = r\u03bb f + (\u03bb \u2212 \u03bc)r\u03bbr\u03bb f + (\u03bb \u2212 \u03bc)2r\u03bbr\u03bbr\u03bc f .\n\nwe again substitute for r\u03bc f , and repeat. since\n\n(cid:21)(\u03bb \u2212 \u03bc)r\u03bb(cid:21) \u2264 |\u03bb \u2212 \u03bc|\n\n,\n\n\u03bb\n\nwhich is less than one in absolute value, (\u03bb \u2212 \u03bc)iri+1\nthe series converges.\n\n\u03bb r\u03bc f converges to zero as i \u2192 \u221e and\n\n "}, {"Page_number": 306, "text": "\u2019\u2019\u2019 ph f \u2212 f\n\nh\n\n\u2212 g\n\n\u2019\u2019\u2019 \u2192 0\n(cid:10)\n\n288\n\ninfinitesimal generators\n\nremark 37.4 in particular, if r\u03bb and s\u03bb are two resolvents that agree at one value of \u03bb, say\n\u03bb0, then corollary 37.3 applied once with r\u03bb and once with s\u03bb implies that if \u03bb < 2\u03bb0, then\n\nr\u03bb f = r\u03bb0 f +\n\n= s\u03bb0 f +\n\n(\u03bb0 \u2212 \u03bb)i(r\u03bb0\n\n)i+1 f\n\n(\u03bb0 \u2212 \u03bb)i(s\u03bb0\n\n)i+1 f = s\u03bb f ,\n\n\u221e(cid:9)\n\u221e(cid:9)\n\ni=1\n\ni=1\n\nor r\u03bb and s\u03bb agree for \u03bb < 2\u03bb0. applying this observation again with \u03bb0 replaced by 3\u03bb0/2,\nthen r\u03bb and s\u03bb agree for \u03bb < 3\u03bb0. continuing this argument, we see that r\u03bb and s\u03bb must\nagree for each positive value of \u03bb.\n\nif for some f \u2208 b,\n\nas h \u2192 0, we say that f is in the domain of the infinitesimal generator of the semigroup, we\nwrite g = l f and write f \u2208 d = d(l). generally d(l) is a proper subset of b. if f \u2208 d\nand t > 0, then\n\nphpt f \u2212 pt f\n\n= ptph f \u2212 pt f\n\nh\n\nh\n\n= pt\n\nph f \u2212 f\n\nh\n\n\u2192 ptl f ,\n\n(37.5)\n\nsince pt is a contraction. therefore pt f \u2208 d when f \u2208 d and l(pt f ) = pt (l f ).\nproposition 37.5 fix \u03bb > 0 and let c = {r\u03bb f : f \u2208 b}. then c = d(l) and for f \u2208 b,\n\n(cid:11)\n\n(37.6)\n\nlr\u03bb f = \u03bbr\u03bb f \u2212 f .\n(cid:3) \u221e\n\n(cid:3) \u221e\n\nproof suppose that g \u2208 c, so that g = r\u03bb f for some f \u2208 b. then\n\nphr\u03bb f =\n\nand so\n\n0\n\n\u2212\u03bbtph+t f dt = e\u03bbh\ne\n(cid:3) \u221e\n\nh\n\n\u2212\u03bbtpt f dt,\ne\n(cid:3)\n\nphg \u2212 g = phr\u03bb f \u2212 r\u03bb f = (e\u03bbh \u2212 1)\n\n(37.7)\ndividing by h and letting h \u2192 0, the first term on the right of (37.7) converges (use exercise\n37.2) to\n\n0\n\nh\n\n\u2212\u03bbtpt f dt \u2212\ne\n\nh\n\n\u2212\u03bbtpt f dt.\ne\n\n(cid:3) \u221e\n\n\u03bb\n\n\u2212\u03bbtpt f dt = r\u03bb f .\ne\n\nsince f \u2208 b, then pt f \u2192 f as t \u2192 0. after dividing by h, the second term on the right-hand\nside of (37.7) converges to f . thus\n\n0\n\nl(r\u03bb f ) = \u03bbr\u03bb f \u2212 f ,\n\n(37.8)\n\nas required.\n\n "}, {"Page_number": 307, "text": "37.1 semigroup properties\n\n289\nwe have shown that c \u2282 d(l), and we now show the opposite inclusion. suppose\nf \u2208 d(l). let g = \u03bb f \u2212 l f , which is in b. since pt and l commute, then r\u03bb and l\ncommute, and by (37.8),\n\nf = \u03bbr\u03bb f \u2212 (\u03bbr\u03bb f \u2212 f ) = \u03bbr\u03bb f \u2212 r\u03bbl f\n= r\u03bbg,\n\nwhich is in c.\n\nexample 37.6 let us compute the infinitesimal generator when (xt , px) is a one-\ndimensional brownian motion. for our space b we take the continuous functions on r\nthat vanish at infinity. suppose f \u2208 c2 with compact support. by a taylor series expansion,\n\nph f (x) = e x f (xh) = f (x) + f\n\n(cid:3)(x)e x(xh \u2212 x) + 1\n2 f\nwhere rh is the remainder term. we know rh is bounded by\n(cid:3)(cid:3)(cid:21)\u221ee x[\u03d5(xh \u2212 x)],\n\n(cid:21) f\n\n(cid:3)(cid:3)(x)e x(xh \u2212 x)2 + rh,\n\nwhere \u03d5 is bounded and |\u03d5(y)/y2| \u2192 0 as y \u2192 0. since wh started at x has mean x and\nvariance h, we have\n\nph f (x) = f (x) + 1\n2 f\nwhere |rh/h| tends to zero as h \u2192 0. therefore\nph f \u2212 f\n\n(cid:3)(cid:3)(x)h + rh,\n\n\u2192 1\n2 f\n\n(cid:3)(cid:3),\n\nh\n\nthe convergence being with respect to the supremum norm. exactly the same argument\nholds in higher dimensions to show that l f = 1\n\u0001 f . we have shown that d(l) contains\nthe c2 functions with compact support, but have not actually identified the domain of the\ninfinitesimal generator. we refer the reader to knight (1981) for a detailed discussion.\n\n2\n\n(cid:3)(cid:3)\n\nthe domain of an infinitesimal generator is nearly as important as the operator itself.\nwe will briefly discuss aspects of the domains of the infinitesimal generator for absorbing\nbrownian motion and for reflecting brownian motion on [0,\u221e). both have the same operator\nl f = 1\n2 f\nabsorbing brownian motion on [0,\u221e) is brownian motion killed on first hitting (\u2212\u221e, 0).\nlet wt be standard brownian motion on r and let xt be wt killed on first hitting (\u2212\u221e, 0).\nif f \u2208 c2[0,\u221e) with f and its first and second derivatives being bounded and uniformly\ncontinuous and x (cid:16)= 0, (e x f (xt ) \u2212 f (x))/t differs from (e x f (wt ) \u2212 f (x))/t by at most\n\nbut different domains.\n\nwhere t0 is the first time wt hits (\u2212\u221e, 0). if x (cid:16)= 0,\n\n(cid:21) f (cid:21)\u221epx(t0 < t )/t,\n\npx(t0 < t )\n\n\u2264 px(sups\u2264t\n\nt\n\n|ws \u2212 w0| \u2265 x)\nt\n\n\u2264 2\nt\n\n\u2212x2/2t \u2192 0\n\ne\n\nas t \u2192 0. therefore for x (cid:16)= 0, the infinitesimal generator of absorbing brownian motion is\nthe same as the infinitesimal generator of standard brownian motion, namely, 1\n2 f\n\n(cid:3)(cid:3)(x).\n\n "}, {"Page_number": 308, "text": "290\n\ninfinitesimal generators\n\nif f = r\u03bbg for g bounded and continuous, we have\n\n(cid:3)\n\nf (0) = r\u03bbg(0) = e 0\n\nt0\n\n\u2212\u03bbtg(xt ) dt = 0.\ne\n\n0\n\nwe use the fact that starting at 0, t0 = 0, a.s., by theorem 7.2. using proposition 37.5, every\nfunction in the domain of the infinitesimal generator of absorbing brownian motion must\nsatisfy f (0) = 0.\nwe can define reflecting brownian motion on [0,\u221e) by xt = |wt|, where w is a one-\ndimensional brownian motion on r. as in the preceding paragraph, the infinitesimal gen-\n(cid:3)(cid:3)(x) if x (cid:16)= 0. for x = 0, an application of taylor\u2019s theorem\nerator for x agrees with 1\n2 f\ngives\n\n(cid:3)(0)e 0|wt| + 1\n2 f\n\ne 0 f (|wt|) = f (0) + f\n\u221a\nt/t \u2192 \u221e as t \u2192 0, the only way we can get convergence is if f\n\nwhere rt is a remainder term. subtracting f (0) from both sides and dividing by t, and noting\ne 0|wt|/t = c1\n(cid:3)(0) = 0.\nthus every function in the domain of the infinitesimal generator of reflecting brownian\nmotion must satisfy f\n\n(cid:3)(cid:3)(0)e 0|wt|2 + e 0rt ,\n\n(cid:3)(0) = 0.\n\nin higher dimensions, the analogous restriction for reflecting brownian motion is that\nthe normal derivative \u2202 f /\u2202n must equal zero on the boundary of the domain, where n is\nthe inward-pointing unit normal vector. in the partial differential equations literature, this is\nknown as the neumann boundary condition, and models situations where there is no heat\nflow across the boundary. for absorbing brownian motion the analogous restriction is that\nf = 0 on the boundary of the domain, and this is called the dirichlet boundary condition.\nexample 37.7 next we compute the generator for a poisson process with parameter \u03bb. we\ncan let b be as in example 37.6. we have\n\n\u221e(cid:9)\n\nph f (x) =\n\n(\u03bbh)i\n\n\u2212\u03bbh\ne\n\nf (x + i)\n\ni!\ni=0\n= e\n\u2212\u03bbh f (x) + e\n\n\u221e(cid:9)\n\ni=2\n\n\u2212\u03bbh\u03bbh f (x + 1) +\n\n\u2212\u03bbh\ne\n\n(\u03bbh)i\n\ni!\n\nf (x + i).\n\nsubtracting f (x) from both sides, dividing by h, and letting h \u2192 0, we obtain\n\nl f (x) = \u2212\u03bb f (x) + \u03bb f (x + 1) = \u03bb[ f (x + 1) \u2212 f (x)].\n\nin this case the domain of l is all of b.\n\na very useful result is dynkin\u2019s formula.\ntheorem 37.8 suppose pt operating on the space b of continuous functions vanishing at\ninfinity is the semigroup of a markov process (xt , px), f \u2208 d(l), and f andl f are bounded.\n(cid:3)\nif x \u2208 s and t is a stopping time with e xt < \u221e, then\n\ne x f (xt ) \u2212 f (x) = e x\n\nt\n\nl f (xr ) dr.\n\n0\n\n "}, {"Page_number": 309, "text": "37.1 semigroup properties\n\n291\n\nif f \u2208 d(l), then l f \u2208 b, and so ptl f is continuous in t. moreover, as we saw in\n\nproof\n(37.5),\n\npt f (y) = ptl f (y).\n\n\u2202\n\u2202t\n\n(cid:3)\n\nby the fundamental theorem of calculus,\n\npt f (y) \u2212 f (y) =\n\nwhich can be rewritten as\n\nt\n\n0\n\nprl f (y) dr,\n(cid:3)\n\nt\n\nl f (xr ) dr;\n\n(37.9)\n\nwe used the fubini theorem here as well. this holds for each y \u2208 s and each t > 0.\n\ne y f (xt ) \u2212 f (y) = e y\n\n0\n\nset mt = f (xt ) \u2212 f (x0) \u2212(cid:15)\n\nt\n0\n\nall t. by the markov property,\n\n(cid:3)\n\nl f (xr ) dr. what (37.9) says is that e ymt = 0 for all y and\n(cid:16)\n(cid:16)(cid:10)\nf (xt ) \u2212 f (xs) \u2212\nf (xt\u2212s) \u2212 f (x0) \u2212\n\n(cid:3)\nl f (xr ) dr | fs\nt\u2212s\nl f (xr ) dr\n\n\u25e6 \u03b8s | fs\n\n(cid:17)\n(cid:11)\n\n(cid:17)\n\ns\n\nt\n\n0\n\ne x[mt \u2212 ms | fs] = e x\n= e x\n= e xsmt\u2212s = 0.\n\ntherefore mt is a martingale with respect to px for each x. if t is a bounded stopping time,\nthen by optional stopping, e xmt = 0. if t is instead only integrable with respect to px,\nwe have e xmt\u2227n = 0 for each n. we then let n \u2192 \u221e and use the fact that f and l f are\nbounded to conclude e xmt = 0, which is what we want.\n\nwe say a few words about the kolmogorov backward and forward equations. suppose the\n\nsemigroup pt can be written\n\npt f (x) =\n\nf (y)p(t, x, y) dy,\n\n(cid:3)\n\nfor functions p(t, x, y), which are called transition densities. provided there are no difficulties\ninterchanging integration and differentiation, the equation\npt f (x) = lpt f (x)\n\n\u2202\n\u2202t\n\ncan be rewritten as\n\n(cid:3)\n\n(cid:3)\n\nf (y)\n\n\u2202\n\u2202t\n\np(t, x, y) dy =\n\nf (y)lp(t, x, y) dy,\n\nwhich leads to the kolmogorov backward equation\n\nwhere l operates on the x variable and y is held fixed.\n\np(t, x, y) = lp(t, x, y),\n\n\u2202\n\u2202t\n\n "}, {"Page_number": 310, "text": "(l\u2217\n\nf )g for f and g in the\n\n292\n\ninfinitesimal generators\n\nif l has an adjoint operator l\u2217\n\n, which means\nand l, respectively, the equation\n\ndomains of l\u2217\n(cid:3)\n\ncan be rewritten as\n\u2202\n\u2202t\n\nf (y)\n\n\u2202\n\u2202t\n\n(cid:3)\n\np(t, x, y) dy =\n\npt f (x) = ptl f (x)\n\n(cid:15)\n\nf (lg) = (cid:15)\n(cid:3)\n\nwhich leads to the kolmogorov forward equation\np(t, x, y) = l\u2217\n\np(t, x, y),\n\n\u2202\n\u2202t\n\nwhere l\u2217\n\noperates on the y variable and x is held fixed.\n\n37.2 the hille\u2013yosida theorem\n\nl f (y)p(t, x, y) dy =\n\nf (y)l\u2217\n\np(t, x, y) dy,\n\nwe now show how to construct a semigroup given the infinitesimal generator. we start with\na few preliminary observations. if a is a bounded operator, we can define\n\nto see that the series converges, note that\n\nwhich will be small if m is large since (cid:21)a(cid:21) is a finite number. similarly,\n\n\u221e(cid:9)\nea = i + a + a2/2! + \u00b7\u00b7\u00b7 =\n\u221e(cid:9)\n\ni=0\n\nai/i!\n\nai/i!\n\n(cid:21)ai(cid:21)/i! \u2264\n\n(cid:21)a(cid:21)i/i!,\n\ni=m\n\n\u2019\u2019\u2019 n(cid:9)\n\ni=m\n\n(cid:21)ea(cid:21) \u2264\n\n(cid:21)a(cid:21)i/i! = e\n\n(cid:21)a(cid:21).\n\n\u2019\u2019\u2019 \u2264 n(cid:9)\n\u221e(cid:9)\n\ni=m\n\ni=0\n\nproposition 37.9 suppose {r\u03bb} is a family of bounded operators defined on b such that\n\n(1) the resolvent equation holds,\n(2) (cid:21)r\u03bb(cid:21) \u2264 1/\u03bb for each \u03bb > 0, and\n(3) (cid:21)\u03bbr\u03bb f \u2212 f (cid:21) \u2192 0 as \u03bb \u2192 \u221e for each f \u2208 b.\n\nthen there exists a strongly continuous semigroup pt whose resolvent is r\u03bb.\nproof let d\u03bb = \u03bb(\u03bbr\u03bb \u2212 i ) and q\u03bb\nd\u03bb and d\u03bc commute and therefore all the operators d\u03bb, q\u03bb\n(cid:21)\u03bbr\u03bb(cid:21) \u2264 1, then\n\n= etd\u03bb. note that the resolvent equation implies that\nt commute. since\n\nt\n\n(cid:21)q\u03bb\n\n(cid:21) = e\n\n\u2212\u03bbt(cid:21)et\u03bb2r\u03bb(cid:21) \u2264 e\n\n\u2212\u03bbte\n\n(cid:21)t\u03bb2r\u03bb(cid:21) \u2264 e\n\nt\n\nwe first show that the set of f such that d\u03bb f converges as \u03bb \u2192 \u221e is a dense subset of b.\nif f = rag for some a > 0 and some g \u2208 b, then by the resolvent equation\n\nt , d\u03bc, and q\u03bc\n\u2212\u03bbte\u03bbt = 1.\n\nd\u03bb f = \u03bb(\u03bbr\u03bb \u2212 i )(rag) = \u03bb2r\u03bbrag \u2212 \u03bbrag\n\n= \u03bb2\n\u03bb \u2212 a\n\n(rag \u2212 r\u03bbg) \u2212 \u03bbrag.\n\n "}, {"Page_number": 311, "text": "37.2 the hille\u2013yosida theorem\n\n293\n\nwe have\n\n\u03bb2\n\n\u03bb \u2212 a\n\nas \u03bb \u2192 \u221e by hypothesis (3) and\n\nr\u03bbg = \u03bb\n\u03bb \u2212 a\n\n\u03bbr\u03bbg \u2192 g\n\n\u03bb2\n\n\u03bb \u2212 a\n\nrag \u2212 \u03bbrag = \u03bb\n\u03bb \u2212 a\n\narag \u2192 arag\n\nas \u03bb \u2192 \u221e. therefore\n\nd\u03bbrag \u2192 arag \u2212 g.\n\n(37.10)\nthus d\u03bb converges on e = \u222aa>0{ra f : f \u2208 b}. but for any f \u2208 b, ara f \u2192 f as a \u2192 \u221e\nand ara f = ra(a f ) \u2208 e, which proves that e is a dense subset of b.\nnext we show that if d\u03bb f converges, then q\u03bb\n\u03b5 > 0. choose m such that if \u03bb, \u03bc \u2265 m, then (cid:21)d\u03bb f \u2212 d\u03bc f (cid:21) < \u03b5. since \u2202q\u03bb\nand q\u03bb\n0\n\nt f converges. suppose d\u03bb f converges and\nt f /dt = d\u03bbq\u03bb\nt f\n\n0 are both the identity operator, we have\n\n, q\u03bc\n\n(cid:3)\n(cid:3)\n(cid:3)\n\nt\n\nt\n\nt\n\n0\n\n0\n\n0\n\nt f \u2212 q\u03bc\nq\u03bb\n\nt f =\n=\n\n=\n\n\u2202\n\u2202s\n\n(q\u03bb\n\ns q\u03bc\nt\u2212s f ) ds\nt\u2212s f \u2212 q\u03bb\ns d\u03bcq\u03bc\n(d\u03bb f \u2212 d\u03bc f )] ds,\n\n[q\u03bb\n\ns d\u03bbq\u03bc\n\n[q\u03bb\n\ns q\u03bc\nt\u2212s\n\nt\u2212s f ] ds\n\nso\n\n(cid:21)q\u03bb\n\nt f \u2212 q\u03bc\nt\u2212s are contractions.\nsince \u03b5 is arbitrary, this proves that q\u03bb\n\ns and q\u03bc\n\nusing that q\u03bb\n\nt f (cid:21) \u2264 t(cid:21)d\u03bb f \u2212 d\u03bc f (cid:21) < \u03b5t,\n\nt f is a cauchy sequence in b and hence converges.\ncall the limit pt f . we can easily check that q\u03bb\nt is a semigroup for each \u03bb > 0 and we saw\nthat q\u03bb\nt is a contraction for each t and \u03bb. it follows that pt is a semigroup and that the norm of\neach pt is bounded by 1. each q\u03bb\nt is strongly continuous, and by the uniform convergence, it\nfollows that pt f \u2192 f as t \u2192 0 for f \u2208 e. since each pt is a contraction and e is dense in b,\nwe can extend each pt so as to have domain b and so that the pt will be a strongly continuous\nsemigroup on b.\nlet s\u03bb be the resolvent for pt. it remains to prove that s\u03bb = r\u03bb. fix a and let f = rag. we\nsaw in( 37.10) that d\u03bbrag \u2192 arag \u2212 g. now q\u03bb\nt is a semigroup for each \u03bb and by exercise\n37.4, the infinitesimal generator of q\u03bb\n\nt is d\u03bb. by the fundamental theorem of calculus,\n\n(cid:3)\n\n(cid:3)\n\n(rag) \u2212 rag =\n\nq\u03bb\nt\n\nletting \u03bb \u2192 \u221e,\n\nt\n\n0\n\n\u2202\n\u2202s\n\n(q\u03bb\n\ns rag) ds =\n(cid:3)\n\nt\n\n0\n\nq\u03bb\ns\n\n(d\u03bbrag) ds.\n\npt (rag) \u2212 rag =\n\nt\n\nps(arag \u2212 g) ds.\n\n0\n\n "}, {"Page_number": 312, "text": "294\n\ninfinitesimal generators\n\nlet b < a. multiply the above equation by e\n\nsb(rag) \u2212 1\nb\n\nrag =\n=\n\n=\n\n(cid:3) \u221e\n(cid:3) \u221e\n(cid:3) \u221e\n\n0\n\n0\n\n(cid:3)\n\n\u2212bt and integrate over t from 0 to \u221e. then\n\u2212bt\ne\n\nt\n\n(cid:3) \u221e\n\ns\n1\nb\n\ne\n\n0\n\nps(arag \u2212 g) ds dt\n\u2212btps(arag \u2212 g) dt ds\ne\n\u2212bsps(arag \u2212 g) ds\n\n0\n\n= 1\nb\n\nsb(arag \u2212 g).\n\ntherefore\n\nsbg = rag + (a \u2212 b)sbrag.\n\napplying this with g replaced by rag, iterating, and using corollary 37.3, we obtain\n\nsbg = rag + (a \u2212 b)r2\n\nag + (a \u2212 b)3r3\n\nag + \u00b7\u00b7\u00b7 = rbg.\n\nby remark 37.4, this proves sb = rb for all b.\nwe now show that under appropriate hypotheses on l, there exists a semigroup whose\ninfinitesimal generator is l. this is known as the hille\u2013yosida theorem. we say that an\noperator l is dissipative if\n\n(cid:21)(\u03bb \u2212 l) f (cid:21) \u2265 \u03bb(cid:21) f (cid:21),\n\nf \u2208 d(l).\n\n(37.11)\n\ntheorem 37.10 suppose l is an operator such that\n\n(1) the domain of l is a dense subset of b,\n(2) the range of \u03bb \u2212 l is b for each \u03bb, and\n(3) l is dissipative.\n\nthen there exists a semigroup on b which has l as its infinitesimal generator.\nproof\n\nif (\u03bb \u2212 l) f = (\u03bb \u2212 l)g, then\n\n\u03bb(cid:21) f \u2212 g(cid:21) \u2264 (cid:21)(\u03bb \u2212 l)( f \u2212 g)(cid:21) = 0,\n\nor f = g. thus \u03bb \u2212 l is a one-to-one map, hence is invertible because the range of \u03bb \u2212 l is\nb. we let r\u03bb be the inverse, and thus the domain of r\u03bb is all of b.\n\nwe first show that the resolvent equation holds. we observe\n\n(\u03bc \u2212 l)\n\n1\n\n\u03bb \u2212 \u03bc\n\nr\u03bc f = 1\n\u03bb \u2212 \u03bc\n\nf\n\nand\n\n(\u03bc \u2212 l)\n\n1\n\n\u03bb \u2212 \u03bc\n\nr\u03bb f = (\u03bc \u2212 \u03bb)\n\n1\n\n\u03bb \u2212 \u03bc\n= \u2212r\u03bb f + 1\n\u03bb \u2212 \u03bc\n\nf .\n\nr\u03bb f + (\u03bb \u2212 l)\n\n1\n\n\u03bb \u2212 \u03bc\n\nr\u03bb f\n\ncombining,\n\n(\u03bc \u2212 l)r\u03bcr\u03bb f = r\u03bb f = (\u03bc \u2212 l)\n\n1\n\n\u03bb \u2212 \u03bc\n\n(r\u03bc \u2212 r\u03bb) f .\n\n "}, {"Page_number": 313, "text": "37.2 the hille\u2013yosida theorem\n\n295\n\napplying r\u03bc to both sides yields the resolvent equation.\nthe hypothesis that (cid:21)\u03bb \u2212 l) f (cid:21) \u2265 \u03bb(cid:21) f (cid:21) immediately implies (cid:21)r\u03bb f (cid:21) \u2264 (cid:21) f (cid:21)/\u03bb.\nwe next show \u03bbr\u03bb f \u2192 f as \u03bb \u2192 \u221e. if f \u2208 d, then\n\nr\u03bbl f = lr\u03bb f = \u03bbr\u03bb f \u2212 f ,\n\nand so\n\n(cid:21)\u03bbr\u03bb f \u2212 f (cid:21) \u2264 1\n\n(cid:21)l f (cid:21) \u2192 0\n\n\u03bb\n\nas \u03bb \u2192 \u221e. since (cid:21)\u03bbr\u03bb(cid:21) \u2264 1 and the domain of l is dense in b, we conclude \u03bbr\u03bb f \u2192 f\nfor all f \u2208 b.\nwe use proposition 37.9 to construct pt. by proposition 37.9, r\u03bb is the resolvent for\npt. if m is the infinitesimal generator for pt, then by proposition 37.5, the domain of m\nf \u2208 b}. since we know l(r\u03bb f ) = \u03bbr\u03bb f \u2212 f \u2208 b, then the domain of l\nis {r\u03bb f\ncontains {r\u03bb f : f \u2208 b}. since m is the infinitesimal generator of pt, by proposition 37.5,\nm(r\u03bb f ) = \u03bbr\u03bb f \u2212 f . therefore l is an extension of m.\nif f \u2208 d(l), then g = (\u03bb \u2212 l) f \u2208 b, and thus\n\n:\n\n(\u03bb \u2212 m)\u22121g \u2208 d(m) \u2282 d(l).\n\nhence\n\n(\u03bb \u2212 l) f = g = (\u03bb \u2212 m)(\u03bb \u2212 m)\u22121g = (\u03bb \u2212 l)(\u03bb \u2212 m)\u22121g.\n\nsince \u03bb \u2212 l is one-to-one, then f = (\u03bb \u2212 m)\u22121g, which implies f \u2208 d(m). therefore\nm = l and so l is the generator of pt.\nwhen applying the hille\u2013yosida theorem, it is quite often the case that it is easier to show\nthat the range of \u03bb \u2212 l is only dense in b, rather than being all of b. when that occurs, one\nneeds to look at a closed extension l of l. an operator l is closed if whenever fn \u2192 f\nand l fn \u2192 g, then f \u2208 d(l) and l f = g. to construct the closed extension of l, where\nwe assume that l is dissipative (defined by (37.11)), let r\u03bbg = f if (\u03bb \u2212 l) f = g. l being\ndissipative is equivalent to the norm of r\u03bb being bounded by 1/\u03bb on the range of \u03bb \u2212 l, and\nso we can extend the domain of r\u03bb uniquely to all of b. now define d(l) to be the range of\nr\u03bb and set\n\nlr\u03bbg = \u03bbr\u03bbg \u2212 g.\n\n(37.12)\n\nwe will soon give two examples where infinitesimal generators can be used to construct\nvery useful processes. the first is where the infinitesimal generator is an elliptic operator of\nsecond order in non-divergence form. the second case studies the infinitesimal generators\nof l\u00b4evy processes.\n\nwe should mention that there is another important example where infinitesimal generators\nare useful in constructing a process, that of infinite particle systems. the name \u201cinfinite\nparticle systems\u201d refers to a class of models with discrete space and continuous time that are\nuseful in mathematical biology and in statistical mechanics. one of the simplest examples\nis the voter model. suppose at every point in z2, the integer lattice in the plane, there is a\nvoter, who is leaning either toward the democrat candidate or the republican candidate. at\neach point, the voter waits a length of time that is exponential with parameter one, chooses\n\n "}, {"Page_number": 314, "text": "296\n\ninfinitesimal generators\n\none of his four nearest neighbors at random, and then changes his view to agree with that\nneighbor. other infinite particle systems include the contact process (modeling the spread of\ninfection), ising model (modeling ferromagnetism), and the exclusion model (used in solid\nstate physics). see liggett (2010) for how to construct these processes using infinitesimal\ngenerators, and for much more.\n\nlet us consider the operator l defined on c2 functions on rd by\n\n37.3 nondivergence form elliptic operators\n\nl f (x) = d(cid:9)\n\ni, j=1\n\n(x) + d(cid:9)\n\ni=1\n\nai j (x)\n\n\u2202 2 f\n\u2202xi\u2202x j\n\nbi(x)\n\n\u2202 f\n\u2202xi\n\n(x).\n\nwe suppose ai j (x) = a ji(x) for all x. we assume the ai j and bi are bounded and h\u00a8older\ncontinuous of order \u03b1 \u2208 (0, 1): there exists c such that\n\nfor i, j = 1, . . . , d. we also assume a uniform ellipticity condition on the ai j: there exists\n\u0001 > 0 such that\n\n|bi(x) \u2212 bi(y)| \u2264 c|x \u2212 y|\u03b1,\n\n|ai j (x) \u2212 ai j (y)| \u2264 c|x \u2212 y|\u03b1,\nd(cid:9)\n\nd(cid:9)\n\nai j (x)yiy j \u2265 \u0001\n\ni, j=1\n\ni=1\n\n,\n\ny2\ni\n\n(y1, . . . , yd ) \u2208 rd .\n\nuniform ellipticity says that the matrix whose (i, j)th element is ai j (x) is positive definite,\nuniformly in x.\nif the ai j and bi were lipschitz continuous, we can construct the markov process with\ninfinitesimal generator l using stochastic differential equations (see chapter 39), which\nis a more probabilistic way of doing it. even when the ai j are continuous and the bi only\nmeasurable, it is possible to construct the markov process via sdes, although this is much\nharder. here we illustrate how the hille\u2013yosida theorem can be used in constructing these\nprocesses.\nlet b be the space of continuous functions that vanish at infinity. we will want the domain\nofl to include the classc of functions f such that f and its first and second partial derivatives\nare continuous and vanish at infinity. then c is dense in b and l maps c into b.\nwe show that l is dissipative. let f \u2208 c and let x0 be a point where | f (x0)| = (cid:21) f (cid:21). there\nis nothing to prove if f is identically zero. if f (x0) < 0, we can look at \u2212 f , so let us suppose\nf (x0) > 0. such a point x0 exists because f is continuous and vanishes at infinity. it suffices\nto show that l f (x0) \u2264 0, since then\n\n\u03bb(cid:21) f (cid:21) = \u03bb f (x0) \u2264 (\u03bb \u2212 l) f (x0) \u2264 (cid:21)(\u03bb \u2212 l) f (cid:21).\n\nlet a be the matrix whose (i, j) element is ai j (x0) and let h be the hessian at x0 so that\n\nhi j = \u2202 2 f\n\u2202xi\u2202x j\n\n(x0).\n\n "}, {"Page_number": 315, "text": "d(cid:9)\n\ni, j=1\n\n37.4 generators of l\u00b4evy processes\n\n297\nlet y \u2208 rd and consider the function f (x0 + ty), t \u2208 r. since x0 is the location of a local\nd\ni, j=1 yiy jhi j, will be less than\n\nmaximum for this function, its second derivative, which is\nor equal to 0. the first derivative of this function will be zero at x0.\nsince a is positive definite, there exists an orthogonal matrix p and a diagonal matrix d\nwith positive entries such that a = pt dp. recall the trace of a square matrix is defined by\n\n(cid:12)\n\ntrace (c) =(cid:12)\n\nd\n\ni=1 cii and trace (ab) = trace (ba). note\n\nai j (x0)\n\n\u2202 2 f\n\u2202xi\u2202x j\n\n(x0) = trace (ah ).\n\nwe have\n\ntrace (ah ) = trace (pt dph ) = trace (ph pt d) = d(cid:9)\n\n(ph pt )iidii,\n\ni=1\n\nsince d is a diagonal matrix. thus to show that trace (ah ) \u2264 0, it suffices to show that\n(ph pt )ii \u2264 0 for each i. if we let ei be the unit vector in the xi direction and y = pt ei, we\nhave\n\niph pt ei = yth y = d(cid:9)\n\ni, j=1\n\n(ph pt )ii = et\n\nyiy jhi j \u2264 0.\n\n(x0) = 0, and we conclude l f (x0) \u2264 0.\nsince x0 is the location of a local maximum, then \u2202 f\nsince l1 = 0, then pt1 = 1 for all t. this and exercise 37.1 imply that the pt are\n\u2202xi\nnon-negative operators.\nto apply the hille\u2013yosida theorem, it remains to show that the range of \u03bb \u2212 l is dense in\nb. for this we refer the reader to the pde literature, e.g., bass (1997), chapter 3 or gilbarg\nand trudinger (1983), chapters 5,6.\n\nlet n be a measure on r \\ {0} satisfying\n\n37.4 generators of l\u00b4evy processes\n\nconsider the operator l defined on c2 functions by\n\n(h2 \u2227 1) n(dh) < \u221e.\n\n(cid:3)\n\n(cid:3)\n\nl f (x) =\n\n[ f (x + h) \u2212 f (x) \u2212 1(|h|\u22641) f\n\n(cid:3)(x)h] n(dh).\n\nwe will show that l is the infinitesimal generator of a markov semigroup. we construct\nthese processes, the l\u00b4evy processes, probabilistically in chapter 42. we confine ourselves\nto the one-dimensional case, although the argument for higher dimensions is completely\nanalogous.\nwe let b be the continuous functions vanishing at infinity. we let c be the class of\nschwartz functions, which is the class of c\nfunctions, all of whose kth partial derivatives\ngo to zero faster than |x|\u2212m as |x| \u2192 \u221e for every k = 0, 1, . . . and every m = 1, 2, . . .; see\nsection b.2.\n\n\u221e\n\n "}, {"Page_number": 316, "text": "(cid:3)\n\n|l f (x)| \u2264\n\n298\n\ninfinitesimal generators\n\nfirst we show that l maps c into b, so that the domain of l contains c, and hence is\n\ndense in b. given m > 1 and f \u2208 c, by taylor\u2019s theorem\n\n| f (x + h) \u2212 f (x) \u2212 1(|h|\u22641) f\n(cid:3)\n\u2264 sup\n|y\u2212x|\u22641\n+ 2\n\n(cid:3)(cid:3)(y)|)\n0<|h|\u22641\n(cid:21) f (cid:21)\u221e n(dh).\n\n(cid:3)(x)h| n(dh)\nh2 n(dh) + 2( sup\n|y\u2212x|\u2264m\n\n(| f\n\n|h|>m\n\n(cid:3)\n\n| f (y)|)\n\n(37.13)\n\nn(dh)\n\n1<|h|\u2264m\n\nthis shows |l f (x)| is finite. given \u03b5 > 0 and f \u2208 c, choose m large so that\n\n(cid:3)\n\n(cid:3)\n\nn(dh) < \u03b5/(cid:21) f (cid:21)\u221e.\n\n|h|>m\n\nsince the first two terms on the right-hand side of (37.13) tend to zero as |x| \u2192 \u221e, then\nl : c \u2192 b.\nto showl is dissipative, let f \u2208 c and choose x0 such that | f (x0)| = (cid:21) f (cid:21). there is nothing\nto prove if (cid:21) f (cid:21) = 0, so assume (cid:21) f (cid:21) > 0. because f is in the schwartz class, it takes on\nits maximum and its minimum. by looking at \u2212 f if necessary, we may suppose f (x0) > 0.\n(cid:3)(x0) = 0 and f (x0 + h)\u2212 f (x0) \u2264 0 for each\nsince x0 is the location of a local maximum, f\nh, hence l f (x0) \u2264 0. then\n\n\u03bb(cid:21) f (cid:21) = \u03bb f (x0) \u2264 (\u03bb \u2212 l) f (x0) \u2264 (cid:21)(\u03bb \u2212 l) f (cid:21).\n\ntaking limits, this holds for every f in the domain of l.\n\nfinally we need to show that the range of \u03bb\u2212l is dense in b. this is the most complicated\nif f \u2208 c. let n\u03b4 (dh) =\n\npart and we break the argument into steps.\nstep 1. we start by computing the fourier transform of l f\n1(|h|\u2265\u03b4)n(dh) and let\n\n[ f (x + h) \u2212 f (x) \u2212 1(|h|\u22641) f\n\n(cid:3)(x)h] n\u03b4 (dh).\n\nof the function x \u2192 f (x + h) is eiuh(cid:2)f (u) and the fourier transform of f\n\nthen n\u03b4 is a finite measure. using the fubini theorem and the fact that the fourier transform\n\n(cid:3)(x) is \u2212iu(cid:2)f (u),\n\n(cid:3)\n\n(cid:3)(x)h] dx n\u03b4 (dh)\n\n[e\n\n(cid:3)\n[eiux f (x + h) \u2212 eiux f (x) \u2212 1(|h|\u22641)eiux f\n(cid:3)\n\n\u2212iuh \u2212 1 + 1(|h|\u22641)iuh] n\u03b4 (dh)\n\u2212iuh \u2212 1 + 1(|h|\u22641)iuh]1(|h|\u2265\u03b4) n(dh).\n[e\n(cid:3)\n\n\u03c8 (u) =\n\n\u2212iuh \u2212 1 + 1(|h|\u22641)iuh] n(dh).\n\n[e\n\n(37.14)\nthe expression in brackets on the last line is bounded by c(h2 \u2227 1) and by dominated\n\nconvergence the last line converges to(cid:2)f (u)\u03c8 (u) as \u03b4 \u2192 0, where\n\n#l\u03b4 f (u) =\n\nl\u03b4 f (x) =\n(cid:3) (cid:3)\n=(cid:2)f (u)\n=(cid:2)f (u)\n\n(37.15)\n\n "}, {"Page_number": 317, "text": "(cid:3)\n\n|h|\u2264k\n\n(cid:3)\n\n|h|\u2264k\n\n37.4 generators of l\u00b4evy processes\n\n299\n\nsince\n\n|&l f (u) \u2212 #l\u03b4 f (u)|\n(cid:3)\n(cid:20)(cid:20)(cid:20)(cid:3)\n(cid:3)\n\neiux\n\n=\n\n\u2264\n\n( sup\n|y\u2212x|<\u03b4\n\n(cid:3)\n\n[ f (x + h) \u2212 f (x) \u2212 1(|h|\u22641) f\n(cid:3)(cid:3)(y)|)\n\nh2 n(dh) dx,\n\n|h|<\u03b4\n| f\n\n|h|<\u03b4\n\n(cid:20)(cid:20)(cid:20)\n\n(cid:3)(x)h] n(dh) dx\n\nwhich tends to zero as \u03b4 \u2192 0 because f \u2208 c, we conclude\n\n&l f (u) =(cid:2)f (u)\u03c8 (u).\n\n(cid:15)\nstep 2. now let g \u2208 c, let \u03b5 > 0, choose k > 1 such that\n|h|\u2265k n(dh) < \u03b5, let mk (dh) =\n1(|h|\u2265k )n(dh), and define lk and \u03c8k in terms of mk. we show there exists f \u2208 c such that\ng = (\u03bb \u2212 lk ) f = g.\nwe have\n\n(37.16)\n\n\u03c8k (u) =\n\n\u2212iuh \u2212 1 + iuh1(|h|\u22641)] n(dh),\n[e\n\nso using dominated convergence,\n(u) =\n\n\u03c8(cid:3)\n\nk\n\n\u2212iuh + ih1(|h|\u22641)] n(dh),\n\n[\u2212ihe\n(cid:3)\n\nk\n\n\u03c8(cid:3)(cid:3)\n\n|h|\u2264k\n\n[\u2212h2e\n\n\u2212iuh] n(dh),\n\n(u) =\n(cid:15)\nwith similar formulas for the higher derivatives. thus all the derivatives of \u03c8k are bounded.\n|h|\u2264k[cos(uh) \u2212 1] n(dh), which is less than or equal to\nmoreover the real part of \u03c8k (u) is\n(cid:2)f (u) =\n\n0. since g \u2208 c, by section b.2,(cid:2)g \u2208 c. if we define f by\n(cid:2)g(u),\n\u03bb \u2212 \u03c8k (u)\nwe see that (cid:2)f and all its derivatives are continuous and tend to zero faster than |u|\u2212m for\nevery m. hence(cid:2)f \u2208 c, which implies f \u2208 c by section b.2.\nnotice (\u03bb \u2212 lk ) f = g because\n\u03bb(cid:2)f (u) \u2212 #lk f (u) = \u03bb \u2212 \u03c8k (u)\n\n(cid:2)g(u) =(cid:2)g(u).\n\n(37.17)\n\n1\n\n\u03bb \u2212 \u03c8k (u)\n\nsince g \u2208 c, then(cid:2)g \u2208 l1. from( 37.17) we have |(cid:2)f (u)| \u2264 |(cid:2)g(u)|/\u03bb. then\nstep 3. we prove that (cid:21)l f \u2212 lk f (cid:21)\u2264 c\u03b5(cid:21)g(cid:21).\n\n(cid:21) f (cid:21)\u221e \u2264 c(cid:21)(cid:2)f (cid:21)l1 \u2264 c(cid:21)(cid:2)g(cid:21)l1\n\n "}, {"Page_number": 318, "text": "300\n\nand\n\n|l f (x) \u2212 lk f (x)| \u2264\n\ninfinitesimal generators\n\n(cid:3)\n\n|h|\u2265k\n\u2264 2(cid:21) f (cid:21)\u221e\n\n(cid:3)\n| f (x + h) \u2212 f (x)| n(dh)\n\u2264 c\u03b5(cid:21)(cid:2)g(cid:21)l1 .\n\nn(dh)\n\n|h|\u2265k\n\nstep 4. we complete the proof that the range of \u03bb \u2212 l is dense in b. since (cid:21)l f \u2212 lk f (cid:21) \u2264\nc\u03b5(cid:21)g(cid:21) by step 3 and (\u03bb \u2212 lk ) f = g, then\n\n(cid:21)(\u03bb \u2212 l) f \u2212 g(cid:21) \u2264 c\u03b5(cid:21)g(cid:21).\n\nbecause f \u2208 c \u2282 d(l) and \u03b5 is arbitrary, this proves the range of \u03bb \u2212 l is dense in c,\nhence in b.\nwe thus have l satisfying all the hypotheses of the hille\u2013yosida theorem, and hence there\nexists a semigroup pt mapping b into b. we again note that l1 = 0, hence pt = 1 for all t,\nand so by exercise 37.1, the pt are non-negative operators.\n\nexercises\n\n37.1 let b be either the space l2 with respect to a finite measure or else the continuous functions\nvanishing at infinity for some locally compact separable metric space s. in the former case, we\nsay f \u2265 0 if f (x) \u2265 0 for almost every x, in the latter case if f (x) \u2265 0 for all x. a semigroup is\nnon-negative if f \u2265 0 implies pt f \u2265 0 for all t \u2265 0. suppose that pt is a semigroup, the space\nb contains the constant functions, and pt1 = 1 for all t. show that pt is a contraction if and only\nif pt is non-negative.\n\n37.2 show that pt and r\u03bb commute and that\n\n(cid:3) \u221e\n\u2019\u2019\u2019 \u2264\n\n0\n\nptr\u03bb f =\n\ne\u03bbtpt f dt\n\ne\n\n\u2212\u03bbsps+t f ds.\n(cid:3)\n\n\u2212\u03bbt(cid:21)pt f (cid:21) dt.\n\nb\n\ne\n\na\n\nshow that for any a < b we have\u2019\u2019\u2019(cid:3)\n\na\n\nb\n\nhint: approximate r\u03bb f by a riemann sum.\n\n37.3 show that if pt is a contraction semigroup and r\u03bb is the resolvent, then\n\n(cid:21)r\u03bb(cid:21) \u2264 1/\u03bb.\n\n(37.18)\n37.4 show that if a is a bounded operator and tt = eta, then tt is a strongly continuous semigroup\n\nof operators with infinitesimal generator a. (we cannot assert that the tt are contractions.)\n\n37.5 prove that if l is dissipative, the domain of l is dense in b, and the range of \u03bb \u2212 l is dense in\nb, then l defined in (37.12) is a closed extension of l that is dissipative and the range of \u03bb \u2212 l\nis equal to b. show there is only one such closed extension of l.\n\n37.6 if the range of \u03bb\u2212 l equals b for a single value of \u03bb, then the range of \u03bb\u2212 l equals b for every\n\nvalue of \u03bb.\nhint: define r\u03bb as the inverse of \u03bb \u2212 l, then use( 37.3) to define ra for other values of a.\n\n "}, {"Page_number": 319, "text": "301\n37.7 let (xt , px ) be a markov process with transition probabilities given by pt f (x) = f (x + t ).\n\nexercises\n\ndetermine l and d(l).\n\n37.8 let pt be a strongly continuous semigroup of contraction operators and let l be the infinitesimal\n\ngenerator. show that d(ln ) is dense in b for every positive integer n.\n\n37.9 this is a continuation of exercise 36.5. prove that if f \u2208 c2 with compact support, pt is the\nsemigroup given in exercise 36.5, and l is the infinitesimal generator, then the fourier transform\n\nof l f is(cid:2)f (u)\u03c8 (u).\n\n37.10 suppose that pt is a strongly continuous semigroup, but not necessarily of contractions. thus\npt+s = ptps and pt f \u2192 f in norm if f \u2208 b, but we do not assume (cid:21)pt(cid:21) \u2264 1. prove that there\nexist constants k, b > 0 such that (cid:21)pt(cid:21) \u2264 kebt for all t \u2265 0.\nhint: use the uniform boundedness principle from functional analysis to prove there exists\nc, t0 such that (cid:21)pt(cid:21) \u2264 c if t \u2264 t0. then use the semigroup property.\n\n "}, {"Page_number": 320, "text": "38\n\ndirichlet forms\n\n(cid:3)\n\n(cid:3)\n\nwhen constructing semigroups, it is sometimes easier to start with a bilinear form, called the\ndirichlet form, than to work with the infinitesimal generator, and to construct the semigroup\nfrom the form. for example, let \u0001 be the laplacian. if f , g \u2208 c2 with compact support, then\nintegration by parts shows\n\nrd\n\nif we write\n\nwe thus have\n\nf (x)( 1\n2\n\n\u0001g)(x) dx = 1\n\n2\n\n(cid:3)\n\n= \u2212 1\n\n2\n\nd(cid:9)\n\ni=1\n\ne ( f , g) = 1\n\n2\n\n(cid:3)\n\n(cid:3)\n\nf (x)\n\nd(cid:9)\nd(cid:9)\n\ni=1\n\u2202 f\n\u2202xi\n\nrd\n\ni=1\n\n(cid:3)\n\nrd\n\n\u2202 2g\n\u2202x2\ni\n\n(x) dx\n\n(x)\n\n\u2202g\n\u2202xi\n\n(x) dx.\n\n\u2202 f\n\u2202xi\n\n(x)\n\n\u2202g\n\u2202xi\n\n(x) dx,\n\n(38.1)\n\nclearly e ( f , g) is symmetric in f and g, so\n\nf ( 1\n2\n\nrd\n\n\u0001g) = \u2212e ( f , g).\n(cid:3)\n\n\u0001g) = \u2212e ( f , g) = \u2212e (g, f ) =\n\nf ( 1\n2\n\nrd\n\nif r\u03bb is the resolvent for brownian motion, (38.1) and the fact that 1\n2\nus that\n\ne (r\u03bb f , g) + \u03bb\n\ng( 1\n2\n\nrd\n\n(cid:3)\n\n\u0001 f ) dx.\n\u0001r\u03bb f = \u03bbr\u03bb f \u2212 f tells\n(cid:3)\n\n(r\u03bb f )g\n\n(38.2)\n\n\u0001r\u03bb f )g + \u03bb\n( 1\n2\n(\u03bbr\u03bb f \u2212 f )g + \u03bb\n\n(r\u03bb f )g\n\n(cid:3)\n\n(cid:3)\n(cid:3)\n\nf g.\n\n(r\u03bb f )g = \u2212\n(cid:3)\n= \u2212\n=\n\nthe bilinear form e ( f , g) makes sense even if f , g are only in c1 with compact support,\nwhich is one major advantage of the dirichlet form. since e is clearly linear in each variable,\nwe have\n\ne ( f , g) = 1\n\n2 [e ( f + g, f + g) \u2212 e ( f , f ) \u2212 e (g, g)],\n\n302\n\n "}, {"Page_number": 321, "text": "38.1 framework\n\n303\nso to specify the dirichlet form, it is only necessary to know e ( f , f ), a number, rather than\nl f , a function. one disadvantage of dirichlet forms is that one needs a self-adjoint operator,\nand not every infinitesimal generator is self-adjoint. another disadvantage is that when\nworking with dirichlet forms, l2 is the natural space to work with, which means there are\nnull sets one has to worry about. in particular, the construction of chapter 36 is not directly\napplicable, because there we required our banach space to be the set of continuous functions\nvanishing at infinity. (modifications of the methods in chapter 36 do work, however.)\n\n38.1 framework\n\nlet us now suppose s is a locally compact separable metric space together with a \u03c3 -finite\nmeasure m defined on the borel subsets of s. we want to give a definition of the dirichlet\nform in this more general context. we suppose there exists a dense subset d = d(e ) of\nl2(s, m) and a non-negative bilinear symmetric form e defined on d \u00d7 d, which means\n\ne ( f , g) = e (g, f ),\ne (a f , g) = ae ( f , g),\n(cid:15)\n\nfor f , g, h \u2208 d, a \u2208 r.\n\nwe will frequently write (cid:22) f , g(cid:23) for\n\ne ( f + g, h) = e ( f , h) + e (g, h)\ne ( f , f ) \u2265 0\n\nf (x)g(x) m(dx). for a > 0 define\n\nea( f , f ) = e ( f , f ) + a(cid:22) f , f (cid:23).\n\nwe can define a norm on d using the inner product ea: the norm of f equals (ea( f , f ))1/2;\nwe call this the norm induced by ea. since a(cid:22) f , f (cid:23) \u2264 ea( f , f ), then\nea( f , f ) \u2264 eb( f , f ) = ea( f , f ) + (b \u2212 a)(cid:22) f , f (cid:23)\n\n(cid:10)\n\n\u2264\n\n1 + b \u2212 a\n\na\n\n(cid:11)\nea( f , f )\n\nif a < b, so the norms induced by different a\u2019s are all equivalent. we say e is closed if d\nis complete with respect to the norm induced by ea for some a. equivalently, e is closed if\nwhenever un \u2208 d satisfies e1(un \u2212 um, un \u2212 um) \u2192 0 as n, m \u2192 \u221e, then there exists u \u2208 d\nsuch that e (un \u2212 u, un \u2212 u) \u2192 0 as n \u2192 \u221e.\nwe saye is markovian if whenever u \u2208 d, then v = 0\u2228(u\u22271) \u2208 d ande (v, v) \u2264 e (u, u).\n(a slightly weaker definition of markovian is sometimes used.) a dirichlet form is a non-\nnegative bilinear symmetric form that is closed and markovian.\nabsorbing brownian motion on [0,\u221e) is a symmetric process. the corresponding dirich-\n\nlet form is\n\ne ( f , f ) = 1\n\n2\n\n| f\n\n(cid:3)(x)|2 dx,\n\n(cid:3) \u221e\n\n0\n\nand the appropriate domain turns out to be the completion of the set of c1 functions with\ncompact support contained in (0,\u221e) with respect to the norm induced by e1. in particular,\nany function with compact support contained in (0,\u221e) will be zero in a neighborhood of\n0. in a domain d in higher dimensions, the dirichlet form for absorbing brownian motion\nbecomes\n\n(cid:3)\n\ne ( f , f ) = 1\n\n2\n\n|\u2207 f (x)|2 dx,\n\n(38.3)\n\n "}, {"Page_number": 322, "text": "304\n\ndirichlet forms\n\nwith the domain of e being the completion with respect to e1 of the c1 functions whose\nsupport is contained in the interior of d.\nreflecting brownian motion is also a symmetric process. for a domain d, the dirichlet\nform is given by (38.3) and the domain d(e ) of the form is given by the completion with\nrespect to the norm induced by e1 of the c1 functions on d with compact support, where d\nis the closure of d. one might expect there to be some restriction on the normal derivative\n\u2202 f /\u2202n on the boundary of d, but in fact there is no such restriction. to examine this further,\nconsider the case of d = (0,\u221e). if one takes the class of functions f which are c1 with\n(cid:3)(0) = 0 and takes the closure with respect to the norm induced\ncompact support and with f\nby e1, one gets the same class as d(e ); this is exercise 38.1.\none nice consequence of the fact that we don\u2019t need to impose a restriction on the normal\nderivative in the domain of e for reflecting brownian motion is that this allows us to define\nreflecting brownian motion in any domain, even when the boundary is not smooth enough\nfor the notion of a normal derivative to be defined.\n\n38.2 construction of the semigroup\n\nwe now want to construct the resolvent corresponding to a dirichlet form. the motivation\ngiven in( 38.2) shows we should expect\n\nea(ra f , g) = (cid:22) f , g(cid:23)\n\n(38.4)\n\nfor all a > 0 and all f , g such that ra f , g \u2208 d. our banach space b will be l2(s, m).\ntheorem 38.1 if e is a dirichlet form, there exists a family of resolvent operators {r\u03bb} such\nthat\n\n(1) the r\u03bb satisfy the resolvent equation,\n(2) (cid:21)\u03bbr\u03bb(cid:21) \u2264 1 for all \u03bb > 0,\n(3) \u03bbr\u03bb f \u2192 f as \u03bb \u2192 \u221e for f \u2208 b, and\n(4) ea(ra f , g) = (cid:22) f , g(cid:23) if a > 0, ra f , g \u2208 d.\nmoreover, if f \u2208 b satisfies 0 \u2264 f (x) \u2264 1, m-a.e., then for all a > 0\n\n0 \u2264 ara f \u2264 1,\n\n(38.5)\nproof fix f \u2208 b and define a linear functional on b by i (g) = (cid:22) f , g(cid:23). this functional is\nalso a bounded linear functional on d with respect to the norm induced by ea, that is, there\nexists c such that |i (g)| \u2264 cea(g, g)1/2. this follows because\n\nm-a.e.\n\n(cid:20)(cid:20)(cid:20)(cid:3)\n\n(cid:20)(cid:20)(cid:20) \u2264 (cid:22) f , f (cid:23)1/2(cid:22)g, g(cid:23)1/2 \u2264 (cid:22) f , f (cid:23)1/2( 1\n\nf g\n\n|i (g)| =\n\nea(g, g))1/2\n\na\n\nby the cauchy\u2013schwarz inequality. since e is closed, d is a hilbert space with respect to\nthe norm induced by ea. by the riesz representation theorem for hilbert spaces (see, e.g.,\nfolland (1999), theorem 5.25), there exists a unique element u \u2208 d such that i (g) = ea(u, g)\nfor all g \u2208 d. we set ra f = u. in particular, (38.4) holds, and ra f \u2208 d.\n\n "}, {"Page_number": 323, "text": "we show the resolvent equation holds. if g \u2208 d,\n\n38.2 construction of the semigroup\n\n305\n\nea(ra f \u2212 rb f , g) = ea(ra f , g) \u2212 e (rb f , g) \u2212 a(cid:22)rb f , g(cid:23)\n= (cid:22) f , g(cid:23) \u2212 e (rb f , g) \u2212 b(cid:22)rb f , g(cid:23) + (b \u2212 a)(cid:22)rb f , g(cid:23)\n= (cid:22) f , g(cid:23) \u2212 eb(rb f , g) + (b \u2212 a)(cid:22)rb f , g(cid:23)\n= (b \u2212 a)(cid:22)rb f , g(cid:23)\n= ea((b \u2212 a)rarb f , g).\n\nsince this holds for all g \u2208 d and d is dense in b, then ra f \u2212 rb f = (b \u2212 a)rarb f .\nnext we show that (cid:21)ara f (cid:21) \u2264 (cid:21) f (cid:21), or equivalently,\n\n(cid:22)ara f , ara f (cid:23) \u2264 (cid:22) f , f (cid:23).\n\n(38.6)\n\nif (cid:22)ra f , ra f (cid:23) is zero, then (38.6) trivially holds, so suppose it is positive. we have\n\na(cid:22)ra f , ra f (cid:23) \u2264 ea(ra f , ra f ) = (cid:22) f , ra f (cid:23) \u2264 (cid:22) f , f (cid:23)1/2(cid:22)ra f , ra f (cid:23)1/2\n\nby (38.4) and the cauchy\u2013schwarz inequality. if we now divide both sides by (cid:22)ra f , ra f (cid:23)1/2\nand then square both sides, we obtain (38.6).\nwe show that brb f \u2192 f as b \u2192 \u221e when f \u2208 b. if f \u2208 d, then by the cauchy\u2013schwarz\ninequality and (38.6)\n\n(cid:22)brb f , f (cid:23) \u2264 (cid:22)brb f , brb f (cid:23)1/2(cid:22) f , f (cid:23)1/2\n\n\u2264 (cid:22) f , f (cid:23).\n\nusing this,\n\nb(cid:22)brb f \u2212 f , brb f \u2212 f (cid:23) \u2264 eb(brb f \u2212 f , brb f \u2212 f )\n\n= b2eb(rb f , rb f ) \u2212 2beb(rb f , f ) + eb( f , f )\n= b2(cid:22)rb f , f (cid:23) \u2212 2b(cid:22) f , f (cid:23) + e ( f , f ) + b(cid:22) f , f (cid:23)\n\u2264 e ( f , f ).\n\nnow divide both sides by b to get (cid:21)brb f \u2212 f (cid:21)2 \u2264 e ( f , f )/b \u2192 0 as b \u2192 \u221e. since d is\ndense in b and (cid:21)brb(cid:21) \u2264 1 for all b, we conclude brb f \u2192 f for all f \u2208 b.\nit remains to show 0 \u2264 brb f \u2264 1, m-a.e., if 0 \u2264 f \u2264 1, m-a.e. fix f \u2208 b with 0 \u2264 f \u2264 1,\n!\nm-a.e., and let a > 0. define a functional \u03c8 on d by\n\n\"\n\n\u03c8 (v) = e (v, v) + a\n\nv \u2212 f\na\n\n, v \u2212 f\na\n\n.\n\nwe claim\n\n\u03c8 (ra f ) + ea(ra f \u2212 v, ra f \u2212 v) = \u03c8 (v),\n\nv \u2208 d.\n\n(38.7)\n\n "}, {"Page_number": 324, "text": "306\n\ndirichlet forms\n\n!\n\nra f \u2212 1\na\n\nto see this, start with the left-hand side, which is equal to\ne (ra f , ra f ) + a\n\n\"\nf , ra f \u2212 1\nf\na\n= ea(ra f , ra f ) \u2212 2(cid:22)ra f , f (cid:23) + 1\n(cid:22) f , f (cid:23) + ea(ra f , ra f ) \u2212 2ea(ra f , v) + ea(v, v)\na\n= 1\na\n= \u03c8 (v).\n\n(cid:22) f , f (cid:23) \u2212 2(cid:22) f , v(cid:23) + e (v, v) + a(cid:22)v, v(cid:23)\n\n+ ea(ra f \u2212 v, ra f \u2212 v)\n\nif follows from (38.7) and the fact that ea(g, g) is non-negative for any g \u2208 d that ra f is the\nfunction that minimizes \u03c8.\nset \u03c6(x) = 0 \u2228 (x \u2227 (1/a)) and let w = \u03c6(ra f ). observe that |\u03c6(t ) \u2212 s| \u2264 |t \u2212 s| for\n\n(cid:20)(cid:20)(cid:20),\n\nand therefore\n\n(cid:20)(cid:20)(cid:20) \u2264\n!\n\n(cid:20)(cid:20)(cid:20)ra f (x) \u2212 f (x)\n\nt \u2208 r and s \u2208 [0, 1/a], so (cid:20)(cid:20)(cid:20)w(x) \u2212 f (x)\n\"\n\n\"\n, ra f \u2212 f\na\nsince e is markovian, then aw = 0 \u2228 ((ara f ) \u2227 1), which leads to\ne (ara f , ara f ) = e (ra f , ra f ).\n\nra f \u2212 f\na\n\n, w \u2212 f\na\n\nw \u2212 f\na\n\n\u2264\n\n!\n\na\n\na\n\n.\n\ne (w, w) \u2264 1\na2\n\n(38.9)\nadding (38.8) and (38.9), we conclude \u03c8 (w) \u2264 \u03c8 (ra f ). since ra f is the minimizer for \u03c8,\nthen w = ra f , m-a.e. but 0 \u2264 w \u2264 1/a, and hence ara f takes values in [0, 1], m-a.e.\n\nif we combine proposition 37.9 and theorem 38.1, we obtain a semigroup pt whose\n\nresolvent satisfies (38.4). we would like to know that the analog of (38.5) holds for pt.\ncorollary 38.2 if 0 \u2264 f \u2264 1, m-a.e., then 0 \u2264 pt f \u2264 1, m-a.e.\nproof\n0 \u2264 (brb)i f \u2264 1, m-a.e., for every i. using the notation of the proof of proposition 37.9,\n\nif 0 \u2264 f \u2264 1, m-a.e., then 0 \u2264 brb f \u2264 1, m-a.e, by theorem 38.1, and iterating,\n\n\u221e(cid:9)\n\nt f (x) = e\nqb\n\n(cid:12)\u221e\ni=0\nwhich will be non-negative, m-a.e., and bounded by e\nlimit as b \u2192 \u221e, we see that pt f takes values in [0, 1], m-a.e.\n\n(bt )i(brb)i f (x)/i!,\n\ni=0\n\n\u2212bt\n\n\u2212bt\n\n(bt )i/i!, m-a.e. passing to the\n\nwhen it comes to using the semigroup pt derived from a dirichlet form to construct a\nmarkov process x , there is a difficulty that we did not have before. since pt is constructed\nusing an l2 procedure, pt f is defined only up to almost everywhere equivalence. without\nsome continuity properties of pt f for enough f \u2019s, we must neglect some null sets. if the only\nnull sets we could work with were sets of m-measure 0, we would be in trouble. for example,\nwhen s is the plane and m is a two-dimensional lebesgue measure, the x axis has measure\nzero, but a continuous process will (in general) hit the x axis. fortunately there is a notion\nof sets of capacity zero, which are null sets that are smaller than sets of measure zero. it is\n\n(38.8)\n\n "}, {"Page_number": 325, "text": "38.3 divergence form elliptic operators\n\n307\npossible to construct a process x starting from all points x in s except for those in a set n\nof capacity zero and to show that starting from any point not in n , the process never hits n .\nthere is another difficulty when working with dirichlet forms. in general, one must look\n\nat (cid:14)s, a certain compactification of s, which is a compact set containing s. even when our\nstate space is a domain in rd,(cid:14)s is not necessarily equal to s, the euclidean closure of s, and\none must work with (cid:14)s instead of s. it can be shown that this problem will not occur if the\ndirichlet form is regular. let ck be the set of continuous functions with compact support. a\ndirichlet form e is regular if d \u2229 ck is dense in d with respect to the norm induced by e1\nand d \u2229 ck also is dense in ck with respect to the supremum norm.\n\nwe want to show how to construct the markov process corresponding to the operator\n\n38.3 divergence form elliptic operators\n\n(cid:10)\n\nl f (x) = d(cid:9)\n\ni, j=1\n\n\u2202\n\u2202xi\n\nai j (\u00b7)\n\n(\u00b7)\n\n\u2202 f\n\u2202x j\n\n(cid:11)\n\n(x).\n\n(38.10)\n\nif the ai j\u2019s are smooth in x, this can be interpreted as first calculating the partial derivative of f\nwith respect to x j, multiplying the result by ai j (x), taking the partial derivative of the product\nwith respect to xi, and then summing over i and j. if, however, the ai j\u2019s are only bounded\nand measurable, one cannot even in general give any nontrivial examples of functions in\nthe domain of l. here is where dirichlet forms are the perfect tool. operators of the form\n(38.10) are known as elliptic operators in divergence form or in variational form, and the\nstudy of their properties has a long history in pde.\nwe assume ai j (x) = a ji(x) for each i and j and each x. we suppose the ai j (x) are\nmeasurable functions and are uniformly bounded in x for each i and j. we also require\nuniform ellipticity: there exists \u0001 such that\n\nd(cid:9)\n\ni, j=1\n\nai j (x)yiy j \u2265 \u0001\n\n,\n\ny2\ni\n\n(y1, . . . , yd ) \u2208 rd .\n\njust as in the nondivergence elliptic operator case, the matrix whose (i, j)th element is ai j (x)\nis positive definite, uniformly in x.\n\nwe will shortly define a dirichlet form, but let us first specify a domain. let c1\n\nk be the\ncollection of c1 functions with compact support, and define h 1 to be the completion of c1\nk\nwith respect to the norm\n\n(cid:21) f (cid:21)h 1 =\n\n(| f (x)|2 + |\u2207 f (x)|2) dx\n\n.\n\n(38.11)\n\n(cid:11)1/2\n\none can show that h 1 with this norm is a banach space; this is exercise 38.2.\n\nnow for f \u2208 c1\n\nk define\n\ne ( f , f ) =\n\nai j (x)\n\n\u2202 f\n\u2202xi\n\n(x)\n\n\u2202 f\n\u2202x j\n\n(x) dx.\n\n(38.12)\n\nd(cid:9)\n\ni=1\n\n(cid:10)(cid:3)\n\n(cid:3)\n\nd(cid:9)\n\nrd\n\ni, j=1\n\n "}, {"Page_number": 326, "text": "308\n\ndirichlet forms\n\nk is dense in h 1 to extend the definition of e to all of h 1 \u00d7 h 1. the\nwe can use the fact that c1\nconnection with the operator l is that when the ai j are smooth, integration by parts yields\n\n(cid:3)\n\n(l f )g dx = \u2212e ( f , g)\n\nbecause of the boundedness and uniform ellipticity, there exist positive constants c1 and\n\nif g is c1 with compact support; cf. (38.1).\n\nc2 not depending on f such that\n\n(cid:3)\n\n(cid:3)\n\n|\u2207 f (x)|2 dx \u2264 e ( f , f ) \u2264 c2\n\n|\u2207 f (x)|2 dx.\n\nc1\n\nlet \u03c6(x) = (0 \u2228 x) \u2227 1. for each \u03b5 > 0 let \u03c6\u03b5 be c\n\ntherefore the norm induced by e1 and the norm in h 1 are equivalent. this implies e is\nclosed. by the definition of h 1, e is regular, and clearly e is symmetric. thus we need only\nto show that e is markovian.\nwith (cid:21)\u03c6(cid:3)\n(cid:3)\npointwise as \u03b5 \u2192 0. note \u2207\u03c6\u03b5 ( f ) = \u03c6(cid:3)\n\n\u03b5(cid:21)\u221e \u2264 1, and such that \u03c6\u03b5 (x) \u2192 \u03c6(x) uniformly in x as \u03b5 \u2192 0 and \u03c6(cid:3)\n\n, bounded, agreeing with \u03c6 on [0, 1],\n\u03b5 (x) \u2192 1[0,1](x)\n\n\u03b5 ( f )\u2207 f , so if f \u2208 c1\nk,\n\n\u221e\n\n(\u03c6(cid:3)\n\n\u03b5 ( f )(x))2ai j (x)\n\n\u2202 f\n\u2202xi\n\n\u2202 f\n\u2202x j\n\n(x)\n\n(x) dx.\n\n(38.13)\n\ne (\u03c6\u03b5 ( f ), \u03c6\u03b5 ( f )) = d(cid:9)\n\ni, j=1\n\nsince\n\nd(cid:9)\n\ni, j=1\n\nai j (x)\n\n\u2202 f\n\u2202xi\n\n(x)\n\n\u2202 f\n\u2202x j\n\n(x) \u2265 \u0001|\u2207 f (x)|2 \u2265 0\n\nand |\u03c6(cid:3)\n\n\u03b5 ( f )(x)| \u2264 1, we see that\n\ne (\u03c6\u03b5 ( f ), \u03c6\u03b5 ( f )) \u2264 e ( f , f ).\n\ntaking the limit as \u03b5 \u2192 0 in( 38.13) we obtain\n\ne (\u03c6( f ), \u03c6( f )) \u2264 e ( f , f ) < \u221e.\n\n(38.14)\nin particular, \u03c6( f ) \u2208 h 1 = d(e ). we now pass to the limit to show that (38.14) holds for\nall f \u2208 h 1, which says that e is markovian.\nwe can therefore apply theorem 38.1 to obtain a semigroup corresponding to the dirichlet\nform e. as mentioned earlier, there is potentially a problem in that the semigroup is only\ndefined for points not in a certain null set. however, a famous result of nash and of degiorgi\nf (y)p(t, x, y) dy with p(t, x, y)\nh\u00a8older continuous in x and y; see bass (1997), chapter vii for a presentation of this result.\nthis allows us to take the null set to be empty and to see that our semigroup satisfies the\nassumptions of chapter 36. therefore there exists a strong markov process having pt as its\nsemigroup.\n\nshows that the semigroup pt can be written as pt f (x) =(cid:15)\n\n "}, {"Page_number": 327, "text": "exercises\n\n309\n\n38.1 let f1 = { f \u2208 c1[0,\u221e) :\n\nf has compact support} and f2 = f1 \u2229 { f \u2208 c1[0,\u221e) :\n(cid:3)(0) = 0}. show that the closures of f1 and f2 with respect to the\n\nf has compact support, f\nnorm (\n\n(| f (x)|2 + | f\n\n(cid:3)(x)|2 ) dx)1/2 are the same.\n\n(cid:15)\n\nexercises\n\n38.2 if h 1 is the completion of c1\n\nk, the c1 functions on rd with compact support, relative to the\n\nnorm given by (38.11), show h 1 is a hilbert space.\n\n38.3 show that the resolvent operator r\u03bb defined in theorem 38.1 is a symmetric operator, that is, if\n\nf , g \u2208 b, then (cid:22)r\u03bb f , g(cid:23) = (cid:22) f , r\u03bbg(cid:23).\n\n38.4 show that if the resolvent operator r\u03bb is a symmetric operator, then the transition operators pt\n\nare also symmetric: if f , g \u2208 b, then (cid:22)pt f , g(cid:23) = (cid:22) f , ptg(cid:23).\n\n38.5 to do the next few exercises, you will have to know some functional analysis, specifically, the\nspectral theorem for self-adjoint operators. see lax (2002).\nlet e be a dirichlet form with domain d(e ) and let l be the infinitesimal generator of the\nsemigroup pt that corresponds to l. let e (d\u03bb) be a spectral resolution of the identity for \u2212l.\n(the operator l is a negative operator, so \u2212l is a positive one.) then a consequence of the\nspectral theorem is that\n\nra f =\n\n0\n\n(cid:3) \u221e\npt f =\n(cid:3) \u221e\n(cid:3) \u221e\n(cid:3) \u221e\n\n0\n\n0\n\n(cid:22) f , g(cid:23) =\n\n\u2212\u03bbt e (d\u03bb) f\n\ne\n\n1\na + \u03bb\n\ne (d\u03bb) f .\n\n(cid:22)e (d\u03bb) f , g(cid:23).\n\nand\n\nalso\n\nshow that if f , g \u2208 d, then\n\ne ( f , g) =\nhint: first prove it for f = rah. write\n\n0\n\ne (rah, g) = (cid:22)h, g(cid:23) \u2212 a(cid:22)rah, g(cid:23) =\n\n(cid:3) \u221e\n\n0\n\n=\n\n(cid:22)e (d\u03bb)h, g(cid:23) =\n\n\u03bb\n\na + \u03bb\n\n0\n\n\u03bb(cid:22)e (d\u03bb) f , g(cid:23).\n\n(cid:3) \u221e\n\n0\n\n(cid:10)\n(cid:3) \u221e\n\n(cid:11)\n\n1 \u2212 a\n(cid:22)e (d\u03bb)h, g(cid:23)\na + \u03bb\n\u03bb(cid:22)e (d\u03bb)(rah), g(cid:23).\n\nto extend this to all f in the domain of e, use the fact that e is closed.\n\n38.6 if l is the infinitesimal generator of the semigroup associated with the dirichlet form e, show\n\nthat d(\n\n\u221a\u2212l) = d(e ).\n\n38.7 show that if f \u2208 d(e ), then ara f converges to f with respect to the norm induced by e1.\n38.8 show that if b > 0, then {rb f\n\n: f \u2208 l2} is a dense subset of d(e ) with respect to the norm\n\n38.9 show that {pt f : f \u2208 l2, t > 0} is a dense subset of d(e ) with respect to the norm induced by\n\ninduced by e1.\n\ne1.\n\n "}, {"Page_number": 328, "text": "310\n38.10 this exercise shows how to approximate e by forms whose domain is all of b. let\n\ndirichlet forms\n\ne (t )( f , g) = 1\nt\n\n(cid:22) f \u2212 pt f , g(cid:23).\n\nshow that if f \u2208 d(e ), then e (t )( f , f ) increases to e ( f , f ). show that if f , g \u2208 d(e ), then\ne (t )( f , g) converges to e ( f , g).\n\n38.11 show that if u \u2208 d(e ), then |u| \u2208 d(e ) and e (|u|,|u|) \u2264 e (u, u).\n\nhint: use exercise 38.10.\n\n38.12 use exericse 38.11 to show that if u \u2208 d(e ), then e (u\n38.13 suppose {pt} are the transition probabilities corresponding to a dirichlet form e. suppose there\n\n\u2212 ) \u2264 0.\n\n+, u\n\nexist functions pt (x, y) such that for each t,\npt f (x) =\n\npt (x, y) m(dy)\n\n(cid:3)\n\nfor almost every x. prove that for almost every pair (x, y) with respect to the product measure\nm \u00d7 m, pt (x, y) = pt (y, x).\n\n38.14 let f \u2208 l2(m) and define the functional\n\n\u03c8 (u) = e (u, u) + \u03bb(cid:22)u, u(cid:23) \u2212 2(cid:22) f , u(cid:23)\n\nfor u in the domain of e. prove that \u03c8 is minimized by u = r\u03bb f , and that this function is the\nunique minimizer.\n\n38.15 let pt be the semigroup associated with a dirichlet form and define\n\nj (dx, dy) = pt (x, dy) m(dx).\n\n(1) prove that if f , g are continuous with compact support, then\n\n(cid:3) (cid:3)\n\n(cid:3) (cid:3)\n\ng(x) f (y) j (dx, dy).\n\n(2) with f and g continuous with compact support, prove that\nf (x)g(y) j (dx, dy) = (cid:22) f , ptg(cid:23)\n\nf (x)g(y) j (dx, dy) =\n(cid:3)\n(cid:3) (cid:3)\n\nand\n\n(3) let k(x) = 1 \u2212 pt1(x). prove that if e (t ) is defined as in exercise 38.10, then\n2te (t )( f , g) =\n\n( f (x) \u2212 f (y))(g(x) \u2212 g(y)) j (dx, dy) +\n\nf (x)g(x)k(x) m(dx).\n\n(cid:3) (cid:3)\n\nf (x)g(x) j (dx, dy) = (cid:22) f g, pt1(cid:23).\n(cid:3)\n\n(4) is e (t ) a dirichlet form? a regular dirichlet form?\n\n "}, {"Page_number": 329, "text": "notes\n\n311\n\n38.16 this is a continuation of the previous exercise. if f is a function on the state space, we say that\ng is a normal contraction of f if |g(x)| \u2264 | f (x)| for all x and |g(x) \u2212 g(y)| \u2264 | f (x) \u2212 f (y)| for\nall x and y. as an example, note that if g(x) = \u22121 \u2228 ( f (x) \u2227 1), then g is a normal contraction\nof f . prove that if f \u2208 d(e ), where e is a dirichlet form and g is a normal contraction of f ,\nthen for each t > 0,\n\ne (t )(g, g) \u2264 e (t )( f , f ) \u2264 e ( f , f ).\n\nsee fukushima et al. (1994) for further information.\n\nnotes\n\n "}, {"Page_number": 330, "text": "39\n\nmarkov processes and sdes\n\none common way of constructing markov processes is via stochastic differential equations.\nroughly speaking, if there is uniqueness for every starting point, then one can create a\nstrong markov process. after proving this, we establish a connection between stochastic\ndifferential equations and partial differential equations, and then we describe what is known\nas the martingale problem.\n\n39.1 markov properties\n\nlet p be a probability and suppose w is a d-dimensional brownian motion with respect to\np. consider the sde\n\ndxt = \u03c3 (xt ) dwt + b(xt ) dt.\n\n(39.1)\nhere \u03c3 is a d \u00d7 d matrix-valued function and b is a vector-valued function, both borel\nmeasurable and bounded. this can be written in terms of components as\ni = 1, . . . , d,\n\n= d(cid:9)\n\nt + bi(xt ) dt,\n\n\u03c3i j (xt ) dw j\n\ndx i\nt\n\nj=1\n\nwhere w = (w 1, . . . , w d ). let x x\nt be the solution to (39.1) when x0 = x. let px be the law\nof x x\nt .\nlet \u0001 = c[0,\u221e), let f be the cylindrical subsets of \u0001, and define zt (\u03c9) = \u03c9(t ). the\nmain result of this section is that if weak existence and weak uniqueness hold for (39.1) for\nevery starting point x, then the solutions (zt , px) form a strong markov process.\n\nwe begin by considering regular conditional probabilities.\n\ndefinition 39.1 let (\u0001,f , p) be a probability space, and let e be a \u03c3 -field contained in f.\na regular conditional probability for e [\u00b7 | e] is a kernel q(\u03c9, d\u03c9(cid:3)) such that\n\n(1) q(\u03c9,\u00b7) is a probability measure on (\u0001,e ) for each \u03c9;\n(2) for each a \u2208 f, q(\u00b7, a) is a random variable that is measurable with respect to f;\n(3) for each a \u2208 f and each b \u2208 e,\n\n(cid:3)\n\nq(\u03c9, a) p(d\u03c9) = p(a \u2229 b).\n\nb\n\nregular conditional probabilities need not always exist, but if the probability space has\nsufficient structure, then they do. we provide a proof in the appendix; see theorem c.1.\nq(\u03c9, a) can be thought of as p(a | e )(\u03c9), regularized so as to have some joint measurability.\n\n312\n\n "}, {"Page_number": 331, "text": "39.1 markov properties\n\n313\n\nrecall that the definition of minimal augmented filtration for a markov process was given in\nsection 20.1.\n\ntheorem 39.2 suppose weak existence and weak uniqueness hold for the sde (39.1) when-\never x0 is a random variable that is in l2 and is measurable with respect to f0. suppose the\nmatrix \u03c3 (y) is invertible for each y. let (\u0001,f , p) be defined as above. let px be the law\nof the weak solution when x0 is identically equal to x. let {ft} be the minimal augmented\nfiltration generated by z. then (px, zt ) is a strong markov process.\n\nproof we will prove that if t is a bounded stopping time and f is a bounded and borel\nmeasurable function on rd, then\n\ne x[ f (zt+t ) | ft ] = e zt f (zt ),\n\na.s.\n\n(39.2)\n\nas in section 20.3, this is sufficient to get the strong markov property.\n\nfix x. let\n\nand\n\n(cid:3)\n\nyt = zt \u2212\n(cid:3)\n\n(cid:3)\n\n=\n\nw\nt\n\n0\n\nt\n\n0\n\nb(zr ) dr\n\nt\n\n\u03c3 \u22121(zr ) dyr.\n\n(39.3)\n\n(39.4)\n\nzt = z0 +\n(cid:3)\n(cid:14)zt =(cid:14)z0 +\n\nzt+t \u2212 zt =\n\n(cid:3)\n\n(cid:3)\n\nsince the px law of zt is the same as the p law of x x\nthe p law of w , or in other words, w\nand (39.4), we have the equation\n\n(cid:3)\n\nis the same as\nis a brownian motion under px. rearranging (39.3)\n\nt , then the px law of w\n\nt\n\nlet q be a regular conditional probability for e x[\u00b7 | ft ]. let(cid:14)zt = zt+t and(cid:14)wt = w\n\n(39.5)\n\u2212w\n(cid:3)\nt .\nusing (39.5) with t replaced by t +t and then with t replaced by t , and taking the difference,\nwe obtain\n\nb(zr ) dr.\n\n(cid:3)\n\u03c3 (zr ) dw\nr\n\n(cid:3)\nt+t\n\n0\n\n0\n\nt\n\n+\n\n(cid:3)\n\n(cid:3)\n\nt+t\n\nt+t\n\nt\n\n(cid:3)\n\n\u03c3 (zr ) dwr +\n(cid:3)\n\n\u03c3 ((cid:14)zr )(cid:14)wr +\n\nt\n\nb(zr ) dr,\n\nt\n\nb((cid:14)zr ) dr.\n\nt\n\nand hence\n\nwe will show in a moment that (cid:14)w is a brownian motion with respect to q(\u03c9,\u00b7) for\npx-almost all \u03c9. thus except for \u03c9 in a px-null set, (39.6) implies that under q(\u03c9,\u00b7),(cid:14)z is a\nsolution to (39.1) with starting point(cid:14)z0 = zt (\u03c9). if e q denotes the expectation with respect\n\n(39.6)\n\n0\n\n0\n\nto q, the weak uniqueness tells us that\n\ne q f ((cid:14)zt ) = e zt f (zt ),\n\npx(d\u03c9)-a.s.\n\n(39.7)\n\non the other hand,\n\ne q f ((cid:14)zt ) = e q f (zt+t ) = e x[ f (zt+t ) | ft ],\n\ncombining (39.7) and (39.8) proves( 39.2).\n\npx(d\u03c9)-a.s.\n\n(39.8)\n\n "}, {"Page_number": 332, "text": "314\n\nit remains to prove that under q the process (cid:14)w is a brownian motion. q(\u03c9,\u00b7) is a\n\n, so t \u2192 (cid:14)wt is continuous for every \u03c9(cid:3)\n\n. let t1 < \u00b7\u00b7\u00b7 < tn and\n\nprobability measure on \u0001(cid:3)\n\nmarkov processes and sdes\n\nn (u2, . . . , un, t1, . . . , tn) =\n\n\u03c9 : e q exp\n\nu j (w\n\n(cid:3)\nt+t j\n\n(cid:3)\nt+t j\u22121\n\n)\n\n(cid:11)\n\n\u2212 w\n(cid:11)(cid:19)\n\n.\n\n(cid:10)\n\nn(cid:9)\n\ni\n\nj=2\n\n(cid:18)\n\n(cid:10)\n\n\u2212 n(cid:9)\n(cid:16)\n\nj=2\n\nexp\n\n(cid:16)= exp\n(cid:11)\n\n= e w\n\n(cid:3)\nt exp\n\n(cid:10)\n\n= exp\n\nj=2\n\n(cid:17)\n\n(cid:11)\nand the definition of q,\n| ft\n(cid:11)\n\n(cid:3)\nt+t j\u22121\n\n\u2212 w\n\n)\n\n(cid:3)\n\n|u j|2(t j \u2212 t j\u22121)/2\n(cid:10)\nn(cid:9)\n(cid:10)\nn(cid:9)\n\u2212 n(cid:9)\n\n(cid:3)\nt+t j\n\n(cid:3)\nt+t j\n\nu j (w\n\nu j (w\n\nj=2\n\ni\n\ni\n\nj=2\n|u j|2(t j \u2212 t j\u22121)/2\n\n\u2212 w\n\n(cid:3)\nt+t j\u22121\n\n)\n\n(cid:11)\n\n,\n\nby the strong markov property of the brownian motion w\n\n(cid:10)\n\nn(cid:9)\n\ne q exp\n\ni\n\nj=2\n\nu j (w\n\n(cid:3)\nt+t j\n\n\u2212 w\n\n(cid:3)\nt+t j\u22121\n\n)\n\n= e\n\n(cid:10)\n\nn(cid:9)\n\nt1 < . . . < tn rational. therefore n is a px-null set.\n\nwhere the second equality holds almost surely, that is, except for a px-null set of \u03c9\u2019s. this\nshows that n (u2, . . . , un, t1, . . . , tn) is a null set with respect to px.\nlet n be the union of all such n (u1, . . . , un, t1, . . . , tn) for n \u2265 1, u1, . . . , un rational, and\n(cid:10)\nsuppose \u03c9 /\u2208 n. by the continuity of the paths of w\n\n\u2212 n(cid:9)\nfor all t, . . . , tn \u2208 [0,\u221e) and u2, . . . , un \u2208 r. thus the finite-dimensional distributions of(cid:14)w\nunder qt (\u03c9,\u00b7) are those of a brownian motion. by the continuity of (cid:14)w and theorem 2.6,\nunder qt ,(cid:14)w is a brownian motion, except for a null set of \u03c9\u2019s.\n\n|u j|2(t j \u2212 t j\u22121)/2\n\n= exp\n\n(cid:3)\nt+t j\u22121\n\ne q exp\n\n\u2212 w\n\n(cid:3)\nt+t j\n\nu j (w\n\n(cid:11)\n\n(cid:11)\n\nj=2\n\nj=2\n\n)\n\ni\n\n,\n\n(cid:3)\n\nby a slight abuse of notation, we will say (xt , px) is a strong markov family when (zt , px)\n\nis a strong markov family.\n\n39.2 sdes and pdes\n\nthe connection between stochastic differential equations and partial differential equations\ncomes about through the following theorem, which is simply an application of it\u02c6o\u2019s formula.\nlet l be the operator on functions in c2 defined by\n\nd(cid:9)\n\ni, j=1\n\n(x) + d(cid:9)\n\ni=1\n\nl f (x) = 1\n\n2\n\nai j (x)\n\n\u2202 2 f\n\u2202xi\u2202x j\n\nbi(x)\n\n\u2202 f\n\u2202xi\n\n(x).\n\n(39.9)\n\n "}, {"Page_number": 333, "text": "39.3 martingale problems\n\n315\n\ntheorem 39.3 suppose xt is a solution to (39.1), \u03c3 and b are bounded and borel measurable,\nand a = \u03c3 \u03c3 t . suppose f \u2208 c2. then\n(cid:3)\n\nf (xt ) = f (x0) + mt +\n\nl f (xs) ds,\n\n(39.10)\n\nwhere\n\n0\n\nt\n\n(cid:3)\n\n(xs)\u03c3i j (xs) dw j\ns\n\n(39.11)\n\nd(cid:9)\n\nt\n\n0\n\ni, j=1\n\n\u2202 f\n\u2202xi\n\nmt =\n\nis a local martingale.\n\nproof since the components of the brownian motion wt are independent, we have\nd(cid:22)w k, w (cid:14)(cid:23)\n\n= 0 if k (cid:16)= (cid:14); see exercise 9.4. therefore\n\nt\n\n(cid:9)\n\n(cid:14)\n\n(cid:9)\n(cid:9)\n\nk\n\nd(cid:22)x i, x j(cid:23)\n\nt\n\n=\n\n=\n\n(cid:3)\n\n(cid:9)\n\n\u03c3ik (xt )\u03c3 t\nk j\n\nt\n\n\u03c3ik (xt )\u03c3 jl (xt ) d(cid:22)w k, w (cid:14)(cid:23)\n(xt ) dt = ai j (xt ) dt.\n(cid:9)\n\n(cid:3)\n\nt\n\nk\n\n(xs) dx i\ns\n\n+ 1\n\n2\n\nwe now apply it\u02c6o\u2019s formula:\n\nf (xt ) = f (x0) +\n\n0\n\n\u2202 f\n\u2202xi\n\n(cid:3)\n\nt\n\n(cid:9)\n(cid:3)\n\ni\n\nt\n\ni\n\n= f (x0) + mt +\n\n= f (x0) + mt +\n\nl f (xs) ds,\n\n0\n\ni, j\n\nt\n\n0\n\n\u2202 f\n\u2202xi\n\n(xs)bi(xs) ds + 1\n\n2\n\n\u2202 2 f\n\u2202xi\u2202x j\n\n(cid:3)\n\nt\n\n(xs) d(cid:22)x i, x j(cid:23)\n(cid:9)\n\ns\n\n\u2202 2 f\n\u2202xi\u2202x j\n\n0\n\ni, j\n\n(xs)ai j (xs) ds\n\nand we are finished.\n\n0\n\n39.3 martingale problems\n\nin this section we consider operators in nondivergence form, that is, operators of the form\ngiven by( 39.9). we assume throughout this section that the coefficients ai j and bi are\nbounded and measurable and that ai j (x) = a ji(x) for all i, j = 1, . . . , d and all x \u2208 rd. the\ncoefficients ai j are called the diffusion coefficients and the bi are called the drift coefficients.\nwe also assume that the operator l is uniformly elliptic, which means that there exists \u0001 > 0\nsuch that\n\nyiai j (x)y j \u2265 \u0001|y|2,\n\ny \u2208 rd , x \u2208 rd .\n\n(39.12)\n\nd(cid:9)\n\ni, j=1\n\nthis says that the matrix ai j (x) is positive definite, uniformly in x.\n\nwe saw in the previous section that if xt is the solution to (39.1), a = \u03c3 \u03c3 t , and f \u2208 c2,\n\nthen\n\nf (xt ) \u2212 f (x0) \u2212\n\nt\n\nl f (xs) ds\n\n0\n\n(39.13)\n\n(cid:3)\n\n "}, {"Page_number": 334, "text": "316\n\nmarkov processes and sdes\n\n(cid:3)\n\nis a local martingale under p. a very fruitful idea of stroock and varadhan is to phrase the\nassociation of xt with l in terms which use (39.13) as a key element. let \u0001 consist of all\ncontinuous functions \u03c9 mapping [0,\u221e) to rd. let xt (\u03c9) = \u03c9(t ) and given a probability p,\nlet {ft} be the minimal augmented filtration generated by x . a probability measure p is a\nsolution to the martingale problem for l started at x0 if\np(x0 = x0) = 1\n\n(39.14)\n\nand\n\nf (xt ) \u2212 f (x0) \u2212\n\nt\n\nl f (xs) ds\n\n(39.15)\nis a local martingale under p whenever f \u2208 c2(rd ). the martingale problem is well posed\nif there exists a solution p and this solution is unique.\nuniqueness of the martingale problem for l is closely connected to weak uniqueness or,\n\n0\n\nequivalently, uniqueness in law of (39.1).\ntheorem 39.4 suppose a = \u03c3 \u03c3 t and suppose the matrix \u03c3 (x) is invertible for each x.\nweak uniqueness for (39.1) holds if and only if the solution for the martingale problem for l\nstarted at x is unique. weak existence for (39.1) holds if and only if there exists a solution to\nthe martingale problem for l started at x.\nproof we prove the uniqueness assertion. let \u0001 be the continuous functions on [0,\u221e)\nand zt the coordinate process: zt (\u03c9) = \u03c9(t ). first suppose the solution to the martingale\nproblem is unique. if (x 1\n, p2) are two weak solutions to (39.1), define\ni on \u0001 to be the law of x i under pi, i = 1, 2. clearly px\n= x) = 1.\nt\npx\ni for each i and each f \u2208 c2. by the\nthe expression in (39.13) is a local martingale under px\nuniqueness for the solution of the martingale problem, px\n2. this implies that the laws\n1\nof x 1\n\n(z0 = x) = pi(x i\n= px\n\n, p1) and (x 2\nt\n\nt and x 2\nt are the same, or weak uniqueness holds.\nnow suppose weak uniqueness holds for (39.1). let\n\n, w 1\nt\n\n, w 2\nt\n\n0\n\ni\n\n(cid:3)\n\n0\n\nyt = zt \u2212\n\nt\n\nb(zs) ds.\n\n1 and px\n\n2 be solutions to the martingale problem. if f (x) = xk, the kth coordinate of x,\nlet px\nthen \u2202 f /\u2202xi(x) = \u03b4ik and \u2202 2 f /\u2202xi\u2202x j (x) = 0, where \u03b4ik is 1 if i = k and 0 otherwise, and\nso l f (zs) = bk (zs). we see from (39.13) that the kth coordinate of yt is a local martingale\n(cid:15)\nunder px\ni .\nnow let f (x) = xkxm. a simple computation shows that l f (x) = akm(x), hence y k\n\u2212\nt\n0 akm(zs) ds is a local martingale. we set\nwt =\n\nt y m\nt\n\n(cid:3)\n\nt\n\nthe stochastic integral is finite since\n\n\u03c3 \u22121(zs) dys.\nd(cid:9)\n\nk=1\n\n(cid:3)\n\nt\n\ne\n\n0\n\nd(cid:9)\n(\u03c3 \u22121)i j (zs)\n(cid:3)\nd(cid:9)\n\nt\n\n0\n\nj=1\n= e\n\n0\n\ni,k=1\n\n\u22121)ik (zs)aik (zs) ds = t < \u221e.\n(a\n\n(\u03c3 \u22121)ik (zs) d(cid:22)y j, y k(cid:23)\n\ns\n\n(39.16)\n\n "}, {"Page_number": 335, "text": "exercises\n\n317\n\nsince yt is a local martingale, it follows that wt is a local martingale, and a calculation\nsimilar to (39.16) shows that w k\ni . by l\u00b4evy\u2019s theorem\n(exercise 12.4), wt is a brownian motion under both px\n) is a weak\nsolution to (39.1). by the weak uniqueness hypothesis, the laws of zt under px\n2 agree,\nwhich is what we wanted to prove.\n\n\u2212 \u03b4kmt is also a martingale under px\n\n2, and (zt , wt , px\ni\n\n1 and px\n\n1 and px\n\nt w m\nt\n\nexercise 39.1 asks you to prove that the existence of a weak solution to (39.1) is equivalent\n\nto the existence of a solution to the martingale problem.\n\nif the \u03c3i j and bi are lipschitz functions, the solution to (39.1) is pathwise unique; see\nexercise 24.5. by proposition 25.2, weak existence and uniqueness hold, and then the\nmartingale problem for l is well posed for every starting point.\n\na process that can be described in terms of a martingale problem (as well as other ways) is\nsuper-brownian motion. super-brownian motion, also known as a measure-valued branching\ndiffusion process, is a process whose state space is the set m of finite positive measures\non rd. the intuitive picture is as follows. given an initial finite measure \u03bc as a starting\npoint, let x n\nt be the process that starts with [n\u03bc(rd )] particles, each with mass 1/n, each\ndistributed according to \u03bc(dx)/\u03bc(rd ), where [\u00b7] denotes the integer part. each particle\nmoves as an independent brownian motion for a time 1/n, at which time each particle splits\ninto two or dies, independently of the other particles. the particles that are now alive move\nas independent brownian motions for time 1/n, at which time each particle splits into two\nor dies, and so on. x n\nis the measure that assigns mass 1/n at each point at which there is\nt\na particle alive at time t. we take the right-continuous version of x n\nt . it turns out that the\nsequence converges weakly with respect to the topology of d[0, 1], but where the state space\nis the set of right-continuous functions with left limits taking values in m (rather than the\nset of real-valued functions) and the limit law can be characterized as the unique solution to a\nmartingale problem. a solution to this martingale problem started at \u03bc \u2208 m is a probability\nmeasure on the space of continuous processes taking values in m such that\n\n(1) p(x0 = \u03bc) = 1;\n(2) if f \u2208 c\n\n\u221e\n\nhas compact support and we write \u03bd( f ) for\n\nf d\u03bd, then\n\n(cid:15)\n\n(cid:3)\n\nt\n\n0\n\nxr( 1\n2\n\n\u0001 f ) dr\n\nt = xt ( f ) \u2212\nm f\n(cid:3)\n\n(cid:22)m f\n\nt (cid:23) =\n\nt\n\nxr( f 2) dr.\n\n0\n\nis a continuous martingale with quadratic variation process given by\n\nsee dawson (1993) and perkins (2002) for more on these processes.\n\nexercises\n\n39.1 show that the existence of a weak solution to (39.1) is equivalent to the existence of a solution\n\nto the martingale problem for l.\n\n39.2 suppose the ai j are lipschitz functions in x and the matrices a(x) are positive definite, uniformly\nin x; see exercise 25.4. show that we can find matrices \u03c3 (x) so that each \u03c3i j is a lipschitz function\nof x and a(x) = \u03c3 (x)\u03c3 t (x) for each x.\n\n "}, {"Page_number": 336, "text": "318\n\nmarkov processes and sdes\n\n39.3 if x is a solution to (39.1), give formulas for at and mt in terms of \u03c3 and b, where mt is a local\nmartingale, at is a process whose paths are locally of bounded variation, and |xt| = mt + at.\n39.4 let a \u2208 (\u22121,\u221e) and let x be a solution to (39.1), where all the bi\u2019s are equal to 0, a = \u03c3 \u03c3 t ,\n\nand\n\nai j (x) = \u03b4i j + axix j/|x|2\n\n1 + a\n\nfor x (cid:16)= 0, where \u03b4i j is equal to 1 if i = j and 0 otherwise. let a(0) be the identity matrix.\n(1) prove that the matrices a(x) are uniformly elliptic.\n(2) show that |xt| has the same law as a bessel process of order\n\nd + a\n1 + a\n\n.\n\nln f (x) = d(cid:9)\n\ni, j=1\n\nconclude that if a is sufficiently close to \u22121, then x is transient, i.e, limt\u2192\u221e |xt| = \u221e, a.s.,\nwhile if a is sufficiently large, there exist arbitrarily large times t such that xt = 0.\n\n39.5 suppose for each n \u2265 1, an\n\ni j\n\n(i, j)th entry is an\ni j\n\n(x) is symmetric in i and j, is continuous in x, and the matrix whose\n\n(x) is positive definite, uniformly in x and n. let\n\nan\ni j\n\n(x)\n\n\u22022 f\n\u2202xi\u2202x j\n\n(x)\n\n(39.17)\n\n(x) converges to ai j (x) uniformly in x as n \u2192 \u221e, and define\nfor f \u2208 c2. suppose an\nl analogously to (39.17). fix x0 and let pn be a solution to the martingale problem for ln\nstarted at x0.\n(1) prove that pn converges weakly to a solution p to the martingale problem for l started\n\ni j\n\nat x0.\n(2) prove that if the ai j are continuously differentiable functions of x whose first partial\nderivatives are bounded, then there exists a solution to the martingale problem for l started\nat x0.\nmartingale problem for l started at x0.\n\n(3) prove that if the ai j are continuous functions of x, then there exists a solution to the\n\n39.6 suppose x is a solution to dxt = \u03c3 (xt ) dwt, where w is a d-dimensional brownian motion,\n\u03c3 (x) is a d \u00d7 d matrix-valued function that is bounded, and \u03c3 t \u03c3 is positive definite, uniformly\nin x. prove the following estimate for the time to leave a ball: there exist constants c1 and c2 not\ndepending on x0 such that\n\nc1r2 \u2264 e x0 \u03c4b(x0,r) \u2264 c2r2,\n\nr > 0,\n\nwhere \u03c4b(x0,r) = inf{> 0 : xt /\u2208 b(x0, r)}.\n\nsee bass (1997) for more information.\n\nnotes\n\n "}, {"Page_number": 337, "text": "40\n\nsolving partial differential equations\n\nwe will be concerned with giving probabilistic representations of the solutions to certain\npdes. throughout we will be assuming that the given pde has a solution, the solution is\nunique, and the solution is sufficiently smooth. we will consider poisson\u2019s equation, the\ndirichlet problem, the cauchy problem (with an application to brownian passage times), and\nschr\u00a8odinger\u2019s equation.\n\nwe let xt be the solution to\n\ndxt = \u03c3 (xt ) dwt + b(xt ) dt.\n\n(40.1)\nhere w is a d-dimensional brownian motion, \u03c3 is a bounded lipschitz continuous d \u00d7 d\nmatrix-valued function, b is a bounded lipschitz continuous d \u00d7 1 matrix-valued function,\nand x takes values in rd. we let a = \u03c3 \u03c3 t and we consider the operator on c2 functions\ngiven by\n\nai j (x)\n\n\u2202 2 f\n\u2202xi\u2202x j\n\nbi(x)\n\n\u2202 f\n\u2202xi\n\n(x).\n\n(40.2)\n\n(x) + d(cid:9)\n\ni=1\n\nd(cid:9)\n\ni, j=1\n\n2\n\nl f (x) = 1\nd(cid:9)\n\ni, j=1\n\nd(cid:9)\n\ni=1\n\nai j (x)yiy j \u2265 \u0001\n\n,\n\ny2\ni\n\ny1, . . . , yd \u2208 rd .\n\nwe suppose the operator l is uniformly elliptic: there exists \u0001 > 0 such that\n\nin fact, the uniform ellipticity of l will be used only to guarantee that the exit times\nof bounded domains are finite, a.s.; see exercise 40.1. for many non-uniformly elliptic\noperators, it is often the case that the finiteness of the exit times is known for other reasons,\nand the results then apply to equations involving these operators.\n\nt be the solution to (40.1) when x0 = x and let px be the law of x x\n\nt . as in\n\nlet x x\n\nchapter 39, we slightly abuse notation and say that (xt , px) is a strong markov process.\n\n40.1 poisson\u2019s equation\n\nwe consider first poisson\u2019s equation in rd. suppose \u03bb > 0 and f\ncompact support. poisson\u2019s equation is\n\nis a c1 function with\n\nlu(x) \u2212 \u03bbu(x) = \u2212 f (x),\n\nx \u2208 rd .\n\n(40.3)\n\n319\n\n "}, {"Page_number": 338, "text": "320\n\nsolving partial differential equations\n\ntheorem 40.1 suppose u is a c2 solution to (40.3) such that u and its first and second\npartial derivatives are bounded. then\n\n(cid:3) \u221e\n\n0\n\nu(x) = e x\n\n\u2212\u03bbt f (xt ) dt.\ne\n(cid:3)\n\nt\n\nlu(xs) ds,\n\nproof let u be the solution to (40.3). by theorem 39.3,\n\nu(xt ) \u2212 u(x0) = mt +\n(cid:3)\n(cid:3)\nwhere mt is a martingale. by the product formula,\n\n0\n\nt\n\n\u2212\u03bbslu(xs) ds \u2212 \u03bb\ne\n\ntaking the expectation with respect to px and letting t \u2192 \u221e,\n\n0\n\nt\n\ne\n\n\u2212\u03bbtu(xt ) \u2212 u(x0) =\n\n\u2212\u03bbsdms +\ne\n(cid:3) \u221e\nsince lu \u2212 \u03bbu = \u2212 f , the result follows.\n\n\u2212u(x) = e x\n\n0\n\n0\n\n\u2212\u03bbs(lu \u2212 \u03bbu)(xs) ds.\ne\n\n(cid:3)\n\n0\n\nt\n\n\u2212\u03bbsu(xs) ds.\ne\n\n(cid:3) \u03c4d\n\n0\n\nu(x) = e x\n\nlet us now let d be a nice bounded domain, e.g., a ball. poisson\u2019s equation in d requires\none to find a function u such that lu \u2212 \u03bbu = \u2212 f in d and u = 0 on \u2202d, where f \u2208 c2(d)\nand \u03bb \u2265 0. here we can allow \u03bb to be equal to 0.\ntheorem 40.2 suppose u is a solution to poisson\u2019s equation in a bounded domain d that is\nc2 in d and continuous on d. then\n\n\u2212\u03bbs f (xs) ds.\ne\n(cid:3)\n\nt\u2227sn\n\n0\n\nproof the proof is nearly identical to that of the previous theorem. we already men-\ntioned that \u03c4d < \u221e, a.s.; see exercise 40.1. let sn = inf{t : dist (xt , \u2202d) < 1/n}. by\ntheorem 39.3,\n\nu(xt\u2227sn\n\n) \u2212 u(x0) = martingale +\n\nby the product formula,\n\ne xe\n\n\u2212\u03bb(t\u2227sn )u(xt\u2227sn\n\n(cid:3)\n\nlu(xs) ds.\n(cid:3)\n\nt\u2227sn\n\n) \u2212 u(x) = e x\n= \u2212e x\n\nt\u2227sn\n\n(cid:3)\n\n0\n\n\u2212\u03bbslu(xs) ds \u2212 e x\ne\nt\u2227sn\n\u2212\u03bbs f (xs) ds.\ne\n\n0\n\n\u2212\u03bbsu(xs) ds\ne\n\nnow let n \u2192 \u221e and then t \u2192 \u221e and use the fact that u is zero on \u2202d.\n\n0\n\n40.2 dirichlet problem\n\nlet d be a ball (or other nice bounded domain) and let us consider the solution to the dirichlet\nproblem: given a continuous function f on \u2202d, find u \u2208 c(d) such that u is c2 in d and\n\nlu = 0 in d,\n\nu = f on \u2202d.\n\n(40.4)\n\n "}, {"Page_number": 339, "text": "321\nwe considered the dirichlet problem in the special case when l is the laplacian in\nsection 21.4.\n\n40.3 cauchy problem\n\ntheorem 40.3 suppose u is a solution to the dirichlet problem specified by (40.4). then u\nsatisfies\n\nu(x) = e x f (x\u03c4d\n\n).\n\nproof as we mentioned above, \u03c4d < \u221e, a.s. let sn = inf{t : dist (xt , \u2202d) < 1/n}. by\ntheorem 39.3,\n\n(cid:3)\n\nt\u2227sn\n\n0\n\nlu(xs) ds.\n\nu(xt\u2227sn\n\n) = u(x0) + martingale +\n\nsince lu = 0 inside d, taking expectations shows\n\nu(x) = e xu(xt\u2227sn\n\n).\n\nwe let t \u2192 \u221e and then n \u2192 \u221e. by dominated convergence, we obtain u(x) = e xu(x\u03c4d\nthis is what we want since u = f on \u2202d.\nif v \u2208 c2 and lv = 0 in d, we say v is l-harmonic in d.\n\n).\n\n40.3 cauchy problem\n\nthe related parabolic partial differential equation\n= lu\n\n\u2202u\n\u2202t\n\nis often of interest. here u is a function of x \u2208 rd and t \u2208 [0,\u221e). when we write lu, we\nmean\n\nlu(x, t ) = d(cid:9)\n\n(x, t ) + d(cid:9)\n\ni=1\n\nai j (x)\n\n\u2202 2u\n\u2202xi\u2202x j\n\nim j=1\n\nbi(x)\n\n\u2202u\n\u2202xi\n\n(x, t ).\n\nwe will sometimes write ut for \u2202u/\u2202t.\n\nsuppose for simplicity that the function f is a continuous function with compact support.\nthe cauchy problem is to find u such that u is bounded, u is c2 with bounded first and second\npartial derivatives in x, u is c1 in t for t > 0, and\n\nut (x, t ) = lu(x, t ),\nu(x, 0) = f (x),\n\nx \u2208 rd .\n\nt > 0, x \u2208 rd ,\n\n(40.5)\n\ntheorem 40.4 suppose there exists a solution to (40.5) that is c2 in x and c1 in t for t > 0.\nthen u satisfies\n\nproof fix t0 and let mt = u(xt , t0 \u2212 t ). note\n\nu(x, t ) = e x f (xt ).\n\nu(x, t0 \u2212 t ) = \u2212ut (x, t0 \u2212 t ).\n\n\u2202\n\u2202t\n\n "}, {"Page_number": 340, "text": "322\n\nsolving partial differential equations\n\nsimilarly to the proof of theorem 39.3 (see exercise 40.2) but using now the multivariate\nversion of it\u02c6o\u2019s formula,\n\nu(xt , t0 \u2212 t ) = martingale +\n\nt\n\nlu(xs, t0 \u2212 s) ds \u2212\n\nt\n\nut (xs, t0 \u2212 s) ds.\n\n(40.6)\n\n(cid:3)\n\n0\n\n(cid:3)\n\n0\n\nsince ut = lu, mt is a martingale, and e xm0 = e xmt0. on the one hand,\n\ne xmt0\n\n= e xu(xt0\n\n, 0) = e x f (xt0\n\n),\n\nwhile on the other hand,\n\ne xm0 = e xu(x0, t0) = u(x, t0).\n\nsince t0 is arbitrary, the result follows.\n\na very similar proof allows one to represent the solution to the cauchy problem in a\n\nbounded domain. suppose u(x, t ) is c2 in the x variable, c1 in the t variable, and satisfies\n\nfor (x, t ) \u2208 d \u00d7 (0, t1], where d is a bounded domain in rd and t1 > 0. suppose u(x, 0) =\nf (x) and u(x, t ) = 0 for all x \u2208 \u2202d. exercise 40.3 asks you to show that in this case\n\n(x, t ) = lu(x, t )\n\n\u2202u\n\u2202t\n\nu(x, t ) = e x f (xt\u2227\u03c4d\n\n),\n\nwhere again \u03c4d is the first exit time of x from the domain d.\n\nthe cauchy problem has an application to the passage times of brownian motion. suppose\n\nwe look at the equation\n\nux(x, t ) = 1\n\n2 uxx(x, t ),\n\n0 < x < b,\n\nt > 0,\n\nwith\n\nu(x, 0) = f (x) for all x,\n\nu(0, t ) = u(b, t ) = 0 for all t,\n\nis a bounded function on [0, b]. this is a partial differential equation (the heat\nwhere f\nequation) that is sometimes solved in undergraduate classes; see, e.g., boyce and diprima\n(2009), section 10.5. using a combination of the technique of separation of variables and\nfourier series expansions, the solution can then be shown to be\n\n(cid:3)\n\nu(x, t ) =\n\u221e(cid:9)\n\nf (y)p0(t, x, y) dy,\n\nwhere\n\np0(t, x, y) = 2\nb\n\nn=1\n\n\u2212n2\u03c0 2t/2b2 sin(n\u03c0x/b) sin(n\u03c0y/b).\ne\n\nsee also knight (1981), p. 62. since u(x, t ) is also equal to e x f (xt\u2227\u03c4d\n), where d is the\ninterval (0, b), then the p0(t, x, y) are the transition densities for brownian motion killed on\nexiting (0, b).\n\nin particular, if we take f identically equal to 1 on (0, b), we see that starting at x inside\n\u2212\u03c0 2t/8.\n\n\u2212\u03c0 2t/2b2. if b is 2, this becomes ce\n\n(0, b), px(t < \u03c4d) is asymptotically equal to ce\n\n "}, {"Page_number": 341, "text": "40.4 schr\u00a8odinger operators\n\n323\nsince the time for a brownian motion started at 0 to leave (\u22121, 1) is the same as the time\nfor a brownian motion started at 1 to leave (0, 2), we obtain the estimate that was used in\nexercise 7.2.\n\n40.4 schr\u00a8odinger operators\n\nfinally we look at what happens when one adds a potential term, that is, when one considers\nthe operator\n\nlu(x) + q(x)u(x).\n\n(40.7)\n\nthis is known as the schr\u00a8odinger operator, and q(x) is known as the potential. equa-\ntions involving the operator in (40.7) are considerably simpler than the quantum mechanics\nschr\u00a8odinger equation because here all terms are real-valued.\nif xt is the diffusion corresponding to l, then solutions to pdes involving the operator in\n(40.7) can be expressed in terms of xt by means of the feynman\u2013kac formula. to illustrate,\nlet d be a nice bounded domain, e.g., a ball, q a c2 function on d, and f a continuous\n+\nfunction on \u2202d; q\n\ndenotes the positive part of q.\n\ntheorem 40.5 let d, q, f be as above. let u be a c2 function on d that agrees with f on\n\u2202d and satisfies lu + qu = 0 in d. if\n\n(cid:10)(cid:3) \u03c4d\n(cid:16)\n\n0\n\n(cid:11)\n+(xs) ds\nq\n(cid:15) \u03c4d\n\n< \u221e,\n\n(cid:17)\n\ne x exp\n\nu(x) = e x\n\nthen\n\nproof let bt =(cid:15)\n\nt\u2227\u03c4d\n0\n\nf (x\u03c4d\n\n)e\n\n0 q(xs ) ds\n\n.\n\n(40.8)\n\n(cid:3)\n\n0\n\n(cid:3)\nu(xr )ebr dbr +\n(cid:3)\n\n0\n\nq(xs) ds. by it\u02c6o\u2019s formula and the product formula,\nt\u2227\u03c4d\n\nt\u2227\u03c4d\n\neb(t\u2227\u03c4d )u(xt\u2227\u03c4d\n\n) = u(x0) + martingale +\n\nebrlu(xr ) dr.\n\n(cid:3)\n\nt\u2227\u03c4d\n\n0\n\ne xeb(t\u2227\u03c4d )u(xt\u2227\u03c4d\nsince lu + qu = 0,\n\ntaking the expectation with respect to px and using proposition 39.3,\n\n) = u(x) + e x\n\nebru(xr )q(xr ) dr + e x\n\nt\u2227\u03c4d\n\nebrlu(xr ) dr.\n\n0\n\n, the result follows.\n\n) = u(x).\nif we let t \u2192 \u221e and use the exponential integrability of q\n+\n\ne xeb(t\u2227\u03c4d )u(xt\u2227\u03c4d\n\n(cid:15) \u03c4d\n\nthe existence of a solution to lu + qu = 0 in d depends on the finiteness of\neven in one dimension with d = (0, 1) and q a constant function, the gauge need not\n\u2212\u03c0 2t/2 as t \u2192 \u221e by\n\ne x exp(\nbe finite. with x = 1/2, px(\u03c4d > t ) is asymptotically equal to ce\n\n+(xs) ds), an expression that is sometimes known as the gauge.\n\n0 q\n\n "}, {"Page_number": 342, "text": "solving partial differential equations\n\n(cid:10)(cid:3) \u03c4d\n\n(cid:11)\n\nq ds\n\n0\n\n324\n\nsection 40.3. hence\n\ne x exp\n\nthis is infinite if q \u2265 \u03c0 2/2.\n\n(cid:3) \u221e\n\n= e xeq\u03c4d\n=\n\nqeqt px(\u03c4d > t ) dt;\n\n0\n\nexercises\n\n40.1 this (lengthy) exercise is designed to guide you through a proof that solutions to (40.1) exit\n\nbounded sets in finite time, a.s.\n\n(1) suppose\n\nxt = wt +\n\n(cid:3)\n\nt\n\n0\n\nas ds,\n\n| > 3l) > \u03b5.\n\n(2) suppose xt = mt +(cid:15)\n\nwhere w is a one-dimensional brownian motion, and as is an adapted process bounded by k.\nlet l > k > 0 and t0 > 0. show that there exists \u03b5 > 0, depending only on l, k, and t0 such\nthat p(|xt0\nt\n0 as ds, where as is as in (1) and m is a continuous martingale with\n\u22121 \u2264 d(cid:22)m(cid:23)t /dt \u2264 k, a.s. use a time change argument to show that there exist l, \u03b5 > 0 such\nk\nthat\n\n|xs| \u2264 l) \u2264 1 \u2212 \u03b5.\n\np(sup\ns\u22641\n\n(3) if now x is a solution to (40.1), a = \u03c3 \u03c3 t , and l given by (40.2) is uniformly elliptic,\n\nshow by looking at the first coordinate of x that there exist l, \u03b5 such that\n\n|xs| \u2264 l) \u2264 1 \u2212 \u03b5,\n\nx \u2208 b(0, l).\n\npx(sup\ns\u22641\n\n(4) what you have proved in (3) can be rephrased as saying that if (xt , px ) is a strong\nmarkov process that solves (40.1) for every starting point and \u03c4 = inf{t : xt /\u2208 b(0, l)}, then\npx(\u03c4 > 1) \u2264 1\u2212 \u03b5, where \u03b5 does not depend on x. now use the strong markov property (cf. the\nproof of proposition 21.2) to show px(\u03c4 > k ) \u2264 (1 \u2212 \u03b5)k. conclude that \u03c4 < \u221e, px-a.s., for\neach starting point x.\n\n40.2 prove (40.6).\n\n40.3 let d be a ball in rd and suppose u is the solution to the cauchy problem in the domain\n\nd \u00d7 [0, t1] as described in section 40.3. show that u(x, t ) = e x f (xt\u2227\u03c4d\n\n).\n\n40.4 suppose f is such that the solution u to\n\nis c2 in x and t and x is the diffusion associated with l. prove that\n\nu(x, 0) = f (x),\n\nut (x, t ) = lu(x, t ) + q(x),\n(cid:16)\n\nu(x, t ) = e x\n\n(cid:15)\n\nf (xt )e\n\nt\n0 q(xs ) ds\n\n.\n\n(cid:17)\n\n "}, {"Page_number": 343, "text": "notes\n\n325\n\n40.5 suppose (xt , px ) is a brownian motion on [0, b] with reflection at 0 and b. find a series expansion\n\nfor p(t, x, y), the transition densities for x .\nhint: imitate the argument for absorbing brownian motion in section 40.3, but now use the\nboundary conditions ux(0, t ) = ux(b, t ) = 0.\n\nsee bass (1997) for more on the connection between probability and pdes.\n\nnotes\n\n "}, {"Page_number": 344, "text": "41\n\none-dimensional diffusions\n\nunder very mild regularity conditions, every one-dimensional diffusion arises from first\ntime-changing a one-dimensional brownian motion and then making a transformation of the\nstate space. we will prove this fact in this chapter.\n\n41.1 regularity\n\nthroughout this chapter we suppose that we have a continuous process (xt , px) defined on\nan interval i contained in r. for almost all of the chapter, we suppose for simplicity that the\ninterval is in fact all of r. we further suppose that (xt , px) is a strong markov process with\nrespect to a right-continuous filtration {f t} such that each ft contains all the sets that are\npx-null for every x. we call such a process a one-dimensional diffusion.\n\nwrite\n\nty = inf{t : xt = y},\n\n(41.1)\n\nthe first time the process x hits the point y. we will also assume that every point can be hit\nfrom every other point: for all x, y,\n\npx(ty < \u221e) = 1.\n\n(41.2)\n\nwhen (41.2) holds, we say the diffusion is regular.\n\nfor any interval j, define \u03c4j = inf{t : xt /\u2208 j}, the first time the process leaves j. when\nxt is a brownian motion, we know (proposition 3.16) that the distribution of xt upon exiting\n[a, b] is\n\npx(x (\u03c4[a,b]) = a) = b \u2212 x\nb \u2212 a\n\n,\n\npx(x (\u03c4[a,b]) = b) = x \u2212 a\nb \u2212 a\n\n.\n\n(41.3)\n\nwe say that a regular diffusion xt is on natural scale if (41.3) holds for every interval [a, b].\nwe also say a regular diffusion x defined on an interval i properly contained in r is on\nnatural scale if (41.3) holds whenever [a, b] \u2282 i and x \u2208 (a, b).\nif xt is regular, then the process started at x must leave x immediately. that is, if s =\ninf{t > 0 : xt\n(cid:16)= x}, then px(s = 0) = 1. to see this, let \u03b5 > 0 and u = inf{t : |xt \u2212 x| \u2265 \u03b5}.\n\u2212u > 0. observe that u = s + u \u25e6 \u03b8s, where \u03b8t is the shift\nby the regularity of x , e xe\noperator. by the strong markov property at time s,\n\u2212u \u25e6 \u03b8s | fs] ] = e x[e\n\n\u2212u ] ] = e x[e\n\n\u2212u = e x[e\n\nsince xs = x by the continuity of the paths of x . the only way this can happen is if\ne xe\n\n\u2212s = 1, which implies s = 0, px-a.s.\n\n\u2212se xs [e\n\n\u2212se x[e\n\n\u2212se xe\n\n\u2212u ],\n\ne xe\n\n326\n\n "}, {"Page_number": 345, "text": "41.2 scale functions\n\n327\n\n41.2 scale functions\n\nwe will show that given a regular diffusion, there exists a scale function that is continuous,\nstrictly increasing, and such that s(xt ) is on natural scale.\n\nwe first look at a special case, when the diffusion is given as the solution to an sde.\n\nsuppose xt is given as the solution to\n\ndxt = \u03c3 (xt ) dwt + b(xt ) dt,\n\n(41.4)\n\nwhere we assume \u03c3 and b are real-valued, continuous and bounded above and \u03c3 is bounded\nbelow by a positive constant. let a(x) = \u03c3 2(x). in this case we can give a formula for the\nscale function.\n\ntheorem 41.1 the scale function s(x) is the solution to\n\nand for some constants c1, c2, and x0 is given by\n\u2212\n\ns(x) = c1 + c2\n\nexp\n\nx\n\n1\n2 a(x)s\n\n(cid:3)(cid:3)(x) + b(x)s\n(cid:3)(x) = 0,\n(cid:3)\n(cid:3)\n(cid:10)\n\ny\n\n2b(w)\na(w)\n\nx0\n\nx0\n\n(cid:11)\n\ndw\n\ndy.\n\n(41.5)\n\nproof to solve the differential equation, we write\n= \u22122\n\n(cid:3)(cid:3)(x)\ns\ns(cid:3)(x)\n\nb(x)\na(x)\n\n,\n\n(cid:3)(x))(cid:3) = \u22122b(x)/a(x), from which (41.5) follows. since we assumed that \u03c3 and b\nor (log s\nare continuous, s(x) given by( 41.5) is c2. since \u03c3 is bounded below by a positive constant\nand b and \u03c3 are bounded, s given by (41.5) is strictly increasing. applying it\u02c6o\u2019s formula,\n\n(cid:3)\n\ns(xt ) \u2212 s(x0) =\n\n(cid:3)(xr )\u03c3 (xr ) dwr\ns\n\nt\n\n0\n\n(41.6)\n\nbecause\n\n(cid:3)\n\nt\n\n(cid:3)(cid:3)(xr )\u03c3 (xr )2 + s\n[ 1\n2 s\n\n(cid:3)(xr )b(xr )] dr = 0.\n\n0\n\nthis implies that s(xt ) \u2212 s(x0) is a martingale, hence a time change of brownian motion.\ntherefore the exit probabilities of s(xt ) for an interval [a, b] are the same as those of a\nbrownian motion, namely, those given by (41.3).\n\nfrom (41.6), if yt = s(xt ), then\n\ndyt = (s\n\n(cid:3)\u03c3 )(s\n\n\u22121(yt )) dwt .\n\n(41.7)\n\nnow we show there exists a scale function for general regular diffusions on r. let j be\n\nan interval [a, b]. we define\n\np(x) = pj (x) = px(x\u03c4j\n\n= b).\n\nproposition 41.2 let j = [a, b] be a finite interval. then p(xt\u2227\u03c4j\n[0, 1] on natural scale.\n\n(41.8)\n\n) is a regular diffusion on\n\n "}, {"Page_number": 346, "text": "328\n\none-dimensional diffusions\n\nproof first we show that p is increasing. to get to the point b starting from x, the process\nmust first hit every point between x and b because x has continuous paths. if a < x < y < b,\nby the strong markov property at time ty, p(x) \u2264 p(y). we claim there is a positive probability\nthat the process starting from x hits a before y, that is,\n\npx(ta < ty) > 0.\n\n(41.9)\n\nif (41.9) did not hold, then the process started at x must hit y before hitting a, then by the\ncontinuity of paths must hit x before hitting a, and once the process is again at x, it again hits\ny with probability one before a and so on. therefore the process never hits a, a contradiction\nto the regularity; exercise 41.2 asks you to make this argument precise. therefore (41.9)\ndoes hold, and by the strong markov property at ty,\n\np(x) = px(ty < ta)p(y).\n\nsince px(ty < ta) = 1 \u2212 px(ta < ty) is strictly less than 1, p is strictly increasing.\nnext we show that p is continuous. we show continuity from the right; the proof of\ncontinuity from the left is similar. suppose xn \u2193 x. the process xt has continuous paths, so\ngiven \u03b5 we can find t small enough so that px(ta < t ) < \u03b5. by the blumenthal 0\u20131 law\n(proposition 20.8), px(t(x,b] = 0) is zero or one, where t(x,b] is the first time the process hits\nthe interval (x, b]. if it is zero, the process immediately moves to the left from x, a.s., and\nby the strong markov property at tx, it never hits b, a contradiction. the probability must\n< t ) \u2265 1 \u2212 \u03b5.\ntherefore be one. thus by the continuity of paths, for n large enough, px(txn\nhence with probability at least 1 \u2212 2\u03b5, xt hits xn before a. since\n\np(x) = px(txn\n\n< ta) p(xn) \u2265 (1 \u2212 2\u03b5)p(xn)\n\nand \u03b5 is arbitrary, we see that p(x) \u2265 lim inf n\u2192\u221e p(xn). since p is strictly increasing, p(xn)\ndecreases, and therefore p(x) = lim p(xn).\n\nfinally, we show p(xt ) is on natural scale. let [e, f ] \u2282 (0, 1) and let\n\u22121(e)).\n\n\u22121( f ) before hitting p\n\nr(y) = py(xt hits p\n\nnote that\n\nfor y \u2208 [p\n\n\u22121(a), p\n\npx(p(xt ) hits f before e) = pp\n= r(p\n\n\u22121 (x)(xt hits p\n\u22121(x)).\n\n\u22121( f ) before p\n\n\u22121(e))\n\n(cid:28)\n\n(cid:28)\n\n\u22121(b)], the strong markov property tells us that\np(y) = py\n\u22121( f )\np\n+ py\n\u22121(e)\np\np\n= r(y) f + (1 \u2212 r(y))e.\n\n\u22121( f ) before p\n\u22121(e) before p\n\n\u22121(e)\np\n\u22121( f )\n\nxt hits p\n\nxt hits p\n\n(cid:28)\n\n(cid:29)\n\n(cid:28)\n(cid:29)\n\n(cid:29)\n\n(41.10)\n\n(41.11)\n\n(cid:29)\n\nsolving for r(y), we obtain r(y) = (p(y) \u2212 e)/( f \u2212 e). substituting in (41.10),\n\u22121(x)) \u2212 e)/( f \u2212 e)\n\npx(p(xt ) hits f before e) = r(p\n\n\u22121(x)) = (p(p\n= (x \u2212 e)/( f \u2212 e),\n\nwhich is the formula we wanted.\n\n "}, {"Page_number": 347, "text": "41.3 speed measures\n\n329\n\nif xt\n\nis on natural scale,\n\nnote that\nc1 > 0, c2 \u2208 r.\ntheorem 41.3 there exists a continuous strictly increasing function s such that s(xt ) is on\nnatural scale on s(r).\n\nfor any constants\n\nthen so is c1xt + c2\n\nproof let jn be closed intervals increasing up to r. pick two points in j1; label them a\nand b with a < b. choose an and bn so that if sn(x) = an pjn\n(x) + bn, then sn(a) = 0 and\nsn(b) = 1.\nwe will show that if n \u2265 m, then sn = sm on jm. once we have that, we can set s(x) = sn(x)\nsuppose jm = [e, f ]. by proposition 41.2, both sm(xt ) and sn(xt ) are on natural scale. for\n\non jn, and the theorem will be proved.\nall x \u2208 jm,\n\n(cid:28)\n\nsm(x) \u2212 sm(e)\nsm( f ) \u2212 sm(e)\n\nsm(xt ) hits sm( f ) before sm(e)\n\n= psm (x)\n= px(xt hits f before e).\n\n(cid:29)\n\nwe have a similar equation with sm replaced everywhere by sn. it follows that\n\nsm(x) \u2212 sm(e)\nsm( f ) \u2212 sm(e)\n\n= sn(x) \u2212 sn(e)\nsn( f ) \u2212 sn(e)\n\nfor all x, which implies that sn(x) = csm(x) + d for some constants c and d. since sn and\nsm are equal at both x = a and x = b, then c must be 1 and d must be 0.\n\n41.3 speed measures\n\n(cid:13)\n\nsuppose that (px, xt ) is a regular diffusion on r on natural scale. for each finite interval\n(a, b), define\n\ngab(x, y) =\n\n2(x\u2212a)(b\u2212y)\n2(y\u2212a)(b\u2212x)\n\nb\u2212a\nb\u2212a\n\n,\n\na < x \u2264 y < b,\na < y \u2264 x < b,\n\n(41.12)\nand set gab(x, y) = 0 if x or y is not in (a, b). a measure m(dx) is the speed measure for the\ndiffusion (xt , px) if\n\n,\n\n(cid:3)\n\n(41.13)\nfor each finite interval (a, b) and each x \u2208 (a, b). as( 41.13) indicates, the speed measure\ngoverns how quickly the diffusion moves through intervals.\n\ngab(x, y) m(dy)\n\nas an example, let us argue that the speed measure for brownian motion is a lebesgue\n\nmeasure. by proposition 3.16, if (xt , px) is a brownian motion,\n\ne x\u03c4(a,b) =\n\non the other hand, a calculation shows that\n\n(cid:3)\n\ne x\u03c4(a,b) = (x \u2212 a)(b \u2212 x).\n\ngab(x, y) dy = (x \u2212 a)(b \u2212 x).\n\n "}, {"Page_number": 348, "text": "330\n\nsince\n\n(cid:3)\n\none-dimensional diffusions\n\ne x\u03c4(a,b) =\n\ngab(x, y) dy\n\nand brownian motion is on natural scale, we see that the speed measure m(dy) of brownian\nmotion is equal to a lebesgue measure.\n\nwe will show that\n(1) a regular diffusion on natural scale has one and only one speed measure,\n(2) the law of the diffusion is determined by the speed measure, and\n(3) there exists a diffusion with a given speed measure.\n\nwe first want to show that any speed measure must satisfy 0 < m(a, b) < \u221e for any finite\ninterval [a, b]. to start we have the following lemma.\n\n(a,b) < \u221e for each positive integer k.\nlemma 41.4 if [a, b] is a finite interval, then supx\nproof pick y \u2208 (a, b). since xt is a regular diffusion, py(ta < \u221e) = 1, and hence there\nexists t0 such that py(ta > t0) < 1/2. similarly, taking t0 larger if necessary, py(tb > t0) \u2264\n1/2. if a < x \u2264 y, then\n\ne x\u03c4 k\n\npx(\u03c4(a,b) > t0) \u2264 px(ta > t0) \u2264 py(ta > t0) \u2264 1/2,\nand similarly, px(\u03c4(a,b) > t0) \u2264 1/2 if y \u2264 x < b. by the markov property,\npx(\u03c4(a,b) > (n + 1)t0) = e x[px (nt0 )(\u03c4(a,b) > t0); \u03c4(a,b) > nt0]\n\npx(\u03c4(a,b) > nt0),\n\n\u2264 1\n\u2212n. the lemma is now immediate.\n\n2\n\nand by induction, px(\u03c4(a,b) > nt0) \u2264 2\nlemma 41.5 if (xt , px) has a speed measure m and [a, b] is a non-empty finite interval,\nthen 0 < m(a, b) < \u221e.\nproof\n\nif m(a, b) = 0, then for x \u2208 (a, b), we have\n\n(cid:3)\n\ne x\u03c4(a,b) =\n\ngab(x, y) m(dy) = 0,\n\nnext we show the finiteness of m(a, b). pick (e, f ) such that [a, b] \u2282 (e, f ). there exists\n\nwhich implies \u03c4(a,b) = 0, px-a.s., a contradiction to the continuity of the paths of xt.\na constant c such that for x, y \u2208 (a, b), ge f (x, y) is bounded below by c, so\n\u22121e x\u03c4(e, f ) < \u221e.\n\nge f (x, y) m(dy) = c\n\nm(a, b) \u2264 c\n\n(cid:3)\n\n\u22121\n\nf\n\nthis completes the proof.\n\ne\n\ntheorem 41.6 a regular diffusion on natural scale on r has one and only one speed\nmeasure.\nproof first let i = (e, f ) be a finite open interval. for n > 1 let xi = e + i( f \u2212 e)/2n,\n2n\u22121(cid:9)\ni = 0, 1, 2, . . . , 2n. let dn = {xi : 0 \u2264 i \u2264 2n}. let\n\nmn(dx) = 2n\n\nb(xi)\u03b4xi\n\n,\n\ni=1\n\n(41.14)\n\n "}, {"Page_number": 349, "text": "(cid:3)\n\ne x\u03c4(a,b) =\n\nwhere b(xi) = e xi \u03c4(xi\u22121,xi+1 ). we first want to show that if [a, b] is a subinterval of i with a, b\neach in dn and x is also in dn, then\n\n41.3 speed measures\n\n331\n\ngab(x, y) mn(dy).\nto see this, let s0 = 0 and s j+1 = inf{t > s j : |xt \u2212 xs j\nthe successive times that x moves 2\n\u2212n with probability 1\non natural scale, xs j+1 is equal to xs j\nprobability 1\nlattice with step size 2\nlet j (xi) = (xi \u2212 2\n\n(41.15)\n\u2212n} \u2227 \u03c4(a,b). the s j\u2019s are\n\u2212n, up until the time of leaving (a, b). because x is\n+ 2\n\u2212n with\n2 , until leaving (a, b). therefore xs j is a simple symmetric random walk on the\n\u2212n) for xi (cid:16)= a, b. let j (a) = j (b) = \u2205. by repeated use of\n\n2 and equal to xs j\n\n| = 2\n\n\u2212 2\n\nthe strong markov property,\ne x\u03c4(a,b) =\n\nlet ni =(cid:12)\u221e\n\n\u2212n, stopped on leaving (a, b).\n\u2212n, xi + 2\n\u221e(cid:9)\ne x(s j+1 \u2212 s j )\n\u221e(cid:9)\n\nj=0\n= e x\n\ne x (s j )[\u03c4j (x0 )] = e x\n\nj=0\n\ne x\u03c4(a,b) = e x\n\n= e x\n\n= e x\n\nj=0\n\n\u221e(cid:9)\n\u221e(cid:9)\n2n\u22121(cid:9)\n\nj=0\n\ni=1\n\nb(xs j\n\n2n\u22121(cid:9)\n\nb(xs j\n\ni=1\n\nb(xi)ni.\n\n\u221e(cid:9)\n\nj=0\n\nb(xs j\n\n)1(a,b)(xs j\n\n).\n\n)1(a,b)(xs j\n\n)\n\n(41.16)\n\n)1{xi}(xs j\n\n)\n\nj=0 1{xi}(xs j\n\n), the number of visits to xi before exiting (a, b). then\n\ne xni must equal 0 when x = a or x = b and satisfies the equation\n(e x j+1ni + e x j\u22121ni),\n\ne x j ni = \u03b4i j + 1\n\n(41.17)\nwhere \u03b4i j is 1 if i = j and 0 otherwise. this holds because for j (cid:16)= i, the process goes left or\nright, each with probability 1/2, while if j = 1, we add one to ni before going left or right.\nthe function x \u2192 e xni is hence piecewise linear on (a, xi) and on (xi, b). some algebra\nshows that we must have\n\n2\n\ne xni = 2ngab(x, xi).\n\n(41.18)\n\ncombining (41.16) and (41.18),\n\ne x\u03c4(a,b) = 2n\u22121(cid:9)\n(cid:3)\n\ni=1\n\n=\n\nb(xi)2ngab(x, xi)\n\ngab(x, y) mn(dy),\n\nwhich is (41.15).\n\n "}, {"Page_number": 350, "text": "332\n\none-dimensional diffusions\n\n(cid:3)\n\nusing (41.15) and the same proof as that of lemma 41.5, mn(a, b) is bounded above by\na constant independent of n. by a diagonalization procedure, there exists a subsequence nk\nsuch that mnk converges weakly to m, where m is a measure that is finite on every subinterval\n(a, b) such that [a, b] \u2282 i. by the continuity of gab,\n\ngab(x, y) m(dy)\n\ne x\u03c4(a,b) =\nwhenever a, b, and x are in dn for some n.\nwe now remove this last restriction. if a, b are not of this form, take ar, br to be in \u222andn\nsuch that (ar, br ) \u2191 (a, b). then \u03c4(ar,br ) \u2191 \u03c4(a,b), and by the continuity of gab in a, b, x, and y,\nwe have (41.19) for all a and b. take yr \u2191 x, zr \u2193 x such that yr and zr are in dn for some n.\nby the strong markov property,\n\n(41.19)\n\n+ e zr \u03c4(a,b)px(x\u03c4(yr ,zr )\nby the continuity of gab in x, and the fact that e x\u03c4(y(cid:3)\nfor all x.\n\ne x\u03c4(a,b) = e x\u03c4(yr,zr ) + e yr \u03c4(a,b)px(x\u03c4(yr ,zr )\n= zr ).\n,zr ) \u2192 0 as r \u2192 \u221e, we obtain (41.19)\n\n= yr )\n\nr\n\nwe leave the uniqueness as exercise 41.3.\nfinally, let ik be finite subintervals increasing up to r. let mk be the speed measure for xt\non the interval ik. by the uniqueness result, mk agrees with m(cid:14) on i(cid:14) if i(cid:14) \u2282 ik. setting m to\nbe the measure whose restriction to ik is mk gives us the speed measure.\n\nthe speed measure completely characterizes occupation times.\n\n(cid:3) \u03c4(a,b)\n\ncorollary 41.7 suppose xt is a diffusion on natural scale on r. if\nmeasurable, for each a < b,\n\nf\n\nis bounded and\n\ne x\n\nf (xs) ds =\n\ngab(x, y) f (y) m(dy).\n\n(41.20)\n\n0\n\nproof suppose that f is continuous and bounded on [a, b]. let xi, s j, b(xi), ni, and mn be\nas in the proof of theorem 41.6. let\n\n(cid:3)\n\n\u03b5n = sup{| f (x) \u2212 f (y)| : |x \u2212 y| \u2264 2\n\n\u2212n}.\n\n\u221e(cid:9)\nnote that if (x \u2212 a)/(b \u2212 a) is a multiple of 2\nf (xs) ds =\n\n\u2212n,\n\n(cid:3) \u03c4(a,b)\n\ne x\n\n(cid:3)\n\n0\n\ne x\n\ns j+1\n\ns j\n\nf (xs) ds\n\n(41.21)\n\nand\n\n\u221e(cid:9)\n\nj=0\n\ne x\n\n)(s j+1 \u2212 s j ) = e x\n\nf (xs j\n\nf (xs j\n\n)1(a,b)(xs j\n\n)e xs j s1\n\n(41.22)\n\nj=0\n\n\u221e(cid:9)\n= 2n\u22121(cid:9)\n\nj=0\n\ni=1\n\nf (xi)b(xi)1(a,b)(xi)e xni.\n\n "}, {"Page_number": 351, "text": "41.4 the uniqueness theorem\n\n333\n\nmoreover, the right-hand side of (41.21) differs from the left-hand side of (41.22) by at most\n\u03b5ne x\u03c4(a,b). by( 41.18) the right-hand side of (41.22) is equal to\n\n(cid:3)\n\n2n f (xi)b(xi)1(a,b)(xi)gab(x, xi) =\n\ngab(x, xi) f (xi) mn(dx).\n\n2n\u22121(cid:9)\n\ni=1\n\nby weak convergence along an appropriate subsequence, the left-hand side and the right-\n\u03b5ne x\u03c4(a,b), which is zero. a limit argument\nhand side of (41.20) differ by at most lim supn\nthen shows that (41.20) holds for all x \u2208 [a, b], and another limit argument shows that (41.20)\nholds for all bounded f .\n\n41.4 the uniqueness theorem\n\nwe next turn to showing that the speed measure characterizes the law of a diffusion.\n\n), i = 1, 2, are two diffusions on natural scale with the same speed\ntheorem 41.8 if (xt , px\n= px\ni\nmeasure m, then px\n2.\n1\nproof we start by letting (a, b) \u2282 r and considering the operator\n\u03bb \u2265 0,\n\n(cid:3) \u03c4(a,b)\n\n\u03bb f (x) = e x\nri\n\n\u2212\u03bbt f (xt ) dt,\ne\n\n(41.23)\n\nfor i = 1, 2. we show first that r1\n\nif f is bounded and borel measurable. this is easy, because by corollary 41.7, both sides\nare equal to\n\nsince ((cid:2)xt , px\n\ni\n\n) is a markov process, where(cid:2)x is the process x killed on exiting (a, b), the\n\ngab(x, y) m(dy).\n\na\n\nresolvent equation (37.2) holds. we have\n\ni\n\n(cid:3) \u03c4(a,b)\n\n0\n\ne x\n1\n\n0\n\n(cid:3) \u03c4(a,b)\n\n0\n= r2\n0, that is, that\nf (xt ) dt = e x\n(cid:3)\n\n2\n\nb\n\nf (xt ) dt\n\n0\n\n(cid:3)\n\n(cid:21)ri\n\ne x\u03c4(a,b)\n\n0 f (cid:21)\u221e \u2264 (cid:21) f (cid:21)\u221e sup\n= (cid:21) f (cid:21)\u221e sup\n\u2264 c(cid:21) f (cid:21)\u221em(a, b) < \u221e.\n\nx\n\nx\n\ngab(x, y) m(dy)\n\n\u221e(cid:9)\n\ni=1\n\n\u03bc f = ri\nri\n\n0 f +\n\n(\u2212\u03bc) j (ri\n\n0\n\n) j+1 f\n\nsince (cid:21)ri\n37.3 with \u03bb = 0 to see that\n\n0\n\n(cid:21)\u221e < \u221e, we can let \u03bc go to zero in (37.2). we can repeat the proof of corollary\n\nprovided \u03bc < (cid:21)ri\n\u03bb for all \u03bb > 0. we\nnow take open intervals in increasing up to r. applying the above to in and letting n \u2192 \u221e,\n\n(cid:21)\u221e. we can then use remark 37.4 to obtain that r1\n\n\u03bb = r2\n\n0\n\n "}, {"Page_number": 352, "text": "334\n\nwe have\n\n(cid:3) \u221e\n\none-dimensional diffusions\n\ne\u2212\u03bbt f (xt ) dt = e x\n\n2\n\ne\u2212\u03bbt f (xt ) dt\n\ne x\n1\n\n0\n\nwhenever f is bounded and borel measurable and x \u2208 r.\n\n(cid:3) \u221e\n\n0\n\n1 f (xt ) = e x\n\nsuppose f is continuous as well. by the uniqueness of the laplace transform, we see\nthat e x\n2 f (xt ) for almost every t, and since both terms are continuous in t, this\nequality holds for all t. by a limit argument, this equality holds for all bounded and borel\nmeasurable f . therefore the one-dimensional distributions of x under px\n\n1 and px\n\n2 agree.\n\ne x\n\nif s < t and f and g are bounded and borel measurable,\nt\u2212sg(xs)] = e x\nt\u2212sg)(xs)] = e x\nt\u2212sg(xs)] = e x\n\n1[ f (xs)g(xt )] = e x\n= e x\n= e x\n\nt\u2212sg(xs)]\n\n1[ f (xs)p2\n\n1[ f (xs)p1\n1[( f p2\n2[ f (xs)p2\n); since the one-dimensional distributions agree, p1\nt\u2212s\n\nt\u2212sg)(xs)]\n2[ f (xs)g(xt )].\n\n2[( f p2\n\nt\u2212s is the semigroup for (xt , px\ni\n\n=\nhere pi\np2\nt\u2212s. we have thus shown the two-dimensional distributions of x under px\n2 agree.\ncontinuing, we see that all the finite-dimensional distributions under px\n2 agree. by\nthe continuity of the paths of x and theorem 2.6, that is enough to show equality of px\n1\nand px\n2.\n\n1 and px\n\n1 and px\n\n(cid:3)\n\n(cid:3)\n\n0\n\n41.5 time change\n\nwe now want to show that if m is a measure such that 0 < m(a, b) < \u221e for all intervals [a, b],\nthen there exists a regular diffusion on natural scale on r having m as a speed measure. if\nm(dx) had a density, say m(dx) = r(x) dx, we would proceed as follows. let wt be a\none-dimensional brownian motion and let\n\nat =\n\nt\n\nr(ws) ds,\n\nbt = inf{u : at > u},\n\nxt = wbt\n\n.\n\nin other words, we let xt be a certain time change of brownian motion. in general, where\nm(dx) does not have a density, we make use of the local times lx\nt of brownian motion; see\nchapter 14.\n\nlet\n\nat =\n\nlx\nt m(dx),\n\nbt = inf{u : au > t},\n\nxt = wbt\n\n.\n\n(41.24)\n\ntheorem 41.9 let (wt , px) be a brownian motion and m a measure on r such that 0 <\nm(a, b) < \u221e for every finite interval (a, b). then, under px, xt as defined by (41.24) is a\nregular diffusion on natural scale with speed measure m.\nproof first we show that xt is a continuous process. fix \u03c9. if we choose a < inf s\u2264t ws and\nb > sups\u2264t ws, then\n\n(cid:3)\n\n(cid:3)\n\nat =\n\nt m(dx) =\nlx\n\nlx\nt 1[a,b](x) m(dx)\n\nt increases only for those times s when ws = x. by the continuity of lx\n\nsince lx\nt and dominated\nconvergence, we conclude that at (\u03c9) is continuous at time t. next we show that at is strictly\nincreasing. fix \u03c9. if s < u, pick t \u2208 (s, u). set x = wt. because the support of the measure\n\n "}, {"Page_number": 353, "text": "41.5 time change\n\n335\n\nt is the set {r : wr = x}, then lx\n\n\u2212 lx\n\ndlx\n> 0 for\nall y in a neighborhood of x, say (x\u2212 \u03b4, x+ \u03b4). since m(x\u2212 \u03b4, x+ \u03b4) > 0, then au \u2212 as > 0.\nhence at is strictly increasing. this and the continuity of at imply that bt is continuous, and\ntherefore xt is continuous.\n\n> 0. by the continuity of local times, ly\nu\n\nu\n\ns\n\ns\n\n\u2212 ly\n\nnext we show that xt is a regular diffusion on natural scale. by monotone convergence\n(a,b) < \u221e, px-a.s.,\n(a,b) denotes the corresponding exit\n\n\u2192 \u221e, a.s., for each x, at \u2191 \u221e, hence bt \u2191 \u221e, so \u03c4 x\n\n(a,b) denotes the exit time of (a, b) by xt and \u03c4 w\n\nand the fact that lx\nt\nwhere \u03c4 x\ntime of wt. moreover,\n\npx(x (\u03c4 x\n\n(a,b)) = b) = px(w (\u03c4 w\n\n(a,b)) = b) = x \u2212 a\nb \u2212 a\n\n,\n\nsince xt is a time change of wt.\nto verify the strong markov property, we repeat the argument of section 22.3. let f(cid:3)\nfbt . then if t is a stopping time for f(cid:3)\ne x[ f (xt+t ) | f(cid:3)\n\nt , we have\nt ] = e x[ f (w (bt+t )) | fbt ].\n\nbt can be seen to be a stopping time for ft and bt+t = bt \u25e6 \u03b8bt where \u03b8t are the shift\noperators, so this is\n\n=\n\nt\n\ne xe w (bt ) f (wbt\n\n) = e xe xt f (xt ).\n\nas in section 20.3, this suffices to show that xt is a strong markov process.\n\nit remains to determine the speed measure of xt. fix (a, b) and write \u03c4x for \u03c4 x\n\n(a,b) and \u03c4w\n\nfor \u03c4 w\n\n(a,b). we have\n\ne x\u03c4x = e x\n= e x\n\n= e x\n\n= e x\n\n= e x\n\n0\n\n(cid:3) \u221e\n(cid:3) \u221e\n(cid:3) \u221e\n(cid:3) (cid:3) \u221e\n(cid:3) (cid:3) \u03c4w\n\n0\n\n0\n\n0\n\n0\n\n1(a,b)(xs\u2227\u03c4x\n\n) ds\n\n1(a,b)(wbs\u2227\u03c4x\n\n1(a,b)(wt\u2227\u03c4w\n\n) ds\n\n) dat\n\n(cid:3)\n\n) ly\n\nt m(dy)\n\n1(a,b)(wt\u2227\u03c4w\nt m(dy) =\nly\n\ne xly\n\n\u03c4w m(dy).\n\nwe also have\n\ne xly\n\u03c4w\n\n= e x|w\u03c4w\n\n\u2212 y| \u2212 |x \u2212 y|\n\nby (14.5). this is equal to\n\n|a \u2212 y|px(w\u03c4w\n= a) + |b \u2212 y|px(w\u03c4w\n= |a \u2212 y| b \u2212 x\n+ |b \u2212 y| x \u2212 a\n(cid:3)\nb \u2212 a\nb \u2212 a\n\n= b) \u2212 |x \u2212 y|\n\n\u2212 |x \u2212 y| = gab(x, y).\n\ne x\u03c4x =\n\ngab(x, y) m(dy),\n\nwe thus have\n\nas required.\n\n "}, {"Page_number": 354, "text": "336\n\none-dimensional diffusions\n\nas a corollary to the proof, we see that a regular diffusion on natural scale is a local\n\nmartingale, since it is a time change of brownian motion.\n\n41.6 examples\n\nlet us look at the solutions to the sde (41.4), but now suppose b is identically zero, or\n\nlet us calculate the scale function and the speed measure for some examples of diffusions.\nfirst we need to connect the speed measure with the coefficients of an sde.\ndxt = \u03c3 (xt ) dwt. we again set a(x) = \u03c3 (x)2.\ntheorem 41.10 suppose c1 < \u03c3 (x) < c2 for all x and \u03c3 is continuous. the speed measure\nof xt is given by\n\nm(dx) = 1\na(x)\nproof since dxt = \u03c3 (xt ) dwt, then (cid:22)x (cid:23)\nt\n0 a(xs) ds. to obtain a brownian motion w t\nby time-changing the martingale xt, we must time-change by the inverse of (cid:22)x (cid:23)\nt. on the\nother hand, from theorem 41.9, xt is the time-change of a brownian motion by bt, where bt\nis given by (41.24). hence\n\ndx.\n\nt\n\n=(cid:15)\n(cid:3)\n\n=\n\nds =\n(cid:3)\ndy =\n(cid:3)\n(cid:3)\n(cid:3)\n\n=\n\n0\nthe inverse of bt, namely, at, must then satisfy\n\nbt = (cid:22)x (cid:23)\n\nt\n\nt\n\na(xs) ds.\n\ndat\ndt\n\n(cid:3)\n\n= 1\na(xat\n\n)\n\n= 1\n\n,\n\na(wt )\n\n(cid:3)\n\nor\n\nfor all t, using theorem 14.4. however, at =(cid:15)\n\na(ws)\n\n1\n\n0\n\nt\n\nat =\n(cid:3)\n\nly\nt\n\n1\n\na(y)\n\ndy\n\nly\nt m(dy) by (41.24). hence\n\nwe know e xly\n\n\u03c4(c,d )\n\n1\n\nly\nt\n\na(y)\n\n(cid:3)\n= gcd (x, y). therefore\ngcd (x, y) m(dy) =\n=\n\nly\nt m(dy).\n\ne xly\n\n\u03c4(c,d ) m(dy)\n\ne xly\n\n\u03c4(c,d )\n\ndy\n\n1\n\na(y)\n1\n\ngcd (x, y)\n\ndy\n\na(y)\n\nfor all c, d, and x, which implies m(dy) = (1/a(y)) dy.\n\nnow we can look at some examples and do calculations.\n\nbrownian motion with constant drift. this process is the solution to the sde dxt = dwt+b dt.\nfrom theorem 41.1, s(x) = exp(\u22122bx) is the scale function. if yt = s(xt ), then\n\n "}, {"Page_number": 355, "text": "exercises\n\n337\n\n\u22121(y)) = \u22122by, or yt corresponds to the operator 2b2y2 f\n\n(cid:3)\u03c3 )(s\n(s\n(4b2y2)\u22121 dx.\nbessel processes. the process is only defined on the state space [0,\u221e) instead of all of r\nand there is a boundary condition at 0. we ignore this here and consider a bessel process of\norder \u03bd up until the first hit of 0. then x solves the sde\n\n, and the speed measure is\n\n(cid:3)(cid:3)\n\ndxt = dwt + \u03bd \u2212 1\n\ndt.\n\n2xt\n\ndyt = (2 \u2212 \u03bd )y (1\u2212\u03bd )/(2\u2212\u03bd )\n\nt\n\ndwt ,\n\nif \u03bd (cid:16)= 2, a calculation using theorem 41.1 shows that s(x) = x2\u2212\u03bd. then yt = s(xt ) satisfies\n\nand the speed measure is\n\nm(dx) = (2 \u2212 \u03bd )\u22122x(2\u03bd\u22122)/(2\u2212\u03bd ) dx,\n\nx > 0.\n\nexercises\n\n41.1 in the proof of proposition 41.2 we used the strong markov property numerous times. write\nout carefully in terms of shift operators and conditional expectations how the strong markov\nproperty is applied in each case.\n\n41.2 give a rigorous proof of (41.9).\n\n41.3 show that if\n\n(cid:3)\n\ngab(x, y) m1(dy) =\n\n(cid:3)\n\ngab(x, y) m2(dy)\n\nfor all x, a, and b, then m1 = m2.\n\n41.4 show that if x is a bessel process of order 2, then the scale function is given by s(x) = log x,\n\nyt = s(xt ) satisfies dyt = e\n\n\u2212yt dwt, and the speed measure is m(dx) = e2x dx.\n\n41.5 suppose x is a regular diffusion whose state space is r. prove that x is on natural scale if and\n\nonly if\n\np(a+b)/2(ta < tb) = 1\n\n2\n\nwhenever a < b.\n\n41.6 let a > 0 and let m(dx) = dx + a \u03b40(dx), where \u03b40 is the point mass at 0. let (xt , px ) be the\ndiffusion on the line on natural scale whose speed measure is given by m. show that under p0,\n\nwith probability one for each t > 0. prove that for each t > 0, zt = {t : xt = 0} contains no\nintervals. thus the zero set of the process x spends an amount of time at 0 that has positive\nlebesgue measure, but the zero set contains no intervals.\n\n41.7 define\n\nma(dx) =\n\n(cid:3)\n\n0\n\nt\n\n1{0}(xs ) ds > 0\n\n(cid:13)\n\ndx,\na dx,\n\nx \u2265 0,\nx < 0.\n\n "}, {"Page_number": 356, "text": "338\n\none-dimensional diffusions\n\n) be the diffusion on natural scale on the line whose speed measure is given by ma.\n\nlet (xt , px\na\nsuppose x > 0.\nkilled) at 0, started at x. what do you think happens when a \u2192 0?\n\nprove that if a \u2192 \u221e, then px\n\na converges weakly to the law of brownian motion absorbed (i.e.,\n\nnotes\n\nwe have considered diffusions on r but most of what we discussed goes through for diffusions\nwhose state space is an interval properly contained in r. in this case, one must specify what\nthe process does when it hits the boundary. being absorbed (i.e., killed) or reflected are two\noptions, but much more complicated behavior is possible. see it\u02c6o and mckean (1965) and\nknight (1981) for the complete story.\n\n "}, {"Page_number": 357, "text": "42\n\nl\u00b4evy processes\n\na l\u00b4evy process is a process with stationary and independent increments whose paths are\nright continuous with left limits. having stationary increments means that the law of xt \u2212 xs\nis the same as the law of xt\u2212s\u2212 x0 whenever s < t. saying that x has independent increments\nmeans that xt \u2212 xs is independent of \u03c3 (xr; r \u2264 s) whenever s < t.\nwe want to examine the structure of l\u00b4evy processes. we have three examples already: the\npoisson process, brownian motion, and the deterministic process xt = t. it turns out that all\nl\u00b4evy processes can be built up out of these building blocks. we will show how to construct\nl\u00b4evy processes and give a representation of an arbitrary l\u00b4evy process.\n\nrecall that we use xt\u2212 = lims<t,s\u2192t xs and \u0001xt = xt \u2212 xt\u2212.\n\n42.1 examples\n\nt , j = 1, . . . , j, be a se-\nlet us begin by looking at some simple l\u00b4evy processes. let p j\nquence of independent poisson processes with parameters \u03bb j, respectively. each p j\nt is a l\u00b4evy\nprocess and the formula for the characteristic function of a poisson random variable (see\nsection a.13) shows that the characteristic function of p j\nt is\nt = exp(t\u03bb j (eiu \u2212 1)).\n\ne eiup j\n\ntherefore the characteristic function of a jp j\n\nt is\n\nand the characteristic function of a jp j\n\ne eiua jp j\n\nt = exp(t\u03bb j (eiua j \u2212 1))\nt \u2212 a j\u03bb jt is\n\ne eiua jpt\n\nj\n\n\u2212a) j\u03bb jt = exp(t\u03bb j (eiua j \u2212 1 \u2212 iua j )).\n\n(dx), where \u03b4a j\n\n(dx) is point\n\nif we let m j be the measure on r defined by m j (dx) = \u03bb j\u03b4a j\n(cid:11)\nmass at a j, then the characteristic function for a jp j\n\n(cid:3)\n\nt can be written as\n\nexp\n\nt\n\n[eiux \u2212 1] m j (dx)\n\nand the one for a jp j\n\nt \u2212 a j\u03bb jt as\n\n(cid:10)\n\nr\n\n[eiux \u2212 1 \u2212 iux] m j (dx)\n\n339\n\n(cid:11)\n\n.\n\nexp\n\nt\n\nr\n\n(cid:10)\n(cid:3)\n\n(42.1)\n\n(42.2)\n\n "}, {"Page_number": 358, "text": "340\n\nnow let\n\nl\u00b4evy processes\n\nxt = j(cid:9)\n\nj=1\n\na jp j\nt .\n\nit is clear that the paths of xt are right continuous with left limits, and the fact that x has\nstationary and independent increments follows from the corresponding property of the p j\u2019s.\nmoreover, the characteristic function of a sum of independent random variables is the product\nof the characteristic functions, so the characteristic function of xt is given by\n\n[eiux \u2212 1] m(dx)\n\n(42.3)\n\n(cid:3)\n\n(cid:10)\n\nt\n\nr\n\n(cid:3)\n\n(cid:10)\n\nt\n\nr\n\nwith m(dx) =(cid:12)\n\n\u03bb j\u03b4a j\nthe process yt = xt \u2212 t\n\nj\nj=1\n\ne eiuxt = exp\n(cid:12)\n\nagain with m(dx) =(cid:12)\n\nj\nj=1\n\n\u03bb j\u03b4a j\n\n(dx).\n\n(dx).\nj\nj=1 a j\u03bb j is also a l\u00b4evy process and its characteristic function is\n\ne eiuyt = exp\n\n[eiux \u2212 1 \u2212 iux] m(dx)\n\n,\n\n(42.4)\n\n(cid:11)\n\n(cid:11)\n\nremark 42.1 recall from proposition a.50 that if \u03d5 is the characteristic function of a\nrandom variable z, then \u03d5(cid:3)(0) = ie z and \u03d5(cid:3)(cid:3)(0) = \u2212e z2. if yt is as in the paragraph above,\nthen clearly e yt = 0, and calculating the second derivative of e eiuyt at 0, we obtain\n\n(cid:3)\n\n= t\n\ne y 2\nt\n\nx2 m(dx).\n\nthe following lemma is a restatement of corollary 4.3.\n\nlemma 42.2 if xt is a l\u00b4evy process and t is a finite stopping time, then xt+t \u2212 xt is a l\u00b4evy\nprocess with the same law as xt \u2212 x0 and independent of f t .\n\n42.2 construction of l\u00b4evy processes\n\na process x has bounded jumps if there exists a real number k > 0 such that supt\nk, a.s.\nlemma 42.3 if xt is a l\u00b4evy process with bounded jumps and with x0 = 0, then xt has\nmoments of all orders, that is, e|xt|p < \u221e for all positive integers p.\n\n|\u0001xt| \u2264\n\nproof suppose the jumps of xt are bounded in absolute value by k. since xt is right\ncontinuous with left limits, there exists m > k such that p(sups\u2264t\n\n|xs| \u2265 2m ) \u2264 1/2.\n\n "}, {"Page_number": 359, "text": "42.2 construction of l\u00b4evy processes\n\nlet t1 = inf{t : |xt| \u2265 m} and ti+1 = inf{t > ti : |xt \u2212 xti\nand then |xt1\n| \u2264 m + k \u2264 2m. we have\n\n341\n| > m}. for s < t1, |xs| \u2264 m,\n\n|xs| \u2265 2(i + 1)m ) \u2264 p(ti+1 \u2264 t ) \u2264 p(ti \u2264 t, ti+1 \u2212 ti \u2264 t )\n\n| \u2264 |xt1\u2212| + |\u0001xt1\np(sup\ns\u2264t\n\n| \u2265 2m, ti \u2264 t )\n\n|xti+s \u2212 xti\n|xs| \u2265 2m )p(ti \u2264 t )\n\n= p(sup\ns\u2264t\n= p(sup\ns\u2264t\n\u2264 1\np(ti \u2264 t ),\n\n2\n\nmn(dx) = n\u22121(cid:9)\n(cid:3)\n\nj=0\n\nusing lemma 42.2 in the last equality. by induction, p(sups\u2264t\nlemma now follows immediately.\n\na key lemma is the following.\n\n|xs| \u2265 2im ) \u2264 2\n\n\u2212i, and the\n\nlemma 42.4 suppose i is a finite interval of the form (a, b), [a, b), (a, b], or [a, b] with\na > 0 and m is a finite measure on r giving no mass to i c. then there exists a l\u00b4evy process\nxt satisfying (42.3).\nproof first let us consider the case where i = [a, b). we approximate m by a discrete\nmeasure. if n \u2265 1, let z j = a + j(b \u2212 a)/n, j = 0, . . . , n \u2212 1, and let\n\nm([z j, z j+1))\u03b4z j\n\n(dx),\n\nwhere \u03b4z j is the point mass at z j. the measures mn converge weakly to m as n \u2192 \u221e in the\nsense that\n\n(cid:3)\n\nf (x) mn(dx) \u2192\n\nf (x) dx\n\nwhenever f is a bounded continuous function on r. for each n, let pn, j\nt\nbe independent poisson processes with parameters m([z j, z j+1)) and let\n\n= n\u22121(cid:9)\n\nj=0\n\nx n\nt\n\nz jpn, j\n\nt\n\n.\n\n, j = 0, . . . , n \u2212 1,\n\nthen x n is a l\u00b4evy process with jumps bounded by b.\n\nby lemma 42.2, if tn is a stopping time for x n, \u03b5 > 0, and \u03b4 > 0, then\n\u03b4 (cid:16)= 0)\n\n(cid:11)\n(cid:10) n\u22121(cid:9)\n\u03b4 | > \u03b5) \u2264 p(x n\n| > \u03b5) = p(|x n\n\ntn+\u03b4 \u2212 x n\n\np(|x n\n\ntn\n\n\u2264 p\n\n(cid:16)= 0\n\n.\n\npn, j\n\n\u03b4\n\nj=0\n\nsince the sum of independent poisson processes is a poisson process, then\npoisson process with parameter\n\nn\u22121(cid:9)\n\nj=0\n\nm([z j, z j+1)) = m(i ).\n\n(42.5)\n\n(cid:12)\n\nn\u22121\nj=0 pn, j\n\nt\n\nis a\n\n "}, {"Page_number": 360, "text": "342\n\nl\u00b4evy processes\n\nthe last line of (42.5) is then bounded by\n1 \u2212 e\n\n\u2212\u03b4m(i ) \u2264 \u03b4m(i ),\n= 0, a.s. we can therefore apply the\nwhich tends to zero uniformly in n as \u03b4 \u2192 0. note x n\naldous criterion (theorem 34.8) to see that the x n are tight with respect to weak convergence\non the space d[0, t0) for any t0.\n\n0\n\nany subsequential weak limit x will have paths that are right continuous with left limits.\n\nfor any continuous bounded function f on r,\n\ne f (x n\nt\n\n\u2212 x n\n\ns\n\n) = e f (x n\nt\u2212s\n\n\u2212 x n\n\n0\n\n).\n\npassing to the limit along an appropriate subsequence,\n\ne f (xt \u2212 xs) = e f (xt\u2212s \u2212 x0).\n\nsince f is an arbitrary bounded continuous function, we see that the laws of xt \u2212 xs and\nxt\u2212s \u2212 x0 are the same. similarly we prove the increments are independent.\nsince x \u2192 eiux is a bounded continuous function and mn converges weakly to m, starting\n\nwith\n\ne exp(iux n\nt\n\n) = exp\n\n[eiux \u2212 1] mn(dx)\n\n,\n\n(cid:3)\n\n(cid:10)\n\nt\n\n(cid:11)\n\nand passing to the limit, we obtain that the characteristic function of x under p is given\nby (42.3).\nif now the interval i contains the point b, we follow the above proof, except we let pn,n\u22121\nbe a poisson random variable with parameter m([zn\u22121, b]). similarly, if i does not contain\nthe point a, we change pn,0\nto be a poisson random variable with parameter m((a, z1)). with\nthese changes, the proof works for intervals i, whether or not they contain either of their\nendpoints.\nremark 42.5 if x is the l\u00b4evy process constructed in lemma 42.4, then yt = xt \u2212 e xt will\nbe a l\u00b4evy process satisfying (42.4).\n\nt\n\nt\n\nhere is the main theorem of this section.\n\ntheorem 42.6 suppose m is a measure on r with m({0}) = 0 and\n\n(cid:3)\n\n(1 \u2227 x2)m(dx) < \u221e.\n\nsuppose b \u2208 r and \u03c3 \u2265 0. there exists a l\u00b4evy process xt such that\n\n(cid:3)\n\n(cid:10)\n\n(cid:18)\n\ne eiuxt = exp\n\niub \u2212 \u03c3 2u2/2 +\n\nt\n\n[eiux \u2212 1 \u2212 iux1(|x|\u22641)]m(dx)\n\nthe above equation is called the l\u00b4evy\u2013khintchine formula. the measure m is called the\n\nr\n\nl\u00b4evy measure. if we let\n\nm(dx) = 1 + x2\nx2 m\n\n(cid:3)(dx)\n\n(cid:19)(cid:11)\n\n.\n\n(42.6)\n\n "}, {"Page_number": 361, "text": "343\n\n(cid:19)(cid:11)\n\n.\n\n42.2 construction of l\u00b4evy processes\n\n(cid:3)\n\n(cid:3)\n\nand\n\n(cid:3) +\n\nb = b\n(cid:18)\n(cid:10)\n\n(|x|\u22641)\n\nx3\n\n1 + x2 m(dx) \u2212\n(cid:16)\n\n(cid:3)\n\n(cid:3) \u2212 \u03c3 2u2/2 +\n\nt\n\niub\n\nthen we can also write\ne eiuxt = exp\n\nx\n\n1 + x2 m(dx),\n\n(|x|>1)\n\n(cid:17)\n\n1 + x2\nx2 m\n\n(cid:3)(dx)\n\nr\n\neiux \u2212 1 \u2212 iux\n1 + x2\n(cid:15)\n\nboth expressions for the l\u00b4evy\u2013khintchine formula are in common use.\n\nx2 m(dx) < \u221e. let mn(dx)\nproof let m(dx) be a measure supported on (0, 1] with\nbe the measure m restricted to (2\nt be independent l\u00b4evy processes whose\n(cid:3)\ncharacteristic functions are given by (42.4) with m replaced by mn; see remark 42.5. note\ne y n\nt\n\n= 0 for all n by remark 42.1. by the independence of the y n\u2019s, if m < n,\n\n\u2212n+1]. let y n\n\n\u2212n, 2\n\n(cid:3)\n\n(cid:10) n(cid:9)\n\n(cid:11)2 = n(cid:9)\n\nn=m\n\n)2 = n(cid:9)\n\nt\n\nn=m\n\ne (y n\nt\n\ne\n\ny n\nt\n\nn=m\n\nx2 mn(dx) = t\n\nx2 m(dx).\n\n\u2212m\n2\n\n2\u2212n\n\nby our assumption on m, this goes to zero as m, n \u2192 \u221e, and we conclude that\nn\nn=0 y n\nt\nconverges in l2 for each t. call the limit yt. it is routine to check that yt has independent and\nstationary increments. each y n\n\nt has independent increments and is mean zero, so\n\n(cid:12)\n\nor y n is a martingale. by doob\u2019s inequalities and the l2 convergence,\n\ne [y n\nt\n\n\u2212 y n\n\ns\n\n| fs] = e [y n\n\n\u2212 y n\n\ns ] = 0,\n\nt\n\n(cid:20)(cid:20)(cid:20)2 \u2192 0\n\ny n\ns\n\n(cid:20)(cid:20)(cid:20) n(cid:9)\n\nn=m\n\ne sup\ns\u2264t\n\n(cid:12)\n\nmk\nn=1 y n\n\nand form xt =(cid:12)\u221e\n(cid:15)\n\nas m, n \u2192 \u221e, and hence there exists a subsequence mk such that\ns converges\nuniformly over s \u2264 t, a.s. therefore the limit yt will have paths that are right continuous with\nleft limits.\nif m is a measure supported in (1,\u221e) with m(r) < \u221e, we do a similar procedure starting\nwith l\u00b4evy processes whose characteristic functions are of the form (42.3). we let mn(dx) be\nthe restriction of m to (2n, 2n+1], let x n\nt be independent l\u00b4evy processes corresponding to mn,\nt . since m(r) < \u221e, for each t0, the number of times t less than t0\njumps is finite. this shows xt has paths that are right continuous\n\nat which any one of the x n\nt\nwith left limits, and it is easy to then see that xt is a l\u00b4evy process.\nt be l\u00b4evy processes with characteristic\nfunctions given by (42.3) with m replaced by the restriction of m to (1,\u221e) and (\u2212\u221e,\u22121),\nrespectively, let x 3\nt be l\u00b4evy processes with characteristic functions given by (42.4) with\n= bt, and let x 6\nm replaced by the restriction of m to (0, 1] and [\u22121, 0), respectively, let x 5\nt\nbe \u03c3 times a brownian motion. suppose the x i\u2019s are all independent. then their sum will be\na l\u00b4evy process whose characteristic function is given by (42.6).\n\nx2 \u2227 1 m(dx) < \u221e. let x 1\n\nfinally, suppose\n\nn=0 x n\n\nt , x 2\n\nt , x 4\n\nt\n\na key step in the construction was the centering of the poisson processes to get l\u00b4evy\nprocesses with characteristic functions given by (42.4). without the centering one is forced\nto work only with characteristic functions given by (42.3).\n\n "}, {"Page_number": 362, "text": "344\n\nl\u00b4evy processes\n\n42.3 representation of l\u00b4evy processes\n\nwe now work toward showing that every l\u00b4evy process has a characteristic function of the\nform given by (42.6).\n\nlemma 42.7 if xt is a l\u00b4evy process and a is a borel subset of r that is a positive distance\nfrom 0, then\n\n(cid:9)\n\nnt (a) =\n\n1a(\u0001xs)\n\ns\u2264t\n\nis a poisson process.\nsaying that a is a positive distance from 0 means that inf{|x| : x \u2208 a} > 0.\nproof since xt has paths that are right continuous with left limits and a is a positive distance\nfrom 0, then there can only be finitely many jumps of x that lie in a in any finite time interval,\nand so nt (a) is finite and has paths that are right continuous with left limits. it follows from\nthe fact that xt has stationary and independent increments that nt (a) also has stationary and\nindependent increments. we now apply proposition 5.4.\ntheorem 42.8 let xt be a l\u00b4evy process with x0 = 0 and let a1, . . . , an be disjoint bounded\nborel subsets of (0,\u221e), each a finite distance from 0. set\n\nand\n\nnt (ak ) =\n\n(cid:9)\nyt = xt \u2212 n(cid:9)\n\ns\u2264t\n\nk=1\n\n1ak\n\n(\u0001xs)\n\nnt (ak ).\n\nthen the processes nt (a1), . . . , nt (an), and yt are mutually independent.\nproof define \u03bb(a) = e n1(a). the previous lemma shows that if \u03bb(a) < \u221e, then\nnt (a) is a poisson process, and clearly its parameter is \u03bb(a). the result now follows from\ntheorem 18.3.\n\nhere is the representation theorem for l\u00b4evy processes.\n\ntheorem 42.9 suppose xt is a l\u00b4evy process with x0 = 0. then there exists a measure m on\nr \u2212 {0} with\n\n(cid:3)\n\n(1 \u2227 x2) m(dx) < \u221e\n\nand real numbers b and \u03c3 such that the characteristic function of xt is given by (42.6).\nproof define m(a) = e n1(a) if a is a bounded borel subset of (0,\u221e) that is a positive\ndistance from 0. since n1(\u222a\u221e\nk=1 n1(ak ) if the ak are pairwise disjoint and each\nis a positive distance from 0, we see that m is a measure on [a, b] for each 0 < a < b < \u221e,\nand m extends uniquely to a measure on (0,\u221e).\n\nk=1ak ) =(cid:12)\u221e\n\n "}, {"Page_number": 363, "text": "42.3 representation of l\u00b4evy processes\n\n345\n\nfirst we want to show that\n\ns\u2264t\n\n\u0001xs1(\u0001xs>1) is a l\u00b4evy process with characteristic function\n\nsince the characteristic function of the sum of independent random variables is equal to the\nproduct of the characteristic functions, it suffices to suppose 0 < a < b and to show that\n\n(cid:12)\n\n(cid:10)\n\n(cid:3) \u221e\n\n(cid:11)\n\n.\n\n(cid:11)\n\nexp\n\nt\n\n1\n\n(cid:10)\ne eiuzt = exp\n(cid:9)\n\nt\n\nzt =\n\ns\u2264t\n\n[eiux \u2212 1] m(dx)\n(cid:3)\n\n(a,b]\n\n\u0001xs1(a,b](\u0001xs).\n\n[eiux \u2212 1] m(dx)\n\n,\n\nwhere\n\n(cid:12)\n\nlet n > 1 and z j = a+ j(b\u2212 a)/n. by lemma 42.7, nt ((z j, z j+1]) is a poisson process with\nparameter\n\n(cid:14) j = e n1((z j\u22121, z j]) = m((z j, z j+1]).\n\nthus\n\nn\u22121\nj=0 z jnt ((z j, z j+1]) has characteristic function\n\nn\u22121(cid:31)\n\nj=0\n\nexp(t(cid:14) j (eiuz j \u2212 1)) = exp\n\n(cid:3)\n\n(cid:10)\n\n(cid:10)\n\nn\u22121(cid:9)\n\nt\n\nj=0\n\n(cid:11)\n\n,\n\n(eiuz j \u2212 1)(cid:14) j\n(cid:11)\n\nwhich is equal to\n\nt\n\nexp\n\nwhere mn(dx) =(cid:12)\n\n(eiux \u2212 1) mn(dx)\n(42.7)\nt converges to zt as n \u2192 \u221e, passing to the limit\nnext we show that m(1,\u221e) < \u221e. (we write m(1,\u221e) instead of m((1,\u221e)) for esthetic\n\nshows that zt has a characteristic function of the form (42.6).\nreasons.) if not, m(1, k ) \u2192 \u221e as k \u2192 \u221e. then for each fixed l and each fixed t,\n\n(dx). since zn\n\nn\u22121\nj=0\n\n(cid:14) j\u03b4z j\n\n,\n\nlim sup\nk\u2192\u221e\n\np(nt (1, k ) \u2264 l) = lim sup\nk\u2192\u221e\n\n\u2212tm(1,k ) m(1, k ) j\ne\n\nj!\n\n= 0.\n\nthis implies that nt (1,\u221e) = \u221e for each t. however, this contradicts the fact that xt has\npaths that are right continuous with left limits.\n\nl(cid:9)\n\nj=0\n\nwe define m on (\u2212\u221e, 0) similarly.\nwe now look at\n\n(cid:9)\n\ns\u2264t\n\nyt = xt \u2212\n\n\u0001xs1(|\u0001xs|>1).\n\nthis is again a l\u00b4evy process, and we need to examine its structure. this process has\nbounded jumps, hence has moments of all orders. by subtracting c1t for an appropriate\nconstant c1, we may suppose yt has mean 0. let i1, i2, . . . be an ordering of the intervals\n{[2\n\n\u2212m), (\u22122\n\n\u2212(m+1), 2\n\n\u2212m,\u22122\n\n(cid:9)\n\u2212(m+1)] : m \u2265 0}. let\n\n(cid:14)x k\n\nt\n\n=\n\n\u0001xs1(\u0001xs\u2208ik )\n\ns\u2264t\n\n "}, {"Page_number": 364, "text": "t . by corollary 18.3 and the fact that all the x k have mean zero,\n\n(cid:11)2(cid:17)\n\n= e (yt )2 < \u221e.\n\n346\n\nl\u00b4evy processes\n\n\u2212 e(cid:14)x k\n\n=(cid:14)x k\n\u221e(cid:9)\n\nt\n\nk=1\n\ne (x k\nt\n\n)2 \u2264 e\n\nand let x k\nt\n\n(cid:16)(cid:10)\n\u221e(cid:9)\nyt \u2212\n(cid:16) n(cid:9)\n\n(cid:11)2(cid:17)\n(cid:16)(cid:10) \u221e(cid:9)\n(cid:17)2 = n(cid:9)\ntends to zero as m, n \u2192 \u221e, and thus xt \u2212(cid:12)\n\n+ e\n\nhence\n\nk=m\n\nk=1\n\nk=1\n\nx k\nt\n\nx k\nt\n\ne (x k\nt\n\ne\n\nwill be a l\u00b4evy process independent of all the x k\ncontinuous. since all the x k have mean zero, then e x c\nt\nincrements,\n\u2212 x c\n\n| fs] = e [x c\n\n\u2212 x c\n\ne [x c\nt\n\ns\n\nx k\nt\n\n)2\n\nt converges in l2. the limit, x c\n\nk=m\nn\nk=1 x k\nt , say,\nt . moreover, x c has no jumps, i.e., it is\n= 0. by the independence of the\ns ] = 0,\n\nt\n\nand we see x c is a continuous martingale. using the stationarity and independence of the\nincrements,\n\ne [(x c\ns+t\n\n)2] = e [(x c\n= e [(x c\n\ns\n\ns\n\n)2] + 2e [x c\n)2] + e [(x c\n\ns\n\nt\n\n(x c\ns+t\n)2],\n\n\u2212 x c\n\ns\n\n)] + e [(x c\ns+t\n\n\u2212 x c\n\ns\n\n)2]\n\nwhich implies that there exists a constant c2 such that e (x c\nt\n\u2212 x c\n\u2212 x c\n\ne [(x c\nt\n\ns\n\n)2 = c2t. we then have\n)2 | fs] \u2212 c2(t \u2212 s)\n)2] \u2212 c2(t \u2212 s)\n\ns\n\ns\n\nthe quadratic variation process of x c\n(theorem 12.1), x c\nt\n\n/\n\nc2 is a constant multiple of brownian motion.\n\nis therefore c2t, and by l\u00b4evy\u2019s theorem\n1\u22121 x2 m(dx) < \u221e. but by remark 42.1,\n\nto complete the proof, it remains to show that\n\ns\n\ns\n\n\u221a\n\n)2 \u2212 c2t | fs] = (x c\n= (x c\n= (x c\n(cid:3)\n(cid:9)\n(cid:15)\n\nik\n\nk\n\n)2 \u2212 c2s + e [(x c\n)2 \u2212 c2s + e [(x c\n)2 \u2212 c2s.\n\nt\n\nt\n\n(cid:15)\nx2 m(dx) = e (x k\n\n)2,\n\n1\n\ne (x k\n1\n\n)2 \u2264 e y 2\n\n1\n\n< \u221e.\n\nand we have seen that\n\ncombining gives the finiteness of\n\n1\u22121 x2 m(dx).\n\nexercises\n\n42.1 let \u03b1 \u2208 (0, 2) and let x be a l\u00b4evy process where b = \u03c3 = 0 in the l\u00b4evy\u2013khintchine formula\nand the l\u00b4evy measure is m(dx) = c|x|\u22121\u2212\u03b1 dx. show that if a > 0 and yt = a1/\u03b1xat, then y has\nthe same law as x . the process x is known as a symmetric stable process of index \u03b1.\n\n42.2 suppose wt = (w 1\n\n, w 2\nt\n\ninf{t > 0 : w 1\n\n> s}. prove that w 2\nhint: use scaling to make a guess.\n\nt\n\nt\n\n) is a two-dimensional brownian motion started at 0. let \u03c4s =\n\n\u03c4t is a l\u00b4evy process and determine the l\u00b4evy measure.\n\n "}, {"Page_number": 365, "text": "exercises\n\n347\n\n42.3 let w be a one-dimensional brownian motion and let l0 be the local time at 0. let tt be the\n\u2265 t}. show tt is a l\u00b4evy process and determine the l\u00b4evy\n\ninverse of l0, that is, tt = inf{s : l0\nmeasure.\n\ns\n\nhint: use scaling to get started.\n\n42.4 let wt be a one-dimensional brownian motion, ly\n\nlocal time at 0, that is, tt = inf{s : l0\n\n\u2265 t}. let x > 0 be fixed. prove that lx\n\nt the local time at level y, and tt the inverse\nis a l\u00b4evy process.\n\ns\n\ntt\n\n42.5 let x be a l\u00b4evy process with l\u00b4evy measure m. prove that if a and b are disjoint closed sets,\n\nthen\n\ne x\n\n1a(xs\u2212 )1b(xs ) = e x\n\nt\n\n1a(xs )m(b \u2212 xs ) ds\n\n(cid:3)\n\n0\n\nfor each x, where b \u2212 y = {z \u2212 y : z \u2208 b}. this is the l\u00b4evy system formula in the case of l\u00b4evy\nprocesses. there is an analogous formula for hunt processes.\n\n42.6 a stable subordinator x of order \u03b1 \u2208 (0, 1) is a l\u00b4evy process whose characteristic function\nis given by( 42.6), where b = \u03c3 2 = 0 and m(dx) = c1(x>0)|x|\u2212\u03b1\u22121 dx. suppose x is a stable\nsubordinator of index \u03b1 and w is a brownian motion. show that, up to a deterministic time\nchange, the process zt = wxt is a symmetric stable process of index 2\u03b1.\nhint: start by using scaling.\n\n42.7 let zt be a symmetric stable process of order \u03b1 \u2208 (0, 2). show that if \u03b5 > 0, then\n\n(cid:9)\n\ns\u2264t\n\n|zt|\nt \u03b1+\u03b5\n\nlim\nt\u2192\u221e\n\n= 0,\n\na.s.\n\n "}, {"Page_number": 366, "text": "appendix a\n\nbasic probability\n\nthis appendix covers the facts from basic probability that we will need. the presentation\nhere is not precisely what i use when i teach such a course. for example, in a course i\nprove the strong law of large numbers without using martingales, i present the inversion\ntheorem for characteristic functions, i make use of l\u00b4evy\u2019s continuity theorem, and so on.\nnevertheless, proofs of all the facts from probability needed in the main part of the text are\ngiven.\n\na.1 first notions\n\na probability or probability measure is a measure whose total mass is one. instead of denoting\na measure space by (x ,a, \u03bc), probabilists use (\u0001,f , p). here \u0001 is a set, f is called a\n\u03c3 -field (which is the same thing as a \u03c3 -algebra), and p is a measure with p(\u0001) = 1. elements\nof f are called events. a typical element of \u0001 is denoted \u03c9.\n\n(cid:15)\n\ninstead of saying a property occurs almost everywhere, we talk about properties occurring\nalmost surely, written a.s. real-valued measurable functions from \u0001 to r are called random\nvariables and are usually denoted by x or y or other capital letters.\n\nvalue, and we write e x for\n\nx dp. the notation e [x ; a] is often used for\n\nintegration (in the sense of lebesgue) with respect to p is called expectation or expected\nthe random variable 1a is the function that is one if \u03c9 \u2208 a and zero otherwise. it is called\nthe indicator of a (the name \u201ccharacteristic function\u201d in probability refers to the fourier\ntransform). events such as {\u03c9 : x (\u03c9) > a} are almost always abbreviated by (x > a) or\n{x > a}.\n\na x dp.\n\n(cid:15)\n\ngiven a random variable x , we can define a probability on the borel \u03c3 -field of r by\n\npx (a) = p(x \u2208 a),\n\na \u2282 r.\n\n(a.1)\nthe probability px is called the law of x or the distribution of x . we define fx : r \u2192 [0, 1]\nby\n\nfx (x) = px ((\u2212\u221e, x]) = p(x \u2264 x).\n\n(a.2)\n\nthe function fx is called the distribution function of x .\n\n348\n\n "}, {"Page_number": 367, "text": "a.1 first notions\n\n349\n\nproposition a.1 the distribution function fx of a random variable x satisfies:\n\n(1) fx is increasing;\n(2) fx is right continuous with left limits;\n(3) limx\u2192\u221e fx (x) = 1 and limx\u2192\u2212\u221e fx (x) = 0.\n\nproof we prove the right continuity of fx and leave the rest of the proof to the reader.\nif xn \u2193 x, then (x \u2264 xn) \u2193 (x \u2264 x), and so p(x \u2264 xn) \u2193 p(x \u2264 x) since p is a finite\nmeasure.\nnote that if xn \u2191 x, then (x \u2264 xn) \u2191 (x < x), and so fx (xn) \u2191 p(x < x).\n\nany function f : r \u2192 [0, 1] satisfying (1)\u2013(3) of proposition 1.1 is called a distribution\n\nfunction, whether or not it comes from a random variable.\n\nproposition a.2 suppose f is a distribution function. there exists a random variable x\nsuch that f = fx .\nproof let \u0001 = [0, 1], f the borel \u03c3 -field, and p a lebesgue measure. define x (\u03c9) =\nsup{x : f (x) < \u03c9}. it is routine to check that fx = f.\n\nin the above proof, essentially x = f\n\n\u22121. however f may have jumps or be constant over\n\nsome intervals, so some care is needed in defining x .\n\ncertain distributions or laws are very common. we list some of them.\n(1) bernoulli. a random variable is bernoulli if p(x = 1) = p, p(x = 0) = 1 \u2212 p for\npk (1 \u2212 p)n\u2212k, where n is a positive\n(2) binomial. this is defined by p(x = k ) =\nk!(n\u2212k )! .\n\n= n!\n\n$\n\n%\n\nn\nk\n\n$\n\nsome p \u2208 [0, 1].\n\ninteger, 0 \u2264 k \u2264 n, p \u2208 [0, 1], and\n\n%\n(3) point mass at a. here p(x = a) = 1.\n(4) poisson. for \u03bb > 0 we set p(x = k ) = e\nf = f\n(cid:3)\ndistribution function of a random variable x , then\n\nif f is absolutely continuous, we call\n\nn\nk\n\n\u2212\u03bb\u03bbk/k! again k is a non-negative integer.\n\n(cid:3)\n\nthe density of f. if such an f is the\n\np(x \u2208 a) =\n\nf (x) dx.\n\na\n\nsome examples of distributions characterized by densities are the following.\n(5) uniform on [a, b]. define f (x) = (b \u2212 a)\u221211[a,b](x).\n\u2212\u03bbx and set f (x) = 0 for x < 0.\n(6) exponential. for x \u2265 0 let f (x) = \u03bbe\n(7) standard normal. define f (x) = 1\u221a\n\u2212x2/2 for x \u2208 r.\n2\u03c0 e\nlet us verify that the integral of f is one. to do that, let\n\n(cid:3) \u221e\n\n0\n\ni =\n\n\u2212x2/2 dx,\ne\n\n "}, {"Page_number": 368, "text": "\u03c0 /2. using the fubini theorem, the monotone convergence\n\ntheorem, and a change of variables to polar coordinates, we write\n\n350\n\nand it suffices to show i = \u221a\n(cid:10)(cid:3) \u221e\n(cid:3) \u221e\n(cid:3) \u221e\n(cid:3) (cid:3)\n(cid:3) \u03c0 /2\n\n0\n\n0\n\n(cid:3)\n\nbasic probability\n\n(cid:11)(cid:10)(cid:3) \u221e\n\n(cid:11)\n\n\u2212x2/2 dx\ne\n\n\u2212y2/2 dy\ne\n\n0\n\n\u2212(x2+y2 )/2 dx dy\ne\n\n\u2212(x2+y2 )/2 dx dy\ne\n\nx,y\u22650,x2+y2\u2264r2\n\nr\n\n\u2212r2/2 r dr d\u03b8\ne\n\u2212r2/2) = \u03c0\n2\n\n0\n\n(1 \u2212 e\n\n0\n\u03c0\n2\n\n0\n\ni 2 =\n=\n= lim\nr\u2192\u221e\n= lim\nr\u2192\u221e\n= lim\nr\u2192\u221e\n\nas desired.\nzero and variance one, which means that e z = 0 and e z2 = 1.\n\nwe shall see later ((a.4) and (a.5)) that a standard normal random variable z has mean\n\n(8) normal random variables with mean \u03bc and variance \u03c3 2. if z is a standard normal\nrandom variable, then a normal random variable x with mean \u03bc and variance \u03c3 2 has the\nsame distribution as \u03bc+ \u03c3 z. it is an exercise in calculus to check that such a random variable\nhas density\n\nf (x) = 1\u221a\n2\u03c0 \u03c3\n\n\u2212(x\u2212\u03bc)2/2\u03c3 2 .\ne\n\n(a.3)\n\n(9) gamma. a random variable x has a gamma distribution with parameters r and \u03bb (both\n\nr and \u03bb must be positive) if it has density\nf (x) = \u03bbe\n\nfor x \u2265 0 and f (x) = 0 if x < 0, where \u0001(r) = (cid:15) \u221e\n\nrecall \u0001(k) = (k \u2212 1)! for k a non-negative integer.\n\n\u2212\u03bbx(\u03bbx)r\u22121/\u0001(r)\n0 e\n\n\u2212yyr\u22121 dy is the gamma function.\n\nwe can use the law of a random variable to calculate expectations.\n\nproposition a.3 let x be a random variable. if g is bounded or non-negative, then\n\n(cid:3)\n\ne g(x ) =\n\ng(x) px (dx).\n\nif g is the indicator of an event a, this is just the definition of px . by linearity, the\nproof\nresult holds for simple functions. by the monotone convergence theorem, the result holds\nfor non-negative functions, and by linearity again, it holds for bounded g.\n\nif fx has a density f , then px (dx) = f (x) dx. in this case e x = (cid:15)\ne x 2 =(cid:15)\n\nx f (x) dx and\nx2 f (x) dx. (we need e|x | finite to justify the first equality if x is not necessarily\nnon-negative.) we define the mean of a random variable to be its expectation, and the variance\nof a random variable is defined by\n\nvar x = e (x \u2212 e x )2.\n\nthe pth moment of x is e x p if p is a positive integer.\n\n "}, {"Page_number": 369, "text": "a.1 first notions\n\n351\n\nnote\n\nvar x = e [x 2 \u2212 2(x )(e x ) + (e x )2] = e x 2 \u2212 (e x )2.\n\nlet us calculate a few examples. since xe\n\n\u2212x2/2 is an odd function, if z is a standard normal\n\n(cid:3)\n\nx\n\n1\u221a\n2\u03c0\n\n\u2212x2/2 dx = 0.\ne\n\nrandom variable, then\n\nusing integration by parts,\n\ne z2 =\n\ne z =\n(cid:3)\n\n(a.4)\n\n(a.5)\n\n\u2212x2/2 dx\ne\n\n(cid:3)\nx2 1\u221a\n\u2212x2/2 dx\ne\n2\u03c0\nx2 1\u221a\nn\n2\u03c0\n\u2212n 2/2 +\n\u2212n\n\u2212x2/2 dx = 1,\ne\n\n\u2212n\n(cid:3)\n\u22122ne\n\n(cid:3)\n\nn\n\n= lim\nn\u2192\u221e\n= lim\nn\u2192\u221e\n= 1\u221a\n2\u03c0\n\n1\u221a\n2\u03c0\n\n\u2212x2/2 dx\ne\n\n(cid:3)\n\n(cid:3)\n\neaxe\n\nea2/2\n\ne eaz = 1\u221a\n2\u03c0\n= 1\u221a\n2\u03c0\n\n\u2212x2/2 dx\n\u2212(x\u2212a)2/2 dx = ea2/2.\ne\n\nand so var z = 1.\n\nby completing the square and a change of variables, we calculate\n\nif x is a normal random variable with mean \u03bc and variance \u03c3 2, we can write x = \u03bc+ \u03c3 z\n\nfor z a standard normal random variable, and obtain\n\ne eax = ea\u03bce ea\u03c3 z = ea\u03bc+a2\u03c3 2/2.\nif x is a poisson random variable with parameter \u03bb, then\n\ne x =\n\n=\n\nke\n\n\u2212\u03bb \u03bbk\nk!\n\n\u221e(cid:9)\n\nk=0\n= \u03bbe\n\u2212\u03bb\n\nke\n\n\u2212\u03bb \u03bbk\nk!\n\n= \u03bb.\n\n\u03bbk\u22121\n(k \u2212 1)!\na similar calculation shows that e [x (x \u2212 1)] = \u03bb2, so\n\nk=1\n\n\u221e(cid:9)\n\nk=1\n\n\u221e(cid:9)\n\nvar x = e [x (x \u2212 1)] + e x \u2212 (e x )2 = \u03bb.\n\n(a.6)\n\n(a.7)\n\n(a.8)\n\na straightforward application of integration by parts shows that if x is an exponential\n\n(cid:3) \u221e\n\nrandom variable with parameter \u03bb, then\ne x =\n\nanother equality that is useful is the following.\n\n0\n\n\u2212\u03bbx dx = 1\n\n\u03bbxe\n\n\u03bb\n\n.\n\n(a.9)\n\n "}, {"Page_number": 370, "text": "352\nproposition a.4 if x \u2265 0, a.s., and p > 0, then\n\nbasic probability\n\n(cid:3) \u221e\n\ne x p =\n\np\u03bbp\u22121p(x > \u03bb) d\u03bb.\n\n0\n\n(cid:3) \u221e\n\nthe proof will show that this equality is also valid if we replace p(x > \u03bb) by p(x \u2265 \u03bb).\nproof using the fubini theorem and writing\np\u03bbp\u22121p(x > \u03bb) d\u03bb = e\n= e\n\np\u03bbp\u221211(\u03bb,\u221e)(x ) d\u03bb\np\u03bbp\u22121 d\u03bb = e x p\n\n(cid:3) \u221e\n(cid:3)\n\nx\n\n0\n\n0\n\ngives the proof.\n\n0\n\nwe need two elementary inequalities. the first is known as chebyshev\u2019s inequality.\n\nproposition a.5 if x \u2265 0,\n\nproof we write\n\np(x \u2265 a) \u2264 e x\n(cid:11)\n(cid:16)(cid:10)\na\nsince x /a is bigger than or equal to 1 when x \u2208 [a,\u221e).\n\np(x \u2265 a) = e\n\n1[a,\u221e)(x )\n\n\u2264 e\n\n(cid:16)\n\n(cid:17)\n\nx\na\n\n.\n\nif we apply this to x = (y \u2212 e y )2, we obtain\n\n(cid:17)\n\n\u2264 e x /a,\n\n1[a,\u221e)(x )\n\np(|y \u2212 e y| \u2265 a) = p((y \u2212 e y )2 \u2265 a2) \u2264 var y/a2.\n\n(a.10)\n\nthis special case of chebyshev\u2019s inequality is sometimes itself referred to as chebyshev\u2019s\ninequality, while proposition a.5 is sometimes called the markov inequality.\n\nthe second inequality we need is jensen\u2019s inequality, not to be confused with jensen\u2019s\n\nformula of complex analysis.\n\nproposition a.6 suppose g is convex and x and g(x ) are both integrable. then\n\ng(e x ) \u2264 e g(x ).\n\nproof one property of convex functions is that they lie above their tangent lines, and more\ngenerally, their support lines. thus if x0 \u2208 r, we have\n\nfor some constant c. letting x = x (\u03c9) and taking expectations, we obtain\n\ng(x) \u2265 g(x0) + c(x \u2212 x0)\n\ne g(x ) \u2265 g(x0) + c(e x \u2212 x0).\n\nnow set x0 equal to e x .\n\nif an is a sequence of sets, define (an i.o.), read \u201can infinitely often,\u201d by\n\n(an i.o.) = \u2229\u221e\nn=1\n\n\u222a\u221e\ni=n ai.\n\nthis set consists of those \u03c9 that are in infinitely many of the an.\n\n "}, {"Page_number": 371, "text": "a.2 independence\n\n353\n\na simple but very important proposition is the borel\u2013cantelli lemma. it has two parts,\n\nand we prove the first part here, leaving the second part to the next section.\n\n(cid:12)\n\np(an) < \u221e,\n\nn\n\nthen\n\nproposition a.7 let a1, a2, . . . be a sequence of events. if\np(an i.o.) = 0.\nproof we write\n\np(an i.o.) = lim\n\nn\u2192\u221e p(\u222a\u221e\n\ni=nai) \u2264 lim sup\nn\u2192\u221e\n\np(ai) = 0,\n\n\u221e(cid:9)\n\ni=n\n\nand we are done.\n\nwe say two events a and b are independent if p(a\u2229 b) = p(a)p(b). the events a1, . . . , an\nare independent if\n\na.2 independence\n\np(ai1\n\n\u2229 ai2\n\n)\u00b7\u00b7\u00b7 p(ai j\nfor each subset {i1, . . . , i j} of {1, . . . , n} with 1 \u2264 i1 < \u00b7\u00b7\u00b7 < i j \u2264 n.\nproposition a.8 if a and b are independent, then ac and b are independent.\n\n\u2229 \u00b7\u00b7\u00b7 \u2229 ai j\n\n) = p(ai1\n\n)p(ai2\n\n)\n\nproof we write\n\np(ac \u2229 b) = p(b) \u2212 p(a \u2229 b) = p(b) \u2212 p(a)p(b)\n\n= p(b)(1 \u2212 p(a)) = p(b)p(ac).\n\nthis is all there is to the proof.\n\nwe say two \u03c3 -fields f and g are independent if a and b are independent whenever a \u2208 f\nand b \u2208 g. two random variables x and y are independent if \u03c3 (x ), the \u03c3 -field generated by\nx , and \u03c3 (y ), the \u03c3 -field generated by y , are independent. (recall that the \u03c3 -field generated\nby a random variable x is given by {(x \u2208 a) : a a borel subset of r}.) we define the\nindependence of n \u03c3 -fields or n random variables in a similar way.\n\nremark a.9 if f and g are borel functions and x and y are independent, then f (x ) and\ng(y ) are independent. this follows because the \u03c3 -field generated by f (x ) is a sub-\u03c3 -field\nof the one generated by x , and similarly for g(y ).\n\nto construct independent random variables, we can use the following.\n\n= fi, i = 1, . . . , n.\n\nproposition a.10 if f1, . . . , fn are distribution functions, there exist independent random\nvariables x1, . . . , xn such that fxi\nproof let \u0001 = [0, 1]n, f the borel \u03c3 -field on \u0001, and p an n-dimensional lebesgue\nmeasure on \u0001. if \u03c9 = (\u03c91, . . . , \u03c9n), define xi(\u03c9) = sup{x : fi(x) < \u03c9i}. as in proposition\n= fi. we deduce the independence from the fact that p is a product measure, in fact,\na.2, fxi\nthe n-fold product of one-dimensional lebesgue measure on [0, 1].\n\n "}, {"Page_number": 372, "text": "354\n\nbasic probability\n\nlet fx ,y (x, y) = p(x \u2264 x, y \u2264 y) denote the joint distribution function of two random\nvariables x and y . (the comma inside the set means \u201cand\"; this is a standard convention in\nprobability.)\nproposition a.11 fx ,y (x, y) = fx (x)fy (y) if and only if x and y are independent.\nproof\n\nif x and y are independent, then\n\nfx ,y (x, y) = p(x \u2264 x, y \u2264 y) = p(x \u2264 x)p(y \u2264 y) = fx (x)fy (y).\n\nconversely, if the inequality holds, fix y and let my denote the collection of sets a for\nwhich p(x \u2208 a, y \u2264 y) = p(x \u2208 a)p(y \u2264 y). my contains all sets of the form (\u2212\u221e, x].\nit follows by linearity that my contains all sets of the form (x, z], and then by linearity\nagain, all sets that are the finite union of such half-open, half-closed intervals. note that the\ncollection of finite unions of such intervals, a, is an algebra generating the borel \u03c3 -field. it\nis clear that my is a monotone class, so by the monotone class theorem (theorem b.2), my\ncontains the borel \u03c3 -field.\nfor a fixed set a, let ma denote the collection of sets b for which p(x \u2208 a, y \u2208 b) =\np(x \u2208 a)p(y \u2208 b). again, ma is a monotone class and by the preceding paragraph\ncontains the \u03c3 -field generated by the collection of finite unions of intervals of the form (x, z],\nand hence contains the borel sets. therefore x and y are independent.\n\nthe following is known as the multiplication theorem.\n\nproposition a.12 if x , y , and x y are integrable and x and y are independent, then\ne [x y ] = (e x )(e y ).\nproof consider the pairs (zx , zy ) with zx being \u03c3 (x ) measurable and zy being \u03c3 (y )\nmeasurable for which the multiplication theorem is true. it holds for zx = 1a(x ) and\nzy = 1b(y ) with a and b borel subsets of r by the definition of x and y being independent.\nit holds for simple random variables (zx , zy ), that is, linear combinations of indicators, by the\nlinearity of both sides. it holds for non-negative random variables by monotone convergence.\nand it holds for integrable random variables by linearity again.\n\nif x1, . . . , xn are independent, then so are x1 \u2212 e x1, . . . , xn \u2212 e xn. assuming everything\n\nis integrable,\n\ne [(x1 \u2212 e x1) + \u00b7\u00b7\u00b7 (xn \u2212 e xn)]2 = e (x1 \u2212 e x1)2 + \u00b7\u00b7\u00b7 + e (xn \u2212 e xn)2,\n\nusing the multiplication theorem to show that the expectations of the cross-product terms are\nzero. we have thus shown\n\nvar (x1 + \u00b7\u00b7\u00b7 + xn) = var x1 + \u00b7\u00b7\u00b7 + var xn.\n\n(a.11)\n\nwe finish up this section by proving the second half of the borel\u2013cantelli lemma.\n\nproposition a.13 suppose an is a sequence of independent events. if\n\n\u221e(cid:9)\n\np(an) = \u221e,\n\nthen p(an i.o.) = 1.\n\nn=1\n\n "}, {"Page_number": 373, "text": "note that here the an are independent, while in the first half of the borel\u2013cantelli lemma\n\na.3 convergence\n\n355\n\nno such assumption was necessary.\n\nproof note\n\n) = 1 \u2212 n(cid:31)\n\ni=n\n\np(ac\ni\n\n(1 \u2212 p(ai)) \u2265 1 \u2212 exp\n\np(\u222an\n\ni=nai) = 1 \u2212 p(\u2229n\n\ni=nac\ni\n\n= 1 \u2212 n(cid:31)\n\ni=n\n\n)\n\n(cid:10)\n\n\u2212 n(cid:9)\n\ni=n\n\n(cid:11)\n\np(ai)\n\n,\n\nusing the inequality 1 \u2212 x \u2264 e\np(\u222a\u221e\n\ni=nai) = 1. this holds for all n, which proves the result.\n\n\u2212x for x > 0. as n \u2192 \u221e, the right-hand side tends to one, so\n\na.3 convergence\n\nin this section we consider three ways a sequence of random variables xn can converge.\nwe say xn converges to x almost surely if the event (xn (cid:16)\u2192 x ) has probability zero. xn\nconverges to x in probability if for each \u03b5, p(|xn \u2212 x | > \u03b5) \u2192 0 as n \u2192 \u221e. for p \u2265 1, xn\nconverges to x in lp if e|xn \u2212 x |p \u2192 0 as n \u2192 \u221e.\n\nthe following proposition shows some relationships among the types of convergence.\n\nproposition a.14 (1) if xn \u2192 x almost surely, then xn \u2192 x in probability.\n(2) if xn \u2192 x in lp, then xn \u2192 x in probability.\n(3) if xn \u2192 x in probability, there exists a subsequence n j such that xn j converges to x\nalmost surely.\nproof to prove (1), note xn \u2212 x tends to zero almost surely, so 1(\u2212\u03b5,\u03b5)c (xn \u2212 x ) also\nconverges to zero almost surely. now apply the dominated convergence theorem.\n\n(2) comes from chebyshev\u2019s inequality:\n\np(|xn \u2212 x | > \u03b5) = p(|xn \u2212 x |p > \u03b5 p) \u2264 e|xn \u2212 x |p/\u03b5 p \u2192 0\n\nas n \u2192 \u221e.\nto prove (3), choose n j larger than n j\u22121 such that p(|xn \u2212 x | > 2\nn \u2265 n j. thus if we let ai = (|xn j\nby the borel\u2013cantelli lemma p(ai i.o.) = 0. this implies xn j\ncomplement of (ai i.o.).\n\n\u2212 x | > 2\n\n\u2212i for some j \u2265 i), then p(ai) \u2264 2\n\n\u2212 j ) < 2\n\n\u2212 j whenever\n\u2212i+1.\n\u2192 x almost surely on the\n\nlet us give some examples to show there need not be any other implications among the\nthree types of convergence.\nlet \u0001 = [0, 1], f the borel \u03c3 -field, and p a lebesgue measure. let xn = n21(0,1/n). then\n= n2p/n \u2192 \u221e for\nclearly xn converges to zero almost surely and in probability, but e x p\nany p \u2265 1.\nn\n\nmakes with the x axis. let tn =(cid:12)\n\nlet \u0001 be the unit circle, and let p be a lebesgue measure on the circle normalized to have\ntotal mass 1. we use \u03b8 to denote the angle that the ray from 0 through a point on the circle\n\u22121, and let an = {ei\u03b8 : tn\u22121 \u2264 \u03b8 < tn}. let xn = 1an.\n\nn\ni=1 i\n\n "}, {"Page_number": 374, "text": "356\n\nbasic probability\n\nany point on the unit circle will be in infinitely many an, so xn does not converge almost\nsurely to zero. but p(an) = 1/(2\u03c0n) \u2192 0, so xn \u2192 0 in probability and in lp.\n\na.4 uniform integrability\na sequence {xi} of random variables is uniformly integrable if\n\n(cid:3)\n\n|xi| dp \u2192 0\n\nsup\n\ni\n\n(|xi|>m )\n\nas m \u2192 \u221e. this can be rephrased by saying: given \u03b5 > 0 there exists m > 0 such that\ne [|xi|;|xi| > m] < \u03b5 for all i. here m can be chosen independently of i.\nlemma a.15 if {xi} is a uniformly integrable sequence of random variables,\nsupi\nproof there exists m such that e [|xi|;|xi| > m] \u2264 1. then\n\ne|xi| < \u221e.\n\nthen\n\ne|xi| \u2264 e [|xi|;|xi| \u2264 m] + e [|xi|;|xi| > m] \u2264 m + 1,\n\nand we are done.\n\nwe say a sequence of random variables {xi} is uniformly absolutely continuous if given \u03b5\n\ne [|xi|; a] \u2264 \u03b5 whenever p(a) < \u03b4.\n\nthere exists \u03b4 such that supi\nproposition a.16 the following are equivalent.\n(1) the sequence {xi} is uniformly integrable.\n(2) the sequence {xi} is uniformly absolutely continuous and supi\nproof\nlet \u03b5 > 0 and choose m such that supi\np(a) < \u03b4, we have\n\nif (1) holds, we showed in lemma a.15 that the expectations are uniformly bounded.\ne [|xi| : |xi| > m] < \u03b5/2. then if \u03b4 = \u03b5/(2m ) and\n\ne|xi| < \u221e.\n\ne [|xi|; a] \u2264 e [|xi|;|xi| > m] + e [|xi|;|xi| \u2264 m, a] <\n\nnow suppose (2) holds. let \u03b5 > 0 and choose \u03b4 such that e [|xi|; a] < \u03b5 for all i if\np(a) \u2264 \u03b4. let m = supi\n\n\u03b5\n2\ne|xi|/\u03b4. then by the chebyshev inequality\n\n+ mp(a) \u2264 \u03b5.\n\np(|xi| > m ) \u2264 e|xi|\n\n= \u03b4,\n\nm\n\nso e [|xi|;|xi| > m] < \u03b5.\nproposition a.17 suppose {xi} and {yi} are each uniformly integrable sequences of random\nvariables. then {xi + yi} is also a uniformly integrable sequence.\nproof by proposition a.16,\n\ne|xi + yi| \u2264 sup\n\ne|xi| + sup\n\ne|yi| < \u221e.\n\ni\n\ni\n\nsup\n\ni\n\nusing proposition a.16 again, given \u03b5 there exists \u03b4 such that e [|xi|; a] < \u03b5/2 and\ne [|yi|; a] < \u03b5/2 if p(a) < \u03b4. but then e [|xi + yi|; a] < \u03b5 and a third use of propo-\nsition a.16 yields our result.\n\n "}, {"Page_number": 375, "text": "a.5 conditional expectation\n\n357\nproposition a.18 suppose there exists \u03d5 : [0,\u221e) \u2192 [0,\u221e) such that \u03d5 is increasing,\n\u03d5(x)/x \u2192 \u221e as x \u2192 \u221e, and supi\ne \u03d5(|xi|) < \u221e. then the sequence {xi} is uniformly\nintegrable.\nproof let \u03b5 > 0 and choose x0 such that x/\u03d5(x) < \u03b5 if x \u2265 x0. if m \u2265 x0,\n\u03d5(|xi|) \u2264 \u03b5 sup\n\n\u03d5(|xi|)1(|xi|>m ) \u2264 \u03b5\n\ne \u03d5(|xi|).\n\n(cid:3)\n\n(cid:3)\n\n(cid:3)\n\n|xi|\n\u03d5(|xi|)\nsince \u03b5 is arbitrary, we are done.\n\n|xi| =\n\n(|xi|>m )\n\ni\n\nthe main result we need in this section is the vitali convergence theorem.\n\ntheorem a.19 if xn \u2192 x almost surely and the sequence {xn} is uniformly integrable, then\ne|xn \u2212 x | \u2192 0.\nproof by proposition a.17 with yi = \u2212x for each i, the sequence xi \u2212 x is uniformly\nintegrable. let \u03b5 > 0 and choose m such that\n\n(cid:3)\n\n|xi \u2212 x | < \u03b5.\n\n(|xi\u2212x |>m )\n\nby dominated convergence,\n\ne|xi \u2212 x | \u2264 lim sup\ni\u2192\u221e\nsince \u03b5 is arbitrary, then e|xi \u2212 x | \u2192 0.\n\nlim sup\ni\u2192\u221e\n\ne [|xi \u2212 x |;|xi \u2212 x | \u2264 m] + \u03b5 = \u03b5.\n\na.5 conditional expectation\n\nif f \u2282 g are two \u03c3 -fields and x is an integrable g measurable random variable, the\nconditional expectation of x given f, written e [x | f] and read as \u201cthe expectation\n(or expected value) of x given f,\u201d is any f measurable random variable y such that\ne [y; a] = e [x ; a] for every a \u2208 f. the conditional probability of a \u2208 g given f is\ndefined by p(a | f ) = e [1a | f].\nif y1, y2 are two f measurable random variables with e [y1; a] = e [y2; a] for all a \u2208 f,\nthen y1 = y2, a.s., and so conditional expectation is unique up to almost sure equivalence.\nin the case x is already f measurable, e [x | f] = x . if x is independent from f,\ne [x | f] = e x . both of these facts follow immediately from the definition. for another\nexample, if {ai} is a finite collection of pairwise disjoint sets whose union is \u0001, p(ai) > 0\nfor all i, and f is the \u03c3 -field generated by the ai\u2019s, then\np(a \u2229 ai)\np(ai)\n\n(a.12)\nthis follows since the right-hand side is f measurable and its expectation over any set ai is\np(a \u2229 ai). equation (a.12) provides the link with the definition of conditional probability\nfrom elementary probability: if p(b) (cid:16)= 0, then\n\np(a | f ) =\n\n(cid:9)\n\n1ai\n\n.\n\ni\n\np(a | b) = p(a \u2229 b)\n\np(b)\n\n.\n\n(a.13)\n\n "}, {"Page_number": 376, "text": "358\n\nwe have\n\nbasic probability\n\ne [e [x | f] ] = e x\n\nbecause e [e [x | f]] = e [e [x | f]; \u0001] = e [x ; \u0001] = e x .\n\nthe following is easy to establish.\n\nproposition a.20 (1) if x \u2265 y are both integrable, then\n\ne [x | f] \u2265 e [y | f],\n\na.s.\n\n(2) if x and y are integrable and a \u2208 r, then\n\ne [ax + y | f] = ae [x | f] + e [y | f].\n\n(a.14)\n\nit is easy to check that limit theorems such as monotone convergence and dominated\nconvergence have conditional expectation versions, as do inequalities like jensen\u2019s and\nchebyshev\u2019s inequalities. thus, for example, we have jensen\u2019s inequality for conditional\nexpectations.\n\nproposition a.21 if g is convex and x and g(x ) are integrable,\n\ne [g(x ) | f] \u2265 g(e [x | f]),\n\na.s.\n\na key fact is the following.\n\nproof\n\nproposition a.22 if x and x y are integrable and y is measurable with respect to f, then\n(a.15)\n\ne [x y | f] = y e [x | f].\n(cid:26)\n(cid:26)\nif a \u2208 f, then for any b \u2208 f,\n\n(cid:27) = e [x ; a \u2229 b] = e [1ax ; b].\n\n1ae [x | f]; b\n\ne\n\ne [x | f]; a \u2229 b\n\n(cid:27) = e\n\nsince 1ae [x | f] is f measurable, this shows that (a.15) holds when y = 1a and a \u2208 f.\nusing linearity shows that (a.15) holds whenever y is a simple f measurable random\nvariable. taking limits, (a.15) holds whenever y \u2265 0 is f measurable and x and x y are\nintegrable. using linearity again completes the proof.\n\ntwo other equalities are contained in the following.\n\nproposition a.23 if e \u2282 f \u2282 g are \u03c3 -fields, then\n\ne [x | f] | e(cid:27) = e [x | e] = e\n\n(cid:26)\n\ne\n\nproof the right equality holds because e [x | e] is e measurable, hence f measurable.\nwe then use the fact that if y is f measurable, e [y | f] = y .\nto show the left equality, let a \u2208 e. then since a is also in f,\n\n(cid:26)\n\n(cid:26)\n\ne [x | f] | e(cid:27); a\n\n(cid:27) = e\n\n(cid:26)\n\ne\n\ne\n\ne [x | f]; a\n\nx | e]; a\n\n.\n\n(cid:27)\n\nsince both sides are e measurable, the equality follows.\n\nto show the existence of e [x | f], we proceed as follows.\n\nproposition a.24 if x is integrable, then e [x | f] exists.\n\n.\n\n(cid:26)\n\ne [x | e] | f(cid:27)\n(cid:27) = e [x ; a] = e [e\n\n(cid:26)\n\n "}, {"Page_number": 377, "text": "a.7 martingales\n\n359\nproof using linearity, we need only consider x \u2265 0. define a finite measure q on f by\nq(a) = e [x ; a] for a \u2208 f. this is trivially absolutely continuous with respect to p|f , the\nrestriction of p to f. let e [x | f] be the radon\u2013nikodym derivative of q with respect to\np|f . since q and p|f are measures on f, the radon\u2013nikodym derivative is f measurable,\nand so provides the desired random variable.\n\nwhen f = \u03c3 (y ), one usually writes e [x | y ] for e [x | f]. notation that is commonly\nused is e [x | y = y]. the definition is as follows. if a \u2208 \u03c3 (y ), then a = (y \u2208 b) for\nsome borel set b by the definition of \u03c3 (y ), or 1a = 1b(y ). by linearity and taking limits, it\nfollows that if z is \u03c3 (y ) measurable, then z = f (y ) for some borel measurable function f .\nset z = e [x | y ] and choose f borel measurable so that z = f (y ). then e [x | y = y] is\ndefined to be f (y).\nif x \u2208 l2 and m = {y \u2208 l2 : y is f measurable}, one can show that e [x | f] is equal\nto the projection of x onto the subspace m.\n\na.6 stopping times\n\nwe next want to talk about stopping times. suppose we have a sequence of \u03c3 -fields fi such\nthat fi \u2282 fi+1 for each i. an example would be if fi = \u03c3 (x1, . . . , xi). a random mapping\nn from \u0001 to {0, 1, 2, . . .} is called a stopping time if for each n, (n \u2264 n) \u2208 fn.\nthe proof of the following is immediate from the definitions.\n\nproposition a.25 (1) fixed times n are stopping times.\n(2) if n1 and n2 are stopping times, then so are n1 \u2227 n2 and n1 \u2228 n2.\n(3) if nn is an increasing sequence of stopping times, then so is n = supn nn.\n(4) if nn is a decreasing sequence of stopping times, then so is n = inf n nn.\n(5) if n is a stopping time, then so is n + n.\nwe define\n\nfn = {a : a \u2229 (n \u2264 n) \u2208 fn for all n}.\n\n(a.16)\n\na.7 martingales\n\nin this section we consider martingales. let fn be an increasing sequence of \u03c3 -fields. a\nsequence of random variables mn is adapted to fn if for each n, mn is fn measurable.\nmn is a martingale if mn is adapted to fn, mn is integrable for all n, and\n\ne [mn | fn\u22121] = mn\u22121,\n\nn = 2, 3, . . .\n\n(a.17)\nif we have e [mn | fn\u22121] \u2265 mn\u22121, a.s., for every n, then mn is a submartingale. if we have\ne [mn | fn\u22121] \u2264 mn\u22121, we have a supermartingale.\nlet us look at some examples. if xi is a sequence of mean zero independent random\n\na.s.,\n\nvariables and sn =(cid:12)\n\nn\n\ni=1 xi, then mn = sn is a martingale, since\ne [mn | fn\u22121] = mn\u22121 + e [mn \u2212 mn\u22121 | fn\u22121]\n= mn\u22121 + e [mn \u2212 mn\u22121] = mn\u22121,\n\nusing independence.\n\n "}, {"Page_number": 378, "text": "360\n\nbasic probability\n\nanother example is the following. if the xi\u2019s are independent and have mean zero and\n\nvariance one, sn is as in the previous example, and mn = s2\n\n\u2212 n, then\n| fn\u22121] = e [(sn \u2212 sn\u22121)2 | fn\u22121] + 2sn\u22121e [sn | fn\u22121] \u2212 s2\nn\u22121\n\ne [s2\nn\n\nn\n\n= 1 + s2\nn\u22121\n\n,\n\nusing independence. it follows that mn is a martingale.\na third example is the following: if x \u2208 l1 and mn = e [x | fn], then mn is a martingale.\nthe proof of this is simple:\n\ne [mn+1 | fn] = e [e [x | fn+1] | fn] = e [x | fn] = mn.\n\nif mn is a martingale, g is convex, and g(mn) is integrable for each n, then by jensen\u2019s\n\ninequality for conditional expectations,\n\ne [g(mn+1) | fn] \u2265 g(e [mn+1 | fn]) = g(mn),\n\n(a.18)\nor g(mn) is a submartingale. similarly if g is convex and increasing on [0,\u221e) and mn is a\npositive submartingale, then g(mn) is a submartingale because\n\ne [g(mn+1) | fn] \u2265 g(e [mn+1 | fn]) \u2265 g(mn).\n\na.8 optional stopping\n\nnote that if one takes expectations in (a.17), one has e mn = e mn\u22121, and by induction\ne mn = e m0. the theorem about martingales that lies at the basis of all other results is\ndoob\u2019s optional stopping theorem, which says that the same is true if we replace n by a\nstopping time n. there are various versions, depending on what conditions one puts on the\nstopping times.\ntheorem a.26 if n is a stopping time with respect to fn that is bounded by a positive real\nk and mn a martingale, then e mn = e m0.\nproof we write\n\ne mn = k(cid:9)\n\ne [mn; n = k] = k(cid:9)\n\nk=0\n\ne [mk; n = k].\n\nk=0\n\nnote (n = k) is f j measurable if j \u2265 k, so\n\ne [mk; n = k] = e [mk+1; n = k] = e [mk+2; n = k]\n\n= \u00b7\u00b7\u00b7 = e [mk; n = k].\n\nhence\n\ne mn = k(cid:9)\n\nk=0\n\ne [mk; n = k] = e mk = e m0.\n\nthis completes the proof.\n\nthe same proof as that in theorem a.26 gives the following corollary.\n\n "}, {"Page_number": 379, "text": "a.9 doob\u2019s inequalities\n\n361\n\ncorollary a.27 if n is a stopping time bounded by k and mn is a submartingale, then\ne mn \u2264 e mk.\n\nthe same proof also gives\ncorollary a.28 if n is a stopping time bounded by k, a \u2208 fn , and mn is a submartingale,\nthen e [mn; a] \u2264 e [mk; a].\nproposition a.29 if n1 \u2264 n2 are stopping times bounded by k and m is a martingale, then\ne [mn2\nproof suppose a \u2208 fn1. we need to show e [mn1\ntime n3 by\n\n; a]. define a new stopping\n\n| fn1] = mn1, a.s.\n\n; a] = e [mn2\n\n(cid:13)\n\nn3(\u03c9) =\n\nn1(\u03c9), \u03c9 \u2208 a\nn2(\u03c9), \u03c9 /\u2208 a.\n\nit is easy to check that n3 is a stopping time, so e mn3\n\n= e mk = e mn2 implies\n\ne [mn1\n\n; a] + e [mn2\n\n; ac] = e [mn2].\n\n; ac] from each side completes the proof.\n\nsubtracting e [mn2\n\nthe following is known as the doob decomposition for discrete time martingales.\n\nak \u2265 0. let ak =(cid:12)\n\nproposition a.30 suppose xk is a submartingale with respect to an increasing sequence of\n\u03c3 -fields fk. then we can write xk = mk + ak such that mk is a martingale adapted to the fk\nand ak is a sequence of random variables with ak being fk\u22121 measurable and a0 \u2264 a1 \u2264 \u00b7\u00b7\u00b7 .\nproof let ak = e [xk | fk\u22121] \u2212 xk\u22121 for k = 1, 2, . . . since xk is a submartingale, each\nk\ni=1 ai. the fact that the ak are increasing and measurable with respect to\nfk\u22121 is clear. set mk = xk \u2212 ak. then\ne [mk+1 \u2212 mk | fk] = e [xk+1 \u2212 xk | fk] \u2212 ak+1 = 0,\n\nor mk is a martingale.\n\ncombining propositions a.29 and a.30 we have\n\ncorollary a.31 suppose xk is a submartingale, and n1 \u2264 n2 are bounded stopping times.\nthen\n\ne [xn2\n\n| fn1] \u2265 xn1\n\n.\n\na.9 doob\u2019s inequalities\n\nthe first interesting consequences of the optional stopping theorems are doob\u2019s inequalities.\n\u2217\nif mn is a martingale, set m\nn\n\n= maxi\u2264n |mi|.\n\ntheorem a.32 if mn is a martingale or a positive submartingale,\n\n\u2217\np(m\nn\n\n\u2265 a) \u2264 1\na\n\ne [|mn|; m\n\n\u2217\nn\n\n\u2265 a] \u2264 1\na\n\ne|mn|.\n\n "}, {"Page_number": 380, "text": "362\nproof fix n. set mn+1 = mn. let n = min{ j : |m j| \u2265 a} \u2227 (n + 1). since the function\n\u2265 a), then a \u2208 fn and we have\nf (x) = |x| is convex, |mn| is a submartingale. if a = (m\n\nbasic probability\n\n\u2217\nn\n\n\u2265 a) \u2264 e [|mn|; a] \u2264 e [|mn|; a] \u2264 e|mn|,\nthe first inequality by the definition of n, the second by corollary a.28.\n\n\u2217\nap(m\nn\n\nfor p > 1, we have the following inequality.\n\ntheorem a.33 if p > 1, m is a martingale or positive submartingale, and e|mi|p < \u221e\nfor i \u2264 n, then\n\n(cid:10)\np\np \u2212 1\n\u2208 lp. we write, using theorem a.32,\n\n(cid:11)p\ne|mn|p.\n(cid:3) \u221e\n\npap\u22121e [|mn|1(m\u2217\n\nn\n\n\u2265a)/a] da\n\n\u2264(cid:12)\n(cid:3) \u221e\n(cid:3)\n\nn\ni=1\n\n0\n\n)p =\n\n\u2217\ne (m\nn\n\n)p \u2264\n|mn|, hence m\n\u2217\nn\n\u2217\npap\u22121p(m\nn\n\n\u2217\nm\nn\n\n> a) da \u2264\npap\u22122|mn| da = p\np \u2212 1\n\n\u2217\ne [(m\nn\n)p)(p\u22121)/p(e|mn|p)1/p.\n\n0\n\n\u2217\n(e (m\nn\n\n= e\n\u2264 p\np \u2212 1\n\n0\n\n)p\u22121|mn|]\n\n\u2217\nproof note m\nn\n\u2217\ne (m\nn\n\nthe last inequality follows by h\u00a8older\u2019s inequality. now divide both sides by the quantity\n\u2217\n(e (m\nn\n\n)p)(p\u22121)/p.\n\na.10 martingale convergence theorem\n\nthe martingale convergence theorem is another important consequence of optional stopping.\nthe main step is the upcrossing lemma. the number of upcrossings of an interval [a, b] is\nthe number of times a process m crosses from below a to above b.\n\nto be more exact, let\n\ns1 = min{k : mk \u2264 a},\n\nt1 = min{k > s1 : mk \u2265 b},\n\nand\n\nsi+1 = min{k > ti : mk \u2264 a},\n\nti+1 = min{k > si+1 : mk \u2265 b}.\n\nthe number of upcrossings un before time n is un = max{ j : tj \u2264 n}.\ntheorem a.34 (upcrossing lemma) if mk is a submartingale,\n\ne un \u2264 1\nb \u2212 a\n\ne [(mn \u2212 a)+\n\n].\n\nproof the number of upcrossings of [a, b] by mk is the same as the number of upcrossings\nof [0, b \u2212 a] by yk = (mk \u2212 a)+\n+ = x \u2228 0. moreover yk is still a submartingale.\nif we obtain the inequality for the number of upcrossings of the interval [0, b \u2212 a] by the\nprocess yk, we will have the desired inequality for upcrossings of m.\nthus we may assume a = 0. fix n and define yn+1 = yn. this will still be a submartingale.\n= ti \u2227 (n + 1). since ti+1 > si+1 > ti,\n\n= si \u2227 (n + 1), t\n\n, where x\n\n(cid:3)\n\ni\n\n(cid:3)\ndefine si, ti as above, and let s\ni\nthen t\n\n= n + 1.\n\n(cid:3)\nn+1\n\n "}, {"Page_number": 381, "text": "a.10 martingale convergence theorem\n\n363\n\nwe write\n\ne yn+1 = e ys\n\n(cid:3)\n1\n\n+ n+1(cid:9)\n\ni=0\n\ni ] + n+1(cid:9)\n\ni=0\n\n\u2212 ys(cid:3)\n\ne [yt (cid:3)\n\ni\n\ne [ys\n\n(cid:3)\ni+1\n\n\u2212 yt (cid:3)\ni ].\n\nall the summands in the third term on the right are non-negative since yk is a submartingale.\nthe first term on the right will be non-negative since y is non-negative. for the jth upcrossing,\nyt\n\n(cid:3)\nj is always greater than or equal to 0. thus\n\n\u2265 b \u2212 a, while yt\n\n\u2212 ys\n\n(cid:3)\nj\n\n(cid:3)\nj\n\n(cid:3)\nj\n\n\u2212 ys\nn+1(cid:9)\n\n\u2212 ys(cid:3)\n\ni\n\n) \u2265 (b \u2212 a)un.\n\n(yt (cid:3)\n\ni\n\nhence\n\ni=0\n\ne un \u2264 1\nb \u2212 a\n\ne yn+1.\n\n(a.19)\n\nthis leads to the martingale convergence theorem.\n\ntheorem a.35 if mn is a submartingale such that supn\nalmost surely as n \u2192 \u221e.\nproof for each a < b, let un(a, b) be the number of upcrossings of [a, b] by m up to time\nn, and let u (a, b) = limn\u2192\u221e un. for each pair a < b of rational numbers, by monotone\nconvergence,\n\n+\ne m\nn\n\n< \u221e, then mn converges\n\ne u (a, b) \u2264 1\nb \u2212 a\n\nsup\nn\n\ne (mn \u2212 a)+ < \u221e.\n\nthus u (a, b) < \u221e, a.s. if na,b is the set of \u03c9\u2019s where u (a, b) = \u221e and n = \u222aa<b,a,b\u2208q+na,b,\nthen p(n ) = 0. if \u03c9 /\u2208 n, we cannot have lim supn\u2192\u221e mn(\u03c9) > lim inf n\u2192\u221e mn(\u03c9). there-\nfore mn converges almost surely, although we still have to rule out the possibility of the limit\nbeing infinite. since mn is a submartingale, e mn \u2265 e m0, and thus\n\u2212 e mn \u2264 2e m\n+\nn\n\ne|mn| = e m\n\n\u2212 e m0.\n\n= 2e m\n\n+ e m\n\n+\nn\n\n+\nn\n\n\u2212\nn\n\nby fatou\u2019s lemma,\n\ne lim\nn\n\n|mn| \u2264 sup\n\nn\n\ne|mn| \u2264 2 sup\n\nn\n\n+\ne m\nn\n\n\u2212 e m0 < \u221e,\n\nor mn converges almost surely to a finite limit.\n\ncorollary a.36 if xn is a positive supermartingale or a martingale bounded above or below,\nxn converges almost surely.\n\nif xn is a positive supermartingale, \u2212xn is a submartingale bounded above by 0. now\nproof\napply theorem a.35.\nif xn is a martingale bounded above, by considering \u2212xn, we may assume xn is bounded\nbelow. looking at xn + m for fixed m will not affect the convergence, so we may assume xn\nis bounded below by 0. now apply the first assertion of the corollary.\n\n "}, {"Page_number": 382, "text": "364\n\nbasic probability\n\nmn is a uniformly integrable martingale if the collection of random variables {mn} is\n\nuniformly integrable.\n\ne|mn|p < \u221e for some p > 1, then the\nproposition a.37 (1) if mn is a martingale with supn\nconvergence is in lp as well as almost surely. this is also true when mn is a submartingale.\n(2) if mn is a uniformly integrable martingale, then the convergence is in l1.\n(3) if mn \u2192 m\u221e in l1, then mn = e [m\u221e | fn].\n+\ne m\nproof\nm\u221e be the limit. then |mn \u2212 m\u221e| \u2192 0, a.s., and\nn\n|mn \u2212 m\u221e|p \u2264 ce sup\n\u2264 ce sup\n\u2264 c sup\n\n|mn|p + ce|m\u221e|p\n|mn|p\ne|mn|p < \u221e.\n\n< \u221e and mn converges almost surely. let\n\ne|mn|p < \u221e, then supn\n\n(1) if supn\n\ne sup\nn\n\nn\n\nn\n\nn\n\nthe second inequality is by fatou\u2019s lemma and the last by doob\u2019s inequalities,\ntheorem a.33. the lp convergence assertion now follows by dominated convergence.\n\n(2) the l1 convergence assertion follows since almost sure convergence together\nwith uniform integrability implies l1 convergence by the vitali convergence theorem,\ntheorem a.19.\n(3) finally, if j < n, we have m j = e [mn | f j]. if a \u2208 f j,\n\ne [m j; a] = e [mn; a] \u2192 e [m\u221e; a]\n\nby the l1 convergence of mn to m\u221e. since this is true for all a \u2208 f j, m j = e [m\u221e | f j].\n\na.11 strong law of large numbers\n\nsuppose we have a sequence x1, x2, . . . of independent and identically distributed random\nvariables. this means that the xi are independent and each has the same law as x1. this\nsituation is very common, and we abbreviate this by saying the xi are i.i.d.\n\ndefine\n\nsn = n(cid:9)\n\ni=1\n\nxi.\n\nthe sn are called partial sums. in this section we suppose e|x1| < \u221e. the strong law of\nlarge number is the precise version of the law of averages.\ntheorem a.38 if xi is an i.i.d. sequence and e|x1| < \u221e, then\n\n\u2192 e x1,\n\nsn\nn\n\na.s.\n\nthe proof we give is a mixture of the standard one and some martingale techniques. the\nstandard proof (see, e.g., chung (2001)) uses no martingale methods, while there is a proof\n(see durrett (1996)) that is entirely martingale based.\n\n "}, {"Page_number": 383, "text": "365\nproof we may assume e xi = 0, for otherwise we replace xi by xi \u2212 e xi. let yn =\nxn1(|xn|\u2264n), zn = yn \u2212 e yn, and\n\na.11 strong law of large numbers\n\nmn = n(cid:9)\n\ni=1\n\nzi\ni\n\n.\n\nlet fn = \u03c3 (x1, . . . , xn). note that the zi are independent but not identically distributed.\nusing the independence, mn is a martingale:\n\nwe will need the estimate\n\ne [mn+1 | fn] = mn + 1\ne [zn+1 | fn] = mn + 1\nn + 1\nn + 1\n(cid:3)\n\u221e(cid:9)\n(cid:3) \u221e\n\np(|x1| \u2265 i) =\n\np(|xi| \u2265 i) dx\n\n\u221e(cid:9)\n\ni=1\n\ni=1\n\ni\n\ni\u22121\np(|x1| \u2265 x) dx = e|x1| < \u221e,\n\n\u2264\n\n0\n\ne [zn+1] = mn.\n\n(a.20)\n\nusing proposition a.4.\n\nwe show that e|mn| is bounded by a constant not depending on n. in fact, again using\n\nproposition a.4,\n\ne m 2\nn\n\n= n(cid:9)\n(cid:3)\n\ni\n\n1\ni2 var yi\n\ni=1\n2yp(|xi| \u2265 y) dy\n\nvar zi\n\ni2\n\n\u221e(cid:9)\n\n1\ni2\n\n0\n\n1\n\ni=1\n1(y\u2264i)yp(|x1| \u2265 y) dy\ni2 1(y\u2264i)yp(|x1| \u2265 y) dy\ni=1\n\u00b7 yp(|x1| \u2265 y) dy\n1\ny\np(|x1| \u2265 y) dy = ce|x1| < \u221e.\n\n1\ni2\n\ne y 2\ni\n\ni=1\n\u2264\n\n= var mn = n(cid:9)\n\u2264 n(cid:9)\n(cid:3) \u221e\n\u221e(cid:9)\n(cid:3) \u221e\n\u221e(cid:9)\n(cid:3) \u221e\n(cid:3) \u221e\n\ni=1\n= 2\n\n= 2\n\n1\ni2\n\ni=1\n\n0\n\n0\n\n0\n\n\u2264 c\n= c\n\n0\n\n(cid:12)\n\nthe uniform bound on e|mn| follows by jensen\u2019s inequality.\n\nby the martingale convergence theorem, mn converges almost surely; let m\u221e be the limit.\nn\ni=1 mi also converges to m\u221e, a.s. we now use\n\nsome elementary calculus shows that 1\nsummation by parts as follows. since i(mi \u2212 mi\u22121) = zi and m0 = 0, then\nn\n(i + 1)mi\n\nimi \u2212 n\u22121(cid:9)\n\nn(cid:9)\n\nn(cid:9)\n\n(cid:11)\n\n1\nn\n\ni=1\n\nzi = 1\nn\n= mn \u2212 n \u2212 1\n\n(imi \u2212 imi\u22121) = 1\nn\u22121(cid:9)\nn\n\n(cid:10)\n\ni=1\n\n1\nn \u2212 1\n\nn\n\nmi\n\ni=1\n\ni=1\n\ni=1\n\u2192 m\u221e \u2212 m\u221e = 0.\n\n(cid:10) n(cid:9)\n(cid:11)\n\n "}, {"Page_number": 384, "text": "366\n\nbasic probability\n\nby dominated convergence and the fact that the xi are identically distributed,\n\ne yn = e [xn1(|xn|\u2264i)] = e [x11(|x1|\u2264n)] \u2192 e x1 = 0\n\n(cid:12)\n\nas n \u2192 \u221e, and this implies 1\n\nn\ni=1\n\nn\n\n1\nn\n\nyi \u2192 0,\n\nn(cid:9)\ne yi \u2192 0. since yi = zi + e yi, we conclude\n\u221e(cid:9)\n\n\u221e(cid:9)\n\na.s.\n\ni=1\n\nfinally,\n\n\u221e(cid:9)\n\np(xi (cid:16)= yi) =\n\np(|xi| \u2265 i) =\n\np(|x1| \u2265 i) < \u221e,\n\ni=1\n\nso by the borel\u2013cantelli lemma, except for a set of probability zero, xi = yi for all i greater\nthan some positive integer i (i depends on \u03c9). hence\n\ni=1\n\nn(cid:9)\n\n(cid:20)(cid:20)(cid:20) 1\n\nn\n\nxi \u2212 1\nn\n\ni=1\n\nn(cid:9)\n\nyi\n\ni=1\n\n(cid:20)(cid:20)(cid:20) \u2264 1\n\ni(cid:9)\n\nn\n\ni=1\n\n|xi \u2212 yi| \u2192 0,\n\na.s.\n\ni=1\nthis completes the proof.\n\nthe following extension of the strong law will be needed when comparing a random walk\n\nand a brownian motion.\nproposition a.39 suppose xi is an i.i.d. sequence and e|x1| < \u221e. then\n\nmaxk\u2264n |sk \u2212 e sk|\n\nn\n\n\u2192 0,\n\na.s.\n\nproof by looking at xi\u2212 e xi, we may assume e xi = 0. let j(n) be (one of) the value(s) of\nj such that |s j| = maxk\u2264n |sk|. suppose sn(\u03c9)/n \u2192 0. it suffices to show |s j(n)(\u03c9)|/n \u2192 0,\na.s.\nif not, for this \u03c9, either\n(1) there is a subsequence nk \u2192 \u221e and \u03b5 > 0 such that j(nk ) \u2192 \u221e and |s j(nk )|/nk \u2265 \u03b5\nfor all k; or\n(2) there exists a subsequence nk \u2192 \u221e, \u03b5 > 0, and n > 1 such that j(nk ) \u2264 n and\n|s j(nk )|/nk \u2265 \u03b5 for all k.\nin case (1), since j(nk ) \u2192 \u221e,\n|s j(nk )|\nnk\n\n= |s j(nk )|\n\n\u2264 |s j(nk )|\n\nj(nk )\nnk\n\n\u2192 0,\n\nj(nk )\n\nj(nk )\n\na contradiction. in case (2),\n\nalso a contradiction.\n\n|s j(nk )|\nnk\n\n\u2264 maxm\u2264n |sm|\n\nnk\n\n\u2192 0,\n\nanother application of the strong law of large numbers is the glivenko\u2013cantelli\ntheorem. let xi be i.i.d. random variables which have a uniform distribution on [0, 1],\n\n "}, {"Page_number": 385, "text": "a.12 weak convergence\n\n367\n\nn(cid:9)\nthat is, p(x1 \u2264 t ) = t if 0 \u2264 t \u2264 1. let\n\nfn(t ) = 1\nn\n\ni=1\n\n1[0,t](xi),\n\n0 \u2264 t \u2264 1.\n\nby the strong law, fn(t ) \u2192 t, a.s., for each t. the glivenko\u2013cantelli theorem says that the\nconvergence is uniform over t.\n\ntheorem a.40 with fn as above,\n\n|fn(t ) \u2212 t| \u2192 0,\n\na.s.\n\nsup\n0\u2264t\u22641\n\nproof for each t \u2208 [0, 1], 1[0,t](xi) is a sequence of i.i.d. random variables with expectation\np(xi \u2264 t ) = t. by the strong law of large numbers, for each t, fn(t ) \u2192 t, a.s. let nt be the\nset of \u03c9 such that fn(t )(\u03c9) does not converge to t, and let n = \u222aq+nt. then p(n ) = 0.\nlet \u03b5 > 0 and take \u03c9 /\u2208 n. take m > 2/\u03b5 and choose n0 large enough (depending on \u03c9)\n\nsuch that\n\n|fn(k/m)(\u03c9) \u2212 (k/m)| < \u03b5/2,\n\nk = 0, 1, 2, . . . , m,\n\nif n \u2265 n0. then if n \u2265 n0 and k/m \u2264 t < (k + 1)/m,\n\nfn(t ) \u2212 t \u2264 fn((k + 1)/m) \u2212 k/m \u2264 fn((k + 1)/m) \u2212 (k + 1)/m + \u03b5/2 < \u03b5,\n\nand similarly fn(t ) \u2212 t > \u2212\u03b5. hence for n \u2265 n0,\n\n|fn(t ) \u2212 t| \u2264 \u03b5.\n\nsup\nt\u2208[0,1]\n\nsince \u03b5 is arbitrary, this proves the uniform convergence.\n\n\u221a\nwe will see soon that if the xi are i.i.d. with mean zero and variance one, then sn/\nconverges in the sense that\n\na.12 weak convergence\n\nn\n\n\u221a\np(sn/\n\nn \u2208 [a, b]) \u2192 p(z \u2208 [a, b]),\n\nwhere z is a standard normal. we want to generalize the above type of convergence.\n\nwe say fn converges weakly to f if fn(x) \u2192 f (x) for all x at which f is continuous. here\nfn and f are distribution functions. we say xn converges weakly to x if fxn converges weakly\nto fx . we also say xn converges in distribution or converges in law to x . probabilities \u03bcn\n(x) =\nconverge weakly if their corresponding distribution functions converge, that is, if f\u03bcn\n\u03bcn(\u2212\u221e, x] converges weakly.\nan example that illustrates why we restrict the convergence to continuity points of f is\nthe following. let xn = 1/n with probability one, and x = 0 with probability one. fxn\n(x) is\n(x) converges to fx (x) for all x except x = 0.\n0 if x < 1/n and 1 otherwise. note fxn\nproposition a.41 xn converges weakly to x if and only if e g(xn) \u2192 e g(x ) for all g\nbounded and continuous.\n\n "}, {"Page_number": 386, "text": "basic probability\n\n368\nproof suppose e g(xn) \u2192 e g(x ) whenever g is bounded and continuous. let \u03b5 > 0\nand suppose x is a continuity point of fx . choose \u03b4 such that fx (x) \u2212 \u03b5 < fx (x \u2212 \u03b4) \u2264\nfx (x + \u03b4) < fx (x) + \u03b5. let g be a continuous function taking values in [0, 1] such that g\nequals 1 on (\u2212\u221e, x] and equals 0 on [x + \u03b4,\u221e). then\n\nlim sup\nn\u2192\u221e fxn\n\n(x) \u2264 lim sup\nn\u2192\u221e\n\ne g(xn)\n\n= e g(x ) \u2264 fx (x + \u03b4) < fx (x) + \u03b5.\nlim inf n\u2192\u221e fxn\n\n(x) = fx (x).\n\n> fx (x) \u2212 \u03b5. since \u03b5 is arbitrary,\na similar argument shows that\nlimn\u2192\u221e fxn\nnow suppose xn \u2192 x weakly. let \u03b5 > 0 and choose m > 0 such that m and \u2212m are\ncontinuity points for fx and also continuity points for each of the fxn, fx (\u2212m ) < \u03b5, and\nfx (m ) > 1 \u2212 \u03b5. suppose g is bounded and continuous on r and without loss of generality\nsuppose g is bounded by 1. then\n\n|e [g(xn); xn /\u2208 [\u2212m, m )]|\n\n(a.21)\n\np(|xn| \u2265 m )\n\n\u2264 lim sup\nn\u2192\u221e\n= lim sup\nn\u2192\u221e fxn\n\u2264 2\u03b5.\n\n(\u2212m ) + lim sup\nn\u2192\u221e\n\n(1 \u2212 fxn\n\n(m ))\n\nlim sup\nn\u2192\u221e\n\nsimilarly,\n\n|e [g(x ); x /\u2208 [\u2212m, m )]| \u2264 2\u03b5.\n\ntake f to be a step function of the form\n\n(a.22)\ni=1 ci1(ai,bi] such that | f (x) \u2212 g(x)| < \u03b5 for\nx \u2208 [\u2212m, m ) and each ai and bi is a continuity point for fx and also continuity points for\neach of the fxn. then\n\nm\n\n(cid:12)\n\ne f (xn) = m(cid:9)\n\u2192 m(cid:9)\n\ni=1\n\ni=1\n\n(bi) \u2212 fxn\n\n(ai))\n\n(a.23)\n\nci(fxn\n\nci(fx (bi) \u2212 fx (ai)) = e f (x ).\n\nfinally, since f differs from g by at most \u03b5 on [\u2212m, m), then\n\n|e f (xn) \u2212 e [g(xn); xn \u2208 [\u2212m, m )]| \u2264 \u03b5\n\n(a.24)\n\nand similarly when xn is replaced by x . combining (a.21), (a.22), (a.23), and (a.24) and\nusing the fact that \u03b5 is arbitrary shows that e g(xn) \u2192 e g(x ).\n\nlet us examine the relationship between weak convergence and convergence in probability.\nif xi is an i.i.d. sequence, then xi converges weakly, in fact, to x1, since all the xi\u2019s have the\nsame distribution. but from the independence it is not hard to see that the sequence xi does\nnot converge in probability unless the xi\u2019s are identically constant. therefore one can have\nweak convergence without convergence in probability.\n\n "}, {"Page_number": 387, "text": "a.12 weak convergence\n\n369\n\nproposition a.42 (1) if xn converges to x in probability, then it converges weakly.\n\n(2) if xn converges weakly to a constant, it converges in probability.\n(3) (slutsky\u2019s theorem) if xn converges weakly to x and yn converges weakly to a constant\n\nb, then xn + yn converges weakly to x + b and xnyn converges weakly to bx .\nproof to prove (1), let g be a bounded and continuous function. if n j is any subsequence,\nthen there exists a further subsequence such that x (n jk\n) converges almost surely to x . then\n)) \u2192 e g(x ). that suffices to show e g(xn) converges\nby dominated convergence, e g(x (n jk\nto e g(x ).\n\nfor (2), if xn converges weakly to b,\np(xn \u2212 b > \u03b5) = p(xn > b + \u03b5) = 1 \u2212 p(xn \u2264 b + \u03b5) \u2192 1 \u2212 p(b \u2264 b + \u03b5) = 0.\n\nwe use the fact that if y is identically equal to b, then b + \u03b5 is a point of continuity for fy .\na similar equation shows p(xn \u2212 b \u2264 \u2212\u03b5) \u2192 0, so p(|xn \u2212 b| > \u03b5) \u2192 0.\nwe now prove the first part of (3), leaving the second part for the reader. let x be a point\nsuch that x \u2212 b is a continuity point of fx . choose \u03b5 so that x \u2212 b + \u03b5 is again a continuity\npoint. then\n\np(xn + yn \u2264 x) \u2264 p(xn + b \u2264 x + \u03b5) + p(|yn \u2212 b| > \u03b5) \u2192 p(x \u2264 x \u2212 b + \u03b5).\n\nhence lim sup p(xn + yn \u2264 x) \u2264 p(x + b \u2264 x + \u03b5). since \u03b5 can be arbitrarily small and\nx \u2212 b is a continuity point of fx , then lim sup p(xn + yn \u2264 x) \u2264 p(x + b \u2264 x). the lim inf\nis done similarly.\n\nwe say a sequence of distribution functions{fn} is tight if for each \u03b5 > 0 there exists m such\nthat fn(m ) \u2265 1 \u2212 \u03b5 and fn(\u2212m ) \u2264 \u03b5 for all n. a sequence of random variables {xn} is tight\nif the corresponding distribution functions are tight; this is equivalent to p(|xn| \u2265 m ) \u2264 \u03b5.\ntheorem a.43 (helly\u2019s theorem) let fn be a sequence of distribution functions that is\ntight. there exists a subsequence n j and a distribution function f such that fn j converges\nweakly to f.\n\nwhat could conceivably happen is that xn is identically equal to n, so that fxn\n\n\u2192 0, but the\nfunction f that is identically equal to 0 is not a distribution function; the tightness precludes\nthis.\nproof let qk be an enumeration of the rationals. since fn(qk ) \u2208 [0, 1], any subsequence\nhas a further subsequence that converges. use a diagonalization argument (as in the proof\n(qk ) converges for each qk and\nof the ascoli\u2013arzel`a theorem; see rudin (1976)) so that fn j\ncall the limit f (qk ). f is increasing, and define f (x) = inf qk\u2265x f (qk ). hence f is right\ncontinuous and increasing.\nr < x < s and f (s) \u2212 \u03b5 < f (x) < f (r) + \u03b5. then\n\nif x is a point of continuity of f and \u03b5 > 0, then there exist r and s rational such that\n\n(x) \u2265 fn j\n\nfn j\n\n(r) \u2192 f (r) > f (x) \u2212 \u03b5\n\nand\n\nsince \u03b5 is arbitrary, fn j\n\n(x) \u2264 fn j\n\nfn j\n\n(x) \u2192 f (x).\n\n(s) \u2192 f (s) < f (x) + \u03b5.\n\n "}, {"Page_number": 388, "text": "370\n\nbasic probability\n\nsince the fn are tight, there exists m such that fn(\u2212m ) < \u03b5. then f (\u2212m ) \u2264 \u03b5, which\nimplies limx\u2192\u2212\u221e f (x) = 0. showing limx\u2192\u221e f (x) = 1 is similar. therefore f is in fact a\ndistribution function.\n\nwe conclude by giving an easily checked criterion for tightness.\n\nproposition a.44 suppose there exists \u03d5 : [0,\u221e) \u2192 [0,\u221e) that is increasing and \u03d5(x) \u2192\n\u221e as x \u2192 \u221e. if a = supn\nproof let \u03b5 > 0. choose m such that \u03d5(x) \u2265 a/\u03b5 if x > m. then\n\ne \u03d5(|xn|) < \u221e, then the sequence {xn} is tight.\n\n(cid:3)\n\np(|xn| > m ) \u2264\n\n\u03d5(|xn|)\na/\u03b5\n\n1(|xn|>m )dp \u2264 \u03b5\na\n\ne \u03d5(|xn|) \u2264 \u03b5.\n\nthe conclusion follows.\n\nin particular, if supn\n\ne|xn|2 < \u221e, the sequence {xn} is tight.\n\na.13 characteristic functions\n\nnote that \u03d5x (t ) =(cid:15)\n\u03d5x (t ) =(cid:15)\n\nwe define the characteristic function of a random variable x by \u03d5x (t ) = e eitx for t \u2208 r.\neitxpx (dx). thus if x and y have the same law, they have the same\ncharacteristic function. also, if the law of x has a density, that is, px (dx) = fx (x) dx, then\neitx fx (x) dx, so in this case the characteristic function is the same as the definition\n\nof the fourier transform of fx .\nproposition a.45 \u03d5(0) = 1, |\u03d5(t )| \u2264 1, \u03d5(\u2212t ) = \u03d5(t ), and \u03d5 is uniformly continuous.\nproof since |eitx| \u2264 1, everything follows immediately from the definitions except the\nuniform continuity. for that we write\n\n|\u03d5(t + h) \u2212 \u03d5(t )| = |e ei(t+h)x \u2212 e eitx| \u2264 e|eitx (eihx \u2212 1)| = e|eihx \u2212 1|.\n\nsince |eihx \u2212 1| tends to zero almost surely as h \u2192 0, the right-hand side tends to zero by\ndominated convergence. note that the right-hand side is independent of t.\nproposition a.46 \u03d5ax (t ) = \u03d5x (at ) and \u03d5x +b(t ) = eitb\u03d5x (t ).\nproof the first follows from e eit (ax ) = e ei(at )x , and the second is similar.\nproposition a.47 if x and y are independent, then\n\n\u03d5x +y (t ) = \u03d5x (t )\u03d5y (t ).\n\nproof from the multiplication theorem,\n\ne eit (x +y ) = e eitx eity = e eitx e eity ,\n\nand we are done.\n\nlet us look at some examples of characteristic functions.\n(1) bernoulli: by direct computation,\n\n\u03d5x (t ) = peit + (1 \u2212 p) = 1 \u2212 p(1 \u2212 eit ).\n\n "}, {"Page_number": 389, "text": "\u03d5x (t ) = n(cid:31)\n\u221e(cid:9)\n\ni=1\n\na.13 characteristic functions\n\n371\n\n(2) binomial: write x as the sum of n independent bernoulli random variables bi with\n\nparameter p. thus\n\n(t ) = [\u03d5bi\n\n\u03d5bi\n\n(t )]n = [1 \u2212 p(1 \u2212 eit )]n.\n\n= e\n\n\u2212\u03bbe\u03bbeit = e\u03bb(eit\u22121).\n\n(3) point mass at a: e eitx = eita. note that when a = 0, then \u03d5 is identically equal to 1.\n(4) poisson:\n\ne eitx =\n\neitke\n\n\u2212\u03bb \u03bbk\nk!\n\nk=0\n\n(5) uniform on [a, b]:\n\n(cid:9) (\u03bbeit )k\n\nk!\n\n= e\n\u2212\u03bb\n(cid:3)\n\n\u03d5(t ) = 1\nb \u2212 a\n\nb\n\na\n\nnote that when a = \u2212b this reduces to sin(bt )/bt.\n\n(cid:3) \u221e\n\n(6) exponential:\n\n\u03d5(t ) =\n\n(7) standard normal:\n\n0\n\n.\n\neitxdx = eitb \u2212 eita\n(b \u2212 a)it\n(cid:3) \u221e\n(cid:3) \u221e\n\n0\n\n\u2212x2/2dx.\n\neitxe\n\n\u2212\u221e\n\n\u2212\u03bbx dx = \u03bb\n\n\u03bbeitxe\n\ne(it\u2212\u03bb)xdx = \u03bb\n\u03bb \u2212 it\n\n.\n\n\u03d5(t ) = 1\u221a\n2\u03c0\n\n(cid:15) \u221e\n\n\u2212\u221e ixeitxe\n\n\u221a\nthis can be done by completing the square and then doing a contour integration. alternately,\n\u03d5(cid:3)(t ) = (1/\n\u2212x2/2dx. (do the real and imaginary parts separately, and use\n2\u03c0 )\nthe dominated convergence theorem to justify taking the derivative inside.) integrating by\nparts (do the real and imaginary parts separately), \u03d5(cid:3)(t ) = \u2212t\u03d5(t ). the only solution to this\ndifferential equation with \u03d5(0) = 1 is \u03d5(t ) = e\n(8) normal with mean \u03bc and variance \u03c3 2: writing x = \u03c3 z + \u03bc, where z is a standard\n\n\u2212t2/2.\n\nnormal, then\n\n\u03d5x (t ) = ei\u03bct \u03d5z (\u03c3t ) = ei\u03bct\u2212\u03c3 2t2/2.\n\n(a.25)\n\n(9) gamma. if x has a gamma distribution with parameters \u03bb and r, then its characteristic\n\nfunction is\n\n(cid:10) \u03bb\n\n\u03bb \u2212 it\n\n(cid:11)r\n\n.\n\ne eiux =\n\nformally, this comes from writing\n\n\u03d5(t ) = 1\n\u0001(r)\n\neitx\u03bbe\n\n\u2212\u03bbx(\u03bbx)r\u22121 dx = \u03bbr\n\u0001(r)\n\n(cid:3) \u221e\n\n0\n\n(cid:3) \u221e\n\n0\n\n\u2212(\u03bb\u2212it )xxr\u22121 dx\ne\n\nand performing a change of variables. to do it properly requires a contour integration around\nthe boundary of the region in the complex plane that is bounded by the positive x axis, the\nray {(\u03bb \u2212 it )r : r > 0}, \u2202b(0, \u03b5), and \u2202b(0, r), and then letting \u03b5 \u2192 0 and r \u2192 \u221e.\n\n "}, {"Page_number": 390, "text": "372\n\nbasic probability\n\na.14 uniqueness and characteristic functions\n\nif f is in the schwartz class, then so is(cid:2)f ; see section b.2. we use the fubini theorem\n\ntheorem a.48 if \u03d5x = \u03d5y , then px = py .\nproof\nand the fourier inversion theorem to write\n\n(cid:16)(cid:3) (cid:2)f (u)e\n\n(cid:17)\n\n(cid:3) (cid:2)f (u)\u03d5x (\u2212u) du,\n\ne f (x ) = (2\u03c0 )\u22121e\n\n\u2212iux du\n\n= (2\u03c0 )\u22121\n\nand similarly for e f (y ). since \u03d5x = \u03d5y , we conclude e f (x ) = e f (y ). by a limit\nprocedure, we have this equality for all bounded and measurable f , in particular, when f is\nthe indicator of a set.\n\nthe same proof works in higher dimensions: if\nj=1 u jx j = e ei\n\ne ei\n\nn\n\n(cid:12)\n\n(cid:12)\n\nn\nj=1 u jyj\n\n(cid:12)\n\nfor all (u1, . . . , un) \u2208 rn, then the joint laws of (x1, . . . , xn) and (y1, . . . , yn) are equal. the\nexpression e ei\n\nj=1 u jx j is called the joint characteristic function of (x1, . . . , xn).\n\nn\n\nthe following proposition can be proved directly, but the proof using characteristic func-\n\ntions is much easier.\n\nproposition a.49 (1) if x and y are independent, x is a normal random variable with\nmean a and variance b2, and y is a normal random variable with mean c and variance d2,\nthen x + y is normal random variable with mean a + c and variance b2 + d2.\n(2) if x and y are independent, x is a poisson random variable with parameter \u03bb1, and y\nis a poisson random variable with parameter \u03bb2, then x + y is a poisson random variable\nwith parameter \u03bb1 + \u03bb2.\n(3) if x and y are independent random variables, where x has a gamma distribution with\nparameters \u03bb and r1 and y has a gamma distribution with parameters \u03bb and r2, then x + y\nhas a gamma distribution with parameters \u03bb and r1 + r2.\nproof for (1),\n\n\u03d5x +y (t ) = \u03d5x (t )\u03d5y (t ) = eiat\u2212b2t2/2eict\u2212c2t2/2 = ei(a+c)t\u2212(b2+d2 )t2/2.\n\nnow use the uniqueness theorem.\n\nparts (2) and (3) are proved similarly.\n\na.15 the central limit theorem\n\nwe need the following estimate on moments.\nproposition a.50 if e|x |k < \u221e for an integer k, then \u03d5x has a continuous derivative of\norder k and\n\n(cid:3)\n\n\u03d5 (k )\nx\n\nin particular, \u03d5 (k )\nx\n\n(0) = ike x k.\n\n(t ) =\n\n(ix)keitxpx (dx).\n\n "}, {"Page_number": 391, "text": "(cid:3)\n\na.15 the central limit theorem\n\n373\n\nproof write\n\n\u03d5x (t + h) \u2212 \u03d5x (t )\n\nei(t+h)x \u2212 eitx\n\n=\n\nh\n\np(dx).\nsince |eihx \u2212 1| \u2264 |h||x|, the integrand is bounded by |x|. thus if\ncan use dominated convergence to obtain the desired formula for \u03d5(cid:3)\nproposition a.45, we see \u03d5(cid:3)\nx at 0 shows \u03d5 (k )\nevaluating \u03d5 (k )\n\n(t ). as in the proof of\n(t ) is continuous. we do the case of general k by induction.\n(0) = ike x k.\n\n(cid:15) |x|px (dx) < \u221e, we\n\nh\n\nx\n\nx\n\nx\n\nby the above,\n\ne x 2 = \u2212\u03d5(cid:3)(cid:3)\n\nx\n\n(0).\n\n(a.26)\n\nwith mean zero and variance one, sn =(cid:12)\n\nweakly to a standard normal. this is the case we prove.\n\nthe simplest case of the central limit theorem (clt) is the case when the xi\u2019s are i.i.d.,\nn converges\nwe need the fact that if wn are complex numbers converging to w, then (1+(wn/n))n \u2192 ew.\nwe leave the proof of this to the reader, with the warning that any proof using loga-\nrithms needs to be done with some care, since log z is a multivalued function when z is\ncomplex.\n\nn\ni=1 xi, and then the clt says that sn/\n\n\u221a\n\ntheorem a.51 suppose the xi\u2019s are i.i.d. random variables with mean zero and variance\none. then sn/\n\nn converges weakly to a standard normal.\n\n\u221a\n\nproof since x1 has finite second moment, then \u03d5x1 has a continuous second derivative by\nproposition a.50. by taylor\u2019s theorem,\n\n(t ) = \u03d5x1\n\n(0) + \u03d5(cid:3)\n\nx1\n\n(0)t + \u03d5(cid:3)(cid:3)\n\nx1\n\n\u03d5x1\n\n(0)t2/2 + r(t ),\n\nwhere |r(t )|/t2 \u2192 0 as |t| \u2192 0. thus\n\nthen\n\nsince t/\n\n\u03d5x1\n\n(t ) = 1 \u2212 t2/2 + r(t ).\n(cid:16)\n\n\u221a\nn))n =\n\n\u221a\nn) = (\u03d5x1\n\n1 \u2212 t2\n2n\n\n\u221a\n\nn(t ) = \u03d5sn\n\n(t/\n\n(t/\n\n\u03d5sn/\n\u221a\nn converges to zero as n \u2192 \u221e, we have\nn(t ) \u2192 e\n\n\u03d5sn/\n\n\u221a\n\n\u2212t2/2.\n\n(cid:17)n\n\n.\n\n+ r(t/\n\n\u221a\nn)\n\nsince e s2\nn\n\n/n = 1 for all n, proposition a.44 tells us that the random variables sn/\n\nn\nare tight, and from theorem a.43, subsequential weak limit points exist. by the preceding\nparagraph, any weak limit of a subsequence is a normal random variable with mean zero and\nvariance one. therefore the entire sequence converges weakly to a normal random variable\nwith mean zero and variance one.\n\n\u221a\n\n "}, {"Page_number": 392, "text": "374\n\nbasic probability\n\na.16 gaussian random variables\n\na normal random variable is also known as a gaussian random variable.\nproposition a.52 if z is a mean zero normal random variable with variance one and x \u2265 1,\nthen\n\n\u2212x2/2 \u2264 p(z \u2265 x) \u2264 e\ne\n\n\u2212x2/2.\n\n1\nx\n\nin particular, if \u03b5 > 0, there exists x0 such that\np(z \u2265 x) \u2265 e\n\n\u2212(1+\u03b5)x2/2\n\nif x \u2265 x0.\nproof for the right-hand inequality,\n\n(cid:3) \u221e\n\n(cid:3) \u221e\n\nx\n\n\u2212y2/2 dy \u2264\ne\n\ny\nx\n\n\u2212y2/2 dy = 1\nx\n\ne\n\n\u2212x2/2.\ne\n\np(z \u2265 x) = 1\u221a\n2\u03c0\n\nx\n\nthe left-hand inequality is left as an exercise.\n\nproposition a.53 if xn is a normal random variable with mean an and variance b2\nn, xn\nconverges to x weakly, an \u2192 a, and bn \u2192 b (cid:16)= 0, then x is a normal random variable with\nmean a and variance b2.\n\nproof since\n\nthen supn\n\ne x 2\nn\n\ne x 2\nn\n\n= var xn + (e xn)2 = b2\n\nn\n\n+ a2\n\nn\n\n,\n\n< \u221e, and the xn are tight. for each t, the characteristic functions converge:\n\n\u03d5x (t ) = lim\n\nn\u2192\u221e \u03d5xn\n\n(t ) = lim\n\nn\u2192\u221e eitan\u2212t2b2\n\nn\n\n/2 = eita\u2212t2b2/2,\n\nand the last term is the characteristic function of a normal random variable with mean a and\nvariance b2. therefore any weak subsequential limit point of the sequence xn is a normal\nrandom variable with mean a and variance b2.\n\nwe next prove\n\nproposition a.54 if\n\ne ei(ux +vy ) = e eiux e eivy\n\n(a.27)\n\nfor all u and v, then x and y are independent random variables.\n(cid:3)\n\n(cid:3)\n\nproof let x\n(cid:3)\nand so that x\nof the first variable, and y\n(cid:3)\nthen since eiux\n\nand eivy\n\n(cid:3)\n\nbe a random variable with the same law as x , y\nis independent of y\n\n. (we let \u0001 = [0, 1]2, p a lebesgue measure, x\n\none with the same law as y ,\na function\na function of the second variable defined as in proposition a.2.)\n\n(cid:3)\n\n(cid:3)\n\n(cid:3)\n\nare independent,\n(cid:3)+vy\n\ne ei(ux\n\n(cid:3) ) = e eiux\n\n(cid:3)\n\ne eivy\n\n(cid:3)\n\n.\n\n(a.28)\n\n(cid:3)\n\nhave the same law, e eiux = e eiux\n\n(cid:3)\n\nsince x , x\n(a.27) and (a.28), (x\n\n. therefore, using\n(cid:3)) has the same joint characteristic function as (x , y ). by the\n\n, and similarly for y, y\n\n(cid:3), y\n\n(cid:3)\n\n "}, {"Page_number": 393, "text": "uniqueness theorem for characteristic functions, (x\nwhich implies that x and y are independent.\n\n(cid:3), y\n\na.16 gaussian random variables\n\n375\n(cid:3)) has the same joint law as (x , y ),\n\na sequence of random variables x1, . . . , xn is said to be jointly normal if there exists a\nsequence of i.i.d. normal random variables z1, . . . , zm with mean zero and variance one and\nconstants bi j and ai such that\n\nbi jz j + ai,\n\ni = 1, . . . , n.\n\n(a.29)\n\nxi = m(cid:9)\n\nj=1\n\nin matrix notation, x = bz + a. for simplicity, in what follows let us take a = 0; the\nmodifications for the general case are easy. the covariance of two random variables x and\ny is defined to be e [(x \u2212 e x )(y \u2212 e y )]. since we are assuming our normal random\nvariables are mean zero, we can omit the centering at expectations. given a sequence of\nmean zero random variables, we can talk about the covariance matrix, which is\n\ncov (x ) = e x x t ,\n\nwhere x t denotes the transpose of the vector x . in the above case, we see cov (x ) =\ne [(bz )(bz )t ] = e [bzzt bt ] = bbt , since e zzt = i, the identity.\n\nlet us compute the joint characteristic function e eiut x of the vector x , where u is an\n\nn-dimensional vector. first, if v is an m-dimensional vector,\n\nm(cid:31)\n\nj=1\n\neiv jz j = m(cid:31)\n\ne eiv jz j = m(cid:31)\n\nj=1\n\nj=1\n\ne eivt z = e\n\n\u2212v2\ne\n\nj\n\n/2 = e\n\n\u2212vt v/2\n\nusing the independence of the z j\u2019s. thus\n\ne eiut x = e eiut bz = e\n\n\u2212ut bbt u/2.\n\nby taking u = (0, . . . , 0, a, 0, . . . , 0) to be a constant times the unit vector in the jth\ncoordinate direction, we deduce that x j is indeed normal, and this is true for each j.\nnote that the joint characteristic function of a jointly normal collection of random vari-\nables x = (x1, . . . , xn) is completely determined by bbt , which is the covariance matrix\nof x . in the case when the xi\u2019s are not mean zero, we can readily check that the joint char-\nacteristic function is determined by the covariance matrix together with the vector of means\ne x . therefore the joint distribution of a jointly normal collection of random variables is\ndetermined by the covariance matrix and the means.\nproposition a.55 if the xi are jointly normal and cov (xi, x j ) = 0 for i (cid:16)= j, then the xi\nare independent.\n\nif cov (x ) = bbt is a diagonal matrix, then the joint characteristic function of the\nproof\nxi\u2019s factors into the product of the characteristic functions of the xi\u2019s, and so by proposition\na.54, the xi\u2019s will in this case be independent.\n\nremark a.56 we note that the analog of proposition a.53 holds for jointly normal random\nvectors. that is, if (x 1\n) is a jointly normal collection of random variables for each\nj\nj converges in probability to x i and each xi is nonconstant, then (x 1, . . . , x n)\nj and each x i\n\n, . . . , x n\nj\n\n "}, {"Page_number": 394, "text": "376\n\nbasic probability\n\nis a jointly normal collection of random variables. this follows by looking at the joint\ncharacteristic functions as in the proof of proposition a.53.\n\nwe present the multidimensional central limit theorem.\n\ntheorem a.57 let x j = (x 1\n) be random vectors taking values in rd and suppose\n, . . . , x d\n= 0 and e (x k\nj\n\u221a\n)2 <\nthe x1, x2, . . . are independent and identically distributed. suppose e x k\n\u221e for k = 1, . . . , d and let ck(cid:14) = e [x k\n1\nn\nj=1 x j, then sn/\nn converges weakly\nto a jointly normal random vector z = (z1, . . . , zd ) where each zk has mean zero and the\ncovariance of zk and z(cid:14) is ck(cid:14).\n\n1 ]. if sn =(cid:12)\n\n1 x (cid:14)\n\n1\n\nj\n\nproof since\n\ne|sn|2/n = n(cid:9)\n\nd(cid:9)\n\nj\n\nj=1\n\nk=1\n\ne|x k\n\n|2/n\n\u221a\nis bounded independently of n, the random vectors sn/\nn are tight, and therefore weak\nsubsequential limit points exist. we need to show that any subsequential limit point is a\n(cid:12)\njointly normal random vector with mean zero and covariance matrix c.\nj , j = 1, 2, . . . , will be a sequence of i.i.d. random\n(cid:12)\n\nd\nk=1 ukx k\nvariables with mean zero and variance\n\nif u1, . . . , ud \u2208 r, then\n\nd\nk,(cid:14)=1 uku(cid:14)ck(cid:14). by theorem a.51,\n\n(cid:12)\n\n(cid:12)\n\nn\nj=1\n\nd\nk=1 ukx k\n\u221a\nj\nn\n\nd\n\nconverges weakly to a mean zero normal random variable with variance equal\nk,(cid:14)=1 uku(cid:14)ck(cid:14). if we write sn = (s1\nd(cid:9)\n\n), then\n\n(cid:10)\n\n(cid:11)\n\n(cid:10)\n\n(cid:11)\n\nn\n\nto\n\n, . . . , sd\nn\n\u221a\nn\n\n/\n\n\u2192 exp\n\n\u2212 d(cid:9)\n\nk,(cid:14)=1\n\ne exp\n\ni\n\nk=1\n\nuksk\nn\n\nuku(cid:14)ck(cid:14)/2\n\n.\n\n\u221a\nn has the required law.\n\nthis shows that any subsequential limit point of the sequence sn/\n\nif (x , y1, . . . , yn) are jointly normal random variables, then the law of x given y1, . . . , yn\n\nis also gaussian.\n\nproposition a.58 suppose x , y1, . . . , yn are jointly normal random variables with mean\nzero. let a be the n \u00d7 1 matrix whose ith entry is cov (x , yi), b the n \u00d7 n matrix whose\n(i, j)th entry is cov (yi, yj ), and y the n \u00d7 1 matrix whose ith entry is yi. suppose b is\ninvertible and let d = b\n\n\u22121a. then for u \u2208 r,\ne [eiux | y1, . . . , yn] = eiudt y e\n\n\u2212(var x \u2212at b\n\n\u22121a)/2.\n\nin particular, the law of x given y1, . . . , yn is that of a normal random variable with mean\ndty and variance equal to var x \u2212 at b\n\n\u22121a.\n\n(cid:12)\n\n "}, {"Page_number": 395, "text": "proof note\n\na.16 gaussian random variables\n\ncov (x \u2212 dty, yj ) = cov (x , yj ) \u2212 n(cid:9)\n\ndicov (yi, yj )\n\n377\n\nso x \u2212 dty is independent of each yj. then\n\ne [eiux | y1, . . . , yn] = eiudt y e [eiu(x \u2212dt y | y1, . . . , yn]\n\nto complete the proof, we calculate\n\nvar (x \u2212 dty ) = var x \u2212 2\n\ndibi jd j\n\n= a j \u2212 n(cid:9)\n\ni=1\n\ni=1\ndibi j = 0,\n\n\u2212var (x \u2212dt y )/2.\n\n= eiudt y e [eiu(x \u2212dt y ]\n= eiudt y e e\n(cid:9)\n(cid:9)\n= var x \u2212 at b\n\ndiai +\n\u22121a,\n\ni, j\n\ni\n\nand we are done.\n\n "}, {"Page_number": 396, "text": "appendix b\n\nsome results from analysis\n\nb.1 the monotone class theorem\n\nthe monotone class theorem is a result from measure theory used in the proof of the fubini\ntheorem.\n\ndefinition b.1 m is a monotone class if m is a collection of subsets of x such that\n(1) if a1 \u2282 a2 \u2282 \u00b7\u00b7\u00b7 , a = \u222aiai, and each ai \u2208 m, then a \u2208 m;\n(2) if a1 \u2283 a2 \u2283 \u00b7\u00b7\u00b7 , a = \u2229iai, and each ai \u2208 m, then a \u2208 m.\n\nrecall that an algebra of sets is a collection a of sets such that if a1, . . . , an \u2208 a, then\na1 \u222a \u00b7\u00b7\u00b7 \u222a an and a1 \u2229 \u00b7\u00b7\u00b7 \u2229 an are also in a, and if a \u2208 a, then ac \u2208 a.\nthe intersection of monotone classes is a monotone class, and the intersection of all mono-\ntone classes containing a given collection of sets is the smallest monotone class containing\nthat collection.\n\ntheorem b.2 suppose a0 is an algebra of sets, a is the smallest \u03c3 -field containing a0,\nand m is the smallest monotone class containing a0. then m = a.\nproof a \u03c3 -algebra is clearly a monotone class, so m \u2282 a. we must show a \u2282 m.\nlet n1 = {a \u2208 m : ac \u2208 m}. note n1 is contained in m, contains a0, and is a\nmonotone class. since m is the smallest monotone class containing a0, then n1 = m, and\ntherefore m is closed under the operation of taking complements.\nlet n2 = {a \u2208 m : a \u2229 b \u2208 m for all b \u2208 a0}. n2 is contained in m; n2 contains a0\nbecause a0 is an algebra; n2 is a monotone class because (\u222a\u221e\n(ai \u2229 b), and\nsimilarly for intersections. therefore n2 = m; in other words, if b \u2208 a0 and a \u2208 m, then\na \u2229 b \u2208 m.\nlet n3 = {a \u2208 m : a \u2229 b \u2208 m for all b \u2208 m}. as in the preceding paragraph, n3\nis a monotone class contained in m. by the last sentence of the preceding paragraph, n3\ncontains a0. hence n3 = m.\nwe thus have that m is a monotone class closed under the operations of tak-\ning complements and taking intersections. this shows m is a \u03c3 -algebra, and so\na \u2282 m.\n\ni=1ai) \u2229 b = \u222a\u221e\ni=1\n\n378\n\n "}, {"Page_number": 397, "text": "b.2 the schwartz class\n\n379\n\nand for each m, k \u2265 0 and each\n\nsuppose that f is in the schwartz class. suppose m, k \u2265 0 and i1, . . . , ik and j1, . . . , jn\nare each integers between 1 and d inclusive, and m1, . . . , mk are even positive integers. let\n\nb.2 the schwartz class\n\u221e\n\n|x|m\n\n(cid:20)(cid:20)(cid:20) \u2192 0\n\na function f : rd \u2192 r is in the schwartz class if f is c\ni1, i2, . . . , ik \u2208 {1, 2, . . . , d},\n\n\u2202 k f\n\u00b7\u00b7\u00b7 \u2202xik\n(x)\nas |x| \u2192 \u221e. (here i1, . . . , ik need not be distinct.)\n\n(cid:20)(cid:20)(cid:20)\n(cid:2)f be the fourier transform of f : (cid:2)f (u) =\n\neiu\u00b7x f (x) dx.\n\n(cid:3)\n\n\u2202xi1\n\nthen\n\n\u00b7\u00b7\u00b7 umk\n\nik\n\num1\ni1\n\nrd\n\n\u2202 j1+\u00b7\u00b7\u00b7+ jn(cid:2)f\n\u00b7\u00b7\u00b7 \u2202u jn\n\n\u2202u j1\n\n(u)\n\nis bounded as a function of u because it is a constant times the fourier transform of\n\nwhich is in l1(rd ) since f is in the schwartz class. we conclude that(cid:2)f is also in the schwartz\n\nx j1\n\nik\n\n,\n\n\u00b7\u00b7\u00b7 x jn\n\n\u2202 m1+\u00b7\u00b7\u00b7+mk f\n\u00b7\u00b7\u00b7 \u2202xmk\n\u2202xm1\ni1\n\nclass.\n\n "}, {"Page_number": 398, "text": "appendix c\n\nregular conditional probabilities\n\nlet e \u2282 f be \u03c3 -fields, where (\u0001,f , p) is a probability space. a regular conditional\nprobability for e [\u00b7 | e] is a map q : \u0001 \u00d7 f \u2192 [0, 1] such that\n\n(1) q(\u03c9,\u00b7) is a probability measure on (\u0001,f ) for each \u03c9;\n(2) for each a \u2208 f, q(\u00b7, a) is an e measurable random variable;\n(3) for each a \u2208 f and each b \u2208 e,\n\n(cid:3)\n\nb\n\nq(\u03c9, a) p(d\u03c9) = p(a \u2229 b).\n\nq(\u03c9, a) can be thought of as p(a | e ).\n\ntheorem c.1 suppose (\u0001,f , p) is a probability space, e \u2282 f, and \u0001 is in addition a\ncomplete and separable metric space. then a regular conditional probability for p(\u00b7 | e )\nexists.\n\nproof since \u0001 is a complete and separable metric space, we can embed \u0001 as a subset of\nthe compact set i = [0, 1]n, where we furnish i with the product topology. let { f j} be a\ncountable collection of uniformly continuous functions on \u0001 such that every finite subset of\ndistinct elements is linearly independent and such thatl0, the set of finite linear combinations\nof the f j\u2019s, is dense in the class of uniformly continuous functions on \u0001; let us assume f1 is\nidentically equal to 1.\nfor each j, let g j = e [ f j | e]. (the random variables g j are only defined up to almost\nsure equivalence. for each j we select an element g j from the equivalence class and keep it\nfixed.) if r1, . . . , rn are rationals with\n\nr1 f1(\u03c9) + \u00b7\u00b7\u00b7 + rn fn(\u03c9) \u2265 0\n\nfor all \u03c9, let\n\nn (r1, . . . , rn) = {\u03c9 : r1g1(\u03c9) + \u00b7\u00b7\u00b7 + rngn(\u03c9) < 0}.\n\nby the definition of g j, p(n (r1, . . . , rn)) = 0. let n1 be the union of all such n (r1, . . . , rn)\nwith n \u2265 1, the r j rational. then n1 \u2208 e and p(n1) = 0.\nfix \u03c9 \u2208 \u0001 \\ n1. define a functional l\u03c9 on l0 by\n\nl\u03c9( f ) = t1g1(\u03c9) + \u00b7\u00b7\u00b7 + tngn(\u03c9)\n\nif\n\nf = t1 f1 + \u00b7\u00b7\u00b7 + tn fn.\n\n380\n\n "}, {"Page_number": 399, "text": "regular conditional probabilities\n\n381\nwe claim l\u03c9 is a positive linear functional. if f = t1 f1 +\u00b7\u00b7\u00b7+ tn fn \u2265 0 and \u03b5 > 0 is rational,\nthen there exist rationals r1, . . . , rn such that r1 f1 + . . . + rn fn \u2265 \u2212\u03b5 and |ti \u2212 ri| \u2264 \u03b5,\ni = 1, . . . , n, or\n\nsince \u03c9 /\u2208 n1, then\n\n(r1 + \u03b5) f1 + r2 f2 + \u00b7\u00b7\u00b7 + rn fn \u2265 0.\n\n(r1 + \u03b5)g1 + r2g2 + \u00b7\u00b7\u00b7 + rngn \u2265 0.\n\nletting \u03b5 \u2192 0, it follows that t1g1 + \u00b7\u00b7\u00b7 + tngn \u2265 0. this proves that l\u03c9 is positive.\nsince l\u03c9( f1) = 1, this implies that l\u03c9 is a bounded linear functional, and by the hahn\u2013\nbanach theorem l\u03c9 can be extended to a positive linear functional on the closure of l0. any\nuniformly continuous function on \u0001 can be extended uniquely to \u0001, the closure of \u0001 in i,\nso l\u03c9 can be considered as a positive linear functional on c(\u0001). by the riesz representation\ntheorem, there exists a probability measure q(\u03c9,\u00b7) such that\nf (\u03c9(cid:3))q(\u03c9, d\u03c9(cid:3)).\n\n(cid:3)\n\nthe mapping \u03c9 \u2192 l\u03c9( f ) is measurable with respect to e for each f \u2208 l0, hence for all\nuniformly continuous functions on \u0001 by a limit argument. if b \u2208 e and f = t1 f1+\u00b7\u00b7\u00b7+tn fn,\n\nl\u03c9( f ) =\n(cid:17)\n\n(cid:3)\n\n(cid:16)(cid:3)\n\nb\n\nf (\u03c9(cid:3)) q(\u03c9, d\u03c9(cid:3))\n\np(d\u03c9) =\n=\n\n(cid:3)\n(cid:3)\n(cid:3)\n(cid:3)\n\n=\n\nb\n\nb\n\nb\n\nl\u03c9 f (\u03c9) p(d\u03c9)\n(t1g1 + \u00b7\u00b7\u00b7 + tngn)(\u03c9) p(d\u03c9)\ne [t1 f1 + \u00b7\u00b7\u00b7 + tn fn | e](\u03c9) p(d\u03c9)\n\nb\n\n=\n\nf (\u03c9) p(d\u03c9)\n\nf (\u03c9(cid:3))q(\u03c9, d\u03c9(cid:3)) is a version of e [ f |e] if f \u2208 l0. by a limit argument, the same is true\n\n(cid:15)\nor\nfor all f that are of the form f = 1a with a \u2208 f.\nlet gni be a sequence of balls of radius 1/n (with respect to the metric on \u0001) contained in \u0001\nand covering \u0001. choose in such that p(\u222ai\u2264ingni) > 1\u2212 1/(n2n). the set hn = \u2229n\u22651 \u222ai\u2264in gni\nis totally bounded; let kn be the closure of hn in \u0001. since \u0001 is complete, kn is complete and\ntotally bounded, and hence compact, and p(kn) \u2265 1 \u2212 1/n. hence\n\ni=1ki); \u0001 \\ n1] \u2265 e [q(\u00b7, kn); \u0001 \\ n1] = p(kn) \u2265 1 \u2212 (1/n)\ne [q(\u00b7,\u222a\u221e\nfor each n, or q(\u03c9,\u222a\u221e\ni=1ki) = 1, a.s. let n2 be the null set for which this fails. thus for\n\u03c9 \u2208 \u0001 \\ (n1 \u222a n2), we see that q(\u03c9, d\u03c9(cid:3)) is a probability measure on \u0001. for \u03c9 \u2208 n1 \u222a n2,\nset q(\u03c9,\u00b7) = p(\u00b7). this q is the desired regular conditional probability.\n\n "}, {"Page_number": 400, "text": "appendix d\n\nkolmogorov extension theorem\n\nsuppose s is a metric space. we use s n for the product space s \u00d7s \u00d7\u00b7\u00b7\u00b7 furnished with the\nproduct topology. we may view s n as the set of sequences (x1, x2, . . .) of elements of s. we\nuse the \u03c3 -field on s n generated by the cylindrical sets. given an element x = (x1, x2, . . .) of\ns n, we define \u03c0n(x) = (x1, . . . , xn) \u2208 s n.\nwe suppose we have a radon probability measure \u03bcn defined on s n for each n. (being\na radon measure means that we can approximate \u03bcn(a) from below by compact sets; see\nfolland (1999) for details.) the \u03bcn are consistent if \u03bcn+1(a \u00d7 s ) = \u03bcn(a) whenever a is a\nborel subset of s n. the kolmogorov extension theorem is the following.\ntheorem d.1 suppose for each n we have a probability measure \u03bcn on s n. suppose the \u03bcn\u2019s\nare consistent. then there exists a probability measure \u03bc ons n such that \u03bc(a\u00d7s n) = \u03bcn(a)\nfor all a \u2282 s n.\nproof define \u03bc on cylindrical sets by \u03bc(a \u00d7 s n) = \u03bcn(a) if a \u2282 s n. by the consistency\nassumption, \u03bc is well defined. by the carath\u00b4eodory extension theorem, we can extend \u03bc\nto the \u03c3 -field generated by the cylindrical sets provided we show that whenever an are\ncylindrical sets decreasing to \u2205, then \u03bc(an) \u2192 0.\nsuppose an are cylindrical sets decreasing to \u2205 but \u03bc(an) does not tend to 0; by taking\na subsequence we may assume without loss of generality that there exists \u03b5 > 0 such that\n\u03bc(an) \u2265 \u03b5 for all n. we will obtain a contradiction.\nwe first want to arrange things so that each an = \u03c0n(an) \u00d7 s n. suppose an is of the\nform\n\nan = {(x1, x2, . . .) : (x1, . . . , x jn\n\n) \u2208 bn},\n\nis a borel subset of s jn. we choose mn = n + max( j1, . . . , jn). let\nwhere bn\na0 = s n. we then replace our original sequence a1, a2, . . . by the sequence\na0, . . . , a0, a1, . . . , a1, a2, . . . , a2, a3, . . . , where we have m1 occurrences of a0, m2 \u2212 m1\noccurrences of a1, m3 \u2212 m2 occurrences of a2, and so on. therefore we may without loss of\ngenerality suppose jn \u2264 n. we then have\n\nan = {(x1, x2, . . .) : (x1, . . . , xn) \u2208 bn \u00d7 s n\u2212 jn}.\n\nreplacing bn by bn \u00d7 s jn\u2212n, we may without\n\u03c0n(an) \u00d7 s n.\n\nloss of generality suppose an =\n\n382\n\n "}, {"Page_number": 401, "text": "i=1\n\n383\n\nkolmogorov extension theorem\n\n\u03bc(ai \\ bi) \u2265 \u03b5/2,\n\n\u03bc(cn) \u2265 \u03bc(an) \u2212 n(cid:9)\n\nwe set(cid:14)an = \u03c0n(an). for each n, choose(cid:14)bn \u2282(cid:14)an so that(cid:14)bn is compact and \u03bc((cid:14)an \\(cid:14)bn) \u2264\n\u03b5/2n+1. let bn =(cid:14)bn \u00d7s n and let cn = b1 \u2229 . . . \u2229 bn. hence cn \u2282 bn \u2282 an, and cn \u2193 \u2205, but\nand(cid:14)cn = \u03c0n(cn), the projection of cn onto s n, is compact.\nin(cid:14)c1, which is compact, hence there is a convergent subsequence {y1(nk )}. let x1 be the limit\nset(cid:14)c2, so a further subsequence {(y1(nk j\n))} converges to a point in(cid:14)c2. since {nk j\n))} is of the form (x1, x2), and this point is in(cid:14)c2. we continue this procedure\nto obtain x = (x1, x2, . . . , xn, . . .). by our construction, (x1, . . . , xn) \u2208(cid:14)cn for each n, hence\n\nwe will find x = (x1, . . . , xn, . . . ) \u2208 \u2229ncn and obtain our contradiction. for each n choose\na point y(n) \u2208 cn. the first coordinates of {y(n)}, namely, {y1(n)}, form a sequence contained\npoint. the first and second coordinates of {y(nk )} form a sequence contained in the compact\n}\nis a subsequence of {nk}, the first coordinate of the limit is x1. therefore the limit point of\n{(y1(nk j\nx \u2208 cn for each n, or x \u2208 \u2229ncn, a contradiction.\na typical application of this theorem is to construct a countable sequence of independent\nrandom variables. we construct x1, . . . , xn as in proposition a.10. here s = [0, 1]. let \u03bcn\nbe the law of (x1, . . . , xn); it is easy to check that the \u03bcn form a consistent family. we use\ntheorem d.1 to obtain a probability measure \u03bc on [0, 1]n. to get random variables out of\nthis, we let xi(\u03c9) = \u03c9i if \u03c9 = (\u03c91, \u03c92, . . .).\n\n), y2(nk j\n\n), y2(nk j\n\n "}, {"Page_number": 402, "text": " "}, {"Page_number": 403, "text": "references\n\naldous, d. 1978. stopping times and tightness. ann. probab. 6, 335\u201340.\nbarlow, m. t. 1982. one-dimensional stochastic differential equations with no strong solution. j. london\n\nmath. soc. 26, 335\u201347.\n\nbass, r. f. 1983. skorokhod imbedding via stochastic integrals. s\u00b4eminaire de probabilit\u00b4es xvii. new york:\n\nspringer-verlag; 221\u20134.\n\nbass, r. f. 1995. probabilistic techniques in analysis. new york: springer-verlag.\nbass, r. f. 1996. the doob\u2013meyer decomposition revisited. can. math. bull. 39, 138\u201350.\nbass, r. f. 1997. diffusions and elliptic operators. new york: springer-verlag.\nbillingsley, p. 1968. convergence of probability measures. new york: john wiley & sons, ltd.\nbillingsley, p. 1971. weak convergence of measures: applications in probability. philadelphia: siam.\nblumenthal, r. m. and getoor, r. k. 1968. markov processes and potential theory. new york: academic\n\npress.\n\nbogachev, v. i. 1998. gaussian measures. providence, ri: american mathematical society.\nboyce, w. e. and diprima, r. c. 2009. elementary differential equations and boundary value problems,\n\n9th edn. new york: john wiley & sons, ltd.\n\nchung, k. l. 2001. a course in probability theory, 3rd edn. san diego: academic press.\nchung, k. l. and walsh, j. b. 1969. to reverse a markov process. acta math. 123, 225\u201351.\ndawson, d. a. 1993. measure-valued markov processes. ecole d\u2019et\u00b4e de probabilit\u00b4es de saint-flour xxi\u2013\n\n1991. berlin: springer-verlag.\n\ndellacherie, c. and meyer, p.-a. 1978. probability and potential. amsterdam: north-holland.\ndudley, r. m. 1973. sample functions of the gaussian process. ann. probab. 1, 66\u2013103.\ndurrett, r. 1996. probability: theory and examples. belmont, ca: duxbury press.\nethier, s. n. and kurtz, t. g. 1986. markov processes: characterization and convergence. new york: john\n\nwiley & sons, ltd.\n\nfeller, w. 1971. an introduction to probability theory and its applications, 2nd edn. new york: john\n\nwiley & sons, ltd.\n\nfolland, g. b. 1999. real analysis: modern techniques and their applications, 2nd edn. new york: john\n\nwiley & sons, ltd.\n\nfukushima, m., oshima, y. and takeda, m. 1994. dirichlet forms and symmetric markov processes. berlin:\n\nde gruyter.\n\ngilbarg, d. and trudinger, n. s. 1983. elliptic partial differential equations of second order, 2nd edn.\n\nnew york: springer-verlag.\n\nit\u02c6o, k. and mckean, jr, h. p. 1965. diffusion processes and their sample paths. berlin: springer-verlag.\nkallianpur, g. 1980. stochastic filtering theory. berlin: springer-verlag.\nkaratzas, i. and shreve, s. e. 1991. brownian motion and stochastic calculus, 2nd edn. new york: springer-\n\nverlag.\n\nknight, f. b. 1981. essentials of brownian motion and diffusion. providence, ri: american mathematical\n\nsociety.\n\nkuo, h. h. 1975. gaussian measures in banach spaces. new york: springer-verlag.\nlax, p. 2002. functional analysis. new york: john wiley & sons, ltd.\n\n385\n\n "}, {"Page_number": 404, "text": "386\n\nreferences\n\nliggett, t. m. 2010. continuous time markov processes: an introduction. providence, ri: american math-\n\nematical society.\n\nmeyer, p.-a., smythe, r. t. and walsh, j. b. 1972. birth and death of markov processes. proceedings of the\nsixth berkeley symposium on mathematical statistics and probability, vol. iii. berkeley, ca: university\nof california press; 295\u2013305.\n\nob\u0142\u00b4oj, j. 2004. the skorokhod embedding problem and its offspring. probab. surv. 1, 321\u201390.\n\u00f8ksendal, b. 2003. stochastic differential equations: an introduction with applications, 6th edn. berlin:\n\nspringer-verlag.\n\nperkins, e. a. 2002. dawson\u2013watanabe superprocesses and measure-valued diffusions. lectures on proba-\n\nbility theory and statistics (saint-flour, 1999). berlin: springer-verlag; 125\u2013324.\n\nrevuz, d. and yor, m. 1999. continuous martingales and brownian motion, 3rd edn. berlin: springer-\n\nverlag.\n\nrogers, l. c. g. and williams, d. 2000a. diffusions, markov processes, and martingales, vol. 1. cambridge:\n\ncambridge university press.\n\nrogers, l. c. g. and williams, d. 2000b. diffusions, markov processes, and martingales, vol. 2. cambridge:\n\ncambridge university press.\n\nrudin, w. 1976. principles of mathematical analysis, 3rd edn. new york: mcgraw-hill.\nrudin, w. 1987. real and complex analysis, 3rd edn. new york: mcgraw-hill.\nskorokhod, a. v. 1965. studies in the theory of random processes. reading, ma: addison-wesley.\nstroock, d. w. 2003. markov processes from k. it\u02c6o\u2019s perspective. princeton, nj: princeton university press.\nstroock, d. w. and varadhan, s. r. s. 1977. multidimensional diffusion processes. berlin: springer-verlag.\nwalsh, j. b. 1978. excursions and local time. ast\u00b4erisque 52\u201353, 159\u201392.\n\n "}, {"Page_number": 405, "text": "adapted, 1, 359\nadditive functional, 169, 180\n\nclassical, 180\n\naldous criterion, 264\nalmost surely, 348\nannounce, 112\n\nbessel processes, 200\nbinomial, 349, 371\nblack\u2013scholes formula, 220\nblumenthal 0\u20131 law, 164\nbmo, 129\nborel\u2013cantelli lemma, 353, 354\nbrownian bridge, 273\nbrownian motion, 6, 153\n\ncovariance, 8\nfractional, 254\nintegrated, 41\nmaximum, 27\nstandard, 6\nwith drift, 24\nzero set, 30, 48, 99, 214, 217\n\nbrownian sheet, 254\nburkholder\u2013davis\u2013gundy\n\ninequalities, 82\n\ncadlag, 2\ncameron\u2013martin space, 253\ncanonical process, 158\ncauchy problem, 321\ncemetery, 156, 177\ncentral limit theorem, 373\nchaining, 51\nchange of variables formula, 71\nchapman\u2013kolmogorov\n\nequations, 155\n\ncharacteristic function, 370\nchebyshev\u2019s inequality, 352\nchung\u2019s law of the iterated\n\nlogarithm, 47\n\nclass d, 57, 124\nclass dl, 126\nclosed form, 303\nclosed operator, 295\ncompensator, 124, 130\n\nindex\n\ncomplete filtration, 1\nconditional expectation, 357\nconditional probability, 357\nconditioned processes, 178\nconsistent, 382\nconstruction of brownian motion, 36, 248, 254, 284\ncontinuation region, 187\ncontinuous process, 2\nconvergence\n\nalmost surely, 355\nin lp, 355\nin distribution, 367\nin law, 367\nin probability, 355\nweak, 367\n\nconvolution semigroup, 285\ncovariance, 375\ncovariance matrix, 375\ncovariation, 58\ncumulative normal distribution function, 227\ncylindrical set, 3\n\nd[0, 1]\n\ncompactness, 263\ncompleteness, 262\nmetrics, 259\n\ndebut, 117\ndebut theorem, 117\ndensity, 349\ndiffusion coefficient, 193, 315\ndirichlet boundary condition, 290\ndirichlet form, 303\ndirichlet problem, 320\ndissipative, 294\ndistribution, 348\ndistribution function, 348\ndivergence form elliptic\n\noperators, 307\n\ndonsker invariance principle, 269\ndoob decomposition, 361\ndoob\u2019s h-path transform, 178\ndoob\u2019s inequalities, 14, 361\ndoob\u2013meyer decomposition, 60, 124\ndrift coefficient, 193, 315\ndual optional projection, 124\n\n387\n\n "}, {"Page_number": 406, "text": "388\n\nindex\n\ndual predictable projection, 124\ndyadic rationals, 49\n\nempirical process, 275\nentry time, 115\nequivalent martingale measure, 223\nevents, 348\nexcessive, 184\nexcessive majorant, 186\nexercise time, 219\nexpected value, 348\nexponential, 349, 371\n\nmartingale, 89\nsemimartingale, 144\n\nexponential random variables, 33\n\nfeller process, 161\nfeynman\u2013kac formula, 323\nfiltration, 1, 2\nfinite-dimensional distributions, 3\nfourier series, 36\n\ngamma, 350, 371\ngauge, 323\ngaussian, 7, 374\ngaussian field, 255\ngirsanov theorem, 89, 93, 144\nglivenko\u2013cantelli theorem, 366\ngood-\u03bb inequality, 86\ngreen\u2019s function, 175\ngronwall\u2019s lemma, 201\n\nh\u00a8older continuous, 43, 47\nharmonic, 173, 321\nhausdorff dimension, 48, 99\nhausdorff measure, 48\nheat equation, 322\nhelly\u2019s theorem, 369\nhille\u2013yosida theorem, 292\nhitting time, 115\nhunt process, 165\nhurst index, 254\n\ni.i.d., 364\nincreasing process, 54, 121\nindependent, 353\nindependent increments, 6, 339\nindicator, 348\nindistinguishable, 2\ninfinite particle systems, 295\ninfinitesimal generator, 288\ninnovation process, 230\ninnovations approach, 229\nintegration by parts formula, 74\ninvariance principle, 108\ninvariant, 178\nit\u02c6o\u2019s formula, 71\n\nmultivariate, 74\n\njensen\u2019s inequality, 352, 358\njohn\u2013nirenberg inequality, 129\njoint characteristic function, 372\njointly normal, 7, 375\n\nkalman\u2013bucy filter, 234\nkarhunen\u2013lo`eve expansion, 253\nkernel, 154\nkilled process, 177\nkolmogorov backward equation, 291\nkolmogorov continuity criterion, 49\nkolmogorov extension theorem , 382\nkolmogorov forward equation, 292\nkunita\u2013watanabe inequality, 70\n\nl\u00b4evy measure, 342\nl\u00b4evy process, 32, 297, 339\nl\u00b4evy system formula, 347\nl\u00b4evy\u2019s theorem, 77\nl\u00b4evy\u2013khintchine formula, 342\nlast exit, 181\nlaw, 3, 10, 348\nlaw of the iterated logarithm, 44\nleast excessive majorant, 186\nleft continuous process, 2\nlifetime, 156, 177\nlil, 44\nlinear equations, 199\nlinear model, 234\nlipschitz function, 100, 193\nlocal time, 94, 209\n\njoint continuity, 96\nlocally bounded, 141\nlower semicontinuous, 186\n\nmarkov property, 25\nmarkov transition probabilities, 154\nmarkovian, 303\nmartingale, 13, 359\n\ncontinuous, 54\nconvergence theorem, 363\nlocal, 54, 139\nlocally square integrable, 139\nproblem, 316\nrepresentation theorem, 80, 81\nuniformly integrable, 54, 364\n\nmaximum principle, 176\nmean, 350\nmean rate of return, 218\nmeasure-valued branching diffusion\n\nprocess, 317\nmetric entropy, 51\nminimal augmented filtration, 2,\n\n160\n\nmodulus of continuity, 247, 260\nmoment, 350\nmonotone class theorem, 378\nmultiplication theorem, 354\n\n "}, {"Page_number": 407, "text": "natural scale, 326\nneumann boundary condition, 290\nnewtonian potential density, 175\nnflvr condition, 223\nno free lunch, 223\nnondivergence form, 296, 315\nnon-negative definite, 254\nnormal, 349, 371\nnowhere differentiable, 46\nnull set, 1, 111\n\nobservation process, 229\noccupation time density, 175\noccupation times, 97\none-dimensional diffusion, 326\noptimal reward, 187\noptimal stopping problem, 184\noptional \u03c3 -field, 111\noptional projection, 119\noptional stopping theorem, 17, 360\noptional time, 15\nornstein\u2013uhlenbeck process, 159, 198\northogonality lemma, 131\nouter probability, 111\n\np-variation, 30, 48\npartial sums, 364\npaths, 2\npaths locally of bounded variation, 54\npicard iteration, 101\npoincar\u00b4e cone condition, 174\npoisson, 349, 371\n\npoint process, 147\nprocess, 32, 171\n\npoisson\u2019s equation, 319\nportmanteau theorem, 237\npotential, 155, 323\npr\u00b4evisible, 111\npredict, 112\npredictable, 64, 130\npredictable \u03c3 -field, 64, 111\npredictable projection, 120\nprobability, 348\nprocess, 1\nproduct formula, 74, 85\nprogressively measurable, 4\nprohorov metric, 241\nprohorov theorem, 239\npurely discontinuous, 143\n\nquadratic variation, 57, 79\nquasi-left continuous, 165\n\nrandom variables, 348\nray\u2013knight theorem, 209\nrecurrence, 167\nreduce, 139\nreflection principle, 27\n\nindex\n\n389\n\nregular, 173, 326\nregular conditional probability, 312, 380\nregular dirichlet form, 307\nreproducing property, 252\nresolvent, 155, 286\nreward function, 184\nright continuous filtration, 1\nright continuous process, 2\nright continuous with left limits, 2\n\nscale function, 327\nscaling, 7\nschr\u00a8odinger operator, 323\nschwartz class, 379\nsection theorem\noptional, 117\npredictable, 117\nself-financing, 219\nsemigroup, 155\nsemigroup property, 155\nsemimartingale, 54, 141\nset-indexed process, 255\nshift operators, 158\nsignal process, 229\nsimple symmetric random walk, 109, 248\nskorokhod embedding, 100\nskorokhod representation, 245\nslutsky\u2019s theorem, 242, 369\nspace-time process, 182\nspectral theorem, 309\nspeed measure, 329\nsquare integrable martingale, 55\nstable subordinator, 347\nstationary increments, 6, 339\nstochastic integral, 64, 134, 150\n\nlocal martingales, 69\nmultiple, 88\nsemimartingales, 69\n\nstochastic process, 1\nstopping time, 15, 359\nstratonovich integral, 84\nstrong feller process, 161\nstrong law of large numbers, 364\nstrong markov process, 165\nstrong markov property, 25\nstrongly reduce, 139\nsub-markov transition probability\n\nkernels, 283\n\nsubmartingale, 359\nsuper-brownian motion, 317\nsupermartingale, 359\nsupport theorem, 93, 208\nsymmetric difference, 12\nsymmetric stable process, 346\n\ntanaka formula, 94, 95\nterminal time, 177\ntight, 369\n\n "}, {"Page_number": 408, "text": "390\n\nindex\n\ntime change, 78, 105, 180\ntime inversion, 11\ntotally inaccessible, 112, 130\ntrading strategy, 219\ntrajectories, 2\ntransience, 167\ntransition densities, 291\ntransition probabilities, 154\n\nuniform ellipticity, 296, 307, 315\nuniformly absolutely continuous, 356\nuniformly integrable, 356\nunique in law, 204\nupcrossings, 18, 362\nusual conditions, 1\n\nvariance, 350\nversions, 2\nvitali convergence theorem,\n\n357\n\nvolatility, 218\n\nweak convergence, 367\nweak feller process, 161\nweak solution, 204\nweak uniqueness, 204\nwell posed, 316\nwell measurable, 111\nwiener measure, 6\n\nyamada\u2013watanabe condition, 196\n\n "}]}
{"Filename": "RossProbability", "Pages": [{"Page_number": 1, "text": " "}, {"Page_number": 2, "text": "a first course in probability\n\n "}, {"Page_number": 3, "text": "this page intentionally left blank \n\n "}, {"Page_number": 4, "text": "a first course in probability\n\neighth edition\n\nsheldon ross\n\nuniversity of southern california\n\nupper saddle river, new jersey 07458\n\n "}, {"Page_number": 5, "text": "library of congress cataloging-in-publication data\nross, sheldon m.\n\na first course in probability / sheldon ross. \u2014 8th ed.\n\np. cm.\n\nincludes bibliographical references and index.\nisbn-13: 978-0-13-603313-4\nisbn-10: 0-13-603313-x\n1. probabilities\u2014textbooks.\nqa273.r83 2010\n519.2\u2014dc22\n\ni. title.\n\n2008033720\n\neditor in chief, mathematics and statistics: deirdre lynch\nsenior project editor: rachel s. reeve\nassistant editor: christina lepre\neditorial assistant: dana jones\nproject manager: robert s. merenoff\nassociate managing editor: bayani mendoza de leon\nsenior managing editor: linda mihatov behrens\nsenior operations supervisor: diane peirano\nmarketing assistant: kathleen dechavez\ncreative director: jayne conte\nart director/designer: bruce kenselaar\nav project manager: thomas benfatti\ncompositor: integra software services pvt. ltd, pondicherry, india\ncover image credit: getty images, inc.\n\n\u00a9 2010, 2006, 2002, 1998, 1994, 1988,\n1984, 1976 by pearson education, inc.,\npearson prentice hall\npearson education, inc.\nupper saddle river, nj 07458\n\nall rights reserved. no part of this book may be reproduced, in any\nform or by any means, without permission in writing from the publisher.\n\npearson prentice hall\u2122 is a trademark of pearson education, inc.\n\nprinted in the united states of america\n\n10 9 8 7 6 5 4 3 2 1\n\nisbn-13: 978-0-13-603313-4\nisbn-10:\n0-13-603313-x\n\npearson education, ltd., london\npearson education australia pty. limited, sydney\npearson education singapore, pte. ltd\npearson education north asia ltd, hong kong\npearson education canada, ltd., toronto\npearson educaci \u00b4on de mexico, s.a. de c.v.\npearson education \u2013 japan, tokyo\npearson education malaysia, pte. ltd\npearson education upper saddle river, new jersey\n\n "}, {"Page_number": 6, "text": "for rebecca\n\n "}, {"Page_number": 7, "text": "this page intentionally left blank \n\n "}, {"Page_number": 8, "text": "contents\n\npreface\n\n1 combinatorial analysis\n\n. . . . . . . .\n. . . . . . .\n\n. . . .\n. . . . .\n\n.\n\n.\n\n. .\n.\n\n. . . .\n\n. . . .\n\n. . . .\n\n. . . .\n\n. . . . .\n\n. . . . . . .\n\n. . . . . . . .\n\nintroduction .\n\n1.1\n1.2 the basic principle of counting .\n1.3 permutations .\n1.4 combinations\n1.5 multinomial coefficients .\n1.6 the number of integer solutions of equations\n. .\n\n. . .\n. . . . . . .\n. . . . .\n\n. . . . . . . .\nsummary . .\n. . .\nproblems . .\ntheoretical exercises .\n. . . .\nself-test problems and exercises .\n\n. . . . .\n. . . . . . .\n\n. . . . . . . .\n. . . . .\n. . . .\n\n. . . .\n. .\n\n. . . . .\n\n. . . .\n\n. .\n\n.\n.\n\n.\n\n.\n\n. . . . .\n\n. . . . . . .\n. . . .\n. . . . . . .\n\n. . . . . . . .\n\n. . . .\n. . . . .\n. . . .\n. . . . . . . . .\n\n. . . . . .\n\n. . .\n. . . .\n. .\n. . .\n. . . .\n. . . .\n. .\n. .\n. . . . . . . . .\n. . . .\n. . . .\n\n. . . . . . .\n\n. . . .\n. . . . . . .\n\n. . .\n\n2 axioms of probability\n.\n\nintroduction .\n\n. . . .\n\n2.1\n. . . . . . .\n2.2 sample space and events . . . . . .\n. . . .\n2.3 axioms of probability .\n. . . .\n. .\n2.4 some simple propositions\n. . . .\n. . . .\n2.5 sample spaces having equally likely outcomes . . .\n2.6 probability as a continuous set function .\n. . . .\n2.7 probability as a measure of belief .\n. . . .\n\n. . . .\n. . .\n. . . . . . .\n. . . . . . .\n\n. .\n. . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n.\n.\n\n.\n\n.\n.\n\n. . . . . . . .\nsummary . .\n. . .\nproblems . .\ntheoretical exercises .\n. . . .\nself-test problems and exercises .\n\n. . . . .\n. . . . . . .\n\n. . . . . . . .\n. . . . .\n. . . .\n\n. . . .\n. .\n\n. .\n\n. .\n\n. . . . .\n\n.\n\n. . . . . . .\n\n. . .\n\n. . . . .\n\n. . . . . . .\n. . . . . . .\n. . . . .\n. . . . .\n. . . . . . .\n. . . . . . . . .\n\n. . . . . .\n. . . . .\n. .\n. .\n. . . .\n. . . .\n. . . .\n. .\n. .\n. . . . . . . . .\n. . . .\n. . . .\n\n. . . .\n. . . . . . .\n\n. . .\n\nxi\n\n1\n1\n1\n3\n5\n9\n12\n15\n16\n18\n20\n\n22\n22\n22\n26\n29\n33\n44\n48\n49\n50\n54\n56\n\n3 conditional probability and independence\n\n.\n\n.\n\n. . . .\n\n. . . . . . .\n. . . .\n. . . . . . .\n\nintroduction .\n\n3.1\n3.2 conditional probabilities .\n3.3 bayes\u2019s formula . . . .\n3.4\n3.5 p(\u00b7|f) is a probability .\n. . . . .\n.\n. . . . . .\n\nindependent events . . . . . .\n\nsummary . .\nproblems . .\ntheoretical exercises .\n. . . .\nself-test problems and exercises .\n\n. . . . . . .\n. . .\n\n. . . .\n\n. .\n\n. .\n\n. .\n\n.\n.\n\n.\n\n.\n\n. . . .\n. . . . .\n. . . .\n\n. . . . . . .\n\n. . .\n\n. . . . . . .\n\n. . . .\n\n. . . . . . .\n\n. . .\n\n. . . . . . . . .\n\n. . . . .\n. . . .\n\n. . . .\n. . . . . . .\n. . . . . . . .\n\n. . . . . .\n. . . .\n. . .\n\n. . . . . . . . .\n\n. .\n\n. . . . . . .\n\n. . . . .\n. . . .\n\n. . . . . . .\n\n. . . .\n. . . . . . .\n\n. . . . .\n\n58\n58\n. . . . . .\n58\n. . . .\n65\n. . . . . .\n79\n. .\n. . . .\n93\n. . . . 101\n. . . . . 102\n. . . . 110\n. . . . 114\n\n4 random variables\n\n. . . . . . .\n\n. . . .\n. . . . .\n\n. . . . . . .\n. . . .\n. . .\n\n4.1 random variables . . .\n4.2 discrete random variables\n4.3 expected value . . . . . . .\n. . . .\n4.4 expectation of a function of a random variable . . .\n4.5 variance . .\n4.6 the bernoulli and binomial random variables . . . .\n4.6.1 properties of binomial random variables . . .\n4.6.2 computing the binomial distribution function . .\n\n. . . . . . . .\n\n. . . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . .\n\n. . . .\n\n.\n\n.\n\n. . .\n\n. . . .\n\n117\n. . . . . . 117\n. . . . 123\n. . . 125\n. . . . 128\n. . . . 132\n. . . . 134\n. . . . 139\n. . 142\n\n. . . .\n\n. . . . . .\n. . . . .\n. . .\n. . . . .\n. . . . .\n\n.\n\nvii\n\n "}, {"Page_number": 9, "text": "viii\n\ncontents\n\n4.7 the poisson random variable . . .\n\n. . . . .\n\n. . . . . . .\n\n. . . .\n\n4.7.1 computing the poisson distribution function .\n. . . .\n4.8 other discrete probability distributions .\n4.8.1 the geometric random variable .\n. . . .\n4.8.2 the negative binomial random variable . . .\n4.8.3 the hypergeometric random variable . . . .\n4.8.4 the zeta (or zipf) distribution . . . .\n4.9 expected value of sums of random variables\n4.10 properties of the cumulative distribution function .\n. . .\n\n. . . . . . .\n\n. .\n. .\n\n. .\n\n.\n.\n\n.\n.\n\n. . . .\nsummary . .\n. . . . . . . .\nproblems . .\ntheoretical exercises .\n. . . .\nself-test problems and exercises .\n\n.\n. . . .\n\n. .\n\n.\n\n. . . . . . .\n. . .\n. . . . .\n. . . .\n\n. . . .\n\n.\n\n. .\n. . . . .\n. . . . .\n. . . . .\n. . . . .\n\n. . . . . . . . . .\n. . . . . . .\n. . . .\n. .\n. . . .\n.\n. . . . . . . .\n\n.\n\n. . . . . . . .\n\n. . . .\n\n. . . . . . .\n\n. . . .\n. . . . . . .\n\n. . . . .\n\n. . . . 143\n. . 154\n. . . . 155\n. . . . 155\n. . . . 157\n. . . . 160\n. . 163\n. . 164\n. . 168\n. . 170\n. . . . . 172\n. . . . 179\n. . . . 183\n\n5 continuous random variables\n\n.\n\n. . . .\n\n. . . . . . .\n\nintroduction .\n\n5.1\n. . .\n5.2 expectation and variance of continuous random variables\n5.3 the uniform random variable .\n. . . .\n. . . . . . .\n5.4 normal random variables .\n5.4.1 the normal approximation to the binomial distribution .\n\n. . . .\n. . . . . . .\n\n. . . . . . .\n. . . .\n\n.\n. . . .\n\n. . . . . . .\n\n. . . .\n\n5.6 other continuous distributions .\n\n5.5 exponential random variables .\n\n.\n5.5.1 hazard rate functions . . .\n.\n5.6.1 the gamma distribution .\n5.6.2 the weibull distribution .\n5.6.3 the cauchy distribution . . . . . . . . .\n5.6.4 the beta distribution . . . .\n\n. . . .\n. . . . .\n. . . .\n. .\n. .\n\n. .\n\n. . .\n. . . . . . . .\n. . .\n\n. . . . .\n\n. . . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . .\n. . . . . . . . . .\n\n. . . . . . .\n. . . .\n\n5.7 the distribution of a function of a random variable . .\n\n.\n.\n\n. . . . . . .\n\n. . . .\nsummary . .\n. . . . . . . .\nproblems . .\ntheoretical exercises .\n. . . .\nself-test problems and exercises .\n\n.\n. . . .\n\n. .\n\n.\n\n. . .\n\n. . . . . . .\n. . .\n. . . . . . .\n. . . .\n\n. . . . . . . .\n. . . .\n\n. . . . .\n\n. . . . . . .\n\n186\n. . . . . . 186\n. . .\n. . 190\n. . . . . 194\n. . 198\n. . 204\n. . . . 208\n. . . . 212\n. . . . . 215\n. . . . . 215\n. .\n. . . . . . . . 216\n. . . . . . . . . . 217\n. . . . . . . . 218\n. . 219\n. . 222\n. . . . . 224\n. . 227\n. . . . 229\n\n. . . .\n. . . . . . . .\n\n. . . .\n. . . . . . .\n\n.\n\n.\n\n6\n\nidentically distributed uniform random variables\n\n.\n\n.\n\njointly distributed random variables\n6.1\n6.2\n. . . .\n6.3 sums of independent random variables .\n\njoint distribution functions . . . . . . . . . .\nindependent random variables .\n\n. . .\n. . . . .\n. .\n\n. . . .\n. . . .\n\n6.3.1\n6.3.2 gamma random variables\n6.3.3 normal random variables\n6.3.4 poisson and binomial random variables\n. . . . .\n6.3.5 geometric random variables . . .\n6.4 conditional distributions: discrete case .\n. .\n6.5 conditional distributions: continuous case .\n6.6 order statistics\n6.7\n6.8 exchangeable random variables .\n\n.\n.\n. . . . . . . .\n\n. . . . .\n. . . . .\n\n. . . . . .\n\n. . . . .\n\n. . . .\n\n. .\n\n. .\n\n. . . .\n\n. . .\n\n. . . .\n. . . .\n. .\n\n.\n.\n\n. . . . . . . .\nsummary . .\n. . .\nproblems . .\ntheoretical exercises .\n. . . .\nself-test problems and exercises .\n\n. . . . .\n. . . . . . .\n\n. . . . . . . .\n. . . . .\n. . . .\n\n. . . .\n. .\n\n. .\n\n. .\n\n. . . . .\n\n.\n\n. . .\n\n. . . . . . .\n\n. . . .\n\n. . . . . . . . .\n. . . . . . .\n. . . . .\n\n. . . . . . .\n. . . . . . .\n. . . . .\n. . . . . . .\n. . . . .\n. . . . .\n\n232\n. . 232\n. . . . 240\n. . . . 252\n. . 252\n. . . . 254\n. . . . 256\n. . . . 259\n. . . . 260\n. . . . 263\n. . . . 266\n. . . . . . . . . . . 270\n. . 274\n. . . . 282\n. . 285\n. .\n. . . . . . . . . 287\n. . . . 291\n. . . . 293\n\n. . . .\n. . . . . . .\n\njoint probability distribution of functions of random variables .\n\n. . . . . . .\n. . . . . . . . .\n\n "}, {"Page_number": 10, "text": "7 properties of expectation\n\nintroduction .\n\n7.1\n7.2 expectation of sums of random variables . .\n7.2.1 obtaining bounds from expectations\n\n. . . . . . .\n\n. . . . .\n\n. . . .\n\n.\n\n. . . .\n\n. . . . . . . .\n\n.\n\n. . . .\n\n. . . . .\n\nvia the probabilistic method . . . .\n\n. . . . .\n\n7.2.2 the maximum\u2013minimums identity . .\n\n. . . .\n.\n7.3 moments of the number of events that occur .\n. . . .\n7.4 covariance, variance of sums, and correlations . . . .\n7.5 conditional expectation .\n7.5.1 definitions . . . . .\n7.5.2 computing expectations by conditioning . . .\n7.5.3 computing probabilities by conditioning . . .\n7.5.4 conditional variance . . . .\n\n.\n. . . . . . .\n\n. . . . .\n. . .\n\n. . . . . . .\n. . . . . . .\n\n. . . . . . .\n\n. . . . .\n\n. . . .\n\n7.6 conditional expectation and prediction .\n7.7 moment generating functions . . .\n\n. . . . .\n\n. .\n\n.\n\n. . . .\n\n. . . . . . .\n\n7.7.1\n\njoint moment generating functions .\n\n.\n\n. . . . . . .\n. . . . .\n. . . . .\n. . . . .\n. . . .\n. . . . . .\n\n7.8 additional properties of normal random variables\n\n. . . .\n\n. . . . .\n. . . . .\n. . . .\n. . . . .\n. . . .\n. . . . .\n. .\n. . . . .\n\n.\n\n. . . .\n.\n. . . .\n\n7.8.1 the multivariate normal distribution .\n7.8.2 the joint distribution of the sample mean\n\nand sample variance . . . .\n7.9 general definition of expectation .\n\n. . . . . . .\n. . . .\n\n. . . . .\n\n.\n.\n\n. . . .\n. .\n\n. . . . . . . .\nsummary . .\n. . .\nproblems . .\ntheoretical exercises .\n. . . .\nself-test problems and exercises .\n\n. . . . .\n. . . . . . .\n\n. . . . . . . .\n. . . . .\n. . . .\n\n. .\n\n. .\n\n.\n\n. . . . .\n\n. . . . . . .\n\ncontents\n\nix\n\n297\n. . . 297\n. . . . 298\n\n. . . . 311\n. . . . 313\n. . . . 315\n. . . . 322\n. . . . 331\n. . 331\n. . . . 333\n. . . . 344\n. . . . 347\n. . . . 349\n. . . . 354\n. . . . 363\n. . 365\n. . . . 365\n\n. . . .\n\n. . . . . . .\n. . . . . . . . .\n\n. . . . . . .\n\n. . 367\n. . . . 369\n. . 370\n. .\n. . . . . . . . . 373\n. . . . 380\n. . . . 384\n\n. . . .\n. . . . . . .\n\n. . .\n\n388\n. . . . . . 388\n\n. . .\n\n8 limit theorems\n\nintroduction .\n\n8.1\n8.2 chebyshev\u2019s inequality and the weak law of large\n\n. . . . . . .\n\n. . . .\n\n. . . .\n\n. . . . . . .\n\n.\n\nnumbers . .\n\n.\n\n. . . .\n\n. . . . .\n\n. . . . . . . .\n\n. .\n\n8.3 the central limit theorem . . . . .\n. . .\n8.4 the strong law of large numbers . . . .\n8.5 other inequalities . . .\n. . . .\n8.6 bounding the error probability when approximating a sum of\n\n. . . . . . .\n. . . . . . .\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . . . .\n. . . . .\n. . . .\n. . .\n\n. .\n. . 388\n. . . . 391\n. . . . . 400\n. . . . . . 403\n\n. . . . .\n\nindependent bernoulli random variables by a poisson\nrandom variable . . .\nsummary . .\nproblems . .\ntheoretical exercises .\n. . . .\nself-test problems and exercises .\n\n. . . . . . .\n. . . .\n\n. . . . . . .\n. . .\n\n.\n. . . . .\n. . . . . .\n\n. . . . .\n. . . .\n\n. . . . . . . . .\n\n. . . .\n. . . . . . . .\n\n. . . . . . .\n\n. . . . .\n\n. .\n\n. .\n\n.\n.\n\n.\n\n. . . . . . .\n\n. . . .\n. . . . . . .\n\n. . .\n\n. . . . . . . .\n\n. . . 410\n. . . . 412\n. . . . . 412\n. . . . 414\n. . . . 415\n\n9 additional topics in probability\n. .\n.\n. . .\n\n. . . .\n9.1 the poisson process .\n9.2 markov chains .\n. . . . . . . .\n9.3 surprise, uncertainty, and entropy . . . .\n9.4 coding theory and entropy . . . .\n. . . .\n\n. . . . . . .\n. . .\n. . . . . . .\n. . . .\n. . .\n\n. . . . . . .\n. . . . . . .\n\n.\n. . . .\n\n.\n\n.\n\n. . . . . . .\n\nsummary . .\nproblems and theoretical exercises . . . .\nself-test problems and exercises .\n. . . .\nreferences .\n\n. . . . .\n\n. . . .\n\n. . . . . . . .\n\n.\n\n. . . . .\n. . . . .\n\n. . . . . . .\n. . . . . . .\n. . . . . . . . .\n\n. .\n\n. . . . . . .\n\n. . . .\n. . . . . . .\n\n. . . .\n. . . . . . .\n. . . . . . . .\n\n417\n. . 417\n. . . . . . 419\n. . . . . 425\n. . 428\n. . 434\n. . . . 435\n. . . . 436\n. .\n. . 436\n\n "}, {"Page_number": 11, "text": "x\n\ncontents\n\n10 simulation\n\n.\n\n.\n\n. . . .\n\n. . . .\n\n. . . .\n\n. . . . .\n\n. . . . . . .\n\n. . . . . . . .\n\n10.2.1 the inverse transformation method .\n10.2.2 the rejection method . . .\n\n10.3 simulating from discrete distributions .\n10.4 variance reduction techniques .\n\n438\n10.1 introduction .\n. . . 438\n10.2 general techniques for simulating continuous random variables . . 440\n. . . . 441\n. . . . 442\n. . . . 447\n. . . . 449\n. . . . 450\n. . . . 451\n. . . . 452\n. . . . 453\n. . . . . 453\n. . . . 455\n. . 455\n. .\n\n.\n. . . .\n10.4.1 use of antithetic variables . . . .\n10.4.2 variance reduction by conditioning .\n10.4.3 control variates\n.\nsummary . .\n. . . . .\nproblems . .\n. . . . . .\nself-test problems and exercises .\nreference .\n\n. . . . .\n. . . .\n. . . . .\n. . . . . . .\n. . . . . . .\n. . . . .\n. . . .\n. . .\n\n.\n. . . . . . .\n. . .\n\n. . . . . . .\n. . . . . . . . .\n\n. . . . . . .\n. . . . . . . .\n\n. .\n. . . . .\n. . . . .\n\n. . . . .\n. . . .\n\n. . . . . . . . .\n\n. . . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . .\n\n. . . .\n\n. . . .\n\n. . . .\n\n. . . .\n\n. .\n\n. .\n\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nanswers to selected problems\n\nsolutions to self-test problems and exercises\n\nindex\n\n457\n\n461\n\n521\n\n "}, {"Page_number": 12, "text": "preface\n\n\u201cwe see that the theory of probability is at bottom only common sense reduced\nto calculation; it makes us appreciate with exactitude what reasonable minds feel\nby a sort of instinct, often without being able to account for it. . . . it is remarkable\nthat this science, which originated in the consideration of games of chance, should\nhave become the most important object of human knowledge. . . . the most important\nquestions of life are, for the most part, really only problems of probability.\u201d so said\nthe famous french mathematician and astronomer (the \u201cnewton of france\u201d) pierre-\nsimon, marquis de laplace. although many people feel that the famous marquis,\nwho was also one of the great contributors to the development of probability, might\nhave exaggerated somewhat, it is nevertheless true that probability theory has become\na tool of fundamental importance to nearly all scientists, engineers, medical practi-\ntioners, jurists, and industrialists. in fact, the enlightened individual had learned to\nask not \u201cis it so?\u201d but rather \u201cwhat is the probability that it is so?\u201d\n\nthis book is intended as an elementary introduction to the theory of probability\nfor students in mathematics, statistics, engineering, and the sciences (including com-\nputer science, biology, the social sciences, and management science) who possess the\nprerequisite knowledge of elementary calculus. it attempts to present not only the\nmathematics of probability theory, but also, through numerous examples, the many\ndiverse possible applications of this subject.\n\nchapter 1 presents the basic principles of combinatorial analysis, which are most\n\nuseful in computing probabilities.\n\nchapter 2 handles the axioms of probability theory and shows how they can be\n\napplied to compute various probabilities of interest.\n\nchapter 3 deals with the extremely important subjects of conditional probability\nand independence of events. by a series of examples, we illustrate how conditional\nprobabilities come into play not only when some partial information is available,\nbut also as a tool to enable us to compute probabilities more easily, even when\nno partial information is present. this extremely important technique of obtaining\nprobabilities by \u201cconditioning\u201d reappears in chapter 7, where we use it to obtain\nexpectations.\n\nthe concept of random variables is introduced in chapters 4, 5, and 6. discrete\nrandom variables are dealt with in chapter 4, continuous random variables in\nchapter 5, and jointly distributed random variables in chapter 6. the important con-\ncepts of the expected value and the variance of a random variable are introduced in\nchapters 4 and 5, and these quantities are then determined for many of the common\ntypes of random variables.\n\nadditional properties of the expected value are considered in chapter 7. many\nexamples illustrating the usefulness of the result that the expected value of a sum of\nrandom variables is equal to the sum of their expected values are presented. sections\non conditional expectation, including its use in prediction, and on moment-generating\nfunctions are contained in this chapter. in addition, the final section introduces the\nmultivariate normal distribution and presents a simple proof concerning the joint\ndistribution of the sample mean and sample variance of a sample from a normal\ndistribution.\n\nxi\n\n "}, {"Page_number": 13, "text": "xii\n\npreface\n\nchapter 8 presents the major theoretical results of probability theory. in partic-\nular, we prove the strong law of large numbers and the central limit theorem. our\nproof of the strong law is a relatively simple one which assumes that the random vari-\nables have a finite fourth moment, and our proof of the central limit theorem assumes\nlevy\u2019s continuity theorem. this chapter also presents such probability inequalities as\nmarkov\u2019s inequality, chebyshev\u2019s inequality, and chernoff bounds. the final section\nof chapter 8 gives a bound on the error involved when a probability concerning\na sum of independent bernoulli random variables is approximated by the corre-\nsponding probability of a poisson random variable having the same expected\nvalue.\n\nchapter 9 presents some additional topics, such as markov chains, the poisson pro-\ncess, and an introduction to information and coding theory, and chapter 10 considers\nsimulation.\n\nas in the previous edition, three sets of exercises are given at the end of each\nchapter. they are designated as problems, theoretical exercises, and self-test prob-\nlems and exercises. this last set of exercises, for which complete solutions appear in\nsolutions to self-test problems and exercises, is designed to help students test their\ncomprehension and study for exams.\n\nchanges in the new edition\nthe eighth edition continues the evolution and fine tuning of the text. it includes\nnew problems, exercises, and text material chosen both for its inherent interest and\nfor its use in building student intuition about probability. illustrative of these goals\nare example 5d of chapter 1 on knockout tournaments, and examples 4k and 5i of\nchapter 7 on multiple player gambler\u2019s ruin problems.\n\na key change in the current edition is that the important result that the expectation\nof a sum of random variables is equal to the sum of the expectations is now first\npresented in chapter 4 (rather than chapter 7 as in previous editions). a new and\nelementary proof of this result when the sample space of the probability experiment\nis finite is given in this chapter.\n\nanother change is the expansion of section 6.3, which deals with the sum of inde-\npendent random variables. section 6.3.1 is a new section in which we derive the\ndistribution of the sum of independent and identically distributed uniform random\nvariables, and then use our results to show that the expected number of random num-\nbers that needs to be added for their sum to exceed 1 is equal to e. section 6.3.5 is a\nnew section in which we derive the distribution of the sum of independent geometric\nrandom variables with different means.\n\nacknowledgments\ni am grateful for the careful work of hossein hamedani in checking the text for accu-\nracy. i also appreciate the thoughtfulness of the following people that have taken the\ntime to contact me with comments for improving the text: amir ardestani, polytech-\nnic university of teheran; joe blitzstein, harvard university; peter nuesch, univer-\nsity of lausaunne; joseph mitchell, suny, stony brook; alan chambless, actuary;\nrobert kriner; israel david, ben-gurion university; t. lim, george mason univer-\nsity; wei chen, rutgers; d. monrad, university of illinois; w. rosenberger, george\nmason university; e. ionides, university of michigan; j. corvino, lafayette college;\nt. seppalainen, university of wisconsin.\n\nfinally, i would like to thank the following reviewers for their many helpful com-\n\nments. reviewers of the eighth edition are marked with an asterisk.\n\n "}, {"Page_number": 14, "text": "k. b. athreya, iowa state university\nrichard bass, university of connecticut\nrobert bauer, university of illinois at\n\nurbana-champaign\n\nphillip beckwith, michigan tech\narthur benjamin, harvey mudd college\ngeoffrey berresford, long island university\nbaidurya bhattacharya, university of delaware\nhoward bird, st. cloud state university\nshahar boneh, metropolitan state college of\n\njean cadet, state university of new york at\n\nsteven chiappari, santa clara university\nnicolas christou, university of california, los\n\ndenver\n\nstony brook\n\nangeles\n\njames clay, university of arizona at tucson\nfrancis conlan, university of santa clara\n*justin corvino, lafayette college\njay devore, california polytechnic university,\n\nsan luis obispo\n\nscott emerson, university of washington\nthomas r. fischer, texas a & m university\nanant godbole, michigan technical\n\nuniversity\n\nzakkula govindarajulu, university of kentucky\nrichard groeneveld, iowa state university\nmike hardy, massachusetts institute of\n\ntechnology\n\nbernard harris, university of wisconsin\nlarry harris, university of kentucky\ndavid heath, cornell university\nstephen herschkorn, rutgers university\njulia l. higle, university of arizona\nmark huber, duke university\n\nacknowledgments xiii\n\n*edward ionides, university of michigan\nanastasia ivanova, university of north carolina\nhamid jafarkhani, university of california,\n\nirvine\n\nchapel hill\n\nchuanshu ji, university of north carolina,\n\nrobert keener, university of michigan\nfred leysieffer, florida state university\nthomas liggett, university of california, los\n\nangeles\n\nhelmut mayer, university of georgia\nbill mccormick, university of georgia\nian mckeague, florida state university\nr. miller, stanford university\n*ditlev monrad, university of illinois\nrobb j. muirhead, university of michigan\njoe naus, rutgers university\nnhu nguyen, new mexico state university\nellen o\u2019brien, george mason university\nn. u. prabhu, cornell university\nkathryn prewitt, arizona state university\njim propp, university of wisconsin\n*william f. rosenberger, george mason\n\nmyra samuels, purdue university\ni. r. savage, yale university\nart schwartz, university of michigan at ann\n\ntherese shelton, southwestern university\nmalcolm sherman, state university of new york\n\nuniversity\n\narbor\n\nat albany\n\nmurad taqqu, boston university\neli upfal, brown university\ned wheeler, university of tennessee\nallen webster, bradley university\n\ns. r.\nsmross@usc.edu\n\n "}, {"Page_number": 15, "text": "this page intentionally left blank \n\n "}, {"Page_number": 16, "text": "c h a p t e r\n\n1\n\ncombinatorial analysis\n\n1.1 introduction\n1.2 the basic principle of counting\n1.3 permutations\n1.4 combinations\n1.5 multinomial coefficients\n1.6 the number of integer solutions of equations\n\n1.1 introduction\n\nhere is a typical problem of interest involving probability: a communication system\nis to consist of n seemingly identical antennas that are to be lined up in a linear order.\nthe resulting system will then be able to receive all incoming signals\u2014and will be\ncalled functional\u2014as long as no two consecutive antennas are defective. if it turns\nout that exactly m of the n antennas are defective, what is the probability that the\nresulting system will be functional? for instance, in the special case where n = 4 and\nm = 2, there are 6 possible system configurations, namely,\n\n0 1 1 0\n0 1 0 1\n1 0 1 0\n0 0 1 1\n1 0 0 1\n1 1 0 0\n\nwhere 1 means that the antenna is working and 0 that it is defective. because the\nresulting system will be functional in the first 3 arrangements and not functional in\nthe remaining 3, it seems reasonable to take 3\n2 as the desired probability. in\n6\nthe case of general n and m, we could compute the probability that the system is\nfunctional in a similar fashion. that is, we could count the number of configurations\nthat result in the system\u2019s being functional and then divide by the total number of all\npossible configurations.\n\n= 1\n\nfrom the preceding discussion, we see that it would be useful to have an effective\nmethod for counting the number of ways that things can occur. in fact, many prob-\nlems in probability theory can be solved simply by counting the number of different\nways that a certain event can occur. the mathematical theory of counting is formally\nknown as combinatorial analysis.\n\n1.2 the basic principle of counting\n\nthe basic principle of counting will be fundamental to all our work. loosely put, it\nstates that if one experiment can result in any of m possible outcomes and if another\nexperiment can result in any of n possible outcomes, then there are mn possible out-\ncomes of the two experiments.\n\n1\n\n "}, {"Page_number": 17, "text": "2\n\nchapter 1\n\ncombinatorial analysis\n\nthe basic principle of counting\n\nsuppose that two experiments are to be performed. then if experiment 1 can result\nin any one of m possible outcomes and if, for each outcome of experiment 1, there\nare n possible outcomes of experiment 2, then together there are mn possible out-\ncomes of the two experiments.\n\nproof of the basic principle: the basic principle may be proven by enumerating all\nthe possible outcomes of the two experiments; that is,\n\n(1, 1), (1, 2),\n(2, 1), (2, 2),\n\n. . . , (1, n)\n. . . , (2, n)\n\n#\n#\n#\n\n(m, 1), (m, 2), . . . , (m, n)\n\nwhere we say that the outcome is (i, j) if experiment 1 results in its ith possible out-\ncome and experiment 2 then results in its jth possible outcome. hence, the set of\npossible outcomes consists of m rows, each containing n elements. this proves the\nresult.\n\nexample 2a\na small community consists of 10 women, each of whom has 3 children. if one woman\nand one of her children are to be chosen as mother and child of the year, how many\ndifferent choices are possible?\n\nsolution. by regarding the choice of the woman as the outcome of the first experi-\nment and the subsequent choice of one of her children as the outcome of the second\nexperiment, we see from the basic principle that there are 10 * 3 = 30 possible\n.\nchoices.\n\nwhen there are more than two experiments to be performed, the basic principle\n\ncan be generalized.\n\nthe generalized basic principle of counting\n\nif r experiments that are to be performed are such that the first one may result in\nany of n1 possible outcomes; and if, for each of these n1 possible outcomes, there\nare n2 possible outcomes of the second experiment; and if, for each of the possible\noutcomes of the first two experiments, there are n3 possible outcomes of the third\nexperiment; and if ..., then there is a total of n1 \u00b7 n2 \u00b7\u00b7\u00b7 nr possible outcomes of the\nr experiments.\n\nexample 2b\na college planning committee consists of 3 freshmen, 4 sophomores, 5 juniors, and 2\nseniors. a subcommittee of 4, consisting of 1 person from each class, is to be chosen.\nhow many different subcommittees are possible?\n\n "}, {"Page_number": 18, "text": "section 1.3\n\npermutations 3\n\nsolution. we may regard the choice of a subcommittee as the combined outcome of\nthe four separate experiments of choosing a single representative from each of the\nclasses. it then follows from the generalized version of the basic principle that there\nare 3 * 4 * 5 * 2 = 120 possible subcommittees.\n.\n\nexample 2c\nhow many different 7-place license plates are possible if the first 3 places are to be\noccupied by letters and the final 4 by numbers?\nsolution. by the generalized version of the basic principle, the answer is 26 \u00b7 26 \u00b7\n26 \u00b7 10 \u00b7 10 \u00b7 10 \u00b7 10 = 175,760,000.\n.\n\nexample 2d\nhow many functions defined on n points are possible if each functional value is either\n0 or 1?\nsolution. let the points be 1, 2, . . . , n. since f (i) must be either 0 or 1 for each i =\n.\n1, 2, . . . , n, it follows that there are 2n possible functions.\n\nexample 2e\nin example 2c, how many license plates would be possible if repetition among letters\nor numbers were prohibited?\nsolution. in this case, there would be 26 \u00b7 25 \u00b7 24 \u00b7 10 \u00b7 9 \u00b7 8 \u00b7 7 = 78,624,000\n.\npossible license plates.\n\n1.3 permutations\n\nhow many different ordered arrangements of the letters a, b, and c are possible? by\ndirect enumeration we see that there are 6, namely, abc, acb, bac, bca, cab, and cba.\neach arrangement is known as a permutation. thus, there are 6 possible permutations\nof a set of 3 objects. this result could also have been obtained from the basic principle,\nsince the first object in the permutation can be any of the 3, the second object in the\npermutation can then be chosen from any of the remaining 2, and the third object\nin the permutation is then the remaining 1. thus, there are 3 \u00b7 2 \u00b7 1 = 6 possible\npermutations.\n\nsuppose now that we have n objects. reasoning similar to that we have just used\n\nfor the 3 letters then shows that there are\n\nn(n \u2212 1)(n \u2212 2)\u00b7\u00b7\u00b7 3 \u00b7 2 \u00b7 1 = n!\n\ndifferent permutations of the n objects.\n\nexample 3a\nhow many different batting orders are possible for a baseball team consisting of 9\nplayers?\nsolution. there are 9! = 362,880 possible batting orders.\n\n.\n\n "}, {"Page_number": 19, "text": "4\n\nchapter 1\n\ncombinatorial analysis\n\nexample 3b\na class in probability theory consists of 6 men and 4 women. an examination is given,\nand the students are ranked according to their performance. assume that no two\nstudents obtain the same score.\n(a) how many different rankings are possible?\n(b) if the men are ranked just among themselves and the women just among them-\n\nselves, how many different rankings are possible?\n\nsolution. (a) because each ranking corresponds to a particular ordered arrangement\nof the 10 people, the answer to this part is 10! = 3,628,800.\n\n(b) since there are 6! possible rankings of the men among themselves and 4! possi-\nble rankings of the women among themselves, it follows from the basic principle that\nthere are (6!)(4!) = (720)(24) = 17,280 possible rankings in this case.\n.\n\nexample 3c\nms. jones has 10 books that she is going to put on her bookshelf. of these, 4 are\nmathematics books, 3 are chemistry books, 2 are history books, and 1 is a language\nbook. ms. jones wants to arrange her books so that all the books dealing with the\nsame subject are together on the shelf. how many different arrangements are\npossible?\n\nsolution. there are 4! 3! 2! 1! arrangements such that the mathematics books are\nfirst in line, then the chemistry books, then the history books, and then the language\nbook. similarly, for each possible ordering of the subjects, there are 4! 3! 2! 1! possible\narrangements. hence, as there are 4! possible orderings of the subjects, the desired\nanswer is 4! 4! 3! 2! 1! = 6912.\n.\n\nwe shall now determine the number of permutations of a set of n objects when cer-\ntain of the objects are indistinguishable from each other. to set this situation straight\nin our minds, consider the following example.\n\nexample 3d\nhow many different letter arrangements can be formed from the letters pepper?\n\nsolution. we first note that there are 6! permutations of the letters p1e1p2p3e2r\nwhen the 3p\u2019s and the 2e\u2019s are distinguished from each other. however, consider\nany one of these permutations\u2014for instance, p1p2e1p3e2r. if we now permute the\np\u2019s among themselves and the e\u2019s among themselves, then the resultant arrangement\nwould still be of the form ppeper. that is, all 3! 2! permutations\n\np1p2e1p3e2r p1p2e2p3e1r\np1p3e1p2e2r p1p3e2p2e1r\np2p1e1p3e2r p2p1e2p3e1r\np2p3e1p1e2r p2p3e2p1e1r\np3p1e1p2e2r p3p1e2p2e1r\np3p2e1p1e2r p3p2e2p1e1r\n\nare of the form ppeper. hence, there are 6!/(3! 2!) = 60 possible letter arrange-\n.\nments of the letters pepper.\n\n "}, {"Page_number": 20, "text": "in general, the same reasoning as that used in example 3d shows that there are\n\nsection 1.4\n\ncombinations 5\n\nn!\n\nn1! n2! \u00b7\u00b7\u00b7 nr!\n\ndifferent permutations of n objects, of which n1 are alike, n2 are alike, . . . , nr are\nalike.\n\nexample 3e\na chess tournament has 10 competitors, of which 4 are russian, 3 are from the united\nstates, 2 are from great britain, and 1 is from brazil. if the tournament result lists just\nthe nationalities of the players in the order in which they placed, how many outcomes\nare possible?\n\nsolution. there are\n\npossible outcomes.\n\n10!\n\n4! 3! 2! 1!\n\n= 12,600\n\n.\n\nexample 3f\nhow many different signals, each consisting of 9 flags hung in a line, can be made\nfrom a set of 4 white flags, 3 red flags, and 2 blue flags if all flags of the same color are\nidentical?\n\nsolution. there are\n\ndifferent signals.\n\n9!\n\n4! 3! 2!\n\n= 1260\n\n.\n\n1.4 combinations\n\nwe are often interested in determining the number of different groups of r objects\nthat could be formed from a total of n objects. for instance, how many different\ngroups of 3 could be selected from the 5 items a, b, c, d, and e? to answer this\nquestion, reason as follows: since there are 5 ways to select the initial item, 4 ways to\nthen select the next item, and 3 ways to select the final item, there are thus 5 \u00b7 4 \u00b7 3\nways of selecting the group of 3 when the order in which the items are selected is\nrelevant. however, since every group of 3\u2014say, the group consisting of items a, b,\nand c\u2014will be counted 6 times (that is, all of the permutations abc, acb, bac,\nbca, cab, and cba will be counted when the order of selection is relevant), it\nfollows that the total number of groups that can be formed is\n\n5 \u00b7 4 \u00b7 3\n3 \u00b7 2 \u00b7 1\n\n= 10\n\nin general, as n(n \u2212 1)\u00b7\u00b7\u00b7 (n \u2212 r + 1) represents the number of different ways that a\ngroup of r items could be selected from n items when the order of selection is relevant,\nand as each group of r items will be counted r! times in this count, it follows that the\nnumber of different groups of r items that could be formed from a set of n items is\n\nn(n \u2212 1)\u00b7\u00b7\u00b7 (n \u2212 r + 1)\n\nr!\n\n=\n\nn!\n\n(n \u2212 r)! r!\n\n "}, {"Page_number": 21, "text": "6\n\nchapter 1\n\ncombinatorial analysis\n\nnotation and terminology\n\nwe define\n\n(cid:2)\n\n(cid:3)\n, for r \u2026 n, by(cid:2)\n(cid:3)\n\nn\nr\n\n(cid:2)\n\n(cid:3)\n\nn\nr\n\n=\n\nn!\n\n(n \u2212 r)! r!\n\nn\nand say that\nr\ntaken r at a time.\u2020\n\n(cid:2)\n\n(cid:3)\n\nn\nr\n\nrepresents the number of possible combinations of n objects\n\n(cid:2)\n\n(cid:3)\n\n20\n3\n\n(cid:2)\n\n(cid:3)\n\n5\n2\n\nthus,\n\nrepresents the number of different groups of size r that could be\nselected from a set of n objects when the order of selection is not considered relevant.\n\nexample 4a\na committee of 3 is to be formed from a group of 20 people. how many different\ncommittees are possible?\n\nsolution. there are\n\n= 20 \u00b7 19 \u00b7 18\n3 \u00b7 2 \u00b7 1\n\n= 1140 possible committees.\n\n.\n\nexample 4b\nfrom a group of 5 women and 7 men, how many different committees consisting of\n2 women and 3 men can be formed? what if 2 of the men are feuding and refuse to\nserve on the committee together?\n\n(cid:2)\n(cid:2)\n\n(cid:3)\n(cid:3) (cid:2)\n\n7\n3\n5\n2\n\n(cid:3)\n\npossible\n=\n\nsolution. as there are\n\npossible groups of 2 women, and\n\n7 \u00b7 6 \u00b7 5\n(cid:3)\n3 \u00b7 2 \u00b7 1\n\n7\n3\n= 350 possible committees consisting of 2 women and 3 men.\n\n(cid:2)\n(cid:3)\ngroups of 3 men, it follows from the basic principle that there are\n5 \u00b7 4\n(cid:3)(cid:2)\n(cid:2)\n2 \u00b7 1\nnow suppose that 2 of the men refuse to serve together. because a total of\n= 35 possible groups of 3 men contain both of\n2\n2\nthe feuding men, it follows that there are 35 \u2212 5 = 30 groups that do not contain\n= 10 ways to choose the 2\nboth of the feuding men. because there are still\nwomen, there are 30 \u00b7 10 = 300 possible committees in this case.\n.\n\n= 5 out of the\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\n5\n1\n\n7\n3\n\n5\n2\n\n\u2020by convention, 0! is defined to be 1. thus,\n\nto 0 when either i < 0 or i > n.\n\n(cid:4)\n\n(cid:5)\n\nn\n0\n\n=\n\n(cid:4)\n\n(cid:5)\n\nn\nn\n\n= 1. we also take\n\n(cid:4)\n\n(cid:5)\n\nn\ni\n\nto be equal\n\n "}, {"Page_number": 22, "text": "section 1.4\n\ncombinations 7\n\nexample 4c\nconsider a set of n antennas of which m are defective and n \u2212 m are functional\nand assume that all of the defectives and all of the functionals are considered indis-\ntinguishable. how many linear orderings are there in which no two defectives are\nconsecutive?\nsolution. imagine that the n \u2212 m functional antennas are lined up among them-\nselves. now, if no two defectives are to be consecutive, then the spaces between the\nfunctional antennas must each contain at most one defective antenna. that is, in the\nn \u2212 m + 1 possible positions\u2014represented in figure 1.1 by carets\u2014between the\nn \u2212 m functional antennas, we must select m of these in which to put the defective\npossible orderings in which there is at\nantennas. hence, there are\n.\nleast one functional antenna between any two defective ones.\n\nn \u2212 m + 1\n\n(cid:2)\n\n(cid:3)\n\nm\n\n^ 1 ^ 1 ^ 1 . . . ^ 1 ^ 1 ^\n\n1 \u2afd functional\n\n^ \u2afd place for at most one defective\n\n1 \u2026 r \u2026 n\n\n(4.1)\n\nfigure 1.1: no consecutive defectives\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\na useful combinatorial identity is\nn \u2212 1\nr \u2212 1\n\n=\n\nn\nr\n\n(cid:3)\n\n(cid:3)\n\n(cid:2)\n\n+\n(cid:2)\n\nr\n\nn \u2212 1\n(cid:3)\n\nequation (4.1) may be proved analytically or by the following combinatorial argu-\nment: consider a group of n objects, and fix attention on some particular one of these\ngroups of size r that contain object\nobjects\u2014call it object 1. now, there are\n1 (since each such group is formed by selecting r \u2212 1 from the remaining n \u2212 1\ngroups of size r that do not contain object 1. as\nobjects). also, there are\n\nn \u2212 1\nr \u2212 1\n\nn \u2212 1\n\n(cid:2)\n\n(cid:3)\n\nr\n\nthere is a total of\n\nthe values\n\ngroups of size r, equation (4.1) follows.\n\nare often referred to as binomial coefficients because of their\n\n(cid:3)\n\n(cid:2)\n(cid:3)\n\nn\nr\n\n(cid:2)\n\nn\nr\n\nprominence in the binomial theorem.\n\nthe binomial theorem\n\n(x + y)n = n(cid:6)\n\nk=0\n\n(cid:3)\n\n(cid:2)\n\nn\nk\n\nxkyn\u2212k\n\n(4.2)\n\nwe shall present two proofs of the binomial theorem. the first is a proof by math-\nematical induction, and the second is a proof based on combinatorial considerations.\n\n "}, {"Page_number": 23, "text": "8\n\nchapter 1\n\ncombinatorial analysis\n\nproof of the binomial theorem by induction: when n = 1, equation (4.2) reduces to\n\n(cid:2)\n\n(cid:3)\n\n1\n0\n\n(cid:2)\n\n(cid:3)\n\n1\n1\n\nx + y =\n\nx0y1 +\n\nx1y0 = y + x\n\nletting i = k + 1 in the first sum and i = k in the second sum, we find that\n\nassume equation (4.2) for n \u2212 1. now,\n(cid:2)\n(cid:3)\n(x + y)n = (x + y)(x + y)n\u22121\nn \u2212 1\n(cid:3)\n\n= (x + y)\n(cid:2)\n\nn\u22121(cid:6)\n\nk\n\nxkyn\u22121\u2212k\n\nk=0\n\nxk+1yn\u22121\u2212k + n\u22121(cid:6)\n(cid:2)\n(cid:3)\nxiyn\u2212i + n\u22121(cid:6)\n(cid:2)\n\n(cid:3)\n\ni=0\n+\n\n(cid:3)\nn \u2212 1\n(cid:3)(cid:8)\n\ni\nn \u2212 1\n\ni\n\nn \u2212 1\ni \u2212 1\n(cid:3)\n\nxiyn\u2212i + yn\n\nk=0\nn \u2212 1\n\nk\n\n= n\u22121(cid:6)\n(x + y)n = n(cid:6)\n\nk=0\n\nn \u2212 1\n(cid:7)(cid:2)\ni \u2212 1\n(cid:2)\n\ni=1\n\ni=1\n\n(cid:2)\n= xn + n\u22121(cid:6)\n= xn + n\u2212i(cid:6)\n(cid:3)\n(cid:2)\n= n(cid:6)\n\ni=1\nn\ni\n\nn\ni\n\ni=0\n\nxiyn\u2212i\n\n(cid:3)\n\n(cid:2)\n\nn \u2212 1\n\nk\n\nxkyn\u2212k\n\nxiyn\u2212i\n\nxiyn\u2212i + yn\n\nwhere the next-to-last equality follows by equation (4.1). by induction, the theorem\nis now proved.\n\ncombinatorial proof of the binomial theorem: consider the product\n\n(x1 + y1)(x2 + y2)\u00b7\u00b7\u00b7 (xn + yn)\n\nits expansion consists of the sum of 2n terms, each term being the product of n factors.\nfurthermore, each of the 2n terms in the sum will contain as a factor either xi or yi\nfor each i = 1, 2, . . . , n. for example,\n\n(x1 + y1)(x2 + y2) = x1x2 + x1y2 + y1x2 + y1y2\n(cid:2)\n\nnow, how many of the 2n terms in the sum will have k of the xi\u2019s and (n \u2212 k) of the yi\u2019s\nas factors? as each term consisting of k of the xi\u2019s and (n \u2212 k) of the yi\u2019s corresponds\nto a choice of a group of k from the n values x1, x2, . . . , xn, there are\nsuch terms.\n(cid:3)\nthus, letting xi = x, yi = y, i = 1, . . . , n, we see that\n\n(cid:3)\n\n(cid:2)\n\nn\nk\n\n(x + y)n = n(cid:6)\n\nk=0\n\nxkyn\u2212k\n\nn\nk\n\n "}, {"Page_number": 24, "text": "section 1.5\n\nmultinomial coefficients 9\n\nexample 4d\nexpand (x + y)3.\nsolution.\n\n(x + y)3 =\n\n(cid:2)\n\n(cid:2)\n\n(cid:3)\n\nx0y3 +\n\n(cid:3)\n\nx1y2 +\n\n3\n1\n\n3\n0\n\n= y3 + 3xy2 + 3x2y + x3\n\n(cid:2)\n\n(cid:3)\n\n3\n2\n\nx2y +\n\n(cid:3)\n\n(cid:2)\n\n3\n3\n\nx3y0\n\n.\n\nexample 4e\nhow many subsets are there of a set consisting of n elements?\n\nsolution. since there are\n\nsubsets of size k, the desired answer is\n\n(cid:2)\n\n(cid:3)\nn(cid:6)\n\nn\nk\n\nk=0\n\n(cid:2)\n\n(cid:3)\n\nn\nk\n\n= (1 + 1)n = 2n\n\nthis result could also have been obtained by assigning either the number 0 or the\nnumber 1 to each element in the set. to each assignment of numbers, there corre-\nsponds, in a one-to-one fashion, a subset, namely, that subset consisting of all ele-\nments that were assigned the value 1. as there are 2n possible assignments, the result\nfollows.\n\nnote that we have included the set consisting of 0 elements (that is, the null set)\nas a subset of the original set. hence, the number of subsets that contain at least one\nelement is 2n \u2212 1.\n.\n\n1.5 multinomial coefficients\n\n(cid:9)\n\nin this section, we consider the following problem: a set of n distinct items is to be\ni=1 ni = n.\ndivided into r distinct groups of respective sizes n1, n2, . . . , nr, where\nhow many different divisions are possible? to answer this question, we note that\nthere are\npossible choices for the first group; for each choice of the first group,\n\nr\n\nthere are\n\npossible choices for the second group; for each choice of the\n\n(cid:2)\n(cid:2)\n\n(cid:3)\nn\nn1\nn \u2212 n1\n\nn2\n\n(cid:3)\n\nfirst two groups, there are\npossible choices for the third group; and\nso on. it then follows from the generalized version of the basic counting principle that\n\nn3\n\n(cid:3)\n\nn \u2212 n1 \u2212 n2\n\n(cid:2)\n\n(cid:2)\n\n(cid:3)\n\n\u00b7\u00b7\u00b7\n\nn \u2212 n1 \u2212 n2 \u2212 \u00b7\u00b7\u00b7 \u2212 nr\u22121\n(n \u2212 n1)!\n\nnr\n(n \u2212 n1 \u2212 n2)! n2!\n\n\u00b7\u00b7\u00b7 (n \u2212 n1 \u2212 n2 \u2212 \u00b7\u00b7\u00b7 \u2212 nr\u22121)!\n\n0! nr!\n\n(cid:3)(cid:2)\nthere are(cid:2)\n\n(cid:3)\n\nn\nn1\n=\n=\n\nn \u2212 n1\n\nn2\nn!\n\n(n \u2212 n1)! n1!\nn1! n2!\u00b7\u00b7\u00b7 nr!\n\nn!\n\npossible divisions.\n\n "}, {"Page_number": 25, "text": "10\n\nchapter 1\n\ncombinatorial analysis\n\nanother way to see this result is to consider the n values 1, 1, . . . , 1, 2, . . . , 2, . . . ,\nr, . . . , r, where i appears ni times, for i = 1, . . . , r. every permutation of these values\ncorresponds to a division of the n items into the r groups in the following manner:\nlet the permutation i1, i2, . . . , in correspond to assigning item 1 to group i1, item 2 to\ngroup i2, and so on. for instance, if n = 8 and if n1 = 4, n2 = 3, and n3 = 1, then\nthe permutation 1, 1, 2, 3, 2, 1, 2, 1 corresponds to assigning items 1, 2, 6, 8 to the first\ngroup, items 3, 5, 7 to the second group, and item 4 to the third group. because every\npermutation yields a division of the items and every possible division results from\nsome permutation, it follows that the number of divisions of n items into r distinct\ngroups of sizes n1, n2, . . . , nr is the same as the number of permutations of n items of\nwhich n1 are alike, and n2 are alike, . . ., and nr are alike, which was shown in section\n1.3 to equal\n\nn!\n\n.\n\nn1!n2!\u00b7\u00b7\u00b7 nr!\n\nnotation\n\nif n1 + n2 + \u00b7\u00b7\u00b7 + nr = n, we define\n(cid:3)\n\n(cid:2)\n\nn\n\nn1, n2, . . . , nr\n\n(cid:2)\n\nn\n\n(cid:3)\n\n(cid:2)\n\nn\n\nn1, n2, . . . , nr\n\n(cid:3)\n\nby\n\n=\n\nn!\n\nn1! n2!\u00b7\u00b7\u00b7 nr!\n\nthus,\nobjects into r distinct groups of respective sizes n1, n2, . . . , nr.\n\nn1, n2, . . . , nr\n\nrepresents the number of possible divisions of n distinct\n\nexample 5a\na police department in a small city consists of 10 officers. if the department policy is\nto have 5 of the officers patrolling the streets, 2 of the officers working full time at the\nstation, and 3 of the officers on reserve at the station, how many different divisions of\nthe 10 officers into the 3 groups are possible?\n\nsolution. there are\n\n10!\n\n5! 2! 3!\n\n= 2520 possible divisions.\n\n.\n\nexample 5b\nten children are to be divided into an a team and a b team of 5 each. the a team\nwill play in one league and the b team in another. how many different divisions are\npossible?\n\nsolution. there are\n\n= 252 possible divisions.\n\n10!\n5! 5!\n\n.\n\nexample 5c\nin order to play a game of basketball, 10 children at a playground divide themselves\ninto two teams of 5 each. how many different divisions are possible?\n\n "}, {"Page_number": 26, "text": "section 1.5\n\nmultinomial coefficients 11\n\nsolution. note that this example is different from example 5b because now the order\nof the two teams is irrelevant. that is, there is no a and b team, but just a division\nconsisting of 2 groups of 5 each. hence, the desired answer is\n\n10!/(5! 5!)\n\n2!\n\n= 126\n\n.\n\nthe proof of the following theorem, which generalizes the binomial theorem, is\n\nleft as an exercise.\n\nthe multinomial theorem\n\n(x1 + x2 + \u00b7\u00b7\u00b7 + xr)n =\n\n(cid:6)\n\n(cid:2)\n\n(n1, . . . , nr) :\n\nn1 + \u00b7\u00b7\u00b7 + nr = n\n\n(cid:3)\n\nxn1\n1 xn2\n2\n\n\u00b7\u00b7\u00b7 xnr\n\nr\n\nn\n\nn1, n2, . . . , nr\n\nthat is, the sum is over all nonnegative integer-valued vectors (n1, n2, . . . , nr) such\nthat n1 + n2 + \u00b7\u00b7\u00b7 + nr = n.\n\n(cid:2)\n\n(cid:3)\n\nn\n\nn1, n2, . . . , nr\n\nthe numbers\n\nare known as multinomial coefficients.\n\nexample 5d\nin the first round of a knockout tournament involving n = 2m players, the n players\nare divided into n/2 pairs, with each of these pairs then playing a game. the losers\nof the games are eliminated while the winners go on to the next round, where the\nprocess is repeated until only a single player remains. suppose we have a knockout\ntournament of 8 players.\n\n(a) how many possible outcomes are there for the initial round? (for instance, one\n\noutcome is that 1 beats 2, 3 beats 4, 5 beats 6, and 7 beats 8. )\n\n(b) how many outcomes of the tournament are possible, where an outcome gives\n\ncomplete information for all rounds?\n\n(cid:2)\n\nsolution. one way to determine the number of possible outcomes for the initial\nround is to first determine the number of possible pairings for that round. to do\nso, note that the number of ways to divide the 8 players into a first pair, a second pair,\n= 8!\n. thus, the number of possible pair-\na third pair, and a fourth pair is\n24\n8!\n24 4!\n\nings when there is no ordering of the 4 pairs is\n. for each such pairing, there are\n2 possible choices from each pair as to the winner of that game, showing that there\n\n2, 2, 2, 2\n\n(cid:3)\n\n8\n\npossible results of round 1. (another way to see this is to note that\n\npossible choices of the 4 winners and, for each such choice, there are\n= 8!\n4!\n\n4! ways to pair the 4 winners with the 4 losers, showing that there are 4!\npossible results for the first round.)\n\n8\n4\n\nare\n\n8!24\n24 4!\nthere are\n\n(cid:2)\n(cid:3)\n= 8!\n4!\n8\n4\n\n(cid:2)\n\n(cid:3)\n\n "}, {"Page_number": 27, "text": "12\n\nchapter 1\n\ncombinatorial analysis\n\nsimilarly, for each result of round 1, there are\n\n4!\n2!\n\npossible outcomes of round 2,\n\n2!\n1!\n\npossible outcomes\nand for each of the outcomes of the first two rounds, there are\nof round 3. consequently, by the generalized basic principle of counting, there are\n= 8! possible outcomes of the tournament. indeed, the same argument\n8!\ncan be used to show that a knockout tournament of n = 2m players has n! possible\n4!\noutcomes.\n\n2!\n1!\n\n4!\n2!\n\nknowing the preceding result, it is not difficult to come up with a more direct\nargument by showing that there is a one-to-one correspondence between the set of\npossible tournament results and the set of permutations of 1, . . . , n. to obtain such\na correspondence, rank the players as follows for any tournament result: give the\ntournament winner rank 1, and give the final-round loser rank 2. for the two play-\ners who lost in the next-to-last round, give rank 3 to the one who lost to the player\nranked 1 and give rank 4 to the one who lost to the player ranked 2. for the four\nplayers who lost in the second-to-last round, give rank 5 to the one who lost to player\nranked 1, rank 6 to the one who lost to the player ranked 2, rank 7 to the one who\nlost to the player ranked 3, and rank 8 to the one who lost to the player ranked 4.\ncontinuing on in this manner gives a rank to each player. (a more succinct descrip-\ntion is to give the winner of the tournament rank 1 and let the rank of a player who\nlost in a round having 2k matches be 2k plus the rank of the player who beat him, for\nk = 0, . . . , m \u2212 1.) in this manner, the result of the tournament can be represented\nby a permutation i1, i2, . . . , in, where ij is the player who was given rank j. because\ndifferent tournament results give rise to different permutations, and because there is\na tournament result for each permutation, it follows that there are the same number\n.\nof possible tournament results as there are permutations of 1, . . . , n.\n\nexample 5e\n\n(x1 + x2 + x3)2 =\n\n(cid:3)\n\n(cid:2)\n\n+\n\n+\n\nx2\n1x0\n\n2x0\n3\n\n+\n\nx0\n1x0\n\n2x2\n3\n\n0, 2, 0\n2\n\n2\n\n(cid:2)\n(cid:2)\n\n2\n\n1, 1, 0\n\n2x0\n3\n\n(cid:3)\n1x2\nx0\n(cid:3)\n\nx1\n1x1\n\n2x0\n3\n\n(cid:2)\n\n(cid:3)\n\n(cid:3)\n(cid:3)\n\n2, 0, 0\n2\n\n+\n\n2\n\n(cid:2)\n(cid:2)\n\n0, 0, 2\n\n2\n1, 0, 1\n+ x2\n\n+\n= x2\n\nx1\n2x1\n1x0\n2x1\n3\n3\n+ 2x1x2 + 2x1x3 + 2x2x3\n+ x2\n\u22171.6 the number of integer solutions of equations\n\nx0\n1x1\n\n0, 1, 1\n\n1\n\n2\n\n3\n\n.\n\nthere are rn possible outcomes when n distinguishable balls are to be distributed into\nr distinguishable urns. this result follows because each ball may be distributed into\nany of r possible urns. let us now, however, suppose that the n balls are indistinguish-\nable from each other. in this case, how many different outcomes are possible? as the\nballs are indistinguishable, it follows that the outcome of the experiment of distribut-\ning the n balls into r urns can be described by a vector (x1, x2, . . . , xr), where xi denotes\nthe number of balls that are distributed into the ith urn. hence, the problem reduces\nto finding the number of distinct nonnegative integer-valued vectors (x1, x2, . . . , xr)\nsuch that\n\nx1 + x2 + \u00b7\u00b7\u00b7 + xr = n\n\n\u2217\n\nasterisks denote material that is optional.\n\n "}, {"Page_number": 28, "text": "section 1.6\n\nthe number of integer solutions of equations 13\n\nto compute this number, let us start by considering the number of positive integer-\nvalued solutions. toward that end, imagine that we have n indistinguishable objects\nlined up and that we want to divide them into r nonempty groups. to do so, we can\nselect r \u2212 1 of the n \u2212 1 spaces between adjacent objects as our dividing points. (see\nfigure 1.2.) for instance, if we have n = 8 and r = 3 and we choose the 2 divisors so\nas to obtain\n\nooo|ooo|oo\n\n0 ^ 0 ^ 0 ^ . . . ^ 0 ^ 0\n\nn objects 0\n\nchoose r \u2afa 1 of the spaces ^.\n\nfigure 1.2: number of positive solutions\n\n(cid:2)\n\n(cid:3)\n\nn \u2212 1\nr \u2212 1\n\npossible\n\nthen the resulting vector is x1 = 3, x2 = 3, x3 = 2. as there are\nselections, we have the following proposition.\n\n(cid:3)\n\n(cid:2)\n\nn \u2212 1\nr \u2212 1\n\nproposition 6.1. there are\nx2, . . . , xr) satisfying the equation\n\ndistinct positive integer-valued vectors (x1,\n\nx1 + x2 + \u00b7\u00b7\u00b7 + xr = n xi > 0, i = 1, . . . , r\n\nto obtain the number of nonnegative (as opposed to positive) solutions, note\nthat the number of nonnegative solutions of x1 + x2 + \u00b7\u00b7\u00b7 + xr = n is the same\nas the number of positive solutions of y1 + \u00b7\u00b7\u00b7 + yr = n + r (seen by letting\nyi = xi + 1,\ni = 1, . . . , r). hence, from proposition 6.1, we obtain the following\nproposition.\n\n(cid:2)\n\n(cid:3)\n\ndistinct nonnegative integer-valued vec-\n\nproposition 6.2. there are\ntors (x1, x2, . . . , xr) satisfying the equation\n\nr \u2212 1\n\nn + r \u2212 1\n\nx1 + x2 + \u00b7\u00b7\u00b7 + xr = n\n\n(6.1)\n\nexample 6a\nhow many distinct nonnegative integer-valued solutions of x1 + x2 = 3 are possible?\n= 4 such solutions: (0, 3), (1, 2), (2, 1), (3, 0). .\n\nsolution. there are\n\n3 + 2 \u2212 1\n\n(cid:2)\n\n(cid:3)\n\n2 \u2212 1\n\nexample 6b\nan investor has 20 thousand dollars to invest among 4 possible investments. each\ninvestment must be in units of a thousand dollars. if the total 20 thousand is to be\n\n "}, {"Page_number": 29, "text": "14\n\nchapter 1\n\ncombinatorial analysis\n\ninvested, how many different investment strategies are possible? what if not all the\nmoney need be invested?\nsolution. if we let xi, i = 1, 2, 3, 4, denote the number of thousands invested in\ninvestment i, then, when all is to be invested, x1, x2, x3, x4 are integers satisfying the\nequation\n\nx1 + x2 + x3 + x4 = 20\n\nxi \u00fa 0\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\n24\n4\n\n(cid:6)(cid:2)\n(cid:2)\n\n= 1771 possible investment strategies. if\nhence, by proposition 6.2, there are\nnot all of the money need be invested, then if we let x5 denote the amount kept in\nreserve, a strategy is a nonnegative integer-valued vector (x1, x2, x3, x4, x5) satisfying\nthe equation\n\n23\n3\n\nx1 + x2 + x3 + x4 + x5 = 20\n\nhence, by proposition 6.2, there are now\n\n= 10,626 possible strategies.\n\n.\n\n(cid:3)\n\n(cid:3)\n\nexample 6c\nhow many terms are there in the multinomial expansion of (x1 + x2 + \u00b7\u00b7\u00b7 + xr)n?\nsolution.\n\n(x1 + x2 + \u00b7\u00b7\u00b7 + xr)n =\n\nn\n\nn1, . . . , nr\n\n\u00b7\u00b7\u00b7 xnr\n\nr\n\nxn1\n1\n\nwhere the sum is over all nonnegative integer-valued (n1, . . . , nr) such that n1 + \u00b7\u00b7\u00b7 +\nnr = n. hence, by proposition 6.2, there are\n.\n\nn + r \u2212 1\n\nsuch terms.\n\nr \u2212 1\n\nexample 6d\nlet us consider again example 4c, in which we have a set of n items, of which m are\n(indistinguishable and) defective and the remaining n \u2212 m are (also indistinguishable\nand) functional. our objective is to determine the number of linear orderings in which\nno two defectives are next to each other. to determine this number, let us imagine\nthat the defective items are lined up among themselves and the functional ones are\nnow to be put in position. let us denote x1 as the number of functional items to the\nleft of the first defective, x2 as the number of functional items between the first two\ndefectives, and so on. that is, schematically, we have\nx1 0 x2 0\u00b7\u00b7\u00b7 xm 0 xm+1\n\nnow, there will be at least one functional item between any pair of defectives as long\nas xi > 0, i = 2, . . . , m. hence, the number of outcomes satisfying the condition is the\nnumber of vectors x1, . . . , xm+1 that satisfy the equation\n\nx1 + \u00b7\u00b7\u00b7 + xm+1 = n \u2212 m x1 \u00fa 0, xm+1 \u00fa 0, xi > 0, i = 2, . . . , m\n\n "}, {"Page_number": 30, "text": "(cid:3)\n\n(cid:2)\n\nn \u2212 m + 1\n\nm\n\nsummary 15\nbut, on letting y1 = x1 + 1, yi = xi, i = 2, . . . , m, ym+1 = xm+1 + 1, we see that\nthis number is equal to the number of positive vectors (y1, . . . , ym+1) that satisfy the\nequation\n\ny1 + y2 + \u00b7\u00b7\u00b7 + ym+1 = n \u2212 m + 2\n\nhence, by proposition 6.1, there are\nwith the results of example 4c.\n\nsuch outcomes, in agreement\n\nsuppose now that we are interested in the number of outcomes in which each pair\nof defective items is separated by at least 2 functional items. by the same reason-\ning as that applied previously, this would equal the number of vectors satisfying the\nequation\n\nx1 + \u00b7\u00b7\u00b7 + xm+1 = n \u2212 m x1 \u00fa 0, xm+1 \u00fa 0, xi \u00fa 2, i = 2, . . . , m\n\nupon letting y1 = x1 + 1, yi = xi \u2212 1, i = 2, . . . , m, ym+1 = xm+1 + 1, we see that\nthis is the same as the number of positive solutions of the equation\n\ny1 + \u00b7\u00b7\u00b7 + ym+1 = n \u2212 2m + 3\n(cid:3)\n\n(cid:2)\n\nn \u2212 2m + 2\n\nhence, from proposition 6.1, there are\n\nsuch outcomes.\n\n.\n\nm\n\nsummary\nthe basic principle of counting states that if an experiment consisting of two phases is\nsuch that there are n possible outcomes of phase 1 and, for each of these n outcomes,\nthere are m possible outcomes of phase 2, then there are nm possible outcomes of the\nexperiment.\nthere are n! = n(n \u2212 1)\u00b7\u00b7\u00b7 3 \u00b7 2 \u00b7 1 possible linear orderings of n items. the\n\nquantity 0! is defined to equal 1.\n\nlet\n\n(cid:2)\n\n(cid:3)\n\nn\ni\n\n=\n\nn!\n\n(n \u2212 i)! i!\n\nwhen 0 \u2026 i \u2026 n, and let it equal 0 otherwise. this quantity represents the number\nof different subgroups of size i that can be chosen from a set of size n. it is often\ncalled a binomial coefficient because of its prominence in the binomial theorem, which\nstates that\n\n(cid:3)\n\n(cid:2)\n\nfor nonnegative integers n1, . . . , nr summing to n,\n\n(x + y)n = n(cid:6)\n(cid:3)\n\ni=0\n\nn\n\nn1, n2, . . . , nr\n\n(cid:2)\n\nxiyn\u2212i\n\nn\ni\n\n=\n\nn!\n\nn1!n2!\u00b7\u00b7\u00b7 nr!\n\nis the number of divisions of n items into r distinct nonoverlapping subgroups of sizes\nn1, n2, . . . , nr.\n\n "}, {"Page_number": 31, "text": "16\n\nchapter 1\n\ncombinatorial analysis\n\nproblems\n\n1. (a) how many different 7-place license plates are\npossible if the first 2 places are for letters and\nthe other 5 for numbers?\n\n(b) repeat part (a) under the assumption that no\nletter or number can be repeated in a single\nlicense plate.\n\n2. how many outcome sequences are possible when a\ndie is rolled four times, where we say, for instance,\nthat the outcome is 3, 4, 3, 1 if the first roll landed\non 3, the second on 4, the third on 3, and the fourth\non 1?\n\n3. twenty workers are to be assigned to 20 different\njobs, one to each job. how many different assign-\nments are possible?\n\n4. john, jim, jay, and jack have formed a band con-\nsisting of 4 instruments. if each of the boys can play\nall 4 instruments, how many different arrange-\nments are possible? what if john and jim can play\nall 4 instruments, but jay and jack can each play\nonly piano and drums?\n\n5. for years, telephone area codes in the united\nstates and canada consisted of a sequence of three\ndigits. the first digit was an integer between 2 and\n9, the second digit was either 0 or 1, and the third\ndigit was any integer from 1 to 9. how many area\ncodes were possible? how many area codes start-\ning with a 4 were possible?\n\n6. a well-known nursery rhyme starts as follows:\n\n\u201cas i was going to st. ives\ni met a man with 7 wives.\neach wife had 7 sacks.\neach sack had 7 cats.\neach cat had 7 kittens. . .\u201d\nhow many kittens did the traveler meet?\n\n7. (a) in how many ways can 3 boys and 3 girls sit in\n\na row?\n\n(b) in how many ways can 3 boys and 3 girls sit in\na row if the boys and the girls are each to sit\ntogether?\n\n(c) in how many ways if only the boys must sit\n\ntogether?\n\n(d) in how many ways if no two people of the\n\nsame sex are allowed to sit together?\n\n8. how many different letter arrangements can be\n\nmade from the letters\n(a) fluke?\n(b) propose?\n(c) mississippi?\n(d) arrange?\n\n9. a child has 12 blocks, of which 6 are black, 4 are\nred, 1 is white, and 1 is blue. if the child puts the\nblocks in a line, how many arrangements are pos-\nsible?\n\n10. in how many ways can 8 people be seated in a\n\nrow if\n(a) there are no restrictions on the seating\n\narrangement?\n\n(b) persons a and b must sit next to each other?\n(c) there are 4 men and 4 women and no 2 men\n\nor 2 women can sit next to each other?\n\n(d) there are 5 men and they must sit next to each\n\n(e) there are 4 married couples and each couple\n\nother?\n\nmust sit together?\n\n11. in how many ways can 3 novels, 2 mathematics\nbooks, and 1 chemistry book be arranged on a\nbookshelf if\n(a) the books can be arranged in any order?\n(b) the mathematics books must be together and\n\nthe novels must be together?\n\n(c) the novels must be together, but the other\n\nbooks can be arranged in any order?\n\n12. five separate awards (best scholarship, best lead-\nership qualities, and so on) are to be presented to\nselected students from a class of 30. how many dif-\nferent outcomes are possible if\n(a) a student can receive any number of awards?\n(b) each student can receive at most 1 award?\n\n13. consider a group of 20 people. if everyone shakes\nhands with everyone else, how many handshakes\ntake place?\n\n14. how many 5-card poker hands are there?\n15. a dance class consists of 22 students, of which 10\nare women and 12 are men. if 5 men and 5 women\nare to be chosen and then paired off, how many\nresults are possible?\n\n16. a student has to sell 2 books from a collection of\n6 math, 7 science, and 4 economics books. how\nmany choices are possible if\n(a) both books are to be on the same subject?\n(b) the books are to be on different subjects?\n\n17. seven different gifts are to be distributed among\n10 children. how many distinct results are possible\nif no child is to receive more than one gift?\n\n18. a committee of 7, consisting of 2 republicans,\n2 democrats, and 3 independents, is to be cho-\nsen from a group of 5 republicans, 6 democrats,\nand 4 independents. how many committees are\npossible?\n\n19. from a group of 8 women and 6 men, a committee\nconsisting of 3 men and 3 women is to be formed.\nhow many different committees are possible if\n(a) 2 of the men refuse to serve together?\n(b) 2 of the women refuse to serve together?\n(c) 1 man and 1 woman refuse to serve together?\n\n "}, {"Page_number": 32, "text": "20. a person has 8 friends, of whom 5 will be invited\n\nto a party.\n(a) how many choices are there if 2 of the friends\n\nare feuding and will not attend together?\n\n(b) how many choices if 2 of the friends will only\n\nattend together?\n\n21. consider the grid of points shown here. suppose\nthat, starting at the point labeled a, you can go one\nstep up or one step to the right at each move. this\nprocedure is continued until the point labeled b is\nreached. how many different paths from a to b\nare possible?\nhint: note that to reach b from a, you must take\n4 steps to the right and 3 steps upward.\n\nb\n\na\n\n22. in problem 21, how many different paths are there\nfrom a to b that go through the point circled in\nthe following lattice?\n\nb\n\na\n\n23. a psychology laboratory conducting dream\nresearch contains 3 rooms, with 2 beds in each\nroom. if 3 sets of identical twins are to be assigned\nto these 6 beds so that each set of twins sleeps\n\nproblems 17\n\nin different beds in the same room, how many\nassignments are possible?\n\n24. expand (3x2 + y)5.\n25. the game of bridge is played by 4 players, each of\nwhom is dealt 13 cards. how many bridge deals are\npossible?\n\n26. expand (x1 + 2x2 + 3x3)4.\n27. if 12 people are to be divided into 3 committees of\nrespective sizes 3, 4, and 5, how many divisions are\npossible?\n\n28. if 8 new teachers are to be divided among 4\nschools, how many divisions are possible? what if\neach school must receive 2 teachers?\n\n29. ten weight lifters are competing in a team weight-\nlifting contest. of the lifters, 3 are from the united\nstates, 4 are from russia, 2 are from china, and 1\nis from canada. if the scoring takes account of the\ncountries that the lifters represent, but not their\nindividual identities, how many different outcomes\nare possible from the point of view of scores? how\nmany different outcomes correspond to results in\nwhich the united states has 1 competitor in the\ntop three and 2 in the bottom three?\n\n30. delegates from 10 countries,\n\nincluding russia,\nfrance, england, and the united states, are to\nbe seated in a row. how many different seat-\ning arrangements are possible if the french and\nenglish delegates are to be seated next to each\nother and the russian and u.s. delegates are not\nto be next to each other?\n\u221731. if 8 identical blackboards are to be divided among\n4 schools, how many divisions are possible? how\nmany if each school must receive at least 1 black-\nboard?\n\u221732. an elevator starts at the basement with 8 peo-\nple (not including the elevator operator) and dis-\ncharges them all by the time it reaches the top\nfloor, number 6. in how many ways could the oper-\nator have perceived the people leaving the eleva-\ntor if all people look alike to him? what if the 8\npeople consisted of 5 men and 3 women and the\noperator could tell a man from a woman?\n\u221733. we have 20 thousand dollars that must be invested\namong 4 possible opportunities. each investment\nmust be integral in units of 1 thousand dollars,\nand there are minimal investments that need to be\nmade if one is to invest in these opportunities. the\nminimal investments are 2, 2, 3, and 4 thousand\ndollars. how many different investment strategies\nare available if\n(a) an investment must be made in each opportu-\n\n(b) investments must be made in at least 3 of the\n\nnity?\n\n4 opportunities?\n\n "}, {"Page_number": 33, "text": "18\n\nchapter 1\n\ncombinatorial analysis\n\ntheoretical exercises\n\n1. prove the generalized version of the basic counting\n\nprinciple.\n\n2. two experiments are to be performed. the first\ncan result in any one of m possible outcomes. if\nthe first experiment results in outcome i, then the\nsecond experiment can result in any of ni possible\noutcomes, i = 1, 2, . . . , m. what is the number of\npossible outcomes of the two experiments?\n\n(cid:2)\n\n(cid:3)\n\n3. in how many ways can r objects be selected from a\nset of n objects if the order of selection is consid-\nered relevant?\nn\nr\n\ndifferent linear arrangements of n\nballs of which r are black and n \u2212 r are white. give\na combinatorial explanation of this fact.\n\n4. there are\n\n5. determine the number of vectors (x1, . . . , xn), such\n\nthat each xi is either 0 or 1 and\n\nn(cid:6)\n\ni=1\n\nxi \u00fa k\n\n6. how many vectors x1, . . . , xk are there for which\neach xi is a positive integer such that 1 \u2026 xi \u2026 n\nand x1 < x2 < \u00b7\u00b7\u00b7 < xk?\n(cid:3)(cid:2)\n(cid:2)\n\n7. give an analytic proof of equation (4.1).\n8. prove that\nn + m\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\n(cid:3)\n\n=\n\nn\n0\n\nr\n\nm\nr \u2212 1\n\n(cid:3)(cid:2)\n(cid:3)\n\nn\n1\n\nm\nr\n\n(cid:2)\n\n+\n(cid:3)(cid:2)\n\nn\nr\n\nm\n0\n\n+\u00b7\u00b7\u00b7 +\n\nhint: consider a group of n men and m women.\nhow many groups of size r are possible?\n9. use theoretical exercise 8 to prove that\n\n(cid:2)\n\n(cid:3)2\n\n(cid:3)\n\n(cid:2)\n\n= n(cid:6)\n\nk=0\n\nn\nk\n\n2n\nn\n\n10. from a group of n people, suppose that we want to\nchoose a committee of k, k \u2026 n, one of whom is to\nbe designated as chairperson.\n(a) by focusing first on the choice of the commit-\ntee and then on the choice of the chair, argue\nthat there are\n\nk possible choices.\n\n(cid:3)\n\n(cid:2)\n\nn\nk\n\n(b) by focusing first on the choice of\n\nthe\nnonchair committee members and then on\n\n(cid:2)\n\n(cid:3)\n\n(cid:3)\n\n(cid:2)\n\nthe choice of the chair, argue that there are\nk \u2212 1\n\n(n \u2212 k + 1) possible choices.\n\nn\n\n(d) conclude from parts (a), (b), and (c) that\n\n(c) by focusing first on the choice of the chair\nand then on the choice of the other committee\nn \u2212 1\nk \u2212 1\nmembers, argue that there are n\npossible choices.\n(cid:4)\n= n\n(cid:3)\n\n= (n \u2212 k + 1)\n\nn \u2212 1\nk \u2212 1\n\n(cid:5)\n(cid:2)\n\nk \u2212 1\n\n(cid:4)\n\n(cid:5)\n\n(cid:4)\n\nn\nk\n\nk\n\nn\n\n(cid:5)\n\n(e) use the factorial definition of\n\nto verify\n\nm\nr\n\nthe identity in part (d).\n\n11. the following identity is known as fermat\u2019s com-\n\nbinatorial identity:\n\n(cid:2)\n\n(cid:3)\n\nn\nk\n\n(cid:2)\n\n= n(cid:6)\n\ni=k\n\n(cid:3)\n\ni \u2212 1\nk \u2212 1\n\nn \u00fa k\n\ngive a combinatorial argument (no computations\nare needed) to establish this identity.\nhint: consider the set of numbers 1 through n.\nhow many subsets of size k have i as their highest-\nnumbered member?\n\n12. consider the following combinatorial identity:\n\n(cid:2)\n\n(cid:3)\n\nn\nk\n\nn(cid:6)\n\nk\n\nk=1\n\n= n \u00b7 2n\u22121\n\n(a) present a combinatorial argument for this\nidentity by considering a set of n people and\ndetermining, in two ways, the number of pos-\nsible selections of a committee of any size and\na chairperson for the committee.\nhint:\n\n(i) how many possible selections are there\nof a committee of size k and its chairper-\nson?\n\n(ii) how many possible selections are there\nof a chairperson and the other commit-\ntee members?\n(b) verify the following identity for n =\n\n1, 2, 3, 4, 5:\n\n(cid:3)\n\n(cid:2)\n\nn(cid:6)\n\nk=1\n\nn\nk\n\nk2 = 2n\u22122n(n + 1)\n\n "}, {"Page_number": 34, "text": "for a combinatorial proof of the preceding,\nconsider a set of n people and argue that both\nsides of the identity represent the number of\ndifferent selections of a committee, its chair-\nperson, and its secretary (possibly the same as\nthe chairperson).\nhint:\n\n(i) how many different selections result in\nthe committee containing exactly k peo-\nple?\n\n(ii) how many different selections are there\nin which the chairperson and the secre-\ntary are the same? (answer: n2n\u22121.)\n(iii) how many different selections result in\nthe chairperson and the secretary being\ndifferent?\n(c) now argue that\n\n(cid:3)\n\n(cid:2)\n\nn(cid:6)\n\nk=1\n\nn\nk\n\nk3 = 2n\u22123n2(n + 3)\n\n13. show that, for n > 0,\n\nn(cid:6)\n(\u22121)i\n\ni=0\n\n(cid:2)\n\n(cid:3)\n\nn\ni\n\n= 0\n\nhint: use the binomial theorem.\n\n14. from a set of n people, a committee of size j is to be\nchosen, and from this committee, a subcommittee\nof size i, i \u2026 j, is also to be chosen.\n(a) derive a combinatorial identity by comput-\ning,\nin two ways, the number of possible\nchoices of the committee and subcommittee\u2014\nfirst by supposing that\nthe committee is\nchosen first and then the subcommittee is\nchosen, and second by supposing that the\nsubcommittee is chosen first and then the\nremaining members of the committee are\nchosen.\n\n(b) use part (a) to prove the following combina-\n\ntorial identity:\n\n(cid:2)\n\nn(cid:6)\n\nj=i\n\nn\nj\n\n(cid:3)(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\n=\n\nj\ni\n\nn\ni\n\n2n\u2212i\n\ni \u2026 n\n\ntheoretical exercises 19\n\n15. let hk(n) be the number of vectors x1, . . . , xk for\nwhich each xi is a positive integer satisfying 1 \u2026\nxi \u2026 n and x1 \u2026 x2 \u2026 \u00b7\u00b7\u00b7 \u2026 xk.\n(a) without any computations, argue that\n\nh1(n) = n\n\nhk(n) = n(cid:6)\n\nj=1\n\nhk\u22121(j) k > 1\n\nhint: how many vectors are there in which\nxk = j?\n\n(b) use the preceding recursion to compute\nh3(5).\nhint: first compute h2(n) for n = 1, 2, 3, 4, 5.\n\n16. consider a tournament of n contestants in which\nthe outcome is an ordering of these contestants,\nwith ties allowed. that is, the outcome partitions\nthe players into groups, with the first group consist-\ning of the players that tied for first place, the next\ngroup being those that tied for the next-best posi-\ntion, and so on. let n(n) denote the number of dif-\nferent possible outcomes. for instance, n(2) = 3,\nsince, in a tournament with 2 contestants, player 1\ncould be uniquely first, player 2 could be uniquely\nfirst, or they could tie for first.\n(a) list all the possible outcomes when n = 3.\n(b) with n(0) defined to equal 1, argue, without\n\nany computations, that\n\nn(n) = n(cid:6)\n\ni=1\n\n(cid:2)\n\n(cid:3)\n\nn\ni\n\nn(n \u2212 i)\n\nhint: how many outcomes are there in\nwhich i players tie for last place?\n\n(c) show that the formula of part (b) is equivalent\n\nto the following:\n\nn(n) = n\u22121(cid:6)\n\ni=0\n\n(cid:2)\n\n(cid:3)\n\nn\ni\n\nn(i)\n\n(c) use part (a) and theoretical exercise 13 to\n\n(d) use the recursion to find n(3) and n(4).\n\nshow that\n\n(cid:2)\n\nn(cid:6)\n\nj=i\n\n(cid:3)(cid:2)\n\n(cid:3)\n\nn\nj\n\nj\ni\n\n(\u22121)n\u2212j = 0 i < n\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n=\n\nn\nr\n\nn\n\nr, n \u2212 r\n\n(cid:3)\n\n.\n\n17. present a combinatorial explanation of why\n\n "}, {"Page_number": 35, "text": "20\n\nchapter 1\n\ncombinatorial analysis\n\n18. argue that\n\n(cid:2)\n\nn\n\nn1, n2, . . . , nr\n\n(cid:3)\n\n=\n\n(cid:2)\n\n+\n\n+\n\n(cid:3)\n\nn \u2212 1\n\n(cid:2)\nn1 \u2212 1, n2, . . . , nr\nn \u2212 1\n(cid:2)\nn \u2212 1\n\nn1, n2 \u2212 1, . . . , nr\nn1, n2, . . . , nr \u2212 1\n\n(cid:3)\n(cid:3)\n\n+ \u00b7\u00b7\u00b7\n\nhint: use an argument similar to the one used to\nestablish equation (4.1).\n\n19. prove the multinomial theorem.\n\u221720. in how many ways can n identical balls be dis-\ntributed into r urns so that the ith urn contains at\nleast mi balls, for each i = 1, . . . , r? assume that\nn \u00fa\n\n(cid:9)\n\nr\ni=1 mi.\n\n\u221721. argue that there are exactly\n\nsolutions of\n\n(cid:3)(cid:2)\n\n(cid:2)\n\nr\nk\n\nn \u2212 1\n\nn \u2212 r + k\n\n(cid:3)\n\nx1 + x2 + \u00b7\u00b7\u00b7 + xr = n\n\nfor which exactly k of the xi are equal to 0.\n\n\u221722. consider a function f (x1, . . . , xn) of n variables.\nhow many different partial derivatives of order r\ndoes f possess?\n\u221723. determine the number of vectors (x1, . . . , xn) such\n\nthat each xi is a nonnegative integer and\n\nn(cid:6)\n\ni=1\n\nxi \u2026 k\n\nself-test problems and exercises\n\n1. how many different linear arrangements are there\n\n7. give a combinatorial explanation of the identity\n\nof the letters a, b, c, d, e, f for which\n(a) a and b are next to each other?\n(b) a is before b?\n(c) a is before b and b is before c?\n(d) a is before b and c is before d?\n(e) a and b are next to each other and c and d\n\nare also next to each other?\n\n(f) e is not last in line?\n\n2. if 4 americans, 3 french people, and 3 british\npeople are to be seated in a row, how many seat-\ning arrangements are possible when people of the\nsame nationality must sit next to each other?\n\n3. a president, treasurer, and secretary, all different,\nare to be chosen from a club consisting of 10 peo-\nple. how many different choices of officers are\npossible if\n(a) there are no restrictions?\n(b) a and b will not serve together?\n(c) c and d will serve together or not at all?\n(d) e must be an officer?\n(e) f will serve only if he is president?\n\n4. a student is to answer 7 out of 10 questions in\nan examination. how many choices has she? how\nmany if she must answer at least 3 of the first 5\nquestions?\n\n5. in how many ways can a man divide 7 gifts among\nhis 3 children if the eldest is to receive 3 gifts and\nthe others 2 each?\n\n6. how many different 7-place license plates are pos-\nsible when 3 of the entries are letters and 4 are\ndigits? assume that repetition of letters and num-\nbers is allowed and that there is no restriction on\nwhere the letters or numbers can be placed.\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n=\n\nn\nr\n\nn\n\nn \u2212 r\n\n(cid:3)\n\n8. consider n-digit numbers where each digit is one\nof the 10 integers 0, 1, . . . , 9. how many such num-\nbers are there for which\n(a) no two consecutive digits are equal?\n(b) 0 appears as a digit a total of i times, i =\n\n0, . . . , n?\n\n9. consider three classes, each consisting of n stu-\ndents. from this group of 3n students, a group of 3\nstudents is to be chosen.\n(a) how many choices are possible?\n(b) how many choices are there in which all 3 stu-\n\ndents are in the same class?\n\n(c) how many choices are there in which 2 of the\n3 students are in the same class and the other\nstudent is in a different class?\n\n(d) how many choices are there in which all 3 stu-\n\ndents are in different classes?\n\n(e) using the results of parts (a) through (d),\n\nwrite a combinatorial identity.\n\n10. how many 5-digit numbers can be formed from\nthe integers 1, 2, . . . , 9 if no digit can appear more\nthan twice? (for instance, 41434 is not allowed.)\n\n11. from 10 married couples, we want to select a\ngroup of 6 people that is not allowed to contain\na married couple.\n(a) how many choices are there?\n(b) how many choices are there if the group must\n\nalso consist of 3 men and 3 women?\n\n "}, {"Page_number": 36, "text": "12. a committee of 6 people is to be chosen from a\ngroup consisting of 7 men and 8 women. if the\ncommittee must consist of at least 3 women and\nat least 2 men, how many different committees are\npossible?\n\u221713. an art collection on auction consisted of 4 dalis, 5\nvan goghs, and 6 picassos. at the auction were 5\nart collectors. if a reporter noted only the number\nof dalis, van goghs, and picassos acquired by each\ncollector, how many different results could have\nbeen recorded if all of the works were sold?\n\u221714. determine the number of vectors (x1, . . . , xn) such\n\nthat each xi is a positive integer and\n\nn(cid:6)\n\ni=1\n\nxi \u2026 k\n\nwhere k \u00fa n.\n\n15. a total of n students are enrolled in a review\ncourse for the actuarial examination in probability.\nthe posted results of the examination will list the\nnames of those who passed, in decreasing order of\ntheir scores. for instance, the posted result will be\n\u201cbrown, cho\u201d if brown and cho are the only ones\nto pass, with brown receiving the higher score.\n\nself-test problems and exercises 21\n\nassuming that all scores are distinct (no ties), how\nmany posted results are possible?\n16. how many subsets of size 4 of the set s =\n{1, 2, . . . , 20} contain at least one of the elements\n1, 2, 3, 4, 5?\n(cid:2)\n(cid:3)\n\n17. give an analytic verification of\n\n(cid:2)\n\n(cid:3)\n\n(cid:3)\n\n=\n\nn\n2\n\nk\n2\n\n+ k(n \u2212 k) +\n\n,\n\n1 \u2026 k \u2026 n\n\n(cid:2)\nn \u2212 k\n\n2\n\nnow, give a combinatorial argument\nidentity.\n\nfor this\n\n18. in a certain community, there are 3 families con-\nsisting of a single parent and 1 child, 3 families\nconsisting of a single parent and 2 children, 5 fam-\nilies consisting of 2 parents and a single child, 7\nfamilies consisting of 2 parents and 2 children, and\n6 families consisting of 2 parents and 3 children. if\na parent and child from the same family are to be\nchosen, how many possible choices are there?\n\n19. if there are no restrictions on where the digits and\nletters are placed, how many 8-place license plates\nconsisting of 5 letters and 3 digits are possible if no\nrepetitions of letters or digits are allowed. what if\nthe 3 digits must be consecutive?\n\n "}, {"Page_number": 37, "text": "c h a p t e r\n\n2\n\naxioms of probability\n\n2.1 introduction\n2.2 sample space and events\n2.3 axioms of probability\n2.4 some simple propositions\n2.5 sample spaces having equally likely outcomes\n2.6 probability as a continuous set function\n2.7 probability as a measure of belief\n\n2.1 introduction\n\nin this chapter, we introduce the concept of the probability of an event and then show\nhow probabilities can be computed in certain situations. as a preliminary, however,\nwe need the concept of the sample space and the events of an experiment.\n\n2.2 sample space and events\n\nconsider an experiment whose outcome is not predictable with certainty. however,\nalthough the outcome of the experiment will not be known in advance, let us suppose\nthat the set of all possible outcomes is known. this set of all possible outcomes of\nan experiment is known as the sample space of the experiment and is denoted by s.\nfollowing are some examples:\n\n1. if the outcome of an experiment consists in the determination of the sex of a\n\nnewborn child, then\n\ns = {g, b}\n\nwhere the outcome g means that the child is a girl and b that it is a boy.\n\n2. if the outcome of an experiment is the order of finish in a race among the 7\n\nhorses having post positions 1, 2, 3, 4, 5, 6, and 7, then\n\ns = {all 7! permutations of (1, 2, 3, 4, 5, 6, 7)}\n\nthe outcome (2, 3, 1, 6, 5, 4, 7) means, for instance, that the number 2 horse\ncomes in first, then the number 3 horse, then the number 1 horse, and so on.\n\n3. if the experiment consists of flipping two coins, then the sample space consists\n\nof the following four points:\n\ns = {(h, h), (h, t), (t, h), (t, t)}\n\nthe outcome will be (h, h) if both coins are heads, (h, t) if the first coin is\nheads and the second tails, (t, h) if the first is tails and the second heads, and\n(t, t) if both coins are tails.\n\n22\n\n "}, {"Page_number": 38, "text": "section 2.2\n\nsample space and events 23\n\n4. if the experiment consists of tossing two dice, then the sample space consists of\n\nthe 36 points\n\ns = {(i, j): i, j = 1, 2, 3, 4, 5, 6}\n\nwhere the outcome (i, j) is said to occur if i appears on the leftmost die and j on\nthe other die.\n\n5. if the experiment consists of measuring (in hours) the lifetime of a transistor,\n\nthen the sample space consists of all nonnegative real numbers; that is,\n\ns = {x: 0 \u2026 x < q}\n\nany subset e of the sample space is known as an event. in other words, an event is\na set consisting of possible outcomes of the experiment. if the outcome of the experi-\nment is contained in e, then we say that e has occurred. following are some examples\nof events.\nin the preceding example 1, if e = {g}, then e is the event that the child is a girl.\nsimilarly, if f = {b}, then f is the event that the child is a boy.\n\nin example 2, if\n\ne = {all outcomes in s starting with a 3}\n\nthe first coin.\n\nthen e is the event that horse 3 wins the race.\n\nthe sum of the dice equals 7.\n\nin example 3, if e = {(h, h), (h, t)}, then e is the event that a head appears on\nin example 4, if e = {(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}, then e is the event that\nin example 5, if e = {x: 0 \u2026 x \u2026 5}, then e is the event that the transistor does\nfor any two events e and f of a sample space s, we define the new event e \u222a f\nto consist of all outcomes that are either in e or in f or in both e and f. that is, the\nevent e \u222a f will occur if either e or f occurs. for instance, in example 1, if event\ne = {g} and f = {b}, then\n\nnot last longer than 5 hours.\n\ne \u222a f = {g, b}\n\nthat is, e \u222a f is the whole sample space s. in example 3, if e = {(h, h), (h, t)} and\nf = {(t, h)}, then\n\ne \u222a f = {(h, h), (h, t), (t, h)}\nthus, e \u222a f would occur if a head appeared on either coin.\n\nthe event e \u222a f is called the union of the event e and the event f.\nsimilarly, for any two events e and f, we may also define the new event ef, called\nthe intersection of e and f, to consist of all outcomes that are both in e and in f.\nthat is, the event ef (sometimes written e \u2229 f) will occur only if both e and f\noccur. for instance, in example 3, if e = {(h, h), (h, t), (t, h)} is the event that at\nleast 1 head occurs and f = {(h, t), (t, h), (t, t)} is the event that at least 1 tail\noccurs, then\n\nef = {(h, t), (t, h)}\n\nis the event that exactly 1 head and 1 tail occur. in example 4, if e = {(1, 6), (2, 5),\n(3, 4), (4, 3), (5, 2), (6, 1)} is the event that the sum of the dice is 7 and f = {(1, 5), (2, 4),\n(3, 3), (4, 2), (5, 1)} is the event that the sum is 6, then the event ef does not contain\n\n "}, {"Page_number": 39, "text": "24\n\nchapter 2\n\naxioms of probability\n\nn=1\n\nq(cid:11)\n\nany outcomes and hence could not occur. to give such an event a name, we shall refer\nto it as the null event and denote it by \u00f8. (that is, \u00f8 refers to the event consisting of\nno outcomes.) if ef = \u00f8, then e and f are said to be mutually exclusive.\nq(cid:10)\n\nwe define unions and intersections of more than two events in a similar manner.\nif e1, e2, . . . are events, then the union of these events, denoted by\nen, is defined\nto be that event which consists of all outcomes that are in en for at least one value\nof n = 1, 2, . . .. similarly, the intersection of the events en, denoted by\nen, is\ndefined to be the event consisting of those outcomes which are in all of the events\nen, n = 1, 2, . . ..\nfinally, for any event e, we define the new event ec, referred to as the com-\nplement of e, to consist of all outcomes in the sample space s that are not in e.\nthat is, ec will occur if and only if e does not occur. in example 4, if event e =\n{(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}, then ec will occur when the sum of the dice\ndoes not equal 7. note that because the experiment must result in some outcome, it\nfollows that sc = \u00f8.\nfor any two events e and f, if all of the outcomes in e are also in f, then we say\nthat e is contained in f, or e is a subset of f, and write e ( f (or equivalently, f ) e,\nwhich we sometimes say as f is a superset of e). thus, if e ( f, then the occurrence\nof e implies the occurrence of f. if e ( f and f ( e, we say that e and f are equal\nand write e = f.\n\nn=1\n\na graphical representation that is useful for illustrating logical relations among\nevents is the venn diagram. the sample space s is represented as consisting of all\nthe outcomes in a large rectangle, and the events e, f, g, . . . are represented as con-\nsisting of all the outcomes in given circles within the rectangle. events of interest\ncan then be indicated by shading appropriate regions of the diagram. for instance, in\nthe three venn diagrams shown in figure 2.1, the shaded areas represent, respec-\ntively, the events e \u222a f, ef, and ec. the venn diagram in figure 2.2 indicates\nthat e ( f.\n\ne\n\ns\n\nf\n\n(a) shaded region: e \u50bc f.\n\ns\n\nf\n\ne\n\ne\n\n(b) shaded region: ef.\n\ns\n\n(c) shaded region: ec.\n\nfigure 2.1: venn diagrams\n\n "}, {"Page_number": 40, "text": "section 2.2\n\nsample space and events 25\n\ns\n\nf\n\ne\n\nfigure 2.2: e ( f\n\nthe operations of forming unions, intersections, and complements of events obey\n\ncertain rules similar to the rules of algebra. we list a few of these rules:\n\ncommutative laws\nassociative laws\ndistributive laws\n\ne\u222a f = f \u222a e\n\n(e\u222a f)\u222a g = e\u222a (f \u222a g)\n(e\u222a f)g = eg\u222a fg\n\nef = fe\n\n(ef)g = e(fg)\nef \u222a g = (e\u222a g)(f \u222a g)\n\nthese relations are verified by showing that any outcome that is contained in the\nevent on the left side of the equality sign is also contained in the event on the right\nside, and vice versa. one way of showing this is by means of venn diagrams. for\ninstance, the distributive law may be verified by the sequence of diagrams in\nfigure 2.3.\n\ne\n\nf\n\ne\n\nf\n\ng\n\ng\n\n(a) shaded region: eg.\n\n(b) shaded region: fg.\n\ne\n\nf\n\n(c) shaded region: (e \u50bc f )g.\n\ng\n\nfigure 2.3: (e \u222a f)g = eg \u222a fg\n\n "}, {"Page_number": 41, "text": "\u239e\n\u23a0c\n\u239e\n\u23a0c\n\nei\n\nei\n\n\u239b\n\u239d n(cid:14)\n\u239b\n\u239d n(cid:17)\n\ni=1\n\ni=1\n\n= n(cid:17)\n= n(cid:14)\n\ni=1\n\ni=1\n\nec\ni\n\nec\ni\n\n26\n\nchapter 2\n\naxioms of probability\n\nthe following useful relationships between the three basic operations of forming\n\nunions, intersections, and complements are known as demorgan\u2019s laws:\n\n(cid:5)c\n\nei\n\n. then\n\n(cid:4)\n\nn(cid:10)\n\ni=1\n\nto prove demorgan\u2019s laws, suppose first that x is an outcome of\n\nn(cid:10)\n\ni=1\n\nn(cid:11)\n\nx is not contained in\nei, which means that x is not contained in any of the events\ni for all i = 1, 2, . . . , n and thus is\nei, i = 1, 2, . . . , n, implying that x is contained in ec\nec\nec\ni . to go the other way, suppose that x is an outcome of\ni . then\ncontained in\nn(cid:10)\ni for all i = 1, 2, . . . , n, which means that x is not contained in ei for\nx is contained in ec\n(cid:5)c\nany i = 1, 2, . . . , n, implying that x is not contained in\nei, in turn implying that x is\n\nn(cid:11)\n\n(cid:4)\n\ni=1\n\ni=1\n\ni\n\nn(cid:10)\n\ncontained in\n\nei\n\n. this proves the first of demorgan\u2019s laws.\n\nto prove the second of demorgan\u2019s laws, we use the first law to obtain\n\n1\n\n\u239b\n\u239d n(cid:14)\nwhich, since (ec)c = e, is equivalent to\u239b\n\u239d n(cid:14)\n\ni=1\n\nec\ni\n\n(ec\ni\n\n)c\n\n= n(cid:17)\n\ni=1\n\n\u239e\n\u23a0c\n\u239e\n\u23a0c\n\u239b\n\u239d n(cid:17)\n\n=\n\nei\n\n= n(cid:17)\n\u239e\n\u23a0c\n\n1\n\nei\n\nec\ni\n\n1\n\nn(cid:14)\n\nec\ni\n\ntaking complements of both sides of the preceding equation yields the result we seek,\nnamely,\n\n2.3 axioms of probability\n\n1\n\n1\n\none way of defining the probability of an event is in terms of its relative frequency.\nsuch a definition usually goes as follows: we suppose that an experiment, whose sam-\nple space is s, is repeatedly performed under exactly the same conditions. for each\nevent e of the sample space s, we define n(e) to be the number of times in the first n\nrepetitions of the experiment that the event e occurs. then p(e), the probability of\nthe event e, is defined as\n\np(e) = lim\nn\u2192q\n\nn(e)\n\nn\n\n "}, {"Page_number": 42, "text": "section 2.3\n\naxioms of probability 27\n\nthat is, p(e) is defined as the (limiting) proportion of time that e occurs. it is thus\nthe limiting frequency of e.\n\nalthough the preceding definition is certainly intuitively pleasing and should always\nbe kept in mind by the reader, it possesses a serious drawback: how do we know that\nn(e)/n will converge to some constant limiting value that will be the same for each\npossible sequence of repetitions of the experiment? for example, suppose that the\nexperiment to be repeatedly performed consists of flipping a coin. how do we know\nthat the proportion of heads obtained in the first n flips will converge to some value\nas n gets large? also, even if it does converge to some value, how do we know that,\nif the experiment is repeatedly performed a second time, we shall obtain the same\nlimiting proportion of heads?\n\nproponents of the relative frequency definition of probability usually answer this\nobjection by stating that the convergence of n(e)/n to a constant limiting value is an\nassumption, or an axiom, of the system. however, to assume that n(e)/n will neces-\nsarily converge to some constant value seems to be an extraordinarily complicated\nassumption. for, although we might indeed hope that such a constant limiting fre-\nquency exists, it does not at all seem to be a priori evident that this need be the\ncase. in fact, would it not be more reasonable to assume a set of simpler and more\nself-evident axioms about probability and then attempt to prove that such a con-\nstant limiting frequency does in some sense exist? the latter approach is the modern\naxiomatic approach to probability theory that we shall adopt in this text. in particular,\nwe shall assume that, for each event e in the sample space s, there exists a value p(e),\nreferred to as the probability of e. we shall then assume that all these probabilities\nsatisfy a certain set of axioms, which, we hope the reader will agree, is in accordance\nwith our intuitive notion of probability.\n\nconsider an experiment whose sample space is s. for each event e of the sample\nspace s, we assume that a number p(e) is defined and satisfies the following three\naxioms:\n\naxiom 1\n\naxiom 2\n\n0 \u2026 p(e) \u2026 1\n\np(s) = 1\n\naxiom 3\nfor any sequence of mutually exclusive events e1, e2, . . . (that is, events for which\neiej = \u00f8 when i z j),\n\n\u239e\n\u23a0 =\n\nq(cid:6)\n\n\u239b\n\u239d q(cid:14)\n\ni=1\n\np\n\nei\n\np(ei)\n\ni=1\n\nwe refer to p(e) as the probability of the event e.\n\nthus, axiom 1 states that the probability that the outcome of the experiment is an\noutcome in e is some number between 0 and 1. axiom 2 states that, with probability\n1, the outcome will be a point in the sample space s. axiom 3 states that, for any\nsequence of mutually exclusive events, the probability of at least one of these events\noccurring is just the sum of their respective probabilities.\n\n "}, {"Page_number": 43, "text": "28\n\nchapter 2\n\naxioms of probability\n\nthen, because the events are mutually exclusive and because s = q(cid:10)\n\nif we consider a sequence of events e1, e2, . . ., where e1 = s and ei = \u00f8 for i > 1,\nei, we have, from\n\naxiom 3,\n\nimplying that\n\np(s) =\n\nq(cid:6)\n\ni=1\n\np(ei) = p(s) +\n\ni=1\n\nq(cid:6)\n\np(\u00f8)\n\ni=2\n\nthat is, the null event has probability 0 of occurring.\n\nnote that it follows that, for any finite sequence of mutually exclusive events e1,\n\ne2, . . . , en,\n\np(ei)\n\n(3.1)\n\np(\u00f8) = 0\n\u239e\n\u23a0 = n(cid:6)\n\nei\n\ni=1\n\n\u239b\n\u239d n(cid:14)\n\n1\n\np\n\nthis equation follows from axiom 3 by defining ei as the null event for all values\nof i greater than n. axiom 3 is equivalent to equation (3.1) when the sample space\nis finite. (why?) however, the added generality of axiom 3 is necessary when the\nsample space consists of an infinite number of points.\n\nexample 3a\nif our experiment consists of tossing a coin and if we assume that a head is as likely\nto appear as a tail, then we would have\n\np({h}) = p({t}) = 1\n2\n\non the other hand, if the coin were biased and we felt that a head were twice as likely\nto appear as a tail, then we would have\np({h}) = 2\n3\n\np({t}) = 1\n3\n\n.\n\nexample 3b\nif a die is rolled and we suppose that all six sides are equally likely to appear, then\nwe would have p({1}) = p({2}) = p({3}) = p({4}) = p({5}) = p({6}) = 1\n6. from\naxiom 3, it would thus follow that the probability of rolling an even number would\nequal\n\np({2, 4, 6}) = p({2}) + p({4}) + p({6}) = 1\n2\n\n.\n\nthe assumption of the existence of a set function p, defined on the events of a\nsample space s and satisfying axioms 1, 2, and 3, constitutes the modern mathemat-\nical approach to probability theory. hopefully, the reader will agree that the axioms\nare natural and in accordance with our intuitive concept of probability as related to\nchance and randomness. furthermore, using these axioms we shall be able to prove\nthat if an experiment is repeated over and over again, then, with probability 1, the\nproportion of time during which any specific event e occurs will equal p(e). this\nresult, known as the strong law of large numbers, is presented in chapter 8. in addi-\ntion, we present another possible interpretation of probability\u2014as being a measure\nof belief\u2014in section 2.7.\n\n "}, {"Page_number": 44, "text": "section 2.4\n\nsome simple propositions 29\n\ntechnical remark. we have supposed that p(e) is defined for all the events e\nof the sample space. actually, when the sample space is an uncountably infinite set,\np(e) is defined only for a class of events called measurable. however, this restriction\nneed not concern us, as all events of any practical interest are measurable.\n\n2.4 some simple propositions\n\nin this section, we prove some simple propositions regarding probabilities. we first\nnote that, since e and ec are always mutually exclusive and since e \u222a ec = s, we\nhave, by axioms 2 and 3,\n\n1 = p(s) = p(e \u222a ec) = p(e) + p(ec)\n\nor, equivalently, we have proposition 4.1.\n\nproposition 4.1.\n\np(ec) = 1 \u2212 p(e)\n\nin words, proposition 4.1 states that the probability that an event does not occur is\n1 minus the probability that it does occur. for instance, if the probability of obtaining\na head on the toss of a coin is 3\n\n8, then the probability of obtaining a tail must be 5\n8.\n\nour second proposition states that if the event e is contained in the event f, then\n\nthe probability of e is no greater than the probability of f.\nproposition 4.2. if e ( f, then p(e) \u2026 p(f).\n\nproof. since e ( f, it follows that we can express f as\n\nf = e \u222a ecf\n\nhence, because e and ecf are mutually exclusive, we obtain, from axiom 3,\n\np(f) = p(e) + p(ecf)\n\nwhich proves the result, since p(ecf) \u00fa 0.\nproposition 4.2 tells us, for instance, that the probability of rolling a 1 with a die is\n\nless than or equal to the probability of rolling an odd value with the die.\n\nthe next proposition gives the relationship between the probability of the union\nof two events, expressed in terms of the individual probabilities, and the probability\nof the intersection of the events.\n\nproposition 4.3.\n\np(e \u222a f) = p(e) + p(f) \u2212 p(ef)\n\nproof. to derive a formula for p(e \u222a f), we first note that e \u222a f can be written\nas the union of the two disjoint events e and ecf. thus, from axiom 3, we obtain\n\np(e \u222a f) = p(e \u222a ecf)\n\n= p(e) + p(ecf)\n\nfurthermore, since f = ef \u222a ecf, we again obtain from axiom 3\n\np(f) = p(ef) + p(ecf)\n\n "}, {"Page_number": 45, "text": "30\n\nchapter 2\n\naxioms of probability\n\ne\n\nf\n\nfigure 2.4: venn diagram\n\ne\n\nf\n\ni\n\nii\n\niii\n\nfigure 2.5: venn diagram in sections\n\nor, equivalently,\n\np(ecf) = p(f) \u2212 p(ef)\n\nthereby completing the proof.\n\nin figure 2.4.\n\nproposition 4.3 could also have been proved by making use of the venn diagram\nlet us divide e \u222a f into three mutually exclusive sections, as shown in figure 2.5.\nin words, section i represents all the points in e that are not in f (that is, efc),\nsection ii represents all points both in e and in f (that is, ef), and section iii repre-\nsents all points in f that are not in e (that is, ecf).\n\nfrom figure 2.5, we see that\n\ne \u222a f = i \u222a ii \u222a iii\n\ne = i \u222a ii\nf = ii \u222a iii\n\nas i, ii, and iii are mutually exclusive, it follows from axiom 3 that\n\np(e \u222a f) = p(i) + p(ii) + p(iii)\n\np(e) = p(i) + p(ii)\np(f) = p(ii) + p(iii)\n\np(e \u222a f) = p(e) + p(f) \u2212 p(ii)\n\nwhich shows that\nand proposition 4.3 is proved, since ii = ef.\nexample 4a\nj is taking two books along on her holiday vacation. with probability .5, she will like\nthe first book; with probability .4, she will like the second book; and with probabil-\nity .3, she will like both books. what is the probability that she likes neither book?\n\n "}, {"Page_number": 46, "text": "some simple propositions 31\nsolution. let bi denote the event that j likes book i, i = 1, 2. then the probability\nthat she likes at least one of the books is\n\nsection 2.4\n\np(b1 \u222a b2) = p(b1) + p(b2) \u2212 p(b1b2) = .5 + .4 \u2212 .3 = .6\n\nbecause the event that j likes neither book is the complement of the event that she\nlikes at least one of them, we obtain the result\n\n(cid:18)\n\n(cid:19) = 1 \u2212 p(b1 \u222a b2) = .4\n\np(bc\n\n1bc\n2\n\n) = p\n\n(b1 \u222a b2)c\n\n.\n\nwe may also calculate the probability that any one of the three events e, f, and g\n\noccurs, namely,\n\np(e \u222a f \u222a g) = p[(e \u222a f) \u222a g]\n\nwhich, by proposition 4.3, equals\n\np(e \u222a f) + p(g) \u2212 p[(e \u222a f)g]\n\nnow, it follows from the distributive law that the events (e \u222a f)g and eg \u222a fg are\nequivalent; hence, from the preceding equations, we obtain\n\np(e \u222a f \u222a g)\n\n= p(e) + p(f) \u2212 p(ef) + p(g) \u2212 p(eg \u222a fg)\n= p(e) + p(f) \u2212 p(ef) + p(g) \u2212 p(eg) \u2212 p(fg) + p(egfg)\n= p(e) + p(f) + p(g) \u2212 p(ef) \u2212 p(eg) \u2212 p(fg) + p(efg)\n\nin fact, the following proposition, known as the inclusion\u2013exclusion identity, can\n\nbe proved by mathematical induction:\n\nproposition 4.4.\n\np(e1 \u222a e2 \u222a \u00b7\u00b7\u00b7 \u222a en) = n(cid:6)\n\n(cid:9)\n\nthe summation\np(ei1ei2\nsets of size r of the set {1, 2, . . . , n}.\n\ni1<i2<\u00b7\u00b7\u00b7<ir\n\n(cid:6)\n(cid:6)\n\ni1<i2\n\np(ei1ei2\n\n) + \u00b7\u00b7\u00b7\n\np(ei) \u2212\ni=1\n+ (\u22121)r+1\n\u00b7\u00b7\u00b7 eir\n(cid:2)\n(cid:3)\n+ \u00b7\u00b7\u00b7 + (\u22121)n+1p(e1e2 \u00b7\u00b7\u00b7 en)\n\u00b7\u00b7\u00b7 eir\n\n) is taken over all of the\n\ni1<i2<\u00b7\u00b7\u00b7<ir\n\np(ei1ei2\n\n)\n\nn\nr\n\npossible sub-\n\nin words, proposition 4.4 states that the probability of the union of n events equals\nthe sum of the probabilities of these events taken one at a time, minus the sum of the\nprobabilities of these events taken two at a time, plus the sum of the probabilities of\nthese events taken three at a time, and so on.\n\nremarks.\n\n1. for a noninductive argument for proposition 4.4, note first that if an\noutcome of the sample space is not a member of any of the sets ei, then its probability\ndoes not contribute anything to either side of the equality. now, suppose that an\noutcome is in exactly m of the events ei, where m > 0. then, since it is in\nei, its\n\n(cid:10)\n\ni\n\n "}, {"Page_number": 47, "text": "32\n\nchapter 2\n\naxioms of probability\n\n(cid:5)\n\n; also, as this outcome is contained in\n\n(cid:3)\n\n(cid:2)\n\nm\nk\n\nprobability is counted once in p\n\nei\n\nsubsets of the type ei1ei2\n\n(cid:2)\n\n\u00b7\u00b7\u00b7 eik, its probability is counted\n(cid:3)\n(cid:2)\n\u2212 \u00b7\u00b7\u00b7 ;\n\u2212\n\n(cid:2)\n\n(cid:3)\n\n+\n\ntimes on the right of the equality sign in proposition 4.4. thus, for m > 0, we must\nshow that\n\n(cid:3)\n\nm\nm\n\n(cid:2)\n\n(cid:3)\n\nm\nm\n\nm\n3\n\n(cid:2)\n\n(cid:3)\n\nm\n3\n\n\u2212 \u00b7\u00b7\u00b7 ;\n\n1 =\n(cid:2)\n\nhowever, since 1 =\n\n(cid:4)(cid:10)\n(cid:3)\n(cid:2)\n\ni\n\nm\n2\n\n(cid:2)\n\n(cid:3)\n\n\u2212\n\nm\n2\n\n+\n\n(cid:3)\n\n(cid:2)\n\nm(cid:6)\n\ni=0\n\nm\ni\n\nm\n1\n\n(cid:2)\n\nm\n0\n\n(cid:3)\n\nm\n1\n\n(cid:3)\n\nand the latter equation follows from the binomial theorem, since\n\n, the preceding equation is equivalent to\n\n(\u22121)i = 0\n(cid:3)\n(cid:2)\n\n0 = (\u22121 + 1)m = m(cid:6)\ni=1ei) = n(cid:6)\n\n(\u22121)r+1\n\ni=0\n\np(\u222an\n\nm\ni\n\n(cid:6)\n\nr=1\n\ni1<\u00b7\u00b7\u00b7<ir\n\n(\u22121)i(1)m\u2212i\n\n\u00b7\u00b7\u00b7 eir\n\n)\n\np(ei1\n\n2. the following is a succinct way of writing the inclusion\u2013exclusion identity:\n\n3. in the inclusion\u2013exclusion identity, going out one term results in an upper bound\non the probability of the union, going out two terms results in a lower bound on the\nprobability, going out three terms results in an upper bound on the probability, going\nout four terms results in a lower bound, and so on. that is, for events e1, . . . , en,\nwe have\n\np(\u222an\n\ni=1ei) \u2026\n\np(\u222an\n\ni=1ei) \u00fa\n\np(\u222an\n\ni=1ei) \u2026\n\ni=1\n\nn(cid:6)\nn(cid:6)\nn(cid:6)\n\ni=1\n\ni=1\n\np(ei)\n\np(ei) \u2212\n\np(ei) \u2212\n\n(cid:6)\n(cid:6)\n\nj<i\n\nj<i\n\np(eiej)\n\np(eiej) +\n\n(cid:6)\n\nk<j<i\n\np(eiejek)\n\n(4.1)\n\n(4.2)\n\n(4.3)\n\nand so on. to prove the validity of these bounds, note the identity\n\u00b7\u00b7\u00b7 ec\n\ni=1ei = e1 \u222a ec\n\u222an\n\n2e3 \u222a \u00b7\u00b7\u00b7 \u222a ec\n\n1e2 \u222a ec\n\n1ec\n\n1\n\nn\u22121en\n\n "}, {"Page_number": 48, "text": "section 2.5\n\nsample spaces having equally likely outcomes 33\n\nthat is, at least one of the events ei occurs if e1 occurs, or if e1 does not occur but\ne2 does, or if e1 and e2 do not occur but e3 does, and so on. because the right-hand\nside is the union of disjoint events, we obtain\n1e2) + p(ec\np(ec\ni\u22121ei)\n1\n\n= p(e1) + n(cid:6)\n\ni=1ei) = p(e1) + p(ec\n\n2e3) + . . . + p(ec\n\n\u00b7\u00b7\u00b7 ec\n\n\u00b7\u00b7\u00b7 ec\n\nn\u22121en)\n\np(\u222an\n\n1ec\n\n1\n\n(4.4)\ni=2\n= (\u222aj<iej)c be the event that none of the first i \u2212 1 events\np(ei) = p(biei) + p(bc\n\ni ei)\n\nnow, let bi = ec\noccur. applying the identity\n\n\u00b7\u00b7\u00b7 ec\ni\u22121\n\n1\n\nbecause probabilities are always nonnegative, inequality (4.1) follows directly from\nequation (4.5). now, fixing i and applying inequality (4.1) to p(\u222aj<ieiej) yields\n\n(4.5)\n\nshows that\n\nor, equivalently,\n\nsubstituting this equation into (4.4) yields\n\np(ei) = p(ec\n\u00b7\u00b7\u00b7 ec\n\np(ec\n1\n\np(\u222an\n\ni=1ei) =\n\n1\n\n\u00b7\u00b7\u00b7 ec\n\ni\u22121ei) + p(ei \u222aj<i ej)\ni\u22121ei) = p(ei) \u2212 p(\u222aj<ieiej)\n(cid:6)\n\n(cid:6)\n\np(ei) \u2212\n\np(\u222aj<ieiej)\n\ni\n\ni\n\n(cid:6)\n\np(eiej)\n\nj<i\n\n(cid:6)\n(cid:6)\n\nk<j<i\n\np(\u222aj<ieiej) \u2026\n\n(cid:6)\n(cid:6)\n\nj<i\n\np(\u222aj<ieiej) \u00fa\n=\n\np(eiej) \u2212\np(eiej) \u2212\n\np(eiejeiek)\n\np(eiejek)\n\nwhich, by equation (4.5), gives inequality (4.2). similarly, fixing i and applying inequal-\nity (4.2) to p(\u222aj<ieiej) yields\n\nj<i\n\nk<j<i\n\nwhich, by equation (4.5), gives inequality (4.3). the next inclusion\u2013exclusion inequal-\nity is now obtained by fixing i and applying inequality (4.3) to p(\u222aj<ieiej), and\nso on.\n\n2.5 sample spaces having equally likely outcomes\n\nin many experiments, it is natural to assume that all outcomes in the sample space\nare equally likely to occur. that is, consider an experiment whose sample space s is a\nfinite set, say, s = {1, 2, . . . , n}. then it is often natural to assume that\n\np({1}) = p({2}) = \u00b7\u00b7\u00b7 = p({n})\n\nwhich implies, from axioms 2 and 3 (why?), that\n\np({i}) = 1\nn\n\ni = 1, 2, . . . , n\n\n "}, {"Page_number": 49, "text": "34\n\nchapter 2\n\naxioms of probability\n\nfrom this equation, it follows from axiom 3 that, for any event e,\n\np(e) = number of outcomes in e\nnumber of outcomes in s\n\nin words, if we assume that all outcomes of an experiment are equally likely to occur,\nthen the probability of any event e equals the proportion of outcomes in the sample\nspace that are contained in e.\n\nexample 5a\nif two dice are rolled, what is the probability that the sum of the upturned faces will\nequal 7?\n\nsolution. we shall solve this problem under the assumption that all of the 36 possible\noutcomes are equally likely. since there are 6 possible outcomes\u2014namely, (1, 6), (2,\n5), (3, 4), (4, 3), (5, 2), and (6, 1)\u2014that result in the sum of the dice being equal to 7,\n.\nthe desired probability is 6\n36\n\n= 1\n6.\n\nexample 5b\nif 3 balls are \u201crandomly drawn\u201d from a bowl containing 6 white and 5 black balls,\nwhat is the probability that one of the balls is white and the other two black?\n\nsolution. if we regard the order in which the balls are selected as being relevant,\nthen the sample space consists of 11 \u00b7 10 \u00b7 9 = 990 outcomes. furthermore, there are\n6 \u00b7 5 \u00b7 4 = 120 outcomes in which the first ball selected is white and the other two\nare black; 5 \u00b7 6 \u00b7 4 = 120 outcomes in which the first is black, the second is white,\nand the third is black; and 5 \u00b7 4 \u00b7 6 = 120 in which the first two are black and the\nthird is white. hence, assuming that \u201crandomly drawn\u201d means that each outcome in\nthe sample space is equally likely to occur, we see that the desired probability is\n\n120 + 120 + 120\n\n990\n\n= 4\n11\n\n(cid:2)\n\n(cid:3)\n\nthis problem could also have been solved by regarding the outcome of the experi-\n=\nment as the unordered set of drawn balls. from this point of view, there are\n165 outcomes in the sample space. now, each set of 3 balls corresponds to 3! out-\ncomes when the order of selection is noted. as a result, if all outcomes are assumed\nequally likely when the order of selection is noted, then it follows that they remain\nequally likely when the outcome is taken to be the unordered set of selected balls.\nhence, using the latter representation of the experiment, we see that the desired\nprobability is\n\n11\n3\n\n(cid:3)(cid:2)\n\n(cid:2)\n\n(cid:3)\n(cid:3) = 4\n\n5\n2\n\n11\n\n6\n1\n\n(cid:2)\n\n11\n3\n\nwhich, of course, agrees with the answer obtained previously.\n\nwhen the experiment consists of a random selection of k items from a set of n\nitems, we have the flexibility of either letting the outcome of the experiment be the\nordered selection of the k items or letting it be the unordered set of items selected.\nin the former case we would assume that each new selection is equally likely to be\n\n "}, {"Page_number": 50, "text": "(cid:18)\n\n(cid:19)\n\nsection 2.5\n\nsample spaces having equally likely outcomes 35\n\nn\nk\n\nany of the so far unselected items of the set, and in the latter case we would assume\npossible subsets of k items are equally likely to be the set selected. for\nthat all\ninstance, suppose 5 people are to be randomly selected from a group of 20 individuals\nconsisting of 10 married couples, and we want to determine p(n), the probability\nthat the 5 chosen are all unrelated. (that is, no two are married to each other.) if\nequally\nwe regard the sample space as the set of 5 people chosen, then there are\nlikely outcomes. an outcome that does not contain a married couple can be thought\nof as being the result of a six-stage experiment: in the first stage, 5 of the 10 couples\nto have a member in the group are chosen; in the next 5 stages, 1 of the 2 members\n25 possible outcomes in which\nof each of these couples is selected. thus, there are\nthe 5 members selected are unrelated, yielding the desired probability of\n\n(cid:18)\n\n(cid:19)\n\n(cid:18)\n\n(cid:19)\n\n20\n5\n\n10\n5\n\n(cid:2)\n(cid:2)\n\n(cid:3)\n(cid:3)\n\n25\n\n10\n5\n20\n5\n\np(n) =\n\nin contrast, we could let the outcome of the experiment be the ordered selection\nof the 5 individuals. in this setting, there are 20 \u00b7 19 \u00b7 18 \u00b7 17 \u00b7 16 equally likely\noutcomes, of which 20 \u00b7 18 \u00b7 16 \u00b7 14 \u00b7 12 outcomes result in a group of 5 unrelated\nindividuals, yielding the result\n\np(n) = 20 \u00b7 18 \u00b7 16 \u00b7 14 \u00b7 12\n20 \u00b7 19 \u00b7 18 \u00b7 17 \u00b7 16\n\nwe leave it for the reader to verify that the two answers are identical.\n\nexample 5c\na committee of 5 is to be selected from a group of 6 men and 9 women. if the selection\nis made randomly, what is the probability that the committee consists of 3 men and 2\nwomen?\n\nsolution. because each of the\nthe desired probability is\n\npossible committees is equally likely to be selected,\n\n(cid:18)\n\n(cid:19)\n(cid:2)\n\n15\n5\n\n(cid:3)\n(cid:3) = 240\n\n9\n2\n\n1001\n\n(cid:3)(cid:2)\n\n15\n5\n\n6\n3\n\n(cid:2)\n\n.\n\n.\n\nexample 5d\nan urn contains n balls, one of which is special. if k of these balls are withdrawn one\nat a time, with each selection being equally likely to be any of the balls that remain at\nthe time, what is the probability that the special ball is chosen?\n\nsolution. since all of the balls are treated in an identical manner, it follows that the\nsets of k balls. therefore,\nset of k balls selected is equally likely to be any of the\n\n(cid:2)\n\n(cid:3)(cid:2)\n(cid:2)\n\n1\n1\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\nn\nk\nn \u2212 1\n(cid:3)\nk \u2212 1\nn\nk\n\n= k\nn\n\np{special ball is selected} =\n\n "}, {"Page_number": 51, "text": "36\n\nchapter 2\n\naxioms of probability\n\nwe could also have obtained this result by letting ai denote the event that the special\nball is the ith ball to be chosen, i = 1, . . . , k. then, since each one of the n balls is\nequally likely to be the ith ball chosen, it follows that p(ai) = 1/n. hence, because\nthese events are clearly mutually exclusive, we have\n\np{special ball is selected} = p\n\n\u239b\n\u239d k(cid:14)\n\ni=1\n\n\u239e\n\u23a0 = k(cid:6)\n\ni=1\n\nai\n\np(ai) = k\nn\n\nwe could also have argued that p(ai) = 1/n, by noting that there are n(n \u2212 1)\u00b7\u00b7\u00b7 (n \u2212\nk + 1) = n!/(n \u2212 k)! equally likely outcomes of the experiment, of which (n \u2212 1)(n \u2212\n2)\u00b7\u00b7\u00b7 (n \u2212 i + 1)(1)(n \u2212 i)\u00b7\u00b7\u00b7 (n \u2212 k + 1) = (n \u2212 1)!/(n \u2212 k)! result in the special\nball being the ith one chosen. from this reasoning, it follows that\n\np(ai) = (n \u2212 1)!\n\nn!\n\n= 1\nn\n\n.\n\nexample 5e\nsuppose that n + m balls, of which n are red and m are blue, are arranged in a linear\norder in such a way that all (n + m)! possible orderings are equally likely. if we\nrecord the result of this experiment by listing only the colors of the successive balls,\nshow that all the possible results remain equally likely.\nsolution. consider any one of the (n + m)! possible orderings, and note that any per-\nmutation of the red balls among themselves and of the blue balls among themselves\ndoes not change the sequence of colors. as a result, every ordering of colorings cor-\nresponds to n! m! different orderings of the n + m balls, so every ordering of the\ncolors has probability n!m!\n\n(n+m)! of occurring.\n\nfor example, suppose that there are 2 red balls, numbered r1, r2, and 2 blue balls,\nnumbered b1, b2. then, of the 4! possible orderings, there will be 2! 2! orderings that\nresult in any specified color combination. for instance, the following orderings result\nin the successive balls alternating in color, with a red ball first:\n\nr1, b1, r2, b2\n\nr1, b2, r2, b1\n\nr2, b1, r1, b2\n\nr2, b2, r1, b1\n\ntherefore, each of the possible orderings of the colors has probability 4\n24\noccurring.\n\n= 1\n\n6 of\n.\n\nexample 5f\na poker hand consists of 5 cards. if the cards have distinct consecutive values and\nare not all of the same suit, we say that the hand is a straight. for instance, a hand\nconsisting of the five of spades, six of spades, seven of spades, eight of spades, and\nnine of hearts is a straight. what is the probability that one is dealt a straight?\n\nsolution. we start by assuming that all\npossible poker hands are equally\nlikely. to determine the number of outcomes that are straights, let us first determine\nthe number of possible outcomes for which the poker hand consists of an ace, two,\nthree, four, and five (the suits being irrelevant). since the ace can be any 1 of the 4\npossible aces, and similarly for the two, three, four, and five, it follows that there are\n45 outcomes leading to exactly one ace, two, three, four, and five. hence, since in 4 of\nthese outcomes all the cards will be of the same suit (such a hand is called a straight\n\n(cid:2)\n\n(cid:3)\n\n52\n5\n\n "}, {"Page_number": 52, "text": "section 2.5\n\nsample spaces having equally likely outcomes 37\nflush), it follows that there are 45 \u2212 4 hands that make up a straight of the form ace,\ntwo, three, four, and five. similarly, there are 45 \u2212 4 hands that make up a straight\nof the form ten, jack, queen, king, and ace. thus, there are 10(45 \u2212 4) hands that are\nstraights, and it follows that the desired probability is\n\n(cid:3)\n(cid:2)\n10(45 \u2212 4)\n\n52\n5\n\nl .0039\n\n.\n\nexample 5g\na 5-card poker hand is said to be a full house if it consists of 3 cards of the same\ndenomination and 2 other cards of the same denomination (of course, different from\nthe first denomination). thus, one kind of full house is three of a kind plus a pair.\nwhat is the probability that one is dealt a full house?\n\nsolution. again, we assume that all\n\npossible hands are equally likely. to\n\ndetermine the number of possible full houses, we first note that there are\ndifferent combinations of, say, 2 tens and 3 jacks. because there are 13 different\nchoices for the kind of pair and, after a pair has been chosen, there are 12 other\nchoices for the denomination of the remaining 3 cards, it follows that the probability\nof a full house is\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)(cid:2)\n\n(cid:3)\n\n4\n2\n\n4\n3\n\n(cid:2)\n\n(cid:3)\n\n52\n5\n\n(cid:3)(cid:2)\n(cid:3)\n\n4\n2\n\n4\n3\n\nl .0014\n\n.\n\n(cid:2)\n13 \u00b7 12 \u00b7\n\n52\n5\n\nexample 5h\nin the game of bridge, the entire deck of 52 cards is dealt out to 4 players. what is the\nprobability that\n(a) one of the players receives all 13 spades;\n(b) each player receives 1 ace?\n\nsolution. (a) letting ei be the event that hand i has all 13 spades, then\n\np(ei) = 1(cid:18)\n\n(cid:19) ,\n\n52\n13\n\ni = 1, 2, 3, 4\n\ni=1ei) = 4(cid:6)\n\ni=1\n\np(\u222a4\n\n(cid:2)\np(ei) = 4/\n\n(cid:3)\n\n52\n13\n\nbecause the events ei, i = 1, 2, 3, 4, are mutually exclusive, the probability that one\nof the hands is dealt all 13 spades is\n\n(b) to determine the number of outcomes in which each of the distinct players\npos-\nreceives exactly 1 ace, put aside the aces and note that there are\nsible divisions of the other 48 cards when each player is to receive 12. because there\n\n12, 12, 12, 12\n\n48\n\nl 6.3 * 10\n\n\u221212\n\n(cid:2)\n\n(cid:3)\n\n "}, {"Page_number": 53, "text": "38\n\nchapter 2\n\naxioms of probability\n\n(cid:2)\n\n(cid:3)\n\n.\n\n.\n\nare 4! ways of dividing the 4 aces so that each player receives 1, we see that the num-\nber of possible outcomes in which each player receives exactly 1 ace is 4!\n\n48\n\n12, 12, 12, 12\n\n(cid:18)\n\n(cid:19)\n\nas there are\n\n52\n\n13,13,13,13\n\npossible hands, the desired probability is thus\n\n(cid:18)\n\n(cid:18)\n\n4!\n\n12,12,12,12\n\n48\n\n52\n\n13,13,13,13\n\n(cid:19)\n(cid:19) l .1055\n\nsome results in probability are quite surprising when initially encountered. our\n\nnext two examples illustrate this phenomenon.\n\nexample 5i\nif n people are present in a room, what is the probability that no two of them cele-\nbrate their birthday on the same day of the year? how large need n be so that this\nprobability is less than 1\n2?\n\nsolution. as each person can celebrate his or her birthday on any one of 365 days,\nthere is a total of (365)n possible outcomes. (we are ignoring the possibility of some-\none\u2019s having been born on february 29.) assuming that each outcome is equally\nlikely, we see that the desired probability is (365)(364)(363) . . . (365 \u2212 n + 1)/(365)n.\nit is a rather surprising fact that when n \u00fa 23, this probability is less than 1\n2. that is, if\nthere are 23 or more people in a room, then the probability that at least two of them\nhave the same birthday exceeds 1\n2. many people are initially surprised by this result,\nsince 23 seems so small in relation to 365, the number of days of the year. however,\n= 1\nof having the same birthday,\nevery pair of individuals has probability\n365\n= 253 different pairs of individuals.\n\nand in a group of 23 people there are\nlooked at this way, the result no longer seems so surprising.\n\n365\n(365)2\n23\n2\n\n(cid:2)\n\n(cid:3)\n\nwhen there are 50 people in the room, the probability that at least two share the\nsame birthday is approximately .970, and with 100 persons in the room, the odds are\n\nbetter than 3,000,000:1. (that is, the probability is greater than\nleast two people have the same birthday.)\n\n3 * 106\n\n3 * 106 + 1\n\nthat at\n.\n\nexample 5j\na deck of 52 playing cards is shuffled, and the cards are turned up one at a time until\nthe first ace appears. is the next card\u2014that is, the card following the first ace\u2014more\nlikely to be the ace of spades or the two of clubs?\n\nsolution. to determine the probability that the card following the first ace is the ace\nof spades, we need to calculate how many of the (52)! possible orderings of the cards\nhave the ace of spades immediately following the first ace. to begin, note that each\nordering of the 52 cards can be obtained by first ordering the 51 cards different from\nthe ace of spades and then inserting the ace of spades into that ordering. furthermore,\nfor each of the (51)! orderings of the other cards, there is only one place where the\nace of spades can be placed so that it follows the first ace. for instance, if the ordering\nof the other 51 cards is\n\n4c, 6h, jd, 5s, ac, 7d, . . . , kh\n\n "}, {"Page_number": 54, "text": "section 2.5\n\nsample spaces having equally likely outcomes 39\n\nthen the only insertion of the ace of spades into this ordering that results in its follow-\ning the first ace is\n\n4c, 6h, jd, 5s, ac, as, 7d, . . . , kh\n\ntherefore, there are (51)! orderings that result in the ace of spades following the first\nace, so\n\np{the ace of spades follows the first ace} = (51)!\n(52)!\n\n= 1\n52\n\nin fact, by exactly the same argument, it follows that the probability that the two of\nclubs (or any other specified card) follows the first ace is also 1\n52. in other words, each\nof the 52 cards of the deck is equally likely to be the one that follows the first ace!\n\nmany people find this result rather surprising. indeed, a common reaction is to\nsuppose initially that it is more likely that the two of clubs (rather than the ace of\nspades) follows the first ace, since that first ace might itself be the ace of spades. this\nreaction is often followed by the realization that the two of clubs might itself appear\nbefore the first ace, thus negating its chance of immediately following the first ace.\nhowever, as there is one chance in four that the ace of spades will be the first ace\n(because all 4 aces are equally likely to be first) and only one chance in five that\nthe two of clubs will appear before the first ace (because each of the set of 5 cards\nconsisting of the two of clubs and the 4 aces is equally likely to be the first of this set\nto appear), it again appears that the two of clubs is more likely. however, this is not\n.\nthe case, and a more complete analysis shows that they are equally likely.\n\nexample 5k\na football team consists of 20 offensive and 20 defensive players. the players are to\nbe paired in groups of 2 for the purpose of determining roommates. if the pairing is\ndone at random, what is the probability that there are no offensive\u2013defensive room-\nmate pairs? what is the probability that there are 2i offensive\u2013defensive roommate\npairs, i = 1, 2, . . . , 10?\nsolution. there are\n\n(cid:2)\n\n(cid:3)\n\n40\n\n2, 2, . . . , 2\n\n= (40)!\n(2!)20\n\nways of dividing the 40 players into 20 ordered pairs of two each. [that is, there\nare (40)!/220 ways of dividing the players into a first pair, a second pair, and so on.]\nhence, there are (40)!/220(20)! ways of dividing the players into (unordered) pairs of\n2 each. furthermore, since a division will result in no offensive\u2013defensive pairs if the\noffensive (and defensive) players are paired among themselves, it follows that there\nare [(20)!/210(10)!]2 such divisions. hence, the probability of no offensive\u2013defensive\nroommate pairs, call it p0, is given by\n\n(cid:2)\n\n(cid:3)2\n\n(20)!\n\n210(10)!\n(40)!\n\n220(20)!\n\n= [(20)!]3\n\n[(10)!]2(40)!\n\np0 =\n(cid:3)2\n\n(cid:2)\n\n20\n2i\n\nto determine p2i, the probability that there are 2i offensive\u2013defensive pairs, we first\n\nways of selecting the 2i offensive players and the 2i defen-\nnote that there are\nsive players who are to be in the offensive\u2013defensive pairs. these 4i players can then\n\n "}, {"Page_number": 55, "text": "40\n\nchapter 2\n\naxioms of probability\n\nbe paired up into (2i)! possible offensive\u2013defensive pairs. (this is so because the first\noffensive player can be paired with any of the 2i defensive players, the second offen-\nsive player with any of the remaining 2i \u2212 1 defensive players, and so on.) as the\nremaining 20 \u2212 2i offensive (and defensive) players must be paired among them-\nselves, it follows that there are(cid:2)\n\n(cid:20)\n\n(cid:21)2\n\ndivisions which lead to 2i offensive\u2013defensive pairs. hence,\n\n20\n2i\n\n(2i)!\n\n(cid:3)2\n(cid:20)\n\n(20 \u2212 2i)!\n210\u2212i(10 \u2212 i)!\n(cid:21)2\n\n(20 \u2212 2i)!\n210\u2212i(10 \u2212 i)!\n(40)!\n\n220(20)!\n\n(cid:2)\n\n(cid:3)2\n\n20\n2i\n\n(2i)!\n\np2i =\n\ni = 0, 1, . . . , 10\n\nthe p2i, i = 0, 1, . . . , 10, can now be computed, or they can be approximated by mak-\n\u221a\ning use of a result of stirling which shows that n! can be approximated by nn+1/2e\n2\u03c0.\nfor instance, we obtain\n\n\u2212n\n\n\u22126\n\np0 l 1.3403 * 10\np10 l .345861\np20 l 7.6068 * 10\n\n\u22126\n\n.\n\nour next three examples illustrate the usefulness of proposition 4.4. in example 5l,\nthe introduction of probability enables us to obtain a quick solution to a counting\nproblem.\n\nexample 5l\na total of 36 members of a club play tennis, 28 play squash, and 18 play badminton.\nfurthermore, 22 of the members play both tennis and squash, 12 play both tennis and\nbadminton, 9 play both squash and badminton, and 4 play all three sports. how many\nmembers of this club play at least one of three sports?\n\nsolution. let n denote the number of members of the club, and introduce probabil-\nity by assuming that a member of the club is randomly selected. if, for any subset c\nof members of the club, we let p(c) denote the probability that the selected member\nis contained in c, then\n\np(c) = number of members inc\n\nn\n\nnow, with t being the set of members that plays tennis, s being the set that plays\nsquash, and b being the set that plays badminton, we have, from proposition 4.4,\np(t \u222a s \u222a b)\n\n= p(t) + p(s) + p(b) \u2212 p(ts) \u2212 p(tb) \u2212 p(sb) + p(tsb)\n= 36 + 28 + 18 \u2212 22 \u2212 12 \u2212 9 + 4\n= 43\nn\n\nn\n\nhence, we can conclude that 43 members play at least one of the sports.\n\n.\n\n "}, {"Page_number": 56, "text": "section 2.5\n\nsample spaces having equally likely outcomes 41\n\nthe next example in this section not only possesses the virtue of giving rise to a\n\nsomewhat surprising answer, but is also of theoretical interest.\n\nexample 5m the matching problem\nsuppose that each of n men at a party throws his hat into the center of the room.\nthe hats are first mixed up, and then each man randomly selects a hat. what is the\nprobability that none of the men selects his own hat?\n\nsolution. we first calculate the complementary probability of at least one man\u2019s\nselecting his own hat. let us denote by ei, i = 1, 2, . . . , n the event that the ith man\nselects his own hat. now, by proposition 4.4 p\n, the probability that at least\n\nn(cid:10)\n\none of the men selects his own hat is given by\n\n\u239e\n\u23a0 = n(cid:6)\n\n\u239b\n\u239d n(cid:14)\n\ni=1\n\np\n\nei\n\n(cid:4)\n\n(cid:5)\n\nei\n\ni=1\n\n(cid:6)\n(cid:6)\n\ni1<i2\n\np(ei1ei2\n\n) + \u00b7\u00b7\u00b7\n\np(ei) \u2212\ni=1\n+ (\u22121)n+1\n\u00b7\u00b7\u00b7 ein\n+ \u00b7\u00b7\u00b7 + (\u22121)n+1p(e1e2 \u00b7\u00b7\u00b7 en)\n\ni1<i2\u00b7\u00b7\u00b7<in\n\np(ei1ei2\n\n)\n\nif we regard the outcome of this experiment as a vector of n numbers, where the ith\nelement is the number of the hat drawn by the ith man, then there are n! possible\noutcomes. [the outcome (1, 2, 3, . . . , n) means, for example, that each man selects\n. . . ein, the event that each of the n men i1, i2, . . . , in\nhis own hat.] furthermore, ei1ei2\nselects his own hat, can occur in any of (n \u2212 n)(n \u2212 n \u2212 1)\u00b7\u00b7\u00b7 3 \u00b7 2 \u00b7 1 = (n \u2212 n)!\npossible ways; for, of the remaining n \u2212 n men, the first can select any of n \u2212 n\nhats, the second can then select any of n \u2212 n \u2212 1 hats, and so on. hence, assuming\nthat all n! possible outcomes are equally likely, we see that\n\nalso, as there are\n\n(cid:2)\n\n(cid:3)\n(cid:6)\n\nn\nn\n\np(ei1ei2\n\nterms in\n\np(ei1ei2\n\ni1<i2\u00b7\u00b7\u00b7<in\n\n\u00b7\u00b7\u00b7 ein\n(cid:9)\n\ni1<i2\u00b7\u00b7\u00b7<in\n\u00b7\u00b7\u00b7 ein\n\n) = (n \u2212 n)!\n\nn!\n\u00b7\u00b7\u00b7 ein\np(ei1ei2\n) = n!(n \u2212 n)!\n(n \u2212 n)!n!n!\n\n= 1\nn!\n\nthus,\n\np\n\n\u239b\n\u239d n(cid:14)\n\ni=1\n\n\u239e\n\u23a0 = 1 \u2212 1\n\n2!\n\n+ 1\n3!\n\n\u2212 \u00b7\u00b7\u00b7 + (\u22121)n+1 1\nn!\n\nei\n\n), it follows that\n\nhence, the probability that none of the men selects his own hat is\n\n1 \u2212 1 + 1\n2!\n\n\u2212 1\n3!\n\n+ \u00b7\u00b7\u00b7 + (\u22121)n\n\nn!\n\n "}, {"Page_number": 57, "text": "42\n\nchapter 2\n\naxioms of probability\n\n\u22121 l .36788 for n large. in other words, for n large,\nwhich is approximately equal to e\nthe probability that none of the men selects his own hat is approximately .37. (how\nmany readers would have incorrectly thought that this probability would go to 1 as\nn\u2192q?)\n.\n\nfor another illustration of the usefulness of proposition 4.4, consider the following\n\nexample.\n\nexample 5n\ncompute the probability that 10 married couples are seated at random at a round\ntable, then no wife sits next to her husband.\nsolution. if we let ei, i = 1, 2, . . . , 10 denote the event that the ith couple sit next\nto each other, it follows that the desired probability is 1 \u2212 p\n. now, from\nproposition 4.4,\n\n10(cid:10)\n\n(cid:4)\n\n(cid:5)\n\ni=1\n\nei\n\n\u239b\n\u239d 10(cid:14)\n\n\u239e\n\u23a0 = 10(cid:6)\n\np\n\nei\n\n1\n\n1\n\np(ei) \u2212 \u00b7\u00b7\u00b7 + (\u22121)n+1\n+ \u00b7\u00b7\u00b7 \u2212 p(e1e2 \u00b7\u00b7\u00b7 e10)\n\u00b7\u00b7\u00b7 ein\n\n(cid:6)\n\ni1<i2<\u00b7\u00b7\u00b7<in\n\np(ei1ei2\n\n\u00b7\u00b7\u00b7 ein\n\n)\n\nto compute p(ei1ei2\n\n), we first note that there are 19! ways of arranging 20\npeople around a round table. (why?) the number of arrangements that result in a\nspecified set of n men sitting next to their wives can most easily be obtained by first\nthinking of each of the n married couples as being single entities. if this were the case,\nthen we would need to arrange 20 \u2212 n entities around a round table, and there are\nclearly (20 \u2212 n \u2212 1)! such arrangements. finally, since each of the n married couples\ncan be arranged next to each other in one of two possible ways, it follows that there\nare 2n(20 \u2212 n \u2212 1)! arrangements that result in a specified set of n men each sitting\nnext to their wives. therefore,\n\np(ei1ei2\n\n\u00b7\u00b7\u00b7 ein\n\n) = 2n(19 \u2212 n)!\n\n(19)!\n\nthus, from proposition 4.4, we obtain that the probability that at least one married\ncouple sits together, namely,\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\n10\n1\n\n21\n\n(18)!\n(19)!\n\n\u2212\n\n10\n2\n\n22\n\n(17)!\n(19)!\n\n+\n\n10\n3\n\n23\n\n(16)!\n(19)!\n\n\u2212 \u00b7\u00b7\u00b7 \u2212\n\n10\n10\n\nand the desired probability is approximately .3395.\n\u2217\n\nexample 5o runs\nconsider an athletic team that had just finished its season with a final record of n\nwins and m losses. by examining the sequence of wins and losses, we are hoping to\ndetermine whether the team had stretches of games in which it was more likely to\nwin than at other times. one way to gain some insight into this question is to count\nthe number of runs of wins and then see how likely that result would be when all\n\n210 9!\n(19)!\n\nl .6605\n\n.\n\n "}, {"Page_number": 58, "text": "(cid:3)\n\n(cid:2)\n\nsection 2.5\n\nsample spaces having equally likely outcomes 43\n(n + m)!/(n! m!) orderings of the n wins and m losses are assumed equally likely. by\na run of wins, we mean a consecutive sequence of wins. for instance, if n = 10, m = 6,\nand the sequence of outcomes was wwllwwwlwlllwwww, then there would\nbe 4 runs of wins\u2014the first run being of size 2, the second of size 3, the third of size 1,\nand the fourth of size 4.\nsuppose now that a team has n wins and m losses. assuming that all (n + m)!/\n(n! m!) =\norderings are equally likely, let us determine the probability\nthat there will be exactly r runs of wins. to do so, consider first any vector of positive\nintegers x1, x2, . . . , xr with x1 + \u00b7\u00b7\u00b7 + xr = n, and let us see how many outcomes result\nin r runs of wins in which the ith run is of size xi, i = 1, . . . , r. for any such outcome,\nif we let y1 denote the number of losses before the first run of wins, y2 the number of\nlosses between the first 2 runs of wins, . . . , yr+1 the number of losses after the last run\nof wins, then the yi satisfy\n\nn + m\n\nn\n\nand the outcome can be represented schematically as\n\ny1 + y2 + \u00b7\u00b7\u00b7 + yr+1 = m\n(cid:25)\n\nww . . . w\n\nll . . . l\n\n(cid:23)(cid:24)\n\n(cid:23)(cid:24)\n\n(cid:22)\n\n(cid:25)\n\n(cid:22)\n\ny1\n\nx1\n\ny1 \u00fa 0, yr+1 \u00fa 0, yi > 0, i = 2, . . . , r\n(cid:22)\n\n\u00b7\u00b7\u00b7 ww(cid:22)(cid:23)(cid:24)(cid:25)\n\n(cid:22) (cid:23)(cid:24) (cid:25)\n\nww . . . w\n\nl . . . l\n\n(cid:23)(cid:24)\n\n(cid:25)\n\nx2\n\nxr\n\nyr+1\n\n(cid:22) (cid:23)(cid:24) (cid:25)\n\nl . . . l\n\ny2\n\nhence, the number of outcomes that result in r runs of wins\u2014the ith of size xi, i =\n1, . . . r\u2014is equal to the number of integers y1, . . . , yr+1 that satisfy the foregoing, or,\nequivalently, to the number of positive integers\n\ny1 = y1 + 1 yi\n\n= yi, i = 2, . . . , r, yr+1 = yr+1 + 1\n\nthat satisfy\n\n(cid:3)\ny1 + y2 + \u00b7\u00b7\u00b7 + yr+1 = m + 2\n(cid:2)\n\n(cid:2)\n\n(cid:3)\n\nm + 1\n\nr\n\nm + 1\n(cid:3)\n\nby proposition 6.1 in chapter 1, there are\n\nsuch outcomes. hence, the\n\n, multiplied by\ntotal number of outcomes that result in r runs of wins is\nthe number of positive integral solutions of x1 + \u00b7\u00b7\u00b7 + xr = n. thus, again from\nproposition 6.1, there are\noutcomes resulting in r runs of wins.\n\nm + 1\n\nr\n\n(cid:3)(cid:2)\n\n(cid:2)\n\nn \u2212 1\nr \u2212 1\n\nas there are\n\nequally likely outcomes, it follows that\n\n(cid:3)\n\nr\n\n(cid:2)\n\nn + m\n\nn\n\n(cid:2)\n\n(cid:3)(cid:2)\n\n(cid:3)\n\nm + 1\n(cid:2)\n\nr\n\nn \u2212 1\n(cid:3)\nr \u2212 1\n\nm + n\n\nn\n\np({r runs of wins}) =\n\nr \u00fa 1\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\nfor example, if n = 8 and m = 6, then the probability of 7 runs is\n14\n8\n\noutcomes are equally likely. hence, if the outcome\nwas wlwlwlwlwwlwlw, then we might suspect that the team\u2019s probability\nof winning was changing over time. (in particular, the probability that the team wins\n\n= 1/429 if all\n\n14\n8\n\n(cid:3)(cid:26)\n\n(cid:3) (cid:2)\n\n(cid:2)\n\n7\n7\n\n7\n6\n\n "}, {"Page_number": 59, "text": "44\n\nchapter 2\n\naxioms of probability\n\nseems to be quite high when it lost its last game and quite low when it won its last\ngame.) on the other extreme, if the outcome were wwwwwwwwllllll, then\nthere would have been only 1 run, and as p({1 run}) =\n= 1/429,\nit would thus again seem unlikely that the team\u2019s probability of winning remained\n.\nunchanged over its 14 games.\n\n14\n8\n\n7\n1\n\n7\n0\n\n(cid:2)\n\n(cid:3)(cid:2)\n\n(cid:3)(cid:26)(cid:2)\n\n(cid:3)\n\n\u22172.6 probability as a continuous set function\n\na sequence of events {en, n \u00fa 1} is said to be an increasing sequence if\n\ne1 ( e2 ( \u00b7\u00b7\u00b7 ( en ( en+1 ( \u00b7\u00b7\u00b7\n\nwhereas it is said to be a decreasing sequence if\n\ne1 ) e2 ) \u00b7\u00b7\u00b7 ) en ) en+1 ) \u00b7\u00b7\u00b7\n\nif {en, n \u00fa 1} is an increasing sequence of events, then we define a new event, denoted\nby lim\n\nn\u2192q en, by\n\nq(cid:14)\n\ni=1\n\nq(cid:17)\n\nn\u2192q en =\n\nlim\n\nei\n\nn\u2192q en =\n\nlim\n\nei\n\ni=1\n\nsimilarly, if {en, n \u00fa 1} is a decreasing sequence of events, we define lim\n\nen by\n\nn\n\nwe now prove the following proposition 1:\n\nproposition 6.1.\nif {en, n \u00fa 1} is either an increasing or a decreasing sequence of events, then\n\nproof. suppose, first, that {en, n \u00fa 1} is an increasing sequence, and define the\nevents fn, n \u00fa 1, by\n\nn\u2192q p(en) = p( lim\n\nn\u2192q en)\n\nlim\n\nf1 = e1\nfn = en\n\n\u239e\n\u23a0c\n\n\u239b\n\u239dn\u22121(cid:14)\nn\u22121(cid:10)\n\n1\n\nei\n\n= enec\n\nn\u22121 n > 1\n\nwhere we have used the fact that\n\nei = en\u22121, since the events are increasing.\nin words, fn consists of those outcomes in en which are not in any of the earlier\nn(cid:14)\nei, i < n. it is easy to verify that the fn are mutually exclusive events such that\n\nq(cid:14)\n\nq(cid:14)\n\n1\n\nfi =\n\ni=1\n\nei\n\nand\n\ni=1\n\ni=1\n\nei\n\nfor all n \u00fa 1\n\nfi = n(cid:14)\n\ni=1\n\n "}, {"Page_number": 60, "text": "section 2.6\n\nprobability as a continuous set function 45\n\nthus,\n\n\u239b\n\u239d q(cid:14)\n\n1\n\np\n\nei\n\n\u239e\n\u23a0\n\nfi\n\n\u239e\n\u239b\n\u239d q(cid:14)\n\u23a0 = p\nq(cid:6)\n\n1\n\n=\n\np(fi)\n\n(by axiom 3)\n\n1\n\n= lim\nn\u2192q\n\n= lim\n\nn\u2192q p\n\np(fi)\n\n1\n\nn(cid:6)\n\u239b\n\u239d n(cid:14)\n\u239b\n\u239d n(cid:14)\n\n1\n\n\u239e\n\u23a0\n\u239e\n\u23a0\n\nfi\n\nei\n\n= lim\n= lim\n\nn\u2192q p\nn\u2192q p(en)\n\n1\n\nn, n \u00fa 1} is an increasing sequence;\n\nwhich proves the result when {en, n \u00fa 1} is increasing.\n\nhence, from the preceding equations,\n\nif {en, n \u00fa 1} is a decreasing sequence, then {ec\n\u239e\n\u23a0 = lim\n\np\n\nec\ni\n\nn\u2192q p(ec\n\nn\n\n)\n\nhowever, because\n\nq(cid:10)\n\n1\n\n=\n\nec\ni\n\n1\n\n\u239b\n\u239d q(cid:14)\n(cid:4)\n(cid:5)c\nq(cid:11)\n\u239b\n\u239b\n\u239d q(cid:17)\n\u239c\u239d\n\nei\n\n1\n\np\n\nei\n\n1\n\n, it follows that\n\n\u239e\n\u23a0c\n\n\u239e\n\u239f\u23a0 = lim\n\nn\u2192q p(ec\n\nn\n\n)\n\nor, equivalently,\n\n1 \u2212 p\n\n\u239b\n\u239d q(cid:17)\n\n1\n\nei\n\nor\n\n\u239e\n\u23a0 = lim\nn\u2192q[1 \u2212 p(en)] = 1 \u2212 lim\n\u239b\n\u239d q(cid:17)\n\n\u239e\n\u23a0 = lim\n\nn\u2192q p(en)\n\nei\n\np\n\nn\u2192q p(en)\n\nwhich proves the result.\n\n1\n\n "}, {"Page_number": 61, "text": "46\n\nchapter 2\n\naxioms of probability\n\nexample 6a probability and a paradox\nsuppose that we possess an infinitely large urn and an infinite collection of balls\nlabeled ball number 1, number 2, number 3, and so on. consider an experiment per-\nformed as follows: at 1 minute to 12 p.m., balls numbered 1 through 10 are placed\nin the urn and ball number 10 is withdrawn. (assume that the withdrawal takes\nno time.) at 1\n2 minute to 12 p.m., balls numbered 11 through 20 are placed in the\nurn and ball number 20 is withdrawn. at 1\n4 minute to 12 p.m., balls numbered 21\nthrough 30 are placed in the urn and ball number 30 is withdrawn. at 1\n8 minute\nto 12 p.m., and so on. the question of interest is, how many balls are in the urn at\n12 p.m.?\n\nthe answer to this question is clearly that there is an infinite number of\nballs in the urn at 12 p.m., since any ball whose number is not of the form 10n,\nn \u00fa 1, will have been placed in the urn and will not have been withdrawn before\n12 p.m. hence,\nis performed as\ndescribed.\n\nthe problem is solved when the experiment\n\nhowever, let us now change the experiment and suppose that at 1 minute to 12 p.m.\nballs numbered 1 through 10 are placed in the urn and ball number 1 is withdrawn; at\n1\n2 minute to 12 p.m., balls numbered 11 through 20 are placed in the urn and ball num-\nber 2 is withdrawn; at 1\n4 minute to 12 p.m., balls numbered 21 through 30 are placed\nin the urn and ball number 3 is withdrawn; at 1\n8 minute to 12 p.m., balls numbered 31\nthrough 40 are placed in the urn and ball number 4 is withdrawn, and so on. for this\nnew experiment, how many balls are in the urn at 12 p.m.?\n\nsurprisingly enough, the answer now is that the urn is empty at 12 p.m. for, consider\n\nany ball\u2014say, ball number n. at some time prior to 12 p.m. [in particular, at\nminutes to 12 p.m.], this ball would have been withdrawn from the urn. hence, for\neach n, ball number n is not in the urn at 12 p.m.; therefore, the urn must be empty at\nthat time.\nwe see then, from the preceding discussion that the manner in which the balls are\nwithdrawn makes a difference. for, in the first case only balls numbered 10n, n \u00fa 1,\nare ever withdrawn, whereas in the second case all of the balls are eventually with-\ndrawn. let us now suppose that whenever a ball is to be withdrawn, that ball is\nrandomly selected from among those present. that is, suppose that at 1 minute to\n12 p.m. balls numbered 1 through 10 are placed in the urn and a ball is randomly\nselected and withdrawn, and so on. in this case, how many balls are in the urn at\n12 p.m.?\n\n(cid:29)\n\n(cid:30)n\u22121\n\n1\n2\n\nsolution. we shall show that, with probability 1, the urn is empty at 12 p.m. let us\nfirst consider ball number 1. define en to be the event that ball number 1 is still in\nthe urn after the first n withdrawals have been made. clearly,\n\np(en) =\n\n9 \u00b7 18 \u00b7 27\u00b7\u00b7\u00b7 (9n)\n\n10 \u00b7 19 \u00b7 28\u00b7\u00b7\u00b7 (9n + 1)\n\n[to understand this equation, just note that if ball number 1 is still to be in the\nurn after the first n withdrawals, the first ball withdrawn can be any one of 9, the\nsecond any one of 18 (there are 19 balls in the urn at the time of the second with-\ndrawal, one of which must be ball number 1), and so on. the denominator is similarly\nobtained.]\n\n "}, {"Page_number": 62, "text": "section 2.6\n\nprobability as a continuous set function 47\n\nq(cid:11)\n\nnow, the event that ball number 1 is in the urn at 12 p.m. is just the event\n\nen.\nbecause the events en, n \u00fa 1, are decreasing events, it follows from proposition 6.1 that\n\nn=1\n\np{ball number 1 is in the urn at 12 p.m.}\n\nwe now show that\n\nsince\n\n(cid:2)\n\nq(cid:31)\n\nn=1\n\nthis is equivalent to showing that\n\n(cid:2)\n\nq(cid:31)\n\nnow, for all m \u00fa 1,\n1 + 1\n9n\n\nn=1\n\n(cid:3)\n\nen\n\n\u239e\n\u23a0\n\n\u239b\n\u239d q(cid:17)\n= p\n(cid:2)\n= lim\nq(cid:31)\nn\u2192q p(en)\n=\n9n\n9n + 1\n\nn=1\n\nn=1\n\n(cid:3)\n\n(cid:3)\u23a4\n\u23a6\u22121\n\nq(cid:31)\n\nn=1\n\n(cid:3)\n\n9n\n\n9n + 1\n(cid:2)\nq(cid:31)\n\nn=1\n\n= 0\n(cid:2)\n\n9n\n\n9n + 1\n\u23a1\n\u23a3 q(cid:31)\n(cid:3)\n\nn=1\n\n=\n\n9n + 1\n\n9n\n\n= q\n\n1 + 1\n9n\n(cid:3)\n\n(cid:2)\n1 + 1\n(cid:3)(cid:2)\n(cid:3)(cid:2)\n9n\n1 + 1\n1 + 1\n18\n27\n+ \u00b7\u00b7\u00b7 + 1\n+ 1\n27\n9m\n\n\u00fa\n\n=\n\nm(cid:31)\n(cid:2)\nn=1\n1 + 1\n9\n+ 1\n1\nm(cid:6)\n9\n18\n= 1\n1\n9\ni\n\n>\n\ni=1\n\nq(cid:9)\n(cid:3)\n\ni=1\n\n1/i = q yields\n\n= q\n\n(cid:2)\n\nq(cid:31)\n\nn=1\n\n1 + 1\n9n\n\nhence, letting m\u2192q and using the fact that\n\n(cid:3)\n\n(cid:2)\n\n\u00b7\u00b7\u00b7\n\n(cid:3)\n\n1 + 1\n9m\n\nthus, letting fi denote the event that ball number i is in the urn at 12 p.m., we have\nshown that p(f1) = 0. similarly, we can show that p(fi) = 0 for all i.\n\n "}, {"Page_number": 63, "text": "48\n\nchapter 2\n\naxioms of probability\n\n(for instance, the same reasoning shows that p(fi) = q$\n\nn=2\n\n(cid:5)\n[9n/(9n + 1)] for i =\nq(cid:10)\n\n(cid:4)\n\n11, 12, . . . , 20.) therefore, the probability that the urn is not empty at 12 p.m., p\n\nfi\n\n,\n\nsatisfies\n\np\n\n\u239e\n\u23a0 \u2026\n\n\u239b\n\u239d q(cid:14)\n\nfi\n\n1\n\n1\n\nq(cid:6)\n\n1\n\np(fi) = 0\n\nby boole\u2019s inequality. (see self-test exercise 14.)\n\nthus, with probability 1, the urn will be empty at 12 p.m.\n\n.\n\n2.7 probability as a measure of belief\n\nthus far we have interpreted the probability of an event of a given experiment as\nbeing a measure of how frequently the event will occur when the experiment is con-\ntinually repeated. however, there are also other uses of the term probability. for\ninstance, we have all heard such statements as \u201cit is 90 percent probable that shake-\nspeare actually wrote hamlet\u201d or \u201cthe probability that oswald acted alone in assas-\nsinating kennedy is .8.\u201d how are we to interpret these statements?\n\nthe most simple and natural interpretation is that the probabilities referred to\nare measures of the individual\u2019s degree of belief in the statements that he or she\nis making. in other words, the individual making the foregoing statements is quite\ncertain that oswald acted alone and is even more certain that shakespeare wrote\nhamlet. this interpretation of probability as being a measure of the degree of one\u2019s\nbelief is often referred to as the personal or subjective view of probability.\n\nit seems logical to suppose that a \u201cmeasure of the degree of one\u2019s belief\u201d should\nsatisfy all of the axioms of probability. for example, if we are 70 percent certain that\nshakespeare wrote julius caesar and 10 percent certain that it was actually mar-\nlowe, then it is logical to suppose that we are 80 percent certain that it was either\nshakespeare or marlowe. hence, whether we interpret probability as a measure of\nbelief or as a long-run frequency of occurrence, its mathematical properties remain\nunchanged.\n\nexample 7a\nsuppose that, in a 7-horse race, you feel that each of the first 2 horses has a 20 percent\nchance of winning, horses 3 and 4 each have a 15 percent chance, and the remaining\n3 horses have a 10 percent chance each. would it be better for you to wager at even\nmoney that the winner will be one of the first three horses or to wager, again at even\nmoney, that the winner will be one of the horses 1, 5, 6, and 7?\n\nsolution. on the basis of your personal probabilities concerning the outcome of\nthe race, your probability of winning the first bet is .2 + .2 + .15 = .55, whereas\nit is .2 + .1 + .1 + .1 = .5 for the second bet. hence, the first wager is more\n.\nattractive.\n\nnote that, in supposing that a person\u2019s subjective probabilities are always consis-\ntent with the axioms of probability, we are dealing with an idealized rather than an\n\n "}, {"Page_number": 64, "text": "actual person. for instance, if we were to ask someone what he thought the chances\nwere of\n\nsummary 49\n\n(a) rain today,\n(b) rain tomorrow,\n(c) rain both today and tomorrow,\n(d) rain either today or tomorrow,\n\nit is quite possible that, after some deliberation, he might give 30 percent, 40 percent,\n20 percent, and 60 percent as answers. unfortunately, such answers (or such subjec-\ntive probabilities) are not consistent with the axioms of probability. (why not?) we\nwould of course hope that, after this was pointed out to the respondent, she would\nchange his answers. (one possibility we could accept is 30 percent, 40 percent, 10\npercent, and 60 percent.)\n\nsummary\nn(cid:10)\nlet s denote the set of all possible outcomes of an experiment. s is called the sample\nspace of the experiment. an event is a subset of s. if ai, i = 1, . . . , n, are events, then\nai, called the union of these events, consists of all outcomes that are in at least\ni=1\none of the events ai, i = 1, . . . , n. similarly,\nai, sometimes written as a1 \u00b7\u00b7\u00b7 an, is\ncalled the intersection of the events ai and consists of all outcomes that are in all of\nthe events ai, i = 1, . . . , n.\nfor any event a, we define ac to consist of all outcomes in the sample space that\nare not in a. we call ac the complement of the event a. the event sc, which is empty\nof outcomes, is designated by \u00f8 and is called the null set. if ab = \u00f8, then we say that\na and b are mutually exclusive.\n\nn(cid:11)\n\ni=1\n\nfor each event a of the sample space s, we suppose that a number p(a), called\n\nthe probability of a, is defined and is such that\n(i) 0 \u2026 p(a) \u2026 1\n(ii) p(s) = 1\n\u239e\n(iii) for mutually exclusive events ai, i \u00fa 1,\n\u23a0 =\n\n\u239b\n\u239d q(cid:14)\n\np\n\nai\n\ni=1\n\nq(cid:6)\n\ni=1\n\np(ai)\n\np(a) represents the probability that the outcome of the experiment is in a.\n\nit can be shown that\n\np(ac) = 1 \u2212 p(a)\n\na useful result is that\n\np(a \u222a b) = p(a) + p(b) \u2212 p(ab)\n\n "}, {"Page_number": 65, "text": "50\n\nchapter 2\n\naxioms of probability\n\nwhich can be generalized to give\n\n\u239e\n\u23a0 = n(cid:6)\n\n\u239b\n\u239d n(cid:14)\n\ni=1\n\np\n\nai\n\n(cid:6)(cid:6)\n\np(ai) \u2212\n\np(aiaj) +\ni=1\n+ \u00b7\u00b7\u00b7 + (\u22121)n+1p(a1 \u00b7\u00b7\u00b7 an)\n\ni<j\n\n(cid:6)(cid:6)(cid:6)\n\ni<j<k\n\np(aiajak)\n\nif s is finite and each one point set is assumed to have equal probability, then\n\np(a) = |a|\n|s|\n\nwhere |e| denotes the number of outcomes in the event e.\n\np(a) can be interpreted either as a long-run relative frequency or as a measure of\n\none\u2019s degree of belief.\n\nproblems\n\n1. a box contains 3 marbles: 1 red, 1 green, and 1\nblue. consider an experiment that consists of tak-\ning 1 marble from the box and then replacing it\nin the box and drawing a second marble from the\nbox. describe the sample space. repeat when the\nsecond marble is drawn without replacing the first\nmarble.\n\n2. in an experiment, die is rolled continually until a\n6 appears, at which point the experiment stops.\nwhat is the sample space of this experiment? let\nen denote the event that n rolls are necessary to\ncomplete the experiment. what points of the sam-\n\n(cid:5)c\n\nple space are contained in en? what is\n\nen\n\n?\n\n(cid:4)\nq(cid:10)\n\n1\n\n3. two dice are thrown. let e be the event that the\nsum of the dice is odd, let f be the event that\nat least one of the dice lands on 1, and let g be\nthe event that the sum is 5. describe the events\nef, e \u222a f, fg, efc, and efg.\n\n4. a, b, and c take turns flipping a coin. the first one\nto get a head wins. the sample space of this exper-\niment can be defined by\n\n%\n\ns =\n\n1, 01, 001, 0001, . . . ,\n0000\u00b7\u00b7\u00b7\n\n(a) interpret the sample space.\n(b) define the following events in terms of s:\n\n(i) a wins = a.\n(ii) b wins = b.\n(iii) (a \u222a b)c.\n\nassume that a flips first, then b, then c,\nthen a, and so on.\n\n5. a system is comprised of 5 components, each of\nwhich is either working or failed. consider an\nexperiment that consists of observing the status of\n\neach component, and let the outcome of the exper-\niment be given by the vector (x1, x2, x3, x4, x5),\nwhere xi is equal to 1 if component i is working\nand is equal to 0 if component i is failed.\n(a) how many outcomes are in the sample space\n\nof this experiment?\n\n(b) suppose that the system will work if compo-\nnents 1 and 2 are both working, or if compo-\nnents 3 and 4 are both working, or if compo-\nnents 1, 3, and 5 are all working. let w be the\nevent that the system will work. specify all the\noutcomes in w.\n\n(c) let a be the event that components 4 and 5\nare both failed. how many outcomes are con-\ntained in the event a?\n\n(d) write out all the outcomes in the event aw.\n\n6. a hospital administrator codes incoming patients\nsuffering gunshot wounds according to whether\nthey have insurance (coding 1 if they do and 0\nif they do not) and according to their condition,\nwhich is rated as good (g), fair (f), or serious (s).\nconsider an experiment that consists of the coding\nof such a patient.\n(a) give the sample space of this experiment.\n(b) let a be the event that the patient is in serious\n\ncondition. specify the outcomes in a.\n\nsured. specify the outcomes in b.\n\n(c) let b be the event that the patient is unin-\n(d) give all the outcomes in the event bc \u222a a.\n7. consider an experiment that consists of determin-\ning the type of job\u2014either blue-collar or white-\ncollar\u2014and the political affiliation\u2014republican,\ndemocratic, or independent\u2014of the 15 members\nof an adult soccer team. how many outcomes are\n(a) in the sample space?\n(b) in the event that at least one of the team mem-\n\nbers is a blue-collar worker?\n\n "}, {"Page_number": 66, "text": "(c) in the event that none of the team members\nconsiders himself or herself an independent?\n8. suppose that a and b are mutually exclusive\nevents for which p(a) = .3 and p(b) = .5. what is\nthe probability that\n(a) either a or b occurs?\n(b) a occurs but b does not?\n(c) both a and b occur?\n\n9. a retail establishment accepts either the american\nexpress or the visa credit card. a total of 24 per-\ncent of its customers carry an american express\ncard, 61 percent carry a visa card, and 11 per-\ncent carry both cards. what percentage of its cus-\ntomers carry a credit card that the establishment\nwill accept?\n\n10. sixty percent of the students at a certain school\nwear neither a ring nor a necklace. twenty per-\ncent wear a ring and 30 percent wear a necklace.\nif one of the students is chosen randomly, what is\nthe probability that this student is wearing\n(a) a ring or a necklace?\n(b) a ring and a necklace?\n\n11. a total of 28 percent of american males smoke\ncigarettes, 7 percent smoke cigars, and 5 percent\nsmoke both cigars and cigarettes.\n(a) what percentage of males smokes neither\n\ncigars nor cigarettes?\n\n(b) what percentage smokes cigars but not\n\ncigarettes?\n\n12. an elementary school\n\nis offering 3 language\nclasses: one in spanish, one in french, and one in\ngerman. the classes are open to any of the 100\nstudents in the school. there are 28 students in the\nspanish class, 26 in the french class, and 16 in the\ngerman class. there are 12 students that are in\nboth spanish and french, 4 that are in both span-\nish and german, and 6 that are in both french and\ngerman. in addition, there are 2 students taking\nall 3 classes.\n(a) if a student is chosen randomly, what is the\nprobability that he or she is not in any of the\nlanguage classes?\n\n(b) if a student is chosen randomly, what is the\nprobability that he or she is taking exactly one\nlanguage class?\n\n(c) if 2 students are chosen randomly, what is the\nprobability that at least 1 is taking a language\nclass?\n\n13. a certain town with a population of 100,000 has\n3 newspapers: i, ii, and iii. the proportions of\ntownspeople who read these papers are as follows:\n\ni: 10 percent\n\ni and ii: 8 percent\n\nii: 30 percent i and iii: 2 percent\niii: 5 percent ii and iii: 4 percent\n\ni and ii and\niii: 1 percent\n\nproblems 51\n\n(the list tells us, for instance, that 8000 people\nread newspapers i and ii.)\n(a) find the number of people who read only one\n\n(b) how many people read at\n\nleast\n\ntwo\n\nnewspaper.\n\nnewspapers?\n\n(c) if i and iii are morning papers and ii is an\nevening paper, how many people read at least\none morning paper plus an evening paper?\n\n(d) how many people do not\n\nread any\n\nnewspapers?\n\n(e) how many people read only one morning\n\npaper and one evening paper?\n\n14. the following data were given in a study of a group\nof 1000 subscribers to a certain magazine: in ref-\nerence to job, marital status, and education, there\nwere 312 professionals, 470 married persons, 525\ncollege graduates, 42 professional college gradu-\nates, 147 married college graduates, 86 married\nprofessionals, and 25 married professional college\ngraduates. show that the numbers reported in the\nstudy must be incorrect.\nhint: let m, w, and g denote, respectively, the\nset of professionals, married persons, and college\ngraduates. assume that one of the 1000 persons\nis chosen at random, and use proposition 4.4 to\nshow that if the given numbers are correct, then\np(m \u222a w \u222a g) > 1.\n15. if it is assumed that all\n\npoker hands are\nequally likely, what is the probability of being dealt\n(a) a flush? (a hand is said to be a flush if all 5\n\n52\n5\n\n(cid:2)\n\n(cid:3)\n\ncards are of the same suit.)\n\n(b) one pair? (this occurs when the cards have\ndenominations a, a, b, c, d, where a, b, c, and\nd are all distinct.)\n\n(c) two pairs? (this occurs when the cards have\ndenominations a, a, b, b, c, where a, b, and c\nare all distinct.)\n\n(d) three of a kind? (this occurs when the cards\nhave denominations a, a, a, b, c, where a, b,\nand c are all distinct.)\n\n(e) four of a kind? (this occurs when the cards\n\nhave denominations a, a, a, a, b.)\n\n16. poker dice is played by simultaneously rolling 5\n\ndice. show that\n(a) p{no two alike} = .0926;\n(b) p{one pair} = .4630;\n(c) p{two pair} = .2315;\n(d) p{three alike} = .1543;\n(e) p{full house} = .0386;\n(f) p{four alike} = .0193;\n(g) p{five alike} = .0008.\n\n17. if 8 rooks (castles) are randomly placed on a\nchessboard, compute the probability that none of\nthe rooks can capture any of the others. that is,\n\n "}, {"Page_number": 67, "text": "52\n\nchapter 2\n\naxioms of probability\n\ncompute the probability that no row or file con-\ntains more than one rook.\n\n18. two cards are randomly selected from an ordinary\nplaying deck. what is the probability that they\nform a blackjack? that is, what is the probability\nthat one of the cards is an ace and the other one is\neither a ten, a jack, a queen, or a king?\n\n19. two symmetric dice have both had two of their\nsides painted red, two painted black, one painted\nyellow, and the other painted white. when this\npair of dice is rolled, what is the probability that\nboth dice land with the same color face up?\n\n20. suppose that you are playing blackjack against a\ndealer. in a freshly shuffled deck, what is the prob-\nability that neither you nor the dealer is dealt a\nblackjack?\n\n21. a small community organization consists of 20\nfamilies, of which 4 have one child, 8 have two chil-\ndren, 5 have three children, 2 have four children,\nand 1 has five children.\n(a) if one of these families is chosen at random,\nwhat is the probability it has i children, i =\n1, 2, 3, 4, 5?\n\n(b) if one of the children is randomly chosen,\nwhat is the probability that child comes from\na family having i children, i = 1, 2, 3, 4, 5?\n\n22. consider the following technique for shuffling a\ndeck of n cards: for any initial ordering of the\ncards, go through the deck one card at a time and\nat each card, flip a fair coin. if the coin comes\nup heads, then leave the card where it is; if the\ncoin comes up tails, then move that card to the\nend of the deck. after the coin has been flipped n\ntimes, say that one round has been completed. for\ninstance, if n = 4 and the initial ordering is 1, 2, 3,\n4, then if the successive flips result in the outcome\nh, t, t, h, then the ordering at the end of the round\nis 1, 4, 2, 3. assuming that all possible outcomes of\nthe sequence of n coin flips are equally likely, what\nis the probability that the ordering after one round\nis the same as the initial ordering?\n\n23. a pair of fair dice is rolled. what is the probabil-\nity that the second die lands on a higher value than\ndoes the first?\n\n24. if two dice are rolled, what is the probability that\nthe sum of the upturned faces equals i? find it for\ni = 2, 3, . . . , 11, 12.\n\n25. a pair of dice is rolled until a sum of either 5 or 7\nappears. find the probability that a 5 occurs first.\nhint: let en denote the event that a 5 occurs on\nthe nth roll and no 5 or 7 occurs on the first n \u2212 1\nrolls. compute p(en) and argue that\np(en) is\nthe desired probability.\n\nq(cid:9)\n\nn=1\n\n26. the game of craps is played as follows: a player\nrolls two dice. if the sum of the dice is either a 2,\n\n3, or 12, the player loses; if the sum is either a 7\nor an 11, the player wins. if the outcome is any-\nthing else, the player continues to roll the dice until\nshe rolls either the initial outcome or a 7. if the 7\ncomes first, the player loses, whereas if the initial\noutcome reoccurs before the 7 appears, the player\nwins. compute the probability of a player winning\nat craps.\nhint: let ei denote the event that the initial out-\ncome is i and the player wins. the desired prob-\n\ni=2\n\nability is\n\np(ei). to compute p(ei), define the\nevents ei,n to be the event that the initial sum is\ni and the player wins on the nth roll. argue that\n\n12(cid:9)\np(ei) = q(cid:9)\n\nn=1\n\np(ei,n).\n\n27. an urn contains 3 red and 7 black balls. players a\nand b withdraw balls from the urn consecutively\nuntil a red ball is selected. find the probability that\na selects the red ball. (a draws the first ball, then\nb, and so on. there is no replacement of the balls\ndrawn.)\n\n28. an urn contains 5 red, 6 blue, and 8 green balls.\nif a set of 3 balls is randomly selected, what is the\nprobability that each of the balls will be (a) of the\nsame color? (b) of different colors? repeat under\nthe assumption that whenever a ball is selected, its\ncolor is noted and it is then replaced in the urn\nbefore the next selection. this is known as sam-\npling with replacement.\n\n29. an urn contains n white and m black balls, where\n\nn and m are positive numbers.\n(a) if two balls are randomly withdrawn, what is\nthe probability that they are the same color?\nis randomly withdrawn and then\nreplaced before the second one is drawn, what\nis the probability that the withdrawn balls are\nthe same color?\n\n(b) if a ball\n\n(c) show that the probability in part (b) is always\n\nlarger than the one in part (a).\n\n30. the chess clubs of two schools consist of, respec-\ntively, 8 and 9 players. four members from each\nclub are randomly chosen to participate in a con-\ntest between the two schools. the chosen play-\ners from one team are then randomly paired with\nthose from the other team, and each pairing plays\na game of chess. suppose that rebecca and her sis-\nter elise are on the chess clubs at different schools.\nwhat is the probability that\n(a) rebecca and elise will be paired?\n(b) rebecca and elise will be chosen to represent\n\ntheir schools but will not play each other?\n\n(c) either rebecca or elise will be chosen to\n\nrepresent her school?\n\n "}, {"Page_number": 68, "text": "31. a 3-person basketball team consists of a guard, a\n\nforward, and a center.\n(a) if a person is chosen at random from each of\nthree different such teams, what is the proba-\nbility of selecting a complete team?\n\n(b) what is the probability that all 3 players\n\nselected play the same position?\n\n32. a group of individuals containing b boys and g\ngirls is lined up in random order; that is, each of\nthe (b + g)! permutations is assumed to be equally\nlikely. what is the probability that the person in\nthe ith position, 1 \u2026 i \u2026 b + g, is a girl?\n\n33. a forest contains 20 elk, of which 5 are captured,\ntagged, and then released. a certain time later, 4\nof the 20 elk are captured. what is the probability\nthat 2 of these 4 have been tagged? what assump-\ntions are you making?\n\n34. the second earl of yarborough is reported to\nhave bet at odds of 1000 to 1 that a bridge hand\nof 13 cards would contain at least one card that is\nten or higher. (by ten or higher we mean that a\ncard is either a ten, a jack, a queen, a king, or an\nace.) nowadays, we call a hand that has no cards\nhigher than 9 a yarborough. what is the proba-\nbility that a randomly selected bridge hand is a\nyarborough?\n\n35. seven balls are randomly withdrawn from an urn\nthat contains 12 red, 16 blue, and 18 green balls.\nfind the probability that\n(a) 3 red, 2 blue, and 2 green balls are withdrawn;\n(b) at least 2 red balls are withdrawn;\n(c) all withdrawn balls are the same color;\n(d) either exactly 3 red balls or exactly 3 blue balls\n\nare withdrawn.\n\n36. two cards are chosen at random from a deck of 52\n\nplaying cards. what is the probability that they\n(a) are both aces?\n(b) have the same value?\n\n37. an instructor gives her class a set of 10 problems\nwith the information that the final exam will con-\nsist of a random selection of 5 of them. if a student\nhas figured out how to do 7 of the problems, what is\nthe probability that he or she will answer correctly\n(a) all 5 problems?\n(b) at least 4 of the problems?\n\n38. there are n socks, 3 of which are red, in a drawer.\nwhat is the value of n if, when 2 of the socks\nare chosen randomly, the probability that they are\nboth red is 1\n2 ?\n\n39. there are 5 hotels in a certain town. if 3 people\ncheck into hotels in a day, what is the probability\nthat they each check into a different hotel? what\nassumptions are you making?\n\n40. a town contains 4 people who repair televisions.\nif 4 sets break down, what is the probability that\n\nproblems 53\n\nexactly i of the repairers are called? solve the\nproblem for i = 1, 2, 3, 4. what assumptions are\nyou making?\n\n41. if a die is rolled 4 times, what is the probability that\n\n6 comes up at least once?\n\n42. two dice are thrown n times in succession. com-\npute the probability that double 6 appears at least\nonce. how large need n be to make this probability\nat least 1\n2 ?\n\n43. (a) if n people, including a and b, are randomly\narranged in a line, what is the probability that\na and b are next to each other?\n\n(b) what would the probability be if the people\n\nwere randomly arranged in a circle?\n\n44. five people, designated as a, b, c, d, e, are\narranged in linear order. assuming that each pos-\nsible order is equally likely, what is the probabil-\nity that\n(a) there is exactly one person between a and b?\n(b) there are exactly two people between a\n\nand b?\n\n(c) there are three people between a and b?\n\n45. a woman has n keys, of which one will open\n\nher door.\n(a) if she tries the keys at random, discarding\nthose that do not work, what is the probability\nthat she will open the door on her kth try?\n\n(b) what if she does not discard previously tried\n\nkeys?\n\n46. how many people have to be in a room in order\nthat the probability that at least two of them cele-\nbrate their birthday in the same month is at least\n1\n2 ? assume that all possible monthly outcomes are\nequally likely.\n\n47. if there are 12 strangers in a room, what is the\nprobability that no two of them celebrate their\nbirthday in the same month?\n\n48. given 20 people, what is the probability that,\namong the 12 months in the year, there are 4\nmonths containing exactly 2 birthdays and 4 con-\ntaining exactly 3 birthdays?\n\n49. a group of 6 men and 6 women is randomly\ndivided into 2 groups of size 6 each. what is the\nprobability that both groups will have the same\nnumber of men?\n\n50. in a hand of bridge, find the probability that you\nhave 5 spades and your partner has the remain-\ning 8.\n\n51. suppose that n balls are randomly distributed into\nn compartments. find the probability that m balls\nwill fall into the first compartment. assume that all\nnn arrangements are equally likely.\n\n "}, {"Page_number": 69, "text": "54\n\nchapter 2\n\naxioms of probability\n\n52. a closet contains 10 pairs of shoes. if 8 shoes\nare randomly selected, what is the probability that\nthere will be\n(a) no complete pair?\n(b) exactly 1 complete pair?\n\n53. if 4 married couples are arranged in a row, find the\n\nprobability that no husband sits next to his wife.\n\n54. compute the probability that a bridge hand is void\n\nin at least one suit. note that the answer is not\n\n(cid:3)\n\n39\n13\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)(cid:2)\n(cid:2)\n\n4\n1\n\n52\n13\n\n(why not?)\nhint: use proposition 4.4.\n\n55. compute the probability that a hand of 13 cards\n\ncontains\n(a) the ace and king of at least one suit;\n(b) all 4 of at least 1 of the 13 denominations.\n\n56. two players play the following game: player a\nchooses one of the three spinners pictured in\nfigure 2.6, and then player b chooses one of the\nremaining two spinners. both players then spin\ntheir spinner, and the one that lands on the higher\nnumber is declared the winner. assuming that\neach spinner is equally likely to land in any of its\n3 regions, would you rather be player a or player\nb? explain your answer!\n\n9\n\n5\n\n3\n\n8\n\na\n\n1\n\nb\n\n4\n\n7\n\n6\n\nc\n\n2\n\nfigure 2.6: spinners\n\nprove the following relations:\n1. ef ( e ( e \u222a f.\n2. if e ( f, then fc ( ec.\n3. f = fe \u222a fec and e \u222a f = e \u222a ecf.\n\n4.\n\ntheoretical exercises\n\n(cid:5)\n(cid:5)\n\nei\n\nei\n\n(cid:4)\n(cid:4)\n\nq(cid:10)\nq(cid:11)\n\n1\n\n1\n\nf = q(cid:10)\n\u222a f = q(cid:11)\n\n1\n\n1\n\neif and\n\n(ei \u222a f).\n\n "}, {"Page_number": 70, "text": "5. for any sequence of events e1, e2, . . ., define a\nnew sequence f1, f2, . . . of disjoint events (that is,\nevents such that fifj = \u00f8 whenever i z j) such\nthat for all n \u00fa 1,\n\nn(cid:14)\n\nfi = n(cid:14)\n\nei\n\n1\n\n1\n\n6. let e, f, and g be three events. find expressions\n\nfor the events so that, of e, f, and g,\n(a) only e occurs;\n(b) both e and g, but not f, occur;\n(c) at least one of the events occurs;\n(d) at least two of the events occur;\n(e) all three events occur;\n(f) none of the events occurs;\n(g) at most one of the events occurs;\n(h) at most two of the events occur;\n(i) exactly two of the events occur;\n(j) at most three of the events occur.\n\n7. find the simplest expression for the following\n\n8. let s be a given set. if,\n\nevents:\n(a) (e \u222a f)(e \u222a fc);\n(b) (e \u222a f)(ec \u222a f)(e \u222a fc);\n(c) (e \u222a f)(f \u222a g).\nk(cid:10)\n\nfor some k > 0,\ns1, s2, . . . , sk are mutually exclusive nonempty\nsi = s, then we\nsubsets of s such that\ncall the set {s1, s2, . . . , sk} a partition of s. let\ntn denote the number of different partitions of\n{1, 2, . . . , n}. thus, t1 = 1 (the only partition being\ns1 = {1}) and t2 = 2 (the two partitions being\n{{1, 2,}},{{1},{2}}).\n(a) show, by computing all partitions, that t3 =\n5, t4 = 15.\n(b) show that\n\ni=1\n\ntn+1 = 1 + n(cid:6)\n\nk=1\n\n(cid:2)\n\n(cid:3)\n\nn\nk\n\ntk\n\nand use this equation to compute t10.\n\nhint: one way of choosing a partition of n + 1\nitems is to call one of the items special. then we\nobtain different partitions by first choosing k, k =\n0, 1, . . . , n, then a subset of size n \u2212 k of the non-\nspecial items, and then any of the tk partitions of\nthe remaining k nonspecial items. by adding the\nspecial item to the subset of size n \u2212 k, we obtain\na partition of all n + 1 items.\n\n9. suppose that an experiment is performed n times.\nfor any event e of the sample space, let n(e)\ndenote the number of times that event e occurs\nand define f (e) = n(e)/n. show that f (\u00b7) satisfies\naxioms 1, 2, and 3.\n\ntheoretical exercises\n\n55\n10. prove that p(e \u222a f \u222a g) = p(e) + p(f) +\np(g) \u2212 p(ecfg) \u2212 p(efcg) \u2212 p(efgc) \u2212\n2p(efg).\n11. if p(e) = .9 and p(f) = .8, show that p(ef) \u00fa .7.\nin general, prove bonferroni\u2019s inequality, namely,\n\np(ef) \u00fa p(e) + p(f) \u2212 1\n\n12. show that the probability that exactly one of the\nevents e or f occurs equals p(e) + p(f) \u2212\n2p(ef).\n\n13. prove that p(efc) = p(e) \u2212 p(ef).\n14. prove proposition 4.4 by mathematical induction.\n15. an urn contains m white and n black balls. if a\nrandom sample of size r is chosen, what is the prob-\nability that it contains exactly k white balls?\n\n16. use induction to generalize bonferroni\u2019s inequal-\n\nity to n events. that is, show that\np(e1e2 \u00b7\u00b7\u00b7 en) \u00fa p(e1) + \u00b7\u00b7\u00b7 + p(en) \u2212 (n \u2212 1)\n\n17. consider the matching problem, example 5m, and\ndefine an to be the number of ways in which the\nn men can select their hats so that no man selects\nhis own. argue that\n\nan = (n \u2212 1)(an\u22121 + an\u22122)\n\nthis formula, along with the boundary conditions\na1 = 0, a2 = 1, can then be solved for an, and\nthe desired probability of no matches would be\nan /n!.\nhint: after the first man selects a hat that is not his\nown, there remain n \u2212 1 men to select among a set\nof n \u2212 1 hats that does not contain the hat of one\nof these men. thus, there is one extra man and one\nextra hat. argue that we can get no matches either\nwith the extra man selecting the extra hat or with\nthe extra man not selecting the extra hat.\n\nn \u00fa 2, where f0 k 1, f1 k 2\n\n18. let fn denote the number of ways of tossing a coin\nn times such that successive heads never appear.\nargue that\nfn = fn\u22121 + fn\u22122\nhint: how many outcomes are there that start\nwith a head, and how many start with a tail? if\npn denotes the probability that successive heads\nnever appear when a coin is tossed n times, find\npn (in terms of fn) when all possible outcomes of\nthe n tosses are assumed equally likely. compute\np10.\n\n19. an urn contains n red and m blue balls. they are\nwithdrawn one at a time until a total of r, r \u2026 n,\n\n "}, {"Page_number": 71, "text": "56\n\nchapter 2\n\naxioms of probability\n\nred balls have been withdrawn. find the probabil-\nity that a total of k balls are withdrawn.\nhint: a total of k balls will be withdrawn if there\nare r \u2212 1 red balls in the first k \u2212 1 withdrawals\nand the kth withdrawal is a red ball.\n\n20. consider an experiment whose sample space con-\nsists of a countably infinite number of points.\nshow that not all points can be equally likely.\ncan all points have a positive probability of\noccurring?\n\u221721. consider example 5o, which is concerned with the\nnumber of runs of wins obtained when n wins and\nm losses are randomly permuted. now\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)(cid:2)\n\nconsider the total number of runs\u2014that is, win\nruns plus loss runs\u2014and show that\nn \u2212 1\n(cid:3)\nk \u2212 1\nm + n\n(cid:2)\n(cid:3)\n\np{2k runs} = 2\n(cid:2)\n(cid:3)(cid:2)\np{2k + 1 runs}\n\nm \u2212 1\n(cid:2)\nk \u2212 1\n\n(cid:3)(cid:2)\n\nn\n\n(cid:3)\n\nm \u2212 1\nk \u2212 1\n\n=\n\nn \u2212 1\n(cid:2)\n\nk\n\nm \u2212 1\n(cid:3)\n\nk\n\nn \u2212 1\nk \u2212 1\n\n+\n\nm + n\n\nn\n\nself-test problems and exercises\n\n1. a cafeteria offers a three-course meal consisting\nof an entree, a starch, and a dessert. the possible\nchoices are given in the following table:\n\ncourse\n\nentree\nstarch\ndessert\n\nchoices\n\nchicken or roast beef\npasta or rice or potatoes\nice cream or jello or apple pie or a peach\n\na person is to choose one course from each cate-\ngory.\n(a) how many outcomes are in the sample space?\n(b) let a be the event that ice cream is chosen.\n\nhow many outcomes are in a?\n\n(c) let b be the event that chicken is chosen.\n\nhow many outcomes are in b?\n\n(d) list all the outcomes in the event ab.\n(e) let c be the event that rice is chosen. how\n\nmany outcomes are in c?\n\n(f) list all the outcomes in the event abc.\n\n2. a customer visiting the suit department of a cer-\ntain store will purchase a suit with probability .22,\na shirt with probability .30, and a tie with proba-\nbility .28. the customer will purchase both a suit\nand a shirt with probability .11, both a suit and a\ntie with probability .14, and both a shirt and a tie\nwith probability .10. a customer will purchase all 3\nitems with probability .06. what is the probability\nthat a customer purchases\n(a) none of these items?\n(b) exactly 1 of these items?\n\n3. a deck of cards is dealt out. what is the proba-\nbility that the 14th card dealt is an ace? what is\nthe probability that the first ace occurs on the 14th\ncard?\n\n\u25e6\n\n4. let a denote the event that the midtown temper-\n\u25e6\nature in los angeles is 70\nf, and let b denote the\nevent that the midtown temperature in new york\nis 70\nf. also, let c denote the event that the max-\nimum of the midtown temperatures in new york\nf. if p(a) = .3, p(b) =\nand in los angeles is 70\n.4, and p(c) = .2, find the probability that the min-\nimum of the two midtown temperatures is 70\n\nf.\n\n\u25e6\n\n\u25e6\n\n5. an ordinary deck of 52 cards is shuffled. what is\n\nthe probability that the top four cards have\n(a) different denominations?\n(b) different suits?\n\n6. urn a contains 3 red and 3 black balls, whereas\nurn b contains 4 red and 6 black balls. if a ball is\nrandomly selected from each urn, what is the prob-\nability that the balls will be the same color?\n\n7. in a state lottery, a player must choose 8 of the\nnumbers from 1 to 40. the lottery commission\nthen performs an experiment that selects 8 of these\n40 numbers. assuming that the choice of the lot-\ntery commission is equally likely to be any of the\ncombinations, what is the probability that a\n\n(cid:2)\n\n(cid:3)\n\n40\n8\n\nplayer has\n(a) all 8 of the numbers selected by the lottery\n\ncommission?\n\n(b) 7 of the numbers selected by the lottery com-\n\nmission?\n\n(c) at least 6 of the numbers selected by the lot-\n\ntery commission?\n\n8. from a group of 3 freshmen, 4 sophomores, 4\njuniors, and 3 seniors a committee of size 4 is ran-\ndomly selected. find the probability that the com-\nmittee will consist of\n(a) 1 from each class;\n(b) 2 sophomores and 2 juniors;\n(c) only sophomores or juniors.\n\n "}, {"Page_number": 72, "text": "9. for a finite set a, let n(a) denote the number of\n\nelements in a.\n(a) show that\n\n(b) more generally, show that\n\nn(a \u222a b) = n(a) + n(b) \u2212 n(ab)\n\u239b\n\u239d n(cid:14)\n\n(cid:6)(cid:6)\n\n\u239e\n\u23a0 =\n\n(cid:6)\n\nn(ai) \u2212\n\nai\n\nn\n\ni=1\n\ni\n\nn(aiaj)\n+ \u00b7\u00b7\u00b7 + (\u22121)n+1n(a1 \u00b7\u00b7\u00b7 an)\n\ni<j\n\n10. consider an experiment that consists of six horses,\nnumbered 1 through 6, running a race, and sup-\npose that the sample space consists of the 6! pos-\nsible orders in which the horses finish. let a be\nthe event that the number-1 horse is among the\ntop three finishers, and let b be the event that the\nnumber-2 horse comes in second. how many out-\ncomes are in the event a \u222a b?\n\n11. a 5-card hand is dealt from a well-shuffled deck of\n52 playing cards. what is the probability that the\nhand contains at least one card from each of the\nfour suits?\n\n12. a basketball team consists of 6 frontcourt and\n4 backcourt players. if players are divided into\nroommates at random, what is the probability that\nthere will be exactly two roommate pairs made up\nof a backcourt and a frontcourt player?\n\n13. suppose that a person chooses a letter at random\nfrom r e s e r v e and then chooses one at ran-\ndom from v e r t i c a l. what is the probability\nthat the same letter is chosen?\n\n14. prove boole\u2019s inequality:\n\n\u239e\n\u23a0 \u2026\n\n\u239b\n\u239d q(cid:14)\n\ni=1\n\np\n\nai\n\nq(cid:6)\n\ni=1\n\np(ai)\n\n15. show that if p(ai) = 1 for all\n\nself -test problems and exercises 57\ni \u00fa 1, then\n\n(cid:4)\n\n(cid:5)\n\nq(cid:11)\n\ni=1\n\np\n\nai\n\n= 1.\n\n16. let tk(n) denote the number of partitions of the\nset {1, . . . , n} into k nonempty subsets, where 1 \u2026\nk \u2026 n. (see theoretical exercise 8 for the defini-\ntion of a partition.) argue that\n\ntk(n) = ktk(n \u2212 1) + tk\u22121(n \u2212 1)\n\nhint: in how many partitions is {1} a subset, and in\nhow many is 1 an element of a subset that contains\nother elements?\n\n17. five balls are randomly chosen, without replace-\nment, from an urn that contains 5 red, 6 white, and\n7 blue balls. find the probability that at least one\nball of each color is chosen.\n\n18. four red, 8 blue, and 5 green balls are randomly\n\narranged in a line.\n(a) what is the probability that the first 5 balls are\n\n(b) what is the probability that none of the first 5\n\nblue?\n\nballs are blue?\n\n(c) what is the probability that the final 3 balls\n\nare differently colored.\n\n(d) what is the probability that all the red balls\n\nare together?\n\n19. ten cards are randomly chosen from a deck of 52\ncards that consists of 13 cards of each of 4 different\nsuits. each of the selected cards is put in one of 4\npiles, depending on the suit of the card.\n(a) what is the probability that the largest pile\nhas 4 cards, the next largest has 3, the next\nlargest has 2, and the smallest has 1 card?\n\n(b) what is the probability that two of the piles\nhave 3 cards, one has 4 cards, and one has no\ncards?\n\n20. balls are randomly removed from an urn initially\ncontaining 20 red and 10 blue balls. what is the\nprobability that all of the red balls are removed\nbefore all of the blue ones have been removed?\n\n "}, {"Page_number": 73, "text": "c h a p t e r\n\n3\n\nconditional probability and independence\n\n3.1 introduction\n3.2 conditional probabilities\n3.3 bayes\u2019s formula\n3.4 independent events\n3.5 p(\u00b7|f ) is a probability\n\n3.1 introduction\n\nin this chapter, we introduce one of the most important concepts in probability theory,\nthat of conditional probability. the importance of this concept is twofold. in the first\nplace, we are often interested in calculating probabilities when some partial informa-\ntion concerning the result of an experiment is available; in such a situation, the desired\nprobabilities are conditional. second, even when no partial information is available,\nconditional probabilities can often be used to compute the desired probabilities more\neasily.\n\n3.2 conditional probabilities\n\nsuppose that we toss 2 dice, and suppose that each of the 36 possible outcomes is\nequally likely to occur and hence has probability 1\n36. suppose further that we observe\nthat the first die is a 3. then, given this information, what is the probability that the\nsum of the 2 dice equals 8? to calculate this probability, we reason as follows: given\nthat the initial die is a 3, there can be at most 6 possible outcomes of our experiment,\nnamely, (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), and (3, 6). since each of these outcomes\noriginally had the same probability of occurring, the outcomes should still have equal\nprobabilities. that is, given that the first die is a 3, the (conditional) probability of\neach of the outcomes (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), and (3, 6) is 1\n6, whereas the\n(conditional) probability of the other 30 points in the sample space is 0. hence, the\ndesired probability will be 1\n6.\n\nif we let e and f denote, respectively, the event that the sum of the dice is 8 and the\nevent that the first die is a 3, then the probability just obtained is called the conditional\nprobability that e occurs given that f has occurred and is denoted by\n\np(e|f)\n\na general formula for p(e|f) that is valid for all events e and f is derived in the same\nmanner: if the event f occurs, then, in order for e to occur, it is necessary that the\nactual occurrence be a point both in e and in f; that is, it must be in ef. now, since\nwe know that f has occurred, it follows that f becomes our new, or reduced, sample\nspace; hence, the probability that the event ef occurs will equal the probability of\nef relative to the probability of f. that is, we have the following definition.\n\n58\n\n "}, {"Page_number": 74, "text": "definition\nif p(f) > 0, then\n\nsection 3.2\n\nconditional probabilities 59\n\np(e|f) = p(ef)\np(f)\n\n(2.1)\n\nexample 2a\na student is taking a one-hour-time-limit makeup examination. suppose the proba-\nbility that the student will finish the exam in less than x hours is x/2, for all 0 \u2026 x \u2026 1.\nthen, given that the student is still working after .75 hour, what is the conditional\nprobability that the full hour is used?\n\nsolution. let lx denote the event that the student finishes the exam in less than x\nhours, 0 \u2026 x \u2026 1, and let f be the event that the student uses the full hour. because\nf is the event that the student is not finished in less than 1 hour,\n\np(f) = p(lc\n\n1\n\n) = 1 \u2212 p(l1) = .5\n\nnow, the event that the student is still working at time .75 is the complement of the\nevent l.75, so the desired probability is obtained from\n\np(f|lc\n\n.75\n\n)\n\n) = p(flc\n.75\np(lc\n)\n.75\np(f)\n1 \u2212 p(l.75)\n\n=\n\n= .5\n.625\n\n= .8\n\n.\n\nif each outcome of a finite sample space s is equally likely, then, conditional on\nthe event that the outcome lies in a subset f ( s, all outcomes in f become equally\nlikely. in such cases, it is often convenient to compute conditional probabilities of\nthe form p(e|f) by using f as the sample space. indeed, working with this reduced\nsample space often results in an easier and better understood solution. our next few\nexamples illustrate this point.\n\nexample 2b\na coin is flipped twice. assuming that all four points in the sample space s = {(h, h),\n(h, t), (t, h), (t, t)} are equally likely, what is the conditional probability that both flips\nland on heads, given that (a) the first flip lands on heads? (b) at least one flip lands\non heads?\nsolution. let b = {(h, h)} be the event that both flips land on heads; let f = {(h, h),\n(h, t)} be the event that the first flip lands on heads; and let a = {(h, h), (h, t), (t, h)} be\nthe event that at least one flip lands on heads. the probability for (a) can be obtained\nfrom\n\n "}, {"Page_number": 75, "text": "60\n\nchapter 3\n\nconditional probability and independence\n\np(b|f) = p(bf)\np(f)\np({(h, h)})\np({(h, h), (h, t)})\n\n=\n\n= 1/4\n2/4\n\n= 1/2\n\nfor (b), we have\n\np(b|a) = p(ba)\np(a)\np({(h, h), (h, t), (t, h)})\n\np({(h, h)})\n\n=\n\n= 1/4\n3/4\n\n= 1/3\n\nthus, the conditional probability that both flips land on heads given that the first\none does is 1/2, whereas the conditional probability that both flips land on heads\ngiven that at least one does is only 1/3. many students initially find this latter result\nsurprising. they reason that, given that at least one flip lands on heads, there are\ntwo possible results: either they both land on heads or only one does. their mistake,\nhowever, is in assuming that these two possibilities are equally likely. for, initially,\nthere are 4 equally likely outcomes. because the information that at least one flip\nlands on heads is equivalent to the information that the outcome is not (t, t), we are\nleft with the 3 equally likely outcomes (h, h), (h, t), (t, h), only one of which results in\n.\nboth flips landing on heads.\n\nexample 2c\nin the card game bridge, the 52 cards are dealt out equally to 4 players\u2014called east,\nwest, north, and south. if north and south have a total of 8 spades among them,\nwhat is the probability that east has 3 of the remaining 5 spades?\n\nsolution. probably the easiest way to compute the desired probability is to work\nwith the reduced sample space. that is, given that north\u2013south have a total of 8\nspades among their 26 cards, there remains a total of 26 cards, exactly 5 of them\nbeing spades, to be distributed among the east\u2013west hands. since each distribution\nis equally likely, it follows that the conditional probability that east will have exactly\n\n(cid:3)(cid:2)\n3 spades among his or her 13 cards is(cid:2)\n(cid:2)\n\n5\n3\n\n21\n10\n\n(cid:3)\n\n(cid:3)\n\n26\n13\n\nl .339\n\n.\n\nexample 2d\na total of n balls are sequentially and randomly chosen, without replacement, from\nan urn containing r red and b blue balls (n \u2026 r + b). given that k of the n balls are\nblue, what is the conditional probability that the first ball chosen is blue?\n\n "}, {"Page_number": 76, "text": "section 3.2\n\nconditional probabilities 61\n\nsolution. if we imagine that the balls are numbered, with the blue balls having num-\nbers 1 through b and the red balls b + 1 through b + r, then the outcome of the\nexperiment of selecting n balls without replacement is a vector of distinct integers\nx1, . . . , xn, where each xi is between 1 and r + b. moreover, each such vector is equally\nlikely to be the outcome. so, given that the vector contains k blue balls (that is, it\ncontains k values between 1 and b), it follows that each of these outcomes is equally\nlikely. but because the first ball chosen is, therefore, equally likely to be any of the n\nchosen balls, of which k are blue, it follows that the desired probability is k/n.\n\nif we did not choose to work with the reduced sample space, we could have solved\nthe problem by letting b be the event that the first ball chosen is blue and bk be the\nevent that a total of k blue balls are chosen. then\np(b|bk) = p(bbk)\np(bk)\n\nnow, p(bk|b) is the probability that a random choice of n \u2212 1 balls from an urn\ncontaining r red and b \u2212 1 blue balls results in a total of k \u2212 1 blue balls being\nchosen; consequently,\n\n= p(bk|b)p(b)\n\np(bk)\n\n(cid:19)\n\n(cid:18)\n\n(cid:19)(cid:18)\n\n(cid:19)\n\n(cid:18)\n\nb\u22121\nr\nn\u2212k\nk\u22121\nr+b\u22121\nn\u22121\n\np(bk|b) =\n\nusing the preceding formula along with\n\nand the hypergeometric probability\n\np(b) = b\nr + b\n(cid:19)(cid:18)\n(cid:18)\n(cid:19)\n(cid:18)\n\np(bk) =\n\nb\nk\n\nr\nn\u2212k\nr+b\nn\n\n(cid:19)\n\nagain yields the result that\n\np(b|bk) = k\nn\n\nmultiplying both sides of equation (2.1) by p(f ), we obtain\n\np(ef ) = p(f)p(e|f )\n\n.\n\n(2.2)\n\nin words, equation (2.2) states that the probability that both e and f occur is equal\nto the probability that f occurs multiplied by the conditional probability of e given\nthat f occurred. equation (2.2) is often quite useful in computing the probability of\nthe intersection of events.\n\nexample 2e\nceline is undecided as to whether to take a french course or a chemistry course. she\nestimates that her probability of receiving an a grade would be 1\n2 in a french course\nand 2\n3 in a chemistry course. if celine decides to base her decision on the flip of a fair\ncoin, what is the probability that she gets an a in chemistry?\n\n "}, {"Page_number": 77, "text": "62\n\nchapter 3\n\nconditional probability and independence\n\nsolution. (a) let the event that celine takes chemistry and a denote the event that\nshe receives an a in whatever course she takes, then the desired probability is p(ca),\nwhich is calculated by using equation (2.2) as follows:\n(cid:3)\np(ca) = p(c)p(a|c)\n= 1\n3\n\n(cid:3)(cid:2)\n\n(cid:2)\n\n=\n\n1\n2\n\n2\n3\n\n.\n\nexample 2f\nsuppose that an urn contains 8 red balls and 4 white balls. we draw 2 balls from the\nurn without replacement. (a) if we assume that at each draw each ball in the urn is\nequally likely to be chosen, what is the probability that both balls drawn are red? (b)\nnow suppose that the balls have different weights, with each red ball having weight r\nand each white ball having weight w. suppose that the probability that a given ball in\nthe urn is the next one selected is its weight divided by the sum of the weights of all\nballs currently in the urn. now what is the probability that both balls are red?\n\nsolution. let r1 and r2 denote, respectively, the events that the first and second\nballs drawn are red. now, given that the first ball selected is red, there are 7 remaining\nred balls and 4 white balls, so p(r2|r1) = 7\n11. as p(r1) is clearly 8\n12, the desired\nprobability is\n(cid:3)\n(cid:3)(cid:2)\n\n(cid:2)\n\nof course, this probability could have been computed by p(r1r2) =(cid:18)\n\n(cid:19)\n\n(cid:18)\n\n(cid:19)\n\n8\n2\n\n/\n\n12\n2\n\n.\n\nfor part (b), we again let ri be the event that the ith ball chosen is red and use\n\np(r1r2) = p(r1)p(r2|r1)\n= 14\n33\n\n7\n11\n\n=\n\n2\n3\n\np(r1r2) = p(r1)p(r2|r1)\n\nnow, number the red balls, and let bi, i = 1, . . . , 8 be the event that the first ball\ndrawn is red ball number i. then\np(r1) = p(\u222a8\n\ni=1bi) = 8(cid:6)\n\np(bi) = 8\n\nr\n\n8r + 4w\n\ni=1\n\nmoreover, given that the first ball is red, the urn then contains 7 red and 4 white balls.\nthus, by an argument similar to the preceding one,\n\np(r2|r1) =\n\n7r\n\n7r + 4w\n\nhence, the probability that both balls are red is\n\np(r1r2) =\n\n8r\n\n8r + 4w\n\n7r\n\n7r + 4w\n\n.\n\na generalization of equation (2.2), which provides an expression for the probabil-\nity of the intersection of an arbitrary number of events, is sometimes referred to as\nthe multiplication rule.\n\n "}, {"Page_number": 78, "text": "section 3.2\n\nconditional probabilities 63\n\nthe multiplication rule\n\np(e1e2e3 \u00b7\u00b7\u00b7 en) = p(e1)p(e2|e1)p(e3|e1e2)\u00b7\u00b7\u00b7 p(en|e1 \u00b7\u00b7\u00b7 en\u22121)\n\nto prove the multiplication rule, just apply the definition of conditional probability\n\nto its right-hand side, giving\n\np(e1)\n\np(e1e2)\np(e1)\n\np(e1e2e3)\np(e1e2)\n\n\u00b7\u00b7\u00b7 p(e1e2 \u00b7\u00b7\u00b7 en)\np(e1e2 \u00b7\u00b7\u00b7 en\u22121)\n\n= p(e1e2 \u00b7\u00b7\u00b7 en)\n\nexample 2g\nin the match problem stated in example 5m of chapter 2, it was shown that pn, the\nprobability that there are no matches when n people randomly select from among\ntheir own n hats, is given by\n\npn = n(cid:6)\n\ni=0\n\n(\u22121)i/i!\n\nwhat is the probability that exactly k of the n people have matches?\n\nsolution. let us fix our attention on a particular set of k people and determine the\nprobability that these k individuals have matches and no one else does. letting e\ndenote the event that everyone in this set has a match, and letting g be the event that\nnone of the other n \u2212 k people have a match, we have\np(eg) = p(e )p(g|e )\n\nnow, let fi, i = 1, . . . , k, be the event that the ith member of the set has a match.\nthen\n\np(e ) = p(f1f2 \u00b7\u00b7\u00b7 fk)\n\n= p(f1)p(f2|f1)p(f3|f1f2)\u00b7\u00b7\u00b7 p(fk|f1 \u00b7\u00b7\u00b7 fk\u22121)\n= 1\nn \u2212 1\nn\n= (n \u2212 k)!\n\nn \u2212 k + 1\n\nn \u2212 2\n\n\u00b7\u00b7\u00b7\n\n1\n\n1\n\n1\n\nn!\n\ngiven that everyone in the set of k has a match, the other n \u2212 k people will be\nrandomly choosing among their own n \u2212 k hats, so the probability that none of\nthem has a match is equal to the probability of no matches in a problem having n \u2212 k\npeople choosing among their own n \u2212 k hats. therefore,\n\np(g|e ) = pn\u2212k = n\u2212k(cid:6)\n\n(\u22121)i/i!\n\ni=0\n\nshowing that the probability that a specified set of k people have matches and no one\nelse does is\n\np(eg) = (n \u2212 k)!\n\nn!\n\npn\u2212k\n\n "}, {"Page_number": 79, "text": "64\n\nchapter 3\n\nconditional probability and independence\n\nbecause there will be exactly k matches if the preceding is true for any of the\nof k individuals, the desired probability is\n\np(exactly k matches) = pn\u2212k/k!\n\u22121/k!\n\nl e\n\nwhen n is large\n\n(cid:19)\n\n(cid:18)\n\nn\nk\n\nsets\n\n.\n\nwe will now employ the multiplication rule to obtain a second approach to solving\n\nexample 5h(b) of chapter 2.\n\nexample 2h\nan ordinary deck of 52 playing cards is randomly divided into 4 piles of 13 cards each.\ncompute the probability that each pile has exactly 1 ace.\nsolution. define events ei, i = 1, 2, 3, 4, as follows:\n\ne1 = {the ace of spades is in any one of the piles}\ne2 = {the ace of spades and the ace of hearts are in different piles}\ne3 = {the aces of spades, hearts, and diamonds are all in different piles}\ne4 = {all 4 aces are in different piles}\n\nthe desired probability is p(e1e2e3e4), and by the multiplication rule,\np(e1e2e3e4) = p(e1)p(e2|e1)p(e3|e1e2)p(e4|e1e2e3)\n\nnow,\n\np(e1) = 1\n\nsince e1 is the sample space s. also,\n\np(e2|e1) = 39\n51\n\nsince the pile containing the ace of spades will receive 12 of the remaining 51 cards,\nand\n\np(e3|e1e2) = 26\n50\n\nsince the piles containing the aces of spades and hearts will receive 24 of the remain-\ning 50 cards. finally,\n\np(e4|e1e2e3) = 13\n49\n\ntherefore, the probability that each pile has exactly 1 ace is\n\np(e1e2e3e4) = 39 \u00b7 26 \u00b7 13\n51 \u00b7 50 \u00b7 49\n\nl .105\n\nthat is, there is approximately a 10.5 percent chance that each pile will contain an\nace. (problem 13 gives another way of using the multiplication rule to solve this\n.\nproblem.)\nremarks. our definition of p(e|f) is consistent with the interpretation of prob-\nability as being a long-run relative frequency. to see this, suppose that n repeti-\ntions of the experiment are to be performed, where n is large. we claim that if\n\n "}, {"Page_number": 80, "text": "section 3.3\n\nbayes\u2019s formula 65\nwe consider only those experiments in which f occurs, then p(e|f) will equal the\nlong-run proportion of them in which e also occurs. to verify this statement, note\nthat, since p(f) is the long-run proportion of experiments in which f occurs,\nit follows that in the n repetitions of the experiment f will occur approximately\nnp(f) times. similarly, in approximately np(ef) of these experiments both e and\nf will occur. hence, out of the approximately np(f) experiments in which\nf occurs,\nthem in which e also occurs is approximately\nequal to\n\nthe proportion of\n\nnp(ef)\nnp(f)\n\n= p(ef)\np(f)\n\nbecause this approximation becomes exact as n becomes larger and larger, we have\nthe appropriate definition of p(e|f).\n\n3.3 bayes\u2019s formula\n\nlet e and f be events. we may express e as\n\ne = ef \u222a efc\n\nfor, in order for an outcome to be in e, it must either be in both e and f or be in\ne but not in f. (see figure 3.1.) as ef and efc are clearly mutually exclusive, we\nhave, by axiom 3,\n\np(e ) = p(ef ) + p(efc)\n\n= p(e|f )p(f ) + p(e|fc)p(fc)\n= p(e|f )p(f ) + p(e|fc)[1 \u2212 p(f)]\n\n(3.1)\n\nequation (3.1) states that the probability of the event e is a weighted average of the\nconditional probability of e given that f has occurred and the conditional proba-\nbility of e given that f has not occurred\u2014each conditional probability being given\nas much weight as the event on which it is conditioned has of occurring. this is an\nextremely useful formula, because its use often enables us to determine the prob-\nability of an event by first \u201cconditioning\u201d upon whether or not some second event\nhas occurred. that is, there are many instances in which it is difficult to compute the\nprobability of an event directly, but it is straightforward to compute it once we know\nwhether or not some second event has occurred. we illustrate this idea with some\nexamples.\n\ne\n\nf\n\nef c\n\nef\n\nfigure 3.1: e = ef \u222a efc. ef = shaded area; efc = striped area\n\n "}, {"Page_number": 81, "text": "66\n\nchapter 3\n\nconditional probability and independence\n\nexample 3a (part 1)\nan insurance company believes that people can be divided into two classes: those\nwho are accident prone and those who are not. the company\u2019s statistics show that\nan accident-prone person will have an accident at some time within a fixed 1-year\nperiod with probability .4, whereas this probability decreases to .2 for a person who is\nnot accident prone. if we assume that 30 percent of the population is accident prone,\nwhat is the probability that a new policyholder will have an accident within a year of\npurchasing a policy?\n\nsolution. we shall obtain the desired probability by first conditioning upon whether\nor not the policyholder is accident prone. let a1 denote the event that the policy-\nholder will have an accident within a year of purchasing the policy, and let a denote\nthe event that the policyholder is accident prone. hence, the desired probability is\ngiven by\n\np(a1) = p(a1|a)p(a) + p(a1|ac)p(ac)\n\n= (.4)(.3) + (.2)(.7) = .26\n\n.\n\nexample 3a (part 2)\nsuppose that a new policyholder has an accident within a year of purchasing a policy.\nwhat is the probability that he or she is accident prone?\n\nsolution. the desired probability is\n\np(a|a1) = p(aa1)\np(a1)\n\n= p(a)p(a1|a)\n= 6\n= (.3)(.4)\n13\n\np(a1)\n\n.26\n\n.\n\nexample 3b\nconsider the following game played with an ordinary deck of 52 playing cards: the\ncards are shuffled and then turned over one at a time. at any time, the player can\nguess that the next card to be turned over will be the ace of spades; if it is, then the\nplayer wins. in addition, the player is said to win if the ace of spades has not yet\nappeared when only one card remains and no guess has yet been made. what is a\ngood strategy? what is a bad strategy?\n\nsolution. every strategy has probability 1/52 of winning! to show this, we will use\ninduction to prove the stronger result that, for an n card deck, one of whose cards\nis the ace of spades, the probability of winning is 1/n, no matter what strategy is\nemployed. since this is clearly true for n = 1, assume it to be true for an n \u2212 1\ncard deck, and now consider an n card deck. fix any strategy, and let p denote the\nprobability that the strategy guesses that the first card is the ace of spades. given that\nit does, the player\u2019s probability of winning is 1/n. if, however, the strategy does not\nguess that the first card is the ace of spades, then the probability that the player wins\nis the probability that the first card is not the ace of spades, namely, (n \u2212 1)/n, multi-\nplied by the conditional probability of winning given that the first card is not the ace\nof spades. but this latter conditional probability is equal to the probability of winning\n\n "}, {"Page_number": 82, "text": "bayes\u2019s formula 67\nwhen using an n \u2212 1 card deck containing a single ace of spades; it is thus, by the\ninduction hypothesis, 1/(n \u2212 1). hence, given that the strategy does not guess the\nfirst card, the probability of winning is\nn \u2212 1\n\nsection 3.3\n\n1\n\nn \u2212 1\n\n= 1\nn\n\nn\n\nthus, letting g be the event that the first card is guessed, we obtain\n\np{win} = p{win|g}p(g) + p{win|gc}(1 \u2212 p(g)) = 1\nn\n\n= 1\nn\n\np + 1\nn\n\n(1 \u2212 p)\n\n.\n\nexample 3c\n\nin answering a question on a multiple-choice test, a student either knows the answer\nor guesses. let p be the probability that the student knows the answer and 1 \u2212 p\nbe the probability that the student guesses. assume that a student who guesses at the\nanswer will be correct with probability 1/m, where m is the number of multiple-choice\nalternatives. what is the conditional probability that a student knew the answer to a\nquestion given that he or she answered it correctly?\n\nsolution. let c and k denote, respectively, the events that the student answers the\nquestion correctly and the event that he or she actually knows the answer. now,\n\np(c|k )p(k )\n\np(k|c ) = p(kc )\np(c )\np(c|k )p(k ) + p(c|kc)p(kc)\np + (1/m)(1 \u2212 p)\n1 + (m \u2212 1)p\n\n=\n=\n=\n\nmp\n\np\n\nfor example, if m = 5, p = 1\nto a question he or she answered correctly is 5\n6.\n\n2, then the probability that the student knew the answer\n.\n\nexample 3d\n\na laboratory blood test is 95 percent effective in detecting a certain disease when it\nis, in fact, present. however, the test also yields a \u201cfalse positive\u201d result for 1 percent\nof the healthy persons tested. (that is, if a healthy person is tested, then, with prob-\nability .01, the test result will imply that he or she has the disease.) if .5 percent of\nthe population actually has the disease, what is the probability that a person has the\ndisease given that the test result is positive?\n\n "}, {"Page_number": 83, "text": "68\n\nchapter 3\n\nconditional probability and independence\n\nsolution. let d be the event that the person tested has the disease and e the event\nthat the test result is positive. then the desired probability is\n\np(d|e) = p(de)\np(e)\np(e|d)p(d) + p(e|dc)p(dc)\n(.95)(.005) + (.01)(.995)\n\np(e|d)p(d)\n\n(.95)(.005)\n\n=\n\n=\n\n= 95\n294\n\nl .323\n\nthus, only 32 percent of those persons whose test results are positive actually have\nthe disease. many students are often surprised at this result (they expect the per-\ncentage to be much higher, since the blood test seems to be a good one), so it is\nprobably worthwhile to present a second argument that, although less rigorous than\nthe preceding one, is probably more revealing. we now do so.\n\nsince .5 percent of the population actually has the disease, it follows that, on the\naverage, 1 person out of every 200 tested will have it. the test will correctly confirm\nthat this person has the disease with probability .95. thus, on the average, out of every\n200 persons tested, the test will correctly confirm that .95 person has the disease.\non the other hand, however, out of the (on the average) 199 healthy people, the\ntest will incorrectly state that (199)(.01) of these people have the disease. hence, for\nevery .95 diseased person that the test correctly states is ill, there are (on the average)\n(199)(.01) healthy persons that the test incorrectly states are ill. thus, the proportion\nof time that the test result is correct when it states that a person is ill is\n\n.95\n\n.95 + (199)(.01)\n\n= 95\n294\n\nl .323\n\n.\n\nequation (3.1) is also useful when one has to reassess one\u2019s personal probabilities\nin the light of additional information. for instance, consider the examples that follow.\n\nexample 3e\nconsider a medical practitioner pondering the following dilemma: \u201cif i\u2019m at least 80\npercent certain that my patient has this disease, then i always recommend surgery,\nwhereas if i\u2019m not quite as certain, then i recommend additional tests that are expen-\nsive and sometimes painful. now, initially i was only 60 percent certain that jones\nhad the disease, so i ordered the series a test, which always gives a positive result\nwhen the patient has the disease and almost never does when he is healthy. the test\nresult was positive, and i was all set to recommend surgery when jones informed me,\nfor the first time, that he was diabetic. this information complicates matters because,\nalthough it doesn\u2019t change my original 60 percent estimate of his chances of having\nthe disease in question, it does affect the interpretation of the results of the a test.\nthis is so because the a test, while never yielding a positive result when the patient\nis healthy, does unfortunately yield a positive result 30 percent of the time in the case\nof diabetic patients who are not suffering from the disease. now what do i do? more\ntests or immediate surgery?\u201d\n\nsolution. in order to decide whether or not to recommend surgery, the doctor should\nfirst compute her updated probability that jones has the disease given that the a test\n\n "}, {"Page_number": 84, "text": "result was positive. let d denote the event that jones has the disease and e the event\nthat the a test result is positive. the desired conditional probability is then\n\nsection 3.3\n\nbayes\u2019s formula 69\n\np(d|e) = p(de)\np(e)\np(e|d)p(d) + p(e|dc)p(dc)\n1(.6) + (.3)(.4)\n\np(d)p(e|d)\n\n(.6)1\n\n=\n\n=\n= .833\n\nnote that we have computed the probability of a positive test result by conditioning\non whether or not jones has the disease and then using the fact that, because jones is\na diabetic, his conditional probability of a positive result given that he does not have\nthe disease, p(e|dc), equals .3. hence, as the doctor should now be over 80 percent\n.\ncertain that jones has the disease, she should recommend surgery.\n\nexample 3f\nat a certain stage of a criminal investigation, the inspector in charge is 60 percent con-\nvinced of the guilt of a certain suspect. suppose, however, that a new piece of evidence\nwhich shows that the criminal has a certain characteristic (such as left-handedness,\nbaldness, or brown hair) is uncovered. if 20 percent of the population possesses this\ncharacteristic, how certain of the guilt of the suspect should the inspector now be if it\nturns out that the suspect has the characteristic?\n\nsolution. letting g denote the event that the suspect is guilty and c the event that\nhe possesses the characteristic of the criminal, we have\n\np(g|c) = p(gc)\np(c)\np(c|g)p(g) + p(c|gc)p(gc)\n1(.6) + (.2)(.4)\n\np(c|g)p(g)\n\n1(.6)\n\n=\n\n=\nl .882\n\nwhere we have supposed that the probability of the suspect having the characteristic\nif he is, in fact, innocent is equal to .2, the proportion of the population possessing the\n.\ncharacteristic.\n\nexample 3g\nin the world bridge championships held in buenos aires in may 1965, the famous\nbritish bridge partnership of terrence reese and boris schapiro was accused of\ncheating by using a system of finger signals that could indicate the number of hearts\nheld by the players. reese and schapiro denied the accusation, and eventually a hear-\ning was held by the british bridge league. the hearing was in the form of a legal\nproceeding with prosecution and defense teams, both having the power to call and\ncross-examine witnesses. during the course of the proceeding, the prosecutor exam-\nined specific hands played by reese and schapiro and claimed that their playing these\n\n "}, {"Page_number": 85, "text": "70\n\nchapter 3\n\nconditional probability and independence\n\nhands was consistent with the hypothesis that they were guilty of having illicit knowl-\nedge of the heart suit. at this point, the defense attorney pointed out that their play\nof these hands was also perfectly consistent with their standard line of play. how-\never, the prosecution then argued that, as long as their play was consistent with the\nhypothesis of guilt, it must be counted as evidence toward that hypothesis. what do\nyou think of the reasoning of the prosecution?\n\nsolution. the problem is basically one of determining how the introduction of new\nevidence (in the preceding example, the playing of the hands) affects the probability\nof a particular hypothesis. if we let h denote a particular hypothesis (such as the\nhypothesis that reese and schapiro are guilty) and e the new evidence, then\n\np(h|e) = p(he)\np(e)\np(e|h)p(h) + p(e|hc)[1 \u2212 p(h)]\n\np(e|h)p(h)\n\n=\n\n(3.2)\n\nwhere p(h) is our evaluation of the likelihood of the hypothesis before the intro-\nduction of the new evidence. the new evidence will be in support of the hypothesis\nwhenever it makes the hypothesis more likely\u2014that is, whenever p(h|e) \u00fa p(h).\nfrom equation (3.2), this will be the case whenever\n\np(e|h) \u00fa p(e|h)p(h) + p(e|hc)[1 \u2212 p(h)]\n\nor, equivalently, whenever\n\np(e|h) \u00fa p(e|hc)\n\nin other words, any new evidence can be considered to be in support of a particular\nhypothesis only if its occurrence is more likely when the hypothesis is true than when\nit is false. in fact, the new probability of the hypothesis depends on its initial proba-\nbility and the ratio of these conditional probabilities, since, from equation (3.2),\n\np(h|e) =\n\np(h)\n\np(h) + [1 \u2212 p(h)]\n\np(e|hc)\np(e|h)\n\nhence, in the problem under consideration, the play of the cards can be con-\nsidered to support the hypothesis of guilt only if such play would have been more\nlikely if the partnership were cheating than if they were not. as the prosecutor never\nmade this claim, his assertion that the evidence is in support of the guilt hypothesis is\n.\ninvalid.\n\nwhen the author of this text drinks iced tea at a coffee shop, he asks for a glass of\nwater along with the (same-sized) glass of tea. as he drinks the tea, he continuously\nrefills the tea glass with water. assuming a perfect mixing of water and tea, he won-\ndered about the probability that his final gulp would be tea. this wonderment led to\npart (a) of the following problem and to a very interesting answer.\n\nexample 3h\nurn 1 initially has n red molecules and urn 2 has n blue molecules. molecules are\nrandomly removed from urn 1 in the following manner: after each removal from\nurn 1, a molecule is taken from urn 2 (if urn 2 has any molecules) and placed in urn 1.\nthe process continues until all the molecules have been removed. (thus, there are\n2n removals in all.)\n\n "}, {"Page_number": 86, "text": "section 3.3\n\nbayes\u2019s formula 71\n\n(a) find p(r), where r is the event that the final molecule removed from urn 1\n\nis red.\n\n(b) repeat the problem when urn 1 initially has r1 red molecules and b1 blue\n\nmolecules and urn 2 initially has r2 red molecules and b2 blue molecules.\n\nsolution. (a) focus attention on any particular red molecule, and let f be the event\nthat this molecule is the final one selected. now, in order for f to occur, the molecule\nin question must still be in the urn after the first n molecules have been removed (at\nwhich time urn 2 is empty). so, letting ni be the event that this molecule is not the ith\nmolecule to be removed, we have\np(f) = p(n1 \u00b7\u00b7\u00b7 nnf)\n(cid:2)\n1 \u2212 1\nn\n\n(cid:2)\n= p(n1)p(n2|n1)\u00b7\u00b7\u00b7 p(nn|n1 \u00b7\u00b7\u00b7 nn\u22121)p(f|n1 \u00b7\u00b7\u00b7 nn)\n=\n\n1 \u2212 1\nn\n\n(cid:3)\n\n(cid:3)\n\n\u00b7\u00b7\u00b7\n\n1\nn\n\nwhere the preceding formula uses the fact that the conditional probability that the\nmolecule under consideration is the final molecule to be removed, given that it is still\nin urn 1 when only n molecules remain, is, by symmetry, 1/n.\n\ntherefore, if we number the n red molecules and let rj be the event that red\nmolecule number j is the final molecule removed, then it follows from the preceding\nformula that\n\n(cid:2)\n\n(cid:3)n 1\n\nn\n\n1 \u2212 1\nn\n\np(rj) =\n\u239e\n\u239f\u23a0 = n(cid:6)\n\nrj\n\nj=1\n\n\u239b\n\u239c\u239d n(cid:14)\n\nj=1\n\nbecause the events rj are mutually exclusive, we obtain\n\np(r) = p\n\np(rj) = (1 \u2212 1\nn\n\n)n l e\n\n\u22121\n\n(cid:2)\n\n(cid:3)r2+b2\n\n(b) suppose now that urn i initially has ri red and bi blue molecules, for i = 1, 2. to\nfind p(r), the probability that the final molecule removed is red, focus attention on\nany molecule that is initially in urn 1. as in part (a), it follows that the probability\nthat this molecule is the final one removed is\n\n1\n\np =\n\n1 \u2212\n\n1\n\n(cid:29)\n1 \u2212 1\nr1+b1\n\n(cid:30)r2+b2 is the probability that the molecule under consideration\n\nr1 + b1\n\nr1 + b1\n\nthat is,\nis the conditional probability,\nis still in urn 1 when urn 2 becomes empty, and\ngiven the preceding event, that the molecule under consideration is the final molecule\nremoved. hence, if we let o be the event that the last molecule removed is one of the\nmolecules originally in urn 1, then\n\n1\nr1+b1\n\n(cid:2)\n\np(o) = (r1 + b1)p =\n\n1 \u2212\n\nto determine p(r), we condition on whether o occurs, to obtain\np(r) = p(r|o)p(o) + p(r|oc)p(oc)\n\n(cid:2)\n\n=\n\nr1\n\nr1 + b1\n\n1 \u2212\n\n1\n\nr1 + b1\n\nr2\n\nr2 + b2\n\n1 \u2212\n\n1\n\nr1 + b1\n\n(cid:3)r2+b2 +\n\n(cid:5)\n\n(cid:3)r2+b2\n\n(cid:3)r2+b2\n\n1\n\nr1 + b1\n(cid:4)\n1 \u2212\n\n(cid:2)\n\n "}, {"Page_number": 87, "text": "72\n\nchapter 3\n\nconditional probability and independence\n\nif r1 + b1 = r2 + b2 = n, so that both urns initially have n molecules, then, when n\nis large,\n.\n\nr1\n\nr2\n\n(1 \u2212 e\n\n\u22121)\n\n\u22121 +\ne\n\np(l) l\n\nr1 + b1\n\nr2 + b2\n\nthe change in the probability of a hypothesis when new evidence is introduced can\nbe expressed compactly in terms of the change in the odds of that hypothesis, where\nthe concept of odds is defined as follows.\n\ndefinition\nthe odds of an event a are defined by\n\np(a)\np(ac)\n\n= p(a)\n\n1 \u2212 p(a)\n\nthat is, the odds of an event a tell how much more likely it is that the event a occurs\nthan it is that it does not occur. for instance, if p(a) = 2\n3, then p(a) = 2p(ac), so\nthe odds are 2. if the odds are equal to \u03b1, then it is common to say that the odds are\n\u201c\u03b1 to 1\u201d in favor of the hypothesis.\n\nconsider now a hypothesis h that is true with probability p(h), and suppose that\nnew evidence e is introduced. then the conditional probabilities, given the evidence\ne, that h is true and that h is not true are respectively given by\n\np(h|e) = p(e|h)p(h)\n\np(e)\n\np(hc|e) = p(e|hc)p(hc)\n\np(e)\n\ntherefore, the new odds after the evidence e has been introduced are\n\np(h|e)\np(hc|e)\n\n= p(h)\np(hc)\n\np(e|h)\np(e|hc)\n\n(3.3)\n\nthat is, the new value of the odds of h is the old value, multiplied by the ratio of the\nconditional probability of the new evidence given that h is true to the conditional\nprobability given that h is not true. thus, equation (3.3) verifies the result of exam-\nple 3f, since the odds, and thus the probability of h, increase whenever the new evi-\ndence is more likely when h is true than when it is false. similarly, the odds decrease\nwhenever the new evidence is more likely when h is false than when it is true.\n\nexample 3i\nan urn contains two type a coins and one type b coin. when a type a coin is flipped,\nit comes up heads with probability 1/4, whereas when a type b coin is flipped, it comes\nup heads with probability 3/4. a coin is randomly chosen from the urn and flipped.\ngiven that the flip landed on heads, what is the probability that it was a type a coin?\nsolution. let a be the event that a type a coin was flipped, and let b = ac be the\nevent that a type b coin was flipped. we want p(a|heads), where heads is the event\nthat the flip landed on heads. from equation (3.3), we see that\n\np(a|heads)\np(ac|heads)\n\np(heads|a)\np(heads|b)\n1/4\n3/4\n\n= p(a)\np(b)\n= 2/3\n1/3\n= 2/3\n\n "}, {"Page_number": 88, "text": "n(cid:14)\n\ni=1\n\nfi = s\n\ne = n(cid:14)\n\ni=1\n\nefi\n\nhence, the odds are 2/3 : 1, or, equivalently, the probability is 2/5 that a type a coin\n.\nwas flipped.\n\nsection 3.3\n\nbayes\u2019s formula 73\n\nequation (3.1) may be generalized as follows: suppose that f1, f2, . . . , fn are mutu-\n\nally exclusive events such that\n\nin other words, exactly one of the events f1, f2, . . . , fn must occur. by writing\n\nand using the fact that the events efi, i = 1, . . . , n are mutually exclusive, we obtain\n\np(e) = n(cid:6)\n= n(cid:6)\n\ni=1\n\ni=1\n\np(efi)\n\np(e|fi)p(fi)\n\n(3.4)\n\nthus, equation (3.4) shows how, for given events f1, f2, . . . , fn, of which one and\nonly one must occur, we can compute p(e) by first conditioning on which one of\nthe fi occurs. that is, equation (3.4) states that p(e) is equal to a weighted average\nof p(e|fi), each term being weighted by the probability of the event on which it is\nconditioned.\n\nexample 3j\nin example 5j of chapter 2, we considered the probability that, for a randomly shuf-\nfled deck, the card following the first ace is some specified card, and we gave a combi-\nnatorial argument to show that this probability is 1\n. here is a probabilistic argument\n52\nbased on conditioning: let e be the event that the card following the first ace is some\nspecified card, say, card x. to compute p(e), we ignore card x and condition on the\nrelative ordering of the other 51 cards in the deck. letting o be the ordering gives\n\n(cid:6)\n\np(e) =\n\np(e|o)p(o)\n\no\n\nnow, given o, there are 52 possible orderings of the cards, corresponding to hav-\ning card x being the ith card in the deck, i = 1,..., 52. but because all 52! possi-\nble orderings were initially equally likely, it follows that, conditional on o, each\nof the 52 remaining possible orderings is equally likely. because card x will follow\nthe first ace for only one of these orderings, we have p(e|o) = 1/52, implying that\np(e) = 1/52.\n.\n\nagain, let f1,..., fn be a set of mutually exclusive and exhaustive events (meaning\n\nthat exactly one of these events must occur).\n\nsuppose now that e has occurred and we are interested in determining which one\nof the fj also occurred. then, by equation (3.4), we have the following proposition.\n\n "}, {"Page_number": 89, "text": "74\n\nchapter 3\n\nconditional probability and independence\n\nproposition 3.1.\n\np(fj|e) = p(efj)\np(e)\nn(cid:6)\n= p(e|fj)p(fj)\np(e|fi)p(fi)\n\ni=1\n\n(3.5)\n\nequation (3.5) is known as bayes\u2019s formula, after the english philosopher thomas\nbayes. if we think of the events fj as being possible \u201chypotheses\u201d about some sub-\nject matter, then bayes\u2019s formula may be interpreted as showing us how opinions\nabout these hypotheses held before the experiment was carried out [that is, the p(fj)]\nshould be modified by the evidence produced by the experiment.\n\nexample 3k\na plane is missing, and it is presumed that it was equally likely to have gone down\nin any of 3 possible regions. let 1 \u2212 \u03b2i, i = 1, 2, 3, denote the probability that the\nplane will be found upon a search of the ith region when the plane is, in fact, in that\nregion. (the constants \u03b2i are called overlook probabilities, because they represent the\nprobability of overlooking the plane; they are generally attributable to the geograph-\nical and environmental conditions of the regions.) what is the conditional probability\nthat the plane is in the ith region given that a search of region 1 is unsuccessful?\nsolution. let ri, i = 1, 2, 3, be the event that the plane is in region i, and let e be\nthe event that a search of region 1 is unsuccessful. from bayes\u2019s formula, we obtain\n\np(r1|e) = p(er1)\np(e)\n\n= p(e|r1)p(r1)\n3(cid:6)\np(e|ri)p(ri)\n\ni=1\n\n(\u03b21) 1\n3\n+ (1) 1\n\n3\n\n+ (1) 1\n\n3\n\n=\n\n(\u03b21) 1\n3\n= \u03b21\n\u03b21 + 2\n\np(rj|e) = p(e|rj)p(rj)\n\n=\n\n=\n\np(e)\n(1) 1\n3\n+ 1\n3\n\n(\u03b21) 1\n3\n1\n\u03b21 + 2\n\n+ 1\n3\nj = 2, 3\n\nfor j = 2, 3,\n\nnote that the updated (that is, the conditional) probability that the plane is in\nregion j, given the information that a search of region 1 did not find it, is greater than\n\n "}, {"Page_number": 90, "text": "section 3.3\n\nbayes\u2019s formula 75\nthe initial probability that it was in region j when j z 1 and is less than the initial prob-\nability when j = 1. this statement is certainly intuitive, since not finding the plane in\nregion 1 would seem to decrease its chance of being in that region and increase its\nchance of being elsewhere. further, the conditional probability that the plane is in\nregion 1 given an unsuccessful search of that region is an increasing function of the\noverlook probability \u03b21. this statement is also intuitive, since the larger \u03b21 is, the\nmore it is reasonable to attribute the unsuccessful search to \u201cbad luck\u201d as opposed\nto the plane\u2019s not being there. similarly, p(rj|e), j z 1, is a decreasing function\n.\nof \u03b21.\n\nthe next example has often been used by unscrupulous probability students to win\n\nmoney from their less enlightened friends.\n\nexample 3l\nsuppose that we have 3 cards that are identical in form, except that both sides of the\nfirst card are colored red, both sides of the second card are colored black, and one\nside of the third card is colored red and the other side black. the 3 cards are mixed\nup in a hat, and 1 card is randomly selected and put down on the ground. if the upper\nside of the chosen card is colored red, what is the probability that the other side is\ncolored black?\n\nsolution. let rr, bb, and rb denote, respectively, the events that the chosen card\nis all red, all black, or the red\u2013black card. also, let r be the event that the upturned\nside of the chosen card is red. then the desired probability is obtained by\n\np(r)\n\np(rb|r) = p(rb \u2229 r)\n(cid:29)\n(cid:29)\n\n=\n\n=\n\np(r|rr)p(rr) + p(r|rb)p(rb) + p(r|bb)p(bb)\n(cid:29)\n\np(r|rb)p(rb)\n(cid:30) = 1\n\n(cid:30)(cid:29)\n(cid:30)(cid:29)\n\n(cid:30)\n(cid:30)\n\n(cid:30)\n\n(cid:29)\n\n1\n2\n\n1\n3\n\n(1)\n\n1\n3\n\n+\n\n1\n2\n\n1\n3\n\n+ 0\n\n1\n3\n\n3\n\n3. some students guess 1\n\nhence, the answer is 1\n2 as the answer by incorrectly reasoning\nthat, given that a red side appears, there are two equally likely possibilities: that the\ncard is the all-red card or the red\u2013black card. their mistake, however, is in assuming\nthat these two possibilities are equally likely. for, if we think of each card as consist-\ning of two distinct sides, then we see that there are 6 equally likely outcomes of the\nexperiment\u2014namely, r1, r2, b1, b2, r3, b3\u2014where the outcome is r1 if the first side\nof the all-red card is turned face up, r2 if the second side of the all-red card is turned\nface up, r3 if the red side of the red\u2013black card is turned face up, and so on. since the\nother side of the upturned red side will be black only if the outcome is r3, we see that\nthe desired probability is the conditional probability of r3 given that either r1 or r2\n.\nor r3 occurred, which obviously equals 1\n3.\n\nexample 3m\na new couple, known to have two children, has just moved into town. suppose that\nthe mother is encountered walking with one of her children. if this child is a girl, what\nis the probability that both children are girls?\n\n "}, {"Page_number": 91, "text": "76\n\nchapter 3\n\nconditional probability and independence\n\nsolution. let us start by defining the following events:\ng1: the first (that is, the oldest) child is a girl.\ng2: the second child is a girl.\ng: the child seen with the mother is a girl.\nalso, let b1, b2, and b denote similar events, except that \u201cgirl\u201d is replaced by \u201cboy.\u201d\nnow, the desired probability is p(g1g2|g), which can be expressed as follows:\n\np(g1g2|g) = p(g1g2g)\np(g)\n= p(g1g2)\np(g)\n\nalso,\n\np(g) = p(g|g1g2)p(g1g2) + p(g|g1b2)p(g1b2)\n+ p(g|b1g2)p(b1g2) + p(g|b1b2)p(b1b2)\n\n= p(g1g2) + p(g|g1b2)p(g1b2) + p(g|b1g2)p(b1g2)\n\nwhere the final equation used the results p(g|g1g2) = 1 and p(g|b1b2) = 0. if we\nnow make the usual assumption that all 4 gender possibilities are equally likely, then\nwe see that\n\np(g1g2|g) =\n=\n\n1\n4\n\n1\n4\n\n+ p(g|g1b2)/4 + p(g|b1g2)/4\n\n1 + p(g|g1b2) + p(g|b1g2)\n\n1\n\nthus, the answer depends on whatever assumptions we want to make about the con-\nditional probabilities that the child seen with the mother is a girl given the event g1b2\nand that the child seen with the mother is a girl given the event g2b1. for instance, if\nwe want to assume, on the one hand, that, independently of the genders of the chil-\ndren, the child walking with the mother is the elder child with some probability p,\nthen it would follow that\n\np(g|g1b2) = p = 1 \u2212 p(g|b1g2)\n\nimplying under this scenario that\n\np(g1g2|g) = 1\n2\n\nif, on the other hand, we were to assume that if the children are of different genders,\nthen the mother would choose to walk with the girl with probability q, independently\nof the birth order of the children, then we would have\n\np(g|g1b2) = p(g|b1g2) = q\n\nimplying that\n\np(g1g2|g) =\n\n1\n\n1 + 2q\n\nfor instance, if we took q = 1, meaning that the mother would always choose to walk\nwith a daughter, then the conditional probability the she has two daughters would be\n\n "}, {"Page_number": 92, "text": "section 3.3\n\nbayes\u2019s formula 77\n\n1\n3, which is in accord with example 2b because seeing the mother with a daughter is\nnow equivalent to the event that she has at least one daughter.\n\nhence, as stated, the problem is incapable of solution. indeed, even when the usual\nassumption about equally likely gender probabilities is made, we still need to make\nadditional assumptions before a solution can be given. this is because the sample\nspace of the experiment consists of vectors of the form s1, s2, i, where s1 is the gen-\nder of the older child, s2 is the gender of the younger child, and i identifies the birth\norder of the child seen with the mother. as a result, to specify the probabilities of\nthe events of the sample space, it is not enough to make assumptions only about\nthe genders of the children; it is also necessary to assume something about the con-\nditional probabilities as to which child is with the mother given the genders of the\n.\nchildren.\n\nexample 3n\na bin contains 3 different types of disposable flashlights. the probability that a type 1\nflashlight will give over 100 hours of use is .7, with the corresponding probabilities for\ntype 2 and type 3 flashlights being .4 and .3, respectively. suppose that 20 percent of\nthe flashlights in the bin are type 1, 30 percent are type 2, and 50 percent are type 3.\n(a) what is the probability that a randomly chosen flashlight will give more than\n\n(b) given that a flashlight lasted over 100 hours, what is the conditional probability\n\n100 hours of use?\nthat it was a type j flashlight, j = 1, 2, 3?\n\nsolution. (a) let a denote the event that the flashlight chosen will give over 100\nhours of use, and let fj be the event that a type j flashlight is chosen, j = 1, 2, 3. to\ncompute p(a), we condition on the type of the flashlight, to obtain\n\np(a) = p(a|f1)p(f1) + p(a|f2)p(f2) + p(a|f3)p(f3)\n\n= (.7)(.2) + (.4)(.3) + (.3)(.5) = .41\n\nthere is a 41 percent chance that the flashlight will last for over 100 hours.\n\n(b) the probability is obtained by using bayes\u2019s formula:\n\np(fj|a) = p(afj)\np(a)\n\n= p(a|fj)p(fj)\n\n.41\n\nthus,\n\np(f1|a) = (.7)(.2)/.41 = 14/41\np(f2|a) = (.4)(.3)/.41 = 12/41\np(f3|a) = (.3)(.5)/.41 = 15/41\n\nfor instance, whereas the initial probability that a type 1 flashlight is chosen is only\n.2, the information that the flashlight has lasted over 100 hours raises the probability\nof this event to 14/41 l .341.\n.\n\nexample 3o\na crime has been committed by a solitary individual, who left some dna at the\nscene of the crime. forensic scientists who studied the recovered dna noted that\n\n "}, {"Page_number": 93, "text": "78\n\nchapter 3\n\nconditional probability and independence\n\nonly five strands could be identified and that each innocent person, independently,\n\u22125 of having his or her dna match on all five strands.\nwould have a probability of 10\nthe district attorney supposes that the perpetrator of the crime could be any of the\none million residents of the town. ten thousand of these residents have been released\nfrom prison within the past 10 years; consequently, a sample of their dna is on file.\nbefore any checking of the dna file, the district attorney feels that each of the ten\nthousand ex-criminals has probability \u03b1 of being guilty of the new crime, while each\nof the remaining 990,000 residents has probability \u03b2, where \u03b1 = c\u03b2. (that is, the\ndistrict attorney supposes that each recently released convict is c times as likely to\nbe the crime\u2019s perpetrator as is each town member who is not a recently released\nconvict.) when the dna that is analyzed is compared against the database of the\nten thousand ex-convicts, it turns out that a. j. jones is the only one whose dna\nmatches the profile. assuming that the district attorney\u2019s estimate of the relationship\nbetween \u03b1 and \u03b2 is accurate, what is the probability that a. j. is guilty?\n\nsolution. to begin, note that, because probabilities must sum to 1, we have\n\n1 = 10,000\u03b1 + 990,000\u03b2 = (10,000c + 990,000)\u03b2\n\nthus,\n\n\u03b2 =\n\n1\n\n10,000c + 990,000\n\n,\n\n\u03b1 =\n\nc\n\n10,000c + 990,000\n\nnow, let g be the event that a. j. is guilty, and let m denote the event that a. j. is\nthe only one of the ten thousand on file to have a match. then\n\np(g|m) = p(gm)\np(m)\np(m|g)p(g) + p(m|gc)p(gc)\n\np(g)p(m|g)\n\n=\n\non the one hand, if a. j. is guilty, then he will be the only one to have a dna match\nif none of the others on file have a match. therefore,\n\np(m|g) = (1 \u2212 10\n\n\u22125)9999\n\non the other hand, if a. j. is innocent, then in order for him to be the only match, his\n\u22125), all others in the database\ndna must match (which will occur with probability 10\nmust be innocent, and none of these others can have a match. now, given that a. j.\nis innocent, the conditional probability that all the others in the database are also\ninnocent is\n\np(all others innocent|aj innocent) = p(all in database innocent)\n\np(aj innocent)\n\n= 1 \u2212 10,000\u03b1\n\n1 \u2212 \u03b1\n(cid:3)\n\nalso, the conditional probability, given their innocence, that none of the others in the\n(cid:2)\ndatabase will have a match is (1 \u2212 10\n\n\u22125)9999. therefore,\n1 \u2212 10,000\u03b1\n\np(m|gc) = 10\n\u22125\n\n(1 \u2212 10\n\n\u22125)9999\n\n1 \u2212 \u03b1\n\n "}, {"Page_number": 94, "text": "section 3.4\n\nindependent events 79\n\nbecause p(g) = \u03b1, the preceding formula gives\n\np(g|m) =\n\n\u03b1\n\n\u03b1 + 10\u22125(1 \u2212 10,000\u03b1)\n\n=\n\n1\n\n.9 + 10\u22125\n\n\u03b1\n\nthus, if the district attorney\u2019s initial feelings were that an arbitrary ex-convict was\n100 times more likely to have committed the crime than was a nonconvict (that is,\nc = 100), then \u03b1 = 1\n\n19,900 and\n\nif the district attorney initially felt that the appropriate ratio was c = 10, then \u03b1 =\n\np(g|m) = 1\n1.099\n\nl 0.9099\n\n1\n\n109, 000\n\nand\n\np(g|m) = 1\n1.99\nif the district attorney initially felt that the criminal was equally likely to be any of\nthe members of the town (c = 1), then \u03b1 = 10\np(g|m) = 1\n10.9\n\nl 0.5025\n\nl 0.0917\n\n\u22126 and\n\nthus, the probability ranges from approximately 9 percent when the district attor-\nney\u2019s initial assumption is that all the members of the population have the same\nchance of being the perpetrator to approximately 91 percent when she assumes that\neach ex-convict is 100 times more likely to be the criminal than is a specified townsper-\n.\nson who is not an ex-convict.\n\n3.4 independent events\n\nthe previous examples of this chapter show that p(e|f), the conditional probabil-\nity of e given f, is not generally equal to p(e), the unconditional probability of e.\nin other words, knowing that f has occurred generally changes the chances of e\u2019s\noccurrence. in the special cases where p(e|f) does in fact equal p(e), we say that e\nis independent of f. that is, e is independent of f if knowledge that f has occurred\ndoes not change the probability that e occurs.\n\nsince p(e|f) = p(ef)/p(f), it follows that e is independent of f if\n\np(ef) = p(e)p(f)\n\n(4.1)\n\nthe fact that equation (4.1) is symmetric in e and f shows that whenever e is inde-\npendent of f, f is also independent of e. we thus have the following definition.\n\ndefinition\ntwo events e and f are said to be independent if equation (4.1) holds.\ntwo events e and f that are not independent are said to be dependent.\n\nexample 4a\na card is selected at random from an ordinary deck of 52 playing cards. if e is the\nevent that the selected card is an ace and f is the event that it is a spade, then e\n\n "}, {"Page_number": 95, "text": "80\n\nchapter 3\n\nconditional probability and independence\n\nand f are independent. this follows because p(ef) = 1\np(f) = 13\n52.\n\n52, whereas p(e) = 4\n\n52 and\n.\n\nexample 4b\ntwo coins are flipped, and all 4 outcomes are assumed to be equally likely. if e is\nthe event that the first coin lands on heads and f the event that the second lands\non tails, then e and f are independent, since p(ef) = p({(h, t)}) = 1\n4, whereas\np(e) = p({(h, h), (h, t)}) = 1\n.\n\n2 and p(f) = p({(h, t), (t, t)}) = 1\n2.\n\nexample 4c\nsuppose that we toss 2 fair dice. let e1 denote the event that the sum of the dice is 6\nand f denote the event that the first die equals 4. then\np(e1f) = p({(4, 2)}) = 1\n36\n(cid:3)\n\n(cid:3)(cid:2)\n\nwhereas\n\n(cid:2)\n\np(e1)p(f) =\n\n5\n36\n\n1\n6\n\n= 5\n216\n\nhence, e1 and f are not independent. intuitively, the reason for this is clear because\nif we are interested in the possibility of throwing a 6 (with 2 dice), we shall be quite\nhappy if the first die lands on 4 (or, indeed, on any of the numbers 1, 2, 3, 4, and 5),\nfor then we shall still have a possibility of getting a total of 6. if, however, the first\ndie landed on 6, we would be unhappy because we would no longer have a chance of\ngetting a total of 6. in other words, our chance of getting a total of 6 depends on the\noutcome of the first die; thus, e1 and f cannot be independent.\n\nnow, suppose that we let e2 be the event that the sum of the dice equals 7. is e2\n\nindependent of f? the answer is yes, since\n\nwhereas\n\np(e2f) = p({(4, 3)}) = 1\n36\n(cid:2)\n\n(cid:3)(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\np(e2)p(f) =\n\n1\n6\n\n=\n\n1\n6\n\n1\n36\n\nwe leave it for the reader to present the intuitive argument why the event that the\n.\n\nsum of the dice equals 7 is independent of the outcome on the first die.\n\nexample 4d\nif we let e denote the event that the next president is a republican and f the event\nthat there will be a major earthquake within the next year, then most people would\nprobably be willing to assume that e and f are independent. however, there would\nprobably be some controversy over whether it is reasonable to assume that e is inde-\npendent of g, where g is the event that there will be a recession within two years\n.\nafter the election.\n\nwe now show that if e is independent of f, then e is also independent of fc.\n\n "}, {"Page_number": 96, "text": "section 3.4\n\nindependent events 81\n\nproposition 4.1. if e and f are independent, then so are e and fc.\n\nproof. assume that e and f are independent. since e = ef \u222a efc and ef and\nefc are obviously mutually exclusive, we have\n\np(e) = p(ef) + p(efc)\n\n= p(e)p(f) + p(efc)\n\nor, equivalently,\n\nand the result is proved.\n\np(efc) = p(e)[1 \u2212 p(f)]\n\n= p(e)p(fc)\n\nthus, if e is independent of f, then the probability of e\u2019s occurrence is unchanged\n\nby information as to whether or not f has occurred.\n\nsuppose now that e is independent of f and is also independent of g. is e then\nnecessarily independent of fg? the answer, somewhat surprisingly, is no, as the fol-\nlowing example demonstrates.\n\nexample 4e\ntwo fair dice are thrown. let e denote the event that the sum of the dice is 7. let f\ndenote the event that the first die equals 4 and g denote the event that the second\ndie equals 3. from example 4c, we know that e is independent of f, and the same\nreasoning as applied there shows that e is also independent of g; but clearly, e is not\nindependent of fg [since p(e|fg) = 1].\n.\n\n(cid:2)\n\n(cid:3)\n\nit would appear to follow from example 4e that an appropriate definition of the\nindependence of three events e, f, and g would have to go further than merely\nassuming that all of the\npairs of events are independent. we are thus led to the\nfollowing definition.\n\n3\n2\n\ndefinition\nthree events e, f, and g are said to be independent if\np(efg) = p(e)p(f)p(g)\np(ef) = p(e)p(f)\np(eg) = p(e)p(g)\np(fg) = p(f)p(g)\n\nnote that if e, f, and g are independent, then e will be independent of any event\n\nformed from f and g. for instance, e is independent of f \u222a g, since\n\np[e(f \u222a g)] = p(ef \u222a eg)\n\n= p(ef) + p(eg) \u2212 p(efg)\n= p(e)p(f) + p(e)p(g) \u2212 p(e)p(fg)\n= p(e)[p(f) + p(g) \u2212 p(fg)]\n= p(e)p(f \u222a g)\n\n "}, {"Page_number": 97, "text": "82\n\nchapter 3\n\nconditional probability and independence\n\nof course, we may also extend the definition of independence to more than three\nevents. the events e1, e2, . . . , en are said to be independent if, for every subset\ne1(cid:8), e2(cid:8), . . . , er(cid:8), r \u2026 n, of these events,\n\np(e1(cid:8)e2(cid:8) \u00b7\u00b7\u00b7 er(cid:8) ) = p(e1(cid:8) )p(e2(cid:8) )\u00b7\u00b7\u00b7 p(er(cid:8) )\n\nfinally, we define an infinite set of events to be independent if every finite subset of\nthose events is independent.\n\nsometimes, a probability experiment under consideration consists of performing a\nsequence of subexperiments. for instance, if the experiment consists of continually\ntossing a coin, we may think of each toss as being a subexperiment. in many cases, it\nis reasonable to assume that the outcomes of any group of the subexperiments have\nno effect on the probabilities of the outcomes of the other subexperiments. if such\nis the case, we say that the subexperiments are independent. more formally, we say\nthat the subexperiments are independent if e1, e2, . . . , en, . . . is necessarily an inde-\npendent sequence of events whenever ei is an event whose occurrence is completely\ndetermined by the outcome of the ith subexperiment.\n\nif each subexperiment has the same set of possible outcomes, then the subexperi-\n\nments are often called trials.\n\nexample 4f\nan infinite sequence of independent trials is to be performed. each trial results in a\nsuccess with probability p and a failure with probability 1 \u2212 p. what is the probabil-\nity that\n(a) at least 1 success occurs in the first n trials;\n(b) exactly k successes occur in the first n trials;\n(c) all trials result in successes?\n\nsolution. in order to determine the probability of at least 1 success in the first n trials,\nit is easiest to compute first the probability of the complementary event: that of no\nsuccesses in the first n trials. if we let ei denote the event of a failure on the ith trial,\nthen the probability of no successes is, by independence,\n\np(e1e2 \u00b7\u00b7\u00b7 en) = p(e1)p(e2)\u00b7\u00b7\u00b7 p(en) = (1 \u2212 p)n\n\n(cid:3)\n\n(cid:2)\n\nhence, the answer to part (a) is 1 \u2212 (1 \u2212 p)n.\nto compute the answer to part (b), consider any particular sequence of the first\nn outcomes containing k successes and n \u2212 k failures. each one of these sequences\nwill, by the assumed independence of trials, occur with probability pk(1 \u2212 p)n\u2212k.\nsuch sequences (there are n!/k!(n \u2212 k)! permutations of k\nsince there are\nsuccesses and n \u2212 k failures), the desired probability in part (b) is\npk(1 \u2212 p)n\u2212k\n\np{exactly k successes} =\n\n(cid:2)\n\n(cid:3)\n\nn\nk\n\nn\nk\n\nto answer part (c), we note that, by part (a), the probability of the first n trials all\n\nresulting in success is given by\n\np(ec\n\n1ec\n2\n\n\u00b7\u00b7\u00b7 ec\n\nn\n\n) = pn\n\n "}, {"Page_number": 98, "text": "thus, using the continuity property of probabilities (section 2.6), we see that the\ndesired probability is given by\n\nsection 3.4\n\nindependent events 83\n\n\u239b\n\u239d q(cid:17)\n\ni=1\n\np\n\nec\ni\n\n\u239e\n\u23a0 = p\n\n\u239b\nn(cid:17)\n\u239d lim\n\u239b\nn\u2192q\n\u239d n(cid:17)\n%\n\ni=1\n\n\u239e\n\u23a0\n\u239e\n\u23a0\n\nec\ni\n\nec\ni\n\n= lim\n\nn\u2192q p\npn =\n\n= lim\n\nn\n\ni=1\n0 if p < 1\n1 if p = 1\n\n.\n\n.\n\nexample 4g\na system composed of n separate components is said to be a parallel system if it\nfunctions when at least one of the components functions. (see figure 3.2.) for such a\nsystem, if component i, which is independent of the other components, functions with\nprobability pi, i = 1, . . . , n, what is the probability that the system functions?\nsolution. let ai denote the event that component i functions. then\np{system functions} = 1 \u2212 p{system does not function}\n\n= 1 \u2212 p{all components do not function}\n= 1 \u2212 p\n\nac\ni\n\n\u239e\n\u23a0\n\n\u239b\n\u239d(cid:17)\n= 1 \u2212 n(cid:31)\n\ni\n\ni=1\n\n(1 \u2212 pi) by independence\n\na\n\n1\n\n2\n\n3\n\nn\n\nb\n\nfigure 3.2: parallel system: functions if current flows from a to b\n\nexample 4h\nindependent trials consisting of rolling a pair of fair dice are performed. what is the\nprobability that an outcome of 5 appears before an outcome of 7 when the outcome\nof a roll is the sum of the dice?\nsolution. if we let en denote the event that no 5 or 7 appears on the first n \u2212 1 trials\nand a 5 appears on the nth trial, then the desired probability is\n\n\u239b\n\u239d q(cid:14)\n\nn=1\n\np\n\nen\n\n\u239e\n\u23a0 =\n\nq(cid:6)\n\np(en)\n\nn=1\n\n "}, {"Page_number": 99, "text": "84\n\nchapter 3\n\nconditional probability and independence\n\nnow, since p{5 on any trial} = 4\npendence of trials,\n\n36, we obtain, by the inde-\n\nthus,\n\np\n\n36\n\n(cid:2)\n\n(cid:3)n\u22121 4\n1 \u2212 10\n36\n(cid:3)n\u22121\n(cid:2)\nq(cid:6)\n\n36 and p{7 on any trial} = 6\np(en) =\n\u239b\n\u239e\n\u239d q(cid:14)\n\u23a0 = 1\n9\n= 1\n9\n= 2\n5\n\nn=1\n1\n1 \u2212 13\n\n13\n18\n\nn=1\n\nen\n\n18\n\nthis result could also have been obtained by the use of conditional probabilities.\nif we let e be the event that a 5 occurs before a 7, then we can obtain the desired\nprobability, p(e), by conditioning on the outcome of the first trial, as follows: let f\nbe the event that the first trial results in a 5, let g be the event that it results in a 7, and\nlet h be the event that the first trial results in neither a 5 nor a 7. then, conditioning\non which one of these events occurs gives\n\np(e) = p(e|f)p(f) + p(e|g)p(g) + p(e|h)p(h)\n\nhowever,\n\np(e|f) = 1\np(e|g) = 0\np(e|h) = p(e)\n\nthe first two equalities are obvious. the third follows because if the first outcome\nresults in neither a 5 nor a 7, then at that point the situation is exactly as it was when\nthe problem first started\u2014namely, the experimenter will continually roll a pair of fair\ndice until either a 5 or 7 appears. furthermore, the trials are independent; therefore,\nthe outcome of the first trial will have no effect on subsequent rolls of the dice. since\np(f) = 4\n\n36, and p(h) = 26\n\n36, p(g) = 6\n\n36, it follows that\n13\n18\n\n+ p(e)\n\np(e) = 1\n9\n\nor\n\np(e) = 2\n5\n\n36 and a 7 with probability 6\n\nthe reader should note that the answer is quite intuitive. that is, because a 5 occurs\non any roll with probability 4\n36, it seems intuitive that the\nodds that a 5 appears before a 7 should be 6 to 4 against. the probability should then\nbe 4\n\n10, as indeed it is.\nthe same argument shows that if e and f are mutually exclusive events of an\nexperiment, then, when independent trials of the experiment are performed, the\nevent e will occur before the event f with probability\n\np(e)\n\np(e) + p(f)\n\n.\n\n "}, {"Page_number": 100, "text": "section 3.4\n\nindependent events 85\n\n(cid:9)\n\nn\n\nexample 4i\nthere are n types of coupons, and each new one collected is independently of type i\ni=1 pi = 1. suppose k coupons are to be collected. if ai is the\nwith probability pi,\nevent that there is at least one type i coupon among those collected, then, for i z j,\nfind\n(a) p(ai)\n(b) p(ai \u222a aj)\n(c) p(ai|aj)\n\nsolution.\n\nwhere the preceding used that each coupon is, independently, not of type i with prob-\nability 1 \u2212 pi. similarly,\n\np(ai) = 1 \u2212 p(ac\n\n)\n\ni\n\n= 1 \u2212 p{no coupon is type i}\n= 1 \u2212 (1 \u2212 pi)k\n(cid:19)\n\n(cid:18)\nai \u222a aj)c\n= 1 \u2212 p{no coupon is either type i or type j}\n= 1 \u2212 (1 \u2212 pi \u2212 pj)k\n\np(ai \u222a aj) = 1 \u2212 p(\n\nwhere the preceding used that each coupon is, independently, neither of type i nor\ntype j with probability 1 \u2212 pi \u2212 pj.\n\nto determine p(ai|aj), we will use the identity\n\np(ai \u222a aj) = p(ai) + p(aj) \u2212 p(aiaj)\n\nwhich, in conjunction with parts (a) and (b), yields\n\np(aiaj) = 1 \u2212 (1 \u2212 pi)k + 1 \u2212 (1 \u2212 pj)k \u2212 [1 \u2212 (1 \u2212 pi \u2212 pj)k]\n\n= 1 \u2212 (1 \u2212 pi)k \u2212 (1 \u2212 pj)k + (1 \u2212 pi \u2212 pj)k\n\nconsequently,\n\np(ai|aj) = p(aiaj)\np(aj)\n\n= 1 \u2212 (1 \u2212 pi)k \u2212 (1 \u2212 pj)k + (1 \u2212 pi \u2212 pj)k\n\n1 \u2212 (1 \u2212 pj)k\n\n.\n\nthe next example presents a problem that occupies an honored place in the his-\ntory of probability theory. this is the famous problem of the points. in general terms,\nthe problem is this: two players put up stakes and play some game, with the stakes\nto go to the winner of the game. an interruption requires them to stop before either\nhas won and when each has some sort of a \u201cpartial score.\u201d how should the stakes be\ndivided?\n\nthis problem was posed to the french mathematician blaise pascal in 1654 by\nthe chevalier de m\u00b4er\u00b4e, who was a professional gambler at that time. in attacking\nthe problem, pascal introduced the important idea that the proportion of the prize\ndeserved by the competitors should depend on their respective probabilities of win-\nning if the game were to be continued at that point. pascal worked out some special\ncases and, more importantly, initiated a correspondence with the famous french-\nman pierre de fermat, who had a great reputation as a mathematician. the resulting\nexchange of letters not only led to a complete solution to the problem of the points,\n\n "}, {"Page_number": 101, "text": "86\n\nchapter 3\n\nconditional probability and independence\n\nbut also laid the framework for the solution to many other problems connected with\ngames of chance. this celebrated correspondence, dated by some as the birth date\nof probability theory, was also important in stimulating interest in probability among\nthe mathematicians in europe, for pascal and fermat were both recognized as being\namong the foremost mathematicians of the time. for instance, within a short time of\ntheir correspondence, the young dutch mathematician christiaan huygens came to\nparis to discuss these problems and solutions, and interest and activity in this new\nfield grew rapidly.\n\nexample 4j the problem of the points\nindependent trials resulting in a success with probability p and a failure with proba-\nbility 1 \u2212 p are performed. what is the probability that n successes occur before m\nfailures? if we think of a and b as playing a game such that a gains 1 point when a\nsuccess occurs and b gains 1 point when a failure occurs, then the desired probability\nis the probability that a would win if the game were to be continued in a position\nwhere a needed n and b needed m more points to win.\n\nsolution. we shall present two solutions. the first is due to pascal and the second to\nfermat.\n\nlet us denote by pn,m the probability that n successes occur before m failures. by\n\nconditioning on the outcome of the first trial, we obtain\n\npn,m = ppn\u22121,m + (1 \u2212 p)pn,m\u22121\n\nn \u00fa 1, m \u00fa 1\n\n(why? reason it out.) using the obvious boundary conditions pn, 0 = 0, p0, m = 1, we\ncan solve these equations for pn,m. rather than go through the tedious details, let us\ninstead consider fermat\u2019s solution.\nfermat argued that, in order for n successes to occur before m failures, it is nec-\nessary and sufficient that there be at least n successes in the first m + n \u2212 1 trials.\n(even if the game were to end before a total of m + n \u2212 1 trials were completed, we\ncould still imagine that the necessary additional trials were performed.) this is true,\nfor if there are at least n successes in the first m + n \u2212 1 trials, there could be at\nmost m \u2212 1 failures in those m + n \u2212 1 trials; thus, n successes would occur before\nm failures. if, however, there were fewer than n successes in the first m + n \u2212 1\ntrials, there would have to be at least m failures in that same number of trials; thus, n\nsuccesses would not occur before m failures.\nm + n \u2212 1 trials is\nprobability of n successes before m failures is\nm + n \u2212 1\n\nhence, since, as shown in example 4f, the probability of exactly k successes in\npk(1 \u2212 p)m+n\u22121\u2212k, it follows that the desired\n\n(cid:2)\npn,m = m+n\u22121(cid:6)\n\nm + n \u2212 1\n(cid:2)\n\npk(1 \u2212 p)m+n\u22121\u2212k\n\n(cid:3)\n\n(cid:3)\n\nk\n\nk\n\n.\n\nour next two examples deal with gambling problems, with the first having a sur-\n\nk=n\n\u2217\nprisingly elegant analysis.\n\nexample 4k\nsuppose that initially there are r players, with player i having ni units, ni > 0, i =\n1, . . . , r. at each stage, two of the players are chosen to play a game, with the winner\n\n\u2217\n\nthe remainder of this section should be considered optional.\n\n "}, {"Page_number": 102, "text": "section 3.4\n\nindependent events 87\n\n(cid:9)\n\nof the game receiving 1 unit from the loser. any player whose fortune drops to 0 is\neliminated, and this continues until a single player has all n k\nr\ni=1 ni units, with\nthat player designated as the victor. assuming that the results of successive games\nare independent and that each game is equally likely to be won by either of its two\nplayers, find pi, the probability that player i is the victor?\n\nsolution. to begin, suppose that there are n players, with each player initially having\n1 unit. consider player i. each stage she plays will be equally likely to result in her\neither winning or losing 1 unit, with the results from each stage being independent.\nin addition, she will continue to play stages until her fortune becomes either 0 or n.\nbecause this is the same for all n players, it follows that each player has the same\nchance of being the victor, implying that each player has probability 1/n of being the\nvictor. now, suppose these n players are divided into r teams, with team i containing\nni players, i = 1, . . . , r. then the probability that the victor is a member of team i is\nni/n. but because\n(a) team i initially has a total fortune of ni units, i = 1, . . . , r, and\n(b) each game played by members of different teams is equally likely to be won by\neither player and results in the fortune of members of the winning team increas-\ning by 1 and the fortune of the members of the losing team decreasing by 1,\n\nit is easy to see that the probability that the victor is from team i is exactly the prob-\nability we desire. thus, pi = ni/n. interestingly, our argument shows that this result\n.\ndoes not depend on how the players in each stage are chosen.\n\nin the gambler\u2019s ruin problem, there are only 2 gamblers, but they are not assumed\nto be of equal skill.\n\nexample 4l the gambler\u2019s ruin problem\ntwo gamblers, a and b, bet on the outcomes of successive flips of a coin. on each\nflip, if the coin comes up heads, a collects 1 unit from b, whereas if it comes up tails,\na pays 1 unit to b. they continue to do this until one of them runs out of money. if\nit is assumed that the successive flips of the coin are independent and each flip results\nin a head with probability p, what is the probability that a ends up with all the money\nif he starts with i units and b starts with n \u2212 i units?\nsolution. let e denote the event that a ends up with all the money when he starts\nwith i and b starts with n \u2212 i, and to make clear the dependence on the initial fortune\nof a, let pi = p(e). we shall obtain an expression for p(e) by conditioning on the\noutcome of the first flip as follows: let h denote the event that the first flip lands on\nheads; then\n\npi = p(e) = p(e|h)p(h) + p(e|hc)p(hc)\n= pp(e|h) + (1 \u2212 p)p(e|hc)\n\nnow, given that the first flip lands on heads, the situation after the first bet is that\na has i + 1 units and b has n \u2212 (i + 1). since the successive flips are assumed to\nbe independent with a common probability p of heads, it follows that, from that point\non, a\u2019s probability of winning all the money is exactly the same as if the game were\njust starting with a having an initial fortune of i + 1 and b having an initial fortune\nof n \u2212 (i + 1). therefore,\n\np(e|h) = pi+1\n\n "}, {"Page_number": 103, "text": "88\n\nchapter 3\n\nconditional probability and independence\n\nand similarly,\n\np(e|hc) = pi\u22121\n\nhence, letting q = 1 \u2212 p, we obtain\n\npi = ppi+1 + qpi\u22121\n\ni = 1, 2, . . . , n \u2212 1\n\n(4.2)\nby making use of the obvious boundary conditions p0 = 0 and pn = 1, we shall\n\nnow solve equation (4.2). since p + q = 1, these equations are equivalent to\n\nppi + qpi = ppi+1 + qpi\u22121\n\nor\n\npi+1 \u2212 pi = q\np\n\n(pi \u2212 pi\u22121)\n\ni = 1, 2, . . . , n \u2212 1\n\n(4.3)\n\nhence, since p0 = 0, we obtain, from equation (4.3),\n(p1 \u2212 p0) = q\n(cid:3)2\n(cid:2)\np\n(p2 \u2212 p1) =\n\np2 \u2212 p1 = q\np\np3 \u2212 p2 = q\np\n\nq\np\n\np1\n\np1\n\n(cid:3)i\u22121\n\n(4.4)\n\nadding the first i \u2212 1 equations of (4.4) yields\n(cid:2)\n\nor\n\n(cid:2)\n\nq\np\n\np1\n\n(cid:3)n\u22121\n\np1\n\n(cid:8)\n\n(cid:3)i\u22121\n\n(cid:2)\n\nq\np\n\n.\n.\n.\n\npi \u2212 pi\u22121 = q\np\n\n(pi\u22121 \u2212 pi\u22122) =\n\n.\n.\n.\n\n(cid:2)\n\nq\np\n\npi \u2212 p1 = p1\n\npn \u2212 pn\u22121 = q\np\n(cid:7)(cid:2)\n\u23a7\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23a9\n\u23a7\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23a9\n\np1 =\n\npi =\n\n(pn\u22121 \u2212 pn\u22122) =\n(cid:3)\n\n(cid:3)2 + \u00b7\u00b7\u00b7 +\n\n+\n\nq\np\n\nq\np\n\n1 \u2212 (q/p)i\n1 \u2212 (q/p)\nip1\n\np1\n\nif\n\nif\n\nq\np\nq\np\n\nz 1\n= 1\n\n1 \u2212 (q/p)\n1 \u2212 (q/p)n\n\n1\nn\n\nif p z 1\n2\nif p = 1\n\n2\n\nusing the fact that pn = 1, we obtain\n\n "}, {"Page_number": 104, "text": "hence,\n\npi =\n\n\u23a7\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23a9 1 \u2212 (q/p)i\n\n1 \u2212 (q/p)n\n\ni\nn\n\nsection 3.4\n\nindependent events 89\n\nif p z 1\n2\nif p = 1\n\n2\n\n(4.5)\n\nlet qi denote the probability that b winds up with all the money when a starts\nwith i and b starts with n \u2212 i. then, by symmetry to the situation described, and on\nreplacing p by q and i by n \u2212 i, it follows that\n1 \u2212 (p/q)n\u2212i\n1 \u2212 (p/q)n\nn \u2212 i\nn\n\nif q z 1\n2\nif q = 1\n\n\u23a7\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23a9\n\nqi =\n\n2\n\nmoreover, since q = 1\n\n2 is equivalent to p = 1\n\n2, we have, when q z 1\n2,\n+ 1 \u2212 (p/q)n\u2212i\n1 \u2212 (p/q)n\n+ qn \u2212 qn(p/q)n\u2212i\n\npi + qi = 1 \u2212 (q/p)i\n1 \u2212 (q/p)n\n= pn \u2212 pn(q/p)i\n= pn \u2212 pn\u2212iqi \u2212 qn + qipn\u2212i\n= 1\n\npn \u2212 qn\n\npn \u2212 qn\n\nqn \u2212 pn\n\nthis result also holds when p = q = 1\n\n2, so\npi + qi = 1\n\nin words, this equation states that, with probability 1, either a or b will wind up\nwith all of the money; in other words, the probability that the game continues indefi-\nnitely with a\u2019s fortune always being between 1 and n \u2212 1 is zero. (the reader must\nbe careful because, a priori, there are three possible outcomes of this gambling game,\nnot two: either a wins, or b wins, or the game goes on forever with nobody winning.\nwe have just shown that this last event has probability 0.)\n\nas a numerical illustration of the preceding result, if a were to start with 5 units\n2, whereas it\n\nand b with 10, then the probability of a\u2019s winning would be 1\nwould jump to\n\n3 if p were 1\n\n(cid:30)5\n(cid:30)15\n\n(cid:29)\n(cid:29)\n\n2\n3\n\n2\n3\n\n1 \u2212\n1 \u2212\n\nl .87\n\nif p were .6.\n\na special case of the gambler\u2019s ruin problem, which is also known as the problem of\nduration of play, was proposed to huygens by fermat in 1657. the version huygens\nproposed, which he himself solved, was that a and b have 12 coins each. they play\nfor these coins in a game with 3 dice as follows: whenever 11 is thrown (by either\u2014it\nmakes no difference who rolls the dice), a gives a coin to b. whenever 14 is thrown,\nb gives a coin to a. the person who first wins all the coins wins the game. since\n\n "}, {"Page_number": 105, "text": "90\n\nchapter 3\n\nconditional probability and independence\n\n216 and p{roll 14} = 15\n\np{roll 11} = 27\n216, we see from example 4h that, for a, this is\njust the gambler\u2019s ruin problem with p = 15\n42, i = 12, and n = 24. the general form\nof the gambler\u2019s ruin problem was solved by the mathematician james bernoulli and\npublished 8 years after his death in 1713.\n\nfor an application of the gambler\u2019s ruin problem to drug testing, suppose that two\nnew drugs have been developed for treating a certain disease. drug i has a cure rate\npi, i = 1, 2, in the sense that each patient treated with drug i will be cured with proba-\nbility pi. these cure rates are, however, not known, and we are interested in finding a\nmethod for deciding whether p1 > p2 or p2 > p1. to decide on one of these alterna-\ntives, consider the following test: pairs of patients are to be treated sequentially, with\none member of the pair receiving drug 1 and the other drug 2. the results for each\npair are determined, and the testing stops when the cumulative number of cures from\none of the drugs exceeds the cumulative number of cures from the other by some\nfixed, predetermined number. more formally, let\n\n%\n%\n\nxj =\nyj =\n\n1 if the patient in the jth pair that receives drug 1 is cured\n0 otherwise\n1 if the patient in the jth pair that receives drug 2 is cured\n0 otherwise\n\nfor a predetermined positive integer m, the test stops after pair n, where n is the\n\nfirst value of n such that either\n\nor\n\nx1 + \u00b7\u00b7\u00b7 + xn \u2212 (y1 + \u00b7\u00b7\u00b7 + yn) = m\nx1 + \u00b7\u00b7\u00b7 + xn \u2212 (y1 + \u00b7\u00b7\u00b7 + yn) = \u2212m\n\nin the former case, we assert that p1 > p2 and in the latter that p2 > p1.\n\nin order to help ascertain whether the foregoing is a good test, one thing we would\nlike to know is the probability that it leads to an incorrect decision. that is, for given\np1 and p2, where p1 > p2, what is the probability that the test will incorrectly assert\nthat p2 > p1? to determine this probability, note that after each pair is checked,\nthe cumulative difference of cures using drug 1 versus drug 2 will go up by 1 with\nprobability p1(1 \u2212 p2)\u2014since this is the probability that drug 1 leads to a cure and\ndrug 2 does not\u2014or go down by 1 with probability (1 \u2212 p1)p2, or remain the same\nwith probability p1p2 + (1 \u2212 p1)(1 \u2212 p2). hence, if we consider only those pairs\nin which the cumulative difference changes, then the difference will go up by 1 with\nprobability\n\np = p{up 1|up 1 or down 1}\n=\n\np1(1 \u2212 p2)\n\np1(1 \u2212 p2) + (1 \u2212 p1)p2\n\nand down by 1 with probability\n1 \u2212 p =\n\np2(1 \u2212 p1)\n\np1(1 \u2212 p2) + (1 \u2212 p1)p2\n\nthus, the probability that the test will assert that p2 > p1 is equal to the probability\nthat a gambler who wins each (one-unit) bet with probability p will go down m before\ngoing up m. but equation (4.5), with i = m, n = 2m, shows that this probability is\ngiven by\n\n "}, {"Page_number": 106, "text": "section 3.4\n\nindependent events 91\n\n(cid:3)m\np{test asserts that p2 > p1}\n1 \u2212 p\n= 1 \u2212 1 \u2212\n(cid:3)2m\np\n1 \u2212 p\n1 \u2212\n(cid:3)m\n\n(cid:2)\n(cid:2)\n(cid:2)\n\n= 1 \u2212\n\n1\n1 \u2212 p\n\np\n\n1 +\n\np\n\nwhere\n\n=\n\n1\n\n1 + \u03b3 m\n\n\u03b3 = p\n\n1 \u2212 p\n\n= p1(1 \u2212 p2)\np2(1 \u2212 p1)\n\nfor instance, if p1 = .6 and p2 = .4, then the probability of an incorrect decision is\n.017 when m = 5 and reduces to .0003 when m = 10.\n.\n\nsuppose that we are presented with a set of elements and we want to determine\nwhether at least one member of the set has a certain property. we can attack this\nquestion probabilistically by randomly choosing an element of the set in such a way\nthat each element has a positive probability of being selected. then the original ques-\ntion can be answered by a consideration of the probability that the randomly selected\nelement does not have the property of interest. if this probability is equal to 1, then\nnone of the elements of the set have the property; if it is less than 1, then at least one\nelement of the set has the property.\n\nthe final example of this section illustrates this technique.\n\nexample 4m\n\n(cid:2)\n\n(cid:3)\n\nn\n2\n\nthe complete graph having n vertices is defined to be a set of n points (called vertices)\nlines (called edges) connecting each pair of vertices. the\nin the plane and the\ncomplete graph having 3 vertices is shown in figure 3.3. suppose now that each edge\nin a complete graph having n vertices is to be colored either red or blue. for a fixed\ninteger k, a question of interest is, is there a way of coloring the edges so that no set\nconnecting edges the same color? it can be shown by\nof k vertices has all of its\na probabilistic argument that if n is not too large, then the answer is yes.\n\n(cid:2)\n\n(cid:3)\n\nk\n2\n\nfigure 3.3\n\n "}, {"Page_number": 107, "text": "92\n\nchapter 3\n\nconditional probability and independence\n\n(cid:2)\n\n(cid:3)\n\nn\nk\n\nnumber the\nfollows:\n\nthe argument runs as follows: suppose that each edge is, independently, equally\nlikely to be colored either red or blue. that is, each edge is red with probability 1\n2.\nas\n\nsets of k vertices and define the events ei, i = 1, . . . ,\nei = {all of the connecting edges of the ith set\n(cid:2)\n\n(cid:3)\nof k vertices are the same color}\n\n(cid:2)\n\n(cid:3)\n\nn\nk\n\nconnecting edges of a set of k vertices is equally likely to\nnow, since each of the\nbe either red or blue, it follows that the probability that they are all the same color is\n\nk\n2\n\n, the probability that there is a set of k vertices all of whose\n\nconnecting edges are similarly colored, satisfies\n\n\u239b\n\u239d(cid:14)\n(cid:5)\n\ni\n\nei\n\np\n\ntherefore, because\n\n(cid:4)(cid:10)\n\nwe find that p\n\nei\n\ni\n\np\n\nhence, if\n\nor, equivalently, if\n\n(cid:2)\n\n(cid:3)k(k\u22121)/2\n\n1\n2\n\np(ei) = 2\n(cid:6)\n\n\u239e\n\u23a0 \u2026\n\np(ei)\n\n(boole\u2019s inequality)\n\ni\n\n\u239b\n\u239d(cid:14)\n(cid:2)\n\ni\n\nei\n\n\u239e\n\u23a0 \u2026\n(cid:3)(cid:2)\n(cid:3)\n(cid:2)\n\n1\n2\n\nn\nk\n\nn\nk\n\n(cid:2)\n\n(cid:3)(cid:2)\n\n(cid:3)k(k\u22121)/2\u22121\n\nn\nk\n\n1\n2\n\n(cid:3)k(k\u22121)/2\u22121\n\n< 1\n\n< 2k(k\u22121)/2\u22121\n(cid:3)\n\n(cid:2)\n\nn\nk\n\nthen the probability that at least one of the\nsets of k vertices has all of its\nconnecting edges the same color is less than 1. consequently, under the preceding\ncondition on n and k, it follows that there is a positive probability that no set of k\nvertices has all of its connecting edges the same color. but this conclusion implies\nthat there is at least one way of coloring the edges for which no set of k vertices has\n.\nall of its connecting edges the same color.\n\nremarks.\n\n(a) whereas the preceding argument established a condition on n and\nk that guarantees the existence of a coloring scheme satisfying the desired property,\nit gives no information about how to obtain such a scheme (although one possibility\nwould be simply to choose the colors at random, check to see if the resulting coloring\nsatisfies the property, and repeat the procedure until it does).\n\n "}, {"Page_number": 108, "text": "section 3.5\n\np(\u00b7|f ) is a probability 93\n\n(b) the method of introducing probability into a problem whose statement is\npurely deterministic has been called the probabilistic method.\u2020 other examples of this\nmethod are given in theoretical exercise 24 and examples 2t and 2u of chapter 7.\n\n3.5 p(\u00b7|f) is a probability\n\nconditional probabilities satisfy all of the properties of ordinary probabilities, as is\nproved by proposition 5.1, which shows that p(e|f) satisfies the three axioms of a\nprobability.\nproposition 5.1.\n(a) 0 \u2026 p(e|f) \u2026 1.\n(b) p(s|f) = 1.\n(c) if ei, i = 1, 2, . . ., are mutually exclusive events, then\n\n\u239b\n\u239d q(cid:14)\n\np\n\nei|f\n\n\u239e\n\u23a0 =\n\nq(cid:6)\n\n1\n\n1\n\np(ei|f)\n\nproof. to prove part (a), we must show that 0 \u2026 p(ef)/p(f) \u2026 1. the left-side\ninequality is obvious, whereas the right side follows because ef ( f, which implies\nthat p(ef) \u2026 p(f). part (b) follows because\n= p(f)\np(s|f) = p(sf)\np(f)\np(f)\n\u239b\n\u239e\n(cid:5)\n\u239d(cid:4)\n\u23a0\n(cid:4)\nq(cid:10)\n\npart (c) follows from\n\n\u239b\n\u239d q(cid:14)\n\n\u239e\n\u23a0 =\n\ni=1\np(f)\n\nei|f\n\n= 1\n\nq(cid:10)\n\n(cid:5)\n\ni=1\n\nei\n\np\n\np\n\nf\n\n\u239e\n\u23a0 f =\n\nq(cid:14)\n\n\u239b\n\u239d q(cid:14)\n\nei\n\n1\n\neif\n\n1\n\nsince\n\np\n\nq(cid:6)\nq(cid:6)\n\n1\n\n=\n\n=\n\n=\n\neif\n\n1\np(f)\n\np(eif)\n\np(f)\np(ei|f)\n\nwhere the next-to-last equality follows because eiej = \u00f8 implies that\neifejf = \u00f8.\n\n1\n\n\u2020see n. alon, j. spencer, and p. erdos, the probabilistic method (new york: john wiley &\n\nsons, inc., 1992).\n\n "}, {"Page_number": 109, "text": "94\n\nchapter 3\n\nconditional probability and independence\n\nif we define q(e) = p(e|f), then, from proposition 5.1, q(e) may be regarded\nas a probability function on the events of s. hence, all of the propositions previously\nproved for probabilities apply to q(e). for instance, we have\n\nq(e1 \u222a e2) = q(e1) + q(e2) \u2212 q(e1e2)\n\nor, equivalently,\n\np(e1 \u222a e2|f) = p(e1|f) + p(e2|f) \u2212 p(e1e2|f)\n\nalso, if we define the conditional probability q(e1|e2) by q(e1|e2) = q(e1e2)/\nq(e2), then, from equation (3.1), we have\n\nq(e1) = q(e1|e2)q(e2) + q(e1|ec\n\n)q(ec\n2\n\n)\n\n(5.1)\n\n2\n\nsince\n\nq(e1|e2) = q(e1e2)\nq(e2)\n= p(e1e2|f)\np(e2|f)\np(e1e2f)\n\n=\n\np(f)\np(e2f)\np(f)\n= p(e1|e2f)\n\nequation (5.1) is equivalent to\n\np(e1|f) = p(e1|e2f)p(e2|f) + p(e1|ec\n\n2f)p(ec\n2\n\n|f)\n\nexample 5a\nconsider example 3a, which is concerned with an insurance company which believes\nthat people can be divided into two distinct classes: those who are accident prone\nand those who are not. during any given year, an accident-prone person will have an\naccident with probability .4, whereas the corresponding figure for a person who is not\nprone to accidents is .2. what is the conditional probability that a new policyholder\nwill have an accident in his or her second year of policy ownership, given that the\npolicyholder has had an accident in the first year?\nsolution. if we let a be the event that the policyholder is accident prone and we let\nai, i = 1, 2, be the event that he or she has had an accident in the ith year, then the\ndesired probability p(a2|a1) may be obtained by conditioning on whether or not the\npolicyholder is accident prone, as follows:\n\np(a2|a1) = p(a2|aa1)p(a|a1) + p(a2|aca1)p(ac|a1)\n\nnow,\n\n= p(a1|a)p(a)\n\np(a|a1) = p(a1a)\np(a1)\n10, and it was shown in example 3a that p(a1) =\n\np(a1)\n\nhowever, p(a) is assumed to equal 3\n.26. hence,\n\np(a|a1) = (.4)(.3)\n.26\n\n= 6\n13\n\n "}, {"Page_number": 110, "text": "section 3.5\n\np(\u00b7|f ) is a probability 95\n\nthus,\n\np(ac|a1) = 1 \u2212 p(a|a1) = 7\n13\nsince p(a2|aa1) = .4 and p(a2|aca1) = .2, it follows that\nl .29\n\np(a2|a1) = (.4)\n\n+ (.2)\n\n6\n13\n\n7\n13\n\n.\n\nexample 5b\na female chimp has given birth. it is not certain, however, which of two male chimps\nis the father. before any genetic analysis has been performed, it is felt that the\nprobability that male number 1 is the father is p and the probability that male number\n2 is the father is 1 \u2212 p. dna obtained from the mother, male number 1, and male\nnumber 2 indicate that, on one specific location of the genome, the mother has the\ngene pair (a, a), male number 1 has the gene pair (a, a), and male number 2 has the\ngene pair (a, a). if a dna test shows that the baby chimp has the gene pair (a, a),\nwhat is the probability that male number 1 is the father?\n\nsolution. let all probabilities be conditional on the event that the mother has the\ngene pair (a, a), male number 1 has the gene pair (a, a), and male number 2 has\nthe gene pair (a, a). now, let mi be the event that male number i, i = 1, 2, is the\nfather, and let ba,a be the event that the baby chimp has the gene pair (a, a). then\np(m1|ba,a) is obtained as follows:\n\n=\n\np(ba,a|m1)p(m1)\n\np(m1|ba,a) = p(m1ba,a)\np(ba,a)\np(ba,a|m1)p(m1) + p(ba,a|m2)p(m2)\n1 \u00b7 p + (1/2)(1 \u2212 p)\n= 2p\n1 + p\n\n1 \u00b7 p\n\n=\n\nbecause 2p\n1+p\n> p when p < 1, the information that the baby\u2019s gene pair is (a, a)\nincreases the probability that male number 1 is the father. this result is intuitive\nbecause it is more likely that the baby would have gene pair (a, a) if m1 is true than\n.\nif m2 is true (the respective conditional probabilities being 1 and 1/2).\n\nthe next example deals with a problem in the theory of runs.\n\nexample 5c\nindependent trials, each resulting in a success with probability p or a failure with\nprobability q = 1 \u2212 p, are performed. we are interested in computing the probability\nthat a run of n consecutive successes occurs before a run of m consecutive failures.\n\nsolution. let e be the event that a run of n consecutive successes occurs before a run\nof m consecutive failures. to obtain p(e), we start by conditioning on the outcome of\nthe first trial. that is, letting h denote the event that the first trial results in a success,\nwe obtain\n\np(e) = pp(e|h) + qp(e|hc)\n\n(5.2)\n\n "}, {"Page_number": 111, "text": "96\n\nchapter 3\n\nconditional probability and independence\n\nnow, given that the first trial was successful, one way we can get a run of n successes\nbefore a run of m failures would be to have the next n \u2212 1 trials all result in successes.\nso, let us condition on whether or not that occurs. that is, letting f be the event that\ntrials 2 through n all are successes, we obtain\n\np(e|h) = p(e|fh)p(f|h) + p(e|fch)p(fc|h)\n\n(5.3)\non the one hand, clearly, p(e|fh) = 1; on the other hand, if the event fch occurs,\nthen the first trial would result in a success, but there would be a failure some time\nduring the next n \u2212 1 trials. however, when this failure occurs, it would wipe out all\nof the previous successes, and the situation would be exactly as if we started out with\na failure. hence,\n\np(e|fch) = p(e|hc)\n\nbecause the independence of trials implies that f and h are independent, and because\np(f) = pn\u22121, it follows from equation (5.3) that\n\np(e|h) = pn\u22121 + (1 \u2212 pn\u22121)p(e|hc)\n\n(5.4)\nwe now obtain an expression for p(e|hc) in a similar manner. that is, we let g\n\ndenote the event that trials 2 through m are all failures. then\n\np(e|hc) = p(e|ghc)p(g|hc) + p(e|gchc)p(gc|hc)\n\n(5.5)\nnow, ghc is the event that the first m trials all result in failures, so p(e|ghc) = 0.\nalso, if gchc occurs, then the first trial is a failure, but there is at least one success\nin the next m \u2212 1 trials. hence, since this success wipes out all previous failures, we\nsee that\n\np(e|gchc) = p(e|h)\n\nthus, because p(gc|hc) = p(gc) = 1 \u2212 qm\u22121, we obtain, from (5.5),\n\np(e|hc) = (1 \u2212 qm\u22121)p(e|h)\n\n(5.6)\n\nsolving equations (5.4) and (5.6) yields\n\np(e|h) =\n\npn\u22121\n\npn\u22121 + qm\u22121 \u2212 pn\u22121qm\u22121\n\np(e|hc) =\n\n(1 \u2212 qm\u22121)pn\u22121\n\npn\u22121 + qm\u22121 \u2212 pn\u22121qm\u22121\n\nand\n\nthus,\n\np(e) = pp(e|h) + qp(e|hc)\n\n= pn + qpn\u22121(1 \u2212 qm\u22121)\npn\u22121 + qm\u22121 \u2212 pn\u22121qm\u22121\n=\npn\u22121 + qm\u22121 \u2212 pn\u22121qm\u22121\n\npn\u22121(1 \u2212 qm)\n\n(5.7)\n\n "}, {"Page_number": 112, "text": "section 3.5\n\np(\u00b7|f ) is a probability 97\n\nit is interesting to note that, by the symmetry of the problem, the probability of\nobtaining a run of m failures before a run of n successes would be given by equa-\ntion (5.7) with p and q interchanged and n and m interchanged. hence, this probabil-\nity would equal\n\np{run of m failures before a run of n successes}\n=\n\nqm\u22121(1 \u2212 pn)\n\nqm\u22121 + pn\u22121 \u2212 qm\u22121pn\u22121\n\n(5.8)\n\nsince equations (5.7) and (5.8) sum to 1, it follows that, with probability 1, either a\nrun of n successes or a run of m failures will eventually occur.\n\nas an example of equation (5.7), we note that, in tossing a fair coin, the probability\n10. for 2 consecutive heads before\n.\n\nthat a run of 2 heads will precede a run of 3 tails is 7\n4 consecutive tails, the probability rises to 5\n6.\n\nin our next example, we return to the matching problem (example 5m, chapter 2)\n\nand this time obtain a solution by using conditional probabilities.\n\nexample 5d\nat a party, n men take off their hats. the hats are then mixed up, and each man\nrandomly selects one. we say that a match occurs if a man selects his own hat. what\nis the probability of\n(a) no matches?\n(b) exactly k matches?\n\nsolution. (a) let e denote the event that no matches occur, and to make explicit the\ndependence on n, write pn = p(e). we start by conditioning on whether or not the\nfirst man selects his own hat\u2014call these events m and mc, respectively. then\n\npn = p(e) = p(e|m)p(m) + p(e|mc)p(mc)\n\nclearly, p(e|m) = 0, so\n\npn = p(e|mc)\n\nn \u2212 1\n\nn\n\n(5.9)\nnow, p(e|mc) is the probability of no matches when n \u2212 1 men select from a set of\nn \u2212 1 hats that does not contain the hat of one of these men. this can happen in either\nof two mutually exclusive ways: either there are no matches and the extra man does\nnot select the extra hat (this being the hat of the man who chose first), or there are\nno matches and the extra man does select the extra hat. the probability of the first of\nthese events is just pn\u22121, which is seen by regarding the extra hat as \u201cbelonging\u201d to\nthe extra man. because the second event has probability [1/(n \u2212 1)]pn\u22122, we have\n\np(e|mc) = pn\u22121 +\n\n1\n\nn \u2212 1\n\npn\u22122\n\nthus, from equation (5.9),\n\npn = n \u2212 1\n\nn\n\npn\u22121 + 1\nn\n\npn\u22122\n\nor, equivalently,\n\npn \u2212 pn\u22121 = \u2212 1\nn\n\n(pn\u22121 \u2212 pn\u22122)\n\n(5.10)\n\n "}, {"Page_number": 113, "text": "98\n\nchapter 3\n\nconditional probability and independence\n\nhowever, since pn is the probability of no matches when n men select among their\nown hats, we have\n\np1 = 0\n\np2 = 1\n2\n\nso, from equation (5.10),\n\np3 \u2212 p2 = \u2212 (p2 \u2212 p1)\np4 \u2212 p3 = \u2212 (p3 \u2212 p2)\n\n3\n\n4\n\n= \u2212 1\n3!\n= 1\n4!\n\nor p3 = 1\n\u2212 1\n3!\n2!\n\u2212 1\n+ 1\nor p4 = 1\n4!\n3!\n2!\n\nand, in general,\n\npn = 1\n2!\n\n\u2212 1\n3!\n\n+ 1\n4!\n\n\u2212 \u00b7\u00b7\u00b7 + (\u22121)n\n\nn!\n\n(b) to obtain the probability of exactly k matches, we consider any fixed group of\n\nk men. the probability that they, and only they, select their own hats is\n\n1\nn\n\n1\n\nn \u2212 1\n\n\u00b7\u00b7\u00b7\n\n1\n\nn \u2212 (k \u2212 1)\n\npn\u2212k = (n \u2212 k)!\n\npn\u2212k\n\n(cid:2)\n\nn!\n\n(cid:3)\n\nwhere pn\u2212k is the conditional probability that the other n \u2212 k men, selecting among\nchoices of a set of k men, the\ntheir own hats, have no matches. since there are\ndesired probability of exactly k matches is\n\u2212 1\n3!\n\n+ \u00b7\u00b7\u00b7 + (\u22121)n\u2212k\n(n \u2212 k)!\n\nn\nk\n\n1\n2!\n\n.\n\npn\u2212k\nk!\n\n=\n\nk!\n\nan important concept in probability theory is that of the conditional independence\nof events. we say that the events e1 and e2 are conditionally independent given f\nif, given that f occurs, the conditional probability that e1 occurs is unchanged by\ninformation as to whether or not e2 occurs. more formally, e1 and e2 are said to be\nconditionally independent given f if\n\nor, equivalently,\n\np(e1|e2f) = p(e1|f)\n\np(e1e2|f) = p(e1|f)p(e2|f)\n\n(5.11)\n\n(5.12)\n\nthe notion of conditional independence can easily be extended to more than two\n\nevents, and this extension is left as an exercise.\n\nthe reader should note that the concept of conditional independence was implic-\nitly employed in example 5a, where it was assumed that the events that a policyholder\nhad an accident in his or her ith year, i = 1, 2, . . ., were conditionally independent\ngiven whether or not the person was accident prone. the following example, some-\ntimes referred to as laplace\u2019s rule of succession, further illustrates the concept of\nconditional independence.\n\nexample 5e laplace\u2019s rule of succession\nthere are k + 1 coins in a box. when flipped, the ith coin will turn up heads with\nprobability i/k, i = 0, 1, . . . , k. a coin is randomly selected from the box and is then\n\n "}, {"Page_number": 114, "text": "section 3.5\n\np(\u00b7|f ) is a probability 99\n\nrepeatedly flipped. if the first n flips all result in heads, what is the conditional prob-\nability that the (n + 1)st flip will do likewise?\nsolution. let ci denote the event that the ith coin, i = 0, 1, . . . , k, is initially selected;\nlet fn denote the event that the first n flips all result in heads; and let h be the event\nthat the (n + 1)st flip is a head. the desired probability, p(h|fn), is now obtained as\nfollows:\n\np(h|fn) = k(cid:6)\n\ni=0\n\np(h|fnci)p(ci|fn)\n\nnow, given that the ith coin is selected, it is reasonable to assume that the outcomes\nwill be conditionally independent, with each one resulting in a head with probability\ni/k. hence,\n\np(h|fnci) = p(h|ci) = i\nk\n\np(ci|fn) = p(cifn)\np(fn)\n\nalso,\n\nthus,\n\n= p(fn|ci)p(ci)\nk(cid:6)\np(fn|cj)p(cj)\n\n= (i/k)n[1/(k + 1)]\nk(cid:6)\n(j/k)n[1/(k + 1)]\n\nj=0\n\nj=0\n\np(h|fn) =\n\nk(cid:6)\n(i/k)n+1\nk(cid:6)\n\ni=0\n\n(j/k)n\n\nj=0\n\nbut if k is large, we can use the integral approximations\n\n1\nk\n\n(cid:2)\nk(cid:6)\nk(cid:6)\n\ni=0\n1\nk\n\nj=0\n\n(cid:3)n+1\n(cid:3)n\n(cid:2)\n\ni\nk\n\nj\nk\n\n*\n*\n\nl\n\nl\n\n1\n\n1\n\nxn+1dx = 1\n\nn + 2\n\nxndx = 1\n\nn + 1\n\n0\n\n0\n\nso, for k large,\n\np(h|fn) l n + 1\nn + 2\n(cid:9)\nexample 5f updating information sequentially\nsuppose there are n mutually exclusive and exhaustive possible hypotheses, with ini-\ni=1 p(hi) = 1. now, if\ntial (sometimes referred to as prior) probabilities p(hi),\ninformation that the event e has occurred is received, then the conditional probabil-\nity that hi is the true hypothesis (sometimes referred to as the updated or posterior\nprobability of hi) is\n\n.\n\nn\n\n(cid:9)\np(hi|e) = p(e|hi)p(hi)\nj p(e|hj)p(hj)\n\n(5.13)\n\n "}, {"Page_number": 115, "text": "100\n\nchapter 3\n\nconditional probability and independence\n\nsuppose now that we learn first that e1 has occurred and then that e2 has occurred.\nthen, given only the first piece of information, the conditional probability that hi is\nthe true hypothesis is\n\np(hi|e1) = p(e1|hi)p(hi)\n\np(e1)\n\n(cid:9)\n= p(e1|hi)p(hi)\nj p(e1|hj)p(hj)\n\nwhereas given both pieces of information, the conditional probability that hi is the\ntrue hypothesis is p(hi|e1e2), which can be computed by\n(cid:9)\np(hi|e1e2) = p(e1e2|hi)p(hi)\nj p(e1e2|hj)p(hj)\n\none might wonder, however, when one can compute p(hi|e1e2) by using the\nright side of equation (5.13) with e = e2 and with p(hj) replaced by p(hj|e1),\nj = 1, . . . , n. that is, when is it legitimate to regard p(hj|e1), j \u00fa 1, as the prior\nprobabilities and then use (5.13) to compute the posterior probabilities?\nsolution. the answer is that the preceding is legitimate, provided that, for each j =\n1, . . . , n, the events e1 and e2 are conditionally independent, given hj. for if this is\nthe case, then\n\np(e1e2|hj) = p(e2|hj)p(e1|hj),\n\nj = 1, . . . , n\n\ntherefore,\n\np(hi|e1e2) = p(e2|hi)p(e1|hi)p(hi)\n\np(e1e2)\n\np(e1e2)\n= p(e2|hi)p(e1hi)\n= p(e2|hi)p(hi|e1)p(e1)\np(e1e2)\n= p(e2|hi)p(hi|e1)\n\nq(1, 2)\n\nwhere q(1, 2) = p(e1e2)\nupon summing,\n\np(e1) . since the preceding equation is valid for all i, we obtain,\n\nshowing that\n\nand yielding the result\n\n1 = n(cid:6)\n\ni=1\n\np(e2|hi)p(hi|e1)\n\ni=1\n\nq(1, 2)\n\np(hi|e1e2) = n(cid:6)\nq(1, 2) = n(cid:6)\n(cid:9)\np(e2|hi)p(hi|e1)\ni=1 p(e2|hi)p(hi|e1)\n\np(e2|hi)p(hi|e1)\n\ni=1\n\nn\n\np(hi|e1e2) =\n\nfor instance, suppose that one of two coins is chosen to be flipped. let hi be the event\nthat coin i, i = 1, 2, is chosen, and suppose that when coin i is flipped, it lands on heads\nwith probability pi, i = 1, 2. then the preceding equations show that, to sequentially\n\n "}, {"Page_number": 116, "text": "summary 101\n\nupdate the probability that coin 1 is the one being flipped, given the results of the\nprevious flips, all that must be saved after each new flip is the conditional probability\nthat coin 1 is the coin being used. that is, it is not necessary to keep track of all earlier\n.\nresults.\n\nsummary\nfor events e and f, the conditional probability of e given that f has occurred is\ndenoted by p(e|f) and is defined by\n\np(e|f) = p(ef)\np(f)\n\nthe identity\n\np(e1e2 \u00b7\u00b7\u00b7 en) = p(e1)p(e2|e1)\u00b7\u00b7\u00b7 p(en|e1 \u00b7\u00b7\u00b7 en\u22121)\n\nis known as the multiplication rule of probability.\n\na valuable identity is\n\np(e) = p(e|f)p(f) + p(e|fc)p(fc)\n\nwhich can be used to compute p(e) by \u201cconditioning\u201d on whether f occurs.\n\np(h)/p(hc) is called the odds of the event h. the identity\n\np(h|e)\np(hc|e)\n\n= p(h) p(e|h)\np(hc)p(e|hc)\n\nshows that when new evidence e is obtained, the value of the odds of h becomes its\nold value multiplied by the ratio of the conditional probability of the new evidence\nwhen h is true to the conditional probability when h is not true.\nlet fi, i = 1, . . . , n, be mutually exclusive events whose union is the entire sample\n\nspace. the identity\n\np(fj|e) = p(e|fj)p(fj)\np(e|fi)p(fi)\n\nn(cid:6)\n\ni=1\n\nis known as bayes\u2019s formula. if the events fi, i = 1, . . . , n, are competing hypotheses,\nthen bayes\u2019s formula shows how to compute the conditional probabilities of these\nhypotheses when additional evidence e becomes available.\nif p(ef) = p(e)p(f), then we say that the events e and f are independent. this\ncondition is equivalent to p(e|f) = p(e) and to p(f|e) = p(f). thus, the events e\nand f are independent if knowledge of the occurrence of one of them does not affect\nthe probability of the other.\n\nthe events e1, . . . , en are said to be independent if, for any subset ei1, . . . , eir\n\nof them,\n\np(ei1\n\n\u00b7\u00b7\u00b7 eir\n\n) = p(ei1\n\n)\u00b7\u00b7\u00b7 p(eir\n\n)\n\nfor a fixed event f, p(e|f) can be considered to be a probability function on the\nevents e of the sample space.\n\n "}, {"Page_number": 117, "text": "102\n\nchapter 3\n\nconditional probability and independence\n\nproblems\n\n3.1. two fair dice are rolled. what is the conditional\nprobability that at least one lands on 6 given that\nthe dice land on different numbers?\n\n3.2. if two fair dice are rolled, what is the conditional\nprobability that the first one lands on 6 given that\nthe sum of the dice is i? compute for all values of\ni between 2 and 12.\n\n3.3. use equation (2.1) to compute,\n\nin a hand of\nbridge, the conditional probability that east has\n3 spades given that north and south have a com-\nbined total of 8 spades.\n\n3.4. what is the probability that at least one of a pair of\nfair dice lands on 6, given that the sum of the dice\nis i, i = 2, 3, . . . , 12?\n\n3.5. an urn contains 6 white and 9 black balls. if 4 balls\nare to be randomly selected without replacement,\nwhat is the probability that the first 2 selected are\nwhite and the last 2 black?\n\n3.6. consider an urn containing 12 balls, of which 8\nare white. a sample of size 4 is to be drawn with\nreplacement (without replacement). what is the\nconditional probability (in each case) that the first\nand third balls drawn will be white given that the\nsample drawn contains exactly 3 white balls?\n\n3.7. the king comes from a family of 2 children. what\nis the probability that the other child is his sister?\n3.8. a couple has 2 children. what is the probability\nthat both are girls if the older of the two is a girl?\n3.9. consider 3 urns. urn a contains 2 white and 4 red\nballs, urn b contains 8 white and 4 red balls, and\nurn c contains 1 white and 3 red balls. if 1 ball is\nselected from each urn, what is the probability that\nthe ball chosen from urn a was white given that\nexactly 2 white balls were selected?\n\n3.10. three cards are randomly selected, without\nreplacement, from an ordinary deck of 52 playing\ncards. compute the conditional probability that\nthe first card selected is a spade given that the sec-\nond and third cards are spades.\n\n3.11. two cards are randomly chosen without replace-\nment from an ordinary deck of 52 cards. let b be\nthe event that both cards are aces, let as be the\nevent that the ace of spades is chosen, and let a be\nthe event that at least one ace is chosen. find\n(a) p(b|as)\n(b) p(b|a)\n\n3.12. a recent college graduate is planning to take the\nfirst three actuarial examinations in the coming\nsummer. she will take the first actuarial exam in\njune. if she passes that exam, then she will take\nthe second exam in july, and if she also passes that\none, then she will take the third exam in septem-\nber. if she fails an exam, then she is not allowed\n\nto take any others. the probability that she passes\nthe first exam is .9. if she passes the first exam, then\nthe conditional probability that she passes the sec-\nond one is .8, and if she passes both the first and\nthe second exams, then the conditional probability\nthat she passes the third exam is .7.\n(a) what is the probability that she passes all\n\nthree exams?\n\n(b) given that she did not pass all three exams,\nwhat is the conditional probability that she\nfailed the second exam?\n\n3.13. suppose that an ordinary deck of 52 cards (which\ncontains 4 aces) is randomly divided into 4 hands\nof 13 cards each. we are interested in determining\np, the probability that each hand has an ace. let ei\nbe the event that the ith hand has exactly one ace.\ndetermine p = p(e1e2e3e4) by using the multi-\nplication rule.\n\n3.14. an urn initially contains 5 white and 7 black balls.\neach time a ball is selected, its color is noted and\nit is replaced in the urn along with 2 other balls of\nthe same color. compute the probability that\n(a) the first 2 balls selected are black and the next\n\n2 are white;\n\n(b) of the first 4 balls selected, exactly 2 are black.\n3.15. an ectopic pregnancy is twice as likely to develop\nwhen the pregnant woman is a smoker as it is when\nshe is a nonsmoker. if 32 percent of women of\nchildbearing age are smokers, what percentage of\nwomen having ectopic pregnancies are smokers?\n\n3.16. ninety-eight percent of all babies survive delivery.\nhowever, 15 percent of all births involve cesarean\n(c) sections, and when a c section is performed,\nthe baby survives 96 percent of the time. if a ran-\ndomly chosen pregnant woman does not have a\nc section, what is the probability that her baby\nsurvives?\n\n3.17. in a certain community, 36 percent of the families\nown a dog and 22 percent of the families that own\na dog also own a cat. in addition, 30 percent of the\nfamilies own a cat. what is\n(a) the probability that a randomly selected fam-\n\nily owns both a dog and a cat?\n\n(b) the conditional probability that a randomly\nselected family owns a dog given that it owns\na cat?\n\n3.18. a total of 46 percent of the voters in a certain city\nclassify themselves as independents, whereas 30\npercent classify themselves as liberals and 24 per-\ncent say that they are conservatives. in a recent\nlocal election, 35 percent of the independents, 62\npercent of the liberals, and 58 percent of the con-\nservatives voted. a voter is chosen at random.\n\n "}, {"Page_number": 118, "text": "given that this person voted in the local election,\nwhat is the probability that he or she is\n(a) an independent?\n(b) a liberal?\n(c) a conservative?\n(d) what fraction of voters participated in the\n\nlocal election?\n\n3.19. a total of 48 percent of the women and 37 percent\nof the men that took a certain \u201cquit smoking\u201d class\nremained nonsmokers for at least one year after\ncompleting the class. these people then attended\na success party at the end of a year. if 62 percent\nof the original class was male,\n(a) what percentage of those attending the party\n\n(b) what percentage of the original class attended\n\nwere women?\n\nthe party?\n\n3.20. fifty-two percent of the students at a certain col-\nlege are females. five percent of the students in\nthis college are majoring in computer science. two\npercent of the students are women majoring in\ncomputer science. if a student is selected at ran-\ndom, find the conditional probability that\n(a) the student is female given that the student is\n\nmajoring in computer science;\n\n(b) this student is majoring in computer science\n\ngiven that the student is female.\n\n3.21. a total of 500 married working couples were\npolled about their annual salaries, with the follow-\ning information resulting:\n\nwife\n\nhusband\n\nless than\n$25,000\n\nmore than\n\n$25,000\n\nless than $25,000\nmore than $25,000\n\n212\n36\n\n198\n54\n\nfor instance, in 36 of the couples, the wife earned\nmore and the husband earned less than $25,000. if\none of the couples is randomly chosen, what is\n(a) the probability that the husband earns less\n\nthan $25,000?\n\n(b) the conditional probability that the wife earns\nmore than $25,000 given that the husband\nearns more than this amount?\n\n(c) the conditional probability that the wife earns\nmore than $25,000 given that the husband\nearns less than this amount?\n\n3.22. a red die, a blue die, and a yellow die (all six\nsided) are rolled. we are interested in the prob-\nability that the number appearing on the blue die\nis less than that appearing on the yellow die, which\nis less than that appearing on the red die. that is,\n\nproblems 103\n\nwith b, y, and r denoting, respectively, the num-\nber appearing on the blue, yellow, and red die, we\nare interested in p(b < y < r).\n(a) what is the probability that no two of the dice\n\nland on the same number?\n\n(b) given that no two of the dice land on the same\nnumber, what is the conditional probability\nthat b < y < r?\n\n(c) what is p(b < y < r)?\n\n3.23. urn i contains 2 white and 4 red balls, whereas urn\nii contains 1 white and 1 red ball. a ball is ran-\ndomly chosen from urn i and put into urn ii, and a\nball is then randomly selected from urn ii. what is\n(a) the probability that the ball selected from urn\n\nii is white?\n\n(b) the conditional probability that the trans-\nferred ball was white given that a white ball\nis selected from urn ii?\n\n3.24. each of 2 balls is painted either black or gold and\nthen placed in an urn. suppose that each ball is col-\nored black with probability 1\n2 and that these events\nare independent.\n(a) suppose that you obtain information that the\ngold paint has been used (and thus at least\none of the balls is painted gold). compute\nthe conditional probability that both balls are\npainted gold.\n\n(b) suppose now that the urn tips over and 1 ball\nfalls out. it is painted gold. what is the prob-\nability that both balls are gold in this case?\nexplain.\n\n3.25. the following method was proposed to estimate\nthe number of people over the age of 50 who reside\nin a town of known population 100,000: \u201cas you\nwalk along the streets, keep a running count of the\npercentage of people you encounter who are over\n50. do this for a few days; then multiply the per-\ncentage you obtain by 100,000 to obtain the esti-\nmate.\u201d comment on this method.\nhint: let p denote the proportion of people in the\ntown who are over 50. furthermore, let \u03b11 denote\nthe proportion of time that a person under the age\nof 50 spends in the streets, and let \u03b12 be the cor-\nresponding value for those over 50. what quantity\ndoes the method suggested estimate? when is the\nestimate approximately equal to p?\n\n3.26. suppose that 5 percent of men and .25 percent\nof women are color blind. a color-blind person\nis chosen at random. what is the probability of\nthis person being male? assume that there are an\nequal number of males and females. what if the\npopulation consisted of twice as many males as\nfemales?\n\n3.27. all the workers at a certain company drive to\nwork and park in the company\u2019s lot. the company\n\n "}, {"Page_number": 119, "text": "104\n\nchapter 3\n\nconditional probability and independence\n\nis interested in estimating the average number of\nworkers in a car. which of the following methods\nwill enable the company to estimate this quantity?\nexplain your answer.\n1. randomly choose n workers, find out how\nmany were in the cars in which they were\ndriven, and take the average of the n values.\n\n2. randomly choose n cars in the lot, find out how\nmany were driven in those cars, and take the\naverage of the n values.\n\n3.28. suppose that an ordinary deck of 52 cards is shuf-\nfled and the cards are then turned over one at a\ntime until the first ace appears. given that the first\nace is the 20th card to appear, what is the condi-\ntional probability that the card following it is the\n(a) ace of spades?\n(b) two of clubs?\n\n3.29. there are 15 tennis balls in a box, of which 9 have\nnot previously been used. three of the balls are\nrandomly chosen, played with, and then returned\nto the box. later, another 3 balls are randomly\nchosen from the box. find the probability that\nnone of these balls has ever been used.\n\n3.30. consider two boxes, one containing 1 black and 1\nwhite marble, the other 2 black and 1 white mar-\nble. a box is selected at random, and a marble is\ndrawn from it at random. what is the probability\nthat the marble is black? what is the probability\nthat the first box was the one selected given that\nthe marble is white?\n\n3.31. ms. aquina has just had a biopsy on a possibly can-\ncerous tumor. not wanting to spoil a weekend fam-\nily event, she does not want to hear any bad news\nin the next few days. but if she tells the doctor to\ncall only if the news is good, then if the doctor does\nnot call, ms. aquina can conclude that the news is\nbad. so, being a student of probability, ms. aquina\ninstructs the doctor to flip a coin. if it comes up\nheads, the doctor is to call if the news is good and\nnot call if the news is bad. if the coin comes up\ntails, the doctor is not to call. in this way, even if\nthe doctor doesn\u2019t call, the news is not necessarily\nbad. let \u03b1 be the probability that the tumor is can-\ncerous; let \u03b2 be the conditional probability that the\ntumor is cancerous given that the doctor does not\ncall.\n(a) which should be larger, \u03b1 or \u03b2?\n(b) find \u03b2 in terms of \u03b1, and prove your answer\n\nin part (a).\n\n3.32. a family has j children with probability pj, where\np1 = .1, p2 = .25, p3 = .35, p4 = .3. a child\nfrom this family is randomly chosen. given that\nthis child is the eldest child in the family, find the\nconditional probability that the family has\n\n(a) only 1 child;\n(b) 4 children.\nredo (a) and (b) when the randomly selected child\nis the youngest child of the family.\n\n3.33. on rainy days, joe is late to work with probability\n.3; on nonrainy days, he is late with probability .1.\nwith probability .7, it will rain tomorrow.\n(a) find the probability that joe is early tomor-\n\nrow.\n\n(b) given that joe was early, what is the condi-\n\ntional probability that it rained?\n\n3.34. in example 3f, suppose that the new evidence is\nsubject to different possible interpretations and in\nfact shows only that it is 90 percent likely that the\ncriminal possesses the characteristic in question. in\nthis case, how likely would it be that the suspect is\nguilty (assuming, as before, that he has the charac-\nteristic)?\n\n3.35. with probability .6, the present was hidden by\nmom; with probability .4, it was hidden by dad.\nwhen mom hides the present, she hides it upstairs\n70 percent of the time and downstairs 30 percent\nof the time. dad is equally likely to hide it upstairs\nor downstairs.\n(a) what is the probability that the present is\n\nupstairs?\n\n(b) given that it is downstairs, what is the proba-\n\nbility it was hidden by dad?\n\n3.36. stores a, b, and c have 50, 75, and 100 employees,\nrespectively, and 50, 60, and 70 percent of them\nrespectively are women. resignations are equally\nlikely among all employees, regardless of sex. one\nwoman employee resigns. what is the probability\nthat she works in store c?\n\n3.37. (a) a gambler has a fair coin and a two-headed\ncoin in his pocket. he selects one of the coins\nat random; when he flips it, it shows heads.\nwhat is the probability that it is the fair coin?\n(b) suppose that he flips the same coin a second\ntime and, again, it shows heads. now what is\nthe probability that it is the fair coin?\n\n(c) suppose that he flips the same coin a third\ntime and it shows tails. now what is the prob-\nability that it is the fair coin?\n\n3.38. urn a has 5 white and 7 black balls. urn b has\n3 white and 12 black balls. we flip a fair coin. if\nthe outcome is heads, then a ball from urn a is\nselected, whereas if the outcome is tails, then a ball\nfrom urn b is selected. suppose that a white ball\nis selected. what is the probability that the coin\nlanded tails?\n\n3.39. in example 3a, what is the probability that some-\none has an accident in the second year given that\nhe or she had no accidents in the first year?\n\n3.40. consider a sample of size 3 drawn in the following\nmanner: we start with an urn containing 5 white\n\n "}, {"Page_number": 120, "text": "and 7 red balls. at each stage, a ball is drawn\nand its color is noted. the ball is then returned\nto the urn, along with an additional ball of the\nsame color. find the probability that the sample\nwill contain exactly\n(a) 0 white balls;\n(b) 1 white ball;\n(c) 3 white balls;\n(d) 2 white balls.\n\n3.41. a deck of cards is shuffled and then divided into\ntwo halves of 26 cards each. a card is drawn from\none of the halves; it turns out to be an ace. the ace\nis then placed in the second half-deck. the half is\nthen shuffled, and a card is drawn from it. com-\npute the probability that this drawn card is an ace.\nhint: condition on whether or not the inter-\nchanged card is selected.\n\n3.42. three cooks, a, b, and c, bake a special kind of\ncake, and with respective probabilities .02, .03, and\n.05, it fails to rise. in the restaurant where they\nwork, a bakes 50 percent of these cakes, b 30 per-\ncent, and c 20 percent. what proportion of \u201cfail-\nures\u201d is caused by a?\n\n3.43. there are 3 coins in a box. one is a two-headed\ncoin, another is a fair coin, and the third is a biased\ncoin that comes up heads 75 percent of the time.\nwhen one of the 3 coins is selected at random and\nflipped, it shows heads. what is the probability that\nit was the two-headed coin?\n\n3.44. three prisoners are informed by their jailer that\none of them has been chosen at random to be\nexecuted and the other two are to be freed. pris-\noner a asks the jailer to tell him privately which of\nhis fellow prisoners will be set free, claiming that\nthere would be no harm in divulging this informa-\ntion because he already knows that at least one of\nthe two will go free. the jailer refuses to answer\nthe question, pointing out that if a knew which\nof his fellow prisoners were to be set free, then\nhis own probability of being executed would rise\nfrom 1\n2 because he would then be one of two\nprisoners. what do you think of\nthe jailer\u2019s\nreasoning?\n\n3 to 1\n\n3.45. suppose we have 10 coins such that if the ith\ncoin is flipped, heads will appear with probabil-\nity i/10, i = 1, 2, . . . , 10. when one of the coins\nis randomly selected and flipped, it shows heads.\nwhat is the conditional probability that it was the\nfifth coin?\n\n3.46. in any given year, a male automobile policyholder\nwill make a claim with probability pm and a female\npolicyholder will make a claim with probability pf ,\nwhere pf z pm. the fraction of the policyholders\nthat are male is \u03b1, 0 < \u03b1 < 1. a policyholder is\nrandomly chosen. if ai denotes the event that this\n\nproblems 105\n\npolicyholder will make a claim in year i, show that\n\np(a2|a1) > p(a1)\n\ngive an intuitive explanation of why the preceding\ninequality is true.\n\n3.47. an urn contains 5 white and 10 black balls. a fair\ndie is rolled and that number of balls is randomly\nchosen from the urn. what is the probability that\nall of the balls selected are white? what is the con-\nditional probability that the die landed on 3 if all\nthe balls selected are white?\n\n3.48. each of 2 cabinets identical in appearance has 2\ndrawers. cabinet a contains a silver coin in each\ndrawer, and cabinet b contains a silver coin in\none of its drawers and a gold coin in the other.\na cabinet is randomly selected, one of its drawers\nis opened, and a silver coin is found. what is the\nprobability that there is a silver coin in the other\ndrawer?\n\nthe test\n\n3.49. prostate cancer is the most common type of can-\ncer found in males. as an indicator of whether a\nmale has prostate cancer, doctors often perform\na test that measures the level of the prostate-\nspecific antigen (psa) that is produced only by the\nprostate gland. although psa levels are indica-\ntive of cancer,\nis notoriously unreli-\nable. indeed, the probability that a noncancerous\nman will have an elevated psa level is approx-\nimately .135, increasing to approximately .268 if\nthe man does have cancer. if, on the basis of other\nfactors, a physician is 70 percent certain that a\nmale has prostate cancer, what is the conditional\nprobability that he has the cancer given that\n(a) the test indicated an elevated psa level?\n(b) the test did not indicate an elevated psa\n\nlevel?\n\nrepeat the preceding calculation, this time assum-\ning that the physician initially believes that there\nis a 30 percent chance that the man has prostate\ncancer.\n\n3.50. suppose that an insurance company classifies peo-\nple into one of three classes: good risks, average\nrisks, and bad risks. the company\u2019s records indi-\ncate that the probabilities that good-, average-, and\nbad-risk persons will be involved in an accident\nover a 1-year span are, respectively, .05, .15, and\n.30. if 20 percent of the population is a good risk,\n50 percent an average risk, and 30 percent a bad\nrisk, what proportion of people have accidents in\na fixed year? if policyholder a had no accidents\nin 1997, what is the probability that he or she is a\ngood or average risk?\n\n3.51. a worker has asked her supervisor for a letter of\nrecommendation for a new job. she estimates that\nthere is an 80 percent chance that she will get the\n\n "}, {"Page_number": 121, "text": "106\n\nchapter 3\n\nconditional probability and independence\n\njob if she receives a strong recommendation, a 40\npercent chance if she receives a moderately good\nrecommendation, and a 10 percent chance if she\nreceives a weak recommendation. she further esti-\nmates that the probabilities that the recommenda-\ntion will be strong, moderate, and weak are .7, .2,\nand .1, respectively.\n(a) how certain is she that she will receive the\n\nnew job offer?\n\n(b) given that she does receive the offer, how\nlikely should she feel that she received a\nstrong recommendation? a moderate recom-\nmendation? a weak recommendation?\n\n(c) given that she does not receive the job offer,\nhow likely should she feel that she received a\nstrong recommendation? a moderate recom-\nmendation? a weak recommendation?\n\n3.52. a high school student is anxiously waiting to\nreceive mail telling her whether she has been\naccepted to a certain college. she estimates that\nthe conditional probabilities of receiving notifica-\ntion on each day of next week, given that she is\naccepted and that she is rejected, are as follows:\n\nday\n\nmonday\ntuesday\nwednesday\nthursday\nfriday\n\np(mail|accepted) p(mail|rejected)\n\n.15\n.20\n.25\n.15\n.10\n\n.05\n.10\n.10\n.15\n.20\n\nshe estimates\naccepted is .6.\n(a) what is the probability that she receives mail\n\nthat her probability of being\n\non monday?\n\n(b) what is the conditional probability that she\nreceived mail on tuesday given that she does\nnot receive mail on monday?\n\n(c) if there is no mail through wednesday, what\nis the conditional probability that she will be\naccepted?\n\n(d) what is the conditional probability that she\nwill be accepted if mail comes on thursday?\n(e) what is the conditional probability that she\nwill be accepted if no mail arrives that week?\n3.53. a parallel system functions whenever at least one\nof its components works. consider a parallel sys-\ntem of n components, and suppose that each com-\nponent works independently with probability 1\n2 .\nfind the conditional probability that component 1\nworks given that the system is functioning.\n\n3.54. if you had to construct a mathematical model for\nevents e and f, as described in parts (a) through\n\n(e), would you assume that they were independent\nevents? explain your reasoning.\n(a) e is the event that a businesswoman has blue\neyes, and f is the event that her secretary has\nblue eyes.\n\n(b) e is the event that a professor owns a car,\nand f is the event that he is listed in the tele-\nphone book.\n\n(c) e is the event that a man is under 6 feet tall,\nand f is the event that he weighs over 200\npounds.\n\n(d) e is the event that a woman lives in the united\nstates, and f is the event that she lives in the\nwestern hemisphere.\n\n(e) e is the event that it will rain tomorrow, and\nf is the event that it will rain the day after\ntomorrow.\n\n3.55. in a class, there are 4 freshman boys, 6 freshman\ngirls, and 6 sophomore boys. how many sopho-\nmore girls must be present if sex and class are to\nbe independent when a student is selected at ran-\ndom?\n\n3.56. suppose that you continually collect coupons and\nthat there are m different types. suppose also that\neach time a new coupon is obtained, it is a type\ni coupon with probability pi, i = 1, . . . , m. suppose\nthat you have just collected your nth coupon. what\nis the probability that it is a new type?\nhint: condition on the type of this coupon.\n\n3.57. a simplified model for the movement of the price\nof a stock supposes that on each day the stock\u2019s\nprice either moves up 1 unit with probability p or\nmoves down 1 unit with probability 1 \u2212 p. the\nchanges on different days are assumed to be inde-\npendent.\n(a) what is the probability that after 2 days the\n\nstock will be at its original price?\n\n(b) what is the probability that after 3 days the\n\nstock\u2019s price will have increased by 1 unit?\n\n(c) given that after 3 days the stock\u2019s price has\nincreased by 1 unit, what is the probability\nthat it went up on the first day?\n\n3.58. suppose that we want to generate the outcome\nof the flip of a fair coin, but that all we have at\nour disposal is a biased coin which lands on heads\nwith some unknown probability p that need not be\nequal to 1\n2 . consider the following procedure for\naccomplishing our task:\n1. flip the coin.\n2. flip the coin again.\n3. if both flips land on heads or both land on tails,\n\nreturn to step 1.\n\n4. let the result of the last flip be the result of the\n\nexperiment.\n\n "}, {"Page_number": 122, "text": "(a) show that the result is equally likely to be\n\neither heads or tails.\n\n(b) could we use a simpler procedure that contin-\nues to flip the coin until the last two flips are\ndifferent and then lets the result be the out-\ncome of the final flip?\n\n3.59. independent flips of a coin that lands on heads\nwith probability p are made. what is the proba-\nbility that the first four outcomes are\n(a) h, h, h, h?\n(b) t, h, h, h?\n(c) what is the probability that the pattern t, h,\nh, h occurs before the pattern h, h, h, h?\nhint for part (c): how can the pattern h, h, h, h\noccur first?\n\n3.60. the color of a person\u2019s eyes is determined by a sin-\ngle pair of genes. if they are both blue-eyed genes,\nthen the person will have blue eyes; if they are\nboth brown-eyed genes, then the person will have\nbrown eyes; and if one of them is a blue-eyed gene\nand the other a brown-eyed gene, then the per-\nson will have brown eyes. (because of the latter\nfact, we say that the brown-eyed gene is dominant\nover the blue-eyed one.) a newborn child inde-\npendently receives one eye gene from each of its\nparents, and the gene it receives from a parent is\nequally likely to be either of the two eye genes of\nthat parent. suppose that smith and both of his\nparents have brown eyes, but smith\u2019s sister has\nblue eyes.\n(a) what is the probability that smith possesses a\n\nblue-eyed gene?\n\n(b) suppose that smith\u2019s wife has blue eyes. what\nis the probability that their first child will have\nblue eyes?\n\n(c) if their first child has brown eyes, what is the\nprobability that their next child will also have\nbrown eyes?\n\n3.61. genes relating to albinism are denoted by a and\na. only those people who receive the a gene from\nboth parents will be albino. persons having the\ngene pair a, a are normal in appearance and,\nbecause they can pass on the trait to their off-\nspring, are called carriers. suppose that a normal\ncouple has two children, exactly one of whom is\nan albino. suppose that the nonalbino child mates\nwith a person who is known to be a carrier for\nalbinism.\n(a) what is the probability that their first off-\n\nspring is an albino?\n\n(b) what is the conditional probability that their\nsecond offspring is an albino given that their\nfirstborn is not?\n\n3.62. barbara and dianne go target shooting. suppose\nthat each of barbara\u2019s shots hits a wooden duck\ntarget with probability p1, while each shot of\n\nproblems 107\n\ndianne\u2019s hits it with probability p2. suppose that\nthey shoot simultaneously at the same target. if\nthe wooden duck is knocked over (indicating that\nit was hit), what is the probability that\n(a) both shots hit the duck?\n(b) barbara\u2019s shot hit the duck?\nwhat independence assumptions have you made?\n3.63. a and b are involved in a duel. the rules of the\nduel are that they are to pick up their guns and\nshoot at each other simultaneously. if one or both\nare hit, then the duel is over. if both shots miss,\nthen they repeat the process. suppose that the\nresults of the shots are independent and that each\nshot of a will hit b with probability pa, and each\nshot of b will hit a with probability pb. what is\n(a) the probability that a is not hit?\n(b) the probability that both duelists are hit?\n(c) the probability that the duel ends after the nth\n\nround of shots?\n\n(d) the conditional probability that the duel ends\nafter the nth round of shots given that a is\nnot hit?\n\n(e) the conditional probability that the duel ends\nafter the nth round of shots given that both\nduelists are hit?\n\n3.64. a true\u2013false question is to be posed to a husband-\nand-wife team on a quiz show. both the husband\nand the wife will independently give the correct\nanswer with probability p. which of the following\nis a better strategy for the couple?\n(a) choose one of them and let that person\n\nanswer the question.\n\n(b) have them both consider the question, and\nthen either give the common answer if they\nagree or, if they disagree, flip a coin to deter-\nmine which answer to give.\n3.65. in problem 3.5, if p = .6 and the couple uses the\nstrategy in part (b), what is the conditional prob-\nability that the couple gives the correct answer\ngiven that the husband and wife (a) agree? (b) dis-\nagree?\n3.66. the probability of the closing of the ith relay in the\ncircuits shown in figure 3.4 is given by pi, i = 1, 2,\n3, 4, 5. if all relays function independently, what is\nthe probability that a current flows between a and\nb for the respective circuits?\nhint for (b): condition on whether relay 3 closes.\n3.67. an engineering system consisting of n compo-\nnents is said to be a k-out-of-n system (k \u2026 n)\nif the system functions if and only if at least\nk of the n components function. suppose that\nall components function independently of each\nother.\n(a) if the ith component functions with probabil-\nity pi, i = 1, 2, 3, 4, compute the probability\nthat a 2-out-of-4 system functions.\n\n "}, {"Page_number": 123, "text": "108\n\nchapter 3\n\nconditional probability and independence\n\n(a)\n\n(b)\n\na\n\na\n\n1\n\n3\n\n1\n\n2\n\n3\n\n2\n\n4\n\n4\n\n5\n\nb\n\n5\n\nb\n\nfigure 3.4: circuits for problem 3.66\n\n(b) repeat part (a) for a 3-out-of-5 system.\n(c) repeat for a k-out-of-n system when all the pi\nequal p (that is, pi = p, i = 1, 2, . . . , n).\n\n3.68. in problem 3.65a, find the conditional probability\nthat relays 1 and 2 are both closed given that a cur-\nrent flows from a to b.\n\n3.69. a certain organism possesses a pair of each of 5\ndifferent genes (which we will designate by the\nfirst 5 letters of the english alphabet). each gene\nappears in 2 forms (which we designate by low-\nercase and capital letters). the capital letter will\nbe assumed to be the dominant gene, in the sense\nthat if an organism possesses the gene pair xx,\nthen it will outwardly have the appearance of the\nx gene. for instance, if x stands for brown eyes\nand x for blue eyes, then an individual having\neither gene pair xx or xx will have brown eyes,\nwhereas one having gene pair xx will have blue\neyes. the characteristic appearance of an organ-\nism is called its phenotype, whereas its genetic\nconstitution is called its genotype. (thus, 2 organ-\nisms with respective genotypes aa, bb, cc, dd,\nee and aa, bb, cc, dd, ee would have different\ngenotypes but the same phenotype.) in a mating\nbetween 2 organisms, each one contributes, at ran-\ndom, one of its gene pairs of each type. the 5\ncontributions of an organism (one of each of the\n5 types) are assumed to be independent and are\nalso independent of the contributions of the organ-\nism\u2019s mate. in a mating between organisms hav-\ning genotypes aa, bb, cc, dd, ee and aa, bb, cc,\ndd, ee what is the probability that the progeny\nwill\n(i) phenotypically and (ii) genotypically\nresemble\n(a) the first parent?\n(b) the second parent?\n\n(c) either parent?\n(d) neither parent?\n\n3.70. there is a 50\u201350 chance that the queen carries the\ngene for hemophilia. if she is a carrier, then each\nprince has a 50\u201350 chance of having hemophilia. if\nthe queen has had three princes without the dis-\nease, what is the probability that the queen is a\ncarrier? if there is a fourth prince, what is the prob-\nability that he will have hemophilia?\n\n3.71. on the morning of september 30, 1982, the won\u2013\nlost records of the three leading baseball teams in\nthe western division of the national league were\nas follows:\n\nteam\n\nwon\n\nlost\n\natlanta braves\nsan francisco giants\nlos angeles dodgers\n\n87\n86\n86\n\n72\n73\n73\n\neach team had 3 games remaining. all 3 of the\ngiants\u2019 games were with the dodgers, and the 3\nremaining games of the braves were against the\nsan diego padres. suppose that the outcomes of\nall remaining games are independent and each\ngame is equally likely to be won by either partic-\nipant. for each team, what is the probability that it\nwill win the division title? if two teams tie for first\nplace, they have a playoff game, which each team\nhas an equal chance of winning.\n\n3.72. a town council of 7 members contains a steering\ncommittee of size 3. new ideas for legislation go\nfirst to the steering committee and then on to the\ncouncil as a whole if at least 2 of the 3 commit-\ntee members approve the legislation. once at the\nfull council, the legislation requires a majority vote\n\n "}, {"Page_number": 124, "text": "(of at least 4) to pass. consider a new piece of\nlegislation, and suppose that each town council\nmember will approve it, independently, with prob-\nability p. what is the probability that a given steer-\ning committee member\u2019s vote is decisive in the\nsense that if that person\u2019s vote were reversed,\nthen the final fate of the legislation would be\nreversed? what is the corresponding probability\nfor a given council member not on the steering\ncommittee?\n\n3.73. suppose that each child born to a couple is equally\nlikely to be a boy or a girl, independently of the\nsex distribution of the other children in the fam-\nily. for a couple having 5 children, compute the\nprobabilities of the following events:\n(a) all children are of the same sex.\n(b) the 3 eldest are boys and the others girls.\n(c) exactly 3 are boys.\n(d) the 2 oldest are girls.\n(e) there is at least 1 girl.\n\n3.74. a and b alternate rolling a pair of dice, stopping\neither when a rolls the sum 9 or when b rolls the\nsum 6. assuming that a rolls first, find the proba-\nbility that the final roll is made by a.\n\n3.75. in a certain village, it is traditional for the eldest\nson (or the older son in a two-son family) and\nhis wife to be responsible for taking care of his\nparents as they age. in recent years, however, the\nwomen of this village, not wanting that responsi-\nbility, have not looked favorably upon marrying an\neldest son.\n(a) if every family in the village has two children,\n\nwhat proportion of all sons are older sons?\n\n(b) if every family in the village has three chil-\ndren, what proportion of all sons are eldest\nsons?\n\nassume that each child is, independently, equally\nlikely to be either a boy or a girl.\n\n3.76. suppose that e and f are mutually exclusive\nevents of an experiment. show that if independent\ntrials of this experiment are performed, then e\nwill occur before f with probability p(e)/[p(e) +\np(f)].\n\n3.77. consider an unending sequence of independent\ntrials, where each trial is equally likely to result in\nany of the outcomes 1, 2, or 3. given that outcome\n3 is the last of the three outcomes to occur, find the\nconditional probability that\n(a) the first trial results in outcome 1;\n(b) the first two trials both result in outcome 1.\n\n3.78. a and b play a series of games. each game is inde-\npendently won by a with probability p and by b\nwith probability 1 \u2212 p. they stop when the total\nnumber of wins of one of the players is two greater\nthan that of the other player. the player with the\n\nproblems 109\n\ngreater number of total wins is declared the winner\nof the series.\n(a) find the probability that a total of 4 games are\n\n(b) find the probability that a is the winner of\n\nplayed.\n\nthe series.\n\n3.79. in successive rolls of a pair of fair dice, what is the\nprobability of getting 2 sevens before 6 even num-\nbers?\n\n3.80. in a certain contest, the players are of equal skill\nand the probability is 1\n2 that a specified one of\nthe two contestants will be the victor. in a group\nof 2n players, the players are paired off against\neach other at random. the 2n\u22121 winners are again\npaired off randomly, and so on, until a single win-\nner remains. consider two specified contestants, a\nand b, and define the events ai, i \u2026 n, e by\n\nai : a plays in exactly i contests:\ne: a and b never play each other.\n\n(a) find p(ai), i = 1, . . . , n.\n(b) find p(e).\n(c) let pn = p(e). show that\n\npn =\n\n1\n\n2n \u2212 1\n\n+ 2n \u2212 2\n2n \u2212 1\n\n(cid:3)2\n\n(cid:2)\n\n1\n2\n\npn\u22121\n\nand use this formula to check the answer you\nobtained in part (b).\nhint: find p(e) by conditioning on which of\nthe events ai, i = 1, . . . , n occur. in simplifying\nyour answer, use the algebraic identity\n\nixi\u22121 = 1 \u2212 nxn\u22121 + (n \u2212 1)xn\n\n(1 \u2212 x)2\n\nn\u22121(cid:6)\n\ni=1\n\nfor another approach to solving this problem,\nnote that there are a total of 2n \u2212 1 games\nplayed.\n\n(d) explain why 2n \u2212 1 games are played.\n\nnumber these games, and let bi denote the\nevent that a and b play each other in game\ni, i = 1, . . . , 2n \u2212 1.\n\n(e) what is p(bi)?\n(f) use part (e) to find p(e).\n\n3.81. an investor owns shares in a stock whose present\nvalue is 25. she has decided that she must sell her\nstock if it goes either down to 10 or up to 40. if each\nchange of price is either up 1 point with probabil-\nity .55 or down 1 point with probability .45, and\nthe successive changes are independent, what is\nthe probability that the investor retires a winner?\n3.82. a and b flip coins. a starts and continues flipping\nuntil a tail occurs, at which point b starts flipping\nand continues until there is a tail. then a takes\n\n "}, {"Page_number": 125, "text": "110\n\nchapter 3\n\nconditional probability and independence\n\nover, and so on. let p1 be the probability of the\ncoin\u2019s landing on heads when a flips and p2 when\nb flips. the winner of the game is the first one\nto get\n(a) 2 heads in a row;\n(b) a total of 2 heads;\n(c) 3 heads in a row;\n(d) a total of 3 heads.\nin each case, find the probability that a wins.\n\n3.83. die a has 4 red and 2 white faces, whereas die b\nhas 2 red and 4 white faces. a fair coin is flipped\nonce. if it lands on heads, the game continues with\ndie a; if it lands on tails, then die b is to be used.\n(a) show that the probability of red at any throw\n\nis 1\n2 .\n\n(b) if the first two throws result in red, what is the\n\nprobability of red at the third throw?\n\n(c) if red turns up at the first two throws, what\nis the probability that it is die a that is being\nused?\n\n3.84. an urn contains 12 balls, of which 4 are white.\nthree players\u2014a, b, and c\u2014successively draw\nfrom the urn, a first, then b, then c, then a, and so\non. the winner is the first one to draw a white ball.\nfind the probability of winning for each player if\n(a) each ball is replaced after it is drawn;\n(b) the balls that are withdrawn are not replaced.\n3.85. repeat problem 3.84 when each of the 3 players\nselects from his own urn. that is, suppose that\nthere are 3 different urns of 12 balls with 4 white\nballs in each urn.\n3.86. let s = {1, 2, . . . , n} and suppose that a and b are,\nindependently, equally likely to be any of the 2n\nsubsets (including the null set and s itself) of s.\n(a) show that\n\n(cid:2)\n\n(cid:3)n\n\n3\n4\n\np{a ( b} =\n\nhint: let n(b) denote the number of ele-\nments in b. use\n\np{a ( b} = n(cid:6)\n\ni=0\n\nshow that p{ab = \u00f8} =\n\n(cid:29)\n\n(cid:30)n\n\n.\n\n3\n4\n\np{a ( b|n(b) = i}p{n(b) = i}\n\n3.87. in example 5e, what is the conditional probability\nthat the ith coin was selected given that the first n\ntrials all result in heads?\n\n3.88. in laplace\u2019s rule of succession (example 5e), are\nthe outcomes of the successive flips independent?\nexplain.\n\n3.89. a person tried by a 3-judge panel is declared guilty\nif at least 2 judges cast votes of guilty. suppose\nthat when the defendant is in fact guilty, each\njudge will independently vote guilty with proba-\nbility .7, whereas when the defendant is in fact\ninnocent, this probability drops to .2. if 70 per-\ncent of defendants are guilty, compute the condi-\ntional probability that judge number 3 votes guilty\ngiven that\n(a) judges 1 and 2 vote guilty;\n(b) judges 1 and 2 cast 1 guilty and 1 not\n\nguilty vote;\n\n(c) judges 1 and 2 both cast not guilty votes.\nlet ei, i = 1, 2, 3 denote the event that judge\ni casts a guilty vote. are these events inde-\npendent. are they conditionally independent?\nexplain.\n\n3.90. suppose that n independent trials, each of which\nresults in any of the outcomes 0, 1, or 2, with\ni=0 pi = 1,\nrespective probabilities p0, p1, and p2,\nare performed. find the probability that outcomes\n1 and 2 both occur at least once.\n\n2\n\n(cid:9)\n\ntheoretical exercises\n\n3.1. show that if p(a) > 0, then\n\np(ab|a) \u00fa p(ab|a \u222a b)\n\n3.2. let a ( b. express the following probabilities as\n\nsimply as possible:\n\np(a|b), p(a|bc), p(b|a), p(b|ac)\n\n3.3. consider a school community of m families, with ni\nni = m.\nof them having i children, i = 1, . . . , k,\nconsider the following two methods for choosing\na child:\n\ni=1\n\nk(cid:9)\n\n1. choose one of the m families at random and\nthen randomly choose a child from that family.\n\n2. choose one of the\n\nini children at random.\n\nk(cid:9)\n\ni=1\n\nshow that method 1 is more likely than method 2\nto result in the choice of a firstborn child.\nhint: in solving this problem, you will need to\nshow that\n\nk(cid:6)\n\nk(cid:6)\n\nini\n\ni=1\n\nj=1\n\n\u00fa\n\nnj\nj\n\nk(cid:6)\n\nk(cid:6)\n\nni\n\nnj\n\nj=1\n\ni=1\n\n "}, {"Page_number": 126, "text": "to do so, multiply the sums and show that, for all\npairs i, j, the coefficient of the term ninj is greater\nin the expression on the left than in the one on the\nright.\n\n3.4. a ball is in any one of n boxes and is in the ith box\nwith probability pi. if the ball is in box i, a search of\nthat box will uncover it with probability \u03b1i. show\nthat the conditional probability that the ball is in\nbox j, given that a search of box i did not uncover\nit, is\n\npj\n\n1 \u2212 \u03b1ipi\n(1 \u2212 \u03b1i)pi\n1 \u2212 \u03b1ipi\n\nif j z i\nif j = i\n\n3.5. an event f is said to carry negative information\n\nabout an event e, and we write f r e, if\n\np(e|f) \u2026 p(e)\n\nprove or give counterexamples to the following\nassertions:\n(a) if f r e, then e r f.\n(b) if f r e and e r g, then f r g.\n(c) if f r e and g r e, then fg r e.\nrepeat parts (a), (b), and (c) when r is replaced\nby q, where we say that f carries positive informa-\ntion about e, written f q e, when p(e|f) \u00fa p(e).\nif e1, e2, . . . , en are independent\n\n3.6. prove that\nevents, then\n\np(e1 \u222a e2 \u222a \u00b7\u00b7\u00b7 \u222a en) = 1 \u2212 n(cid:31)\n\n[1 \u2212 p(ei)]\n\ni=1\n\n3.7. (a) an urn contains n white and m black balls.\nthe balls are withdrawn one at a time until\nonly those of the same color are left. show\nthat, with probability n/(n + m), they are all\nwhite.\nhint: imagine that the experiment continues\nuntil all the balls are removed, and consider\nthe last ball withdrawn.\n\n(b) a pond contains 3 distinct species of fish,\nwhich we will call the red, blue, and green\nfish. there are r red, b blue, and g green fish.\nsuppose that the fish are removed from the\npond in a random order. (that is, each selec-\ntion is equally likely to be any of the remain-\ning fish.) what is the probability that the red\nfish are the first species to become extinct in\nthe pond?\nhint: write p{r} = p{rbg} + p{rgb},\nand compute the probabilities on the right\nby first conditioning on the last species to be\nremoved.\n\ntheoretical exercises\n\n111\n\n3.8. let a, b, and c be events relating to the experi-\n\nment of rolling a pair of dice.\n(a) if\n\nand p(a|cc) > p(b|cc)\np(a|c) > p(b|c)\neither prove that p(a) > p(b) or give a coun-\nterexample by defining events a, b, and c for\nwhich that relationship is not true.\n\n(b) if\n\np(a|c) > p(a|cc)\nand p(b|c) > p(b|cc)\neither prove that p(ab|c) > p(ab|cc) or\ngive a counterexample by defining events\na, b, and c for which that relationship is not\ntrue.\n\nhint: let c be the event that the sum of a pair of\ndice is 10; let a be the event that the first die lands\non 6; let b be the event that the second die lands\non 6.\n\n3.9. consider two independent tosses of a fair coin. let\na be the event that the first toss results in heads, let\nb be the event that the second toss results in heads,\nand let c be the event that in both tosses the coin\nlands on the same side. show that the events a, b,\nand c are pairwise independent\u2014that is, a and b\nare independent, a and c are independent, and b\nand c are independent\u2014but not independent.\n\n3.10. consider a collection of n individuals. assume that\neach person\u2019s birthday is equally likely to be any of\nthe 365 days of the year and also that the birthdays\nare independent. let ai,j, i z j, denote the event\nthat persons i and j have the same birthday. show\nthat these events are pairwise independent, but not\nindependent. that is, show that ai,j and ar,s are\nevents ai,j, i z j are not\nindependent, but the\nindependent.\n\n(cid:2)\n\n(cid:3)\n\nn\n2\n\n3.11. in each of n independent tosses of a coin, the coin\nlands on heads with probability p. how large need\nn be so that the probability of obtaining at least\none head is at least 1\n2 ?\n\n3.12. show that 0 \u2026 ai \u2026 1, i = 1, 2, . . ., then\n\n\u23a1\n\u23a2\u23a3ai\n\nq(cid:6)\n\ni=1\n\ni\u22121(cid:31)\n\nj=1\n\n\u23a4\n\u23a5\u23a6 +\n(1 \u2212 aj)\n\nq(cid:31)\n\ni=1\n\n(1 \u2212 ai) = 1\n\nhint: suppose that an infinite number of coins are\nto be flipped. let ai be the probability that the ith\ncoin lands on heads, and consider when the first\nhead occurs.\n\n3.13. the probability of getting a head on a single toss\nof a coin is p. suppose that a starts and continues\nto flip the coin until a tail shows up, at which point\n\n "}, {"Page_number": 127, "text": "112\n\nchapter 3\n\nconditional probability and independence\n\nb starts flipping. then b continues to flip until a\ntail comes up, at which point a takes over, and so\non. let pn,m denote the probability that a accu-\nmulates a total of n heads before b accumulates\nm. show that\n\npn,m = ppn\u22121,m + (1 \u2212 p)(1 \u2212 pm,n)\n\n\u22173.14. suppose that you are gambling against an infinitely\nrich adversary and at each stage you either win\nor lose 1 unit with respective probabilities p and\n1 \u2212 p. show that the probability that you eventu-\nally go broke is\n\n1\n\n(q/p)i\n\nif p \u2026 1\n2\nif p > 1\n2\n\nwhere q = 1 \u2212 p and where i is your initial fortune.\n3.15. independent trials that result in a success with\nprobability p are successively performed until a\ntotal of r successes is obtained. show that the prob-\nability that exactly n trials are required is\n\n(cid:3)\n\n(cid:2)\n\nn \u2212 1\nr \u2212 1\n\npr(1 \u2212 p)n\u2212r\n\nuse this result to solve the problem of the points\n(example 4j).\nhint: in order for it to take n trials to obtain r suc-\ncesses, how many successes must occur in the first\nn \u2212 1 trials?\n3.16. independent trials that result in a success with\nprobability p and a failure with probability 1 \u2212\np are called bernoulli trials. let pn denote the\nprobability that n bernoulli trials result in an even\nnumber of successes (0 being considered an even\nnumber). show that\n\npn = p(1 \u2212 pn\u22121) + (1 \u2212 p)pn\u22121 n \u00fa 1\nand use this formula to prove (by induction) that\n\npn = 1 + (1 \u2212 2p)n\n\n2\n\n3.17. suppose that n independent trials are performed,\nwith trial i being a success with probability 1/(2i +\n1). let pn denote the probability that the total\nnumber of successes that result is an odd number.\n(a) find pn for n = 1, 2, 3, 4, 5.\n(b) conjecture a general formula for pn.\n(c) derive a formula for pn in terms of pn\u22121.\n(d) verify that your conjecture in part (b) satisfies\nthe recursive formula in part (d). because the\nrecursive formula has a unique solution, this\nthen proves that your conjecture is correct.\n\n3.18. let qn denote the probability that no run of 3 con-\nsecutive heads appears in n tosses of a fair coin.\nshow that\n\nqn\u22121 + 1\nqn = 1\n4\n2\nq0 = q1 = q2 = 1\n\nqn\u22122 + 1\n8\n\nqn\u22123\n\nfind q8.\nhint: condition on the first tail.\n\n3.19. consider the gambler\u2019s ruin problem, with the\nexception that a and b agree to play no more than\nn games. let pn,i denote the probability that a\nwinds up with all the money when a starts with\ni and b starts with n \u2212 i. derive an equation\nfor pn,i in terms of pn\u22121, i+1 and pn\u22121, i\u22121, and\ncompute p7, 3, n = 5.\n\n3.20. consider two urns, each containing both white\nand black balls. the probabilities of drawing white\nballs from the first and second urns are, respec-\ntively, p and p\n. balls are sequentially selected with\nreplacement as follows: with probability \u03b1, a ball\nis initially chosen from the first urn, and with prob-\nability 1 \u2212 \u03b1, it is chosen from the second urn. the\nsubsequent selections are then made according to\nthe rule that whenever a white ball is drawn (and\nreplaced), the next ball is drawn from the same\nurn, but when a black ball is drawn, the next ball is\ntaken from the other urn. let \u03b1n denote the prob-\nability that the nth ball is chosen from the first urn.\nshow that\n\n(cid:8)\n\n\u03b1n+1 = \u03b1n(p + p\n\n(cid:8) \u2212 1) + 1 \u2212 p\n\n(cid:8)\n\nn \u00fa 1\n\n(cid:4)\n\nand use this formula to prove that\n\n(cid:8)\n\n\u03b1n = 1 \u2212 p\n2 \u2212 p \u2212 p(cid:8) +\n* (p + p\n\n(cid:8) \u2212 1)n\u22121\n\n\u03b1 \u2212\n\n1 \u2212 p\n\n(cid:8)\n\n2 \u2212 p \u2212 p(cid:8)\n\n(cid:5)\n\nlet pn denote the probability that\nthe nth\nball selected is white. find pn. also, compute\nlimn\u2192q \u03b1n and limn\u2192q pn.\n\n3.21. the ballot problem. in an election, candidate a\nreceives n votes and candidate b receives m votes,\nwhere n > m. assuming that all of the (n +\nm)!/n! m! orderings of the votes are equally likely,\nlet pn,m denote the probability that a is always\nahead in the counting of the votes.\n(a) compute p2,1, p3,1, p3,2, p4,1, p4,2, p4,3.\n(b) find pn,1, pn,2.\n(c) on the basis of your results in parts (a) and\n\n(b), conjecture the value of pn,m.\n\n(d) derive a recursion for pn,m in terms of pn\u22121,m\nand pn,m\u22121 by conditioning on who receives\nthe last vote.\n\n "}, {"Page_number": 128, "text": "(e) use part (d) to verify your conjecture in part\n\n(c) by an induction proof on n + m.\n\n3.22. as a simplified model for weather forecasting, sup-\npose that the weather (either wet or dry) tomor-\nrow will be the same as the weather today with\nprobability p. show that the weather is dry on jan-\nuary 1, then pn, the probability that it will be dry n\ndays later, satisfies\n\npn = (2p \u2212 1)pn\u22121 + (1 \u2212 p)\np0 = 1\nprove that\n\nn \u00fa 1\n\npn = 1\n2\n\n+ 1\n2\n\n(2p \u2212 1)n\n\nn \u00fa 0\n\n3.23. a bag contains a white and b black balls. balls\nare chosen from the bag according to the following\nmethod:\n1. a ball is chosen at random and is discarded.\n2. a second ball is then chosen. if its color is\ndifferent from that of the preceding ball, it is\nreplaced in the bag and the process is repeated\nfrom the beginning. if its color is the same, it is\ndiscarded and we start from step 2.\n\nin other words, balls are sampled and discarded\nuntil a change in color occurs, at which point the\nlast ball is returned to the urn and the process\nstarts anew. let pa,b denote the probability that\nthe last ball in the bag is white. prove that\n\npa,b = 1\n2\n\n(cid:3)\n\n(cid:2)\nhint: use induction on k k a + b.\n\n\u22173.24. a round-robin tournament of n contestants is a\ntournament in which each of the\npairs of\ncontestants play each other exactly once, with the\noutcome of any play being that one of the contes-\ntants wins and the other loses. for a fixed integer\nk, k < n, a question of interest is whether it is pos-\nsible that the tournament outcome is such that, for\nevery set of k players, there is a player who beat\neach member of that set. show that if\n\nn\n2\n\n(cid:2)\n\n(cid:3)(cid:7)\n\n(cid:2)\n\n(cid:8)n\u2212k\n\n(cid:3)k\n\nn\nk\n\n1 \u2212\n\n1\n2\n\n< 1\n\nthen such an outcome is possible.\nhint: suppose that the results of the games are\nindependent and that each game is equally likely\n\ntheoretical exercises\n\n(cid:2)\n\n113\n\n(cid:3)\n\nto be won by either contestant. number the\nsets of k contestants, and let bi denote the event\nthat no contestant beat all of the k players in\nthe ith set. then use boole\u2019s inequality to bound\n\n(cid:5)\n\nn\nk\n\n(cid:4)(cid:10)\n\ni\n\np\n\nbi\n\n.\n\n3.25. prove directly that\n\np(e|f) = p(e|fg)p(g|f) + p(e|fgc)p(gc|f)\n3.26. prove the equivalence of equations (5.11) and\n\n(5.12).\n\n3.27. extend the definition of conditional independence\n\nto more than 2 events.\n\n3.28. prove or give a counterexample. if e1 and e2 are\nindependent, then they are conditionally indepen-\ndent given f.\n\n3.29. in laplace\u2019s rule of succession (example 5e),\nshow that if the first n flips all result in heads,\nthen the conditional probability that the next m\nflips also result in all heads is (n + 1)/(n +\nm + 1).\n\n3.30. in laplace\u2019s rule of succession (example 5e), sup-\npose that the first n flips resulted in r heads and\nn \u2212 r tails. show that the probability that the\n(n + 1)st flip turns up heads is (r + 1)/(n + 2). to\ndo so, you will have to prove and use the identity\n\n*\n\n1\n\nyn(1 \u2212 y)mdy =\n\nn!m!\n\n(n + m + 1)!\n\n1\n\n0\n\n-\nhint: to prove the identity,\n0 yn(1 \u2212 y)mdy. integrating by parts yields\nc(n + 1, m \u2212 1)\n\nc(n, m) = m\nn + 1\n\nlet c(n, m) =\n\nstarting with c(n, 0) = 1/(n + 1), prove the iden-\ntity by induction on m.\n\n3.31. suppose that a nonmathematical, but philosophi-\ncally minded, friend of yours claims that laplace\u2019s\nrule of succession must be incorrect because it can\nlead to ridiculous conclusions. \u201cfor instance,\u201d says\nhe, \u201cthe rule states that if a boy is 10 years old,\nhaving lived 10 years, the boy has probability 11\n12 of\nliving another year. on the other hand, if the boy\nhas an 80-year-old grandfather, then, by laplace\u2019s\nrule, the grandfather has probability 81\n82 of sur-\nviving another year. however, this is ridiculous.\nclearly, the boy is more likely to survive an addi-\ntional year than the grandfather is.\u201d how would\nyou answer your friend?\n\n "}, {"Page_number": 129, "text": "114\n\nchapter 3\n\nconditional probability and independence\n\nself-test problems and exercises\n\n3.1. in a game of bridge, west has no aces. what is the\nprobability of his partner\u2019s having (a) no aces? (b)\n2 or more aces? (c) what would the probabilities\nbe if west had exactly 1 ace?\n\n3.2. the probability that a new car battery functions\nfor over 10,000 miles is .8, the probability that it\nfunctions for over 20,000 miles is .4, and the prob-\nability that it functions for over 30,000 miles is .1. if\na new car battery is still working after 10,000 miles,\nwhat is the probability that\n(a) its total life will exceed 20,000 miles?\n(b) its additional life will exceed 20,000 miles?\n\n3.3. how can 20 balls, 10 white and 10 black, be put\ninto two urns so as to maximize the probability of\ndrawing a white ball if an urn is selected at random\nand a ball is drawn at random from it?\n\n3.4. urn a contains 2 white balls and 1 black ball,\nwhereas urn b contains 1 white ball and 5 black\nballs. a ball is drawn at random from urn a and\nplaced in urn b. a ball is then drawn from urn b.\nit happens to be white. what is the probability that\nthe ball transferred was white?\n\n3.5. an urn has r red and w white balls that are ran-\ndomly removed one at a time. let ri be the event\nthat the ith ball removed is red. find\n(a) p(ri)\n(b) p(r5|r3)\n(c) p(r3|r5)\n\n3.6. an urn contains b black balls and r red balls. one\nof the balls is drawn at random, but when it is\nput back in the urn, c additional balls of the same\ncolor are put in with it. now, suppose that we\ndraw another ball. show that the probability that\nthe first ball was black, given that the second ball\ndrawn was red, is b/(b + r + c).\n\n3.7. a friend randomly chooses two cards, without\nreplacement, from an ordinary deck of 52 playing\ncards. in each of the following situations, deter-\nmine the conditional probability that both cards\nare aces.\n(a) you ask your friend if one of the cards is the\nace of spades, and your friend answers in the\naffirmative.\n\n(b) you ask your friend if the first card selected\nis an ace, and your friend answers in the affir-\nmative.\n\n(c) you ask your friend if\n\nthe second card\nselected is an ace, and your friend answers in\nthe affirmative.\n\n(d) you ask your friend if either of the cards\nselected is an ace, and your friend answers in\nthe affirmative.\n\n3.8. show that\n\np(h|e)\np(g|e)\n\n= p(h)\np(g)\n\np(e|h)\np(e|g)\n\nsuppose that, before new evidence is observed, the\nhypothesis h is three times as likely to be true as\nis the hypothesis g. if the new evidence is twice\nas likely when g is true than it is when h is true,\nwhich hypothesis is more likely after the evidence\nhas been observed?\n\n3.9. you ask your neighbor to water a sickly plant\nwhile you are on vacation. without water, it will\ndie with probability .8; with water, it will die with\nprobability .15. you are 90 percent certain that\nyour neighbor will remember to water the plant.\n(a) what is the probability that the plant will be\n\nalive when you return?\n\n(b) if the plant is dead upon your return, what is\nthe probability that your neighbor forgot to\nwater it?\n\n3.10. six balls are to be randomly chosen from an urn\n\ncontaining 8 red, 10 green, and 12 blue balls.\n(a) what is the probability at least one red ball is\n\nchosen?\n\n(b) given that no red balls are chosen, what is the\nconditional probability that there are exactly\n2 green balls among the 6 chosen?\n\n3.11. a type c battery is in working condition with prob-\nability .7, whereas a type d battery is in work-\ning condition with probability .4. a battery is ran-\ndomly chosen from a bin consisting of 8 type c and\n6 type d batteries.\n(a) what\n\nis the probability that\n\nthe battery\n\nworks?\n\n(b) given that the battery does not work, what is\nthe conditional probability that it was a type\nc battery?\n\n3.12. maria will take two books with her on a trip. sup-\npose that the probability that she will like book 1\nis .6, the probability that she will like book 2 is .5,\nand the probability that she will like both books\nis .4. find the conditional probability that she will\nlike book 2 given that she did not like book 1.\n\n3.13. balls are randomly removed from an urn that ini-\n\ntially contains 20 red and 10 blue balls.\n(a) what is the probability that all of the red balls\nare removed before all of the blue ones have\nbeen removed?\nnow suppose that the urn initially contains 20\nred, 10 blue, and 8 green balls.\n\n(b) now what is the probability that all of the red\nballs are removed before all of the blue ones\nhave been removed?\n\n "}, {"Page_number": 130, "text": "(c) what is the probability that the colors are\n\ndepleted in the order blue, red, green?\n\n(d) what is the probability that the group of blue\nballs is the first of the three groups to be\nremoved?\n\n3.14. a coin having probability .8 of landing on heads\nis flipped. a observes the result\u2014either heads or\ntails\u2014and rushes off to tell b. however, with prob-\nability .4, a will have forgotten the result by the\ntime he reaches b. if a has forgotten, then, rather\nthan admitting this to b, he is equally likely to tell\nb that the coin landed on heads or that it landed\ntails. (if he does remember, then he tells b the cor-\nrect result.)\n(a) what is the probability that b is told that the\n\ncoin landed on heads?\n\n(b) what is the probability that b is told the cor-\n\nrect result?\n\n(c) given that b is told that the coin landed on\nheads, what is the probability that it did in fact\nland on heads?\n\n3.15. in a certain species of rats, black dominates over\nbrown. suppose that a black rat with two black\nparents has a brown sibling.\n(a) what is the probability that this rat is a pure\nblack rat (as opposed to being a hybrid with\none black and one brown gene)?\n\n(b) suppose that when the black rat is mated with\na brown rat, all 5 of their offspring are black.\nnow what is the probability that the rat is a\npure black rat?\n\n3.16. (a) in problem 3.65b, find the probability that a\ncurrent flows from a to b, by conditioning on\nwhether relay 1 closes.\n\n3.17. for\n\nthe\n\nk-out-of-n\n\nsystem described\n\n(b) find the conditional probability that relay 3 is\nclosed given that a current flows from a to b.\nin\nproblem 3.67, assume that each component\nindependently works with probability 1\n2 . find the\nconditional probability that component 1 is work-\ning, given that the system works, when\n(a) k = 1, n = 2;\n(b) k = 2, n = 3.\n\n3.18. mr. jones has devised a gambling system for win-\nning at roulette. when he bets, he bets on red and\nplaces a bet only when the 10 previous spins of\nthe roulette have landed on a black number. he\nreasons that his chance of winning is quite large\nbecause the probability of 11 consecutive spins\nresulting in black is quite small. what do you think\nof this system?\n\n3.19. three players simultaneously toss coins. the coin\ntossed by a(b)[c] turns up heads with probability\np1(p2)[p3]. if one person gets an outcome differ-\nent from those of the other two, then he is the odd\n\nself-test problems and exercises 115\n\nman out. if there is no odd man out, the players flip\nagain and continue to do so until they get an odd\nman out. what is the probability that a will be the\nodd man?\n\nn(cid:9)\n\ni=1\n\n3.20. suppose that there are n possible outcomes of\na trial, with outcome i resulting with probability\npi = 1. if two independent tri-\npi, i = 1, . . . , n,\nals are observed, what is the probability that the\nresult of the second trial is larger than that of the\nfirst?\n3.21. if a flips n + 1 and b flips n fair coins, show that\nthe probability that a gets more heads than b is 1\n2 .\nhint: condition on which player has more heads\nafter each has flipped n coins. (there are three\npossibilities.)\n\n3.22. prove or give counterexamples to the following\n\nstatements:\n(a) if e is independent of f and e is independent\n\nof g, then e is independent of f \u222a g.\n(b) if e is independent of f, and e is independent\nof g, and fg = \u00f8, then e is independent of\nf \u222a g.\n\n(c) if e is independent of f, and f is independent\nof g, and e is independent of fg, then g is\nindependent of ef.\n\n3.23. let a and b be events having positive probabil-\nity. state whether each of the following statements\nis (i) necessarily true, (ii) necessarily false, or (iii)\npossibly true.\n(a) if a and b are mutually exclusive, then they\n\nare independent.\n\nmutually exclusive.\n\n(b) if a and b are independent, then they are\n(c) p(a) = p(b) = .6, and a and b are mutually\n(d) p(a) = p(b) = .6, and a and b are indepen-\n\nexclusive.\n\ndent.\n\n3.24. rank the following from most likely to least likely\n\nto occur:\n1. a fair coin lands on heads.\n2. three independent trials, each of which is a suc-\n\ncess with probability .8, all result in successes.\n\n3. seven independent trials, each of which is a suc-\n\ncess with probability .9, all result in successes.\n\n3.25. two local factories, a and b, produce radios. each\nradio produced at factory a is defective with prob-\nability .05, whereas each one produced at factory b\nis defective with probability .01. suppose you pur-\nchase two radios that were produced at the same\nfactory, which is equally likely to have been either\nfactory a or factory b. if the first radio that you\ncheck is defective, what is the conditional proba-\nbility that the other one is also defective?\n\n "}, {"Page_number": 131, "text": "conditional probability and independence\n\nchapter 3\n\n116\n3.26. show that if p(a|b) = 1, then p(bc|ac) = 1.\n3.27. an urn initially contains 1 red and 1 blue ball.\nat each stage, a ball is randomly withdrawn and\nreplaced by two other balls of the same color.\n(for instance, if the red ball is initially chosen,\nthen there would be 2 red and 1 blue ball in\nthe urn when the next selection occurs.) show\ninduction that the probability\nby mathematical\nthat there are exactly i red balls in the urn\nn+1 , 1 \u2026\nafter n stages have been completed is\ni \u2026 n + 1.\n\n1\n\n3.28. a total of 2n cards, of which 2 are aces, are\nto be randomly divided among two players, with\neach player receiving n cards. each player is then\nto declare, in sequence, whether he or she has\nreceived any aces. what is the conditional proba-\nbility that the second player has no aces, given that\nthe first player declares in the affirmative, when\n(a) n = 2? (b) n = 10? (c) n = 100? to what\ndoes the probability converge as n goes to infinity?\nwhy?\n\n3.29. there are n distinct\n\nn\n\n(cid:9)\n\ntypes of coupons, and\neach coupon obtained is, independently of prior\ntypes collected, of type i with probability pi,\ni=1 pi = 1.\n(a) if n coupons are collected, what is the proba-\nbility that one of each type is obtained?\n(b) now suppose that p1 = p2 = \u00b7\u00b7\u00b7 = pn = 1/n.\nlet ei be the event that there are no type\ni coupons among the n collected. apply the\ninclusion\u2013exclusion identity for the probabil-\nity of the union of events to p(\u222aiei) to prove\nthe identity\n\nn! = n(cid:6)\n\nk=0\n\n(\u22121)k\n\n(cid:3)\n(n \u2212 k)n\n\n(cid:2)\n\nn\nk\n\n3.30. show that, for any events e and f,\np(e|e \u222a f) \u00fa p(e|f)\n\nhint: compute p(e|e \u222a f) by conditioning on\nwhether f occurs.\n\n "}, {"Page_number": 132, "text": "c h a p t e r\n\n4\n\nrandom variables\n\n4.1 random variables\n4.2 discrete random variables\n4.3 expected value\n4.4 expectation of a function of a random variable\n4.5 variance\n4.6 the bernoulli and binomial random variables\n4.7 the poisson random variable\n4.8 other discrete probability distributions\n4.9 expected value of sums of random variables\n4.10 properties of the cumulative distribution function\n\n4.1 random variables\n\nfrequently, when an experiment is performed, we are interested mainly in some func-\ntion of the outcome as opposed to the actual outcome itself. for instance, in tossing\ndice, we are often interested in the sum of the two dice and are not really concerned\nabout the separate values of each die. that is, we may be interested in knowing\nthat the sum is 7 and may not be concerned over whether the actual outcome was\n(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), or (6, 1). also, in flipping a coin, we may be inter-\nested in the total number of heads that occur and not care at all about the actual\nhead\u2013tail sequence that results. these quantities of interest, or, more formally, these\nreal-valued functions defined on the sample space, are known as random variables.\n\nbecause the value of a random variable is determined by the outcome of the exper-\n\niment, we may assign probabilities to the possible values of the random variable.\n\nexample 1a\nsuppose that our experiment consists of tossing 3 fair coins. if we let y denote the\nnumber of heads that appear, then y is a random variable taking on one of the values\n0, 1, 2, and 3 with respective probabilities\n\np{y = 0} = p{(t, t, t)} = 1\n8\np{y = 1} = p{(t, t, h), (t, h, t), (h, t, t)} = 3\n8\np{y = 2} = p{(t, h, h), (h, t, h), (h, h, t)} = 3\n8\np{y = 3} = p{(h, h, h)} = 1\n8\n\n117\n\n "}, {"Page_number": 133, "text": "118\n\nchapter 4\n\nrandom variables\n\nsince y must take on one of the values 0 through 3, we must have\n\n\u239b\n\u239d 3(cid:14)\n\n\u239e\n\u23a0 = 3(cid:6)\n{y = i}\n\n1 = p\n\ni=0\n\np{y = i}\n\ni=0\n\n(cid:2)\n\n(cid:3)\n\ni \u2212 1\n(cid:2)\n(cid:3)\n2\n20\n3\n\nwhich, of course, is in accord with the preceding probabilities.\n\n.\n\nexample 1b\nthree balls are to be randomly selected without replacement from an urn contain-\ning 20 balls numbered 1 through 20. if we bet that at least one of the balls that are\ndrawn has a number as large as or larger than 17, what is the probability that we\nwin the bet?\n\nsolution. let x denote the largest number selected. then x is a random variable\ntaking on one of the values 3, 4, . . . , 20. furthermore, if we suppose that each of the\n\npossible selections are equally likely to occur, then\n\n(cid:2)\n\n(cid:3)\n\n20\n3\n\np{x = i} =\n\ni = 3, . . . , 20\n\n(1.1)\n\n(cid:3)\nequation (1.1) follows because the number of selections that result in the event\n{x = i} is just the number of selections that result in the ball numbered i and two\n(cid:2)\nof the balls numbered 1 through i \u2212 1 being chosen. because there are clearly\n1\n1\ni \u2212 1\n2\n\nsuch selections, we obtain the probabilities expressed in equation (1.1),\n\n(cid:2)\n\n(cid:3)\n\nfrom which we see that\n\n20\n\n(cid:3)\n(cid:3) = 3\n(cid:3)\n(cid:3) = 51\n(cid:3)\n(cid:3) = 34\n(cid:3)\n(cid:3) = 2\n\n19\n\n380\n\n285\n\n(cid:2)\n(cid:2)\n(cid:2)\n(cid:2)\n(cid:2)\n(cid:2)\n(cid:2)\n(cid:2)\n\n19\n2\n20\n3\n\n18\n2\n20\n3\n\n17\n2\n20\n3\n\n16\n2\n20\n3\n\n= .150\n\nl .134\n\nl .119\n\nl .105\n\np{x = 20} =\n\np{x = 19} =\n\np{x = 18} =\n\np{x = 17} =\n\n "}, {"Page_number": 134, "text": "random variables 119\nhence, since the event {x \u00fa 17} is the union of the disjoint events {x = i},\n\ni = 17, 18, 19, 20, it follows that the probability of our winning the bet is given by\n\nsection 4.1\n\np{x \u00fa 17} l .105 + .119 + .134 + .150 = .508\n\n.\n\nexample 1c\n\nindependent trials consisting of the flipping of a coin having probability p of coming\nup heads are continually performed until either a head occurs or a total of n flips is\nmade. if we let x denote the number of times the coin is flipped, then x is a random\nvariable taking on one of the values 1, 2, 3, . . . , n with respective probabilities\n\np{x = 1} = p{h} = p\np{x = 2} = p{(t, h)} = (1 \u2212 p)p\np{x = 3} = p{(t, t, h)} = (1 \u2212 p)2p\n\nas a check, note that\n\n#\n#\n#\n\nn\u22122\n\n(cid:23)(cid:24)\n(cid:23)(cid:24)\n\n(cid:25)\np{x = n \u2212 1} = p{(t, t, . . . , t,\n(cid:25)\np{x = n} = p{(t, t, . . . , t,\n\u239b\n\u239d n(cid:14)\n\nn\u22121\n\n{x = i}\n\np\n\ni=1\n\n(cid:22)\n(cid:22)\n\u239e\n\u23a0 = n(cid:6)\n= n\u22121(cid:6)\n(cid:7)\n\ni=1\n= p\n\ni=1\n\np{x = i}\n\nh)} = (1 \u2212 p)n\u22122p\n(cid:25)\n\nt), (t, t, . . . , t,\n\n(cid:23)(cid:24)\n\n(cid:22)\n\nn\u22121\n\nh)} = (1 \u2212 p)n\u22121\n\np(1 \u2212 p)i\u22121 + (1 \u2212 p)n\u22121\n\n(cid:8)\n\n1 \u2212 (1 \u2212 p)n\u22121\n1 \u2212 (1 \u2212 p)\n\n+ (1 \u2212 p)n\u22121\n\n= 1 \u2212 (1 \u2212 p)n\u22121 + (1 \u2212 p)n\u22121\n= 1\n\n.\n\nexample 1d\n\nthree balls are randomly chosen from an urn containing 3 white, 3 red, and 5 black\nballs. suppose that we win $1 for each white ball selected and lose $1 for each red ball\n\n "}, {"Page_number": 135, "text": "120\n\nchapter 4\n\nrandom variables\n\nselected. if we let x denote our total winnings from the experiment, then x is a ran-\ndom variable taking on the possible values 0, ;1, ;2, ;3 with respective probabilities\n\n(cid:3)(cid:2)\n\n(cid:3)\n\n3\n1\n\n= 39\n165\n\np{x = 0} =\n\n(cid:2)\n\n(cid:3)\n\n5\n3\n\n+\n\n(cid:2)\n(cid:2)\n\n(cid:3)(cid:2)\n(cid:3)\n(cid:2)\n\n3\n1\n11\n3\n\np{x = 1} = p{x = \u22121} =\n\np{x = 2} = p{x = \u22122} =\n\np{x = 3} = p{x = \u22123} =\n\n(cid:2)\n\n(cid:2)\n(cid:2)\n\n(cid:3)(cid:2)\n\n(cid:3)\n\n(cid:3)(cid:2)\n\n(cid:3)(cid:2)\n\n5\n1\n\n3\n2\n\n5\n2\n\n+\n\n= 55\n165\n(cid:2)\n(cid:3)\n\n(cid:3)\n(cid:2)\n(cid:3)\n(cid:3) = 15\n\n11\n3\n\n5\n1\n\n165\n\n11\n3\n\n(cid:3)\n(cid:3) = 1\n\n165\n\n3\n1\n\n3\n1\n\n3\n2\n\n(cid:2)\n\n3\n3\n11\n3\n\nthese probabilities are obtained, for instance, by noting that in order for x to\nequal 0, either all 3 balls selected must be black or 1 ball of each color must be\nselected. similarly, the event {x = 1} occurs either if 1 white and 2 black balls are\nselected or if 2 white and 1 red is selected. as a check, we note that\n\n3(cid:6)\n\ni=0\n\np{x = i} + 3(cid:6)\n\ni=1\n\np{x = \u2212i} = 55 + 39 + 15 + 1 + 39 + 15 + 1\n\n165\n\n= 1\n\nthe probability that we win money is given by\n\n3(cid:6)\n\ni=1\n\np{x = i} = 55\n165\n\n= 1\n3\n\n.\n\nexample 1e\nsuppose that there are n distinct types of coupons and that each time one obtains a\ncoupon, it is, independently of previous selections, equally likely to be any one of the\nn types. one random variable of interest is t, the number of coupons that needs to\nbe collected until one obtains a complete set of at least one of each type. rather than\nderive p{t = n} directly, let us start by considering the probability that t is greater\nthan n. to do so, fix n and define the events a1, a2, . . . , an as follows: aj is the event\n\n "}, {"Page_number": 136, "text": "random variables 121\nthat no type j coupon is contained among the first n coupons collected, j = 1, . . . , n.\nhence,\n\nsection 4.1\n\n\u239b\n\u239c\u239d n(cid:14)\np{t > n} = p\n(cid:6)\n\n=\n\naj\n\n\u239e\n\u239f\u23a0\nj=1\np(aj) \u2212\n+ (\u22121)k+1\n+ (\u22121)n+1p(a1a2 \u00b7\u00b7\u00b7 an)\n\n(cid:6)(cid:6)\n(cid:6)(cid:6)(cid:6)\n\nj1<j2<\u00b7\u00b7\u00b7<jk\n\nj1<j2\n\nj\n\n) + \u00b7\u00b7\u00b7\n\np(aj1aj2\n\np(aj1aj2\n\n\u00b7\u00b7\u00b7 ajk\n\n)\u00b7\u00b7\u00b7\n\nnow, aj will occur if each of the n coupons collected is not of type j. since each of the\ncoupons will not be of type j with probability (n \u2212 1)/n, we have, by the assumed\nindependence of the types of successive coupons,\nn \u2212 1\n\n(cid:3)n\n\n(cid:2)\n\np(aj) =\n\nalso, the event aj1aj2 will occur if none of the first n coupons collected is of either\ntype j1 or type j2. thus, again using independence, we see that\n\n(cid:3)n\n\nn\n\n(cid:2)\n\nn\n\nn \u2212 2\n(cid:2)\n\nn \u2212 k\n\n(cid:3)n\n(cid:3)n +\n(cid:2)\n\nn\n\n) =\n\np(aj1aj2\n\nthe same reasoning gives\n\np(aj1aj2\n\n(cid:2)\n\n(cid:2)\n\np{t > n} = n\n\n(cid:3)n \u2212\nand we see that, for n > 0,\nn \u2212 1\n(cid:2)\nn\n+ (\u22121)n\n(cid:3)(cid:2)\n(cid:2)\nn \u2212 1\nn \u2212 i\nn\n\n= n\u22121(cid:6)\n\nn\ni\n\nn\n\ni=1\n\n\u00b7\u00b7\u00b7 ajk\n) =\n(cid:3)(cid:2)\nn \u2212 2\n(cid:3)n\n(cid:3)(cid:2)\n(cid:3)n\n\n1\nn\n(\u22121)i+1\n\nn\n2\n\nn\n\n(cid:3)(cid:2)\n\nn\n3\n\nn \u2212 3\n\nn\n\n(cid:3)n \u2212 \u00b7\u00b7\u00b7\n\n(1.2)\n\nthe probability that t equals n can now be obtained from the preceding formula by\nthe use of\n\np{t > n \u2212 1} = p{t = n} + p{t > n}\n\nor, equivalently,\n\np{t = n} = p{t > n \u2212 1} \u2212 p{t > n}\n\nanother random variable of interest is the number of distinct types of coupons\nthat are contained in the first n selections\u2014call this random variable dn. to compute\n\n "}, {"Page_number": 137, "text": "122\n\nchapter 4\n\nrandom variables\np{dn = k}, let us start by fixing attention on a particular set of k distinct types,\nand let us then determine the probability that this set constitutes the set of distinct\ntypes obtained in the first n selections. now, in order for this to be the situation, it is\nnecessary and sufficient that, of the first n coupons obtained,\n\na:\nb:\n\neach is one of these k types.\neach of these k types is represented.\n\nnow, each coupon selected will be one of the k types with probability k/n, so the\nprobability that a will be valid is (k/n)n. also, given that a coupon is of one of the k\ntypes under consideration, it is easy to see that it is equally likely to be of any one of\nthese k types. hence, the conditional probability of b given that a occurs is the same\nas the probability that a set of n coupons, each equally likely to be any of k possible\ntypes, contains a complete set of all k types. but this is just the probability that the\nnumber needed to amass a complete set, when choosing among k types, is less than\nor equal to n and is thus obtainable from equation (1.2) with k replacing n. thus,\nwe have\n\nk\nn\n\n(cid:2)\n\np(a) =\n\n(cid:3)n\np(b|a) = 1 \u2212 k\u22121(cid:6)\n(cid:3)\n(cid:2)\n(cid:2)\n(cid:2)\n\n(cid:3)\n(cid:3)(cid:2)\n\np(ab)\n\ni=1\n\nn\nk\n\n(cid:3)n\n\n=\n\nn\nk\n\nk\nn\n\nn\nk\np{dn = k} =\n\n(cid:3)(cid:2)\n\n(cid:2)\n\nk\ni\n\nk \u2212 i\nk\n\n(cid:3)n\n\n(\u22121)i+1\n\n\u23a1\n\u23a31 \u2212 k\u22121(cid:6)\n\ni=1\n\n(cid:3)(cid:2)\n\n(cid:2)\n\nk\ni\n\nk \u2212 i\nk\n\n(cid:3)n\n\n\u23a4\n\u23a6\n\n(\u22121)i+1\n\nfinally, as there are\n\npossible choices for the set of k types, we arrive at\n\nremark. since one must collect at least n coupons to obtain a compete set, it\nfollows that p{t > n} = 1 if n < n. therefore, from equation (1.2), we obtain the\ninteresting combinatorial identity that, for integers 1 \u2026 n < n,\n\n(cid:3)(cid:2)\n\n(cid:3)(cid:2)\n\n(cid:3)n\n\nn \u2212 i\nn\n\nn \u2212 i\nn\n\n(\u22121)i+1 = 1\n\n(cid:3)n\n\n(\u22121)i+1 = 0\n\nwhich can be written as\n\n(cid:2)\n\ni=1\n\nn\u22121(cid:6)\n\ni=0\n\n(cid:2)\n\nn\u22121(cid:6)\n\nn\ni\n\nn\ni\n\n(cid:3)\n\n(cid:2)\n\nn(cid:6)\n\nj=1\n\nor, upon multiplying by (\u22121)nnn and letting j = n \u2212 i,\n\nn\nj\n\njn(\u22121)j\u22121 = 0\n\n1 \u2026 n < n\n\n.\n\n "}, {"Page_number": 138, "text": "section 4.2\n\ndiscrete random variables 123\n\nfor a random variable x, the function f defined by\n\nf(x) = p{x \u2026 x}\n\n\u2212 q < x < q\n\nis called the cumulative distribution function, or, more simply, the distribution func-\ntion, of x. thus, the distribution function specifies, for all real values x, the probability\nthat the random variable is less than or equal to x.\nnow, suppose that a \u2026 b. then, because the event {x \u2026 a} is contained in the\nevent {x \u2026 b}, it follows that f(a), the probability of the former, is less than or\nequal to f(b), the probability of the latter. in other words, f(x) is a nondecreas-\ning function of x. other general properties of the distribution function are given in\nsection 4.10.\n\n4.2 discrete random variables\n\na random variable that can take on at most a countable number of possible values is\nsaid to be discrete. for a discrete random variable x, we define the probability mass\nfunction p(a) of x by\n\np(a) = p{x = a}\n\nthe probability mass function p(a) is positive for at most a countable number of val-\nues of a. that is, if x must assume one of the values x1, x2, . . . , then\n\np(xi) \u00fa 0\np(x) = 0\n\nfor i = 1, 2, . . .\nfor all other values of x\n\nsince x must take on one of the values xi, we have\np(xi) = 1\n\nq(cid:6)\n\ni=1\n\nit is often instructive to present the probability mass function in a graphical format\nby plotting p(xi) on the y-axis against xi on the x-axis. for instance, if the probability\nmass function of x is\n\np(0) = 1\n4\n\np(1) = 1\n2\n\np(2) = 1\n4\n\np(x)\n\n1\n\n1\u2013\n2\n\n1\u2013\n4\n\n0\n\n1\n\n2\n\nx\n\nfigure 4.1\n\n "}, {"Page_number": 139, "text": "124\n\nchapter 4\n\nrandom variables\n\np(x)\n\n6\u2014\n36\n\n5\u2014\n36\n\n4\u2014\n36\n\n3\u2014\n36\n\n2\u2014\n36\n\n1\u2014\n36\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\nx\n\nfigure 4.2\n\nwe can represent this function graphically as shown in figure 4.1. similarly, a graph\nof the probability mass function of the random variable representing the sum when\ntwo dice are rolled looks like figure 4.2.\n\nexample 2a\nthe probability mass function of a random variable x is given by p(i) = c\u03bbi/i!,\ni = 0, 1, 2, . . . , where \u03bb is some positive value. find (a) p{x = 0} and (b) p{x > 2}.\n\nsolution. since\n\np(i) = 1, we have\n\nq(cid:9)\n\ni=0\n\nwhich, because e x = q(cid:9)\n\ni=0\n\nq(cid:6)\n\ni=0\n\n= 1\n\n\u03bbi\ni!\n\nc\n\nxi/i!, implies that\nce\u03bb = 1 or\n\nc = e\n\n\u2212\u03bb\n\nhence,\n(a) p{x = 0} = e\n(b) p{x > 2} = 1 \u2212 p{x \u2026 2} = 1 \u2212 p{x = 0} \u2212 p{x = 1}\n\n\u2212\u03bb\n\n\u2212\u03bb\u03bb0/0! = e\n\u2212 p{x = 2}\n\u2212\u03bb \u2212 \u03bbe\n\n= 1 \u2212 e\n\n.\n\nthe cumulative distribution function f can be expressed in terms of p(a) by\n\n\u2212\u03bb\n\n\u2212\u03bb \u2212 \u03bb2e\n2\n(cid:6)\n\nf(a) =\n\np(x)\n\nall x \u2026 a\n\nif x is a discrete random variable whose possible values are x1, x2, x3, . . . , where\nx1 < x2 < x3 < \u00b7\u00b7\u00b7 , then the distribution function f of x is a step function. that is,\n\n "}, {"Page_number": 140, "text": "the value of f is constant in the intervals [xi\u22121, xi) and then takes a step (or jump) of\nsize p(xi) at xi. for instance, if x has a probability mass function given by\n\nsection 4.3\n\nexpected value 125\n\np(1) = 1\n4\n\np(2) = 1\n2\n\np(3) = 1\n8\n\np(4) = 1\n8\n\nthen its cumulative distribution function is\n\n\u23a7\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9\n\n3\n\n0 a < 1\n4 1 \u2026 a < 2\n1\n4 2 \u2026 a < 3\n8 3 \u2026 a < 4\n7\n1 4 \u2026 a\n\nf(a) =\n\nthis function is depicted graphically in figure 4.3.\n\nf(a)\n\n1\n7\u2013\n8\n3\u2013\n4\n\n1\u2013\n4\n\n1\n\n2\n\n3\n\n4\n\na\n\nfigure 4.3\n\nnote that the size of the step at any of the values 1, 2, 3, and 4 is equal to the\n\nprobability that x assumes that particular value.\n\n4.3 expected value\n\none of the most important concepts in probability theory is that of the expectation\nof a random variable. if x is a discrete random variable having a probability mass\nfunction p(x), then the expectation, or the expected value, of x, denoted by e[x], is\ndefined by\n\n(cid:6)\n\ne[x] =\n\nxp(x)\n\nx:p(x)>0\n\nin words, the expected value of x is a weighted average of the possible values that\nx can take on, each value being weighted by the probability that x assumes it. for\ninstance, on the one hand, if the probability mass function of x is given by\n\nthen\n\np(0) = 1\n(cid:3)\n(cid:2)\n2\n\n= p(1)\n(cid:2)\n\ne[x] = 0\n\n+ 1\n\n1\n2\n\n(cid:3)\n\n= 1\n2\n\n1\n2\n\n "}, {"Page_number": 141, "text": "126\n\nchapter 4\n\nrandom variables\n\nis just the ordinary average of the two possible values, 0 and 1, that x can assume.\non the other hand, if\n\nthen\n\np(0) = 1\n3\n(cid:2)\n\n(cid:3)\n\ne[x] = 0\n\n1\n3\n\np(1) = 2\n3\n(cid:3)\n(cid:2)\n\n+ 1\n\n2\n3\n\n= 2\n3\n\nis a weighted average of the two possible values 0 and 1, where the value 1 is given\ntwice as much weight as the value 0, since p(1) = 2p(0).\n\nanother motivation of the definition of expectation is provided by the frequency\ninterpretation of probabilities. this interpretation (partially justified by the strong\nlaw of large numbers, to be presented in chapter 8) assumes that if an infinite sequence\nof independent replications of an experiment is performed, then, for any event e,\nthe proportion of time that e occurs will be p(e). now, consider a random vari-\nable x that must take on one of the values x1, x2, . . . xn with respective probabilities\np(x1), p(x2), . . . , p(xn), and think of x as representing our winnings in a single game of\nchance. that is, with probability p(xi) we shall win xi units i = 1, 2, . . . , n. by the fre-\nquency interpretation, if we play this game continually, then the proportion of time\nthat we win xi will be p(xi). since this is true for all i, i = 1, 2, . . . , n, it follows that our\naverage winnings per game will be\n\nn(cid:6)\n\ni=1\n\nxip(xi) = e[x]\n\nexample 3a\nfind e[x], where x is the outcome when we roll a fair die.\nsolution. since p(1) = p(2) = p(3) = p(4) = p(5) = p(6) = 1\n(cid:2)\n(cid:3)\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n6, we obtain\n\n(cid:2)\n\n(cid:3)\n\ne[x] = 1\n\n1\n6\n\n+ 2\n\n1\n6\n\n+ 3\n\n1\n6\n\n+ 4\n\n1\n6\n\n+ 5\n\n1\n6\n\n+ 6\n\n1\n6\n\n= 7\n2\n\n.\n\nexample 3b\nwe say that i is an indicator variable for the event a if\n\n%\n\ni =\n\n1 if a occurs\n0 if ac occurs\n\nfind e[i].\nsolution. since p(1) = p(a), p(0) = 1 \u2212 p(a), we have\n\ne[i] = p(a)\n\nthat is, the expected value of the indicator variable for the event a is equal to the\n.\nprobability that a occurs.\n\n "}, {"Page_number": 142, "text": "section 4.3\n\nexpected value 127\n\nexample 3c\na contestant on a quiz show is presented with two questions, questions 1 and 2, which\nhe is to attempt to answer in some order he chooses. if he decides to try question i\nfirst, then he will be allowed to go on to question j, j z i, only if his answer to question\ni is correct. if his initial answer is incorrect, he is not allowed to answer the other ques-\ntion. the contestant is to receive vi dollars if he answers question i correctly, i = 1, 2.\nfor instance, he will receive v1 + v2 dollars if he answers both questions correctly.\nif the probability that he knows the answer to question i is pi, i = 1, 2, which question\nshould he attempt to answer first so as to maximize his expected winnings? assume\nthat the events ei, i = 1, 2, that he knows the answer to question i are independent\nevents.\n\nsolution. on the one hand, if he attempts to answer question 1 first, then he will win\n\nwith probability 1 \u2212 p1\nwith probability p1(1 \u2212 p2)\n\n0\nv1\nv1 + v2 with probability p1p2\n\nhence, his expected winnings in this case will be\n\nv1p1(1 \u2212 p2) + (v1 + v2)p1p2\n\non the other hand, if he attempts to answer question 2 first, his expected winnings\nwill be\n\nv2p2(1 \u2212 p1) + (v1 + v2)p1p2\n\ntherefore, it is better to try question 1 first if\n\nv1p1(1 \u2212 p2) \u00fa v2p2(1 \u2212 p1)\n\nor, equivalently, if\n\nv1p1\n1 \u2212 p1\n\n\u00fa v2p2\n1 \u2212 p2\n\nfor example, if he is 60 percent certain of answering question 1, worth $200, correctly\nand he is 80 percent certain of answering question 2, worth $100, correctly, then he\nshould attempt to answer question 2 first because\n\n400 = (100)(.8)\n\n.2\n\n>\n\n(200)(.6)\n\n.4\n\n= 300\n\n.\n\nexample 3d\na school class of 120 students is driven in 3 buses to a symphonic performance. there\nare 36 students in one of the buses, 40 in another, and 44 in the third bus. when the\nbuses arrive, one of the 120 students is randomly chosen. let x denote the number\nof students on the bus of that randomly chosen student, and find e[x].\n\nsolution. since the randomly chosen student is equally likely to be any of the 120\nstudents, it follows that\n\nhence,\n\np{x = 36} = 36\n(cid:3)\n120\n\n(cid:2)\n\np{x = 40} = 40\n(cid:2)\n(cid:2)\n120\n\n(cid:3)\n\ne[x] = 36\n\n3\n10\n\n+ 40\n\n1\n3\n\n+ 44\n\n11\n30\n\n(cid:3)\n\np{x = 44} = 44\n120\n\n= 1208\n30\n\n= 40.2667\n\n "}, {"Page_number": 143, "text": "128\n\nchapter 4\n\nrandom variables\nhowever, the average number of students on a bus is 120/3 = 40, showing that the\nexpected number of students on the bus of a randomly chosen student is larger than\nthe average number of students on a bus. this is a general phenomenon, and it occurs\nbecause the more students there are on a bus, the more likely it is that a randomly\nchosen student would have been on that bus. as a result, buses with many students\nare given more weight than those with fewer students. (see self-test problem 4.) .\nremark. the probability concept of expectation is analogous to the physical con-\ncept of the center of gravity of a distribution of mass. consider a discrete random\nvariable x having probability mass function p(xi), i \u00fa 1. if we now imagine a weight-\nless rod in which weights with mass p(xi), i \u00fa 1, are located at the points xi, i \u00fa 1\n(see figure 4.4), then the point at which the rod would be in balance is known as the\ncenter of gravity. for those readers acquainted with elementary statics, it is now a\n.\nsimple matter to show that this point is at e[x].\u2020\n\n\u20131\n\n0\n\n^ 1\n\n2\n\np(\u20131) = .10, p(0) = .25,\n\np(1) = .30,\n\np(2) = .35\n\n^ = center of gravity = .9\n\nfigure 4.4\n\n4.4 expectation of a function of a random variable\n\nsuppose that we are given a discrete random variable along with its probability mass\nfunction and that we want to compute the expected value of some function of x,\nsay, g(x). how can we accomplish this? one way is as follows: since g(x) is itself a\ndiscrete random variable, it has a probability mass function, which can be determined\nfrom the probability mass function of x. once we have determined the probability\nmass function of g(x), we can compute e[g(x)] by using the definition of expected\nvalue.\n\nexample 4a\nlet x denote a random variable that takes on any of the values \u22121, 0, and 1 with\nrespective probabilities\n\np{x = \u22121} = .2 p{x = 0} = .5 p{x = 1} = .3\n\ncompute e[x2].\nsolution. let y = x2. then the probability mass function of y is given by\n\np{y = 1} = p{x = \u22121} + p{x = 1} = .5\np{y = 0} = p{x = 0} = .5\n\nhence,\n\ne[x2] = e[y] = 1(.5) + 0(.5) = .5\n\nis equal to 0. that is, we must show that 0 =(cid:9)\n\n\u2020to prove this, we must show that the sum of the torques tending to turn the point around e[x]\n\n(xi \u2212 e[x])p(xi), which is immediate.\n\ni\n\n "}, {"Page_number": 144, "text": "section 4.4\n\nexpectation of a function of a random variable 129\n\nnote that\n\n.5 = e[x2] z (e[x])2 = .01\n\n.\n\nalthough the preceding procedure will always enable us to compute the expected\nvalue of any function of x from a knowledge of the probability mass function of x,\nthere is another way of thinking about e[g(x)]: since g(x) will equal g(x) whenever\nx is equal to x, it seems reasonable that e[g(x)] should just be a weighted average of\nthe values g(x), with g(x) being weighted by the probability that x is equal to x. that\nis, the following result is quite intuitive:\nproposition 4.1.\nif x is a discrete random variable that takes on one of the values xi, i \u00fa 1, with\nrespective probabilities p(xi), then, for any real-valued function g,\n\n(cid:6)\n\ne[g(x)] =\n\ng(xi)p(xi)\n\ni\n\nbefore proving this proposition, let us check that it is in accord with the results of\n\nexample 4a. applying it to that example yields\n\ne{x2} = (\u22121)2(.2) + 02(.5) + 12(.3)\n\n= 1(.2 + .3) + 0(.5)\n= .5\n\nwhich is in agreement with the result given in example 4a.\n\nproof of proposition 4.1: the proof of proposition 4.1 proceeds, as in the preceding\ng(xi)p(xi) having the same value\nverification, by grouping together all the terms in\nof g(xi). specifically, suppose that yj, j \u00fa 1, represent the different values of g(xi), i \u00fa 1.\nthen, grouping all the g(xi) having the same value gives\n\ni\n\n(cid:9)\n\n(cid:6)\n\ni\n\ng(xi)p(xi) =\n=\n\n=\n\ng(xi)p(xi)\n\nj\n\n(cid:6)\n(cid:6)\n(cid:6)\n(cid:6)\n\nj\n\nj\n\n(cid:6)\n(cid:6)\n(cid:6)\n\ni:g(xi)=yj\n\nyjp(xi)\n\ni:g(xi)=yj\nyj\np(xi)\nyjp{g(x) = yj}\n\ni:g(xi)=yj\n\n=\n= e[g(x)]\n\nj\n\n(cid:2)\n\nexample 4b\na product that is sold seasonally yields a net profit of b dollars for each unit sold and\na net loss of (cid:7) dollars for each unit left unsold when the season ends. the number of\nunits of the product that are ordered at a specific department store during any season\nis a random variable having probability mass function p(i), i \u00fa 0. if the store must\nstock this product in advance, determine the number of units the store should stock\nso as to maximize its expected profit.\n\n "}, {"Page_number": 145, "text": "130\n\nchapter 4\n\nrandom variables\n\nsolution. let x denote the number of units ordered. if s units are stocked, then the\nprofit\u2014call it p(s)\u2014can be expressed as\n\np(s) = bx \u2212 (s \u2212 x)(cid:2)\n\n= sb\nhence, the expected profit equals\n\ne[p(s)] = s(cid:2)\n\ni=0\n\n[bi \u2212 (s \u2212 i)(cid:2)]p(i) +\ns(cid:2)\n\nip(i) \u2212 s(cid:2)\n\n= (b + (cid:2))\n\nif x \u2026 s\nif x > s\n\nq(cid:2)\n\n\u23a1\n\u23a31 \u2212 s(cid:2)\n\n\u23a4\n\u23a6\n\np(i)\n\ni=0\n\nsbp(i)\n\ni=s+1\np(i) + sb\ns(cid:2)\n\np(i) + sb\n\ni=0\n\ns(cid:2)\ns(cid:2)\n\ni=0\n\ni=0\n\n= (b + (cid:2))\n\n= sb + (b + (cid:2))\n\nip(i) \u2212 (b + (cid:2))s\ns(cid:2)\n\n(i \u2212 s)p(i)\n\ni=0\n\ni=0\n\nto determine the optimum value of s, let us investigate what happens to the profit\nwhen we increase s by 1 unit. by substitution, we see that the expected profit in this\ncase is given by\n\ne[p(s + 1)] = b(s + 1) + (b + (cid:2))\n\n= b(s + 1) + (b + (cid:2))\n\ns+1(cid:2)\n(i \u2212 s \u2212 1)p(i)\ns(cid:2)\n(i \u2212 s \u2212 1)p(i)\n\ni=0\n\ni=0\n\ns(cid:2)\n\ni=0\n\np(i)\n\ntherefore,\n\ne[p(s + 1)] \u2212 e[p(s)] = b \u2212 (b + (cid:2))\n\nthus, stocking s + 1 units will be better than stocking s units whenever\n\ns(cid:2)\n\np(i) <\n\ni=0\n\nb\n\nb + (cid:2)\n\n(4.1)\n\nbecause the left-hand side of equation (4.1) is increasing in s while the right-hand\n\u2217\nside is constant, the inequality will be satisfied for all values of s \u2026 s\nis the\nlargest value of s satisfying equation (4.1). since\n\n\u2217\n, where s\n\ne[p(0)] < \u00b7\u00b7\u00b7 < e[p(s\n\n\u2217)] < e[p(s\n\n\u2217 + 2)] > \u00b7\u00b7\u00b7\n\u2217 + 1 items will lead to a maximum expected profit.\n\n\u2217 + 1)] > e[p(s\n\nit follows that stocking s\n\n.\n\nexample 4c utility\nsuppose that you must choose one of two possible actions, each of which can result\nin any of n consequences, denoted as c1, . . . , cn. suppose that if the first action is\n\n "}, {"Page_number": 146, "text": "i=1\n\ni=1\n\nn(cid:9)\n\nsection 4.4\n\npi = n(cid:9)\n\nexpectation of a function of a random variable 131\nchosen, then consequence ci will result with probability pi, i = 1, . . . , n, whereas\nif the second action is chosen, then consequence ci will result with probability qi,\ni = 1, . . . , n, where\nqi = 1. the following approach can be used to deter-\nmine which action to choose: start by assigning numerical values to the different\nconsequences in the following manner: first, identify the least and the most desir-\nable consequences\u2014call them c and c, respectively; give consequence c the value 0\nand give c the value 1. now consider any of the other n \u2212 2 consequences, say, ci. to\nvalue this consequence, imagine that you are given the choice between either receiv-\ning ci or taking part in a random experiment that either earns you consequence c\nwith probability u or consequence c with probability 1 \u2212 u. clearly, your choice will\ndepend on the value of u. on the one hand, if u = 1, then the experiment is certain\nto result in consequence c, and since c is the most desirable consequence, you will\nprefer participating in the experiment to receiving ci. on the other hand, if u = 0,\nthen the experiment will result in the least desirable consequence\u2014namely, c\u2014so in\nthis case you will prefer the consequence ci to participating in the experiment. now,\nas u decreases from 1 to 0, it seems reasonable that your choice will at some point\nswitch from participating in the experiment to the certain return of ci, and at that\ncritical switch point you will be indifferent between the two alternatives. take that\nindifference probability u as the value of the consequence ci. in other words, the\nvalue of ci is that probability u such that you are indifferent between either receiv-\ning the consequence ci or taking part in an experiment that returns consequence c\nwith probability u or consequence c with probability 1 \u2212 u. we call this indifference\nprobability the utility of the consequence ci, and we designate it as u(ci).\nto determine which action is superior, we need to evaluate each one. consider the\nfirst action, which results in consequence ci with probability pi, i = 1, . . . , n. we can\nthink of the result of this action as being determined by a two-stage experiment. in the\nfirst stage, one of the values 1, . . . , n is chosen according to the probabilities p1, . . . , pn;\nif value i is chosen, you receive consequence ci. however, since ci is equivalent to\nobtaining consequence c with probability u(ci) or consequence c with probability\n1 \u2212 u(ci), it follows that the result of the two-stage experiment is equivalent to an\nexperiment in which either consequence c or consequence c is obtained, with c being\nobtained with probability\n\nn(cid:6)\n\npiu(ci)\n\ni=1\n\nsimilarly, the result of choosing the second action is equivalent to taking part in an\nexperiment in which either consequence c or consequence c is obtained, with c being\nobtained with probability\n\nn(cid:6)\n\nqiu(ci)\n\ni=1\n\nsince c is preferable to c, it follows that the first action is preferable to the second\n\naction if\n\nn(cid:6)\n\nn(cid:6)\n\npiu(ci) >\n\nqiu(ci)\n\ni=1\n\ni=1\n\nin other words, the worth of an action can be measured by the expected value of the\nutility of its consequence, and the action with the largest expected utility is the most\n.\npreferable.\n\n "}, {"Page_number": 147, "text": "132\n\nchapter 4\n\nrandom variables\n\na simple logical consequence of proposition 4.1 is corollary 4.1.\n\ncorollary 4.1.\n\nif a and b are constants, then\n\ne[ax + b] = ae[x] + b\n\nproof.\n\n(cid:6)\n(cid:6)\n\ne[ax + b] =\n= a\n= ae[x] + b\n\nx:p(x)>0\n\nx:p(x)>0\n\n(ax + b)p(x)\nxp(x) + b\n\n(cid:6)\n\np(x)\n\nx:p(x)>0\n\nthe expected value of a random variable x, e[x], is also referred to as the mean\nor the first moment of x. the quantity e[xn], n \u00fa 1, is called the nth moment of x.\nby proposition 4.1, we note that\n\n(cid:6)\n\ne[xn] =\n\nxnp(x)\n\n4.5 variance\n\nx:p(x)>0\n\ngiven a random variable x along with its distribution function f, it would be\nextremely useful if we were able to summarize the essential properties of f by cer-\ntain suitably defined measures. one such measure would be e[x], the expected value\nof x. however, although e[x] yields the weighted average of the possible values of\nx, it does not tell us anything about the variation, or spread, of these values. for\ninstance, although random variables w, y, and z having probability mass functions\ndetermined by\n\n\u23a7\u23a8\nw = 0 with probability 1\n\u23a9\u22121 with probability 1\ny =\n\u23a7\u23a8\n+1 with probability 1\n\u23a9\u2212100 with probability 1\n+100 with probability 1\n\nz =\n\n2\n\n2\n\n2\n\n2\n\nall have the same expectation\u2014namely, 0\u2014there is a much greater spread in the pos-\nsible values of y than in those of w (which is a constant) and in the possible values\nof z than in those of y.\n\nbecause we expect x to take on values around its mean e[x], it would appear that\na reasonable way of measuring the possible variation of x would be to look at how\nfar apart x would be from its mean, on the average. one possible way to measure this\nvariation would be to consider the quantity e[|x \u2212 \u03bc|], where \u03bc = e[x]. however,\nit turns out to be mathematically inconvenient to deal with this quantity, so a more\ntractable quantity is usually considered\u2014namely, the expectation of the square of the\ndifference between x and its mean. we thus have the following definition.\n\n "}, {"Page_number": 148, "text": "section 4.5\n\nvariance 133\n\ndefinition\nif x is a random variable with mean \u03bc, then the variance of x, denoted by var(x),\nis defined by\n\nvar(x) = e[(x \u2212 \u03bc)2]\n\nan alternative formula for var(x) is derived as follows:\n\nvar(x) = e[(x \u2212 \u03bc)2]\n\n=\n\n=\n\n(cid:6)\n(x \u2212 \u03bc)2p(x)\n(cid:6)\n(x2 \u2212 2\u03bcx + \u03bc2)p(x)\n(cid:6)\nx2p(x) \u2212 2\u03bc\n\n(cid:6)\n\nx\n\nx\n\nx\n\n=\n= e[x2] \u2212 2\u03bc2 + \u03bc2\n= e[x2] \u2212 \u03bc2\n\nx\n\nxp(x) + \u03bc2\n\n(cid:6)\n\nx\n\np(x)\n\nthat is,\n\nvar(x) = e[x2] \u2212 (e[x])2\n\nin words, the variance of x is equal to the expected value of x2 minus the square\nof its expected value. in practice, this formula frequently offers the easiest way to\ncompute var(x).\n\nexample 5a\ncalculate var(x) if x represents the outcome when a fair die is rolled.\nsolution. it was shown in example 3a that e[x] = 7\n(cid:2)\n\n2. also,\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\n+ 22\n\n1\n6\n\n+ 32\n\n1\n6\n\n+ 42\n\n1\n6\n\n+ 52\n\n1\n6\n\n+ 62\n\n(cid:2)\n(cid:3)\n\n1\n6\n\n(cid:2)\ne[x2] = 12\n1\n6\n\n=\n\nhence,\n\n(91)\n\n(cid:2)\n\n(cid:3)2 = 35\n\n12\n\n7\n2\n\nvar(x) = 91\n6\n\n\u2212\n\n(cid:3)\n\n(cid:2)\n\n1\n6\n\n.\n\na useful identity is that, for any constants a and b,\n\nvar(ax + b) = a2var(x)\n\n "}, {"Page_number": 149, "text": "134\n\nchapter 4\n\nrandom variables\nto prove this equality, let \u03bc = e[x] and note from corollary 4.1 that e[ax + b] =\na\u03bc + b. therefore,\n\nvar(ax + b) = e[(ax + b \u2212 a\u03bc \u2212 b)2]\n\n= e[a2(x \u2212 \u03bc)2]\n= a2e[(x \u2212 \u03bc)2]\n= a2var(x)\n\nremarks.\n\n(a) analogous to the means being the center of gravity of a distribu-\ntion of mass, the variance represents, in the terminology of mechanics, the moment\nof inertia.\n\n(b) the square root of the var(x) is called the standard deviation of x, and we\n\ndenote it by sd(x). that is,\n\n.\n\nsd(x) =\n\nvar(x)\n\ndiscrete random variables are often classified according to their probability mass\n\nfunctions. in the next few sections, we consider some of the more common types.\n\n4.6 the bernoulli and binomial random variables\n\nsuppose that a trial, or an experiment, whose outcome can be classified as either a\nsuccess or a failure is performed. if we let x = 1 when the outcome is a success and\nx = 0 when it is a failure, then the probability mass function of x is given by\n\np(0) = p{x = 0} = 1 \u2212 p\np(1) = p{x = 1} = p\n\n(6.1)\n\nwhere p, 0 \u2026 p \u2026 1, is the probability that the trial is a success.\n\na random variable x is said to be a bernoulli random variable (after the swiss\nmathematician james bernoulli) if its probability mass function is given by equa-\ntions (6.1) for some p \u2208 (0, 1).\nsuppose now that n independent trials, each of which results in a success with prob-\nability p and in a failure with probability 1 \u2212 p, are to be performed. if x represents\nthe number of successes that occur in the n trials, then x is said to be a binomial\nrandom variable with parameters (n, p). thus, a bernoulli random variable is just a\nbinomial random variable with parameters (1, p).\n\nthe probability mass function of a binomial random variable having parameters\n\n(n, p) is given by\n\np(i) =\n\npi(1 \u2212 p)n\u2212i\n\ni = 0, 1, . . . , n\n\n(6.2)\n\n(cid:2)\n\n(cid:3)\n\nn\ni\n\n(cid:3)\n\n(cid:2)\n\nthe validity of equation (6.2) may be verified by first noting that the probability of\nany particular sequence of n outcomes containing i successes and n \u2212 i failures is,\nby the assumed independence of trials, pi(1 \u2212 p)n\u2212i. equation (6.2) then follows,\n(cid:3)\nsince there are\ndifferent sequences of the n outcomes leading to i successes and\nn \u2212 i failures. this perhaps can most easily be seen by noting that there are\nn\ni\ndifferent choices of the i trials that result in successes. for instance, if n = 4, i = 2,\n= 6 ways in which the four trials can result in two successes,\nthen there are\n\n(cid:2)\n\n(cid:2)\n\n(cid:3)\n\nn\ni\n\n4\n2\n\n "}, {"Page_number": 150, "text": "section 4.6\n\nthe bernoulli and binomial random variables 135\n\nnamely, any of the outcomes (s, s, f , f ), (s, f , s, f ), (s, f , f , s), (f , s, s, f ), (f , s, f , s), and\n(f , f , s, s), where the outcome (s, s, f , f ) means, for instance, that the first two trials\n(cid:2)\nare successes and the last two failures. since each of these outcomes has probability\np2(1 \u2212 p)2 of occurring, the desired probability of two successes in the four trials\nis\n\n(cid:3)\n\nnote that, by the binomial theorem, the probabilities sum to 1; that is,\n\n4\n2\n\np2(1 \u2212 p)2.\nq(cid:6)\n\np(i) = n(cid:6)\n\n(cid:18)\n\ni=0\n\ni=0\n\n(cid:19)\n\npi(1 \u2212 p)n\u2212i = [p + (1 \u2212 p)]n = 1\n\nni\n\nexample 6a\nfive fair coins are flipped. if the outcomes are assumed independent, find the proba-\nbility mass function of the number of heads obtained.\n\nsolution. if we let x equal the number of heads (successes) that appear, then x\n. hence, by equa-\nis a binomial random variable with parameters\ntion (6.2),\n\n2\n\n(cid:30)\n\n32\n\n(cid:29)\nn = 5, p = 1\n(cid:3)0(cid:2)\n(cid:3)1(cid:2)\n(cid:3)2(cid:2)\n(cid:3)3(cid:2)\n(cid:3)4(cid:2)\n(cid:3)5(cid:2)\n\n(cid:3)5 = 1\n(cid:3)4 = 5\n(cid:3)3 = 10\n(cid:3)2 = 10\n(cid:3)1 = 5\n(cid:3)0 = 1\n\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n\n32\n\n32\n\n32\n\n32\n\n32\n\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n\n(cid:3)(cid:2)\n(cid:3)(cid:2)\n(cid:3)(cid:2)\n(cid:3)(cid:2)\n(cid:3)(cid:2)\n(cid:3)(cid:2)\n\n(cid:2)\n(cid:2)\n(cid:2)\n(cid:2)\n(cid:2)\n(cid:2)\n\n5\n0\n\n5\n1\n\n5\n2\n\n5\n3\n\n5\n4\n\n5\n5\n\np{x = 0} =\n\np{x = 1} =\np{x = 2} =\np{x = 3} =\n\np{x = 4} =\n\np{x = 5} =\n\n.\n\nexample 6b\nit is known that screws produced by a certain company will be defective with proba-\nbility .01, independently of each other. the company sells the screws in packages of\n10 and offers a money-back guarantee that at most 1 of the 10 screws is defective.\nwhat proportion of packages sold must the company replace?\n\nsolution. if x is the number of defective screws in a package, then x is a binomial\nrandom variable with parameters (10, .01). hence, the probability that a package will\nhave to be replaced is\n\n(cid:2)\n\n(cid:3)\n\n10\n0\n\n(.01)0(.99)10 \u2212\n\n(cid:2)\n\n(cid:3)\n\n10\n1\n\n1 \u2212 p{x = 0} \u2212 p{x = 1} = 1 \u2212\nl .004\n\n(.01)1(.99)9\n\n.\n\nthus, only .4 percent of the packages will have to be replaced.\n\n "}, {"Page_number": 151, "text": "136\n\nchapter 4\n\nrandom variables\n\nexample 6c\nthe following gambling game, known as the wheel of fortune (or chuck-a-luck), is\nquite popular at many carnivals and gambling casinos: a player bets on one of the\nnumbers 1 through 6. three dice are then rolled, and if the number bet by the player\nappears i times, i = 1, 2, 3, then the player wins i units; if the number bet by the player\ndoes not appear on any of the dice, then the player loses 1 unit. is this game fair to\nthe player? (actually, the game is played by spinning a wheel that comes to rest on\na slot labeled by three of the numbers 1 through 6, but this variant is mathematically\nequivalent to the dice version.)\n\n(cid:29)\n\n(cid:30)\n\nsolution. if we assume that the dice are fair and act independently of each other,\nthen the number of times that the number bet appears is a binomial random variable\n. hence, letting x denote the player\u2019s winnings in the game,\nwith parameters\nwe have\n\n3, 1\n6\n\n(cid:2)\n(cid:2)\n(cid:2)\n(cid:2)\n\n(cid:3)(cid:2)\n(cid:3)(cid:2)\n(cid:3)(cid:2)\n(cid:3)(cid:2)\n\n(cid:3)0(cid:2)\n(cid:3)1(cid:2)\n(cid:3)2(cid:2)\n(cid:3)3(cid:2)\n\n216\n\n(cid:3)3 = 125\n(cid:3)2 = 75\n(cid:3)1 = 15\n(cid:3)0 = 1\n\n216\n\n216\n\n216\n\n5\n6\n5\n6\n5\n6\n5\n6\n\n1\n6\n1\n6\n1\n6\n1\n6\n\n3\n0\n\n3\n1\n\n3\n2\n\n3\n3\n\np{x = \u22121} =\n\np{x = 1} =\n\np{x = 2} =\np{x = 3} =\n\nin order to determine whether or not this is a fair game for the player, let us calcu-\n\nlate e[x]. from the preceding probabilities, we obtain\n\ne[x] = \u2212125 + 75 + 30 + 3\n\n216\n\n= \u221217\n\n216\n\nhence, in the long run, the player will lose 17 units per every 216 games he plays. .\nin the next example, we consider the simplest form of the theory of inheritance as\n\ndeveloped by gregor mendel (1822\u20131884).\n\nexample 6d\nsuppose that a particular trait (such as eye color or left-handedness) of a person is\nclassified on the basis of one pair of genes, and suppose also that d represents a dom-\ninant gene and r a recessive gene. thus, a person with dd genes is purely dominant,\none with rr is purely recessive, and one with rd is hybrid. the purely dominant and the\nhybrid individuals are alike in appearance. children receive 1 gene from each parent.\nif, with respect to a particular trait, 2 hybrid parents have a total of 4 children, what is\nthe probability that 3 of the 4 children have the outward appearance of the dominant\ngene?\n\nsolution. if we assume that each child is equally likely to inherit either of 2 genes\nfrom each parent, the probabilities that the child of 2 hybrid parents will have dd,\nrr, and rd pairs of genes are, respectively, 1\n2. hence, since an offspring will\n\n4, and 1\n\n4, 1\n\n "}, {"Page_number": 152, "text": "section 4.6\n\nthe bernoulli and binomial random variables 137\n\n(cid:29)\n\n(cid:30)\n\nhave the outward appearance of the dominant gene if its gene pair is either dd or rd,\nit follows that the number of such children is binomially distributed with parameters\n4, 3\n4\n\n. thus, the desired probability is\n\n(cid:2)\n\n(cid:3)(cid:2)\n\n(cid:3)3(cid:2)\n\n4\n3\n\n3\n4\n\n(cid:3)1 = 27\n\n64\n\n1\n4\n\n.\n\nexample 6e\nconsider a jury trial in which it takes 8 of the 12 jurors to convict the defendant; that\nis, in order for the defendant to be convicted, at least 8 of the jurors must vote him\nguilty. if we assume that jurors act independently and that, whether or not the defen-\ndant is guilty, each makes the right decision with probability \u03b8, what is the probability\nthat the jury renders a correct decision?\n\nsolution. the problem, as stated, is incapable of solution, for there is not yet enough\ninformation. for instance, if the defendant is innocent, the probability of the jury\u2019s\nrendering a correct decision is\n\nwhereas, if he is guilty, the probability of a correct decision is\n\n(cid:3)\n\n(cid:3)\n\n(cid:2)\n\n(cid:2)\n\n12(cid:6)\n\ni=5\n\n12(cid:6)\n\ni=8\n\n12\ni\n\n12\ni\n\n\u03b8 i(1 \u2212 \u03b8 )12\u2212i\n\n\u03b8 i(1 \u2212 \u03b8 )12\u2212i\n\ntherefore, if \u03b1 represents the probability that the defendant is guilty, then, by condi-\ntioning on whether or not he is guilty, we obtain the probability that the jury renders\na correct decision:\n\n\u03b8 i(1 \u2212 \u03b8 )12\u2212i + (1 \u2212 \u03b1)\n\n12\ni\n\n\u03b8 i(1 \u2212 \u03b8 )12\u2212i\n\n.\n\n(cid:3)\n\n(cid:2)\n\n12(cid:6)\n\n\u03b1\n\ni=8\n\n12\ni\n\n(cid:3)\n\n(cid:2)\n\n12(cid:6)\n\ni=5\n\nexample 6f\na communication system consists of n components, each of which will, indepen-\ndently, function with probability p. the total system will be able to operate effectively\nif at least one-half of its components function.\n(a) for what values of p is a 5-component system more likely to operate effectively\n(b) in general, when is a (2k + 1)-component system better than a (2k \u2212 1)-\n\nthan a 3-component system?\n\ncomponent system?\n\nsolution. (a) because the number of functioning components is a binomial random\nvariable with parameters (n, p), it follows that the probability that a 5-component\n\nsystem will be effective is(cid:2)\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\n5\n4\n\np3(1 \u2212 p)2 +\n\np4(1 \u2212 p) + p5\n\n5\n3\n\n "}, {"Page_number": 153, "text": "138\n\nchapter 4\n\nrandom variables\n\nwhereas the corresponding probability for a 3-component system is\n\n(cid:2)\n\n(cid:3)\n\n3\n2\n\np2(1 \u2212 p) + p3\n\nhence, the 5-component system is better if\n\n10p3(1 \u2212 p)2 + 5p4(1 \u2212 p) + p5 > 3p2(1 \u2212 p) + p3\n\nwhich reduces to\n\nor\n\n3(p \u2212 1)2(2p \u2212 1) > 0\n\np >\n\n1\n2\n\n(b) in general, a system with 2k + 1 components will be better than one with\n2. to prove this, consider a system of 2k + 1\n\n2k \u2212 1 components if (and only if) p > 1\ncomponents and let x denote the number of the first 2k \u2212 1 that function. then\n\np2k+1(effective) = p{x \u00fa k + 1} + p{x = k}(1 \u2212 (1 \u2212 p)2)\n\n+ p{x = k \u2212 1}p2\n\nwhich follows because the (2k + 1)-component system will be effective if either\n(i) x \u00fa k + 1;\n(ii) x = k and at least one of the remaining 2 components function; or\n(iii) x = k \u2212 1 and both of the next 2 components function.\nsince\n\np2k\u22121(effective) = p{x \u00fa k}\n\n= p{x = k} + p{x \u00fa k + 1}\n\nwe obtain\n\np2k+1(effective) \u2212 p2k\u22121(effective)\n\n(cid:2)\n\n(cid:3)\n(cid:3)\n\n2k \u2212 1\nk \u2212 1\n2k \u2212 1\n\n(cid:2)\n= p{x = k \u2212 1}p2 \u2212 (1 \u2212 p)2p{x = k}\n(cid:2)\n=\npk\u22121(1 \u2212 p)kp2 \u2212 (1 \u2212 p)2\npk(1 \u2212 p)k[ p \u2212 (1 \u2212 p)] since\n1\n2\n\n> 0 3 p >\n\n=\n\nk\n\n(cid:3)\n\n2k \u2212 1\n(cid:2)\n\nk\n2k \u2212 1\nk \u2212 1\n\n(cid:2)\n\n(cid:3)\npk(1 \u2212 p)k\u22121\n2k \u2212 1\n\n=\n\nk\n\n(cid:3)\n\n.\n\n "}, {"Page_number": 154, "text": "section 4.6\n\nthe bernoulli and binomial random variables 139\n\n4.6.1 properties of binomial random variables\nwe will now examine the properties of a binomial random variable with parameters\nn and p. to begin, let us compute its expected value and variance. now,\n\n(cid:3)\n(cid:3)\n\nn\ni\n\nn\ni\n\n(cid:2)\n\n(cid:2)\n(cid:2)\n\nik\n\nik\n\n= n\n(cid:3)\n\ni=0\n\ne[xk] = n(cid:6)\n= n(cid:6)\n(cid:3)\n(cid:2)\n\ni=1\n\nn\ni\n\ni\n\n(cid:2)\n\nik\u22121\n\npi(1 \u2212 p)n\u2212i\n\npi(1 \u2212 p)n\u2212i\n\n(cid:3)\n\nn \u2212 1\ni \u2212 1\n\nusing the identity\n\ngives\n\ne[xk] = np\n\nn(cid:6)\nn\u22121(cid:6)\n\ni=1\n\npi\u22121(1 \u2212 p)n\u2212i\n\nn \u2212 1\ni \u2212 1\n(cid:2)\n\n(cid:3)\n\nn \u2212 1\n\nj\n\npj(1 \u2212 p)n\u22121\u2212j\n\nby letting\nj = i \u2212 1\n\n= np\n(j + 1)k\u22121\n= npe[(y + 1)k\u22121]\n\nj=0\n\nwhere y is a binomial random variable with parameters n \u2212 1, p. setting k = 1 in the\npreceding equation yields\n\ne[x] = np\n\nthat is, the expected number of successes that occur in n independent trials when\neach is a success with probability p is equal to np. setting k = 2 in the preceding equa-\ntion, and using the preceding formula for the expected value of a binomial random\nvariable yields\n\ne[x2] = npe[y + 1]\n\n= np[(n \u2212 1)p + 1]\n\nsince e[x] = np, we obtain\n\nvar(x) = e[x2] \u2212 (e[x])2\n\n= np[(n \u2212 1)p + 1] \u2212 (np)2\n= np(1 \u2212 p)\n\nsumming up, we have shown the following:\n\nif x is a binomial random variable with parameters n and p, then\n\ne[x] = np\n\nvar(x) = np(1 \u2212 p)\n\n "}, {"Page_number": 155, "text": "140\n\nchapter 4\n\nrandom variables\n\nthe following proposition details how the binomial probability mass function first\n\nincreases and then decreases.\nproposition 6.1. if x is a binomial random variable with parameters (n, p), where\n0 < p < 1, then as k goes from 0 to n, p{x = k} first increases monotonically and\nthen decreases monotonically, reaching its largest value when k is the largest integer\nless than or equal to (n + 1)p.\n\nproof. we prove the proposition by considering p{x = k}/p{x = k \u2212 1} and\ndetermining for what values of k it is greater or less than 1. now,\npk(1 \u2212 p)n\u2212k\n\nn!\n\np{x = k}\np{x = k \u2212 1} =\n\n(n \u2212 k)!k!\nn!\n\n(n \u2212 k + 1)!(k \u2212 1)!\n\n= (n \u2212 k + 1)p\nk(1 \u2212 p)\n\npk\u22121(1 \u2212 p)n\u2212k+1\n\nhence, p{x = k} \u00fa p{x = k \u2212 1} if and only if\n\n(n \u2212 k + 1)p \u00fa k(1 \u2212 p)\n\nor, equivalently, if and only if\n\nand the proposition is proved.\n\nk \u2026 (n + 1)p\n\n.\n\nas an illustration of proposition 6.1 consider figure 4.5, the graph of the probabil-\n\nity mass function of a binomial random variable with parameters (10, 1\n2\n\n).\n\nexample 6g\nin a u.s. presidential election, the candidate who gains the maximum number of\nvotes in a state is awarded the total number of electoral college votes allocated to\n\n1024 \u2afb p(k)\n\n252\n\n210\n\n120\n\n45\n\n10\n1\n\n0\n\n4\n\n3\n\n2\n\n1\n\nfigure 4.5 graph of p(k) =(cid:18)\n\n5\n\n6\n\n7\n\n(cid:19)(cid:29)\n\n10\nk\n\nk\n\n9\n\n10\n\n8\n\n(cid:30)10\n\n1\n2\n\n "}, {"Page_number": 156, "text": "section 4.6\n\nthe bernoulli and binomial random variables 141\n\nthat state. the number of electoral college votes of a given state is roughly propor-\ntional to the population of that state\u2014that is, a state with population n has roughly\nnc electoral votes. (actually, it is closer to nc + 2, as a state is given an electoral\nvote for each member it has in the house of representatives, with the number of\nsuch representatives being roughly proportional to the population of the state, and\none electoral college vote for each of its two senators.) let us determine the average\npower of a citizen in a state of size n in a close presidential election, where, by average\npower in a close election, we mean that a voter in a state of size n = 2k + 1 will be\ndecisive if the other n \u2212 1 voters split their votes evenly between the two candidates.\n(we are assuming here that n is odd, but the case where n is even is quite similar.)\nbecause the election is close, we shall suppose that each of the other n \u2212 1 = 2k\nvoters acts independently and is equally likely to vote for either candidate. hence,\nthe probability that a voter in a state of size n = 2k + 1 will make a difference to\nthe outcome is the same as the probability that 2k tosses of a fair coin land heads and\ntails an equal number of times. that is,\n\np{voter in state of size 2k + 1 makes a difference}\n\n(cid:2)\n\n(cid:3)(cid:2)\n\n(cid:3)k(cid:2)\n\n(cid:3)k\n\n=\n\n2k\nk\n\n1\n2\n\n1\n2\n\n= (2k)!\nk!k!22k\n\nto approximate the preceding equality, we make use of stirling\u2019s approximation,\nwhich says that, for k large,\n\nk! \u223c kk+1/2e\n\n\u2212k\n\n\u221a\n\n2\u03c0\n\nwhere we say that ak \u223c bk when the ratio ak/bk approaches 1 as k approaches q.\nhence, it follows that\n\np{voter in state of size 2k + 1 makes a difference}\n\n\u221a\n(2k)2k+1/2e\n2\u03c0\nk2k+1e\u22122k(2\u03c0 )22k\n\n\u22122k\n\n\u223c\n\n= 1\u221a\nk\u03c0\n\nbecause such a voter (if he or she makes a difference) will affect nc electoral votes,\nthe expected number of electoral votes a voter in a state of size n will affect\u2014or the\nvoter\u2019s average power\u2014is given by\n\naverage power = ncp{makes a difference}\n\nnc.\n.\n= c\n\n\u223c\n\nn\u03c0/2\n2n/\u03c0\n\nthus, the average power of a voter in a state of size n is proportional to the square\nroot of n, showing that, in presidential elections, voters in large states have more\n.\npower than do those in smaller states.\n\n "}, {"Page_number": 157, "text": "142\n\nchapter 4\n\nrandom variables\n\n4.6.2 computing the binomial distribution function\nsuppose that x is binomial with parameters (n, p). the key to computing its distribu-\ntion function\n\np{x \u2026 i} = i(cid:6)\n\nk=0\n\n(cid:2)\n\n(cid:3)\n\nn\nk\n\npk(1 \u2212 p)n\u2212k\n\ni = 0, 1, . . . , n\n\nis to utilize the following relationship between p{x = k + 1} and p{x = k}, which\nwas established in the proof of proposition 6.1:\n\np{x = k + 1} = p\n\n1 \u2212 p\n\nn \u2212 k\nk + 1\n\np{x = k}\n\n(6.3)\n\nexample 6h\nlet x be a binomial random variable with parameters n = 6, p = .4. then, starting\nwith p{x = 0} = (.6)6 and recursively employing equation (6.3), we obtain\n\np{x = 0} = (.6)6 l .0467\np{x = 1} = 4\n6\np{x = 2} = 4\n6\np{x = 3} = 4\n6\np{x = 4} = 4\n6\np{x = 5} = 4\n6\np{x = 6} = 4\n6\n\np{x = 0} l .1866\np{x = 1} l .3110\np{x = 2} l .2765\np{x = 3} l .1382\np{x = 4} l .0369\np{x = 5} l .0041\n\n6\n1\n5\n2\n4\n3\n3\n4\n2\n5\n1\n6\n\n.\n\na computer program that utilizes the recursion (6.3) to compute the binomial\ndistribution function is easily written. to compute p{x \u2026 i}, the program should\nfirst compute p{x = i} and then use the recursion to successively compute p{x =\ni \u2212 1}, p{x = i \u2212 2}, and so on.\n\nhistorical note\nindependent trials having a common probability of success p were first studied\nby the swiss mathematician jacques bernoulli (1654\u20131705). in his book ars con-\njectandi (the art of conjecturing), published by his nephew nicholas eight years\nafter his death in 1713, bernoulli showed that if the number of such trials were\nlarge, then the proportion of them that were successes would be close to p with a\nprobability near 1.\n\njacques bernoulli was from the first generation of the most famous mathemat-\nical family of all time. altogether, there were between 8 and 12 bernoullis, spread\nover three generations, who made fundamental contributions to probability, statis-\ntics, and mathematics. one difficulty in knowing their exact number is the fact that\nseveral had the same name. (for example, two of the sons of jacques\u2019s brother jean\n\n "}, {"Page_number": 158, "text": "section 4.7\n\nthe poisson random variable 143\n\nwere named jacques and jean.) another difficulty is that several of the bernoullis\nwere known by different names in different places. our jacques (sometimes writ-\nten jaques) was, for instance, also known as jakob (sometimes written jacob) and\nas james bernoulli. but whatever their number, their influence and output were\nprodigious. like the bachs of music, the bernoullis of mathematics were a family\nfor the ages!\n\nexample 6i\nif x is a binomial random variable with parameters n = 100 and p = .75, find p{x =\n70} and p{x \u2026 70}.\nsolution. the answer is shown here in figure 4.6.\n\nbinomial distribution\n\nenter value for p:.75\n\nenter value for n:100\n\nenter value for i:70\n\nstart\n\nquit\n\nprobability (number of successes = i) = .04575381\n\nprobability (number of successes < = i) = .14954105\n\nfigure 4.6\n\n4.7 the poisson random variable\n\na random variable x that takes on one of the values 0, 1, 2, . . . is said to be a poisson\nrandom variable with parameter \u03bb if, for some \u03bb > 0,\n\nequation (7.1) defines a probability mass function, since\n\n\u2212\u03bb \u03bbi\ni!\n\np(i) = p{x = i} = e\nq(cid:6)\nq(cid:6)\n\np(i) = e\n\n\u2212\u03bb\n\ni=0\n\n= e\n\n\u2212\u03bbe\u03bb = 1\n\n\u03bbi\ni!\n\ni=0\n\ni = 0, 1, 2, . . .\n\n(7.1)\n\nthe poisson probability distribution was introduced by sim\u00b4eon denis poisson in a\nbook he wrote regarding the application of probability theory to lawsuits, criminal\ntrials, and the like. this book, published in 1837, was entitled recherches sur la prob-\nabilit\u00b4e des jugements en mati`ere criminelle et en mati`ere civile (investigations into the\nprobability of verdicts in criminal and civil matters).\n\nthe poisson random variable has a tremendous range of applications in diverse\nareas because it may be used as an approximation for a binomial random variable\nwith parameters (n, p) when n is large and p is small enough so that np is of moderate\n\n "}, {"Page_number": 159, "text": "144\n\nchapter 4\n\nrandom variables\n\nsize. to see this, suppose that x is a binomial random variable with parameters (n, p),\nand let \u03bb = np. then\n\nn!\n\n(cid:3)n\u2212i\n(cid:3)i(cid:2)\n(cid:2)\npi(1 \u2212 p)n\u2212i\np{x = i} =\n(n \u2212 i)!i!\n=\n1 \u2212 \u03bb\n(n \u2212 i)!i!\nn\n(1 \u2212 \u03bb/n)n\n= n(n \u2212 1)\u00b7\u00b7\u00b7 (n \u2212 i + 1)\n(1 \u2212 \u03bb/n)i\n\n\u03bbi\ni!\n\n\u03bb\nn\n\nn!\n\nni\n\nnow, for n large and \u03bb moderate,\n\n(cid:2)\n\n(cid:3)n\n\n1 \u2212 \u03bb\nn\n\n\u2212\u03bb\n\nl e\n\nn(n \u2212 1)\u00b7\u00b7\u00b7 (n \u2212 i + 1)\n\nni\n\n(cid:3)i\n\n(cid:2)\n1 \u2212 \u03bb\nn\n\nl 1\n\nl 1\n\nhence, for n large and \u03bb moderate,\n\np{x = i} l e\n\n\u2212\u03bb \u03bbi\ni!\n\nin other words, if n independent trials, each of which results in a success with\nprobability p, are performed, then, when n is large and p is small enough to make\nnp moderate, the number of successes occurring is approximately a poisson random\nvariable with parameter \u03bb = np. this value \u03bb (which will later be shown to equal the\nexpected number of successes) will usually be determined empirically.\n\nsome examples of random variables that generally obey the poisson probability\n\nlaw [that is, they obey equation (7.1)] are as follows:\n\n1. the number of misprints on a page (or a group of pages) of a book\n2. the number of people in a community who survive to age 100\n3. the number of wrong telephone numbers that are dialed in a day\n4. the number of packages of dog biscuits sold in a particular store each day\n5. the number of customers entering a post office on a given day\n6. the number of vacancies occurring during a year in the federal judicial system\n7. the number of \u03b1-particles discharged in a fixed period of time from some radioac-\n\ntive material\n\neach of the preceding, and numerous other random variables, are approximately\npoisson for the same reason\u2014namely, because of the poisson approximation to the\nbinomial. for instance, we can suppose that there is a small probability p that each\nletter typed on a page will be misprinted. hence, the number of misprints on a page\nwill be approximately poisson with \u03bb = np, where n is the number of letters on a\npage. similarly, we can suppose that each person in a community has some small\nprobability of reaching age 100. also, each person entering a store may be thought of\nas having some small probability of buying a package of dog biscuits, and so on.\n\nexample 7a\nsuppose that the number of typographical errors on a single page of this book has a\npoisson distribution with parameter \u03bb = 1\n2. calculate the probability that there is at\nleast one error on this page.\n\n "}, {"Page_number": 160, "text": "section 4.7\n\nthe poisson random variable 145\n\nsolution. letting x denote the number of errors on this page, we have\n\np{x \u00fa 1} = 1 \u2212 p{x = 0} = 1 \u2212 e\n\n\u22121/2 l .393\n\n.\n\nexample 7b\nsuppose that the probability that an item produced by a certain machine will be\ndefective is .1. find the probability that a sample of 10 items will contain at most\n1 defective item.\n\n(cid:2)\n\n(cid:3)\n\n10\n0\n\n(.1)0(.9)10 +\n\n(cid:2)\n\u22121 + e\n\n(cid:3)\n10\n1\n\u22121 l .7358.\n\n(.1)1(.9)9 = .7361,\n.\n\nsolution. the desired probability is\n\nwhereas the poisson approximation yields the value e\n\nexample 7c\nconsider an experiment that consists of counting the number of \u03b1 particles given\noff in a 1-second interval by 1 gram of radioactive material. if we know from past\nexperience that, on the average, 3.2 such \u03b1 particles are given off, what is a good\napproximation to the probability that no more than 2 \u03b1 particles will appear?\n\nsolution. if we think of the gram of radioactive material as consisting of a large num-\nber n of atoms, each of which has probability of 3.2/n of disintegrating and sending off\nan \u03b1 particle during the second considered, then we see that, to a very close approx-\nimation, the number of \u03b1 particles given off will be a poisson random variable with\nparameter \u03bb = 3.2. hence, the desired probability is\n\np{x \u2026 2} = e\n\n\u22123.2 + 3.2e\n\n\u22123.2 + (3.2)2\n\nl .3799\n\n2\n\n\u22123.2\ne\n\n.\n\nbefore computing the expected value and variance of the poisson random variable\nwith parameter \u03bb, recall that this random variable approximates a binomial random\nvariable with parameters n and p when n is large, p is small, and \u03bb = np. since such\na binomial random variable has expected value np = \u03bb and variance np(1 \u2212 p) =\n\u03bb(1 \u2212 p) l \u03bb (since p is small), it would seem that both the expected value and the\nvariance of a poisson random variable would equal its parameter \u03bb. we now verify\nthis result:\n\ne[x] =\n\nq(cid:6)\nq(cid:6)\n\ni=0\n= \u03bb\n\ni=1\n\u2212\u03bb\n\n= \u03bbe\n\n\u2212\u03bb\u03bbi\nie\ni!\n\u2212\u03bb\u03bbi\u22121\ne\n(i \u2212 1)!\nq(cid:6)\n\n\u03bbj\nj!\n\nj=0\n\n= \u03bb\n\nby letting\nj = i \u2212 1\nq(cid:6)\n\n= e\u03bb\n\n\u03bbj\nj!\n\nsince\n\nj=0\n\n "}, {"Page_number": 161, "text": "146\n\nchapter 4\n\nrandom variables\n\nthus, the expected value of a poisson random variable x is indeed equal to its\n\nparameter \u03bb. to determine its variance, we first compute e[x2]:\n\ne[x2] =\n\ni=0\n= \u03bb\n\ni2e\n\nq(cid:6)\nq(cid:6)\nq(cid:6)\n\u23a1\n\u23a2\u23a3 q(cid:6)\n\nj=0\n\ni=1\n\nj=0\n\n\u2212\u03bb\u03bbi\ni!\n\u2212\u03bb\u03bbi\u22121\nie\n(i \u2212 1)!\n(j + 1)e\nj!\n\n\u2212\u03bb\u03bbj\nje\nj!\n\n= \u03bb\n\n= \u03bb\n\n= \u03bb(\u03bb + 1)\n\n\u2212\u03bb\u03bbj\nq(cid:6)\n\n+\n\nj=0\n\nby letting\n\u23a4\nj = i \u2212 1\n\u23a5\u23a6\n\n\u2212\u03bb\u03bbj\ne\nj!\n\nwhere the final equality follows because the first sum is the expected value of a pois-\nson random variable with parameter \u03bb and the second is the sum of the probabilities\nof this random variable. therefore, since we have shown that e[x] = \u03bb, we obtain\n\nvar(x) = e[x2] \u2212 (e[x])2\n\n= \u03bb\n\nhence, the expected value and variance of a poisson random variable are both\n\nequal to its parameter \u03bb.\n\nwe have shown that the poisson distribution with parameter np is a very good\napproximation to the distribution of the number of successes in n independent trials\nwhen each trial has probability p of being a success, provided that n is large and p\nsmall. in fact, it remains a good approximation even when the trials are not inde-\npendent, provided that their dependence is weak. for instance, recall the matching\nproblem (example 5m of chapter 2) in which n men randomly select hats from a set\nconsisting of one hat from each person. from the point of view of the number of men\nwho select their own hat, we may regard the random selection as the result of n tri-\nals where we say that trial i is a success if person i selects his own hat, i = 1, . . . , n.\ndefining the events ei, i = 1, . . . , n, by\n\nei = {trial i is a success}\n\nit is easy to see that\n\np{ei} = 1\nn\n\nand p{ei|ej} = 1\n\nn \u2212 1\n\n,\n\nj z i\n\nthus, we see that, although the events ei, i = 1, . . . , n are not independent, their\ndependence, for large n, appears to be weak. because of this it seems reasonable to\nexpect that the number of successes will approximately have a poisson distribution\nwith parameter n * 1/n = 1, and indeed this is verified in example 5m of chapter 2.\nfor a second illustration of the strength of the poisson approximation when the\ntrials are weakly dependent, let us consider again the birthday problem presented in\nexample 5i of chapter 2. in this example, we suppose that each of n people is equally\nlikely to have any of the 365 days of the year as his or her birthday, and the problem\n\n "}, {"Page_number": 162, "text": "section 4.7\n\nthe poisson random variable 147\n\n(cid:3)\n\n(cid:2)\n\nis to determine the probability that a set of n independent people all have different\nbirthdays. a combinatorial argument was used to determine this probability, which\nwas shown to be less than 1\n\n2 when n = 23.\n\nwe can approximate the preceding probability by using the poisson approximation\npairs of individuals i and\nas follows: imagine that we have a trial for each of the\nj, i z j, and say that trial i, j is a success if persons i and j have the same birthday. if\nwe let eij denote the event that trial i, j is a success, then, whereas the\nevents\neij, 1 \u2026 i < j \u2026 n, are not independent (see theoretical exercise 21), their depen-\ndence appears to be rather weak. (indeed, these events are even pairwise indepen-\ndent, in that any 2 of the events eij and ekl are independent\u2014again, see theoretical\nexercise 21). since p(eij) = 1/365, it is reasonable to suppose that the number of\n365 =\nsuccesses should approximately have a poisson distribution with mean\nn(n \u2212 1)/730. therefore,\n\n(cid:3)(cid:26)\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\nn\n2\n\nn\n2\n\nn\n2\n\np{no 2 people have the same birthday} = p{0 successes}\n\n%\u2212n(n \u2212 1)\n\n/\n\n730\n\nl exp\n\nto determine the smallest integer n for which this probability is less than 1\n\n2, note that\n\nexp\n\nis equivalent to\n\n/\n\n%\u2212n(n \u2212 1)\n%\n/\n\nn(n \u2212 1)\n\n730\n\nexp\n\n730\n\n\u2026 1\n2\n\n\u00fa 2\n\ntaking logarithms of both sides, we obtain\n\nn(n \u2212 1) \u00fa 730 log 2\nl 505.997\n\nwhich yields the solution n = 23, in agreement with the result of example 5i of\nchapter 2.\n\nsuppose now that we wanted the probability that, among the n people, no 3 of\nthem have their birthday on the same day. whereas this now becomes a difficult com-\nbinatorial problem, it is a simple matter to obtain a good approximation. to begin,\ntriplets i, j, k, where 1 \u2026 i < j < k \u2026 n,\nimagine that we have a trial for each of the\nand call the i, j, k trial a success if persons i, j, and k all have their birthday on the same\nday. as before, we can then conclude that the number of successes is approximately\na poisson random variable with parameter\n\n(cid:2)\n\n(cid:3)\n\nn\n3\n\n(cid:2)\n\n(cid:3)\n\nn\n3\n\np{i, j, k have the same birthday} =\n\n(cid:2)\n\n(cid:3)(cid:2)\n\n(cid:3)2\n= n(n \u2212 1)(n \u2212 2)\n\n1\n365\n\nn\n3\n\n6 * (365)2\n\n "}, {"Page_number": 163, "text": "148\n\nchapter 4\n\nrandom variables\n\nhence,\n\np{no 3 have the same birthday} l exp\n\n%\u2212n(n \u2212 1)(n \u2212 2)\n\n/\n\n799350\n\nthis probability will be less than 1\n\n2 when n is such that\n\nn(n \u2212 1)(n \u2212 2) \u00fa 799350 log 2 l 554067.1\n\nwhich is equivalent to n \u00fa 84. thus, the approximate probability that at least 3 people\nin a group of size 84 or larger will have the same birthday exceeds 1\n2.\n\nfor the number of events to occur to approximately have a poisson distribution, it\nis not essential that all the events have the same probability of occurrence, but only\nthat all of these probabilities be small. the following is referred to as the poisson\nparadigm.\n\npoisson paradigm. consider n events, with pi equal to the probability that event i\noccurs, i = 1, . . . , n. if all the pi are \u201csmall\u201d and the trials are either independent or at\nmost \u201cweakly dependent,\u201d then the number of these events that occur approximately\nhas a poisson distribution with mean\n\n(cid:9)\n\nour next example not only makes use of the poisson paradigm, but also illustrates\n\nn\ni=1 pi.\n\na variety of the techniques we have studied so far.\n\nexample 7d length of the longest run\na coin is flipped n times. assuming that the flips are independent, with each one\ncoming up heads with probability p, what is the probability that there is a string of k\nconsecutive heads?\n\nsolution. we will first use the poisson paradigm to approximate this probability.\nnow, if, for i = 1, . . . , n \u2212 k + 1, we let hi denote the event that flips i, i + 1, . . . , i +\nk \u2212 1 all land on heads, then the desired probability is that at least one of the events\nhi occur. because hi is the event that, starting with flip i, the next k flips all land on\nheads, it follows that p(hi) = pk. thus, when pk is small, we might think that the num-\nber of the hi that occur should have an approximate poisson distribution. however,\nsuch is not the case, because, although the events all have small probabilities, some of\ntheir dependencies are too great for the poisson distribution to be a good approxima-\ntion. for instance, because the conditional probability that flips 2, . . . , k + 1 are all\nheads given that flips 1, . . . , k are all heads is equal to the probability that flip k + 1\nis a head, it follows that\n\np(h2|h1) = p\n\nwhich is far greater than the unconditional probability of h2.\n\nthe trick that enables us to use a poisson approximation is to note that there\nwill be a string of k consecutive heads either if there is such a string that is imme-\ndiately followed by a tail or if the final k flips all land on heads. consequently, for\ni = 1, . . . , n \u2212 k, let ei be the event that flips i, . . . , i + k \u2212 1 are all heads and flip\ni + k is a tail; also, let en\u2212k+1 be the event that flips n \u2212 k + 1, . . . , n are all heads.\nnote that\n\np(ei) = pk(1 \u2212 p),\n\np(en\u2212k+1) = pk\n\ni \u2026 n \u2212 k\n\n "}, {"Page_number": 164, "text": "section 4.7\n\nthe poisson random variable 149\n\nthus, when pk is small, each of the events ei has a small probability of occurring.\nmoreover, for i z j, if the events ei and ej refer to nonoverlapping sequences of flips,\nthen p(ei|ej) = p(ei); if they refer to overlapping sequences, then p(ei|ej) = 0.\nhence, in both cases, the conditional probabilities are close to the unconditional ones,\nindicating that n, the number of the events ei that occur, should have an approximate\npoisson distribution with mean\n\ne[n] = n\u2212k+1(cid:6)\n\ni=1\n\np(ei) = (n \u2212 k)pk(1 \u2212 p) + pk\n\nbecause there will not be a run of k heads if (and only if) n = 0, thus the preceding\ngives\n\np(no head strings of length k) = p(n = 0) l exp{\u2212(n \u2212 k)pk(1 \u2212 p) \u2212 pk}\n\nif we let ln denote the largest number of consecutive heads in the n flips, then,\nbecause ln will be less than k if (and only if) there are no head strings of length\nk, the preceding equation can be written as\n\np{ln < k} l exp{\u2212 (n \u2212 k)pk(1 \u2212 p) \u2212 pk}\n\nnow, let us suppose that the coin being flipped is fair; that is, suppose that p = 1/2.\nthen the preceding gives\n\n/\n\n%\n\n/\n\n%\n\np{ln < k} l exp\n\n\u2212 n \u2212 k + 2\n\n2k+1\n\nl exp\n\n\u2212 n\n2k+1\n\nwhere the final approximation supposes that e\nj = log2 n, and assume that j is an integer. for k = j + i,\n\nk\u22122\n2k+1 l 1 (that is, that k\u22122\n2k+1\n\nl 0). let\n\nconsequently,\n\nn\n2k+1\n\n= n\n2j2i+1\n\n= 1\n2i+1\n\np{ln < j + i} l exp{\u2212(1/2)i+1}\n\nwhich implies that\n\np{ln = j + i} = p{ln < j + i + 1} \u2212 p{ln < j + i}\n\nl exp{\u2212(1/2)i+2} \u2212 exp{\u2212(1/2)i+1}\n\nfor instance,\n\np{ln < j \u2212 3} l e\np{ln = j \u2212 3} l e\np{ln = j \u2212 2} l e\np{ln = j \u2212 1} l e\np{ln = j} l e\np{ln = j + 1} l e\np{ln = j + 2} l e\np{ln = j + 3} l e\np{ln \u00fa j + 4} l 1 \u2212 e\n\n\u22124 l .0183\n\u22122 \u2212 e\n\u22121 \u2212 e\n\u22121/2 \u2212 e\n\u22121/4 \u2212 e\n\u22121/8 \u2212 e\n\u22121/16 \u2212 e\n\u22121/32 \u2212 e\n\n\u22124 l .1170\n\u22122 l .2325\n\u22121 l .2387\n\u22121/2 l .1723\n\u22121/4 l .1037\n\u22121/8 l .0569\n\u22121/16 l .0298\n\n\u22121/32 l .0308\n\n "}, {"Page_number": 165, "text": "150\n\nchapter 4\n\nrandom variables\n\nthus, we observe the rather interesting fact that no matter how large n is, the length\nof the longest run of heads in a sequence of n flips of a fair coin will be within 2 of\nlog2\n\n(n) \u2212 1 with a probability approximately equal to .86.\nwe now derive an exact expression for the probability that there is a string of\nk consecutive heads when a coin that lands on heads with probability p is flipped\nn times. with the events ei, i = 1, . . . , n \u2212 k + 1, as defined earlier, and with ln\ndenoting, as before, the length of the longest run of heads,\n\np(ln \u00fa k) = p(there is a string of k consecutive heads) = p(\u222an\u2212k+1\n\ni=1 ei)\nthe inclusion\u2013exclusion identity for the probability of a union can be written as\n\ni=1 ei) = n\u2212k+1(cid:6)\n\np(\u222an\u2212k+1\n\nr=1\n\n(cid:6)\n\ni1<\u00b7\u00b7\u00b7<ir\n\n(\u22121)r+1\n\n\u00b7\u00b7\u00b7 eir\n\n)\n\np(ei1\n\nlet si denote the set of flip numbers to which the event ei refers. (so, for instance,\ns1 = {1, . . . , k + 1}.) now, consider one of the r-way intersection probabilities that\n) where i1 < \u00b7\u00b7\u00b7 <\ndoes not include the event en\u2212k+1. that is, consider p(ei1\nir < n \u2212 k + 1. on the one hand, if there is any overlap in the sets si1, . . . , sir then this\nprobability is 0. on the other hand, if there is no overlap, then the events ei1, . . . , eir\nare independent. therefore,\n\n\u00b7\u00b7\u00b7 eir\n\n0\n\n\u00b7\u00b7\u00b7 eir\n\n) =\n\np(ei1\n\n0,\nprk(1 \u2212 p)r,\n\nif there is any overlap in si1, . . . , sir\nif there is no overlap\n\nwe must now determine the number of different choices of i1 < \u00b7\u00b7\u00b7 < ir < n \u2212 k + 1\nfor which there is no overlap in the sets si1, . . . , sir. to do so, note first that each of\nthe sij, j = 1, . . . , r, refer to k + 1 flips, so, without any overlap, they together refer\nto r(k + 1) flips. now consider any permutation of r identical letters a (one for each\n) and of n \u2212 r(k + 1) identical letters b (one for each of the\nof the sets si1, . . . , sir\u22121\ntrials that are not part of any of si1, . . . , sir\u22121, sn\u2212k+1). interpret the number of b\u2019s\nbefore the first a as the number of flips before si1, the number of b\u2019s between the first\nand second a as the number of flips between si1 and si2, and so on, with the number\nof b\u2019s after the final a representing the number of flips after sir. because there are\nn\u2212rk\npermutations of r letters a and of n \u2212 r(k + 1) letters b, with every such\nr\npermutation corresponding (in a one-to-one fashion) to a different nonoverlapping\n\n(cid:19)\n\n(cid:18)\nchoice, it follows that(cid:6)\n\n(cid:2)\n\n(cid:3)\nn \u2212 rk\n\n\u00b7\u00b7\u00b7 eir\n\n) =\n\np(ei1\n\nprk(1 \u2212 p)r\n\nr\n\ni1<\u00b7\u00b7\u00b7<ir<n\u2212k+1\n\nwe must now consider r-way intersection probabilities of the form\n\np(ei1\n\n\u00b7\u00b7\u00b7 eir\u22121en\u2212k+1),\n\nwhere i1 < \u00b7\u00b7\u00b7 < ir\u22121 < n \u2212 k + 1. now, this probability will equal 0 if there is any\noverlap in si1, . . . , sir\u22121, sn\u2212k; if there is no overlap, then the events of the intersection\nwill be independent, so\n\np(ei1\n\n\u00b7\u00b7\u00b7 eir\u22121en\u2212k+1) = [pk(1 \u2212 p)]r\u22121pk = pkr(1 \u2212 p)r\u22121\n\nby a similar argument as before, the number of nonoverlapping sets si1, . . . , sir\u22121, sn\u2212k\nwill equal the number of permutations of r \u2212 1 letters a (one for each of the sets\n\n "}, {"Page_number": 166, "text": "the poisson random variable 151\n) and of n \u2212 (r \u2212 1)(k + 1) \u2212 k = n \u2212 rk \u2212 (r \u2212 1) letters b (one\nsi1, . . . , sir\u22121\nfor each of the trials that are not part of any of si1, . . . , sir\u22121, sn\u2212k+1). since there are\nn\u2212rk\nr\u22121\n\npermutations of r \u2212 1 letters a and of n \u2212 rk \u2212 (r \u2212 1) letters b, we have\n\nsection 4.7\n\n(cid:19)\n\n(cid:18)\n\nputting it all together yields the exact expression, namely,\n\n(cid:6)\n\ni1<...<ir\u22121<n\u2212k+1\n\np(ei1\n\np(ln \u00fa k) = n\u2212k+1(cid:6)\n\nr=1\n\n(\u22121)r+1\n\n(cid:2)\n\n(cid:3)\nn \u2212 rk\nr \u2212 1\n(cid:2)\nn \u2212 rk\nr \u2212 1\n(cid:19) = 0 if m < j.\n\n\u00b7\u00b7\u00b7 eir\u22121en\u2212k+1) =\n(cid:7)(cid:2)\n(cid:3)\nn \u2212 rk\n(cid:18)\n\n+ 1\np\n\nr\n\nm\nj\n\npkr(1 \u2212 p)r\u22121\n(cid:3)(cid:8)\n\npkr(1 \u2212 p)r\n\nwhere we utilize the convention that\n\nfrom a computational point of view, a more efficient method for computing the\ndesired probability than the use of the preceding identity is to derive a set of recur-\nsive equations. to do so, let an be the event that there is a string of k consecutive\nheads in a sequence of n flips of a fair coin, and let pn = p(an). we will derive a\nset of recursive equations for pn by conditioning on when the first tail appears. for\nj = 1, . . . , k, let fj be the event that the first tail appears on flip j, and let h be the\nevent that the first k flips are all heads. because the events f1, . . . , fk, h are mutually\nexclusive and exhaustive (that is, exactly one of these events must occur), we have\n\np(an) = k(cid:6)\n\nj=1\n\np(an|fj)p(fj) + p(an|h)p(h)\n\nnow, given that the first tail appears on flip j, where j < k, it follows that those j\nflips are wasted as far as obtaining a string of k heads in a row; thus, the conditional\nprobability of this event is the probability that such a string will occur among the\nremaining n \u2212 j flips. therefore,\n\np(an|fj) = pn\u2212j\nbecause p(an|h) = 1, the preceding equation gives\n\npn = p(an)\n\n= k(cid:6)\n= k(cid:6)\n\nj=1\n\nj=1\n\npn\u2212j p(fj) + p(h)\n\npn\u2212j p j\u22121(1 \u2212 p) + pk\n\nstarting with pj = 0, j < k, and pk = pk, we can use the latter formula to recursively\ncompute pk+1, pk+2, and so on, up to pn. for instance, suppose we want the prob-\nability that there is a run of 2 consecutive heads when a fair coin is flipped 4 times.\nthen, with k = 2, we have p1 = 0, p2 = (1/2)2. because, when p = 1/2, the recursion\nbecomes\n\npn = k(cid:6)\n\nj=1\n\npn\u2212j (1/2)j + (1/2)k\n\n "}, {"Page_number": 167, "text": "152\n\nchapter 4\n\nrandom variables\n\nwe obtain\n\nand\n\np3 = p2(1/2) + p1(1/2)2 + (1/2)2 = 3/8\n\np4 = p3(1/2) + p2(1/2)2 + (1/2)2 = 1/2\n\nwhich is clearly true because there are 8 outcomes that result in a string of 2 consec-\nutive heads: hhhh, hhht, hhth, hthh, thhh, hhtt, thht, and tthh. each of these outcomes\n.\noccurs with probability 1/16.\n\nanother use of the poisson probability distribution arises in situations where\n\u201cevents\u201d occur at certain points in time. one example is to designate the occurrence\nof an earthquake as an event; another possibility would be for events to correspond\nto people entering a particular establishment (bank, post office, gas station, and so\non); and a third possibility is for an event to occur whenever a war starts. let us sup-\npose that events are indeed occurring at certain (random) points of time, and let us\nassume that, for some positive constant \u03bb, the following assumptions hold true:\n\n1. the probability that exactly 1 event occurs in a given interval of length h is equal\nf (h)/h = 0.\n\nto \u03bbh + o(h), where o(h) stands for any function f (h) for which lim\nh\u21920\n[for instance, f (h) = h2 is o(h), whereas f (h) = h is not.]\n\n2. the probability that 2 or more events occur in an interval of length h is equal\n\nto o(h).\n\n3. for any integers n, j1, j2, . . . , jn and any set of n nonoverlapping intervals, if we\ndefine ei to be the event that exactly ji of the events under consideration occur\nin the ith of these intervals, then events e1, e2, . . . , en are independent.\n\nloosely put, assumptions 1 and 2 state that, for small values of h, the probability\nthat exactly 1 event occurs in an interval of size h equals \u03bbh plus something that is\nsmall compared with h, whereas the probability that 2 or more events occur is small\ncompared with h. assumption 3 states that whatever occurs in one interval has no\n(probability) effect on what will occur in other, nonoverlapping intervals.\n\nwe now show that, under assumptions 1, 2, and 3, the number of events occurring\nin any interval of length t is a poisson random variable with parameter \u03bbt. to be\nprecise, let us call the interval [0, t] and denote the number of events occurring in that\ninterval by n(t). to obtain an expression for p{n(t) = k}, we start by breaking the\ninterval [0, t] into n nonoverlapping subintervals, each of length t/n (figure 4.7).\n\nnow,\n\np{n(t) = k} = p{k of the n subintervals contain exactly 1 event\n+ p{n(t) = k and at least 1 subinterval contains\n\nand the other n \u2212 k contain 0 events}\n2 or more events}\n\n(7.2)\n\nthe proceding equation holds because the event on the left side of equation (7.2),\nthat is, {n(t) = k}, is clearly equal to the union of the two mutually exclusive events\n\n0\n\nt\u2013\nn\n\n2t\u2014\nn\n\n3t\u2014\nn\n\nt =\n\nnt\u2014\nn\n\n(n \u2013 1)\n\nt\u2013\nn\n\nfigure 4.7\n\n "}, {"Page_number": 168, "text": "section 4.7\n\nthe poisson random variable 153\n\non the right side of the equation. letting a and b denote the two mutually exclusive\nevents on the right side of equation (7.2), we have\n\n\u239e\np(b) \u2026 p{at least one subinterval contains 2 or more events}\n\u23a0\n{ith subinterval contains 2 or more events}\n\n(cid:3)\n\ni=1\np{ith subinterval contains 2 or more events}\n(cid:2)\n(cid:3)\n\nt\nn\n\nby boole\u2019s\ninequality\n\nby assumption 2\n\n\u2026\n\n\u239b\n\u239d n(cid:14)\n= p\nn(cid:6)\n= n(cid:6)\n(cid:2)\ni=1\n(cid:20)\n= no\n= t\n\ni=1\n\nt\nn\n\no\n\n(cid:21)\n\no(t/n)\n\nt/n\n\nnow, in addition for any t, t/n\u21920 as n\u2192q, so o(t/n)/(t/n)\u21920 as n\u2192q, by the defini-\ntion of o(h). hence,\n\np(b)\u21920 as n\u2192q\n\n(7.3)\n\nmoreover, since assumptions 1 and 2 imply that\u2020\n\np{0 events occur in an interval of length h}\n\n= 1 \u2212 [\u03bbh + o(h) + o(h)] = 1 \u2212 \u03bbh \u2212 o(h)\n\nwe see from the independence assumption (number 3) that\n\np(a) = p{k of the subintervals contain exactly 1 event and the other\n\n=\n\nhowever, since\n\n(cid:2)\n\n(cid:3)(cid:7)\n(cid:3)(cid:8)k(cid:7)\n(cid:2)\nn \u2212 k contain 0 events}\n1 \u2212\nn\nk\n(cid:20)\n\n+ o\n(cid:3)(cid:8)\n(cid:2)\n\n\u03bbt\nn\n\n(cid:7)\n\nt\nn\n\nn\n\n+ o\n\n\u03bbt\nn\n\nt\nn\n\n= \u03bbt + t\n\no(t/n)\n\nt/n\n\n(cid:2)\n\n(cid:3)\n\n\u03bbt\nn\n\n(cid:2)\n\n(cid:3)(cid:8)n\u2212k\n\n\u2212 o\n\nt\nn\n\n(cid:21)\n\n\u2192\u03bbt\n\nas n\u2192q\n\nit follows, by the same argument that verified the poisson approximation to the bino-\nmial, that\n\np(a)\u2192e\n\n\u2212\u03bbt\n\n(\u03bbt)k\nk!\n\nas n\u2192q\n\nthus, from equations (7.2), (7.3), and (7.4), by letting n\u2192q, we obtain\n\np{n(t) = k} = e\n\n\u2212\u03bbt\n\n(\u03bbt)k\nk!\n\nk = 0, 1, . . .\n\n(7.4)\n\n(7.5)\n\n\u2020the sum of two functions, both of which are o(h),\n\nlimh\u21920 f (h)/h = limh\u21920 g(h)/h = 0, then limh\u21920[f (h) + g(h)]/h = 0.\n\nis also o(h). this is so because if\n\n "}, {"Page_number": 169, "text": "154\n\nchapter 4\n\nrandom variables\n\nhence, if assumptions 1, 2, and 3 are satisfied, then the number of events occurring\nin any fixed interval of length t is a poisson random variable with mean \u03bbt, and we say\nthat the events occur in accordance with a poisson process having rate \u03bb. the value \u03bb,\nwhich can be shown to equal the rate per unit time at which events occur, is a constant\nthat must be empirically determined.\n\nthe preceding discussion explains why a poisson random variable is usually a good\n\napproximation for such diverse phenomena as the following:\n\n1. the number of earthquakes occurring during some fixed time span\n2. the number of wars per year\n3. the number of electrons emitted from a heated cathode during a fixed time\n\nperiod\n\n4. the number of deaths, in a given period of time, of the policyholders of a life\n\ninsurance company\n\nexample 7e\nsuppose that earthquakes occur in the western portion of the united states in accor-\ndance with assumptions 1, 2, and 3, with \u03bb = 2 and with 1 week as the unit of time.\n(that is, earthquakes occur in accordance with the three assumptions at a rate of 2\nper week.)\n(a) find the probability that at least 3 earthquakes occur during the next 2 weeks.\n(b) find the probability distribution of the time, starting from now, until the next\n\nearthquake.\n\nsolution. (a) from equation (7.5), we have\n\np{n(2) \u00fa 3} = 1 \u2212 p{n(2) = 0} \u2212 p{n(2) = 1} \u2212 p{n(2) = 2}\n\n\u22124 \u2212 4e\n\n= 1 \u2212 e\n= 1 \u2212 13e\n\u22124\n\n\u22124 \u2212 42\n2\n\n\u22124\ne\n\n(b) let x denote the amount of time (in weeks) until the next earthquake. because\nx will be greater than t if and only if no events occur within the next t units of time,\nwe have, from equation (7.5),\n\np{x > t} = p{n(t) = 0} = e\n\n\u2212\u03bbt\n\nso the probability distribution function f of the random variable x is given by\n\nf(t) = p{x \u2026 t} = 1 \u2212 p{x > t} = 1 \u2212 e\n= 1 \u2212 e\n\n\u2212\u03bbt\n\u22122t\n\n4.7.1 computing the poisson distribution function\nif x is poisson with parameter \u03bb, then\n\np{x = i + 1}\np{x = i} = e\n\n\u2212\u03bb\u03bbi+1/(i + 1)!\n\ne\u2212\u03bb\u03bbi/i!\n\n= \u03bb\ni + 1\n\n.\n\n(7.6)\n\n "}, {"Page_number": 170, "text": "starting with p{x = 0} = e\n\nother discrete probability distributions 155\n\nsection 4.8\n\u2212\u03bb, we can use (7.6) to compute successively\n\np{x = 1} = \u03bbp{x = 0}\np{x = 2} = \u03bb\np{x = 1}\n2\n\n.\n.\n.\n\np{x = i + 1} = \u03bb\ni + 1\n\np{x = i}\n\nthe website includes a program that uses equation (7.6) to compute poisson prob-\n\nabilities.\n\nexample 7f\n(a) determine p{x \u2026 90} when x is poisson with mean 100.\n(b) determine p{y \u2026 1075} when y is poisson with mean 1000.\nsolution. from the website, we obtain the solutions:\n(a) p{x \u2026 90} l .1714;\n(b) p{y \u2026 1075} l .9894.\n\n.\n\n4.8 other discrete probability distributions\n\n4.8.1 the geometric random variable\nsuppose that independent trials, each having a probability p, 0 < p < 1, of being a\nsuccess, are performed until a success occurs. if we let x equal the number of trials\nrequired, then\n\np{x = n} = (1 \u2212 p)n\u22121p\n\nn = 1, 2, . . .\n\n(8.1)\n\nequation (8.1) follows because, in order for x to equal n, it is necessary and sufficient\nthat the first n \u2212 1 trials are failures and the nth trial is a success. equation (8.1) then\nfollows, since the outcomes of the successive trials are assumed to be independent.\n\nsince\n\nq(cid:6)\n\nn=1\n\nq(cid:6)\n\nn=1\n\np{x = n} = p\n\n(1 \u2212 p)n\u22121 =\n\np\n\n1 \u2212 (1 \u2212 p)\n\n= 1\n\nit follows that, with probability 1, a success will eventually occur. any random vari-\nable x whose probability mass function is given by equation (8.1) is said to be a\ngeometric random variable with parameter p.\n\nexample 8a\nan urn contains n white and m black balls. balls are randomly selected, one at a\ntime, until a black one is obtained. if we assume that each ball selected is replaced\nbefore the next one is drawn, what is the probability that\n(a) exactly n draws are needed?\n(b) at least k draws are needed?\n\nsolution. if we let x denote the number of draws needed to select a black ball, then\nx satisfies equation (8.1) with p = m/(m + n). hence,\n\n "}, {"Page_number": 171, "text": "156\n\nchapter 4\n\nrandom variables\n\n(a)\n\n(b)\n\n(cid:2)\n\nn\n\nm + n\n\np{x = n} =\n\n(cid:3)n\u22121 m\n\n= mnn\u22121\n(m + n)n\n\nm + n\n(cid:3)n\u22121\n(cid:3)k\u22121\n\nn\n\nm + n\n\n(cid:2)\nq(cid:6)\n(cid:3)(cid:2)\n(cid:3)k\u22121\nm + n\n\nn=k\n\nn\n\n1(cid:20)\n1 \u2212\n\n(cid:21)\n\nn\n\nm + n\n\np{x \u00fa k} = m\n\n(cid:2)\nm + n\nm\n(cid:2)\n\nm + n\n\nn\n\nm + n\n\n=\n\n=\n\nof course, part (b) could have been obtained directly, since the probability that at\nleast k trials are necessary to obtain a success is equal to the probability that the first\nk \u2212 1 trials are all failures. that is, for a geometric random variable,\n\np{x \u00fa k} = (1 \u2212 p)k\u22121\n\n.\n\niqi\u22121p\n\ne[x] =\n\nexample 8b\nfind the expected value of a geometric random variable.\nsolution. with q = 1 \u2212 p, we have\nq(cid:6)\nq(cid:6)\n(i \u2212 1 + 1)qi\u22121p\nq(cid:6)\nq(cid:6)\n(i \u2212 1)qi\u22121p +\nq(cid:6)\njqjp + 1\nq(cid:6)\n\ni=1\n\ni=1\n\ni=1\n\ni=1\n\n=\n\n=\n\n=\n\nj=0\n= q\n= qe[x] + 1\n\nj=1\n\njqj\u22121p + 1\n\nqi\u22121p\n\nhence,\n\nyielding the result\n\npe[x] = 1\n\ne[x] = 1\np\n\n "}, {"Page_number": 172, "text": "section 4.8\n\nother discrete probability distributions 157\n\nin other words, if independent trials having a common probability p of being success-\nful are performed until the first success occurs, then the expected number of required\ntrials equals 1/p. for instance, the expected number of rolls of a fair die that it takes\n.\nto obtain the value 1 is 6.\n\nexample 8c\nfind the variance of a geometric random variable.\nsolution. to determine var(x), let us first compute e[x2]. with q = 1 \u2212 p, we have\n\ne[x2] =\n\n=\n\n=\n\n=\n\ni=1\n\nq(cid:6)\nq(cid:6)\nq(cid:6)\nq(cid:6)\n\ni=1\n\ni=1\n\nj=0\n\ni2qi\u22121p\n\n(i \u2212 1 + 1)2qi\u22121p\nq(cid:6)\n\n(i \u2212 1)2qi\u22121p +\nq(cid:6)\n\nj2qjp + 2\n\ni=1\n\njqjp + 1\n\nj=1\n\n= qe[x2] + 2qe[x] + 1\n\n2(i \u2212 1)qi\u22121p +\n\nq(cid:6)\n\ni=1\n\nqi\u22121p\n\nusing e[x] = 1/p, the equation for e[x2] yields\n\nhence,\n\ngiving the result\n\npe[x2] = 2q\np\n\n+ 1\n\ne[x2] = 2q + p\n\np2\n\n= q + 1\np2\n\nvar(x) = q + 1\n\np2\n\n\u2212 1\np2\n\n= q\np2\n\n= 1 \u2212 p\np2\n\n.\n\n4.8.2 the negative binomial random variable\nsuppose that independent trials, each having probability p, 0 < p < 1, of being a\nsuccess are performed until a total of r successes is accumulated. if we let x equal the\nnumber of trials required, then\n\np{x = n} =\n\npr(1 \u2212 p)n\u2212r n = r, r + 1, . . .\n\n(8.2)\n\nequation (8.2) follows because, in order for the rth success to occur at the nth trial,\nthere must be r \u2212 1 successes in the first n \u2212 1 trials and the nth trial must be a\nsuccess. the probability of the first event is\n\n(cid:3)\n\n(cid:2)\n\nn \u2212 1\nr \u2212 1\n\n(cid:3)\n\n(cid:2)\n\nn \u2212 1\nr \u2212 1\n\npr\u22121(1 \u2212 p)n\u2212r\n\n "}, {"Page_number": 173, "text": "158\n\nchapter 4\n\nrandom variables\n\nand the probability of the second is p; thus, by independence, equation (8.2) is estab-\nlished. to verify that a total of r successes must eventually be accumulated, either we\ncan prove analytically that\n\nq(cid:6)\n\nn=r\n\n(cid:2)\n\nq(cid:6)\n\nn=r\n\n(cid:3)\n\nn \u2212 1\nr \u2212 1\n\np{x = n} =\n\npr(1 \u2212 p)n\u2212r = 1\n\n(8.3)\n\nor we can give a probabilistic argument as follows: the number of trials required\nto obtain r successes can be expressed as y1 + y2 + \u00b7\u00b7\u00b7 + yr, where y1 equals\nthe number of trials required for the first success, y2 the number of additional trials\nafter the first success until the second success occurs, y3 the number of additional\ntrials until the third success, and so on. because the trials are independent and all\nhave the same probability of success, it follows that y1, y2, . . . , yr are all geometric\nrandom variables. hence, each is finite with probability 1, so\nyi must also be finite,\n\nr(cid:9)\n\nestablishing equation (8.3).\n\nany random variable x whose probability mass function is given by equation (8.2)\nis said to be a negative binomial random variable with parameters (r, p). note that a\ngeometric random variable is just a negative binomial with parameter (1, p).\n\nin the next example, we use the negative binomial to obtain another solution of\n\ni=1\n\nthe problem of the points.\n\nexample 8d\nif independent trials, each resulting in a success with probability p, are performed,\nwhat is the probability of r successes occurring before m failures?\n\nsolution. the solution will be arrived at by noting that r successes will occur before\nm failures if and only if the rth success occurs no later than the (r + m \u2212 1)th\ntrial. this follows because if the rth success occurs before or at the (r + m \u2212 1)th\ntrial, then it must have occurred before the mth failure, and conversely. hence, from\nequation (8.2), the desired probability is\nn \u2212 1\nr \u2212 1\n\npr(1 \u2212 p)n\u2212r\n\nr+m\u22121(cid:6)\n\n(cid:2)\n\n(cid:3)\n\n.\n\nn=r\n\nexample 8e the banach match problem\nat all times, a pipe-smoking mathematician carries 2 matchboxes\u20141 in his left-hand\npocket and 1 in his right-hand pocket. each time he needs a match, he is equally likely\nto take it from either pocket. consider the moment when the mathematician first\ndiscovers that one of his matchboxes is empty. if it is assumed that both matchboxes\ninitially contained n matches, what is the probability that there are exactly k matches,\nk = 0, 1, . . . , n, in the other box?\nsolution. let e denote the event that the mathematician first discovers that the right-\nhand matchbox is empty and that there are k matches in the left-hand box at the\ntime. now, this event will occur if and only if the (n + 1)th choice of the right-hand\nmatchbox is made at the (n + 1 + n \u2212 k)th trial. hence, from equation (8.2) (with\np = 1\n\n2, r = n + 1, and n = 2n \u2212 k + 1), we see that\n\n(cid:2)\n\n(cid:3)(cid:2)\n\n(cid:3)2n\u2212k+1\n\np(e) =\n\n2n \u2212 k\n\nn\n\n1\n2\n\n "}, {"Page_number": 174, "text": "section 4.8\n\nother discrete probability distributions 159\n\nsince there is an equal probability that it is the left-hand box that is first discovered\nto be empty and there are k matches in the right-hand box at that time, the desired\nresult is\n\n(cid:3)(cid:2)\n\n(cid:3)2n\u2212k\n\n(cid:2)\n\n2p(e) =\n\n2n \u2212 k\n\nn\n\n1\n2\n\n.\n\nexample 8f\ncompute the expected value and the variance of a negative binomial random variable\nwith parameters r and p.\n\nsolution. we have\n\ne[xk] =\n\n(cid:3)\n(cid:3)\n\n(cid:2)\n\nnk\n\nq(cid:6)\nq(cid:6)\nnk\u22121\nq(cid:6)\n\nn=r\n\nn \u2212 1\n(cid:2)\nr \u2212 1\n\nn\nr\n\n(m \u2212 1)k\u22121\n\nm=r+1\ne[(y \u2212 1)k\u22121]\n\nn=r\n= r\np\n= r\np\n= r\np\n\npr(1 \u2212 p)n\u2212r\n\npr+1(1 \u2212 p)n\u2212r\n(cid:3)\n(cid:2)\n\nm \u2212 1\n\nr\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\nsince n\n\nn \u2212 1\nr \u2212 1\npr+1(1 \u2212 p)m\u2212(r+1)\n\n= r\n\nn\nr\nby setting\nm = n + 1\n\nwhere y is a negative binomial random variable with parameters r + 1, p. setting\nk = 1 in the preceding equation yields\n\ne[x] = r\np\n\nsetting k = 2 in the equation for e[xk] and using the formula for the expected value\nof a negative binomial random variable gives\n\ntherefore,\n\ne[x2] = r\np\n= r\np\n(cid:2)\n\n(cid:2)\ne[y \u2212 1]\nr + 1\n\np\n\n(cid:3)\n\u2212 1\n(cid:3)\n\u2212 1\n\n\u2212\n\n(cid:2)\n\nvar(x) = r\np\n\nr + 1\n= r(1 \u2212 p)\n\np\n\np2\n\n(cid:3)2\n\nr\np\n\n.\n\nthus, from example 8f, if independent trials, each of which is a success with prob-\nability p, are performed, then the expected value and variance of the number of trials\nthat it takes to amass r successes is r/p and r(1 \u2212 p)/p2, respectively.\nsince a geometric random variable is just a negative binomial with parameter\nr = 1, it follows from the preceding example that the variance of a geometric ran-\ndom variable with parameter p is equal to (1 \u2212 p)/p2, which checks with the result\nof example 8c.\n\n "}, {"Page_number": 175, "text": "160\n\nchapter 4\n\nrandom variables\n\nexample 8g\nfind the expected value and the variance of the number of times one must throw a\ndie until the outcome 1 has occurred 4 times.\n\nsolution. since the random variable of interest is a negative binomial with parame-\nters r = 4 and p = 1\n\n6, it follows that\n\n(cid:29)\ne[x] = 24\n(cid:29)\nvar(x) = 4\n\n(cid:30)\n(cid:30)2\n\n5\n6\n\n= 120\n\n.\n\n1\n6\n\n4.8.3 the hypergeometric random variable\nsuppose that a sample of size n is to be chosen randomly (without replacement) from\nan urn containing n balls, of which m are white and n \u2212 m are black. if we let x\ndenote the number of white balls selected, then\nn \u2212 m\n(cid:3)\nn \u2212 i\nn\nn\n\ni = 0, 1, . . . , n\n\np{x = i} =\n\n(cid:3)(cid:2)\n(cid:2)\n\n(8.4)\n\n(cid:2)\n\n(cid:3)\n\nm\ni\n\na random variable x whose probability mass function is given by equation (8.4) for\nsome values of n, n, m is said to be a hypergeometric random variable.\nremark. although we have written the hypergeometric probability mass func-\ntion with i going from 0 to n, p{x = i} will actually be 0, unless i satisfies the inequali-\nties n \u2212 (n \u2212 m) \u2026 i \u2026 min(n, m). however, equation (8.4) is always valid because\n.\nof our convention that\n\nis equal to 0 when either k < 0 or r < k.\n\n(cid:2)\n\n(cid:3)\n\nr\nk\n\nexample 8h\nan unknown number, say, n, of animals inhabit a certain region. to obtain some\ninformation about the size of the population, ecologists often perform the follow-\ning experiment: they first catch a number, say, m, of these animals, mark them in\nsome manner, and release them. after allowing the marked animals time to disperse\nthroughout the region, a new catch of size, say, n, is made. let x denote the number\nof marked animals in this second capture. if we assume that the population of ani-\nmals in the region remained fixed between the time of the two catches and that each\ntime an animal was caught it was equally likely to be any of the remaining uncaught\nanimals, it follows that x is a hypergeometric random variable such that\n\n(cid:2)\n\n(cid:3)(cid:2)\n(cid:2)\n\nm\ni\n\n(cid:3)\n\nn \u2212 m\n(cid:3)\nn \u2212 i\nn\nn\n\np{x = i} =\n\nk pi(n)\n\nsuppose now that x is observed to equal i. then, since pi(n) represents the proba-\nbility of the observed event when there are actually n animals present in the region, it\nwould appear that a reasonable estimate of n would be the value of n that maximizes\npi(n). such an estimate is called a maximum likelihood estimate. (see theoretical\nexercises 13 and 18 for other examples of this type of estimation procedure.)\n\n "}, {"Page_number": 176, "text": "section 4.8\n\nother discrete probability distributions 161\n\nthe maximization of pi(n) can be done most simply by first noting that\n\npi(n)\n\npi(n \u2212 1)\n\n= (n \u2212 m)(n \u2212 n)\nn(n \u2212 m \u2212 n + i)\n\nnow, the preceding ratio is greater than 1 if and only if\n\n(n \u2212 m)(n \u2212 n) \u00fa n(n \u2212 m \u2212 n + i)\n\nor, equivalently, if and only if\n\nn \u2026 mn\ni\n\nthus, pi(n) is first increasing and then decreasing, and reaches its maximum value\nat the largest integral value not exceeding mn/i. this value is the maximum likeli-\nhood estimate of n. for example, suppose that the initial catch consisted of m =\n50 animals, which are marked and then released. if a subsequent catch consists of\nn = 40 animals of which i = 4 are marked, then we would estimate that there are\nsome 500 animals in the region. (note that the preceding estimate could also have\nbeen obtained by assuming that the proportion of marked animals in the region,\nm/n, is approximately equal to the proportion of marked animals in our second\n.\ncatch, i/n.)\n\nexample 8i\na purchaser of electrical components buys them in lots of size 10. it is his policy\nto inspect 3 components randomly from a lot and to accept the lot only if all 3 are\nnondefective. if 30 percent of the lots have 4 defective components and 70 percent\nhave only 1, what proportion of lots does the purchaser reject?\n\nsolution. let a denote the event that the purchaser accepts a lot. now,\n(cid:3)(cid:2)\n+ p(a|lot has 1 defective)\n9\n3\n\n(cid:2)\np(a) = p(a|lot has 4 defectives)\n3\n10\n1\n0\n\n(cid:3)(cid:2)\n\n(cid:2)\n\n(cid:3)\n\n(cid:3)\n\n4\n0\n\n6\n3\n\n(cid:3)\n(cid:3) (cid:2)\n\n(cid:3)\n(cid:3) (cid:2)\n\n(cid:2)\n\n(cid:2)\n\n=\n\n+\n\n7\n10\n\n3\n10\n\n10\n3\n\n7\n10\n\n10\n3\n\n= 54\n100\n\nhence, 46 percent of the lots are rejected.\n\n.\n\nif n balls are randomly chosen without replacement from a set of n balls of which\nthe fraction p = m/n is white, then the number of white balls selected is hypergeo-\nmetric. now, it would seem that when m and n are large in relation to n, it shouldn\u2019t\nmake much difference whether the selection is being done with or without replace-\nment, because, no matter which balls have previously been selected, when m and n\nare large, each additional selection will be white with a probability approximately\nequal to p. in other words, it seems intuitive that when m and n are large in relation\nto n, the probability mass function of x should approximately be that of a binomial\nrandom variable with parameters n and p. to verify this intuition, note that if x is\nhypergeometric, then, for i \u2026 n,\n\n "}, {"Page_number": 177, "text": "162\n\nchapter 4\n\nrandom variables\n\np{x = i} =\n\n=\n\n=\n\nl\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)(cid:2)\n(cid:2)\n\nm\ni\n\nn \u2212 m\n(cid:3)\nn \u2212 i\nn\nn\n(cid:2)\n(m \u2212 i)! i!\nn\ni\n\nm \u2212 1\nn \u2212 1\n\n(cid:3)\n\nm\nn\n\nm!\n\n(n \u2212 m)!\n\u00b7\u00b7\u00b7 m \u2212 i + 1\nn \u2212 i + 1\n\n(n \u2212 m \u2212 n + i)! (n \u2212 i)!\nn \u2212 m\nn \u2212 i\n\n(n \u2212 n)! n!\nn \u2212 m \u2212 1\nn \u2212 i \u2212 1\n\nn!\n\n\u00b7\u00b7\u00b7 n \u2212 m \u2212 (n \u2212 i \u2212 1)\n(cid:3)\n(cid:2)\nn \u2212 i \u2212 (n \u2212 i \u2212 1)\nn\ni\n\npi(1 \u2212 p)n\u2212i\n\nwhen p = m/n and m and n are\n\nlarge in relation to n and i\n\nexample 8j\ndetermine the expected value and the variance of x, a hypergeometric random vari-\nable with parameters n, n, and m.\n\nsolution.\n\ni=0\n\ne[xk] = n(cid:6)\n= n(cid:6)\n(cid:2)\n\ni=1\n\n(cid:3)\n\n(cid:3)\n\nm \u2212 1\ni \u2212 1\n(cid:2)\n\n= m\nn(cid:6)\nn\u22121(cid:6)\n(j + 1)k\u22121\n\nik\u22121\n\ni=1\n\nm \u2212 1\ni \u2212 1\n(cid:2)\n\nj=0\ne[(y + 1)k\u22121]\n\n(cid:2)\n\nm\ni\n\n= nm\nn\n= nm\nn\n\nikp{x = i}\n(cid:2)\n(cid:3)(cid:2)\n\nik\n\nm\ni\n\n(cid:3)1(cid:2)\n\n(cid:3)\n\nn\nn\n\nn \u2212 m\nn \u2212 i\n(cid:2)\n\n(cid:3)\n\nn\nn\n\n= n\n(cid:3)1(cid:2)\n\nand n\n\n(cid:3)(cid:2)\n\nn \u2212 m\nn \u2212 i\n(cid:3)(cid:2)\n\nm \u2212 1\n\nj\n\nn \u2212 m\nn \u2212 1 \u2212 j\n\n(cid:3)\n\n(cid:2)\n\nn \u2212 1\nn \u2212 1\n(cid:3)\nn \u2212 1\n(cid:3)1(cid:2)\nn \u2212 1\n\n(cid:3)\n\nn \u2212 1\nn \u2212 1\n\nusing the identities\n\ni\n\nwe obtain\n\ne[xk] = nm\nn\n\nwhere y is a hypergeometric random variable with parameters n \u2212 1, n \u2212 1, and\nm \u2212 1. hence, upon setting k = 1, we have\n\ne[x] = nm\nn\n\nin words, if n balls are randomly selected from a set of n balls, of which m are white,\nthen the expected number of white balls selected is nm/n.\n\n "}, {"Page_number": 178, "text": "section 4.8\n\nother discrete probability distributions 163\n\nupon setting k = 2 in the equation for e[xk], we obtain\n\ne[x2] = nm\nn\n= nm\nn\n\n(cid:20)\ne[y + 1]\n(n \u2212 1)(m \u2212 1)\n\nn \u2212 1\n\n(cid:21)\n\n+ 1\n\nwhere the final equality uses our preceding result to compute the expected value of\nthe hypergeometric random variable y.\n\nbecause e[x] = nm/n, we can conclude that\n\n(cid:20)\n\n(cid:21)\n\nvar(x) = nm\nn\n\n(n \u2212 1)(m \u2212 1)\n\nn \u2212 1\n\n+ 1 \u2212 nm\nn\n\nletting p = m/n and using the identity\n\nm \u2212 1\nn \u2212 1\n\n= np \u2212 1\nn \u2212 1\n\n= p \u2212 1 \u2212 p\nn \u2212 1\n\nshows that\n\nvar(x) = np[(n \u2212 1)p \u2212 (n \u2212 1)\n= np(1 \u2212 p)(1 \u2212 n \u2212 1\nn \u2212 1\n\n)\n\n1 \u2212 p\nn \u2212 1\n\n+ 1 \u2212 np]\n\n.\n\nremark. we have shown in example 8j that if n balls are randomly selected with-\nout replacement from a set of n balls, of which the fraction p are white, then the\nexpected number of white balls chosen is np. in addition, if n is large in relation to n\n[so that (n \u2212 n)/(n \u2212 1) is approximately equal to 1], then\n\nvar(x) l np(1 \u2212 p)\n\nin other words, e[x] is the same as when the selection of the balls is done with\nreplacement (so that the number of white balls is binomial with parameters n\nand p), and if the total collection of balls is large, then var(x) is approximately equal\nto what it would be if the selection were done with replacement. this is, of course,\nexactly what we would have guessed, given our earlier result that when the number\nof balls in the urn is large, the number of white balls chosen approximately has the\n.\nmass function of a binomial random variable.\n\n4.8.4 the zeta (or zipf) distribution\na random variable is said to have a zeta (sometimes called the zipf) distribution if\nits probability mass function is given by\n\nfor some value of \u03b1 > 0. since the sum of the foregoing probabilities must equal 1, it\nfollows that\n\np{x = k} = c\nk\u03b1+1\n(cid:2)\n\n\u23a1\n\u23a3 q(cid:6)\n\nc =\n\nk = 1, 2, . . .\n\u23a4\n(cid:3)\u03b1+1\n\u23a6\u22121\n\n1\nk\n\nk=1\n\n "}, {"Page_number": 179, "text": "164\n\nchapter 4\n\nrandom variables\n\nthe zeta distribution owes its name to the fact that the function\n\n(cid:2)\n\n1\n2\n\n(cid:2)\n\n(cid:3)s +\n\n(cid:3)s + \u00b7\u00b7\u00b7 +\n\n(cid:2)\n\n(cid:3)s + \u00b7\u00b7\u00b7\n\n1\nk\n\n1\n3\n\n\u03b6 (s) = 1 +\n\nis known in mathematical disciplines as the riemann zeta function (after the german\nmathematician g. f. b. riemann).\n\nthe zeta distribution was used by the italian economist v. pareto to describe the\ndistribution of family incomes in a given country. however, it was g. k. zipf who\napplied zeta distribution to a wide variety of problems in different areas and, in doing\nso, popularized its use.\n\n4.9 expected value of sums of random variables\n\na very important property of expectations is that the expected value of a sum of\nrandom variables is equal to the sum of their expectations. in this section, we will\nprove this result under the assumption that the set of possible values of the proba-\nbility experiment\u2014that is, the sample space s\u2014is either finite or countably infinite.\nalthough the result is true without this assumption (and a proof is outlined in the\ntheoretical exercises), not only will the assumption simplify the argument, but it will\nalso result in an enlightening proof that will add to our intuition about expectations.\nso, for the remainder of this section, suppose that the sample space s is either a finite\nor a countably infinite set.\nfor a random variable x, let x(s) denote the value of x when s \u2208 s is the outcome\nof the experiment. now, if x and y are both random variables, then so is their sum.\nthat is, z = x + y is also a random variable. moreover, z(s) = x(s) + y(s).\n\nexample 9a\nsuppose that the experiment consists of flipping a coin 5 times, with the outcome\nbeing the resulting sequence of heads and tails. suppose x is the number of heads in\nthe first 3 flips and y is the number of heads in the final 2 flips. let z = x + y. then,\nfor instance, for the outcome s = (h, t, h, t, h),\n\nx(s) = 2\ny(s) = 1\nz(s) = x(s) + y(s) = 3\n\nmeaning that the outcome (h, t, h, t, h) results in 2 heads in the first three flips, 1 head\n.\nin the final two flips, and a total of 3 heads in the five flips.\nlet p(s) = p({s}) be the probability that s is the outcome of the experiment.\nbecause we can write any event a as the finite or countably infinite union of the\nmutually exclusive events {s}, s \u2208 a, it follows by the axioms of probability that\n\np(a) =\n(cid:6)\nwhen a = s, the preceding equation gives\n\n(cid:6)\n\ns\u2208a\n\np(s)\n\n1 =\n\np(s)\n\ns\u2208s\n\n "}, {"Page_number": 180, "text": "section 4.9\n\nexpected value of sums of random variables 165\n\nnow, let x be a random variable, and consider e[x]. because x(s) is the value of x\nwhen s is the outcome of the experiment, it seems intuitive that e[x]\u2014the weighted\naverage of the possible values of x, with each value weighted by the probability that\nx assumes that value\u2014should equal a weighted average of the values x(s), s \u2208 s,\nwith x(s) weighted by the probability that s is the outcome of the experiment. we\nnow prove this intuition.\nproposition 9.1.\n\n(cid:6)\n\ne[x] =\n\nx(s) p(s)\n\ns\u2208s\n\nproof. suppose that the distinct values of x are xi, i \u00fa 1. for each i, let si be the\nevent that x is equal to xi. that is, si = {s : x(s) = xi}. then,\n\ne[x] =\n=\n\n=\n\n=\n\n=\n\n=\n\ni\n\ni\n\n(cid:6)\n(cid:6)\n(cid:6)\n(cid:6)\n(cid:6)\n(cid:6)\n\ni\n\ni\n\ni\n\ns\u2208s\n\nxip(si)\n\nxip{x = xi}\n(cid:6)\n(cid:6)\n(cid:6)\n\ns\u2208si\nxip(s)\n\ns\u2208si\n\np(s)\n\nxi\n\nx(s)p(s)\n\ns\u2208si\nx(s)p(s)\n\nwhere the final equality follows because s1, s2, . . . are mutually exclusive events\nwhose union is s.\n\nexample 9b\nsuppose that two independent flips of a coin that comes up heads with probability p\nare made, and let x denote the number of heads obtained. because\n\np(x = 0) = p(t, t) = (1 \u2212 p)2,\np(x = 1) = p(h, t) + p(t, h) = 2p(1 \u2212 p)\np(x = 2) = p(h, h) = p2\n\nit follows from the definition of expected value that\n\ne[x] = 0 \u00b7 (1 \u2212 p)2 + 1 \u00b7 2p(1 \u2212 p) + 2 \u00b7 p2 = 2p\n\nwhich agrees with\ne[x] = x(h, h)p2 + x(h, t)p(1 \u2212 p) + x(t, h)(1 \u2212 p)p + x(t, t)(1 \u2212 p)2\n\n= 2p2 + p(1 \u2212 p) + (1 \u2212 p)p\n= 2p\n\n.\n\nwe now prove the important and useful result that the expected value of a sum of\nrandom variables is equal to the sum of their expectations.\n\n "}, {"Page_number": 181, "text": "166\n\nchapter 4\n\nrandom variables\n\ncorollary 9.2. for random variables x1, x2, . . . , xn,\n\n\u23a1\n\u23a3 n(cid:6)\n\ni=1\n\n\u23a4\n\u23a6 = n(cid:6)\n\ni=1\n\ne\n\nxi\n\ne[xi]\n\nproof. let z =(cid:9)\n(cid:6)\n(cid:6)\n(cid:6)\n\ne[z] =\n=\n\ns\u2208s\n\nn\ni=1 xi. then, by proposition 9.1,\n\n(cid:19)\n\n(cid:18)\nz(s)p(s)\nx1(s) + x2(s) + . . . + xn(s)\nx1(s)p(s) +\n\np(s)\nx2(s)p(s) + . . . +\n\n(cid:6)\n\n=\n= e[x1] + e[x2] + . . . + e[xn]\n\ns\u2208s\n\ns\u2208s\n\ns\u2208s\n\n(cid:6)\n\nxn(s)p(s)\n\ns\u2208s\n\n.\n\nexample 9c\nfind the expected value of the sum obtained when n fair dice are rolled.\n\nsolution. let x be the sum. we will compute e[x] by using the representation\n\nx = n(cid:6)\n\ni=1\n\nxi\n\nwhere xi is the upturned value on die i. because xi is equally likely to be any of the\nvalues from 1 to 6, it follows that\n\nwhich yields the result\n\ne[xi] = 6(cid:6)\n\u23a1\n\u23a3 n(cid:6)\n\ni=1\n\ne[x] = e\n\ni(1/6) = 21/6 = 7/2\n\u23a4\n\u23a6 = n(cid:6)\n\ne[xi] = 3.5 n\n\nxi\n\ni=1\n\ni=1\n\n.\n\nexample 9d\nfind the expected total number of successes that result from n trials when trial i is a\nsuccess with probability pi, i = 1, . . . , n.\nsolution. letting\n\n%\n\nxi =\n\nwe have the representation\n\n1,\n0,\n\nif trial i is a success\nif trial i is a failure\n\nx = n(cid:6)\n\ni=1\n\nxi\n\n "}, {"Page_number": 182, "text": "section 4.9\n\nexpected value of sums of random variables 167\n\nconsequently,\n\ne[x] = n(cid:6)\n\ni=1\n\ne[xi] = n(cid:6)\n\ni=1\n\npi\n\nnote that this result does not require that the trials be independent. it includes as a\nspecial case the expected value of a binomial random variable, which assumes inde-\npendent trials and all pi = p, and thus has mean np. it also gives the expected value\nof a hypergeometric random variable representing the number of white balls selected\nwhen n balls are randomly selected, without replacement, from an urn of n balls of\nwhich m are white. we can interpret the hypergeometric as representing the number\nof successes in n trials, where trial i is said to be a success if the ith ball selected is\nwhite. because the ith ball selected is equally likely to be any of the n balls and thus\nhas probability m/n of being white, it follows that the hypergeometric is the num-\nber of successes in n trials in which each trial is a success with probability p = m/n.\nhence, even though these hypergeometric trials are dependent, it follows from the\nresult of example 9d that the expected value of the hypergeometric is np = nm/n. .\n\nexample 9e\nderive an expression for the variance of the number of successful trials in example\n9d, and apply it to obtain the variance of a binomial random variable with parameters\nn and p, and of a hypergeometric random variable equal to the number of white balls\nchosen when n balls are randomly chosen from an urn containing n balls of which m\nare white.\n\ntation for x\u2014namely, x =(cid:9)\n\nsolution. letting x be the number of successful trials, and using the same represen-\n\nn\ni=1 xi\u2014as in the previous example, we have\n\ne[x2] = e\n\n\u239e\n\u239f\u23a0\n\n\u23a4\n\u23a5\u23a5\u23a6\n\u239e\n\u23a0\n\nxj\n\nxixj\n\n\u23a4\n\u23a5\u23a6\n\u23a4\n\u23a6\n\ne[xixj]\n\nxj\n\nxi\n\nxi\n\ni=1\n\nj=1\n\n= e\n\n\u23a1\n\u239b\n\u239d n(cid:6)\n\u23a2\u23a2\u23a3\n\u23a1\n\u23a2\u23a3 n(cid:6)\n\u23a1\n\u23a3 n(cid:6)\n= n(cid:6)\n(cid:6)\n\n\u239b\n\u239e\n\u239c\u239d n(cid:6)\n\u23a0\n\u239b\n(cid:6)\n\u239dxi +\n(cid:6)\n+ n(cid:6)\n(cid:6)\ni ] + n(cid:6)\n(cid:6)\npi + n(cid:6)\n\n= e\n\ne[x2\n\nx2\ni\n\ni=1\n\ni=1\n\ni=1\n\ni=1\n\ni=1\n\n=\n\njzi\n\njzi\n\njzi\n\ne[xixj]\n\ni=1\n\njzi\n\ni\n\n%\n\nxixj =\n\nif xi = 1, xj = 1\n\n1,\n0, otherwise\n\nwhere the final equation used that x2\ni\nboth xi and xj are 0 or 1, it follows that\n\n= xi. however, because the possible values of\n\n(9.1)\n\n "}, {"Page_number": 183, "text": "168\n\nchapter 4\n\nrandom variables\n\nhence,\n\ne[xixj] = p{xi = 1, xj = 1} = p(trials i and j are successes)\n\nnow, on the one hand, if x is binomial, then, for i z j, the results of trial i and trial j\nare independent, with each being a success with probability p. therefore,\n\ne[xixj] = p2,\n\ni z j\n\ntogether with equation (9.1), the preceding equation shows that, for a binomial ran-\ndom variable x,\n\ne[x2] = np + n(n \u2212 1)p2\n\nimplying that\n\nvar(x) = e[x2] \u2212 (e[x])2 = np + n(n \u2212 1)p2 \u2212 n2p2 = np(1 \u2212 p)\n\non the other hand, if x is hypergeometric, then, given that a white ball is chosen\nin trial i, each of the other n \u2212 1 balls, of which m \u2212 1 are white, is equally likely to\nbe the jth ball chosen, for j z i. consequently, for j z i,\n\np{xi = 1, xj = 1} = p{xi = 1}p{xj = 1|xi = 1} = m\nn\n\nm \u2212 1\nn \u2212 1\n\nusing pi = m/n, we now obtain, from equation (9.1),\nm\nn\n\ne[x2] = nm\nn\n\n+ n(n \u2212 1)\n\nconsequently,\n\nm \u2212 1\nn \u2212 1\nwhich, as shown in example 8j, can be simplified to yield\n\nvar(x) = nm\nn\n\n+ n(n \u2212 1)\n\nm\nn\n\n\u2212\n\nm \u2212 1\nn \u2212 1\n(cid:2)\n\nvar(x) = np(1 \u2212 p)\n\nwhere p = m/n.\n\n(cid:3)\n\n(cid:2)\n1 \u2212 n \u2212 1\nn \u2212 1\n\n(cid:3)2\n\nnm\nn\n\n.\n\n4.10 properties of the cumulative distribution function\n\nrecall that, for the distribution function f of x, f(b) denotes the probability that the\nrandom variable x takes on a value that is less than or equal to b. following are some\nproperties of the cumulative distribution function (c.d.f.) f:\n\nlim\n\n1. f is a nondecreasing function; that is, if a < b, then f(a) \u2026 f(b).\nb\u2192q f(b) = 1.\n2.\nb\u2192\u2212q f(b) = 0.\n3.\n4. f is right continuous. that is, for any b and any decreasing sequence bn, n \u00fa 1,\n\nlim\n\nthat converges to b, lim\n\nn\u2192q f(bn) = f(b).\n\nproperty 1 follows, as was noted in section 4.1, because, for a < b, the event\n{x \u2026 a} is contained in the event {x \u2026 b} and so cannot have a larger probabil-\nity. properties 2, 3, and 4 all follow from the continuity property of probabilities\n\n "}, {"Page_number": 184, "text": "section 4.10\n\nproperties of the cumulative distribution function 169\n(section 2.6). for instance, to prove property 2, we note that if bn increases to q, then\nthe events {x \u2026 bn}, n \u00fa 1, are increasing events whose union is the event {x < q}.\nhence, by the continuity property of probabilities,\n\nn\u2192q p{x \u2026 bn} = p{x < q} = 1\n\nlim\n\nwhich proves property 2.\nthe proof of property 3 is similar and is left as an exercise. to prove property 4,\nwe note that if bn decreases to b, then {x \u2026 bn}, n \u00fa 1, are decreasing events whose\nintersection is {x \u2026 b}. the continuity property then, yields\n\np{x \u2026 bn} = p{x \u2026 b}\n\nlim\nn\n\nwhich verifies property 4.\n\nall probability questions about x can be answered in terms of the c.d.f., f. for\n\nexample,\n\np{a < x \u2026 b} = f(b) \u2212 f(a)\n\n(8.1)\nthis equation can best be seen to hold if we write the event {x \u2026 b} as the union of\nthe mutually exclusive events {x \u2026 a} and {a < x \u2026 b}. that is,\n\nfor all a < b\n\n{x \u2026 b} = {x \u2026 a} \u222a {a < x \u2026 b}\n\nso\n\np{x \u2026 b} = p{x \u2026 a} + p{a < x \u2026 b}\n\nif we want to compute the probability that x is strictly less than b, we can again\n\nwhich establishes equation (9.1).\n\napply the continuity property to obtain\np{x < b} = p\n\n(cid:4)\n\n/(cid:5)\n%\nx \u2026 b \u2212 1\n(cid:3)\nn\n\n(cid:2)\nn\u2192q\nlim\n(cid:3)\n(cid:2)\nx \u2026 b \u2212 1\nn\nb \u2212 1\nn\n\n= lim\n= lim\n\nn\u2192q p\nn\u2192q f\n\nnote that p{x < b} does not necessarily equal f(b), since f(b) also includes the\nprobability that x equals b.\n\nexample 10a\nthe distribution function of the random variable x is given by\n\n\u23a7\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9\n\n0\nx\n2\n2\n3\n11\n12\n1\n\nx < 0\n0 \u2026 x < 1\n\n1 \u2026 x < 2\n\n2 \u2026 x < 3\n3 \u2026 x\n\nf(x) =\n\n "}, {"Page_number": 185, "text": "170\n\nchapter 4\n\nrandom variables\n\nf(x)\n\n1\n11\u2014\n12\n\n2\u2013\n3\n1\u2013\n2\n\n1\n\n2\n\n3\n\nx\n\nfigure 4.8: graph of f(x).\n\na graph of f(x) is presented in figure 4.8. compute (a) p{x < 3}, (b) p{x = 1}, (c)\np{x > 1\n\n(cid:3)\n\n(cid:2)\n3 \u2212 1\nn\n\n= 11\n12\n\n\u2212 1\n2\n\n= 1\n6\n\nn\n\nn\n\nf\n\np\n\n/\n\n%\n}, and (d) p{2 < x \u2026 4}.\nx \u2026 3 \u2212 1\n(a) p{x < 3} = lim\nn\n(cid:2)\n\n= lim\n(cid:3)\np{x = 1} = p{x \u2026 1} \u2212 p{x < 1}\n1 \u2212 1\nn\n%\n(cid:2)\n(cid:3)\nx \u2026 1\n2\n= 3\n1\n2\n4\n\n= f(1) \u2212 lim\n/\n%\n\n= 1 \u2212 p\n= 1 \u2212 f\n\n= 2\n3\n/\n\nx >\n\n1\n2\n\np\n\nf\n\nn\n\n2\n\nsolution.\n\n(b)\n\n(c)\n\n(d)\n\np{2 < x \u2026 4} = f(4) \u2212 f(2)\n\n= 1\n12\n\n.\n\nsummary\na real-valued function defined on the outcome of a probability experiment is called\na random variable.\n\nif x is a random variable, then the function f(x) defined by\n\nf(x) = p{x \u2026 x}\n\nis called the distribution function of x. all probabilities concerning x can be stated\nin terms of f.\n\n "}, {"Page_number": 186, "text": "a random variable whose set of possible values is either finite or countably infinite\n\nis called discrete. if x is a discrete random variable, then the function\n\nsummary 171\n\nis called the probability mass function of x. also, the quantity e[x] defined by\n\nis called the expected value of x. e[x] is also commonly called the mean or the expec-\ntation of x.\n\na useful identity states that, for a function g,\n\np(x) = p{x = x}\n\ne[x] =\n\nxp(x)\n\nx:p(x)>0\n\n(cid:6)\n\n(cid:6)\n\ne[g(x)] =\n\ng(x)p(x)\n\nx:p(x)>0\n\nthe variance of a random variable x, denoted by var(x), is defined by\n\nvar(x) = e[(x \u2212 e[x])2]\n\nthe variance, which is equal to the expected square of the difference between x\nand its expected value, is a measure of the spread of the possible values of x. a useful\nidentity is\n\nvar(x) = e[x2] \u2212 (e[x])2\n\nthe quantity\n\nvar(x) is called the standard deviation of x.\n\nwe now note some common types of discrete random variables. the random vari-\n\nable x whose probability mass function is given by\n\n\u221a\n\n(cid:2)\n\n(cid:3)\n\nn\ni\n\np(i) =\n\npi(1 \u2212 p)n\u2212i\n\ni = 0, . . . , n\n\nis said to be a binomial random variable with parameters n and p. such a random\nvariable can be interpreted as being the number of successes that occur when n inde-\npendent trials, each of which results in a success with probability p, are performed.\nits mean and variance are given by\n\ne[x] = np var(x) = np(1 \u2212 p)\n\nthe random variable x whose probability mass function is given by\n\np(i) = e\n\n\u2212\u03bb\u03bbi\ni!\n\ni \u00fa 0\n\nis said to be a poisson random variable with parameter \u03bb. if a large number of (approx-\nimately) independent trials are performed, each having a small probability of being\nsuccessful, then the number of successful trials that result will have a distribution\nwhich is approximately that of a poisson random variable. the mean and variance of\na poisson random variable are both equal to its parameter \u03bb. that is,\n\ne[x] = var(x) = \u03bb\n\nthe random variable x whose probability mass function is given by\n\np(i) = p(1 \u2212 p)i\u22121\n\ni = 1, 2, . . .\n\n "}, {"Page_number": 187, "text": "172\n\nchapter 4\n\nrandom variables\n\nis said to be a geometric random variable with parameter p. such a random variable\nrepresents the trial number of the first success when each trial is independently a\nsuccess with probability p. its mean and variance are given by\n\nthe random variable x whose probability mass function is given by\n\ne[x] = 1\np\n(cid:2)\n\ni \u2212 1\nr \u2212 1\n\np(i) =\n\nvar(x) = 1 \u2212 p\n(cid:3)\n\np2\n\npr(1 \u2212 p)i\u2212r\n\ni \u00fa r\n\nis said to be a negative binomial random variable with parameters r and p. such a\nrandom variable represents the trial number of the rth success when each trial is\nindependently a success with probability p. its mean and variance are given by\n\ne[x] = r\np\n\nvar(x) = r(1 \u2212 p)\n\np2\n\na hypergeometric random variable x with parameters n, n, and m represents the\nnumber of white balls selected when n balls are randomly chosen from an urn that\ncontains n balls of which m are white. the probability mass function of this random\nvariable is given by\n\n(cid:2)\n\n(cid:3)(cid:2)\n(cid:2)\n\nm\ni\n\n(cid:3)\n\nn \u2212 m\n(cid:3)\nn \u2212 i\nn\nn\n\np(i) =\n\ni = 0, . . . , m\n\nwith p = m/n, its mean and variance are\n\ne[x] = np var(x) = n \u2212 n\nn \u2212 1\n\nnp(1 \u2212 p)\n\nan important property of the expected value is that the expected value of a sum of\nrandom variables is equal to the sum of their expected values. that is,\n\n\u23a1\n\u23a3 n(cid:6)\n\ni=1\n\n\u23a4\n\u23a6 = n(cid:6)\n\ni=1\n\ne\n\nxi\n\ne[xi]\n\nproblems\n\n4.1. two balls are chosen randomly from an urn con-\ntaining 8 white, 4 black, and 2 orange balls. sup-\npose that we win $2 for each black ball selected\nand we lose $1 for each white ball selected. let\nx denote our winnings. what are the possible val-\nues of x, and what are the probabilities associated\nwith each value?\nthe\nproduct of the 2 dice. compute p{x = i} for\ni = 1, . . . , 36.\n\n4.2. two fair dice are rolled. let x equal\n\n4.3. three dice are rolled. by assuming that each of\nthe 63 = 216 possible outcomes is equally likely,\nfind the probabilities attached to the possible val-\nues that x can take on, where x is the sum of\nthe 3 dice.\n\n4.4. five men and 5 women are ranked according to\ntheir scores on an examination. assume that no\ntwo scores are alike and all 10! possible rankings\nare equally likely. let x denote the highest rank-\ning achieved by a woman. (for instance, x = 1\n\n "}, {"Page_number": 188, "text": "if the top-ranked person is female.) find p{x = i},\ni = 1, 2, 3, . . . , 8, 9, 10.\n\n4.5. let x represent the difference between the num-\nber of heads and the number of tails obtained\nwhen a coin is tossed n times. what are the pos-\nsible values of x?\n4.6. in problem 5, for n = 3, if the coin is assumed fair,\nwhat are the probabilities associated with the val-\nues that x can take on?\n\n4.7. suppose that a die is rolled twice. what are the\npossible values that the following random vari-\nables can take on:\n(a) the maximum value to appear in the two rolls;\n(b) the minimum value to appear in the two rolls;\n(c) the sum of the two rolls;\n(d) the value of the first roll minus the value of\n\nthe second roll?\n\n4.8. if the die in problem 7 is assumed fair, calculate\nthe probabilities associated with the random vari-\nables in parts (a) through (d).\n\n4.9. repeat example 1b when the balls are selected\n\nwith replacement.\n\n4.10. in example 1d, compute the conditional probabil-\nity that we win i dollars, given that we win some-\nthing; compute it for i = 1, 2, 3.\n\n4.11. (a) an integer n is to be selected at random from\n{1, 2, . . . , (10)3} in the sense that each integer\nhas the same probability of being selected.\nwhat is the probability that n will be divis-\nible by 3? by 5? by 7? by 15? by 105? how\nwould your answer change if (10)3 is replaced\nby (10)k as k became larger and larger?\n\n(b) an important function in number theory\u2014\none whose properties can be shown to be\nrelated to what is probably the most impor-\ntant unsolved problem of mathematics, the\nriemann hypothesis\u2014is the m \u00a8obius function\n\u03bc(n), defined for all positive integral values n\nas follows: factor n into its prime factors. if\nthere is a repeated prime factor, as in 12 =\n2 \u00b7 2 \u00b7 3 or 49 = 7 \u00b7 7, then \u03bc(n) is defined\nto equal 0. now let n be chosen at random\nfrom {1, 2, . . . (10)k}, where k is large. deter-\nmine p{\u03bc(n) = 0} as k\u2192q.\nhint: to compute p{\u03bc(n) z 0}, use the identity\n(cid:3)(cid:2)\nq(cid:31)\n\u00b7\u00b7\u00b7 = 6\n\u03c0 2\n\n(cid:3)(cid:2)\n\n(cid:3)(cid:2)\n\n(cid:2)\n\n(cid:3)\n\np2\ni\n\n=\n\n24\n25\n\n48\n49\n\n3\n4\n\n8\n9\n\n\u2212 1\np2\ni\n\ni=1\n\nwhere pi is the ith-smallest prime. (the number 1\nis not a prime.)\n\n4.12. in the game of two-finger morra, 2 players show\n1 or 2 fingers and simultaneously guess the number\nof fingers their opponent will show. if only one of\nthe players guesses correctly, he wins an amount\n\nproblems 173\n\n(in dollars) equal to the sum of the fingers shown\nby him and his opponent. if both players guess\ncorrectly or if neither guesses correctly, then no\nmoney is exchanged. consider a specified player,\nand denote by x the amount of money he wins in\na single game of two-finger morra.\n(a) if each player acts independently of the other,\nand if each player makes his choice of the\nnumber of fingers he will hold up and the\nnumber he will guess that his opponent will\nhold up in such a way that each of the 4 pos-\nsibilities is equally likely, what are the possi-\nble values of x and what are their associated\nprobabilities?\n\n(b) suppose that each player acts independently\nof the other. if each player decides to hold up\nthe same number of fingers that he guesses his\nopponent will hold up, and if each player is\nequally likely to hold up 1 or 2 fingers, what\nare the possible values of x and their associ-\nated probabilities?\n\n4.13. a salesman has scheduled two appointments to\nsell encyclopedias. his first appointment will lead\nto a sale with probability .3, and his second will\nlead independently to a sale with probability .6.\nany sale made is equally likely to be either for the\ndeluxe model, which costs $1000, or the standard\nmodel, which costs $500. determine the probabil-\nity mass function of x, the total dollar value of all\nsales.\n\n4.14. five distinct numbers are randomly distributed\nto players numbered 1 through 5. whenever two\nplayers compare their numbers, the one with the\nhigher one is declared the winner. initially, players\n1 and 2 compare their numbers; the winner then\ncompares her number with that of player 3, and so\non. let x denote the number of times player 1 is a\nwinner. find p{x = i}, i = 0, 1, 2, 3, 4.\n\n4.15. the national basketball association (nba) draft\nlottery involves the 11 teams that had the worst\nwon\u2013lost records during the year. a total of 66\nballs are placed in an urn. each of these balls is\ninscribed with the name of a team: eleven have\nthe name of the team with the worst record, 10\nhave the name of the team with the second-worst\nrecord, 9 have the name of the team with the third-\nworst record, and so on (with 1 ball having the\nname of the team with the 11th-worst record).\na ball is then chosen at random, and the team\nwhose name is on the ball is given the first pick\nin the draft of players about to enter the league.\nanother ball is then chosen, and if it \u201cbelongs\u201d\nto a team different from the one that received the\nfirst draft pick, then the team to which it belongs\nreceives the second draft pick. (if the ball belongs\n\n "}, {"Page_number": 189, "text": "174\n\nchapter 4\n\nrandom variables\n\nto the team receiving the first pick, then it is dis-\ncarded and another one is chosen; this continues\nuntil the ball of another team is chosen.) finally,\nanother ball is chosen, and the team named on\nthe ball (provided that it is different from the\nprevious two teams) receives the third draft pick.\nthe remaining draft picks 4 through 11 are then\nawarded to the 8 teams that did not \u201cwin the lot-\ntery,\u201d in inverse order of their won\u2013lost records.\nfor instance, if the team with the worst record did\nnot receive any of the 3 lottery picks, then that\nteam would receive the fourth draft pick. let x\ndenote the draft pick of the team with the worst\nrecord. find the probability mass function of x.\n\n4.16. in problem 15, let team number 1 be the team\nwith the worst record, let team number 2 be the\nteam with the second-worst record, and so on. let\nyi denote the team that gets draft pick number i.\n(thus, y1 = 3 if the first ball chosen belongs to\nteam number 3.) find the probability mass func-\ntion of (a) y1, (b) y2, and (c) y3.\n\n4.17. suppose that the distribution function of x is\n\ngiven by\n\n\u23a7\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9\n\nf(b) =\n\n+ b \u2212 1\n\n4\n\n0\nb\n4\n1\n2\n11\n12\n1\n\nb < 0\n0 \u2026 b < 1\n\n1 \u2026 b < 2\n\n2 \u2026 b < 3\n3 \u2026 b\n\n(a) find p{x = i}, i = 1, 2, 3.\n(b) find p{ 1\n\n< x < 3\n2\n\n}.\n\n2\n\n4.18. four independent flips of a fair coin are made. let\nx denote the number of heads obtained. plot the\nprobability mass function of the random variable\nx \u2212 2.\n\n4.19. if the distribution function of x is given by\n\n\u23a7\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9\n\n0\n1\n2\n3\n5\n4\n5\n9\n10\n1\n\nb < 0\n0 \u2026 b < 1\n\n1 \u2026 b < 2\n\n2 \u2026 b < 3\n\n3 \u2026 b < 3.5\nb \u00fa 3.5\n\nf(b) =\n\ncalculate the probability mass function of x.\n\n(cid:18)\n\n(cid:19)\n\n(cid:19)\n\n(cid:18)\n\nwhich has probability 18\n38\n\n4.20. a gambling book recommends the following \u201cwin-\nning strategy\u201d for the game of roulette: bet $1 on\nred. if red appears\n, then\ntake the $1 profit and quit. if red does not appear\nand you lose this bet\n38 of\noccurring\n, make additional $1 bets on red on each\nof the next two spins of the roulette wheel and then\nquit. let x denote your winnings when you quit.\n(a) find p{x > 0}.\n(b) are you convinced that the strategy is indeed\n\nwhich has probability 20\n\na \u201cwinning\u201d strategy? explain your answer!\n\n(c) find e[x].\n\n4.21. four buses carrying 148 students from the same\nschool arrive at a football stadium. the buses\ncarry, respectively, 40, 33, 25, and 50 students. one\nof the students is randomly selected. let x denote\nthe number of students that were on the bus car-\nrying the randomly selected student. one of the 4\nbus drivers is also randomly selected. let y denote\nthe number of students on her bus.\n(a) which of e[x] or e[y] do you think is larger?\n\nwhy?\n\n(b) compute e[x] and e[y].\n\n4.22. suppose that two teams play a series of games that\nends when one of them has won i games. suppose\nthat each game played is, independently, won by\nteam a with probability p. find the expected num-\nber of games that are played when (a) i = 2 and (b)\ni = 3. also, show in both cases that this number is\nmaximized when p = 1\n2 .\n\n4.23. you have $1000, and a certain commodity\npresently sells for $2 per ounce. suppose that after\none week the commodity will sell for either $1\nor $4 an ounce, with these two possibilities being\nequally likely.\n(a) if your objective is to maximize the expected\namount of money that you possess at the\nend of the week, what strategy should you\nemploy?\n\n(b) if your objective is to maximize the expected\namount of the commodity that you possess at\nthe end of the week, what strategy should you\nemploy?\n\n4.24. a and b play the following game: a writes down\neither number 1 or number 2, and b must guess\nwhich one. if the number that a has written down\nis i and b has guessed correctly, b receives i units\nfrom a. if b makes a wrong guess, b pays 3\n4 unit to\na. if b randomizes his decision by guessing 1 with\nprobability p and 2 with probability 1 \u2212 p, deter-\nmine his expected gain if (a) a has written down\nnumber 1 and (b) a has written down number 2.\n\nwhat value of p maximizes the minimum pos-\nsible value of b\u2019s expected gain, and what is\nthis maximin value? (note that b\u2019s expected\n\n "}, {"Page_number": 190, "text": "gain depends not only on p, but also on what\na does.)\n\nconsider now player a. suppose that she also\nrandomizes her decision, writing down number 1\nwith probability q. what is a\u2019s expected loss if (c)\nb chooses number 1 and (d) b chooses number 2?\nwhat value of q minimizes a\u2019s maximum\nexpected loss? show that the minimum of a\u2019s max-\nimum expected loss is equal to the maximum of b\u2019s\nminimum expected gain. this result, known as the\nminimax theorem, was first established in general-\nity by the mathematician john von neumann and\nis the fundamental result in the mathematical disci-\npline known as the theory of games. the common\nvalue is called the value of the game to player b.\n\n4.25. two coins are to be flipped. the first coin will land\non heads with probability .6, the second with prob-\nability .7. assume that the results of the flips are\nindependent, and let x equal the total number of\nheads that result.\n(a) find p{x = 1}.\n(b) determine e[x].\n\n4.26. one of the numbers 1 through 10 is randomly cho-\nsen. you are to try to guess the number chosen by\nasking questions with \u201cyes\u2013no\u201d answers. compute\nthe expected number of questions you will need to\nask in each of the following two cases:\n(a) your ith question is to be \u201cis it i?\u201d i =\n\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10.\n\n(b) with each question you try to eliminate one-\nhalf of the remaining numbers, as nearly as\npossible.\n\n4.27. an insurance company writes a policy to the effect\nthat an amount of money a must be paid if some\nevent e occurs within a year. if the company esti-\nmates that e will occur within a year with probabil-\nity p, what should it charge the customer in order\nthat its expected profit will be 10 percent of a?\n\n4.28. a sample of 3 items is selected at random from a\nbox containing 20 items of which 4 are defective.\nfind the expected number of defective items in the\nsample.\n\n4.29. there are two possible causes for a breakdown of\na machine. to check the first possibility would cost\nc1 dollars, and, if that were the cause of the break-\ndown, the trouble could be repaired at a cost of r1\ndollars. similarly, there are costs c2 and r2 asso-\nciated with the second possibility. let p and 1 \u2212\np denote, respectively, the probabilities that the\nbreakdown is caused by the first and second possi-\nbilities. under what conditions on p, ci, ri, i = 1, 2,\nshould we check the first possible cause of break-\ndown and then the second, as opposed to reversing\nthe checking order, so as to minimize the expected\ncost involved in returning the machine to working\norder?\n\nproblems 175\n\nnote: if the first check is negative, we must still\ncheck the other possibility.\n\n4.30. a person tosses a fair coin until a tail appears for\nthe first time. if the tail appears on the nth flip, the\nperson wins 2n dollars. let x denote the player\u2019s\nwinnings. show that e[x] = +q. this problem is\nknown as the st. petersburg paradox.\n(a) would you be willing to pay $1 million to play\n\nthis game once?\n\n(b) would you be willing to pay $1 million for\neach game if you could play for as long as\nyou liked and only had to settle up when you\nstopped playing?\n\n4.31. each night different meteorologists give us the\nprobability that it will rain the next day. to judge\nhow well these people predict, we will score each\nof them as follows: if a meteorologist says that it\nwill rain with probability p, then he or she will\nreceive a score of\n\n1 \u2212 (1 \u2212 p)2\n1 \u2212 p2\n\nif it does rain\nif it does not rain\n\nwe will then keep track of scores over a cer-\ntain time span and conclude that the meteorologist\nwith the highest average score is the best predictor\nof weather. suppose now that a given meteorolo-\ngist is aware of our scoring mechanism and wants\nto maximize his or her expected score. if this per-\nson truly believes that it will rain tomorrow with\n, what value of p should he or she\nprobability p\nassert so as to maximize the expected score?\n\n\u2217\n\n4.32. to determine whether they have a certain dis-\nease, 100 people are to have their blood tested.\nhowever, rather than testing each individual sepa-\nrately, it has been decided first to place the peo-\nple into groups of 10. the blood samples of the\n10 people in each group will be pooled and ana-\nlyzed together. if the test is negative, one test will\nsuffice for the 10 people, whereas if the test is posi-\ntive, each of the 10 people will also be individually\ntested and, in all, 11 tests will be made on this\ngroup. assume that the probability that a person\nhas the disease is .1 for all people, independently\nof each other, and compute the expected number\nof tests necessary for each group. (note that we are\nassuming that the pooled test will be positive if at\nleast one person in the pool has the disease.)\n\n4.33. a newsboy purchases papers at 10 cents and sells\nthem at 15 cents. however, he is not allowed to\nreturn unsold papers. if his daily demand is a bino-\nmial random variable with n = 10, p = 1\n3 , approxi-\nmately how many papers should he purchase so as\nto maximize his expected profit?\n\n4.34. in example 4b, suppose that the department store\nincurs an additional cost of c for each unit of unmet\n\n "}, {"Page_number": 191, "text": "176\n\nchapter 4\n\nrandom variables\n\ndemand. (this type of cost is often referred to as\na goodwill cost because the store loses the good-\nwill of those customers whose demands it can-\nnot meet.) compute the expected profit when the\nstore stocks s units, and determine the value of s\nthat maximizes the expected profit.\n\n4.35. a box contains 5 red and 5 blue marbles. two mar-\nbles are withdrawn randomly. if they are the same\ncolor, then you win $1.10; if they are different col-\nors, then you win \u2212$1.00. (that is, you lose $1.00.)\ncalculate\n(a) the expected value of the amount you win;\n(b) the variance of the amount you win.\n\n4.36. consider problem 22 with i = 2. find the variance\nof the number of games played, and show that this\nnumber is maximized when p = 1\n2 .\n\n4.37. find var(x) and var(y) for x and y as given in\n4.38. if e[x] = 1 and var(x) = 5, find\n\nproblem 21.\n(a) e[(2 + x)2];\n(b) var(4 + 3x).\n\n4.39. a ball is drawn from an urn containing 3 white and\n3 black balls. after the ball is drawn, it is replaced\nand another ball is drawn. this process goes on\nindefinitely. what is the probability that, of the\nfirst 4 balls drawn, exactly 2 are white?\n\n4.40. on a multiple-choice exam with 3 possible answers\nfor each of the 5 questions, what is the probability\nthat a student will get 4 or more correct answers\njust by guessing?\n\n4.41. a man claims to have extrasensory perception. as\na test, a fair coin is flipped 10 times and the man is\nasked to predict the outcome in advance. he gets\n7 out of 10 correct. what is the probability that\nhe would have done at least this well if he had no\nesp?\n4.42. suppose that, in flight, airplane engines will fail\nwith probability 1 \u2212 p, independently from engine\nto engine. if an airplane needs a majority of its\nengines operative to complete a successful flight,\nfor what values of p is a 5-engine plane preferable\nto a 3-engine plane?\n\n4.43. a communications channel transmits the digits 0\nand 1. however, due to static, the digit transmitted\nis incorrectly received with probability .2. suppose\nthat we want to transmit an important message\nconsisting of one binary digit. to reduce the\nchance of error, we transmit 00000 instead of 0 and\n11111 instead of 1. if the receiver of the message\nuses \u201cmajority\u201d decoding, what is the probability\nthat the message will be wrong when decoded?\nwhat independence assumptions are you making?\n4.44. a satellite system consists of n components and\nfunctions on any given day if at least k of the n\ncomponents function on that day. on a rainy day\n\neach of the components independently functions\nwith probability p1, whereas on a dry day they each\nindependently function with probability p2. if the\nprobability of rain tomorrow is \u03b1, what is the prob-\nability that the satellite system will function?\n\n4.45. a student is getting ready to take an important\noral examination and is concerned about the pos-\nsibility of having an \u201con\u201d day or an \u201coff\u201d day. he\nfigures that if he has an on day, then each of his\nexaminers will pass him, independently of each\nother, with probability .8, whereas if he has an off\nday, this probability will be reduced to .4. sup-\npose that the student will pass the examination if a\nmajority of the examiners pass him. if the student\nfeels that he is twice as likely to have an off day as\nhe is to have an on day, should he request an exam-\nination with 3 examiners or with 5 examiners?\n\n4.46. suppose that it takes at least 9 votes from a 12-\nmember jury to convict a defendant. suppose also\nthat the probability that a juror votes a guilty per-\nson innocent is .2, whereas the probability that the\njuror votes an innocent person guilty is .1. if each\njuror acts independently and if 65 percent of the\ndefendants are guilty, find the probability that the\njury renders a correct decision. what percentage\nof defendants is convicted?\n\n4.47. in some military courts, 9 judges are appointed.\nhowever, both the prosecution and the defense\nattorneys are entitled to a peremptory challenge\nof any judge, in which case that judge is removed\nfrom the case and is not replaced. a defendant is\ndeclared guilty if the majority of judges cast votes\nof guilty, and he or she is declared innocent other-\nwise. suppose that when the defendant is, in fact,\nguilty, each judge will (independently) vote guilty\nwith probability .7, whereas when the defendant is,\nin fact, innocent, this probability drops to .3.\n(a) what is the probability that a guilty defendant\nis declared guilty when there are (i) 9, (ii) 8,\nand (iii) 7 judges?\n\n(b) repeat part (a) for an innocent defendant.\n(c) if the prosecution attorney does not exercise\nthe right to a peremptory challenge of a judge,\nand if the defense is limited to at most two\nsuch challenges, how many challenges should\nthe defense attorney make if he or she is 60\npercent certain that the client is guilty?\n\n4.48. it is known that diskettes produced by a cer-\ntain company will be defective with probability\n.01, independently of each other. the company\nsells the diskettes in packages of size 10 and\noffers a money-back guarantee that at most 1 of\nthe 10 diskettes in the package will be defective.\nthe guarantee is that the customer can return the\nentire package of diskettes if he or she finds more\n\n "}, {"Page_number": 192, "text": "than one defective diskette in it. if someone buys\n3 packages, what is the probability that he or she\nwill return exactly 1 of them?\n\n4.49. when coin 1 is flipped, it lands on heads with prob-\nability .4; when coin 2 is flipped, it lands on heads\nwith probability .7. one of these coins is randomly\nchosen and flipped 10 times.\n(a) what is the probability that the coin lands on\n\nheads on exactly 7 of the 10 flips?\n\n(b) given that the first of these ten flips lands\nheads, what is the conditional probability that\nexactly 7 of the 10 flips land on heads?\n\n4.50. suppose that a biased coin that lands on heads with\nprobability p is flipped 10 times. given that a total\nof 6 heads results, find the conditional probability\nthat the first 3 outcomes are\n(a) h, t, t (meaning that the first flip results in\nheads, the second in tails, and the third in\ntails);\n(b) t, h, t.\n\n4.51. the expected number of typographical errors on a\npage of a certain magazine is .2. what is the prob-\nability that the next page you read contains (a) 0\nand (b) 2 or more typographical errors? explain\nyour reasoning!\n\n4.52. the monthly worldwide average number of air-\nplane crashes of commercial airlines is 3.5. what\nis the probability that there will be\n(a) at least 2 such accidents in the next month;\n(b) at most 1 accident in the next month?\nexplain your reasoning!\n\n4.53. approximately 80,000 marriages took place in the\nstate of new york last year. estimate the proba-\nbility that, for at least one of these couples,\n(a) both partners were born on april 30;\n(b) both partners celebrated their birthday on the\n\nsame day of the year.\n\nstate your assumptions.\n\n4.54. suppose that the average number of cars aban-\ndoned weekly on a certain highway is 2.2. approx-\nimate the probability that there will be\n(a) no abandoned cars in the next week;\n(b) at least 2 abandoned cars in the next week.\n\n4.55. a certain typing agency employs 2 typists. the\naverage number of errors per article is 3 when\ntyped by the first typist and 4.2 when typed by the\nsecond. if your article is equally likely to be typed\nby either typist, approximate the probability that it\nwill have no errors.\n\n4.56. how many people are needed so that the probabil-\nity that at least one of them has the same birthday\nas you is greater than 1\n2 ?\n\nproblems 177\n\n4.57. suppose that the number of accidents occurring on\na highway each day is a poisson random variable\nwith parameter \u03bb = 3.\n(a) find the probability that 3 or more accidents\n\noccur today.\n\n(b) repeat part (a) under the assumption that at\n\nleast 1 accident occurs today.\n\n4.58. compare the poisson approximation with the cor-\n\nrect binomial probability for the following cases:\n(a) p{x = 2} when n = 8, p = .1;\n(b) p{x = 9} when n = 10, p = .95;\n(c) p{x = 0} when n = 10, p = .1;\n(d) p{x = 4} when n = 9, p = .2.\n\n4.59. if you buy a lottery ticket in 50 lotteries, in each of\n1\nwhich your chance of winning a prize is\n100 , what\nis the (approximate) probability that you will win\na prize\n(a) at least once?\n(b) exactly once?\n(c) at least twice?\n\n4.60. the number of times that a person contracts a cold\nin a given year is a poisson random variable with\nparameter \u03bb = 5. suppose that a new wonder drug\n(based on large quantities of vitamin c) has just\nbeen marketed that reduces the poisson parame-\nter to \u03bb = 3 for 75 percent of the population. for\nthe other 25 percent of the population, the drug\nhas no appreciable effect on colds. if an individ-\nual tries the drug for a year and has 2 colds in that\ntime, how likely is it that the drug is beneficial for\nhim or her?\n\n4.61. the probability of being dealt a full house in a\nhand of poker is approximately .0014. find an\napproximation for the probability that, in 1000\nhands of poker, you will be dealt at least 2 full\nhouses.\n\n4.62. consider n independent trials, each of which\nresults in one of the outcomes 1, . . . , k with respec-\ni=1 pi = 1. show\ntive probabilities p1, . . . , pk,\n(cid:9)\nthat if all the pi are small, then the probability that\nno trial outcome occurs more than once is approx-\nimately equal to exp(\u2212n(n \u2212 1)\n\n/2).\n\nk\n\ni p2\ni\n\n4.63. people enter a gambling casino at a rate of 1 every\n\n2 minutes.\n(a) what is the probability that no one enters\n\n(cid:9)\n\nbetween 12:00 and 12:05?\n\n(b) what is the probability that at least 4 people\n\nenter the casino during that time?\n\n4.64. the suicide rate in a certain state is 1 suicide per\n\n100,000 inhabitants per month.\n(a) find the probability that, in a city of 400,000\ninhabitants within this state, there will be 8 or\nmore suicides in a given month.\n\n "}, {"Page_number": 193, "text": "178\n\nchapter 4\n\nrandom variables\n\n(b) what is the probability that there will be at\nleast 2 months during the year that will have 8\nor more suicides?\n\n(c) counting the present month as month num-\nber 1, what is the probability that the first\nmonth to have 8 or more suicides will be\nmonth number i, i \u00fa 1?\n\nwhat assumptions are you making?\n\n4.65. each of 500 soldiers in an army company indepen-\ndently has a certain disease with probability 1/103.\nthis disease will show up in a blood test, and to\nfacilitate matters, blood samples from all 500 sol-\ndiers are pooled and tested.\n(a) what is the (approximate) probability that\nthe blood test will be positive (that is, at least\none person has the disease)?\n\nsuppose now that the blood test yields a positive\nresult.\n(b) what is the probability, under this circum-\nstance, that more than one person has the dis-\nease?\n\none of the 500 people is jones, who knows that he\nhas the disease.\n(c) what does jones think is the probability that\n\nmore than one person has the disease?\n\nbecause the pooled test was positive, the author-\nities have decided to test each individual sepa-\nrately. the first i \u2212 1 of these tests were negative,\nand the ith one\u2014which was on jones\u2014was posi-\ntive.\n(d) given the preceding, scenario, what is the\nprobability, as a function of i, that any of the\nremaining people have the disease?\n\n4.66. a total of 2n people, consisting of n married cou-\nples, are randomly seated (all possible orderings\nbeing equally likely) at a round table. let ci\ndenote the event that the members of couple i are\nseated next to each other, i = 1, . . . , n.\n(a) find p(ci).\n(b) for j z i, find p(cj|ci).\n(c) approximate the probability, for n large, that\nthere are no married couples who are seated\nnext to each other.\n\n4.67. repeat the preceding problem when the seating is\nrandom but subject to the constraint that the men\nand women alternate.\n\n4.68. in response to an attack of 10 missiles, 500 antibal-\nlistic missiles are launched. the missile targets of\nthe antiballistic missiles are independent, and each\nantiballstic missile is equally likely to go towards\nany of the target missiles. if each antiballistic mis-\nsile independently hits its target with probability\n.1, use the poisson paradigm to approximate the\nprobability that all missiles are hit.\n\n4.69. a fair coin is flipped 10 times. find the probability\n\nthat there is a string of 4 consecutive heads by\n(a) using the formula derived in the text;\n(b) using the recursive equations derived in the\n\ntext.\n\n(c) compare your answer with that given by the\n\npoisson approximation.\n\n4.70. at time 0, a coin that comes up heads with prob-\nability p is flipped and falls to the ground. sup-\npose it lands on heads. at times chosen accord-\ning to a poisson process with rate \u03bb, the coin is\npicked up and flipped. (between these times the\ncoin remains on the ground.) what is the proba-\nbility that the coin is on its head side at time t?\nhint what would be the conditional probability\nif there were no additional flips by time t, and\nwhat would it be if there were additional flips by\ntime t?\n\n4.71. consider a roulette wheel consisting of 38 num-\nbers 1 through 36, 0, and double 0. if smith always\nbets that the outcome will be one of the numbers 1\nthrough 12, what is the probability that\n(a) smith will lose his first 5 bets;\n(b) his first win will occur on his fourth bet?\n\n4.72. two athletic teams play a series of games; the first\nteam to win 4 games is declared the overall win-\nner. suppose that one of the teams is stronger than\nthe other and wins each game with probability .6,\nindependently of the outcomes of the other games.\nfind the probability, for i = 4, 5, 6, 7, that the\nstronger team wins the series in exactly i games.\ncompare the probability that the stronger team\nwins with the probability that it would win a 2-out-\nof-3 series.\n\n4.73. suppose in problem 72 that the two teams are\nevenly matched and each has probability 1\n2 of win-\nning each game. find the expected number of\ngames played.\n\n4.74. an interviewer is given a list of people she can\ninterview. if the interviewer needs to interview 5\npeople, and if each person (independently) agrees\nto be interviewed with probability 2\n3 , what is the\nprobability that her list of people will enable her\nto obtain her necessary number of interviews if\nthe list consists of (a) 5 people and (b) 8 people?\nfor part (b), what is the probability that the inter-\nviewer will speak to exactly (c) 6 people and (d) 7\npeople on the list?\n\n4.75. a fair coin is continually flipped until heads\nappears for the 10th time. let x denote the num-\nber of tails that occur. compute the probability\nmass function of x.\n\n4.76. solve the banach match problem (example 8e)\nwhen the left-hand matchbox originally contained\n\n "}, {"Page_number": 194, "text": "n1 matches and the right-hand box contained n2\nmatches.\n\n4.77. in the banach matchbox problem, find the prob-\nability that, at the moment when the first box is\nemptied (as opposed to being found empty), the\nother box contains exactly k matches.\n\n4.78. an urn contains 4 white and 4 black balls. we ran-\ndomly choose 4 balls. if 2 of them are white and\n2 are black, we stop. if not, we replace the balls\nin the urn and again randomly select 4 balls. this\ncontinues until exactly 2 of the 4 chosen are white.\nwhat is the probability that we shall make exactly\nn selections?\n\n4.79. suppose that a batch of 100 items contains 6 that\nare defective and 94 that are not defective. if x\nis the number of defective items in a randomly\ndrawn sample of 10 items from the batch, find (a)\np{x = 0} and (b) p{x > 2}.\n\n4.80. a game popular in nevada gambling casinos is\nkeno, which is played as follows: twenty num-\nbers are selected at random by the casino from the\nset of numbers 1 through 80. a player can select\nfrom 1 to 15 numbers; a win occurs if some frac-\ntion of the player\u2019s chosen subset matches any of\nthe 20 numbers drawn by the house. the payoff is a\nfunction of the number of elements in the player\u2019s\nselection and the number of matches. for instance,\nif the player selects only 1 number, then he or she\nwins if this number is among the set of 20, and\nthe payoff is $2.2 won for every dollar bet. (as\nthe player\u2019s probability of winning in this case is\n1\n4 , it is clear that the \u201cfair\u201d payoff should be $3\nwon for every $1 bet.) when the player selects 2\nnumbers, a payoff (of odds) of $12 won for every\n$1 bet is made when both numbers are among\nthe 20,\n(a) what would be the fair payoff in this case?\n\nlet pn, k denote the probability that exactly\nk of the n numbers chosen by the player are\namong the 20 selected by the house.\n\n(b) compute pn, k\n(c) the most typical wager at keno consists of\nselecting 10 numbers. for such a bet the\ncasino pays off as shown in the following\ntable. compute the expected payoff:\n\ntheoretical exercises 179\n\nkeno payoffs in 10 number bets\n\nnumber of matches dollars won for each $1 bet\n\n0\u20134\n5\n6\n7\n8\n9\n10\n\n-1\n1\n17\n179\n1, 299\n2, 599\n24, 999\n\n4.81. in example 8i, what percentage of i defective lots\ndoes the purchaser reject? find it for i = 1, 4.\ngiven that a lot is rejected, what is the condi-\ntional probability that it contained 4 defective\ncomponents?\n\n4.82. a purchaser of transistors buys them in lots of 20.\nit is his policy to randomly inspect 4 components\nfrom a lot and to accept the lot only if all 4 are\nnondefective. if each component in a lot is, inde-\npendently, defective with probability .1, what pro-\nportion of lots is rejected?\n\n4.83. there are three highways in the county. the num-\nber of daily accidents that occur on these high-\nways are poisson random variables with respective\nparameters .3, .5, and .7. find the expected num-\nber of accidents that will happen on any of these\nhighways today.\n\n4.84. suppose that 10 balls are put into 5 boxes, with\neach ball independently being put in box i with\nprobability pi,\n(a) find the expected number of boxes that do\n\ni=1 pi = 1.\n\n(cid:9)\n\n5\n\n(b) find the expected number of boxes that have\n\nnot have any balls.\n\nexactly 1 ball.\n\n(cid:9)\n\n4.85. there are k types of coupons. independently of\nthe types of previously collected coupons, each\nnew coupon collected is of type i with probability\ni=1 pi = 1. if n coupons are collected, find\npi,\nthe expected number of distinct types that appear\nin this set. (that is, find the expected number of\ntypes of coupons that appear at least once in the\nset of n coupons.)\n\nk\n\ntheoretical exercises\n\n4.1. there are n distinct types of coupons, and each\nindependently of\ntime one is obtained it will,\npast choices, be of type i with probability pi, i =\n1, . . . , n. let t denote the number one need select\n\nto obtain at least one of each type. compute\np{t = n}.\nhint: use an argument similar to the one used in\nexample 1e.\n\n "}, {"Page_number": 195, "text": "180\n\nchapter 4\n\nrandom variables\n\n4.2. if x has distribution function f, what is the distri-\n\nbution function of ex?\n4.3. if x has distribution function f, what is the distri-\nbution function of the random variable \u03b1x + \u03b2,\nwhere \u03b1 and \u03b2 are constants, \u03b1 z 0?\n\n4.4. for a nonnegative integer-valued random vari-\n\nable n, show that\n\ne[n] =\n\nq(cid:6)\np{n \u00fa i}\nq(cid:9)\np{n \u00fa i} = q(cid:9)\n\ni=1\n\ni=1\n\nk=i\n\nq(cid:9)\n\ni=1\n\nhint:\n\nchange the order of summation.\n\np{n = k}. now inter-\n\n4.5. for a nonnegative integer-valued random variable\n\nn, show that\n\nq(cid:6)\nip{n > i} = q(cid:9)\nq(cid:9)\n\nip{n > i} = 1\n2\n\ni=0\n\nhint:\n\ni=0\n\n(e[n2] \u2212 e[n])\nq(cid:9)\n\np{n = k}. now\n\ni\n\ni=0\n\nk=i+1\n\ninterchange the order of summation.\n\n4.6. let x be such that\n\np{x = 1} = p = 1 \u2212 p{x = \u22121}\n\nfind c z 1 such that e[cx] = 1.\n\n4.7. let x be a random variable having expected value\n\u03bc and variance \u03c3 2. find the expected value and\nvariance of\n\ny = x \u2212 \u03bc\n\n\u03c3\n\n4.8. find var(x) if\n\np(x = a) = p = 1 \u2212 p(x = b)\n\n4.9. show how the derivation of the binomial probabil-\n\nities\n\np{x = i} =\n\ni = 0, . . . , n\n\n(cid:2)\n\nn\ni\n\n(cid:3)\npi(1 \u2212 p)n\u2212i,\n(cid:3)\n\n(cid:2)\n\n(x + y)n = n(cid:6)\n\nxiyn\u2212i\n\nn\ni\n\ni=0\n\nleads to a proof of the binomial theorem\n\n4.10. let x be a binomial random variable with param-\n\nwhen x and y are nonnegative.\nhint: let p = x\nx+y\n\n.\n\neters n and p. show that\n\n(cid:20)\n\n(cid:21)\n\ne\n\n1\n\nx + 1\n\n= 1 \u2212 (1 \u2212 p)n+1\n\n(n + 1)p\n\n4.11. consider n independent sequential trials, each of\nwhich is successful with probability p. if there\nis a total of k successes, show that each of the\nn!/[k!(n \u2212 k)!] possible arrangements of the k suc-\ncesses and n \u2212 k failures is equally likely.\n\n4.12. there are n components lined up in a linear\narrangement. suppose that each component inde-\npendently functions with probability p. what is the\nprobability that no 2 neighboring components are\nboth nonfunctional?\nhint: condition on the number of defective com-\nponents and use the results of example 4c of\nchapter 1.\n4.13. let x be a binomial random variable with param-\neters (n, p). what value of p maximizes p{x =\nk}, k = 0, 1, . . . , n? this is an example of a sta-\ntistical method used to estimate p when a bino-\nmial (n, p) random variable is observed to equal\nk. if we assume that n is known, then we estimate\np by choosing that value of p which maximizes\np{x = k}. this is known as the method of maxi-\nmum likelihood estimation.\nwhere \u03b1 \u2026 (1 \u2212 p)/p.\n(a) what proportion of families has no children?\n(b) if each child is equally likely to be a boy or a\ngirl (independently of each other), what pro-\nportion of families consists of k boys (and any\nnumber of girls)?\n\n4.14. a family has n children with probability \u03b1pn, n \u00fa 1,\n\n4.15. suppose that n independent tosses of a coin having\nprobability p of coming up heads are made. show\nthat the probability that an even number of heads\n2 [1 + (q \u2212 p)n], where q = 1 \u2212 p. do\nresults is 1\n(cid:3)\nthis by proving and then utilizing the identity\n3\n\n2\n(p + q)n + (q \u2212 p)n\n\n[n/2](cid:6)\n\n(cid:2)\n\np2iqn\u22122i = 1\n2\n\nn\n2i\n\ni=0\n\nwhere [n/2] is the largest integer less than or equal\nto n/2. compare this exercise with theoretical\nexercise 3.5 of chapter 3.\n4.16. let x be a poisson random variable with parame-\nter \u03bb. show that p{x = i} increases monotonically\nand then decreases monotonically as i increases,\nreaching its maximum when i is the largest integer\nnot exceeding \u03bb.\nhint: consider p{x = i}/p{x = i \u2212 1}.\n\n4.17. let x be a poisson random variable with\n\nparameter \u03bb.\n(a) show that\n\n4\n1 + e\n\n\u22122\u03bb\n\n5\n\np{x is even} = 1\n2\n\n "}, {"Page_number": 196, "text": "by using the result of theoretical exercise\n15 and the relationship between poisson and\nbinomial random variables.\n\n(b) verify the formula in part (a) directly by mak-\n\ning use of the expansion of e\n\n\u2212\u03bb + e\u03bb.\n\n4.18. let x be a poisson random variable with parame-\nter \u03bb. what value of \u03bb maximizes p{x = k}, k \u00fa 0?\n4.19. show that x is a poisson random variable with\n\nparameter \u03bb, then\n\ne[xn] = \u03bbe[(x + 1)n\u22121]\nnow use this result to compute e[x3].\n\n4.20. consider n coins, each of which independently\ncomes up heads with probability p. suppose that\nn is large and p is small, and let \u03bb = np. suppose\nthat all n coins are tossed; if at least one comes\nup heads, the experiment ends; if not, we again\ntoss all n coins, and so on. that is, we stop the\nfirst time that at least one of the n coins come up\nheads. let x denote the total number of heads that\nappear. which of the following reasonings con-\ncerned with approximating p{x = 1} is correct\n(in all cases, y is a poisson random variable with\nparameter \u03bb)?\n(a) because the total number of heads that occur\nwhen all n coins are rolled is approximately a\npoisson random variable with parameter \u03bb,\n\np{x = 1} l p{y = 1} = \u03bbe\n\n\u2212\u03bb\n\n(b) because the total number of heads that occur\nwhen all n coins are rolled is approximately\na poisson random variable with parameter \u03bb,\nand because we stop only when this number is\npositive,\np{x = 1} l p{y = 1|y > 0} = \u03bbe\n\n\u2212\u03bb\n1 \u2212 e\u2212\u03bb\n(c) because at least one coin comes up heads, x\nwill equal 1 if none of the other n \u2212 1 coins\ncome up heads. because the number of heads\nresulting from these n \u2212 1 coins is approxi-\nmately poisson with mean (n \u2212 1)p l \u03bb,\n\np{x = 1} l p{y = 0} = e\n\n\u2212\u03bb\n\n4.21. from a set of n randomly chosen people, let eij\ndenote the event that persons i and j have the same\nbirthday. assume that each person is equally likely\nto have any of the 365 days of the year as his or her\nbirthday. find\n(a) p(e3,4|e1,2);\n(b) p(e1,3|e1,2);\n(c) p(e2,3|e1,2 \u2229 e1,3).\n\ntheoretical exercises 181\n\n(cid:2)\n\n(cid:3)\n\nwhat can you conclude from your answers to\nparts (a)\u2013(c) about the independence of the\nevents eij?\n\nn\n2\n\n4.22. an urn contains 2n balls, of which 2 are numbered\n1, 2 are numbered 2, . . . , and 2 are numbered n.\nballs are successively withdrawn 2 at a time with-\nout replacement. let t denote the first selection\nin which the balls withdrawn have the same num-\nber (and let it equal infinity if none of the pairs\nwithdrawn has the same number). we want to\nshow that, for 0 < \u03b1 < 1,\n\np{t > \u03b1n} = e\n\n\u2212\u03b1/2\n\nlim\nn\n\nto verify the preceding formula, let mk denote the\nnumber of pairs withdrawn in the first k selections,\nk = 1, . . . , n.\n(a) argue that when n is large, mk can be\nregarded as the number of successes in k\n(approximately) independent trials.\n\n(b) approximate p{mk = 0} when n is large.\n(c) write the event {t > \u03b1n} in terms of the value\n\n(d) verify the limiting probability given for\n\nof one of the variables mk.\np{t > \u03b1n}.\n\n4.23. consider a random collection of n individuals. in\napproximating the probability that no 3 of these\nindividuals share the same birthday, a better pois-\nson approximation than that obtained in the text\n(at least for values of n between 80 and 90) is\nobtained by letting ei be the event that there are\nat least 3 birthdays on day i, i = 1, . . . , 365.\n(a) find p(ei).\n(b) give an approximation for the probability\nthat no 3 individuals share the same birthday.\n(c) evaluate the preceding when n = 88 (which\ncan be shown to be the smallest value of n for\nwhich the probability exceeds .5).\n\n4.24. here is another way to obtain a set of recur-\nsive equations for determining pn, the probability\nthat there is a string of k consecutive heads in a\nsequence of n flips of a fair coin that comes up\nheads with probability p:\n(a) argue that, for k < n, there will be a string of\n\nk consecutive heads if either\n\n1. there is a string of k consecutive heads\n\nwithin the first n \u2212 1 flips, or\n2. there is no string of k consecutive heads\nwithin the first n \u2212 k \u2212 1 flips, flip n \u2212 k\nis a tail, and flips n \u2212 k + 1, . . . , n are\nall heads.\n\n(b) using the preceding, relate pn to pn\u22121. start-\ning with pk = pk, the recursion can be used to\nobtain pk+1, then pk+1, and so on, up to pn.\n\n "}, {"Page_number": 197, "text": "182\n\nchapter 4\n\nrandom variables\n\n4.25. suppose that the number of events that occur\nin a specified time is a poisson random variable\nwith parameter \u03bb. if each event is counted with\nprobability p, independently of every other event,\nshow that the number of events that are counted\nis a poisson random variable with parameter \u03bbp.\nalso, give an intuitive argument as to why this\nshould be so. as an application of the preceding\nresult, suppose that the number of distinct ura-\nnium deposits in a given area is a poisson random\nvariable with parameter \u03bb = 10. if, in a fixed\nperiod of time, each deposit is discovered inde-\npendently with probability 1\n50 , find the probability\nthat (a) exactly 1, (b) at least 1, and (c) at most 1\ndeposit is discovered during that time.\n\n4.26. prove\n\n* q\n\n\u03bb\n\ne\n\n\u2212\u03bb \u03bbi\ni!\n\n= 1\nn!\n\n\u2212xxndx\n\ne\n\nn(cid:6)\n\ni=0\n\nhint: use integration by parts.\n\n4.27. if x is a geometric random variable, show analyti-\n\ncally that\n\np{x = n + k|x > n} = p{x = k}\n\nusing the interpretation of a geometric random\nvariable, give a verbal argument as to why the pre-\nceding equation is true.\n\n4.28. let x be a negative binomial random variable with\nparameters r and p, and let y be a binomial ran-\ndom variable with parameters n and p. show that\n\np{x > n} = p{y < r}\n\n(cid:2)\n\nq(cid:6)\n\nhint: either one could attempt an analytical proof\nof the preceding equation, which is equivalent to\nproving the identity\n\n(cid:3)\n\npr(1 \u2212 p)i\u2212r = r\u22121(cid:6)\n\n(cid:2)\n\n(cid:3)\n\nn\ni\n\ni \u2212 1\nr \u2212 1\n\ni=n+1\n\ni=0\n* pi(1 \u2212 p)n\u2212i\nor one could attempt a proof that uses the prob-\nabilistic interpretation of these random variables.\nthat is, in the latter case, start by considering a\nsequence of independent trials having a common\nprobability p of success. then try to express the\nevents {x > n} and {y < r} in terms of the out-\ncomes of this sequence.\n\n4.29. for a hypergeometric random variable, determine\n\np{x = k + 1}/p{x = k}\n\n4.30. balls numbered 1 through n are in an urn. sup-\npose that n, n \u2026 n, of them are randomly selected\n\nwithout replacement. let y denote the largest\nnumber selected.\n(a) find the probability mass function of y.\n(b) derive an expression for e[y] and then use\nfermat\u2019s combinatorial identity (see theoret-\nical exercise 11 of chapter 1) to simplify the\nexpression.\ncontains m + n chips, numbered\n1, 2, . . . , n + m. a set of size n is drawn. if we\nlet x denote the number of chips drawn having\nnumbers that exceed each of the numbers of those\nremaining, compute the probability mass function\nof x.\n\n4.31. a jar\n\n4.32. a jar contains n chips. suppose that a boy succes-\nsively draws a chip from the jar, each time replac-\ning the one drawn before drawing another. the\nprocess continues until the boy draws a chip that\nhe has previously drawn. let x denote the num-\nber of draws, and compute its probability mass\nfunction.\n\n4.33. show that equation (8.6) follows from equation\n\n(8.5).\n\n4.34. from a set of n elements, a nonempty subset is\nchosen at random in the sense that all of the\nnonempty subsets are equally likely to be selected.\nlet x denote the number of elements in the cho-\nsen subset. using the identities given in theoreti-\ncal exercise 12 of chapter 1, show that\n\n(cid:29)\n\n(cid:30)n\u22121\n\nn\n\ne[x] =\nvar(x) = n \u00b7 22n\u22122 \u2212 n(n + 1)2n\u22122\n\n2 \u2212\n\n1\n2\n\n(2n \u2212 1)2\n\nshow also that, for n large,\n\nvar(x) (cid:3) n\n4\n\nthe ratio var(x)\n\nin the sense that\nto n/4\napproaches 1 as n approaches q. compare this\nformula with the limiting form of var(y) when\np{y = i} = 1/n, i = 1, . . . , n.\n\n4.35. an urn initially contains one red and one blue ball.\nat each stage, a ball is randomly chosen and then\nreplaced along with another of the same color. let\nx denote the selection number of the first chosen\nball that is blue. for instance, if the first selec-\ntion is red and the second blue, then x is equal\nto 2.\n(a) find p{x > i}, i \u00fa 1.\n(b) show that, with probability 1, a blue ball is\neventually chosen. (that is, show that p{x <\nq} = 1.)\n\n(c) find e[x].\n\n "}, {"Page_number": 198, "text": "4.36. suppose the possible values of x are {xi}, the pos-\nsible values of y are {yj}, and the possible values\nof x + y are {zk}. let ak denote the set of all\npairs of indices (i, j) such that xi + yj = zk; that is,\nak = {(i, j) : xi + yj = zk}.\n(cid:6)\n(a) argue that\n\np{x + y = zk} =\n\np{x = xi, y = yj}\n\n(i,j)\u2208ak\n\nself-test problems and exercises 183\n\n(cid:6)\n(c) using the formula from part (b), argue that\n(xi + yj)p{x = xi,\n\ne[x + y] =\n\n(cid:6)\n\nj\n\ni\n\ny = yj}\n(cid:6)\n(cid:6)\n\nj\n\n(d) show that\n\np(x = xi) =\np(y = yj) =\n\np(x = xi, y = yj),\np{x = xi, y = yj}\n\n(b) show that\n\ne[x + y] =\n\n(cid:6)\n\n(cid:6)\n\nk\n\n(i,j)\u2208ak\ny = yj}\n\n(xi + yj)p{x = xi,\n\n(e) prove that\n\ni\n\ne[x + y] = e[x] + e[y]\n\nself-test problems and exercises\n\nr(cid:9)\n\n4.1. suppose that the random variable x is equal to\nthe number of hits obtained by a certain base-\nball player in his next 3 at bats. if p{x = 1} =\n.3, p{x = 2} = .2, and p{x = 0} = 3p{x = 3},\nfind e[x].\n4.2. suppose that x takes on one of the values 0, 1,\nand 2. if for some constant c, p{x = i} = cp{x =\ni \u2212 1}, i = 1, 2, find e[x].\n\n4.3. a coin that, when flipped, comes up heads with\nprobability p is flipped until either heads or tails\nhas occurred twice. find the expected number of\nflips.\n\ni=1\n\nr(cid:9)\n\n4.4. a certain community is composed of m families,\nni = m. if one of\nni of which have i children,\nthe families is randomly chosen, let x denote the\nnumber of children in that family. if one of the\nini children is randomly chosen, let y denote\ni=1\nthe total number of children in the family of that\nchild. show that e[y] \u00fa e[x].\n4.5. suppose that p{x = 0} = 1 \u2212 p{x = 1}. if\ne[x] = 3var(x), find p{x = 0}.\n\n4.6. there are 2 coins in a bin. when one of them is\nflipped, it lands on heads with probability .6, and\nwhen the other is flipped, it lands on heads with\nprobability .3. one of these coins is to be randomly\nchosen and then flipped. without knowing which\ncoin is chosen, you can bet any amount up to 10\ndollars, and you then either win that amount if the\ncoin comes up heads or lose it if it comes up tails.\nsuppose, however, that an insider is willing to sell\nyou, for an amount c, the information as to which\ncoin was selected. what is your expected payoff\nif you buy this information? note that if you buy\n\nit and then bet x, you will end up either winning\nx \u2212 c or \u2212x \u2212 c (that is, losing x + c in the lat-\nter case). also, for what values of c does it pay to\npurchase the information?\n\n4.7. a philanthropist writes a positive number x on a\npiece of red paper, shows the paper to an impar-\ntial observer, and then turns it face down on the\ntable. the observer then flips a fair coin. if it shows\nheads, she writes the value 2x and, if tails, the value\nx/2, on a piece of blue paper, which she then turns\nface down on the table. without knowing either\nthe value x or the result of the coin flip, you have\nthe option of turning over either the red or the\nblue piece of paper. after doing so and observing\nthe number written on that paper, you may elect\nto receive as a reward either that amount or the\n(unknown) amount written on the other piece of\npaper. for instance, if you elect to turn over the\nblue paper and observe the value 100, then you\ncan elect either to accept 100 as your reward or\nto take the amount (either 200 or 50) on the red\npaper. suppose that you would like your expected\nreward to be large.\n(a) argue that there is no reason to turn over the\nred paper first, because if you do so, then no\nmatter what value you observe, it is always\nbetter to switch to the blue paper.\n\n(b) let y be a fixed nonnegative value, and con-\nsider the following strategy: turn over the\nblue paper, and if its value is at least y, then\naccept that amount. if it is less than y, then\nswitch to the red paper. let ry(x) denote the\nreward obtained if the philanthropist writes\nthe amount x and you employ this strat-\negy. find e[ry(x)]. note that e[r0(x)] is the\n\n "}, {"Page_number": 199, "text": "184\n\nchapter 4\n\nrandom variables\n\nexpected reward if the philanthropist writes\nthe amount x when you employ the strategy\nof always choosing the blue paper.\n\n4.8. let b(n, p) represent a binomial random variable\n\nwith parameters n and p. argue that\np{b(n, p) \u2026 i} = 1 \u2212 p{b(n, 1 \u2212 p) \u2026 n \u2212 i \u2212 1}\nhint: the number of successes less than or equal to\ni is equivalent to what statement about the number\nof failures?\nvalue 6 and variance 2.4, find p{x = 5}.\n\n4.9. if x is a binomial random variable with expected\n\n4.10. an urn contains n balls numbered 1 through n. if\nyou withdraw m balls randomly in sequence, each\ntime replacing the ball selected previously, find\np{x = k}, k = 1, . . . , m, where x is the maximum\nof the m chosen numbers.\nhint: first find p{x \u2026 k}.\n\n4.11. teams a and b play a series of games, with the\nfirst team to win 3 games being declared the winner\nof the series. suppose that team a independently\nwins each game with probability p. find the condi-\ntional probability that team a wins\n(a) the series given that it wins the first game;\n(b) the first game given that it wins the series.\n\n4.12. a local soccer team has 5 more games left to play.\nif it wins its game this weekend, then it will play\nits final 4 games in the upper bracket of its league,\nand if it loses, then it will play its final games in\nthe lower bracket. if it plays in the upper bracket,\nthen it will independently win each of its games in\nthis bracket with probability .4, and if it plays in\nthe lower bracket, then it will independently win\neach of its games with probability .7. if the proba-\nbility that the team wins its game this weekend is\n.5, what is the probability that it wins at least 3 of\nits final 4 games?\n\n4.13. each of the members of a 7-judge panel inde-\npendently makes a correct decision with probabil-\nity .7. if the panel\u2019s decision is made by majority\nrule, what is the probability that the panel makes\nthe correct decision? given that 4 of the judges\nagreed, what is the probability that the panel made\nthe correct decision?\n\n4.14. on average, 5.2 hurricanes hit a certain region in a\nyear. what is the probability that there will be 3 or\nfewer hurricanes hitting this year?\n\n4.15. the number of eggs laid on a tree leaf by an insect\nof a certain type is a poisson random variable with\nparameter \u03bb. however, such a random variable\ncan be observed only if it is positive, since if it is\n0 then we cannot know that such an insect was on\nthe leaf. if we let y denote the observed number\nof eggs, then\n\np{y = i} = p{x = i|x > 0}\n\nwhere x is poisson with parameter \u03bb. find e[y].\n\n4.16. each of n boys and n girls, independently and ran-\ndomly, chooses a member of the other sex. if a\nboy and girl choose each other, they become a\ncouple. number the girls, and let gi be the event\nthat girl number i is part of a couple. let p0 =\n1 \u2212 p(\u222an\ni=1gi) be the probability that no couples\nare formed.\n(a) what is p(gi)?\n(b) what is p(gi|gj)?\n(c) when n is large, approximate p0.\n(d) when n is large, approximate pk, the proba-\n\nbility that exactly k couples are formed.\n\n(e) use the inclusion\u2013exclusion identity to evalu-\n\nate p0.\n\n4.17. a total of 2n people, consisting of n married cou-\nples, are randomly divided into n pairs. arbitrarily\nnumber the women, and let wi denote the event\nthat woman i is paired with her husband.\n(a) find p(wi).\n(b) for i z j, find p(wi|wj).\n(c) when n is large, approximate the probability\n\nthat no wife is paired with her husband.\n\n(d) if each pairing must consist of a man and a\n\nwoman, what does the problem reduce to?\n\n4.18. a casino patron will continue to make $5 bets on\n\nred in roulette until she has won 4 of these bets.\n(a) what is the probability that she places a total\n\n(b) what is her expected winnings when she\n\nof 9 bets?\n\nstops?\n\nremark: on each bet, she will either win $5 with\nprobability 18\n\n38 or lose $5 with probability 20\n38 .\n\n4.19. when three friends go for coffee, they decide who\nwill pay the check by each flipping a coin and then\nletting the \u201codd person\u201d pay. if all three flips pro-\nduce the same result (so that there is no odd per-\nson), then they make a second round of flips, and\nthey continue to do so until there is an odd person.\nwhat is the probability that\n(a) exactly 3 rounds of flips are made?\n(b) more than 4 rounds are needed?\n\n4.20. show that if x is a geometric random variable with\n\nparameter p, then\n\ne[1/x] = \u2212p log(p)\n1 \u2212 p\nai/i. to do so, write ai/i = -\n\nhint: you will need to evaluate an expression of\n0 xi\u22121dx,\nthe form\n\na\n\nq(cid:9)\n\ni=1\n\nand then interchange the sum and the integral.\n\n4.21. suppose that\n\np{x = a} = p, p{x = b} = 1 \u2212 p\n\n "}, {"Page_number": 200, "text": "(a) show that x\u2212b\n\nable.\n\n(b) find var(x).\n\na\u2212b is a bernoulli random vari-\n\n4.22. each game you play is a win with probability p.\nyou plan to play 5 games, but if you win the fifth\ngame, then you will keep on playing until you\nlose.\n(a) find the expected number of games that\n\nyou play.\n\nyou lose.\n\n(b) find the expected number of games that\n\n4.23. balls are randomly withdrawn, one at a time with-\nout replacement, from an urn that initially has\nn white and m black balls. find the probability\nthat n white balls are drawn before m black balls,\nn \u2026 n, m \u2026 m.\n\n4.24. ten balls are to be distributed among 5 urns,\nwith each ball going into urn i with probabil-\ni=1 pi = 1. let xi denote the number of\nity pi,\n\n(cid:9)\n\n5\n\nself-test problems and exercises 185\n\nballs that go into urn i. assume that events cor-\nresponding to the locations of different balls are\nindependent.\n(a) what type of random variable is xi? be as\n(b) for i z j, what type of random variable is\n(c) find p{x1 + x2 + x3 = 7}.\n\nspecific as possible.\nxi + xj?\n\n4.25. for\n\nthe match problem (example\n\n5m in\n\nchapter 2), find\n(a) the expected number of matches.\n(b) the variance of the number of matches.\n\n4.26. let \u03b1 be the probability that a geometric random\n\nvariable x with parameter p is an even number.\n\n(a) find \u03b1 by using the identity \u03b1 = (cid:9)q\n\ni=1\n(b) find \u03b1 by conditioning on whether x = 1 or\n\np{x = 2i}.\n\nx > 1.\n\n "}, {"Page_number": 201, "text": "c h a p t e r\n\n5\n\ncontinuous random variables\n\n5.1 introduction\n5.2 expectation and variance of continuous random variables\n5.3 the uniform random variable\n5.4 normal random variables\n5.5 exponential random variables\n5.6 other continuous distributions\n5.7 the distribution of a function of a random variable\n\n5.1 introduction\n\nin chapter 4, we considered discrete random variables\u2014that is, random variables\nwhose set of possible values is either finite or countably infinite. however, there also\nexist random variables whose set of possible values is uncountable. two examples are\nthe time that a train arrives at a specified stop and the lifetime of a transistor. let x\nbe such a random variable. we say that x is a continuous\u2020 random variable if there\nexists a nonnegative function f , defined for all real x \u2208 (\u2212q, q), having the property\nthat, for any set b of real numbers,\u2021\n\n*\n\np{x \u2208 b} =\n\nf (x) dx\n\nb\n\n(1.1)\n\nthe function f is called the probability density function of the random variable x.\n(see figure 5.1.)\n\nin words, equation (1.1) states that the probability that x will be in b may be\nobtained by integrating the probability density function over the set b. since x must\nassume some value, f must satisfy\n\n* q\n\n1 = p{x \u2208 (\u2212q, q)} =\n\nf (x) dx\n\n\u2212q\n\nall probability statements about x can be answered in terms of f . for instance, from\nequation (1.1), letting b = [a, b], we obtain\np{a \u2026 x \u2026 b} =\n\nf (x) dx\n\n(1.2)\n\n*\n\nb\n\na\n\n\u2020sometimes called absolutely continuous.\n\u2021actually, for technical reasons equation (1.1) is true only for the measurable sets b, which,\n\nfortunately, include all sets of practical interest.\n\n186\n\n "}, {"Page_number": 202, "text": "section 5.1\n\nintroduction 187\n\nf\n\na\n\nb\n\nx\n\np(a \u2c55 x \u2c55 b) = area of shaded region\n\nfigure 5.1: probability density function f.\n\nif we let a = b in equation (1.2), we get\np{x = a} =\n\n*\n\na\n\na\n\nf (x) dx = 0\n*\n\nin words, this equation states that the probability that a continuous random variable\nwill assume any fixed value is zero. hence, for a continuous random variable,\n\np{x < a} = p{x \u2026 a} = f(a) =\n\na\n\u2212q\n\nf (x) dx\n\nexample 1a\nsuppose that x is a continuous random variable whose probability density function\nis given by\n\n0\n\nf (x) =\n\nc(4x \u2212 2x2)\n0\n\n0 < x < 2\notherwise\n\n(a) what is the value of c?\n(b) find p{x > 1}.\nsolution. (a) since f is a probability density function, we must have\nimplying that\n\n- q\n\u2212q f (x) dx = 1,\n\n*\n(cid:7)\n\nc\n\n0\n\n2\n\n(4x \u2212 2x2) dx = 1\n\n(cid:8)66666x=2\n\nx=0\n\n= 1\n\n2x2 \u2212 2x3\n3\n\nor\n\nor\n\nc\n\nhence,\n\n(b) p{x > 1} =- q\n\n1 f (x) dx = 3\n\n8\n\nc = 3\n8\n\n(4x \u2212 2x2) dx = 1\n\n2\n\n-\n\n2\n1\n\n.\n\n "}, {"Page_number": 203, "text": "188\n\nchapter 5\n\ncontinuous random variables\n\n0\n\n*\n\nexample 1b\nthe amount of time in hours that a computer functions before breaking down is a\ncontinuous random variable with probability density function given by\n\nf (x) =\n\n\u2212x/100 x \u00fa 0\nx < 0\n\n\u03bbe\n0\n\nwhat is the probability that\n(a) a computer will function between 50 and 150 hours before breaking down?\n(b) it will function for fewer than 100 hours?\n\nsolution. (a) since\n\nwe obtain\n\n* q\n\n\u2212q\n\n1 =\n\n* q\n\n0\n\n\u2212x/100 dx\ne\n\nf (x) dx = \u03bb\n66q\n\n\u2212x/100\n\n1 = \u2212\u03bb(100)e\n\n= 100\u03bb or \u03bb = 1\n100\n\n0\n\nhence, the probability that a computer will function between 50 and 150 hours before\nbreaking down is given by\n\n66150\n\n50\n\n\u2212x/100\n\np{50 < x < 150} =\n= e\n\n(b) similarly,\n\np{x < 100} =\n\n*\n\n0\n\n100\n\n1\n100\n\n150\n\n50\n\n\u2212x/100 dx = \u2212e\n1\ne\n100\n\u22121/2 \u2212 e\n\u22123/2 l .384\n66100\n\n\u2212x/100\n\n\u2212x/100 dx = \u2212e\ne\n\n= 1 \u2212 e\n\n\u22121 l .633\n\n0\n\nin other words, approximately 63.3 percent of the time, a computer will fail before\n.\nregistering 100 hours of use.\n\nexample 1c\nthe lifetime in hours of a certain kind of radio tube is a random variable having a\nprobability density function given by\n\n\u23a7\u23aa\u23a8\n\u23aa\u23a9 0\nx \u2026 100\n100\nx2 x > 100\n\nf (x) =\n\nwhat is the probability that exactly 2 of 5 such tubes in a radio set will have to be\nreplaced within the first 150 hours of operation? assume that the events ei, i =\n1, 2, 3, 4, 5, that the ith such tube will have to be replaced within this time are\nindependent.\n\n "}, {"Page_number": 204, "text": "solution. from the statement of the problem, we have\n\nsection 5.1\n\nintroduction 189\n\n*\n\n150\n\n*\n\n0\n\np(ei) =\n\n= 100\n= 1\n3\n(cid:3)(cid:2)\n\n(cid:3)2(cid:2)\n\n1\n3\n\n(cid:2)\n\n5\n2\n\nf (x) dx\n\n\u22122 dx\nx\n\n150\n\n100\n\n2\n3\n\n(cid:3)3 = 80\n*\n\n243\n\nhence, from the independence of the events ei, it follows that the desired probabil-\nity is\n\n.\n\nthe relationship between the cumulative distribution f and the probability density\n\nf is expressed by\n\nf(a) = p{x \u2208 (\u2212q, a]} =\n\na\n\u2212q\n\nf (x) dx\n\ndifferentiating both sides of the preceding equation yields\n\nf(a) = f (a)\n\nd\nda\n\n/\n\nthat is, the density is the derivative of the cumulative distribution function. a some-\nwhat more intuitive interpretation of the density function may be obtained from\n%\nequation (1.2) as follows:\na \u2212 \u03b5\n2\n\n\u2026 x \u2026 a + \u03b5\n2\n\nf (x) dx l \u03b5f (a)\n\na+\u03b5/2\na\u2212\u03b5/2\n\nwhen \u03b5 is small and when f (\u00b7) is continuous at x = a. in other words, the probability\nthat x will be contained in an interval of length \u03b5 around the point a is approximately\n\u03b5f (a). from this result we see that f (a) is a measure of how likely it is that the random\nvariable will be near a.\n\n*\n\n=\n\np\n\nexample 1d\nif x is continuous with distribution function fx and density function fx, find the\ndensity function of y = 2x.\nsolution. we will determine fy in two ways. the first way is to derive, and then\ndifferentiate, the distribution function of y:\n\nfy (a) = p{y \u2026 a}\n= p{2x \u2026 a}\n= p{x \u2026 a/2}\n= fx (a/2)\n\nfy (a) = 1\n2\n\nfx (a/2)\n\ndifferentiation gives\n\n "}, {"Page_number": 205, "text": "190\n\nchapter 5\n\ncontinuous random variables\n\nanother way to determine fy is to note that\n\u0001fy (a) l p{a \u2212 \u0001\n2\n= p{a \u2212 \u0001\n2\n= p{a\n\u2212 \u0001\n2\n4\nl \u0001\nfx (a/2)\n2\n\n\u2026 y \u2026 a + \u0001\n}\n2\n\u2026 2x \u2026 a + \u0001\n2\n+ \u0001\n\u2026 x \u2026 a\n2\n4\n\n}\n}\n\ndividing through by \u0001 gives the same result as before.\n\n.\n\n5.2 expectation and variance of continuous random variables\n\nin chapter 4, we defined the expected value of a discrete random variable x by\n\nif x is a continuous random variable having probability density function f (x), then,\nbecause\n\nf (x) dx l p{x \u2026 x \u2026 x + dx}\n\nfor dx small\n\nit is easy to see that the analogous definition is to define the expected value of x by\n\n(cid:6)\n\nx\n\n* q\n\ne[x] =\n\nxp{x = x}\n\ne[x] =\n\nxf (x) dx\n\n\u2212q\n\nexample 2a\nfind e[x] when the density function of x is\n\n%\n\nf (x) =\n\nsolution.\n\n0 \u2026 x \u2026 1\n\n2x if\n0 otherwise\n\n*\n*\n\ne[x] =\n=\n\nxf (x) dx\n\n1\n\n2x2 dx\n\n.\n\n0\n= 2\n3\n\nexample 2b\nthe density function of x is given by\nf (x) =\n\n%\n\n1 if 0 \u2026 x \u2026 1\n0 otherwise\n\nfind e[ex].\n\n "}, {"Page_number": 206, "text": "section 5.2\n\nexpectation and variance of continuous random variables 191\nsolution. let y = ex. we start by determining fy, the probability distribution func-\ntion of y. now, for 1 \u2026 x \u2026 e,\n\nfy (x) = p{y \u2026 x}\n= p{ex \u2026 x}\n*\n= p{x \u2026 log(x)}\n=\n= log(x)\n\nf (y) dy\n\nlog(x)\n\n0\n\nby differentiating fy (x), we can conclude that the probability density function of y\nis given by\n\nhence,\n\nfy (x) = 1\nx\n\n1 \u2026 x \u2026 e\n\n* q\n*\n\nxfy (x) dx\n\ne[ex] = e[y] =\n=\ndx\n= e \u2212 1\n\n\u2212q\ne\n\n1\n\n.\n\nalthough the method employed in example 2b to compute the expected value of\na function of x is always applicable, there is, as in the discrete case, an alternative\nway of proceeding. the following is a direct analog of proposition 4.1. of chapter 4.\n\nproposition 2.1. if x is a continuous random variable with probability density func-\ntion f (x), then, for any real-valued function g,\n\n* q\n\ne[g(x)] =\n\ng(x)f (x) dx\n\n\u2212q\n\nan application of proposition 2.1 to example 2b yields\n\ne[ex] =\n\n1\n\nex dx\n\nsincef (x) = 1,\n\n0 < x < 1\n\n*\n\n0\n\n= e \u2212 1\n\nwhich is in accord with the result obtained in that example.\n\nthe proof of proposition 2.1 is more involved than that of its discrete random\nvariable analog. we will present such a proof under the provision that the random\nvariable g(x) is nonnegative. (the general proof, which follows the argument in the\ncase we present, is indicated in theoretical exercises 2 and 3.) we will need the fol-\nlowing lemma, which is of independent interest.\n\nlemma 2.1\nfor a nonnegative random variable y,\n\n* q\n\n0\n\ne[y] =\n\np{y > y} dy\n\n "}, {"Page_number": 207, "text": "192\n\nchapter 5\n\ncontinuous random variables\n\nproof. we present a proof when y is a continuous random variable with probabil-\nity density function fy. we have\n\n0\n\n* q\n\np{y > y} dy =\n\n* q\n* q\nwhere we have used the fact that p{y > y} =- q\n(cid:2)*\n* q\n* q\np{y > y} dy =\n=\n= e[y]\n\nof integration in the preceding equation yields\n\n* q\n\n0\n\n0\n\n0\n\n0\n\n0\n\ny\n\nx\n\nxfy (x) dx\n\nfy (x) dx dy\n\n(cid:3)\n\ndy\n\nfy (x) dx\n\ny fy (x) dx. interchanging the order\n\n.\n\nproof of proposition 2.1. from lemma 2.1, for any function g for which g(x) \u00fa 0,\n\n0\n\n* q\n* q\n*\n*\n\n0\n\n*\np{g(x) > y} dy\n\n*\n\nx:g(x)>y\n\nf (x) dx dy\n\ng(x)\n\n0\n\ndy f (x) dx\n\nx:g(x)>0\n\ng(x)f (x) dx\n\nx:g(x)>0\n\ne[g(x)] =\n=\n\n=\n\n=\n\nwhich completes the proof.\n\nexample 2c\na stick of length 1 is split at a point u that is uniformly distributed over (0, 1). deter-\nmine the expected length of the piece that contains the point p, 0 \u2026 p \u2026 1.\n\nsolution. let lp(u) denote the length of the substick that contains the point p, and\nnote that\n\n%\n\n1 \u2212 u u < p\nu > p\nu\n\n(see figure 5.2.) hence, from proposition 2.1,\n\nlp(u) =\n*\n*\n\n0\n\n1\n\np\n\ne[lp(u)] =\n=\n\n*\n\n1\n\nu du\n\n\u2212 p2\n2\n\np\n+ 1\n2\n\nlp(u) du\n(1 \u2212 u)du +\n\u2212 (1 \u2212 p)2\n+ p(1 \u2212 p)\n\n2\n\n0\n= 1\n2\n= 1\n2\n\n "}, {"Page_number": 208, "text": "section 5.2\n\nexpectation and variance of continuous random variables 193\n\n0\n\n0\n\nu\n\nu\n\np\n\np\n\n1 \u2013 u\n\nu\n\n(a)\n\n(b)\n\n1\n\n1\n\nfigure 5.2: substick containing point p: (a) u < p; (b) u > p.\n\nsince p(1 \u2212 p) is maximized when p = 1\n2, it is interesting to note that the expected\nlength of the substick containing the point p is maximized when p is the midpoint of\n.\nthe original stick.\n\nexample 2d\nsuppose that if you are s minutes early for an appointment, then you incur the cost cs,\nand if you are s minutes late, then you incur the cost ks. suppose also that the travel\ntime from where you presently are to the location of your appointment is a continuous\nrandom variable having probability density function f . determine the time at which\nyou should depart if you want to minimize your expected cost.\n\nsolution. let x denote the travel time. if you leave t minutes before your appoint-\nment, then your cost\u2014call it ct(x)\u2014is given by\n\n%\n\nct(x) =\n\nc(t \u2212 x) ifx \u2026 t\nk(x \u2212 t) ifx \u00fa t\n\n* q\n*\n\n0\n\ntherefore,\n\ne[ct(x)] =\n=\n\n* q\n\nt\n\nt\n\nct(x)f (x) dx\n*\n*\nc(t \u2212 x)f (x) dx +\nf (x) dx \u2212 c\n\nt\n\nt\n\n0\n\n0\n\n= ct\n\n0\n\nk(x \u2212 t)f (x) dx\n\n* q\n\nt\n\nxf (x) dx + k\n\nxf (x) dx \u2212 kt\n\n* q\n\nt\n\nf (x) dx\n\nthe value of t that minimizes e[ct(x)] can now be obtained by calculus. differentia-\ntion yields\n\nd\ndt\n\ne[ct(x)] = ct f (t) + cf(t) \u2212 ct f (t) \u2212 kt f (t) + kt f (t) \u2212 k[1 \u2212 f(t)]\n\n= (k + c)f(t) \u2212 k\n\nequating the rightmost side to zero shows that the minimal expected cost is obtained\nwhen you leave t\n\nminutes before your appointment, where t\n\nsatisfies\n\n\u2217\n\n\u2217\n\nf(t\n\n\u2217) = k\n\nk + c\n\n.\n\nas in chapter 4, we can use proposition 2.1 to show the following.\n\ncorollary 2.1.\n\nif a and b are constants, then\n\ne[ax + b] = ae[x] + b\n\n "}, {"Page_number": 209, "text": "194\n\nchapter 5\n\ncontinuous random variables\n\nthe proof of corollary 2.1 for a continuous random variable x is the same as the\none given for a discrete random variable. the only modification is that the sum is\nreplaced by an integral and the probability mass function by a probability density\nfunction.\n\nthe variance of a continuous random variable is defined exactly as it is for a dis-\ncrete random variable, namely, if x is a random variable with expected value \u03bc, then\nthe variance of x is defined (for any type of random variable) by\n\nthe alternative formula,\n\nvar(x) = e[(x \u2212 \u03bc)2]\n\nvar(x) = e[x2] \u2212 (e[x])2\n\nis established in a manner similar to its counterpart in the discrete case.\n\nexample 2e\nfind var(x) for x as given in example 2a.\nsolution. we first compute e[x2].\n\n* q\n*\n\n\u2212q\n1\n\ne[x2] =\n=\n\nx2f (x) dx\n\n2x3 dx\n\n0\n= 1\n2\n\nhence, since e[x] = 2\n\n3, we obtain\n\nvar(x) = 1\n2\n\n\u2212\n\n(cid:2)\n\n(cid:3)2 = 1\n\n18\n\n2\n3\n\n.\n\nit can be shown that, for constants a and b,\n\nvar(ax + b) = a2var(x)\n\nthe proof mimics the one given for discrete random variables.\n\nthere are several important classes of continuous random variables that appear\nfrequently in applications of probability; the next few sections are devoted to a study\nof some of them.\n\n5.3 the uniform random variable\n\n%\n\na random variable is said to be uniformly distributed over the interval (0, 1) if its\nprobability density function is given by\n\nf (x) =\n\n- q\n(3.1)\n-\n\u2212q f (x) dx =\nnote that equation (3.1) is a density function, since f (x) \u00fa 0 and\n0 dx = 1. because f (x) > 0 only when x \u2208 (0, 1), it follows that x must assume\n1\na value in interval (0, 1). also, since f (x) is constant for x \u2208 (0, 1), x is just as likely to\n\n1\n0 otherwise\n\n0 < x < 1\n\n "}, {"Page_number": 210, "text": "section 5.3\n\nthe uniform random variable 195\n\nf(a)\n1\n\nf(a)\n\n1\u2014\u2014\u2013\n\u2424 \u2423\n   \u2013   \n\n\u2423\n\n(a)\n\na\n\n\u2424\n\n\u2423\n\na\n\n\u2424\n\n(b)\n\nfigure 5.3: graph of (a) f (a) and (b) f(a) for a uniform (\u03b1, \u03b2) random variable.\n\nbe near any value in (0, 1) as it is to be near any other value. to verify this statement,\nnote that, for any 0 < a < b < 1,\n\np{a \u2026 x \u2026 b} =\n\nb\n\nf (x) dx = b \u2212 a\n\n*\n\na\n\nin other words, the probability that x is in any particular subinterval of (0, 1) equals\nthe length of that subinterval.\n\nin general, we say that x is a uniform random variable on the interval (\u03b1, \u03b2) if the\n\nprobability density function of x is given by\n\nf (x) =\n\n1\n\n\u03b2 \u2212 \u03b1\n0\n\nif \u03b1 < x < \u03b2\n\notherwise\n\n(3.2)\n\na\u2212q f (x) dx, it follows from equation (3.2) that the distribution function\n\nof a uniform random variable on the interval (\u03b1, \u03b2) is given by\n\nsince f(a) =-\n\n\u23a7\u23aa\u23a8\n\u23aa\u23a9\n\u23a7\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23a9\n\nf(a) =\n\n0\na \u2212 \u03b1\n\u03b2 \u2212 \u03b1\n1\n\na \u2026 \u03b1\n\u03b1 < a < \u03b2\na \u00fa \u03b2\n\nfigure 5.3 presents a graph of f (a) and f(a).\n\nexample 3a\nlet x be uniformly distributed over (\u03b1, \u03b2). find (a) e[x] and (b) var(x).\n\nsolution.\n\n(a)\n\nxf (x) dx\n\n\u2212q\n\n* q\ne[x] =\n* \u03b2\n=\n\u03b2 \u2212 \u03b1\n= \u03b22 \u2212 \u03b12\n2(\u03b2 \u2212 \u03b1)\n= \u03b2 + \u03b1\n\nx\n\n\u03b1\n\ndx\n\n2\n\n "}, {"Page_number": 211, "text": "196\n\nchapter 5\n\ncontinuous random variables\n\nin words, the expected value of a random variable that is uniformly distributed\nover some interval is equal to the midpoint of that interval.\n\n(b) to find var(x), we first calculate e[x2].\n\ne[x2] =\n\nx2 dx\n\n1\n\n* \u03b2\n\u03b2 \u2212 \u03b1\n= \u03b23 \u2212 \u03b13\n3(\u03b2 \u2212 \u03b1)\n= \u03b22 + \u03b1\u03b2 + \u03b12\n\n\u03b1\n\n3\n\nhence,\n\nvar(x) = \u03b22 + \u03b1\u03b2 + \u03b12\n\n3\n\n= (\u03b2 \u2212 \u03b1)2\n\n12\n\n\u2212 (\u03b1 + \u03b2)2\n\n4\n\ntherefore, the variance of a random variable that is uniformly distributed over\n.\nsome interval is the square of the length of that interval divided by 12.\n\n*\n\nexample 3b\nif x is uniformly distributed over (0, 10), calculate the probability that (a) x < 3, (b)\nx > 6, and (c) 3 < x < 8.\n(a) p{x < 3} =\n*\n\ndx = 3\n10\n\nsolution.\n(b) p{x > 6} =\n(c) p{3 < x < 8} =\n\n1\n10\ndx = 4\n10\ndx = 1\n1\n2\n10\n\n1\n10\n\n*\n\n.\n\n10\n\n3\n\n0\n\n6\n\n8\n\n3\n\nexample 3c\nbuses arrive at a specified stop at 15-minute intervals starting at 7 a.m. that is, they\narrive at 7, 7:15, 7:30, 7:45, and so on. if a passenger arrives at the stop at a time that\nis uniformly distributed between 7 and 7:30, find the probability that he waits\n(a) less than 5 minutes for a bus;\n(b) more than 10 minutes for a bus.\n\nsolution. let x denote the number of minutes past 7 that the passenger arrives at\nthe stop. since x is a uniform random variable over the interval (0, 30), it follows that\nthe passenger will have to wait less than 5 minutes if (and only if) he arrives between\n7:10 and 7:15 or between 7:25 and 7:30. hence, the desired probability for part (a) is\n\n*\n\n*\n\np{10 < x < 15} + p{25 < x < 30} =\n\n15\n\n10\n\ndx +\n\n1\n30\n\n30\n\n25\n\n1\n30\n\ndx = 1\n3\n\nsimilarly, he would have to wait more than 10 minutes if he arrives between 7 and\n7:05 or between 7:15 and 7:20, so the probability for part (b) is\np{0 < x < 5} + p{15 < x < 20} = 1\n3\n\n.\n\n "}, {"Page_number": 212, "text": "section 5.3\n\nthe uniform random variable 197\n\nthe next example was first considered by the french mathematician joseph\nl. f. bertrand in 1889 and is often referred to as bertrand\u2019s paradox. it represents\nour initial introduction to a subject commonly referred to as geometrical probability.\n\nexample 3d\nconsider a random chord of a circle. what is the probability that the length of the\nchord will be greater than the side of the equilateral triangle inscribed in that circle?\n\nsolution. as stated, the problem is incapable of solution because it is not clear what\nis meant by a random chord. to give meaning to this phrase, we shall reformulate the\nproblem in two distinct ways.\n\nthe first formulation is as follows: the position of the chord can be determined\nby its distance from the center of the circle. this distance can vary between 0 and\nr, the radius of the circle. now, the length of the chord will be greater than the side\nof the equilateral triangle inscribed in the circle if the distance from the chord to the\ncenter of the circle is less than r/2. hence, by assuming that a random chord is a chord\nwhose distance d from the center of the circle is uniformly distributed between 0 and\nr, we see that the probability that the length of the chord is greater than the side of\nan inscribed equilateral triangle is\n\n%\n\n/\n\np\n\nd <\n\nr\n2\n\n= r/2\nr\n\n= 1\n2\n\nfor our second formulation of the problem, consider an arbitrary chord of the cir-\ncle; through one end of the chord, draw a tangent. the angle \u03b8 between the chord and\n\u25e6\n, determines the position of the chord. (see\nthe tangent, which can vary from 0\nfigure 5.4.) furthermore, the length of the chord will be greater than the side of the\n\u25e6\n. hence, assuming\ninscribed equilateral triangle if the angle \u03b8 is between 60\n\u25e6\nthat a random chord is a chord whose angle \u03b8 is uniformly distributed between 0\nand\n\u25e6\n180\n\n, we see that the desired answer in this formulation is\n\n\u25e6\nand 120\n\n\u25e6\nto 180\n\np{60 < \u03b8 < 120} = 120 \u2212 60\n\n= 1\n3\n\n180\n\nnote that random experiments could be performed in such a way that 1\n3 would\nbe the correct probability. for instance, if a circular disk of radius r is thrown on a\ntable ruled with parallel lines a distance 2r apart, then one and only one of these lines\nwould cross the disk and form a chord. all distances from this chord to the center of\nthe disk would be equally likely, so that the desired probability that the chord\u2019s length\nwill be greater than the side of an inscribed equilateral triangle is 1\n2. in contrast, if the\n\n2 or 1\n\n\u242a\n\na\n\nfigure 5.4\n\n "}, {"Page_number": 213, "text": "198\n\nchapter 5\n\ncontinuous random variables\n\nexperiment consisted of rotating a needle freely about a point a on the edge (see\n.\nfigure 5.4) of the circle, the desired answer would be 1\n3.\n\n5.4 normal random variables\n\nwe say that x is a normal random variable, or simply that x is normally distributed,\nwith parameters \u03bc and \u03c3 2 if the density of x is given by\n\nf (x) = 1\u221a\n2\u03c0 \u03c3\n\n\u2212(x\u2212\u03bc)2/2\u03c3 2\ne\n\n\u2212 q < x < q\n\nthis density function is a bell-shaped curve that is symmetric about \u03bc. (see figure 5.5.)\n\n.399\n\n\u20133\n\n\u20132\n\n\u20131\n\n1\n\n2\n\n3\n\n0\n(a)\n\n.399\u2014\u2014\u2013\u2434\n\n\u2013 \u2434\u242e\n\n+ \u2434\u242e\n\n\u242e\n\n\u2013 2\n\n\u2434\n\n\u242e\n\n+ 2\u2434\n\u242e\n\n(b)\n\nfigure 5.5: normal density function: (a) \u03bc = 0, \u03c3 = 1; (b) arbitrary \u03bc, \u03c3 2.\n\nthe normal distribution was introduced by the french mathematician abraham\ndemoivre in 1733, who used it to approximate probabilities associated with bino-\nmial random variables when the binomial parameter n is large. this result was later\nextended by laplace and others and is now encompassed in a probability theorem\nknown as the central limit theorem, which is discussed in chapter 8. the central limit\ntheorem, one of the two most important results in probability theory,\u2020 gives a theo-\nretical base to the often noted empirical observation that, in practice, many random\nphenomena obey, at least approximately, a normal probability distribution. some\nexamples of random phenomena obeying this behavior are the height of a man, the\nvelocity in any direction of a molecule in gas, and the error made in measuring a\nphysical quantity.\n\nto prove that f (x) is indeed a probability density function, we need to show that\n\n* q\n\n\u2212q\n\n1\u221a\n2\u03c0 \u03c3\n\n\u2212(x\u2212\u03bc)2/2\u03c3 2 dx = 1\ne\n\n\u2020the other is the strong law of large numbers.\n\n "}, {"Page_number": 214, "text": "section 5.4\n\nnormal random variables 199\n\n\u2212y2/2 dy\ne\n\n* q\n\nmaking the substitution y = (x \u2212 \u03bc)/\u03c3 , we see that\n\u2212(x\u2212\u03bc)2/2\u03c3 2 dx = 1\u221a\ne\n* q\n2\u03c0\n\u221a\n2\u03c0\n\nhence, we must show that\n\n1\u221a\n2\u03c0 \u03c3\n\n\u2212q\n\n* q\n\n\u2212q\n\ntoward this end, let i =- q\n\n\u2212q\n\n\u2212y2/2 dy =\ne\n* q\n\n* q\n\u2212y2/2 dy. then\n* q\n* q\n\u2212y2/2 dy\ne\n\n\u2212q\n\n\u2212q e\ni2 =\n=\n\n\u2212x2/2 dx\ne\n\u2212(y2+x2)/2 dy dx\ne\n\n\u2212q\n\nwe now evaluate the double integral by means of a change of variables to polar\n\ncoordinates. (that is, let x = r cos \u03b8, y = r sin \u03b8, and dy dx = r d\u03b8 dr.) thus,\n\n2\u03c0, and the result is proved.\n\nhence, i = \u221a\nan important fact about normal random variables is that if x is normally dis-\ntributed with parameters \u03bc and \u03c3 2, then y = ax + b is normally distributed with\nparameters a\u03bc + b and a2\u03c3 2. to prove this statement, suppose that a > 0. (the proof\nwhen a < 0 is similar.) let fy denote the cumulative distribution function of y. then\n\ni2 =\n\n\u2212q\n\n\u2212q\n\n0\n\n* q\n*\n* q\n= 2\u03c0\n= \u22122\u03c0e\n= 2\u03c0\n\n0\n\n0\n\n2\u03c0\n\n\u2212r2/2r d\u03b8 dr\ne\n66q\n\u2212r2/2dr\nre\n\u2212r2/2\n\n0\n\nfy (x) = p{y \u2026 x}\n\n= p{ax + b \u2026 x}\n= p{x \u2026 x \u2212 b\n}\n= fx (\n\nx \u2212 b\n\na\n\n)\n\na\n\nwhere fx is the cumulative distribution function of x. by differentiation, the density\nfunction of y is then\n\nx \u2212 b\n\n)\n\nfy (x) = 1\nfx (\na\n=\n1\u221a\n2\u03c0a\u03c3\n1\u221a\n2\u03c0a\u03c3\n\na\nexp{\u2212(\nexp{\u2212(x \u2212 b \u2212 a\u03bc)2/2(a\u03c3 )2}\nwhich shows that y is normal with parameters a\u03bc + b and a2\u03c3 2.\n\n\u2212 \u03bc)2/2\u03c3 2}\n\nx \u2212 b\n\n=\n\na\n\n "}, {"Page_number": 215, "text": "200\n\nchapter 5\n\ncontinuous random variables\n\nan important implication of the preceding result is that if x is normally distributed\nwith parameters \u03bc and \u03c3 2, then z = (x \u2212 \u03bc)/\u03c3 is normally distributed with parame-\nters 0 and 1. such a random variable is said to be a standard, or a unit, normal random\nvariable.\n\nwe now show that the parameters \u03bc and \u03c3 2 of a normal random variable represent,\n\nrespectively, its expected value and variance.\n\nexample 4a\nfind e[x] and var(x) when x is a normal random variable with parameters \u03bc and \u03c3 2.\n\n* q\nsolution. let us start by finding the mean and variance of the standard normal ran-\ndom variable z = (x \u2212 \u03bc)/\u03c3 . we have\n\ne[z] =\n\nxfz(x) dx\n\n* q\n\n\u2212x2/2 dx\nxe\n\u2212q\n\u2212x2/2|q\u2212q\ne\n\n\u2212q\n= 1\u221a\n2\u03c0\n= \u2212 1\u221a\n2\u03c0\n= 0\n\nthus,\n\n* q\nvar(z) = e[z2]\n\n= 1\u221a\n2\u03c0\n\nx2e\n\n\u2212q\nintegration by parts (with u = x and dv = xe\n\u2212x2/2) now gives\n\n\u2212x2/2 dx\n* q\n\n\u2212q\n\n\u2212x2/2 dx)\ne\n\nvar(z) = 1\u221a\n2\u03c0\n= 1\u221a\n2\u03c0\n= 1\n\n* q\n(\u2212xe\n\n\u2212q\n\n\u2212x2/2|q\u2212q +\n\u2212x2/2 dx\ne\n\nbecause x = \u03bc + \u03c3 z, the preceding yields the results\ne[x] = \u03bc + \u03c3 e[z] = \u03bc\n\nand\n\nvar(x) = \u03c3 2var(z) = \u03c3 2\n\n.\n\nit is customary to denote the cumulative distribution function of a standard normal\n\nrandom variable by \u0001(x). that is,\n\n*\n\n\u0001(x) = 1\u221a\n2\u03c0\n\n\u2212y2/2 dy\ne\n\nx\n\u2212q\n\nthe values of \u0001(x) for nonnegative x are given in table 5.1. for negative values of x,\n\u0001(x) can be obtained from the relationship\n\u0001(\u2212x) = 1 \u2212 \u0001(x)\n\n\u2212 q < x < q\n\n(4.1)\n\n "}, {"Page_number": 216, "text": "section 5.4\n\nnormal random variables 201\n\ntable 5.1: area \u0001(x) under the standard normal curve to the left of x\n\n.00\n\n.01\n\n.02\n\n.03\n\n.04\n\n.05\n\n.06\n\n.07\n\n.08\n\n.09\n\n.5000\n.5398\n.5793\n.6179\n.6554\n.6915\n.7257\n.7580\n.7881\n.8159\n.8413\n.8643\n.8849\n.9032\n.9192\n.9332\n.9452\n.9554\n.9641\n.9713\n.9772\n.9821\n.9861\n.9893\n.9918\n.9938\n.9953\n.9965\n.9974\n.9981\n.9987\n.9990\n.9993\n.9995\n.9997\n\n.5040\n.5438\n.5832\n.6217\n.6591\n.6950\n.7291\n.7611\n.7910\n.8186\n.8438\n.8665\n.8869\n.9049\n.9207\n.9345\n.9463\n.9564\n.9649\n.9719\n.9778\n.9826\n.9864\n.9896\n.9920\n.9940\n.9955\n.9966\n.9975\n.9982\n.9987\n.9991\n.9993\n.9995\n.9997\n\n.5080\n.5478\n.5871\n.6255\n.6628\n.6985\n.7324\n.7642\n.7939\n.8212\n.8461\n.8686\n.8888\n.9066\n.9222\n.9357\n.9474\n.9573\n.9656\n.9726\n.9783\n.9830\n.9868\n.9898\n.9922\n.9941\n.9956\n.9967\n.9976\n.9982\n.9987\n.9991\n.9994\n.9995\n.9997\n\n.5120\n.5517\n.5910\n.6293\n.6664\n.7019\n.7357\n.7673\n.7967\n.8238\n.8485\n.8708\n.8907\n.9082\n.9236\n.9370\n.9484\n.9582\n.9664\n.9732\n.9788\n.9834\n.9871\n.9901\n.9925\n.9943\n.9957\n.9968\n.9977\n.9983\n.9988\n.9991\n.9994\n.9996\n.9997\n\n.5160\n.5557\n.5948\n.6331\n.6700\n.7054\n.7389\n.7704\n.7995\n.8264\n.8508\n.8729\n.8925\n.9099\n.9251\n.9382\n.9495\n.9591\n.9671\n.9738\n.9793\n.9838\n.9875\n.9904\n.9927\n.9945\n.9959\n.9969\n.9977\n.9984\n.9988\n.9992\n.9994\n.9996\n.9997\n\n.5199\n.5596\n.5987\n.6368\n.6736\n.7088\n.7422\n.7734\n.8023\n.8289\n.8531\n.8749\n.8944\n.9115\n.9265\n.9394\n.9505\n.9599\n.9678\n.9744\n.9798\n.9842\n.9878\n.9906\n.9929\n.9946\n.9960\n.9970\n.9978\n.9984\n.9989\n.9992\n.9994\n.9996\n.9997\n\n.5239\n.5636\n.6026\n.6406\n.6772\n.7123\n.7454\n.7764\n.8051\n.8315\n.8554\n.8770\n.8962\n.9131\n.9279\n.9406\n.9515\n.9608\n.9686\n.9750\n.9803\n.9846\n.9881\n.9909\n.9931\n.9948\n.9961\n.9971\n.9979\n.9985\n.9989\n.9992\n.9994\n.9996\n.9997\n\n.5279\n.5675\n.6064\n.6443\n.6808\n.7157\n.7486\n.7794\n.8078\n.8340\n.8577\n.8790\n.8980\n.9147\n.9292\n.9418\n.9525\n.9616\n.9693\n.9756\n.9808\n.9850\n.9884\n.9911\n.9932\n.9949\n.9962\n.9972\n.9979\n.9985\n.9989\n.9992\n.9995\n.9996\n.9997\n\n.5319\n.5714\n.6103\n.6480\n.6844\n.7190\n.7517\n.7823\n.8106\n.8365\n.8599\n.8810\n.8997\n.9162\n.9306\n.9429\n.9535\n.9625\n.9699\n.9761\n.9812\n.9854\n.9887\n.9913\n.9934\n.9951\n.9963\n.9973\n.9980\n.9986\n.9990\n.9993\n.9995\n.9996\n.9997\n\n.5359\n.5753\n.6141\n.6517\n.6879\n.7224\n.7549\n.7852\n.8133\n.8389\n.8621\n.8830\n.9015\n.9177\n.9319\n.9441\n.9545\n.9633\n.9706\n.9767\n.9817\n.9857\n.9890\n.9916\n.9936\n.9952\n.9964\n.9974\n.9981\n.9986\n.9990\n.9993\n.9995\n.9997\n.9998\n\nx\n\n.0\n.1\n.2\n.3\n.4\n.5\n.6\n.7\n.8\n.9\n1.0\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\n1.7\n1.8\n1.9\n2.0\n2.1\n2.2\n2.3\n2.4\n2.5\n2.6\n2.7\n2.8\n2.9\n3.0\n3.1\n3.2\n3.3\n3.4\n\nthe proof of equation (4.1), which follows from the symmetry of the standard nor-\nmal density, is left as an exercise. this equation states that if z is a standard normal\nrandom variable, then\n\np{z \u2026 \u2212x} = p{z > x}\n\n\u2212 q < x < q\n\nsince z = (x \u2212 \u03bc)/\u03c3 is a standard normal random variable whenever x is normally\ndistributed with parameters \u03bc and \u03c3 2, it follows that the distribution function of x\ncan be expressed as\n\nfx (a) = p{x \u2026 a} = p\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\nx \u2212 \u03bc\n\n\u03c3\n\n\u2026 a \u2212 \u03bc\n\n\u03c3\n\n= \u0001\n\na \u2212 \u03bc\n\n\u03c3\n\n "}, {"Page_number": 217, "text": "202\n\nchapter 5\n\ncontinuous random variables\n\nexample 4b\nif x is a normal random variable with parameters \u03bc = 3 and \u03c3 2 = 9, find (a) p{2 <\nx < 5}; (b) p{x > 0}; (c) p{|x \u2212 3| > 6}.\nsolution.\n\n(a)\n\np{2 < x < 5} = p\n= p\n= \u0001\n\np{x > 0} = p\n\n/\n\n5 \u2212 3\n\n3\n\n<\n\n(cid:3)(cid:8)\n\n1\n3\n\nl .3779\n\n%\n2 \u2212 3\n%\n3\n(cid:2)\n(cid:3)\n\u22121\n3\n(cid:3)\n(cid:2)\n\n2\n3\n2\n3\n\n<\n\nx \u2212 3\n/\n3\n(cid:3)\n(cid:2)\n2\n< z <\n3\n(cid:7)\n\u2212 \u0001\n\u22121\n(cid:2)\n3\n1 \u2212 \u0001\n/\n\n\u2212\n\n0 \u2212 3\n\n>\n\n3\n\n= \u0001\n%\n\nx \u2212 3\n\n3\n\n= p{z > \u22121}\n= 1 \u2212 \u0001(\u22121)\n= \u0001(1)\nl .8413\n\n(b)\n\n(c)\n\n/\np{|x \u2212 3| > 6} = p{x > 9} + p{x < \u22123}\n+ p\n\nx \u2212 3\n\n9 \u2212 3\n\n%\n\n3\n\n>\n\n= p\n3\n= p{z > 2} + p{z < \u22122}\n= 1 \u2212 \u0001(2) + \u0001(\u22122)\n= 2[1 \u2212 \u0001(2)]\nl .0456\n\n%\n\n/\n\nx \u2212 3\n\n3\n\n<\n\n\u22123 \u2212 3\n\n3\n\n.\n\nexample 4c\nan examination is frequently regarded as being good (in the sense of determining\na valid grade spread for those taking it) if the test scores of those taking the exami-\nnation can be approximated by a normal density function. (in other words, a graph\nof the frequency of grade scores should have approximately the bell-shaped form of\nthe normal density.) the instructor often uses the test scores to estimate the normal\nparameters \u03bc and \u03c3 2 and then assigns the letter grade a to those whose test score\nis greater than \u03bc + \u03c3 , b to those whose score is between \u03bc and \u03bc + \u03c3 , c to those\nwhose score is between \u03bc \u2212 \u03c3 and \u03bc, d to those whose score is between \u03bc \u2212 2\u03c3\nand \u03bc \u2212 \u03c3 , and f to those getting a score below \u03bc \u2212 2\u03c3 . (this strategy is sometimes\nreferred to as grading \u201con the curve.\u201d) since\n\n "}, {"Page_number": 218, "text": "p{x > \u03bc + \u03c3} = p\np{\u03bc < x < \u03bc + \u03c3} = p\np{\u03bc \u2212 \u03c3 < x < \u03bc} = p\n\n/\n\nsection 5.4\nx \u2212 \u03bc\n\n> 1\nx \u2212 \u03bc\nx \u2212 \u03bc\n\n\u03c3\n\n\u03c3\n\n0 <\n\u22121 <\n\nnormal random variables 203\n/\n= 1 \u2212 \u0001(1) l .1587\n\n/\n= \u0001(1) \u2212 \u0001(0) l .3413\n\n< 1\n\np{\u03bc \u2212 2\u03c3 < x < \u03bc \u2212 \u03c3} = p\n\n\u03c3\n\n< 0\n/\n= \u0001(0) \u2212 \u0001(\u22121) l .3413\n< \u22121\n/\n= \u0001(2) \u2212 \u0001(1) l .1359\n\nx \u2212 \u03bc\n\n\u22122 <\n\n\u03c3\n\nx \u2212 \u03bc\n\n\u03c3\n\np{x < \u03bc \u2212 2\u03c3} = p\n\n< \u22122\n\n= \u0001(\u22122) l .0228\n\n%\n%\n%\n%\n%\n\nit follows that approximately 16 percent of the class will receive an a grade on the\nexamination, 34 percent a b grade, 34 percent a c grade, and 14 percent a d grade; 2\n.\npercent will fail.\n\nexample 4d\nan expert witness in a paternity suit testifies that the length (in days) of human ges-\ntation is approximately normally distributed with parameters \u03bc = 270 and \u03c3 2 = 100.\nthe defendant in the suit is able to prove that he was out of the country during a\nperiod that began 290 days before the birth of the child and ended 240 days before\nthe birth. if the defendant was, in fact, the father of the child, what is the probability\nthat the mother could have had the very long or very short gestation indicated by the\ntestimony?\n\nsolution. let x denote the length of the gestation, and assume that the defendant\nis the father. then the probability that the birth could occur within the indicated\nperiod is\n\n%\np{x > 290orx < 240} = p{x > 290} + p{x < 240}\n\n/\n\n%\n\n/\n\nx \u2212 270\n\n> 2\n\n+ p\n= p\n= 1 \u2212 \u0001(2) + 1 \u2212 \u0001(3)\nl .0241\n\n10\n\nx \u2212 270\n\n10\n\n< \u22123\n\n.\n\nexample 4e\nsuppose that a binary message\u2014either 0 or 1\u2014must be transmitted by wire from\nlocation a to location b. however, the data sent over the wire are subject to a channel\nnoise disturbance, so, to reduce the possibility of error, the value 2 is sent over the\nwire when the message is 1 and the value \u22122 is sent when the message is 0. if x, x = ;2,\nis the value sent at location a, then r, the value received at location b, is given by\nr = x + n, where n is the channel noise disturbance. when the message is received\nat location b, the receiver decodes it according to the following rule:\n\nif r \u00fa .5, then 1 is concluded.\nif r < .5, then 0 is concluded.\n\n "}, {"Page_number": 219, "text": "204\n\nchapter 5\n\ncontinuous random variables\n\nbecause the channel noise is often normally distributed, we will determine the error\nprobabilities when n is a standard normal random variable.\n\ntwo types of errors can occur: one is that the message 1 can be incorrectly deter-\nmined to be 0, and the other is that 0 can be incorrectly determined to be 1. the first\ntype of error will occur if the message is 1 and 2 + n < .5, whereas the second will\noccur if the message is 0 and \u22122 + n \u00fa .5. hence,\n\np{error|message is 1} = p{n < \u22121.5}\n\n= 1 \u2212 \u0001(1.5) l .0668\n\nand\n\np{error|message is 0} = p{n \u00fa 2.5}\n\n= 1 \u2212 \u0001(2.5) l .0062\n\n.\n\n5.4.1 the normal approximation to the binomial distribution\nan important result in probability theory known as the demoivre\u2013laplace limit the-\norem states that when n is large, a binomial random variable with parameters n and p\nwill have approximately the same distribution as a normal random variable with the\nsame mean and variance as the binomial. this result was proved originally for the spe-\ncial case of p = 1\n2 by demoivre in 1733 and was then extended to general p by laplace\nin 1812. it formally states that if we \u201cstandardize\u201d the binomial by first subtracting its\nnp(1 \u2212 p), then the\nmean np and then dividing the result by its standard deviation\ndistribution function of this standardized random variable (which has mean 0 and\nvariance 1) will converge to the standard normal distribution function as n\u2192q.\n\n.\n\nthe demoivre\u2013laplace limit theorem\n\nif sn denotes the number of successes that occur when n independent trials, each\nresulting in a success with probability p, are performed, then, for any a < b,\n\n0\n\n.\nsn \u2212 np\nnp(1 \u2212 p)\n\n7\n\n\u2026 b\n\n\u2192\u0001(b) \u2212 \u0001(a)\n\np\n\na \u2026\n\nas n\u2192q.\n\nbecause the preceding theorem is only a special case of the central limit theorem,\n\nwhich is presented in chapter 8, we shall not present a proof.\n\nnote that we now have two possible approximations to binomial probabilities: the\npoisson approximation, which is good when n is large and p is small, and the normal\napproximation, which can be shown to be quite good when np(1 \u2212 p) is large. (see\nfigure 5.6.) [the normal approximation will, in general, be quite good for values of n\nsatisfying np(1 \u2212 p) \u00fa 10.]\n\nexample 4f\nlet x be the number of times that a fair coin that is flipped 40 times lands on heads.\nfind the probability that x = 20. use the normal approximation and then compare\nit with the exact solution.\n\n "}, {"Page_number": 220, "text": "section 5.4\n\nnormal random variables 205\n\n(10, 0.7)\n\n(20, 0.7)\n\n0.20\n\n0.15\n\n0.10\n\n0.05\n\n0\n\n2\n\n4\n\nx\n\n6\n\n8\n\n10\n\n0.0\n\n0\n\n5\n\n15\n\n20\n\n10\nx\n\n(50, 0.7)\n\n(30, 0.7)\n\n0\n\n5\n\n10\n\n15\nx\n\n20\n\n25\n\n30\n\n0.14\n0.12\n0.10\n0.08\n0.06\n0.04\n0.02\n0.0\n\n0.30\n0.25\n0.20\n0.15\n0.10\n0.05\n0.0\n\n0.16\n0.14\n0.12\n0.10\n0.08\n0.06\n0.04\n0.02\n0.0\n\n0\n\n10\n\n20\n\nx\n\n30\n\n40\n\n50\n\nfigure 5.6: the probability mass function of a binomial (n, p) random variable becomes more and more\n\u201cnormal\u201d as n becomes larger and larger.\n\nsolution. to employ the normal approximation, note that because the binomial is\na discrete integer-valued random variable, whereas the normal is a continuous ran-\ndom variable, it is best to write p{x = i} as p{i \u2212 1/2 < x < i + 1/2} before\napplying the normal approximation (this is called the continuity correction). doing\nso gives\n\np{x = 20} = p{19.5 \u2026 x < 20.5}\n\n7\n\nx \u2212 20\u221a\n\n<\n\n10\n\n20.5 \u2212 20\n\n\u221a\n10\n\n<\n\n7\n\n= p\n\n0\n19.5 \u2212 20\n0\n\n\u221a\n10\n\u2212.16 <\n(cid:2)\n\nx \u2212 20\u221a\nl p\n< .16\nl \u0001(.16) \u2212 \u0001(\u2212.16) l .1272\n(cid:3)(cid:2)\n(cid:3)40\n\n10\n\np{x = 20} =\n\n40\n20\n\n1\n2\n\nl .1254\n\n.\n\nthe exact result is\n\nexample 4g\nthe ideal size of a first-year class at a particular college is 150 students. the college,\nknowing from past experience that, on the average, only 30 percent of those accepted\nfor admission will actually attend, uses a policy of approving the applications of 450\nstudents. compute the probability that more than 150 first-year students attend this\ncollege.\n\nsolution. if x denotes the number of students that attend, then x is a binomial ran-\ndom variable with parameters n = 450 and p = .3. using the continuity correction,\n\n "}, {"Page_number": 221, "text": "206\n\nchapter 5\n\ncontinuous random variables\n\nwe see that the normal approximation yields\n\np{x \u00fa 150.5} = p\n\n0\n\nx \u2212 (450)(.3)\n\u221a\n450(.3)(.7)\n\nl 1 \u2212 \u0001(1.59)\nl .0559\n\n7\n\n\u00fa 150.5 \u2212 (450)(.3)\n\n\u221a\n450(.3)(.7)\n\nhence, less than 6 percent of the time do more than 150 of the first 450 accepted\n.\nactually attend. (what independence assumptions have we made?)\n\nexample 4h\nto determine the effectiveness of a certain diet in reducing the amount of cholesterol\nin the bloodstream, 100 people are put on the diet. after they have been on the diet\nfor a sufficient length of time, their cholesterol count will be taken. the nutritionist\nrunning this experiment has decided to endorse the diet if at least 65 percent of the\npeople have a lower cholesterol count after going on the diet. what is the probability\nthat the nutritionist endorses the new diet if, in fact, it has no effect on the cholesterol\nlevel?\n\nsolution. let us assume that if the diet has no effect on the cholesterol count, then,\nstrictly by chance, each person\u2019s count will be lower than it was before the diet with\nprobability 1\n2. hence, if x is the number of people whose count is lowered, then the\nprobability that the nutritionist will endorse the diet when it actually has no effect on\nthe cholesterol count is\n\n(cid:2)\n\n100(cid:6)\n\ni=65\n\n(cid:3)(cid:2)\n\n100\n\ni\n\n1\n2\n\n(cid:3)100 = p{x \u00fa 64.5}\n\u23a7\u23aa\u23a8\n8\n\u23aa\u23a9x \u2212 (100)( 1\n100( 1\n2\nl 1 \u2212 \u0001(2.9)\nl .0019\n\n= p\n\n)( 1\n2\n\n2\n\n)\n\n\u23ab\u23aa\u23ac\n\u23aa\u23ad\n\n\u00fa 2.9\n\n)\n\n.\n\nexample 4i\nfifty-two percent of the residents of new york city are in favor of outlawing cigarette\nsmoking in publicly owned areas. approximate the probability that more than 50\npercent of a random sample of n people from new york are in favor of this prohibi-\ntion when\n(a) n = 11\n(b) n = 101\n(c) n = 1001\nhow large would n have to be to make this probability exceed .95?\n\nsolution. let n denote the number of residents of new york city. to answer the\npreceding question, we must first understand that a random sample of size n is a\nsample such that the n people were chosen in such a manner that each of the\nsubsets of n people had the same chance of being the chosen subset. consequently,\n\nn\nn\n\n(cid:2)\n\n(cid:3)\n\n "}, {"Page_number": 222, "text": "section 5.4\n\nnormal random variables 207\n\nsn, the number of people in the sample who are in favor of the smoking prohibition,\nis a hypergeometric random variable. that is, sn has the same distribution as the\nnumber of white balls obtained when n balls are chosen from an urn of n balls, of\nwhich .52n are white. but because n and .52n are both large in comparison with the\nsample size n, it follows from the binomial approximation to the hypergeometric (see\nsection 4.8.3) that the distribution of sn is closely approximated by a binomial dis-\ntribution with parameters n and p = .52. the normal approximation to the binomial\ndistribution then shows that\n\n7\n\np{sn > .5n} = p\n\n0\n0\n\nsn \u2212 .52n\n\u221a\nn(.52)(.48)\nsn \u2212 .52n\n= p\n\u221a\nn(.52)(.48)\n\u221a\nl \u0001(.04\nn)\n\n>\n\n.5n \u2212 .52n\n\u221a\n7\nn(.52)(.48)\n\n\u221a\n> \u2212.04\nn\n\nthus,\n\n\u23a7\u23a8\nif n = 11\n\u23a9 \u0001(.1328) = .5528,\nif n = 101\n\u0001(.4020) = .6562,\n\u0001(1.2665) = .8973, if n = 1001\n\u221a\nn) > .95. because\nin order for this probability to be at least .95, we would need \u0001(.04\n\u0001(x) is an increasing function and \u0001(1.645) = .95, this means that\n\np{sn > .5n} l\n\n\u221a\n.04\nn > 1.645\n\nor\n\nn \u00fa 1691.266\n\nthat is, the sample size would have to be at least 1692.\n\n.\n\nhistorical notes concerning the normal distribution\n\nthe normal distribution was introduced by the french mathematician abraham\ndemoivre in 1733. demoivre, who used this distribution to approximate proba-\nbilities connected with coin tossing, called it the exponential bell-shaped curve. its\nusefulness, however, became truly apparent only in 1809, when the famous german\nmathematician karl friedrich gauss used it as an integral part of his approach to\npredicting the location of astronomical entities. as a result, it became common after\nthis time to call it the gaussian distribution.\n\nduring the mid- to late 19th century, however, most statisticians started to\nbelieve that the majority of data sets would have histograms conforming to the\ngaussian bell-shaped form. indeed, it came to be accepted that it was \u201cnormal\u201d\nfor any well-behaved data set to follow this curve. as a result, following the lead of\nthe british statistician karl pearson, people began referring to the gaussian curve\nby calling it simply the normal curve. (a partial explanation as to why so many data\nsets conform to the normal curve is provided by the central limit theorem, which is\npresented in chapter 8.)\n\nabraham demoivre (1667\u20131754)\ntoday there is no shortage of statistical consultants, many of whom ply their trade\nin the most elegant of settings. however, the first of their breed worked, in the early\n\n "}, {"Page_number": 223, "text": "208\n\nchapter 5\n\ncontinuous random variables\n\nyears of the 18th century, out of a dark, grubby betting shop in long acres, london,\nknown as slaughter\u2019s coffee house. he was abraham demoivre, a protestant\nrefugee from catholic france, and, for a price, he would compute the probability of\ngambling bets in all types of games of chance.\n\nalthough demoivre, the discoverer of the normal curve, made his living at the\ncoffee shop, he was a mathematician of recognized abilities. indeed, he was a mem-\nber of the royal society and was reported to be an intimate of isaac newton.\n\nlisten to karl pearson imagining demoivre at work at slaughter\u2019s coffee\nhouse: \u201ci picture demoivre working at a dirty table in the coffee house with a broken-\ndown gambler beside him and isaac newton walking through the crowd to his corner\nto fetch out his friend. it would make a great picture for an inspired artist.\u201d\n\nkarl friedrich gauss\nkarl friedrich gauss (1777\u20131855), one of the earliest users of the normal curve,\nwas one of the greatest mathematicians of all time. listen to the words of the\nwell-known mathematical historian e. t. bell, as expressed in his 1954 book men\nof mathematics: in a chapter entitled \u201cthe prince of mathematicians,\u201d he writes,\n\u201carchimedes, newton, and gauss; these three are in a class by themselves among\nthe great mathematicians, and it is not for ordinary mortals to attempt to rank them\nin order of merit. all three started tidal waves in both pure and applied mathemat-\nics. archimedes esteemed his pure mathematics more highly than its applications;\nnewton appears to have found the chief justification for his mathematical inventions\nin the scientific uses to which he put them; while gauss declared it was all one to him\nwhether he worked on the pure or on the applied side.\u201d\u2021\n\n5.5 exponential random variables\n\na continuous random variable whose probability density function is given, for some\n\u03bb > 0, by\n\n%\n\nf (x) =\n\n\u2212\u03bbx\n\n\u03bbe\n0\n\nif x \u00fa 0\nif x < 0\n\nis said to be an exponential random variable (or, more simply, is said to be exponen-\ntially distributed) with parameter \u03bb. the cumulative distribution function f(a) of an\nexponential random variable is given by\n\n*\n\na\n\nf(a) = p{x \u2026 a}\n66a\n\u2212\u03bbx dx\n\n=\n\u03bbe\n= \u2212e\n\u2212\u03bbx\n= 1 \u2212 e\n\n0\n\u2212\u03bba\n\n0\n\na \u00fa 0\n\nnote that f(q) = - q\n\n0\n\nbe shown to equal the reciprocal of the expected value.\n\n\u2212\u03bbx dx = 1, as, of course, it must. the parameter \u03bb will now\n\u03bbe\n\nexample 5a\nlet x be an exponential random variable with parameter \u03bb. calculate (a) e[x] and\n(b) var(x).\n\n "}, {"Page_number": 224, "text": "section 5.5\n\nexponential random variables 209\n\nsolution. (a) since the density function is given by\n\n%\n\nf (x) =\n\nx \u00fa 0\nx < 0\n\n\u2212\u03bbx\n\n\u03bbe\n0\n\n* q\n\nxn\u03bbe\n\ne[xn] =\n\u2212\u03bbx dx\n* q\n\u2212\u03bbx = dv and u = xn) yields\n\n0\n\n+\n\n\u2212\u03bbxnxn\u22121 dx\ne\n\n0\n\n\u2212\u03bbxxn\u22121 dx\n\n\u03bbe\n\nwe obtain, for n > 0,\n\nintegrating by parts (with \u03bbe\n\n\u2212\u03bbx|q\n0\n\ne[xn] = \u2212xne\n= 0 + n\n= n\n\u03bb\nletting n = 1 and then n = 2 gives\n\n* q\ne[xn\u22121]\n\n\u03bb\n\n0\n\ne[x] = 1\n\n\u03bb\n\n(b) hence,\n\ne[x2] = 2\n\n\u03bb\n\nvar(x) = 2\n\u03bb2\n\ne[x] = 2\n(cid:2)\n\u03bb2\n\n(cid:3)2 = 1\n\n\u2212\n\n1\n\u03bb\n\n\u03bb2\n\nthus, the mean of the exponential is the reciprocal of its parameter \u03bb, and the vari-\n.\nance is the mean squared.\n\nin practice, the exponential distribution often arises as the distribution of the\namount of time until some specific event occurs. for instance, the amount of time\n(starting from now) until an earthquake occurs, or until a new war breaks out, or\nuntil a telephone call you receive turns out to be a wrong number are all random\nvariables that tend in practice to have exponential distributions. (for a theoretical\nexplanation of this phenomenon, see section 4.7.)\n\nexample 5b\nsuppose that the length of a phone call in minutes is an exponential random variable\nwith parameter \u03bb = 1\n10. if someone arrives immediately ahead of you at a public\ntelephone booth, find the probability that you will have to wait\n(a) more than 10 minutes;\n(b) between 10 and 20 minutes.\n\nsolution. let x denote the length of the call made by the person in the booth. then\nthe desired probabilities are\n(a)\n\np{x > 10} = 1 \u2212 f(10)\n\u22121 l .368\n\n= e\n\n "}, {"Page_number": 225, "text": "210\n\nchapter 5\n\ncontinuous random variables\n\n(b)\n\np{10 < x < 20} = f(20) \u2212 f(10)\n\n= e\n\n\u22121 \u2212 e\n\n\u22122 l .233\n\nwe say that a nonnegative random variable x is memoryless if\n\np{x > s + t | x > t} = p{x > s}\n\nfor all s, t \u00fa 0\n\n.\n\n(5.1)\n\nif we think of x as being the lifetime of some instrument, equation (5.1) states that\nthe probability that the instrument survives for at least s+ t hours, given that it has\nsurvived t hours, is the same as the initial probability that it survives for at least s\nhours. in other words, if the instrument is alive at age t, the distribution of the remain-\ning amount of time that it survives is the same as the original lifetime distribution.\n(that is, it is as if the instrument does not \u201cremember\u201d that it has already been in use\nfor a time t.)\n\nequation (5.1) is equivalent to\n\np{x > s + t, x > t}\n\np{x > t}\n\n= p{x > s}\n\nor\n\np{x > s + t} = p{x > s}p{x > t}\n\n(5.2)\n\u2212\u03bb(s+t) =\n\u2212\u03bbt), it follows that exponentially distributed random variables are memoryless.\n\nsince equation (5.2) is satisfied when x is exponentially distributed (for e\n\u2212\u03bbse\ne\n\nexample 5c\nconsider a post office that is staffed by two clerks. suppose that when mr. smith\nenters the system, he discovers that ms. jones is being served by one of the clerks\nand mr. brown by the other. suppose also that mr. smith is told that his service will\nbegin as soon as either ms. jones or mr. brown leaves. if the amount of time that\na clerk spends with a customer is exponentially distributed with parameter \u03bb, what\nis the probability that, of the three customers, mr. smith is the last to leave the post\noffice?\n\nsolution. the answer is obtained by reasoning as follows: consider the time at which\nmr. smith first finds a free clerk. at this point, either ms. jones or mr. brown would\nhave just left, and the other one would still be in service. however, because the expo-\nnential is memoryless, it follows that the additional amount of time that this other\nperson (either ms. jones or mr. brown) would still have to spend in the post office is\nexponentially distributed with parameter \u03bb. that is, it is the same as if service for that\nperson were just starting at this point. hence, by symmetry, the probability that the\n.\nremaining person finishes before smith leaves must equal 1\n2.\n\nit turns out that not only is the exponential distribution memoryless, but it is also\nthe unique distribution possessing this property. to see this, suppose that x is mem-\noryless and let f(x) = p{x > x}. then, by equation (5.2),\n\nthat is, f(\u00b7) satisfies the functional equation\n\nf(s + t) = f(s)f(t)\n\ng(s + t) = g(s)g(t)\n\n "}, {"Page_number": 226, "text": "section 5.5\n\nexponential random variables 211\n\nhowever, it turns out that the only right continuous solution of this functional\nequation is\u2020\n\ng(x) = e\n\n\u2212\u03bbx\n\n(5.3)\n\nand, since a distribution function is always right continuous, we must have\n\nf(x) = e\n\n\u2212\u03bbx or f(x) = p{x \u2026 x} = 1 \u2212 e\n\n\u2212\u03bbx\n\nwhich shows that x is exponentially distributed.\n\nexample 5d\nsuppose that the number of miles that a car can run before its battery wears out is\nexponentially distributed with an average value of 10,000 miles. if a person desires to\ntake a 5000-mile trip, what is the probability that he or she will be able to complete the\ntrip without having to replace the car battery? what can be said when the distribution\nis not exponential?\n\nsolution. it follows by the memoryless property of the exponential distribution that\nthe remaining lifetime (in thousands of miles) of the battery is exponential with\nparameter \u03bb = 1\n\n10. hence, the desired probability is\n\np{remaining lifetime > 5} = 1 \u2212 f(5) = e\n\n\u22125\u03bb = e\n\n\u22121/2 l .604\n\nhowever, if the lifetime distribution f is not exponential, then the relevant probabil-\nity is\n\np{lifetime > t + 5|lifetime > t} = 1 \u2212 f(t + 5)\n1 \u2212 f(t)\n\nwhere t is the number of miles that the battery had been in use prior to the start of\nthe trip. therefore, if the distribution is not exponential, additional information is\nneeded (namely, the value of t) before the desired probability can be calculated. .\na variation of the exponential distribution is the distribution of a random variable\nthat is equally likely to be either positive or negative and whose absolute value is\nexponentially distributed with parameter \u03bb, #\u03bb \u00fa 0. such a random variable is said to\nhave a laplace distribution,\u2021 and its density is given by\n\n\u2020one can prove equation (5.3) as follows: if g(s + t) = g(s)g(t), then\n\nf (x) = 1\n2\n(cid:3)\n(cid:2)\n\n\u2212\u03bb|x| \u2212 q < x < q\n\u03bbe\n(cid:3)\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\ng\n\n2\nn\n\n= g\n\n1\nn\n\n1\nn\n\n+ 1\nn\n(cid:2)\n\n= g2\n(cid:3)\n\n+ 1\nn\n\n+ \u00b7\u00b7\u00b7 + 1\nn\n\n= gn\n\n1\nn\n\n(cid:3)\n\n(cid:2)\n\n1\nn\n\nor\n\ng\n\n= (g(1))1/n\n\nand repeating this yields g(m/n) = gm(1/n). also,\n(cid:3)\n\n(cid:2)\n\n1\nn\n\ng(1) = g\n(cid:30)(cid:3)2\n(cid:2)\n(cid:29)\n\ng\n\n1\n2\n\nhence, g(m/n) = (g(1))m/n, which, since g is right continuous, implies that g(x) = (g(1))x.\n\nbecause g(1) =\n\n\u00fa 0, we obtain g(x) = e\n\n\u2212\u03bbx, where \u03bb = \u2212 log(g(1)).\n\n\u2021it also is sometimes called the double exponential random variable.\n\n "}, {"Page_number": 227, "text": "212\n\nchapter 5\n\ncontinuous random variables\n\nits distribution function is given by\n\n1\n2\n\n*\n*\n\nx\n\u2212q\n0\n\u2212q\n\n\u23a7\u23aa\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23aa\u23a9\n\u23a7\u23a8\n\u23a9 1\n2e\u03bbx\n1 \u2212 1\n2e\n\n1\n2\n\n\u03bbe\u03bbx dx\n\n\u03bbe\u03bbx dx + 1\n2\n\n\u2212\u03bbx\n\nx < 0\nx > 0\n\nf(x) =\n\n=\n\n*\n\n0\n\nx < 0\n\nx\n\n\u2212\u03bbx dx\n\n\u03bbe\n\nx > 0\n\nexample 5e\nconsider again example 4e, which supposes that a binary message is to be transmit-\nted from a to b, with the value 2 being sent when the message is 1 and \u22122 when it is\n0. however, suppose now that, rather than being a standard normal random variable,\nthe channel noise n is a laplacian random variable with parameter \u03bb = 1. suppose\nagain that if r is the value received at location b, then the message is decoded as\nfollows:\n\nif r \u00fa .5, then 1 is concluded.\nif r < .5, then 0 is concluded.\n\nin this case, where the noise is laplacian with parameter \u03bb = 1, the two types of\nerrors will have probabilities given by\n\np{error|message 1 is sent} = p{n < \u22121.5}\n\n= 1\n\u22121.5\ne\n2\nl .1116\n\np{error|message 0 is sent} = p{n \u00fa 2.5}\n\n= 1\n\u22122.5\ne\n2\nl .041\n\non comparing this with the results of example 4e, we see that the error probabilities\nare higher when the noise is laplacian with \u03bb = 1 than when it is a standard normal\nvariable.\n\n5.5.1 hazard rate functions\nconsider a positive continuous random variable x that we interpret as being the\nlifetime of some item. let x have distribution function f and density f . the hazard\nrate (sometimes called the failure rate) function \u03bb(t) of f is defined by\n\n\u03bb(t) = f (t)\nf(t)\n\n, where f = 1 \u2212 f\n\nto interpret \u03bb(t), suppose that the item has survived for a time t and we desire the\nprobability that it will not survive for an additional time dt. that is, consider p{x \u2208\n(t, t + dt)|x > t}. now,\n\n "}, {"Page_number": 228, "text": "section 5.5\n\nexponential random variables 213\n\np{x \u2208 (t, t + dt)|x > t} = p{x \u2208 (t, t + dt), x > t}\n\np{x > t}\n= p{x \u2208 (t, t + dt)}\nl f (t)\nf(t)\n\np{x > t}\ndt\n\nthus, \u03bb(t) represents the conditional probability intensity that a t-unit-old item will fail.\nsuppose now that the lifetime distribution is exponential. then, by the memoryless\nproperty, it follows that the distribution of remaining life for a t-year-old item is the\nsame as that for a new item. hence, \u03bb(t) should be constant. in fact, this checks out,\nsince\n\n\u03bb(t) = f (t)\nf(t)\n\u2212\u03bbt\n= \u03bbe\ne\u2212\u03bbt\n= \u03bb\n\nthus, the failure rate function for the exponential distribution is constant. the param-\neter \u03bb is often referred to as the rate of the distribution.\n\nit turns out that the failure rate function \u03bb(t) uniquely determines the distribu-\n\ntion f. to prove this, note that, by definition,\n\u03bb(t) = d\n\nintegrating both sides yields\n\nor\n\nletting t = 0 shows that k = 0; thus,\n\ndt f(t)\n1 \u2212 f(t)\n*\n0\n\n0\n\nt\n\n\u2212\n*\n\n0\n\nlog(1 \u2212 f(t)) = \u2212\n\n1 \u2212 f(t) = ek exp\n0\n\n\u03bb(t) dt + k\n7\n*\n\n\u03bb(t) dt\n\nt\n\n0\n\nf(t) = 1 \u2212 exp\n\n\u2212\n\nt\n\n\u03bb(t) dt\n\n7\n\n(5.4)\n\nhence, a distribution function of a positive continuous random variable can be\nspecified by giving its hazard rate function. for instance, if a random variable has a\nlinear hazard rate function\u2014that is, if\n\n\u03bb(t) = a + bt\n\nthen its distribution function is given by\n\nf(t) = 1 \u2212 e\nand differentiation yields its density, namely,\n\n\u2212at\u2212bt2/2\n\n "}, {"Page_number": 229, "text": "214\n\nchapter 5\n\ncontinuous random variables\n\nf (t) = (a + bt)e\n\n\u2212(at+bt2/2)\n\nt \u00fa 0\n\nwhen a = 0, the preceding equation is known as the rayleigh density function.\n\nexample 5f\none often hears that the death rate of a person who smokes is, at each age, twice that\nof a nonsmoker. what does this mean? does it mean that a nonsmoker has twice the\nprobability of surviving a given number of years as does a smoker of the same age?\n\nsolution. if \u03bbs(t) denotes the hazard rate of a smoker of age t and \u03bbn(t) that of a\nnonsmoker of age t, then the statement at issue is equivalent to the statement that\n\n\u03bbs(t) = 2\u03bbn(t)\n\nthe probability that an a-year-old nonsmoker will survive until age b, a < b, is\n\np{a-year-old nonsmoker reaches age b}\n\n= p{nonsmoker\u2019s lifetime > b|nonsmoker\u2019s lifetime > a}\n= 1 \u2212 fnon(b)\n1 \u2212 fnon(a)\nexp\n\n\u03bbn(t) dt\n\n\u2212\n\nb\n\n*\n*\n*\n\n0\n\n0\n\n0\n0\n0\n\n\u2212\n\n\u2212\n\n=\n\nexp\n\n= exp\n\na\n\n\u03bbn(t) dt\n\nb\n\na\n\n\u03bbn(t) dt\n\n7\n7\n7\n\nfrom (5.4)\n\nwhereas the corresponding probability for a smoker is, by the same reasoning,\n\np{a-year-old smoker reaches age b} = exp\n\n\u2212\n\n\u03bbs(t) dt\n\n0\n0\n\nb\n\n*\n*\n*\n\na\n\nb\n\na\n\nb\n\na\n\n= exp\n\u23a1\n\u23a3exp\n\n=\n\n\u22122\n0\n\n\u2212\n\n\u03bbn(t) dt\n\n\u03bbn(t) dt\n\n7\n7\n7\u23a4\n\u23a62\n\nin other words, for two people of the same age, one of whom is a smoker and\nthe other a nonsmoker, the probability that the smoker survives to any given age\nis the square (not one-half) of the corresponding probability for a nonsmoker. for\ninstance, if \u03bbn(t) = 1\n30, 50 \u2026 t \u2026 60, then the probability that a 50-year-old nonsmoker\n\u22121/3 l .7165, whereas the corresponding probability for a smoker\nreaches age 60 is e\n\u22122/3 l .5134.\n.\nis e\n\n "}, {"Page_number": 230, "text": "5.6 other continuous distributions\n\nsection 5.6\n\nother continuous distributions 215\n\n5.6.1 the gamma distribution\na random variable is said to have a gamma distribution with parameters (\u03b1, \u03bb), \u03bb > 0,\n\u03b1 > 0, if its density function is given by\n\n\u23a7\u23aa\u23a8\n\u23aa\u23a9 \u03bbe\n\n0\n\nf (x) =\n\n\u2212\u03bbx(\u03bbx)\u03b1\u22121\n\n\u0001(\u03b1)\n\nx \u00fa 0\nx < 0\n\n* q\n* q\n\n0\n\nwhere \u0001(\u03b1), called the gamma function, is defined as\n\u2212yy\u03b1\u22121 dy\ne\n\n\u0001(\u03b1) =\n\nintegration of \u0001(\u03b1) by parts yields\n+\n\u2212y(\u03b1 \u2212 1)y\u03b1\u22122 dy\ne\n\u2212yy\u03b1\u22122 dy\ne\n\n\u0001(\u03b1) = \u2212e\n\n666q\n* q\n\u2212yy\u03b1\u22121\n= (\u03b1 \u2212 1)\n= (\u03b1 \u2212 1)\u0001(\u03b1 \u2212 1)\n\n0\n\n0\n\n0\n\n(6.1)\n\nfor integral values of \u03b1, say, \u03b1 = n, we obtain, by applying equation (6.1) repeatedly,\n\nsince \u0001(1) =- q\n\n0 e\n\n\u0001(n) = (n \u2212 1)\u0001(n \u2212 1)\n\n= (n \u2212 1)(n \u2212 2)\u0001(n \u2212 2)\n= \u00b7\u00b7\u00b7\n= (n \u2212 1)(n \u2212 2)\u00b7\u00b7\u00b7 3 \u00b7 2\u0001(1)\n\n\u2212x dx = 1, it follows that, for integral values of n,\n\n\u0001(n) = (n \u2212 1)!\n\nwhen \u03b1 is a positive integer, say, \u03b1 = n, the gamma distribution with parameters\n(\u03b1, \u03bb) often arises, in practice as the distribution of the amount of time one has to\nwait until a total of n events has occurred. more specifically, if events are occurring\nrandomly and in accordance with the three axioms of section 4.7, then it turns out\nthat the amount of time one has to wait until a total of n events has occurred will be a\ngamma random variable with parameters (n, \u03bb). to prove this, let tn denote the time\nat which the nth event occurs, and note that tn is less than or equal to t if and only\nif the number of events that have occurred by time t is at least n. that is, with n(t)\nequal to the number of events in [0, t],\n\np{tn \u2026 t} = p{n(t) \u00fa n}\n\nq(cid:6)\nq(cid:6)\n\nj=n\n\n=\n\n=\n\np{n(t) = j}\n\u2212\u03bbt(\u03bbt)j\ne\n\nj=n\n\nj!\n\n "}, {"Page_number": 231, "text": "216\n\nchapter 5\n\ncontinuous random variables\n\nwhere the final\nidentity follows because the number of events in [0, t] has a\npoisson distribution with parameter \u03bbt. differentiation of the preceding now yields\nthe density function of tn:\nf (t) =\n\n\u2212\u03bbtj(\u03bbt)j\u22121\u03bb\ne\n\n\u2212\n\n\u03bbe\n\nq(cid:6)\nq(cid:6)\n\nj=n\n\nj!\n\n\u2212\n\n=\n\nj=n\n= \u03bbe\n\n\u2212\u03bbt(\u03bbt)j\u22121\n\u03bbe\n(j \u2212 1)!\n\u2212\u03bbt(\u03bbt)n\u22121\n(n \u2212 1)!\n\nq(cid:6)\nq(cid:6)\n\nj=n\n\nj=n\n\n\u2212\u03bbt(\u03bbt)j\nj!\n\n\u03bbe\n\n\u2212\u03bbt(\u03bbt)j\nj!\n\nhence, tn has the gamma distribution with parameters (n, \u03bb). (this distribution is\noften referred to in the literature as the n-erlang distribution.) note that when n = 1,\nthis distribution reduces to the exponential distribution.\nthe gamma distribution with \u03bb = 1\n2 and \u03b1 = n/2, n a positive integer, is called the\n\u03c7 2\nn (read \u201cchi-squared\u201d) distribution with n degrees of freedom. the chi-squared dis-\ntribution often arises in practice as the distribution of the error involved in attempt-\ning to hit a target in n-dimensional space when each coordinate error is normally\ndistributed. this distribution will be studied in chapter 6, where its relation to the\nnormal distribution is detailed.\n\nexample 6a\nlet x be a gamma random variable with parameters \u03b1 and \u03bb. calculate (a) e[x] and\n(b) var(x).\n\nsolution.\n\n(a)\n\n0\n\n* q\n* q\ne[x] = 1\n\u0001(\u03b1)\n= 1\n= \u0001(\u03b1 + 1)\n= \u03b1\n\u03bb\n\n\u03bb\u0001(\u03b1)\n\n\u03bb\u0001(\u03b1)\n\n0\n\n\u2212\u03bbx(\u03bbx)\u03b1\u22121 dx\n\u2212\u03bbx(\u03bbx)\u03b1 dx\n\n\u03bbxe\n\n\u03bbe\n\nby equation (6.1)\n\n(b) by first calculating e[x2], we can show that\nvar(x) = \u03b1\n\u03bb2\n\nthe details are left as an exercise.\n\n.\n\n5.6.2 the weibull distribution\nthe weibull distribution is widely used in engineering practice due to its versatility. it\nwas originally proposed for the interpretation of fatigue data, but now its use has been\nextended to many other engineering problems. in particular, it is widely used in the\nfield of life phenomena as the distribution of the lifetime of some object, especially\nwhen the \u201cweakest link\u201d model is appropriate for the object. that is, consider an\n\n "}, {"Page_number": 232, "text": "section 5.6\n\nother continuous distributions 217\n\nobject consisting of many parts, and suppose that the object experiences death (fail-\nure) when any of its parts fail. it has been shown (both theoretically and empirically)\nthat under these conditions a weibull distribution provides a close approximation to\nthe distribution of the lifetime of the item.\n\nthe weibull distribution function has the form\n\n(cid:2)\n\n0\n\n\u2212\n\n0\n1 \u2212 exp\n\nx \u2212 \u03bd\n\n\u03b1\n\n7\n\n(cid:3)\u03b2\n\nx \u2026 \u03bd\n\nx > \u03bd\n\n(6.2)\n\n\u23a7\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23a9\n\nf(x) =\n\na random variable whose cumulative distribution function is given by equation (6.2)\nis said to be a weibull random variable with parameters \u03bd, \u03b1, and \u03b2. differentiation\nyields the density:\n\n\u23a7\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23a9\n\n(cid:2)\n\n0\n\n\u03b2\n\n\u03b1\n\nf (x) =\n\n(cid:3)\u03b2\u22121\n\n0\n\n(cid:2)\n\n\u2212\n\nexp\n\nx \u2212 \u03bd\n\n\u03b1\n\n7\n\n(cid:3)\u03b2\n\nx \u2212 \u03bd\n\n\u03b1\n\nx \u2026 \u03bd\n\nx > \u03bd\n\n5.6.3 the cauchy distribution\na random variable is said to have a cauchy distribution with parameter \u03b8,\u2212q < \u03b8 <\nq, if its density is given by\n\nf (x) = 1\n\n\u03c0\n\n1\n\n1 + (x \u2212 \u03b8 )2\n\n\u2212 q < x < q\n\nexample 6b\nsuppose that a narrow-beam flashlight is spun around its center, which is located a\nunit distance from the x-axis. (see figure 5.7.) consider the point x at which the\nbeam intersects the x-axis when the flashlight has stopped spinning. (if the beam is\nnot pointing toward the x-axis, repeat the experiment.)\n\n\u242a\n\n1\n\n0\n\nx\n\nx-axis\n\nfigure 5.7\n\nas indicated in figure 5.7, the point x is determined by the angle \u03b8 between the\nflashlight and the y-axis, which, from the physical situation, appears to be uniformly\ndistributed between \u2212\u03c0/2 and \u03c0/2. the distribution function of x is thus given by\n\nf(x) = p{x \u2026 x}\n\n= p{tan \u03b8 \u2026 x}\n= p{\u03b8 \u2026 tan\n= 1\ntan\n2\n\n+ 1\n\u03c0\n\n\u22121 x}\n\u22121 x\n\n "}, {"Page_number": 233, "text": "218\n\nchapter 5\n\ncontinuous random variables\nwhere the last equality follows since \u03b8, being uniform over (\u2212\u03c0/2, \u03c0/2), has\ndistribution\n\np{\u03b8 \u2026 a} = a \u2212 (\u2212\u03c0/2)\n\n= 1\n2\nhence, the density function of x is given by\n\n\u03c0\n\n+ a\n\u03c0\n\n\u2212 \u03c0\n2\n\n< a <\n\n\u03c0\n2\n\nf (x) = d\ndx\n\nf(x) =\n\n1\n\n\u03c0(1 + x2)\n\n\u2212 q < x < q\n\nand we see that x has the cauchy distribution.\u2020\n\n.\n\n5.6.4 the beta distribution\na random variable is said to have a beta distribution if its density is given by\n\n\u23a7\u23aa\u23a8\n\u23aa\u23a9\n\nf (x) =\n\n1\n\nb(a, b)\n0\n\nxa\u22121(1 \u2212 x)b\u22121\n(cid:11)\n\n0 < x < 1\n\notherwise\n\n1\n\nxa\u22121(1 \u2212 x)b\u22121 dx\n\n0\n\nwhere\n\nb(a, b) =\n\nthe beta distribution can be used to model a random phenomenon whose set of\npossible values is some finite interval [c, d]\u2014which, by letting c denote the origin and\ntaking d \u2212 c as a unit measurement, can be transformed into the interval [0, 1].\n\nwhen a = b, the beta density is symmetric about 1\n\n2, giving more and more weight\nto regions about 1\n2 as the common value a increases. (see figure 5.8.) when b > a,\nthe density is skewed to the left (in the sense that smaller values become more likely);\nand it is skewed to the right when a > b. (see figure 5.9.)\n\nthe relationship\n\nb(a, b) = \u0001(a)\u0001(b)\n\u0001(a + b)\n\n(6.3)\n\n(cid:11)\n\ncan be shown to exist between\n\nb(a, b) =\n\nand the gamma function.\n\n1\n\nxa\u22121(1 \u2212 x)b\u22121 dx\n\n0\n\n\u2020that d\ndx\n\nor\n\n\u22121 x) = 1/(1 + x2) can be seen as follows: if y = tan\n\n\u22121 x, then tan y = x, so\n\n(tan\n\n1 = d\ndx\n\n(tan y) = d\ndy\n\n(tan y)\n\ndy\ndx\n\n= d\ndy\n\nsin y\ncos y\n\n=\n\ndy\ndx\n\n(cid:12)\n\n(cid:13)\n\n(cid:14)\n\n(cid:15)\n\ncos2 y + sin2 y\n\ncos2 y\n\ndy\ndx\n\n=\n\ndy\ndx\n\ncos2 y\n\nsin2 y + cos2 y\n\n=\n\n1\n\ntan2 y + 1\n\n=\n\n1\n\nx2 + 1\n\n "}, {"Page_number": 234, "text": "section 5.7\n\nthe distribution of a function of a random variable 219\n\nf(x)\n\n0\n\na =\n\n1\u2013\n4\n\na = 10\n\na = 3\n\na = 1\n\n1\u2013\n2\n\nx\n\n1\n\nfigure 5.8: beta densities with parameters (a, b) when a = b.\n\nf(x)\n\na = 6\n\na =\n\n3\u2013\n2\n\nx\n\nfigure 5.9: beta densities with parameters (a, b) when a/(a + b) = 1/20.\n\nupon using equation (6.1) along with the identity (6.3), it is an easy matter to show\nthat if x is a beta random variable with parameters a and b, then\n\ne[x] = a\n\nvar(x) =\n\na + b\n(a + b)2(a + b + 1)\n\nab\n\nremark. a verification of equation (6.3) appears in example 7c of chapter 6. .\n\n5.7 the distribution of a function of a random variable\n\noften, we know the probability distribution of a random variable and are interested\nin determining the distribution of some function of it. for instance, suppose that we\nknow the distribution of x and want to find the distribution of g(x). to do so, it is\n\n "}, {"Page_number": 235, "text": "220\n\nchapter 5\n\ncontinuous random variables\nnecessary to express the event that g(x) \u2026 y in terms of x being in some set. we\nillustrate with the following examples.\n\nexample 7a\nlet x be uniformly distributed over (0, 1). we obtain the distribution of the random\nvariable y, defined by y = xn, as follows: for 0 \u2026 y \u2026 1,\n\nfy (y) = p{y \u2026 y}\n= p{xn \u2026 y}\n= p{x \u2026 y1/n}\n= fx (y1/n)\n= y1/n\n\n\u23a7\u23aa\u23a8\n\u23aa\u23a9 1\n\nn\n0\n\nfor instance, the density function of y is given by\n\nfy (y) =\n\ny1/n\u22121\n\n0 \u2026 y \u2026 1\notherwise\n\n.\n\nexample 7b\nif x is a continuous random variable with probability density fx, then the distribution\nof y = x2 is obtained as follows: for y \u00fa 0,\nfy (y) = p{y \u2026 y}\n= p{x2 \u2026 y}\n= p{\u2212\u221a\n\u221a\ny}\ny \u2026 x \u2026\ny) \u2212 fx (\u2212\u221a\n\u221a\n= fx (\ny)\n\ndifferentiation yields\nfy (y) = 1\n\u221a\n2\n\n[fx (\n\ny\n\ny) + fx (\u2212\u221a\n\u221a\n\ny)]\n\n.\n\nexample 7c\nif x has a probability density fx, then y = |x| has a density function that is obtained\nas follows: for y \u00fa 0,\n\nfy (y) = p{y \u2026 y}\n= p{|x| \u2026 y}\n= p{\u2212y \u2026 x \u2026 y}\n= fx (y) \u2212 fx (\u2212y)\n\nhence, on differentiation, we obtain\n\nfy (y) = fx (y) + fx (\u2212y)\n\ny \u00fa 0\n\n.\n\n "}, {"Page_number": 236, "text": "section 5.7\n\nthe distribution of a function of a random variable 221\n\nthe method employed in examples 7a through 7c can be used to prove\n\ntheorem 7.1.\n\ntheorem 7.1. let x be a continuous random variable having probability density func-\ntion fx. suppose that g(x) is a strictly monotonic (increasing or decreasing), differen-\ntiable (and thus continuous) function of x. then the random variable y defined by\ny = g(x) has a probability density function given by\n\n\u23a7\u23aa\u23a8\n\u23aa\u23a9 fx[g\n\n6666 d\n\n6666\n\ng\n\ndy\n\n\u22121(y)\n\n\u22121(y)]\n\nfy (y) =\n\nif y = g(x) for some x\nif y z g(x) for all x\n\u22121(y) is defined to equal that value of x such that g(x) = y.\nwe shall prove theorem 7.1 when g(x) is an increasing function.\nproof. suppose that y = g(x) for some x. then, with y = g(x),\n\n0\n\nwhere g\n\nfy (y) = p{g(x) \u2026 y}\n\u22121(y)}\n\n= p{x \u2026 g\n= fx (g\n\u22121(y))\n\ndifferentiation gives\n\nfy (y) = fx (g\n\n\u22121(y))\n\n.\n\u22121(y) is nondecreasing, so its derivative is non-\nwhen y z g(x) for any x, then fy (y) is either 0 or 1, and in either case fy (y) = 0.\n\nwhich agrees with theorem 7.1, since g\nnegative.\n\ng\n\n\u22121(y)\n\nd\ndy\n\nexample 7d\nlet x be a continuous nonnegative random variable with density function f , and let\ny = xn. find fy, the probability density function of y.\nsolution. if g(x) = xn, then\n\n\u22121(y) = y1/n\n\ng\n\nand\n\nd\ndy\n\n{g\n\n\u22121(y)} = 1\nn\n\ny1/n\u22121\n\nhence, from theorem 7.1, we obtain\n\nfor n = 2, this gives\n\nfy (y) = 1\nn\n\ny1/n\u22121f (y1/n)\n\nfy (y) = 1\n\u221a\n2\ny\n\n\u221a\ny)\n\nf (\n\nwhich (since x \u00fa 0) is in agreement with the result of example 7b.\n\n.\n\n "}, {"Page_number": 237, "text": "222\n\nchapter 5\n\ncontinuous random variables\n\nsummary\na random variable x is continuous if there is a nonnegative function f , called the\nprobability density function of x, such that, for any set b,\n\n*\n\np{x \u2208 b} =\n\nf (x) dx\n\nb\n\nif x is continuous, then its distribution function f will be differentiable and\n\nthe expected value of a continuous random variable x is defined by\n\na useful identity is that, for any function g,\n\nd\ndx\n\nf(x) = f (x)\n* q\n* q\n\n\u2212q\n\ne[x] =\n\nxf (x) dx\n\ne[g(x)] =\n\ng(x)f (x) dx\n\n\u2212q\n\nas in the case of a discrete random variable, the variance of x is defined by\n\nvar(x) = e[(x \u2212 e[x])2]\n\na random variable x is said to be uniform over the interval (a, b) if its probability\ndensity function is given by\n\n\u23a7\u23aa\u23a8\n\u23aa\u23a9 1\nb \u2212 a\n0\n\nf (x) =\n\na \u2026 x \u2026 b\notherwise\n\nits expected value and variance are\n\ne[x] = a + b\n\n2\n\nvar(x) = (b \u2212 a)2\n\n12\n\na random variable x is said to be normal with parameters \u03bc and \u03c3 2 if its probability\ndensity function is given by\n\nit can be shown that\n\n\u2212(x\u2212\u03bc)2/2\u03c3 2 \u2212 q < x < q\ne\n\nf (x) = 1\u221a\n2\u03c0 \u03c3\n\u03bc = e[x] \u03c3 2 = var(x)\n\nif x is normal with mean \u03bc and variance \u03c3 2, then z, defined by\n\nz = x \u2212 \u03bc\n\n\u03c3\n\nis normal with mean 0 and variance 1. such a random variable is said to be a standard\nnormal random variable. probabilities about x can be expressed in terms of proba-\nbilities about the standard normal variable z, whose probability distribution function\ncan be obtained either from table 5.1 or from a website.\n\n "}, {"Page_number": 238, "text": "summary 223\n\nwhen n is large, the probability distribution function of a binomial random vari-\nable with parameters n and p can be approximated by that of a normal random\nvariable having mean np and variance np(1 \u2212 p).\n\na random variable whose probability density function is of the form\n\n%\n\nf (x) =\n\n\u03bbe\n\n\u2212\u03bbx\n0\n\nx \u00fa 0\notherwise\n\nis said to be an exponential random variable with parameter \u03bb. its expected value and\nvariance are, respectively,\n\ne[x] = 1\n\n\u03bb\n\nvar(x) = 1\n\u03bb2\n\na key property possessed only by exponential random variables is that they are mem-\noryless, in the sense that, for positive s and t,\n\np{x > s + t|x > t} = p{x > s}\n\nif x represents the life of an item, then the memoryless property states that, for any\nt, the remaining life of a t-year-old item has the same probability distribution as the\nlife of a new item. thus, one need not remember the age of an item to know its\ndistribution of remaining life.\n\nlet x be a nonnegative continuous random variable with distribution function f\n\nand density function f . the function\n\u03bb(t) =\n\nf (t)\n\n1 \u2212 f(t)\n\nt \u00fa 0\n\nis called the hazard rate, or failure rate, function of f. if we interpret x as being the\nlife of an item, then, for small values of dt, \u03bb(t) dt is approximately the probability\nthat a t-unit-old item will fail within an additional time dt. if f is the exponential\ndistribution with parameter \u03bb, then\n\n\u03bb(t) = \u03bb t \u00fa 0\n\nin addition, the exponential is the unique distribution having a constant failure rate.\na random variable is said to have a gamma distribution with parameters \u03b1 and \u03bb\n\nif its probability density function is equal to\n\nand is 0 otherwise. the quantity \u0001(\u03b1) is called the gamma function and is defined by\n\nf (x) = \u03bbe\n\nx \u00fa 0\n\n\u0001(\u03b1)\n\n\u2212\u03bbx(\u03bbx)\u03b1\u22121\n* q\n\n\u0001(\u03b1) =\n\n\u2212xx\u03b1\u22121 dx\ne\n\n0\n\nthe expected value and variance of a gamma random variable are, respectively,\n\ne[x] = \u03b1\n\n\u03bb\n\nvar(x) = \u03b1\n\u03bb2\n\n "}, {"Page_number": 239, "text": "224\n\nchapter 5\n\ncontinuous random variables\n\na random variable is said to have a beta distribution with parameters (a, b) if its\nprobability density function is equal to\n\nand is equal to 0 otherwise. the constant b(a, b) is given by\nxa\u22121(1 \u2212 x)b\u22121 dx\n\nb(a, b) =\n\n1\n\nf (x) =\n\n1\n\nb(a, b)\n\n0 \u2026 x \u2026 1\n\nxa\u22121(1 \u2212 x)b\u22121\n*\n\n0\n\nthe mean and variance of such a random variable are, respectively,\n\ne[x] = a\n\na + b\n\nvar(x) =\n\nab\n\n(a + b)2(a + b + 1)\n\nproblems\n\n5.1. let x be a random variable with probability den-\n\nsity function\n\n0\nc(1 \u2212 x2) \u22121 < x < 1\n0\n\notherwise\n\nf (x) =\n\n(a) what is the value of c?\n(b) what is the cumulative distribution function\n\nof x?\n\n5.2. a system consisting of one original unit plus a\nspare can function for a random amount of time x.\nif the density of x is given (in units of months) by\n\n0\n\nf (x) =\n\n\u2212x/2\n\nx > 0\nx \u2026 0\n\ncxe\n0\n\nwhat is the probability that the system functions\nfor at least 5 months?\n5.3. consider the function\n\nf (x) =\n\nc(2x \u2212 x3)\n0\n\n0 < x < 5\n2\notherwise\n\ncould f be a probability density function? if so,\ndetermine c. repeat if f (x) were given by\n\nf (x) =\n\nc(2x \u2212 x2)\n0\n\n0 < x < 5\n2\notherwise\n\n0\n\n0\n\n5.4. the probability density function of x, the lifetime\nof a certain type of electronic device (measured in\nhours), is given by\n\n\u23a7\u23aa\u23a8\n\u23aa\u23a9 10\n(a) find p{x > 20}.\n(b) what is the cumulative distribution function\n\nx > 10\nx \u2026 10\n\nf (x) =\n\nx2\n0\n\nof x?\n\n(c) what is the probability that, of 6 such types of\ndevices, at least 3 will function for at least 15\nhours? what assumptions are you making?\n\n5.5. a filling station is supplied with gasoline once a\nweek. if its weekly volume of sales in thousands of\ngallons is a random variable with probability den-\nsity function\n\n0\n5(1 \u2212 x)4\n0\n\nf (x) =\n\n0 < x < 1\notherwise\n\nwhat must the capacity of the tank be so that the\nprobability of the supply\u2019s being exhausted in a\ngiven week is .01?\n\n5.6. compute e[x] if x has a density function given by\n\n;\n\nxe\n\n\u2212x/2\n\nx > 0\n\n\u23a7\u23aa\u23a8\n\u23aa\u23a9 1\n0\n4\n0\nc(1 \u2212 x2) \u22121 < x < 1\n\u23a7\u23aa\u23a8\n0\n\u23aa\u23a9 5\n\notherwise\n\notherwise\n\nx > 5\nx \u2026 5\n\nx2\n0\n\n.\n\n;\n\n(a) f (x) =\n\n(b) f (x) =\n\n(c) f (x) =\n\n "}, {"Page_number": 240, "text": "0\n\n5.7. the density function of x is given by\n\nf (x) =\n\na + bx2\n0\n\n0 \u2026 x \u2026 1\notherwise\n\nif e[x] = 3\n\n5 , find a and b.\n\n5.8. the lifetime in hours of an electronic tube is a ran-\ndom variable having a probability density function\ngiven by\n\nf (x) = xe\n\u2212x\n\nx \u00fa 0\n\ncompute the expected lifetime of such a tube.\n\n5.9. consider example 4b of chapter 4, but now sup-\npose that the seasonal demand is a continuous ran-\ndom variable having probability density function f .\nshow that the optimal amount to stock is the value\n\u2217\ns\n\nthat satisfies\n\nf(s\n\n\u2217) = b\n\nb + (cid:7)\n\nwhere b is net profit per unit sale, (cid:7) is the net loss\nper unit unsold, and f is the cumulative distribu-\ntion function of the seasonal demand.\n\n5.10. trains headed for destination a arrive at the train\nstation at 15-minute intervals starting at 7 a.m.,\nwhereas trains headed for destination b arrive at\n15-minute intervals starting at 7:05 a.m.\n(a) if a certain passenger arrives at the station at\na time uniformly distributed between 7 and\n8 a.m. and then gets on the first train that\narrives, what proportion of time does he or\nshe go to destination a?\n\n(b) what if the passenger arrives at a time uni-\nformly distributed between 7:10 and 8:10\na.m.?\n\n5.11. a point is chosen at random on a line segment\nof length l. interpret this statement, and find the\nprobability that the ratio of the shorter to the\nlonger segment is less than 1\n4 .\n\n5.12. a bus travels between the two cities a and b,\nwhich are 100 miles apart. if the bus has a break-\ndown, the distance from the breakdown to city a\nhas a uniform distribution over (0, 100). there is a\nbus service station in city a, in b, and in the cen-\nter of the route between a and b. it is suggested\nthat it would be more efficient to have the three\nstations located 25, 50, and 75 miles, respectively,\nfrom a. do you agree? why?\n\n5.13. you arrive at a bus stop at 10 o\u2019clock, knowing\nthat the bus will arrive at some time uniformly dis-\ntributed between 10 and 10:30.\n(a) what is the probability that you will have to\n\nwait longer than 10 minutes?\n\nproblems 225\n\n(b) if, at 10:15, the bus has not yet arrived, what\nis the probability that you will have to wait at\nleast an additional 10 minutes?\n\n5.15. if x is a normal random variable with parameters\n\n5.14. let x be a uniform (0, 1) random variable. com-\npute e[xn] by using proposition 2.1, and then\ncheck the result by using the definition of expec-\ntation.\n\u03bc = 10 and \u03c3 2 = 36, compute\n(a) p{x > 5};\n(b) p{4 < x < 16};\n(c) p{x < 8};\n(d) p{x < 20};\n(e) p{x > 16}.\n5.16. the annual rainfall (in inches) in a certain region is\nnormally distributed with \u03bc = 40 and \u03c3 = 4. what\nis the probability that, starting with this year, it will\ntake over 10 years before a year occurs having a\nrainfall of over 50 inches? what assumptions are\nyou making?\n\n5.17. a man aiming at a target receives 10 points if his\nshot is within 1 inch of the target, 5 points if it is\nbetween 1 and 3 inches of the target, and 3 points if\nit is between 3 and 5 inches of the target. find the\nexpected number of points scored if the distance\nfrom the shot to the target is uniformly distributed\nbetween 0 and 10.\n5.18. suppose that x is a normal random variable with\nmean 5. if p{x > 9} = .2, approximately what is\nvar(x)?\n\n5.19. let x be a normal random variable with mean\n12 and variance 4. find the value of c such that\np{x > c} = .10.\n\n5.20. if 65 percent of the population of a large commu-\nnity is in favor of a proposed rise in school taxes,\napproximate the probability that a random sample\nof 100 people will contain\n(a) at least 50 who are in favor of the proposition;\n(b) between 60 and 70 inclusive who are in favor;\n(c) fewer than 75 in favor.\n\n5.21. suppose that the height, in inches, of a 25-year-old\nman is a normal random variable with parameters\n\u03bc = 71 and \u03c3 2 = 6.25. what percentage of 25-\nyear-old men are over 6 feet, 2 inches tall? what\npercentage of men in the 6-footer club are over 6\nfeet, 5 inches?\n5.22. the width of a slot of a duralumin forging is (in\ninches) normally distributed with \u03bc = .9000 and\n\u03c3 = .0030. the specification limits were given as\n.9000 ; .0050.\n(a) what percentage of forgings will be defec-\n\ntive?\n\n "}, {"Page_number": 241, "text": "226\n\nchapter 5\n\ncontinuous random variables\n\n(b) what is the maximum allowable value of \u03c3\nthat will permit no more than 1 in 100 defec-\ntives when the widths are normally distributed\nwith \u03bc = .9000 and \u03c3 ?\n\n5.23. one thousand independent rolls of a fair die will\nbe made. compute an approximation to the prob-\nability that the number 6 will appear between 150\nand 200 times inclusively. if the number 6 appears\nexactly 200 times, find the probability that the\nnumber 5 will appear less than 150 times.\n\n5.24. the lifetimes of interactive computer chips pro-\nduced by a certain semiconductor manufacturer\nare normally distributed with parameters \u03bc =\n1.4 * 106 hours and \u03c3 = 3 * 105 hours. what is the\napproximate probability that a batch of 100 chips\nwill contain at least 20 whose lifetimes are less than\n1.8 * 106?\n\n5.25. each item produced by a certain manufacturer is,\nindependently, of acceptable quality with proba-\nbility .95. approximate the probability that at most\n10 of the next 150 items produced are unaccept-\nable.\n\n5.26. two types of coins are produced at a factory: a fair\ncoin and a biased one that comes up heads 55 per-\ncent of the time. we have one of these coins, but\ndo not know whether it is a fair coin or a biased\none. in order to ascertain which type of coin we\nhave, we shall perform the following statistical test:\nwe shall toss the coin 1000 times. if the coin lands\non heads 525 or more times, then we shall conclude\nthat it is a biased coin, whereas if it lands on heads\nless than 525 times, then we shall conclude that it\nis a fair coin. if the coin is actually fair, what is the\nprobability that we shall reach a false conclusion?\nwhat would it be if the coin were biased?\n\n5.27. in 10,000 independent tosses of a coin, the coin\nlanded on heads 5800 times. is it reasonable to\nassume that the coin is not fair? explain.\n\n5.28. twelve percent of the population is left handed.\napproximate the probability that there are at least\n20 left-handers in a school of 200 students. state\nyour assumptions.\n\n5.29. a model for the movement of a stock supposes\nthat if the present price of the stock is s, then, after\none period, it will be either us with probability p\nor ds with probability 1 \u2212 p. assuming that suc-\ncessive movements are independent, approximate\nthe probability that the stock\u2019s price will be up\nat least 30 percent after the next 1000 periods if\nu = 1.012, d = 0.990, and p = .52.\n\n5.30. an image is partitioned into two regions, one\nwhite and the other black. a reading taken from\na randomly chosen point in the white section will\ngive a reading that is normally distributed with \u03bc =\n4 and \u03c3 2 = 4, whereas one taken from a randomly\n\nchosen point in the black region will have a nor-\nmally distributed reading with parameters (6, 9).\na point is randomly chosen on the image and has\na reading of 5. if the fraction of the image that is\nblack is \u03b1, for what value of \u03b1 would the probabil-\nity of making an error be the same, regardless of\nwhether one concluded that the point was in the\nblack region or in the white region?\n\n5.31. (a) a fire station is to be located along a road of\nlength a, a < q. if fires occur at points uni-\nformly chosen on (0, a), where should the sta-\ntion be located so as to minimize the expected\ndistance from the fire? that is, choose a so\nas to\n\nminimize e[|x \u2212 a|]\n\nwhen x is uniformly distributed over (0, a).\n(b) now suppose that the road is of infinite\nlength\u2014stretching from point 0 outward to q.\nif the distance of a fire from point 0 is expo-\nnentially distributed with rate \u03bb, where should\nthe fire station now be located? that is, we\nwant to minimize e[|x \u2212 a|], where x is now\nexponential with rate \u03bb.\n\n5.32. the time (in hours) required to repair a machine is\nan exponentially distributed random variable with\nparameter \u03bb = 1\n(a) the probability that a repair time exceeds 2\n\n2 . what is\n\nhours?\n\n(b) the conditional probability that a repair takes\nat least 10 hours, given that its duration\nexceeds 9 hours?\n\n5.33. the number of years a radio functions is exponen-\ntially distributed with parameter \u03bb = 1\n8 . if jones\nbuys a used radio, what is the probability that it\nwill be working after an additional 8 years?\n\n5.34. jones figures that the total number of thousands\nof miles that an auto can be driven before it would\nneed to be junked is an exponential random vari-\nable with parameter 1\n20 . smith has a used car that\nhe claims has been driven only 10,000 miles. if\njones purchases the car, what is the probability\nthat she would get at least 20,000 additional miles\nout of it? repeat under the assumption that the\nlifetime mileage of the car is not exponentially dis-\ntributed, but rather is (in thousands of miles) uni-\nformly distributed over (0, 40).\n\n5.35. the lung cancer hazard rate \u03bb(t) of a t-year-old\n\nmale smoker is such that\n\n\u03bb(t) = .027 + .00025(t \u2212 40)2\n\nt \u00fa 40\n\nassuming that a 40-year-old male smoker survives\nall other hazards, what is the probability that he\nsurvives to (a) age 50 and (b) age 60 without con-\ntracting lung cancer?\n\n "}, {"Page_number": 242, "text": "5.36. suppose that the life distribution of an item has the\nhazard rate function \u03bb(t) = t3, t > 0. what is the\nprobability that\n(a) the item survives to age 2?\n(b) the item\u2019s lifetime is between .4 and 1.4?\n(c) a 1-year-old item will survive to age 2?\n\n5.37. if x is uniformly distributed over (\u22121, 1), find\n\n(a) p{|x| > 1\n(b) the density function of the random variable\n\n};\n\n2\n\n|x|.\n\n5.38. if y is uniformly distributed over (0, 5), what\nis the probability that the roots of the equation\n4x2 + 4xy + y + 2 = 0 are both real?\n\ntheoretical exercises 227\n\n5.39. if x is an exponential random variable with\nparameter \u03bb = 1, compute the probability den-\nsity function of the random variable y defined by\ny = log x.\ndensity function of y = ex.\n\n5.40. if x is uniformly distributed over (0, 1), find the\n5.41. find the distribution of r = a sin \u03b8, where a is\na fixed constant and \u03b8 is uniformly distributed on\n(\u2212\u03c0/2, \u03c0/2). such a random variable r arises in\nthe theory of ballistics. if a projectile is fired from\nthe origin at an angle \u03b1 from the earth with a speed\n\u03bd, then the point r at which it returns to the earth\ncan be expressed as r = (v2/g) sin 2\u03b1, where g is\nthe gravitational constant, equal to 980 centime-\nters per second squared.\n\ntheoretical exercises\n\n5.1. the speed of a molecule in a uniform gas at equi-\nlibrium is a random variable whose probability\ndensity function is given by\n\nable y,\n\n5.5. use the result that, for a nonnegative random vari-\n\n0\n\nf (x) =\n\nax2e\n0\n\n\u2212bx2 x \u00fa 0\nx < 0\n\nwhere b = m/2kt and k, t, and m denote, respec-\ntively, boltzmann\u2019s constant, the absolute temper-\nature of the gas, and the mass of the molecule.\nevaluate a in terms of b.\n\n* q\n\n* q\n\n0\n\n0\n\nhint: show that\n\np{y > y} dy \u2212\n\n5.2. show that\ne[y] =\n*\n* q\n* q\n* q\np{y < \u2212y} dy = \u2212\np{y > y} dy =\n* q\n\n0\n\n0\n\n0\n\np{y < \u2212y} dy\n\nxfy (x) dx\n\n0\n\u2212q\n\nxfy (x) dx\n\ne[g(x)] =\n* q\n\n0\n\ng(x)f (x) dx\n\n\u2212q\n\n* q\n\n0\n\nhint: using theoretical exercise 2, start with\ne[g(x)] =\n\np{g(x) > y} dy \u2212\n\np{g(x) < \u2212y} dy\n\nand then proceed as in the proof given in the text\nwhen g(x) \u00fa 0.\n\n5.4. prove corollary 2.1.\n\nto show that, for a nonnegative random vari-\nable x,\n\n* q\n\n0\n\np{y > t} dt\n\ne[y] =\n* q\nnxn\u22121p{x > x} dx\n* q\n\n0\n\ne[xn] =\n\nhint: start with\n\n0\n\ne[xn] =\n\np{xn > t} dt\nand make the change of variables t = xn.\n5.6. define a collection of events ea, 0 < a < 1, hav-\ning the property that p(ea) = 1 for all a but\np\n\n(cid:4)(cid:11)\n\n= 0.\n\n(cid:5)\n\nea\n\na\n\nhint: let x be uniform over (0, 1) and define each\nea in terms of x.\n\n5.7. the standard deviation of x, denoted sd(x), is\n\n.\n\nsd(x) =\n\nvar(x)\n\nfind sd(ax + b) if x has variance \u03c3 2.\n5.8. let x be a random variable that takes on values\nbetween 0 and c. that is, p{0 \u2026 x \u2026 c} = 1.\nshow that\n\nvar(x) \u2026 c2\n4\n\nhint: one approach is to first argue that\n\ne[x2] \u2026 ce[x]\n\n5.3. show that if x has density function f , then\n\ngiven by\n\n "}, {"Page_number": 243, "text": "228\n\nchapter 5\n\ncontinuous random variables\n\nand then use this inequality to show that\n\nvar(x) \u2026 c2[\u03b1(1 \u2212 \u03b1)] where \u03b1 = e[x]\nc\n\n5.9. show that z is a standard normal random variable,\n\nthen, for x > 0,\n(a) p{z > x} = p{z < \u2212x};\n(b) p{|z| > x} = 2p{z > x};\n(c) p{|z| < x} = 2p{z < x} \u2212 1.\n\n5.10. let f (x) denote the probability density function of\na normal random variable with mean \u03bc and vari-\nance \u03c3 2. show that \u03bc \u2212 \u03c3 and \u03bc + \u03c3 are points\nof inflection of this function. that is, show that\n(cid:8)(cid:8)(x) = 0 when x = \u03bc \u2212 \u03c3 or x = \u03bc + \u03c3 .\nf\n\n(cid:8)\n\n5.11. let z be a standard normal random variable z,\nand let g be a differentiable function with deriva-\ntive g\n(cid:8)(z)] = e[zg(z)]\n(a) show that e[g\n(b) show that e[zn+1] = ne[zn\u22121]\n(c) find e[z4].\n\n.\n\n5.12. use the identity of theoretical exercise 5 to derive\ne[x2] when x is an exponential random variable\nwith parameter \u03bb.\n\n5.13. the median of a continuous random variable hav-\ning distribution function f is that value m such that\nf(m) = 1\n2 . that is, a random variable is just as\nlikely to be larger than its median as it is to be\nsmaller. find the median of x if x is\n(a) uniformly distributed over (a, b);\n(b) normal with parameters \u03bc, \u03c3 2;\n(c) exponential with rate \u03bb.\n\n5.14. the mode of a continuous random variable having\ndensity f is the value of x for which f (x) attains its\nmaximum. compute the mode of x in cases (a),\n(b), and (c) of theoretical exercise 5.13.\n\n5.15. if x is an exponential random variable with\nparameter \u03bb, and c > 0, show that cx is exponen-\ntial with parameter \u03bb/c.\n\n5.16. compute the hazard rate function of x when x is\n\nuniformly distributed over (0, a).\n\n5.17. if x has hazard rate function \u03bbx (t), compute the\nhazard rate function of ax where a is a positive\nconstant.\n\n5.18. verify that the gamma density function integrates\n\nto 1.\n\n5.19. if x is an exponential random variable with mean\n\n1/\u03bb, show that\n\ne[xk] = k!\n\n\u03bbk\n\nk = 1, 2, . . .\n\nhint: make use of the gamma density function to\nevaluate the preceding.\n\n5.20. verify that\n\nvar(x) = \u03b1\n\u03bb2\n\n5.21. show that \u0001\n\nwhen x is a gamma random variable with param-\neters \u03b1 and \u03bb.\n\n(cid:29)\n(cid:30)\n= \u221a\n= - q\n\u03c0.\n\u22121/2 dx. make the change\n\u2212xx\nof variables y = \u221a\n0 e\n2x and then relate the resulting\n\nhint: \u0001\n\n(cid:29)\n\n(cid:30)\n\n1\n2\n\n1\n2\n\nexpression to the normal distribution.\n\n5.22. compute the hazard rate function of a gamma ran-\ndom variable with parameters (\u03b1, \u03bb) and show it is\nincreasing when \u03b1 \u00fa 1 and decreasing when \u03b1 \u2026 1.\n5.23. compute the hazard rate function of a weibull\nrandom variable and show it is increasing when\n\u03b2 \u00fa 1 and decreasing when \u03b2 \u2026 1.\n5.24. show that a plot of log(log(1 \u2212 f(x))\u22121) against\nlog x will be a straight line with slope \u03b2 when\nf(\u00b7) is a weibull distribution function. show also\nthat approximately 63.2 percent of all observa-\ntions from such a distribution will be less than \u03b1.\nassume that v = 0.\n\n5.25. let\n\ny =\n\n(cid:3)\u03b2\n\n(cid:2)\n\nx \u2212 \u03bd\n\n\u03b1\n\nshow that if x is a weibull random variable with\nparameters \u03bd, \u03b1, and \u03b2, then y is an exponential\nrandom variable with parameter \u03bb = 1 and vice\nversa.\n\n5.26. if x is a beta random variable with parameters a\n\nand b, show that\n\ne[x] = a\n\nvar(x) =\n\na + b\n(a + b)2(a + b + 1)\n\nab\n\n5.27. if x is uniformly distributed over (a, b), what ran-\ndom variable, having a linear relation with x, is\nuniformly distributed over (0, 1)?\n\n5.28. consider the beta distribution with parameters\n\n(a, b). show that\n(a) when a > 1 and b > 1, the density is uni-\nmodal (that is, it has a unique mode) with\nmode equal to (a \u2212 1)/(a + b \u2212 2);\n(b) when a \u2026 1, b \u2026 1, and a + b < 2, the den-\nsity is either unimodal with mode at 0 or 1 or\nu-shaped with modes at both 0 and 1;\n(c) when a = 1 = b, all points in [0, 1] are modes.\n5.29. let x be a continuous random variable having\ncumulative distribution function f. define the ran-\ndom variable y by y = f(x). show that y is\nuniformly distributed over (0, 1).\n\n5.30. let x have probability density fx. find the prob-\nability density function of the random variable y\ndefined by y = ax + b.\n\n "}, {"Page_number": 244, "text": "5.31. find the probability density function of y = ex\nwhen x is normally distributed with parameters \u03bc\nand \u03c3 2. the random variable y is said to have a\nlognormal distribution (since log y has a normal\ndistribution) with parameters \u03bc and \u03c3 2.\n\n5.32. let x and y be independent random vari-\nables that are both equally likely to be either\n1, 2, . . . , (10)n, where n is very large. let d denote\nthe greatest common divisor of x and y, and let\nqk = p{d = k}.\n(a) give a heuristic argument that qk = 1\n\nk2 q1.\n\nhint: note that in order for d to equal k, k\nmust divide both x and y and also x/k, and\ny/k must be relatively prime. (that is, x/k,\nand y/k must have a greatest common divisor\nequal to 1.)\n\nq1 = p{x and y are relatively prime}\n\n=\n\nq(cid:6)\n\nk=1\n\n1\n\n1/k2\n\nq(cid:9)\n\nself-test problems and exercises 229\n\n1/k2 =\nit is a well-known identity that\n\u03c0 2/6, so q1 = 6/\u03c0 2. (in number theory, this\nis known as the legendre theorem.)\n\n1\n\n(c) now argue that\nq1 =\n\n(cid:5)\n\n(cid:4)\n\nq(cid:31)\n\ni=1\n\np2\ni\n\n\u2212 1\np2\ni\n\nis the ith-smallest prime greater\n\nwhere pi\nthan 1.\nhint: x and y will be relatively prime if they\nhave no common prime factors. hence, from\npart (b), we see that\n\n(cid:4)\n\nq(cid:31)\n\n(cid:5)\n\np2\ni\n\n\u2212 1\np2\ni\n\n= 6\n\u03c0 2\n\nwhich was noted without explanation in\nproblem 11 of chapter 4. (the relationship\nbetween this problem and problem 11 of\nchapter 4 is that x and y are relatively prime\nif xy has no multiple prime factors.)\n\n5.33. prove theorem 7.1 when g(x) is a decreasing func-\n\ntion.\n\n(b) use part (a) to show that\n\ni=1\n\nself-test problems and exercises\n\n5.1. the number of minutes of playing time of a certain\nhigh school basketball player in a randomly cho-\nsen game is a random variable whose probability\ndensity function is given in the following figure:\n\n.050\n\n.025\n\n10\n\n20\n\n30\n\n40\n\nfind the probability that the player plays\n(a) over 15 minutes;\n(b) between 20 and 35 minutes;\n(c) less than 30 minutes;\n(d) more than 36 minutes.\n\n5.2. for some constant c, the random variable x has\n\nthe probability density function\n\n%\n\nf (x) =\n\ncxn\n0\n\n0 < x < 1\notherwise\n\nfind (a) c and (b) p{x > x}, 0 < x < 1.\n\n5.3. for some constant c, the random variable x has\n\nthe probability density function\n\n0\n\nf (x) =\n\ncx4\n0\n\n0 < x < 2\notherwise\n\nfind (a) e[x] and (b) var(x).\n\n5.4. the random variable x has the probability density\n\nfunction\n\n0\n\nax + bx2\n0\n\nf (x) =\n\n0 < x < 1\notherwise\n} and (b) var(x).\n5.5. the random variable x is said to be a discrete uni-\nform random variable on the integers 1, 2, . . . , n if\n\nif e[x] = .6, find (a) p{x < 1\n\n2\n\np{x = i} = 1\nn\n\ni = 1, 2, . . . , n\n\nfor any nonnegative real number x, let int(x)\n(sometimes written as [x]) be the largest inte-\nger that is less than or equal to x. show that\nu is a uniform random variable on (0, 1), fain\n\n "}, {"Page_number": 245, "text": "230\n\ncontinuous random variables\n\nchapter 5\nx = int(nu) + 1 is a discrete uniform random\nvariable on 1, . . . , n.\n\n5.6. your company must make a sealed bid for a con-\nstruction project. if you succeed in winning the\ncontract (by having the lowest bid), then you plan\nto pay another firm 100 thousand dollars to do\nthe work. if you believe that the minimum bid\n(in thousands of dollars) of the other participating\ncompanies can be modeled as the value of a ran-\ndom variable that is uniformly distributed on (70,\n140), how much should you bid to maximize your\nexpected profit?\n\n5.7. to be a winner in a certain game, you must be\nsuccessful in three successive rounds. the game\ndepends on the value of u, a uniform random vari-\nable on (0, 1). if u > .1, then you are successful\nin round 1; if u > .2, then you are successful in\nround 2; and if u > .3, then you are successful in\nround 3.\n(a) find the probability that you are successful in\n\nround 1.\n\n(b) find the conditional probability that you are\nsuccessful in round 2 given that you were suc-\ncessful in round 1.\n\n(c) find the conditional probability that you are\nsuccessful in round 3 given that you were suc-\ncessful in rounds 1 and 2.\n\n(d) find the probability that you are a winner.\n\n5.8. a randomly chosen iq test taker obtains a score\nthat is approximately a normal random variable\nwith mean 100 and standard deviation 15. what is\nthe probability that the score of such a person is\n(a) above 125; (b) between 90 and 110?\n\n5.9. suppose that the travel time from your home to\nyour office is normally distributed with mean 40\nminutes and standard deviation 7 minutes. if you\nwant to be 95 percent certain that you will not be\nlate for an office appointment at 1 p.m., what is the\nlatest time that you should leave home?\n\n5.10. the life of a certain type of automobile tire is nor-\nmally distributed with mean 34,000 miles and stan-\ndard deviation 4000 miles.\n(a) what is the probability that such a tire lasts\n\nover 40,000 miles?\n\n(b) what is the probability that it lasts between\n\n30,000 and 35,000 miles?\n\n(c) given that it has survived 30,000 miles, what\nis the conditional probability that the tire sur-\nvives another 10,000 miles?\n\n5.11. the annual rainfall in cleveland, ohio is approx-\nimately a normal random variable with mean 40.2\ninches and standard deviation 8.4 inches. what is\nthe probability that\n(a) next year\u2019s rainfall will exceed 44 inches?\n(b) the yearly rainfalls in exactly 3 of the next 7\n\nyears will exceed 44 inches?\n\nassume that if ai is the event that the rainfall\nexceeds 44 inches in year i (from now), then the\nevents ai, i \u00fa 1, are independent.\n\n5.12. the following table uses 1992 data concerning the\npercentages of male and female full-time workers\nwhose annual salaries fall into different ranges:\n\nearnings range\n\npercentage\nof females\n\npercentage\n\nof males\n\n\u20269999\n10,000\u201319,999\n20,000\u201324,999\n25,000\u201349,999\n\u00fa50,000\n\n8.6\n38.0\n19.4\n29.2\n4.8\n\n4.4\n21.1\n15.8\n41.5\n17.2\n\nsuppose that random samples of 200 male and 200\nfemale full-time workers are chosen. approximate\nthe probability that\n(a) at least 70 of the women earn $25,000 or more;\n(b) at most 60 percent of the men earn $25,000\n\nor more;\n\n(c) at least three-fourths of the men and at least\n\nhalf the women earn $20,000 or more.\n\n5.13. at a certain bank, the amount of time that a cus-\ntomer spends being served by a teller is an expo-\nnential random variable with mean 5 minutes. if\nthere is a customer in service when you enter the\nbank, what is the probability that he or she will still\nbe with the teller after an additional 4 minutes?\n\n5.14. suppose that the cumulative distribution function\n\nof the random variable x is given by\n\nf(x) = 1 \u2212 e\n\n\u2212x2\n\nx > 0\n\nevaluate (a) p{x > 2}; (b) p{1 < x < 3}; (c) the\nhazard rate function of f; (d) e[x]; (e) var(x).\nhint: for parts (d) and (e), you might want to\nmake use of the results of theoretical exercise 5.\n5.15. the number of years that a washing machine func-\ntions is a random variable whose hazard rate func-\ntion is given by\n\n\u23a7\u23a8\n\u23a9 .2\n.2 + .3(t \u2212 2)\n1.1\n\n\u03bb(t) =\n\n0 < t < 2\n2 \u2026 t < 5\nt > 5\n\n(a) what is the probability that the machine will\nstill be working 6 years after being purchased?\n(b) if it is still working 6 years after being pur-\nchased, what is the conditional probability\nthat it will fail within the next 2 years?\n\n5.16. a standard cauchy random variable has density\n\nfunction\n\nf (x) =\n\n1\n\n\u03c0(1 + x2)\n\n\u2212 q < x < q\n\n "}, {"Page_number": 246, "text": "show that x is a standard cauchy random\nvariable, then 1/x is also a standard cauchy ran-\ndom variable.\n\n5.17. a roulette wheel has 38 slots, numbered 0, 00, and\n1 through 36. if you bet 1 on a specified num-\nber then you either win 35 if the roulette ball\nlands on that number or lose 1 if it does not.\nif you continually make such bets, approximate\nthe probability that\n(a) you are winning after 34 bets;\n(b) you are winning after 1000 bets;\n(c) you are winning after 100,000 bets.\nassume that each roll of the roulette ball is equally\nlikely to land on any of the 38 numbers.\n\n5.18. there are two types of batteries in a bin. when in\nuse, type i batteries last (in hours) an exponentially\ndistributed time with rate \u03bbi, i = 1, 2. a battery\nthat is randomly chosen from the bin will be a type\npi = 1. if a ran-\ni battery with probability pi,\ndomly chosen battery is still operating after t hours\nof use, what is the probability that it will still be\noperating after an additional s hours?\n\n2(cid:9)\n\ni=1\n\n5.19. evidence concerning the guilt or innocence of a\ndefendant in a criminal investigation can be sum-\nmarized by the value of an exponential random\n\nself-test problems and exercises 231\n\nvariable x whose mean \u03bc depends on whether the\ndefendant is guilty. if innocent, \u03bc = 1; if guilty,\n\u03bc = 2. the deciding judge will rule the defen-\ndant guilty if x > c for some suitably chosen\nvalue of c.\n(a) if the judge wants to be 95 percent certain\nthat an innocent man will not be convicted,\nwhat should be the value of c?\n\n(b) using the value of c found in part (a), what is\nthe probability that a guilty defendant will be\nconvicted?\n\n5.20. for any real number y, define y\nif y \u00fa 0\nif y < 0\n\n+ = y,\n0,\n\ny\n\n+\n\nby\n\nlet c be a constant.\n(a) show that\n\ne[(z \u2212 c)+\n\n] = 1\u221a\n2\u03c0\n\n\u2212c2/2 \u2212 c(1 \u2212 \u0001(c))\n\ne\n\n(b) find e[(x \u2212 c)+\n\nwhen z is a standard normal random variable.\n] when x is normal with\n\nmean \u03bc and variance \u03c3 2.\n\n "}, {"Page_number": 247, "text": "c h a p t e r\n\n6\n\njointly distributed random variables\n\n6.1 joint distribution functions\n6.2 independent random variables\n6.3 sums of independent random variables\n6.4 conditional distributions: discrete case\n6.5 conditional distributions: continuous case\n6.6 order statistics\n6.7 joint probability distribution of functions of random variables\n6.8 exchangeable random variables\n\n6.1 joint distribution functions\n\nthus far, we have concerned ourselves only with probability distributions for single\nrandom variables. however, we are often interested in probability statements con-\ncerning two or more random variables. in order to deal with such probabilities, we\ndefine, for any two random variables x and y, the joint cumulative probability distri-\nbution function of x and y by\n\nf(a, b) = p{x \u2026 a, y \u2026 b} \u2212 q < a, b < q\n\nthe distribution of x can be obtained from the joint distribution of x and y as\nfollows:\n\nfx (a) = p{x \u2026 a}\n\n(cid:2)\nb\u2192q\nlim\n\n(cid:3)\n= p{x \u2026 a, y < q}\n= p\n{x \u2026 a, y \u2026 b}\n= lim\nb\u2192q p{x \u2026 a, y \u2026 b}\n= lim\nb\u2192q f(a, b)\nk f(a, q)\n\nnote that, in the preceding set of equalities, we have once again made use of the fact\nthat probability is a continuous set (that is, event) function. similarly, the cumulative\ndistribution function of y is given by\n\nfy (b) = p{y \u2026 b}\n= lim\na\u2192q f(a, b)\nk f(q, b)\n\n232\n\n "}, {"Page_number": 248, "text": "section 6.1\n\njoint distribution functions 233\n\nthe distribution functions fx and fy are sometimes referred to as the marginal dis-\ntributions of x and y.\n\nall joint probability statements about x and y can, in theory, be answered in terms\nof their joint distribution function. for instance, suppose we wanted to compute the\njoint probability that x is greater than a and y is greater than b. this could be done\nas follows:\n\np{x > a, y > b} = 1 \u2212 p({x > a, y > b}c)\n\n= 1 \u2212 p({x > a}c \u222a {y > b}c)\n= 1 \u2212 p({x \u2026 a} \u222a {y \u2026 b})\n= 1 \u2212 [p{x \u2026 a} + p{y \u2026 b} \u2212 p{x \u2026 a, y \u2026 b}]\n= 1 \u2212 fx (a) \u2212 fy (b) + f(a, b)\n\n(1.1)\n\nequation (1.1) is a special case of the following equation, whose verification is left as\nan exercise:\n\np{a1 < x \u2026 a2, b1 < y \u2026 b2}\n\n= f(a2, b2) + f(a1, b1) \u2212 f(a1, b2) \u2212 f(a2, b1)\n\n(1.2)\n\nwhenever a1 < a2, b1 < b2.\n\nin the case when x and y are both discrete random variables, it is convenient to\n\ndefine the joint probability mass function of x and y by\np(x, y) = p{x = x, y = y}\n\nthe probability mass function of x can be obtained from p(x, y) by\n\npx (x) = p{x = x}\n\n=\n\np(x, y)\n\ny:p(x,y)>0\n\n(cid:6)\n(cid:6)\n\np(x, y)\n\nx:p(x,y)>0\n\nsimilarly,\n\npy (y) =\n\nexample 1a\nsuppose that 3 balls are randomly selected from an urn containing 3 red, 4 white, and\n5 blue balls. if we let x and y denote, respectively, the number of red and white balls\nchosen, then the joint probability mass function of x and y, p(i, j) = p{x = i, y = j},\nis given by\n\n(cid:2)\n(cid:2)\n(cid:2)\n(cid:2)\n\n5\n3\n4\n1\n4\n2\n4\n3\n\n(cid:3)(cid:26)(cid:2)\n(cid:3)(cid:2)\n(cid:3)(cid:2)\n(cid:3)(cid:26)(cid:2)\n\n5\n2\n5\n1\n\n12\n3\n\n(cid:3)\n(cid:3)(cid:26)(cid:2)\n(cid:3)(cid:26)(cid:2)\n(cid:3)\n\n12\n3\n\n(cid:3)\n= 10\n220\n(cid:3)\n\n12\n3\n12\n3\n= 4\n220\n\np(0, 0) =\np(0, 1) =\np(0, 2) =\np(0, 3) =\n\n= 40\n220\n= 30\n220\n\n "}, {"Page_number": 249, "text": "234\n\nchapter 6\n\njointly distributed random variables\n\n(cid:2)\n(cid:2)\n(cid:2)\n(cid:2)\n(cid:2)\n(cid:2)\n\n(cid:3)(cid:2)\n(cid:3)(cid:2)\n(cid:3)(cid:2)\n(cid:3)(cid:2)\n(cid:3)(cid:2)\n(cid:3)(cid:26)(cid:2)\n\n5\n2\n4\n1\n4\n2\n5\n1\n4\n1\n\n5\n1\n\n(cid:3)(cid:26)(cid:2)\n(cid:3)(cid:2)\n(cid:3)(cid:26)(cid:2)\n(cid:3)(cid:26)(cid:2)\n(cid:3)(cid:26)(cid:2)\n(cid:3)\n\n12\n3\n\n12\n3\n\n(cid:3)\n(cid:3)(cid:26)(cid:2)\n(cid:3)\n(cid:3)\n(cid:3)\n\n12\n3\n12\n3\n12\n3\n= 1\n220\n\n3\n1\n3\n1\n3\n1\n3\n2\n3\n2\n3\n3\n\n(cid:3)\n= 30\n220\n\n12\n3\n= 18\n220\n= 15\n220\n= 12\n220\n\np(1, 0) =\np(1, 1) =\np(1, 2) =\np(2, 0) =\np(2, 1) =\np(3, 0) =\n\n= 60\n220\n\nthese probabilities can most easily be expressed in tabular form, as in table 6.1. the\nreader should note that the probability mass function of x is obtained by computing\nthe row sums, whereas the probability mass function of y is obtained by comput-\ning the column sums. because the individual probability mass functions of x and y\nthus appear in the margin of such a table, they are often referred to as the marginal\n.\nprobability mass functions of x and y, respectively.\n\ni\n\n0\n\n1\n\n2\n\n3\n\ntable 6.1: p{x = i, y = j}\n\nj\n\n0\n\n1\n\n2\n\n3\n\nrow sum = p{x = i}\n\n10\n220\n30\n220\n15\n220\n1\n220\n\n56\n220\n\n40\n220\n60\n220\n12\n220\n\n0\n\n112\n220\n\n30\n220\n18\n220\n\n0\n\n0\n\n4\n220\n\n0\n\n0\n\n0\n\n48\n220\n\n4\n220\n\n84\n220\n108\n220\n27\n220\n1\n220\n\ncolumn sum = p{y = j}\n\nexample 1b\nsuppose that 15 percent of the families in a certain community have no children, 20\npercent have 1 child, 35 percent have 2 children, and 30 percent have 3. suppose\nfurther that in each family each child is equally likely (independently) to be a boy or\na girl. if a family is chosen at random from this community, then b, the number of\nboys, and g, the number of girls, in this family will have the joint probability mass\nfunction shown in table 6.2.\n\n "}, {"Page_number": 250, "text": "section 6.1\n\njoint distribution functions 235\n\ntable 6.2: p{b = i, g = j}\n\nj\n\n0\n\n1\n\n2\n\n3\n\nrow sum = p{b = i}\n\n.15\n.10\n.0875\n.0375\n\n.10\n.175\n.1125\n0\n\n.0875\n.1125\n0\n0\n\n.0375\n0\n0\n0\n\n.3750\n.3875\n.2000\n.0375\n\ni\n\n0\n1\n2\n3\n\ncolumnsum = p{g = j}\n\n.3750\n\n.3875\n\n.2000\n\n.0375\n\nthe probabilities shown in table 6.2 are obtained as follows:\n\np{b = 0, g = 0} = p{no children} = .15\np{b = 0, g = 1} = p{1 girl and total of 1 child}\n\n= p{1 child}p{1 girl|1 child} = (.20)\np{b = 0, g = 2} = p{2 girls and total of 2 children}\n\n(cid:3)\n\n(cid:2)\n\n1\n2\n\n= p{2 children}p{2 girls|2 children} = (.35)\n\n(cid:3)2\n\n(cid:2)\n\n1\n2\n\nwe leave the verification of the remaining probabilities in the table to the reader. .\nwe say that x and y are jointly continuous if there exists a function f (x, y), defined\nfor all real x and y, having the property that, for every set c of pairs of real numbers\n(that is, c is a set in the two-dimensional plane),\n\np{(x, y) \u2208 c} =\n\nf (x, y) dx dy\n\n(1.3)\n\n(x,y)\u2208c\n\nthe function f (x, y) is called the joint probability density function of x and y. if a\nand b are any sets of real numbers, then, by defining c = {(x, y) : x \u2208 a, y \u2208 b}, we\nsee from equation (1.3) that\n\np{x \u2208 a, y \u2208 b} =\n\nf (x, y) dx dy\n\nb\n\na\n\n(1.4)\n\nbecause\n\nf(a, b) = p{x \u2208 (\u2212q, a], y \u2208 (\u2212q, b]}\n\n*\n\n*\n\n=\n\nb\n\u2212q\n\na\n\u2212q\n\nf (x, y) dx dy\n\nit follows, upon differentiation, that\n\nf (a, b) = \u22022\n\u2202a \u2202b\n\nf(a, b)\n\nwherever the partial derivatives are defined. another interpretation of the joint den-\nsity function, obtained from equation (1.4), is\n\n**\n\n*\n\n*\n\n "}, {"Page_number": 251, "text": "236\n\nchapter 6\n\njointly distributed random variables\n\np{a < x < a + da, b < y < b + db} =\n\n*\n\n*\n\nd+db\n\na+da\n\nb\n\na\n\nl f (a, b) da db\n\nf (x, y) dx dy\n\nwhen da and db are small and f (x, y) is continuous at a, b. hence, f (a, b) is a measure\nof how likely it is that the random vector (x, y) will be near (a, b).\n\nif x and y are jointly continuous, they are individually continuous, and their prob-\n\nability density functions can be obtained as follows:\n\np{x \u2208 a} = p{x \u2208 a, y \u2208 (\u2212q, q)}\n\n* q\n\n*\n*\n\na\n\n=\n\n=\n\nf (x, y) dy dx\n\n\u2212q\n\nfx (x) dx\n\n* q\n\nf (x, y) dy\n\n\u2212q\n\n* q\n\nfy (y) =\n\nf (x, y) dx\n\n\u2212q\n\nwhere\n\na\n\nfx (x) =\n\nis thus the probability density function of x. similarly, the probability density func-\ntion of y is given by\n\nexample 1c\nthe joint density function of x and y is given by\n\n0\n\nf (x, y) =\n\n\u22122y\n\n\u2212xe\n2e\n0\n\n0 < x < q, 0 < y < q\notherwise\n\ncompute (a) p{x > 1, y < 1}, (b) p{x < y}, and (c) p{x < a}.\nsolution.\n\n(a)\n\np{x > 1, y < 1} =\n=\n\n* q\n\n*\n*\n\n1\n\n1\n\n1\n\n0\n\n2e\n\n*\n\n\u22122y\n\n\u22122y dx dy\n\u2212xe\n(cid:29)\n(cid:30)\n66q\n2e\n\u2212e\n\u2212x\n0\n\u22121\n\u22122y dy\n2e\n\u22121(1 \u2212 e\n\u22122)\n\n1\n\n1\n\n0\n\ndy\n\n(b)\n\np{x < y} =\n\n=\n\n\u2212xe\n2e\n\n\u22122y dx dy\n\n\u2212xe\n2e\n\n\u22122y dx dy\n\n= e\n= e\n**\n* q\n*\n\n(x,y):x<y\ny\n\n0\n\n0\n\n "}, {"Page_number": 252, "text": "(c)\n\nsection 6.1\n\njoint distribution functions 237\n\n* q\n* q\n\n0\n\n=\n\n* q\n\u22122y(1 \u2212 e\n\u2212y)dy\n2e\n\u22122ydy \u2212\n=\n2e\n= 1 \u2212 2\n3\n= 1\n3\n\n0\n\n0\n\n\u22123ydy\n2e\n\n* q\n\n*\n*\n\na\n\np{x < a} =\n\u22122ye\n2e\n=\n\u2212x dx\ne\n= 1 \u2212 e\n\u2212a\n\n0\n\n0\n\n0\n\na\n\n\u2212x dy dx\n\n.\n\nexample 1d\nconsider a circle of radius r, and suppose that a point within the circle is randomly\nchosen in such a manner that all regions within the circle of equal area are equally\nlikely to contain the point. (in other words, the point is uniformly distributed within\nthe circle.) if we let the center of the circle denote the origin and define x and y to be\nthe coordinates of the point chosen (figure 6.1), then, since (x, y) is equally likely\nto be near each point in the circle, it follows that the joint density function of x and\ny is given by\n\n0\n\nf (x, y) =\n\nc ifx2 + y2 \u2026 r2\n0 ifx2 + y2 > r2\n\nfor some value of c.\n(a) determine c.\n(b) find the marginal density functions of x and y.\n(c) compute the probability that d, the distance from the origin of the point selected,\n\nis less than or equal to a.\n\n(d) find e [d].\n\ny\n\nr\n\n(x, y)\n\n(0, 0)\n\nx\n\nfigure 6.1: joint probability distribution.\n\n "}, {"Page_number": 253, "text": "238\n\nchapter 6\n\njointly distributed random variables\n\nsolution.\n\n(a) because\n\nit follows that\n\n\u2212q\n\n* q\n\n* q\n**\n\n\u2212q\n\nf (x, y) dy dx = 1\n\ndy dx = 1\n\nc\nx2+y2\u2026r2\n\n--\n\nx2+y2\u2026r2 dy dx either by using polar coordinates or,\nwe can evaluate\nmore simply, by noting that it represents the area of the circle and is thus equal\nto \u03c0r2. hence,\n\nc = 1\n\u03c0r2\n\n* q\n\n(b)\n\ndy\n\n.\n\nf (x, y) dy\n\nx2 \u2026 r2\n\nfx (x) =\n\nr2 \u2212 x2\n\ndy, where c =\n\n\u2212q\n= 1\n\u03c0r2\n= 1\n\u03c0r2\n= 2\n\u03c0r2\n\nand it equals 0 when x2 > r2. by symmetry, the marginal density of y is given by\n\n*\n*\nx2+y2\u2026r2\nc\n.\n\u2212c\nr2 \u2212 x2\n8\n.\nx2 + y2, the distance from the origin, is\n.\nobtained as follows: for 0 \u2026 a \u2026 r,\nfd(a) = p{\nx2 + y2 \u2026 a}\n**\n= p{x2 + y2 \u2026 a2}\n=\nf (x, y) dy dx\n\nfy (y) = 2\n\u03c0r2\n= 0\n(c) the distribution function of d =\n\ny2 \u2026 r2\ny2 > r2\n\nr2 \u2212 y2\n\n**\n\nx2+y2\u2026a2\n= 1\n\u03c0r2\n\nx2+y2\u2026a2\n\ndy dx\n\n= \u03c0a2\n\u03c0r2\n= a2\n--\nr2\nx2+y2\u2026a2 dy dx is the area of a circle of radius\n\nwhere we have used the fact that\na and thus is equal to \u03c0a2.\n\n "}, {"Page_number": 254, "text": "(d) from part (c), the density function of d is\n\nsection 6.1\n\njoint distribution functions 239\n\nhence,\n\nfd(a) = 2a\nr2\n*\n\ne[d] = 2\nr2\n\n0 \u2026 a \u2026 r\n\nr\n\na2da = 2r\n3\n\n0\n\n.\n\nexample 1e\nthe joint density of x and y is given by\n\n%\n\nf (x, y) =\n\n\u2212(x+y) 0 < x < q, 0 < y < q\ne\n0\n\notherwise\n\nfind the density function of the random variable x/y.\n\nsolution. we start by computing the distribution function of x/y. for a > 0,\n\nx\ny\n\n*\n\n%\n**\nfx/y (a) = p\n=\n* q\n* q\n0\n\n=\n\n=\n\n0\n\n0\n\nx/y\u2026a\n\n=\n\n\u2212e\n\n/\n\n\u2026 a\n\u2212(x+y) dx dy\ne\n\n0\n\nay\n\n\u2212(x+y) dx dy\ne\n(1 \u2212 e\n\u2212ay)e\n\u2212ydy\n\u2212(a+1)y\na + 1\n\n\u2212y + e\n\n7666666q\n\n0\n\n= 1 \u2212\n\n1\n\na + 1\n\ndifferentiation shows that the density function of x/y is given by fx/y (a) = 1/\n(a + 1)2, 0 < a < q.\n.\n\nwe can also define joint probability distributions for n random variables in exactly\nthe same manner as we did for n = 2. for instance, the joint cumulative probabil-\nity distribution function f(a1, a2, . . . , an) of the n random variables x1, x2, . . . , xn is\ndefined by\n\nf(a1, a2, . . . , an) = p{x1 \u2026 a1, x2 \u2026 a2, . . . , xn \u2026 an}\n\nfurther, the n random variables are said to be jointly continuous if there exists a\nfunction f (x1, x2, . . . , xn), called the joint probability density function, such that, for\nany set c in n-space,\n\np{(x1, x2, . . . , xn) \u2208 c} =\n\nf (x1, . . . , xn)dx1dx2 \u00b7\u00b7\u00b7 dxn\n\n**\n\n*\n\u00b7\u00b7\u00b7\n(x1,...,xn)\u2208c\n\n "}, {"Page_number": 255, "text": "240\n\nchapter 6\n\njointly distributed random variables\n\nin particular, for any n sets of real numbers a1, a2, . . . , an,\n\np{x1 \u2208 a1, x2,\u2208 a2, . . . , xn \u2208 an}\n\n*\n\n*\n\n*\n\n=\n\n\u00b7\u00b7\u00b7\n\nf (x1, . . . , xn) dx1dx2 \u00b7\u00b7\u00b7 dxn\n\nan\n\nan\u22121\n\na1\n\nexample 1f the multinomial distribution\none of the most important joint distributions is the multinomial distribution, which\narises when a sequence of n independent and identical experiments is performed.\nsuppose that each experiment can result in any one of r possible outcomes, with\npi = 1. if we let xi denote the number of the\nrespective probabilities p1, p2, . . . , pr,\nn experiments that result in outcome number i, then\n\nr(cid:9)\n\ni=1\n\np{x1 = n1, x2 = n2, . . . , xr = nr} =\nr(cid:9)\nni = n.\n\nwhenever\n\ni=1\n\nn!\n\nn1!n2!\u00b7\u00b7\u00b7 nr!\n\npn1\n1 pn2\n2\n\n\u00b7\u00b7\u00b7 pnr\n\nr\n\n(1.5)\n\n. . . pnr\n\n1 pn2\n2\n\nequation (1.5) is verified by noting that any sequence of outcomes for the n experi-\nments that leads to outcome i occurring ni times for i = 1, 2, . . . , r will, by the assumed\nindependence of experiments, have probability pn1\nr of occurring. because\nthere are n!/(n1!n2! . . . nr!) such sequences of outcomes (there are n!/n1! . . . nr! dif-\nferent permutations of n things of which n1 are alike, n2 are alike, . . . , nr are alike),\nequation (1.5) is established. the joint distribution whose joint probability mass func-\ntion is specified by equation (1.5) is called the multinomial distribution. note that\nwhen r = 2, the multinomial reduces to the binomial distribution.\nthat is, if n ( {1, 2, . . . , r}, then\n\n(cid:8)\ni s will have a binomial distribution.\ni\u2208n xi will be a binomial random variable with\ni\u2208n xi represents the number\nof the n experiments whose outcome is in n, and each experiment will independently\nhave such an outcome with probability\n\nparameters n and p =(cid:9)\n\nnote also that any sum of a fixed set of the x\n\ni\u2208n pi. this follows because\n\n(cid:9)\n\n(cid:9)\n\n(cid:9)\n\ni\u2208n pi.\n\nas an application of the multinomial distribution, suppose that a fair die is rolled\n9 times. the probability that 1 appears three times, 2 and 3 twice each, 4 and 5 once\neach, and 6 not at all is\n\n(cid:2)\n\n(cid:3)3(cid:2)\n\n(cid:3)2(cid:2)\n\n(cid:3)2(cid:2)\n\n(cid:3)1(cid:2)\n\n(cid:3)1(cid:2)\n\n9!\n\n3!2!2!1!1!0!\n\n1\n6\n\n1\n6\n\n1\n6\n\n1\n6\n\n1\n6\n\n1\n6\n\n(cid:3)0 = 9!\n\n3!2!2!\n\n(cid:2)\n\n(cid:3)9\n\n1\n6\n\n.\n\n6.2 independent random variables\n\nthe random variables x and y are said to be independent if, for any two sets of real\nnumbers a and b,\n\np{x \u2208 a, y \u2208 b} = p{x \u2208 a}p{y \u2208 b}\n\n(2.1)\nin other words, x and y are independent if, for all a and b, the events ea = {x \u2208 a}\nand fb = {y \u2208 b} are independent.\nit can be shown by using the three axioms of probability that equation (2.1) will\nfollow if and only if, for all a, b,\n\np{x \u2026 a, y \u2026 b} = p{x \u2026 a}p{y \u2026 b}\n\n "}, {"Page_number": 256, "text": "section 6.2\n\nindependent random variables 241\n\nhence, in terms of the joint distribution function f of x and y, x and y are inde-\npendent if\n\nf(a, b) = fx (a)fy (b)\n\nfor all a, b\n\nwhen x and y are discrete random variables, the condition of independence (2.1) is\nequivalent to\n\np(x, y) = px (x)py (y)\n\nfor all x, y\n\n(2.2)\n\nthe equivalence follows because, if equation (2.1) is satisfied, then we obtain equa-\ntion (2.2) by letting a and b be, respectively, the one-point sets a = {x} and b = {y}.\nfurthermore, if equation (2.2) is valid, then, for any sets a, b,\n\np{x \u2208 a, y \u2208 b} =\n=\n\n(cid:6)\n(cid:6)\n\nx\u2208a\n\n(cid:6)\n(cid:6)\n(cid:6)\n\ny\u2208b\n\ny\u2208b\n\np(x, y)\n\n(cid:6)\n\npx (x)py (y)\n\nx\u2208a\npy (y)\n\n=\npx (x)\n= p{y \u2208 b}p{x \u2208 a}\n\nx\u2208a\n\ny\u2208b\n\nand equation (2.1) is established.\n\nin the jointly continuous case, the condition of independence is equivalent to\n\nf (x, y) = fx (x)fy (y)\n\nfor all x, y\n\nthus, loosely speaking, x and y are independent if knowing the value of one does\nnot change the distribution of the other. random variables that are not independent\nare said to be dependent.\n\nexample 2a\nsuppose that n + m independent trials having a common probability of success p are\nperformed. if x is the number of successes in the first n trials, and y is the number\nof successes in the final m trials, then x and y are independent, since knowing the\nnumber of successes in the first n trials does not affect the distribution of the number\nof successes in the final m trials (by the assumption of independent trials). in fact, for\nintegral x and y,\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\np{x = x, y = y} =\n\nn\nx\n\npx(1 \u2212 p)n\u2212x\n= p{x = x}p{y = y}\n\nm\ny\n\npy(1 \u2212 p)m\u2212y\n\n0 \u2026 x \u2026 n,\n0 \u2026 y \u2026 m\n\nin contrast, x and z will be dependent, where z is the total number of successes in\nthe n + m trials. (why?)\n.\n\nexample 2b\nsuppose that the number of people who enter a post office on a given day is a poisson\nrandom variable with parameter \u03bb. show that if each person who enters the post office\nis a male with probability p and a female with probability 1 \u2212 p, then the number of\nmales and females entering the post office are independent poisson random variables\nwith respective parameters \u03bbp and \u03bb(1 \u2212 p).\n\n "}, {"Page_number": 257, "text": "242\n\nchapter 6\n\njointly distributed random variables\n\nsolution. let x and y denote, respectively, the number of males and females that\nenter the post office. we shall show the independence of x and y by establishing\nequation (2.2). to obtain an expression for p{x = i, y = j}, we condition on x + y\nas follows:\n\np{x = i, y = j} = p{x = i, y = j|x + y = i + j}p{x + y = i + j}\n\n+ p{x = i, y = j|x + y z i + j}p{x + y z i + j}\n\n[note that this equation is merely a special case of the formula p(e) = p(e|f)p(f) +\np(e|fc)p(fc).]\n\nsince p{x = i, y = j|x + y z i + j} is clearly 0, we obtain\n\np{x = i, y = j} = p{x = i, y = j|x + y = i + j}p{x + y = i + j}\n\n(2.3)\nnow, because x + y is the total number of people who enter the post office, it\nfollows, by assumption, that\n\n\u2212\u03bb \u03bbi+j\n(i + j)!\n\np{x + y = i + j} = e\n\n(2.4)\nfurthermore, given that i + j people do enter the post office, since each person\n(cid:2)\nentering will be male with probability p, it follows that the probability that exactly\ni of them will be male (and thus j of them female) is just the binomial probability\ni + j\ni\n\npi(1 \u2212 p)j. that is,\n\n(cid:3)\n\np{x = i, y = j|x + y = i + j} =\n\npi(1 \u2212 p)j\n\n(2.5)\n\n(cid:3)\n\n(cid:2)\n\ni + j\ni\n\nsubstituting equations (2.4) and (2.5) into equation (2.3) yields\n\u2212\u03bb \u03bbi+j\n(i + j)!\n\ni + j\ni\n\n(cid:2)\n\n(cid:3)\n\np{x = i, y = j} =\n\u2212\u03bb (\u03bbp)i\n= e\ni!j!\n\u2212\u03bbp(\u03bbp)i\n= e\n(cid:6)\n\ni!\n\npi(1 \u2212 p)je\n[\u03bb(1 \u2212 p)]j\n\u2212\u03bb(1\u2212p) [\u03bb(1 \u2212 p)]j\ne\n\nj!\n\np{x = i} = e\n\n\u2212\u03bbp\n\n(\u03bbp)i\n\ni!\n\nj\n\n\u2212\u03bb(1\u2212p) [\u03bb(1 \u2212 p)]j\ne\n\n= e\n\n\u2212\u03bbp\n\n(\u03bbp)i\n\ni!\n\nhence,\n\nand similarly,\n\np{y = j} = e\n\n\u2212\u03bb(1\u2212p) [\u03bb(1 \u2212 p)]j\n\nequations (2.6), (2.7), and (2.8) establish the desired result.\n\nj!\n\nj!\n\n(2.6)\n\n(2.7)\n\n(2.8)\n\n.\n\n "}, {"Page_number": 258, "text": "section 6.2\n\nindependent random variables 243\n\nexample 2c\na man and a woman decide to meet at a certain location. if each of them indepen-\ndently arrives at a time uniformly distributed between 12 noon and 1 p.m., find the\nprobability that the first to arrive has to wait longer than 10 minutes.\n\nsolution. if we let x and y denote, respectively, the time past 12 that the man and\nthe woman arrive, then x and y are independent random variables, each of which is\nuniformly distributed over (0, 60). the desired probability, p{x + 10 < y} + p{y +\n10 < x}, which, by symmetry, equals 2p{x + 10 < y}, is obtained as follows:\n\n2p{x + 10 < y} = 2\n\n= 2\n\n**\n**\n\nx+10<y\n\n*\n\nx+10<y\n\n*\n*\n\n60\n\n= 2\n10\n= 2\n(60)2\n\n= 25\n36\n\nf (x, y) dx dy\n\nfx (x)fy (y) dx dy\n\n(cid:2)\n\n(cid:3)2\n\n1\n60\n\ndx dy\n\ny\u221210\n\n0\n\n60\n\n10\n\n(y \u2212 10) dy\n\n.\n\nour next example presents the oldest problem dealing with geometrical probabil-\nities. it was first considered and solved by buffon, a french naturalist of the 18th\ncentury, and is usually referred to as buffon\u2019s needle problem.\n\nexample 2d buffon\u2019s needle problem\na table is ruled with equidistant parallel lines a distance d apart. a needle of length l,\nwhere l \u2026 d, is randomly thrown on the table. what is the probability that the nee-\ndle will intersect one of the lines (the other possibility being that the needle will be\ncompletely contained in the strip between two lines)?\n\nsolution. let us determine the position of the needle by specifying (1) the distance x\nfrom the middle point of the needle to the nearest parallel line and (2) the angle \u03b8\nbetween the needle and the projected line of length x. (see figure 6.2.) the needle\nwill intersect a line if the hypotenuse of the right triangle in figure 6.2 is less than\nl/2\u2014that is, if\n\nx\n\ncos \u03b8\n\nl\n2\n\nor x <\n\ncos \u03b8\n\nl\n2\n\n<\n\n\u242ax\n\nfigure 6.2\n\n "}, {"Page_number": 259, "text": "244\n\nchapter 6\n\njointly distributed random variables\n\nas x varies between 0 and d/2 and \u03b8 between 0 and \u03c0/2, it is reasonable to assume\nthat they are independent, uniformly distributed random variables over these respec-\ntive ranges. hence,\n\n%\n\np\n\nx <\n\nl\n2\n\ncos \u03b8\n\n=\n\nfx (x)f\u03b8 (y) dx dy\n\n/\n\nx<l/2 cos y\n\n**\n* \u03c0/2\n* \u03c0/2\n\n0\n\n0\n\n*\n\n0\nl\n2\n\n= 4\n\u03c0d\n= 4\n\u03c0d\n= 2l\n\u03c0d\n\nl/2 cos y\n\ndx dy\n\ncos y dy\n\n.\n\n\u2217\n\nexample 2e characterization of the normal distribution\nlet x and y denote the horizontal and vertical miss distances when a bullet is fired\nat a target, and assume that\n\n1. x and y are independent continuous random variables having differentiable\n2. the joint density f (x, y) = fx (x)fy (y) of x and y depends on (x, y) only through\n\ndensity functions.\nx2 + y2.\n\nloosely put, assumption 2 states that the probability of the bullet landing on any\npoint of the x\u2013y plane depends only on the distance of the point from the target and\nnot on its angle of orientation. an equivalent way of phrasing this assumption is to\nsay that the joint density function is rotation invariant.\n\nit is a rather interesting fact that assumptions 1 and 2 imply that x and y are\nnormally distributed random variables. to prove this, note first that the assumptions\nyield the relation\n\nf (x, y) = fx (x)fy (y) = g(x2 + y2)\n\nfor some function g. differentiating equation (2.9) with respect to x yields\n\n(cid:8)\nf\nx\n\n(x)fy (y) = 2xg\n\n(cid:8)(x2 + y2)\n\ndividing equation (2.10) by equation (2.9) gives\n\nor\n\n(cid:8)\nf\n(x)\nx\n2xfx (x)\n\n(cid:8)\nf\n(x)\nx\nfx (x)\n\n= 2xg\n\n(cid:8)(x2 + y2)\ng(x2 + y2)\n(cid:8)(x2 + y2)\ng(x2 + y2)\n\n= g\n\nbecause the value of the left-hand side of equation (2.11) depends only on x,\nwhereas the value of the right-hand side depends on x2 + y2, it follows that the\nleft-hand side must be the same for all x. to see this, consider any x1, x2 and let y1, y2\nbe such that x2\n1\n\n= x2\n\n2\n\n1\n\n+ y2\n(cid:8)\nf\nx\n\n(x1)\n\n2x1fx (x1)\n\n+ y2\n2. then, from equation (2.11), we obtain\n(cid:8)(x2\n= g\n1\ng(x2\n1\n\n+ y2\n+ y2\n\n+ y2\n+ y2\n\n(cid:8)(x2\n2\ng(x2\n2\n\n(cid:8)\n= f\nx\n\n2x2fx (x2)\n\n= g\n\n(x2)\n\n1\n)\n\n2\n)\n\n1\n\n2\n\n)\n\n)\n\n(2.9)\n\n(2.10)\n\n(2.11)\n\n "}, {"Page_number": 260, "text": "section 6.2\n\nindependent random variables 245\n\nhence,\n\n(cid:8)\nf\n(x)\nx\nxfx (x)\n\n= c or\n\n(log fx (x)) = cx\n\nd\ndx\n\nwhich implies, upon integration of both sides, that\n\n- q\n\u2212q fx (x) dx = 1, it follows that c is necessarily negative, and we may write\n\nlog fx (x) = a + cx2\n2\n\nfx (x) = kecx2/2\n\nor\n\nsince\nc = \u22121/\u03c3 2. thus,\n\nfx (x) = ke\n\n\u2212x2/2\u03c3 2\n\nthat is, x is a normal random variable with parameters \u03bc = 0 and \u03c3 2. a similar\nargument can be applied to fy (y) to show that\n\nfy (y) =\n\n1\u221a\n2\u03c0 \u03c3\n\n\u2212y2/2\u03c3 2\ne\n\nfurthermore, it follows from assumption 2 that \u03c3 2 = \u03c3 2 and that x and y are thus\nindependent, identically distributed normal random variables with parameters \u03bc = 0\n.\nand \u03c3 2.\n\na necessary and sufficient condition for the random variables x and y to be\nindependent is for their joint probability density function (or joint probability mass\nfunction in the discrete case) f (x, y) to factor into two terms, one depending only on\nx and the other depending only on y.\nproposition 2.1. the continuous (discrete) random variables x and y are indepen-\ndent if and only if their joint probability density (mass) function can be expressed as\n\nfx,y (x, y) = h(x)g(y)\n\n\u2212 q < x < q,\u2212q < y < q\n\nproof. let us give the proof in the continuous case. first, note that independence\nimplies that the joint density is the product of the marginal densities of x and y, so\nthe preceding factorization will hold when the random variables are independent.\nnow, suppose that\n\nthen\n\nwhere c1 =- q\n\nfx,y (x, y) dx dy\n\n\u2212q\n\n\u2212q\n\n* q\n\n* q\n\nfx,y (x, y) = h(x)g(y)\n* q\n* q\n1 =\n=\n\u2212q\n\u2212q h(x) dx and c2 =- q\n= c1c2\n* q\n* q\n\nh(x) dx\n\n\u2212q\n\n\u2212q\n\nfx (x) =\nfy (y) =\n\n\u2212q g(y) dy. also,\nfx,y (x, y) dy = c2h(x)\nfx,y (x, y) dx = c1g(y)\n\ng(y) dy\n\n\u2212q\n\n "}, {"Page_number": 261, "text": "246\n\nchapter 6\n\njointly distributed random variables\nsince c1c2 = 1, it follows that\n\nfx,y (x, y) = fx (x)fy (y)\n\nand the proof is complete.\n\nexample 2f\nif the joint density function of x and y is\n\nf (x, y) = 6e\n\n\u22122xe\n\n\u22123y\n\n0 < x < q, 0 < y < q\n\nand is equal to 0 outside this region, are the random variables independent? what if\nthe joint density function is\nf (x, y) = 24xy\nand is equal to 0 otherwise?\n\n0 < x < 1, 0 < y < 1, 0 < x + y < 1\n\nsolution. in the first instance, the joint density function factors, and thus the random\nvariables, are independent (with one being exponential with rate 2 and the other\nexponential with rate 3). in the second instance, because the region in which the joint\ndensity is nonzero cannot be expressed in the form x \u2208 a, y \u2208 b, the joint density does\nnot factor, so the random variables are not independent. this can be seen clearly by\nletting\n\nif 0 < x < 1, 0 < y < 1, 0 < x + y < 1\notherwise\n\n%\n\n1\n0\n\ni(x, y) =\n\nand writing\n\nf (x, y) = 24xy i(x, y)\n\nwhich clearly does not factor into a part depending only on x and another depending\n.\nonly on y.\n\nthe concept of independence may, of course, be defined for more than two random\nvariables. in general, the n random variables x1, x2, . . . , xn are said to be indepen-\ndent if, for all sets of real numbers a1, a2, . . . , an,\n\np{x1 \u2208 a1, x2 \u2208 a2, . . . , xn \u2208 an} = n(cid:31)\n\np{xi \u2208 ai}\n\ni=1\n\nas before, it can be shown that this condition is equivalent to\n\np{x1 \u2026 a1, x2 \u2026 a2, . . . , xn \u2026 an}\n\np{xi \u2026 ai}\n\nfor all a1, a2, . . . , an\n\n= n(cid:31)\n\ni=1\n\nfinally, we say that an infinite collection of random variables is independent if every\nfinite subcollection of them is independent.\n\nexample 2g how can a computer choose a random subset?\nmost computers are able to generate the value of, or simulate, a uniform (0, 1) random\nvariable by means of a built-in subroutine that (to a high degree of approximation)\n\n "}, {"Page_number": 262, "text": "section 6.2\n\nindependent random variables 247\n\nproduces such \u201crandom numbers.\u201d as a result, it is quite easy for a computer to sim-\nulate an indicator (that is, a bernoulli) random variable. suppose i is an indicator\nvariable such that\n\np{i = 1} = p = 1 \u2212 p{i = 0}\n\nthe computer can simulate i by choosing a uniform (0, 1) random number u and\nthen letting\n\nsuppose that we are interested in having the computer select k, k \u2026 n, of the num-\nbers 1, 2, . . . , n in such a way that each of the\nsubsets of size k is equally likely\nto be chosen. we now present a method that will enable the computer to solve this\ntask. to generate such a subset, we will first simulate, in sequence, n indicator vari-\nables i1, i2, . . . , in, of which exactly k will equal 1. those i for which ii = 1 will then\nconstitute the desired subset.\nto generate the random variables i1, . . . , in, start by simulating n independent uni-\n\nn\nk\n\nform (0, 1) random variables u1, u2, . . . , un. now define\n\ni = 1 if u < p\n0 if u \u00fa p\n(cid:3)\n(cid:2)\n\n\u23a7\u23aa\u23a8\n\u23aa\u23a9 1\n\n0\n\ni1 =\n\nk\nif u1 <\nn\notherwise\n\nand then, once i1, . . . , ii are determined, recursively set\n\n\u23a7\u23aa\u23a8\n\u23aa\u23a9 1\n\n0\n\nii+1 =\n\nk \u2212 (i1 + \u00b7\u00b7\u00b7 + ii)\n\nn \u2212 i\n\nif ui+1 <\notherwise\n\n\u239b\n\u239e\nin words, at the (i + 1)th stage we set ii+1 equal to 1 (and thus put i + 1 into the\n\u239dnamely, k \u2212 i(cid:9)\n\u23a0, divided by the remaining number of possibilities (namely,\ndesired subset) with a probability equal to the remaining number of places in the sub-\n\nset\nn \u2212 i). hence, the joint distribution of i1, i2, . . . , in is determined from\n\nj=1\n\nij\n\np{i1 = 1} = k\nn\n\nk \u2212 i(cid:6)\n\nj=1\nn \u2212 i\n\np{ii+1 = 1|i1, . . . , ii} =\n\nij\n\n1 < i < n\n\nthe proof that the preceding formula results in all subsets of size k being equally\nlikely to be chosen is by induction on k + n. it is immediate when k + n = 2 (that\nis, when k = 1, n = 1), so assume it to be true whenever k + n \u2026 l. now, suppose\nthat k + n = l + 1, and consider any subset of size k\u2014say, i1 \u2026 i2 \u2026 \u00b7\u00b7\u00b7 \u2026 ik\u2014and\nconsider the following two cases.\n\n "}, {"Page_number": 263, "text": "248\n\nchapter 6\n\njointly distributed random variables\n\ncase 1:\n\ni1 = 1\np{i1 = ii2\n\n= \u00b7\u00b7\u00b7 = iik\n= p{i1 = 1}p{ii2\n\n= 1, ij = 0 otherwise|i1 = 1}\n\n= 1, ij = 0 otherwise}\n= \u00b7\u00b7\u00b7 = iik\n(cid:2)\n\n(cid:3)\n\nnow given that i1 = 1, the remaining elements of the subset are chosen as if a\nsubset of size k \u2212 1 were to be chosen from the n \u2212 1 elements 2, 3, . . . , n. hence,\nby the induction hypothesis, the conditional probability that this will result in a given\nsubset of size k \u2212 1 being selected is 1/\np{i1 = ii2\n= \u00b7\u00b7\u00b7 = iik\n(cid:2)\n= k\nn \u2212 1\nn\nk \u2212 1\n\nn \u2212 1\nk \u2212 1\n= 1, ij = 0 otherwise}\n(cid:3) = 1(cid:2)\n(cid:3)\n\n. hence,\n\nn\nk\n\n1\n\ncase 2:\n\ni1 z 1\np{ii1\n\n= 1, ij = 0 otherwise}\n(cid:3)\n= 1, ij = 0 otherwise|i1 = 0}p{i1 = 0}\n\n= ii2\n= p{ii1\n(cid:2)\n=\n\n= \u00b7\u00b7\u00b7 = iik\n= \u00b7\u00b7\u00b7 = iik\n1\n\n(cid:3)(cid:2)\n\nn \u2212 1\n\nk\n\n1 \u2212 k\nn\n\n= 1(cid:2)\n\n(cid:3)\n\nn\nk\n\nwhere the induction hypothesis was used to evaluate the preceding conditional prob-\nability.\n\nthus, in all cases, the probability that a given subset of size k will be the subset\n.\n\n<(cid:2)\n\n(cid:3)\n\nchosen is 1\n\nn\nk\n\n.\n\nremark. the foregoing method for generating a random subset has a very low\nmemory requirement. a faster algorithm that requires somewhat more memory is\npresented in section 10.1. (the latter algorithm uses the last k elements of a random\n.\npermutation of 1, 2, . . . , n.)\n\nexample 2h\nlet x, y, z be independent and uniformly distributed over (0, 1). compute p{x \u00fa\nyz}.\nsolution. since\n\n0 \u2026 x \u2026 1, 0 \u2026 y \u2026 1, 0 \u2026 z \u2026 1\n\nwe have\n\nfx,y,z(x, y, z) = fx (x)fy (y)fz(z) = 1\n* * *\n*\n*\n\np{x \u00fa yz} =\n\nx\u00fayz\n1\n\n*\n\n1\n\n1\n\nfx, y, z(x, y, z) dx dy dz\n\n=\n\ndx dy dz\n\n0\n\n0\n\nyz\n\n "}, {"Page_number": 264, "text": "section 6.2\n\nindependent random variables 249\n\n*\n*\n\n1\n\n1\n\n0\n\n1\n\n*\n(cid:2)\n(1 \u2212 yz) dy dz\n1 \u2212 z\n2\n\n(cid:3)\n\ndz\n\n0\n\n=\n\n=\n\n0\n= 3\n4\n\n.\n\nexample 2i probabilistic interpretation of half-life\nlet n(t) denote the number of nuclei contained in a radioactive mass of material at\ntime t. the concept of half-life is often defined in a deterministic fashion by stating\nthis it is an empirical fact that, for some value h, called the half-life,\n\nn(t) = 2\n\n\u2212t/hn(0)\n\nt > 0\n\n[note that n(h) = n(0)/2.] since the preceding implies that, for any nonnegative s\nand t,\n\nn(t + s) = 2\n\n\u2212(s+t)/hn(0) = 2\n\n\u2212t/hn(s)\n\nit follows that no matter how much time s has already elapsed, in an additional time t\nthe number of existing nuclei will decrease by the factor 2\n\n\u2212t/h.\n\nbecause the deterministic relationship just given results from observations of radio-\nactive masses containing huge numbers of nuclei, it would seem that it might be\nconsistent with a probabilistic interpretation. the clue to deriving the appropriate\nprobability model for half-life resides in the empirical observation that the propor-\ntion of decay in any time interval depends neither on the total number of nuclei at\nthe beginning at the interval nor on the location of this interval (since n(t + s)/n(s)\ndepends neither on n(s) nor on s). thus, it appears that the individual nuclei act inde-\npendently and with a memoryless life distribution. consequently, since the unique life\ndistribution that is memoryless is the exponential distribution, and since exactly one-\nhalf of a given amount of mass decays every h time units, we propose the following\nprobabilistic model for radioactive decay.\n\nprobabilistic interpretation of the half-life h: the lifetimes of the individual nuclei\nare independent random variables having a life distribution that is exponential with\nmedian equal to h. that is, if l represents the lifetime of a given nucleus, then\n\n(because p{l < h} = 1\n\n\u2212t/h\n\np{l < t} = 1 \u2212 2\n%\n\u2212t\n\np{l < t} = 1 \u2212 exp\n\n2 and the preceding can be written as\n\n/\n\nlog 2\n\nh\n\nit can be seen that l indeed has an exponential distribution with median h.)\n\nnote that, under the probabilistic interpretation of half-life just given, if one starts\nwith n(0) nuclei at time 0, then n(t), the number of nuclei that remain at time t,\nwill have a binomial distribution with parameters n = n(0) and p = 2\n\u2212t/h. results of\nchapter 8 will show that this interpretation of half-life is consistent with the determin-\nistic model when considering the proportion of a large number of nuclei that decay\nover a given time frame. however, the difference between the deterministic and prob-\nabilistic interpretation becomes apparent when one considers the actual number of\n\n "}, {"Page_number": 265, "text": "250\n\nchapter 6\n\njointly distributed random variables\n\ndecayed nuclei. we will now indicate this with regard to the question of whether\nprotons decay.\nthere is some controversy over whether or not protons decay. indeed, one theory\npredicts that protons should decay with a half-life of about h = 1030 years. to check\nthis prediction empirically, it has been suggested that one follow a large number of\nprotons for, say, one or two years and determine whether any of them decay within\nthat period. (clearly, it would not be feasible to follow a mass of protons for 1030\nyears to see whether one-half of it decays.) let us suppose that we are able to keep\ntrack of n(0) = 1030 protons for c years. the number of decays predicted by the\ndeterministic model would then be given by\n\nn(0) \u2212 n(c) = h(1 \u2212 2\n\u2212c/h)\n= 1 \u2212 2\n\u2212c/h\n\n\u2212cx\n\n1/h\n1 \u2212 2\n\u221230 l 0\nx\n\u2212cx log 2) by l\u2019h \u02c6opital\u2019s rule\n\n= 10\n\nsince\n\n1\nh\n\nl lim\nx\u21920\n= lim\nx\u21920\n= c log 2 l .6931c\n\n(c2\n\nfor instance, the deterministic model predicts that in 2 years there should be 1.3863\ndecays, and it would thus appear to be a serious blow to the hypothesis that protons\ndecay with a half-life of 1030 years if no decays are observed over those 2 years.\n\nlet us now contrast the conclusions just drawn with those obtained from the prob-\nabilistic model. again, let us consider the hypothesis that the half-life of protons is\nh = 1030 years, and suppose that we follow h protons for c years. since there is a\nhuge number of independent protons, each of which will have a very small probabil-\nity of decaying within this time period, it follows that the number of protons which\ndecay will have (to a very strong approximation) a poisson distribution with parame-\nter equal to h(1 \u2212 2\n\n\u2212c/h) l c log 2. thus,\np{0 decays} = e\n\u2212c log 2\n= e\n\u2212 log(2c) = 1\n2c\n\nand, in general,\n\np{n decays} = 2\n\n\u2212c[c log 2]n\n\nn!\n\nn \u00fa 0\n\nthus we see that even though the average number of decays over 2 years is (as pre-\ndicted by the deterministic model) 1.3863, there is 1 chance in 4 that there will not\nbe any decays, thereby indicating that such a result in no way invalidates the original\n.\nhypothesis of proton decay.\n\nremark.\n\nindependence is a symmetric relation. the random variables x and y\nare independent if their joint density function (or mass function in the discrete case)\nis the product of their individual density (or mass) functions. therefore, to say that\nx is independent of y is equivalent to saying that y is independent of x\u2014or just\nthat x and y are independent. as a result, in considering whether x is independent\nof y in situations where it is not at all intuitive that knowing the value of y will not\nchange the probabilities concerning x, it can be beneficial to interchange the roles of\n\n "}, {"Page_number": 266, "text": "section 6.2\n\nindependent random variables 251\n\nx and y and ask instead whether y is independent of x. the next example illustrates\n.\nthis point.\n\nexample 2j\nif the initial throw of the dice in the game of craps results in the sum of the dice\nequaling 4, then the player will continue to throw the dice until the sum is either 4 or\n7. if this sum is 4, then the player wins, and if it is 7, then the player loses. let n denote\nthe number of throws needed until either 4 or 7 appears, and let x denote the value\n(either 4 or 7) of the final throw. is n independent of x? that is, does knowing which\nof 4 or 7 occurs first affect the distribution of the number of throws needed until that\nnumber appears? most people do not find the answer to this question to be intuitively\nobvious. however, suppose that we turn it around and ask whether x is independent\nof n. that is, does knowing how many throws it takes to obtain a sum of either 4 or\n7 affect the probability that that sum is equal to 4? for instance, suppose we know\nthat it takes n throws of the dice to obtain a sum of either 4 or 7. does this affect the\nprobability distribution of the final sum? clearly not, since all that is important is that\nits value is either 4 or 7, and the fact that none of the first n \u2212 1 throws were either 4\nor 7 does not change the probabilities for the nth throw. thus, we can conclude that\nx is independent of n, or equivalently, that n is independent of x.\n\nas another example, let x1, x2, . . . be a sequence of independent and identically\ndistributed continuous random variables, and suppose that we observe these random\nvariables in sequence. if xn > xi for each i = 1, . . . , n \u2212 1, then we say that xn is\na record value. that is, each random variable that is larger than all those preceding\nit is called a record value. let an denote the event that xn is a record value. is an+1\nindependent of an? that is, does knowing that the nth random variable is the largest\nof the first n change the probability that the (n + 1)st random variable is the largest\nof the first n + 1? while it is true that an+1 is independent of an, this may not be\nintuitively obvious. however, if we turn the question around and ask whether an is\nindependent of an+1, then the result is more easily understood. for knowing that\nthe (n + 1)st value is larger than x1, . . . , xn clearly gives us no information about\nthe relative size of xn among the first n random variables. indeed, by symmetry, it is\nclear that each of these n random variables is equally likely to be the largest of this\nset, so p(an|an+1) = p(an) = 1/n. hence, we can conclude that an and an+1 are\n.\nindependent events.\n\nit follows from the identity\n\nremark.\np{x1 \u2026 a1, . . . , xn \u2026 an}\n= p{x1 \u2026 a1}p{x2 \u2026 a2|x1 \u2026 a1}\u00b7\u00b7\u00b7 p{xn \u2026 an|x1 \u2026 a1, . . . , xn\u22121 \u2026 an\u22121}\n\nthat the independence of x1, . . . , xn can be established sequentially. that is, we can\nshow that these random variables are independent by showing that\n\nx2 is independent of x1\nx3 is independent of x1, x2\nx4 is independent of x1, x2, x3\n\n#\n#\n#\n\nxn is independent of x1, . . . , xn\u22121\n\n "}, {"Page_number": 267, "text": "252\n\nchapter 6\n\njointly distributed random variables\n\n6.3 sums of independent random variables\n\nit is often important to be able to calculate the distribution of x + y from the dis-\ntributions of x and y when x and y are independent. suppose that x and y are\nindependent, continuous random variables having probability density functions fx\nand fy. the cumulative distribution function of x + y is obtained as follows:\n\nfx+y (a) = p{x + y \u2026 a}\n\n*\n*\n\nx+y\u2026a\n\n**\n* q\n* q\n* q\n\n\u2212q\n\n\u2212q\n\n\u2212q\n\n=\n\n=\n\n=\n\n=\n\nfx (x)fy (y) dx dy\n\nfx (x)fy (y) dx dy\n\na\u2212y\n\u2212q\na\u2212y\n\u2212q\nfx (a \u2212 y)fy (y) dy\n\nfx (x) dxfy (y) dy\n\n* q\n* q\nfx+y (a) = d\nda\n* q\n\n\u2212q\n\n=\n\n=\n\n\u2212q\n\nfx (a \u2212 y)fy (y) dy\nfx (a \u2212 y)fy (y) dy\n\n\u2212q\nd\nda\nfx (a \u2212 y)fy (y) dy\n\n(3.1)\n\n(3.2)\n\nthe cumulative distribution function fx+y is called the convolution of the distribu-\ntions fx and fy (the cumulative distribution functions of x and y, respectively).\nfx+y of x + y is given by\n\nby differentiating equation (3.1), we find that the probability density function\n\n6.3.1 identically distributed uniform random variables\nit is not difficult to determine the density function of the sum of two independent\nuniform (0, 1) random variables.\n\nexample 3a sum of two independent uniform random variables\nif x and y are independent random variables, both uniformly distributed on (0, 1),\ncalculate the probability density of x + y.\n%\nsolution. from equation (3.2), since\n\nwe obtain\n\nfx (a) = fy (a) =\n*\n\nfx+y (a) =\n\n1 0 < a < 1\n0 otherwise\n\n1\n\n0\n\nfx (a \u2212 y) dy\n*\n\na\n\ndy = a\n\n0\n\nfor 0 \u2026 a \u2026 1, this yields\n\nfx+y (a) =\n\n "}, {"Page_number": 268, "text": "section 6.3\n\nsums of independent random variables 253\n\nf(x)\n\n1\n\n0\n\n1\n\nx\n\n2\n\nfigure 6.3: triangular density function.\n\nfor 1 < a < 2, we get\n\nhence,\n\n*\n\n1\na\u22121\n\nfx+y (a) =\n\u23a7\u23a8\n\u23a9 a\n2 \u2212 a\n0\n\ndy = 2 \u2212 a\n\n0 \u2026 a \u2026 1\n1 < a < 2\notherwise\n\nfx+y (a) =\n\nbecause of the shape of its density function (see figure 6.3), the random variable\nx + y is said to have a triangular distribution.\n.\nnow, suppose that x1, x2, . . . , xn are independent uniform (0, 1) random variables,\nand let\n\nfn(x) = p{x1 + . . . + xn \u2026 x}\n\nwhereas a general formula for fn(x) is messy, it has a particularly nice form when\nx \u2026 1. indeed, we now use mathematical induction to prove that\n\nfn(x) = xn/n! ,\n\n0 \u2026 x \u2026 1\n\nbecause the proceeding equation is true for n = 1, assume that\n0 \u2026 x \u2026 1\n\nfn\u22121(x) = xn\u22121/(n \u2212 1)! ,\n\nnow, writing\n\nn(cid:6)\n\ni=1\n\nxi = n\u22121(cid:6)\n\ni=1\n\nxi + xn\n\nand using the fact that the xi are all nonnegative, we see from equation 3.1 that, for\n0 \u2026 x \u2026 1,\n\n*\n\n*\n\n1\n\n0\n\nfn(x) =\nfn\u22121(x \u2212 y)fxn\n=\n1\n= xn/n!\n\n(n \u2212 1)!\n\n0\n\nx\n\n(y)dy\n\nwhich completes the proof.\n\n(x \u2212 y)n\u22121 dy by the induction hypothesis\n\n "}, {"Page_number": 269, "text": "254\n\nchapter 6\n\njointly distributed random variables\n\nfor an interesting application of the preceding formula, let us use it to determine\nthe expected number of independent uniform (0, 1) random variables that need to\nbe summed to exceed 1. that is, with x1, x2, . . . being independent uniform (0, 1)\nrandom variables, we want to determine e[n], where\n\nnoting that n is greater than n > 0 if and only if x1 + . . . + xn \u2026 1, we see that\n\nn = min{n : x1 + . . . + xn > 1}\n\np{n > n} = fn(1) = 1/n! ,\n\nn > 0\n\np{n > 0} = 1 = 1/0!\n\nbecause\n\nwe see that, for n > 0,\n\np{n = n} = p{n > n \u2212 1} \u2212 p{n > n} =\n\n1\n\n(n \u2212 1)!\n\n\u2212 1\nn!\n\n= n \u2212 1\n\nn!\n\ntherefore,\n\ne[n] =\n\nq(cid:6)\nq(cid:6)\n=\nn=2\n= e\n\nn=1\n\nn(n \u2212 1)\n\nn!\n\n1\n\n(n \u2212 2)!\n\nthat is, the mean number of independent uniform (0, 1) random variables that must\nbe summed for the sum to exceed 1 is equal to e.\n\n6.3.2 gamma random variables\nrecall that a gamma random variable has a density of the form\n\nf (y) = \u03bbe\n\n\u2212\u03bby(\u03bby)t\u22121\n\u0001(t)\n\n0 < y < q\n\nan important property of this family of distributions is that, for a fixed value of \u03bb, it\nis closed under convolutions.\nproposition 3.1. if x and y are independent gamma random variables with respec-\ntive parameters (s, \u03bb) and (t, \u03bb), then x + y is a gamma random variable with param-\neters (s + t, \u03bb).\n\nproof. using equation (3.2), we obtain\n\nfx+y (a) =\n\na\n\n\u2212\u03bb(a\u2212y)[\u03bb(a \u2212 y)]s\u22121\u03bbe\n\u03bbe\n\n\u2212\u03bby(\u03bby)t\u22121 dy\n\n*\n\n0\n\na\n\n\u0001(s)\u0001(t)\n\n*\n\n1\n\n\u2212\u03bba\n\n= ke\n= ke\n= ce\n\n\u2212\u03bbaas+t\u22121\n\u2212\u03bbaas+t\u22121\n\n*\n(a \u2212 y)s\u22121yt\u22121dy\n\n0\n\n1\n\n(1 \u2212 x)s\u22121xt\u22121 dx by letting x = y\na\n\n0\n\n "}, {"Page_number": 270, "text": "section 6.3\n\nsums of independent random variables 255\n\nwhere c is a constant that does not depend on a. but, as the preceding is a density\nfunction and thus must integrate to 1, the value of c is determined, and we have\n\nfx+y (a) = \u03bbe\n\n\u2212\u03bba(\u03bba)s+t\u22121\n\u0001(s + t)\n\nhence, the result is proved.\n\nit is now a simple matter to establish, by using proposition 3.1 and induction, that if\nxi, i = 1, . . . , n are independent gamma random variables with respective parameters\n(ti, \u03bb), i = 1, . . . , n, then\n. we leave the\nproof of this statement as an exercise.\n\nxi is gamma with parameters\n\nn(cid:9)\n\nn(cid:9)\n\nti, \u03bb\n\ni=1\n\ni=1\n\n(cid:4)\n\n(cid:5)\n\nexample 3b\nlet x1, x2, . . . , xn be n independent exponential random variables, each having param-\neter \u03bb. then, since an exponential random variable with parameter \u03bb is the same as\na gamma random variable with parameters (1, \u03bb), it follows from proposition 3.1 that\nx1 + x2 + \u00b7\u00b7\u00b7 + xn is a gamma random variable with parameters (n, \u03bb).\n.\nn(cid:9)\nif z1, z2, . . . , zn are independent standard normal random variables, then y k\nz2\nis said to have the chi-squared (sometimes seen as \u03c7 2) distribution with n\ni\ni=1\ndegrees of freedom. let us compute the density function of y. when n = 1, y = z2\n1,\nand from example 7b of chapter 5, we see that its probability density function is\ngiven by\n\n(cid:29)\n\n(cid:30)\n\nbut we recognize the preceding as the gamma distribution with parameters\n\nis gamma\n, it follows from proposition 3.1 that the \u03c7 2 distribution with n degrees of\n\n[a by-product of this analysis is that \u0001\n2, 1\n1\n2\nfreedom is just the gamma distribution with parameters\nprobability density function given by\n\n\u03c0.] but since each z2\ni\n\nand hence has a\n\nn/2, 1\n2\n\n(cid:29)\n\n(cid:30)\n\n1\n2\n\n= \u221a\n\n(cid:29)\n\n(cid:30)\n\n.\n\n1\n2, 1\n2\n\n\u221a\ny) + fz(\u2212\u221a\nfz2 (y) = 1\n\u221a\n2\ny\n= 1\n\u2212y/2\n\u221a\ne\n2\ny\n\u2212y/2(y/2)1/2\u22121\n= 1\n2e\n\ny)]\n\n[fz(\n2\u221a\n2\u03c0\n\u221a\n\u03c0\n(cid:29)\n\n(cid:30)\n\nf\u03c7 2 (y) =\n\n1\n2\n\n\u2212y/2\ne\n\n(cid:3)n/2\u22121\n(cid:2)\n(cid:3)\n(cid:2)\ny\n2\nn\n(cid:3)\n(cid:2)\n2\n\u2212y/2yn/2\u22121\nn\n2n/2\u0001\n2\n\n\u0001\n\n= e\n\ny > 0\n\ny > 0\n\n "}, {"Page_number": 271, "text": "256\n\nchapter 6\n\n(cid:30)\n\n(cid:29)\n\n(cid:30)\n\n(cid:29)\n\njointly distributed random variables\n(cid:30)\nwhen n is an even integer, \u0001(n/2) = [(n/2) \u2212 1]!, whereas when n is odd, \u0001(n/2) can\nbe obtained from iterating the relationship \u0001(t) = (t \u2212 1)\u0001(t \u2212 1) and then using\n=\nthe previously obtained result that \u0001\n3\n2\n\n\u0001\nin practice, the chi-squared distribution often arises as the distribution of the square\nof the error involved when one attempts to hit a target in n-dimensional space when\nthe coordinate errors are taken to be independent standard normal random variables.\nit is also important in statistical analysis.\n\n\u03c0. [for instance, \u0001\n\n= \u221a\n\n= 3\n2\n\n= 3\n4\n\n(cid:29)\n\n(cid:30)\n\n(cid:29)\n\n\u03c0.]\n\n\u221a\n\n1\n2\n\n1\n2\n\n5\n2\n\n3\n2\n\n1\n2\n\n\u0001\n\n6.3.3 normal random variables\nwe can also use equation (3.2) to prove the following important result about normal\nrandom variables.\nproposition 3.2. if xi, i = 1, . . . , n, are independent random variables that are nor-\nmally distributed with respective parameters \u03bci, \u03c3 2\nxi is normally\n\ni , i = 1, . . . , n, then\n\nn(cid:9)\n\ni=1\n\ndistributed with parameters\n\n\u03bci and\n\nn(cid:9)\n\ni=1\n\nn(cid:9)\n\ni=1\n\n\u03c3 2\ni .\n\nproof of proposition 3.2: to begin, let x and y be independent normal random\nvariables with x having mean 0 and variance \u03c3 2 and y having mean 0 and vari-\nance 1. we will determine the density function of x + y by utilizing equation (3.2).\nnow, with\n\nwe have\n\nfx (a \u2212 y)fy (y) = 1\u221a\n2\u03c0 \u03c3\n\nhence, from equation (3.2),\nfx+y (a) = 1\n* q\n2\u03c0 \u03c3\n\nexp\n\nc = 1\n2\u03c3 2\n\n+ 1\n2\n0\n\n= 1 + \u03c3 2\n2\u03c3 2\n7\n\u2212 (a \u2212 y)2\n0\n2\u03c3 2\n\n7\n\nexp\n\n0\n\n= 1\n2\u03c0 \u03c3\n\nexp\n\n\u2212 a2\n2\u03c3 2\n\n\u2212c\n\nexp\n\n0\n\n0\n\n\u2212 a2\n0\n2\u03c3 2\n\u2212c\n\nexp\n\n7\n(cid:2)\ny \u2212\n7\n2(1 + \u03c3 2)\na2\n\na2\n\n\u2212\n\n2(1 + \u03c3 2)\n\n*\n\n\u2212q\n\nexp\n\n0\n\nexp\n\n0\n\n= 1\n2\u03c0 \u03c3\n= c exp\n\n\u2212\n\n0\n\n7\n\n(cid:3)7\n\nexp\n\n\u2212y2\n2\n\n1\u221a\n(cid:2)\n2\u03c0\ny2 \u2212 2y\n1 + \u03c3 2\n7\n\na\n\na2\n\n(cid:3)2\n\n7\n2\u03c3 2(1 + \u03c3 2)\na\n7* q\n1 + \u03c3 2\n\ndy\n\nexp{\u2212cx2} dx\n\n\u2212q\n\nwhere c does not depend on a. but this implies that x + y is normal with mean 0\nand variance 1 + \u03c3 2.\n\n "}, {"Page_number": 272, "text": "section 6.3\n\nsums of independent random variables 257\n\nnow, suppose that x1 and x2 are independent normal random variables with xi\n\nhaving mean \u03bci and variance \u03c3 2\n\ni , i = 1, 2. then\n(cid:2)\nx1 \u2212 \u03bc1\n\n\u03c32\n\n+ x2 \u2212 \u03bc2\n\n\u03c32\n\n(cid:3)\n\n+ \u03bc1 + \u03bc2\n\nx1 + x2 = \u03c32\n\n2 , and (x2 \u2212 \u03bc2)/\u03c32\nbut since (x1 \u2212 \u03bc1)/\u03c32 is normal with mean 0 and variance \u03c3 2\nis normal with mean 0 and variance 1, it follows from our previous result that (x1 \u2212\n\u03bc1)/\u03c32 + (x2 \u2212 \u03bc2)/\u03c32 is normal with mean 0 and variance 1 + \u03c3 2\n/\u03c3 2\n2 , implying\nthat x1 + x2 is normal with mean \u03bc1 + \u03bc2 and variance \u03c3 2\n+ \u03c3 2\n) = \u03c3 2\n1\n/\u03c3 2\n2 .\nthus, proposition 3.2 is established when n = 2. the general case now follows by\n2\n1\ninduction. that is, assume that proposition 3.2 is true when there are n \u2212 1 random\nvariables. now consider the case of n, and write\n\n(1 + \u03c3 2\n\n/\u03c3 2\n\n1\n\n2\n\n1\n\nn(cid:6)\n\ni=1\n\nxi = n\u22121(cid:6)\n\ni=1\n\nxi + xn\n\nn\u22121(cid:9)\nby the induction hypothesis,\ntherefore, by the result for n = 2,\nn(cid:9)\n\ni=1\n\nn(cid:9)\n\ni=1\n\n\u03c3 2\ni .\n\ni=1\n\nxi is normal with mean\n\n\u03bci and variance\n\nxi is normal with mean\n\n\u03bci and variance\n\nn\u22121(cid:9)\n\ni=1\n\nn(cid:9)\n\ni=1\n\nn\u22121(cid:9)\n\ni=1\n\n\u03c3 2\ni .\n\nexample 3c\na basketball team will play a 44-game season. twenty-six of these games are against\nclass a teams and 18 are against class b teams. suppose that the team will win each\ngame against a class a team with probability .4 and will win each game against a class\nb team with probability .7. suppose also that the results of the different games are\nindependent. approximate the probability that\n\n(a) the team wins 25 games or more;\n(b) the team wins more games against class a teams than it does against class b\n\nteams.\n\nsolution. (a) let xa and xb respectively denote the number of games the team\nwins against class a and against class b teams. note that xa and xb are independent\nbinomial random variables and\n\ne[xa] = 26(.4) = 10.4 var(xa) = 26(.4)(.6) = 6.24\ne[xb] = 18(.7) = 12.6 var(xb) = 18(.7)(.3) = 3.78\n\nby the normal approximation to the binomial, xa and xb will have approximately\nthe same distribution as would independent normal random variables with the pre-\nceding expected values and variances. hence, by proposition 3.2, xa + xb will have\n\n "}, {"Page_number": 273, "text": "258\n\nchapter 6\n\njointly distributed random variables\n\napproximately a normal distribution with mean 23 and variance 10.02. therefore,\nletting z denote a standard normal random variable, we have\n\n0\np{xa + xb \u00fa 25} = p{xa + xb \u00fa 24.5}\nxa + xb \u2212 23\n0\n7\n\n= p\n\n7\n\n\u00fa 24.5 \u2212 23\n\n\u221a\n\n10.02\n\n\u221a\n10.02\n1.5\u221a\n10.02\n\nz \u00fa\n\nl p\nl 1 \u2212 p{z < .4739}\nl .3178\n\n(b) we note that xa \u2212 xb will have approximately a normal distribution with\n\nmean \u22122.2 and variance 10.02. hence,\n0\np{xa \u2212 xb \u00fa 1} = p{xa \u2212 xb \u00fa .5}\nxa \u2212 xb + 2.2\n0\n7\n\n= p\n\n7\n\n\u00fa .5 + 2.2\n\n\u221a\n10.02\n\n\u221a\n10.02\n2.7\u221a\n10.02\n\nz \u00fa\n\nl p\nl 1 \u2212 p{z < .8530}\nl .1968\n\ntherefore, there is approximately a 31.78 percent chance that the team will win at\nleast 25 games and approximately a 19.68 percent chance that it will win more games\n.\nagainst class a teams than against class b teams.\n\nthe random variable y is said to be a lognormal random variable with parame-\nters \u03bc and \u03c3 if log (y) is a normal random variable with mean \u03bc and variance \u03c3 2.\nthat is, y is lognormal if it can be expressed as\ny = ex\n\nwhere x is a normal random variable.\n\nexample 3d\nstarting at some fixed time, let s(n) denote the price of a certain security at the end\nof n additional weeks, n \u00fa 1. a popular model for the evolution of these prices\nassumes that the price ratios s(n)/s(n \u2212 1), n \u00fa 1, are independent and identi-\ncally distributed lognormal random variables. assuming this model, with parameters\n\u03bc = .0165, \u03c3 = .0730, what is the probability that\n(a) the price of the security increases over each of the next two weeks?\n(b) the price at the end of two weeks is higher than it is today?\n\nsolution. let z be a standard normal random variable. to solve part (a), we use the\nfact that log(x) increases in x to conclude that x > 1 if and only if log(x) > log(1) = 0.\nas a result, we have\n\n "}, {"Page_number": 274, "text": "section 6.3\n\nsums of independent random variables 259\n\n%\n\n/\n\np\n\ns(1)\ns(0)\n\n> 1\n\n0\n%\n\n(cid:2)\n\n(cid:3)\n\n7\n\n> 0\n\n/\n\nlog\n\n= p\n\ns(1)\ns(0)\n\u2212.0165\n= p\n.0730\n= p{z < .2260}\n= .5894\n\nz >\n\nin other words, the probability that the price is up after one week is .5894. since the\nsuccessive price ratios are independent, the probability that the price increases over\neach of the next two weeks is (.5894)2 = .3474.\n\n%\n\n/\n\nto solve part (b), we reason as follows:\ns(2)\ns(1)\n\ns(2)\ns(0)\n\n= p\n\n> 1\n\np\n\ns(1)\ns(0)\n\n%\n0\n\n/\n\n(cid:2)\n\nlog\n\ns(2)\ns(1)\n\n> 1\n\n(cid:3)\n\n+ log\n\n(cid:2)\n\n(cid:3)\n\ns(1)\ns(0)\n\n7\n\n> 0\n\nhowever, log\n, being the sum of two independent normal random\nvariables with a common mean .0165 and a common standard deviation .0730, is a\nnormal random variable with mean .0330 and variance 2(.0730)2. consequently,\n\n(cid:29)\n\n(cid:30)\n\ns(2)\ns(1)\n\n= p\n(cid:30)\n\n(cid:29)\n\ns(1)\ns(0)\n\n+ log\n%\n\np\n\ns(2)\ns(0)\n\n> 1\n\n7\n\n/\n\n0\n\nz >\n\n\u2212.0330\n= p\n\u221a\n.0730\n2\n= p{z < .31965}\n= .6254\n\n.\n\n6.3.4 poisson and binomial random variables\nrather than attempt to derive a general expression for the distribution of x + y in\nthe discrete case, we shall consider some examples.\n\nexample 3e sums of independent poisson random variables\nif x and y are independent poisson random variables with respective parameters \u03bb1\nand \u03bb2, compute the distribution of x + y.\nsolution. because the event {x + y = n} may be written as the union of the disjoint\nevents {x = k, y = n \u2212 k}, 0 \u2026 k \u2026 n, we have\n\np{x + y = n} = n(cid:6)\n= n(cid:6)\n= n(cid:6)\n\nk=0\n\nk=0\n\nk=0\n\np{x = k, y = n \u2212 k}\n\np{x = k}p{y = n \u2212 k}\n\n\u2212\u03bb1\ne\n\n\u2212\u03bb2\ne\n\n\u03bbk\n1\nk!\n\n\u03bbn\u2212k\n2\n(n \u2212 k)!\n\n "}, {"Page_number": 275, "text": "260\n\nchapter 6\n\njointly distributed random variables\n\n= e\n\n\u2212(\u03bb1+\u03bb2)\n\nn(cid:6)\nn(cid:6)\n\nk=0\n\n\u03bbn\u2212k\n\u03bbk\n2\n1\nk!(n \u2212 k)!\n\n= e\n\n= e\n\n\u2212(\u03bb1+\u03bb2)\n\n\u2212(\u03bb1+\u03bb2)\n\nn!\n\nn!\n\nn!\n\nk!(n \u2212 k)!\n\n\u03bbn\u2212k\n2\n\n\u03bbk\n1\n\nk=0\n(\u03bb1 + \u03bb2)n\n\n.\n\nthus, x1 + x2 has a poisson distribution with parameter \u03bb1 + \u03bb2.\nexample 3f sums of independent binomial random variables\nlet x and y be independent binomial random variables with respective parameters\n(n, p) and (m, p). calculate the distribution of x + y.\nsolution. recalling the interpretation of a binomial random variable, and without\nany computation at all, we can immediately conclude that x + y is binomial with\nparameters (n + m, p). this follows because x represents the number of successes in\nn independent trials, each of which results in a success with probability p; similarly,\ny represents the number of successes in m independent trials, each of which results\nin a success with probability p. hence, given that x and y are assumed independent,\nit follows that x + y represents the number of successes in n + m independent\ntrials when each trial has a probability p of resulting in a success. therefore, x + y\nis a binomial random variable with parameters (n + m, p). to check this conclusion\nanalytically, note that\n\ni=0\n\np{x + y = k} = n(cid:6)\n= n(cid:6)\n= n(cid:6)\n(cid:3)\n(cid:2)\n\ni=0\n\ni=0\n\np{x = i, y = k \u2212 i}\n\np{x = i}p{y = k \u2212 i}\n(cid:3)\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\npiqn\u2212i\n\nn\ni\n\nm\nk \u2212 i\n\npk\u2212iqm\u2212k+i\n\nr\nj\n\n= 0 when j < 0. thus,\n(cid:3)(cid:2)\n\n(cid:2)\n\nn(cid:6)\n\n(cid:3)\n\np{x + y = k} = pkqn+m\u2212k\n(cid:2)\n\n(cid:2)\n\n(cid:3)\n\ni=0\n\n(cid:3)(cid:2)\n\nn\ni\n\nm\nk \u2212 i\n(cid:3)\n\nn + m\n\nk\n\n= n(cid:6)\n\ni=0\n\nn\ni\n\nm\nk \u2212 i\n\nwhere q = 1 \u2212 p and where\n\nand the conclusion follows upon application of the combinatorial identity\n\n.\n\n6.3.5 geometric random variables\nlet x1, . . . , xn be independent geometric random variables, with xi having parame-\nter pi for i = 1, . . . , n. we are interested in computing the probability mass function\n\n "}, {"Page_number": 276, "text": "section 6.3\n\nsums of independent random variables 261\n\nof their sum sn = (cid:9)\nvariables with respective parameters p1, p2, . . . , pn, and sn =(cid:9)\n\nn\ni=1 xi. for an application, consider n coins, with coin i having\nprobability pi of coming up heads when flipped, i = 1, . . . , n. suppose that coin 1 is\nflipped until heads appears, at which point coin 2 is flipped until it shows heads, and\nthen coin 3 is flipped until it shows heads, and so on. if we let xi denote the number\nof flips made with coin i, then x1, x2, . . . , xn will be independent geometric random\nn\ni=1 xi will represent\nthe total number of flips. if all the pi are equal\u2014say, all pi = p\u2014then sn has the same\ndistribution as the number of flips of a coin having probability p of coming up heads\nthat are needed to obtain a total of n heads, and so sn is a negative binomial random\n(cid:2)\nvariable with probability mass function\nk \u2212 1\nn \u2212 1\n\n(cid:3)\npn(1 \u2212 p)k\u2212n,\n\np{sn = k} =\n\nk \u00fa n\n\nas a prelude to determining the probability mass function of sn when the pi are all\ndistinct, let us first consider the case n = 2. letting qj = 1 \u2212 pj, j = 1, 2, we obtain\n\np{x1 = j, x2 = k \u2212 j}\n\np{x1 = j} p{x2 = k \u2212 j}\n\n(by independence)\n\nj=1\n\np(s2 = k) = k\u22121(cid:6)\n= k\u22121(cid:6)\n= k\u22121(cid:6)\n= p1p2qk\u22122\n\nj=1\n\nj=1\n\n2\n\n1 p2qk\u2212j\u22121\np1qj\u22121\nk\u22121(cid:6)\n(q1/q2)j\u22121\n\n2\n\n2\n\n= p1p2qk\u22122\n= p1p2qk\u22121\nq2 \u2212 q1\n= p2qk\u22121\n\n2\n\n2\n\nj=1\n1 \u2212 (q1/q2)k\u22121\n1 \u2212 q1/q2\n\u2212 p1p2qk\u22121\nq2 \u2212 q1\np1\n\n+ p1qk\u22121\n\n1\n\n1\n\np1 \u2212 p2\n\np2\n\np2 \u2212 p1\n\nif we now let n = 3 and compute p{s3 = k} by starting with the identity\n\np{s3 = k} = k\u22121(cid:6)\n\np{s2 = j, x3 = k \u2212 j} = k\u22121(cid:6)\n\np{s2 = j}p{x3 = k \u2212 j}\n\nj=1\n\nj=1\n\nand then substituting the derived formula for the mass function of s2, we would\nobtain, after some computations,\n\np{s3 = k} = p1qk\u22121\n\n1\n\np2\n\np3\n\np2 \u2212 p1\np1\n\np3 \u2212 p1\np2\n\np1 \u2212 p3\n\np2 \u2212 p3\n\n+ p3qk\u22121\n\n3\n\n+ p2qk\u22121\n\n2\n\np1\n\np1 \u2212 p2\n\np3\n\np3 \u2212 p2\n\n "}, {"Page_number": 277, "text": "262\n\nchapter 6\n\njointly distributed random variables\n\nthe mass functions of s2 and s3 lead to the following conjecture for the mass function\nof sn.\nproposition 3.3. let x1, . . . , xn be independent geometric random variables, with xi\nhaving parameter pi for i = 1, . . . , n. if all the pi are distinct, then, for k \u00fa n,\n\np{sn = k} = n(cid:6)\n\n(cid:31)\n\njzi\n\npiqk\u22121\n\ni\n\ni=1\n\npj\n\npj \u2212 pi\n\nproof of proposition 3.3: we will prove this proposition by induction on the value\nof n + k. because the proposition is true when n = 2, k = 2, take as the induction\nhypothesis that it is true for any k \u00fa n for which n + k \u2026 r. now, suppose k \u00fa n are\nsuch that n + k = r + 1. to compute p{sn = k}, we condition on whether xn = 1.\nthis gives\n\np{sn = k} = p{sn = k|xn = 1}p{xn = 1} + p{sn = k|xn > 1}p{xn > 1}\n\n= p{sn = k|xn = 1}pn + p{sn = k|xn > 1}qn\n\nnow,\n\np{sn = k|xn = 1} = p{sn\u22121 = k \u2212 1|xn = 1}\n\n= p{sn\u22121 = k \u2212 1}\n(cid:31)\n= n\u22121(cid:6)\n\npiqk\u22122\n\ni\n\nizj\u2026n\u22121\n\ni=1\n\n(by independence)\npj\n\npj \u2212 pi\n\n(by the induction hypothesis)\n\nnow, if x is geometric with parameter p, then the conditional distribution of x given\nthat it is larger than 1 is the same as the distribution of 1 (the first failed trial) plus\na geometric with parameter p (the number of additional trials after the first until a\nsuccess occurs). consequently,\n\np{sn = k|xn > 1} = p{x1 + . . . + xn\u22121 + xn + 1 = k}\n\n(cid:31)\n= p{sn = k \u2212 1}\n\n= n(cid:6)\n\npiqk\u22122\n\ni\n\nizj\u2026n\n\ni=1\n\npj\n\npj \u2212 pi\n\nwhere the final equality follows from the induction hypothesis. thus, from the pre-\nceding, we obtain\n\np{sn = k} = pn\n\npj\n\npj \u2212 pi\n\n+ qn\n\ni\n\ni=1\n\npiqk\u22122\n\nn\u22121(cid:6)\nn\u22121(cid:6)\n\n(cid:31)\n(cid:31)\npiqk\u22122\n(cid:31)\n+ qnpnqk\u22122\n\ni=1\n\ni\n\nn\n\nizj\u2026n\u22121\n\npj\n\npj \u2212 pi\nizj\u2026n\u22121\npj\n\npj \u2212 pn\n\nj<n\n\npiqk\u22122\n\ni\n\npn(1 +\n\nqn\n\npn \u2212 pi\n\n)\n\n= pn\n\n= n\u22121(cid:6)\n\ni=1\n\n+ qn\n\n(cid:31)\n\nizj\u2026n\u22121\n\nn(cid:6)\nn\u22121(cid:6)\n\ni=1\n\ni=1\n\npiqk\u22122\n\ni\n\npiqk\u22122\n\ni\n\n(cid:31)\n(cid:31)\n\nizj\u2026n\n\nizj\u2026n\n\npj\n\npj \u2212 pi\n\npj\n\npj \u2212 pi\n\npj\n\npj \u2212 pi\n\n+ pnqk\u22121\n\nn\n\n(cid:31)\n\nj<n\n\npj\n\npj \u2212 pn\n\n "}, {"Page_number": 278, "text": "section 6.4\n\nconditional distributions: discrete case 263\n\nnow, using that\n\n1 +\n\nqn\n\npn \u2212 pi\n\n= pn \u2212 pi + qn\n\npn \u2212 pi\n\n=\n\nthe preceding gives\n\np{sn = k} = n\u22121(cid:6)\n= n(cid:6)\n\ni=1\n\ni=1\n\n(cid:31)\n(cid:31)\n\nizj\u2026n\n\njzi\n\npj\n\npj \u2212 pi\npj\n\npj \u2212 pi\n\npiqk\u22121\n\ni\n\npiqk\u22121\n\ni\n\nqi\n\npn \u2212 pi\n(cid:31)\n\n+ pnqk\u22121\n\nn\n\npj\n\npj \u2212 pn\n\nj<n\n\nand the proof by induction is complete.\n\n.\n\n6.4 conditional distributions: discrete case\n\nrecall that, for any two events e and f, the conditional probability of e given f is\ndefined, provided that p(f) > 0, by\n\np(e|f) = p(ef)\np(f)\n\nhence, if x and y are discrete random variables, it is natural to define the conditional\nprobability mass function of x given that y = y, by\n\npx|y (x|y) = p{x = x|y = y}\n= p{x = x, y = y}\n= p(x, y)\npy (y)\n\np{y = y}\n\nfor all values of y such that py (y) > 0. similarly, the conditional probability distribu-\ntion function of x given that y = y is defined, for all y such that py (y) > 0, by\n\nfx|y (x|y) = p{x \u2026 x|y = y}\n\n=\n\npx|y (a|y)\n\n(cid:6)\n\na\u2026x\n\nin other words, the definitions are exactly the same as in the unconditional case,\nexcept that everything is now conditional on the event that y = y. if x is indepen-\ndent of y, then the conditional mass function and the distribution function are the\nsame as the respective unconditional ones. this follows because if x is independent\nof y, then\n\npx|y (x|y) = p{x = x|y = y}\n= p{x = x, y = y}\n= p{x = x}p{y = y}\n= p{x = x}\n\np{y = y}\np{y = y}\n\n "}, {"Page_number": 279, "text": "264\n\nchapter 6\n\njointly distributed random variables\n\nexample 4a\nsuppose that p(x, y), the joint probability mass function of x and y, is given by\n\np(0, 0) = .4 p(0, 1) = .2 p(1, 0) = .1 p(1, 1) = .3\n\ncalculate the conditional probability mass function of x given that y = 1.\nsolution. we first note that\npy (1) =\n\np(x, 1) = p(0, 1) + p(1, 1) = .5\n\n(cid:6)\n\nhence,\n\nand\n\nx\n\npx|y (0|1) = p(0, 1)\npy (1)\n\n= 2\n5\n\npx|y (1|1) = p(1, 1)\npy (1)\n\n= 3\n5\n\n.\n\nexample 4b\nif x and y are independent poisson random variables with respective parameters \u03bb1\nand \u03bb2, calculate the conditional distribution of x given that x + y = n.\nsolution. we calculate the conditional probability mass function of x given that x +\ny = n as follows:\n\np{x = k|x + y = n} = p{x = k, x + y = n}\n= p{x = k, y = n \u2212 k}\n= p{x = k}p{y = n \u2212 k}\n\np{x + y = n}\np{x + y = n}\np{x + y = n}\n\n(cid:7)\n\nwhere the last equality follows from the assumed independence of x and y. recalling\n(example 3e) that x + y has a poisson distribution with parameter \u03bb1 + \u03bb2, we see\nthat the preceding equals\n\n\u2212(\u03bb1+\u03bb2)(\u03bb1 + \u03bb2)n\ne\n\n2\n\n\u2212\u03bb1 \u03bbk\n1\nk!\n\n\u2212\u03bb2 \u03bbn\u2212k\np{x = k|x + y = n} = e\ne\n(n \u2212 k)!\n\u03bbn\u2212k\n\u03bbk\n(cid:3)(cid:2)\n(cid:2)\n(cid:3)k(cid:2)\n2\n1\n(\u03bb1 + \u03bb2)n\n(n \u2212 k)! k!\n\u03bb1\nn\nk\n\n=\n\n=\n\nn!\n\nn!\n\n(cid:3)n\u2212k\n\n\u03bb2\n\n\u03bb1 + \u03bb2\nin other words, the conditional distribution of x given that x + y = n is the binomial\ndistribution with parameters n and \u03bb1/(\u03bb1 + \u03bb2).\n.\n\n\u03bb1 + \u03bb2\n\n(cid:8)\u22121\n\nwe can also talk about joint conditional distributions, as is indicated in the next\n\ntwo examples.\n\n "}, {"Page_number": 280, "text": "section 6.4\n\nconditional distributions: discrete case 265\n\nexample 4c\nconsider the multinomial distribution with joint probability mass function\n\np{xi = ni, i = 1, . . . , k} =\n\nn!\n\nn1!\u00b7\u00b7\u00b7 nk!\n\npn1\n1\n\nni \u00fa 0,\n\nni = n\n\nk(cid:6)\n\ni=1\n\n\u00b7\u00b7\u00b7 pnk\nk ,\n(cid:9)\n\nsuch a mass function results when n independent trials are performed, with each\ni=1 pi = 1. the random variables\ntrial resulting in outcome i with probability pi,\nxi, i = 1, . . . , k, represent, respectively, the number of trials that result in outcome i,\ni = 1, . . . , k. suppose we are given that nj of the trials resulted in outcome j, for j =\nr + 1, . . . , k, where\nj=r+1 nj = m \u2026 n. then, because each of the other n \u2212 m trials\nmust have resulted in one of the trials 1, . . . , r, it would seem that the conditional dis-\ntribution of x1, . . . , xr is the multinomial distribution on n \u2212 m trials with respective\ntrial outcome probabilities\n\n(cid:9)\n\nk\n\nk\n\nwhere fr = (cid:9)\n\np{outcome i|outcome is not any of r + 1, . . . , k} = pi\nfr\n(cid:9)\n\nr\ni=1 pi is the probability that a trial results in one of the outcomes\ni=1 ni = n \u2212 m. then\n\n, i = 1, . . . , r\n\nr\n\nsolution. to verify this intuition, let n1, . . . , nr, be such that\np{x1 = n1, . . . , xr = nr|xr+1 = nr+1, . . . xk = nk}\n\n1, . . . , r.\n\n= p{x1 = n1, . . . , xk = nk}\np{xr+1 = nr+1, . . . xk = nk}\n\u00b7\u00b7\u00b7 pnk\n\u00b7\u00b7\u00b7 pnr\nr pnr+1\nn1!\u00b7\u00b7\u00b7nk!pn1\nr+1\n(n\u2212m)!nr+1!\u00b7\u00b7\u00b7nk!fn\u2212m\n\u00b7\u00b7\u00b7 pnk\npnr+1\nr+1\n\n=\n\nn!\n\nn!\n\nk\n\nk\n\n1\n\nr\n\nwhere the probability in the denominator was obtained by regarding outcomes 1, . . . , r\n(cid:9)\nas a single outcome having probability fr, thus showing that the probability is a multi-\nnomial probability on n trials with outcome probabilities fr, pr+1, . . . , pk. because\ni=1 ni = n \u2212 m, the preceding can be written as\np{x1 = n1, . . . , xr = nr|xr+1 = nr+1, . . . xk = nk}\n\nr\n\n= (n \u2212 m)!\nn1!\u00b7\u00b7\u00b7 nr!\n\n(\n\np1\nfr\n\n)n1 \u00b7\u00b7\u00b7 (\n\n)nr\n\npr\nfr\n\n.\n\nand our intuition is upheld.\n\nexample 4d\nconsider n independent trials, with each trial being a success with probability p.\ngiven a total of k successes, show that all possible orderings of the k successes and\nn \u2212 k failures are equally likely.\nsolution. we want to show that, given a total of k successes, each of the\npossible\norderings of k successes and n \u2212 k failures is equally likely. let x denote the number\nof successes, and consider any ordering of k successes and n \u2212 k failures, say, o =\n(s, s, f , f , . . . , f ). then\n\n(cid:18)\n\n(cid:19)\n\nn\nk\n\n "}, {"Page_number": 281, "text": "266\n\nchapter 6\n\njointly distributed random variables\n\np(o|x = k) = p(o, x = k)\np(x = k)\n= p(o)\np(x = k)\n(cid:19)\n(cid:18)\n= pk(1 \u2212 p)n\u2212k\npk(1 \u2212 p)n\u2212k\n= 1(cid:18)\n(cid:19)\n\nn\nk\n\nn\nk\n\n.\n\n6.5 conditional distributions: continuous case\n\nif x and y have a joint probability density function f (x, y), then the conditional prob-\nability density function of x given that y = y is defined, for all values of y such that\nfy (y) > 0, by\n\nfx|y (x|y) = f (x, y)\nfy (y)\n\nto motivate this definition, multiply the left-hand side by dx and the right-hand side\nby (dx dy)/dy to obtain\n\nfx|y (x|y) dx = f (x, y) dx dy\nfy (y) dy\n\nl p{x \u2026 x \u2026 x + dx, y \u2026 y \u2026 y + dy}\n= p{x \u2026 x \u2026 x + dx|y \u2026 y \u2026 y + dy}\n\np{y \u2026 y \u2026 y + dy}\n\nin other words, for small values of dx and dy, fx|y (x|y)dx represents the conditional\nprobability that x is between x and x + dx given that y is between y and y + dy.\nthe use of conditional densities allows us to define conditional probabilities of\nevents associated with one random variable when we are given the value of a second\nrandom variable. that is, if x and y are jointly continuous, then, for any set a,\n\np{x \u2208 a|y = y} =\n\n*\n\na\n\nfx|y (x|y) dx\n*\n\nin particular, by letting a = (\u2212q, a], we can define the conditional cumulative distri-\nbution function of x given that y = y by\n\nfx|y (a|y) k p{x \u2026 a|y = y} =\n\nfx|y (x|y) dx\n\na\n\u2212q\n\nthe reader should note that, by using the ideas presented in the preceding discussion,\nwe have been able to give workable expressions for conditional probabilities, even\nthough the event on which we are conditioning (namely, the event {y = y}) has\nprobability 0.\n\nexample 5a\nthe joint density of x and y is given by\n5 x(2 \u2212 x \u2212 y)\n0\n\nf (x, y) =\n\n0\n\n12\n\n0 < x < 1, 0 < y < 1\notherwise\n\ncompute the conditional density of x given that y = y, where 0 < y < 1.\n\n "}, {"Page_number": 282, "text": "section 6.5\n\nconditional distributions: continuous case 267\n\nsolution. for 0 < x < 1, 0 < y < 1, we have\n\n=\n\nfx|y (x|y) = f (x, y)\n- q\nfy (y)\nf (x, y)\n\u2212q f (x, y) dx\n-\nx(2 \u2212 x \u2212 y)\n=\n0 x(2 \u2212 x \u2212 y) dx\n= x(2 \u2212 x \u2212 y)\n\u2212 y/2\n= 6x(2 \u2212 x \u2212 y)\n4 \u2212 3y\n\n2\n3\n\n1\n\nexample 5b\nsuppose that the joint density of x and y is given by\n\n\u23a7\u23aa\u23a8\n\u2212x/ye\n\u23aa\u23a9 e\ny\n\n0\n\n\u2212y\n\nf (x, y) =\n\n0 < x < q, 0 < y < q\n\notherwise\n\nfind p{x > 1|y = y}.\nsolution. we first obtain the conditional density of x given that y = y.\n\nfx|y (x|y) = f (x, y)\n- q\nfy (y)\n\u2212x/ye\ne\n(1/y)e\u2212x/y dx\n\n\u2212y/y\n\n=\n\ne\u2212y\n= 1\ny\n\n0\n\u2212x/y\ne\n\n* q\n\n\u2212x/y dx\n1\ne\ny\n\u2212x/y\n\n666q\n\n1\n\n1\n\n= \u2212e\n= e\n\u22121/y\n\nhence,\n\np{x > 1|y = y} =\n\n.\n\n.\n\nif x and y are independent continuous random variables, the conditional density\nof x given that y = y is just the unconditional density of x. this is so because, in the\nindependent case,\n\nfx|y (x|y) = f (x, y)\nfy (y)\n\n= fx (x)fy (y)\n\nfy (y)\n\n= fx (x)\n\nwe can also talk about conditional distributions when the random variables are nei-\nther jointly continuous nor jointly discrete. for example, suppose that x is a continu-\nous random variable having probability density function f and n is a discrete random\nvariable, and consider the conditional distribution of x given that n = n. then\n\n "}, {"Page_number": 283, "text": "268\n\nchapter 6\n\njointly distributed random variables\n\np{x < x < x + dx|n = n}\n= p{n = n|x < x < x + dx}\n\ndx\np{n = n}\n\np{x < x < x + dx}\n\ndx\n\nand letting dx approach 0 gives\n\np{x < x < x + dx|n = n}\n\ndx\u21920\nlim\n\ndx\n\n= p{n = n|x = x}\n\np{n = n}\n\nf (x)\n\nthus showing that the conditional density of x given that n = n is given by\n\nfx|n(x|n) = p{n = n|x = x}\n\nf (x)\n\n.\n\np{n = n}\nexample 5c the bivariate normal distribution\none of the most important joint distributions is the bivariate normal distribution.\nwe say that the random variables x, y have a bivariate normal distribution if, for\nconstants \u03bcx, \u03bcy, \u03c3x > 0, \u03c3y > 0, \u22121 < \u03c1 < 1, their joint density function is given,\nfor all \u2212q < x, y < q, by\nf (x, y) =\n\n\u23ab\u23aa\u23ac\n\u23a4\n\u23a6\n\u23aa\u23ad\n- q\nwe now determine the conditional density of x given that y = y. in doing so, we\nwill continually collect all factors that do not depend on x and represent them by the\n\u2212q fx|y (x|y) dx = 1.\nconstants ci. the final constant will then be found by using that\nwe have\n\n(cid:7)(cid:2)\n(cid:3)2\n(cid:5)2 \u2212 2\u03c1\n\n(x \u2212 \u03bcx)(y \u2212 \u03bcy)\n\n2(1 \u2212 \u03c12)\n(cid:4)\n\n\u23a7\u23a8\n\u23a9\u2212\n\nx \u2212 \u03bcx\n\ny \u2212 \u03bcy\n\n1 \u2212 \u03c12\n\n2\u03c0 \u03c3x\u03c3y\n\n\u03c3x\u03c3y\n\nexp\n\n+\n\n\u03c3x\n\n\u03c3y\n\n1\n\n1\n\nfx|y (x|y) = f (x, y)\n\u23a7\u23a8\nfy (y)\n= c1f (x, y)\n\u23a9\u2212\n= c2 exp\n\u23a7\u23aa\u23a8\n\u23aa\u23a9\u2212\n\u23a7\u23aa\u23a8\n\u23aa\u23a9\u2212\n\n= c3 exp\n\n= c4 exp\n\n\u03c3x\n\n(cid:7)(cid:2)\nx \u2212 \u03bcx\n\u23a1\n\u23a3x2 \u2212 2x\n\u23a1\n(cid:4)\n\u23a3x \u2212\n\n1\n\n2(1 \u2212 \u03c12)\n\n1\n(1 \u2212 \u03c12)\n\n1\n(1 \u2212 \u03c12)\n\n2\u03c3 2\nx\n\n2\u03c3 2\nx\n\n(cid:8)\u23ab\u23ac\n\u23ad\n\nx(y \u2212 \u03bcy)\n\n\u03c3x\u03c3y\n\n(cid:3)2 \u2212 2\u03c1\n(cid:4)\n\n\u03bcx + \u03c1\n\n\u03c3x\n\u03c3y\n\n(y \u2212 \u03bcy)\n(cid:5)\u23a4\n\u23a62\n\n\u03bcx + \u03c1\n\n(y \u2212 \u03bcy)\n\n\u03c3x\n\u03c3y\n\n\u23ab\u23aa\u23ac\n\u23aa\u23ad\n\n(cid:5)\u23a4\n\u23a6\n\u23ab\u23aa\u23ac\n\u23aa\u23ad\n\nrecognizing the preceding equation as a normal density, we can conclude that, given\ny = y, the random variable x is normally distributed with mean \u03bcx + \u03c1 \u03c3x\n(y \u2212 \u03bcy)\n(1 \u2212 \u03c12). also, because the joint density of y, x is exactly the same as\nand variance \u03c3 2\nx\nthat of x, y, except that \u03bcx, \u03c3x are interchanged with \u03bcy, \u03c3y, it similarly follows that\n\n\u03c3y\n\n "}, {"Page_number": 284, "text": "\u03c3y\n\u03c3x\n\nsection 6.5\n\n(x \u2212 \u03bcx) and variance \u03c3 2\n\nconditional distributions: continuous case 269\nthe conditional distribution of y given x = x is the normal distribution with mean\n\u03bcy + \u03c1\n(1 \u2212 \u03c12). it follows from these results that the\nnecessary and sufficient condition for the bivariate normal random variables x and\ny to be independent is that \u03c1 = 0 (a result that also follows directly from their joint\ndensity, because it is only when \u03c1 = 0 that the joint density factors into two terms,\none depending only on x and the other only on y).\n\ny\n\nwith c =\n\n\u221a\n1\n2\u03c0 \u03c3x\u03c3y\n\nfx (x) =\n\n* q\n1\u2212\u03c12 , the marginal density of x can be obtained from\n* q\n\nf (x, y) dy\n\n(cid:5)2\n\n(cid:4)\n\n\u2212q\n\ny \u2212 \u03bcy\n\n\u23a1\n\u23a3(cid:2)\n\n= c\n\nexp\n\n\u2212q\n\n\u23a7\u23aa\u23a8\n\u23aa\u23a9\u2212\n\u22122\u03c1\n\n0\n\n1\n\n2(1 \u2212 \u03c12)\n\u03c3x\n(x \u2212 \u03bcx)(y \u2212 \u03bcy)\n\n\u03c3x\u03c3y\n\n(cid:3)2 +\nx \u2212 \u03bcx\n(cid:8)\u23ab\u23ac\n\u23ad dy\n7\n\n(cid:2)\n\n(cid:3)2\n\ngives\nx \u2212 \u03bcx\n(cid:20)\n\u03c3x\nw2 \u2212 2\u03c1\n(cid:3)2\n\nx \u2212 \u03bcx\n(cid:20)\n\u03c3x\nw \u2212 \u03c1\n(cid:20)\n\nw \u2212 \u03c1\n\u03c3x\n\n1\n\n\u2212\n0\n2(1 \u2212 \u03c12)\n\u2212\n1\n\n(cid:2)\n2(1 \u2212 \u03c12)\n1\n\n\u2212\n0\n2(1 \u2212 \u03c12)\n\u2212\n1\n0\n\n2(1 \u2212 \u03c12)\n\n\u2212\n\n1\n\n2(1 \u2212 \u03c12)\n8\n\nfx (x) = c\u03c3y\n1\u221a\n2\u03c0 \u03c3x\n\n=\n\n2\u03c0(1 \u2212 \u03c12) e\n\u2212(x\u2212\u03bcx)2/2\u03c3 2\ne\n\nx\n\n\u2212(x\u2212\u03bcx)2/2\u03c3 2\n\nx\n\n\u03c3y\n\n(cid:21)7\n\nw\n\n\u03c3x\n\nx \u2212 \u03bcx\n\n7\n(1 \u2212 \u03c12)\n7\n(cid:21)2\nx \u2212 \u03bcx\n\ndw\n\ndw\n\n\u03c3x\n\n(x \u2212 \u03bcx)\n\n7\n\n(cid:21)2\n\ndw = 1\n\nmaking the change of variables w = y\u2212\u03bcy\n\n\u03c3y\n\nfx (x) = c\u03c3y exp\n* q\n\n\u2212q\n= c\u03c3y exp\n* q\n\n*\n\n*\n\nexp\n\n0\n\nexp\n\n\u2212q\n\n* q\n\nexp\n\n\u2212q\n\nbecause\n\n.\n\n1\n\n2\u03c0(1 \u2212 \u03c12)\n\nwe see that\n\nthat is, x is normal with mean \u03bcx and variance \u03c3 2\nmean \u03bcy and variance \u03c3 2\ny .\n\nx . similarly, y is normal with\n.\n\nexample 5d\nconsider n + m trials having a common probability of success. suppose, however,\nthat this success probability is not fixed in advance but is chosen from a uniform (0, 1)\n\n "}, {"Page_number": 285, "text": "270\n\nchapter 6\n\njointly distributed random variables\n\npopulation. what is the conditional distribution of the success probability given that\nthe n + m trials result in n successes?\nsolution. if we let x denote the probability that a given trial is a success, then x\nis a uniform (0, 1) random variable. also, given that x = x, the n + m trials are\nindependent with common probability of success x, so n, the number of successes,\nis a binomial random variable with parameters (n + m, x). hence, the conditional\ndensity of x given that n = n is\n(cid:2)\n\nfx|n(x|n) = p{n = n|x = x}fx (x)\nxn(1 \u2212 x)m\n\np{n = n}\n\nn + m\n\n(cid:3)\n\nn\n\n=\n= cxn(1 \u2212 x)m\n\np{n = n}\n\n0 < x < 1\n\nwhere c does not depend on x. thus, the conditional density is that of a beta random\nvariable with parameters n + 1, m + 1.\n\nthe preceding result is quite interesting, for it states that if the original or prior\n(to the collection of data) distribution of a trial success probability is uniformly dis-\ntributed over (0, 1) [or, equivalently, is beta with parameters (1, 1)] then the posterior\n(or conditional) distribution given a total of n successes in n + m trials is beta with\nparameters (1 + n, 1 + m). this is valuable, for it enhances our intuition as to what\n.\nit means to assume that a random variable has a beta distribution.\n\n\u22176.6 order statistics\n\nlet x1, x2, . . . , xn be n independent and identically distributed continuous random\nvariables having a common density f and distribution function f. define\n\nx(1) = smallest of x1, x2, . . . , xn\nx(2) = second smallest of x1, x2, . . . , xn\n\n#\n#\n#\n\nx(j) = jth smallest of x1, x2, . . . , xn\n\n#\n#\n#\n\nx(n) = largest of x1, x2, . . . , xn\n\nthe ordered values x(1) \u2026 x(2) \u2026 \u00b7\u00b7\u00b7 \u2026 x(n) are known as the order statistics cor-\nresponding to the random variables x1, x2, . . . , xn. in other words, x(1), . . . , x(n) are\nthe ordered values of x1, . . . , xn.\nthe joint density function of the order statistics is obtained by noting that the order\nstatistics x(1), . . . , x(n) will take on the values x1 \u2026 x2 \u2026 \u00b7\u00b7\u00b7 \u2026 xn if and only if, for\nsome permutation (i1, i2, . . . , in) of (1, 2, . . . , n),\n\nx1 = xi1, x2 = xi2, . . . , xn = xin\n\n "}, {"Page_number": 286, "text": "section 6.6\n\norder statistics 271\n\nsince, for any permutation (i1, . . . , in) of (1, 2, . . . , n),\n\n%\n\np\n\n, . . . , xin\n\n< x1 < xi1\n\n\u2212 \u03b5\n+ \u03b5\nxi1\n2\n2\nl \u03b5nfx1,\u00b7\u00b7\u00b7 , xn(xi1, . . . , xin\n)\u00b7\u00b7\u00b7 f (xin\n= \u03b5nf (xi1\n= \u03b5nf (x1)\u00b7\u00b7\u00b7 f (xn)\n\n)\n\n)\n\n\u2212 \u03b5\n2\n\n< xn < xin\n\n/\n\n+ \u03b5\n2\n\nit follows that, for x1 < x2 < \u00b7\u00b7\u00b7 < xn,\n< x(1) < x1 + \u03b5\n2\n\n%\nx1 \u2212 \u03b5\n2\nl n! \u03b5nf (x1)\u00b7\u00b7\u00b7 f (xn)\ndividing by \u03b5n and letting \u03b5\u21920 yields\n\np\n\n/\n\n, . . . , xn \u2212 \u03b5\n2\n\n< x(n) < xn + \u03b5\n2\n\nfx(1),...,x(n)\n\n(x1, x2, . . . , xn) = n!f (x1)\u00b7\u00b7\u00b7 f (xn) x1 < x2 < \u00b7\u00b7\u00b7 < xn\n\n(6.1)\n\nequation (6.1) is most simply explained by arguing that, in order for the vector\n(cid:10)x(1), . . . , x(n)(cid:11) to equal (cid:10)x1, . . . , xn(cid:11), it is necessary and sufficient for (cid:10)x1, . . . , xn(cid:11) to\nequal one of the n! permutations of (cid:10)x1, . . . , xn(cid:11). since the probability (density) that\n(cid:10)x1, . . . , xn(cid:11) equals any given permutation of (cid:10)x1, . . . , xn(cid:11) is just f (x1)\u00b7\u00b7\u00b7 f (xn), equa-\ntion (6.1) follows.\n\nexample 6a\nalong a road 1 mile long are 3 people \u201cdistributed at random.\u201d find the probability\nthat no 2 people are less than a distance of d miles apart when d \u2026 1\n2.\n\nsolution. let us assume that \u201cdistributed at random\u201d means that the positions of the\n3 people are independent and uniformly distributed over the road. if xi denotes the\nposition of the ith person, then the desired probability is p{x(i) > x(i\u22121) + d, i =\n2, 3}. because\n\nit follows that\np{x(i) > x(i\u22121) + d, i = 2, 3} =\n\nfx(1),x(2),x(3)\n\n0 < x1 < x2 < x3 < 1\n\n(x1, x2, x3) = 3!\n* * *\n*\n= 3!\n*\n*\n\n0\n1\u22122d\n\n= 6\n\n1\u22122d\n\n0\n\n1\u22122d\n\n= 6\n\n*\n\n*\n*\n*\n\nxi>xj\u22121+d\n1\u2212d\nx1+d\n1\u2212d\nx1+d\n1\u22122d\u2212x1\n\n0\n\n0\n\ny2 dy2 dx1\n\nfx(1),x(2),x(3)\n\n(x1, x2, x3) dx1 dx2 dx3\n\ndx3 dx2 dx1\n\n1\nx2+d\n(1 \u2212 d \u2212 x2) dx2 dx1\n\n "}, {"Page_number": 287, "text": "272\n\nchapter 6\n\njointly distributed random variables\nwhere we have made the change of variables y2 = 1 \u2212 d \u2212 x2. continuing the string\nof equalities yields\n\n(1 \u2212 2d \u2212 x1)2 dx1\n\n*\n*\n\n1\u22122d\n\n0\n\n1\u22122d\n\n= 3\n= 3\ny2\n1 dy1\n= (1 \u2212 2d)3\n\n0\n\nhence, the desired probability that no 2 people are within a distance d of each other\nwhen 3 people are uniformly and independently distributed over an interval of size\n1 is (1 \u2212 2d)3 when d \u2026 1\n2. in fact, the same method can be used to prove that\nwhen n people are distributed at random over the unit interval, the desired\nprobability is\n\n[1 \u2212 (n \u2212 1)d]n\n\nwhen d \u2026\n\n1\n\nn \u2212 1\n\nthe proof is left as an exercise.\n\n.\n\nthe density function of the jth-order statistic x(j) can be obtained either by inte-\ngrating the joint density function (6.1) or by direct reasoning as follows: in order for\nx(j) to equal x, it is necessary for j \u2212 1 of the n values x1, . . . , xn to be less than\nx, n \u2212 j of them to be greater than x, and 1 of them to equal x. now, the probability\ndensity that any given set of j \u2212 1 of the xi\u2019s are less than x, another given set of\nn \u2212 j are all greater than x, and the remaining value is equal to x equals\n\n[f(x)]j\u22121[1 \u2212 f(x)]n\u2212jf (x)\n\nhence, since there are(cid:2)\n\n(cid:3)\n\nn\n\nj \u2212 1, n \u2212 j, 1\n\n=\n\nn!\n\n(n \u2212 j)!(j \u2212 1)!\n\ndifferent partitions of the n random variables x1, . . . , xn into the preceding three\ngroups, it follows that the density function of x(j) is given by\n\n(x) =\n\nfx(j)\n\nn!\n\n(n \u2212 j)!(j \u2212 1)!\n\n[f(x)]j\u22121[1 \u2212 f(x)]n\u2212jf (x)\n\n(6.2)\n\nexample 6b\nwhen a sample of 2n + 1 random variables (that is, when 2n + 1 independent and\nidentically distributed random variables) is observed, the (n + 1)st smallest is called\nthe sample median. if a sample of size 3 from a uniform distribution over (0, 1) is\nobserved, find the probability that the sample median is between 1\n\n4 and 3\n4.\n\nsolution. from equation (6.2), the density of x(2) is given by\n\nfx(2)\n\n(x) = 3!\n1!1!\n\nx(1 \u2212 x)\n\n0 < x < 1\n\n "}, {"Page_number": 288, "text": "section 6.6\n\norder statistics 273\n\nhence,\n\n%\n\n1\n4\n\np\n\n< x(2) <\n\n/\n\n3\n4\n\n= 6\n\n= 6\n\n*\n0\n\n3/4\n\nx(1 \u2212 x) dx\n\n1/4\n\n7666666x=3/4\n\nx=1/4\n\nx2\n2\n\n\u2212 x3\n3\n\n= 11\n16\n\n.\n\nthe cumulative distribution function of x(j) can be found by integrating equa-\n\ntion (6.2). that is,\n\n*\n\n(y) =\n\nfx(j)\n\nn!\n\n(n \u2212 j)!(j \u2212 1)!\n\n[f(x)]j\u22121[1 \u2212 f(x)]n\u2212jf (x) dx\n\n(6.3)\n\ny\n\u2212q\n\nhowever, fx(j)\n(y) could also have been derived directly by noting that the jth order\nstatistic is less than or equal to y if and only if there are j or more of the xi\u2019s that are\nless than or equal to y. thus, because the number of xi\u2019s that are less than or equal\nto y is a binomial random variable with parameters n, p = f(y), it follows that\n\nfx(j)\n\n(y) = p{x(j) \u2026 y} = p{j or more of the xi\u2019s are \u2026 y}\n[f(y)]k[1 \u2212 f(y)]n\u2212k\n\n= n(cid:6)\n\n(cid:2)\n\n(cid:3)\n\nn\nk\n\nk=j\n\n(6.4)\n\nif, in equations (6.3) and (6.4), we take f to be the uniform (0, 1) distribution [that\n\nis, f (x) = 1, 0 < x < 1], then we obtain the interesting analytical identity\nn(cid:6)\n\n(cid:2)\n\n(cid:3)\n\n*\n\ny\n\nxj\u22121(1 \u2212 x)n\u2212j dx\n\n0 \u2026 y \u2026 1 (6.5)\n\nn\nk\n\nk=j\n\nyk(1 \u2212 y)n\u2212k =\n\nn!\n\n(n \u2212 j)!(j \u2212 1)!\n\n0\n\nby employing the same type of argument\n\nthat we used in establishing\nequation (6.2), we can show that the joint density function of the order statistics x(i)\nand x(j) when i < j is\n\nfx(i),x(j)\n\n(xi, xj) =\n\nfor all xi < xj.\n\nn!\n\n(i \u2212 1)!(j \u2212 i \u2212 1)!(n \u2212 j)!\n* [f(xj) \u2212 f(xi)]j\u2212i\u22121[1 \u2212 f(xj)]n\u2212jf (xi)f (xj)\n\n[f(xi)]i\u22121\n\n(6.6)\n\nexample 6c distribution of the range of a random sample\nsuppose that n independent and identically distributed random variables x1, x2, . . . ,\nxn are observed. the random variable r defined by r = x(n) \u2212 x(1) is called the\nrange of the observed random variables. if the random variables xi have distribution\nfunction f and density function f , then the distribution of r can be obtained from\nequation (6.6) as follows: for a \u00fa 0,\n\n "}, {"Page_number": 289, "text": "274\n\nchapter 6\n\njointly distributed random variables\n\np{r \u2026 a} = p{x(n) \u2212 x(1) \u2026 a}\n\n**\n* q\n*\n\nxn\u2212x1\u2026a\n\n\u2212q\n\nx1\n\n=\n\n=\n\n(x1, xn) dx1 dxn\n\nfx(1),x(n)\nx1+a\n\nn!\n\n(n \u2212 2)!\n\n[f(xn) \u2212 f(x1)]n\u22122f (x1)f (xn) dxn dx1\n\nmaking the change of variable y = f(xn) \u2212 f(x1), dy = f (xn) dxn yields\n\n[f(xn) \u2212 f(x1)]n\u22122f (xn) dxn =\n\nf(x1+a)\u2212f(x1)\n\nyn\u22122dy\n\n*\n\n*\n\nx1+a\n\nx1\n\n0\n= 1\n\nn \u2212 1\n\n[f(x1 + a) \u2212 f(x1)]n\u22121\n\nthus,\n\n* q\n\n\u2212q\n\np{r \u2026 a} = n\n\n[f(x1 + a) \u2212 f(x1)]n\u22121f (x1) dx1\n\n(6.7)\n\nequation (6.7) can be evaluated explicitly only in a few special cases. one such case\nis when the xi\u2019s are all uniformly distributed on (0, 1). in this case, we obtain, from\nequation (6.7), that for 0 < a < 1,\n\n*\n*\n\n1\n\n[f(x1 + a) \u2212 f(x1)]n\u22121f (x1) dx1\n1\u2212a\n\n*\n\n(1 \u2212 x1)n\u22121 dx1\n\n1\n1\u2212a\n\n0\n\np{r < a} = n\n= n\nan\u22121 dx1 + n\n= n(1 \u2212 a)an\u22121 + an\n0\n\n0\n\nfr(a) =\n\nn(n \u2212 1)an\u22122(1 \u2212 a)\n0\n\n0 \u2026 a \u2026 1\notherwise\n\ndifferentiation yields the density function of the range: given in this case by\n\nthat is, the range of n independent uniform (0, 1) random variables is a beta random\nvariable with parameters n \u2212 1, 2.\n.\n\n6.7 joint probability distribution of functions of random variables\n\nlet x1 and x2 be jointly continuous random variables with joint probability density\nfunction fx1,x2. it is sometimes necessary to obtain the joint distribution of the ran-\ndom variables y1 and y2, which arise as functions of x1 and x2. specifically, suppose\nthat y1 = g1(x1, x2) and y2 = g2(x1, x2) for some functions g1 and g2.\nassume that the functions g1 and g2 satisfy the following conditions:\n1. the equations y1 = g1(x1, x2) and y2 = g2(x1, x2) can be uniquely solved for x1\nand x2 in terms of y1 and y2, with solutions given by, say, x1 = h1(y1, y2), x2 =\nh2(y1, y2).\nand are such that the 2 * 2 determinant\n\n2. the functions g1 and g2 have continuous partial derivatives at all points (x1, x2)\n\n "}, {"Page_number": 290, "text": "section 6.7\n\njoint distribution of functions of random variables 275\n\n666666666\n\n\u2202g1\n\u2202x1\n\u2202g2\n\u2202x1\n\n\u2202g1\n\u2202x2\n\u2202g2\n\u2202x2\n\n666666666 k \u2202g1\n\n\u2202x1\n\n\u2202g2\n\u2202x2\n\n\u2212 \u2202g1\n\u2202x2\n\n\u2202g2\n\u2202x1\n\nz 0\n\nj(x1, x2) =\n\nat all points (x1, x2).\n\nunder these two conditions, it can be shown that the random variables y1 and y2\n\nare jointly continuous with joint density function given by\n\n(y1, y2) = fx1,x2\n\n(x1, x2)|j(x1, x2)|\u22121\n\nfy1y2\n\n(7.1)\n\nwhere x1 = h1(y1, y2), x2 = h2(y1, y2).\n\na proof of equation (7.1) would proceed along the following lines:\n\n* *\n\np{y1 \u2026 y1, y2 \u2026 y2} =\n\nfx1,x2\n\n(x1, x2) dx1 dx2\n\n(7.2)\n\n(x1, x2) :\n\ng1(x1, x2) \u2026 y1\ng2(x1, x2) \u2026 y2\n\nthe joint density function can now be obtained by differentiating equation (7.2) with\nrespect to y1 and y2. that the result of this differentiation will be equal to the right-\nhand side of equation (7.1) is an exercise in advanced calculus whose proof will not\nbe presented in this book.\n\nexample 7a\nlet x1 and x2 be jointly continuous random variables with probability density func-\ntion fx1,x2. let y1 = x1 + x2, y2 = x1 \u2212 x2. find the joint density function of y1\nand y2 in terms of fx1,x2.\nsolution. let g1(x1, x2) = x1 + x2 and g2(x1, x2) = x1 \u2212 x2. then\n\n6666 1 1\n\n1 \u22121\n\n6666 = \u22122\n\nj(x1, x2) =\n\nalso, since the equations y1 = x1 + x2 and y2 = x1 \u2212 x2 have x1 = (y1 + y2)/2, x2 =\n(y1 \u2212 y2)/2 as their solution, it follows from equation (7.1) that the desired density is\n\n(cid:2)\n\n(cid:3)\n\nfy1,y2\n\n(y1, y2) = 1\n2\n\nfx1,x2\n\ny1 + y2\n\n2\n\n,\n\ny1 \u2212 y2\n\n2\n\nfor instance, if x1 and x2 are independent uniform (0, 1) random variables, then\n\n0\n\n(y1, y2) =\n\nfy1,y2\n\n1\n2\n0\n\n0 \u2026 y1 + y2 \u2026 2, 0 \u2026 y1 \u2212 y2 \u2026 2\notherwise\n\n "}, {"Page_number": 291, "text": "276\n\nchapter 6\n\njointly distributed random variables\n\nor if x1 and x2 are independent exponential random variables with respective param-\neters \u03bb1 and \u03bb2, then\n\n\u23a7\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23a9\n\nfy1,y2\n\n(y1, y2)\n\u03bb1\u03bb2\n\n=\n\n2\n\n0\n\n0\n\n(cid:2)\n\nexp\n\n\u2212\u03bb1\n\n(cid:3)\n\n(cid:2)\n\ny1 + y2\n\n2\n\n\u2212 \u03bb2\n\ny1 \u2212 y2\n\n2\n\n(cid:3)7\n\ny1 + y2 \u00fa 0, y1 \u2212 y2 \u00fa 0\n\notherwise\n\ny\n\nr\n\nx\n\n\u235c\n\nfigure 6.4: \u2022 = random point. (x, y ) = (r, \u03b8).\n\nfinally, if x1 and x2 are independent standard normal random variables, then\n\nfy1,y2\n\n(y1, y2) = 1\n4\u03c0\n= 1\n4\u03c0\n= 1\u221a\n4\u03c0\n\n\u2212[(y1+y2)2/8+(y1\u2212y2)2/8]\ne\n\u2212(y2\ne\n\n+y2\n\n)/4\n\n1\n\n2\n\n\u2212y2\ne\n\n1\n\n/4 1\u221a\n4\u03c0\n\n\u2212y2\ne\n\n2\n\n/4\n\nthus, not only do we obtain (in agreement with proposition 3.2) that both x1 + x2\nand x1 \u2212 x2 are normal with mean 0 and variance 2, but we also conclude that these\ntwo random variables are independent. (in fact, it can be shown that if x1 and x2\nare independent random variables having a common distribution function f, then\nx1 + x2 will be independent of x1 \u2212 x2 if and only if f is a normal distribution\n.\nfunction.)\n\nexample 7b\nlet (x, y) denote a random point in the plane, and assume that the rectangular\ncoordinates x and y are independent standard normal random variables. we are\ninterested in the joint distribution of r, \u03b8, the polar coordinate representation of\n(x, y). (see figure 6.4.)\nsuppose first that x and y are both positive. for x and y positive, letting r =\ng1(x, y) =\n\nx2 + y2 and \u03b8 = g2(x, y) = tan\n\n\u22121 y/x, we see that\n\n.\n\nx.\nx2 + y2\ny.\nx2 + y2\n\n=\n\n=\n\n\u2202g1\n\u2202x\n\u2202g1\n\u2202y\n\n "}, {"Page_number": 292, "text": "section 6.7\n\njoint distribution of functions of random variables 277\n\n(cid:3)\n\n(cid:2)\u2212y\n\n=\n\n=\n\n\u2202g2\n\u2202x\n\u2202g2\n\u2202y\n\n1\n\n1 + (y/x)2\nx[1 + (y/x)2]\n\n1\n\nx2\n=\n\n= \u2212y\nx2 + y2\nx\n\nx2 + y2\n\nhence,\n\nj(x, y) =\n\nx2\n\n(x2 + y2)3/2\n\n+\n\ny2\n\n(x2 + y2)3/2\n\n=\n\n1.\nx2 + y2\n\n= 1\nr\n\nbecause the conditional joint density function of x, y given that they are both\n\npositive is\n\nf (x, y|x > 0, y > 0) =\n\nf (x, y)\n\np(x > 0, y > 0)\n\n= 2\n\u03c0\n\nwe see that the conditional joint density function of r =\ntan\n\n\u22121(y/x), given that x and y are both positive, is\n\n\u2212(x2+y2)/2, x > 0, y > 0\ne\n\n.\nx2 + y2 and \u0001 =\n\nf (r, \u03b8|x > 0, y > 0) = 2\n\n\u03c0\n\n\u2212r2/2,\nre\n\n0 < \u03b8 < \u03c0/2,\n\n0 < r < q\n\nsimilarly, we can show that\n\nf (r, \u03b8|x < 0, y > 0) = 2\nf (r, \u03b8|x < 0, y < 0) = 2\nf (r, \u03b8|x > 0, y < 0) = 2\n\n\u03c0\n\n\u03c0\n\n\u03c0\n\n\u2212r2/2, \u03c0/2 < \u03b8 < \u03c0,\nre\n\u2212r2/2, \u03c0 < \u03b8 < 3\u03c0/2,\nre\n\u2212r2/2,\nre\n\n3\u03c0/2 < \u03b8 < 2\u03c0,\n\n0 < r < q\n\n0 < r < q\n\n0 < r < q\n\nas the joint density is an equally weighted average of these 4 conditional joint densi-\nties, we obtain that the joint density of r, \u03b8 is given by\n\nf (r, \u03b8 ) = 1\n2\u03c0\n\n\u2212r2/2\n\nre\n\n0 < \u03b8 < 2\u03c0,\n\n0 < r < q\n\nnow, this joint density factors into the marginal densities for r and \u03b8, so r and \u03b8\nare independent random variables, with \u03b8 being uniformly distributed over (0, 2\u03c0 )\nand r having the rayleigh distribution with density\n\nf (r) = re\n\n\u2212r2/2\n\n0 < r < q\n\n(for instance, when one is aiming at a target in the plane, if the horizontal and vertical\nmiss distances are independent standard normals, then the absolute value of the error\nhas the preceding rayleigh distribution.)\n\nthis result is quite interesting, for it certainly is not evident a priori that a ran-\ndom vector whose coordinates are independent standard normal random variables\nwill have an angle of orientation that not only is uniformly distributed, but also is\nindependent of the vector\u2019s distance from the origin.\n\n "}, {"Page_number": 293, "text": "278\n\nchapter 6\n\njointly distributed random variables\n\nif we wanted the joint distribution of r2 and \u03b8, then, since the transformation\n\nd = g1(x, y) = x2 + y2 and \u03b8 = g2(x, y) = tan\n\n\u22121 y/x has the jacobian\n\n6666666\n\nj =\n\n2x\n\u2212y\n\nx2 + y2\n\n2y\nx\n\nx2 + y2\n\n6666666 = 2\n\nit follows that\n\nf (d, \u03b8 ) = 1\n2\n\n\u2212d/2 1\ne\n2\u03c0\n\n0 < d < q,\n\n0 < \u03b8 < 2\u03c0\n\ntherefore, r2 and \u03b8 are independent, with r2 having an exponential distribution\n2. but because r2 = x2 + y2, it follows by definition that r2 has\nwith parameter 1\na chi-squared distribution with 2 degrees of freedom. hence, we have a verification\nof the result that the exponential distribution with parameter 1\n2 is the same as the\nchi-squared distribution with 2 degrees of freedom.\n\nthe preceding result can be used to simulate (or generate) normal random vari-\nables by making a suitable transformation on uniform random variables. let u1 and\nu2 be independent random variables, each uniformly distributed over (0, 1). we will\ntransform u1, u2 into two independent unit normal random variables x1 and x2\nby first considering the polar coordinate representation (r, \u03b8) of the random vec-\ntor (x1, x2). from the preceding, r2 and \u03b8 will be independent, and, in addition,\nr2 = x2\n2. but\n%\n\u22122 log u1 has such a distribution, since, for x > 0,\n\n2 will have an exponential distribution with parameter \u03bb = 1\n\n+ x2\n\n/\n\n1\n\np{\u22122 log u1 < x} = p\n\nlog u1 > \u2212x\n2\n\u2212x/2}\n\n= p{u1 > e\n= 1 \u2212 e\n\u2212x/2\n\nalso, because 2\u03c0u2 is a uniform (0, 2\u03c0 ) random variable, we can use it to generate \u03b8.\nthat is, if we let\n\nr2 = \u22122 log u1\n\u03b8 = 2\u03c0u2\n\nthen r2 can be taken to be the square of the distance from the origin and \u03b8 can be\ntaken to be the angle of orientation of (x1, x2). now, since x1 = r cos \u03b8, x2 =\nr sin \u03b8, it follows that\n\n.\n.\n\u22122 log u1 cos(2\u03c0u2)\n\u22122 log u1 sin(2\u03c0u2)\n\nx1 =\nx2 =\n\nare independent standard normal random variables.\n\n.\n\nexample 7c\nif x and y are independent gamma random variables with parameters (\u03b1, \u03bb) and\n(\u03b2, \u03bb), respectively, compute the joint density of u = x + y and v = x/(x + y).\n\n "}, {"Page_number": 294, "text": "section 6.7\n\njoint distribution of functions of random variables 279\n\nsolution. the joint density of x and y is given by\n\nfx,y (x, y) = \u03bbe\n\n\u2212\u03bbx(\u03bbx)\u03b1\u22121\n\n\u2212\u03bby(\u03bby)\u03b2\u22121\n\n\u03bbe\n\n\u0001(\u03b2)\n\n\u0001(\u03b1)\n\n= \u03bb\u03b1+\u03b2\n\n\u0001(\u03b1)\u0001(\u03b2)\n\n\u2212\u03bb(x+y)x\u03b1\u22121y\u03b2\u22121\ne\n\nnow, if g1(x, y) = x + y, g2(x, y) = x/(x + y), then\n\n\u2202g1\n\u2202x\n\n= \u2202g1\n\u2202y\n\nso\n\n= 1\n\n=\n\n\u2202g2\n\u2202x\n\ny\n\n(x + y)2\n\n1\ny\n\n(x + y)2\n\n1\n\u2212x\n\n(x + y)2\n\n\u2202g2\n\u2202y\n\n= \u2212\n\nx\n\n(x + y)2\n\n6666666 = \u2212 1\n\nx + y\n\n6666666\n\nj(x, y) =\n\nfinally, as the equations u = x + y, v = x/(x + y) have as their solutions x = uv, y =\nu(1 \u2212 v), we see that\n\nfu,v (u, v) = fx,y[uv, u(1 \u2212 v)]u\n\n= \u03bbe\n\n\u2212\u03bbu(\u03bbu)\u03b1+\u03b2\u22121\n\u0001(\u03b1 + \u03b2)\n\nv\u03b1\u22121(1 \u2212 v)\u03b2\u22121\u0001(\u03b1 + \u03b2)\n\n\u0001(\u03b1)\u0001(\u03b2)\n\nhence, x + y and x/(x + y) are independent, with x + y having a gamma dis-\ntribution with parameters (\u03b1 + \u03b2, \u03bb) and x/(x + y) having a beta distribution with\nparameters (\u03b1, \u03b2). the preceding reasoning also shows that b(\u03b1, \u03b2), the normalizing\nfactor in the beta density, is such that\n\nb(\u03b1, \u03b2) k\n\n1\n\nv\u03b1\u22121(1 \u2212 v)\u03b2\u22121dv\n\n*\n\n0\n= \u0001(\u03b1)\u0001(\u03b2)\n\u0001(\u03b1 + \u03b2)\n\nthis entire result is quite interesting. for suppose there are n + m jobs to be per-\nformed, each (independently) taking an exponential amount of time with rate \u03bb to be\ncompleted and suppose that we have two workers to perform these jobs. worker i\nwill do jobs 1, 2, . . . , n, and worker ii will do the remaining m jobs. if we let x and y\ndenote the total working times of workers i and ii, respectively, then (either from the\nforegoing result or from example 3b) x and y will be independent gamma random\nvariables having parameters (n, \u03bb) and (m, \u03bb), respectively. it then follows that, inde-\npendently of the working time needed to complete all n + m jobs (that is, of x + y),\nthe proportion of this work that will be performed by worker i has a beta distribution\n.\nwith parameters (n, m).\n\nwhen the joint density function of the n random variables x1, x2, . . . , xn is given\n\nand we want to compute the joint density function of y1, y2, . . . , yn, where\ny1 = g1(x1, . . . , xn) y2 = g2(x1, . . . , xn), . . . yn = gn(x1, . . . , xn)\n\n "}, {"Page_number": 295, "text": "280\n\nchapter 6\n\njointly distributed random variables\n\nthe approach is the same\u2014namely, we assume that the functions gi have continuous\npartial derivatives and that the jacobian determinant.\n\n66666666666666\n\n\u2202g1\n\u2202x1\n\u2202g2\n\u2202x1\n\u2202gn\n\u2202x1\n\n\u2202g1\n\u2202x2\n\u2202g2\n\u2202x2\n\u2202gn\n\u2202x2\n\n\u00b7\u00b7\u00b7 \u2202g1\n\u2202xn\n\u00b7\u00b7\u00b7 \u2202g2\n\u2202xn\n\u00b7\u00b7\u00b7 \u2202gn\n\u2202xn\n\n66666666666666\n\nz 0\n\nj(x1, . . . , xn) =\n\nat all points (x1, . . . , xn). furthermore, we suppose that the equations y1 =\ng1(x1, . . . , xn), y2 = g2(x1, . . . , xn), . . . , yn = gn(x1, . . . , xn) have a unique solution, say,\nx1 = h1(y1, . . . , yn), . . . , xn = hn(y1, . . . , yn). under these assumptions, the joint den-\nsity function of the random variables yi is given by\n\nfy1,...,yn\n\n(y1, . . . , yn) = fx1,...,xn\n\n(x1, . . . , xn)|j(x1, . . . , xn)|\u22121\n\n(7.3)\n\nwhere xi = hi(y1, . . . , yn), i = 1, 2, . . . , n.\nexample 7d\nlet x1, x2, and x3 be independent standard normal random variables. if y1 = x1 +\nx2 + x3, y2 = x1 \u2212 x2, and y3 = x1 \u2212 x3, compute the joint density function of\ny1, y2, y3.\nsolution. letting y1 = x1 + x2 + x3, y2 = x1 \u2212 x2, y3 = x1 \u2212 x3, the jacobian\nof these transformations is given by\n\n666666 1\n\nj =\n\n1\n1\n1 \u22121\n0\n0 \u22121\n1\n\n666666 = 3\n\nas the preceding transformations yield that\n\nx1 = y1 + y2 + y3\n\n3\n\nx2 = y1 \u2212 2y2 + y3\n\n3\n\nx3 = y1 + y2 \u2212 2y3\n\n3\n\nwe see from equation (7.3) that\n\nfy1, y2, y3\n\n(y1, y2, y3)\n= 1\n3\n\nfx1, x2, x3\n\n(cid:2)\n\ny1 + y2 + y3\n\n3\n\n,\n\ny1 \u2212 2y2 + y3\n\n3\n\ny1 + y2 \u2212 2y3\n\n3\n\n,\n\n(cid:3)\n\nhence, as\n\nwe see that\n\nfx1, x2, x3\n\n(x1, x2, x3) =\n\n\u2212(cid:9)\n(2\u03c0 )3/2 e\n\n1\n\n3\ni=1 x2\ni\n\n/2\n\nfy1, y2, y3\n\n(y1, y2, y3) =\n\n1\n\n3(2\u03c0 )3/2 e\n\n\u2212q(y1, y2, y3)/2\n\n "}, {"Page_number": 296, "text": "section 6.7\n\njoint distribution of functions of random variables 281\n\nwhere\n\n(cid:2)\n\nq(y1, y2, y3)\n=\n\n(cid:2)\n\n(cid:3)2 +\n\ny1 \u2212 2y2 + y3\n\n3\n\n(cid:2)\n\n(cid:3)2 +\n\ny1 + y2 \u2212 2y3\n\n3\n\ny1 + y2 + y3\n\n3\n\ny2\n2\n\n= y2\n1\n3\n\n+ 2\n3\n\n+ 2\n3\n\ny2\n3\n\n\u2212 2\n3\n\ny2y3\n\n(cid:3)2\n\n.\n\nexample 7e\nlet x1, x2, . . . , xn be independent and identically distributed exponential random\nvariables with rate \u03bb. let\n\nyi = x1 + \u00b7\u00b7\u00b7 + xi\n\ni = 1, . . . , n\n\n(a) find the joint density function of y1, . . . , yn.\n(b) use the result of part (a) to find the density of yn.\n\n(a) the jacobian of the transformations y1 = x1, y2 = x1 + x2, . . .,\n\nsolution.\n\nyn = x1 + \u00b7\u00b7\u00b7 + xn is\n\n666666666666\n\nj =\n\n1\n1\n1\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n1\n\n0\n1\n1\n\n1\n\n0\n0\n1\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n1\n\n0 \u00b7\u00b7\u00b7 0\n0 \u00b7\u00b7\u00b7 0\n0 \u00b7\u00b7\u00b7 0\n\n1 \u00b7\u00b7\u00b7 1\n\n666666666666\n\nsince only the first term of the determinant will be nonzero, we have j = 1.\nnow, the joint density function of x1, . . . , xn is given by\n\n(x1, . . . , xn) = n(cid:31)\n\ni=1\n\nfx1,...,xn\n\n\u2212\u03bbxi\n\n\u03bbe\n\n0 < xi < q, i = 1, . . . , n\n\nhence, because the preceding transformations yield\n\nx1 = y1, x2 = y2 \u2212 y1, . . . , xi = yi \u2212 yi\u22121, . . . , xn = yn \u2212 yn\u22121\n\nit follows from equation (7.3) that the joint density function of y1, . . . , yn is\nfy1,...,yn\n\n(y1, y2, . . . , yn)\n= fx1,...,xn\n= \u03bbn exp\n\n= \u03bbne\n= \u03bbne\n\n\u2212\u03bbyn\n\u2212\u03bbyn\n\n\u23ab\u23aa\u23ac\n\u23a4\n\u23a6\n\u23aa\u23ad\n\n\u23a1\n\u23a3y1 + n(cid:6)\n\n\u23a7\u23aa\u23a8\n(y1, y2 \u2212 y1, . . . , yi \u2212 yi\u22121, . . . , yn \u2212 yn\u22121)\n\u23aa\u23a9\u2212\u03bb\n0 < y1, 0 < yi \u2212 yi\u22121, i = 2, . . . , n\n0 < y1 < y2 < \u00b7\u00b7\u00b7 < yn\n\n(yi \u2212 yi\u22121)\n\ni=2\n\n "}, {"Page_number": 297, "text": "282\n\nchapter 6\n\njointly distributed random variables\n\n(b) to obtain the marginal density of yn, let us integrate out the other variables one\n\nat a time. doing this gives\n\nfy2,...,yn\n\n(y2, . . . , yn) =\n\n*\n\ncontinuing, we obtain\n\nfy3,...,yn\n\n(y3, . . . , yn) =\n\ny3\n\n\u03bbny2e\n\n\u2212\u03bbyndy2\n\n0\n\ny2\n\n\u2212\u03bbyndy1\n\u03bbne\n= \u03bbny2e\n\u2212\u03bbyn\n*\n\n0 < y2 < y3 < \u00b7\u00b7\u00b7 < yn\n\n0\n\n= \u03bbn y2\n3\n2\n\n\u2212\u03bbyn\ne\n\n0 < y3 < y4 < \u00b7\u00b7\u00b7 < yn\n\nthe next integration yields\n\nfy4,...,yn\n\n(y4, . . . , yn) = \u03bbn y3\n4\n3!\n\n\u2212\u03bbyn\ne\n\n0 < y4 < \u00b7\u00b7\u00b7 < yn\n\ncontinuing in this fashion gives\n(yn) = \u03bbn\n\nfyn\n\nyn\u22121\n(n \u2212 1)!\n\nn\n\n\u2212\u03bbyn\ne\n\n0 < yn\n\nwhich, in agreement with the result obtained in example 3b, shows that x1 +\n\u00b7\u00b7\u00b7 + xn is a gamma random variable with parameters n and \u03bb.\n.\n\n\u22176.8 exchangeable random variables\n\nthe random variables x1, x2, . . . , xn are said to be exchangeable if, for every permu-\ntation i1, . . . , in of the integers 1, . . . , n,\n\np{xi1\n\n\u2026 x1, xi2\n\n\u2026 x2, . . . , xin\n\n\u2026 xn} = p{x1 \u2026 x1, x2 \u2026 x2, . . . , xn \u2026 xn}\n\nfor all x1, . . . , xn. that is, the n random variables are exchangeable if their joint distri-\nbution is the same no matter in which order the variables are observed.\n\ndiscrete random variables will be exchangeable if\n\np{xi1\n\n= x1, xi2\n\n= x2, . . . , xin\n\n= xn} = p{x1 = x1, x2 = x2, . . . , xn = xn}\n\nfor all permutations i1, . . . , in, and all values x1, . . . , xn. this is equivalent to stating\nthat p(x1, x2, . . . , xn) = p{x1 = x1, . . . , xn = xn} is a symmetric function of the vector\n(x1, . . . , xn), which means that its value does not change when the values of the vector\nare permuted.\n\nexample 8a\nsuppose that balls are withdrawn one at a time and without replacement from an\nurn that initially contains n balls, of which k are considered special, in such a manner\nthat each withdrawal is equally likely to be any of the balls that remain in the urn\nat the time. let xi = 1 if the ith ball withdrawn is special and let xi = 0 otherwise.\nwe will show that the random variables x1, . . . , xn are exchangeable. to do so, let\n(x1, . . . , xn) be a vector consisting of k ones and n \u2212 k zeros. however, before consid-\nering the joint mass function evaluated at (x1, . . . , xn), let us try to gain some insight by\n\n "}, {"Page_number": 298, "text": "section 6.8\n\nexchangeable random variables 283\n\nconsidering a fixed such vector\u2014for instance, consider the vector (1, 1, 0, 1, 0, . . . , 0, 1),\nwhich is assumed to have k ones and n \u2212 k zeros. then\nk \u2212 2\nn \u2212 3\n\np(1, 1, 0, 1, 0, . . . , 0, 1) = k\nn\n\nn \u2212 k \u2212 1\n\nn \u2212 k\nn \u2212 2\n\nk \u2212 1\nn \u2212 1\n\nn \u2212 4\n\n\u00b7\u00b7\u00b7 1\n2\n\n1\n1\n\nwhich follows because the probability that the first ball is special is k/n, the condi-\ntional probability that the next one is special is (k \u2212 1)/(n \u2212 1), the conditional\nprobability that the next one is not special is (n \u2212 k)/(n \u2212 2), and so on. by the same\nargument, it follows that p(x1, . . . , xn) can be expressed as the product of n fractions.\nthe successive denominator terms of these fractions will go from n down to 1. the\nnumerator term at the location where the vector (x1, . . . , xn) is 1 for the ith time is\nk \u2212 (i \u2212 1), and where it is 0 for the ith time it is n \u2212 k \u2212 (i \u2212 1). hence, since the\nvector (x1, . . . , xn) consists of k ones and n \u2212 k zeros, we obtain\nxi = k\n\np(x1, . . . , xn) = k!(n \u2212 k)!\n\nxi = 0, 1,\n\nn(cid:6)\n\nn!\n\ni=1\n\nsince this is a symmetric function of (x1, . . . , xn), it follows that the random variables\n.\nare exchangeable.\nremark. another way to obtain the preceding formula for the joint probability\nmass function is to regard all the n balls as distinguishable from each other. then,\nsince the outcome of the experiment is an ordering of these balls, it follows that\nthere are n! equally likely outcomes. finally, because the number of outcomes hav-\ning special and nonspecial balls in specified places is equal to the number of ways of\npermuting the special and the nonspecial balls among themselves, namely k!(n \u2212 k)!,\n.\nwe obtain the preceding density function.\n\nit is easily seen that if x1, x2, . . . , xn are exchangeable, then each xi has the same\nprobability distribution. for instance, if x and y are exchangeable discrete random\nvariables, then\n\np{x = x} =\n\np{x = x, y = y} =\n\np{x = y, y = x} = p{y = x}\n\n(cid:6)\n\n(cid:6)\n\ny\n\ny\n\nfor example, it follows from example 8a that the ith ball withdrawn will be special\nwith probability k/n, which is intuitively clear, since each of the n balls is equally\nlikely to be the ith one selected.\n\nexample 8b\nin example 8a, let yi denote the selection number of the first special ball withdrawn,\nlet y2 denote the additional number of balls that are then withdrawn until the second\nspecial ball appears, and, in general, let yi denote the additional number of balls\nwithdrawn after the (i \u2212 1)st special ball is selected until the ith is selected, i =\n1, . . . , k. for instance, if n = 4, k = 2 and x1 = 1, x2 = 0, x3 = 0, x4 = 1, then\ny1 = 1, y2 = 3. now, y1 = i1, y2 = i2, . . . , yk = ik 3 xi1\n= \u00b7\u00b7\u00b7 =\n= 1, xj = 0, otherwise; thus, from the joint mass function of the xi, we\nxi1+\u00b7\u00b7\u00b7+ik\nobtain\np{y1 = i1, y2 = i2, . . . , yk = ik} = k!(n \u2212 k)!\n\ni1 + \u00b7\u00b7\u00b7 + ik \u2026 n\n\n= xi1+i2\n\nn!\n\nhence, the random variables y1, . . . , yk are exchangeable. note that it follows from\nthis result that the number of cards one must select from a well-shuffled deck until\n\n "}, {"Page_number": 299, "text": "284\n\nchapter 6\n\njointly distributed random variables\n\nan ace appears has the same distribution as the number of additional cards one must\n.\nselect after the first ace appears until the next one does, and so on.\n\nexample 8c\nthe following is known as polya\u2019s urn model: suppose that an urn initially contains n\nred and m blue balls. at each stage, a ball is randomly chosen, its color is noted, and\nit is then replaced along with another ball of the same color. let xi = 1 if the ith ball\nselected is red and let it equal 0 if the ith ball is blue, i \u00fa 1. to obtain a feeling for the\njoint probabilities of these xi, note the following special cases:\n\nn\n\np{x1 = 1, x2 = 1, x3 = 0, x4 = 1, x5 = 0}\nn + m + 2\n\nn + 2\nn + m + 3\nn(n + 1)(n + 2)m(m + 1)\n\nn + m\n(n + m)(n + m + 1)(n + m + 2)(n + m + 3)(n + m + 4)\n\nn + m + 1\n\nn + m + 4\n\nm + 1\n\nn + 1\n\n=\n=\n\nm\n\nand\n\np{x1 = 0, x2 = 1, x3 = 0, x4 = 1, x5 = 1}\nn + m + 2\n\nn + 1\nn + m + 3\nn(n + 1)(n + 2)m(m + 1)\n\nn + m\n(n + m)(n + m + 1)(n + m + 2)(n + m + 3)(n + m + 4)\n\n= m\n=\n\nn + m + 1\n\nn + m + 4\n\nm + 1\n\nn + 2\n\nn\n\nby the same reasoning, for any sequence x1, . . . , xk that contains r ones and k \u2212 r\nzeros, we have\n\np{x1 = x1, . . . , xk = xk}\n\n= n(n + 1)\u00b7\u00b7\u00b7 (n + r \u2212 1)m(m + 1)\u00b7\u00b7\u00b7 (m + k \u2212 r \u2212 1)\n\n(n + m)\u00b7\u00b7\u00b7 (n + m + k \u2212 1)\n\ntherefore, for any value of k, the random variables x1, . . . , xk are exchangeable.\n\nan interesting corollary of the exchangeability in this model is that the probability\nthat the ith ball selected is red is the same as the probability that the first ball selected\nn\nn+m. (for an intuitive argument for this initially nonintuitive result,\nis red, namely,\nimagine that all the n + m balls initially in the urn are of different types. that is, one\nis a red ball of type 1, one is a red ball of type 2, . . ., one is a red ball type of n, one is\na blue ball of type 1, and so on, down to the blue ball of type m. suppose that when\na ball is selected it is replaced along with another of its type. then, by symmetry, the\nith ball selected is equally likely to be of any of the n + m distinct types. because n\nof these n + m types are red, the probability is\n.\n\n.)\n\nn\nn+m\n\nour final example deals with continuous random variables that are exchangeable.\n\nexample 8d\nlet x1, x2, . . . , xn be independent uniform (0, 1) random variables, and denote their\norder statistics by x(1), . . . , x(n). that is, x(j) is the jth smallest of x1, x2, . . . , xn.\n\n "}, {"Page_number": 300, "text": "summary 285\n\nalso, let\n\ny1 = x(1),\nyi = x(i) \u2212 x(i\u22121),\n\ni = 2, . . . n\n\nshow that y1, . . . , yn are exchangeable.\nsolution. the transformations\n\ny1 = x1, . . . , yi = xi \u2212 xi\u22121\n\ni = 2, . . . , n\n\nyield\n\nxi = y1 + \u00b7\u00b7\u00b7 + yi\n\ni = 1, . . . , n\n\nas it is easy to see that the jacobian of the preceding transformations is equal to 1,\nso, from equation (7.3), we obtain\n\nfy1,...,yn\n\n(y1, y2, . . . , yn) = f (y1, y1 + y2, . . . , y1 + \u00b7\u00b7\u00b7 + yn)\n\nwhere f is the joint density function of the order statistics. hence, from equation (6.1),\nwe obtain that\n\nfy1,...,yn\n\n(y1, y2, . . . , yn) = n! 0 < y1 < y1 + y2 < \u00b7\u00b7\u00b7 < y1 + \u00b7\u00b7\u00b7 + yn < 1\n\nor, equivalently,\n\nfy1,...,yn\n\n(y1, y2, . . . , yn) = n!\n\n0 < yi < 1, i = 1, . . . , n,\n\ny1 + \u00b7\u00b7\u00b7 + yn < 1\n\nbecause the preceding joint density is a symmetric function of y1, . . . , yn, we see that\n.\nthe random variables y1, . . . , yn are exchangeable.\n\nsummary\nthe joint cumulative probability distribution function of the pair of random variables\nx and y is defined by\n\nf(x, y) = p{x \u2026 x, y \u2026 y}\n\n\u2212 q < x, y < q\n\nall probabilities regarding the pair can be obtained from f. to find the individual\nprobability distribution functions of x and y, use\n\nfx (x) = lim\n\ny\u2192q f(x, y) fy (y) = lim\n\nx\u2192q f(x, y)\n\nif x and y are both discrete random variables, then their joint probability mass\n\nfunction is defined by\n\nthe individual mass functions are\n\np{x = i} =\n\np(i, j) = p{x = i, y = j}\n(cid:6)\n\np{y = j} =\n\np(i, j)\n\nj\n\n(cid:6)\n\ni\n\np(i, j)\n\nthe random variables x and y are said to be jointly continuous if there is a func-\n\ntion f (x, y), called the joint probability density function, such that for any two-dimensional\nset c,\n\n**\n\np{(x, y) \u2208 c} =\n\nf (x, y) dx dy\n\nc\n\n "}, {"Page_number": 301, "text": "286\n\nchapter 6\n\njointly distributed random variables\n\nit follows from the preceding formula that\n\np{x < x < x + dx, y < y < y + dy} l f (x, y) dx dy\n\n* q\n\n* q\n\nif x and y are jointly continuous, then they are individually continuous with density\nfunctions\n\nfx (x) =\n\nf (x, y)dy\n\n\u2212q\n\nfy (y) =\n\nf (x, y) dx\n\n\u2212q\n\nthe random variables x and y are independent if, for all sets a and b,\n\np{x \u2208 a, y \u2208 b} = p{x \u2208 a}p{y \u2208 b}\n\nif the joint distribution function (or the joint probability mass function in the discrete\ncase, or the joint density function in the continuous case) factors into a part depending\nonly on x and a part depending only on y, then x and y are independent.\n\nin general, the random variables x1, . . . , xn are independent if, for all sets of real\n\nnumbers a1, . . . , an,\n\np{x1 \u2208 a1, . . . , xn \u2208 an} = p{x1 \u2208 a1}\u00b7\u00b7\u00b7 p{xn \u2208 an}\n\nif x and y are independent continuous random variables, then the distribution func-\ntion of their sum can be obtained from the identity\n\n* q\n\n\u2212q\n\nfx+y (a) =\nn(cid:9)\n\nn(cid:9)\n\nfx (a \u2212 y)fy (y)dy\n\nif xi, i = 1, . . . , n, are independent normal random variables with respective param-\n\u03c3 2\ni .\n\ni , i = 1, . . . , n, then\n\nxi is normal with parameters\n\nif xi, i = 1, . . . , n, are independent poisson random variables with respective param-\n\n\u03bci and\n\neters \u03bci and \u03c3 2\n\ni=1\n\ni=1\n\ni=1\n\nn(cid:9)\n\nn(cid:9)\n\neters \u03bbi, i = 1, . . . , n, then\nfunction of x given that y = y is defined by\n\ni=1\n\nxi is poisson with parameter\n\n\u03bbi.\n\nif x and y are discrete random variables, then the conditional probability mass\n\nn(cid:9)\n\ni=1\n\np{x = x|y = y} = p(x, y)\npy (y)\n\nwhere p is their joint probability mass function. also, if x and y are jointly continu-\nous with joint density function f , then the conditional probability density function of\nx given that y = y is given by\n\nfx|y (x|y) = f (x, y)\nfy (y)\n\nthe ordered values x(1) \u2026 x(2) \u2026 \u00b7\u00b7\u00b7 \u2026 x(n) of a set of independent and identically\ndistributed random variables are called the order statistics of that set. if the random\nvariables are continuous and have density function f , then the joint density function\nof the order statistics is\n\nf (x1, . . . , xn) = n!f (x1)\u00b7\u00b7\u00b7 f (xn)\n\nx1 \u2026 x2 \u2026 \u00b7\u00b7\u00b7 \u2026 xn\n\nthe random variables x1, . . . , xn are called exchangeable if the joint distribution of\nxi1, . . . , xin is the same for every permutation i1, . . . , in of 1, . . . , n.\n\n "}, {"Page_number": 302, "text": "problems\n\nproblems 287\n\n6.1. two fair dice are rolled. find the joint probability\n\nmass function of x and y when\n(a) x is the largest value obtained on any die and\n\ny is the sum of the values;\n\n(b) x is the value on the first die and y is the\n\nlarger of the two values;\n\n(c) x is the smallest and y is the largest value\n\nobtained on the dice.\n\n6.2. suppose that 3 balls are chosen without replace-\nment from an urn consisting of 5 white and 8 red\nballs. let xi equal 1 if the ith ball selected is white,\nand let it equal 0 otherwise. give the joint proba-\nbility mass function of\n(a) x1, x2;\n(b) x1, x2, x3.\n\n6.3. in problem 2, suppose that the white balls are\nnumbered, and let yi equal 1 if the ith white ball is\nselected and 0 otherwise. find the joint probability\nmass function of\n(a) y1, y2;\n(b) y1, y2, y3.\n\n6.4. repeat problem 2 when the ball selected is\n\nreplaced in the urn before the next selection.\n\n6.5. repeat problem 3a when the ball selected is\n\nreplaced in the urn before the next selection.\n\n6.6. a bin of 5 transistors is known to contain 2 that\nare defective. the transistors are to be tested, one\nat a time, until the defective ones are identified.\ndenote by n1 the number of tests made until the\nfirst defective is identified and by n2 the number of\nadditional tests until the second defective is identi-\nfied. find the joint probability mass function of n1\nand n2.\n\n6.7. consider a sequence of independent bernoulli tri-\nals, each of which is a success with probability p.\nlet x1 be the number of failures preceding the\nfirst success, and let x2 be the number of failures\nbetween the first two successes. find the joint mass\nfunction of x1 and x2.\n\n6.8. the joint probability density function of x and y\n\nis given by\nf (x, y) = c(y2 \u2212 x2)e\n\n\u2212y \u2212 y \u2026 x \u2026 y, 0 < y < q\n\n(a) find c.\n(b) find the marginal densities of x and y.\n(c) find e[x].\n\n6.9. the joint probability density function of x and y\n\nis given by\nf (x, y) = 6\n7\n\n(cid:3)\n\n(cid:2)\nx2 + xy\n2\n\n(a) verify that this is indeed a joint density func-\n\ntion.\n\n(b) compute the density function of x.\n(c) find p{x > y}.\n(d) find p{y > 1\n(e) find e[x].\n(f) find e[y].\n\n|x < 1\n\n}.\n\n2\n\n2\n\n6.10. the joint probability density function of x and y\n\nis given by\nf (x, y) = e\n\n\u2212(x+y)\n\n0 \u2026 x < q, 0 \u2026 y < q\n\nfind (a) p{x < y} and (b) p{x < a}.\n\n6.11. a television store owner figures that 45 percent of\nthe customers entering his store will purchase an\nordinary television set, 15 percent will purchase\na plasma television set, and 40 percent will just\nbe browsing. if 5 customers enter his store on\na given day, what is the probability that he will\nsell exactly 2 ordinary sets and 1 plasma set on\nthat day?\n\n6.12. the number of people that enter a drugstore in\na given hour is a poisson random variable with\nparameter \u03bb = 10. compute the conditional prob-\nability that at most 3 men entered the drugstore,\ngiven that 10 women entered in that hour. what\nassumptions have you made?\n\n6.13. a man and a woman agree to meet at a certain\nlocation about 12:30 p.m. if the man arrives at\na time uniformly distributed between 12:15 and\n12:45, and if the woman independently arrives at\na time uniformly distributed between 12:00 and 1\np.m., find the probability that the first to arrive\nwaits no longer than 5 minutes. what is the proba-\nbility that the man arrives first?\n\n6.14. an ambulance travels back and forth at a con-\nstant speed along a road of length l. at a certain\nmoment of time, an accident occurs at a point uni-\nformly distributed on the road. [that is, the dis-\ntance of the point from one of the fixed ends of the\nroad is uniformly distributed over (0, l).] assum-\ning that the ambulance\u2019s location at the moment\nof the accident is also uniformly distributed, and\nassuming independence of the variables, compute\nthe distribution of the distance of the ambulance\nfrom the accident.\n\n6.15. the random vector (x, y) is said to be uniformly\ndistributed over a region r in the plane if, for some\nconstant c, its joint density is\n\n%\n\nif(x, y) \u2208 r\notherwise\n\nc\n0\n\n0 < x < 1, 0 < y < 2\n\nf (x, y) =\n\n "}, {"Page_number": 303, "text": "288\n\njointly distributed random variables\n\nchapter 6\n(a) show that 1/c = area of region r.\nsuppose that (x, y) is uniformly distributed over\nthe square centered at (0, 0) and with sides of\nlength 2.\n(b) show that x and y are independent, with\neach being distributed uniformly over (\u22121, 1).\n(c) what is the probability that (x, y) lies in the\ncircle of radius 1 centered at the origin? that\nis, find p{x2 + y2 \u2026 1}.\n\n6.16. suppose that n points are independently chosen at\nrandom on the circumference of a circle, and we\nwant the probability that they all lie in some semi-\ncircle. that is, we want the probability that there is\na line passing through the center of the circle such\nthat all the points are on one side of that line, as\nshown in the following diagram:\n\nlet p1, . . . , pn denote the n points. let a denote\nthe event that all the points are contained in some\nsemicircle, and let ai be the event that all the\npoints lie in the semicircle beginning at the point\n\u25e6\npi and going clockwise for 180\n(a) express a in terms of the ai.\n(b) are the ai mutually exclusive?\n(c) find p(a).\n\n, i = 1, . . . , n.\n\n6.17. three points x1, x2, x3 are selected at random\non a line l. what is the probability that x2 lies\nbetween x1 and x3?\n\n6.18. two points are selected randomly on a line of\nlength l so as to be on opposite sides of the mid-\npoint of the line. [in other words, the two points\nx and y are independent random variables such\nthat x is uniformly distributed over (0, l/2) and\ny is uniformly distributed over (l/2, l).] find\nthe probability that the distance between the two\npoints is greater than l/3.\n6.19. show that f (x, y) = 1/x, 0 < y < x < 1, is a joint\ndensity function. assuming that f is the joint den-\nsity function of x, y, find\n(a) the marginal density of y;\n(b) the marginal density of x;\n(c) e[x];\n(c) e[y].\n\n0\n\n%\n\n6.20. the joint density of x and y is given by\n\nf (x, y) =\n\n\u2212(x+y)\n\nxe\n0\n\nx > 0, y > 0\notherwise\n\nare x and y independent? if, instead, f (x, y) were\ngiven by\n\nf (x, y) =\n\n2\n0\n\n0 < x < y, 0 < y < 1\notherwise\n\nwould x and y be independent?\n\n6.21. let\n\nf (x, y) = 24xy 0 \u2026 x \u2026 1, 0 \u2026 y \u2026 1, 0 \u2026 x + y \u2026 1\nand let it equal 0 otherwise.\n(a) show that f (x, y) is a joint probability density\n\nfunction.\n\n(b) find e[x].\n(c) find e[y].\n\n%\n\n6.22. the joint density function of x and y is\n\nf (x, y) =\n\nx + y\n0\n\n0 < x < 1, 0 < y < 1\notherwise\n\n(a) are x and y independent?\n(b) find the density function of x.\n(c) find p{x + y < 1}.\n\n6.23. the random variables x and y have joint density\n\nfunction\nf (x, y) = 12xy(1 \u2212 x) 0 < x < 1, 0 < y < 1\nand equal to 0 otherwise.\n(a) are x and y independent?\n(b) find e[x].\n(c) find e[y].\n(d) find var(x).\n(e) find var(y).\n6.24. consider independent trials, each of which results\nk(cid:9)\nin outcome i, i = 0, 1, . . . , k, with probability\npi = 1. let n denote the number of trials\npi,\nneeded to obtain an outcome that is not equal to 0,\nand let x be that outcome.\n(a) find p{n = n}, n \u00fa 1.\n(b) find p{x = j}, j = 1, . . . , k.\n(c) show that p{n = n, x = j} = p{n =\n\ni=0\n\nn}p{x = j}.\n\nof x?\n\nof n?\n\n(d) is it intuitive to you that n is independent\n\n(e) is it intuitive to you that x is independent\n\n6.25. suppose that 106 people arrive at a service station\nat times that are independent random variables,\n\n "}, {"Page_number": 304, "text": "each of which is uniformly distributed over (0, 106).\nlet n denote the number that arrive in the first\nhour. find an approximation for p{n = i}.\n\n6.26. suppose that a, b, c, are independent random\nvariables, each being uniformly distributed over\n(0, 1).\n(a) what is the joint cumulative distribution func-\n\n(b) what is the probability that all of the roots of\n\ntion of a, b, c?\nthe equation ax2 + bx + c = 0 are real?\n\n6.27. if x1 and x2 are independent exponential ran-\ndom variables with respective parameters \u03bb1 and\n\u03bb2, find the distribution of z = x1/x2. also com-\npute p{x1 < x2}.\n\n6.28. the time that it takes to service a car is an expo-\n\nnential random variable with rate 1.\n(a) if a. j. brings his car in at time 0 and m. j.\nbrings her car in at time t, what is the probabil-\nity that m. j.\u2019s car is ready before a. j.\u2019s car?\n(assume that service times are independent\nand service begins upon arrival of the car.)\n\n(b) if both cars are brought in at time 0, with\nwork starting on m. j.\u2019s car only when a. j.\u2019s\ncar has been completely serviced, what is the\nprobability that m. j.\u2019s car is ready before\ntime 2?\n\n6.29. the gross weekly sales at a certain restaurant is\na normal random variable with mean $2200 and\nstandard deviation $230. what is the probabil-\nity that\n(a) the total gross sales over the next 2 weeks\n\nexceeds $5000;\n\n(b) weekly sales exceed $2000 in at least 2 of the\n\nnext 3 weeks?\n\nwhat independence assumptions have you made?\n6.30. jill\u2019s bowling scores are approximately normally\ndistributed with mean 170 and standard deviation\n20, while jack\u2019s scores are approximately normally\ndistributed with mean 160 and standard deviation\n15. if jack and jill each bowl one game, then\nassuming that their scores are independent ran-\ndom variables, approximate the probability that\n(a) jack\u2019s score is higher;\n(b) the total of their scores is above 350.\n\n6.31. according to the u.s. national center for health\nstatistics, 25.2 percent of males and 23.6 percent of\nfemales never eat breakfast. suppose that random\nsamples of 200 men and 200 women are chosen.\napproximate the probability that\n(a) at least 110 of these 400 people never eat\n\nbreakfast;\n\n(b) the number of the women who never eat\nbreakfast is at least as large as the number of\nthe men who never eat breakfast.\n\nproblems 289\n\n6.32. the expected number of typographical errors on a\npage of a certain magazine is .2. what is the prob-\nability that an article of 10 pages contains (a) 0 and\n(b) 2 or more typographical errors? explain your\nreasoning!\n\n6.33. the monthly worldwide average number of air-\nplane crashes of commercial airlines is 2.2. what\nis the probability that there will be\n(a) more than 2 such accidents in the next month?\n(b) more than 4 such accidents in the next 2\n\nmonths?\n\nmonths?\n\n(c) more than 5 such accidents in the next 3\n\nexplain your reasoning!\n\n6.34. jay has two jobs to do, one after the other. each\nattempt at job i takes one hour and is successful\nwith probability pi. if p1 = .3 and p2 = .4, what\nis the probability that it will take jay more than 12\nhours to be successful on both jobs?\n\n6.35. in problem 4, calculate the conditional probability\n\nmass function of x1 given that\n(a) x2 = 1;\n(b) x2 = 0.\n\n6.36. in problem 3, calculate the conditional probability\n\nmass function of y1 given that\n(a) y2 = 1;\n(b) y2 = 0.\n\n6.37. in problem 5, calculate the conditional probability\n\nmass function of y1 given that\n(a) y2 = 1;\n(b) y2 = 0.\n6.38. choose a number x at random from the set of\nnumbers {1, 2, 3, 4, 5}. now choose a number at\nrandom from the subset no larger than x, that is,\nfrom {1, . . . , x}. call this second number y.\n(a) find the joint mass function of x and y.\n(b) find the conditional mass function of x given\n\nthat y = i. do it for i = 1, 2, 3, 4, 5.\n(c) are x and y independent? why?\n\n6.39. two dice are rolled. let x and y denote, respec-\ntively, the largest and smallest values obtained.\ncompute the conditional mass function of y given\nx = i, for i = 1, 2, . . . , 6. are x and y indepen-\ndent? why?\n\n6.40. the joint probability mass function of x and y is\n\ngiven by\n\np(1, 1) = 1\n8\np(2, 1) = 1\n8\ngiven y = i, i = 1, 2.\n\np(1, 2) = 1\n4\np(2, 2) = 1\n2\n\n(b) are x and y independent?\n\n(a) compute the conditional mass function of x\n\n "}, {"Page_number": 305, "text": "290\n\njointly distributed random variables\n\nchapter 6\n(c) compute p{xy \u2026 3}, p{x + y > 2},\n\np{x/y > 1}.\n\n6.41. the joint density function of x and y is given by\n\nf (x, y) = xe\n\n\u2212x(y+1)\n\nx > 0, y > 0\n\n(a) find the conditional density of x, given y =y,\n(b) find the density function of z = xy.\n\nand that of y, given x = x.\n\n6.42. the joint density of x and y is\n\n\u2212x\n\n0 \u2026 x < q, \u2212x \u2026 y \u2026 x\nf (x, y) = c(x2 \u2212 y2)e\nfind the conditional distribution of y, given\nx = x.\n\n6.43. an insurance company supposes that each per-\nson has an accident parameter and that the yearly\nnumber of accidents of someone whose accident\nparameter is \u03bb is poisson distributed with mean \u03bb.\nthey also suppose that the parameter value of a\nnewly insured person can be assumed to be the\nvalue of a gamma random variable with param-\neters s and \u03b1. if a newly insured person has n\naccidents in her first year, find the conditional den-\nsity of her accident parameter. also, determine the\nexpected number of accidents that she will have in\nthe following year.\n\n6.44. if x1, x2, x3 are independent random variables\nthat are uniformly distributed over (0, 1), com-\npute the probability that the largest of the three\nis greater than the sum of the other two.\n\n6.45. a complex machine is able to operate effectively\nas long as at least 3 of its 5 motors are functioning.\nif each motor independently functions for a ran-\ndom amount of time with density function f (x) =\n\u2212x, x > 0, compute the density function of the\nxe\nlength of time that the machine functions.\n\n6.46. if 3 trucks break down at points randomly dis-\ntributed on a road of length l, find the probability\nthat no 2 of the trucks are within a distance d of\neach other when d \u2026 l/2.\n\n6.47. consider a sample of size 5 from a uniform distri-\nbution over (0, 1). compute the probability that\nthe median is in the interval\n\n.\n\n(cid:29)\n\n(cid:30)\n\n1\n4 , 3\n4\n\n6.48. if x1, x2, x3, x4, x5 are independent and iden-\ntically distributed exponential random variables\nwith the parameter \u03bb, compute\n(a) p{min(x1, . . . , x5) \u2026 a};\n(b) p{max(x1, . . . , x5) \u2026 a}.\n\n6.49. let x(1), x(2), . . . , x(n)\n\nbe the order statistics\nof a set of n independent uniform (0, 1) random\nvariables. find the conditional distribution of x(n)\ngiven that x(1) = s1, x(2) = s2, . . . , x(n\u22121) =\nsn\u22121.\n\n6.50. let z1 and z2 be independent standard normal\nrandom variables. show that x, y has a bivariate\nnormal distribution when x = z1, y = z1 + z2.\n6.51. derive the distribution of the range of a sample of\nsize 2 from a distribution having density function\nf (x) = 2x, 0 < x < 1.\n\n6.52. let x and y denote the coordinates of a point uni-\nformly chosen in the circle of radius 1 centered at\nthe origin. that is, their joint density is\nx2 + y2 \u2026 1\n\nf (x, y) = 1\n\n\u03c0\n\nfind the joint density function of the polar coordi-\nnates r = (x2 + y2)1/2 and \u0001 = tan\n.\n\n6.53. if x and y are independent random variables\nboth uniformly distributed over (0, 1), find the\nx2 + y2, \u03b8 =\njoint density function of r =\ntan\n\n\u22121 y/x.\n\n\u22121 y/x.\n\n6.54. if u is uniform on (0, 2\u03c0 ) and z, independent of\nu, is exponential with rate 1, show directly (with-\nout using the results of example 7b) that x and y\ndefined by\n\n\u221a\n2z cos u\n\u221a\n2z sin u\n\nx =\ny =\n\nare independent standard normal random vari-\nables.\n\n6.55. x and y have joint density function\n\nf (x, y) = 1\nx2y2\n\nx \u00fa 1, y \u00fa 1\n\n(a) compute the joint density function of u =\n\nxy, v = x/y.\n\n(b) what are the marginal densities?\n\n6.56. if x and y are independent and identically dis-\ntributed uniform random variables on (0, 1),\ncompute the joint density of\n(a) u = x + y, v = x/y;\n(b) u = x, v = x/y;\n(c) u = x + y, v = x/(x + y).\n\n6.57. repeat problem 6.56 when x and y are inde-\npendent exponential random variables, each with\nparameter \u03bb = 1.\n\n6.58. if x1 and x2 are independent exponential random\nvariables, each having parameter \u03bb, find the joint\ndensity function of y1 = x1 + x2 and y2 = ex1.\n6.59. if x, y, and z are independent random variables\nhaving identical density functions f (x) = e\n\u2212x, 0 <\nx < q, derive the joint distribution of u = x +\ny, v = x + z, w = y + z.\n\n "}, {"Page_number": 306, "text": "6.60. in example 8b, let yk+1 = n + 1 \u2212 k(cid:9)\n\nyi. show\nthat y1, . . . , yk, yk+1 are exchangeable. note that\nyk+1 is the number of balls one must observe to\nobtain a special ball if one considers the balls in\ntheir reverse order of withdrawal.\n\ni=1\n\ntheoretical exercises 291\n\n6.61. consider an urn containing n balls numbered\n1, . . . , n, and suppose that k of them are randomly\nwithdrawn. let xi equal 1 if ball number i is\nremoved and let xi be 0 otherwise. show that\nx1, . . . , xn are exchangeable.\n\ntheoretical exercises\n\n(cid:9)\n\n6.1. verify equation (1.2).\n6.2. suppose that the number of events occurring in\na given time period is a poisson random vari-\nable with parameter \u03bb. if each event is classi-\nfied as a type i event with probability pi, i =\npi = 1, independently of other events,\n1, . . . , n,\nshow that the numbers of type i events that occur,\ni = 1, . . . , n, are independent poisson random vari-\nables with respective parameters \u03bbpi, i = 1, . . . , n.\n6.3. suggest a procedure for using buffon\u2019s needle\nproblem to estimate \u03c0. surprisingly enough, this\nwas once a common method of evaluating \u03c0.\n6.4. solve buffon\u2019s needle problem when l > d.\n\n(1 \u2212 sin \u03b8 ) + 2\u03b8/\u03c0, where cos \u03b8 =\n\nanswer:\nd/l.\n\n2l\n\u03c0d\n\n6.5. if x and y are independent continuous positive\nrandom variables, express the density function of\n(a) z = x/y and (b) z = xy in terms of the\ndensity functions of x and y. evaluate the density\nfunctions in the special case where x and y are\nboth exponential random variables.\n6.6. if x and y are jointly continuous with joint density\nfunction fx,y (x, y), show that x + y is continuous\nwith density function\nfx+y (t) =\n\nfx,y (x, t \u2212 x) dx\n\n* q\n\n\u2212q\n\n6.7. (a) if x has a gamma distribution with parame-\nters (t, \u03bb), what is the distribution of cx, c > 0?\n\n(b) show that\n\n1\n2\u03bb\n\n\u03c7 2\n2n\n\nhas a gamma distribution with parameters n, \u03bb\nwhen n is a positive integer and \u03c7 2\n2n is a\nchi-squared random variable with 2n degrees\nof freedom.\n\n6.8. let x and y be independent continuous ran-\ndom variables with respective hazard rate func-\ntions \u03bbx (t) and \u03bby (t), and set w = min(x, y).\n(a) determine the distribution function of w in\n\nterms of those of x and y.\n\n(b) show that \u03bbw (t), the hazard rate function of\n\nw, is given by\n\n\u03bbw (t) = \u03bbx (t) + \u03bby (t)\n\n6.9. let x1, . . . , xn be independent exponential ran-\ndom variables having a common parameter \u03bb.\ndetermine the distribution of min(x1, . . . , xn).\n\n6.10. the lifetimes of batteries are independent expo-\nnential random variables, each having parame-\nter \u03bb. a flashlight needs 2 batteries to work. if one\nhas a flashlight and a stockpile of n batteries, what\nis the distribution of time that the flashlight can\noperate?\n\n6.11. let x1, x2, x3, x4, x5 be independent continu-\nous random variables having a common distribu-\ntion function f and density function f , and set\n\ni = p{x1 < x2 < x3 < x4 < x5}\n\n(a) show that i does not depend on f.\nhint: write i as a five-dimensional integral and\nmake the change of variables ui = f(xi), i =\n1, . . . , 5.\n(b) evaluate i.\n(c) give an intuitive explanation for your answer\n\nto (b).\n\n6.12. show that the jointly continuous (discrete) ran-\ndom variables x1, . . . , xn are independent if and\nonly if their joint probability density (mass) func-\ntion f (x1, . . . , xn) can be written as\n\nf (x1, . . . , xn) = n(cid:31)\n\ngi(xi)\n\ni=1\n\nfor nonnegative functions gi(x), i = 1, . . . , n.\n\n6.13. in example 5c we computed the conditional den-\nsity of a success probability for a sequence of tri-\nals when the first n + m trials resulted in n suc-\ncesses. would the conditional density change if\nwe specified which n of these trials resulted in\nsuccesses?\n\n "}, {"Page_number": 307, "text": "292\n\nchapter 6\n\njointly distributed random variables\n\n6.14. suppose that x and y are independent geometric\n\nrandom variables with the same parameter p.\n(a) without any computations, what do you think\n\nis the value of\n\np{x = i|x + y = n}?\n\nhint: imagine that you continually flip a coin hav-\ning probability p of coming up heads. if the second\nhead occurs on the nth flip, what is the probability\nmass function of the time of the first head?\n(b) verify your conjecture in part (a).\n\n6.15. consider a sequence of independent trials, with\neach trial being a success with probability p. given\nthat the kth success occurs on trial n, show that\nall possible outcomes of the first n \u2212 1 trials that\nconsist of k \u2212 1 successes and n \u2212 k failures are\nequally likely.\n\n6.16. if x and y are independent binomial random\nvariables with identical parameters n and p, show\nanalytically that the conditional distribution of x\ngiven that x + y = m is the hypergeometric dis-\ntribution. also, give a second argument that yields\nthe same result without any computations.\nhint: suppose that 2n coins are flipped. let x\ndenote the number of heads in the first n flips and\ny the number in the second n flips. argue that\ngiven a total of m heads, the number of heads in\nthe first n flips has the same distribution as the\nnumber of white balls selected when a sample of\nsize m is chosen from n white and n black balls.\n6.17. suppose that xi, i = 1, 2, 3 are independent pois-\nson random variables with respective means \u03bbi, i =\n1, 2, 3. let x = x1 + x2 and y = x2 + x3.\nthe random vector x, y is said to have a bivari-\nate poisson distribution. find its joint probability\nmass function. that is, find p{x = n, y = m}.\n\n6.18. suppose x and y are both integer-valued random\n\nvariables. let\n\np(i|j) = p(x = i|y = j)\n\nand\n\nshow that\n\nq(j|i) = p(y = j|x = i)\np(x = i, y = j) = p(i|j)(cid:9)\n\np(i|j)\nq(j|i)\n\ni\n\n6.19. let x1, x2, x3 be independent and identically dis-\n\ntributed continuous random variables. compute\n(a) p{x1 > x2|x1 > x3};\n(b) p{x1 > x2|x1 < x3};\n(c) p{x1 > x2|x2 > x3};\n(d) p{x1 > x2|x2 < x3}.\n\n6.20. let u denote a random variable uniformly dis-\ntributed over (0, 1). compute the conditional dis-\ntribution of u given that\n(a) u > a;\n(b) u < a;\nwhere 0 < a < 1.\n\n6.21. suppose that w, the amount of moisture in the\nair on a given day,\nis a gamma random vari-\nable with parameters (t, \u03b2). that is, its density is\nf (w) = \u03b2e\n\u2212\u03b2w(\u03b2w)t\u22121/ \u0001(t), w > 0. suppose also\nthat given that w = w, the number of accidents\nduring that day\u2014call it n\u2014has a poisson distribu-\ntion with mean w. show that the conditional dis-\ntribution of w given that n = n is the gamma\ndistribution with parameters (t + n, \u03b2 + 1).\n\n6.22. let w be a gamma random variable with param-\neters (t, \u03b2), and suppose that conditional on\nw = w, x1, x2, . . . , xn are independent exponen-\ntial random variables with rate w. show that the\nconditional distribution of w given that x1 =\nx1, x2 = x2, . . . , xn = xn is gamma with parame-\nters\n\nt + n, \u03b2 + n(cid:9)\n\n(cid:4)\n\n(cid:5)\n\n.\n\nxi\n\n6.23. a rectangular array of mn numbers arranged in n\nrows, each consisting of m columns, is said to con-\ntain a saddlepoint if there is a number that is both\nthe minimum of its row and the maximum of its\ncolumn. for instance, in the array\n\ni=1\n\n1\n3\n0 \u22122\n12\n.5\n\n2\n6\n3\n\nthe number 1 in the first row, first column is a\nsaddlepoint. the existence of a saddlepoint is of\nsignificance in the theory of games. consider a\nrectangular array of numbers as described previ-\nously and suppose that there are two individuals\u2014\na and b\u2014that are playing the following game: a\nis to choose one of the numbers 1, 2, . . . , n and b\none of the numbers 1, 2, . . . , m. these choices are\nannounced simultaneously, and if a chose i and\nb chose j, then a wins from b the amount spec-\nified by the number in the ith row, jth column of\nthe array. now suppose that the array contains\na saddlepoint\u2014say the number in the row r and\ncolumn k\u2014call this number xrk. now if player a\nchooses row r, then that player can guarantee her-\nself a win of at least xrk (since xrk is the minimum\nnumber in the row r). on the other hand, if player\nb chooses column k, then he can guarantee that he\nwill lose no more than xrk (since xrk is the maxi-\nmum number in the column k). hence, as a has a\nway of playing that guarantees her a win of xrk and\nas b has a way of playing that guarantees he will\nlose no more than xrk, it seems reasonable to take\n\n "}, {"Page_number": 308, "text": "these two strategies as being optimal and declare\nthat the value of the game to player a is xrk.\n\nif the nm numbers in the rectangular array\ndescribed are independently chosen from an\narbitrary continuous distribution, what\nis the\nprobability that the resulting array will contain a\nsaddle-point?\n6.24. if x is exponential with rate \u03bb, find p{[x] = n, x \u2212\n[x] \u2026 x}, where [x] is defined as the largest integer\nless than or equal to x. can you conclude that [x]\nand x \u2212 [x] are independent?\n6.25. suppose that f(x) is a cumulative distribution\nfunction. show that (a) fn(x) and (b) 1 \u2212 [1 \u2212\nf(x)]n are also cumulative distribution functions\nwhen n is a positive integer.\nhint: let x1, . . . , xn be independent random vari-\nables having the common distribution function f.\ndefine random variables y and z in terms of the\nxi so that p{y \u2026 x} = fn(x) and p{z \u2026 x} =\n1 \u2212 [1 \u2212 f(x)]n.\n\n6.26. show that if n people are distributed at random\nalong a road l miles long, then the probability that\nno 2 people are less than a distance d miles apart\nis when d \u2026 l/(n \u2212 1), [1 \u2212 (n \u2212 1)d/l]n. what\nif d > l/(n \u2212 1)?\n\ntion (6.4).\n\n6.27. establish equation (6.2) by differentiating equa-\n6.28. show that the median of a sample of size 2n + 1\nfrom a uniform distribution on (0, 1) has a beta\ndistribution with parameters (n + 1, n + 1).\n\n6.29. verify equation (6.6), which gives the joint density\n\nof x(i) and x(j).\n\n6.30. compute the density of the range of a sample of\nsize n from a continuous distribution having den-\nsity function f .\n6.31. let x(1) \u2026 x(2) \u2026 \u00b7\u00b7\u00b7 \u2026 x(n) be the ordered values\nof n independent uniform (0, 1) random variables.\n\nself-test problems and exercises 293\n\nprove that for 1 \u2026 k \u2026 n + 1,\n\np{x(k) \u2212 x(k\u22121) > t} = (1 \u2212 t)n\n\nwhere x(0) k 0, x(n+1) k t.\n\n6.32. let x1, . . . , xn be a set of independent and identi-\ncally distributed continuous random variables hav-\ning distribution function f, and let x(i), i = 1, . . . , n\ndenote their ordered values. if x, independent of\nthe xi, i = 1, . . . , n, also has distribution f, deter-\nmine\n(a) p{x > x(n)};\n(b) p{x > x(1)};\n(c) p{x(i) < x < x(j)}, 1 \u2026 i < j \u2026 n.\n\n6.33. let x1, . . . , xn be independent and identically\ndistributed random variables having distribution\nfunction f and density f . the quantity m k [x(1) +\nx(n)]/2, defined to be the average of the small-\nest and largest values in x1, . . ., xn, is called the\nmidrange of the sequence. show that its distribu-\ntion function is\nfm(m) = n\n\n[f(2m \u2212 x) \u2212 f(x)]n\u22121f (x) dx\n\n*\n\nm\n\u2212q\n\n6.34. let x1, . . . , xn be independent uniform (0, 1) ran-\ndom variables. let r = x(n) \u2212 x(1) denote the\nrange and m = [x(n) + x(1)]/2 the midrange of\nx1, . . ., xn. compute the joint density function of\nr and m.\n\n6.35. if x and y are independent standard normal ran-\ndom variables, determine the joint density func-\ntion of\n\nu = x v = x\ny\n\nthen use your result to show that x/y has a\ncauchy distribution.\n\nself-test problems and exercises\n\n6.1. each throw of an unfair die lands on each of the\nodd numbers 1, 3, 5 with probability c and on each\nof the even numbers with probability 2c.\n(a) find c.\n(b) suppose that the die is tossed. let x equal\n1 if the result is an even number, and let it\nbe 0 otherwise. also, let y equal 1 if the\nresult is a number greater than three and\nlet it be 0 otherwise. find the joint prob-\nability mass function of x and y. suppose\nnow that 12 independent tosses of the die are\nmade.\n\n(c) find the probability that each of the six out-\n\ncomes occurs exactly twice.\n\n(d) find the probability that 4 of the outcomes\nare either one or two, 4 are either three or\nfour, and 4 are either five or six.\n\n(e) find the probability that at least 8 of the\n\ntosses land on even numbers.\n\n6.2. the joint probability mass function of the random\n\nvariables x, y, z is\np(1, 2, 3) = p(2, 1, 1) = p(2, 2, 1) = p(2, 3, 2) = 1\n4\nfind (a) e[xyz], and (b) e[xy + xz + yz].\n\n6.3. the joint density of x and y is given by\n\nf (x, y) = c(y \u2212 x)e\n\n\u2212y \u2212 y < x < y,\n\n0 < y < q\n\n "}, {"Page_number": 309, "text": "294\n\nchapter 6\n\njointly distributed random variables\n\n(a) find c.\n(b) find the density function of x.\n(c) find the density function of y.\n(d) find e[x].\n(e) find e[y].\n\n6.4. let r = r1 + . . . + rk, where all ri are positive\nintegers. argue that if x1, . . . , xr has a multino-\nmial distribution, then so does y1, . . . , yk where,\nwith r0 = 0,\n\nyi = ri\u22121+ri(cid:6)\n\nj=ri\u22121+1\n\nxj ,\n\ni \u2026 k\n\n(cid:8)\n\ns, y2\n\nthat is, y1 is the sum of the first r1 of the x\nis the sum of the next r2, and so on.\n\n6.5. suppose that x, y, and z are independent random\nvariables that are each equally likely to be either\n1 or 2. find the probability mass function of (a)\nxyz, (b) xy + xz + yz, and (c) x2 + yz.\n\n6.6. let x and y be continuous random variables with\n\njoint density function\n+ cy\n\nf (x, y) =\n\n\u23a7\u23a8\n\u23a9 x\n\n5\n0\n\n0 < x < 1, 1 < y < 5\n\notherwise\n\nwhere c is a constant.\n(a) what is the value of c?\n(b) are x and y independent?\n(c) find p{x + y > 3}.\n\n6.7. the joint density function of x and y is\n\n%\n\nf (x, y) =\n\nxy\n0\n\n0 < x < 1, 0 < y < 2\notherwise\n\n(a) are x and y independent?\n(b) find the density function of x.\n(c) find the density function of y.\n(d) find the joint distribution function.\n(e) find e[y].\n(f) find p{x + y < 1}.\n\n6.8. consider two components and three types of\nshocks. a type 1 shock causes component 1 to fail,\na type 2 shock causes component 2 to fail, and a\ntype 3 shock causes both components 1 and 2 to\nfail. the times until shocks 1, 2, and 3 occur are\nindependent exponential random variables with\nrespective rates \u03bb1, \u03bb2, and \u03bb3. let xi denote the\ntime at which component i fails, i = 1, 2. the\nrandom variables x1, x2 are said to have a joint\nbivariate exponential distribution. find p{x1 >\ns, x2 > t}.\n6.9. consider a directory of classified advertisements\nthat consists of m pages, where m is very large.\nsuppose that the number of advertisements per\n\npage varies and that your only method of finding\nout how many advertisements there are on a spec-\nified page is to count them. in addition, suppose\nthat there are too many pages for it to be feasi-\nble to make a complete count of the total num-\nber of advertisements and that your objective is\nto choose a directory advertisement in such a way\nthat each of them has an equal chance of being\nselected.\n(a) if you randomly choose a page and then\nrandomly choose an advertisement from that\npage, would that satisfy your objective? why\nor why not?\nlet n(i) denote the number of advertise-\nments on page i, i = 1, . . . , m, and suppose\nthat whereas these quantities are unknown,\nwe can assume that they are all less than\nor equal to some specified value n. con-\nsider the following algorithm for choosing an\nadvertisement.\n\nstep 1. choose a page at random. suppose it is\npage x. determine n(x) by counting the\nnumber of advertisements on page x.\n\nstep 2. \u201caccept\u201d page x with probability n(x)/n.\nif page x is accepted, go to step 3. other-\nwise, return to step 1.\n\nstep 3. randomly choose one of the advertise-\n\nments on page x.\n\ncall each pass of the algorithm through step\n1 an iteration. for instance, if the first ran-\ndomly chosen page is rejected and the second\naccepted, than we would have needed 2 itera-\ntions of the algorithm to obtain an advertise-\nment.\n\n(b) what is the probability that a single iteration\nof the algorithm results in the acceptance of\nan advertisement on page i?\n\n(c) what is the probability that a single iteration\nof the algorithm results in the acceptance of\nan advertisement?\n\n(d) what is the probability that the algorithm\ngoes through k iterations, accepting the jth\nadvertisement on page i on the final iteration?\n(e) what is the probability that the jth advertise-\nment on page i is the advertisement obtained\nfrom the algorithm?\n\n(f) what is the expected number of iterations\n\ntaken by the algorithm?\n\n6.10. the \u201crandom\u201d parts of the algorithm in self-test\nproblem 8 can be written in terms of the generated\nvalues of a sequence of independent uniform (0,\n1) random variables, known as random numbers.\nwith [x] defined as the largest integer less than or\nequal to x, the first step can be written as follows:\n\n "}, {"Page_number": 310, "text": "step 1. generate a uniform (0, 1) random variable u.\nlet x = [mu] + 1, and determine the value of\nn(x).\n\n(a) explain why the above is equivalent to step 1\n\nhint: what is the probability mass function of x?\n(b) write the remaining steps of the algorithm in\n\nof problem 8.\n\na similar style.\n\n6.11. let x1, x2, . . . be a sequence of independent uni-\nform (0, 1) random variables. for a fixed con-\nstant c, define the random variable n by\n\nn = min{n : xn > c}\n\nis n independent of xn? that is, does know-\ning the value of the first random variable that is\ngreater than c affect the probability distribution of\nwhen this random variable occurs? give an intu-\nitive explanation for your answer.\n\n6.12. the accompanying dartboard is a square whose\n\nsides are of length 6:\n\n10\n20\n\n30\n\nthe three circles are all centered at the center of\nthe board and are of radii 1, 2, and 3, respectively.\ndarts landing within the circle of radius 1 score 30\npoints, those landing outside this circle, but within\nthe circle of radius 2, are worth 20 points, and\nthose landing outside the circle of radius 2, but\nwithin the circle of radius 3, are worth 10 points.\ndarts that do not land within the circle of radius\n3 do not score any points. assuming that each\ndart that you throw will, independently of what\noccurred on your previous throws, land on a point\nuniformly distributed in the square, find the prob-\nabilities of the accompanying events:\n(a) you score 20 on a throw of the dart.\n(b) you score at least 20 on a throw of the\n\n(c) you score 0 on a throw of the dart.\n(d) the expected value of your score on a throw\n\ndart.\n\nof the dart.\n\nself-test problems and exercises 295\n\nscored by the visiting team is approximately a nor-\nmal random variable with mean 1.5 and variance\n6. in addition, the model supposes that the point\ndifferentials for the four quarters are independent.\nassume that this model is correct.\n(a) what is the probability that the home team\n\nwins?\n\n(b) what is the conditional probability that the\nhome team wins, given that it is behind by 5\npoints at halftime?\n\n(c) what is the conditional probability that the\nhome team wins, given that it is ahead by 5\npoints at the end of the first quarter?\n\n6.14. let n be a geometric random variable with param-\neter p. suppose that the conditional distribution\nof x given that n = n is the gamma distribu-\ntion with parameters n and \u03bb. find the condi-\ntional probability mass function of n given that\nx = x.\n\n6.15. let x and y be independent uniform (0, 1) ran-\ndom variables.\n(a) find the joint density of u = x, v = x + y.\n(b) use the result obtained in part (a) to compute\n\nthe density function of v.\n\n6.16. you and three other people are to place bids for an\nobject, with the high bid winning. if you win, you\nplan to sell the object immediately for 10 thousand\ndollars. how much should you bid to maximize\nyour expected profit if you believe that the bids\nof the others can be regarded as being indepen-\ndent and uniformly distributed between 7 and 11\nthousand dollars?\n\n6.17. find the probability that x1, x2, . . . , xn is a per-\nmutation of 1, 2, . . . , n, when x1, x2, . . . , xn are\nindependent and\n(a) each is equally likely to be any of the values\n(b) each has the probability mass function p{xi =\n\n1, . . . , n;\nj} = pj, j = 1, . . . , n.\n\n6.18. let x1, . . . , xn and y1, . . . , yn be independent ran-\ndom vectors, with each vector being a random\nordering of k ones and n \u2212 k zeroes. that is, their\njoint probability mass functions are\nn(cid:6)\np{x1= i1, . . . , xn= in}= p{y1= i1, . . . , yn= in}\nij = k\n\n(cid:3) , ij = 0, 1,\n\n= 1(cid:2)\n\nn\nk\n\nj=1\n\n(e) both of your first two throws score at least 10.\n(f) your total score after two throws is 30.\n\n6.13. a model proposed for nba basketball supposes\nthat when two teams with roughly the same record\nplay each other, the number of points scored in\na quarter by the home team minus the number\n\nlet\n\nn = n(cid:6)\n\ni=1\n\n|xi \u2212 yi|\n\n "}, {"Page_number": 311, "text": "296\n\nchapter 6\n\njointly distributed random variables\n\ndenote the number of coordinates at which the two\nvectors have different values. also, let m denote\nthe number of values of i for which xi = 1, yi = 0.\n(a) relate n to m.\n(b) what is the distribution of m?\n(c) find e[n].\n(d) find var(n).\n\n\u22176.19. let z1, z2, . . . , zn be independent standard nor-\n\nmal random variables, and let\n\nsj = j(cid:6)\n\ni=1\n\nzi\n\n(a) what is the conditional distribution of sn\ngiven that sk = y, for k = 1, . . . , n?\n(b) show that, for 1 \u2026 k \u2026 n, the conditional\ndistribution of sk given that sn = x is\nnormal with mean xk/n and variance k(n \u2212\nk)/n.\n\n6.20. let x1, x2, . . . be a sequence of independent and\nidentically distributed continuous random vari-\nables. find\n(a) p{x6 > x1|x1 = max(x1, . . . , x5)}\n(b) p{x6 > x2|x1 = max(x1, . . . , x5)}\n\n "}, {"Page_number": 312, "text": "c h a p t e r\n\n7\n\nproperties of expectation\n\n7.1 introduction\n7.2 expectation of sums of random variables\n7.3 moments of the number of events that occur\n7.4 covariance, variance of sums, and correlations\n7.5 conditional expectation\n7.6 conditional expectation and prediction\n7.7 moment generating functions\n7.8 additional properties of normal random variables\n7.9 general definition of expectation\n\n7.1 introduction\n\nin this chapter, we develop and exploit additional properties of expected values. to\nbegin, recall that the expected value of the random variable x is defined by\n\nwhere x is a discrete random variable with probability mass function p(x), and by\n\n(cid:6)\n\nxp(x)\n\ne[x] =\n* q\n\nx\n\ne[x] =\n\nxf (x) dx\n\n\u2212q\n\nwhen x is a continuous random variable with probability density function f(x).\n\nsince e[x] is a weighted average of the possible values of x, it follows that if x\n\nmust lie between a and b, then so must its expected value. that is, if\n\nthen\n\np{a \u2026 x \u2026 b} = 1\n\na \u2026 e[x] \u2026 b\n\nto verify the preceding statement, suppose that x is a discrete random variable for\nwhich p{a \u2026 x \u2026 b} = 1. since this implies that p(x) = 0 for all x outside of the\ninterval [a, b], it follows that\n\n(cid:6)\n(cid:6)\n\nx:p(x)>0\n\ne[x] =\n\n\u00fa\n\nxp(x)\n\nap(x)\n\nx:p(x)>0\n\n297\n\n "}, {"Page_number": 313, "text": "298\n\nchapter 7\n\nproperties of expectation\n\n(cid:6)\n\np(x)\n\nx:p(x)>0\n\n= a\n= a\n\nin the same manner, it can be shown that e[x] \u2026 b, so the result follows for discrete\nrandom variables. as the proof in the continuous case is similar, the result follows.\n\n7.2 expectation of sums of random variables\n\nfor a two-dimensional analog of propositions 4.1 of chapter 4 and 2.1 of chapter 5,\nwhich give the computational formulas for the expected value of a function of a ran-\ndom variable, suppose that x and y are random variables and g is a function of two\nvariables. then we have the following result.\nproposition 2.1. if x and y have a joint probability mass function p(x,y), then\n\nlet us give a proof of proposition 2.1 when the random variables x and y are\njointly continuous with joint density function f (x, y) and when g(x, y) is a nonneg-\native random variable. because g(x, y) \u00fa 0, we have, by lemma 2.1 of chapter 5,\nthat\n\nif x and y have a joint probability density function f(x,y), then\n\n(cid:6)\n\n(cid:6)\n\ng(x, y)p(x, y)\n\nx\n\ne[g(x, y)] =\n* q\n\ny\n\n* q\n\ne[g(x, y)] =\n\ng(x, y)f (x, y) dx dy\n\n\u2212q\n\n\u2212q\n\ne[g(x, y)] =\n\n* q\n* *\np{g(x, y) > t} =\n* q\n* *\n\n0\n\ne[g(x, y)] =\n\np{g(x, y) > t} dt\n\nf (x, y) dy dx\n\n(x,y):g(x,y)>t\n\n0\n\n(x,y):g(x,y)>t\n\nf (x, y) dy dx dt\n\n*\n\n*\n*\n\n*\n*\n\nx\n\ny\n\nx\n\ny\n\ne[g(x, y) =\n=\n\nf (x, y) dt dy dx\n\ng(x,y)\nt=0\ng(x, y) f (x, y) dy dx\n\nwriting\n\nshows that\n\ninterchanging the order of integration gives\n\nthus, the result is proven when g(x, y) is a nonnegative random variable. the gen-\neral case then follows as in the one-dimensional case. (see theoretical exercises 2\nand 3 of chapter 5.)\n\nexample 2a\nan accident occurs at a point x that is uniformly distributed on a road of length l.\nat the time of the accident, an ambulance is at a location y that is also uniformly\n\n "}, {"Page_number": 314, "text": "section 7.2\n\nexpectation of sums of random variables 299\n\ndistributed on the road. assuming that x and y are independent, find the expected\ndistance between the ambulance and the point of the accident.\nsolution. we need to compute e[|x \u2212 y|]. since the joint density function of x and\ny is\n\nf (x, y) = 1\nl2 ,\nit follows from proposition 2.1 that\n\nnow,\n\n*\n\n0\n\nl\n\ntherefore,\n\ne[|x \u2212 y|] = 1\nl2\n*\n\n|x \u2212 y|dy =\n\nx\n\n0 < x < l,\n\n0 < y < l\n\n*\n\n*\n\nl\n\nl\n\n0\n\n0\n\n|x \u2212 y| dy dx\n*\n\nl\n\n(y \u2212 x)dy\n\u2212 x(l \u2212 x)\n\nx\n\n0\n= x2\n2\n= l2\n2\n\n(x \u2212 y)dy +\n+ l2\n\u2212 x2\n2\n2\n+ x2 \u2212 xl\n(cid:4)\n*\n\nl\n\n0\n\nl2\n2\n\n(cid:5)\n+ x2 \u2212 xl\n\ndx\n\n.\n\nfor an important application of proposition 2.1, suppose that e[x] and e[y] are\n\nboth finite and let g(x, y) = x + y. then, in the continuous case,\n\ne[|x \u2212 y|] = 1\nl2\n= l\n3\n\n\u2212q\n\n\u2212q\n\n* q\n* q\n\n* q\n* q\n* q\n=\n= e[x] + e[y]\n\n\u2212q\nxfx (x) dx +\n\n\u2212q\n\n\u2212q\n\ne[x + y] =\n=\n\n* q\n(x + y)f (x, y) dx dy\n* q\nxf (x, y) dy dx +\n\n\u2212q\n\n* q\n\n\u2212q\n\nyf (x, y) dx dy\n\nyfy (y) dy\n\n\u2212q\n\nthe same result holds in general; thus, whenever e[x] and e[y] are finite,\n\ne[x + y] = e[x] + e[y]\n\n(2.1)\n\nexample 2b\nsuppose that, for random variables x and y,\nx \u00fa y\n\nthat is, for any outcome of the probability experiment, the value of the random vari-\nable x is greater than or equal to the value of the random variable y. since x \u00fa y is\nequivalent to the inequality x \u2212 y \u00fa 0, it follows that e[x \u2212 y] \u00fa 0, or, equivalently,\n.\n\ne[x] \u00fa e[y]\n\n "}, {"Page_number": 315, "text": "300\n\nchapter 7\n\nproperties of expectation\n\nusing equation (2.1), we may show by a simple induction proof that if e[xi] is\n\nfinite for all i = 1, . . . , n, then\n\ne[x1 + \u00b7\u00b7\u00b7 + xn] = e[x1] + \u00b7\u00b7\u00b7 + e[xn]\n\n(2.2)\n\nequation (2.2) is an extremely useful formula whose utility will now be illustrated by\na series of examples.\n\nexample 2c the sample mean\nlet x1, . . . , xn be independent and identically distributed random variables having\ndistribution function f and expected value \u03bc. such a sequence of random variables is\nsaid to constitute a sample from the distribution f. the quantity\n\nx = n(cid:6)\n\ni=1\n\nxi\nn\n\nis called the sample mean. compute e[x].\n\nsolution.\n\ne[x] = e\n\n= 1\nn\n\n\u23a4\n\u23a6\n\u23a4\n\u23a6\n\nxi\n\nxi\nn\n\n\u23a1\n\u23a3 n(cid:6)\n\u23a1\n\u23a3 n(cid:6)\nn(cid:6)\n\ni=1\n\ni=1\n\ne\n\ne[xi]\n\n= 1\nn\n= \u03bc since e[xi] k \u03bc\n\ni=1\n\nthat is, the expected value of the sample mean is \u03bc, the mean of the distribution.\nwhen the distribution mean \u03bc is unknown, the sample mean is often used in statistics\n.\nto estimate it.\n\nexample 2d boole\u2019s inequality\nlet a1, . . . , an denote events, and define the indicator variables xi, i = 1, . . . , n, by\n\nxi =\n\nlet\n\n%\n\n1\n0\n\nif ai occurs\notherwise\n\nx = n(cid:6)\n%\n\ni=1\n\nxi\n\ny =\n\n1\n0\n\nif x \u00fa 1\notherwise\n\nso x denotes the number of the events ai that occur. finally, let\n\n "}, {"Page_number": 316, "text": "section 7.2\n\nexpectation of sums of random variables 301\n\nso y is equal to 1 if at least one of the ai occurs and is 0 otherwise. now, it is imme-\ndiate that\n\nx \u00fa y\n\nso\n\nbut since\n\nand\n\ne[x] \u00fa e[y]\n\ne[x] = n(cid:6)\n\ne[xi] = n(cid:6)\n\ni=1\n\np(ai)\n\ni=1\n\ne[y] = p{at least one of the ai occur} = p\n\nwe obtain boole\u2019s inequality, namely,\n\n\u239b\n\u239d n(cid:14)\n\ni=1\n\n\u239e\n\u23a0 \u2026\n\nn(cid:6)\n\ni=1\n\np(ai)\n\np\n\nai\n\n\u239e\n\u23a0\n\nai\n\n\u239b\n\u239d n(cid:14)\n\ni=1\n\n.\n\nthe next three examples show how equation (2.2) can be used to calculate the\nexpected value of binomial, negative binomial, and hypergeometric random vari-\nables. these derivations should be compared with those presented in chapter 4.\n\nexample 2e expectation of a binomial random variable\nlet x be a binomial random variable with parameters n and p. recalling that such\na random variable represents the number of successes in n independent trials when\neach trial has probability p of being a success, we have that\n\nwhere\n\nx = x1 + x2 + \u00b7\u00b7\u00b7 + xn\n%\n\nxi =\n\n1\n0\n\nif the ith trial is a success\nif the ith trial is a failure\n\nhence, xi is a bernoulli random variable having expectation e[xi] = 1(p) + 0(1 \u2212\np). thus,\n.\n\ne[x] = e[x1] + e[x2] + \u00b7\u00b7\u00b7 + e[xn] = np\n\nexample 2f mean of a negative binomial random variable\nif independent trials having a constant probability p of being successes are performed,\ndetermine the expected number of trials required to amass a total of r successes.\n\nsolution. if x denotes the number of trials needed to amass a total of r successes,\nthen x is a negative binomial random variable that can be represented by\n\nx = x1 + x2 + \u00b7\u00b7\u00b7 + xr\n\nwhere x1 is the number of trials required to obtain the first success, x2 the number\nof additional trials until the second success is obtained, x3 the number of additional\n\n "}, {"Page_number": 317, "text": "302\n\nchapter 7\n\nproperties of expectation\n\ntrials until the third success is obtained, and so on. that is, xi represents the num-\nber of additional trials required after the (i \u2212 1)st success until a total of i successes\nis amassed. a little thought reveals that each of the random variables xi is a geo-\nmetric random variable with parameter p. hence, from the results of example 8b of\nchapter 4, e[xi] = 1/p, i = 1, 2, . . . , r; thus,\n\ne[x] = e[x1] + \u00b7\u00b7\u00b7 + e[xr] = r\np\n\n.\n\nexample 2g mean of a hypergeometric random variable\nif n balls are randomly selected from an urn containing n balls of which m are white,\nfind the expected number of white balls selected.\n\nsolution. let x denote the number of white balls selected, and represent x as\n\nwhere\n\nnow\n\nhence,\n\n%\n\n1\n0\n\nxi =\n\nx = x1 + \u00b7\u00b7\u00b7 + xm\n\nif the ith white ball is selected\notherwise\n\n(cid:2)\n= p{ith white ball is selected}\n\n(cid:3)\n\ne[xi] = p{xi = 1}\n(cid:3)(cid:2)\n(cid:2)\n\n1\n1\n\n=\n\nn \u2212 1\n(cid:3)\nn \u2212 1\nn\nn\n\n= n\nn\n\ne[x] = e[x1] + \u00b7\u00b7\u00b7 + e[xm] = mn\nn\n\nwe could also have obtained the preceding result by using the alternative represen-\ntation\n\nx = y1 + \u00b7\u00b7\u00b7 + yn\n\nwhere\n\nyi =\n\nif the ith ball selected is white\notherwise\n\n%\n\n1\n0\n\nsince the ith ball selected is equally likely to be any of the n balls, it follows that\n\ne[yi] = m\nn\n\nso\n\ne[x] = e[y1] + \u00b7\u00b7\u00b7 + e[yn] = nm\nn\n\n.\n\n "}, {"Page_number": 318, "text": "section 7.2\n\nexpectation of sums of random variables 303\n\nexample 2h expected number of matches\nsuppose that n people throw their hats into the center of a room. the hats are mixed\nup, and each person randomly selects one. find the expected number of people that\nselect their own hat.\n\nsolution. letting x denote the number of matches, we can compute e[x] most eas-\nily by writing\n\nx = x1 + x2 + \u00b7\u00b7\u00b7 + xn\n\nwhere\n\nxi =\n\nif the ith person selects his own hat\notherwise\n\n%\n\n1\n0\n\nsince, for each i, the ith person is equally likely to select any of the n hats,\n\nthus,\n\ne[xi] = p{xi = 1} = 1\n(cid:2)\nn\n\ne[x] = e[x1] + \u00b7\u00b7\u00b7 + e[xn] =\n\n(cid:3)\n\n1\nn\n\nn = 1\n\nhence, on the average, exactly one person selects his own hat.\n\n.\n\nexample 2i coupon-collecting problems\nsuppose that there are n different types of coupons, and each time one obtains a\ncoupon, it is equally likely to be any one of the n types. find the expected number of\ncoupons one need amass before obtaining a complete set of at least one of each type.\n\nsolution. let x denote the number of coupons collected before a complete set is\nattained. we compute e[x] by using the same technique we used in computing the\nmean of a negative binomial random variable (example 2f). that is, we define xi, i =\n0, 1, . . . , n \u2212 1 to be the number of additional coupons that need be obtained after\ni distinct types have been collected in order to obtain another distinct type, and we\nnote that\n\nx = x0 + x1 + \u00b7\u00b7\u00b7 + xn\u22121\n\nwhen i distinct types of coupons have already been collected, a new coupon obtained\nwill be of a distinct type with probability (n \u2212 i)/n. therefore,\n\n(cid:2)\n\n(cid:3)k\u22121\n\np{xi = k} = n \u2212 i\n\nn\n\ni\nn\n\nk \u00fa 1\n\nor, in other words, xi is a geometric random variable with parameter (n \u2212 i)/n.\n\nhence,\n\ne[xi] = n\nn \u2212 i\n\nimplying that\n\nn\n\n(cid:20)\ne[x] = 1 +\n+\nn \u2212 1\n= n\n1 + \u00b7\u00b7\u00b7 +\n\nn\n\nn \u2212 2\n1\nn \u2212 1\n\n(cid:21)\n\n+ \u00b7\u00b7\u00b7 + n\n1\n+ 1\nn\n\n.\n\n "}, {"Page_number": 319, "text": "304\n\nchapter 7\n\nproperties of expectation\n\nexample 2j\nten hunters are waiting for ducks to fly by. when a flock of ducks flies overhead, the\nhunters fire at the same time, but each chooses his target at random, independently\nof the others. if each hunter independently hits his target with probability p, com-\npute the expected number of ducks that escape unhurt when a flock of size 10 flies\noverhead.\nsolution. let xi equal 1 if the ith duck escapes unhurt and 0 otherwise, for i = 1,\n2, . . . , 10. the expected number of ducks to escape can be expressed as\n\ne[x1 + \u00b7\u00b7\u00b7 + x10] = e[x1] + \u00b7\u00b7\u00b7 + e[x10]\n\nto compute e[xi] = p{xi = 1}, we note that each of the hunters will, independently,\nhit the ith duck with probability p/10, so\n\n(cid:2)\n\n(cid:3)10\n1 \u2212 p\n10\n(cid:3)10\n\n1 \u2212 p\n10\n\np{xi = 1} =\n(cid:2)\n\ne[x] = 10\n\nhence,\n\n.\n\nexample 2k expected number of runs\nsuppose that a sequence of n 1\u2019s and m 0\u2019s is randomly permuted so that each of the\n(n + m)!/(n!m!) possible arrangements is equally likely. any consecutive string of 1\u2019s\nis said to constitute a run of 1\u2019s\u2014for instance, if n = 6, m = 4, and the ordering is 1,\n1, 1, 0, 1, 1, 0, 0, 1, 0, then there are 3 runs of 1\u2019s\u2014and we are interested in computing\nthe mean number of such runs. to compute this quantity, let\n\n%\n\n1\n0\n\nii =\n\nif a run of 1\u2019s starts at the ith position\notherwise\n\ntherefore, r(1), the number of runs of 1, can be expressed as\n\nand it follows that\n\nnow,\n\nr(1) = n+m(cid:6)\ne[r(1)] = n+m(cid:6)\n\ni=1\n\ni=1\n\nii\n\ne[ii]\n\ne[i1] = p{\u201c1\u201d in position 1}\n\n=\n\nn\n\nn + m\n\n "}, {"Page_number": 320, "text": "section 7.2\n\nexpectation of sums of random variables 305\n\nand for 1 < i \u2026 n + m,\n\ne[ii] = p{\u201c0\u201d in position i \u2212 1, \u201c1\u201d in position i}\n\n= m\n\nn + m\n\nn\n\nn + m \u2212 1\n\nhence,\n\ne[r(1)] =\n\nn\n\nn + m\n\n+ (n + m \u2212 1)\n\nnm\n\n(n + m)(n + m \u2212 1)\n\nsimilarly, e[r(0)], the expected number of runs of 0\u2019s, is\n+ nm\nn + m\n\ne[r(0)] = m\n\nn + m\n\nand the expected number of runs of either type is\n\ne[r(1) + r(0)] = 1 + 2nm\nn + m\n\n.\n\nexample 2l a random walk in the plane\nconsider a particle initially located at a given point in the plane, and suppose that it\nundergoes a sequence of steps of fixed length, but in a completely random direction.\nspecifically, suppose that the new position after each step is one unit of distance from\nthe previous position and at an angle of orientation from the previous position that is\nuniformly distributed over (0, 2\u03c0). (see figure 7.1.) compute the expected square of\nthe distance from the origin after n steps.\n\n2\n\n1\n\n\u242a2\n\n1\n\n1\n\n\u242a\n1\n\n0\n\n0 = initial position\n1 = position after first step\n2 = position after second step\n\nfigure 7.1\n\nsolution. letting (xi, yi) denote the change in position at the ith step, i = 1, . . . , n,\nin rectangular coordinates, we have\n\nxi = cos \u03b8i\nyi = sin \u03b8i\n\n "}, {"Page_number": 321, "text": "306\n\nchapter 7\n\nproperties of expectation\n(cid:5)\nwhere \u03b8i, i = 1, . . . , n, are, by assumption, independent uniform (0, 2\u03c0) random vari-\nables. because the position after n steps has rectangular coordinates\n,\nyi\n\ni=1\nit follows that d2, the square of the distance from the origin, is given by\n\nn(cid:9)\n\nn(cid:9)\n\n(cid:4)\n\ni=1\n\nxi,\n\n\u239b\n\u239e\n\u239d n(cid:6)\n\u23a02\n(cid:6)(cid:6)\n\ni=1\n) +\n\nyi\n\nd2 =\n\n+\n\nxi\n\n\u239b\n\u239d n(cid:6)\n= n(cid:6)\n\ni=1\n\ni=1\n= n +\n\n\u239e\n\u23a02\n\nizj\n\n(x2\ni\n\n+ y2\n(cid:6)(cid:6)\n\ni\n\n(xixj + yiyj)\n(cos \u03b8i cos \u03b8j + sin \u03b8i sin \u03b8j)\n\nizj\n\nwhere cos2 \u03b8i + sin2 \u03b8i = 1. taking expectations and using the independence of \u03b8i and\n\u03b8j when i z j and the fact that\n2\u03c0e[cos \u03b8i] =\n2\u03c0e[sin \u03b8i] =\n\ncos u du = sin 2\u03c0 \u2212 sin 0 = 0\nsin u du = cos 0 \u2212 cos 2\u03c0 = 0\n\n*\n*\n\n2\u03c0\n\n2\u03c0\n\n0\n\n0\n\nwe arrive at\n\ne[d2] = n\n\n.\n\nexample 2m analyzing the quick-sort algorithm\nsuppose that we are presented with a set of n distinct values x1, x2, . . . , xn and that\nwe desire to put them in increasing order, or as it is commonly stated, to sort them.\nan efficient procedure for accomplishing this task is the quick-sort algorithm, which\nis defined as follows. when n = 2, the algorithm compares the two values and then\nputs them in the appropriate order. when n > 2, one of the elements is randomly\nchosen\u2014say it is xi\u2014and then all of the other values are compared with xi. those\nsmaller than xi are put in a bracket to the left of xi and those larger than xi are put\nin a bracket to the right of xi. the algorithm then repeats itself on these brackets and\ncontinues until all values have been sorted. for instance, suppose that we desire to\nsort the following 10 distinct values:\n\n5, 9, 3, 10, 11, 14, 8, 4, 17, 6\n\nwe start by choosing one of them at random (that is, each value has probability 1\n10 of\nbeing chosen). suppose, for instance, that the value 10 is chosen. we then compare\neach of the others to this value, putting in a bracket to the left of 10 all those values\nsmaller than 10 and to the right all those larger. this gives\n{5, 9, 3, 8, 4, 6}, 10,{11, 14, 17}\n\nwe now focus on a bracketed set that contains more than a single value\u2014say the one\non the left of the preceding\u2014and randomly choose one of its values\u2014say that 6 is\nchosen. comparing each of the values in the bracket with 6 and putting the smaller\n\n "}, {"Page_number": 322, "text": "section 7.2\n\nexpectation of sums of random variables 307\n\nones in a new bracket to the left of 6 and the larger ones in a bracket to the right\nof 6 gives\n\n{5, 3, 4}, 6,{9, 8}, 10,{11, 14, 17}\n\nif we now consider the leftmost bracket, and randomly choose the value 4 for com-\nparison then the next iteration yields\n\n{3}, 4,{5}, 6,{9, 8}, 10,{11, 14, 17}\n\nthis continues until there is no bracketed set that contains more than a single value.\nif we let x denote the number of comparisons that it takes the quick-sort algorithm\nto sort n distinct numbers, then e[x] is a measure of the effectiveness of this algo-\nrithm. to compute e[x], we will first express x as a sum of other random variables\nas follows. to begin, give the following names to the values that are to be sorted:\nlet 1 stand for the smallest, let 2 stand for the next smallest, and so on. then, for\n1 \u2026 i < j \u2026 n, let i(i, j) equal 1 if i and j are ever directly compared, and let it equal\n0 otherwise. with this definition, it follows that\n\nimplying that\n\ne[x] = e\n\ni(i, j)\n\nn(cid:6)\n\nj=i+1\n\n\u23a4\n\u23a5\u23a6\n\ni(i, j)\n\ni=1\n\nx = n\u22121(cid:6)\n\u23a1\nn(cid:6)\n\u23a2\u23a3n\u22121(cid:6)\nn(cid:6)\n= n\u22121(cid:6)\n= n\u22121(cid:6)\nn(cid:6)\n\nj=i+1\n\nj=i+1\n\ni=1\n\ni=1\n\ni=1\n\nj=i+1\n\ne[i(i, j)]\n\np{i and j are ever compared}\n\nto determine the probability that i and j are ever compared, note that the values\ni, i + 1, . . . , j \u2212 1, j will initially be in the same bracket (since all values are initially\nin the same bracket) and will remain in the same bracket if the number chosen for\nthe first comparison is not between i and j. for instance, if the comparison number is\nlarger than j, then all the values i, i + 1, . . . , j \u2212 1, j will go in a bracket to the left of\nthe comparison number, and if it is smaller than i, then they will all go in a bracket\nto the right. thus all the values i, i + 1, . . . , j \u2212 1, j will remain in the same bracket\nuntil the first time that one of them is chosen as a comparison value. at that point all\nthe other values between i and j will be compared with this comparison value. now,\nif this comparison value is neither i nor j, then upon comparison with it, i will go into\na left bracket and j into a right bracket, and thus i and j will be in different brackets\nand so will never be compared. on the other hand, if the comparison value of the set\ni, i + 1, . . . , j \u2212 1, j is either i or j, then there will be a direct comparison between\ni and j. now, given that the comparison value is one of the values between i and j,\nit follows that it is equally likely to be any of these j \u2212 i + 1 values, and thus the\nprobability that it is either i or j is 2/(j \u2212 i + 1). therefore, we can conclude that\n\np{i and j are ever compared} =\n\n2\n\nj \u2212 i + 1\n\n "}, {"Page_number": 323, "text": "308\n\nchapter 7\n\nproperties of expectation\n\nand\n\ne[x] = n\u22121(cid:6)\n\ni=1\n\nn(cid:6)\n\nj=i+1\n\n2\n\nj \u2212 i + 1\n\nto obtain a rough approximation of the magnitude of e[x] when n is large, we can\napproximate the sums by integrals. now\n\n*\n\nn(cid:6)\n\nj=i+1\n\n2\n\nj \u2212 i + 1\n\nthus\n\ne[x] l\n\n2\n\nn\ni+1\n\nl\n\ni+1\n\n66n\nx \u2212 i + 1\ndx\n= 2 log(x \u2212 i + 1)\n= 2 log(n \u2212 i + 1) \u2212 2 log(2)\nl 2 log(n \u2212 i + 1)\nn\u22121(cid:6)\n*\n*\n\n2 log(n \u2212 i + 1)\nn\u22121\n\nlog(n \u2212 x + 1) dx\n\ni=1\n\n1\nn\n\nl 2\n= 2\n= 2(y log(y) \u2212 y)|n\nl 2n log(n)\n\nlog(y)dy\n\n2\n\n2\n\nthus we see that when n is large, the quick-sort algorithm requires, on average,\n.\napproximately 2n log(n) comparisons to sort n distinct values.\n\nexample 2n the probability of a union of events\nlet a1, . . . an denote events, and define the indicator variables xi, i = 1, . . . , n, by\n\nnow, note that\n\nhence,\n\n%\n\n1\n0\n\nxi =\n\nif ai occurs\notherwise\n\n%\n\n1\n0\n\nif \u222a ai occurs\notherwise\n\n(1 \u2212 xi) =\n\ni=1\n\n1 \u2212 n(cid:31)\n\u23a1\n\u23a31 \u2212 n(cid:31)\n\ne\n\ni=1\n\n\u23a4\n\u23a6 = p\n\n\u239b\n\u239d n(cid:14)\n\ni=1\n\n\u239e\n\u23a0\n\nai\n\n(1 \u2212 xi)\n\n "}, {"Page_number": 324, "text": "p\n\nhowever,\n\nso\n\nsection 7.2\n\nexpectation of sums of random variables 309\n\nexpanding the left side of the preceding formula yields\n\n\u239e\n\u23a0 = e\n\nai\n\n\u239b\n\u239d n(cid:14)\n\ni=1\n\n\u23a1\n\u23a2\u23a3 n(cid:6)\n\ni=1\n\n(cid:6)(cid:6)\n\ni<j\n\nxi \u2212\n\n(cid:6)(cid:6)(cid:6)\n\nxixj +\n\u23a4\n\u23a6\n\nxixjxk\n\ni<j<k\n\n(2.3)\n\n\u2212\u00b7\u00b7\u00b7 + (\u22121)n+1x1 \u00b7\u00b7\u00b7 xn\n\n0\n\n1\n0\n\nxi1xi2\n\n\u00b7\u00b7\u00b7 xik\n\n=\n\n\u00b7\u00b7\u00b7 aik occurs\n\nif ai1ai2\notherwise\n\ne[xi1\n\n\u00b7\u00b7\u00b7 xik] = p(ai1\n(cid:6)(cid:6)\n\n)\n\n\u00b7\u00b7\u00b7 aik\n(cid:6)(cid:6)(cid:6)\n\n(cid:6)\n\np(ai) \u2212\n\np(aiaj) +\n\u2212 \u00b7\u00b7\u00b7 + (\u22121)n+1p(a1 \u00b7\u00b7\u00b7 an)\n\ni<j\n\ni\n\np(aiajak)\n\ni<j<k\n\np(\u222aai) =\n\nthus, equation (2.3) is just a statement of the well-known formula for the union of\nevents:\n\n.\nwhen one is dealing with an infinite collection of random variables xi, i \u00fa 1, each\n\nhaving a finite expectation, it is not necessarily true that\n\nto determine when (2.4) is valid, we note that\n\n(2.4)\n\nn(cid:9)\n\ni=1\n\nxi. thus,\n\n\u23a1\n\u23a3 q(cid:6)\n\ni=1\n\ne\n\n\u23a1\n\u23a3 q(cid:6)\n\ni=1\n\ne\n\nxi\n\n\u23a4\n\u23a6 =\n\nxi\n\ni=1\n\ni=1\n\ne[xi]\n\nq(cid:6)\nq(cid:9)\nxi = lim\n\u23a4\n\u23a1\nn\u2192q\nn(cid:6)\n\u23a6\n\u23a3 lim\n\u23a1\n\u23a4\nn\u2192q\n\u23a3 n(cid:6)\n\u23a6\n?= lim\nn\u2192q e\nn(cid:6)\n\n\u23a4\n\u23a6 = e\n\ni=1\n\ni=1\n\nxi\n\nxi\n\ne[xi]\n\ni=1\n\n= lim\nn\u2192q\nq(cid:6)\n\n=\n\ne[xi]\n\ni=1\n\n(2.5)\n\nhence, equation (2.4) is valid whenever we are justified in interchanging the expec-\ntation and limit operations in equation (2.5). although, in general, this interchange\nis not justified, it can be shown to be valid in two important special cases:\nq(cid:9)\n1. the xi are all nonnegative random variables. (that is, p{xi \u00fa 0} = 1 for all i.)\n2.\n\ne[|xi|] < q.\n\ni=1\n\n "}, {"Page_number": 325, "text": "310\n\nchapter 7\n\nproperties of expectation\n\nexample 2o\nconsider any nonnegative, integer-valued random variable x. if, for each i \u00fa 1, we\ndefine\n\n%\n\nxi =\n\n1\n0\n\nif x \u00fa i\nif x < i\n\nthen\n\nq(cid:6)\n\ni=1\n\nxi = x(cid:6)\n= x(cid:6)\n\ni=1\n\ni=1\n= x\n\nq(cid:6)\nq(cid:6)\n\ni=x+1\n\nxi\n\n0\n\ni=x+1\n\nxi +\n\n1 +\n\nq(cid:6)\nq(cid:6)\n\ni=1\n\ni=1\n\nhence, since the xi are all nonnegative, we obtain\n\ne[x] =\n\n=\n\na useful identity.\n\ne(xi)\n\np{x \u00fa i}\n\n(2.6)\n\n.\n\nexample 2p\nsuppose that n elements\u2014call them 1, 2, . . ., n\u2014must be stored in a computer in the\nform of an ordered list. each unit of time, a request will be made for one of these\nelements\u2014i being requested, independently of the past, with probability p(i), i \u00fa 1,\np(i) = 1. assuming that these probabilities are known, what ordering minimizes\n\n(cid:9)\n\ni\nthe average position in the line of the element requested?\nsolution. suppose that the elements are numbered so that p(1) \u00fa p(2) \u00fa \u00b7\u00b7\u00b7 \u00fa p(n).\nto show that 1, 2, . . ., n is the optimal ordering, let x denote the position of the\nrequested element. now, under any ordering\u2014say, o = i1, i2, . . . , in,\n\npo{x \u00fa k} = n(cid:6)\nn(cid:6)\n\nj=k\n\np(ij)\n\np(j)\n\n\u00fa\n= p1,2, . . . , n{x \u00fa k}\n\nj=k\n\nsumming over k and using equation (2.6) yields\n\neo[x] \u00fa e1,2, . . . , n[x]\n\nthus showing that ordering the elements in decreasing order of the probability that\n.\nthey are requested minimizes the expected position of the element requested.\n\n "}, {"Page_number": 326, "text": "section 7.2\n\nexpectation of sums of random variables 311\n\n\u22177.2.1 obtaining bounds from expectations via the probabilistic method\nthe probabilistic method is a technique for analyzing the properties of the elements\nof a set by introducing probabilities on the set and then studying an element chosen\naccording to those probabilities. the technique was previously seen in example 4l of\nchapter 3, where it was used to show that a set contained an element that satisfied a\ncertain property. in this subsection, we show how it can sometimes be used to bound\ncomplicated functions.\nlet f be a function on the elements of a finite set s, and suppose that we are\n\ninterested in\n\nm = max\n\ns\u2208s f (s)\n\na useful lower bound for m can often be obtained by letting s be a random element\nof s for which the expected value of f (s) is computable and then noting that m \u00fa f (s)\nimplies that\n\nm \u00fa e[f (s)]\n\nwith strict inequality if f (s) is not a constant random variable. that is, e[f (s)] is a\nlower bound on the maximum value.\n\n(cid:2)\n\n(cid:3)\n\nn\n2\n\nexample 2q the maximum number of hamiltonian paths in a tournament\na round-robin tournament of n > 2 contestants is a tournament in which each of the\npair of contestants play each other exactly once. suppose that the players are\nnumbered 1, 2, 3, . . . , n. the permutation i1, i2, . . . in is said to be a hamiltonian path if\ni1 beats i2, i2 beats i3, . . ., and in\u22121 beats in. a problem of some interest is to determine\nthe largest possible number of hamiltonian paths.\n\nas an illustration, suppose that there are 3 players. on the one hand, one of them\nwins twice, then there is a single hamiltonian path. (for instance, if 1 wins twice and\n2 beats 3, then the only hamiltonian path is 1, 2, 3.) on the other hand, if each of\nthe players wins once, than there are 3 hamiltonian paths. (for instance, if 1 beats 2,\n2 beats 3, and 3 beats 1, then 1, 2, 3; 2, 3, 1; and 3, 1, 2, are all hamiltonians). hence,\nwhen n = 3, there is a maximum of 3 hamiltonian paths.\nwe now show that there is an outcome of the tournament that results in more than\nn!/2n\u22121 hamiltonian paths. to begin, let the outcome of the tournament specify the\n\n(cid:4)\ngames played, and let s denote the set of all 2\n\npossible\nresult of each of the\ntournament outcomes. then, with f (s) defined as the number of hamiltonian paths\nthat result when the outcome is s \u2208 s, we are asked to show that\n\n(cid:2)\n\n(cid:3)\n\n(cid:5)\n\nn\n2\n\nn\n2\n\nmax\n\ns\n\nf (s) \u00fa n!\n2n\u22121\n\n(cid:2)\n\n(cid:3)\n\nn\n2\n\nto show this, consider the randomly chosen outcome s that is obtained when the\nresults of the\ngames are independent, with each contestant being equally likely\nto win each encounter. to determine e[f (s)], the expected number of hamiltonian\npaths that result from the outcome s, number the n! permutations, and, for i =\n1, . . . , n!, let\n\n%\n\nxi =\n\n1,\n0,\n\nif permutation i is a hamiltonian\notherwise\n\n "}, {"Page_number": 327, "text": "312\n\nchapter 7\n\nproperties of expectation\n\nsince\n\nit follows that\n\nf (s) =\n\ne[f (s)] =\n\n(cid:6)\n(cid:6)\n\ni\n\ni\n\nxi\n\ne[xi]\n\nbecause, by the assumed independence of the outcomes of the games, the probability\nthat any specified permutation is a hamiltonian is (1/2)n\u22121, it follows that\n\ntherefore,\n\ne[xi] = p{xi = 1} = (1/2)n\u22121\n\ne[f (s)] = n!(1/2)n\u22121\n\nsince f (s) is not a constant random variable, the preceding equation implies that\nthere is an outcome of the tournament having more than n!/2n\u22121 hamiltonian\n.\npaths.\n\nexample 2r\na grove of 52 trees is arranged in a circular fashion. if 15 chipmunks live in these\ntrees, show that there is a group of 7 consecutive trees that together house at least 3\nchipmunks.\n\nsolution. let the neighborhood of a tree consist of that tree along with the next six\ntrees visited by moving in the clockwise direction. we want to show that, for any\nchoice of living accommodations of the 15 chipmunks, there is a tree that has at least\n3 chipmunks living in its neighborhood. to show this, choose a tree at random and\nlet x denote the number of chipmunks that live in its neighborhood. to determine\ne[x], arbitrarily number the 15 chipmunks and for i = 1, . . . , 15, let\nxi =\n\nif chipmunk i lives in the neighborhood of the randomly chosen tree\notherwise\n\n%\n\n1,\n0,\n\nbecause\n\nwe obtain that\n\nx = 15(cid:6)\ne[x] = 15(cid:6)\n\ni=1\n\ni=1\n\nxi\n\ne[xi]\n\nhowever, because xi will equal 1 if the randomly chosen tree is any of the 7 trees\nconsisting of the tree in which chipmunk i lives along with its 6 neighboring trees\nwhen moving in the counterclockwise direction,\n\nconsequently,\n\ne[xi] = p{xi = 1} = 7\n52\n\ne[x] = 105\n52\n\n> 2\n\nshowing that there exists a tree with more than 2 chipmunks living in its neigh-\n.\nborhood.\n\n "}, {"Page_number": 328, "text": "section 7.2\n\nexpectation of sums of random variables 313\n\n\u22177.2.2 the maximum\u2013minimums identity\nwe start with an identity relating the maximum of a set of numbers to the minimums\nof the subsets of these numbers.\n(cid:6)\nproposition 2.2. for arbitrary numbers xi, i = 1, . . . , n,\n\n(cid:6)\n\n(cid:6)\n\nxi =\n\nmax\n\ni\n\nxi \u2212\n\nmin(xi, xj) +\n\ni\n\ni<j\n\n+ . . . + (\u22121)n+1 min(x1, . . . , xn)\n\ni<j<k\n\nmin(xi, xj, xk)\n\nproof. we will give a probabilistic proof of the proposition. to begin, assume that\nall the xi are in the interval [0, 1]. let u be a uniform (0, 1) random variable, and\ndefine the events ai, i = 1, . . . , n, by ai = {u < xi}. that is, ai is the event that\nthe uniform random variable is less than xi. because at least one of these events ai\nwill occur if u is less than at least one of the values xi, we have that\n\ntherefore,\n\nalso,\n\np(\u222aiai) = p\n\nu < max\n\nxi\n\ni\n\n= max\n\nxi\n\ni\n\np(ai) = p\n\nu < xi\n\n%\n\n\u222aiai =\n%\n\nu < max\n\ni\n\n/\n\nxi\n\n/\n> = xi\n\n7\n\n=\n\n0\n\nimplying that\n\nai1\n\n. . . air\n\np(ai1\n\n. . . air\n\n) = p\n\n=\n0\n\nu < min\nj=1,...r\n\nxij\n\n7\n\nu < min\nj=1,...r\n\nxij\n\n= min\nj=1,...r\n\nxij\n\nin addition, because all of the events ai1, . . . , air will occur if u is less than all the\nvalues xi1, . . . , xir, we see that the intersection of these events is\n\nthus, the proposition follows from the inclusion\u2013exclusion formula for the proba-\nbility of the union of events:\n\np(\u222aiai) =\n\n(cid:6)\n\n(cid:6)\n\np(ai) \u2212\n\np(aiaj) +\n+ . . . + (\u22121)n+1p(a1 . . . an)\n\ni<j\n\ni\n\n(cid:6)\n\ni<j<k\n\np(aiajak)\n\nwhen the xi are nonnegative, but not restricted to the unit interval, let c be such\nthat all the xi are less than c. then the identity holds for the values yi = xi/c, and\nthe desired result follows by multiplying through by c. when the xi can be negative,\nlet b be such that xi + b > 0 for all i. therefore, by the preceding,\nmin(xi + b, xj + b)\n\n(cid:6)\n(xi + b) \u2212\n+ \u00b7\u00b7\u00b7 + (\u22121)n+1 min(x1 + b, . . . , xn + b)\n\n(xi + b) =\n\n(cid:6)\n\nmax\n\ni<j\n\ni\n\ni\n\n "}, {"Page_number": 329, "text": "314\n\nchapter 7\n\nproperties of expectation\n\nletting\n\n(cid:6)\n\nxi \u2212\n\n(cid:6)\n\nm =\n\nmin(xi, xj) + \u00b7\u00b7\u00b7 + (\u22121)n+1 min(x1, . . . , xn)\n(cid:3)(cid:5)\n\n(cid:2)\n\n+ \u00b7\u00b7\u00b7 + (\u22121)n+1\n(cid:2)\n\n+ \u00b7\u00b7\u00b7 + (\u22121)n\n\nn\nn\n\n(cid:3)\n\nn\nn\n\ni\n\ni<j\n\n(cid:4)\nwe can rewrite the foregoing identity as\nn \u2212\n\nxi + b = m + b\n\nmax\n\ni\n\nbut\n\n0 = (1 \u2212 1)n = 1 \u2212 n +\n\nthe preceding two equations show that\n\n(cid:2)\n(cid:2)\n\n(cid:3)\n(cid:3)\n\nn\n2\n\nn\n2\n\nxi = m\n\nmax\n\ni\n\nand the proposition is proven.\n\n(cid:6)\n\n(cid:20)\n\n(cid:6)\n\n(cid:21)\n\n(cid:6)\n\ni\n\nit follows from proposition 2.2 that, for any random variables x1, . . . , xn,\nmin(xi, xj) + \u00b7\u00b7\u00b7 + (\u22121)n+1 min(x1, . . . , xn)\n\nxi \u2212\n\nxi =\n\nmax\n\ni\n\ni\n\ni<j\n\ntaking expectations of both sides of this equality yields the following relationship\nbetween the expected value of the maximum and those of the partial minimums:\n\ne\n\nmax\n\ni\n\nxi\n\n=\n\ne[xi] \u2212\n\ne[min(xi, xj)]\n\n+ \u00b7\u00b7\u00b7 + (\u22121)n+1e[min(x1, . . . , xn)]\n\n(2.7)\n\n(cid:6)\n\ni<j\n\nexample 2s coupon collecting with unequal probabilities\nsuppose there are n different types of coupons and that each time one collects a\ncoupon, it is, independently of previous coupons collected, a type i coupon with prob-\npi = 1. find the expected number of coupons one needs to collect to\nability pi,\nobtain a complete set of at least one of each type.\n\nn(cid:9)\n\ni=1\n\nsolution. if we let xi denote the number of coupons one needs collect to obtain a\ntype i, then we can express x as\n\nx = max\ni=1,...,n\n\nxi\n\nbecause each new coupon obtained is a type i with probability pi, xi is a geometric\nrandom variable with parameter pi. also, because the minimum of xi and xj is the\nnumber of coupons needed to obtain either a type i or a type j, it follows that, for\ni z j, min (xi, xj) is a geometric random variable with parameter pi + pj. similarly,\nmin (xi, xj, xk), the number needed to obtain any of types i, j, and k, is a geometric\n\n "}, {"Page_number": 330, "text": "(cid:6)\n\n* q\n\n0\n\nsection 7.3\n\nmoments of the number of events that occur 315\nrandom variable with parameter pi + pj + pk, and so on. therefore, the identity (2.7)\nyields\n\n(cid:6)\n\ne[x] =\n\n(cid:6)\n\ni<j<k\n\n1\n\n+\n\n\u2212\n\n1\n\n1\npi + pj\npi\n+ \u00b7\u00b7\u00b7 + (\u22121)n+1\n\ni<j\n\np1 + \u00b7\u00b7\u00b7 + pn\n\n1\n\npi + pj + pk\n\ni\n\nnoting that\n\nand using the identity\n\n1 \u2212 n(cid:31)\n\ni=1\n\n(1 \u2212 e\n\n\u2212pix) =\n\n\u2212(pi+pj)x + \u00b7\u00b7\u00b7 + (\u22121)n+1e\ne\n\n\u2212(p1+\u00b7\u00b7\u00b7+pn)x\n\nshows, upon integrating the identity, that\n\n\u2212px dx = 1\ne\np\n(cid:6)\n\n(cid:6)\n\ni\n\ni<j\n\n\u2212pix \u2212\ne\n\u239b\n\u239d1 \u2212 n(cid:31)\n\n* q\n\n0\n\ni=1\n\ne[x] =\n\n\u239e\n\u23a0 dx\n\n(1 \u2212 e\n\n\u2212pix)\n\nis a more useful computational form.\n\n.\n\n7.3 moments of the number of events that occur\n\nmany of the examples solved in the previous section were of the following form: for\ngiven events a1, . . . , an, find e[x], where x is the number of these events that occur.\nthe solution then involved defining an indicator variable ii for event ai such that\n\nbecause\n\nwe obtained the result\n\ne[x] = e\n\n\u23a1\n\u23a3 n(cid:6)\n\ni=1\n\ne[ii] = n(cid:6)\n\ni=1\n\np(ai)\n\n(3.1)\n\nnow suppose we are interested in the number of pairs of events that occur.\nbecause iiij will equal 1 if both ai and aj occur, and will equal 0 otherwise, it fol-\ni<j iiij. but because x is the number of\nlows that the number of pairs is equal to\nevents that occur, it also follows that the number of pairs of events that occur is\n.\nconsequently,\n\n(cid:19)\n\n(cid:18)\n\nx\n2\n\n%\n\nii =\n\n1,\n0,\n\nif ai occurs\notherwise\n\nii\n\nii\n\ni=1\n\nx = n(cid:6)\n\u23a4\n\u23a6 = n(cid:6)\n(cid:9)\n(cid:6)\n\n(cid:3)\n\ni=1\n\n=\n\n(cid:2)\n\nx\n2\n\niiij\n\ni<j\n\n "}, {"Page_number": 331, "text": "316\n\nchapter 7\n\nproperties of expectation\n\n(cid:18)\n\n(cid:19)\n\nn\n2\n\nwhere there are\n\nor\n\ngiving that\n\nterms in the summation. taking expectations yields\n\n(cid:6)\n\ni<j\n\n=\n\n(cid:7)(cid:2)\n\n(cid:3)(cid:8)\n\ne\n\nx\n2\n\n(cid:20)\n\ne\n\nx(x \u2212 1)\n\n2\n\n(cid:6)\n\np(aiaj)\n\ni<j\n\np(aiaj)\n\ne[iiij] =\n(cid:21)\n(cid:6)\n(cid:6)\n\n=\n\ni<j\n\ne[x2] \u2212 e[x] = 2\n\np(aiaj)\n\ni<j\n\n(3.2)\n\n(3.3)\n\n(cid:2)\n\n(cid:3)\n\nx\nk\n\n=\n\n(cid:6)\n\ni1<i2<...<ik\n\nwhich yields e[x2], and thus var(x) = e[x2] \u2212 (e[x])2.\n\nmoreover, by considering the number of distinct subsets of k events that all occur,\n\nwe see that\n\ntaking expectations gives the identity\n\n(cid:7)(cid:2)\n\n(cid:3)(cid:8)\n\ne\n\nx\nk\n\n=\n\n(cid:6)\n\ne[ii1ii2\n\n\u00b7\u00b7\u00b7 iik] =\n\ni1<i2<...<ik\n\ni1<i2<...<ik\n\nii1ii2\n\n\u00b7\u00b7\u00b7 iik\n(cid:6)\n\np(ai1ai2\n\n\u00b7\u00b7\u00b7 aik\n\n)\n\n(3.4)\n\nexample 3a moments of binomial random variables\nconsider n independent trials, with each trial being a success with probability p. let\nai be the event that trial i is a success. when i z j, p(aiaj) = p2. consequently,\nequation (3.2) yields\n\n(cid:7)(cid:2)\n\n(cid:3)(cid:8)\n\n(cid:2)\n\n(cid:3)\n\n(cid:6)\n\ni<j\n\ne\n\nx\n2\n\n=\n\np2 =\n\nn\n2\n\np2\n\ne[x(x \u2212 1)] = n(n \u2212 1)p2\n\ne[x2] \u2212 e[x] = n(n \u2212 1)p2\n\nor\n\nor\n\nnow, e[x] =(cid:9)\n\nn\n\ni=1 p(ai) = np, so, from the preceding equation\n\nvar(x) = e[x2] \u2212 (e[x])2 = n(n \u2212 1)p2 + np \u2212 (np)2 = np(1 \u2212 p)\n\nwhich is in agreement with the result obtained in section 4.6.1.\n\nin general, because p(ai1ai2\n\n(cid:3)(cid:8)\n\u00b7\u00b7\u00b7 aik\n=\n\n(cid:7)(cid:2)\n\nx\nk\n\n) = pk, we obtain from equation (3.4) that\n(cid:6)\n\n(cid:2)\n\n(cid:3)\n\npk =\n\nn\nk\n\npk\n\ni1<i2<...<ik\n\ne\n\nor, equivalently,\n\ne[x(x \u2212 1)\u00b7\u00b7\u00b7 (x \u2212 k + 1)] = n(n \u2212 1)\u00b7\u00b7\u00b7 (n \u2212 k + 1)pk\n\n "}, {"Page_number": 332, "text": "section 7.3\n\nmoments of the number of events that occur 317\n\nthe successive values e[xk], k \u00fa 3, can be recursively obtained from this identity.\nfor instance, with k = 3, it yields\n\ne[x(x \u2212 1)(x \u2212 2)] = n(n \u2212 1)(n \u2212 2)p3\n\ne[x3 \u2212 3x2 + 2x] = n(n \u2212 1)(n \u2212 2)p3\n\nor\n\nor\n\ne[x3] = 3e[x2] \u2212 2e[x] + n(n \u2212 1)(n \u2212 2)p3\n= 3n(n \u2212 1)p2 + np + n(n \u2212 1)(n \u2212 2)p3\n\n.\n\nexample 3b moments of hypergeometric random variables\nsuppose n balls are randomly selected from an urn containing n balls, of which m\nare white. let ai be the event that the ith ball selected is white. then x, the number\nof white balls selected, is equal to the number of the events a1, . . . , an that occur.\nbecause the ith ball selected is equally likely to be any of the n balls, of which m are\ni=1 p(ai) =\n\nwhite, p(ai) = m/n. consequently, equation (3.1) gives that e[x] =(cid:9)\n\nn\n\nnm/n. also, since\n\nwe obtain, from equation (3.2), that\n\ne\n\nor\n\nshowing that\n\nm \u2212 1\np(aiaj) = p(ai)p(aj|ai) = m\nn \u2212 1\nn\n(cid:7)(cid:2)\n(cid:3)(cid:8)\n(cid:2)\n(cid:3)\n\nm(m \u2212 1)\nn(n \u2212 1)\n\n=\n\nm(m \u2212 1)\nn(n \u2212 1)\n\nn\n2\n\n(cid:6)\n\ni<j\n\n=\n\nx\n2\n\ne[x(x \u2212 1)] = n(n \u2212 1)\n\nm(m \u2212 1)\nn(n \u2212 1)\n\ne[x2] = n(n \u2212 1)\n\nm(m \u2212 1)\nn(n \u2212 1)\n\n+ e[x]\n\nthis formula yields the variance of the hypergeometric, namely,\n\nvar(x) = e[x2] \u2212 (e[x])2\n\n(cid:20)\n\n= n(n \u2212 1)\n= mn\nn\n\nm(m \u2212 1)\nn(n \u2212 1)\n(n \u2212 1)(m \u2212 1)\nn \u2212 1\n\n(cid:21)\n\u2212 n2m2\n+ nm\nn2\nn\n+ 1 \u2212 mn\nn\n\nwhich agrees with the result obtained in example 8j of chapter 4.\n\nhigher moments of x are obtained by using equation (3.4). because\n\np(ai1ai2\n\n\u00b7\u00b7\u00b7 aik\n\n) = m(m \u2212 1)\u00b7\u00b7\u00b7 (m \u2212 k + 1)\nn(n \u2212 1)\u00b7\u00b7\u00b7 (n \u2212 k + 1)\n\n "}, {"Page_number": 333, "text": "318\n\nchapter 7\n\nproperties of expectation\n\nequation (3.4) yields\n\ne\n\n(cid:7)(cid:2)\n\n(cid:3)(cid:8)\n\n=\n\nx\nk\n\n(cid:2)\n\n(cid:3)\n\nn\nk\n\nm(m \u2212 1)\u00b7\u00b7\u00b7 (m \u2212 k + 1)\nn(n \u2212 1)\u00b7\u00b7\u00b7 (n \u2212 k + 1)\n\nor\n\ne[x(x \u2212 1)\u00b7\u00b7\u00b7 (x \u2212 k + 1)]\n\n= n(n \u2212 1)\u00b7\u00b7\u00b7 (n \u2212 k + 1)\n\nm(m \u2212 1)\u00b7\u00b7\u00b7 (m \u2212 k + 1)\nn(n \u2212 1)\u00b7\u00b7\u00b7 (n \u2212 k + 1)\n\n.\n\nexample 3c moments in the match problem\nfor i = 1, . . . , n, let ai be the event that person i selects his or her own hat in the\nmatch problem. then\n\np(aiaj) = p(ai)p(aj|ai) = 1\nn\n\n1\n\nn \u2212 1\n\nwhich follows because, conditional on person i selecting her own hat, the hat selected\nby person j is equally likely to be any of the other n \u2212 1 hats, of which one is his\nown. consequently, with x equal to the number of people who select their own hat,\nit follows from equation (3.2) that\n\n(cid:7)(cid:2)\n\n(cid:3)(cid:8)\n\ne\n\nx\n2\n\n(cid:6)\n\ni<j\n\n=\n\n(cid:2)\n\n(cid:3)\n\n1\n\nn(n \u2212 1\n\n=\n\nn\n2\n\n1\n\nn(n \u2212 1)\n\nthus showing that\n\ntherefore, e[x2] = 1 + e[x]. because e[x] =(cid:9)\n\ne[x(x \u2212 1)] = 1\n\nn\n\ni=1 p(ai) = 1, we obtain that\n\nhence, both the mean and variance of the number of matches is 1. for higher moments,\nn(n\u22121)\u00b7\u00b7\u00b7(n\u2212k+1),\nwe use equation (3.4), along with the fact that p(ai1ai2\nto obtain\n\n\u00b7\u00b7\u00b7 aik\n\n) =\n\n1\n\n(cid:7)(cid:2)\n\nvar(x) = e[x2] \u2212 (e[x])2 = 1.\n(cid:3)(cid:8)\n\n(cid:2)\n\n(cid:3)\n\nx\nk\n\n=\n\nn\nk\n\n1\n\nn(n \u2212 1)\u00b7\u00b7\u00b7 (n \u2212 k + 1)\n\ne\n\nor\n\ne[x(x \u2212 1)\u00b7\u00b7\u00b7 (x \u2212 k + 1)] = 1\n\n.\n\nexample 3d another coupon-collecting problem\nsuppose that there are n distinct types of coupons and that, independently of past\nj=1 pj = 1. find\ntypes collected, each new one obtained is type j with probability pj,\nthe expected value and variance of the number of different types of coupons that\nappear among the first n collected.\n\nn\n\n(cid:9)\n\nsolution. we will find it more convenient to work with the number of uncollected\ntypes. so, let y equal the number of different types of coupons collected, and let\nx = n \u2212 y denote the number of uncollected types. with ai defined as the event\nthat there are no type i coupons in the collection, x is equal to the number of the\n\n "}, {"Page_number": 334, "text": "section 7.3\n\nmoments of the number of events that occur 319\n\nevents a1, . . . , an that occur. because the types of the successive coupons collected\nare independent, and, with probability 1 \u2212 pi each new coupon is not type i, we have\n\nhence, e[x] =(cid:9)\n\np(ai) = (1 \u2212 pi)n\n\nn\ni=1\n\n(1 \u2212 pi)n, from which it follows that\n\ne[y] = n \u2212 e[x] = n \u2212 n(cid:6)\n\n(1 \u2212 pi)n\n\ni=1\n\nsimilarly, because each of the n coupons collected is neither a type i nor a type j\ncoupon, with probability 1 \u2212 pi \u2212 pj, we have\n\nthus,\n\nor\n\np(aiaj) = (1 \u2212 pi \u2212 pj)n,\n(cid:6)\n\ne[x(x \u2212 1)] = 2\n\np(aiaj) = 2\n\ni z j\n\n(1 \u2212 pi \u2212 pj)n\n\ni<j\n\ne[x2] = 2\n\n(1 \u2212 pi \u2212 pj)n + e[x]\n\n(cid:6)\n(cid:6)\n\ni<j\n\ni<j\n\nhence, we obtain\n\nvar(y) = var(x)\n(cid:6)\n\n= e[x2] \u2212 (e[x])2\n= 2\n\ni<j\n\nin the special case where pi = 1/n, i = 1, . . . , n, the preceding formula gives\n\n\u239e\n\u23a02\n\n(1 \u2212 pi)n\n\n\u239b\n\u239d n(cid:6)\n\ni=1\n\ni=1\n\n(cid:7)\n\n(1 \u2212 pi \u2212 pj)n + n(cid:6)\n(1 \u2212 pi)n \u2212\n(cid:8)\n(cid:3)n\n(cid:2)\n1 \u2212 1\nn\n(cid:3)n \u2212 n2\n(cid:3)n + n\n(cid:2)\n1 \u2212 1\nn\n\ne[y] = n\n(cid:2)\n1 \u2212 2\nn\n\n1 \u2212\n\n(cid:3)2n\n\n(cid:2)\n1 \u2212 1\nn\n\n.\n\nand\n\nvar(y) = n(n \u2212 1)\n\nexample 3e the negative hypergeometric random variables\nsuppose an urn contains n + m balls, of which n are special and m are ordinary. these\nitems are removed one at a time, with each new removal being equally likely to be\nany of the balls that remain in the urn. the random variable y, equal to the number\nof balls that need be withdrawn until a total of r special balls have been removed,\nis said to have a negative hypergeometric distribution. the negative hypergeometric\ndistribution bears the same relationship to the hypergeometric distribution as the\nnegative binomial does to the binomial. that is, in both cases, rather than considering\na random variable equal to the number of successes in a fixed number of trials (as are\nthe binomial and hypergeometric variables), they refer to the number of trials needed\nto obtain a fixed number of successes.\n\n "}, {"Page_number": 335, "text": "320\n\nchapter 7\n\nproperties of expectation\n\nto obtain the probability mass function of a negative hypergeometric random vari-\nable x, note that x will equal k if both\n(a) the first k \u2212 1 withdrawals consist of r \u2212 1 special and k \u2212 r ordinary balls and\n(b) the kth ball withdrawn is special.\n\nconsequently,\n\np{x = k} =\n\n(cid:19)\n\n(cid:18)\n\n(cid:19)(cid:18)\n\n(cid:18)\n\nn\nr\u22121\n\n(cid:19)\n\nm\nk\u2212r\nn+m\nk\u22121\n\nn \u2212 r + 1\n\nn + m \u2212 k + 1\n\nwe will not, however, utilize the preceding probability mass function to obtain the\nmean and variance of y. rather, let us number the m ordinary balls as o1, . . . , om,\nand then, for each i = 1, . . . , n, let ai be the event that oi is withdrawn before r\nspecial balls have been removed. then, if x is the number of the events a1, . . . , am\nthat occur, it follows that x is the number of ordinary balls that are withdrawn before\na total of r special balls have been removed. consequently,\n\nshowing that\n\ny = r + x\n\ne[y] = r + e[x] = r + m(cid:6)\n\np(ai)\n\ni=1\n\nto determine p(ai), consider the n + 1 balls consisting of oi along with the n special\nballs. of these n + 1 balls, oi is equally likely to be the first one withdrawn, or the\nsecond one withdrawn, . . . , or the final one withdrawn. hence, the probability that\nit is among the first r of these to be selected (and so is removed before a total or r\nspecial balls have been withdrawn) is\n\nr\nn+1. consequently,\n\np(ai) = r\n\nn + 1\n= r(n + m + 1)\n\nand\n\ne[y] = r + m\n\nr\n\nn + 1\nthus, for instance, the expected number of cards of a well-shuffled deck that would\n= 3.786, and the expected\nneed to be turned over until a spade appears is 1 + 39\nnumber of cards that would need to be turned over until an ace appears is\n1 + 48\n\n= 10.6.\n\nn + 1\n\nto determine var(y) = var(x), we use the identity\n\n14\n\n5\n\ne[x(x \u2212 1)] = 2\n\n(cid:6)\n\np(aiaj)\n\ni<j\n\nnow, p(aiaj) is the probability that both oi and oj are removed before there have\nbeen a total of r special balls removed. so consider the n + 2 balls consisting of oi, oj,\nand the n special balls. because all withdrawal orderings of these balls are equally\nlikely, the probability that oi and oj are both among the first r + 1 of them to be\nremoved (and so are both removed before r special balls have been withdrawn) is\n\n(cid:19)\n(cid:19) =\n\n(cid:18)\n\n(cid:19)(cid:18)\n(cid:18)\n\n2\n2\n\nn\nr\u22121\nn+2\nr+1\n\np(aiaj) =\n\nr(r + 1)\n\n(n + 1)(n + 2)\n\n "}, {"Page_number": 336, "text": "moments of the number of events that occur 321\n\nconsequently,\n\nso\n\nsection 7.3\n\n(cid:2)\ne[x(x \u2212 1)] = 2\n\n(cid:3)\n\ne[x2] = m(m \u2212 1)\n\nr(r + 1)\n\n(n + 1)(n + 2)\n\nm\n2\nr(r + 1)\n\n(n + 1)(n + 2)\n\n+ e[x]\n\nbecause e[x] = m r\n\nn+1, this yields\n\nvar(y) = var(x) = m(m \u2212 1)\n\na little algebra now shows that\n\nr(r + 1)\n\nr\n\nm\n\n(n + 1)(n + 2)\n\nn + 1\nvar(y) = mr(n + 1 \u2212 r)(n + m + 1)\n\n(n + 1)2(n + 2)\n\n(cid:3)2\n\n(cid:2)\n\n\u2212\n\nm\n\nr\n\nn + 1\n\n.\n\nexample 3f singletons in the coupon collector\u2019s problem\nsuppose that there are n distinct types of coupons and that, independently of past\ntypes collected, each new one obtained is equally likely to be any of the n types.\nsuppose also that one continues to collect coupons until a complete set of at least\none of each type has been obtained. find the expected value and variance of the\nnumber of types for which exactly one coupon of that type is collected.\nsolution. let x equal the number of types for which exactly one of that type is col-\nlected. also, let ti denote the ith type of coupon to be collected, and let ai be the\nevent that there is only a single type ti coupon in the complete set. because x is\nequal to the number of the events a1, . . . , an that occur, we have\n\ne[x] = n(cid:6)\n\ni=1\n\np(ai)\n\nnow, at the moment when the first type ti coupon is collected, there remain n \u2212 i\ntypes that need to be collected to have a complete set. because, starting at this moment,\neach of these n \u2212 i + 1 types (the n \u2212 i not yet collected and type ti) is equally\nlikely to be the last of these types to be collected, it follows that the type ti will be the\n. consequently,\nlast of these types (and so will be a singleton) with probability\np(ai) = 1\n\nn\u2212i+1\n\nn\u2212i+1, yielding\n\n1\n\ne[x] = n(cid:6)\n\ni=1\n\n1\n\nn \u2212 i + 1\n\n= n(cid:6)\n\ni=1\n\n1\ni\n\nto determine the variance of the number of singletons, let si, j, for i < j, be the event\nthat the first type ti coupon to be collected is still the only one of its type to have\nbeen collected at the moment that the first type tj coupon has been collected. then\n\np(aiaj) = p(aiaj|si,j)p(si,j)\n\nnow, p(si,j) is the probability that when a type ti has just been collected, of the\nn \u2212 i + 1 types consisting of type ti and the n \u2212 i as yet uncollected types, a type ti\nis not among the first j \u2212 i of these types to be collected. because type ti is equally\nlikely to be the first, or second, or . . . , n \u2212 i + 1 of these types to be collected, we have\n\np(si,j) = 1 \u2212\n\nj \u2212 i\n\nn \u2212 i + 1\n\n= n + 1 \u2212 j\nn + 1 \u2212 i\n\n "}, {"Page_number": 337, "text": "322\n\nchapter 7\n\nproperties of expectation\n\nnow, conditional on the event si,j, both ai and aj will occur if, at the time the first\ntype tj coupon is collected, of the n \u2212 j + 2 types consisting of types ti, tj, and the\nn \u2212 j as yet uncollected types, ti and tj are both collected after the other n \u2212 j. but\nthis implies that\n\np(aiaj|si,j) = 2\n\n1\n\nn \u2212 j + 2\n\n1\n\nn \u2212 j + 1\n\ntherefore,\n\nyielding\n\np(aiaj) =\n\n2\n\n(n + 1 \u2212 i)(n + 2 \u2212 j)\n\n,\n\ni < j\n\ne[x(x \u2212 1)] = 4\n\n(cid:6)\n\ni<j\n\nconsequently, using the previous result for e[x], we obtain\n\n(cid:6)\n\ni<j\n\nvar(x) = 4\n\n1\n\n(n + 1 \u2212 i)(n + 2 \u2212 j)\n\n1\n\n(n + 1 \u2212 i)(n + 2 \u2212 j)\n\u239b\n\u239d n(cid:6)\n\n+ n(cid:6)\n\n\u2212\n\n1\ni\n\ni=1\n\ni=1\n\n\u239e\n\u23a02\n\n1\ni\n\n.\n\n7.4 covariance, variance of sums, and correlations\n\nthe following proposition shows that the expectation of a product of independent\nrandom variables is equal to the product of their expectations.\n\nproposition 4.1. if x and y are independent, then, for any functions h and g,\n\nproof. suppose that x and y are jointly continuous with joint density f (x, y). then\n\ne[g(x)h(y)] =\n=\n\ne[g(x)h(y)] = e[g(x)]e[h(y)]\n\n\u2212q\n\n\u2212q\n\n* q\n* q\n\n* q\n* q\n* q\n=\nh(y)fy (y)dy\n= e[h(y)]e[g(x)]\n\n\u2212q\n\n\u2212q\n\n\u2212q\n\n* q\n\ng(x)h(y)f (x, y) dx dy\n\ng(x)h(y)fx (x)fy (y) dx dy\n\ng(x)fx (x) dx\n\n\u2212q\n\nthe proof in the discrete case is similar.\n\njust as the expected value and the variance of a single random variable give us\ninformation about that random variable, so does the covariance between two random\nvariables give us information about the relationship between the random variables.\n\ndefinition\nthe covariance between x and y, denoted by cov (x, y), is defined by\n\ncov(x, y) = e[(x \u2212 e[x])(y \u2212 e[y])]\n\n "}, {"Page_number": 338, "text": "section 7.4\n\ncovariance, variance of sums, and correlations 323\n\nupon expanding the right side of the preceding definition, we see that\n\ncov(x, y) = e[xy \u2212 e[x]y \u2212 xe[y] + e[y]e[x]]\n\n= e[xy] \u2212 e[x]e[y] \u2212 e[x]e[y] + e[x]e[y]\n= e[xy] \u2212 e[x]e[y]\n\nnote that if x and y are independent, then, by proposition 4.1, cov(x, y) = 0.\nhowever, the converse is not true. a simple example of two dependent random vari-\nables x and y having zero covariance is obtained by letting x be a random variable\nsuch that\n\np{x = 0} = p{x = 1} = p{x = \u22121} = 1\n3\n\n%\n\nand defining\n\nif x z 0\nif x = 0\nnow, xy = 0, so e[xy] = 0. also, e[x] = 0. thus,\n\ny =\n\n0\n1\n\ncov(x, y) = e[xy] \u2212 e[x]e[y] = 0\n\nhowever, x and y are clearly not independent.\n\nthe following proposition lists some of the properties of covariance.\n\nproposition 4.2.\n(i) cov(x, y) = cov(y, x)\n(ii) cov(x, x) = var(x)\n\u239e\n(iii) cov(ax, y) = a cov(x, y)\n\u239f\u23a0 = n(cid:6)\n\n\u239b\n\u239c\u239d n(cid:6)\n\nm(cid:6)\n\n(iv) cov\n\nxi,\n\nyj\n\ni=1\n\nj=1\n\ni=1\n\nm(cid:6)\n\nj=1\n\ncov(xi, yj)\n\nproof of proposition 4.2: parts (i) and (ii) follow immediately from the definition\nof covariance, and part (iii) is left as an exercise for the reader. to prove part (iv),\nwhich states that the covariance operation is additive (as is the operation of taking\n\u23a1\nexpectations), let \u03bci = e[xi] and vj = e[yj]. then\n\u23a2\u23a3 m(cid:6)\n\ne\n\n\u03bci, e\n\nvj\n\nxi\n\ni=1\n\ni=1\n\n\u23a1\n\u23a3 n(cid:6)\n\u239e\n\u239f\u23a0 = e\n\n\u23a4\n\u23a6 = n(cid:6)\n\u23a1\n\u239b\n\u239d n(cid:6)\n\u23a2\u23a2\u23a3\n\u23a1\n\u23a2\u23a3 n(cid:6)\n\n= e\n\ni=1\n\nyj\n\nm(cid:6)\n\nj=1\n\nyj\n\nj=1\n\nxi \u2212 n(cid:6)\nm(cid:6)\n\ni=1\n\n\u03bci\n\n\u23a4\n\u23a5\u23a6 = m(cid:6)\n\u239b\n\u239e\n\u239c\u239d m(cid:6)\n\u23a0\n\nj=1\n\nj=1\n\n(xi \u2212 \u03bci)\n\n(yj \u2212 vj)\n\ni=1\n\nj=1\n\n\u239e\n\u239f\u23a0\n\n\u23a4\n\u23a5\u23a5\u23a6\n\nvj\n\nyj \u2212 m(cid:6)\n\u23a4\n\u23a5\u23a6\n\nj=1\n\nand\n\n\u239b\n\u239c\u239d n(cid:6)\n\ni=1\n\nxi,\n\ncov\n\n "}, {"Page_number": 339, "text": "324\n\nchapter 7\n\nproperties of expectation\n\n\u23a1\n\u23a2\u23a3 n(cid:6)\n= n(cid:6)\nm(cid:6)\n\n= e\n\ni=1\n\ni=1\n\nj=1\n\n\u23a4\n\u23a5\u23a6\n\nm(cid:6)\n\n(xi \u2212 \u03bci)(yj \u2212 vj)\n\nj=1\ne[(xi \u2212 \u03bci)(yj \u2212 vj)]\n\nwhere the last equality follows because the expected value of a sum of random vari-\nables is equal to the sum of the expected values.\nit follows from parts (ii) and (iv) of proposition 4.2, upon taking yj = xj, j =\n\n1, . . . , n, that\n\n\u239b\n\u239d n(cid:6)\n\ni=1\n\nvar\n\nxi\n\n\u239e\n\u239f\u23a0\n\nn(cid:6)\n\nj=1\n\nxj\n\nxi,\n\n\u239b\n\u239e\n\u239c\u239d n(cid:6)\n\u23a0 = cov\n= n(cid:6)\nn(cid:6)\n= n(cid:6)\n\ni=1\n\ni=1\n\ni=1\n\ncov(xi, xj)\n\nj=1\nvar(xi) +\n\n(cid:6)(cid:6)\n\ncov(xi, xj)\n\nizj\n\nsince each pair of indices i, j, i z j, appears twice in the double summation, the pre-\nceding formula is equivalent to\n\n\u239b\n\u239d n(cid:6)\n\ni=1\n\n\u239e\n\u23a0 = n(cid:6)\n\ni=1\n\nvar\n\nxi\n\n(cid:6)(cid:6)\n\nvar(xi) + 2\n\ncov(xi, xj)\n\n(4.1)\n\ni<j\n\nif x1, . . . , xn are pairwise independent, in that xi and xj are independent for i z j,\n\nthen equation (4.1) reduces to\n\n\u239b\n\u239d n(cid:6)\n\ni=1\n\n\u239e\n\u23a0 = n(cid:6)\n\ni=1\n\nvar\n\nxi\n\nvar(xi)\n\nthe following examples illustrate the use of equation (4.1).\n\nexpected value \u03bc and variance \u03c3 2, and as in example 2c, let x = n(cid:9)\n\nexample 4a\nlet x1, . . . , xn be independent and identically distributed random variables having\nxi/n be the sam-\nple mean. the quantities xi \u2212 x, i = 1, . . . , n, are called deviations, as they equal the\ndifferences between the individual data and the sample mean. the random variable\n\ni=1\n\ns2 = n(cid:6)\n\ni=1\n\n(xi \u2212 x)2\nn \u2212 1\n\nis called the sample variance. find (a) var(x) and (b) e[s2].\n\n "}, {"Page_number": 340, "text": "covariance, variance of sums, and correlations 325\n\nsolution.\n\nsection 7.4\n\n(a) var(x) =\n\n\u239e\n\u23a0\n\nxi\n\n\u239b\n\u239d n(cid:6)\n\ni=1\n\n(cid:2)\n(cid:2)\n\n1\nn\n\n(cid:3)2\n(cid:3)2 n(cid:6)\n\nvar\n\nvar(xi) by independence\n\ni=1\n\n=\n\n1\nn\n= \u03c3 2\nn\n\n(x \u2212 \u03bc)2 \u2212 2(x \u2212 \u03bc)\n\nn(cid:6)\n(xi \u2212 \u03bc)\n\ni=1\n\n(xi \u2212 \u03bc)2 + n(x \u2212 \u03bc)2 \u2212 2(x \u2212 \u03bc)n(x \u2212 \u03bc)\n\n(b) we start with the following algebraic identity:\n\n(xi \u2212 \u03bc + \u03bc \u2212 x)2\n\n(xi \u2212 \u03bc)2 + n(cid:6)\n\ni=1\n\ni=1\n\ni=1\n\n(n \u2212 1)s2 = n(cid:6)\n= n(cid:6)\n= n(cid:6)\n= n(cid:6)\n(n \u2212 1)e[s2] = n(cid:6)\n\ni=1\n\ni=1\n\n(xi \u2212 \u03bc)2 \u2212 n(x \u2212 \u03bc)2\n\ntaking expectations of the preceding yields\n\ne[(xi \u2212 \u03bc)2] \u2212 ne[(x \u2212 \u03bc)2]\n\ni=1\n\n= n\u03c3 2 \u2212 nvar(x)\n= (n \u2212 1)\u03c3 2\n\nwhere the final equality made use of part (a) of this example and the one preceding\nit made use of the result of example 2c, namely, that e[x] = \u03bc. dividing through\nby n \u2212 1 shows that the expected value of the sample variance is the distribution\n.\nvariance \u03c3 2.\n\nour next example presents another method for obtaining the variance of a bino-\n\nmial random variable.\n\nexample 4b variance of a binomial random variable\ncompute the variance of a binomial random variable x with parameters n and p.\n\nsolution. since such a random variable represents the number of successes in n inde-\npendent trials when each trial has the common probability p of being a success, we\nmay write\n\nx = x1 + \u00b7\u00b7\u00b7 + xn\n\nwhere the xi are independent bernoulli random variables such that\n\n%\n\n1\n0\n\nxi =\n\nif the ith trial is a success\notherwise\n\n "}, {"Page_number": 341, "text": "326\n\nchapter 7\n\nproperties of expectation\n\nhence, from equation (4.1), we obtain\n\nvar(x) = var(x1) + \u00b7\u00b7\u00b7 + var(xn)\n\nbut\n\nthus,\n\nvar(xi) = e[x2\n\ni ] \u2212 (e[xi])2\n= e[xi] \u2212 (e[xi])2\n= p \u2212 p2\n\nsince x2\ni\n\n= xi\n\nvar(x) = np(1 \u2212 p)\n\n.\n\nexample 4c sampling from a finite population\nconsider a set of n people, each of whom has an opinion about a certain subject that\nis measured by a real number v that represents the person\u2019s \u201cstrength of\nfeeling\u201d about the subject. let vi represent the strength of feeling of person i,\ni = 1, . . . n.\n(cid:3)\n(cid:2)\nsuppose that the quantities vi, i = 1, . . . , n, are unknown and, to gather informa-\ntion, a group of n of the n people is \u201crandomly chosen\u201d in the sense that all of the\nn\nsubsets of size n are equally likely to be chosen. these n people are then ques-\nn\ntioned and their feelings determined. if s denotes the sum of the n sampled values,\ndetermine its mean and variance.\n\ni=1\n\nthen v = n(cid:9)\n\nan important application of the preceding problem is to a forthcoming election\nin which each person in the population is either for or against a certain candidate or\nproposition. if we take vi to equal 1 if person i is in favor and 0 if he or she is against,\nvi/n represents the proportion of the population that is in favor. to\nestimate v, a random sample of n people is chosen, and these people are polled. the\nproportion of those polled who are in favor\u2014that is, s/n\u2014is often used as an estimate\nof v.\nsolution. for each person i, i = 1, . . . , n, define an indicator variable ii to indicate\nwhether or not that person is included in the sample. that is,\n\n%\n\n1\n0\n\nii =\n\nnow, s can be expressed by\n\nso\n\nif person i is in the random sample\notherwise\n\ns = n(cid:6)\ne[s] = n(cid:6)\n\ni=1\n\ni=1\n\nviii\n\nvie[ii]\n\n "}, {"Page_number": 342, "text": "covariance, variance of sums, and correlations 327\n\n(cid:6)(cid:6)\n(cid:6)(cid:6)\n\ni<j\n\nvar(viii) + 2\n\ni var(ii) + 2\nv2\n\ncov(viii, vjij)\n\nvivjcov(ii, ij)\n\ni<j\n\nsection 7.4\n\nvar(s) = n(cid:6)\n= n(cid:6)\n\ni=1\n\ni=1\n\nbecause\n\nit follows that\n\ne[ii] = n\nn\ne[iiij] = n\nn\n(cid:2)\n\nn \u2212 1\nn \u2212 1\n(cid:3)\n\nvar(ii) = n\n1 \u2212 n\nn\nn\ncov(ii, ij) = n(n \u2212 1)\n\u2212\nn(n \u2212 1)\n= \u2212n(n \u2212 n)\nn2(n \u2212 1)\n\n(cid:3)2\n\n(cid:2)\n\nn\nn\n\nhence,\n\ne[s] = n\n\nvar(s) = n\nn\n\n(v1 + \u00b7\u00b7\u00b7 + vn)2 = n(cid:9)\n\n+ 2\n\nv2\ni\n\ni=1\n\ni<j\n\nn(cid:6)\n(cid:2)\n\ni=1\n\n= nv\nvi\n(cid:3) n(cid:6)\nn\nn \u2212 n\n(cid:9)(cid:9)\n\ni=1\n\nn\n\n\u2212 2n(n \u2212 n)\nn2(n \u2212 1)\n\nv2\ni\n\n(cid:6)(cid:6)\n\nvivj\n\ni<j\n\nthe expression for var(s) can be simplified somewhat by using the identity\n\nvivj. after some simplification, we obtain\n\n\u239b\n\u239c\u239c\u239d n(cid:6)\n\ni=1\nn\n\nv2\ni\n\n\u239e\n\u239f\u239f\u23a0\n\n\u2212 v2\n\nvar(s) = n(n \u2212 n)\nn \u2212 1\n\nconsider now the special case in which np of the v\u2019s are equal to 1 and the remain-\nder equal to 0. then, in this case, s is a hypergeometric random variable and has\nmean and variance given, respectively, by\n\nand\n\ne[s] = nv = np\n\nsince v = np\nn\n(cid:2)\n\n= p\n(cid:3)\n\nvar(s) = n(n \u2212 n)\nn \u2212 1\n= n(n \u2212 n)\nn \u2212 1\n\nnp\nn\n\n\u2212 p2\np(1 \u2212 p)\n\n "}, {"Page_number": 343, "text": "328\n\nchapter 7\n\nproperties of expectation\n\nthe quantity s/n, equal to the proportion of those sampled which have values equal\nto 1, is such that\n\n(cid:20)\n(cid:2)\n\ne\n\n(cid:21)\n(cid:3)\n\ns\nn\ns\nn\n\nvar\n\n= p\n= n \u2212 n\nn(n \u2212 1)\n\np(1 \u2212 p)\n\n.\n\nthe correlation of two random variables x and y, denoted by \u03c1(x, y), is defined,\n\nas long as var(x) var(y) is positive, by\n\u03c1(x, y) =\n\u221a\n\ncov(x, y)\nvar(x)var(y)\n\nit can be shown that\n\nto prove equation (4.2), suppose that x and y have variances given by \u03c3 2\nrespectively. then, on the one hand,\n\n(4.2)\nx and \u03c3 2\ny ,\n\n\u22121 \u2026 \u03c1(x, y) \u2026 1\n(cid:4)\n\n(cid:5)\n\n0 \u2026 var\n\n+ y\n\u03c3y\n\nx\n\u03c3x\n= var(x)\n\u03c3 2\ny\n= 2[1 + \u03c1(x, y)]\n\n\u03c3 2\nx\n\n+ var(y)\n\n+ 2cov(x, y)\n\n\u03c3x\u03c3y\n\nimplying that\n\non the other hand,\n\n(cid:4)\n\n\u22121 \u2026 \u03c1(x, y)\n\n(cid:5)\n\n0 \u2026 var\n\nx\n\u03c3x\n= var(x)\n= 2[1 \u2212 \u03c1(x, y)]\n\n\u2212 y\n\u03c3y\n+ vary\n(\u2212\u03c3y)2\n\n\u03c3 2\nx\n\n\u2212 2cov(x, y)\n\n\u03c3x\u03c3y\n\nimplying that\n\n\u03c1(x, y) \u2026 1\n\nwhich completes the proof of equation (4.2).\n\nin fact, since var(z) = 0 implies that z is constant with probability 1 (this intu-\nitive relationship will be rigorously proven in chapter 8), it follows from the proof of\nequation (4.2) that \u03c1(x, y) = 1 implies that y = a + bx, where b = \u03c3y/\u03c3x > 0 and\n\u03c1(x, y) = \u22121 implies that y = a + bx, where b = \u2212\u03c3y/\u03c3x < 0. we leave it as an\nexercise for the reader to show that the reverse is also true: that if y = a + bx, then\n\u03c1(x, y) is either +1 or \u22121, depending on the sign of b.\nthe correlation coefficient is a measure of the degree of linearity between x and y.\na value of \u03c1(x, y) near +1 or \u22121 indicates a high degree of linearity between\nx and y, whereas a value near 0 indicates that such linearity is absent. a positive\n\n "}, {"Page_number": 344, "text": "section 7.4\n\ncovariance, variance of sums, and correlations 329\n\nvalue of \u03c1(x, y) indicates that y tends to increase when x does, whereas a negative\nvalue indicates that y tends to decrease when x increases. if \u03c1(x, y) = 0, then x\nand y are said to be uncorrelated.\n\nexample 4d\nlet ia and ib be indicator variables for the events a and b. that is,\n\n%\n%\n\nia =\nib =\n\n1\n0\n1\n0\n\nif a occurs\notherwise\nif b occurs\notherwise\n\nthen\n\nso\n\ne[ia] = p(a)\ne[ib] = p(b)\ne[iaib] = p(ab)\n\ncov(ia, ib) = p(ab) \u2212 p(a)p(b)\n= p(b)[p(a|b) \u2212 p(a)]\n\nthus, we obtain the quite intuitive result that the indicator variables for a and b\nare either positively correlated, uncorrelated, or negatively correlated, depending on\nwhether p(a|b) is, respectively, greater than, equal to, or less than p(a).\n.\n\nour next example shows that the sample mean and a deviation from the sample\n\nmean are uncorrelated.\n\nexample 4e\nlet x1, . . . , xn be independent and identically distributed random variables having\nvariance \u03c3 2. show that\n\ncov(xi \u2212 x, x) = 0\n\nsolution. we have\n\ncov(xi \u2212 x, x) = cov(xi, x) \u2212 cov(x, x)\n\n\u239e\n\u239f\u23a0 \u2212 var(x)\n\nn(cid:6)\n\n\u239b\n\u239c\u239dxi,\n= cov\nn(cid:6)\ncov(xi, xj) \u2212 \u03c3 2\nn\n\nj=1\n\n1\nn\n\nxj\n\n= 1\nn\n= \u03c3 2\nn\n\nj=1\n\u2212 \u03c3 2\nn\n\n= 0\n\n "}, {"Page_number": 345, "text": "330\n\nchapter 7\n\nproperties of expectation\n\nwhere the next-to-last equality uses the result of example 4a and the final equality\nfollows because\n\n0\n\ncov(xi, xj) =\n\n0\n\u03c3 2\n\nif j z i by independence\nif j = i since var(xi) = \u03c3 2\n\nalthough x and the deviation xi \u2212 x are uncorrelated, they are not, in gen-\neral, independent. however, in the special case where the xi are normal random\nvariables, it turns out that not only is x independent of a single deviation, but it is\nindependent of the entire sequence of deviations xj \u2212 x, j = 1, . . . , n. this result\nwill be established in section 7.8, where we will also show that, in this case, the sam-\nple mean x and the sample variance s2 are independent, with (n \u2212 1)s2/\u03c3 2 having\na chi-squared distribution with n \u2212 1 degrees of freedom. (see example 4a for the\n.\ndefinition of s2.)\n\nexample 4f\nconsider m independent trials, each of which results in any of r possible outcomes\npi = 1. if we let ni, i = 1, . . . , r, denote the num-\nwith probabilities p1, p2, . . . , pr,\nber of the m trials that result in outcome i, then n1, n2, . . . , nr have the multinomial\ndistribution\n\nr(cid:9)\n\n1\n\np{n1 = n1, n2 = n2, . . . , nr = nr} =\n\nm!\n\nn1!n2! . . . nr!\n\npn1\n1 pn2\n2\n\n\u00b7\u00b7\u00b7 pnr\n\nr\n\nni = m\n\nfor i z j, it seems likely that when ni is large, nj would tend to be small; hence, it is\nintuitive that they should be negatively correlated. let us compute their covariance\nby using proposition 4.2(iv) and the representation\n\nr(cid:6)\n\ni=1\n\nwhere\n\nk=1\n\nni = m(cid:6)\n%\n%\n\nii(k) =\nij(k) =\n\n1\n0\n1\n0\n\nii(k)\n\nand\n\nij(k)\n\nnj = m(cid:6)\n\nk=1\n\nif trial k results in outcome i\notherwise\nif trial k results in outcome j\notherwise\n\nfrom proposition 4.2(iv), we have\n\ncov(ni, nj) = m(cid:6)\n\n(cid:7)=1\n\nm(cid:6)\n\nk=1\n\ncov(ii(k), ij((cid:7)))\n\nnow, on the one hand, when k z (cid:7),\n\ncov(ii(k), ij((cid:7))) = 0\n\n "}, {"Page_number": 346, "text": "section 7.5\n\nconditional expectation 331\n\nsince the outcome of trial k is independent of the outcome of trial (cid:7). on the other hand,\n\ncov(ii((cid:7)), ij((cid:7))) = e[ii((cid:7))ij((cid:7))] \u2212 e[ii((cid:7))]e[ij((cid:7))]\n\n= 0 \u2212 pipj = \u2212pipj\n\nwhere the equation uses the fact that ii((cid:7))ij((cid:7)) = 0, since trial (cid:7) cannot result in both\noutcome i and outcome j. hence, we obtain\n\ncov(ni, nj) = \u2212mpipj\n\nwhich is in accord with our intuition that ni and nj are negatively correlated.\n\n.\n\n7.5 conditional expectation\n\n7.5.1 definitions\nrecall that if x and y are jointly discrete random variables, then the conditional\nprobability mass function of x, given that y = y, is defined, for all y such that\np{y = y} > 0, by\n\npx|y (x|y) = p{x = x|y = y} = p(x, y)\npy (y)\n\nit is therefore natural to define, in this case, the conditional expectation of x given\nthat y = y, for all values of y such that py (y) > 0, by\n\ne[x|y = y] =\n=\n\nxp{x = x|y = y}\nxpx|y (x|y)\n\n(cid:6)\n(cid:6)\n\nx\n\nx\n\nexample 5a\nif x and y are independent binomial random variables with identical parameters n\nand p, calculate the conditional expected value of x given that x + y = m.\nsolution. let us first calculate the conditional probability mass function of x given\nthat x + y = m. for k \u2026 min(n, m),\n\np{x = k|x + y = m} = p{x = k, x + y = m}\np{x + y = m}\n= p{x = k, y = m \u2212 k}\np{x + y = m}\n= p{x = k}p{y = m \u2212 k}\n(cid:3)\n(cid:2)\n(cid:2)\np{x + y = m}\n(cid:3)\n(cid:2)\npk(1 \u2212 p)n\u2212k\n(cid:3)(cid:2)\n(cid:3)\n2n\nm\n(cid:2)\n(cid:3)\nm \u2212 k\n2n\nm\n\n(cid:2)\n\n=\n\n=\n\nn\nk\n\nn\nk\n\nn\n\nn\n\n(cid:3)\nm \u2212 k\npm(1 \u2212 p)2n\u2212m\n\npm\u2212k(1 \u2212 p)n\u2212m+k\n\n "}, {"Page_number": 347, "text": "332\n\nchapter 7\n\nproperties of expectation\nwhere we have used the fact (see example 3f of chapter 6) that x + y is a binomial\nrandom variable with parameters 2n and p. hence, the conditional distribution of x,\ngiven that x + y = m, is the hypergeometric distribution, and from example 2g, we\nobtain\n\ne[x|x + y = m] = m\n2\n\n.\n\nsimilarly, let us recall that if x and y are jointly continuous with a joint probabil-\nity density function f (x, y), then the conditional probability density of x, given that\ny = y, is defined, for all values of y such that fy (y) > 0, by\n\nfx|y (x|y) = f (x, y)\nfy (y)\n\nit is natural, in this case, to define the conditional expectation of x, given that y =\ny, by\n\n* q\n\n\u2212q\n\ne[x|y = y] =\n\nxfx|y (x|y) dx\n\nprovided that fy (y) > 0.\n\nexample 5b\nsuppose that the joint density of x and y is given by\n\nf (x, y) = e\n\n\u2212y\n\n\u2212x/ye\ny\n\n0 < x < q, 0 < y < q\n\ncompute e[x|y = y].\n\nsolution. we start by computing the conditional density\n\n=\n\nfx|y (x|y) = f (x, y)\n* q\nfy (y)\n* q\n* q\n\n\u2212q\n\n=\n\n=\n\n0\n\nf (x, y)\n\n(1/y)e\n\n(1/y)e\n\nf (x, y) dx\n\u2212x/ye\n\u2212y\n\u2212x/ye\n\u2212y dx\n\u2212x/y\n\u2212x/y dx\n\n(1/y)e\n\n0\n\n(1/y)e\n\u2212x/y\ne\n\n= 1\ny\n\n "}, {"Page_number": 348, "text": "conditional expectation 333\nhence, the conditional distribution of x, given that y = y, is just the exponential\ndistribution with mean y. thus,\n\nsection 7.5\n\ne[x|y = y] =\n\n\u2212x/y dx = y\ne\n\nx\ny\n\n.\n\n* q\n\n0\n\nremark.\n\njust as conditional probabilities satisfy all of the properties of ordinary\nprobabilities, so do conditional expectations satisfy the properties of ordinary expec-\ntations. for instance, such formulas as\n\ne[g(x)|y = y] =\n\nand\n\ne\n\n(cid:6)\ng(x)px|y (x|y)\n* q\n\nx\n\ng(x)fx|y (x|y) dx\n\n\u2212q\n\n\u23a4\n\u23a6 = n(cid:6)\nxi|y = y\n\n\u23a7\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23a9\n\u23a1\n\u23a3 n(cid:6)\n\ni=1\n\ni=1\n\ne[xi|y = y]\n\nin the discrete case\n\nin the continuous case\n\nremain valid. as a matter of fact, conditional expectation given that y = y can be\nthought of as being an ordinary expectation on a reduced sample space consisting\nonly of outcomes for which y = y.\n.\n\n7.5.2 computing expectations by conditioning\nlet us denote by e[x|y] that function of the random variable y whose value at y = y\nis e[x|y = y]. note that e[x|y] is itself a random variable. an extremely important\nproperty of conditional expectations is given by the following proposition.\n\nproposition 5.1.\n\nif y is a discrete random variable, then equation (5.1) states that\n\ne[x] = e[e[x|y]]\n(cid:6)\n\ny\n\ne[x|y = y]p{y = y}\n* q\n\ne[x|y = y]fy (y) dy\n\n\u2212q\n\ne[x] =\n\ne[x] =\n\n(5.1)\n\n(5.1a)\n\n(5.1b)\n\nwhereas if y is continuous with density fy (y), then equation (5.1) states\n\nwe now give a proof of equation (5.1) in the case where x and y are both discrete\nrandom variables.\n\nproof of equation (5.1) when x and y are discrete: we must show that\n\ne[x] =\n\ne[x|y = y]p{y = y}\n\n(5.2)\n\n(cid:6)\n\ny\n\n "}, {"Page_number": 349, "text": "334\n\nchapter 7\n\nproperties of expectation\n\nnow, the right-hand side of equation (5.2) can be written as\n\ne[x|y = y]p{y = y} =\n\n(cid:6)\n\ny\n\nx\n\nxp{x = x|y = y}p{y = y}\np{x = x, y = y}\n\n(cid:6)\n(cid:6)\n(cid:6)\nx\nxp{x = x, y = y}\n(cid:6)\np{x = x, y = y}\n\np{y = y}\n\np{y = y}\n\nx\n\nx\n\nx\nxp{x = x}\n\ny\n\ny\n\ny\n\n=\n\n(cid:6)\n(cid:6)\n(cid:6)\n(cid:6)\n(cid:6)\n=\n= e[x]\n\n=\n\n=\n\nx\n\nx\n\ny\n\nand the result is proved.\n\none way to understand equation (5.2) is to interpret it as follows: to calculate\ne[x], we may take a weighted average of the conditional expected value of x given\nthat y = y, each of the terms e[x|y = y] being weighted by the probability of\nthe event on which it is conditioned. (of what does this remind you?) this is an\nextremely useful result that often enables us to compute expectations easily by first\nconditioning on some appropriate random variable. the following examples illustrate\nits use.\n\nexample 5c\na miner is trapped in a mine containing 3 doors. the first door leads to a tunnel that\nwill take him to safety after 3 hours of travel. the second door leads to a tunnel that\nwill return him to the mine after 5 hours of travel. the third door leads to a tunnel\nthat will return him to the mine after 7 hours. if we assume that the miner is at all\ntimes equally likely to choose any one of the doors, what is the expected length of\ntime until he reaches safety?\n\nsolution. let x denote the amount of time (in hours) until the miner reaches safety,\nand let y denote the door he initially chooses. now,\n\ne[x] = e[x|y = 1]p{y = 1} + e[x|y = 2]p{y = 2}\n\n+ e[x|y = 3]p{y = 3}\n(e[x|y = 1] + e[x|y = 2] + e[x|y = 3])\n\n= 1\n3\n\nhowever,\n\ne[x|y = 1] = 3\ne[x|y = 2] = 5 + e[x]\ne[x|y = 3] = 7 + e[x]\n\n(5.3)\n\nto understand why equation (5.3) is correct, consider, for instance, e[x|y = 2]\nand reason as follows: if the miner chooses the second door, he spends 5 hours in\nthe tunnel and then returns to his cell. but once he returns to his cell, the prob-\nlem is as before; thus his expected additional time until safety is just e[x]. hence,\n\n "}, {"Page_number": 350, "text": "conditional expectation 335\ne[x|y = 2] = 5 + e[x]. the argument behind the other equalities in equation (5.3)\nis similar. hence,\n\nsection 7.5\n\ne[x] = 1\n3\n\n(3 + 5 + e[x] + 7 + e[x])\n\nor\n\ne[x] = 15\n\n.\n\nexample 5d expectation of a sum of a random number of random variables\nsuppose that the number of people entering a department store on a given day is\na random variable with mean 50. suppose further that the amounts of money spent\nby these customers are independent random variables having a common mean of $8.\nfinally, suppose also that the amount of money spent by a customer is also inde-\npendent of the total number of customers who enter the store. what is the expected\namount of money spent in the store on a given day?\n\nsolution. if we let n denote the number of customers that enter the store and xi the\namount spent by the ith such customer, then the total amount of money spent can be\n\nn(cid:9)\n\ni=1\n\nxi. now,\n\n\u23a4\n\u23a6 = e\nxi|n = n\n\nexpressed as\n\nbut\n\n\u23a1\n\u23a3 n(cid:6)\n\n1\n\ne\n\nwhich implies that\n\nthus,\n\ne\n\n1\n\n1\n\n1\n\ne\n\nxi\n\n\u23a4\n\u23a6\n\n\u23a4\n\u23a5\u23a6\n\nxi|n\n\n\u23a4\n\u23a6 = e\n\n\u23a1\n\u23a1\n\u23a3 n(cid:6)\n\u23a2\u23a3e\n\u23a4\n\u23a6\nxi|n = n\n\u23a4\n\u23a6 by the independence of the xi and n\n\n\u23a1\n\u23a3 n(cid:6)\n\u23a1\n\u23a3 n(cid:6)\n\u23a1\n\u23a3 n(cid:6)\n= e\n= ne[x] where e[x] = e[xi]\n\u23a4\n\u23a1\n\u23a3 n(cid:6)\n\u23a6 = ne[x]\n\u23a4\n\u23a6 = e[ne[x]] = e[n]e[x]\n\n\u23a1\n\u23a3 n(cid:6)\n\nxi|n\n\nxi\n\ne\n\n1\n\n1\n\nxi\n\ni=1\n\nhence, in our example, the expected amount of money spent in the store is 50 * $8,\n.\nor $400.\n\nexample 5e\nthe game of craps is begun by rolling an ordinary pair of dice. if the sum of the dice is\n2, 3, or 12, the player loses. if it is 7 or 11, the player wins. if it is any other number i,\n\n "}, {"Page_number": 351, "text": "336\n\nchapter 7\n\nproperties of expectation\n\nthe player continues to roll the dice until the sum is either 7 or i. if it is 7, the player\nloses; if it is i, the player wins. let r denote the number of rolls of the dice in a game\nof craps. find\n(a) e[r];\n(b) e[r|player wins];\n(c) e[r|player loses].\n\nsolution. if we let pi denote the probability that the sum of the dice is i, then\n\nto compute e[r], we condition on s, the initial sum, giving\n\ni = 2, . . . , 7\n\n,\n\ne[r|s = i]pi\n\n36\n\npi = p14\u2212i = i \u2212 1\ne[r] = 12(cid:6)\n\u23a7\u23a8\n\u23a91,\n1 +\n\ni=2\n\n1\n\npi + p7\n\ne[r|s = i] =\n\nif i = 2, 3, 7, 11, 12\notherwise\n\n,\n\nhowever,\n\nthe preceding equation follows because if the sum is a value i that does not end\nthe game, then the dice will continue to be rolled until the sum is either i or 7, and\nthe number of rolls until this occurs is a geometric random variable with parameter\npi + p7. therefore,\n\ne[r] = 1 + 6(cid:6)\n\n+ 10(cid:6)\n\ni=8\n\npi\n\npi + p7\n\ni=4\n\npi\n\npi + p7\n\n= 1 + 2(3/9 + 4/10 + 5/11) = 3.376\n\nto determine e[r|win], let us start by determining p, the probability that the player\nwins. conditioning on s yields\n\np{win|s = i}pi\n\np = 12(cid:6)\n= p7 + p11 + 6(cid:6)\n\ni=2\n\n= 0.493\n\npi + 10(cid:6)\n\ni=8\n\npi\n\npi + p7\n\npi\n\npi\n\npi + p7\n\ni=4\n\nwhere the preceding uses the fact that the probability of obtaining a sum of i before\none of 7 is pi/(pi + p7). now, let us determine the conditional probability mass\nfunction of s, given that the player wins. letting qi = p{s = i|win}, we have\n\nq2 = q3 = q12 = 0, q7 = p7/p, q11 = p11/p\n\n "}, {"Page_number": 352, "text": "and, for i = 4, 5, 6, 8, 9, 10,\n\nsection 7.5\n\nconditional expectation 337\n\nqi = p{s = i, win}\np{win}\n= pip{win|s = i}\n\n=\n\np\np2\ni\n\np(pi + p7)\n(cid:6)\n\ni\n\nnow, conditioning on the initial sum gives\n\ne[r|win] =\n\ne[r|win, s = i]qi\n\nhowever, as was noted in example 2j of chapter 6, given that the initial sum is i,\nthe number of additional rolls needed and the outcome (whether a win or a loss) are\nindependent. (this is easily seen by first noting that, conditional on an initial sum\nof i, the outcome is independent of the number of additional dice rolls needed and\nthen using the symmetry property of independence, which states that if event a is\nindependent of event b, then event b is independent of event a.) therefore,\n\ne[r|win] =\n\ne[r|s = i]qi\n\n+ 10(cid:6)\n\ni=8\n\nqi\n\npi + p7\n\nqi\n\npi + p7\n\n(cid:6)\n= 1 + 6(cid:6)\n\ni\n\ni=4\n\n= 2.938\n\nalthough we could determine e[r|player loses] exactly as we did e[r|player wins],\n\nit is easier to use\n\nimplying that\n\ne[r] = e[r|win]p + e[r|lose](1 \u2212 p)\n\ne[r|lose] = e[r] \u2212 e[r|win]p\n\n1 \u2212 p\n\n= 3.801\n\n.\n\nexample 5f\nas defined in example 5c of chapter 6, the bivariate normal joint density function of\nthe random variables x and y is\n\n.\n\n1\n\nf (x, y) =\n\n2\u03c0 \u03c3x\u03c3y\n\n1 \u2212 \u03c12\n\nexp\n\n1\n\n2(1 \u2212 \u03c12)\n\n\u23a7\u23aa\u23a8\n\u23aa\u23a9\u2212\n\n\u23a1\n\u23a3(cid:2)\n\n(cid:4)\n\n(cid:3)2 +\n\nx \u2212 \u03bcx\n\n\u03c3x\n\u03c3y\n(x \u2212 \u03bcx)(y \u2212 \u03bcy)\n\n\u03c3x\u03c3y\n\n\u2212 2\u03c1\n\n(cid:5)2\n\u23ab\u23aa\u23ac\n\u23aa\u23ad\n\ny \u2212 \u03bcy\n\u23a4\n\u23a6\n\n "}, {"Page_number": 353, "text": "338\n\nchapter 7\n\nproperties of expectation\n\nwe will now show that \u03c1 is the correlation between x and y. as shown in exam-\nple 5c, \u03bcx = e[x], \u03c3 2\n\n= var(y). consequently,\n\nx\n\n= var(x), and \u03bcy = e[y], \u03c3 2\ncorr(x, y) = cov(x, y)\n\ny\n\nto determine e[xy], we condition on y. that is, we use the identity\n\nrecalling from example 5c that the conditional distribution of x given that y = y is\nnormal with mean \u03bcx + \u03c1 \u03c3x\n\n\u03c3x\u03c3y\n\n\u03c3x\u03c3y\n\n= e[xy] \u2212 \u03bcx\u03bcy\n3\n\n2\ne[xy|y]\n\ne[xy] = e\n\n\u03c3y\n\n(y \u2212 \u03bcy), we see that\ne[xy|y = y] = e[xy|y = y]\n= ye[x|y = y]\n\u03bcx + \u03c1\n(y \u2212 \u03bcy)\n= y\n\u03c3x\n\u03c3y\n= y\u03bcx + \u03c1\n(y2 \u2212 \u03bcyy)\n\u03c3x\n\u03c3y\n\n(cid:7)\n\n(cid:8)\n\nconsequently,\n\nimplying that\n\ne[xy|y] = y\u03bcx + \u03c1\n\n(cid:7)\n\ne[xy] = e\n\ny\u03bcx + \u03c1\n= \u03bcxe[y] + \u03c1\n= \u03bcx\u03bcy + \u03c1\n\u03c3x\n\u03c3y\n= \u03bcx\u03bcy + \u03c1\n\u03c3x\n\u03c3y\n= \u03bcx\u03bcy + \u03c1\u03c3x\u03c3y\n\n\u03c3x\n\u03c3y\n\n(y2 \u2212 \u03bcyy)\n(cid:8)\n\n(y2 \u2212 \u03bcyy)\n\u03c3x\n\u03c3y\n(cid:30)\ne[y2 \u2212 \u03bcyy]\n\u03c3x\n\u03c3y\ne[y2] \u2212 \u03bc2\n\n(cid:29)\n\ny\n\nvar(y)\n\ntherefore,\n\ncorr(x, y) = \u03c1\u03c3x\u03c3y\n\u03c3x\u03c3y\n\n= \u03c1\n\n.\n\nsometimes e[x] is easy to compute, and we use the conditioning identity to com-\npute a conditional expected value. this approach is illustrated by our next example.\n\nexample 5g\nconsider n independent trials, each of which results in one of the outcomes 1, . . . , k,\ni=1 pi = 1. let ni denote the number of\nwith respective probabilities p1, . . . , pk,\ntrials that result in outcome i, i = 1, . . . , k. for i z j, find\n\nk\n\n(cid:9)\n\n(a) e[nj|ni > 0]\n\nand\n\n(b) e[nj|ni > 1]\n\n "}, {"Page_number": 354, "text": "solution. to solve (a), let\n\n%\n\ni =\n\nsection 7.5\n\nconditional expectation 339\n\nif ni = 0\nif ni > 0\n\n0,\n1,\n\nthen\n\ne[nj] = e[nj|i = 0]p{i = 0} + e[nj|i = 1]p{i = 1}\n\nor, equivalently,\n\ne[nj] = e[nj|ni = 0]p{ni = 0} + e[nj|ni > 0]p{ni > 0}\n\nnow, the unconditional distribution of nj is binomial with parameters n, pj. also,\ngiven that ni = r, each of the n \u2212 r trials that do not result in outcome i will,\nindependently, result in outcome j with probability p(j|not i) = pj\n1\u2212pi\n. consequently,\nthe conditional distribution of nj, given that ni = r, is binomial with parameters\nn \u2212 r, pj\n1\u2212pi\n. (for a more detailed argument for this conclusion, see example 4c of\nchapter 6.) because p{ni = 0} = (1 \u2212 pi)n, the preceding equation yields\n\nnpj = n\n\npj\n\n1 \u2212 pi\n\n(1 \u2212 pi)n + e[nj|ni > 0](1 \u2212 (1 \u2212 pi)n\n\ngiving the result\n\ne[nj|ni > 0] = npj\n\n1 \u2212 (1 \u2212 pi)n\u22121\n1 \u2212 (1 \u2212 pi)n\n\nwe can solve part (b) in a similar manner. let\nif ni = 0\nif ni = 1\nif ni > 1\n\nj =\n\n1,\n2,\n\n\u23a7\u23a8\n\u23a9 0,\n\nthen\n\ne[nj] = e[nj|j = 0]p{j = 0} + e[nj|j = 1]p{j = 1}\n\n+ e[nj|j = 2]p{j = 2}\n\nor, equivalently,\n\ne[nj] = e[nj|ni = 0]p{ni = 0} + e[nj|ni = 1]p{ni = 1}\n\n+ e[nj|ni > 1]p{ni > 1}\n\nthis equation yields\nnpj = n\n\npj\n\n(1 \u2212 pi)n + (n \u2212 1)\n\nnpi(1 \u2212 pi)n\u22121\n1 \u2212 pi\n+ e[nj|ni > 1](1 \u2212 (1 \u2212 pi)n \u2212 npi(1 \u2212 pi)n\u22121)\n\n1 \u2212 pi\n\npj\n\ngiving the result\n\ne[nj|ni > 1] = npj[1 \u2212 (1 \u2212 pi)n\u22121 \u2212 (n \u2212 1)pi(1 \u2212 pi)n\u22122]\n\n1 \u2212 (1 \u2212 pi)n \u2212 npi(1 \u2212 pi)n\u22121\n\n.\n\nit is also possible to obtain the variance of a random variable by conditioning. we\n\nillustrate this approach by the following example.\n\n "}, {"Page_number": 355, "text": "340\n\nchapter 7\n\nproperties of expectation\n\nexample 5h variance of the geometric distribution\nindependent trials, each resulting in a success with probability p, are successively\nperformed. let n be the time of the first success. find var(n).\nsolution. let y = 1 if the first trial results in a success and y = 0 otherwise. now,\n\nvar(n) = e[n2] \u2212 (e[n])2\n\nto calculate e[n2], we condition on y as follows:\n\nhowever,\n\ne[n2] = e[e[n2|y]]\n\ne[n2|y = 1] = 1\ne[n2|y = 0] = e[(1 + n)2]\n\nthese two equations follow because, on the one hand, if the first trial results in a\nsuccess, then, clearly, n = 1; thus, n2 = 1. on the other hand, if the first trial results\nin a failure, then the total number of trials necessary for the first success will have the\nsame distribution as 1 (the first trial that results in failure) plus the necessary number\nof additional trials. since the latter quantity has the same distribution as n, we obtain\ne[n2|y = 0] = e[(1 + n)2]. hence,\n\ne[n2] = e[n2|y = 1]p{y = 1} + e[n2|y = 0]p{y = 0}\n\n= p + (1 \u2212 p)e[(1 + n)2]\n= 1 + (1 \u2212 p)e[2n + n2]\n\nhowever, as was shown in example 8b of chapter 4, e[n] = 1/p; therefore,\n\ne[n2] = 1 + 2(1 \u2212 p)\n\np\n\n+ (1 \u2212 p)e[n2]\n\nor\n\nconsequently,\n\ne[n2] = 2 \u2212 p\n\np2\n\n(cid:3)2\nvar(n) = e[n2] \u2212 (e[n])2\n\n(cid:2)\n\n\u2212\n\n1\np\n\n= 2 \u2212 p\np2\n= 1 \u2212 p\np2\n\n.\n\nexample 5i\nconsider a gambling situation in which there are r players, with player i initially hav-\ning ni units, ni > 0, i = 1,..., r. at each stage, two of the players are chosen to play\na game, with the winner of the game receiving 1 unit from the loser. any player\nwhose fortune drops to 0 is eliminated, and this continues until a single player has all\nn k\nr\ni=1 ni units, with that player designated as the victor. assuming that the results\nof successive games are independent and that each game is equally likely to be won\n\n(cid:9)\n\n "}, {"Page_number": 356, "text": "section 7.5\n\nconditional expectation 341\n\nby either of its two players, find the average number of stages until one of the players\nhas all n units.\n\nsolution. to find the expected number of stages played, suppose first that there are\nonly 2 players, with players 1 and 2 initially having j and n \u2212 j units, respectively.\nlet xj denote the number of stages that will be played, and let mj = e[xj]. then, for\nj = 1,..., n \u2212 1,\n\nxj = 1 + aj\n\nwhere aj is the additional number of stages needed beyond the first stage. taking\nexpectations gives\n\nmj = 1 + e[aj]\n\nconditioning on the result of the first stage then yields\n\nmj = 1 + e[aj|1 wins first stage]1/2 + e[aj|2 wins first stage]1/2\n\nnow, if player 1 wins at the first stage, then the situation from that point on is exactly\nthe same as in a problem which supposes that player 1 starts with j + 1 and player 2\nwith n \u2212 ( j + 1) units. consequently,\n\ne[aj|1 wins first stage] = mj+1\n\ne[aj|2 wins first stage] = mj\u22121\n\nmj = 1 + 1\n2\n\nmj+1 + 1\n2\n\nmj\u22121\n\nand, analogously,\n\nthus,\n\nor, equivalently,\n\nmj+1 = 2mj \u2212 mj\u22121 \u2212 2,\n\nj = 1, . . . , n \u2212 1\n\n(5.4)\n\nusing that m0 = 0, the preceding equation yields\n\nm2 = 2m1 \u2212 2\nm3 = 2m2 \u2212 m1 \u2212 2 = 3m1 \u2212 6 = 3(m1 \u2212 2)\nm4 = 2m3 \u2212 m2 \u2212 2 = 4m1 \u2212 12 = 4(m1 \u2212 3)\n\nsuggesting that\n\nmi = i(m1 \u2212 i + 1),\n\ni = 1, . . . , n\n\n(5.5)\n\nto prove the preceding equality, we use mathematical induction. since we\u2019ve already\nshown the equation to be true for i = 1, 2, we take as the induction hypothesis that\nit is true whenever i \u2026 j < n. now we must prove that it is true for j + 1. using\nequation (5.4) yields\nmj+1 = 2mj \u2212 mj\u22121 \u2212 2\n\n= 2j(m1 \u2212 j + 1) \u2212 (j \u2212 1)(m1 \u2212 j + 2) \u2212 2 (by the induction hypothesis)\n= (j + 1)m1 \u2212 2j2 + 2j + j2 \u2212 3j + 2 \u2212 2\n= (j + 1)m1 \u2212 j2 \u2212 j\n= (j + 1)(m1 \u2212 j)\n\n "}, {"Page_number": 357, "text": "342\n\nchapter 7\n\nproperties of expectation\nwhich completes the induction proof of (5.5). letting i = n in (5.5), and using that\nmn = 0, now yields that\n\nm1 = n \u2212 1\n\nwhich, again using (5.5), gives the result\n\nmi = i(n \u2212 i)\n\nr\n\nthus, the mean number of games played when there are only 2 players with initial\namounts i and n \u2212 i is the product of their initial amounts. because both players play\n(cid:9)\nall stages, this is also the mean number of stages involving player 1.\nnow let us return to the problem involving r players with initial amounts ni, i =\ni=1 ni = n. let x denote the number of stages needed to obtain a victor,\n1,..., r,\nand let xi denote the number of stages involving player i. now, from the point of\nview of player i, starting with ni, he will continue to play stages, independently being\nequally likely to win or lose each one, until his fortune is either n or 0. thus, the\nnumber of stages he plays is exactly the same as when he has a single opponent with\nan initial fortune of n \u2212 ni. consequently, by the preceding result it follows that\n\nso\n\n\u23a1\n\u23a3 r(cid:6)\n\ni=1\n\ne\n\nxi\n\n\u23a4\n\u23a6 = r(cid:6)\n\ni=1\n\nbut because each stage involves two players,\n\ne[xi] = ni(n \u2212 ni)\n\nni(n \u2212 ni) = n2 \u2212 r(cid:6)\nr(cid:6)\n\ni=1\n\nn2\ni\n\ntaking expectations now yields\n\nxi\n\ni=1\n\nx = 1\n2\n\u239b\n\u239dn2 \u2212 r(cid:6)\n\ni=1\n\n\u239e\n\u23a0\n\nn2\ni\n\ne[x] = 1\n2\n\nit is interesting to note that while our argument shows that the mean number of stages\ndoes not depend on the manner in which the teams are selected at each stage, the\nsame is not true for the distribution of the number of stages. to see this, suppose\nr = 3, n1 = n2 = 1, and n3 = 2. if players 1 and 2 are chosen in the first stage, then it\nwill take at least three stages to determine a winner, whereas if player 3 is in the first\n.\nstage, then it is possible for there to be only two stages.\n\nin our next example, we use conditioning to verify a result previously noted in\nsection 6.3.1: that the expected number of uniform (0, 1) random variables that need\nto be added for their sum to exceed 1 is equal to e.\n\nexample 5j\nlet u1, u2, . . . be a sequence of independent uniform (0, 1) random variables. find\ne[n] when\n\n\u23a7\u23a8\n\u23a9n:\n\nn(cid:6)\n\nui > 1\n\ni=1\n\n\u23ab\u23ac\n\u23ad\n\nn = min\n\n "}, {"Page_number": 358, "text": "*\n\nm(x) =\n\nsolution. we will find e[n] by obtaining a more general result. for x \u2208 [0, 1], let\n\nsection 7.5\n\nconditional expectation 343\n\n\u23a7\u23a8\n\u23a9n:\n\nn(cid:6)\n\nui > x\n\ni=1\n\n\u23ab\u23ac\n\u23ad\n\nn(x) = min\n\nand set\n\nm(x) = e[n(x)]\n\nthat is, n(x) is the number of uniform (0, 1) random variables we must add until\ntheir sum exceeds x, and m(x) is its expected value. we will now derive an equation\nfor m(x) by conditioning on u1. this gives, from equation (5.1b),\n\nnow,\n\ne[n(x)|u1 = y] =\n\n1\n\n0\n\ne[n(x)|u1 = y] dy\n%\n1\n1 + m(x \u2212 y)\n\nif y > x\nif y \u2026 x\n\n(5.6)\n\n(5.7)\n\nthe preceding formula is obviously true when y > x. it is also true when y \u2026 x, since,\nif the first uniform value is y, then, at that point, the remaining number of uniform\nrandom variables needed is the same as if we were just starting and were going to add\nuniform random variables until their sum exceeded x \u2212 y. substituting equation (5.7)\ninto equation (5.6) gives\n\n*\n*\n\n0\n\n0\n\nm(x) = 1 +\n= 1 +\n\nx\n\nx\n\nm(x \u2212 y) dy\n\nm(u)du\n\nby letting\nu = x \u2212 y\n\ndifferentiating the preceding equation yields\n\nor, equivalently,\n\nintegrating this equation gives\n\n(cid:8)(x) = m(x)\nm\n\n(cid:8)(x)\nm\nm(x)\n\n= 1\n\nlog[m(x)] = x + c\n\nor\n\nm(x) = kex\nsince m(0) = 1, it follows that k = 1, so we obtain\n\nm(x) = ex\n\ntherefore, m(1), the expected number of uniform (0, 1) random variables that need\n.\nto be added until their sum exceeds 1, is equal to e.\n\n "}, {"Page_number": 359, "text": "344\n\nchapter 7\n\nproperties of expectation\n\n7.5.3 computing probabilities by conditioning\nnot only can we obtain expectations by first conditioning on an appropriate random\nvariable, but we may also use this approach to compute probabilities. to see this, let\ne denote an arbitrary event, and define the indicator random variable x by\n\n%\n\nx =\n\n1\n0\n\nif e occurs\nif e does not occur\n\nit follows from the definition of x that\n\ntherefore, from equations (5.1a) and (5.1b), we obtain\n\nfor any random variable y\n\ne[x|y = y] = p(e|y = y)\n\ne[x] = p(e)\n(cid:6)\np(e|y = y)p(y = y)\n* q\n\ny\n\np(e) =\n\nif y is discrete\n\n=\n\n\u2212q\n\np(e|y = y)fy (y)dy if y is continuous\n\n(5.8)\n\nnote that if y is a discrete random variable taking on one of the values y1, . . . , yn,\nthen, by defining the events fi, i = 1, . . . , n, by fi = {y = yi}, equation (5.8) reduces\nto the familiar equation\n\np(e) = n(cid:6)\n\ni=1\n\np(e|fi)p(fi)\n\nwhere f1, . . . , fn are mutually exclusive events whose union is the sample space.\n\nexample 5k the best-prize problem\nsuppose that we are to be presented with n distinct prizes, in sequence. after being\npresented with a prize, we must immediately decide whether to accept it or to reject\nit and consider the next prize. the only information we are given when deciding\nwhether to accept a prize is the relative rank of that prize compared to ones already\nseen. that is, for instance, when the fifth prize is presented, we learn how it compares\nwith the four prizes we\u2019ve already seen. suppose that once a prize is rejected, it is\nlost, and that our objective is to maximize the probability of obtaining the best prize.\nassuming that all n! orderings of the prizes are equally likely, how well can we do?\nsolution. rather surprisingly, we can do quite well. to see this, fix a value k, 0 \u2026\nk < n, and consider the strategy that rejects the first k prizes and then accepts the\nfirst one that is better than all of those first k. let pk(best) denote the probability that\nthe best prize is selected when this strategy is employed. to compute this probability,\ncondition on x, the position of the best prize. this gives\n\npk(best) = n(cid:6)\npk(best|x = i)p(x = i)\nn(cid:6)\n\npk(best|x = i)\n\ni=1\n= 1\nn\n\ni=1\n\n "}, {"Page_number": 360, "text": "section 7.5\n\nconditional expectation 345\n\nnow, on the one hand, if the overall best prize is among the first k, then no prize is\never selected under the strategy considered. that is,\n\npk(best|x = i) = 0 if i \u2026 k\n\non the other hand, if the best prize is in position i, where i > k, then the best prize\nwill be selected if the best of the first i \u2212 1 prizes is among the first k (for then none\nof the prizes in positions k + 1, k + 2, . . . , i \u2212 1 would be selected). but, conditional\non the best prize being in position i, it is easy to verify that all possible orderings of\nthe other prizes remain equally likely, which implies that each of the first i \u2212 1 prizes\nis equally likely to be the best of that batch. hence, we have\n\npk(best|x = i) = p{best of first i \u2212 1 is among the first k|x = i}\n\n= k\ni \u2212 1\n\nif i > k\n\nfrom the preceding, we obtain\n\npk(best) = k\nn\nl k\nn\n= k\nn\nl k\nn\n\nn(cid:6)\n*\n\ni=k+1\nn\nk+1\nlog\n\n1\ni \u2212 1\n1\n\n(cid:2)\nx \u2212 1\nn \u2212 1\n(cid:2)\n(cid:3)\n\nk\n\nlog\n\ndx\n\n(cid:3)\n\nnow, if we consider the function\n\ng(x) = x\nn\n\nthen\n\nso\n\ng\n\ng\n\n(cid:8)(x) = 1\nn\n(cid:8)(x) = 0 * log\n\nlog\n\n(cid:2)\n\nlog\n\n(cid:2)\n(cid:3)\n\nn\nx\n\nn\nx\n\n\u2212 1\nn\n= 1 * x = n\n\ne\n\n(cid:3)\n\nn\nk\n\n(cid:2)\n(cid:3)\n\nn\nx\n\nthus, since pk(best) l g(k), we see that the best strategy of the type considered is to\nlet the first n/e prizes go by and then accept the first one to appear that is better than\nall of those. in addition, since g(n/e) = 1/e, the probability that this strategy selects\nthe best prize is approximately 1/e l .36788.\n\nremark. most people are quite surprised by the size of the probability of obtain-\ning the best prize, thinking that this probability would be close to 0 when n is large.\nhowever, even without going through the calculations, a little thought reveals that\nthe probability of obtaining the best prize can be made reasonably large. consider\nthe strategy of letting half of the prizes go by and then selecting the first one to appear\nthat is better than all of those. the probability that a prize is actually selected is the\nprobability that the overall best is among the second half, and this is 1\n2. in addition,\ngiven that a prize is selected, at the time of selection that prize would have been\n\n "}, {"Page_number": 361, "text": "346\n\nchapter 7\n\nproperties of expectation\n\nthe best of more than n/2 prizes to have appeared and would thus have probability of\nat least 1\n2 of being the overall best. hence, the strategy of letting the first half of all\nprizes go by and then accepting the first one that is better than all of those prizes has\n.\na probability greater than 1\n\n4 of obtaining the best prize.\n\nexample 5l\nlet u be a uniform random variable on (0, 1), and suppose that the conditional dis-\ntribution of x, given that u = p, is binomial with parameters n and p. find the\nprobability mass function of x.\n\nsolution. conditioning on the value of u gives\n\n*\n*\n\n1\n\n1\n\n0\n\n0\n\np{x = i} =\n=\n\n=\n\np{x = i|u = p}fu (p) dp\np{x = i|u = p} dp\nn!\n\n*\n\n1\n\npi(1 \u2212 p)n\u2212i dp\n\ni!(n \u2212 i)!\n\n0\n\n*\n\n1\n\npi(1 \u2212 p)n\u2212idp = i!(n \u2212 i)!\n(n + 1)!\n\n0\n\nnow, it can be shown (a probabilistic proof is given in section 6.6) that\n\nhence, we obtain\n\np{x = i} = 1\n\nn + 1\n\ni = 0, . . . , n\n\nthat is, we obtain the surprising result that if a coin whose probability of coming up\nheads is uniformly distributed over (0, 1) is flipped n times, then the number of heads\noccurring is equally likely to be any of the values 0, . . . , n.\n\nbecause the preceding conditional distribution has such a nice form, it is worth try-\ning to find another argument to enhance our intuition as to why such a result is true.\nto do so, let u, u1, . . . , un be n + 1 independent uniform (0, 1) random variables,\nand let x denote the number of the random variables u1, . . . , un that are smaller than\nu. since all the random variables u, u1, . . . , un have the same distribution, it follows\nthat u is equally likely to be the smallest, or second smallest, or largest of them; so\nx is equally likely to be any of the values 0, 1, . . . , n. however, given that u = p, the\nnumber of the ui that are less than u is a binomial random variable with parameters\n.\nn and p, thus establishing our previous result.\n\nexample 5m\nsuppose that x and y are independent continuous random variables having densities\nfx and fy, respectively. compute p{x < y}.\nsolution. conditioning on the value of y yields\n\n* q\n* q\n\n\u2212q\n\n\u2212q\n\np{x < y} =\n=\n\np{x < y|y = y}fy (y) dy\np{x < y|y = y}fy (y) dy\n\n "}, {"Page_number": 362, "text": "* q\n* q\n\n\u2212q\n\n\u2212q\n\n=\n\n=\n\nwhere\n\nsection 7.5\n\nconditional expectation 347\n\np{x < y}fy (y} dy by independence\n\nfx (y)fy (y) dy\n\n*\n\ny\n\u2212q\n\nfx (y) =\n\nfx (x) dx\n\n.\n\nexample 5n\nsuppose that x and y are independent continuous random variables. find the distri-\nbution of x + y.\n\nsolution. by conditioning on the value of y, we obtain\n\np{x + y < a} =\n=\n\n=\n\n=\n\np{x + y < a|y = y}fy (y) dy\np{x + y < a|y = y}fy (y) dy\np{x < a \u2212 y}fy (y) dy\nfx (a \u2212 y)fy (y) dy\n\n.\n\n\u2212q\n\n* q\n* q\n* q\n* q\n\n\u2212q\n\n\u2212q\n\n\u2212q\n\n7.5.4 conditional variance\njust as we have defined the conditional expectation of x given the value of y, we can\nalso define the conditional variance of x given that y = y:\nvar(x|y) k e[(x \u2212 e[x|y])2|y]\n\nthat is, var(x|y) is equal to the (conditional) expected square of the difference\nbetween x and its (conditional) mean when the value of y is given. in other words,\nvar(x|y) is exactly analogous to the usual definition of variance, but now all expec-\ntations are conditional on the fact that y is known.\nthere is a very useful relationship between var(x), the unconditional variance of\nx, and var(x|y), the conditional variance of x given y, that can often be applied to\ncompute var(x). to obtain this relationship, note first that, by the same reasoning\nthat yields var(x) = e[x2] \u2212 (e[x])2, we have\n\nvar(x|y) = e[x2|y] \u2212 (e[x|y])2\n\nso\n\ne[var(x|y)] = e[e[x2|y]] \u2212 e[(e[x|y])2]\n\n= e[x2] \u2212 e[(e[x|y])2]\n\n(5.9)\n\n "}, {"Page_number": 363, "text": "348\n\nchapter 7\n\nproperties of expectation\nalso, since e[e[x|y]] = e[x], we have\n\nvar(e[x|y]) = e[(e[x|y])2] \u2212 (e[x])2\n\n(5.10)\n\nhence, by adding equations (5.9) and (5.10), we arrive at the following proposition.\n\nproposition 5.2. the conditional variance formula\n\nvar(x) = e[var(x|y)] + var(e[x|y])\n\nexample 5o\nsuppose that by any time t the number of people that have arrived at a train depot is\na poisson random variable with mean \u03bbt. if the initial train arrives at the depot at a\ntime (independent of when the passengers arrive) that is uniformly distributed over\n(0, t), what are the mean and variance of the number of passengers who enter the\ntrain?\nsolution. for each t \u00fa 0, let n(t) denote the number of arrivals by t, and let y denote\nthe time at which the train arrives. the random variable of interest is then n(y).\nconditioning on y gives\n\ne[n(y)|y = t] = e[n(t)|y = t]\n\n= e[n(t)] by the independence of y and n(t)\n= \u03bbt\nsince n(t) is poisson with mean \u03bbt\n\nhence,\n\nso taking expectations gives\n\ne[n(y)|y] = \u03bby\n\ne[n(y)] = \u03bbe[y] = \u03bbt\n2\n\nto obtain var(n(y)), we use the conditional variance formula:\n\nvar(n(y)|y = t) = var(n(t)|y = t)\n\n= var(n(t)) by independence\n= \u03bbt\n\nthus,\n\nvar(n(y)|y) = \u03bby\ne[n(y)|y] = \u03bby\n\nhence, from the conditional variance formula,\n\nvar(n(y)) = e[\u03bby] + var(\u03bby)\n\n+ \u03bb2 t2\n12\nwhere we have used the fact that var(y) = t2/12.\n\n= \u03bb\n\nt\n2\n\n.\n\n "}, {"Page_number": 364, "text": "section 7.6\n\nconditional expectation and prediction 349\n\nxi\n\ni=1\n\n(cid:5)\n\n, we condition on n:\n\nexample 5p variance of a sum of a random number of random variables\n(cid:4)\nlet x1, x2, . . . be a sequence of independent and identically distributed random vari-\nn(cid:9)\nables, and let n be a nonnegative integer-valued random variable that is independent\nof the sequence xi, i \u00fa 1. to compute var\n\u23a4\n\u23a6 = ne[x]\nxi|n\n\u239e\n\u23a0 = nvar(x)\n(cid:9)\n\n\u23a1\n\u23a3 n(cid:6)\n\u239b\n\u239d n(cid:6)\n\nn\ni=1 xi is just the sum of a fixed num-\nthe preceding result follows because, given n,\nber of independent random variables, so its expectation and variance are just the\nsums of the individual means and variances, respectively. hence, from the conditional\nvariance formula,\n\nxi|n\n\nvar\n\ni=1\n\ni=1\n\ne\n\n\u239b\n\u239d n(cid:6)\n\ni=1\n\nvar\n\nxi\n\n\u239e\n\u23a0 = e[n]var(x) + (e[x])2var(n)\n\n.\n\n7.6 conditional expectation and prediction\n\nsometimes a situation arises in which the value of a random variable x is observed\nand then, on the basis of the observed value, an attempt is made to predict the value\nof a second random variable y. let g(x) denote the predictor; that is, if x is observed\nto equal x, then g(x) is our prediction for the value of y. clearly, we would like to\nchoose g so that g(x) tends to be close to y. one possible criterion for closeness is to\nchoose g so as to minimize e[(y \u2212 g(x))2]. we now show that, under this criterion,\nthe best possible predictor of y is g(x) = e[y|x].\nproposition 6.1.\n\ne[(y \u2212 g(x))2] \u00fa e[(y \u2212 e[y|x])2]\n\nproof.\n\ne[(y \u2212 g(x))2|x] = e[(y \u2212 e[y|x] + e[y|x] \u2212 g(x))2|x]\n\n= e[(y \u2212 e[y|x])2|x]\n\n+ e[(e[y|x] \u2212 g(x))2|x]\n+ 2e[(y \u2212 e[y|x])(e[y|x] \u2212 g(x))|x]\n\n(6.1)\nhowever, given x, e[y|x] \u2212 g(x), being a function of x, can be treated as a\nconstant. thus,\n\ne[(y \u2212 e[y|x])(e[y|x] \u2212 g(x))|x]\n\n= (e[y|x] \u2212 g(x))e[y \u2212 e[y|x]|x]\n= (e[y|x] \u2212 g(x))(e[y|x] \u2212 e[y|x])\n= 0\n\n(6.2)\n\n "}, {"Page_number": 365, "text": "350\n\nchapter 7\n\nproperties of expectation\n\nhence, from equations (6.1) and (6.2), we obtain\n\ne[(y \u2212 g(x))2|x] \u00fa e[(y \u2212 e[y|x])2|x]\n\nand the desired result follows by taking expectations of both sides of the preceding\nexpression.\n\nremark. a second, more intuitive, although less rigorous, argument verifying\nproposition 6.1 is as follows. it is straightforward to verify that e[(y \u2212 c)2] is mini-\nmized at c = e[y]. (see theoretical exercise 1.) thus, if we want to predict the value\nof y when there are no data available to use, the best possible prediction, in the\nsense of minimizing the mean square error, is to predict that y will equal its mean.\nhowever, if the value of the random variable x is observed to be x, then the predic-\ntion problem remains exactly as in the previous (no-data) case, with the exception\nthat all probabilities and expectations are now conditional on the event that x = x.\nhence, the best prediction in this situation is to predict that y will equal its condi-\ntional expected value given that x = x, thus establishing proposition 6.1.\n.\n\nexample 6a\nsuppose that the son of a man of height x (in inches) attains a height that is normally\ndistributed with mean x + 1 and variance 4. what is the best prediction of the height\nat full growth of the son of a man who is 6 feet tall?\n\nsolution. formally, this model can be written as\ny = x + 1 + e\n\nwhere e is a normal random variable, independent of x, having mean 0 and variance\n4. the x and y, of course, represent the heights of the man and his son, respectively.\nthe best prediction e[y|x = 72] is thus equal to\n\ne[y|x = 72] = e[x + 1 + e|x = 72]\n\n= 73 + e[e|x = 72]\n= 73 + e(e) by independence\n= 73\n\n.\n\nexample 6b\nsuppose that if a signal value s is sent from location a, then the signal value received\nat location b is normally distributed with parameters (s, 1). if s, the value of the signal\nsent at a, is normally distributed with parameters (\u03bc, \u03c3 2), what is the best estimate of\nthe signal sent if r, the value received at b, is equal to r?\n\nsolution. let us start by computing the conditional density of s given r. we have\n\nfs|r(s|r) = fs, r(s, r)\nfr(r)\n\n= fs(s)fr|s(r|s)\nfr(r)\n= ke\n\u2212(s\u2212\u03bc)2/2\u03c3 2e\n\n\u2212(r\u2212s)2/2\n\n "}, {"Page_number": 366, "text": "section 7.6\n\nconditional expectation and prediction 351\n\n2\n\n2\u03c3 2\n\n(cid:2)\n\n(s \u2212 \u03bc)2\n\n(cid:2)\n\u2212\n(cid:4)\n\n+ (r \u2212 s)2\n\nwhere k does not depend on s. now,\n= s2\n1\n2\u03c3 2\n= 1 + \u03c3 2\n2\u03c3 2\n= 1 + \u03c3 2\n2\u03c3 2\n(cid:7)\ns \u2212 (\u03bc + r\u03c3 2)\n(cid:4)\n(cid:5)\n1 + \u03c3 2\n\u03c3 2\n\n(cid:3)\n(cid:3)\n\u23a4\n\u23a1\n+ r\n+ 1\ns + c1\n(cid:5)\n\u03bc\n\u03c3 2\n2\n\u23a6 + c1\n\u23a3s2 \u2212 2\n\u03bc + r\u03c3 2\n(cid:5)2 + c2\n(cid:4)\n1 + \u03c3 2\ns \u2212 (\u03bc + r\u03c3 2)\n1 + \u03c3 2\n\u23ab\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23ac\n(cid:8)2\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23ad\n\nwhere c1 and c2 do not depend on s. hence,\n\nfs|r(s|r) = c exp\n\n\u23a7\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9\n\n\u2212\n\n1 + \u03c3 2\n\n2\n\ns\n\nwhere c does not depend on s. thus, we may conclude that the conditional distribu-\ntion of s, the signal sent, given that r is received, is normal with mean and variance\nnow given by\n\ne[s|r = r] = \u03bc + r\u03c3 2\n1 + \u03c3 2\nvar(s|r = r) = \u03c3 2\n1 + \u03c3 2\n\nconsequently, from proposition 6.1, given that the value received is r, the best esti-\nmate, in the sense of minimizing the mean square error, for the signal sent is\n\ne[s|r = r] =\n\n1\n\n1 + \u03c3 2\n\n\u03bc +\n\n\u03c3 2\n\n1 + \u03c3 2 r\n\nwriting the conditional mean as we did previously is informative, for it shows that it\nequals a weighted average of \u03bc, the a priori expected value of the signal, and r, the\nvalue received. the relative weights given to \u03bc and r are in the same proportion to\neach other as 1 (the conditional variance of the received signal when s is sent) is to \u03c3 2\n.\n(the variance of the signal to be sent).\n\nexample 6c\nin digital signal processing, raw continuous analog data x must be quantized, or dis-\ncretized, in order to obtain a digital representation. in order to quantize the raw\ni\u2192+q ai = q and\ndata x, an increasing set of numbers ai, i = 0, ;1, ;2, . . ., such that\ni\u2192\u2212q ai = \u2212q is fixed, and the raw data are then quantized according to the interval\nlim\n(ai, ai+1] in which x lies. let us denote by yi the discretized value when x \u2208 (ai, ai+1],\nand let y denote the observed discretized value\u2014that is,\nif ai < x \u2026 ai+1\n\ny = yi\n\nlim\n\n "}, {"Page_number": 367, "text": "352\n\nchapter 7\n\nproperties of expectation\n\nthe distribution of y is given by\n\np{y = yi} = fx (ai+1) \u2212 fx (ai)\n\nsuppose now that we want to choose the values yi, i = 0, ;1, ;2, . . . so as to mini-\nmize e[(x \u2212 y)2], the expected mean square difference between the raw data and\ntheir quantized version.\n(a) find the optimal values yi, i = 0, ;1, . . ..\nfor the optimal quantizer y, show that\n(b) e[y] = e[x], so the mean square error quantizer preserves the input mean;\n(c) var(y) = var(x) \u2212 e[(x \u2212 y)2].\nsolution. (a) for any quantizer y, upon conditioning on the value of y, we obtain\n\ne[(x \u2212 y)2] =\n\ne[(x \u2212 yi)2|ai < x \u2026 ai+1]p{ai < x \u2026 ai+1}\n\n(cid:6)\n\nnow, if we let\n\nthen\n\ni\n\ni = i\n\nif ai < x \u2026 ai+1\n\ne[(x \u2212 yi)2|ai < x \u2026 ai+1] = e[(x \u2212 yi)2|i = i]\n\nand by proposition 6.1, this quantity is minimized when\n\nyi = e[x|i = i]\n*\n= e[x|ai < x \u2026 ai+1]\n=\nxfx (x) dx\n\nai+1\n\nfx (ai+1) \u2212 fx (ai)\n\nai\n\nnow, since the optimal quantizer is given by y = e[x|i], it follows that\n\n(b) e[y] = e[x]\n(c)\n\nvar(x) = e[var(x|i)] + var(e[x|i])\n= e[e[(x \u2212 y)2|i]] + var(y)\n= e[(x \u2212 y)2] + var(y)\n\n.\n\nit sometimes happens that the joint probability distribution of x and y is not\ncompletely known; or if it is known, it is such that the calculation of e[y|x = x]\nis mathematically intractable. if, however, the means and variances of x and y and\nthe correlation of x and y are known, then we can at least determine the best linear\npredictor of y with respect to x.\nb so as to minimize e[(y \u2212 (a + bx))2]. now,\n\nto obtain the best linear predictor of y with respect to x, we need to choose a and\n\ne[(y \u2212 (a + bx))2] = e[y2 \u2212 2ay \u2212 2bxy + a2 + 2abx + b2x2]\n\n= e[y2] \u2212 2ae[y] \u2212 2be[xy] + a2\n\n+ 2abe[x] + b2e[x2]\n\n "}, {"Page_number": 368, "text": "section 7.6\n\nconditional expectation and prediction 353\n\ntaking partial derivatives, we obtain\n\ne[(y \u2212 a \u2212 bx)2] = \u22122e[y] + 2a + 2be[x]\ne[(y \u2212 a \u2212 bx)2] = \u22122e[xy] + 2ae[x] + 2be[x2]\n\n\u2202\n\u2202a\n\u2202\n\u2202b\n\nsetting equations (6.3) to 0 and solving for a and b yields the solutions\n\nb = e[xy] \u2212 e[x]e[y]\ne[x2] \u2212 (e[x])2\na = e[y] \u2212 be[x] = e[y] \u2212 \u03c1\u03c3ye[x]\n\n= cov(x, y)\n\n\u03c3 2\nx\n\n= \u03c1\n\n\u03c3y\n\u03c3x\n\n\u03c3x\nwhere \u03c1 = correlation(x, y), \u03c3 2\n= var(y), and \u03c3 2\n= var(x). it is easy to ver-\nify that the values of a and b from equation (6.4) minimize e[(y \u2212 a \u2212 bx)2];\nthus, the best (in the sense of mean square error) linear predictor y with respect\nto x is\n\ny\n\nx\n\n(6.3)\n\n(6.4)\n\n\u03bcy + \u03c1\u03c3y\n\u03c3x\n\n(x \u2212 \u03bcx)\n\nthe mean square error of this predictor is given by\n\nwhere \u03bcy = e[y] and \u03bcx = e[x].\n(cid:7)(cid:2)\n(cid:3)2\ny \u2212 \u03bcy \u2212 \u03c1\n\ne\n\n(x \u2212 \u03bcx)\n5\n\n+ \u03c12\n\u2212 2\u03c12\u03c3 2\n\ny\n\n\u03c3y\n4\n\u03c3x\n(y \u2212 \u03bcy)2\n+ \u03c12\u03c3 2\ny\n(1 \u2212 \u03c12)\n\n\u03c3 2\ny\n\u03c3 2\nx\n\ne\n\n= e\n= \u03c3 2\ny\n= \u03c3 2\ny\n\n(cid:8)\n4\n\n5\n\n\u2212 2\u03c1\n\n\u03c3y\n\u03c3x\n\ne\n\n2\n(y \u2212 \u03bcy)(x \u2212 \u03bcx)\n\n3\n\n(x \u2212 \u03bcx)2\n\n(6.5)\nwe note from equation (6.5) that if \u03c1 is near +1 or \u22121, then the mean square error\n.\nof the best linear predictor is near zero.\n\nexample 6d\nan example in which the conditional expectation of y given x is linear in x, and\nhence in which the best linear predictor of y with respect to x is the best overall\npredictor, is when x and y have a bivariate normal distribution. for, as shown in\nexample 5c of chapter 6, in that case,\n\ne[y|x = x] = \u03bcy + \u03c1\n\n(x \u2212 \u03bcx)\n\n\u03c3y\n\u03c3x\n\n.\n\n "}, {"Page_number": 369, "text": "354\n\nchapter 7\n\nproperties of expectation\n\n7.7 moment generating functions\n\nthe moment generating function m(t) of the random variable x is defined for all real\nvalues of t by\n\n\u23a7\u23aa\u23aa\u23aa\u23a8\n(cid:6)\nm(t) = e[etx]\n* q\n\u23aa\u23aa\u23aa\u23a9\n\n=\n\nx\n\n\u2212q\n\netxp(x)\n\nif x is discrete with mass function p(x)\n\netxf (x) dx\n\nif x is continuous with density f (x)\n\nwe call m(t) the moment generating function because all of the moments of x can be\nobtained by successively differentiating m(t) and then evaluating the result at t = 0.\nfor example,\n\nwhere we have assumed that the interchange of the differentiation and expectation\noperators is legitimate. that is, we have assumed that\n\n(cid:21)\n\n(7.1)\n\n(etx )\n\nm\n\n(cid:20)\n\ne[etx]\nd\ndt\n\n(cid:8)(t) = d\ndt\n= e\n= e[xetx]\n\u23a4\n\u23a6 =\n(cid:21)\n\n(cid:6)\n\n*\n\nx\n\netxp(x)\n\nd\ndt\n\n\u23a1\n\u23a3(cid:6)\n(cid:20)*\n\nx\n\nd\ndt\n\n[etxp(x)]\n\nin the discrete case and\n\nd\ndt\n\netxf (x) dx\n\n=\n\nd\ndt\n\n[etxf (x)] dx\n\nin the continuous case. this assumption can almost always be justified and, indeed, is\nvalid for all of the distributions considered in this book. hence, from equation (7.1),\nevaluated at t = 0, we obtain\n\nsimilarly,\n\nm\n\n(cid:8)(0) = e[x]\n\nm\n\nm\n\n(cid:20)\n\n(cid:8)(t)\n\n(cid:8)(cid:8)(t) = d\ndt\n= d\ndt\n= e\n= e[x2etx]\n\ne[xetx]\nd\ndt\n\n(xetx )\n\n(cid:21)\n\nthus,\n\n(cid:8)(cid:8)(0) = e[x2]\n\nm\n\n "}, {"Page_number": 370, "text": "section 7.7\n\nmoment generating functions 355\n\nin general, the nth derivative of m(t) is given by\n\nimplying that\n\nmn(t) = e[xnetx] n \u00fa 1\n\nmn(0) = e[xn] n \u00fa 1\n\nwe now compute m(t) for some common distributions.\n\nexample 7a binomial distribution with parameters n and p\nif x is a binomial random variable with parameters n and p, then\n\n(cid:3)\n\nm(t) = e[etx]\n(cid:2)\n\n= n(cid:6)\n= n(cid:6)\n\nk=0\n\netk\n\n(cid:2)\n(cid:3)\n\nn\nk\n\nk=0\n\n= (pet + 1 \u2212 p)n\n\npk(1 \u2212 p)n\u2212k\n\nn\nk\n(pet)k(1 \u2212 p)n\u2212k\n\nwhere the last equality follows from the binomial theorem. differentiation yields\n\nthus,\n\n(cid:8)(t) = n(pet + 1 \u2212 p)n\u22121pet\n\nm\n\ne[x] = m\n\n(cid:8)(0) = np\n\ndifferentiating a second time yields\n\n(cid:8)(cid:8)(t) = n(n \u2212 1)(pet + 1 \u2212 p)n\u22122(pet)2 + n(pet + 1 \u2212 p)n\u22121pet\n\nm\n\nso\n\ne[x2] = m\n\n(cid:8)(cid:8)(0) = n(n \u2212 1)p2 + np\n\nthe variance of x is given by\n\nvar(x) = e[x2] \u2212 (e[x])2\n\n= n(n \u2212 1)p2 + np \u2212 n2p2\n= np(1 \u2212 p)\n\nverifying the result obtained previously.\n\n.\n\nexample 7b poisson distribution with mean \u03bb\nif x is a poisson random variable with parameter \u03bb, then\n\n=\n\nq(cid:6)\nm(t) = e[etx]\netne\nq(cid:6)\n\nn=0\n= e\n\u2212\u03bb\n\nn=0\n\n\u2212\u03bb\u03bbn\nn!\n\n(\u03bbet)n\n\nn!\n\n "}, {"Page_number": 371, "text": "356\n\nchapter 7\n\nproperties of expectation\n\n\u2212\u03bbe\u03bbet\n\n= e\n= exp{\u03bb(et \u2212 1)}\n\ndifferentiation yields\n\n(cid:8)(t) = \u03bbet exp{\u03bb(et \u2212 1)}\nm\n(cid:8)(cid:8)(t) = (\u03bbet)2 exp{\u03bb(et \u2212 1)} + \u03bbet exp{\u03bb(et \u2212 1)}\nm\n\nthus,\n\ne[x] = m\n(cid:8)(0) = \u03bb\ne[x2] = m\n(cid:8)(cid:8)(0) = \u03bb2 + \u03bb\nvar(x) = e[x2] \u2212 (e[x])2\n\n= \u03bb\n\nhence, both the mean and the variance of the poisson random variable equal \u03bb.\n\n.\n\nexample 7c exponential distribution with parameter \u03bb\n\n* q\nm(t) = e[etx]\n* q\n\n=\n\n0\n= \u03bb\n0\n= \u03bb\n\u03bb \u2212 t\n\netx\u03bbe\n\n\u2212\u03bbx dx\n\u2212(\u03bb\u2212t)x dx\ne\n\nfor t < \u03bb\n\nwe note from this derivation that, for the exponential distribution, m(t) is defined\nonly for values of t less than \u03bb. differentiation of m(t) yields\n\nhence,\n\n(cid:8)(t) =\n\nm\n\n\u03bb\n\n(\u03bb \u2212 t)2 m\n\n(cid:8)(cid:8)(t) =\n\n2\u03bb\n\n(\u03bb \u2212 t)3\n\ne[x] = m\n\n(cid:8)(0) = 1\n\n\u03bb\n\ne[x2] = m\n\n(cid:8)(cid:8)(0) = 2\n\u03bb2\n\nthe variance of x is given by\n\nvar(x) = e[x2] \u2212 (e[x])2\n\n= 1\n\u03bb2\n\n.\n\nexample 7d normal distribution\nwe first compute the moment generating function of a unit normal random variable\nwith parameters 0 and 1. letting z be such a random variable, we have\n\n* q\nmz(t) = e[etz]\n\n= 1\u221a\n2\u03c0\n\n\u2212q\n\n\u2212x2/2 dx\n\netxe\n\n "}, {"Page_number": 372, "text": "section 7.7\n\nmoment generating functions 357\n\n7\n\ndx\n\n7\n\n+ t2\n2\n\ndx\n\n0\n0\n\n\u2212 (x2 \u2212 2tx)\n\u2212 (x \u2212 t)2\n\n2\n\n2\n\n\u2212(x\u2212t)2/2 dx\ne\n\nexp\n\nexp\n\n* q\n\n\u2212q\n\n* q\n* q\n\n\u2212q\n\n\u2212q\n\n= 1\u221a\n2\u03c0\n= 1\u221a\n2\u03c0\n\n= et2/2 1\u221a\n2\u03c0\n= et2/2\n\nhence, the moment generating function of the unit normal random variable z is given\nby mz(t) = et2/2. to obtain the moment generating function of an arbitrary normal\nrandom variable, we recall (see section 5.4) that x = \u03bc + \u03c3 z will have a normal\ndistribution with parameters \u03bc and \u03c3 2 whenever z is a unit normal random variable.\nhence, the moment generating function of such a random variable is given by\n\nmx (t) = e[etx]\n\n7\n\n+ \u03bct\n\n0\n\n= e[et(\u03bc+\u03c3 z)]\n= e[et\u03bcet\u03c3 z]\n= et\u03bce[et\u03c3 z]\n= et\u03bcmz(t\u03c3 )\n= et\u03bce(t\u03c3 )2/2\n\u03c3 2t2\n= exp\n2\n0\n0\n\n7\n7\n\n+ \u03bct\n\n\u03c3 2t2\n2\n\u03c3 2t2\n2\n\n+ \u03bct\n\n0\n\n+ \u03c3 2 exp\n\n7\n\n\u03c3 2t2\n2\n\n+ \u03bct\n\ne[x] = m\ne[x2] = m\n\n(cid:8)(0) = \u03bc\n(cid:8)(cid:8)(0) = \u03bc2 + \u03c3 2\n\nvar(x) = e[x2] \u2212 e([x])2\n\n= \u03c3 2\n\n.\n\nby differentiating, we obtain\n\n(t) = (\u03bc + t\u03c3 2) exp\n\n(t) = (\u03bc + t\u03c3 2)2 exp\n\n(cid:8)\nx\n\nm\n\n(cid:8)(cid:8)\nx\n\nm\n\nthus,\n\nimplying that\n\ntables 7.1 and 7.2 give the moment generating functions for some common dis-\n\ncrete and continuous distributions.\n\nan important property of moment generating functions is that the moment gen-\nerating function of the sum of independent random variables equals the product of\nthe individual moment generating functions. to prove this, suppose that x and y are\n\n "}, {"Page_number": 373, "text": "358\n\nchapter 7\n\nproperties of expectation\n\ntable 7.1: discrete probability distribution\n\nprobability mass\nfunction, p(x)\n\n(cid:2)\n\n(cid:3)\n\npx(1 \u2212 p)n\u2212x\n\nn\nx\nx = 0, 1, . . . , n\n\u2212\u03bb \u03bbx\ne\nx!\nx = 0, 1, 2, . . .\np(1 \u2212 p)x\u22121\nx = 1, 2, . . .\n(cid:2)\n(cid:3)\nn \u2212 1\nr \u2212 1\nn = r, r + 1, . . .\n\npr(1 \u2212 p)n\u2212r\n\nbinomial with\nparameters n, p;\n0 \u2026 p \u2026 1\n\npoisson with\nparameter \u03bb > 0\n\ngeometric with\nparameter\n0 \u2026 p \u2026 1\n\nnegative\nbinomial with\nparameters r, p;\n0 \u2026 p \u2026 1\n\nmoment\ngenerating\nfunction, m(t)\n\nmean variance\n\n(pet + 1 \u2212 p)n\n\nnp\n\nnp(1 \u2212 p)\n\nexp{\u03bb(et \u2212 1)}\n\n\u03bb\n\n\u03bb\n\npet\n\n1 \u2212 (1 \u2212 p)et\n(cid:7)\n\npet\n\n1 \u2212 (1 \u2212 p)et\n\n(cid:8)r\n\n1\np\n\nr\np\n\n1 \u2212 p\np2\n\nr(1 \u2212 p)\n\np2\n\nindependent and have moment generating functions mx (t) and my (t), respectively.\nthen mx+y (t), the moment generating function of x + y, is given by\n\nmx+y (t) = e[et(x+y)]\n= e[etxety]\n= e[etx]e[ety]\n= mx (t)my (t)\n\nwhere the next-to-last equality follows from proposition 4.1, since x and y are inde-\npendent.\nanother important result is that the moment generating function uniquely deter-\nmines the distribution. that is, if mx (t) exists and is finite in some region about t = 0,\nthen the distribution of x is uniquely determined. for instance, if\n\n(cid:2)\n\n(cid:3)10\n\n1\n2\n\nmx (t) =\n\n(et + 1)10,\n\nthen it follows from table 7.1 that x is a binomial random variable with parameters\n10 and 1\n2.\n\nexample 7e\nsuppose that the moment generating function of a random variable x is given by\nm(t) = e3(et\u22121). what is p{x = 0}?\n\n "}, {"Page_number": 374, "text": "table 7.2: continuous probability distribution\n\nprobability mass function, f (x)\n\na < x < b\n\notherwise\n\n\u03bbe\n0\n\nf (x) =\n\nf (x) =\n\n\u23a7\u23aa\u23a8\n\u23aa\u23a9 1\nb \u2212 a\n0\n0\n\u2212\u03bbx x \u00fa 0\n\u23a7\u23aa\u23aa\u23a8\nx < 0\n\u2212\u03bbx(\u03bbx)s\u22121\n\u23aa\u23aa\u23a9 \u03bbe\n\u0001(s)\n0\nf (x) = 1\u221a\n2\u03c0 \u03c3\n\nf (x) =\n\ne\n\nx \u00fa 0\n\nx < 0\n\nuniform over (a, b)\n\nexponential with\nparameter \u03bb > 0\n\ngamma with parameters\n(s, \u03bb), \u03bb > 0\n\nnormal with parameters\n(\u03bc, \u03c3 2)\n\n\u2212(x\u2212\u03bc)2/2\u03c3 2 \u2212 q < x < q exp\n\nmoment\ngenerating\nfunction, m(t)\netb \u2212 eta\nt(b \u2212 a)\n\nmean\na + b\n\n2\n\nvariance\n(b \u2212 a)2\n\n12\n\n\u03bb\n\n\u03bb \u2212 t\n(cid:2)\n\n\u03bb\n\n(cid:3)s\n\n\u03bb \u2212 t\n0\n\u03bct + \u03c3 2t2\n2\n\n1\n\u03bb\n\ns\n\u03bb\n\n\u03bc\n\n7\n\n1\n\u03bb2\n\ns\n\u03bb2\n\n\u03c3 2\n\ns\ne\nc\nt\ni\no\nn\n7\n.\n7\n\nm\no\nm\ne\nn\nt\ng\ne\nn\ne\nr\na\nt\ni\nn\ng\nf\nu\nn\nc\nt\ni\no\nn\ns\n\n3\n5\n9\n\n "}, {"Page_number": 375, "text": "360\n\nchapter 7\n\nproperties of expectation\nsolution. we see from table 7.1 that m(t) = e3(et\u22121) is the moment generating func-\ntion of a poisson random variable with mean 3. hence, by the one-to-one correspon-\ndence between moment generating functions and distribution functions, it follows\nthat x must be a poisson random variable with mean 3. thus, p{x = 0} = e\n.\n\n\u22123.\n\nexample 7f sums of independent binomial random variables\nif x and y are independent binomial random variables with parameters (n, p) and\n(m, p), respectively, what is the distribution of x + y?\nsolution. the moment generating function of x + y is given by\n\nmx+y (t) = mx (t)my (t) = (pet + 1 \u2212 p)n(pet + 1 \u2212 p)m\n\n= (pet + 1 \u2212 p)m+n\n\nhowever, (pet + 1 \u2212 p)m+n is the moment generating function of a binomial ran-\ndom variable having parameters m + n and p. thus, this must be the distribution\nof x + y.\n.\n\nexample 7g sums of independent poisson random variables\ncalculate the distribution of x + y when x and y are independent poisson random\nvariables with means respective \u03bb1 and \u03bb2.\n\nsolution.\n\nmx+y (t) = mx (t)my (t)\n\n= exp{\u03bb1(et \u2212 1)} exp{\u03bb2(et \u2212 1)}\n= exp{(\u03bb1 + \u03bb2)(et \u2212 1)}\n\nhence, x + y is poisson distributed with mean \u03bb1 + \u03bb2, verifying the result given\n.\nin example 3e of chapter 6.\n\nexample 7h sums of independent normal random variables\nshow that if x and y are independent normal random variables with respective\n), then x + y is normal with mean \u03bc1 + \u03bc2 and\nparameters (\u03bc1, \u03c3 2\n+ \u03c3 2\n1\nvariance \u03c3 2\n2 .\n1\n\n) and (\u03bc2, \u03c3 2\n2\n\nsolution.\n\n0\n0\n\nmx+y (t) = mx (t)my (t)\n\u03c3 2\n1 t2\n2\n(\u03c3 2\n1\n\n= exp\n\n= exp\n\n+ \u03bc1t\n+ \u03c3 2\n2\n2\n\n)t2\n\n7\n\n0\n\nexp\n\n\u03c3 2\n2 t2\n2\n\n7\n\n+ \u03bc2t\n7\n\n+ (\u03bc1 + \u03bc2)t\n\nwhich is the moment generating function of a normal random variable with mean\n\u03bc1 + \u03bc2 and variance \u03c3 2\n2 . the desired result then follows because the moment\n.\ngenerating function uniquely determines the distribution.\n\n+ \u03c3 2\n\n1\n\n "}, {"Page_number": 376, "text": "section 7.7\n\nmoment generating functions 361\n\nexample 7i\ncompute the moment generating function of a chi-squared random variable with n\ndegrees of freedom.\n\nsolution. we can represent such a random variable as\n\n+ \u00b7\u00b7\u00b7 + z2\n\nn\n\nz2\n1\n\nwhere z1, . . . , zn are independent standard normal random variables. let m(t) be its\nmoment generating function. then, by the preceding,\n\nm(t) = (e[etz2])n\nwhere z is a standard normal random variable. now,\n\n* q\n* q\n\n\u2212q\n\ne[etz2] = 1\u221a\n2\u03c0\n= 1\u221a\n2\u03c0\n= \u03c3\n= (1 \u2212 2t)\u22121/2\n\n\u2212q\n\n\u2212x2/2 dx\n\netx2e\n\u2212x2/2\u03c3 2 dx where \u03c3 2 = (1 \u2212 2t)\u22121\ne\n\nwhere the next-to-last equality uses the fact that the normal density with mean 0 and\nvariance \u03c3 2 integrates to 1. therefore,\n\nm(t) = (1 \u2212 2t)\u2212n/2\n\n.\n\nexample 7j moment generating function of the sum of a random number of\nrandom variables\nlet x1, x2, . . . be a sequence of independent and identically distributed random vari-\nables, and let n be a nonnegative, integer-valued random variable that is independent\nof the sequence x, i \u00fa 1. we want to compute the moment generating function of\n\ny = n(cid:6)\n\ni=1\n\nxi\n\n(in example 5d, y was interpreted as the amount of money spent in a store on a\ngiven day when both the amount spent by a customer and the number of customers\nare random variables.)\n\nto compute the moment generating function of y, we first condition on n as\n\nfollows:\n\n\u23a1\n\u23a2\u23a2\u23a3exp\n\n\u23a7\u23a8\n\u23a9t\n\ne\n\n\u23a4\n6666666 n = n\n\u23ab\u23ac\n\u23a5\u23a5\u23a6 = e\n\u23ad\n\nn(cid:6)\n\n1\n\nxi\n\n\u23a1\n\u23a7\u23a8\n\u23a2\u23a2\u23a3exp\n\u23a9t\n\u23a1\n\u23a7\u23a8\n\u23a2\u23a3exp\n\u23a9t\n= e\n= [mx (t)]n\n\n\u23a4\n6666666 n = n\n\u23ab\u23ac\n\u23a5\u23a5\u23a6\n\u23ad\n\u23a4\n\u23ab\u23ac\n\u23a5\u23a6\n\u23ad\n\nn(cid:6)\nn(cid:6)\n\n1\n\nxi\n\nxi\n\n1\n\n "}, {"Page_number": 377, "text": "362\n\nchapter 7\n\nproperties of expectation\n\nwhere\n\nhence,\n\nthus,\n\nmx (t) = e[etxi]\n\ne[ety|n] = (mx (t))n\n\nmy (t) = e[(mx (t))n]\n\nthe moments of y can now be obtained upon differentiation, as follows:\n\n(cid:8)\nm\ny\n\n(t) = e[n(mx (t))n\u22121m\n\n(cid:8)\nx\n\n(t)]\n\nso\n\ne[y] = m\n\n(cid:8)\ny\n\n(0)\n\n= e[n(mx (0))n\u22121m\n= e[nex]\n= e[n]e[x]\n\n(cid:8)\nx\n\n(0)]\n\n(7.2)\n\nverifying the result of example 5d. (in this last set of equalities, we have used the fact\nthat mx (0) = e[e0x] = 1.)\n\nalso,\n\n(cid:8)(cid:8)\nm\ny\n\n(t) = e[n(n \u2212 1)(mx (t))n\u22122(m\n\n(cid:8)\nx\n\n(t))2 + n(mx (t))n\u22121m\n\n(cid:8)(cid:8)\nx\n\n(t)]\n\nso\n\ne[y2] = m\n\n(cid:8)(cid:8)\ny\n\n(0)\n\n= e[n(n \u2212 1)(e[x])2 + ne[x2]]\n= (e[x])2(e[n2] \u2212 e[n]) + e[n]e[x2]\n= e[n](e[x2] \u2212 (e[x])2) + (e[x])2e[n2]\n= e[n]var(x) + (e[x])2e[n2]\n\nhence, from equations (7.2) and (7.3), we have\n\nvar(y) = e[n]var(x) + (e[x])2(e[n2] \u2212 (e[n])2)\n\n= e[n]var(x) + (e[x])2var(n)\n\n(7.3)\n\n.\n\nexample 7k\nlet y denote a uniform random variable on (0, 1), and suppose that, conditional on\ny = p, the random variable x has a binomial distribution with parameters n and p.\nin example 5k, we showed that x is equally likely to take on any of the values\n0, 1, . . . , n. establish this result by using moment generating functions.\n\nsolution. to compute the moment generating function of x, start by conditioning\non the value of y. using the formula for the binomial moment generating function\ngives\n\ne[etx|y = p] = (pet + 1 \u2212 p)n\n\n "}, {"Page_number": 378, "text": "now, y is uniform on (0, 1), so, upon taking expectations, we obtain\n\nsection 7.7\n\nmoment generating functions 363\n\n*\n\n1\n\n0\n\ne[etx] =\n=\n\n(pet + 1 \u2212 p)n dp\n\n*\n\net\n\n1\n\nyndy\net(n+1) \u2212 1\net \u2212 1\n\n1\n\net \u2212 1\nn + 1\nn + 1\n\n= 1\n= 1\n\n(1 + et + e2t + \u00b7\u00b7\u00b7 + ent)\n\n(by the substitution y = pet + 1 \u2212 p)\n\nbecause the preceding is the moment generating function of a random variable that\nis equally likely to be any of the values 0, 1, . . . , n, the desired result follows from the\nfact that the moment generating function of a random variable uniquely determines\n.\nits distribution.\n\n7.7.1 joint moment generating functions\nit is also possible to define the joint moment generating function of two or more\nrandom variables. this is done as follows: for any n random variables x1, . . . , xn,\nthe joint moment generating function, m(t1, . . . , tn), is defined, for all real values of\nt1, . . . , tn, by\n\nm(t1, . . . , tn) = e[et1x1+\u00b7\u00b7\u00b7+tnxn]\n\nthe individual moment generating functions can be obtained from m(t1, . . . , tn) by\nletting all but one of the tj\u2019s be 0. that is,\n\n(t) = e[etxi] = m(0, . . . , 0, t, 0, . . . , 0)\n\nmxi\n\nwhere the t is in the ith place.\n\nit can be proven (although the proof is too advanced for this text) that the joint\nmoment generating function m(t1, . . . , tn) uniquely determines the joint distribution\nof x1, . . . , xn. this result can then be used to prove that the n random variables\nx1, . . . , xn are independent if and only if\n\nm(t1, . . . , tn) = mx1\n\n(t1)\u00b7\u00b7\u00b7 mxn\n\n(tn)\n\n(7.4)\n\nfor the proof in one direction, if the n random variables are independent, then\n\nm(t1, . . . , tn) = e[e(t1x1+\u00b7\u00b7\u00b7+tnxn)]\n= e[et1x1 \u00b7\u00b7\u00b7 etnxn]\n= e[et1x1]\u00b7\u00b7\u00b7 e[etnxn] by independence\n= mx1\n\n(t1)\u00b7\u00b7\u00b7 mxn\n\n(tn)\n\nfor the proof in the other direction, if equation (7.4) is satisfied, then the joint\nmoment generating function m(t1, . . . , tn) is the same as the joint moment generating\nfunction of n independent random variables, the ith of which has the same distri-\nbution as xi. as the joint moment generating function uniquely determines the joint\ndistribution, this must be the joint distribution; hence, the random variables are\nindependent.\n\n "}, {"Page_number": 379, "text": "364\n\nchapter 7\n\nproperties of expectation\n\nexample 7l\nlet x and y be independent normal random variables, each with mean \u03bc and vari-\nance \u03c3 2. in example 7a of chapter 6, we showed that x + y and x \u2212 y are inde-\npendent. let us now establish this result by computing their joint moment generating\nfunction:\n\ne[et(x+y)+s(x\u2212y)] = e[e(t+s)x+(t\u2212s)y]\n\n= e[e(t+s)x]e[e(t\u2212s)y]\n= e\u03bc(t+s)+\u03c3 2(t+s)2/2e\u03bc(t\u2212s)+\u03c3 2(t\u2212s)2/2\n= e2\u03bct+\u03c3 2t2e\u03c3 2s2\n\nbut we recognize the preceding as the joint moment generating function of the sum\nof a normal random variable with mean 2\u03bc and variance 2\u03c3 2 and an independent\nnormal random variable with mean 0 and variance 2\u03c3 2. because the joint moment\ngenerating function uniquely determines the joint distribution, it follows that x + y\nand x \u2212 y are independent normal random variables.\n.\n\nin the next example, we use the joint moment generating function to verify a result\n\nthat was established in example 2b of chapter 6.\n\nexample 7m\nsuppose that the number of events that occur is a poisson random variable with\nmean \u03bb and that each event is independently counted with probability p. show that\nthe number of counted events and the number of uncounted events are independent\npoisson random variables with respective means \u03bbp and \u03bb(1 \u2212 p).\nsolution. let x denote the total number of events, and let xc denote the number\nof them that are counted. to compute the joint moment generating function of xc,\nthe number of events that are counted, and x \u2212 xc, the number that are uncounted,\nstart by conditioning on x to obtain\n\ne[esxc+t(x\u2212xc)|x = n] = etne[e(s\u2212t)xc|x = n]\n= etn(pes\u2212t + 1 \u2212 p)n\n= (pes + (1 \u2212 p)et)n\n\nwhich follows because, conditional on x = n, xc is a binomial random variable with\nparameters n and p. hence,\n\ne[esxc+t(x\u2212xc)|x] = (pes + (1 \u2212 p)et)x\n\ntaking expectations of both sides of this equation yields\n\ne[esxc+t(x\u2212xc)] = e[(pes + (1 \u2212 p)et)x]\n\nnow, since x is poisson with mean \u03bb, it follows that e[etx] = e\u03bb(et\u22121). therefore, for\nany positive value a we see (by letting a = et) that e[ax] = e\u03bb(a\u22121). thus\n\ne[esxc+t(x\u2212xc)] = e\u03bb(pes+(1\u2212p)et\u22121)\n\n= e\u03bbp(es\u22121 )e\u03bb(1\u2212p)(et\u22121)\n\n "}, {"Page_number": 380, "text": "section 7.8\n\nadditional properties of normal random variables 365\n\nas the preceding is the joint moment generating function of independent poisson\nrandom variables with respective means \u03bbp and \u03bb(1 \u2212 p), the result is proven.\n.\n\n7.8 additional properties of normal random variables\n\n7.8.1 the multivariate normal distribution\nlet z1, . . . , zn be a set of n independent unit normal random variables. if, for some\nconstants aij, 1 \u2026 i \u2026 m, 1 \u2026 j \u2026 n, and \u03bci, 1 \u2026 i \u2026 m,\n\nx1 = a11z1 + \u00b7\u00b7\u00b7 + a1nzn + \u03bc1\nx2 = a21z1 + \u00b7\u00b7\u00b7 + a2nzn + \u03bc2\n\n.\n.\n.\n\nxi = ai1z1 + \u00b7\u00b7\u00b7 + ainzn + \u03bci\n\n.\n.\n.\n\nxm = am1z1 + \u00b7\u00b7\u00b7 + amnzn + \u03bcm\n\nthen the random variables x1, . . . , xm are said to have a multivariate normal distri-\nbution.\n\nfrom the fact that the sum of independent normal random variables is itself a\nnormal random variable, it follows that each xi is a normal random variable with\nmean and variance given, respectively, by\n\ne[xi] = \u03bci\n\nvar(xi) = n(cid:6)\n\na2\nij\n\nj=1\n\nlet us now consider\n\nm(t1, . . . , tm) = e[exp{t1x1 + \u00b7\u00b7\u00b7 + tmxm}]\n\nm(cid:9)\n\ni=1\n\nthe joint moment generating function of x1, . . . , xm. the first thing to note is that\nsince\ntixi is itself a linear combination of the independent normal random vari-\n\nables z1, . . . , zn, it is also normally distributed. its mean and variance are\n\nand\n\n\u239b\n\u239d m(cid:6)\n\ni=1\n\nvar\n\ne\n\nti\u03bci\n\ni=1\n\ni=1\n\ntixi\n\n\u23a1\n\u23a4\n\u23a3 m(cid:6)\n\u23a6 = m(cid:6)\n\u239b\n\u239e\n\u239c\u239d m(cid:6)\n\u23a0 = cov\nm(cid:6)\n= m(cid:6)\n\ni=1\n\ni=1\n\nj=1\n\ntixi\n\ntixi,\n\n\u239e\n\u239f\u23a0\n\ntjxj\n\nm(cid:6)\n\nj=1\n\ntitjcov(xi, xj)\n\n "}, {"Page_number": 381, "text": "366\n\nchapter 7\n\nproperties of expectation\n\nnow, if y is a normal random variable with mean \u03bc and variance \u03c3 2, then\n\nthus,\n\ne[ey] = my (t)|t=1 = e\u03bc+\u03c3 2/2\nm(cid:6)\n\nm(cid:6)\n\n\u23a7\u23aa\u23a8\n\u23aa\u23a9 m(cid:6)\n\ni=1\n\nti\u03bci + 1\n2\n\ntitjcov(xi, xj)\n\ni=1\n\nj=1\n\nm(t1, . . . , tm) = exp\n\n\u23ab\u23aa\u23ac\n\u23aa\u23ad\n\nwhich shows that the joint distribution of x1, . . . , xm is completely determined from\na knowledge of the values of e[xi] and cov(xi, xj), i, j = 1, . . . , m.\nit can be shown that when m = 2, the multivariate normal distribution reduces to\nthe bivariate normal.\n\nexample 8a\nfind p(x < y) for bivariate normal random variables x and y having parameters\n\n\u03bcx = e[x], \u03bcy = e[y], \u03c3 2\n\n= var(x), \u03c3 2\nsolution. because x \u2212 y is normal with mean\n\nx\n\ny\n\n= var(y), \u03c1 = corr(x, y)\n\ne[x \u2212 y] = \u03bcx \u2212 \u03bcy\n\nand variance\n\nvar(x \u2212 y) = var(x) + var(\u2212y) + 2cov(x,\u2212y)\n\n= \u03c3 2\nx\n\n+ \u03c3 2\ny\n\n\u2212 2\u03c1\u03c3x\u03c3y\n\nwe obtain\n\np{x < y} = p{x \u2212 y < 0}\n\n\u23a7\u23aa\u23a8\n8\n\u23aa\u23a9x \u2212 y \u2212 (\u03bcx \u2212 \u03bcy)\n(cid:5)\n(cid:4)\n\u2212 2\u03c1\u03c3x\u03c3y\n8\n\n+ \u03c3 2\ny\n\u03bcy \u2212 \u03bcx\n+ \u03c3 2\ny\n\n\u2212 2\u03c1\u03c3x\u03c3y\n\n\u03c3 2\nx\n\n\u03c3 2\nx\n\n= p\n\n= \u0001\n\n8\n\n<\n\n\u2212(\u03bcx \u2212 \u03bcy)\n+ \u03c3 2\ny\n\n\u2212 2\u03c1\u03c3x\u03c3y\n\n\u03c3 2\nx\n\n\u23ab\u23aa\u23ac\n\u23aa\u23ad\n\n.\n\nexample 8b\nsuppose that the conditional distribution of x, given that \u0001 = \u03b8, is normal with\nmean \u03b8 and variance 1. moreover, suppose that \u0001 itself is a normal random variable\nwith mean \u03bc and variance \u03c3 2. find the conditional distribution of \u0001 given that x = x.\nsolution. rather than using and then simplifying bayes\u2019s formula, we will solve this\nproblem by first showing that x, \u0001 has a bivariate normal distribution. to do so, note\nthat the joint density function of x, \u0001 can be written as\nfx,\u0001(x, \u03b8 ) = fx|\u0001(x|\u03b8 )f\u0001(\u03b8 )\n\nwhere fx|\u0001(x|\u03b8 ) is a normal density with mean \u03b8 and variance 1. however, if we let z\nbe a standard normal random variable that is independent of \u0001, then the conditional\ndistribution of z + \u0001, given that \u0001 = \u03b8, is also normal with mean \u03b8 and variance 1.\n\n "}, {"Page_number": 382, "text": "section 7.8\n\nadditional properties of normal random variables 367\nconsequently, the joint density of z + \u0001, \u0001 is the same as that of x, \u0001. because the\nformer joint density is clearly bivariate normal (since z + \u0001 and \u0001 are both linear\ncombinations of the independent normal random variables z and \u0001), it follows that\nx, \u0001 has a bivariate normal distribution. now,\n\ne[x] = e[z + \u0001] = \u03bc\n\nvar(x) = var(z + \u0001) = 1 + \u03c3 2\n\nand\n\n\u03c1 = corr(x, \u0001)\n= corr(z + \u0001, \u0001)\ncov(z + \u0001, \u0001)\n=\n\u221a\nvar(z + \u0001)var(\u0001)\n\u03c3.\n=\n1 + \u03c3 2\n\nbecause x, \u0001 has a bivariate normal distribution, the conditional distribution of \u0001,\ngiven that x = x, is normal with mean\n\n?\n\ne[\u0001|x = x] = e[\u0001] + \u03c1\n\n(x \u2212 e[x])\n\nvar(\u0001)\nvar(x)\n(x \u2212 \u03bc)\n\n= \u03bc +\n\n\u03c3 2\n\n1 + \u03c3 2\n\nand variance\n\nvar(\u0001|x = x) = var(\u0001)(1 \u2212 \u03c12)\n\n= \u03c3 2\n\n1 + \u03c3 2\n\n.\n\n7.8.2 the joint distribution of the sample mean and sample variance\nlet x1, . . . , xn be independent normal random variables, each with mean \u03bc and vari-\nxi/n denote their sample mean. since the sum of independent\n\nance \u03c3 2. let x = n(cid:9)\n\ni=1\n\nnormal random variables is also a normal random variable, it follows that x is a nor-\nmal random variable with (from examples 2c and 4a) expected value \u03bc and variance\n\u03c3 2/n.\n\nnow, recall from example 4e that\n\ncov(x, xi \u2212 x) = 0,\n\ni = 1, . . . , n\n\n(8.1)\nalso, note that since x, x1 \u2212 x, x2 \u2212 x, . . . , xn \u2212 x are all linear combinations\nof the independent standard normals (xi \u2212 \u03bc)/\u03c3 , i = 1, . . . , n, it follows that x, xi \u2212\nx, i = 1, . . . , n has a joint distribution that is multivariate normal. if we let y be\na normal random variable, with mean \u03bc and variance \u03c3 2/n, that is independent of\nthe xi, i = 1, . . . , n, then y, xi \u2212 x, i = 1, . . . , n also has a multivariate normal dis-\ntribution and, indeed, because of equation (8.1), has the same expected values and\ncovariances as the random variables x, xi \u2212 x, i = 1, . . . , n. but since a multivariate\nnormal distribution is determined completely by its expected values and covariances,\n\n "}, {"Page_number": 383, "text": "368\n\nchapter 7\n\nproperties of expectation\nit follows that y, xi \u2212 x, i = 1, . . . , n and x, xi \u2212 x, i = 1, . . . , n have the same\njoint distribution, thus showing that x is independent of the sequence of deviations\nxi \u2212 x, i = 1, . . . , n.\nsince x is independent of the sequence of deviations xi \u2212 x, i = 1, . . . , n, it is also\n\nindependent of the sample variance s2 k n(cid:9)\n\n(xi \u2212 x)2/(n \u2212 1).\n\nsince we already know that x is normal with mean \u03bc and variance \u03c3 2/n, it remains\nonly to determine the distribution of s2. to accomplish this, recall, from example 4a,\nthe algebraic identity\n\ni=1\n\n(xi \u2212 x)2\n\ni=1\n\n(n \u2212 1)s2 = n(cid:6)\n= n(cid:6)\n(cid:4)\n\ni=1\n\n(n \u2212 1)s2\n\n+\n\n(xi \u2212 \u03bc)2 \u2212 n(x \u2212 \u03bc)2\n(cid:3)2\n\n(cid:2)\n\nxi \u2212 \u03bc\n\n\u03c3\n\n(cid:5)2 = n(cid:6)\n(cid:3)2\n\ni=1\n\nx \u2212 \u03bc\n\u221a\nn\n\u03c3/\n(cid:2)\nn(cid:6)\n\ni=1\n\n\u03c3\n\nxi \u2212 \u03bc\n\nupon dividing the preceding equation by \u03c3 2, we obtain\n\n\u03c3 2\n\nnow,\n\n(8.2)\n\nis the sum of the squares of n independent standard normal random variables and so\nis a chi-squared random variable with n degrees of freedom. hence, from example 7i,\nits moment generating function is (1 \u2212 2t)\u2212n/2. also, because\n\n(cid:5)2\n\n(cid:4)\n\nx \u2212 \u03bc\n\u221a\nn\n\u03c3/\n\nis the square of a standard normal variable, it is a chi-squared random variable with\n1 degree of freedom, and so has moment generating function (1 \u2212 2t)\u22121/2. now, we\nhave seen previously that the two random variables on the left side of equation (8.2)\nare independent. hence, as the moment generating function of the sum of indepen-\ndent random variables is equal to the product of their individual moment generating\nfunctions, we have\n\nor\n\ne[et(n\u22121)s2/\u03c3 2](1 \u2212 2t)\u22121/2 = (1 \u2212 2t)\u2212n/2\n\ne[et(n\u22121)s2/\u03c3 2] = (1 \u2212 2t)\u2212(n\u22121)/2\n\nbut as (1 \u2212 2t)\u2212(n\u22121)/2 is the moment generating function of a chi-squared random\nvariable with n \u2212 1 degrees of freedom, we can conclude, since the moment gener-\nating function uniquely determines the distribution of the random variable, it follows\nthat that is the distribution of (n \u2212 1)s2/\u03c3 2.\nsumming up, we have shown the following.\n\n "}, {"Page_number": 384, "text": "section 7.9\n\ngeneral definition of expectation 369\n\nproposition 8.1. if x1, . . . , xn are independent and identically distributed normal\nrandom variables with mean \u03bc and variance \u03c3 2, then the sample mean x and the\nsample variance s2 are independent. x is a normal random variable with mean \u03bc and\nvariance \u03c3 2/n; (n \u2212 1)s2/\u03c3 2 is a chi-squared random variable with n \u2212 1 degrees of\nfreedom.\n\n7.9 general definition of expectation\n\nup to this point, we have defined expectations only for discrete and continuous ran-\ndom variables. however, there also exist random variables that are neither discrete\nnor continuous, and they, too, may possess an expectation. as an example of such a\nrandom variable, let x be a bernoulli random variable with parameter p = 1\n2, and let\ny be a uniformly distributed random variable over the interval [0, 1]. furthermore,\nsuppose that x and y are independent, and define the new random variable w by\n\n%\nx if x = 1\ny if x z 1\n\nw =\n\nclearly, w is neither a discrete (since its set of possible values, [0, 1], is uncountable)\nnor a continuous (since p{w = 1} = 1\n\n2) random variable.\n\nin order to define the expectation of an arbitrary random variable, we require the\nnotion of a stieltjes integral. before defining this integral, let us recall that, for any\nfunction g,\n\nb\na g(x) dx is defined by\n\n-\n\nb\n\ng(x) dx = lim\n\na\n\ng(xi)(xi \u2212 xi\u22121)\n\nwhere the limit is taken over all a = x0 < x1 < x2 \u00b7\u00b7\u00b7 < xn = b as n\u2192q and where\nmax\ni=1,...,n\n\nfor any distribution function f, we define the stieltjes integral of the nonnegative\n\nfunction g over the interval [a, b] by\n\n(xi \u2212 xi\u22121)\u21920.\n*\n\nb\n\ng(x) df(x) = lim\n\ng(xi)[f(xi) \u2212 f(xi\u22121)]\n\nwhere, as before, the limit is taken over all a = x0 < x1 < \u00b7\u00b7\u00b7 < xn = b as n\u2192q and\n(xi \u2212 xi\u22121)\u21920. further, we define the stieltjes integral over the whole\nwhere max\ni=1,...,n\nreal line by\n\n*\n\nn(cid:6)\n\ni=1\n\nn(cid:6)\n\ni=1\n\n*\n\na\n\n* q\n\n\u2212q\n\ng(x) df(x) =\n\nlim\n\na\u2192 \u2212 q\nb\u2192 + q\n\nb\n\na\n\ng(x) df(x)\n\nfinally, if g is not a nonnegative function, we define g\n\n+\n\nand g\n\n\u2212\n\nby\n\n%\n%\ng(x)\n0\n0\n\u2212g(x)\n\n+(x) =\n\u2212(x) =\n\ng\n\ng\n\nif g(x) \u00fa 0\nif g(x) < 0\nif g(x) \u00fa 0\nif g(x) < 0\n\n "}, {"Page_number": 385, "text": "370\n\nchapter 7\n\nproperties of expectation\nbecause g(x) = g\n+(x) \u2212 g\nnatural to define* q\n- q\n\nand we say that\nare not both equal to +q.\n\n\u2212q\n\ng(x) df(x) =\n\nthe expected value of x by\n\n+\n\n\u2212\n\nand g\n\n\u2212(x) and g\n* q\n\n* q\n\nare both nonnegative functions, it is\n\ng\n\n\u2212q\n\n+(x) df(x) \u2212\n- q\n\ng\n\n\u2212(x) df(x)\n- q\n\u2212q\n+(x) df(x) and\n\n\u2212(x) df(x)\n\n\u2212q g\n\n\u2212q g(x) df(x) exists as long as\n\n\u2212q g\n\nif x is an arbitrary random variable having cumulative distribution f, we define\n\nx df(x)\n\n(9.1)\n\nit can be shown that if x is a discrete random variable with mass function p(x), then\n\n* q\n\n\u2212q\n\ne[x] =\n* q\n\nxdf(x) =\n\n(cid:6)\n* q\n\nx:p(x)>0\n\nxp(x)\n\nxdf(x) =\n\nxf (x) dx\n\n\u2212q\n\nxi[f(xi) \u2212 f(xi\u22121)]\n\n\u2212q\n\n* q\n\n\u2212q\n\nn(cid:6)\n\ni=1\n\nwhereas if x is a continuous random variable with density function f (x), then\n\nthe reader should note that equation (9.1) yields an intuitive definition of e[x];\n\nconsider the approximating sum\n\nof e[x]. because f(xi) \u2212 f(xi\u22121) is just the probability that x will be in the interval\n(xi\u22121, xi], the approximating sum multiplies the approximate value of x when it is in\nthe interval (xi\u22121, xi] by the probability that it will be in that interval and then sums\nover all the intervals. clearly, as these intervals get smaller and smaller in length, we\nobtain the \u201cexpected value\u201d of x.\n\nstieltjes integrals are mainly of theoretical interest because they yield a compact\nway of defining and dealing with the properties of expectation. for instance, the use\nof stieltjes integrals avoids the necessity of having to give separate statements and\nproofs of theorems for the continuous and the discrete cases. however, their prop-\nerties are very much the same as those of ordinary integrals, and all of the proofs\npresented in this chapter can easily be translated into proofs in the general case.\n\nsummary\nif x and y have a joint probability mass function p(x, y), then\n\n(cid:6)\n(cid:6)\ne[g(x, y)] =\n* q\n* q\n\ny\n\nx\n\ng(x, y)p(x, y)\n\nwhereas if they have a joint density function f (x, y), then\n\ne[g(x, y)] =\n\ng(x, y)f (x, y) dx dy\n\n\u2212q\n\n\u2212q\n\na consequence of the preceding equations is that\n\ne[x + y] = e[x] + e[y]\n\n "}, {"Page_number": 386, "text": "m(cid:6)\n\nj=1\n\nxi,\n\n\u239b\n\u239c\u239d n(cid:6)\n\u239e\n\u23a0 = n(cid:6)\n\ni=1\n\nxi\n\ni=1\n\n\u03c1(x, y) =\n\ne[x|y = y] =\n\nwhich generalizes to\n\n\u23a1\n\u23a3 n(cid:6)\n\ni=1\n\ne\n\n\u23a4\n\u23a6 = n(cid:6)\n\ni=1\n\nxi\n\ne[xi]\n\nsummary 371\n\nthe covariance between random variables x and y is given by\n\ncov(x, y) = e[(x \u2212 e[x])(y \u2212 e[y])] = e[xy] \u2212 e[x]e[y]\n\na useful identity is\n\ncov\n\n\u239b\n\u239d n(cid:6)\n\ni=1\n\nvar\n\n\u239e\n\u239f\u23a0 = n(cid:6)\n\ni=1\n\nyj\n\nm(cid:6)\n\nj=1\n\ncov(xi, yj)\n\n(cid:6)(cid:6)\n\nvar(xi) + 2\n\ncov(xi, yj)\n\ni<j\n\nwhen n = m and yi = xi, i = 1, . . . , n, the preceding formula gives\n\nthe correlation between x and y, denoted by \u03c1(x, y), is defined by\n\ncov(x, y)\nvar(x)var(y)\n\n\u221a\n(cid:6)\nxp{x = x|y = y]\n* q\n\nx\n\nif x and y are jointly discrete random variables, then the conditional expected value\nof x, given that y = y, is defined by\n\nif x and y are jointly continuous random variables, then\nxfx|y (x|y)\n\ne[x|y = y] =\n\n\u2212q\n\nwhere\n\nfx|y (x|y) = f (x, y)\nfy (y)\n\nis the conditional probability density of x given that y = y. conditional expecta-\ntions, which are similar to ordinary expectations except that all probabilities are now\ncomputed conditional on the event that y = y, satisfy all the properties of ordinary\nexpectations.\nlet e[x|y] denote that function of y whose value at y = y is e[x|y = y]. a very\n\nuseful identity is\n\nin the case of discrete random variables, this equation reduces to the identity\n\ne[x] = e[e[x|y]]\n(cid:6)\ne[x|y = y]p{y = y}\n* q\n\ny\n\ne[x|y = y]fy (y)\n\n\u2212q\n\ne[x] =\n\ne[x] =\n\nand, in the continuous case, to\n\n "}, {"Page_number": 387, "text": "372\n\nchapter 7\n\nproperties of expectation\n\nthe preceding equations can often be applied to obtain e[x] by first \u201cconditioning\u201d\non the value of some other random variable y. in addition, since, for any event a,\np(a) = e[ia], where ia is 1 if a occurs and is 0 otherwise, we can use the same\nequations to compute probabilities.\n\nthe conditional variance of x, given that y = y, is defined by\nvar(x|y = y) = e[(x \u2212 e[x|y = y])2|y = y]\n\nlet var(x|y) be that function of y whose value at y = y is var(x|y = y). the\nfollowing is known as the conditional variance formula:\n\nvar(x) = e[var(x|y)] + var(e[x|y])\n\nsuppose that the random variable x is to be observed and, on the basis of its value,\none must then predict the value of the random variable y. in such a situation, it turns\nout that, among all predictors, e[y|x] has the smallest expectation of the square of\nthe difference between it and y.\n\nthe moment generating function of the random variable x is defined by\n\nm(t) = e[etx]\n\nthe moments of x can be obtained by successively differentiating m(t) and then\nevaluating the resulting quantity at t = 0. specifically, we have\n\n66666\n\ne[xn] = dn\n\ndtn m(t)\n\nn = 1, 2, . . .\n\nt=0\n\ntwo useful results concerning moment generating functions are, first, that the\nmoment generating function uniquely determines the distribution function of the\nrandom variable and, second, that the moment generating function of the sum of\nindependent random variables is equal to the product of their moment generating\nfunctions. these results lead to simple proofs that the sum of independent normal\n(poisson, gamma) random variables remains a normal (poisson, gamma) random\nvariable.\n\nif x1, . . . , xm are all linear combinations of a finite set of independent standard\nnormal random variables, then they are said to have a multivariate normal distri-\nbution. their joint distribution is specified by the values of e[xi], cov(xi, xj), i, j =\n1, . . . , m.\nif x1, . . . , xn are independent and identically distributed normal random variables,\n\nthen their sample mean\n\nand their sample variance\n\nxi\nn\n\nx = n(cid:6)\ns2 = n(cid:6)\n\ni=1\n\ni=1\n\n(xi \u2212 x)2\nn \u2212 1\n\nare independent. the sample mean x is a normal random variable with mean \u03bc and\nvariance \u03c3 2/n; the random variable (n \u2212 1)s2/\u03c3 2 is a chi-squared random variable\nwith n \u2212 1 degrees of freedom.\n\n "}, {"Page_number": 388, "text": "problems\n\nproblems 373\n\n7.1. a player throws a fair die and simultaneously\nflips a fair coin. if the coin lands heads, then she\nwins twice, and if tails, then one-half of the value\nthat appears on the die. determine her expected\nwinnings.\n\n7.2. the game of clue involves 6 suspects, 6 weapons,\nand 9 rooms. one of each is randomly chosen and\nthe object of the game is to guess the chosen three.\n(a) how many solutions are possible?\n\nin one version of the game, the selection is\nmade and then each of the players is randomly\ngiven three of the remaining cards. let s, w,\nand r be, respectively, the numbers of sus-\npects, weapons, and rooms in the set of three\ncards given to a specified player. also, let x\ndenote the number of solutions that are possi-\nble after that player observes his or her three\ncards.\n\n(b) express x in terms of s, w, and r.\n(c) find e[x].\n\n7.3. gambles are independent, and each one results in\nthe player being equally likely to win or lose 1 unit.\nlet w denote the net winnings of a gambler whose\nstrategy is to stop gambling immediately after his\nfirst win. find\n(a) p{w > 0}\n(b) p{w < 0}\n(c) e[w]\n%\n\n7.4. if x and y have joint density function\n\nfx,y (x, y) =\n\n1/y,\n0,\n\nif 0 < y < 1, 0 < x < y\notherwise\n\nfind\n(a) e[xy]\n(b) e[x]\n(c) e[y]\n\n7.5. the county hospital is located at the center of a\nsquare whose sides are 3 miles wide. if an accident\noccurs within this square, then the hospital sends\nout an ambulance. the road network is rectangu-\nlar, so the travel distance from the hospital, whose\ncoordinates are (0, 0), to the point (x, y) is |x| + |y|.\nif an accident occurs at a point that is uniformly\ndistributed in the square, find the expected travel\ndistance of the ambulance.\n\n7.6. a fair die is rolled 10 times. calculate the expected\n\nsum of the 10 rolls.\n\n7.7. suppose that a and b each randomly and indepen-\ndently choose 3 of 10 objects. find the expected\nnumber of objects\n(a) chosen by both a and b;\n(b) not chosen by either a or b;\n(c) chosen by exactly one of a and b.\n\n7.8. n people arrive separately to a professional din-\nner. upon arrival, each person looks to see if he\nor she has any friends among those present. that\nperson then sits either at the table of a friend or\nat an unoccupied table if none of those present\nis a friend. assuming that each of the\npairs of people is, independently, a pair of friends\nwith probability p, find the expected number of\noccupied tables.\nhint: let xi equal 1 or 0, depending on whether\nthe ith arrival sits at a previously unoccupied\ntable.\n\n(cid:2)\n\n(cid:3)\n\nn\n2\n\n7.9. a total of n balls, numbered 1 through n, are put\ninto n urns, also numbered 1 through n in such a\nway that ball i is equally likely to go into any of the\nurns 1, 2, . . . , i. find\n(a) the expected number of urns that are empty;\n(b) the probability that none of\nthe urns is\n\nempty.\n\n7.10. consider 3 trials, each having the same proba-\nbility of success. let x denote the total num-\nber of successes in these trials. if e[x] = 1.8,\nwhat is\n(a) the largest possible value of p{x = 3}?\n(b) the smallest possible value of p{x = 3}?\nin both cases, construct a probability scenario that\nresults in p{x = 3} having the stated value.\nhint: for part (b), you might start by letting u be a\nuniform random variable on (0, 1) and then defin-\ning the trials in terms of the value of u.\n\n7.11. consider n independent flips of a coin having\nprobability p of landing on heads. say that a\nchangeover occurs whenever an outcome differs\nfrom the one preceding it. for instance, if n =\n5 and the outcome is hhtht, then there are\n3 changeovers. find the expected number of\nchangeovers.\nhint: express the number of changeovers as the\nsum of n \u2212 1 bernoulli random variables.\n\n7.12. a group of n men and n women is lined up at\n\nrandom.\n(a) find the expected number of men who have a\n\nwoman next to them.\n\n(b) repeat part (a), but now assuming that the\n\ngroup is randomly seated at a round table.\n\n7.13. a set of 1000 cards numbered 1 through 1000\nis randomly distributed among 1000 people with\neach receiving one card. compute the expected\nnumber of cards that are given to people whose\nage matches the number on the card.\n\n7.14. an urn has m black balls. at each stage, a black\nball is removed and a new ball that is black with\n\n "}, {"Page_number": 389, "text": "374\n\nproperties of expectation\n\nchapter 7\nprobability p and white with probability 1 \u2212 p\nis put in its place. find the expected number of\nstages needed until there are no more black balls\nin the urn.\nnote: the preceding has possible applications\nto understanding the aids disease. part of the\nbody\u2019s immune system consists of a certain class\nof cells known as t-cells. there are 2 types of t-\ncells, called cd4 and cd8. now, while the total\nnumber of t-cells in aids sufferers is (at least in\nthe early stages of the disease) the same as that\nin healthy individuals, it has recently been dis-\ncovered that the mix of cd4 and cd8 t-cells is\ndifferent. roughly 60 percent of the t-cells of a\nhealthy person are of the cd4 type, whereas the\npercentage of the t-cells that are of cd4 type\nappears to decrease continually in aids sufferers.\na recent model proposes that the hiv virus (the\nvirus that causes aids) attacks cd4 cells and that\nthe body\u2019s mechanism for replacing killed t-cells\ndoes not differentiate between whether the killed\nt-cell was cd4 or cd8. instead, it just produces\na new t-cell that is cd4 with probability .6 and\ncd8 with probability .4. however, although this\nwould seem to be a very efficient way of replac-\ning killed t-cells when each one killed is equally\nlikely to be any of the body\u2019s t-cells (and thus has\nprobability .6 of being cd4), it has dangerous con-\nsequences when facing a virus that targets only the\ncd4 t-cells.\n\n7.15. in example 2h, say that i and j, i z j, form a\nmatched pair if i chooses the hat belonging to j and\nj chooses the hat belonging to i. find the expected\nnumber of matched pairs.\n\n7.16. let z be a standard normal random variable, and,\n\nfor a fixed x, set\n\n%\n\nx =\n\nz if z > x\notherwise\n0\nshow that e[x] = 1\u221a\n\u2212x2/2.\n2\u03c0\n\ne\n\n7.17. a deck of n cards numbered 1 through n is thor-\noughly shuffled so that all possible n! orderings can\nbe assumed to be equally likely. suppose you are\nto make n guesses sequentially, where the ith one\nis a guess of the card in position i. let n denote\nthe number of correct guesses.\n(a) if you are not given any information about\nyour earlier guesses show that, for any strat-\negy, e[n] = 1.\n\n(b) suppose that after each guess you are shown\nthe card that was in the position in question.\nwhat do you think is the best strategy? show\nthat, under this strategy,\n\n*\ne[n] = 1\nn\n\nl\n\n+\nn\n\n1\nx\n\n1\n\n+ \u00b7\u00b7\u00b7 + 1\n\n1\n\nn \u2212 1\ndx = log n\n\n(c) suppose that you are told after each guess\nwhether you are right or wrong. in this case,\nit can be shown that the strategy which maxi-\nmizes e[n] is one that keeps on guessing the\nsame card until you are told you are correct\nand then changes to a new card. for this strat-\negy, show that\n\ne[n] = 1 + 1\n2!\nl e \u2212 1\n\n+ 1\n3!\n\n+ \u00b7\u00b7\u00b7 + 1\nn!\n\nhint: for all parts, express n as the sum of indica-\ntor (that is, bernoulli) random variables.\n\n7.18. cards from an ordinary deck of 52 playing cards\nare turned face up one at a time. if the 1st card\nis an ace, or the 2nd a deuce, or the 3rd a three,\nor . . ., or the 13th a king, or the 14 an ace, and so\non, we say that a match occurs. note that we do\nnot require that the (13n + 1)th card be any par-\nticular ace for a match to occur but only that it be\nan ace. compute the expected number of matches\nthat occur.\n\n7.19. a certain region is inhabited by r distinct types of\na certain species of insect. each insect caught will,\nindependently of the types of the previous catches,\nbe of type i with probability\npi, i = 1, . . . , r\n\npi = 1\n\nr(cid:6)\n\n1\n\n(a) compute the mean number of insects that are\n\ncaught before the first type 1 catch.\n\n(b) compute the mean number of types of insects\n\nthat are caught before the first type 1 catch.\n7.20. in an urn containing n balls, the ith ball has weight\nw(i), i = 1, . . . , n. the balls are removed with-\nout replacement, one at a time, according to the\nfollowing rule: at each selection, the probabil-\nity that a given ball in the urn is chosen is equal\nto its weight divided by the sum of the weights\nremaining in the urn. for instance, if at some\ntime i1, . . . , ir is the set of balls remaining in the\nurn, then the next selection will be ij with prob-\nw(ik), j = 1, . . . , r. compute\nability w(ij)\nthe expected number of balls that are withdrawn\nbefore ball number 1 is removed.\n\n(cid:26) r(cid:9)\n\nk=1\n\n7.21. for a group of 100 people, compute\n\n(a) the expected number of days of the year that\n\nare birthdays of exactly 3 people:\n\n(b) the expected number of distinct birthdays.\n\n "}, {"Page_number": 390, "text": "7.22. how many times would you expect to roll a fair die\n\nbefore all 6 sides appeared at least once?\n\n7.23. urn 1 contains 5 white and 6 black balls, while urn\n2 contains 8 white and 10 black balls. two balls\nare randomly selected from urn 1 and are put into\nurn 2. if 3 balls are then randomly selected from\nurn 2, compute the expected number of white balls\nin the trio.\nhint: let xi = 1 if the ith white ball initially in urn\n1 is one of the three selected, and let xi = 0 other-\nwise. similarly, let yi = 1 if the ith white ball from\nurn 2 is one of the three selected, and let yi = 0\notherwise. the number of white balls in the trio\ncan now be written as\n\nxi + 8(cid:9)\n\n5(cid:9)\n\nyi.\n\n1\n\n1\n\n7.24. a bottle initially contains m large pills and n small\npills. each day, a patient randomly chooses one of\nthe pills. if a small pill is chosen, then that pill is\neaten. if a large pill is chosen, then the pill is bro-\nken in two; one part is returned to the bottle (and\nis now considered a small pill) and the other part\nis then eaten.\n(a) let x denote the number of small pills in the\nbottle after the last large pill has been chosen\nand its smaller half returned. find e[x].\nhint: define n + m indicator variables, one for\neach of the small pills initially present and one\nfor each of the m small pills created when a large\none is split in two. now use the argument of\nexample 2m.\n(b) let y denote the day on which the last large\n\npill is chosen. find e[y].\n\nhint: what is the relationship between x and y?\n7.25. let x1, x2, . . . be a sequence of independent and\nidentically distributed continuous random vari-\nables. let n \u00fa 2 be such that\n\nx1 \u00fa x2 \u00fa \u00b7\u00b7\u00b7 \u00fa xn\u22121 < xn\n\nthat is, n is the point at which the sequence stops\ndecreasing. show that e[n] = e.\nhint: first find p{n \u00fa n}.\n\n7.26. if x1, x2, . . . , xn are independent and identically\ndistributed random variables having uniform dis-\ntributions over (0, 1), find\n(a) e[max(x1, . . . , xn)];\n(b) e[min(x1, . . . , xn)].\n\n\u22177.27. if 101 items are distributed among 10 boxes, then\nat least one of the boxes must contain more than 10\nitems. use the probabilistic method to prove this\nresult.\n\u22177.28. the k-of-r-out-of-n circular reliability system, k \u2026\nr \u2026 n, consists of n components that are arranged in\na circular fashion. each component is either func-\ntional or failed, and the system functions if there\nis no block of r consecutive components of which\n\nproblems 375\n\nat least k are failed. show that there is no way\nto arrange 47 components, 8 of which are failed,\nto make a functional 3-of-12-out-of-47 circular\nsystem.\n\u22177.29. there are 4 different types of coupons, the first\n2 of which compose one group and the second 2\nanother group. each new coupon obtained is type i\nwith probability pi, where p1 = p2 = 1/8, p3 =\np4 = 3/8. find the expected number of coupons\nthat one must obtain to have at least one of\n(a) all 4 types;\n(b) all the types of the first group;\n(c) all the types of the second group;\n(d) all the types of either group.\n\n7.30. if x and y are independent and identically dis-\n\ntributed with mean \u03bc and variance \u03c3 2, find\n\ne[(x \u2212 y)2]\n\n7.31. in problem 6, calculate the variance of the sum of\n\nthe rolls.\n\n7.32. in problem 9, compute the variance of the number\n7.33. if e[x] = 1 and var(x) = 5, find\n\nof empty urns.\n(a) e[(2 + x)2];\n(b) var(4 + 3x).\n\n7.34. if 10 married couples are randomly seated at a\nround table, compute (a) the expected number and\n(b) the variance of the number of wives who are\nseated next to their husbands.\n\n7.35. cards from an ordinary deck are turned face up\none at a time. compute the expected number of\ncards that need to be turned face up in order to\nobtain\n(a) 2 aces;\n(b) 5 spades;\n(c) all 13 hearts.\n\n7.36. let x be the number of 1\u2019s and y the number\nof 2\u2019s that occur in n rolls of a fair die. compute\ncov(x, y).\n\n7.37. a die is rolled twice. let x equal the sum of the\noutcomes, and let y equal the first outcome minus\nthe second. compute cov(x, y).\n\n7.38. the random variables x and y have a joint density\n\nfunction given by\n\n0\n\nf (x, y) =\n\n\u22122x/x 0 \u2026 x < q, 0 \u2026 y \u2026 x\n2e\n0\n\notherwise\n\ncompute cov(x, y).\n7.39. let x1, . . . be independent with common mean \u03bc\nand common variance \u03c3 2, and set yn = xn +\nxn+1 + xn+2. for j \u00fa 0, find cov(yn, yn+j).\n\n "}, {"Page_number": 391, "text": "376\n\nchapter 7\n\nproperties of expectation\n\n7.40. the joint density function of x and y is given by\n\nf (x, y) = 1\ny\n\n\u2212(y+x/y),\n\ne\n\nx > 0, y > 0\n\nfind e[x], e[y], and show that cov(x, y) = 1.\n\n7.41. a pond contains 100 fish, of which 30 are carp. if 20\nfish are caught, what are the mean and variance of\nthe number of carp among the 20? what assump-\ntions are you making?\n\n7.42. a group of 20 people consisting of 10 men and\n10 women is randomly arranged into 10 pairs of\n2 each. compute the expectation and variance of\nthe number of pairs that consist of a man and a\nwoman. now suppose the 20 people consist of 10\nmarried couples. compute the mean and variance\nof the number of married couples that are paired\ntogether.\n\n7.43. let x1, x2, . . . , xn be independent random vari-\nables having an unknown continuous distribution\nfunction f, and let y1, y2, . . . , ym be independent\nrandom variables having an unknown continuous\ndistribution function g. now order those n + m\nvariables, and let\n\n\u23a7\u23a8\n\u23a91\n\nii =\n\nif the ith smallest of the n + m\nvariables is from the x sample\n\n0 otherwise\n\nthe random variable r = n+m(cid:9)\n\ni=1\n\niii is the sum of the\nranks of the x sample and is the basis of a standard\nstatistical procedure (called the wilcoxon sum-of-\nranks test) for testing whether f and g are iden-\ntical distributions. this test accepts the hypothesis\nthat f = g when r is neither too large nor too\nsmall. assuming that the hypothesis of equality is\nin fact correct, compute the mean and variance\nof r.\nhint: use the results of example 3e.\n\n7.44. between two distinct methods for manufacturing\ncertain goods, the quality of goods produced by\nmethod i is a continuous random variable having\ndistribution fi, i = 1, 2. suppose that n goods are\nproduced by method 1 and m by method 2. rank\nthe n + m goods according to quality, and let\nif the jth best was produced from\nmethod 1\n2 otherwise\n\n\u23a7\u23a8\n\u23a91\n\nxj =\n\n7.45. if x1, x2, x3, and x4 are (pairwise) uncorrelated\nrandom variables, each having mean 0 and vari-\nance 1, compute the correlations of\n(a) x1 + x2 and x2 + x3;\n(b) x1 + x2 and x3 + x4.\n\n7.46. consider the following dice game, as played at a\ncertain gambling casino: players 1 and 2 roll a pair\nof dice in turn. the bank then rolls the dice to\ndetermine the outcome according to the follow-\ning rule: player i, i = 1, 2, wins if his roll is strictly\ngreater than the bank\u2019s. for i = 1, 2, let\n\n%\n\nii =\n\nif i wins\n\n1\n0 otherwise\n\n7.47. consider a graph having n vertices\n\n(cid:3)\n\n(cid:2)\n\nand show that i1 and i2 are positively correlated.\nexplain why this result was to be expected.\n\nn\n2\n\nlabeled\n1, 2, . . . , n, and suppose that, between each of the\npairs of distinct vertices, an edge is indepen-\ndently present with probability p. the degree of\nvertex i, designated as di, is the number of edges\nthat have vertex i as one of their vertices.\n(a) what is the distribution of di?\n(b) find \u03c1(di, dj), the correlation between di\n\nand dj.\n\n7.48. a fair die is successively rolled. let x and y\ndenote, respectively, the number of rolls necessary\nto obtain a 6 and a 5. find\n(a) e[x];\n(b) e[x|y = 1];\n(c) e[x|y = 5].\n\n7.49. there are two misshapen coins in a box; their\nprobabilities for landing on heads when they are\nflipped are, respectively, .4 and .7. one of the coins\nis to be randomly chosen and flipped 10 times.\ngiven that two of the first three flips landed on\nheads, what is the conditional expected number of\nheads in the 10 flips?\n\n7.50. the joint density of x and y is given by\n\n0 < x < q,\n\n0 < y < q\n\n\u2212y\n\n\u2212x/ye\ny\n\nf (x, y) = e\ncompute e[x2|y = y].\n\n,\n\nf (x, y) = e\n\n\u2212y\ny\n\ncompute e[x3|y = y].\n\n7.51. the joint density of x and y is given by\n\n,\n\n0 < x < y,\n\n0 < y < q\n\nfor the vector x1, x2, . . . , xn+m, which consists of\nn 1\u2019s and m 2\u2019s, let r denote the number of runs\nof 1. for instance, if n = 5, m = 2, and x =\n1, 2, 1, 1, 1, 1, 2, then r = 2. if f1 = f2 (that is,\nif the two methods produce identically distributed\ngoods), what are the mean and variance of r?\n\n7.52. a population is made up of r disjoint subgroups.\nlet pi denote the proportion of the population that\nis in subgroup i, i = 1, . . . , r. if the average weight\nof the members of subgroup i is wi, i = 1, . . . , r,\nwhat is the average weight of the members of the\npopulation?\n\n "}, {"Page_number": 392, "text": "7.53. a prisoner is trapped in a cell containing 3 doors.\nthe first door leads to a tunnel that returns him\nto his cell after 2 days\u2019 travel. the second leads\nto a tunnel that returns him to his cell after 4\ndays\u2019 travel. the third door leads to freedom after\n1 day of travel. if it is assumed that the pris-\noner will always select doors 1, 2, and 3 with\nrespective probabilities .5, .3, and .2, what is the\nexpected number of days until the prisoner reaches\nfreedom?\n\n7.54. consider the following dice game: a pair of dice\nis rolled. if the sum is 7, then the game ends and\nyou win 0. if the sum is not 7, then you have the\noption of either stopping the game and receiv-\ning an amount equal to that sum or starting over\nagain. for each value of i, i = 2, . . . , 12, find your\nexpected return if you employ the strategy of stop-\nping the first time that a value at least as large\nas i appears. what value of i leads to the largest\nexpected return?\nhint: let xi denote the return when you use the\ncritical value i. to compute e[xi], condition on the\ninitial sum.\n\n7.55. ten hunters are waiting for ducks to fly by. when\na flock of ducks flies overhead, the hunters fire at\nthe same time, but each chooses his target at ran-\ndom, independently of the others. if each hunter\nindependently hits his target with probability .6,\ncompute the expected number of ducks that are\nhit. assume that the number of ducks in a flock is\na poisson random variable with mean 6.\n\n7.56. the number of people who enter an elevator on\nthe ground floor is a poisson random variable with\nmean 10. if there are n floors above the ground\nfloor, and if each person is equally likely to get off\nat any one of the n floors, independently of where\nthe others get off, compute the expected number\nof stops that the elevator will make before dis-\ncharging all of its passengers.\n\n7.57. suppose that the expected number of accidents per\nweek at an industrial plant is 5. suppose also that\nthe numbers of workers injured in each accident\nare independent random variables with a common\nmean of 2.5. if the number of workers injured in\neach accident is independent of the number of\naccidents that occur, compute the expected num-\nber of workers injured in a week.\n\n7.58. a coin having probability p of coming up heads is\ncontinually flipped until both heads and tails have\nappeared. find\n(a) the expected number of flips;\n(b) the probability that the last flip lands on\n7.59. there are n + 1 participants in a game. each\nperson independently is a winner with probabil-\nity p. the winners share a total prize of 1 unit.\n\nheads.\n\nproblems 377\n\n(for instance, if 4 people win, then each of them\nreceives 1\n4 , whereas if there are no winners, then\nnone of the participants receive anything.) let a\ndenote a specified one of the players, and let x\ndenote the amount that is received by a.\n(a) compute the expected total prize shared by\n\nthe players.\n\n(b) argue that e[x] = 1 \u2212 (1 \u2212 p)n+1\n\n.\n\nn + 1\n\n(c) compute e[x] by conditioning on whether a\n\nis a winner, and conclude that\n\ne[(1 + b)\u22121] = 1 \u2212 (1 \u2212 p)n+1\n\n(n + 1)p\n\nwhen b is a binomial random variable with param-\neters n and p.\n7.60. each of m + 2 players pays 1 unit to a kitty in\norder to play the following game: a fair coin is to\nbe flipped successively n times, where n is an odd\nnumber, and the successive outcomes are noted.\nbefore the n flips, each player writes down a pre-\ndiction of the outcomes. for instance, if n = 3,\nthen a player might write down (h, h, t), which\nmeans that he or she predicts that the first flip\nwill land on heads, the second on heads, and the\nthird on tails. after the coins are flipped, the play-\ners count their total number of correct predictions.\nthus, if the actual outcomes are all heads, then the\nplayer who wrote (h, h, t) would have 2 correct\npredictions. the total kitty of m + 2 is then evenly\nsplit up among those players having the largest\nnumber of correct predictions.\n\nsince each of the coin flips is equally likely to\nland on either heads or tails, m of the players have\ndecided to make their predictions in a totally ran-\ndom fashion. specifically, they will each flip one\nof their own fair coins n times and then use the\nresult as their prediction. however, the final 2 of\nthe players have formed a syndicate and will use\nthe following strategy: one of them will make pre-\ndictions in the same random fashion as the other\nm players, but the other one will then predict\nexactly the opposite of the first. that is, when the\nrandomizing member of the syndicate predicts an\nh, the other member predicts a t. for instance,\nif\nthe syndicate\npredicts (h, h, t), then the other one predicts (t,\nt, h).\n(a) argue that exactly one of the syndicate mem-\nbers will have more than n/2 correct predic-\ntions. (remember, n is odd.)\n\nthe randomizing member of\n\n(b) let x denote the number of the m nonsyndi-\ncate players that have more than n/2 correct\npredictions. what is the distribution of x?\n\n "}, {"Page_number": 393, "text": "378\n\nchapter 7\n\nproperties of expectation\n\n(c) with x as defined in part (b), argue that\ne[payoff to the syndicate] = (m + 2)\n1\n\nx + 1\n(d) use part (c) of problem 59 to conclude that\n\n*e\n\n(cid:20)\n\n(cid:21)\n\ne[payoff to the syndicate] = 2(m + 2)\n(cid:7)\n(cid:2)\nm + 1\n1 \u2212\n\n*\n\n1\n2\n\n(cid:8)\n(cid:3)m+1\n\nand explicitly compute this number when m =\n1, 2, and 3. because it can be shown that\n\n(cid:7)\n\n(cid:2)\n\n(cid:8)\n\n(cid:3)m+1\n\n2(m + 2)\nm + 1\n\n1 \u2212\n\n1\n2\n\n> 2\n\nit follows that the syndicate\u2019s strategy always\ngives it a positive expected profit.\n\n7.61. let x1, . . . be independent random variables with\nthe common distribution function f, and sup-\npose they are independent of n, a geometric\nrandom variable with parameter p. let m =\nmax(x1, . . . , xn ).\n(a) find p{m \u2026 x} by conditioning on n.\n(b) find p{m \u2026 x|n = 1}.\n(c) find p{m \u2026 x|n > 1}.\n(d) use (b) and (c) to rederive the probability you\n\nfound in (a).\n\n7.62. let u1, u2, . . . be a sequence of independent uni-\nform (0, 1) random variables. in example 5i we\nshowed that, for 0 \u2026 x \u2026 1, e[n(x)] = ex, where\n\n\u23a7\u23a8\n\u23a9n :\n\nn(cid:6)\n\nui > x\n\ni=1\n\n\u23ab\u23ac\n\u23ad\n\nn(x) = min\n\nthis problem gives another approach to establish-\ning that result.\n(a) show by induction on n that, for 0 < x \u2026 1\n\nand all n \u00fa 0,\n\np{n(x) \u00fa n + 1} = xn\nn!\n\nhint: first condition on u1 and then use the\ninduction hypothesis.\nuse part (a) to conclude that\ne[n(x)] = ex\n\n7.63. an urn contains 30 balls, of which 10 are red and\n8 are blue. from this urn, 12 balls are randomly\nwithdrawn. let x denote the number of red and y\n\nthe number of blue balls that are withdrawn. find\ncov(x, y)\n(a) by defining appropriate indicator (that is,\n\nbernoulli) random variables\n\nxi, yj such that x = 10(cid:6)\n\nxi, y = 8(cid:6)\n\ni=1\n\nyj\n\nj=1\n\n(b) by conditioning (on either x or y) to deter-\n\nmine e[xy].\n\n7.64. type i light bulbs function for a random amount\nof time having mean \u03bci and standard deviation\n\u03c3i, i = 1, 2. a light bulb randomly chosen from a\nbin of bulbs is a type 1 bulb with probability p and\na type 2 bulb with probability 1 \u2212 p. let x denote\nthe lifetime of this bulb. find\n(a) e[x];\n(b) var(x).\n\n7.65. the number of winter storms in a good year is a\npoisson random variable with mean 3, whereas the\nnumber in a bad year is a poisson random variable\nwith mean 5. if next year will be a good year with\nprobability .4 or a bad year with probability .6, find\nthe expected value and variance of the number of\nstorms that will occur.\n\n7.66. in example 5c, compute the variance of the length\n\nof time until the miner reaches safety.\n\n7.67. consider a gambler who, at each gamble, either\nwins or loses her bet with respective probabilities p\nand 1 \u2212 p. a popular gambling system known\nas the kelley strategy is to always bet the frac-\ntion 2p \u2212 1 of your current fortune when p > 1\n2 .\ncompute the expected fortune after n gambles of\na gambler who starts with x units and employs the\nkelley strategy.\n\n7.68. the number of accidents that a person has in\na given year is a poisson random variable with\nmean \u03bb. however, suppose that the value of \u03bb\nchanges from person to person, being equal to 2\nfor 60 percent of the population and 3 for the other\n40 percent. if a person is chosen at random, what\nis the probability that he will have (a) 0 accidents\nand (b) exactly 3 accidents in a certain year? what\nis the conditional probability that he will have 3\naccidents in a given year, given that he had no acci-\ndents the preceding year?\n\n7.69. repeat problem 68 when the proportion of the\npopulation having a value of \u03bb less than x is equal\nto 1 \u2212 e\n\n\u2212x.\n\n7.70. consider an urn containing a large number of\ncoins, and suppose that each of the coins has some\nprobability p of turning up heads when it is flipped.\nhowever, this value of p varies from coin to coin.\nsuppose that the composition of the urn is such\nthat if a coin is selected at random from it, then\n\n "}, {"Page_number": 394, "text": "the p-value of the coin can be regarded as being\nthe value of a random variable that is uniformly\ndistributed over [0, 1]. if a coin is selected at ran-\ndom from the urn and flipped twice, compute the\nprobability that\n(a) the first flip results in a head;\n(b) both flips result in heads.\n\n7.71. in problem 70, suppose that the coin is tossed n\ntimes. let x denote the number of heads that\noccur. show that\n\np{x = i} = 1\n\nn + 1\n\ni = 0, 1, . . . , n\n\n*\n\n0\n\nhint: make use of the fact that\n\n1\n\nxa\u22121(1 \u2212 x)b\u22121 dx = (a \u2212 1)!(b \u2212 1)!\n(a + b \u2212 1)!\n\nwhen a and b are positive integers.\n\n7.72. suppose that in problem 70 we continue to flip the\ncoin until a head appears. let n denote the num-\nber of flips needed. find\n(a) p{n \u00fa i}, i \u00fa 0;\n(b) p{n = i};\n(c) e[n].\n\n7.73. in example 6b, let s denote the signal sent and r\n\nthe signal received.\n(a) compute e[r].\n(b) compute var(r).\n(c) is r normally distributed?\n(d) compute cov(r, s).\n\n7.74. in example 6c, suppose that x is uniformly dis-\ntributed over (0, 1). if the discretized regions are\ndetermined by a0 = 0, a1 = 1\n2 , and a2 = 1,\ncalculate the optimal quantizer y and compute\ne[(x \u2212 y)2].\n7.75. the moment generating function of x is given by\nmx (t) = exp{2et \u2212 2} and that of y by my (t) =\n4 et + 1\n( 3\n)10. if x and y are independent, what are\n(a) p{x + y = 2}?\n(b) p{xy = 0}?\n(c) e[xy]?\n\n4\n\n7.76. let x be the value of the first die and y the sum of\nthe values when two dice are rolled. compute the\njoint moment generating function of x and y.\n\n7.77. the joint density of x and y is given by\n\nf (x, y) = 1\u221a\n2\u03c0\n\n\u2212ye\ne\n\n\u2212(x\u2212y)2/2\n\n0 < y < q,\n\u2212q < x < q\n\nproblems 379\n\n(a) compute the joint moment generating func-\n\n(b) compute the individual moment generating\n\ntion of x and y.\n\nfunctions.\n\n7.78. two envelopes, each containing a check, are\nplaced in front of you. you are to choose one of\nthe envelopes, open it, and see the amount of the\ncheck. at this point, either you can accept that\namount or you can exchange it for the check in\nthe unopened envelope. what should you do? is it\npossible to devise a strategy that does better than\njust accepting the first envelope?\nlet a and b, a < b, denote the (unknown)\namounts of the checks, and note that the strat-\negy that randomly selects an envelope and always\naccepts its check has an expected return of\n(a + b)/2. consider the following strategy: let\nf(\u00b7) be any strictly increasing (that is, continu-\nous) distribution function. choose an envelope\nrandomly and open it. if the discovered check has\nthe value x, then accept it with probability f(x) and\nexchange it with probability 1 \u2212 f(x).\n(a) show that if you employ the latter strategy,\nthen your expected return is greater than\n(a + b)/2.\nhint: condition on whether the first envelope\nhas the value a or b.\nnow consider the strategy that fixes a value x\nand then accepts the first check if its value is\ngreater than x and exchanges it otherwise.\n\n(b) show that, for any x, the expected return\nleast\nunder\nthe x-strategy is always at\n(a + b)/2 and that it is strictly larger than\n(a + b)/2 if x lies between a and b.\n\n(c) let x be a continuous random variable on the\nwhole line, and consider the following strat-\negy: generate the value of x, and if x = x,\nthen employ the x-strategy of part (b). show\nthat the expected return under this strategy is\ngreater than (a + b)/2.\n\n7.79. successive weekly sales, in units of one thousand\ndollars, have a bivariate normal distribution with\ncommon mean 40, common standard deviation 6,\nand correlation .6.\n(a) find the probability that the total of the next\n\n2 weeks\u2019 sales exceeds 90.\n\n(b) if the correlation were .2 rather than .6, do\nyou think that this would increase or decrease\nthe answer to (a)? explain your reasoning.\n\n(c) repeat (a) when the correlation is .2.\n\n "}, {"Page_number": 395, "text": "380\n\nchapter 7\n\nproperties of expectation\n\ntheoretical exercises\n\n7.1. show that e[(x \u2212 a)2] is minimized at a = e[x].\n7.2. suppose that x is a continuous random variable\nwith density function f . show that e[|x \u2212 a|] is\nminimized when a is equal to the median of f.\nhint: write\n\n*\n\ne[|x \u2212 a|] =\n\n|x \u2212 a|f (x) dx\n\nnow break up the integral into the regions where\nx < a and where x > a, and differentiate.\n\n7.3. prove proposition 2.1 when\n\n(a) x and y have a joint probability mass func-\n\n(b) x and y have a joint probability density func-\n\ntion;\ntion and g(x, y) \u00fa 0 for all x, y.\n\n7.4. let x be a random variable having finite expec-\ntation \u03bc and variance \u03c3 2, and let g(\u00b7) be a twice\ndifferentiable function. show that\n\ne[g(x)] l g(\u03bc) + g\u201d(\u03bc)\n2\n\n\u03c3 2\n\nhint: expand g(\u00b7) in a taylor series about \u03bc. use\nthe first three terms and ignore the remainder.\nck = {at least k of the ai occur}. show that\n\n7.5. let a1, a2, . . . , an be arbitrary events, and define\n\nn(cid:6)\n\nk=1\n\np(ck) = n(cid:6)\n\nk=1\n\np(ak)\n\nhint: let x denote the number of the ai that\noccur. show that both sides of the preceding equa-\ntion are equal to e[x].\n\n7.6. in the text, we noted that\n\n\u23a4\n\u23a6 =\n\n\u23a1\n\u23a3 q(cid:6)\n\ni=1\n\ne\n\nxi\n\nq(cid:6)\n\ni=1\n\ne[xi]\n\nwhen the xi are all nonnegative random variables.\nsince an integral is a limit of sums, one might\nexpect that\n\n(cid:20)* q\n\n* q\n\n(cid:21)\n\n=\n\nwhenever x(t), 0 \u2026 t < q, are all nonnegative ran-\ndom variables; and this result is indeed true. use it\nto give another proof of the result that, for a non-\nnegative random variable x,\n\n* q\n\n0\n\ne[x) =\n\np{x > t} dt\n\nhint: define, for each nonnegative t, the random\nvariable x(t) by\n\n%\n\nx(t) =\n- q\n\n1\n0\n\nif t < x\nif t \u00fa x\n\n7.7. we say that x is stochastically larger than y, writ-\n\n0 x(t)dt to x.\n\nnow relate\nten x \u00fast y, if, for all t.\n\np{x > t} \u00fa p{y > t}\n\nshow that if x \u00fast y, then e[x] \u00fa e[y] when\n(a) x and y are nonnegative random variables;\n(b) x and y are arbitrary random variables.\nhint: write x as\n\nx = x\n\n+ \u2212 x\n\n\u2212\n\n%\n\nwhere\n+ =\n\nx\n\nonly if\n\nx if x \u00fa 0\n0\n\nif x < 0 , x\n\n\u2212 =\n\nsimilarly, represent y as y\nuse of part (a).\n\n%\nif x \u00fa 0\n\u2212x if x < 0\n\u2212\n\n. then make\n\n0\n\n+ \u2212 y\n\n7.8. show that x is stochastically larger than y if and\n\ne[f (x)] \u00fa e[f (y)]\n\nfor all increasing functions f .\nhint: show that x \u00fast y, then e[f (x)] \u00fa e[f (y)]\nby showing that f (x) \u00fast f (y) and then using the-\noretical exercise 7.7. to show that if e[f (x)] \u00fa\ne[f (y)] for all increasing functions f , then p{x >\nt} \u00fa p{y > t}, define an appropriate increasing\nfunction f .\n\n7.9. a coin having probability p of landing on heads\nis flipped n times. compute the expected number\nof runs of heads of size 1, of size 2, and of size\nk, 1 \u2026 k \u2026 n.\n\n7.10. let x1, x2, . . . , xn be independent and identically\ndistributed positive random variables. for k \u2026 n,\nfind\n\n\u23a1\n\n\u23a4\n\n\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a3\n\nk(cid:6)\nn(cid:6)\n\ni=1\n\ni=1\n\n\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a6\n\nxi\n\nxi\n\n7.11. consider n independent trials, each resulting in\nany one of r possible outcomes with probabilities\np1, p2, . . . , pr. let x denote the number of out-\ncomes that never occur in any of the trials. find\n\ne\n\n0\n\nx(t)dt\n\ne[x(t)] dt\n\n0\n\ne\n\n "}, {"Page_number": 396, "text": "e[x] and show that, among all probability vectors\np1, . . . , pr, e[x] is minimized when pi = 1/r, i =\n1, . . . , r.\n\n7.12. let x1, x2,... be a sequence of\n\nindependent\nrandom variables having the probability mass\nfunction\n\np{xn = 0} = p{xn = 2} = 1/2 ,\n\nthe random variable x = (cid:9)q\n\nn=1 xn/3n is said\nto have the cantor distribution. find e[x] and\nvar(x).\n\nn \u00fa 1\n\n7.13. let x1, . . . , xn be independent and identically dis-\ntributed continuous random variables. we say that\na record value occurs at time j, j \u2026 n, if xj \u00fa xi for\nall 1 \u2026 i \u2026 j. show that\n\n(a) e[number of record values] = n(cid:6)\n(b) var(number of record values)= n(cid:6)\n\n(j \u2212 1)/j2.\n\nj=1\n\n1/j;\n\nj=1\n\n7.14. for example 2i, show that the variance of the\nnumber of coupons needed to amass a full set is\nequal to\n\nn\u22121(cid:6)\n\ni=1\n\nin\n\n(n \u2212 i)2\n\nwhen n is large,\nthis can be shown to be\napproximately equal (in the sense that their ratio\napproaches 1 as n\u2192q) to n2\u03c0 2/6.\n\n7.15. consider n independent trials, the ith of which\n\nresults in a success with probability pi.\n(a) compute the expected number of successes in\n\n\u22177.16. suppose that each of\n\nthe n trials\u2014call it \u03bc.\n\n(b) for a fixed value of \u03bc, what choice of\np1, . . . , pn maximizes the variance of the num-\nber of successes?\n\n(c) what choice minimizes the variance?\nthe elements of s =\n{1, 2, . . . , n} is to be colored either red or blue.\nr(cid:9)\nshow that if a1, . . . , ar are subsets of s, there\nis a way of doing the coloring so that at most\n(1/2)|ai|\u22121 of these subsets have all their ele-\ni=1\nments the same color (where |a| denotes the num-\nber of elements in the set a).\n\n1 and var(x2) = \u03c3 2\n\n7.17. suppose that x1 and x2 are independent random\nvariables having a common mean \u03bc. suppose also\nthat var(x1) = \u03c3 2\n2 . the value\nof \u03bc is unknown, and it is proposed that \u03bc be esti-\nmated by a weighted average of x1 and x2. that\nis, \u03bbx1 + (1 \u2212 \u03bb)x2 will be used as an estimate\nof \u03bc for some appropriate value of \u03bb. which value\nof \u03bb yields the estimate having the lowest possible\n\ntheoretical exercises 381\n\nvariance? explain why it is desirable to use this\nvalue of \u03bb.\n\n7.18. in example 4f, we showed that the covariance of\nthe multinomial random variables ni and nj is\nequal to \u2212mpipj by expressing ni and nj as the\nsum of indicator variables. we could also have\nobtained that result by using the formula\nvar(ni + nj)=var(ni) + var(nj) + 2 cov(ni, nj)\n(a) what is the distribution of ni + nj?\n(b) use the preceding identity to show that\n\ncov(ni, nj) = \u2212mpipj.\n\n7.19. show that x and y are identically distributed and\n\nnot necessarily independent, then\n\ncov(x + y, x \u2212 y) = 0\n\n7.20. the conditional covariance formula. the con-\nis\n\nditional covariance of x and y, given z,\ndefined by\ncov(x, y|z) k e[(x \u2212 e[x|z])(y \u2212 e[y|z])|z]\n\n(a) show that\n\ncov(x, y|z) = e[xy|z] \u2212 e[x|z]e[y|z]\n\n(b) prove the conditional covariance formula\n\ncov(x, y) = e[cov(x, y|z)]\n\n+ cov(e[x|z], e[y|z])\n\ntional variance formula.\n\n(c) set x = y in part (b) and obtain the condi-\n7.21. let x(i), i = 1, . . . , n, denote the order statis-\ntics from a set of n uniform (0, 1) random vari-\nables, and note that the density function of x(i) is\ngiven by\nf (x) =\n\nxi\u22121(1 \u2212 x)n\u2212i\n\n0 < x < 1\n\nn!\n\n(i \u2212 1)!(n \u2212 i)!\n\n(a) compute var(x(i)), i = 1, . . . , n.\n(b) which value of i minimizes, and which value\n\nmaximizes, var(x(i))?\n\n7.22. show that y = a + bx, then\n\n%+1\n\n\u22121\n\n\u03c1(x, y) =\n\nif b > 0\nif b < 0\n\n7.23. show that z is a standard normal random variable\nand if y is defined by y = a + bz + cz2, then\n\n\u03c1(y, z) =\n\nb.\nb2 + 2c2\n\n "}, {"Page_number": 397, "text": "382\n\nchapter 7\n\nproperties of expectation\n\n7.24. prove the cauchy\u2013schwarz inequality, namely,\n\n(e[xy])2 \u2026 e[x2]e[y2]\n\nhint: unless y = \u2212tx for some constant, in which\ncase the inequality holds with equality, if follows\nthat, for all t,\n0 < e[(tx + y)2] = e[x2]t2 + 2e[xy]t + e[y2]\nhence, the roots of the quadratic equation\ne[x2]t2 + 2e[xy]t + e[y2] = 0\n\nmust be imaginary, which implies that the discrim-\ninant of this quadratic equation must be negative.\n\n7.25. show that if x and y are independent, then\n\ne[x|y = y] = e[x]\n\nfor all y\n\n(a) in the discrete case;\n(b) in the continuous case.\n\n7.26. prove that e[g(x)y|x] = g(x)e[y|x].\n7.27. prove that if e[y|x = x] = e[y] for all x, then x\nand y are uncorrelated; give a counterexample to\nshow that the converse is not true.\nhint: prove and use the fact that e[xy] =\ne[xe[y|x]].\n\n7.28. show that cov(x, e[y|x]) = cov(x, y).\n7.29. let x1, . . . , xn be independent and identically dis-\n\ntributed random variables. find\n\ne[x1|x1 + \u00b7\u00b7\u00b7 + xn = x]\n\n7.30. consider example 4f, which is concerned with the\nmultinomial distribution. use conditional expec-\ntation to compute e[ninj], and then use this to\nverify the formula for cov(ni, nj) given in exam-\nple 4f.\n\n7.31. an urn initially contains b black and w white balls.\nat each stage, we add r black balls and then with-\ndraw, at random, r balls from the b + w + r balls\nin the urn. show that\n\ne[number of white balls after stage t]\n\n(cid:2)\n\n=\n\n(cid:3)t\n\nw\n\nb + w\n\nb + w + r\n\nhint: condition on the time of the first occurrence\nof tails to obtain the equation\n\ne[x] = (1 \u2212 p)\n\ni=1\n+(1 \u2212 p)\n\nr(cid:6)\npi\u22121(i + e[x])\nq(cid:6)\n\npi\u22121r\n\ni=r+1\n\nsimplify and solve for e[x].\n\n7.34. for another approach to theoretical exercise 33,\nlet tr denote the number of flips required to obtain\na run of r consecutive heads.\n(a) determine e[tr|tr\u22121].\n(b) determine e[tr] in terms of e[tr\u22121].\n(c) what is e[t1]?\n(d) what is e[tr]?\n\n7.35. the probability generating function of the dis-\ncrete nonnegative integer valued random variable\nx having probability mass function pj, j \u00fa 0, is\ndefined by\n\n\u03c6 (s) = e[sx] =\n\npjsj\n\nq(cid:6)\n\nj=0\n\nlet y be a geometric random variable with param-\neter p = 1 \u2212 s, where 0 < s < 1. suppose that y\nis independent of x, and show that\n\u03c6 (s) = p{x < y}\n\n7.36. one ball at a time is randomly selected from an\nurn containing a white and b black balls until all of\nthe remaining balls are of the same color. let ma,b\ndenote the expected number of balls left in the urn\nwhen the experiment ends. compute a recursive\nformula for ma,b and solve when a = 3 and b = 5.\n7.37. an urn contains a white and b black balls. after a\nball is drawn, it is returned to the urn if it is white;\nbut if it is black, it is replaced by a white ball from\nanother urn. let mn denote the expected number\nof white balls in the urn after the foregoing opera-\ntion has been repeated n times.\n(a) derive the recursive equation\n\n(cid:2)\n1 \u2212\n\n(cid:3)\n\n1\n\na + b\n(cid:2)\n\nmn + 1\n(cid:3)n\n\n1\n\na + b\n\n(b) use part (a) to prove that\nmn = a + b \u2212 b\n\n1 \u2212\n\n(c) what is the probability that the (n + 1)st ball\n\ndrawn is white?\n\n7.32. for an event a, let ia equal 1 if a occurs and\nlet it equal 0 if a does not occur. for a random\nvariable x, show that\n\nmn+1 =\n\ne[x|a] = e[xia]\np(a)\n\n7.33. a coin that lands on heads with probability p is\ncontinually flipped. compute the expected num-\nber of flips that are made until a string of r heads\nin a row is obtained.\n\n "}, {"Page_number": 398, "text": "7.38. the best linear predictor of y with respect to x1\nand x2 is equal to a + bx1 + cx2, where a, b, and\nc are chosen to minimize\n\ne[(y \u2212 (a + bx1 + cx2))2]\n\ndetermine a, b, and c.\n7.39. the best quadratic predictor of y with respect to\nx is a + bx + cx2, where a, b, and c are chosen to\nminimize e[(y \u2212 (a + bx + cx2))2]. determine\na, b, and c.\n\n7.40. use the conditional variance formula to determine\nthe variance of a geometric random variable x\nhaving parameter p.\n7.41. let x be a normal random variable with parame-\nters \u03bc = 0 and \u03c3 2 = 1, and let i, independent of x,\n= p{i = 0}. now define\nbe such that p{i = 1} = 1\ny by\n\n2\n\n%\n\ny =\n\nx if i = 1\n\u2212x if i = 0\n\nin words, y is equally likely to equal either x\nor \u2212x.\n(a) are x and y independent?\n(b) are i and y independent?\n(c) show that y is normal with mean 0 and vari-\n(d) show that cov(x, y) = 0.\n\nance 1.\n\n7.42. it follows from proposition 6.1 and the fact that\nthe best linear predictor of y with respect to x is\n\u03bcy + \u03c1\n\n(x \u2212 \u03bcx) that if\n\n\u03c3y\n\u03c3x\n\ne[y|x] = a + bx\n\nthen\n\na = \u03bcy \u2212 \u03c1\n\n\u03c3y\n\u03c3x\n\n\u03bcx\n\nb = \u03c1\n\n\u03c3y\n\u03c3x\n\n(why?) verify this directly.\n\n7.43. show that, for random variables x and z,\ne[(x \u2212 y)2] = e[x2] \u2212 e[y2]\n\nwhere\n\ny = e[x|z]\n\n7.44. consider a population consisting of individuals\nable to produce offspring of the same kind. sup-\npose that, by the end of its lifetime, each individual\nwill have produced j new offspring with probability\npj, j \u00fa 0, independently of the number produced\nby any other individual. the number of individu-\nals initially present, denoted by x0, is called the\nsize of the zeroth generation. all offspring of the\nzeroth generation constitute the first generation,\n\ntheoretical exercises 383\n\nj=0\n\n\u03bc = q(cid:9)\n\njpj and \u03c3 2 = q(cid:9)\n\nand their number is denoted by x1. in general,\nlet xn denote the size of the nth generation. let\n(j \u2212 \u03bc)2pj denote, respec-\ntively, the mean and the variance of the number\nof offspring produced by a single individual. sup-\npose that x0 = 1\u2014that is, initially there is a single\nindividual in the population.\n(a) show that\n\nj=0\n\ne[xn] = \u03bce[xn\u22121]\n\n(b) use part (a) to conclude that\ne[xn] = \u03bcn\n\n(c) show that\n\nvar(xn) = \u03c3 2\u03bcn\u22121 + \u03bc2var(xn\u22121)\n\n(d) use part (c) to conclude that\n\n(cid:2)\n\n\u23a7\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23a9\u03c3 2\u03bcn\u22121\n\nn\u03c3 2\n\n\u03bcn \u2212 1\n\u03bc \u2212 1\n\n(cid:3)\n\nif \u03bc z 1\nif \u03bc = 1\n\nvar(xn) =\n\njust described is known as a\nthe model\nbranching process, and an important ques-\ntion for a population that evolves along such\nlines is the probability that the population will\neventually die out. let \u03c0 denote this proba-\nbility when the population starts with a single\nindividual. that is,\n\u03c0 = p{population eventually dies out|x0 = 1)\n\n(e) argue that \u03c0 satisfies\n\nq(cid:6)\n\nj=0\n\n\u03c0 =\n\npj\u03c0 j\n\nhint: condition on the number of offspring of\nthe initial member of the population.\n\n7.45. verify the formula for the moment generating\nfunction of a uniform random variable that is given\nin table 7.7. also, differentiate to verify the for-\nmulas for the mean and variance.\n7.46. for a standard normal random variable z, let \u03bcn =\n\ne[zn]. show that\n\n\u23a7\u23aa\u23a8\n\u23aa\u23a90\n\n(2j)!\n2jj!\n\n\u03bcn =\n\nwhen n is odd\nwhen n = 2j\n\n "}, {"Page_number": 399, "text": "384\n\nchapter 7\n\nproperties of expectation\n\nhint: start by expanding the moment generating\nfunction of z into a taylor series about 0 to obtain\n\n7.47. let x be a normal random variable with mean\n\u03bc and variance \u03c3 2. use the results of theoretical\nexercise 46 to show that\n\nq(cid:6)\ne[etz] = et2/2\n\n(t2/2)j\n\nj=0\n\nj!\n\n=\n\n(cid:2)\n\n(cid:3)\n\nn\n2j\n\n\u03bcn\u22122j\u03c3 2j(2j)!\n2jj!\n\ne[xn] = [n/2](cid:6)\n\nj=0\n\nin the preceding equation, [n/2] is the largest inte-\nger less than or equal to n/2. check your answer by\nletting n = 1 and n = 2.\n7.48. if y = ax + b, where a and b are constants,\nexpress the moment generating function of y in\nterms of the moment generating function of x.\n\n7.49. the positive random variable x is said to be a log-\nnormal random variable with parameters \u03bc and \u03c3 2\nif log(x) is a normal random variable with mean\n\u03bc and variance \u03c3 2. use the normal moment gener-\nating function to find the mean and variance of a\nlognormal random variable.\ndefine \u0001(t) = log m(t). show that\n\u0001(cid:8)(cid:8)(t)|t=0 = var(x)\n\n7.50. let x have moment generating function m(t), and\n\nn(cid:9)\n\n7.51. use table 7.2 to determine the distribution of\nxi when x1, . . . , xn are independent and\ni=1\nidentically distributed exponential random vari-\nables, each having mean 1/\u03bb.\n\n7.52. show how to compute cov(x, y) from the joint\n\nmoment generating function of x and y.\n\n7.53. suppose that x1, . . . , xn have a multivariate nor-\nmal distribution. show that x1, . . . , xn are inde-\npendent random variables if and only if\ncov(xi, xj) = 0 when i z j\n\n7.54. if z is a standard normal random variable, what is\n\ncov(z, z2)?\n\n7.55. suppose that y is a normal random variable with\nmean \u03bc and variance \u03c3 2, and suppose also that the\nconditional distribution of x, given that y = y, is\nnormal with mean y and variance 1.\n(a) argue that the joint distribution of x, y is the\nsame as that of y + z, y when z is a standard\nnormal random variable that is independent\nof y.\n\n(b) use the result of part (a) to argue that x, y\n\nhas a bivariate normal distribution.\n(c) find e[x], var(x), and corr(x, y).\n(d) find e[y|x = x].\n(e) what is the conditional distribution of y given\n\nthat x = x?\n\nself-test problems and exercises\n\n7.1. consider a list of m names, where the same name\nmay appear more than once on the list. let n(i),\ni = 1, . . . , m, denote the number of times that the\nname in position i appears on the list, and let d\ndenote the number of distinct names on the list.\n(a) express d in terms of the variables m, n(i), i =\n1, . . . , m. let u be a uniform (0, 1) random\nvariable, and let x = [mu] + 1.\n\n(b) what is the probability mass function of x?\n(c) argue that e[m/n(x)] = d.\n\n7.2. an urn has n white and m black balls that are\nremoved one at a time in a randomly chosen order.\nfind the expected number of instances in which a\nwhite ball is immediately followed by a black one.\n7.3. twenty individuals consisting of 10 married cou-\nples are to be seated at 5 different tables, with 4\npeople at each table.\n(a) if the seating is done \u201cat random,\u201d what is the\nexpected number of married couples that are\nseated at the same table?\n\n(b) if 2 men and 2 women are randomly chosen to\nbe seated at each table, what is the expected\n\nnumber of married couples that are seated at\nthe same table?\n\n7.4. if a die is to be rolled until all sides have appeared\nat least once, find the expected number of times\nthat outcome 1 appears.\n\n7.5. a deck of 2n cards consists of n red and n black\ncards. the cards are shuffled and then turned over\none at a time. suppose that each time a red card is\nturned over, we win 1 unit if more red cards than\nblack cards have been turned over by that time.\n(for instance, if n = 2 and the result is r b r b, then\nwe would win a total of 2 units.) find the expected\namount that we win.\n7.6. let a1, a2, . . . , an be events, and let n denote the\nnumber of them that occur. also, let i = 1 if all of\nthese events occur, and let it be 0 otherwise. prove\nbonferroni\u2019s inequality, namely,\n\nn(cid:6)\n\ni=1\n\np(a1 \u00b7\u00b7\u00b7 an) \u00fa\n\np(ai) \u2212 (n \u2212 1)\n\nhint: argue first that n \u2026 n \u2212 1 + i.\n\n "}, {"Page_number": 400, "text": "7.7. let x be the smallest value obtained when k num-\nbers are randomly chosen from the set 1, . . . , n.\nfind e[x] by interpreting x as a negative hyper-\ngeometric random variable.\n\n7.8. an arriving plane carries r families. a total of\nnj of these families have checked in a total of j\nnj = r. suppose that when\npieces of luggage,\njnj pieces of luggage\n\nthe plane lands, the n = (cid:9)\n\nj\n\n(cid:9)\n\nj\n\ncome out of the plane in a random order. as soon\nas a family collects all of its luggage, it immediately\ndeparts the airport. if the sanchez family checked\nin j pieces of luggage, find the expected number of\nfamilies that depart after they do.\n\u22177.9. nineteen items on the rim of a circle of radius 1 are\nto be chosen. show that, for any choice of these\npoints, there will be an arc of (arc) length 1 that\ncontains at least 4 of them.\n\n7.10. let x be a poisson random variable with mean \u03bb.\n\nshow that if \u03bb is not too small, then\n\n\u221a\nvar(\n\u221a\nhint: use the result of theoretical exercise 4 to\napproximate e[\n\nx) l .25\n\nx].\n\n7.11. suppose in self-test problem 3 that the 20 peo-\nple are to be seated at seven tables, three of which\nhave 4 seats and four of which have 2 seats. if\nthe people are randomly seated, find the expected\nvalue of the number of married couples that are\nseated at the same table.\n\n7.12. individuals 1 through n, n > 1, are to be recruited\ninto a firm in the following manner: individual 1\nstarts the firm and recruits individual 2. individ-\nuals 1 and 2 will then compete to recruit indi-\nvidual 3. once individual 3 is recruited, individu-\nals 1, 2, and 3 will compete to recruit individual 4,\nand so on. suppose that when individuals 1, 2, . . . , i\ncompete to recruit individual i + 1, each of them\nis equally likely to be the successful recruiter.\n(a) find the expected number of the individuals\n\n1, . . . , n who did not recruit anyone else.\n\n(b) derive an expression for the variance of the\nnumber of individuals who did not recruit\nanyone else, and evaluate it for n = 5.\n\n7.13. the nine players on a basketball team consist of 2\ncenters, 3 forwards, and 4 backcourt players. if the\nplayers are paired up at random into three groups\nof size 3 each, find (a) the expected value and (b)\nthe variance of the number of triplets consisting of\none of each type of player.\n\n7.14. a deck of 52 cards is shuffled and a bridge hand\nof 13 cards is dealt out. let x and y denote,\nrespectively, the number of aces and the number\nof spades in the hand.\n\nself-test problems and exercises 385\n\n(a) show that x and y are uncorrelated.\n(b) are they independent?\n\n7.15. each coin in a bin has a value attached to it. each\ntime that a coin with value p is flipped, it lands\non heads with probability p. when a coin is ran-\ndomly chosen from the bin, its value is uniformly\ndistributed on (0, 1). suppose that after the coin is\nchosen, but before it is flipped, you must predict\nwhether it will land on heads or on tails. you will\nwin 1 if you are correct and will lose 1 otherwise.\n(a) what is your expected gain if you are not told\n\nthe value of the coin?\n\n(b) suppose now that you are allowed to inspect\nthe coin before it is flipped, with the result of\nyour inspection being that you learn the value\nof the coin. as a function of p, the value of the\ncoin, what prediction should you make?\n\n(c) under the conditions of part (b), what is your\n\nexpected gain?\n\n7.16. in self-test problem 1, we showed how to use\nthe value of a uniform (0, 1) random variable\n(commonly called a random number) to obtain the\nvalue of a random variable whose mean is equal\nto the expected number of distinct names on a list.\nhowever, its use required that one choose a ran-\ndom position and then determine the number of\ntimes that the name in that position appears on\nthe list. another approach, which can be more effi-\ncient when there is a large amount of replication of\nnames, is as follows: as before, start by choosing\nthe random variable x as in problem 1. now iden-\ntify the name in position x, and then go through\nthe list, starting at the beginning, until that name\nappears. let i equal 0 if you encounter that name\nbefore getting to position x, and let i equal 1 if\nyour first encounter with the name is at position x.\nshow that e[mi] = d.\nhint: compute e[i] by using conditional expecta-\ntion.\n\n7.17. a total of m items are to be sequentially dis-\ntributed among n cells, with each item indepen-\ndently being put in cell j with probability pj, j =\n1, . . . , n. find the expected number of collisions\nthat occur, where a collision occurs whenever an\nitem is put into a nonempty cell.\n\n7.18. let x be the length of the initial run in a random\nordering of n ones and m zeroes. that is, if the first\nk values are the same (either all ones or all zeroes),\nthen x \u00fa k. find e[x].\n\n7.19. there are n items in a box labeled h and m in a\nbox labeled t. a coin that comes up heads with\nprobability p and tails with probability 1 \u2212 p is\nflipped. each time it comes up heads, an item is\nremoved from the h box, and each time it comes\nup tails, an item is removed from the t box. (if a\nbox is empty and its outcome occurs, then no items\n\n "}, {"Page_number": 401, "text": "386\n\nchapter 7\n\nproperties of expectation\n\nare removed.) find the expected number of coin\nflips needed for both boxes to become empty.\nhint: condition on the number of heads in the first\nn + m flips.\n7.20. let x be a nonnegative random variable having\ndistribution function f. show that if f(x) = 1 \u2212\nf(x), then\n\nhint: start with the identity\n\n* q\n\n0\n\nxn\u22121f(x) dx\n\ne[xn] =\n*\n* q\nxn\u22121 dx\nxn\u22121ix (x) dx\n%\n\nxn = n\n= n\n\n0\n\n0\n\nx\n\n1,\nif x < x\n0, otherwise\n\nix(x) =\n(cid:9)\n\nwhere\n\n(cid:9)\n\nn\n\n< 0.\n\nn\nj=1 aijaij+1\n\n\u22177.21. let a1, . . . , an, not all equal to 0, be such that\ni=1 ai = 0. show that there is a permutation\ni1, . . . , in such that\nhint: use the probabilistic method. (it is interest-\ning that there need not be a permutation whose\nsum of products of successive pairs is positive. for\ninstance, if n = 3, a1 = a2 = \u22121, and a3 = 2, there\nis no such permutation.)\n7.22. suppose that xi, i = 1, 2, 3, are independent pois-\nson random variables with respective means \u03bbi,\ni = 1, 2, 3. let x = x1 + x2 and y = x2 + x3.\nthe random vector x, y is said to have a bivariate\npoisson distribution.\n(a) find e[x] and e[y].\n(b) find cov(x, y).\n(c) find the joint probability mass function\np{x = i, y = j}.\n7.23. let (xi, yi), i = 1, . . . , be a sequence of indepen-\ndent and identically distributed random vectors.\nthat is, x1, y1 is independent of, and has the same\ndistribution as x2, y2, and so on. although xi and\nyi can be dependent, xi and yj are independent\nwhen i z j. let\n\n\u03bcx = e[xi], \u03bcy = e[yi],\n\u03c3 2\ny\n\n(cid:9)\n\n= var(yi),\nn\ni=1 xi,\n\n\u03c3 2\nx\n\u03c1 = corr(xi, yi)\n(cid:9)\nn\nj=1 yj).\n\nfind corr(\n\n= var(xi),\n\n7.24. three cards are randomly chosen without replace-\nment from an ordinary deck of 52 cards. let x\ndenote the number of aces chosen.\n(a) find e[x|the ace of spades is chosen].\n(b) find e[x|at least one ace is chosen].\n\n7.25. let \u0001 be the standard normal distribution func-\ntion, and let x be a normal random variable with\nmean \u03bc and variance 1. we want to find e[\u0001(x)].\nto do so, let z be a standard normal random vari-\nable that is independent of x, and let\n\n%\n\ni =\n\n1,\n0,\n\nif z < x\nif z \u00fa x\n(a) show that e[i|x = x] = \u0001(x).\n(b) show that e[\u0001(x)] = p{z < x}.\n(c) show that e[\u0001(x)] = \u0001( \u03bc\u221a\n\n).\n\nhint: what is the distribution of x \u2212 z?\n\n2\n\nthe preceding comes up in statistics. suppose\nyou are about to observe the value of a random\nvariable x that is normally distributed with an\nunknown mean \u03bc and variance 1, and suppose that\nyou want to test the hypothesis that the mean \u03bc\nis greater than or equal to 0. clearly you would\nwant to reject this hypothesis if x is sufficiently\nsmall. if it results that x = x, then the p-value\nof the hypothesis that the mean is greater than\nor equal to 0 is defined to be the probability that\nx would be as small as x if \u03bc were equal to 0\n(its smallest possible value if the hypothesis were\ntrue). (a small p-value is taken as an indication\nthat the hypothesis is probably false.) because x\nhas a standard normal distribution when \u03bc = 0,\nthe p-value that results when x = x is \u0001(x).\ntherefore, the preceding shows that the expected\np-value that results when the true mean is \u03bc\nis \u0001( \u03bc\u221a\n2\n\n).\n\n7.26. a coin that comes up heads with probability p\nis flipped until either a total of n heads or of\nm tails is amassed. find the expected number of\nflips.\nhint: imagine that one continues to flip even after\nthe goal is attained. let x denote the number of\nflips needed to obtain n heads, and let y denote\nthe number of flips needed to obtain m tails. note\nthat max(x, y) + min(x, y) = x + y. com-\npute e[max(x, y)] by conditioning on the number\nof heads in the first n + m \u2212 1 flips.\n\n7.27. a deck of n cards numbered 1 through n, initially\nin any arbitrary order, is shuffled in the following\nmanner: at each stage, we randomly choose one\nof the cards and move it to the front of the deck,\nleaving the relative positions of the other cards\nunchanged. this procedure is continued until all\nbut one of the cards has been chosen. at this point\nit follows by symmetry that all n! possible order-\nings are equally likely. find the expected number\nof stages that are required.\n\n7.28. suppose that a sequence of independent trials in\nwhich each trial is a success with probability p is\n\n "}, {"Page_number": 402, "text": "performed until either a success occurs or a total of\nn trials has been reached. find the mean number\nof trials that are performed.\nhint: the computations are simplified if you use\nthe identity that, for a nonnegative integer valued\nrandom variable x,\n\nq(cid:6)\n\ni=1\n\ne[x] =\n\np{x \u00fa i}\n\nself-test problems and exercises 387\n\n7.29. suppose that x and y are both bernoulli random\nvariables. show that x and y are independent if\nand only if cov(x, y) = 0.\n\n(cid:9)\n\n7.30. in the generalized match problem, there are n\ni=1 ni =\nindividuals of whom ni wear hat size i,\nn. there are also n hats, of which hi are of\ni=1 hi = n. if each individual randomly\nsize i,\nchooses a hat (without replacement), find the\nexpected number who choose a hat that is their\nsize.\n\nr\n\nr\n\n(cid:9)\n\n "}, {"Page_number": 403, "text": "c h a p t e r\n\n8\n\nlimit theorems\n\n8.1 introduction\n8.2 chebyshev\u2019s inequality and the weak law of large numbers\n8.3 the central limit theorem\n8.4 the strong law of large numbers\n8.5 other inequalities\n8.6 bounding the error probability when approximating a sum of independent bernoulli\n\nrandom variables by a poisson random variable\n\n8.1 introduction\n\nthe most important theoretical results in probability theory are limit theorems. of\nthese, the most important are those classified either under the heading laws of large\nnumbers or under the heading central limit theorems. usually, theorems are consid-\nered to be laws of large numbers if they are concerned with stating conditions under\nwhich the average of a sequence of random variables converges (in some sense) to\nthe expected average. by contrast, central limit theorems are concerned with deter-\nmining conditions under which the sum of a large number of random variables has a\nprobability distribution that is approximately normal.\n\n8.2 chebyshev\u2019s inequality and the weak law of large numbers\n\nwe start this section by proving a result known as markov\u2019s inequality.\nproposition 2.1. markov\u2019s inequality\nif x is a random variable that takes only nonnegative values, then, for any value\na > 0,\n\nproof. for a > 0, let\n\ni =\n\nand note that, since x \u00fa 0,\n\np{x \u00fa a} \u2026 e[x]\na\n\n%\n\nif x \u00fa a\n1\n0 otherwise\n\ni \u2026 x\na\n\ntaking expectations of the preceding inequality yields\n\ne[i] \u2026 e[x]\na\n\nwhich, because e[i] = p{x \u00fa a}, proves the result.\nas a corollary, we obtain proposition 2.2.\n\n388\n\n "}, {"Page_number": 404, "text": "section 8.2\n\nchebyshev\u2019s inequality and the weak law of large numbers 389\n\nproposition 2.2. chebyshev\u2019s inequality\nif x is a random variable with finite mean \u03bc and variance \u03c3 2, then, for any value\nk > 0,\n\np{|x \u2212 \u03bc| \u00fa k} \u2026 \u03c3 2\nk2\n\nproof. since (x \u2212 \u03bc)2 is a nonnegative random variable, we can apply markov\u2019s\ninequality (with a = k2) to obtain\n\np{(x \u2212 \u03bc)2 \u00fa k2} \u2026 e[(x \u2212 \u03bc)2]\n\n(2.1)\nbut since (x \u2212 \u03bc)2 \u00fa k2 if and only if |x \u2212 \u03bc| \u00fa k, equation (2.1) is equivalent to\n\nk2\n\np{|x \u2212 \u03bc| \u00fa k} \u2026 e[(x \u2212 \u03bc)2]\n\nk2\n\n= \u03c3 2\nk2\n\nand the proof is complete.\n\nthe importance of markov\u2019s and chebyshev\u2019s inequalities is that they enable us to\nderive bounds on probabilities when only the mean, or both the mean and the vari-\nance, of the probability distribution are known. of course, if the actual distribution\nwere known, then the desired probabilities could be computed exactly and we would\nnot need to resort to bounds.\n\nexample 2a\nsuppose that it is known that the number of items produced in a factory during a\nweek is a random variable with mean 50.\n(a) what can be said about the probability that this week\u2019s production will\n\nexceed 75?\n\n(b) if the variance of a week\u2019s production is known to equal 25, then what can\nbe said about the probability that this week\u2019s production will be between 40\nand 60?\n\nsolution. let x be the number of items that will be produced in a week.\n(a) by markov\u2019s inequality,\n\np{x > 75} \u2026 e[x]\n75\n\n= 50\n75\n\n= 2\n3\n\n(b) by chebyshev\u2019s inequality,\n\nhence,\n\np{|x \u2212 50| \u00fa 10} \u2026 \u03c3 2\n102\n\n= 1\n4\n\np{|x \u2212 50| < 10} \u00fa 1 \u2212 1\n4\n\n= 3\n4\n\nso the probability that this week\u2019s production will be between 40 and 60 is at\n.\nleast .75.\n\n "}, {"Page_number": 405, "text": "390\n\nchapter 8\n\nlimit theorems\n\nas chebyshev\u2019s inequality is valid for all distributions of the random variable x,\nwe cannot expect the bound on the probability to be very close to the actual proba-\nbility in most cases. for instance, consider example 2b.\n\nexample 2b\nif x is uniformly distributed over the interval (0, 10), then, since e[x] = 5 and\nvar(x) = 25\n\n3 , it follows from chebyshev\u2019s inequality that\n\np{|x \u2212 5| > 4} \u2026 25\n3(16)\n\nl .52\n\nwhereas the exact result is\n\np{|x \u2212 5| > 4} = .20\n\nthus, although chebyshev\u2019s inequality is correct, the upper bound that it provides is\nnot particularly close to the actual probability.\n\nsimilarly, if x is a normal random variable with mean \u03bc and variance \u03c3 2,\n\nchebyshev\u2019s inequality states that\n\nwhereas the actual probability is given by\n\np{|x \u2212 \u03bc| > 2\u03c3} = p\n\n= 2[1 \u2212 \u0001(2)] l .0456\n\n.\n\np{|x \u2212 \u03bc| > 2\u03c3} \u2026 1\n4\n7\n\n06666x \u2212 \u03bc\n\n6666 > 2\n\n\u03c3\n\nchebyshev\u2019s inequality is often used as a theoretical tool in proving results. this\nuse is illustrated first by proposition 2.3 and then, most importantly, by the weak law\nof large numbers.\nproposition 2.3. if var(x) = 0, then\n\np{x = e[x]} = 1\n\nin other words, the only random variables having variances equal to 0 are those which\nare constant with probability 1.\n\n%\n|x \u2212 \u03bc| >\n\np\n\n/\n\n1\nn\n\nproof. by chebyshev\u2019s inequality, we have, for any n \u00fa 1,\n\n/7\nletting n\u2192q and using the continuity property of probability yields\n\n0\n\n/\n\n%\n|x \u2212 \u03bc| >\n\n0 = lim\n\nn\u2192q p\n\n= 0\n%\n|x \u2212 \u03bc| >\n\n1\nn\n\n1\nn\n\n= p\nn\u2192q\nlim\n= p{x z \u03bc}\n\nand the result is established.\n\n "}, {"Page_number": 406, "text": "section 8.3\n\nthe central limit theorem 391\n\ntheorem 2.1 the weak law of large numbers\nlet x1, x2, . . . be a sequence of independent and identically distributed random vari-\nables, each having finite mean e[xi] = \u03bc. then, for any \u03b5 > 0,\n\nproof. we shall prove the theorem only under the additional assumption that the\nrandom variables have a finite variance \u03c3 2. now, since\n\n7\n\n6666 \u00fa \u03b5\n\n\u2212 \u03bc\n\np\n\n06666x1 + \u00b7\u00b7\u00b7 + xn\n(cid:21)\n\nn\n\nx1 + \u00b7\u00b7\u00b7 + xn\n\n(cid:20)\n\ne\n\n= \u03bc and var\n\nn\n\n06666x1 + \u00b7\u00b7\u00b7 + xn\n\np\n\nn\n\n\u2212 \u03bc\n\n\u21920\n\nas n\u2192q\n\n(cid:2)\n6666 \u00fa \u03b5\n\nx1 + \u00b7\u00b7\u00b7 + xn\n7\n\nn\n\n\u2026 \u03c3 2\nn\u03b52\n\n(cid:3)\n\n= \u03c3 2\nn\n\nit follows from chebyshev\u2019s inequality that\n\nand the result is proven.\n\nthe weak law of large numbers was originally proven by james bernoulli for the\nspecial case where the xi are 0, 1 (that is, bernoulli) random variables. his statement\nand proof of this theorem were presented in his book ars conjectandi, which was\npublished in 1713, eight years after his death, by his nephew nicholas bernoulli. note\nthat, because chebyshev\u2019s inequality was not known in bernoulli\u2019s time, bernoulli\nhad to resort to a quite ingenious proof to establish the result. the general form of\nthe weak law of large numbers presented in theorem 2.1 was proved by the russian\nmathematician khintchine.\n\n8.3 the central limit theorem\n\nthe central limit theorem is one of the most remarkable results in probability theory.\nloosely put, it states that the sum of a large number of independent random variables\nhas a distribution that is approximately normal. hence, it not only provides a simple\nmethod for computing approximate probabilities for sums of independent random\nvariables, but also helps explain the remarkable fact that the empirical frequencies of\nso many natural populations exhibit bell-shaped (that is, normal) curves.\n\nin its simplest form the central limit theorem is as follows.\ntheorem 3.1 the central limit theorem\nlet x1, x2, . . . be a sequence of independent and identically distributed random vari-\nables, each having mean \u03bc and variance \u03c3 2. then the distribution of\n\ntends to the standard normal as n\u2192q. that is, for \u2212q < a < q,\n\n0\n\np\n\nx1 + \u00b7\u00b7\u00b7 + xn \u2212 n\u03bc\n\n\u221a\nn\n\n\u03c3\n\nx1 + \u00b7\u00b7\u00b7 + xn \u2212 n\u03bc\n*\n\n\u221a\nn\n7\n\n\u03c3\n\n\u2026 a\n\n\u2192 1\u221a\n2\u03c0\n\na\n\u2212q\n\n\u2212x2/2 dx\ne\n\nas n\u2192q\n\nthe key to the proof of the central limit theorem is the following lemma, which we\n\nstate without proof.\n\n "}, {"Page_number": 407, "text": "392\n\nchapter 8\n\nlimit theorems\n\nlemma 3.1\nlet z1, z2, . . . be a sequence of random variables having distribution functions fzn\nand moment generating functions mzn, n \u00fa 1; and let z be a random variable\n(t) \u2192\nhaving distribution function fz and moment generating function mz. if mzn\nmz(t) for all t, then fzn\nif we let z be a standard normal random variable, then, since mz(t) = et2/2, it\n(t) \u2192 \u0001(t) as n\u2192 q.\n\n(t) \u2192 fz(t) for all t at which fz(t) is continuous.\n\n(t) \u2192 et2/2 as n \u2192 q, then fzn\n\nfollows from lemma 3.1 that if mzn\n\nwe are now ready to prove the central limit theorem.\n\nproof of the central limit theorem: let us assume at first that \u03bc = 0 and \u03c3 2 = 1.\nwe shall prove the theorem under the assumption that the moment generating func-\n\u221a\ntion of the xi, m(t), exists and is finite. now, the moment generating function of\nxi/\n\nn is given by\n\n(cid:4)\n\n(cid:5)\n\n0\n\n\u23a1\n\u23a3exp\n\ne\n\n7\u23a4\n\u23a6 = m\nn(cid:9)\n\ntxi\u221a\nn\n\nt\u221a\nn\n\n\u221a\nn is given by\nxi/\nl(t) = log m(t)\n\ni=1\n\n(cid:30)(cid:21)n\n\n(cid:20)\n\n(cid:29)\n\nm\n\nt\u221a\n\nn\n\n. let\n\nthus, the moment generating function of\n\nand note that\n\n(cid:8)(0)\nm(0)\n\nl(0) = 0\n(cid:8)(0) = m\nl\n= \u03bc\n= 0\n\n(cid:8)(cid:8)(0) \u2212 [m\n[m(0)]2\n\n(cid:8)(0)]2\n\n(cid:8)(cid:8)(0) = m(0)m\nl\n= e[x2]\n= 1\n\n\u221a\n\u221a\nnow, to prove the theorem, we must show that [m(t/\nequivalently, that nl(t/\n\nn)]n \u2192 et2/2 as n\u2192 q, or,\n\n\u221a\nl(t/\nn\u22121\n\nn)\n\nn\u2192q\nlim\n\n= lim\nn\u2192q\n= lim\nn\u2192q\n= lim\nn\u2192q\n\nby l\u2019h \u02c6opital\u2019s rule\n\nn) \u2192 t2/2 as n \u2192 q. to show this, note that\n\u221a\n\u2212l\n(cid:8)(t/\n\u22123/2t\n(cid:7)\n(cid:8)\nn)n\n\u22122n\u22122\n\u221a\n(cid:8)(t/\nn)t\nl\n(cid:7)\n2n\u22121/2\n\u221a\n\u2212l\n\u22123/2t2\n(cid:8)(cid:8)(t/\nn)n\n\u23a1\n\u23a4\n(cid:4)\n(cid:5)\n\u22122n\u22123/2\n\u23a3l\n\u23a6\nt2\nt\u221a\n2\nn\n\n(cid:8)\n\n(cid:8)(cid:8)\n\n= lim\nn\u2192q\n\nagain by l\u2019h \u02c6opital\u2019s rule\n\n= t2\n2\n\n "}, {"Page_number": 408, "text": "section 8.3\n\nthe central limit theorem 393\nthus, the central limit theorem is proven when \u03bc = 0 and \u03c3 2 = 1. the result\nnow follows in the general case by considering the standardized random variables\n) = 1.\n\u2217\nx\ni\n\n= (xi \u2212 \u03bc)/\u03c3 and applying the preceding result, since e[x\nremark. although theorem 3.1 states only that, for each a,\n\ni ] = 0, var(x\n\u2217\n\n\u2217\ni\n\n0\n\n7\n\nx1 + \u00b7\u00b7\u00b7 + xn \u2212 n\u03bc\n\n\u221a\nn\n\n\u03c3\n\np\n\n\u2026 a\n\n\u2192\u0001(a)\n\nit can, in fact, be shown that the convergence is uniform in a. [we say that fn(a) \u2192 f (a)\nuniformly in a if, for each \u03b5 > 0, there exists an n such that |fn(a) \u2212 f (a)| < \u03b5 for all\na whenever n \u00fa n.]\n.\n\nthe first version of the central limit theorem was proven by demoivre around\n1733 for the special case where the xi are bernoulli random variables with p = 1\n2.\nthe theorem was subsequently extended by laplace to the case of arbitrary p. (since\na binomial random variable may be regarded as the sum of n independent and identi-\ncally distributed bernoulli random variables, this justifies the normal approximation\nto the binomial that was presented in section 5.4.1.) laplace also discovered the more\ngeneral form of the central limit theorem given in theorem 3.1. his proof, however,\nwas not completely rigorous and, in fact, cannot easily be made rigorous. a truly\nrigorous proof of the central limit theorem was first presented by the russian mathe-\nmatician liapounoff in the period 1901\u20131902.\n\nthis important theorem is illustrated by the central limit theorem module on the\ntext website. this website yields plots of the density function of the sum of n inde-\npendent and identically distributed random variables that each take on one of the\nvalues 0, 1, 2, 3, 4. when using it, one enters the probability mass function and the\ndesired value of n. figure 8.1 shows the resulting plots for a specified probability mass\nfunction when (a) n = 5, (b) n = 10, (c) n = 25, and (d) n = 100.\n\nexample 3a\nan astronomer is interested in measuring the distance, in light-years, from his obser-\nvatory to a distant star. although the astronomer has a measuring technique, he\nknows that, because of changing atmospheric conditions and normal error, each time\na measurement is made it will not yield the exact distance, but merely an estimate.\nas a result, the astronomer plans to make a series of measurements and then use the\naverage value of these measurements as his estimated value of the actual distance.\nif the astronomer believes that the values of the measurements are independent and\nidentically distributed random variables having a common mean d (the actual dis-\ntance) and a common variance of 4 (light-years), how many measurements need he\nmake to be reasonably sure that his estimated distance is accurate to within ;.5 light-\nyear?\n\nsolution. suppose that the astronomer decides to make n observations. if x1,\nx2, . . . , xn are the n measurements, then, from the central\nit\nfollows that\n\nlimit theorem,\n\nn(cid:6)\n\nzn =\n\ni=1\n\nxi \u2212 nd\n\u221a\n2\n\nn\n\n "}, {"Page_number": 409, "text": "394\n\nchapter 8\n\nlimit theorems\n\ncentral limit theorem\n\nenter the probabilities and the number of random\nvariables to be summed. the output gives the mass\nfunction of the sum along with its mean and\nvariance.\n\nstart\n\nquit\n\np0\n\np1\n\np2\n\np3\n\np4\n\n.25\n\n.15\n\n.1\n\n.2\n\n.3\n\nn =\n\n5\n\nmean = 10.75\n\nvariance = 12.6375\n\np(i)\n\n0.15\n\n0.10\n\n0.05\n\n0.00\n\n0\n\n5\n\n10\n\ni\n\n15\n\n20\n\nfigure 8.1(a)\n\nhas approximately a standard normal distribution. hence,\n\nn(cid:6)\n\n\u23a7\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23a9 \u2212 .5 \u2026\n\np\n\nxi\n\ni=1\nn\n\n\u2212 d \u2026 .5\n\n\u23ab\u23aa\u23aa\u23ac\n\u23aa\u23aa\u23ad = p\n\nl \u0001\n\n0\n\u2212.5\n(cid:4)\u221a\n\n7\n\n\u221a\nn\n(cid:5)\n2\n\n\u221a\nn\n(cid:4)\n(cid:5)\n\u2026 zn \u2026 .5\n2\n\u221a\n\u2212\n= 2\u0001\nn\n4\n\n\u2212 \u03c6\n\nn\n4\n\n(cid:5)\n\n(cid:4)\u221a\n\nn\n4\n\n\u2212 1\n\ntherefore, if the astronomer wants, for instance, to be 95 percent certain that his\n\u2217\nmeasurements,\nestimated value is accurate to within .5 light year, he should make n\n\u2217\nwhere n\n\nis such that\n\n\u2212 1 = .95\n\nor\n\n\u0001\n\n= .975\n\n(cid:5)\n\n(cid:4)\u221a\nn\u2217\n4\n\nthus, from table 5.1 of chapter 5,\n\n\u2217\n\nas n\n\nis not integral valued, he should make 62 observations.\n\n= 1.96\n\nor\n\n\u2217 = (7.84)2 l 61.47\nn\n\n(cid:5)\n\n(cid:4)\u221a\nn\u2217\n4\n\n2\u0001\n\n\u221a\nn\u2217\n4\n\n "}, {"Page_number": 410, "text": "section 8.3\n\nthe central limit theorem 395\n\ncentral limit theorem\n\nenter the probabilities and the number of random\nvariables to be summed. the output gives the mass\nfunction of the sum along with its mean and\nvariance.\n\nstart\n\nquit\n\np0\n\np1\n\np2\n\np3\n\np4\n\n.25\n\n.15\n\n.1\n\n.2\n\n.3\n\nn =\n\n10\n\nmean = 21.5\n\nvariance = 25.275\n\n0.08\n\n0.06\n\np(i)\n\n0.04\n\n0.02\n\n0.00\n\n0\n\n10\n\n20\n\ni\n\n30\n\n40\n\nfigure 8.1(b)\n\nnote, however, that the preceding analysis has been done under the assumption\nthat the normal approximation will be a good approximation when n = 62. although\nthis will usually be the case, in general the question of how large n need be before\nthe approximation is \u201cgood\u201d depends on the distribution of the xi. if the astronomer\nis concerned about this point and wants to take no chances, he can still solve his\nproblem by using chebyshev\u2019s inequality. since\n\nchebyshev\u2019s inequality yields\n\ni=1\n\n\u23a1\n\u23a3 n(cid:6)\n\u23a7\u23aa\u23a8\n666666 n(cid:6)\n\u23aa\u23a9\n\ni=1\n\ne\n\np\n\n\u239b\n\u239d n(cid:6)\n\ni=1\n\nvar\n\n\u23a4\n\u23a6 = d\n\u23ab\u23aa\u23ac\n666666 > .5\n\u23aa\u23ad \u2026\n\n\u2212 d\n\nxi\nn\n\nxi\nn\n\n\u239e\n\u23a0 = 4\n\nn\n\nxi\nn\n\n4\n\nn(.5)2\n\n= 16\nn\n\nhence, if he makes n = 16/.05 = 320 observations, he can be 95 percent certain that\n.\nhis estimate will be accurate to within .5 light-year.\n\n "}, {"Page_number": 411, "text": "396\n\nchapter 8\n\nlimit theorems\n\ncentral limit theorem\n\nenter the probabilities and the number of random\nvariables to be summed. the output gives the mass\nfunction of the sum along with its mean and\nvariance.\n\nstart\n\nquit\n\np0\n\np1\n\np2\n\np3\n\np4\n\n.25\n\n.15\n\n.1\n\n.2\n\n.3\n\nn =\n\n25\n\nmean = 53.75\n\nvariance = 63.1875\n\np(i)\n\n0.05\n\n0.04\n\n0.03\n\n0.02\n\n0.01\n\n0.00\n\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n\ni\n\nfigure 8.1(c)\n\nexample 3b\nthe number of students who enroll in a psychology course is a poisson random vari-\nable with mean 100. the professor in charge of the course has decided that if the\nnumber enrolling is 120 or more, he will teach the course in two separate sections,\nwhereas if fewer than 120 students enroll, he will teach all of the students together\nin a single section. what is the probability that the professor will have to teach two\nsections?\n\nsolution. the exact solution\n\nq(cid:6)\n\n(100)i\n\ni=120\n\ni!\n\n\u2212100\ne\n\ndoes not readily yield a numerical answer. however, by recalling that a poisson ran-\ndom variable with mean 100 is the sum of 100 independent poisson random variables,\neach with mean 1, we can make use of the central limit theorem to obtain an approx-\nimate solution. if x denotes the number of students that enroll in the course, we\nhave\n\np{x \u00fa 120} = p{x \u00fa 119.5}\n\n(the continuity correction)\n\n "}, {"Page_number": 412, "text": "section 8.3\n\nthe central limit theorem 397\n\ncentral limit theorem\n\nenter the probabilities and the number of random\nvariables to be summed. the output gives the mass\nfunction of the sum along with its mean and\nvariance.\n\nstart\n\nquit\n\np0\n\np1\n\np2\n\np3\n\np4\n\n.25\n\n.15\n\n.1\n\n.2\n\n.3\n\nn =\n\n100\n\nmean = 215.\n\nvariance = 252.75\n\np(i)\n\n0.030\n0.025\n0.020\n0.015\n0.010\n0.005\n0.000\n\n0\n\n100\n\n200\n\ni\n\n300\n\n400\n\nfigure 8.1(d)\n\n7\n\n\u00fa 119.5 \u2212 100\n\n\u221a\n100\n\n0\n\nx \u2212 100\n= p\n\u221a\n100\nl 1 \u2212 \u0001(1.95)\nl .0256\n\nwhere we have used the fact that the variance of a poisson random variable is equal\n.\nto its mean.\n\nexample 3c\nif 10 fair dice are rolled, find the approximate probability that the sum obtained is\nbetween 30 and 40, inclusive.\nsolution. let xi denote the value of the ith die, i = 1, 2, . . . , 10. since\n\ne(xi) = 7\n2\n\n, var(xi) = e[x2\n\ni ] \u2212 (e[xi])2 = 35\n12\n\n,\n\n "}, {"Page_number": 413, "text": "398\n\nchapter 8\n\nlimit theorems\n\nthe central limit theorem yields\n\np{29.5 \u2026 x \u2026 40.5} = p\n\n\u23a7\u23aa\u23aa\u23aa\u23a8\n@\n\u23aa\u23aa\u23aa\u23a929.5 \u2212 35\n\n350\n12\n\nl 2\u0001(1.0184) \u2212 1\nl .692\n\n@\n\n\u2026 x \u2212 35\n\n350\n12\n\n@\n\n\u2026 40.5 \u2212 35\n\n350\n12\n\n\u23ab\u23aa\u23aa\u23aa\u23ac\n\u23aa\u23aa\u23aa\u23ad\n\n.\n\n)\n\n1\n12\n\n10(\n\u221a\n1.2)\n\nl 1 \u2212 \u0001(\nl .1367\n\n7\n\n0\n\n10(cid:9)\n\nexample 3d\nlet xi, i = 1, . . . , 10, be independent random variables, each uniformly distributed\nover (0, 1). calculate an approximation to p\n2 and var(xi) = 1\nsolution. since e[xi] = 1\n10(cid:6)\n@\n\n12, we have, by the central limit theorem,\nxi \u2212 5\n\nxi > 6\n\ni=1\n\n>\n\n1\n\n.\n\np\n\nxi > 6\n\n\u23ab\u23ac\n\u23ad = p\n\n\u23a7\u23a8\n\u23a9 10(cid:6)\n\n1\n\n\u23ab\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23ac\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23ad\n\n@\n6 \u2212 5\n1\n12\n\n10(\n\n)\n\n\u23a7\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9\n\n10(cid:9)\n\ni=1\n\nhence,\n\nxi will be greater than 6 only 14 percent of the time.\n\n.\n\nexample 3e\nan instructor has 50 exams that will be graded in sequence. the times required to\ngrade the 50 exams are independent, with a common distribution that has mean\n20 minutes and standard deviation 4 minutes. approximate the probability that the\ninstructor will grade at least 25 of the exams in the first 450 minutes of work.\n\nsolution.\n\nif we let xi be the time that it takes to grade exam i, then\n\nx = 25(cid:6)\n\ni=1\n\nxi\n\nis the time it takes to grade the first 25 exams. because the instructor will grade at\nleast 25 exams in the first 450 minutes of work if the time it takes to grade the first 25\nexams is less than or equal to 450, we see that the desired probability is p{x \u2026 450}.\nto approximate this probability, we use the central limit theorem. now,\n\nand\n\ne[x] = 25(cid:6)\nvar(x) = 25(cid:6)\n\ni=1\n\ni=1\n\ne[xi] = 25(20) = 500\n\nvar(xi) = 25(16) = 400\n\n "}, {"Page_number": 414, "text": "section 8.3\n\nthe central limit theorem 399\n\nconsequently, with z being a standard normal random variable, we have\n\n\u221a\n\np{x \u2026 450} = p{x \u2212 500\n400\nl p{z \u2026 \u22122.5}\n= p{z \u00fa 2.5}\n= 1 \u2212 \u0001(2.5) = .006\n\n\u2026 450 \u2212 500\n\n\u221a\n400\n\n}\n\n.\n\ncentral limit theorems also exist when the xi are independent, but not necessarily\nidentically distributed random variables. one version, by no means the most general,\nis as follows.\n\ntheorem 3.2 central limit theorem for independent random variables\nlet x1, x2, . . . be a sequence of independent random variables having respective\nmeans and variances \u03bci = e[xi], \u03c3 2\n= var(xi). if (a) the xi are uniformly\n=q\u2014then\nbounded\u2014that is, if for some m, p{|xi| < m}=1 for all i, and (b)\n\nq(cid:9)\n\ni\n\n\u03c3 2\ni\n\ni=1\n\n\u23a7\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9\n\nn(cid:6)\n(xi \u2212 \u03bci)\nabbc n(cid:6)\n\ni=1\n\n\u03c3 2\ni\n\ni=1\n\n\u23ab\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23ac\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23ad\n\n\u2026 a\n\n\u2192\u0001(a) as n\u2192 q\n\np\n\nhistorical note\n\npierre-simon, marquis de laplace\nthe central limit theorem was originally stated and proven by the french math-\nematician pierre-simon, marquis de laplace, who came to the theorem from his\nobservations that errors of measurement (which can usually be regarded as being\nthe sum of a large number of tiny forces) tend to be normally distributed. laplace,\nwho was also a famous astronomer (and indeed was called \u201cthe newton of france\u201d),\nwas one of the great early contributors to both probability and statistics. laplace\nwas also a popularizer of the uses of probability in everyday life. he strongly believed\nin its importance, as is indicated by the following quotations of his taken from his\npublished book analytical theory of probability: \u201cwe see that the theory of proba-\nbility is at bottom only common sense reduced to calculation; it makes us appreciate\nwith exactitude what reasonable minds feel by a sort of instinct, often without being\nable to account for it.. . . it is remarkable that this science, which originated in the\nconsideration of games of chance, should become the most important object of\nhuman knowledge.. . . the most important questions of life are, for the most part,\nreally only problems of probability.\u201d\n\nthe application of the central limit theorem to show that measurement errors\nare approximately normally distributed is regarded as an important contribution to\nscience. indeed, in the 17th and 18th centuries the central limit theorem was often\ncalled the law of frequency of errors. listen to the words of francis galton (taken\nfrom his book natural inheritance, published in 1889): \u201ci know of scarcely anything\nso apt to impress the imagination as the wonderful form of cosmic order expressed\nby the \u2018law of frequency of error.\u2019 the law would have been personified by the\ngreeks and deified, if they had known of it. it reigns with serenity and in complete\nself-effacement amidst the wildest confusion. the huger the mob and the greater the\napparent anarchy, the more perfect is its sway. it is the supreme law of unreason.\u201d\n\n "}, {"Page_number": 415, "text": "400\n\nchapter 8\n\nlimit theorems\n\n8.4 the strong law of large numbers\n\nthe strong law of large numbers is probably the best-known result in probability the-\nory. it states that the average of a sequence of independent random variables having\na common distribution will, with probability 1, converge to the mean of that distribu-\ntion.\n\ntheorem 4.1 the strong law of large numbers\nlet x1, x2, . . . be a sequence of independent and identically distributed random vari-\nables, each having a finite mean \u03bc = e[xi]. then, with probability 1,\n\nx1 + x2 + \u00b7\u00b7\u00b7 + xn\n\nn\n\n\u2192\u03bc\n\nas\n\nn\u2192 q\u2020\n\nas an application of the strong law of large numbers, suppose that a sequence of\nindependent trials of some experiment is performed. let e be a fixed event of the\nexperiment, and denote by p(e) the probability that e occurs on any particular trial.\nletting\n\n%\n\nxi =\n\n1\n0\n\nif e occurs on the ith trial\nif e does not occur on the ith trial\n\nwe have, by the strong law of large numbers, that with probability 1,\n\nx1 + \u00b7\u00b7\u00b7 + xn\n\n\u2192 e[x] = p(e)\n\nn\n\n(4.1)\nsince x1 + \u00b7\u00b7\u00b7 + xn represents the number of times that the event e occurs in the\nfirst n trials, we may interpret equation (4.1) as stating that, with probability 1, the\nlimiting proportion of time that the event e occurs is just p(e).\n\nalthough the theorem can be proven without this assumption, our proof of the\nstrong law of large numbers will assume that the random variables xi have a finite\nfourth moment. that is, we will suppose that e[x4\n\ni ] = k < q.\n\nproof of the strong law of large numbers: to begin, assume that \u03bc, the mean of\n\nthe xi, is equal to 0. let sn = n(cid:9)\n\nxi and consider\n\ni=1\n\ne[s4\n\nn] = e[(x1 + \u00b7\u00b7\u00b7 + xn)(x1 + \u00b7\u00b7\u00b7 + xn)\n* (x1 + \u00b7\u00b7\u00b7 + xn)(x1 + \u00b7\u00b7\u00b7 + xn)]\n\nexpanding the right side of the preceding equation results in terms of the form\n\nx4\ni , x3\n\ni xj, x2\n\ni x2\n\nj , x2\n\ni xjxk,\n\nand\n\nxixjxkxl\n\nwhere i, j, k, and l are all different. because all the xi have mean 0, it follows by\nindependence that\n\ni xj] = e[x3\ne[x3\ni xjxk] = e[x2\n\ne[x2\ne[xixjxkxl] = 0\n\ni ]e[xj] = 0\ni ]e[xj]e[xk] = 0\n\n\u2020that is, the strong law of large numbers states that\n\nn\u2192q(x1 + \u00b7\u00b7\u00b7 + xn)/n = \u03bc} = 1\np{ lim\n\n "}, {"Page_number": 416, "text": "(cid:2)\n\n(cid:3)\n\nsection 8.4\n\nthe strong law of large numbers 401\n= 6 terms in the expansion that will\nj . hence, upon expanding the preceding product and taking expectations\n(cid:3)\n\nnow, for a given pair i and j, there will be\nequal x2\nterm by term, it follows that\n\ni x2\n\n(cid:2)\n\n4\n2\n\ne[s4\n\ni ] + 6\n\nn] = ne[x4\ne[x2\n= nk + 3n(n \u2212 1)e[x2\n\nn\n2\n\ni x2\nj ]\ni ]e[x2\nj ]\n\nwhere we have once again made use of the independence assumption. now, since\n\nwe have\n\n0 \u2026 var(x2\ni\n\n) = e[x4\n\n(e[x2\n\ni ])2 \u2026 e[x4\n\ni ])2\n\ni ] \u2212 (e[x2\ni ] = k\n\ntherefore, from the preceding, we obtain\n\nwhich implies that\n\ntherefore,\n\ne[s4\n\nn] \u2026 nk + 3n(n \u2212 1)k\n(cid:7)\n\ns4\nn\nn4\n\n(cid:8)\n\u23a4\n\u23a6 =\n\ne\n\n\u23a1\n\u23a3 q(cid:6)\n\nn=1\n\ne\n\ns4\nn\nn4\n\n\u2026 k\nn3\n\nq(cid:6)\n\ne\n\nn=1\n\n+ 3k\nn2\n(cid:7)\n(cid:8)\nq(cid:9)\n\ns4\nn\nn4\n\n< q\n\nbut the preceding implies that, with probability 1,\n\n/n4 < q. (for if there is a\npositive probability that the sum is infinite, then its expected value is infinite.) but the\nconvergence of a series implies that its nth term goes to 0; so we can conclude that,\nwith probability 1,\n\nn=1\n\ns4\nn\n\n/n4 = (sn/n)4 goes to 0, then so must sn/n; hence, we have proven that, with\n\nbut if s4\nn\nprobability 1,\n\nwhen \u03bc, the mean of the xi, is not equal to 0, we can apply the preceding argument\n\nto the random variables xi \u2212 \u03bc to obtain that with probability 1,\n\nn\u2192q\nlim\n\n= 0\n\ns4\nn\nn4\n\n\u2192 0\n\nsn\nn\n\nas\n\nn \u2192 q\n\nn(cid:6)\n\ni=1\n\nn\u2192q\nlim\n\n= 0\n\nn\n\n(xi \u2212 \u03bc)\nn(cid:6)\n\n= \u03bc\n\nxi\nn\n\ni=1\n\nor, equivalently,\n\nwhich proves the result.\n\nn\u2192q\nlim\n\n "}, {"Page_number": 417, "text": "402\n\nchapter 8\n\nlimit theorems\n\nn(cid:9)\n\nthe strong law is illustrated by two modules on the text website that consider inde-\npendent and identically distributed random variables which take on one of the values\n0, 1, 2, 3, and 4. the modules simulate the values of n such random variables; the\nproportions of time that each outcome occurs, as well as the resulting sample mean\nxi/n, are then indicated and plotted. when using these modules, which differ only\ni=1\nin the type of graph presented, one enters the probabilities and the desired value of n.\nfigure 8.2 gives the results of a simulation using a specified probability mass function\nand (a) n = 100, (b) n = 1000, and (c) n = 10,000.\n\n, (x1 + \u00b7\u00b7\u00b7 + xn\u2217 )/n\n\u2217\n\nmany students are initially confused about the difference between the weak and\nthe strong laws of large numbers. the weak law of large numbers states that, for any\n\u2217\nis likely to be near \u03bc. however, it does\nspecified large value n\nnot say that (x1 + \u00b7\u00b7\u00b7 + xn)/n is bound to stay near \u03bc for all values of n larger than\n. thus, it leaves open the possibility that large values of |(x1 + \u00b7\u00b7\u00b7 + xn)/n \u2212 \u03bc|\n\u2217\nn\ncan occur infinitely often (though at infrequent intervals). the strong law shows that\nthis cannot occur. in particular, it implies that, with probability 1, for any positive\nvalue \u03b5,\n\n666666 n(cid:6)\n\n1\n\n666666\n\n\u2212 \u03bc\n\nxi\nn\n\nwill be greater than \u03b5 only a finite number of times.\n\nstrong law of large numbers\n\nenter the probabilities and the number of trials \nto be simulated. the output gives the total number \nof times each outcome occurs, and the average \nof all outcomes.\n\nstart\n\nquit\n\np0\n\np1\n\np2\n\np3\n\np4\n\n.1\n\n.2\n\n.3\n\n.35\n\n.05\n\nn = 100\n\ntheoretical mean = 2.05\n\nsample mean = 1.89\n\n0\n\n15\n\n1\n\n20\n\n2\n\n30\n\n3\n\n31\n\n4\n\n4\n\nfigure 8.2(a)\n\n "}, {"Page_number": 418, "text": "section 8.5\n\nother inequalities 403\n\nstrong law of large numbers\n\nenter the probabilities and the number of trials \nto be simulated. the output gives the total number \nof times each outcome occurs, and the average \nof all outcomes.\n\nstart\n\nquit\n\np0\n\np1\n\np2\n\np3\n\np4\n\n.1\n\n.2\n\n.3\n\n.35\n\n.05\n\nn = 1000\n\ntheoretical mean = 2.05\n\nsample mean = 2.078\n\n0\n\n106\n\n1\n\n189\n\n2\n\n285\n\n3\n\n361\n\n4\n\n59\n\nfigure 8.2(b)\n\nthe strong law of large numbers was originally proven, in the special case of\nbernoulli random variables, by the french mathematician borel. the general form of\nthe strong law presented in theorem 4.1 was proven by the russian mathematician\na. n. kolmogorov.\n\n8.5 other inequalities\n\nwe are sometimes confronted with situations in which we are interested in obtaining\nan upper bound for a probability of the form p{x \u2212 \u03bc \u00fa a}, where a is some positive\nvalue and when only the mean \u03bc = e[x] and variance \u03c3 2 = var(x) of the distribu-\ntion of x are known. of course, since x \u2212 \u03bc \u00fa a > 0 implies that |x \u2212 \u03bc| \u00fa a, it\nfollows from chebyshev\u2019s inequality that\n\np{x \u2212 \u03bc \u00fa a} \u2026 p{|x \u2212 \u03bc| \u00fa a} \u2026 \u03c3 2\n\na2 when a > 0\n\nhowever, as the following proposition shows, it turns out that we can do better.\n\nproposition 5.1. one-sided chebyshev inequality\nif x is a random variable with mean 0 and finite variance \u03c3 2, then, for any a > 0,\n\np{x \u00fa a} \u2026\n\n\u03c3 2\n\n\u03c3 2 + a2\n\n "}, {"Page_number": 419, "text": "404\n\nchapter 8\n\nlimit theorems\n\nstrong law of large numbers\n\nenter the probabilities and the number of trials \nto be simulated. the output gives the total number \nof times each outcome occurs, and the average \nof all outcomes.\n\nstart\n\nquit\n\np0\n\np1\n\np2\n\np3\n\np4\n\n.1\n\n.2\n\n.3\n\n.35\n\n.05\n\nn = 10000\n\ntheoretical mean = 2.05\n\nsample mean = 2.0416\n\n0\n\n1\n\n2\n\n3\n\n1041\n\n2027\n\n2917\n\n3505\n\n4\n\n510\n\nfigure 8.2(c)\n\nproof. let b > 0 and note that\n\nx \u00fa a\n\nis equivalent to x + b \u00fa a + b\n\nhence,\n\np{x \u00fa a} = p{x + b \u00fa a + b}\n\n\u2026 p{(x + b)2 \u00fa (a + b)2}\n\nwhere the inequality is obtained by noting that since a + b > 0, x + b \u00fa a + b\nimplies that (x + b)2 \u00fa (a + b)2. upon applying markov\u2019s inequality, the\npreceding yields that\n\np{x \u00fa a} \u2026 e[(x + b)2]\n(a + b)2\n\n= \u03c3 2 + b2\n(a + b)2\n\nletting b = \u03c3 2/a [which is easily seen to be the value of b that minimizes\n(\u03c3 2 + b2)/(a + b)2] gives the desired result.\n\n "}, {"Page_number": 420, "text": "section 8.5\n\nother inequalities 405\n\nexample 5a\nif the number of items produced in a factory during a week is a random variable with\nmean 100 and variance 400, compute an upper bound on the probability that this\nweek\u2019s production will be at least 120.\n\nsolution. it follows from the one-sided chebyshev inequality that\n\np{x \u00fa 120} = p{x \u2212 100 \u00fa 20} \u2026\n\n400\n\n400 + (20)2\n\n= 1\n2\n\nhence, the probability that this week\u2019s production will be 120 or more is at most 1\n2.\n\nif we attempted to obtain a bound by applying markov\u2019s inequality, then we would\n\nhave obtained\n\np{x \u00fa 120} \u2026 e(x)\n120\n\n= 5\n6\n\nwhich is a far weaker bound than the preceding one.\n\n.\nsuppose now that x has mean \u03bc and variance \u03c3 2. since both x \u2212 \u03bc and \u03bc \u2212 x\nhave mean 0 and variance \u03c3 2, it follows from the one-sided chebyshev inequality\nthat, for a > 0,\n\np{x \u2212 \u03bc \u00fa a} \u2026\n\n\u03c3 2\n\n\u03c3 2 + a2\n\nand\n\np{\u03bc \u2212 x \u00fa a} \u2026\n\n\u03c3 2\n\n\u03c3 2 + a2\n\nthus, we have the following corollary.\n\ncorollary 5.1.\n\nif e[x] = \u03bc and var(x) = \u03c3 2, then, for a > 0,\n\np{x \u00fa \u03bc + a} \u2026\np{x \u2026 \u03bc \u2212 a} \u2026\n\n\u03c3 2\n\n\u03c3 2 + a2\n\u03c3 2 + a2\n\n\u03c3 2\n\nexample 5b\na set of 200 people consisting of 100 men and 100 women is randomly divided into\n100 pairs of 2 each. give an upper bound to the probability that at most 30 of these\npairs will consist of a man and a woman.\nsolution. number the men arbitrarily from 1 to 100, and for i = 1, 2, . . . 100, let\n\n%\n\nxi =\n\n1 if man i is paired with a woman\n0 otherwise\n\nthen x, the number of man\u2013woman pairs, can be expressed as\n\nx = 100(cid:6)\n\ni=1\n\nxi\n\n "}, {"Page_number": 421, "text": "406\n\nchapter 8\n\nlimit theorems\n\nbecause man i is equally likely to be paired with any of the other 199 people, of which\n100 are women, we have\n\ne[xi] = p{xi = 1} = 100\n199\n\nsimilarly, for i z j,\n\ne[xixj] = p{xi = 1, xj = 1}\n\n= p{xi = 1}p{xj = 1|xi = 1} = 100\n199\n\n99\n197\n\nwhere p{xj = 1|xi = 1} = 99/197, since, given that man i is paired with a woman,\nman j is equally likely to be paired with any of the remaining 197 people, of which 99\nare women. hence, we obtain\n\ne[x] = 100(cid:6)\n\nvar(x) = 100(cid:6)\n\ne[xi]\n\ni=1\n= (100)\nl 50.25\n\n100\n199\n\ni=1\n= 100\n100\n199\nl 25.126\n\n(cid:6)\n(cid:2)\n\ni<j\n\n(cid:6)\n(cid:3)(cid:7)\n\nvar(xi) + 2\n\ncov(xi, xj)\n\n+ 2\n\n99\n199\n\n100\n2\n\n100\n199\n\n99\n197\n\n\u2212\n\n(cid:8)\n\n(cid:3)2\n\n(cid:2)\n\n100\n199\n\nthe chebyshev inequality then yields\n\np{x \u2026 30} \u2026 p{|x \u2212 50.25| \u00fa 20.25} \u2026 25.126\n(20.25)2\n\nl .061\n\nthus, there are fewer than 6 chances in a hundred that fewer than 30 men will be\npaired with women. however, we can improve on this bound by using the one-sided\nchebyshev inequality, which yields\n\np{x \u2026 30} = p{x \u2026 50.25 \u2212 20.25}\n\n25.126\n\n25.126 + (20.25)2\n\n\u2026\nl .058\n\n.\n\nwhen the moment generating function of the random variable x is known, we can\n\nobtain even more effective bounds on p{x \u00fa a}. let\n\nm(t) = e[etx]\n\nbe the moment generating function of the random variable x. then, for t > 0,\n\np{x \u00fa a} = p{etx \u00fa eta}\n\n\u2026 e[etx]e\n\n\u2212ta by markov\u2019s inequality\n\n "}, {"Page_number": 422, "text": "section 8.5\n\nother inequalities 407\n\nsimilarly, for t < 0,\n\np{x \u2026 a} = p{etx \u00fa eta}\n\u2212ta\n\n\u2026 e[etx]e\n\nthus, we have the following inequalities, known as chernoff bounds.\nproposition 5.2. chernoff bounds\n\np{x \u00fa a} \u2026 e\np{x \u2026 a} \u2026 e\n\n\u2212tam(t)\n\u2212tam(t)\n\nfor all\nfor all\n\nt > 0\nt < 0\n\nsince the chernoff bounds hold for all t in either the positive or negative quadrant,\nwe obtain the best bound on p{x \u00fa a} by using the t that minimizes e\n\n\u2212tam(t).\n\nexample 5c chernoff bounds for the standard normal random variable\nif z is a standard normal random variable, then its moment generating function is\nm(t) = et2/2, so the chernoff bound on p{z \u00fa a} is given by\n\np{z \u00fa a} \u2026 e\n\n\u2212taet2/2\n\nfor all\n\nt > 0\n\nnow the value of t, t > 0, that minimizes et2/2\u2212ta is the value that minimizes t2/2 \u2212 ta,\nwhich is t = a. thus, for a > 0, we have\n\np{z \u00fa a} \u2026 e\n\n\u2212a2/2\n\nsimilarly, we can show that, for a < 0,\n\np{z \u2026 a} \u2026 e\n\n\u2212a2/2\n\n.\n\nexample 5d chernoff bounds for the poisson random variable\nif x is a poisson random variable with parameter \u03bb, then its moment generating func-\ntion is m(t) = e\u03bb(et\u22121). hence, the chernoff bound on p{x \u00fa i} is\n\np{x \u00fa i} \u2026 e\u03bb(et\u22121)e\n\n\u2212it\n\nt > 0\n\nminimizing the right side of the preceding inequality is equivalent to minimizing\n\u03bb(et \u2212 1) \u2212 it, and calculus shows that the minimal value occurs when et = i/\u03bb. pro-\nvided that i/\u03bb > 1, this minimizing value of t will be positive. therefore, assuming\n(cid:3)i\nthat i > \u03bb and letting et = i/\u03bb in the chernoff bound yields\n\n(cid:2)\n\np{x \u00fa i} \u2026 e\u03bb(i/\u03bb\u22121)\n\n\u03bb\ni\n\nor, equivalently,\n\np{x \u00fa i} \u2026 e\n\n\u2212\u03bb(e\u03bb)i\n\nii\n\n.\n\nexample 5e\nconsider a gambler who is equally likely to either win or lose 1 unit on every play,\nindependently of his past results. that is, if xi is the gambler\u2019s winnings on the ith\nplay, then the xi are independent and\n\n "}, {"Page_number": 423, "text": "408\n\nchapter 8\n\nlimit theorems\n\nlet sn = n(cid:9)\n\np{xi = 1} = p{xi = \u22121} = 1\n2\n\nxi denote the gambler\u2019s winnings after n plays. we will use the chernoff\n\nbound on p{sn \u00fa a}. to start, note that the moment generating function of xi is\n\ni=1\n\n2\n\ne[etx] = et + e\n\u2212t\n(cid:4)\n\u2212t, we see that\n1 \u2212 t + t2\n2!\n\n+ \u00b7\u00b7\u00b7 +\n7\n\n(cid:5)\n\n+ \u00b7\u00b7\u00b7\n\n\u2212 t3\n3!\n\nnow, using the mclaurin expansions of et and e\n\net + e\n\n= 2\n\n\u2212t = 1 + t + t2\n+ t3\n0\n3!\n2!\n1 + t2\n+ t4\n+ \u00b7\u00b7\u00b7\nq(cid:6)\n4!\n2!\nt2n\nq(cid:6)\n(2n)!\n\n= 2\n\nn=0\n\n(t2/2)n\n\n\u2026 2\nn=0\n= 2et2/2\n\nn!\n\nsince (2n)! \u00fa n!2n\n\ntherefore,\n\ne[etx] \u00fa et2/2\n\nsince the moment generating function of the sum of independent random variables\nis the product of their moment generating functions, we have\n\ne[etsn] = (e[etx])n\n\n\u2026 ent2/2\n\nusing the preceding result along with the chernoff bound gives\n\np{sn \u00fa a} \u2026 e\n\n\u2212taent2/2\n\nt > 0\n\nthe value of t that minimizes the right side of the preceding is the value that min-\nimizes nt2/2 \u2212 ta, and this value is t = a/n. supposing that a > 0 (so that the\nminimizing t is positive) and letting t = a/n in the preceding inequality yields\n\np{sn \u00fa a} \u2026 e\n\n\u2212a2/2n\n\na > 0\n\nthis latter inequality yields, for example,\np{s10 \u00fa 6} \u2026 e\n\n\u221236/20 l .1653\n\n "}, {"Page_number": 424, "text": "section 8.5\n\nother inequalities 409\n\nwhereas the exact probability is\n\np{s10 \u00fa 6} = p{gambler wins at least 8 of the first 10 games}\n\n(cid:2)\n\n(cid:2)\n\n(cid:3)\n\n+\n\n10\n8\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\n+\n\n10\n10\n\n=\n\n10\n9\n210\n\n= 56\n1024\n\nl .0547\n\n.\n\nthe next inequality is one having to do with expectations rather than probabilities.\n\nbefore stating it, we need the following definition.\n\ndefinition\n\na twice-differentiable real-valued function f (x) is said to be convex if f\nall x; similarly, it is said to be concave if f\nsome examples of convex functions are f (x) = x2, f (x) = eax, and f (x) = \u2212x1/n for\n\n(cid:8)(cid:8)(x) \u2026 0.\n\nx \u00fa 0. if f (x) is convex, then g(x) = \u2212f (x) is concave, and vice versa.\nproposition 5.3.\nif f (x) is a convex function, then\n\njensen\u2019s inequality\n\n(cid:8)(cid:8)(x) \u00fa 0 for\n\ne[ f (x)] \u00fa f (e[x])\n\nprovided that the expectations exist and are finite.\n\nproof. expanding f (x) in a taylor\u2019s series expansion about \u03bc = e[x] yields\n\nf (x) = f (\u03bc) + f\n\n(cid:8)(\u03bc)(x \u2212 \u03bc) + f\n\n(cid:8)(cid:8)(\u03be )(x \u2212 \u03bc)2\n\n2\n\n(cid:8)(cid:8)(\u03be ) \u00fa 0, we obtain\n\nwhere \u03be is some value between x and \u03bc. since f\n\nhence,\n\nf (x) \u00fa f (\u03bc) + f\n\n(cid:8)(\u03bc)(x \u2212 \u03bc)\n\nf (x) \u00fa f (\u03bc) + f\n\n(cid:8)(\u03bc)(x \u2212 \u03bc)\n\ntaking expectations yields\n\ne[ f (x)] \u00fa f (\u03bc) + f\n\n(cid:8)(\u03bc)e[x \u2212 \u03bc] = f (\u03bc)\n\nand the inequality is established.\n\nexample 5f\nan investor is faced with the following choices: either she can invest all of her money\nin a risky proposition that would lead to a random return x that has mean m, or\nshe can put the money into a risk-free venture that will lead to a return of m with\nprobability 1. suppose that her decision will be made on the basis of maximizing the\nexpected value of u(r), where r is her return and u is her utility function. by jensen\u2019s\ninequality, it follows that if u is a concave function, then e[u(x)] \u2026 u(m), so the risk-\nfree alternative is preferable, whereas if u is convex, then e[u(x)] \u00fa u(m), so the\n.\nrisky investment alternative would be preferred.\n\n "}, {"Page_number": 425, "text": "410\n\nchapter 8\n\nlimit theorems\n\n8.6 bounding the error probability when approximating a sum of independent\n\nbernoulli random variables by a poisson random variable\n\nin this section, we establish bounds on how closely a sum of independent bernoulli\nrandom variables is approximated by a poisson random variable with the same mean.\nsuppose that we want to approximate the sum of independent bernoulli random\nvariables with respective means p1, p2, . . . , pn. starting with a sequence y1, . . . , yn\nof independent poisson random variables, with yi having mean pi, we will construct\na sequence of independent bernoulli random variables x1, . . . , xn with parameters\np1, . . . , pn such that\n\np{xi z yi} \u2026 p2\n\ni\n\nfor each i\n\nletting x = n(cid:9)\n\nxi and y = n(cid:9)\n\ni=1\n\ni=1\n\nclude that\n\nyi, we will use the preceding inequality to con-\n\np{x z y} \u2026\n\nn(cid:6)\n\ni=1\n\np2\ni\n\nfinally, we will show that the preceding inequality implies that, for any set of real\nnumbers a,\n\n|p{x \u2208 a} \u2212 p{y \u2208 a}| \u2026\n\nn(cid:6)\n\ni=1\n\np2\ni\n\nsince x is the sum of independent bernoulli random variables and y is a poisson\nrandom variable, the latter inequality will yield the desired bound.\nto show how the task is accomplished, let yi, i = 1, . . . , n be independent pois-\nson random variables with respective means pi. now let u1, . . . , un be independent\nrandom variables that are also independent of the yi\u2019s and which are such that\n\n%\n0 with probability (1 \u2212 pi)epi\n1 with probability 1 \u2212 (1 \u2212 pi)epi\n\nui =\n\nthis definition implicitly makes use of the inequality\n\n\u2212p \u00fa 1 \u2212 p\ne\n\nin assuming that (1 \u2212 pi)epi \u2026 1.\n\nnext, define the random variables xi, i = 1, . . . , n, by\nif yi = ui = 0\n\n%\n\nxi =\n\n0\n1 otherwise\n\nnote that\n\np{xi = 0} = p{yi = 0}p{ui = 0} = e\np{xi = 1} = 1 \u2212 p{xi = 0} = pi\n\n\u2212pi (1 \u2212 pi)epi = 1 \u2212 pi\n\n "}, {"Page_number": 426, "text": "section 8.6\n\nbounding the error probability 411\n\nnow, if xi is equal to 0, then so must yi equal 0 (by the definition of xi). therefore,\n\np{xi z yi} = p{xi = 1, yi z 1}\n\n= p{yi = 0, xi = 1} + p{yi > 1}\n= p{yi = 0, ui = 1} + p{yi > 1}\n= e\n\u2212pi[1 \u2212 (1 \u2212 pi)epi] + 1 \u2212 e\n= pi \u2212 pie\n\u2026 p2\ni\n\n\u2212pi\n(since 1 \u2212 e\n\n\u2212p \u2026 p)\n\n\u2212pi \u2212 pie\n\n\u2212pi\n\nnow let x = n(cid:9)\nyi, and note that x is the sum of independent\nn(cid:9)\nbernoulli random variables and y is poisson with the expected value e[y] = e[x] =\n\nxi and y = n(cid:9)\n\npi. note also that the inequality x z y implies that xi z yi for some i, so\n\ni=1\n\ni=1\n\ni=1\n\np{x z y} \u2026 p{xi z yi for some i}\n\nn(cid:6)\nn(cid:6)\n\ni=1\n\ni=1\n\n\u2026\n\n\u2026\n\np{xi z yi}\n\n(boole\u2019s inequality)\n\np2\ni\n\n%\n\nib =\n\n1 if b occurs\n0 otherwise\n\nfor any event b, let ib, the indicator variable for the event b, be defined by\n\nnote that, for any set of real numbers a,\n\ni{x\u2208a} \u2212 i{y\u2208a} \u2026 i{xzy}\n\nthe preceding inequality follows from the fact that, since an indicator variable is\neither 0 or 1, the left-hand side equals 1 only when i{x\u2208a} = 1 and i{y\u2208a} = 0. but\nthis would imply that x \u2208 a and y (cid:12)\u2208 a, which means that x z y, so the right side\nwould also equal 1. upon taking expectations of the preceding inequality, we obtain\n\np{x \u2208 a} \u2212 p{y \u2208 a} \u2026 p{x z y}\n\nby reversing x and y, we obtain, in the same manner,\n\np{y \u2208 a} \u2212 p{x \u2208 a} \u2026 p{x z y}\n\nthus, we can conclude that\n\n|p{x \u2208 a} \u2212 p{y \u2208 a}| \u2026 p{x z y}\n\ntherefore, we have proven that with \u03bb = n(cid:9)\n\u23ab\u23ac\n(cid:6)\n\u23ad \u2212\nxi \u2208 a\n\n\u23a7\u23a8\n\u23a9 n(cid:6)\n\ni=1\n\npi,\n\n6666666p\n\ni=1\n\n\u2212\u03bb\u03bbi\ne\ni!\n\ni\u2208a\n\n6666666 \u2026\n\nn(cid:6)\n\ni=1\n\np2\ni\n\n "}, {"Page_number": 427, "text": "412\n\nchapter 8\n\nlimit theorems\n\nremark. when all the pi are equal to p, x is a binomial random variable. hence,\n\nthe preceding inequality shows that, for any set of nonnegative integers a,\n\n(cid:3)\n\n(cid:2)\n\n666666\n\n(cid:6)\n\ni\u2208a\n\nn\ni\n\npi(1 \u2212 p)n\u2212i \u2212\n\n(cid:6)\n\n\u2212np(np)i\ne\n\ni\u2208a\n\ni!\n\n666666 \u2026 np2\n\n.\n\nsummary\ntwo useful probability bounds are provided by the markov and chebyshev inequali-\nties. the markov inequality is concerned with nonnegative random variables and says\nthat, for x of that type,\n\np{x \u00fa a} \u2026 e[x]\na\n\nfor every positive value a. the chebyshev inequality, which is a simple consequence\nof the markov inequality, states that if x has mean \u03bc and variance \u03c3 2, then, for every\npositive k,\n\np{|x \u2212 \u03bc| \u00fa k\u03c3} \u2026 1\nk2\n\nthe two most important theoretical results in probability are the central limit theorem\nand the strong law of large numbers. both are concerned with a sequence of indepen-\ndent and identically distributed random variables. the central limit theorem says that\nif the random variables have a finite mean \u03bc and a finite variance \u03c3 2, then the distri-\nbution of the sum of the first n of them is, for large n, approximately that of a normal\nrandom variable with mean n\u03bc and variance n\u03c3 2. that is, if xi, i \u00fa 1, is the sequence,\nthen the central limit theorem states that, for every real number a,\n\n7\n\n*\n\nn\u2192q p\nlim\n\nx1 + \u00b7\u00b7\u00b7 + xn \u2212 n\u03bc\n\n\u221a\nn\n\n\u03c3\n\n\u2026 a\n\n= 1\u221a\n2\u03c0\n\n\u2212x2/2 dx\ne\n\na\n\u2212q\n\n0\n\nthe strong law of large numbers requires only that the random variables in the sequence\nhave a finite mean \u03bc. it states that, with probability 1, the average of the first n of them\nwill converge to \u03bc as n goes to infinity. this implies that if a is any specified event\nof an experiment for which independent replications are performed, then the limit-\ning proportion of experiments whose outcomes are in a will, with probability 1, equal\np(a). therefore, if we accept the interpretation that \u201cwith probability 1\u201d means \u201cwith\ncertainty,\u201d we obtain the theoretical justification for the long-run relative frequency\ninterpretation of probabilities.\n\nproblems\n\n8.1. suppose that x is a random variable with mean\nand variance both equal to 20. what can be said\nabout p{0 < x < 40}?\n\n8.2. from past experience, a professor knows that the\ntest score of a student taking her final examination\nis a random variable with mean 75.\n(a) give an upper bound for the probability that\na student\u2019s test score will exceed 85. sup-\npose, in addition, that the professor knows\n\nthat the variance of a student\u2019s test score is\nequal to 25.\n\n(b) what can be said about the probability that a\n\nstudent will score between 65 and 85?\n\n(c) how many students would have to take the\nexamination to ensure, with probability at\nleast .9, that the class average would be\nwithin 5 of 75? do not use the central limit\ntheorem.\n\n "}, {"Page_number": 428, "text": "8.3. use the central limit theorem to solve part (c) of\n\nproblem 2.\n\n8.4. let x1, . . . , x20 be independent poisson random\n\nvariables with mean 1.\n(a) use the markov inequality to obtain a\n\nbound on\n\n(b) use the central limit theorem to approximate\n\n\u23a7\u23a8\n\u23a9 20(cid:6)\n\u23a7\u23a8\n\u23a9 20(cid:6)\n\n1\n\n1\n\np\n\nxi > 15\n\np\n\nxi > 15\n\n\u23ab\u23ac\n\u23ad\n\u23ab\u23ac\n\u23ad .\n\n8.5. fifty numbers are rounded off to the nearest inte-\nger and then summed. if the individual round-\noff errors are uniformly distributed over (\u2212.5, .5),\napproximate the probability that the resultant sum\ndiffers from the exact sum by more than 3.\n\n8.6. a die is continually rolled until the total sum of\nall rolls exceeds 300. approximate the probability\nthat at least 80 rolls are necessary.\n\n8.7. a person has 100 light bulbs whose lifetimes are\nindependent exponentials with mean 5 hours. if\nthe bulbs are used one at a time, with a failed bulb\nbeing replaced immediately by a new one, approx-\nimate the probability that there is still a working\nbulb after 525 hours.\n\n8.8. in problem 7, suppose that it takes a random time,\nuniformly distributed over (0,\n.5), to replace a\nfailed bulb. approximate the probability that all\nbulbs have failed by time 550.\n\n8.9. if x is a gamma random variable with parameters\n(n, 1), approximately how large need n be so that\n\n06666 x\n\nn\n\np\n\n7\n\n6666 > .01\n\n\u2212 1\n\n< .01?\n\n8.10. civil engineers believe that w, the amount of\nweight (in units of 1000 pounds) that a certain span\nof a bridge can withstand without structural dam-\nage resulting, is normally distributed with mean\n400 and standard deviation 40. suppose that the\nweight (again, in units of 1000 pounds) of a car\nis a random variable with mean 3 and standard\ndeviation .3. approximately how many cars would\nhave to be on the bridge span for the probability\nof structural damage to exceed .1?\n\n8.11. many people believe that the daily change of price\nof a company\u2019s stock on the stock market is a ran-\ndom variable with mean 0 and variance \u03c3 2. that\nis, if yn represents the price of the stock on the nth\nday, then\n\nyn = yn\u22121 + xn n \u00fa 1\n\nwhere x1, x2, . . . are independent and identically\ndistributed random variables with mean 0 and\n\nproblems 413\n\nvariance \u03c3 2. suppose that the stock\u2019s price today\nis 100. if \u03c3 2 = 1, what can you say about the prob-\nability that the stock\u2019s price will exceed 105 after\n10 days?\n\n8.12. we have 100 components that we will put in use in\na sequential fashion. that is, component 1 is ini-\ntially put in use, and upon failure, it is replaced\nby component 2, which is itself replaced upon fail-\nure by component 3, and so on. if the lifetime\nof component i is exponentially distributed with\nmean 10 + i/10, i = 1, . . . , 100, estimate the prob-\nability that the total life of all components will\nexceed 1200. now repeat when the life distribu-\ntion of component i is uniformly distributed over\n(0, 20 + i/5), i = 1, . . . , 100.\n\n8.13. student scores on exams given by a certain instruc-\ntor have mean 74 and standard deviation 14. this\ninstructor is about to give two exams, one to a class\nof size 25 and the other to a class of size 64.\n(a) approximate the probability that the average\n\ntest score in the class of size 25 exceeds 80.\n\n(b) repeat part (a) for the class of size 64.\n(c) approximate the probability that the average\ntest score in the larger class exceeds that of\nthe other class by over 2.2 points.\n\n(d) approximate the probability that the average\ntest score in the smaller class exceeds that of\nthe other class by over 2.2 points.\n\n8.14. a certain component is critical to the operation of\nan electrical system and must be replaced immedi-\nately upon failure. if the mean lifetime of this type\nof component is 100 hours and its standard devi-\nation is 30 hours, how many of these components\nmust be in stock so that the probability that the\nsystem is in continual operation for the next 2000\nhours is at least .95?\n\n8.15. an insurance company has 10,000 automobile pol-\nicyholders. the expected yearly claim per policy-\nholder is $240, with a standard deviation of $800.\napproximate the probability that the total yearly\nclaim exceeds $2.7 million.\n\n8.16. a.j. has 20 jobs that she must do in sequence, with\nthe times required to do each of these jobs being\nindependent random variables with mean 50 min-\nutes and standard deviation 10 minutes. m.j. has\n20 jobs that he must do in sequence, with the times\nrequired to do each of these jobs being indepen-\ndent random variables with mean 52 minutes and\nstandard deviation 15 minutes.\n(a) find the probability that a.j. finishes in less\n\n(b) find the probability that m.j. finishes in less\n\n(c) find the probability that a.j. finishes\n\nthan 900 minutes.\n\nthan 900 minutes.\n\nbefore m.j.\n\n "}, {"Page_number": 429, "text": "414\n\nchapter 8\n\nlimit theorems\n\n8.17. redo example 5b under the assumption that the\nnumber of man\u2013woman pairs is (approximately)\nnormally distributed. does this seem like a reason-\nable supposition?\n\n8.18. repeat part (a) of problem 2 when it is known that\nthe variance of a student\u2019s test score is equal to 25.\n8.19. a lake contains 4 distinct types of fish. suppose\nthat each fish caught is equally likely to be any\none of these types. let y denote the number of\nfish that need be caught to obtain at least one of\neach type.\n(a) give an interval (a, b) such that p{a \u2026 y \u2026 b}\n\u00fa .90.\n\n(b) using the one-sided chebyshev inequality,\nhow many fish need we plan on catching so as\nto be at least 90 percent certain of obtaining\nat least one of each type.\n\n8.20. if x is a nonnegative random variable with mean\n\n25, what can be said about\n\u221a\n(a) e[x3]?\n(b) e[\nx]?\n(c) e[log x]?\n\u2212x]?\n(d) e[e\n\n8.21. let x be a nonnegative random variable.\n\nprove that\n\ne[x] \u2026 (e[x2])1/2 \u2026 (e[x3])1/3 \u2026 \u00b7\u00b7\u00b7\n\n8.22. would the results of example 5f change if the\ninvestor were allowed to divide her money and\ninvest the fraction \u03b1, 0 < \u03b1 < 1, in the risky propo-\nsition and invest the remainder in the risk-free\nventure? her return for such a split investment\nwould be r = \u03b1x + (1 \u2212 \u03b1)m.\n\n8.23. let x be a poisson random variable with mean 20.\n(a) use the markov inequality to obtain an upper\n\nbound on\n\np = p{x \u00fa 26}\n\n(b) use the one-sided chebyshev inequality to\n\nobtain an upper bound on p.\n\n(c) use the chernoff bound to obtain an upper\n\n(d) approximate p by making use of the central\n\n(e) determine p by running an appropriate pro-\n\nbound on p.\n\nlimit theorem.\n\ngram.\n\ntheoretical exercises\n\n8.1. if x has variance \u03c3 2, then \u03c3 , the positive square\nroot of the variance, is called the standard devia-\ntion. if x has mean \u03bc and standard deviation \u03c3 ,\nshow that\n\np{|x \u2212 \u03bc| \u00fa k\u03c3} \u2026 1\nk2\n\n8.2. if x has mean \u03bc and standard deviation \u03c3 , the\nratio r k |\u03bc|/\u03c3 is called the measurement signal-\nto-noise ratio of x. the idea is that x can be\nexpressed as x = \u03bc + (x \u2212 \u03bc), with \u03bc represent-\ning the signal and x \u2212 \u03bc the noise. if we define\n|(x \u2212 \u03bc)/\u03bc| k d as the relative deviation of x\nfrom its signal (or mean) \u03bc, show that, for \u03b1 > 0,\n\np{d \u2026 \u03b1} \u00fa 1 \u2212 1\nr2\u03b12\n\n8.3. compute the measurement signal-to-noise ratio\u2014\nthat is, |\u03bc|/\u03c3 , where \u03bc = e[x] and \u03c3 2 = var(x)\u2014\nof the following random variables:\n(a) poisson with mean \u03bb;\n(b) binomial with parameters n and p;\n(c) geometric with mean 1/p;\n(d) uniform over (a, b);\n(e) exponential with mean 1/\u03bb;\n(f) normal with parameters \u03bc, \u03c3 2.\n\n8.4. let zn, n \u00fa 1, be a sequence of random variables\nand c a constant such that, for each \u03b5 > 0, p{|zn \u2212\nc| > \u03b5}\u2192 0 as n\u2192 q. show that, for any bounded\ncontinuous function g,\n\ne[g(zn)]\u2192 g(c) as n\u2192q\n\n8.5. let f (x) be a continuous function defined for 0 \u2026\n\nx \u2026 1. consider the functions\n\nbn(x) = n(cid:6)\n\nf\n\nk=0\n\n(cid:2)\n\n(cid:3)(cid:2)\n\n(cid:3)\n\nk\nn\n\nn\nk\n\nxk(1 \u2212 x)n\u2212k\n\n(called bernstein polynomials) and prove that\n\nlim\n\nn\u2192q bn(x) = f (x)\n(cid:7)\n\nhint: let x1, x2, . . . be independent bernoulli\nrandom variables with mean x. show that\nx1 + \u00b7\u00b7\u00b7 + xn\n\n(cid:3)(cid:8)\n\n(cid:2)\n\nbn(x) = e\n\nf\n\nn\n\nand then use theoretical exercise 4.\nsince it can be shown that the convergence of\nbn(x) to f (x) is uniform in x, the preceding rea-\nsoning provides a probabilistic proof of the famous\nweierstrass theorem of analysis, which states that\n\n "}, {"Page_number": 430, "text": "any continuous function on a closed interval can be\napproximated arbitrarily closely by a polynomial.\n8.6. (a) let x be a discrete random variable whose\npossible values are 1, 2, . . . . if p{x = k} is\nnonincreasing in k = 1, 2, . . . , prove that\n\nheads would you expect on the final 900 tosses?\ncomment on the statement \u201cthe strong law of\nlarge numbers swamps, but does not compensate.\u201d\n8.10. if x is a poisson random variable with mean \u03bb,\n\nshow that for i < \u03bb,\n\nself-test problems and exercises 415\n\np{x = k} \u2026 2\n\ne[x]\n\nk2\n\n(b) let x be a nonnegative continuous random\nvariable having a nonincreasing density func-\ntion. show that\n\nf (x) \u2026 2e[x]\n\nx2\n\nfor all\n\nx > 0\n\n8.7. suppose that a fair die is rolled 100 times. let xi\nbe the value obtained on the ith roll. compute an\napproximation for\n\n\u23a7\u23a8\n\u23a9100(cid:31)\n\n1\n\np\n\nxi \u2026 a100\n\n\u23ab\u23ac\n\u23ad 1 < a < 6\n\np{x \u2026 i} \u2026 e\n\n\u2212\u03bb(e\u03bb)i\n\nii\n\ne\n\nt>0\n\n8.11. let x be a binomial random variable with param-\n\u2212tie[etx] occurs when t is such that\n\neters n and p. show that, for i > np,\n(a) minimum\net = iq\n(n\u2212i)p , where q = 1 \u2212 p.\n\nii(n\u2212i)n\u2212i pi(1 \u2212 p)n\u2212i.\n\n(b) p{x \u00fa i} \u2026\n8.12. the chernoff bound on a standard normal random\nvariable z gives p{z > a} \u2026 e\n\u2212a2/2, a > 0. show,\nby considering the density of z, that the right side\nof the inequality can be reduced by the factor 2.\nthat is, show that\n\nnn\n\n8.8. explain why a gamma random variable with\nparameters (t, \u03bb) has an approximately normal dis-\ntribution when t is large.\n\n8.9. suppose a fair coin is tossed 1000 times. if the first\n100 tosses all result in heads, what proportion of\n\np{z > a} \u2026 1\n2\n\ne\n\n\u2212a2/2\n\na > 0\n\n8.13. show that if e[x] < 0 and \u03b8 z 0 is such that\n\ne[e\u03b8x] = 1, then \u03b8 > 0.\n\nself-test problems and exercises\n\n8.1. the number of automobiles sold weekly at a cer-\ntain dealership is a random variable with expected\nvalue 16. give an upper bound to the probabil-\nity that\n(a) next week\u2019s sales exceed 18;\n(b) next week\u2019s sales exceed 25.\n\n8.2. suppose in problem 1 that the variance of the\n\nnumber of automobiles sold weekly is 9.\n(a) give a lower bound to the probability that\nnext week\u2019s sales are between 10 and 22,\ninclusively.\n\n(b) give an upper bound to the probability that\n\nnext week\u2019s sales exceed 18.\n\n8.3. if\n\ne[x] = 75 e[y] = 75 var(x) = 10\nvar(y) = 12 cov(x, y) = \u22123\n\nat factory b is a random variable with mean\n18 and standard deviation 6. assuming indepen-\ndence, derive an upper bound for the probability\nthat more units are produced today at factory b\nthan at factory a.\n\n8.5. the amount of time that a certain type of compo-\nnent functions before failing is a random variable\nwith probability density function\n\nf (x) = 2x 0 < x < 1\n\nit\n\nfails,\n\nbe put in use, then sn = n(cid:9)\n\nonce the component\nis immediately\nreplaced by another one of the same type. if we\nlet xi denote the lifetime of the ith component to\nxi represents the time\nof the nth failure. the long-term rate at which fail-\nures occur, call it r, is defined by\n\ni=1\n\ngive an upper bound to\n(a) p{|x \u2212 y| > 15};\n(b) p{x > y + 15};\n(c) p{y > x + 15}.\n\n8.4. suppose that the number of units produced daily\nat factory a is a random variable with mean 20 and\nstandard deviation 3 and the number produced\n\nr = lim\nn\u2192q\n\nn\nsn\n\nassuming that the random variables xi, i \u00fa 1, are\nindependent, determine r.\n\n8.6. in self-test problem 5, how many compo-\nnents would one need to have on hand to be\n\n "}, {"Page_number": 431, "text": "416\n\nchapter 8\n\nlimit theorems\n\napproximately 90 percent certain that the stock\nwill last at least 35 days?\n\n8.7. the servicing of a machine requires two separate\nsteps, with the time needed for the first step being\nan exponential random variable with mean .2 hour\nand the time for the second step being an inde-\npendent exponential random variable with mean\n.3 hour. if a repair person has 20 machines to ser-\nvice, approximate the probability that all the work\ncan be completed in 8 hours.\n\n8.8. on each bet, a gambler loses 1 with probability .7,\nloses 2 with probability .2, or wins 10 with prob-\nability .1. approximate the probability that the\ngambler will be losing after his first 100 bets.\n\n8.9. determine t so that the probability that the repair\nperson in self-test problem 7 finishes the 20 jobs\nwithin time t is approximately equal to .95.\n\n8.10. a tobacco company claims that the amount of\nnicotine in one of its cigarettes is a random\nvariable with mean 2.2 mg and standard devia-\ntion .3 mg. however, the average nicotine con-\ntent of 100 randomly chosen cigarettes was 3.1\nmg. approximate the probability that the average\nwould have been as high as or higher than 3.1 if the\ncompany\u2019s claims were true.\n\n8.11. each of the batteries in a collection of 40 batter-\nies is equally likely to be either a type a or a type\nb battery. type a batteries last for an amount of\ntime that has mean 50 and standard deviation 15;\n\ntype b batteries last for an amount of time that has\nmean 30 and standard deviation 6.\n(a) approximate the probability that the total life\n\nof all 40 batteries exceeds 1700.\n\n(b) suppose it is known that 20 of the batteries\nare type a and 20 are type b. now approx-\nimate the probability that the total life of all\n40 batteries exceeds 1700.\n\n8.12. a clinic is equally likely to have 2, 3, or 4 doctors\nvolunteer for service on a given day. no matter\nhow may volunteer doctors there are on a given\nday, the numbers of patients seen by these doctors\nare independent poisson random variables with\nmean 30. let x denote the number of patients\nseen in the clinic on a given day.\n(a) find e[x].\n(b) find var(x).\n(c) use a table of the standard normal probability\n\ndistribution to approximate p{x > 65}.\n\n8.13. the strong law of large numbers states that, with\nprobability 1, the successive arithmetic averages\nof a sequence of independent and identically dis-\ntributed random variables converge to their com-\nmon mean \u03bc. what do the successive geometric\naverages converge to? that is, what is\n\nn(cid:31)\n\n(\n\ni=1\n\nn\u2192q\nlim\n\nxi)1/n\n\n "}, {"Page_number": 432, "text": "c h a p t e r\n\n9\n\nadditional topics in probability\n\n9.1 the poisson process\n9.2 markov chains\n9.3 surprise, uncertainty, and entropy\n9.4 coding theory and entropy\n\n9.1 the poisson process\n\nbefore we define a poisson process, let us recall that a function f is said to be o(h) if\n\nh\u21920\nlim\n\nf (h)\n\nh\n\n= 0.\n\nonly on the length of that interval and not on its location.\n\nthat is, f is o(h) if, for small values of h, f (h) is small even in relation to h. sup-\npose now that \u201cevents\u201d are occurring at random points at time, and let n(t) denote\nthe number of events that occur in the time interval [0, t]. the collection of random\nvariables {n(t), t \u00fa 0} is said to be a poisson process having rate \u03bb, \u03bb > 0, if\n(i) n(0) = 0.\n(ii) the numbers of events that occur in disjoint time intervals are independent.\n(iii) the distribution of the number of events that occur in a given interval depends\n(iv) p{n(h) = 1} = \u03bbh + o(h).\n(v) p{n(h) \u00fa 2} = o(h).\nthus, condition (i) states that the process begins at time 0. condition (ii), the inde-\npendent increment assumption, states, for instance, that the number of events that\noccur by time t [that is, n(t)] is independent of the number of events that occur\nbetween t and t + s [that is, n(t + s) \u2212 n(t)]. condition (iii), the stationary increment\nassumption, states that the probability distribution of n(t + s) \u2212 n(t) is the same for\nall values of t.\n\nin chapter 4, we presented an argument, based on the poisson distribution being\na limiting version of the binomial distribution, that the foregoing conditions imply\nthat n(t) has a poisson distribution with mean \u03bbt. we will now obtain this result by a\ndifferent method.\n\nlemma 1.1\nfor a poisson process with rate \u03bb,\n\np{n(t) = 0} = e\n\n\u2212\u03bbt\n\n417\n\n "}, {"Page_number": 433, "text": "418\n\nchapter 9\n\nadditional topics in probability\n\nproof. let p0(t) = p{n(t) = 0}. we derive a differential equation for p0(t) in the\nfollowing manner:\n\np0(t + h) = p{n(t + h) = 0}\n\n= p{n(t) = 0, n(t + h) \u2212 n(t) = 0}\n= p{n(t) = 0}p{n(t + h) \u2212 n(t) = 0}\n= p0(t)[1 \u2212 \u03bbh + o(h)]\n\nwhere the final two equations follow from condition (ii) plus the fact that condi-\ntions (iv) and (v) imply that p{n(h) = 0} = 1 \u2212 \u03bbh + o(h). hence,\n\np0(t + h) \u2212 p0(t)\n\n= \u2212\u03bbp0(t) + o(h)\nh\n\nnow, letting h\u21920, we obtain\n\nh\n\nor, equivalently,\n\n(cid:8)\np\n0\n\n(t) = \u2212\u03bbp0(t)\n\n(cid:8)\np\n(t)\n0\np0(t)\n\n= \u2212\u03bb\n\nwhich implies, by integration, that\n\nlog p0(t) = \u2212\u03bbt + c\n\nor\n\np0(t) = ke\nsince p0(0) = p{n(0) = 0} = 1, we arrive at\np0(t) = e\n\n\u2212\u03bbt\n\n\u2212\u03bbt\n\nfor a poisson process, let t1 denote the time the first event occurs. further, for\nn > 1, let tn denote the time elapsed between the (n \u2212 1)st and the nth event. the\nsequence {tn, n = 1, 2, . . .} is called the sequence of interarrival times. for instance, if\nt1 = 5 and t2 = 10, then the first event of the poisson process would have occurred\nat time 5 and the second at time 15.\nwe shall now determine the distribution of the tn. to do so, we first note that the\nevent {t1 > t} takes place if and only if no events of the poisson process occur in the\ninterval [0, t]; thus,\n\np{t1 > t} = p{n(t) = 0} = e\n\n\u2212\u03bbt\n\nhence, t1 has an exponential distribution with mean 1/\u03bb. now,\n\np{t2 > t} = e[p{t2 > t|t1}]\n\nhowever,\n\np{t2 > t|t1 = s} = p{0 events in (s, s + t]|t1 = s}\n\n= p{0 events in (s, s + t]}\n= e\n\n\u2212\u03bbt\n\n "}, {"Page_number": 434, "text": "section 9.2\n\nmarkov chains 419\n\nwhere the last two equations followed from the assumptions about independent and\nstationary increments. from the preceding, we conclude that t2 is also an exponen-\ntial random variable with mean 1/\u03bb and, furthermore, that t2 is independent of t1.\nrepeating the same argument yields proposition 1.1.\nproposition 1.1. t1, t2, . . . are independent exponential random variables, each with\nmean 1/\u03bb.\n\nanother quantity of interest is sn, the arrival time of the nth event, also called the\n\nwaiting time until the nth event. it is easily seen that\n\nsn = n(cid:6)\n\ni=1\n\nti n \u00fa 1\n\nhence, from proposition 1.1 and the results of section 5.6.1, it follows that sn has a\ngamma distribution with parameters n and \u03bb. that is, the probability density of sn is\ngiven by\n\n(x) = \u03bbe\n\n\u2212\u03bbx\n\nfsn\n\n(\u03bbx)n\u22121\n(n \u2212 1)!\n\nx \u00fa 0\n\nwe are now ready to prove that n(t) is a poisson random variable with mean \u03bbt.\n\ntheorem 1.1. for a poisson process with rate \u03bb,\np{n(t) = n} = e\n\n\u2212\u03bbt(\u03bbt)n\n\nn!\n\nproof. note that the nth event of the poisson process will occur before or at time\nt if and only if the number of events that occur by t is at least n. that is,\n\nn(t) \u00fa n 3 sn \u2026 t\n\nso\n\np{n(t) = n} = p{n(t) \u00fa n} \u2212 p{n(t) \u00fa n + 1}\n\nt\n\n\u03bbe\n\n\u2212\u03bbx\n\n*\n*\n= p{sn \u2026 t} \u2212 p{sn+1 \u2026 t}\n=\ndx \u2212\nu dv = uv \u2212 -\n*\n\n(\u03bbx)n\u22121\n-\n(n \u2212 1)!\n\n0\n\n0\n\nt\n\nbut the integration-by-parts formula\ndv = \u03bb[(\u03bbx)n\u22121/(n \u2212 1)!] dx yields\n\n*\n\n\u2212\u03bbx\n\n\u03bbe\n\nt\n\n0\n\n(\u03bbx)n\u22121\n(n \u2212 1)!\n\ndx = e\n\n\u2212\u03bbt\n\n+\n\n(\u03bbt)n\nn!\n\nt\n\n0\n\n\u2212\u03bbx\n\n\u03bbe\n\n(\u03bbx)n\nn!\n\ndx\n\n\u2212\u03bbx\n\n(\u03bbx)n\nn!\n\ndx\n\n\u03bbe\nv du with u = e\n\n\u2212\u03bbx and\n\nwhich completes the proof.\n\n9.2 markov chains\n\nconsider a sequence of random variables x0, x1, . . . , and suppose that the set of pos-\nsible values of these random variables is {0, 1, . . . , m}. it will be helpful to interpret\nxn as being the state of some system at time n, and, in accordance with this interpre-\ntation, we say that the system is in state i at time n if xn = i. the sequence of random\nvariables is said to form a markov chain if, each time the system is in state i, there is\n\n "}, {"Page_number": 435, "text": "420\n\nchapter 9\n\nadditional topics in probability\n\nsome fixed probability\u2014call it pij\u2014that the system will next be in state j. that is, for\nall i0, . . . , in\u22121, i, j,\n\np{xn+1 = j|xn = i, xn\u22121 = in\u22121, . . . , x1 = i1, x0 = i0} = pij\n\nthe values pij, 0 \u2026 i \u2026 m, 0 \u2026 j \u2026 n, are called the transition probabilities of the\nmarkov chain, and they satisfy\n\n(why?) it is convenient to arrange the transition probabilities pij in a square array as\nfollows:\n\npij \u00fa 0\n\npij = 1\n\ni = 0, 1, . . . , m\n\nm(cid:6)\n\nj=0\n\ndddddddddddd\n\n\u00b7\u00b7\u00b7 p0m\n\u00b7\u00b7\u00b7 p1m\n\np00 p01\np10 p11\n#\n#\n#\npm0 pm1 \u00b7\u00b7\u00b7 pmm\n\ndddddddddddd\n\nsuch an array is called a matrix.\n\nknowledge of the transition probability matrix and of the distribution of x0 enables\nus, in theory, to compute all probabilities of interest. for instance, the joint probabil-\nity mass function of x0, . . . , xn is given by\n\np{xn = in, xn\u22121 = in\u22121, . . . , x1 = i1, x0 = i0}\n= p{xn=in|xn\u22121 = in\u22121, . . . , x0=i0}p{xn\u22121 = in\u22121, . . . , x0 = i0}\n= pin\u22121, inp{xn\u22121 = in\u22121, . . . , x0 = i0}\n\nand continual repetition of this argument demonstrates that the preceding is equal to\n\npin\u22121, inpin\u22122, in\u22121\n\n\u00b7\u00b7\u00b7 pi1, i2pi0, i1p{x0 = i0}\n\nexample 2a\nsuppose that whether it rains tomorrow depends on previous weather conditions only\nthrough whether it is raining today. suppose further that if it is raining today, then it\nwill rain tomorrow with probability \u03b1, and if it is not raining today, then it will rain\ntomorrow with probability \u03b2.\n\nif we say that the system is in state 0 when it rains and state 1 when it does not,\nthen the preceding system is a two-state markov chain having transition probability\nmatrix\n\ndddd \u03b1 1 \u2212 \u03b1\n\ndddd\n\n\u03b2 1 \u2212 \u03b2\nthat is, p00 = \u03b1 = 1 \u2212 p01, p10 = \u03b2 = 1 \u2212 p11.\n\n.\n\nexample 2b\nconsider a gambler who either wins 1 unit with probability p or loses 1 unit with\nprobability 1 \u2212 p at each play of the game. if we suppose that the gambler will quit\n\n "}, {"Page_number": 436, "text": "playing when his fortune hits either 0 or m, then the gambler\u2019s sequence of fortunes\nis a markov chain having transition probabilities\n\nsection 9.2\n\nmarkov chains 421\n\npi,i+1 = p = 1 \u2212 pi,i\u22121\np00 = pmm = 1\n\ni = 1, . . . , m \u2212 1\n\nexample 2c\nthe husband-and-wife physicists paul and tatyana ehrenfest considered a concep-\ntual model for the movement of molecules in which m molecules are distributed\namong 2 urns. at each time point one of the molecules is chosen at random and\nis removed from its urn and placed in the other one. if we let xn denote the number\nof molecules in the first urn immediately after the nth exchange, then {x0, x1, . . .} is\na markov chain with transition probabilities\n\n0 \u2026 i \u2026 m\n\npi,i+1 = m \u2212 i\npi,i\u22121 = i\nm\n\nm\n\n0 \u2026 i \u2026 m\npij = 0 if |j \u2212 i| > 1\n\n.\n\n.\n\nthus, for a markov chain, pij represents the probability that a system in state i\nwill enter state j at the next transition. we can also define the two-stage transition\nprobability p(2)\nthat a system presently in state i will be in state j after two additional\nij\ntransitions. that is,\n\n= p{xm+2 = j|xm = i}\n\np(2)\nij\n\nthe p(2)\n\nij can be computed from the pij as follows:\n\n= p{x2 = j|x0 = i}\n\np(2)\nij\n\nk=0\n\n= m(cid:6)\n= m(cid:6)\n= m(cid:6)\n\nk=0\n\nk=0\n\np{x2 = j, x1 = k|x0 = i}\n\np{x2 = j|x1 = k, x0 = i}p{x1 = k|x0 = i}\n\npkjpik\n\nin general, we define the n-stage transition probabilities, denoted as p(n)\nij\n\n, by\n\n= p{xn+m = j|xm = i}\n\np(n)\nij\n\nproposition 2.1, known as the chapman\u2013kolmogorov equations, shows how the p(n)\nij\ncan be computed.\nproposition 2.1. the chapman\u2013kolmogorov equations\n\n= m(cid:6)\n\nk=0\n\np(n)\nij\n\nik p(n\u2212r)\np(r)\n\nkj\n\nfor all 0 < r < n\n\n "}, {"Page_number": 437, "text": "422\n\nchapter 9\n\nadditional topics in probability\n\nproof.\n\np(n)\nij\n\n(cid:6)\n= p{xn = j|x0 = i}\n=\n(cid:6)\n(cid:6)\n\np{xn = j, xr = k|x0 = i}\np{xn = j|xr = k, x0 = i}p{xr = k|x0 = i}\np(n\u2212r)\nkj p(r)\nik\n\n=\n\n=\n\nk\n\nk\n\nk\n\nexample 2d a random walk\nan example of a markov chain having a countably infinite state space is the random\nwalk, which tracks a particle as it moves along a one-dimensional axis. suppose that\nat each point in time the particle will move either one step to the right or one step to\nthe left with respective probabilities p and 1 \u2212 p. that is, suppose the particle\u2019s path\nfollows a markov chain with transition probabilities\n\npi, i+1 = p = 1 \u2212 pi, i\u22121\n\ni = 0, ;1, . . .\n\n=\n\n(cid:2)\n\npn\nij\n\n(cid:3)\n\nn\nx\n\n(cid:2)\n(cid:2)\n\nif the particle is at state i, then the probability that it will be at state j after n transitions\nis the probability that (n \u2212 i + j)/2 of these steps are to the right and n \u2212 [(n \u2212 i +\nj)/2] = (n + i \u2212 j)/2 are to the left. since each step will be to the right, independently\nof the other steps, with probability p, it follows that the above is just the binomial\nprobability\n\n(cid:3)\n\n(cid:2)\n\nn\n\n(n \u2212 i + j)/2\n\np(n\u2212i+j)/2(1 \u2212 p)(n+i\u2212j)/2\n\nwhere\nto n. the preceding formula can be rewritten as\n\nis taken to equal 0 when x is not a nonnegative integer less than or equal\n\np2n\ni,i+2k\np2n+1\ni,i+2k+1\n\n=\n\n=\n\n2n\nn + k\n2n + 1\nn + k + 1\n\n(cid:3)\npn+k(1 \u2212 p)n\u2212k k = 0, ;1, . . . , ;n\n\npn+k+1(1 \u2212 p)n\u2212k\n\nk = 0, ;1, . . . , ;n,\u2212(n + 1)\n\n.\n\nalthough the p(n)\n\nij denote conditional probabilities, we can use them to derive\nexpressions for unconditional probabilities by conditioning on the initial state. for\ninstance,\n\np{xn = j} =\n=\n\np{xn = j|x0 = i}p{x0 = i}\nij p{x0 = i}\np(n)\n\nconverges, as n\u2192q, to a\nfor a large number of markov chains, it turns out that p(n)\nij\nvalue \u03c0j that depends only on j. that is, for large values of n, the probability of being\n\ni\n\n(cid:3)\n\n(cid:6)\n(cid:6)\n\ni\n\n "}, {"Page_number": 438, "text": "section 9.2\n\nmarkov chains 423\n\nin state j after n transitions is approximately equal to \u03c0j, no matter what the initial\nstate was. it can be shown that a sufficient condition for a markov chain to possess\nthis property is that, for some n > 0,\n\np(n)\nij\n\n> 0\n\nfor all i, j = 0, 1, . . . , m\n\n(2.1)\n\nmarkov chains that satisfy equation (2.1) are said to be ergodic. since proposition 2.1\nyields\n\np(n)\nik pkj\nit follows, by letting n\u2192q, that, for ergodic chains,\n\nij\n\np(n+1)\n\nk=0\n\n= m(cid:6)\n\u03c0j = m(cid:6)\n\nk=0\n\n\u03c0kpkj\n\nfurthermore, since 1 = m(cid:9)\n\nj=0\n\np(n)\nij\n\n, we also obtain, by letting n\u2192q,\n\nm(cid:6)\n\nj=0\n\n\u03c0j = 1\n\n(2.2)\n\n(2.3)\n\nin fact, it can be shown that the \u03c0j, 0 \u2026 j \u2026 m, are the unique nonnegative solutions\nof equations (2.2) and (2.3). all this is summed up in theorem 2.1, which we state\nwithout proof.\n\ntheorem 2.1. for an ergodic markov chain,\n\u03c0j = lim\n\nn\u2192q p(n)\n\nij\n\nexists, and the \u03c0j, 0 \u2026 j \u2026 m, are the unique nonnegative solutions of\n\n\u03c0j = m(cid:6)\n\nk=0\n\u03c0j = 1\n\nm(cid:6)\n\nj=0\n\n\u03c0kpkj\n\nexample 2e\nconsider example 2a, in which we assume that if it rains today, then it will rain tomor-\nrow with probability \u03b1, and if it does not rain today, then it will rain tomorrow with\nprobability \u03b2. from theorem 2.1, it follows that the limiting probabilities \u03c00 and \u03c01\nof rain and of no rain, respectively, are given by\n\u03c00 = \u03b1\u03c00 + \u03b2\u03c01\n\u03c01 = (1 \u2212 \u03b1)\u03c00 + (1 \u2212 \u03b2)\u03c01\n\n\u03c00 + \u03c01 = 1\n\n "}, {"Page_number": 439, "text": "424\n\nchapter 9\n\nadditional topics in probability\n\nwhich yields\n\n\u03c00 =\n\n\u03b2\n\n1 + \u03b2 \u2212 \u03b1\n\n\u03c01 = 1 \u2212 \u03b1\n1 + \u03b2 \u2212 \u03b1\n\nfor instance, if \u03b1 = .6 and \u03b2 = .3, then the limiting probability of rain on the nth day\nis \u03c00 = 3\n.\n7.\n\nthe quantity \u03c0j is also equal to the long-run proportion of time that the markov\nchain is in state j, j = 0, . . . , m. to see intuitively why this might be so, let pj denote\nthe long-run proportion of time the chain is in state j. (it can be proven using the\nstrong law of large numbers that, for an ergodic chain, such long-run proportions\nexist and are constants.) now, since the proportion of time the chain is in state k is\npk, and since, when in state k, the chain goes to state j with probability pkj, it follows\nthat the proportion of time the markov chain is entering state j from state k is equal\nto pkpkj. summing over all k shows that pj, the proportion of time the markov chain\nis entering state j, satisfies\n\n(cid:6)\n\npj =\n(cid:6)\n\npkpkj\n\nk\n\npj = 1\n\nsince clearly it is also true that\n\nit thus follows, since by theorem 2.1 the \u03c0j, j = 0, . . . , m are the unique solution of\nthe preceding, that pj = \u03c0j, j = 0, . . . , m. the long-run proportion interpretation of \u03c0j\nis generally valid even when the chain is not ergodic.\n\nj\n\nexample 2f\nsuppose in example 2c that we are interested in the proportion of time that there are\nj molecules in urn 1, j = 0, . . . , m. by theorem 2.1, these quantities will be the unique\nsolution of\n\n\u03c00 = \u03c01 * 1\nm\n\u03c0j = \u03c0j\u22121 * m \u2212 j + 1\n\u03c0m = \u03c0m\u22121 * 1\nm(cid:6)\nm\n\u03c0j = 1\n\nm\n\nj=0\n\n+ \u03c0j+1 * j + 1\n\nm\n\nj = 1, . . . , m\n\nhowever, as it is easily checked that\n\n(cid:2)\n\n\u03c0j =\n\nm\nj\n\n(cid:3)(cid:2)\n\n(cid:3)m\n\n1\n2\n\nj = 0, . . . , m\n\nsatisfy the preceding equations, it follows that these are the long-run proportions of\ntime that the markov chain is in each of the states. (see problem 11 for an explanation\n.\nof how one might have guessed at the foregoing solution.)\n\n "}, {"Page_number": 440, "text": "9.3 surprise, uncertainty, and entropy\n\nsection 9.3\n\nsurprise, uncertainty, and entropy 425\n\nconsider an event e that can occur when an experiment is performed. how surprised\nwould we be to hear that e does, in fact, occur? it seems reasonable to suppose that\nthe amount of surprise engendered by the information that e has occurred should\ndepend on the probability of e. for instance, if the experiment consists of rolling a\npair of dice, then we would not be too surprised to hear that e has occurred when\ne represents the event that the sum of the dice is even (and thus has probability 1\n2),\nwhereas we would certainly be more surprised to hear that e has occurred when e is\nthe event that the sum of the dice is 12 (and thus has probability 1\n\nin this section, we attempt to quantify the concept of surprise. to begin, let us\nagree to suppose that the surprise one feels upon learning that an event e has occurred\ndepends only on the probability of e, and let us denote by s(p) the surprise evoked\nby the occurrence of an event having probability p. we determine the functional form\nof s(p) by first agreeing on a set of reasonable conditions that s(p) should satisfy and\nthen proving that these axioms require that s(p) have a specified form. we assume\nthroughout that s(p) is defined for all 0 < p \u2026 1, but is not defined for events having\np = 0.\n\nour first condition is just a statement of the intuitive fact that there is no surprise\n\n36).\n\nin hearing that an event which is sure to occur has indeed occurred.\n\naxiom 1\n\ns(1) = 0\n\nour second condition states that the more unlikely an event is to occur, the greater\n\nis the surprise evoked by its occurrence.\n\naxiom 2\ns(p) is a strictly decreasing function of p; that is, if p < q, then s(p) > s(q).\nthe third condition is a mathematical statement of the fact that we would intu-\n\nitively expect a small change in p to correspond to a small change in s(p).\n\naxiom 3\ns(p) is a continuous function of p.\nto motivate the final condition, consider two independent events e and f having\nrespective probabilities p(e) = p and p(f) = q. since p(ef) = pq, the surprise\nevoked by the information that both e and f have occurred is s(pq). now, suppose\nthat we are told first that e has occurred and then, afterward, that f has also occurred.\nsince s(p) is the surprise evoked by the occurrence of e, it follows that s(pq) \u2212\ns(p) represents the additional surprise evoked when we are informed that f has also\noccurred. however, because f is independent of e, the knowledge that e occurred\ndoes not change the probability of f; hence, the additional surprise should just be\ns(q). this reasoning suggests the final condition.\n\naxiom 4\n\ns(pq) = s(p) + s(q)\n\n0 < p \u2026 1,\n\n0 < q \u2026 1\n\nwe are now ready for theorem 3.1, which yields the structure of s(p).\n\ntheorem 3.1. if s(\u00b7) satisfies axioms 1 through 4, then\n\ns(p) = \u2212c log2 p\n\nwhere c is an arbitrary positive integer.\n\n "}, {"Page_number": 441, "text": "426\n\nchapter 9\n\nadditional topics in probability\n\nproof. it follows from axiom 4 that\n\ns(p2) = s(p) + s(p) = 2s(p)\n\nand by induction that\n\ns(pm) = ms(p)\n\n(3.1)\n\nalso, since, for any integral n, s(p) = s(p1/n \u00b7\u00b7\u00b7 p1/n) = n s(p1/n), it follows that\n\ns(p1/n) = 1\nn\n\ns(p)\n\nthus, from equations (3.1) and (3.2), we obtain\n\nwhich is equivalent to\n\ns(pm/n) = ms(p1/n)\ns(p)\n\n= m\nn\n\ns(px) = xs(p)\n\n(3.2)\n\n(3.3)\n\nwhenever x is a positive rational number. but by the continuity of s (axiom 3), it\nfollows that equation (3.3) is valid for all nonnegative x. (reason this out.)\n\nnow, for any p, 0 < p \u2026 1, let x = \u2212 log2 p. then p =\n\n, and from equa-\n\n(cid:29)\n\n(cid:30)x\n\n1\n2\n\ntion (3.3),\n\n(cid:30)\n\n(cid:29)\n\n1\n2\n\nwhere c = s\n\n(cid:4)(cid:2)\n\n(cid:5)\n\n(cid:3)x\n\n(cid:2)\n\n(cid:3)\n\n1\n2\n\n= xs\n\ns(p) = s\n\n1\n2\n> s(1) = 0 by axioms 2 and 1.\n\n= \u2212c log2 p\n\nit is usual to let c equal 1, in which case the surprise is said to be expressed in units\n\nof bits (short for binary digits).\nnext, consider a random variable x that must take on one of the values x1, . . . , xn\nwith respective probabilities p1, . . . , pn. since \u2212 log pi represents the surprise evoked\nif x takes on the value xi,\u2020 it follows that the expected amount of surprise we shall\nreceive upon learning the value of x is given by\n\nh(x) = \u2212 n(cid:6)\n\npi log pi\n\ni=1\n\nthe quantity h(x) is known in information theory as the entropy of the random\nvariable x. (in case one of the pi = 0, we take 0 log 0 to equal 0.) it can be shown\n(and we leave it as an exercise) that h(x) is maximized when all of the pi are equal.\n(is this intuitive?)\n\nsince h(x) represents the average amount of surprise one receives upon learning\nthe value of x, it can also be interpreted as representing the amount of uncertainty\nthat exists as to the value of x. in fact, in information theory, h(x) is interpreted as\nthe average amount of information received when the value of x is observed. thus,\nthe average surprise evoked by x, the uncertainty of x, or the average amount of\n\n\u2020for the remainder of this chapter, we write log x for log2 x. also, we use ln x for loge x.\n\n "}, {"Page_number": 442, "text": "(cid:6)\n\n(cid:6)\n\n(cid:6)\n\ni\n\n(cid:6)\n\nsection 9.3\n\nsurprise, uncertainty, and entropy 427\n\ninformation yielded by x all represent the same concept viewed from three slightly\ndifferent points of view.\n\nnow consider two random variables x and y that take on the respective values\n\nx1, . . . , xn and y1, . . . , ym with joint mass function\n\np(xi, yj) = p{x = xi, y = yj}\n\nit follows that the uncertainty as to the value of the random vector (x, y), denoted\nby h(x, y), is given by\n\nh(x, y) = \u2212\n\np(xi, yj) log p(xi, yj)\n\ni\n\nj\n\nsuppose now that y is observed to equal yj. in this situation, the amount of uncer-\ntainty remaining in x is given by\n\nhy=yj\n\n(x) = \u2212\n\np(xi|yj) log p(xi|yj)\n\nwhere\n\np(xi|yj) = p{x = xi|y = yj}\n\nhence, the average amount of uncertainty that will remain in x after y is observed\nis given by\n\nhy (x) =\n\nhy=yj\n\n(x)py (yj)\n\nwhere\n\nj\n\npy (yj) = p{y = yj}\n\nproposition 3.1 relates h(x, y) to h(y) and hy (x). it states that the uncertainty as\nto the value of x and y is equal to the uncertainty of y plus the average uncertainty\nremaining in x when y is to be observed.\nproposition 3.1.\n\nproof. using the identity p(xi, yj) = py (yj)p(xi|yj) yields\n\nh(x, y) = h(y) + hy (x)\n(cid:6)\n(cid:6)\n(cid:6)\n(cid:6)\n(cid:6)\n(cid:6)\n\nj\npy (yj) log py (yj)\n\n(cid:6)\n\n(cid:6)\n\ni\n\nj\n\ni\n\nj\n\np(xi|yj)\np(xi|yj) log p(xi|yj)\n\ni\n\npy (yj)\n\np(xi, yj) log p(xi, yj)\npy (yj)p(xi|yj)[log py (yj) + log p(xi|yj)]\n\nh(x, y) = \u2212\n= \u2212\n\n= \u2212\n\n\u2212\n\nj\n\n= h(y) + hy (x)\n\ni\n\nit is a fundamental result in information theory that the amount of uncertainty in\na random variable x will, on the average, decrease when a second random variable\ny is observed. before proving this statement, we need the following lemma, whose\nproof is left as an exercise.\n\n "}, {"Page_number": 443, "text": "428\n\nchapter 9\n\nadditional topics in probability\n\nlemma 3.1\n\nwith equality only at x = 1.\n\nln x \u2026 x \u2212 1\n\nx > 0\n\ntheorem 3.2.\n\nj\n\ni\n\nproof.\n\nhy (x) \u2026 h(x)\nwith equality if and only if x and y are independent.\n(cid:6)\n(cid:6)\np(xi|yj) log[p(xi|yj)]p(yj)\nhy (x) \u2212 h(x) = \u2212\n(cid:6)\n(cid:6)\n+\n(cid:6)\n(cid:6)\n(cid:6)\n(cid:6)\n\u23a1\n(cid:6)\n\u23a3(cid:6)\n\np(xi)\n(cid:7)\np(xi|yj)\np(xi)\np(xi|yj)\n(cid:6)\n\np(xi, yj) log p(xi)\n\np(xi, yj) log\n\n\u2026 log e\n\np(xi, yj)\n\n(cid:8)\n\n(cid:7)\n\n=\n\ni\n\nj\n\ni\n\nj\n\ni\n\nj\n\np(xi)p(yj) \u2212\n\n(cid:8)\n\u2212 1\n(cid:6)\n\n= log e\n= log e[1 \u2212 1]\n= 0\n\ni\n\nj\n\nby lemma 3.1\n\n\u23a4\n\u23a6\n\np(xi, yj)\n\ni\n\nj\n\n9.4 coding theory and entropy\n\nsuppose that the value of a discrete random vector x is to be observed at location\na and then transmitted to location b via a communication network that consists of\ntwo signals, 0 and 1. in order to do this, it is first necessary to encode each possible\nvalue of x in terms of a sequence of 0\u2019s and 1\u2019s. to avoid any ambiguity, it is usually\nrequired that no encoded sequence can be obtained from a shorter encoded sequence\nby adding more terms to the shorter.\n\nfor instance, if x can take on four possible values x1, x2, x3, and x4, then one pos-\n\nsible coding would be\n\nthat is, if x = x1, then the message 00 is sent to location b, whereas if x = x2, then\n01 is sent to b, and so on. a second possible coding is\n\n(4.1)\n\n(4.2)\n\nx1 % 00\nx2 % 01\nx3 % 10\nx4 % 11\n\nx1 % 0\nx2 % 10\nx3 % 110\nx4 % 111\n\n "}, {"Page_number": 444, "text": "section 9.4\n\ncoding theory and entropy 429\n\nhowever, a coding such as\n\nx1 % 0\nx2 % 1\nx3 % 00\nx4 % 01\n\nis not allowed because the coded sequences for x3 and x4 are both extensions of the\none for x1.\n\none of the objectives in devising a code is to minimize the expected number of\nbits (that is, binary digits) that need to be sent from location a to location b. for\nexample, if\n\np{x = x1} = 1\n2\np{x = x2} = 1\n4\np{x = x3} = 1\n8\np{x = x4} = 1\n8\n\n(3) +\nthen the code given by equation (4.2) would expect to send 1\n(3) = 1.75 bits, whereas the code given by equation (4.1) would expect to send 2\n2\n1\n8\nbits. hence, for the preceding set of probabilities, the encoding in equation (4.2) is\nmore efficient than that in equation (4.1).\n\n(2) + 1\n\n(1) + 1\n\nthe preceding discussion raises the following question: for a given random\nvector x, what is the maximum efficiency achievable by an encoding scheme? the\nanswer is that, for any coding, the average number of bits that will be sent is at least\nas large as the entropy of x. to prove this result, known in information theory as the\nnoiseless coding theorem, we shall need lemma 4.1.\n\n4\n\n8\n\nlemma 4.1\nlet x take on the possible values x1, . . . , xn. then, in order to be able to encode\nthe values of x in binary sequences (none of which is an extension of another) of\nrespective lengths n1, . . . , nn, it is necessary and sufficient that\n\n(cid:3)ni \u2026 1\n\n(cid:2)\n\nn(cid:6)\n\ni=1\n\n1\n2\n\nproof. for a fixed set of n positive integers n1, . . . , nn, let wj denote the number\nof the ni that are equal to j, j = 1, . . . . for there to be a coding that assigns ni\nbits to the value xi, i = 1, . . . , n, it is clearly necessary that w1 \u2026 2. furthermore,\nbecause no binary sequence is allowed to be an extension of any other, we must\nhave w2 \u2026 22 \u2212 2w1. (this follows because 22 is the number of binary sequences\nof length 2, whereas 2w1 is the number of sequences that are extensions of the\nw1 binary sequence of length 1.) in general, the same reasoning shows that we\nmust have\n\nwn \u2026 2n \u2212 w12n\u22121 \u2212 w22n\u22122 \u2212 \u00b7\u00b7\u00b7 \u2212 wn\u221212\n\n(4.3)\n\n "}, {"Page_number": 445, "text": "430\n\nchapter 9\n\nadditional topics in probability\n\nfor n = 1, . . . . in fact, a little thought should convince the reader that these con-\nditions are not only necessary, but also sufficient for a code to exist that assigns ni\nbits to xi, i = 1, . . . , n.\n\nrewriting inequality (4.3) as\n\nwn + wn\u221212 + wn\u2212222 + \u00b7\u00b7\u00b7 + w12n\u22121 \u2026 2n n = 1, . . .\nand dividing by 2n yields the necessary and sufficient conditions, namely,\n\nn(cid:9)\n\nhowever, because\n\nj=1\nbe true if and only if\n\n(cid:29)\n\n1\n2\n\nwj\n\n\u2026 1\n\nfor all n\n\n(4.4)\n\nis increasing in n, it follows that equation (4.4) will\n\n(cid:2)\n\n(cid:3)j\n\n1\n2\n\nwj\n\nn(cid:6)\n(cid:30)j\n\nj=1\n\n(cid:2)\n\n(cid:3)j\n\n1\n2\n\nq(cid:6)\n\nwj\n\nj=1\n\n\u2026 1\n\nthe result is now established, since, by the definition of wj as the number of ni that\nequal j, it follows that\n\n(cid:2)\n\n1\n2\n\nq(cid:6)\n\nwj\n\nj=1\n\n(cid:2)\n\n(cid:3)j = n(cid:6)\n\ni=1\n\n(cid:3)ni\n\n1\n2\n\nwe are now ready to prove theorem 4.1.\n\ntheorem 4.1 the noiseless coding theorem\nlet x take on the values x1, . . . , xn with respective probabilities p(x1), . . . , p(xn).\nthen, for any coding of x that assigns ni bits to xi,\n\nproof. let pi = p(xi), qi = 2\n(cid:3)\n\n(cid:2)\n\n\u2212 n(cid:6)\n\ni=1\n\npi log\n\npi\nqi\n\nnip(xi) \u00fa h(x) = \u2212 n(cid:6)\n\nn(cid:6)\n\ni=1\n\n(cid:26) n(cid:9)\n\np(xi) log p(xi)\n\ni=1\n\n\u2212ni\n\nj=1\n\n(cid:3)\n\n(cid:2)\n\n\u2212nj, i = 1, . . . , n. then\n2\nn(cid:6)\n= \u2212 log e\nn(cid:6)\nn(cid:6)\n\n= log e\n\npi ln\n\npi ln\n\npi\nqi\n\n(cid:2)\n\n(cid:3)\n\ni=1\n\ni=1\n\nqi\npi\n\u2212 1\n\n(cid:3)\n(cid:2)\npi = n(cid:6)\nn(cid:6)\n\nqi\npi\n\npi\n\ni=1\n\ni=1\n\nqi = 1\n\n\u2026 log e\n\ni=1\n= 0 since\n\nby lemma 3.1\n\n "}, {"Page_number": 446, "text": "section 9.4\n\ncoding theory and entropy 431\n\nhence,\n\n\u2212 n(cid:6)\n\ni=1\n\npi log qi\n\npi log pi \u2026 \u2212 n(cid:6)\n= n(cid:6)\ni=1\nnipi + log\nn(cid:6)\n\ni=1\n\n\u239e\n\u239f\u23a0\n\n\u2212nj\n2\n\n\u239b\n\u239c\u239d n(cid:6)\n\nj=1\n\n\u2026\n\nnipi by lemma 4.1\n\ni=1\n\nexample 4a\nconsider a random variable x with probability mass function\np(x3) = p(x4) = 1\n8\n(cid:21)\n\np(x1) = 1\n2\n\np(x2) = 1\n4\n\nsince\n\n(cid:20)\n\n+ 1\n4\n\nlog\n\n1\n4\n\n+ 1\n4\n\nlog\n\n1\n8\n\nh(x) = \u2212\n= 1\n2\n= 1.75\n\n1\nlog\n2\n+ 2\n4\n\n1\n2\n+ 3\n4\n\nit follows from theorem 4.1 that there is no more efficient coding scheme than\n\nx1 % 0\nx2 % 10\nx3 % 110\nx4 % 111\n\n.\n\nfor most random vectors, there does not exist a coding for which the average\nnumber of bits sent attains the lower bound h(x). however, it is always possible\nto devise a code such that the average number of bits is within 1 of h(x). to prove\nthis, define ni to be the integer satisfying\n\nnow,\n\n\u2212 log p(xi) \u2026 ni < \u2212 log p(xi) + 1\nn(cid:6)\n\n2log p(xi) = n(cid:6)\n\n\u2212ni \u2026\n2\n\nn(cid:6)\n\np(xi) = 1\n\ni=1\n\ni=1\n\ni=1\n\nso, by lemma 4.1, we can associate sequences of bits having lengths ni with the xi, i =\n1, . . . , n. the average length of such a sequence,\n\nl = n(cid:6)\n\ni=1\n\nni p(xi)\n\n "}, {"Page_number": 447, "text": "432\n\nchapter 9\n\nadditional topics in probability\n\nsatisfies\n\nor\n\n\u2212 n(cid:6)\n\np(xi) log p(xi) \u2026 l < \u2212 n(cid:6)\n\ni=1\n\np(xi) log p(xi) + 1\n\ni=1\n\nh(x) \u2026 l < h(x) + 1\n\nexample 4b\nsuppose that 10 independent tosses of a coin having probability p of coming up heads\nare made at location a and the result is to be transmitted to location b. the outcome\nof this experiment is a random vector x = (x1, . . . , x10), where xi is 1 or 0 according\nto whether or not the outcome of the ith toss is heads. by the results of this section, it\nfollows that l, the average number of bits transmitted by any code, satisfies\n\nwith\n\nh(x) \u2026 l\n\nl \u2026 h(x) + 1\n\nfor at least one code. now, since the xi are independent, it follows from proposi-\ntion 3.1 and theorem 3.2 that\n\nh(x) = h(x1, . . . , xn) = n(cid:6)\n\nh(xi)\n\ni=1\n\n= \u221210[p log p + (1 \u2212 p) log(1 \u2212 p)]\n\nif p = 1\n2, then h(x) = 10, and it follows that we can do no better than just encoding\nx by its actual value. for example, if the first 5 tosses come up heads and the last 5\ntails, then the message 1111100000 is transmitted to location b.\ninstance, if p = 1\n\n2, we can often do better by using a different coding scheme. for\n\nhowever, if p z 1\n\n4, then\n\n(cid:2)\n\n(cid:3)\n\nh(x) = \u221210\n\n1\n4\n\nlog\n\n1\n4\n\n+ 3\n4\n\nlog\n\n3\n4\n\n= 8.11\n\nthus, there is an encoding for which the average length of the encoded message is no\ngreater than 9.11.\none simple coding that is more efficient in this case than the identity code is to\nbreak up (x1, . . . , x10) into 5 pairs of 2 random variables each and then, for i =\n1, 3, 5, 7, 9, code each of the pairs as follows:\n\nxi = 0, xi+1 = 0 % 0\nxi = 0, xi+1 = 1 % 10\nxi = 1, xi+1 = 0 % 110\nxi = 1, xi+1 = 1 % 111\n\nthe total message transmitted is the successive encodings of the preceding pairs.\n\nfor instance, if the outcome ttthhtttth is observed, then the message 01011\n0010 is sent. the average number of bits needed to transmit the message with this\ncode is\n\n "}, {"Page_number": 448, "text": "(cid:7)\n\n(cid:2)\n\n5\n\n1\n\n3\n4\n\n(cid:2)\n\n(cid:3)2 + 2\n\n1\n4\n\n(cid:3)(cid:2)\n\n3\n4\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n(cid:8)\n\n(cid:3)2\n\n+ 3\n\n1\n4\n\n3\n4\n\n+ 3\n\n1\n4\n\nsection 9.4\n\ncoding theory and entropy 433\n\n= 135\n16\nl 8.44\n\n.\n\nup to this point, we have assumed that the message sent at location a is received\nwithout error at location b. however, there are always certain errors that can occur\nbecause of random disturbances along the communications channel. such random\ndisturbances might lead, for example, to the message 00101101, sent at a, being\nreceived at b in the form 01101101.\n\nlet us suppose that a bit transmitted at location a will be correctly received at\nlocation b with probability p, independently from bit to bit. such a communications\nsystem is called a binary symmetric channel. suppose further that p = .8 and we\nwant to transmit a message consisting of a large number of bits from a to b. thus,\ndirect transmission of the message will result in an error probability of .20 for each\nbit, which is quite high. one way to reduce this probability of bit error would be to\ntransmit each bit 3 times and then decode by majority rule. that is, we could use the\nfollowing scheme:\n\nencode decode\n\nencode decode\n\n\u23ab\u23aa\u23aa\u23ac\n\u23aa\u23aa\u23ad\u21920\n\n0\u2192000\n\n000\n001\n010\n100\n\n\u23ab\u23aa\u23aa\u23ac\n\u23aa\u23aa\u23ad\u21921\n\n1\u2192111\n\n111\n110\n101\n011\n\nnote that if no more than one error occurs in transmission, then the bit will be\n\ncorrectly decoded. hence, the probability of bit error is reduced to\n\n(.2)3 + 3(.2)2(.8) = .104\n\na considerable improvement. in fact, it is clear that we can make the probability of\nbit error as small as we want by repeating the bit many times and then decoding by\nmajority rule. for instance, the scheme\n\ndecode\n\nencode\n0\u2192string of 17 0\u2019s by majority rule\n1\u2192string of 17 1\u2019s\n\nwill reduce the probability of bit error to below .01.\n\nthe problem with this type of encoding scheme is that, although it decreases the\nprobability of bit error, it does so at the cost of also decreasing the effective rate of\nbits sent per signal. (see table 9.1.)\n\nin fact, at this point it may appear inevitable to the reader that decreasing the\nprobability of bit error to 0 always results in also decreasing the effective rate at which\nbits are transmitted per signal to 0. however, a remarkable result of information\ntheory known as the noisy coding theorem and due to claude shannon demonstrates\nthat this is not the case. we now state this result as theorem 4.2.\n\n "}, {"Page_number": 449, "text": "434\n\nchapter 9\n\nadditional topics in probability\n\ntable 9.1: repetition of bits encoding scheme\n\nprobability of error rate\n(per bit)\n\n(bits transmitted per signal)\n\n.20\n.10\n\n.01\n\n1\n.33\n\n.06\n\n(cid:30)\n(cid:29)\n(cid:30)\n(cid:29)\n= 1\n3\n= 1\n17\n\ntheorem 4.2 the noisy coding theorem\nthere is a number c such that, for any value r which is less than c, and for any \u03b5 >0,\nthere exists a coding\u2013decoding scheme that transmits at an average rate of r bits sent\nper signal and with an error (per bit) probability of less than \u03b5. the largest such\n\u2217\u2020 \u2014is called the channel capacity, and for the binary symmetric\nvalue of c\u2014call it c\nchannel,\n\u2217 = 1 + p log p + (1 \u2212 p) log(1 \u2212 p)\n\nc\n\nsummary\nthe poisson process having rate \u03bb is a collection of random variables {n(t), t \u00fa 0} that\nrelate to an underlying process of randomly occurring events. for instance, n(t) rep-\nresents the number of events that occur between times 0 and t. the defining features\nof the poisson process are as follows:\n\n(i) the number of events that occur in disjoint time intervals are independent.\n(ii) the distribution of the number of events that occur in an interval depends only\n\non the length of the interval.\n(iii) events occur one at a time.\n(iv) events occur at rate \u03bb.\nit can be shown that n(t) is a poisson random variable with mean \u03bbt. in addition,\nif ti, i \u00fa 1, are the times between the successive events, then they are independent\nexponential random variables with rate \u03bb.\na sequence of random variables xn, n \u00fa 0, each of which takes on one of the\nvalues 0, . . . , m, is said to be a markov chain with transition probabilities pi, j if, for\nall n, i0, . . . , in, i, j,\n\np{xn+1 = j|xn = i, xn\u22121 = in\u22121, . . . , x0 = i0} = pi, j\n\nif we interpret xn as the state of some process at time n, then a markov chain is a\nsequence of successive states of a process which has the property that whenever it\nenters state i, then, independently of all past states, the next state is j with probability\npi,j, for all states i and j. for many markov chains, the probability of being in state j at\ntime n converges to a limiting value that does not depend on the initial state. if we let\n\u03c0j, j = 0, . . . , m, denote these limiting probabilities, then they are the unique solution\nof the equations\n\n\u03c0ipi, j\n\nj = 0, . . . , m\n\n\u03c0j = m(cid:6)\n\ni=0\n\u03c0j = 1\n\nm(cid:6)\n\nj=1\n\n\u2020for an entropy interpretation of c\n\n\u2217\n\n, see theoretical exercise 9.18.\n\n "}, {"Page_number": 450, "text": "problems and theoretical exercises 435\n\nmoreover, \u03c0j is equal to the long-run proportion of time that the chain is in state j.\nset of probabilities {p1, . . . , pn}. the quantity\n\nlet x be a random variable that takes on one of n possible values according to the\n\nh(x) = \u2212 n(cid:6)\n\ni=1\n\npi log2\n\n(pi)\n\nis called the entropy of x. it can be interpreted as representing either the average\namount of uncertainty that exists regarding the value of x or the average information\nreceived when x is observed. entropy has important implications for binary codings\nof x.\n\nproblems and theoretical exercises\n\n9.1. customers arrive at a bank at a poisson rate \u03bb.\nsuppose that two customers arrived during the first\nhour. what is the probability that\n(a) both arrived during the first 20 minutes?\n(b) at least one arrived during the first 20 min-\n\nutes?\n\n9.2. cars cross a certain point in the highway in accor-\ndance with a poisson process with rate \u03bb = 3 per\nminute. if al runs blindly across the highway, what\nis the probability that he will be uninjured if the\namount of time that it takes him to cross the road\nis s seconds? (assume that if he is on the highway\nwhen a car passes by, then he will be injured.) do\nthis exercise for s = 2, 5, 10, 20.\n\n9.3. suppose that in problem 9.2 al is agile enough to\nescape from a single car, but if he encounters two\nor more cars while attempting to cross the road,\nthen he is injured. what is the probability that he\nwill be unhurt if it takes him s seconds to cross? do\nthis exercise for s = 5, 10, 20, 30.\n\n9.4. suppose that 3 white and 3 black balls are dis-\ntributed in two urns in such a way that each urn\ncontains 3 balls. we say that the system is in state\ni if the first urn contains i white balls, i = 0, 1, 2, 3.\nat each stage, 1 ball is drawn from each urn and\nthe ball drawn from the first urn is placed in the\nsecond, and conversely with the ball from the\nsecond urn. let xn denote the state of the sys-\ntem after the nth stage, and compute the transition\nprobabilities of the markov chain {xn, n \u00fa 0}.\n\n9.5. consider example 2a. if there is a 50\u201350 chance of\nrain today, compute the probability that it will rain\n3 days from now if \u03b1 = .7 and \u03b2 = .3.\n\n9.6. compute the limiting probabilities for the model\n\nof problem 9.4.\n\n9.7. a transition probability matrix is said to be doubly\n\nstochastic if\n\nm(cid:6)\n\ni=0\n\n$\n\nfor all states j = 0, 1, . . . , m. show that such a\n= 1/(m + 1), j =\nmarkov chain is ergodic, then\n0, 1, . . . , m.\n\nj\n\n9.8. on any given day, buffy is either cheerful (c), so-so\n(s), or gloomy (g). if she is cheerful today, then she\nwill be c, s, or g tomorrow with respective probabil-\nities .7, .2, and .1. if she is so-so today, then she will\nbe c, s, or g tomorrow with respective probabilities\n.4, .3, and .3. if she is gloomy today, then buffy will\nbe c, s, or g tomorrow with probabilities .2, .4, and\n.4. what proportion of time is buffy cheerful?\n\n9.9. suppose that whether it rains tomorrow depends\non past weather conditions only through the last\n2 days. specifically, suppose that if it has rained\nyesterday and today, then it will rain tomorrow\nwith probability .8; if it rained yesterday but not\ntoday, then it will rain tomorrow with probability\n.3; if it rained today but not yesterday, then it will\nrain tomorrow with probability .4; and if it has not\nrained either yesterday or today, then it will rain\ntomorrow with probability .2. what proportion of\ndays does it rain?\n\n9.10. a certain person goes for a run each morning.\nwhen he leaves his house for his run, he is equally\nlikely to go out either the front or the back door,\nand similarly, when he returns, he is equally likely\nto go to either the front or the back door. the run-\nner owns 5 pairs of running shoes, which he takes\noff after the run at whichever door he happens to\nbe. if there are no shoes at the door from which he\nleaves to go running, he runs barefooted. we are\ninterested in determining the proportion of time\nthat he runs barefooted.\n(a) set this problem up as a markov chain. give\n\nthe states and the transition probabilities.\n\n(b) determine the proportion of days that he runs\n\nbarefooted.\n\n$\n\npij = 1\n\n9.11. this problem refers to example 2f.\n\n(a) verify that the proposed value of\n\nthe necessary equations.\n\nj satisfies\n\n "}, {"Page_number": 451, "text": "436\n\nchapter 9\n\nadditional topics in probability\n\n(b) for any given molecule, what do you think is\n\nthe (limiting) probability that it is in urn 1?\n(c) do you think that the events that molecule j,\nj \u00fa 1, is in urn 1 at a very large time would be\n(in the limit) independent?\n\n(d) explain why the limiting probabilities are as\n\ngiven.\n\n9.12. determine the entropy of the sum that is obtained\n\nwhen a pair of fair dice is rolled.\n\n9.13. prove that if x can take on any of n possible values\nwith respective probabilities p1, . . . , pn, then h(x)\nis maximized when pi = 1/n, i = 1, . . . , n. what is\nh(x) equal to in this case?\n\n9.14. a pair of fair dice is rolled. let\n\n%\n\nx =\n\nif the sum of the dice is 6\n\n1\n0 otherwise\n\nand let y equal the value of the first die. compute\n(a) h(y), (b) hy (x), and (c) h(x, y).\n\n9.15. a coin having probability p = 2\n\n3 of coming up\nheads is flipped 6 times. compute the entropy of\nthe outcome of this experiment.\n\n9.16. a random variable can take on any of n possi-\nble values x1, . . . , xn with respective probabilities\n\np(xi), i = 1, . . . , n. we shall attempt to determine\nthe value of x by asking a series of questions,\neach of which can be answered \u201cyes\u201d or \u201cno.\u201d for\ninstance, we may ask \u201cis x = x1?\u201d or \u201cis x equal\nto either x1 or x2 or x3?\u201d and so on. what can you\nsay about the average number of such questions\nthat you will need to ask to determine the value\nof x?\n\n9.17. show that, for any discrete random variable x and\n\nfunction f ,\n\nh(f (x)) \u2026 h(x)\n\n9.18. in transmitting a bit from location a to location\nb, if we let x denote the value of the bit sent at\nlocation a and y denote the value received at loca-\ntion b, then h(x) \u2212 hy (x) is called the rate of\ntransmission of information from a to b. the max-\nimal rate of transmission, as a function of p{x =\n1} = 1 \u2212 p{x = 0}, is called the channel capac-\nity. show that, for a binary symmetric channel with\np{y = 1|x = 1} = p{y = 0|x = 0} = p, the chan-\nnel capacity is attained by the rate of transmission\nof information when p{x = 1} = 1\n2 and its value is\n1 + p log p + (1 \u2212 p) log(1 \u2212 p).\n\nself-test problems and exercises\n\n9.1. events occur according to a poisson process with\n\nrate \u03bb = 3 per hour.\n(a) what is the probability that no events occur\n\nbetween times 8 and 10 in the morning?\n\n(b) what is the expected value of the number of\nevents that occur between times 8 and 10 in\nthe morning?\n\n(c) what is the expected time of occurrence of\n\nthe fifth event after 2 p.m.?\n\n9.2. customers arrive at a certain retail establishment\naccording to a poisson process with rate \u03bb per\nhour. suppose that two customers arrive during\nthe first hour. find the probability that\n(a) both arrived in the first 20 minutes;\n(b) at least one arrived in the first 30 minutes.\n\n9.3. four out of every five trucks on the road are fol-\nlowed by a car, while one out of every six cars is\nfollowed by a truck. what proportion of vehicles\non the road are trucks?\n\n9.4. a certain town\u2019s weather is classified each day as\nbeing rainy, sunny, or overcast, but dry. if it is\nrainy one day, then it is equally likely to be either\nsunny or overcast the following day. if it is not\nrainy, then there is one chance in three that the\nweather will persist in whatever state it is in for\nanother day, and if it does change, then it is equally\nlikely to become either of the other two states. in\nthe long run, what proportion of days are sunny?\nwhat proportion are rainy?\n\n9.5. let x be a random variable that takes on 5 pos-\nsible values with respective probabilities .35, .2, .2,\n.2, and .05. also, let y be a random variable that\ntakes on 5 possible values with respective proba-\nbilities .05, .35, .1, .15, and .35.\n(a) show that h(x) > h(y).\n(b) using the result of problem 9.13, give an intu-\nitive explanation for the preceding inequality.\n\nreferences\n\nsections 9.1 and 9.2\n\n[1] kemeny, j., l. snell, and a. knapp. denumerable markov chains. new york:\n\nd. van nostrand company, 1966.\n\n[2] parzen, e. stochastic processes. san francisco: holden-day, inc., 1962.\n\n "}, {"Page_number": 452, "text": "references 437\n\n[3] ross, s. m. introduction to probability models, 9th ed. san diego: academic press,\n\ninc., 2007.\n\n[4] ross, s. m. stochastic processes, 2d ed. new york: john wiley & sons, inc., 1996.\n\nsections 9.3 and 9.4\n\n[5] abramson, n. information theory and coding. new york: mcgraw-hill book\n\ncompany, 1963.\n\n[6] mceliece, r. theory of information and coding. reading, ma: addison-wesley\n\npublishing co., inc., 1977.\n\n[7] peterson, w., and e. weldon. error correcting codes, 2d ed. cambridge, ma: the\n\nmit press, 1972.\n\n "}, {"Page_number": 453, "text": "c h a p t e r\n\n10\n\nsimulation\n\n10.1 introduction\n10.2 general techniques for simulating continuous random variables\n10.3 simulating from discrete distributions\n10.4 variance reduction techniques\n\n10.1 introduction\n\nhow can we determine the probability of our winning a game of solitaire?\n(by solitaire, we mean any one of the standard solitaire games played with an ordi-\nnary deck of 52 playing cards and with some fixed playing strategy.) one possible\napproach is to start with the reasonable hypothesis that all (52)! possible arrange-\nments of the deck of cards are equally likely to occur and then attempt to determine\nhow many of these lead to a win. unfortunately, there does not appear to be any sys-\ntematic method for determining the number of arrangements that lead to a win, and\nas (52)! is a rather large number and the only way to determine whether a particular\narrangement leads to a win seems to be by playing the game out, it can be seen that\nthis approach will not work.\n\nin fact, it might appear that the determination of the probability of winning at\nsolitaire is mathematically intractable. however, all is not lost, for probability falls\nnot only within the realm of mathematics, but also within the realm of applied sci-\nence; and, as in all applied sciences, experimentation is a valuable technique. for our\nsolitaire example, experimentation takes the form of playing a large number of such\ngames or, better yet, programming a computer to do so. after playing, say, n games,\nif we let\n\n%\n\nxi =\n\n1 if the ith game results in a win\n0 otherwise\n\nthen xi, i = 1, . . . , n will be independent bernoulli random variables for which\n\ne[xi] = p{win at solitaire}\nhence, by the strong law of large numbers, we know that\n\nn(cid:6)\n\ni=1\n\nxi\nn\n\n= number of games won\nnumber of games played\n\nwill, with probability 1, converge to p{win at solitaire}. that is, by playing a large\nnumber of games, we can use the proportion of games won as an estimate of the prob-\nability of winning. this method of empirically determining probabilities by means of\nexperimentation is known as simulation.\n\n438\n\n "}, {"Page_number": 454, "text": "section 10.1\n\nintroduction 439\n\nin order to use a computer to initiate a simulation study, we must be able to gener-\nate the value of a uniform (0, 1) random variable; such variates are called random\nnumbers. to generate them, most computers have a built-in subroutine, called a\nrandom-number generator, whose output is a sequence of pseudorandom numbers\u2014\na sequence of numbers that is, for all practical purposes, indistinguishable from a sam-\nple from the uniform (0, 1) distribution. most random-number generators start with\nan initial value x0, called the seed, and then recursively compute values by specifying\npositive integers a, c, and m, and then letting\n\nxn+1 = (axn + c) modulo m n \u00fa 0\n\n(1.1)\nwhere the foregoing means that axn + c is divided by m and the remainder is taken\nas the value of xn+1. thus, each xn is either 0, 1, . . . , m \u2212 1, and the quantity xn/m is\ntaken as an approximation to a uniform (0, 1) random variable. it can be shown that,\nsubject to suitable choices for a, c, and m, equation (1.1) gives rise to a sequence of\nnumbers that look as if they were generated from independent uniform (0, 1) random\nvariables.\n\nas our starting point in simulation, we shall suppose that we can simulate from\nthe uniform (0, 1) distribution, and we shall use the term random numbers to mean\nindependent random variables from this distribution.\n\nin the solitaire example, we would need to program a computer to play out the\ngame starting with a given ordering of the cards. however, since the initial order-\ning is supposed to be equally likely to be any of the (52)! possible permutations, it\nis also necessary to be able to generate a random permutation. using only random\nnumbers, the following algorithm shows how this can be accomplished. the algorithm\nbegins by randomly choosing one of the elements and then putting it in position n; it\nthen randomly chooses among the remaining elements and puts the choice in position\nn \u2212 1, and so on. the algorithm efficiently makes a random choice among the remain-\ning elements by keeping these elements in an ordered list and then randomly choosing\na position on that list.\n\nexample 1a generating a random permutation\nsuppose we are interested in generating a permutation of the integers 1, 2, . . . , n such\nthat all n! possible orderings are equally likely. then, starting with any initial permu-\ntation, we will accomplish this after n \u2212 1 steps, where we interchange the positions\nof two of the numbers of the permutation at each step. throughout, we will keep\ntrack of the permutation by letting x(i), i = 1, . . . , n denote the number currently in\nposition i. the algorithm operates as follows:\n\n1. consider any arbitrary permutation, and let x(i) denote the element in posi-\n\ntion i, i = 1 . . . , n. [for instance, we could take x(i) = i, i = 1, . . . , n.]\n\n2. generate a random variable nn that is equally likely to equal any of the values\n\n1, 2, . . . , n.\n3. interchange the values of x(nn) and x(n). the value of x(n) will now remain\nfixed. [for instance, suppose that n = 4 and initially x(i) = i, i = 1, 2, 3, 4. if\nn4 = 3, then the new permutation is x(1) = 1, x(2) = 2, x(3) = 4, x(4) = 3,\nand element 3 will remain in position 4 throughout.]\n4. generate a random variable nn\u22121 that is equally likely to be either 1, 2, . . . ,\nn \u2212 1.\n5. interchange the values of x(nn\u22121) and x(n \u2212 1). [if n3 = 1, then the new\npermutation is x(1) = 4, x(2) = 2, x(3) = 1, x(4) = 3.]\n\n "}, {"Page_number": 455, "text": "440\n\nchapter 10\n\nsimulation\n6. generate nn\u22122, which is equally likely to be either 1, 2, . . . , n \u2212 2.\n7. interchange the values of x(nn\u22122) and x(n \u2212 2). [if n2 = 1, then the new\npermutation is x(1) = 2, x(2) = 4, x(3) = 1, x(4) = 3, and this is the final\npermutation.]\n\n8. generate nn\u22123, and so on. the algorithm continues until n2 is generated, and\n\nafter the next interchange the resulting permutation is the final one.\n\nto implement this algorithm, it is necessary to be able to generate a random vari-\nable that is equally likely to be any of the values 1, 2, . . . , k. to accomplish this, let\nu denote a random number\u2014that is, u is uniformly distributed on (0, 1)\u2014and note\nthat ku is uniform on (0, k). hence,\n\np{i \u2212 1 < ku < i} = 1\nk\n\ni = 1, . . . , k\n\nso if we take nk = [ku] + 1, where [x] is the integer part of x (that is, the largest\ninteger less than or equal to x), then nk will have the desired distribution.\n\nthe algorithm can now be succinctly written as follows:\n\nset x(i) = i, i = 1, . . . , n.]\n\nstep 1. let x(1), . . . , x(n) be any permutation of 1, 2, . . . , n. [for instance, we can\nstep 2. let i = n.\nstep 3. generate a random number u and set n = [iu] + 1.\nstep 4.\nstep 5. reduce the value of i by 1, and if i > 1, go to step 3.\nstep 6. x(1), . . . , x(n) is the desired random generated permutation.\n\ninterchange the values of x(n) and x(i).\n\nthe foregoing algorithm for generating a random permutation is extremely useful.\n(cid:9)\nfor instance, suppose that a statistician is developing an experiment to compare the\neffects of m different treatments on a set of n subjects. he decides to split the subjects\ni=1 ni = n, with\ninto m different groups of respective sizes n1, n2, . . . , nm, where\nthe members of the ith group to receive treatment i. to eliminate any bias in the\nassignment of subjects to treatments (for instance, it would cloud the meaning of the\nexperimental results if it turned out that all the \u201cbest\u201d subjects had been put in the\nsame group), it is imperative that the assignment of a subject to a given group be done\n\u201cat random.\u201d how is this to be accomplished?\u2020\n\na simple and efficient procedure is to arbitrarily number the subjects 1 through\nn and then generate a random permutation x(1), . . . , x(n) of 1, 2, . . . , n. now assign\nsubjects x(1), x(2), . . . , x(n1) to be in group 1, x(n1 + 1), . . . , x(n1 + n2) to be in\ngroup 2, and, in general, group j is to consist of subjects numbered x(n1 + n2 + \u00b7\u00b7\u00b7 +\nnj\u22121 + k), k = 1, . . . , nj.\n.\n\nm\n\n10.2 general techniques for simulating continuous\n\nrandom variables\n\nin this section, we present two general methods for using random numbers to simulate\ncontinuous random variables.\n\n\u2020another technique for randomly dividing the subjects when m = 2 was presented in exam-\nple 2g of chapter 6. the preceding procedure is faster, but requires more space than the one of\nexample 2g.\n\n "}, {"Page_number": 456, "text": "section 10.2\n\nsimulating continuous random variables 441\n\n10.2.1 the inverse transformation method\na general method for simulating a random variable having a continuous distribution\u2014\ncalled the inverse transformation method\u2014is based on the following proposition.\n\nproposition 2.1. let u be a uniform (0, 1) random variable. for any continuous\ndistribution function f, if we define the random variable y by\n\ny = f\n\n\u22121(u)\n\nthen the random variable y has distribution function f. [f\nthat value y for which f(y) = x.]\n\n\u22121(x) is defined to equal\n\nproof.\n\nfy (a) = p{y \u2026 a}\n\n= p{f\n\n\u22121(u) \u2026 a}\n\nnow, since f(x) is a monotone function, it follows that f\nu \u2026 f(a). hence, from equation (2.1), we have\n\nfy (a) = p{u \u2026 f(a)}\n\n= f(a)\n\n(2.1)\n\u22121(u) \u2026 a if and only if\n\nit follows from proposition 2.1 that we can simulate a random variable x having\na continuous distribution function f by generating a random number u and then\nsetting x = f\n\n\u22121(u).\n\nexample 2a simulating an exponential random variable\nif f(x) = 1 \u2212 e\n\n\u22121(u) is that value of x such that\n\n\u2212x, then f\n\nor\n\n1 \u2212 e\n\n\u2212x = u\n\nx = \u2212 log(1 \u2212 u)\n\nhence, if u is a uniform (0, 1) variable, then\n\n\u22121(u) = \u2212 log(1 \u2212 u)\n\nf\n\nis exponentially distributed with mean 1. since 1 \u2212 u is also uniformly distributed on\n(0, 1), it follows that \u2212 log u is exponential with mean 1. since cx is exponential with\nmean c when x is exponential with mean 1, it follows that \u2212c log u is exponential\n.\nwith mean c.\n\nthe results of example 2a can also be utilized to stimulate a gamma random\n\nvariable.\n\nexample 2b simulating a gamma (n, \u03bb) random variable\nto simulate from a gamma distribution with parameters (n, \u03bb) when n is an integer,\nwe use the fact that the sum of n independent exponential random variables, each\nhaving rate \u03bb, has this distribution. hence, if u1, . . . , un are independent uniform\n(0, 1) random variables, then\n\n "}, {"Page_number": 457, "text": "442\n\nchapter 10\n\nsimulation\n\nx = \u2212 n(cid:6)\n\ni=1\n\n1\n\u03bb\n\nlog ui = \u2212 1\n\n\u03bb\n\nlog\n\n\u239e\n\u23a0\n\nui\n\n\u239b\n\u239d n(cid:31)\n\ni=1\n\nhas the desired distribution.\n\n.\n\n10.2.2 the rejection method\nsuppose that we have a method for simulating a random variable having density\nfunction g(x). we can use this method as the basis for simulating from the contin-\nuous distribution having density f (x) by simulating y from g and then accepting the\nsimulated value with a probability proportional to f (y)/g(y).\n\nspecifically, let c be a constant such that\n\nf (y)\ng(y)\n\n\u2026 c\n\nfor all y\n\nwe then have the following technique for simulating a random variable having\ndensity f .\n\nrejection method\n\nstep 1. simulate y having density g and simulate a random number u.\nstep 2.\n\nif u \u2026 f (y)/cg(y), set x = y. otherwise return to step 1.\n\nthe rejection method is expressed pictorially in figure 10.1. we now prove that it\n\nworks.\n\nstart\n\ngenerate\n\ny \u2b03 g\n\ngenerate a\n\nrandom number\n\nu \n\nis\nf(y)\n\u2014\u2014\u2013\u2013\ncg(y)  \n\nu \u2b50\n\nyes\n\nset x = y\n\nno\n\nfigure 10.1: rejection method for simulating a random variable x having density function f.\n\nproposition 2.2. the random variable x generated by the rejection method has den-\nsity function f .\n\nproof. let x be the value obtained and let n denote the number of necessary\niterations. then\n\np{x \u2026 x} = p{yn \u2026 x}\n\n%\n%\ny \u2026 x|u \u2026 f (y)\ncg(y)\ny \u2026 x, u \u2026 f (y)\ncg(y)\n\n/\n/\n\n= p\n\n= p\n\nk\n\nwhere k = p{u \u2026 f (y)/cg(y)}. now, by independence, the joint density function\nof y and u is\n\nf (y, u) = g(y)\n\n0 < u < 1\n\n "}, {"Page_number": 458, "text": "section 10.2\n\nso, using the foregoing, we have\n\np{x \u2026 x} = 1\nk\n\nsimulating continuous random variables 443\n\n* *\n\ny \u2026 x\n\n0 \u2026 u \u2026 f (y)/cg(y)\nf (y)/cg(y)\n\n0\n\nf (y) dy\n\ng(y) du dy\n\ndu g(y) dy\n\n(2.2)\n\n*\n\n*\n*\n* q\n\nx\n\u2212q\nx\n\u2212q\n\n\u2212q\n\n= 1\nk\n= 1\nck\n\n1 = 1\nck\n\np{x \u2026 x} =\n\nf (y) dy = 1\nck\n*\n\nx\n\u2212q\n\nf (y) dy\n\nletting x approach q and using the fact that f is a density gives\n\nhence, from equation (2.2), we obtain\n\nwhich completes the proof.\n\nremarks.\n\n(a) note that the way in which we \u201caccept the value y with prob-\nability f (y)/cg(y)\u201d is by generating a random number u and then accepting y if\nu \u2026 f (y)/cg(y).\n(b) since each iteration will independently result in an accepted value with proba-\nbility p{u \u2026 f (y)/cg(y)} = k = 1/c, it follows that the number of iterations has a\n.\ngeometric distribution with mean c.\n\nexample 2c simulating a normal random variable\nto simulate a unit normal random variable z (that is, one with mean 0 and variance\n1), note first that the absolute value of z has probability density function\n\nf (x) = 2\u221a\n2\u03c0\n\n\u2212x2/2\ne\n\n0 < x < q\n\n(2.3)\n\nwe will start by simulating from the preceding density function by using the rejection\nmethod, with g being the exponential density function with mean 1\u2014that is,\n\nnow, note that\n\nf (x)\ng(x)\n\n=\n\n=\n\n=\n\n\u2026\n\n2\n\u03c0\n\nexp\n\n7\n\n0 < x < q\n\n\u2212(x2 \u2212 2x)\n\ng(x) = e\n\u2212x\n0\n@\n0\n@\n\u2212(x2 \u2212 2x + 1)\n0\n7\n@\n@\n\n2\n\u2212(x \u2212 1)2\n\n2e\n\u03c0\n\nexp\n\nexp\n\n2\n\u03c0\n\n2\n\n2\n\n2e\n\u03c0\n\n7\n\n+ 1\n2\n\n(2.4)\n\n "}, {"Page_number": 459, "text": "444\n\nchapter 10\n\nsimulation\n\nhence, we can take c =.\n\n0\n\n7\n\n2e/\u03c0; so, from equation (2.4),\n\nf (x)\ncg(x)\n\n= exp\n\n\u2212(x \u2212 1)2\n\n2\n\nrate 1 and u being uniform on (0, 1).\n\ntherefore, using the rejection method, we can simulate the absolute value of a unit\nnormal random variable as follows:\n(a) generate independent random variables y and u, y being exponential with\n(b) if u \u2026 exp{\u2212(y \u2212 1)2/2}, set x = y. otherwise, return to (a).\nonce we have simulated a random variable x having equation (2.3) as its density\nfunction, we can then generate a unit normal random variable z by letting z be\nequally likely to be either x or \u2212x.\nin step (b), the value y is accepted if u \u2026 exp{\u2212(y \u2212 1)2/2}, which is equiva-\nlent to \u2212 log u \u00fa (y \u2212 1)2/2. however, in example 2a it was shown that \u2212 log u is\nexponential with rate 1, so steps (a) and (b) are equivalent to\n(cid:8)\n(a\n(cid:8)\n(b\nsuppose now that the foregoing results in y1\u2019s being accepted\u2014so we know that y2\nis larger than (y1 \u2212 1)2/2. by how much does the one exceed the other? to answer\nthis question, recall that y2 is exponential with rate 1; hence, given that it exceeds\nsome value, the amount by which y2 exceeds (y1 \u2212 1)2/2 [that is, its \u201cadditional\nlife\u201d beyond the time (y1 \u2212 1)2/2] is (by the memoryless property) also exponentially\n(cid:8)), not only do we obtain x (the\ndistributed with rate 1. that is, when we accept step (b\nabsolute value of a unit normal), but, by computing y2 \u2212 (y1 \u2212 1)2/2, we also can\ngenerate an exponential random variable (that is independent of x) having rate 1.\nsumming up, then, we have the following algorithm that generates an exponential\n\n) generate independent exponentials y1 and y2, each with rate 1.\n) if y2 \u00fa (y1 \u2212 1)2/2, set x = y1. otherwise, return to (a\u2019).\n\nwith rate 1 and an independent unit normal random variable:\n\nstep 1. generate y1, an exponential random variable with rate 1.\nstep 2. generate y2, an exponential random variable with rate 1.\nstep 3.\n\nif y2 \u2212 (y1 \u2212 1)2/2 > 0, set y = y2 \u2212 (y1 \u2212 1)2/2 and go to step 4.\notherwise, go to step 1.\n\nstep 4. generate a random number u, and set\n\n0\ny1 if u \u2026 1\n2\n\u2212y1 if u > 1\n\n2\n\nz =\n\nthe random variables z and y generated by the foregoing algorithm are indepen-\ndent, with z being normal with mean 0 and variance 1 and y being exponential with\nrate 1. (if we want the normal random variable to have mean \u03bc and variance \u03c3 2, we\njust take \u03bc + \u03c3 z.)\n\nremarks.\n\n2e/\u03c0 l 1.32, the algorithm requires a geometrically\n\n(a) since c = .\n\ndistributed number of iterations of step 2 with mean 1.32.\n\n(b) if we want to generate a sequence of unit normal random variables, then we can\nuse the exponential random variable y obtained in step 3 as the initial exponential\nneeded in step 1 for the next normal to be generated. hence, on the average, we\ncan simulate a unit normal by generating 1.64(= 2 * 1.32 \u2212 1) exponentials and\n.\ncomputing 1.32 squares.\n\n "}, {"Page_number": 460, "text": "section 10.2\n\nsimulating continuous random variables 445\n\nexample 2d simulating normal random variables: the polar method\nit was shown in example 7b of chapter 6 that if x and y are independent unit normal\nrandom variables, then their polar coordinates r =\n\u22121(y/x)\nare independent, with r2 being exponentially distributed with mean 2 and \u2330 being\nuniformly distributed on (0, 2\u03c0 ). hence, if u1 and u2 are random numbers, then,\nusing the result of example 2a, we can set\n\n.\nx2 + y2, \u2330 = tan\n\nr = (\u22122 log u1)1/2\n\u03b8 = 2\u03c0u2\n\nfrom which it follows that\n\nx = r cos \u03b8 = (\u22122 log u1)1/2 cos(2\u03c0u2)\ny = r sin \u03b8 = (\u22122 log u1)1/2 sin(2\u03c0u2)\n\nare independent unit normals.\n\n(2.5)\n.\nthe preceding approach to generating unit normal random variables is called the\nbox\u2013muller approach. its efficiency suffers somewhat from its need to compute the\nsine and cosine values. there is, however, a way to get around this potentially time-\nconsuming difficulty. to begin, note that if u is uniform on (0, 1) then 2u is uniform\non (0, 2), so 2u \u2212 1 is uniform on (\u22121, 1). thus, if we generate random numbers u1\nand u2 and set\n\nv1 = 2u1 \u2212 1\nv2 = 2u2 \u2212 1\n\nthen (v1, v2) is uniformly distributed in the square of area 4 centered at (0, 0). (see\nfigure 10.2.)\n\nsuppose now that we continually generate such pairs (v1, v2) until we obtain one\n\u2026 1.\nthat is contained in the disk of radius 1 centered at (0, 0)\u2014that is, until v2\n1\nit then follows that such a pair (v1, v2) is uniformly distributed in the disk. now, let\n\n+ v2\n\n2\n\n(\u20131, 1)\n\n(1, 1)\n\nv2\n\nr\n\n\u242a\n\nv1\n\n2 +\n\nv 1\n\n2\nv 2\n\n= 1\n\n(\u20131, \u20131)\n\n(1, \u20131)\n\n= (0, 0)\n= (v1, v2)\n\nfigure 10.2:\n\n "}, {"Page_number": 461, "text": "446\n\nchapter 10\n\nsimulation\n\nr, \u2330 denote the polar coordinates of this pair. then it is easy to verify that r and \u2330\nare independent, with r2 being uniformly distributed on (0, 1) and \u2330 being uniformly\ndistributed on (0, 2\u03c0 ). (see problem 13.)\n\nsince\n\nsin \u2330 = v2\nr\ncos \u2330 = v1\nr\n\n=\n\n=\n\n8\n8\n\nv2\n+ v2\nv2\n1\nv1\n+ v2\n\nv2\n1\n\n2\n\n2\n\nit follows from equation (2.5) that we can generate independent unit normals x and\ny by generating another random number u and setting\nx = (\u22122 log u)1/2v1/r\ny = (\u22122 log u)1/2v2/r\n\n\u2026 1) r2 is uniform on (0, 1) and is inde-\nin fact, because (conditional on v2\n1\npendent of \u03b8, we can use it instead of generating a new random number u, thus\nshowing that\n\n2\n\n+ v2\n\nx = (\u22122 log r2)1/2 v1\nr\ny = (\u22122 log r2)1/2 v2\nr\n\n=\n\n=\n\n@\n@\n\n\u22122 log s\n\u22122 log s\n\ns\n\ns\n\nv1\n\nv2\n\nare independent unit normals, where\n\ns = r2 = v2\n\n1\n\n+ v2\n\n2\n\nsumming up, we have the following approach to generating a pair of independent\nunit normals:\n\nstep 1. generate random numbers u1 and u2.\nstep 2. set v1 = 2u1 \u2212 1, v2 = 2u2 \u2212 1, s = v2\n+ v2\n2.\nstep 3.\n@\nstep 4. return the independent unit normals\n\nif s > 1, return to step 1.\n\n@\n\n1\n\nx =\n\n\u22122 log s\n\ns\n\nv1, y =\n\n\u22122 log s\n\nv2\n\ns\n\nthe preceding algorithm is called the polar method. since the probability that a\nrandom point in the square will fall within the circle is equal to \u03c0/4 (the area of the\ncircle divided by the area of the square), it follows that, on average, the polar method\nwill require 4/\u03c0 l 1.273 iterations of step 1. hence, it will, on average, require 2.546\nrandom numbers, 1 logarithm, 1 square root, 1 division, and 4.546 multiplications to\ngenerate 2 independent unit normals.\n\nexample 2e simulating a chi-squared random variable\nthe chi-squared distribution with n degrees of freedom is the distribution of \u03c7 2\nn\nz2\n1\n\n=\nn, where zi, i = 1, . . . , n are independent unit normals. now, it was\n\n+ \u00b7\u00b7\u00b7 + z2\n\n "}, {"Page_number": 462, "text": "(cid:30)\n\nsection 10.3\n\nsimulating from discrete distributions 447\n+ z2\n(cid:29)\nshown in section 6.3 of chapter 6 that z2\n2 has an exponential distribution with\n2. hence, when n is even (say, n = 2k), \u03c7 2\n1\nrate 1\n2k has a gamma distribution with param-\nk\nk, 1\ni=1 ui) has a chi-squared distribution with 2k degrees\neters\n2\nof freedom. accordingly, can simulate a chi-squared random variable with 2k + 1\ndegrees of freedom by first simulating a unit normal random variable z and then\nadding z2 to the foregoing. that is,\n\n. thus, \u22122 log(\n\n$\n\n\u03c7 2\n2k+1\n\n= z2 \u2212 2 log\n\n\u239e\n\u23a0\n\nui\n\n\u239b\n\u239d k(cid:31)\n\ni=1\n\nwhere z, u1, . . . , un are independent, z is a unit normal, and u1, . . ., un are uniform\n(0, 1) random variables.\n\n10.3 simulating from discrete distributions\n\nall of the general methods for simulating random variables from continuous distribu-\ntions have analogs in the discrete case. for instance, if we want to simulate a random\nvariable z having probability mass function\n\np{x = xj} = pj,\n\nj = 0, 1, . . . ,\n\npj = 1\n\n(cid:6)\n\nj\n\nwe can use the following discrete time analog of the inverse transform technique:\n\nto simulate x for which p{x = xj} = pj, let u be uniformly distributed over (0, 1)\n\nand set\n\n\u23a7\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9\n\nx1\nx2\n#\n#\n#\n\nxj\n\n#\n#\n#\n\nx =\n\nif u \u2026 p1\nif p1 < u \u2026 p1 + p2\nj(cid:6)\nj\u22121(cid:6)\n\npi < u \u2026\n\nif\n\n1\n\npi\n\ni\n\n\u23a7\u23a8\n\u23a9 j\u22121(cid:6)\n\n1\n\npi < u \u2026\n\n\u23ab\u23ac\n\u23ad = pj\n\npi\n\nj(cid:6)\n\n1\n\nsince\n\np{x = xj} = p\n\nit follows that x has the desired distribution.\n\nexample 3a the geometric distribution\nsuppose that independent trials, each of which results in a \u201csuccess\u201d with probability\np, 0 < p < 1, are continually performed until a success occurs. letting x denote the\nnecessary number of trials; then\n\np{x = i} = (1 \u2212 p)i\u22121p i \u00fa 1\n\n "}, {"Page_number": 463, "text": "448\n\nchapter 10\n\nsimulation\nwhich is seen by noting that x = i if the first i \u2212 1 trials are all failures and the ith\ntrial is a success. the random variable x is said to be a geometric random variable\nwith parameter p. since\n\nj\u22121(cid:6)\n\ni=1\n\np{x = i} = 1 \u2212 p{x > j \u2212 1}\n\n= 1 \u2212 p{first j \u2212 1 are all failures}\n= 1 \u2212 (1 \u2212 p)j\u22121\n\nj \u00fa 1\n\nwe can simulate such a random variable by generating a random number u and then\nsetting x equal to that value j for which\n\n1 \u2212 (1 \u2212 p)j\u22121 < u \u2026 1 \u2212 (1 \u2212 p)j\n\nor, equivalently, for which\n\n(1 \u2212 p)j \u2026 1 \u2212 u < (1 \u2212 p)j\u22121\n\nsince 1 \u2212 u has the same distribution as u, we can define x by\n\nx = min{j : (1 \u2212 p)j \u2026 u}\n\n7\n= min{j : j log(1 \u2212 p) \u2026 log u}\n= min\n\nj : j \u00fa\n\nlog u\n\n0\n\nlog(1 \u2212 p)\n\nwhere the inequality has changed sign because log(1 \u2212 p) is negative [since log(1 \u2212\np) < log 1 = 0]. using the notation [x] for the integer part of x (that is, [x] is the\nlargest integer less than or equal to x), we can write\n\n(cid:7)\n\n(cid:8)\n\nx = 1 +\n\nlog u\n\nlog(1 \u2212 p)\n\n.\n\nas in the continuous case, special simulating techniques have been developed for\n\nthe more common discrete distributions. we now present two of these.\n\nexample 3b simulating a binomial random variable\na binomial (n, p) random variable can easily be simulated by recalling that it can\nbe expressed as the sum of n independent bernoulli random variables. that is, if\nu1, . . . , un are independent uniform (0, 1) variables, then letting\n\n%\n\nxi =\n\nif ui < p\n1\n0 otherwise\n\nxi is a binomial random variable with parameters n and p.\n\nit follows that x k n(cid:9)\n\ni=1\n\n "}, {"Page_number": 464, "text": "section 10.4\n\nvariance reduction techniques 449\n\nexample 3c simulating a poisson random variable\nto simulate a poisson random variable with mean \u03bb, generate independent uniform\n(0, 1) random variables u1, u2, . . . stopping at\n\n\u23a7\u23a8\n\u23a9n:\n\nn(cid:31)\n\ni=1\n\n\u23ab\u23ac\n\u23ad\n\nn = min\n\n\u2212\u03bb\n\nui < e\n\nthe random variable x k n \u2212 1 has the desired distribution. that is, if we continue\n\u2212\u03bb, then the number\ngenerating random numbers until their product falls below e\nrequired, minus 1, is poisson with mean \u03bb.\nthat x k n \u2212 1 is indeed a poisson random variable having mean \u03bb can perhaps\n\nbe most easily seen by noting that\n\nx + 1 = min\n\nis equivalent to\n\n\u23a7\u23a8\n\u23a9n:\n\nn(cid:31)\n\ni=1\n\nx = max\n\nor, taking logarithms, to\n\nor\n\nx = max\n\nx = max\n\n\u2212\u03bb\n\nui \u00fa e\n\n\u23a7\u23a8\n\u23a9n:\n\u23a7\u23a8\n\u23a9n:\n\nn(cid:6)\n\ni=1\n\nn(cid:6)\n\ni=1\n\n\u23a7\u23a8\n\u23a9n:\n\n\u2212\u03bb\n\ni=1\n\nui < e\n\nn(cid:31)\n\u23ab\u23ac\n\u23ad where\n\nlog ui \u00fa \u2212\u03bb\n\n\u2212 log ui \u2026 \u03bb\n\n\u23ab\u23ac\n\u23ad\n0(cid:31)\n\u23ab\u23ac\n\u23ad\n\u23ab\u23ac\n\u23ad\n\ni=1\n\nui k 1\n\nhowever, \u2212 log ui is exponential with rate 1, so x can be thought of as being the\nmaximum number of exponentials having rate 1 that can be summed and still be less\nthan \u03bb. but by recalling that the times between successive events of a poisson process\nhaving rate 1 are independent exponentials with rate 1, it follows that x is equal to the\nnumber of events by time \u03bb of a poisson process having rate 1; thus x has a poisson\n.\ndistribution with mean \u03bb.\n\n10.4 variance reduction techniques\n\nlet x1, . . . , xn have a given joint distribution, and suppose that we are interested in\ncomputing\n\n\u03b8 k e[g(x1, . . . , xn)]\n\nwhere g is some specified function. it sometimes turns out that it is extremely difficult\nto analytically compute \u03b8, and when such is the case, we can attempt to use simulation\nto estimate \u03b8. this is done as follows: generate x (1)\nn having the same joint\ndistribution as x1, . . . , xn and set\n\n1 , . . . , x (1)\n\ny1 = g(x (1)\n\n1 , . . . , x (1)\nn\n\n)\n\n "}, {"Page_number": 465, "text": "450\n\nchapter 10\n\nsimulation\n\nsimulate a second set of random variables (independent of the\n\nn\n\n1 , . . . , x (2)\n\nnow let x (2)\nfirst set) having the distribution of x1, . . . , xn and set\n1 , . . . , x (2)\nn\n\ny2 = g(x (2)\n\n)\n\ncontinue this until you have generated k (some predetermined number) sets and so\nhave also computed y1, y2, . . . , yk. now, y1, . . . , yk are independent and identically\ndistributed random variables, each having the same distribution as g(x1, . . . , xn).\nthus, if we let y denote the average of these k random variables\u2014that is, if\n\ny = k(cid:6)\n\ni=1\n\nyi\nk\n\nthen\n\ne[y] = \u03b8\n\ne[(y \u2212 \u03b8 )2] = var(y)\n\nhence, we can use y as an estimate of \u03b8. since the expected square of the difference\nbetween y and \u03b8 is equal to the variance of y, we would like this quantity to be as\nsmall as possible. [in the preceding situation, var(y) = var(yi)/k, which is usually\nnot known in advance, but must be estimated from the generated values y1, . . . , yn.]\nwe now present three general techniques for reducing the variance of our estimator.\n\n10.4.1 use of antithetic variables\nin the foregoing situation, suppose that we have generated y1 and y2, which are\nidentically distributed random variables having mean \u03b8. now,\n\n(cid:2)\n\n(cid:3)\n\nvar\n\ny1 + y2\n\n2\n\n= 1\n4\n\n[var(y1) + var(y2) + 2cov(y1, y2)]\n\n= var(y1)\n\n2\n\n+ cov(y1, y2)\n\n2\n\nhence, it would be advantageous (in the sense that the variance would be reduced)\nif y1 and y2 were negatively correlated rather than being independent. to see how\nwe could arrange this, let us suppose that the random variables x1, . . . , xn are inde-\npendent and, in addition, that each is simulated via the inverse transform technique.\n\u22121\nthat is, xi is simulated from f\n(ui), where ui is a random number and fi is the\ni\ndistribution of xi. thus, y1 can be expressed as\n\ny1 = g(f\n\n\u22121\n1\n\n\u22121\n(u1), . . . , f\nn\n\n(un))\n\nnow, since 1 \u2212 u is also uniform over (0, 1) whenever u is a random number (and\nis negatively correlated with u), it follows that y2 defined by\n(1 \u2212 un))\n\n(1 \u2212 u1), . . . , f\n\ny2 = g(f\n\n\u22121\nn\n\n\u22121\n1\n\nwill have the same distribution as y1. hence, if y1 and y2 were negatively correlated,\nthen generating y2 by this means would lead to a smaller variance than if it were\ngenerated by a new set of random numbers. (in addition, there is a computational\nsavings because, rather than having to generate n additional random numbers, we\nneed only subtract each of the previous n numbers from 1.) although we cannot, in\n\n "}, {"Page_number": 466, "text": "section 10.4\n\nvariance reduction techniques 451\n\ngeneral, be certain that y1 and y2 will be negatively correlated, this often turns out to\nbe the case, and indeed it can be proven that it will be so whenever g is a monotonic\nfunction.\n\n10.4.2 variance reduction by conditioning\nlet us start by recalling the conditional variance formula (see section 7.5.4)\n\nvar(y) = e[var(y|z)] + var(e[y|z])\n\nnow, suppose that we are interested in estimating e[g(x1, . . . , xn)] by simulating\nx = (x1, . . . , xn) and then computing y = g(x). if, for some random variable z\nwe can compute e[y|z], then, since var(y|z) \u00fa 0, it follows from the preceding\nconditional variance formula that\n\nvar(e[y|z]) \u2026 var(y)\n\nthus, since e[e[y|z]] = e[y], it follows that e[y|z] is a better estimator of e[y]\nthan is y.\n\nexample 4a estimation of \u03c0\nlet u1 and u2 be random numbers and set vi = 2ui \u2212 1, i = 1, 2. as noted in\nexample 2d,(v1, v2) will be uniformly distributed in the square of area 4 centered at\n(0, 0). the probability that this point will fall within the inscribed circle of radius 1\ncentered at (0, 0) (see figure 10.2) is equal to \u03c0/4 (the ratio of the area of the circle\nto that of the square). hence, upon simulating a large number n of such pairs and\nsetting\n\n%\n\nij =\n\n1\n0 otherwise\n\nif the jth pair falls within the circle\n\nit follows that ij, j = 1, . . . , n, will be independent and identically distributed random\nvariables having e[ij] = \u03c0/4. thus, by the strong law of large numbers,\n\ni1 + \u00b7\u00b7\u00b7 + in\n\nn\n\n\u2192 \u03c0\n4\n\nas n\u2192q\n\ntherefore, by simulating a large number of pairs (v1, v2) and multiplying the propor-\ntion of them that fall within the circle by 4, we can accurately approximate \u03c0.\n\nthe preceding estimator can, however, be improved upon by using conditional\nexpectation. if we let i be the indicator variable for the pair (v1, v2), then, rather\nthan using the observed value of i, it is better to condition on v1 and so utilize\n\np{v2\n\n2\n\n\u2026 1 \u2212 v2\n\n1\n\nnow,\n\nso\n\n1\n\n2\n\n2\n\n1\n\n\u2026 1|v1}\n|v1}\n\ne[i|v1] = p{v2\n= p{v2\n\n+ v2\n\u2026 1 \u2212 v2\n.\n|v1 = \u03bd} = p{v2\n\u2026 1 \u2212 \u03bd2}\n.\n= p{\u2212\n1 \u2212 \u03bd2 \u2026 v2 \u2026\n1 \u2212 \u03bd2\n=\n8\n\n2\n\ne[i|v1] =\n\n1 \u2212 v2\n\n1\n\n.\n1 \u2212 \u03bd2}\n\n "}, {"Page_number": 467, "text": "452\n\nchapter 10\n\nsimulation\n\n8\n\n0\n\n1\n\ne[\n\n1\n2\n\n*\n\n.\n\n.\n\n1\n\u22121\n\n1] =\n\n1 \u2212 u2]\n\n1. indeed, since\n\n1 \u2212 v2\n*\n\n1 \u2212 u2du = e[\n\n.\n1 \u2212 \u03bd2d\u03bd =\n\nthus, an improvement on using the average value of i to estimate \u03c0/4 is to use the\naverage value of\n\n8\n1 \u2212 v2\n.\nwhere u is uniform over (0, 1), we can generate n random numbers u and use the\n1 \u2212 u2 as our estimate of \u03c0/4. (problem 14 shows that this esti-\naverage value of\n.\nmator has the same variance as the average of the n values,\nthe preceding estimator of \u03c0 can be improved even further by noting that the\n1 \u2212 u2, 0 \u2026 u \u2026 1, is a monotonically decreasing function of u,\nfunction g(u) =\n.\nand so the method of antithetic variables will reduce the variance of the estimator\n1 \u2212 u2]. that is, rather than generating n random numbers and using the\nof e[\n1 \u2212 u2 as an estimator of \u03c0/4, we would obtain an improved\naverage value of\nestimator by generating only n/2 random numbers u and then using one-half the\naverage of\nthe following table gives the estimates of \u03c0 resulting from simulations, using n =\n\n.\n1 \u2212 (1 \u2212 u)2 as the estimator of \u03c0/4.\n\n.\n1 \u2212 u2 +\n\n.\n1 \u2212 v2.)\n\n.\n\n10, 000, based on the three estimators.\n\nmethod\n\n.\n.\n\nproportion of the random points that fall in the circle\naverage value of\naverage value of\n\n1 \u2212 u2\n1 \u2212 u2 +\n\n1 \u2212 (1 \u2212 u)2\n\n.\n\nestimate of \u03c0\n\n3.1612\n3.128448\n3.139578\n\na further simulation using the final approach and n = 64, 000 yielded the estimate\n.\n3.143288.\n\n10.4.3 control variates\nagain, suppose that we want to use simulation to estimate e[g(x)], where x =\n(x1, . . . , xn). but suppose now that, for some function f , the expected value of f (x)\nis known\u2014say, it is e[f (x)] = \u03bc. then, for any constant a, we can also use\n\nw = g(x) + a[f (x) \u2212 \u03bc]\n\nas an estimator of e[g(x)]. now,\n\nvar(w) = var[g(x)] + a2var[f (x)] + 2a cov[g(x), f (x)]\n\nsimple calculus shows that the foregoing is minimized when\n\na = \u2212cov[f (x), g(x)]\n\nvar[f (x)]\n\nand for this value of a,\n\nvar(w) = var[g(x)] \u2212 [cov[f (x), g(x)]2\n\nvar[f (x)]\n\n(4.1)\n\n(4.2)\n\n(4.3)\n\n "}, {"Page_number": 468, "text": "unfortunately, neither var[f (x)] nor cov[f (x)], g(x)] is usually known, so we can-\nnot in general obtain the foregoing reduction in variance. one approach in practice\nis to use the simulated data to estimate these quantities. this approach usually yields\nalmost all of the theoretically possible reduction in variance.\n\nproblems 453\n\nsummary\nlet f be a continuous distribution function and u a uniform (0, 1) random variable.\n\u22121(u) is that\nthen the random variable f\nvalue x such that f(x) = u. applying this result, we can use the values of uniform (0,\n1) random variables, called random numbers, to generate the values of other random\nvariables. this technique is called the inverse transform method.\n\n\u22121(u) has distribution function f, where f\n\nanother technique for generating random variables is based on the rejection method.\n\nsuppose that we have an efficient procedure for generating a random variable from\nthe density function g and that we desire to generate a random variable having den-\nsity function f . the rejection method for accomplishing this starts by determining a\nconstant c such that\n\nmax\n\nf (x)\ng(x)\n\n\u2026 c\n\nit then proceeds as follows:\n\n1. generate y having density g.\n2. generate a random number u.\n3. if u \u2026 f (y)/cg(y), set x = y and stop.\n4. return to step 1.\n\nthe number of passes through step 1 is a geometric random variable with mean c.\n\nstandard normal random variables can be efficiently simulated by the rejection\nmethod (with g being exponential with mean 1) or by the technique known as the\npolar algorithm.\n\nto estimate a quantity \u03b8, one often generates the values of a partial sequence\nof random variables whose expected value is \u03b8. the efficiency of this approach is\nincreased when these random variables have a small variance. three techniques that\ncan often be used to specify random variables with mean \u03b8 and relatively small vari-\nances are\n\n1. the use of antithetic variables,\n2. the use of conditional expectations, and\n3. the use of control variates.\n\nproblems\n\n10.1. the following algorithm will generate a random\npermutation of the elements 1, 2, . . . , n. it is some-\nwhat faster than the one presented in example 1a\nbut is such that no position is fixed until the algo-\nrithm ends. in this algorithm, p(i) can be inter-\npreted as the element in position i.\nstep 1. set k = 1.\nstep 2. set p(1) = 1.\nstep 3. if k = n, stop. otherwise, let k = k + 1.\n\nstep 4. generate a random number u and let\n\np(k) = p([ku] + 1)\n\np([ku] + 1) = k\n\ngo to step 3.\n(a) explain in words what the algorithm is doing.\n(b) show that at iteration k\u2014that is, when the\nvalue of p(k) is initially set\u2014p(1), p(2), . . . ,\np(k) is a random permutation of 1, 2, . . . , k.\n\n "}, {"Page_number": 469, "text": "454\n\nchapter 10\n\nsimulation\n\nhint: use induction and argue that\n\npk{i1, i2, . . . , ij\u22121, k, ij, . . . , ik\u22122, i}\n\n= pk\u22121{i1, i2, . . . , ij\u22121, i, ij, . . . , ik\u22122} 1\nk\n= 1\nk!\n\nby the induction hypothesis\n\n10.2. develop a technique for simulating a random vari-\n\nable having density function\n\nf (x) =\n\ne2x\n\u22122x\ne\n\n\u2212q < x < 0\n0 < x < q\n\n10.3. give a technique for simulating a random variable\n\nhaving the probability density function\n\n0\n\n1\n2\n1\n2\n0\n\n\u23a7\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9\n\u23a7\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9\n\nf (x) =\n\n(cid:3)\n\n(cid:2)\n(x \u2212 2)\n2 \u2212 x\n3\n\n2 \u2026 x \u2026 3\n\n3 < x \u2026 6\n\notherwise\n\nf(x) =\n\n0\n1\n2\n1\n2\n1\n\n+ x\n6\n+ x2\n32\n\nx \u2026 \u22123\n\u22123 < x < 0\n\n0 < x \u2026 4\nx > 4\n\n10.4. present a method for simulating a random variable\n\nhaving distribution function\n\n10.5. use the inverse transformation method to present\nan approach for generating a random variable\nfrom the weibull distribution\n\u2212at\u03b2\n\nf(t) = 1 \u2212 e\n\nt \u00fa 0\n\n10.6. give a method for simulating a random variable\n\nhaving failure rate function\n(a) \u03bb(t) = c;\n(b) \u03bb(t) = ct;\n(c) \u03bb(t) = ct2;\n(d) \u03bb(t) = ct3.\n\n10.7. let f be the distribution function\n\nf(x) = xn\n\n0 < x < 1\n\n(a) give a method for simulating a random vari-\nable having distribution f that uses only a sin-\ngle random number.\n\n(b) let u1, . . . , un be independent random num-\n\nbers. show that\n\np{max(u1, . . . , un) \u2026 x} = xn\n\n(c) use part (b) to give a second method of sim-\nulating a random variable having distribu-\ntion f.\n\n10.8. suppose it is relatively easy to simulate from fi for\n\neach i = 1, . . . , n. how can we simulate from\n\n(a) f(x) = n$\n(b) f(x) = 1 \u2212 n$\n\ni=1\n\nfi(x)?\n\n[1 \u2212 fi(x)]?\n\ni=1\n\n10.9. suppose we have a method for simulating random\nvariables from the distributions f1 and f2. explain\nhow to simulate from the distribution\n\nf(x) = pf1(x) + (1 \u2212 p)f2(x) 0 < p < 1\n\ngive a method for simulating from\n\nf(x) =\n\n(1 \u2212 e\n(1 \u2212 e\n\n\u22123x) + 2\n\u22123x) + 2\n3\n\n3 x 0 < x \u2026 1\n\nx > 1\n\n\u23a7\u23a8\n\u23a9 1\n\n3\n1\n3\n\n10.10. in example 2c we simulated the absolute value\nof a unit normal by using the rejection procedure\non exponential random variables with rate 1. this\nraises the question of whether we could obtain a\nmore efficient algorithm by using a different expo-\nnential density\u2014that is, we could use the density\ng(x) = \u03bbe\n\u2212\u03bbx. show that the mean number of iter-\nations needed in the rejection scheme is minimized\nwhen \u03bb = 1.\n10.11. use the rejection method with g(x) = 1, 0 < x < 1,\nto determine an algorithm for simulating a random\nvariable having density function\n\n60x3(1 \u2212 x)2\n0\n\n0 < x < 1\notherwise\n\n10.12. explain how you could use random numbers to\n1\n0 k(x) dx, where k(x) is an arbitrary\n\napproximate\nfunction.\nhint: if u is uniform on (0, 1), what is e[k(u)]?\n\n10.13. let (x, y) be uniformly distributed in the circle\nof radius 1 centered at the origin. its joint density\nis thus\n\nf (x, y) = 1\n\n\u03c0\n\n0 \u2026 x2 + y2 \u2026 1\n\nlet r = (x2 + y2)1/2 and \u03b8 = tan\n\u22121(y/x) denote\nthe polar coordinates of (x, y). show that r and\n\u03b8 are independent, with r2 being uniform on (0, 1)\nand \u03b8 being uniform on (0, 2\u03c0 ).\n10.14. in example 4a, we showed that\n\ne[(1 \u2212 v2)1/2] = e[(1 \u2212 u2)1/2] = \u03c0\n4\n\n0\n\nf (x) =\n-\n\n "}, {"Page_number": 470, "text": "when v is uniform (\u22121, 1) and u is uniform (0, 1).\nnow show that\n\nvar[(1 \u2212 v2)1/2] = var[(1 \u2212 u2)1/2]\n\nand find their common value.\n\n10.15. (a) verify that the minimum of (4.1) occurs when\n\na is as given by (4.2).\n\n(b) verify that the minimum of (4.1) is given by\n\n(4.3).\n\n10.16. let x be a random variable on (0, 1) whose den-\n1\nsity is f (x). show that we can estimate\n0 g(x) dx\nby simulating x and then taking g(x)/f (x) as\nour estimate. this method, called importance sam-\npling, tries to choose f similar in shape to g, so that\ng(x)/f (x) has a small variance.\n\n-\n\nreference 455\n\nself-test problems and exercises\n\n10.1. the random variable x has probability density\n\nfunction\n\nf (x) = cex\n\n0 < x < 1\n(a) find the value of the constant c.\n(b) give a method for simulating such a random\n\nvariable.\n\n10.2. give an approach for simulating a random variable\n\nhaving probability density function\n\nf (x) = 30(x2 \u2212 2x3 + x4)\n\n0 < x < 1\n\np1 = .15 p2 = .2 p3 = .35 p4 = .30\n\n10.4. if x is a normal random variable with mean \u03bc\nand variance \u03c3 2, define a random variable y that\nhas the same distribution as x and is negatively\ncorrelated with it.\n\n10.5. let x and y be independent exponential random\n\nvariables with mean 1.\n(a) explain how we could use simulation to esti-\n\nmate e[exy].\n\n10.3. give an efficient algorithm to simulate the value of\na random variable with probability mass function\n\n(b) show how to improve\n\nestimation\napproach in part (a) by using a control variate.\n\nthe\n\nreference\n\n[1] ross, s. m. simulation. 4th ed. san diego: academic press, inc., 2006.\n\n "}, {"Page_number": 471, "text": "this page intentionally left blank \n\n "}, {"Page_number": 472, "text": "answers to selected problems\n\n2. 1296\n9. 27,720\n12. 24,300,000; 17,100,720\n\nchapter 1\n1. 67,600,000; 19,656,000\n8. 120; 1260; 34,650\n72\n72; 144\n17. 604,800\n25. 52!/(13!)4\n31. 165; 35\n\n18. 600\n\n27. 27,720\n\n32. 1287; 14,112\n\n19. 896; 1000; 910\n\n28. 65,536; 2520\n33. 220; 572\n\n4. 24; 4\n\n5. 144; 18\n\n6. 2401\n\n10. 40,320; 10,080; 1152; 2880; 384\n13. 190\n16. 42; 94\n\n14. 2,598,960\n\n7. 720; 72; 144;\n11. 720;\n\n20. 36; 26\n\n21. 35\n29. 12,600; 945\n\n22. 18\n30. 564,480\n\n23. 48\n\n11. 70; 2\n\n10. .4; .1\n\n14. 1.057\n19. 5/18\n\nchapter 2\n9. 74\n10,000\n18. .048\n27. .0888; .2477; .1243; .2099\n36. .0045; .0588\n41. .5177\n52. .09145; .4268\n\n44. .3; .2; .1\n\n53. 12/35\n\n37. .0833; .5\n46. 5\n\n15. .0020; .4226; .0475; .0211; .00024\n20. .9052\n\n12. .5; .32; 149/198\n22. (n + 1)/2n\n30. 1/18; 1/6; 1/2\n39. .48\n\n38. 4\n48. 1.0604 * 10\n\n13. 20,000; 12,000; 11,000; 68,000;\n\n17. 9.10947 * 10\n\n\u22126\n\n26. .492929\n\n25. .4\n\n23. 5/12\n31. 2/9; 1/9\n40. 1/64; 21/64; 36/64; 6/64\n\u22123\n\n33. 70/323\n\n49. .4329\n\n50. 2.6084 * 10\n\n\u22126\n\n54. .0511\n\n55. .2198; .0343\n\n9. 7/11\n\n16. .9835\n\n20. .4; 1/26\n\n26. 20/21; 40/41\n\nchapter 3\n1. 1/3\n2. 1/6; 1/5; 1/4; 1/3; 1/2; 1\n8. 1/2\n10. .22\n15. .4848\n41.18\n1/2\n49/76\n40. 3/13; 5/13; 5/52; 15/52\n50. 17.5; 38/165; 17/33\n55. 9\n12/27; 3/5; 9/25\n1/2\n73. 1/16; 1/32; 5/16; 1/4; 31/32\n79. .5550\n89. 97/142; 15/26; 33/102\n\n34. 27/31\n\n81. .9530\n\n3. .339\n11. 1/17; 1/33\n\n5. 6/91\n\n17. .0792; .264\n\n6. 1/2\n\n7. 2/3\n\n12. .504; .3629\n\n14. 35/768; 210/768\n\n18. .331; .383; .286; 48.62\n\n19. 44.29;\n\n21. .496; 3/14; 9/62\n\n22. 5/9; 1/6; 5/54\n\n23. 4/9; 1/2\n\n24. 1/3;\n\n28. 3/128; 29/1536\n36. 1/2\n\n35. .62, 10/19\n\n41. 43/459\n\n29. .0893\n37. 1/3; 1/5; 1\n43. 4/9\n\n42. 34.48\n\n51. .65; 56/65; 8/65; 1/65; 14/35; 12/35; 9/35\n57. (c) 2/3\n\n60. 2/3; 1/3; 3/4\n\n30. 7/12; 3/5\n\n33. .76,\n\n38. 12/37\n45. 1/11\n\n39. 46/185\n48. 2/3\n\n52. .11; 16/89;\n\n61. 1/6; 3/20\n65. 9/13;\n78. p2/(1 \u2212 2p + 2p2)\n\n71. 38/64; 13/64; 13/64\n\n74. 9/19\n\n75. 3/4, 7/12\n\n83. .5; .6; .8\n\n84. 9/19; 6/19; 4/19; 7/15; 53/165; 7/33\n\n69. 9; 9; 18; 110; 4; 4; 8; 120 all over 128\n\n70. 1/9; 1/18\n\nchapter 4\n1. p(4) = 6/91; p(2) = 8/91; p(1) = 32/91; p(0) = 1/91; p(\u22121) = 16/91;\np(\u22122) = 28/91\n\n4. (a) 1/2; 5/18; 5/36; 5/84; 5/252; 1/252; 0; 0; 0; 0\n\n5. n \u2212 2i;\n\n457\n\n "}, {"Page_number": 473, "text": "458 answers to selected problems\n\n12. p(4) = 1/16;\n13. p(0) = .28;\n14. p(0) = 1/2;\n\n20. .5918; no; \u2212.108\n25. .46, 1.3\n\ni = 0, . . . , n\n6. p(3) = p(\u22123) = 1/8; p(1) = p(\u22121) = 3/8\np(3) = 1/8; p(2) = 1/16; p(0) = 1/2; p(\u2212i) = p(i); p(0) = 1\np(500) = .27, p(1000) = .315; p(1500) = .09; p(2000) = .045\np(1) = 1/6; p(2) = 1/12; p(3) = 1/20; p(4) = 1/5\n1/10; 1/5; 1/10; 1/10\nmaximum = 23/72\n\u2217\n31. p\n40. 11/243\n53. 1 \u2212 e\n60. .8886\n2/(2n \u2212 2); e\n\u22121\n70. p + (1 \u2212 p)e\n160/729; 160/729\n82. .3439\n\n32. 11 \u2212 10(.9)10\n42. p \u00fa 1/2\n\u2212.6; 1 \u2212 e\n\u2212219.18\n61. .4082\n67. 2/n; (2n \u2212 3)/(n \u2212 1)2; e\n\u22122\n\u2212\u03bbt\n73. 5.8125\n71. .1500; .1012\n78. 18(17)n\u22121/(35)n\n81. 3/10; 5/6; 75/138\n\n26. 11/2; 17/5\n35. \u2212.067; 1.089\n50. 1/10; 1/10\n\n17. 1/4; 1/6; 1/12; 1/2\n27. a(p + 1/10)\n37. 82.2; 84.5\n\u2212.2; 1 \u2212 1.2e\n59. .3935; .3033; .0902\n65. .3935; .2293; .3935\n\u221210e\n\u22125\n\n68. e\n74. 32/243; 4864/6561;\n\n28. 3/5\n39. 3/8\n\u2212.2\n\n24. p = 11/18;\n\n45. 3\n56. 253\n\n57. .5768; .6070\n\n63. .0821; .2424\n\n21. 39.28; 37\n\n83. 1.5\n\n33. 3\n\n51. e\n\n66. 2/(2n \u2212 1);\n\n19. 1/2;\n\nchapter 5\n\u22125/2\n2. 3.5e\n10. 2/3; 2/3\n16. (.9938)10\n23. .9258; .1762\n\u22121; 1/3\n34. e\n\n3. no; no\n11. 2/5\n\n4. 1/2\n\n13. 2/3; 1/3\n\n18. 22.66\n\n19. 14.56\n\n26. .0606; .0525\n\n38. 3/5\n\n40. 1/y\n\n5. 1 \u2212 (.01)1/5\n\n6. 4, 0, q 7. 3/5; 6/5\n15. .7977; .6827; .3695; .9522; .1587\n\u22121; e\n\n20. .9994; .75; .977\n29. .9993\n\n22. 9.5; .0019\n\u22121/2\n32. e\n\n28. .8363\n\n8. 2\n\n4. 25/169; 40/169; 40/169; 64/169\n\nchapter 6\n2. (a) 14/39; 10/39; 10/39; 5/39 (b) 84; 70; 70; 70; 40; 40; 40; 15 all divided by 429\n7. p(i, j) = p2(1 \u2212 p)i+j\n3. 15/26; 5/26; 5/26; 1/26\n9. (12x2 + 6x)/7; 15/56; .8625; 5/7; 8/7\n8. c = 1/8; e[x] = 0\n10. 1/2; 1 \u2212 e\n\u2212a\n16. n(1/2)n\u22121\n\u22125\n18. 7/9\n17. 1/3\n11. .1458\n12. 39.3e\n\u22121/i!\n\u2212t;\n28. 1\n25. e\n21. 2/5; 2/5\n19. 1/2\n2e\n\u22122; 1 \u2212 3e\n1\u22123e\n\u22122\n\u22122\n29. .0326\n\u2212xy; e\n\u2212x\n35. 5/13; 8/13\n42. 1/2 + 3y/(4x) \u2212 y3/(4x3)\n48. 1 \u2212 e\n\u22125\u03bba;\n(1 \u2212 e\n\u2212\u03bba)5\n53. r\n\n23. 1/2; 2/3; 1/20; 1/18\n41. (y + 1)2xe\n46. (1 \u2212 2d/l)3\n56. (a) u/(\u03bd + 1)2\n\n22. no; 1/3\n30. .3772; .2061\n\n32. e\n\u2212x(y+1); xe\n47. .79297\n\n36. 1/6; 5/6; 1/4; 3/4\n\n31. .0829; .3766\n\n13. 1/6; 1/2\n\n15. \u03c0/4\n\n52. r/\u03c0\n\n2. 324; 199.6\n8. (1 \u2212 (1 \u2212 p)n)/p\n\nchapter 7\n1. 52.5/12\n3. 1/2; 1/4; 0\n10. .6; 0\n4.2\n12. (3n2 \u2212 n)/(4n \u2212 2), 3n2/(4n \u2212 2)\n21. .9301; 87.5755\n4; 123\n31. 175/6\n36. \u2212n/36\n35\n37. 0\n45. 1/2; 0\n3240/6137\n51. y3/4\n53. 12\n67. x[1 + (2p \u2212 1)2]n\n66. 218\n72. 1/i; [i(i + 1)]\n79. .176; .141\n\n47. 1/(n \u2212 1)\n56. n(1 \u2212 e\n\n22. 14.7\n33. 14\n38. 1/8\n\n54. 8\n\n23. 147/110\n\n34. 20/19; 360/361\n41. 6; 112/33\n\n48. 6; 7; 5.8192\n\u221210/n)\n57. 12.5\n\u22121; q 73. \u03bc; 1 + \u03c3 2; yes; \u03c3 2\n\n69. 1/2; 1/16; 2/81\n\n42. 100/19; 16,200/6137; 10/19;\n50. 2y2\n\n49. 6.06\n63. \u221296/145\n\n65. 5.16\n\n70. 1/2, 1/3\n\n6. 35\n\n7. .9; 4.9;\n\n4. 1/6; 1/4; 1/2\n5. 3/2\n11. 2(n \u2212 1)p(1 \u2212 p)\n14. m/(1 \u2212 p)\n\n15. 1/2\n\n26. n/(n + 1); 1/(n + 1)\n35. 21.2; 18.929; 49.214\n\n18. 4\n\n29. 437\n\n35 ; 12;\n\n "}, {"Page_number": 474, "text": "answers to selected problems 459\n\nchapter 8\n1. \u00fa19/20\n7. .3085\n.2514; .2514\n.4267; .1093; .112184\n\n2. 15/17; \u00fa3/4; \u00fa10\n9. (327)2\n8. .6932\n\n14. n \u00fa 23\n\n3. \u00fa3\n10. 117\n\n4. \u20264/3; .8428\n11. \u00fa.057\n18. \u2026.2\n\n5. .1416\n\n6. .9431\n\n13. .0162; .0003;\n\n23. .769; .357;\n\n16. .013; .018; .691\n\nchapter 9\n1. 1/9; 5/9\n15. 5.5098\n\n3. .9735; .9098; .7358; .5578\n\n10. (b) 1/6\n\n14. 2.585; .5417; 3.1267\n\n "}, {"Page_number": 475, "text": "this page intentionally left blank \n\n "}, {"Page_number": 476, "text": "solutions to self-test problems and\nexercises\n\nchapter 1\n\n1.1. (a) there are 4! different orderings of the letters c, d, e, f. for each of these\norderings, we can obtain an ordering with a and b next to each other by\ninserting a and b, either in the order a, b or in the order b, a, in any\nof 5 places, namely, either before the first letter of the permutation of\nc, d, e, f, or between the first and second, and so on. hence, there are\n2 \u00b7 5 \u00b7 4! = 240 arrangements. another way of solving this problem is\nto imagine that b is glued to the back of a. then there are 5! orderings\nin which a is immediately before b. since there are also 5! orderings in\nwhich b is immediately before a, we again obtain a total of 2 \u00b7 5! = 240\ndifferent arrangements.\n(b) there are 6! = 720 possible arrangements, and since there are as many\n\nwith a before b as with b before a, there are 360 arrangements.\n\n(c) of the 720 possible arrangements, there are as many that have a before\nb before c as have any of the 3! possible orderings of a, b, and c. hence,\nthere are 720/6 = 120 possible orderings.\n\n(d) of the 360 arrangements that have a before b, half will have c before d\nand half d before c. hence, there are 180 arrangements having a before\nb and c before d.\n(e) gluing b to the back of a and d to the back of c yields 4! = 24 different\norderings in which b immediately follows a and d immediately follows\nc. since the order of a and b and of c and d can be reversed, there are\n4 \u00b7 24 = 96 different arrangements.\n(f) there are 5! orderings in which e is last. hence, there are 6! \u2212 5! = 600\n\norderings in which e is not last.\n\n1.2. 3! 4! 3! 3!, since there are 3! possible orderings of countries and then the coun-\n1.3. (a) 10 \u00b7 9 \u00b7 8 = 720\n\ntrymen must be ordered.\n(b) 8 \u00b7 7 \u00b7 6 + 2 \u00b7 3 \u00b7 8 \u00b7 7 = 672. the result of part (b) follows because\nthere are 8 \u00b7 7 \u00b7 6 choices not including a or b and there are 3 \u00b7 8 \u00b7 7\nchoices in which a specified one of a and b, but not the other, serves. the\nlatter follows because the serving member of the pair can be assigned to\nany of the 3 offices, the next position can then be filled by any of the other\n8 people, and the final position by any of the remaining 7.\n\n(c) 8 \u00b7 7 \u00b7 6 + 3 \u00b7 2 \u00b7 8 = 384.\n(d) 3 \u00b7 9 \u00b7 8 = 216.\n(e) 9 \u00b7 8 \u00b7 7 + 9 \u00b7 8 = 576.\n\n461\n\n "}, {"Page_number": 477, "text": "462\n\nsolutions to self-test problems and exercises\n\n(cid:2)\n(cid:2)\n\n(cid:3)\n(cid:3)(cid:2)\n(cid:3)\n\n10\n7\n5\n3\n\n(cid:3)\n\n+\n\n5\n4\n= 210\n(cid:2)\n(cid:3)\n\n1.4. (a)\n\n(b)\n\n(cid:2)\n\n1.5.\n\n7\n\n3, 2, 2\n\n(cid:2)\n\n(cid:3)(cid:2)\n\n(cid:3)\n\n5\n4\n\n5\n3\n\n+\n\n(cid:2)\n\n(cid:3)(cid:2)\n\n(cid:3)\n\n5\n5\n\n5\n2\n\n7\n3\n\n(b)\n\n(cid:3)\n\n1.6. there are\n\nitems not selected.\n\n= 35 choices of the three places for the letters. for each choice,\nthere are (26)3(10)4 different license plates. hence, altogether there are 35 \u00b7\n(26)3 \u00b7 (10)4 different plates.\n1.7. any choice of r of the n items is equivalent to a choice of n \u2212 r, namely, those\n(cid:2)\n1.8. (a) 10 \u00b7 9 \u00b7 9\u00b7\u00b7\u00b7 9 = 10 \u00b7 9n\u22121\nn\ni\n\n(cid:3)\n(cid:2)\n9n\u2212i, since there are\nn\n(cid:3)\n(cid:2)\ni\nthen each of the other n \u2212 i positions can be any of the digits 1, . . . , 9.\n(cid:3)\n(cid:2)\n3n\n3\n(cid:3)(cid:2)\n(cid:2)\nn\n3\n(cid:3)\n(cid:2)\n\nchoices of the i places to put the zeroes and\n\n(cid:3)(cid:2)\n(cid:2)\n\n= 3n2(n \u2212 1)\n\n(cid:3)(cid:2)\n(cid:3)\n\n1.9. (a)\n\n(b) 3\n\n(cid:3)\n\n(c)\n\n+ 3n2(n \u2212 1) + n3\n\n(cid:3)\n\n(cid:2)\n\n3n\n3\n\n= 3\n(cid:3)\n(cid:2)\n5\nthere are 9\n2\nthere are 7 \u00b7\n7 \u00b7\nare\n\n1.10. there are 9 \u00b7 8 \u00b7 7 \u00b7 6 \u00b7 5 numbers in which no digit is repeated. there are\n\u00b7 8 \u00b7 7 \u00b7 6 numbers in which only one specified digit appears twice, so\n\u00b7 8 \u00b7 7 \u00b7 6 numbers in which only a single digit appears twice.\n(cid:3)\n5!\n2!2! numbers in which two specified digits appear twice, so there\n5!\n2!2! numbers in which two digits appear twice. thus, the answer is\n\n(cid:2)\n\n3\n1\n(d) n3\n(e)\n\nn\n2\n\nn\n1\n\nn\n3\n\n5\n2\n\n2\n1\n\n9\n2\n\n9 \u00b7 8 \u00b7 7 \u00b7 6 \u00b7 5 + 9\n\n\u00b7 8 \u00b7 7 \u00b7 6 +\n\n(cid:2)\n\n(cid:3)\n\n5\n2\n\n(cid:2)\n\n(cid:3)\n\n9\n2\n\n7 \u00b7 5!\n2!2!\n\n1.11. (a) we can regard this as a seven-stage experiment. first choose the 6 mar-\nried couples that have a representative in the group, and then select one of\nthe members of each of these couples. by the generalized basic principle\nof counting, there are\n\n26 different choices.\n\n(cid:19)\n\n(b) first select the 6 married couples that have a representative in the group,\nand then select the 3 of those couples that are to contribute a man. hence,\nthere are\n4!3!3! different choices. another way to solve this is\nto first select 3 men and then select 3 women not related to the selected\nmen. this shows that there are\n\n(cid:19) = 10!\n\n3!3!4! different choices.\n\n(cid:19)(cid:18)\n\n(cid:18)\n\n10\n6\n\n10\n3\n\n6\n3\n\n7\n3\n\n(cid:18)\n(cid:19) = 10!\n\n10\n6\n\n(cid:18)\n\n(cid:19)(cid:18)\n\n "}, {"Page_number": 478, "text": "(cid:2)\n\n(cid:3)(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)(cid:2)\n\n(cid:3)\n\n7\n2\n\n8\n4\n\n7\n3\n\n8\n3\n\n+\n\n1.12.\n\nsolutions to self-test problems and exercises 463\n= 3430. the first term gives the number of committees\nthat have 3 women and 3 men; the second gives the number that have 4 women\nand 2 men.\n1.13. (number of solutions of x1 + \u00b7\u00b7\u00b7 + x5 = 4)(number of solutions of x1 + \u00b7\u00b7\u00b7 +\nx5 = 5)(number of solutions of x1 + \u00b7\u00b7\u00b7 + x5 = 6) =\n(cid:2)\nk(cid:9)\nis the number of subsets of size n\nj=n\nfrom the set of numbers {1, . . . , k} in which j is the largest element in the sub-\nis just the total number of subsets of size n\nset. consequently,\n\n(cid:2)\nj \u2212 1\nn \u2212 1\n(cid:3)\n(cid:2)\nj \u2212 1\nn \u2212 1\n\npositive vectors whose sum is j, there must be\n\n(cid:2)\n(cid:3)\nj \u2212 1\nn \u2212 1\n\n1.14. since there are\n\nsuch vectors. but\n\nj \u2212 1\nn \u2212 1\n\n(cid:3)(cid:2)\n\n(cid:3)(cid:2)\n\nk(cid:9)\n\n10\n4\n\n(cid:2)\n\n(cid:3)\n\n(cid:3)\n\n(cid:3)\n\n8\n4\n\n9\n4\n\n.\n\nfrom a set of size k, showing that the preceding answer is equal to\n\n1.15. let us first determine the number of different results in which k people pass.\ndifferent groups of size k and k! possible orderings of\n\nbecause there are\n\n(cid:2)\n\n(cid:3)\n\nk\nn\n\n.\n\nj=n\n\n(cid:2)\n\n(cid:3)\n\nn\nk\n\nn(cid:9)\n\n(cid:2)\n(cid:18)\n\nn\nk\n\nk! possible results in which k people\n\n(cid:3)\n(cid:2)\n(cid:3)\n(cid:19) = 4845. because the number of these\n(cid:19) = 1365, the number that\n\nk! possible results.\n\n(cid:18)\n\nn\nk\n20\n4\n\ntheir scores, it follows that there are\n\npass. consequently, there are\n\nk=0\n1.16. the number of subsets of size 4 is\n\n(cid:18)\n\n(cid:19)(cid:18)\nthat contain none of the first five elements is\ncontain at least one is 3480. another way to solve this problem is to note that\n15\n4\u2212i\nthere are\nthat contain exactly i of the first five elements and sum this\nfor i = 1, 2, 3, 4.\n\n(cid:19)\n\n15\n4\n\n5\ni\n\n1.17. multiplying both sides by 2, we must show that\n\nn(n \u2212 1) = k(k \u2212 1) + 2k(n \u2212 k) + (n \u2212 k)(n \u2212 k \u2212 1)\n\nthis follows because the right side is equal to\n\nk2(1 \u2212 2 + 1) + k(\u22121 + 2n \u2212 n \u2212 n + 1) + n(n \u2212 1)\n\nk\n2\n\n(cid:19)\n\n(cid:18)\n\n(cid:19)\n\nfor a combinatorial argument, consider a group of n items and a subgroup\nis the number of subsets of size 2 that contain\nof k of the n items. then\n2 items from the subgroup of size k, k(n \u2212 k) is the number that contain 1\nn\u2212k\nitem from the subgroup, and\nis the number that contain 0 items from the\n2\nsubgroup. adding these terms gives the total number of subgroups of size 2,\nnamely,\n1.18. there are 3 choices that can be made from families consisting of a single parent\nand 1 child; there are 3 \u00b7 1 \u00b7 2 = 6 choices that can be made from families\nconsisting of a single parent and 2 children; there are 5 \u00b7 2 \u00b7 1 = 10 choices\nthat can be made from families consisting of 2 parents and a single child; there\nare 7 \u00b7 2 \u00b7 2 = 28 choices that can be made from families consisting of 2 parents\nand 2 children; there are 6 \u00b7 2 \u00b7 3 = 36 choices that can be made from families\nconsisting of 2 parents and 3 children. hence, there are 80 possible choices.\n\nn\n2\n\n.\n\n(cid:18)\n\n(cid:19)\n(cid:18)\n\n "}, {"Page_number": 479, "text": "464\n\nsolutions to self-test problems and exercises\n\n(cid:18)\n\n1.19. first choose the 3 positions for the digits, and then put in the letters and digits.\n\n(cid:19) \u00b7 26 \u00b7 25 \u00b7 24 \u00b7 23 \u00b7 22 \u00b7 10 \u00b7 9 \u00b7 8 different plates. if the\n\nthus, there are\ndigits must be consecutive, then there are 6 possible positions for the digits,\nshowing that there are now 6 \u00b7 26 \u00b7 25 \u00b7 24 \u00b7 23 \u00b7 22 \u00b7 10 \u00b7 9 \u00b7 8 different\nplates.\n\n8\n3\n\nchapter 2\n\n2.1. (a) 2 \u00b7 3 \u00b7 4 = 24\n\n(b) 2 \u00b7 3 = 6\n(c) 3 \u00b7 4 = 12\n(d) ab = {(c, pasta, i), (c, rice, i), (c, potatoes, i)}\n(e) 8\n(f) abc = {(c, rice, i)}\n\n2.2. let a be the event that a suit is purchased, b be the event that a shirt is pur-\n\nchased, and c be the event that a tie is purchased. then\n\np(a \u222a b \u222a c) = .22 + .30 + .28 \u2212 .11 \u2212 .14 \u2212 .10 + .06 = .51\n\n(a) 1 \u2212 .51 = .49\n(b) the probability that two or more items are purchased is\n\np(ab \u222a ac \u222a bc) = .11 + .14 + .10 \u2212 .06 \u2212 .06 \u2212 .06 + .06 = .23\n\nhence, the probability that exactly 1 item is purchased is .51 \u2212 .23 = .28.\n\n2.3. by symmetry, the 14th card is equally likely to be any of the 52 cards; thus, the\nprobability is 4/52. a more formal argument is to count the number of the 52!\noutcomes for which the 14th card is an ace. this yields\n= 4\n52\n\np = 4 \u00b7 51 \u00b7 50\u00b7\u00b7\u00b7 2 \u00b7 1\n\n(52)!\n\nletting a be the event that the first ace occurs on the 14th card, we have\n\np(a) = 48 \u00b7 47\u00b7\u00b7\u00b7 36 \u00b7 4\n52 \u00b7 51\u00b7\u00b7\u00b7 40 \u00b7 39\n\n= .0312\n\n2.4. let d denote the event that the minimum temperature is 70 degrees. then\n\np(a \u222a b) = p(a) + p(b) \u2212 p(ab) = .7 \u2212 p(ab)\np(c \u222a d) = p(c) + p(d) \u2212 p(cd) = .2 + p(d) \u2212 p(dc)\n\nsince a \u222a b = c \u222a d and ab = cd, subtracting one of the preceding\nequations from the other yields\n\nor p(d) = .5.\n\n2.5. (a)\n\n(b)\n\n52 \u00b7 48 \u00b7 44 \u00b7 40\n52 \u00b7 51 \u00b7 50 \u00b7 49\n52 \u00b7 39 \u00b7 26 \u00b7 13\n52 \u00b7 51 \u00b7 50 \u00b7 49\n\n0 = .5 \u2212 p(d)\n\n= .6761\n= .1055\n\n "}, {"Page_number": 480, "text": "2.6. let r be the event that both balls are red, and let b be the event that both are\n\nsolutions to self-test problems and exercises 465\n\n+ 3 \u00b7 6\n6 \u00b7 10\n\n= 1/2\n\nblack. then\n\n8\n\n1\n\n7\n\n8\n\n2\n\n6\n\n(c)\n\n\u22128\n\n(b)\n\n2.7. (a)\n\n\u22128 + 3.3 * 10\n\np(r \u222a b) = p(r) + p(b) = 3 \u00b7 4\n6 \u00b7 10\n\u239e\n1\u239b\n\u23a0 = 1.3 * 10\n\u239d40\n\u239e\n\u239b\n\u239b\n\u239e\n\u239d32\n\u239d8\n\u23a0\n\u23a0\n\u239b\n\u239e\n\u23a0 = 3.3 * 10\n\u22126\n\u239d40\n\u239e\n\u239b\n\u239b\n\u239e\n\u239d32\n\u239d8\n\u23a0\n\u23a0\n\u239b\n\u239e\n\u23a0 + 1.3 * 10\n\u239d40\n(cid:3)\n(cid:2)\n3 \u00b7 4 \u00b7 4 \u00b7 3\n\u239e\n\u239b\n\u239b\n\u239e\n\u239d4\n\u23a0\n\u23a0\n\u239d4\n\u239b\n\u239e\n\u23a0 = .0360\n\u239d14\n\u239b\n\u239e\n\u239d8\n\u23a0\n\u239b\n\u239e\n\u23a0 = .0699\n\u239d14\n2.9. let s = n(cid:10)\n\n= .1439\n\n2.8. (a)\n\n14\n4\n\n(b)\n\n(c)\n\n8\n\n2\n\n2\n\n4\n\n4\n\n4\n\n\u22126 = 1.8 * 10\n\u22124\n\ni=1\n\nai, and consider the experiment of randomly choosing an element\nof s. then p(a) = n(a)/n(s), and the results follow from propositions 4.3\nand 4.4.\n2.10. since there are 5! = 120 outcomes in which the position of horse number\n1 is specified, it follows that n(a) = 360. similarly, n(b) = 120, and\nn(ab) = 2 \u00b7 4! = 48. hence, from self-test problem 9, we obtain\nn(a \u222a b) = 432.\n2.11. one way to solve this problem is to start with the complementary probability\nthat at least one suit does not appear. let ai, i = 1, 2, 3, 4, be the event that no\ncards from suit i appear. then\n\n\u239e\n\u23a0 =\n\n(cid:6)\n\n\u239b\n\u239d 4(cid:14)\n\ni=1\n\np\n\nai\n\n(cid:6)\n\n(cid:6)\n\np(ai) \u2212\n\np(aiaj) + \u00b7\u00b7\u00b7 \u2212 p(a1a2a3a4)\n\ni\n\nj\n\ni:i<j\n\n "}, {"Page_number": 481, "text": "466\n\nsolutions to self-test problems and exercises\n\n(cid:2)\n(cid:2)\n(cid:2)\n(cid:2)\n\n(cid:3)\n(cid:3) \u2212\n(cid:3)\n(cid:3) \u2212 6\n\n(cid:2)\n(cid:2)\n(cid:2)\n\n4\n2\n\n39\n5\n52\n5\n39\n5\n52\n5\n\n26\n5\n52\n5\n\n(cid:3)(cid:2)\n(cid:2)\n(cid:3)\n(cid:3) + 4\n\n(cid:3)\n(cid:3) +\n(cid:2)\n(cid:2)\n\n26\n5\n52\n5\n\n(cid:2)\n\n(cid:3)(cid:2)\n(cid:2)\n\n(cid:3)\n(cid:3)\n\n13\n5\n52\n5\n\n4\n3\n\n(cid:3)\n(cid:3)\n\n13\n5\n52\n5\n\n= 4\n\n= 4\n\nthe desired probability is then 1 minus the preceding. another way to solve is\nto let a be the event that all 4 suits are represented, and then use\np(a) = p(n, n, n, n, o) + p(n, n, n, o, n) + p(n, n, o, n, n) + p(n, o, n, n, n)\n\nwhere p(n, n, n, o, n), for instance, is the probability that the first card is from a\nnew suit, the second is from a new suit, the third is from a new suit, the fourth\nis from an old suit (that is, one which has already appeared) and the fifth is\nfrom a new suit. this gives\n\n52 \u00b7 51 \u00b7 50 \u00b7 49 \u00b7 48\n\np(a) = 52 \u00b7 39 \u00b7 26 \u00b7 13 \u00b7 48 + 52 \u00b7 39 \u00b7 26 \u00b7 36 \u00b7 13\n+ 52 \u00b7 39 \u00b7 24 \u00b7 26 \u00b7 13 + 52 \u00b7 12 \u00b7 39 \u00b7 26 \u00b7 13\n52 \u00b7 51 \u00b7 50 \u00b7 49 \u00b7 48\n= 52 \u00b7 39 \u00b7 26 \u00b7 13(48 + 36 + 24 + 12)\n= .2637\n\n52 \u00b7 51 \u00b7 50 \u00b7 49 \u00b7 48\n\n(cid:2)\n\n(cid:3)(cid:2)\n\n(cid:3)\n\n2.12. there are (10)!/25 different divisions of the 10 players into a first roommate\npair, a second roommate pair, and so on. hence, there are (10)!/(5!25) divi-\nways of choosing the front-\nsions into 5 roommate pairs. there are\ncourt and backcourt players to be in the mixed roommate pairs and then\n2 ways of pairing them up. as there is then 1 way to pair up the\nremaining two backcourt players and 4!/(2!22) = 3 ways of making two\nroommate pairs from the remaining four frontcourt players, the desired\nprobability is\n\n6\n2\n\n4\n2\n\n(cid:2)\n\n(cid:3)(cid:2)\n\n(cid:3)\n\np{2 mixed pairs} =\n\n(2)(3)\n\n4\n2\n\n6\n2\n(10)!/(5!25)\n\n= .5714\n\n2.13. let r denote the event that letter r is repeated; similarly, define the events e\n\nand v. then\n\np{same letter} = p(r) + p(e) + p(v) = 2\n7\n\n1\n8\n\n+ 3\n7\n\n1\n8\n\n+ 1\n7\n\n1\n8\n\n= 3\n28\n\n "}, {"Page_number": 482, "text": "2.14. let b1 = a1, bi = ai\n\n\u239b\n\u239di\u22121(cid:10)\n\nj=1\n\nsolutions to self-test problems and exercises 467\n\naj\n\n, i > 1. then\n\n\u239e\n\u23a0c\n\u239b\n\u239d q(cid:14)\n\ni=1\n\np\n\nai\n\n\u239e\n\u23a0\n\nbi\n\n\u239e\n\u239b\n\u239d q(cid:14)\n\u23a0 = p\nq(cid:6)\nq(cid:6)\n\ni=1\n\ni=1\n\n=\n\n\u2026\n\np(bi)\n\np(ai)\n\ni=1\n\n2.15.\n\nwhere the final equality uses the fact that the bi are mutually exclusive. the\ninequality then follows, since bi ( ai.\n\n\u239b\n\u239d q(cid:17)\n\ni=1\n\np\n\nai\n\n\u239e\n\u239f\u23a0\n\n\u239e\n\u23a0 = 1 \u2212 p\n\n\u239b\n\u239b\n\u239d q(cid:17)\n\u239c\u239d\n\u239b\n\u239d q(cid:14)\n= 1 \u2212 p\nq(cid:6)\n\ni=1\n\ni=1\n\nac\ni\n\n\u239e\n\u23a0c\n\u239e\n\u23a0\n\nai\n\n\u00fa 1 \u2212\n= 1\n\np(ac\ni\n\n)\n\ni=1\n\n2.16. the number of partitions for which {1} is a subset is equal to the number of\npartitions of the remaining n \u2212 1 elements into k \u2212 1 nonempty subsets,\nnamely, tk\u22121(n \u2212 1). because there are tk(n \u2212 1) partitions of {2, . . . , n \u2212 1}\ninto k nonempty subsets and then a choice of k of them in which to place\nelement 1, it follows that there are ktk(n \u2212 1) partitions for which {1} is not a\nsubset. hence, the result follows.\n2.17. let r, w, b denote, respectively, the events that there are no red, no white,\n\nand no blue balls chosen. then\n\np(r \u222a w \u222a b) = p(r) + p(w) + p(b) \u2212 p(rw) \u2212 p(rb)\n(cid:3)\n(cid:2)\n(cid:2)\n(cid:3) \u2212\n\n(cid:3)\n(cid:2)\n(cid:2)\n\u2212 p(wb) + p(rwb)\n(cid:2)\n(cid:3) +\n(cid:2)\n13\n5\n18\n5\n\n(cid:3)\n(cid:3) \u2212\n\n(cid:2)\n(cid:2)\n\n11\n5\n18\n5\n\n12\n5\n18\n5\n\n7\n5\n18\n5\n\n(cid:2)\n(cid:2)\n\n=\n\n(cid:3)\n(cid:3)\n\n6\n5\n18\n5\n\n(cid:3)\n(cid:3) +\n(cid:3)\n(cid:2)\n(cid:2)\n(cid:3)\n\n\u2212\n\n5\n5\n18\n5\nl 0.2933\n\n "}, {"Page_number": 483, "text": "468\n\nsolutions to self-test problems and exercises\n\nthus, the probability that all colors appear in the chosen subset is approxi-\nmately 1 \u2212 0.2933 = 0.7067.\n\n2.18. (a)\n\n8\u00b77\u00b76\u00b75\u00b74\n\n17\u00b716\u00b715\u00b714\u00b713\n\n= 2\n221\n\n9\u00b78\u00b77\u00b76\u00b75\n\n= 9\n442.\n\n2.19. (a) the probability that the 10 cards consist of 4 spades, 3 hearts, 2 diamonds,\n\n(b) because there are 9 nonblue balls, the probability is\n(c) because there are 3! possible orderings of the different colors and all pos-\nsibilities for the final 3 balls are equally likely, the probability is\n3!\u00b74\u00b78\u00b75\n17\u00b716\u00b715\n\n17\u00b716\u00b715\u00b714\u00b713\n\n= 4\n17.\n\n(d) the probability that the red balls are in a specified 4 spots is\n\n17\u00b716\u00b715\u00b714\n.\nbecause there are 14 possible locations of the red balls where they are all\ntogether, the probability is\n\n4\u00b73\u00b72\u00b71\n\n.\n\n= 1\n170\n\n13\n1\n\n13\n2\n\n13\n3\n\n13\n4\n\n52\n10\n\n(cid:29)\n\n(cid:29)\n\n(cid:30)(cid:29)\n\n(cid:30)(cid:29)\n\nand 1 club is\n\n. because there are 4! possible choices\n\nof the suits to have 4, 3, 2, and 1 cards, respectively, it follows that the\n\n(cid:30)(cid:29)\n(cid:29)\n(cid:30)\n(cid:30)(cid:29)\n(cid:30)\n(cid:30)\n= 6 choices of the two suits that are to have 3\ncards and then 2 choices for the suit to have 4 cards, the probability is\n12\n\nprobability is\n\n(cid:30)(cid:29)\n(cid:29)\n\n13\n3\n52\n10\n\n(cid:30)(cid:29)\n\n(cid:30)\n\n(cid:29)\n\n(cid:29)\n\n(cid:30)\n\n13\n4\n\n13\n2\n\n13\n1\n\n24\n\n4\n2\n\n.\n\n14\u00b74\u00b73\u00b72\u00b71\n17\u00b716\u00b715\u00b714\n(cid:30)\n\n(b) because there are\n\n(cid:30)(cid:29)\n(cid:29)\n\n(cid:30)(cid:29)\n(cid:30)\n\n13\n3\n52\n10\n\n13\n3\n\n13\n4\n\n.\n\n2.20. all the red balls are removed before all the blue ones if and only if the very\nlast ball removed is blue. because all 30 balls are equally likely to be the last\nball removed, the probability is 10/30.\n\nchapter 3\n\n3.1. (a) p(no aces) =\n\n(cid:3)\n\n(cid:2)\n\n35\n13\n\n(cid:3)(cid:26)(cid:2)\n(cid:2)\n(cid:2)\n35\n(b) 1 \u2212 p(no aces) \u2212 4\n12\n39\n\u239e\n\u239b\n13\n\u23a0\n\u239d 36\n13 \u2212 i\n\u239b\n\u239e\n\u239d 39\n\u23a0\n\n(c) p(i aces) =\n\n\u239b\n\u239d 3\n\ni\n\n39\n13\n\n(cid:3)\n(cid:3)\n\u239e\n\u23a0\n\n13\n\n3.2. let li denote the event that the life of the battery is greater than 10, 000 *\n\ni miles.\n(a) p(l2|l1) = p(l1l2)/p(l1) = p(l2)/p(l1) = 1/2\n(b) p(l3|l1) = p(l1l3)/p(l1) = p(l3)/p(l1) = 1/8\n\n3.3. put 1 white and 0 black balls in urn one, and the remaining 9 white and 10\n\nblack balls in urn two.\n\n "}, {"Page_number": 484, "text": "solutions to self-test problems and exercises 469\n\n3.4. let t be the event that the transferred ball is white, and let w be the event\n\nthat a white ball is drawn from urn b. then\n\np(t|w) =\n=\n\np(w|t)p(t)\n\np(w|t)p(t) + p(w|tc)p(tc)\n= 4/5\n(2/7)(2/3) + (1/7)(1/3)\n\n(2/7)(2/3)\n\n3.5. (a)\n\nr\n\nr+w, because each of the r + w balls is equally likely to be the ith ball\nremoved.\n\n(b), (c)\n\np(rj|ri) = p(rirj)\np(ri)\n(r\n2)\n(r+w\n2 )\nr\nr+w\nr \u2212 1\nr + w \u2212 1\n\n=\n\n=\n\na simpler argument is to note that, for i z j, given that the ith removal\nis a red ball, the jth removal is equally likely to be any of the remaining\nr + w \u2212 1 balls, of which r \u2212 1 are red.\n\n3.6. let bi denote the event that ball i is black, and let ri = bc\n\ni . then\n\np(r2|b1)p(b1)\n\n[r/[(b + r + c)][b/(b + r)]\n\np(r2|b1)p(b1) + p(r2|r1)p(r1)\n[r/(b + r + c)][b/(b + r)] + [(r + c)/(b + r + c)][r/(b + r)]\nb + r + c\n\nb\n\n3.7. let b denote the event that both cards are aces.\n\np(b1|r2) =\n=\n\n=\n\n(a)\n\np{b|yes to ace of spades} = p{b, yes to ace of spades}\n(cid:3)\n(cid:3)(cid:2)\n(cid:3)(cid:2)\n(cid:2)\np{yes to ace of spades}\n(cid:3)\n(cid:2)\n(cid:2)\n51\n1\n1\n1\n\n(cid:3)\n(cid:3) (cid:26)\n\n(cid:2)\n\n3\n1\n\n1\n1\n\n=\n\n52\n2\n\n= 3/51\n\n52\n2\n\n(b) since the second card is equally likely to be any of the remaining 51, of\n\nwhich 3 are aces, we see that the answer in this situation is also 3/51.\n\n(c) because we can always interchange which card is considered first and\nwhich is considered second, the result should be the same as in part (b).\na more formal argument is as follows:\n\n "}, {"Page_number": 485, "text": "470\n\nsolutions to self-test problems and exercises\n\np{b|second is ace} = p{b, second is ace}\np{second is ace}\np(b) + p{first is not ace, second is ace}\n(4/52)(3/51) + (48/52)(4/51)\n\n(4/52)(3/51)\n\np(b)\n\n=\n\n=\n= 3/51\n\n(d)\n\n3.8.\n\np{b|at least one} =\n=\n= 1/33\n\np(b)\n\np{at least one}\n(4/52)(3/51)\n1 \u2212 (48/52)(47/51)\n\np(h|e)\np(g|e)\n\n= p(he)\np(ge)\n\n= p(h)p(e|h)\np(g)p(e|g)\n\nhypothesis h is 1.5 times as likely.\n\n3.9. let a denote the event that the plant is alive and let w be the event that it was\n\nwatered.\n(a)\n\n(b)\n\np(a) = p(a|w)p(w) + p(a|wc)p(wc)\n\n= (.85)(.9) + (.2)(.1) = .785\np(wc|ac) = p(ac|wc)p(wc)\n\np(ac)\n= 16\n43\n\n= (.8)(.1)\n(cid:30)\n.215\n(cid:30)\n\n22\n6\n30\n6\n\np(2 green|no red) =\n\n(cid:29)\n\n(cid:30)\n\n(cid:30)(cid:29)\n(cid:29)\n(cid:30)\n\n10\n2\n\n12\n4\n\n22\n6\n\n3.10. (a)\n\n1 \u2212 p(no red balls) = 1 \u2212\n\n(cid:29)\n(cid:29)\n\n(b) given that no red balls are chosen, the 6 chosen are equally likely to be\n\nany of the 22 nonred balls. thus,\n\n3.11. let w be the event that the battery works, and let c and d denote the events\n\nthat the battery is a type c and that it is a type d battery, respectively.\n(a) p(w) = p(w|c)p(c) + p(w|d)p(d) = .7(8/14) + .4(6/14) = 4/7\n(b)\n\np(c|wc) = p(cwc)\np(wc)\n\n= p(wc|c)p(c)\n\n3/7\n\n= .3(8/14)\n\n3/7\n\n= .4\n\n "}, {"Page_number": 486, "text": "solutions to self-test problems and exercises 471\n\n3.12. let li be the event that maria likes book i, i = 1, 2. then\n= p(lc\n1l2)\n.4\n\n) = p(lc\n1l2)\np(lc\n)\n1\n\np(l2|lc\n\n1\n\nusing that l2 is the union of the mutually exclusive events l1l2 and lc\nsee that\n\n1l2, we\n\nthus,\n\n1l2) = .4 + p(lc\n\n1l2)\n\n.5 = p(l2) = p(l1l2) + p(lc\n(cid:19) = .1\n\n(cid:18)\nl2|lc\n\np\n\n1\n\n= .25\n\n.4\n\n3.13. (a) this is the probability that the last ball removed is blue. because each of\nthe 30 balls is equally likely to be the last one removed, the probability is\n1/3.\n\n(b) this is the probability that the last red or blue ball to be removed is a blue\nball. because it is equally likely to be any of the 30 red or blue balls, the\nprobability that it is blue is 1/3.\n\n(c) let b1, r2, g3 denote, respectively, the events that the first color removed\n\nis blue, the second is red, and the third is green. then\np(b1r2g3) = p(g3)p(r2|g3)p(b1|r2g3) = 8\n38\n\n20\n30\n\n= 8\n57\n\nwhere p(g3) is just the probability that the very last ball is green and\np(r2|g3) is computed by noting that, given that the last ball is green,\neach of the 20 red and 10 blue balls is equally likely to be the last of that\ngroup to be removed, so the probability that it is one of the red balls is\n20/30. (of course, p(b1|r2g3) = 1.)\n\n(d) p(b1) = p(b1g2r3) + p(b1r2g3) = 20\n\n8\n18\n\n38\n\n+ 8\n57\n\n= 64\n171\n\n3.14. let h be the event that the coin lands heads, let th be the event that b is told\nthat the coin landed heads, let f be the event that a forgets the result of the\ntoss, and let c be the event that b is told the correct result. then\n(a)\n\n(b)\n\n(c)\n\np(th) = p(th|f)p(f) + p(th|fc)p(fc)\n\n= (.5)(.4) + p(h)(.6)\n= .68\n\np(c) = p(c|f)p(f) + p(c|fc)p(fc)\n\n= (.5)(.4) + 1(.6) = .80\n\np(h|th) = p(hth)\np(th)\n\n "}, {"Page_number": 487, "text": "472\n\nsolutions to self-test problems and exercises\n\nnow,\n\np(hth) = p(hth|f)p(f) + p(hth|fc)p(fc)\n= p(h|f)p(th|hf)p(f) + p(h)p(fc)\n= (.8)(.5)(.4) + (.8)(.6) = .64\n\n3.15. since the black rat has a brown sibling, we can conclude that both of its parents\n\ngiving the result p(h|th) = .64/.68 = 16/17.\nhave one black and one brown gene.\n(a)\n\np(2 black|at least one) =\n\np(2)\n\np(at least one)\n\n= 1/4\n3/4\n\n= 1\n3\n\n(b) let f be the event that all 5 offspring are black, let b2 be the event that\nthe black rat has 2 black genes, and let b1 be the event that it has 1 black\nand 1 brown gene. then\np(b2|f) =\n=\n\np(f|b2)p(b2) + p(f|b1)p(b1)\n(1)(1/3) + (1/2)5(2/3)\n\np(f|b2)p(b2)\n\n= 16\n17\n\n(1)(1/3)\n\n3.16. let f be the event that a current flows from a to b, and let ci be the event\n\nthat relay i closes. then\n\nnow,\n\nalso,\n\np(f) = p(f|c1)p1 + p(f|cc\n\n1\n\n)(1 \u2212 p1)\n\np(f|c1) = p(c4 \u222a c2c5)\n\n= p(c4) + p(c2c5) \u2212 p(c4c2c5)\n= p4 + p2p5 \u2212 p4p2p5\n\np(f|cc\n\n1\n\n) = p(c2c5 \u222a c2c3c4)\n= p2p5 + p2p3p4 \u2212 p2p3p4p5\n\nhence, for part (a), we obtain\n\np(f) = p1(p4 + p2p5 \u2212 p4p2p5) + (1 \u2212 p1)p2(p5 + p3p4 \u2212 p3p4p5)\n\nfor part (b), let qi = 1 \u2212 pi. then\n\np(c3|f) = p(f|c3)p(c3)/p(f)\n\u222a cc\n\n= p3[1 \u2212 p(cc\n= p3(1 \u2212 q1q2 \u2212 q4q5 + q1q2q4q5)/p(f)\n\n)]/p(f)\n\n1cc\n2\n\n4cc\n5\n\n3.17. let a be the event that component 1 is working, and let f be the event that\n\nthe system functions.\n(a)\n\np(a|f) = p(af)\np(f)\n\n= p(a)\np(f)\n\n=\n\n1/2\n\n1 \u2212 (1/2)2\n\n= 2\n3\n\n "}, {"Page_number": 488, "text": "solutions to self-test problems and exercises 473\n\n(b)\n\nwhere p(f) was computed by noting that it is equal to 1 minus the prob-\nability that components 1 and 2 are both failed.\n\np(a|f) = p(af)\np(f)\n\n= p(f|a)p(a)\n\np(f)\n\n=\n\n(3/4)(1/2)\n\n(1/2)3 + 3(1/2)3\n\n= 3\n4\n\nwhere p(f) was computed by noting that it is equal to the probability that\nall 3 components work plus the three probabilities relating to exactly 2 of\nthe components working.\n\n3.18. if we assume that the outcomes of the successive spins are independent, then\nthe conditional probability of the next outcome is unchanged by the result that\nthe previous 10 spins landed on black.\n\n3.19. condition on the outcome of the initial tosses:\n\np(a odd) = p1(1 \u2212 p2)(1 \u2212 p3) + (1 \u2212 p1)p2p3 + p1p2p3(a odd)\n\n+ (1 \u2212 p1)(1 \u2212 p2)(1 \u2212 p3)p(a odd)\n\nso,\n\np(a odd) = p1(1 \u2212 p2)(1 \u2212 p3) + (1 \u2212 p1)p2p3\np1 + p2 + p3 \u2212 p1p2 \u2212 p1p3 \u2212 p2p3\n\n3.20. let a and b be the events that the first trial is larger and that the second is\nlarger, respectively. also, let e be the event that the results of the trials are\nequal. then\n\n1 = p(a) + p(b) + p(e)\n\nbut, by symmetry, p(a) = p(b): thus,\n\n1 \u2212 n(cid:6)\n\np2\ni\n\ni=1\n2\nanother way of solving the problem is to note that\n\n=\n\n2\n\np(b) = 1 \u2212 p(e)\n(cid:6)\n(cid:6)\n\nj>i\n\n(cid:6)\n(cid:6)\n\ni\n\np(b) =\n=\n\npipj\n\ni\n\nj>i\n\np{first trial results in i, second trial results in j}\n\nto see that the two expressions derived for p(b) are equal, observe that\n\ni=1\n\n1 = n(cid:6)\n(cid:6)\n(cid:6)\n(cid:6)\n\n=\n\n=\n\ni\n\ni\n\n=\n\nn(cid:6)\n(cid:6)\n\npi\n\npj\n\nj=1\npipj\n+\n\nj\np2\ni\n\np2\ni\n\n+ 2\n\n(cid:6)\n(cid:6)\n(cid:6)\n(cid:6)\n\njzi\n\ni\n\npipj\n\npipj\n\ni\n\ni\n\nj>i\n\n "}, {"Page_number": 489, "text": "474\n\nsolutions to self-test problems and exercises\n\n3.21. let e = {a gets more heads than b}; then\n\np(e) = p(e|a leads after both flip n)p(a leads after both flip n)\n+ p(e| even after both flip n)p( even after both flip n)\n+ p(e|b leads after both flip n)p(b leads after both flip n)\n\n= p(a leads) + 1\n2\n\np( even)\n\nnow, by symmetry,\n\nhence,\n\np(a leads) = p(b leads)\n= 1 \u2212 p(even)\n\n2\n\np(e) = 1\n2\n\n3.22. (a) not true: in rolling 2 dice, let e = {sum is 7}, f ={1st die does not land on 4},\n\nand g={2nd die does not land on 3}. then\n\np(e|f \u222a g) = p{7, not (4, 3)}\n\np{not (4, 3)} = 5/36\n\n35/36\n\n= 5/35 z p(e)\n\n(b)\n\n(c)\n\np(e(f \u222a g)) = p(ef \u222a eg)\n\n= p(ef) + p(eg)\n= p(e)[p(f) + p(g)]\n= p(e)p(f \u222a g)\n\nsince efg = \u2205\nsince fg = \u2205\n\np(g|ef) = p(efg)\np(ef)\n\np(ef)\n\n= p(e)p(fg)\n= p(e)p(f)p(g)\n= p(g).\n\np(e)p(f)\n\nsince e is independent of fg\n\nby independence\n\n3.23. (a) necessarily false; if they were mutually exclusive, then we would have\n\n0 = p(ab) z p(a)p(b)\n\n(b) necessarily false; if they were independent, then we would have\n\np(ab) = p(a)p(b) > 0\n\n(c) necessarily false; if they were mutually exclusive, then we would have\n\np(a \u222a b) = p(a) + p(b) = 1.2\n\n(d) possibly true\n\n "}, {"Page_number": 490, "text": "solutions to self-test problems and exercises 475\n3.24. the probabilities in parts (a), (b), and (c) are .5, (.8)3 = .512, and (.9)7 l .4783,\n\nrespectively.\n\n3.25. let di, i = 1, 2, denote the event that radio i is defective. also, let a and b\nbe the events that the radios were produced at factory a and at factory b,\nrespectively. then\n\np(d2|d1) = p(d1d2)\np(d1)\n= p(d1d2|a)p(a) + p(d1d2|b)p(b)\np(d1|a)p(a) + p(d1|b)p(b)\n= (.05)2(1/2) + (.01)2(1/2)\n(.05)(1/2) + (.01)(1/2)\n= 13/300\n\n3.26. we are given that p(ab) = p(b) and must show that this implies that p(bcac) =\n\np(ac). one way is as follows:\n\np(bcac) = p((a \u222a b)c)\n\n= 1 \u2212 p(a \u222a b)\n= 1 \u2212 p(a) \u2212 p(b) + p(ab)\n= 1 \u2212 p(a)\n= p(ac)\n\n3.27. the result is true for n = 0. with ai denoting the event that there are i red\n\nballs in the urn after stage n, assume that\n\nnow let bj, j = 1, . . . , n + 2, denote the event that there are j red balls in the\nurn after stage n + 1. then\n\np(ai) = 1\n\nn + 1\n\ni = 1, . . . , n + 1\n\n,\n\np(bj) = n+1(cid:6)\n\np(bj|ai)p(ai)\nn+1(cid:6)\n\np(bj|ai)\n\ni=1\n= 1\n\nn + 1\n\n= 1\n\nn + 1\n\ni=1\n[p(bj|aj\u22121) + p(bj|aj)]\n\nbecause there are n + 2 balls in the urn after stage n, it follows that p(bj|aj\u22121)\nis the probability that a red ball is chosen when j \u2212 1 of the n + 2 balls in the\nurn are red and p(bj|aj) is the probability that a red ball is not chosen when j\nof the n + 2 balls in the urn are red. consequently,\n\np(bj|aj\u22121) = j \u2212 1\nn + 2\n\n,\n\np(bj|aj) = n + 2 \u2212 j\nn + 2\n\n "}, {"Page_number": 491, "text": "476\n\nsolutions to self-test problems and exercises\n\nsubstituting these results into the equation for p(bj) gives\n\n(cid:20)\n\n(cid:21)\n\n+ n + 2 \u2212 j\nn + 2\n\n= 1\n\nn + 2\n\np(bj) = 1\n\nj \u2212 1\nn + 2\nthis completes the induction proof.\n\nn + 1\n\n3.28. if ai is the event that player i receives an ace, then\n\n(cid:2)\n\n(cid:3)\n2n \u2212 2\n(cid:3) = 1 \u2212 1\n(cid:2)\n\nn\n2n\nn\n\np(ai) = 1 \u2212\n\nn \u2212 1\n2n \u2212 1\n\n= 3n \u2212 1\n4n \u2212 2\n\n2\n\nby arbitrarily numbering the aces and noting that the player who does not\nreceive ace number one will receive n of the remaining 2n \u2212 1 cards, we\nsee that\n\np(a1a2) =\n\nn\n\n2n \u2212 1\n\ntherefore,\n\np(ac\n2\n\n|a1) = 1 \u2212 p(a2|a1) = 1 \u2212 p(a1a2)\np(a1)\n\n= n \u2212 1\n3n \u2212 1\n\nwe may regard the card division outcome as the result of two trials, where trial\ni, i = 1, 2, is said to be a success if ace number i goes to the first player. because\nthe locations of the two aces become independent as n goes to infinity, with\neach one being equally likely to be given to either player, it follows that the\ntrials become independent, each being a success with probability 1/2. hence,\nin the limiting case where n\u2192q, the problem becomes one of determining\nthe conditional probability that two heads result, given that at least one does,\nwhen two fair coins are flipped. because n\u22121\n3n\u22121 converges to 1/3, the answer\nagrees with that of example 2b.\n\n3.29. (a) for any permutation i1, . . . , in of 1, 2, . . . , n, the probability that the suc-\nn\ni=1 pi. consequently,\n\ncessive types collected is i1, . . . , in is pi1\nthe desired probability is n!\n\nn\ni=1 pi.\n\n$\n\n(b) for i1, . . . , ik all distinct,\n\n= $\n(cid:3)n\n\n\u00b7\u00b7\u00b7 pin\n(cid:2)\n\nn \u2212 k\n\nn\n\np(ei1\n\n\u00b7\u00b7\u00b7 eik\n\n) =\n\nwhich follows because there are no coupons of types i1, . . . , ik when each\nof the n independent selections is one of the other n \u2212 k types. it now\nfollows by the inclusion\u2013exclusion identity that\n\ni=1ei) = n(cid:6)\n\nk=1\n\np(\u222an\n\n(cid:2)\n\n(cid:3)(cid:2)\n\n(\u22121)k+1\n\nn\nk\n\nn \u2212 k\n\nn\n\n(cid:3)n\n\n "}, {"Page_number": 492, "text": "solutions to self-test problems and exercises 477\n\nbecause 1 \u2212 p(\u222an\nby part (a) it is equal to n!\ngives\n\ni=1ei) is the probability that one of each type is obtained,\nnn . substituting this into the preceding equation\n\nk=1\n\n1 \u2212 n!\nnn\n\n= n(cid:6)\nn! = nn \u2212 n(cid:6)\nn! = n(cid:6)\n\nk=1\n\nk=0\n\n(\u22121)k+1\n\n(cid:3)n\n\nn \u2212 k\n\nn\n\n(cid:2)\n\n(cid:3)(cid:2)\n\nn\nk\n\n(cid:2)\n\n(cid:3)\n\n(n \u2212 k)n\n\nn\nk\n\n(\u22121)k+1\n(cid:2)\n(cid:3)\n(n \u2212 k)n\n\n(\u22121)k\n\nn\nk\n\nor\n\nor\n\n3.30.\n\np(e|e \u222a f) = p(e|f(e \u222a f))p(f|e \u222a f) + p(e|fc(e \u222a f))p(fc|e \u222a f)\nusing\n\nf(e \u222a f) = f and fc(e \u222a f) = fce\n\ngives\n\np(e|e \u222a f) = p(e|f)p(f|e \u222a f) + p(e|efc)p(fc|e \u222a f)\n= p(e|f)p(f|e \u222a f) + p(fc|e \u222a f)\n\u00fa p(e|f)p(f|e \u222a f) + p(e|f)p(fc|e \u222a f)\n= p(e|f)\n\nchapter 4\n\n4.1. since the probabilities sum to 1, we must have 4p{x = 3} + .5 = 1, implying\nthat p{x = 0} = .375, p{x = 3} = .125. hence, e[x] = 1(.3) + 2(.2) +\n3(.125) = 1.075.\n4.2. the relationship implies that pi = cip0, i = 1, 2, where pi = p{x = i}. because\n\nthese probabilities sum to 1, it follows that\n\np0(1 + c + c2) = 1 * p0 =\n\n1\n\n1 + c + c2\n\nhence,\n\ne[x] = p1 + 2p2 = c + 2c2\n1 + c + c2\n\n4.3. let x be the number of flips. then the probability mass function of x is\n\np2 = p2 + (1 \u2212 p)2,\n\np3 = 1 \u2212 p2 = 2p(1 \u2212 p)\n\n "}, {"Page_number": 493, "text": "478\n\nsolutions to self-test problems and exercises\n\nhence,\n\ne[x] = 2p2 + 3p3 = 2p2 + 3(1 \u2212 p2) = 3 \u2212 p2 \u2212 (1 \u2212 p)2\n\n4.4. the probability that a randomly chosen family will have i children is ni/m.\n\nthus,\n\nr(cid:9)\n\ni=1\n\nalso, since there are ini children in families having i children, it follows that\nthe probability that a randomly chosen child is from a family with i children is\nini/\n\nini. therefore,\n\ne[x] = r(cid:6)\n\ni=1\n\nini/m\n\nr(cid:6)\nr(cid:6)\n\ni=1\n\ni=1\n\ni2ni\n\nini\n\nr(cid:6)\nr(cid:6)\n\ni=1\n\nini\n\ne[y] =\n\nr(cid:6)\nr(cid:6)\n\ni=1\n\ni2ni\n\n\u00fa\n\nini\n\nthus, we must show that\n\nor, equivalently, that\n\nor, equivalently, that\n\ni=1\n\nr(cid:6)\n\ni=1\n\nr(cid:6)\n\nj=1\n\nnj\n\ni2ni \u00fa\n\nr(cid:6)\n\nr(cid:6)\n\ni=1\n\nj=1\n\ni2ninj \u00fa\n\nni\n\ni=1\n\nr(cid:6)\n\ni=1\n\nini\n\nr(cid:6)\n\nj=1\n\njnj\n\nr(cid:6)\n\nr(cid:6)\n\ni=1\n\nj=1\n\nijninj\n\nbut, for a fixed pair i, j, the coefficient of ninj in the left-side summation of\nthe preceding inequality is i2 + j2, whereas its coefficient in the right-hand\nsummation is 2ij. hence, it suffices to show that\ni2 + j2 \u00fa 2ij\n\nwhich follows because (i \u2212 j)2 \u00fa 0.\n\n4.5. let p = p{x = 1}. then e[x] = p and var(x) = p(1 \u2212 p), so\n\np = 3p(1 \u2212 p)\nimplying that p = 2/3. hence, p{x = 0} = 1/3.\n\n "}, {"Page_number": 494, "text": "solutions to self-test problems and exercises 479\n\n4.6. if you wager x on a bet that wins the amount wagered with probability p and\n\nloses that amount with probability 1 \u2212 p, then your expected winnings are\n\nxp \u2212 x(1 \u2212 p) = (2p \u2212 1)x\n\nwhich is positive (and increasing in x) if and only if p > 1/2. thus, if p \u2026 1/2,\none maximizes one\u2019s expected return by wagering 0, and if p > 1/2, one maxi-\nmizes one\u2019s expected return by wagering the maximal possible bet. therefore,\nif the information is that the .6 coin was chosen, then you should bet 10, and if\nthe information is that the .3 coin was chosen, then you should bet 0. hence,\nyour expected payoff is\n\n1\n2\n\n(1.2 \u2212 1)10 + 1\n2\n\n0 \u2212 c = 1 \u2212 c\n\nsince your expected payoff is 0 without the information (because in this case\nthe probability of winning is 1\n(.3) < 1/2), it follows that if the infor-\n2\nmation costs less than 1, then it pays to purchase it.\n\n(.6) + 1\n\n2\n\n4.7. (a) if you turn over the red paper and observe the value x, then your expected\n\nreturn if you switch to the blue paper is\n\n2x(1/2) + x/2(1/2) = 5x/4 > x\n\nthus, it would always be better to switch.\n(b) suppose the philanthropist writes the amount x on the red paper. then\nthe amount on the blue paper is either 2x or x/2. note that if x/2 \u00fa y, then\nthe amount on the blue paper will be at least y and will thus be accepted.\nhence, in this case, the reward is equally likely to be either 2x or x/2, so\n\ne[ry(x)] = 5x/4,\n\nif x/2 \u00fa y\n\nif x/2 < y \u2026 2x, then the blue paper will be accepted if its value is 2x and\nrejected if it is x/2. therefore,\n\ne[ry(x)] = 2x(1/2) + x(1/2) = 3x/2,\n\nif x/2 < y \u2026 2x\n\nfinally, if 2x < y, then the blue paper will be rejected. hence, in this case,\nthe reward is x, so\n\nthat is, we have shown that when the amount x is written on the red\npaper, the expected return under the y-policy is\n\nif 2x < y\n\nry(x) = x,\n\u23a7\u23a8\n\u23a9 x\n\ne[ry(x)] =\n\nif x < y/2\n3x/2 if y/2 \u2026 x < 2y\n5x/4 if x \u00fa 2y\n\n4.8. suppose that n independent trials, each of which results in a success with prob-\nability p, are performed. then the number of successes will be less than or\nequal to i if and only if the number of failures is greater than or equal to n \u2212 i.\nbut since each trial is a failure with probability 1 \u2212 p, it follows that the num-\nber of failures is a binomial random variable with parameters n and 1 \u2212 p.\nhence,\n\n "}, {"Page_number": 495, "text": "480\n\nsolutions to self-test problems and exercises\n\np{bin(n, p) \u2026 i} = p{bin (n, 1 \u2212 p) \u00fa n \u2212 i}\n\n= 1 \u2212 p{bin (n, 1 \u2212 p) \u2026 n \u2212 i \u2212 1}\n\nthe final equality follows from the fact that the probability that the number\nof failures is greater than or equal to n \u2212 i is 1 minus the probability that it is\nless than n \u2212 i.\n\n4.9. since e[x] = np, var(x) = np(1 \u2212 p), we are given that np = 6, np(1 \u2212 p) =\n\n2.4. thus, 1 \u2212 p = .4, or p = .6, n = 10. hence,\n\n(cid:2)\n\n(cid:3)\n\n10\n5\n\np{x = 5} =\n\n(.6)5(.4)5\n\n4.10. let xi, i = 1, . . . , m, denote the number on the ith ball drawn. then\n\np{x \u2026 k} = p{x1 \u2026 k, x2 \u2026 k, . . . , xm \u2026 k}\n\n(cid:3)m\n\n(cid:2)\n= p{x1 \u2026 k}p{x2 \u2026 k}\u00b7\u00b7\u00b7 p{xm \u2026 k}\n=\n\nk\nn\n\ntherefore,\n\np{x = k} = p{x \u2026 k} \u2212 p{x \u2026 k \u2212 1} =\n\n(cid:2)\n\n(cid:3)m \u2212\n\n(cid:2)\n\nk\nn\n\nk \u2212 1\n\nn\n\n(cid:3)m\n\n4.11. (a) given that a wins the first game, it will win the series if, from then on, it\n\nwins 2 games before team b wins 3 games. thus,\n\np{a wins|a wins first} = 4(cid:6)\n\n(cid:2)\n\n(cid:3)\n\n4\ni\n\ni=2\n\npi(1 \u2212 p)4\u2212i\n\n(b)\n\np{a wins first|a wins} = p{a wins|a wins first}p{a wins first}\n\n(cid:2)\n4(cid:6)\n(cid:2)\n5(cid:6)\n\ni=2\n\n4\ni\n\n(cid:3)\np{a wins}\npi+1(1 \u2212 p)4\u2212i\n(cid:3)\n\npi(1 \u2212 p)5\u2212i\n\n5\ni\n\ni=3\n\n=\n\n4.12. to obtain the solution, condition on whether the team wins this weekend:\n\n(cid:2)\n\n4(cid:6)\n\n(cid:3)\n\n4\ni\n\n.5\n\ni=3\n\n(.4)i(.6)4\u2212i + .5\n\n(.7)i(.3)4\u2212i\n\n4\ni\n\n(cid:2)\n\n4(cid:6)\n\n(cid:3)\n\ni=3\n\n "}, {"Page_number": 496, "text": "solutions to self-test problems and exercises 481\n\n4.13. let c be the event that the jury makes the correct decision, and let f be the\n\nevent that four of the judges agreed. then\n\np(c) = 7(cid:6)\n\ni=4\n\n(cid:3)\n(.7)i(.3)7\u2212i\n\n(cid:2)\n\n7\ni\n\nalso,\n\np(c|f) = p(cf)\np(f)\n(cid:30)\n\n(cid:29)\n\n(cid:30)\n(.7)4(.3)3 +\n\n7\n4\n\n(.7)4(.3)3\n\n(cid:29)\n\n(cid:30)\n\n=\n\n(cid:29)\n= .7\n\n7\n4\n\n7\n3\n\n(.7)3(.3)4\n\n4.14. assuming that the number of hurricanes can be approximated by a poisson\n\nrandom variable, we obtain the solution\n\n4.15.\n\ne[y] =\n\n3(cid:6)\n\ni=0\n\nq(cid:6)\n\ni=1\n\n\u22125.2(5.2)i/i!\ne\n\nip{x = i}/p{x > 0}\n\n= e[x]/p{x > 0}\n=\n\n\u03bb\n\n1 \u2212 e\u2212\u03bb\n\n4.16. (a) 1/n\n\n(b) let d be the event that girl i and girl j choose different boys. then\n\np(gigj) = p(gigj|d)p(d) + p(gigj|dc)p(dc)\n\n= (1/n)2(1 \u2212 1/n)\n= n \u2212 1\nn3\n\ntherefore,\n\np(gi|gj) = n \u2212 1\n\nn2\n\n(c), (d) because, when n is large, p(gi|gj) is small and nearly equal to p(gi),\nit follows from the poisson paradigm that the number of couples is\ni=1 p(gi) = 1. hence,\napproximately poisson distributed with mean\np0 l e\n(e) to determine the probability that a given set of k girls all are coupled,\ncondition on whether or not d occurs, where d is the event that they all\nchoose different boys. this gives\n\n\u22121 and pk l e\n\n\u22121/k!\n\n(cid:9)\n\nn\n\n "}, {"Page_number": 497, "text": "482\n\nsolutions to self-test problems and exercises\n\np(gi1\n\n\u00b7\u00b7\u00b7 gik\n\n\u00b7\u00b7\u00b7 gik\n\n|dc)p(dc)\n\n\u00b7\u00b7\u00b7 gik\n\u00b7\u00b7\u00b7 gik\n\n) = p(gi1\n|d)p(d) + p(gi1\n= p(gi1\n|d)p(d)\n= (1/n)k n(n \u2212 1)\u00b7\u00b7\u00b7 (n \u2212 k + 1)\n=\n(n \u2212 k)!n2k\n(cid:3)\n\n(cid:2)\n\nnk\n\nn!\n\ntherefore,(cid:6)\n\np(gi1\n\ni1<...<ik\n\n\u00b7\u00b7\u00b7 gik\n\n) =\n\nn\nk\n\np(gi1\n\n\u00b7\u00b7\u00b7 gik\n\n) =\n\nn!n!\n\n(n \u2212 k)!(n \u2212 k)!k!n2k\n\nand the inclusion\u2013exclusion identity yields\n\n1 \u2212 p0 = p(\u222an\n\n(\u22121)k+1\n\nn!n!\n\n(n \u2212 k)!(n \u2212 k)!k!n2k\n\ni=1gi) = n(cid:6)\n\nk=1\n\n4.17. (a) because woman i is equally likely to be paired with any of the remaining\n\n2n \u2212 1 people, p(wi) = 1\n2n\u22121\nany of 2n \u2212 3 people, p(wi|wj) = 1\n(cid:9)\n2n\u22123\n\n(b) because, conditional on wj, woman i is equally likely to be paired with\n\n(c) when n is large, the number of wives paired with their husbands will\nl 1/2. there-\n\u22121/2.\n\napproximately be poisson with mean\nfore, the probability that there is no such pairing is approximately e\n\ni=1 p(wi) = n\n2n\u22121\n\nn\n\n(d) it reduces to the match problem.\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\n4.18. (a)\n\n(9/19)3(10/19)5(9/19) =\n\n8\n3\nsince she would have won 4 bets and lost x \u2212 4 bets, it follows that\n\n(b) if w is her final winnings and x is the number of bets she makes, then,\n\n(9/19)4(10/19)5\n\n8\n3\n\nw = 20 \u2212 5(x \u2212 4) = 40 \u2212 5x\n\nhence,\n\ne[w] = 40 \u2212 5e[x] = 40 \u2212 5[4/(9/19)] = \u221220/9\n\n4.19. the probability that a round does not result in an \u201codd person\u201d is equal to 1/4,\n\nthe probability that all three coins land on the same side.\n(a) (1/4)2(3/4) = 3/64\n(b) (1/4)4 = 1/256\n\n "}, {"Page_number": 498, "text": "4.20. let q = 1 \u2212 p. then\n\nsolutions to self-test problems and exercises 483\n\ne[1/x] =\n\nq\n\n1\ni\n\ni=1\n\nqi/i\n\nq(cid:6)\nqi\u22121p\nq(cid:6)\nq(cid:6)\n*\n*\n*\n\n*\nq(cid:6)\n\ni=1\nq\n\ni=1\n= p\nq\n= p\nq\n= p\nq\n= p\nq\n= p\nq\n= \u2212p\nq\n\np\nlog(p)\n\ni=1\n1\n1 \u2212 x\n1\ny\n\ndy\n\nq\n\n0\n\n0\n\n1\n\n0\n\nxi\u22121 dx\n\nxi\u22121 dx\n\ndx\n\n4.21. since x\u2212b\n\na\u2212b will equal 1 with probability p or 0 with probability 1 \u2212 p, it follows\nthat it is a bernoulli random variable with parameter p. because the variance\nof such a bernoulli random variable is p(1 \u2212 p), we have\n(a \u2212 b)2 var(x \u2212 b) =\n\n(a \u2212 b)2 var(x)\n\np(1 \u2212 p) = var\n\nx \u2212 b\na \u2212 b\n\n(cid:2)\n\n(cid:3)\n\n=\n\n1\n\n1\n\nhence,\n\nvar(x) = (a \u2212 b)2p(1 \u2212 p)\n\n4.22. let x denote the number of games that you play and y the number of games\n\nthat you lose.\n(a) after your fourth game, you will continue to play until you lose. there-\n\nfore, x \u2212 4 is a geometric random variable with parameter 1 \u2212 p, so\n\ne[x] = e[4 + (x \u2212 4)] = 4 + e[x \u2212 4] = 4 +\n\n1\n\n1 \u2212 p\n\n(b) if we let z denote the number of losses you have in the first 4 games, then\nz is a binomial random variable with parameters 4 and 1 \u2212 p. because\ny = z + 1, we have\n\ne[y] = e[z + 1] = e[z] + 1 = 4(1 \u2212 p) + 1\n\n4.23. a total of n white balls will be withdrawn before a total of m black balls if\nand only if there are at least n white balls in the first n + m \u2212 1 withdrawals.\n(compare with the problem of the points, example 4j of chapter 3.) with x\nequal to the number of white balls among the first n + m \u2212 1 balls withdrawn,\nx is a hypergeometric random variable, and it follows that\n\n "}, {"Page_number": 499, "text": "484\n\nsolutions to self-test problems and exercises\n\np{x \u00fa n} = n+m\u22121(cid:6)\n\ni=n\n\n(cid:2)\n\nn\ni\n\np{x = i} = n+m\u22121(cid:6)\n\ni=n\n\n(cid:3)\n\n(cid:3)(cid:2)\n(cid:2)\n\n(cid:3)\nm\nn + m \u2212 1 \u2212 i\nn + m\nn + m \u2212 1\n\n4.24. because each ball independently goes into urn i with the same probability pi, it\nfollows that xi is a binomial random variable with parameters n = 10, p = pi.\nfirst note that xi + xj is the number of balls that go into either urn i or\nurn j. then, because each of the 10 balls independently goes into one of these\nurns with probability pi + pj, it follows that xi + xj is a binomial random\nvariable with parameters 10 and pi + pj.\nby the same logic, x1 + x2 + x3 is a binomial random variable with param-\n(cid:3)\n(cid:2)\neters 10 and p1 + p2 + p3. therefore,\n( p1 + p2 + p3)7( p4 + p5)3\n10\n7\n\np{x1 + x2 + x3 = 7} =\n\n4.25. let xi equal 1 if person i has a match, and let it equal 0 otherwise. then\n\nxi\n\ni=1\n\nx = n(cid:6)\ne[xi] = n(cid:6)\n\ni=1\n\nis the number of matches. taking expectations gives\n\nn(cid:6)\ne[x] = e[\n\nxi] = n(cid:6)\n\ni=1\n\ni=1\n\np{xi = 1} = n(cid:6)\n\ni=1\n\n1/n = 1\n\nwhere the final equality follows because person i is equally likely to end up\nwith any of the n hats.\n\nto compute var(x), we use equation (9.1), which states that\n\ne[x2] = n(cid:6)\n\ne[xi] + n(cid:6)\n\n(cid:6)\n\ni=1\n\ne[xixj]\n\njzi\n\ni=1\n\nnow, for i z j,\n\ne[xixj] = p{xi = 1, xj = 1} = p{xi = 1}p{xj = 1|xi = 1} = 1\nn\n\n1\n\nn \u2212 1\n\nhence,\n\ne[x2] = 1 + n(cid:6)\n\ni=1\n\n(cid:6)\n\njzi\n\n1\n\nn(n \u2212 1)\n1\n\nn(n \u2212 1)\n\n= 2\n\n= 1 + n(n \u2212 1)\n\nwhich yields\n\nvar(x) = 2 \u2212 12 = 1\n\n "}, {"Page_number": 500, "text": "solutions to self-test problems and exercises 485\n\n4.26. with q = 1 \u2212 p, we have, on the one hand,\np{x = 2i}\n\np(e) =\n\ni=1\n\nq(cid:6)\nq(cid:6)\npq2i\u22121\nq(cid:6)\n(q2)i\u22121\n\n=\n\ni=1\n= pq\n\n1\n\ni=1\n1 \u2212 q2\npq\n\n= pq\n=\n\n(1 \u2212 q)(1 + q)\n\n= q\n\n1 + q\n\non the other hand,\n\np(e) = p(e|x = 1)p + p(e|x > 1)q = qp(e|x > 1)\n\nhowever, given that the first trial is not a success, the number of trials needed\nfor a success is 1 plus the geometrically distributed number of additional trials\nrequired. therefore,\n\np(e|x > 1) = p(x + 1 is even) = p(ec) = 1 \u2212 p(e)\n\nwhich yields p(e) = q/(1 + q).\n\nchapter 5\n\n1\n\n5.1. let x be the number of minutes played.\n\n5.2. (a) 1 =-\n\n(a) p{x > 15} = 1 \u2212 p{x \u2026 15} = 1 \u2212 5(.025) = .875\n(b) p{20 < x < 35} = 10(.05) + 5(.025) = .625\n(c) p{x < 30} = 10(.025) + 10(.05) = .75\n(d) p{x > 36} = 4(.025) = .1\n0 cxndx = c/(n + 1) * c = n + 1\n6661\n-\n(b) p{x > x} = (n + 1)\nx xndx = xn+1\n*\n5.3. first, let us find c by using\ncx4dx = 32c/5 * c = 5/32\n1 =\n= 5/3\n= 20/7 * var(x) = 20/7 \u2212 (5/3)2 = 5/63\n\n(a) e[x] = 5\n(b) e[x2] = 5\n\n= 1 \u2212 xn+1\n\n32\n\n1\n\n2\n\n2\n\nx\n\n64\n6\n128\n7\n\n32\n\n32\n\n5.4. since\n\n2\n\n0\n\n32\n\n-\n-\n0 x5dx = 5\n0 x6dx = 5\n*\n*\n\n1\n\n0\n\n1 =\n.6 =\n\n1\n\n0\n\n(ax + bx2)dx = a/2 + b/3\n(ax2 + bx3)dx = a/3 + b/4\n\n "}, {"Page_number": 501, "text": "486\n\nsolutions to self-test problems and exercises\n\nwe obtain a = 3.6, b = \u22122.4. hence,\n\n(a) p{x < 1/2} =-\n(b) e[x2] =-\n\n1\n0\n\n5.5. for i = 1, . . . , n,\n\n1/2\n0\n\n(3.6x \u2212 2.4x2)dx = (1.8x2 \u2212 .8x3)\n(3.6x3 \u2212 2.4x4)dx = .42 * var(x) = .06\n\n6661/2\n\n0\n\n= .35\n\n%\n\np{x = i} = p{int(nu) = i \u2212 1}\n= p{i \u2212 1 \u2026 nu < i}\n= p\ni\nn\n= 1/n\n\ni \u2212 1\nn\n\n\u2026 u <\n\n/\n\n5.6. if you bid x, 70 \u2026 x \u2026 140, then you will either win the bid and make a profit\nof x \u2212 100 with probability (140 \u2212 x)/70 or lose the bid and make a profit of\n0 otherwise. therefore, your expected profit if you bid x is\n\n1\n70\n\n(x \u2212 100)(140 \u2212 x) = 1\n70\n\n(240x \u2212 x2 \u2212 14000)\n\ndifferentiating and setting the preceding equal to 0 gives\n\n240 \u2212 2x = 0\n\ntherefore, you should bid 120 thousand dollars. your expected profit will be\n40/7 thousand dollars.\n5.7. (a) p{u > .1} = 9/10\n(b) p{u > .2|u > .1} = p{u > .2}/p{u > .1} = 8/9\n(c) p{u > .3|u > .2, u > .1} = p{u > .3}/p{u > .2} = 7/8\n(d) p{u > .3} = 7/10\nthe answer to part (d) could also have been obtained by multiplying the prob-\nabilities in parts (a), (b), and (c).\n5.8. let x be the test score, and let z = (x \u2212 100)/15. note that z is a standard\n\nnormal random variable.\n(a) p{x > 125} = p{z > 25/15} l .0478\n(b)\n\np{90 < x < 110} = p{\u221210/15 < z < 10/15}\n\n= p{z < 2/3} \u2212 p{z < \u22122/3}\n= p{z < 2/3} \u2212 [1 \u2212 p{z < 2/3}]\nl .4950\n\n5.9. let x be the travel time. we want to find x such that\n\nwhich is equivalent to\n\n%\n\np\n\nthat is, we need to find x such that\n\np{x > x} = .05\n/\n\nx \u2212 40\n%\n\n7\n\np\n\nz >\n\n>\n\nx \u2212 40\n/\n\n7\n\nx \u2212 40\n\n7\n\n= .05\n\n= .05\n\n "}, {"Page_number": 502, "text": "solutions to self-test problems and exercises 487\n\nwhere z is a standard normal random variable. but\n\np{z > 1.645} = .05\n\nthus,\n\nx \u2212 40\n\n7\n\n= 1.645 or\n\nx = 51.515\n\ntherefore, you should leave no later than 8.485 minutes after 12 p.m.\n\n5.10. let x be the tire life in units of one thousand, and let z = (x \u2212 34)/4. note\n\nthat z is a standard normal random variable.\n(a) p{x > 40} = p{z > 1.5} l .0668\n(b) p{30 < x < 35} = p{\u22121 < z < .25} = p{z < .25} \u2212 p{z > 1} l .44\n(c)\n\np{x > 40|x > 30} = p{x > 40}/p{x > 30}\n\n= p{z > 1.5}/p{z > \u22121} l .079\n\n5.11. let x be next year\u2019s rainfall and let z = (x \u2212 40.2)/8.4.\n\n(a) p{x > 44} = p{z > 3.8/8.4} l p{z > .4524} l .3255\n(b)\n\n(.3255)3(.6745)4\n\n(cid:2)\n\n(cid:3)\n\n7\n3\n\n5.12. let mi and wi denote, respectively, the numbers of men and women in the\nsamples that earn, in units of one thousand dollars, at least i per year. also, let\nz be a standard normal random variable.\n(a)\n\np{w25 \u00fa 70} = p{w25 \u00fa 69.5}\n\n0\n\nw25 \u2212 200(.34)\n\u221a\n200(.34)(.66)\n\n= p\nl p{z \u00fa .2239}\nl .4114\n\n7\n\n\u00fa 69.5 \u2212 200(.34)\n\n\u221a\n\n200(.34)(.66)\n\n(b)\n\np{m25 \u2026 120} = p{m25 \u2026 120.5}\n\n0\n\nm25 \u2212 (200)(.587)\n\u221a\n(200)(.587)(.413)\n\n= p\nl p{z \u2026 .4452}\nl .6719\n\n7\n\n\u2026 120.5 \u2212 (200)(.587)\n\n\u221a\n\n(200)(.587)(.413)\n\n "}, {"Page_number": 503, "text": "488\n\nsolutions to self-test problems and exercises\n\n(c)\n\n0\n\nm20 \u2212 (200)(.745)\n\u221a\n(200)(.745)(.255)\n\np{m20 \u00fa 150} = p{m20 \u00fa 149.5}\n\n= p\nl p{z \u00fa .0811}\nl .4677\np{w20 \u00fa 100} = p{w20 \u00fa 99.5}\n\n0\n\n7\n\n\u00fa 149.5 \u2212 (200)(.745)\n\n\u221a\n\n(200)(.745)(.255)\n\n7\n\n\u00fa 99.5 \u2212 (200)(.534)\n\n\u221a\n(200)(.534)(.466)\n\nw20 \u2212 (200)(.534)\n\u221a\n(200)(.534)(.466)\n\n= p\nl p{z \u00fa \u22121.0348}\n= l.8496\np{m20 \u00fa 150}p{w20 \u00fa 100} l .3974\n\nhence,\n\n5.13. the lack of memory property of the exponential gives the result e\n5.14. (a) e\n\n\u221222 = e\n\u22124\n(b) f(3) \u2212 f(1) = e\n- q\n(c) \u03bb(t) = 2te\n\u2212t2 /e\n(d) let z be a standard normal random variable. use the identity e[x] =\n0 p{x > x} dx to obtain\n\n\u22121 \u2212 e\n\u22129\n\u2212t2 = 2t\n\n\u22124/5.\n\n* q\n\ne[x] =\n= 2\n= 2\n= \u221a\n* q\n\n* q\n\u2212x2 dx\ne\n0\n\u2212y2/2 dy\n\u22121/2\ne\n2\u03c0p{z > 0}\n\u22121/2\n\n\u221a\n0\n\n\u03c0 /2\n\n(e) use the result of theoretical exercise 5 to obtain\n\u2212x2\n\n\u2212x2 dx = \u2212e\n\n2xe\n\n= 1\n\ne[x2] =\nhence, var(x) = 1 \u2212 \u03c0/4.\n\n5.15. (a) p{x > 6} = exp{\u2212-\n\n0\n\n\u03bb(t)dt} = e\n\n\u22123.45\n\n6\n0\n\n(b)\n\n6666q\n\n0\n\np{x < 8|x > 6} = 1 \u2212 p{x > 8|x > 6}\n\n= 1 \u2212 p{x > 8}/p{x > 6}\n= 1 \u2212 e\nl .8892\n\n\u22125.65/e\n\n\u22123.45\n\n5.16. for x \u00fa 0,\n\nf1/x (x) = p{1/x \u2026 x}\n\n= p{x \u2026 0} + p{x \u00fa 1/x}\n= 1/2 + 1 \u2212 fx (1/x)\n\n "}, {"Page_number": 504, "text": "solutions to self-test problems and exercises 489\n\ndifferentiation yields\n\nf1/x (x) = x\n=\n= fx (x)\n\n\u22122fx (1/x)\n1\nx2\u03c0(1 + (1/x)2)\n\nthe proof when x < 0 is similar.\n\n5.17. if x denotes the number of the first n bets that you win, then the amount that\n\nyou will be winning after n bets is\n\n35x \u2212 (n \u2212 x) = 36x \u2212 n\n\nthus, we want to determine\n\np = p{36x \u2212 n > 0} = p{x > n/36}\n\nwhen x is a binomial random variable with parameters n and p = 1/38.\n(a) when n = 34,\n\n7\n\np = p{x \u00fa 1}\n= p{x > .5}\n= p\n\n0\n0\n\n= p\nl \u0001(.4229)\nl .6638\n\n.\n.\n\n(the continuity correction)\n.\n.5 \u2212 34/38\n7\n34(1/38)(37/38)\n> \u2212.4229\n\nx \u2212 34/38\n34(1/38)(37/38)\nx \u2212 34/38\n34(1/38)(37/38)\n\n>\n\n(because you will be ahead after 34 bets if you win at least 1 bet, the exact\nprobability in this case is 1 \u2212 (37/38)34 = .5961.)\n\n(b) when n = 1000,\n0\n\n.\n\np = p{x > 27.5}\n= p\nl 1 \u2212 \u0001(.2339)\nl .4075\n\nx \u2212 1000/38\n1000(1/38)(37/38)\n\n7\n\n.\n\n27.5 \u2212 1000/38\n1000(1/38)(37/38)\n\n>\n\nthe exact probability\u2014namely, the probability that a binomial n = 1000,\np = 1/38 random variable is greater than 27\u2014is .3961.\n\n "}, {"Page_number": 505, "text": "490\n\nsolutions to self-test problems and exercises\n\n(c) when n = 100, 000,\n.\n\n0\n\np = p{x > 2777.5}\n= p\nl 1 \u2212 \u0001(2.883)\nl .0020\n\nx \u2212 100000/38\n100000(1/38)(37/38)\n\n7\n\n.\n2777.5 \u2212 100000/38\n100000(1/38)(37/38)\n\n>\n\nthe exact probability in this case is .0021.\n\n5.18. if x denotes the lifetime of the battery, then the desired probability,\n\np{x > s + t|x > t}, can be determined as follows:\n\np{x > s + t|x > t} = p{x > s + t, x > t}\n\np{x > t}\n= p{x > s + t}\np{x > t}\np{x>s+t|battery is type 1}p1\n+p{x>s+t|battery is type 2}p2\np{x>t|battery is type 1}p1\n+p{x>t|battery is type 2}p2\n\n=\n\n= e\n\n\u2212\u03bb1(s+t)p1 + e\ne\u2212\u03bb1tp1 + e\u2212\u03bb2tp2\n\n\u2212\u03bb2(s+t)p2\n\nanother approach is to directly condition on the type of battery and then\nuse the lack-of-memory property of exponential random variables. that is,\nwe could do the following:\np{x > s + t|x > t} = p{x > s + t|x > t, type 1}p{type 1|x > t}\n\n+ p{x > s + t|x > t, type 2}p{type 2|x > t}\n\u2212\u03bb2sp{type 2|x > t}\n\u2212\u03bb1sp{type 1|x > t} + e\n\n= e\n\nnow for i = 1, 2, use\n\np{type i|x > t} = p{type i, x > t}\n\np{x > t}\n\n=\n\n=\n\np{x > t|type i}pi\n\np{x > t|type 1}p1 + p{x > t|type 2}p2\ne\u2212\u03bb1tp1 + e\u2212\u03bb2tp2\n\n\u2212\u03bbitpi\ne\n\n5.19. let xi be an exponential random variable with mean i, i = 1, 2.\n\n(a) the value c should be such that p{x1 > c} = .05. therefore,\n\n\u2212c = .05 = 1/20\ne\n\nor c = log(20) = 2.996.\n\n(b)\n\np{x2 > c} = e\n\n\u2212c/2 = 1\u221a\n20\n\n= .2236\n\n "}, {"Page_number": 506, "text": "5.20. (a)\n\ne[(z \u2212 c)+\n\nsolutions to self-test problems and exercises 491\n\n* q\n* q\n* q\n\nc\n\n\u2212q\n\n(x \u2212 c)+\n(x \u2212 c)e\n\n] = 1\u221a\n\u2212x2/2 dx\ne\n2\u03c0\n= 1\u221a\n\u2212x2/2 dx\n2\u03c0\n\u2212x2/2 dx \u2212 1\u221a\n= 1\u221a\nxe\n2\u03c0\n2\u03c0\n= \u2212 1\u221a\n\u2212x2/2 |q\ne\nc\n2\u03c0\n\u2212c2/2 \u2212 c(1 \u2212 \u0001(c))\ne\n\nc\n\n= 1\u221a\n2\u03c0\n\n\u2212 c(1 \u2212 \u0001(c))\n\n* q\n\nc\n\n\u2212x2/2 dx\n\nc e\n\n(b) using the fact that x has the same distribution as \u03bc + \u03c3 z, where z is a\n\nstandard normal random variable, yields\n\ne[(x \u2212 c)+\n\nwhere a = c\u2212\u03bc\n\n\u03c3\n\n.\n\n]\n\n\u03c3\n\n(cid:3)(cid:5)+\u23a4\n(cid:2)\n] = e[(\u03bc + \u03c3 z \u2212 c)+\n\u23a6\nz \u2212 c \u2212 \u03bc\n= e\n(cid:21)\n(cid:3)+(cid:8)\n\n\u23a1\n\u23a3(cid:4)\n(cid:20)\n\u03c3 (z \u2212 c \u2212 \u03bc\n(cid:7)(cid:2)\n)+\nz \u2212 c \u2212 \u03bc\n= \u03c3 e\n(cid:7)\n\n= e\n\n\u03c3\n\n\u03c3\n\n\u03c3\n\n= \u03c3\n\n1\u221a\n2\u03c0\n\n\u2212a2/2 \u2212 a(1 \u2212 \u0001(a))\ne\n\n(cid:8)\n\nchapter 6\n\n6.1. (a) 3c + 6c = 1 * c = 1/9\n\n(b) let p(i, j) = p{x = i, y = j}. then\n\np(1, 1) = 4/9, p(1, 0) = 2/9, p(0, 1) = 1/9, p(0, 0) = 2/9\n\n(1/9)6(2/9)6\n\n(c)\n\n(d)\n\n(e)\n\n(12)!\n26\n(12)!\n(4!)3\n\n(cid:2)\n\n12(cid:9)\n\ni=8\n\n(1/3)12\n\n(cid:3)\n\n12\ni\n\n(2/3)i(1/3)12\u2212i\n\n6.2. (a) with pj = p{xyz = j}, we have\n\np6 = p2 = p4 = p12 = 1/4\n\nhence,\n\ne[xyz] = (6 + 2 + 4 + 12)/4 = 6\n\n "}, {"Page_number": 507, "text": "492\n\nsolutions to self-test problems and exercises\n\n(b) with qj = p{xy + xz + yz = j}, we have\n\nq11 = q5 = q8 = q16 = 1/4\n\nhence,\n\ne[xy + xz + yz] = (11 + 5 + 8 + 16)/4 = 10\n\n6.3. in this solution, we will make use of the identity\n\u2212xxn dx = n!\ne\n\n\u2212xxn/n!, x > 0, is the density function of a gamma\nwhich follows because e\nrandom variable with parameters n + 1 and \u03bb and must thus integrate to 1.\n(a)\n\n*\n\n(b) since the joint density is nonzero only when y > x and y > \u2212x, we have,\n\nhence, c = 1/4.\n\nfor x > 0,\n\n* q\n\n0\n\n* q\n* q\n\n0\n\n0\n\n1 = c\n= c\n\n(y \u2212 x) dx dy\n\ny\n\u2212y\n\n\u2212y\ne\n\u2212y2y2 dy = 4c\ne\n\n(y \u2212 x)e\n\n\u2212y dy\n\n\u2212(x+u) du\n\nue\n\n* q\n* q\n\nx\n\n0\n\u2212x\ne\n\nfx (x) = 1\n4\n= 1\n4\n= 1\n4\n\n* q\n\nfor x < 0,\n\n-\ny\u2212y\n\nfy (y) = 1\n4e\n\n\u2212y\n\n(c)\n(d)\n\n\u2212y dy\n\u2212y + xe\n\n\u2212y]q\u2212x\n\n(cid:8)\n(\u22122x2ex + xex) dx\n\n(cid:21)\n\u2212y) dy\n\n\u2212y\n\n2y2e\n\n\u2212x\n[\u2212ye\n\n(y \u2212 x)e\nfx (x) = 1\n4\n= 1\n\u2212y \u2212 e\n4\n= (\u22122xex + ex)/4\n(y \u2212 x) dx = 1\n(cid:7)* q\n(cid:20)\n1 \u2212\n[1 \u2212 4 \u2212 1] = \u22121\n\n\u2212xdx +\n* q\nxe\n\n0\n\u2212q\n\n(2y2e\n\n*\n\n0\n\n\u2212y + ye\n\n0\n\ne[x] = 1\n4\n= 1\n4\n= 1\n4\n\u2212y dy = 3\n\n0 y3e\n\n- q\n\n(e) e[y] = 1\n\n2\n\n "}, {"Page_number": 508, "text": "solutions to self-test problems and exercises 493\n6.4. the multinomial random variables xi, i = 1, . . . , r, represent the numbers of\neach of the types of outcomes 1, . . . , r that occur in n independent trials when\neach trial results in one of the outcomes 1, . . . , r with respective probabili-\nties p1, . . . , pr. now, say that a trial results in a category 1 outcome if that\ntrial resulted in any of the outcome types 1, . . . , r1; say that a trial results\nin a category 2 outcome if that trial resulted in any of the outcome types\nr1 + 1, . . . , r1 + r2; and so on. with these definitions, y1, . . . , yk represent\nthe numbers of category 1 outcomes, category 2 outcomes, up to category k\noutcomes when n independent trials that each result in one of the categories\n1, . . . , k with respective probabilities\nbut by definition, such a vector has a multinomial distribution.\n\n(cid:9)ri\u22121+ri\nj=ri\u22121+1 pj, i = 1, . . . , k, are performed.\n\n6.5. (a) letting pj = p{xyz = j}, we have\np2 = 3/8,\n\np4 = 3/8,\n(b) letting pj = p{xy + xz + yz = j}, we have\np8 = 3/8,\n\np1 = 1/8,\n\np3 = 1/8,\n\np5 = 3/8,\n\np8 = 1/8\n\np12 = 1/8\n\n(c) letting pj = p{x2 + yz = j}, we have\np5 = 1/4,\n\np2 = 1/8,\n\np6 = 1/4,\n\np8 = 1/8\n\n6.6. (a)\n\np3 = 1/4,\n*\n*\n\n0\n\n*\n\n5\n\n1\n\n1\n\n1 =\n=\n= 12c + 2/5\n\n1\n\n0\n\n(x/5 + cy) dy dx\n\n(4x/5 + 12c) dx\n\nhence, c = 1/20.\n\n(b) no, the density does not factor.\n(c)\n\n*\n\n*\n*\n\n1\n\n(x/5 + y/20) dy dx\n\np{x + y > 3} =\n=\n= 1/5 + 1/15 + 5/8 \u2212 19/120 = 11/15\n\n5\n3\u2212x\n[(2 + x)x/5 + 25/40 \u2212 (3 \u2212 x)2/40] dx\n\n0\n\n1\n\n0\n\n6.7. (a) yes, the joint density function factors.\n\n-\n-\n0 ydy = 2x,\n0 xdx = y/2,\n\n2\n\n1\n\n0 < x < 1\n0 < y < 2\n\np{x < x, y < y} = p{x < x}p{y < y}\n\n= min(1, x2) min(1, y2/4),\n\nx > 0, y > 0\n\n2\n\n0 y2/2 dy = 4/3\n\n(b) fx (x) = x\nfy (y) = y\n(c)\n(d)\n\n(e) e[y] =-\n\n "}, {"Page_number": 509, "text": "494\n\nsolutions to self-test problems and exercises\n\n(f)\n\n*\n\n*\n\n0\n\n1\u2212x\n\ny dy dx\n\nx(1 \u2212 x)2 dx = 1/24\n\np{x + y < 1} =\n0\n= 1\n2\n\n1\n\n*\n\nx\n\n1\n\n0\n\n6.8. let ti denote the time at which a shock type i, of i = 1, 2, 3, occurs. for\n\ns > 0, t > 0,\n\np{x1 > s, x2 > t} = p{t1 > s, t2 > t, t3 > max(s, t)}\n\n= p{t1 > s}p{t2 > t}p{t3 > max(s, t)}\n= exp{\u2212\u03bb1s} exp{\u2212\u03bb2t} exp{\u2212\u03bb3 max(s, t)}\n= exp{\u2212(\u03bb1s + \u03bb2t + \u03bb3 max(s, t))}\n\n6.9. (a) no, advertisements on pages having many ads are less likely to be chosen\n\nthan are ones on pages with few ads.\n1\nm\n\nn(i)\nn\n\n(b)\n\nn(i)\n\n= n/n, where n = m(cid:6)\n\nm(cid:6)\n\ni=1\n\nn(i)/m\n\ni=1\n\nnm\n\n(c)\n(d) (1 \u2212 n/n)k\u22121 1\nm\n\nq(cid:6)\n\nn(i)\nn\n\n1\nn(i)\n\n= (1 \u2212 n/n)k\u22121/(nm)\n\n(e)\n\n1\nnm\n\n(1 \u2212 n/n)k\u22121 = 1\nnm\n\n.\n\nk=1\n\n\u221a\n(f) the number of iterations is geometric with mean n\nn\n\n6.10. (a) p{x = i} = 1/m,\n\ni = 1, . . . , m.\n\n(b) step 2. generate a uniform (0, 1) random variable u. if u < n(x)/n,\n\ngo to step 3. otherwise return to step 1.\nstep 3. generate a uniform (0, 1) random variable u, and select the\nelement on page x in position [n(x)u] + 1.\n\n6.11. yes, they are independent. this can be easily seen by considering the equiva-\nlent question of whether xn is independent of n. but this is indeed so, since\nknowing when the first random variable greater than c occurs does not affect\nthe probability distribution of its value, which is the uniform distribution\non (c, 1).\n\n6.12. let pi denote the probability of obtaining i points on a single throw of the dart.\n\nthen\n\np30 = \u03c0/36\np20 = 4\u03c0/36 \u2212 p30 = \u03c0/12\np10 = 9\u03c0/36 \u2212 p20 \u2212 p30 = 5\u03c0/36\np0 = 1 \u2212 p10 \u2212 p20 \u2212 p30 = 1 \u2212 \u03c0/4\n\n(a) \u03c0/12\n(b) \u03c0/9\n(c) 1 \u2212 \u03c0/4\n\n "}, {"Page_number": 510, "text": "solutions to self-test problems and exercises 495\n\n(d) \u03c0(30/36 + 20/12 + 50/36) = 35\u03c0/9\n(e) (\u03c0/4)2\n(f) 2(\u03c0/36)(1 \u2212 \u03c0/4) + 2(\u03c0/12)(5\u03c0/36)\n4(cid:6)\n\n6.13. let z be a standard normal random variable.\n\n(a)\n\n\u23a7\u23a8\n\u23a9 4(cid:6)\n\ni=1\n\np\n\nxi > 0\n\n\u23a7\u23a8\n\u23a9 4(cid:6)\n\ni=1\n\np\n\n666 2(cid:6)\n\ni=1\n\nxi > 0\n\nxi = \u22125\n\n>\n\n24\n\n24\n\ni=1\n\n\u22126\u221a\n\nxi \u2212 6\n\u221a\n\n\u23ab\u23aa\u23aa\u23ac\n\u23a7\u23aa\u23aa\u23a8\n\u23ab\u23ac\n\u23aa\u23aa\u23ad\n\u23aa\u23aa\u23a9\n\u23ad = p\nl p{z > \u22121.2247} l .8897\n\u23ab\u23ac\n\u23ad = p{x3 + x4 > 5}\n\u221a\nx3 + x4 \u2212 3\n= p\n> 2/\nl p{z > .5774} l .2818\n\n\u221a\n12\n\n0\n\n7\n\n12\n\n(b)\n\n(c)\n\n\u23a7\u23a8\n\u23a9 4(cid:6)\n\ni=1\n\np\n\n\u23ab\u23ac\n\u23ad = p{x2 + x3 + x4 > \u22125}\nxi > 0|x1 = 5\nx2 + x3 + x4 \u2212 4.5\n= p\nl p{z > \u22122.239} l .9874\n\n\u221a\n18\n\n0\n\n7\n\n\u221a\n> \u22129.5/\n18\n\n6.14. in the following, c does not depend on n.\n\np{n = n|x = x} = fx|n(x|n)p{n = n}/fx (x)\n\n1\n\n(n \u2212 1)!\n\n(\u03bbx)n\u22121(1 \u2212 p)n\u22121\n\n= c\n= c(\u03bb(1 \u2212 p)x)n\u22121/(n \u2212 1)!\n\nwhich shows that, conditional on x = x, n \u2212 1 is a poisson random variable\nwith mean \u03bb(1 \u2212 p)x. that is,\n\np{n = n|x = x} = p{n \u2212 1 = n \u2212 1|x = x}\n\n= e\n\n\u2212\u03bb(1\u2212p)x(\u03bb(1 \u2212 p)x)n\u22121/(n \u2212 1)!, n \u00fa 1.\n\n6.15. (a) the jacobian of the transformation is\n\n6666 1 0\n\n1 1\n\n6666 = 1\n\nj =\n\nas the equations u = x, v = x + y imply that x = u, y = v \u2212 u, we obtain\n\nfu,v (u, v) = fx,y (u, v \u2212 u) = 1,\n\n0 < u < 1,\n\n0 < v \u2212 u < 1\n\n "}, {"Page_number": 511, "text": "496\n\nsolutions to self-test problems and exercises\n\nor, equivalently,\n\nfu,v (u, v) = 1, max(v \u2212 1, 0) < u < min(v, 1)\n\n(b) for 0 < v < 1,\n\nfor 1 \u2026 v \u2026 2,\n\n*\n\nv\n\ndu = v\n\n0\n\nfv (v) =\n*\n\nfv (v) =\n\n1\nv\u22121\n\ndu = 2 \u2212 v\n\n6.16. let u be a uniform random variable on (7, 11). if you bid x, 7 \u2026 x \u2026 10, you\n\nwill be the high bidder with probability\nu \u2212 7\n\n(p{u < x})3 =\n\np\n\n(cid:4)\n\n%\n\nx \u2212 7\n\n4\n\n<\n\n4\n\n(cid:2)\n\n/(cid:5)3 =\n\n(cid:3)3\n\nx \u2212 7\n\n4\n\nhence, your expected gain\u2014call it e[g(x)]\u2014if you bid x is\n\ne[g(x)] = 1\n4\n\n(x \u2212 7)3(10 \u2212 x)\n\ncalculus shows this is maximized when x = 37/4.\n\n6.17. let i1, i2, . . . , in, be a permutation of 1, 2, . . . , n. then\n\np{x1 = i1, x2 = i2, . . . , xn = in} = p{x1 = i1}p{x2 = i2}\u00b7\u00b7\u00b7 p{xn = in}\n\n\u00b7\u00b7\u00b7 pin\n= pi1pi2\n= p1p2 \u00b7\u00b7\u00b7 pn\n\ni=1\n\ni=1\n\nnn when\n\ntherefore, the desired probability is n! p1p2 \u00b7\u00b7\u00b7 pn, which reduces to n!\nall pi = 1/n.\n6.18. (a) because\n\nxi = n(cid:9)\n\nn(cid:9)\n\nyi, it follows that n = 2m.\n(cid:2)\n\n(cid:3)\n\n(b) consider the n \u2212 k coordinates whose y-values are equal to 0, and call\nthem the red coordinates. because the k coordinates whose x-values are\nequal to 1 are equally likely to be any of the\nsets of k coordinates,\nit follows that the number of red coordinates among these k coordinates\nhas the same distribution as the number of red balls chosen when one\nrandomly chooses k of a set of n balls of which n \u2212 k are red. therefore,\nm is a hypergeometric random variable.\n\n(c) e[n] = e[2m] = 2e[m] = 2k(n\u2212k)\n(d) using the formula for the variance of a hypergeometric given in\n\nn\nk\n\nn\n\nexample 8j of chapter 4, we obtain\nvar(n) = 4 var(m) = 4\n\n6.19. (a) first note that sn \u2212 sk = n(cid:9)\n\nn \u2212 k\nn \u2212 1\n\nk(1 \u2212 k/n)(k/n)\n\nzi is a normal random variable with mean\n0 and variance n \u2212 k that is independent of sk. consequently, given that\nsk = y, sn is a normal random variable with mean y and variance n \u2212 k.\n\ni=k+1\n\n "}, {"Page_number": 512, "text": "(y)\n\n(cid:5)\n\n(cid:4)\n\nfsk|sn\n\nsolutions to self-test problems and exercises 497\n(b) because the conditional density function of sk given that sn = x is a\ndensity function whose argument is y, anything that does not depend on\ny can be regarded as a constant. (for instance, x is regarded as a fixed\nconstant.) in the following, the quantities ci, i = 1, 2, 3, 4 are all constants\nthat do not depend on y:\n(y|x) = fsk,sn\n(y, x)\nfsn\n(x)\n(x|y)fsk\n= c1fsn|sk\n0\n= c1\n\u221a\n\u221a\n1\nn \u2212 k\n2\u03c0\n\u2212 (x \u2212 y)2\n= c2 exp\n0\n2(n \u2212 k)\n\u2212\n2xy\n0\n2(n \u2212 k)\n\u2212\n\u23a7\u23a8\nn\n\u23a9\u2212\n0\n\u2212\n\n\u2212 y2\n(cid:3)7\n(cid:2)\n2(n \u2212 k)\n2k\ny2 \u2212 2\nk\n(cid:7)(cid:2)\n(cid:3)2 \u2212\nn\ny \u2212 k\n7\n(cid:2)\n(cid:3)2\nn\n\nwhere c1 = 1\nfsn\n(x)\n\u221a\n\n\u2212 y2\n2k\ny2\n\n2k(n \u2212 k)\n\n2k(n \u2212 k)\n\n\u2212(x\u2212y)2/2(n\u2212k)\n\n(cid:8)\u23ab\u23ac\n\u23ad\n\n= c3 exp\n\n= c3 exp\n\n= c3 exp\n\n\u2212y2/2k\ne\n\n1\u221a\n2\u03c0\n\n(cid:3)2\n\n7\n\n7\n\n(cid:2)\n\nk\nn\n\nxy\n\nk\n\nn\n\nn\n\nx\n\nx\n\ne\n\n= c4 exp\n\n2k(n \u2212 k)\n\ny \u2212 k\nn\n\nx\n\nbut we recognize the preceding as the density function of a normal ran-\ndom variable with mean\n\nk(n \u2212 k)\n\nx and variance\n\n.\n\nk\nn\n\nn\n\n6.20. (a)\n\np{x6 > x1|x1 = max(x1, . . . , x5)}\n\n= p{x6 > x1, x1 = max(x1, . . . , x5)}\n\np{x1 = max(x1, . . . , x5)}\n\n= p{x6 = max(x1, . . . , x6), x1 = max(x1, . . . , x5)}\n= 5\n\n1/5\n\n1\n6\n\n1\n5\n\n= 1\n6\n\nthus, the probability that x6 is the largest value is independent of which\nis the largest of the other five values. (of course, this would not be true if\nthe xi had different distributions.)\n\n(b) one way to solve this problem is to condition on whether x6 > x1. now,\n\np{x6 > x2|x1 = max(x1, . . . , x5), x6 > x1} = 1\n\n "}, {"Page_number": 513, "text": "498\n\nsolutions to self-test problems and exercises\n\nalso, by symmetry,\n\np{x6 > x2|x1 = max(x1, . . . , x5), x6 < x1} = 1\n2\n\nfrom part (a),\n\np{x6 > x1|x1 = max(x1, . . . , x5)} = 1\n6\nthus, conditioning on whether x6 > x1 yields the result\n+ 1\n5\n2\n6\n\np{x6 > x2|x1 = max(x1, . . . , x5)} = 1\n6\n\n= 7\n12\n\nchapter 7\n\n7.1. (a) d = m(cid:9)\n(cid:20)\n\ni=1\n\n1/n(i)\n\n(cid:21)\n\n(b) p{x = i} = p{[mu] = i \u2212 1} = p{i \u2212 1 \u2026 mu < i} = 1/m,\n(c) e\n\np{x = i} = m(cid:6)\n\n= m(cid:9)\n\n= d\n\nm\n\nn(x)\n\nm\nn(i)\n\ni=1\n\nm\nn(i)\n\n1\nm\n\ni=1\n\n7.2. let ij equal 1 if the jth ball withdrawn is white and the (j + 1)st is black, and\nlet ij equal 0 otherwise. if x is the number of instances in which a white ball is\nimmediately followed by a black one, then we may express x as\n\ni = 1, . . . , m\n\nx = n+m\u22121(cid:6)\n\nij\n\nj=1\n\nthus,\n\ne[x] = n+m\u22121(cid:6)\n= n+m\u22121(cid:6)\n\nj=1\n\nj=1\n\ne[ij]\n\np{jth selection is white, (j + 1)st is black}\n\np{ jth selection is white}p{ j + 1)st is black|jth is white}\n\n= n+m\u22121(cid:6)\n= n+m\u22121(cid:6)\n\nj=1\n\nj=1\n= nm\nn + m\n\nn\n\nn + m\n\nm\n\nn + m \u2212 1\n\nthe preceding used the fact that each of the n + m balls is equally likely to\nbe the jth one selected and, given that that selection is a white ball, each of the\nother n + m \u2212 1 balls is equally likely to be the next ball chosen.\n\n "}, {"Page_number": 514, "text": "solutions to self-test problems and exercises 499\n\n7.3. arbitrarily number the couples, and then let ij equal 1 if married couple num-\nber j, j = 1, . . . , 10, is seated at the same table. then, if x represents the\nnumber of married couples that are seated at the same table, we have\n\nso\n\n(cid:2)\n\n(cid:3)\n\n19\n3\n\n(a) to compute e[ij] in this case, consider wife number j. since each of the\ngroups of size 3 not including her is equally likely to be the remain-\ning members of her table, it follows that the probability that her husband\nis at her table is\n\n(cid:2)\n\nx = 10(cid:6)\ne[x] = 10(cid:6)\n\nj=1\n\nj=1\n\nij\n\ne[ij]\n\n(cid:3)(cid:2)\n(cid:3)\n(cid:3) = 3\n(cid:2)\n\n18\n2\n\n1\n1\n\n19\n\n19\n3\n\nhence, e[ij] = 3/19 and so\n\ne[x] = 30/19\n\n(b) in this case, since the 2 men at the table of wife j are equally likely to be\nany of the 10 men, it follows that the probability that one of them is her\nhusband is 2/10, so\n\ne[ij] = 2/10\n\nand e[x] = 2\n\n7.4. from example 2i, we know that the expected number of times that the die\nneed be rolled until all sides have appeared at least once is 6(1 + 1/2 + 1/3 +\n1/4 + 1/5 + 1/6) = 14.7. now, if we let xi denote the total number of times\nthat side i appears, then, since\nxi is equal to the total number of rolls, we\n\n6(cid:9)\n\u23a1\n\u23a3 6(cid:6)\n\ni=1\n\nxi\n\ni=1\n\n\u23a4\n\u23a6 = 6(cid:6)\n\ni=1\n\n14.7 = e\n\ne[xi]\n\nhave\n\n7.5. let ij equal 1 if we win 1 when the jth red card to show is turned over, and let\nij equal 0 otherwise. (for instance, i1 will equal 1 if the first card turned over\nis red.) hence, if x is our total winnings, then\n\nbut, by symmetry, e[xi] will be the same for all i, and thus it follows from the\npreceding that e[x1] = 14.7/6 = 2.45.\n\u23a1\n\u23a2\u23a3 n(cid:6)\n\n\u23a4\n\u23a5\u23a6 = n(cid:6)\n\ne[x] = e\n\ne[ij]\n\nij\n\nj=1\n\nj=1\n\n "}, {"Page_number": 515, "text": "500\n\nsolutions to self-test problems and exercises\n\nnow, ij will equal 1 if j red cards appear before j black cards. by symmetry, the\nprobability of this event is equal to 1/2; therefore, e[ij] = 1/2 and e[x] = n/2.\n7.6. to see that n \u2026 n \u2212 1 + i, note that if all events occur, then both sides of\nthe preceding inequality are equal to n, whereas if they do not all occur, then\nthe inequality reduces to n \u2026 n \u2212 1, which is clearly true in this case. taking\nexpectations yields\n\ne[n] \u2026 n \u2212 1 + e[i]\n\u23a1\n\u23a3 n(cid:6)\n\ne[ii] = n(cid:6)\n\nhowever, if we let ii equal 1 if ai occurs and 0 otherwise, then\n\n\u23a4\n\u23a6 = n(cid:6)\ni=1\nsince e[i] = p(a1 \u00b7\u00b7\u00b7 an), the result follows.\n7.7. imagine that the values 1, 2, . . . , n are lined up in their numerical order and that\nthe k values selected are considered special. from example 3e, the position\nof the first special value, equal to the smallest value chosen, has mean 1 +\nn \u2212 k\nk + 1\nfor a more formal argument, note that x \u00fa j if none of the j \u2212 1 smallest\nvalues are chosen. hence,\n\n= n + 1\nk + 1\n\ne[n] = e\n\np(ai)\n\ni=1\n\ni=1\n\nii\n\n.\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n(cid:2)\n\n=\n\n(cid:3)\n(cid:3)\n\nn \u2212 k\nj \u2212 1\nn\nj \u2212 1\n\nn \u2212 j + 1\n(cid:2)\n(cid:3)\n\nk\nn\nk\n\np{x \u00fa j} =\n\nwhich shows that x has the same distribution as the random variable of exam-\nple 3e (with the notational change that the total number of balls is now n and\nthe number of special balls is k).\n7.8. let x denote the number of families that depart after the sanchez family\nleaves. arbitrarily number all the n \u2212 1 non-sanchez families, and let ir,\n1 \u2026 r \u2026 n \u2212 1, equal 1 if family r departs after the sanchez family does. then\n\nx = n\u22121(cid:6)\n\nir\n\nr=1\n\ntaking expectations gives\n\ne[x] = n\u22121(cid:6)\n\nr=1\n\np{family r departs after the sanchez family}\n\nnow consider any non-sanchez family that checked in k pieces of luggage.\nbecause each of the k + j pieces of luggage checked in either by this family or\nby the sanchez family is equally likely to be the last of these k + j to appear,\nthe probability that this family departs after the sanchez family is k\nk+j . because\nthe number of non-sanchez families who checked in k pieces of luggage is nk\nwhen k z j, or nj \u2212 1 when k = j, we obtain\nknk\nk + j\n\ne[x] =\n\n\u2212 1\n2\n\n(cid:6)\n\nk\n\n "}, {"Page_number": 516, "text": "solutions to self-test problems and exercises 501\n\n7.9. let the neighborhood of any point on the rim be the arc starting at that point\nand extending for a length 1. consider a uniformly chosen point on the rim\nof the circle\u2014that is, the probability that this point lies on a specified arc of\nlength x is\n\u2014and let x denote the number of points that lie in its neighbor-\nhood. with ij defined to equal 1 if item number j is in the neighborhood of the\nrandom point and to equal 0 otherwise, we have\n\nx\n2\u03c0\n\nx = 19(cid:6)\n\nij\n\nj=1\n\ntaking expectations gives\n\ne[x] = 19(cid:6)\n\nj=1\n\np{item j lies in the neighborhood of the random point}\n\nbut because item j will lie in its neighborhood if the random point is located\non the arc of length 1 going from item j in the counterclockwise direction, it\nfollows that\n\np{item j lies in the neighborhood of the random point} = 1\n2\u03c0\n\nhence,\n\ne[x] = 19\n2\u03c0\n\n> 3\n\nbecause e[x] > 3, at least one of the possible values of x must exceed 3,\nproving the result.\n7.10. if g(x) = x1/2, then\n\ng\n\n(cid:8)(x) = 1\n2\n\n\u22123/2\nx\n\ng\n\n(cid:8)(cid:8)(x) = \u22121\n4\n\n\u22121/2,\nx\n\u221a\nx about \u03bb gives\n\u03bb\u22121/2(x \u2212 \u03bb) \u2212 1\n8\n\n\u03bb\u22123/2(x \u2212 \u03bb)2\n\nso the taylor series expansion of\n\n\u221a\n\n\u221a\nx l\n\n\u03bb + 1\n2\n\ntaking expectations yields\n\ne[\n\n\u221a\nx] l\n=\n=\n\n\u221a\n\u03bb\u22121/2e[x \u2212 \u03bb] \u2212 1\n\u03bb + 1\n8\n2\n\u221a\n\u03bb \u2212 1\n\u03bb\u22123/2\u03bb\n8\n\u221a\n\u03bb \u2212 1\n\u03bb\u22121/2\n8\n\n\u03bb\u22123/2e[(x \u2212 \u03bb)2]\n\n "}, {"Page_number": 517, "text": "502\n\nsolutions to self-test problems and exercises\n\nhence,\n\nvar(\n\n(cid:3)2\n\n\u221a\n\u221a\n(cid:2)\u221a\nx) = e[x] \u2212 (e[\nx])2\nl \u03bb \u2212\n\u03bb\u22121/2\n= 1/4 \u2212 1\n64\u03bb\nl 1/4\n\n\u03bb \u2212 1\n8\n\n7.11. number the tables so that tables 1, 2, and 3 are the ones with four seats and\ntables 4, 5, 6, and 7 are the ones with two seats. also, number the women, and\nlet xi,j equal 1 if woman i is seated with her husband at table j. note that\n\n(cid:3)\n\n18\n2\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)(cid:2)\n(cid:2)\n\n2\n2\n\n20\n4\n\ne[xi,j] =\n\n= 3\n95\n\n,\n\nj = 1, 2, 3\n\nand\n\ne[xi,j] =\n\n(cid:3) = 1\n\n190\n\n1(cid:2)\n\n20\n2\n\nj = 4, 5, 6, 7\n\n,\n\nnow, x denotes the number of married couples that are seated at the same\ntable, we have\n\ne[x] = e\n\n\u23a1\n\u23a2\u23a3 10(cid:6)\n3(cid:6)\n= 22(cid:6)\n\ni=1\n\ni=1\n\nj=1\n\n\u23a4\n7(cid:6)\n\u23a5\u23a6\ne[xi,j] + 19(cid:6)\n\nj=1\n\nxi,j\n\ni=1\n\n7(cid:6)\n\nj=4\n\ne[xi,j]\n\n7.12. let xi equal 1 if individual i does not recruit anyone, and let xi equal 0 other-\n\nwise. then\n\ne[xi] = p{i does not recruit any of i + 1, i + 2, . . . , n}\n\nhence,\n\ni + 1\n\ni\n\ni\n\n= i \u2212 1\n= i \u2212 1\nn \u2212 1\n\u23a1\n\u23a3 n(cid:6)\n\ne\n\ni=1\n\n\u00b7\u00b7\u00b7 n \u2212 2\nn \u2212 1\n\n\u23a4\n\u23a6 = n(cid:6)\n\ni=1\n\nxi\n\ni \u2212 1\nn \u2212 1\n\n= n\n2\n\n "}, {"Page_number": 518, "text": "solutions to self-test problems and exercises 503\n\nfrom the preceding we also obtain\n\nvar(xi) = i \u2212 1\nn \u2212 1\n\nnow, for i < j,\n\n(cid:2)\n1 \u2212 i \u2212 1\nn \u2212 1\n\n(cid:3)\n\n= (i \u2212 1)(n \u2212 i)\n\n(n \u2212 1)2\n\ne[xixj] = i \u2212 1\n\n\u00b7\u00b7\u00b7 j \u2212 2\nj \u2212 1\n= (i \u2212 1)(j \u2212 2)\n(n \u2212 2)(n \u2212 1)\n\ni\n\nj \u2212 2\n\nj\n\nj \u2212 1\nj + 1\n\n\u00b7\u00b7\u00b7 n \u2212 3\nn \u2212 1\n\nthus,\n\ntherefore,\n\n\u239b\n\u239d n(cid:6)\n\ni=1\n\nvar\n\n\u2212 i \u2212 1\nn \u2212 1\n\nj \u2212 1\nn \u2212 1\n\ncov(xi, xj) = (i \u2212 1)(j \u2212 2)\n(n \u2212 2)(n \u2212 1)\n= (i \u2212 1)(j \u2212 n)\n(n \u2212 2)(n \u2212 1)2\nn(cid:6)\nn\u22121(cid:6)\n\nvar(xi) + 2\ni=1\n(i \u2212 1)(n \u2212 i)\n\n\u239e\n\u23a0 = n(cid:6)\n= n(cid:6)\n\nj=i+1\n+ 2\n\nn\u22121(cid:6)\n\ni=1\n\nxi\n\n(n \u2212 1)2\nn(cid:6)\n\ni=1\n(i \u2212 1)(n \u2212 i)\n\ni=1\n\n1\n\n(n \u2212 1)2\n\n=\n\n\u2212\n\n(n \u2212 2)(n \u2212 1)2\n(cid:3)(cid:2)\n(cid:2)\n\ne[xi] =\n\n(cid:2)\n\n2\n1\n\ni=1\n\n(cid:3)\n\n(cid:3)(cid:2)\n(cid:3)\n\n4\n1\n\n3\n1\n9\n3\n\n= 2\n7\n\ncov(xi, xj)\n\nn(cid:6)\n\nj=i+1\n\n(i \u2212 1)(j \u2212 n)\n(n \u2212 2)(n \u2212 1)2\n\ni=1\n\n1\n\nn\u22121(cid:6)\n(i \u2212 1)(n \u2212 i)(n \u2212 i \u2212 1)\n\n7.13. let xi equal 1 if the ith triple consists of one of each type of player. then\n\nhence, for part (a), we obtain\n\ne\n\n\u23a4\n\u23a6 = 6/7\n\nxi\n\n\u23a1\n\u23a3 3(cid:6)\n\ni=1\n\nit follows from the preceding that\n\nvar(xi) = (2/7)(1 \u2212 2/7) = 10/49\n\n "}, {"Page_number": 519, "text": "504\n\nsolutions to self-test problems and exercises\n\nalso, for i z j,\n\ne[xixj] = p{xi = 1, xj = 1}\n(cid:3)\n\n(cid:3)(cid:2)\n(cid:2)\n\n(cid:3)(cid:2)\n(cid:3)\n\n(cid:2)\n(cid:3)(cid:2)\n= p{xi = 1}p{xj = 1|xi = 1}\n(cid:2)\n2\n1\n6\n3\n\n3\n1\n9\n3\n\n(cid:2)\n\n=\n\n2\n1\n\n4\n1\n\n1\n1\n\n(cid:3)\n\n(cid:3)(cid:2)\n(cid:3)\n\n3\n1\n\n= 6/70\n\u239e\n\u23a0 = 3(cid:6)\nhence, for part (b), we obtain\n\n\u239b\n\u239d 3(cid:6)\n\nvar\n\nxi\n\ni=1\n\ni=1\n\n(cid:6)(cid:6)\nvar(xi) + 2\n(cid:3)(cid:2)\n\n(cid:2)\n\ncov(xi, xj)\n\n(cid:3)\n\nj>1\n\u2212 4\n49\n\n6\n70\n\n3\n2\n\n= 30/49 + 2\n= 312\n490\n\n7.14. let xi, i = 1, . . ., 13, equal 1 if the ith card is an ace and let xi be 0 otherwise.\nlet yj equal 1 if the jth card is a spade and let i, j = 1, . . . , 13, be 0 otherwise.\nnow,\n\n\u239b\n\u239c\u239d n(cid:6)\ncov(x, y) = cov\nn(cid:6)\n\ni=1\n\n= n(cid:6)\n\n\u239e\n\u239f\u23a0\n\nn(cid:6)\n\nxi,\n\nyj\n\nj=1\n\ncov(xi, yj)\n\nj=1\n\ni=1\n\nhowever, xi is clearly independent of yj because knowing the suit of a par-\nticular card gives no information about whether it is an ace and thus cannot\naffect the probability that another specified card is an ace. more formally, let\nai, s, ai, h, ai, d, ai, c be the events, respectively, that card i is a spade, a heart, a\ndiamond, and a club. then\np{yj = 1} = 1\n(p{yj = 1|ai,s} + p{yj = 1|ai,h}\n4\n+ p{yj = 1|ai,d} + p{yj = 1|ai,c})\n\nbut, by symmetry, we have\n\np{yj = 1|ai,s} = p{yj = 1|ai,h} = p{yj = 1|ai,d} = p{yj = 1|ai,c}\n\ntherefore,\n\np{yj = 1} = p{yj = 1|ai,s}\n\nas the preceding implies that\n\np{yj = 1} = p{yj = 1|ac\n\ni,s\n\n}\n\n "}, {"Page_number": 520, "text": "solutions to self-test problems and exercises 505\nwe see that yj and xi are independent. hence, cov(xi, yj) = 0, and thus\ncov(x, y) = 0.\nthe random variables x and y, although uncorrelated, are not indepen-\n\ndent. this follows, for instance, from the fact that\n\np{y = 13|x = 4} = 0 z p{y = 13}\n\n7.15. (a) your expected gain without any information is 0.\n\n(b) you should predict heads if p > 1/2 and tails otherwise.\n(c) conditioning on v, the value of the coin, gives\n\n*\n*\n\n1\n\n0\n\ne[gain] =\n=\n= 1/2\n\n0\n\ne[gain|v = p] dp\n[1(1 \u2212 p) \u2212 1(p)] dp +\n\n1/2\n\n*\n\n1\n\n1/2\n\n[1(p) \u2212 1(1 \u2212 p)] dp\n\n7.16. given that the name chosen appears in n(x) different positions on the list,\nsince each of these positions is equally likely to be the one chosen, it fol-\nlows that\n\ne[i|n(x)] = p{i = 1|n(x)} = 1/n(x)\n\nhence,\n\ne[i] = e[1/n(x)]\n\nthus, e[mi] = e[m/n(x)] = d.\n\n7.17. letting xi equal 1 if a collision occurs when the ith item is placed, and letting\n\nit equal 0 otherwise, we can express the total number of collisions x as\n\nx = m(cid:6)\ne[x] = m(cid:6)\n\ni=1\n\ni=1\n\nxi\n\ne[xi]\n\ntherefore,\n\nto determine e[xi], condition on the cell in which it is placed.\n\ne[xi] =\n=\n\n=\n\nj\n\n(cid:6)\ne[xi| placed in cellj]pj\n(cid:6)\np{i causes collision|placed in cell j]pj\n(cid:6)\n[1 \u2212 (1 \u2212 pj)i\u22121]pj\n(cid:6)\n(1 \u2212 pj)i\u22121pj\n\nj\n\nj\n\n= 1 \u2212\n\nj\n\nthe next to last equality used the fact that, conditional on item i being placed\nin cell j, item i will cause a collision if any of the preceding i \u2212 1 items were\nput in cell j. thus,\n\n "}, {"Page_number": 521, "text": "506\n\nsolutions to self-test problems and exercises\n\ninterchanging the order of the summations gives\n\nn(cid:6)\n\ne[x] = m \u2212 m(cid:6)\ne[x] = m \u2212 n + n(cid:6)\n\nj=1\n\ni=1\n\n(1 \u2212 pj)i\u22121pj\n\n(1 \u2212 pj)m\n\nj=1\n\nlooking at the result shows that we could have derived it more easily by taking\nexpectations of both sides of the identity\n\nnumber of nonempty cells = m \u2212 x\n\nthe expected number of nonempty cells is then found by defining an indicator\nvariable for each cell, equal to 1 if that cell is nonempty and to 0 otherwise,\nand then taking the expectation of the sum of these indicator variables.\n\n7.18. let l denote the length of the initial run. conditioning on the first value gives\n\ne[l] = e[l|first value is one]\n\nn\n\nn + m\n\n+ e[l|first value is zero]\n\nm\n\nn + m\n\nnow, if the first value is one, then the length of the run will be the position of\nthe first zero when considering the remaining n + m \u2212 1 values, of which n \u2212 1\nare ones and m are zeroes. (for instance, if the initial value of the remaining\nn + m \u2212 1 is zero, then l = 1.) as a similar result is true given that the\nfirst value is a zero, we obtain from the preceding, upon using the result from\nexample 3e, that\n\ne[l] = n + m\nm + 1\nm + 1\n\n=\n\nn\n\nn\n\nn + m\n+ m\nn + 1\n\n+ n + m\nn + 1\n\nm\n\nn + m\n\n7.19. let x be the number of flips needed for both boxes to become empty, and let\n\ny denote the number of heads in the first n + m flips. then\n\ne[x] = n+m(cid:6)\n= n+m(cid:6)\n\ni=0\n\ni=0\n\ne[x|y = i]p{y = i}\nn + m\n\ne[x|y = i]\n\n(cid:2)\n\ni\n\n(cid:3)\n\npi(1 \u2212 p)n+m\u2212i\n\nnow, if the number of heads in the first n + m flips is i, i \u2026 n, then the number\nof additional flips is the number of flips needed to obtain an additional n \u2212 i\nheads. similarly, if the number of heads in the first n + m flips is i, i > n,\nthen, because there would have been a total of n + m \u2212 i < m tails, the\nnumber of additional flips is the number needed to obtain an additional i \u2212 n\nheads. since the number of flips needed for j outcomes of a particular type is a\nnegative binomial random variable whose mean is j divided by the probability\nof that outcome, we obtain\n\n "}, {"Page_number": 522, "text": "solutions to self-test problems and exercises 507\n\n(cid:2)\n\ne[x] = n(cid:6)\n+ n+m(cid:6)\n\ni=0\n\nn \u2212 i\np\n\n(cid:3)\n\nn + m\n(cid:2)\n\ni\n\npi(1 \u2212 p)n+m\u2212i\n(cid:3)\n\npi(1 \u2212 p)n+m\u2212i\n\n7.20. taking expectations of both sides of the identity given in the hint yields\n\n(cid:21)\n\ni=n+1\n\ni\n\ni \u2212 n\nn + m\n1 \u2212 p\n(cid:20)\n* q\n* q\n* q\n* q\n\nxn\u22121ix (x) dx\ne[xn\u22121ix (x)] dx\nxn\u22121e[ix (x)] dx\nxn\u22121f(x) dx\n\nn\n\n0\n\n0\n\n0\n\ne[xn] = e\n= n\n= n\n= n\n\n0\n\ntaking the expectation inside the integral sign is justified because all the ran-\ndom variables ix (x), 0 < x < q, are nonnegative.\n\n7.21. consider a random permutation i1, . . . , in that is equally likely to be any of the\n\nn! permutations. then\n\n(cid:6)\n(cid:6)\n(cid:6)\n\nk\n\ne[aijaij+1] =\nk\n= 1\nn\n= 1\nn\n\ne[aijaij+1\n\n|ij = k]p{ij = k}\n\nake[aij+1\n\n|ij = k]\n\ni\n\n(cid:6)\naip{ij+1 = i|ij = k}\n(cid:6)\n(cid:6)\n\n(cid:6)\n\nak\nak(\u2212ak)\n\nizk\n\nai\n\nk\n\nak\n\nk\n\n1\n\n1\n\nn(n \u2212 1)\n\nn(n \u2212 1)\n\nk\n\n=\n\n=\n\n< 0\n\n(cid:9)\ni=1 ai = 0. since\n\nn\n\nwhere the final equality followed from the assumption that\nthe preceding shows that\n\n\u23a1\n\u23a2\u23a3 n(cid:6)\n\nj=1\n\ne\n\naijaij+1\n\n\u23a4\n\u23a5\u23a6 < 0\n\nit follows that there must be some permutation i1, . . . , in for which\n\nn(cid:6)\n\naijaij+1\n\n< 0\n\nj=1\n\n "}, {"Page_number": 523, "text": "508\n\nsolutions to self-test problems and exercises\n\n7.22. (a) e[x] = \u03bb1 + \u03bb2, e[x] = \u03bb2 + \u03bb3\n\n(b)\n\ncov(x, y) = cov(x1 + x2, x2 + x3)\n\n(c) conditioning on x2 gives\n\np{x = i, y = j} =\n=\n\n7.23.\n\n\u239b\n\u239d(cid:6)\n\nxi,\n\ncorr\n\n(cid:6)\n\ni\n\nj\n\n=\n\n\u2212\u03bb2 \u03bbk\n2\n\n/k!\n\nk\n\nk\n\np{x = i, y = j|x2 = k}p{x2 = k}\np{x1 = i \u2212 k, x3 = j \u2212 k|x2 = k}e\np{x1 = i \u2212 k, x3 = j \u2212 k}e\n\u2212\u03bb2 \u03bbk\n2\np{x1 = i \u2212 k}p{x3 = j \u2212 k}e\n\n= cov(x1, x2 + x3) + cov(x2, x2 + x3)\n= cov(x2, x2)\n= var(x2)\n= \u03bb2\n(cid:6)\n(cid:6)\n(cid:6)\n(cid:6)\n= min(i,j)(cid:6)\n\u239e\n\u23a0 =\n\n\u03bbi\u2212k\n1\n(i \u2212 k)!\n(cid:9)\n(cid:9)\n\n\u03bbj\u2212k\n(j \u2212 k)!\n\n/k!\n\u2212\u03bb2 \u03bbk\n2\n\n\u2212\u03bb1\ne\n\n\u2212\u03bb3\ne\n\n\u2212\u03bb2\ne\n\n(cid:9)\n\n(cid:9)\n\n\u03bbk\n2\nk!\n\nj yj)\n\nk=0\n\n=\n\n=\n\nyj\n\nk\n\nk\n\n3\n\n/k!\n\n8\n(cid:9)\n(cid:9)\n\nj yj)\n\nvar(\n\ncov(\n\ni xi,\ni xi)var(\nj cov(xi, yj)\nn\u03c3 2\n\n(cid:9)\n8\ni cov(xi, yi) + (cid:9)\n\nx n\u03c3 2\ny\n\ni\n\ni\nn\u03c3x\u03c3y\n\n=\n= n\u03c1\u03c3x\u03c3y\nn\u03c3x\u03c3y\n= \u03c1\n\n(cid:9)\n\njzi cov(xi, yj)\n\nwhere the next to last equality used the fact that cov(xi, yi) = \u03c1\u03c3x\u03c3y\n\n7.24. let xi equal 1 if the ith card chosen is an ace, and let it equal 0 otherwise.\n\nbecause\n\nx = 3(cid:6)\n\ni=1\n\nxi\n\nand e[xi] = p{xi = 1} = 1/13, it follows that e[x] = 3/13. but, with a being\nthe event that the ace of spades is chosen, we have\n\n "}, {"Page_number": 524, "text": "solutions to self-test problems and exercises 509\n\ne[x] = e[x|a]p(a) + e[x|ac]p(ac)\n\u23a4\n\u23a6\n\n= e[x|a]\n\n49\n52\nxi|ac\n\n= e[x|a]\n\n\u23a1\n+ e[x|ac]\n\u23a3 3(cid:6)\n3(cid:6)\n\n+ 49\n52\n\ne\n\n3\n52\n3\n52\n\ni=1\ne[xi|ac]\n\n3\n52\n3\n52\n\n+ 49\n52\n+ 49\n52\n\n= e[x|a]\n\n= e[x|a]\n(cid:2)\n\ni=1\n3\n3\n51\n\n(cid:3)\n\nusing that e[x] = 3/13 gives the result\n\u2212 49\n52\n\ne[x|a] = 52\n3\n\n3\n13\n\n3\n17\n\n= 19\n17\n\n= 1.1176\n\nsimilarly, letting l be the event that at least one ace is chosen, we have\n\ne[x] = e[x|l]p(l) + e[x|lc]p(lc)\n(cid:3)\n\n(cid:2)\n= e[x|l]p(l)\n= e[x|l]\n\n1 \u2212 48 \u00b7 47 \u00b7 46\n52 \u00b7 51 \u00b7 50\n\nthus,\n\ne[x|l] =\n\nl 1.0616\n\nanother way to solve this problem is to number the four aces, with the ace of\nspades having number 1, and then let yi equal 1 if ace number i is chosen and\n0 otherwise. then\n\ne[x|a] = e\n\nyi|y1 = 1\n\n3/13\n1 \u2212 48\u00b747\u00b746\n52\u00b751\u00b750\n\u23a1\n\u23a3 4(cid:6)\n= 1 + 4(cid:6)\n\ni=1\n\ni=2\n\n= 1 + 3 \u00b7 2\n51\n\n\u23a4\n\u23a6\n\ne[yi|y1 = 1]\n= 19/17\n\nwhere we used that the fact given that the ace of spades is chosen the other\ntwo cards are equally likely to be any pair of the remaining 51 cards; so the\nconditional probability that any specified card (not equal to the ace of spades)\nis chosen is 2/51. also,\ne[x|l] = e\n\n\u23a4\n\u23a6 = 4(cid:6)\nyi|l\n\ne[yi|l] = 4p{y1 = 1|l}\n\n\u23a1\n\u23a3 4(cid:6)\n\ni=1\n\ni=1\n\nbecause\n\np{y1 = 1|l} = p(a|l) = p(al)\np(l)\n\n= p(a)\np(l)\n\n=\n\nwe obtain the same answer as before.\n\n3/52\n1 \u2212 48\u00b747\u00b746\n52\u00b751\u00b750\n\n "}, {"Page_number": 525, "text": "510\n\nsolutions to self-test problems and exercises\n\n7.25. (a) e[i|x = x] = p{z < x|x = x} = p{z < x|x = x} = p{z < x} = \u0001(x)\n\n(b) it follows from part (a) that e[i|x] = \u0001(x). therefore,\n\ne[i] = e[e[i|x]] = e[\u0001(x)]\n\nthe result now follows because e[i] = p{i = 1} = p{z < x}.\n(c) since x \u2212 z is normal with mean \u03bc and variance 2, we have\n\n/\n\n\u2212\u03bc\n2\n\n>\n\n%\n\np{x > z} = p{x \u2212 z > 0}\nx \u2212 z \u2212 \u03bc\n(cid:2)\u2212\u03bc\n(cid:3)\n= p\n(cid:2)\n(cid:3)\n= 1 \u2212 \u0001\n= \u0001\n\n2\n\n2\n\n\u03bc\n2\n\n7.26. let n be the number of heads in the first n + m \u2212 1 flips. let m = max(x, y)\nbe the number of flips needed to amass at least n heads and at least m tails.\nconditioning on n gives\n\ne[m] =\n\n(cid:6)\n= n\u22121(cid:6)\n\ni\n\ni=0\n\ne[m|n = i]p{n = i}\n\ne[m|n = i]p{n = i} + n+m\u22121(cid:6)\n\ni=n\n\ne[m|n = i]p{n = i}\n\nnow, suppose we are given that there are a total of i heads in the first n+ m\u2212 1\ntrials. if i < n, then we have already obtained at least m tails, so the additional\nnumber of flips needed is equal to the number needed for an additional n \u2212 i\nheads; similarly, if i \u00fa n, then we have already obtained at least n heads, so\nthe additional number of flips needed is equal to the number needed for an\nadditional m \u2212 (n + m \u2212 1 \u2212 i) tails. consequently, we have\n\ne[m] = n\u22121(cid:6)\n\np\n\ni=0\n\n(cid:3)\n\n(cid:2)\nn + m \u2212 1 + n \u2212 i\n+ n+m\u22121(cid:6)\n= n + m \u2212 1 + n\u22121(cid:6)\n+ n+m\u22121(cid:6)\n\np{n = i}\n(cid:3)\n(cid:2)\nn + m \u2212 1 + i + 1 \u2212 n\np{n = i}\n1 \u2212 p\n(cid:3)\nn + m \u2212 1\nn \u2212 i\n(cid:2)\n(cid:3)\np\ni\nn + m \u2212 1\npi(1 \u2212 p)n+m\u22121\u2212i\n\ni=0\ni + 1 \u2212 n\n1 \u2212 p\n\n(cid:2)\n\ni=n\n\ni\n\ni=n\n\npi(1 \u2212 p)n+m\u22121\u2212i\n\nthe expected number of flips to obtain either n heads or m tails, e[min(x, y)],\nis now given by\n\ne[min(x, y)] = e[x + y \u2212 m] = n\np\n\n+ m\n1 \u2212 p\n\n\u2212 e[m]\n\n "}, {"Page_number": 526, "text": "solutions to self-test problems and exercises 511\n7.27. this is just the expected time to collect n \u2212 1 of the n types of coupons in\n\nexample 2i. by the results of that example the solution is\n+ . . . + n\n2\n\nn \u2212 1\n\nn \u2212 2\n\n1 +\n\n+\n\nn\n\nn\n\n7.28. with q = 1 \u2212 p,\ne[x] =\n\nq(cid:6)\n\ni=1\n\np{x \u00fa i} = n(cid:6)\n\ni=1\n\np{x \u00fa i} = n(cid:6)\n\ni=1\n\nqi\u22121 = 1 \u2212 qn\n\np\n\n7.29.\n\ncov(x, y) = e[xy] \u2212 e[x]e[y] = p(x = 1, y = 1) \u2212 p(x = 1)p(y = 1)\nhence,\n\ncov(x, y) = 0 3 p(x = 1, y = 1) = p(x = 1)p(y = 1)\n\nbecause\ncov(x, y) = cov(1 \u2212 x, 1 \u2212 y) = \u2212cov(1 \u2212 x, y) = \u2212cov(x, 1 \u2212 y)\nthe preceding shows that all of the following are equivalent when x and y are\nbernoulli:\n1. cov(x, y) = 0\n2. p(x = 1, y = 1) = p(x = 1)p(y = 1)\n3. p(1 \u2212 x = 1, 1 \u2212 y = 1) = p(1 \u2212 x = 1)p(1 \u2212 y = 1)\n4. p(1 \u2212 x = 1, y = 1) = p(1 \u2212 x = 1)p(y = 1)\n5. p(x = 1, 1 \u2212 y = 1) = p(x = 1)p(1 \u2212 y = 1)\n\n7.30. number the individuals, and let xi,j equal 1 if the jth individual who has hat\nsize i chooses a hat of that size, and let xi,j equal 0 otherwise. then the number\nof individuals who choose a hat of their size is\n\nni(cid:6)\n\nx = r(cid:6)\ne[xi,j] = r(cid:6)\n\nj=1\n\ni=1\n\ni=1\n\nxi,j\n\nni(cid:6)\n\nj=1\n\nr(cid:6)\n\ni=1\n\nhini\n\nhi\nn\n\n= 1\nn\n\nhence,\n\ne[x] = r(cid:6)\n\ni=1\n\nni(cid:6)\n\nj=1\n\nchapter 8\n\n8.1. let x denote the number of sales made next week, and note that x is integral.\n\nfrom markov\u2019s inequality, we obtain the following:\n(a) p{x > 18} = p{x \u00fa 19} \u2026 e[x]\n19\n(b) p{x > 25} = p{x \u00fa 26} \u2026 e[x]\n26\n\n= 16/19\n= 16/26\n\n "}, {"Page_number": 527, "text": "512\n\nsolutions to self-test problems and exercises\n\n8.2. (a)\n\np{10 \u2026 x \u2026 22} = p{|x \u2212 16| \u2026 6}\n= p{|x \u2212 \u03bc| \u2026 6}\n= 1 \u2212 p{|x \u2212 \u03bc| > 6}\n\u00fa 1 \u2212 9/36 = 3/4\n\n(b) p{x \u00fa 19} = p{x \u2212 16 \u00fa 3} \u2026\n\n9 + 9\nin part (a), we used chebyshev\u2019s inequality; in part (b), we used its\none-sided version. (see proposition 5.1.)\n\n= 1/2\n\n9\n\n8.3. first note that e[x \u2212 y] = 0 and\n\nvar(x \u2212 y) = var(x) + var(y) \u2212 2cov(x, y) = 28\n\nusing chebyshev\u2019s inequality in part (a) and the one-sided version in parts (b)\nand (c) gives the following results:\n(a) p{|x \u2212 y| > 15} \u2026 28/225\n(b) p{x \u2212 y > 15} \u2026\n28\n28 + 225\n(c) p{y \u2212 x > 15} \u2026\n28 + 225\n\n= 28/253\n= 28/253\n\n28\n\n8.4. if x is the number produced at factory a and y the number produced at fac-\n\ntory b, then\n\ne[y \u2212 x] = \u22122, var(y \u2212 x) = 36 + 9 = 45\n\np{y \u2212 x > 0} = p{y \u2212 x \u00fa 1} = p{y \u2212 x + 2 \u00fa 3} \u2026\n\n8.5. note first that\n\n*\n\n0\n\ne[xi] =\n\n1\n\n2x2 dx = 2/3\n\nnow use the strong law of large numbers to obtain\n\n45\n\n45 + 9\n\n= 45/54\n\nn\nsn\n1\n\nr = lim\nn\u2192q\n= lim\nn\u2192q\nsn/n\n=\n1\n= 1/(2/3) = 3/2\n\nn\u2192q sn/n\nlim\n\n8.6. because e[xi] = 2/3 and\n\n*\n\n0\n\ne[x2\n\ni ] =\n\n1\n\n2x3 dx = 1/2\n\n "}, {"Page_number": 528, "text": "solutions to self-test problems and exercises 513\nwe have var(xi) = 1/2 \u2212 (2/3)2 = 1/18. thus, if there are n components on\nhand, then\n\n(the continuity correction)\n\n0\np{sn \u00fa 35} = p{sn \u00fa 34.5}\n.\n.\nsn \u2212 2n/3\n7\n0\nn/18\n.\nz \u00fa 34.5 \u2212 2n/3\n\n= p\n\nl p\n\n\u2026 34.5 \u2212 2n/3\n\nn/18\n\n7\n\nn/18\n\nwhere z is a standard normal random variable. since\n\np{z > \u22121.284} = p{z < 1.284} l .90\n\nwe see that n should be chosen so that\n\n.\n(34.5 \u2212 2n/3) l \u22121.284\na numerical computation gives the result n = 55.\n8.7. if x is the time required to service a machine, then\ne[x] = .2 + .3 = .5\n\nn/18\n\nalso, since the variance of an exponential random variable is equal to the\nsquare of its mean, we have\n\nvar(x) = (.2)2 + (.3)2 = .13\n\ntherefore, with xi being the time required to service job i, i = 1, . . . , 20, and z\nbeing a standard normal random variable, it follows that\n\np{x1 + \u00b7\u00b7\u00b7 + x20 < 8} = p\n\nx1 + \u00b7\u00b7\u00b7 + x20 \u2212 10\n\n\u221a\n\n8 \u2212 10\u221a\n\n2.6\n\n<\n\n7\n\n2.6\nl p{z < \u22121.24035}\nl .1074\n\n8.8. note first that if x is the gambler\u2019s winnings on a single bet, then\n\ne[x] = \u2212.7 \u2212 .4 + 1 = \u2212.1, e[x2] = .7 + .8 + 10 = 11.5\n\n\u2192var(x) = 11.49\n\ntherefore, with z having a standard normal distribution,\n\np{x1 + \u00b7\u00b7\u00b7 + x100 \u2026 \u2212.5} = p\n\nx1 + \u00b7\u00b7\u00b7 + x100 + 10\n\n\u221a\n1149\nl p{z \u2026 .2803}\nl .6104\n\n7\n\n\u2026\n\n\u2212.5 + 10\n\u221a\n1149\n\n0\n\n0\n\n "}, {"Page_number": 529, "text": "514\n\nsolutions to self-test problems and exercises\n\n8.9. using the notation of problem 7, we have\n\np{x1 + \u00b7\u00b7\u00b7 + x20 < t} = p\n\n0\nx1 + \u00b7\u00b7\u00b7 + x20 \u2212 10\n0\n\n7\n\n2.6\n\n7\n\nt \u2212 10\u221a\n\n2.6\n\n<\n\n\u221a\nt \u2212 10\u221a\n\n2.6\n\nl p\n\nz <\n\nnow, p{z < 1.645} l .95, so t should be such that\n\nt \u2212 10\u221a\n\n2.6\n\nl 1.645\n\nwhich yields t l 12.65.\n\n8.10. if the claim were true, then, by the central limit theorem, the average nicotine\ncontent (call it x) would approximately have a normal distribution with mean\n2.2 and standard deviation .03. thus, the probability that it would be as high\nas 3.1 is\n\np{x > 3.1} = p\n\n7\n\n3.1 \u2212 2.2\n\n\u221a\n.03\n\n0\n\n>\n\nx \u2212 2.2\u221a\nl p{z > 5.196}\nl 0\n\n.03\n\nwhere z is a standard normal random variable.\n\n8.11. (a) if we arbitrarily number the batteries and let xi denote the life of battery\ni, i = 1, . . . , 40, then the xi are independent and identically distributed\nrandom variables. to compute the mean and variance of the life of, say,\nbattery 1, we condition on its type. letting i equal 1 if battery 1 is type a\nand letting it equal 0 if it is type b, we have\n\ne[x1|i = 1] = 50 ,\n\ne[x1|i = 0] = 30\n\nyielding\n\ne[x1] = 50p{i = 1} + 30p{i = 0} = 50(1/2) + 30(1/2) = 40\n\nin addition, using the fact that e[w2] = (e[w])2 + var(w), we have\ne[x2\n1\n\n|i = 1] = (50)2 + (15)2 = 2725 ,\n\n|i = 0] = (30)2 + 62 = 936\n\ne[x2\n1\n\nyielding\n\ne[x2\n\n1 ] = (2725)(1/2) + (936)(1/2) = 1830.5\n\nthus, x1, . . . , x40 are independent and identically distributed random\nvariables having mean 40 and variance 1830.5 \u2212 1600 = 230.5. hence,\n\nwith s =(cid:9)\n\n40\ni=1 xi, we have\n\ne[s] = 40(40) = 1600 ,\n\nvar(s) = 40(230.5) = 9220\n\n "}, {"Page_number": 530, "text": "solutions to self-test problems and exercises 515\n\nand the central limit theorem yields\n\np{s > 1700} = p\n\ns \u2212 1600\n\u221a\n9220\nl p{z > 1.041}\n= 1 \u2212 \u0001(1.041) = .149\n\n>\n\n1700 \u2212 1600\n\n\u221a\n9220\n\n(b) for this part, let sa be the total life of all the type a batteries and let sb be\nthe total life of all the type b batteries. then, by the central limit theorem,\nsa has approximately a normal distribution with mean 20(50) = 1000 and\nvariance 20(225) = 4500, and sb has approximately a normal distribution\nwith mean 20(30) = 600 and variance 20(36) = 720. because the sum of\nindependent normal random variables is also a normal random variable,\nit follows that sa + sb is approximately normal with mean 1600 and\nvariance 5220. consequently, with s = sa + sb,\n\n0\n\n0\n\n7\n\n7\n\np{s > 1700} = p\n\ns \u2212 1600\n\u221a\n5220\nl p{z > 1.384}\n= 1 \u2212 \u0001(1.384) = .084\n\n>\n\n1700 \u2212 1600\n\n\u221a\n5220\n\n8.12. let n denote the number of doctors who volunteer. conditional on the event\nn = i, the number of patients seen is distributed as the sum of i indepen-\ndent poisson random variables with common mean 30. because the sum of\nindependent poisson random variables is also a poisson random variable, it\nfollows that the conditional distribution of x given that n = i is poisson with\nmean 30i. therefore,\n\nas a result,\n\ne[x|n] = 30n\n\nvar(x|n) = 30n\n\ne[x] = e[e[x|n]] = 30e[n] = 90\n\nalso, by the conditional variance formula,\n\nvar(x) = e[var(x|n)] + var(e[x|n]) = 30e[n] + (30)2var(n)\n\nbecause\n\nvar(n) = 1\n3\n\n(22 + 32 + 42) \u2212 9 = 2/3\n\nwe obtain var(x) = 690.\n\nto approximate p{x > 65}, we would not be justified in assuming that the\ndistribution of x is approximately that of a normal random variable with mean\n90 and variance 690. what we do know, however, is that\n\np{x > 65} = 4(cid:6)\n\ni=2\n\np{x > 65|n = i}p{n = i} = 1\n3\n\n4(cid:6)\n\npi(65)\n\ni=2\n\n "}, {"Page_number": 531, "text": "516\n\nsolutions to self-test problems and exercises\n\nwhere pi(65) is the probability that a poisson random variable with mean 30i\nis greater than 65. that is,\n\npi(65) = 1 \u2212 65(cid:6)\n\nj=0\n\n\u221230i(30i)j/j!\ne\n\nbecause a poisson random variable with mean 30i has the same distribution\nas does the sum of 30i independent poisson random variables with mean 1,\nit follows from the central limit theorem that its distribution is approximately\nnormal with mean and variance equal to 30i. consequently, with xi being a\npoisson random variable with mean 30i and z being a standard normal ran-\ndom variable, we can approximate pi(65) as follows:\n\npi(65) = p{x > 65}\n0\n= p{x \u00fa 65.5}\nx \u2212 30i\n= p\n\u221a\n\u221a\n0\n7\n30i\nz \u00fa 65.5 \u2212 30i\n\nl p\n\n\u221a\n\n\u00fa 65.5 \u2212 30i\n\n30i\n\n7\n\n30i\n\ntherefore,\n\np2(65) l p{z \u00fa .7100} l .2389\np3(65) l p{z \u00fa \u22122.583} l .9951\np4(65) l p{z \u00fa \u22124.975} l 1\n\nleading to the result\n\np{x > 65} l .7447\n\nif we would have mistakenly assumed that x was approximately normal, we\nwould have obtained the approximate answer .8244. (the exact probability is\n.7440.)\n\n8.13. take logarithms and then apply the strong law of large numbers to obtain\n\nlog\n\ntherefore,\n\n\u23a1\n\u23a2\u23a3\n\n\u239b\n\u239d n(cid:31)\n\ni=1\n\nxi\n\nn(cid:6)\n\ni=1\n\n\u239e\n\u23a01/n\n\u239b\n\u239d n(cid:31)\n\n\u23a4\n\u23a5\u23a6 = 1\n\u239e\n\u23a01/n\n\nn\n\nxi\n\n\u2192ee[log(xi)]\n\nlog(xi)\u2192e[log(xi)]\n\nchapter 9\n\ni=1\n\n9.1. from axiom (iii), it follows that the number of events that occur between times\n8 and 10 has the same distribution as the number of events that occur by time\n2 and thus is a poisson random variable with mean 6. hence, we obtain the\nfollowing solutions for parts (a) and (b):\n(a) p{n(10) \u2212 n(8) = 0} = e\n\u22126\n(b) e[n(10) \u2212 n(8)] = 6\n\n "}, {"Page_number": 532, "text": "(c)\n\n9.2. (a)\n\n(b)\n\nsolutions to self-test problems and exercises 517\n\nit follows from axioms (ii) and (iii) that, from any point in time onward,\nthe process of events occurring is a poisson process with rate \u03bb. hence,\nthe expected time of the fifth event after 2 p.m. is 2 + e[s5] = 2 + 5/3.\nthat is, the expected time of this event is 3:40 p.m.\n\np{n(1/3) = 2|n(1) = 2}\n\np{n(1) = 2}\n\np{n(1) = 2}\np{n(1) = 2}\n\n= p{n(1/3) = 2, n(1) = 2}\n= p{n(1/3) = 2, n(1) \u2212 n(1/3) = 0}\n= p{n(1/3) = 2}p{n(1) \u2212 n(1/3) = 0}\n= p{n(1/3) = 2}p{n(2/3) = 0}\n= e\n= 1/9\n\n\u2212\u03bb/3(\u03bb/3)2/2!e\ne\u2212\u03bb\u03bb2/2!\n\np{n(1) = 2}\n\u22122\u03bb/3\n\n(by axiom (ii))\n\n(by axiom (iii))\n\np{n(1) = 2}\n\np{n(1/2) \u00fa 1|n(1) = 2} = 1 \u2212 p{n(1/2) = 0|n(1) = 2}\n= 1 \u2212 p{n(1/2) = 0, n(1) = 2}\n= 1 \u2212 p{n(1/2) = 0, n(1) \u2212 n(1/2) = 2}\n= 1 \u2212 p{n(1/2) = 0}p{n(1) \u2212 n(1/2) = 2}\n= 1 \u2212 p{n(1/2) = 0}p{n(1/2) = 2}\n= 1 \u2212 e\n= 1 \u2212 1/4 = 3/4\n\np{n(1) = 2}\n\u2212\u03bb/2(\u03bb/2)2/2!\ne\u2212\u03bb\u03bb2/2!\n\np{n(1) = 2}\np{n(1) = 2}\n\n\u2212\u03bb/2e\n\n9.3. fix a point on the road and let xn equal 0 if the nth vehicle to pass is a car\nand let it equal 1 if it is a truck, n \u00fa 1. we now suppose that the sequence\nxn, n \u00fa 1, is a markov chain with transition probabilities\n\np0,0 = 5/6, p0,1 = 1/6, p1,0 = 4/5, p1,1 = 1/5\n\nthen the long-run proportion of times is the solution of\n\u03c00 = \u03c00(5/6) + \u03c01(4/5)\n\u03c01 = \u03c00(1/6) + \u03c01(1/5)\n\n\u03c00 + \u03c01 = 1\nsolving the preceding equations gives\n\u03c00 = 24/29\n\n\u03c01 = 5/29\n\nthus, 2400/29 l 83 percent of the vehicles on the road are cars.\n\n "}, {"Page_number": 533, "text": "518\n\nsolutions to self-test problems and exercises\n\n9.4. the successive weather classifications constitute a markov chain. if the states\nare 0 for rainy, 1 for sunny, and 2 for overcast, then the transition probability\nmatrix is as follows:\n\np = 0\n\n1/2 1/2\n1/3 1/3 1/3\n1/3 1/3 1/3\n\nthe long-run proportions satisfy\n\n\u03c00 = \u03c01(1/3) + \u03c02(1/3)\n\u03c01 = \u03c00(1/2) + \u03c01(1/3) + \u03c02(1/3)\n\u03c02 = \u03c00(1/2) + \u03c01(1/3) + \u03c02(1/3)\n1 = \u03c00 + \u03c01 + \u03c02\n\nthe solution of the preceding system of equations is\n\n\u03c00 = 1/4, \u03c01 = 3/8, \u03c02 = 3/8\n\nhence, three-eighths of the days are sunny and one-fourth are rainy.\n\n9.5. (a) a direct computation yields\n\nh(x)/h(y) l 1.06\n\n(b) both random variables take on two of their values with the same prob-\nabilities .35 and .05. the difference is that if they do not take on either\nof those values, then x, but not y, is equally likely to take on any of its\nthree remaining possible values. hence, from theoretical exercise 13, we\nwould expect the result of part (a).\n\nchapter 10\n10.1. (a)\n\n1 = c\n*\n\n*\n\n0\n\nx\n\n0\n\n1\n\nexdx * c = 1/(e \u2212 1)\n\neydy = ex \u2212 1\ne \u2212 1\n\n,\n\n0 \u2026 x \u2026 1\n\n(b)\n\nf(x) = c\nhence, if we let x = f\n\n\u22121(u), then\n\nor\n\nu = ex \u2212 1\ne \u2212 1\n\nx = log(u(e \u2212 1) + 1)\n\nthus, we can simulate the random variable x by generating a random\nnumber u and then setting x = log(u(e \u2212 1) + 1).\n10.2. use the acceptance\u2013rejection method with g(x) = 1, 0 < x < 1. calculus shows\nthat the maximum value of f (x)/g(x) occurs at a value of x, 0 < x < 1, such that\n\n2x \u2212 6x2 + 4x3 = 0\n\n "}, {"Page_number": 534, "text": "solutions to self-test problems and exercises 519\n\nor, equivalently, when\n\n4x2 \u2212 6x + 2 = (4x \u2212 2)(x \u2212 1) = 0\nthe maximum thus occurs when x = 1/2, and it follows that\n\nc = max f (x)/g(x) = 30(1/4 \u2212 2/8 + 1/16) = 15/8\n\nhence, the algorithm is as follows:\nstep 1. generate a random number u1.\nstep 2. generate a random number u2.\nstep 3. if u2 \u2026 16(u2\n1\n\n\u2212 2u3\n\n+ u4\n\n1\n\n1\n\n), set x = u1; else return to step 1.\n\n10.3. it is most efficient to check the higher probability values first, as in the follow-\n\ning algorithm:\nstep 1. generate a random number u.\nstep 2. if u \u2026 .35, set x = 3 and stop.\nstep 3. if u \u2026 .65, set x = 4 and stop.\nstep 4. if u \u2026 .85, set x = 2 and stop.\nstep 5. x = 1.\n\n10.4. 2\u03bc \u2212 x\n10.5. (a) generate 2n independent exponential random variables with mean 1, xi, yi, i =\n\n1, . . . , n, and then use the estimator\n\nexiyi /n.\n\nn(cid:9)\n\ni=1\n\n(b) we can use xy as a control variate to obtain an estimator of the type\n\nn(cid:6)\n(exiyi + cxiyi)/n\n\ni=1\n\nanother possibility would be to use xy + x2y2/2 as the control variate\nand so obtain an estimator of the type\n\nn(cid:6)\n\ni=1\n\n(exiyi + c[xiyi + x2\n\ni y2\ni\n\n/2 \u2212 1/2])/n\n\nthe motivation behind the preceding formula is based on the fact that the\nfirst three terms of the maclaurin series expansion of exy are 1 + xy +\n(x2y2)/2.\n\n "}, {"Page_number": 535, "text": "this page intentionally left blank \n\n "}, {"Page_number": 536, "text": "index\n\na\nabsolutely continuous random\n\nvariables, see continuous\nrandom variables\n\nalgorithm, polar, 453\nanalytical theory of probability\n\n(laplace), 399\n\nanswers to selected problems,\n\n456\u2013457\n\nantithetic variables, variance\n\nreduction, 450\u2013451\n\narchimedes, 208\nars conjectandi (the art of\n\nconjecturing), 142, 391\n\nassociative laws, 25\naxiom, defined, 27\naxioms of probability, 26\u201329\n\nb\nbanach match problem, 158\u2013159\nbasic principle of counting, 1\u20133\n\nproof of, 2\n\nbayes, thomas, 74\nbayes\u2019s formula, 65\u201379, 101\nbell, e. t., 208\nbernoulli, jacques, 142\u2013143\nbernoulli, james, 134, 143, 391\nbernoulli, nicholas, 391\nbernoulli random variables,\n\n134\u2013139, 403\n\nbernoulli trials, 112\nbernstein polynomials, 414\nbertrand, joseph l. f., 197\nbertrand\u2019s paradox, 197\nbest-prize problem, 344\u2013346\nbeta distribution, 218\u2013219\nbinary symmetric channel, 433\nbinomial coefficients, 7, 15\nbinomial distribution, normal\n\napproximation to, 204\u2013207\n\nbinomial random variables,\n\n134\u2013139, 259\u2013260\n\nbinomial distribution function,\n\ncomputing, 142\u2013143\n\nmoments of, 316\u2013317\nproperties of, 139\u2013141\nsimulating, 448\nvariance of, 325\u2013331\n\nbinomial theorem, 7\n\ncombinatorial proof of, 8\u20139\nproof by mathematical\n\ninduction, 8\n\nbits, 426\nbivariate normal distribution,\n\n268\u2013269\n\nboole\u2019s inequality, 300\u2013301\nborel, \u00b4e., 403\nbox\u2013muller approach, 445\nbranching process, 383\nbuffon\u2019s needle problem,\n\n243\u2013246\n\nc\ncantor distribution, 381\ncauchy distribution, 217\u2013218\ncenter of gravity, 128\ncentral limit theorem, 198, 412\nchannel capacity, 434, 436\nchapman\u2013kolmogorov equations,\n\n421\u2013423\n\nchebyshev\u2019s inequality:\n\ndefined, 389\none-sided, 403\u2013407\nand weak law of large numbers,\n\n388\u2013391\n\nchernoff bounds, 407\u2013409\nchi-squared distribution, 216, 255\nchi-squared random variable,\n\nsimulating, 446\u2013447\n\ncoding theory:\n\nbinary symmetric channel, 433\nand entropy, 428\u2013434\nnoiseless coding theorem,\n\n429\u2013431, 433\u2013434\n\n521\n\n "}, {"Page_number": 537, "text": "522\n\nindex\n\ncombinatorial analysis, 1\u201321\n\ncombinations, 5\u20139\ninteger solutions of equations,\n\nnumber of, 12\u201315\n\nmultinomial coefficients, 9\u201312\npermutations, 3\u20135\nprinciple of counting, 1\u20133\n\ncombinatorial identity, 7\ncommutative laws, 25\ncomplement, 24, 49\nconditional covariance formula,\n\n372, 381\n\nconditional distributions:\n\ncontinuous case, 266\u2013274\n\nbivariate normal distribution,\n\n268\u2013269\n\ndiscrete case, 263\u2013266\n\nconditional expectation, 331\u2013349,\n\n371\n\ncomputing expectations by\nconditioning, 333\u2013343\ncomputing probabilities by\nconditioning, 344\u2013347\n\nbest-prize problem, 344\u2013346\nconditional variance, 347\u2013349\ndefinitions, 331\u2013333\nand prediction, 349\u2013353\n\nconditional independence, 98\nconditional probability, 58\u201365\n\nbayes\u2019s formula, 65\u201379\nindependent events, 79\u201393\nconditional probability density\n\nfunction, 286\n\nconditional probability mass\n\nfunction, 286\n\nconditional variance, 347\u2013349\n\nvariance of a sum of a random\n\nnumber of random\nvariables, 349\n\nconditional variance formula, 372\nconditioning:\n\ncomputing expectations by,\n\n333\u2013343\n\ncomputing probabilities by,\n\n344\u2013347\n\nbest-prize problem, 344\u2013346\nvariance reduction by, 451\u2013452\n\ncontinuity correction, 205\ncontinuous random variables,\n\n186\u2013231\n\nbeta distribution, 218\u2013219\ncauchy distribution, 217\u2013218\nexpectation of, 190\u2013194\ngamma distribution, 215\u2013216\nsimulation of:\n\ngeneral techniques for,\n\n440\u2013447\n\ninverse transformation\nmethod, 441\u2013442, 453\n\nrejection method, 442\u2013444,\n\n453\u2013454\n\nweibull distribution, 216\u2013217\n\ncontrol variates, variance reduction,\n\n452\u2013453\nconvolution, 252\ncorrelation, 371\ncorrelation coefficient, 322\u2013331\ncounting, basic principle of, 1\u20133\n\nproof of, 2\n\ncoupon-collecting problem,\n\n318\u2013319\n\nsingletons in, 321\u2013322\n\ncoupon collecting with unequal\n\nprobabilities, 314\u2013315\n\ncovariance, 322\u2013323, 371\n\ndefined, 322\n\ncumulative distribution function\n\n(distribution function),\n123\u2013125\n\nproperties, 168\u2013170\n\nd\ndemoivre, abraham, 198, 207\u2013208,\n\n393\n\ndemoivre\u2013laplace limit theorem,\n\n204\n\ndemorgan\u2019s laws, 26\ndependent random variables, 241\ndeviations, 324\ndiscrete distributions:\n\nsimulation from, 447\u2013449\n\nbinomial random variable,\n\n448\n\n "}, {"Page_number": 538, "text": "geometric distribution,\n\nevents, 23\u201326\n\nindex 523\n\n447\u2013448\n\npoisson random variable, 449\n\ndiscrete probability distribution,\n\n358\n\ndiscrete random variables, 123\u2013125,\n\n171\n\ndistribution function (distribution\n\nfunction), 123, 170\n\ndistributions:\n\nbeta, 218-219\nbinomial, normal\n\napproximation to, 204-207\n\nbivariate normal, 268-269\ncantor, 381\ncauchy, 217-218\nchi-squared, 216, 255\nconditional, 263-274\ncontinuous probability, 359\ndiscreet, 447-449\ndiscrete probability, 358\ngamma, 215-216\ngaussian, 207\ngeometric, 447-448\nlaplace, 211\nmarginal, 233\nmulltinomial, 240\nmultivariate, 372\nmultivariate normal, 365-367\nn-erlang, 216\nnegative hypergeometric, 319\nnormal, 356-357\nweibull, 216-217\nzeta (zipf), 163\u2013164\n\ndistributive laws, 25\ndouble exponential random\n\nvariable, 211fn\n\nduration of play, problem of, 89\u201390\n\ne\nedges, 91\nehrenfest, paul and tatyana, 421\nentropy, 426\u2013428, 435\n\nand coding theory, 428\u2013434\n\nequations, number of integer\n\nsolutions of, 12\u201315\n\nergodic markov chain, 423\u2013424\n\nindependent, 79\u201393, 101\nmutually exclusive, 24, 49\nodds of, 101\npairwise independent, 147\n\nexchangeable random variables,\n\n282\u2013285\n\nexpectation, see continuous\n\nrandom variables\n\nconditional, 331\u2013349, 371\nand prediction, 349\u2013353\n\ncorrelations, 322\u2013331\ncovariance, 322\u2013323\ngeneral definition of, 369\u2013370\nmoment generating functions,\n\n354\u2013365\n\nbinomial distribution with\nparameters n and p, 355\n\ncontinuous probability\n\ndistribution, 359\n\ndetermination of the\n\ndistribution, 358\ndiscrete probability\ndistribution, 358\n\nexponential distribution with\n\nparameter \u03bb, 356\n\nindependent binomial random\n\nvariables, sums of, 360\n\nindependent normal random\n\nvariables, sums of, 360\n\nindependent poisson random\n\nvariables, sums of, 360\n\njoint, 363\u2013365\nnormal distribution, 356\u2013357\npoisson distribution with\n\nmean \u03bb, 355\u2013356\n\nof the sum of a random\n\nnumber of random\nvariables, 361\u2013363\n\nmoments of the number of\n\nevents that occur, 315\u2013322,\n319\n\nbinomial random variables,\n\nmoments of, 316\u2013317\n\ncoupon-collecting problem,\n\n318\u2013319, 321\u2013322\n\n "}, {"Page_number": 539, "text": "524\n\nindex\n\nexpectation, see continuous\n\nrandom variables\n(continued)\n\nhypergeometric random\nvariables, moments of,\n317\u2013318\n\nmoments in the match\n\nproblem, 318\n\nnegative hypergeometric\n\nrandom variables, 319\u2013321\nprobabilistic method, obtaining\nbounds from expectations\nvia, 311\u2013312\n\nproperties of, 297\u2013387\nof sums of random variables,\n\n298\u2013315\n\nboole\u2019s inequality, 300\u2013301\ncoupon-collecting problems,\n\n303\n\ncoupon collecting with\nunequal probabilities,\n314\u2013315\n\nexpectation of a binomial\n\nrandom variable, 301\n\nexpected number of matches,\n\n303\n\nexpected number of runs,\n\n304\u2013305\n\nhypergeometric random\nvariable, mean of, 302\n\nmaximum\u2013minimums\n\nidentity, 313\u2013314\n\nnegative binomial random\n\nvariable, mean of, 301\u2013302\n\nprobability of a union of\n\nevents, 308\u2013310\n\nquick-sort algorithm,\nanalyzing, 306\u2013308\n\nrandom walk in a plane,\n\n305\u2013306\n\nsample mean, 300\n\nvariance of sums, 322\u2013331\nexpected value (expectation),\n\n125\u2013128, 171\n\nexponential random variables,\n\n208\u2013214, 223\n\nf\nfailure rate function, 212, 223\nfermat, pierre de, 85\u201386, 89\nfermat\u2019s combinatorial identity, 18\nfinite population, sampling from,\n\n326\u2013331\n\nfirst of moment of x, 132\nfunctional system, 1\n\ng\ngalton, francis, 399\ngambler\u2019s ruin problem, 87\u201388\ngamma distribution, 215\u2013216\ngamma function, 215, 223\ngamma random variables,\n\n254\u2013255\n\ngauss, karl friedrich, 207\u2013208\ngaussian curve, 207\ngaussian distribution, 207\ngeneralized basic principle of\n\ncounting, 2\n\ngeometric distribution, 447\u2013448\n\nvariance of, 340\n\ngeometric random variable with\n\nparameter p, 448\n\ngeometric random variables,\n\n155\u2013157, 260\u2013263\n\ngeometrical probability, 197\ngoodwill cost, defined, 176\n\nh\nhalf-life, probabilistic interpretation\n\nof (example), 249\u2013251\n\nhamiltonian path:\n\ndefined, 311\nmaximum number of, in a\ntournament, 311\u2013312\n\nhazard rate functions, 212\u2013214, 223\nhuygens, christiaan, 86, 89\nhypergeometric random variables,\n\n160\u2013163\n\nmoments of, 317\u2013318\n\ni\nidentically distributed uniform\n\nrandom variables, 252\u2013254\n\nhazard rate functions, 212\u2013214\n\nimportance sampling, 455\n\n "}, {"Page_number": 540, "text": "independent bernoulli random\n\nvariables, bounding error\nprobability when\napproximating a sum of,\n410\u2013412\n\nindependent binomial random\n\nvariables, sums of, 260, 360\n\nindependent events, 79\u201393, 101\nindependent increment assumption,\n\n417\n\nindependent normal random\n\nvariables, sums of, 360\nindependent poisson random\n\nvariables, sums of, 259\u2013260,\n360\n\nindependent random variables,\n\n240\u2013251, 286\n\nbinomial random variables,\n\n259\u2013260\n\nbuffon\u2019s needle problem,\n\n243\u2013246\n\nconditional distributions:\n\ncontinuous case, 266\u2013274\ndiscrete case, 263\u2013266\n\ngamma random variables,\n\n254\u2013255\n\ngeometric random variables,\n\n260\u2013263\n\nhalf-life, probabilistic\n\ninterpretation of (example),\n249\u2013251\n\nidentically distributed uniform\nrandom variables, 252\u2013254\n\nnormal random variables,\n\n256\u2013259\n\npoisson random variables,\n\n259\u2013260\n\nrandom subsets, 246\u2013249\nsums of, 252\u2013263\n\nindependent uniform random\nvariables, sum of two,\n252\u2013253\n\ninequality:\n\nboole\u2019s, 300\u2013301\nchebyshev\u2019s, 388\u2013391\njensen\u2019s, 409\nmarkov\u2019s, 388\n\nindex 525\n\ninitial probabilities, 99\ninteger solutions of equations,\n\nnumber of, 12\u201315\n\ninterarrival times, sequence of,\n\n418\n\nintersection, 23\u201324, 49\ninverse transformation method,\n\n441\u2013442, 453\n\nexponential random variable,\n\nsimulating, 441\n\ngamma random variable,\nsimulating, 441\u2013442\n\nj\njacobian determinant, 280\njensen\u2019s inequality, 409\njoint cumulative probability\ndistribution function,\n232\u2013240, 282\u2013285\n\njoint density function of order\n\nstatistics, 270\n\njoint distribution functions,\n\n232\u2013240\n\njoint cumulative probability\n\ndistribution function,\n232\u2013240, 282\u2013285\n\njoint probability mass function,\n\n233\u2013234\n\nmarginal distributions, 233\nmultinomial distribution, 240\n\njoint moment generating functions,\n\n363\u2013365\n\njoint probability density function,\n\n235\u2013239, 285\n\njoint probability mass function,\n\n233\u2013234\n\njointly continuous random\n\nvariables, 235, 239, 285\n\njointly distributed random\nvariables, 232\u2013297\n\njoint distribution functions,\n\n232\u2013240\n\njoint probability density\n\nfunction, 235\u2013239, 285\nmarginal probability mass\n\nfunctions, 234\n\n "}, {"Page_number": 541, "text": "526\n\nindex\n\nk\nkelley strategy, 378\nkhinchine, a. y., 391\nkolmogorov, a. n., 403\nl\nlaplace distribution, 211\nlaplace, pierre-simon, 399\nlaplace\u2019s rule of succession, 98\u201399\nlaw of frequency of errors, 399\nlaws of large numbers, 388\nlegendre theorem, 229\nlength of the longest run\n\n(example), 148\u2013154\n\nl\u2019h \u02c6opital\u2019s rule, 392\nliapounoff, a., 393\nlimit theorems, 388\u2013417\n\ncentral limit theorem, 391\u2013399,\n\n412\n\nchebyshev\u2019s inequality, 388\u2013391\nstrong law of large numbers,\n\n400\u2013403, 412\n\nweak law of large numbers,\n\n388\u2013391\n\nlognormal random variable, 258\nm\nmarginal distributions, 233\nmarginal probability mass functions,\n\n234\n\nmarkov chain, 419\u2013424, 434\nchapman\u2013kolmogorov\nequations, 421\u2013422\n\nergodic, 423\u2013424\nmatrix, 420\nrandom walk, 422\ntransition probabilities, 420\n\nmarkov\u2019s inequality, 388\nmatching problem (example),\n\n41\u201342, 63\n\nmatrix, 420\nmaximum likelihood estimate, 160\nmaximum\u2013minimums identity,\n\n313\u2013314\n\nmean, 132, 171\nmeasureable events, 29\nmeasurement signalto-noise ratio,\n\n414\n\nmemoryless, use of term, 223\nmen of mathematics (bell), 208\nmethod of maximum likelihood\n\nestimation, 180\n\nmidrange of a sequence, 293\nminimax theorem, 175\nmoment generating functions,\n\n354\u2013365, 372\n\nbinomial distribution with\n\nparameters n and p, 355\n\ncontinuous probability\n\ndistribution, 359\ndetermination of the\ndistribution, 358\n\ndiscrete probability distribution,\n\n358\n\nexponential distribution with\n\nparameter \u03bb, 356\n\nindependent binomial random\n\nvariables, sums of, 360\n\nindependent normal random\n\nvariables, sums of, 360\n\nindependent poisson random\n\nvariables, sums of, 360\n\njoint, 363\u2013365\nnormal distribution, 356\u2013357\npoisson distribution with mean\n\n\u03bb, 355\u2013356\n\nof the sum of a random number\n\nof random variables,\n361\u2013363\n\nmultinomial coefficients, 9\u201312\n\ndefined, 11\n\nmultinomial distribution, 240\nmultinomial theorem, 11\nmultiplication rule, 62\u201363, 101\nmultivariate distributions, 372\nmutually exclusive events, 24, 49\n\nn\nn-erlang distribution, 216\nnatural inheritance (galton), 399\nnegative binomial random\nvariables, 157\u2013160\nnegative hypergeometric\ndistribution, 319\n\n "}, {"Page_number": 542, "text": "negative hypergeometric random\n\nsequence of interarrival times,\n\nindex 527\n\nvariables, 319\u2013321\n\nnewton, isaac, 208\nnoiseless coding theorem, 429\u2013431,\n\n433\u2013434\n\nnormal curve, 207\nnormal random variables,\n\n256\u2013259\n\njoint distribution of the sample\nmean and sample variance,\n367\u2013369\n\nmultivariate normal\n\ndistribution, 365\u2013367\n\nsimulating, 443\u2013444\n\npolar method, 445\u2013446\n\nnotation/terminology, 6, 10\nnth moment of x, 132\nnull event, 24\nnull set, 49\n\no\nodds, of an event, 101\none-sided chebyshev\u2019s inequality,\n\n403\u2013407\n\norder statistics, 270\u2013274, 286\n\ndistribution of the range of a\nrandom sample, 273\u2013274\njoint density function of, 270\n\noverlook probabilities, 74\n\np\npairwise independent events, 147\npareto, v., 164\npascal, blaise, 85\u201386\npearson, karl, 207\u2013208\npermutations, 3\u20135\npersonal view of probability, 48\npierre-simon, marquis de laplace,\n\n399\n\npoints, problem of, 86\npoisson distribution function,\ncomputing, 154\u2013155\n\npoisson paradigm, 148\npoisson process, 417\u2013419\n\ndefined, 417, 434\nindependent increment\n\nassumption, 417\n\n418\n\nstationary increment\nassumption, 417\n\nwaiting time, 419\n\npoisson random variables, 143\u2013145,\n\n171, 259\u2013260\n\nsimulating, 449\n\npoisson, sim\u00b4eon denis, 143\npolar algorithm, 453\npolar method, 445\u2013446\npolya\u2019s urn model, 284\nposterior probability, 99\u2013100\nprinciple of counting, 1\u20133\nprior probabilities, 99\nprobabilistic method, 93\n\nobtaining bounds from\n\nexpectations via, 311\u2013312\n\nmaximum number of\n\nhamiltonian paths in a\ntournament, 311\u2013312\n\nprobabilistic method, the\n\n(alon/spencer/erdos), 93fn\n\nprobability:\n\naxioms of, 26\u201329\nas a continuous set function,\n\n44\u201348\n\ndefining, 26\nof an event, 27\ngeometrical, 197\nas a measure of belief, 48\u201349\nmultiplication rule, 62\u201363, 101\npersonal view of, 48\nsample space and events, 22\u201326\nsimple propositions, 29\u201333\nsubjective view of, 48\n\nprobability density function, 222\n\ndefined, 186\n\nprobability mass function, 123, 171,\n\n233\n\nprobability, personal view of, 48\nproblem of duration of play, 89\u201390\npseudorandom numbers, 439\n\nq\nquick-sort algorithm, analyzing,\n\n306\u2013308\n\n "}, {"Page_number": 543, "text": "528\n\nindex\n\nr\nrandom-number generator, 439\n\nseed, 439\n\nrandom numbers, 385, 439, 453\nrandom permutation, generating,\n\n439\u2013440\n\nrandom samples, distribution of the\n\nrange of, 273\u2013274\n\nrandom variables, 117\u2013186, 134\u2013139\n\nbernoulli, 134\u2013139\nbinomial, 134\u2013139, 259\u2013260\ncontinuous, 186\u2013231\ncumulative distribution\n\nfunction, 123\u2013125\n\nproperties of, 168\u2013170\n\ndefined, 117, 170\ndependent, 241\ndiscrete, 123\u2013125, 171\ndistribution of a function of,\n\n219\u2013221\n\nexchangeable, 282\u2013285\nexpectation of a function of,\n\n128\u2013132\n\nexpectation of a sum of a\n\nrandom number of, 335\n\nexpectation of sums of, 298\u2013315\nexpected value (expectation),\n\n125\u2013128\n\nsums of, 164\u2013168\n\nexponential, 208\ngamma, 254\u2013255\ngeometric, 155\u2013157, 260\u2013263\nhypergeometric, 160\u2013163\nidentically distributed uniform,\n\n252\u2013254\n\nindependent, 240\u2013251\njoint probability distribution of\n\nfunctions of, 274\u2013282\n\njointly continuous, 239\nmoment generating functions,\n\n354\u2013365\n\nof the sum of a random\n\nnumber of, 361\u2013363\n\nnegative binomial, 157\u2013160\nnormal, 198\u2013204, 256\u2013259\norder statistics, 270\u2013274, 286\npoisson, 143\u2013145, 171, 259\u2013260\n\nuniform, 194\u2013198\nvariance, 171\nvariance of a sum of a random\n\nnumber of, 349\n\nweibull, 217\nzeta (zipf) distribution, 163\u2013164\n\nrandom walk, 422\nrate of transmission of information,\n\n436\n\nrayleigh density function, 229\nrecherches sur la probabilil\u00b4e des\n\njugements en mati`ere\ncriminelle et en mati`ere\ncivile (investigations into the\nprobability of verdicts in\ncriminal and civil matters),\n143\n\nrejection method, 442\u2013444, 453\u2013454\n\nsimulating a chi-squared\n\nrandom variable, 446\u2013447\n\nsimulating a normal random\n\nvariable, 443\u2013444\n\npolar method, 445\u2013446\n\nriemann, g. f. b., 164\n\ns\nsample mean, 300, 372\n\njoint distribution of, 367\u2013369\njoint distribution of sample\nvariance and, 367\u2013369\n\nsample median, 272\nsample spaces:\n\nand events, 22\u201326, 49\nhaving equally likely outcomes,\n\n33\u201344\n\nsample variance, 324, 372\n\njoint distribution of sample\n\nmean and, 367\u2013369\n\nseed, 439\nselected problems, answers to,\n\n456\u2013457\n\nself-text problems/exercises,\n\n458\u2013516\n\nsequence of interarrival times, 418\nsequential updating of information,\n\n99\u2013101\n\nshannon, claude, 433\n\n "}, {"Page_number": 544, "text": "simulation, 438\u2013456\n\nof continuous random variables:\n\ngeneral techniques for,\n\n440\u2013447\n\ninverse transformation\nmethod, 441\u2013442, 453\n\nrejection method, 442\u2013444,\n\n453\u2013454\ndefined, 438\nfrom discrete distributions,\n\n447\u2013449\n\npseudorandom numbers, 439\nrandom-number generator, 439\nrandom numbers, 439, 453\nrandom permutation,\n\ngenerating (example),\n439\u2013440\n\nseed, 439\nvariance reduction techniques,\n\n449\u2013453\n\nsimulation, defined, 246\nsingletons, in coupon-collecting\n\nproblem, 321\u2013322\n\nsize of the zeroth generation, 383\nst. petersburg paradox, 175\nstandard deviation of x, 134, 171\nstandard deviations, 414\nstationary increment assumption,\n\n417\n\nstieltjes integrals, 369\u2013370\nstrong law of large numbers,\n\n400\u2013403, 412\n\nsubjective view of probability, 48\nsums of random variables:\nexpectation of, 298\u2013315\n\nbinomial random variable,\n\n300\u2013301\n\nboole\u2019s inequality, 300\u2013301\ncoupon-collecting problems,\n\n303\n\ncoupon collecting with\nunequal probabilities,\n314\u2013315\n\nexpectation of a binomial\n\nrandom variable, 301\n\nexpected number of matches,\n\n303\n\nindex 529\n\nexpected number of runs,\n\n304\u2013305\n\nhypergeometric random\nvariable, mean of, 302\n\nmaximum\u2013minimums\n\nidentity, 313\u2013314\n\nnegative binomial random\n\nvariable, mean of, 301\u2013302\n\nprobability of a union of\n\nevents, 308\u2013310\n\nquick-sort algorithm,\nanalyzing, 306\u2013308\n\nrandom walk in a plane,\n\n305\u2013306\n\nsample mean, 300\n\nsuperset, 24\nsurprise concept, 425\u2013426\n\nt\ntheory of games, 175\ntransition probabilities, markov\n\nchains, 420\n\ntrials, 82\n\nu\nuncertainty, 426\u2013427\nuniform random variables, 194\u2013198\nunion, 23\u201324, 49\nupdated probability, 99\u2013100\nupdating information sequentially,\n\n99\u2013101\nutility, 130\u2013131\n\nv\nvariables, 163\u2013164, see also\nrandom variables\n\nantithetic, 450\u2013451\n\nvariance, 171\n\nconditional, 347\u2013349\ncovariance, 322\u2013323\nof geometric distribution, 340\nsample, 324, 372\nvariance reduction:\n\nantithetic variables, use of,\n\n450\u2013451\n\nby conditioning, 451\u2013452\ncontrol variates, 452\u2013453\ntechniques, 449\u2013453\n\n "}, {"Page_number": 545, "text": "530\n\nindex\n\nvenn diagram, 24\u201325\nvertices, 91\n\nw\nwaiting time, 419\nweak law of large numbers, 388\u2013391\nweibull distribution, 216\u2013217\nweibull random variables, 217\n\nwheel of fortune game\n\n(chuck-a-luck) (example),\n136\n\nwilcoxon sum-of ranks test, 376\nz\nzeroth generation, size of, 383\nzeta (zipf) distribution, 163\u2013164\nzipf, g. k., 164\n\n "}]}
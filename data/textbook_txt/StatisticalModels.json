{"Filename": "StatisticalModels", "Pages": [{"Page_number": 1, "text": " "}, {"Page_number": 2, "text": "this page intentionally left blank\n\n "}, {"Page_number": 3, "text": "statistical models\n\n "}, {"Page_number": 4, "text": "cambridge series in statistical and probabilistic\n\nmathematics\n\neditorial board:\n\nr. gill, department of mathematics, utrecht university\nb.d. ripley, department of statistics, university of oxford\ns. ross, department of industrial engineering, university of california, berkeley\nm. stein, department of statistics, university of chicago\nd. williams, school of mathematical sciences, university of bath\n\nthis series of high-quality upper-division textbooks and expository monographs cov-\ners all aspects of stochastic applicable mathematics. the topics range from pure and\napplied statistics to probability theory, operations research, optimization, and mathe-\nmatical programming. the books contain clear presentations of new developments\nin the field and also of the state of the art in classical methods. while emphasizing\nrigorous treatment of theoretical methods, the books also contain applications and\ndiscussions of new techniques made possible by advances in computational practice.\n\nalready published\n1. bootstrap methods and their application, a.c. davison and d.v. hinkley\n2. markov chains, j.norris\n3. asymptotic statistics, a.w. van der vaart\n4. wavelet methods for time series analysis, d.b. percival and a.t. walden\n5. bayesian methods, t.leonard and j.s.j. hsu\n6. empirical processes in m-estimation, s. van de geer\n7. numerical methods of statistics, j.monahan\n8. a user\u2019s guide to measure-theoretic probability, d.pollard\n9. the estimation and tracking of frequency, b.g. quinn and e.j. hannan\n\n "}, {"Page_number": 5, "text": "statistical models\n\na. c. davison\n\nswiss federal institute of technology,\n\nlausanne\n\n "}, {"Page_number": 6, "text": "cambridge university press\ncambridge, new york, melbourne, madrid, cape town, singapore,\ns\u00e3o paulo, delhi, dubai, tokyo\n\ncambridge university press\nthe edinburgh building, cambridge cb2 8ru, uk\n\npublished in the united states of america by cambridge university press, new york\n\nwww.cambridge.org\ninformation on this title: www.cambridge.org/9780521773393\n\n\u00a9 cambridge university press 2003, 2008\n\nthis publication is in copyright. subject to statutory exception and to the \nprovision of relevant collective licensing agreements, no reproduction of any part\nmay take place without the written permission of cambridge university press.\n\nfirst published in print format\n\n2003\n\nisbn-13 978-0-511-66887-6\n\nebook (adobe reader)\n\nisbn-13    978-0-521-77339-3\n\nhardback\n\ncambridge university press has no responsibility for the persistence or accuracy \nof urls for external or third-party internet websites referred to in this publication, \nand does not guarantee that any content on such websites is, or will remain, \naccurate or appropriate.\n\n "}, {"Page_number": 7, "text": "contents\n\npreface\n\nintroduction\n\nvariation\n\n1\n\n2\n\nstatistics and sampling variation\n\n2.1\n2.2 convergence\n2.3 order statistics\n2.4 moments and cumulants\n2.5 bibliographic notes\n2.6\n\nproblems\n\n3\n\nuncertainty\n\n3.1 confidence intervals\n3.2 normal model\n3.3\n3.4 bibliographic notes\n3.5\n\nsimulation\n\nproblems\n\n4\n\nlikelihood\n\n4.1 likelihood\nsummaries\n4.2\n4.3\ninformation\n4.4 maximum likelihood estimator\n4.5 likelihood ratio statistic\n4.6 non-regular models\n\nix\n\n1\n\n15\n\n15\n28\n37\n44\n48\n49\n\n52\n\n52\n62\n77\n90\n90\n\n94\n\n94\n101\n109\n115\n126\n140\n\nv\n\n "}, {"Page_number": 8, "text": "vi\n\ncontents\n\n4.7 model selection\n4.8 bibliographic notes\n4.9\n\nproblems\n\n5 models\n\nstraight-line regression\n\n5.1\n5.2 exponential family models\n5.3 group transformation models\n5.4\nsurvival data\n5.5 missing data\n5.6 bibliographic notes\n5.7\n\nproblems\n\n6\n\nstochastic models\n\n6.1 markov chains\n6.2 markov random fields\n6.3 multivariate normal data\n6.4 time series\n6.5\n6.6 bibliographic notes\n6.7\n\npoint processes\n\nproblems\n\n7\n\nestimation and hypothesis testing\n\n7.1 estimation\n7.2 estimating functions\n7.3 hypothesis tests\n7.4 bibliographic notes\n7.5\n\nproblems\n\n8\n\nlinear regression models\n\nintroduction\n\n8.1\n8.2 normal linear model\n8.3 normal distribution theory\n8.4 least squares and robustness\n8.5 analysis of variance\n8.6 model checking\n8.7 model building\n8.8 bibliographic notes\n8.9\n\nproblems\n\n150\n156\n156\n\n161\n\n161\n166\n183\n188\n203\n218\n219\n\n225\n\n225\n244\n255\n266\n274\n292\n293\n\n300\n\n300\n315\n325\n348\n349\n\n353\n\n353\n359\n370\n374\n378\n386\n397\n409\n409\n\n "}, {"Page_number": 9, "text": "contents\n\n9\n\ndesigned experiments\n\nsome standard designs\nfurther notions\n\n9.1 randomization\n9.2\n9.3\n9.4 components of variance\n9.5 bibliographic notes\n9.6\n\nproblems\n\n10 nonlinear regression models\n\n10.1 introduction\n10.2 inference and estimation\n10.3 generalized linear models\n10.4 proportion data\n10.5 count data\n10.6 overdispersion\n10.7 semiparametric regression\n10.8 survival data\n10.9 bibliographic notes\n10.10 problems\n\n11 bayesian models\n\n11.1 introduction\n11.2 inference\n11.3 bayesian computation\n11.4 bayesian hierarchical models\n11.5 empirical bayes inference\n11.6 bibliographic notes\n11.7 problems\n\n12 conditional and marginal inference\n\n12.1 ancillary statistics\n12.2 marginal likelihood\n12.3 conditional inference\n12.4 modified profile likelihood\n12.5 bibliographic notes\n12.6 problems\n\nvii\n\n417\n\n417\n426\n439\n449\n463\n464\n\n468\n\n468\n471\n480\n487\n498\n511\n518\n540\n554\n555\n\n565\n\n565\n578\n596\n619\n627\n637\n639\n\n645\n\n646\n656\n665\n680\n691\n692\n\n "}, {"Page_number": 10, "text": "viii\n\nappendix a. practicals\n\nbibliography\nname index\nexample index\nindex\n\ncontents\n\n696\n\n699\n712\n716\n718\n\n "}, {"Page_number": 11, "text": "preface\n\na statistical model is a probability distribution constructed to enable inferences to\nbe drawn or decisions made from data. this idea is the basis of most tools in the\nstatistical workshop, in which it plays a central role by providing economical and\ninsightful summaries of the information available.\n\nthis book is intended as an integrated modern account of statistical models covering\nthe core topics for studies up to a masters degree in statistics. it can be used for a\nvariety of courses at this level and for reference. after outlining basic notions, it\ncontains a treatment of likelihood that includes non-regular cases and model selection,\nfollowed by sections on topics such as markov processes, markov random fields,\npoint processes, censored and missing data, and estimating functions, as well as more\nstandard material. simulation is introduced early to give a feel for randomness, and\nlater used for inference. there are major chapters on linear and nonlinear regression\nand on bayesian ideas, the latter sketching modern computational techniques. each\nchapter has a wide range of examples intended to show the interplay of subject-matter,\nmathematical, and computational considerations that makes statistical work so varied,\nso challenging, and so fascinating.\n\nthe target audience is senior undergraduate and graduate students, but the book\nshould also be useful for others wanting an overview of modern statistics. the reader\nis assumed to have a good grasp of calculus and linear algebra, and to have followed\na course in probability including joint and conditional densities, moment-generating\nfunctions, elementary notions of convergence and the central limit theorem, for ex-\nample using grimmett and welsh (1986) or stirzaker (1994). measure is not required.\nsome sections involve a basic knowledge of stochastic processes, but they are intended\nto be as self-contained as possible. to have included full proofs of every statement\nwould have made the book even longer and very tedious. instead i have tried to\ngive arguments for simple cases, and to indicate how results generalize. readers in\nsearch of mathematical rigour should see knight (2000), schervish (1995), shao\n(1999), or van der vaart (1998), amongst the many excellent books on mathematical\nstatistics.\n\nsolution of problems is an integral part of learning a mathematical subject. most\nsections of the book finish with exercises that test or deepen knowledge of that section,\nand each chapter ends with problems which are generally broader or more demanding.\nreal understanding of statistical methods comes from contact with data.\nappendix a outlines practicals intended to give the reader this experience. the\npracticals themselves can be downloaded from\n\nhttp://statwww.epfl.ch/people/~davison/sm\n\nix\n\n "}, {"Page_number": 12, "text": "x\n\npreface\n\ntogether with a library of functions and data to go with the book, and errata. the\npracticals are written in two dialects of the s language, for the freely available package\nr and for the commercial package s-plus, but it should not be hard for teachers to\ntranslate them for use with other packages.\n\nbiographical sketches of some of the people mentioned in the text are given as\n\nsidenotes; the sources for many of these are heyde and seneta (2001) and\n\nhttp://www-groups.dcs.st-and.ac.uk/~history/\n\npart of the work was performed while i was supported by an advanced research\nfellowship from the uk engineering and physical science research council. i am\ngrateful to them and to my past and present employers for sabbatical leaves during\nwhich the book advanced. many people have helped in various ways, for example\nby supplying data, examples, or figures, by commenting on the text, or by test-\ning the problems. i thank marc-olivier boldi, alessandra brazzale, angelo canty,\ngorana capkun, james carpenter, val\u00b4erie chavez, stuart coles, john copas, tom\ndiciccio, debbie dupuis, david firth, christophe girardet, david hinkley, wilfred\nkendall, diego kuonen, stephan morgenthaler, christophe osinski, brian ripley,\ngareth roberts, sylvain sardy, jamie stafford, trevor sweeting, val\u00b4erie ventura,\nsimon wood, and various anonymous reviewers. particular thanks go to jean-yves\nle boudec, nancy reid, and alastair young, who gave valuable comments on much of\nthe book. david tranah of cambridge university press displayed exemplary patience\nduring the interminable wait for me to finish. despite all their efforts, errors and\nobscurities doubtless remain. i take responsibility for this and would appreciate being\ntold of them, in order to correct any future versions.\n\nmy long-suffering family deserve the most thanks. i dedicate this book to them,\nand particularly to claire, without whose love and support the project would never\nhave been finished.\n\nlausanne, january 2003\n\n "}, {"Page_number": 13, "text": "1\n\nintroduction\n\nstatistics concerns what can be learned from data. applied statistics comprises a\nbody of methods for data collection and analysis across the whole range of science,\nand in areas such as engineering, medicine, business, and law \u2014 wherever variable\ndata must be summarized, or used to test or confirm theories, or to inform decisions.\ntheoretical statistics underpins this by providing a framework for understanding the\nproperties and scope of methods used in applications.\n\nstatistical ideas may be expressed most precisely and economically in mathemat-\nical terms, but contact with data and with scientific reasoning has given statistics a\ndistinctive outlook. whereas mathematics is often judged by its elegance and gener-\nality, many statistical developments arise as a result of concrete questions posed by\ninvestigators and data that they hope will provide answers, and elegant and general\nsolutions are not always available. the huge variety of such problems makes it hard\nto develop a single over-arching theory, but nevertheless common strands appear.\nuniting them is the idea of a statistical model.\n\nthe key feature of a statistical model is that variability is represented using probabil-\nity distributions, which form the building-blocks from which the model is constructed.\ntypically it must accommodate both random and systematic variation. the random-\nness inherent in the probability distribution accounts for apparently haphazard scatter\nin the data, and systematic pattern is supposed to be generated by structure in the\nmodel. the art of modelling lies in finding a balance that enables the questions at\nhand to be answered or new ones posed. the complexity of the model will depend on\nthe problem at hand and the answer required, so different models and analyses may\nbe appropriate for a single set of data.\n\nexamples\n\nexample 1.1 (maize data) charles darwin collected data over a period of years\non the heights of zea mays plants. the plants were descended from the same parents\nand planted at the same time. half of the plants were self-fertilized, and half were\ncross-fertilized, and the purpose of the experiment was to compare their heights. to\n\n1\n\ncharles robert darwin\n(1809\u20131882) was rich\nenough not to have to earn\nhis living. his reading and\nstudies at edinburgh and\ncambridge exposed him\nto contemporary scientific\nideas, and prepared him\nfor the voyage of the\nbeagle (1831\u20131836),\nwhich formed the basis of\nhis life\u2019s work as a\nnaturalist \u2014 at one point\nhe spent 8 years dissecting\nand classifying barnacles.\nhe wrote numerous books\nincluding the origin of\nspecies, in which he laid\nout the theory of evolution\nby natural selection.\nalthough his proposed\nmechanism for natural\nvariation was never\naccepted, his ideas led to\nthe biggest intellectual\nrevolution of the 19th\ncentury, with\nrepercussions that\ncontinue today. ironically,\nhis own family was\nin-bred and his health\npoor. see desmond and\nmoore (1991).\n\n "}, {"Page_number": 14, "text": "2\n\n1 \u00b7 introduction\n\ntable 1.1 heights of\nyoung zea mays plants,\nrecorded by charles\ndarwin (fisher, 1935a,\np. 30).\n\nheight (eighths of an inch)\n\npot\n\ncrossed\n\nself-fertilized\n\ndifference\n\ni\n\nii\n\niii\n\niv\n\n188\n96\n168\n176\n153\n172\n177\n163\n146\n173\n186\n168\n177\n184\n96\n\n139\n163\n160\n160\n147\n149\n149\n122\n132\n144\n130\n144\n102\n124\n144\n\n49\n\u221267\n8\n16\n6\n23\n28\n41\n14\n29\n56\n24\n75\n60\n\u221248\n\nt\n\ni\n\nh\ng\ne\nh\n\n0\n8\n1\n\n0\n6\n1\n\n0\n4\n1\n\n0\n2\n1\n\n0\n0\n1\n\ncross\n\nself\n\ntype\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022 \u2022\n\u2022\n\n\u2022\n\n\u2022\n\nfigure 1.1 summary\nplots for darwin\u2019s zea\nmays data. the left panel\ncompares the heights for\nthe two different types of\nfertilization. the right\npanel shows the difference\nfor each pair plotted\nagainst the pair average.\n\n0\n0\n1\n\n0\n5\n\n0\n\ne\nc\nn\ne\nr\ne\n\nf\nf\ni\n\nd\n\n0\n5\n-\n\n\u2022\n\n0\n0\n1\n-\n\n\u2022\n\n120 130\n\n140\n\n150\n\n160\n\naverage\n\nthis end darwin planted them in pairs in different pots. table 1.1 gives the resulting\nheights. all but two of the differences between pairs in the fourth column of the table\nare positive, which suggests that cross-fertilized plants are taller than self-fertilized\nones.\n\nthis impression is confirmed by the left-hand panel of figure 1.1, which sum-\nmarizes the data in table 1.1 in terms of a boxplot. the white line in the centre of\neach box shows the median or middle observation, the ends of each box show the\nobservations roughly one-quarter of the way in from each end, and the bars attached\nto the box by the dotted lines show the maximum and minimum, provided they are\nnot too extreme.\n\ncross-fertilized plants seem generally higher than self-fertilized ones. overlaid\non this systematic variation, there seems to be variation that might be ascribed to\nchance: not all the plants within each group have the same height. it might be possible,\n\n "}, {"Page_number": 15, "text": "1 \u00b7 introduction\n\n3\n\nand for some purposes even desirable, to construct a mechanistic model for plant\ngrowth that could explain all the variation in such data. this would take into account\ngenetic variation, soil and moisture conditions, ventilation, lighting, and so forth,\nthrough a vast system of equations requiring numerical solution. for most purposes,\nhowever, a deterministic model of this sort is quite unnecessary, and it is simpler and\nmore useful to express variability in terms of probability distributions.\n\nif the spread of heights within each group is modelled by random variability, the\nsame cause will also generate variation between groups. this occurred to darwin,\nwho asked his cousin, francis galton, whether the difference in heights between the\ntypes of plants was too large to have occurred by chance, and was in fact due to\nthe effect of fertilization. if so, he wanted to estimate the average height increase.\ngalton proposed an analysis based essentially on the following model. the height of\na self-fertilized plant is taken to be\n\n(1.1)\n\n(1.2)\n\ny = \u00b5 + \u03c3 \u03b5,\n\nx = \u00b5 + \u03b7 + \u03c3 \u03b5,\n\nwhere \u00b5 and \u03c3 are fixed unknown quantities called parameters, and \u03b5 is a random\nvariable with mean zero and unit variance. thus the mean of y is \u00b5 and its variance\nis \u03c3 2. the height of a cross-fertilized plant is taken to be\n\nwhere \u03b7 is another unknown parameter. the mean height of a cross-fertilized plant\nis \u00b5 + \u03b7 and its variance is \u03c3 2. in(1.1) and (1.2) variation within the groups is\naccounted for by the randomness of \u03b5, whereas variation between groups is modelled\ndeterministically by the difference between the means of y and x. under this model\nthe questions posed by darwin amount to:\n\nr is \u03b7 non-zero?\nr can we estimate \u03b7 and state the uncertainty of our estimate?\n\ngalton\u2019s analysis proceeded as if the observations from the self-fertilized plants,\ny1, . . . ,y 15, were independent and identically distributed according to (1.1), and\nthose from the cross-fertilized plants, x1, . . . , x15, were independent and identically\ndistributed according to (1.2). if so, it is natural to estimate the group means by\ny = (y1 + \u00b7\u00b7\u00b7 + y15)/15 and x = (x1 + \u00b7\u00b7\u00b7 + x15)/15, and to compare y and x.\nin fact galton proposed another analysis which we do not pursue.\n\nin discussing this experiment many years later, r. a. fisher pointed out that the\nmodel based on (1.1) and (1.2) is inappropriate. in order to minimize differences in\nhumidity, growing conditions, and lighting, darwin had taken the trouble to plant the\nseeds in pairs in the same pots. comparison of different pairs would therefore involve\nthese differences, which are not of interest, whereas comparisons within pairs would\ndepend only on the type of fertilization. a model for this writes\n\ny j = \u00b5 j + \u03c3 \u03b51 j ,\n\nx j = \u00b5 j + \u03b7 + \u03c3 \u03b52 j ,\n\nj = 1, . . . ,15.\n\n(1.3)\n\nthe parameter \u00b5 j represents the effects of the planting conditions for the jth pair,\nand the \u03b5g j are taken to be independent random variables with mean zero and unit\n\nfrancis galton\n(1822\u20131911) was a cousin\nof darwin from the same\nwealthy background. he\nexplored in africa before\nturning to scientific work,\nin which he showed a\nstrong desire to quantify\nthings. he was one of the\nfirst to understand the\nimplications of evolution\nfor homo sapiens, he\ninvented the term\nregression and contributed\nto statistics as a\nby-product of his belief in\nthe improvement of\nsociety via eugenics. see\nstigler (1986).\n\nronald aylmer fisher\n(1890\u20131962) was born in\nlondon and educated\nthere and at cambridge,\nwhere he had his first\nexposure to mendelian\ngenetics and the biometric\nmovement. after\nobtaining the exact\ndistributions of the t\nstatistic and the\ncorrelation coefficient, but\nalso having begun a\nlife-long endeavour to\ngive a mendelian basis for\ndarwin\u2019s evolutionary\ntheory, he moved in 1919\nto rothamsted\nexperimental station,\nwhere he built the\ntheoretical foundations of\nmodern statistics, making\nfundamental contributions\nto likelihood inference,\nanalysis of variance,\nrandomization and the\ndesign of experiments. he\nwrote highly influential\nbooks on statistics and on\ngenetics. he later held\nposts at university\ncollege london and\ncambridge, and died in\nadelaide. see fisher box\n(1978).\n\n "}, {"Page_number": 16, "text": "4\n\n1 \u00b7 introduction\n\nstress (n/mm2)\n\n950\n\n900\n\n850\n\n800\n\n750\n\n225\n171\n198\n189\n189\n135\n162\n135\n117\n162\n\n168\n33\n\n216\n162\n153\n216\n225\n216\n306\n225\n243\n189\n\n215\n43\n\n324\n321\n432\n252\n279\n414\n396\n379\n351\n333\n\n348\n58\n\n627\n1051\n1434\n2020\n525\n402\n463\n431\n365\n715\n\n803\n544\n\ny\ns\n\n3402\n9417\n1802\n4326\n11520+\n7152\n2969\n3012\n1550\n11211\n\n5636\n3864\n\n9828\n3355\n\ntable 1.2 failure times\n(in units of 103 cycles) of\nsprings at cycles of\nrepeated loading under the\ngiven stress (cox and\noakes, 1984, p. 8). +\nindicates that an\nobservation is\nright-censored. the\naverage and estimated\nstandard deviation for\neach level of stress are y\nand s.\n\n700\n12510+\n12505+\n3027\n12505+\n6253\n8011\n7795\n11604+\n11604+\n12470+\n\nvariance. the \u00b5 j could be eliminated by basing the analysis on the x j \u2212 y j , which\nhave mean \u03b7 and variance 2\u03c3 2.\nthe right panel of figure 1.1 shows a scatterplot of pair differences x j \u2212 y j against\npair averages (y j + x j )/2. the two negative differences correspond to the pairs with\nthe lowest averages. the averages vary widely, and it seems wise to allow for this by\n(cid:1)\nanalyzing the differences, as fisher suggested.\n\nboth models in example 1.1 summarize the effect of interest, namely the mean\ndifference in heights of the plants, in terms of a fixed but unknown parameter. other\naspects of secondary interest, such as the mean height of self-fertilized plants, are\nalso summarized by the parameters \u00b5 and \u03c3 of (1.1) and (1.2), and \u00b51, . . . , \u00b515 and \u03c3\nof (1.3). but even if the values of all these parameters were known, the distributions\nof the heights would still not be known completely, because the distribution of \u03b5 has\nnot been fully specified. such a model is called nonparametric. if wewere willing\nto assume that \u03b5 has a given distribution, then the distributions of y and x would be\ncompletely specified once the parameters were known, giving a parametric model.\nmost of this book concerns such models.\n\nthe focus of interest in example 1.1 is the relation between the height of a plant\nand something that can be controlled by the experimenter, namely whether it is self-\nor cross-fertilized. the essence of the model is to regard the height as random with a\ndistribution that depends on the type of fertilization, which is fixed for each plant. the\nvariable of primary interest, in this instance height, is called the response, and the vari-\nable on which it depends, the type of fertilization, is called an explanatory variable or\na covariate. many questions arising in data analysis involve the dependence of one or\nmore variables on another or others, but virtually limitless complications can arise.\n\nexample 1.2 (spring failure data)\nin industrial experiments to assess their reli-\nability, springs were subjected to cycles of repeated loading until they failed. the\nfailure \u2018times\u2019, in units of 103 cycles of loading, are given in table 1.2. there were\n60 springs divided into groups of 10 at each of six different levels of stress.\n\n "}, {"Page_number": 17, "text": "1 \u00b7 introduction\n\nfigure 1.2 failure times\n(in units of 103 cycles) of\nsprings at cycles of\nrepeated loading under the\ngiven stress. the left\npanel shows failure time\nboxplots for the different\nstresses. the right panel\nshows a rough linear\nrelation between log\naverage and log variance\nat the different stresses.\n\ne\nr\nu\n\nl\ni\n\na\n\nf\n \n\no\n\nl\n\nt\n \ns\ne\nc\ny\nc\n\n0\n0\n0\n0\n1\n\n0\n0\n0\n6\n\n0\n0\n0\n2\n\n0\n\n5\n\n\u2022\n\n\u2022\n\n6\n1\n\n4\n1\n\n2\n1\n\n0\n1\n\n8\n\ne\nc\nn\na\ni\nr\na\nv\n \n\ng\no\nl\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n700 750 800 850 900 950\n\n5\n\n6\n\n7\n\n8\n\n9\n\nstress\n\nlog average\n\nas stress decreases there is a rapid increase in the average number of cycles to\nfailure, to the extent that at the lowest levels, where the failure time is longest, the\nexperiment had to be stopped before all the springs had failed. the observations are\nright-censored: the recorded value is a lower bound for the number of cycles to failure\nthat would have been observed had the experiment been continued to the bitter end.\na right-censored observation is indicated as, say, 11520+, indicating that the failure\ntime would be greater than 11520.\nlet us represent the jth number of cycles to failure at the kth loading by yl j , for\nj = 1, . . . ,10 and l = 1, . . . ,6. table 1.2 shows the average failure time for each\nloading, yl\u00b7 = 10\n\u22121\nj yl j , and the sample standard deviation, sl, where the sample\nj (yl j \u2212 yl\u00b7)2. the average and variance at the lowest\nvariance is s2\nl\nstresses underestimate the true values, because of the censoring. the average and\nstandard deviation decrease as stress increases.\n\n(cid:1)\n= (10 \u2212 1)\n\n(cid:1)\n\n\u22121\n\nthe boxplots in the left panel of figure 1.2 show that the cycles to failure at\neach stress have the marked pattern already described. the right panel shows the log\nvariance, log s2\nl , plotted against the log average, log yl\u00b7. itshows a linear pattern with\nslope approximately two, suggesting that variance is proportional to mean squared\nfor these data.\n\nour inspection has revealed that:\nfailure times are positive and range from 117\u201312510\u00d7103 or more cycles;\n\n(a)\n(b) there is strong dependence between the mean and variance;\n(c)\n(d) some observations are censored.\n\nthere is strong dependence of failure time on stress; and\n\nto proceed further, we would need to know how the data were gathered. do system-\natic patterns, of which we have been told nothing, underlie the data? for example, were\nall 60 springs selected at random from a larger batch and then allocated to the different\nstresses at random? or were the ten springs at 950 n/mm2 selected from one batch,\nthe ten springs at 900 n/mm2 from another, and so on? if so, the apparent dependence\non stress might be due to differences among batches. were all measurements made\n\n "}, {"Page_number": 18, "text": "6\n\n1 \u00b7 introduction\n\nwith the same machine? if the answers to these and other such questions were un-\nsatisfactory, we might suggest that better data be produced by performing another\nexperiment designed to control the effects of different sources of variability.\n\nsuppose instead that we are provisionally satisfied that we can treat observations\nat each loading as independent and identically distributed, and that the apparent\ndependence between cycles to failure and stress is not due to some other factor. with\n(a) and (b) in mind, we aim to represent the failure time at a given stress level by a\nrandom variable y that takes continuous positive values and whose probability density\nfunction f (y; \u03b8) keeps the ratio (mean)2/variance constant. clearly it is preferable if\nthe same parametric form is used at each stress and the effect of changing stress enters\nonly through \u03b8. asimple model is that y has exponential density\n\nf (y; \u03b8) = \u03b8\u22121 exp(\u2212y/\u03b8),\n\ny > 0, \u03b8 > 0,\n\n(1.4)\n\nwhose mean and variance are \u03b8 and \u03b8 2, sothat (mean) 2 = variance. we can express\nsystematic variation in the density of y in terms of stress, x, by\n\n\u03b8 = 1\n\u03b2x\n\n,\n\nx > 0, \u03b2 > 0,\n\n(1.5)\n\nthough of course other forms of dependence are possible.\n\nequations (1.4) and (1.5) imply that when x = 0 the mean failure time is infinite,\nbut itdecreases to zero as stress x increases. expression (1.4) represents the random\ncomponent of the model, for a given value of \u03b8, and (1.5) the systematic component,\n(cid:1)\nwhich determines how mean failure time \u03b8 depends on x.\n\nin examples 1.1 and 1.2 the response is continuous, and there is a single explanatory\nvariable. but data with a discrete response or more than one explanatory variable often\narise in practice.\n\nexample 1.3 (challenger data) the space shuttle challenger exploded shortly\nafter its launch on 28 january 1986, with a loss of seven lives. the subsequent us\npresidential commission concluded that the accident was caused by leakage of gas\nfrom one of the fuel-tanks. rubber insulating rings, so-called \u2018o-rings\u2019, were not\n\u25e6\nf, and did not plug the joint\npliable enough after the overnight low temperature of 31\nbetween the fuel in the tanks and the intense heat outside.\n\nthere are two types of joint, nozzle-joints and field-joints, each containing a pri-\nmary o-ring and a secondary o-ring, together with putty that insulates both rings\nfrom the propellant gas. table 1.3 gives the number of primary rings, r, out of the\ntotal m = 6 field-joints, that had experienced \u2018thermal distress\u2019 on previous flights.\nthermal distress occurs when excessive heat pits the ring \u2014 \u2018erosion\u2019 \u2014 or when\ngases rush past the ring \u2014- \u2018blowby\u2019. blowby can occur in the short gap after igni-\ntion before an o-ring seals. it can also occur if the ring seals and then fails, perhaps\nbecause it has been eroded by the hot gas. bench tests had suggested that one cause\nof blowby was that the o-rings lost their resilience at low temperatures. it was also\nsuspected that pressure tests conducted before each launch holed the putty, making\nerosion of the rings more likely.\n\n "}, {"Page_number": 19, "text": "table 1.3 o-ring\nthermal distress data. r is\nthe number of field-joint\no-rings showing thermal\ndistress out of 6, for a\nlaunch at the given\ntemperature (\nf) and\npressure (pounds per\nsquare inch) (dalal et al.,\n1989).\n\n\u25e6\n\n1 \u00b7 introduction\n\n7\n\nnumber of o-rings with\n\ntemperature (\n\n\u25e6\n\nf)\n\npressure (psi)\n\nflight\n\ndate\n\nthermal distress, r\n\n1\n2\n3\n5\n6\n7\n8\n9\n41-b\n41-c\n41-d\n41-g\n51-a\n51-c\n51-d\n51-b\n51-g\n51-f\n51-i\n51-j\n61-a\n61-b\n61-c\n\n21/4/81\n12/11/81\n22/3/82\n11/11/82\n4/4/83\n18/6/83\n30/8/83\n28/11/83\n3/2/84\n6/4/84\n30/8/84\n5/10/84\n8/11/84\n24/1/85\n12/4/85\n29/4/85\n17/6/85\n29/7/85\n27/8/85\n3/10/85\n30/10/85\n26/11/86\n21/1/86\n\n0\n1\n0\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n2\n0\n0\n0\n0\n0\n0\n2\n0\n1\n\n61-i\n\n28/1/86\n\n\u2014\n\nx1\n\n66\n70\n69\n68\n67\n72\n73\n70\n57\n63\n70\n78\n67\n53\n67\n75\n70\n81\n76\n79\n75\n76\n58\n\n31\n\nx2\n\n50\n50\n50\n50\n50\n50\n100\n100\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n\n200\n\nfigure 1.3 o-ring\nthermal distress data. the\nleft panel shows the\nproportion of incidents as\na function of joint\ntemperature, and the right\npanel shows the\ncorresponding plot against\npressure. the x-values\nhave been jittered to avoid\noverplotting multiple\npoints. the solid lines\nshow the fitted proportions\nof failures under a model\ndescribed in chapter 4.\n\nn\no\n\ni\nt\nr\no\np\no\nr\np\n\n0\n\n.\n\n1\n\n5\n\n.\n\n0\n\n0\n\n.\n\n0\n\n0\n\n.\n\n1\n\n5\n\n.\n\n0\n\nn\no\n\ni\nt\nr\no\np\no\nr\np\n\n+\n\n+\n\n+\n\n+\n+\n\n+\n+ +\n\n+\n+++ ++ +\n+ ++ ++ +\n\n++\n+\n\n0\n\n.\n\n0\n\n+\n+ +++\n\n++\n\n+\n\n+\n\n+\n\n+ ++\n\n++\n++ ++++\n+\n\n30\n\n40\n\n50\n\n60\n\n70\n\n80\n\n90\n\n0\n\n50\n\n100\n\n150\n\n200\n\ntemperature (degrees f)\n\npressure (psi)\n\ntable 1.3 shows the temperatures x1 and test pressures x2 associated with thermal\ndistress of the o-rings for flights before the disaster. the pattern becomes clearer\nwhen the proportion of failures, r/m, isplotted against temperature and pressure in\nfigure 1.3. as temperature decreases, r/m appears to increase. there is less pattern\nin the corresponding plot for pressure.\n\n "}, {"Page_number": 20, "text": "8\n\nyears of\nsmoking t\n\n15\u201319\n20\u201324\n25\u201329\n30\u201334\n35\u201339\n40\u201344\n45\u201349\n50\u201354\n55\u201359\n\n1 \u00b7 introduction\n\ndaily cigarette consumption d\n\nnonsmokers\n\n1\u20139\n\n10\u201314\n\n15\u201319\n\n20\u201324\n\n25\u201334\n\n35+\n\n10366/1\n8162\n5969\n4496\n3512\n2201\n1421\n1121\n826/2\n\n3121\n2937\n2288\n2015\n1648/1\n1310/2\n927\n710/3\n606\n\n3577\n3286/1\n2546/1\n2219/2\n1826\n1386/1\n988/2\n684/4\n449/3\n\n4317\n4214\n3185\n2560/4\n1893\n1334/2\n849/2\n470/2\n280/5\n\n5683\n6385/1\n5483/1\n4687/6\n3646/5\n2411/12\n1567/9\n857/7\n416/7\n\n3042\n4050/1\n4290/4\n4268/9\n3529/9\n2424/11\n1409/10\n663/5\n284/3\n\n670\n1166\n1482\n1580/4\n1336/6\n924/10\n556/7\n255/4\n104/1\n\ntable 1.4 lung cancer\ndeaths in british male\nphysicians (frome, 1983).\nthe table gives man-years\nat risk/number of cases of\nlung cancer,\ncross-classified by years\nof smoking, taken to be\nage minus 20 years, and\nnumber of cigarettes\nsmoked per day.\n\nfor these data, the response variable takes one of the values 0, 1, . . . ,6, with fairly\nstrong dependence on temperature and possibly weaker dependence on pressure.\nif we assume that at a given temperature and pressure, each of the six rings fails\nindependently with equal probability, we can treat the number of failures r as binomial\nwith denominator m and probability \u03c0,\n\npr(r = r) =\n\nm!\n\nr!(m \u2212 r)!\n\n\u03c0 r (1 \u2212 \u03c0)m\u2212r ,\n\nr = 0, 1, . . . ,m , 0 < \u03c0 <1.\n\n(1.6)\n\none possible relation between temperature x1, pressure x2, and the probability of\nfailure is \u03c0 = \u03b20 + \u03b21x1 + \u03b22x2, where the parameters \u03b20, \u03b21, and \u03b22 must be derived\nfrom the data. this has the drawback of predicting probabilities outside the range [0, 1]\nfor certain values of x1 and x2. it ismore satisfactory to use a function such as\n\n\u03c0 = exp(\u03b20 + \u03b21x1 + \u03b22x2)\n1 + exp(\u03b20 + \u03b21x1 + \u03b22x2)\n\n,\n\nso 0 < \u03c0 <1 wherever \u03b20 + \u03b21x1 + \u03b22x2 roams in the real line. it turns out that the\nfunction eu/(1 + eu), the logistic distribution function, has an elegant connection to\nthe binomial density, but any other continuous distribution function with domain the\nreal line might be used.\n\nthe night before the challenger was launched, there was a lengthy discussion\nabout how the o-rings might behave at the low predicted launch temperature. one\napproach, which was not taken, would have been to try and predict how many o-rings\nmight fail based on an estimated relationship between temperature and pressure. the\nlines in figure 1.3 represent the estimated dependence of failure probability on x1\nand x2, and show a high probability of failure at the actual launch temperature. when\nthis is used as input to a probability model of how failures occur, the probability of\ncatastrophic failure for a launch at 31\nf isestimated to be as high as 0.16. to obtain\nthis estimate involves extrapolation outside the available data, but there would have\n(cid:1)\nbeen little alternative in the circumstances of the launch.\n\n\u25e6\n\nexample 1.4 (lung cancer data) table 1.4 shows data on the lung cancer mortality\nof cigarette smokers among british male physicians. the table shows the man-years\n\n "}, {"Page_number": 21, "text": "figure 1.4 lung cancer\ndeaths in british male\nphysicians. the figure\nshows the rate of deaths\nper 1000 man-years at\nrisk, for each of three\nlevels of daily cigarette\nconsumption.\n\n1 \u00b7 introduction\n\ne\n\nt\n\na\nr\n \n\nh\n\nt\n\na\ne\nd\n\n5\n1\n\n0\n1\n\n5\n\n0\n\n9\n\n   cigarettes\n\n20+\n1-19\n0\n\n15-19 20-24 25-29 30-34 35-39 40-44 45-49 50-54 55-59\n\nyears smoking\n\nat risk and the number of cases with lung cancer, cross-classified by the number of\nyears of smoking, taken to be age minus twenty years, and the number of cigarettes\nsmoked daily. the man-years at risk in each category is the total period for which the\nindividuals in that category were at risk of death.\n\nas the eye moves from top left to the bottom right of the table, the figures suggest\nthat death rate increases with increased total cigarette consumption. this is confirmed\nby figure 1.4, which shows the death rate per 100,000 man-years at risk, grouped by\nthree levels of cigarette consumption. data for the first two groups show that death\nrate for smokers increases with cigarette consumption and with years of smoking.\nthe only nonsmoker deaths are one in the age-group 35\u201339 and two in the age-group\n75\u201379.\n\nin this problem the aspect of primary interest is how death rate depends on cigarette\nconsumption and smoking, and we treat the number of deaths in each category as the\nresponse. to build a model, we suppose that the death rate for those smoking d\ncigarettes per day after t years of smoking is \u03bb(d, t) deaths per man-year. thus we\nmay imagine deaths occurring at random in the total t man-years at risk in that\ncategory, at rate \u03bb(d, t). if deaths are independent point events in a continuum of\nlength t , the number of deaths, y , will have approximately a poisson density with\nmean t \u03bb(d, t),\n\npr(y = y) = {t \u03bb(d, t)}y\n\ny!\n\nexp{\u2212t \u03bb(d, t)},\n\ny = 0, 1, 2, . . . .\n\none possible form for the mean deaths per man-year is\n\n\u03bb(d, t) = \u03b20t \u03b21\n\n1 + \u03b22d \u03b23\n\n(cid:2)\n\n(cid:3)\n\n,\n\n(1.7)\n\n(1.8)\n\nbased on a deterministic argument and used in animal cancer mortality studies. in\n(1.8) there are four unknown parameters, and power-law dependence of death rate on\nexposure duration, t, and cigarette consumption, d. we expect that all the parameters\n\u03b2r are positive. the background death-rate in the absence of smoking is given by \u03b20t \u03b21,\nthe death-rate for nonsmokers. this represents the overall effect of other causes of\nlung cancer.\n\n "}, {"Page_number": 22, "text": "10\n\n1 \u00b7 introduction\n\nexpressions (1.7) and (1.8) give the random and systematic components for a simple\nmodel for the data, based on a blend of stochastic and deterministic arguments. an\nincreasingly important development in statistics is the use of very complex models\nfor real-world phenomena. stochastic processes often provide the blocks with which\n(cid:1)\nsuch models are built.\n\nthere is an important difference between example 1.4 and the previous examples.\nin example 1.1, darwin could decide which plants to cross and where to plant them,\nin example 1.2 the springs could be allocated to different stresses by the experimenter,\nand in example 1.3 the test pressure for field joints was determined by engineers. the\nengineers would have no control over the temperature at the proposed time of a launch,\nbut they could decide whether or not to launch at a given temperature. in each case,\nthe allocation of treatments could in principle be controlled, albeit to different extents.\nsuch situations, called controlled experiments, often involve a random allocation of\ntreatments \u2014 type of fertilization, level of stress or test pressure \u2014 to units \u2014 plants,\nsprings, or flights. strong conclusions can in principle be drawn when randomization\nis used \u2014 though it played no part in examples 1.1 or 1.3, and we do not know about\nexample 1.2.\n\nin example 1.4, however, a new problem rears its head. there is no question of\nallocating a level of cigarette consumption over a given period to individuals \u2014 the\npractical difficulties would be insuperable, quite apart from ethical considerations. in\ncommon with many other epidemiological, medical, and environmental studies, the\ndata are observational, and this limits what conclusions may be drawn. it might be\npostulated that propensities to smoking and to lung cancer were genetically related,\ncausing the apparent dependence in table 1.4. then for an individual to stop smoking\nwould not reduce their chance of contracting lung cancer. in such cases data of\ndifferent types from different sources must be gathered and their messages carefully\ncollated and interpreted in order to put together an unambiguous story.\n\ndespite differences in interpretation, the use of probability models to summarize\nvariability and express uncertainty is the basis of each example. it is the subject of\nthis book.\n\noutline\n\nthe idea of treating data as outcomes of random variables has implications for how\nthey should be treated. for example, graphical and numerical summaries of the ob-\nservations will show variation, and it is important to understand its consequences.\nchapter 2 is devoted to this. it deals with basic ideas such as parameters, statistics,\nand sampling variation, simple graphs and other summary quantities, and then turns\nto notions of convergence, which are essential for understanding variability in large\nsamples and generating approximations for small ones. many statistics are based on\nquantities such as the largest item in a sample, and order statistics are also discussed.\nthe chapter finishes with an account of moments and cumulants.\n\n "}, {"Page_number": 23, "text": "thomas bayes\n(1702\u20131761) was a\nnonconformist minister\nand also a mathematician.\nhis theorem is contained\nin his essay towards\nsolving a problem in the\ndoctrine of chances, found\nin his papers after his\ndeath and published in\n1764.\n\nf (y)\n\n1 \u00b7 introduction\n\n11\n\nvariation in observed data leads to uncertainty about the reality behind it. un-\ncertainty is a more complicated notion, because it entails considering what it is\nreasonable to infer from the data, and people differ in what they find reasonable.\nchapter 3 explains one of the main approaches to expressing uncertainty, leading\nto the construction of confidence intervals via quantities known as pivots. in most\ncases these can only be approximate, but they are often exact for models based on the\nnormal distribution, which are then described. the chapter ends with a brief account\nof monte carlo simulation, which is used both to appreciate variability and to assess\nuncertainty.\n\nin some cases information about model parameters \u03b8 can be expressed as a density\n\u03c0(\u03b8), separate from the data y. then the prior uncertainty \u03c0(\u03b8) may be updated to\nposterior uncertainty \u03c0(\u03b8 | y) using bayes\u2019 theorem\n\u03c0(\u03b8 | y) = \u03c0(\u03b8) f (y | \u03b8)\n\n,\n\nwhich converts the conditional density f (y | \u03b8) ofobserving data y, given that the true\nparameter is \u03b8, into a conditional density for \u03b8, given that y has been observed. this\nbayesian approach to inference is attractive and conceptually simple, and modern\ncomputing techniques make it feasible to apply it to many complex models. however\nmany statisticians do not agree that prior knowledge can or indeed should always be\nexpressed as a prior density, and believe that information in the data should be kept\nseparate from prior beliefs, preferring to base inference on the second term f (y | \u03b8)\nin the numerator of bayes\u2019 theorem, known as the likelihood.\n\nlikelihood is a central idea for parametric models, and it and its ramifications are\ndescribed in chapter 4. definitions of likelihood, the maximum likelihood estimator\nand information are followed by a discussion of inference based on maximum like-\nlihood estimates and likelihood ratio statistics. the chapter ends with brief accounts\nof non-regular models and model selection.\n\nchapters 5 and 6 describe some particular classes of models. accounts are given\nof the simplest form of linear model, of exponential family and group transformation\nmodels, of models for survival and missing data, and of those with more complex\ndependence structures such as markov chains, markov random fields, point processes,\nand the multivariate normal distribution.\n\nchapter 7 discusses more traditional topics of mathematical statistics, with a more\ngeneral treatment of point and interval estimation and testing than in the previous\nchapters. it also includes an account of estimating functions, which are needed sub-\nsequently.\n\nregression models describe how a response variable, treated as random, depends\non explanatory variables, treated as fixed. the vast majority of statistical modelling\ninvolves some form of regression, and three chapters of the book are devoted to it.\nchapter 8 describes the linear model, including its basic properties, analysis of vari-\nance, model building, and variable selection. chapter 9 discusses the ideas underlying\nthe use of randomization and designed experiments, and closes with an account of\nmixed effect models, in which some parameters are treated as random. these two\n\n "}, {"Page_number": 24, "text": "12\n\n1 \u00b7 introduction\n\nchapters are largely devoted to the classical linear model, in which the responses\nare supposed normally distributed, but since around 1970 regression modelling has\ngreatly broadened. chapter 10 is devoted to nonlinear models. it starts with an account\nof likelihood estimation using the iterative weighted least squares algorithm, which\nsubsequently plays a unifying role and then describes generalized linear models,\nbinary data and loglinear models, semiparametric regression by local likelihood esti-\nmation and by penalized likelihood. it closes with an account of regression modelling\nof survival data.\n\nbayesian statistics is discussed in chapter 11, starting with discussion of the role\nof prior information, followed by an account of bayesian analogues of procedures\ndeveloped in the earlier chapters. this is followed by a brief overview of bayesian\ncomputation, including laplace approximation, the gibbs sampler and metropolis\u2013\nhastings algorithm. the chapter closes with discussion of hierarchical and empirical\nbayes and a very brief account of decision theory.\n\nlikelihood is a favourite tool of statisticians but sometimes gives poor inferences.\nchapter 12 describes some reasons for this, and outlines how conditional or marginal\nlikelihoods can give better procedures.\n\nthe main links among the chapters of this book are shown in figure 1.5.\n\nnotation\n\nthe notation used in this book is fairly standard, but there are not enough letters in\nthe roman and greek alphabets for total consistency. greek letters generally denote\nparameters or other unknowns, with \u03b1 largely reserved for error rates and confidence\nlevels in connection with significance tests and confidence sets. roman letters x, y ,\nz, and so forth are mainly used for random variables, which take values x, y, z.\nprobability, expectation, variance, covariance, and correlation are denoted pr(\u00b7),\ne(\u00b7), var(\u00b7) cov(\u00b7,\u00b7), and corr(\u00b7,\u00b7), while cum(\u00b7,\u00b7,\u00b7\u00b7\u00b7) isoccasionally used to denote\na cumulant. we use i (a) todenote the indicator random variable, which equals 1 if\nthe event a occurs and 0 otherwise. a related function is the heaviside function\n\n(cid:4)\n\nh(u) =\n\n0,\n1,\n\nu < 0,\nu \u2265 0,\n\nwhose generalized derivative is the dirac delta function \u03b4(u). this satisfies\n\n(cid:5)\n\n\u03b4(y \u2212 u)g(u) du = g(y)\n\nfor any function g.\n\nsubscripts coincide, and equal zero otherwise.\n\nthe kronecker delta symbols \u03b4rs, \u03b4rst , and so forth all equal unity when all their\nwe use (cid:3)x(cid:4) to denote the largest integer smaller than or equal to x, and (cid:5)x(cid:6) to\nthe symbol \u2261 indicates that constants have been dropped in defining a log likeli-\n.\u223c ind\u223c , and iid\u223c are\n\n.= means \u2018approximately equals\u2019. the symbols \u223c,\n\ndenote the smallest integer larger than or equal to x.\n\nhood, while\n\n "}, {"Page_number": 25, "text": "1 \u00b7 introduction\n\n13\n\nfigure 1.5 a mapof the\nmain dependencies among\nchapters of this book. a\nsolid line indicates strong\ndependence and a dashed\nline indicates partial\ndependence through the\ngiven subsections.\n\nshorthand for \u2018is distributed as\u2019, \u2018is approximately distributed as\u2019, \u2018are independently\ndistributed as\u2019, and \u2018are independent and identically distributed as\u2019, while d= means\n\u2018has the same distribution as\u2019. x \u22a5 y means \u2018 x is independent of y \u2019. we use\nd\u2212\u2192 and\np\u2212\u2192 to denote convergence in distribution and in probability. to say that y1, . . . ,y n\nare a random sample from some distribution means that they are independent and\nidentically distributed according to that distribution.\n\nwe mostly reserve z for standard normal random variables. as usual n (\u00b5, \u03c3 2)\nrepresents the normal distribution with mean \u00b5 and variance \u03c3 2. the standard normal\ncumulative distribution and density functions are denoted \u0001 and \u03c6. weuse c\u03bd(\u03b1),\nt\u03bd(\u03b1), and f\u03bd1,\u03bd2(\u03b1) todenote the \u03b1 quantiles of the chi-squared distribution, student\nt distribution with \u03bd degrees of freedom, and f distribution with \u03bd1 and \u03bd2 degrees of\n\n "}, {"Page_number": 26, "text": "14\n\n1 \u00b7 introduction\n\nfreedom, while u (0, 1) denote the uniform distribution on the unit interval. almost\neverywhere, z\u03b1 is the \u03b1 quantile of the n (0, 1) distribution.\nthe data values in a sample of size n, typically denoted y1, . . . , yn, are the observed\nvalues of the random variables y1, . . . ,y n; their average is y = n\ny j and their\nsample variance is s2 = (n \u2212 1)\n\n(y j \u2212 y)2.\n\n(cid:1)\n\n(cid:1)\n\n\u22121\n\n\u22121\n\nwe avoid boldface type, and rely on the context to make it plain when we are\ndealing with vectors or matrices; at denotes the matrix transpose of a vector or matrix\na. the identity matrix of side n is denoted in, and 1n is a n \u00d7 1 vector of ones. if\n\u03b8 is a p \u00d7 1 vector and (cid:13)(\u03b8) ascalar, then \u2202(cid:13)(\u03b8)/\u2202\u03b8 is the p \u00d7 1 vector whose rth\nelement is \u2202(cid:13)(\u03b8)/\u2202\u03b8r , and \u2202 2(cid:13)(\u03b8)/\u2202\u03b8 \u2202\u03b8 t is the p \u00d7 p matrix whose (r, s) element is\n\u2202 2(cid:13)(\u03b8)/\u2202\u03b8r \u2202\u03b8s.\n(cid:1)\nthe end of each example is marked thus:\nexercise 2.1.3 denotes the third exercise at the end of section 2.1, problem 2.3 is\n\nthe third problem at the end of chapter 2, and so forth.\n\n "}, {"Page_number": 27, "text": "2\n\nvariation\n\nthe key idea in statistical modelling is to treat the data as the outcome of a random\nexperiment. the purpose of this chapter is to understand some consequences of this:\nhow to summarize and display different aspects of random data, and how to use\nresults of probability theory to appreciate the variation due to this randomness. we\noutline the elementary notions of statistics and parameters, and then describe how\ndata and statistics derived from them vary under sampling from statistical models.\nmany quantities used in practice are based on averages or on ordered sample values,\nand these receive special attention. the final section reviews moments and cumulants,\nwhich will be useful in later chapters.\n\n2.1 statistics and sampling variation\n2.1.1 data summaries\nthe most basic element of data is a single observation, y \u2014 usually a number, but\nperhaps a letter, curve, or image. throughout this book we shall assume that whatever\ntheir original form, the data can be recoded as numbers. we shall mostly suppose that\nsingle observations are scalar, though sometimes they are vectors or matrices.\n\nwe generally deal with an ensemble of n observations, y1, . . . , yn, known as a\nsample. occasionally interest centres on the given sample alone, and if n is not tiny it\nwill be useful to summarize the data in terms of a few numbers. we say that a quantity\ns = s(y1, . . . , yn) that can be calculated from y1, . . . , yn is a statistic. such quantities\nmay be wanted for many different purposes.\n\nlocation and scale\ntwo basic features of a sample are its typical value and a measure of how spread\nout the sample is, sometimes known respectively as location and scale. they can be\nsummarized in many ways.\n\nexample 2.1 (sample moments) sample moments are calculated by putting mass\n\u22121 on each of the y j , and then calculating the mean, variance, and so forth. the\nn\n\n15\n\n "}, {"Page_number": 28, "text": "16\n\n2 \u00b7 variation\n\nsimplest of these sample moments are\n\nn(cid:1)\nj=1\n\ny = 1\nn\n\ny j = 1\nn\n\n(y1 + \u00b7\u00b7\u00b7 + yn)\n\nand\n\nn(cid:1)\nj=1\n\n1\nn\n\n(y j \u2212 y)2;\n\nwe call the first of these the average. inpractice the denominator n in the second\nmoment is usually replaced by n \u2212 1, giving the sample variance\n\ns2 = 1\nn \u2212 1\n\nn(cid:1)\nj=1\n\n(y j \u2212 y)2.\n\n(2.1)\n\nthe denominator n \u2212 1 isjustified in example 2.14.\n\nhere y and s have the same dimensions as the y j , and are measures of location and\n(cid:1)\n\nscale respectively.\n\npotential confusion is avoided by using the word average to refer to a quantity cal-\nculated from data, and the words mean or expectation for the corresponding theoretical\nquantity; this convention is used throughout this book.\n\nexample 2.2 (order statistics) the order statistics of y1, . . . , yn are their values\nput in increasing order, which we denote y(1) \u2264 y(2) \u2264 \u00b7\u00b7\u00b7 \u2264 y(n). if y1 = 5, y2 = 2\nand y3 = 4, then y(1) = 2, y(2) = 4 and y(3) = 5. examples of order statistics are the\nsample minimum y(1) and sample maximum y(n), and the lower and upper quartiles\ny((cid:2)n/4(cid:3)) and y((cid:2)3n/4(cid:3)). the lowest quarter of the sample lies below the lower quartile,\nand the highest quarter lies above the upper quartile.\n\namong statistics that can be based on the y( j) are the sample median, defined as\n\nmedian(y j ) =\n\n(cid:2)\n\n(cid:3)\ny((n+1)/2),\n1\n2\n\ny(n/2) + y(n/2+1)\n\n(cid:4)\n\nn odd,\nn even.\n\n,\n\n(2.2)\n\n(cid:2)u(cid:3) denotes the smallest\ninteger greater than or\nequal to u.\n\nthis is the centre of the sample: equal proportions of the data lie above and below it.\nall these statistics are examples of sample quantiles. the pth sample quantile is\nthe value with a proportion p of the sample to its left. thus the minimum, maximum,\nquartiles, and median are (roughly) the 0, 1, 0.25, 0.75 and 0.5 sample quantiles. like\nthe median (2.2) when n is even, the pth sample quantile for non-integer pn is usually\ncalculated by linear interpolation between the order statistics that bracket it.\nanother measure of location is the average of the central observations of the sample.\nsuppose that p lies in the interval [0, 0.5), and that k = pn is an integer. then the\np\u00d7100% trimmed average is defined as\n\nn\u2212k(cid:1)\nj=k+1\n\ny( j),\n\n1\n\nn \u2212 2k\n\nwhich is the usual average y when p = 0. the 50% trimmed average ( p = 0.5) is\ndefined to be the median, while other values of p interpolate between the average and\nthe median. linear interpolation is used when pn is non-integer.\nthe statistics above measure different aspects of sample location. some mea-\nsures of scale based on the order statistics are the range, y(n) \u2212 y(1), the interquartile\n\n "}, {"Page_number": 29, "text": "2.1 \u00b7 statistics and sampling variation\n\nrange and the median absolute deviation,\n\n17\n\niqr = y((cid:2)3n/4(cid:3)) \u2212 y((cid:2)n/4(cid:3)), mad = median{|yi \u2212 median(y j )|}.\n\nthese are, respectively, the difference between the largest and smallest observations,\nthe difference between the observations at the ends of the central 50% of the sample,\nand the median of the absolute deviations of the observations from the sample median.\none would expect the range of a sample to grow with its size, but the iqr and mad\nshould depend less on the sample size and in this sense are more stable measures of\n(cid:1)\nscale.\nit is easy to establish that the mapping y1, . . . , yn (cid:4)\u2192 a + by1, . . . , a + byn changes\nthe values of location and scale measures in the previous examples by m, s (cid:4)\u2192 a +\nbm, bs (exercise 2.1.1); this seems entirely reasonable.\n\nbad data\nthe statistics described in examples 2.1 and 2.2 measure different aspects of location\nand of scale. they also differ in their susceptibility to bad data. consider what happens\nwhen an error, due perhaps to mistyping, results in an observation that is unusual\ny1 is replaced by y1 + \u03b4, the\ncompared to the others \u2014 an outlier. ifthe \u2018true\u2019\naverage changes from y to y + n\n\u22121\u03b4, which could be arbitrarily large, while the\nsample median changes by a bounded amount \u2014 the most that can happen is that it\nmoves to an adjacent observation. we say that the sample median is resistant, while\nthe average is not. roughly a quarter of the data would have to be contaminated\nbefore the interquartile range could change by an arbitrarily large amount, while the\nrange and sample variance are sensitive to a single bad observation. the large-sample\nproportion of contaminated observations needed to change the value of a statistic by\nan arbitrarily large amount is called its breakdown point; it is acommon measure of\nthe resistance of a statistic.\n\nexample 2.3 (birth data) table 2.1 shows data extracted from a census of all the\nwomen who arrived to give birth at the john radcliffe hospital in oxford during a\nthree-month period. the table gives the times that women with vaginal deliveries \u2014\nthat is, without caesarian section \u2014 spent in the delivery suite, for the first seven of\n92 successive days of data.\n\nthe initial step in dealing with data is to scrutinize them closely, and to under-\nstand how they were collected. in this case the time for each birth was recorded\nby the midwife who attended it, and numerous problems might have arisen in the\nrecording. for example, one midwife might intend 4.20 to mean 4.2 hours, but an-\nother might mean 4 hours and 20 minutes. moreover it is difficult to believe that a\ntime can be known as exactly as 2 hours and 6 minutes, as would be implied by the\nvalue 2.10. furthermore, there seems to be a fair degree of rounding of the data. in\nfact the data collection form was carefully prepared, and the midwives were trained\nin how to compile it, so the data are of high quality. nevertheless it is important\nalways to ask how the data were collected, and if possible to see the process at\nwork.\n\nideally the statistician\nassists in deciding what\ndata are collected, and\nhow.\n\n "}, {"Page_number": 30, "text": "table 2.1 seven\nsuccessive days of times\n(hours) spent by women\ngiving birth in the delivery\nsuite at the john radcliffe\nhospital. (data kindly\nsupplied by ethel burns.)\n\n18\n\n2 \u00b7 variation\n\nwoman\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nday\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n2.10\n3.40\n4.25\n5.60\n6.40\n7.30\n8.50\n8.75\n8.90\n9.50\n9.75\n10.00\n10.40\n10.40\n16.00\n19.00\n\n4.00\n4.10\n5.00\n5.50\n5.70\n6.50\n7.25\n7.30\n7.50\n8.20\n8.50\n9.75\n11.00\n11.20\n15.00\n16.50\n\n1.50\n4.70\n4.70\n7.20\n7.25\n8.10\n8.50\n9.20\n9.50\n10.70\n11.50\n\n2.60\n3.60\n3.60\n6.40\n6.80\n7.50\n7.50\n8.25\n8.50\n10.40\n10.75\n14.25\n14.50\n\n2.50\n2.50\n3.40\n4.20\n5.90\n6.25\n7.30\n7.50\n7.80\n8.30\n8.30\n10.25\n12.90\n14.30\n\n4.00\n4.00\n5.25\n6.10\n6.50\n6.90\n7.00\n8.45\n9.25\n10.10\n10.20\n12.75\n14.60\n\n2.00\n2.70\n2.75\n3.40\n4.20\n4.30\n4.90\n6.25\n7.00\n9.00\n9.25\n10.70\n\nthe average of the n = 95 times in table 2.1 is y = 7.57 hours. the variance of the\ntime spent in the delivery suite can be estimated by the sample variance, s2 = 12.97\nsquared hours. the minimum, median, and maximum are 1.5, 7.5 and 19 hours\nrespectively, and the quartiles are 4.95 and 9.75 hours. the 0.2 and 0.4 trimmed\naverages, 7.48 and 7.55 hours, are similar to y because there are no gross outliers.\n(cid:1)\n\nshape\nthe shape of a sample is also important. for example, the upper tails of annual income\ndistributions are typically very fat, because a few individuals earn enormously more\nthan most of us. the shape of such a distribution can be used to assess inequality, for\nexample by considering the proportion of individuals whose annual income is less\nthan one-half the median. since shape does not depend on location or scale, statistics\nintended to summarize it should be invariant to location and scale shifts of the data.\n\nexample 2.4 (sample skewness) one measure of shape is the standardized sample\nskewness,\n\ng1 =\n\n(cid:6)\n\n(cid:5)\nj=1(y j \u2212 y)3\n\u22121\n(cid:5)\n(n \u2212 1)\u22121\nj=1(y j \u2212 y)2\n\nn\n\nn\n\nn\n\n(cid:7)3/2\n\n.\n\nif the data are perfectly symmetric, g1 = 0, while if they have a heavy upper tail,\ng1 > 0, and conversely. for the times in the delivery suite, g1 = 0.65: the data are\n(cid:1)\nsomewhat skewed to the right.\n\nexample 2.5 (sample shape) measures of shape can also be based on the sample\nquantiles. one is (y((cid:2)0.95n(cid:3)) \u2212 y((cid:2)0.5n(cid:3)))/(y((cid:2)0.5n(cid:3)) \u2212 y((cid:2)0.05n(cid:3))), which takes value one for\na symmetric distribution, and is more resistant to outliers than is the sample skewness.\n\n "}, {"Page_number": 31, "text": "this can lead to\ninter-ocular trauma.\n\n2.1 \u00b7 statistics and sampling variation\n\n19\n\nfor the times in the delivery suite, this is 1.43, again showing skewness to the right.\n(cid:1)\na value less than one would indicate skewness to the left.\n\nit is straightforward to show that both these statistics are invariant to changes in\n\nthe location and scale of y1, . . . , yn.\n\ngraphs\ngraphs are indispensable in data analysis, because the human visual system is so\ngood at recognizing patterns that the unexpected can leap out and hit the investigator\nbetween the eyes. an adverse effect of this ability is that patterns may be imagined\neven when they are absent, so experience, often aided by suitable statistics, is needed\nto interpret a graph. as any plot can be represented numerically, it too is a statistic,\nthough to treat it merely as a set of numbers misses the point.\n\nexample 2.6 (histogram) perhaps the best-known statistical graph is the his-\ntogram, constructed from scalar data by dividing the horizontal axis into disjoint\n(cid:5)\nbins \u2014 the intervals i1, . . . , ik \u2014 and then counting the observations in each. let nk\ndenote the number of observations in ik, for k = 1, . . . , k , so\nk nk = n. ifthe bins\nhave equal width \u03b4, then ik = [l + (k \u2212 1)\u03b4, l + k\u03b4), where l, \u03b4, and k are chosen\nso that all the y j lie between l and l + k \u03b4. wethen plot the proportion nk /n of\nthe data in each bin as a column over it, giving the probability density function for a\ndiscretized version of the data.\nthe upper left panel of figure 2.1 shows this for the birth data in table 2.1, with\nl = 0, \u03b4 = 2, and k = 13; the rug of tickmarks shows the data values themselves.\nas we would expect from examples 2.4 and 2.5, the plot shows a density skewed to\nthe right, with the most popular values in the range 5\u201310 hours. to increase \u03b4 would\ngive fewer, wider, bins, while decreasing \u03b4 would give more, narrower, bins. it might\nbe better to vary the bin width, with narrower bins in the centre of the data, and wider\n(cid:1)\nones at the tails.\n\nexample 2.7 (empirical distribution function) the empirical distribution func-\n\u22121 at each\ntion (edf) is the cumulative probability distribution that puts probability n\nof y1, . . . , yn. this is expressed mathematically as\nh(y \u2212 y j ),\n\n(2.3)\n\n\u22121\n\nn\n\nwhere the distribution function that puts mass one at u = 0, that is,\n\nn(cid:1)\nj=1\n(cid:2)\n\nh(u) =\n\n0,\n1,\n\nu < 0,\nu \u2265 0,\n\n\u22121 at\nis known as the heaviside function. the edf is a step function that jumps by n\neach of the y j ; ofcourse it jumps by more at values that appear in the sample several\ntimes.\n\nthe upper right panel of figure 2.1 shows the edf of the times in the delivery suite.\nit is more detailed than the histogram, but perhaps conveys less information about the\n\n "}, {"Page_number": 32, "text": "20\n\n2\n1\n\n.\n\n0\n\n8\n0\n\n.\n\n0\n\n4\n0\n\n.\n\n0\n\n0\n\n.\n\n0\n\n5\n2\n\n0\n2\n\n5\n1\n\n0\n1\n\n5\n\n0\n\ns\nr\nu\no\nh\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nhours\n\n2 \u00b7 variation\n\nfigure 2.1 summary\nplots for times in the\ndelivery suite, in hours.\nclockwise from top left:\nhistogram, with rug\nshowing values of\nobservations; empirical\ndistribution function;\nscatter plot of daily\naverage hours against\ndaily median hours, for all\n92 days of data, with a\nline of unit slope through\nthe origin; and boxplots\nfor the first seven days.\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nhours\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\nn\no\n\ni\nt\nc\nn\nu\n\nf\n \n\nn\no\n\ni\nt\n\nu\nb\ni\nr\nt\ns\nd\n\ni\n\n \nl\n\na\nc\ni\nr\ni\np\nm\ne\n\ni\n\nn\na\nd\ne\nm\n \ny\n\nl\ni\n\na\nd\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n2\n1\n\n0\n1\n\n8\n\n6\n\n4\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n4\n\n6\n\n8\n\n10\n\n12\n\nday\n\ndaily average\n\nshape of the data. which is preferable is partly a matter of taste, and depends on the\n(cid:1)\nuse to which they will be put.\nexample 2.8 (scatterplot) when an observation has two components, y j =\n(u j , v j ), a scatter plot is a plot of the v j on the vertical axis against the u j on the\nhorizontal axis. an example is given in the lower right panel of figure 2.1, which\nshows the median daily time in the delivery suite plotted against the average daily\ntime, for the full 92 days for which data are available. as most points lie below the\nline with unit slope, and as the slope of the point cloud is slightly greater than one,\nthe medians are generally smaller and somewhat more variable than the averages. the\naverage and sample variance of the medians are 7.03 hours and 2.15 hours squared;\n(cid:1)\nthe corresponding figures for the averages are 7.90 and 1.54.\n\nexample 2.9 (boxplot) boxplots are usually used to compare related sets of data.\nan illustration is in the lower left panel of figure 2.1, which compares the hours in\nthe delivery suite for the seven different days in table 2.1. for each day, the ends\nof the central box show the quartiles and the white line in its centre represents the\n\n "}, {"Page_number": 33, "text": "2.1 \u00b7 statistics and sampling variation\n\n21\n\ndaily median: thus about one-half of the data lie in the box, and its length shows the\ninterquartile range iqr for that day. the bracket above the box shows the largest\nobservation less than or equal to the upper quartile plus 1.5iqr. likewise the bracket\nbelow shows the smallest observation greater than or equal to the lower quartile minus\n1.5iqr. values outside the brackets are plotted individually. the aim is to give a good\nidea of the location, scale, and shape of the data, and to show potential outliers clearly,\nin order to facilitate comparison of related samples. here, for example, we see that\nthe daily median varies from 5\u201310 hours, and that the daily iqr is fairly stable. (cid:1)\n\nit takes thought to make good graphs. some points to bear in mind are:\n\nr the data should be made to stand out, in particular by avoiding so-called\n\nchart-junk \u2014 unnecessary labels, lines, shading, symbols and so forth;\n\nr the axis labels and caption should make the graph as self-explanatory as possi-\nble, in particular containing the names and units of measurement of variables;\nr comparison of related quantities should be made easy, for example by using\n\nidentical scales of measurement, and placing plots side by side;\n\nr scales should be chosen so that the most important systematic relations between\n\nvariables are at about 45\n\nto the axes;\n\n\u25e6\n\nperception experiments\nhave shown that the eye is\nbest at judging departures\nfrom 45\n\n\u25e6\n\n.\n\nr the aspect ratio \u2014 the ratio of the height of a plot to its width \u2014 can be varied\n\nto highlight different features of the data;\n\nr graphs should be laid out so that departures from \u2018standard\u2019 appear as departures\n\nfrom linearity or from random scatter; and\n\nr major differences in the precision of points should be indicated, at least roughly.\n\nnowadays it is easy to produce graphs, but unfortunately even easier to produce bad\nones: there is no substitute for drafting and redrafting each graph to make it as clear\nand informative as possible.\n\n2.1.2 random sample\nso far we have supposed that the sample y1, . . . , yn is of interest for its own sake. in\npractice, however, data are usually used to make inferences about the system from\nwhich they came. one reason for gathering the birth data, for example, was to assess\nhow the delivery suite should be staffed, a task that involves predicting the patterns\nwith which women will arrive to give birth, and how long they are likely to stay in\nthe delivery suite once they are there. though it is not useful to do this for births that\nhave already occurred, the data available can help in making predictions, provided\nwe can forge a link between the past and future. this is one use of a statistical model.\nthe fundamental idea of statistical modelling is to treat data as the observed values\nof random variables. the most basic model is that the data y1, . . . , yn available are\nthe observed values of a random sample of size n, defined to be a collection of n\nindependent identically distributed random variables, y1, . . . ,y n. wesuppose that\neach of the y j has the same cumulative distribution function, f, which represents\nthe population from which the sample has been taken. if f were known, we could in\n\nor sometimes a simple\nrandom sample.\n\n "}, {"Page_number": 34, "text": "22\n\n2 \u00b7 variation\n\nprinciple use the rules of probability calculus to deduce any of its properties \u2014 such\nas its mean and variance, or the probability distribution for a future observation \u2014 and\nany difficulties would be purely computational. in practice, however, f is unknown,\nand we must try to infer its properties from the data. often the quantity of central\ninterest is a nonrandom function of f, such as its mean or its p quantile,\n\u22121( p) = inf{y : f(y) \u2265 p};\n\ne(y ) =\n\ny d f(y),\n\nyp = f\n\n(2.4)\n\n(cid:8)\n\nthese are the population analogues of the sample average and quantiles defined in\n\u22121 and the infimum is\nexamples 2.1 and 2.2. often there is a simple form for f\n4 ) \u2212\nunnecessary. other population quantities such as the interquartile range, f\n\u22121( 1\nf\nexample 2.10 (laplace distribution) a random variable y for which\n\n4 ), are defined similarly.\n\n\u22121( 3\n\nf (y; \u03b7, \u03c4 ) = 1\n2\u03c4\n\nexp (\u2212|y \u2212 \u03b7|/\u03c4 ) , \u2212\u221e < y < \u221e, \u2212\u221e < \u03b7 <\u221e, \u03c4 > 0,\n\n(2.5)\nis said to have the laplace distribution. as f (\u03b7 + u; \u03b7, \u03c4 ) = f (\u03b7 \u2212 u; \u03b7, \u03c4 ) for any\nu, the density is symmetric about \u03b7. its integral is clearly finite, so e(y ) = \u03b7, and\nevidently its median y0.5 = \u03b7 also. its variance is\n\n(y \u2212 \u03b7)2 exp (\u2212|y \u2212 \u03b7|/\u03c4 ) dy = \u03c4 2\n\n\u2212u du = 2\u03c4 2,\n\nu2e\n\n(cid:8) \u221e\n\n\u2212\u221e\n\nvar(y ) = 1\n2\u03c4\n\nas follows after the substitution u = (y \u2212 \u03b7)/\u03c4 and integration by parts; see\nexercise 2.1.3. integration of (2.5) gives\n\n(cid:8) \u221e\n\n0\n\nso\n\nf\n\n(cid:2)\n\nf(y) =\n\n\u22121( p) =\n(cid:9)\n\n1\n\n2 exp{(y \u2212 \u03b7)/\u03c4} ,\n1 \u2212 1\n(cid:2)\n\n2 exp{\u2212(y \u2212 \u03b7)/\u03c4} ,\n\u03b7 + \u03c4 log(2 p),\n\u03b7 \u2212 \u03c4 log{2(1 \u2212 p)},\n\ny \u2264 \u03b7,\ny > \u03b7,\n\np < 1\n2 ,\np \u2265 1\n2 ,\n\n(cid:9)\n\n(cid:10)\n\nthe interquartile range is\n\u2212 f\n\n\u22121\n\nf\n\n\u22121\n\n3\n4\n\n1\n4\n\n(cid:10)\n\n= \u03b7 + \u03c4 log 2 \u2212 (\u03b7 \u2212 \u03c4 log 2) = 2\u03c4 log 2,\n\nwe use d f(y) to\naccommodate the\npossibility that f is\ndiscrete. if it bothers you,\ntake d f(y) = f (y) dy.\n\npierre-simon laplace\n(1749\u20131827) helped\nestablish the metric\nsystem during the french\nrevolution but was\ndismissed by napoleon\n\u2018because he brought the\nspirit of the infinitely\nsmall into the\ngovernment\u2019 \u2014\npresumably bonaparte\nwas unimpressed by\ndifferentiation. laplace\nworked on celestial\nmechanics, published an\nimportant book on\nprobability, and derived\nthe least squares rule.\n\nquantities such as e(y ), var(y ) and f\n\nand the median absolute deviation is \u03c4 log 2 (exercise 2.1.5).\n\n(cid:1)\n\u22121( p) are called parameters, and as their\nvalues depend on f, they are typically unknown. if f is determined by a finite\nnumber of parameters, \u03b8, the model is parametric, and we may write f = f(y; \u03b8), we use the term\nwith corresponding probability density function f (y; \u03b8). ignorance about f then boils\ndown to uncertainty about \u03b8.\n\nit is natural to use sample quantities for inference about model parameters. suppose\nthat the data y1, . . . ,y n are a random sample from a distribution f, that we are\ninterested in a parameter \u03b8 that depends on f, and that we wish to use the statistic\n\nprobability density\nfunction to mean the\ndensity function for a\ncontinuous variable, and\nthe mass function for a\ndiscrete variable, and use\nthe notation f (y; \u03b8) in\nboth cases.\n\n "}, {"Page_number": 35, "text": "figure 2.2 comparisons\nof 92 days of delivery\nsuite data with poisson\nand gamma models. the\nleft panel shows a\nhistogram of the numbers\nof arrivals per day, with\nthe pdf of the poisson\ndistribution with mean\n\u03b8 = 12.9 overlaid. the\nright panel shows a\nhistogram of the hours in\nthe delivery suite for the\n1187 births, with the pdfs\nof gamma distributions\noverlaid. the gamma\ndistributions all have\nmean \u03ba/\u03bb = 7.93 hours.\ntheir shape parameters\nare \u03ba = 3.15 (solid), 0.8\n(dots), 1 (small dashes),\nand 5 (large dashes).\n\nsim\u00b4eon denis poisson\n(1781\u20131840) learned\nmathematics in paris from\nlaplace and lagrange. he\ndid major work on definite\nintegrals, on fourier\nseries, on elasticity and\nmagnetism, and in 1837\npublished an important\nbook on probability.\n\n\u0001(\u03ba) is thegamma\nfunction; see\nexercise 2.1.3 for some of\nits properties.\n\n2.1 \u00b7 statistics and sampling variation\n\n23\n\ny\nt\ni\ns\nn\ne\nd\n\n \ny\nt\ni\nl\ni\n\nb\na\nb\no\nr\np\n\n5\n1\n\n.\n\n0\n\n0\n1\n\n.\n\n0\n\n5\n0\n\n.\n\n0\n\n0\n\n.\n\n0\n\ny\nt\ni\ns\nn\ne\nd\n\n \ny\nt\ni\nl\ni\n\nb\na\nb\no\nr\np\n\n5\n1\n\n.\n\n0\n\n0\n1\n\n.\n\n0\n\n5\n0\n\n.\n\n0\n\n0\n\n.\n\n0\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n0\n\n10\n\n20\n\n30\n\narrivals/day\n\nhours\n\ns = s(y1, . . . ,y n) tomake inferences about \u03b8, for example hoping that s will be close\nto \u03b8. then we call s an estimator of \u03b8 and say that the particular value that s takes\nwhen the observed data are y1, . . . , yn, that is, s = s(y1, . . . , yn), is an estimate of \u03b8.\nthis is the usual distinction between a random variable and the value that it takes,\nhere s and s.\n\nexample 2.11 (poisson distribution) the poisson distribution with mean \u03b8 has\nprobability density function\n\npr(y = y) = f (y; \u03b8) = \u03b8 y\ny!\n\n\u2212\u03b8 ,\n\ne\n\ny = 0, 1, 2, . . . ,\n\n\u03b8 > 0.\n\n(2.6)\n\nthis discrete distribution is used for count data. for example, the left panel of\nfigure 2.2 shows a histogram of the number of women arriving at the delivery suite for\neach of the 92 days of data, together with the probability density function (2.6) with\n\u03b8 = 12.9, equal to the average number of arrivals over the 92 days. this distribution\n(cid:1)\nseems to fit the data more or less adequately.\n\nexample 2.12 (gamma distribution) the gamma distribution with scale parameter\n\u03bb and shape parameter \u03ba has probability density function\n\nf (y; \u03bb, \u03ba) = \u03bb\u03ba y\u03ba\u22121\n\n\u0001(\u03ba)\n\nexp(\u2212\u03bby),\n\ny > 0,\n\n\u03bb, \u03ba > 0.\n\n(2.7)\n\nthis distribution has mean \u03ba/\u03bb and variance \u03ba/\u03bb2.\n\nwhen \u03ba = 1 the density is exponential, for 0 < \u03ba <1 it is l-shaped, and for \u03ba > 1\nit falls smoothly on either side of its maximum. these shapes are illustrated in the right\npanel of figure 2.2, which shows the hours in the delivery suite for the 1187 births\nthat took place over the three months of data. in each case the mean of the density\nmatches the data average of 7.93 hours; the value \u03ba = 3.15 of the shape parameter\nwas chosen to match the variance of the data by solving simultaneously the equa-\ntions \u03ba/\u03bb = 7.93, \u03ba/\u03bb2 = 12.97. evidently the solid curve gives the best fit of those\nshown.\n\n "}, {"Page_number": 36, "text": "24\n\n2 \u00b7 variation\n\nit is important to appreciate that the parametrization of f is not carved in stone.\nhere it might be better to rewrite (2.7) in terms of its mean \u00b5 = \u03ba/\u03bb and the shape\nparameter \u03ba, inwhich case the density is expressed as\n\ny\u03ba\u22121 exp(\u2212\u03bay/\u00b5),\n\ny > 0, \u00b5, \u03ba > 0,\n\n(2.8)\n\n(cid:9)\n\n(cid:10)\u03ba\n\n1\n\n\u0001(\u03ba)\n\n\u03ba\n\n\u00b5\n\nwith variance \u00b52/\u03ba. asfunctions of y the shapes of (2.7) and (2.8) are the same,\nbut their expression in terms of parameters is not. the range of possible densities is\nthe same for any 1\u20131 reparametrization of (\u03ba, \u03bb), so one might write the density in\nterms of two important quantiles, for example, if this made sense in the context of a\nparticular application. the central issue in choice of parametrization is directness of\n(cid:1)\ninterpretation in the situation at hand.\n\nexample 2.13 (laplace distribution) to express the laplace density (2.5) in terms\nof its mean and variance \u03b7 and 2\u03c4 2, we set\u03c4 2 = \u03c3 2/2, giving\n\n\u221a\n2|y \u2212 \u03b7|/\u03c3 ) \u2212 \u221e < y < \u221e, \u2212\u221e < \u03b7 <\u221e, \u03c3 > 0.\n\nexp(\u2212\n\n1\u221a\n2\u03c3\n\nits shape as a function of y is unchanged, but the new formula is uglier.\n\n(cid:1)\n\n2.1.3 sampling variation\nif the data y1, . . . , yn are regarded as the observed values of random variables, then it\nfollows that the sample and any statistics derived from it might have been different.\nhowever, although we would expect variation over possible sets of data, we would\nalso expect to see systematic patterns induced by the underlying model. for instance,\nhaving inspected the lower left panel of figure 2.1, we would be surprised to be told\nthat the median hours in the delivery suite on day 8 was 15 hours, though any value\nbetween 5 and 10 hours would seem quite reasonable. from a statistical viewpoint,\ndata have both a random and a systematic component, and one common goal of data\nanalysis is to disentangle these as far as possible. in order to understand the systematic\naspect, it makes sense to ask how we would expect a statistic s(y1, . . . , yn) tobehave\non average, that is, to try and understand the properties of the corresponding random\nvariable, s = s(y1, . . . ,y n).\nexample 2.14 (sample moments) suppose that y1, . . . ,y n is a random sample\nfrom a distribution with mean \u00b5 and variance \u03c3 2. then the average y has expectation\nand variance\n\ne(y ) = e\n\nvar(y ) = var\n\n(cid:11)\n\n1\nn\n\n(cid:11)\n\ny j\n\nn(cid:1)\nj=1\nn(cid:1)\nj=1\n\n1\nn\n\n(cid:12)\n\n= n\n(cid:12)\nn\n\ny j\n\n= 1\nn2\n\ne(y j ) = \u00b5,\n\nn(cid:1)\nj=1\n\nvar(y j ) = \u03c3 2\nn\n\n,\n\n "}, {"Page_number": 37, "text": "2.1 \u00b7 statistics and sampling variation\n\n25\n\nbecause the y j are independent identically distributed random variables. thus the\nexpected value of the random variable y is the population mean \u00b5.\nj (y j \u2212 y )2, note\n\nto find the expectation of the sample variance s2 = (n \u2212 1)\n\n(cid:5)\n\n\u22121\n\nthat\n\nn(cid:1)\nj=1\n\nj=1\n\n(y j \u2212 y )2 = n(cid:1)\n= n(cid:1)\n= n(cid:1)\n= n(cid:1)\n\nj=1\n\nj=1\n\nj=1\n\n{y j \u2212 \u00b5 \u2212 (y \u2212 \u00b5)}2\n\nn(cid:1)\nj=1\n\n(y j \u2212 \u00b5)2 \u2212 2\n\n(y j \u2212 \u00b5)(y \u2212 \u00b5) + n(cid:1)\n(y j \u2212 \u00b5)2 \u2212 2n(y \u2212 \u00b5)2 + n(y \u2212 \u00b5)2\n\nj=1\n\n(y \u2212 \u00b5)2\n\n(y j \u2212 \u00b5)2 \u2212 n(y \u2212 \u00b5)2.\n\nas\n\ne{(n \u2212 1)s2} =n e{(y j \u2212 \u00b5)2} \u2212n e{(y \u2212 \u00b5)2}\n\n= n\u03c3 2 \u2212 n\u03c3 2/n\n= (n \u2212 1)\u03c3 2,\n\nwe see that s2 has expected value \u03c3 2. this explains our use of the denominator n \u2212 1\nwhen defining the sample variance s2 in (2.1): the expectation of the corresponding\nrandom variable equals the population variance.\nthe birth data of table 2.1 have n = 95, and the realized values of the random\nvariables y and s2 are y = 7.57 and s2 = 12.97. thus y has estimated variance\ns2/n = 12.97/95 = 0.137 and estimated standard deviation 0.1371/2 = 0.37. this\nsuggests that the underlying \u2018true\u2019 mean \u00b5 of the population of times spent in the\n(cid:1)\ndelivery suite by women giving birth is close to 7.6 hours.\n\nexample 2.15 (birth data) figure 2.2 suggests the following simple model for the\nbirth data. each day the number n of women arriving to give birth is poisson with\nmean \u03b8. the jth of these women spends a time y j in the delivery suite, where y j is a\ngamma random variable with mean \u00b5 and variance \u03c3 2. the values of these parameters\n.= 8 hours and \u03c3 2 .= 13 hours squared. the average time and median\nare \u03b8\ntimes spent, y = n\ny j and m, vary from day to day, with the lower right panel\nof figure 2.1 suggesting that e(m) < e(y ) and var(m) > var(y ), properties we shall\n(cid:1)\nsee theoretically in example 2.30.\n\n.= 13, \u00b5\n\n(cid:5)\n\n\u22121\n\nmuch of this book is implicitly or explicitly concerned with distinguishing random\nand systematic variation. the notions of sampling variation and of a random sample\nare central, and before continuing we describe a useful tool for comparison of data\nand a distribution.\n\n "}, {"Page_number": 38, "text": "26\n\n2 \u00b7 variation\n\n2.1.4 probability plots\nit is often useful to be able to check graphically whether data y1, . . . , yn come from a\nparticular distribution. suppose that in addition to the data we had a random sample\nx1, . . . , xn known to be from f. inorder to compare the shapes of the samples, we\ncould sort them to get y(1) \u2264 \u00b7\u00b7\u00b7 \u2264 y(n) and x(1) \u2264 \u00b7\u00b7\u00b7 \u2264 x(n), and make a quantile-\nquantile or q-q plot of y(1) against x(1), y(2) against x(2), and so forth. a straight\nline would mean that y( j) = a + bx( j), sothat the shape of the samples was identical,\nwhile distinct curvature would indicate systematic differences between them. if the\nline was close to straight, we could be fairly confident that y1, . . . , yn looks like a\nsample from f \u2014 after all, it would have a shape similar to the sample x1, . . . , xn\nwhich is from f.\n\nquantile-quantile plots are helpful for comparison of two samples, but when com-\nparing a single sample with a theoretical distribution it is preferable to use f di-\nrectly in a probability plot, inwhich the y( j) are graphed against the plotting posi-\n\u22121{ j/(n + 1)}. this use of the j/(n + 1) quantile of f is justified in sec-\ntions f\ntion 2.3 as an approximation to e(x( j)), where x( j) is the random variable of which\nx( j) is a particular value. for example, the jth plotting positions for the normal\nand exponential distributions \u0001{(x \u2212 \u00b5)/\u03c3} and 1 \u2212 e\n\u2212\u03bbx are \u00b5 + \u03c3 \u0001\u22121{ j/(n + 1)}\nand \u2212\u03bb\u22121 log{1 \u2212 j/(n + 1)}. when parameters such as \u00b5, \u03c3 , and \u03bb are unknown,\nthe plotting positions used are for standardized distributions, here \u0001\u22121{ j/(n + 1)}\nand \u2212 log{1 \u2212 j/(n + 1)}, which are sometimes called normal scores and expo-\nnential scores. probability plots for the normal distribution are particularly com-\nmon in applications and are also called normal scores plots. the interpretation of\na probability plot is aided by adding the straight line that corresponds to perfect\nfit of f.\n\nexample 2.16 (birth data) the top left panel of figure 2.3 shows a probability\nplot to compare the 95 times in the delivery suite with the normal distribution. the\ndistribution does not fit the largest and smallest observations, and the data show\nsome upward curvature relative to the straight line. the top right panel shows that\nthe exponential distribution would fit the data very poorly. the bottom left panel,\na probability plot of the log y j against normal plotting positions, corresponding to\nchecking the log-normal distribution, shows slight downward curvature. the bottom\nright panel, a probability plot of the y j against plotting positions for the gamma\ndistribution with mean y and variance s2, shows the best fit overall, though it is not\nperfect.\n\nin the normal and gamma plots the dotted line corresponds to the theoretical dis-\ntribution whose mean equals y and whose variance equals s2; the dotted line in the\nexponential plot is for the exponential distribution whose mean equals y; and the dot-\nted line in the log-normal plot is for the normal distribution whose mean and variance\n(cid:1)\nequal the average and variance of the log y j .\n\nsome experience with interpreting probability plots may be gained from\n\npractical 2.3.\n\n "}, {"Page_number": 39, "text": "2.1 \u00b7 statistics and sampling variation\n\n27\n\n\u2022\n\n\u2022\n\n\u2022\n\nfigure 2.3 probability\nplots for hours in the\ndelivery suite, for the\nnormal, exponential,\ngamma, and log-normal\ndistributions (clockwise\nfrom top left). in each\npanel the dotted line is for\na fitted distribution whose\nmean and variance match\nthose of the data. none of\nthe fits is perfect, but the\ngamma distribution fits\nbest, and the exponential\nworst.\n\n0\n2\n\n5\n1\n\n0\n1\n\n5\n\n0\n\n0\n\n.\n\n3\n\n5\n\n.\n\n2\n\n0\n\n.\n\n2\n\n5\n\n.\n\n1\n\n0\n\n.\n\n1\n\n5\n0\n\n.\n\n0\n\n.\n\n0\n\ns\nr\nu\no\nh\n\ns\nr\nu\no\nh\n\n \n\ng\no\nl\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022 \u2022 \u2022\n\u2022\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022 \u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\ns\nr\nu\no\nh\n\n0\n2\n\n5\n1\n\n0\n1\n\n5\n\n0\n\n\u2022\u2022\u2022 \u2022 \u2022\n\u2022\u2022\n\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\n0\n\n1\n\n2\n\n3\n\n4\n\nstandard normal plotting positions\n\nstandard exponential plotting positions\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022 \u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\ns\nr\nu\no\nh\n\n0\n2\n\n5\n1\n\n0\n1\n\n5\n\n0\n\n\u2022\n\n\u2022 \u2022\n\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n12\n\nstandard normal plotting positions\n\ngamma plotting positions\n\nexercises 2.1\n1\n\nlet m and s be the values of location and scale statistics calculated from y1, . . . , yn; m and\ns may be any of the quantities described in examples 2.1 and 2.2. show that the effect of\nthe mapping y1, . . . , yn (cid:4)\u2192 a + by1, . . . , a + byn b > 0, is to send m, s (cid:4)\u2192 a + bm, bs.\nshow also that the measures of shape in examples 2.4 and 2.5 are unchanged by this\ntransformation.\n(a) show that when \u03b4 is added to one of y1, . . . , yn and |\u03b4| \u2192 \u221e, the average y changes\nby an arbitrarily large amount, but the sample median does not. by considering such\nperturbations when n is large, deduce that the sample median has breakdown point 0.5.\n(b) find the breakdown points of the other statistics in examples 2.1 and 2.2.\n(a) if \u03ba > 0 isreal and k a positive integer, show that the gamma function\n\n(cid:8) \u221e\n\n\u0001(\u03ba) =\n\nu\u03ba\u22121e\n\n\u2212u du,\n\n0\n\n2 ) = \u03c0 1/2, but you need not prove this.\n\nhas properties \u0001(1) = 1, \u0001(\u03ba + 1) = \u03ba\u0001(\u03ba) and \u0001(k) = (k \u2212 1)!. it is useful to know that\n\u0001( 1\n(b) use (a) to verify the mean and variance of (2.7).\n(c) show that for 0 < \u03ba \u2264 1 the maximum value of (2.7) is at y = 0, and find its mode\nwhen \u03ba > 1.\n\na sketch may help.\n\n2\n\n3\n\nthe mode of a density f\nis a value y such that\nf (y) \u2265 f (x) for all x.\n\n "}, {"Page_number": 40, "text": "28\n\n4\n\n5\n\n6\n\n7\n\n8\n\n2 \u00b7 variation\n\ngive formulae analogous to (2.4) for the variance, skewness and \u2018shape\u2019 of a distribution\nf. dothey behave sensibly when a variable y with distribution f is transformed to\na + by , so f(y) isreplaced by f{(y \u2212 a)/b}?\nlet y have continuous distribution function f. forany \u03b7, show that x = |y \u2212 \u03b7| has\ndistribution g(x) = f(\u03b7 + x) \u2212 f(\u03b7 \u2212 x), x > 0. hence give a definition of the median\n\u22121. ifthe density of y is symmetric about\nabsolute deviation of f in terms of f\nthe origin, show that g(x) = 2f(x) \u2212 1. hence find the median absolute deviation of the\nlaplace density (2.5).\na probability plot in which y1, . . . , yn and x1, . . . , xn are two random samples is called a\nquantile-quantile or q-q plot. construct this plot for the first two columns in table 2.1.\nare the samples the same shape?\nthe stem-and-leaf display for the data 2.1, 2.3, 4.5, 3.3, 3.7, 1.2 is\n\n\u22121 and g\n\n1 | 2\n2 | 13\n3 | 37\n4 | 5\n\nif you turn the page on its side this gives a histogram showing the data values themselves\n(perhaps rounded); the units corresponding to intervals [1, 2), [2, 3) and so forth are to\nthe left of the vertical bars, and the digits are to the right. construct this for the combined\ndata for days 1\u20133 in table 2.1. hence find their median, quartiles, interquartile range, and\nrange.\ndo figures 2.1\u20132.3 follow the advice given on page 21? if not, how could they be im-\nproved? browse some textbooks and newspapers and think critically about any statistical\ngraphics you find.\n\n2.2 convergence\n2.2.1 modes of convergence\nintuition tells us that the bigger our sample, the more faith we can have in our\ninferences, because our sample is more representative of the distribution f from\nwhich it came \u2014 if the sample size n was infinite, we would effectively know f. as\nn \u2192 \u221e we can think of our sample y1, . . . ,y n as converging to f, and of a statistic\ns = s(y1, . . . ,y n) asconverging to a limit that depends on f. for our purposes there\nare two main ways in which a sequence of random variables, s1, s2, . . ., can converge\nto another random variable s.\n\nconvergence in probability\n\nwe say that sn converges in probability to s, sn\n\npr(|sn \u2212 s| > \u03b5) \u2192 0\n\np\u2212\u2192 s, if forany \u03b5 > 0\nasn \u2192 \u221e.\n\n(2.9)\n\na special case of this is the weak law of large numbers, whose simplest form is that\nif y1, y2, . . . is a sequence of independent identically distributed random variables\neach with finite mean \u00b5, and if y = n\n\u22121(y1 + \u00b7\u00b7\u00b7 + yn) isthe average of y1, . . . ,y n,\nthen y p\u2212\u2192 \u00b5. wesometimes call this simply the weak law. it isillustrated in the\nleft-hand panels of figure 2.4, which show histograms of 10,000 averages of random\nsamples of n exponential random variables, with n = 1, 5, 10, and 20. the individual\n\n "}, {"Page_number": 41, "text": "figure 2.4 convergence\nin probability and in\ndistribution. the left\npanels show how\nhistograms of the averages\ny of 10,000 samples of n\nstandard exponential\nrandom variables become\nmore concentrated at the\nmean \u00b5 = 1 asn\nincreases through 1, 5, 10,\nand 20, due to the\nconvergence in probability\nof y to \u00b5. the right panels\nshow how the distribution\nof zn = n1/2(y \u2212 1)\napproaches the standard\nnormal distribution, due to\nthe convergence in\ndistribution of zn to\nnormality.\n\n2.2 \u00b7 convergence\n\n29\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n2\n\ny\nt\ni\ns\nn\ne\nd\n\n1\n\n0\n\n2\n\ny\nt\ni\ns\nn\ne\nd\n\n1\n\n0\n\n2\n\ny\nt\ni\ns\nn\ne\nd\n\n1\n\n0\n\n2\n\ny\nt\ni\ns\nn\ne\nd\n\n1\n\n0\n\nn=1\n\n2\n\ny\n\nn=5\n\n2\n\ny\n\nn=10\n\n2\n\ny\n\nn=20\n\n0\n\n.\n\n1\n\ny\nt\ni\ns\nn\ne\nd\n\n5\n\n.\n\n0\n\n0\n\n.\n\n0\n\n3\n\n4\n\n-3\n\n-2\n\n-1\n\n0\n\n.\n\n1\n\ny\nt\ni\ns\nn\ne\nd\n\n5\n\n.\n\n0\n\n0\n\n.\n\n0\n\n3\n\n4\n\n-3\n\n-2\n\n-1\n\n0\n\n.\n\n1\n\ny\nt\ni\ns\nn\ne\nd\n\n5\n\n.\n\n0\n\n0\n\n.\n\n0\n\n3\n\n4\n\n-3\n\n-2\n\n-1\n\nn=1\n\n0\n\nz\n\nn=5\n\n0\n\nz\n\nn=10\n\n0\n\nz\n\nn=20\n\n1\n\n2\n\n3\n\n1\n\n2\n\n3\n\n1\n\n2\n\n3\n\n0\n\n.\n\n1\n\ny\nt\ni\ns\nn\ne\nd\n\n5\n\n.\n\n0\n\n0\n\n.\n\n0\n\n0\n\n1\n\n2\n\ny\n\n3\n\n4\n\n-3\n\n-2\n\n-1\n\n1\n\n2\n\n3\n\n0\n\nz\n\n\u2212y for y > 0, so their mean \u00b5 and variance \u03c3 2 both equal one.\nvariables have density e\nas n increases, the values of sn = y become increasingly concentrated around \u00b5, so\nas the figure illustrates, pr(|sn \u2212 \u00b5| > \u03b5) decreases for each positive \u03b5.\nstatistics that converge in probability have some useful properties. for example, if\np\u2212\u2192 s0, itfollows that\n\ns0 is a constant, and h is a function continuous at s0, then if sn\nh(sn)\n\np\u2212\u2192 h(s0) (exercise 2.2.1).\n\n "}, {"Page_number": 42, "text": "jacob bernoulli\n(1654\u20131705) was a\nmember of a mathematical\nfamily split by rivalry. his\nmajor work on probability,\nars conjectandi, was\npublished in 1713, but he\nalso worked on many\nother areas of\nmathematics.\n\n30\n\nan estimator sn of a parameter \u03b8 is consistent if sn\n\n2 \u00b7 variation\np\u2212\u2192 \u03b8 as n \u2192 \u221e, whatever the\nvalue of \u03b8. consistency is desirable, but a consistent estimator that has poor properties\nfor any realistic sample size will be useless in practice.\n\nexample 2.17 (binomial distribution) a binomial random variable r = (cid:5)\n\nm\nj=1 i j\ncounts the numbers of ones in the random sample i1, . . . , im, each of which has a\nbernoulli distribution,\n\npr(i j = 1) = \u03c0, pr(i j = 0) = 1 \u2212 \u03c0,\n\n0 \u2264 \u03c0 \u2264 1.\n\nit is easy to check that e(i j ) = \u03c0 and var(i j ) = \u03c0(1 \u2212 \u03c0). thus the weak law applies\nto the proportion of successes (cid:13)\u03c0 = r/m, giving (cid:13)\u03c0 p\u2212\u2192 \u03c0 as m \u2192 \u221e. evidently (cid:13)\u03c0\nis a consistent estimator of \u03c0. however, the useless estimator(cid:13)\u03c0 + 106/ log m is also\neach of the i j has variance \u03c0(1 \u2212 \u03c0), and this is estimated by(cid:13)\u03c0(1 \u2212(cid:13)\u03c0), a contin-\nuous function of(cid:13)\u03c0 that converges in probability to \u03c0(1 \u2212 \u03c0).\n\nconsistent \u2014 consistency is a minimal requirement, not a guarantee that the estimator\ncan safely be used in practice.\n\n(cid:1)\n\nasn \u2192 \u221e\n\npr(zn \u2264 z) \u2192 pr(z \u2264 z)\n\nconvergence in distribution\nwe say that the sequence z1, z2, . . ., converges in distribution to z, zn\n\nd\u2212\u2192 z, if\n(2.10)\nat every z for which the distribution function pr(z \u2264 z) iscontinuous. the most\nimportant case of this is the central limit theorem, whose simplest version applies\nto a sequence of independent identically distributed random variables y1, y2, . . .,\nwith finite mean \u00b5 and finite variance \u03c3 2 > 0. if the sample average is y =\n\u22121(y1 + \u00b7\u00b7\u00b7 + yn), the central limit theorem states that\nn\nd\u2212\u2192 z ,\n\nzn = n1/2 (y \u2212 \u00b5)\n\n(2.11)\n\n\u03c3\n\nwhere z is a standard normal random variable, that is, one having the normal distri-\nbution with mean zero and variance one, written n (0, 1); see section 3.2.1.\n\nthe right panels of figure 2.4 illustrate such convergence. they show histograms\nof zn for the averages in the left-hand panels, with the standard normal probability\ndensity function superimposed. each of the right-hand panels is a translation to zero\nof the histogram to its left, followed by \u2018zooming in\u2019: multiplication by a scale factor\nn1/2/\u03c3 . asn increases, zn approaches its limiting standard normal distribution.\n\nexample 2.18 (average) consider the average y of a random sample with mean \u00b5\nand finite variance \u03c3 2 > 0. the weak law implies that y is a consistent estimator\nof its expected value \u00b5, and (2.11) implies that in addition y = \u00b5 + n\n\u22121/2\u03c3 zn,\nd\u2212\u2192 z. this supports our intuition that y is a better estimate of \u00b5 for\nwhere zn\nlarge n, and makes explicit the rate at which y converges to \u00b5: inlarge samples y is\n(cid:1)\nessentially a normal variable with mean \u00b5 and variance \u03c3 2/n.\n\nexample 2.19 (empirical distribution function) let y1, . . . ,y n be a random sam-\nple from f, and let i j (y) bethe indicator random variable for the event y j \u2264 y. thus\n\n "}, {"Page_number": 43, "text": "31\n\n2.2 \u00b7 convergence\ni j (y) equals one if y j \u2264 y and zero otherwise. the empirical distribution function of\nthe sample is\n\na step function that increases by n\n\n(cid:13)f(y) = n\n\u22121 at each observation, as in the upper right panel\nof figure 2.1. we thought of (2.3) as a summary of the data y1, . . . , yn; (cid:13)f(y) isthe\npr{i j (y) = 1} = f(y). therefore (cid:13)f(y) is an average of independent identically dis-\nthe i j (y) are independent and each has the bernoulli distribution with probability\ntributed variables and has mean f(y) and variance f(y){1 \u2212 f(y)}/n. at a value y\nfor which 0 < f(y) < 1,\n\ncorresponding random variable.\n\nn(cid:1)\nj=1\n\ni j (y),\n\n\u22121\n\n(cid:13)f(y)\n\np\u2212\u2192 f(y),\n\nn1/2\n\n{(cid:13)f(y) \u2212 f(y)}\n[f(y){1 \u2212 f(y)}]1/2\n\nd\u2212\u2192 z , as n \u2192 \u221e,\n\n(2.12)\n\nwhere z is a standard normal variate. it can be shown that this pointwise convergence\n\nfor each y extends to convergence of the function (cid:13)f(y) to f(y). the empirical dis-\n\ntribution function in figure 2.1 is thus an estimate of the true distribution of times in\n(cid:1)\nthe delivery suite.\n\nthe alert reader will have noticed a sleight-of-word in the previous sentence. con-\nvergence results tell us what happens as n \u2192 \u221e, but in practice the sample size is fixed\nmations for finite n \u2014 for example, (2.12) leads us to hope that n1/2{(cid:13)f(y) \u2212 f(y)}/\nand finite. how then are limiting results relevant? they are used to generate approxi-\n[f(y){1 \u2212 f(y)}]1/2 has approximately a standard normal distribution even when n\nis quite small. in practice it is important to check the adequacy of such approxima-\ntions, and to develop a feel for their accuracy. this may be done analytically or by\nsimulation (section 3.3), while numerical examples are also valuable.\n\nslutsky\u2019s lemma\nconvergence in distribution is useful in statistical applications because we generally\nwant to compare probabilities. it is weaker than convergence in probability because\nit does not involve the joint distribution of sn and s. ifs 0 and u0 are constants, these\nmodes of convergence are related as follows:\n\np\u2212\u2192 s \u21d2 sn\nd\u2212\u2192 s0 \u21d2 sn\np\u2212\u2192 u0 \u21d2 sn + un\nthe third of these is known as slutsky\u2019s lemma.\n\nsn\nsn\nd\u2212\u2192 s and un\n\nd\u2212\u2192 s,\np\u2212\u2192 s0,\n\nsn\n\nd\u2212\u2192 s + u0, snun\n\n(2.13)\n\n(2.14)\n\n(2.15)\n\nd\u2212\u2192 su0.\n\nexample 2.20 (sample variance) suppose that y1, . . . ,y n is a random sample of\nvariables with finite mean \u00b5 and variance \u03c3 2. let\n(y j \u2212 y )2 = n\n\u22121\n\nsn = n\n\n\u2212 y 2,\n\n\u22121\n\ny 2\nj\n\nn(cid:1)\nj=1\n\nn(cid:1)\nj=1\n\nevgeny evgenievich\nslutsky (1880\u20131948)\nmade fundamental\ncontributions to stochastic\nconvergence and to\neconomic time series\nduring the 1920s and\n1930s. in 1902 he was\nexpelled from university\nin kiev for political\nactivity. he studied in\nmunich and kiev and\nworked in kiev and\nmoscow.\n\ndevotees of tricky\nanalysis will find\nreferences to proofs of\n(2.13)\u2013(2.15) in\nsection 2.5.\n\n "}, {"Page_number": 44, "text": "(cid:3)\n\n2 \u00b7 variation\n32\nwhere y is the sample average. the weak law implies that y p\u2212\u2192 \u00b5, and the function\nh(x) = x 2 is continuous everywhere, so y 2\n\np\u2212\u2192 \u00b52. moreover\n(cid:4) = var(y j ) + {e(y j )}2 = \u03c3 2 + \u00b52,\np\u2212\u2192 \u03c3 2 + \u00b52 also. now (2.13) implies that n\n\n(cid:5)\ny 2\nd\u2212\u2192 \u03c3 2. but \u03c3 2 is constant, so sn\nj\n\nd\u2212\u2192 \u03c3 2 + \u00b52,\nso n\np\u2212\u2192 \u03c3 2.\nand therefore (2.15) implies that sn\nthe sample variance s2 may be written as sn \u00d7 n/(n \u2212 1), which evidently also\ntends in probability to \u03c3 2. thus not only is it true that for all n, e(s2) = \u03c3 2, butthe\n(cid:1)\ndistribution of s2 is increasingly concentrated at \u03c3 2 in large samples.\n\n(cid:5)\n\ny 2\nj\n\ny 2\nj\n\n\u22121\n\n\u22121\n\ne\n\nthese ideas extend to functions of several random variables.\n\nexample 2.21 (covariance and correlation) the covariance between random vari-\nables x and y is\n\n\u03b3 = e[{x \u2212 e(x)}{y \u2212 e(y )}] = e(xy ) \u2212 e(x)e(y ).\n\nan estimate of \u03b3 based on a random sample of data pairs (x1, y1), . . . ,( xn, yn) is\nthe sample covariance\n\n(cid:12)\n\nc = 1\nn \u2212 1\n\n(x j \u2212 x)(y j \u2212 y ) = n\nn \u2212 1\n\n\u22121\n\nn\n\nx j y j \u2212 xy\n\n,\n\nn(cid:1)\nj=1\n\nwhere x and y are the averages of the x j and y j . provided the moments e(xy ), e(x)\nx j y j , x and y , which\nand e(y ) are finite, the weak law applies to each of n\nconverge in probability to their expectations. the convergence is also in distribution,\nby (2.13), so (2.15) implies that c d\u2212\u2192 \u03b3 . but \u03b3 is constant, so (2.14) implies that\nc p\u2212\u2192 \u03b3 .\n\n\u22121\n\nthe correlation between x and y ,\n\n(cid:11)\n\nn(cid:1)\nj=1\n(cid:5)\n\n\u03c1 = e(xy ) \u2212 e(x)e(y )\n{var(x)var(y )}1/2\n\n,\n\nis such that \u22121 \u2264 \u03c1 \u2264 1. when |\u03c1| =1 there is a linear relation between x and y ,\nso that a + bx + cy = 0 for some nonzero b and c (exercise 2.2.3). values of \u03c1\nclose to \u00b11 indicate strong linear dependence between the distributions of x and y ,\nthough values close to zero do not indicate independence, just lack of a linear relation.\nthe parameter \u03c1 can be estimated from the pairs (x j , y j ) bythe sample correlation also known as the\ncoefficient,\n\nproduct moment\ncorrelation coefficient.\n\nr =\n\n(cid:6)(cid:5)\n\n(cid:5)\ni=1(xi \u2212 x)2\n\nj=1(x j \u2212 x)(y j \u2212 y )\n(cid:5)\nk=1(yk \u2212 y )2\n\nn\n\nn\n\nn\n\n(cid:7)1/2\n\n.\n\nthe keen reader will enjoy showing that r p\u2212\u2192 \u03c1.\n(cid:1)\nexample 2.22 (studentized statistic) suppose that (tn \u2212 \u03b8)/var(tn)1/2 converges\nin distribution to a standard normal random variable, z, and that var(tn) = \u03c4 2/n,\nwhere \u03c4 2 > 0 isunknown but finite. let vn be a statistic that estimates \u03c4 2/n, with the\n\n "}, {"Page_number": 45, "text": "2.2 \u00b7 convergence\n\nproperty that nvn\n\u03c4/(nvn)1/2\n\np\u2212\u2192 1. therefore\n\n33\np\u2212\u2192 \u03c4 2. the function h(x) = \u03c4/(nx)1/2 is continuous at x = 1, so\nzn = n1/2 (tn \u2212 \u03b8)\n\nd\u2212\u2192 z \u00d7 1,\n\n\u00d7\n\n\u03c4\n\n\u03c4\n\n(nvn)1/2\n\nby (2.15). thus zn has a limiting standard normal distribution provided that nvn is a\nconsistent estimator of \u03c4 2.\nthe best-known instance of this is the average of a random sample, y =\n\u22121(y1 + \u00b7\u00b7\u00b7 + yn). if the y j have finite mean \u03b8 and finite positive variance, \u03c3 2,\nn\ny has mean \u03b8 and variance \u03c3 2/n. the central limit theorem states that\n\nn1/2 (y \u2212 \u03b8)\n\nd\u2212\u2192 z .\nconsider zn = n1/2(y \u2212 \u03b8)/s, where s2 = (n \u2212 1)\n\u22121\nd\u2212\u2192 z.\nshows that s2\n\np\u2212\u2192 \u03c3 2, and it follows that zn\n\n\u03c3\n\n(cid:5)\n\n(y j \u2212 y )2. example 2.20\n\nthe replacement of var(tn) by anestimate is called studentization to honour\nw. s. gossett. publishing under the pseudonym \u2018student\u2019 in 1908, he considered\n(cid:1)\nthe effect of replacing \u03c3 by s for normal data; see section 3.2.\n\nintuition suggests that bigger samples always give better estimates, but intuition\n\ncan mislead or fail.\n\nexample 2.23 (cauchy distribution) a cauchy random variable centred at \u03b8 has\ndensity\n\nf (y; \u03b8) =\n\n1\n\n\u03c0{1 + (y \u2212 \u03b8)2} , \u2212\u221e < y < \u221e, \u2212\u221e < \u03b8 <\u221e.\n\n(2.16)\n\nalthough (2.16) is symmetric with mode at \u03b8, none of its moments exist, and in fact\nthe average y of a random sample y1, . . . ,y n of such data has the same distribution\nas a single observation. so if we were unlucky enough to have such a sample, it would\nbe useless to estimate \u03b8 by y : wemight as well use y1. the difficulty is that the tails\nof the cauchy density decrease very slowly. data with similar characteristics arise in\nmany financial and insurance contexts, so this is not a purely mathematical issue: the\n(cid:1)\naverage may be a poor estimate, and better ones are discussed later.\n\n2.2.2 delta method\nvariances and variance estimates are often required for smooth functions of random\nvariables. suppose that the quantity of interest is h(tn), and\n\n(tn \u2212 \u00b5)/var(tn)1/2 d\u2212\u2192 z ,\n\nnvar(tn)\n\np\u2212\u2192 \u03c4 2 > 0,\n\nas n \u2192 \u221e, and z has the standard normal distribution. then we may write tn =\n\u00b5 + n\nat \u00b5,\ntaylor series expansion gives\n\nd\u2212\u2192 z. if h has a continuous non-zero derivative h\n(cid:4)\n\n\u22121/2\u03c4 zn, where zn\n\n(cid:3)\n\n(cid:11)\n\nh(tn) = h\n\n\u00b5 + n\n\n\u22121/2\u03c4 zn\n\n\u22121/2\u03c4 znh\n\n\u00b5 + n\n\n\u22121/2\u03c4 wn\n\n,\n\n(cid:4) = h(\u00b5) + n\n\n(cid:11)(cid:3)\n\nwilliam sealy gossett\n(1876\u20131937) worked at\nthe guinness brewery in\ndublin. apart from his\ncontributions to beer and\nstatistics, he also invented\na boat with two rudders\nthat would be easy to\nmanoeuvre when fly\nfishing.\n\naugustin louis cauchy\n(1789\u20131857) made\ncontributions to all the\nareas of mathematics\nknown at his time. he was\na pioneer of real and\ncomplex analysis, but also\ndeveloped applied\ntechniques such as fourier\ntransforms and the\ndiagonalization of\nmatrices in order to work\non elasticity and the\ntheory of light. his\nrelations with\ncontemporaries were often\npoor because of his rigid\ncatholicism and his\ndifficult character.\n\n "}, {"Page_number": 46, "text": ".\u223c means \u2018is\napproximately distributed\nas\u2019.\n\n34\n\nwhere wn lies between zn and zero. as h\n\u22121/2\u03c4 wn)\nn\n\n(\u00b5), so (2.15) gives\n\np\u2212\u2192 h\n\n(cid:11)\n\nn1/2{h(tn) \u2212 h(\u00b5)}\n\n\u03c4 h(cid:11)(\u00b5)\n\n(cid:11)\n\nis continuous at \u00b5, itfollows that h\n\n2 \u00b7 variation\n(\u00b5 +\n\n(cid:11)\n\n(cid:4)\n\n(cid:11)(cid:3)\n\n\u00b5 + n\n\n\u22121/2\u03c4 wn\n\nh(cid:11)(\u00b5)\n\n= n1/2{h(tn) \u2212 h(\u00b5)}\n\u00b5 + n\u22121/2\u03c4 wn\n(cid:11)(cid:3)\n\n(cid:4) \u00d7 h\n(cid:4)\n\u22121/2\u03c4 wn\n\n\u00b5 + n\n\n\u03c4 h(cid:11)(cid:3)\n= zn \u00d7 h\nd\u2212\u2192 z\n\nh(cid:11)(\u00b5)\n\nas n \u2192 \u221e. this implies that in large samples, h(tn) has approximately the normal\n(cid:11)\ndistribution with mean h(\u00b5) and variance var(tn)h\n.\u223c n (h(\u00b5), var(tn)h\n\n(\u00b5)2, that is,\n(cid:11)\n\n(\u00b5)2).\n\n(2.17)\n\nh(tn)\n\nthis result is often called the delta method. analogous results apply if the limiting\ndistribution of zn is non-normal.\n\nfurthermore, if h\n\n(\u00b5) isreplaced by h\n\n(tn) and \u03c4 2 is replaced by a consistent\n\n(cid:11)\n\n(cid:11)\n\nestimator, sn, amodification of the argument in example 2.22 gives\n\nn1/2{h(tn) \u2212 h(\u00b5)}\n\n|h(cid:11)(tn)|\n\ns1/2\nn\n\nd\u2212\u2192 z .\n\n(2.18)\n\nthus the same limiting results apply if the variance of h(tn) isreplaced by a consistent\n(\u00b5)2 by consistent\nestimator. in particular, replacement of the parameters in var(tn)h\nestimators gives a consistent estimator of var{h(tn)}.\nexample 2.24 (exponential transformation) consider h(y ) = exp(y ), where y\nis the average of a random sample of size n, and each of the y j has mean \u00b5 and\n(\u00b5) = e\u00b5, so exp(y ) isasymptotically normal with mean e\u00b5 and\nvariance \u03c3 2. here h\n\u22121s2 exp(2y ), where s2 is the sample\nvariance n\n(cid:1)\nvariance.\n\n\u22121\u03c3 2e2\u00b5. this can be estimated by n\n\n(cid:11)\n\n(cid:11)\n\n\u22121/2(tr \u2212 \u03b8r ) d\u2212\u2192 n (0, \u03c9rr ), that the joint limiting distribution of n\n\nseveral variables\nthe delta method extends to functions of several random variables t1, . . . , tp; we\nsuppress dependence on n for ease of notation. as n \u2192 \u221e, suppose that for each\n\u22121/2(tr \u2212 \u03b8r )\nr, n\nis multivariate normal (see section 3.2.3) and ncov(tr , ts) \u2192 \u03c9rs, where the p \u00d7 p\nmatrix \u0001 whose (r, s) element is \u03c9rs is positive-definite; note that \u0001 is symmetric.\nnow suppose that a variance is required for the scalar function h(t1, . . . , tp). an\nargument like that above gives\n\n.\u223c n{h(\u03b81, . . . , \u03b8 p), n\n\n(cid:11)\n\n\u22121h\n\n(cid:11)\n\n(cid:11)\n\n(\u03b8)t\u0001h\n\nh(t1, . . . , tp)\n\n(2.19)\n(\u03b8) isthe p \u00d7 1 vector whose rth element is \u2202h(\u03b81, . . . , \u03b8 p)/\u2202\u03b8r ; the require-\n(\u03b8) (cid:13)= 0 also holds here. as in the univariate case, the variance can be\n\nwhere h\nment that h\nestimated by replacing parameters with consistent estimators.\nexample 2.25 (ratio) let \u03b81 = e(x) (cid:13)= 0 and \u03b82 = e(y ), and suppose we are\ninterested in h(\u03b81, \u03b82) = \u03b82/\u03b81. estimates of \u03b81 and \u03b82 based on random samples\n\n(cid:11)\n\n(\u03b8)},\n\n "}, {"Page_number": 47, "text": "2.2 \u00b7 convergence\nx1, . . . , xn and y1, . . . ,y n are t1 = x and t2 = y , sothe ratio is consistently es-\ntimated by t2/t1. the derivative vector is h\n1 )t, and the limiting\nmean and variance of t2/t1 are\n\n(\u03b8) = (\u2212\u03b82/\u03b8 2\n\n, \u03b8\u22121\n\n35\n\n1\n\n(cid:11)\n\nthe second of which equals\n\n\u22121\n\nn\n\n\u03b82\n\u03b81\n\n,\n\n(cid:3) \u2212 \u03b82/\u03b8 2\n(cid:14)\n\n1\n\n(cid:4)\u22121\n\n\u03c911\n\n(cid:3)\n\nn\u03b8 2\n1\n\n(cid:4)(cid:9)\n\n\u03b8\u22121\n\n1\n\n\u03c911 \u03c912\n\u03c921 \u03c922\n\n(cid:10)(cid:9)\u2212\u03b82/\u03b8 2\n\n1\n\n(cid:10)\n\n,\n\n1\n\n\u03b8\u22121\n(cid:15)\n\n,\n\n(cid:10)2 \u2212 2\u03c912\n\n(cid:9)\n\n\u03b82\n\u03b81\n\n+ \u03c922\n\n\u03b82\n\u03b81\n\nassumed finite and positive. the variance tends to zero as n \u2192 \u221e, so weshould aim\nto estimate nvar(t2/t1), which is not a moving target.\n= (n \u2212 1)\n\nexamples 2.20 and 2.21 imply that \u03c911, \u03c922, and \u03c912 are consistently esti-\n(cid:5)\n(y j \u2212 y )2, and c = (n \u2212\n(x j \u2212 x)(y j \u2212 y ) respectively. therefore nvar(y /x) isconsistently esti-\n\n(cid:5)\n\n(cid:5)\n\n\u22121\n\n\u22121\n\n\u22121\n\nmated by s2\n1\n1)\nmated by\n\n\u22122\n\nx\n\n= (n \u2212 1)\n\uf8f1\uf8f2\n\uf8f3s2\n\n(cid:11)\n\n1\n\ny\nx\n\n2\n\n(x j \u2212 x)2, s2\n\uf8fc\uf8fd\n\uf8fe =\n\n+ s2\n\n2\n\ny\nx\n\n(cid:12)2 \u2212 2c\n\n(cid:11)\n\nn(cid:1)\nj=1\n\ny j \u2212 y\nx\n\n(cid:12)2\n\nx j\n\n,\n\n1\n\n(n \u2212 1)x 2\n\nas we see after simplification.\n\n(cid:1)\n\nexample 2.26 (gamma shape)\nin example 2.12 the shape parameter \u03ba of the\ngamma distribution was taken to be y2/s2 = 3.15, based on n = 95 observations.\n/t2, where t1 = y and t2 = s2 are cal-\nthe corresponding random variable is t 2\n1\nculated from the random sample y1, . . . ,y n, supposed to be gamma with mean\n\u03b81 = \u03ba/\u03bb and variance \u03b82 = \u03ba/\u03bb2. wetake h(\u03b81, \u03b82) = \u03b8 2\n(\u03b81, \u03b82) =\n(2\u03b81/\u03b82,\u2212\u03b8 2\n\u22121\u03ba/\u03bb2, and it turns out that\n\n2 )t. the variance of t1 is \u03b82/n, that is, n\n\n/\u03b82, giving h\n\n/\u03b8 2\n\n1\n\n1\n\n(cid:11)\n\nvar(t2) = var(s2) = \u03ba4\nn\n\ncov(t1, t2) = cov(y , s2) = \u03ba3\nn\n\nwhere \u03ba2 = \u03ba/\u03bb2, \u03ba3 = 2\u03ba/\u03bb3, and \u03ba4 = 6\u03ba/\u03bb4. thus\n\n(cid:3)\n\nvar\n\nt 2\n1\n\n/t2\n\n2\n\n,\n\n+ 2\u03ba 2\nn \u2212 1\n(cid:9) \u03ba\n(cid:4) .= ( 2\u03bb \u2212\u03bb2 )\n(cid:10)\n= 2\u03ba\n1 + n\u03ba\nn \u2212 1\nn\n\nn\u03bb2\n2\u03ba\nn\u03bb3\n\n(cid:9)\n\n(cid:10)(cid:9)\n\n(cid:10)\n\n2\u03bb\u2212\u03bb2\n\n2\u03ba\nn\u03bb3\nn\u03bb4 + 2\u03ba 2\n(n\u22121)\u03bb4\n\n6\u03ba\n\n,\n\n,\n\n(cid:1)\n\nor roughly 2n\n\n\u22121\u03ba(\u03ba + 1).\n\nthis can be skipped on a\nfirst reading.\n\nbig and little oh notation: o and o\nfor two sequences of constants, {sn} and {an} such that an \u2265 0 for all n, wewrite\nsn = o(an) if limn\u2192\u221e(sn/an) = 0, and sn = o(an) ifthere is a finite constant k such\nthat limn\u2192\u221e |sn| \u2264 ank. asequence of random variables {sn} is said to be o p(an) if\np\u2212\u2192 0 asn \u2192 \u221e, and is said to be o p(an) if sn/an is bounded in probability\n(sn/an)\n\n "}, {"Page_number": 48, "text": "36\n\n2 \u00b7 variation\nas n \u2192 \u221e, that is, given \u03b5 > 0 there exist n0 and a finite k such that for all n > n0,\n\npr(|sn/an| < k) > 1 \u2212 \u03b5.\n\nthis gives a useful shorthand for expansions of random quantities.\nto illustrate this, suppose that {y j} is a sequence of independent identically dis-\ntributed variables with finite mean \u00b5, and let sn = n\n\u22121(y1 + \u00b7\u00b7\u00b7 + yn). then the weak\nlaw may be restated as sn = \u00b5 + o p(1), and if in addition the y j have finite variance\n\u03c3 2, the central limit theorem implies that y = \u00b5 + o p(n\n\u22121/2). more precisely,\ny d= \u00b5 + n\n\u22121/2), where z has a standard normal distribution. such\nexpressions are sometimes used in later chapters.\n\n\u22121/2\u03c3 z + o p(n\n\nd= means \u2018has the same\ndistribution as\u2019.\n\nexercises 2.2\n\n1 suppose that sn\n\np\u2212\u2192 s0, and that the function h is continuous at s0, that is, for any \u03b5 > 0\nthere exists a \u03b4 > 0 such that |x \u2212 y| < \u03b4 implies that |h(x) \u2212 h(y)| < \u03b5. explain why\nthis implies that\n\npr(|sn \u2212 s0| < \u03b4) \u2264 pr{|h(sn) \u2212 h(s0)| < \u03b5} \u22641,\n\nand deduce that pr{|h(s0) \u2212 h(sn)| < \u03b5} \u21921 asn \u2192 \u221e. that is, h(sn)\n\np\u2212\u2192 h(s0).\n\n2 let s0 be a constant. by writing\n\npr(|sn \u2212 s0| \u2264\u03b5 ) = pr(sn \u2264 s0 + \u03b5) \u2212 pr(sn \u2264 s0 \u2212 \u03b5),\n\n3\n\nd\u2212\u2192 s0 implies that sn\n\np\u2212\u2192 s0.\n\nfor \u03b5 > 0, show that sn\n(a) let x and y be two random variables with finite positive variances. use the fact that\nvar(a x + y ) \u2265 0, with equality if and only if the linear combination a x + y is constant\nwith probability one, to show that cov(x, y )2 \u2264 var(x)var(y ); this is a version of the\ncauchy\u2013schwarz inequality. hence show that \u22121 \u2264 corr(x, y ) \u2264 1, and say under what\nconditions equality is attained.\n(b) show that if x and y are independent, corr(x, y ) = 0. show that the converse is false\nby considering the variables x and y = x 2 \u2212 1, where x has mean zero, variance one,\nand e(x 3) = 0.\n\n\u2212\u03bbx , x > 0, and \u03bb\u22121e\n\n4 let x1, . . . , xn and y1, . . . ,y n be independent random samples from the exponential\n\u2212y/\u03bb, y > 0, with \u03bb >0. if x and y are the sample\n5 show that as n \u2192 \u221e the skewness measure in example 2.4 converges in probability to\n\ndensities \u03bbe\naverages, show that x y p\u2212\u2192 1 asn \u2192 \u221e.\nthe corresponding theoretical quantity(cid:22)\n(cid:6)(cid:22)\n\n,\n\n(y \u2212 \u00b5)3d f(y)\n(cid:7)3/2\n(y \u2212 \u00b5)2d f(y)\n\nprovided this has finite numerator and positive denominator. under what additional con-\ndition(s) is the skewness measure asymptotically normal?\n\n6\n\niid\u223c n (\u00b5, \u03c3 2), show that n1/2(y \u2212 \u00b5)2\n\np\u2212\u2192 0 asn \u2192 \u221e. given that var{(y j \u2212\nif y1, . . . ,y n\n\u00b5)2} =2\u03c3 4, deduce that (s2 \u2212 \u03c3 2)/(2\u03c3 4/n)1/2 d\u2212\u2192 z, where z \u223c n (0, 1). when is this\ntrue for non-normal data?\nare m\u03c0 and m\u03c0(1 \u2212 \u03c0). the empirical logistic transform of r is\n\n7 let r be a binomial variable with probability \u03c0 and denominator m; its mean and variance\n\nh(r) = log\n\n(cid:11)\n\nr + 1\n\n2\n\nm \u2212 r + 1\n\n2\n\n(cid:12)\n\n.\n\niid\u223c means \u2018are\nindependent and\nidentically distributed as\u2019.\n\n "}, {"Page_number": 49, "text": "2.3 \u00b7 order statistics\n\nshow that for large m,\n\n.\u223c n\n\nh(r)\n\n(cid:10)\n\n(cid:2)\n\n(cid:9)\n\nlog\n\n\u03c0\n\n1 \u2212 \u03c0\n\n,\n\n1\n\nm\u03c0(1 \u2212 \u03c0)\n\n(cid:23)\n\n.\n\n37\n\nwhat is the exact value of e[log{r/(m \u2212 r)}]? are the 1\n\n2 s necessary in practice?\n\n8 truncated poisson variables y arise when counting quantities such as the sizes of groups,\n\neach of which must contain at least one element. the density is\n\npr(y = y) =\n\n\u2212\u03b8\n\n\u03b8 ye\n\ny!(1 \u2212 e\u2212\u03b8 )\n\ny = 1, 2, . . . ,\n\n,\n\n\u03b8 > 0.\n\np\u2212\u2192 \u00b5(\u03b8). hence show that(cid:13)\u03b8 = \u00b5\u22121(y )\n\nfind an expression for e(y ) = \u00b5(\u03b8) interms of \u03b8. if y1, . . . ,y n is a random sample from\nthis density and n \u2192 \u221e, show that y\np\u2212\u2192 \u03b8.\n9 let y = exp(x), where x \u223c n (\u00b5, \u03c3 2); y has the log-normal distribution. use the\nmoment-generating function of x to show that e(y r ) = exp(r \u00b5 + r 2\u03c3 2/2), and hence\nfind e(y ) and var(y ).\nif y1, . . . ,y n is a log-normal random sample, show that both t1 = y and t2 = exp(x +\ns2/2) are consistent estimators of e(y ), where x j = log y j and s2 is the sample variance\nof the x j . give the corresponding estimators of var(y ).\nare the estimators based on the y j or on the x j preferable? why?\n\n(cid:5)\n\n10 the binomial distribution models the number of \u2018successes\u2019 among independent vari-\nables with two outcomes such as success/failure or white/black. the multinomial distri-\nbution extends this to p possible outcomes, for example total failure/failure/success or\nyr = (cid:5)\nwhite/black/red/blue/. . .. that is, each of the discrete variables x1, . . . , xm takes values\n1, . . . , p, independently with probability pr(x j = r) = \u03c0r , where\n\u03c0r = 1, \u03c0r \u2265 0. let\nj i (x j = r) bethe number of x j that fall into category r, for r = 1, . . . , p, and\nconsider the distribution of (y1, . . . ,y p).\n(a) show that the marginal distribution of yr is binomial with probability \u03c0r , and that\ncov(yr , ys) = \u2212m\u03c0r \u03c0s, for r (cid:13)= s. is itsurprising that the covariance is negative?\n(b) hence give consistent estimators of positive probabilities \u03c0r . what happens if some\n\u03c0r = 0?\n(d) suppose that p = 4 with \u03c01 = (2 + \u03b8)/4, \u03c02 = (1 \u2212 \u03b8)4, \u03c03 = (1 \u2212 \u03b8)/4 and \u03c04 =\n\u03b8/4. show that t = m\n\u22121(y1 + y4 \u2212 y2 \u2212 y3) issuch that e( t ) = \u03b8 and var(t ) = a/m\nfor some a > 0. hence deduce that t is consistent for \u03b8 as m \u2192 \u221e.\ngive the value of t and its estimated variance when (y1, y2, y3, y4) equals\n(125, 18, 20, 34).\n\n2.3 order statistics\n\nsummary statistics such as the sample median, interquartile range, and median ab-\nsolute deviation are based on the ordered values of a sample y1, . . . , yn, and they are\nalso useful in assessing how closely a sample matches a specified distribution. in this\nsection we study properties of ordered random samples.\n\nthe rth order statistic of a random sample y1, . . . ,y n is y(r), where\n\ny(1) \u2264 y(2) \u2264 \u00b7\u00b7\u00b7 \u2264 y(n\u22121) \u2264 y(n)\n\nis the ordered sample. we assume that the cumulative distribution f of the y j is\ncontinuous, so y(r) < y(r+1) with probability one for each r and there are no ties.\n\n "}, {"Page_number": 50, "text": "38\n\n2 \u00b7 variation\n\ndensity function\nto find the probability density of y(r), we argue heuristically. divide the line into\nthree intervals: (\u2212\u221e, y), [y, y + dy), and [y + dy,\u221e). the probabilities that a sin-\ngle observation falls into each of these intervals are f(y), f (y)dy, and 1 \u2212 f(y)\nrespectively. therefore the probability that y(r) = y is\n\nthe dy is a rhetorical\ndevice so that we can say\nthe probability that y = y\nis f (y)dy.\n\nn!\n\n\u00d7 f(y)r\u22121 \u00d7 f (y)dy \u00d7 {1 \u2212 f(y)}n\u2212r ,\n\n(r \u2212 1)! 1! (n \u2212 r)!\n\n(2.20)\nwhere the second term is the probability that a prespecified r \u2212 1 of they j fall in\n(\u2212\u221e, y), the third the probability that a prespecified one falls in [y, y + dy), the\nfourth the probability that a prespecified n \u2212 r fall in [y + dy,\u221e), and the first is a\ncombinatorial multiplier giving the number of ways of prespecifying disjoint groups\nof sizes r \u2212 1, 1, and n \u2212 r out of n.\n(cid:8) \u221e\n\nif we drop the dy, expression (2.20) becomes a probability density function, from\n\nwhich we can derive properties of y(r). for example, its mean is\n\ny f (y)f(y)r\u22121{1 \u2212 f(y)}n\u2212r dy\n\n(2.21)\n\n(cid:3)\n\ne\n\ny(r)\n\n(cid:4) =\n\nn!\n\n(r \u2212 1)!(n \u2212 r)!\n\n\u2212\u221e\n\nwhen it exists; of course we expect that e(y(1)) < \u00b7\u00b7\u00b7 < e(y(n)).\nexample 2.27 (uniform distribution) let u1, . . . , un be a random sample from\nthe uniform distribution on the unit interval,\n\npr(u \u2264 u) =\n\n\uf8f1\uf8f2\n\uf8f3 0,\n\nu,\n1,\n\nu \u2264 0,\n0 < u \u2264 1,\n1 < u;\n\n(2.22)\n\niid\u223c u (0, 1). as f (u) = 1 when 0 < u < 1, u(r) has density\n\nur\u22121(1 \u2212 u)n\u2212r ,\n\n0 < u < 1,\n\n(2.23)\n\nwe write u1, . . . , un\nfu(r)(u) =\n\nn!\n\n(r \u2212 1)!(n \u2212 r)!\n(cid:8)\n\nand (2.21) shows that e(u(r)) equals\n\nn!\n\n(r \u2212 1)!(n \u2212 r)!\n\n0\n\n1\n\nu ur\u22121(1 \u2212 u)n\u2212r dy =\n\nr!(n \u2212 r)!\n(n + 1)!\n\nn!\n\n(r \u2212 1)!(n \u2212 r)!\nn + 1\n\n;\n\n= r\n\nthe value of the integral follows because (2.23) must have integral one for any r in\nthe range 1, . . . ,n and any positive integer n. the expected positions of the n order\nstatistics divide the unit interval and hence the total probability under the density into\nn + 1 equal parts.\nit is an exercise to show that u(r) has variance r(n \u2212 r + 1)/{(n + 1)2(n + 2)}\n\u22121 p(1 \u2212 p), where p = r/n,\n(exercise 2.3.1). for large n this is approximately n\nand hence we can write u(r) = r/(n + 1) + { p(1 \u2212 p)/n}1/2\u03b5, where \u03b5 is a random\n(cid:1)\nvariable with mean zero and variance approximately one.\n\n "}, {"Page_number": 51, "text": "2.3 \u00b7 order statistics\n\n39\n\nintegrals such as (2.21) are nasty, but a good approximation is often available. let\n\nrecall that every\ndistribution function is\nright-continuous.\n\nu, u1, . . . , un\n\niid\u223c u (0, 1) and f\n\n\u22121(u) = min{y : f(y) \u2265 u}. then\n\u22121(u ) \u2264 y} =pr{ u \u2264 f(y)} = f(y),\n\npr{f\n\nit follows that f\n\n\u22121(u1), . . . , f\n\nwhich is the distribution function of y . hence y d= f\n\u22121(u ); note that for continu-\nous f the variable f(y ) has the u (0, 1) distribution; f(y ) iscalled the probability\n\u22121(un) is arandom sam-\nintegral transform of y .\nple from f and that the joint distributions of the order statistics y(1), . . . ,y (n) and\n\u22121(u(1)), . . . , f\n\u22121(u(n)) are the same; in fact this is true for general f. conse-\nof f\n\u22121(u(r))}. but example 2.27 implies that u(r)\nquently e(y(r)) = e{f\nd= r/(n + 1) +\n{ p(1 \u2212 p)/n}1/2\u03b5, where \u03b5 is a random variable with mean zero and unit variance. if\nwe apply the delta method with h = f\n\u22121, weobtain\n(cid:6)\n(cid:4) = e\n(cid:6)\n(cid:3)\n(cid:4)(cid:7) .= f\n(cid:4)(cid:7) = f\n\u22121\n(2.24)\n\u22121{r/(n + 1)} are approximate expected order statis-\n\nhence the plotting positions f\ntics, justifying their use in probability plots; see section 2.1.4.\n\n\u22121{r/(n + 1)}.\n\nu(r)\n\nu(r)\n\ny(r)\n\n\u22121\n\n(cid:3)\n\n(cid:3)\n\ne\n\ne\n\nf\n\nseveral order statistics\nthe argument leading to (2.20) can be extended to the joint distribution of any col-\nlection of order statistics. for example, the probability that the maximum, y(n), takes\nvalue v and that the minimum, y(1), takes value u, is\n\nn!\n\n1!(n \u2212 2)!1!\n\n\u00d7 f (u)du \u00d7 {f(v) \u2212 f(u)}n\u22122 \u00d7 f (v)dv,\n\nu < v,\n\nand is zero otherwise. similarly the joint density of all n order statistics is\ny1 < \u00b7\u00b7\u00b7 < yn.\n\nfy(1),...,y(n)(y1, . . . , yn) = n! f (y1) \u00d7 \u00b7\u00b7\u00b7 \u00d7 f (yn),\n\n(2.25)\n\nin principle one can use (2.25) to calculate other properties of the joint distribution\nof the y(r), but this can be very tedious. here is an elegant exception:\n\nexample 2.28 (exponential order statistics) consider the order statistics of a ran-\ndom sample y1, . . . ,y n from the exponential density with parameter \u03bb >0, for which\npr(y > y) = e\n\u2212\u03bby. let e1, . . . , en denote a random sample of standard exponential\nvariables, with \u03bb = 1. thus y j\nthe reasoning uses two facts. first, the distribution function of min(y1, . . . ,y r ) is\n\nd= e j /\u03bb.\n\n1 \u2212 pr{min(y1, . . . ,y r ) > y} = 1 \u2212 pr{y1 > y, . . . ,y r > y}\n\n= 1 \u2212 pr(y1 > y) \u00d7 \u00b7\u00b7\u00b7 \u00d7pr( yr > y)\n= 1 \u2212 exp(\u2212r \u03bby);\n\nthis is exponential with parameter r \u03bb. second, the exponential density has the lack-\nof-memory property\n\npr(y \u2212 x > y | y > x) = pr(y > x + y)\n\npr(y > x)\n\n= exp{\u2212\u03bb(x + y)}\n\nexp(\u2212\u03bbx)\n\n= exp(\u2212\u03bby),\n\n "}, {"Page_number": 52, "text": "40\n\nr\ne\nb\nm\nu\nn\nn\no\n\n \n\ni\nt\n\na\nv\nr\ne\ns\nb\no\n\n5\n\n \n \n \n \n \n \n \n \n\n4\n\n \n \n \n \n \n \n \n \n\n3\n\n \n \n \n \n \n \n \n \n\n2\n\n1\n\n0\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\ny(1)\n\ny(2)\n\ny(3)\n\ny(4)\n\n0\n\n1\n\n2\n\n                         4\n3  \n\nobservation value\n\n2 \u00b7 variation\n\n\u2022\n\ny(5)\n\nfigure 2.5 exponential\norder statistics for a\nsample of size n = 5. the\ntime to y(1) is the time to\nfirst event in a poisson\nprocess of rate 5\u03bb, and so\nit has the exponential\ndistribution with mean\n1/(5\u03bb). the spacing\ny(2) \u2212 y(1) is the time to\nfirst event in a poisson\nprocess of rate 4\u03bb, and is\nindependent of y(1)\nbecause of the\nlack-of-memory property.\nit follows likewise that the\nspacings are independent\nand that the rth spacing\nhas the exponential\ndistribution with\nparameter (n + 1 \u2212 r)\u03bb.\n\nimplying that given that y \u2212 x is positive, its distribution is the same as the original\ndistribution of y , whatever the value of x.\nwe now argue as follows. since y(1) = min(y1, . . . ,y n), its distribution is expo-\nd= e1/(n\u03bb). given y(1), n \u2212 1 ofthe y j remain, and\nnential with parameter n\u03bb: y(1)\nby the lack-of-memory property the distribution of y j \u2212 y(1) for each of them is the\nsame as if the experiment had started at y(1) with just n \u2212 1 variables; see figure 2.5. during the second world\nthus y(2) \u2212 y(1) is exponential with parameter (n \u2212 1)\u03bb, independent of y(1), giving\nd= e2/{(n \u2212 1)\u03bb}. but given y(2), just n \u2212 2 ofthe y j remain, and by the\ny(2) \u2212 y(1)\nlack-of-memory property the distribution of y j \u2212 y(2) for each of them is exponential\nindependent of the past; hence y(3) \u2212 y(2)\nd= e3/{(n \u2212 2)\u03bb}. this argument yields the\nr\u00b4enyi representation\nr(cid:1)\nj=1\n\nwar alfr\u00b4ed r\u00b4enyi\n(1921\u20131970) escaped\nfrom a labour camp and\nrescued his parents from\nthe budapest ghetto. he\nmade major contributions\nto number theory and to\nprobability. he was a\ngifted raconteur who\ndefined a mathematician\nas \u2018a machine for turning\ncoffee into theorems\u2019.\n\nn + 1 \u2212 j\n\nd= \u03bb\u22121\n\n(2.26)\n\ny(r)\n\ne j\n\n,\n\nfrom which properties of the y(r) are easily derived. for example,\n\n(cid:3)\n\ne\n\ny(r)\n\n(cid:4) = \u03bb\u22121\n\nr(cid:1)\nj=1\n\n(cid:3)\n\n1\n\nn + 1 \u2212 j\n\n, cov\n\ny(r), y(s)\n\n(cid:4) = \u03bb\u22122\n\nr(cid:1)\nj=1\n\n1\n\n(n + 1 \u2212 j)2\n\n, s \u2265 r.\n\n(cid:5)\nsuite against standard exponential plotting positions or exponential scores,\n\u22121 .= \u2212 log{1 \u2212 r/(n + 1)}. the exponential model fits very poorly.\n1 \u2212 j)\n\nthe upper right panel of figure 2.3 shows a plot of the ordered times in the delivery\nj=1(n +\n\nr\n\nthe argument leading to (2.26) may be phrased in terms of poisson processes. a\nsuperposition of independent poisson processes is itself a poisson process with rate\nthe sum of the individual rates, so the period from zero to y(1) is the time to the first\nevent in a poisson process of rate n\u03bb, the time from y(1) to y(2) is the time to first\nevent in a poisson process of rate (n \u2212 1)\u03bb, and so on, with the times between events\nindependent by definition of a poisson process; see figure 2.5. exercise 2.3.4 gives\n(cid:1)\nanother derivation.\n\n "}, {"Page_number": 53, "text": "2.3 \u00b7 order statistics\n\n41\n\napproximate density\nalthough (2.20) gives the exact density of an order statistic for a random sample of\nany size, approximate results are usually more convenient in practice. suppose that r\nis the smallest integer greater than or equal to np, r = (cid:2)np(cid:3), for some p in the range\n0 < p < 1. then provided that f {f\n\u22121( p)} > 0, we prove at the end of this section\n\u22121( p) and variance\nthat y(r) has an approximate normal distribution with mean f\n\u22121( p)}2 as n \u2192 \u221e. more formally,\n\u22121 p(1 \u2212 p)/ f {f\n(cid:7)\n(cid:6)\nn\n\u221a\ny(r) \u2212 f\nn\n\nd\u2212\u2192 z as\n\nn \u2192 \u221e,\n\n\u22121( p)}\n\n(2.27)\n\n\u22121( p)\n\nf {f\n{ p(1 \u2212 p)}1/2\n\nwhere z has a standard normal distribution.\n\nexample 2.29 (normal median) suppose that y1, . . . ,y n is a random sample from\nthe n (\u00b5, \u03c3 2) distribution, and that n = 2m + 1 isodd. the median of the sample is\nits central order statistic, y(m+1). tofind its approximate distribution in large samples,\nnote that (m + 1)/(2m + 1)\n2 for large m, and since the normal density is sym-\n2 ) = \u00b5. moreover f (y) = (2\u03c0 \u03c3 2)\n\u22121/2 exp{\u2212(y \u2212 \u00b5)2/2\u03c3 2}, so\nmetric about \u00b5, f\nf {f\n\u22121/2. thus (2.27) implies that in large samples y(m+1) is approx-\n(cid:1)\nimately normal with mean \u00b5 and variance \u03c0 \u03c3 2/(2n).\n\n\u22121( 1\n2 )} =(2\u03c0 \u03c3 2)\n\n\u22121( 1\n\n.= 1\n\nexample 2.30 (birth data)\nin figure 2.1 and example 2.8 we saw that the daily\nmedians of the birth data were generally smaller but more variable than the daily\naverages. to understand why, suppose that we have a sample of n = 13 observa-\ntions from the gamma distribution f with mean \u00b5 = 8 and shape parameter \u03ba = 3;\nthese are close to the values for the data. then the average y has mean \u00b5 and vari-\nance \u00b52/(n\u03ba); these are 8 and 1.64, comparable with the data values 7.90 and 1.54.\n2 ) = 7.13 and variance\nthe sample median has approximate expected value f\n2 )}2 = 4.02, where f denotes the density (2.8); these values\n2 (1 \u2212 1\n\u22121 1\nn\nare to be compared with the average and variance of the daily medians, 7.03 and 2.15.\nthe expected values are close, but the variances are not; we should not rely on an\nasymptotic approximation when n = 13. the theoretical variance of the median ex-\nceeds that of the average, so the sampling properties of the daily average and median\nare roughly what we might have expected: var(m) > var(y ), and e(m) < e(y ). our\ncalculation presupposes constant n, but in the data n changes daily; this is one source\n(cid:1)\nof error in the asymptotic approximation.\n\n2 )/ f {f\n\n\u22121( 1\n\n\u22121( 1\n\nexpression (2.27) gives asymptotic distributions for central order statistics, that is,\ny(r) where r/n \u2192 p and 0 < p < 1; as n \u2192 \u221e such order statistics have increasingly\nmore values on each side. different limits arise for extreme order statistics such as\nthe minimum, for which r = 1 and r/n \u2192 0, and the maximum, for which r = n and\nr/n \u2192 1. we discuss these more fully in section 6.5.2, but here is a simple example.\nexample 2.31 (pareto distribution)\nsuppose that y1, . . . ,y n is a random sample\nfrom the pareto distribution, whose distribution function is\ny < a,\ny \u2265 a,\n\n0,\n1 \u2212 (y/a)\n\nf(y) =\n\n\u2212\u03b3 ,\n\n(cid:2)\n\nvilfredo pareto\n(1848\u20131923) studied\nmathematics and physics\nat turin, and then became\nan engineer and director\nof a railway, before\nbecoming professor of\npolitical economy in\nlausanne. he pioneered\nsociology and the use of\nmathematics in economic\nproblems. the pareto\ndistributions were\ndeveloped by him to\nexplain the spread of\nwealth in society.\n\n "}, {"Page_number": 54, "text": "42\n\n2 \u00b7 variation\n\nwhere a, \u03b3 > 0. the minimum y(1) exceeds y if and only if all the y1, . . . ,y n ex-\nceed y, so pr(y(1) > y) = (y/a)\n\u2212n\u03b3 . toobtain a non-degenerate limiting distribution,\nconsider m = \u03b3 n(y(1) \u2212 a)/a. now\n(cid:9)\n\n(cid:12)\u2212n\u03b3\n\n(cid:11)\n\n(cid:10)\n\npr(m > z) = pr\n\ny(1) >\n\n+ a\n\n=\n\naz\nn\u03b3\n\naz\nn\u03b3\n\n+ a\na\n\n\u2192 e\n\n\u2212z\n\nas n \u2192 \u221e. consequently \u03b3 n(y(1) \u2212 a)/a converges in distribution to the standard\nexponential distribution.\n\nthere are two differences between this result and (2.27). first, and most obvi-\nously, the limiting distribution is not normal. second, as the power of n by which\ny(1) \u2212 a must be multiplied to obtain a non-degenerate limit is higher than in (2.27),\nthe rate of convergence to the limit is faster than for central order statistics. accel-\nerated convergence of extreme order statistics does not always occur, however; see\n(cid:1)\nexample 6.32.\n\nd= f\n\nderivation of (2.27)\nconsider y(r), where r = (cid:2)np(cid:3) and 0 < p < 1 is fixed; hence r/n \u2192 p as n \u2192 \u221e.\n\u22121(u(r)), where u(r) is the rth order statistic of a random\nwe saw earlier that y(r)\nsample u1, . . . , un from the u (0, 1) density, and that u(r) = r/(n + 1) + { p(1 \u2212\np)/n}1/2\u03b5, where \u03b5 has mean zero and variance tending to one as n \u2192 \u221e. recall that\n.=\nf is a distribution whose density f exists. hence the delta method gives e(y(r))\n\u22121{r/(n + 1)} .= f\nf\n\n\u22121( p), and as\n(cid:4) = var\n(cid:3)\n(cid:6)\n\n\u22121\n\nf\n\n(cid:3)\n\ny(r)\n\n(cid:4)(cid:7) .= var\n(cid:3)\n\nu(r)\n\nu(r)\n\n(cid:2)\n\n(cid:4) \u00d7\n\n(cid:23)2\n\nd f\n\n\u22121( p)\ndp\n\nvar\n\nand\n\nd\ndp\n\nf{f\n\n\u22121( p)} = f {f\n\n\u22121( p)} d\ndp\n\n\u22121( p) = 1,\n\nf\n\nwe have var{y(r)} .= p(1 \u2212 p)/[ f {f\nto find the limiting distribution of y(r), note that\n\n\u22121( p)}2n] provided f {f\n(cid:12)\n(cid:4) = pr\n\ni j (y) \u2265 r\n\n(cid:11)(cid:1)\n\n,\n\n\u22121( p)} > 0.\n\n(2.28)\n\n(cid:3)\n\ny(r) \u2264 y\n\npr\n\nthis may be omitted at a\nfirst reading.\n\n(cid:5)\n\nwhere i j (y) isthe indicator of the event y j \u2264 y. the i j (y) are independent, so their\nsum\nj i j (y) isbinomial with probability f(y) and denominator n. therefore (2.28)\nand the central limit theorem imply that for large n,\n\nj\n\npr\nnow choose y = f\n\n(cid:3)\n\n(cid:10)\n\nr \u2212 n f(y)\n\n(cid:9)\n\n(cid:4) .= 1 \u2212 \u0001\n\u22121/2z{ p(1 \u2212 p)/ f {f\n\n[n f(y){1 \u2212 f(y)}]1/2\n(cid:4)\n\n\u22121( p)}2}1/2, sothat\n\n(cid:3)\n\n\u22121/2z{ p(1 \u2212 p)}1/2 + o\n\n\u22121/2\n\nn\n\n,\n\ny(r) \u2264 y\n\u22121( p) + n\nf(y) = p + n\n\n.\n\n(2.29)\n\n "}, {"Page_number": 55, "text": "2.3 \u00b7 order statistics\nand recall that r = (cid:2)np(cid:3) .= np. then (2.28) and (2.29) imply that, as required,\n\n43\n\n(cid:11)\n\n(cid:6)\n\n(cid:7)\n\ny(r) \u2212 f\n\n\u22121( p)\n\npr\n\nn1/2\n\n{ p(1 \u2212 p)/ f {f\u22121( p)}2}1/2\n\n(cid:12)\n\n\u2264 z\n\napproximately equals\n\n(cid:24)\n\n1 \u2212 \u0001\n\nnp \u2212 np \u2212 n1/2z { p(1 \u2212 p)}1/2\n\n{np(1 \u2212 p)}1/2\n\n(cid:25)\n\n= 1 \u2212 \u0001(\u2212z) = \u0001(z).\n\nexercises 2.3\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n2\n\n, (8m)\n\n.\u223c n{ 1\n\n\u22121} for large m.\n\nif u(1) < \u00b7\u00b7\u00b7 < u(n) are the order statistics of a u (0, 1) random sample, show that\nvar(u(r)) = r(n \u2212 r + 1)/{(n + 1)2(n + 2)}. find cov(u(r), u(s)), r < s and hence show\nthat corr(u(r), u(s)) \u2192 1 for large n as r \u2192 s.\nlet u1, . . . , u2m+1 be a random sample from the u (0, 1) distribution. find the exact\ndensity of the median, u(m+1), and show that u(m+1)\nlet the x1, . . . , xn be independent exponential variables with rates \u03bb j . show that y =\nmin(x1, . . . , xn) isalso exponential, with rate \u03bb1 + \u00b7\u00b7\u00b7 + \u03bbn, and that pr(y = x j ) =\n\u03bb j /(\u03bb1 + \u00b7\u00b7\u00b7 + \u03bbn).\nverify that the joint distribution of all the order statistics of a sample of size n from a\ncontinuous distribution with density f (y) is(2.25). hence find the joint density of the\nspacings, s1 = y(1), s2 = y(2) \u2212 y(1), . . . , sn = y(n) \u2212 y(n\u22121), when f (y) = \u03bbe\n\u2212\u03bby, y > 0,\n\u03bb >0. use this to establish (2.26).\np\u2212\u2192 f\n\n\u22121( p) asn \u2192 \u221e, where r = (cid:2) pn(cid:3) and 0 < p < 1 is\nuse (2.27) to show that y(r)\nconstant.\nconsider iqr and mad (example 2.2). show that iqr p\u2212\u2192 1.35\u03c3 for normal data and\nhence give an estimator of \u03c3 . find also the estimator based on mad.\nlet n be a random variable taking values 0, 1, . . ., let g(u) bethe probability-generating\nfunction of n , let x1, x2, . . . be independent variables each having distribution function\nf, and let y = max{x1, . . . , x n}. show that y has distribution function g{f(y)}, and\nfind this when n is poisson and the x j exponential.\nlet m and iqr be the median and interquartile range of a random sample y1, . . . ,y n from\na density of form \u03c4 \u22121g{(y \u2212 \u03b7)/\u03c4}, where g(u) issymmetric about u = 0 and g(0) > 0.\nshow that as n \u2192 \u221e,\n\nn1/2 m \u2212 \u03b7\n\niqr\n\nd\u2212\u2192 n (0, c),\n\nfor some c > 0, and give c in terms of g and its integral g.\ngive c when g(u) equals 1\nthe probability that events in a poisson process of rate \u03bb >0 observed over the interval\n(0, t0) occur at 0 < t1 < t2 < \u00b7\u00b7\u00b7 < tn < t0 is\n\n2 exp(\u2212|u|) and exp(u)/{1 + exp(u)}2.\n\n\u03bbn exp(\u2212\u03bbt0),\n\n0 < t1 < t2 < \u00b7\u00b7\u00b7 < tn < t0.\n\nby integration over t1, . . . ,t n, show that the probability that n events occur, regardless of\ntheir positions, is\n\n(\u03bbt0)n\nn!\n\nexp(\u2212\u03bbt0),\n\nn = 0, 1, . . . ,\n\nand deduce that given that n events occur, the conditional density of their times is n!/t n\n0 ,\n0 < t1 < t2 < \u00b7\u00b7\u00b7 < tn < t0. hence show that the times may be considered to be order\nstatistics from a random sample of size n from the uniform distribution on (0, t0).\n\n "}, {"Page_number": 56, "text": "44\n\n9\n\n2 \u00b7 variation\n\nfind the exact density of the median m of a random sample y1, . . . ,y 2m+1 from the uniform\ndensity on the interval (\u03b8 \u2212 1\n\n2 ). deduce that z = m1/2(m \u2212 \u03b8) has density\n\n2\n\n, \u03b8 + 1\nf (z) = (2m + 1)!\n\n(m!)2m1/2\n\n(cid:9)\n\n(cid:10)m\n\n1\n4\n\n+ z2\nm\n\n|z| <\n\n,\n\n1\n2\n\nm1/2,\n\nand by considering the behaviour of log f (z) asm \u2192 \u221e or otherwise, show that for large\n.\u223c n (0, 1/8). check that this agrees with the general formula for the asymptotic\nm, z\ndistribution of a central order statistic.\n\n2 log(2\u03c0) +\n\nstirling\u2019s formula implies\nthat log m! \u223c 1\n(m + 1\nm \u2192 \u221e.\n\n2 ) log m \u2212 m as\n\n2.4 moments and cumulants\n\ncalculations involving moments often arise in statistics, but they are generally simpler\nwhen expressed in terms of equivalent quantities known as cumulants.\nthe moment-generating function of the random variable y is m(t) = e(ety ), pro-\nvided m(t) < \u221e. let\n(t) = d m(t)\ndt\n\n, m (r)(t) = dr m(t)\ndtr\ndenote derivatives of m. iffinite, the rth moment of y is \u00b5(cid:11)\ngiving the power series expansion\n\n= m (r)(0) = e(y r ),\n\n(t) = d2 m(t)\ndt 2\n\nr = 3, . . . ,\n\n, m\n\nm\n\n(cid:11)(cid:11)\n\n,\n\n(cid:11)\n\nr\n\nm(t) =\n\n\u00b5(cid:11)\nr tr /r!.\n\n\u221e(cid:1)\nr=0\n\nr is sometimes called the rth moment about the origin, whereas \u00b5r = the characteristic\n\nthe quantity \u00b5(cid:11)\ne{(y \u2212 \u00b5(cid:11)\n1)r} is the rth moment about the mean. among elementary properties of the\nmoment-generating function are the following: m(0) = 1; the mean and variance of\ny may be written\n\nfunction e(eity ), with\ni 2 = \u22121 is defined more\nbroadly than m(t), but as\nwe shall not need the extra\ngenerality, m(t) isused\nalmost everywhere in this\nbook.\n\ne(y ) = m\n\n(cid:11)\n\n(0), var(y ) = m\n\n(cid:11)(cid:11)\n\n(o) \u2212 {m\n\n(cid:11)\n\n(0)}2;\n\nrandom variables y1, . . . ,y n are independent if and only if their joint moment-\ngenerating function factorizes as\n\ne{exp(y1t1 + \u00b7\u00b7\u00b7 + yntn)} = e{exp(y1t1)}\u00b7\u00b7\u00b7e {exp(yntn)} ;\n\nand the fact that any moment-generating function corresponds to a unique probability\ndistribution.\n\ncumulants\nthe cumulant-generating function or cumulant generator of y is the function k (t) =\nlog m(t), and the rth cumulant is \u03bar = k (r)(0) = dr k (0)/dtr , giving the power series\nexpansion\n\nk (t) =\n\n\u221e(cid:1)\nr=1\n\ntr \u03bar /r!,\n\n(2.30)\n\n "}, {"Page_number": 57, "text": "2.4 \u00b7 moments and cumulants\n\n45\n\nprovided all the cumulants exist. differentiation of (2.30) shows that the mean and\nvariance of y are its first two cumulants\n, \u03ba2 = k\n\n(0) = m\n\n(0) = m\n\n\u03ba1 = k\n\n\u2212 (\u00b5(cid:11)\n\n\u2212 m\n\n= \u00b5(cid:11)\n\n= \u00b5(cid:11)\n\n(cid:11)(cid:11)\n\n(cid:11)(cid:11)\n\n(cid:11)\n\n(cid:11)\n\n(cid:11)\n\n1)2.\n\n1\n\n2\n\n(0)\nm(0)\n\n(0)2\nm(0)2\n\n(0)\nm(0)\n\nfurther differentiation gives higher-order cumulants. cumulants are mathematically\nequivalent to moments, and can be defined as combinations of powers of moments,\nbut weshall see below that their statistical interpretation is much more natural than\nis that of moments.\n\nexample 2.32 (normal distribution)\nmoment-generating function is m(t) = exp(t \u00b5 + 1\nfunction is k (t) = t \u00b5 + 1\nhigher-order cumulants are zero. the standard normal distribution has k (t) = 1\n\nif y has the n (\u00b5, \u03c3 2) distribution, its\n2 t 2\u03c3 2) and its cumulant-generating\n2 t 2\u03c3 2. the first two cumulants are \u00b5 and \u03c3 2, and all its\n2 t 2.\n(cid:1)\n\nthe cumulant-generating function is very convenient for statistical work. consider\nindependent random variables y1, . . . ,y n with respective cumulant-generating func-\ntions k1(t), . . . , kn(t). their sum y1 + \u00b7\u00b7\u00b7 + yn has cumulant-generating function\nlog my1+\u00b7\u00b7\u00b7+yn (t) = log e{exp(ty1 + \u00b7\u00b7\u00b7 +ty n)} = log\n\nmy j (t) = n(cid:1)\n\nk j (t).\n\nn(cid:26)\nj=1\n\nj=1\n\nit follows that the rth cumulant of a sum of independent random variables is the\nsum of their rth cumulants. similarly, the cumulant-generating function of a linear\ncombination of independent random variables is\n\nj=1 b j y j (t) = log e{exp(ta + tb1y1 + \u00b7\u00b7\u00b7 +tb nyn)} = ta + n(cid:1)\n\nn\n\nka+(cid:5)\n\nj=1\n\nk j (b j t).\n\n(2.31)\n\nexample 2.33 (chi-squared distribution)\nif z1, . . . , z \u03bd are independent standard\nnormal variables, each z 2\nj has the chi-squared distribution on one degree of freedom,\nand (3.10) gives its moment-generating function, (1 \u2212 2t)\n\u22121/2. therefore each z 2\n(cid:5)\u03bd\n2 log(1 \u2212 2t), and the \u03c7 2\nhas cumulant-generating function \u2212 1\n\u03bd random variable w =\nj\nj=1 z 2\n\u221e(cid:1)\n\u221e(cid:1)\nr=1\nr=1\n\nlog(1 \u2212 2t) = \u2212 \u03bd\n2\n\nj has cumulant-generating function\n\n(\u22121)r\u22121 (\u22122t)r\n\nk (t) = \u2212 \u03bd\n2\n\n2r\u22121(r \u2212 1)!\n\n= \u03bd\n\ntr\nr!\n\nr\n\n,\n\n2 . therefore w has rth cumulant \u03bar = \u03bd2r\u22121(r \u2212 1)!. in particu-\nprovided that |t| < 1\n(cid:1)\nlar, the mean and variance of w are \u03bd and 2\u03bd.\n(cid:5)\nexample 2.34 (linear combination of normal variables) let l = a +\nn\nj=1 b j y j be a linear combination of independent random variables, where y j has the\n\n "}, {"Page_number": 58, "text": "46\n\n2 \u00b7 variation\n\nnormal distribution with mean \u00b5 j and variance \u03c3 2\nfunction\n\n(cid:2)\n\nat + n(cid:1)\n\n(b j t)\u00b5 j + 1\n2\ncorresponding to a n (a +(cid:5)\n\nj=1\n\n(b j t)2\u03c3 2\nj\n(cid:5)\n\nb j \u00b5 j ,\n\nj . then l has cumulant-generating\n\n(cid:23)\n\n= t\n\n(cid:11)\na + n(cid:1)\n\nj=1\n\n(cid:12)\n\nb j \u00b5 j\n\n+ t 2\n2\n\n(cid:11)\nn(cid:1)\nj=1\n\n(cid:12)\n\nb2\nj\n\n\u03c3 2\nj\n\n,\n\n(cid:1)\n\nb2\nj\n\n\u03c3 2\nj ) random variable.\n\nskewness and kurtosis\nthe third and fourth cumulants of y are called its skewness, \u03ba3, and kurtosis, \u03ba4.\nexample 2.32 showed that \u03ba3 = \u03ba4 = 0 for normal variables. this suggests that they\nbe used to assess the closeness of a variable to normality. however, they are not\ninvariant to changes in the scale of y , and the standardized skewness \u03ba3/\u03ba 3/2\nand\nstandardized kurtosis \u03ba4/\u03ba 2\n2 are used instead for this purpose; small values suggest\nthat y is close to normal.\n\n2\n\nsome authors define the\nkurtosis to be \u03ba4 + 3\u03ba 2\nour notation.\n\n2 , in\n\nthe average y of a random sample of observations, each with cumulant-generating\n\u22121\u03ba2. expression (2.31) shows that the\nfunction k (t), has mean and variance \u03ba1 and n\nrandom variable zn = n1/2\u03ba\n(y \u2212 \u03ba1), which is asymptotically standard normal,\nhas cumulant-generating function\n\n\u22121/2\n2\n(cid:3)\n\nnk\n\nn\n\n\u22121/2\u03ba\n\n\u22121/2\n2\n\nt\n\n(cid:4) \u2212 n1/2\u03ba\n\n\u22121/2\n2\n\n\u03ba1t,\n\nand this equals\n\n(cid:14)\n\nn\n\nt\n\nn1/2\n\n\u03ba1\n\u03ba 1/2\n2\n\n+ 1\n2\n\nt 2\nn\n\n\u03ba2\n\u03ba2\n\n+ 1\n6\n\nt 3\nn3/2\n\n\u03ba3\n\u03ba 3/2\n2\n\n+ 1\n24\n\nt 4\nn2\n\n\u03ba4\n\u03ba 2\n2\n\n+ o\n\n(cid:10)(cid:15)\n\n(cid:9)\n\nt 4\nn2\n\n\u2212 n1/2t\n\n.\n\n\u03ba1\n\u03ba 1/2\n2\n\nafter simplification we find that the cumulant-generating function of zn is\n\n(cid:9)\n\n(cid:10)\n\n1\n2\n\nt 2 + 1\n3\n\n\u22121/2t 3\n\nn\n\n\u03ba3\n\u03ba 3/2\n2\n\n+ 1\n24\n\n\u22121t 4\n\nn\n\n\u03ba4\n\u03ba 2\n2\n\n+ o\n\nt 4\nn\n\nhence convergence of the cumulant-generating function of zn to 1\ncontrolled by the standardized skewness and kurtosis \u03ba3/\u03ba 3/2\n\nand \u03ba4/\u03ba 2\n2 .\n\n2\n\n.\n\n(2.32)\n\n2 t 2 as n \u2192 \u221e is\n\nexample 2.35 (poisson distribution) let y1, . . . ,y n be independent poisson\nobservations with means \u00b51, . . . , \u00b5n. the moment-generating function of y j\nis\nexp{\u00b5 j (et \u2212 1)}, soits cumulant-generating function is k j (t) = \u00b5 j (et \u2212 1) and all\n(cid:5)\nits cumulants equal \u00b5 j . asthe cumulant-generating function of y1 + \u00b7\u00b7\u00b7 + yn is\n\n\u00b5 j (et \u2212 1), the sum\nj\nnow suppose that all the \u00b5 j equal \u00b5, say. from (2.31), the cumulant-generating\n\ny j has a poisson distribution with mean\n\n(cid:5)\n\n(cid:5)\n\n\u00b5 j .\n\n(cid:6)\n\nfunction of the standardized average, n1/2\u00b5\u22121/2(y \u2212 \u00b5), is\n\u22121/2 \u2212 1\ntr\n\n(cid:7) \u2212 t(n\u00b5)1/2 = n\u00b5\n= n\u00b5\n\nt(n\u00b5)\n\n\u22121/2\n\nnk\n\n(cid:6)\n\u221e(cid:1)\net(n\u00b5)\nr=2\n\n.\n\n(n\u00b5)r/2r!\n\n(cid:7) \u2212 t(n\u00b5)1/2\n\n "}, {"Page_number": 59, "text": "2.4 \u00b7 moments and cumulants\n\n47\n\u22121; ingeneral\n\u2212(r\u22122)/2 for r = 2, 3, . . . hence y approaches normality for fixed \u00b5 and\n(cid:1)\n\nthus y has standardized skewness and kurtosis (n\u00b5)\n\u03bar = (n\u00b5)\nlarge n or fixed n and large \u00b5.\n\n\u22121/2 and (n\u00b5)\n\nvector case\na vector random variable y = (y1, . . . ,y p)t has moment-generating function m(t) =\ne(et ty ), where t t = (t1, . . . ,t p). the joint moments of the yr are the derivatives\n\n(cid:3)\n\n\u00b7\u00b7\u00b7 y r p\n\np\n\ny r1\n1\n\ne\n\n(cid:4) = \u2202r1+\u00b7\u00b7\u00b7+r p m(t)\n\u00b7\u00b7\u00b7\u2202 tr p\n\n\u2202tr1\n1\n\np\n\n(cid:27)(cid:27)(cid:27)(cid:27)(cid:27)\n\n.\n\nt=0\n\nthe cumulant-generating function is again k (t) = log m(t), and the joint cumulants\nof the yr are given by mixed partial derivatives of k (t) with respect to the elements\nof t. for example, the covariance matrix of y is the p \u00d7 p symmetric matrix whose\n(r, s) element is \u03bar,s = \u2202 2 k (t)/\u2202tr \u2202ts, evaluated at t = 0.\nsuppose that y = (y1, y2)t, and that the scalar random variables y1 and y2 are\nindependent. then their joint cumulant-generating function is\n\nk (t) = log e{exp(t1y1 + t2y2)} = log e{exp(t1y1)} + log e{exp(t2y2)} ,\n\nbecause the moment-generating function of independent variables factorizes. but\nsince every mixed derivative of k (t) equals zero, all the joint cumulants of y1 and y2\nequal zero also. this observation generalizes to several variables: the joint cumulants\nof independent random variables are all zero. this is not true for moments, and partly\nexplains why cumulants are important in statistical work.\n\np(cid:1)\nr=1\n\nexample 2.36 (multinomial distribution) the probability density of a multi-\nnomial random variable y = (y1, . . . ,y p)t with denominator m and probabilities\n\u03c0 = (\u03c01, . . . , \u03c0 p), that is pr(y1 = y1, . . . ,y p = yp), equals\n\n\u00b7\u00b7\u00b7\u03c0 yp\np ,\n\n\u03c0 y1\n1\n\nyr = 0, 1, . . . ,m ,\n\nyr = m;\n\ny1!\u00b7\u00b7\u00b7 yp!\n\nm!\n\n(cid:5)\n\nr\n\nnote that \u03c0r \u2265 0,\n\u03c0r = 1. this arises when m independent observations take values\nin one of p categories, each falling into the rth category with probability \u03c0r . then\nyr is the total number falling into the rth category. if y1, . . . ,y p are independent\npoisson variables with means \u00b51, . . . , \u00b5 p, then their joint distribution conditional\non y1 + \u00b7\u00b7\u00b7 + y p = m is multinomial with denominator m and probabilities \u03c0r =\n\u00b5r /\n\n(cid:5)\n\n\u00b5s.\n\nthe moment-generating function of y is\n\u00b7\u00b7\u00b7\u03c0 yp\n\n(cid:1) m!\n\n(cid:4) =\n\n(cid:3)\n\ne\n\net ty\n\ny1!\u00b7\u00b7\u00b7 yp!\n\n\u03c0 y1\n1\n\np ey1t1+\u00b7\u00b7\u00b7+ypt p = (\u03c01et1 + \u00b7\u00b7\u00b7 +\u03c0 pet p )m;\n\n(cid:5)\nr yr = m.\nthe sum is over all vectors (y1, . . . , yp)t of non-negative integers such that\nthus k (t) = m log(\u03c01et1 + \u00b7\u00b7\u00b7 +\u03c0 pet p ). it follows that the joint cumulants of the\n\njoint derivatives are not\nneeded to obtain first\ncumulants, which are not\njoint cumulants.\n\n "}, {"Page_number": 60, "text": "48\n\n2 \u00b7 variation\n\nelements of y are\n\u03bar = m\u03c0r ,\n\u03bar,s = m (\u03c0r \u03b4rs \u2212 \u03c0r \u03c0s) ,\n\u03bar,s,t = m (\u03c0r \u03b4rst \u2212 \u03c0r \u03c0s \u03b4rt [3] + 2\u03c0r \u03c0s \u03c0t ) ,\n\u03bar,s,t,u = m {\u03c0r \u03b4rstu \u2212 \u03c0r \u03c0s (\u03b4rt \u03b4su[3] + \u03b4stu[4]) + 2\u03c0r \u03c0s \u03c0t \u03b4ru[6] \u2212 6\u03c0r \u03c0s \u03c0t \u03c0u} ;\nhere a kronecker delta symbol such as \u03b4rst equals 1 if r = s = t and 0 otherwise,\nand a term such as \u03c0r \u03c0s \u03b4rt [3] indicates \u03c0r \u03c0s \u03b4rt + \u03c0s \u03c0t \u03b4rs + \u03c0r \u03c0t \u03b4st . the value of\n\u03bar,s implies that components of y are negatively correlated, because a large value for\none entails low values for the rest. zero covariance occurs only if \u03c0r = 0, in which\n(cid:1)\ncase yr is constant.\n\nexercises 2.4\n1\n\n1\n\n3\n\n4\n\n2\n\n1\n\n3\n\n\u00b5(cid:11)\n\n\u00b5(cid:11)\n\n\u2212 3\u00b5(cid:11)\n\n\u2212 4\u00b5(cid:11)\n\n\u2212 3(\u00b5(cid:11)\n\n+ 2(\u00b5(cid:11)\n\n2)2 + 12\u00b5(cid:11)\n\n1)3, \u03ba4 = \u00b5(cid:11)\n\nshow that the third and fourth cumulants of a scalar random variable in terms of its\nmoments are\n\u03ba3 = \u00b5(cid:11)\n\nshow that the cumulant-generating function for the gamma density (2.7) is \u2212\u03ba log(1 \u2212\nt /\u03bb). hence show that \u03bar = \u03ba(r \u2212 1)!/\u03bbr , and confirm the mean, variance, skewness and\nkurtosis in examples 2.12 and 2.26.\nif y1, . . . ,y n are independent gamma variables with parameters \u03ba1, . . . , \u03ban and the same\n\u03bb, show that their sum has a gamma density, and give its parameters.\nthe cauchy density (2.16) has no moment-generating function, but its characteristic\nfunction is e(eity ) = exp(it \u03b8 \u2212 |t|), where i 2 = \u22121. show that the average y of a random\nsample y1, . . . ,y n of such variables has the same characteristic function as y1. what does\nthis imply?\n\n1)2 \u2212 6(\u00b5(cid:11)\n\n2(\u00b5(cid:11)\n\n1)4.\n\n2\n\n3\n\n2.5 bibliographic notes\n\nthe idea that variation observed around us can be represented using probability mod-\nels provides much of the motivation for the study of probability theory and underpins\nthe development of statistics. cox (1990) and lehmann (1990) give complementary\ngeneral discussions of statistical modelling and a glance at any statistical library will\nreveal hordes of books on specific topics, references to some of which are given in\nsubsequent chapters. real data, however, typically refuse to conform to neat proba-\nbilistic formulations, and for useful statistical work it is essential to understand how\nthe data arise. initial data analysis typically involves visualising the observations in\nvarious ways, examining them for oddities, and intensive discussion to establish what\nthe key issues of interest are. this requires creative lateral thinking, problem solving,\nand communication skills. chatfield (1988) gives very useful discussion of this and\nrelated topics.\n\nj. w. tukey and his co-workers have played an important role in stimulating devel-\nopment of approaches to exploratory data analysis both numerical and graphical; see\ntukey (1977), mosteller and tukey (1977), and hoaglin et al. (1983, 1985, 1991).\n\nthis demands nodding\nacquaintance with\ncharacteristic functions.\n\njohn wilder tukey\n(1915\u20132000) was\neducated at home and then\nstudied chemistry and\nmathematics at brown\nuniversity before\nbecoming interested in\nstatistics during the\n1939\u201345 war, at the end of\nwhich he joined princeton\nuniversity. he made\nimportant contributions to\nareas including time\nseries, analysis of\nvariance, and\nsimultaneous inference.\nhe underscored the\nimportance of data\nanalysis, computing,\nrobustness, and\ninteraction with other\ndisciplines at a time when\nmathematical statistics\nhad become somewhat\nintroverted, and invented\nmany statistical terms and\ntechniques. see fernholtz\nand morgenthaler (2000).\n\n "}, {"Page_number": 61, "text": "2.6 \u00b7 problems\n\nfigure 2.6 match the\nsample to the density.\nupper panels: four\ndensities compared to the\nstandard normal (heavy).\nlower panels: normal\nprobability plots for\nsamples of size 100 from\neach density.\n\n4\n0\n\n.\n\n3\n0\n\n.\n\nf\nd\np\n\n2\n0\n\n.\n\n1\n0\n\n.\n\n0\n0\n\n.\n\n8\n\n6\n\n4\n\n2\n\n0\n\n2\n-\n\ny\n\na\n\n4\n0\n\n.\n\n3\n0\n\n.\n\nf\nd\np\n\n2\n0\n\n.\n\n1\n0\n\n.\n\n0\n0\n\n.\n\nb\n\n4\n0\n\n.\n\n3\n0\n\n.\n\nf\nd\np\n\n2\n0\n\n.\n\n1\n0\n\n.\n\n0\n0\n\n.\n\nc\n\n4\n0\n\n.\n\n3\n0\n\n.\n\nf\nd\np\n\n2\n0\n\n.\n\n1\n0\n\n.\n\n0\n0\n\n.\n\n49\n\nd\n\n-4 -2\n\n0\n\n2\n\n4\n\n6\n\n8 10\n\n-4 -2\n\n0\n\n2\n\n4\n\n6\n\n8 10\n\n-4 -2\n\n0\n\n2\n\n4\n\n6\n\n8 10\n\n-4 -2\n\n0\n\n2\n\n4\n\n6\n\n8 10\n\ny\n\ny\n\ny\n\ny\n\n\u2022 \u2022\n\n1\n\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\n\u2022\n\n2\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\n\u2022 \u2022\u2022\u2022\u2022\u2022\n\n\u2022\n\n\u2022\n\ny\n\n8\n\n6\n\n4\n\n2\n\n0\n\n2\n-\n\n\u2022\n\n\u2022\n\n\u2022\n\ny\n\n8\n\n6\n\n4\n\n2\n\n0\n\ny\n\n2\n-\n\n\u2022\n\n3\n\n\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\n8\n\n6\n\n4\n\n2\n\n0\n\n2\n-\n\n4\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\n\n\u2022 \u2022\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\nquantiles of standard normal\n\nquantiles of standard normal\n\nquantiles of standard normal\n\nquantiles of standard normal\n\ntwo excellent books on statistical graphics are cleveland (1993, 1994), while tufte\n(1983, 1990) gives more general discussions of visualizing data. for a brief account\nsee cox (1978).\n\ncox and snell (1981) give an excellent general account of applied statistics.\nmost introductory texts on probability and random processes discuss the main\nconvergence results; see for example grimmett and stirzaker (2001). bickel and\ndoksum (1977) give a more statistical account; see their page 461 for a proof of\nslutsky\u2019s lemma. see also knight (2000).\n\narnold et al. (1992) give a full account of order statistics and many further\n\nreferences.\n\nmost elementary statistics texts do not describe cumulants despite their usefulness.\nmccullagh (1987) contains forceful advocacy for them, including powerful methods\nfor cumulant calculations. see also kendall and stuart (1977), whose companion\nvolumes (kendall and stuart, 1973, 1976) overlap considerably with parts of this\nbook, from a quite different viewpoint.\n\n2.6 problems\n\npin the tail on the density.\n\n1 figure 2.6 shows normal probability plots for samples from four densities. which goes\n\nwith which?\n\n(cid:3)\n\n2 suppose that conditional on \u00b5, x and y are independent poisson variables with means\n\u2212\u03bb\u00b5/ \u0001(\u03bd), \u00b5 >0,\n\n\u00b5, but that \u00b5 is a realization of random variable with density \u03bb\u03bd \u00b5\u03bd\u22121e\n(cid:4) = \u03bb\u03bd{\u03bb \u2212 (es \u2212 1) \u2212 (et \u2212 1)}\u2212\u03bd ,\n\u03bd, \u03bb > 0. show that the joint moment-generating function of x and y is\n\nand hence find the mean and covariance matrix of (x, y ). what happens if \u03bb = \u03bd/\u03be and\n\u03bd \u2192 \u221e?\n3 show that a binomial random variable r with denominator m and probability \u03c0 has\ncumulant-generating function k (t) = m log(1 \u2212 \u03c0 + \u03c0et ). find lim k (t) asm \u2192 \u221e and\n\nes x+ty\n\ne\n\n "}, {"Page_number": 62, "text": "50\n\n4\n\n2 \u00b7 variation\n\n\u03c0 \u2192 0 insuch a way that m\u03c0 \u2192 \u03bb >0. show that\npr(r = r) \u2192 \u03bbr\nr!\n\ne\n\n\u2212\u03bb,\n\nand hence establish that r converges in distribution to a poisson random variable. this\nyields the poisson approximation to the binomial distribution, sometimes called the law\nof small numbers. for a numerical check in the s language, try\n\ny <-0:10;\nround(cbind(y,pbinom(y,size=m,prob=p),ppois(y,lambda)),digits=3)\n\nlambda <- 1; m <- 10; p <- lambda/m\n\nwith various other values of m and \u03bb.\n(a) let x be the number of trials up to and including the first success in a asequence\nof independent bernoulli trials having success probability \u03c0. show that pr(x = k) =\n\u03c0(1 \u2212 \u03c0)k\u22121, k = 1, 2, . . ., and deduce that x has moment-generating function \u03c0et /{1 \u2212\n(1 \u2212 \u03c0)et}; hence find its mean and variance. x has the geometric distribution.\n(b) now let yn be the number of trials up to and including the nth success in such a\nsequence of trials. show that\npr(yn = k) =\n\nk = n, n + 1, . . .;\n\n\u03c0 n(1 \u2212 \u03c0)k\u2212n,\n\n(cid:10)\n\n(cid:9)\nk \u2212 1\nn \u2212 1\n\nthis is the negative binomial distribution. find the mean and variance of yn, and show\nthat as n \u2192 \u221e the sequence {yn} satisfies the conditions of the central limit theorem.\ndeduce that\n\n(cid:9)\n\n(cid:10)\nk + n \u2212 1\nn \u2212 1\n\n1\n2k\n\nn(cid:1)\nk=0\n\n= 1.\n\nn\u2192\u221e 21\u2212n\n\nlim\n\n(c) find the limiting cumulant-generating function of \u03c0yn/(1 \u2212 \u03c0) as\u03c0 \u2192 0, and hence\nshow that the limiting distribution is gamma.\n\n5 let y1, . . . ,y n be a random sample from a distribution with mean \u00b5 and variance \u03c3 2. find\n\nthe mean of\n\nt =\n\n1\n\n2n(n \u2212 1)\n\n(y j \u2212 yk)2,\n\n(cid:1)\nj(cid:13)=k\n\nand by writing y j \u2212 yk = y j \u2212 y \u2212 (yk \u2212 y ), show that t = s2.\n, \u03b8 + 1\n1\n2\ny(1), is\n\n6 let y1, . . . ,y n be a random sample from the uniform distribution on the interval (\u03b8 \u2212\n2 ). show that the joint density of the sample maximum and minimum, y(n) and\n\nfy(1),y(n)(u, v) = n(n \u2212 1)(v \u2212 u)n\u22122,\n\n\u03b8 \u2212 1\n2\n\n< u < v < \u03b8 + 1\n2\n\n.\n\nthe sample range is r = y(n) \u2212 y(1), and a natural estimator of \u03b8 is the midrange, t =\n(y(n) + y(1))/2. show that the conditional density of t given r is\n\nf (t | r; \u03b8) = (1 \u2212 r)\n\n\u22121,\n\n0 < r < 1, \u03b8 + 1\n2\n\n\u2212 r\n2\n\n> t > \u03b8 \u2212 1\n2\n\n+ r\n2\n\n.\n\nhow precisely is \u03b8 determined by this density as r \u2192 0 and r \u2192 1?\n7 a random variable x with the weibull distribution with index \u03b1 has distribution function\n1 \u2212 exp{\u2212(x/\u03bb)\u03b1}, x > 0, \u03bb, \u03b1 > 0. the idea that a system with many similar components\nwill fail when the weakest component fails has led to widespread use of this distribution\nin industrial reliability.\n(a) suppose that x1, . . . , xn are independent identically distributed continuous non-\nnegative random variables such that as t \u2192 0, the density and distribution functions are\nasymptotically at \u03ba\u22121 and at \u03b1/\u03b1 respectively, where a, \u03b1 > 0. let y = min(x1, . . . , xn)\n\nwaloddi weibull\n(1887\u20131979) was a\nswedish engineer who in\n1937 published the\ndistribution that bears his\nname; it is widely used in\nreliability.\n\n "}, {"Page_number": 63, "text": "2.6 \u00b7 problems\n\n51\nand let w = (a/\u03b1)1/\u03b1n1/\u03b1y . show that as n \u2192 \u221e, w has as its limiting distribution the\nweibull distribution with index \u03b1.\n(b) explain why a probability plot for the weibull distribution may be based on plotting\nthe logarithm of the rth order statistic against log{\u2212 log(1 \u2212 r\nintercept of such a plot. check whether the data in table 1.2 follow weibull distributions.\n\nn+1 )}, and give the slope and\n\n8 let y1, . . . ,y 2m+1 be a random sample from the uniform density\n\n(cid:2)\n\nf (y) =\n\n\u03b8\u22121, 0 \u2264 y \u2264 \u03b8,\notherwise.\n0,\n\nderive the density function of the sample median t = y(m+1) and find its exact mean and\nvariance.\nfind the density function of z = 2(2m + 3)1/2(y(m+1) \u2212 \u03b8/2)/\u03b8 and use stirling\u2019s formula\nto show directly that, as m \u2192 \u221e, z has asymptotically a standard normal distribution.\ndeduce that asymptotically var(t ) \u223c 3var(y ).\n9 the coefficient of variation of a random sample y1, . . . ,y n is c = s/y , where y and\ns2 are the sample average and variance. it estimates the ratio \u03c8 = \u03c3/\u00b5 of the standard\ndeviation relative to the mean. show that\n\n(cid:9)\n\n(cid:10)\n\n.= \u03c8,\n\ne(c)\n\nvar(c)\n\n.= n\n\n\u22121\n\n\u03c8 4 \u2212 \u03b33\u03c8 3 + 1\n4\n\n\u03b34\u03c8 2\n\n+ \u03c8 2\n\n2(n \u2212 1)\n\n.\n\n\u22121\n\n(cid:5)\n\n10 if t1 and t2 are two competing estimators of a parameter \u03b8, based on a random sample\ny1, . . . ,y n, the asymptotic efficiency of t1 relative to t2 is limn\u2192\u221e var(t2)/var(t1) \u00d7\n100%. if n = 2m + 1, find the asymptotic efficiency of the sample median y(m+1) relative\nto the average y = n\nj y j when the density of the y j is: (a) normal with mean \u03b8 and\n\u22121 exp{\u2212|y \u2212 \u03b8|/\u03c3} for \u2212\u221e < y < \u221e; and (c) cauchy,\nvariance \u03c3 2; (b) laplace, (2\u03c3 )\n\u03c3/[\u03c0{\u03c3 2 + (y \u2212 \u03b8)2}] for \u2212\u221e < y < \u221e.\n11 show that the covariance matrix for the multinomial distribution may be written\nm(diag{\u03c0} \u2212\u03c0 \u03c0 t), and deduce that it has determinant zero. explain why the distribution\nis degenerate.\n\n12 (a) if x has the n (\u00b5, \u03c3 2) distribution, show that x 2 has cumulant-generating function\n\nt \u00b52/(1 \u2212 2t \u03c3 2) \u2212 1\n2\n\nlog(1 \u2212 2t \u03c3 2).\n\n(b) if x1, . . . , x \u03bd are independent normal variables with variance \u03c3 2 and means\n\u00b51, . . . , \u00b5\u03bd, show that the cumulant-generating function of w = x 2\n\n+ \u00b7\u00b7\u00b7 + x 2\n\u03bd is\n\n1\n\nt \u03b42\u03c3 2/(1 \u2212 2t \u03c3 2) \u2212 \u03bd\n2\n\nlog(1 \u2212 2t \u03c3 2),\n\nwhere \u03b42 = (\u00b52\n\u03bd)/\u03c3 2. the distribution of w/\u03c3 2 is said to be non-central chi-\nsquared with \u03bd degrees of freedom and non-centrality parameter \u03b42. show that the moment-\ngenerating function of w may be written\n\n1\n\n+ \u00b7\u00b7\u00b7 + \u00b52\n(cid:2)\n\n(cid:23)\n\nexp\n\nand that this equals\n\n(1 \u2212 2t \u03c3 2)\n\n\u2212\u03bd/2,\n\n\u03b42(1 \u2212 2t \u03c3 2)\n\u22121\n(cid:9)\n\n(cid:10)r\n\n\u2212 1\n2\n\n\u03b42 + 1\n2\n\n\u221e(cid:1)\nr=0\n\n1\nr!\n\n\u2212\u03b42/2\n\ne\n\n(1 \u2212 2t \u03c3 2)\n\n\u2212r\u2212\u03bd/2.\n\n\u03b42\n2\n\n(2.33)\n\nuse (2.33) and (3.10) to write down an expression for the density of w .\n(c) hence deduce that (i) w d= w\u03bd + w2n , where w \u223c \u03c3 2\u03c7 2\nand (ii) w d= (\u03b4\u03c3 + y1)2 + y 2\n\n\u03bd independent of w2n \u223c\n0 taking value 0 with unit probability, and n is poisson with mean \u03b42/2,\n\n+ \u00b7\u00b7\u00b7 + y 2\n\u03bd .\n\n2n , with \u03c7 2\n\n\u03c3 2\u03c7 2\n\n2\n\n "}, {"Page_number": 64, "text": "3\n\nuncertainty\n\nin the previous chapter we saw how variation arises in data generated by a model. we\nnow confront a central issue: how to transform knowledge of this variation into state-\nments about the uncertainty surrounding the model parameters. uncertainty is a more\nelusive concept than variation and there is more disagreement about how it should\nbe expressed. one important approach described in chapter 11 uses bayes\u2019 theorem to\nconvert prior knowledge into posterior uncertainty, conditional on the data observed.\nthe route taken below is more common in applications, and is usually known as the\nfrequentist, repeated sampling, orclassical approach. the next section describes how\nuncertainty may be expressed in terms of confidence intervals. in practice confidence\nintervals are usually approximate, but exact inferences are possible from some central\nmodels derived from the normal distribution, and these are described in the following\nsection, followed by a brief summary of methods for prediction. there follows an\nintroduction to the use of simulated data to appreciate both variation and uncertainty,\nfor example in assessing the quality of approximate confidence intervals.\n\n3.1 confidence intervals\n3.1.1 standard errors and pivots\nin section 2.2 we saw that many statistics approach limiting distributions in large\nsamples. in practice a sample size is never infinite, but nevertheless these limits may\nbe used to help quantify uncertainty. suppose that t is an estimator of a parameter\n\u03c8 based on a random sample y1, . . . ,y n, that its unknown variance var(t ) has form\n\u03c4 2/n, and that nv is a consistent estimator of \u03c4 2, sonv p\u2212\u2192 \u03c4 2 as n \u2192 \u221e. statements\n\u22121/2\u03c4 , but\nof uncertainty about an estimator often involve its standard deviation n\nusually \u03c4 is unknown and must be estimated. an estimated standard deviation is\nknown as a standard error, so v 1/2 is a standard error for t .\n\nexample 3.1 (average) suppose each of the y j has mean \u00b5 and variance \u03c3 2. the\nsample average, y , has mean \u00b5 and variance \u03c3 2/n. now s2 = (n \u2212 1)\n(y j \u2212 y )2\nis an estimator of \u03c3 2, so v 1/2 = n\n(cid:1)\n\n\u22121/2s is a standard error for y .\n\n\u22121\n\n(cid:1)\n\n52\n\n "}, {"Page_number": 65, "text": "3.1 \u00b7 confidence intervals\n\n53\n\nexample 3.2 (gamma shape)\nin example 2.26 we saw that the shape parameter\n\u03ba of a gamma random sample may be estimated by t = y 2/s2, and that this es-\ntimate has variance approximately 2\u03ba(\u03ba + 1)/n, which may itself be estimated by\nv = 2t (t + 1)/n. we saw also that for the n = 95 observations in example 2.3,\ny2/s2 = 3.15. it follows that a standard error for this estimate is\n\n{2 \u00d7 3.15(3.15 + 1)/95}1/2 = 0.52.\n\n(cid:1)\n\nin statements of uncertainty for an unknown parameter \u03c8, acentral role is played\nby a pivot \u2014 afunction of the data and the parameter whose distribution is known.\nin the discussion below we must distinguish a generic value of \u03c8 from its true but\nunknown value \u03c80. the condition that a quantity z(\u03c80) be pivotal means that for\neach z pr{z(\u03c80) \u2264 z} is the same for every \u03c80; that is, the distribution of z(\u03c80) does\nnot depend on \u03c80.\n\nexample 3.3 (exponential sample) let y1, . . . ,y n be a random sample from the\nexponential distribution 1 \u2212 exp(\u2212y/\u03c80), y > 0, where \u03c80 > 0 isunknown. then\ny j /\u03c80 has distribution function\n\npr(y j /\u03c80 \u2264 u) = pr(y j \u2264 u\u03c80) = 1 \u2212 exp(\u2212u),\n\n0\n\n(cid:1)\n\nwhich is known, even though the distribution of y j\nitself is not. each of the\ny j /\u03c80 has this same distribution, and they are independent, so the distribution of\nz(\u03c80) = \u03c8\u22121\ny j is known, at least in principle. in fact the density of z(\u03c80) is\nzn\u22121 exp(\u2212z)/(n \u2212 1)! for z > 0; this is the gamma density (2.7) with parameters\n\u03bb = 1 and \u03ba = n. asn is known, every property of the distribution of z(\u03c80) may be\n(cid:1)\nobtained.\nexact pivots are rare, but approximate ones are legion. for example, let z(\u03c80) =\n(t \u2212 \u03c80)/v 1/2 be based on a sample of size n, and suppose that the limiting distribu-\ntion of z(\u03c80) asn \u2192 \u221e is standard normal; the results of chapter 2 suggest that this\nwill often be the case if t is based on averages. then if n is large, z(\u03c80) isroughly\nstandard normal, and so is an approximate pivot. now\n\u2264 z\n\npr{z(\u03c80) \u2264 z} = pr\n\n.= \u0001(z),\n\n(cid:2)\n\n(cid:3)\n\nwhere \u0001 is the standard normal distribution function. then\n\nwhere z\u03b1 is the \u03b1 quantile of this distribution, that is, \u0001(z\u03b1) = \u03b1. equivalently\n\n(cid:4)\n\nt \u2212 v 1/2z1\u2212\u03b1 \u2264 \u03c80 \u2264 t \u2212 v 1/2z\u03b1\n\npr\n\n.= 1 \u2212 2\u03b1,\n\n(cid:5) .= 1 \u2212 2\u03b1.\n\n(3.1)\n\n(3.2)\n\nhence the random interval whose endpoints are\n\nt \u2212 v 1/2z1\u2212\u03b1,\n\nt \u2212 v 1/2z\u03b1\n\n(3.3)\ncontains \u03c80 with probability approximately (1 \u2212 2\u03b1), whatever the value of \u03c80. this\ninterval is variously called an approximate (1 \u2212 2\u03b1) \u00d7 100% confidence interval for\n\nt \u2212 \u03c80\nv 1/2\n(cid:3)\n\n\u2264 z1\u2212\u03b1\n\n(cid:2)\n\npr\n\nz\u03b1 \u2264 t \u2212 \u03c80\n\nv 1/2\n\n "}, {"Page_number": 66, "text": "54\n\n3 \u00b7 uncertainty\n\u03c80 or a confidence interval for \u03c80 with approximate coverage probability (1 \u2212 2\u03b1);\nwe call it a (1 \u2212 2\u03b1) confidence interval for \u03c80. we regard the interval as random,\ncontaining \u03c80 with a specified probability. conventionally \u03b1 is a number such as 0.1,\n0.05, 0.025, or 0.005, corresponding to 0.8, 0.9, 0.95 and 0.99 confidence intervals\nfor \u03c80; these intervals will be increasingly wide. as z\u03b1 = \u2212z1\u2212\u03b1, (3.3) may be written\nt \u00b1 v 1/2z\u03b1. when 1 \u2212 2\u03b1 = 0.95, z\u03b1 = \u22121.96\n.= \u22122, so (3.3) is roughly t \u00b1 2v 1/2.\ngiven a particular set of data, y1, . . . , yn, wecalculate the confidence interval from\n(3.3) by replacing t and v with their observed values t and v; this gives t \u00b1 v 1/2z\u03b1.\nthis interval either does or does not contain \u03c80, though we do not know which in any\nparticular case. we interpret this by reference to a hypothetical infinite sequence of sets\nof data generated by the same mechanism or experiment that gave the data from which\nthe interval was calculated. we then argue that if the observed data had been selected\nat random from these sets of data, then the interval actually obtained could be regarded\nas being selected randomly from a sequence of intervals with the property (3.2), and in\nthis sense it would contain \u03c80 with probability (1 \u2212 2\u03b1). with this interpretation, on\naverage 19 out of every 20 confidence intervals with coverage 0.95 will contain \u03c80,\nand on average 99 out of every 100 intervals with coverage 0.99 will contain \u03c80, and\nso forth. such an interval will also contain other values of \u03c8, but we would like it to\nbe as short as possible on average, so that it does not contain too many of them.\n\nexample 3.4 (birth data) we use the data from example 2.3 to construct a 95% con-\nfidence interval for the population mean time in the delivery suite, \u00b50 hours, assuming\nthat the times for each day are a random sample y1, . . . ,y n from the population.\n(cid:1)\nan obvious choice of estimator t is the average, y , and we may take v to equal\n\u22121s2 = {n(n \u2212 1)}\u22121\n(y j \u2212 y )2. inthis case a (1 \u2212 2\u03b1) \u00d7 100% confidence in-\nn\nterval has endpoints y \u00b1 n\n\u22121/2sz\u03b1, and if (1 \u2212 2\u03b1) = 0.95, then \u03b1 = 0.025 and\nz\u03b1 = \u22121.96. on day 1 there were n = 16 deliveries, with average y = 8.77 and\nsample variance s2 = 18.46, so a 95% confidence interval for \u00b50 based on these data\nis y \u00b1 n\n\n\u22121/2sz0.025 = (6.66, 10.87) hours.\n\nthe upper left panel of figure 3.1 shows 95% confidence intervals for \u00b50 based\non data for each of the first 20 days. the dotted line shows the average time in the\ndelivery suite for all three months of data, which should be close to \u00b50. the intervals\nvary in length and in location, with 18 of them containing the three-month average.\nwe expect about 19 of these 20 intervals to contain the true parameter, and the data\nseem consistent with this.\n\nthe upper right panel illustrates the calculation of the confidence interval from\nthe day 1 data. the horizontal axis shows values of \u00b5, and the diagonal line shows\nthe function z(\u00b5) = (8.77 \u2212 \u00b5)/(18.46/16)1/2. the confidence interval is obtained\nby reading off those values of \u00b5 for which z(\u00b5) = z0.025, z0.975 = \u00b11.96, and these\nare shown by the vertical dashed lines, values of \u00b5 between which lie in the interval.\nother values of y and s2 that might have been observed would give different func-\ntions z(\u00b5) = (y \u2212 \u00b5)/(s2/n)1/2. the lower right panel shows the observed values\nz(\u00b5) ofthese for each of the first ten days of data. an infinite number of days would\ninduce a probability density for z(\u00b50), corresponding to the points where the solid\n\n "}, {"Page_number": 67, "text": "figure 3.1 confidence\nintervals for the mean\ntime in the delivery suite.\nupper left: 95%\nconfidence intervals\ncalculated using each of\nthe first 20 days of data,\nwith the average time for\nthree months (92 days) of\ndata (dots). upper right:\nz(\u00b5) = (y \u2212 \u00b5)/(s2/n)1/2\nas a function of \u00b5 for the\ndata from day 1 (diagonal\nline). the dotted lines\nshow z0.025 = \u22121.96 and\nz0.975 = 1.96, from which\nthe confidence interval is\nread off by solving\nz(\u00b5) = \u00b11.96. lower\nright: lines z(\u00b5) for ten\ndifferent samples; their\nintersections z(\u00b50) with\nthe vertical line at \u00b50\n(blobs) have the standard\nnormal density shown. if\n\u00b50 were different, the\ndensity would be\ntranslated in the\nx-direction but remain\nunchanged, because\nz(\u00b50) is a pivot. lower\nleft: proportion of all 92\n95% confidence intervals\nthat include different\nvalues of \u00b5. the vertical\nline (dots) shows the most\nlikely value of \u00b50, where\nthe coverage probability\nshould be 0.95, given by\nthe horizontal line\n(dashes).\n\n3.1 \u00b7 confidence intervals\n\n55\n\n0\n2\n\n5\n1\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\ny\na\nd\n\n0\n1\n\n\u2022\n\n\u2022\n\n5\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n)\nu\nm\n(\nz\n\n4\n\n2\n\n0\n\n2\n-\n\n4\n-\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n4\n\n6\n\n8\n\n10\n\n12\n\n14\n\n16\n\n4\n\n6\n\n8\n\n10\n\n12\n\n14\n\n16\n\nhours in delivery suite\n\nmu\n\ny\nt\ni\nl\ni\n\nb\na\nb\no\nr\np\n\n0\n\n.\n\n1\n\n8\n0\n\n.\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n)\nu\nm\n(\nz\n\n4\n\n2\n\n0\n\n2\n-\n\n4\n-\n\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\n\u2022\n\u2022\n\n4\n\n6\n\n8\n\n10\n\n12\n\n14\n\n16\n\n4\n\n6\n\n8\n\n10\n\n12\n\n14\n\n16\n\nhours in delivery suite\n\nmu\n\nvertical line intersects with the diagonal lines, and this density is illustrated also. if\n\u00b50 was equal to the three-month average of 7.93 hours, we would expect a proportion\n0.025 of the blobs at z(7.93) to lie outside \u00b11.96. exact pivotality of z(\u00b50) would\nmean that even if \u00b50 was not 7.93 hours, so that the density was shifted horizontally,\nit would not change shape. in fact the normal approximation is not perfect here, as\nwe shall see in example 3.6.\nwe can compute the probability that the confidence interval (3.3) contains any\nvalue of \u00b5. for\u00b5 0 this should be (1 \u2212 2\u03b1), but it will be lower for other values of \u00b5.\nthe lower left panel of figure 3.1 shows the proportion of the 92 separate daily\n95% confidence intervals containing each value of \u00b5. this shows the shape we would\nexpect: values close to the three-month average lie in most of the intervals, while\nvalues far from it are rarely covered. the corresponding proportions from an infinite\nnumber of days of data are the coverage probabilities\nt \u2212 z1\u2212\u03b1v 1/2 \u2264 \u00b5 \u2264 t \u2212 z\u03b1v 1/2\n\n(cid:6)(cid:6) true value is \u00b50\n\nif the approximation (3.2) was perfect, this probability would equal 0.95 when \u00b5 =\n\u00b50, but a poor approximation would give a probability different from 0.95. we would\n\npr\n\n(cid:5)\n\n(cid:4)\n\n.\n\n "}, {"Page_number": 68, "text": "56\n\n3 \u00b7 uncertainty\n\nhope that this function would be as peaked as possible, to reduce the probability that\na value other than \u00b50 is contained in the interval: we want the average length of the\n(cid:1)\nintervals to be as short as possible.\n\nexample 3.5 (binomial distribution)\nin opinion polls about the status of the\npolitical parties in the uk, m = 1000 people are typically asked about their voting\nsupposed binomial with probability \u03c0. anestimate of \u03c0 is (cid:7)\u03c0 = r/m, and since (cid:7)\u03c0\nintentions. let the number of these who support a particular party be denoted by r,\nhas variance \u03c0(1 \u2212 \u03c0)/m, the standard error of(cid:7)\u03c0 is {(cid:7)\u03c0(1 \u2212(cid:7)\u03c0)/m}1/2. example 2.17\ncombined with slutsky\u2019s lemma (2.15) implies that ((cid:7)\u03c0 \u2212 \u03c0)/{(cid:7)\u03c0(1 \u2212(cid:7)\u03c0)/m}1/2 con-\nverges in distribution to a standard normal variable, and consequently a (1 \u2212 2\u03b1)\nconfidence interval for \u03c0 has endpoints\n\n(cid:7)\u03c0 \u2212 z1\u2212\u03b1{(cid:7)\u03c0(1 \u2212(cid:7)\u03c0)/m}1/2, (cid:7)\u03c0 \u2212 z\u03b1{(cid:7)\u03c0(1 \u2212(cid:7)\u03c0)/m}1/2.\n\nfor the main parties.\n\nmentioned when the results of such a poll are reported. the margin depends little on\n\nfor the two main parties \u03c0 usually lies in the range 0.3\u20130.4, so suppose that(cid:7)\u03c0 = 0.35,\nm = 1000, and we want a 95% confidence interval for \u03c0, sothat z0.975 = \u2212z0.025 =\n.= 2. then as (0.35 \u00d7 0.65/1000)1/2 .= 0.015, the interval lies roughly 0.03 on\neither side of (cid:7)\u03c0. inpercentage terms this is the \u20183% margin of error\u2019 sometimes\n1.96\n(cid:7)\u03c0 because the function \u03c0(1 \u2212 \u03c0) is fairly flat over the usual range 0.2\u20130.5 of support\n(cid:1)\nthere are infinitely many confidence intervals with coverage (1 \u2212 2\u03b1), because\nwe can replace z1\u2212\u03b1 and z\u03b1 in (3.3) with any pair z1\u2212\u03b11, z\u03b12 such that \u03b11, \u03b12 \u2265 0\nand 1 \u2212 \u03b11 \u2212 \u03b12 = 1 \u2212 2\u03b1. the choice \u03b11 = \u03b12 = \u03b1 gives the equi-tailed intervals\ndiscussed above, and these are common in practice. other standard choices are \u03b11 =\n2\u03b1, \u03b12 = 0 or\u03b1 1 = 0, \u03b12 = 2\u03b1, which give one-sided intervals (t \u2212 v 1/2z1\u22122\u03b1,\u221e)\nor (\u2212\u221e, t \u2212 v 1/2z2\u03b1) respectively. these are appropriate when a lower or an upper\nconfidence bound is required for \u03c80. for example, insurance companies are interested\nin upper confidence bounds for potential losses, lower bounds being of little interest.\n\ncomplications\nin order not to obscure the main points, the discussion above has been deliberately\noversimplified. one complication is that realistic models rarely have just one para-\nmeter, so our notion of a pivot must be generalized.\n\nsuppose that in addition to \u03c8, the model has another parameter \u03bb whose value\nis not of interest, and that we seek to construct a confidence interval for \u03c80 using a\npivot z(\u03c80). our previous definition must be extended to mean that the distribution\nof z(\u03c80) depends neither on \u03c80 nor on \u03bb. this is a stronger requirement than before\nand harder to satisfy.\n\na second complication is that there may be several possible (approximate) pivots,\nso that some basis is needed for choosing the best of them. obviously we would\nlike a pivot whose distribution depends as little as possible on the parameters, and\npreferably one that is exact, but we should also like short confidence intervals and a\nreliable general procedure for obtaining them. we describe some such procedures in\n\n "}, {"Page_number": 69, "text": "3.1 \u00b7 confidence intervals\n\n57\n\nfigure 3.2 densities of\ntwo approximate pivots\nfor setting confidence\nintervals for the gamma\nmean, based on samples\nof size n = 15 from the\ngamma distribution. left\npanel: density estimates\nbased on 10,000 values of\nz1(\u00b50) =\nn1/2(y \u2212 \u00b50)/s, for shape\nparameter \u03ba = 2 (solid), 3\n(dots), 4 (dashes), with\nn (0, 1) density (heavy).\nright panel: density of\nz2(\u00b50) = y /\u00b50 for \u03ba = 2\n(line), 3 (dots) 4 (dashes).\n\ny\nt\ni\ns\nn\ne\nd\n\n5\n\n.\n\n0\n\n4\n\n.\n\n0\n\n3\n\n.\n\n0\n\n2\n\n.\n\n0\n\n1\n\n.\n\n0\n\n0\n\n.\n0\n\ny\nt\ni\ns\nn\ne\nd\n\n4\n\n3\n\n2\n\n1\n\n0\n\n-4\n\n-2\n\n0\n\nz1\n\n2\n\n4\n\n0.0\n\n0.5\n\n1.5\n\n2.0\n\n1.0\n\nz2\n\nchapter 4, and return to a general discussion in chapter 7. the following example\nillustrates some of the difficulties.\n\n1\n\nexample 3.6 (gamma distribution) a random variable y with gamma density\n(2.8) may be expressed as y = \u00b5x, where x has density (2.8) with \u00b5 = 1, that is,\nit has unit mean and shape parameter \u03ba. if y1, . . . ,y n is a sample from the gamma\ndensity with parameters \u00b50 and \u03ba, then\n\n(cid:8)\n\n(cid:8)\n\n=\n\nn(n\u22121)\n\nn(n\u22121)\n\n(cid:9)1/2\n\n(cid:9)1/2\n\nz1(\u00b50) =\n\nisindependent of\n\nx \u2212 1\n(cid:1)\n(x j \u2212 x)2\n\ny \u2212 \u00b50\n(cid:1)\n(y j \u2212 y )2\n\u00b50. asn \u2192 \u221e,\nand hence the distribution of z1(\u00b50)\nz1(\u00b50) d\u2212\u2192 n (0, 1), giving the confidence interval (3.3), but for any given n the\ndistribution of z1(\u00b50) depends on n and on \u03ba. estimates of this density for n = 16\nand \u03ba = 2, 3, and 4 are shown in the left panel of figure 3.2. the density seems\nstable over \u03ba, but it is skewed to the left compared to the limiting normal density.\nthus although z1(\u00b50) appears to be roughly pivotal, values of the normal quantiles\nz\u03b1 might not give good confidence bounds; this would chiefly affect the upper limit.\nanother possible pivot here is z2(\u00b50) = y /\u00b50 = x, which turns out to have the\ngamma density (2.8) with unit mean and shape parameter n\u03ba. let g\u03b1(n\u03ba) be the\u03b1\nquantile of this distribution. then\n\n,\n\n1\n\n1 \u2212 2\u03b1 = pr{g\u03b1(n\u03ba) \u2264 y /\u00b50 \u2264 g1\u2212\u03b1(n\u03ba)}\n\n= pr{y /g1\u2212\u03b1(n\u03ba) \u2264 \u00b50 \u2264 y /g\u03b1(n\u03ba)},\n\ngiving a (1 \u2212 2\u03b1) confidence interval (y/g1\u2212\u03b1(n\u03ba), y/g\u03b1(n\u03ba)) based on a sample\ny1, . . . , yn. inpractice \u03ba is unknown and must be replaced by an estimate (cid:7)\u03ba, so\nz2(\u00b50) isalso an approximate pivot.\nconsider the day 1 data for the delivery suite, for which n = 16, y = 8.77 and\nsuppose (cid:7)\u03ba = 3. with \u03b1 = 0.025 we find that g\u03b1(n(cid:7)\u03ba) = 0.737, g1\u2212\u03b1(n(cid:7)\u03ba) = 1.302.\n\nthis gives 95% confidence interval (6.74, 11.89) hours for \u00b50. this interval is longer\nthan that given by the pivot z1(\u00b50), (6.66, 10.87), and it is not symmetric about y.\n\n "}, {"Page_number": 70, "text": "58\n\n3 \u00b7 uncertainty\n\ndensities for z2(\u00b50) shown in the right panel of figure 3.2 depend much more on \u03ba\nthan those for z1(\u00b50). thus here we have a choice between two approximate pivots,\none which is close to pivotal but whose distribution can only be estimated, and another\n(cid:1)\nwhich is further from pivotal but whose quantiles are known.\n\ninterpretation\nthe repeated sampling basis for interpretation of confidence intervals is not univer-\nsally accepted. the central issue is whether or not hypothetical repetitions bear any\nrelevance to the data actually obtained. one view is that since every set of data is\nunique, such repetitions would be irrelevant even if they existed, and another basis\nmust be found for statements of uncertainty; see chapter 11. however it is reassuring\nthat intervals derived from different principles are often similar and sometimes iden-\ntical for standard problems, and in practice most users do not worry greatly about the\nprecise interpretation of the uncertainty measures they report. the essential point is\nto provide some assessment of uncertainty, as honest as possible.\n\nanother view is that the repeated sampling interpretation is secure provided the\nhypothetical data contain the same information, defined suitably, as the original data,\nbut that if the set of hypothetical datasets taken is too large then it is irrelevant to\nthe data actually observed. thus in the delivery suite example we might argue that as\nday 1 had 16 arrivals, the relevant hypothetical repetitions are for days with 16 arrivals,\nbecause to know the number of arrivals is informative about the precision of any\nparameter estimate, though not about its value.\n\n3.1.2 choice of scale\nthe delta method provides standard errors and limiting distributions for smooth\nfunctions of random variables. this poses a problem, however: on what scale\nshould a confidence interval for \u03c80 be calculated? for suppose that h is a mono-\ntone function, and that (l , u ) is a (1 \u2212 2\u03b1) confidence interval for h(\u03c80), that is,\npr{l \u2264 h(\u03c80) \u2264 u} .= 1 \u2212 2\u03b1. then, as\n\n\u22121(l) \u2264 \u03c80 \u2264 h\n\n\u22121(u )} .= 1 \u2212 2\u03b1,\n\npr{h\n\u22121(u )) is a (1 \u2212 2\u03b1) confidence interval for \u03c80. which of the\nthe interval (h\nmany possible transformations h should we use? sometimes the choice is suggested\nby the need to avoid intervals that contain silly values of \u03c8, as inthe following\nexample.\n\n\u22121(l), h\n\nexample 3.7 (binomial distribution) suppose that we want a 95% confidence\ninterval for the support \u03c0 for a small political party, based on a sample of m = 100\nindividuals. if (cid:7)\u03c0 = 0.02, the standard error is (0.02 \u00d7 0.98/100)1/2 = 0.014, so the\n95% interval, roughly (\u22120.008, 0.034), contains negative values of \u03c0.\nto avoid this, let us construct an interval for h(\u03c0) = log \u03c0 instead, so that\n(\u03c0) = \u03c0\u22121. nowlog (cid:7)\u03c0 = \u22123.91, with standard error(cid:7)\u03c0\u22121{(cid:7)\u03c0(1 \u2212(cid:7)\u03c0)/m}1/2 = 0.7.\n(cid:5)\nh\nhence the 95% interval for log \u03c0 is roughly \u22123.91 \u00b1 1.96 \u00d7 0.7, and the corre-\nsponding interval for \u03c0 is (exp(\u22123.91 \u2212 1.4), exp(3.91 + 1.4)) = (0.005, 0.08). the\n\n "}, {"Page_number": 71, "text": "3.1 \u00b7 confidence intervals\n\n59\n\ntable 3.1 exact mean\nand variance of\nvariance-stabilized form\ny 1/2 of poisson random\nvariable.\n\n\u03b8\n\n0.25\n\n0.5\n\n1\n\n2\n\n5\n\n10\n\n20\n\ne(y 1/2)\nvar(y 1/2)\n\n0.23\n0.20\n\n0.44\n0.31\n\n0.77\n0.40\n\n1.27\n0.39\n\n2.17\n0.29\n\n3.12\n0.26\n\n4.44\n0.26\n\ndistribution of r/m is too far from normal here to take this interval very seriously,\n(cid:1)\nbut atleast it contains only positive values.\na different approach is to choose a transformation for which var{h(t )} is roughly\nconstant, independent of \u03c8. let t be an estimator of \u03c8, and suppose that var(t ) =\n\u03c6v (\u03c8)/n, where \u03c6 is independent of \u03c8. the function v (\u03c8) iscalled the variance\nfunction of t . weaim to choose h such that\n\n1 \u221d var{h(t )} .= h\n\n(cid:5)\n\n(\u03c8)2var(t ) = h\n\n(cid:5)\n\n(\u03c8)2\u03c6v (\u03c8)/n,\n\nwhere the approximation results from the delta method. this implies that\n\n(cid:10) \u03c8\n\nh(\u03c8) \u221d\n\ndu\n\nv (u)1/2\n\n,\n\n(3.4)\n\nwhich is called the variance-stabilizing transformation for t .\n\nexample 3.8 (poisson distribution) the mean and variance of the poisson density\n(2.6) are both \u03b8, sothe average of a random sample of n such variables has mean\nis h(\u03b8) = (cid:11) \u03b8 u\n\u03b8 and variance \u03b8/n, giving v (\u03b8) = \u03b8 and \u03c6 = 1. the variance-stabilizing transform\n\u22121/2 du \u221d \u03b8 1/2; the constant of proportionality is irrelevant. the delta\n.= 0.25. the exact mean and variance of y 1/2 are given in\nmethod gives var(y 1/2)\ntable 3.1. variance-stabilization does not work perfectly, but var(y 1/2) depends much\nless on \u03b8 than var(y ) does.\nto apply this to the birth data, we use the 16 arrivals on the first day. to con-\nstruct a (1 \u2212 2\u03b1) confidence interval for the mean arrivals per day, we recall that the\npoisson mean and variance both equal \u03b8 and suppose that (y \u2212 \u03b8)/\u03b8 1/2 .\u223c n (0, 1).\nan estimator of the denominator is y 1/2, and taking (y \u2212 \u03b8)/y 1/2 .\u223c n (0, 1) gives\n(y \u2212 y 1/2z1\u2212\u03b1, y \u2212 y 1/2z\u03b1) asapproximate confidence interval. with \u03b1 = 0.025 and\ny = 16 this yields (8.2, 23.8).\nit is better to take y 1/2 .\u223c n (\u03b8 1/2, 0.25), giving (1 \u2212 2\u03b1) confidence intervals\n(cid:13)\n(cid:2)\n(cid:3)2\n\n(cid:12)(cid:2)\n\n(cid:3)2\n\n(cid:3)\n\n(cid:2)\n\ny 1/2 \u2212 1\n2\n\nz1\u2212\u03b1, y 1/2 \u2212 1\n2\n\nz\u03b1\n\n,\n\ny 1/2 \u2212 1\n2\n\nz1\u2212\u03b1\n\n,\n\ny 1/2 \u2212 1\n2\n\nz\u03b1\n\nfor \u03b8 1/2 and \u03b8. with \u03b1 = 0.025 and y = 16 this gives (9.1, 24.8), which is shifted to\nthe right relative to the interval above, and is not symmetric about y. here the effect\n(cid:1)\nof transformation is small, but it can be much larger in other problems.\n\n "}, {"Page_number": 72, "text": "60\n\n3 \u00b7 uncertainty\n\n3.1.3 tests\nthe distribution of the pivot z(\u03c80) implies that some values of \u03c8 are more plausible\nthan others, and we can gauge this using confidence intervals: values of \u03c8 close to the\ncentre of a (say) 95% confidence interval are evidently more plausible than are those\nthat only just lie within it. in some applications a particular value of \u03c8 has special\nmeaning and we may want to assess its plausibility in the light of some data. given\na set of data, a pivot z(\u03c8) and a value \u03c80 whose plausibility we wish to establish,\none approach is to obtain the observed value of the pivot, z(\u03c80), and then regard the\nprobability pr{z(\u03c80) \u2264 z(\u03c80)} as a measure of the consistency of \u03c80 with the data.\nthe key point is that if \u03c80 was the value of \u03c8 which generated the data, then we\nwould expect z(\u03c80) to be aplausible value for z(\u03c80), but if not, we would expect\nz(\u03c80) to bemore extreme relative to the known distribution of the pivot.\n\nexample 3.9 (birth data)\nif the average time in the delivery suite for 10,000 women\nat a hospital in manchester was 6 hours, then we might want to see if this is consistent\nwith the times in oxford; the manchester sample is so large that we can treat the\n6 hours as fixed. the times for day 1 of the oxford data seem longer, but how sure\ncan we be?\n\nif \u03c80 for oxford was equal to 6 hours, then the observed value of z(\u03c80) for day 1\n\nof the oxford data,\n\nz(\u03c80) = (y \u2212 \u03c80)/(s2/n)1/2 = (8.77 \u2212 6)/(18.46/16)1/2 = 2.58,\n\nwould be the value of an approximately normal variable. however this seems unlikely:\nwith \u03c80 equal to 6 we get\n\npr{z(\u03c80) \u2264 2.58} .= \u0001(2.58) = 0.995.\n\nthis is an event which might take place about once in 200 repetitions, and it suggests\ntwo possibilities: either the manchester and oxford data actually are consistent but\nan unusual event has occurred, or they are not consistent, and in fact the average time\n(cid:1)\nis indeed shorter in manchester.\n\ntests and their relation to confidence intervals are discussed further in sections 4.5\n\nand 7.3.4.\n\n3.1.4 prediction\nin some applications the focus of interest is the likely value of an as-yet unobserved\nrandom variable y+, to bepredicted using known data y, taken to be a realization of\na random variable y . byanalogy with using pivots to make inferences on unknown\nparameters, it may then be possible to construct a function q = q(y+, y ) whose\ndistribution is independent of the parameters and such that\n\npr{q(y+, y ) \u2208 r\u03b1} =pr{ l\u03b1(y ) \u2264 y+ \u2264 u\u03b1(y )} =1 \u2212 2\u03b1.\n\nthen (l\u03b1(y), u\u03b1(y)) is a (1 \u2212 2\u03b1) prediction interval for y+.\n\nprediction intervals are\nalso known as tolerance\nintervals.\n\n "}, {"Page_number": 73, "text": "3.1 \u00b7 confidence intervals\n\n61\n\nexample 3.10 (location-scale model) suppose that y+ is to be predicted using an\nindependent random sample y1, . . . ,y n from a location-scale model. we can write\ny+ = \u03b7 + \u03c4 \u03b5+ and y j = \u03b7 + \u03c4 \u03b5 j , where the \u03b5s have common and known density\ng, say. if y and s2 are the sample average and variance of y1, . . . ,y n, then the\ndistribution of q = (y+ \u2212 y )/s depends only on g, and its quantiles q\u03b1 may be\nfound numerically. then\n\npr{q\u03b1 \u2264 (y+ \u2212 y )/s \u2264 q1\u2212\u03b1} =pr( y + sq\u03b1 \u2264 y+ \u2264 y + sq1\u2212\u03b1) = 1 \u2212 2\u03b1,\n\nand hence (y + sq\u03b1, y + sq1\u2212\u03b1) is anequitailed (1 \u2212 2\u03b1) prediction interval\n(cid:1)\nfor y+.\n\nexercises 3.1\n1 calculate a two-sided 0.95 confidence interval for the mean population time in the delivery\nsuite based on day 2 of the data in table 2.1. obtain also lower and upper 0.90 confidence\nintervals.\n2 let y1, . . . ,y n be defined by y j = \u00b5 + \u03c3 x j , where x1, . . . , xn is a random sample\nfrom a known density g with distribution function g. if m = m(y ) and s = s(y ) are\nlocation and scale statistics based on y1, . . . ,y n, that is, they have the properties that\nm(y ) = \u00b5 + \u03c3 m(x) and s(y ) = \u03c3 s(x) for all x1, . . . , xn, \u03c3 > 0 and real \u00b5, then show\nthat z(\u00b5) = n1/2(m \u2212 \u00b5)/s is a pivot.\nwhen n is odd and large, g is the standard normal density, m is the median of y1, . . . ,y n\nand s = iqr their interquartile range, show that s/1.35 p\u2212\u2192 \u03c3 , and hence show that as\nn \u2192 \u221e, z(\u00b5) d\u2212\u2192 n (0, \u03c4 2), for known \u03c4 > 0. hence give the form of a 95% confidence\ninterval for \u00b5.\ncompare this interval and that based on using z(\u00b5) with m = y and s2 the sample\nvariance, for the data for day 4 in table 2.1.\n3 if y is poisson with large mean \u03b8, then (y \u2212 \u03b8)/\u03b8 1/2 .\u223c n (0, 1). show that the limits of\na (1 \u2212 2\u03b1) confidence interval for \u03b8 are the solutions of the equation (y \u2212 \u03b8)2 = z2\n\u03b1\u03b8.\nobtain them and compare them with the intervals for the birth data in example 3.8.\n\n4 suppose that the unemployment rate \u03c0 is estimated by sampling randomly from the\nis found, giving(cid:7)\u03c0 = r/m. how large should m be if \u03c0\npotential workforce. a total of m individuals are sampled and the number unemployed r\n.= 0.05 and a standard error of at\nmost 0.005 is required? what if \u03c0 = 0.1?\nin some countries such surveys are conducted by telephone interviews with a fixed number\nof households chosen randomly from the phone book and then asking how many people\nin the household are eligible for work (not children, retired, . . .) and how many are\nworking. suppose that the total number of people is n, ofwhom m are eligible to\nwork; suppose that m is binomial with denominator n and probability \u03b8. ofthe m,\n\nr are eligible to work, so (cid:7)\u03c0 = r/m with m now random. if n = 12, 000, \u03b8 = 0.5 and\n\u03c0 = 0.05, use the delta method to compute a variance for (cid:7)\u03c0. compute also the variance\nwhen m = 6000 is treated as fixed. does the variability of m change the variance by\nmuch?\nwhat problems might arise when sampling from the phone book?\n5 one way to construct a confidence interval for a real parameter \u03b8 is to take the interval\n(\u2212\u221e,\u221e) with probability (1 \u2212 2\u03b1), and otherwise take the empty set \u2205. show that this\nprocedure has exact coverage (1 \u2212 2\u03b1). is it a good procedure?\n6 a binomial variable r has mean m\u03c0 and variance m\u03c0(1 \u2212 \u03c0). find the variance function\nof y = r/m, and hence obtain the variance-stabilizing transform for r.\n\n "}, {"Page_number": 74, "text": "62\n\n3 \u00b7 uncertainty\n\n7 let i be a confidence interval for \u00b5 based on an estimator t whose distribution is n (\u00b5, \u03c3 2).\nshow that exp(i ) is aconfidence interval for the median of the distribution of exp( t ).\nhow would you compute a confidence interval for its mean, if \u03c3 2 is (i) known and (ii)\nunknown?\nif r is binomial with denominator m and probability \u03c0, show that\nd\u2212\u2192 z \u223c n (0, 1),\n\nr/m \u2212 \u03c0\n\n8\n\nand that the limits of a (1 \u2212 2\u03b1) confidence interval for \u03c0 are the solutions to\n\n{\u03c0(1 \u2212 \u03c0)/m}1/2\n(cid:5)\nr2 \u2212(cid:4)\n\n2m r + mz2\n\n\u03b1\n\n(cid:4)\n\n(cid:5)\n\n\u03c0 + m\n\nm + z2\n\n\u03b1\n\n\u03c0 2 = 0.\n\n9\n\ngive expressions for them.\nin a sample with m = 100 and 20 positive responses, the 0.95 confidence interval is\n(0.13, 0.29). as this interval either does or does not contain the true \u03c0, what is the\nmeaning of the 0.95?\ni am uncertain about what will happen when i next roll a die, about the exact amount of\nmoney at present in my bank account, about the weather tomorrow, and about what will\nhappen when i die. does uncertainty mean the same thing in all these contexts? for which\nis variation due to repeated sampling meaningful, do you think?\n10 let y1, . . . ,y n be a random sample from a model in which y j = \u03b8 x j , where the x j\nare independent with known density g. show that\ny j /\u03b8 is a pivot, and deduce that a\n(1 \u2212 2\u03b1) confidence interval for \u03b8 based on\ny j /b), where a\nand b are known constants.\nif g(x) = e\n\u2212x , x > 0, is the exponential density, then the 0.025, 0.05, 0.1, 0.5, 0.9, 0.95\nx j for n = 12 are 6.20, 6.92, 7.83, 11.67, 16.60, 18.21 and\nand 0.975 quantiles of\n19.68. use them to give two-sided 0.80 and 0.95 confidence intervals for \u03b8, based on the\ndata in practical 2.5. give also upper and lower 0.90 confidence intervals for \u03b8.\n\ny j has form (\n\ny j /a,\n\n(cid:1)\n\n(cid:1)\n\n(cid:1)\n\n(cid:1)\n\n(cid:1)\n\n3.2 normal model\n3.2.1 normal and related distributions\nthe previous section described an approach to approximate statements of uncertainty,\nuseful in many contexts. we now discuss exact inference for a model of central im-\nportance, when the data available form a random sample from the normal distribution.\nthat is, we treat the data y1, . . . , yn as the observed values of y1, . . . ,y n, where the\ny j are independently taken from the normal density\n\n(cid:14)\n\n(cid:15)\n\nf (y; \u00b5, \u03c3 2) =\n\n1\n\n(2\u03c0 \u03c3 2)1/2 exp\n\n2\u03c3 2 (y \u2212 \u00b5)2\n\u2212 1\n\n, \u2212\u221e < y < \u221e,\n\n(3.5)\n\nwith \u00b5 real and \u03c3 positive. the normal model owes its ubiquity to the central limit\ntheorem, which, in addition to applying to functions of many observations, may apply\nto individual measurements themselves. for example, in example 1.1 it is reasonable\nto suppose that a plant\u2019s height is determined by the effects of many genes, to which\nan averaging effect may apply, leading to a normal distribution of heights for the\npopulation to which the individual belongs, and therefore suggesting the use of normal\ndistributions in (1.1), (1.2), and (1.3). in other situations the simplicity of inference\nfor the normal distribution leads to its use as an approximation even where no such\n\nlaplace named this the\ngaussian density, after\njohann carl friedrich\ngauss (1777\u20131855), who\nderived it while writing on\nthe combination of\nastronomical observations\nby least squares.\n\n "}, {"Page_number": 75, "text": "3.2 \u00b7 normal model\n\n63\n\nargument applies. of course it is important to check that the data do appear normally\ndistributed, for example by a normal probability plot (section 2.1.4).\n\nbefore considering inference for the normal sample, we discuss the normal and\nsome related distributions. all are widely tabulated,and their density and distribution\nfunctions and quantiles are readily calculated in statistical packages.\n\nnormal distribution\nif we change variable in (3.5) from y to z = (y \u2212 \u00b5)/\u03c3 , wesee that the corresponding\nrandom variable z = (y \u2212 \u00b5)/\u03c3 has density\n\u2212 1\n2\n\n, \u2212\u221e < z < \u221e;\n\n\u03c6(z) = (2\u03c0)\n\n\u22121/2 exp\n\n(cid:2)\n\n(cid:3)\n\nz2\n\n(3.6)\n\nthis is the density of the standard normal random variable z. the density (3.6) is\nsymmetric about z = 0, and e(z) = 0 and var(z) = 1 (exercise 3.2.1). consequently\nthe mean and variance of y = \u00b5 + \u03c3 z are \u00b5 and \u03c3 2. wewrite y \u223c n (\u00b5, \u03c3 2) as\nshorthand for \u2018y has the normal distribution with mean \u00b5 and variance \u03c3 2\u2019.\n\nthe distribution function corresponding to (3.6),\n\n\u0001(z) = (2\u03c0)\n\n\u22121/2\n\nexp\n\n(3.7)\nhas no closed form, and neither do its quantiles, z p = \u0001\u22121( p). two useful values are\nz0.025 = \u22121.96 and z0.05 = \u22121.65. the symmetry of (3.6) about z = 0 implies that\nz p = \u2212z1\u2212 p.\nthe moment-generating function of y is\n\ndu,\n\n\u2212\u221e\n\n(cid:10)\n\nz\n\n(cid:2)\n\n(cid:3)\n\n\u2212 1\n2\n\nu2\n\nm(t) = e(ety )\n1\n\n=\n\n(2\u03c0 \u03c3 2)1/2\n\n(cid:15)\n\n(cid:10) \u221e\n(cid:10) \u221e\n\n\u2212\u221e\n\nexp\n\n(cid:14)\nt y \u2212 1\n(cid:14)\n\u00b5t + \u03c3 2 t 2\n(cid:10) \u221e\n2\n\ndy\n\n2\u03c3 2 (y \u2212 \u00b5)2\n2\u03c3 2 (y \u2212 \u00b5 \u2212 t \u03c3 )2\n\u2212 1\n\n(cid:15)\n\ndy\n\nf (y; \u00b5 + \u03c3 t, \u03c3 2) dy\n\n1\n\n\u2212\u221e\n\n(2\u03c0 \u03c3 2)1/2\n\n=\nexp\n= exp (\u00b5t + \u03c3 2t 2/2)\n= exp (\u00b5t + \u03c3 2t 2/2),\n\n\u2212\u221e\n\n(3.8)\nsince for any real t, f (y; \u00b5 + \u03c3 t, \u03c3 2) isjust a normal density and has unit integral.\nwe often use variants of this argument to sidestep integration.\nthe mean and variance of y can be read off from its cumulant-generating function,\nk (t) = log m(t) = \u00b5t + \u03c3 2t 2/2: \u03ba1 = e(y ) = \u00b5 and \u03ba2 = var(y ) = \u03c3 2.\nchi-squared distribution\nif z1, . . . , z \u03bd are independent standard normal random variables, we say that w =\nz 2\n\u03bd has the chi-squared distribution on \u03bd degrees of freedom: wewrite\nw \u223c \u03c7 2\n1\n\n\u03bd . the probability density function of w ,\n\n+ \u00b7\u00b7\u00b7 + z 2\n\nf (w) =\n\n1\n\n2\u03bd/2\u0001(\u03bd/2)\n\nw \u03bd/2\u22121e\n\n\u2212w /2, w > 0, \u03bd = 1, 2, . . . ,\n\n(3.9)\n\nsee lindley and scott\n(1984) or pearson and\nhartley (1976), for\nexample.\n\nhere\n\n\u0001(\u03ba) = (cid:11) \u221e\n\n0 u\u03ba\u22121e\n\n\u2212u du is\nthe gamma function; see\nexercise 2.1.3.\n\n "}, {"Page_number": 76, "text": "figure 3.3 chi-squared\nand student t density\nfunctions (3.9) and (3.11).\nleft panel: chi-squared\ndensities with 1 (solid), 2\n(dots), 4 (dashes), 6\n(larger dashes), and 10\n(largest dashes) degrees of\nfreedom. right panel: t\ndensities with 1 (solid), 2\n(dots), 4 (dashes), and 20\n(large dashes) degrees of\nfreedom, and standard\nnormal density (heavy\nsolid). the scale is chosen\nto show the much heavier\ntails of the t density with\nfew degrees of freedom.\n\n64\n\nf\nd\np\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n1\n\n2\n\n4\n\n6\n\n10\n\n0\n\n5\n\n10\n\nw\n\n3 \u00b7 uncertainty\n\nf\nd\np\n\n4\n\n.\n\n0\n\n3\n\n.\n\n0\n\n2\n\n.\n\n0\n\n1\n\n.\n\n0\n\n0\n\n.\n\n0\n\n15\n\n20\n\n-4\n\n-2\n\n2\n\n4\n\n0\n\nt\n\npr(w \u2264 w) = pr(\u2212\u221a\n\nis shown in the left panel of figure 3.3 for various values of \u03bd. asone would expect\nfrom its definition, both the mean and variance of w increase with \u03bd. its p quantile,\ndenoted c\u03bd( p), has the property that pr{w \u2264 c\u03bd( p)} = p. when \u03bd = 1, w = z 2,\nwhere z \u223c n (0, 1), so\n\nw \u2264 z \u2264 \u221a\nimplying that c1(1 \u2212 2 p) = z2\np.\nit is clear from the definition of w that if w1 \u223c \u03c7 2\n\u03bd2 and they are\nindependent, then w1 + w2 \u223c \u03c7 2\n\u03bd1+\u03bd2; evidently this extends to finite sums of indepen-\ndent chi-squared variables. chi-squared and gamma distributions are closely related:\nif x has the gamma density (2.7) with parameter \u03bb and shape \u03ba, then \u03bbx \u223c 1\n\u03c7 2\n2\u03ba\n(exercise 3.2.2).\n\n\u03bd1 and w2 \u223c \u03c7 2\n\nw),\n\n2\n\nto find the moment-generating function of w , wefirst find the moment-generating\n\nfunction of z 2\n\nj , namely\n(cid:4)\n\ne\n\net z 2\n\nj\n\n1\n\n(cid:5) =\n(2\u03c0)1/2\n= (1 \u2212 2t)\n= (1 \u2212 2t)\n\n(cid:10) \u221e\n\n\u2212\u221e\n\u22121/2\n\netz2\u2212z2/2 dz\n(cid:10) \u221e\n1\n\n(2\u03c0)1/2\n\n\u22121/2,\n\nt <\n\n\u2212\u221e\n\n,\n\n1\n2\n\n\u2212u2/2 du\n\ne\n\n(3.10)\n\nwhere we have changed variable from z to u = (1 \u2212 2t)1/2z. the z 2\nand identically distributed, so w has moment-generating function {(1 \u2212 2t)\n(1 \u2212 2t)\nand 2\u03bd.\n\nj are independent\n\u22121/2}\u03bd =\n\u2212\u03bd/2, differentiation of which shows that the mean and variance of w are \u03bd\n\nstudent t distribution\nsuppose now that z and w are independent, that z is standard normal and w is chi-\nsquared with \u03bd degrees of freedom, and let t = z /(w/\u03bd)1/2. the random variable\nt is said to have a student t distribution on \u03bd degrees of freedom; wewrite t \u223c t\u03bd.\nits density is\nf (t) = \u0001{(\u03bd + 1)/2}\n\u221a\n\n, \u2212\u221e < t < \u221e, \u03bd = 1, 2, . . . .\n\n(3.11)\n\n1\n\n\u03bd\u03c0 \u0001(\u03bd/2)\n\n(1 + t 2/\u03bd)(\u03bd+1)/2\n\n "}, {"Page_number": 77, "text": "3.2 \u00b7 normal model\n\n65\n\nthe right panel of figure 3.3 shows (3.11) for various values of \u03bd. the distribution\np\u2212\u2192 1 as\u03bd \u2192 \u221e\nof t approaches that of z for large \u03bd, because the fact that w/\u03bd\nimplies that t d\u2212\u2192 z; see example 2.22. the extra variability induced by dividing z\nby (w/\u03bd)1/2 spreads out the distribution of t relative to that of z, by alarge amount\nwhen \u03bd is small, but by less when \u03bd is large. one consequence of this is that as\n\u03bd \u2192 \u221e the quantiles of t , denoted t\u03bd( p), approach those of z, that is, t\u03bd( p) \u2192 z p.\nfor example, the 0.025 quantiles for \u03bd = 2, 10, and 20 are \u22124.30, \u22122.23 and \u22122.09,\nwhile t\u221e(0.025) = z0.025 = \u22121.96. the symmetry of (3.11) about t = 0 implies that\nt\u03bd( p) = \u2212t\u03bd(1 \u2212 p).\n\nnot all the moments of t are finite, because the function tr f (t) isintegrable only\nif r < \u03bd. one simple way to calculate its mean and variance, when they exist, is to\nuse the identities\n\ne{h(z , w )} = ew [e{h(z , w ) | w}] ,\nvar{h(z , w )} = ew [var{h(z , w ) | w}] + varw [e{h(z , w ) | w}] ,\n\n(3.12)\n\n(3.13)\n\nwhich hold for any random variables z and w ; the inner expectation and variance\nare over the distribution of z for w fixed (exercise 3.2.3). if h(z , w ) = z /(w/\u03bd)1/2\nand z and w are independent, then\n\ne{z /(w/\u03bd)1/2 | w} =( w/\u03bd)\nvar{z /(w/\u03bd)1/2 | w} =( w/\u03bd)\n\n\u22121/2e(z) = 0,\n\u22121var(z) = (w/\u03bd)\n\n\u22121.\n\nconsequently (3.12) and (3.13) imply that e(t ) = ew{z /(w/\u03bd)1/2} =0 and\n\nvar(t ) = ew (\u03bd/w )\n\n(cid:10) \u221e\n\n\u03bd\n\n\u03bd\n\n2\u03bd/2\u0001(\u03bd/2)\n\n=\n=\n2\u03bd/2\u0001(\u03bd/2)\n= \u03bd\n\u03bd \u2212 2\n\n,\n\n0\n\nw\n\n\u22121 \u00b7 w \u03bd/2\u22121e\n2\u03bd/2\u22121\u0001(\u03bd/2 \u2212 1)\n\u03bd = 3, 4, . . . ,\n\n\u2212w /2 dw\n\nthe first equality following from (3.13), the second from (3.9), the third on noticing\nthat the integrand is proportional to the chi-squared density on \u03bd \u2212 2 degrees of\nfreedom \u2014 whose integral must equal one \u2014 and the fourth on using the fact that\n\u0001(\u03ba + 1) = \u03ba\u0001(\u03ba), for \u03ba > 0 (exercise 2.1.3). the variance of t is finite only if\n\u03bd \u2265 3, and its mean is finite only if \u03bd \u2265 2. setting \u03bd = 1 in(3.11) gives the cauchy\ndensity (2.16), useful for counter-examples.\n\nf distribution\nsuppose that w1 and w2 have independent chi-squared distributions with \u03bd1 and \u03bd2\ndegrees of freedom respectively. then\n\nf = w1/\u03bd1\nw2/\u03bd2\n\n "}, {"Page_number": 78, "text": "66\n\n(cid:4)\n\n(cid:5)\n\n3 \u00b7 uncertainty\nhas the f distribution on \u03bd1 and \u03bd2 degrees of freedom: wewrite f \u223c f\u03bd1,\u03bd2. its\ndensity function is\n\u03bd1 + 1\n(cid:5)\n(cid:4)\nf (u) = \u0001\n2\n\u03bd1\n\nu > 0, \u03bd1, \u03bd2 = 1, 2, . . . ,\n(3.14)\nand its p quantile is denoted f\u03bd1,\u03bd2( p). when \u03bd1 = 1, f = z 2/(w2/\u03bd2), where z \u223c\nn (0, 1) is independent of w2 \u223c \u03c7 2\n\u03bd2, so f then has the same distribution as t 2, where\nt \u223c t\u03bd2.\n\n(\u03bd2 + \u03bd1u)(\u03bd1+\u03bd2)/2\n\n\u03bd\u03bd1/2\n1\n1\n2\n\n\u03bd\u03bd2/2\n2\n\n\u03bd2\n\u0001\n\n1\n2\n\u0001\n\n\u03bd1\u22121\n\nu 1\n\n\u03bd2\n\n(cid:4)\n\n(cid:5)\n\n1\n2\n\n,\n\n2\n\n3.2.2 normal random sample\nwhen a random sample y1, . . . ,y n is normal, there are compelling reasons to base\ninference for \u00b5 and \u03c3 2 on its average and variance, y and s2. at the end of this we suppose that n is two\nor more, so s2 > 0 with\nsection we shall prove that their joint distribution is given by\nprobability 1.\n\n(cid:15)\n\ny \u223c n (\u00b5, n\n\u22121\u03c3 2),\n(n \u2212 1)s2 \u223c \u03c3 2\u03c7 2\nn\u22121\n\nindependently.\n\n,\n\n(3.15)\n\nanother way to express this is\n\nz \u223c n (0, 1),\n\u22121/2\u03c3 z ,\n\u22121\u03c3 2w, w \u223c \u03c7 2\nn\u22121\nthe studentized form of y may therefore be written\n\nd= \u00b5 + n\nd= (n \u2212 1)\n\ny\ns2\n\n,\n\n(cid:15)\n\nz , w independent.\n\nt = y \u2212 \u00b5\nd=\n=\n\n\u22121/2\u03c3 z\n\n(s2/n)1/2\n{\u03c3 2(n \u2212 1)\u22121w/n}1/2\n{w/(n \u2212 1)}1/2\n\nz\n\nn\n\n,\n\n(3.16)\n\nwhich has the t distribution with n \u2212 1 degrees of freedom.\n\nas the distribution of t = (y \u2212 \u00b5)/(s2/n)1/2 is known, t is an exact pivot, and\nthere is no need for large-sample approximation when a confidence interval is required\nfor \u00b5. that is,\n\n(cid:17)\n\n1 \u2212 2\u03b1 = pr\n= pr\n\ntn\u22121(\u03b1) \u2264 y \u2212 \u00b5\ny \u2212 n\n\n\u2264 tn\u22121(1 \u2212 \u03b1)\n\u22121/2stn\u22121(1 \u2212 \u03b1) \u2264 \u00b5 \u2264 y \u2212 n\n\n(s2/n)1/2\n\n(cid:9)\n\n.\n\n\u22121/2stn\u22121(\u03b1)\n\n(cid:16)\n(cid:8)\n\nas the t distribution is symmetric, the random interval with endpoints\n\ny \u00b1 n\n\n\u22121/2stn\u22121(\u03b1)\n\n(3.17)\ncontains \u00b5 with probability exactly (1 \u2212 2\u03b1), for all n \u2265 2. in practice, y and s are\nreplaced by their observed values y and s, and the resulting interval has the repeated\nsampling interpretation outlined in section 3.1.\n\n "}, {"Page_number": 79, "text": "3.2 \u00b7 normal model\n\n67\n\nexample 3.11 (maize data) the final column of table 1.1 contains the differences\nin heights between n = 15 pairs of self- and cross-fertilized plants. suppose that\nthese differences are a random sample from the n (\u00b5, \u03c3 2) distribution; here \u00b5 and \u03c3\nhave units of eighths of an inch, and represent the mean and standard deviation of a\npopulation of such differences.\nthe values of the average and sample variance are y = 20.93 and s2 = 1424.6.\nas t14(0.025) = \u22122.14, the 95% confidence interval for \u00b5 is y \u00b1 n\n\u22121/2stn\u22121(\u03b1),\nthat is, 20.93 \u00b1 (1424.6/15)1/2 \u00d7 2.14 = (0.03, 41.84) eighths of an inch. this in-\nterval suggests that the mean difference in heights is positive; the best estimate\n2 inches. however, the value \u00b5 = 0 isonly just outside the inter-\nof \u00b5 is about 2 1\nval, so the evidence for a height difference between the two types of plants is not\n(cid:1)\noverwhelming.\n\na similar argument gives confidence intervals for \u03c3 2. if (n \u2212 1)s2 \u223c \u03c3 2\u03c7 2\n\nn\u22121, then\n\n(n \u2212 1)s2/\u03c3 2 \u223c \u03c7 2\n(cid:14)\nn\u22121 is another exact pivot. thus\ncn\u22121(\u03b1) \u2264 (n \u2212 1)s2\n\n\u2264 cn\u22121(1 \u2212 \u03b1)\nleading to the exact (1 \u2212 2\u03b1) confidence interval for \u03c3 2,\n\npr\n\n\u03c3 2\n\n(cid:15)\n\n= 1 \u2212 2\u03b1,\n\n((n \u2212 1)s2/cn\u22121(1 \u2212 \u03b1), (n \u2212 1)s2/cn\u22121(\u03b1)).\n\n(3.18)\nexample 3.12 (maize data) table 1.1 shows samples of sizes n1 = n2 = 15 on the\n= 269.4 for the cross-\nheights of plants; the sample variances are s2\n1\nand self-fertilized plants respectively.\nif we take \u03b1 = 0.025, then c14(0.025) = 5.629 and c14(0.975) = 26.119. hence\nthe 95% confidence interval (3.18) for the variance for the cross-fertilized\ndata is (14s2\n/c14(0.025)), that is, (449, 2082) eighths of inches\n1\n(cid:1)\nsquared.\n\n= 837.3 and s2\n\n/c14(0.975), 14s2\n1\n\n2\n\n1 and s2\n\nthe f distribution gives a means to compare the variances of two normal samples.\nsuppose that s2\n2 are the sample variances for two independent normal samples\nof respective sizes n1 and n2, and that the variances of those samples are \u03c3 2 and\n\u03c8 \u03c3 2. that is, \u03c8 is the ratio of the variances of the samples. then (n1 \u2212 1)s2\n/\u03c3 2 and\n(n2 \u2212 1)s2\n/(\u03c8 \u03c3 2) have independent chi-squared distributions on n1 \u2212 1 and n2 \u2212 1\n(cid:14)\ndegrees of freedom, and\n\n(cid:15)\n\n1\n\n2\n\npr\n\n/\u03c3 2\n\n1\n/(\u03c8 \u03c3 2)\n\nfn1\u22121,n2\u22121(\u03b1) \u2264 s2\ns2\n2\n(cid:14)\n\n\u2264 fn1\u22121,n2\u22121(1 \u2212 \u03b1)\n(cid:15)\n\nor equivalently\n\n= 1 \u2212 2\u03b1,\n\npr\n\nfn1\u22121,n2\u22121(\u03b1)\n\n\u2264 \u03c8 \u2264 fn1\u22121,n2\u22121(1 \u2212 \u03b1)\n\n= 1 \u2212 2\u03b1.\n\ns2\n2\ns2\n1\n\ns2\n2\ns2\n1\n\n(cid:18)\n(cid:5)\nthus, given two normal random samples whose variances are s2\n1 and s2\n2,\ns2\n1\n\n, fn1\u22121,n2\u22121(1 \u2212 \u03b1)s2\n\nfn1\u22121,n2\u22121(\u03b1)s2\n2\n\n(cid:18)\n\ns2\n1\n\n(cid:4)\n\n2\n\n(3.19)\n\n "}, {"Page_number": 80, "text": "/s2\n\n2, which has an exact fn1\u22121,n2\u22121 distribution.\n\n3 \u00b7 uncertainty\n68\nis a (1 \u2212 2\u03b1) confidence interval for the ratio of variances, \u03c8. here the pivot is\n\u03c8 s2\n1\nexample 3.13 (maize data) following on from example 3.12, we take \u03b1 = 0.025,\ngiving f14,14(0.025) = 0.336, f14,14(0.975) = 2.979. the 95% confidence inter-\nval (3.19) for the ratio of the variances for self- and cross-fertilized plants is\n(0.108, 0.958). the value \u03c8 = 1 isnot in this interval, which suggests that the self-\n(cid:1)\nfertilized plants are less variable in height than the cross-fertilized ones.\n\nthe comparison of variance estimates using f statistics is a crucial ingredient in\n\nthe analysis of variance, discussed in section 8.5.\n\n3.2.3 multivariate normal distribution\nthe normal distribution plays a central role in inference for scalar data. its simple\nproperties generalize elegantly to vectors of variables, and these we study now.\n\none measure of the strength of association between scalar random variables y1 and\n\ny2 is their covariance,\n\ncov(y1, y2) = e [{y1 \u2212 e(y1)}{y2 \u2212 e(y2)}] .\n\nevidently cov(y1, y1) = var(y1), cov(y1, y2) = cov(y2, y1), and if a and b are con-\nstants then cov(a + by1, y2) = bcov(y1, y2).\nin general we may have several random variables. if y denotes the p \u00d7 1 vector\n(y1, . . . ,y p)t and z denotes the q \u00d7 1 vector (z1, . . . , zq)t, let e(y ) be the\np \u00d7 1 vector whose rth element is e(yr ). we define the covariance of y and z\nto be the p \u00d7 q matrix\n\ncov(y, z) = e\n\n(cid:19){y \u2212 e(y )}{z \u2212 e(z)}t\n\n(cid:20)\n\nwhose (r, s) element is cov(yr , zs). in particular, cov(y, y ) = \u0001, the p \u00d7 p sym-\nmetric matrix whose (r, s) element is \u03c9rs = cov(yr , ys); this is called the covariance\nmatrix of y . it issymmetric because cov( yr , ys) = cov(ys , yr ), positive semi-definite or sometimes just the\nbecause\n\nvariance matrix.\n\nvar(aty ) = cov(aty, aty ) = atcov(y, y )a = at\u0001a \u2265 0\n\nfor any constant p \u00d7 1 vector a, and positive definite unless the distribution of y is\ndegenerate, here meaning that some yr is constant or can be expressed in terms of a\nlinear combination of the others (exercise 3.2.14).\nthe covariance matrix of the linear combinations a + b ty and c + dty , where a\nand c are respectively q \u00d7 1 and r \u00d7 1 constant vectors, and b and d are respectively\np \u00d7 q and p \u00d7 r constant matrices, is\n(cid:19){b ty \u2212 e(b ty )}{dty \u2212 e(dty )}t\ncov(a + b ty, c + dty ) = e\n(cid:19)\nb t {y \u2212 e(y )}{y \u2212 e(y )}t d\n= e\n= b t\u0001d.\n\n(cid:20)\n\n(cid:20)\n\n "}, {"Page_number": 81, "text": "3.2 \u00b7 normal model\nwhen a, b, c, d are constants, cov(a + by1, c + dy2) = bdcov(y1, y2), and thus co-\nvariance is not an absolute measure of the association between the variables, because\nit depends on their units. a measure that is invariant to the choice of units is the\ncorrelation of y1 and y2, namely\n\n69\n\ncorr(y1, y2) =\n\ncov(y1, y2)\n\n{var(y1)var(y2)}1/2\n\n,\n\nsome of whose properties were outlined in example 2.21 and exercise 2.2.3. positive\ncorrelation between y1 and y2 indicates that large values of y1 and y2 tend to occur\ntogether, and conversely; whereas negative correlation means that if y1 is larger than\ne(y1), y2 tends to be smaller than e(y2). the correlation matrix of a p \u00d7 1 vector y\nhas as its (r, s) element the correlation between yr and ys, and may be expressed as\n\u22121/2\n, where \u0001d is the diagonal matrix diag(\u03c911, . . . , \u03c9 pp). the diagonal of\n\u0001\n\u22121/2\nd\nconsists of ones.\n\u0001\nd\n\n\u22121/2\n\u22121/2\nd\nd\n\n\u0001\u0001\n\u0001\u0001\n\n(cid:15)\n\n(cid:14)\n\n\u2212 1\n2\n\nmultivariate normal distribution\na p-dimensional multivariate normal random variable y = (y1, . . . ,y p)t with p \u00d7 1\nvector mean \u00b5 and p \u00d7 p covariance matrix \u0001 has density\n\n(y \u2212 \u00b5)t\u0001\u22121(y \u2212 \u00b5)\n\nf (y; \u00b5, \u0001) =\n\n1\n\n;\n\n(2\u03c0) p/2|\u0001|1/2 exp\n\n(3.20)\nwe write y \u223c n p(\u00b5, \u0001). here y , y, and \u00b5 take values in ir p. weassume that the\ndistribution is not degenerate, in which case \u0001 is positive definite, implying amongst\nother things that its determinant |\u0001| > 0.\n(cid:10)\nthe moment-generating function of y is\nm(t) = e\n\n(y \u2212 \u00b5)t\u0001\u22121(y \u2212 \u00b5)\n\n(cid:5) =\n\net ty\n\nexp\n\ndy,\n\n(cid:14)\n\n(cid:15)\n\n(cid:4)\n\n1\n\n(2\u03c0) p/2|\u0001|1/2\n\nt t y \u2212 1\n2\n\nwhere t t is the 1 \u00d7 p vector (t1, . . . ,t p) and y = (y1, . . . ,y p)t; the integral is over\ny \u2208 ir p. tosimplify m(t) wewrite the exponent inside the integral as\n2 (y \u2212 \u00b5 \u2212 \u0001t)t\u0001\u22121(y \u2212 \u00b5 \u2212 \u0001t).\n2 t t\u0001t \u2212 1\n(cid:2)\n(cid:3)(cid:10)\n\nthe first two terms of this do not depend on y, so\n\nt t\u00b5 + 1\n(cid:2)\n\n(cid:3)\n\nm(t) = exp\n\nt t\u00b5 + 1\n2\n\nt t\u0001t\n\nf (y; \u00b5 + \u0001t, \u0001) dy = exp\n\nt t\u00b5 + 1\n2\n\nt t\u0001t\n\n,\n\nbecause for any value of \u00b5, (3.20) is a probability density function. we obtain the\nmoments of y by differentiation:\n= \u00b5r ,\n\u2212 \u2202 m(0)\n\u2202tr\n\ne(yr ) = \u2202 m(0)\n\u2202tr\ncov(yr , ys) = \u2202 2 m(0)\n\u2202tr \u2202ts\n\n= \u03c9rs + \u00b5r \u00b5s \u2212 \u00b5r \u00b5s = \u03c9rs .\n\n\u2202 m(0)\n\n\u2202ts\n\n "}, {"Page_number": 82, "text": "figure 3.4 the bivariate\nnormal density, with\ncorrelation \u03c1 = 0, 0.3,\nand 0.9. the lower right\npanel shows contours of\nthe density when \u03c1 = 0.3;\nnote that they are\nelliptical. in higher\ndimensions the contours\nof equal density are\nellipsoids.\n\nrho=0.0\n\nrho=0.3\n\n3 \u00b7 uncertainty\n\n70\n\n3\n\n.\n\n0\n\n2\n\n.\n\n0\n1\n\n.\n\n0\n\n0\n\n \n\n2\n\n1\n\n 0\ny2\n\n-1\n\n-2\n\n3\n\n.\n\n0\n\n2\n\n.\n\n0\n1\n\n.\n\n0\n\n0\n\n \n\n2\n\n1\n\n 0\ny2\n\n-1\n\n-2\n\n2\n\n2\n\n1\n\n1\n\n1\n\n-\n\n2\n\n-\n\n  0\ny\n\n1\n\nrho=0.9\n\n1\n\n-\n\n2\n\n-\n\n  0\ny\n\n1\n\n3\n\n.\n\n0\n\n2\n\n.\n\n0\n1\n\n.\n\n0\n\n0\n\n \n\n2\n\n1\n\n 0\ny2\n\n-1\n\n-2\n\n2\n\n1\n\n2\ny\n\n0\n\n1\n-\n\n2\n-\n\n2\n\n1\n\n1\n\n-\n\n2\n\n-\n\n  0\ny\n\n1\n\n0.02\n\n0.05\n\n0.1\n\n0.15\n\nthe cumulant-generating function of y is\n\nk (t) = log m(t) = t t\u00b5 + 1\n2\n\nt t\u0001t = p(cid:21)\n\nr=1\n\n-2\n\n-1\n\n1\n\n2\n\n0\n\ny1\n\ntr \u00b5r + 1\n2\n\np(cid:21)\nr=1\n\np(cid:21)\ns=1\n\ntr ts \u03c9rs .\n\nthus the first and second cumulants are \u03bar = \u00b5r and \u03bar,s = \u03c9rs, which are respec-\ntively the rth element of \u00b5 and the (r, s) element of \u0001; all higher cumulants are\nzero.\n\na special case of (3.20) is the bivariate normal distribution, whose covariance\n\nmatrix is\n\n(cid:2)\n\n(cid:3)\n\n;\n\n\u03c911 \u03c912\n\u03c921 \u03c922\n\nthe correlation between y1 and y2 is \u03c1 = \u03c912/(\u03c911\u03c922)1/2. this density is shown\nin figure 3.4 for \u00b5 = 0; the effect of increasing \u03c1 is to concentrate the probabil-\nity mass close to the line y1 = y2. the corresponding densities for negative \u03c1 are\nobtained by reflection in the line y1 = 0. when p = 2 the contours of constant den-\nsity are ellipses, but when p > 2 they are the ellipsoids given by constant values of\n(y \u2212 \u00b5)t\u0001\u22121(y \u2212 \u00b5).\n\n "}, {"Page_number": 83, "text": "3.2 \u00b7 normal model\n\n71\n\nmarginal and conditional distributions\nto study the distribution of a subset of y , wewrite y t = (y t\n2 ), where now y1\nhas dimension q \u00d7 1 and y2 has dimension ( p \u2212 q) \u00d7 1. partition t, \u00b5, and \u0001 con-\nformably, so that\n\n, y t\n\n1\n\n(cid:2)\n\n(cid:3)\n\nt1\nt2\n\nt =\n\n, \u00b5 =\n\n, \u0001 =\n\n(cid:2)\n\n(cid:3)\n\n\u00b51\n\u00b52\n\n(cid:2)\n\n(cid:3)\n\n,\n\n\u000111 \u000112\n\u000121 \u000122\n\nwhere t1 and \u00b51 are q \u00d7 1 vectors and \u000111 is a q \u00d7 q matrix, t2 and \u00b52 are ( p \u2212 q) \u00d7 1\nvectors and \u000122 is a ( p \u2212 q) \u00d7 ( p \u2212 q) matrix, and \u000112 = \u0001t\n21 is a q \u00d7 ( p \u2212 q)\nmatrix. the moment-generating function of y is\n\n(cid:4)\n\ne\n\net ty\n\n(cid:4)\n(cid:5) = e\n(cid:8)\n1 y1+t t\net t\n2 y2\n= exp\nt t\n1\n\n(cid:5)\n\u00b51 + t t\n\n2\n\n(cid:4)\n\n\u00b52 + 1\n\n2\n\n\u000111t1 + 2t t\n\n1\n\n\u000112t2 + t t\n\n2\n\nt t\n1\n\n\u000122t2\n\n(cid:5)(cid:9)\n\n,\n\nfrom which we obtain the moment-generating functions of y1 and y2 by setting t2 and\nt1 respectively equal to zero, giving\n\n\u00b51 + 1\n2 t t\n1\n\nt t\n1\n\n\u000111t1\n\n(cid:5)\n\n(cid:4)\n\n, e\n\net t\n2 y2\n\n(cid:5) = exp\n\n(cid:4)\n\n\u00b52 + 1\n2 t t\n2\n\nt t\n2\n\n\u000122t2\n\n(cid:5)\n\n.\n\n(cid:4)\n\ne\n\net t\n1 y1\n\n(cid:5) = exp\n\n(cid:4)\n\nthus the marginal distributions of y1 and y2 are multivariate normal also. note that\ny1 and y2 are independent if and only if their joint moment-generating function\nfactorizes, that is,\n\n(cid:5) = e\n(cid:4)\net t\n1 y1\nwhich occurs if and only if \u000112 = \u0001t\nequivalently and more elegantly, the cumulant-generating function of y1 and y2 is\n\n1 y1+t t\net t\n2 y2\n\nfor all t1, t2,\n\net t\n2 y2\n\n(cid:4)\n\n(cid:4)\n\n(cid:5)\n\ne\n\n21\n\n,\n\n(cid:5)\ne\n= 0.\n(cid:4)\n\nk (t1, t2) = t t\n\n1\n\n\u00b51 + t t\n\n2\n\n\u00b52 + 1\n\n2\n\n\u000111t1 + 2t t\n\n1\n\n\u000112t2 + t t\n\n2\n\n\u000122t2\n\nt t\n1\n\n(cid:5)\n\n,\n\nand y1 and y2 are independent if and only if its coefficient in t1 and t2, t t\nidentically zero; this is the case if \u000112 = 0 but not otherwise.\n1\nthus for normal random variables zero covariance is equivalent to independence.\none implication is that if y1, . . . ,y n is a random sample from the normal distribution\nwith mean \u00b5 and variance \u03c3 2, then we can write\n\n\u000112t2, is\n\ny \u223c nn(\u00b51n, \u03c3 2 in).\n\n1n denotes the n \u00d7 1\nvector of 1s and in the\nn \u00d7 n identity matrix.\n\nthe conditional distribution of y1 given that y2 = y2 is (exercise 3.2.18)\n\nin the bivariate normal distribution with zero mean and unit variances,\n\n(cid:4)\n\nnq\n\n\u00b51 + \u000112\u0001\u22121\n22 (y2 \u2212 \u00b52), \u000111 \u2212 \u000112\u0001\u22121\n(cid:14)(cid:2)\n\n(cid:3)(cid:15)\n\n(cid:3)\n\n(cid:2)\n\n22\n\nn2\n\n0\n0\n\n,\n\n1 \u03c1\n1\n\u03c1\n\n,\n\n(cid:5)\n\n\u000121\n\n.\n\n(3.21)\n\nthe conditional mean of y1 given y2 = y2 is \u03c1y2, and the conditional variance is 1 \u2212 \u03c12.\nthus var(y1 | y2 = y2) \u2192 0 as|\u03c1 | \u21921. in the lower right panel of figure 3.4 this\n\n "}, {"Page_number": 84, "text": "72\n\n3 \u00b7 uncertainty\n\nconditional density is supported on a horizontal line passing through y2, and the\nconditional mean of y1 increases with y2.\nexample 3.14 (trivariate distribution) let y \u223c n3(\u00b5, \u0001), where\n\n\uf8eb\n\uf8ed 1\n\n\uf8f6\n\uf8f8 , \u0001 =\n\n\u00b5 =\n\n2\n1\n\n\uf8eb\n\uf8ed 2 0 1\n\n0 2 1\n1 1 2\n\n\uf8f6\n\uf8f8 .\n\nthe marginal distribution of y1 is n (1, 2) and the marginal distribution of (y1, y2)t\nis\n\n(cid:14)(cid:2)\n\n(cid:3)\n\n(cid:2)\n\nn2\n\n1\n2\n\n,\n\n2 0\n0 2\n\n(cid:3)(cid:15)\n\n;\n\n(cid:3)\n\n(cid:2)\n\ny1 and y2 are marginally independent.\n(cid:2)\n\nfor the conditional distribution of (y1, y2)t given y3 we set\n, \u000122 = ( 2 ) .\n=\n\u00b51 =\ngiven y3 = y3, (y1, y2)t is bivariate normal with mean vector and variance matrix\n(cid:3)\n(cid:3)\n(cid:2)\n\n, \u00b52 = ( 1 ) , \u000111 =\n(cid:3)\n(cid:2)\n\n, \u000112 = \u0001t\n(cid:3)\n(cid:2)\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n(cid:2)\n\n(cid:3)\n\n(cid:3)\n\n1\n2\n\n2\n0\n\n0\n2\n\n1\n1\n\n21\n\n1\n2\n\n+\n\n1\n1\n\n\u22121(y3 \u2212 1),\n2\n\n2 0\n0 2\n\n\u2212\n\n1\n1\n\n\u22121(1, 1) =\n2\n\n3/2 \u22121/2\n\u22121/2\n3/2\n\n.\n\nthus knowledge of y3 induces correlation between y1 and y2 despite their marginal\nindependence. moreover the conditional variance of y1 is smaller than the marginal\nvariance: knowing y3 makes one more certain about y1. the positive covariance\nbetween y1 and y3 means that if y3 is known to exceed its mean, that is, y3 > 1, then\nthe conditional mean of y1 exceeds its marginal mean by an amount that depends on\nthe difference y3 \u2212 1.\n(cid:1)\nlinear combinations of normal variables\nlinear combinations of normal random variables often arise. the moment-generating\nfunction of the linear combination a + bty , where the constants a and b are respec-\ntively a scalar and a p \u00d7 1 vector, is\n\n(cid:8)\n\net(a+bty )\n\ne\n\n(cid:15)\n\n(cid:14)\n\n(cid:9) = eta exp\n(cid:14)\n= exp\n\n(bt)t\u00b5 + 1\n2\nt(a + bt\u00b5) + t 2\n2\n\n(bt)t\u0001(bt)\n\n(cid:15)\n\nbt\u0001b\n\n,\n\nand hence a + bty has the normal distribution with mean a + bt\u00b5 and variance bt\u0001b.\nthis extends to vectors u = a + b ty , where a is a q \u00d7 1 constant vector and b is a\np \u00d7 q constant matrix. then u has moment-generating function\n\n(cid:4)\n\ne\n\net tu\n\n(cid:5) = et tae\n\n(cid:4)\n\net t bty\n\n(cid:5)\n\n(cid:4)\n(cid:5) = et tae\n(cid:8)\ne(bt)ty\n= exp\nt ta + (bt)t\u00b5 + 1\n(cid:8)\n= exp\nt t(a + b t\u00b5) + 1\n\n(cid:9)\n\n(cid:9)\n2 (bt)t\u0001(bt)\n2 t t b t\u0001bt\n,\n\n "}, {"Page_number": 85, "text": "3.2 \u00b7 normal model\n73\nand so u has a multivariate normal distribution with q \u00d7 1 mean a + b t\u00b5 and q \u00d7 q\ncovariance matrix b t\u0001b; this is singular and the distribution degenerate unless b\nhas full rank and q \u2264 p. that is, if y \u223c n p(\u00b5, \u0001), then\n\na + b ty \u223c nq(a + b t\u00b5, b t\u0001b).\n\n(3.22)\n\nexample 3.15 (trivariate distribution)\ndistribution of u1 = y1 + y2 + y3 \u2212 4 and u2 = y1 \u2212 y2 + y3:\n\uf8f6\n\uf8f8 .\n\n(cid:3)\uf8eb\n\uf8ed y1\n\n(cid:2)\u22124\n\n(cid:3)\n\n(cid:2)\n\n+\n\nu =\n\n1\n1\n1 \u22121\n\n1\n1\n\n0\n\nin the previous example, consider the joint\n\nthe mean vector and covariance matrix of u are\n\n(cid:2)\n\n(cid:3)\n(cid:2)\u22124\n+\n(cid:3)\uf8eb\n\uf8ed 2 0\n\n0\n\n0 2\n1 1\n\n1\n1\n\n1\n1\n1 \u22121\n\uf8f6\n\uf8f8\n1\n1\n2\n\n(cid:3)\uf8eb\n\uf8ed 1\n\n1\n1\n\n2\n\uf8eb\n1\n\uf8ed 1\n1\n1 \u22121\n1\n1\n\n(cid:2)\n\n1\n1\n1 \u22121\n\ny2\ny3\n\n\uf8f6\n\uf8f8 =\n\uf8f6\n\uf8f8 =\n\n(cid:2)\n\n(cid:2)\n\n(cid:3)\n\n,\n\n0\n0\n\n10\n4\n\n4\n6\n\n(cid:3)\n\n.\n\n(cid:1)\na further consequence of (3.22) follows from the spectral decomposition \u0001 =\ne l e t, where the columns of e are eigenvectors of \u0001, l is the diagonal matrix\ncontaining the corresponding eigenvalues, and e e t = e t e = i p. for positive def-\ninite \u0001, the elements of l are strictly positive and hence \u0001\u22121 = e l\n\u22121 e t. weset\nu = l\n\n\u22121/2 e t(y \u2212 \u00b5), and note that u \u223c n p(0, i p), so\n(y \u2212 \u00b5)t\u0001\u22121(y \u2212 \u00b5) = (y \u2212 \u00b5)t e l\n\n\u22121 e t(y \u2212 \u00b5) = u tu \u223c \u03c7 2\n\n.\n\n(3.23)\n\np\n\ntwo samples\nresult (3.22) has many uses. for example, suppose that a random sample of size n1 is\navailable from the n (\u00b51, \u03c3 2\n1 ) density and an independent random sample of size n2\nis available from the n (\u00b52, \u03c3 2\n2 ) density, and that the focus of interest is the difference\nof means \u00b51 \u2212 \u00b52. this is the situation in example 1.1. then since (3.15) applies to\neach sample separately,(cid:2)\n\n(cid:14)(cid:2)\n\n(cid:3)(cid:15)\n\n(cid:3)\n\n(cid:2)\n\n(cid:3)\n\n\u223c n2\n\ny 1\ny 2\n\n\u00b51\n\u00b52\n\n,\n\n\u03c3 2\n1\n\n/n1\n0\n\n0\n/n2\n\n\u03c3 2\n2\n\n,\n\nand an application of (3.22) with a = 0 and b t = (1,\u22121) gives that y 1 \u2212 y 2 has a\nnormal distribution with mean \u00b51 \u2212 \u00b52 and variance n\n\u03c3 2\n2 . tosimplify\nmatters, let us suppose that the variances \u03c3 2\n\n\u22121\n1\n\n\u22121\n2\n\ny 1 \u2212 y 2\n\nd= (\u00b51 \u2212 \u00b52) + \u03c3\n\n(cid:4)\n\n1 and \u03c3 2\n2 both equal \u03c3 2, inwhich case\n+ n\n\u22121\n\u22121\n2\n1\n/\u03c3 2 and (n2 \u2212 1)s2\n\nn\n\nwhere z \u223c n (0, 1), and (n1 \u2212 1)s2\n/\u03c3 2 are independent chi-\nsquared variables with n1 \u2212 1 and n2 \u2212 1 degrees of freedom respectively, so\n\n1\n\n2\n\n\u03c3 2\n1\n\n+ n\n(cid:5)1/2 z ,\n\n "}, {"Page_number": 86, "text": "74\n\n3 \u00b7 uncertainty\n\nn1+n2\u22122. hence the pooled estimate of \u03c3 2, s2, has\n\n2\n\n1\n\n\u223c \u03c3 2\u03c7 2\n\n(n1 \u2212 1)s2\ndistribution given by\n\n+ (n2 \u2212 1)s2\ns2 = (n1 \u2212 1)s2\nwhere w \u223c \u03c7 2\nn1+n2\u22122, independently of y 1 \u2212 y 2. consequently the quantity\ny 1 \u2212 y 2 \u2212 (\u00b51 \u2212 \u00b52)\n(cid:4)\n(cid:8)\n(cid:5)(cid:9)1/2\ns2\n\nd= \u03c3 2w/(n1 + n2 \u2212 2),\n\n{w/(n1 + n2 \u2212 2)}1/2\n\n+ (n2 \u2212 1)s2\n\nn1 + n2 \u2212 2\n\n\u223c tn1+n2\u22122\n\n+ n\n\nd=\n\nz\n\nn\n\n1\n\n2\n\n\u22121\n1\n\n\u22121\n2\n\nis a pivot from which confidence intervals for \u00b51 \u2212 \u00b52 may be determined. the\nargument parallels that leading to (3.17) and shows that the two-sample t confidence\ninterval whose endpoints are\n\n(y 1 \u2212 y 2) \u00b1(cid:8)\n\ns2\n\n(cid:4)\n\n(cid:5)(cid:9)1/2 tn1+n2\u22122(\u03b1)\n\n\u22121\n1\n\n+ n\n\n\u22121\n2\n\n1\n\nn\n\n= 837.3, y2 = 140.6 and s2\n\n(3.24)\nis a (1 \u2212 2\u03b1) confidence interval for \u00b51 \u2212 \u00b52 based on the two samples. in practice,\nthe random variables in (3.24) are replaced by their observed values, and the resulting\ninterval is given the repeated sampling interpretation.\nexample 3.16 (maize data) for the data in example 1.1, we have n1 = n2 = 15,\ny1 = 161.5, s2\n= 269.4. the difference of averages is\n20.9 and the pooled estimate of variance is 553.3; note that pooling here ignores\nthe evidence of example 3.13 that the self-fertilized plants are less variable, that is,\n\u03c3 2\nthe 0.025 quantile of t28 is \u22122.05, so the two-sample 0.95 confidence interval\n2\nfor \u00b51 \u2212 \u00b52 is 20.9 \u00b1 553.31/2(1/15 + 1/15)1/2 \u00d7 2.05 = (3.34, 38.53) eighths of\nan inch. this confidence interval is slightly narrower than that given in example 3.11,\nbased on differences of pairs of plants, and gives correspondingly stronger evidence\nfor a height difference in mean heights. however, this interval is less appropriate,\nboth because of the pairing of plants in the original experiment, and because of the\n(cid:1)\nevidence for a difference in variances.\n\n< \u03c3 2\n1 .\n\n2\n\n(cid:11)= \u03c3 2\n\n2\n\n(cid:4)\n\nn2\n\ns2\n1\n\ns2\n1\n\n(cid:18)\n\n(cid:18)\n\n\u03bd =\n\n.\u223c t\u03bd ,\n\nn1 + s2\n\nif there are two normal samples with unequal variances, \u03c3 2\n2 , there is no exact\n1\npivot. one fairly accurate approach to confidence intervals for the difference of sample\nmeans, \u00b51 \u2212 \u00b52, isbased on the approximate pivot\n(cid:18)\n(cid:18)\n(cid:4)\nn1 + s2\nt = y 1 \u2212 y 2 \u2212 (\u00b51 \u2212 \u00b52)\n(cid:18)(cid:8)\n(cid:18)(cid:8)\n(cid:9) + s4\n(cid:5)1/2\n1(n1 \u2212 1)\nn2\nthe idea of this is to replace the exact variance of y 1 \u2212 y 2, \u03c3 2\n\u22121\n2 , by an\nestimate, and then to find the t distribution whose degrees of freedom give the best\nmatch to the moments of t .\nexample 3.17 (maize data) for the data in example 1.1, we have \u03bd = 22.16, and\nt\u03bd(0.025) = \u22122.07. now s2\n/n2 = 73.78, so an approximate 95% confidence\ninterval is 20.9 \u00b1 2.07 \u00d7 73.781/2, that is, (3.13, 38.74). as mentioned before, this\ninterval is more appropriate for these data, but it differs only slightly from the interval\n(cid:1)\nin example 3.16.\n\n(cid:5)2\n2(n2 \u2212 1)\nn2\n+ \u03c3 2\n/n\n\n/n1 + s2\n\n2\n\u22121\n1\n\n(cid:9) .\n\ns4\n1\n\n/n\n\nn2\n\n2\n\n1\n\n2\n\n1\n\n2\n\n "}, {"Page_number": 87, "text": "3.2 \u00b7 normal model\n\n75\n\njoint distribution of y and s2\nwe now derive the key result (3.15). the most direct route starts from noting that\nif y1, . . . ,y n is a random sample from the n (\u00b5, \u03c3 2) distribution, the distribution of\ny = (y1, . . . ,y n)t is nn(\u00b51n, \u03c3 2 in). we now consider the random variable u = b ty ,\nwhere the n \u00d7 n matrix b t equals\n\n\uf8eb\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n1\nn1/2\n1\n21/2\n1\n61/2\n\n...\n\n1\nn1/2\n\n\u2212 1\n\n21/2\n1\n61/2\n\n...\n\n1\n\n1\nn1/2\n0\n\u2212 2\n...\n\n61/2\n\n1\nn1/2\n0\n0\n...\n\n\uf8f6\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 .\n\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n\n1\nn1/2\n0\n0\n...\n\u00b7\u00b7\u00b7 \u2212 n\u22121\n\n{n(n\u22121)}1/2\n\n1\n\n{n(n\u22121)}1/2\n\n{n(n\u22121)}1/2\n\n1\n\n{n(n\u22121)}1/2\n\n1\n\n{n(n\u22121)}1/2\n\nfor j = 2, . . . ,n , the jth row contains { j( j \u2212 1)}\u22121/2 repeated j \u2212 1 times, followed\nby \u2212( j \u2212 1){ j( j \u2212 1)}\u22121/2 once, with any remaining places filled by zeros. note that\nb t b = in and b t1n = (n1/2, 0, . . . ,0) t, which imply that\nt, \u03c3 2 in\n\nu \u223c nn\n\nn1/2\u00b5, 0, . . . ,0\n\n(cid:8)(cid:4)\n\n(cid:9)\n\n(cid:5)\n\n.\n\nthus the components of u are independent, and only the first, u1, has non-zero mean;\nin fact u1 = n\n\u22121\u03c3 2), thus\nestablishing the first line of (3.15). now\n\ny j = n1/2y , from which we see that y \u223c n (\u00b5, n\n\n\u22121/2\n\n(cid:1)\n\nn(cid:21)\nj=1\n\ny 2\nj\n\n= y ty = y t b t by = u tu = n(cid:21)\n(y j \u2212 y )2 = n(cid:21)\n(n \u2212 1)s2 = n(cid:21)\n\nj=1\n\ny 2\nj\n\nj=1\n\nj=1\n\nwhich implies that\n\n= ny 2 + u 2\n\n2\n\n+ \u00b7\u00b7\u00b7 + u 2\n\nn\n\n,\n\nu 2\nj\n\n\u2212 ny 2 = u 2\n\n2\n\n+ \u00b7\u00b7\u00b7 + u 2\n\nn\n\n.\n\nthus (n \u2212 1)s2/\u03c3 2 equals the sum of the squares of the n \u2212 1 standard normal vari-\nables u2/\u03c3, . . . , un/\u03c3 , and therefore has the chi-squared distribution with n \u2212 1\ndegrees of freedom, independent of u1, and hence independent of y . this establishes\nthe remainder of (3.15).\n\ngration by parts to find the mean and variance of (3.6).\n\nexercises 3.2\n1 show that the first two derivatives of \u03c6(z) are \u2212z\u03c6(z) and (z2 \u2212 1)\u03c6(z). hence use inte-\n2 if x has density (2.7), show that 2\u03bbx has density (3.9) with \u03bd = 2\u03ba.\n3 let h(z , w ) be afunction of two random variables z and w whose variance is finite,\nand let g(w ) = ew{h(z , w ) | w}. show that h(z , w ) \u2212 g(w ) has mean zero and is\nuncorrelated with g(w ). hence establish (3.13).\n\n4 let n be a random variable taking values 0, 1, . . ., let g(u) bethe probability-generating\nfunction of n , and let x1, x2, . . . be independent variables each having moment-\ngenerating function m(t). use (3.12) to show that y = x1 + \u00b7\u00b7\u00b7 + x n has moment-\ngenerating function g{m(t)}, and hence find the mean and variance of y in terms of those\nof x and n .\nuse (3.12) and (3.13) to find e(y ) and var(y ) directly.\n\n "}, {"Page_number": 88, "text": "76\n\n3 \u00b7 uncertainty\n\n9\n\n10\n\n\u03bd , show that e(w ) = \u03bd, var(w ) = 2\u03bd and (w \u2212 \u03bd)/\n\n5 use (3.6) and (3.9) to derive (3.11).\n6 use (3.9) to derive (3.14).\n7 check carefully the derivations of (3.8) and (3.10).\n8 assuming that the times for each day in table 2.1 are a random sample from the normal\ndistribution, use the day 2 data to compute (i) a two-sided 0.95 confidence interval for the\npopulation mean time in delivery suite and (ii) a 0.95 confidence interval for the population\nvariance. also give two-sided 0.95 confidence intervals for the difference in mean times\nfor day 1 and day 2, assuming that their variances are (iii) equal and (iv) unequal. give\na 0.95 confidence interval for the ratio of their variances. repeat (i) and (ii) giving 0.95\nupper and lower confidence intervals.\nif z \u223c n (0, 1), derive the density of y = z 2. although y is determined by z, show they\nare uncorrelated.\nif w \u223c \u03c7 2\n2\u03bd d\u2212\u2192 n (0, 1) as \u03bd \u2192\n\u221e.\n(a) if f \u223c f\u03bd1,\u03bd2, show that 1/f \u223c f\u03bd2,\u03bd1. give the quantiles of 1/f in terms of those of\nf.\n(b) show that as \u03bd2 \u2192 \u221e, \u03bd1 f tends in distribution to a chi-squared variable, and give its\ndegrees of freedom.\n\u2212y, y > 0, show that y1/y2 has\n(c) if y1 and y2 are independent variables with density e\nthe f distribution, and give its degrees of freedom.\n12 let f (t) denote the probability density function of t \u223c t\u03bd.\n(a) use f (t) tocheck that e( t ) = 0, var(t ) = \u03bd/(\u03bd \u2212 2), provided \u03bd > 1, 2 respectively.\n(b) by considering log f (t), show that as \u03bd \u2192 \u221e, f (t) \u2192 \u03c6(t).\nif y and z are p \u00d7 1 and q \u00d7 1 vectors of random variables, show that cov(y, z) =\n13\ne(y z t) \u2212 e(y )e(z)t.\nsingle value with probability one or yr = (cid:1)\n14 verify that if there is a non-zero vector a such that var(aty ) = 0, either some yr takes a\ns(cid:11)=r bsys , for some r, bs not all equal to zero.\n15 suppose y \u223c n p(\u00b5, \u0001) and a and b are p \u00d7 1 vectors of constants. find the distribution\nof x1 = aty conditional on x2 = bty = x2. under what circumstances does this not\ndepend on x2?\n\n\u221a\n\n11\n\n16 otherwise, or by noting that\n\u0001(a + by)\u03c6\n\n\u03c3 \u22121\n\n(cid:10)\n\n(cid:2)\n\n(cid:3)\n\ny \u2212 \u00b5\n\n\u03c3\n\ndy = ey {pr(z \u2264 a + by | y = y)} ,\n\nrecall stirling\u2019s formula.\n\n(cid:10)\n\nwhere z \u223c n (0, 1), independent of y \u223c n (\u00b5, \u03c3 2), show that\n\n(cid:2)\n\n(cid:3)\n\ny \u2212 \u00b5\n\n(cid:14)\n\ndy = \u0001\n\n(cid:15)\n\na + b\u00b5\n\n\u03c3 \u22121\n\n\u0001(a + by)\u03c6\n\n.\n\n\u03c3\n\n(cid:2)\n\n(1 + b2\u03c3 2)1/2\n\n(3.25)\n17 let y = x1 + bx2, where the x j are independent normal variables with means \u00b5 j and\nj . show that conditional on x2 = x, the distribution of y is normal with mean\n(cid:17)\n\nvariances \u03c3 2\n\u00b51 + bx and variance \u03c3 2\n1 , and hence establish that\n(cid:3)\n(cid:10)\ny \u2212 \u00b51 \u2212 bx\n(cid:4)\ndx =\n\n(cid:5)1/2\n18 to establish (3.21), show that the variables x = y1 \u2212 \u000112\u0001\u22121\n\n22 y2 and y2 have a joint\nmultivariate normal distribution and are independent, find the mean of x, and show that\nits variance matrix is \u000111 \u2212 \u000112\u0001\u22121\n\u000121. then use the fact that if x and y2 are independent,\nconditioning on y2 = y2 will not change the distribution of x, to give (3.21).\n\ny \u2212 \u00b51 \u2212 b\u00b52\n(cid:4)\n(cid:5)1/2\n\nx \u2212 \u00b52\n\u03c32\n\n+ b2\u03c3 2\n\n+ b2\u03c3 2\n\n1\n\u03c31\n\n1\n\u03c32\n\n(cid:16)\n\n(cid:2)\n\n(cid:3)\n\n\u03c3 2\n1\n\n\u03c3 2\n1\n\n\u03c31\n\n1\n\n\u03c6\n\n\u03c6\n\n\u03c6\n\n22\n\n2\n\n2\n\n.\n\n "}, {"Page_number": 89, "text": "3.3 \u00b7 simulation\n\n77\n\n19 let y have the p-variate multivariate normal distribution with mean vector \u00b5 and co-\n2 ), where y1 has dimension q \u00d7 1 and y2\nvariance matrix \u0001. partition y t as (y t\n, y t\nhas dimension r \u00d7 1, and partition \u00b5 and \u0001 conformably. find the conditional dis-\n1\ntribution of y1 given that y2 = y2 direct from the probability density functions of y\nand y2.\n20 conditional on m = m, y1, . . . ,y n is a random sample from the n (m, \u03c3 2) distribution.\nfind the unconditional joint distribution of y1, . . . ,y n when m has the n (\u00b5, \u03c4 2) distribu-\ntion. use induction to show that the covariance matrix \u0001 has determinant \u03c3 2n\u22122(\u03c3 2 + n\u03c4 2),\nand show that \u0001\u22121 has diagonal elements {\u03c3 2 + (n \u2212 1)\u03c4 2)/{\u03c3 2(\u03c3 2 + n\u03c4 2)} and off-\ndiagonal elements \u2212\u03c4 2/{\u03c3 2(\u03c3 2 + n\u03c4 2)}.\n\n3.3 simulation\n3.3.1 pseudo-random numbers\nsimulation, or the computer generation of artificial data, has many purposes. among\nthem are:\n\nr to see how much variability to expect in sampling from a particular model. for\nexample, a probability plot for a small sample can be hard to interpret, and in\nassessing whether any pattern in it is imagined or real it is helpful to compare\nit with those for sets of simulated data;\n\nr to assess the adequacy of a theoretical approximation. this is illustrated by\nfigure 2.4, which compares histograms of the average of n simulated exponen-\ntial variables with the normal density arising from the central limit theorem.\nthe simulations suggest that the approximation is poor when n \u2264 5, but much\nimproved when n \u2265 20;\n\nr to check the sensitivity of conclusions to assumptions \u2014 for example, how\nbadly do the methods of the previous section fail when the data are not normal?\nwe discuss this in example 3.24 below;\n\nr to give insight or confirm a hunch, on the principle that a rough answer to the\nright question is worth more than a precise answer to the wrong question; and\n\nr to provide numerical solutions when analytical ones are unavailable.\n\nthe starting point is an algorithm that provides a stream of pseudo-random vari-\nables, u1, u2, . . ., supposed independent and uniformly distributed on the interval\n(0, 1). these are called pseudo-random because although the algorithm should ensure\nthat they seem independent and identically distributed, they are predictable to anyone\nknowing the algorithm. one important class is the linear congruential generators\ndefined by\n\nx j+1 = (a x j + c) mod m, u j = x j /m,\n\nfor some natural number m, with a, c \u2208 {0, 1, . . . , m \u2212 1}; such a generator will\nrepeat with period at most m. the values of m, a and c are chosen to maximize the\nperiod and speed of the generator, and the apparent randomness of the output. an\nexample is m = 248, a = 517 and c = 1, giving m/4 elements of the set {0, . . . , m \u2212\n1}/m in what appears to be a random order.\n\nsome authors call them\nquasi-random.\n\n "}, {"Page_number": 90, "text": "78\n\n3 \u00b7 uncertainty\n\nnot only is it important that the u j are uniform, but also that they seem independent.\none way to do this is to consider k-tuples (u j , u j+1, . . . , u j+k\u22121) ofsuccessive\nvalues as points in the set (0, 1)k, where they should be uniformly distributed; see\npractical 3.5. many of the algorithms in standard packages have been thoroughly\ntested, but it is wise to store the seed x0 so that if necessary the sequence can be\nrepeated, and to perform important calculations using two different generators. below\nwe suppose it safe to assume that u1, u2, . . . are independent identically distributed\nvariables from the u (0, 1) distribution (2.22) and refer to them as random rather than\npseudo-random.\n\ninversion\nthe simplest way to convert uniform variables into those from other distributions\nis inversion. let f be the distribution function of a random variable, y , and let\n\u22121(u) = inf{y : f(y) \u2265 u}. if u has the u (0, 1) distribution (2.22), we saw on\nf\npage 39 that y d= f\n\u22121(un) is arandom sample\nfrom f.\n\n\u22121(u ), and that f\n\n\u22121(u1), . . . , f\n\nexample 3.18 (exponential distribution) the distribution function of an expo-\nnential random variable with parameter \u03bb >0 is\n\n(cid:14)\n\nf(y) =\n\n0,\n1 \u2212 exp(\u2212\u03bby),\n\ny \u2264 0,\n0 < y,\n\nand for 0 < u < 1 the solution to f(y) = u is y = \u2212\u03bb\u22121 log(1 \u2212 u). therefore a\nrandom variable from f is y = \u2212\u03bb\u22121 log(1 \u2212 u ) d= \u2212 \u03bb\u22121 log u , because u and\n1 \u2212 u have the same distribution.\n(cid:1)\n\nexample 3.19 (normal, chi-squared and t distributions) a normal random vari-\nable with mean \u00b5 and variance \u03c3 2 has distribution function f(y) = \u0001{(y \u2212 \u00b5)/\u03c3},\nif z1, z2, . . . is a stream of standard normal variables, v = (cid:1)\u03bd\nand therefore \u00b5 + \u03c3 \u0001\u22121(u1), . . . , \u00b5 + \u03c3 \u0001\u22121(un) is anormal random sample.\nj is chi-squared\nwith \u03bd degrees of freedom, and t = z \u03bd+1/(v /\u03bd)1/2 has the student t distribution with\n\u03bd degrees of freedom. since z j = \u0001\u22121(u j ), v and t are easily obtained.\n(cid:1)\npseudo-random variables from other distributions and processes can be con-\nstructed using their definitions, though statistical packages usually contain specially-\nprogrammed algorithms. one general approach for discrete variables is the look-up\nmethod. suppose that y takes values in {1, 2, . . . } and that we have created a ta-\nble containing the values of \u0001r = pr(y \u2264 r) and \u03c0r = pr(y = r). then inversion\namounts to this algorithm:\n\nj=1 z 2\n\ngenerate u \u223c u (0, 1) and set r = 1; then\n1\n2 while \u0001r \u2264 u set r = r + 1; and finally\nreturn y = r.\n3\n\nthe number of comparisons at step 2 can be reduced by sorting the \u03c0r into decreasing\norder and re-ordering {1, 2, . . . } accordingly. an alternative is to begin searching at\n\n "}, {"Page_number": 91, "text": "3.3 \u00b7 simulation\n\n79\n\na place that depends on u . each involves initial expense in obtaining and manipu-\nlating the \u03c0r \u2019s, and as the trade-off between this and the number of comparisons is\ncomplicated, fast algorithms for discrete distributions can be complex.\n\nrejection\n\u22121. another\ninversion is simple, but to be efficient it requires a fast algorithm for f\napproach is rejection. suppose we wish to generate from an awkward density f ,\nand can easily generate from the uniform distribution and from a density g for which\nsupy f (y)/g(y) = b < \u221e; note that b > 1. the rejection algorithm to generate y\nfrom f is:\n\nsometimes called the\nacceptance-rejection or\nenvelope method.\n\ngenerate x from g and u from the u (0, 1) density, independently;\nset y = x if u bg(x) \u2264 f (x), and otherwise go to 1; finally\nreturn y .\n\n1\n2\n3\nto see why this works, note that the interpretation of pr(x \u2264 a) asthe area under g\nto the left of a implies that (x, u bg(x)) is uniformly distributed on the set {(x, w) :\n0 \u2264 w \u2264 bg(x)}, and a value y is returned only if u bg(x) \u2264 f (x). for a single pair\n(x, u ), the probability a value y is returned and is less than y is\n\npr{u bg(x) \u2264 f (x) and x \u2264 y} =\n\n=\n\n(cid:15)\n\n(cid:6)(cid:6)(cid:6)(cid:6) x = x\n\ng(x) dx,\n\ny\n\n(cid:10)\n(cid:10)\n\u2212\u221e\ny\n(cid:10)\n\u2212\u221e\n\u22121\n\n= b\n\n(cid:14)\n\npr\n\nu \u2264 f (x)\nbg(x)\n\ng(x) dx\n\nf (x)\nbg(x)\ny\n\nf (x) dx,\n\n\u2212\u221e\n\nbecause u is uniform, independent of x. hence\n\npr(y \u2264 y | value returned) = pr{u bg(x) \u2264 f (x) and x \u2264 y}\npr{u bg(x) \u2264 f (x) and x \u2264 \u221e}\n(cid:10)\n\u2212\u221e\n\nf (x) dx;\n\n=\n\ny\n\n\u22121, sothe algorithm\nthe density of y is indeed f . the probability a value is returned is b\nis most efficient when b is as small as possible, and the envelope function bg(x) should\nensure both this and fast simulation from g.\nexample 3.20 (half-normal density) a half-normal variable is defined by y =\n|z|, where z \u223c n (0, 1). its density, f (y) = 2\u03c6(y) for y > 0, is shown by the solid\nline in the left panel of figure 3.5. the exponential density g(y) = \u03bbe\n\u2212\u03bby, declines\n(cid:3)(cid:15)\nmore slowly than f (y) for large y, and the ratio\n\n(cid:2)\n\n(cid:14)\n\n\u2212y2/2\n\n\u22121/2e\n\u03bbe\u2212\u03bby\n\nf (y)\ng(y)\n\n= 2(2\u03c0)\n\ny2 + 1\nlog\n2\nis maximized at y = \u03bb, giving b = supy f (y)/g(y) = (2/\u03c0 \u03bb2)\nbg(x) with \u03bb = 1 isshown by the dotted line in the figure.\n\n\u03bby \u2212 1\n2\n\n= exp\n\n2\n\u03c0 \u03bb2\n\n\u22121/2e\u03bb2/2. the function\n\n "}, {"Page_number": 92, "text": "80\n\ny\nt\ni\ns\nn\ne\nd\n\n5\n\n.\n\n1\n\n0\n\n.\n\n1\n\n5\n\n.\n\n0\n\n0\n\n.\n\n0\n\n3 \u00b7 uncertainty\n\n0\n\n.\n\n1\n\n5\n\n.\n\n0\n\n2\nv\n\n0\n\n.\n\n0\n\n5\n\n.\n\n0\n-\n\n0\n\n.\n\n1\n-\n\n0\n\n1\n\n2\n\nx\n\n3\n\n4\n\n-1.0\n\n-0.5\n\n0.5\n\n1.0\n\n0.0\n\nv1\n\ncircles shows pairs (x, u bg(x)) accepted, giving y = x, and crosses show pairs\nfor which x is rejected. these lie in the set {(x, w) : f (x) \u2264 w \u2264 bg(x)}, whose\narea is b \u2212 1, while the area under bg(x) is ofcourse b. the proportion of re-\njections is minimized by choosing \u03bb to minimize b, and this occurs when \u03bb = 1,\n\u22121 = 0.760. whether the resulting algorithm is faster than simply taking y =\ngiving b\n|\u0001\u22121(u )| will depend on the speeds of the functions and the arithmetical operations\n(cid:1)\ninvolved.\n\nrejection can be combined with other methods to give efficient algorithms.\n\nfigure 3.5 simulation\nby rejection algorithms.\nleft panel: half-normal\ndensity f (solid) and\nenvelope function bg\n(dots), with points for\nwhich x rejected shown\nby crosses and those\naccepted by circles. right\npanel: pairs (v1, v2) are\ngenerated uniformly in the\nsquare [\u22121, 1] \u00d7 [\u22121, 1],\nbut only those in the disk\n\u2264 1 are accepted.\nv 2\n1\nthey are then transformed\ninto two independent\nnormal variables.\n\n+ v 2\n\n2\n\n(cid:14)\n\n(cid:4)\n\n(cid:5)(cid:15)\n\nexample 3.21 (normal distribution) let z1 and z2 be two independent standard\nnormal variables. their joint density is\n\nf (z1, z2) = \u03c6(z1)\u03c6(z2) = 1\n2\u03c0\n\nexp\n\n\u2212 1\n2\n\n+ z2\n\n2\n\nz2\n1\n\n, \u2212\u221e < z1, z2 < \u221e.\n\n2)1/2 and \u03b8 =\n\u22121(z2/z1), in terms of which z1 = r cos \u03b8, z2 = r sin \u03b8. the transformation from\n\nthe polar coordinates of the point (z1, z2) inthe plane are r = (z2\ntan\n(z1, z2) to (r, \u03b8) has jacobian\n\n+ z2\n\n1\n\n(cid:6)(cid:6)(cid:6)(cid:6) =\n(cid:6)(cid:6)(cid:6)(cid:6) cos \u03b8\n(cid:6)(cid:6)(cid:6)(cid:6) \u2202(z1, z2)\nsin \u03b8\n\u2212r sin \u03b8\nr cos \u03b8\n2)1/2 and \u0001 = tan\n(cid:6)(cid:6)(cid:6)(cid:6) = 1\n(cid:6)(cid:6)(cid:6)(cid:6) \u2202(z1, z2)\n\u2212 1\n2\n\n+ z 2\n\n\u2202(r, \u03b8)\n\nr exp\n\n(cid:2)\n\n2\u03c0\n\nr 2\n\n1\n\n(cid:6)(cid:6)(cid:6)(cid:6) = r > 0,\n\u22121(z2/z1) is\n(cid:3)\n\n\u2202(r, \u03b8)\nso the joint density of r = (z 2\n\nf (r, \u03b8) = f (z1, z2)\n\nr > 0, 0 \u2264 \u03b8 < 2\u03c0.\n\n,\n\nevidently r and \u0001 are independent, with \u0001 uniform on the interval [0, 2\u03c0) and r\niid\u223c u (0, 1), we can\nhaving distribution pr(r \u2264 r) = 1 \u2212 exp(\u2212r 2/2). thus if u1, u2\ngenerate z1 and z2 by setting z1 = r cos \u0001, z2 = r sin \u0001, where \u0001 = 2\u03c0u1 and\nr = (\u22122 log u2)1/2; this amounts to inversion for r and \u0001.\na drawback of this method is that trigonometric functions such as sin(\u00b7) and cos(\u00b7)\ntend to be slow. it is better to avoid them by using rejection, as follows. we first generate\niid\u223c u (0, 1) and set v1 = 2u1 \u2212 1 and v2 = 2u2 \u2212 1; (v1, v2) isuniformly\nu1, u2\n\n "}, {"Page_number": 93, "text": "2\n\n1\n\n81\n\n3.3 \u00b7 simulation\n+ v 2\ndistributed in the square [\u22121, 1] \u00d7 [\u22121, 1]. if s = v 2\n> 1, we reject (v1, v2)\nand start again; see the right panel of figure 3.5. if it is accepted, the point (v1, v2) is\nuniform in the unit disk, s is independent of the angle \u0001 = tan\n\u22121(v2/v1) bysymmetry,\nand comparison of areas gives pr(s \u2264 s) = (s\u03c0)/\u03c0 = s, 0 \u2264 s \u2264 1, so s \u223c u (0, 1);\nthis implies that r d= (\u22122 log s)1/2. furthermore, if (v1, v2) has been accepted, then\ncos \u0001 = v1/s1/2, sin \u0001 = v2/s1/2. then z1 = r cos \u0001 = v1(\u22122s\n\u22121 log s)1/2 and\nz2 = r sin \u0001 = v2(\u22122s\n\u22121 log s)1/2 are independent standard normal variables, and\nmay be obtained without recourse to trigonometric functions. the efficiency of this\n.= 0.785.\nalgorithm is \u03c0/4\niid\u223c n (0, 1), then their ratio c = z2/z1 has a cauchy distribution. thus\nif we want to generate a cauchy variable, we need only take r sin \u0001/(r cos \u0001) =\nv2/v1, where (v1, v2) lies inside the unit disk. this suggests the ratio of uniforms\n(cid:1)\nmethod (problem 3.7).\n\nif z1, z2\n\nit may be hard to find an envelope density g(y) for f (y), leading to a high initializa-\ntion cost for rejection sampling. if f (y) islog-concave, however, so h(y) = log f (y)\nis concave in y, then it turns out to be easy to find an envelope from which quick\nsimulation is possible. to see how, let\nf (y) be alog-concave density with known\nsupport [yl , yu ], where possibly yl = \u2212\u221e or yu = \u221e or both. then for any y1, y2\nin [yl , yu ],\n\nh{\u03b3 y1 + (1 \u2212 \u03b3 )y2} \u2265\u03b3 h(y1) + (1 \u2212 \u03b3 )h(y2),\n\n0 \u2264 \u03b3 \u2264 1,\n\n(y) =\nand if h(y) ispiecewise differentiable, as we henceforth assume, then h\ndh(y)/dy is monotonic decreasing in y, though perhaps h(y) has straight line seg-\nments or h\n\n(cid:5)\n(y) isdiscontinuous.\nyl \u2264 y1 < \u00b7\u00b7\u00b7 < yk \u2264 yu\n(yk) are known. if yl = \u2212\u221e we choose y1 so that h\n\nlet\nand\n(cid:5)\n(y1) > 0.\n(y1), . . . , h\nh\nlikewise if yu = \u221e, wechoose yk so that h\n(yk) < 0. we then define a function\nh+(y) bytaking the upper boundary of the convex hull generated by the tangents to\nh(y) at y1, . . . , yk; see figure 3.6. that is,\n\nh(y1), . . . , h(yk)\n\nsuppose\n\nthat\n\nand\n\n(cid:5)\n\n(cid:5)\n\n(cid:5)\n\n(cid:5)\n\n\uf8f1\uf8f2\n\uf8f3 h(y1) + (y \u2212 y1)h\n(y1),\nh(y j+1) + (y \u2212 y j+1)h\n(cid:5)\nh(yk) + (y \u2212 yk)h\n(yk),\n\n(cid:5)\n\n(cid:5)\n\n(y j+1),\n\nh+(y) =\n\nyl < y \u2264 z1,\nz j \u2264 y \u2264 z j+1, j = 1, . . . ,k \u2212 1,\nzk \u2264 y < yu ,\n\nwhere\n\nz j = y j + h(y j ) \u2212 h(y j+1) + (y j+1 \u2212 y j )h\n\nh(cid:5)(y j+1) \u2212 h(cid:5)(y j )\n\n(cid:5)\n\n(y j+1)\n\n,\n\nj = 1, . . . ,k \u2212 1,\n\nare the values of y at which the tangents at y j and y j+1 intersect; we also set z0 = yl\nand zk = yu . asthe density g+(y) \u221d exp{h+(y)} consists of k piecewise exponential\nportions, a variable x with density g+ may be generated by inversion and then rejection\n(cid:5)\napplied. if the x thus generated is rejected, then h(x) and h\n(x) can be used to update\nh+ and provide a better envelope for subsequent simulation.\n\n "}, {"Page_number": 94, "text": "figure 3.6 adaptive\nrejection sampling from\nlog-concave density\nproportional to h(y)\n(solid). the left panel\nshows the initial envelope\n(heavy), formed as the\nconcave hull of tangents\n(dotted) to h(y) at\ny = \u22123.1, 1.9 (rug). the\nenvelope density looks\nlike two exponential\ndensities, back to back,\nfrom which a value shown\nby a cross is generated.\nthis value is rejected but\nused to update the\nenvelope to that on the\nright, so the corresponding\ndensity has three\nexponential parts. this\ntime the value generated\nby rejection sampling\n(circle) is accepted.\n\n82\n\n)\ny\n(\nh\n\n0\n1\n\n0\n\n0\n1\n-\n\n0\n2\n-\n\n0\n3\n-\n\n0\n4\n-\n\n0\n5\n-\n\n3 \u00b7 uncertainty\n\n)\ny\n(\nh\n\n0\n1\n\n0\n\n0\n1\n-\n\n0\n2\n-\n\n0\n3\n-\n\n0\n4\n-\n\n0\n5\n-\n\n-8 -6 -4 -2\n\n0\n\n2\n\n4\n\n-8 -6 -4 -2\n\n0\n\n2\n\n4\n\ny\n\ny\n\nthis discussion suggests an adaptive rejection sampling algorithm:\n\n1. initialize by choosing\n\n(cid:5)\n\ny1 < \u00b7\u00b7\u00b7 < yk,\n(yk), h+(y) and g+(y). then\n\n(cid:5)\n\n(y1), . . . , h\nh\nu \u2264 exp{h(x) \u2212 h+(x)} then set y = x and return y ; otherwise\n\n2. generate independent variables x from g+ and u from the u (0, 1) density. if\n3. replace k by k + 1, update y1, . . . , yk, h(y1), . . . , h(yk) and h\n\n(y1), . . . , h\n\n(yk) by\n\n(cid:5)\n\n(cid:5)\n\ncalculating h(y1), . . . , h(yk)\n\nand\n\nadding x, h(x) and h\n\n(cid:5)\n\n(x), recompute h+(y) and g+(y) and go to 2.\n(cid:5)\n\n(cid:5)\n\nthis can be accelerated by using h(y1), . . . , h(yk) and h\n(yk) toadd a\nlower envelope h\u2212(y) and then accepting x if u \u2264 exp{h\u2212(x) \u2212 h+(x)}, inwhich\ncase h(x) need not be computed (problem 3.12).\n\n(y1), . . . , h\n\nexample 3.22 (adaptive rejection) to illustrate this we take\n\nh(y) = r y \u2212 m log(1 + ey) \u2212 (y \u2212 \u00b5)2\n\n+ c, \u2212\u221e < y < \u221e,\n\n2\u03c3 2\n\nwhere m, \u03c3 2 > 0 and c is the constant ensuring that exp{h(y)} has unit integral; see\nexample 11.26. as we deal only with ratios of densities we can ignore c below, and\nfigure 3.6 shows h(y) for r = 2, m = 10, \u00b5 = 0, \u03c3 2 = 1 and when we set c = 0;\nhere yl = \u2212\u221e and yu = \u221e.\nan initial search establishes that h\n\n(1.9) < 0, and the resulting\nenvelope is shown in the left panel. the corresponding density g+(y) looks like two\nback-to-back exponential densities, from which it is easy to simulate a value x. this\nis accepted if ug+(x) < h(x), where u \u223c u (0, 1). in the event, the value \u22120.5 is\ngenerated but not accepted, and the envelope is updated to that shown in the right\npanel. a value generated from the new g+(y) isaccepted, terminating the algorithm.\n(cid:1)\notherwise the envelope would again be updated, and the process repeated.\n\n(\u22123.1) > 0 and h\n\n(cid:5)\n\n(cid:5)\n\napplications\nfast, tested generators are available in many statistical packages, so the details can\noften \u2014 but not always \u2014 be ignored. here are two uses of them.\n\n "}, {"Page_number": 95, "text": "3.3 \u00b7 simulation\n\n83\n\nfigure 3.7 numbers of\nwomen in the delivery\nsuite over a week of\nsimulations from the\nmodel for the birth data.\nalso shown are arrival and\ndeparture times for the\nfirst 25 simulated women.\n\nn\ne\nm\no\nw\n\n \nf\n\no\n\n \nr\ne\nb\nm\nu\nn\n\n5\n1\n\n0\n1\n\n5\n\n0\n\n0\n\n2\n\n4\n\ndays\n\n6\n\nexample 3.23 (birth data) the data in example 2.3 were collected in order to\nassess the workload in the delivery suite. examples 2.11 and 2.12 suggest that the daily\nnumber of arrivals leading to normal deliveries is poisson with mean about \u03bb = 12.9,\nand that each woman remains for a period whose density is roughly gamma with\nshape \u03b1 = 3.15 and mean \u00b5 = 7.93 hours, independent of the others. to simulate t\ndays of data from this model, we generate a poisson random variable n with mean \u03bb\nfor each day, and then generate n arrival times uniformly through the day. we create\ndeparture times by adding a gamma variable with mean \u00b5 and shape \u03b1 to each arrival\ntime; of course a woman may not depart on the day she arrived. we repeat this for\neach day, and record how many women are present at each arrival and departure.\n\nfigure 3.7 shows a week of simulated workload. note the initial \u2018burn-in\u2019 period,\ndue to starting with no women present rather than in steady state. the number present\nhas long-run average 12.9 \u00d7 7.93/24 = 4.26, but it fluctuates widely, with bursts of\nactivity when several women arrive almost together.\n\nsuch simulations show the random variation in the process due to the model, but\nthey do not reflect the fact that the model itself is uncertain, because it has been\nestimated. however it would be easy to change \u03bb, \u03b1, and \u00b5, or toreplace the gamma\nby a different distribution, and then to repeat the simulation. this would help assess\nthe effect of model uncertainty.\n\non leaving the delivery suite, women and their babies go to a ward where midwives\ngive post-natal care. at one stage hospital managers hoped to save money by imposing\na rigid demarcation between ward and delivery suite, but this would have been counter-\nproductive. according to hospital guidelines, each woman in the delivery suite should\nhave a midwife with her at all times, so when bursts of activity begin it is essential to\nbe able to call in midwives immediately. it is more expensive to do so from outside,\nso costs are reduced by allowing easy transfer of workers between ward and suite. (cid:1)\n\nthe previous example illustrates a particularly simple queueing system \u2014 each\n\u2018customer\u2019 must be dealt with at once, so there is no queue! more complicated queues\narise in many contexts, and discrete-event simulation packages exist to help operations\nresearchers estimate quantities such as the average waiting-time.\n\n "}, {"Page_number": 96, "text": "84\n\n3 \u00b7 uncertainty\n\nwe now use simulation use to assess properties of a statistical procedure.\n\nexample 3.24 (t statistic) the elements of a random sample y1, . . . ,y n from the\nn (\u00b5, \u03c3 2) density may be expressed y j = \u00b5 + \u03c3 z j , where the z j are standard normal\nvariables. the t statistic may be written as\n\nt = y \u2212 \u00b5\n\n(s2/n)1/2\n\n=\n\n(cid:8)\n\nn1/2(\u00b5 + \u03c3 z \u2212 \u00b5)\nj (z j \u2212 z)2\n\n(cid:1)\n\n(n \u2212 1)\u22121\u03c3 2\n\n(cid:9)1/2\n\n= n1/2 z\nsz\n\n,\n\nsay, whether or not the z j are normal. when they are, t has a student t distribution\non n \u2212 1 degrees of freedom and its quantiles tn\u22121(\u03b1) may be explicitly calculated,\nleading to the exact (1 \u2212 2\u03b1) confidence interval (3.17). how badly does that interval\nfail when the data are not normal?\n\nsuppose the z j have mean zero but distribution f otherwise unspecified. then the\n\nconfidence interval (3.17) contains \u00b5 with probability\n\u22121/2stn\u22121(1 \u2212 \u03b1) \u2264 \u00b5 \u2264 y \u2212 n\n\ny \u2212 n\n\npr\n\n(cid:8)\n\n(cid:9)\n\n\u22121/2stn\u22121(\u03b1)\n\n,\n\n(3.26)\n\nand this equals\n\npr{tn\u22121(\u03b1) \u2264 t \u2264 tn\u22121(1 \u2212 \u03b1)} = pr\n\n(cid:16)\n\n(cid:17)\n\n\u2264 tn\u22121(1 \u2212 \u03b1)\n\ntn\u22121(\u03b1) \u2264 n1/2 z\nsz\n\n= p(1 \u2212 \u03b1, n, f) \u2212 p(\u03b1, n, f),\n(cid:16)\n\n(cid:17)\n\np(\u03b1, n, f) = pr\n\nn1/2 z\n\nsz\n\n\u2264 tn\u22121(\u03b1)\n\n.\n\nsay, where\n\nwhen f is normal, p(\u03b1, n, f) = \u03b1 and (3.26) is (1 \u2212 2\u03b1), as it should be.\ngiven any f, \u03b1 and n, weestimate p(\u03b1, n, f) thus. for r = 1 . . . , r,\n\ni\n\nr\n\ne\n\nir\n\n\u22121\n\n!\n\n(cid:31)\n\n(cid:13)\n\nn1/2 z\n\n= e\n\n(cid:17)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6) z1, . . . , zn\n\niid\u223c f;\nr generate z1, . . . , zn\nr calculate tr = n1/2 z /sz ; then\nr set ir = i {tr \u2264 tn\u22121(\u03b1)}.\nhaving obtained i1, . . . , ir, wecompute (cid:7)p = r\n(cid:12)\n(cid:16)\n\u2264 tn\u22121(\u03b1)\n\n(cid:1)\nr ir , whose expectation is\nr(cid:21)\n= p(\u03b1, n, f).\n\u22121\n(cid:1)\nr=1\nr ir is binomial with denominator r and probability p(\u03b1, n, f), so (cid:7)p has\nnow\n\u22121 p(\u03b1, n, f){1 \u2212 p(\u03b1, n, f)}. this can be used to gauge the value of r\nvariance r\n.= \u03b1 =\nneeded to estimate p(\u03b1, n, f) with given precision. for example, if p(\u03b1, n, f)\n0.05, then r = 1600 gives standard deviation roughly {0.05(1 \u2212 0.05)/1600}1/2 =\n0.0054, and a crude 95% confidence interval for p(\u03b1, n, f) is(cid:7)p \u00b1 0.01.\ntable 3.2 shows values of 100(cid:7)p for various distributions f, using n = 10 and\nr = 1600. the second and third columns are for \u03b1 = 0.05, 0.95, while the fourth\nshows the estimated probability that the confidence interval contains \u00b5; ideally this\n\niid\u223c f\n\nsz\n\nrecall that y and s2 are\nthe average and sample\nvariance of y1, . . . ,y n.\n\ni{a} is the indicator of\nthe event a.\n\n "}, {"Page_number": 97, "text": "table 3.2 estimated\ncoverage probabilities\np(\u03b1, n, f),\np(1 \u2212 \u03b1, n, f), and p(1 \u2212\n\u03b1, n, f) \u2212 p(\u03b1, n, f), for\n\u03b1 = 0.05 and 0.025, for\n1600 samples of size\nn = 10 from various\ndistributions. the laplace\nand mixture densities are\n2 exp(\u2212|z|) and\n1\n0.9\u03c6(z) + 0.1\u03c6(z/3)/3,\nfor z \u2208 ir, and t\u03bd denotes\nthe t density on \u03bd degrees\nof freedom. the \u2018slash\u2019\ndistribution is that of\nz /u , where z \u223c n (0, 1)\nand u \u223c u (0, 1)\nindependently. the\nestimates have been\nmultiplied by 100 for\nconvenience and have\nstandard errors of\nabout 0.5.\n\n3.3 \u00b7 simulation\n\n85\n\ntarget\n\nf\n\n5\n\n95\n\n90\n\n2.5\n\n97.5\n\n95\n\nnormal\nlaplace\nmixture\nt20\nt10\nt5\nt1 (cauchy)\nslash\ngamma, \u03b1 = 2\n\n4.9\n4.1\n4.0\n5.4\n6.1\n4.6\n2.3\n2.6\n9.7\n\n94.7\n94.9\n94.9\n95.4\n93.9\n95.3\n97.3\n97.4\n97.9\n\n88.8\n90.8\n90.9\n90.1\n87.8\n90.7\n95.1\n94.9\n88.3\n\n2.6\n2.2\n1.9\n2.2\n2.6\n2.5\n0.8\n1.3\n6.3\n\n97.1\n98.1\n98.0\n97.7\n97.0\n98.1\n99.1\n99.3\n99.1\n\n94.4\n95.9\n96.1\n95.5\n94.4\n95.6\n98.3\n98.0\n92.8\n\nwould be 1 \u2212 2\u03b1 = 0.90 . columns 5\u20137 give the same quantities for 95% confidence\nintervals. the first row is included to check the simulation: it does not hit the target\nexactly, due to simulation randomness, but it is close. laplace, mixture, \u2018slash\u2019 and\nt\u03bd densities have heavier tails than the normal; the mixture corresponds to n (0, 1)\nsamples that are occasionally contaminated by n (0, 32) variables. the results suggest\nthat heavy-tailed data have little effect on the probabilities until the extreme cases\n\u03bd = 1 and the \u2018slash\u2019 distribution, for both of which the z j have infinite mean. then\nthe intervals are too wide and therefore have too great a chance of containing \u00b5. the\ngamma distribution is the only asymmetric case, and this shows in the estimated one-\ntailed probabilities p, though the estimates of p(1 \u2212 \u03b1, n, f) \u2212 p(\u03b1, n, f) remain\nreasonably close to (1 \u2212 2\u03b1). overall the performance of t seems fairly satisfactory\nunless the data are grossly non-normal.\n\nsimulation timings depend on the computer and language used, as well as the skill\nof the programmer, so they are often uninformative. having said this, it took about\n20 seconds to obtain each row of the table, using about 25 lines of code in total. this\ncompares very favourably with the time and effort that would be involved in getting\n(cid:1)\nsuch results analytically.\n\n3.3.2 variance reduction\n\nthis section may be\nskipped on a first reading. even though it involves no chemicals or nasty smells, a simulation experiment is\nnonetheless an experiment, and it may be worth considering how to increase its\nprecision for a given effort. there are numerous ways to do this, but as they all\ninvolve extra work on the part of the experimenter, they are only worthwhile when\nthe amount of simulation is large: a reduction from 30 to five seconds matters much\nless than one from 30 to five days.\nsuppose that we wish to estimate properties of a rather awkward statistic t =\nt(y1, . . . ,y n) that is correlated with a statistic w = w(y1, . . . ,y n) with known prop-\nerties. then one way to use w is to write t = w + (t \u2212 w ) = w + d, say, work\nout the relevant properties of the control variate w analytically, and use simulation\nonly for the difference d. for example, if moments of w are available explicitly but\n\n "}, {"Page_number": 98, "text": "86\n\nf\n\nnormal\n\nt5\n\n3 \u00b7 uncertainty\n\np\n\n0 (average)\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5 (median)\n\ntable 3.3 estimated\nvariances of p \u00d7 100%\ntrimmed averages in\nsamples of size n = 21\nfrom the normal and t5\ndistributions.\n\nnvar(t )\ncorrelation\nefficiency gain\n\nnvar(t )\ncorrelation\nefficiency gain\n\n1\n1\n\u221e\n\n1.67\n1\n\u221e\n\n1.05\n0.98\n10.4\n\n1.38\n0.93\n2.1\n\n1.13\n0.95\n4.9\n\n1.37\n0.89\n1.4\n\n1.23\n0.91\n3.1\n\n1.42\n0.84\n1.1\n\n1.35\n0.86\n2.3\n\n1.53\n0.80\n1\n\n1.54\n0.81\n1.9\n\n1.73\n0.75\n0.9\n\nwe want to estimate the variance of t , wewrite\n\nvar(t ) = var(w ) + 2cov(w, d) + var(d),\n\nwhere only terms involving d need to be estimated by simulation. we then gener-\nate r independent samples y1, . . . ,y n and calculate t , w and d for each, giving\n(tr , wr , dr ), r = 1, . . . , r. then var(t ) isestimated by\n(wr \u2212 w )(dr \u2212 d) + 1\nr \u2212 1\n\nv1 = var(w ) + 2\nr \u2212 1\n\n(dr \u2212 d)2,\n\n(cid:1)\nwhere the exact quantity var(w ) replaces the sample variance of w1, . . . , wr. the\nr (tr \u2212 t )2. if var(w ) is alarge\nusual estimate of var(t ) would be v2 = (r \u2212 1)\npart of var(t ), then var(v1) may be much smaller than var(v2), but the efficiency gain\nvar(v2)/var(v1) will depend on the correlation between w and t .\n\nr(cid:21)\nr=1\n\nr(cid:21)\nr=1\n\n(3.27)\n\n\u22121\n\nexample 3.25 (trimmed average) let y1, . . . ,y n be a random sample from a\ndistribution f with mean \u00b5 and variance \u03c3 2. one estimate of \u00b5 is the sample average\ny , but as this is sensitive to bad values it may be preferable to use the p \u00d7 100%\ntrimmed average\n\nt = (n \u2212 2k)\n\n\u22121\n\nn\u2212k(cid:21)\nj=k+1\n\ny( j),\n\nwhere y(1) \u2264 \u00b7\u00b7\u00b7 \u2264 y(n) are the order statistics of the sample and k = pn is an integer.\none measure of the precision of t is its variance, and if we found that var(t ) < var(y )\nfor many different distributions f, wemight choose to use t rather than y . given\nf, var(t ) can in principle be obtained exactly, but as the calculations are tedious it is\nsimpler to simulate.\nan obvious control variate is w = y = n\nj y( j), which has variance \u03c3 2/n and\nis perfectly correlated with t if p = 0. we simulate as described above, obtaining r\nvalues of wr , tr and dr = tr \u2212 wr , and estimate var(t ) using (3.27). table 3.3 shows\nvalues of nv1 for samples of size n = 21 from the normal and the t5 distribution, using\nvarious values of p; wetook r = 1000 replicates. the table also shows the estimated\ncorrelation between w and t , and the efficiency gains due to use of control variates,\nestimated by repeating the experiment 50 times. in practice one would have just one\n\n(cid:1)\n\n\u22121\n\n "}, {"Page_number": 99, "text": "3.3 \u00b7 simulation\n\n87\n\nvalue of v1 and one of v2; the repetition here was needed only to find the efficiency\ngains. these are largest when p is small, and even infinite when p = 0, when w = t\nand var(v2) = 0. in this case d = t \u2212 w = 0, and as var(w ) = var(y ) isknown\nexactly, v1 is constant and hence var(v1) = 0; simulation is then unnecessary. the\nefficiency gains depend not only on the correlation between w and t , but also on the\nunderlying distribution f.\n\nfor normal data, the increase in variance when using t rather than y is modest\nfor p < 0.3, and for t5 data var(t ) < var(y ) when 0 < p < 0.5. this suggests that\na lightly trimmed average may be preferable to y for non-normal data and not much\nmore variable than y for normal data, but we would need more extensive results to\n(cid:1)\nbe sure.\n\nimportance sampling\nanother approach to variance reduction is importance sampling. the key idea here is\nthat sometimes most of the sampling is unproductive, and then it is better to concentrate\non the parts of the sample space where it is most valuable. the idea is often used in\nmonte carlo integration. suppose we want to estimate\n\n(cid:10)\n\n\u03c8 = e{m(y )} =\n\nm(y)g(y) dy.\n\nthe direct approach is to generate y1, . . . ,y r independently from density g, and to\n\nset (cid:7)\u03c8 = r\n\n\u22121\n\n(cid:1)\n(cid:10)\nr m(yr ). this has mean and variance\ne((cid:7)\u03c8) = e{m(y )} =\u03c8,\n\nvar((cid:7)\u03c8) =\n\nm(y)2g(y) dy \u2212 \u03c8 2,\n\nbut itmay be a very poor estimate. for example, if m(y ) = i (y \u2264 a) and \u03c8 =\npr(y \u2264 a) is very small, then most of the yr will not contribute to (cid:7)\u03c8, and the effort\n\nspent in generating them will be wasted. instead we try simulating from a density h,\nchosen to concentrate effort in the important part of the sample space; the support\n(cid:1)\nof h must include the support of g. the resulting estimator is the raw importance\nr m(yr )w(yr ), where w = w(y ) = g(y )/ h(y ) is\n(cid:10)\n\nsampling estimator (cid:7)\u03c8raw = r\nknown as the importance sampling weight. the mean and variance of (cid:7)\u03c8raw are\nm(y)g(y) dy = \u03c8,\n\nh(y) dy =\n\n(cid:10)\n\n\u22121\n\nm(y)\n\ng(y)\nh(y)\n\ne((cid:7)\u03c8raw) = eh{m(y )w(y )} =\nvar((cid:7)\u03c8raw) = r\n\u22121varh{m(y )w(y )}\n(cid:14)(cid:10)\n= r\n\u22121[eh{m(y )2w(y )2} \u2212e h{m(y )w(y )}2]\n= r\n\u22121\n\ng(y) dy \u2212 \u03c8 2\nhence (cid:7)\u03c8raw will be a big improvement on (cid:7)\u03c8 if\n(cid:11)\n\nm(y)2 g(y)\nh(y)\n\n(cid:15)\n\n.\n\nvar((cid:7)\u03c8)\nvar((cid:7)\u03c8raw)\n\n(cid:11)\n\n=\n\nm(y)2g(y) dy \u2212 \u03c8 2\nh(y) g(y) dy \u2212 \u03c8 2\nm(y)2 g(y)\n\nis large. this ratio depends on h, abad choice of which can make (cid:7)\u03c8raw much more\nvariable than is (cid:7)\u03c8. the trick is to choose h well.\n\n(3.28)\n\nthe support of a density f\nis {y : f (y) > 0}.\n\neh denotes expectation\nwith respect to density h.\n\n "}, {"Page_number": 100, "text": "88\n\nz\n\u00b5z\nefficiency gain\n\ny\nt\ni\ns\nn\ne\nd\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n.\n0\n\n0\n.\n0\n\n3 \u00b7 uncertainty\n\n\u22122\n\n\u22123\n\u22123.15 \u22122.22 \u22121.34 \u22120.62 \u22120.18 \u22120.03 \u22120.002\n1.004\n222\n\n\u22121\n\n1.75\n\n1.19\n\n1.04\n\n4.1\n\n19\n\n0\n\n1\n\n2\n\n3\n\n\u2022\n\n\u2022\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\nt\n\ni\n\nh\ng\ne\nw\n\n0\n0\n5\n\n.\n\n0\n5\n\n.\n0\n\n5\n0\n.\n0\n\n1\n0\n.\n0\n\n-4\n\n-2\n\n0\n\nz\n\n2\n\n4\n\n-4\n\n-2\n\n2\n\n4\n\n0\n\nz\n\ntable 3.4 efficiency\ngains in importance\nsampling to estimate\nnormal probability \u0001(z).\n\u00b5z is the optimal tilting\nparameter.\n\nfigure 3.8\nimportance\nsampling for normal tail\nprobability. left: n (0, 1)\ndensity and area \u0001(z) to\nbe estimated (heavy\nshading), with importance\nsampling density\nn (\u00b5z , 1), whose lightly\n(cid:7)\u03c8raw. right: weights for\nshaded area contributes to\nsamples with r = 50\nfrom n (0, 1) (circles) and\nfrom n (\u00b5z , 1) (blobs).\nthe vertical line shows\nz = \u22121; only points to the\nleft of that line contribute\nto estimation of \u0001(z).\n\nexample 3.26 (normal probability) marooned on a desert island with only parrots\nfor company, a shipwrecked statistician decides to realize his lifelong ambition of\nmemorizing values of the normal integral \u0001(z); he hopes to make himself more\nattractive to the statisticienne of his dreams. his statistical tables have been ruined\nby salt water, but washed up on the beach he finds a programmable solar-powered\ncalculator on which he is able to implement a slow but reliable normal random number\ngenerator.\nrather than estimate \u03c8 = \u0001(z) directly, he decides to use importance sampling\nfrom the n (\u00b5, 1) distribution, taking m(y ) = i (y \u2264 z), g(y) = \u03c6(y), and h(y) =\n\u03c6(y \u2212 \u00b5). if y1, . . . ,y r\ni (yr \u2264 z) has mean \u0001(z) and vari-\n\u22121\u0001(z){1 \u2212 \u0001(z)}. if hesamples from h, the importance sampling estimate is\n(cid:7)\u03c8raw = r\nance r\n\u00b52 \u2212 \u00b5y),\n\n(cid:1)\nr w(yr )i (yr \u2264 z), where w(y) = \u03c6(y)/\u03c6(y \u2212 \u00b5) = exp( 1\n\niid\u223c g, then (cid:7)\u03c8 = r\n\n(cid:1)\n\n\u22121\n\n\u22121\n\n2\n\nand it turns out that\n\nvar((cid:7)\u03c8raw) = r\n\n\u22121{exp(\u00b52)\u0001(z + \u00b5) \u2212 \u0001(z)2}.\n\n(3.29)\ngiven z, therefore, the optimal value \u00b5z of \u00b5 minimizes e\u00b52 \u0001(z + \u00b5). table 3.4\nshows values of \u00b5z and the efficiency gain (3.28) for a few values of z. note how\n.= z for z < 0, but not for z > 0, and how importance sampling becomes in-\ncreasingly effective as z \u2192 \u2212\u221e, when almost none of the yr contribute to (cid:7)\u03c8. for\n\u00b5z\nz > 0, most of the observations contribute to (cid:7)\u03c8 and importance sampling gives little\n\nimprovement.\nthe panels of figure 3.8 show the optimal importance sampling distribution when\nz = \u22121 and the weights obtained in samples of size r = 50 from the n (0, 1) and\nn (\u00b5z, 1) distributions. most of the observations generated from \u03c6(y \u2212 \u00b5z) contribute\nto (cid:7)\u03c8raw, whereas only a few of those from \u03c6(y) contribute to (cid:7)\u03c8. the efficiency gain\n\n "}, {"Page_number": 101, "text": "3.3 \u00b7 simulation\n\n89\n\nof 4.1 implies that 50 observations from the n (\u00b5z, 1) distribution are worth about 200\nfrom the n (0, 1) distribution. the gains are larger when z \u2192 \u2212\u221e, and combined\nwith the fact that \u0001(z) = 1 \u2212 \u0001(\u2212z) should enable our hero to fulfil his ambition\n(cid:1)\nbefore he is rescued.\na difficulty with (cid:7)\u03c8raw is that the weights wr can be very variable, with one or\ntwo large ones dominating the rest, leading to the average weight w being very\ndifferent from its expectation eh(w ) = 1. this can be dealt with by rescaling the\n= wr /w , for which w (cid:5) = 1, resulting in the importance sampling\n(cid:5)\nratio estimator (cid:7)\u03c8rat = r\nweights to w\nr\n(cid:5)\nr m(yr ). another approach treats w as a control\nvariate, assuming that the pair (t , w ) = (m(y )w(y ), w(y )) has approximately a\nbivariate normal distribution, and then estimating the conditional mean of t given\nw = 1. this results in the importance sampling regression estimator\n\n(cid:1)\nr w\n\n\u22121\n\n(cid:7)\u03c8reg = t +\n\n(cid:1)\nr (wr \u2212 w )(tr \u2212 t )\n(cid:1)\nr (wr \u2212 w )2\n\n(1 \u2212 w ),\n\ntr = m(yr )w(yr );\n\nnote that(cid:7)\u03c8raw = t . if t and w are positively correlated, the ratio here will be positive,\nand if w > 1 the adjustment reduces(cid:7)\u03c8raw by an amount that depends on 1 \u2212 w . this\nmakes sense because if t and w are positively correlated and w > e(w ) = 1, then\n(cid:7)\u03c8raw.\nit is likely that t > e(t ). both ratio and regression estimators tend to improve on\n\nexercises 3.3\n1\n\n(cid:1)\n\nshow how to use inversion to generate bernoulli random variables. if 0 < \u03c0 <1, what\ndistribution has\n\nj=1 i (u j \u2264 \u03c0)?\n\nm\n\n4\n\n3\n\n2 write down algorithms to generate values from the gamma density with small integer shape\nparameter by (a) direct construction using exponential variables, (b) rejection sampling\nwith an exponential envelope.\nthe cholesky decomposition of an p \u00d7 p symmetric positive matrix \u0001 is the unique lower\ntriangular p \u00d7 p matrix l such that l l t = \u0001. find the distribution of \u00b5 + l z, where z\nis a vector containing a standard normal random sample z1, . . . , z p, and hence give an\nalgorithm to generate from the multivariate normal distribution.\nif inversion can be used to generate a variable y with distribution function f, discuss how\nto generate values from f conditioned on the events (a) y \u2264 yu , (b) yl < y \u2264 yu . under\nwhat circumstances might rejection sampling be sensible?\ndefine z by setting z = j when y \u2264 y j , for y1 < \u00b7\u00b7\u00b7 < yk\u22121 < yk = \u221e. give analgo-\nrithm to generate z.\nif x has density \u03bbe\nif y has geometric density pr(y = r) = \u03c0(1 \u2212 \u03c0)r\u22121, for r = 1, 2, . . . and 0 < \u03c0 <1,\nshow that y d= (cid:12)log u/ log(1 \u2212 \u03c0)(cid:13). hence give an algorithm to generate geometric vari-\nables.\nconstruct a rejection algorithm to simulate from f (x) = 30x(1 \u2212 x)4, 0 \u2264 x \u2264 1, using\nthe u (0, 1) density as the proposal function g. give its efficiency.\nverify (3.29).\n\n\u2212\u03bbx , x > 0, show that pr(r \u2212 1 \u2264 x \u2264 r) = e\n\n\u2212\u03bb(r\u22121)(1 \u2212 e\n\n\u2212\u03bb).\n\n5\n\n7\n\n6\n\n "}, {"Page_number": 102, "text": "(1894\u20131981) was born in\nmoldavia and studied\nmathematics at kharkov\nuniversity and then\nstatistics in warsaw and\nuniversity college\nlondon, where he worked\non the basis of hypothesis\ntesting with egon\npearson, on experimental\ndesign, and on sampling\ntheory. in 1938 he moved\nto berkeley and became a\nleading figure in the\ndevelopment of statistics\nin the usa.\n\n90\n\n3.4 bibliographic notes\n\n3 \u00b7 uncertainty\n\nthe idea of a confidence interval belongs to statistical folklore, but its mathematical\nformulation and the repeated sampling interpretation were developed by j. neyman\nin the 1930s. fisher argued strongly against the repeated sampling interpretation and jerzy neyman\ndeveloped his own approaches based on conditioning and fiducial inference. welsh\n(1996) gives a thoughtful comparison of these and other approaches to inference.\n\ninference procedures for normal samples are treated in many basic statistics texts.\nstochastic simulation is a very large topic. in addition to books such as rubinstein\n(1981), fishman (1996), morgan (1984), ripley (1987), and robert and casella\n(1999), there is a rapidly growing literature on simulation for stochastic processes,\noften using markov chain theory; see the bibliographic notes to chapter 11.\n\n3.5 problems\n\n1 suppose that y1, . . . ,y 4 are independent normal variables, each with variance \u03c3 2, but with\nmeans \u00b5 + \u03b1 + \u03b2 + \u03b3 , \u00b5 + \u03b1 \u2212 \u03b2 \u2212 \u03b3 , \u00b5 \u2212 \u03b1 + \u03b2 \u2212 \u03b3 , \u00b5 \u2212 \u03b1 \u2212 \u03b2 + \u03b3 . let\nz t = 1\n4 (y1 + y2 + y3 + y4, y1 + y2 \u2212 y3 \u2212 y4, y1 \u2212 y2 + y3 \u2212 y4, y1 \u2212 y2 \u2212 y3 + y4).\ncalculate the mean vector and covariance matrix of z, and give the joint distribution\nof z1 and v = z 2\n4 when \u03b1 = \u03b2 = \u03b3 = 0. what is then the distribution of\nz1/(v /3)1/2?\n2 wi , xi , yi , and zi , i = 1, 2, are eight independent, normal random variables with common\nvariance \u03c3 2 and expectations \u00b5w , \u00b5x , \u00b5y and \u00b5z . find the joint distribution of the random\nvariables\n\n+ z 2\n\n+ z 2\n\n2\n\n3\n\n(w1 + w2) \u2212 \u00b5w , t2 = 1\n(x1 + x2) \u2212 \u00b5x ,\n2\n(y1 + y2) \u2212 \u00b5y , t4 = 1\n(z1 + z2) \u2212 \u00b5z ,\n2\n\nt1 = 1\n2\nt3 = 1\n2\nt5 = w1 \u2212 w2, t6 = x1 \u2212 x2, t7 = y1 \u2212 y2, t8 = z1 \u2212 z2.\n\nhence obtain the distribution of\n\nu = 4\n\n+ t 2\n+ t 2\n\n2\n\n+ t 2\n+ t 2\n\n3\n\n+ t 2\n+ t 2\n\n4\n\n.\n\nt 2\n1\nt 2\n5\n\n7\n\n6\n\n8\n\nshow that the random variables u/(1 + u ) and 1/(1 + u ) are identically distributed,\nwithout finding their probability density functions. find their common density function\nand hence determine pr(u \u2264 2).\n3 figure 3.9 shows samples of size 100 from densities in which (i) x and y are independent;\n(ii) corr(x, y ) = \u22120.7; (iii) corr(x, y ) = 0.7; (iv) corr(x, y ) = 0. say which is which\nand why.\n(a) suppose that conditional on \u03b7, y1, . . . ,y n is a random sample from the n (\u03b7, \u03c3 2)\ndistribution, but that \u03b7 has itself a n (\u00b5, \u03c3 2\n\u03b7 ) distribution. show that the unconditional dis-\ntribution of y1, . . . ,y n is multivariate normal, with correlation \u03c1 = \u03c3 2\n\u03b7 ) between\ndifferent variables.\n(b) show that\n\n\u03b7 /(\u03c3 2 + \u03c3 2\n\n4\n\nw = (y \u2212 \u00b5)/(s2/n)1/2 d= {1 + n\u03c1/(1 \u2212 \u03c1)}1/2t ,\n\nwhere t \u223c tn\u22121. hence show that the probability that the usual confidence interval (3.17)\ncontains \u00b5 is 1 \u2212 2pr{t \u2264 tn\u22121(\u03b1)(1 + n\u03c3 2\n\u22121/2} and verify that when \u03b1 = 0.025,\nn = 10 and \u03c1 = 0.1, this probability is 0.85, and that when n = 100 and \u03c1 = 0.01, 0.02,\nit is 0.84, 0.74.\n\n\u03b7 /\u03c3 2)\n\n "}, {"Page_number": 103, "text": "figure 3.9 samples\nfrom bivariate\ndistributions with\ncorrelations \u22120.7, 0, 0.7;\none sample has\nindependent components.\nwhich is which? why?\n\n3.5 \u00b7 problems\n\na\n\n2\n\n1\n\ny\n\n0\n\n1\n-\n\n2\n-\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\nb\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n2\n\n1\n\n\u2022\n\ny\n\n0\n\n1\n-\n\n2\n-\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n1\n\n2\n\n2\n\n1\n\ny\n\n0\n\n1\n-\n\n2\n-\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\nc\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n-2\n\n-1\n\n0\n\nx\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n1\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n2\n\nd\n\n\u2022\n\n2\n\n1\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n0\n\nx\n\n1\n\n\u2022\n\n-2\n\n-1\n\ny\n\n0\n\n1\n-\n\n2\n-\n\n91\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n2\n\n\u2022\n\n-2\n\n-1\n\n1\n\n2\n\n0\n\nx\n\n\u2022\n\n-2\n\n-1\n\n0\n\nx\n\nwhat does this tell you about the assumptions underlying (3.17)?\n5 if z is standard normal, then y = exp(\u00b5 + \u03c3 z) issaid to have the log-normal distribution.\nshow that e(y r ) = exp(r \u00b5)mz (r \u03c3 ) and hence give expressions for the mean and variance\nof y . show that although all its moments are finite, y does not have a moment-generating\nfunction.\n6 (a) let y = z1 and w = z2 \u2212 \u03bbz1, where z1, z2 are independent standard normal vari-\nables and \u03bb is a real number. show that the conditional density of y given that w < 0 is\nf (y; \u03bb) = 2\u03c6(y)\u0001(\u03bby); y is said to have a skew-normal distribution. sketch f (y; \u03bb) for\nvarious values of \u03bb. what happens when \u03bb = 0?\n(b) show that y 2 \u223c \u03c7 2\n1 .\n(c) use exercise 3.2.16 to show that y has cumulant-generating function t 2/2 + log \u0001(\u03b4t),\nwhere \u03b4 = \u03bb/(1 + \u03bb2)1/2, and hence find its mean and variance. show that the standardized\nskewness of y varies in the range (\u22120.995, 0.955).\n\n7 for h(x) anon-negative function of real x with finite integral, let\n\nch = (cid:8)\n\n(u, v) : 0\u2264 u \u2264 h(v/u)1/2\n\n(cid:9)\n\n.\n\n(cid:11)\n\n(a) by considering the change of variables (u, v) \u2192 (w = u, x = v/u), show that ch has\nfinite area, and that if (u, v ) isuniformly distributed on ch, then x = v /u has density\n(b) if h(x) and x 2h(x) are bounded and a = \u221a\nh(x)/\n\nh(y) dy.\n\n\"\nsup{x 2h(x) : x \u2265 0},\n\nb+ =\n\nsup{h(x) :\u2212\u221e < x < \u221e},\n\"\nsup{x 2h(x) : x \u2264 0},\nb\u2212 = \u2212\n\nshow that ch \u2282 [0, a] \u00d7 [b\u2212, b+]. hence justify the following algorithm:\n1 repeat\n\niid\u223c u (0, 1);\n\nr generate u1, u2\nr let u = au1, v = b\u2212 + (b+ \u2212 b\u2212)u2;\nuntil (u, v ) \u2208 ch.\n2 return x = v /u .\n(c) if h(x) = (1 + x 2)\ngenerating cauchy variables described in example 3.21.\n(d) if h(x) = e\nalgorithm.\n(e) if h(x) = e\nis accepted if and only if v 2 \u2264 \u22124u 2 log u . hence give the algorithm.\nnominators m1, m2, and let pi = ri /mi . it isdesired to test if \u03c01 = \u03c02.\nlet(cid:7)\u03c0 = (m1 p1 + m2 p2)/(m1 + m2). show that when \u03c01 = \u03c02, the statistic\n\n\u22121 on \u2212\u221e < x < \u221e, show that this algorithm gives the method for\n\u2212x on 0 < x < \u221e, show that a = 1, b\u2212 = 0, and b+ = 2/e, and give the\n\u2212x2/2 on \u2212\u221e < x < \u221e, find the values of a, b\u2212 and b+, and show that x\n\n8 let r1, r2 be independent binomial random variables with probabilities \u03c01, \u03c02 and de-\n\nd\u2212\u2192 n (0, 1)\nwhen m1, m2 \u2192 \u221e in such a way that m1/m2 \u2192 \u03be for 0 < \u03be <1.\n\n\u221a(cid:7)\u03c0(1 \u2212(cid:7)\u03c0)(1/m1 + 1/m2)\n\nz =\n\np1 \u2212 p2\n\n "}, {"Page_number": 104, "text": "92\n\n)\n$\n(\n \ns\ne\nc\ni\nr\np\n \ng\nn\ns\no\nc\n \no\no\nh\na\ny\n\nl\n\ni\n\nt\n\nn\nr\nu\ne\nr\n \ng\no\nl\n\nn\nr\nu\nt\ne\nr\n \ng\no\nl\n \nd\ne\nr\ne\nd\nr\no\n\n0\n\n200\n\n400\n\n600\n\n800\n\n1000\n\ntrading days since 12/4/96\n\n1\n+\n\n \n\n \n\nj\n \ny\na\nd\nn\no\nn\nr\nu\ne\nr\n \ng\no\nl\n\nt\n\n0\n0\n2\n\n0\n5\n1\n\n0\n0\n1\n\n0\n5\n\n0\n\n0\n2\n\n0\n1\n\n0\n\n0\n1\n-\n\n0\n2\n-\n\n0\n2\n\n0\n1\n\n0\n\n0\n1\n-\n\n0\n2\n-\n\n0\n2\n\n0\n1\n\n0\n\n0\n1\n-\n\n0\n2\n-\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n. .\n.\n.\n.\n.\n..\n.\n.\n..\n.\n.\n. .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n..\n.\n.\n...\n.\n.\n.\n.\n. .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n..\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n..\n.\n..\n.\n.\n.\n.\n.\n.\n. .\n. .\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n..\n.. .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n..\n. .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n.\n..\n.\n.\n.\n.\n.\n..\n.\n..\n.\n.\n.\n.\n.\n..\n..\n.\n.\n.\n....\n.\n. .\n.\n.\n.\n. .\n.\n.\n.\n.\n.\n.\n.\n.. . .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n..\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n. .\n.\n...\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n.\n.\n.\n.\n......\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n...\n.\n.\n.\n..\n.\n..\n..\n.\n. .\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n..\n. ..\n.\n.\n..\n.\n.\n..\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n.\n.\n. .\n.\n.\n..\n..\n.\n.\n. .\n.\n.\n. .\n..\n..\n.\n.\n. .\n..\n.\n.\n.\n.\n..\n..\n.\n.\n.\n.\n...\n.\n.\n. .\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n...\n.\n.\n.\n.\n.\n.\n...\n.\n.\n..\n.\n.\n.\n.\n.\n. . .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n..\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n...\n.\n.\n.\n.\n.\n.\n.. .\n..\n.\n..\n.\n.\n.\n.\n..\n.. .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.. .\n..\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n...\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n..\n. .\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n..\n. .\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n..\n.\n. .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n.\n..\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\nl\n\nn\nr\nu\nt\ne\nr\n \ny\nk\ne\ne\nw\n \ng\no\nl\n \nd\ne\nr\ne\nd\nr\no\n\n-2\n\n0\n\n2\n\nquantiles of standard normal\n\n.\n\n.\n\n.\n.\n\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n.\n. .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n.\n.\n.\n.\n.\n.\n.. .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n...\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. .....\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n....\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n.\n. .\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.. .\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n\n.\n.\n\n.\n.\n.\n\n.\n.\n\n.\n\n.\n\nl\n\nt\n\nn\nr\nu\ne\nr\n \ny\nk\ne\ne\nw\ng\no\nl\n\n \n\n.\n\n.\n\n.\n\n.\n\n3 \u00b7 uncertainty\n\n.\n\n.\n\n.\n\n.\n\n.\n\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n. .\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n. .\n.\n.\n.\n.\n.\n..\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n...\n.\n.\n.\n.\n.\n...\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n...\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.....\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n-3\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\n3\n\nquantiles of standard normal\n\nfigure 3.10 analysis of\nyahoo.com share values.\nleft: share price x j from\n12 april 1996 to 26 april\n2000 (above); log daily\nreturns\ny j = 100 log(x j /x j\u22121)\n(below). centre: normal\nprobability plot of y j\n(above) and plot of y j+1\nagainst y j (below). right:\nnormal probability plot of\nlog weekly returns\n(above); log weekly\nreturns (below).\n\n0\n3\n\n0\n2\n\n0\n1\n\n0\n\n0\n1\n-\n\n0\n2\n-\n\n0\n3\n-\n\n0\n3\n\n0\n2\n\n0\n1\n\n0\n\n0\n1\n-\n\n0\n2\n-\n\n0\n3\n-\n\n0\n\n200\n\n400\n\n600\n\n800\n\n1000\n\n-20\n\n-10\n\n0\n\n10\n\n20\n\n0\n\n50\n\n100\n\n150\n\n200\n\ntrading days since 12/4/96\n\nlog return on day j\n\ntrading weeks since 12/4/96\n\nnow consider a 2 \u00d7 2 table formed using two independent binomial variables and having\nentries ri , si where ri + si = mi , ri /mi = pi , for i = 1, 2. show that if \u03c01 = \u03c02 and\nm1, m2 \u2192 \u221e, then\n\nx 2 = (n1 + n2)(r1s2 \u2212 r2s1)2/{n1n2(r1 + r2)(s1 + s2)}\n\nd\u2212\u2192 \u03c7 2\n\n1\n\n.\n\n9\n\n10\n\n.= log \u03b8 + n\n\ntwo batches of trees were planted in a park: 250 were obtained from nursery a and 250\nfrom nursery b. subsequently 41 and 64 trees from the two groups die. do trees from\nthe two nurseries have the same survival probabilities? are the assumptions you make\nreasonable?\nif y is the average of a random sample y1, . . . ,y n from density \u03b8\u22121 exp(\u2212y/\u03b8), y > 0,\n\u03b8 > 0, give the limiting distribution of z(\u03b8) = n1/2(y \u2212 \u03b8)/\u03b8 as n \u2192 \u221e. hence obtain\nan approximate two-sided 95% confidence interval for \u03b8.\n\u22121/2 z, find an approximate mean and variance\nshow that for large n, log(y )\nfor log y , and hence give another approximate two-sided 95% confidence interval for \u03b8.\nwhich interval would you prefer in practice?\nindependent pairs (x j , y j ), j = 1, . . . ,m arise in such a way that x j is normal with mean\n\u03bb j and y j is normal with mean \u03bb j + \u03c8, x j and y j are independent, and each has variance\n\u03c3 2. find the joint distribution of z1, . . . , zm, where z j = y j \u2212 x j , and hence show that\nthere is a (1 \u2212 2\u03b1) confidence interval for \u03c8 of form a \u00b1 m\n\u22121/2 bc, where a and b are\nrandom variables and c is a constant.\nobtain a 0.95 confidence interval for the mean difference \u03c8 given (x, y) pairs (27, 26),\n(34, 30), (31, 31), (30, 32), (29, 25), (38, 35), (39, 33), (42, 32). is it plausible that \u03c8 (cid:11)= 0?\n11 the upper left panel of figure 3.10 shows daily closing share prices x j for yahoo.com from\n12 april 1996 to 26 april 2000. we define the log daily returns y j = 100 log(x j /x j\u22121);\ny j is roughly the daily percentage change in price.\n(a) the lower left panel shows the y j . does their distribution seem to change with time?\n(b) the upper central panel shows a normal probability plot of the y j . dothey seem normal\nto you? if not, describe how they differ from normal variates.\n\n "}, {"Page_number": 105, "text": "remember: past\nperformance is no guide\nto the future!\n\n3.5 \u00b7 problems\n\n93\n\n(c) the lower central panel shows a plot of y j+1 against y j . are successive daily log\nreturns correlated? what would be the implication if they were?\n(d) the n = 1015 values of y j have average and variance y = 0.376 and s2 = 25.35. is\ne(y j ) > 0?\n(e) we can also define the log weekly returns, w j = y5( j\u22121)+1 + \u00b7\u00b7\u00b7 + y5 j , whose normal\nprobability plot is shown in the top right panel. are they normal? they have average and\nvariance 1.878 and 110.07. is their mean positive?\n(f) the data suggest the simple geometric brownian motion model that the stock value at\nthe end of week k is sk = s0 exp\n, where the z j are a standard normal\nrandom sample and s0 is the initial stock value. if i bought $100 worth of stock when it was\nlaunched and its value on 26 april 2000 was $4527, give its median predicted value and\na 95% prediction interval for its value 400 weeks after launch. do you find this credible?\nunder the normal model, how long must i wait before the probability is 0.5 that i am a\nmillionaire?\n\nk\u00b5 + \u03c3\n\nk\nj=1 z j\n\n(cid:1)\n\n#\n\n$\n\n12 (a) check the expressions for z j for adaptive rejection sampling.\n\n(b) show that g+(y) = (cid:11)\ng+(zi ) =\n\n(cid:1)\ny\u2212\u221e g+(x) dx satisfies\n(cid:1)\n\n1\n\ni\nj=0\nk\u22121\nj=0\n\n{exp h+(z j+1) \u2212 exp h+(z j )}\n{exp h+(z j+1) \u2212 exp h+(z j )} ;\n\nlet ck denote the denominator of this expression. show that a value x from g+ is generated\nby taking u \u223c u (0, 1), finding the largest zi such that g+(zi ) < u and setting\n\nx = zi +\n\n1\n\nh(cid:5)(yi+1)\n\nlog\n\n1 + h\n\n(cid:5)\n\n(yi+1)ck {u \u2212 g+(zi )}\n\nexp h+(zi )\n\n&\n\n.\n\n(c) let h\u2212(y) bedefined by taking the chords between the points ( y j , h(y j )), for j =\n1, . . . ,k , and let it be \u2212\u221e outside [y1, yk]. explain how to use h\u2212(y) tospeed up sampling\nfrom f when h is complicated, by performing a pretest based on exp{h\u2212(x) \u2212 h+(x)}.\n(gilks and wild, 1992; wild and gilks, 1993)\n\n1\n\nh(cid:5)(y j+1)\nh(cid:5)(z j+1)\n%\n\n "}, {"Page_number": 106, "text": "4\n\nlikelihood\n\n4.1 likelihood\n4.1.1 definition and examples\nsuppose we have observed the value y of a random variable y , whose probability\ndensity function is supposed known up to the value of a parameter \u03b8. wewrite f (y; \u03b8)\nto emphasize that the density is a function of both data and a parameter. in general\nboth y and \u03b8 will be vectors whose respective elements we denote by y j and \u03b8r .\nthe parameter takes values in a parameter space \u0001, and the data y take values in a\nsample space y. our goal is to make statements about the distribution of y , based on\nthe observed data y. the assumption that f is known apart from uncertainty about \u03b8\nreduces the problem to making statements about what range of values of \u03b8 within \u0001\nis plausible, given that y has been observed.\n\na fundamental tool is the likelihood for \u03b8 based on y, which is defined to be\n\nl(\u03b8) = f (y; \u03b8),\n\n\u03b8 \u2208 \u0001,\n\n(4.1)\n\nregarded as a function of \u03b8 for fixed y. our interest in this is motivated by the idea\nthat it will be relatively larger for values of \u03b8 near that which generated the data.\nwhen y is discrete we use f (y; \u03b8) = pr(y = y; \u03b8), while if y is continuous, we take\nf (y; \u03b8) to beits probability density function. owing to rounding, the recorded y is\nalways discrete in practice, and occasional minor difficulties can be avoided by taking\nthis into account, as we shall see in example 4.42. however in constructing (4.1) for\ncontinuous y we almost always use its density function. when y = (y1, . . . , yn) is a\ncollection of independent observations the likelihood is\n\nl(\u03b8) = f (y; \u03b8) = n(cid:1)\n\nj=1\n\nf (y j ; \u03b8).\n\n(4.2)\n\nexample 4.1 (poisson distribution) suppose that y consists of a single observation\nfrom the poisson density (2.6). here the data and the parameter are both scalars,\nand l(\u03b8) = \u03b8 ye\n\u2212\u03b8 /y!. the parameter space is {\u03b8 : \u03b8 > 0} and the sample space is\n\n94\n\n "}, {"Page_number": 107, "text": "4.1 \u00b7 likelihood\n\nfigure 4.1 likelihoods\nfor the spring failure data\nat stress 950 n/mm2. the\nupper left panel is the\nlikelihood for the\nexponential model, and\nbelow it is a perspective\nplot of the likelihood for\nthe weibull model. the\nupper right panel shows\ncontours of the log\nlikelihood for the weibull\nmodel; the exponential\nlikelihood is obtained by\nsetting \u03b1 = 1. that is,\nslicing l along the\nvertical dotted line. the\nlower right panel shows\nthe profile log likelihood\nfor \u03b1, which corresponds\nto the log likelihood\nvalues along the dashed\nline in the panel above,\nplotted against \u03b1.\n\n)\n7\n2\n-\n^\n0\n1\n\n \nx\n(\n \n\nd\no\no\nh\n\ni\nl\n\ne\nk\nl\n\ni\n\n3\n\n2\n\n1\n\n0\n\n)\n2\n2\n-\n^\n0\n1\n\n6\n\n \n\nx\n(\n \n\n4\n\n95\n\n-80\n\n-60\n\n-50\n\n-55\n\n-80\n\n-200\n\na\n\nt\n\ne\nh\n\nt\n\n0\n5\n2\n\n0\n0\n2\n\n0\n5\n1\n\n0\n0\n1\n\n0\n\n100 200 300 400 500 600\n\n0\n\n2\n\n4\n\n6\n\n8 10 12 14\n\ntheta\n\nalpha\n\n0\n5\n-\n\n5\n5\n-\n\n0\n6\n-\n\n5\n6\n-\n\nd\no\no\nh\n\ni\nl\n\ne\nk\n\ni\nl\n \n\ng\no\n\nl\n \n\ne\n\nl\ni\nf\n\no\nr\np\n\n4\n\n1\n\n2\n\n1\n\n0\n\n1\na\n\n6\n\n8\na l p\n\nh\n\n100\n\n4\n\n2\n\n2\n\nd\no\no\nh\n\n0\n\ni\nl\n\n \n\ne\nk\n\ni\n\nl\n\n250\n\n200\ntheta\n\n150\n\n0\n\n2\n\n4\n\n6\n\n8 10 12 14\n\nalpha\n\n{0, 1, 2, . . .}. if y = 0, l(\u03b8) is amonotonic decreasing function of \u03b8, whereas if y > 0\nit has a maximum at \u03b8 = y, and limit zero as \u03b8 approaches zero or infinity.\n(cid:1)\n\nexample 4.2 (exponential distribution)\nfrom the exponential density f (y; \u03b8) = \u03b8\u22121e\nis \u0001 = ir+ and the sample space the cartesian product irn+. here (4.2) gives\n\nlet y be a random sample y1, . . . , yn\n\u2212y/\u03b8 , y > 0, \u03b8 > 0. the parameter space\n(cid:2)\n\u2212 1\n\u03b8\n\n, \u03b8 > 0.\n\n(4.3)\n\n(cid:4)\n\ny j\n\nn(cid:3)\nj=1\n\nl(\u03b8) = n(cid:1)\n\nj=1\n\n\u03b8\u22121e\n\n\u2212y j /\u03b8 = \u03b8\u2212n exp\n\nthe spring failure times at stress 950 n/mm2 in example 1.2 are\n\n225, 171, 198, 189, 189, 135, 162, 135, 117, 162,\n\nand the top left panel of figure 4.1 shows the likelihood (4.3). the function is\n\u221227. at\u03b8 = 150, l(\u03b8)\nunimodal and is maximized at \u03b8\nequals 2.32 \u00d7 10\n\u221227, sothat 150 is 2.32/2.49 = 0.93 times less likely than \u03b8 = 168\nas an explanation for the data. if we were to declare that any value of \u03b8 for which\n\n.= 168; l(168)\n\n.= 2.49 \u00d7 10\n\n "}, {"Page_number": 108, "text": "96\n\n)\n9\n2\n-\n^\n0\n1\n\n \nx\n(\n \n\nd\no\no\nh\n\ni\nl\n\ne\nk\nl\n\ni\n\n4\n\n3\n\n2\n\n1\n\n0\n\nd\no\no\nh\n\ni\nl\n\ne\nk\n\ni\nl\n \n\ng\no\nl\n\n6\n6\n-\n\n8\n6\n-\n\n0\n7\n-\n\n2\n7\n-\n\n4\n7\n-\n\n6\n7\n-\n\n4 \u00b7 likelihood\n\nfigure 4.2 cauchy\nlikelihood and log\nlikelihood for the spring\nfailure data at stress\n950n/mm2.\n\n150 160 170 180 190 200\n\n150 160 170 180 190 200\n\ntheta\n\ntheta\n\nl(\u03b8) > cl(168) was \u201cplausible\u201d based on these data, values of \u03b8 in the range\n(120, 260) or so would be plausible when c = 1\n(cid:1)\n2 .\nthe cauchy density centered at \u03b8 is f (y; \u03b8) =\n\u22121, where y \u2208 ir and \u03b8 \u2208 ir. hence the likelihood for a random\nl(\u03b8) = n(cid:1)\n\nexample 4.3 (cauchy distribution)\n[\u03c0{1 + (y \u2212 \u03b8)2}]\nsample y1, . . . , yn is\n\n\u03c0{1 + (y j \u2212 \u03b8)2} , \u2212\u221e < \u03b8 <\u221e.\n\n1\n\nj=1\n\nthe sample space is irn and the parameter space is ir.\n\nthe left panel of figure 4.2 shows l(\u03b8) for the spring data in example 4.2. there\nseem to be three local maxima in the range for which l(\u03b8) isplotted, with a global\n.= 162. we can see more detail in the log likelihood log l(\u03b8) shown in\nmaximum at \u03b8\nthe right panel of the figure. there are at least four local maxima \u2014 apparently one at\neach observation, with a more prominent one when observations are duplicated. by\ncontrast with the previous example, for some values of c a \u201cplausible\u201d set for \u03b8 here\n(cid:1)\nconsists of disjoint intervals.\n\nexample 4.4 (weibull distribution) the weibull density is\n\nf (y; \u03b8, \u03b1) = \u03b1\n\n(cid:5)\n\n(cid:6)\u03b1\u22121\n\ny\n\u03b8\n\n(cid:6)\u03b1(cid:8)\n\n(cid:5)\n\n(cid:7)\n\n\u2212\n\ny\n\u03b8\n\n,\n\n\u03b8\n\nexp\n\ny > 0,\n\n(4.4)\nwhen \u03b1 = 1 this is the exponential density of example 4.2; the exponential model\nis nested within the weibull model, the parameter space for which is ir2+, and the\nsample space for which is irn+.\na random sample y = (y1, . . . , yn) from (4.4) has joint density\n(cid:5)\n(cid:7)\n\n\u03b8, \u03b1 > 0.\n\n(cid:9)\n\n(cid:5)\n\n(cid:6)\u03b1(cid:8)(cid:10)\n\n(cid:6)\u03b1\u22121\n\nf (y; \u03b8, \u03b1) = n(cid:1)\n\nf (y j ; \u03b8, \u03b1) = n(cid:1)\n\nexp\n\n\u2212\n\ny j\n\u03b8\n\n\u03b1\n\n\u03b8\n\ny j\n\u03b8\n\nj=1\n\nj=1\n\n "}, {"Page_number": 109, "text": "4.1 \u00b7 likelihood\n\nand hence the likelihood is\nl(\u03b8, \u03b1) = \u03b1n\n\n\u03b8 n\u03b1\n\n(cid:2)\n\n97\n\n(cid:4)\u03b1\u22121\n\ny j\n\nexp\n\n(cid:12)\n\n(cid:6)\u03b1\n\n(cid:11)\n\n(cid:5)\n\n\u2212 n(cid:3)\n\nj=1\n\ny j\n\u03b8\n\nn(cid:1)\nj=1\n\n,\n\n\u03b8, \u03b1 > 0.\n\n(4.5)\n\n.= 181 and \u03b1\n\n.= 6, and l(181, 6) equals 6.7 \u00d7 10\n\nthe lower left panel of figure 4.1 shows l(\u03b8, \u03b1) for the data of example 4.2. the\n\u221222.\nlikelihood is maximized at \u03b8\nthis is 2.7 \u00d7 105 times greater than the largest value for the exponential model. the top\nright panel shows contours of the log likelihood, log l(\u03b8, \u03b1). the dotted line indicates\nthe slice corresponding to the exponential density obtained when \u03b1 = 1. the factor\n2.5 \u00d7 105 gives a difference of log(2.7 \u00d7 105) = 12.5 between the maximum log\nlikelihoods. this big improvement suggests that the weibull model fits the data better.\nhowever, if we judge model fit by the maximum likelihood value, the weibull model is\nbound to fit at least as well as the exponential, because max\u03b8,\u03b1 l(\u03b8, \u03b1) \u2265 max\u03b8 l(\u03b8, 1),\nwith equality only if the maximum occurs on the line \u03b1 = 1.\n(cid:1)\n\nthe examples above involve random samples, but (4.1) and (4.2) apply also to\n\nmore complex situations.\n\nexample 4.5 (challenger data) consider the data in table 1.3 on o-ring thermal\ndistress. for now we ignore the effect of pressure, and treat the temperature x1 at\nlaunch as fixed and the number of o-rings with thermal distress as binomial variables\nwith denominator m and probability \u03c0, giving\n\npr(r = r) =\n\nm!\n\nr!(m \u2212 r)!\n\n\u03c0 r (1 \u2212 \u03c0)m\u2212r ,\n\nr = 0, 1, . . . ,m .\n\nif \u03c0 depends on temperature through the relation\n\n\u03c0 = exp(\u03b20 + \u03b21x1)\n1 + exp(\u03b20 + \u03b21x1)\n\n,\n\nthen the parameter \u03b20 determines the probability of thermal distress when x1 = 0\n\u25e6\nf,\nwhich is e\u03b20 /(1 + e\u03b20). the parameter \u03b21 determines how \u03c0 depends on temperature;\nwe expect that \u03b21 < 0, since \u03c0 decreases with increasing x1.\nif the data for the jth flight consist of r j o-rings with thermal distress at launch\ntemperature x1 j , j = 1, . . . ,n , and n = 23 and m = 6, we have\n(cid:13)\n\n(cid:13)\n\n(cid:14)m\u2212r j\n\n(cid:14)r j\n\npr(r j = r j ; \u03b20, \u03b21) =\n=\n\nm!\n\nr j !(m \u2212 r j )!\nr j !(m \u2212 r j )!\n\nm!\n\ne\u03b20+\u03b21x1 j\n1 + e\u03b20+\u03b21x1 j\nexp{r j (\u03b20 + \u03b21x1 j )}\n{1 + exp(\u03b20 + \u03b21x1 j )}m\n\n.\n\n1\n\n1 + e\u03b20+\u03b21x1 j\n\nif the r j are independent, the likelihood for the entire set of data is\n\nl(\u03b20, \u03b21) = n(cid:1)\n= n(cid:1)\n\nj=1\n\nj=1\n\npr(r j = r j ; \u03b20, \u03b21)\n(cid:15)\n(cid:18)\n\u00d7 exp\n\nr j !(m \u2212 r j )!\n\nm!\n\n(cid:16)\n\n(cid:16)\nj=1 r j + \u03b21\nn\nj=1 r j x1 j\n{1 + exp(\u03b20 + \u03b21x1 j )}m\n\nn\n\n\u03b20\nn\nj=1\n\n(cid:17)\n\n.\n\n(4.6)\n\n "}, {"Page_number": 110, "text": "98\n\n1\na\ne\nb\n\nt\n\n5\n0\n0\n\n.\n\n5\n0\n0\n-\n\n.\n\n5\n1\n\n.\n\n0\n-\n\n5\n2\n\n.\n\n0\n-\n\n-40\n\n-200\n\n-20\n\n-20\n\na\nd\nb\nm\na\n\nl\n\n-1000\n\n-200\n\n-100\n\n-40\n\n-40\n\n-20\n\n4 \u00b7 likelihood\n\n-200\n\n-16\n\n-20\n\n-40\n\n-40\n\n-100\n\n-40\n\n5\n0\n0\n\n.\n\n5\n0\n0\n-\n\n.\n\n5\n1\n\n.\n\n0\n-\n\n5\n2\n\n.\n\n0\n-\n\n-5\n\n0\n\n5\n\n10\n\n15\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\nbeta0\n\nphi\n\nfigure 4.3 log\nlikelihoods for a binomial\nmodel for the o-ring\nthermal distress data. the\nprobability of thermal\ndistress is taken to be \u03c8 =\nexp(\u03b20 + \u03b21x1)/{1 +\nexp(\u03b20 + \u03b21x1)}. the left\npanel gives the log\nlikelihood for parameters\n\u03b20 and \u03b21, and the right\npanel the log likelihood\nfor the probability of\n\u25e6\nthermal distress at 31\n\u03c8 =\nexp(\u03b20 + 31\u03b21)/{1 +\nexp(\u03b20 + 31\u03b21)} and\n\u03bb = \u03b21.\n\nf,\n\n.= 5 and \u03b21\n\nthe left panel of figure 4.3 shows contours of this function, which is largest at\n.= \u22120.1. however it is difficult to interpret because of the strong\n\u03b20\n.= 0\nnegative association between \u03b20 and \u03b21: the values of \u03b21 most plausible for \u03b20\n(cid:1)\nare different from those most plausible when \u03b20\n\n.= 10.\n\nthis is sometimes called\nthe prediction\ndecomposition.\n\nn(cid:1)\nj=2\n\nn(cid:1)\nj=2\n\ndependent data\nin the examples above the data are assumed independent, though not necessarily\nidentically distributed. in more complicated problems the dependence structure of the\ndata may be very complex, making it hard to write down f (y; \u03b8) explicitly. matters\nsimplify when the data are recorded in time order, so that y1 precedes y2 precedes\ny3, . . . . then it can help to write\n\nf (y; \u03b8) = f (y1, . . . , yn; \u03b8) = f (y1; \u03b8)\n\nf (y j | y1, . . . , y j\u22121; \u03b8).\n\n(4.7)\n\nfor example, if the data arise from a markov process, (4.7) becomes\n\nf (y; \u03b8) = f (y1; \u03b8)\n\nf (y j | y j\u22121; \u03b8),\n\n(4.8)\n\nwhere we have used the markov property, that given the \u201cpresent\u201d y j\u22121, the \u2018future\u2019,\ny j , y j+1, . . ., isindependent of the \u2018past\u2019,\n\n. . . , y j\u22123, y j\u22122.\n\nexample 4.6 (poisson birth process) suppose that y0, . . . ,y n are such that, given\nthat y j = y j , the conditional density of y j+1 is poisson with mean \u03b8 y j . that is,\n\nf (y j+1 | y j ; \u03b8) = (\u03b8 y j )y j+1\ny j+1!\n\nexp(\u2212\u03b8 y j ),\n\ny j+1 = 0, 1, . . . ,\n\n\u03b8 > 0.\n\nif y0 is poisson with mean \u03b8, the joint density of data y0, . . . , yn is\n\nf (y0; \u03b8)\n\nf (y j | y j\u22121; \u03b8) = \u03b8 y0\ny0!\n\nexp(\u2212\u03b8)\n\n(\u03b8 y j )y j+1\ny j+1!\n\nexp(\u2212\u03b8 y j ),\n\nn\u22121(cid:1)\nj=0\n\nn(cid:1)\nj=1\n\n "}, {"Page_number": 111, "text": "4.1 \u00b7 likelihood\n\nso the likelihood (4.8) equals\n\n(cid:2)\n\nn(cid:1)\nj=0\n\n(cid:4)\u22121\nl(\u03b8) =\nj=0 y j and s1 = 1 +(cid:16)\n\ny j !\n\nn\n\nexp (s0 log \u03b8 \u2212 s1\u03b8) ,\nn\u22121\nj=0 y j .\n\nwhere s0 = (cid:16)\n\n\u03b8 > 0,\n\n99\n\n(cid:1)\n\n4.1.2 basic properties\nit can be convenient to plot the likelihood on a logarithmic scale. this scale is also\nmathematically convenient, and we define the log likelihood to be\n\n(cid:8)(\u03b8) = log l(\u03b8).\n\nstatements about relative likelihoods become statements about differences between\nlog likelihoods. when y has independent components, y1, . . . , yn, wecan write\n\n(cid:8)(\u03b8) = n(cid:3)\n\nlog f (y j ; \u03b8) = n(cid:3)\n\n(4.9)\nwhere (cid:8) j (\u03b8) \u2261 (cid:8)(\u03b8; y j ) = log f (y j ; \u03b8) isthe contribution to the log likelihood from\nthe jth observation. the arguments of f and (cid:8) are reversed to stress that we are\nprimarily interested in f as a function of y, and in (cid:8) as a function of \u03b8.\n\n(cid:8) j (\u03b8),\n\nj=1\n\nj=1\n\nto combine the likelihoods for two independent sets of data y and z that both carry\ninformation about \u03b8, note that their joint probability density is just the product of their\nindividual densities, and therefore the likelihood based on y and z is the product of\nthe individual likelihoods:\n\nl(\u03b8; y, z) = f (y; \u03b8) f (z; \u03b8) = l(\u03b8; y)l(\u03b8; z),\n\nsay, where for clarity the data are an additional argument in the likelihoods.\n\nan important property of likelihood is its invariance to known transformations of\nthe data. suppose that there are two observers of the same experiment, and that one\nrecords the value y of a continuous random variable, y , while the other records the\nvalue z of z, where z is a known 1\u20131 transformation of y . then the probability\ndensity function of z is\n\nf z (z; \u03b8) = fy (y; \u03b8)\n\ndz\n\n(4.10)\nwhere y is regarded as a function of z, and|dy/dz| is the jacobian of the transformation\nfrom y to z. as(4.10) differs from (4.1) only by a constant that does not depend on\nthe parameter, the log likelihood based on z equals that based on y plus a constant:\nthe relative likelihoods of different values of \u03b8 are the same. this implies that within\na particular model f the absolute value of the likelihood is irrelevant to inference\nabout \u03b8. when the maximum value of the likelihood is finite we define the relative\nlikelihood of \u03b8 to be\n\n(cid:19)(cid:19)(cid:19)(cid:19) dy\n\n(cid:19)(cid:19)(cid:19)(cid:19),\n\nrl(\u03b8) =\n\nl(\u03b8)\n\nmax\u03b8(cid:6) l(\u03b8(cid:6))\n\n.\n\n "}, {"Page_number": 112, "text": "100\n\n4 \u00b7 likelihood\n\nthis takes values between one and zero, and its logarithm takes values between zero\nand minus infinity. as the absolute value of l(\u03b8), or equivalently (cid:8)(\u03b8), is irrelevant\nto inference about \u03b8, wecan neglect constants and use whatever version of l we\nwish. henceforth we use the notation \u2261 to indicate that constants have been ignored\nin defining a log likelihood. however we may not neglect constants if our goal is to\ncompare models from different families of distributions.\n\nexample 4.7 (spring failure data) we can compare the cauchy and weibull mod-\nels for the data in examples 4.2\u20134.4 in terms of the maximum likelihood value\nachieved. under this criterion, the weibull model, for which the largest log likelihood\nis about \u221248, is a much better model than is the cauchy, for which the maximum log\nlikelihood is about \u221266. evidently it makes no sense to add a constant to one of these\n(cid:1)\nand not to the other.\n\nsuppose that the distribution of y is determined by \u03c8, which is a 1\u20131 transformation\nof \u03b8, sothat \u03b8 = \u03b8(\u03c8). then the likelihood for \u03c8, l\n(\u03c8), and the likelihood for \u03b8,\n(\u03c8) = l{\u03b8(\u03c8)}. the value of l is not changed\nl(\u03b8), are related by the expression l\nby this transformation, so the likelihood is invariant to 1\u20131 reparametrization. we\ncan use a parametrization that has a direct interpretation in terms of our particular\nproblem.\n\n\u2217\n\n\u2217\n\nexample 4.8 (challenger data) we focus on the probability of thermal distress at\n\u25e6\n31\n\nf, expressed in terms of the original parameters as\n\n\u03c8 = exp(\u03b20 + 31\u03b21)\n1 + exp(\u03b20 + 31\u03b21)\n\n.\n\n\u2217\n\n\u2217\n\n(\u03c8, \u03bb) = l{\u03b20(\u03c8, \u03bb), \u03bb}. the plot of the log likelihood (cid:8)\u2217\n\nif we reparametrize l in terms of \u03c8 and \u03bb = \u03b21, we have\u03b2 0(\u03c8, \u03bb) = log{\u03c8/(1 \u2212\n\u03c8)} \u221231\u03bb, and l\n(\u03c8, \u03bb) =\nlog l\n(\u03c8, \u03bb) inthe right panel of figure 4.3 is easier to interpret than the plot of\n(cid:8)(\u03b20, \u03b21) inthe left panel, because the plausible range of values for \u03c8 changes more\nslowly with \u03bb. the contours in the left panel seem roughly elliptical, but those in the\nright are not. the most plausible range of values for \u03c8 is (0.7, 0.9), throughout which\nthe value of \u03bb is roughly \u22120.1.\n(cid:1)\n\ninterpretation\nwhen there is a particular parametric model for a set of data, likelihood provides\na natural basis for assessing the plausibility of different parameter values, but how\nshould it be interpreted? one viewpoint is that values of \u03b8 can be compared using a\nscale such as\n\n,\n\n3\n\n,\n\n1 \u2265 rl(\u03b8) > 1\n\u2265 rl(\u03b8) > 1\n\u2265 rl(\u03b8) > 1\n\u2265 rl(\u03b8) > 1\n\u2265 rl(\u03b8) > 0,\n\n10\n\n1\n3\n1\n10\n1\n100\n1\n\n100\n\n1000\n\n,\n\n1000\n\n\u03b8 strongly supported,\n\u03b8 supported,\n\u03b8 weakly supported,\n\u03b8 poorly supported,\n\u03b8 very poorly supported.\n\n,\n\n(4.11)\n\n "}, {"Page_number": 113, "text": "4.2 \u00b7 summaries\n\n101\n\nunder this pure likelihood approach, values of \u03b8 are compared solely in terms of\nrelative likelihoods. a scale such as (4.11) is simple and directly interpretable, but\nas it has the disadvantages that the numbers 1\n10 and so forth are arbitrary and take\nno account of the dimension of \u03b8, this interpretation is not the most common one in\npractice. we discuss repeated sampling calibration of likelihood values in section 4.5.\n\n3 , 1\n\nexercises 4.1\n1\n\nsketch the cauchy likelihood for the observations 1.1, 2.3, 1.5, 1.4.\nshow that the distribution function of the two-parameter cauchy density,\n\n2\n\n\u03c3\n\n+ \u03c0\u22121 tan\n\nf (u; \u03b8, \u03c3 ) =\n\n\u03c0{\u03c3 2 + (u \u2212 \u03b8)2} , \u2212\u221e < u < \u221e, \u03c3 > 0,\u2212\u221e < \u03b8 <\u221e,\n\u22121{(u \u2212 \u03b8)/\u03c3}. hence find pr(|y \u2212 \u03b8| < 20) when \u03c3 = 1, and with\n\nis f(u) = 1\nhindsight explain why the model in example 4.3 fits poorly.\nfind the likelihood for a random sample y1, . . . , yn\npr(y = y) = \u03c0(1 \u2212 \u03c0)y, y = 0, 1, . . ., where 0 < \u03c0 <1.\nverify that\nreparametrization \u03c8 = 1/\u03bb.\nshow that the log likelihood for two independent sets of data is the sum of their log\nlikelihoods.\nlet an \u2282 an\u22121 \u2282 \u00b7\u00b7\u00b7 \u2282 a1 be events on the same probability space. show that\npr(an) = pr(an | an\u22121)pr(an\u22121) = pr(an | an\u22121)\u00b7\u00b7\u00b7pr( a2 | a1)pr(a1)\n\nf (y; \u03bb) = \u03bb exp(\u2212\u03bby), y, \u03bb > 0,\n\nfrom the geometric density\n\nthe likelihood for\n\nis invariant\n\nto the\n\n2\n\n3\n\n4\n\n5\n\nand hence establish (4.7).\n\n4.2 summaries\n4.2.1 quadratic approximation\nin a problem with one or two parameters, the likelihood can be visualized. however\nmodels with a few dozen parameters are commonplace, and sometimes there are many\nmore, so we often need to summarize the likelihood.\n\na key idea is that in many cases the log likelihood is approximately quadratic as\na function of the parameter. to illustrate this, the left panel of figure 4.4 shows log\nlikelihoods for random samples of size n = 5, 10, 20, 40 and 80 from an exponential\ndensity, \u03b8\u22121 exp(\u2212u/\u03b8), \u03b8 > 0, u > 0. in each case the sample has average y = e\n\u22121.\nthe panel has two general features. first, the maximum of each log likelihood is at\n\u03b8 = e\n\n\u22121. tosee why, note that (4.3) implies that\n\n(cid:8)(\u03b8) = \u2212n log \u03b8 \u2212 \u03b8\u22121\n\ny j = \u2212n (log \u03b8 + y/\u03b8) ,\nwhich is maximized when d(cid:8)(\u03b8)/d\u03b8 = 0, that is, when \u03b8 = y. now\n\n(cid:21)\n\nd2(cid:8)(\u03b8)\nd\u03b8 2\n\n= \u2212n\n\n\u2212 1\n\u03b8 2\n\n+ 2y\n\u03b8 3\n\nn(cid:3)\nj=1\n(cid:20)\n\nfor example, an image of\n512 \u00d7 512 pixels may\nhave a parameter for each\npixel.\n\nas usual, y = n\n\n\u22121\n\n(cid:16)\n\ny j .\n\n "}, {"Page_number": 114, "text": "102\n\nd\no\no\nh\n\ni\nl\n\ne\nk\n\ni\nl\n \n\ng\no\nl\n\n0\n\n5\n-\n\n0\n1\n-\n\n4 \u00b7 likelihood\n\nfigure 4.4 log\nlikelihoods and relative\nlikelihoods for\nexponential samples with\nsample sizes n = 5, 10,\n20, 40, 80. the curvature\nof the functions increases\nwith n, sothe highest\ncurve in each panel is for\nn = 5, and the lowest is\nfor n = 80.\n\nd\no\no\nh\n\ni\nl\n\ne\nk\n\ni\nl\n \n\ne\nv\ni\nt\n\nl\n\na\ne\nr\n\n0\n\n.\n\n1\n\n5\n\n.\n\n0\n\n0\n\n.\n0\n\n0.0\n\n0.5\n\n1.0\n\n1.5\n\n0.0\n\n0.5\n\n1.0\n\n1.5\n\ntheta\n\ntheta\n\ntakes the value\u2212n/y2 at \u03b8 = y, so y gives the unique maximum of (cid:8). the value of \u03b8 for\nwhich l, orequivalently (cid:8), isgreatest is called the maximum likelihood estimate,(cid:22)\u03b8. in\nthis case(cid:22)\u03b8 = y. for future reference, note that the values of \u2212n\n\u22121d2(cid:8)(\u03b8)/d\u03b8 2 and its\n\u22121d3(cid:8)(\u03b8)/d\u03b8 3 are bounded in a neighbourhood n = {\u03b8 : |\u03b8 \u2212(cid:22)\u03b8| < \u03b4}\nderivative \u2212n\nof(cid:22)\u03b8, provided n excludes \u03b8 = 0.\n\nsecond, the curvature of the log likelihood at the maximum increases with n,\nbecause the second derivative of (cid:8), which measures its curvature as a function of \u03b8, is\na linear function of n. the function \u2212d2(cid:8)(\u03b8)/d\u03b8 2 is called the observed information.\nin this case its value at(cid:22)\u03b8 is n/y2 = n/(cid:22)\u03b8 2.\n\nthe right panel of figure 4.4 shows the relative likelihoods corresponding to the left\npanel. the effect of increasing n is that the likelihood becomes more concentrated\nabout the maximum, and so it becomes relatively less and less plausible that each\n\nvalue of \u03b8 a fixed distance from(cid:22)\u03b8 generated the data. to express this algebraically,\nwe write the log relative likelihood, log rl(\u03b8), as (cid:8)(\u03b8) \u2212 (cid:8)((cid:22)\u03b8) and expand (cid:8)(\u03b8) in a\ntaylor series about(cid:22)\u03b8 to obtain\nlog rl(\u03b8) = (cid:8)((cid:22)\u03b8) + (\u03b8 \u2212(cid:22)\u03b8)(cid:8)(cid:6)\n(\u03b8 \u2212(cid:22)\u03b8)2(cid:8)(cid:6)(cid:6)\n\u03b81 lies between \u03b8 and(cid:22)\u03b8. wedenote differentiation with respect to \u03b8 by a prime, thus\n((cid:22)\u03b8) = 0. each derivative of (cid:8) is a sum of\n(\u03b8) = d(cid:8)(\u03b8)/d\u03b8, and so forth; note that (cid:8)(cid:6)\n(cid:8)(cid:6)\nn terms. as n increases, we see that the bound on \u2212n\nwill become increasingly negative except at \u03b8 =(cid:22)\u03b8. hence rl(\u03b8) tends to zero unless\n\u03b8 =(cid:22)\u03b8, while rl((cid:22)\u03b8) = 1 for all n.\n\n(\u03b81) \u2212 (cid:8)((cid:22)\u03b8) = 1\n\n(\u03b8 \u2212(cid:22)\u03b8)2(cid:8)(cid:6)(cid:6)\n\n(\u03b81) implies that (4.12)\n\n((cid:22)\u03b8) + 1\n\n(\u03b81);\n(4.12)\n\n\u22121(cid:8)(cid:6)(cid:6)\n\n2\n\n2\n\nto examine the behaviour of the log likelihood more closely, we take another term\n\nin the taylor expansion leading to (4.12), to find that\n\nlog rl(\u03b8) = 1\n2\n\n(\u03b8 \u2212(cid:22)\u03b8)2(cid:8)(cid:6)(cid:6)\n\n((cid:22)\u03b8) + 1\n\n(\u03b8 \u2212(cid:22)\u03b8)3(cid:8)(cid:6)(cid:6)(cid:6)\n\n(\u03b82),\n\nwhere \u03b82 lies between \u03b8 and(cid:22)\u03b8. now consider what happens, not at a fixed distance\nfrom(cid:22)\u03b8, butat \u03b8 =(cid:22)\u03b8 + n\n\u22121/2\u03b4. asn increases this corresponds to \u201czooming in\u201d and\n\n6\n\n "}, {"Page_number": 115, "text": "103\n\n4.2 \u00b7 summaries\nexamining the region around(cid:22)\u03b8 ever more closely. now\n((cid:22)\u03b8) + 1\n\n(cid:17) = 1\n(\u03b8) are linear functions of n. the bound on\u2212n\n((cid:22)\u03b8)}, which in this case is\u2212 1\n\nlog rl\nand crucially, both (cid:8)(cid:6)(cid:6)\n(\u03b8)\nimplies that the last term on the right of (4.13) disappears as n \u2192 \u221e, but the quadratic\nterm becomes\u2212 1\n\u03b42/y2. thus in large samples\nin terms of the maximum likelihood estimate(cid:22)\u03b8 and the observed information \u2212(cid:8)(cid:6)(cid:6)\n((cid:22)\u03b8).\nthe likelihood close to the maximum is a quadratic function and can be summarized\n\n(cid:15)(cid:22)\u03b8 + n\n(\u03b8) and (cid:8)(cid:6)(cid:6)(cid:6)\n\n\u03b42{\u2212n\n\n\u22123/2(cid:8)(cid:6)(cid:6)(cid:6)\n\n\u22121/2\u03b4\n\n\u22121(cid:8)(cid:6)(cid:6)(cid:6)\n\n\u22121(cid:8)(cid:6)(cid:6)\n\n\u22121(cid:8)(cid:6)(cid:6)\n\n(4.13)\n\n(\u03b82),\n\n\u03b42n\n\n\u03b43n\n\n2\n\n6\n\n2\n\n2\n\none implication of this is that if we restrict ourselves to parameter values that are\nplausible relative to the maximum likelihood estimate, say those values of \u03b8 such that\nrl(\u03b8) > c, wefind log rl(\u03b8) > log c. comparison with (4.13) shows that our range\nof \u2018plausible\u2019 \u03b8 is decreasing with n and has length roughly proportional to n\n\n\u22121/2.\n\nthis discussion concerns a scalar parameter, but extends to higher dimensions,\n\nwhere d2(cid:8)/d\u03b8 2 is replaced by the matrix of second derivatives of (cid:8).\n\nwhether a quadratic approximation to (cid:8) is useful depends on the problem. to\nsummarize the log likelihood in figure 4.2 in such terms would be very misleading,\nunless a summary was required only very close to the maximum. if feasible, it is\nsensible to plot the likelihood.\n\nexample 4.9 (uniform distribution)\nsample y1, . . . , yn from the uniform density on (0, \u03b8):\n\nsuppose we are presented with a random\n\n(cid:13)\n\nf (u; \u03b8) =\n\n\u03b8\u22121,\n0,\n\n0 < u < \u03b8,\notherwise.\n\nthe likelihood is\n\nl(\u03b8) =\n\n(cid:13)\n\n(cid:1)\n\nf (y j ; \u03b8) =\n\n\u03b8\u2212n,\n0,\n\n0 < y1, . . . , yn < \u03b8,\notherwise.\n\nj\n\nit is maximized at(cid:22)\u03b8 = max(y j ), but d(cid:8)((cid:22)\u03b8)/d\u03b8 (cid:10)= 0 and \u2212d2(cid:8)((cid:22)\u03b8)/d\u03b8 2 = \u2212n/(cid:22)\u03b8 2 < 0,\nand (cid:8)(\u03b8) becomes increasingly spikey as n \u2192 \u221e and is not approximately quadratic\nnear(cid:22)\u03b8 for any n.\n(cid:1)\n\n4.2.2 sufficient statistics\nin well-behaved problems and with large samples the likelihood may be summarized\nin terms of the maximum likelihood estimate and observed information, though ex-\namples 4.3 and 4.9 show that this can fail. a better approach rests on the fact that\nthe likelihood often depends on the data only through some low-dimensional func-\ntion s(y) of the y j , and then a suitable summary can be given in terms of this. thus\ny j ) and\nin examples 4.2 and 4.9 the likelihoods depend on the data through (n,\n(n, max y j ) respectively. if we believe that our model is correct, we need only these\nfunctions to calculate the likelihoods for any value of \u03b8. these functions are examples\nof sufficient statistics.\n\n(cid:16)\n\n "}, {"Page_number": 116, "text": "104\n\n4 \u00b7 likelihood\n\nsuppose that we have observed data, y, generated by a distribution whose density\nis f (y; \u03b8), and that the statistic s(y) is afunction of y such that the conditional density\nof the corresponding random variable y , given that s = s(y ), is independent of \u03b8.\nthat is,\n\nfy|s(y | s; \u03b8)\n\n(4.14)\n\ndoes not depend on \u03b8. then s is said to be a sufficient statistic for \u03b8 based on y , or\njust a sufficient statistic for \u03b8. the idea is that any extra information in y but not in s\nis given by the conditional density (4.14), and if this conditional density is free of \u03b8,\ny contains no more information about \u03b8 than does s. weshall see later that s is not\nunique.\n\ndefinition (4.14) is hard to use, because we must guess that a given statistic s is\nsufficient before we can calculate the conditional density. an equivalent and more\nuseful definition is via the factorization criterion. this states that a necessary and\nsufficient condition for a statistic s to be a sufficient statistic for a parameter \u03b8 in\na family of probability density functions f (y; \u03b8) isthat the density of y can be\nexpressed as\n\nf (y; \u03b8) = g{s(y); \u03b8}h(y).\n\n(4.15)\n\nthus the density of y factorizes into a function g of s(y) and \u03b8, and a function of y,\nh, that does not depend on \u03b8.\n\nthe equivalence of these two definitions is almost self-evident. first note that if s\nis a sufficient statistic, the conditional distribution of y given s is independent of \u03b8,\nthat is,\n\nfy|s(y | s) = fy,s(y, s; \u03b8)\nfs(s; \u03b8)\n\n(4.16)\n\nis free of \u03b8. but as s is a function s(y ) of y , the joint density of s and y is zero except\nwhere s = s(y ), and so the numerator of the right-hand side of (4.16) is just fy (y; \u03b8).\nrearrangement of (4.16) implies that if s is sufficient, (4.15) holds with g(\u00b7) = fs(\u00b7)\nand h(\u00b7) = fy|s(\u00b7).\nconversely, if (4.15) holds, we find the density of s at s by summing or integrating\n(4.15) over the range of y for which s(y) = s. inthe discrete case\nh(y),\n\ng{s(y); \u03b8}h(y) = g{s; \u03b8}\n\nfs(s; \u03b8) =\n\n(cid:3)\n\n(cid:3)\n\nbecause the sum is over those y for which s(y) equals s. therefore the conditional\ndensity of y given s is\n\nfy (y; \u03b8)\nfs(s; \u03b8)\n\n= g{s(y); \u03b8}h(y)\ng{s; \u03b8}(cid:16)\n\nh(y)\n\n= h(y)(cid:16)\n\n,\n\nh(y)\n\nwhich shows that s is sufficient.\n\nexample 4.10 (bernoulli distribution) a bernoulli random variable y records the\n\u2018success\u2019 or \u2018failure\u2019 of a binary trial. thus\n\npr(y = 1) = 1 \u2212 pr(y = 0) = \u03c0,\n\n0 \u2264 \u03c0 \u2264 1,\n\nproof in the continuous\ncase would replace the\nsum here by an integral,\nbut adetailed proof is not\nsimple because all\nelements of the parametric\nmodel must be dominated\nby a single measure. see\nfor example theorem 2.21\nof schervish (1995).\n\n "}, {"Page_number": 117, "text": "4.2 \u00b7 summaries\n105\nwith y = 1 representing success and y = 0 failure. the likelihood contribution from\na single trial with outcome y = y may be written \u03c0 y(1 \u2212 \u03c0)1\u2212y, and hence the like-\nlihood for \u03c0 based on the outcomes of n independent trials is\n\nl(\u03c0) = n(cid:1)\n\n\u03c0 y j (1 \u2212 \u03c0)1\u2212y j = \u03c0 r (1 \u2212 \u03c0)n\u2212r ,\n\nsay, where r = (cid:16)\nthe corresponding random variable, r = (cid:16)\n\nj=1\n\ny j is the number of successes in the n trials. the distribution of\ny j , isbinomial with probability \u03c0 and\n\ndenominator n, that is,\n\npr(r = r) =\n\n(cid:20)\n\n(cid:21)\n\nn\nr\n\nhence the distribution of y1, . . . ,y n conditional on r is\ny j = r\n\ny1 = y1, . . . ,y n = yn |\n\npr\n\n(cid:5)\n\n\u03c0 r (1 \u2212 \u03c0)n\u2212r ,\n(cid:3)\n\nr = 0, . . . ,n .\n\n(cid:6)\n\n= 1(cid:15)\n\n(cid:17) ,\n\nn\nr\n\nwhich puts equal probability on each of the ( n\nthis conditional distribution does not depend on \u03c0, so r is sufficient for \u03c0, as is\nintuitively clear.\n\nr ) permutations of r 1\u2019s and n \u2212 r 0\u2019s.\n\nalthough there is no loss of information about \u03c0 when y1, . . . ,y n is reduced to\nr, the original data are more useful for some purposes. for example, if y1, . . . , yn\nconsisted of a sequence of zeros followed by a sequence of ones, we might want to\n(cid:16)\nrevise our belief that the trials were independent, but we could not know this if only\n(cid:1)\n\ny j had been reported.\n\nexample 4.11 (exponential distribution) suppose that y1 and y2 are indepen-\ndently exponentially distributed. then their joint density is\n\nf (y; \u03bb) = \u03bbe\n\n\u2212\u03bby1 \u00b7 \u03bbe\n\n\u2212\u03bby2 ,\n\n= \u03bb2 exp{\u2212\u03bb(y1 + y2)}\n= \u03bb2 exp(\u2212\u03bbs) \u00b7 1,\n\ny1, y2 > 0,\n\nwhich factorizes into a function of s = y1 + y2 and the constant 1. therefore s =\ny1 + y2 is sufficient, using the factorization criterion (4.15).\nto verify this using the original definition (4.14), note that s is a sum of two\nindependent exponential random variables, and so has the gamma distribution with\ndensity\n\nf (s; \u03bb) = \u03bb2s exp(\u2212\u03bbs),\n\ns > 0.\n\nthus the conditional density of y1 and y2 given that s = y1 + y2 = s is\n\nf (y1, y2; \u03bb)\n\nf (s; \u03bb)\n\n= \u03bb2 exp{\u2212\u03bb(y1 + y2)}\n\n\u03bb2s exp(\u2212\u03bbs)\n\n= 1\ns\n\n,\n\ny1 + y2 = s > 0.\n\nthis, the uniform density on (0, s), is free of \u03bb. thus given the particular value s for\nthe line y1 + y2 = s on which the point (y1, y2) lies, the position of (y1, y2) onthe\n(cid:1)\nline conveys no extra information about \u03bb.\n\n "}, {"Page_number": 118, "text": "106\n\n4 \u00b7 likelihood\n\nexample 4.12 (random sample) let y1, . . . ,y n be a random sample of scalar\nobservations from a density f (y; \u03b8). now as all the observations are on an equal\nfooting, their order is irrelevant. it follows that the order statistics y(1), . . . ,y (n) are\nsufficient for \u03b8. tosee this, note that we saw at (2.25) that the joint density of the\norder statistics is\n\nn! f (y(1); \u03b8) \u00d7 \u00b7\u00b7\u00b7 \u00d7 f (y(n); \u03b8),\n\ny(1) \u2264 \u00b7\u00b7\u00b7 \u2264 y(n).\n\nhence the conditional density of y1, . . . ,y n given y(1), . . . ,y (n) is 1/n!, provided\nthat y(1), . . . ,y (n) is a permutation of y1, . . . ,y n, and is zero otherwise. evidently\nthis conditional density is free of \u03b8, and hence the order statistics are a sufficient\nstatistic of dimension n for \u03b8.\n\nif we are willing to make more specific assumptions about\n\nf (y; \u03b8), we can re-\nduce the data further. for the exponential density, for example, the likelihood is\n\u03b8\u2212n exp(\u2212\u03b8\u22121\ny j ) isalso sufficient for \u03b8. thus there\n(cid:1)\ncan be different sufficient statistics for a single model.\n\ny j ), so it follows that (n ,\n\n(cid:16)\n\n(cid:16)\n\nexample 4.13 (capture-recapture model) capture-recapture models are widely\nused to estimate the sizes of animal populations and survival rates from one year to\nthe next. the idea is to capture animals on a number of separate occasions, to mark\nthem, and to return them to the wild after each occasion. the proportion of marked\nanimals seen on the second and subsequent occasions gives an idea of the quantities\nof interest. for example, if the population is large and only a small proportion of it is\nseen on the first occasion, then few of the animals captured next time will already be\nmarked.\n\nsuppose there are three capture occasions (years) labelled 0, 1, and 2, that the\nprobability of survival from one occasion to the next is \u03c8, and that, for an animal\nalive in year s, the probability of recapture is \u03bbs. then the possible capture histories\nand their probabilities are\n111 \u03c8 \u03bb1 \u00d7 \u03c8 \u03bb2,\n110 \u03c8 \u03bb1 \u00d7 {1 \u2212 \u03c8 + \u03c8(1 \u2212 \u03bb2)},\n101 \u03c8(1 \u2212 \u03bb1) \u00d7 \u03c8 \u03bb2,\n100 1 \u2212 \u03c8 + \u03c8(1 \u2212 \u03bb1){1 \u2212 \u03c8 + \u03c8(1 \u2212 \u03bb2)}\n\n011 \u03c8 \u03bb2,\n010 1 \u2212 \u03c8 + \u03c8(1 \u2212 \u03bb2),\n001 1,\n\nwhere, for example, 110 represents an animal seen in years 0 and 1, but not 2. the\nprobability of being alive and seen in year 1 is \u03c8 \u03bb1, and conditional on being alive\nin year 1, the animal may be dead in year 2, with probability 1 \u2212 \u03c8, oralive but not\nseen, with probability \u03c8(1 \u2212 \u03bb2). without further assumptions we can say nothing\nabout animals with history 000, which we never see.\n\nif animals are assumed independent, the likelihood is a product of such terms, and\nwe notice that, for example, there is a contribution \u03c8 \u03bb1 from animals with history\n111 or 110, a contribution \u03c8 \u03bb2 from animals with history 111 or 011, and so on. thus\nthe likelihood may be written as\n\n(\u03c8 \u03bb1)r01{\u03c8(1 \u2212 \u03bb1)\u03c8 \u03bb2}r02 {1 \u2212 \u03c8 \u03bb1 \u2212 \u03c8(1 \u2212 \u03bb1)\u03c8 \u03bb2}m0\u2212r01\u2212r02\n\u00d7 (\u03c8 \u03bb2)r11(1 \u2212 \u03c8 \u03bb2)m1\u2212r11 ,\n\n "}, {"Page_number": 119, "text": "4.2 \u00b7 summaries\n\n107\n\ntable 4.1 sufficient\nstatistics and probabilities\nfor capture-recapture\nmodel.\n\nnumber\ncaptured\n\nyear\n\n0\n1\n\nm0\nm1\n\nnumber first recaptured in year\n\n1\n\nr01\n\n2\n\nr02\nr11\n\nnumber never\n\nrecaptured\nm0 \u2212 r01 \u2212 r02\n\nm1 \u2212 r11\n\nyear\n\n0\n1\n\nprobability first recaptured in year\n\n1\n\n\u03c8 \u03bb1\n\n2\n\n\u03c8(1 \u2212 \u03bb1)\u03c8 \u03bb2\n\n\u03c8 \u03bb2\n\nprobability never\n\nrecaptured\n\n1 \u2212 \u03c8 \u03bb1 \u2212 \u03c8(1 \u2212 \u03bb1)\u03c8 \u03bb2\n\n1 \u2212 \u03c8 \u03bb2\n\nwhere ms is the number of animals seen in year s, ofwhom rst are first seen again\nin year t. evidently the quantities ms and rst are sufficient statistics. we lay out\nthese and the corresponding probabilities in table 4.1, which is a standard represen-\ntation for such data. with k occasions the number of individual histories is 2k \u2212 1\n2 (k + 2)(k \u2212 1) elements, so the reduction can be consid-\nbut the table contains just 1\nerable, but more importantly the data structure is clearer in terms of the sufficient\n(cid:1)\nstatistics.\n\nminimal sufficiency\neven for a single model, sufficient statistics are not unique. apart from the possi-\nbility that different functions s(y ) might satisfy the factorization criterion, the data\nthemselves form a sufficient statistic. moreover it is easy to see from (4.15) that any\nknown 1\u20131 function of a sufficient statistic is itself sufficient. what is unique to each\nsufficient statistic is the partition that it induces on the sample space.\nto see this, we say that two samples y1 and y2 with corresponding sufficient\nstatistics s1 = s(y1) and s2 = s(y2) are equivalent if s1 = s2. this evidently satisfies\nthe three properties of an equivalence relation:\n\nr reflexivity, y is equivalent to itself;\nr symmetry, y1 is equivalent to y2 if y2 is equivalent to y1; and\nr transitivity, y1 is equivalent to y3 whenever y1 is equivalent to y2 and y2 is\n\nequivalent to y3.\n\ntherefore the sample space is partitioned by the relation into equivalence classes,\ncorresponding to each of the distinct values that s can take. unlike the sufficient\nstatistic itself, this partitioning is invariant under 1\u20131 transformation of s. bythe\nfactorization criterion it has the property that the conditional density of the data y\ngiven that y falls into a particular equivalence class is independent of the parameter,\nand hence is called a sufficient partition. such a partition has the property that if we\nare told into which of its equivalence classes the data fall, we can reconstruct the log\nlikelihood up to additive constants. a mathematical discussion of sufficiency would\n\n "}, {"Page_number": 120, "text": "108\n\n4 \u00b7 likelihood\n\nbe in terms of sufficient partitions rather than sufficient statistics. however it is more\nnatural to think in terms of sufficient statistics, and we mostly do so.\n\nas sufficient statistics are not unique, we can choose which to use. the biggest\nreduction of the data is obtained by taking a sufficient statistic whose dimension is\nas small as possible, that is, a minimal sufficient statistic. asufficient statistic is said\nto be minimal if it is a function of any other sufficient statistic. this corresponds to\nthe coarsest sufficient partition of the sample space, while the data generate the finest\nsufficient partition. to find a minimal sufficient statistic, we return to the likelihood.\nsuppose that the likelihoods of two sets of data, y and z, are the same up to a constant.\nthen l(\u03b8; y)/l(\u03b8; z) does not depend on \u03b8, and the partition that this equivalence re-\nlation generates is minimally sufficient. thus a minimal sufficient statistic is obtained\nby examining the likelihood to see on what functions of the data it depends.\n\nexample 4.14 (exponential distribution)\nin example 4.11 the sample space into\nwhich (y1, y2) falls is ir2+, and this is partitioned by the lines y1 + y2 = s, s > 0,\neach of which corresponds to an equivalence class.\nin order to find a minimal sufficient statistic, note that the likelihood based on data\ny1, y2 is \u03bb2 exp{\u2212\u03bb(y1 + y2)}, whereas the likelihood based on x1, . . . , xm would\nbe \u03bbm exp{\u2212\u03bb(x1 + \u00b7\u00b7\u00b7 + xm)} the ratio of these would be independent of \u03bb only\nif m = 2 and x1 + x2 = y1 + y2. hence a minimal sufficient statistic is (n , s), the\nnumber of observations in the sample, and their sum. usually n is chosen without\n(cid:1)\nregard to \u03bb, and s alone is regarded as minimal sufficient.\n\nexample 4.15 (poisson birth process) we saw in example 4.6 that the likelihood\nbased on data y0, . . . , yn from such a process is\n\n(cid:2)\n\nn(cid:1)\nj=0\n\n(cid:4)\u22121\nl(\u03b8) =\nj=0 y j and s1 = 1 +(cid:16)\n\ny j !\n\nexp (s0 log \u03b8 \u2212 s1\u03b8) ,\nwhere s0 = (cid:16)\nn\u22121\nj=0 y j . the factorization criterion shows that\na sufficient statistic is (s0, s1) ,but equally so is (s0, yn), since s1 = s0 + 1 \u2212 yn.\n(cid:1)\nevidently either of these is also minimal sufficient.\n\n\u03b8 > 0,\n\nn\n\nexample 4.16 (logistic regression) suppose that independent binomial random\nvariables r j have denominators m j and probabilities \u03c0 j , where\nj = 1, . . . ,n ,\n\n\u03c0 j = exp(\u03b20 + \u03b21x1 j )\n1 + exp(\u03b20 + \u03b21x1 j )\n\n,\n\n(cid:16)\n\n(cid:16)\n\nand the x1 j are known constants. the likelihood is (4.6), and on applying the\nfactorization criterion we see that a minimal sufficient statistic for (\u03b20, \u03b21) is\ns = (\nr j x1 j ). although the m j , x1 j , and n are needed to calculate the like-\n(cid:1)\nlihood, they are non-random and not included in s.\n\nr j ,\n\nexercises 4.2\n1 find the maximum likelihood estimate and observed information in example 4.1. find\n\nalso the maximum likelihood estimate of pr(y = 0).\n\n "}, {"Page_number": 121, "text": "4.3 \u00b7 information\n\n109\n\n2 find maximum likelihood estimates for \u03b8 based on a random sample of size n from the\n\u2212\u03b8\u22122,\n\n\u2212\u03b8 y, y > 0, \u03b8 > 0; and (iii) (\u03b8 + 1)y\n\ndensities (i) \u03b8 y\u03b8\u22121, 0 < y < 1, \u03b8 > 0; (ii) \u03b8 2 ye\ny > 1, \u03b8 > 0;\n\n3 plot the likelihood for \u03b8 based on a random sample y1, . . . , yn from the density\n\n(cid:7)\n\nf (x; \u03b8) =\n\n1/(2c),\n0,\n\n\u03b8 \u2212 c < x < \u03b8 + c,\notherwise,\n\nwhere c is a known constant. find a maximum likelihood estimate, and show that it is not\nunique.\n\n4 in the discussion following (4.13), show that if the log likelihood was exactly quadratic\nand we agreed that values of \u03b8 such that rl(\u03b8) > c were \u2018plausible\u2019, the range of plausible\n\n\u03b8 would be(cid:22)\u03b8 \u00b1 {2 log c/(cid:8)(cid:6)(cid:6)\n\n((cid:22)\u03b8)}1/2.\n\n.=(cid:22)(cid:8) j \u2212 1\n\n2 j j ((cid:22)\u03b8 j )(\u03b8 \u2212(cid:22)\u03b8 j )2, where(cid:22)\u03b8 j is the maximum likelihood estimate and j j ((cid:22)\u03b8 j )\n\n5 data are available from n independent experiments concerning a scalar parameter \u03b8.\nthe log likelihood for the jth experiment may be summarized as a quadratic function,\n(cid:8) j (\u03b8)\nis the observed information. show that the overall log likelihood may be summarized as a\nquadratic function of \u03b8, and find the overall maximum likelihood estimate and observed\ninformation.\n\n6 in a first-order autoregressive process, y0, . . . ,y n, the conditional distribution of y j given\nthe previous observations, y1, . . . ,y j\u22121, isnormal with mean \u03b1y j\u22121 and variance one.\nthe initial observation y0 has the normal distribution with mean zero and variance one.\nj=1(y j \u2212 \u03b1y j\u22121)2, and hence find\nshow that the log likelihood is proportional to y2\n0\nthe maximum likelihood estimate of \u03b1 and the observed information.\n\n+(cid:16)\n\nn\n\n7 find a minimal sufficient statistic for \u03b8 based on a random sample y1, . . . ,y n from the\n\npoisson density (2.6).\n\n8 let y1, . . . ,y n be a random sample from the n (\u00b5, \u03c3 2) distribution.\n\ny j ,\n\ny 2\nj ) issufficient for (\u00b5, \u03c3 2). say,\n(a) use the factorization criterion to show that (\ngiving your reasons, which of the following are also sufficient: (i) (y , s2); (ii) (y 2, s);\n(iii) the order statistics y(1) < \u00b7\u00b7\u00b7 < y(n).\n(c) suppose that \u00b5 equals the known value \u00b50. show that s = (cid:16)\n(b) if \u03c3 2 = 1, show that the sample average is minimal sufficient for \u00b5.\n\n(y j \u2212 \u00b50)2 is a minimal\nsufficient statistic for \u03c3 2, and give its distribution. show that s is a function of the minimal\nsufficient statistic when both parameters are unknown.\n\n9 find the minimal sufficient statistic based on a random sample y1, . . . ,y n from the gamma\n\ndensity (2.7).\n\n10 use the factorization criterion to show that the maximum likelihood estimate and observed\ninformation based on f (y; \u03b8) are functions of data y only through a sufficient statistic\ns(y).\n\n11 verify that the relation \u2018y1 is equivalent to y2\u2019 if l(\u03b8; y1)/l(\u03b8; y2) isindependent of \u03b8 is\nan equivalence relation and that the corresponding partition is sufficient. deduce that the\nlikelihood itself is minimal sufficient.\n\n4.3 information\n4.3.1 expected and observed information\nin a model with log likelihood (cid:8)(\u03b8), the observed information is defined to be\n\nj (\u03b8) = \u2212 d2(cid:8)(\u03b8)\nd\u03b8 2\n\n.\n\n(cid:16)\n\n(cid:16)\n\n "}, {"Page_number": 122, "text": "110\n\n4 \u00b7 likelihood\n\nwhen (cid:8)(\u03b8) is asum of n components, so too is j (\u03b8), because (4.9) implies that\n\nj (\u03b8) = \u2212 d2(cid:8)(\u03b8)\nd\u03b8 2\n\n= \u2212 d2\nd\u03b8 2\n\n\u2212 d2 log f (y j ; \u03b8)\n\nd\u03b8 2\n\n.\n\n(4.17)\n\nn(cid:3)\nj=1\n\n(cid:8) j (\u03b8) = n(cid:3)\n\nj=1\n\nwe saw in section 4.2.1 that when the log likelihood is roughly quadratic, the\nrelative plausibility of parameter values near the maximum likelihood estimate is\ndetermined by the observed information. high information, or equivalently high\ncurvature, will pin down \u03b8 more tightly than if the observed information is low.\nthe amount of information is typically related to the size of the dataset, a fact\nuseful in planning experiments. before we conduct an experiment it is valuable\nto assess what information there will be in the data, to see if the proposed sam-\nple is large enough. otherwise we may need more data or a more informative ex-\nperiment. before the experiment is performed we have no data, so we cannot ob-\ntain the observed information. however we can calculate the expected or fisher\ninformation,\n\n(cid:13)\n\n(cid:14)\n\n,\n\ni (\u03b8) = e\n\n\u2212 d2(cid:8)(\u03b8)\nd\u03b8 2\n\nwhich is the mean information the data will contain when collected, if the model is\ncorrect and the true parameter value is \u03b8.\nif the data are a random sample, (4.17) implies that i (\u03b8) = ni(\u03b8), where i(\u03b8) isthe\n\ninformation from a single observation,\n\n(cid:13)\n\ni(\u03b8) = e\n\n\u2212 d2 log f (y j ; \u03b8)\n\nd\u03b8 2\n\nwhen \u03b8 is a p \u00d7 1 vector, the information matrices are\n\nj (\u03b8) = \u2212 \u2202 2(cid:8)(\u03b8)\n\n\u2202\u03b8 \u2202\u03b8 t\n\n,\n\ni (\u03b8) = \u2212e\n\n\u2202 2(cid:8)(\u03b8)\n\u2202\u03b8 \u2202\u03b8 t\n\n(cid:14)\n\n;\n\nthese are symmetric p \u00d7 p matrices whose (r, s) elements are respectively\n\n(cid:14)\n\n.\n\n(cid:13)\n\n(cid:14)\n\n.\n\n(cid:13)\n\n\u2212 \u2202 2(cid:8)(\u03b8)\n\u2202\u03b8r \u2202\u03b8s\n\n, e\n\n\u2212 \u2202 2(cid:8)(\u03b8)\n\u2202\u03b8r \u2202\u03b8s\n\nexample 4.17 (binomial distribution) the likelihood for a binomial variable r\nwith denominator m and probability of success 0 < \u03c0 <1 is l(\u03c0) = ( m\nr )\u03c0 r (1 \u2212\n\u03c0)m\u2212r , so(cid:8)(\u03c0 ) \u2261 r log \u03c0 + (m \u2212 r) log(1 \u2212 \u03c0) and\n\nj (\u03c0) = \u2212 d2(cid:8)(\u03c0)\nd\u03c0 2\n\n= r\n\u03c0 2\n\n+ m \u2212 r\n(1 \u2212 \u03c0)2\n\n,\n\ngiven an observed value r of r. before the experiment has been performed the value\nof r is unknown, and we replace it by the corresponding random variable r. inthis\n\nfor a p \u00d7 1 vector \u03b8 we\nuse \u2202(cid:8)/\u2202\u03b8 to denote the\np \u00d7 1 vector whose rth\nelement is \u2202(cid:8)/\u2202\u03b8r , and\n\u22022(cid:8)/\u2202\u03b8 \u2202\u03b8 t to denote the\np \u00d7 p matrix whose (r, s)\nelement is \u22022(cid:8)/\u2202\u03b8r \u2202\u03b8s .\n\n "}, {"Page_number": 123, "text": "4.3 \u00b7 information\n\n111\n\ncase j (\u03c0) too is random, and\n\ni (\u03c0) = e{j (\u03c0)}\n\n(cid:13)\n\n(cid:14)\n\n= e\n= m\u03c0\n\u03c0 2\n\nr\n\u03c0 2\n\n+ m \u2212 r\n(1 \u2212 \u03c0)2\n+ m(1 \u2212 \u03c0)\n=\n(1 \u2212 \u03c0)2\n\nm\n\n\u03c0(1 \u2212 \u03c0)\n\n,\n\nsince e(r) = m\u03c0. the expected information i (\u03c0) increases linearly with m and is\n(cid:1)\nsymmetric in \u03c0, for 0 < \u03c0 <1.\n\nexample 4.18 (normal distribution) the density function of a normal random\nvariable with mean \u00b5 and variance \u03c3 2 is (3.5), so the log likelihood for a random\nsample y1, . . . , yn is\n\n(cid:8)(\u00b5, \u03c3 ) \u2261 \u2212 n\n2\n(cid:3)\n\n(y j \u2212 \u00b5),\n\nits first derivatives are\n= \u03c3 \u22122\n\n\u2202(cid:8)\n\n\u2202\u00b5\n\nlog \u03c3 2 \u2212 1\n2\u03c3 2\n\nn(cid:3)\nj=1\n\n(y j \u2212 \u00b5)2.\n(cid:3)\n\n\u2202(cid:8)\n\n\u2202\u03c3 2\n\n= \u2212 n\n2\u03c3 2\n\n+ 1\n2\u03c3 4\n\n(y j \u2212 \u00b5)2,\n\nand the elements of the observed information matrix j (\u00b5, \u03c3 2) are given by\n(cid:3)\n\u2202 2(cid:8)\n\u2202\u00b52\n\n\u03c3 4 (y \u2212 \u00b5),\n\n= \u2212 n\n\u03c3 2\n\n+ 1\n\u03c3 6\n\n= \u2212 n\n\n\u2202\u00b5\u2202\u03c3 2\n\n\u2202 2(cid:8)\n\n\u2202 2(cid:8)\n\n,\n\n\u2202(\u03c3 2)2\n\n(y j \u2212 \u00b5)2.\n\non replacing y j with y j and taking expectations, we get\n\ni (\u00b5, \u03c3 2) =\n\nn/\u03c3 2\n\n0\n\nn/(2\u03c3 4)\n\n0\nbecause e(y j ) = \u00b5 and e{(y j \u2212 \u00b5)2} =\u03c3 2.\n\n(cid:20)\n\n,\n\n(4.18)\n\n(cid:1)\n\n= \u2212 n\n2\u03c3 4\n(cid:21)\n\n4.3.2 efficiency\nsuppose that we might adopt one of two sampling schemes, and we wish to see which\nis most efficient in the sense of needing least data to pin down the parameter to a\ngiven range. one way to do this is to compare the information in each likelihood. if \u03b8\nis scalar, the asymptotic efficiency of sampling scheme a relative to sampling scheme\nb is\n\nia(\u03b8)\nib(\u03b8)\n\n,\n\n(4.19)\n\nwhere ia(\u03b8) and ib(\u03b8) are the expected information quantities for schemes a and\nb. in simple random samples (4.19) equals naia(\u03b8)/{nbib(\u03b8)}, where na and nb\nobservations are used by the sampling schemes. the information from both schemes\nis equal if\n\nnb\nna\n\n= ia(\u03b8)\nib(\u03b8)\n\n(4.20)\n\n "}, {"Page_number": 124, "text": "112\n\n4 \u00b7 likelihood\n\nand we see that ia(\u03b8)/ib(\u03b8) can be interpreted as the number of observations an\nobserver using scheme b would need in order to get the information in a single\nobservation sampled under scheme a, when the parameter value is \u03b8. expression\n(4.19) is called the asymptotic efficiency because this use of the information rests on\nthe quadratic likelihoods usually entailed by large samples.\n\nexample 4.19 (poisson process) over short periods the times at which vehicles\npass an observer on a country road might be modelled as a poisson process of rate\n\u03bb vehicles/hour. observer a decides to estimate \u03bb by counting how many cars pass in\na period of t0 minutes. observer b, who is more diligent, records the times at which\nthey pass.\n\nthe total number of events, n , when a poisson process of rate \u03bb is observed for\na period of length t0 has the poisson distribution with mean \u03bbt0. hence a bases her\ninference on the likelihood\n\nl a(\u03bb) = (\u03bbt0)n\nn !\n\n\u2212\u03bbt0 ,\n\ne\n\n\u03bb > 0,\n\nfor which the observed and expected information quantities are\n\nja(\u03bb) = n /\u03bb2,\n\nia(\u03bb) = t0/\u03bb,\n\nsince e(n ) = \u03bbt0.\nthe times between events in a poisson process of rate \u03bb have independent expo-\n\u2212\u03bbu, u > 0. therefore if observer b records cars\nnential distributions with density \u03bbe\npassing at times 0 < t1 < \u00b7\u00b7\u00b7 < tn < t0, his likelihood is\n\n\u2212\u03bbt1 \u00d7 \u03bbe\n\n\u2212\u03bb(t2\u2212t1) \u00d7 \u00b7\u00b7\u00b7 \u00d7\u03bbe\n\n\u2212\u03bb(tn\u2212tn\u22121) \u00d7 e\n\n\u03bbe\n\n\u2212\u03bb(t0\u2212tn ),\n\nwhere the final term corresponds to observing no cars in the interval (tn , t0). thus b\nbases his inference on\n\nl b(\u03bb) = \u03bbn e\n\n\u2212\u03bbt0 ,\n\nfor which the observed and expected information quantities are the same as those for\na. thus the efficiency of a relative to b is ia(\u03bb)/ib(\u03bb) = 1: no information is lost\nby recording only the number of cars. this is because l a(\u03bb) \u221d lb(\u03bb); under either\nsampling scheme, the statistic n is sufficient for \u03bb.\n(cid:1)\n\ninference for poisson processes is discussed in section 6.5.1.\n\nexample 4.20 (censoring) a widget has lifetime t , but trials to estimate widget\nlifetimes finish after a known time c when the vice president for widget testing\nhas a tea break. the available data are the observed lifetime y = min(t , c), and\nd = i (t \u2264 c), where d indicates whether t has been observed. if t > c then t is\nsaid to be right-censored: weknow only that its value exceeds c.\n\nif t has density and distribution functions f (t; \u03b8) and f(t; \u03b8), the likelihood\n\ncontribution from (y, d) is\n\nf (y ; \u03b8)d {1 \u2212 f(c; \u03b8)}1\u2212d ,\n\ni (\u00b7) isthe indicator\nfunction of the event \u2018\u00b7\u2019.\n\n "}, {"Page_number": 125, "text": "4.3 \u00b7 information\n\n113\n\nso the likelihood for a random sample of data (y1, d1), . . . ,( yn, dn) is\n\n[ f (y j ; \u03b8)d j{1 \u2212 f(y j ; \u03b8)}1\u2212d j ] =\n\nf (y j ; \u03b8) \u00d7\n\n{1 \u2212 f(c; \u03b8)},\n\n(cid:1)\n\n(cid:1)\n\nuncens\n\ncens\n\nn(cid:1)\nj=1\n\ne\n\ncens\n\n\u03bbe\n\nuncens\n\n(cid:4)\n\n(cid:1)\n\n(cid:1)\n\n\u2212\u03bby j \u00d7\n\nn(cid:3)\nj=1\n\nthe likelihood for a random sample with exponential density f (u; \u03bb) = \u03bbe\n\nwhere the first product is over uncensored data, and the second is over censored data.\n\u2212\u03bbu,\nu > 0, \u03bb >0, and distribution f(u; \u03bb) = 1 \u2212 e\n\u2212\u03bbu, u > 0, is\n(cid:2)\nn(cid:3)\nd j log \u03bb \u2212 \u03bb\nj=1\n\n\u2212\u03bbc = exp\nthe observed information is j (\u03bb) = (cid:16)\n\nd j decreases:\nif n is known, there is information only in observations that were seen to fail. to find\nthe expected information ic(\u03bb) when there is censoring at c, note that\n\n(cid:2)\nn(cid:3)\ne\nj=1\nso that ic(\u03bb) = n(1 \u2212 e\n\u2212\u03bbc)/\u03bb2. byletting c \u2192 \u221e we can obtain the expected infor-\nmation when there is no censoring, i\u221e(\u03bb) = n/\u03bb2. therefore the relative efficiency\nwhen there is censoring at c is\n\n= npr(y \u2264 c) = n(1 \u2212 e\n\nd j /\u03bb2, which decreases as\n\n\u2212\u03bbc),\n\n(cid:16)\n\n(cid:4)\n\nd j\n\ny j\n\n.\n\nic(\u03bb)\ni\u221e(\u03bb)\n\n= n(1 \u2212 e\n\n\u2212\u03bbc)/\u03bb2\n\nn/\u03bb2\n\n= 1 \u2212 e\n\n\u2212\u03bbc.\n\nthis equals the proportion of uncensored data, which is unsurprising, as we saw above\nthat censored observations do not contribute to j (\u03bb). as one would anticipate, the\nloss of information becomes more severe as c decreases.\n\n(cid:1)\n\n|c| is the determinant of\nthe p \u00d7 p matrix c.\n\ninference for censored data is discussed in sections 5.4 and 10.8.\nwhen \u03b8 is a p \u00d7 1 vector, we replace (4.19) by the ratio\n\n(cid:13)|ia(\u03b8)|\n|ib(\u03b8)|\n\n(cid:14)1/ p\n\n,\n\na (\u03b8), where i rr\n\nwhich preserves the interpretation of efficiency given at (4.20) in terms of numbers of\nobservations. this is an overall measure of the efficiency of the schemes, but often in\npractice one may want to compare the efficiency of estimation for a single component\nof \u03b8, say \u03b8r . for reasons to be given in section 4.4.2, the appropriate measure is then\ni rr\nb (\u03b8)/i rr\nexample 4.21 (rounding) what information is lost when the sample 2.71828,\n3.14159, . . . is rounded to 2.7, 3.1, . . .? let y denote a real-valued continuous random\nvariable with distribution function f(y; \u03b8). in recording the data, y is rounded to x,\n2 )\u03b4 \u2264 y < (k + 1\nthe nearest multiple of \u03b4. thus x = k\u03b4 if (k \u2212 1\n2 )\u03b4, an event with\n(cid:14)\n(cid:13)(cid:20)\nprobability\n\na (\u03b8) isthe ( r, r)th element of the inverse matrix ia(\u03b8)\n\n(cid:13)(cid:20)\n\n\u22121.\n\n(cid:21)\n\n(cid:21)\n\n(cid:14)\n\n\u03c0k(\u03b8) = f\n\nk + 1\n2\n\n\u2212 f\n\n\u03b4; \u03b8\n\nk \u2212 1\n2\n\n\u03b4; \u03b8\n\n.\n\n "}, {"Page_number": 126, "text": "114\n\n4 \u00b7 likelihood\n\n\u03b4/\u03c3\n\n0.001\n\n0.01\n\n0.1\n\n0.2\n\n0.5\n\n1\n\n1.5\n\n2\n\n3\n\noverall efficiency\nefficiency for \u00b5\nefficiency for \u03c3 2\n\n100\n100\n100\n\n100\n100\n100\n\n99.9\n99.9\n99.8\n\n99.5\n99.7\n99.3\n\n97.0\n98.0\n96.0\n\n88.9\n92.3\n85.5\n\n77.9\n84.2\n72.0\n\n64.0\n75.5\n54.2\n\n37.5\n54.2\n25.9\n\ntable 4.2 efficiency (%)\nof likelihood inference\nwhen n (0, \u03c3 2) data are\nrounded to the nearest \u03b4.\n\n(cid:18)\n\n\u03c0k(\u03b8)i (x=k\u03b4), so\n\nk\n\nthe density of a single rounded observation may be written\n\nthe log likelihood for \u03b8 based on x is\n\n(cid:8)(\u03b8) =\n\ni (x = k\u03b4) log \u03c0k(\u03b8).\n\n\u221e(cid:3)\nk=\u2212\u221e\n(cid:13)\n\non differentiation we find that\n\n\u221e(cid:3)\nk=\u2212\u221e\n\n=\n\n\u2202 2(cid:8)(\u03b8)\n(cid:16)\n\u2202\u03b8r \u2202\u03b8s\n\ni (x = k\u03b4)\n\n(cid:21)(cid:20)\n\n(cid:20)\n\n1\n\u03c0k\n\n\u2202\u03c0k\n\u2202\u03b8r\n\n1\n\u03c0k\n\n\u2202\u03c0k\n\u2202\u03b8s\n\n(cid:21)(cid:14)\n\n,\n\n\u2212\n\n\u2202 2\u03c0k\n\u2202\u03b8r \u2202\u03b8s\n\n1\n\u03c0k\n\n\u03c0k(\u03b8) = 1 for all \u03b8 and e{i (x = k\u03b4)} =\u03c0 k(\u03b8), the (r, s) element of the\n\nand as\nexpected information matrix for a random sample x1, . . . , xn is\n\nk\n\n\u221e(cid:3)\nk=\u2212\u221e\n\nn\n\n1\n\n\u2202\u03c0k(\u03b8)\n\n\u2202\u03c0k(\u03b8)\n\n\u03c0k(\u03b8)\n\n\u2202\u03b8r\n\n\u2202\u03b8s\n\n.\n\n(4.21)\n\nfor concreteness, suppose that y is normally distributed with mean \u00b5 and variance\n\u03c3 2, inwhich case \u03c0k(\u00b5, \u03c3 2) = \u0001(zk+1) \u2212 \u0001(zk) and\n\n\u2202\u03c0k\n\u2202\u00b5\n\n= \u2212 1\n\u03c3\n\n{\u03c6(zk+1) \u2212 \u03c6(zk)} ,\n\n\u2202\u03c0k\n\u2202\u03c3 2\n\n= \u2212 1\n2\u03c3 2\n\n{zk+1\u03c6(zk+1) \u2212 zk \u03c6(zk)},\n\n(4.22)\n\nwhere zk = \u03c3 \u22121{(k \u2212 1\nmay be written as\n\n2 )\u03b4 \u2212 \u00b5}. with \u00b5 = 0 itturns out that the expected information\n(cid:20)\n\n(cid:21)\n\n\u03c3 \u22122 i\u00b5\u00b5(\u03b4/\u03c3 )\n\nn\n\n0\n\n(4\u03c3 4)\n\n0\n\n\u22121 i\u03c3 \u03c3 (\u03b4/\u03c3 )\n\n,\n\nwhere the elements are given by substituting (4.22) into (4.21). on comparing\nthis with (4.18), we see that the overall efficiency for the two parameters is\n{i\u00b5\u00b5(\u03b4/\u03c3 )i\u03c3 \u03c3 (\u03b4/\u03c3 )/2}1/2, while the efficiencies for \u00b5 and \u03c3 2 separately are i\u00b5\u00b5(\u03b4/\u03c3 )\nand 1\n2 i\u03c3 \u03c3 (\u03b4/\u03c3 ). table 4.2 shows that these are remarkably high even with quite heavy\nrounding. when \u03b4 = \u03c3 = 1, rounding y to x gives a discrete distribution with almost\nall its probability on the seven values \u22123,\u22122, . . . ,3, but a sample x1, . . . , x100 of\nsuch values gives almost the same efficiency as 89 of the corresponding ys: the overall\nloss of efficiency is only 11%. if the data are rounded to the equivalent of one decimal\nplace, \u03b4 = 0.1\u03c3 , there is effectively no information lost. with \u03b4 = 1.5\u03c3 or more the\nloss is more dramatic, particularly for estimation of \u03c3 , and with \u03b4 = 3\u03c3 the data are\nalmost binary.\n\nalthough suggestive, these results should be regarded with caution for two reasons.\nfirst, they apply to large samples, and the efficiency loss might be different in small\n\n "}, {"Page_number": 127, "text": "4.4 \u00b7 maximum likelihood estimator\n\n115\n\nsamples. second, they rest on the assumption that the multinomial likelihood based on\nthe x j is used, but in practice the rounded data would usually be treated as continuous\nand inference based on the (incorrect) log likelihood\nj log f (x j ; \u03b8). practical 4.1\n(cid:1)\nconsiders the effect of this.\n\n(cid:16)\n\nexercises 4.3\n1\n\n(cid:3)\n\n(cid:3)\n\n(a) show that the log likelihood for a random sample from density (2.7) is\ny j \u2212 n log \u0001(\u03ba),\n(cid:6)\n\n(cid:8)(\u03bb, \u03ba) = n\u03ba log \u03bb + (\u03ba \u2212 1)\ndeduce that the observed information is\n\nlog y j \u2212 \u03bb\n\n(cid:5)\n\nj (\u03bb, \u03ba) = n\n\n\u22121/\u03bb\n\n\u03ba/\u03bb2\n\n\u22121/\u03bb d 2 log \u0001(\u03ba)/d\u03ba 2\n\n,\n\na sketch may help.\n\n2\n3\n\n4\n\n5\n\n6\n\n(cid:16)\n\nand find the expected information i (\u03bb, \u03ba).\n(b) suppose that we write \u03bb = \u03ba/\u00b5, where \u00b5 is the distribution mean. find the log\ni (\u00b5, \u03ba) =\nlikelihood in terms of \u00b5 and \u03ba, and show that j (\u00b5, \u03ba) israndom and\nndiag{2\u03ba/\u00b52, d 2 log \u0001(\u03ba)/d\u03ba 2 \u2212 1/\u03ba}.\ncheck the details of example 4.19.\ny1, . . . ,y n are independent normal random variables with unit variances and means\ne(y j ) = \u03b2x j , where the x j are known quantities in (0, 1] and \u03b2 is an unknown parameter.\nshow that (cid:8)(\u03b2) \u2261 \u2212 1\nsuppose that n = 10 and that an experiment to estimate \u03b2 is to be designed by choosing\nthe x j appropriately. show that i (\u03b2) ismaximized when all the x j equal 1. is this design\nsensible if there is any possibility that e(y j ) = \u03b1 + \u03b2x j , with \u03b1 unknown?\nuse (4.21) and (4.22) to give expressions for the quantities i\u00b5\u00b5(\u03b4/\u03c3 ) and i\u03c3 \u03c3 (\u03b4/\u03c3 ) in\nexample 4.21. show that i\u00b5\u03c3 (\u03b4/\u03c3 ) = 0 when \u00b5 = 0.\nfind the expected information for \u03b8 based on a random sample y1, . . . ,y n from the\ngeometric density\n\n(y j \u2212 x j \u03b2)2 and find the expected information i (\u03b2) for \u03b2.\n\n2\n\nf (y; \u03b8) = \u03b8(1 \u2212 \u03b8)y\u22121,\n\ny = 1, 2, 3, . . . , 0 < \u03b8 <1.\n\na statistician has a choice between observing random samples from the bernoulli or\ngeometric densities with the same \u03b8. which will give the more precise inference on \u03b8?\nsuppose a random sample y1, . . . ,y n from the exponential density is rounded down to\nthe nearest \u03b4, giving \u03b4 z j , where z j = (cid:13)y j /\u03b4(cid:14). show that the likelihood contribution\nfrom a rounded observation can be written (1 \u2212 e\n\u2212z \u03bb\u03b4, and deduce that the expected\ninformation for \u03bb based on the entire sample is n\u03b42 exp(\u2212\u03bb\u03b4){1 \u2212 exp(\u2212\u03bb\u03b4)}\u22122. show\nthat this has limit n/\u03bb2 as \u03b4 \u2192 0, and that if \u03bb = 1, the loss of information when data are\nrounded down to the nearest integer rather than recorded exactly, is less than 10%. find\nthe loss of information when \u03b4 = 0.1, and comment briefly.\n\n\u2212\u03bb\u03b4)e\n\n4.4 maximum likelihood estimator\n4.4.1 computation\nthe maximum likelihood estimate of \u03b8,(cid:22)\u03b8, is a value of \u03b8 that maximizes the likelihood,\nor equivalently the log likelihood. suppose \u03c8 = \u03c8(\u03b8) is a1\u20131 function of \u03b8. then in\nterms of \u03c8 the likelihood is\n\n\u2217\n\n(\u03c8) = l\n\n\u2217{\u03c8(\u03b8)} = l(\u03b8),\n\nl\n\n "}, {"Page_number": 128, "text": "\u2202(cid:8)((cid:22)\u03b8)\n\n= 0.\n\n\u2217\n\n116\n\nso the largest values of l\n\n4 \u00b7 likelihood\n\u03c8 is (cid:22)\u03c8 = \u03c8((cid:22)\u03b8). this simplifies calculation of maximum likelihood estimates, as we\noften, though not invariably,(cid:22)\u03b8 satisfies the likelihood equation\n\ncan compute them in the most convenient parametrization, and then transform them\nto the scale of interest.\n\nand l coincide, and the maximum likelihood estimate of\n\n(4.23)\nif \u03b8 is a p \u00d7 1 vector, (4.23) is a p \u00d7 1 system of equations that must be solved\nsimultaneously for the components of(cid:22)\u03b8. wecheck that (cid:22)\u03b8 gives a local maximum by\nverifying that \u2212d2(cid:8)((cid:22)\u03b8)/d\u03b8 2 > 0, or in the vector case that the observed information\nmatrix j (\u03b8) = \u2212d2(cid:8)(\u03b8)/d\u03b8d\u03b8 t is positive definite at(cid:22)\u03b8. ifthere are several solutions to\n\n\u2202\u03b8\n\n(4.23), in principle we find them all, check which are maxima, and then evaluate (cid:8)(\u03b8)\nat each local maximum, thereby obtaining the global maximum. if there are numerous\nlocal maxima, as in figure 4.2, doubt is cast on the usefulness of summarizing (cid:8)(\u03b8)\n\nin terms of(cid:22)\u03b8 and j ((cid:22)\u03b8), but many log likelihoods can be shown to be strictly concave.\n\nthen a local maximum is also the global maximum, so there is a unique maximum;\nmoreover if there is a solution to (4.23), it is unique and gives the maximum.\n\n\u2202\u00b5\n\n\u2202\u03c3 2\n\n2\u03c3 4\n\n=\n\n=\n\n(cid:20)\n\n(cid:21)\n\n(cid:21)\n\n(cid:20)\n\n(cid:21)\n\n\u2202(cid:8)(\u00b5,\u03c3 2)\n\n(y j \u2212 \u00b5)2\n\n(cid:16)\n(y j \u2212 \u00b5)\n\nexample 4.22 (normal distribution) the likelihood equation for a random\nsample y1, . . . , yn from the normal distribution with mean \u00b5 and variance \u03c3 2 is\n\n(example 4.18)(cid:20) \u2202(cid:8)(\u00b5,\u03c3 2)\n(cid:16)\n\u03c3 \u22122\n\u2212 n\n2\u03c3 2 + 1\nthe first of these has the sole solution (cid:22)\u00b5 = y for all values of \u03c3 2, and (cid:8)((cid:22)\u00b5, \u03c3 2) is\n(cid:16)\nunimodal with maximum at(cid:22)\u03c3 2 = n\n(y j \u2212 y)2. atthe point ((cid:22)\u00b5,(cid:22)\u03c3 2), the observed\n\u22121\ninformation matrix j (\u00b5, \u03c3 2) isdiagonal with elements diag{n /(cid:22)\u03c3 2, n/(2(cid:22)\u03c3 4)}, and so is\n(cid:16)\n(y j \u2212 y)2 are the sole solutions to the likelihood\n\u22121\nif we wish to estimate the mean of exp(y ), which is \u03c8 = exp(\u00b5 + \u03c3 2/2), then\nrather than reparametrize in terms of \u03c8 and \u00b5, say, and maximizing directly, we use\n\u03c8 is (cid:22)\u03c8 = exp((cid:22)\u00b5 +(cid:22)\u03c3 2/2).\nthe earlier results on transformations to see that the maximum likelihood estimate of\n(cid:1)\n\npositive definite. hence y and n\nequation, and therefore are the maximum likelihood estimates.\n\n0\n0\n\n.\n\nto obtain\n\nin most realistic cases (4.23) must be solved iteratively, and often variants of the\n, we expand (4.23)\n\nnewton\u2013raphson algorithm can be used. given a starting-value \u03b8\u2020\nby taylor series about \u03b8\u2020\n\n0 = \u2202(cid:8)((cid:22)\u03b8)\non rearranging (4.24) we obtain(cid:22)\u03b8\n\u22121u (\u03b8\u2020\n(4.25)\n)\nwhere u (\u03b8) = \u2202(cid:8)(\u03b8)/\u2202\u03b8 is called the score statistic or score vector, and j (\u03b8) isthe\nobserved information (4.17). in the vector case(cid:22)\u03b8, \u03b8\u2020\n) are p \u00d7 1 vectors and\n\n.= \u03b8\u2020 + j (\u03b8\u2020\n\n+ \u2202 2(cid:8)(\u03b8\u2020\n\n((cid:22)\u03b8 \u2212 \u03b8\u2020\n\n.= \u2202(cid:8)(\u03b8\u2020\n\nand u (\u03b8\u2020\n\n)\n\u2202\u03b8 \u2202\u03b8 t\n\n(4.24)\n\n\u2202\u03b8\n\n\u2202\u03b8\n\n),\n\n).\n\n)\n\n "}, {"Page_number": 129, "text": "4.4 \u00b7 maximum likelihood estimator\n117\n) is a p \u00d7 p matrix. the log likelihood is usually maximized in a few iterations\nj (\u03b8\u2020\nof (4.25), using(cid:22)\u03b8 from one iteration as \u03b8\u2020\nfor the next. in doubtful cases it is wise to\ntry several initial values of \u03b8\u2020\nthe iteration (4.25) gives(cid:22)\u03b8 in one step if (cid:8)(\u03b8) isactually quadratic, so convergence\n\n.\n\nis accelerated by choosing a parametrization in which (cid:8)(\u03b8) is asclose to quadratic\nas possible. often it helps to transform components of \u03b8 to take values in the real\nline, for example removing the restrictions \u03bb >0 and 0 < \u03c0 <1 bymaximizing in\n\nterms of log \u03bb and log{\u03c0/(1 \u2212 \u03c0)}. this also avoids steps that take (cid:22)\u03b8 outside the\nparameter space. another simple trick is to use a variable step-length in (4.25). we\nreplace j (\u03b8\u2020\n), choose c to maximize (cid:8) along this line, then\nrecalculate u and j , and try again. many standard models are readily fitted with a\nfew lines of code in statistical packages, but fitting more adventurous models may\ninvolve writing special programs.\n\n) byc j (\u03b8\u2020\n\n\u22121u (\u03b8\u2020\n)\n\n\u22121u (\u03b8\u2020\n\n)\n\nexample 4.23 (weibull distribution) the log likelihood for a random sample from\nthe weibull density (4.4) is\n\n(cid:8)(\u03b8, \u03b1) = n log \u03b1 \u2212 n log \u03b8 + (\u03b1 \u2212 1)\n\nlog\n\nn(cid:3)\nj=1\n\n(cid:5)\n\n(cid:6)\n\n(cid:5)\n\n\u2212 n(cid:3)\n\nj=1\n\ny j\n\u03b8\n\ny j\n\u03b8\n\n(cid:6)\u03b1\n\n,\n\nthe score function is\n\n(cid:20)\n\nu (\u03b8, \u03b1) =\n\n\u2202(cid:8)/\u2202\u03b8\n\u2202(cid:8)/\u2202\u03b1\n\n(cid:21)\n\n(cid:20)\n\n=\n\n(cid:16)\nlog(y j /\u03b8) \u2212(cid:16)\n\u2212n\u03b1/\u03b8 + \u03b1\u03b8\u22121\n(y j /\u03b8)\u03b1 log(y j /\u03b8)\n\n(y j /\u03b8)\u03b1\n\nn/\u03b1 +(cid:16)\n\n(cid:21)\n\n,\n\nand the likelihood equation (4.23) cannot be solved analytically. the observed infor-\nmation matrix j (\u03b8, \u03b1) is\n\n(cid:20)\n\n(cid:16)\n\n(cid:16)\n\u03b1(\u03b1 + 1)/\u03b8 2\n\n(y j /\u03b8)\u03b1 \u2212 n\u03b1\u03b8\u22122\n\n[1 \u2212 (y j /\u03b8)\u03b1{1 + \u03b1 log(y j /\u03b8)}]\n\n\u03b8\u22121\n\n\u03b8\u22121\n\n(cid:16)\nn/\u03b12 +(cid:16)\n[1 \u2212 (y j /\u03b8)\u03b1{1 + \u03b1 log(y j /\u03b8)}]\n\n(y j /\u03b8)\u03b1{log(y j /\u03b8)}2\n\n(cid:21)\n\n,\n\nand to obtain maximum likelihood estimates we would iterate (4.25) until it converged.\nsuitable starting-values could be obtained by setting \u03b1\u2020 = 1, in which case \u03b8\u2020 = y.\n\u03c8 = (log \u03b8, log \u03b1)t, and iterate based on (cid:22)\u03c8 = \u03c8\u2020 + j (\u03c8\u2020\nif trouble arose in using (4.25), it would be sensible to write the problem in terms of\n\n\u22121u (\u03c8\u2020\n\n).\n\n)\n\nin this case a two-dimensional maximization can be avoided by noticing that for\n\nfixed \u03b1 the unique maximum likelihood estimate of \u03b8 is\n\n(cid:2)\n\n(cid:22)\u03b8\u03b1 =\n\n\u22121\n\nn\n\n(cid:4)1/\u03b1\n\ny\u03b1\nj\n\n.\n\nn(cid:3)\nj=1\n\nthe dashed line in the upper right panel of figure 4.1 shows the curve traced out by\n\n(cid:22)\u03b8\u03b1 as a function of \u03b1. the value of (cid:8) along this curve, the profile log likelihood for \u03b1,\n\n(cid:8)p(\u03b1) = max\n\n\u03b8\n\n(cid:8)(\u03b8, \u03b1) = (cid:8)((cid:22)\u03b8\u03b1, \u03b1),\n\nis shown in the lower right panel of the figure. this function is unimodal, and from it\n.= 6. more precise estimates are obtained maximizing (cid:8)p(\u03b1) numerically\n(cid:1)\n\nwe see that(cid:22)\u03b1\nover \u03b1, toobtain (cid:22)\u03b1 and hence(cid:22)\u03b8 =(cid:22)\u03b8(cid:22)\u03b1.\n\n "}, {"Page_number": 130, "text": "118\n\n4 \u00b7 likelihood\n\n). this is useful when j (\u03b8\u2020\n\na variant of the newton\u2013raphson method, fisher scoring, replaces j (\u03b8\u2020\n\n) in(4.25)\nwith the expected information i (\u03b8\u2020\n) isbadly behaved \u2014 for\nexample, not positive definite \u2014 but typically (4.25) works well. it has the advantage\nthat it can be implemented in an automatic way using numerical first and second\nderivatives of (cid:8)(\u03b8). in simple problems where minimizing programming time is more\nimportant than saving computing time it is generally simplest to maximize the log\nlikelihood directly using a packaged routine.\n\n4.4.2 large-sample distribution\nthus far we have treated the maximum likelihood estimate as a summary of a likeli-\nhood based on a given sample y1, . . . , yn, rather than as a random variable. evidently,\nhowever, we may consider its properties when samples are repeatedly taken from the\nmodel. suppose we have a random sample y1, . . . ,y n from a density f (y; \u03b8) that\nsatisfies the regularity conditions:\n\nr the true value \u03b8 0 of \u03b8 is interior to the parameter space \u0001, which has finite\n\ndimension and is compact;\n\nr the densities defined by any two different values of \u03b8 are distinct;\nr there is a neighbourhood n of \u03b8 0 within which the first three derivatives of the\nlog likelihood with respect to \u03b8 exist almost surely, and for r, s, t = 1, . . . , p,\n\u22121e{|\u2202 3(cid:8)(\u03b8)/\u2202\u03b8r \u2202\u03b8s \u2202\u03b8t|} is uniformly bounded for \u03b8 \u2208 n ; and\nn\nr within n , the fisher information matrix i (\u03b8) isfinite and positive definite, and\n\nits elements satisfy\ni (\u03b8)rs = e\n\n(cid:13)\n\n(cid:14)\n\n(cid:13)\n\n(cid:14)\n\n\u2202(cid:8)(\u03b8)\n\u2202\u03b8r\n\n\u2202(cid:8)(\u03b8)\n\u2202\u03b8s\n\n= e\n\n\u2212 \u2202 2(cid:8)(\u03b8)\n\u2202\u03b8r \u2202\u03b8s\n\nr, s = 1, . . . , p.\n\n,\n\nwe shall see below that this implies that i (\u03b8) isthe variance matrix of the score\nvector.\n\nsome cases where these conditions fail are described in section 4.6. if they do hold,\nthe main results below also apply to many situations where the data are neither\nindependent nor identically distributed.\nat the end of this section we establish two key results. first, as n \u2192 \u221e there is a\nvalue(cid:22)\u03b8 of \u03b8 such that (cid:8)((cid:22)\u03b8) is alocal maximum of (cid:8)(\u03b8) and pr((cid:22)\u03b8 \u2192 \u03b8 0) = 1; this is a\n\nstrongly consistent estimator of \u03b8. second,\n\ni (\u03b8 0)1/2((cid:22)\u03b8 \u2212 \u03b8 0) d\u2212\u2192 z as\n\nn \u2192 \u221e,\n\n(4.26)\n\nwhere z has the n p(0, i p) distribution. the first holds very generally, but the second\nrequires smoothness of certain log likelihood derivatives. the condition n \u2192 \u221e can\noften be replaced by i (\u03b8 0) \u2192 \u221e.\nanother way to express (4.26) is to say that for large n,(cid:22)\u03b8\n\u22121), and\nthis explains our definition of asymptotic relative efficiency for components of vector\nparameters, on page 113: we compare asymptotic variances of two different estimators\nof \u03b8 0.\n\n.\u223c n (\u03b8 0, i (\u03b8 0)\n\n "}, {"Page_number": 131, "text": "figure 4.5 repeated\nsampling likelihood\ninference for the\nexponential mean. the\nupper left panel shows the\nfunctions log rl(\u03b8) for\nten random samples of\nsize n = 10 from the\nexponential distribution\nwith mean \u03b8 0 = 1; the\ndashed line shows \u03b8 0. the\nlower left panel shows a\nhistogram of 5000\nmaximum likelihood\n\nestimates(cid:22)\u03b8, together with\n\ntheir approximate normal\ndensity. the upper right\npanel shows a probability\nplot of 5000 replicates of\nw (\u03b8 0) = \u22122 log rl(\u03b8 0)\nagainst quantiles of the \u03c7 2\n1\ndistribution. the lower\nright panel shows the\nconstruction of a 95%\nconfidence region for the\nvalue of \u03b8 using ten\nobservations from the\nspring failure data. the\nregion is the set of all \u03b8\nsuch that\nlog rl(\u03b8) \u2265 \u2212 1\n2 c1(0.95),\nwhere c1(0.95) is the 0.95\nquantile of the \u03c7 2\n1\ndistribution; the dotted\nhorizontal line shows\n1\n2 c1(0.95) and the limits\nof the region are the\ndashed vertical lines.\n\n)\na\nt\ne\nh\nt\n(\nl\nr\n \ng\no\n\nl\n\nf\nd\np\n\n0\n\n2\n-\n\n4\n-\n\n6\n-\n\n8\n-\n\n5\n1\n\n.\n\n0\n1\n\n.\n\n5\n0\n\n.\n\n0\n0\n\n.\n\n4.4 \u00b7 maximum likelihood estimator\n\n119\n\n. .\n\n. .\n\nc\ni\nt\ns\ni\nt\na\nt\ns\n \no\ni\nt\na\nr\n \nd\no\no\nh\n\ni\nl\n\ne\nk\nl\n\ni\n\n0\n1\n\n8\n\n6\n\n4\n\n2\n\n0\n\n............................................................................................................................................................................................................................................................................................................................................................................................................\n\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\n\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\ntheta\n\nquantiles of chi-squared distribution\n\nt\n\n)\na\ne\nh\n\nt\n(\nl\nr\ng\no\n\n \n\nl\n\n0\n\n2\n-\n\n4\n-\n\n6\n-\n\n8\n-\n\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\n\n0\n\n100 200 300 400 500\n\ntheta\n\ntheta\n\nwe illustrate (4.26) with random samples of size n = 10 from the exponential\ndistribution with true mean \u03b8 0 = 1. as we saw in section 4.2.1, the log likelihood for\na random sample y1, . . . , yn is (cid:8)(\u03b8) \u2261 \u2212n(log \u03b8 + y/\u03b8), and the maximum likelihood\nestimate is(cid:22)\u03b8 = y. the observed information and expected information are j (\u03b8) =\nn(2y/\u03b8 3 \u2212 1/\u03b8 2) and i (\u03b8) = n/\u03b8 2. the upper left panel of figure 4.5 shows the\nmaximum, so the distribution of(cid:22)\u03b8 is skewed; see the lower left panel. the density\nlog relative likelihoods for ten such samples. each curve is asymmetric about its\nof(cid:22)\u03b8 is roughly normal with mean \u03b8 0 = 1 and variance i (\u03b8 0)\n\u22121 = 1/10, but this is a\npoor approximation. in fact y has an exact gamma density with shape parameter 10\non replacing i (\u03b8 0) in(4.26) by i ((cid:22)\u03b8), we obtain the approximation\nand unit mean.\n\n(4.27)\nwhere v = i ((cid:22)\u03b8)\n\u22121 is the inverse expected information. provided (4.26) is true, re-\nplacement of i (\u03b8 0) by i ((cid:22)\u03b8) or j ((cid:22)\u03b8) isjustified by the fact that both converge in\n\nprobability to i (\u03b8 0), so we can apply slutsky\u2019s lemma (2.15). the main use of (4.27)\nis to construct confidence regions for components of \u03b8 0.\n\n(cid:22)\u03b8\n\n.\u223c n p(\u03b8 0, v ),\n\n "}, {"Page_number": 132, "text": "120\n\n4 \u00b7 likelihood\n\nscalar parameter\nif \u03b8 is scalar, (4.27) boils down to\n\nthus i ((cid:22)\u03b8)1/2((cid:22)\u03b8 \u2212 \u03b8 0) is anapproximate pivot from which to find confidence intervals\n\ni ((cid:22)\u03b8)1/2((cid:22)\u03b8 \u2212 \u03b8 0)\n\n.\u223c n (0, 1).\n\n(cid:24)\n\nfor \u03b8 0. that is,\n\n(cid:23)\nz\u03b1 \u2264 i ((cid:22)\u03b8)1/2((cid:22)\u03b8 \u2212 \u03b8 0) \u2264 z1\u2212\u03b1\n1 \u2212 2\u03b1 = pr\n(cid:23)(cid:22)\u03b8 \u2212 z1\u2212\u03b1 i ((cid:22)\u03b8)\n= pr\n(cid:15)(cid:22)\u03b8 \u2212 z1\u2212\u03b1 i ((cid:22)\u03b8)\n(cid:15)(cid:22)\u03b8 \u2212 z1\u2212\u03b1 j ((cid:22)\u03b8)\n\n\u22121/2 \u2264 \u03b8 0 \u2264(cid:22)\u03b8 \u2212 z\u03b1 i ((cid:22)\u03b8)\n\u22121/2,(cid:22)\u03b8 \u2212 z\u03b1 i ((cid:22)\u03b8)\n\u22121/2,(cid:22)\u03b8 \u2212 z\u03b1 j ((cid:22)\u03b8)\n\ngiving the (1 \u2212 2\u03b1) confidence interval for \u03b8 0,\nthe corresponding interval using the observed information j ((cid:22)\u03b8),\n\n\u22121/2\n\n\u22121/2\n\n(cid:17)\n\n(cid:17)\n\n,\n\n.\n\n(4.28)\n\n(4.29)\n\nz\u03b1 is the \u03b1 quantile of the\nstandard normal\ndistribution.\n\n(cid:24)\n\n,\n\n\u22121/2\n\nexample 4.24 (spring failure data) we reconsider the exponential model fitted\n\nis easier to calculate than (4.28) because it requires no expectations, and moreover its\ncoverage probability is often closer to the nominal level. both intervals are symmetric\n\nabout(cid:22)\u03b8.\nto the data of example 4.2, for which n = 10 and (cid:22)\u03b8 = y = 168.3. for this model\ni ((cid:22)\u03b8) = j ((cid:22)\u03b8) = n/y2, sothe 95% confidence intervals (4.28) and (4.29) for the true\nmean both equal y \u00b1 z0.025n\nexample 4.25 (cauchy data) to see the quality of these confidence intervals, we\ntake samples of size n from the cauchy density (2.16), for which\n1 \u2212 (y j \u2212 \u03b8)2\n{1 + (y j \u2212 \u03b8)2}2\n\n(cid:8)(\u03b8) \u2261 \u2212 n(cid:3)\nwe take \u03b8 0 = 0. the basis of (4.28) and (4.29) is large-sample normality of z i =\ni ((cid:22)\u03b8)1/2((cid:22)\u03b8 \u2212 \u03b8 0) and z j = j ((cid:22)\u03b8)1/2((cid:22)\u03b8 \u2212 \u03b8 0), and to assess this we compare z i and\n\nlog{1 + (y j \u2212 \u03b8)2}, j (\u03b8) = 2\n\n\u22121/2 y, that is, (64.0, 272.6).\n\n, i (\u03b8) = 1\n2\n\nn(cid:3)\nj=1\n\nj=1\n\n(cid:1)\n\nn;\n\nz j with a standard normal variable z. symmetry of the cauchy density about \u03b8 0\nimplies that z i and z j are distributed symmetrically about the origin, so the left\npanel of figure 4.6 compares quantiles of |z j| with those of |z| in a half-normal\nplot (practical 3.1), for 5000 simulated cauchy samples of size n = 15. evidently the\ndistribution of z j is close to normal; its empirical 0.9, 0.95, 0.975 and 0.99 quantiles\nare 1.34, 1.76, 2.08 and 2.55, compared with 1.28, 1.65, 1.96 and 2.33 for z. with\n\u03b1 = 0.025, (4.29) has estimated coverage probability 0.93, close to the nominal 0.95.\nthe right panel shows that z i has heavier tails than z j ; the coverage probability\nfor (4.28) with \u03b1 = 0.025 is 0.91. use of observed information is preferable, but\nthe large-sample approximations seem accurate enough for practical use even with\nn = 15.\n5000 samples with n = 10; the rest appeared unimodal. thus(cid:22)\u03b8 was almost invariably\n\njust one of the 5000 log likelihoods had two local maxima, compared to 36 for\n\nthe sole solution to the likelihood equation.\n\n(cid:1)\n\n "}, {"Page_number": 133, "text": "4.4 \u00b7 maximum likelihood estimator\n\n..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n\n4\n\n3\n\n2\n\n1\n\n0\n\n|\nj\n_\nz\n\n|\n\nfigure 4.6 inference\nbased on observed and\nexpected information in\nsamples of n = 15\ncauchy observations.\n|z j| = j ((cid:22)\u03b8)1/2|(cid:22)\u03b8 \u2212 \u03b8 0|;\nleft: half-normal plot of\n\nthe dotted line shows the\nideal, so z j is slightly\nheavier-tailed than\nnormal. right:\n|z i| = i ((cid:22)\u03b8)1/2|(cid:22)\u03b8 \u2212 \u03b8 0|\ncomparison of\nwith |z j|. |z i| has\nheavier tails.\n\n121\n\n4\n\n3\n\n|\ni\n\n_\nz\n\n|\n\n2\n\n1\n\n0\n\n.....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n\n0\n\n1\n\n2\n\n3\n\n4\n\n0\n\n1\n\n2\n\n3\n\n4\n\nhalf-normal quantiles\n\n|z_j|\n\nvector parameter\nwhen \u03b8 is a vector, confidence sets for the rth element of \u03b8 0, \u03b8 0\n\n, vrr ) distribution, where vrr is the (r, r) element of v = i ((cid:22)\u03b8)\n\nfact that the corresponding maximum likelihood estimator,(cid:22)\u03b8r , has approximately the\nr , may be based on the\n\u22121. this\ngives intervals (4.28) and (4.29), but with(cid:22)\u03b8, i ((cid:22)\u03b8)\n\u22121 replaced by(cid:22)\u03b8r , vrr ,\nand the (r, r) element of j ((cid:22)\u03b8)\n\u22121.\n\n\u22121, and j ((cid:22)\u03b8)\n\n\u22121 or j ((cid:22)\u03b8)\n\nn (\u03b8 0\nr\n\nexample 4.26 (normal distribution)\nin examples 4.18 and 4.22 we saw that\ntion based on a random sample y1, . . . , yn are (cid:22)\u00b5 = y and (cid:22)\u03c3 2 = n\nthe maximum likelihood estimates of the mean and variance of the normal distribu-\n(y j \u2212 y)2,\nand that the expected information matrix is diag{n/\u03c3 2, n/(2\u03c3 4)}. hence v =\n\u221212(cid:22)\u03c3 4}, and the (1 \u2212 2\u03b1) confidence intervals for \u00b5 and \u03c3 2 based on\ndiag{n\nthe large-sample results above are\n\n\u22121(cid:22)\u03c3 2, n\n\n(cid:16)\n\n\u22121\n\ny \u00b1 n\n\n\u22121/2(cid:22)\u03c3 z\u03b1, (cid:22)\u03c3 2 \u00b1 (2/n)1/2(cid:22)\u03c3 2z\u03b1.\n\ns2 = n(cid:22)\u03c3 2/(n \u2212 1) is the\n\nunbiased estimate of \u03c3 2.\n\n\u22121/2stn\u22121(\u03b1), but with s replaced by(cid:22)\u03c3 and the t quantile replaced by\nthe asymptotic approximation gives an interval for \u00b5 with the same form as the exact\ninterval, y \u00b1 n\nthe corresponding normal quantile. provided that n > 20 or so, these alterations will\nfor \u03c3 2 to be good, because normal approximation to the distribution of(cid:22)\u03c3 2 is poorer\ntypically have little effect on the interval. larger samples are needed for the interval\nthan to the distribution of(cid:22)\u00b5.\n(cid:1)\nthat (4.27) entails ((cid:22)\u03b8 \u2212 \u03b8 0)tv\n\nthe use of (4.27) to give confidence regions for the whole of \u03b8 rests on the fact\np. hence an approximate (1 \u2212 2\u03b1) con-\n\n\u22121((cid:22)\u03b8 \u2212 \u03b8 0)\n\n.\u223c \u03c7 2\n\nfidence region is\n\n{\u03b8 : ((cid:22)\u03b8 \u2212 \u03b8)tv\n\n\u22121((cid:22)\u03b8 \u2212 \u03b8) \u2264 c p(1 \u2212 2\u03b1)};\n\nan ellipsoid centred at(cid:22)\u03b8, with shape determined by the elements of v and volume\ndetermined by c p(1 \u2212 2\u03b1). another version replaces i ((cid:22)\u03b8)\n\n\u22121 with j ((cid:22)\u03b8)\n\n\u22121.\n\n "}, {"Page_number": 134, "text": "122\n\n4 \u00b7 likelihood\n\nexample 4.27 (challenger data) examples 4.5 and 4.8 discuss a model for the\nchallenger data, where the probability of o-ring thermal distress depends on the\n\nlaunch temperature. the maximum likelihood estimates for this model are(cid:22)\u03b20 = 5.084\nand(cid:22)\u03b21 = \u22120.116, and the inverse observed information is\n9.289 \u22120.142\n\u22120.142\n0.00220\n\nj ((cid:22)\u03b20,(cid:22)\u03b21)\n\n\u22121 =\n\n(cid:21)\n\n(cid:20)\n\n,\n\nyielding standard errors 9.2891/2 = 3.048 and 0.002201/2 = 0.0469. the estimated\ncorrelation of(cid:22)\u03b20 and(cid:22)\u03b21, \u22120.142/(9.289 \u00d7 0.00220)1/2, equals \u22120.993, and we see\nthat the matrix j ((cid:22)\u03b20,(cid:22)\u03b21) isclose to singular. in view of the left panel of figure 4.3\n\nthis is not surprising.\n\na joint 95% confidence region for (\u03b20, \u03b21) isthe ellipsoid given by\n\n(\u03b20 \u2212 5.084, \u03b21 + 0.116)j ((cid:22)\u03b20,(cid:22)\u03b21)\n\n(cid:21)\n\n(cid:20)\n\n\u03b20 \u2212 5.084\n\u03b21 + 0.116\n\n\u2264 c2(0.95) = 5.99.\n\noften we focus on a scalar parameter \u03c8 = \u03c8(\u03b8), estimated by (cid:22)\u03c8 = \u03c8((cid:22)\u03b8). to ap-\nproximate the variance of (cid:22)\u03c8 we apply the delta method (2.19), giving\n\n(cid:1)\n\n\u03c8((cid:22)\u03b8)\n\n.= \u03c8(\u03b8 0) + \u2202\u03c8(\u03b8 0)\n\n\u2202\u03b8 t\n\n((cid:22)\u03b8 \u2212 \u03b8 0).\n\nconsequently\n\nvar{\u03c8((cid:22)\u03b8)} .= \u2202\u03c8(\u03b8 0)\n\nvar((cid:22)\u03b8)\n\n\u2202\u03b8 t\n\n\u2202\u03c8(\u03b8 0)\n\n\u2202\u03b8\n\n.= \u2202\u03c8((cid:22)\u03b8)\n\n\u2202\u03b8 t\n\nj ((cid:22)\u03b8)\n\n\u2202\u03c8((cid:22)\u03b8)\n\n\u22121\n\n,\n\n\u2202\u03b8\n\nwhere \u2202\u03c8((cid:22)\u03b8)/\u2202\u03b8 is the p \u00d7 1 vector of derivatives of \u03c8 evaluated at (cid:22)\u03b8. thus an\napproximate (1 \u2212 2\u03b1) confidence interval for \u03c8 is\n\u03c8((cid:22)\u03b8) \u00b1 z\u03b1{\u2202\u03c8((cid:22)\u03b8)/\u2202\u03b8 t j ((cid:22)\u03b8)\n\n\u22121\u2202\u03c8((cid:22)\u03b8)/\u2202\u03b8}1/2.\n\n(4.30)\n\n\u25e6\n\nexample 4.28 (challenger data) one quantity of particular interest is the probabil-\nf, \u03c8 = e\u03b20+31\u03b21 /(1 + e\u03b20+31\u03b21). its maximum likelihood estimate\nity of failure at 31\nand derivatives are\n(cid:22)\u03b20+31(cid:22)\u03b21\n(cid:22)\u03b20+31(cid:22)\u03b21\n\n= 31\u03c8(1 \u2212 \u03c8).\n\n= \u03c8(1 \u2212 \u03c8),\n\n= 0.816,\n\n(cid:22)\u03c8 = e\n1 + e\n\nthe 95% confidence interval (4.30) for \u03c8 is 0.816 \u00b1 1.96 \u00d7 0.242 = (0.34, 1.29).\nas this contains values greater than one it is less than satisfactory, so we need a better\n(cid:1)\napproach, such as the one described in section 4.5.2.\n\n\u2202\u03c8\n\u2202\u03b20\n\n\u2202\u03c8\n\u2202\u03b20\n\nconsistency of(cid:22)\u03b8\n\nwe now obtain the key convergence results for maximum likelihood estimation of a\nscalar, subject to the regularity conditions on page 118.\n\n "}, {"Page_number": 135, "text": "4.4 \u00b7 maximum likelihood estimator\nlet h : ir\u2192 ir be convex. then for any real x1, x2,\n\nh{\u03c0 x1 + (1 \u2212 \u03c0)x2} \u2264\u03c0 h(x1) + (1 \u2212 \u03c0)h(x2),\n\n0 \u2264 \u03c0 \u2264 1.\n\n123\n\nif x is a real-valued random variable, then jensen\u2019s inequality says that e{h(x)} \u2265\nh{e(x)}, with equality if and only if x is degenerate.\ntrue value \u03b8 0, and let (cid:8)(\u03b8) = n\n\nlet y1, . . . ,y n be a random sample from a density f (y; \u03b8), where \u03b8 is scalar with\n\nlog f (y j ; \u03b8). now\n\n(cid:16)\n\n\u22121\n\ne{(cid:8)(\u03b8) \u2212 (cid:8)(\u03b8 0)} =e\n\nlog\n\n(cid:9)\n\n(cid:13)\n(cid:13)\n\n(cid:14)(cid:10)\nf (y1; \u03b8)\n(cid:14)\nf (y1; \u03b8 0)\nf (y1; \u03b8)\nf (y1; \u03b8 0)\nf (y; \u03b8)\nf (y; \u03b8 0)\n\n\u2264 log e\n(cid:25)\n= log\n\nf (y; \u03b8 0) dy = 0,\n\n(4.31)\n\nwhere we have applied jensen\u2019s inequality to the convex function \u2212 log x. the in-\nequality is strict unless the density ratio is constant, so that the densities are the same,\nand according to our regularity conditions this may occur only if \u03b8 = \u03b8 0. asn \u2192 \u221e,\nthe weak law of large numbers applies to the average (cid:8)(\u03b8) \u2212 (cid:8)(\u03b8 0), which converges\nin probability to (cid:25)\n\n(cid:13)\n\n(cid:14)\n\nlog\n\nf (y; \u03b8)\nf (y; \u03b8 0)\n\nf (y; \u03b8 0) dy = \u2212d( f\u03b8 , f\u03b8 0),\n\nsolomon kullback\n(1907\u20131994) was born\nand educated in new\nyork. he had careers in\nthe us defense\ndepartment and then at\ngeorge washington\nuniversity. his main\nscientific contribution is to\ninformation theory.\nrichard arthur leibler\n(1914\u2013) has spent much of\nhis life working in the us\ndefense community. their\ndefinition of information\nwas published in 1951.\n\nsay. this is negative unless \u03b8 = \u03b8 0. the quantity d( f, g) \u2265 0 isknown as the\nkullback\u2013leibler discrepancy between f and g; it isminimized when f = g. in\nfact this convergence is almost sure, that is, (cid:8)(\u03b8) \u2212 (cid:8)(\u03b8 0) converges to \u2212d( f\u03b8 , f\u03b8 0)\nwith probability one. this shores up our earlier informal discussion of figure 4.4, for\nwe see that if \u03b8 (cid:10)= \u03b8 0, then\n\n(cid:8)(\u03b8) \u2212 (cid:8)(\u03b8 0) \u223c n d( f\u03b8 , f\u03b8 0) \u2192 \u2212\u221e\n\n(cid:6)\n\nwith probability one as n \u2192 \u221e.\nnow for any \u03b4 > 0, (cid:8)(\u03b8 0 \u2212 \u03b4) \u2212 (cid:8)(\u03b8 0) and (cid:8)(\u03b8 0 + \u03b4) \u2212 (cid:8)(\u03b8 0) converge with prob-\nability one to the negative quantities \u2212d( f\u03b8 0\u2212\u03b4, f\u03b8 0) and \u2212d( f\u03b8 0+\u03b4, f\u03b8 0). hence for\nhas a local maximum in the interval (\u03b8 0 \u2212 \u03b4, \u03b8 0 + \u03b4). if we let(cid:22)\u03b8 denote the value at\nany sequence of random variables y1, . . . ,y n there is an n\n, (cid:8)(\u03b8)\nwhich this local maximum occurs, then pr((cid:22)\u03b8 \u2192 \u03b8 0) = 1 and(cid:22)\u03b8 is said to be a strongly\nconsistent estimate of \u03b8 0. this implies (cid:22)\u03b8\np\u2212\u2192 \u03b8 0, so (cid:22)\u03b8 is consistent in our usual,\nabout uniqueness of(cid:22)\u03b8, merely that a strongly consistent local maximum exists, but if\n(cid:8)(\u03b8) has just one maximum, then(cid:22)\u03b8 must also be the global maximum. a more delicate\nargument is needed when \u03b8 is vector, because it is then not enough to consider only\nthe two values \u03b8 0 \u00b1 \u03b4.\n\nas this proof does not require f (y; \u03b8) to besmooth it is very general. it says nothing\n\nsuch that for n > n\n\nweaker, sense.\n\n(cid:6)\n\n "}, {"Page_number": 136, "text": "124\n\n4 \u00b7 likelihood\nasymptotic normality of(cid:22)\u03b8\nto prove asymptotic normality of(cid:22)\u03b8, weassume that (cid:22)\u03b8 satisfies the likelihood equation\nand consider the score statistic, u (\u03b8) = d(cid:8)(\u03b8)/d\u03b8. its mean and variance are\n\ne{u (\u03b8)} = n(cid:3)\nvar{u (\u03b8)} = n(cid:3)\n\nj=1\n\n(cid:13)\n\n(cid:13)\n\ne\n\nvar\n\nj=1\n\n(cid:14)\n\nd log f (y j ; \u03b8)\n\nd\u03b8\n\nd log f (y j ; \u03b8)\n\nd\u03b8\n\n= n e{u(\u03b8)},\n(cid:14)\n\n= n var{u(\u03b8)},\n\nwhere u(\u03b8) = d log f (y j ; \u03b8)/d\u03b8 is the score function for a single random variable.\nprovided the order of differentiation and integration may be interchanged, the mean\nof u(\u03b8) is\ne{u(\u03b8)} =\n\nf (y; \u03b8) dy =\n\nd log f (y; \u03b8)\n\nd f (y; \u03b8)\n\n(cid:25)\n\n(cid:25)\n\n(cid:25)\n\ndy = d\nd\u03b8\n\nd\u03b8\n\nd\u03b8\n\nf (y; \u03b8)dy = 0,\n(4.32)\n\nbecause f (y; \u03b8) has integral one for each value of \u03b8. furthermore\n\n(cid:25)\n\n0 = d\n(cid:25)\nd\u03b8\n=\n\nand so\n\nd log f (y; \u03b8)\n\nd\u03b8\n\nd2 log f (y; \u03b8)\n\nd\u03b8 2\n\nf (y; \u03b8) dy\nf (y; \u03b8)dy +\n\n(cid:25) (cid:13)\n\n(cid:14)2\n\nd log f (y; \u03b8)\n\nd\u03b8\n\nf (y; \u03b8) dy,\n\n(cid:25)\n\nd2 log f (y; \u03b8)\n\nf (y; \u03b8)dy = i(\u03b8),\n\nvar{u(\u03b8)} =e{u (\u03b8)2} = \u2212\nnow both u (\u03b8 0) and j (\u03b8 0) = \u2212(cid:16)\nthe expected information from a single observation.\nd2(cid:8)(\u03b8 0)/d\u03b8 2 are sums of n independent ran-\ndom variables, and e{u (\u03b8 0)} =0, var{ u (\u03b8 0)} = i (\u03b8 0) = ni(\u03b8 0), while e{j (\u03b8 0)} =\ni (\u03b8 0) = ni(\u03b8 0). hence the central limit theorem (2.11) and the weak law of large\nnumbers imply that\n\n(4.33)\n\nd\u03b8 2\n\ni (\u03b8 0)\n\n\u22121/2u (\u03b8 0)\n\nd\u2212\u2192 z ,\n\ni (\u03b8 0)\n\n\u22121 j (\u03b8 0)\n\np\u2212\u2192 1,\n\n(4.34)\n\nwhere z has the standard normal distribution.\n\nif the log likelihood is sufficiently smooth to allow taylor series expansion, then(cid:22)\u03b8\n\nsatisfies the likelihood equation\n\n0 = u ((cid:22)\u03b8)\n\n.= u (\u03b8 0) + d2(cid:8)(\u03b8 0)\nd\u03b8 2\n\n((cid:22)\u03b8 \u2212 \u03b8 0),\n\nrearrangement of which gives\n\n(cid:22)\u03b8 \u2212 \u03b8 0 .= j (\u03b8 0)\n\n\u22121u (\u03b8 0),\n\n "}, {"Page_number": 137, "text": "4.4 \u00b7 maximum likelihood estimator\n\n125\n\nwhere j (\u03b8 0) isthe observed information and we require that the missing terms of the\ntaylor series are asymptotically small enough to be ignored. if so,\n\ni (\u03b8 0)1/2((cid:22)\u03b8 \u2212 \u03b8 0)\n\n.= i (\u03b8 0)1/2 j (\u03b8 0)\n= i (\u03b8 0)1/2 j (\u03b8 0)\nd\u2212\u2192 z ,\n\n\u22121u (\u03b8 0)\n\u22121 i (\u03b8 0)1/2 \u00d7 i (\u03b8 0)\n\n\u22121/2u (\u03b8 0)\n\nby (4.34) and slutsky\u2019s lemma (2.15). replacement of i (\u03b8 0) by i ((cid:22)\u03b8) or j ((cid:22)\u03b8) isjustified\nby the fact that both converge in probability to i (\u03b8 0) asn \u2192 \u221e.\nthis argument is generalized to vector \u03b8 by interpreting the score as a p \u00d7 1\nvector and the information quantities as p \u00d7 p matrices, with z having a n p(0, i p)\ndistribution.\n\nexercises 4.4\n1\n\nin example 4.23, show that(cid:22)\u03b1 is the solution of the equation\n\n(cid:11)(cid:16)\n\n(cid:22)\u03b1 =\n\n(cid:22)\u03b1\n(cid:16)\nj log y j\nj y\nj y\n\n(cid:22)\u03b1\n\nj\n\n\u2212 n\n\n\u22121\n\n(cid:3)\n\nj\n\n(cid:12)\u22121\n\nlog y j\n\n.\n\n2\n\n3\n\n4\n\n5\n\n6\n\nz\u03b1 is the \u03b1 quantile of the\nstandard normal\ndistribution.\n\nif the log likelihood for a p \u00d7 1 vector of parameters is (cid:8)(\u03b8) = a + bt\u03b8 \u2212 1\n\u03b8 tc\u03b8, where\nthe constants a, b and c are respectively scalar, a p \u00d7 1 vector, and a p \u00d7 p symmetric\npositive definite matrix, show that the score statistic can be written b \u2212 c\u03b8. find the\nobserved information j (\u03b8), and show that(cid:22)\u03b8 is attained in one step of (4.25) from any\n\n2\n\nexp (\u2212|y \u2212 \u00b5|/\u03c3 ) ,\n\ninitial value of \u03b8.\nthe laplace or double exponential distribution has density\n\u2212\u221e < y < \u221e, \u2212\u221e < \u00b5 <\u221e, \u03c3 > 0.\nf (y; \u00b5, \u03c3 ) = 1\n2\u03c3\nsketch the log likelihood for a typical sample, and explain why the maximum likelihood\nestimate is only unique when the sample size is odd. derive the score statistic and observed\ninformation. is maximum likelihood estimation regular for this distribution?\neggs are thought to be infected with a bacterium salmonella enteriditis so that the number\nof organisms, y , ineach has a poisson distribution with mean \u00b5. the value of y cannot be\nobserved directly, but after a period it becomes certain whether the egg is infected (y > 0)\nor not (y = 0). out of m such eggs, r are found to be infected. find the maximum likelihood\nestimator(cid:22)\u00b5 of \u00b5 and its asymptotic variance. is the exact variance of(cid:22)\u00b5 defined?\nif y1, . . . ,y n is a random sample from density \u03b8\u22121e\n\u2212x/\u03b8 , show that the maximum likelihood\nestimator(cid:22)\u03b8 has an asymptotic normal distribution with mean \u03b8 and variance \u03b8 2/n. deduce\nthat an approximate (1 \u2212 2\u03b1) confidence interval for \u03b8 is\n(cid:22)\u03b8\n\n(cid:22)\u03b8\n\n1 + z\u03b1n\u22121/2\n\n\u2265 \u03b8 \u2265\n\n1 + z1\u2212\u03b1n\u22121/2\n\n.\n\nshow that(cid:22)\u03b8 /\u03b8 is an exact pivot, having the gamma distribution with unit mean and shape\nparameter \u03ba = n. hence find an exact confidence interval for \u03b8, and compare it with the\napproximate one when n = 10 and(cid:22)\u03b8 = 100.\n\niid\u223c n (\u00b5, c\u00b52), where c is a known constant, show that the minimal sufficient\nif y1, . . . ,y n\nstatistic for \u00b5 is the same as for the n (\u00b5, \u03c3 2) distribution. find the maximum likelihood\nestimate of \u00b5 and give its large-sample standard error. show that the distribution of y 2/s2\ndoes not depend on \u00b5.\n\n "}, {"Page_number": 138, "text": "126\n\n4 \u00b7 likelihood\n\n4.5 likelihood ratio statistic\n4.5.1 basic ideas\nsuppose that our model is determined by a parameter \u03b8 of dimension p, whose true\n\nbut unknown value is \u03b8 0, and for which the maximum likelihood estimate is(cid:22)\u03b8. then\n\nprovided the model satisfies the conditions for asymptotic normality of the maximum\nlikelihood estimator given in the previous section, in large samples the likelihood\nratio statistic\n\nw (\u03b8 0) = \u22122 log rl(\u03b8 0) = 2{(cid:8)((cid:22)\u03b8) \u2212 (cid:8)(\u03b8 0)}\n\n(4.35)\n\nhas an approximate chi-squared distribution on p degrees of freedom under repeated\nsampling of data from the model. that is, as i (\u03b8 0) \u2192 \u221e,\n\nw (\u03b8 0)\n\nd\u2212\u2192 \u03c7 2\n\np\n\n,\n\n(4.36)\n\n.\u223c \u03c7 2\n\nso w (\u03b8 0)\n1 when \u03b8 is scalar. in practice this result is used to generate approx-\nimations for finite samples. it is illustrated in the upper right panel of figure 4.5,\nwhich compares 5000 simulated values of w (\u03b8 0), based on exponential samples of\nsize n = 10, with quantiles of the \u03c7 2\n1 distribution. here p = 1, w (\u03b8 0) = 2n{y /\u03b8 0 \u2212\n1 \u2212 log(y /\u03b8 0)}, and \u03b8 0 = 1. this approximation seems better than that for(cid:22)\u03b8.\nto establish (4.36), we note that d(cid:8)((cid:22)\u03b8)/d\u03b8 = 0 and make a taylor series expansion\nw (\u03b8 0) = 2{(cid:8)((cid:22)\u03b8) \u2212 (cid:8)(\u03b8 0)}\n\nof w (\u03b8 0), giving\n(cid:13)\n\n(cid:14)\n\n\u2202(cid:8)((cid:22)\u03b8)\n\n\u2202\u03b8\n\n(\u03b8 0 \u2212(cid:22)\u03b8)t\n\n\u2212 1\n2\n\n\u2202 2(cid:8)((cid:22)\u03b8)\n\n\u2202\u03b8 \u2202\u03b8 t\n\n(\u03b8 0 \u2212(cid:22)\u03b8)\n\n(cid:8)((cid:22)\u03b8) \u2212 (cid:8)((cid:22)\u03b8) \u2212 (\u03b8 0 \u2212(cid:22)\u03b8)t\n.= 2\n= ((cid:22)\u03b8 \u2212 \u03b8 0)t j ((cid:22)\u03b8)((cid:22)\u03b8 \u2212 \u03b8 0)\n.= ((cid:22)\u03b8 \u2212 \u03b8 0)t i (\u03b8 0)((cid:22)\u03b8 \u2212 \u03b8 0),\n\nand the limiting normal distribution for(cid:22)\u03b8 at (4.26) and the relation (3.23) linking this\n\nto the chi-squared distribution yield (4.36).\n\nexpression (4.36) shows that w (\u03b8 0) is anapproximate pivot which may be used to\n\nprovide confidence regions for \u03b8 0. forif w (\u03b8 0)\n\n.\u223c \u03c7 2\n\np, then\npr{w (\u03b8 0) \u2264 c p(1 \u2212 2\u03b1)} .= 1 \u2212 2\u03b1,\n\nc p(\u03b1) denotes the \u03b1\nquantile of the \u03c7 2\np\ndistribution.\n\nand hence values of \u03b8 for which w (\u03b8) \u2264 c p(1 \u2212 2\u03b1) may be regarded as \u2018plausible\u2019\nat the (1 \u2212 2\u03b1) level. equivalently, the set\n\n(cid:13)\n\n\u03b8 : (cid:8)(\u03b8) \u2265 (cid:8)((cid:22)\u03b8) \u2212 1\n\nc p(1 \u2212 2\u03b1)\n\n(cid:14)\n\n(4.37)\nis a (1 \u2212 2\u03b1) confidence region for the unknown \u03b8 0. weuse (1 \u2212 2\u03b1) here for con-\nsistency with our earlier discussion of confidence intervals.\n\n2\n\nthese \u2018plausible\u2019 sets of \u03b8 based on w (\u03b8 0) under repeated sampling have the same\nform as those for the pure likelihood approach described at the end of section 4.1.2,\n\n "}, {"Page_number": 139, "text": "4.5 \u00b7 likelihood ratio statistic\n127\nsince the condition rl(\u03b8) \u2265 c is equivalent to w (\u03b8) \u2264 \u22122 log c. here however the\nconstant \u22122 log c is replaced by c p(1 \u2212 2\u03b1), chosen with respect to the approximate\ndistribution of w (\u03b8 0) under repeated sampling. often \u03b1 is taken to be 0.05, 0.025 or\n0.005, values that correspond to regions containing \u03b8 0 with approximate probabilities\n0.9, 0.95 and 0.99.\n\nexample 4.29 (spring failure data) the likelihood ratio statistic for the exponen-\ntial model in example 4.2 is w (\u03b8) = 2n{y/\u03b8 \u2212 1 \u2212 log(y/\u03b8)}. asc 1(0.95) = 3.84,\na 95% confidence region for \u03b8 based on w (\u03b8) isthe set\n\n{\u03b8 : 2n {y/\u03b8 \u2212 1 \u2212 log(y/\u03b8)} \u2264 3.84} .\n\n2\n\nwhich (cid:8)(\u03b8) \u2265 (cid:8)((cid:22)\u03b8) \u2212 1\nthis set is found by plotting the log likelihood and reading off the values of \u03b8 for\n\u00d7 3.84. the lower right panel of figure 4.5 shows this region,\n(96, 335), which is not symmetric about the maximum likelihood estimate y = 168.3.\ntotic normal distribution of(cid:22)\u03b8, (64, 273), is symmetric about(cid:22)\u03b8. the difference between\nwe saw in example 4.24 that the 95% confidence interval for \u03b8 based on the asymp-\nintervals based on w (\u03b8) and(cid:22)\u03b8 would vanish in sufficiently large samples, but it can\nto the distribution of(cid:22)\u03b8, which may be problematic when (cid:8)(\u03b8) ismultimodal.\n\nbe important to capture the asymmetry of (cid:8)(\u03b8) when n is small or moderate. regions\ndefined by (4.37) need not be connected, unlike those based on normal approximation\n(cid:1)\n\nwhen \u03b8 is vector, confidence regions for \u03b8 0 can in principle be obtained from (4.37)\nthrough contour plots of (cid:8). this seems infeasible when p exceeds three. we discuss\none resolution of this in the next section.\n\n4.5.2 profile log likelihood\nin the previous section we treated all elements of \u03b8 equally, but in practice some\nare more important than others. we write \u03b8 t = (\u03c8 t, \u03bbt), where \u03c8 is a p \u00d7 1 vec-\ntor of parameters of interest, and \u03bb is a q \u00d7 1 vector of nuisance parameters. our\nenquiry centres on \u03c8, but we cannot avoid including \u03bb in the model. we may wish\nto check whether a particular value \u03c8 0 of \u03c8 is consistent with the data, or to find a\nplausible range of values for \u03c8, but in either case the value of \u03bb is irrelevant or of at\nmost secondary interest. the division into \u03c8 and \u03bb may change in the course of an\ninvestigation.\n\ntwo models are said to be nested if one reduces to the other when certain parameters\nare fixed. thus a model with parameters (\u03c8 0, \u03bb) isnested within the more general\nmodel with parameters (\u03c8, \u03bb); the corresponding parameter spaces are {\u03c8 0} \u00d7\u0001\nand \u0001 \u00d7 \u0001, where \u03c8 0 \u2208 \u0001. under the more restrictive model the value of \u03bb that\nmaximizes the log likelihood (cid:8)(\u03c8 0, \u03bb) is(cid:22)\u03bb\u03c8 0, whereas the overall maximum likelihood\nestimate, ((cid:22)\u03c8 ,(cid:22)\u03bb), maximizes (cid:8) over both parameters. of course, (cid:8)((cid:22)\u03c8 ,(cid:22)\u03bb) \u2265 (cid:8)(\u03c8 0,(cid:22)\u03bb\u03c8 0).\n\nexample 4.30 (weibull distribution) the weibull density (4.4) has two parameters\n\u03b1 and \u03b8, and reduces to the exponential density when \u03b1 = 1. in terms of our general\ndiscussion we set \u03b1 = \u03c8 and \u03bb = \u03b8, with \u03c8 0 = 1, \u0001 = ir+, and \u0001 = ir+. then the\n\n "}, {"Page_number": 140, "text": "4 \u00b7 likelihood\n128\nvertical dotted line in the upper right panel of figure 4.1 corresponds to {\u03c8 0} \u00d7\u0001,\nwhile the entire upper right quadrant of the plane is \u0001 \u00d7 \u0001. evidently the likelihood\nreaches its maximum away from the exponential submodel. the maximum likelihood\nestimates under the submodel are (1, 168), while overall they are roughly (6, 181);\n(cid:1)\nthe difference of log likelihoods is 12.5.\n\na natural statistic with which to compare two nested models is the log ratio of\n\nmaximized likelihoods,\n\nwp(\u03c8 0) = 2{(cid:8)((cid:22)\u03c8 ,(cid:22)\u03bb) \u2212 (cid:8)(\u03c8 0,(cid:22)\u03bb\u03c8 0)}.\n\n(4.38)\n\nthis is sometimes called the generalized likelihood ratio statistic because it gener-\nalizes (4.35), but as (4.38) is the version almost invariably used in practice we shall\nrefer to both simply as likelihood ratio statistics. at the end of this section we show\nthat for regular models (4.36) generalizes to\n\nwp(\u03c8 0)\n\nd\u2212\u2192 \u03c7 2\n\np\n\n.\n\n(4.39)\n\nthat is, even though nuisance parameters are estimated, the likelihood ratio statistic\nhas an approximate chi-squared distribution in large samples.\n\noften the parameter of interest, \u03c8, isscalar or has much smaller dimension than\nthe nuisance parameter, \u03bb, and we wish to form a confidence region for its true value\n\u03c8 0 regardless of \u03bb. to do so we use theprofile log likelihood,\n\n(cid:8)p(\u03c8) = max\n\n(cid:8)(\u03c8, \u03bb) = (cid:8)(\u03c8,(cid:22)\u03bb\u03c8 ),\n\n\u03bb\n\nwhere(cid:22)\u03bb\u03c8 is the maximum likelihood estimate of \u03bb for fixed \u03c8. the above result for\nwp(\u03c8 0) implies that confidence regions for \u03c8 0 can be based on (cid:8)p for regular models.\na (1 \u2212 2\u03b1) confidence region for \u03c8 0 is the set\n\u03c8 : (cid:8)p(\u03c8) \u2265 (cid:8)p((cid:22)\u03c8) \u2212 1\n\nc p(1 \u2212 2\u03b1)\n\n(4.40)\n\n(cid:13)\n\n(cid:14)\n\n.\n\n2\n\nthis is our primary approach to finding confidence regions from likelihoods. it often\nyields good approximations to standard intervals.\n\nwhen \u03c8 is scalar we define the signed likelihood ratio statistic\n\nthe relation between the normal and chi-squared distributions implies that\nc1(1 \u2212 2\u03b1) = z2\n\nz(\u03c8 0) = sign((cid:22)\u03c8 \u2212 \u03c8 0)[2{(cid:8)((cid:22)\u03c8 ,(cid:22)\u03bb) \u2212 (cid:8)(\u03c8 0,(cid:22)\u03bb\u03c8 0)}]1/2.\n\u03b1 = z2\n1 \u2212 2\u03b1\n\n.= pr{wp(\u03c8 0) \u2264 c1(1 \u2212 2\u03b1)}\n= pr{z(\u03c8 0) \u2264 z1\u2212\u03b1} \u2212pr{ z(\u03c8 0) \u2264 z\u03b1},\n\n1\u2212\u03b1, so\n\nand z(\u03c8 0) may be regarded as having an approximate standard normal distribution\nand is an approximate pivot on which inference for \u03c8 0 may be based; when p = 1,\na different way of writing (4.40) is\n\n{\u03c8 : z\u03b1 \u2264 z(\u03c8) \u2264 z1\u2212\u03b1} .\n\n(4.41)\n\nthis is sometimes called\nthe directed deviance\nstatistic.\n\n "}, {"Page_number": 141, "text": "4.5 \u00b7 likelihood ratio statistic\n\n129\n\nwe have briefly mentioned the effect of reparametrization on likelihood. if \u03c8 is of\ncentral interest, inference should be invariant to interest-preserving transformations,\nunder which \u03c8, \u03bb (cid:16)\u2192 \u03b7(\u03c8), \u03b6 (\u03c8, \u03bb), where the map \u03c8 \u2192 \u03b7 is one-one for each value\nof \u03c8, and so too is the map \u03bb (cid:16)\u2192 \u03b6 . for such a reparametrization, (cid:8)p(\u03b7) = (cid:8)p(\u03c8), so\nwp(\u03c8) is invariant; so too is z(\u03c8) apart from a possible change in sign.\n\nexample 4.31 (normal distribution) the log likelihood for a normal sample\ny1, . . . , yn is\n\n(cid:11)\n\n(cid:8)(\u00b5, \u03c3 2) \u2261 \u2212 1\n2\n\nn log \u03c3 2 + 1\n\n\u03c3 2\n\n(y j \u2212 \u00b5)2\n\n.\n\n(cid:12)\n\nn(cid:3)\nj=1\n\nto use the profile log likelihood to find a confidence region for \u00b5, we set\u03c8 = \u00b5,\n\u03bb = \u03c3 2, and note that for fixed \u00b5, the maximum likelihood estimate of \u03c3 2 is\n\n\u22121\n\n(cid:22)\u03c3 2\n\u00b5 = n\n= n\n\u22121\n= n \u2212 1\n\n(cid:3)\n(y j \u2212 \u00b5)2\n(cid:7)(cid:3)\n(y j \u2212 y)2 + n(y \u2212 \u00b5)2\n(cid:13)\n1 + t(\u00b5)2\nn \u2212 1\n\n(cid:14)\n\ns2\n\n,\n\nn\n\n(cid:8)\n\n(cid:16)\n\n,\n\n,\n\n\u00b5\n\n2\n\n(cid:15)\n\n\u22121\n\n(cid:9)\n\n(cid:13)\n\n(cid:14)\n\nn log\n\n\u00b5,(cid:22)\u03c3 2\n\n(cid:14)(cid:10)1/2\n\n(cid:8)p(\u00b5) = (cid:8)\n(cid:13)\n\nwhere t(\u00b5) = (y \u2212 \u00b5)/(s2/n)1/2 is the observed value of the t statistic (3.16) and\ns2 = (n \u2212 1)\n\n(y j \u2212 y)2. thus the profile log likelihood for \u00b5 is\nlog[s2{1 + t(\u00b5)2/(n \u2212 1)}],\n\n(cid:17) \u2261 \u2212 n\n\n1 + t (\u00b5)2\nn \u2212 1\n\nz(\u00b5) = sign(y \u2212 \u00b5)\n\nand as the overall maximum likelihood estimate of \u00b5 is(cid:22)\u00b5 = y, t((cid:22)\u00b5) = 0 and\n1 + t (\u00b5)2\nwp(\u00b5) = n log\nn \u2212 1\nwhose values are large when t (\u00b5) = (y \u2212 \u00b5)/(s2/n)1/2 is large, that is, when \u00b5\ndiffers from y in either direction. evidently the confidence interval (4.40) has the\nform t (\u00b5)2 \u2264 c and may be written y \u00b1 n\n\u22121/2sc1/2. the usual (1 \u2212 2\u03b1) confidence\ninterval, based on the exact distribution of t (\u00b5), sets c1/2 to be a quantile of the\nstudent t distribution, tn\u22121(1 \u2212 \u03b1). for n = 15 and \u03b1 = 0.025, tn\u22121(1 \u2212 \u03b1) = 2.14,\nwhile the value of c1/2 from (4.40) is 2.05. this close agreement is not surprising, as\n.= nt (\u00b5)2/(n \u2212 1), t (\u00b5)2 has the f1,n\u22121\ntaylor series expansion shows that wp(\u00b5)\n1 distribution when \u03bd2 \u2192 \u221e.\ndistribution, and the f1,\u03bd2 distribution approaches the \u03c7 2\nthe lower left panel of figure 4.7 shows z(\u00b5) = sign(y \u2212 \u00b5)wp(\u00b5)1/2 for the differ-\nences between cross- and self-fertilized plant heights in table 1.1, for which n = 15,\ny = 20.93, and s2 = 1424.6. the function z(\u00b5) differs only slightly from the straight\nline t(\u00b5) = (y \u2212 \u00b5)/(s2/n)1/2. the inner dotted lines at z\u03b1, z1\u2212\u03b1 = \u00b11.96 lead to the\nconfidence set (4.41), here (1.23, 40.63), shown by the inner vertical dashed lines.\nthis is only slightly narrower than the exact interval (0.03, 41.84) obtained by solving\nt(\u00b5) = \u00b1t14(0.025); this interval is shown by the outer dotted and dashed lines.\n\n "}, {"Page_number": 142, "text": "inference\n\nfigure 4.7\nfrom likelihood ratio\nstatistics. top left and\nright: profile log\nlikelihoods for the shape\nparameter of the weibull\nmodel for the springs\nfailure data, and for the\nprobability, \u03c8, of o-ring\n\u25e6\nthermal distress at 31\nf\nfor the challenger data.\nthe dashed vertical lines\nshow 95% confidence\nintervals based on the\napproximate distribution\nof the likelihood ratio\nstatistic, that is, the set of\n\u03c8 such that (cid:8)p(\u03c8) \u2265\n(cid:8)p((cid:22)\u03c8) \u2212 1\n2 c1(0.95), with\nthe horizontal dotted line\nat \u2212 1\nleft and right: signed\nlikelihood ratio statistics\nfor the maize data and the\nchallenger data\nprobability \u03c8. the solid\ncurves are z(\u00b5) and\nz(\u03c8), and the dotted\nhorizontal lines are at\nz\u03b1 , z1\u2212\u03b1 = \u00b11.96; the\ndashed vertical lines show\n95% confidence intervals.\nthe dashed diagonal line\nin the right panel shows\n(0.816 \u2212 \u03c8)/0.242 and\ncorresponds to using\n(cid:22)\u03c8 to set a confidence\napproximate normality of\n\n2 c1(0.95). bottom\n\ninterval. the dashed\ndiagonal line in the left\npanel shows the student t\nquantity t(\u00b5), with the\nouter dotted lines showing\n\u00b1t14(0.025), from which\nthe t confidence interval\nshown by the outer dashed\nlines is read off.\n\n130\n\nd\no\no\nh\n\ni\nl\n\ne\nk\n\ni\nl\n \n\ng\no\n\nl\n \n\ne\n\nl\ni\nf\n\no\nr\np\n\no\n\ni\nt\n\na\nr\n \n\nd\no\no\nh\n\ni\nl\n\ne\nk\n\ni\nl\n \n\nd\ne\nn\ng\ns\n\ni\n\n0\n\n1\n-\n\n2\n-\n\n3\n-\n\n4\n-\n\n5\n-\n\n3\n\n2\n\n1\n\n0\n\n1\n-\n\n2\n-\n\n3\n-\n\n4 \u00b7 likelihood\n\nd\no\no\nh\n\ni\nl\n\ne\nk\n\ni\nl\n \n\ng\no\n\nl\n \n\ne\n\nl\ni\nf\n\no\nr\np\n\n0\n\n1\n-\n\n2\n-\n\n3\n-\n\n4\n-\n\n5\n-\n\n2\n\n4\n\n6\n\n8\n\n10\n\n12\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\nalpha\n\npsi\n\no\n\ni\nt\n\na\nr\n \n\nd\no\no\nh\n\ni\nl\n\ne\nk\n\ni\nl\n \n\nd\ne\nn\ng\ns\n\ni\n\n3\n\n2\n\n1\n\n0\n\n1\n-\n\n2\n-\n\n3\n-\n\n-10\n\n0\n\n10 20 30 40 50\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\nmu\n\npsi\n\nin practice the exact interval would be used, but such results build confidence in\n(cid:1)\n\nuse of (4.40) and (4.41) when there is no exact interval.\n\nexample 4.32 (weibull distribution) for the data in example 4.4, we saw that the\ndifference of maximized likelihoods for the weibull and exponential models is roughly\n\n12.5, and so wp(\u03b10) = 2{(cid:8)((cid:22)\u03b8 ,(cid:22)\u03b1) \u2212 (cid:8)((cid:22)\u03b8\u03b10 , \u03b10)} .= 25. if \u03b10 = 1 was the true value for\n\u03b1, (4.39) implies that the distribution of wp(\u03b10) would be approximately \u03c7 2\n1 . however\nthe 0.95 and 0.99 quantiles of this distribution are respectively c1(0.95) = 3.84 and\nc1(0.99) = 6.635, and a value as large as 25 is very unlikely to arise by chance. thus\nthe weibull model fits the data appreciably better than the exponential one.\nis the set of \u03b1 such that (cid:8)p(\u03b1) \u2265 (cid:8)p((cid:22)\u03b1) \u2212 1\na 95% confidence region for the true value of \u03b1 based on the profile log likelihood\n\u00d7 3.84; we read this off from the top left\npanel of figure 4.7 and obtain (3.5, 9.2). as we would expect, this interval does not\ncontain \u03b1 = 1.\n(cid:1)\n\n2\n\nexample 4.33 (challenger data) examples 4.5, 4.8, and 4.27 concern likelihood\nanalysis of a binomial model for the data in table 1.3. our model is that at temperature\nx1 and pressure x2, the number of o-rings suffering thermal distress is binomial with\n\n "}, {"Page_number": 143, "text": "4.5 \u00b7 likelihood ratio statistic\ndenominator m = 6 and probability\n\n\u03c0(\u03b20, \u03b21, \u03b22) = exp(\u03b20 + \u03b21x1 + \u03b22x2)\n1 + exp(\u03b20 + \u03b21x1 + \u03b22x2)\n\n131\n\n.\n\napart from a constant, the corresponding log likelihood is\n\nn(cid:3)\nj=1\n\n\u03b20\n\nn(cid:3)\nj=1\n\nn(cid:3)\nj=1\n\nn(cid:3)\nj=1\n\nr j + \u03b21\n\nr j x1 j + \u03b22\n\nr j x2 j \u2212 m\n\nlog{1 + exp(\u03b20 + \u03b21x1 j + \u03b22x2 j )}.\n\nwe maximize this first as it is, then with \u03b22 held equal to zero, and then with both \u03b21\nand \u03b22 held equal to zero, and obtain \u221215.05, \u221215.82 and \u221218.90. to check whether\nthere is a pressure effect when temperature is included, we calculate the corresponding\nlikelihood ratio statistic, 2 \u00d7 {\u221215.05 \u2212 (\u221215.82)} =1.54. this is smaller than the\n1 distribution, c1(0.95) = 3.84, so any pressure effect is slight.\n0.95 quantile of the \u03c7 2\nassuming no pressure effect, the likelihood ratio statistic for no temperature effect\nis 2 \u00d7 {\u221215.82 \u2212 (\u221218.90)} =6.16, which we again compare to the \u03c7 2\n1 distribution.\n\u2265 6.16) = 0.013, so 6.16 is unlikely to occur by chance if the true value\nbut pr(\u03c7 2\n1\nof \u03b21 is zero: there seems to be a temperature effect.\nthe focus in this problem is the probability of thermal distress at temperature\nx1 = 31\n\u25e6\nf, and if there is an effect of temperature but not of pressure this probability\nis \u03c8 = \u03c0(\u03b20, \u03b21, 0), for which we would like confidence intervals. in example 4.28\nwe saw how to apply the delta method to (cid:22)\u03c8, but it gave the unsatisfactory 95%\n\nconfidence interval (0.34, 1.29).\n\nthe upper right panel of figure 4.7 shows the profile log likelihood (cid:8)p(\u03c8). a 95%\nconfidence interval based on this is (0.14, 0.99); unlike intervals based on normal\n\napproximation to(cid:22)\u03c8, this is guaranteed to be a subset of (0, 1). the panel below shows\ninterval based on the normal distribution of (cid:22)\u03c8 contains values outside the interval\n[0, 1]; an interval symmetric about (cid:22)\u03c8 is wholly inappropriate.\n\nthe signed likelihood ratio statistic, which is far from a straight line because the profile\nlog likelihood is far from quadratic in \u03c8. the dashed diagonal line shows how the\n\n(cid:1)\n\nin both the preceding examples the profile log likelihood is asymmetric. particularly\nin the second example, the profile log likelihood or equivalently wp(\u03c8) or z(\u03c8),\nprovide better confidence intervals than normal approximation to the distribution of\nthe maximum likelihood estimate.\n\n4.5.3 model fit\nso far we have supposed that the model is known apart from parameter values, but\nthis is rarely the case in practice and it is essential to check model fit. graphs play an\nimportant role in this, with variants of probability plots (section 2.1.4) particularly\nuseful. a more formal approach is to nest the model in a larger one, and then to assess\nwhether the expanded model fits the data appreciably better. if its log likelihood is\n(cid:8)(\u03c8, \u03bb) and the original model restricts \u03c8 to \u03c80, the two may be compared using a\nlikelihood ratio statistic. the usefulness of this approach depends on the expanded\n\n "}, {"Page_number": 144, "text": "132\n\n4 \u00b7 likelihood\n\nmodel: if it is uninteresting, so too will be the comparison. we have already seen an\napplication of this in example 4.33.\n\nexample 4.34 (generalized gamma distribution) a random variable y with the\ngeneralized gamma distribution has density function\n\nf (y; \u03bb, \u03b1, \u03ba) = \u03b1\u03bb\u03ba y\u03b1\u03ba\u22121\n\n\u0001(\u03ba)\n\nexp(\u2212\u03bby\u03b1),\n\ny > 0,\n\n\u03bb, \u03b1, \u03ba > 0.\n\n(4.42)\n\nthis arises on supposing that for some \u03b1, y \u03b1 has a gamma distribution, and reduces\nto the gamma density (2.7) when \u03b1 = 1, to the weibull density (4.4) with \u03b8 = \u03bb\u22121/\u03b1\nwhen \u03ba = 1, and to the exponential density when \u03b1 = \u03ba = 1; it is a flexible general-\nization of these models. in terms of our general discussion \u03c8 = \u03b1, with \u03c8 0 = 1, and\n\u03bb = (\u03ba, \u03bb)t.\nwhen applied to the data in table 2.1, the maximized log likelihoods are \u2212250.65\nfor the generalized gamma model, \u2212251.12 for the gamma model, and \u2212251.17 for\nthe weibull model. the likelihood ratio statistic for comparison of the gamma and\ngeneralized gamma densities is 2 \u00d7 {\u2212250.65 \u2212 (\u2212251.12)} =0.94, to be treated as\n\u03c7 2\n1 . there is no evidence that (4.42) fits better than the gamma density, which fits\n(cid:1)\nabout equally as well as the weibull density.\n\none useful approach in this context is a score test. suppose that \u03c8 and \u03bb have\ndimensions p \u00d7 1 and q \u00d7 1, and let i\u03bb\u03c8 = e(\u2212\u2202 2(cid:8)/\u2202\u03bb\u2202\u03c8 t), and so forth. the idea\nis that if the restricted model is adequate, then the maximized log likelihood (cid:8)(\u03c8 0,(cid:22)\u03bb\u03c8 0)\n(\u03c8 0,(cid:22)\u03bb\u03c8 0) should be modest. we show at the end of this section that\n\nwill not increase sharply in the \u03c8-direction, so its gradient \u2202(cid:8)(\u03c8, \u03bb)/\u2202\u03c8 evaluated at\n\n\u2202(cid:8)(\u03c8 0,(cid:22)\u03bb\u03c8 0)\n\n\u2202\u03c8\n\n(cid:15)\n\n.\u223c n p\n\n0, i\u03c8 \u03c8 \u2212 i\u03c8 \u03bb i\n\n\u22121\n\u03bb\u03bb i\u03bb\u03c8\n\n(cid:17)\n\n,\n\n(4.43)\n\nimplying that if the simpler model is adequate, then\n\ns = \u2202(cid:8)(\u03c8 0,(cid:22)\u03bb\u03c8 0)\n\n(cid:15)\n\n\u2202\u03c8 t\n\ni\u03c8 \u03c8 \u2212 i\u03c8 \u03bb i\n\n\u22121\n\u03bb\u03bb i\u03bb\u03c8\n\n(cid:17)\u22121 \u2202(cid:8)(\u03c8 0,(cid:22)\u03bb\u03c8 0)\n\n\u2202\u03c8\n\n.\u223c \u03c7 2\n\n,\n\np\n\nwhere s is evaluated at (\u03c8 0,(cid:22)\u03bb\u03c8 0). when p = 1 the signed square root of s should\n\nhave an approximate standard normal distribution. the statistic s is asymptotically\nequivalent to the likelihood ratio statistic wp(\u03c8 0), but is more convenient because it\ninvolves maximization only under the simpler model. expected information quantities\nmay be replaced by observed information quantities.\n\nexample 4.35 (spring failure data) we illustrate the score test by checking\nwhether \u03b1 = 1 for the spring failure data. in terms of our general discussion,\n\u03c8 = \u03b1, with \u03c8 0 = 1, and \u03bb = \u03b8. the score and observed information are given in\nexample 4.23. when \u03b1 = 1,(cid:22)\u03b8 = y = 168.3. at ((cid:22)\u03b8 , 1), we have \u2202(cid:8)(\u03b8, \u03b1)/\u2202\u03b1 = 9.64\n\u22121 = 0.097, so s takes value 8.99. compared to the \u03c7 2\nand (j\u03b1\u03b1 \u2212 j\u03b1\u03b8 j\ndistribution this gives strong evidence that \u03b1 (cid:10)= 1.\n1\n(cid:1)\n\n\u22121\n\u03b8 \u03b8 j\u03b8 \u03b1)\n\n "}, {"Page_number": 145, "text": "4.5 \u00b7 likelihood ratio statistic\n\n133\n\nchi-squared statistics\nsometimes it is useful to assess fit without a specific alternative in mind. one approach\nis to group the data and to use a chi-squared statistic.\n\nsuppose we have n independent observations that fall into categories 1, . . . ,k ,\nwith yi denoting the number of observations in category i. the probability that a\n\u03c0i = 1,\nsingle observation falls into this category is \u03c0i , where 0 < \u03c0i < 1 and\nbut as \u03c0k = 1 \u2212 \u03c01 \u2212 \u00b7\u00b7\u00b7 \u2212\u03c0 k\u22121, the parameter space is the interior of a simplex in\nk dimensions, that is, the set\n\n(cid:16)\nk\ni=1\n\n(cid:12)\n\n(\u03c01, . . . , \u03c0k) :\n\n\u03c0i = 1, 0 < \u03c01, . . . , \u03c0k < 1\n\n(4.44)\n\n(cid:11)\n\nk(cid:3)\ni=1\n\n(cid:16)\n\ni\n\nof dimension k \u2212 1. the model whose fit we wish to assess is that category i has\n\u03c0i (\u03bb) = 1 for each \u03bb and the parameter \u03bb has dimen-\nprobability \u03c0i (\u03bb), where\nsion p. this is multinomial with probabilities \u03c01, . . . , \u03c0k and denominator n; see\nexample 2.36. we suppose that there is a 1\u20131 mapping between \u03c0 = (\u03c01, . . . , \u03c0k\u22121)t\nand (\u03c8, \u03bb), and that setting \u03c8 = \u03c80 corresponds to the restricted model \u03c0(\u03bb) =\n(\u03c01(\u03bb), . . . , \u03c0k\u22121(\u03bb))t. thus our model of interest restricts \u03c0 to a p-dimensional\nsubset of (4.44), where p < k \u2212 1, and is nested within the full multinomial model\nwith k \u2212 1 parameters.\n\ngiven data y1, . . . , yk, the likelihood under the general model is\n\nk(cid:3)\ni=1\n\n\u03c0i = 1, 0 < \u03c01, . . . , \u03c0k < 1,\n\nn!\n\n\u03c0 y1\n1\n\n\u00d7 \u00b7\u00b7\u00b7 \u00d7\u03c0 yk\n\ny1!\u00b7\u00b7\u00b7 yk!\n\nl(\u03c0) =\n(cid:16)\ni yi = n, sothe log likelihood is\n\nk\n\n,\n\nwhere\n\n(cid:8)(\u03c0) \u2261 k\u22121(cid:3)\n\ni=1\n\nyi log \u03c0i + yk log(1 \u2212 \u03c01 \u2212 \u00b7\u00b7\u00b7 \u2212\u03c0 k\u22121),\n\n(4.45)\n\nresulting in score vector and observed information matrix with components\n\n\u2202(cid:8)(\u03c0)\n\u2202\u03c0i\n\u2212 \u2202 2(cid:8)(\u03c0)\n\u2202\u03c0i d\u03c0 j\n\n= yi\n(cid:11)\n\u03c0i\n\n=\n\nyk\n\n\u2212\n1 \u2212 \u03c01 \u2212 \u00b7\u00b7\u00b7 \u2212\u03c0 k\u22121\n+\n(1\u2212\u03c01\u2212\u00b7\u00b7\u00b7\u2212\u03c0k\u22121)2 ,\nyk\n\nyk\n\nyi\n\u03c0 2\ni\n\n(1\u2212\u03c01\u2212\u00b7\u00b7\u00b7\u2212\u03c0k\u22121)2 ,\n\ni = j,\ni (cid:10)= j,\n\n,\n\n(4.46)\n\nwhere i and j run over 1, . . . ,k \u2212 1. manipulation of the likelihood equations shows\nthat the maximum likelihood estimators are(cid:22)\u03c0i = yi /n (exercise 4.5.4). the expected\n\ninformation matrix involves e(yi ), which may be calculated by noting that if we regard\nan observation in category i as a \u2018success\u2019, yi is the number of successes out of n\nindependent trials, so its marginal distribution is binomial with denominator n and\nprobability \u03c0i and mean n\u03c0i ; see example 2.36. the expected information is the\n\n "}, {"Page_number": 146, "text": "1/\u03c0k\n...\n1/\u03c0k\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\n134\n(k \u2212 1) \u00d7 (k \u2212 1) matrix\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\ni (\u03c0) = n\n\n1/\u03c01 + 1/\u03c0k\n\n4 \u00b7 likelihood\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8 ,\n\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n...\n\u00b7\u00b7\u00b7 1/\u03c0k\u22121 + 1/\u03c0k\n\n1/\u03c0k\n1/\u03c0k\n...\n\n(4.47)\n\n1/\u03c0k\n\n1/\u03c02 + 1/\u03c0k\n\n...\n1/\u03c0k\n\nand it is straightforward to verify that its inverse is\n\n\u22121 = n\n\ni (\u03c0)\n\n\u22121\n\n\u03c01(1 \u2212 \u03c01)\n\u2212\u03c02\u03c01\n\n\u2212\u03c01\u03c02\n\u03c02(1 \u2212 \u03c02)\n\n...\n\n\u2212\u03c0k\u22121\u03c01\n\n\u2212\u03c0k\u22121\u03c02\n\n\u2212\u03c01\u03c0k\u22121\n\u2212\u03c02\u03c0k\u22121\n\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n...\n\u00b7\u00b7\u00b7 \u03c0k\u22121(1 \u2212 \u03c0k\u22121)\n\n...\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8 ;\n\nthis is unsurprising, because(cid:22)\u03c0i = yi /n. provided none of the \u03c0i equals zero or one,\nn \u2192 \u221e, and in particular(cid:22)\u03c0 has a limiting normal distribution.\n\nthe usual large-sample properties of maximum likelihood estimates are satisfied as\n\nwe now return to the restricted model, whose log likelihood is\n\n(cid:8)(\u03bb) = (cid:8){\u03c0(\u03bb)} \u2261 k\u22121(cid:3)\n\ni=1\n\nyi log \u03c0i (\u03bb) + yk log{1 \u2212 \u03c01(\u03bb) \u2212 \u00b7\u00b7\u00b7 \u2212 \u03c0k\u22121(\u03bb)} ,\n\nmaximization of which gives the maximum likelihood estimator (cid:22)\u03bb. the first and\n\nsecond derivatives of (cid:8)(\u03bb) are\n\n= k\u22121(cid:3)\n= k\u22121(cid:3)\n\ni=1\n\ni=1\n\n\u2202(cid:8)(\u03bb)\n\u2202\u03bbr\n\u2202 2(cid:8)(\u03bb)\n\u2202\u03bbr \u2202\u03bbs\n\n\u2202(cid:8)(\u03c0)\n\u2202\u03c0i\n\n,\n\n\u2202\u03c0i\n\u2202\u03bbr\n\u2202 2\u03c0i\n\u2202\u03bbr \u2202\u03bbs\n\n\u2202(cid:8)(\u03c0)\n\u2202\u03c0i\n\n+ k\u22121(cid:3)\n\ni=1\n\nk\u22121(cid:3)\nj=1\n\n\u2202\u03c0i\n\u2202\u03bbr\n\n\u2202\u03c0 j\n\u2202\u03bbs\n\n\u2202 2(cid:8)(\u03c0)\n\u2202\u03c0i \u2202\u03c0 j\n\n,\n\nand as e{\u2202(cid:8)(\u03c0)/\u2202\u03c0i} =0, the expected information for \u03bb is the p \u00d7 p matrix\n\n(cid:13)\n\n(cid:14)\n\ni (\u03bb) = \u2202\u03c0 t\n\n\u2202\u03bb\n\ne\n\n\u2212 \u2202 2(cid:8)(\u03c0)\n\u2202\u03c0 \u2202\u03c0 t\n\n\u2202\u03c0\n\n\u2202\u03bbt\n\n= \u2202\u03c0 t\n\u2202\u03bb\n\ni (\u03c0)\n\n\u2202\u03c0\n\n\u2202\u03bbt\n\n,\n\nwhere \u2202\u03c0 t/\u2202\u03bb is the p \u00d7 (k \u2212 1) matrix of partial derivatives of the \u03c0i with respect\nto the \u03bbr , and i (\u03c0) is given by (4.47); see problem 4.2. thus provided \u2202\u03c0 t/\u2202\u03bb (cid:10)= 0,\nthe parameter \u03bb has a large-sample normal distribution under the restricted model,\nand the general results in section 4.5.2 imply that the likelihood ratio statistic used\nto compare the two models satisfies\n\nw = 2\n\nyi log\n\nk(cid:3)\ni=1\n\n(cid:14)\n\n(cid:13) (cid:22)\u03c0i\n\u03c0i ((cid:22)\u03bb)\n\n= 2\n\nk(cid:3)\ni=1\n\n(cid:13)\n\n(cid:16)\n\nyi log\n\n(cid:14)\n\nif the simpler model is true. we may write w = 2\nand ei = n\u03c0i ((cid:22)\u03bb) are the ith observed and expected values under the fitted model;\n\nyi\n\nk\u22121\u2212 p\n\n.\u223c \u03c7 2\n\nn\u03c0i ((cid:22)\u03bb)\noi log(oi /ei ), where oi = yi we take 0 log 0 =\nlimy\u21930 y log y = 0.\n\n "}, {"Page_number": 147, "text": "karl pearson (1857\u20131936)\nwas aleader of the\nenglish biometrical\nschool, which applied\nstatistical ideas to heredity\nand evolution. his energy\nwas astonishing: he\npractised law and wrote\nbooks on history and\nreligion as well as the\nclassic \u2018the grammar of\nscience\u2019 and over 500\nother publications. he\ncoined the terms \u2018standard\ndeviation\u2019, \u2018histogram\u2019\nand \u2018mode\u2019. he invented\nthe correlation coefficient\nand also the chi-square\ntest. he feuded with\nfisher, who pointed out\nthat pearson gave p too\nmany degrees of freedom.\nthe statistic p is\nsometimes denoted x 2 or\n\u03c7 2.\n\n4.5 \u00b7 likelihood ratio statistic\n(cid:16)\n\u03c0i ((cid:22)\u03bb) = 1, it is true that\n.= (cid:16)\n(oi \u2212 ei )2/ei (exercise 4.5.5), leading to pearson\u2019s statistic,\n\n(cid:16)\n\nas\nw\n\noi = n. taylor series expansion shows that\n{yi \u2212 n\u03c0i ((cid:22)\u03bb)}2\n\n135\n\nei = (cid:16)\np = k(cid:3)\n\ni=1\n\nn\u03c0i ((cid:22)\u03bb)\n\n;\n\nthis too has an approximate \u03c7 2\n\nk\u22121\u2212 p distribution if the simpler model is true.\n\nboth w and p provide checks on the adequacy of the restricted multinomial\ncompared to the most general multinomial possible, which requires only that the\nprobabilities sum to one. the approximate distributions of w and p apply when\nthere are large counts, and experience suggests that the chi-squared approximations\nare more accurate if most of the fitted values exceed five. though asymptotically\nequivalent to w , p behaves better in small samples because it does not involve\nlogarithms.\n\nexample 4.36 (birth data) figure 2.2 shows the poisson density with mean(cid:22)\u03b8 =\n12.9 fitted to the numbers of daily arrivals for the delivery suite data. how good is\nthe fit? here p = 1 parameters are estimated under the poisson model. with the n =\n92 daily counts split among the k = 13 categories [0, 7.5), [7.5, 8.5), . . . ,[18.5, \u221e),\nthe values for o and e are\n\no 6\ne\n\n5.23\n\n3\n4.37\n\n3\n6.26\n\n8\n8.08\n\n13\n9.48\n\n10\n10.19\n\n11\n10.11\n\n11\n9.32\n\n8\n8.01\n\n6\n6.46\n\n4\n4.91\n\n4\n3.52\n\n5\n6.07\n\nand p takes value 4.39, to be treated as a \u03c7 2\npoisson model fits very well, perhaps surprisingly so.\n\n.= 0.96, the\na minor problem here is that(cid:22)\u03b8 is obtained from the original data rather than from\n\n11 variable. as pr(\u03c7 2\n11\n\n\u2265 4.39)\n\nthe data grouped into the k categories. however the maximum likelihood estimate\nfrom the grouped data is 12.89, so the fit is hardly affected at all. use of the parameter\nestimate from the ungrouped data increases the degrees of freedom for the test, because\nslightly fewer than p degrees of freedom must be subtracted from the k \u2212 1. the\n(cid:1)\nestimates will usually be similar unless the grouping is very coarse.\n\nexample 4.37 (two-way contingency table) suppose that each of n individuals\nchosen at random from a population is classified according to two sets of categories.\nthe first corresponds to the r rows of the table, and the second to the c columns;\nthere are k = rc cells indexed by (i, j), i = 1, . . . , r, j = 1, . . . ,c . such a setup is\nknown as an r \u00d7 c contingency table or two-way contingency table. the top part of\ntable 4.3 shows an example in which 422 people have been cross-classified according\nto presence or absence of the antigens \u2018a\u2019 and \u2018b\u2019 in their blood. there are 202 people\nwithout either antigen, 179 with antigen \u2018a\u2019 but not \u2018b\u2019, and so forth. this is the\n(cid:16)\nsimplest cross-classification, a 2 \u00d7 2 table.\ni, j yi j = n. if the\nindividuals are independently chosen at random from a population in which the pro-\nportion in cell (i, j) is\u03c0 i j , the joint density of the cell counts yi j is multinomial with\n\nsuppose that there are yi j individuals in the (i, j) cell, so\n\n "}, {"Page_number": 148, "text": "136\n\n4 \u00b7 likelihood\n\nantigen \u2018b\u2019\n\nabsent\n\npresent\n\ntotal\n\nantigen \u2018a\u2019\n\nabsent\npresent\n\n\u2018o\u2019: 202\n\u2018a\u2019: 179\n\n\u2018b\u2019: 35\n\u2018ab\u2019: 6\n\ntotal\n\n381\n\n41\n\n237\n185\n\n422\n\ntwo-locus model\n\none-locus model\n\ngroup\n\ngenotype\n\nprobability\n\ngenotype\n\n\u2018a\u2019\n\u2018b\u2019\n\u2018ab\u2019\n\n\u2018o\u2019\n\n(a a; bb), ( aa; bb)\n(aa; b b), (aa; bb)\n(a a; b b), (aa; b b),\n(a a; bb), (aa; bb)\n\n(aa; bb)\n\n\u03b1(1 \u2212 \u03b2)\n(1 \u2212 \u03b1)\u03b2\n\n\u03b1\u03b2\n\n(1 \u2212 \u03b1)(1 \u2212 \u03b2)\n\n(a a), (ao)\n(b b), (b o)\n\n(ab)\n\n(o o)\n\nprobability\n+ 2\u03bba\u03bbo\n+ 2\u03bbb \u03bbo\n2\u03bba\u03bbb\n\n\u03bb2\na\n\u03bb2\nb\n\n\u03bb2\no\n\ntable 4.3 blood groups\nin england (taylor and\nprior, 1938). the upper\npart of the table shows a\ncross-classification of 422\npersons by presence or\nabsence of antigens \u2018a\u2019\nand \u2018b\u2019, giving the groups\n\u2018a\u2019, \u2018b\u2019, \u2018ab\u2019, \u2018o\u2019 of the\nhuman blood group\nsystem. the lower part\nshows genotypes and\ncorresponding\nprobabilities under one-\nand two-locus models. see\nexample 4.38 for details.\n\ndenominator n and probabilities \u03c0i j , that is,\n\u00b7\u00b7\u00b7\u03c0 yrc\n\nn!\n\n,\n\n\u03c0 y11\n11\n\n\u03c0 y12\n12\n\nrc\n\nyi j = 0, . . . ,n ,\n\n(cid:3)\n\ni, j\n\nyi j = n,\n\ny11!y12!\u00b7\u00b7\u00b7 yrc!\n(cid:16)\n(cid:3)\n\ni, j\n\nwhere 0 < \u03c0i j < 1 and\n(cid:8)(\u03c0) \u2261\n\ni, j\n\n\u03c0i j = 1. the log likelihood is\n(cid:3)\n\nyi j log \u03c0i j ,\n\n0 < \u03c0i j < 1,\n\n\u03c0i j = 1;\n\ni, j\n\nthere are rc \u2212 1 parameters because of the constraint that the probabilities sum to one.\ncell (i, j) isthe sample proportion in that cell, that is, (cid:22)\u03c0i j = yi j /n, sothe maximized\nthe preceding general results imply that estimated proportion of the population in\n\nlog likelihood is\n\n(cid:16)\ni, j yi j log(yi j /n).\n\ni\n\n(cid:16)\n\noften the question arises whether the row and column classifications are indepen-\n\u03b1i = (cid:16)\ndent. if so, and if the proportion of the population in row category i is \u03b1i , and that\n(cid:16)\nin column category j is \u03b2 j , then \u03c0i j = \u03b1i \u03b2 j . as\n\u03b2 j = 1, this model has\np = (r \u2212 1) + (c \u2212 1) parameters. the log likelihood is\nj\ni, j yi j log(\u03b1i \u03b2 j ), and to\nmaximize it subject to the constraints on the \u03b1i and \u03b2 j we use lagrange multipliers\n(cid:4)\n\u03b6 and \u03b7 and seek extremal points of\n\u03b2 j \u2212 1\nj yi j and y\u00b7 j = (cid:16)\n\n\u03b1i \u2212 1\nwe find that(cid:22)\u03b1i = yi\u00b7/n and(cid:22)\u03b2 j = y\u00b7 j /n, where yi\u00b7 = (cid:16)\ncategories. the fitted value in cell (i, j) isn(cid:22)\u03b1i\n\ni yi j ; these\nare respectively the observed proportions of observations in the ith row and jth column\n\n(cid:22)\u03b2 j = yi\u00b7 y\u00b7 j /n, and the maximized log\n\nyi j log(\u03b1i \u03b2 j ) + \u03b6\n\n(\u03b1, \u03b2, \u03b6, \u03b7) =\n\n(cid:2)(cid:3)\n\n(cid:2)(cid:3)\n\n(cid:3)\n\n+ \u03b7\n\n(cid:4)\n\n(cid:8)\u2217\n\ni, j\n\n.\n\ni\n\nj\n\n(cid:16)\ni, j yi j log((cid:22)\u03b1i\n\n(cid:22)\u03b2 j ).\n\nlikelihood is\n\n "}, {"Page_number": 149, "text": "4.5 \u00b7 likelihood ratio statistic\n\n137\n\nthe likelihood ratio statistic for comparing the independence model with the more\n\ngeneral model is\n\n(cid:3)\n\n(cid:7)\n\nw = 2\n\nyi j log\n\ni, j\n\n(cid:5)\n\n(cid:6)\n\nyi j\nn\n\n(cid:5)\n\n\u2212 yi j log\n\nyi\u00b7 y\u00b7 j\nn2\n\n(cid:20)\n\n(cid:6)(cid:8)\n\n(cid:3)\n\ni, j\n\n= 2\n\nyi j log\n\nnyi j\nyi\u00b7 y\u00b7 j\n\n(cid:21)\n\n,\n\nand when the independence model is true, the approximate distribution of w is \u03c7 2\nhere k \u2212 1 \u2212 p = rc \u2212 1 \u2212 {(r \u2212 1) + (c \u2212 1)} =( r \u2212 1)(c \u2212 1).\n\nk\u22121\u2212 p;\n\nin this case pearson\u2019s statistic may be expressed as\n(yi j \u2212 yi\u00b7 y\u00b7 j /n)2\n\n(cid:3)\n\np =\n\nyi\u00b7 y\u00b7 j /n\n\ni, j\n\n,\n\nwith an approximate \u03c7 2\n\n(r\u22121)(c\u22121) distribution when the categorizations are independent.\n(cid:1)\n\nexample 4.38 (abo blood group system) the most important classification of\nhuman blood types is into the four groups \u2018a\u2019, \u2018b\u2019, \u2018ab\u2019, and \u2018o\u2019, corresponding to\npresence or absence of the antigens \u2018a\u2019 and \u2018b\u2019; \u2018ab\u2019 refers to the presence of both\nand \u2018o\u2019 to their absence. in a set of data shown in table 4.3, the frequencies of these\ngroups were 179, 35, 6, and 202.\n\naccording to a model thought credible until the 1920s, the blood group of a person\nis controlled by two loci (1; 2) on a pair of chromosomes, one chromosome being\ninherited from each parent. at the loci they independently inherit alleles (x1; y1)\nfrom their mother and (x2; y2) from their father, where x1 and x2 are one of a or\na, and y1 and y2 are one of b or b. thus their genotype (x1x2; y1 y2) is anyone of\n(aa; bb), . . . ,( a a, b b), and they have the antigen \u2018a\u2019 only if allele a is present;\nsimilarly for antigen \u2018b\u2019. in fact (aa; bb) isindistinguishable from ( a a; bb) and so\nforth, so under this model there are nine genotypes shown in the second column of\nthe lower part of table 4.3. since the loci are independent, the probabilities that a\nperson randomly taken from the population will have blood groups \u2018a\u2019, \u2018b\u2019, \u2018ab\u2019 and\n\u2018o\u2019 may be written as \u03b1(1 \u2212 \u03b2), (1 \u2212 \u03b1)\u03b2, \u03b1\u03b2, and (1 \u2212 \u03b1)(1 \u2212 \u03b2), where \u03b1 and \u03b2\nare the probabilities that they have antigens \u2018a\u2019 and \u2018b\u2019.\n\nan alternative model posits a single locus at which three alleles, a, b, and o\nmay appear, a and b conferring the respective antigens, and o conferring nothing.\nif \u03bba, \u03bbb and \u03bbo denote the probabilities that a parent has the three alleles on one\nchromosome, and if the population is in equilibrium, then the probabilities that the\nchild has blood types \u2018a\u2019, \u2018b\u2019, \u2018ab\u2019 and \u2018o\u2019 are\n\n\u03c0a = \u03bb2\n\na\n\n+ 2\u03bba\u03bbo , \u03c0b = \u03bb2\n\nb\n\n+ 2\u03bbb \u03bbo , \u03c0ab = 2\u03bba\u03bbb , \u03c0o = \u03bb2\n\n.\n\no\n\nwhere \u03bbo = 1 \u2212 \u03bba \u2212 \u03bbb.\nestimates of \u03b1 and \u03b2 are the corresponding sample proportions,(cid:22)\u03b1 = 185/422 = 0.438\nunder the two-locus model, example 4.37 implies that the maximum likelihood\nand(cid:22)\u03b2 = 41/422 = 0.097. the fitted values, 213.97, 167.03, 23.02, 17.97, are rather\nfar from 202, 179, 35, 6. the values for w and p are 17.66 and 15.73, to be treated\nk\u22121\u2212 p if the two-locus model is adequate; here k \u2212 1 \u2212 p = 4 \u2212 1 \u2212 2 = 1. as\nas \u03c7 2\nc1(0.95) = 3.84, the fit is poor.\n\n "}, {"Page_number": 150, "text": "this may be skipped at a\nfirst reading.\n\n(cid:15)\n\n138\n\nunder the single-locus model, the log likelihood is\n\n4 \u00b7 likelihood\n(cid:17)\nwhere \u03bbo = 1 \u2212 \u03bba \u2212 \u03bbb, and maximization in terms of (log \u03bba, log \u03bbb) gives(cid:22)\u03bba =\n0.252,(cid:22)\u03bbb = 0.050. the fitted values for the blood groups are 205.85, 174.99, 30.54,\n\n(cid:17) + 6 log(2\u03bba\u03bbb) + 202 log\n\n(cid:17) + 35 log\n\n+ 2\u03bbb \u03bbo\n\n+ 2\u03bba\u03bbo\n\n179 log\n\n\u03bb2\no\n\n\u03bb2\na\n\n\u03bb2\nb\n\n(cid:15)\n\n(cid:15)\n\n,\n\nand 10.62, and the values of w and p are 3.17 and 2.82. the single-locus model is\n(cid:1)\nmuch better supported by the data.\n\n(cid:21)\n\n(cid:20)\n\n(cid:21)\n\n(cid:20)\n\n(cid:20)\n\nu\u03c8\nu\u03bb\n\nj\u03c8 \u03bb\nj\u03bb\u03bb\n\nj\u03c8 \u03c8\nj\u03bb\u03c8\n\n, i (\u03c8 0, \u03bb0) =\n\n, j (\u03c8 0, \u03bb0) =\n\nderivations of (4.39) and (4.43)\nin the regular case when the model is correct and the true values of the p \u00d7 1 and\nq \u00d7 1 vectors \u03c8 and \u03bb are \u03c8 0 and \u03bb0, wedenote the score vector and observed and\n(cid:21)\nexpected information matrices by\nu (\u03c8 0, \u03bb0) =\nwhere, for example, u\u03bb is the q \u00d7 1 vector \u2202(cid:8)/\u2202\u03bb, j\u03bb\u03c8 is the q \u00d7 p matrix\n\u2212\u2202 2(cid:8)/\u2202\u03bb\u2202\u03c8 t, and and i\u03bb\u03c8 = e(\u2212\u2202 2(cid:8)/\u2202\u03bb\u2202\u03c8 t), evaluated at (\u03c8 0, \u03bb0). the compo-\nto establish (4.43), we expand the likelihood equations u ((cid:22)\u03c8 ,(cid:22)\u03bb) = 0 and\nnents of u are o p(n1/2), those of j are o p(n), and those of i are o(n).\n\u2202(cid:8)(\u03c8 0,(cid:22)\u03bb\u03c8 0)/\u2202\u03bb = 0 about (\u03c8 0, \u03bb0), giving\n(cid:21)\n(cid:20)(cid:22)\u03c8 \u2212 \u03c8 0\n(cid:22)\u03bb \u2212 \u03bb0\n(cid:21)\n(cid:20)(cid:22)\u03c8 \u2212 \u03c8 0\n(cid:22)\u03bb \u2212 \u03bb0\n(cid:15)\nu\u03bb = j\u03bb\u03bb((cid:22)\u03bb\u03c8 0 \u2212 \u03bb0) + o p\n\n= j (\u03c8 0, \u03bb0)\n\n= i (\u03c8 0, \u03bb0)\n\ni\u03c8 \u03c8\ni\u03bb\u03c8\n\ni\u03c8 \u03bb\ni\u03bb\u03bb\n\nu\u03c8\nu\u03bb\n\nn1/2\n\n(cid:20)\n\n(cid:21)\n\n(cid:15)\n\n(cid:17)\n\n,\n\n.\n\n,\n\nn1/2\n\n(cid:17)\nn1/2\n(cid:17)\n\n(cid:15)\n+ o p\n(cid:15)\n+ o p\n(cid:17) = i\u03bb\u03bb((cid:22)\u03bb\u03c8 0 \u2212 \u03bb0) + o p\n\u03bb\u03bb i\u03bb\u03c8 ((cid:22)\u03c8 \u2212 \u03c8 0) + o p\n(cid:17) = u\u03c8 \u2212 i\u03c8 \u03bb i\n\n\u22121/2\n\n\u22121\n\nn\n\n(cid:15)\n\nn1/2\n(cid:15)\n\n\u03bb\u03bb u\u03bb + o p\n\u22121\n\n(cid:17)\n\n.\n\n\u22121/2\n\nn\n\n(cid:15)\n\n(cid:17) =(cid:22)\u03bb \u2212 \u03bb0 + i\n\n\u22121/2\n\nn\n\n\u03bb\u03bb u\u03bb + o p\n\u22121\ntaylor series expansion gives\n\nthus(cid:22)\u03bb\u03c8 0 \u2212 \u03bb0 = i\n\u2202(cid:8)(\u03c8 0,(cid:22)\u03bb\u03c8 0)\n\n\u2202\u03c8\n\n= u\u03c8 \u2212 i\u03c8 \u03bb((cid:22)\u03bb\u03c8 0 \u2212 \u03bb0) + o p\n\nand the joint limiting normal distribution\n\n(cid:20)\n\n(cid:21)\n\nu\u03c8\nu\u03bb\n\n.\u223c n p+q{0, i (\u03c8 0, \u03bb0)}\n\nimplies that\n\nso\n\n\u2202(cid:8)(\u03c8 0,(cid:22)\u03bb\u03c8 0)\n\n\u2202\u03c8\n\n(cid:15)\n\n.\u223c n p\n\n0, i\u03c8 \u03c8 \u2212 i\u03c8 \u03bb i\n\n\u22121\n\u03bb\u03bb i\u03bb\u03c8\n\n(cid:17)\n\n,\n\n\u2202(cid:8)(\u03c8 0,(cid:22)\u03bb\u03c8 0)\n\n(cid:15)\n\n\u2202\u03c8 t\n\ni\u03c8 \u03c8 \u2212 i\u03c8 \u03bb i\n\n\u22121\n\u03bb\u03bb i\u03bb\u03c8\n\n(cid:17)\u22121 \u2202(cid:8)(\u03c8 0,(cid:22)\u03bb\u03c8 0)\n\n\u2202\u03c8\n\n.\u223c \u03c7 2\n\np\n\n.\n\n(cid:15)\n\n(cid:17)\n\n,\n\n\u22121/2\n\nn\n\n(4.48)\n\n "}, {"Page_number": 151, "text": "4.5 \u00b7 likelihood ratio statistic\n\n139\n\nto establish (4.39), we write the likelihood ratio statistic (4.38) as\n\nwp(\u03c8 0) = 2{(cid:8)((cid:22)\u03c8 ,(cid:22)\u03bb) \u2212 (cid:8)(\u03c8 0, \u03bb0)} \u22122{(cid:8)(\u03c8 0,(cid:22)\u03bb\u03c8 0) \u2212 (cid:8)(\u03c8 0, \u03bb0)},\n\nabout (\u03c8 0, \u03bb0). the results above imply that wp(\u03c8 0) isapproximately\n(cid:20)\n\nand then replace (cid:8)((cid:22)\u03c8 ,(cid:22)\u03bb) and (cid:8)(\u03c8 0,(cid:22)\u03bb\u03c8 0) with second-order taylor series expansions\n(cid:20)(cid:22)\u03c8 \u2212 \u03c8 0\n(cid:22)\u03bb \u2212 \u03bb0\nand replacement of(cid:22)\u03bb\u03c8 0 with its expression in terms of ((cid:22)\u03c8 ,(cid:22)\u03bb) gives\n((cid:22)\u03c8 \u2212 \u03c8 0) + o p(1).\n\n(cid:20)(cid:22)\u03c8 \u2212 \u03c8 0\n(cid:22)\u03bb \u2212 \u03bb0\n(cid:15)\n\n0(cid:22)\u03bb\u03c8 0 \u2212 \u03bb0\n(cid:17)\n\n0(cid:22)\u03bb\u03c8 0 \u2212 \u03bb0\n\ni\u03c8 \u03c8 \u2212 i\u03c8 \u03bb i\n\ni (\u03c8 0, \u03bb0)\n\ni (\u03c8 0, \u03bb0)\n\n\u22121\n\u03bb\u03bb i\u03bb\u03c8\n\nwp(\u03c8 0)\n\n(4.49)\n\n(cid:21)\n\n(cid:20)\n\n(cid:21)\n\n(cid:21)\n\n(cid:21)\n\n\u2212\n\n,\n\nt\n\nt\n\nbut as our previous asymptotics for the maximum likelihood estimators under the\nfull model give\n\n(cid:13)(cid:20)\n\n(cid:21)\n\n\u03c8 0\n\u03bb0\n\n.\u223c n p+q\n\n(cid:14)\n\n, i (\u03c8 0, \u03bb0)\n\n\u22121\n\n,\n\n(4.50)\n\n\u22121\n\u03bb\u03bb i\u03bb\u03c8 )\n\n\u22121, and (4.49) and (3.23)\n\nthe asymptotic covariance matrix of(cid:22)\u03c8 is (i\u03c8 \u03c8 \u2212 i\u03c8 \u03bb i\nwp(\u03c8 0) = 2{(cid:8)((cid:22)\u03c8 ,(cid:22)\u03bb) \u2212 (cid:8)(\u03c8 0,(cid:22)\u03bb\u03c8 0)}\n\ngive\n\n.\u223c \u03c7 2\np :\n\n.= ((cid:22)\u03c8 \u2212 \u03c80)t\n(cid:20)(cid:22)\u03c8(cid:22)\u03bb\n\n(cid:21)\n\nthe asymptotic distribution of the likelihood ratio statistic for comparison of two nested\nmodels is chi-squared with degrees of freedom equal to the number of parameters that\nare restricted by the less general model. this result applies only to nested models,\n\nand the expansions leading to it are valid only when ((cid:22)\u03bb,(cid:22)\u03c8) converges to (\u03c8 0, \u03bb0).\n\nthis may need checking in applications.\n\nexercises 4.5\n1\n\n4 (1 \u2212 \u03b8), 1\n\n4 on the values 1, 2, 3, 4, where\u2212 1\n\nif y1, . . . ,y n is a random sample from the n (\u00b5, \u03c3 2) distribution with known \u03c3 2, show\nthat the likelihood ratio statistic for comparing \u00b5 = \u00b50 with general \u00b5 is w (\u00b50) =\nn(y \u2212 \u00b5)2/\u03c3 2. show that w (\u00b50) is apivot, and give the likelihood ratio confidence\nregion for \u00b5.\n4 (1 + 2\u03b8),\nindependent values y1, . . . , yn arise from a distribution putting probabilities 1\n4 (1 \u2212 \u03b8), 1\n1\n< \u03b8 <1. show that the likelihood\nfor \u03b8 is proportional to (1 + 2\u03b8)m1(1 \u2212 \u03b8)m2 and express m1 and m2 in terms of y1, . . . , yn.\nfind the maximum likelihood estimate of \u03b8 in terms of m1 and m2.\nobtain the maximum likelihood estimate and the likelihood ratio statistic for \u03b8 = 0 based\non data in which the frequencies of 1, 2, 3, 4 were 55, 11, 8, 26. is it plausible that \u03b8 = 0?\nconsider examples 4.27 and 4.33. show that the standard error for \u03b7 = \u03b20 + 31\u03b21 is\n(9.289 \u2212 2 \u00d7 31 \u00d7 0.142 + 312 \u00d7 0.00220)1/2, and hence obtain a 95% confidence inter-\nval for \u03b7. use this to construct an interval for \u03c6 = e\u03b7/(1 + e\u03b7), and compare it with the\ninterval based on the profile log likelihood for \u03c6.\nuse (4.46) to show that(cid:22)\u03c0 j = y j /n, and verify the contents of the corresponding observed,\n\n2\n\nexpected, and inverse expected information matrices.\n\n2\n\n3\n\n4\n\n "}, {"Page_number": 152, "text": "140\n\n5\n\n6\n\n7\n\n4 \u00b7 likelihood\n2 (o \u2212 e)2/e + \u00b7\u00b7\u00b7 is valid\nverify that the taylor expansion o log(o/e)\nfor small o \u2212 e, and hence check that provided oi \u2212 ei is small relative to ei , pearson\u2019s\nstatistic p is close to the likelihood ratio statistic w .\nlet y1, . . . ,y n and z1, . . . , zm be two independent random samples from the n (\u00b51, \u03c3 2\n1 )\nand n (\u00b52, \u03c3 2\n2 ) distributions respectively. consider comparison of the model in which\n\u03c3 2\n2 and the model in which no restriction is placed on the variances, with no restriction\n1\non the means in either case. show that the likelihood ratio statistic wp to compare these\n(z j \u2212 z)2 is large or small, and that\n\n= \u03c3 2\nmodels is large when the ratio t = (cid:16)\n\n.= o \u2212 e + 1\n\n(y j \u2212 y )2/\n\nt is proportional to a random variable with the f distribution.\nin an experiment to assess the effectiveness of a treatment to reduce blood pressure in heart\npatients, n independent pairs of heart patients are matched according to their sex, weight,\nsmoking history, initial blood pressure, and so forth. then one of each pair is selected at\nrandom and given the treatment. after a set time the blood pressures are again recorded,\nand it is desired to assess whether the treatment had any effect. a simple model for this\nis that the jth pair of final measurements, (y j1, y j2) is two independent normal variables\nwith means \u00b5 j and \u00b5 j + \u03b2, and variances \u03c3 2. it isdesired to assess whether \u03b2 = 0 ornot.\none approach is a t confidence interval based on z j = y j2 \u2212 y j1. explain this, and give\nthe degrees of freedom for the t statistic. show that the likelihood ratio statistic for \u03b2 = 0\nis equivalent to z 2/\n\n(z j \u2212 z)2.\n\n(cid:16)\n\n(cid:16)\n\n4.6 non-regular models\n\nthe large-sample normal and chi-squared approximations (4.26) and (4.39) apply to\nmany important models. there are exceptions, however, due to failure of regularity\nconditions for the parameter space, the likelihood and its derivatives, and convergence\nof information quantities. a model can be non-regular in many ways, and rather\nthan attempt a general discussion we give some examples intended to flag possible\nproblems.\n\nparameter space\nif standard asymptotics are to apply, the true parameter value must be interior to the\nparameter space \u0001. one way to ensure this is to insist that \u0001 be an open subset of\nir p endowed with its usual topology. if not, and if the true \u03b8 0 lies on the edge of\nthe parameter space, then the maximum likelihood estimator cannot fall on \u2018both\nsides\u2019 of \u03b8 0, and therefore cannot have a limiting normal distribution with mean \u03b8 0.\nalternatively, if one or more components of \u03b8 are discrete, we cannot expect the\nmaximum likelihood estimator to be approximately normal.\n\nexample 4.39 (t distribution) one model for heavy-tailed data is\n\nf (y; \u00b5, \u03c3 2, \u03c8) = \u0001{(\u03c8\u22121 + 1)/2}\u03c8 1/2\n\n(\u03c3 2\u03c0)1/2\u0001{1/(2\u03c8)} {1 + \u03c8(y \u2212 \u00b5)2/\u03c3 2}\u2212(\u03c8\u22121+1)/2,\n\nwhere \u03c8, \u03c3 > 0 and \u2212\u221e < \u00b5, y < \u221e. this generalizes the student t density with\n\u03c8\u22121 = \u03bd degrees of freedom to continuous \u03c8. its tails are heavier than those of\nthe normal density, obtained when \u03c8 \u2192 0; f (y; \u00b5, \u03c3 2, 1) is cauchy. the left panel\nof figure 4.8 shows the profile log likelihood for \u03c8 based on the n = 15 differ-\nences between heights of plants in the fourth column of table 1.1; \u03c8 = 0 is of\nparticular interest. the likelihood ratio statistic for comparing t and normal models\n\n "}, {"Page_number": 153, "text": "4.6 \u00b7 non-regular models\n\nfigure 4.8 likelihood\ninference for t\u03bd\ndistribution. left: profile\nlog likelihoods for\n\u03c8 = \u03bd\u22121 for maize data\n(solid), and for 19\nsimulated normal samples\n(dots); \u03c8 = 0 corresponds\nto the n (\u00b5, \u03c3 2) density.\nright: \u03c7 2\nfor the 1237 positive\nvalues of the likelihood\nratio statistic wp(0)\nobserved in 5000\nsimulated normal samples\nof size 15; the rest had\nwp(0) = 0.\n\n1 probability plot\n\nd\no\no\nh\n\ni\nl\n\ne\nk\n\ni\nl\n \n\ng\no\n\nl\n \n\ne\n\nl\ni\nf\n\no\nr\np\n\n2\n\n1\n\n0\n\n1\n-\n\n2\n-\n\n141\n\n... . .\n\n.......................................................................................................................................................................................................................................................................................................................................................................................................................................................\n\nc\ni\nt\ns\ni\nt\n\na\n\nt\ns\n \no\n\ni\nt\n\na\nr\n \n\nd\no\no\nh\n\ni\nl\n\ne\nk\nl\n\ni\n\n0\n1\n\n8\n\n6\n\n4\n\n2\n\n0\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\npsi\n\nquantiles of chi-squared\n\n0\n\n, 0)}, where(cid:22)\u00b50 and(cid:22)\u03c3 2\n\nis wp(0) = 2{(cid:8)((cid:22)\u00b5,(cid:22)\u03c3 2,(cid:22)\u03c8) \u2212 (cid:8)((cid:22)\u00b50,(cid:22)\u03c3 2\n0 are maximum likelihood\nestimates for the n (\u00b5, \u03c3 2) density. its observed value of 1.366 suggests that the t fit\nis only marginally better, but \u03c8 = 0 is onthe boundary of the parameter space and\nlated normal samples of size 15. in many cases (cid:22)\u03c8 = 0, so wp(0) = 0: its distribution\nstandard asymptotics do not apply, as we see from profile log likelihoods for simu-\nto understand this, we expand log f (\u00b5, \u03c3 2, \u03c8) about \u03c8 = 0, giving\nz4 \u2212 1\n3\n\n(z4 \u2212 2z2 \u2212 1) + \u03c8 2\n2\n\ncannot be \u03c7 2\n1 .\n\n(cid:20)\n\n(cid:21)\n\n1\n2\n\nz6\n\n{z2 + log(2\u03c0 \u03c3 2)} + \u03c8\n4\n(3z8 \u2212 4z6 \u2212 1) + o(\u03c8 4),\n\n\u2212 1\n2\n+ \u03c8 3\n24\n\n= 1\n2\n\ninvolve \u03c8 are\n\nwhere z = (y \u2212 \u00b5)/\u03c3 . the first and second derivatives that\n\u2202 log f /\u2202\u03c8 = (z4 \u2212 2z2 \u2212 1)/4 and\n\u2202 2 log f\n\u2202 2 log f\n\u2202\u03c8 2\n\u2202\u03c8 \u2202\u00b5\n\n= (z2 \u2212 z4)/(2\u03c3 2)\nevaluated at \u03c8 = 0, while example 4.18 gives the other derivatives needed. when \u03c8 =\n0, z = (y \u2212 \u00b5)/\u03c3 \u223c n (0, 1), with odd moments zero and first three even moments\n1, 3, and 15, so cov(z 4, z 4) = 96, cov(z 2, z 4) = 12, and var(z 2) = 2. the expected\ninformation matrix,\n\n= (z \u2212 z3)/\u03c3,\n\n\u2202 2 log f\n\u2202\u03c8 \u2202\u03c3 2\n\nz4 \u2212 1\n3\n\nz6,\n\ni(\u00b5, \u03c3 2, 0) =\n\n\uf8eb\n\uf8ed \u03c3 \u22122\n\n0\n0\n\n\uf8f6\n\uf8f8 ,\n\n0\n0\n\u03c3 \u22124 \u03c3 \u22122\n\u03c3 \u22122\n\n1\n2\n\n7\n2\n\nequals the covariance matrix of the score statistic, and the third derivatives of log f\nare well-behaved, so the large-sample distribution of the score vector when \u03c8 = 0 is\nnormal with mean zero and covariance matrix ni(\u00b5, \u03c3 2, 0). on setting \u03bb = (\u00b5, \u03c3 2)\nand \u03c8 = 0, (4.48) entails\n\n\u2202(cid:8)((cid:22)\u00b50,(cid:22)\u03c3 2\n\n0\n\n\u2202\u03c8\n\n, 0)\n\n.\u223c n (0, 3n/2).\n\n "}, {"Page_number": 154, "text": "4 \u00b7 likelihood\n\n142\n\ns\ne\ns\na\nc\n \nl\n\na\nu\nn\nn\na\n\n5\n1\n\n0\n1\n\n5\n\n0\n\ny\nt\ni\ns\nn\ne\nd\n\n0\n3\n\n.\n\n0\n\n0\n2\n\n.\n\n0\n\n0\n1\n\n.\n\n0\n\n0\n0\n\n.\n\n\u2022 \u2022\n\n\u2022\n\n\u2022 \u2022 \u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022 \u2022\n\n\u2022\n\n\u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022\n\n1970 1975 1980 1985 1990\n\n0\n\n5\n\n10\n\nyear\n\n20\n\n25\n\n30\n\n15\n\nw\n\nfigure 4.9 changepoint\nanalysis for data on\ndiarrhoea-associated\nhaemolytic uraemic\nsyndrome (hus)\n(henderson and\nmatthews, 1993). left:\ncounts of cases of hus\ntreated in birmingham,\n1970\u20131989 (solid), and\nscaled likelihood ratio\nstatistic wp(\u03c4 )/10 (blobs).\nright: density of w ,\nestimated from 10,000\nsimulations, and \u03c7 2\n1\ndensity (solid).\n\n2\n\n2 , and then wp(0) = 0;\n1 . thus\n\nin large samples this derivative is negative with probability 1\nwhile if it is positive the usual taylor series expansion applies and wp(0) \u223c \u03c7 2\n+ 1\nthe limiting distribution of wp(0) is 1\n\u03c7 2\n1 , giving\n2\npr{wp(0) \u2264 1.366} = 1\n+ 1\npr(\u03c7 2\n1\nthe asymptotic distribution of(cid:22)\u03c8 puts mass 1\n2\n2\n2 at \u03c8 = 0, with the remaining probability\nto assess the quality of such approximations, 5000 normal samples of size n =\n15 were generated. just 1237 of the wp(0) were positive, but those that were had\n(cid:17) = 0.94,\ndistribution close to \u03c7 2\n\n1 , asthe right panel of figure 4.8 shows. hence\n\u2264 1.366\n\npr{wp(0) \u2264 1.366} .= (3763/5000) + (1237/5000)pr\n\nspread as a normal density confined to the positive half-line.\n\n\u2264 1.366) = 0.88.\n\n(cid:15)\n\n\u03c7 2\n1\n\nstronger though not decisive evidence for the t model. large-sample results are un-\nreliable even with n = 100, when pr{wp(0) = 0} .= 0.37.\nample, despite being normal in large samples, when n is small the distribution of (cid:22)\u03c8\nsuch problems also arise if the favoured model is close to the boundary. for ex-\nwould have a point mass at \u03c8 = 0. if several parameters lie on their boundaries, then\n(cid:1)\nasymptotics become yet more cumbersome. simulation seems preferable.\n\nexample 4.40 (hus data) the left panel of figure 4.9 shows annual numbers\nof cases of \u2018diarrhoea-associated haemolytic uraemic syndrome\u2019 (hus) treated at a\nclinic in birmingham from 1970 to 1989. hus is a disease that can threaten the lives\nof small children; physicians have speculated that it is linked to levels of e. coli. the\ndata suggest a sharp rise in incidence at about 1980.\n\na simple model for this increase is that the annual counts y1, . . . , yn are realizations\n\nof independent poisson variables y1, . . . ,y n with positive means\n\n(cid:13)\n\n\u03bb1,\n\u03bb2,\n\ne(y j ) =\n\nj = 0, . . . , \u03c4 ,\nj = \u03c4 + 1, . . . ,n .\n\nhere the changepoint \u03c4 is a discrete parameter with possible values 0, . . . ,n . the\nsimpler model of no change appears when \u03c4 = 0 orn , and then \u03bb1 or \u03bb2 vanishes\n\n "}, {"Page_number": 155, "text": "4.6 \u00b7 non-regular models\n\n143\n\n(cid:8)(\u03c4, \u03bb1, \u03bb2) \u2261 s\u03c4 log \u03bb1 \u2212 \u03c4 \u03bb1 + (sn \u2212 s\u03c4 ) log \u03bb2 \u2212 (n \u2212 \u03c4 )\u03bb2,\n\nfrom the model. obviously these two situations are indistinguishable. moreover, there\nwould be no changepoint to detect if \u03bb1 = \u03bb2.\nin terms of si = y1 + \u00b7\u00b7\u00b7 + yi the log likelihood may be written\nand given \u03c4 , the maximum likelihood estimates are(cid:22)\u03bb1(\u03c4 ) = s\u03c4 /\u03c4 and(cid:22)\u03bb2(\u03c4 ) = (sn \u2212\ns\u03c4 )/(n \u2212 \u03c4 ). hence the profile log likelihood for \u03c4 is\n(cid:8)p(\u03c4 ) = s\u03c4 log(s\u03c4 /\u03c4 ) + (sn \u2212 s\u03c4 ) log{(sn \u2212 s\u03c4 )/(n \u2212 \u03c4 )} \u2212 sn,\n\u03c4 = 0, . . . ,n ,\n\nand the likelihood ratio statistic for comparing the model of change at \u03c4 with that of\nconstant \u03bb is\n\nwp(\u03c4 ) = 2\n\ns\u03c4 log\n\n+ (sn \u2212 s\u03c4 ) log\n\n(cid:9)\n\n(cid:20)\n\n(cid:21)\n\ns\u03c4 /\u03c4\nsn/n\n\n(cid:13)\n\n(sn \u2212 s\u03c4 )/(n \u2212 \u03c4 )\n\nsn/n\n\n(cid:14)(cid:10)\n\n,\n\nis the random variable corresponding to si . as si\n\nis a sum of inde-\nwhere si\npendent poisson variables, its distribution is poisson. for completeness we set\nwp(0) = wp(n) = 0. the values of wp(\u03c4 )/10 shown in the left panel of figure 4.9\ngive strong evidence of change in the rate.\n\nif we wish to test for change at a known value of \u03c4 , the usual asymptotics will\napply provided \u03bb1 and \u03bb2 can be estimated consistently from the independent poisson\nvariables s\u03c4 and sn \u2212 s\u03c4 , and this will be so if their means \u03c4 \u03bb1 and (n \u2212 \u03c4 )\u03bb2 both\ntend to infinity. two asymptotic frameworks for this are:\n\nr \u03bb1, \u03bb2 \u2192 \u221e with n and \u03c4 fixed; and\nr n \u2192 \u221e and \u03c4/n \u2192 a, with 0 < a < 1 and \u03bb1, \u03bb2 positive and fixed.\n\nthe practical implication is that if \u03c4 is so close to one of the endpoints that \u03c4 \u03bb1 or\n(n \u2212 \u03c4 )\u03bb2 is small, a \u03c7 2\n1 approximation for the null distribution of wp(\u03c4 ) will be poor,\nand its quality should be checked; otherwise no new issues arise. they do, however,\nif \u03c4 is unknown.\n\nthe likelihood ratio statistic for existence of a changepoint, regardless of its loca-\n\ntion, is\n\nw = max{wp(\u03c4 ) :\u03c4 = 1, . . . ,n \u2212 1}.\n\nthe values of wp(\u03c4 ) inthe left panel of figure 4.9 show that (cid:22)\u03c4 = 11, corresponding to\na change between 1980 and 1981; the observed value of w is w = 74.14. this seems\nto be the strong evidence for change that we would have anticipated from plotting the\ndata, but can we be sure?\nto find the distribution of w when \u03bb1 = \u03bb2 = \u03bb, wefirst note that y1, . . . ,y n are\nthen a poisson random sample with mean \u03bb. for reasons given in sections 5.2.3 it is\nappropriate to treat w conditional on sn = m, and example 2.36 implies that the joint\ndistribution of y1, . . . ,y n conditional on sn = m is multinomial with denominator m\nand probability vector \u03c0 = (n\n\u22121)t. wecan simulate the exact distribution of\nw under this setup, because no parameters are involved. the right panel of figure 4.9\nshows a histogram of 10,000 simulated values of w . clearly w is stochastically\n\n\u22121, . . . ,n\n\n "}, {"Page_number": 156, "text": "144\n\n4 \u00b7 likelihood\n\n1 density, that is, pr(w > v) > pr(\u03c7 2\n1\n\nlarger than the \u03c7 2\n> v) for any v > 0. even so,\nw = 74.14 is much too large to have occurred by chance: there is overwhelming\nhere the maximum likelihood estimator(cid:22)\u03c4 has a discrete distribution on 0, . . . ,20\nevidence for a change.\nand normal approximation would be foolish. other approaches have more appeal, and\n(cid:1)\nwe revisit these data in example 11.13.\n\nparameter identifiability\nthere must be a 1\u20131 mapping between models and elements of the parameter space,\n\notherwise there may be no unique value of \u03b8 for(cid:22)\u03b8 to converge to. a model in which\neach \u03b8 generates a different distribution is called identifiable. we saw a failure of this\nin example 4.40, where setting \u03bb1 = \u03bb2 gave the same model for any changepoint\n\u03c4 . ararer possibility is that a parameter cannot be estimated from a particular set\nof data. in the changepoint example, for instance, the profile likelihood for \u03c4 is flat\nwhen y1 = \u00b7\u00b7\u00b7 = yn. the probability of such an event vanishes asymptotically, but\nsuch likelihoods do occasionally occur in practice; they demand a simpler model,\nmore data or external knowledge about parameter values.\n\nsometimes a model has been set up in such a way that its parameters are non-\nidentifiable from any dataset. suppose we have data y1, . . . , yn with corresponding\nparameters \u03b71, . . . , \u03b7n, and that we may write both \u03b7 j = \u03b7 j (\u03b8) and \u03b7 j = \u03b7 j (\u03b2), where\n\u03b8 and \u03b2 = \u03b2(\u03b8) are p \u00d7 1 and q \u00d7 1 vectors of parameters, with q < p. then the\nmodel with \u03b7(\u03b8) issaid to be parameter redundant. the chain rule gives\n\n\u2202\u03b7t\n\n\u2202\u03b8\n\n= \u2202\u03b2 t\n\u2202\u03b8\n\n\u2202\u03b7t\n\n\u2202\u03b2\n\n,\n\nwhere both matrices on the right have rank q or lower for any \u03b8. hence the matrix on\nthe left is symbolically rank-deficient: there is a 1 \u00d7 p vector function \u03b3 (\u03b8), non-zero\nfor all \u03b8, such that \u03b3 (\u03b8)\u2202\u03b7t/\u2202\u03b8 = 0 for all \u03b8. it is fairly straightforward to see that\nthe converse is true, so the model is parameter redundant if and only if \u2202\u03b7t/\u2202\u03b8 is\nsymbolically rank-deficient. computer algebra can be used to check the symbolic\nrank of \u2202\u03b7t/\u2202\u03b8 for a complex model.\n\nexample 4.41 (exponential density) let y1, . . . ,y n be independent exponential\nvariables with mean \u03b7, and set \u03b7 = \u03b81\u03b82, where \u03b81 = \u03b2 and \u03b82 = \u03b2. evidently \u03b81 and\n\u03b82 cannot be estimated separately, and this is reflected by the n \u00d7 2 matrix \u2202\u03b7t/\u2202\u03b8,\nwhich consists of a row of \u03b82\u2019s above a row of \u03b81\u2019s. ithas symbolic rank one, as is\nseen on premultiplying it by \u03b3 (\u03b8) = (\u03b81,\u2212\u03b82).\nthe likelihood l(\u03b8) isconstant on the curves (\u03b8 1, \u03b82) = (\u03c8\u03b2, \u03b2\u22121) in ir2+ and is\nmaximized not at a single point but everywhere on the curve (\u03b81, \u03b82) = (yt, t\n\u22121),\n(cid:1)\nt > 0. a ridge such as this is a feature of parameter-redundant likelihoods.\n\nscore and information\nfor regular inference the log likelihood and its derivatives must be well-behaved\nenough to allow taylor series expansions and the neglect of their higher-order terms,\nand the score must have the asymptotic normal distribution at (4.34). for a random\n\n "}, {"Page_number": 157, "text": "4.6 \u00b7 non-regular models\n145\nsample, i (\u03b8 0) = ni(\u03b8 0), and so the expected information increases without limit as\nn \u2192 \u221e; inorder to have a normal limit in more complicated situations we also need\ni (\u03b8 0) \u2192 \u221e. furthermore the observed information must converge in probability as\nat (4.34).\n\nexample 4.42 (normal mixture) for an example of a non-smooth likelihood, let\nl(\u00b51, \u00b52, \u03c3 2\n, \u03b3 ) bethe likelihood for a random sample y1, . . . , yn from the mix-\n1\n(cid:13)\nture of normal densities\n\n, \u03c3 2\n2\n\n(cid:14)\n\n(cid:13)\n\n(cid:14)\n\n\u03b3\n\nexp\n\n(2\u03c0)1/2\u03c31\n\n\u2212 (y \u2212 \u00b51)2\n\n2\u03c3 2\n1\n\n+ 1 \u2212 \u03b3\n\n(2\u03c0)1/2\u03c32\n\nexp\n\n\u2212 (y \u2212 \u00b52)2\n\n2\u03c3 2\n2\n\n0 \u2264 \u03b3 \u2264 1,\n\n,\n\nwith the means and variances in their usual ranges. this corresponds to taking ob-\nservations in proportions \u03b3 , 1 \u2212 \u03b3 from two normal populations, not knowing from\nwhich they come. if \u03b3 (cid:10)= 0, 1, then for each y j\n(cid:17) = lim\n(cid:15)\nl\n\n(cid:17) = +\u221e,\n\n(cid:15)\n\n, \u03b3\n\n, \u03b3\n\nl\n\n\u00b51, y j , \u03c3 2\n1\n\n, \u03c3 2\n2\n\ny j , \u00b52, \u03c3 2\n1\n\n, \u03c3 2\n2\n\nlim\n\u03c31\u21920\n\n\u03c32\u21920\n\nso l is a smooth surface pocked with singularities, each of which corresponds to\nestimating the mean and variance of one of the populations from a single observation.\nfor large n the strong consistency result guarantees the existence of a smooth local\nmaximum of l near the true parameter values. when finding this numerically a careful\nchoice of starting values can help one avoid ending up at a spike instead, but it is worth\nasking why they occur.\n\nthe issue is rounding. as we saw in example 4.21, the fiction that data are con-\ntinuous is usually harmless and convenient. here it is not harmless, however, be-\ncause it results in infinite likelihoods. the spikes can be removed by accounting for\nthe rounding of the y j . ifthey are rounded to multiples of \u03b4, then pr(y = k\u03b4) =\nf(k\u03b4 + \u03b4/2) \u2212 f(k\u03b4 \u2212 \u03b4/2), where\n\nf(y) = \u03b3 \u0001\n\n+ (1 \u2212 \u03b3 )\u0001\n\n(cid:20)\n\n(cid:21)\n\ny \u2212 \u00b51\n\u03c31\n\n(cid:20)\n\n(cid:21)\n\n.\n\ny \u2212 \u00b52\n\u03c32\n\nas 0 < f(y j ) < 1, the largest possible contribution to l is then finite. see exam-\n(cid:1)\nple 5.36 for further discussion.\n\nexample 4.43 (shifted exponential density) to see a failure of regularity con-\nditions for the score statistic, let y1, . . . , yn be an exponential random sample with\nlower endpoint \u03c6 and mean \u03b8 + \u03c6, so\n\nf (y; \u03c6, \u03b8) = \u03b8\u22121 exp{\u2212(y \u2212 \u03c6)/\u03b8} ,\n\ny > \u03c6, \u03b8 >0.\n\nthe corresponding random variables y1, . . . ,y n have the same distribution as\n\u03c6 + \u03b8 e1, . . . , \u03c6 + \u03b8 en, where e1, . . . , en is a random sample from the standard\nexponential density. the log likelihood contribution from a single observation y > \u03c6\nis (cid:8)(\u03c6, \u03b8) = \u2212 log \u03b8 \u2212 (y \u2212 \u03c6)/\u03b8, so\n=\n\u2202(cid:8)(\u03c6, \u03b8)\n\n(cid:13)\n\n\u03b8\u22121,\n0,\n\ny > \u03c6,\notherwise.\n\n\u2202\u03c6\n\n "}, {"Page_number": 158, "text": "146\n\n4 \u00b7 likelihood\n\nset {y : g(y) > 0}.\n\nfor aregular model this would have mean zero, but here the interchange of differ-\nentiation and integration that yields (4.32) fails because the support of the density the support of g(y) is the\ndepends on \u03c6, and e(\u2202(cid:8)/\u2202\u03c6) = \u03b8\u22121.\nthe likelihood is l(\u03c6, \u03b8) = \u03b8\u2212n exp{\u2212n(y \u2212 \u03c6)/\u03b8} for y1, . . . , yn > \u03c6 and \u03b8 > 0,\nand for any \u03b8 this increases as \u03c6 \u2191 min y j and is zero thereafter. thus \u03c6 has maximum\nlikelihood estimate(cid:22)\u03c6 = y(1), while(cid:22)\u03b8 = y \u2212(cid:22)\u03c6 = y \u2212 y(1).\nto find limiting distributions of(cid:22)\u03c6 and(cid:22)\u03b8, recall from example 2.28 that the rth order\n(cid:16)\nj=1(n + 1 \u2212\nstatistic e(r) of a standard exponential random sample may be written\nd= \u03c6 + \u03b8 e(r), we\n\u22121 e j , where e1, . . . , en is an exponential random sample. as y(r)\n\u22121\u03b8 e1, implying that n\u03b8\u22121((cid:22)\u03c6 \u2212 \u03c6) d= e1: the rescaled endpoint\nj)\nestimate (cid:22)\u03c6 has a non-normal limit distribution. moreover it converges faster than\nsee that y(1)\nusual because (cid:22)\u03c6 \u2212 \u03c6 must be multiplied by n rather than n1/2 in order to give a\nfor the distribution of(cid:22)\u03b8, note that as y \u2212 y(1) = n\n(cid:22)\u03b8 d= n\n\n(cid:16)\nr=1 y(r) \u2212 y(1),\n(cid:12)\n= n\n\n\u22121(n \u2212 1)\u03b8 e ,\n\nnon-degenerate limit.\n\n\u2212 n\u03c6 \u2212 \u03b8 e1\n\nd= \u03c6 + n\n\nn\u03c6 + \u03b8\n\n(cid:11)\n\n\u22121\n\n\u22121\n\ne j\n\nn\n\nr\n\nn(cid:3)\nr=1\n\nr(cid:3)\nj=1\n\nn + 1 \u2212 j\n\nwith e the average of e2, . . . , en. the central\n\nn1/2((cid:22)\u03b8 \u2212 \u03b8)/\u03b8 d\u2212\u2192 n (0, 1), so standard asymptotics apply to (cid:22)\u03b8 despite their fail-\nure for(cid:22)\u03c6, which converges so fast that its randomness has no impact on the limiting\ndistribution of(cid:22)\u03b8.\n\ntheorem implies that\n\nlimit\n\nin this problem exact inference is possible for any n (exercise 4.6.4), but the general\n(cid:1)\n\nconclusion is that endpoints must be treated gingerly.\n\nthough artificial, our next example illustrates how trouble in stochastic process\n\nproblems can stem from the information quantities.\n\nexample 4.44 (poisson birth process) consider a sequence y0, . . . ,y n such\nthat given the values of y0, . . . ,y j\u22121, the variable y j has a poisson density with\nmean \u03b8y j\u22121, and e(y0) = \u03b8. the likelihood for \u03b8 based on such data was given in\nexample 4.6, and the log likelihood and observed information are\n\n(cid:8)(\u03b8) \u2261 n(cid:3)\n\nj=0\n\n(cid:4)\n\n(cid:2)\n\n1 + n\u22121(cid:3)\n\nj=0\n\ny j log \u03b8 \u2212 \u03b8\n\ny j\n\n,\n\nj (\u03b8) = \u03b8\u22122\n\ny j .\n\nn(cid:3)\nj=0\n\nthe expected value of y j , given y j\u22121, is\u03b8 y j\u22121, soits unconditional expectation is\n\u03b8 j+1. hence the expected information is i (\u03b8) = \u03b8\u22122(\u03b8 + \u00b7\u00b7\u00b7 + \u03b8 n+1). if \u03b8 \u2265 1, then\ni (\u03b8) \u2192 \u221e as n \u2192 \u221e, but if not, i (\u03b8) isasymptotically bounded. in fact, as n \u2192 \u221e,\n=\nthe process is certain to become extinct \u2014 that is, there will be an n0 such that yn0\nyn0+1 = \u00b7\u00b7\u00b7 = 0 \u2014unless \u03b8 > 1, and even then there is a non-zero probability of\nextinction. hence j (\u03b8) remains finite with probability one unless \u03b8 > 1, and remains\n(cid:22)\u03b8 = (y0 + \u00b7\u00b7\u00b7 + yn)/(1 + y0 + \u00b7\u00b7\u00b7 + yn\u22121) isneither consistent nor asymptotically\nfinite with non-zero probability for any \u03b8. thus the maximum likelihood estimator\nnormal if \u03b8 \u2264 1.\n\n "}, {"Page_number": 159, "text": "4.6 \u00b7 non-regular models\n\n147\n\nfrom a practical viewpoint, this failure of standard asymptotics is less critical than\nit might appear. the limit (4.26) is used to obtain finite-sample approximations such\nas (4.27), but we can still use these if they can be justified by other means. inference\n(cid:1)\nis not impossible, merely more difficult than with independent data.\n\nwrong model\nup to now we have supposed that the model fitted to the data is correct, with only\nparameter values unknown. to explore some consequences of fitting the wrong model,\nsuppose the true model is g(y), but that ignorant of this we attempt to fit\nf (y; \u03b8)\n(cid:16)\nto a random sample y1, . . . , yn. under mild conditions the log likelihood (cid:8)(\u03b8) =\nlog f (y j ; \u03b8) will be maximized at (cid:22)\u03b8, say, and as n \u2192 \u221e the quantity (cid:8)((cid:22)\u03b8) =\n\u22121(cid:8)((cid:22)\u03b8) will tend to\n\nn\n\n(cid:25)\n\nlog f (y; \u03b8g)g(y) dy,\n\nwhere \u03b8g is the value of \u03b8 that minimizes the kullback\u2013leibler discrepancy\n\n(cid:25)\n\n(cid:13)\n\n(cid:14)\n\nd( f\u03b8 , g) =\n\nlog\n\ng(y)\nf (y; \u03b8)\n\ng(y) dy\n\nwith respect to \u03b8. thus \u03b8g is the \u2018least bad\u2019 value of \u03b8 given our wrong model; of\ncourse \u03b8g depends on g. differentiation gives\n\n(cid:25)\n\n0 =\n\n\u2202 log f (y; \u03b8g)\n\ng(y) dy,\n\nwith(cid:22)\u03b8 determined by the finite-sample version of this,\n\u2202 log f (y j ;(cid:22)\u03b8)\n\n\u2202\u03b8\n\n0 = n\n\n\u22121\n\nn(cid:3)\nj=1\n\n.\n\n(4.51)\n\nexpansion of (4.51) about \u03b8g yields\n\n(cid:11)\n\n(cid:22)\u03b8\n\n.= \u03b8g +\n\n\u2212n\n\n\u22121\n\n\u2202 2 log f (y j ; \u03b8g)\n\n\u2202\u03b8 \u2202\u03b8 t\n\n(cid:12)\n\nn(cid:3)\nj=1\n\n\u22121\n\nn\n\n\u2202 log f (y j ; \u03b8g)\n\n\u2202\u03b8\n\n\u2202\u03b8\n\n(cid:12)\u22121(cid:11)\n\nand a modification of the derivation that starts on page 124 gives\n\nwhere the information sandwich variance matrix depends on\n\n\u22121 k (\u03b8g)i (\u03b8g)\n\n\u22121},\n\n.\u223c n p{\u03b8g, i (\u03b8g)\n(cid:25)\n\nk (\u03b8g) = n\n\n\u2202 log f (y; \u03b8)\n\n\u2202 log f (y; \u03b8)\n\n\u2202\u03b8\n\n\u2202\u03b8 t\n\ng(y) dy,\n\n(cid:25)\n\nig(\u03b8g) = \u2212n\n\n\u2202 2 log f (y; \u03b8)\n\n\u2202\u03b8 \u2202\u03b8 t\n\ng(y) dy.\n\n(4.52)\n\n(4.53)\n\nif g(y) = f (y; \u03b8), so that the supposed density is correct, then \u03b8g is the true \u03b8, the\nmultivariate version of (4.33) gives k (\u03b8g) = ig(\u03b8g) = i (\u03b8), and (4.52) reduces to the\nusual approximation.\n\nn(cid:3)\nj=1\n\n(cid:22)\u03b8\n\n "}, {"Page_number": 160, "text": "148\n\n4 \u00b7 likelihood\n\nin practice g(y) is ofcourse unknown, and then k (\u03b8g) and ig(\u03b8g) may be esti-\n\nmated by\n\n(cid:22)k = n(cid:3)\n\nj=1\n\n\u2202 log f (y j ;(cid:22)\u03b8)\n\n\u2202 log f (y j ;(cid:22)\u03b8)\n\n\u2202\u03b8\n\n\u2202\u03b8 t\n\n\u2202 2 log f (y j ;(cid:22)\u03b8)\n\n\u2202\u03b8 \u2202\u03b8 t\n\n;\n\n(4.54)\n\n, (cid:22)j = \u2212 n(cid:3)\n\u22121(cid:22)k(cid:22)j\n\u22121.\n\nj=1\n\nthe latter is just the observed information matrix. we may then construct confidence\n\nintervals for \u03b8g using (4.52) with variance matrix(cid:22)j\n\nfor future reference we give the approximate distribution of the likelihood ratio\n\nstatistic. taylor series approximation gives\n\n2{(cid:8)((cid:22)\u03b8) \u2212 (cid:8)(\u03b8g)} .= ((cid:22)\u03b8 \u2212 \u03b8g)t\n\n(cid:11)\n\n\u2212 n(cid:3)\n\n\u2202 2 log f (y j ; \u03b8g)\n\n.= n((cid:22)\u03b8 \u2212 \u03b8g)t ig(\u03b8g)((cid:22)\u03b8 \u2212 \u03b8g)\n\nj=1\n\n\u2202\u03b8 \u2202\u03b8 t\n\n(cid:12)\n\n((cid:22)\u03b8 \u2212 \u03b8g)\n\nand the normal distribution (4.52) of(cid:22)\u03b8 implies that the likelihood ratio statistic has\n\u22121 k (\u03b8g)}. ifthe model is\na distribution proportional to \u03c7 2\ncorrect, ig(\u03b8g) = k (\u03b8g), giving the previous mean, p.\n\np, but with mean tr{ig(\u03b8g)\n\nexample 4.45 (exponential and log-normal models) let f (y; \u03b8) bethe exponen-\ntial density with mean \u03b8, while in fact y = e\u03c3 z , where z is standard normal. then y\nis log-normal, with mean e\u03c3 2/2 and variance e\u03c3 2(e\u03c3 2 \u2212 1).\nthe presumed log likelihood is \u2212 log \u03b8 \u2212 y/\u03b8, sothat\n(cid:25)\n\n(cid:25)\n\nlog f (y; \u03b8)g(y) dy = \u2212 log \u03b8 \u2212 \u03b8\u22121\n\nyg(y) dy = \u2212 log \u03b8 \u2212 \u03b8\u22121e\u03c3 2/2,\n\ng\n\nand k (\u03b8g) = 1 \u2212 \u03b8\u22122\ng ,\n\nand differentiation of this with respect to \u03b8 gives \u03b8g = e\u03c3 2/2. here the \u2018least bad\u2019\nexponential model has the same mean as the true log-normal distribution, which must\nalways exceed one. further calculation gives i (\u03b8g) = \u03b8\u22122\nthe maximum likelihood estimate of \u03b8 is(cid:22)\u03b8 = y , and either directly or using the\ninformation sandwich we see that var((cid:22)\u03b8) = n\n\u2212 1). note that replacement of\n\u03b8g with its estimate(cid:22)\u03b8 could result in a negative variance. this is not the case if we use\nthe empirical variance \u2014 simple calculations give(cid:22)j = n/y2 and (cid:22)k = y\n(y j \u2212\ny)2, so (cid:22)j\n(y j \u2212 y)2. reassuringly, this is a consistent estimate of the\nvariance of the average of a random sample from any distribution with finite variance\n(example 2.20).\n\u2212 1, the likelihood ratio statistic may be over-\n(cid:1)\n\n\u22121 k (\u03b8g) = e\u03c3 2 \u2212 1 = \u03b8 2\ng\nor under-dispersed relative to the \u03c7 2\n1 distribution.\n\n\u22122(cid:22)k = n\n\nas ig(\u03b8g)\n\n\u22121\u03b8 2\n\n(cid:16)\n\n(cid:16)\n\ng (\u03b8 2\ng\n\n\u22124\n\n\u22122\n\nthe discussion above is too crude to be the last word. in practice the model fitted\nwill often be elaborate enough to be reasonably close to the data, in the sense that\nonly glaring departures from the model are certain to be detected. thus it would be\n\nbetter to examine the properties of(cid:22)\u03b8 and related quantities when f (y; \u03b8) isnear g(y)\n\nin a suitable sense.\n\n "}, {"Page_number": 161, "text": "4.6 \u00b7 non-regular models\n\n149\n\nexercises 4.6\n1\n\ndata arise from a mixture of two exponential populations, one with probability \u03c0 and\nparameter \u03bb1, and the other with probability 1 \u2212 \u03c0 and parameter \u03bb2. the exponential\nparameters are both positive real numbers and \u03c0 lies in the range [0, 1], so \u0001 = [0, 1] \u00d7\nir2+ and\n\n2\n\n3\n\n4\n\n5\n\n6\n\nthis requires basic\nknowledge of partial\ndifferential equations.\n\nf (y; \u03c0, \u03bb1, \u03bb2) = \u03c0 \u03bb1e\n\n\u2212\u03bb1 y + (1 \u2212 \u03c0)\u03bb2e\n\n\u2212\u03bb2 y ,\n\ny > 0, 0 \u2264 \u03c0 \u2264 1, \u03bb1, \u03bb2 > 0.\n\nare the parameters identifiable?\ndoes standard likelihood theory apply when (i) using a likelihood ratio statistic to test if\n\u03c0 = 0? (ii) estimating \u03c0 when \u03bb1 = \u03bb2?\none model for outliers in a normal sample is the mixture\n\nf (y; \u00b5, \u03c0) = (1 \u2212 \u03c0)\u03c6(y \u2212 \u00b5) + \u03c0g(y \u2212 \u00b5),\n\n0 \u2264 \u03c0 \u2264 1,\u221e < \u00b5 <\u221e,\nwhere g(z) has heavier tails than the standard normal density \u03c6(z); take g(z) = 1\n\u2212|z|\n,\n2 e\nfor example. typically \u03c0 will be small or zero. show that when \u03c0 = 0 the likelihood\nderivative for \u03c0 has zero mean but infinite variance, and discuss the implications for the\nlikelihood ratio statistic comparing normal and mixture models.\nshow that the capture-recapture model in example 4.13 is not parameter redundant, but\nthat it is if different survival probabilities are allowed in each year. why is this obvious?\nin example 4.43, use relations between the exponential, gamma, chi-squared and f dis-\ntributions (section 3.2.1) to show that\n\n2n(cid:22)\u03b8\n\n\u03b8\n\n\u223c \u03c7 2\n\n2(n\u22121)\n\n,\n\nn((cid:22)\u03c6 \u2212 \u03c6)(cid:22)\u03b8\n\n\u223c n\nn \u2212 1\n\nf2,2(n\u22121);\n\nhence give exact (1 \u2212 2\u03b1) confidence intervals for the parameters.\nshow that the score statistic for a variable y from the uniform density on (0, \u03b8) is u (\u03b8) =\n\u2212\u03b8\u22121 in the range 0 < y < \u03b8 and zero otherwise, and deduce that e{u (\u03b8)} = \u22121 and\ni(\u03b8) = \u2212\u03b8\u22121. why is this model non-regular?\nsketch the likelihood based on a random sample y1, . . . ,y n, and verify that(cid:22)\u03b8 = y(n). to\n\n(cid:11)\n\nfind its limiting distribution, note that\n\npr((cid:22)\u03b8 \u2264 a) =\nshow that as n \u2192 \u221e, zn = n(\u03b8 \u2212(cid:22)\u03b8)/\u03b8\n\na < 0,\n\n0,\n(a/\u03b8)n, 0 \u2264 a \u2264 \u03b8,\n1,\nd\u2212\u2192 e, where e is exponential.\n\na > \u03b8.\n\nsuppose that \u2202\u03b7t/\u2202\u03b8 is symbolically rank-deficient, that is, there exist \u03b3r (\u03b8), non-zero for\nall \u03b8, such that\n\np(cid:3)\nr=1\n\n\u03b3r (\u03b8)\n\n\u2202\u03b7 j\n\u2202\u03b8r\n\n= 0,\n\nj = 1, . . . ,n .\n\nshow that the auxiliary equations\n\nd\u03b81\n\u03b31(\u03b8)\n\n= \u00b7\u00b7\u00b7 = d\u03b8 p\n\u03b3 p(\u03b8)\n\nhave p \u2212 1 solutions given implicitly by \u03b2t (\u03b8) = ct for constants c1, . . . ,c p\u22121. deduce\nthat the model is parameter redundant.\n(catchpole and morgan, 1997)\n\n "}, {"Page_number": 162, "text": "150\n\n4.7 model selection\n\n4 \u00b7 likelihood\n\nmodel formulation involves judgement, experience, trial, and error. evidently models\nshould be consistent with knowledge of the system under study, extrapolate to related\nsets of data, and if possible have reasonable mathematical and statistical properties.\nthus, for example, we prefer discrete distributions for discrete quantities and contin-\nuous for continuous, while if a probability \u03c0(x) depends on a quantity x, the relation\n\u03c0(x) = e\u03b2x /(1 + e\u03b2x ) ispreferable to \u03c0(x) = \u03b2x, because the latter may lie outside\nthe interval (0, 1); see example 4.5. often subject-matter considerations suggest a\nstochastic argument for a range of suitable models, which typically have primacy\nover purely ad hoc ones. even after such principles have been applied, however, there\nare usually several competing models, and a basis is needed for comparing them.\n\na principle already used but as yet unstated is the principle of parsimony or\n\nockham\u2019s razor: \u2018it is vain to do with more what can be done with fewer\u2019. according william of ockham or\nto this, given several explanations of the same phenomenon, we should prefer the\nsimplest, or, in our terms, favour simple models over complex ones that fit our data\nabout equally well. but what does this last phrase mean? if we have models with 1, 2,\nand 3 parameters and maximized log likelihoods of 0, 10, and 11, the second clearly\nimproves on the first, but do the second and third fit \u2018about equally well\u2019? for regular\nnested models, standard asymptotics could be applied, but more generally there are\ndifficulties. first, model selection usually involves many fits to the same set of data,\nso our previous discussion focussing on comparing two prespecified models may be\nwildly inappropriate. second, useful asymptotics may be unavailable, for example be-\ncause the models to be compared are not nested. third, we may wish to treat none of\nthe models as the truth. an example is in prediction, where a fitted model is sometimes\ntreated as a \u2018black box\u2019 whose contents have no intrinsic interest but are merely used to\ngenerate predictions; we should then adopt the agnostic position described at the end\nof section 4.6. here we outline how those ideas may be applied to model selection.\nsuppose we have a random sample y1, . . . ,y n from the unknown true model\nlog f (y j ; \u03b8), giving\n\ng(y). we fit a candidate model f (y; \u03b8) bymaximizing (cid:8)(\u03b8) = (cid:16)\np \u00d7 1 parameter estimate(cid:22)\u03b8; equivalently we could minimize \u2212(cid:8)(\u03b8). the fact that the\n\noccam\n(?1285\u20131347/1349) was\nan english franciscan\nwho studied at oxford and\nparis, was imprisoned by\npope john xxii for\narguing that the\nfranciscan ideal of\npoverty was prefigured in\nthe gospels, and then\nescaped to bavaria where\nhe wrote in defense of\nemperor louis iv against\npapal claims; eco (1984)\ngives some idea of these\ncontroversies. regarded\nas the most important\nscholastic philosopher\nafter thomas aquinas, his\ninsistence that logic and\nhuman knowledge could\nbe studied without\nreference to theology and\nmetaphysics encouraged\nscientific research. he\nprobably died in the black\ndeath of 1349.\n\nkullback\u2013leibler discrepancy is positive,\n\nd( f\u03b8 , g) =\n\nlog\n\n(cid:25)\n\n(cid:13)\n\n(cid:14)\n\ng(y)\nf (y; \u03b8)\n\ng(y) dy \u2265 0,\n\nwith equality if and only if f (y; \u03b8) = g(y), suggests that we aim to choose the can-\ndidate that minimizes d( f\u03b8 , g). let \u03b8g denote the corresponding value of \u03b8. unfortu-\nnately this approach to model selection is not sufficiently discriminating. the catch is\n, g) = 0. to see why, suppose that by\nthat an infinity of candidate models have d( f\u03b8g\na lucky chance the candidate model contains the true one. then f (y; \u03b8g) = g(y) and\nwe call f\u03b8 correct. as g has fewer parameters we prefer it to f\u03b8 , but d( f\u03b8 , g) \u2265 0\nwith equality when \u03b8 = \u03b8g. hence on this basis any correct model is indistinguish-\nable from the true one. we want to pick out the simplest correct model, so we should\nfavour models with few rather than many parameters, provided they fit about equally\n\n "}, {"Page_number": 163, "text": "4.7 \u00b7 model selection\n\n151\n\nwell. for example, if g is the exponential density with unit mean, f\u03b8 might be the\nweibull density with unknown shape and scale parameters. this is correct because it\nreduces to g when both its parameters take value one, but given the choice we would\nprefer g. a example of a wrong model is the log normal density, which does not\nbecome exponential for any values of its parameters.\n\nthe expected likelihood ratio statistic for comparing g with f\u03b8 at \u03b8 =(cid:22)\u03b8 for another\n\n+\n!\nrandom sample y\nn(cid:3)\n1\nj=1\n\n+\ne\ng\n\nlog\n\n, . . . ,y\n\n+\n(cid:11)\nn from g, independent of y1, . . . ,y n, is\n\n(cid:12)\"\n\n= n d( f(cid:22)\u03b8 , g) \u2265 n d\n\nf\u03b8g\n\n, g\n\n(cid:15)\n\n(cid:17)\n\n,\n\n+\nj )\ng(y\n+\nf (y\n\nj ;(cid:22)\u03b8)\n\n+\n\ng (\u00b7) denotes expectation over the density g of y\n+\n. if f\u03b8 is close to g, then\nwhere e\n, g) will be close to n d(g, g), and we may hope that n d( f(cid:22)\u03b8 , g) isclose to\nn d( f\u03b8g\ndegrees of freedom gives(cid:22)\u03b8 more latitude to miss \u03b8g, and the corresponding increase\nboth. but if further parameters do not give a worthwhile reduction in d( f\u03b8g\n, \u03b8), adding\nin d( f(cid:22)\u03b8 , g) will tend to outweigh any decrease in d( f\u03b8g\non(cid:22)\u03b8, we average over its distribution, giving\nj ;(cid:22)\u03b8)\n\n= neg{d( f(cid:22)\u03b8 , g)};\nthe outer expectation is over the distribution of(cid:22)\u03b8, independent of y\nexpansion shows that log f (y;(cid:22)\u03b8) approximately equals\n((cid:22)\u03b8 \u2212 \u03b8g)t\n\n, g). to remove dependence\n\n((cid:22)\u03b8 \u2212 \u03b8g),\n\n+\nj )\ng(y\n+\nf (y\n\n\u2202 2 log f (y; \u03b8g)\n\nn(cid:3)\nj=1\n\n\u2202 log f (y; \u03b8g)\n\n. taylor series\n\n(cid:12)\"(cid:4)\n\n(4.55)\n\n(cid:2)\n\n(cid:11)\n\n!\n\nlog\n\neg\n\n+\ng\n\ne\n\n+\n\n+ 1\n2\n\n\u2202\u03b8\n\n\u2202\u03b8 \u2202\u03b8 t\n\nlog f (y; \u03b8g) + ((cid:22)\u03b8 \u2212 \u03b8g)t\nand as \u03b8g minimizes d( f\u03b8 , g),(cid:25)\n\nhence\n\nn d( f(cid:22)\u03b8 , g) = n\n\n\u2202 log f (y; \u03b8g)\n\n\u2202\u03b8\n\n(cid:13)\n\n(cid:25)\n\nlog\n.= n d( f\u03b8g\n\ng(y)\n\nf (y;(cid:22)\u03b8)\n, g) + 1\n2\n\ng(y) dy = 0.\n(cid:14)\ntr{((cid:22)\u03b8 \u2212 \u03b8g)((cid:22)\u03b8 \u2212 \u03b8g)t ig(\u03b8g)},\n\ng(y) dy\n\nwhere ig(\u03b8g) is given at (4.53) and we have used the fact that the trace of a scalar is\nmodel, and saw that for regular models (cid:22)\u03b8 is asymptotically normal with mean \u03b8g\nitself. at the end of section 4.6 we discussed likelihood estimation under the wrong\n\u22121, where k (\u03b8g) too is given at (4.53); both\n\n\u22121 k (\u03b8g)ig(\u03b8g)\n\nand variance matrix ig(\u03b8g)\nig(\u03b8g) and k (\u03b8g) are positive definite. hence\n\nneg{d( f(cid:22)\u03b8 , g)} .= n d( f\u03b8g\n\n, g) + 1\n2\n\ntr{ig(\u03b8g)\n\n\u22121 k (\u03b8g)},\n\n(4.56)\n\nwhere the second term penalizes the dimension p of \u03b8. the first term here is o(n),\nbut asboth ig(\u03b8) and k (\u03b8) are o(n), the second term is o( p). when f\u03b8 is correct\nand regular, ig(\u03b8g) = k (\u03b8g) so tr{ig(\u03b8g)\n\n\u22121 k (\u03b8g)} = p.\n\n "}, {"Page_number": 164, "text": "4 \u00b7 likelihood\n\nlog g(y) g(y) dy is constant\n\n152\n\nto build an estimator of (4.56), note first that the term\n\n#\nand can be ignored. now (cid:8)((cid:22)\u03b8) = (cid:8)(\u03b8g) + {(cid:8)((cid:22)\u03b8) \u2212 (cid:8)(\u03b8g)}, so\n(cid:25)\n\neg{\u2212(cid:8)((cid:22)\u03b8)} = \u2212eg\n.= n d( f\u03b8g\n\n(cid:8)(\u03b8g) + 1\nw (\u03b8g)\n2\n, g) \u2212 1\ntr{i (\u03b8g)\n2\n\n(cid:13)\n\n(cid:14)\n\n\u22121 k (\u03b8g)} \u2212n\n\u22121 k (\u03b8g)}. hence \u2212(cid:8)((cid:22)\u03b8) tends to under-\nwhere we have used the fact that under the wrong model, the likelihood ratio statis-\ntic w (\u03b8g) has mean approximately tr{i (\u03b8g)\n(cid:8)((cid:22)\u03b8) \u2265 (cid:8)(\u03b8g) bydefinition of (cid:22)\u03b8. as p increases, so will the extent of overestimation.\nestimate n d( f\u03b8g\nlog g(y) g(y) dy. onreflection this is obvious, because\nan estimator of (4.56) is \u2212(cid:8)((cid:22)\u03b8) + c, where c estimates tr{i (\u03b8g)\n\u22121 k (\u03b8g)}. two\npossible choices of c are p and tr((cid:22)j\n\u22121(cid:22)k ), where(cid:22)j and (cid:22)k are defined at (4.54), and\n\nlog g(y) g(y) dy,\n\n, g) \u2212 n\n\n#\n\nthese lead to\n\naic = 2{\u2212(cid:8)((cid:22)\u03b8) + p}, nic = 2{\u2212(cid:8)((cid:22)\u03b8) + tr((cid:22)j\n\n\u22121(cid:22)k )};\n\nanother possibility derived in section 11.3.1 is bic = \u22122(cid:8)((cid:22)\u03b8) + p log n. the model\n\n(4.57)\n\nis chosen to minimize aic, say, with the factor 2 putting differences of aic on the\nsame scale as likelihood ratio statistics. in practice aic, bic, and nic are used far\nbeyond random samples.\n\nfor insight into properties of aic, suppose that by rare good fortune we fit the\n\ntrue and a correct model, getting maximized log likelihoods(cid:22)(cid:8)g and (cid:8)((cid:22)\u03b8) with q and\np parameters respectively, and p > q. weprefer f\u03b8 to g if (cid:8)((cid:22)\u03b8) \u2212 p >(cid:22)(cid:8)g \u2212 q, butas\n\ng is nested within f\u03b8 , properties of the likelihood ratio statistic give\n\npr{(cid:8)((cid:22)\u03b8) \u2212 p >(cid:22)(cid:8)g \u2212 q} .= pr\n\n(cid:23)\n\n> 2( p \u2212 q)\n\n\u03c7 2\np\u2212q\n\n(cid:24)\n\n.\n\nfor every large n, and with p \u2212 q = 1, 2, 4 and 10, g is selected with probability\n0.84, 0.86, 0.91 and 0.97. hence model selection using aic is inconsistent:\n\npr(true model selected) (cid:10)\u2192 1\n\nasn \u2192 \u221e.\n\nin applications many models would be fitted, and the probability of selecting the true\none might be much lower than these calculations suggest.\n\nmodification of this argument shows that nic also gives an inconsistent procedure.\nfor consistent model selection differences of the penalty must lie between o(1) and\no(n) \u2014for example, o(log n) \u2014 but in practice the true model is rarely among those\nfitted and finite-sample properties are more important. bic does give consistent model\nselection when f\u03b8 is correct, but in finite samples it typically leads to underfitting\nbecause it tends to suggest too parsimonious a model.\n\nif the candidate model f\u03b8 is not correct, then\n\neg{(cid:22)(cid:8)g \u2212 (cid:8)((cid:22)\u03b8)} .= n d( f\u03b8g\n\nso the weak law of large numbers implies that pr{(cid:22)(cid:8)g \u2212 q > (cid:8)((cid:22)\u03b8) \u2212 p} \u21921 asn \u2192 \u221e\n\nfor fixed p. hence with enough data we can always distinguish the true model from\na fixed incorrect one.\n\n, g) > 0,\n\naic was introduced by\nakaike (1973) and is\nknown as akaike\u2019s\ninformation criterion.\nhirotugu akaike (1927\u2013)\nwas educated in tokyo\nand worked at the institute\nof statistical mathematics.\nhe has made important\ncontributions to time\nseries and model\nselection, and also to\nproduction engineering;\nsee findley and parzen\n(1995). nic and bic are\nthe network information\ncriterion, and bayes\u2019\ninformation criterion.\nthey may be modified to\nimprove their behaviour\nfor particular models.\n\n "}, {"Page_number": 165, "text": "4.7 \u00b7 model selection\n\n153\n\nb b\n\nb b b\n\nb\n\nb\n\nb\n\nb b\n\nb\n\nb b b b b\na a a a a a a a a a a a a a a\nn n n n n n n n n n n n n n n\n\nb\n\nb\n\na a a\nn n n\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022 \u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022 \u2022\n\n\u2022 \u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\nn\no\ni\nr\ne\n\nt\ni\nr\nc\n \nf\n\no\n\n \n\nl\n\ne\nu\na\nv\n\n0\n0\n1\n\n0\n8\n\n0\n6\n\n0\n4\n\n0\n2\n\n0\n\n0\n2\n-\n\nb b\na a\nn n\n\n5\n\n10\n\n0\n\n5\n\n10\n\n15\n\n20\n\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022 \u2022 \u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\n0\n\nx\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022 \u2022\n\u2022 \u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\n-10\n\n-5\n\norder of polynomial\n\nb\n\nb\n\nb\n\nb\n\nb\n\nn\na\n\u2022\n+\n\nb\n\nb\n\nb\n\nb\n\nb\n\nb b\n\nb b\n\nb\n\na a\nn n\n\u2022 \u2022\n+\n+\n\na a a a a a a a a a a a a a a a\nn n n n n n n n n n n n n n n n\n\u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022\n+ +\n+ + + + + + + + + + + + + +\n\n0 a\n\u2022\nn\n+\n0\n1\n\n0\n8\n\n0\n6\n\n0\n4\n\n0\n2\n\n0\n\n0\n2\n-\n\nn\no\ni\nr\ne\n\nt\ni\nr\nc\n \nf\n\nl\n\n \n\no\ne\nu\na\nv\n \nt\nc\na\nx\ne\n\nb b\na\n\u2022\nn\n+\nn\na\n\u2022\n+\n\nb b b b b b b b b b b b b b b b b b\n\na a a a a a a a a a a a a a a a\nn n n n n n n n n n n n n n n n\n\u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022\n+ + + + + + + + + + + + + + + +\n\na a\nn n\n\u2022 \u2022\n+ +\n\nfigure 4.10 model\nselection using likelihood\ncriteria. upper left: 21n\nobservations (blobs) with\ntrue mean (solid) and\npolynomial fits r = 1, 2, 3\n(dots, small dashes, large\ndashes); n = 3. upper\nright: empirical versions\nof aic, bic and nic for\ndata on left. all are\nmaximized with r = 3.\nlower left: twice expected\nlog likelihood 2eg((cid:8)(\u03b8g)}\n(blobs) and theoretical\nversions of aic, bic and\nnic for the panel above.\nthe crosses show how\n\n2eg{(cid:8)((cid:22)\u03b8)} increases with\n\nthe dimension of the fitted\nmodel. lower right: as\nlower left panel, but with\nn = 8 observations at\neach value of x.\n\n0\n4\n\n0\n3\n\ny\n\n0\n2\n\n0\n1\n\n0\n\n0\n0\n1\n\n0\n8\n\n0\n6\n\n0\n4\n\n0\n2\n\n0\n\n0\n2\n-\n\nn\no\ni\nr\ne\n\nt\ni\nr\nc\n \nf\n\nl\n\n \n\no\ne\nu\na\nv\n \nt\nc\na\nx\ne\n\n0\n\n5\n\n10\n\n15\n\n20\n\n0\n\n5\n\n10\n\n15\n\n20\n\norder of polynomial\n\norder of polynomial\n\nexample 4.46 (poisson model) we illustrate this discussion with data whose mean\n\u00b5(x) = 8 exp q(x) isshown in the upper left panel of figure 4.10, together with\nobservations generated by taking n = 3 independent poisson variables with means\n\u00b5(\u221210), \u00b5(\u22129), . . . , \u00b5(10); 21n variables in all. this is the true model g.\nwe fit candidate models f\u03b8 with poisson variables having means \u03bb(x) = exp(\u03b80 +\n\u03b81x + \u00b7\u00b7\u00b7 + \u03b8r xr ). the dimension is p = r + 1, and taking r = 1, . . . ,19 gives in-\ncreasingly complex incorrect models, because q(x) = 1.2ex /(1 + ex ) isnot poly-\nnomial. a polynomial with r = 20 terms can mimic q(x) exactly at x = \u221210,\n\u22129, . . . ,10, however, so taking r = 20 is correct but hardly parsimonious. the dif-\nference between the linear and the quadratic fits shown in the upper left panel of\nfigure 4.10 is small, but adding a cubic term seems to improve the fit.\nthe upper right panel shows aic, nic, and bic for these data. all three suggest\nthe choice of r = 3, but bic penalizes complexity much more drastically than the\nothers. in practice one should not only look at such a graph, but also examine any\nmodels for which the chosen criterion is close to the optimum.\n\nto see the theoretical quantities estimated by aic, bic, and nic, note that the data\nhere comprise n variables y1,x , . . . ,y n,x at each value of x. the log likelihood for an\n\n "}, {"Page_number": 166, "text": "154\n\n4 \u00b7 likelihood\n\nx\n\nj=1\n\n(cid:8)(\u03b8) \u2261 n(cid:3)\n\nincorrect model which takes y j,x to be poisson with mean \u03bb(x) is\n\n{y j,x log \u03bb(x) \u2212 \u03bb(x)}.\n(cid:16)\n\n10(cid:3)\nx=\u221210\n{\u00b5(x) log \u03bb(x) \u2212 \u03bb(x)}; the values of\nnow eg(y j,x ) = \u00b5(x), so eg{(cid:8)(\u03b8)} =n\n\u03b80, . . . , \u03b8r that maximize this give eg{(cid:8)(\u03b8g)}. the blobs in the lower left panel of\nthe figure show how \u22122eg{(cid:8)(\u03b8g)} depends on r. initially there are big decreases, but\nafter r = 5 adding further parameters is barely worthwhile. the crosses show how\n\u22122eg{(cid:8)((cid:22)\u03b8)} depends on r: not penalizing the log likelihood would lead to choosing\nr = 20. the exact values of aic, bic, and nic all indicate r = 5. however bic\nindicates fits about equally good for r = 5 and the simpler model r = 3, whereas\nfor aic and nic the best fit is similar to that with the more complex model r = 7.\nthe penalty applied by bic is substantially larger than for the others, which are very\nsimilar. these functions are what is being estimated in the upper right panel.\nto see the effect of increased sample size, the lower right panel of the figure shows\nexact values of \u2212eg{(cid:8)(\u03b8g)}, aic, bic and nic when n = 8. the jumps in \u2212eg{(cid:8)(\u03b8g)}\nare larger than with n = 3, and with this larger sample r = 7 seems appreciably better\nthan r = 5: more data make it worthwhile to fit more complex models, because we can\ndistinguish them more clearly. enormous values of n, however, are required to separate\nr = 10 and r = 20 reliably: \u2212eg{(cid:8)(\u03b8g)} .= \u22120.08 when n = 3 and r = 10, so even\na sample with n = 100 might indicate that r = 10. with n = 8, bic is much more\npeaked than when m = 3, so the value r = 5 itindicates is better determined, even\nthough the more complex choice r = 7 seems sensible on the basis of \u2212eg{(cid:8)(\u03b8g)}. by\ncontrast the penalties applied by aic and nic are unchanged. both indicate r = 7,\nbut evidently their empirical counterparts might have minima anywhere in the range\nr = 5, . . . ,20.\n\nthe closeness of nic to aic in this context leads us to ignore nic below.\n\n(cid:1)\n\nexample 4.47 (spring failure data)\nto analyze the full set of spring failure data\nin example 1.2, suppose that the data have weibull densities whose parameters \u03b1 and\n\u03b8 may depend on stress x, and consider the models:\nm1: unconnected values of \u03b1 and \u03b8 at each stress, with p = 12 parameters;\nm2: acommon value of \u03b1 but unconnected \u03b8 at each stress, with p = 7;\n\u22121, with p = 2; and\nm3: acommon value of \u03b1, and \u03b8 = (\u03b2x)\nm4: common values of \u03b1 and \u03b8 at every stress, with p = 2.\nthe nesting structure of these models is m4, m3 \u2286 m2 \u2286 m1, where \u2286 means \u2018is\nnested within\u2019; neither m3 nor m4 is nested within the other. we anticipate from\nfigure 1.2 that m4 will fit the data very poorly.\n\nto deal with the censoring at lower stresses, note that example 4.20 implies that\n\nthe likelihood for a censored weibull random sample y1, . . . , yn is\n\n(cid:6)\u03b1\u22121\n\n(cid:5)\n\n\u03b1\n\n\u03b8\n\ny j\n\u03b8\n\n(cid:1)\n\nu\n\n(cid:5)\n\n(cid:7)\n\n\u2212\n\ny j\n\u03b8\n\nexp\n\n(cid:6)\u03b1(cid:8)(cid:1)\n\nexp\n\nc\n\n(cid:5)\n\n(cid:7)\n\n\u2212\n\ny j\n\u03b8\n\n(cid:6)\u03b1(cid:8)\n\n,\n\n "}, {"Page_number": 167, "text": "4.7 \u00b7 model selection\n\n155\n\nmodel\n\np maximized log likelihood\n\naic\n\nbic\n\nm1\nm2\nm3\nm4\n\n12\n7\n2\n2\n\n\u2212360.40\n\u2212378.90\n\u2212411.50\n\u2212460.56\n\n744.8\n771.8\n827.0\n925.1\n\n769.9\n786.5\n831.2\n929.3\n\nstress xs\n\n700\n\n750\n\n800\n\n850\n\n900\n\n950\n\n1.59 (0.82)\n18044 (7295)\n\n1.44 (0.39)\n6609 (1566)\n\n1.69 (0.39)\n907 (180)\n\n7.36 (1.85)\n372 (16.9)\n\n5.37 (1.23)\n232 (14.5)\n\n5.97 (2.13)\n181 (10.2)\n\ntable 4.4 model\nselection for spring failure\ndata.\n\ntable 4.5 parameter\nestimates and standard\nerrors based on observed\ninformation for model m1\nfor the spring failure data,\nfitting separate parameters\nat each stress.\n\n(cid:22)\u03b1 (se)\n(cid:22)\u03b8 (se)\n(cid:18)\nu and\n\n(cid:18)\n\nwhere\nc denote products over uncensored and censored data. we regard all\nobservations as independent, with parameters \u03b1s and \u03b8s at stress xs, and with indicator\nds j equalling one if the jth observation at stress xs, ys j , isuncensored and equalling\nzero otherwise. the overall likelihood is then\n\n(cid:11)\n\n(cid:20)\n\n\u03b1s\n\u03b8s\n\nys j\n\u03b8s\n\n6(cid:1)\ns=1\n\n10(cid:1)\nj=1\n\n(cid:21)\u03b1s\u22121\n\n(cid:12)ds j\n\n(cid:13)\n\n(cid:20)\n\n\u2212\n\nexp\n\n(cid:21)\u03b1s\n\n(cid:14)\n\n.\n\nys j\n\u03b8s\n\ntable 4.4 shows that m4 fits much worse than any of the other models, and m3,\nwhich has the same number of parameters, is more promising. evidently m1 is best\nby a large margin.\n\ntable 4.5 gives estimates for m1, with standard errors based on observed informa-\n\ntion. the values of(cid:22)\u03b1 depend strongly on the stress, and suggest one value of \u03b1 at the\n\nthree lower stresses and another at the higher ones. the standard errors are useless at\nthe lower stresses, with heavy censoring: with so little information any inference will\nbe very uncertain.\nthe model with six separate values of \u03b8s and two values of \u03b1, one for the three\nupper and one for the three lower levels of xs, has maximized log likelihood \u2212360.92,\naic = 737.8, and bic = 754.6, so it beats m1. aplot of log (cid:22)\u03b8s against log stress is\nclose to a straight line, suggesting a three-parameter model with \u03b8 = 1/(\u03b2x) and two\ndifferent levels for \u03b1, but smooth dependence of \u03b1 on x is both more plausible and\nmore useful for prediction: what value of \u03b1 is suitable at stress 825 n/mm2? absent\n(cid:1)\nmore knowledge about the purpose of the experiment, we proceed no further.\n\nfurther discussion of model selection and the related topic of model uncertainty\n\nmay be found in sections 8.7.3 and 11.2.4.\n\nexercises 4.7\n1\n\nshow that both sides of (4.56) are invariant to 1\u20131 reparametrizations \u03b8 = \u03b8(\u03c6). why is\nthis important?\nuse aic and bic to compare the models fitted in example 4.34.\n\n2\n\n "}, {"Page_number": 168, "text": "156\n\n3\n\n4\n\n4 \u00b7 likelihood\ntwo densities for counts y = 0, 1, . . . are the poisson \u03b8 ye\n\u2212\u03b8 /y!, \u03b8 > 0 and the geometric\n\u03c0(1 \u2212 \u03c0)y, 0 < \u03c0 <1; their means are \u03b8 and \u03c0\u22121 \u2212 1. show that if the true model is one\nbut the other is fitted, the \u2018least bad\u2019 parameter value matches the means. how easy is\nit to tell them apart when the data are poisson with \u03b8 = 1, 5, 10, and when the data are\ngeometric?\nconsider a regular penalized log likelihood (cid:8)((cid:22)\u03b8) \u2212 cn, where cn\ntrue model. show that 2{(cid:8)((cid:22)\u03b8) \u2212 cn \u2212 (cid:8)g} d\u2212\u2192 \u03c7 2\n\np\u2212\u2192 c as n \u2192 \u221e, (cid:8)(\u03b8)\nis based on a correct model, \u03b8 has dimension p, and (cid:8)g is the log likelihood for the\n\u2212 2c, and deduce that the probability of\n\u2264 2c). hence show that while model selection based on\n\nselecting the true model is pr(\u03c7 2\np\nbic is consistent, that based on aic is not.\n\np\n\n4.8 bibliographic notes\n\nthe ideas of likelihood, information, sufficiency and efficient estimation were devel-\noped in a remarkable series of papers by r. a. fisher in the 1920s and 1930s. most\nintroductions to mathematical statistics contain this core material. a recent excellent\naccount is knight (2000). the approach here is influenced by silvey (1970), edwards\n(1972), cox and hinkley (1974) and kalbfleisch (1985). see also barndorff-nielsen\nand cox (1994) and pace and salvan (1997).\n\nthe literature on non-regular models is diffuse. see self and liang (1987), smith\n(1985, 1989b, 1994) and cheng and traylor (1995), or davison (2001) for a partial\nreview. parameter redundancy is discussed by catchpole and morgan (1997), with\napplications to capture-recapture models.\n\nmodel selection and uncertainty are topics of current research interest, with much\nheat generated by chatfield (1995) and discussants. for a longer discussion, see\nburnham and anderson (2002).\n\n4.9 problems\n\n1 the logistic density with location and scale parameters \u00b5 and \u03c3 is\n\nf (y; \u00b5, \u03c3 ) =\n\nexp{(y \u2212 \u00b5)/\u03c3}\n\n\u03c3 [1 + exp{(y \u2212 \u00b5)/\u03c3}]2\n\n, \u2212\u221e < y < \u221e, \u2212\u221e < \u00b5 <\u221e, \u03c3 > 0.\n\n(a) if y has density f (y; \u00b5, 1), show that the expected information for \u00b5 is 1/3.\n(b) instead of observing y , weobserve the indicator z of whether or not y is positive.\nwhen \u03c3 = 1, show that the expected information for \u00b5 based on z is e\u00b5/(1 + e\u00b5)2, and\ndeduce that the maximum efficiency of sampling based on z rather than y is 3/4. why is\nthis greatest at \u00b5 = 0?\n(c) find the expected information i (\u00b5, \u03c3 ) based on y when \u03c3 is unknown. without doing\nany calculations, explain why both parameters cannot be estimated based only on z.\n\n2 let \u03c8(\u03b8) be a1\u20131 transformation of \u03b8, and consider a model with log likelihoods (cid:8)(\u03b8)\n(\u03c8) inthe two parametrizations respectively; (cid:8) has a unique maximum at which the\n\nand (cid:8)\u2217\nlikelihood equation is satisfied. show that\n\n(\u03c8)\n\n\u2202(cid:8)\u2217\n\u2202\u03c8r\n\n= \u2202\u03b8 t\n\u2202\u03c8r\n\n\u2202(cid:8)(\u03b8)\n\n\u2202\u03b8\n\n,\n\n\u2202 2(cid:8)\u2217\n(\u03c8)\n\u2202\u03c8r \u2202\u03c8s\n\n= \u2202\u03b8 t\n\u2202\u03c8r\n\n\u2202 2(cid:8)(\u03b8)\n\u2202\u03b8 \u2202\u03b8 t\n\n\u2202\u03b8\n\u2202\u03c8s\n\n+ \u2202 2\u03b8 t\n\u2202\u03c8r \u2202\u03c8s\n\n\u2202(cid:8)(\u03b8)\n\n\u2202\u03b8\n\n "}, {"Page_number": 169, "text": "4.9 \u00b7 problems\n\nand deduce that\n\n157\n\n,\n\n\u2217\n\n(\u03c8) = \u2202\u03b8 t\n\ni\n\n\u2202\u03c8\n\ni (\u03b8)\n\n\u2202\u03b8\n\n\u2202\u03c8 t\n\nbut that a similar equation holds for observed information only when \u03b8 =(cid:22)\u03b8.\n\n3 a location-scale model with parameters \u00b5 and \u03c3 has density\n\nf (y; \u00b5, \u03c3 ) = 1\n\n\u03c3\n\ng\n\n(cid:20)\n\n(cid:21)\n\ny \u2212 \u00b5\n\n\u03c3\n\n, \u2212\u221e < y < \u221e, \u2212\u221e < \u00b5 <\u221e, \u03c3 > 0.\n\n# means \u2018the number of\ntimes\u2019.\n\n(a) show that the information in a single observation has form\n\ni(\u00b5, \u03c3 ) = \u03c3 \u22122\n\n(cid:5)\n\n(cid:6)\n\n,\n\na\nb\n\nb\nc\n\nand express a, b, and c in terms of h(\u00b7) = log g(\u00b7). show that b = 0 if g is symmetric about\nestimators(cid:22)\u00b5 and(cid:22)\u03c3 when g is regular.\nzero, and discuss the implications for the joint distribution of the maximum likelihood\n\u2212u2/2 and the log-gamma density\n(b) find a, b, and c for the normal density (2\u03c0)\nexp(\u03bau \u2212 eu)/ \u0001(\u03ba), where \u03ba > 0 isknown.\n\u22121 exp(\u2212|y \u2212 \u00b5|/\u03c3 ), \u2212\u221e <\ny, \u00b5 < \u221e, \u03c3 > 0; this is the laplace density.\n(a) write down the log likelihood for \u00b5 and \u03c3 and by showing that\n\n4 let y1, . . . , yn be a random sample from f (y; \u00b5, \u03c3 ) = (2\u03c3 )\n\n\u22121/2e\n\n(cid:3)\n\nd\nd\u00b5\n\n|y j \u2212 \u00b5| =#{ y j < \u00b5} \u2212#{ y j > \u00b5} =n \u2212 2r,\n\nwhere r = #{y j > \u00b5}, show that for any fixed \u03c3 > 0 the maximum likelihood estimate\nof \u00b5 is (cid:22)\u00b5 = median{y j}, and deduce that the maximum likelihood estimate of \u03c3 is the\nmean absolute deviation(cid:22)\u03c3 = n\n(b) use the results of section 2.3 to show that in large samples (cid:22)\u00b5\n.\u223c n (\u00b5, \u03c3 2/n) and\n(cid:22)\u03c3 p\u2212\u2192 \u03c3 . hence give an approximate confidence interval for the difference of means\n\n(cid:16)|y j \u2212(cid:22)\u00b5|.\n\n\u22121\n\nbased on the data in table 1.1.\n(c) is this a regular model for maximum likelihood estimation?\n\n5 show that the expected information for a random sample of size n from the weibull density\n\n(cid:5)\n\nin example 4.4 is\n\n\u03b12/\u03b8 2\n\n\u2212\u03c8(2)/\u03b8\n\ni (\u03b8, \u03b1) = n\nwhere \u03c8(z) = d log \u0001(z)/dz.\ngiven that \u03c8(2) = 0.42278 and \u03c8(cid:6)\n\u22121(\u03b8, \u03b1) = n\n\ni\n\n\u2212\u03c8(2)/\u03b8\n(2) + \u03c8(2)2}/\u03b12\n\n{1 + \u03c8(cid:6)\n\n(cid:6)\n\n,\n\n(cid:5)\n\n(2) = 0.64493, show that\n\u22121\n0.257\u03b8\n0.608\u03b12\n\n1.108\u03b8 2/\u03b12\n\n0.257\u03b8\n\n(cid:6)\n\n.\n\nhence find standard errors based on expected information for the estimates in the last\ncolumn of table 4.5. what problem arises in a similar calculation for the column with\nstress x = 700?\n6 persons who catch an infectious disease either die almost at once during its initial phase,\nor live an exponential time; denote the survival time y and declare that y = 0 ifdeath\noccurs in the initial phase. explain why the likelihood can be written as a product of terms\nof form\n\n(1 \u2212 p)1\u2212i \u00d7 { p\u03b8\u22121 exp(\u2212y /\u03b8)}i ,\n\n0 < p < 1, \u03b8 > 0,\n\nwhere i is an indicator of survival beyond the initial phase. give interpretations of p\nand \u03b8.\n\n "}, {"Page_number": 170, "text": "158\n\n4 \u00b7 likelihood\n\nmmm\n(1 + \u03b8)2/8\n\n953\n\nmmf\n914\n\n(1 \u2212 \u03b8 2)/8\n\nmfm\n846\n\n(1 \u2212 \u03b8)2/8\n\nmff\n845\n\n(1 \u2212 \u03b8 2)/8\n\nfmm\n825\n\n(1 \u2212 \u03b8 2)/8\n\nfmf\n748\n\n(1 \u2212 \u03b8)2/8\n\nffm\n852\n\n(1 \u2212 \u03b8 2)/8\n\nfff\n923\n\n(1 + \u03b8)2/8\n\ntable 4.6 frequencies\nof eight possible\nsequences, with their\nprobabilities based on a\nmodel in which the\nprobability of a male at\nfirst birth is 1\nprobability that the next\nchild has the same sex is\n(1 + \u03b8)/2, for 6906\nthree-child families.\n\n2 but the\n\nwhere r = (cid:16)\n\ngiven data (i1, y1), . . . ,( in, yn) onthe survival of n persons, show that the log likelihood\nhas form\n\n(cid:8)( p, \u03b8) = r log p + (n \u2212 r) log(1 \u2212 p) \u2212 r log \u03b8 \u2212 \u03b8\u22121\n\ni j y j ,\n\nn(cid:3)\nj=1\n\ni j , and hence find the maximum likelihood estimators of p and \u03b8, together\n\nwith the observed and expected information matrices.\ncomment on the form of the information matrices and give approximate 95% confidence\nintervals for the parameters.\n\n7 the administrator of a private hospital system is comparing legal claims for damages\nagainst two of the hospitals in his system. in the last five years at hospital a the following\n19 claims ($, inflation-adjusted) have been paid:\n\n59\n882\n\n172\n22793\n\n4762\n30002\n\n1000\n55\n\n2885\n32591\n\n1905\n853\n\n7094\n2153\n\n6259\n738\n\n1950\n311\n\n1208\n\nat hospital b, in the same period, there were 16 claims settled out of court for $800 or\nless, and 16 claims settled in court for\n\n36539\n19772\n\n3556\n31992\n\n1194\n1640\n\n1010\n1985\n\n5000\n2977\n\n1370\n1304\n\n1494\n1176\n\n55945\n1385\n\nthe proposed model is that claims within a hospital follow an exponential distribution.\nhow would you check this for hospital a?\nassuming that the exponential model is valid, set up the equations for calculating max-\nimum likelihood estimates of the means for hospitals a and b. indicate how you would\nsolve the equation for hospital b.\nthe maximum likelihood estimate for hospital b is 5455.7. if a common mean is fitted for\nboth hospitals, the maximum likelihood estimate is 5730.6. use these results to calculate\nthe likelihood ratio statistic for comparing the mean claims of the two hospitals, and\ninterpret the answer.\n\n8 are the sexes of successive children within a family dependent? table 4.6 gives for 6906\nthree-child families the frequencies of the eight possible sequences, with their probabilities\nbased on a model in which the probability of a male at first birth is 1\n2 but the probability\nthat the next child has the same sex is (1 + \u03b8)/2; here \u22121 < \u03b8 <1. what is special about\nthe model in which \u03b8 = 0?\n(a) if ymmm, ymmf and so forth denote the numbers of families with orders mmm, mmf,\nin a sample of m families, write down the likelihood for \u03b8 and show that the numbers of\nconsecutive pairs mm and ff is a sufficient statistic.\n(b) obtain the score statistic and observed information, and verify that for the data above\n.= 0.04 with standard error 0.0085. give a 95%\n\nthe maximum likelihood estimate is(cid:22)\u03b8\n\nconfidence interval for \u03b8. discuss.\n(c) is it true that the probability that the first child is male is 1\n2 ? suggest how you might\ngeneralize the model to allow for (i) this probability being unequal to 1\n2 , and (ii) the\nprobability that a female follows a female being unequal to the probability that a male\nfollows a male. write down the probabilities for table 4.6. if you are feeling energetic,\nconduct a full likelihood analysis of the data.\n\n "}, {"Page_number": 171, "text": "4.9 \u00b7 problems\n159\n9 let yi j , j = 1, . . . ,n i , i = 1, . . . ,k , beindependent normal random variables with means\n= \u03c3 2, with no restrictions\nni(cid:3)\nj=1\n\ni , and ni \u2265 2; set y i\u00b7 = n\n(cid:5)(cid:22)\u03c3 2/(cid:22)\u03c3 2\n\n\u00b5i and variances \u03c3 2\n(a) show that the likelihood ratio statistic for \u03c3 2\n1\non the \u00b5i , is given by\n\n, (cid:22)\u03c3 2 = k(cid:3)\n\nw = k(cid:3)\n\nj yi j .\n= \u00b7\u00b7\u00b7 = \u03c3 2\n\n(yi j \u2212 y i\u00b7)2,\n\nni , (cid:22)\u03c3 2\n\nk(cid:3)\ni=1\n\nni(cid:22)\u03c3 2\n\n= n\n\nni log\n\n(4.58)\n\n(cid:16)\n\n\u22121\ni\n\n\u22121\ni\n\ni=1\n\ni=1\n\n(cid:6)\n\n/\n\nk\n\ni\n\ni\n\ni\n\n10 in a normal linear model through the origin, independent observations y1, . . . ,y n are such\n\n1\n\n1\n\ni may be written as 2\u03c3 2\n\n2 may be based on (cid:22)\u03c3 2\n\n/(cid:22)\u03c3 2\n2 , and give its exact\n\nand give its approximate distribution for large ni .\n(b) a modification to w to improve its behaviour in small samples replaces the ni in (4.58)\nwith \u03bdi = ni \u2212 1. use the modified statistic to check the homogeneity of the variances for\nthe data in table 1.2 at the three highest stresses, and comment.\n(c) if k = 2 show that a test of \u03c3 2\n= \u03c3 2\n(d) if n1 = \u00b7\u00b7\u00b7 = nk = 3, show that (cid:22)\u03c3 2\ndistribution.\ni ei /3, where the ei are\nordered (cid:22)\u03c3 2\nindependent exponential random variables with unit means. explain how a plot of the\ni against exponential plotting positions can be used to check variance homo-\ngeneity and to assess the adequacy of the assumption of normality. what could be done if\nn1 = \u00b7\u00b7\u00b7 = nk = 2?\nthat y j \u223c n (\u03b2x j , \u03c3 2). show that the log likelihood for a sample y1, . . . , yn is\n\nn(cid:3)\n(y j \u2212 \u03b2x j )2.\nj=1\n(cid:16)\nx j (y j \u2212(cid:22)\u03b2x j ) = 0 and (cid:22)\u03c3 2 =\n(y j \u2212(cid:22)\u03b2x j )2, and hence find the maximum likelihood estimates(cid:22)\u03b2 and(cid:22)\u03c3 2 for data\n\ndeduce that the likelihood equations are equivalent to\n\u22121\nn\nwith x = (1, 2, 3, 4, 5) and y = (2.81, 5.48, 7.11, 8.69, 11.28).\nshow that the observed information matrix evaluated at the maximum likelihood estimates\nplot the data and your fitted line y =(cid:22)\u03b2x. say whether you think the model is correct, with\nis diagonal and use it to obtain approximate 95% confidence intervals for the parameters.\n\nlog(2\u03c0 \u03c3 2) \u2212 1\n2\u03c3 2\n\n(cid:8)(\u03b2, \u03c3 2) = \u2212 n\n2\n\n(cid:16)\n\nreasons. discuss the adequacy of the normal approximations in this example.\n\n11 in some measurements of \u00b5-meson decay by l. janossy and d. kiss the following ob-\nservations were recorded from a four channel discriminator: in 844 cases the decay time\nwas less than 1 second; in 467 cases the decay time was between 1 and 2 seconds; in 374\ncases the decay time was between 2 and 3 seconds; and in 564 cases the decay time was\ngreater than 3 seconds.\n\u2212\u03bbt , t > 0, \u03bb >0, find the likelihood for \u03bb. find\nthe maximum likelihood estimate,(cid:22)\u03bb, find its standard error, and give a 95% confidence\nassuming that decay time has density \u03bbe\n\ninterval for \u03bb.\ncheck whether the data are consistent with an exponential distribution by comparing the\nobserved and fitted frequencies.\n12 a family has two children a and b. child a catches an infectious disease d which is so\nrare that the probability that b catches it other than from a can be ignored. child a is\n\u2212\u03b1u , u \u2265 0, and in any small\ninfectious for a time u having probability density function \u03b1e\ninterval of time [t, t + \u03b4t] in [0, u ), b will catch d from a with probability \u03b2\u03b4t + o(\u03b4t),\nwhere \u03b1, \u03b2 > 0. calculate the probability \u03c1 that b does catch d. show that, in a family\n\u2212\u03b3 t , t \u2265 0,\nwhere b is actually infected, the density function of the time to infection is \u03b3 e\nwhere \u03b3 = \u03b1 + \u03b2.\nan epidemiologist observes n independent similar families, in r of which the second child\ncatches d from the first, at times t1, . . . ,t r . write down the likelihood of the data as the\nfind the maximum likelihood estimators (cid:22)\u03c1 and (cid:22)\u03b3 of \u03c1 and \u03b3 , and the asymptotic vari-\nproduct of the probability of observing r and the likelihood of the fixed sample t1, . . . ,t r .\nance of(cid:22)\u03b3 .\n\n "}, {"Page_number": 172, "text": "160\n\n4 \u00b7 likelihood\n\nround\n\nwrinkled\n\nyellow\ngreen\n\n315 (9/16)\n108 (3/16)\n\n101 (3/16)\n32 (1/16)\n\ntable 4.7 mendel\u2019s data\non four kinds of pea seeds\n(theoretical probability)\n(kendall and stuart, 1973,\np. 439).\n\n(cid:3)\n\n13 counts y1, y2, y3 are observed from a multinomial density\n\nm!\n\n\u03c0 y3\n3\n\n\u03c0 y2\n2\n\n\u03c0 y1\n1\n\ny1!y2!y3!\n\n2 (y1 + y2)/m.\n\n, yr = 0, . . . ,m ,\n\npr(y1 = y1, y2 = y2, y3 = y3) =\nyr = m,\nwhere 0 < \u03c01, \u03c02, \u03c03 < 1 and \u03c01 + \u03c02 + \u03c03 = 1. show that the maximum likelihood\nestimate of \u03c0r is yr /m.\nit is suspected that in fact \u03c01 = \u03c02 = \u03c0, say, where 0 < \u03c0 <1. show that the maximum\nlikelihood estimate of \u03c0 is then 1\ngive the likelihood ratio statistic for comparing the models, and state its asymptotic\ndistribution.\nin experiments on cross-breeding peas, mendel noted frequencies of seeds of different gregor mendel\nkinds when crossing plants with round yellow seeds and plants with wrinkled green\nseeds. his data and the theoretical probabilities according to his theory of inheritance are\nin table 4.7.\ncalculate the expected values under the model, and check the adequacy of the theory\nusing the likelihood ratio and pearson statistics w and p. how would the degrees of\nfreedom change if the table was treated as a two-way contingency table with unknown\nprobabilities?\n\n14\n\n15 the negative binomial density may be written\n(\u03c8 \u00b5)y\n\nf (y; \u00b5, \u03c8) = \u0001(y + \u03c8\u22121)\n\u0001(\u03c8\u22121)y!\n\n(1 + \u03c8 \u00b5)y+1/\u03c8\n\ny = 0, 1, . . . , \u00b5, \u03c8 > 0;\n\n,\n\nits limit as \u03c8 \u2192 0 isthe poisson density. taylor series expansion about \u03c8 = 0 shows that\nlog f (y; \u00b5, \u03c8) is\ny log \u00b5 \u2212 \u00b5 \u2212 log y! + \u03c8\n2\n\n{6\u00b52 y \u2212 4\u00b53 \u2212 y(1 \u2212 3y + 2y2)}\n\n{(y \u2212 \u00b5)2 \u2212 y} + \u03c8 2\n12\n\n+ \u03c8 3\n12\n\n{3\u00b54 \u2212 4\u00b53 y + y2(y \u2212 1)2} + o(\u03c8 4).\n\nfind the expected information i (\u00b5, \u03c8) when \u03c8 = 0, and show that the asymptotic dis-\ntribution of the score \u2202(cid:8)((cid:22)\u00b5\u03c8 , \u03c8)/\u2202\u03c8 based on a sample of size n is then n (0, n\u00b52/2).\n\ndiscuss properties of the likelihood ratio statistic for comparison of poisson and negative\nbinomial models.\n\n16 a possible model for the data in table 11.7 is that pumps are independent, and that the\nfailures for the jth pump have the poisson distribution with mean \u03bbx j , where x j is the\noperating hours (1000s). find the maximum likelihood estimate of \u03bb under this model\nand give its standard error. construct the likelihood ratio statistic to compare this with the\nmodel in which all the pumps have different rates. justifying your reasoning, say whether\nyou expect this statistic to have an approximate \u03c7 2 distribution.\nif y1, . . . , yn is a random sample with density \u03c3 \u22121 f {(y \u2212 \u00b5)/\u03c3 ; \u03bb}, where f is the skew-\nnormal density function (problem 3.6), write down the log likelihood for \u00b5, \u03c3 , and \u03bb, and\ninvestigate likelihood inference for this model.\n\n17\n\n(1823\u20131884) was the\nsecond child of farmers in\nbrunn, moravia. he\nshowed early promise but\nhis family\u2019s poverty meant\nthat he could continue his\neducation only as an\naugustinian monk. his\nwork on pea plants was\nbegun out of curiosity; it\ntook seven years to amass\nenough data to formulate\nhis theory of genetic\ninheritance based on\ndiscrete inheritable\ncharacteristics, which we\nknow as genes.\n\n "}, {"Page_number": 173, "text": "5\n\nmodels\n\nchapter 4 described methods related to a central notion in inference, namely like-\nlihood. this chapter and the next discuss how those ideas apply to some particular\nsituations, beginning with the simplest model for the dependence of one variable on\nanother, straight-line regression. there is then an account of exponential family distri-\nbutions, which include many models commonly used in practice, such as the normal,\nexponential, gamma, poisson and binomial densities, and which play a central role in\nstatistical theory. we then briefly describe group transformation models, which are\nalso important in statistical theory. this is followed by a description of models for\ndata in the form of lifetimes, which are common in medical and industrial settings,\nand a discussion of missing data and the em algorithm.\n\n5.1 straight-line regression\n\nwe have already met situations where we focus on how one variable depends on\nothers. in such problems there are two or more variables, some of which are regarded\nas fixed, and others as random. the random quantities are known as responses and\nthe fixed ones as explanatory variables. weshall suppose that only one variable is\nregarded as a response. such models, known as regression models, are discussed\nextensively in chapters 8, 9, and 10. here we outline the basic results for the simplest\nregression model, where a single response depends linearly on a single covariate. we\nstart with an example.\n\nexample 5.1 (venice sea level data) table 5.1 and figure 5.1 show annual maxi-\nmum sea levels in venice for 1931\u20131981. the most obvious feature is that the maxi-\nmum sea level increased by about 25 cm over that period. a simple model is of linear\ntrend in the sea level, y, so inyear j,\n\ny j = \u03b20 + \u03b21 j + \u03b5 j ,\n\n(5.1)\nwhere \u03b20 (cm) represents the expected maximum sea level in year j = 0, \u03b21 the\nannual increase (cm/year) , and \u03b5 j is a random variable with mean zero and variance\n\n161\n\n "}, {"Page_number": 174, "text": "table 5.1 annual\nmaximum sea levels (cm)\nin venice, 1931\u20131981\n(pirazzoli, 1982). to be\nread across rows.\n\nfigure 5.1 annual\nmaximum sea levels in\nvenice, 1931\u20131981, with\nfitted regression line.\n\niid\u223c means \u2018are\nindependent and\nidentically distributed as\u2019.\n\n162\n\n5 \u00b7 models\n\n78\n91\n116\n114\n120\n\n121\n97\n107\n118\n114\n\n116\n106\n112\n107\n96\n\n115\n105\n97\n110\n125\n\n147\n136\n95\n194\n124\n\n119\n126\n119\n138\n120\n\n114\n132\n124\n144\n132\n\n89\n104\n118\n138\n166\n\n102\n117\n145\n123\n134\n\n103\n99\n151\n122\n122\n138\n\n\u2022\n\n)\n\nm\nc\n(\n \nl\n\ne\nv\ne\n\nl\n \n\na\ne\ns\n\n0\n8\n1\n\n0\n6\n1\n\n0\n4\n1\n\n0\n2\n1\n\n0\n0\n1\n\n0\n8\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\u2022\u2022\n\n\u2022\n\n\u2022\n\n1930\n\n1940\n\n1950\n\n1960\n\n1970\n\n1980\n\nyear\n\n\u03c3 2 (cm2) representing scatter about the trend. here the response is sea level, y j , and\n(cid:1)\nthe year, j, isthe sole explanatory variable.\n\nthe simplest linear model is that independent random variables y j satisfy\n\ny j = \u03b20 + \u03b21x j + \u03b5 j ,\n\nj = 1, . . . ,n ,\n\n(5.2)\niid\u223c n (0, \u03c3 2), and \u03b20, \u03b21 and \u03c3 2 are unknown\nwhere the x j are known constants, the \u03b5 j\nparameters, thus y j is normal with mean \u03b20 + \u03b21x j and variance \u03c3 2. the data arise\nas pairs (x1, y1), . . . ,( xn, yn), from which \u03b20, \u03b21, and \u03c3 2 are to be estimated. in\nexample 5.1 the pairs are (1931, 103), . . . ,(1981, 138). if all the x j are equal, we\ncannot estimate the slope of the dependence of y on x, so weassume that at least two\nx j are distinct.\n\na reparametrization of (5.2) is more convenient, so we consider instead\n\n\u22121\n\n(cid:1)\n\nj = 1, . . . ,n ,\n\ny j = \u03b30 + \u03b31(x j \u2212 x) + \u03b5 j ,\n(5.3)\nx j . interms of the original parameters, \u03b31 = \u03b21, and \u03b30 = \u03b20 +\nwhere x = n\n\u03b21x. this can make better statistical sense too. in (5.1) the interpretation of \u03b20 as a\nmean sea level at the start of the christian era \u2014 when j = 0 \u2014 involves a ludicrous\nextrapolation of the straight-line model over two millenia, whereas \u03b30 concerns its\nlevel when j = x = 1956; this is clearly more sensible.\n\n "}, {"Page_number": 175, "text": "5.1 \u00b7 straight-line regression\nunder (5.3) the y j are independent and normal with means and variances \u03b30 +\n\u03b31(x j \u2212 x) and \u03c3 2, sothe likelihood based on ( x1, y1), . . . ,( xn, yn) is\n(cid:4)\n\n163\n\n(cid:3)\n\n1\n\nn(cid:2)\n(2\u03c0 \u03c3 2)1/2 exp\nj=1\n\u2212\u221e < \u03b30, \u03b31 < \u221e, \u03c3 2 > 0.\n\n\u2212 1\n2\u03c3 2\n\n{y j \u2212 \u03b30 \u2212 \u03b31(x j \u2212 x)}2\n\n,\n\nthe log likelihood is\n\n(cid:6)(\u03b30, \u03b31, \u03c3 2) \u2261 \u2212 1\n2\n\n(cid:5)\n\nn log \u03c3 2 + 1\n\n\u03c3 2\n\nn(cid:6)\nj=1\n\n{y j \u2212 \u03b30 \u2212 \u03b31(x j \u2212 x)}2\n\n(cid:7)\n\n.\n\n(5.4)\n\nfor any \u03c3 2, maximizing this over \u03b30 and \u03b31 is equivalent to minimizing the sum of\nsquares\n\nss(\u03b30, \u03b31) = n(cid:6)\n\nj=1\n\n{y j \u2212 \u03b30 \u2212 \u03b31(x j \u2212 x)}2,\n\nwhich is the sum of squared vertical deviations between the y j and their means\n\u03b30 + \u03b31(x j \u2212 x) under the linear model. its derivatives are\n\nn(cid:6)\nj=1\nn(cid:6)\nj=1\n\n= \u22122\n\n= \u22122\n\n= 2n,\n\n\u2202 ss\n\u2202\u03b30\n\n\u2202 ss\n\u2202\u03b31\n\u2202 2ss\n\u2202\u03b3 2\n0\n\n{y j \u2212 \u03b30 \u2212 \u03b31(x j \u2212 x)},\n\n(x j \u2212 x){y j \u2212 \u03b30 \u2212 \u03b31(x j \u2212 x)},\n\nn(cid:6)\nj=1\n\n= 2\n\n\u2202 2ss\n\u2202\u03b3 2\n1\n\n(x j \u2212 x)2,\n\n\u2202 2ss\n\u2202\u03b30\u2202\u03b31\n\n= 2\n\nn(cid:6)\nj=1\n\n(x j \u2212 x) = 0.\n\nthe solutions to the equations \u2202 ss/\u2202\u03b30 = \u2202 ss/\u2202\u03b31 = 0 are the least squares\nestimates,\n\n(cid:8)\u03b30 = y,\n\n(cid:8)\u03b31 =\n\n(cid:1)\nj=1 y j (x j \u2212 x)\n(cid:1)\nj=1(x j \u2212 x)2\n\nn\n\n(5.5)\nas anticipated, \u03b31 cannot be estimated if all the x j are equal, for then x j \u2261 x and\n(cid:8)\u03b31 is undefined. the matrix of second derivatives of ss is positive definite, so the\n\nn\n\n.\n\nestimates (5.5) minimize the sum of squares and hence maximize (cid:6)(\u03b30, \u03b31, \u03c3 2) with\nrespect to \u03b30 and \u03b31.\n\n(cid:9)\n\n(cid:10)\n\nimum likelihood estimate of \u03c3 2 is\n\nas the log likelihood may be written as \u2212 1\nn(cid:6)\nj=1\n\n\u22121ss((cid:8)\u03b30,(cid:8)\u03b31) = 1\n\n(cid:8)\u03c3 2 = n\n\nn\n\n2\n\nn log \u03c3 2 + ss(\u03b30, \u03b31)/\u03c3 2\n{y j \u2212(cid:8)\u03b30 \u2212(cid:8)\u03b31(x j \u2212 x)}2.\n\nthe quantity ss((cid:8)\u03b30,(cid:8)\u03b31), known as the residual sum of squares, isthe smallest sum\n\n, the max-\n\nof squares attainable by fitting (5.3) to the data.\n\n "}, {"Page_number": 176, "text": "164\n\n5 \u00b7 models\n\nthe least squares estimators are linear combinations of normal variables, so their\n\ndistributions are also normal. if we rewrite them as\nn(cid:6)\n{\u03b30 + \u03b31(x j \u2212 x) + \u03b5 j} =\u03b3 0 + n\nj=1\n{\u03b30 + \u03b31(x j \u2212 x) + \u03b5 j}(x j \u2212 x)\nn\nj=1\n\n(cid:8)\u03b30 = n\n\u22121\n(cid:1)\n(cid:8)\u03b31 =\n\nn(cid:6)\nj=1\n= \u03b31 +\n\n(cid:1)\n\n\u22121\n\nn\n\nj=1(x j \u2212 x)2\n\nwe see that because the \u03b5 j are independent with means zero and variances \u03c3 2,(cid:8)\u03b30 has\nmean \u03b30 and variance \u03c3 2/n, and that(cid:8)\u03b31 has mean \u03b31 and variance \u03c3 2/\n(x j \u2212 x)2.\n\nmoreover\n\n\u03b5 j ,\n\nn\n\n(cid:1)\nj=1(x j \u2212 x)\u03b5 j\n(cid:1)\nj=1(x j \u2212 x)2\n(cid:1)\n\nn\n\n,\n\ncov((cid:8)\u03b30,(cid:8)\u03b31) = cov\n(cid:1)\n\n(cid:11)\n\n(cid:6)\n\n\u22121\n\nn\n\n\u03b5 j ,\n\n(cid:12)\n\nn\n\n(cid:1)\nj=1(x j \u2212 x)\u03b5 j\n(cid:1)\nj=1(x j \u2212 x)2\n= 0 :\n\nn\n\n=\n\n(cid:1)\nn\nj=1 n\n\n\u22121(x j \u2212 x)var(\u03b5 j )\nj=1(x j \u2212 x)2\n\nn\n\nif \u03c3 2 is known, confidence intervals for the true values of \u03b30 and \u03b31 may be based\n\nas(cid:8)\u03b30 and(cid:8)\u03b31 are uncorrelated normal random variables, they are independent.\non the normal distributions of (cid:8)\u03b30 and (cid:8)\u03b31. a (1 \u2212 2\u03b1) confidence interval for \u03b31, for\nexample, is(cid:8)\u03b31 \u00b1 \u03c3 z\u03b1/{(cid:1)\nwe shall see in chapter 8 that the residual sum of squares ss((cid:8)\u03b30,(cid:8)\u03b31) \u223c \u03c3 2\u03c7 2\nindependent of(cid:8)\u03b30 and(cid:8)\u03b31. thus when \u03c3 2 is unknown, the estimator\nn\u22122,\n\n(x j \u2212 x)2}1/2.\n\ns2 = 1\nn \u2212 2\n\nss((cid:8)\u03b30,(cid:8)\u03b31)\n\nsatisfies e(s2) = \u03c3 2, and as s2 is independent of (cid:8)\u03b30 and (cid:8)\u03b31, a (1 \u2212 2\u03b1) confidence\ninterval for \u03b31 is(cid:8)\u03b31 \u00b1 stn\u22122(\u03b1)/{(cid:1)\n(x j \u2212 x)2}1/2, because\n(cid:8)\u03b31 \u2212 \u03b31\n(cid:1)\n\u223c tn\u22122.\n(x j \u2212 x)2\n\n(cid:10)1/2\n\ns2/\n\n(cid:9)\n\nexample 5.2 (venice sea level data) for the model y j = \u03b20 + \u03b21 j + \u03b5 j of exam-\nple 5.1, we have n = 51, x1 = 1931, . . . , xn = 1981, so x = 1956. in parametrization\n(5.3), \u03b30 is the expected annual maximum sea level in 1956 in cm, and \u03b31 is the mean\nstraightforward calculation yields (cid:8)\u03b30 = 119.61 cm and (cid:8)\u03b31 = 0.567 cm/year,\nannual increase in maximum sea level in cm/year.\nss((cid:8)\u03b30,(cid:8)\u03b31) = 16988.1, and\n(x j \u2212 x)2 = 11050. the unbiased estimate of \u03c3 2 is\ns2 = 16988.1/(51 \u2212 2) = 346.7, so we estimate \u03c3 by s = 18.6. this is very large\nrelative to the annual increase in sea level, which as we see from figure 5.1 is small\n(cid:10)1/2 = 0.177,\nstandard errors for(cid:8)\u03b30 and(cid:8)\u03b31 are s/n1/2 = 2.61 and s/\nrelative to the overall vertical variation.\nand a 95% confidence interval for \u03b31 is(cid:8)\u03b31 \u00b1 0.177t49(0.025), that is, (0.213, 0.921).\n\n(x j \u2212 x)2\n\n(cid:9)(cid:1)\n\n(cid:1)\n\nthis does not include zero, confirming that the trend in figure 5.1 is real.\n\n(cid:1)\n\n "}, {"Page_number": 177, "text": "5.1 \u00b7 straight-line regression\ndistributional results for linear functions of(cid:8)\u03b30 and(cid:8)\u03b31 are readily obtained. for exam-\nlinear combinations\nple, in the original linear model (5.2) we have \u03b20 = \u03b30 \u2212 \u03b31x, the maximum likelihood\nestimator of which is(cid:8)\u03b20 =(cid:8)\u03b30 \u2212(cid:8)\u03b31x. this has expected value \u03b30 \u2212 \u03b31x and variance\n(cid:12)\nvar((cid:8)\u03b30\u2212(cid:8)\u03b31x) = var((cid:8)\u03b30) \u2212 2xcov((cid:8)\u03b30,(cid:8)\u03b31) + x 2var((cid:8)\u03b31) = \u03c3 2\n\n(cid:1)\n\n(cid:11)\n\n+\n\n165\n\nx 2\n\n.\n\n1\nn\n\nn\n\nj=1(x j \u2212 x)2\n\nas\n\ncov((cid:8)\u03b20,(cid:8)\u03b21) = cov((cid:8)\u03b30 \u2212(cid:8)\u03b31x,(cid:8)\u03b31) = cov((cid:8)\u03b30,(cid:8)\u03b31) \u2212 xvar((cid:8)\u03b31) =\n\u2212\u03c3 2x\nj=1(x j \u2212 x)2\nthe normal random variables(cid:8)\u03b20 and(cid:8)\u03b21 are independent if and only if x = 0.\n\n(cid:1)\n\nn\n\n,\n\nsuppose we wish to predict the response value at x+,\ny+ = \u03b30 + \u03b31(x+ \u2212 x) + \u03b5+.\n\nhere \u03b5+ represents the random variation about the expected value, which is inde-\npendent of the other responses, because of our modelling assumptions. the random\nvariable y+ has expected value \u03b30 + \u03b31(x+ \u2212 x). the maximum likelihood estimator\nof this,(cid:8)\u03b30 +(cid:8)\u03b31(x+ \u2212 x), has mean and variance\n\n\u03b30 + \u03b31(x+ \u2212 x),\n\n\u03c3 2\n\n(cid:11)\n\n(cid:12)\n\n.\n\n+ (x+ \u2212 x)2\n(cid:1)\nj=1(x j \u2212 x)2\n\nn\n\n1\nn\n\nthis is the variance not of y+ but of(cid:8)\u03b30 +(cid:8)\u03b31(x+ \u2212 x): it does not account for the extra\n\nvariability introduced by \u03b5+. the variance appropriate for the predicted response\nactually observed is\n\nvar(y+) = var{(cid:8)\u03b30 +(cid:8)\u03b31(x+ \u2212 x) + \u03b5+} = \u03c3 2\n\n+ (x+ \u2212 x)2\n(cid:1)\nj=1(x j \u2212 x)2\n\nn\n\n1\nn\n\n+ \u03c3 2.\n\n(5.6)\n\n(cid:11)\n\n(cid:12)\n\nthe final \u03c3 2 is due to \u03b5+ and would remain even if the parameters were known.\nexample 5.3 (venice sea level data) for illustration we take x+ = 1993. our pre-\ndicted value for y+ is (cid:8)\u03b30 +(cid:8)\u03b31(x+ \u2212 x) = 140.59, with estimated variance 49.75 +\n346.70 = 396.45, obtained by replacing \u03c3 2 with s2 in (5.6). the estimated variance\n(cid:8)\u03b30 +(cid:8)\u03b31(x+ \u2212 x). a confidence interval for y+ could be obtained from the t statistic.\nof \u03b5+, 346.70, is much larger than the estimated variance 49.75 of the fitted value\nnoting that simple estimates of the errors \u03b5 j are the raw residuals e j = y j \u2212(cid:8)\u03b20 \u2212\n(cid:8)\u03b21x j , which should be normal and approximately independent of x if the model is\n\nour model (5.2) presupposes that the errors \u03b5 j are normal, and that the dependence\nof y on x is linear. we discuss how to check these assumptions in section 8.6.1, here\n\ncorrect. we check linearity by looking for patterns in a plot of the e j against the x j ,\nand check normality by a normal probability plot of the e j ; see figure 5.2. linearity\nseems justifiable, but the errors seem too skewed to be normally distributed.\n\n "}, {"Page_number": 178, "text": "166\n\nl\n\ni\n\na\nu\nd\ns\ne\nr\n\n0\n6\n\n0\n4\n\n0\n2\n\n0\n\n0\n2\n-\n\n5 \u00b7 models\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\u2022\n\u2022\u2022\n\n\u2022\n\u2022\u2022\n\u2022\u2022\n\nl\n\na\nu\nd\ns\ne\nr\n \n\ni\n\nd\ne\nr\ne\nd\nr\no\n\n0\n6\n\n0\n4\n\n0\n2\n\n0\n\n0\n2\n-\n\n0\n4\n-\n\n\u2022\n\n\u2022\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\u2022\n\u2022\n\n\u2022\u2022\u2022\n\u2022\n\n\u2022\n\n\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022 \u2022 \u2022\n\nfigure 5.2 straight-line\nregression fit to annual\nmaximum sea levels in\nvenice, 1931\u20131981. left:\nraw residuals plotted\nagainst time. right:\nnormal scores plot of raw\nresiduals; the line has\n\nslope(cid:8)\u03c3 . the skewness of\n\nthe residuals suggests that\nthe errors are not normal.\n\n1930\n\n1950\n\n1970\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\nyear\n\nnormal score\n\nthe astute reader will realise that the changing sea level is due not to the rising\nwaters of the adriatic, but to the sinking of the marker that measures water height,\n(cid:1)\nalong with venice, to which it is attached.\n\nexercises 5.1\n1\n\nfind the observed and expected information matrices for the parameters in (5.4), and\nconfirm that general likelihood theory gives the same variances and covariance for the\nleast squares estimates as the direct argument on page 164.\n\nshow that ((cid:8)\u03b30,(cid:8)\u03b31, s2) are minimal sufficient for the parameters of the straight-line regres-\n\nsion model.\nconsider data from the straight-line regression model with n observations and\n\n(cid:13)\n\nx j =\n\nj = 1, . . . ,m ,\n\n0,\n1, otherwise,\n\nwhere m \u2264 n. give acareful interpretation of the parameters \u03b20 and \u03b21, and find their least\nsquares estimates. for what value(s) of m is var((cid:8)\u03b21) minimized, and for which maximized?\nlet y1, . . . ,y n be observations satisfying (5.2), with not all the x j equal. find var((cid:8)\u03b20 +\nx+(cid:8)\u03b21), where x+ is fixed. hence give exact 0.95 confidence intervals for \u03b20 + \u03b21x+ when\n\ndo your results make qualitative sense?\n\n\u03c3 2 is known and when it is unknown.\n\n2\n\n3\n\n4\n\n5.2 exponential family models\n\nexponential families include most of the models we have met so far and are widely\nused in applications. densities such as the normal, gamma, poisson, multinomial,\nand so forth have the same underlying structure with elegant properties giving them\na central role in statistical theory. this section outlines those properties, first giving\nthe basic ideas for scalar random variables, then extending them to more complex\nmodels, and finally considering inference.\n\n5.2.1 basic notions\nlet f0(y) be a given probability density, discrete or continuous, under which random\nvariable y has support y = {y : f0(y) > 0} that is a subset of the real line ir. for\n\n "}, {"Page_number": 179, "text": "5.2 \u00b7 exponential family models\nexample, f0(y) might be the uniform density on the unit interval y = (0, 1), or might\n\u22121/y! ony = {0, 1, . . .}. let s(y ) be afunction of\nhave probability mass function e\ny , and let\n\n167\n\n(cid:14)\n\n(cid:15)\n\n(cid:16)\n\nn =\n\n\u03b8 : \u03ba(\u03b8) = log\n\nes(y)\u03b8 f0(y) dy < \u221e\n\nwhen y is discrete we\ninterpret the integrals as\nsums over y \u2208 y.\n\ndenote the values of \u03b8 for which the cumulant-generating function \u03ba(\u03b8) ofs (y ) is\nfinite. evidently 0 \u2208 n . toavoid trivial cases we suppose that n has at least one\nother element and that var{s(y )} > 0 under f0, sos (y ) isnot a degenerate random\nvariable. in fact the set n is convex, because if \u03b81, \u03b82 \u2208 n and \u03b1 \u2208 [0, 1], then\n\u03b1\u03b81 + (1 \u2212 \u03b1)\u03b82 \u2208 n :\nes(y){\u03b1\u03b81+(1\u2212\u03b1)\u03b82}\n\n(cid:10)\u03b1 (cid:9)\n\nes(y)\u03b82\n\nes(y)\u03b81\n\n(cid:15)\n\nf0(y) dy\n\n(cid:10)1\u2212\u03b1\n(cid:16)\u03b1 (cid:14)(cid:15)\n\n(cid:16)1\u2212\u03b1\n\nes(y)\u03b81 f0(y) dy\n\nes(y)\u03b82 f0(y) dy\n\n(cid:15) (cid:9)\n(cid:14)(cid:15)\n\nf0(y) dy =\n\u2264\n< \u221e;\n\nthe second line follows from h\u00a8older\u2019s inequality (exercise 5.2.1). moreover, as\n\u03ba{\u03b1\u03b81 + (1 \u2212 \u03b1)\u03b82} \u2264\u03b1\u03ba (\u03b81) + (1 \u2212 \u03b1)\u03ba(\u03b82), the function \u03ba(\u03b8) is convex on the\nset n . equality occurs only if \u03b81 = \u03b82, so in fact\u03ba (\u03b8) isstrictly convex.\na single fixed density f0 is not flexible enough to be useful in practice, for which\n\nwe need families of distributions. hence we embed f0 in the larger class\n\n(cid:17)\n\nf (y; \u03b8) = es(y)\u03b8 f0(y)\nes(x)\u03b8 f0(x) dx\n\ny \u2208 y, \u03b8 \u2208 n ,\n\n,\n\nf0 has been tilted by multiplication by es(y)\u03b8 and then the\nby exponential tilting:\nresulting positive function has been renormalized to have unit integral. evidently\nf (y; \u03b8) has support y for every \u03b8. ifs (y ) = y , we have anatural exponential family\nof order 1,\n\nf (y; \u03b8) = exp{y\u03b8 \u2212 \u03ba(\u03b8)} f0(y),\n\ny \u2208 y, \u03b8 \u2208 n .\n\n(5.7)\n\nthe family is called regular if the natural parameter space n is an open set.\nexample 5.4 (uniform density) let f0(y) = 1 for y \u2208 y = (0, 1). now\ney\u03b8 dy = log{(e\u03b8 \u2212 1)/\u03b8} < \u221e\n\ney\u03b8 f0(y) dy = log\n\n\u03ba(\u03b8) = log\n\n(cid:15)\n\n(cid:15)\n\n1\n\nfor all \u03b8 \u2208 n = (\u2212\u221e,\u221e), and the natural exponential family\n\n0\n\n(cid:14)\n\nf (y; \u03b8) =\n\n\u03b8e\u03b8 y/(e\u03b8 \u2212 1),\n0,\n\n0 < y < 1,\notherwise,\n\n(5.8)\nis plotted in the left panel of figure 5.3 for \u03b8 = \u22123, 0, 1. for this or any natural\nexponential family with bounded y, n = (\u2212\u221e,\u221e) and the family is regular.\n\n "}, {"Page_number": 180, "text": "168\n\nf\nd\np\n\n0\n\n.\n\n3\n\n0\n\n.\n\n2\n\n0\n\n.\n\n1\n\n0\n0\n\n.\n\n)\na\n\nt\n\ne\nh\n\nt\n(\nu\nm\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n5 \u00b7 models\n\nfigure 5.3 exponential\nfamilies generated by\ntilting the u (0, 1) density.\nleft: original density\n(solid), natural\nexponential family when\n\u03b8 = \u22123 (dots) and \u03b8 = 1\n(small dashes), and\ndensity generated when\ns(y) = log{y/(1 \u2212 y)}\nwhen \u03b8 = 3/4 (large\ndashes). right: mean\nfunction \u00b5(\u03b8) for the\nnatural exponential\nfamily.\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n-30 -20 -10\n\n0\n\n10\n\n20\n\n30\n\ny\n\ntheta\n\na different choice of s(y ) will generate a different exponential family. with s(y ) =\n\nlog{y /(1 \u2212 y )}, for example, the cumulant-generating function is given by\n\n(cid:15)\n\n1\n\ne\u03b8 log{y/(1\u2212y)}\n\ndy =\n\n1\n\ny(1+\u03b8)\u22121(1 \u2212 y)(1\u2212\u03b8)\u22121 dy\n\n(cid:15)\n\n0\n\n1\n\n(cid:17)\nfor a, b > 0, b(a, b) =\n0 ua\u22121(1 \u2212 u)b\u22121 du is\nthe beta function. it equals\n\u0001(a)\u0001(b)/ \u0001(a + b),\n\u0001(a) = (cid:17) \u221e\nwhere\n0 ua\u22121e\n\n\u2212u du is\nthe gamma function; see\nexercise 2.1.3.\n\n0\n\n= b(1 + \u03b8, 1 \u2212 \u03b8)\n= \u0001(1 + \u03b8)\u0001(1 \u2212 \u03b8)\n\u0001(1 + \u03b8 + 1 \u2212 \u03b8)\n\n|\u03b8| < 1,\n\n,\n\nand as \u0001(2) = 1, we have \u03ba(\u03b8) = log \u0001(1 + \u03b8) + log \u0001(1 \u2212 \u03b8). here the set n =\n(\u22121, 1) is open, so the resulting family is regular. figure 5.3 shows how this family\ndiffers from the natural one, being unbounded unless \u03b8 = 0.\n(cid:1)\n\nthe natural exponential family of order 1 generated by a tilted version of f0 is\nthe same as that generated by f0 itself. to see why, note that if s(y ) has density\n(5.7) for some \u03b8 = \u03b81, say, exponential tilting generates a density proportional to\nexp{s(y)\u03b8} exp{s(y)\u03b81 \u2212 \u03ba(\u03b81)} f0(y) with cumulant-generating function \u03ba(\u03b8 + \u03b81) \u2212\n\u03ba(\u03b81) for \u03b8 + \u03b81 \u2208 n . the new density is exp{s(y)(\u03b8 + \u03b81) \u2212 \u03ba(\u03b8 + \u03b81)} f0(y), for\n\u03b8 + \u03b81 \u2208 n . this is (5.7) apart from replacement of \u03b8 by \u03b8 + \u03b81. hence just one\nfamily is generated by a specific choice of f0 and s(y ), and this family is obtained\nby tilting any of its members.\n\nfor many purposes discussion of an exponential family is simplified if it is expressed\n\nwithout reference to a baseline density f0. if adensity may be written as\ny \u2208 y, \u03c9 \u2208 \u0001,\n\nf (y; \u03c9) = exp{s(y)\u03b8(\u03c9) \u2212 b(\u03c9) + c(y)},\n\n(5.9)\nwhere y is independent of the parameter \u03c9 and \u03b8 is a function of \u03c9, it issaid to be\nan exponential family of order 1. here \u03b8 and s are called the natural parameter and\nnatural observation.\n\nexample 5.5 (exponential density) the exponential density with mean \u03c9 is\nf (y; \u03c9) = \u03c9\u22121 exp(\u2212y/\u03c9), for y > 0 and \u03c9 > 0. here \u0001 = y = (0,\u221e), with\nnatural observation and parameter s(y) = y and \u03b8(\u03c9) = \u22121/\u03c9, and b(\u03c9) = log \u03c9.\nthe cumulant-generating function is \u03ba(\u03b8) = b{\u03c9\u22121(\u03b8)} = \u2212 log(\u2212\u03b8), which has\n\n "}, {"Page_number": 181, "text": "5.2 \u00b7 exponential family models\n169\nderivatives (r \u2212 1)!(\u22121)r \u03b8\u2212r = (r \u2212 1)!\u03c9r , the usual formula for cumulants of an\n(cid:1)\nexponential variable.\n\nif r is binomial with denominator m and proba-\n\n(cid:19)\n\n(cid:18)\n\nexample 5.6 (binomial density)\nbility 0 < \u03c0 <1, its density is\n\u03c0 r (1 \u2212 \u03c0)m\u2212r = exp\n(cid:18)\n\nm\nr\n\n(cid:14)\n\ns(r) = r,\n\n\u03b8(\u03c0) = log\n\n(cid:19)\n\n\u03c0\n\n1 \u2212 \u03c0\n\n(cid:18)\n\n(cid:19)\n\nr log\n\n\u03c0\n\n1 \u2212 \u03c0\n\nfor r \u2208 y = {0, 1, . . . ,m }. this has form (5.9) with \u03c9 = \u03c0,\n\n+ m log(1 \u2212 \u03c0) + log\n\n, b(\u03c0) = m log(1 \u2212 \u03c0), c(r) = log\n\n(cid:18)\n\n(cid:19)(cid:16)\n\nm\nr\n\n(cid:18)\n\n,\n\n(cid:19)\n\n.\n\nm\nr\n\nthe natural parameter is the log odds \u03b8 = log{\u03c0/(1 \u2212 \u03c0)} \u2208(\u2212\u221e, \u221e). this family\nis regular, with cumulant-generating function \u03ba(\u03b8) = m log(1 + e\u03b8 ).\n(cid:1)\n\n\u03b8(\u0001) denotes the set\n{\u03b8(\u03c9) :\u03c9 \u2208 \u0001}.\n\nif the function \u03b8(\u03c9) in(5.9) is 1\u20131, the density of s = s(y ) has form\ns \u2208 s(y), \u03b8 \u2208 \u03b8(\u0001).\n\nf (s; \u03b8) = exp [s\u03b8 \u2212 b {\u03c9\u22121(\u03b8)}]h(s),\n\nif \u0001 = \u03b8(\u0001) = n for some baseline density f0 then this is a natural exponential\nfamily with cumulant-generating function \u03ba(\u03b8) = b {\u03c9\u22121(\u03b8)}.\n\nexpressed as a function of \u03b8 rather than \u03c9, the moment-generating function of s(y )\n\nunder (5.9) is, if finite,\n\n(cid:9)\n\ne\n\nets(y )\n\n(cid:15)\n\n(cid:10) =\n= exp{\u03ba(\u03b8 + t) \u2212 \u03ba(\u03b8)}\n= exp{\u03ba(\u03b8 + t) \u2212 \u03ba(\u03b8)} ,\n\n(cid:15)\n\nexp{ts(y) + \u03b8s(y) \u2212 \u03ba(\u03b8) + c(y)} dy\n\nexp{(\u03b8 + t)y \u2212 \u03ba(\u03b8 + t) + c(y)} dy\n\nbecause the second integral equals unity; here \u03b8 = \u03b8(\u03c9) and \u03ba(\u03b8) = b {\u03c9\u22121(\u03b8)}. hence\nwhen y has density (5.9), the cumulant-generating function of s(y ) is\u03ba (\u03b8 + t) \u2212 \u03ba(\u03b8).\nthe cumulants result from differentiating \u03ba(\u03b8 + t) \u2212 \u03ba(\u03b8) with respect to t and then\nsetting t = 0, or equivalently differentiating \u03ba(\u03b8) with respect to \u03b8.\n\nmean parameter\nunder (5.7) the cumulant-generating function of y is \u03ba(\u03b8 + t) \u2212 \u03ba(\u03b8), so its mean\nand variance are\n\ne(y ) = d\u03ba(\u03b8)\nd\u03b8\n\n= \u03ba(cid:6)\n\n(\u03b8),\n\nvar(y ) = d2\u03ba(\u03b8)\nd\u03b8 2\n\n= \u03ba(cid:6)(cid:6)\n\n(\u03b8),\n\nsay. as y is non-degenerate under f0, var(y ) > 0 for all \u03b8 \u2208 n , and hence \u03ba(cid:6)\n(\u03b8) is\na strictly monotonic increasing function of \u03b8. thus there is a smooth 1\u20131 mapping\nbetween \u03b8 and the mean parameter \u00b5 = \u00b5(\u03b8) = \u03ba(cid:6)\n(\u03b8), and as \u03b8 varies in n , \u00b5 varies\nin the expectation space m.\nthe function \u00b5(\u03b8) isimportant for likelihood inference. a natural exponential\nfamily is called steep if |\u00b5(\u03b8i )| \u2192 \u221e for any sequence {\u03b8i} in intn that converges\n\n "}, {"Page_number": 182, "text": "5 \u00b7 models\n170\nto a boundary point of n . let us define the closed convex hull of y to be c(y), the\nsmallest closed set containing\n\n{y : y = \u03b1y1 + (1 \u2212 \u03b1)y2, 0 \u2264 \u03b1 \u2264 1, y1, y2 \u2208 y} .\n\nnow m \u2286 c(y), because every density (5.7) reweights elements of y. itcan be\nshown that a regular natural exponential family is steep, and that for such a family,\nsteepness is equivalent to m = int c(y). thus there is a duality between int c(y)\nand the expectation space m, and hence between int c(y) and intn : for every\n\u00b5 \u2208 int c(y) there is a unique \u03b8 \u2208 n such that f (y; \u03b8) has mean \u00b5. this equivalence\napplies widely because most natural exponential families are regular. as we shall see\nbelow, it implies that there is a unique maximum likelihood estimator of \u03b8 except for\npathological samples.\n\nthe interior of a set,\nintn , is what remains\nwhen its boundary is\nsubtracted from its\nclosure.\n\nexample 5.7 (uniform density) the mean function for the natural exponential\n\u22121 \u2212 \u03b8\u22121, is shown in the\nfamily generated by the u (0, 1) density, \u00b5(\u03b8) = (1 \u2212 e\nright panel of figure 5.3. here y = (0, 1), so c(y) = [0, 1] and int c(y) = (0, 1) =\nm. the family is steep because the only boundary points of n = (\u2212\u221e,\u221e) are \u00b1\u221e,\nto which no sequence {\u03b8i} \u2282n can converge.\nthe family with \u0001 = [0,\u221e) isnot steep, because \u00b5(\u03b8) \u2192 1/2 as\u03b8 \u2193 0.\n\u22121/y!, then\n\nexample 5.8 (poisson density)\n\n\u2212\u03b8 )\n\n(cid:1)\n\nif y = {0, 1, . . .} and f0(y) = e\n(cid:20) \u221e(cid:6)\n\n(cid:21)\n\ne\u03b8 y\u22121/y!\n\n= e\u03b8 \u2212 1\n\n\u03ba(\u03b8) = log\n\ny=0\n\nis finite for all \u03b8 \u2208 n = (\u2212\u221e,\u221e). hence\n\nf (y; \u03b8) = exp (\u03b8 y \u2212 e\u03b8 )/y!,\n\ny \u2208 y, \u03b8 \u2208 n ,\n\nis a regular natural exponential family. here c(y) = [0,\u221e), and the mean function\nis \u00b5(\u03b8) = \u03ba(cid:6)\n\n(\u03b8) = e\u03b8 , som = (0,\u221e) = int c(y); the family is steep.\n\nin terms of \u00b5 we have the familiar expression\nf (y; \u00b5) = exp (y log \u00b5 \u2212 \u00b5) /y! = \u00b5ye\n\n\u2212\u00b5/y!,\n\ny = 0, 1, . . . , \u00b5 > 0.\n\n(cid:1)\n\nvariance function\nwhen y has a natural exponential family density with cumulant-generating function\n\u03ba(\u03b8), its mean is \u00b5(\u03b8) = \u03ba(cid:6)\n(\u03b8). now \u03ba(\u03b8) issmooth and strictly convex, so the mapping\nbetween \u03b8 and \u00b5 = \u00b5(\u03b8) = \u03ba(cid:6)\n(\u03b8) issmooth and monotone. it follows that the density\n(5.7) can be reparametrized in terms of \u00b5, setting \u03b8 = \u03b8(\u00b5). in terms of \u00b5, \u03ba(\u03b8) =\n\u03ba{\u03b8(\u00b5)}, so\n\nvar(y ) = \u03ba(cid:6)(cid:6)\n\n(\u03b8) = d\u00b5\nd\u03b8\n\n= v (\u00b5), \u00b5 \u2208 m,\n\n\u03b8=\u03b8(\u00b5)\n\n(cid:22)(cid:22)(cid:22)(cid:22)\n\nsay, where v (\u00b5) isthe variance function of the family. as we saw in section 3.1.2, the\nvariance function determines the variance-stabilizing transformation for y . itplays a\n\n "}, {"Page_number": 183, "text": "5.2 \u00b7 exponential family models\n\n171\n\nwhere we have used (5.10). hence\n\ncentral role in generalized linear models, which we shall study in section 10.3. the\nvariance function and its domain m together determine their exponential family, as\nwe shall now see.\n\u00b5(cid:6){\u03b8(\u00b5)}d\u03b8/d\u00b5 = 1, and this implies that\n1\n\non differentiating the identity \u00b5{\u03b8(\u00b5)} =\u00b5 with respect\n\nto \u00b5, weobtain\n\nd\u03b8(\u00b5)\n\n(5.10)\n\n=\n\n\u00b5(cid:6){\u03b8(\u00b5)} = 1\n\n.\n\nd\u00b5\n\nv (\u00b5)\nas var(y ) > 0, this derivative is finite for any \u00b5 \u2208 m, so\ndu = \u03b8(\u00b5) \u2212 \u03b8(\u00b50),\n\n(cid:15) \u00b5\n\n1\n\nv (u)\n\n\u00b50\n\nand as 0 \u2208 n we can choose \u00b50 \u2208 m to give \u03b8(\u00b50) = 0. now\n(cid:15) \u00b5\nd\u00b5 =\n\n\u03ba(\u03b8) =\n\n(cid:15) \u00b5\n\n(cid:15) \u03b8\n\n(cid:15) \u03b8\n\n\u03ba(cid:6)\n\n\u00b5\n\nu\n\nv (u)\n\ndu,\n\n\u00b50\n\n0\n\n(t) dt =\n(cid:14)(cid:15) \u00b5\n\n0\n\n\u00b5(t) dt =\n(cid:16)\n\n1\n\n=\n\n\u00b50\n\n(cid:15) \u00b5\n\ndt\nd\u00b5\n\nu\n\n\u03ba\n\n\u00b50\n\n\u00b50\n\ndu\n\ndu,\n\nv (u)\n\nv (u)\n\n(5.11)\nimplicitly. the natural parameter space n is traced out by \u03b8(\u00b5) = (cid:17) \u00b5\nand given m and v (\u00b5), we have expressed \u03ba in terms of \u00b5; this determines \u03ba(\u03b8)\n\u22121 du as\n\u00b5 varies in m.\nexample 5.9 (linear variance function) let y be a random variable with v (\u00b5) =\n\u00b5 and m = (0,\u221e). then\n(cid:15) \u00b5\ndu =\n\ndu = \u00b5 \u2212 \u00b50,\n\n= log(\u00b5/\u00b50),\n\n(cid:15) \u00b5\n\n(cid:15) \u00b5\n\nv (u)\n\n\u00b50\n\n1\n\nu\n\nv (u)\n\n\u00b50\n\ndu\nu\n\n\u00b50\n\nand if \u00b50 = 1, (5.11) gives \u03ba(log \u00b5) = \u00b5 \u2212 1. on setting \u03b8 = log \u00b5, we have\u03ba (\u03b8) =\ne\u03b8 \u2212 1, and as \u00b5 varies inm, \u03b8 = log \u00b5 varies in (\u2212\u221e,\u221e). as e\u03b8 \u2212 1 isthe cumulant-\ngenerating function of the poisson density with mean e\u03b8 and there is a 1\u20131 correspon-\ndence between cumulant-generating functions and distributions, y is poisson with\nmean \u00b5 = e\u03b8 .\n(cid:1)\n\nv (u)\n\n\u00b50\n\n5.2.2 families of order p\nto generalize the preceding discussion to models with several parameters, we again\nstart from a base density f0(y), now supposing that its support y \u2286 ird, for d \u2265 1,\nis not a subset of any space of dimension lower than d. let the p \u00d7 1 vector s(y) =\n(s1(y), . . . ,s p(y))t consist of functions of y for which the set {1, s1(y), . . . ,s p(y)} is\nlinearly independent, and define\n\n(cid:14)\n\n(cid:15)\n\n(cid:16)\n\nn =\n\n\u03b8 \u2208 ir p : \u03ba(\u03b8) = log\n\nes(y)t\u03b8 f0(y) dy < \u221e\n\n,\n\n "}, {"Page_number": 184, "text": "172\n\n5 \u00b7 models\nwhere \u03b8 = (\u03b81, . . . , \u03b8 p)t. ingeneral \u03b8 = \u03b8(\u03c9) may depend on a parameter \u03c9 taking\nvalues in \u0001 \u2282 irq, where \u03b8(\u0001) \u2286 n .\n\nan exponential family of order p has density\n\nf (y; \u03c9) = exp{s(y)t\u03b8(\u03c9) \u2212 b(\u03c9)} f0(y),\n\ny \u2208 y, \u03c9 \u2208 \u0001,\n\n(5.12)\n\nwhere b(\u03c9) = \u03ba{\u03b8(\u03c9)}. this is called a minimal representation if\nthe set\n{1, \u03b81(\u03c9), . . . , \u03b8 p(\u03c9)} is linearly independent. if there is a 1\u20131 mapping between\nn and \u0001 the family can be written as a natural exponential family of order p,\n\nf (y; \u03c9) = exp{s(y)t\u03b8 \u2212 \u03ba(\u03b8)} f0(y),\n\ny \u2208 y, \u03b8 \u2208 n .\n\n(5.13)\n\n(cid:15)\n\n0\n\nterms such as natural observation, natural parameter space, expectation space,\nregular model, and steep family generalize to families of order p and we shall use\nthem below without further comment. our proofs that the natural parameter space\nn is convex, that the family may be generated by any of its members, that \u03ba(\u03b8) is\nstrictly convex, and that s(y ) has cumulant-generating function \u03ba(\u03b8 + t) \u2212 \u03ba(\u03b8) also\ngeneralize with minor changes. the mean vector and covariance matrix of s(y ) are\nnow the p \u00d7 1 vector and p \u00d7 p matrix\ne{s(y )} = d\u03ba(\u03b8)\nd\u03b8\nif\n\nvar{s(y )} = d2\u03ba(\u03b8)\nd\u03b8d\u03b8 t\n\nf0(y) isuniform on (0, 1) and s(y) equals\n\n,\n\n.\n\nexample 5.10 (beta density)\n(log y, log(1 \u2212 y))t, then\n\n\u03ba(\u03b8) = log\n\n1\n\nexp{\u03b81 log y + \u03b82 log(1 \u2212 y)} dy = log b(1 + \u03b81, 1 + \u03b82),\n\nwhere b(a, b) = \u0001(a)\u0001(b)/ \u0001(a + b) isthe beta function; see example 5.4. the re-\nsulting model is usually written in terms of a = \u03b81 + 1 and b = \u03b82 + 1, giving the\nbeta density\n\nf (y; a, b) = ya\u22121(1 \u2212 y)b\u22121\n\n,\n\nb(a, b)\n\n0 < y < 1,\n\n(5.14)\nin this parametrization the natural parameter space is n = (0,\u221e) \u00d7 (0,\u221e). in\nexample 5.4 we took s(y) = log{y/(1 \u2212 y)}, thereby generating the one-parameter\nsubfamily in which b = 2 \u2212 a. this subfamily is also obtained by taking s(y) =\n(log y, log(1 \u2212 y))t and \u03b8(\u03c9) = (\u03c9,\u2212\u03c9)t, but this representation is not minimal be-\ncause (1, 1)\u03b8(\u03c9) = 0.\n\na, b > 0.\n\ncomparison of figures 5.4 and 5.3 shows how tilting with two parameters broadens\n(cid:1)\n\nthe variety of densities the family contains.\n\nexample 5.11 (von mises density) directional data are those where the observa-\ntions y j are angles \u2014 see table 5.2, which gives the bearings of 29 homing pigeons\n30, 60, and 90 seconds after release and on vanishing from sight. another example is\na wind direction, while the position of a star in the sky is an instance of directional\ndata on a sphere.\n\n "}, {"Page_number": 185, "text": "table 5.2 homing\npigeon data (artes, 1997).\nbearings (degrees) of 29\nhoming pigeons 30, 60\nand 90 seconds after\nrelease, with their\nbearings on vanishing\nfrom sight.\n\nfigure 5.4 beta\ndensities for different\nvalues of a and b.\nswapping a and b reflects\nthe densities about\ny = 0.5.\n\n5.2 \u00b7 exponential family models\n\n1\n240\n250\n270\n275\n\n16\n320\n325\n15\n60\n\n2\n300\n290\n305\n285\n\n17\n340\n335\n320\n345\n\n3\n225\n210\n215\n185\n\n18\n355\n25\n30\n35\n\n4\n285\n325\n295\n290\n\n19\n40\n330\n335\n65\n\n5\n210\n205\n195\n195\n\n20\n225\n220\n215\n250\n\n6\n265\n240\n210\n225\n\n21\n50\n50\n55\n60\n\n7\n310\n330\n335\n335\n\n22\n200\n195\n185\n175\n\n8\n330\n315\n315\n285\n\n23\n330\n320\n325\n325\n\n9\n325\n285\n135\n120\n\n24\n325\n315\n345\n330\n\n10\n290\n335\n10\n30\n\n25\n330\n290\n285\n280\n\n11\n15\n10\n5\n10\n\n26\n280\n285\n280\n350\n\n12\n330\n305\n325\n85\n\n27\n180\n155\n160\n185\n\n13\n100\n95\n90\n90\n\n28\n50\n25\n15\n20\n\n14\n35\n65\n70\n80\n\n29\n20\n0\n25\n30\n\n30\n60\n90\nvan\n\n30\n60\n90\nvan\n\n173\n\n15\n340\n345\n330\n350\n\na=0.5, b=0.5\n\na=0.5, b=2\n\na=1, b=1\n\nf\nd\np\n\n6\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\nf\nd\np\n\n6\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\ny\n\na=3, b=5\n\ny\n\na=5, b=5\n\ny\n\na=15, b=10\n\nf\nd\np\n\n6\n\n5\n\n4\n\n3\n2\n\n1\n\n0\n\nf\nd\np\n\n6\n\n5\n\n4\n\n3\n2\n\n1\n\n0\n\nf\nd\np\n\nf\nd\np\n\n6\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\n6\n\n5\n\n4\n\n3\n2\n\n1\n\n0\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.\n\n.0\n\ny\n\ny\n\ny\n\nto build a class of densities for circular data we start from the uniform density on\nthe circle, f0(y) = (2\u03c0)\n\n\u22121 for 0 \u2264 y < 2\u03c0, and take\n\ns(y) = (cos y, sin y)t,\n\n\u03b8(\u03c9) = (\u03c4 cos \u03b3 , \u03c4 sin \u03b3 )t,\n\nwhere \u03c9 = (\u03c4, \u03b3 ) lies in \u0001 = [0,\u221e) \u00d7 [0, 2\u03c0). this choice of s(y) ensures the desir-\nable property f (y) = f (y \u00b1 2k\u03c0) for all integer k. nows (y)t\u03b8(\u03c9) = \u03c4 cos(y \u2212 \u03b3 )\nand(cid:15)\n\n(cid:15)\n\n(cid:15)\n\nes(y)t\u03b8(\u03c9) f0(y) dy = 1\n2\u03c0\n\n2\u03c0\n\ne\u03c4 cos(y\u2212\u03b3 ) dy = 1\n2\u03c0\n\n0\n\n0\n\n2\u03c0\n\ne\u03c4 cos y dy = i0(\u03c4 ),\n\n "}, {"Page_number": 186, "text": "174\n\ni\ni\n\ng\ng\nn\nn\nh\nh\nt\nt\nr\nr\no\no\nn\nn\n\n4\n4\n\n2\n2\n\n0\n0\n\n2\n2\n-\n-\n\n4\n4\n-\n-\n\n5 \u00b7 models\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n3\n.\n0\n\n2\n.\n0\n\n1\n.\n0\n\n1\n.\n0\n-\n\n.\n\n3\n0\n-\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n-4\n-4\n\n-2\n-2\n\n0\n0\n\n2\n2\n\n4\n4\n\n-0.3\n\n-0.1\n\n0.1 0.2 0.3\n\neasting\neasting\n\nwhere i\u03bd(\u03c4 ) isthe modified bessel function of the first kind and order \u03bd. the resulting\nexponential family is the von mises density\nf (y; \u03c4, \u03b3 ) = {2\u03c0 i0(\u03c4 )}\u22121e\u03c4 cos(y\u2212\u03b3 ),\n\n0 \u2264 y < 2\u03c0, \u03c4 > 0, 0 \u2264 \u03b3 < 2\u03c0;\n\nsee figure 5.5. the mean direction \u03b3 gives the direction in which observations are\nconcentrated, and the precision \u03c4 gives the strength of that concentration. notice that\n\u03c4 = 0 gives the uniform distribution on the circle, whatever the value of \u03b3 . here\ninterest focuses on y rather than on s(y ), which is introduced purely in order to\nthe estimates and standard errors for the data in table 5.2 are (cid:8)\u03b3 = 320 (15) and\ngenerate a natural class of densities for y.\n(cid:8)\u03c4 = 1.08 (0.32) at 30 seconds, with corresponding figures 316 (15) and 1.05 (0.32)\n\nat 60 seconds, 329 (21) and 0.75 (0.29) at 90 seconds, and 357 (29) and 0.52 (0.28)\non vanishing. thus as figure 5.5 shows, the bearings of the pigeons become more\ndispersed as they fly away. the likelihood ratio statistics that compare the fitted\ntwo-parameter model with the uniform density are 13.80, 13.34, 7.33, and 3.75. as\nthe mean direction \u03b3 vanishes under the uniform model, the situation is non-regular\n(section 4.6), but the evidence against uniformity clearly weakens as time passes.\n(cid:1)\n\ncurved exponential families\nin the examples above, the natural parameter \u03b8 = (\u03b81(\u03c9), . . . , \u03b8 p(\u03c9))t is a 1\u20131 function\nof \u03c9 = (\u03c91, . . . , \u03c9q)t, so ofcourse p = q. another possibility is that q > p, inwhich\ncase \u03c9 cannot be identified from data. such models are not useful in practice, and it\nis more interesting to consider the case q < p. now\u03b8 (\u03c9) varies in the q-dimensional\nsubspace \u03b8(\u0001) ofn . if\u03b8 = a + b\u03c9 is a linear function of \u03c9, where a and b are a p \u00d7 1\nvector and a p \u00d7 q matrix of constants, then s(y)t\u03b8(\u03c9) = s(y)ta + {s(y)t b}\u03c9, and\n(y) =\nthe exponential family may be generated from f\nb ts(y). hence it is just an exponential family of order q and no new issues arise: the\noriginal representation was not minimal. if \u03b8(\u03c9) is anonlinear function, however, and\nthe representation is minimal, we have a ( p, q) curved exponential family.\n\n0(y) \u221d eats(y) f0(y) bytaking s\n(cid:6)\n\n(cid:6)\n\nfigure 5.5 circular\ndata. left: bearings of 29\nhoming pigeons at various\nintervals after release.\nright: von mises densities\nfor different values of \u03b3\nand \u03c4 . shownare the\nbaseline uniform density\n\u22121, and von\n(heavy) (2\u03c0)\nmises densities with\n\u03c4 = 0.3, \u03b3 = 5\u03c0/4\n(solid), \u03c4 = 0.7,\n\u03b3 = 3\u03c0/8 (dots), and\n\u03c4 = 1, \u03b3 = 7\u03c0/4\n(dashes). in each case the\ndensity f (y; \u03c4, \u03b3 ) is given\nby the distance from the\norigin to the curve, so the\nareas do not integrate to\none.\n\nrichard von mises\n(1883\u20131953) was born in\nlvov and educated in\nvienna and brno. he\nbecame professor of\napplied mathematics in\nstrasbourg, dresden and\nberlin, then left for\nistanbul to escape the\nnazis, finishing his career\nat harvard. a man of wide\ninterests, he spent the\n1914\u201318 war as a pilot in\nthe austro-hungarian\narmy, gave the first\nuniversity course on\npowered flight, and made\ncontributions to\naeronautics, aerodynamics\nand fluid dynamics as well\nas philosophy, probability\nand statistics; he was also\nan authority on the\naustrian poet rainer\nmaria rilke. he is now\nperhaps best known for\nhis frequency theory basis\nfor probability.\n\n "}, {"Page_number": 187, "text": "5.2 \u00b7 exponential family models\n\n175\n\nexample 5.12 (multinomial density) the multinomial density with denominator\nm and probability vector \u03c0 = (\u03c01, . . . , \u03c0 p)t is\n\nm!\n\ny1!\u00b7\u00b7\u00b7 yp!\n\n\u03c0 y1\n1\n\n\u00b7\u00b7\u00b7\u03c0 yp\n\np \u221d exp{y1 log \u03c01 + \u00b7\u00b7\u00b7 + yp log \u03c0 p}\n\n= exp{y1 log \u03c01 + \u00b7\u00b7\u00b7 + yp\u22121 log \u03c0 p\u22121\n= exp{y1\u03b81 + \u00b7\u00b7\u00b7 + yp\u22121\u03b8 p\u22121 \u2212 \u03ba(\u03b8)},\n\n+ (m \u2212 y1 \u2212 \u00b7\u00b7\u00b7 \u2212 yp\u22121) log(1 \u2212 \u03c01 \u2212 \u00b7\u00b7\u00b7 \u2212\u03c0 p\u22121)}\n\nwhere\n\n\u03c0r =\n\ne\u03b8r\n\n1 + e\u03b81 + \u00b7\u00b7\u00b7 +e \u03b8 p\u22121\n\n\u03ba(\u03b8) = m log (1 + e\u03b81 + \u00b7\u00b7\u00b7 + e\u03b8 p\u22121).\n\n,\n\nthis is a minimal representation of a natural exponential family of order p \u2212 1 with\ns(y) = (y1, . . . , yp\u22121)t, n = (\u2212\u221e,\u221e) p\u22121 and\n(cid:23)\nf0(y) = p\ny is a subset of the scaled p-dimensional simplex\n\n(y1, . . . , yp) : y1, . . . , yp \u2208 {0, . . . ,m },\n(cid:23)\n\n\u2212mm!\ny1!\u00b7\u00b7\u00b7 yp!\n\nyr = m\n\n, y =\n\n(cid:6)\n\n(cid:13)\n\n(cid:13)\n\n;\n\n(cid:6)\n\n(y1, . . . , yp) : 0\u2264 y1, . . . , yp \u2264 m,\n\nyr = m\n\n.\n\nc(y) =\n\nnow\n\ne{s(y )} =\n\nm\n\n(e\u03b81 , . . . ,e \u03b8 p\u22121),\n\nmany multinomial models are curved exponential families. in example 4.38, for\n\n1 + e\u03b81 + \u00b7\u00b7\u00b7 +e \u03b8 p\u22121\nand as e(y p) = m \u2212 e(y1) \u2212 \u00b7\u00b7\u00b7 \u2212e( y p\u22121), the expectation space in which \u00b5(\u03b8) =\ne(y ) varies equals int c(y): the model is steep.\ninstance, the abo blood group data had p = 4 groups with\n\u03c0a = \u03bb2\n(5.15)\nwhere \u03bba + \u03bbb + \u03bbo = 1. this is a (3, 2) curved exponential family. in the full family\nof order p, the probabilities \u03c0a, \u03c0b and \u03c0ab vary in the set\n\n+ 2\u03bbb \u03bbo , \u03c0o = \u03bb2\n\n+ 2\u03bba\u03bbo , \u03c0b = \u03bb2\n\n, \u03c0ab = 2\u03bba\u03bbb ,\n\no\n\na\n\nb\n\na = {(\u03c0a, \u03c0b , \u03c0ab) : 0\u2264 \u03c0a, \u03c0b , \u03c0ab \u2264 1, 0 \u2264 \u03c0a + \u03c0b + \u03c0ab \u2264 1},\n\nshown in figure 5.6. in the sub-family given by (5.15), when \u03bbo is fixed we have\n\u03bba + \u03bbb = 1 \u2212 \u03bbo, and as \u03bba varies from 0 to 1 \u2212 \u03bbo, (\u03c0a, \u03c0b , \u03c0ab) traces a curve\nfrom (0, 1 \u2212 \u03bb2\n, 0, 0) shown in the figure. as \u03bbo varies from 0 to 1,\n+ 2 p\u03bbo , (1 \u2212 \u03bba \u2212 \u03bbo)2 + 2(1 \u2212 \u03bba \u2212 \u03bbo)\u03bbo ,\n\n(\u03c0a, \u03c0b , \u03c0ab) = (cid:24)\n\n, 0) to (1 \u2212 \u03bb2\n\no\n\no\n\n\u03bb2\na\n2\u03bba(1 \u2212 \u03bba \u2212 \u03bbo)\n\n(cid:25)\n\ntraces out the intersection of a cone with the set a. thus although any value of\n(\u03c0a, \u03c0b , \u03c0ab) inside the tetrahedron with corners (0, 0, 0), (0, 0, 1), (0, 1, 0) and\n(1, 0, 0) is possible under the full model, the curved submodel restricts the probabil-\n(cid:1)\nities to the hatched surface.\n\n "}, {"Page_number": 188, "text": "176\n\n5 \u00b7 models\n\n1\n\npab 0.5\n\n0\n\n0\n0\n\n0.5\n0.5\n\npa\npa\n\n0\n\n0.5\n\npb\n\n1\n\nfigure 5.6 parameter\nspace for four-category\nmultinomial model. the\nfull parameter space for\n(\u03c0a, \u03c0b , \u03c0ab ) is the\ntetrahedron with corners\n(0, 0, 0), (0, 0, 1), (0, 1, 0)\nand (1, 0, 0), whose outer\nface is shaded. the other\nparameter \u03c0o =\n1 \u2212 \u03c0a \u2212 \u03c0b \u2212 \u03c0ab . the\ntwo-parameter sub-model\ngiven by (5.15) is shown\nby the hatched surface.\n\n5.2.3 inference\nlet y1, . . . ,y n be a random sample from an exponential family of order p. their joint\ndensity is\n\nf (y j ; \u03c9) = exp\n\ns(y j )t\u03b8(\u03c9) \u2212 nb(\u03c9)\n\nf0(y j ), \u03c9 \u2208 \u0001,\n\n(5.16)\n\n(cid:11)\nn(cid:6)\nj=1\n\nn(cid:2)\nj=1\n\nand consequently the density of s = (cid:1)\n\nf (s; \u03c9) =\n\nf (y j ; \u03c9) dy = exp{s t\u03b8(\u03c9) \u2212 nb(\u03c9)}\n\nf0(y j ) dy\n\nn(cid:2)\nj=1\n\n(cid:15)\n\n(cid:11)\n\n(cid:15)\n\ns(y j ) is\n\nn(cid:2)\nj=1\n= exp{s t\u03b8(\u03c9) \u2212 nb(\u03c9)}g0(s),\n(cid:12)\n\nsay, where the integral is over\n\n(y1, . . . , yn) : y1, . . . , yn \u2208 y,\n\ns(y j ) = s\n\n.\n\n(cid:12)\n\nn(cid:2)\nj=1\n\nn(cid:6)\nj=1\n\nhence s too has an exponential family density of order p. that is, the sum of n\nindependent variables from an exponential family belongs to the same family, with\ncumulant-generating function n\u03ba(\u03b8) = nb(\u03c9). the factorization criterion (4.15) ap-\nplied to (5.16) implies that s is a sufficient statistic for \u03c9 based on y1, . . . ,y n, and if\nf (y; \u03c9) is aminimal representation, s is minimal sufficient (exercise 5.2.12). thus\ninference for \u03c9 may be based on the density of s, while the joint density of y1, . . . ,y n\ngiven the value of s is independent of \u03c9:\n\nf (y1, . . . , yn; \u03c9) = f (y1, . . . , yn | s) f (s; \u03c9).\n\n(5.17)\n\nthis decomposition allows us to split the inference into two parts, corresponding to\nthe factors on its right, the first of which may be used to assess model adequacy. if\nsatisfied of an adequate fit, we use the second term for inference on \u03c9. we now discuss\nthese aspects in turn.\n\n "}, {"Page_number": 189, "text": "5.2 \u00b7 exponential family models\n\n177\n\nmodel adequacy\nthe argument for using the first factor on the right of (5.17) to assess model adequacy\nis that the value of \u03c9 is irrelevant to deciding if f (y; \u03c9) fits the random sample\ny1, . . . ,y n. hence we should assess fit using the conditional distribution of y given\ns; see example 4.10.\n\nexample 5.13 (poisson density)\nif y1, . . . ,y n is a random sample from a poisson\nthe natural observation is s(y j ) = y j . hence s = (cid:1)\ndensity with mean \u00b5, their common cumulant-generating function is \u00b5(et \u2212 1) and\ny j has cumulant-\ngenerating function n\u00b5(et \u2212 1). the joint conditional density of y1, . . . , yn given that\ns = s,\n\ns(y j ) = (cid:1)\n\nf (y1, . . . , yn | s) = f (y1, . . . , yn; \u03b8)\n(cid:26)\n\u2212\u00b5/y j !\nn\nj=1\n(cid:14)\n(n\u00b5)se\u2212n\u00b5/s!\ny1!\u00b7\u00b7\u00b7yn! n\n0,\n\nf (s; \u03b8)\n\u00b5y j e\n\n\u2212s ,\n\n=\n\n=\n\ns!\n\ny1 + \u00b7\u00b7\u00b7 + yn = s,\notherwise,\n\nis multinomial with denominator s and n \u00d7 1 probability vector (n\ndensity is independent of \u00b5 by its construction.\n\n\u22121, . . . ,n\n\n\u22121). this\n\n(cid:1)\n\nthe mean and variance of a poisson variable both equal \u00b5, sopoissonness of a\nrandom sample of counts can be assessed by comparing their average y and sample\nsion, which is suggested if p = (cid:1)\nvariance (n \u2212 1)\n(y j \u2212 y )2. acommon problem with such data is overdisper-\n\u22121\n(y j \u2212 y )2/y greatly exceeds n \u2212 1. how big is\n\u2018greatly\u2019? as(cid:8)\u00b5 = y is the maximum likelihood estimate of \u00b5, p is pearson\u2019s statistic\n(section 4.5.3) and has an asymptotic \u03c7 2\nn\u22121 distribution. the argument above suggests\nof s = (cid:1)\nthat we assess if p is large compared to its conditional distribution given the value\ny j = ny , sothe distribution we seek is that of p conditional on y . the\nconditional mean and variance of p are (n \u2212 1) and 2(n \u2212 1)(1 \u2212 s\n.= 2(n \u2212 1),\nand the conditional distribution of p is very close to \u03c7 2\nn\u22121 unless s and n are both\nvery small. hence the poisson dispersion test compares p to the \u03c7 2\nn\u22121 distribution,\nwith large values suggesting that the counts are more variable than poisson data\nwould be.\n\n\u22121)\n\nin table 2.1, for example, the daily numbers of arrivals are 16, 16, 13, 11, 14, 13,\n12, so p takes value 1.6, to be treated as \u03c7 2\n6 , sothe counts seem under- rather than\noverdispersed. in example 4.40, by constrast, with counts 1, 5, 3, 2, 2, 1, 0, 0, 2, 1,\n1, 7, 11, 4, 7, 10, 16, 16, 9, 15, we have p = 99.92, which is very large compared\n.= 0 to 12decimal places. as one\nto the \u03c7 2\nmight expect, these data are highly overdispersed relative to the poisson model.\n\n19 distribution; and in fact pr(p \u2265 99.92)\n\nanother possibility is that although all poisson, the y j have different means. in\nexample 4.40 we compared the changepoint model under which y1, . . . ,y \u03c4 and\ny\u03c4+1, . . . ,y n have different means with the model of equal means. the comparison\ninvolved the likelihood ratio statistic, whose exact conditional distribution was\n(cid:1)\nsimulated under the simpler model; see figure 4.9.\n\n "}, {"Page_number": 190, "text": "178\n\n5 \u00b7 models\n\nexample 5.14 (normal model) the normal density may be written\n\n(cid:14)\n\n(cid:16)\n\nf (y; \u00b5, \u03c3 2) =\n\n1\n\n(cid:14)\n\nexp\n\n(2\u03c0)1/2\u03c3\n\u00b5\n\n= exp\n\n\u03c3 2 y \u2212 1\n\n\u2212 1\n2\u03c3 2 (y \u2212 \u00b5)2\n2\u03c3 2 y2 \u2212 \u00b52\n\n2\u03c3 2\n\n\u2212 log \u03c3 \u2212 1\n2\n\n(cid:16)\n\nlog(2\u03c0)\n\n.\n\n(5.18)\n\nthis is a minimal representation of an exponential family of order 2 with\n\n\u03c9 = (\u00b5, \u03c3 2) \u2208 \u0001 = (\u2212\u221e,\u221e) \u00d7 (0,\u221e),\n\n\u03b8(\u03c9)t = (\u00b5/\u03c3 2, 1/(2\u03c3 2)) \u2208 n = (\u2212\u221e,\u221e) \u00d7 (0,\u221e),\ns(y)t = (y,\u2212y2),\n\u03ba(\u03b8) = \u03b8 2\n\nlog(2\u03b82),\n\n/(4\u03b82) \u2212 1\n2\n\n1\n\narising from tilting the standard normal density (2\u03c0)\n\n\u22121/2e\n\n\u2212y2/2.\n\n\u22121\n\nn\u22121.\n\n(cid:1)\n\n\u22121\u03c3 2\u03c7 2\n\nwe now consider how decomposition (5.17) applies for the normal model with n >\ny j ,\u2212(cid:1)\n2. when y1, . . . ,y n is a random sample from (5.18), our general discussion implies\n(cid:1)\ny 2\nthat (\nj ) isminimal sufficient. as this is in 1\u20131 correspondence with y ,\ns2 = (n \u2212 1)\n(y j \u2212 y )2, our old friends the average and sample variance are also\nminimal sufficient. when n > 1 the joint distribution of y and s2 is nondegenerate\nwith probability one, and (3.15) states that they are independently distributed as\nn (\u00b5, \u03c3 2/n) and (n \u2212 1)\nin order to compute the conditional density of y1, . . . ,y n given y and s, it is\n(cid:1)\n(cid:1)\nneatest to set e j = (y j \u2212 y )/s and consider the conditional density of e1, . . . , en.\n= n \u2212 1, the random vector (e1, . . . , en) \u2208 irn lies on the\nas\nintersection of the hypersphere of radius n \u2212 1 and the hyperplane\ne j = 0. as this\nis a (n \u2212 2)-dimensional subset of irn, the joint density of e1, . . . , en is degenerate\nbut that of e3, . . . , en is not.\nto find the joint density of t3 = e3, . . . , tn = en given t1 = y and t2 = s, we\nneed the jacobian of the transformation from y1, . . . , yn to t1, . . . ,t n. inorder to\n(cid:1)\nobtain this jacobian, we first note that y j = t1 + t2t j , for j = 3, . . . ,n . as\ne j = 0\nand\n\ne j = 0 and\n\n(cid:1)\n\n(cid:1)\n\ne 2\nj\n\ne2\nj\n\n= n \u2212 1, we can write\ne1 + e2 = \u2212 n(cid:6)\n\nj=3\n\nt j ,\n\nn \u2212 1 \u2212 e2\n\n1\n\n\u2212 e2\n\n2\n\n= n(cid:6)\n\nj=3\n\n,\n\nt 2\nj\n\nimplying that there are functions h1 and h2 such that\n\ne1 = h1(t3, . . . ,t n),\n\ne2 = h2(t3, . . . ,t n),\n\nwhich in turn gives\n\ny1 = t1 + t2h1(t3, . . . ,t n),\n\ny2 = t1 + t2h2(t3, . . . ,t n).\n\n "}, {"Page_number": 191, "text": "5.2 \u00b7 exponential family models\nlet hi j = \u2202hi (t3, . . . ,t n)/\u2202t j . the jacobian we seek is\n\n(cid:22)(cid:22)(cid:22)(cid:22) \u2202(y1, . . . , yn)\n\n\u2202(t1, . . . ,t n)\n\n(cid:22)(cid:22)(cid:22)(cid:22) =\n\n(cid:22)(cid:22)(cid:22)(cid:22)(cid:22)(cid:22)(cid:22)(cid:22)(cid:22)(cid:22)(cid:22)(cid:22)(cid:22)\n\n1\n1\n1\n1\n...\n1\n\nh1\nh2\nt3\nt4\n...\ntn\n\nt2h13\nt2h23\n\nt2h14\nt2h24\n\nt2\n0\n...\n0\n\n0\nt2\n...\n0\n\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n...\n\u00b7\u00b7\u00b7\n\nt2h1n\nt2h2n\n\n0\n0\n...\nt2\n\n179\n\n(cid:22)(cid:22)(cid:22)(cid:22)(cid:22)(cid:22)(cid:22)(cid:22)(cid:22)(cid:22)(cid:22)(cid:22)(cid:22)\n\n= t n\u22122\n\n2\n\n(cid:6)\n\nh\n\n(t3, . . . ,t n)\n\n= sn\u22122 h(e),\n\n(5.19)\n\nsay. hence\n\nf (e3, . . . ,e n | y, s) = f (y1, . . . , yn; \u00b5, \u03c3 2)sn\u22122 h(e)\n\nf (y; \u00b5, \u03c3 2) f (s; \u03c3 2)\n\n\u221d h(e)\n\nafter a straightforward calculation. as this depends on e1, . . . ,e n alone, the corre-\nsponding random variables e1, . . . , en are independent of y and s2.\n\nthus assessment of fit of the normal model should be based on the raw residuals\ne1, . . . ,e n. one simple tool is a normal probability plot of the e j , which should be a\nstraight line of unit gradient through the origin. such plots and variants are common\nin regression (section 8.6.1). further support for use of the e j for model checking is\n(cid:1)\ngiven in section 5.3.\n\nlikelihood\nlet y1, . . . ,y n be a random sample from an exponential family of order p. inference\nfor the parameter may be based on the sufficient statistic s = n\ns(y j ), which\nalso belongs to a natural exponential family of order p, with support s, say. hence\nthe log likelihood may be written\n\n(cid:1)\n\n\u22121\n\n(cid:6)(\u03c9) \u2261 n {st\u03b8(\u03c9) \u2212 b(\u03c9)} =n [st\u03b8(\u03c9) \u2212 \u03ba {\u03b8(\u03c9)}], \u03c9 \u2208 \u0001,\n\n(cid:14)\n\n(cid:16)\n\n,\n\nand the score vector and observed information matrix are given by\n\nu (\u03c9) = \u2202(cid:6)(\u03c9)\nj (\u03c9)rs = \u2212 \u2202 2(cid:6)(\u03c9)\n\u2202\u03c9r \u2202\u03c9s\n\n= \u2202\u03b8 t\n\u2202\u03c9\n= \u2212 \u2202 2\u03b8 t\n\u2202\u03c9r \u2202\u03c9s\n\ns \u2212 \u2202\u03ba(\u03b8 )\n(cid:14)\n\n\u2202\u03c9\n\n\u2202\u03b8\n\nn\n\nn\n\n(cid:14)\n\n(cid:16)\n\n+ \u2202\u03b8 t\n\u2202\u03c9r\n\n\u2202 2\u03ba(\u03b8)\n\u2202\u03b8 \u2202\u03b8 t\n\nn\n\n(cid:16)\n\n\u2202\u03b8\n\u2202\u03c9s\n\n.\n\ns \u2212 \u2202\u03ba(\u03b8 )\n\n\u2202\u03b8\n\nthe observed information is random unless the family is in natural form, in which\ncase \u03b8 = \u03c9 and hence \u2202 2\u03b8/\u2202\u03c9r \u2202\u03c9s = 0; then i (\u03b8) = e{j (\u03b8)} = j (\u03b8).\nif the family is steep, there is a 1\u20131 relation between the interior of the closure of s,\nint c(s), the expectation space m of s, and the natural parameter space n = \u03b8(\u0001).\nthus if s \u2208 int c(s), there is a single value of \u03b8 such that s = \u00b5(\u03b8) and u(\u03b8) = 0,\nand moreover there is a 1\u20131 map between(cid:8)\u03b8 and (cid:8)\u03c9. hence the maximum likelihood\n\nestimators satisfy\n\n(cid:8)\u00b5 = \u00b5((cid:8)\u03b8) = \u00b5{\u03b8((cid:8)\u03c9)} = s.\n\n "}, {"Page_number": 192, "text": "180\n\n5 \u00b7 models\nhood. moreover, as \u0001 is open and(cid:8)\u03c9 \u2208 \u0001, standard likelihood asymptotics will apply,\nthus the likelihood equation has just one solution, which maximizes the log likeli-\nso(cid:8)\u03c9\ns (cid:13)\u2208 m,\nstandard asymptotics will break down. the same difficulty could arise if the true\nparameter lies on the boundary of the parameter space.\n\n\u22121} and 2{(cid:6)((cid:8)\u03c9) \u2212 (cid:6)(\u03c9)}\n\np. ifthe model permits\n\n.\u223c n{\u03c9, i (\u03c9)\n\n.\u223c \u03c7 2\n\nexample 5.15 (uniform density) the average y of a random sample from (5.8)\n\nmust lie in the interval (0, 1). given y, the maximum likelihood estimate(cid:8)\u03b8 is read off\nfrom the right panel of figure 5.3 as the value of \u03b8 on the horizontal axis for which\n\u00b5(\u03b8) = y on the vertical axis.\nas mentioned in example 5.7, when \u03b8 is restricted to \u0001 = [0,\u221e) the family is\nnot steep, because m = [1/2, 1) (cid:13)= (0, 1) = int c(y). a value y < 1/2 ispossible\nfor any sample size and any \u03b8 \u2208 \u0001, and as(cid:8)\u03b8 = 0 isthe maximum likelihood estimate\nfor any such y, the 1\u20131 mapping between y and(cid:8)\u03b8 is destroyed. furthermore, this\n\u0001 is not open, so the limiting distribution of(cid:8)\u03b8 and the likelihood ratio statistic are\nnon-standard if \u03b8 = 0; see example 4.39.\n\n(cid:1)\n\nexample 5.16 (binomial density) the binomial model with denominator m,\nprobability 0 < \u03c0 <1 and natural parameter \u03b8 = log{\u03c0/(1 \u2212 \u03c0)} \u2208(\u2212\u221e, \u221e) has\ny = {0, 1, . . . ,m } and int c(y) = m = (0, m). the average r of a random sample\nr1, . . . , rn lies outside (0, m) with probability\n\npr(r1 = \u00b7\u00b7\u00b7 = rn = 0) + pr(r1 = \u00b7\u00b7\u00b7 = rn = m) = (1 \u2212 \u03c0)mn + \u03c0 mn > 0,\nso the maximum likelihood estimator (cid:8)\u03b8 = log\nmay not be finite. as\nthe family is steep, a unique value of \u03b8 corresponds to each r \u2208 m, sothe only\nproblem that can arise is that (cid:8)\u03b8 = \u00b1\u221e with small probability. on the other hand\npr(|(cid:8)\u03b8| = \u221e) \u2192 0 exponentially fast as n \u2192 \u221e, soinfinite (cid:8)\u03b8 is rare in practice,\nthough not unknown. it corresponds to(cid:8)\u03c0 = 0 or(cid:8)\u03c0 = 1.\n\nr/(m \u2212 r)\n\n(cid:9)\n\n(cid:10)\n\nthis difficulty also arises with other discrete exponential families.\n\n(cid:1)\n\nexample 5.17 (normal density) example 4.18 gives the score and information\nquantities for a sample from the normal model in terms of \u00b5 and \u03c3 2; inthis\nparametrization the observed information is random. in example 4.22 we saw that\nthe log likelihood (cid:6)(\u00b5, \u03c3 2) isunimodal and that the maximum likelihood estimators\nare the sole solution to the likelihood equation; this is an instance of the general result\n(cid:1)\nabove.\n\nderived densities\nvarious models derived from exponential families are themselves exponential fami-\nlies, and this can be useful in inference.\n\nconsider a natural exponential family of order p with st and \u03b8 t partitioned as\n2) and (\u03c8 t, \u03bbt), where s1 and \u03c8 have dimension q < p. the marginal density\n, st\n\n(st\n1\n\n "}, {"Page_number": 193, "text": "5.2 \u00b7 exponential family models\n(cid:15)\n\n(cid:9)\n\nf (s2; \u03b8) =\n\nof s2, obtained by integration over the values of s1, is\n(cid:10)\n\u03bb \u2212 \u03ba(\u03b8)\n\u03c8 + s t\n(cid:10)(cid:15)\ns t\n(cid:24)\n1\n\u03bb \u2212 \u03ba(\u03b8)\n(cid:10)\nexp\n\u03bb \u2212 \u03ba(\u03b8) + d\u03c8 (s2)\n\n= exp\n= exp\n\n(cid:9)\n(cid:9)\n\nexp\n\ns t\n2\n\ns t\n1\n\n,\n\n2\n\ns t\n2\n\ng0(s1, s2) ds1\n(cid:25)\n\n\u03c8\n\ng0(s1, s2) ds1\n\n181\n\nsay, so for fixed \u03c8 the marginal density of s2 is an exponential family with natural\nparameter \u03bb.\nthe conditional density of s1 given s2 = s2 is\n\u03c8 + s t\n(cid:9)\ns t\n2\n\u03c8 \u2212 \u03bas2(\u03c8)\n\nfs1|s2(s1 | s2; \u03b8) = exp\n= exp\n\n\u03bb \u2212 \u03ba(\u03b8) + d\u03c8 (s2)\ngs2(s1),\n\n\u03bb \u2212 \u03ba(\u03b8)\n\ng0(s1, s2)\n\ns t\n1\nexp\n\n(cid:9)\n(cid:9)\n\n(cid:10)\n\n(cid:10)\n\n(cid:10)\n\n2\n\ns t\n1\n\nsay. this is an exponential family of order q with natural parameter \u03c8, but the base\ndensity and cumulant-generating function depend on s2. such a removal of \u03bb by\nconditioning is a powerful way to deal with nuisance parameters.\n\nn(cid:2)\nj=1\n\n\u03bb\u03ba j y\u03ba j\u22121\n\u0001(\u03ba j )\n\nindependent gamma variables y1, . . . ,y n with\n\nexample 5.18 (gamma density)\nscale parameter \u03bb and shape parameters \u03ba1, . . . , \u03ban have joint density\ny\u03ba j\u22121\n\u0001(\u03ba j )\nas y j has cumulant-generating function \u2212\u03ba j log(1 \u2212 \u03bbt), s1 = s = (cid:1)\n\n(cid:1)\nexp(\u2212\u03bby j ) = \u03bb\n(cid:1)\ny j is gamma\n\u03ba j . the conditional density of y1, . . . ,y n given s = s is\n(cid:25)\n\nwith parameters \u03bb and\n\n(cid:20)\n\u2212\u03bb\n\nn(cid:6)\nj=1\n\nn(cid:2)\nj=1\n\n\u03ba j exp\n\n(cid:24)(cid:1)\n\n(cid:21)\n\ny j\n\n.\n\nj\n\nj\n\n(cid:26)\n\n\u0001\nn\nj=1\n\n\u03ba j\n\u0001(\u03ba j )\n\n\u2212n\n\ns\n\n(cid:28)\u03ba j\u22121\n\ny j\ns\n\n(cid:27)\n\nn(cid:2)\nj=1\n(cid:24)(cid:1)\n\n,\n\ny j > 0,\n\ny j = s.\n\nn(cid:6)\nj=1\n\nthus the joint density of u1 = y1/s, . . . , un = yn/s,\n(cid:26)\nu\u03ba j\u22121\nf (u1, . . . ,u n; \u03ba1, . . . , \u03ban) = \u0001\n\n(cid:25)\n\n,\n\nj\n\n\u03ba j\n\u0001(\u03ba j )\n\nn\nj=1\n\nn(cid:2)\nj=1\n\nn(cid:6)\nj=1\n\nu j > 0,\n\nu j = 1,\n\n(5.20)\n\nlies on the simplex in n dimensions; it is called the dirichlet density. hence we may\nbase inferences for \u03ba1, . . . , \u03ban on the conditional density of y1, . . . ,y n given their\n(cid:1)\nsum, or equivalently on the observed values of the u j .\n\nthe discussion above suggests that we may write\n\nf (s; \u03b8) = fs1|s2(s1 | s2; \u03c8) fs2(s2; \u03c8, \u03bb).\n\n(5.21)\nif the model can be reparametrized in terms of a ( p \u2212 q) \u00d7 1 vector \u03c1 = \u03c1(\u03c8, \u03bb)\nwhich is variation independent of \u03c8, insuch a way that the second term on the right\n\n "}, {"Page_number": 194, "text": "182\n\n5 \u00b7 models\n\nof (5.21) depends only on \u03c1, then s2 is said to be a cut. the log likelihood based\non (5.21) then has form (cid:6)1(\u03c8) + (cid:6)2(\u03c1), maximum likelihood estimates of \u03c1 and \u03c8\ndo not depend on each other, and the observed information matrix is block diagonal.\ninferences on \u03c8 and \u03c1 may be made separately, using the conditional density of s1\ngiven s2 and the marginal density of s2. the cut most commonly encountered in\npractice arises with poisson variables; see example 7.34 and page 501.\n\nexercises 5.2\n1 here is a version of h\u00a8older\u2019s inequality: let\n\np > 1, and let g(y) and h(y) be any two real functions such that the integrals\n\nf (x) be adensity supported in [ a, b], let\n\n(cid:15)\n\nb\n\n|g(y)| p f (y) dy,\n\u22121 = 1. then\n\na\n\n\u22121 + q\n\n(cid:15)\n\na\n\nb\n\n|h(y)|q f (y) dy,\n\nare finite, where p\n\n(cid:15)\n\ng(y)h(y) f (y) dy \u2264\n\nb\n\n|g(y)| p f (y) dy\n\n(cid:14)(cid:15)\n\na\n\n(cid:16)1/ p (cid:14)(cid:15)\n\n(cid:16)1/q\n\n.\n\nb\n\n|h(y)|q f (y) dy\n\na\n\nif g and h are both non-zero, there is equality if and only if c|g(y)| p = d|h(y)|q for positive\nconstants c and d.\nshow strict convexity of the cumulant-generating function \u03ba(\u03b8) of an exponential family.\n\u2212y, y > 0, and (b) f0(y) =\n\n2 what natural exponential families are generated by (a) f0(y) = e\n\n\u2212|y|\n\n, \u2212\u221e < y < \u221e?\n\n1\n2 e\n\n3 which of examples 4.1\u20134.6 are exponential families? what about the u (0, \u03b8) density?\n4 show that the gamma density (2.7) is an exponential family. what about the inverse gamma\n\ndensity, for 1/y when y is gamma?\n\n5 show that the inverse gaussian density\n\n(cid:19)1/2\n\n(cid:18)\n\n\u03bb\n\n2\u03c0 y3\n\nf (y; \u00b5, \u03bb) =\n\nexp{\u2212\u03bb(y \u2212 \u00b5)2/(2\u00b52 y)},\n\ny > 0, \u03bb, \u00b5 > 0,\n\n6 find the exponential families with variance functions (i) v (\u00b5) = a\u00b5(1 \u2212 \u00b5), m = (0, 1),\n7 for what values of a is there an exponential family with variance function v (\u00b5) = a\u00b5,\n\nis an exponential family of order 2. give a general form for its cumulants.\n(ii) v (\u00b5) = a\u00b52, m = (0,\u221e), and (iii) v (\u00b5) = a\u00b52, m = (\u2212\u221e, 0).\nm = (0,\u221e)?\n8 show that the n (\u00b5, \u00b52) model is a curved exponential family and sketch how the density\nchanges as \u00b5 varies in (\u2212\u221e, 0) \u222a (0,\u221e). sketch also the subset of the natural parameter\nspace for the n (\u00b5, \u03c3 2) distribution generated by this model.\n\n9 find a connection between example 4.11 and (5.20), and hence suggest methods of\n\nchecking the fit of the exponential model.\n\n10 explain how (5.20) may be generated as an exponential family, by showing that it gener-\n\nalizes (5.14).\n\n(cid:1)\n\n11 use example 5.18 to construct a simulation algorithm for dirichlet random variables.\n12 show that\n\ns(y j ) isminimal sufficient for the parameter \u03c9 of an exponential family of\n\norder p in a minimal representation.\n\n "}, {"Page_number": 195, "text": "5.3 \u00b7 group transformation models\n\n5.3 group transformation models\n\n183\n\nanother important class of models stems from observing that many inferences should\nhave invariance properties. if, for instance, data y are recorded in degrees celsius, one\nmight obtain a conclusion s(y) directly from the original data, or one might transform\nthem to degrees fahrenheit, giving g(y), say, obtain the conclusion s{g(y)} in these\n\u22121[s{g(y)}].\nterms, and then back-transform to celsius scale, giving conclusion g\n\u22121[s{g(y)}] = s(y). the transformation from celsius to\nit is clearly essential that g\nfahrenheit is just one of many possible invertible linear transformations that might\nbe applied to y, however, any of which should leave the inference unchanged. more\ngenerally we might insist that inferences be invariant when any element g of a group\nof transformations acts on the sample space. this section explores some consequences\nof this requirement.\n\na group g is a mathematical structure having an operation \u25e6 such that:\nr if g, g\nr g contains an identity element e such that e \u25e6 g = g \u25e6 e = g for each g \u2208 g;\nr each g \u2208 g possesses an inverse g\n\n\u22121 \u2208 g such that g \u25e6 g\n\n(cid:6) \u2208 g, then g \u25e6 g\n\n\u22121 \u25e6 g = e.\n\n\u22121 = g\n\n(cid:6) \u2208 g;\n\nand\n\na subgroup is a subset of g that is also a group.\n\n(cid:6)\n\n(cid:6)\n\nif and only if there is a g \u2208 g such that y = g(y\n\na group action arises when elements of a group act on those of a set y. in the\npresent case the group elements g\u03b8 typically correspond to elements of a parameter\nspace \u0001 and y is the sample space of a random variable y . the action of g on y,\ng(y), say, is defined for each y \u2208 y and g(y) is anelement of y for each g \u2208 g.\nsetting y \u2248 y\n) gives an equivalence\nrelation, which partitions y into equivalence classes called orbits and labelled by an\nindex a, say. each y belongs to precisely one orbit, and can be represented by a\nand its position on the orbit. hence we can write y = g(a) for some g \u2208 g. ifthis\nrepresentation is unique for a given choice of index, the group action is said to be free.\nexample 5.19 (location model) let y = \u03b8 + \u03b5, where \u03b8 \u2208 \u0001 = ir and \u03b5 is a\nscalar random variable with known density f (y), where y \u2208 ir. the density of y\nis f (y \u2212 \u03b8) = f (y; \u03b8), say, and that of \u03b8(cid:6) + y = \u03b8(cid:6) + \u03b8 + \u03b5 is f (y; \u03b8 + \u03b8(cid:6)\n). thus\nto y changes the parameter of the density. taking \u03b8(cid:6) = \u2212\u03b8 gives the baseline\nadding \u03b8(cid:6)\ndensity f (y; 0) = f (y) of\u03b5 .\nhere group elements may be written g\u03b8 , corresponding to the parameters \u03b8, and\nthe group operation is equivalent to addition. hence g\u03b8 \u25e6 g\u03b8(cid:6) = g\u03b8+\u03b8(cid:6), the identity e\nis g0 and the inverse of g\u03b8 is g\u2212\u03b8 . each element of the group corresponds to a point\nin \u0001, but it induces a group action g\u03b8 (y) = \u03b8 + y on the sample space.\nfor arandom sample y1, . . . ,y n, wetake y = irn and interpret expressions such\nas g\u03b8 (y ) = \u03b8 + y as vectors, with \u03b8 \u2261 \u03b81n and y = (y1, . . . ,y n)t. then y and y\n(cid:6)\nbelong to the same orbit if there exists a g\u03b8 such that g\u03b8 (y) = y\n, that is, there exists\na \u03b8 such that \u03b8 + y = y\nis a location shift of y. ontaking\n\u03b8 = y\n, implying that we can represent the orbit by\n\n(cid:6) \u2212 y we see that y \u2212 y = y\n\n, and this implies that y\n\n(cid:6) \u2212 y\n\n(cid:6)\n\n(cid:6)\n\n(cid:6)\n\n(cid:6)\n\n1n is the n \u00d7 1 vector of\nones.\n\n "}, {"Page_number": 196, "text": "5 \u00b7 models\n\n(cid:6)\n\n184\nthe vector a(y) = y \u2212 y, because this choice of index gives a(y) = a(y\n). thus y is\nequivalently written as (y \u2212 y, y), where the first term indexes the orbit and the second\nthe position of y within it. in terms of this representation we write y as gy(a) = y +\na = y + y \u2212 y = y. the group action is free because g\u03b8 (a) = y implies that \u03b8 = y.\nin geometric terms, a(y) lies on the (n \u2212 1)-dimensional hyperplane\na j = 0,\neach point of which determines a different orbit. the orbits themselves are lines\n\u03b8 + a(y) passing through these points, with \u03b8 \u2208 ir. when n = 2, each point (y1, y2)\nin ir2 is indexed by a point on the line y1 + y2 = 0, which determines the orbit, a\n(cid:1)\nstraight line perpendicular to this.\non the same orbit have the same index a = a(y), which is\nsaid to be invariant to the action of the group because its value does not depend on\nwhether y or g(y) was observed, for any g \u2208 g. it ismaximal invariant if every other\ninvariant statistic is a function of it, or equivalently\n\ntwo points y and y\n\n(cid:1)\n\n(cid:6)\n\na(y) = a(y\n\n(cid:6)\n\n) implies that y\n\n(cid:6) = g(y) for some g \u2208 g.\n\nthe distribution of a = a(y ) does not depend on the elements of g. inthe present\ncontext these are identified with parameter values, so the distribution of a does not\ndepend on parameters and is known in principle; a is said to be distribution constant. a\nmaximal invariant can be thought of as a reduced version of the data that represents it as\nclosely as possible while remaining invariant to the action ofg. insome sense it is what\nremains of y once minimal information about the parameter values has been extracted.\noften there is a 1\u20131 correspondence between the elements of g and the parameter\nspace \u0001, and then the action of g on y induces a group action on \u0001. if wecan write g\u03b8\nfor a general element of g, then g \u25e6 g\u03b8 = g\u03b8(cid:6) for some \u03b8(cid:6) \u2208 \u0001. hence g has mapped\n\u03b8 to \u03b8(cid:6)\n, thereby inducing an action on \u0001. inprinciple the action of g on \u0001 might be\ndifferent from its action on y, and it is clearer to think of two related groups g and g\u2217\n,\n\u03b8 to denote the element of g\u2217\n\u2217\nthe second of which acts on \u0001. weuse g\nthat corresponds\nto g\u03b8 \u2208 g. inmany cases the action of g\u2217\nis transitive, that is, each parameter can be\nobtained by applying an element of the group to a single baseline parameter.\n\nexample 5.20 (permutation group) permutation of the indices of a random sample\ny1, . . . ,y n should leave any inference unaffected. hence we may consider the group\nof permutations \u03c0, with g\u03c0 (y) representing the permuted version of y \u2208 irn. note that\n\u03c0\u22121 is also a permutation, as is the operation that leaves the indices of y unchanged.\nin the location model we might let g be the group containing all n! ofthe g\u03c0 in\naddition to the g\u03b8 . though well-defined on the sample space, g\u03c0 has no counterpart\nin the parameter space, and so the enlarged group is not transitive.\nto check that a(y) = (y(1) \u2212 y, . . . , y(n) \u2212 y)t is a maximal invariant, note that if\na(y) = a(y\n(cid:6)\n(cid:6)(y\n).\n\u22121\u2212y\nthis in turn implies that g\n. hence a is a maximal\ninvariant.\nif permutations are not included in the group, the same argument shows that (y1 \u2212\ny, . . . , yn \u2212 y)t is a maximal invariant. thus the maximal invariant depends on the\n(cid:1)\nchosen group.\n\n), then permutations \u03c0, \u03c0(cid:6)\n\u03c0(cid:6) \u25e6 g\u03c0 \u25e6 g\u2212y(y) = y\n\u22121\n\nexist such that g\u03c0 \u25e6 g\u2212y(y) = g\u03c0(cid:6) \u25e6 g\u2212y\n\n(cid:6) \u25e6 g\n\n(cid:6)\n\n(cid:6)\n\n "}, {"Page_number": 197, "text": "5.3 \u00b7 group transformation models\n\n185\n\nwe shall usually ignore permutations of the order of a random sample, because the\n\ndiscussion below is simpler if the group considered is transitive.\n\nequivariance\na statistic s = s(y ) defined on y and taking values in the parameter space \u0001 is said\nto be equivariant if s(g\u03b8 (y )) = g\n\u03b8 (s(y )) for all g\u03b8 \u2208 g. often s is chosen to be an\n\u2217\nestimator of \u03b8, and then it is called an equivariant estimator. maximum likelihood\nestimators are equivariant, because of their transformation property, that if \u03c6 = \u03c6(\u03b8)\nis a 1\u20131 transformation of the parameter \u03b8, then (cid:8)\u03c6 = \u03c6((cid:8)\u03b8), where (cid:8)\u03b8 = s(y ) is the\n\u03c6 \u2208 g\u2217\n\u2217\nand g\u03c6(y ) isthe transformation of y whose maximum likelihood estimator is(cid:8)\u03c6, then\n(cid:8)\u03c6 = s(g\u03c6(y )), while \u03c6((cid:8)\u03b8) = g\n\nmaximum likelihood estimator of \u03b8. ifthe transformation \u03c6 corresponds to g\n\n\u03c6(s(y )). hence s(g\u03c6(y )) = g\n\u2217\n\n\u2217\n\u03c6(s(y )) for all such g\u03c6,\n\n,\n\nwhich is the requirement for equivariance.\nan equivariant estimator can be used to construct a maximal invariant. note first\nthat as s(y ) \u2208 \u0001, the corresponding group elements g\nand gs(y ) \u2208 g exist.\n\u2208 g\u2217\n\u2217\nnow consider a(y ) = g\ns(y )(y ). if a(y ) = a(y\ns(y )(y ) = g\n\u22121\n\u22121\ns(y )\n), then g\n), and it\n(cid:6) = gs(y (cid:6)) \u25e6 g\ns(y )(y ). hence a = a(y ) = g\n\u22121\n\u22121\nfollows that y\ns(y )(y ) ismaximal invariant.\nexample 5.21 (location-scale model) let y = \u03b7 + \u03c4 \u03b5, where as before \u03b5 has a\nknown density f , and the parameter \u03b8 = (\u03b7, \u03c4 ) \u2208 \u0001 = ir \u00d7 ir+. the group action is\ng\u03b8 (y) = g(\u03b7,\u03c4 )(y) = \u03b7 + \u03c4 y, so\n\n\u22121\ns(y (cid:6))(y\n\n(cid:6)\n\n(cid:6)\n\ng(\u03b7,\u03c4 ) \u25e6 g(\u00b5,\u03c3 )(y) = g(\u03b7,\u03c4 )(\u00b5 + \u03c3 y) = \u03b7 + \u03c4 \u00b5 + \u03c4 \u03c3 y = g(\u03b7+\u03c4 \u00b5,\u03c4 \u03c3 )(y).\n\n(5.22)\n\ng = (cid:9)\n\nthe set of such transformations is closed with identity g(0,1). it iseasy to check that\ng(\u03b7,\u03c4 ) has inverse g(\u2212\u03b7/\u03c4,\u03c4\u22121). therefore\n\ng(\u03b7,\u03c4 ) : (\u03b7, \u03c4 ) \u2208 ir \u00d7 ir+\nis indeed a group under the operation \u25e6 defined above.\nthe action of g(\u03b7,\u03c4 ) on a random sample is g(\u03b7,\u03c4 )(y ) = \u03b7 + \u03c4 y , with \u03b7 \u2261 \u03b71n and\ny an n \u00d7 1 vector, as in example 5.19. expression (5.22) implies that the implied\ngroup action on \u0001 is\n\n(cid:10)\n\n(\u03b7,\u03c4 )((\u00b5, \u03c3 )) = ( \u03b7 + \u03c4 \u00b5, \u03c4 \u03c3 ) .\n\u2217\ng\n\nthe sample average and standard deviation are equivariant, because with s(y ) =\n\n(y , v 1/2), where v = (n \u2212 1)\n\u22121\n\n(cid:1)\n(y j \u2212 y )2, we have\n(cid:13)\n(n \u2212 1)\n(cid:13)\n\n(cid:6)\n(cid:6)\n\n\u22121\n\n(cid:18)\ns(g(\u03b7,\u03c4 )(y )) =\n(cid:18)\n=\n= (cid:24)\n= g\n\n\u03b7 + \u03c4 y ,\n\n\u22121\n\n(n \u2212 1)\n(cid:25)\n\n\u03b7 + \u03c4 y ,\n\u03b7 + \u03c4 y , \u03c4 v 1/2\n\u2217\n(\u03b7,\u03c4 ) (s(y )) .\n\n(cid:19)\n(cid:19)\n\n(cid:23)1/2\n(cid:23)1/2\n\n(\u03b7 + \u03c4 y j \u2212 \u03b7 + \u03c4 y )2\n(\u03b7 + \u03c4 y j \u2212 \u03b7 \u2212 \u03c4 y )2\n\n "}, {"Page_number": 198, "text": "186\na maximal invariant is a = g\n(\u2212y /v 1/2, v\n\n\u22121\ns(y )(y ), and the parameter corresponding to g\n\n\u22121\ns(y ) is\n\n\u22121/2). hence a maximal invariant is the vector of residuals\n\n5 \u00b7 models\n\n(cid:20)\n\n(cid:21)\n\na = (y \u2212 y )/v 1/2 =\n\ny1 \u2212 y\nv 1/2\n\n, . . . ,\n\nyn \u2212 y\nv 1/2\n\nt\n\n,\n\n(5.23)\n\n(cid:1)\n\n(cid:1)\n\nalso called the configuration. itcan be checked directly that the distribution of a\ndepends on n and f but not on \u03b8. any function of a is invariant. if permutations are\nadded to g, amaximal invariant is a = (y(\u00b7) \u2212 y )/v 1/2, where y(\u00b7) = (y(1), . . . ,y (n))\nrepresents the vector of ordered values of y .\nthe orbits are determined by different values a of the statistic a, and y has a unique\nrepresentation as gs(y )(a) = y + v 1/2 a. hence the group action is free.\n= n \u2212 1, so a lies\nthe elements of a satisfy the equations\non an (n \u2212 2)-dimensional surface in irn. when n = 3 this is easily visualized; it\nis the circle that forms the intersection of the sphere of radius 2 with the plane\na1 + a2 + a3 = 0. the entire space ir3 is generated by first choosing an element of\nthis circle, then multiplying it by a positive number to rescale it to lie on a ray passing\nthrough the origin, and finally adding the vector y13.\nanother equivariant estimator is (y(1), y(2) \u2212 y(1)), where y(r) is the rth order statis-\ntic, and the argument above shows that the vector (y \u2212 y(1))/(y(2) \u2212 y(1)) iscorre-\nsponding maximal invariant. evidently this is just one of many possible location-scale\nshifts of a, which can be thought of as the \u2018shape\u2019 of the sample, shorn of information\n(cid:1)\nabout its location and scale.\n\na j = 0 and\n\na2\nj\n\nthe group-averse reader may wonder whether the generality of the discussion\nabove is needed to deal with our motivating example of temperatures in celsius\nand fahrenheit. in fact we have not yet raised a crucial distinction between invari-\nances intrinsic to a context and those stemming only from the mathematical structure\nof the model. invariances of the first sort are more defensible than are the second,\nbecause not every mathematical expression of a statistical problem successfully pre-\nserves aspects such the interpretation of key parameters. thus the sensible choice of\ngroup in a particular context may not be mathematically most natural. furthermore\nappeal to invariance is not sensible if external information suggests that some pa-\nrameter values should be favoured over others. invariance arguments require careful\nthought.\n\nexample 5.22 (venice sea level data) the straight-line regression model (5.2) can\nbe expressed as\n\ny = x\u03b2 + \u03b5,\n\nwhere\n\n\uf8f6\n\uf8f8 ,\n\n\uf8eb\n\uf8ed y1\n...\nyn\n\ny =\n\n\uf8eb\n\uf8ed 1\n...\n1\n\n\uf8f6\n\uf8f8 ,\n\nx1\n...\nxn\n\nx =\n\n(cid:19)\n\n(cid:18)\n\n\u03b20\n\u03b21\n\n\u03b2 =\n\n\u03b5 =\n\n,\n\n\uf8f6\n\uf8f8 .\n\n\uf8eb\n\uf8ed \u03b51\n...\n\u03b5n\n\n "}, {"Page_number": 199, "text": "an n \u00d7 n orthogonal\nmatrix of real numbers o\nhas the properties that\not o = o ot = in.\n\n5.3 \u00b7 group transformation models\nif the \u03b5 j are independent normal variables then y \u223c nn(x\u03b2, \u03c3 2 in). hence oy \u223c\nn p(o x\u03b2, \u03c3 2 in) for any n \u00d7 n orthogonal matrix o that preserves the column space\n\u22121 x o x = o x. it isstraightforward to check that\nof x, that is, such that x(x t x)\nsuch matrices form a group. now e(oy ) = x \u03b3 , where \u03b3 = (x t x)\n\u22121 x t o x\u03b2 =\n\u22121\u03b2, say, is the result of applying the corresponding group element in the parameter\na\nspace.\n\n187\n\nthe transformation giving (5.3), with\n\n(cid:18)\n\n(cid:19)\n\n\u03b20\n\u03b21\n\n(cid:18)\n\n= \u03b2 = a\u03b3 =\n\na11\na21\n\na12\na22\n\n(cid:19)\n\n(cid:18)\n\n\u03b3 =\n\n1 \u2212x\n1\n0\n\n(cid:18)\n\n(cid:19)\n\n\u03b3 =\n\n\u03b30 \u2212 \u03b31x\n\n\u03b31\n\n(cid:19)\n\n,\n\npreserves the interpretation of \u03b21 = a22\u03b31 as a rate of change of e(y ) with respect\nto time, though the time origin is shifted. from a mathematical viewpoint there is no\nreason not to take more general invertible transformations \u03b2 = a\u03b3 , for example with\na21 (cid:13)= 0, but this makes no sense statistically. moreover even with a21 = 0 not every\nchoice of a22 makes sense: taking a22 < 0 orsuch that the units of \u03b31 were seconds\n(cid:1)\nwould have little appeal.\n\nin some cases the full parameter space does not give a useful group of transforma-\ntions, but subspaces of it do. if the parameter space has form \u0001 \u00d7 \u0001, with the same\ngroup of transformations g = {g\u03bb : \u03bb \u2208 \u0001} acting on the sample space for each value\nof \u03c8, then we have a composite group transformation model.\n\nexample 5.23 (location-scale model)\nin the previous example, suppose that the\ndensity f\u03c8 of \u03b5 depends on a further parameter \u03c8. an example is the t\u03c8 density.\nthen for each fixed \u03c8 we have a location-scale model in terms of \u03bb = (\u03b7, \u03c4 ), with\ng\u03bb(y) = \u03b7 + \u03c4 y, and our previous discussion applies.\nfor each \u03c8 a maximal invariant based on a random sample y1, . . . ,y n is\na = (y \u2212 y )/v 1/2, whose distribution depends on the sample size and on f\u03c8 but\n(cid:1)\nnot on \u03bb.\n\nexercises 5.3\n1\n2\n\n(cid:1)\n\nshow that \u2248 is an equivalence relation.\nsuppose y = \u03c4 \u03b5, where \u03c4 \u2208 ir+ and \u03b5 is a random variable with known density f . show\nthat this scale model is a group transformation model with free action g\u03c4 (y) = \u03c4 y. show\nthat s1(y ) = y and s2(y ) = (\nj )1/2 are equivariant and find the corresponding maximal\ny 2\ninvariants. sketch the orbits when n = 2.\nsuppose that \u03b5 has known density f with support on the unit circle in the complex\nplane, and that y = ei \u03b8 \u03b5 for \u03b8 \u2208 ir. show that this is a group transformation model. is it\ntransitive? is the action free?\n4 write the configuration (5.23) in terms of \u03b51, . . . , \u03b5n, where y j = \u00b5 + \u03c3 \u03b5 j , and thereby\n\n3\n\nshow that its distribution does not depend on the parameters.\nshow that the gamma density with shape and scale parameters \u03c8 and \u03bb, is acomposite\ntransformation model under the mapping from y to \u03c4 y , where \u03c4 > 0.\n\n5\n\n "}, {"Page_number": 200, "text": "figure 5.7 hazard\nfunctions. left panel:\nweibull hazards with\n\u03b8 = 1 and \u03b1 = 0.5 (dots),\n\u03b1 = 1 (large dashes),\n\u03b1 = 1.5 (dashes), and\nbi-weibull hazard with\n\u03b81 = 0.3, \u03b11 = 0.5,\n\u03b82 = \u03b12 = 5 (solid). right\npanel: log-logistic\nhazards with \u03bb = 1 and\n\u03b1 = 0.5 (solid), \u03b1 = 5\n(dots), gamma hazard\nwith \u03bb = 0.6 and \u03b1 = 2\n(dashes), and standard\nnormal hazard (large\ndashes).\n\n188\n\nn\no\n\ni\nt\nc\nn\nu\n\nf\n \n\nd\nr\na\nz\na\nh\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\n5 \u00b7 models\n\nn\no\n\ni\nt\nc\nn\nu\n\nf\n \n\nd\nr\na\nz\na\nh\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\n0\n\n1\n\n23\n\n4\n\n5\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\ny\n\ny\n\n5.4 survival data\n5.4.1 basic ideas\nthe focus of interest in survival data is the time to an event. an important area of\napplication is medicine, where, for example, interest may centre on whether a new\ntreatment lengthens the life of a cancer patient, relative to those who receive existing\ntreatments. other common applications are in industrial reliability, where the aim may\nbe to estimate the distribution of time to failure for a fridge, a computer program,\nor a pacemaker. examples also abound in the social sciences, where for example the\nlength of a period of unemployment may be of interest. in each case the time y to\nthe event is non-negative and may be censored. for example, a patient may be lost to\nfollow-up for some reason unrelated to his disease, so that it is unknown whether or\nnot he died from the cause under study. in general discussion we refer to the items\nliable to fail as units; these may be persons, widgets, marriages, cars, or whatever.\n\nthis section outlines some basic notions in survival analysis, concentrating on\n\nsingle samples. more complex models are discussed in section 10.8.\n\nhazard and survivor functions\na central concept is the hazard function of y , defined loosely as the probability density\nof failure at time y, given survival to then. if y is a continuous random variable this is\n\nh(y) = lim\n\u03b4y\u21920\n\n1\n\u03b4y\n\npr (y \u2264 y < y + \u03b4y | y \u2265 y) = f (y)\nf(y)\n\n,\n\nwhere f(y) = pr(y \u2265 y) = 1 \u2212 f(y) is thesurvivor function of y . anolder term\nfor h(y) is theforce of mortality, and it is also called the age-specific failure rate.\nevidently h(y) \u2265 0; some example hazard functions are shown in figure 5.7.\nthe exponential density with rate \u03bb has f(y) = exp(\u2212\u03bby) and constant hazard\nfunction h(y) = \u03bb, and although data are rarely so simple, this model of a constant\nfailure rate independent of the past is a natural baseline from which to develop more\nrealistic models.\n\n "}, {"Page_number": 201, "text": "5.4 \u00b7 survival data\n\n189\n\nor integrated hazard\nfunction.\n\nthe cumulative hazard function is\n\n(cid:15)\n\n(cid:15)\n\n0\n\nh(y) =\n\ny\n\nh(u) du =\n\ny\n\n0\n\nf (u)\n1 \u2212 f(u)\n\ndu = \u2212 log{1 \u2212 f(y)} ,\n\nas f(0) = 0. thus the survivor function may be written as f(y) = exp{\u2212h(y)}, and\nf (y) = h(y) exp{\u2212h(y)}. if limy\u2192\u221e h(y) < \u221e, then f(\u221e) > 0 and the distribu-\ntion is defective, putting positive probability on an infinite survival time. this may\narise in practice if, for example, the endpoint for a study is death from a disease, but\ncomplete recovery is possible.\nh(y) = (cid:1)\nfor adiscrete distribution with probabilities fi at 0 \u2264 t1 < t2 < \u00b7\u00b7\u00b7, wemay write\nhi \u03b4(y \u2212 ti ), where\n\nhi = pr(y = ti | y \u2265 ti ) =\n\nfi\n\nfi + fi+1 + \u00b7\u00b7\u00b7 .\n\nthus\n\npr(y > ti | y \u2265 ti ) = 1 \u2212 hi ,\n\nfi = hi\n\ni\u22121(cid:2)\nj=1\n\n(1 \u2212 h j ),\n\n(5.24)\n\nand if ti < y \u2264 ti+1 then\n(cid:2)\n\n=\n\n(1 \u2212 hi ).\n\nf(y) = pr(y > ti | y \u2265 ti )pr(y > ti\u22121 | y \u2265 ti\u22121)\u00b7\u00b7\u00b7pr( y > t1)\n\n(5.25)\n\ni:ti <y\n\nwe define the cumulative hazard as h(y) = \u2212(cid:1)\n(cid:1)\ni:ti <y log(1 \u2212 hi ), again giving\nf(y) = exp{\u2212h(y)}. the more natural definition\ni:ti <y hi is approximately equal\nto h(y) ifthe individual hi are small.\n\nmixed discrete-continuous variables are important in a general treatment of survival\ndata \u2014 for example, a patient may die so fast from complications after an operation\nthat the survival time is effectively zero, but otherwise may live for years \u2014 but here\nwe avoid them and the complications they bring.\nsuppose that y = min(y1, . . . ,y k), where the yi are continuous times to failure\nfrom k independent causes, and that their hazard functions are hi (y). then y exceeds\ny if and only if all the yi exceed y, so\n\nf(y) = k(cid:2)\n\n\u2212 k(cid:6)\nand it follows that y has hazard function h(y) = (cid:1)\n\npr(yi \u2265 y) = exp\n\ni=1\n\ni=1\n\n(cid:15)\n\ny\n\n0\n\n(cid:11)\n\n(cid:12)\n\nhi (u) du\n\n,\n\nhi (y). that is, hazards for inde-\n\npendent causes of failure are added.\n\nexample 5.24 (weibull density) the weibull density (4.4) has survivor function\nf(y) = exp{\u2212(y/\u03b8)\u03b1}, soits hazard function is \u03b1\u03b8\u2212\u03b1 y\u03b1\u22121. this is constant when\n\u03b1 = 1, decreasing when \u03b1 < 1, and increasing when \u03b1 > 1, as shown in the left\npanel of figure 5.7. this flexibility and the tractability of its density and distribution\nfunctions make the weibull a popular choice in reliability studies.\n\n "}, {"Page_number": 202, "text": "190\n\n5 \u00b7 models\n\n\u2212\u03b11\n1\n\ny\u03b11\u22121 + \u03b12\u03b8\n\nthis density is the basis of the bi-weibull model, which corresponds to the minimum\nof two independent weibull variables, shown by the argument above to have hazard\ny\u03b12\u22121. ifthe shape parameters lie on opposite sides\nfunction \u03b11\u03b8\nof unity, so 0 < \u03b11 < 1 < \u03b12, say, h(y) isbathtub-shaped: there is a high early failure\nrate during a \u2018burn-in period\u2019, then a flattish hazard and an eventual increase in failure\nrate; see figure 5.7. if \u03b11 = \u03b12 the hazard is indistinguishable from the weibull hazard\n(cid:1)\nand \u03b81 and \u03b82 are not identifiable.\n\n\u2212\u03b12\n2\n\nexample 5.25 (log-logistic density) the log-logistic distribution has survivor and\nhazard functions\n\nf(y) = {1 + (\u03bby)\u03b1}\u22121,\n\nh(y) = \u03b1\n\n,\n\ny > 0, \u03b1, \u03bb > 0.\n\n\u03bb\u03b1 y\u03b1\u22121\n1 + (\u03bby)\u03b1\n\ntwo examples of h(y) are shown in the right panel of figure 5.7. it is decreasing for\n\u03b1 \u2264 1 and unimodal otherwise. the log-normal distribution, that is, the distribution\nof y = ez , where z has a normal distribution, is similar to the log-logistic, and its\nhazard can take similar shapes. the normal hazard, also shown, increases very rapidly\n(cid:1)\ndue to the light tails of the normal density.\n\nexample 5.26 (gamma density) the gamma survivor and hazard functions are\n\n(cid:15) \u221e\n\ny\n\nf(y) =\n\n\u03bb\u03b1u\u03b1\u22121\n\u0001(\u03b1)\n\n\u2212\u03bbu du,\n\ne\n\nh(y) =\n\n(cid:17) \u221e\n\ny\n\n\u2212\u03bby\n\n\u03bb\u03b1 y\u03b1\u22121e\n\u03bb\u03b1u\u03b1\u22121e\u2212\u03bbu du\n\n.\n\nfigure 5.7 shows an example of the gamma hazard function.\n\n(cid:1)\n\ncensoring\nthe simplest form of censoring occurs when a random variable y is watched until a\npre-determined time c. if y \u2264 c, weobserve the value y of y , butif y > c, weknow\nonly that y survived beyond c. this is known as type i censoring. type ii censoring\narises when n independent variables are observed until there have been r failures, so\nthe first r order statistics 0 < y(1) < \u00b7\u00b7\u00b7 < y(r) are observed, all that is known about\nthe n \u2212 r remaining observations is that they exceed y(r). this scheme is typically\nused in industrial life-testing.\n\nfor simplicity we assume\nno ties.\n\nj . the time actually observed is y j = min(y 0\n\nunder random censoring we suppose that the jth of n independent units has an\nassociated censoring time c j drawn from a distribution g, independent of its survival\ntime y 0\n, c j ), and it is known whether\nor not y j = y 0\nj , an event indicated by d j . thus a pair (y j , d j ) isobserved for each\nunit, with d j = 1 if y j is the survival time and d j = 0 if y j is the censoring time. this\ntype of censoring is important in medical applications, where a patient may die of a\ncause unrelated to the reason they are being studied, may withdraw from the study or\nbe lost to follow-up, or the study may end before their survival time is observed.\n\nj\n\nfigure 5.8 shows the relation between calendar time and time on trial for a medical\nstudy, with censoring both before and at the end of the trial. we assume below that\nfailure does not depend on the calendar time at which an individual enters the study;\n\n "}, {"Page_number": 203, "text": "figure 5.8 lexis\ndiagram showing typical\npattern of censoring in a\nmedical study. each\nindividual is shown as a\nline whose x coordinates\nrun from the calendar time\nof entry to the trial to the\ncalendar time of failure\n(blob) or censoring\n(circle). censoring occurs\nat the end of the trial,\nmarked by the vertical\ndotted line, or earlier. the\nvertical axis shows time\non trial, which starts when\nindividuals enter the\nstudy. the risk set for the\nfailure at calendar time\n4.5 comprises those\nindividuals whose lines\ntouch the horizontal\ndashed line; see page 543.\n\n5.4 \u00b7 survival data\n\n191\n\nl\n\na\ni\nr\nt\n \n\nn\no\n\n \n\ne\nm\nt\n\ni\n\n0\n\n.\n\n3\n\n5\n\n.\n\n2\n\n0\n\n.\n\n2\n\n5\n\n.\n\n1\n\n0\n\n.\n\n1\n\n5\n\n.\n\n0\n\n0\n\n.\n\n0\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\ncalendar time\n\nthus we study events on the vertical axis. calendar time may be used to account for\nchanges in medical practice over the course of a trial.\n\nin applications the assumption that c j and y 0\n\nj are independent is critical. there\nwould be serious bias if the illest patients drop out of a trial because the treatment\nmakes them feel even worse, thereby inducing association between survival and cen-\nsoring variables because patients die soon after they withdraw.\n\nthe examples above all involve right-censoring. less common is left-censoring,\nwhere the time of origin is not known exactly, for example if time to death from a\ndisease is observed, but the time of infection is unknown.\n\nin practice a high proportion of the data may be censored, and there may be a\nserious loss of efficiency if they are ignored (example 4.20). there will also be bias,\nas survival probabilities will be underestimated if censoring is not taken into account.\nhence it is crucial to make proper allowance for censoring.\n\n5.4.2 likelihood inference\nsuppose that the survival times are continuous, that data (y1, d1), . . . ,( yn, dn) onn\nindependent units are available, and that there is a parametric model for survival\ntimes, with survivor and hazard functions f(y; \u03b8) and h(y; \u03b8). recall that the density\nmay be written f (y; \u03b8) = h(y; \u03b8)f(y; \u03b8) and that in terms of the integrated hazard\nfunction, f(y; \u03b8) = exp{\u2212h(y; \u03b8)}. under random censoring in which the censoring\nvariables have density and distribution functions g and g, the likelihood contribution\nfrom y j is\n\nf (y j ; \u03b8){1 \u2212 g(y j )}\n\nif d j = 1,\n\nand f(y j ; \u03b8)g(y j )\n\nif d j = 0.\n\nif the censoring distribution does not depend on \u03b8, then g(y j ) and g(y j ) are constant\nand the overall log likelihood is\n\n(cid:6)\n\n(cid:6)\n\n(cid:6)(\u03b8) \u2261\n\nlog f (y j ; \u03b8) +\n\nlogf(y j ; \u03b8),\n\nu\n\nc\n\n "}, {"Page_number": 204, "text": "192\n\n0+\n20+\n47+\n73\n0+\n11\n27\n\n1+\n22+\n47+\n75+\n0+\n12+\n28\n\n1+\n22+\n49+\n77+\n2+\n13\n32+\n\n3+\n24+\n53+\n83+\n2+\n13+\n35+\n\n3+\n25+\n53+\n84+\n2+\n18+\n36\n\n7\n26+\n55+\n88+\n2+\n22+\n40+\n\n10+\n31+\n56+\n89+\n\n3\n22+\n43+\n\n11+\n36+\n57+\n99\n3+\n24+\n50+\n\n12+\n36+\n61+\n121+\n4+\n24+\n54\n\n5 \u00b7 models\n\n12+\n36\n67+\n122+\n5+\n24+\n\n15+\n38\n67+\n123+\n9+\n25+\n\n18+\n40\n70\n141+\n10+\n26+\n\ntable 5.3\nblalock\u2013taussig shunt\ndata (oakes, 1991). the\ntable gives survival time\nof shunt (months after\noperation) for 48 infants\naged over one month at\ntime of operation,\nfollowed by times for 33\ninfants aged 30 or fewer\ndays at operation. infants\nwhose shunt has not yet\nfailed are marked +.\n\nwhere the sums are over uncensored and censored units. this amounts to treating the\ncensoring pattern as fixed, and encompasses type i censoring, for which g puts all its\nprobability at c. interms of the hazard function and its integral, the log likelihood is\n\n(cid:6)(\u03b8) = n(cid:6)\n\nj=1\n\n{d j log h(y j ; \u03b8) \u2212 h(y j ; \u03b8)}.\n\n(5.26)\n\nj=1\n\nd j \u2212 \u03bb\n\nn(cid:6)\n(cid:1)\nj=1\n\n(cid:6)(\u03bb) = n(cid:6)\n\ninference for \u03b8 is based on this in the usual way. as calculation of expected information\ninvolves assumptions about the censoring mechanism, standard errors for parameter\nestimates are based on observed information.\nexample 5.27 (exponential distribution) when f (y; \u03bb) = \u03bbe\n\u2212\u03bby, the hazard is\nh(y; \u03bb) = \u03bb, and hence the log likelihood for a random sample (y1, d1), . . . ,( yn, dn) is\nn(cid:6)\nj=1\n\n(d j log \u03bb \u2212 \u03bby j ) = log \u03bb\ngiving maximum likelihood estimate (cid:8)\u03bb = (cid:1)\nj (\u03bb) = (cid:1)\ny j and observed information\nd j /\u03bb2; see example 4.20. hence the estimate of \u03bb is zero if there are\nno failures, and censored data contribute no information about \u03bb.\nthe expected information i (\u03bb) = e{j (\u03bb)} involves e(d j ), where d j indicates\nwhether a failure or censoring time is observed for the jth observation, but this\nexpectation cannot be obtained without some assumption about the censoring dis-\ntribution g. although this is feasible for theoretical calculations such as those in\nerror j ((cid:8)\u03bb)\n\u22121/2 for(cid:8)\u03bb.\nexample 4.20, in practice the inverse observed information is used to give a standard\nestimate is(cid:8)\u03b8 = (cid:1)\nthe mean of the exponential density is \u03b8 = \u03bb\u22121, and its maximum likelihood\nimized log likelihood (cid:6)((cid:8)\u03b8) = \u2212(1 + log(cid:8)\u03b8)\nd j and max-\n(cid:1)\n\nd j , with observed information j ((cid:8)\u03b8) =(cid:8)\u03b8 2/\n\n(cid:1)\n\n(cid:1)\n\n(cid:1)\n\nd j /\n\ny j /\n\ny j ,\n\nd j .\n\nexample 5.28 (blalock\u2013taussig shunt data) the blalock\u2013taussig shunt is an\noperative procedure for infants with congential cyanotic heart disease. table 5.3\ncontains data from the university of rochester on survival times for the shunt for\n81 infants, divided into two age groups. many of the survival times are censored,\nmeaning that the shunt was still functioning after the given survival time; its time to\nfailure is not known for these children, whereas it is known for the others. there are\njust seven failures in each group. the table suggests that the shunt fails sooner for\nyounger children, and it is of interest to see how failure depends on age.\n\n "}, {"Page_number": 205, "text": "5.4 \u00b7 survival data\n\n193\n\na simple model for these data is that the failure times are independent exponential\nthat(cid:8)\u03b8 = 209.1 and the maximized log likelihood is\u221288.79. if the means are different,\nvariables, with common mean \u03b8 for both groups. formulae from example 5.27 show\n\u03b81 and \u03b82, say, then the maximized log likelihood is \u221285.98, so the likelihood ratio\nstatistic for comparing these models is 2 \u00d7 (88.79 \u2212 85.98) = 5.62, to be compared\n.= 0.018, there is strong evidence that\nwith the \u03c7 2\nthe mean survival time is shorter for the younger group, if the exponential model is\ncorrect.\n\n1 distribution. as pr(\u03c7 2\n1\n\n\u2265 5.62)\n\nif the data were uncensored, it would be straightforward to assess the fit of this\nmodel using probabability plots, but the amount of censoring is so high that this\nis not sensible. more specialized methods are needed, and they are discussed in\nsection 5.4.3.\none way to judge adequacy of the exponential model is to embed it in a larger one.\na simple alternative is to suppose that the data are weibull, with h(y) = (y/\u03b8)\u03b1.\nthe maximized log likelihoods are \u221283.72 when this model is fitted separately to\neach group, and \u221283.74 when the same value of \u03b1 is used for both groups. the\nlikelihood ratio statistic for comparison of these is 2 \u00d7 (83.74 \u2212 83.72) = 0.04, which\nis negligible, but that for comparison with the best exponential model, 2 \u00d7 (85.98 \u2212\n83.74) = 4.48, suggests that the weibull model gives the better fit. the corresponding\nestimates and their standard errors are(cid:8)\u03b81 = 181.1 (52.7),(cid:8)\u03b82 = 57.6 (15.1), and(cid:8)\u03b1 =\n1.64 (0.35). the value of(cid:8)\u03b1 corresponds to an increasing hazard.\n\n(cid:1)\n\ndiscrete data\nsuppose that events could occur at pre-assigned times 0 \u2264 t1 < t2 < \u00b7\u00b7\u00b7, and that\nunder a parametric model of interest the hazard function at ti is hi = hi (\u03b8). we adopt\nthe convention that a unit censored at time ti could have been observed to fail there,\nso giving likelihood contribution\n\nf(y) = (1 \u2212 h1)\u00b7\u00b7\u00b7(1 \u2212 hi ),\n\nlim\ny\u2193ti\n\nfrom (5.25); one way to think of this is that censoring at ti in fact takes place im-\nmediately afterwards. the contribution to the likelihood from a unit that fails at ti\nis (1 \u2212 h1)\u00b7\u00b7\u00b7(1 \u2212 hi\u22121)hi ; see (5.24). although the likelihood can be written down\ndirectly, it is more useful to express it in terms of the ri units still in the risk set \u2014\nthat is not yet failed or censored \u2014 at time ti and the number di of units who fail\nthere. this modifies our previous notation: now di is the sum of the indicators of unit\nfailures at time ti , and can take one of values 0, 1, . . . , ri . each of the di failures at ti\ncontributes hi to the likelihood, and the other units then still in view each contribute\n1 \u2212 hi . itfollows that the log likelihood may be written as\n\n(cid:6)(\u03b8) =\n\n{di log hi + (ri \u2212 di ) log (1 \u2212 hi )} ,\n\n(5.27)\n\n(cid:6)\n\ni\n\nwith the interpretation that the probability of failure at ti conditional on survival to ti\nis hi , and di of the ri units in view at ti fail then. thus (5.27) is a sum of contributions\nfrom independent binomial variables representing the numbers of failures di at each\n\n "}, {"Page_number": 206, "text": "\u22121),\n\ntable 5.4 historical\nestimates of the force of\nmortality (year\naveraged for 5-year age\ngroups (thatcher, 1999).\nthe bottom line gives the\nestimated number of\ndeaths at age 30 years and\nabove, on which the force\nof mortality is based.\n\n194\n\nage\ngroup\n\nhungary\n900\u20131100\n\nengland\n1640\u201389\n\n5 \u00b7 models\n\nengland & wales, 1980\u201382\n\nengland & wales, 1841\n\nbreslau\n1687\u201391 males\n\nfemales\n\nmales\n\nfemales\n\n30\u201335\n35\u201340\n40\u201345\n45\u201350\n50\u201355\n55\u201360\n60\u201365\n65\u201370\n70\u201375\n75\u201380\n80\u201385\n85\u201390\n90\u201395\n95\u2013100\n\n0.0235\n0.0291\n0.0337\n0.0402\n0.0696\n0.0814\n0.1033\n0.1485\n0.1877\n0.3008\n\n0.0171\n0.0205\n0.0195\n0.0244\n0.0307\n0.0459\n0.0513\n0.0701\n0.1129\n0.1445\n0.1974\n\n0.0164\n0.0195\n0.0233\n0.0282\n0.0342\n0.0383\n0.0474\n0.0630\n0.0995\n0.1589\n\n0.0108\n0.0123\n0.0140\n0.0159\n0.0181\n0.0254\n0.0375\n0.0553\n0.0815\n0.1201\n0.1771\n0.2617\n0.3884\n\n0.0107\n0.0118\n0.0131\n0.0145\n0.0162\n0.0220\n0.0331\n0.0493\n0.0736\n0.1097\n0.1638\n0.2448\n0.3674\n\n0.0010\n0.0014\n0.0024\n0.0043\n0.0079\n0.0138\n0.0227\n0.0365\n0.0587\n0.0930\n0.1432\n0.2110\n0.2900\n0.3894\n\n0.0006\n0.0009\n0.0016\n0.0028\n0.0047\n0.0076\n0.0119\n0.0187\n0.0308\n0.0527\n0.0919\n0.1567\n0.2374\n0.3215\n\ndeaths\n\n2300\n\n3133\n\n2675\n\n71,000\n\n74,000\n\n834,000\n\n828,000\n\ntime ti , with denominators ri and failure probabilities hi . in fact ri depends on the\nhistory of failures and censorings up to time ti , sothe di are not independent, but\nit turns out that for large sample inference we may proceed as if they were. this\ncan be formalized using the theory of counting processes and martingales; see the\nbibliographic notes to this chapter and to chapter 10.\n\nexample 5.29 (human lifetime data) the virtual elimination of many infectious\ndiseases due to improved medical care and living conditions have led to increased\nlife expectancy in the developed world. if the trend continues there are potentially\nmajor consequences for social security systems. some physicians have asserted that\nan upper limit to the length of human life is imposed by physical constraints, and\nthat the consequence of improved health care is that senesence will eventually be\ncompressed into a short period just prior to death at or near this upper limit. this view\nis controversial, however, and there is a lively debate about the future of old age.\n\na natural way to assess the plausibility of the hypothesized upper limit is to exam-\nine data on mortality. table 5.4 contains historical snapshots of the force of mortality,\nobtained from census data, records of births and deaths, and other sources. the ear-\nliest data were obtained by forensic examination of adult skeletons in hungarian\ngraveyards, using a procedure that probably underestimates ages over 60 years and\noverestimates those below. the table shows estimates of the average probability of\ndying per year, conditional on survival to then, using the following argument. for\ncontinuous-time data with survivor function f(y) and corresponding hazard function\nh(y), the probability of failure in the period [ti , ti+1) given survival to ti would be\n\n(cid:14)\n\nf(ti ) \u2212 f(ti+1)\n\nf(ti )\n\n= 1 \u2212 exp\n\n\u2212(ti+1 \u2212 ti )\n\n1\n\nti+1 \u2212 ti\n\n(cid:16)\n\nh(y) dy\n\n,\n\n(cid:15)\n\nti+1\n\nti\n\n "}, {"Page_number": 207, "text": "5.4 \u00b7 survival data\n\n195\n\nfigure 5.9 force of\nmortality for historical\ndata, in units of deaths per\nperson-year. left panel,\nfrom top to bottom: data\nfor medieval hungary,\nengland 1640\u201389, breslau\n1687\u201391 (dots), english\nand welsh females 1841\nand 1980\u201382. right panel:\ndata for england and\nwales, 1980\u201382, males\n(above) and females\n(below) and fitted hazard\nfunctions (dots).\n\n)\nr\na\ne\ny\n \nr\ne\np\n(\n \ny\nt\ni\nl\n\na\n\nt\nr\no\nm\n\n \nf\n\no\n\n \n\ne\nc\nr\no\nf\n\n4\n\n.\n\n0\n\n3\n0\n\n.\n\n2\n\n.\n\n0\n\n1\n\n.\n\n0\n\n0\n0\n\n.\n\n)\nr\na\ne\ny\n \nr\ne\np\n(\n \ny\nt\ni\nl\n\na\n\nt\nr\no\nm\n\n \nf\n\no\n\n \n\ne\nc\nr\no\nf\n\n0\n1\n\n.\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n0\n\n.\n\n40 50 60 70 80 90 100\n\n40 60 80 100\n\n140\n\nage (years)\n\n(cid:17)\n\nage (years)\n\nti+1\nti\n\nwhere (ti+1 \u2212 ti )\n\u22121\nh(y) dy is the average hazard over the interval. given dis-\ncretized data with ri people alive at time ti , ofwhom di fail in [ti , ti+1), the corre-\nsponding empirical hazard is \u2212(ti+1 \u2212 ti )\n\u22121 log(1 \u2212 di /ri ), and this is reported in the\ntable; the corresponding di and ri are unavailable to us. for british males dying in\n\u22121 at age 30 years to about\n1980 the empirical hazard rose from about 0.001 year\n\u22121 at 95 years; for females the probabilities\n0.1 year\nwere slightly lower. figure 5.9 shows the force of mortality of some of the columns\nof the table; it is no surprise that it is lower in later than in earlier periods.\n\n\u22121 at 80 years to about 0.4 year\n\none model for such data is that\n\nh(y; \u03b8) = \u03bb + \u03b1e\u03b2y\n1 + \u03b1e\u03b2y\n\n,\n\nwhere \u03b8 = (\u03b1, \u03b2, \u03bb), corresponding to integrated hazard and survivor functions\nh(y; \u03b8) = \u03bby + 1\n\n, f(y; \u03b8) = e\n\n\u2212\u03bby \u00d7\n\n(cid:19)\n\n(cid:18)\n\n(cid:19)\n\n(cid:18)\n\n1/\u03b2 ,\n\nlog\n\ny \u2265 0.\n\n1 + \u03b1e\u03b2y\n1 + \u03b1\n\n\u03b2\n\n1 + \u03b1\n1 + \u03b1e\u03b2y\n\none interpretation of this model is that there are two competing causes of death, one\nwith a constant hazard, and the other with a logistic hazard.\n\nin order to use (5.27) to fit this model to the data given in table 5.4, we must calculate\nhi (\u03b8) and (di , ri ). the probability of dying in [ti , ti+1) conditional on survival to ti is\n\nhi (\u03b8) = pr(ti \u2264 y \u2264 ti+1 | y \u2265 ti )\n\n= f(ti ; \u03b8) \u2212 f(ti+1; \u03b8)\n= 1 \u2212 exp{h(ti ; \u03b8) \u2212 h(ti+1; \u03b8)} ,\n\nf(ti ; \u03b8)\n\nand this is calculated using the logistic hazard given above. the empirical values of\nthe hazard function hi = di /ri , where di is the number of deaths among the ri persons\nat risk, can be obtained from the columns of table 5.4. some calculation gives\n\nd1 = nh1,\n\ndi = nhi (1 \u2212 h1)\u00b7\u00b7\u00b7(1 \u2212 hi\u22121),\n\ni = 2, . . . ,k ,\n\n "}, {"Page_number": 208, "text": "196\n\n5 \u00b7 models\n\ndata set\n\ndeaths at age 30\nyears and over\n\n104(cid:8)\u03b1\n\nestimate (standard error)\n\n102(cid:8)\u03b2\n\n102(cid:8)\u03bb\n\nhungary, 900\u20131100\nengland, 1640\u201389\nbreslau, 1687\u201391\nengland & wales, 1841, males\nengland & wales, 1841, females\nengland & wales, 1980\u201382, males\nengland & wales, 1980\u201382, females\n\n2300\n3133\n2675\n71,000\n74,000\n834,000\n828,000\n\n8.76 (3.78)\n1.87 (0.66)\n1.44 (0.76)\n0.50 (0.03)\n0.32 (0.02)\n0.46 (0.00)\n0.12 (0.00)\n\n7.68 (0.65)\n8.65 (0.48)\n8.88 (0.73)\n10.08 (0.08)\n10.50 (0.08)\n9.93 (0.01)\n10.92(0.01)\n\n1.27 (0.32)\n1.40 (0.12)\n1.57 (0.15)\n0.97 (0.01)\n0.97 (0.01)\n\u22120.04 (0.00)\n0.03 (0.00)\n\ntable 5.5 maximum\nlikelihood estimates for\nfits of logistic hazard\nmodel to the data in\ntable 5.4. standard errors\ngiven as 0.00 are smaller\nthan 0.005.\n\nwhere n = r1 is the number initially at risk, an estimate of which is given at the foot\nof the table; once the di are known the ri are given by di / hi . when these pieces are\nput together, maximum likelihood estimates of \u03b8 may be obtained by numerical max-\nimization of (5.27), with standard errors based on the inverse observed information\n\nmatrix, also obtained numerically. table 5.5 shows that(cid:8)\u03b1 and(cid:8)\u03bb decrease systemati-\ncally with time, while the value of(cid:8)\u03b2 increases slightly but is broadly constant, close\nchange in its shape, that we see in the left panel of figure 5.9. the values of(cid:8)\u03bb are gen-\nis that(cid:8)\u03bb represents the danger from the principal risks at this age, namely infectious\n\nto 0.1. these are consistent with the overall decrease in the hazard function, but no\n\nerally similar to the observed force of mortality at age 30\u201335, and one interpretation\n\ndiseases and child-bearing, which has sharply reduced over the last 150 years.\n\nthe fits for the 1980\u201382 data are shown in the right panel of figure 5.9. although the\nfit is good, the extrapolation beyond the range of the data must be treated skeptically.\nit shows that although the model imposes no absolute upper limit on lifetimes, for a\nperson dying in 1980\u201382 there was an effective limit of about 140 years, well beyond\nthe limits of 110 or 115 years which have been suggested by physicians. in fact the\nlongest life for which there is good documentation is that of mme jeanne calment,\nwho died in 1997 aged 122 years, and there is unlikely ever to be enough data to see\nif there is an upper limit well above this.\n\nexample 5.32 gives further discussion of this model.\n\n(cid:1)\n\n5.4.3 product-limit estimator\ngraphical procedures are essential for initial data inspection, for suggesting plausible\nmodels and for checking their fit. one standard tool is a nonparametric estimator of\nthe survivor function, in effect extending the empirical distribution function (exam-\nple 2.7) to censored data.\n\nthe simplest derivation of it is based on the model for failures at discrete pre-\nspecified times given above (5.25), though the estimator is useful more widely. we\ntherefore start with expression (5.27), which gives the log likelihood for such data in\nterms of the hazard function h1, h2, . . .. for parametric analysis of a discrete failure\ndistribution the hi are functions of a parameter \u03b8, but for nonparametric estimation\nwe treat each hi as a separate parameter and estimate it by maximum likelihood.\n\n "}, {"Page_number": 209, "text": "1 \u2212(cid:8)hi\n(cid:14)\n\n5.4 \u00b7 survival data\ndifferentiation of (5.27) with respect to hi gives(cid:8)hi = di /ri and hence\n\n(cid:2)\n\n(cid:24)\n\n(cid:8)f(y) =\n\n(cid:18)\n\n(cid:2)\n\n(cid:25) =\n\n(cid:19)\n\n.\n\n1 \u2212 di\nri\n\ni:ti <y\n\ni:ti <y\n\n197\n\nthis is known as the product-limit or kaplan\u2013meier estimator. note that\n\ni = j,\notherwise,\n\n,\n\n=\n\n\u2212 \u2202 2(cid:6)\n\u2202hi \u2202h j\n\nri(cid:8)hi (1\u2212(cid:8)hi )\n0,\nimplying that that the (cid:8)hi are asymptotically independent, with diagonal variance\nmatrix whose ith element is(cid:8)hi (1 \u2212(cid:8)hi )/ri .\nthis derivation extends to continuous failure times by supposing that the y j are\nordered and that there are no ties, giving t1 = y1 < \u00b7\u00b7\u00b7 < tn = yn. then d j simply\nindicates whether y j is a failure or a censoring time, and\n\n(cid:8)f(y) =\n\n(cid:18)\n\n(cid:2)\n\n(cid:19)d j\n\n1 \u2212 1\nr j\n\n,\n\nj:y j <y\n\n(5.28)\nso the estimate decreases only at those values of t j with d j = 1. this estimate is\nus too far afield. if there is no censoring, then 1 \u2212 (cid:8)f(y) isthe empirical distribution\nvalid also when the y j are not pre-specified, but full justification of this would take\nwe find the variance of (cid:8)f(y) by arguing that if the di are asymptotically independent\n\nfunction.\n\nbinomial variables with denominators ri , then\n\n(cid:12)\n\ni:yi <y\n\n(cid:11) (cid:6)\nvar{log (cid:8)f(y)} =var\nlog(1 \u2212(cid:8)hi )\n(cid:6)\nvar{log(1 \u2212(cid:8)hi )}\n.=\n(cid:6)\n(1 \u2212(cid:8)hi )2\n(cid:6)\nri (ri \u2212 di )\n\ni:yi <y\n\ni:yi <y\n\n.=\n\n=\n\ndi\n\nri\n\n1\n\n,\n\n(cid:8)hi (1 \u2212(cid:8)hi )\n\n(5.29)\n\nwhere the first approximation uses the asymptotic independence of the (cid:8)hi and the\nsecond uses the delta method. as var{log (cid:8)f(y)} .= var{(cid:8)f(y)}/(cid:8)f(y)2, weobtain\n\ni:yi <y\n\ngreenwood\u2019s formula,\n\nvar{(cid:8)f(y)} .= (cid:8)f(y)2\n\n(cid:6)\n\ndi\n\nri (ri \u2212 di )\n\n,\n\ni:yi <y\n\nvariants of which are widely used to assess the uncertainty of (cid:8)f(y). in practice it is\nbetter to use (5.29) to compute approximate normal confidence intervals for logf(y),\nthe cumulative hazard function can be estimated as (cid:8)h(y) = (cid:1)\nand then to transform these intervals back to the original scale.\n\ni:yi <y di /ri ; this is\n\na step function with jumps at failure times and approximate variance (5.29).\n\nedward kaplan and paul\nmeier were former\nstudents of john tukey\nwho submitted separate\npapers to journal of the\namerican statistical\nassociation. they were\nencouraged to merge them\nby the editor. despite\nmixed reviews the editor\ndecided to publish the\njoint paper (kaplan and\nmeier, 1958), which has\nbecome one of the\nmost-cited articles in\nstatistics.\n\nmajor greenwood\n(1880\u20131949) qualified as\na physician before turning\nto statistics and\nepidemiology under the\ninfluence of karl pearson.\nhe was the first resident\nstatistician at any medical\nresearch institute, and\nworked for the british\nmedical research council\nand the london school of\nhygiene and tropical\nmedicine. he studied\ninfant mortality,\ntuberculosis and hospital\nfatality rates, pioneered\nclinical trials and\ngradually persuaded\nsceptical physicians of the\nvalue of statistical\nthinking. major was not\nhis military rank but his\nfirst name.\n\n "}, {"Page_number": 210, "text": "198\n\n5 \u00b7 models\n\nfailure time, yi\nnumber in view, ri\nnumber failing, di\n1 \u2212 di /ri\n(cid:8)f(yi+)\n\nstandard error\n\n7\n43\n1\n0.977\n0.977\n0.023\n\n36\n29\n1\n0.966\n0.944\n0.040\n\n38\n26\n1\n0.962\n0.908\n0.052\n\n40\n25\n1\n0.960\n0.872\n0.062\n\n70\n13\n1\n0.923\n0.804\n0.086\n\n73\n12\n1\n0.916\n0.737\n0.102\n\n99\n5\n1\n0.8\n0.590\n0.155\n\nn\no\n\ni\nt\nc\nn\nu\n\ni\n\nf\n \nr\no\nv\nv\nr\nu\ns\n\n0\n1\n\n.\n\n8\n0\n\n.\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\nd\nr\na\nz\na\nh\n\n \n\ne\nv\ni\nt\n\nl\n\na\nu\nm\nu\nc\n\n2\n1\n\n.\n\n0\n1\n\n.\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n0 20 40 60 80 100\n\n140\n\n0\n\n20\n\n40\n\n60\n\n80 100 120\n\nshunt survival (months)\n\nshunt survival (months)\n\nexample 5.30 (blalock\u2013taussig shunt data) table 5.6 illustrates the calculation\nof the product-limit estimator using data from table 5.3. as the estimator changes\nonly at times of failures, it need not be calculated at censoring times. the estimate\ndoes not approach zero for large y because the largest observation in the sample is\ncensored.\n\ntable 5.6 product-limit\nestimator for older group\nof infants in table 5.3.\n\nfigure 5.10\nnonparametric analysis of\nshunt data. left panel:\nproduct-limit estimates of\nsurvivor function for older\n(upper heavy line) and\nyounger infants (lower\nheavy line), with 95%\nconfidence intervals (dots\nand light solid). pluses on\nthe product-limit\nestimates mark times of\ncensored data. right\npanel: estimated\ncumulative hazard\nfunctions for older (solid)\nand younger (dots)\ninfants, using\nnonparametric estimate\nand fitted weibull model\n(smooth curves).\n\nestimated survivor functions for both groups are shown in the left panel of\nfigure 5.10, together with approximate 95% confidence intervals. there is a strong\neffect of age, with shunts failing appreciably sooner for the younger children. the\n\nright panel compares the cumulative hazard function estimators (cid:8)h(y) = (cid:1)\n\n(cid:8)hi\n\nwith their parametric counterparts under the best weibull model of example 5.28.\nthe parametric fits overstate the hazards appreciably. the apparent large difference\nafter 60 months is largely due to a single failure in the younger group that strongly\n(cid:1)\ninfluences the analysis.\n\ni:yi\u2264y\n\n5.4.4 other ideas\ncompeting risks\nin some applications there may be different types of failure due to k different causes,\nsay, and each failure time y is accompanied by an indicator i showing which type of\nfailure occurred. we can then define cause-specific hazard functions\n\nhi (y) = lim\n\u03b4y\u21920\n\npr (y \u2264 y \u2264 y + \u03b4y, i = i | y \u2265 y)\n\ny \u2265 0, i = 1, . . . ,k ,\n\n,\n\n\u03b4y\n\n "}, {"Page_number": 211, "text": "5.4 \u00b7 survival data\n\n199\n\ncorresponding to the rate at which failure of type i occurs, given survival to y. the\noverall hazard, cumulative hazard and survivor functions may be written\n\nh(y) = k(cid:6)\n\nhi (y), h(y) = k(cid:6)\n\n(cid:15)\n\ni=1\n\ny\n\nhi (u) du, f(y) = exp\n\n0\n\ni=1\n\n(cid:15)\n\n(cid:11)\nk(cid:6)\ni=1\n\ny\n\n0\n\n(cid:12)\n\nhi (u) du\n\n.\n\nif we imagine observing a population of values of (y, i ), then each of the hi (y) would\nbe known, but we would observe no other aspect of the population. thus without\nfurther assumptions the only estimable quantities are functions of the hi (y) such as\nh(y) and f(y).\nthe likelihood contribution from an uncensored failure of type i is hi (y)f(y),\nwhile provided censoring is independent, that from a censored failure is f(y), be-\ncause the corresponding i is unknown. suppose that we have independent triplets\n(y1, i1, d1), . . . ,( yn, in, dn), where y j is the jth survival time and d j = 1 if it is un-\ncensored. if so, i j indicates its failure type, while i j = 0, say, if d j = 0. the likelihood\nbased on these data is\n\nn(cid:2)\nj=1\n\nf(y j )\n\nk(cid:2)\ni=1\n\nhi j (y j )d j = k(cid:2)\n\ni=1\n\n(cid:5)\n\nn(cid:2)\nj=1\n\n(cid:14)\n\n(cid:15)\n\n\u2212\n\n0\n\nexp\n\n(cid:16)\n\n(cid:7)\n\ny j\n\nhi (y) du\n\nhi (y j )d j i (i j=i)\n\n,\n\nso it follows that to estimate hi (y) wetreat any failure not of type i as a censoring.\nthus, for example, the survivor function for hi (y) may be estimated by the product-\nlimit estimator (5.28) with d j replaced by d j i (i j = i). failures of types other than i\nare treated as censorings. likewise for estimation of a parametric hi .\nfor simplicity let k = 2. one way to think of competing risks is in terms of latent\nor potential failure times y1, y2 corresponding to the failure types. the observed\nquantities are y = min(y1, y2) and i = {i : yi = y}. here y1 is interpreted as the\ntime to failure that would be observed if cause 2 was removed, assuming that the\nfailure time distribution for cause 1 when both causes of failure operate remains\nunchanged if cause 2 is eliminated. this assumption may be plausible in situations\nsuch as a reliability study where different types of failure are due to physically separate\nsub-systems and it is possible to imagine that all but one of these have been perfected,\nbut the elimination of one failure type may alter the risk for others, particularly in\nmedical contexts, where the assumption is often unsustainable. if it can be justified\nby appeal to subject-matter considerations it is very useful \u2014 the case for vaccination\nagainst infectious diseases, for example, presumes that removal of their risks increases\noverall survival.\n\nan even stronger assertion is that y1 and y2 actually exist for each unit under\nstudy, with independence of causes of failure equivalent to independence of y1 and\ny2. in fact it is impossible to contradict this model. as mentioned above, the only\nobservable quantities are functions of the cause-specific hazards h1(y) and h2(y). the\njoint survivor function\n\nf(y1, y2) = pr(y1 > y1, y2 > y2) = exp\n\n\u2212\n\ny1\n\nh1(u) du \u2212\n\n(cid:14)\n\n(cid:15)\n\n0\n\n(cid:16)\n\n(cid:15)\n\ny2\n\n0\n\nh2(u) du\n\n "}, {"Page_number": 212, "text": "200\n\nlymphoma\n\nsarcoma\n\nother\n\nlymphoma\n\nsarcoma\n\nother\n\n159\n256\n428\n317\n558\n636\n697\n163\n341\n517\n647\n\n158\n237\n434\n430\n752\n136\n658\n777\n873\n\n189\n261\n432\n318\n571\n643\n700\n179\n366\n517\n651\n\n192\n240\n444\n590\n760\n246\n660\n800\n882\n\n191\n265\n\n399\n586\n647\n705\n206\n385\n524\n686\n\n193\n244\n485\n606\n778\n255\n662\n807\n895\n\n198\n266\n\n495\n594\n648\n712\n222\n407\n564\n761\n\n194\n247\n496\n638\n821\n376\n675\n825\n910\n\n200\n280\n\n525\n596\n649\n713\n228\n420\n567\n763\n\n195\n259\n529\n655\n986\n421\n681\n855\n934\n\n207\n343\n\n536\n605\n661\n738\n249\n431\n586\n\n202\n300\n537\n679\n\n565\n734\n857\n942\n\n220\n356\n\n549\n612\n663\n748\n252\n441\n619\n\n212\n301\n624\n691\n\n235\n383\n\n552\n621\n666\n753\n282\n461\n620\n\n215\n321\n707\n693\n\n616\n736\n864\n1015\n\n617\n737\n868\n1019\n\ntable 5.7 mouse data\n(hoel and walburg, 1972).\nage at death (days) of\nrfm male mice exposed\nto 300 rads of x-radiation\nat 5\u20136 weeks of age. the\ncauses of death were\nthymic lymphoma,\nreticulum cell sarcoma\nand other. the upper\ngroup of 95 mice were\nkept in a conventional\nenvironment; the lower 82\nin a germ-free\nenvironment.\n\n5 \u00b7 models\n\n245\n403\n\n554\n628\n670\n\n324\n462\n621\n\n229\n337\n800\n696\n\n652\n757\n870\n\n250\n414\n\n557\n631\n695\n\n333\n482\n622\n\n230\n415\n\n747\n\n655\n769\n870\n\nis a model for independent failures that yields cause-specific hazard functions h1 and\nh2, sowhatever the form of these functions, data of form (y , i ) cannot give evidence\nagainst independent risks. dependence can only be inferred from data in which both\ny1 and y2 are observed for certain units, or from subject-matter considerations. this\nis important because interest often focuses on the effect of eliminating failures of\none type, say type 2, in which case the survivor function is f(y, 0). as this is not\na function of h1 and h2 it is inestimable unless assumptions, typically unverifiable\nones, are made about the relation between the risks. some statisticians therefore insist\nthat the only valid inferences from competing risk data concern the hi and quantities\nderived from them.\n\nexample 5.31 (mouse data) the data in table 5.7 are from a experiment in which\ntwo groups of rfm strain male mice were exposed to 300 rad of radiation at age\n5\u20136 weeks. the first group lived in a conventional laboratory environment, and the\nsecond group lived in a germ-free environment. after their deaths, a pathologist\nascertained whether the death was due to one of two types of cancer or to other\ncauses. one purpose of the experiment was to assess the effect of environment on\ndifferent causes of death. as irradiation took place when the mice were aged between\n35 and 42 days old, it might be better to take age since irradiation as the response,\nbut its exact value is unknown.\n\nthe panels of figure 5.11 shows the estimated cumulative hazard functions for\ndeath from lymphoma and from other causes. mortality from the lymphoma arises\nearly, and seems to depend little on the environment. deaths from other causes\narise earlier in the conventional environment than in the germ-free one. see also\n(cid:1)\nexample 10.38.\n\n "}, {"Page_number": 213, "text": "5.4 \u00b7 survival data\n\n201\n\nfigure 5.11 estimated\ncumulative hazard\nfunctions for deaths from\nlymphoma (left) and other\ncauses (right), in\nconventional (solid) and\ngerm-free (dots)\nenvironments.\n\nd\nr\na\nz\na\nh\n\n \n\ne\nv\ni\nt\n\nl\n\na\nu\nm\nu\nc\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n0\n\n.\n\n0\n\n.\n\n0\n\nd\nr\na\nz\na\nh\n\n \n\ne\nv\ni\nt\n\nl\n\na\nu\nm\nu\nc\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n0\n\n.\n\n0\n\n.\n\n0\n\n0\n\n200 400 600 800 1000\n\n0\n\n200 400 600 800 1000\n\ndays\n\ndays\n\nfrailty\nthe discussion above presupposes that all units have the same propensity to fail. in\npractice this is unrealistic \u2014 some cars are more reliable than others, some persons\nhealthier than others, and so forth \u2014 and it may be important to build heterogene-\nity into models for survival. one reason for this is that allowing the failure rate to\nvary across units may greatly change the interpretation of the hazard function. it is\ntempting to view the population hazard function as a measure of how the risk for\neach unit changes as a function of time. for example, the fact that the divorce rate\ntypically increases to a maximum a few years after marriage and thereafter decreases\nis sometimes interpreted as meaning that the typical marriage experiences increas-\ning difficulties, but that if these are resolved there is eventually a more stable union.\na unimodal divorce rate can be generated, however, by supposing that the hazard\nof failure increases with the duration of each marriage, but that the initial value of\nthis hazard varies randomly from couple to couple. if this second interpretation is\ncorrect, then the population hazard function depends both on hazards for individual\nmarriages and on variation across them, and reflects a selection process whereby the\nmarriages most at risk tend to fail quickly, leaving those that were more stable to begin\nwith. thus the hazard rate is a more complicated quantity than it might seem at first\nsight.\n\none approach is to represent heterogeneity using the outcome of a positive random\nvariable, z, known as a frailty. wesuppose that z varies across units according to\na density f z (z), and that at time y the hazard function for a unit for whom z = z is\nzh(y); thus the cumulative hazard to that time is z h(y). units whose z is large have\nhigh hazard functions and tend to fail sooner than those whose frailty is low. if known,\nthe value of z could be incorporated into the analysis by modifying the likelihood,\nbut wesuppose it is unobserved, perhaps representing unobserveable genetic and\nenvironmental differences among units, and use it to model heterogeneity in the\ndata.\nas the survivor function for a unit with frailty z may be expressed as pr(y \u2265 y |\nz = z) = exp{\u2212z h(y)}, the survivor function for a unit taken randomly from the\n\n "}, {"Page_number": 214, "text": "5 \u00b7 models\n\n202\n\npopulation is\n\n(cid:15) \u221e\n(cid:15) \u221e\n\n0\n\npr(y \u2265 y) =\n=\n= m {\u2212h(y)} ,\n\npr(y \u2265 y | z = z) f z (z) dz\nexp{\u2212z h(y)} f z (z) dz\n\n0\n\nwhere m is the moment-generating function of z. thus the cumulative hazard function\nfor the population is \u2212 log m{\u2212h(y)}. the densities of z conditional on failure at y\nand conditional on survival at least to y,\n\nf (z | y = y) =\n\nf z (z | y \u2265 y) =\n\n0\n\nz f z (z) exp{\u2212z h(y)}\nz f z (z) exp{\u2212z h(y)} dz\n\n(cid:17) \u221e\n(cid:17) \u221e\n0 exp{\u2212z h(y)} f z (z) dz\n\n\u2212z h(y) f z (z)\n\ne\n\n,\n\n,\n\nz > 0,\n\ncan be used to see how frailty depends on failure and on survival.\nexample 5.32 (logistic hazard) let \u03b2 > 0 and h(y) = e\u03b2y \u2212 1, so a unit with\nfrailty z has hazard z\u03b2e\u03b2y; this increases exponentially. suppose also that z has the\ngamma density with mean \u03b1\u03b2\u22121/(1 + \u03b1) and shape parameter \u03b2\u22121. then m(u) =\n{1 \u2212 \u03b1u/(1 + \u03b1)}\u22121/\u03b2, and the population cumulative hazard function,\n\n\u2212 log m {\u2212h(y)} = 1\n\n\u03b2\n\nlog\n\n(cid:18)\n\n(cid:19)\n\n,\n\n1 + \u03b1e\u03b2y\n1 + \u03b1\n\nis the same as that fitted to the data on old age in example 5.29. thus although each\nunit has a constant hazard, the effect of frailty is that the population hazard has an s-\nshaped logistic form, because of the selective effect of the early failure of the weakest\nunits.\nsimple calculations show that the density of frailties among those units failing\nat time y is gamma with mean \u03b1(1 + \u03b2\u22121)/(1 + \u03b1e\u03b2y) and shape parameter 1 +\n\u03b2\u22121, while that among those units who have not failed at time y is gamma with\ncorresponding parameters \u03b1\u03b2\u22121/(1 + \u03b1e\u03b2y) and \u03b2\u22121. both of these are decreasing in\ny, showing how the tendency for units with high frailties to fail first leads to survival\nof the fittest.\n\ninformation on unit hazard functions would be needed before such a model could\nbe regarded as a serious explanation of the good fit of the logistic hazard for the\ndata on old age. absent such knowledge, the model is best regarded as suggesting a\npossible mechanism for the observed phenomenon, and as indicating the type of data\n(cid:1)\nneeded for a more detailed investigation.\n\nevidently frailty has the potential to greatly complicate the analysis of population\n\nphenomena. it also complicates group comparisons (problem 5.15).\n\n "}, {"Page_number": 215, "text": "5.5 \u00b7 missing data\n\n203\n\nexercises 5.4\n1\n\nshow that if there is no censoring, the product-limit estimator may be written (cid:8)f(y) =\n\u22121#{i : yi > y}, and hence show that in this case 1 \u2212 (cid:8)f(y) equals the empirical distri-\n\nn\nbution function (2.3). find greenwood\u2019s formula, and comment.\nsuggest physical phenomena that might give increasing, decreasing, and bathtub-shaped\nuse the relation f(y) = exp{\u2212(cid:17)\nhazard functions. sketch the corresponding survivor functions.\n0 h(u)du} between the survivor and hazard functions\nto find the survivor functions corresponding to the following hazards: (a) h(y) = \u03bb; (b)\nh(y) = \u03bby\u03b1; (c) h(y) = \u03b1y\u03ba\u22121/(\u03b2 + y\u03ba). in each case state what the distribution is.\nshow that e{1/ h(y )} =e( y ) and hence find the means in (a), (b), and (c).\nthe mean excess life function is defined as e(y) = e(y \u2212 y | y > y). show that\n\ny\n\n(cid:15) \u221e\n\ny\n\n(cid:14)\n\n(cid:15)\n\n\u2212\n\ne(y) = f(y)\n\u22121\n\nf(u) du\n(y) + q(y) = 0 for a suitable q(y).\n(cid:6)\n\nand deduce that e(y) satisfies the equation e(y)q\nhence show that provided the underlying density is continuous,\n\n(cid:16)\n\nf(y) = e(0)\ne(y)\n\nexp\n\ny\n\n1\n\ne(u)\n\n0\n\ndu\n\n.\n\nas a check on this, find e(y) and hence f(y) for the exponential density.\none approach to modelling survival is in terms of e(y). for human lifetime data, let\ne(y) = \u03b3 (1 \u2212 y/\u03b8)\u03b2, where \u03b8 is an upper endpoint and \u03b2, \u03b3 > 0. find the corresponding\nsurvivor and hazard functions, and comment.\n(cid:26)fi (y)\u03b2i is also a survivor function, and find the corre-\nif f1(y), . . . ,f k(y) are the survivor functions of independent positive random variables\nand \u03b21, . . . , \u03b2k > 0, show that\nsponding hazard and cumulative hazard functions.\nsuppose that k = 2 and the survivor functions are (i) log-logistic, (ii) log-normal and (iii)\nweibull. show that in the first two cases new models are obtained, but that in the third the\nparameters are not identifiable.\nan empirical estimate of the survivor function f(y) when data y1, . . . , yn are not cen-\nsored is given by (cid:8)f(y) = #{ j : y j > y}/(n + 1). suggest how plots of log{\u2212 log (cid:8)f(y j )}\nagainst log y j may be used to indicate if the data have weibull or exponential distri-\nbutions. describe the corresponding plot for the gumbel distribution function f(y) =\nexp[\u2212 exp{\u2212(y \u2212 \u03b7)/\u03b1}].\nshow that the log likelihood (5.26) may be expressed as\n\n(cid:15) \u221e\n\n(cid:15) \u221e\n\n(cid:6)(\u03b8) =\n\nlog h(y; \u03b8) d d(y) \u2212\n\nr(y) d h(y; \u03b8),\n\nif in doubt, think of\nfailures of your car,\nfridge, computer, . . .\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n0\n\n0\n\nwhere d(y) is astep function with jumps of size one at the values of y that are failures\nand r(y) isthe number of units at risk of failure at time y. establish that both integrals\nare over finite ranges. such expressions are useful in a general treatment of likelihood\ninference for failure data.\n\n5.5 missing data\n5.5.1 types of missingness\nmissing observations arise in many applications, but particularly in data from living\nsubjects, for example when frost kills a plant or the laboratory cat kills some experi-\nmental mice. they are common in data on humans, who may agree to take part in a\n\n "}, {"Page_number": 216, "text": "204\n\n5 \u00b7 models\n\ntwo-year study and then drop out after six months, or refuse to answer questions about\ntheir salaries or sex-lives. they may occur by accident or by design, for example when\nlifetimes are censored at the end of a survival study (section 5.4).\n\nthe central problem they pose is obvious: little can be said about unknown data,\neven if the pattern of missingness suggests its cause and hence indicates to what\nextent remaining observations can be trusted and lost ones imputed. loss of data\nwill clearly increase uncertainty, but a more malign effect is that inferences from the\ndata are sharply limited unless we are prepared to make assumptions that the data\nthemselves cannot verify. thus, if data are missing or might be missing it is essential\nto consider possible underlying mechanisms and their potential effect on inferences.\nthe discussion below is intended to focus thought about these.\n\nsuppose that our goal is inference for a parameter \u03b8 based on data that would\nideally consist of n independent pairs (x, y ), but that some values of y are missing,\nas shown by an indicator variable, i . thus the data on an individual have form (x, y, 1)\nor (x, ?, 0). we suppose that although the missingness mechanism pr(i = 0 | x, y)\nmay depend on x and y, itdoes not involve \u03b8. then the likelihood contribution from\nan individual with complete data is the joint density of x, y and i , which we write as\n\npr(i = 1 | x, y) f (y | x; \u03b8) f (x; \u03b8),\n\nwhile if y is unknown we use the marginal density of x and i ,\npr(i = 0 | x, y) f (y | x; \u03b8) f (x; \u03b8) dy.\n\n(cid:15)\n\n(5.30)\n\nthere are now three possibilities:\n\nr data are missing completely at random, that is, pr(i = 0 | x, y) = pr(i = 0) is\nr data are missing at random, that is, pr(i = 0 | x, y) = pr(i = 0 | x) depends\nr there is non-ignorable non-response, meaning that pr(i = 0 | x, y) depends on\n\nindependent both of x and y, and (5.30) reduces to pr(i = 0) f (x; \u03b8);\non x but not on y, and (5.30) equals pr(i = 0 | x) f (x; \u03b8); and\n\ny and possibly also on x.\n\nin the first two of these, which are often grouped as ignorable non-response,\ni carries no information about \u03b8 and can be omitted for most likelihood infer-\nences. to see why, suppose that we have n independent observations of form\n(x1, y1, i1), . . . ,( xn, yn, in), let m be the set of j for which y j is unobserved, and\nsuppose that data are missing at random. then the likelihood is\n\npr(i j = 1 | x j ) f (x j , y j ; \u03b8)\n\nl(\u03b8) =\n\u221d\n\n(cid:2)\n(cid:2)\nj\u2208m\nj\u2208m\n\n(cid:2)\nj(cid:13)\u2208m\n\npr(i j = 0 | x j ) f (x j ; \u03b8) \u00d7\nf (x j ; \u03b8) \u00d7\n\nf (x j , y j ; \u03b8),\n\n(cid:2)\nj(cid:13)\u2208m\n\nbecause the terms involving i j do not depend on \u03b8. thus the missing data mecha-\n\nnism does not affect maximum likelihood estimates(cid:8)\u03b8, likelihood ratio statistics or\nthe observed information j ((cid:8)\u03b8). it does affect the expected information, however, so\nstandard errors for (cid:8)\u03b8 should be based on j ((cid:8)\u03b8)\n\u22121; see the discussion of likelihood\n\n "}, {"Page_number": 217, "text": "figure 5.12 missing\ndata in straight-line\nregression for venice\nsea-level data. clockwise\nfrom top left: original\ndata, data with values\nmissing completely at\nrandom, data with values\nmissing at random \u2014\nmissingness depends on x\nbut not on y, and data with\nnon-ignorable\nnon-response \u2014\nmissingness depends on\nboth x and y. missing\nvalues are represented by\na small dot. the dotted\nline is the fit from the full\ndata, the solid lines those\nfrom the non-missing\ndata.\n\n5.5 \u00b7 missing data\n\n\u2022\n\n205\n\n\u2022\n\n)\n\nm\nc\n(\n \nl\n\ne\nv\ne\n\nl\n \n\na\ne\ns\n\n0\n8\n1\n\n0\n4\n1\n\n0\n0\n1\n\n0\n8\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\n\u2022\u2022\n\n\u2022\n\u2022\u2022\u2022\n\n\u2022\n\n)\n\nm\nc\n(\n \nl\n\ne\nv\ne\n\nl\n \n\na\ne\ns\n\n0\n8\n1\n\n0\n4\n1\n\n0\n0\n1\n\n0\n8\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\u2022\n\u2022\u2022\n\n\u2022\u2022\n\u2022\n\n\u2022\n\u2022\u2022\n\u2022\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n..\n\u2022\n\u2022\n\n\u2022\n.\n\n\u2022\n...\n\n\u2022\n\n\u2022\n\n\u2022\n\n.\n\u2022\n.\n\n\u2022\n.\n\n\u2022 \u2022\n.\n..\n\n\u2022\n.\n\u2022\n\n.\n..\n.\n\u2022\n\n\u2022\n\n. ..\n\u2022\n\u2022\n\n.\n\n\u2022\n\n\u2022\u2022\n\u2022\u2022 \u2022\n.\n\u2022\n\n1930\n\n1950\n\n1970\n\n1930\n\n1950\n\n1970\n\nyear\n\n.\n\nyear\n\n.\n\n)\n\nm\nc\n(\n \nl\n\ne\nv\ne\n\nl\n \n\na\ne\ns\n\n0\n8\n1\n\n0\n4\n1\n\n0\n0\n1\n\n0\n8\n\n.\n\n.\n.\n.\n\u2022\u2022\u2022\n\u2022\n\n..\n\n\u2022\n.\n\u2022\u2022\n\n\u2022\n\n)\n\nm\nc\n(\n \nl\n\ne\nv\ne\n\nl\n \n\na\ne\ns\n\n0\n8\n1\n\n0\n4\n1\n\n0\n0\n1\n\n0\n8\n\n.\n\n.\n\n.\n.\n.\n\n\u2022\u2022\n\u2022 \u2022\n\u2022\u2022\n\u2022\u2022\n\n\u2022 \u2022\n\u2022\u2022\n\u2022\u2022\n\n\u2022\u2022 \u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\n.\n\n.\n\u2022\u2022 \u2022\n.\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n.\n\u2022\n..\n\u2022\n.\n\n\u2022\u2022\n\n\u2022\n\u2022\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n.\n.\n.\n\n\u2022\n.\n\n\u2022 \u2022\n.\n\u2022\n.\n\n\u2022\u2022\n\u2022\n\n\u2022\n..\n.\n\u2022\n\n.\n\n\u2022\n.\n\n...\n\n\u2022\n\n.\n\n.\n\u2022\n..\n.\n.\n.\n\n1930\n\n1950\n\n1970\n\n1930\n\n1950\n\n1970\n\nyear\n\nyear\n\ninference in section 5.4 and problem 5.16. a similar argument applies if data are\nmissing completely at random. if the non-response is non-ignorable, however, the\ndensity of i is no longer a constant of integration in (5.30). in that case, knowledge\nof the observed i j is informative about \u03b8, and likelihood inference is possible only if\npr(i = 0 | x, y) can be specified.\n\nexample 5.33 (venice sea level data) the upper left panel of figure 5.12 shows\nthe data of example 5.1. here x represents a year in the range 1931\u20131981; in the\nabsence of sea level it contains no information about any trend. the annual maximum\nsea level y is taken to be a normal variable with mean \u03b20 + \u03b21(x j \u2212 x) and variance\n\u03c3 2; hence \u03b8 = (\u03b20, \u03b21, \u03c3 2) and the full data likelihood has form f (y | x; \u03b8) f (x), of\nwhich f (x) isignored.\n\nthe upper right panel of figure 5.12 shows the effect of data missing completely at\nrandom, while in the panel below the probability that a value is unobserved depends\non x but not on y; the data are missing at random, with earlier observations missing\nmore often than later ones. the lower left panel shows non-ignorable non-response,\nbecause the probability of missingness depends on y and on x; values of y that are\nlarger than their means are more likely to be missing. here the fitted line differs from\nthose in the other panels due to bias induced by the missingness mechanism.\n\n "}, {"Page_number": 218, "text": "table 5.8 average\nestimates and standard\nerrors for missing value\nsimulation based on\nvenice data, for full\ndataset, with data missing\ncompletely at random\n(mcar), missing at\nrandom (mar) and with\nnon-ignorable\nnon-response (nin). 1000\nsamples were taken.\nstandard errors for the\n\naverages for(cid:8)\u03b20 and(cid:8)\u03b21 are\n\nat most 0.16 and 0.01;\nthose for their standard\nerrors are at most 0.03 and\n0.002.\n\n206\n\n5 \u00b7 models\n\naverage estimate (average standard error)\n\ntruth\n\nfull\n\nmcar\n\nmar\n\nnin\n\n\u03b20\n\u03b21\n\n120\n0.50\n\n120 (2.79)\n0.49 (0.19)\n\n120 (4.02)\n0.48 (0.28)\n\n120 (4.73)\n0.50 (0.32)\n\n132 (3.67)\n0.20 (0.25)\n\nto assess the extent of this bias, we generated 1000 samples from a model with\nparameters \u03b20 = 120, \u03b21 = 0.5 and \u03c3 = 20, close to the estimates for the venice data\nand with the same covariate x. wethen computed maximum likelihood estimates for\nthe full data and for those observations that remain after applying the non-response\nmechanisms\n\n\uf8f1\uf8f2\n\uf8f3 0.5,\n\u0001{0.05(x \u2212 x)} ,\n\u0001 [0.05(x \u2212 x) + {y \u2212 \u03b20 \u2212 \u03b21(x \u2212 x)} /\u03c3 ] ,\n\npr(i = 1 | x, y) =\n\nto give data missing completely at random, missing at random, and with non-ignorable\nnon-response. in each case roughly one-half of the observations are missing. table 5.8\nshows that although data loss increases the variability of the estimates, their means\nare unaffected, provided the probability of non-response does not depend on y. if the\nprobability of missingness depends on the response, however, estimates based on the\n(cid:1)\nremaining data become entirely unreliable.\n\nthe message of this example is bleak: when there is non-ignorable non-response\nand a non-negligible proportion of the data is missing, the only possible rescue is\nto specify the missingness mechanism correctly. in practice it is typically hard to\ntell if missingness is ignorable or not, so fully reliable inference is largely out of\nreach. sensitivity analysis to assess how heavily the conclusions depend on plausible\nmechanisms for non-response is then useful, and we now outline one approach to this.\n\npublication bias\nbreakthroughs in medical science are regularly reported, offering hope of a new cure\nor suggesting that some enjoyable activity has dire consequences. it is unwise to take\nthem all at face value, however, as some turn out to be spurious. one reason for this\nis the publication process to which they are subjected. once a study is completed, an\narticle describing it is typically submitted to a medical journal for peer review. if the\nstudy design and analysis are found to be satisfactory, a decision is taken whether the\narticle should be published. this decision is likely to be positive if the study reports a\nsignificant result or if it involved a large number of patients, but will often be negative\nif no association is found \u2014 there is no \u2018significant finding\u2019 \u2014 particularly if the\nstudy is small and hence deemed unreliable. the end-result of this selection process\nis publication bias, whereby studies finding associations tend to be the ones published,\neven if in fact there is no effect. recommendations to change medical practice are\nusually based not on a single study \u2014 unless it is huge, involving many thousands of\npatients \u2014 but on a meta-analysis that combines results from all published studies.\n\n "}, {"Page_number": 219, "text": "5.5 \u00b7 missing data\n\n207\n\nas studies finding no effect are more likely to remain unpublished, however, wrong\nconclusions can be drawn.\n\nfor asimple model of this selection process, suppose that we wish to estimate a\nparameter \u00b5 that represents the effect of a treatment, subject to possible publication\n\nbias. a study based on n individuals produces an estimate (cid:8)\u00b5, normally distributed\nby a variable z, with the study published if z is positive. we suppose that (cid:8)\u00b5 and z\n\nwith mean \u00b5 and variance \u03c3 2/n. the vagaries of the editorial process are represented\n\nare related by\n\n(cid:8)\u00b5 = \u00b5 + \u03c3 n\n\n\u22121/2u1,\n\nz = \u03b30 + \u03b31n1/2 + u2,\n\nwith u1 and u2 standard normal variables with correlation \u03c1 \u2265 0. one interpretation of\nu1 is as the standardized form n1/2((cid:8)\u00b5 \u2212 \u00b5)/\u03c3 of(cid:8)\u00b5, which is used to assess significance\ndiscussion, y and x correspond to(cid:8)\u00b5 and n, but now neither is observed if the study\n\nof the treatment effect. if \u03c1 > 0 then publication becomes increasingly likely as\nu1 increases, because z is positively correlated with u1. interms of our previous\n\nis unpublished.\n\n(cid:25)\n\nthe missingness indicator i equals one if z > 0 and zero otherwise, so the marginal\n\nprobability of publication is\n\n(cid:24)\nu2 > \u2212\u03b30 \u2212 \u03b31n1/2\n\n(cid:25) = \u0001\n\n(cid:24)\n\n.\n\n(cid:14)\n\n(5.31)\n\n\u03b30 + \u03b31n1/2\n\n\u03b30 + \u03b31n1/2 + \u03c1n1/2((cid:8)\u00b5 \u2212 \u00b5)/\u03c3\n\nif \u03b31 > 0 this increases with n: large studies are then more likely to be published,\n\npr(i = 1) = pr(z > 0) = pr\nwhatever their outcome. conditional on the value of (cid:8)\u00b5, (3.21) implies that z is\nnormal with mean \u03b30 + \u03b31n1/2 + \u03c1n1/2((cid:8)\u00b5 \u2212 \u00b5)/\u03c3 and variance 1 \u2212 \u03c12. hence the\nconditional probability of publication given(cid:8)\u00b5 is\npr(i = 1 | (cid:8)\u00b5) = pr (z > 0 | (cid:8)\u00b5) = \u0001\nif \u03c1 > 0, this is increasing in (cid:8)\u00b5: the probability that a study is published increases\nwith the estimated treatment effect, at each study size n. moreover, as (cid:8)\u00b5 appears in\n(5.32), non-response \u2014 non-publication of a study \u2014 is non-ignorable. if \u03c1 = 0,\nthat a study is published depend on its size n but not on its outcome(cid:8)\u00b5.\n(5.32) reduces to (5.31). unpublished studies are then missing at random: the odds\nconditional on publication, the mean of(cid:8)\u00b5 is\ne ((cid:8)\u00b5 | z > 0) = \u00b5 + \u03c1\u03c3 n\n\u22121/2\u03b6\n\n(5.33)\nwhere \u03b6 (u) = \u03c6(u)/\u0001(u) isthe ratio of the standard normal density and distribution\nfunctions. if \u03b31, \u03c1 > 0, then e((cid:8)\u00b5 | z > 0) > \u00b5, sothe mean of a published (cid:8)\u00b5 is\n\n\u03b30 + \u03b31n1/2\n\n(1 \u2212 \u03c12)1/2\n\n(5.32)\n\n(cid:16)\n\n(cid:24)\n\n(cid:25)\n\n,\n\n.\n\nalways larger than \u00b5, but by an amount that decreases with n. for small \u03b31, taylor\nexpansion gives\n\ne ((cid:8)\u00b5 | z > 0)\n\n.= \u00b5 + \u03c1\u03c3 \u03b31\u03b6 (cid:6)\n\n(\u03b30) + \u03c1\u03c3 \u03b6 (\u03b30) n\n\n\u22121/2,\n\nso the conditional mean of (cid:8)\u00b5 in published studies is roughly linear in n\n\n\u22121/2. asjust\nthree parameters \u2014 intercept, slope and variance \u2014 can be estimated from a linear\nfit, simultaneous estimation of \u00b5, \u03c1, \u03c3 2, \u03b30, and \u03b31 is infeasible. in order to assess\n\n "}, {"Page_number": 220, "text": "208\n\n5 \u00b7 models\n\ntrial\n\nmagnesium\n\nr/m\n\ncontrol\n\nr/m\n\nn\n\n(cid:8)\u00b5\n\n(v/n)1/2\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n1/25\n1/40\n2/48\n1/50\n4/56\n3/66\n2/92\n27/135\n10/160\n90/1159\n\n3/23\n2/36\n2/46\n9/53\n14/56\n6/66\n7/93\n43/135\n8/156\n\n118/1157\n\nmeta-analysis\n\nisis-4\n\n2216/29011\n\n2103/29039\n\n48\n1.18\n76\n0.80\n94\n0.04\n103\n2.14\n112\n1.25\n132\n0.69\n185\n1.24\n270\n0.47\n316 \u22120.20\n0.27\n2316\n\n3652\n0.41\n58050 \u22120.05\n\n1.05\n0.83\n0.75\n0.72\n0.69\n0.63\n0.53\n0.44\n0.41\n0.15\n\n0.11\n\n0.03\n\ntable 5.9 data from 11\nclinical trials to compare\nmagnesium treatment for\nheart attacks with control,\nwith n patients randomly\nallocated to treatment and\ncontrol; there are r deaths\nout of m patients in each\ngroup (copas, 1999). the\nestimated log treatment\n\neffect(cid:8)\u00b5 will be positive if\n\ntreatment is effective;\n(v/n)1/2 is its standard\nerror. the huge isis-4\ntrial is not included in the\nmeta-analysis.\n\nthe impact of selection in the following example, we fix \u03b30 and \u03b31 to give plausible\nprobabilities of publication for small and large samples, and consider inference for\n\u03b8 = (\u00b5, \u03c1, \u03c3 ).\n(cid:8)\u00b51, . . . ,(cid:8)\u00b5k from published studies of sizes n1, . . . ,n k. as(cid:8)\u00b5 j is observed only con-\n\nnow suppose that we wish to estimate \u00b5 based on k independent estimates\n\nditional on its publication, the likelihood contribution from study j is\n\nf ((cid:8)\u00b5 j | z j > 0; \u03b8) = f ((cid:8)\u00b5 j ; \u03b8)pr(z j > 0 | (cid:8)\u00b5 j ; \u03b8)\n\n.\n\npr(z j > 0)\n\n(cid:14)\n\nlog \u03c3 2 + n j\n\nrecalling (5.31) and (5.32), we see that the overall log likelihood is\n\nthe marginal density of (cid:8)\u00b5 j is normal with mean \u00b5 and variance \u03c3 2/n j , and on\n(cid:6)(\u00b5, \u03c1, \u03c3 2) \u2261 \u2212 k(cid:6)\nj=1\nwhere a j = \u03b30 + \u03b31n1/2\nthe simplest meta-analysis ignores the possibility of selection bias and amounts\nto setting \u03c1 = 0, presuming the publication of a study to be unrelated to its result.\nif this is so, then a j = b j and the log likelihood is easily maximized, the maximum\nlikelihood estimate of \u00b5 being the weighted average\n\n2\u03c3 2 ((cid:8)\u00b5 j \u2212 \u00b5)2 + log \u0001(a j ) \u2212 log \u0001(b j )\n\n1\n2\nand b j = (1 \u2212 \u03c12)\n\n((cid:8)\u00b5 j \u2212 \u00b5)/\u03c3}.\n\n\u22121/2{a j + \u03c1n1/2\n\n(5.34)\n\n(cid:16)\n\n,\n\nj\n\nj\n\nwhen \u03c1 = 0, this estimator is normal with mean \u00b5 and variance \u03c3 2/\n\u03c1 > 0, then (5.33) implies that(cid:8)\u00b50 will tend to exceed \u00b5; the treatment effect will tend\n\nn j . if in fact\n\nto be overstated by the published data.\n\nexample 5.34 (magnesium data) table 5.9 shows data from clinical trials on the\nuse of intraveneous magnesium to treat patients with suspected acute myocardial\n\n(5.35)\n\n(cid:1)\n\n(cid:1)\nn j(cid:8)\u00b5 j(cid:1)\n\n.\n\nn j\n\n "}, {"Page_number": 221, "text": "figure 5.13 likelihood\nanalysis of magnesium\ndata. left: funnel plot\n\nshowing variation of(cid:8)\u00b5\n\nwith trial size n, with 95%\nconfidence interval for \u00b5\nbased on each trial. the\nvertical dotted line is the\ncombined estimate of \u00b5\nfrom the ten small trials,\nignoring the possibility of\npublication bias; the\nvertical solid line shows\nno treatment effect. the\nsolid line is the estimated\nconditional mean (5.33).\n\nright: contours of(cid:8)\u00b5 as a\n\nfunction of \u03b30 and \u03b31.\n\nmyocardial infarction is\nthe medical term for heart\nattack \u2014 death of part of\nthe heart muscle because\nof lack of oxygen and\nother nutrients.\n\n5.5 \u00b7 missing data\n\n\u2022\n\nn\n\n \n\ne\nz\ns\n \nl\n\ni\n\na\ni\nr\nt\n\n0\n0\n5\n\n0\n0\n1\n\n0\n5\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n0.5\n\n5.0\n\nestimate\n\n1\na\nm\nm\na\ng\n\n2\n1\n\n.\n\n0\n\n0\n1\n\n.\n\n0\n\n8\n0\n\n.\n\n0\n\n6\n0\n\n.\n\n0\n\n4\n0\n0\n\n.\n\n209\n\n0.38\n\n0.22\n\n0.26\n\n0.3\n\n0.34\n\n-2.5\n\n-1.5\n\n-0.5\n\n0.0\n\ngamma0\n\nlog(r1/m1). now m1\n\ntween control and treated groups, the estimated treatment effect (cid:8)\u00b5 = log(r2/m2) \u2212\ninfarction. for each trial, we consider the difference in log proportion of deaths be-\n.= m2 for each trial and the proportion of deaths is small, so\nthe delta method suggests that an approximate variance for (cid:8)\u00b5 is 4/((cid:8)\u03bbn), where\n(cid:8)\u03bb = 0.097 is the death rate estimated from all the trials and n = m1 + m2 is the size\nof each trial. the combined sample is large enough to treat(cid:8)\u03bb and hence \u03c3 2 = 4/(cid:8)\u03bb\nas constant. although the estimated treatment effects (cid:8)\u00b5 from the ten small trials\n\nfigure 5.13; note the logarithmic axes. symmetry about the overall weighted average\n(5.35) would show lack of publication bias, but the visible asymmetry suggests that\n\nare individually inconclusive, the meta-analysis estimate (5.35) is 0.41 with stan-\ndard error 0.11; this gives an estimated reduction in the probability of death by\na factor exp(0.41) = 1.51 with 0.95 confidence interval (1.22,1.86). a similar pub-\nlished meta-analysis concluded that the magnesium treatment was \u2018effective, safe and\nfor amore skeptical view, consider the funnel plot of n and exp((cid:8)\u00b5) inthe left panel of\nsimple\u2019.\nsmall studies tend to be published only if(cid:8)\u00b5 is sufficiently positive.\nthe right panel shows how the maximum likelihood estimate of \u00b5 from (5.34)\ndepends on \u03b30 and \u03b31. the contours are very roughly parallel with slope \u22120.05,\nsuggesting that the maximum likelihood estimate varies mainly as a function of\n\u03b30 + 4001/2\u03b31, orequivalently the probability \u0001(\u03b30 + 4001/2\u03b31) that a study of size\nn = 400 is published. for example, if the selection probabilities are 0.9 and 0.1 for\nthe largest and smallest studies in table 5.9, then this probability is 0.32, (cid:8)\u03c1 = 0.5\n\nand the estimated treatment effect is 0.27 with standard error 0.12 from observed\ninformation. this estimate is substantially less than the value 0.41 obtained when\n\u03c1 = 0, and the significance of the estimated treatment effect is much reduced. the\nestimated conditional mean (5.33) in the left panel shows how the selection due to\nhaving \u03c1 > 0 affects the mean of published studies.\n\nthe sensitivity of the estimated effect to potential publication bias suggests that\ntreatment policy conclusions cannot be based on table 5.9. indeed, a subsequent\n(cid:1)\nmuch larger trial \u2014 isis-4 \u2014 found no evidence that magnesium is effective.\n\n "}, {"Page_number": 222, "text": "210\n\n5 \u00b7 models\n\npublication bias is an example of selection bias, where the mechanism underlying\nthe choice of data introduces an uncontrolled bias into the sample. this is endemic\nin observational studies, for example in epidemiology and the social sciences, and it\ncan greatly weaken what conclusions may be drawn.\n\n5.5.2 em algorithm\nthe fitting of certain models is simplified by treating the observed data as an in-\ncomplete version of an ideal dataset whose analysis would have been easy. the key\nidea is to estimate the log likelihood contribution from the missing data by its con-\nditional value given the observed data. this yields a very general and widely used\nestimation-maximization or em algorithm for maximum likelihood estimation.\n\nlet y denote the observed data and u the unobserved variables. our goal is to use\nthe observed value y of y for inference on a parameter \u03b8, inmodels where we cannot\neasily calculate the density\n\n(cid:15)\n\nf (y; \u03b8) =\n\nf (y | u; \u03b8) f (u; \u03b8) du\n\nand hence cannot readily compute the likelihood for \u03b8 based only on y. wewrite the\ncomplete-data log likelihood based on both y and the value u of u as\n\nlog f (y, u; \u03b8) = log f (y; \u03b8) + log f (u | y; \u03b8),\n\n(5.36)\n\nwhere the first term on the right is the observed-data log likelihood (cid:6)(\u03b8). as the value\nof u is unobserved, the best we can do is to remove it by taking expectation of (5.36)\nwith respect to the conditional density f (u | y; \u03b8(cid:6)\n) of u given that y = y; for reasons\nthat will become apparent we use \u03b8(cid:6)\n\nrather than \u03b8 for this expectation. this yields\n\ne{log f (y, u ; \u03b8) | y = y; \u03b8(cid:6)} =(cid:6)(\u03b8 ) + e{log f (u | y ; \u03b8) | y = y; \u03b8(cid:6)},\n\n(5.37)\n\nwhich we express as\n\nq(\u03b8; \u03b8(cid:6)\n\n) = (cid:6)(\u03b8) + c(\u03b8; \u03b8(cid:6)\n\n).\n\n(5.38)\n\nand treat q(\u03b8; \u03b8(cid:6)\n\nwe now fix\u03b8 (cid:6)\n) asfunctions of \u03b8. ifthe conditional\ndistribution of u given y = y is non-degenerate and no two values of \u03b8 give the\nsame model, then the argument at (4.31) applied to f (y | u; \u03b8) shows that c(\u03b8(cid:6)\n) \u2265\nc(\u03b8; \u03b8(cid:6)\n\n) and c(\u03b8; \u03b8(cid:6)\n\n; \u03b8(cid:6)\n\n; \u03b8(cid:6)\n\n. hence\n\n), with equality only when \u03b8 = \u03b8(cid:6)\nq(\u03b8; \u03b8(cid:6)\n\n) \u2265 q(\u03b8(cid:6)\n\n) implies (cid:6)(\u03b8) \u2212 (cid:6)(\u03b8(cid:6)\n\n) isstationary at \u03b8 = \u03b8(cid:6)\n\n) \u2265 c(\u03b8(cid:6)\nmoreover under mild smoothness conditions, c(\u03b8; \u03b8(cid:6)\nhence if q(\u03b8; \u03b8(cid:6)\nlog f (y, u ; \u03b8) | y = y; \u03b8(cid:6)(cid:10)\n; then\n) over \u03b8, giving \u03b8\u2020\n, say; and\n) \u2212 (cid:6)(\u03b8(cid:6)\n\nthis leads to the em algorithm: starting from an initial value \u03b8(cid:6)\ncompute q(\u03b8; \u03b8(cid:6)\ncheck if the algorithm has converged, using (cid:6)(\u03b8\u2020\nor both. if not, set \u03b8(cid:6) = \u03b8\u2020\n\nfixed, maximize q(\u03b8; \u03b8(cid:6)\n\n1.\n2. with \u03b8(cid:6)\n3.\n\n, sotoo is (cid:6)(\u03b8).\n\nand go to 1.\n\n) = e\n\n(cid:9)\n\nof \u03b8,\n\n) \u2212 c(\u03b8; \u03b8(cid:6)\n\n; \u03b8(cid:6)\n(5.39)\n) has a stationary point at \u03b8 = \u03b8(cid:6)\n\n) \u2265 0.\n\n.\n\n) if available, or|\u03b8\u2020 \u2212 \u03b8(cid:6)|,\n\n "}, {"Page_number": 223, "text": "5.5 \u00b7 missing data\n\n211\n\n; \u03b8(cid:6)\n\n; \u03b8(cid:6)\n\n) \u2265 q(\u03b8(cid:6)\n\n), we see from (5.39) that (cid:6)(\u03b8\u2020\n\nsteps 1 and 2 are the expectation (e) and maximization (m) steps of the algorithm. as\n) \u2265 (cid:6)(\u03b8(cid:6)\nthe m-step ensures that q(\u03b8\u2020\n):\n) eventually reaches a stationary value at(cid:8)\u03b8, then(cid:8)\u03b8 must maximize (cid:6)(\u03b8).\nthe log likelihood never decreases. moreover, if (cid:6)(\u03b8) has just one stationary point,\nand if q(\u03b8; \u03b8(cid:6)\nif (cid:6)(\u03b8) has more than one stationary point the algorithm may converge to a local\nmaximum of the log likelihood or to a turning point. as the em algorithm never\ndecreases the log likelihood it is more stable than newton\u2013raphson-type algorithms,\nwhich do not have this desirable property.\nas one might expect, the convergence rate of the algorithm depends on the amount\nof missing information. if knowledge of y tells us little about u , then q(\u03b8; \u03b8(cid:6)\n) and (cid:6)(\u03b8)\nwill be very different and the algorithm slow. this may be quantified by differentiating\n(5.36) and taking expectations with respect to the conditional distribution of u given\ny , to give\n\n(cid:14)\n\n\u2212 \u2202 2(cid:6)(\u03b8)\n\u2202\u03b8 \u2202\u03b8 t\n\n= e\n\n\u2212 \u2202 2 log f (y, u ; \u03b8)\n(cid:14)\n\u2212 \u2202 2 log f (u | y; \u03b8)\n\n\u2202\u03b8 \u2202\u03b8 t\n\n\u2202\u03b8 \u2202\u03b8 t\n\n\u2212 e\n\n(cid:22)(cid:22)(cid:22)(cid:22) y = y; \u03b8\n(cid:16)\n(cid:22)(cid:22)(cid:22)(cid:22) y = y; \u03b8\n\n(cid:16)\n\n,\n\nor j (\u03b8) = ic(\u03b8; y) \u2212 im(\u03b8; y), interpreted as meaning that the observed information\nequals the complete-data information minus the missing information; this is some-\ntimes called the missing information principle. if u is determined by y , then the\nconditional density f (u | y; \u03b8) is degenerate and under mild conditions the missing\ninformation will be zero. it turns out that the rate of convergence of the algorithm\n\u22121 im(\u03b8; y); values of this eigen-\nequals the largest eigenvalue of the matrix ic(\u03b8; y)\nvalue close to one imply slow convergence and occur if the missing information is a\nhigh proportion of the total.\n\nwhen the em algorithm is slow it may be worth trying to accelerate it by replacing\nthe m-step with direct maximization, assuming of course that (cid:6)(\u03b8) isunavailable. it\nturns out that (exercise 5.5.5)\n\n\u2202(cid:6)(\u03b8)\n\n\u2202\u03b8\n\n= \u2202 q(\u03b8; \u03b8(cid:6)\n\n\u2202\u03b8\n\n)\n\n,\n\n\u2202 2(cid:6)(\u03b8)\n\u2202\u03b8 \u2202\u03b8 t\n\n=\n\n\u03b8(cid:6)=\u03b8\n\n\u2202 2 q(\u03b8; \u03b8(cid:6)\n\n)\n\n\u2202\u03b8 \u2202\u03b8 t\n\n+ \u2202 2 q(\u03b8; \u03b8(cid:6)\n\n)\n\n\u2202\u03b8 \u2202\u03b8\n\n(cid:6)\n\nt\n\n.\n\n(5.40)\n\n\u03b8(cid:6)=\u03b8\n\nthus even if (cid:6)(\u03b8) isinaccessible, its derivatives may be obtained from those of q(\u03b8; \u03b8(cid:6)\n)\nprovides standard errors for the maximum likelihood estimate (cid:8)\u03b8 when q(\u03b8; \u03b8(cid:6)\nand used in a generic maximization algorithm. the second of these formulae also\n) is\n\nknown but (cid:6)(\u03b8) isnot.\n\nexample 5.35 (negative binomial model) for a toy example, suppose that con-\nditional on u = u, y is a poisson variable with mean u, and that u is gamma with\nmean \u03b8 and variance \u03b8 2/\u03bd. inference is required for \u03b8 with the shape parameter \u03bd > 0\nsupposed known. here (5.36) equals\n\ny log u \u2212 u \u2212 log y! + \u03bd log \u03bd \u2212 \u03bd log \u03b8 + (\u03bd \u2212 1) log u \u2212 \u03bdu/\u03b8 \u2212 log \u0001(\u03bd),\n\n(cid:16)(cid:22)(cid:22)(cid:22)(cid:22)\n\n(cid:14)\n\n(cid:22)(cid:22)(cid:22)(cid:22)\n\n "}, {"Page_number": 224, "text": ") for\n\nfigure 5.14 em\nalgorithm for negative\nbinomial example. left\npanel: observed-data log\nlikelihood (cid:6)(\u03b8) (solid) and\nfunctions q(\u03b8; \u03b8(cid:6)\n\u03b8(cid:6) = 1.5, 1.347 and 1.028\n(dots, from right). the\nblobs show the values of \u03b8\nthat maximize these\nfunctions, which\ncorrespond to the first,\nfifth and fortieth iterations\nof the em algorithm.\nright: convergence of em\nalgorithm (dots) and\nnewton\u2013raphson\nalgorithm (solid). the\npanel shows how\nsuccessive em iterations\nupdate \u03b8(cid:6)\nthat the em iterates\nalways increase (cid:6)(\u03b8),\nwhile the\nnewton\u2013raphson steps do\nnot.\n\nand(cid:8)\u03b8. notice\n\n212\n\n0\n\n1\n-\n\n2\n-\n\n3\n-\n\nd\no\no\nh\n\ni\nl\n\ne\nk\n\ni\nl\n \n\ng\no\nl\n\n5 \u00b7 models\n\ne\n\nt\n\na\nm\n\ni\nt\ns\ne\n\n4\n\n.\n \n\n1\n\n2\n\n.\n\n1\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n\u2022\n\n\u2022\u2022\n\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\n\n0\n\n10 20 30\n\n40 50 60\n\ntheta\n\niteration\n\n) \u2212 (1 + \u03bd/\u03b8)e(u | y = y; \u03b8(cid:6)\n\n) \u2212 \u03bd log \u03b8\n\nand hence (5.37) equals\n) = (y + \u03bd \u2212 1)e(log u | y = y; \u03b8(cid:6)\nq(\u03b8; \u03b8(cid:6)\nplus terms that depend neither on u nor on \u03b8.\ne(log u | y = y; \u03b8(cid:6)\nto compute e(u | y = y; \u03b8(cid:6)\n\nthe e-step, computation of q(\u03b8; \u03b8(cid:6)\n\n), involves two expectations, but fortunately\n) does not appear in terms that involve \u03b8 and so is not required.\n\n), note that y and u have joint density\n\nf (y | u) f (u; \u03b8) = u y\ny!\n\ne\n\n\u2212u \u00d7 \u03bd\u03bdu\u03bd\u22121\n\n\u03b8 \u03bd \u0001(\u03bd)\n\n\u2212\u03bdu/\u03b8 ,\n\ne\n\ny = 0, 1, . . . , u > 0,\n\n\u03b8 > 0,\n\nso the marginal density of y is\n\nf (y; \u03b8) =\n\nf (y | u) f (u; \u03b8, \u03bd) du = \u0001(y + \u03bd)\u03bd\u03bd\n\n(cid:15) \u221e\n\n0\n\nhence the conditional density f (u | y; \u03b8(cid:6)\nmean e(u | y = y; \u03b8(cid:6)\nq(\u03b8; \u03b8(cid:6)\n\n) = (y + \u03bd)/(1 + \u03bd/\u03b8(cid:6)\n) \u2261 \u2212(1 + \u03bd/\u03b8)(y + \u03bd)/(1 + \u03bd/\u03b8(cid:6)\nwhere we have ignored terms independent of both \u03b8 and \u03b8(cid:6)\n\nthe m-step involves maximization of q(\u03b8; \u03b8(cid:6)\n\n), and we can take\n\n) \u2212 \u03bd log \u03b8,\n.\n\n\u03b8 y\n\ny = 0, 1, . . .\n) is gamma with shape parameter y + \u03bd and\n\n(\u03b8 + \u03bd)y+\u03bd\n\n\u0001(\u03bd)y!\n\n,\n\n) over \u03b8 for fixed \u03b8(cid:6)\n\n, so wedifferentiate\n\nwith respect to \u03b8 and find that the maximizing value is\n(y + \u03bd)/(\u03b8(cid:6) + \u03bd).\n\n\u03b8\u2020 = \u03b8(cid:6)\n\n(5.41)\nin this example, therefore, the em algorithm boils down to choosing an initial \u03b8(cid:6)\nupdating it to \u03b8\u2020\n\nusing (5.41), setting \u03b8(cid:6) = \u03b8\u2020\n\nand iterating to convergence.\n\n,\n\nthe log likelihood based only on the observed data y is\n\n(cid:6)(\u03b8) = log f (y; \u03b8) \u2261 y log \u03b8 \u2212 (y + \u03bd) log(\u03b8 + \u03bd),\n\n\u03b8 > 0.\n\nthis is shown in the left panel of figure 5.14 for y = 1 and \u03bd = 15. the panel also\n) onthe first, fifth and fourtieth iterations starting at \u03b8(cid:6) =\nshows the functions q(\u03b8; \u03b8(cid:6)\n1.5, which gives the sequence \u03b8(cid:6) = 1.5, 1.45, 1.41, . . .. the functions q(\u03b8; \u03b8(cid:6)\n) are\n\n "}, {"Page_number": 225, "text": "5.5 \u00b7 missing data\n\n213\n\nmuch more concentrated than is (cid:6)(\u03b8), showing that the amount of missing information\nis large. the difference in curvature corresponds to the information lost through not\nobserving u .\nhere the unmodified em algorithm converges slowly. the right panel of figure 5.14\nillustrates this, as successive values of \u03b8\u2020\ndescend gently towards the limiting value\n\u03b8 = 1: convergence has still not been achieved after 100 iterations, at which point\n\u03b8\u2020 = 1.00056. the ratio of missing to complete-data information, 15/16, indicates\nconverges much faster, with(cid:8)\u03b8 = 1 to seven decimal places after only five iterations,\nslow convergence. the newton\u2013raphson algorithm (4.25) using the derivatives (5.40)\n\nso here it pays handsomely to use the derivative information in (5.40).\n\n(cid:1)\n\nexample 5.36 (mixture density) mixture models arise when an observation y\nis taken from a population composed of distinct subpopulations, but it is unknown\nfrom which of these y is taken. if the number p of subpopulations is finite, y has a\np-component mixture density\n\nf (y; \u03b8) = p(cid:6)\n\nr=1\n\n\u03c0r fr (y; \u03b8),\n\n0 \u2264 \u03c0r \u2264 1,\n\n\u03c0r = 1,\n\np(cid:6)\nr=1\n\nwhere \u03c0r is the probability that y comes from the rth subpopulation and fr (y; \u03b8) is its\ndensity conditional on this event. an indicator u of the subpopulation from which y\narises takes values 1, . . . , p with probabilities \u03c01, . . . , \u03c0 p. inmany applications the\ncomponents have a physical meaning, but sometimes a mixture is used simply as a\nflexible class of densities. for simplicity of notation below, let \u03b8 contain all unknown\nparameters including the \u03c0r .\n\nif the value u of u were known, the likelihood contribution from (y, u) would be\n{ fr (y; \u03b8)\u03c0r}i (u=r), giving contribution\n\n(cid:26)\n\nr\n\nlog f (y, u; \u03b8) = p(cid:6)\n\nr=1\n\ni (u = r){log \u03c0r + log fr (y; \u03b8)}\n\nto the complete-data log likelihood. in order to apply the em algorithm we must\ncompute the expectation of log f (y, u; \u03b8) over the conditional distribution\n\npr(u = r | y = y; \u03b8(cid:6)\n\n) =\n\nr = 1, . . . , p.\n\n,\n\n(5.42)\n\n(cid:1) p\n\u03c0(cid:6)\nr fr (y; \u03b8(cid:6)\n\u03c0(cid:6)\ns fs(y; \u03b8(cid:6))\ns=1\n\n)\n\nthis probability can be regarded as the weight attributable to component r if y has\nbeen observed; for compactness below we denote it by wr (y; \u03b8(cid:6)\n). the expected value\nof i (u = r) with respect to (5.42) is wr (y; \u03b8(cid:6)\n), so the expected value of the log\nlikelihood based on a random sample (y1, u1), . . . ,( yn, un) is\n\nq(\u03b8; \u03b8(cid:6)\n\n) = n(cid:6)\n= p(cid:6)\n\nj=1\n\nr=1\n\np(cid:6)\nwr (y j ; \u03b8(cid:6)\n(cid:11)\nr=1\nn(cid:6)\nj=1\n\nwr (y j ; \u03b8(cid:6)\n\n){log \u03c0r + log fr (y j ; \u03b8)}\n(cid:12)\nn(cid:6)\nj=1\n\nlog \u03c0r + p(cid:6)\n\nr=1\n\n)\n\nwr (y j ; \u03b8(cid:6)\n\n) log fr (y j ; \u03b8).\n\n "}, {"Page_number": 226, "text": "214\n\n9172\n18552\n19529\n19989\n20821\n22185\n22914\n24129\n32789\n\n9350\n18600\n19541\n20166\n20846\n22209\n23206\n24285\n34279\n\n5 \u00b7 models\n\n9483\n18927\n19547\n20175\n20875\n22242\n23241\n24289\n\n9558\n19052\n19663\n20179\n20986\n22249\n23263\n24366\n\n9775\n19070\n19846\n20196\n21137\n22314\n23484\n24717\n\n10227\n19330\n19856\n20215\n21492\n22374\n23538\n24990\n\n10406\n19343\n19863\n20221\n21701\n22495\n23542\n25633\n\n16084\n19349\n19914\n20415\n21814\n22746\n23666\n26960\n\n16170\n19440\n19918\n20629\n21921\n22747\n23706\n26995\n\n18419\n19473\n19973\n20795\n21960\n22888\n23711\n32065\n\ntable 5.10 velocities\n(km/second) of 82\ngalaxies in a survey of the\ncorona borealis region\n(roeder, 1990). the error\nis thought to be less than\n50 km/second.\n\nr\n\n=\n\u03c3 2\u2020\nr ) \u2261 \u03b8(cid:6)\n\n\u22121\n\n(cid:1)\n\nr = n\n\u2020\n\n) over \u03b8 for fixed \u03b8(cid:6)\n\nthe m step of the algorithm entails maximizing q(\u03b8; \u03b8(cid:6)\n\n. asthe\n\u2020\n\u03c0r do not usually appear in the component density fr , the maximizing values \u03c0\nr are\nobtained from the first term of q, which corresponds to a multinomial log likelihood;\nsee (4.45). thus \u03c0\nthat form the second term of q(\u03b8; \u03b8(cid:6)\n(cid:1)\nvariance \u03c3 2\nr , simple calculations give the weighted estimates\n)(y j \u2212 \u00b5\n(cid:1)\nj=1 wr (y j ; \u03b8(cid:6)\nn\n)y j\nj=1 wr (y j ; \u03b8(cid:6))\n\nestimates of the parameters of the fr are obtained from the weighted log likelihoods\n). for example, if fr is normal with mean \u00b5r and\n\n), the average weight for component r.\n\n(cid:1)\nj=1 wr (y j ; \u03b8(cid:6)\n\nr = 1, . . . , p.\n\nj=1 wr (y j ; \u03b8(cid:6))\n\nj wr (y j ; \u03b8(cid:6)\n\n(cid:1)\n\n\u2020\nr )2\n\n\u00b5\u2020\n\n=\n\n,\n\nn\n\nn\n\nn\n\nr\n\ngiven initial values of (\u03c0r , \u00b5r , \u03c3 2\n, the em algorithm simply involves computing\nr ) \u2261 \u03b8\u2020\nthe weights wr (y j ; \u03b8(cid:6)\n,\nand checking convergence using the log likelihood, |\u03b8\u2020 \u2212 \u03b8(cid:6)|, orboth. if convergence\nis not yet attained, \u03b8(cid:6)\n\n) for these initial values, updating to obtain (\u03c0\nis replaced by \u03b8\u2020\n\nand the cycle repeated.\n\nr , \u03c3 2\u2020\n\u2020\n\n\u2020\nr , \u00b5\n\nwe illustrate these calculations using the data in table 5.10, which gives the ve-\nlocities at which 82 galaxies in the corona borealis region are moving away from\nour own galaxy. it is thought that after the big bang the universe expanded very fast,\nand that as it did so galaxies formed because of the local attraction of matter. owing\nto the action of gravity they tend to cluster together, but there seem also to be \u2018su-\nperclusters\u2019 of galaxies surrounded by voids. if galaxies are indeed super-clustered\nthe distribution of their velocities estimated from the red-shift in their light-spectra\nwould be multimodal, and unimodal otherwise. the data given are from sections of\nthe northern sky carefully sampled to settle whether there are superclusters.\n\ncursory examination of the data strongly suggests clustering. in order to estimate\nthe number of clusters we fit mixtures of normal densities by the em algorithm with\ninitial values chosen by eye. the maximized log likelihood for p = 2 is\u2212220.19,\nfound after 26 iterations. in fact this is the highest of several local maxima; the global\nmaximum of +\u221e is found by centering one component of the mixture at any of the\n\u2192 \u221e; see example 4.42. only the local maxima\ny j and letting the corresponding \u03c3 2\nr\nyield sensible fits, the best of which is found using randomly chosen initial values. the\nnumber of iterations needed depends on these and on the number of components, but\nis typically less than 40. this procedure gives maximized log likelihoods \u2212240.42,\n\u2212203.48, \u2212202.52 and \u2212192.42 for fits with p = 1, 3, 4 and 5. the latter gives a\nsingle component to the two observations around 16,000 and so does not seem very\n\n "}, {"Page_number": 227, "text": "5.5 \u00b7 missing data\n\n215\n\nfigure 5.15 fit of a\n4-component mixture of\nnormal densities to the\ndata in table 5.10 (103\nkm/second). individual\n\ncomponents(cid:8)\u03c0r fr (y;(cid:8)\u03b8)\n\nare shown by dotted lines.\n\n0\n2\n\n.\n\n0\n\n5\n1\n\n.\n\n0\n\nf\nd\np\n\n0\n1\n\n.\n\n0\n\n5\n0\n\n.\n\n0\n\n0\n\n.\n\n0\n\n0\n\n10\n\n20\n\nvelocity\n\n30\n\n40\n\nsensible. standard likelihood asymptotics do not apply here, but evidently there is\nlittle difference between the 3- and 4-component fits, the second of which is shown\nin figure 5.15. both fits have three modes, and the evidence for clustering is very\nstrong.\n\nan alternative is to apply a newton\u2013raphson algorithm directly to the log likeli-\nhood (cid:6)(\u03b8) based on the mixture density, but if this is to be reliable the model must\nbe reparametrized so that the parameter space is unconstrained, using log \u03c3 2\nr and\nexpressing \u03c01, . . . , \u03c0 p in terms of \u03b81, . . . , \u03b8 p\u22121 of example 5.12. as mentioned in\nexample 4.42, the effect of the spikes in (cid:6)(\u03b8) can be reduced by replacing fr (y; \u03b8)\nby fr (y + h; \u03b8) \u2212 fr (y \u2212 h; \u03b8), where h is the degree of rounding of the data, here\n(cid:1)\n50 km/second.\n\nexponential family models\nthe em algorithm has a particularly simple form when the complete-data log likeli-\nhood stems from an exponential family, giving\n\nlog f (y, u; \u03b8) = s(y, u)t\u03b8 \u2212 \u03ba(\u03b8) + c(y, u).\n\nthe expected value of this is needed with respect to the conditional density f (u |\ny; \u03b8(cid:6)\n). evidently the final term will not depend on \u03b8 and can be ignored, so the m-step\nwill involve maximizing\n\nq(\u03b8; \u03b8(cid:6)\n\n) = e{s(y, u )t\u03b8 | y = y; \u03b8(cid:6)} \u2212\u03ba (\u03b8),\n\nor equivalently solving for \u03b8 the equation\n\ne{s(y, u ) | y = y; \u03b8(cid:6)} = d\u03ba(\u03b8)\nd\u03b8\n\n.\n\nthe likelihood equation for \u03b8 based on the complete data would be s(y, u) =\nd\u03ba(\u03b8)/d\u03b8, sothe em algorithm simply involves replacing s(y, u) byits conditional\nexpectation e{s(y, u ) | y = y; \u03b8(cid:6)} and solving the likelihood equation. thus a rou-\ntine to fit the complete-data model can readily be adapted for missing data if the\nconditional expectations are available.\n\n "}, {"Page_number": 228, "text": "216\n\n5 \u00b7 models\n\nexample 5.37 (positron emission tomography) positron emission tomography\nis performed by introducing a radioactive tracer into an animal or human subject.\nradioactive emissions are then used to assess levels of metabolic activity and blood\nflow in organs of interest. positrons emitted by the tracer annihilate with nearby\nelectrons, giving pairs of photons that fly off in opposite directions. some of these\nare counted by bands of gamma detectors placed around the subject\u2019s body, but\nothers miss the detectors. the detected counts are used to form an image of the level\nof metabolic activity in the organs based on the estimated spatial concentration of\nisotope.\n\nfor astatistical model, the region of interest is divided into n pixels or voxels and pixels and voxels are\npicture and volume\nelements, in 2 and\n3 dimensions respectively.\n\nit is assumed that the number of emissions ui j from the jth pixel detected at the ith\ndetector is a poisson variable with mean pi j \u03bb j ; here \u03bb j is the intensity of emissions\nfrom that pixel and pi j the probability that a single emission is detected at the ith\ndetector. the pi j depend on the geometry of the detection system, the isotope and\nother factors, but can be taken to be known. the ui j are unknown but can plausibly\nbe assumed independent. the counts yi at the d detectors are observed and have\nindependent poisson distributions with means\n\n(cid:1)\n\nn\nj=1 pi j \u03bb j .\n\nthe complete-data log likelihood,\n\nd(cid:6)\ni=1\n\nn(cid:6)\nj=1\n\n{ui j log( pi j \u03bb j ) \u2212 pi j \u03bb j},\n\n(cid:1)\n\u03bb j have the simple form(cid:8)\u03bb j = (cid:1)\nis an exponential family in which the maximum likelihood estimates of the unknown\ni pi j . the e-step requires only the condi-\ni ui j /\ntional expectations e(ui j | y ; \u03bb(cid:6)\n). as yi = ui1 + \u00b7\u00b7\u00b7 + uin, the conditional density of\nui j given yi = yi is binomial with denominator yi and probability pi j \u03bb(cid:6)\nh pih\u03bb(cid:6)\nh.\nthus the m-step yields\n\n(cid:1)\n\n/\n\nj\n\n(cid:1)\ni=1 e(ui j | y j = y j ; \u03bb(cid:6)\n\nd\n\n(cid:1)\nd\ni=1 pi j\n\n)\n\n=\n\n=\n\n\u2020\n\u03bb\nj\n\nd\n\n/\n\n(cid:1)\n\n(cid:1)\ni=1 y j pi j \u03bb(cid:6)\n(cid:1)\nj\nd\nd(cid:6)\ni=1 pi j\ni=1\n\n1(cid:1)\nd\ni=1 pi j\n\nj\n\nn\n\nh\n\nh=1 pih\u03bb(cid:6)\nyi pi j(cid:1)\n\u03bb(cid:6)\nn\nh=1\nh pih\n\n= \u03bb(cid:6)\n\n, j = 1, . . . ,n .\n\nthe algorithm converges to a unique global maximum of the observed-data log like-\nlihood provided that d > n, with the positivity constraints on the \u03bb j satisfied at each\nstep.\n\nthough simple, this algorithm has the undesirable property that the resulting images\nare too rough if it is iterated to full convergence. the difficulty is that although we\nwould anticipate that adjacent pixels would be similar, the model places no constraint\non the \u03bb j and so the final image is too close to the data. some modification is required,\nsuch as adding a smoothing step to the algorithm or introducing a roughness penalty\n(cid:1)\n(section 10.7.2).\n\nthe em algorithm is particularly attractive in exponential family problems, but\nis used much more widely. in more general situations both e- and m-steps may\n\n "}, {"Page_number": 229, "text": "5.5 \u00b7 missing data\n\n217\n\nbe complicated, and it often pays to break them into smaller components, perhaps\ninvolving monte carlo simulation to compute the conditional expectations required\nfor the e-step. discussion of this here would take us too far afield, but some of the\nrecent research devoted to this is mentioned in the bibliographic notes.\n\nexercises 5.5\n1\n\n2\n\ndata are observed at random if pr(i = 0 | x, y) = pr(i = 0 | y), where i is the indicator\nthat y is missing. show that if data are observed at random and missing data are missing\nat random, then data are missing completely at random.\nshow that bayesian inference for \u03b8 is unaffected by the model for non-response if data\nare missing completely at random or missing at random, but not if there is non-ignorable\nnon-response. what happens when pr(i | x, y) depends on \u03b8?\nin example 5.33, suppose that y is normal with mean \u03b20 + \u03b21x and variance \u03c3 2, and that\nit is missing with probability \u0001(a + by + cx), where a, b and c are unknown. use (3.25)\nto find the likelihood contributions from pairs (x, y) and (x, ?), and discuss whether the\nparameters are estimable.\n4 when \u03c1 = 0, show that (5.35) is the maximum likelihood estimate of \u00b5 and find its\n\n3\n\nf (u | y; \u03b8) du = 1 for all y and \u03b8 to show that\n\n(cid:17)\n\nvariance.\nuse the fact that\n0 = e\n0 = e\n\n(cid:14)\n(cid:14)\n\n\u2202 log f (u | y ; \u03b8)\n\u2202 2 log f (u | y ; \u03b8)\n\n\u2202\u03b8\n\n\u2202\u03b8 \u2202\u03b8 t\n\n(cid:16)\n\n(cid:22)(cid:22)(cid:22)(cid:22) y = y; \u03b8\n+ \u2202 log f (u | y ; \u03b8)\n\n,\n\n\u2202\u03b8\n\n\u2202 log f (u | y ; \u03b8)\n\n\u2202\u03b8 t\n\n(cid:16)\n\n(cid:22)(cid:22)(cid:22)(cid:22) y = y; \u03b8\n\n.\n\nnow use (5.38) to establish (5.40).\ncheck this in the special case of example 5.35, and hence give the newton\u2013raphson step\nfor maximization of the observed-data log likelihood, even though (cid:6)(\u03b8) itself is unknown.\nwrite a program to compare the convergence of the em and newton\u2013raphson algorithms\nin that example.\n(oakes, 1999)\n(cid:1)\nr and \u03c3 2\u2020\ncheck the forms of \u03c0\u2020\n> 0, 0 \u2264 \u03c0r \u2264 1 and\nconstraints \u03c3 2\nr\ncheck the details of example 5.37.\n(a) to apply the em algorithm to data censored at a constant c, let u denote the underlying\nfailure time and suppose that y = min(u, c) and d = i (u \u2264 c) are observed. thus the\ncomplete-data log likelihood is log f (u; \u03b8). show that\n\nin example 5.36, and verify that they respect the\n\u03c0r = 1 onthe parameter values.\n\nr , \u00b5\u2020\n\nr\n\n(cid:14)\n\n\u03b4(u \u2212 y), d = 1,\nf (u;\u03b8)\n1\u2212f(c;\u03b8)\n\nf (u | y, d; \u03b8) =\n\u2212\u03b8u, show that e(u | y = y, d = d; \u03b8(cid:6)\n\n,\n\nu > c, d = 0.\n\n(b) if f (u; \u03b8) = \u03b8e\nand deduce that the iteration for a random sample (y1, d1), . . . ,( yn, dn) is\n\n) = dy + (1 \u2212 d)(c + 1/\u03b8(cid:6)\n(cid:10) .\n\n),\n\n\u03b8\u2020 =\n\n(cid:1)\n\n(cid:9)\n\nn\nj=1\n\nn\n\nd j y j + (1 \u2212 d j )(c + 1/\u03b8(cid:6))\n(cid:1)\n\nshow that the missing information is\nthe algorithm. discuss briefly.\n\n(1 \u2212 d j )/\u03b8 2 and find the rate of convergence of\n\n5\n\n6\n\n7\n8\n\n\u03b4(\u00b7) isthe dirac delta\nfunction.\n\n "}, {"Page_number": 230, "text": "218\n\n5.6 bibliographic notes\n\n5 \u00b7 models\n\nlinear regression is discussed in more depth in chapter 8, and references to the\nenormous literature on the topic can be found in section 8.8. exponential family\nmodels date to work of fisher and others in the 1930s, are widely used in applications\nand have been intensively studied. chapter 5 of pace and salvan (1997) is a good\nreference, while longer more mathematical accounts are barndorff-nielsen (1978)\nand brown (1986). the term natural exponential family was introduced by morris\n(1982, 1983), who highlighted the importance of the variance function.\n\nthe roots of group transformation models go back to pitman (1938, 1939), but owe\nmuch of their modern development to d. a. s. fraser, summarized in fraser (1968,\n1979).\n\nsurvival analysis is a huge field with inter-related literatures on industrial and med-\nical problems, though time-to-event data arise in many other fields also. the early\nliterature is mostly concerned with reliability, of which crowder et al. (1991) is an\nelementary account, while the literature on biostatistical and medical applications\nhas grown enormously over the last 30 years. cox and oakes (1984), miller (1981),\nkalbfleisch and prentice (1980), and collett (1995) are standard accounts at about\nthis level; see also klein and moeschberger (1997). competing risks are surveyed by\ntsiatis (1998); a helpful earlier account is prentice et al. (1978). their nonidentifia-\nbility was first pointed out by cox (1959). aalen (1994) gives an elementary account\nof frailty models, with further references. keiding (1990) describes inference using\nthe lexis diagram.\n\nthe formal study of missing data began with rubin (1976), though ad hoc pro-\ncedures for dealing with missing observations in standard models were widely used\nmuch earlier. a standard reference is little and rubin (1987). more recently the related\nnotion of data coarsening, which encompasses censoring, truncation and grouping as\nwell as missingness, has been discussed by heitjan (1994).\n\nalthough data in areas such as epidemiology and the social and economic sci-\nences are often analyzed as if they were selected randomly from some well-defined\npopulation, the possibility that bias has entered the selection process is ever-present;\npublication bias is just one example of this. there is a large literature on selection bias\nfrom many points of view, much of which is mentioned by copas and li (1997) and\nits discussants. example 5.34 is taken from copas (1999). molenberghs et al. (2001)\ngive an example of analysis of sensitivity to missing data in contingency tables, with\nreferences to related literature.\n\nspecial cases of the em algorithm were used well before it was crystallized and\nnamed by dempster et al. (1977), who gave numerous applications and pointed the\nway for the substantial further work largely summarized in mclachlan and krishnan\n(1997). a useful shorter account is chapter 4 of tanner (1996). one common criticism\nof the algorithm is its slowness, and meng and van dyk (1997) and jamshidian\nand jennrich (1997) describe some of the many approaches to speeding it up; they\nalso contain further references. oakes (1999) gives references to the literature on\ncomputing standard errors for em estimates. modern applications go far beyond the\n\n "}, {"Page_number": 231, "text": "5.7 \u00b7 problems\n\n219\n\nsimple exponential family models used initially and may require complex e- and\nm-steps including monte carlo simulation; see for example mcculloch (1997).\n\nmixture models and their generalizations are widely used in applications, partic-\nularly for classification and discrimination problems; see titterington et al. (1985)\nand lindsay (1995). the thorny problem of selecting the number of components is\ngiven an airing by richardson and green (1997) and their discussants, using methods\ndiscussed in section 11.3.3.\n\n5.7 problems\n1 in the linear model (5.3), suppose that n = 2r is an even integer and define w j = yn\u2212 j+1 \u2212\n\ny j for j = 1, . . . , r. find the joint distribution of the w j and hence show that\n\n(cid:1)\nj=1(xn\u2212 j+1 \u2212 x j )w j\n(cid:1)\nj=1(xn\u2212 j+1 \u2212 x j )2\n\nr\n\nr\n\n=\n\nsatisfies e( \u02dc\u03b3\n\n\u02dc\u03b3\n1\n1) = \u03b31. show that\n(cid:11)\nn(cid:6)\nj=1\n\nvar( \u02dc\u03b3\n\ndeduce that var( \u02dc\u03b3\nall j = 1 . . . , r.\n\n(x j \u2212 x)2 \u2212 1\n2\n\n1) = \u03c3 2\n1) \u2265 var((cid:8)\u03b31) with equality if and only if xn\u2212 j+1 + x j = c for some c and\n\n(xn\u2212 j+1 + x j \u2212 2x)2\n\n.\n\nr(cid:6)\nj=1\n\n(cid:12)\u22121\n\n2 show that the scaled chi-squared density with known degrees of freedom \u03bd,\n\nf (v; \u03c3 2) =\n\n(cid:24)\nv \u03bd/2\u22121\n(2\u03c3 2)\u03bd/2\u0001\n\n\u03bd\n\n1\n2\n\n(cid:27)\n\n(cid:25) exp\n\n(cid:28)\n\n\u2212 v\n2\u03c3 2\n\nv > 0, \u03c3 2 > 0, \u03bd = 1, 2, . . . ,\n\n,\n\nis an exponential family, and find its canonical parameter and observation and cumulant-\ngenerating function.\n\n3 show that the geometric density\n\nf (y; \u03c0) = \u03c0(1 \u2212 \u03c0)y ,\n\ny = 0, 1, . . . , 0 < \u03c0 <1,\nis an exponential family, and give its cumulant-generating function.\nshow that s = y1 + \u00b7\u00b7\u00b7 + yn has negative binomial density\n\n(cid:19)\n\n(cid:18)\n\nn + s \u2212 1\nn \u2212 1\n\n\u03c0 n(1 \u2212 \u03c0)s ,\n\ns = 0, 1, . . . ,\n\nand that this is also an exponential family.\nshow that the conditional density of y1 given y1 + y2 = s is\n\n4 (a) suppose that y1 and y2 have gamma densities (2.7) with parameters \u03bb, \u03ba1 and \u03bb, \u03ba2.\n\n\u0001(\u03ba1 + \u03ba2)\n\ns \u03ba1+\u03ba2\u22121\u0001(\u03ba1)\u0001(\u03ba2)\n\nu\u03ba1\u22121(s \u2212 u)\u03ba2\u22121,\n\n0 < u < s, \u03ba1, \u03ba2 > 0,\n\nand establish that this is an exponential family. give its mean and variance.\n(b) show that y1/(y1 + y2) has the beta density.\n(c) discuss how you would use samples of form y1/(y1 + y2) tocheck the fit of this model\nwith known \u03bd1 and \u03bd2.\n5 if y has density (5.7) and y1 is a proper subset of y, show the the conditional density of\ny given that y (cid:13)\u2208 y1 is also a natural exponential family.\nfind the cumulant-generating function for the truncated poisson density given by f0(y) \u221d\n1/y!, y = 1, 2, . . ., and give the likelihood equation and information quantities.\ncompare with practical 4.3.\n\n "}, {"Page_number": 232, "text": "220\n\n5 \u00b7 models\n\n6 show that the two-locus multinomial model in example 4.38 is a natural exponen-\ntial family of order 2 with natural observation and parameter s(y ) = (ya + yab , yb +\nyab)t and (\u03b8a, \u03b8b)t = (log{\u03b1/(1 \u2212 \u03b1)}, log{\u03b2/(1 \u2212 \u03b2)}) and cumulant-generating func-\ntion m log(1 + e\u03b8a ) + m log(1 + e\u03b8b ). deduce that the elements of s(y ) are independent.\nunder what circumstances will maximum likelihood estimation of \u03b8a, \u03b8b give infinite\nestimates?\n\n7 suppose that y1, . . . ,y n follow (5.2). show that the joint density of the y j is a linear\nexponential family of order three, and give the canonical statistics and parameters and the\ncumulant-generating function. find the minimal representations in the cases where the x j\n(i) are, and (ii) are not, all equal.\nis the model an exponential family when e(y j ) = \u03b20 exp(x j \u03b21)?\n8 show that the multivariate normal distribution n p(\u00b5, \u0001) is agroup transformation model\nunder the map y (cid:18)\u2192 a + by , where a is a p \u00d7 1 vector and b an invertible p \u00d7 p matrix.\ngiven a random sample y1, . . . ,y n from this distribution, show that\n\ny = n\n\n\u22121\n\ny j ,\n\n(y j \u2212 y )(y j \u2212 y )t\n\nn(cid:6)\nj=1\n\nn(cid:6)\nj=1\n\n10\n\n9 show that the model in example 4.5 is an exponential family. is it steep? what happens\n\nis a minimal sufficient statistic for \u00b5 and \u0001, and give equivariant estimators of them. use\nthese estimators to find the maximal invariant.\nwhen r j = 0 whenever x j < a and r j = m j otherwise?\nfind its minimal representation when all the x j are equal.\nindependent observations y1, . . . , yn from the exponential density \u03bb exp(\u2212\u03bby), y > 0,\n\u03bb >0, are subject to type ii censoring stopping at the rth failure. show that a minimal\nsufficient statistic for \u03bb is s = y(1) + \u00b7\u00b7\u00b7 + y(r) + (n \u2212 r)y(r), where 0 < y(1) < y(2) < \u00b7\u00b7\u00b7\nare order statistics of the y j , and that 2\u03bbs has a chi-squared distribution on 2r degrees of\nfreedom.\na type ii censored sample was 0.2, 0.8, 1.1, 1.4, 2.1, 2.4, 2.4+, 2.4+, 2.4+, where + denotes\ncensoring. on the assumption that the sample is from the exponential distribution, find a\n90% confidence interval for \u03bb. how would you check whether the data are exponential?\n11 let x1, . . . , xn be an exponential random sample with density \u03bb exp(\u2212\u03bbx), x > 0, \u03bb >0.\nfor simplicity suppose that n = mr. let y1 be the total time at risk from time zero to the\nrth failure, y2 be the total time at risk between the rth and the 2rth failure, y3 the total\ntime at risk between the 2rth and 3rth failures, and so forth.\n(a) let x(1) \u2264 x(2) \u2264 \u00b7\u00b7\u00b7 \u2264 x(n) be the ordered values of the x j . show that the joint\ndensity of the order statistics is\n\nf x(1),...,x(n)(x1, . . . , xn) = n! f (x1) f (x2)\u00b7\u00b7\u00b7 f (xn),\n\nx1 < x2 < \u00b7\u00b7\u00b7 < xn,\n\nand by writing x(1) = z1, x(2) = z1 + z2, . . ., x(n) = z1 + \u00b7\u00b7\u00b7 + zn, where the z j are\nthe spacings between the order statistics x( j), show that the z j are independent exponential\nrandom variables with hazard rates (n + 1 \u2212 j)\u03bb.\n(b) hence show that the y j have independent gamma distributions with means r/\u03bb and\nvariances r/\u03bb2. deduce that the variables log y j are independently distributed with con-\nstant variance.\n(c) now suppose that the hazard rate is not constant, but is a slowly-varying smooth\nfunction of time, \u03bb(t). explain how a plot of log y j against the midpoint of the time\ninterval between the (r \u2212 1) jth and the r jth failures can be used to estimate log \u03bb(t).\n(cox, 1979)\n\n12 let y1, . . . ,y n be independent exponential variables with hazard \u03bb subject to type i\ncensoring at time c. show that the observed information for \u03bb is d/\u03bb2, where d is\nthe number of the y j that are uncensored, and deduce that the expected information is\ni(\u03bb | c) = n{1 \u2212 exp(\u2212\u03bbc)}/\u03bb2 conditional on c.\n\n "}, {"Page_number": 233, "text": "5.7 \u00b7 problems\n\n221\n\nnow suppose that the censoring time c is a realization of a random variable c, whose\ndensity is gamma with index \u03bd and parameter \u03bb\u03b1:\nexp(\u2212c\u03bb\u03b1),\n\nf (c) = (\u03bb\u03b1)\u03bdc\u03bd\u22121\n\nc > 0, \u03b1, \u03bd > 0.\n\n\u0001(\u03bd)\n\nshow that the expected information for \u03bb after averaging over c is\n\ni(\u03bb) = n{1 \u2212 (1 + 1/\u03b1)\n\n\u2212\u03bd}/\u03bb2.\n\nconsider what happens when (i) \u03b1 \u2192 0, (ii) \u03b1 \u2192 \u221e, (iii) \u03b1 = 1, \u03bd = 1, (iv) \u03bd \u2192 \u221e but\n\u00b5 = \u03bd/\u03b1 is held fixed. in each case explain qualitatively the behaviour of i(\u03bb).\n\n13 in a competing risks model with k = 2, write\n\npr(y \u2264 y) = pr(y \u2264 y | i = 1)pr(i = 1) + pr(y \u2264 y | i = 2)pr(i = 2)\n\n= pf1(y) + (1 \u2212 p)f2(y),\n\nsay. hence find the cause-specific hazard functions h1 and h2, and express f1, f2 and p\nin terms of them.\nshow that the likelihood for an uncensored sample may be written\n\npr (1 \u2212 p)n\u2212r\n\nf1(y j )\n\nf2(y j )\n\nr(cid:2)\nj=1\n\nn(cid:2)\nj=r+1\n\nand find the likelihood when there is censoring.\nif f( y1 | y2) and f (y2 | y1) bearbitrary densities with support [ y2,\u221e) and [y1,\u221e), then\nshow that the joint density\n\n(cid:13)\n\nf (y1, y2) =\n\np f1(y1) f (y2 | y1),\n(1 \u2212 p) f2(y2) f (y1 | y2),\n\ny1 \u2264 y2,\ny1 > y2,\n\nproduces the same likelihoods. deduce that the joint density is not identifiable.\n\n14 find the cause-specific hazard functions for the bivariate survivor functions\n\nf(y1, y2) = exp[1 \u2212 \u03b81 y1 \u2212 \u03b82 y2 \u2212 exp{\u03b2(\u03b81 y1 + \u03b82 y2)}],\n(y1, y2) = exp\nf\u2217\n\n1 \u2212 \u03b81 y1 \u2212 \u03b82 y2 \u2212 2(cid:6)\n\n\u03b8i\n\n(cid:5)\n\nexp{\u03b2(\u03b81 + \u03b82)yi}\n\n(cid:7)\n\n,\n\n\u03b81 + \u03b82\n\ni=1\n\nwhere y1, y2 > 0, \u03b81, \u03b82 > 0 and \u03b2 > \u22121. under what condition doesf yield independent\nvariables?\nthese two models. discuss the interpretation of(cid:8)\u03b2 (cid:19) 0 inthe absence of external evidence\nwrite down the likelihoods based on random samples (y1, i1, d1), . . . ,( yn, in, dn) from\nfor f over f\u2217\n(prentice et al., 1978)\n\n.\n\n15 (a) let z = x1 + \u00b7\u00b7\u00b7 + x n , where n is poisson with mean \u00b5 and the xi are independent\nidentically distributed variables with moment-generating function m(t). show that the\ncumulant-generating function of z is k z (t) = \u00b5{m(t) \u2212 1} and that pr(z = 0) = e\n\u2212\u00b5.\nif the xi are gamma variables, show that k z (t) may be written as\n\nz is a continuous variable\nfor 0 < \u03b1 <1, but you\nneed not show this.\n\n\u03b1\n\n[{1 \u2212 \u03b1t /(\u03b3 \u03b4)}1\u2212\u03b1 \u2212 1],\n\n(\u03b1 \u2212 1)\u03b4\n\n(5.43)\nwhere \u03b1 > 1, show that e(z) = \u03b3 and var(z)/e(z)2 = \u03b4, and find pr(z = 0) in terms of\n\u03b1, \u03b4 and \u03b3 . show that as \u03b1 \u2192 1 the limiting distribution of z is gamma, and explain why.\n(b) for a frailty model, set \u03b3 = 1 and suppose that an individual has hazard zh(y), y > 0.\ncompute the population cumulative hazard hy (y) and show that if \u03b1 > 1 then\n\n\u03b3 , \u03b4 > 0,\n\ny\u2192\u221e hy (y) < \u221e.\n\nlim\n\n "}, {"Page_number": 234, "text": "222\n\n5 \u00b7 models\n\ngive an interpretation of this in terms of the distribution of the lifetime y . (are all the\nindividuals in the population liable to fail?)\n(c) obtain the population hazard rate hy (y), take h(y) = y2, and graph hy (y) for \u03b4 =\n0, 0.5, 1, 2.5. discuss this in relation to the divorce rate example on page 201.\n(d) now suppose that there are two groups of individuals, the first with individual hazards\nh(y) and the second with individual hazards rh(y), where r > 1. thus the effect of trans-\nferring an individual from group 1 to group 2, if this were possible, would be to increase\nhis hazard by a factor r. iffrailties in the two groups have the same cumulant-generating\nfunction (5.43), show that the ratio of group hazard functions is\n\n(cid:14)\n\n= r\n\nh2(y)\nh1(y)\n\n1 + \u03b1\u22121\u03b4 h(y)\n1 + r \u03b1\u22121\u03b4 h(y)\n\n(cid:16)\u03b1\n\n.\n\nestablish that this is a decreasing function of y, and explain why its limiting value is less\nthan one, that is, the risk is eventually lower in group 2, if \u03b1 > 1. what difficulties does\nthis pose for the interpretation of group differences in survival?\n(aalen, 1994; hougaard, 1984)\n(a) show that when data (x, y ) are available, but with values of y missing at random, the\nlog likelihood contribution can be written\n\n16\n\n(cid:6)(\u03b8) \u2261 i log f (y | x; \u03b8) + log f (x; \u03b8),\n\nand deduce that the expected information for \u03b8 depends on the missingness mechanism\nbut that the observed information does not.\n(b) consider binary pairs (x, y ) with indicator i equal to zero when y is missing; x is\nalways seen. their joint distribution is given by\n\npr(y = 1 | x = 0) = \u03b80,\n\npr(y = 1 | x = 1) = \u03b81,\n\npr(x = 1) = \u03bb,\n\nwhile the missingness mechanism is\n\n(i) show that the likelihood contribution from (x, y, i ) is\n\npr(i = 1 | x = 0) = \u03b70,\n\n%(cid:9)\n(cid:10)x (cid:9)\n1 (1 \u2212 \u03b81)1\u2212y\n0 (1 \u2212 \u03b80)1\u2212y\n\u00d7(cid:9)\n(cid:10)1\u2212x (cid:9)\n0(1 \u2212 \u03b70)1\u2212i\n\n\u03b8 y\n\n\u03b8 y\n\n\u03b7i\n\npr(i = 1 | x = 1) = \u03b71.\n(cid:10)1\u2212x\n1(1 \u2212 \u03b71)1\u2212i\n(cid:14)\n= n(cid:6)\n\n&i\n(cid:10)x \u00d7 \u03bbx (1 \u2212 \u03bb)1\u2212x .\n\n+ 1 \u2212 y j\n(1 \u2212 \u03b81)2\n\ni j x j\n\n(cid:16)\n\n\u03b7i\n\ny j\n\u03b8 2\n1\n\n.\n\nj=1\n\n\u2212 \u2202 2(cid:6)(\u03b80, \u03b81)\n\n\u2202\u03b8 2\n1\n\ndeduce that the observed information for \u03b81 based on a random sample of size n is\n\n0 and \u2202 2(cid:6)(\u03b80, \u03b81)/\u2202\u03b80\u2202\u03b81.\n\ngive corresponding expressions for \u2202 2(cid:6)(\u03b80, \u03b81)/\u2202\u03b8 2\n\u03b81)}, where m = (cid:1)\n(ii) statistician a calculates the expected information treating i1, . . . , in as fixed and\nthereby ignores the missing data mechanism. show that he gets i a(\u03b81, \u03b81) = m\u03bb/{\u03b81(1 \u2212\ni j , and find the corresponding quantities i a(\u03b80, \u03b81) and i a(\u03b80, \u03b80). if\nhe uses this procedure for many sets of data, deduce that on average m is replaced by\nnpr(i = 1) = n{\u03bb\u03b71 + (1 \u2212 \u03bb)\u03b70}.\n(iii) statistician b calculates the expected information taking into account the missingness\nmechanism. show that she gets i b(\u03b81, \u03b81) = n\u03bb\u03b71/{\u03b81(1 \u2212 \u03b81)}, and obtain i b(\u03b80, \u03b81) and\ni b(\u03b80, \u03b80).\n(iv) show that a and b get the same expected information matrices only if y is missing\ncompletely at random. does this accord with the discussion above?\n(c) statistician c argues that expected information should never be used in data analysis:\neven if the data actually observed are complete, unless it can be guaranteed that data\n\n "}, {"Page_number": 235, "text": "5.7 \u00b7 problems\n\n223\n\ncould not be missing at random for any reason, every expected information calculation\nshould involve every potential missingness mechanism. such a guarantee is impossible\nin practice, so no expected information calculation is ever correct. do you agree?\n(kenward and molenberghs, 1998)\n17 (a) in example 5.34, suppose that n patients are divided randomly into control and treat-\nment groups of equal sizes nc = nt = n/2, with death rates \u03bbc and \u03bbt . ifthe numbers\nthe difference in log rates is roughly(cid:8)\u00b5 = log rc \u2212 log rt . what would you conclude if\nof deaths rc and rt are small, use a poisson approximation to the binomial to show that\n(cid:8)\u00b5\n.= 0?\n.= 4/(n\u03bb), and use the estimates(cid:8)\u03bbc = rc /nc ,\n(cid:8)\u03bbt = rt /nt and(cid:8)\u03bb = (rc + rt )/(nc + nt ) tocheck a few values of (cid:8)\u00b5 and the standard\n(b) show that if \u03bbc\n\n.= \u03bbt = \u03bb, then var((cid:8)\u00b5)\n\nerrors in table 5.9.\n(c) in practice the variance in (b) is typically too small, because it does not allow for\ninter-trial variability. different studies are performed with different populations, in which\nthe treatment may have different effects. we can imagine two stages: we first choose a\npopulation in which the treatment effect is \u00b5 + \u03b7, where \u03b7 is random with mean zero\nand variance \u03c3 2; then we perform a trial with n subjects and produce an estimator (cid:8)\u00b5 of\n\u00b5 + \u03b7 with variance v/n. show that(cid:8)\u00b5 may be written \u00b5 + \u03b7 + \u03b5, give the variance of \u03b5,\nand deduce that when both stages of the trial are taken into account, (cid:8)\u00b5 has mean \u00b5 and\nvariance \u03c3 2 + v/n.\nhow would this affect the calculations in example 5.34?\nconditional density of y given u = u is n (\u00b5, \u03bd\u03c3 2/u) and that u \u223c \u03c7 2\nu d= v /{\u03bd + (y \u2212 \u00b5)2/\u03c3 2} conditional on y , where v \u223c \u03c7 2\nduce that\n\n18 (a) show that the t density of example 4.39 may be obtained by supposing that the\n\u03bd . show that\n\u03bd+1, and with \u03b8 = (\u00b5, \u03c3 2) de-\n\ne(u | y ; \u03b8) =\n\n\u03bd + 1\n\n\u03bd + (y \u2212 \u00b5)2/\u03c3 2\n\n.\n\n(b) consider the em algorithm for estimation of \u03b8 when \u03bd is known. show that the\ncomplete-data log likelihood contribution from (y, u) may be written\n\n\u2212 1\n2\n\n\u03c3 2 \u2212 1\n2\n\nu(y \u2212 \u00b5)2/2(\u03bd\u03c3 2),\n\nand hence give the m-step. write down the algorithm in detail.\n(c) show that the result of the em algorithm satisfies the self-consistency relation \u03b8 = g(\u03b8),\nand given the form of g when \u03c3 2 is both known and unknown.\n(d) the cauchy log likelihood shown in the right panel of figure 4.2 corresponds to setting\n\u03bd = \u03c3 2 = 1. in this case explain why \u00b5\u2020\nconverges to a local or a global maximum or a\nlocal minimum, depending on the initial value for \u00b5.\n\n19 suppose that u1, . . . , uq have a multinomial distribution with denominator m and proba-\nhowever, so the observed data are y1, . . . ,y p, where yr = (cid:1)\nbilities \u03c01, . . . , \u03c0q that depend on a parameter \u03b8, and that the maximum likelihood estima-\ntor of \u03b8 based on the us has a simple form. some of the categories are indistinguishable,\ns\u2208ar us;a1, . . . ,a p partition\n{1, . . . , q} and none is empty.\n(a) show that the e-step of the em algorithm for estimation of \u03b8 involves\n\ne(us | y = y; \u03b8(cid:6)\n\ns(cid:1)\n) = yr \u03c0(cid:6)\nt\u2208ar\n\ns \u2208 ar ,\n\n,\n\n\u03c0(cid:6)\nt\n\nand say how the m-step is performed.\n(b) let (\u03c01, . . . , \u03c05) = (1/2, \u03b8/4, (1 \u2212 \u03b8)/4, (1 \u2212 \u03b8)/4, \u03b8/4), and suppose that y1 = u1 +\nu2 = 125, y2 = u3 = 18, y3 = u4 = 20 and y4 = u5 = 34. these data arose in a genetic\nlinkage problem and are often used to illustrate the em algorithm. show that\n\n\u03b8\u2020 = y4 + y1\u03b8(cid:6)/(2 + \u03b8(cid:6)\n)\nm \u2212 2y1/(2 + \u03b8(cid:6))\n\n,\n\n "}, {"Page_number": 236, "text": "224\n\n5 \u00b7 models\nand find the maximum likelihood estimate starting with \u03b8(cid:6) = 0.5.\n(c) show that the maximum likelihood estimator of (cid:8)\u03bba in the single-locus model of\nexample 4.38 may be written(cid:8)\u03bba = (2u1 + u2 + u5)/m and establish that\n\u2212 \u03bb(cid:6)\ngive the corresponding expressions for(cid:8)\u03bbb and e(u2 | y ; \u03bb(cid:6)\n\na).\n). hence give the m-step for\nthis model. apply the em algorithm to the data in table 4.3, using starting-values obtained\nfrom categories with probabilities 2\u03bba\u03bbb and \u03bb2\no.\n(d) compute standard errors for your estimates in (b) and (c).\n(rao, 1973, p. 369)\n\ne(u1 | y ; \u03bb(cid:6)\n\n/(2 \u2212 2\u03bb(cid:6)\n\n) = y1\u03bb(cid:6)\n\na\n\nb\n\n "}, {"Page_number": 237, "text": "6\n\nstochastic models\n\nthe previous chapter outlined likelihood analysis of some standard models. here\nwe turn to data in which the dependence among the observations is more complex.\nwe start by explaining how our earlier discussion extends to markov processes in\ndiscrete and continuous time. we then extend this to more complex indexing sets and\nin particular to markov random fields, in which basic concepts from graph theory play\nan important role. a special case is the multivariate normal distribution, an important\nmodel for data with several responses. we give some simple notions for time series, a\nvery widespread form of dependent data, and then turn to point processes, describing\nmodels for rare events in passing.\n\n6.1 markov chains\n\nin certain applications interest is focused on transitions among a small number of\nstates. a simple example is rainfall modelling, where a sequence . . .010011 . . . indi-\ncates whether or not it has rained each day. another is in panel studies of employment,\nwhere many individuals are interviewed periodically about their employment status,\nwhich might be full-time, part-time, home-worker, unemployed, retired, and so forth.\nhere interest will generally focus on how variables such as age, education, family\nevents, health, and changes in the job market affect employment history for each\ninterviewee, so that there are many short sequences of state data taken at unequal\nintervals, unlike the single long rainfall sequence. in each case, however, the key\naspect is that transitions occur amongst discrete states, even though these typically\nare crude summaries of reality.\n\nexample 6.1 (dna data) when the double helix of deoxyribonucleic acid (dna)\nis unwound it consists of two oriented linked sequences of the bases adenine (a), cy-\ntosine (c), guanine (g), and thymine (t). just one chain determines a dna sequence,\nbecause a in one sequence is always linked to t on the other, and likewise with c\nand g. an example is table 6.1, which shows the first 150 bases from a sequence of\n\n225\n\n "}, {"Page_number": 238, "text": "table 6.1 first 150\nbases of the first\nintervening sequence of\nthe human\npreproglucagon gene\n(avery and henderson,\n1999). to be read across\nrows.\n\nfigure 6.1 estimated\nproportions of bases a, c,\ng and t inthe first\nintervening sequence of\nthe human\npreproglucagon gene. at a\npoint t on the x-axis, the\nvertical distances between\nthe lines above correspond\nto the proportions of times\nthe bases appear in a\nwindow of width 100\ncentred at t.\n\n226\n\n6 \u00b7 stochastic models\n\ngtattaaatccgtagtctcgaactaacata\ntcaatatggttggaataaagcctgtgaaaa\nctatgattagtgaataaggtctcagtaatt\ntagaataaatattctgcacaatgatcaaat\ngtttaaagtatccttgtgataaaagcagac\n\nt\n\n \n,\n\ng\n\n \n,\n\nc\n\n \n,\n\na\n\n8\n\n.\n\n0\n\n4\n\n.\n\n0\n\n0\n\n.\n\n0\n\n0\n\n500\n\n1000\n\n1500\n\nposition\n\n1572 bases found in the human preproglucagon gene. figure 6.1 shows proportions\nof the different bases along the sequence, smoothed using a form of moving aver-\n100 centred at t has been counted, giving estimated proportions ((cid:1)\u03c0a,(cid:1)\u03c0c,(cid:1)\u03c0g,(cid:1)\u03c0t).\nage. roughly speaking, the number of times each base occurs in a window of width\nthese are plotted at t, and then the procedure is repeated at t + 1, and so forth. al-\nthough there is local variation, the proportions seem fairly stable along the sequence,\nwith a occurring about 30 times in every 100, c about 15 times, and so forth.\n\ncertain sequences of bases such as ctgac \u2014 known as words \u2014 are of biological\ninterest. if they occur very often in particular stretches of the entire sequence, it may\nbe supposed that they serve some purpose. but before trying to see what that purpose\nmight be, it is necessary to see if they have occurred more often then chance dictates,\nfor example by comparing the actual number of occurrences with that expected under\na model. it is simplest to suppose that bases occur randomly, but the code of life is not\nso simple. table 6.2 contains observed frequencies for pairs of consecutive bases.\nthe pair aa occurs 185 times, ac 74 times, and so forth. the lower table shows\ncorresponding proportions, obtained by dividing the frequencies by their row totals.\nabout 80% of the bases following a care a or t, while a g israre; gs occur much\n(cid:1)\nmore frequently after a, g, or t. the sequence does not seem purely random.\n\nexample 6.2 (breast cancer data) table 6.3 gives data on 37 women with breast\ncancer treated for spinal metastases at the london hospital. their ambulatory status \u2014\ndefined as ability to walk unaided or not \u2014 was recorded before treatment began, as\nit started, and then 3, 6, 12, 24, and 60 months after treatment. the three states are:\nable to walk unaided (1); unable to walk unaided (2); and dead (3). thus a sequence\n111113 means that the patient was able to walk unaided each time she was seen, but\nwas dead five years after the treatment began. she may have been unable to walk\nfor periods between the times at which her state was recorded. this is illustrated in\n\n "}, {"Page_number": 239, "text": "6.1 \u00b7 markov chains\n\n227\n\ntable 6.2 observed\nfrequencies of the 16\npossible successive pairs\nof bases in the first\nintervening sequence of\nthe human\npreproglucagon gene.\nthere are 1571 such pairs.\nthe lower table shows the\nproportion of times the\nsecond base follows the\nfirst.\n\nfrequencies for second base\n\nfirst base\n\na\n\nc\n\ng\n\nt\n\ntotal\n\na\nc\ng\nt\n\ntotal\n\n185\n101\n69\n161\n\n516\n\n74\n41\n45\n103\n\n263\n\n86\n6\n34\n100\n\n226\n\n171\n115\n78\n202\n\n566\n\n516\n263\n226\n566\n\n1571\n\nproportion for second base\n\nfirst base\n\na\n\nc\n\ng\n\nt\n\ntotal\n\na\nc\ng\nt\n\n0.359\n0.384\n0.305\n0.284\n\n0.143\n0.156\n0.199\n0.182\n\n0.167\n0.023\n0.150\n0.177\n\n0.331\n0.437\n0.345\n0.357\n\noverall\n\n0.328\n\n0.167\n\n0.144\n\n0.360\n\n1.0\n1.0\n1.0\n1.0\n\n1.0\n\ntable 6.3 breast cancer\ndata (de stavola, 1988).\nthe table gives the initial\nand follow-up status for\n37 breast cancer patients\ntreated for spinal\nmetastases. the status is\nable to walk unaided (1),\nunable to walk unaided\n(2), or dead (3), and the\ntimes of follow-up are 0,\n3, 6, 12, 24, and 60\nmonths after treatment\nbegan. woman 24 was\nalive after 6 months but\nher ability to walk was not\nrecorded.\n\ninitial\n\nfollow-up\n\ninitial\n\nfollow-up\n\ninitial\n\nfollow-up\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n1\n1\n2\n2\n1\n1\n1\n2\n1\n2\n2\n1\n\n111113\n1113\n23\n121113\n111123\n1113\n12113\n123\n1111\n23\n23\n1113\n\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n2\n2\n2\n2\n1\n2\n1\n1\n2\n2\n2\n1\n\n23\n1113\n2\n23\n1113\n223\n13\n12223\n23\n11111\n23\n12?3\n\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n1\n2\n2\n2\n2\n2\n2\n1\n2\n1\n1\n2\n2\n\n11113\n22223\n12223\n11113\n1223\n1123\n1222\n11223\n1223\n1113\n113\n23\n23\n\nthe left panel of figure 6.2, which shows a possible sample path for a woman with\nsighting history 111223. although there is a visit to state 1 between 12 and 24 months,\nit is unobserved, and the data suggest that her sojourn in state 2 is uninterrupted. the\nnumber of sightings varies from woman to woman; case 9, for example, was able to\nwalk when seen after 12 months, but her later history is unknown.\n\none aspect of interest here is whether inability to walk always precedes death, while\nanother is whether a woman\u2019s state before treatment affects her subsequent history.\nalthough no explanatory variables are available here, their effect on the transition\n(cid:1)\nprobabilities would often be of importance in practice.\n\n "}, {"Page_number": 240, "text": "228\n\n6 \u00b7 stochastic models\n\nlet xt denote a process taking values in the state space s = {1, . . . , s}, where s\nmay be infinite. for general discussion we call the quantity t on which xt depends\ntime, and suppose that our data have form x0 = s0, xt1\n= sk, where\n0 < t1 < \u00b7\u00b7\u00b7 < tk. inthe dna example t is in fact location, k = 1571, and s =\n{1, 2, 3, 4} \u2261 {a, c, g, t}. inthe breast cancer example there are s = 3 states, k = 5\nat most, and t0 = 0, t1 = 3, t2 = 6, t3 = 12, t4 = 24, and t5 = 60 months.\nlet x(t j ) = s( j) denote the composite event xt j\nj =\n0, . . . ,k \u2212 1. then the joint density of the data may be written\n(cid:2)\n(cid:2)\n\n= s j , . . . , x0 = s0, for\n\n= s1, . . . , xtk\n\n(cid:3)\n\npr\n\nxt j\n\n= s j | x(t j\u22121) = s( j\u22121)\n\n;\n\nx0 = s0, . . . , xtk\n\n= sk\n\npr\n\n(cid:3) = pr(x0 = s0)\n\nk(cid:4)\nj=1\n\nusing the prediction decomposition (4.7). the conditional probabilities may be com-\nplicated, but modelling is greatly simplified if the process has the markov property\n\n(cid:2)\n\npr\n\n= s j | x0 = s0, . . . , xt j\u22121\n\n= s j\u22121\n\nxt j\n\n(cid:2)\n\n(cid:3) = pr\n\n= s j | xt j\u22121\n\n= s j\u22121\n\nxt j\n\n(cid:3)\n\n.\n\nthus the \u2018future\u2019 xt j is independent of the \u2018past\u2019 xt j\u22122\n, . . . , x0, given the \u2018present\u2019\nxt j\u22121 \u2014 all information available about the future evolution of xt is contained in its\ncurrent state. if so, then\nx0 = s0, . . . , xtk\n\n(cid:3) = pr(x0 = s0)\n\n= s j | xt j\u22121\n\n= s j\u22121\n\n= sk\n\n(6.1)\n\nxt j\n\npr\n\npr\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n.\n\nk(cid:4)\nj=1\n\nmatters simplify further if the process is stationary, for then the conditional proba-\nbilities in (6.1) depend only on differences among the t j . thus\n\npr(xt = s | xu = r) = pr(xt\u2212u = s | x0 = r),\n\nand we assume this to be the case below. these simplications yield a rich structure\nwith many important and interesting models, in which these transition probabilities\nplay a central role. they determine the likelihood (6.1), apart from the initial term\n\nfigure 6.2 markov\nchain model for breast\ncancer data. left: possible\nsample path (solid) for a\nwoman with states 111223\nobserved at 0, 3, 6, 12, 24,\n60 months shown by the\ndotted lines. right:\nparameters for possible\ntransitions among the\nstates.\n\nandrei andreyevich\nmarkov (1856\u20131922)\nstudied with chebyshev in\nst petersburg and initially\nworked on pure\nmathematics. his study of\ndependent sequences of\nvariables stemmed from\nan attempt to understand\nthe central limit\ntheorem.\n\nsome authors use the term\nhomogeneous rather than\nstationary.\n\n "}, {"Page_number": 241, "text": "if infinite matrices worry\nyou, think of s as finite.\n\n1s is the s \u00d7 1 vector of\nones.\n\nsome authors use the\nterms persistent and\nnon-null rather than\nrecurrent and positive.\n\n6.1 \u00b7 markov chains\npr(x0 = s0). if k is large this term usually contains little information and can safely\nbe dropped, but it may be important to include it when k is small; see example 6.10.\n\n229\n\n6.1.1 markov chains\nwe call a markov model observed at discrete equally-spaced times a markov chain.\nin this section we consider inference for simple markov chain models, but in sec-\ntion 11.3.3 we describe the use of markov chains for inference. as the following\noutline of their properties serves both purposes, it is slightly more detailed than im-\nmediately required.\na stationary chain xt on the countable set s of size s observed at equally-spaced\ntimes t = 0, 1, . . . ,k has properties determined by the transition probabilities\n\nprs = pr(x1 = s | x0 = r),\n\nr, s \u2208 s,\n\n(cid:5)\n\nwhich form the s \u00d7 s transition matrix p whose (r, s) element is prs. the elements\ns prs = 1 implies that p1s = 1s, so p is\nof p are non-negative and the fact that\npr = pr(x0 = r), then the sth element of pt p is pr(x1 = s) = (cid:5)\na stochastic matrix. if the rth element of the s \u00d7 1 vector p is the initial probability\nr pr prs. iteration\nshows that the density of xn is given by pt p n, sothe ( r, s) element of p n is the\nn-step transition probability prs(n) = pr(xn = s | x0 = r). hence properties of xt\nare governed by p. the probability of a run of m \u2265 1 successive visits to state s is\npm\u22121\n\u22121 (exercise 6.1.8).\n\n(1 \u2212 pss); this is the geometric density with mean (1 \u2212 pss)\n\nss\n\nclassification of chains\nit is useful to classify the states of a chain. a state s is recurrent if\n\npr(xt = s for some t > 0 | x0 = s) = 1,\n\nmeaning that having started from s, eventual return is certain; s is transient if this\nprobability is strictly less than one. if trs = min{t > 0 : xt = s | x0 = r} is the\nfirst-passage time from r to s, then e(tss) is themean recurrence time of state s; we\nset e(tss) = \u221e if s is transient, and say that a recurrent state is positive recurrent if\ne(tss) < \u221e; otherwise it is null recurrent. the period of s is d = gcd{n : pss(n) > 0},\nthe greatest common divisor of those times at which return to s is possible; s is\naperiodic if d = 1, and periodic otherwise.\nwe now classify chains themselves. we say that r communicates with s, r \u2192 s, if\nprs(n) > 0 for some n > 0, and that r and s intercommunicate, r \u2194 s, if r \u2192 s and\ns \u2192 r. itmay be shown that two intercommunicating states have the same period,\nwhile if one is transient so is the other, and similarly for null recurrence. a set c\nof states is closed if prs = 0 for all r \u2208 c, s (cid:7)\u2208 c, and irreducible if r \u2194 s for all\nr, s \u2208 c; aclosed set with just one state is called absorbing. itmay be proved that\ns may be partitioned uniquely as t \u222a c1 \u222a c2 \u222a \u00b7\u00b7\u00b7, where t is the set of transient\nstates and the ci are irreducible closed sets of recurrent states; if s is finite, then\nat least one state is recurrent, and all recurrent states are positive. a chain is called\naperiodic, positive recurrent, and so forth if its states all share the corresponding\nproperty. an aperiodic irreducible positive recurrent chain is ergodic.\n\n "}, {"Page_number": 242, "text": "6 \u00b7 stochastic models\n230\nexample 6.3 (breast cancer data) heret = {1, 2} contains the two transient states\nwith the patient alive, while c = {3}, death, is absorbing.\n(cid:1)\nexample 6.4 (dna data) as transitions occur between every pair of states, c =\n{a, c, g, t} is an irreducible aperiodic closed set of states, all recurrent and hence\n(cid:1)\nall positive recurrent. this chain is ergodic.\n\neach of the properties of an ergodic chain is important. irreducibility means that\nany state is accessible from any other. positive recurrence implies that the chain has\nat least one stationary distribution with probability vector \u03c0 such that \u03c0 t p = \u03c0 t,\nand the mean recurrence time for state s is e(tss) = \u03c0\u22121\n< \u221e. there is a unique\nstationary distribution when the chain is both irreducible and positive recurrent. in\nthis case each state is visited infinitely often as t \u2192 \u221e, but the chain need not be\nstationary because it might oscillate among states. aperiodicity stops this.\nwhen s is infinite and the chain has all three properties, the transition probabilities\nthe initial state. moreover, if m(xt ) issuch that e \u03c0 {|m(xt )|} = (cid:5)\nprs(n) \u2192 \u03c0s as n \u2192 \u221e: the chain converges to its stationary distribution whatever\n\u03c0r|m(r)| < \u221e,\n(cid:8)\n\nthen\n\n(cid:6)\n\ns\n\nr\n\n\u03c0r m(r) asn \u2192 \u221e\n\n= 1 :\n\n(6.2)\n\n\u22121\n\nn\n\npr\n\nn(cid:7)\nt=1\n\nm(xt ) \u2192 s(cid:7)\n\nr=1\n\n\u22121\n\nstarting from any x0, the average of m(xt ) converges almost surely to the mean\n(cid:5)\ne\u03c0{m(xt )} of m(xt ) with respect to \u03c0. this ensures the convergence of so-called\nn\nt=1 m(xt ) and is crucial to the use of markov chains for\nergodic averages n\ninference. when s is finite, an irreducible aperiodic chain is automatically positive\nrecurrent and hence ergodic.\nif s is finite then p is an s \u00d7 s matrix, whose eigenvalues l1, . . . , ls are roots of\nits characteristic polynomial det(p \u2212 \u03bbis). if the lr are distinct, then\n\np = e\n\u22121l e ,\n(6.3)\nr of the s \u00d7 s matrix e is the left eigenvector\nwhere l = diag(l1, . . . , ls), the rth row et\n\u22121 is the right eigenvector of p\nof p corresponding to lr and the rth column e\ncorresponding to lr . the lr are complex numbers with modulus no greater than unity,\nbut as p is real, any complex roots of its characteristic polynomial occur in conjugate\npairs a \u00b1 ib. for some real r > 0,\n\n(cid:9)\nr of e\n\n(a \u00b1 ib)n = r n exp(\u00b1in\u03c9) = r n(cos n\u03c9 \u00b1 i sin n\u03c9).\n\nas p n is a real matrix, it may be better to express its elements in terms of sines and\ncosines when p has complex eigenvalues.\nif s is finite and the chain is irreducible with period d, then the d complex roots\nof unity l1 = exp(2\u03c0i /d), . . . , ld = exp{2\u03c0i(d \u2212 1)/d} are eigenvalues of p and\nld+1, . . . , ls satisfy |ls| < 1. if the chain is irreducible and aperiodic, then l1 = 1,\nand |ls| < 1 for s = 2, . . . , s. now\u03c0 t p = \u03c0 t and p1s = 1s, so if xt has stationary\ndistribution \u03c0, then \u03c0 t and 1s are the left and right eigenvectors of p corresponding\n\nhere i = \u221a\u22121.\n\n "}, {"Page_number": 243, "text": "6.1 \u00b7 markov chains\nto l1 = 1, that is, e1 = \u03c0 and e\ndistinct eigenvalues is obvious, because\n\n(cid:9)\n1\n\n= 1s. the convergence of an ergodic chain with\n\n231\n\np n = (e\n\n\u22121l e)n = e\n\n(cid:9)\nr et\nr\n\nln\nr e\n\n\u2192 e\n\n(cid:9)\n1et\n1\n\n= 1s\u03c0 t\n\nas n \u2192 \u221e:\n\n\u22121l n e = s(cid:7)\n\nr=1\n\nthe (r, s) element of p n, prs(n), tends to \u03c0s. moreover, if p(0) is the probability\nvector of x0 then xn has distribution p(0)t p n, which converges to p(0)t1s\u03c0 t = \u03c0 t\nwhatever the initial vector p(0).\n\nif s is infinite and the chain ergodic, its first eigenvalue l1 equals 1 and corresponds\nto the unique stationary distribution \u03c0, but the second eigenvalue l2 need not exist.\nif l2 exists and |l2| < 1, then |l2| controls the rate at which the chain approaches its\nstationary distribution. more precisely, the chain is geometrically ergodic if there\nexists a function v (\u00b7) > 1 such that\n\n(cid:7)\n\n| prs(n) \u2212 \u03c0s| \u2264 v (r)|l2|n\n\nfor all n;\n\n|l2| is then the rate of convergence of the chain.\nan irreducible chain is reversible if there exists a \u03c0 such that\n\ns\n\n\u03c0r prs = \u03c0s psr ,\n\nfor all r, s \u2208 s;\n\n(6.4)\n\n(6.5)\n\nthe chain is then positive recurrent with stationary distribution \u03c0. another way to\nexpress the detailed balance condition (6.5) is\n\npr(xt = r, xt+1 = s) = pr(xt = s, xt+1 = r),\n\nfor all r, s \u2208 s,\n\nor \u0001p = p\u0001, where \u0001 is the s \u00d7 s diagonal matrix whose elements are the com-\nponents of the stationary distribution \u03c0.\n\ndecomposition (6.3) applies to reversible chains, whose eigenvalues and eigenvec-\ntors lr and er are real. chains that fail to be geometrically ergodic have an infinite\nnumber of eigenvalues in any open interval containing one of \u00b11, but those that are\ngeometrically ergodic have all their eigenvalues but l1 uniformly bounded in modulus\nbelow unity.\n\nexample 6.5 (two-state chain) consider the chain for which\n0 \u2264 p, q \u2264 1.\n\np =\n\n,\n\n1 \u2212 p\nq\n\np\n1 \u2212 q\n\n(cid:9)\n\n(cid:10)\n\nwhen p = q = 0, there are two absorbing states c1 = {1} and c2 = {2} and the chain\nis entirely uninteresting. when both p and q are positive it is clearly irreducible and\n\u03c0 t p = \u03c0 t, where \u03c0 t = ( p + q)\n\u22121(q, p). the chain is then positive recurrent with\ne(t11) = ( p + q)/q and e(t22) = ( p + q)/ p.\nwhen p = q = 1, xt takes values . . . ,1, 2, 1, 2, 1 . . . and is periodic with period\ntwo, so t11 = t22 = 2 with probability one. if p(0) = \u03c0, then xt has this distribution\nfor all t, but if not, then the fact that p 2 = i2 implies that x0, x1, x2, . . . have dis-\ntributions p(0)t, p(0)t p, p(0)t, . . .; the chain cycles among these and never reaches\nstationarity.\n\n "}, {"Page_number": 244, "text": "6 \u00b7 stochastic models\n(cid:10)\n\n232\n\nthe eigenvalues of p are l1 = 1, l2 = 1 \u2212 p \u2212 q. its eigendecomposition is\n\n(cid:9)\n\n(cid:10)\n\n(cid:9)\n\n\u00b7\n\n1\np\n1 \u2212q\n\n0\n\n1\n0 1 \u2212 p \u2212 q\n\n(cid:10)\n\n\u00b7\n\n(cid:9)\n\n.\n\n1\np + q\n(cid:9)\n\nq\np\n1 \u22121\n(cid:10)\n\nif |l2| < 1, then 0 < p < 1 or 0 < q < 1 orboth, the chain is aperiodic and\np n = 1\np + q\n\n\u2192 1\np + q\n\nq + pln\nq \u2212 qln\n\np \u2212 pln\np + qln\n\n= 12\u03c0 t\n\n(cid:9)\n\n(cid:10)\n\nq\nq\n\np\np\n\nas\n\n2\n\n2\n\n2\n\nn \u2192 \u221e.\n\n(cid:1)\n\nexample 6.6 (five-state chain) the state space of the chain with\n\n2\n\n\uf8eb\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n1\n2\n1\n4\n0\n0\n1\n4\n\np =\n\n\uf8f6\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n1\n2\n3\n4\n00\n01\n0\n\n0\n0\n1\n0\n\n1\n4\n\n0\n0\n0\n0\n0\n\n0\n0\n\n1\n2\n\ndecomposes as c1 \u222a c2 \u222a t , where c1 = {1, 2}, c2 = {3, 4} and t = {5}. evidently\nc1 is a special case of the previous example, so it is ergodic. the set c2 is closed and\nirreducible, but it is periodic because xt = xt+2 = xt+4 = \u00b7\u00b7\u00b7. the sett is transient:\n2 , with equal probabilities of landing in c1\nat each step the probability of leaving it is 1\nand c2. although c1 is ergodic, the chain as a whole is not.\nowing to the presence of two irreducible sets, one with period two, the eigen-\nvalues include l1 = 1, l2 = 1 and l3 = \u22121. the repeated eigenvalue means that the\n\uf8eb\n\uf8f6\neigendecomposition of p is not unique. one version is\n\n\uf8f6\n\n\uf8eb\n\n\uf8f6\n\n\uf8eb\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n1\n0\n1\n1\n0\n1\n1 \u22121 \u22126\n1 \u22121\n6\n1\n0\n1\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n0 \u22121\n0\n0\n0\n1\n\n1\n2\n0\n0\n1\n\n0\n0\n\n1 0\n0 1\n0 0\u22121\n0 0\n0 0\n\n0\n0\n\n0\n0\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n0\n\n0\n\n1\n2\n0\n\n0\n1\n4\n\n0\n0\n\n1\n6\n1\n6\n0\n1\n\u2212 2\n2\n\n3\n\n1\n4\n\n1\n3\n1\n3\n\n4\n\n\u2212 1\n0 \u2212 1\n\u22121 \u2212 1\n6\n0\n\n12\n\n2\n3\n\n1\n4\n\n4\n\n1\n12\n\n\u2212 1\n\u2212 1\n3\n0\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 .\n\n0\n0\n0\n1\n0\n\nfor large n we have approximately\n\n\uf8eb\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n1\n3\n1\n3\n0\n0\n1\n6\n\np n =\n\n2\n3\n2\n3\n0\n0\n1\n3\n\n0\n0\n\n1\n2\n\n{1 + (\u22121)n}\n{1 + (\u22121)n+1}\n\n1\n2\n\n1\n2\n\n1\n3\n\n{1 + (\u22121)n+1}\n{1 + (\u22121)n}\n\n1\n2\n\n0\n0\n\n1\n6\n\n\uf8f6\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 .\n\n0\n0\n0\n0\n\u2212n\n2\n\nif x0 \u2208 c1, the stationary distribution of xt is ( 1\n, 0, 0, 0)t and the states have mean\n2 . if x0 = 3, then x2 = x4 = \u00b7\u00b7\u00b7 =3 and x1 = x3 = \u00b7\u00b7\u00b7 =\nrecurrence times 3 and 3\n4, while the converse is true if x0 = 4; xt oscillates within c2 but has a stationary\n, 0)t. if x0 = 5, the\ndistribution only if the initial probability vector is (0, 0, 1\nprobability that xn = 5 isessentially zero for large n and the process is equally likely\n2\nto end up in c1 or c2.\n(cid:1)\n\n, 1\n2\n\n, 2\n3\n\n3\n\n "}, {"Page_number": 245, "text": "6.1 \u00b7 markov chains\n\n233\n\nlikelihood inference\nwe now consider inference from data s0, s1, . . . ,s k at times 0, 1, . . . ,k from a sta-\ntionary discrete-time markov chain xt with finite state space. the likelihood is\npr (xt+1 = st+1 | xt = st )\n\npr(x0 = s0, . . . , xk = sk) = pr(x0 = s0)\n\nk\u22121(cid:4)\nt=0\nk\u22121(cid:4)\nt=0\ns(cid:4)\nr=1\n\n= pr(x0 = s0)\n\n= pr(x0 = s0)\n\npst st+1\ns(cid:4)\ns=1\n\npnrs\nrs\n\n,\n\n(6.6)\n\nwhere nrs is the observed number of transitions from r to s. apart from the first term\nin (6.6), the log likelihood is\n\n(cid:5)( p) = s(cid:7)\n\ns(cid:7)\ns=1\n\nr=1\n\nnrs log prs ,\n\n(6.7)\n(cid:5)\nso the s \u00d7 s table of transition counts nrs is a sufficient statistic; see table 6.2.\nr psr = 1 for each s, (6.7) sums log likelihood contributions from s separate\nas\nmultinomial distributions (nr1, . . . ,n r s) whose denominators nr\u00b7 equal the row sums\n(cid:5)\nnr1 + \u00b7\u00b7\u00b7 + nr s and whose probability vectors ( pr1, . . . , pr s) correspond to transi-\ns prs = 1 for each r, this model has s(s \u2212 1)\ntions out of state r; see (4.45). as\nthe results of section 4.5.3 imply that prs has maximum likelihood estimate(cid:1)prs =\nparameters.\nnrs /nr\u00b7. standard likelihood asymptotics will apply if 0 < prs < 1 for all r and s and\nif the denominators nr\u00b7 \u2192 \u221e as k \u2192 \u221e. nown r\u00b7 is the number of visits the chain\ninfinitely often as k \u2192 \u221e. the(cid:1)prs then have an approximate joint normal distribution\nmakes to state r during the period 1, . . . ,k , and if the chain is ergodic r is visited\n\nwith covariances estimated by\n\ncov((cid:1)prs ,(cid:1)ptu)\n\n.=\n\n\uf8f1\uf8f2\n\uf8f3\n\n(cid:1)prs(1 \u2212(cid:1)prs)/nr\u00b7,\n\u2212(cid:1)prs(cid:1)pru/nr\u00b7,\n\n0,\n\nr = t, s = u,\nr = t, s (cid:7)= u,\notherwise.\n\nthe above discussion ignores the first term in (6.6). if k is large it will add only\na small contribution to (cid:5)( p) and can safely be dropped, but if k is small it might be\nreplaced by the stationary probability \u03c0s0, found from the elements of p. ingeneral\nthe log likelihood must then be maximized numerically.\nan alternative asymptotic scenario is that m independent finite segments of markov\nchains having the same parameters are observed, and m \u2192 \u221e. the overall infor-\nmation in the initial terms of the segments is then o(m) and retrival of it may be\nworthwhile, particularly if the segments are short. below we continue to suppose that\nthere is a single chain of length k.\nin simpler models the prs might depend on a parameter with dimension smaller\nthan s(s \u2212 1). for instance, setting p = q in example 6.5 gives a one-parameter\n\n "}, {"Page_number": 246, "text": "234\n\n6 \u00b7 stochastic models\n\nobserved frequency\n\nexpected frequency\n\nfirst base\n\na\n\nc\n\ng\n\nt\n\na\n\nc\n\ng\n\nt\n\na\nc\ng\nt\n\n185\n101\n69\n161\n\n74\n41\n45\n103\n\n86\n6\n34\n100\n\n171\n115\n78\n202\n\n169.5\n86.4\n74.2\n185.9\n\n86.4\n44.0\n37.8\n94.8\n\n74.2\n37.8\n32.5\n81.4\n\n185.9\n94.8\n81.4\n203.9\n\ntable 6.4 fit of\nindependence model to\ndna data: observed and\nfitted frequencies of\none-step transitions.\n\nmodel. if the chain is ergodic, likelihood inference for such models will be regular\nunder the usual conditions on the parameter space.\n\nthus far transition probabilities have depended only on the current state, so\nour chains have been first-order. the simpler independence model posits transi-\ntion probabilities independent of the current state, prs \u2261 ps; this zeroth-order chain\nhas just s \u2212 1 parameters. row and column classifications in the table of counts\nn\u00b7s log ps, and (cid:1)ps = n\u00b7s /n\u00b7\u00b7, where\nn\u00b7s = n1s + \u00b7\u00b7\u00b7 +n ss and n\u00b7\u00b7 = (cid:5)\nnrs are then independent, (6.7) reduces to\ns n\u00b7s. thus the likelihood ratio statistic for com-\n(cid:9)(cid:1)prs(cid:1)ps\n\nparison of the zeroth- and first-order chains is\n\nnrsn\u00b7\u00b7\nnr\u00b7n\u00b7s\n\nw = 2\n\n(cid:7)\n\n(cid:7)\n\nnrs log\n\nnrs log\n\n= 2\n\n(cid:5)\n\n(cid:9)\n\n(cid:10)\n\n(cid:10)\n\n;\n\nr,s\n\nr,s\n\nthis is the likelihood ratio statistic for testing row-column independence in the square\ntable of counts nrs. under the zeroth-order chain the rows of p all equal ( p1, . . . , ps),\nrow and column classifications are independent, and w is a natural statistic to assess\nthis; its asymptotic distribution is chi-squared with s(s \u2212 1) \u2212 (s \u2212 1) = (s \u2212 1)2\nstatistic p = (cid:5)\ndegrees of freedom. as we saw in section 4.5.3, w approximately equals pearson\u2019s\n(o \u2212 e)2/e, where o and e denote the observed count nrs and its\nexpected counterpart nr\u00b7n\u00b7s /n\u00b7\u00b7 under the independence model and the sum is over\nthe cells of the table. the quantities (o \u2212 e)/e 1/2 squared give the contribution of\neach cell to p.\n\nexample 6.7 (dna data) the lowest line of table 6.2 gives maximum like-\nlihood estimates for the zeroth-order independence model, while the four previ-\nous lines give estimates for the first-order model. for the independence model we\n\nhave(cid:1)pa = 516/1571 = 0.328 and(cid:1)pc = 263/1571 = 0.167, for example, while un-\nder the first-order model (cid:1)paa = 185/516 = 0.359, (cid:1)pac = 74/516 = 0.143, (cid:1)pcg =\n(cid:5)\n6/263 = 0.023 and so forth. if the independence model was correct, w =\nr,s nrs log{nrs /(nr\u00b7(cid:1)ps}) would have a \u03c7 2\n2\n9 distribution, but the observed value\nw = 64.45 makes this highly implausible. the value of p is 50.3.\ntable 6.4 shows the counts nrs and the fitted values nr\u00b7n\u00b7s /n\u00b7\u00b7 under the indepen-\ndence model. the largest discrepancy is for the cg cell, for which (o \u2212 e)/e 1/2 =\n\u22125.18, so this cell contributes 26.79 to the value of p. the normal probability plot of\n\n "}, {"Page_number": 247, "text": "6.1 \u00b7 markov chains\n\nfigure 6.3 fit of zeroth-\nand first-order markov\nchains to the dna data.\nthe panels show normal\nprobability plots of the\nsigned contributions\n(o \u2212 e)/e 1/2 made by\nthe 16 cells of the\ntwo-way table under the\nindependence model (left)\nand the 64 cells of the\nthree-way table under the\nfirst-order model (right).\nthe large negative value\non the left is due to the\ncg cell. the dots show\nthe null line x = y.\n\n)\n\ne\n\n(\nt\nr\nq\ns\n/\n)\n\ne\n\n \n-\n \n\no\n\n(\n\n4\n\n2\n\n0\n\n2\n-\n\n4\n-\n\n\u2022\n\n-2\n\n-4\n\n235\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022 \u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n)\n\ne\n\n(\nt\nr\nq\ns\n/\n)\n\n-\n\ne\no\n\n(\n\n2\n\n1\n\n0\n\n1\n-\n\n2\n-\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n0\n\n2\n\n4\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\nquantiles of standard normal\n\nquantiles of standard normal\n\nthe (o \u2212 e)/e 1/2 in the left panel of figure 6.3 shows that the other cells contribute\nmuch less. the values of w and p remain large even if this cell is dropped from\nthe table, however, so it is not the sole cause of the poor fit of the independence\n(cid:1)\nmodel.\n\nhigher-order models\nfirst-order markov chains extend to chains of order m, where the probability of\ntransition into s depends on the m preceding states. one way to think of this is that\nthe state of the chain is augmented from x j to y j = (x j , x j\u22121, . . . , x j\u2212m+1) and the\ntransition probabilities change to\n\npr(y j = y j | y j\u22121 = y j\u22121) = pr(x j = s | x j\u22121 = s j\u22121, . . . , x j\u2212m = s j\u2212m)\n\n= ps j\u2212m s j\u2212m+1\u00b7\u00b7\u00b7s j\u22121s ,\n\nsay. thus the \u2018current\u2019 state y j\u22121 = (s j\u22121, . . . ,s j\u2212m) contains information not only\nfrom time j \u2212 1 but also from the m \u2212 1 previous times. whereas with m = 1 the\nproperties of the chain were determined by the s vectors of transition probabilities\n( pr1, . . . , pr s), there are now sm such vectors, so much more data is needed in order to\nget reliable estimates of the transition probabilities. a compromise is a variable-order\nchain, the simplest example of which is when m = 2 and s = 2, so that the chain\nof order two is determined by the probabilities p111, p121, p211 and p221, giving the\ntransition probabilities \u03c0sur from (s, u) to r. asimple variable-order chain is obtained\nby specifying \u03c0111 = \u03c0211, that is, given that u = 1, the transition probabilities do not\ndepend on s. this chain is first-order when u = 1, but not when u = 2. in this case\nthe number of parameters only diminishes by one, but in general the reduction might\nbe much larger.\n\nlikelihood ratio statistics or criteria such as aic enable systematic comparison of\nmarkov chains of different orders, but care is needed when computing them. suppose\nthat we fit models of orders up to m to a sequence of length k. there are k \u2212 1\nsuccessive pairs, k \u2212 2 triplets and so forth, so the fit of the mth-order model is based\n\n "}, {"Page_number": 248, "text": "236\n\n6 \u00b7 stochastic models\n\ntable 6.5 observed\ntransition counts for\nsecond-order markov\nchain for dna data.\n\nfirst base\n\nsecond base\n\na\n\nc\n\ng\n\nt\n\na\nc\ng\nt\na\nc\ng\nt\na\nc\ng\nt\na\nc\ng\nt\n\nfrequencies for third base\n\na\n\n81\n30\n29\n54\n30\n15\n2\n28\n30\n18\n12\n27\n44\n38\n26\n51\n\nc\n\n22\n7\n18\n23\n20\n2\n1\n26\n3\n10\n5\n11\n29\n22\n21\n43\n\ng\n\n29\n2\n11\n33\n15\n1\n0\n20\n14\n1\n10\n12\n28\n2\n13\n35\n\nt\n\n53\n35\n27\n61\n36\n23\n3\n41\n22\n16\n7\n27\n60\n41\n40\n73\n\ntotal\n\n185\n74\n86\n171\n101\n41\n6\n115\n69\n45\n34\n77\n160\n103\n100\n202\n\non the k \u2212 m successive (m + 1)-tuples from which the transition probabilities and\nmaximized log likelihood are computed, treating the last k \u2212 m of the k observations\nas responses. standard likelihood methods presuppose that the same responses are\nused throughout, so fits for chains of smaller order must also treat only the last k \u2212 m\nobservations as responses.\nexample 6.8 (dna data) we compare models of order up to m = 3. the preceding\ndiscussion implies that as the data in table 6.1 begin gtat. . ., the first response is\nthe second t, so the initial gta, gt and g should be ignored when fitting the\nzeroth-, first- and second-order models respectively. the frequencies for the k \u2212\nm = 1572 \u2212 3 = 1569 triplets of transition counts in our sequence are shown in\ntable 6.5. the implied numbers of ta and gt transitions, 54 + 28 + 27 + 51 = 160\nand 27 + 3 + 7 + 40 = 77, are smaller than the numbers 161 and 78 in table 6.2\nwhich include such transitions in the initial gtat.\nrow by its total, giving(cid:1)paaa = 81/185,(cid:1)paac = 22/185,(cid:1)paca = 30/74 and so forth.\nevidently estimates such as(cid:1)pcga = 2/6 are very unreliable.\nestimates under the first-order model are computed from the two-way table of\ncounts obtained by collapsing the table over the first base, giving a 4 \u00d7 4 table whose\ntop left (aa) element is 81 + 30 + 30 + 44 = 185, whose cg element is 2 + 1 +\n1 + 2 = 6 and so forth. for estimates under the independence model we use the\n1 \u00d7 4 table from a further collapse over the second base; both sets of estimates are\nessentially unaffected by dropping the first few bases.\nthe maximized log likelihoods for the zeroth-, first-, second- and third-order mod-\nels are \u22122058.44, \u22122026.02, \u22121998.41, and \u22121923.25 on 3, 12, 48, and 192 degrees\n\nestimates under the second-order model are obtained as before, by dividing each\n\n "}, {"Page_number": 249, "text": "6.1 \u00b7 markov chains\n\n237\n\n> 55.2)\n\n> 150.3)\n\nof freedom, so the aic values are 4122.9, 4076.0, 4092.8, and 4230.5 and the like-\nlihood ratio statistics for comparison of each model with the next are 64.8, 55.2,\nand 150.3, on 9, 36, and 144 degrees of freedom. there is strong evidence for\n.= 0.02\nfirst-order dependence compared to independence, while as pr(\u03c7 2\n.= 0.34 the evidence for second- compared to first-order de-\n36\nand pr(\u03c7 2\n144\npendence is weaker, and there is no suggestion of third-order dependence. the aic\nvalues clearly indicate the first-order model.\nthe signed contributions (o \u2212 e)/e 1/2 to pearson\u2019s statistic under the first-order\nis (81 \u2212 e)/e 1/2, where e = 185(cid:1)paa, with (cid:1)paa calculated under the first-order\nmodel can be obtained using table 6.5. the contribution for the aaa cell, for example,\nmodel. the value of pearson\u2019s statistic is 52.84. the right panel of figure 6.3 shows\nthe eigenvalues for the observed first-order matrix of transition probabilities(cid:1)p are\nno highly unusual cells and apparently good fit.\n1, \u22120.0147 \u00b1 0.0704i and 0.0524. the small absolute values of the last three suggest\nthat the chain is close to independence, and indeed the rows of (cid:1)p 4 are essentially\n\nequal: four steps are (almost) enough to forget the past.\nour earlier discussion suggested that the main departures from independence occur\nafter c, suggesting taking a model where prs = \u03c8s whenever r (cid:7)= c and pcs = \u03c6s.\nthat is, for each s we have\n\npr(xt+1 = s | xt = a) = pr(xt+1 = s | xt = g) = pr(xt+1 = s | xt = t),\n\n(cid:5)\n\n(cid:5)\nr(cid:7)=c nrs) log \u03c8s +(cid:5)\n\ns(\n\nbut these do not equal pcs. this model has six independent parameters and as its\nlog likelihood\ns ncs log \u03c6s is of multinomial form, their\nestimates are readily obtained. the maximized log likelihood is \u22122031.0, so aic =\n4074.0 is lower than for the full first-order chain and this model seems marginally\n(cid:1)\npreferable. see exercise 6.1.7 for further details.\n\nwe have presumed above that xt is stationary. if instead the transition prob-\n(cid:20)\nabilities are of form prs(t; \u03b8), dependent on a parameter \u03b8, then the likelihood\npr(x0 = s0; \u03b8)\nk\u22121\nt=0 pst st+1(t; \u03b8) isfound by the argument leading to (6.6). in many\ncases the initial probability pr(x0 = s0; \u03b8) may be unknown, and if the series is long\nlittle will be lost by ignoring it. if the transition probabilities do not share dependence\non a common \u03b8, they can only be estimated if they are repeated. large amounts of\ndata will then be needed.\n\n6.1.2 continuous-time models\nwe now turn to stationary continuous-time markov models with finite state space s.\nthe basic assumption is that over small intervals [t, t + \u03b4t), transitions between states\nhave probabilities\n\npr(xt+\u03b4t = s | xt = r) =\n\n\u03b3rs \u03b4t + o(\u03b4t),\n1 + \u03b3rr \u03b4t + o(\u03b4t),\n\ns (cid:7)= r,\ns = r,\n\n(6.8)\n\n(cid:21)\n\no(\u03b4t) is small enough that\no(\u03b4t)/\u03b4t \u2192 0 as\u03b4 t \u2192 0.\n\n "}, {"Page_number": 250, "text": "238\n\n6 \u00b7 stochastic models\nwhere \u03b3rs is interpreted as the rate at which transitions r \u2192 s occur. the transition\n\u03b3rs = 0,\nprobabilities do not depend on t, so xt is time homogeneous. note that\nfor each r, because the probabilities in (6.8) sum to one.\nlet p(t) denote the s \u00d7 1 vector whose rth element is pr (t) = pr(xt = r); note\ns p(t) = 1 for all t. then\nthat 1t\nps(t + \u03b4t) = s(cid:7)\n\npr(xt+\u03b4t = s | xt = r) pr (t)\n\n.= ps(t) + s(cid:7)\n\n\u03b3rs pr (t)\u03b4t + o(\u03b4t),\n\n(cid:5)\n\ns\n\nr=1\n\nr=1\n\nimplying that\n\ndps(t)\n\ndt\n\n= lim\n\u03b4t\u21920\n\nps(t + \u03b4t) \u2212 ps(t)\n\n\u03b4t\n\n= s(cid:7)\n\nr=1\n\n\u03b3rs pr (t),\n\ns = 1, . . . , s,\n\nwritten in matrix form as\n\n(cid:9)\n\ndp1(t)\n\ndt\n\n\u00b7\u00b7\u00b7\n\ndps(t)\n\ndt\n\n(cid:10)\n\n= ( p1(t)\n\n\u00b7\u00b7\u00b7\n\nps(t) )\n\n\uf8eb\n\uf8ed \u03b311\n...\n\u03b3s1\n\n\uf8f6\n\uf8f8 .\n\n\u00b7\u00b7\u00b7\n\u03b31s\n...\n...\n\u00b7\u00b7\u00b7 \u03b3ss\n\nin terms of the infinitesimal generator of the chain, the matrix g whose (r, s) element\nis \u03b3rs, wewrite\n\nto which the formal solution is\n\ndp(t)t\n\ndt\n\n= p(t)tg,\n\np(t)t = p(0)t exp(tg),\n\n(6.9)\n\n(cid:5)\u221e\nwhere p(0) is the probability vector for the states of x0, and the matrix exponential\nm=0(tg)m /m!, with g0 = is. ifthe initial state was\nexp(tg) isinterpreted as\nx0 = r, p(0) consists of zeros except for its rth component, implying that pr(xt =\ns | x0 = r) = prs(t) isthe ( r, s) element of exp(tg).\nany stationary distribution \u03c0 for xt must be time-independent, so the right-hand\nside of (6.9) will be zero when p(0) = \u03c0. hence \u03c0 t will be a left eigenvector of g\nwith eigenvalue zero.\nthe chain is reversible if and only if there is a distribution \u03c0 satisfying the detailed\nbalance condition \u03c0r \u03b3rs = \u03c0s \u03b3sr .\nif g is diagonalizable the eigendecomposition (6.3) is again useful. for if g =\n\u22121l e then gm = e\n\u22121l m e, so\nexp(tg) = e\n\n\u22121diag{exp(tl1), . . . ,exp(tl s)}e .\n\ne\n\nhence the sth row of e and column of e\nof exp(tg) with eigenvalue exp(tls). the fact that\ng1s = 0, so e\n\n(cid:5)\n(cid:9)\ns, are left and right eigenvectors\ns and e\n\u03b3rs = 0 for each r implies that\n= 1s is a right eigenvalue of g with eigenvalue l1 = 0, while e1 = \u03c0,\ns\n\n\u22121, et\n\n(cid:9)\n1\n\n "}, {"Page_number": 251, "text": "6.1 \u00b7 markov chains\n\n239\n\nas we saw above. the remaining eigenvalues of g all have strictly negative real parts.\nhence\n\n\uf8eb\n\uf8ed exp(tl1)\n\n\u00b7\u00b7\u00b7\n\n(cid:9)\ns)\n\ne\n\n0\n\n...\n\n0\n\nexp(tls)\n\n\uf8f6\n\uf8f7\uf8f8\n\n\uf8f6\n\uf8f8\n\n\uf8eb\n\uf8ec\uf8ed et\n...\net\ns\n\n1\n\n(cid:9)\n1\n\nexp(tg) = (e\n= s(cid:7)\nr=1\n\u2192 e\n(cid:9)\n1et\n1\n\n(cid:9)\nr et\nr\n\nexp(tlr )e\n= 1s\u03c0 t\n\nt \u2192 \u221e:\n\nas\n\n(cid:3)\n\n(cid:3)\n\n,\n\nstarting from any x0, the (r, s) element of exp(tg), pr(xt = s | x0 = r) \u2192 \u03c0s.\nthis transition probability may be written as a linear combination of exponentials,\ncrs,1etl1 + \u00b7\u00b7\u00b7 + crs,setls , where crs,v is the (r, s) element of e\nv, that is, the product\nof the rth element of e\n\n(cid:9)\nv and the sth element of ev.\n\n(cid:9)\nvet\n\nfully observed trajectory\nif xt had been fully observed during [0, t0], say, we would know exactly when and\nbetween which states transitions occurred. to write down the likelihood we would\nneed probabilities for events such as xu = r, 0 \u2264 u < t, followed by transition from\nr to s at time t, so xt = s. toobtain this we divide [0, t) into m intervals of length\n\u03b4t and apply the markov property to see that\n\nx \u03b4t = x2\u03b4t = \u00b7\u00b7\u00b7 = x(m\u22121)\u03b4t = r, xm\u03b4t = s | x0 = r\n\npr\n\n(cid:2)\n\nequals\n\n(cid:2)\n\npr\n\nxm\u03b4t = s | x(m\u22121)\u03b4t = r\n\n(cid:3) m\u22121(cid:4)\ni=1\n\npr\n\nand this itself is\n\n(cid:2)\n\nxi \u03b4t = r | x(i\u22121)\u03b4t = r\n(cid:9)\n\n(cid:10)m\u22121\n\n{1 + \u03b3rr \u03b4t + o(\u03b4t)}m\u22121 {\u03b3rs \u03b4t + o(\u03b4t)} =\n\non dividing by \u03b4t and letting m \u2192 \u221e, then recalling that \u03b3rr = \u2212(cid:5)\n\u03b3rv, we see\nthat the density corresponding to observing xu = r, 0 \u2264 u < t, followed by transition\nto xt = s, is\n\nv(cid:7)=r\n\n\u03b3rs \u03b4t + o(\u03b4t).\n\n1 + \u03b3rr t\nm\n\n\u03b3rs exp (t \u03b3rr ) = \u03b3rs exp\n\n(cid:22)\n\u2212t\n\n(cid:7)\nv(cid:7)=r\n\n(cid:23)\n\n\u03b3rv\n\n.\n\nthis has the simple interpretation that the first transition out of r occurs at t =\nmin{t : xt\n(cid:7)= r} =min v(cid:7)=r{trv}, where the trv are independent exponential variables\nwith parameters \u03b3rv, that is, with means \u03b3 \u22121\nrv . this suggests an algorithm for simulating\ndata from such a process (exercise 6.1.11).\nthe probability of a trajectory fully observed for the period [0, t0] and with transi-\ntions at t1 < \u00b7\u00b7\u00b7 < tk is calculated by using the markov property to express\n\npr (xt = s0, 0 \u2264 t < t1, xt = s1, t1 \u2264 t < t2, . . . , xt = sk , tk \u2264 t \u2264 t0)\n\n "}, {"Page_number": 252, "text": "240\n\nas\n\n6 \u00b7 stochastic models\n\npr (x0 = s0) pr(xt = s0, 0 < t < t1, xt1\n\n(cid:2)\n\npr\n\n\u00d7 k\u22121(cid:4)\n\nj=1\n\n= s1 | x0 = s0)\n= s j+1 | xt j\n\n(cid:3)\n\n= s j\n\nxt = s j , t j < t < t j+1, xt j+1\n\u00d7pr\nxt = sk , tk < t \u2264 t0 | xtk\n\n(cid:2)\n\n(cid:3)\n\n.\n\n= sk\n\nthus the likelihood for the \u03b3rs based on such data is\n\npr(x0 = s0) \u00d7 \u03b3s0s1et1\u03b3s0s0 \u00d7 k\u22121(cid:4)\n\n\u03b3s j s j+1e(t j+1\u2212t j )\u03b3s j s j \u00d7 e(t0\u2212tk )\u03b3sk sk .\n\nj=1\n\n(6.10)\nthe initial probability pr(x0 = s0) might be replaced by the s0th element of the\nstationary distribution of xt , ordropped from the likelihood. in either case (6.10) may\nbe maximized with respect to the \u03b3rs, s (cid:7)= r, ifenough transitions have occurred \u2014\nin general, no inferences can be made about transitions from r to s if none have been\nobserved.\n\n(cid:3)\n\nxt j\n\n= s j | xt j\u22121\n\npartially observed trajectory\nin practice trajectories may not be fully observed. one possibility is that the states\ns0, s1, . . . ,s k of xt at times 0 < t1 < \u00b7\u00b7\u00b7 < tk are known, as are the numbers and\ntypes of transitions between the s j , but that the times of these intervening transi-\ntions are unknown. a less informative possibility is that nothing is known about\n(cid:2)\ntransitions, so that only the s j and t j are known. the likelihood is then (6.1) with\nequal to the (s j\u22121, s j ) element of exp{(t j \u2212 t j\u22121)g}, that\npr\nis, ps j\u22121s j (t j \u2212 t j\u22121), and pr(x0 = s0) chosen according to context.\nexample 6.9 (two-state markov chain) the simplest case has s = 2 states with\ntransition intensities given by\ng =\nits eigendecomposition is\n\n(cid:9)\u2212\u03b312\n\u03b312\n\u03b321 \u2212\u03b321\n(cid:10)(cid:9)\n\n\u03b312, \u03b321 > 0.\n(cid:10)\n(cid:9)\n\n= s j\u22121\n\n(cid:10)\n\n(cid:9)\n\n(cid:10)\n\n,\n\ng =\n\n0\n0 \u2212(\u03b321 + \u03b312)\nso the limiting distribution is \u03c0 t = (\u03b312 + \u03b321)\n\n1\n\u03b312\n1 \u2212\u03b321\n\n0\n\n1\n\n\u03b312 + \u03b321\n\n\u22121(\u03b321, \u03b312), and\n\n(cid:9)\n\n,\n\n\u03b321\n\u03b312\n1 \u22121\n(cid:10)\n\nexp(tg) =\n\n1\n\n\u03b312 + \u03b321\n\n\u03b321 + \u03b312el2t\n\u03b321(1 \u2212 el2t )\n\n\u03b312(1 \u2212 el2t )\n\u03b312 + \u03b321el2t\n\n,\n\nwhere l2 = \u2212(\u03b312 + \u03b321) < 0 except in the trivial case \u03b312 = \u03b321 = 0, when the chain\nstays forever in its initial state.\nthe holding time in state r is exponential with parameter \u03b3rs, sothe likelihood based\non a trajectory fully observed on the interval [0, t0] with transitions 1 \u2192 2 \u2192 1 \u2192 2\nat t1 < t2 < t3 is\n\u03b312 + \u03b321\n\n\u2212(t2\u2212t1)\u03b321 \u00d7 \u03b312e\n\n\u2212(t3\u2212t2)\u03b312 \u00d7 e\n\n\u2212t1\u03b312 \u00d7 \u03b321e\n\n\u2212(t0\u2212t3)\u03b321 ,\n\n\u00d7 \u03b312e\n\n\u03b321\n\n "}, {"Page_number": 253, "text": "(cid:9)\n1\n\n241\n\n+ n21 log \u03b321 \u2212 \u03b321t\n\n6.1 \u00b7 markov chains\nthe first and last terms being the stationary probability pr(x0 = 1) and the probability\nthat no transition occurs in (t3, t0]. apart from the first term, the log likelihood is\n2, where nrs is the number of r \u2192 s transitions\nn12 log \u03b312 \u2212 \u03b312t\n(cid:9)\n(cid:9)\nand t\nr the total time spent in state r.\neach row of exp(tg) tends to \u03c0 t as t \u2192 \u221e. one effect of this is that if the process is\nobserved so intermittantly that x0, xt1\n, . . . are essentially independent, the transition\nprobabilities prs(t j \u2212 t j\u22121) will almost equal elements of \u03c0 t, because exp{l2(t j \u2212\nt j\u22121)} .= 0. if so, then although \u03b321/(\u03b312 + \u03b321) will be estimable \u2014 it will be roughly\nthe proportion of occasions that xt = 1 \u2014the individual rates \u03b312 and \u03b321 will not. the\nimplication for design of studies involving such models is that xt must be observed\noften enough that its successive values are correlated; otherwise only the stationary\ndistribution is estimable. if several transitions occur every week, data obtained at\n(cid:1)\nmonthly intervals will be essentially uninformative.\n\nexample 6.10 (breast cancer data) a model for these data has\n\n\uf8eb\n\uf8ed\u2212\u03b312 \u2212 \u03b313\n\n\u03b321\n0\n\ng =\n\n\uf8f6\n\uf8f8 ;\n\n\u03b312\n\n\u2212\u03b321 \u2212 \u03b323\n\n0\n\n\u03b313\n\u03b323\n0\n\nof course \u03b331 = \u03b332 = 0 because death is absorbing. a simpler model sets \u03b313 =\n0, so a woman with the disease cannot die without first being unable to walk.\nappropriate asymptotics take the number of women, rather than the number of\nobservations on each, large; below we suppose that large-sample approximations\nare applicable with just 37 women. in practice it would be wise to check this by\nsimulation.\n\nthe overall likelihood l is the product of independent contributions of form (6.1),\none for each woman. appreciable information might be lost by ignoring the terms\npr(x0 = s0), which comprise 37 of the 135 terms of l. owing to the absorbing state,\nwe cannot replace pr(x0 = s0) with its stationary value\n\nt\u2192\u221e pr(xt = 1) = lim\n\nt\u2192\u221e pr(xt = 2) = 0,\n\nlim\n\n(cid:7)= 3) instead, because only living women entered\n\nand we use limt\u2192\u221e pr(xt = s0 | xt\nthe study. now for s = 1, 2,\n\npr(xt = s | x0 = r) = crs,1etl1 + crs,2etl2 + crs,3etl3 ,\n\nwhere l3 < l2 < l1 = 0, and as this probability has limit zero we must have crs,1 = 0.\nas t \u2192 \u221e, therefore,\npr(xt = s | xt\n\ncrs,2el2t + crs,3el3t\n\n(cid:7)= 3, x0 = r) =\n\ncr1,2el2t + cr1,3el3t + cr2,2el2t + cr2,3el3t\ncr1,2 + cr2,2\ne2,1 + e2,2\n\n\u2192 crs,2\n=\ne2,s\n\n,\n\n "}, {"Page_number": 254, "text": "242\n\n6 \u00b7 stochastic models\n\nindependent of r, where e2,v is the vth element of et\ncorresponding to l2.\n\n2, the left eigenvector of g\n\nthe missing value complicates the likelihood contribution for woman 24,\n\nwhich is\n\ne2,1\n\ne2,1 + e2,2\n\n\u00d7 p12(3) \u00d7 { p21(3) p13(6) + p22(3) p23(6)} .\n\n2\n\n+ 1\n\nthe maximized log likelihoods for the three- and four-parameter models are\n\u2212107.43 and \u2212107.39. as \u03b313 = 0 lies on the boundary of the parameter space, the\nasymptotic distribution of the likelihood ratio statistic is 1\n\u03c7 2\n1 ; see example 4.39.\nits value, 2{\u2212107.39 \u2212 (\u2212107.43)} =0.08, supports the simpler model, for which\n2\nmaximum likelihood estimates and standard errors are (cid:1)\u03b312 = 0.116 (0.025), (cid:1)\u03b321 =\n0.057 (0.035) and(cid:1)\u03b323 = 0.238 (0.043). the transition rate \u03b321 is poorly determined,\nand taking the 95% confidence interval based on its profile likelihood, (0.014, 0.170),\n2 are(cid:1)\u03b3 \u22121\nis preferable to using its standard error. the estimated mean times spent in states 1 and\n\u22121 = 3.4 months, with death then occurring with es-\ntimated probability(cid:1)\u03b323/((cid:1)\u03b321 +(cid:1)\u03b323) = 0.81. confidence intervals for these quantities\nthe non-zero eigenvalues of (cid:1)g are \u22120.33 and \u22120.08, and examination of the\n\n= 8.6 and ((cid:1)\u03b321 +(cid:1)\u03b323)\n\nshould be based on profile likelihoods.\n\n12\n\nestimated transition matrices between the later follow-up times suggests that there is\nsome information in the small number of later transitions.\n\na more thorough analysis would assess the effect of initial status, for example by\nseeing if the likelihood increases significantly when the three-parameter model is fitted\nseparately to each of the two initial groups. of particular concern is the stationarity\nassumption, which is hard to justify here. the data are too sparse, however, for much\n(cid:1)\nfurther modelling to be conclusive.\n\ninhomogeneous chains\nif the transition rates \u03b3rs(t) depend on time then the fundamental equation (6.9) be-\nequations, whose solution may be written formally as p(t)t = p(0)t exp{(cid:24)\ncomes dp(t)t/dt = p(t)tg(t). this is a system of first-order ordinary differential\n0 g(s) ds}.\n\ntypically this will not be available explicitly, and the transition probabilities must\nbe obtained using packages for solving systems of ordinary differential equa-\ntions, or by discretizing time and fitting suitable models to the resulting transition\nprobabilities.\n\nt\n\nexercises 6.1\n1 classify the states of markov chains with transition matrices\n\n(cid:22)\n\n0\n0\n1\n2\n\n1\n0\n1\n2\n\n(cid:23)\n\n,\n\n0\n1\n0\n\n\uf8eb\n\uf8ec\uf8ed 0 1 0 0\n\n0 0 0 1\n0 0 1 0\n1 0 0 0\n\n\uf8f6\n\uf8f7\uf8f8 ,\n\n\uf8eb\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n1\n2\n1\n4\n1\n4\n1\n4\n0\n0\n\n1\n2\n3\n4\n1\n4\n0\n0\n0\n\n0\n0\n1\n4\n1\n4\n0\n0\n\n0\n0\n1\n4\n1\n4\n0\n0\n\n0\n0\n0\n0\n1\n2\n1\n2\n\n\uf8f6\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 .\n\n0\n0\n0\n1\n4\n1\n2\n1\n2\n\n "}, {"Page_number": 255, "text": "6.1 \u00b7 markov chains\n\n2 find the eigendecomposition of\n\n(cid:22)\n\np =\n\n0\n0\n1\n2\n\n1\n1\n2\n0\n\n0\n1\n2\n1\n2\n\n243\n\n(cid:23)\n\nand show that p11(n) = a + 2\nc. write down p11(n) for n = 0, 1 and 2 and hence find a, b and c.\n\n\u2212n {b cos(n\u03c0/2) + c sin(n\u03c0/2)} for some constants a, b and\n3 in example 6.5, sketch how p11(n) depends on n when l2 < 0, l2 > 0 and l2 = 0. find\n\ne(t11) byfirst showing that\n\npr(t11 = k) =\n\n4 say when\n\n(cid:22)\n\np =\n\n1 \u2212 p\n0\np\n\n(cid:25)\n\n1 \u2212 p,\npq(1 \u2212 q)k\u22122,\n(cid:23)\n\np\n1 \u2212 p\n0\n\n0\np\n1 \u2212 p\n\nk = 1,\nk = 2, 3, . . .\n\n0 \u2264 p \u2264 1,\n\n,\n\nhas an equilibrium distribution, and write it down. show that p has eigenvalues 1, (2 \u2212\n3 p \u00b1 i31/2 p)/2, and use them to say when the chain is ergodic.\n5 let xt be a stationary first-order markov chain with state space {1, . . . , s}, s > 2, and\nlet it indicate the event xt = 1. is {it} a markov chain?\n6 consider a sequence 0100 . . .10 of variables i j and let it = (2k + 1)\naverage of the 2k + 1 variables centred at t.\n(a) verify the calculations in example 6.5.\n(b) let the stationary first-order chain {it} have state space {0, 1} and transition probability\nmatrix p. inthe notation of example 6.5, show that\n\nk\nj=\u2212k it+ j be the\n\n(cid:5)\n\n\u22121\n\ncov(it , it+ j ) = pr(it = it+ j = 1) \u2212 pr(it = 1)pr(it+ j = 1) = pql j\n\n/( p + q)2,\n\n2\n\nand deduce that with m = 2k + 1,\nvar(it ) = 2\nm2\n\nm\u22121(cid:7)\nj=0\n\n(m \u2212 j)cov(i0, i j ) \u2212 var(i0)\n\n.\n\nm\n\n(cid:5)\n\nit may be useful to know\nthat for large n,\n.= p/(1 \u2212 p)2.\n\nn\nj=0 j p j\n\ngive an expression for var(it ), and show that it is roughly (2 \u2212 p \u2212 q)/( p + q) times the\ncorresponding expression for independent i j .\n\n7 check the log likelihood for the six-parameter model given at the end of example 6.8,\nobtain the maximum likelihood estimates and the fitted counts, and calculate pearson\u2019s\nstatistic. give its degrees of freedom and assess the fit of the model.\n\nss\n\n(cid:7)= s, xt+1 = \u00b7\u00b7\u00b7 = xt+m = s, xt+m+1 (cid:7)= s. show that this has probability pm\u22121\n\n8 a run of length m of a stationary markov chain occurs when there is a sequence of form\n(1 \u2212\nxt\npss) for m = 1, 2, . . .: the geometric density with mean (1 \u2212 pss)\n\u22121. show that in a first-\norder chain the lengths of separate runs are independent. is this true in higher-order chains?\ncan you construct a non-trivial 3 \u00d7 3 transition matrix for which it is impossible to use\nruns to falsify the independence model, whatever the length of the chain?\n9 recall that trs denotes the first-passage time from state r to state s. for the three-parameter\n\u22121(1 + g12)e(t13) and find the\nmodel in example 6.10, show that e(t23) = (g12 + g23)\ncorresponding equation for e(t13). hence give expressions for e(t13) and e(t23) and\nshow that their maximum likelihood estimates are 17 and 8.4 months respectively.\nwhat additional information do you need to compute standard errors for these estimates?\n10 modify the argument from the preceding question to find the moment-generating functions\nof t13 and t23 in terms of \u03b312, \u03b321, and \u03b323. hence check your formulae for e(t13) and\ne(t23).\n\n "}, {"Page_number": 256, "text": "6 \u00b7 stochastic models\n244\n11 let x1, . . . , xn be independent exponential variables with rates \u03bb j . show that y =\nmin(x1, . . . , xn) isalso exponential, with rate \u03bb1 + \u00b7\u00b7\u00b7 + \u03bbn, and that pr(y = x j ) =\n\u03bb j /(\u03bb1 + \u00b7\u00b7\u00b7 + \u03bbn). hence write down an algorithm to simulate data from a continuous-\ntime markov chain with finite state space, using exponential and multinomial random\nnumber generators.\n12 observations s0, . . . ,s k on a discrete-time markov chain with one-step transition matrix\np are obtained at times 0 < t1 < . . . < tk, where not all the t j \u2212 t j\u22121 equal unity. write\ndown the likelihood in terms of elements prs(n) of p n, n = 1, 2, . . .. give explicitly the\nthe likelihood when the states 12311 of a three-state chain with stationary distribution \u03c0\nare observed at times 0, 1, 3, 4, 6.\nexplain how you would calculate the likelihood l for the data in table 6.3, with three-\nmonth transition probability matrix\n\n(cid:22)\n\np =\n\n1 \u2212 p12\np21\n0\n\n(cid:23)\n\n.\n\np12\n\n1 \u2212 p21 \u2212 p23\n\n0\n\n0\np23\n1\n\n13 check the eigendecomposition of g in example 6.9. calculate the stationary distribution\n\nwhat value has l under this model? how could p be made more plausible?\nwhen \u03b312 = 0. is this a surprise?\n\nlook carefully at the data.\n\n6.2 markov random fields\n6.2.1 basic notions\nthe previous section described simple models for random variables indexed by a\nscalar, often time, so the variables can be visualized at points along an axis. many\napplications require variables associated to points in space or in space-time, however,\nand then more general indexing sets are needed. think, for example, of the colours\nof pixels in an image, the fertility of parts of a field or the occurrence of cancer cases\nat points on a map. this section outlines how our earlier ideas extend to some more\ncomplex settings. there is a close connection to notions of statistical physics, from\nwhich some of the terminology is derived.\n\nour earlier discussion owed its relative simplicity to the markov property \u2014 that the\n\u2018future\u2019 is independent of the \u2018past\u2019, conditional on the \u2018present\u2019 \u2014 whose importance\nsuggests that we should seek its analogy here. the notions of \u2018past\u2019, \u2018present\u2019, and\n\u2018future\u2019 have no obvious spatial counterparts, but another formulation does generalize\nin a natural way. a sequence y1, . . . ,y n satisfies the markov property if\n\npr(y j+1 = y j+1 | y1 = y1, . . . ,y j = y j ) = pr(y j+1 = y j+1 | y j = y j )\n\nfor j = 1, . . . ,n \u2212 1 and all y. this is equivalent to having each y j depend on the\nremaining variables y\u2212 j = (y1, . . . ,y j\u22121, y j+1, . . . ,y n) only through the adjacent\nvariables y j\u22121 and y j+1 (exercise 6.2.1). to prepare for our generalization, let n j\ndenote the set of neighbours of j, given by n j = { j \u2212 1, j + 1} for j = 2, . . . ,n \u2212\n1, with n1 = {2} and nn = {n \u2212 1}; hence yn j\n= (y j\u22121, y j+1) for j (cid:7)= 1, n, while\n= yn\u22121. then the markov property for variables along an axis is\nyn1\n(cid:2)\nequivalent to\n\n= y2 and ynn\n\n(cid:3)\n\npr(y j = y j | y\u2212 j = y\u2212 j ) = pr\n\ny j = y j | yn j\n\n= yn j\n\n,\n\n(6.11)\n\n "}, {"Page_number": 257, "text": "6.2 \u00b7 markov random fields\n\n245\n\nfigure 6.4 markov\nrandom fields. left:\nneighbourhood structure\nfor first-order markov\nchain and its cliques and\ntheir subsets. right:\nfirst-order neighbourhood\nstructure, cliques and their\nsubsets for rectangular\ngrid of sites.\n\n1\n\n2\n\n3\n\nn\u20131\n\nn\n\ne\n\ne\n\ne\n\ne\n\ne\n\ne\n\ne\n\ne\n\ne\n\ne\n\ne\n\ne\n\ne\n\ne\n\ne\n\ne\n\ne\n\ne\n\ne\n\ne\n\ne\n\nfor all values of j and y. thus y j depends on the other variables only through the\nneighbouring variables yn j . the probability densities on the left of (6.11) are known\nto statisticians as full conditional densities, while those on the right are called local\ncharacteristics in statistical physics.\nfor more complicated settings, let j = {1, . . . ,n } be a finite set of sites, each with\na random variable y j attached. in many applications each y j takes the same finite\nnumber k of values, and then y1, . . . ,y n may have at most kn possible configurations;\nthough finite, this number may be very large indeed. for any subset a \u2282 j , let\nya denote the corresponding subset of y \u2261 yj , and let y\u2212a indicate yj \u2212a, with\ny j = y{ j} and y\u2212 j defined as above. we impose a topology on j by defining a\nneighbourhood system n = {n j , j \u2208 j }. the neighbours of j are the elements of\nn j \u2282 j , the neighbourhoods n j having the properties that\n\nr\n\nj (cid:7)\u2208 n j and\n\nr i \u2208 n j if and only if j \u2208 ni .\n\nwe visualize this as a graph (j ,n ) whose nodes correspond to sites, with two nodes\njoined by an edge if the sites are neighbours. we denote the union of { j} and its\nneighbourhood by \u02dcn j = n j \u222a { j}. asubset c \u2282 j is complete if there are edges\nbetween all its nodes, and a maximal complete subset is a clique of (j ,n ); every pair\nof distinct elements of c are then neighbours, but c cannot be enlarged and retain this\nproperty. let c denote the set of cliques and their subsets; in particular, c contains all\nsingletons { j} and the empty set \u2205.\n\nexample 6.11 (markov chain) for the graph on the left of figure 6.4, each interior\nvariable has just two neighbours, and the end variables have just one. hence c =\n{\u2205,{1}, . . . ,{n },{1, 2}, . . . ,{n \u2212 1, n}}; the cliques are the n \u2212 1 adjacent pairs. (cid:1)\nexample 6.12 (pixillated image) let j be an m \u00d7 m rectangular array of sites,\nwith neighbourhood structure shown on the right of figure 6.4. here n = m2. interior\n\nsome authors do not insist\nthat cliques be maximal.\n\n "}, {"Page_number": 258, "text": "or sometimes a markov\nfield or a locally\ndependent markov\nrandom field.\n\n246\n\n6 \u00b7 stochastic models\n\nsites have four neighbours, while boundary sites have two or three neighbours. the\ncliques are horizontal or vertical pairs of adjacent sites.\n\nthis neighbourhood system is said to be first-order. it is easy to envisage enlarging\nthe neighbourhoods, for example by adding adjacent diagonal sites to give a second-\n(cid:1)\norder neighbourhood system.\n\nhaving defined a neighbourhood system analogous to that implicit in a markov\nchain, the extension of the markov property is clear: a probability distribution for y\nis said to be a markov random field with respect to n if y j is independent of y\u2212 \u02dcn j\ngiven yn j , orequivalently, if (6.11) holds: the conditional distribution of y j depends\non the other variables only through those at the neighbouring sites.\nalthough the local characteristics of y are determined by its joint density, it is not\ntrue that any collection pr(y1 | yn1), . . . ,pr(y n | ynn ) oflocal characteristics yields a\nproper joint density. this is awkward, because in practice the local characteristics are\nmuch easier to deal with than the full joint density. hence we ask which collections\nof pr(y j | yn j ) = pr(y j | y\u2212 j ) give well-defined joint distributions. it turns out that\na positivity condition is needed, that for any y1, . . . , yn,\npr(y j = y j ) > 0 for j = 1, . . . ,n implies pr(y1 = y1, . . . ,y n = yn) > 0 :\nif values of y j can occur singly they can occur together. in this case\n(cid:9)\nn)\n, . . . , y\n, . . . , y(cid:9)\nn)\n\npr(y = y)\npr(y = y(cid:9))\n\n= n(cid:4)\n\n(cid:9)\nj+1\n(cid:9)\nj+1\n\n(6.12)\n\n(6.13)\n\npr(y j | y1, . . . , y j\u22121, y\n| y1, . . . , y j\u22121, y\npr(y\n(cid:9)\n\nof y (exercise 6.2.5). hence (6.13) may\nfor any two possible realizations y and y\nand using the full\nbe found for every possible y simply by taking a baseline y\nconditional densities, the value of pr(y = y\n) being found by summing the ratios.\nunder the positivity condition, therefore, the full conditional densities determine a\nunique joint density for y . this density must be unaffected by the labelling of the sites\nof j , any change to which will leave (6.13) unaltered. this is a severe restriction,\nand we shall see at the end of this section that the joint density must have form\n\nj=1\n\n(cid:9)\nj\n\n(cid:9)\n\n(cid:9)\n\npr(y = y) \u221d exp{\u2212\u03c8(y)} ,\n\n(6.14)\n\nwhere\n\n\u03c8(y) =\n\n(cid:7)\nc\u2208c\n\n\u03c6c(y),\n\n(6.15)\nis a sum over all complete subsets c associated with the graph (j ,n ); this result,\nthe hammersley\u2013clifford theorem, isproved at the end of this section. hence the\nonly contributions to the joint density come from cliques of (j ,n ) and their sub-\nsets. moreover the functions \u03c6c can be arbitrary, provided the total probability of\n(6.14) is finite. many standard models have functions \u03c6c chosen so that (6.14) is\nan exponential family, but though convenient this is not essential. the sum in (6.15)\ncould involve only cliques, as contributions from other complete subsets could be sub-\nsumed into those from the cliques. the collection of functions {\u03c6c : c \u2208 c} is called a\npotential.\n\n "}, {"Page_number": 259, "text": "6.2 \u00b7 markov random fields\n\n247\n\nthe representation given by (6.14) and (6.15) is powerful because it enables systems\nwhose global behaviour is very complex to be built from simple local components,\nnamely the local characteristics determined by the \u03c6c. this is analogous to the notion\nthat the transition probabilities of a markov chain entirely determine its behaviour.\nin example 6.11 c contains the empty set, single-\n\nexample 6.13 (markov chain)\ntons, and pairs of adjacent sites, and hence\n\n\u03c8(y) = a + n(cid:7)\n\nj=1\n\nb j (y j ) +\n\n(cid:7)\ni\u223c j\n\nci j (yi , y j ),\n\nwhere the second sum is over all distinct pairs of neighbours, or equivalently all edges\nof the graph. the proportionality in (6.14) means that we can set a = 0, while setting\nb j \u2261 b and ci j (\u00b7,\u00b7) = c(\u00b7,\u00b7) for all i and j gives a homogeneous field.\nif the field is homogeneous and the y j take only values 0 and 1, we may write\n\n\u03c8(y) = n(cid:7)\n\nby j + n\u22121(cid:7)\n\nj=1\n\nj=1\n\n(c10 y j + c01 y j+1 + c11 y j y j+1),\n\nand a little algebra gives\n\npr(y j+1 = y j+1 | y1 = y1, . . . ,y j = y j ) = e(\u03b2+\u03b3 y j )y j+1\n1 + e(\u03b2+\u03b3 y j )\n\n,\n\nwhere \u03b2 and \u03b3 are functions of b, c10, c01 and c11. as expected, this conditional\nprobability depends on y1, . . . , y j only through y j and does not depend upon j\ndirectly. hence it corresponds to a stationary first-order markov chain with transition\nprobabilities pr(0 | 0) = (1 + e\u03b2)\n\u22121 and pr(0 | 1) = (1 + e\u03b2+\u03b3 )\n(cid:3)\n\nif the y j take values in the real line and we set\n\n\u22121.\n\n(cid:2)\n\n\u2212 2\u00b5y j\n\ny2\nj\n\n/(2\u03c3 2),\n\nc(yi , y j ) = (yi \u2212 y j )2/(2\u03c3 2),\n\nb(y j ) = \u03c4\n\uf8eb\n\nthen \u03c8(y) = (ytv y \u2212 2\u00b5ytv 1n)/(2\u03c3 2), where\n\u00b7\u00b7\u00b7\n0\n\u00b7\u00b7\u00b7\n0\n\u00b7\u00b7\u00b7\n0\n...\n...\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7 \u22121\n\n\u03c4 + 1 \u22121\n0\n\u03c4 + 2 \u22121\n\u22121\n\u22121\n\u03c4 + 2\n0\n...\n...\n0\n0\n0\n0\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n0\n0\n0\n...\n\u03c4 + 2 \u22121\n\u03c4 + 1\n\nv =\n\n...\n0\n0\n\nand 1n is an n \u00d7 1 vector of ones. it follows that\n\n(cid:21)\n\nexp{\u2212\u03c8(y)} \u221d exp\n\n\u2212 1\n2\u03c3 2 (y \u2212 \u00b51n)t v (y \u2212 \u00b51n)\n\n\uf8f6\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 ,\n\n(cid:26)\n\n,\n\nwhich corresponds to the multivariate normal distribution with mean vector \u00b51n and\ncovariance matrix v /(2\u03c3 2).\nif \u03c4 = 0, the rows of v sum to zero and the distribution is degenerate. moreover\n(6.14) is integrable only if \u03c3 2 > 0. this underlines the fact that although any choice\nof b j and ci j yields a proper joint density when each y j takes only a finite number\n\n "}, {"Page_number": 260, "text": "248\n\n6 \u00b7 stochastic models\n\nof values, restrictions may be needed to ensure this when any of the y j has infinite\nsupport.\n\nexample 11.27 gives an application of this.\n\n(cid:1)\nexample 6.14 (ising model) let j be an m \u00d7 m grid of pixels, the jth of which\ncan take values 0 and 1, corresponding to the colours white and black. as n = m2,\nthe sample space has size 2m2, about 104932 even for a small image with m = 128.\nunder a first-order neighbourhood system the cliques are horizontal and vertical pairs\nof adjacent pixels; see figure 6.4. hence if b j and ci j are homogeneous, we can take\n\n\u03c8(y) =\n\nb(y j ) +\n\nc(yi , y j )\n\n(cid:7)\n\nj\n\n(cid:7)\ni\u223c j\n\nthe second sum being over all distinct cliques. the resulting probability distribu-\ntion is the ising model of statistical physics, which is important in investigations of\nferromagnetism.\nthe conditional probability that y j = 0 given y\u2212 j is\npr(y j = 0, y\u2212 j = y\u2212 j )\n\npr(y j = 0, y\u2212 j = y\u2212 j ) + pr(y j = 1, y\u2212 j = y\u2212 j )\n\n,\n\nand on using (6.14) and cancelling all terms not involving y j , weobtain\n\n(cid:25)\n\nexp\n\n\u2212b(0) \u2212(cid:5)\n\nexp\n\ni\u2208n j\n\n(cid:25)\n\n\u2212b(0) \u2212(cid:5)\n(cid:27)\n(cid:25)\ni\u2208n j\n+ exp\n\nc(yi , 0)\n\n(cid:27)\n\u2212b(1) \u2212(cid:5)\n\nc(yi , 0)\n\n(cid:27) ;\n\nc(yi , 1)\n\ni\u2208n j\n\nthus the full conditional densities have form\n\npr(y j = 0 | y\u2212 j ) =\n\nb(0) \u2212 b(1) +(cid:5)\nlet n1 denote\nand define n0 similarly; note that n0 = |n j| \u2212n 1. now\n\n(cid:5)\ni\u2208n j\n\n1 + exp\n\n1\n\n(cid:25)\n\ni\u2208n j\n\ni (yi = 1), the number of neighbours of site j that equal one,\n\n(cid:27) .\n\nc(yi , 0) \u2212 c(yi , 1)\n\nc(yi , 0) \u2212 c(yi , 1) = n0c(0, 0) + n1c(1, 0) \u2212 n0c(0, 1) \u2212 n1c(1, 1)\n\n(cid:7)\ni\u2208n j\n\n= n0 {c(0, 0) + c(1, 1) \u2212 c(0, 1) \u2212 c(1, 0)}\n\n+ |n j|{c(1, 0) \u2212 c(1, 1)} ,\n\nfrom which it follows that we can write\n\npr(y j = 0 | y\u2212 j ) = pr(y j = 0 | yn j ) =\n\n(6.16)\nwe interpret \u03b2 + |n j|\u03b3 as controlling the overall size of the probability and \u03b4 its\ndependence on the number of its white neighbours: \u03b3 = 0 means that the colour\nof cell j is independent of the colours around it, while (6.16) increases to one as\n\u03b3 \u2192 \u2212\u221e.\n\n1 + exp(\u03b2 + \u03b3|n j| +\u03b4 n0)\n\n1\n\n.\n\nimages with more colours may be dealt with by letting y j take k > 2 values, with\nan analogous argument giving the local characteristics. more complex neighbourhood\n\nernst ising (1900\u20131998)\nwas one of the generation\nof german scientists\nwhose careers were\ndestroyed by the rise of\nnazism. after a period of\nforced labour during the\nwar he emigrated to the\nusa in 1949. the ising\nmodel described in his\n1924 phd thesis was later\nused to account for the\nphase transition between\nthe ferromagnetic and\nparamagnetic states.\n\n|a| is the cardinality of\nthe set a.\n\n "}, {"Page_number": 261, "text": "6.2 \u00b7 markov random fields\n\n249\n\nfigure 6.5 a small\ngeneology. females are\nshown as circles, males as\nsquares, and marriages\nleading to offspring as\ndots. thus the male shown\nby the solid square has\ntwo parents and three\nchildren by two\nmarriages. this would be\nhis neighbourhood in\npotentially a much larger\npedigree.\n\nstructures will introduce more parameters into the model, while these ideas can be\n(cid:1)\nextended to fields that allow lines, textures and other features of real images.\n\nexample 6.15 (genetic pedigree)\nin the analysis of a genealogy, the sites typically\ncorrespond to individuals and y j to the genotype at a particular locus on the jth\nindividual\u2019s dna. typically the genotype cannot be observed, but the phenotypes of\nsome of the individuals are known. a simple example is in the abo blood group\nsystem, where the observable phenotype blood group \u2018a\u2019 arises with genotypes aa\nand ao which are harder to observe; see example 4.38. two individuals in a pedigree\nare said to be spouses if they have mutual offspring in the pedigree, and each such\npairing constitutes a marriage. apedigree may be represented as a graph in which both\nindividuals and marriages correspond to nodes, while the edges link each individual\nto his or her marriages and each marriage to the resulting offspring. see figure 6.5.\nthe laws of genetic inheritance are markovian. genes are passed from parents\nto offspring in such a way that conditional on their parents\u2019 genotypes, individuals\nare independent of their earlier direct ancestors. it turns out that this dependence\nimposes a neighbourhood structure on the genotypes, with the neighbourhood for any\nindividual defined to contain his parents, children and spouses. however distributions\ndefined on this structure need not satisfy the positivity condition. a simple example\nis the abo blood system: a person whose parents are both of type ab cannot be of\ntype o. the fact that genetic models usually do not satisfy the positivity condition\n(cid:1)\ncomplicates statistical analysis of pedigree data.\n\nstatistical inference for markov random fields is generally based on the iterative\n\nsimulation methods discussed in section 11.3.3.\n\n6.2.2 directed acyclic graphs\nthus far we have supposed that all the y j have the same support and that the neigh-\nbourhood structure of the random field is known. the idea of expressing dependencies\namong variables as a graph is useful in more general settings, however, and it is then\nnecessary to read off neighbourhoods from the joint distribution of y1, . . . ,y n. often\n\n "}, {"Page_number": 262, "text": "250\n\n6 \u00b7 stochastic models\n\nfigure 6.6 directed\nacyclic and moral graphs.\nleft: directed acyclic\ngraph representing (6.17).\nright: moral graph,\nformed by moralizing the\ndirected acyclic graph,\nthat is, \u2018marrying\u2019 parents\nand dropping arrowheads.\n\nthe dependence structure is specified hierarchically, for example by stating the condi-\ntional distributions of y1 given y2 and of y2 given y3, y4 and so forth. the hierarchy\nmay then be expressed using a directed graph, in which dependence of y1 on y2 is\nshown by an arrow from the parent y2 to the child y1, and y1 is a descendent of y3\nif there is a sequence of arrows from y3 to y1. such a graph is directed because each\nedge is an arrow, and acyclic if it is impossible to start from a node, traverse a path\nby following arrows, and return to the starting-point. the left of figure 6.6 shows the\ndirected acyclic graph for a model in which the joint density of y1, . . . ,y 6 factorizes as\n\nf (y) = f (y1 | y2, y5) f (y2 | y3, y6) f (y3) f (y4 | y5) f (y5 | y6) f (y6).\n\nfor any directed acyclic graph we have\ny j \u22a5 non-descendents of y j\n\n| parents of y j ,\n\nfor all j,\n\nand (6.17) generalizes to\n\nf (y) =\n\n(cid:4)\nj\u2208j\n\nf (y j | parents of y j ).\n\n\u22a5 means \u2018is independent\nof\u2019.\n\n(6.17)\n\n(6.18)\n\nthe density is then said to be recursive with respect to the directed acyclic graph.\nacyclicity prevents a variable from introducing a degenerate density by being its own\ndescendent.\n\na directed acyclic graph does not display all the neighbourhoods of the resulting\n\nmarkov random field, but its moral graph does. this is obtained by moralizing the also called a conditional\ndirected acyclic graph \u2014 \u2018marrying\u2019 or putting edges between any parents that share\na child and then cutting off the arrowheads. in figure 6.6, for example, the directed\nacyclic graph on the left shows us that y2 and y5 are parents of y1, sothey are joined\nin the moral graph on the right. this shows us that\n\nindependence graph.\na moral graph contains no\nunmarried parents.\n\nn1 = {2, 5}, n2 = {1, 3, 5, 6}, n3 = {2, 6},\nn4 = {5}, n5 = {1, 2, 4, 6}, n6 = {2, 3, 5}.\n\nin general the full conditional density of y j is\n\nf (y j | y\u2212 j ) =\n\nf (y)\nf (y) dy j\n\n(cid:24)\n(cid:20)\ni\u2208j f (yi | parents of yi )\n(cid:24) (cid:20)\n=\ni\u2208j f (yi | parents of yi ) dy j\n(cid:4)\n\u221d f (y j | parents of y j )\n\ni: yi is child of y j\n\nf (yi | parents of yi ),\n\n "}, {"Page_number": 263, "text": "6.2 \u00b7 markov random fields\n\n251\n\nbecause the integral only affects terms where y j appears. in order for the denominator\nto be positive for any y\u2212 j , the positivity condition must hold. if so, we see that n j\ncomprises the parents and children of y j , and any parents of y j \u2019s children, precisely\nthose variables joined to y j in the moral graph. thus the distribution of y satisfies\n(6.11), also called the local markov property.\nconsider a directed acyclic graph, let the family f j consist of j and its parents,\nif any, and let c denote the cliques of the corresponding moral graph. then as the\nfamilies f j yield cliques c \u2208 c, wemay write\ng(yf j ) =\n\n(6.19)\ntaking g(yf j ) = f (y j | parents of y j ). thus we may write the joint density in terms\nof the cliques of an moral graph, analogous to (6.14) and (6.15). let a and b be\ndisjoint subsets of j that are separated by d, that is, any path from an element\nof a to an element of b must pass through d. then under the positivity condition\nthe distribution on the moral graph has the global markov property, that ya and yb\nare independent conditional on yd. tosee this in the case where all the variables are\ndiscrete, suppose for now that a \u222a b \u222a d = j , and note that as no clique can contain\nelements of both a and b, (6.19) implies that the joint density can be written as\n\n(cid:4)\nc\u2208c\n\nf (y) =\n\nhc(y),\n\n(cid:4)\n\nj\n\nthus\n\nf (y) = f (ya, yb, yd) = g1(ya, yd)g2(yb, yd).\n\nf (ya, yb | yd) =\n\n(cid:5)\n\n(cid:5)\ng1(ya, yd)g2(yb, yd)\n\nyb g1(ya, yd)g2(yb, yd)\n\n,\n\nya\n\nwhich factorizes in terms of ya and yb, showing that any subset of ya is independent\nof any subset of yb, conditional on yd. the positivity condition ensures that the\ndenominator here is positive for any yd. we now have only to note that ifa \u222a b \u222a d (cid:7)=\nj , then a, b can be enlarged to give sets a(cid:9)\nwhich together with d partition j\nb | yd, implying that ya \u22a5 yb | yd, which\nsuch that d separates a(cid:9)\n(cid:9)\nis the global markov property. the moral graph in figure 6.6, for example, shows\nthat\n\na \u22a5 y\n(cid:9)\n\n. then y\n\n, b(cid:9)\n\n, b(cid:9)\n\ny1 \u22a5 y3, y4 | y2, y5,\n\nas can be verified from (6.17).\n\nmarkov properties of this sort are useful because they enable the computation of\nf (y) orderived quantities to be broken into practicable steps. sometimes the moral\ngraph must be triangulated by adding edges to ensure that every cycle of length four\nor more contains an edge between two nodes that are not adjacent in the cycle itself.\ntriangulation can accelerate computation of f (y) bymaking closed-form calculations\npossible for some model classes.\n\nexample 6.16 (belief network) graphs may be used to represent supposed logical\nor causal relationships among variables and play an important role in probabilistic\nexpert systems. figure 6.7, for instance, shows a directed acyclic graph that represents\n\n "}, {"Page_number": 264, "text": "252\n\n6 \u00b7 stochastic models\n\n1: birth\n    asphyxia?\n\n2: disease?\n\nfigure 6.7 directed\nacyclic graph representing\nthe incidence and\npresentation of six\npossible diseases that\nwould lead to a \u2018blue\u2019\nbaby (spiegelhalter et al.,\n1993). lvh means left\nventricular hypertrophy.\n\n3: age at \n    presentation?\n\n4: lvh?\n\n5: duct\n    flow?\n\n6: cardiac\n    mixing?\n\n7: lung\n    parenchema?\n\n8: lung\n    flow?\n\n9: sick?\n\n10: hypoxia\n      distribution?\n\n11: hypoxia\n      in o2?\n\n12: co2?\n\n13: chest\n      x-ray?\n\n14: grunting?\n\n15: lvh\n      report?\n\n16: lower\n      body o2?\n\nup.\n17: right\n      quad. o2?\n\n18: co2\n      report?\n\n19: x-ray\n      report?\n\n20: grunting\n      report?\n\nthe incidence and presentation of six diseases that would lead to a \u2018blue\u2019 baby. early\nappropriate treatment is essential when such a child is born, and this expert system\nwas developed to increase the accuracy of preliminary diagnoses. the graph shows,\nfor example, that the level of oxygen in the lower body (node 16) is thought to be\ndirectly related to hypoxia distribution (node 10) and to its level when breathing\noxygen (node 11). this last variable depends on the degree of mixing of blood in the\nheart (node 6) and the state of the blood vessels (parenchyma) in the lungs (node 7),\nand these two variables are directly influenced by which of the six possible levels the\nvariable disease (node 2) has taken. links such as those between nodes 6 and node\n11 might be regarded as causal if poor cardiac mixing was known to contribute to\nhypoxia.\n\neach variable in such a network is typically treated as discrete, so the joint distri-\nbution of the variables is determined by a large number of multinomial distributions\ngiving the terms on the right of (6.18). these are often obtained by eliciting opin-\nions from experts and then updating these opinions, and perhaps the structure of the\ngraph, as data become available. table 6.6, for example, shows the expert view that\nleft ventricular hypertrophy (lvh) would be present in 10% of cases of persistent\nfoetal circulation, and that if present, it would be correctly reported in 90% of cases.\nthe full distribution is given by specifying such tables for each of the 20 nodes of the\ngraph, giving a sample space with more than one billion elements.\n\nnow imagine that the lvh report for a baby is positive. in the light of this evidence\nthe probabilities for the other variables will need updating, for example to ascribe new\nprobabilities to the diseases or to determine which other diagnostic report will be most\ninformative. thus evidence must be propogated through the network to give the joint\ndistribution of the other variables conditional on a positive lvh report. this involves\n\n "}, {"Page_number": 265, "text": "6.2 \u00b7 markov random fields\n\n253\n\ntable 6.6 subjective\nexpert assessments of\nconditional probability\ntables for links node 2 \u2192\nnode 4 and node 4 \u2192\nnode 15 in figure 6.7\n(spiegelhalter et al.,\n1993).\n\nnode 2: disease\n\nnode 4: lvh\n\nyes\n\nno\n\npersistent foetal circulation\ntransposition of the great arteries\nteralogy of fallot\npulmonary atresia with intact ventricular septum\nobstructed total anomalous pulmonary venous connection\nlung disease\n\n0.10\n0.10\n0.10\n0.90\n0.05\n0.10\n\n0.90\n0.90\n0.90\n0.10\n0.95\n0.90\n\nnode 15: lvh report\n\nnode 4: lvh\n\nyes\n\nyes\nno\n\n0.90\n0.05\n\nno\n\n0.10\n0.95\n\nthe cliques of the triangulated moral graph of figure 6.7. details are given in the\n(cid:1)\nreferences in the bibliographic notes.\n\ndirected acyclic and their moral graphs play a useful role in the iterative simulation\n\nmethods described in section 11.3.3.\n\nthis can be omitted at a\nfirst reading.\n\nhammersley\u2013clifford theorem\nwe now show that if the positivity condition (6.12) holds when all the y j take values\nin {0, . . . , l}, then the most general form that their joint density f (y) can take is\ngiven by (6.14) and (6.15). conversely these equations entail the markov property\n(6.11) and positivity condition (6.12).\nlet y = {0, . . . , l}n denote the sample space for y1, . . . ,y n, and for any y \u2208 y\nj denote the vector (y1, . . . , y j\u22121, 0, y j+1, . . . ,n ). under the positivity condi-\nlet y0\ntion every element of y occurs with positive probability, so we can define \u03c8(y) =\nlog{ f (y)/ f (0)}, where 0 represents a vector of n zeros. now\n= f (y j | yn j )\nf (0 | yn j )\nbecause the joint density satisfies the local markov property, so knowing \u03c8 will\ndetermine the full conditional densities and therefore the local characteristics of f (y).\nnote that this implies that \u03c8(y) \u2212 \u03c8(y0\n\u03c8(y) = n(cid:7)\n\n(cid:3) = f (y j | y1, . . . , y j\u22121, y j+1, . . . , yn)\nf (0 | y1, . . . , y j\u22121, y j+1, . . . , yn)\n\nnow any function \u03c8(y) has an expansion\n\nj ) depends only on y j and yn j .\n\n(cid:28)\n\u03c8(y) \u2212 \u03c8\n\n(cid:3)(cid:29) = f (y)\n(cid:2)\n\ny j yka jk(y j , yk)\n\nexp\n\ny0\nj\n\ny0\nj\n\n(cid:2)\n\nf\n\n,\n\n(cid:7)\n1\u2264 j <k\u2264n\n\ny j yk yla jkl(y j , yk , yl) + \u00b7\u00b7\u00b7 + y1 \u00b7\u00b7\u00b7 yna1\u00b7\u00b7\u00b7n(y1, . . . , yn),\n\ny j a j (y j ) +\n(cid:7)\n1\u2264 j <k<l\u2264n\n\nj=1\n+\n\n "}, {"Page_number": 266, "text": "254\n\n6 \u00b7 stochastic models\nbecause we can set y j a j (y j ) = \u03c8(0, . . . ,0, y j , 0, . . . ,0) \u2212 \u03c8(0), with analogous for-\nmulae for the other a-functions. we must now show that for any subset c of{1, . . . ,n },\nthe corresponding a-function may be non-null if and only if c is a clique of the graph.\nconsider y1 without loss of generality, and recall that owing to (6.11), \u03c8(y) \u2212 \u03c8(y0\n1)\ndepends only on y1 and yn1. now\u03c8 (y) \u2212 \u03c8(y0\ny j a1 j (y1, y j ) +\n\n(cid:6)\na1(y1) +\n\ny1\n\n(cid:7)\n2\u2264 j\u2264n\n\n1) equals\n(cid:7)\n2\u2264 j <k\u2264n\n+ \u00b7\u00b7\u00b7 + y2 \u00b7\u00b7\u00b7 yna1\u00b7\u00b7\u00b7n(y1, . . . , yn)\n\n,\n\ny j yka1 jk(y1, y j , yk)\n(cid:8)\n\nand this must be free of yl for any l (cid:7)\u2208 n1. onsetting y j = 0 for j (cid:7)= 1, l, wesee that\na1l(y1, yl) = 0 for every possible yl. suitable choices of y show in like wise that every\nother a-function involving yl must be identically zero. as the same is true for every\nother node, the markov property (6.11) implies that the only non-zero functions a j1\u00b7\u00b7\u00b7 jk\nare those in which j1, . . . , jk are all neighbours, that is, form a subset of a clique, and\nthis entails (6.14) and (6.15).\nfor the converse, note that any set of a-functions gives a density f (y) that sat-\nisfies the positivity condition (6.12). now \u03c8(y) \u2212 \u03c8(y0\nj ) depends on xl only if\nthere is a non-null a-function containing both y j and yl, sothe local characteristic\nf (y j | y1, . . . , y j\u22121, y j+1, . . . , yn) also depends only on those yl that are neighbours\nof y j . thus (6.14) and (6.15) together imply (6.11) and (6.12).\n\nexercises 6.2\n1 show that the markov property for a sequence y1, . . . ,y n is equivalent to (6.11).\n2 give the cliques for the second-order neighbourhood system in example 6.12.\n3 give the cliques for a second-order markov chain, and hence write down the form of the\n\nmost general density for it, under the positivity condition.\n\n4 consider two binary random variables with local characteristics\npr(y1 = 1 | y2 = 0) = pr(y1 = 0 | y2 = 1) = 1,\npr(y2 = 0 | y1 = 0) = pr(y2 = 1 | y1 = 1) = 1.\n\nshow that these do not determine a joint density for (y1, y2). is the positivity condition\nsatisfied?\n5 let the density of a random variable y = (y1, . . . ,y n) satisfy (6.12), and let the sample\nspace be the set y = {y : f (y) > 0}. if (y1, . . . , yn) and (x1, . . . , xn) are two elements\nof y, use the identity\n\nto show that\n\nand then that\n\nf (y1, . . . , yn) = f (yn | y1, . . . , yn\u22121) f (y1, . . . , yn\u22121)\nf (y1, . . . , yn) = f (yn | y1, . . . , yn\u22121)\nf (xk | y1, . . . , yn\u22121)\nf (y1, . . . , yn\u22121, xn) = f (yn\u22121 | y1, . . . , yn\u22122, xn)\nf (xn\u22121 | y1, . . . , yn\u22122, xn)\n\nf (y1, . . . , yn\u22121, xn),\n\nf (y1, . . . , yn\u22122, xn\u22121, xn),\n\n "}, {"Page_number": 267, "text": "6.3 \u00b7 multivariate normal data\n\nhence establish (6.13).\n(besag, 1974)\n\n255\n\n6 use induction on the number of variables to prove (6.18).\n7 let gm be the graph obtained by moralizing a finite directed acyclic graph g. show that\nevery family of g is a clique of gm but that the converse is false (consider the moral graph\nfor figure 6.7).\n8 a subset a of the nodes of a graph g is ancestral if a contains the parents and neighbours\nof a whenever a \u2208 a. show that if the density of y is recursive with respect to g, the\nmarginal density of ya is recursive with respect to to subgraph induced ona. now consider\nm of the smallest ancestral set containing a \u222a b \u222a d, and suppose that\nthe moral graph g(cid:9)\nd separates a from b in g(cid:9)\n\nm. show that ya \u22a5 yb | yd.\n\n9 are the moral graphs for figures 6.6 and 6.7 triangulated?\n10 write down the directed acylic and moral graphs for x, y , and i under the missing data\nmodels described in section 5.5. use them to give an equation-free explanation of the\ndifferences among the models and of their consequences.\n11 give the form of the a-functions of page 253 when l = 1 and n = 3, and hence verify\nthe hammersley\u2013clifford theorem when (i) none of 1, 2, and 3 are neighbours; (ii) 1 \u223c 2\nonly; (iii) 1 \u223c 2 \u223c 3 only; and (iv) 1 \u223c 2 \u223c 3 \u223c 1, where \u223c means \u2018is a neighbour of\u2019.\nin each case give (6.15).\n\n(cid:8)(cid:31)\n\n(cid:30)\n\n(cid:6)\n\n\u2212 1\n2\n\nf (u; \u03b8) = c(\u03b8) exp\n\n12 suppose that the variables u1, . . . , un of a markov random field have joint density\n\u03b81 > 0, \u03b82 \u2265 0,\n\n(cid:7)\nj\u223c j(cid:9)\nwhere the first sum is over all pairs of neighbours, each pair being taken only once. let\nn j denote the neighbours of node j.\n(a) show that their joint distribution is normal with covariance matrix determined by\n\u03b81 in + \u03b82 a, where the jth diagonal element of the n \u00d7 n adjacency matrix a is the number\nof neighbours of node j, and its off-diagonal elements are\n\n(u j \u2212 u j(cid:9))2 + \u03b82\n\nn(cid:7)\nj=1\n\nu2\nj\n\n\u03b81\n\n,\n\n(cid:25)\u22121,\n\n0,\n\na j j(cid:9) =\n\n(cid:9)\n\nj \u223c j\notherwise.\n\n(cid:20)\n\n\u2212n/2\n\nj (\u03b82 + \u03b81a\n\n(cid:9)\nj ), where the a\n\nthe density is degenerate when \u03b82 = 0, but\n(cid:9)\nj are the eigenvalues of a.\n\n(b) show that\n(2\u03c0)\n(c) suppose that conditional on the values u of unseen variables u , y1, . . . ,y n are indepen-\ndent with y j | u \u223c n (u j , \u03c3 2). show that the joint density of u given y is well-defined\neven if \u03b82 = 0 provided that \u03c3 2 > 0, and that the density of u j | y, u\u2212 j involves on y j\nand un j .\n(d) work out the details using the matrix v in example 6.13.\n\nthat otherwise c(\u03b8) =\n\n6.3 multivariate normal data\n6.3.1 multivariate dependence\nwhen there is a single response variable, analysis is relatively simple, the crucial\naspect being how the distribution of that variable depends on any covariates. problems\nwith just one response variable are common in practice and typically are readily\ninterpretable, but cases arise where relations among two or more responses are to\nbe modelled, and to these we now briefly turn. we motivate our discussion by an\nexample.\n\n "}, {"Page_number": 268, "text": "256\n\n6 \u00b7 stochastic models\n\nmechanics (c)\n\nvectors (c)\n\nalgebra (o)\n\nanalysis (o)\n\nstatistics (o)\n\n77\n63\n75\n55\n63\n...\n15\n5\n12\n5\n0\n\n82\n78\n73\n72\n63\n...\n38\n30\n30\n26\n40\n\n67\n80\n71\n63\n65\n...\n39\n44\n32\n15\n21\n\n67\n70\n66\n70\n70\n...\n28\n36\n35\n20\n9\n\n81\n81\n81\n68\n63\n...\n17\n18\n21\n20\n14\n\ntable 6.7 marks out of\n100 in five mathematics\nexaminations for the first\nand last five of 88 students\n(mardia et al., 1979,\npp. 3\u20134). some of the\nexaminations were\nclosed-book (c), and\nothers were open-book\n(o).\n\nexample 6.17 (maths marks data) table 6.7 gives marks out of 100 for the first\nand last five students out of 88 who took five mathematics examinations. as we would\nanticipate, the top students tend to do best in all the exams, and the worst dismally in\nall, so the marks for each student are related. this is shown by the modified scatterplot\nmatrix in figure 6.8, the below-diagonal part of which contains scatterplots of the\nmarks for each examination against those for every other one, for all 88 students.\neach column of marks has been centred at zero for comparison with the panels above\nthe diagonal, which are discussed in example 6.20. the original marks are shown\nby the light histograms along the diagonal. the average marks in mechanics and in\nvectors were about 40 and 50, but the panel in the second row and first column shows\nthat one candidate got about 40 more marks than this in mechanics and about 30\nmore in vectors; this is the first person in table 6.7. the below-diagonal panels show\ngenerally positive association between marks for different subjects, but its strength\nvaries. for example, algebra is strongly associated with all the other subjects, whereas\nthe first column suggests that while mechanics is related to vectors and algebra, its\nassociation with analysis and statistics is weaker.\n\nwe could rank students by their overall averages, but this would not be useful if\nthe question of interest is how the marks for different exams relate to one another.\nwe would then have five response variables, and we would need to model the forms\n(cid:1)\nof dependence that might arise.\n\nsimpson\u2019s paradox\nthe easiest way to deal with the multitude of potential dependencies in such situations\nis to ignore as many of them as possible. for example, if the response is bivariate,\n(y1, y2), and there is a single explanatory variable, x, one might just model y1 as\na function of x, regardless of y2. unfortunately this can be badly misleading, as\nfigure 6.9 illustrates. its left panel shows how a continuous variable y1 depends on\nx for two values of the discrete variable y2. for each value of y2, the mean of y1,\ne(y1 | y2 = y2, x), increases with x, asshown by the positive slope of the regression\nof y1 on x. however, the right panel shows that when y2 is ignored the mean of y1\n\n "}, {"Page_number": 269, "text": "figure 6.8 modified\nscatterplot matrix for the\nfull maths marks data.\nbelow the diagonal are\nscatterplots (and sample\ncorrelation coefficient) for\nthe centred pairs of marks;\nfor example, the lower left\npanel shows results for\nstatistics plotted against\nthose for mechanics.\nabove the diagonal are\nscatterplots of residuals\n(and sample partial\ncorrelation coefficients):\nfor example, the top right\npanel shows the\ndependence remaining\nbetween mechanics and\nstatistics after adjusting\nfor the other variables.\nthe diagonal shows\nhistograms of the\nvariables (light) and of the\nresiduals from regression\non all other variables,\ncentred at the variable\nmean (dark), with the\nmarginal and partial\nstandard deviations.\n\nfigure 6.9 artificial\ndata illustrating\nsimpson\u2019s paradox. the\nleft panel shows how y1\ndepends on x for each of\ntwo values of y2, with\nobservations with y2 = 0\nshown by blobs and those\nwith y2 = 1 shown as\ncircles. the lines are from\nseparate straight-line\nregression fits of y1 on x\nfor each value of y2 and\nshow positive association.\nthe right panel shows the\nfit to the data ignoring y2,\nfor which the association\nis negative.\n\ne. h. simpson called\nattention to this effect in\n1951, although it was\nknown to g. u. yule\nalmost 50 years earlier.\n\n6.3 \u00b7 multivariate normal data\n\n-20\n\n0\n\n20\n\n-30\n\n-10\n\n10\n\n30\n\n-40\n\n-20\n\n0\n\n10 20\n\n-20\n\n0\n\n20\n\n10.6\n\n6.09\n\n0.43\n\n0.36\n\n17.5\n\n13.8\n\n-40\n\n0.33\n\n5\n1\n0\n\n.\n\n0\n\n0\n0\n\n.\n\n0\n\n40\n\n20\nmechanics\n\n60\n\n80\n\n\u2022\n\n\u2022\n\n0.23\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n0\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n0\n2\n\n0\n\n0\n2\n-\n\n0\n4\n-\n\n0\n3\n\n0\n2\n\n0\n1\n\n0\n1\n-\n\n0\n3\n-\n\n0\n2\n\n0\n1\n\n0\n\n0\n2\n-\n\n0\n4\n-\n0\n4\n\n0\n2\n\n0\n\n0\n2\n-\n\n0.55\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n0.55\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n0.41\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n0.39\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022 \u2022\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n0.28\n\n0.08\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n2\n0\n0\n\n.\n\n0\n0\n\n.\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\n13.1\n\n9.79\n\n0\n\n20\n\n60\n\n40\nvectors\n\n80\n\n0.61\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n4\n0\n0\n\n.\n\n0\n0\n\n.\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\n0.49\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n0.44\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n20\n\n40\nalgebra\n\n60\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022 \u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n0.71\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\u2022\u2022\n\n0.66\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\u2022 \u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n0\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n0.02\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n0.02\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n14.8\n\n10.1\n\n0\n\n20\n\n40\nanalysis\n\n60\n\n80\n\n\u2022\n\n5\n0\n0\n\n.\n\n2\n0\n0\n\n.\n\n0\n0\n\n.\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n0.25\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n0.61\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022 \u2022\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n2\n0\n0\n\n.\n\n0\n0\n\n.\n\n17.3\n\n12.5\n\n0\n\n20\n\n40\n\n60\nstatistics\n\n80\n\n-40\n\n-20\n\n0\n\n20\n\n40\n\n-40\n\n-20\n\n20\n\n-30\n\n-10\n\n10\n\n30\n\n-40\n\n-20\n\n0\n\n10 20\n\n257\n\n40\n\n0\n4\n\n0\n2\n\n0\n\n0\n2\n-\n\n0\n4\n-\n\n0\n2\n\n0\n\n0\n2\n-\n\n0\n4\n-\n\n0\n3\n\n0\n2\n\n0\n1\n\n0\n1\n-\n\n0\n3\n-\n\n0\n2\n\n0\n1\n\n0\n\n0\n2\n-\n\n0\n4\n-\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n5\n\n1\ny\n\n0\n\n5\n-\n\n5\n\n1\ny\n\n0\n\n5\n-\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\n2\n\n4\n\n\u2022\n\n\u2022\n\u2022\n\n0\n\nx\n\n-4\n\n-2\n\n0\n\nx\n\n2\n\n4\n\n-4\n\n-2\n\ndecreases with x, that is, e(y1 | x) has negative slope as a function of x. this effect \u2014\nsimpson\u2019s paradox \u2014 isdue to the fact that marginalization of the joint distribution\nof (y1, y2) over y2 has reversed the sign of the association between y1 and x. here a\nplot at once reveals that it is a bad idea to fit a common line to both groups, but the\n\n "}, {"Page_number": 270, "text": "258\n\n6 \u00b7 stochastic models\n\nage (years)\n\nsmokers\n\nnon-smokers\n\noverall\n\n139/582 (24)\n\n230/732 (31)\n\n18\u201324\n25\u201334\n35\u201344\n45\u201354\n55\u201364\n65\u201374\n75+\n\n2/55 (4)\n3/124 (2)\n14/109 (13)\n27/130 (21)\n51/115 (44)\n29/36 (81)\n13/13 (100)\n\n1/62 (2)\n5/157 (3)\n7/121 (6)\n12/78 (15)\n40/121 (33)\n101/129 (78)\n64/64 (100)\n\ntable 6.8 twenty-year\nsurvival and smoking\nstatus for 1314 women\n(appleton et al., 1996).\nthe smoker and\nnon-smoker columns\ncontain number dead/total\n(% dead).\n\nparadox arises also in contexts where it is not obvious what to plot, and association\nmay be strengthened or weakened as well as reversed.\n\nexample 6.18 (smoking and the grim reaper) table 6.8 shows data on smoking\nand survival for 1314 women in whickham, near newcastle upon tyne in the north\nof england. in 1972\u20131974 data were collected by surveying people on the town\nelectoral register, and among the many variables collected were age and smoking\nhabits at that time. twenty years later a follow-up survey was conducted and it was\ndetermined if each woman in the original survey had yet died. just 162 women\nhad smoked before the first survey but were non-smokers at that time, and there\nwere only 18 whose smoking habits were not recorded; these 180 women have been\nexcluded.\n\nthe first line of the table shows a surprising apparent health bonus of smoking:\nthe death rate for smokers, 24%, is lower than that for non-smokers, 31%. however\na breakdown of the overall figures by age shows that the death rate for smokers is\nhigher for every age category except 25\u201334 and over 75 years. on inspection the\nreason for this discrepancy is obvious: in the early 1970s there were many more non-\nsmokers than smokers in the age range 65\u201374, and as old age makes death even more\nlikely than does smoking, most of these did not survive until the follow-up survey. to\nlearn about the effect of smoking on health, we should compare women of the same\nage. thus the appropriate analysis involves comparisons within age groups, not after\n(cid:1)\nmerging the data across them.\n\nthese cautionary examples show that it is unwise to collapse data by ignoring\nvariables without first examining the full dependence structure to ensure that it is safe\nto do so. data with several responses are common in the social sciences, psychology,\nepidemiology, and public health, where outcomes may depend on numerous variables\nthat are related in a complicated way. the modelling of such data has been extensively\nstudied, and we only scratch its surface here, confining our discussion to the most\nbasic models for continuous data. discrete data such as those in table 6.8 arise in\nmany applications; see chapter 10.\n\n "}, {"Page_number": 271, "text": "6.3 \u00b7 multivariate normal data\n\n259\n\n6.3.2 multivariate normal distribution\nlet y1, . . . ,y n be a random sample from the n p(\u00b5, \u0001) density. that is, y j =\n(y1 j , . . . ,y pj )t has the multivariate normal distribution with mean vector \u00b5 =\n(\u00b51, . . . , \u00b5 p)t and p \u00d7 p nonsingular covariance matrix \u0001, whose (r, s) element\nis \u03c9rs. aseach of the y j has density (3.20), the log likelihood is\n(cid:5)(\u00b5, \u0001) \u2261 \u2212 1\n2\n= \u2212 1\n2\n= \u2212 1\n2\n= \u2212 1\n2\n\nn log|\u0001| + n(cid:7)\nn log|\u0001| + n(cid:7)\nn log|\u0001| +n (y \u2212 \u00b5)t\u0001\u22121(y \u2212 \u00b5) + n(cid:7)\n{n log|\u0001| +n (y \u2212 \u00b5)t\u0001\u22121(y \u2212 \u00b5) + (n \u2212 1)tr(\u0001\u22121s)}\n\n(y j \u2212 y + y \u2212 \u00b5)t\u0001\u22121(y j \u2212 y + y \u2212 \u00b5)\n\n(y j \u2212 y )t\u0001\u22121(y j \u2212 y )\n\n(y j \u2212 \u00b5)t\u0001\u22121(y j \u2212 \u00b5)\n\n(cid:6)\n(cid:6)\n(cid:6)\n\n(6.20)\n\n(cid:8)\n\n(cid:8)\n\n(cid:8)\n\nj=1\n\nj=1\n\nj=1\n\nwhere the p \u00d7 1 vector and p \u00d7 p matrix\n\n(cid:7)\n\n(cid:7)\n\ny = n\n\n\u22121\n\ns = (n \u2212 1)\n\u22121\n\n(y j \u2212 y )(y j \u2212 y )t,\n\ny j ,\n\nare the sample average and covariance matrix. these are minimal sufficient statistics\nfor \u00b5 and \u0001. the second and third equalities before (6.20) rest on the identities\n\n(cid:7)\n(cid:7)\n\n(y j \u2212 y )\u0001\u22121(y \u2212 \u00b5) = 0,\n(cid:7)\n(y j \u2212 y )t\u0001\u22121(y j \u2212 y ) =\n(cid:7)\n=\n(cid:25)\n= tr\n\ntr{(y j \u2212 y )t\u0001\u22121(y j \u2212 y )}\ntr{\u0001\u22121(y j \u2212 y )(y j \u2212 y )t}\n(cid:27)\n(y j \u2212 y )(y j \u2212 y )t\n\u0001\u22121\n\n(cid:7)\n\n.\n\nexercise 6.3.1 gives the\ndetails.\n\nas \u0001 is a positive definite matrix, so is \u0001 = \u0001\u22121, and it follows that for any \u0001, the\nmaximum of (6.20) with respect to \u00b5 is at(cid:1)\u00b5 = y , the maximum likelihood estimator.\nin order to maximize (cid:5)((cid:1)\u00b5, \u0001), we make the 1\u20131 transformation from \u0001 to \u0001, interms\nof which (cid:5)((cid:1)\u00b5, \u0001) = 1\n{n log|\u0001| \u2212(n \u2212 1)tr(\u0001s)}. differentiation with respect to \u0001\nshows that the maximum likelihood estimate of \u0001\u22121 = \u0001 is (cid:1)\u0001 = n\n\u22121(n \u2212 1)s; with\nprobability one this has rank p if n > p. otherwise its rank is n \u2212 1.\nas in the scalar case s is unbiased. we let (cid:1)\u03c9rs denote the (r, s) element of s;\nvariances lie on the diagonal of s. the sample correlations are (cid:1)\u03c9rs /((cid:1)\u03c9rr(cid:1)\u03c9ss)1/2, the\n\u22121/2, where d is the diagonal matrix diag((cid:1)\u03c91,1, . . . ,(cid:1)\u03c9 p, p)\n\nthis is the sample covariance between the rth and sth components of y . the sample\n\n\u22121/2s d\n\n2\n\n(r, s) elements of d\n(exercise 6.3.2).\n\nexample 6.19 (maths marks data) table 6.9 shows the averages, variances, and\ncorrelations for the maths marks data. the best results are on vectors and algebra,\nand the worst on mechanics and statistics. the numbers below the diagonal show\npositive correlations among the variables, with the strongest those between algebra\nand the other subjects. the most variable marks are for mechanics and statistics, with\n\n "}, {"Page_number": 272, "text": "260\n\n6 \u00b7 stochastic models\n\nmechanics\n\nvectors\n\nalgebra\n\nanalysis\n\nstatistics\n\nmechanics\nvectors\nalgebra\nanalysis\nstatistics\n\n17.5/13.8\n\n0.55\n0.55\n0.41\n0.39\n\n0.33\n13.2/9.8\n0.61\n0.49\n0.44\n\n0.23\n0.28\n10.6/6.1\n0.71\n0.66\n\n\u22120.00\n0.08\n0.43\n\n14.8/10.1\n\n0.03\n0.02\n0.36\n0.25\n\n0.61\n\n17.3/12.5\n\naverage\n\n39.0\n\n50.6\n\n50.6\n\n46.7\n\n42.3\n\ntable 6.9 summary\nstatistics for maths marks\ndata. the sample\ncorrelations between\nvariables are below the\ndiagonal, and the sample\npartial correlations are\nabove the diagonal. the\ndiagonal contains sample\nstandard deviation/ sample\npartial standard deviation.\n\nsample standard deviations(cid:1)\u03c91/2\n\nrr of 17.5 and 17.3 respectively, while that for algebra\nis smallest, at 10.6. although the averages for mechanics and statistics are smallest,\nthere is a wider spread of results for these subjects. the values above the diagonal are\n(cid:1)\ndiscussed in example 6.20.\n\nextensions of the arguments for univariate data show that\n\ny \u223c n p(\u00b5, n\n\n\u22121\u0001),\n\n(n \u2212 1)s \u223c wp(n \u2212 1, \u0001),\n\nindependent of\n\n(6.21)\nwhere wp(\u03bd, \u0001) denotes the p-dimensional wishart distribution with p \u00d7 p para-\nmeter matrix \u0001 and \u03bd degrees of freedom. in fact, if z1, . . . , z \u03bd is a random sample\n\u03bd \u223c wp(\u03bd, \u0001); when p = 1\nfrom the n p(0, \u0001) distribution, then z1 z t\nand \u0001 = 1, the wishart distribution reduces to the chi-squared.\n1\n\n+ \u00b7\u00b7\u00b7 + z \u03bd z t\n\nthe multivariate extension of the t statistic is hotelling\u2019s t 2 statistic,\n\nt 2 = n(y \u2212 \u00b5)ts\n\n\u22121(y \u2212 \u00b5) \u223c p(n \u2212 1)\nn \u2212 p\n\nfp,n\u2212 p,\n\nwhich can be used to test hypotheses and form confidence regions for elements of \u00b5.\n\n6.3.3 graphical gaussian models\nthe structure of the multivariate normal density means that variables depend on\neach other in a particularly simple way. before getting into details, we need some\nnotation. let s be a subset of the integers {1, . . . , p}, ofcardinality |s|, and let ys\nand y\u2212s be the sets of variables {ys , s \u2208 s} and {ys , s (cid:7)\u2208 s}. ifs = {r}, wewrite\nys = yr and y\u2212s = y\u2212r . for two such subsets a and b, let \u0001a,b be the |a| \u00d7 |b|\nmatrix with elements \u03c9ab = cov(ya, yb), and let \u0001a|b = cov(ya | yb) be the|a| \u00d7\n|a| conditional covariance matrix of ya given the value of yb; wewrite its elements\nas \u03c9a1,a2|b.\nequation (3.21) establishes that the conditional distribution of ys given y\u2212s = y\u2212s\n\nis normal with mean vector and covariance matrix\n\n\u00b5s + \u0001s,\u2212s \u0001\u22121\u2212s,\u2212s(y\u2212s \u2212 \u00b5\u2212s), \u0001s,s \u2212 \u0001s,\u2212s \u0001\u22121\u2212s,\u2212s \u0001\u2212s,s .\n\n(6.22)\n\nthus the conditional mean depends linearly on the values of the known variables,\nand the conditional variance is independent of them. if s = {r} and the conditional\nvariance of yr , \u03c9rr|\u2212r , ismuch smaller than the unconditional variance \u03c9rr , then\n\n "}, {"Page_number": 273, "text": "6.3 \u00b7 multivariate normal data\n\n261\n\nknowing y\u2212r is highly informative about the distribution of yr . thus it will be useful\nto compare estimates of these variances. it is also useful to learn how knowledge\nof the other variables affects the covariance of yr and ys. their 2 \u00d7 2 conditional\ncovariance matrix is given by (6.22), with s = {r, s}, and their partial correlation,\n\n\u03c1rs|\u2212s =\n\n\u03c9rs|\u2212s\n\n(\u03c9rr|\u2212s \u03c9ss|\u2212s )1/2\n\n,\n\nrepresents the correlation between yr and ys conditional on the remaining variables.\nthe quantities on the right are sometimes called the partial variances and partial\ncovariance. onpage 264 we show that the partial correlation equals minus one\ntimes the (r, s) element of the correlation matrix constructed from \u0001\u22121. thus partial\nvariances, correlations and covariances of y are readily computed from \u0001, and we\n\u03c1rs|\u2212s and so forth by the same functions of (cid:1)\u0001.\ncan use the transformation property of maximum likelihood estimators to estimate\ngive the sample partial standard deviations (cid:1)\u03c91/2\n\nexample 6.20 (maths marks data) the second diagonal elements in table 6.9\nrr|\u2212r for each subject. according to\nthe normal model, our best guess of a student\u2019s mark in algebra without knowledge\nof his other marks would be 50.6, with standard deviation 10.6: a 95% confidence\ninterval is 51 \u00b1 1.96 \u00d7 11 = (29, 73), which is virtually useless. if we knew y and\n(cid:1)\u0001 and his marks y\u2212r for the other four subjects, however, we could replace the\ncomponents of \u00b5 and \u0001 in (6.22) with s = {r} by estimates, giving estimated score\nyr + (cid:1)\u0001r,\u2212r\n=\n\n(cid:1)\u0001\u22121\u2212r,\u2212r (y\u2212r \u2212 y\u2212r ). the estimated conditional standard deviation(cid:1)\u03c91/2\n\nrr|\u2212r\n\n6.1 isappreciably smaller than the unconditional value.\n\nthe above-diagonal part of table 6.9 shows the sample partial correlations. a\ngood mark at algebra is correlated positively with each of the other variables, given\nthe remainder. given the other variables, however, mechanics seems to be unrelated\nto analysis or statistics, and likewise for vectors: the upper right corner of the matrix\nis essentially zero. thus the subjects split into three groups: vectors and mechanics;\nanalysis and statistics; and algebra. variables in the first two pairs are partially cor-\nrelated with each other and with algebra, which itself is partially correlated with all\nfour other variables.\nthis information is displayed more fully in the above-diagonal panels of figure 6.8.\nset s = {r, s}, and let y denote the n \u00d7 p data matrix whose jth row is yt\nj , yr the\nrth column of y, and y\u2212s the n \u00d7 ( p \u2212 2) array comprising all columns of y but the\nrth and sth. then the vertical axes show the n \u00d7 1 vectors of sample values\n\nyr|\u2212s = yr \u2212 yr \u2212(cid:1)\u0001r,\u2212s(cid:1)\u0001\u22121\u2212s,\u2212s(y\u2212s \u2212 y\u2212s)\n\nof the scalar random variable\n\nyr|\u2212s = yr \u2212 \u00b5r \u2212 \u0001r,\u2212s \u0001\u22121\u2212s,\u2212s(y\u2212s \u2212 \u00b5\u2212s),\n\nwhile the horizontal axes show the ys|\u2212s\u2019s. the quantities yr|\u2212s are normal with\nmeans zero and variances \u0001r,\u2212s \u0001\u22121\u2212s,\u2212s \u0001\u2212s,r , and partial correlation\n\ncorr(yr , ys | y\u2212s) = corr(yr|\u2212s , ys|\u2212s) = \u03c1rs|\u2212s ,\n\n "}, {"Page_number": 274, "text": "262\n\n6 \u00b7 stochastic models\nsample quantity(cid:1)\u03c1rs|\u2212s. thus the scatterplot in the first row and third column shows\n\nwhile the correlation coefficient between the sample versions is the corresponding\n\nthe association between mechanics on the vertical axis and algebra on the horizontal\naxis after adjusting for dependence on the other variables. the partial correlation\nof 0.23 shows that some positive correlation remains after allowing for the other\nvariables. summary in terms of partial correlations seems reasonable, as none of the\npanels shows much nonlinearity, but there is a possible outlier in the lower left corner\n= (3, 9, 51, 47, 40) are\nof panels (1, 2) and (2, 1). this is a person whose marks yt\n81\ndire for applied mathematics but not for pure mathematics or statistics. dropping him\nmakes little change to the correlations or partial correlations.\nthe diagonal of the scatterplot matrix compares histograms of the raw marks yr\nand the marks yr|\u2212r + yr after adjusting for all the other variables, with the sample\n(cid:1)\nstandard deviations of these vectors.\n\nconditional independence graphs\nas their third and higher-order joint cumulants are identically zero (section 3.2.3),\ndependence among normal variables is expressed through their correlations, calcu-\nlated from \u0001, orequivalently their partial correlations, calculated from \u0001\u22121. consider\nthe graph with p nodes corresponding to the variables y1, . . . ,y p. now yr and ys are\nindependent conditional on all the other variables if and only if their partial correla-\ntion is zero, and we encode this by the absence of an edge between the corresponding\nnodes. thus two nodes are neighbours \u2014 joined by an edge \u2014 if and only if the cor-\nresponding partial correlation is non-zero and hence if and only if the corresponding\nelement of \u0001 is non-zero. this yields a conditional independence graph for y1, . . . ,y p\n(section 6.2.2).\nif the density of y1, . . . ,y p is non-degenerate, then the global markov property\nholds. to see this, leta,b, andd be any disjoint nonempty subsets ofj = {1, . . . , p}\nsuch that d separates a from b and a \u222a b \u222a d = j . asthere are no edges between\na and b, the density of y has exponent\n\n\u2212 1\n2\n\n(y \u2212 \u00b5)t\u0001\u22121(y \u2212 \u00b5) = \u2212 1\n2\n\n(y \u2212 \u00b5)t\n\n\uf8eb\n\uf8ed \u0001aa \u0001ad\n\u0001da \u0001dd \u0001db\n\u0001bd \u0001bb\n\n0\n\n0\n\n\uf8f6\n\uf8f8 (y \u2212 \u00b5),\n\nwith quadratic term in ya and yb identically zero. hence\n\nf (y) = f (ya, yb, yd) = g1(ya, yd)g2(yb, yd),\n\nfor some positive functions g1 and g2, implying that ya and yb are conditionally\nindependent given yd; ofcourse this property is inherited by any subsets of ya and\nyb. as any disjoint subsets of j separated by d can be augmented to give sets a, b\nwhich are separated by d and which together with d partition j , the global markov\nproperty holds.\n\nin graphical terms it is natural to restrict the degree of dependence among\ncomponents of y by deleting edges from its graph, and this means setting ele-\nments of \u0001\u22121 to zero. suppose that the inverse covariance matrix resulting from\n\n "}, {"Page_number": 275, "text": "0\n\n263\n\n6.3 \u00b7 multivariate normal data\n= \u00010, for which the profile log likelihood is (cid:5)((cid:1)\u00b5, \u00010) \u2261\nsuch deletions is \u0001\u22121\n(cid:1)\u0001\u22121)}. for an idea of the difficulties involved in max-\n{n log|\u00010| \u2212(n \u2212 1)tr(\u00010\n1\n2\nimizing this with respect to the non-zero elements of \u00010, weconsider the simplest\nnon-trivial case, with p = 3 variables and \u03b432 = 0, implying that y2 and y3 are inde-\npendent given y1. inthis case the log likelihood may be written down and differentiated\nof (cid:1)\u00010. welay these equations out as\ndirectly, giving five simultaneous equations to be solved for the non-zero components\n\uf8f6\n\uf8f8 ,\n|(cid:1)\u00010|\n(cid:1)\u03b411\n(cid:1)\u03b422 \u2212(cid:1)\u03b42\nwhere there is a missing equation ?=? corresponding to \u03b432, which does not appear in\nthe likelihood. the structure of these equations shows that in general we must solve\na system of polynomial equations of degree p, and the properties of the graph of \u00010\nplay a crucial role in determining the character of the solution. here it turns out that\n\nif the missing equation is replaced by(cid:1)\u03b421\n(cid:1)\u03b431/|(cid:1)\u00010| =(n \u2212 1)(cid:1)\u03c921(cid:1)\u03c931/(n(cid:1)\u03c911) and the\nmatrices are completed by symmetry, the (cid:1)\u03b4rs can be found explicitly in terms of\nthe(cid:1)\u03c9rs.\n\n\uf8eb\n\uf8ed(cid:1)\u03c911(cid:1)\u03c921 (cid:1)\u03c922\n(cid:1)\u03c931\n\n\uf8f6\n\uf8f8 = n \u2212 1\n\n(cid:1)\u03b433\n(cid:1)\u03b422\n\u2212(cid:1)\u03b421\n(cid:1)\u03b433\n\u2212(cid:1)\u03b431\n(cid:1)\u03b422\n\n(cid:1)\u03b433 \u2212(cid:1)\u03b42\n(cid:1)\u03b411\n\n? (cid:1)\u03c933\n\n\uf8eb\n\uf8ed\n\n1\n\nn\n\n21\n\n31\n\n?\n\ncomparisons between two nested graphical models may be based on likelihood\nratio statistics, though large-sample asymptotics can be unreliable. exact comparison\nof the full model with the one with a single edge missing may be based on the\ncorresponding partial correlation coefficient (exercise 6.3.6).\n\nexample 6.21 (maths marks data) the above-diagonal part of table 6.9 suggests\na graphical model in which the upper right 2 \u00d7 2 corner of \u0001 is set equal to zero.\nthe likelihood ratio statistic for comparison of this model with the full model is\n0.90, which is not large relative to the \u03c7 2\n4 distribution. this suggests strongly that the\nsimpler model fits as well as the full one, an impression confirmed by comparing the\noriginal and fitted partial correlations,\n\n0.33\n\n0.23 \u22120.00\n0.08\n0.28\n0.43\n\n0.03\n0.02\n0.36\n0.25\n\n0.33\n\n0.24\n0.33\n\n0.00\n0.00\n0.45\n\n0.00\n0.00\n0.37\n0.26\n\nfigure 6.10 shows the graphs for these two models. in the full model every variable\nis joined to every other, and there is no simple interpretation. the reduced model has\na butterfly-like graph whose interpretion is that given the result for algebra, results\nfor mechanics and vectors are independent of those for analysis and statistics. thus a\nresult for mechanics can be predicted from those for algebra and vectors alone, while\n(cid:1)\nprediction for algebra requires all four other results.\n\nthe graphs described above have the drawback of taking no account of the logical\nstatus of the variables. for example, it may be known that y1 influences y2 but not\nvice versa, but this is not reflected in an undirected graph. in applications, therefore, it\nis useful to have different types of edges, with directed edges representing supposed\ncausal effects and undirected edges linking variables that are to be put on an equal\n\n "}, {"Page_number": 276, "text": "264\n\n6 \u00b7 stochastic models\n\nstatistics\n\nvectors\n\nstatistics\n\nvectors\n\nalgebra\n\nalgebra\n\nanalysis\n\nmechanics\n\nanalysis\n\nmechanics\n\nfigure 6.10 graphs for\nthe full model (left) and a\nreduced model (right) for\nthe maths marks data. the\ninterpretation of the\nreduced model is that\ngiven the result for\nalgebra, results for vectors\nand mechanics are\nindependent of those for\nanalysis and statistics.\n\nfooting. this important topic is beyond the scope of this book; see the bibliographic\nnotes.\n\ncalculation of partial correlation\nlet s = {r, s}, where without loss of generality r < s. then the conditional variance\nmatrix for yr and ys given y\u2212s is \u0001s,s \u2212 \u0001s,\u2212s \u0001\u22121\u2212s,\u2212s \u0001\u2212s,s, and hence their partial\ncorrelation is\n\nthis may be skipped on a\nfirst reading.\n\n\u03c1rs|\u2212s =\n\n(cid:28)(cid:2)\n\n\u03c9rr \u2212 \u0001r,\u2212s \u0001\u22121\u2212s,\u2212s \u0001\u2212s,r\n\n\u03c9rs \u2212 \u0001r,\u2212s \u0001\u22121\u2212s,\u2212s \u0001\u2212s,s\n\n(cid:3)(cid:2)\n\u03c9ss \u2212 \u0001s,\u2212s \u0001\u22121\u2212s,\u2212s \u0001\u2212s,s\n\n(cid:3)(cid:29)1/2\n\n.\n\nthe (r, s) element of \u0001\u22121 is (\u22121)r+s \u0001rs /|\u0001|, where \u0001rs\nis the (r, s) mi-\nnor of \u0001. thus the (r, s) element of the \u2018correlationized\u2019 version of \u0001\u22121 is\n(\u22121)r+s \u0001rs /(\u0001rr \u0001ss)1/2. toshow how this is related to \u03c1rs|\u2212s, weuse the formula\n\n!!!! a11 a12\n\na21 a22\n\n!!!! = |a11 \u2212 a12 a\n\n22 a21| \u00b7 |a22|\n\u22121\n\n(6.23)\n\n!!!! \u03c9ss\n\n\u0001s,\u2212s\n\u0001\u2212s,s \u0001\u2212s,\u2212s\n\n!!!! = (cid:2)\n!!!! = (\u22121)r+s\u22121\n\nfor the determinant of a partitioned matrix for which a\nand column interchanges that bring \u03c9ss to the (1, 1) position of \u0001\u2212r,\u2212r , wesee that\n\n\u22121\n22 exists. on making the row\n\n\u0001rr = (\u22121)2(s\u22121)\n\n\u03c9ss \u2212 \u0001s,\u2212s \u0001\u22121\u2212s,\u2212s \u0001\u2212s,s\n\n(cid:3)|\u0001\u2212s,\u2212s|,\n\n!!!! \u03c9sr\n\n\u0001s,\u2212s\n\u0001\u2212s,r \u0001\u2212s,\u2212s\n\nwith a similar expression for \u0001ss, while \u0001rs equals\n(cid:3)|\u0001\u2212s,\u2212s|,\n(\u22121)r+(s\u22121)\nas \u03c9rs = \u03c9sr by symmetry of \u0001. onsubstituting the expressions for \u0001rr , \u0001ss, and \u0001rs\ninto (\u22121)r+s \u0001rs /(\u0001rr \u0001ss)1/2, wesee that the ( r, s) element of the \u2018correlationalized\u2019\nversion of \u0001\u22121 equals \u2212\u03c1rs|\u2212s, as was tobe proved.\n\n\u03c9rs \u2212 \u0001s,\u2212s \u0001\u22121\u2212s,\u2212s \u0001\u2212s,r\n\n(cid:2)\n\n "}, {"Page_number": 277, "text": "6.3 \u00b7 multivariate normal data\n\n265\n\nexercises 6.3\n1\n\nif a is a p \u00d7 p matrix, all of whose elements are distinct and if ai j denotes the cofactor\nof the (i, j) element ai j of a, then \u2202|a|/\u2202ai j = ai j , whereas if a is symmetric, then\n\n(cid:21)\n\n\u2202|a|\n\u2202ai j\n\n=\n\naii ,\n2ai j ,\n\ni = j,\ni (cid:7)= j.\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nif a and b have dimensions p \u00d7 q and q \u00d7 p, then\n\n(cid:21)\n\n\u2202tr(ab)\n\n=\n\n\u2202 a\n\nbt,\nb + b t \u2212 diag(b), a symmetric.\n\nall elements of a distinct,\n\n\u22121\n\n(cid:5)\n\n\u22121(n \u2212 1)s solves the likelihood equations for \u0001 for\nuse these identities to verify that n\nthe multivariate normal model on page 259. check that this maximizes the likelihood\nwhen p = 2.\nshow that the (r, s) element of (cid:1)\u0001 is(cid:1)\u03c9rs = (n \u2212 1)\nj (yr j \u2212 yr )(ys j \u2212 ys), where yr is\nthe rth element of the p \u00d7 1 vector y, and that although(cid:1)\u03c9rs is not the maximum likelihood\nequals(cid:1)\u03c9rs /((cid:1)\u03c9rr(cid:1)\u03c9ss)1/2.\nestimate of \u03c9rs, the maximum likelihood estimate of the correlation between yr and ys\nlet \u0001 be the variance matrix of a p-dimensional normal variable y . use cramer\u2019s rule to\nshow that the rth diagonal element of \u0001\u22121 is var(yr | y\u2212r ).\nlet y t = (y1, . . . ,y 3) be amultivariate normal variable with\n\uf8f6\n\uf8f8 .\n\n1\n2\n\u22121/2\n1\nfind \u0001\u22121 and hence write down the moral graph for y .\nif m \u2192 \u221e, show that the distribution of y becomes degenerate while that of (y1, y3)\ngiven y2 remains unchanged. is the graph an adequate summary of the joint limiting\ndistribution? is the markov property stable in the limit?\nsuppose that w1, . . . , wn may be written w j = \u00b5 + \u03c3 z j + \u03c4 x, where z1, . . . , zn and\nx are independent standard normal variables. obtain the correlation matrix \u0001 of y t =\n(x, w1, . . . , wn), write down the moral graph for y , and hence obtain \u0001\u22121.\nlet y1, . . . , yn be a n p(\u00b5, \u0001) random sample and let \u0001 = \u0001\u22121 have elements \u03b4rs. show\n2 n log|(cid:1)\u0001|,\nthat apart from constants, the value of (6.20) maximized over both \u00b5 and \u0001 is \u2212 1\nmodel obtained by constraining elements of \u0001 (or \u0001) may be written n log|(cid:1)\u0001\u22121(cid:1)\u00010|, in\nand deduce that the likelihood ratio statistic for comparison of the full model and a sub-\n\n\uf8eb\n\uf8ed 1\n\u22121/2\n1\n2\n\n\u22121/2\n2\nm\n\u22121/2\n\n\u0001 =\n\nm\n\nm\n\nm\n\nm\n\nan obvious notation.\n(a) show that the likelihood ratio statistic for testing if all the components of y are\nindependent is a function of the determinant of the sample correlation matrix.\n(b) use (6.23) to show that the likelihood ratio statistic to test if \u03b412 = 0 may be written\n\u2212n log(1 \u2212(cid:1)\u03c12\n1,2|\u2212s), where s = {1, 2}, and check for what values of the partial correlation\n(cid:1)\u03c112|\u2212s this is large.\nin the discussion on page 263, verify that if \u03b432 = 0, then the likelihood equations are\nequivalent to\n\n(cid:22)(cid:1)\u03c911(cid:1)\u03c921\n= n \u2212 1\n(cid:1)\u03c931 (cid:1)\u03c921(cid:1)\u03c931/(cid:1)\u03c911 (cid:1)\u03c933\nand hence find (cid:1)\u00010 in terms of the(cid:1)\u03c9rs.\nfind also the maximum likelihood estimate of (cid:1)\u00010 when \u03b431 = \u03b432 = 0 and when \u03b431 =\n\u03b432 = \u03b421 = 0.\ngive the graphs corresponding to each of these models.\n\n(cid:1)\u0001\u22121\n\n(cid:1)\u03c922\n\n(cid:23)\n\nn\n\n,\n\n0\n\n "}, {"Page_number": 278, "text": "266\n\n)\n\nc\n\n(\n \ne\nr\nu\nt\na\nr\ne\np\nm\ne\nt\n \ny\nd\no\nb\n\n5\n.\n8\n3\n\n0\n.\n8\n3\n\n5\n.\n7\n3\n\n0\n.\n7\n3\n\n5\n.\n6\n3\n\n6 \u00b7 stochastic models\n\n\u25e6\n\nfigure 6.11 example\ntime series. left: body\ntemperatures (\nc) of a\nfemale canadian beaver\nmeasured at 10-minute\nintervals (reynolds,\n1994). the vertical line\nmarks where she left her\nlodge. right: ftse\nclosing prices,\n1991\u20131998.\n\n0\n0\n0\n6\n\n0\n0\n0\n5\n\n0\n0\n0\n4\n\n0\n0\n0\n3\n\ne\ns\nt\nf\n\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n\n1992\n\n1994\n\n1996\n\n1998\n\ntime (10-minute intervals)\n\ntime (trading days)\n\n6.4 time series\n\na time series consists of data recorded in time order. examples are monthly inflation\nrate, weekly demand for electricity, daily maximum temperature, number of packets\nof information sent per second over a communication network, and so forth. the\nmeasurements may be instantaneous, such as the daily closing prices of some stock,\nor may be an average, such as annual temperature averaged over the surface of the\nglobe. typically such data show variation on several scales. data on internet traffic, for\nexample, show strong diurnal variation as well as long-term upward trend. time series\nare ubiquitous and their analysis is well-developed, with many techniques specific to\nparticular areas of application. in many cases the goal of time series modelling is the\nforecasting of future values, while in others the intention is to control the underlying\nprocess. here we simply introduce a few basic notions in the most common situation,\nwhere the observations are continuous and arise at regular intervals. irregular and\ndiscrete time series also occur \u2014 see example 6.2 \u2014 but their modelling is less well\nexplored.\n\nexample 6.22 (beaver body temperature data) the left panel of figure 6.11\nshows 100 consecutive telemetric measurements on the body temperature of a female\ncanadian beaver, castor canadensis, taken at 10-minute intervals. the animal re-\nmains in its lodge for the first 38 recordings and then moves outside, at which point\nthere is a sustained temperature rise. this is likely to be of main interest in such an\napplication, with the dependence structure of the series regarded as secondary. the de-\npendence must be accounted for, however, if confidence intervals for the rise are to be\n(cid:1)\nreliable.\n\nexample 6.23 (ftse data) the right panel of figure 6.11 shows the closing prices\nof the financial times stock exchange index of london closing prices from 1991\u2013\n1998. prices are available only for days on which the exchange was open so there\nare many fewer than 365 observations per year. the dominant feature is the strong\nupward trend. here interest would typically focus on short-term forecasting, though\n\n "}, {"Page_number": 279, "text": "{yt} is also called\ncovariance stationary,\nweakly stationary, or\nstationary in the wide\nsense.\n\n6.4 \u00b7 time series\n\n267\n\nportfolio managers will also wish to understand the relationship between this and\n(cid:1)\nother markets. in either case the dependence structure is of crucial importance.\n\nstationarity and autocorrelation\nstatistical inference cannot proceed without some assumption of stochastic regularity,\nand in time series this is provided by the notion of stationarity.\nconsider data y1, . . . , yn, supposed to be a realization of the random variables\ny1, . . . ,y n, themselves forming a contiguous stretch of a stochastic process {yt} =\n{. . . , y\u22121, y0, y1, . . .}. then {yt} is said to be second-order stationary if its first and\nsecond moments are finite and time-independent, so that the mean e(ys) = \u00b5 is\nconstant and the covariances cov(ys , ys+t ) = \u03b3t do not depend on s. finiteness of\n\u03b30 = var(yt ) guarantees that |\u00b5|,|\u03b3t| < \u221e for all t. the first and second moments\nof a second-order stationary series do not depend on the point at which they are\ncalculated. neither panel of figure 6.11 looks stationary, though it is plausible that\nthe temperature data to the right of the vertical line are.\n\na series is said to be strictly stationary if the joint distribution of any finite subset\nya does not depend on the origin; thus the distributions of ys+a and of ya are the\nsame for any s. this is a stronger condition than second-order stationarity, because\nit constrains the entire distribution of the series. in particular it implies that the joint\ncumulants of ys+a are independent of s, ifthey exist. evidently strict stationarity\nyields more powerful theoretical results, but as it is impossible to verify from data,\nthey are less useful in practice. the definitions coincide if {yt} has a multivariate\nnormal distribution, as this is determined by its first and second moments. the term\nstationary used without qualification in this section means second-order stationary.\nthe second-order structure of a stationary process is summarized in its autocorre-\nlation function \u03c1t = corr(y0, yt ), t = \u00b11,\u00b12, . . ., where \u03c1t = \u03b3t /\u03b30; \u03b30 = var(y0)\nis the marginal variance of the process {yt}. note that\n\n\u03c1\u2212t = corr(ys , ys\u2212t ) = corr(ys+t , ys) = \u03c1t\n\n=\nby stationarity. a related function is the partial autocorrelation function \u03c1(cid:9)\ncorr(y0, yt | y1, . . . ,y t\u22121), which summarizes any correlation between observations\nt lags apart after conditioning on the intervening data; see section 6.3.3.\na white noise process {\u03b5t} is an uncorrelated sample from some distribution with\n\u2261 0. we shall use the term normal\nmean zero and variance \u03c3 2; evidently it has \u03c1t = \u03c1(cid:9)\nwhite noise when \u03b5t\nplots of estimated \u03c1t and \u03c1(cid:9)\n\nt against positive values of t are called the correlo-\ngram and partial correlogram. under mild conditions their ordinates are asymptotic\n\u22121) variables for a white noise series of length n, from which\nindependent n (0, n\nsignificance can be assessed; see figure 6.12.\n\niid\u223c n (0, \u03c3 2).\n\nt\n\nt\n\nexample 6.24 (autoregressive process) about the simplest time series model is\nthe autoregressive process of order one, or ar(1) model\n\nyt \u2212 \u00b5 = \u03b1(yt\u22121 \u2212 \u00b5) + \u03b5t ,\n\nt = . . . ,\u22121, 0, 1, . . . ,\n\n(6.24)\n\n "}, {"Page_number": 280, "text": "268\n\n6 \u00b7 stochastic models\n\nwhere the innovation series {\u03b5t} is normal white noise and \u03b5t\nis independent\nof . . . , yt\u22122, yt\u22121. taking variances in (6.24) yields \u03b30 = \u03b12\u03b30 + \u03c3 2. hence \u03b30 =\n\u03c3 2/(1 \u2212 \u03b12), so a necessary condition for stationarity is |\u03b1| < 1. this condition is\nalso sufficient, and if it is satisfied then e(yt ) = \u00b5 and \u03c1t = \u03b1\u2212|t|\n(exercise 6.4.1).\nthis is a markov process, because yt depends on the previous observations only\n= \u03b1. ifthe \u03b5t\nthrough yt\u22121, and hence the only non-zero partial autocorrelation is \u03c1(cid:9)\nare normal, then yt is a linear combination of normal variables and so y1, . . . ,y n are\njointly normal with mean vector \u00b51n and covariance matrix\n\n1\n\none can verify directly that \u0001\u22121 is the tridiagonal matrix (example 6.13)\n\n\uf8eb\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n\u03b1\n1\n\u03b1\n\n\u03b12\n1\n\u03b1\n\u03b1\n\u03b12\n1\n...\n...\n\u03b1n\u22121 \u03b1n\u22122 \u03b1n\u22123\n\n...\n\n\uf8f6\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 .\n\n\u00b7\u00b7\u00b7 \u03b1n\u22121\n\u00b7\u00b7\u00b7 \u03b1n\u22122\n\u00b7\u00b7\u00b7 \u03b1n\u22123\n...\n...\n\u00b7\u00b7\u00b7\n1\n\n0\n\u2212\u03b1\n1 + \u03b12\n\n...\n0\n0\n\n\u00b7\u00b7\u00b7\n0\n\u00b7\u00b7\u00b7\n0\n\u00b7\u00b7\u00b7\n0\n...\n...\n\u00b7\u00b7\u00b7 1 + \u03b12 \u2212\u03b1\n\u00b7\u00b7\u00b7\n1\n\n0\n0\n0\n...\n\u2212\u03b1\n\n\uf8f6\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 .\n\n\u0001 = \u03c3 2\n1 \u2212 \u03b12\n\n\uf8eb\n\n\u2212\u03b1\n1\n\u2212\u03b1 1 + \u03b12\n\u2212\u03b1\n0\n...\n...\n0\n0\n0\n0\n\n\u03c3 \u22122\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nyt \u2212 \u00b5 = p(cid:7)\n\nj=1\n\nthe autoregressive process of order p or ar( p) model satisfies\n\n\u03b1 j (yt\u2212 j \u2212 \u00b5) + \u03b5t ,\n\nt = . . . ,\u22121, 0, 1, . . . ,\n\nand is therefore a markov process of order p. constraints on \u03b11, . . . , \u03b1 p are needed\nfor this process to be stationary, but if they are satisfied, there is a sharp cut-off in the\n= 0 when t > p. this should be reflected in the partial\npartial autocorrelations: \u03c1(cid:9)\n(cid:1)\ncorrelogram of ar( p) data. the constraints are discussed after example 6.26.\n\nt\n\nexample 6.25 (beaver body temperature data) figure 6.12 shows the correlo-\ngram and partial correlogram for the apparently stationary observations 39\u2013100 of\nthe beaver temperature data. the correlogram shows positive correlations at lags 1\u20133.\nany further evidence of structure must be treated very cautiously, as the values around\nlag 15 are not very significant, and as each panel of the figure shows 20 correlations es-\ntimated from only 62 observations. the partial correlogram is suggestive of an ar(1)\n.= 0.75, consistent with the geometric decrease in the correlogram at\nmodel with \u03b1\nshort lags.\n\nthe change in level evident in figure 6.11 suggests that we take\n\nyt =\n\n\u03b20 + \u03b7t ,\n\u03b20 + \u03b21 + \u03b7t ,\n\nt = 1, . . . ,38,\nt = 39, . . . ,100,\n\n(6.25)\nwhile the partial correlogram suggests that the \u03b7t follow (6.24) with \u00b5 = 0. this\nyields a markov model with parameters (\u03b20, \u03b21, \u03b1, \u03c3 2). if we assume normal white\n\n(cid:21)\n\n "}, {"Page_number": 281, "text": "6.4 \u00b7 time series\n\n269\n\nfigure 6.12\ncorrelogram and partial\ncorrelogram for\nobservations 39\u2013100 of\nthe beaver body\ntemperature data. the\ndotted horizontal lines at\n\u00b12n\n\u22121/2 show 95%\nconfidence bounds for the\ncorrelation coefficients, if\nthe data are white noise.\nstrong systematic\ndepartures from these are\nsuggestive of structure in\nthe data.\n\nm\na\nr\ng\no\ne\nr\nr\no\nc\n\nl\n\n0\n1\n\n.\n\n5\n\n.\n\n0\n\n0\n\n.\n\n0\n\n.\n\n5\n0\n-\n\n0\n\n.\n1\n-\n\nm\na\nr\ng\no\ne\nr\nr\no\nc\n \nl\n\nl\n\na\n\ni\nt\nr\na\np\n\n0\n1\n\n.\n\n5\n\n.\n\n0\n\n0\n\n.\n\n0\n\n.\n\n5\n0\n-\n\n0\n\n.\n1\n-\n\n0\n\n5\n\n10\n\nlag\n\n15\n\n20\n\n0\n\n5\n\n15\n\n20\n\n10\n\nlag\n\nnoise and initial n{\u03b20, \u03c3 2/(1 \u2212 \u03b12)} distribution for y1 then the log likelihood is\nreadily obtained from (4.8); see exercise 6.4.3. the log likelihood can be maximized\nmatrix, giving (cid:1)\u03b20 = 37.19 (0.119), (cid:1)\u03b21 = 0.61 (0.138), (cid:1)\u03b1 = 0.87 (0.068), and\nnumerically and standard errors obtained from the inverse observed information\n(cid:1)\u03c3 2 = 0.015 (0.002). body temperature rises by about 0.6\nas independent gives standard error 0.044 for (cid:1)\u03b21, sothe autocorrelation greatly\ninactive period we define residuals rt = {yt \u2212(cid:1)\u03b20 \u2212(cid:1)\u03b1(yt\u22121 \u2212(cid:1)\u03b20)}/(cid:1)\u03c3 , with a similar\n\nc when the beaver is\nactive, and successive measurements are quite highly correlated. treating the data\n\nresiduals can be constructed by estimating the scaled innovations \u03b5t /\u03c3 . inthe\n\nincreases the uncertainty for \u03b21.\n\n\u25e6\n\nexpression in the active period. then the correlogram, partial correlogram, and prob-\nability plots of r2, . . . , r100 help assess model adequacy. judged by these criteria, the\nmodel seems to fit well, though (6.25) does not account for the gradual rise in body\n(cid:1)\ntemperature before the beaver left the lodge.\n\nyt \u2212 \u00b5 = q(cid:7)\n\nexample 6.26 (moving average process) a moving average process of order q or\nma(q) model satisfies the equation\n\nj=1\n\n\u03b2 j \u03b5t\u2212 j + \u03b5t ,\n\nt = . . . ,\u22121, 0, 1, . . .\nwhere {\u03b5t} is white noise. here e(yt ) = \u00b5 and var(yt ) = \u03c3 2(1 + \u03b22\nq ) for\nall t, and it is easy to check that this process is stationary and that \u03c1t = 0 for t > q\n(exercise 6.4.2). thus the correlogram of such data should show a sharp cut-off after\n(cid:1)\nlag q.\n\n+ \u00b7\u00b7\u00b7 + \u03b22\n\n1\n\nstationary autoregressive and moving average processes are linear processes, as\nthe current observation yt may be expressed as an infinite moving average of the\ninnovations,\nyt =\n\nt = . . . ,\u22121, 0, 1, . . . , with\n\n|c j| < \u221e.\n\nc j \u03b5t\u2212 j ,\n\n(6.26)\n\n\u221e(cid:7)\nj=0\n\n\u221e(cid:7)\nj=0\n\n "}, {"Page_number": 282, "text": "270\n\n6 \u00b7 stochastic models\n\nthis expresses the current yt in terms of past innovations, provides useful models in\n<\n\nmany applications, and leads to simple computations. for example, var(yt ) = (cid:5)\n\u221e and \u03b3t = (cid:5)\n\nc j c j+t .\n\nc2\nj\n\nc2\ni\n\nevidently an ma(q) model with zero mean has a representation (6.26). to see\nwhen this is true for an ar( p) model, it is useful to introduce the backshift\noperator b such that byt = yt\u22121 and bdyt = yt\u2212d, with b0 = i the identity\nnomial a(z) = 1 \u2212(cid:5) p\noperator. then an ar( p) process is expressible as a(b)yt = \u03b5t , where the poly-\n\u22121\u03b5t = (cid:5)\u221e\n\u03b1 j z j corresponds to the autoregression, and we can for-\nj=1\n(cid:5)\n< \u221e. now a(z) = (cid:20) p\nmally write yt = a(b)\ni=0 ci \u03b5t\u2212i , say, which is stationary if and only if\nj=1(1 \u2212 a j z), where a\nare the possibly complex roots\n(cid:5) p\n\u22121 may be written us-\nj=1 b j /(1 \u2212 a j z) for some b j . if we take z sufficiently\n\u22121 can be expressed as a sum of geometric series with coefficients\n(cid:5)\nj , giving the infinite moving average (6.26). for this to be sta-\n< \u221e, which occurs if and only if |a j| < 1 for each\ntionary we must have\nj, orequivalently all the roots of a(z) lie outside the unit disk in the complex\nplane. thus properties of the polynomial a(z) are intimately related to those of the\nprocess {yt}.\n\nof a(z), and provided that no two of the a j are equal, a(z)\ning partial fractions as\nsmall then a(z)\nj=1 b j ai\n\nci = (cid:5) p\n\n\u22121\nj\n\nc2\ni\n\nexample 6.27 (arma process) the autoregressive process is formed as a linear\ncombination of previous observations, while a moving average process is based on\na weighted combination of the innovations at previous steps. an obvious general-\nization is to combine the two, giving the autoregressive moving average process or\narma( p, q) model\n\nyt \u2212 \u00b5 = p(cid:7)\n\n\u03b1 j (yt\u2212 j \u2212 \u00b5) + q(cid:7)\n\nj=1\n\ni=1\n\n\u03b2i \u03b5t\u2212i + \u03b5t ,\n\nt = . . . ,\u22121, 0, 1, . . . .\n\nj=1\n\n\u22121b(b)\u03b5t = (cid:5)\u221e\n\nas in the preceding examples, the yt will have a joint normal distribution if the process\nis stationary and the \u03b5t represent normal white noise. let \u00b5 = 0 for simplicity.\na(z) = 1 \u2212(cid:5) p\n\u03b1 j z j and b(z) = 1 +(cid:5)q\nin terms of the backshift operator we have a(b)yt = b(b)\u03b5t , where the polynomials\n\u03b2i zi represent the autoregressive and\ni=1\nmoving average components. thus yt = a(b)\nj=\u2212\u221e c j \u03b5t\u2212 j , where the\n\u22121b(z). once again, properties of\ncoefficients c j are those of the infinite series a(z)\nthese polynomials determine those of {yt}.\nthe class of arma processes is typically regarded as a useful \u2018black box\u2019 for\nfitting and forecasting, though fitted models sometimes have a substantive interpre-\ntation. for instance, the values of aic when (6.25) is fitted to the beaver data and\nthe \u03b7t follow an arma( p, q) process with ( p, q) equal to (1, 1), (0, 1), (1, 2), and\n(2, 0) are \u2212128.34, \u221290.06, \u2212126.54, and \u2212128.78, compared with \u2212127.55 for the\nar(1) model, which therefore seems a good compromise between quality of fit and\nsimplicity of interpretation, the latter following from its markov structure. it is con-\nsiderably harder to explain the arma(1,2) model in simple terms, despite its slightly\n(cid:1)\nbetter fit.\n\n "}, {"Page_number": 283, "text": "6.4 \u00b7 time series\n\n271\n\ntrend removal\nin practice data are rarely stationary, and trends or periodic changes must be removed\nbefore fitting standard models. one simple approach to removing polynomial trends\nis differencing. suppose that yt = \u03b30 + \u03b31t + \u03b5t , sothere is linear trend with possibly\ncorrelated noise superimposed. then\n\nxt = yt \u2212 yt\u22121 = (\u03b30 + \u03b31t + \u03b5t ) \u2212 {\u03b30 + \u03b31(t \u2212 1) + \u03b5t\u22121} = \u03b31 + \u03b7t ,\n\nsay, where \u03b7t = \u03b5t \u2212 \u03b5t\u22121. thus differencing removes linear trend but complicates the\nerror structure: if {\u03b5t} had been white noise, then the differenced process {\u03b7t} follows\nan ma(1) model with \u03b21 = \u22121. it is straightforward to show that d-fold differencing\nwill remove a polynomial trend of order d (exercise 6.4.4). over-differencing does\nlittle harm: if there had been no trend originally present then {xt} merely has a more\ncomplicated error structure than had {yt}. differencing can also be used to remove\nseasonal components.\nif an arma( p, q) model fits the d-fold difference of {yt}, then we have a(b)(i \u2212\nb)dyt = b(b)\u03b5t , and this is known as an integrated autoregressive-moving average\nor arima( p, d, q) process. this generalizes the class of arma models to allow\nnon-stationarity.\n\nexample 6.28 (ftse data) trends such as that in the right panel of figure 6.11\nare generally removed by differencing the log closing prices, and the upper panel of\nfigure 6.13 shows yt = 100 log(xt /xt\u22121), where xt is the original series. thus yt is\nproportional to the differences of the log xt and represents daily percentage returns to\ninvestors. differencing has removed the trend, but it is not clear that the yt are station-\nary \u2014 their variability seems to increase from time to time. such changes in volatility\ncannot be mimicked by linear processes and much effort has been expended in mod-\nelling them. probability plots show that the yt are somewhat asymmetric with heavier\ntails than the normal distribution, so the marginal distribution of {yt} is non-normal.\nsuggestive of slight autoregressive behaviour. its value,(cid:1)\u03c11 = 0.09, is too small to be\nthe partial correlogram of yt shows small but significant autocorrelation at lag one,\n\nof much use in predicting movements of yt . this makes sense: high correlation could\nbe exploited by everyone for gain, but there must be both winners and losers when\nshares are traded. the partial correlogram of the (yt \u2212 y)2 shows generally positive\nautocorrelations to about lag 20.\n\nthe yt have average 0.043 with standard error 0.018, so if the data were independent\nthere would be evidence that e(yt ) > 0, corresponding to an average daily increase\n(cid:1)\nof about 0.043% in the ftse over 1991\u20131998.\n\nother approaches to trend removal can involve local smoothing by methods like\nthose to be described in section 10.7; very roughly the idea is to use weighted averages\nof the data to estimate changes in the process mean. such averaging can be applied\non different scales, for example giving separate estimates of systematic decadal,\nannual, and monthly variation. robust versions of these smoothers exist and are often\npreferable in practice.\n\n "}, {"Page_number": 284, "text": "272\n\n)\n\n%\n\n(\n \ns\nn\nr\nu\n\nt\n\ne\nr\n \ny\n\nl\ni\n\na\nd\n\n6\n\n4\n\n2\n\n0\n\n2\n-\n\n4\n-\n\n6\n-\n\ny\n \nr\no\n\nf\n \n\nm\na\nr\ng\no\ne\nr\nr\no\nc\n \nl\n\nl\n\na\n\ni\nt\nr\na\np\n\n2\n\n.\n\n0\n\n1\n0\n\n.\n\n0\n\n.\n\n0\n\n1\n\n.\n\n0\n-\n\n2\n\n.\n\n0\n-\n\n6 \u00b7 stochastic models\n\nfigure 6.13 daily\nreturns (%) from the\nftse, 1991\u20131998. the\nlower panels show the\npartial correlograms of the\nyt and their squares. the\n95% confidence bands\nshown by the dotted\nhorizontal lines are much\nnarrower than in\nfigure 6.12 because there\nare many more data.\n\n1992\n\n1994\n\n1996\n\n1998\n\ntime\n\n2\n^\ny\n \nr\no\n\nf\n \n\nm\na\nr\ng\no\ne\nr\nr\no\nc\n \nl\n\nl\n\na\n\ni\nt\nr\na\np\n\n2\n\n.\n\n0\n\n1\n0\n\n.\n\n0\n\n.\n\n0\n\n1\n\n.\n\n0\n-\n\n2\n\n.\n\n0\n-\n\n0\n\n20\n\n40\n\n60\n\n80 100\n\n0\n\n20\n\n40\n\n60\n\n80 100\n\nlag\n\nlag\n\nvolatility models\na key feature of financial time series such as that in the top panel of figure 6.13\nis their changing volatility, which leads to periods of high variability interspersed\nwith quieter periods. a standard model for this in the financial context is the linear\nautoregressive conditional heteroscedastic model of order one or linear arch(1)\nprocess, which sets\n\nt = . . . ,\u22121, 0, 1, . . . ,\n\nyt = \u03c3t \u03b5t ,\n\n= \u03b20 + \u03b21y 2\nt\u22121\n\n\u03c3 2\nt\n\n,\n\n(6.27)\nwhere {\u03b5t} is normal white noise with unit variance with \u03b5t independent of yt\u22121,\n\u03b20 > 0 and \u03b21 \u2265 0. the current variance \u03c3 2\nis increased if the previous observation\nwas far from zero, giving bursts of high volatility when this occurs. a necessary\nt ) = e(\u03c3 2\nt ) < \u221e, implying that \u03b30 = \u03b20 + \u03b21\u03b30\ncondition for stationarity is e(y 2\nor equivalently that \u03b21 < 1. in this case {yt} is zero-mean white noise, but as we can\nt ) = \u03b20 + \u03b21y 2\n+ \u03b7t , where \u03b7t = \u03c3 2\n\u2212 1) has mean\n+ (y 2\n\u2212 \u03c3 2\nwrite y 2\nt\u22121\nzero, we see that {y 2\n} follows an autoregressive process, albeit with non-constant\nt\nvariance. in order for the process {y 2\n} to be stationary e(y 4\nt ) must be finite, and\nthis occurs when \u03b22\n< 1/3. then yt has fatter tails than the normal distribution.\n1\n\nt )e(\u03b52\n\n= \u03c3 2\n\nt (\u03b52\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\n "}, {"Page_number": 285, "text": "6.4 \u00b7 time series\n\n273\n\nthus arch models mimic two important features of financial time series: volatility\nclustering and fat-tailed marginal distributions.\nthe assumption of normal innovations can be replaced by other distributions, a\npopular choice being to set \u03bd\u03b5t /(\u03bd \u2212 2) iid\u223c t\u03bd; the scaling ensures that var(\u03b5t ) = 1.\narch models can be extended to allow dependence on y 2\nt\u22122\n, . . . , a\nparticularly widely-used case being the generalized arch or garch(1,1) process\nin which \u03c3 2\nt\n\n= \u03b20 + \u03b21y 2\nt\u22121\n\n, . . . and on \u03c3 2\nt\u22121\n\n+ \u03b4\u03c3 2\n\nt\u22121.\n\nexample 6.29 (ftse data) example 6.28 suggests that an unadorned arch model\nis unlikely to fit these data because it cannot account for the non-zero mean and non-\nzero correlations. inspired by (6.27), we therefore let yt \u2212 \u00b5 = \u03b1(yt\u22121 \u2212 \u00b5) + \u03c3t \u03b5t\n= \u03b20 + \u03b21(yt\u22121 \u2212 \u00b5)2. this combines autoregressive structure for the means\nwith \u03c3 2\nt\nof the yt with arch structure for their variance. the result is a markov process,\nand with normal \u03b5t the log likelihood contribution from the conditional density\nf (yt | yt\u22121) is\n\u2212 1\n2\n\nlog{\u03b20 + \u03b21(yt\u22121 \u2212 \u00b5)2} \u2212 {yt \u2212 \u00b5 \u2212 \u03b1(yt\u22121 \u2212 \u00b5)}2\n2{\u03b20 + \u03b21(yt\u22121 \u2212 \u00b5)2} .\n\nthe overall log likelihood is a sum of such terms for t = 2, . . . ,n plus log f (y1),\nbut the series is so long that this initial term, which involves knowing the stationary\ndensity of yt , can safely be ignored.\n\nthe log likelihood is readily maximized numerically, but a correlogram suggests\n\nthat structure remains in the squares of the residuals\n\nrt = yt \u2212(cid:1)\u00b5 \u2212(cid:1)\u03b1(yt\u22121 \u2212(cid:1)\u00b5)\n{(cid:1)\u03b20 +(cid:1)\u03b21(yt\u22121 \u2212(cid:1)\u00b5)2}1/2\n= \u03b20 + \u03b21(yt\u22121 \u2212 \u00b5)2 + \u03b4\u03c3 2\n\n,\n\n= \u03b20 + \u03b21(yt\u22121 \u2212 \u00b5)2 + \u03b41\u03c3 2\n\nso this model is not adequate. as an alternative, we retain the ar mean structure but\nuse garch structure \u03c3 2\nt\u22121 for the variances. a crude\nt\nway to fitthis is to estimate \u03c3 2\nm by the variance of y1, . . . , ym, and then to com-\nt\u22121 for t = m + 1, . . . ,n . the likelihood based\npute \u03c3 2\non f (ym+1, . . . , yn | y1, . . . , ym) isthen readily obtained and may be maximized.\nt\nhere n is large so little information is lost by conditioning on y1, . . . , ym. with\nm = 30 the maximized log likelihood is \u22122100.27, and both the residuals and their\nsquares look like white noise, so the structure of the model seems correct. how-\never a normal probability plot of the residuals suggests that slightly heavier-tailed\ninnovations may be needed. we therefore let the \u03b5t have t\u03bd distributions, scaled so\nthat var(\u03b5t ) = 1. the resulting log likelihood is \u22122075.64, an appreciable improve-\nment. the maximum likelihood estimates and standard errors are(cid:1)\u00b5 = 0.051 (0.018),\n(cid:1)\u03b1 = 0.070 (0.024),(cid:1)\u03b20 = 0.006 (0.004),(cid:1)\u03b21 = 0.036 (0.011),(cid:1)\u03b4 = 0.955 (0.016) and\n(cid:1)\u03bd = 9.7 (1.86). thus \u00b5 and \u03b1 seem necessary for successful modelling. over the\nperiod of these data the return on investment was on average 100(cid:1)\u00b5\n.= 5% every\n(cid:1)\u03b1 = 0.07 between yt and yt+1 for short-term prediction. the value of(cid:1)\u03b4 shows the\n100 trading days, but little would be gained from using the estimated correlation\n\nstrong dependence of \u03c3 2\n\nt on \u03c3 2\n\nt\u22121 that leads to volatility persistence. a condition for\n\n "}, {"Page_number": 286, "text": "274\n\n6 \u00b7 stochastic models\nstationarity of a garch process {yt} is that \u03b21 + \u03b4 < 1, and this is satisfied by the\nestimates. the value of (cid:1)\u03bd indicates innovations somewhat heavier than normal, in\nagreement with the residual plot. overall the model seems to fit surprisingly well.\n(cid:1)\n\ntime series is a large and important topic, whose surface has barely been scratched\n\nabove. the bibliographic notes give some points of entry to the literature.\n\nexercises 6.4\n1\n\nconsider (6.24) for t = 1, . . . ,n , and suppose that y0 has a known distribution with finite\nvariance, independent of \u03b51, . . . , \u03b5n. deduce that\n\nyn \u2212 \u00b5 = n(cid:7)\n\nj=1\n\n\u03b1n\u2212 j \u03b5 j + \u03b1n(y0 \u2212 \u00b5)\n\n(cid:5)\n\nn\nj=1\n\n\u03b12 j < \u221e.\n\nand establish that a limiting distribution for yn as n \u2192 \u221e exists only when\nlimn\u2192\u221e\nhence show that a condition for stationarity is |\u03b1| < 1, in which case the limiting distri-\nbution for yn is normal with mean \u00b5 and variance \u03c3 2/(1 \u2212 \u03b12). show also that if y0 has\nthis distribution, so too do all the y j . show that the covariance matrix \u0001 of y1, . . . ,y n is\nthen that given in example 6.24, and write down the corresponding moral graph.\nconsider the ma(1) process; see example 6.26. show that its covariances are\n\ncov(yt , yt+s) =\n\n(cid:3)\n\n,\n\n1 + \u03b22\n\n1\n\n\uf8f1\uf8f2\n\uf8f3 \u03c3 2\n\n(cid:2)\n\u03c3 2\u03b21,\n0\n\ns = 0,\ns = 1,\notherwise,\n\nfind the autocorrelation function and use the matrices in example 6.24 to deduce that there\nis no cut-off in the partial autocorrelations.\ngeneralize this to the ma(q) model.\nsuppose that yt = (cid:5)\ngive an expression for the log likelihood in example 6.25.\n\u03be j t j + \u03b5t , where {\u03b5t} is a stationary process. show by induction\nthat d-fold differencing yields a series that is stationary for any d \u2265 k.\nlet yt = s(t) + \u03b5t , where s(t) = s(t + kp), for a fixed integer p and all integers t and k.\nshow that (i \u2212 b p)yt is stationary, and discuss the implications for removal of seasonality\nfrom a monthly time series.\ngive aformula for\nexample 6.29.\n\n= \u03b20 + \u03b21(yt\u22121 \u2212 \u00b5)2 + \u03b4\u03c3 2\nt\u22121\n\nrt when \u03c3 2\nt\n\nthe residual\n\nk\nj=0\n\nin\n\n2\n\n3\n\n4\n\n5\n\n6.5 point processes\n\ndata that can be summarized by points in a continuum arise in many applications.\nexamples are the epicentres of earthquakes, the locations of cases of leukaemia,\nand the times are which emails are sent. the \u2018point\u2019 may be merely a convenient\nrepresentation of something small compared to its surroundings, and other information\nmay be available, such as the strength of the earthquake, but here we assume that\nsummary as a point is sensible and ignore other aspects.\n\n6.5.1 poisson process\nthe poisson process in the line is the simplest point process and the basis for many\nmore complex models. suppose that we observe points in a time interval [0, t0].\n\n "}, {"Page_number": 287, "text": "o(\u03b4t) is small enough that\no(\u03b4t)/\u03b4t \u2192 0 as\u03b4 t \u2192 0.\n\n6.5 \u00b7 point processes\n275\nlet n (w , w + t) denote how many fall into the subinterval (w , w + t]; we write\nn (t) = n (0, t), t > 0, and n (a) for the number of points in the set a. let \u03bb(t) be\na well-behaved non-negative function whose integral is finite on [0, t0], and suppose\nthat\n\nof n (a2) whenever a1 \u2229 a2 = \u2205;\n\nr events in disjoint subsets of [0, t0] are independent, that is, n (a1) isindependent\nr pr{n (t, t + \u03b4t) = 0} =1 \u2212 \u03bb(t)\u03b4t + o(\u03b4t) for small \u03b4t; and\nr pr{n (t, t + \u03b4t) = 1} =\u03bb(t )\u03b4t + o(\u03b4t) for small \u03b4t.\n\nthe last two properties imply that pr{n (t, t + \u03b4t) > 1} =o (\u03b4t), so the process is\norderly: multiple occurrences at the same t may not occur. the intensity \u03bb(t) is\ninterpreted as the rate at which points occur in a small interval at t, somore points\n\u03bb(u) du ensures that n (t0) < \u221e\nfall where \u03bb(t) isrelatively high. finiteness of\nwith probability one, as we shall see below.\nwe find the probability that there are no points in the interval (w , w + t] by dividing\nit into k subintervals of length \u03b4t = t /k, and then letting \u03b4t \u2192 0. then the properties\nabove imply that\n\n(cid:24)\n\nt0\n0\n\npr{n (w , w + t) = 0} = k\u22121(cid:4)\n.= k\u22121(cid:4)\ni=0\n= exp\n\ni=0\n\npr [n {w + i \u03b4t, w + (i + 1)\u03b4t} = 0]\n\n{1 \u2212 \u03bb(w + i \u03b4t)\u03b4t + o(\u03b4t)}\n(cid:30)\nk\u22121(cid:7)\nlog{1 \u2212 \u03bb(w + i \u03b4t)\u03b4t + o(\u03b4t)}\n(cid:6)\ni=0\n\u2212 k\u22121(cid:7)\n(cid:21)\n\"\ni=0\n\u2212\n\n\u03bb(w + i \u03b4t)\u03b4t + o(k\u03b4t)\nw+t\n\n(cid:8)\n\n(cid:26)\n\n,\n\n\u03bb(u) du\n\n(cid:31)\n\n(6.28)\n\nw\n\n= exp\n\n\u2192 exp\n\nwhere the limit follows because as \u03b4t \u2192 0 with t fixed, o(k\u03b4t) = t o(\u03b4t)/\u03b4t \u2192 0. as\nthe length of the random time t from w to the next point exceeds t if and only if\nn (w , w + t) = 0, t has probability density function\n(cid:21)\nft (t) = \u2212 dpr{n (w , w + t) = 0}\nand hazard function ft (t)/pr(t \u2265 t) = \u03bb(w + t).\nnow suppose that points in (0, t0] have been observed at times t1, . . . ,t n, where\n0 < t1 < \u00b7\u00b7\u00b7 < tn < t0. as events in non-overlapping sets are independent, the joint\nprobability density of the data is\n\n= \u03bb(w + t) exp\n\n\u03bb(u) du\n\nt > 0,\n\nw+t\n\n(cid:26)\n\n\u2212\n\n\"\n\ndt\n\nw\n\n,\n\n\u2212(cid:24) t1\n\n0\n\n\u03bb(t1)e\n\n\u03bb(u) du \u00d7 \u03bb(t2)e\n\n\u2212(cid:24)\n\nt2\nt1\n\n\u03bb(u) du \u00d7 \u00b7\u00b7\u00b7 \u00d7 \u03bb(tn)e\n\ntn\ntn\u22121\n\n\u03bb(u) du \u00d7 e\n\nt0\ntn\n\n\u03bb(u) du,\n\n\u2212(cid:24)\n\n\u2212(cid:24)\n\n "}, {"Page_number": 288, "text": "276\n\n6 \u00b7 stochastic models\n\nwhere the final term is the probability of no events in (tn, t0]. this joint density\nreduces to\n\nt0\n\n\u03bb(u) du\n\n\u03bb(t j ),\n\n0 < t1 < \u00b7\u00b7\u00b7 < tn < t0.\n\n(6.29)\n\n(cid:21)\n\n\"\n\n\u2212\n\n0\n\nexp\n\n(cid:26)\n\nn(cid:4)\nj=1\n\ngiven a parametric form for \u03bb(t), (6.29) gives the likelihood on which inferences may\nbe based. in practice the integral is usually unavailable in closed form and a numerical\napproximation must be used.\n\nthe probability of n events occurring in the interval [0, t0] isobtained by integrating\n\n(6.29) with respect to t1, . . . ,t n and is (exercise 6.5.2)\nexp{\u2212\u0001(t0)} ,\n\npr{n (t0) = n} = \u0001(t0)n\nwhere we have written \u0001(t0) = (cid:24)\nn!\nt0\n\u03bb(u) du. thus n (t0) is apoisson variable with\n0\nmean \u0001(t0). as events in disjoint subsets are independent and sums of independent\nnumber of events in a subset a is a poisson variable whose mean \u0001(a) = (cid:24)\npoisson variables are poisson (example 2.35), we see that in a poisson process, the\na \u03bb(u) du\nis the integral of the rate function \u03bb over a. moreover these counts are independent\nfor disjoint subsets.\n\nn = 0, 1, . . . ,\n\n(6.30)\n\ndivision of (6.29) by (6.30) gives the probability that points arise at t1, . . . ,t n\n\nconditional on there being n points, namely\n\nn(cid:4)\nj=1\n\nn!\n\n\u03bb(t j )\n\u0001(t0)\n\n,\n\n0 < t1 < \u00b7\u00b7\u00b7 < tn < t0.\n\nthis is the joint density of the order statistics of a random sample of size n with\ndensity \u03bb(t)/\u0001(t0) onthe interval [0, t0]; see (2.25). as we shall see, this result is\nuseful in model-checking.\n\nso \u0001(t0) =\nexample 6.30 (exponential\ne\u03b20(e\u03b21t0 \u2212 1)/\u03b21. when \u03b21 = 0 this yields a constant intensity. the log likelihood\ncorresponding to (6.29) equals\n\ntrend) let \u03bb(t) = exp(\u03b20 + \u03b21t),\n\n(cid:5)(\u03b20, \u03b21) = n\u03b20 + \u03b21\n\nt j \u2212 e\u03b2\n\n0 (e\u03b21t0 \u2212 1)/\u03b21\n\nn(cid:7)\nj=1\n\nand is of exponential family form.\nthe ratio \u03bb(t)/\u0001(t0) equals \u03b21e\u03b21t /(e\u03b21t0 \u2212 1), corresponding to an exponential tilt\nof the uniform density on [0, t0], so when \u03b21 > 0 events tend to pile up toward the\n(cid:1)\nright end of the interval, and conversely.\n\nthere is an intimate connection between two ways to think about such data, in\nterms of the counts in subsets of the region of observation and in terms of the spacings\nbetween points. although the second approach is natural in one dimension, the count\nrepresentation is generally simpler in several dimensions. to see how it extends, let s\nsuch that \u0001(s) = (cid:24)\nbe a subset of ird and suppose that an integrable non-negative function \u03bb(t) isdefined\ns \u03bb(u) du is finite. then under conditions that extend those for\n\n "}, {"Page_number": 289, "text": "6.5 \u00b7 point processes\nthe univariate case, the numbers of events in disjoint subsets a1, . . . ,a m of s have\nindependent poisson distributions with means \u0001(a1), . . . , \u0001(am). the probability\ndensity for points observed at {t1, . . . ,t n} \u2282s is\n\n277\n\n\u03bb(t j ) \u00d7 exp{\u2212\u0001(s)} ,\n\n(6.31)\n\nn(cid:4)\nj=1\n\nfrom which a likelihood can again be constructed. such models play in important role\nin event history and survival data, as described in sections 5.4 and 10.8. in terms of\nfigure 5.8, the idea is to treat failures as events of an inhomogeneous poisson process\nin the region of the plane bounded by the line x = y, the horizontal axis, and the\nvertical line marking the end of the trial; see section 10.8.2. another application, to\nstatistics of extremes, will be described shortly.\n\n|a| is the length\n(lebesgue measure) of the\nset a.\n\nhomogeneous poisson process\nthe simplest situation is when the intensity function \u03bb(t) is aconstant \u03bb. then \u0001(a) =\n\u03bb|a| and \u0001(t0) = \u03bbt0. the number of points in [0, t0] isthen poisson with mean \u03bbt0,\n\u2212\u03bby.\nand intervals between them are independent exponential variables with density \u03bbe\nthe log likelihood from (6.29) is\n\n(cid:5)(\u03bb) \u2261 n log \u03bb \u2212 \u03bbt0,\n\nfrom which the maximum likelihood estimate(cid:1)\u03bb = n/t0 and information quantities\nmay be derived; see example 4.19.\nwhen \u03bb(t) isconstant, the density \u03bb(t)/\u0001(t0) = t\nis uniform on the interval [0, t0],\nand hence the n points u j = t j /t0 are distributed as order statistics of a random sample\nplot the empirical distribution function of the u j , (cid:1)f(u). departures from the uniform\nfrom the uniform distribution on [0, 1]; see section 2.3. a graphical check of this is to\ndistribution f(u) = u, 0 \u2264 u \u2264 1 suggest that the intensity is not constant. formal\ntests of fit using this are discussed in section 7.3.1.\n\n\u22121\n0\n\ndata often exhibit clustering relative to a poisson process. if so, there will\ntend to be an excess of short intervals between points, relative to the exponen-\ntial distribution. under the poisson process model the spacings y1 = t1 \u2212 0, y2 =\nt2 \u2212 t1, . . . , yn+1 = t0 \u2212 tn form a (non-independent) sample from the exponen-\ntial distribution with mean \u03bb\u22121, so aplot of ordered spacings against exponential\norder statistics should be a straight line, departures from which will suggest model\nfailure.\n\nexample 6.31 (danish fire data) figure 6.14 shows data on the times and amounts\nof major insurance claims due to fire in denmark from 1980\u20131990. the upper left\npanel shows the original 2492 claims; the original amounts have been rescaled. the\ndata are dominated by a few large claims, shown in more detail in the upper right\npanel, which gives the logarithms of the 254 claims that exceed 5 units. this is\na two-dimensional point process of times and log amounts, which reduces to the\none-dimensional data shown as a rug at the foot of the panel if the amounts are\nignored.\n\n "}, {"Page_number": 290, "text": "figure 6.14 data on\nmajor insurance claims\ndue to fires in denmark,\n1980\u20131990 (embrechts\net al., 1997, pp. 298\u2013303).\nthe upper left panel\nshows the original data\nand the upper right panel\nthe logs of the 254 losses\nexceeding five units, with\nthe rug below showing\ntheir times. the lower\nright panel shows the\nempirical distribution of\nthe 254 u j = t j /t0, and\nthe lower left panel an\nexponential probability\nplot of spacings between\nthese t j . ineach case the\ndotted line shows the\nexpected pattern under a\nhomogeneous poisson\nprocess. the lower right\npanel suggests that the\nrate of the process may be\nnon-uniform, with an\nexcess of early points\nfollowed by a deficiency.\nthe solid diagonal lines in\nthe lower right panel show\nsignificance for a\nkolmogorov\u2013smirnov\nstatistic at levels 0.05 and\n0.01 and are explained in\nexample 7.23. the lower\nleft panel suggests that the\nspacings are close to\nexponentially distributed.\n\n278\n\ni\n\nm\na\nc\n\nl\n\n0\n5\n2\n\n0\n0\n2\n\n0\n5\n1\n\n0\n0\n1\n\n0\n5\n\n0\n\n6\n\n5\n\n4\n\n3\n\n2\n\ni\n\nm\na\nc\n \n\nl\n\ng\no\nl\n\n6 \u00b7 stochastic models\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n..\n.\n.\n.\n.\n...\n.\n....\n..\n..\n..\n.\n.\n.\n....\n..\n..\n.\n.\n.\n.\n..\n.\n\n.\n.\n.\n.\n.\n.\n.\n..\n.\n\n....\n..\n....\n.\n.\n..\n.\n.\n...\n.\n..\n.\n.\n.\n\n.\n.\n\n.\n.\n.\n.\n..\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n..\n..\n...\n....\n.\n.\n.\n.\n....\n.\n..\n..\n.\n.\n.\n.\n.\n\n.\n\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n....\n.\n.\n..\n.\n.\n......\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n..\n.\n..\n..\n.\n...\n.\n..\n.\n.\n.\n..\n.\n\n030180\n\n030184\n\n030188\n\n030180\n\n030184\n\n030188\n\ntime\n\ntime\n\n)\ns\ny\na\nd\n(\n \nl\n\na\nv\nr\ne\n\nt\n\nn\n\ni\n \n\nd\ne\nr\ne\nd\nr\no\n\n0\n0\n1\n\n0\n8\n\n0\n6\n\n0\n4\n\n0\n2\n\n0\n\n.\n\n.\n\n.\n\n. .\n\n...\n\n...\n\n................................................................................................................................................................................................................\n\nn\no\n\ni\nt\nc\nn\nu\n\nf\n \n\nn\no\n\ni\nt\n\nu\nb\ni\nr\nt\ns\nd\n\ni\n\n \nl\n\na\nc\ni\nr\ni\np\nm\ne\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n0.0 0.2 0.4 0.6 0.8 1.0\n\nexponential plotting position\n\nnormalized time\n\nwe consider only the times of these 254 largest claims. the lower right panel shows\nthe empirical distribution function of the corresponding u j = t j /t0, with t1, . . . ,t n the\nrug in the panel above. relative to the uniform distribution there is a slight excess of\nclaims up to about 1983, followed by a deficiency from 1984 to 1990. example 7.23\ngives further discussion of the fit.\n\nthe exponential probability plot of the spacings in the lower left panel of the figure\nwith a slightly longer tail. the value of(cid:1)\u03bb is roughly 254/(11 \u00d7 365) = 0.063 days\nsuggests that the times between claims are fairly close to exponential, though perhaps\n\u22121.\nbetween claims of(cid:1)\u03bb\u22121 = 15.8 days; this has standard error 1.0 calculated from the\nthus the rate of arrival of claims per day is about 0.06, corresponding to a mean time\n\nobserved information.\n\nwe return to these data in examples 6.34 and 7.23.\n\n(cid:1)\n\n6.5.2 statistics of extremes\nan important application of poisson processes is to rare events \u2014 high sea levels,\nlow temperatures, record times to run a mile, large insurance claims, and so forth.\nto see how, we make a detour and consider properties of the maximum of a random\n\n "}, {"Page_number": 291, "text": "the upper support point is\nthe smallest x0 such that\nlimx(cid:19)x0 f(x) = 1;\npossibly x0 = +\u221e.\n\n6.5 \u00b7 point processes\n\n279\n\nsample x1, . . . , xm from a continuous distribution function f(x) with upper support\npoint x0. asm \u2192 \u221e, independence of the xi implies that for any fixed x < x0,\n\npr{max(x1, . . . , xm) \u2264 x} = pr(xi \u2264 x, i = 1, . . . ,m )\n\n= pr(x1 \u2264 x) \u00d7 \u00b7\u00b7\u00b7 \u00d7pr( xm \u2264 x)\n= f(x)m \u2192 0,\n\nso in order to obtain a non-degenerate limiting distribution for the maximum, we\nmust rescale the xi . weconsider ym = a\nm (maxi xi \u2212 bm) for sequences of constants\n\u22121\n{am} > 0 and {bm}, and ask under what conditions ym\nd\u2212\u2192 y as m \u2192 \u221e for some\nnon-degenerate random variable y . asm \u2192 \u221e,\n\n#\n\n{max(x1, . . . , xm) \u2212 bm} \u2264 y\n\na\n\n\u22121\nm\n\npr (ym \u2264 y) = pr\n= f(bm + am y)m\n%\n=\n\n1 \u2212 m {1 \u2212 f(bm + am y)}\n\n&m\n\n$\n\nm\n\n(6.32)\ncan be shown to possess a limit if and only if limm\u2192\u221e m {1 \u2212 f(bm + am y)} exists. as\nm {1 \u2212 f(bm + am y)} is the number of the x1, . . . , xm expected to exceed bm + am y,\nsuitable sequences {am} and {bm} exist for most, but not all, continuous distributions.\nif they do exist, a remarkable result is that the only possible non-trivial limit is of\nform\n\n1 + \u03be\n\nm\u2192\u221e m {1 \u2212 f(bm + am y)} =\n\nlim\n\n(6.33)\nwith the right-hand side taken to be exp{\u2212(y \u2212 \u03b7)/\u03c4} if \u03be = 0. the parameters \u03c4\nand \u03b7 control the scale and location of the limit, and account for the effect of minor\nchanges to {am} and {bm} \u2014 for example, replacing am by 1\n2 am would rescale any\nlimit, but would not affect its existence or its shape.\n\n+\n\n\u03c4\n\n,\n\non putting together (6.32) and (6.33), we see that if a limiting distribution for the\n\nmaximum exists, it must be the generalized extreme-value distribution\n\n(cid:9)\n\n(cid:10)\u22121/\u03be\n\ny \u2212 \u03b7\n\n(cid:6)\n\n(cid:9)\n\n(cid:8)\n\n(cid:10)\u22121/\u03be\n\n1 + \u03be\n\ny \u2212 \u03b7\n\n\u03c4\n\n+\n\nh(y; \u03b7, \u03c4, \u03be) = exp\n\n\u2212\n\n, \u2212\u221e < \u03be, \u03b7 <\u221e, \u03c4 > 0,\n\n(6.34)\n\nwhere the range of y is such that 1 + \u03be(y \u2212 \u03b7)/\u03c4 > 0. the parameter \u03be controls the\nshape of the density, which has a heavy right tail and finite lower support point if\n\u03be > 0, and a finite upper support point if \u03be < 0. the gumbel distribution\nh(y; \u03b7, \u03c4, 0) = exp[\u2212 exp{\u2212(y \u2212 \u03b7)/\u03c4}], \u2212\u221e < y < \u221e,\n\narises as \u03be \u2192 0; see problem 6.11. expression (6.34) gives the only possible limiting\ndistribution for maxima. minima are dealt with by noting that any limit distribution\nfor mini (xi ) = \u2212 maxi (\u2212xi ) must have form 1 \u2212 h(\u2212y; \u03b7, \u03c4, \u03be).\n\n(a)+ = a if a > 0 and\notherwise equals zero.\n\nemil julius gumbel\n(1891\u20131966) was born\nand studied in munich.\nhis radical pacifist views\nand jewish background\ncaused conflict with his\nuniversity colleagues and\nauthorities in heidelberg,\nand led to his exile in\nfrance in 1932 and later in\nthe usa. he highlighted\nthe importance of\nstatistical extremes, on\nwhich he wrote an\nimportant book (gumbel,\n1958), and through his\nconsulting strongly\ninfluenced hydrologists,\nmeteorologists, and\nengineers.\n\n "}, {"Page_number": 292, "text": "280\n\nf\nd\nc\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n0\n\n.\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n6 \u00b7 stochastic models\n\nfigure 6.15\nconvergence for sample\nmaxima. left panel:\ndistributions of maxima of\nm = 1, 7, 30, 365, 3650\nstandard normal variables\n(from left to right). right\npanel: distributions of\nrenormalized maxima of\nm = 1, 7, 30, 365, 3650\nstandard normal variables.\nthe distributions on the\nright converge to the\ngumbel distribution\n(heavy).\n\nf\nd\nc\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n0\n\n.\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n-4\n\n-2\n\n0\n\ny\n\n2\n\n4\n\n-4\n\n-2\n\n2\n\n4\n\n0\n\ny\n\n(cid:26)\n\n(cid:21)\n\ntegration by parts gives 1 \u2212 f(x) = (cid:24) \u221e\nexample 6.32 (normal distribution) for the standard normal distribution, in-\n.= \u03c6(x)/x as x \u2192 \u221e. hence\nm {1 \u2212 f(bm + am y)} approximately equals\n\n\u03c6(x) dx\n\nx\n\n1\n\n,\n\nexp\n\n\u2212 1\n2\n\nlog 2\u03c0\n\u22121/2 and bm = a\n\n(bm + am y)2 \u2212 log(bm + am y) + log m \u2212 1\n(6.35)\n2\n\u2212\nand some tedious algebra shows that with am = (2 log m)\n\u22121\n2 am(log log m + log 4\u03c0), (6.35) converges to exp(\u2212y) asm \u2192 \u221e. however the con-\nm\nvergence is very slow. with y = 4 the probabilities \u0001(bm + am y)m are 0.9907, 0.9871,\n0.9859, 0.9855 for m = 30, 365, 1825, 3650, while the target gumbel probability is\n0.9819. these values of m are chosen to correspond to random sampling of a normal\ndistribution daily for periods of one month, and one, five, and ten years. even with\nthis amount of daily data the limiting probability is not attained, because the right tail\nof the normal distribution is so light compared to that of the gumbel distribution that\nenormous samples are needed for the limit to work well.\nfigure 6.15 shows the convergence graphically. the left panel shows the distribu-\ntions of maxima of m standard normal variables, with m = 1, 7, 30, 365, and 3650,\ncorresponding to maxima over a day, a week, a month, a year and ten years of daily\nnormal data. the distribution becomes increasingly concentrated as m increases, and\ndoes not converge to a useful limit. the right panel shows how the distribution of\n{max(x1, . . . , xm) \u2212 bm} does converge to a limiting gumbel distribution, given\n\u22121\na\nm\nby the heavy solid line. as mentioned above, the convergence is rather slow. fortu-\nnately the generalized extreme-value distribution usually gives a better approximation\n(cid:1)\nfor sample maxima than this example might suggest.\n\nthe upshot is that the generalized extreme-value distribution provides the natural\nmodel to fit to sample maxima or minima. for example, if a series of annual maximum\nsea levels y1, . . . , yn is available, we suppose that they are a random sample from\n(6.34) and fit it by maximum likelihood. often the parameter of interest is the p\nquantile of the distribution, that is yp = \u03b7 + \u03c4{(\u2212 log p)\nthis context as the (1 \u2212 p)\n\u22121-year return level: it isthe level exceeded once on average\n\n\u2212\u03be \u2212 1}/\u03be, which is known in 1/(1 \u2212 p) is known as the\n\nreturn period.\n\n "}, {"Page_number": 293, "text": "6.5 \u00b7 point processes\n\n281\n\nfigure 6.16 annual\nmaximum sea levels (m)\nat yarmouth, 1899\u20131976.\nlower left: gumbel\nprobability plot of the\ndata. lower right: fitted\n(solid) and empirical\nexceedance probabilities\n(points), with inference\ntools for 100-year return\nlevel y0.99. the vertical\nline shows the value of\n\n(cid:1)y0.99, while its profile\n\nlikelihood and 95%\nconfidence interval are\nshown by the dotted and\ndashed lines. note the\nstrong asymmetry of the\nconfidence interval.\n\n)\n\nm\n\n(\n \nl\n\ne\nv\ne\n\nl\n \n\na\ne\ns\n \nm\nu\nm\nx\na\nm\n\ni\n\n \nl\n\na\nu\nn\nn\na\n\n0\n\n.\n\n3\n\n5\n\n.\n\n2\n\n0\n\n.\n\n2\n\n5\n\n.\n\n1\n\ny\n\n0\n\n.\n\n3\n\n5\n\n.\n\n2\n\n0\n\n.\n\n2\n\n5\n\n.\n\n1\n\n1900\n\n1920\n\n1940\n\n1960\n\n\u2022\n\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022 \u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\n-1\n\n0\n\n1\n\n2\n\n3\n\n4\n\ny\nt\ni\nl\ni\n\nb\na\nb\no\nr\np\n\n0\n0\n0\n0\n\n.\n\n1\n\n0\n0\n1\n0\n\n.\n\n0\n\n1\n0\n0\n0\n\n.\n\n0\n\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\n\u2022\n\n2\n\n3\n\n4\n\nreturn level\n\ngumbel plotting positions\nevery (1 \u2212 p)\n\u22121 years. this would be important if the data were being analyzed in\norder to suggest how high coastal defenses should be built. of course quantities such\nas the expected insurance loss should flooding occur are also of interest.\nmaximum likelihood estimation is regular if \u03be > \u22121/2, as seems common in appli-\ncations. when \u03be \u2264 \u22121/2, the likelihood derivatives do not have their usual properties\nand example 4.43 is relevant, as the upper support point of the density can be estimated\nwith rate faster than the usual n\n\n\u22121/2.\n\nthe return level is estimated by replacing \u03b7, \u03c4 , and \u03be by their maximum likelihood\nestimates. its standard error may be obtained using the delta method (page 122),\nthough the profile log likelihood for yp gives a more reliable confidence set. in practice\nn is often substantially smaller than (1 \u2212 p)\n\u22121 and the return level is estimated well\noutside the range of the data. then it is important to consider whether there are enough\ndata underlying the y1, . . . , yn for the generalized extreme-value model to give a good\napproximate distribution for the maxima, and to check whether n is large enough for\nlarge-sample likelihood theory to be a good basis for inference. the crucial aspect\nis however the extent to which extrapolation to high quantiles of the distribution is\nsensible based on limited data, and this bears careful consideration.\n\nexample 6.33 (yarmouth sea level data) the upper panel of figure 6.16 shows a\ntime series of annual maximum sea levels at yarmouth on the east coast of england for\n\n "}, {"Page_number": 294, "text": "282\n\n6 \u00b7 stochastic models\n\n1899\u20131976. as is typical with such data, the largest value is considerably greater than\nthe rest; it arose in 1953 when there was widespread flooding. the correlogram and\npartial correlogram show no serial dependence, so we treat the values as independent.\nthe lower left panel of the figure shows a probability plot of the data against gumbel\nplotting positions. upward curvature would here suggest that \u03be > 0, and downward\n.= 0. the\ncurvature that \u03be < 0. in fact the plot is close to straight, indicating that \u03be\nlarge value from 1953 does not appear outlying, because of the heavy right tail of the\nthe maximum likelihood estimates and standard errors are(cid:1)\u03b7 = 1.90 (0.034),(cid:1)\u03c4 =\ndensity.\n0.26 (0.025), and(cid:1)\u03be = 0.04 (0.096); the latter give no evidence against the gumbel\n\nmodel, in agreement with the probability plot. the location and scale parameters are\nwell determined compared to \u03be.\nthe lower right panel of figure 6.16 compares the estimated survivor function\npr(y > y) with its empirical counterpart, obtained by plotting 1 \u2212 j/(n + 1) against\ny( j). the vertical line indicates the estimated 100-year return level,(cid:1)y0.99, while the\n\nbroken lines show the profile likelihood for y0.99 and the corresponding 95% confi-\ndence interval. this is highly asymmetric, so this interval is much preferable to using\nnormal approximation. in practice 1000- or even 10,000-year return levels may be\n(cid:1)\nneeded, and then of course the statistical uncertainty is very large indeed.\n\npoint process approximation\nif more extensive data are available it is potentially wasteful to use only the annual\nmaxima, and we now show how a poisson process model can overcome this. let\nx1, . . . , x(cid:20)mt0(cid:21) be a random sample from f(x) and consider the pattern of points\nm (xi \u2212 bm)), i = 1, . . . ,(cid:20)mt 0(cid:21) that fall into the subset s = [0, t0] \u00d7 [u,\u221e)\n\u22121\n(i /m, a\nm (xi \u2212 bm) > y occurs if and only if xi > bm + am y, so\n\u22121\nof the plane. the event a\nthe number of points that fall into a = [t1, t2] \u00d7 [y,\u221e) may be expressed as the sum\nof indicator random variables\n\nnm(a) =\n\ni (xi > bm + am y) ,\n\n0 \u2264 t1 < t2 \u2264 t0, y \u2265 u.\n\n(cid:22)mt2(cid:23)(cid:7)\ni=(cid:20)mt1(cid:21)\n\nthe xi are independent and identically distributed, so nm(a) isbinomial with de-\nnominator (cid:22)mt2(cid:23) \u2212 (cid:20)mt1(cid:21) +1 and probability 1 \u2212 f(bm + am y) that satisfies (6.33).\nhence the poisson limit for the binomial distribution (problem 2.3) gives\nn = 0, 1, . . . ,\n\nm\u2192\u221e pr{nm(a) = n} = \u0001(a)n\n\nexp{\u2212\u0001(a)} ,\n\nlim\n\nn!\n\nwhere \u0001(a) equals\n\u0001{[t1, t2] \u00d7 [y,\u221e)} = (t2 \u2212 t1)\n\n(cid:9)\n\n(cid:10)\u22121/\u03be\n\ny \u2212 \u03b7\n\n0 \u2264 t1 < t2 \u2264 t0, y \u2265 u,\n(6.36)\nwith the second term on the right replaced by exp{\u2212(y \u2212 \u03b7)/\u03c4} if \u03be = 0. that is,\nnm(a) d\u2212\u2192 n (a), where n (a) ispoisson with mean \u0001(a).\n\n1 + \u03be\n\n+\n\n\u03c4\n\n,\n\n(cid:20)a(cid:21) and (cid:22)a(cid:23) are\nrespectively the smallest\ninteger larger than a and\nthe largest integer smaller\nthan a.\n\n "}, {"Page_number": 295, "text": "6.5 \u00b7 point processes\n\n283\n\nfigure 6.17 poisson\nprocess limit for rare\nevents. the panels show\nthe values of\nm (xi \u2212 bm ) plotted\n\u22121\na\nagainst i /m for random\nsamples of size m = 10,\n100, 1000 and 10,000\nfrom the exponential\ndistribution. the pattern\nof points above the\nthreshold at u = \u22122 tends\nto a bivariate poisson\nprocess with intensity\ngiven by (6.36).\n\na\n/\n)\nb\n-\nx\n\n(\n\n2\n\n0\n\n2\n-\n\n4\n-\n\n6\n-\n\n8\n-\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n2\n\n0\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\n\u2022\n\n\u2022\n\n2\n-\n\na\n/\n)\nb\n-\nx\n\n(\n\n4\n-\n\n6\n-\n\n8\n-\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n2\n\n\u2022\n\n0\n\n2\n-\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\na\n/\n)\nb\n-\nx\n\n(\n\n4\n-\n\n6\n-\n\n8\n-\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\na\n/\n)\nb\n-\nx\n\n(\n\n2\n\n0\n\n2\n-\n\n4\n-\n\n6\n-\n\n8\n-\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n\nt\n\nt\n\nt\n\nt\n\nmore sophisticated techniques reveal that as m \u2192 \u221e, the limiting joint distribu-\ntions of counts nm(a1), nm(a2), . . . in any collection of disjoint subsets a1,a2, . . .\nof s is that of independent poisson variables with means \u0001(a1), \u0001(a2), . . .. hence\nas m \u2192 \u221e, the limiting positions of random values xi , suitably rescaled, have the\njoint distribution of points of a poisson process n in s with intensity (6.36), with\narbitrary u. figure 6.17 illustrates this for exponential samples.\nto see the connection to extremes, suppose we have daily data for t0 years and\nthat t2 \u2212 t1 = 1 year. then if we apply the poisson limit to these data with a =\n[t1, t2] \u00d7 [y,\u221e), effectively assuming that the limit has set in when m = 365 days,\nand let y 1 \u2265 \u00b7\u00b7\u00b7 \u2265 y r denote the r largest values for that year, we see that in an\nobvious shorthand notation,\n\npr(y 1 \u2264 y) = pr{n (a) = 0}\n1 + \u03be\n\n= exp\n\n(cid:9)\n\n\u2212\n\n(cid:6)\n\n(cid:10)\u22121/\u03be\n\ny \u2212 \u03b7\n\n(cid:8)\n\n,\n\n\u03c4\n\n+\n\npr(y r \u2264 yr , . . . ,y 1 \u2264 y1) = pr{n (yr , yr\u22121) = 1, . . . , n (y2, y1) = 1}.\n\nthe first of these identities recovers (6.34), while the joint density of y 1 \u2265 \u00b7\u00b7\u00b7 \u2265 y r at\ny1 \u2265 \u00b7\u00b7\u00b7 \u2265 yr is obtained either by differentating the second identity or from (6.31),\nwith s replaced by [t1, t2) \u00d7 [yr ,\u221e). both routes show that the limiting joint density\nof the r largest values is\n\n(cid:9)\n\n\u03c4\u22121\n\n1 + \u03be\n\n(cid:10)\u22121/\u03be\u22121\n\nyi \u2212 \u03b7\n\n\u03c4\n\n+\n\nr(cid:4)\ni=1\n\n(cid:6)\n\n(cid:9)\n\n\u00d7 exp\n\n\u2212\n\n1 + \u03be\n\n(cid:8)\n\n(cid:10)\u22121/\u03be\n\nyr \u2212 \u03b7\n\n\u03c4\n\n+\n\n.\n\n(6.37)\n\nindependence of counts in disjoint subsets implies that data for different years may\nbe treated as independent, so an overall likelihood based on the r largest values for\neach year is simply the product of such terms for all t0 years.\nin many ways a more satisfactory approach to inference starts by noticing that\n(6.36) has form \u00011{[t1, t2]}\u00012{[y,\u221e)}, implying that the points result from two\nindependent poisson processes, one giving the random \u2018times\u2019 t at which xi >\nm (xi \u2212 bm) ofthese xi . the times of\n\u22121\nu, and the other giving the rescaled sizes a\n\n "}, {"Page_number": 296, "text": "284\n\n6 \u00b7 stochastic models\nexceedances fall according to a homogeneous poisson process of intensity \u03bb1(t) =\n{1 + \u03be(u \u2212 \u03b7)/\u03c4}\u22121/\u03be+ \u2261 \u03bb, say, while their sizes follow an inhomogeneous poisson\nprocess whose intensity is\n\n\u03bb2(y) = \u2212 d\u00012{[y,\u221e)}\n\ndy\n\n= \u03c4\u22121\n\n1 + \u03be\n\ny \u2212 \u03b7\n\n\u03c4\n\n,\n\ny > u.\n\n+\n\n(cid:9)\n\n(cid:10)\u22121/\u03be\u22121\n\nthis implies that the number of exceedances over level u has a poisson distribution\nwith mean \u03bbt0, and conditional on nu exceedances, their sizes w j = x j \u2212 u are a\nrandom sample of size nu from the generalized pareto distribution (problem 6.15)\n\nthe log likelihood (6.31) may be written as\n(cid:5)(\u03bb, \u03c3, \u03be) \u2261 nu log \u03bb \u2212 t0\u03bb \u2212 nu log \u03c3 \u2212\n\n\u22121/\u03be+\n\n,\n\n1 \u2212 (1 + \u03bew /\u03c3 )\n1 \u2212 exp(\u2212w /\u03c3 ),\n(cid:9)\n\n+ 1\n\n1\n\u03be\n\n\u03be (cid:7)= 0,\n\u03be = 0.\n\u2019\n\n(cid:10) nu(cid:7)\n\nlog\n\nj=1\n\n(6.38)\n\n(\n\n1 + \u03be\n\nw j\n\u03c3\n\n.\n\n(6.39)\n\n(cid:21)\n\ng(w) =\n\nwe apply this discussion by taking a threshold u over which the poisson approxi-\nmation seems to hold; then the exceedance times should be a homogeneous poisson\nprocess, and their sizes should follow (6.38), as typically assessed by a probability\nplot. if the fit is satisfactory, estimates and standard errors are obtained by our usual\nlikelihood methods. as with the generalized extreme-value distribution, estimation\nof \u03c3 and \u03be is not regular if \u03be \u2264 \u22121/2, and example 4.43 is again relevant.\n\nwe now briefly discuss the choice of u. if it ischosen so that the number of\nexceedances is small, then the poisson process approximation to the extremes may\nbe good, but the parameter estimators will have large variance. the variance can be\nreduced by lowering u, but at the cost of bias because the poisson approximation for\nextremes cannot be expected to give good inferences when applied to the bulk of the\ndata. formal procedures for choosing u attempt to trade off these two aspects, but in\npractice graphical approaches are more common. these rest on the threshold stability\nproperty of a random variable w following (6.38), that is,\n\npr(w > w | w > u) = {1 + \u03be(w \u2212 u)/\u03c3u}\u22121/\u03be , w \u2265 u \u2265 0,\n\nwhere \u03c3u = \u03c3 + \u03beu. the operation of thresholding by considering only the tail of\nw above u yields another random variable wu = w \u2212 u, say, following (6.38) but\ntransforms the parameters as (\u03c3, \u03be) (cid:24)\u2192 (\u03c3 + \u03beu, \u03be). when \u03be = 0 this is the lack-of-\nmemory property of the exponential distribution.\none graphical approach uses the fact that e(w ) = \u03c3/(1 \u2212 \u03be) provided \u03be < 1, so\ne(wu | w > u) = (\u03c3 + \u03beu)/(1 \u2212 \u03be), for u \u2265 0. thus if the generalized pareto ap-\nproximation is adequate for the upper tail of a random sample x1, . . . , xn, agraph\nagainst u of the empirical version of this conditional mean, given by\n\n(x j \u2212 u)i (x j > u), where\n\ni (x j > u),\n\n(6.40)\n\nnu = n(cid:7)\n\nj=1\n\nn(cid:7)\nj=1\n\n\u22121\nu\n\nn\n\n "}, {"Page_number": 297, "text": "6.5 \u00b7 point processes\n\n285\n\nfigure 6.18 analysis of\ndanish fire data. upper\nleft: mean residual life\nplot, with 95% confidence\nband (dots) and number of\nexceedances nu at the foot\nof the panel. upper right\nand lower left: plots of\n\n(cid:1)\u03c3u \u2212(cid:1)\u03beuu and(cid:1)\u03beu against\n\nthreshold u, with 95%\nconfidence bands. lower\nright: exponential\nprobability plot of\nresiduals\n\n(cid:1)\u03be\u22121 log(1 +(cid:1)\u03be w j /(cid:1)\u03c3 ).\n\ne\n\nf\ni\nl\n \nl\n\na\nu\nd\ns\ne\nr\n \n\ni\n\nn\na\ne\nm\n\nr\ne\n\nt\n\ne\nm\na\nr\na\np\n\n \n\ne\np\na\nh\ns\n\n0\n5\n\n0\n4\n\n0\n3\n\n0\n2\n\n0\n1\n\n0\n\n5\n\n.\n\n1\n\n0\n\n.\n\n1\n\n5\n\n.\n\n0\n\n0\n\n.\n\n0\n\n0\n2\n\n0\n1\n\n0\n\n0\n2\n-\n\n0\n4\n-\n\nr\ne\n\nt\n\ne\nm\na\nr\na\np\n\n \n\nl\n\ne\na\nc\ns\n\n878 292 151 110 82 61 50 36 28 24\n\n878 292 151 110 82 61 50 36 28 24\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nthreshold\n\nthreshold\n\nl\n\ni\n\na\nu\nd\ns\ne\nr\n\n.\n\n.\n\n.\n\n6\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\n............................................................................................................................................................................................................................... .\n\n878 292 151 110 82 61 50 36 28 24\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\nthreshold\n\nexponential plotting positions\n\nshould be a straight line of gradient \u03be /(1 \u2212 \u03be). the idea is to take the threshold to be\nanother approach to choosing u uses the fact that if (cid:1)\u03beu and (cid:1)\u03c3u are maximum\nthe smallest u above which this mean residual life plot appears linear.\nlikelihood estimators based on the nu positive exceedances x j \u2212 u over u, and if\nthe generalized pareto approximation holds, then(cid:1)\u03beu and(cid:1)\u03c3u \u2212(cid:1)\u03beuu should estimate \u03be\nand \u03c3 for all u. thus graphs of(cid:1)\u03beu and(cid:1)\u03c3u \u2212(cid:1)\u03beuu against u should be constant above a\n\ncertain point, and this is the minimum threshold for which it is reasonable to apply the\napproximation. interpretation of such graphs is aided by adding confidence intervals.\n\nexample 6.34 (danish fire data)\nin example 6.31 we saw that exceedance times\nfor the data in the upper right panel of figure 6.14 seem to follow a homogeneous\n\u22121. for threshold modelling we first choose\nthe threshold u. figure 6.18 shows the mean residual life plot and values of(cid:1)\u03c3u \u2212(cid:1)\u03beuu\npoisson process with rate about 0.06 days\nand(cid:1)\u03beu plotted against u. the mean residual life plot is roughly linear from u = 7\n\nonwards, and its positive slope suggests that \u03be > 0. the other two plots do not tend\nto constants, but in each case the confidence intervals are wide enough to contain a\nconstant above about u = 5. for illustration we take u = 5, let w j = y j \u2212 u denote\nthe 254 claims that exceed u = 5 units, and fit the generalized pareto distribution\n\n "}, {"Page_number": 298, "text": "286\n\n6 \u00b7 stochastic models\n(6.38) to the w j . the maximum likelihood estimates are(cid:1)\u03c3 = 3.809 and(cid:1)\u03be = 0.632,\nwith standard errors 0.464 and 0.111 from observed information. the value of (cid:1)\u03be\ncorresponds to a very heavy upper tail for w = y \u2212 u.\nthe form of (6.38) shows that \u03be\u22121 log(1 + \u03be w/\u03c3 ) has a standard exponential\ntial probability plot of the residuals(cid:1)\u03be\u22121 log(1 +(cid:1)\u03bew j /(cid:1)\u03c3 ), shown in the left panel of\ndistribution, so the fit of the model for exceedances can be assessed by an exponen-\n\nfigure 6.18. the distribution fits fairly well but not perfectly.\n\nestimates and confidence regions for quantities of interest such as return levels\nare found in ways analogous to example 6.33. in practice it is important to vary the\n(cid:1)\nthreshold to see if the conclusions depend strongly on u.\n\nin applications the underlying variables are typically neither identically distributed\nnor independent. for concreteness, consider using daily temperature data to model\nthe occurrence of hot days at a site in england. these will occur in the summer\nmonths, so one way to proceed is to retain only the data for june, july, and august,\nto suppose that over this period the temperature distribution is roughly constant, and\nthen to hope that about 90 rather than 365 days of data will suffice for the point\nprocess paradigm to be applicable. however, even if the summer data are roughly\nstationary, they will display short-term correlation owing to clustering of hot days.\nsome detailed mathematics establishes that if extremes far apart are asymptotically\nindependent and the data are stationary \u2014 so that in particular all the xi have the same\nmarginal distribution \u2014 then the poisson process representation with intensity (6.36)\nstill applies, but now to the largest value in a cluster. clusters then occur at the times\nof a homogeneous poisson process, but the cluster size is random and its distribution\ndepends on the local dependence of the xi . this leads to the practical issues of\nidentifying clusters from data, and of modelling their properties, which are topics of\ncurrent research.\n\n6.5.3 more general models\nin a poisson process events in disjoint intervals are independent. in practice point\nprocess data can show complex dependencies, so this property must be weakened for\nrealistic modelling. this weakening can be done in many ways and below we merely\nsketch a few possibilities. we continue to suppose that the process is orderly, so events\ncannot coincide.\nlet ht denote the entire history of the process up to time t, that is, the positions of\nall the points in (\u2212\u221e, t], and define the complete intensity function to be\n\n\u03bbh(t) = lim\n\u03b4t\u21920\n\n\u22121pr{n (t, t + \u03b4t) > 0 | ht} ;\n\n(\u03b4t)\n\nthis is the intensity of arrival of points just after t, given the history to t. it isakin\nto the hazard function of section 5.4, but here potentially dependent on the entire\nhistory of the process. the requirement of orderliness is that\npr{n (t, t + \u03b4t) > 1 | ht} = o(\u03b4t)\n\n "}, {"Page_number": 299, "text": "287\n\n6.5 \u00b7 point processes\nfor all t and all possibleht . the complete intensity must be uniquely defined and well-\nbehaved for any possible ht and must moreover determine the probabilistic structure\nof the process. we shall take this for granted here, though a careful mathematical\nargument is needed in a formal discussion.\nnow consider the probability of no event in (w , w + t] conditional on hw .\nwe divide (w , w + t] into disjoint subintervals ii = (w + i \u03b4t, w + (i + 1)\u03b4t], i =\n0, . . . ,k \u2212 1, where \u03b4t = t /k, and note that\npr{n (w , w + t) = 0 | hw} .= k\u22121(cid:4)\n= k\u22121(cid:4)\n\npr{n (ii ) = 0 | hw+i \u03b4t}\n\n{1 \u2212 \u03bbh (w + i \u03b4t) \u03b4t + o(\u03b4t)} ,\n\ni=0\n\nwhere hw+i \u03b4t represents hw followed by no events up to time w + i \u03b4t. the argument\nleading to (6.28) applies with \u03bb(u) replaced by \u03bbh(u), so\nw+t\n\n(cid:21)\n\n(cid:26)\n\n\"\n\npr{n (w , w + t) = 0 | hw} = exp\n\n\u2212\n\n\u03bbh(u) du\n\n,\n\nw\n\nand the probability density that the first point subsequent to w is at t, givenh w , is\n\u2212dpr{n (w , w + t) = 0 | hw} /dt. atleast in principle, this enables the likelihood\nfor points in an interval (0, t0], conditional on h0, to bewritten down by extending\nour arguments for the poisson process, giving\n\ni=0\n\nn(cid:4)\nj=1\n\n\u03bbh(t j ) exp\n\n(cid:21)\n\n\"\n\n\u2212\n\n(cid:26)\n\nt0\n\n0\n\n\u03bbh(u) du\n\n(6.41)\n\nt\n0\n\nas the likelihood based on events at t1, . . . ,t n when the process is observed over\n(0, t0]. in practice it is often hard to specify a tractable but realistic form for \u03bbh(t).\nand we write \u0001h(t) = (cid:24)\na useful implication is that if events are observed at times 0 < t1 < \u00b7\u00b7\u00b7 < tn < t0\n\u03bbh(u) du, then the transformed times \u0001h(t1), . . . , \u0001h(tn)\nform a poisson process of unit rate on (0, \u0001h(t0)], the transformation \u0001h being\nmated (cid:1)\u0001h.\nrandom. thus our earlier tools may be used to check the adequacy of an esti-\nexample 6.35 (poisson process) the complete intensity function for a poisson\nprocess may depend on t, but not on the history of the process. thus \u03bbh(t) = \u03bb(t),\n(cid:1)\nwhich is a constant \u03bb for a homogeneous process.\n\nexample 6.36 (renewal process) the inter-event intervals in a homogeneous\npoisson process are independent exponential variables. the renewal process gener-\nalizes this to possibly non-exponential intervals and is a standard model in reliability\nstudies, where failing components in a system may be immediately replaced by ap-\nparently identical ones, thereby renewing the system. if system failure is identified\nwith failure of the component and the process is stationary then the complete intensity\nfunction depends only on the time since the last event. thus if previous events have\ntaken place at times ti , the complete intensity at time t depends only on v = min(t \u2212 ti )\n\n "}, {"Page_number": 300, "text": "288\n\n6 \u00b7 stochastic models\n\nand has form \u03bb(v). this is the hazard function corresponding to the density of interval\nlengths, f . statistical analysis for such a process is straightforward. time series tools\nsuch as the correlogram and partial correlogram can be used to find serial dependence\namong successive intervals between events, though it may be clear from the context\nthat these are independent. if independent and stationary, they can be treated as a\n(cid:1)\nrandom sample from f and inference performed in the usual way.\n\nexample 6.37 (birth process)\nin a birth process the intensity at time t depends on\nthe number of previous events. assuming that the number n of events up to t is finite,\nthen \u03bbh(t) = \u03b20 + \u03b21n, where \u03b20 > 0, \u03b21 \u2265 0. the complete intensity function is a\nstep function which jumps \u03b21 at each event; if \u03b21 = 0 the process is a homogeneous\n(cid:1)\npoisson process.\n\nbefore giving a numerical example, we briefly describe two functions useful for\nmodel checking and exploratory analysis of stationary processes.\nthe variance-time curve is defined as v (t) = var{n (t)}, for t > 0. a homoge-\nneous poisson process of intensity \u03bb has v (t) = \u03bbt, comparisons with which may be\ninformative. estimation of v (t) isdescribed in problem 6.12.\n\nthe conditional intensity function is defined as\n\nm f (t) = lim\n\u03b4s,\u03b4t\u21920\n\n\u22121pr{n (t, t + \u03b4t) > 0 | n (\u2212\u03b4s, 0) > 0} ,\n\n(\u03b4t)\n\nt > 0,\n\nwhich gives the intensity of events at t conditionally on there being an event at the\norigin. evidently m f (t) = \u03bb for a homogeneous poisson process. an event at time t\nneed not be the first event after that at the origin.\n\nexample 6.38 (japanese earthquake data) figure 6.19 shows the times and mag-\nnitudes of earthquakes with epicentre less than 100km deep in an offshore region west\nof the main japanese island of honsh\u00afu and south of the northern island of hokkaid\u00afo.\nthe figure shows all 483 earthquakes of magnitude 6 or more on the richter scale\nin the period 1885\u20131980, about 5 tremors per year, in one of the most seismically\nactive areas of japan. a cumulative plot of the times rises fairly evenly and suggests\nthat the data may be regarded as stationary; we shall assume this below. we take days\nas the units, giving t0 = 35,175.\nthis is a marked point process, as inaddition to the event times there is a mark \u2014\nthe magnitude \u2014 attached to each event. if we let the times be 0 < t1 < \u00b7\u00b7\u00b7 < tn < t0\nand the associated magnitudes m1, . . . ,m n, their joint density may be written\n\nf (m j | m( j\u22121), t( j))\n\nf (t j | m( j\u22121), t( j\u22121)),\n\n(6.42)\n\nn(cid:4)\nj=1\n\nn(cid:4)\nj=1\n\nwhere t( j\u22121) and m( j\u22121) represent t1, . . . ,t j\u22121 and m1, . . . ,m j\u22121. here we concen-\ntrate on inference for the times using the second term, leaving the magnitudes to\nexamples 10.7 and 10.31. the lower panels of figure 6.19 show the estimated\nvariance-time curve and conditional intensity function for the times, which are are\nclearly far from poisson. the variance-time curve grows more quickly than for\na poisson process, indicating clustering of events, and this is confirmed by the\n\n "}, {"Page_number": 301, "text": "6.5 \u00b7 point processes\n\n289\n\nfigure 6.19 japanese\nearthquake data (ogata,\n1988). the upper panel\nshows the times and\nmagnitudes (richter\nscale) of 483 shallow\nearthquakes. lower left:\nestimated variance-time\ncurve for earthquake\ntimes, with theoretical line\nfor a poisson process\n(solid) and two-sided 95%\nand 99% pointwise\nconfidence limits (dots).\nlower right: estimated\nconditional intensity, with\nbaseline for poisson\nprocess (solid) and\ntwo-sided 95% pointwise\nconfidence limits (dots).\n\ne\nd\nu\n\nt\ni\n\nn\ng\na\nm\n\n5\n\n.\n\n8\n\n0\n\n.\n\n8\n\n5\n\n.\n\n7\n\n0\n7\n\n.\n\n5\n\n.\n\n6\n\n0\n\n.\n\n6\n\ne\nc\nn\na\ni\nr\na\nv\n\n0\n0\n5\n\n0\n0\n3\n\n0\n0\n1\n\n0\n\n0\n\n10000\n\n20000\n\n30000\n\n\u2022 \u2022 \u2022 \u2022 \u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022 \u2022\n\n\u2022\n\n\u2022\u2022\u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022\n\ny\na\nd\n\n \nr\ne\np\n\n \ns\nt\n\nn\ne\nv\ne\n\n \nf\n\no\n\n \nr\ne\nb\nm\nu\nn\n\n5\n1\n\n.\n\n0\n\n0\n1\n0\n\n.\n\n5\n0\n\n.\n\n0\n\n0\n\n.\n\n0\n\n0\n\n1000\n\n3000\n\n5000\n\n0\n\n50\n\n100\n\n150\n\n200\n\ntime (days)\n\nlag (days)\n\nconditional intensity: for about 2\u20133 months after each shock the probability of another\nis increased.\n\none possible model for such data is a self-exciting process in which\n\n(cid:7)\n\n\u03bbh(t) = \u00b5 +\n\nw(t \u2212 t j ),\n\nj:t j <t\n\nwhere \u00b5 is a positive constant and w(u) isnon-negative for u > 0 and otherwise zero.\nhere the intensity at any time is affected by the occurrence of previous events; often\nw(u) ismonotonic decreasing, so recent events affect the current intensity more than\ndistant ones. this may be interpreted as asserting that events occur in clusters, whose\ncentres occur as a poisson process of rate \u00b5. subsidiary events are then spawned\nby the increase in intensity that occurs due to the superposition of the w(t \u2212 t j ) for\nprevious events. seismological considerations suggest letting this function depend on\nm j also, taking\n\nw(t \u2212 t j ; m j ) = \u03bae\u03b2(m j\u22126)\n(t \u2212 t j + \u03b3 )\u03c1\n\n,\n\nt > t j ,\n\nwhere \u03c1, \u03b3 , \u03ba, \u03b2, \u00b5 > 0, with \u03b2\ndepends not only on the time since an event but also on its magnitude.\n\n.= 2. under this formulation the increase in intensity\n\n "}, {"Page_number": 302, "text": "figure 6.20\njapanese\nearthquake data fit. the\nupper panel shows the\n\nestimated intensity(cid:1)\u03bbh(t)\nevents/day with(cid:1)\u00b5 (dots)\n\nand the mean intensity\n(dashes). the tick marks\nat the top of panel show\nthe event times. lower\nleft: estimated cumulative\n\nnumber of events (cid:1)\u0001h(t j )\n\n(solid) and two-sided 95%\nand 99% overall\nconfidence limits (solid\ndiagonal), based on the\nkolmogorov\u2013smirnov\nstatistic; the dotted line\nshows perfect fit of the\nmodel. lower right:\nvariance-time function for\ntransformed process\n\n(cid:1)\u0001h(t j ) (blobs), with\n\nbaseline for poisson\nprocess (solid) and\ntwo-sided 95% and 99%\npointwise confidence\nlimits (dots)\n\n6 \u00b7 stochastic models\n\n290\n\ny\nt\ni\ns\nn\ne\n\nt\n\nn\n\ni\n \n\nd\ne\n\nt\n\na\nm\n\ni\nt\ns\ne\n\n0\n5\n0\n\n.\n\n0\n\n5\n0\n0\n\n.\n\n0\n\n0\n\n10000\n\n20000\n\n30000\n\ntime (days)\n\n0\n0\n5\n\n0\n0\n4\n\n0\n0\n3\n\n0\n0\n2\n\n0\n0\n1\n\n0\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\ne\nc\nn\na\ni\nr\na\nv\n\n0\n5\n2\n\n0\n0\n2\n\n0\n5\n1\n\n0\n0\n1\n\n0\n5\n\n0\n\n\u2022\u2022\u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022\n\ns\nk\nc\no\nh\ns\n \nf\n\no\n\n \nr\ne\nb\nm\nu\nn\n\n \n\ne\nv\ni\nt\n\nl\n\na\nu\nm\nu\nc\n\n0\n\n100 200 300 400\n\n0\n\n20\n\n40\n\n60\n\n80 100\n\ntransformed time\n\ntime (days)\n\nthe log likelihood (6.41) corresponding to the second term of (6.42) with the\nself-exciting model is readily obtained. its maximized value is \u22122232.01, but this\nchanges only to \u22122232.25 on fixing \u03c1 = 1. with this restriction the estimates and\nstandard errors are (cid:1)\u00b5 = 0.0049 (0.0007) events/day,(cid:1)\u03ba = 0.020 (0.003) events/day,\n(cid:1)\u03b3 = 0.054 (0.024) days, and(cid:1)\u03b2 = 1.61 (0.14). these imply that after an earthquake\nof size m j = 6, \u03bbh (t) jumps by(cid:1)\u03ba/(cid:1)\u03b3\n.= 0.37 events/day, while a shock of size m j = 8\ninduces a jump of(cid:1)\u03bae2(cid:1)\u03b2 /(cid:1)\u03b3\n.= 9.2 events/day. the rate at which clusters arise is about\n365(cid:1)\u00b5\nthe top panel of figure 6.20 shows the fitted intensity(cid:1)\u03bbh(t), with the value of (cid:1)\u00b5\nand the mean intensity; note the logarithmic scale. the fitted value is initially low\nperhaps because of the lack of data before t = 0, and it would be preferable to use\ncumulative intensity for the transformed process (cid:1)\u0001h(t j ), which would be a straight\nonly a portion of the likelihood, as in example 6.29. the lower panels show the\n\n.= 1.8 events/year, so each gives rise to a further 3.2 shocks on average.\n\nline of unit gradient if the model fitted perfectly. the cumulative intensity lies within\noverall 95% confidence limits and gives no evidence against the model. however the\nvariance-time curve of the transformed times shows clear overdispersion relative to\na poisson process. the data include an unusual series of about 25 large earthquakes\nin november\u2013december 1938, all occurring in the same region. when these are\n\n "}, {"Page_number": 303, "text": "recall that\n(1 + a/k)k \u2192 ea as\nk \u2192 \u221e.\n\ndeletion of points of a\nprocess is known as\nthinning.\n\n6.5 \u00b7 point processes\n\n291\n\nremoved, the remainder have variance-time curve falling within the poisson limits\n(cid:1)\nand the model then seems adequate.\n\nexercises 6.5\n1 for a poisson process on [0, t0] ofconstant rate \u03bb, show directly that n (t0) has a poisson\n\npr{n (t0) = m} .=\n\ndistribution of mean \u03bbt0 by showing that\nk!\nm!(k \u2212 m)!\nwhere \u03b4t = t0/k, and letting k \u2192 \u221e.\n\"\n\n\"\n\n\"\n\n2 check that\n\nt0\n\ndtn\n\ntn\n\ndtn\u22121 \u00b7\u00b7\u00b7\n\nt3\n\n{\u03bb\u03b4t + o(\u03b4t)}m {1 \u2212 \u03bb\u03b4t + o(\u03b4t)}k\u2212m ,\n\n\"\n\ndt2\n\nt2\n\ndt1 \u03bb(t1)\u00b7\u00b7\u00b7\u03bb(t n)e\n\n\u2212\u0001(t0)\n\nequals (6.30).\n\n0\n\n0\n\n0\n\n0\n\n3 consider a poisson process of intensity \u03bb in the plane. find the distribution of the area of\n\nthe largest disk centred on one point but containing no other points.\n\n4 show that the time to the rth event in a poisson process of rate \u03bb has the gamma distribution.\n5 if t is the time to the first event in a one-dimensional poisson process of positive intensity\n\u03bb(t), show that \u0001(t ) has a standard exponential distribution.\nwrite down an algorithm to generate the points 0 < t1 < \u00b7\u00b7\u00b7 < tn < t0 of a poisson\nprocess of rate \u03bb(t) on [0, t0]. test it.\n\n6 over the centuries natural disasters in a particular country have occurred as a poisson\nprocess of rate \u03bb(t). any disaster at time t is known to have occurred only with proba-\nbility \u03c0(t), due to the patchiness of historical records. if records of different disasters are\npreserved independently, show that the point process of known disasters is poisson with\nintensity \u03bb(t)\u03c0(t).\n7 find sequences {am} > 0 and {bm} such that (6.33) holds in the following cases: (i)\n1 \u2212 f(x) = e\n\u2212x for x > 0; (ii) the distribution has a power-law upper tail, 1 \u2212 f(x) \u223c\n\u2212\u03b3 , \u03b3 > 0, with x0 = \u221e; and (iii) f(x) = x for 0 \u2264 x \u2264 1.\nx\nin each case give the value of \u03ba and sketch the limiting distribution.\n\n8 let mn be the maximum of the random sample x1, . . . , xn from a distribution f, and\n\nsuppose that the limit\n\n(cid:10)\n\nmn \u2212 bn\n\nan\n\n\u2264 y\n\nlim\nn\u2192\u221e pr\n\n(cid:9)\n\n(cid:10)\n\nis a nondegenerate distribution function, h(y), for some sequences of constants an > 0\nand bn. show that\n\n(cid:9)\n\npr\n\nmn \u2212 an\n\nbn\n\n\u2264 y\n\n= pr\n\n(cid:9)\n\nmm \u2212 an\n\nbn\n\n(cid:10)l\n\n\u2264 y\n\n,\n\nwhere n = ml, and deduce that h must be max-stable, that is, for any l there must exist\nconstants cl and dl such that h(y)l = h(cl + dl y). verify that the generalized extreme-\nvalue distribution (6.34) is max-stable.\n\n\u2019\n\n9 show that the fisher information for an observation from (6.38) is\n\ni(\u03c3, \u03be) = (1 + 2\u03be)\n\u22121\n\n1\n(1 + \u03be)\n\n\u22121\n\n(1 + \u03be)\n\u22121\n2(1 + \u03be)\n\u22121\n\n\u03be > \u22121/2.\n\n,\n\nwhat happens if \u03be \u2264 \u22121/2?\n10 (a) if w follows (6.38) and u > 0, show that conditional on w > u, w \u2212 u follows (6.38)\nwith parameters \u03be and \u03c3u = \u03c3 + \u03beu. show also that e(w \u2212 u | w > u) = \u03c3/(1 \u2212 \u03be),\nprovided \u03be < 1. what happens if \u03be \u2265 1? and if \u03be \u2265 1/2?\n\n(\n\n "}, {"Page_number": 304, "text": "292\n\n6 \u00b7 stochastic models\n\n(b) derive a standard error for (6.40). for what values of \u03be is it valid? explain the saw-tooth\nform of the mean residual life plot.\n\n(c) discuss how confidence bands in plots of (cid:1)\u03beu and (cid:1)\u03c3u \u2212(cid:1)\u03beuu against u might be\n11 by reparametrizing (6.38) in terms of \u03b6 = \u03be /\u03c3 and \u03be, show how to obtain maximum\nlikelihood estimates of \u03be and \u03c3 based on a random sample w1, . . . ,w n from g, using only\na one-dimensional maximization.\n\nconstructed.\n\n6.6 bibliographic notes\n\na useful general account of stochastic modelling dealing with several of the topics in\nthis chapter is isham (1991).\n\nthere are many books on markov chains. cox and miller (1965), grimmett and\nstirzaker (2001) and norris (1997) give standard accounts of their probabilistic as-\npects, while billingsley (1961) describes inference for them. guttorp (1995) has a nice\nblend of probabilistic and statistical considerations. multi-state modelling, including\nthe use of markov processes, is discussed in chapters 5 and 6 of hougaard (2000).\nmacdonald and zucchini (1997) and k\u00a8unsch (2001) describe inference for hidden\nmarkov processes. prum et al. (1995) describe a systematic approach to finding words\nin dna sequences, with further references to this area.\n\nmarkov random fields emerged around 1970 as a natural generalization of markov\nchains to more complex phenomena, though the ising and related models had been\nknown to physicists since the 1920s. the key result relating markov random fields\nand gibbs distributions was proved in 1971 by j. m. hammersley and p. clifford but\nnot published at that time; clifford (1990) describes its history and some more recent\nideas and gives their version of the proof. a simpler proof was given in the important\npaper of besag (1974), which discusses a wide range of topics related to spatial\nmodelling; see smith (1997). applications to image analysis were described in geman\nand geman (1984) and besag (1986), which strongly influenced later work on image\nanalysis; see for example chellappa and jain (1993). applications to point processes\nare reviewed by isham (1981), while kinderman and snell (1980) give a gentle\nintroduction oriented towards problems of classical physics; see also br\u00b4emaud (1999).\nsheehan (2000) and thompson (2001) discuss applications in statistical genetics, with\nnumerous further references.\n\ngraphical models have played an increasingly important role in statistics since\nabout 1980, though similar ideas were used in other fields decades earlier. edwards\n(2000) gives an applied account of graphical models with many examples, and in-\ncludes a description of the software package mim with which certain families of\nmodels can be fitted. lauritzen (1996) is more mathematical, with details of the nec-\nessary graph theory and its statistical application. whittaker (1990) lies between the\ntwo, with a blend of applications and theory, while cox and wermuth (1996) give a\ngeneral view of the subject with some substantial applications. all these books con-\ntain references to the primary literature. those by lauritzen and cox and wermuth\ndescribe graphs in which different types of edges appear; see also wermuth and\nlauritzen (1990) and lauritzen and richardson (2002).\n\n "}, {"Page_number": 305, "text": "6.7 \u00b7 problems\n\n293\n\ngraphical representations of probabilistic expert systems are described by\nlauritzen and spiegelhalter (1988) and spiegelhalter et al. (1993), from which\nexample 6.16 is taken. pearl (1988), neopolitan (1990), almond (1995), castillo\net al. (1997), cowell et al. (1999) and jensen (2001) provide fuller accounts.\n\nthere are books on multivariate statistics at all levels and in all styles. accounts\nof classical models for multivariate data are anderson (1958), mardia et al. (1979),\nand seber (1985). chatfield and collins (1980) is more practical, but all predate the\nemergence of graphical gaussian modelling. the bibliographic notes for chapter 10\ngive references for discrete multivariate data.\n\nchatfield (1996), diggle (1990) and brockwell and davis (1996) are standard ele-\nmentary books on time series, while brockwell and davis (1991) is a more advanced\ntreatment. beran (1994) and tong (1990) describe respectively series with long-range\ndependence and nonlinearity. with the growth of financial markets over the last two\ndecades financial time series has become an area of major research effort summa-\nrized by shephard (1996); for longer accounts see gouri\u00b4eroux (1997) and tsay (2002).\nthese references primarily describe modelling in the so-called time domain, in which\nrelationships among the observations themselves are central, but a complementary\napproach based on frequency analysis is the main focus of bloomfield (1976), priest-\nley (1981), brillinger (1981), and percival and walden (1993). this second approach\nis particularly useful in physical applications.\n\nthe poisson process is a fundamental stochastic model and its probabilistic aspects\nare described in any of the large number of excellent introductory books on stochastic\nprocesses; see for example grimmett and stirzaker (2001). there are also various\nmore specialised accounts such as in rolski et al. (1999). accounts of point process\ntheory are by cox and isham (1980) and daley and vere-jones (1988). cox and\nlewis (1966) is a thorough account of inference for one-dimensional data, while\nspatial point processes are the focus of diggle (1983). karr (1991) gives a theoretical\naccount of inference for point processes. ripley (1981, 1988) and cressie (1991) are\nmore general accounts of the analysis of spatial data. point processes based on notions\nallied to markov random fields are reviewed by isham (1981), and a fuller treatment\nis given by van lieshout (2000).\n\nstatistics of extremes may be said to have started with fisher and tippett (1928), but\nthe first systematic book-length treatment of the subject was gumbel (1958). modern\naccounts from roughly the viewpoint taken here are smith (1990) and coles (2001),\nwhile embrechts et al. (1997) is a systematic mathematical treatment emphasising\napplications in finance and insurance. the approach using point processes is described\nby smith (1989a). davison and smith (1990) give a thorough treatment of threshold\nmethods. books on probabilistic aspects include leadbetter et al. (1983) and resnick\n(1987).\n\n6.7 problems\n\n1 dataframe alofi contains three-state data derived from daily rainfall over three years at\nalofi in the niue island group in the pacific ocean. the states are 1 (no rain), 2 (up to\n\n "}, {"Page_number": 306, "text": "294\n\n6 \u00b7 stochastic models\n\nfrom\n\n1\n\n11\n12\n13\n\n247\n70\n13\n\nto\n\n2\n\n34\n27\n16\n\nfrom\n\n1\n\n1\n2\n3\n\n106\n41\n8\n\nto\n\n2\n\n86\n32\n16\n\n3\n\n14\n10\n15\n\n3\n\nfrom\n\n1\n\n29\n24\n31\n\n21\n22\n23\n\n86\n29\n17\n\nto\n\n2\n\n27\n35\n17\n\n3\n\nfrom\n\n1\n\n23\n26\n34\n\n31\n32\n33\n\n29\n37\n20\n\nto\n\n2\n\n29\n21\n17\n\n3\n\n17\n13\n32\n\n1\n\n97\n32\n13\n\nto\n\n2\n\n24\n27\n27\n\n3\n\n16\n25\n52\n\n1\n\n60\n27\n13\n\nto\n\n2\n\n13\n35\n45\n\n1\n\n98\n36\n15\n\n3\n\n8\n18\n59\n\nto\n\n2\n\n39\n13\n15\n\n3\n\n12\n18\n25\n\ntable 6.10 counts for\nrainfall data at alofi\n(avery and henderson,\n1999). states are 1 (no\nrain), 2 (up to 5mm rain)\nand 3 (over 5mm). upper:\ntransition counts for\nsuccessive triplets for the\nentire data. lower:\ntransition counts for\nsuccessive pairs for four\nsub-sequences of length\n274.\n\nshow that\n\n5mm rain) and 3 (over 5mm). triplets of transition counts for all 1096 observations are\ngiven in the upper part of table 6.10; its lower part gives transition counts for successive\npairs for sub-sequences 1\u2013274, 275\u2013548, 549\u2013822 and 823\u20131096.\n(a) the maximized log likelihoods for first-, second-, and third-order markov chains fitted\nto the entire dataset are \u22121038.06, \u22121025.10, and \u22121005.56. compute the log likelihood\nfor the zeroth-order model, and compare the four fits using likelihood ratio statistics and\nusing aic. give the maximum likelihood estimates for the best-fitting model. does it\nsimplify to a varying-order chain?\n(b) matrices of transition counts {nirs} are available for m independent s-state chains\nwith transition matrices pi = ( pirs), i = 1, . . . ,m . show that the maximum likelihood\nestimates are(cid:1)pirs = nirs /ni\u00b7s, where \u00b7 denotes summation over the corresponding index.\np1 = \u00b7\u00b7\u00b7 = pm = ( prs) are (cid:1)prs = n\u00b7rs /n\u00b7\u00b7s. deduce that the likelihood ratio statistic to\n\n(cid:5)\ni,r,s nirs log((cid:1)pirs /(cid:1)prs) and give its degrees of freedom.\n\nthe maximum likelihood estimates under the simpler model\n\ncompare these models is 2\n(c) consider the lower part of table 6.10. explain how to use the statistic from (b) to test\nfor equal transition probabilities in each section, and hence check stationarity of the data.\n2 the nematode steinername feltiae is a tiny worm used for biological control of mushroom\nfly larvae. once one has found and penetrated a larva, it kills it by releasing bacteria, but\ndeath is not immediate and other nematodes may also penetrate the larva before it dies. in\nexperiments to assess their effectiveness, m nematodes challenged a single healthy larva.\nlet xt \u2208 {0, . . . ,m } denote the number of nematodes that have invaded the larva at time\nt, and let pr (t) = pr(xt = r), with initial condition p0(0) = 1.\n(a) if the invasion process is modelled as a continuous-time markov process with transition\nprobabilities independent of t, explain why we may write\n\nin which\n\npr(xt+\u03b4t = r + 1 | xt = r) = \u03bbr \u03b4t + o(\u03b4t),\n\nt \u2265 0,\nwhere \u03bbm = 0, and give an interpretation of \u03bbr . deduce that\n\nr = 0, . . . ,m \u2212 1,\n\ndp0(t)\n\ndt\n\n= \u2212\u03bb0 p0(t),\n\ndpr+1(t)\n\n= \u2212\u03bbr+1 pr+1(t) + \u03bbr pr (t),\n\nr = 0, . . . ,m \u2212 1.\n\nif \u03bbr = (m \u2212 r)\u03b2 for some \u03b2 > 0, verify that these equations have solution\n\npt (r) =\n\nand give its interpretation.\n\n{1 \u2212 exp(\u2212\u03b2t)}r exp(\u2212\u03b2t)m\u2212r ,\n\ndt\n\n(cid:9)\n\n(cid:10)\n\nm\nr\n\n "}, {"Page_number": 307, "text": "table 6.11 numbers of\nnematodes invading\nindividual fly larvae for\nvarious initial numbers of\nchallengers (faddy and\nfenlon, 1999).\n\ntable 6.12 numbers of\nsites showing differences\nbetween introns of human\nand owl monkey insulin\ngenes (li, 1997, p. 83).\n\n6.7 \u00b7 problems\n\n295\n\nnumber of fly larvae with r = 0, . . . ,10 invading nematodes\n\nchallengers m\n\n0\n\n1\n\n2\n\n3\n\n4\n\n10\n7\n4\n2\n1\n\n1\n9\n28\n44\n158\n\n8\n14\n18\n26\n60\n\n12\n27\n17\n6\n\n11\n15\n7\n\n11\n6\n3\n\n5\n\n6\n3\n\n6\n\n9\n1\n\n7\n\n6\n0\n\n8\n\n6\n\n9\n\n2\n\n10\n\ntotal\n\n0\n\n72\n75\n73\n76\n218\n\nowl monkey\n\nhuman\n\na\n\nc\n\ng\n\nt\n\na\nc\ng\nt\n\n20\n0\n1\n2\n\n0\n24\n5\n2\n\n0\n5\n45\n0\n\n2\n1\n0\n56\n\n(b) a total of n independent experiments performed with t = 1 (in arbitrary units) gave data\n(m1, r1), . . . ,(m n, rn) shown in table 6.11. thus, for example, of the 72 larvae challenged\nby 10 nematodes, 1 was not penetrated, 8 were penetrated by just one nematode, 12 were\npenetrated by two nematodes, and so forth. show that the corresponding log likelihood\nmay be written as\n\n(cid:5)(\u03b2) = (sm \u2212 sr )\u03b2 + sr log(1 \u2212 e\n\n\u2212\u03b2),\n\nand deduce that \u03b2 has maximum likelihood estimate(cid:1)\u03b2 = log{sm /(sm \u2212 sr )} with standard\nerror [sr /{sm(sm \u2212 sr )}]1/2.\n(c) find the values of (cid:1)\u03b2 and their standard errors for models in which the value of \u03b2 is\n(i) the same for all m and (ii) different for each m. discuss which fits the data better, given\nthat the likelihood ratio statistic to compare them equals 11.2.\n(d) a different model has \u03bbr = (m \u2212 r) exp(\u03b30 + \u03b31r), so the larva\u2019s resistance to penetra-\ntion changes each time it is invaded. what feature of table 6.11 suggests that this model\nmight be better? what difficulties would arise in fitting it?\n\n3 one way to estimate the evolutionary distance between species is to identify sections of\ntheir dna which are similar and so must derive from a common ancestor species. if such\nsections differ at very few sites, the species are closely related and must have separated\nrecently in the evolutionary past, but if the sections differ by more, the species are further\napart. for example, data from the first introns of human and owl monkey insulin genes\nare in table 6.12. the first row means that there are 20 sites with a on both genes, 0 with\na onthe human and c on the monkey, and so on. if all the data lay on the diagonal,\nthis section would be identical in both species. note that even if sites on both genes have\nthe same base, there could have been changes such as (ancestor) a\u2192g\u2192t (human) and\n(ancestor) a\u2192c\u2192a\u2192t (monkey).\nhere is a (greatly simplified) model for evolutionary distance. we suppose that at a time\nt0 in the past the two species we now see began to evolve away from a common ancestor\nspecies, which had a section of dna of length n similar to those we now see. each site on\nthat section had one of the four bases a, c, g, or t, and for each species the base at each\nsite has since changed according to a continuous-time markov chain with infinitesimal\n\n "}, {"Page_number": 308, "text": "296\n\ngenerator\n\n\uf8eb\n\uf8ec\uf8ed\n\n\u22123\u03b3\n\n\u03b3\n\u03b3\n\u03b3\n\ng =\n\n\uf8f6\n\uf8f7\uf8f8 ,\n\n\u03b3\n\n\u22123\u03b3\n\n\u03b3\n\u03b3\n\n\u03b3\n\u03b3\n\n\u22123\u03b3\n\n\u03b3\n\n\u03b3\n\u03b3\n\u03b3\n\n\u22123\u03b3\n\n6 \u00b7 stochastic models\n\n1\n4\n\n\uf8f6\n\uf8f7\uf8f8\n\n\uf8eb\n\uf8ec\uf8ed 1 \u22121 \u22121 \u22121\n1 \u22121 \u22121\n3\n1 \u22121\n3\u22121\n\u22121\n3\u22121\n1\n\nindependent of other sites. that is, the rate at which one base changes into, or is substituted\nby, another is the same for any pair of bases.\n\uf8eb\n(a) check that g has eigendecomposition\n\uf8ec\uf8ed 0\n0\n0 \u22124\u03b3\n0\u22124\u03b3\n0\n0\n0\n\n0\n0\n0\u22124\u03b3\nfind its equilibrium distribution \u03c0, and show that the chain is reversible.\n(b) show that exp(tg) has diagonal elements (1 + 3e\n(1 \u2212 e\nbased on data like those above is proportional to\n\n\u22124\u03b3 t )/4 and off-diagonal elements\n\u22124\u03b3 t )/4. use this and reversibility of the chain to explain why the likelihood for \u03b3\n\n\uf8eb\n\uf8ec\uf8ed 1\n\u22121\n\u22121\n\u22121\n\n1 1\n0 0\n0 1\n1 0\n\n\uf8f6\n\uf8f7\uf8f8 ,\n\n\uf8f6\n\uf8f7\uf8f8\n\n1\n1\n0\n0\n\n0\n0\n0\n\n(1 + 3e\n\n\u22128\u03b3 t0)n\u2212r(1 \u2212 e\n\n\u22128\u03b3 t0)r,\n\nwhere r is the number of sites at which the two sections disagree. hence find an estimate\nand standard error for \u03b3 t0 for the data above.\n(c) show that for each site, the probability of no substitution on either species in period\nt is 1 \u2212 exp(\u22126\u03b3 t), deduce that substitutions occur as a poisson process of rate 6\u03b3 , and\nhence show that the estimated mean number of substitutions per site for the data above is\n0.120.\ndiscuss the fit of this model.\n\nin fact substitutions can be\nof various types, but we do\nnot distinguish them here.\n\n4 let y1, . . . ,y n represent the trajectory of a stationary two-state discrete-time markov\n\nchain, in which\n\npr(y j = a | y1, . . . ,y j\u22121) = pr(y j = a | y j\u22121 = b) = \u03b8ba,\n\na, b = 1, 2;\n\nnote that \u03b811 = 1 \u2212 \u03b812 and \u03b822 = 1 \u2212 \u03b821, where \u03b812 and \u03b821 are the transition probabilities\nfrom state 1 to 2 and vice versa.\n21 (1 \u2212 \u03b821)n22, where nab\nshow that the likelihood can be written in form \u03b8 n12\nis the number of a \u2192 b transitions in y1, . . . , yn. find a minimal sufficient statistic for\n(\u03b812, \u03b821), the maximum likelihood estimates(cid:1)\u03b812 and(cid:1)\u03b821, and their asymptotic variances.\n5 let y(1) < \u00b7\u00b7\u00b7 < y(n) be the order statistics of a sample from the exponential density, \u03bbe\n\u2212\u03bby,\ny > 0, \u03bb >0. show that for r = 2, . . . ,n ,\n(cid:3) = exp\n\n(cid:28)\u2212\u03bbr(y \u2212 y(r\u22121))\n\ny(r) > y | y(1), . . . ,y (r\u22121)\n\n12 (1 \u2212 \u03b812)n11 \u03b8 n21\n\ny > y(r\u22121),\n\n(cid:29)\n\npr\n\n(cid:2)\n\n,\n\nya \u22a5 yb | yd;\n\nand deduce that the order statistics from a general continuous distribution form a markov\nprocess.\n)\n6 let g denote an undirected graph with nodes j and for any a \u2282 j let cl(a) denote the\na\u2208a({a} \u222an a). then we can write the local, global and pairwise markov properties\nset\nas\n(g) if a, b, d is a triple of disjoint sets such that d separates a from b in g, then\n(l) for any node a, ya \u22a5 yj \u2212cl({a}) | yna ;\n(p) if a, b are non-adjacent nodes, then ya \u22a5 yb | yj \u2212{a,b}.\n(a) show that (g) \u21d2 (l) \u21d2 (p).\n(b) we say that y satisfies (f) if the density factorizes according to (6.14) and (6.15).\nshow that (f) \u21d2 (g). interpret the hammersley\u2013clifford theorem as showing that if in\naddition (6.12) holds, then (p) \u21d2 (f).\n7 consider a rectangular grid of pixels with a first-order neighbourhood structure, and\ndenote its random variables by ui j , i, j = 1, . . . ,m . suppose that the observed data are\n\n "}, {"Page_number": 309, "text": "6.7 \u00b7 problems\n\n297\nyi j = ui j + \u03b5i j where \u03b5i j\niid\u223c n (0, \u03c3 2). thus the ui j are observed with noise. give the moral\ngraph for the ui j and yi j . hence show that the local characteristics f (ui j | y, u\u2212i j ) depends\non the neighbouring us and yi j and find f (ui j | y, u\u2212i j ) when the ui j follow an ising model.\n\u03bd . show that\n\n8 (a) suppose that conditional on u = u, y \u223c n p(\u00b5, \u03bdu\n\n\u22121\u0001), where u \u223c \u03c7 2\n(cid:3) {1 + (y \u2212 \u00b5)t\u0001\u22121(y \u2212 \u00b5)/\u03bd}\u2212( p+\u03bd)/2,\n\n(cid:3)|\u0001|\u22121/2\n(cid:2)\n\n(cid:2)\n\np+\u03bd\n2\n\nthe marginal density of y is multivariate t,\n\nf (y; \u00b5, \u0001) = \u0001\n\n(\u03c0 \u03bd) p/2\u0001\n\n\u03bd\n2\n\nand establish that e(u | y = y) = (\u03bd + p)/{\u03bd + (y \u2212 \u00b5)t\u0001\u22121(y \u2212 \u00b5}.\n(b) use this as the basis for an em algorithm for estimation of \u00b5 and \u0001, extending that of\nproblem 5.18.\n(c) the density of y is called elliptical because of the shape of its contours. other such\ndensities may be produced by supposing that y \u223c n p(\u00b5, u\n\u22121\u0001) conditional on u = u\nand letting u \u223c g, where g has support in the positive half-line. what changes to the\nalgorithm in (b) are then needed to produce an em algorithm for estimation of \u00b5 and \u0001?\n(section 5.5.2)\n9 show that the ma(1) models yt = \u03b5t + \u03b2\u03b5t\u22121 and yt = \u03b5t + \u03b2\u22121\u03b5t\u22121 have the same cor-\nrelations and deduce that they are indistinguishable from their correlograms alone. if\nyt = (1 + \u03b2 b)\u03b5t in terms of the backshift operator b, show that \u03b5t may be expressed as a\nlinear combination of yt , yt\u22121, . . . in which the infinite past has no effect only if |\u03b2| < 1.\nthe arma process a(b)yt = b(b)\u03b5t is said to be invertible if the zeros of the polynomial\nb(z) all lie outside the unit disk. show that the ma(1) process is invertible only if |\u03b2| < 1.\ncompare this with the condition for stationarity of the ar(1) model. discuss.\n10 show that strict stationarity of a time series {y j} means that for any r we have\n\ncum(y j1\n\n, . . . ,y jr ) = cum(y0, . . . ,y jr\u2212 j1) = \u03ba j2\u2212 j1,..., jr\u2212 j1 ,\n\n(cid:5)\nsay. suppose that {y j} is stationary with mean zero and that for each r it is true that\n|\u03ba u1,...,ur\u22121| =c r < \u221e.\nthe rth cumulant of t = n\n\nu\n\ncum{n\n\n\u22121/2(y1 + \u00b7\u00b7\u00b7 + yn)} =n\n\nj1,..., jr\n\n\u2212r/2\n\n\u22121/2(y1 + \u00b7\u00b7\u00b7 + yn) is\n(cid:7)\nn(cid:7)\nj1=1\n\u2212r/2\n(cid:7)\n\n= n\n= n \u00d7 n\n\u2264 n1\u2212r/2\n\n\u2212r/2\n\n, . . . ,y jr )\n\ncum(y j1\n(cid:7)\n(cid:7)\n\nj2,..., jr\n\n\u03ba j2\u2212 j1,..., jr\u2212 j1\n\n\u03ba j2\u2212 j1,..., jr\u2212 j1\n\nj2,..., jr\n\n|\u03ba j2\u2212 j1,..., jr\u2212 j1| \u2264n 1\u2212r/2cr .\n\nj2,..., jr\n\n11 (a) check that the gumbel distribution arises from (6.34) in the limit as \u03be \u2192 0.\n\njustify this reasoning, and explain why it suggests that t has a limiting normal distribution\nas n \u2192 \u221e, despite the dependence among the y j .\nobtain the cumulants of t for the ma(1) model, and convince yourself that your argument\nextends to the ma(q) model.\ncan you extend the argument to arbitrary linear combinations of the y j ?\n(b) derive the densities for (6.34) and the gumbel distribution, and plot them for \u03be = \u22121,\n\u22120.5, 0, 0.5, and 1. which do you think is most plausible for extreme rainfall, for high\ntides, and for the fastest times to run a mile?\n(c) write a function that generates random samples from (6.34) by inversion.\n(d) show that the gumbel plotting positions are\u2212 log[\u2212 log{1 \u2212 i /(n + 1)}] and use these\nand your simulation routine to see how easy it is to detect departures from \u03be = 0 inrandom\nsamples of size n = 40 with \u03be = \u22120.3, 0.3. try varying \u03be and n, and write a brief account\nof your conclusions.\n\nthis condition applies to\nmany common models,\nbut excludes those where\nvariables far apart are\nhighly correlated.\n\n "}, {"Page_number": 310, "text": "298\n\n6 \u00b7 stochastic models\n\n12 consider a stationary point process and denote the numbers of counts in successive inter-\nvals (k\u03c4, (k + 1)\u03c4 ] oflength \u03c4 by nk, where k = . . . ,\u22121, 0, 1, . . .. let var(n0) < \u221e and\nset \u03b3 j = cov(n0, n j ).\n(a) show that {n j} is a stationary time series and deduce that\n\nvar{n (m\u03c4 )} = m\u03b30 + 2\n\n(m \u2212 j)\u03b3 j , m = 1, 2, . . . .\n\nm\u22121(cid:7)\nj=1\n\n.=\n\n(cid:21)\n\nhence explain how the variance-time curve v (t) for t = \u03c4, 2\u03c4, . . . may be estimated\nusing the empirical covariances (cid:1)\u03b3 j of counts of data observed over (0, t0]. call the\nestimator (cid:1)v (t).\n(b) if k\u03c4 = t0 and the data follow a poisson process of rate \u03bb, then\n(cid:25)\n\u03bb\u03c4 (2\u03bb\u03c4 + 1)/k + o(k\n(\u03bb\u03c4 )2/k + o(k\n\n(k \u2212 1)\u03bb\u03c4/k,\n0,\n\nj = 0,\notherwise,\n\nvar((cid:1)\u03b3 j ) =\n\ne((cid:1)\u03b3 j )\n\nwhile cov((cid:1)\u03b3i ,(cid:1)\u03b3 j ) = o(k\n(cid:28)(cid:1)v (t)\n\nt /t0)v (t) and\n\n\u22121) when i (cid:7)= j. hence show that in this case e{(cid:1)v (t)} .= (1 \u2212\n(cid:29) = {2/3 + 4/(3m)} (\u03bbt)2(t /t0) + (\u03bbt)(t /t0) + o(\u03c4/t0),\n\nvar\nwhere t = m\u03c4 .\n(c) explain the construction of the lower left panel of figure 6.19.\n\n\u22121),\n\n\u22121),\n\n13 sampling of point processes is not straightforward. if the process is running already\nand sampling begins at an arbitrary time origin, then this origin is likely to fall into\nan interval that is longer than is typical, and this length-biased sampling has knock-\non effects for subsequent intervals unless their lengths are independent. suppose that a\nvery long stretch of n intervals is available from a stationary process with mean interval\nlength \u00b5 and marginal density f (y) for times between events, into which the origin falls\nrandomly. of the total length n\u00b5 of the intervals, a length n f (y) \u00d7 y will be taken by\nintervals of length y. explain why the probability that the origin falls into one of these\nis g(y)dy = ny f (y)dy/(n\u00b5), and hence show that the length of the selected interval has\nprobability density g.\nnow consider the forward recurrence time to the next event starting from the origin. the\norigin having fallen uniformly at random into an interval of length y, the conditional\n\u22121. show that the forward recurrence time\ndensity of its position within that interval is y\nhas density\n\n\" \u221e\n\nx\n\n\u22121g(y) dy = \u00b5\u22121f(x),\n\ny\n\nwhere f is the survivor function of f , and find the density of the backward recurrence\ntime to the point before the origin.\nshow that in a homogeneous poisson process of rate \u03bb the interval into which the origin\n\u2212\u03bby, y > 0, and that the forward and backward recurrence times are\nfalls has density \u03bb2 ye\nboth exponential variables. explain why these results are obvious intuitively.\n14 a poisson process of rate \u03bb(t) onthe set s \u2282 irk is a collection of random points with\n\u0001(a) = (cid:24)\nthe following properties (among others):\nr the number of points na in a subset a of s has the poisson distribution with mean\n(cid:24)\nr given na = n, the positions of the points are sampled randomly from the density\na \u03bb(s) ds, t \u2208 a.\n\na \u03bb(t) dt;\n\n\u03bb(t)/\n\n(a) assuming that you have reliable generators of u (0, 1) and poisson variables, show\nhow to generate the points of a poisson process of constant rate \u03bb on the interval [0, t0].\n(b) let t = (x, y) \u2208 ir2, \u03b7, \u03be \u2208 ir, \u03c4 > 0, \u03bb(x, y) = \u03c4 \u22121 {1 + \u03be(y \u2212 \u03b7)/\u03c4}\u22121/\u03be\u22121. give an\nalgorithm to generate realisations from the poisson process with rate \u03bb(x, y) on\n\ns = {(x, y) : 0\u2264 x \u2264 1, y \u2265 u, \u03bb(x, y) > 0} .\n\n "}, {"Page_number": 311, "text": "table 6.13 times (days)\nbetween successive\nfailures of a piece of\nsoftware developed as part\nof a large data system\n(jelinski and moranda,\n1972). the software was\nreleased after the first 31\nfailures. the last three\nfailures occurred after\nrelease. the data are to be\nread across rows.\n\n6.7 \u00b7 problems\n\n299\n\n9\n3\n\n12\n6\n\n11\n1\n\n4\n11\n\n7\n33\n\n2\n7\n\n5\n91\n\n8\n2\n\n5\n1\n\n7\n87\n\n1\n47\n\n6\n12\n\n1\n9\n\n9\n135\n\n4\n258\n\n1\n16\n\n3\n35\n\n15 show that the likelihood for data (t1, y1), . . . ,(t n, yn) observed in [0, t0] \u00d7 [u,\u221e) and\n\nwith intensity (6.36) is\n\n(cid:9)\n\n\u03c4 \u22121\n\n1 + \u03be\n\ny j \u2212 \u03b7\n\n\u03c4\n\nn(cid:4)\nj=1\n\n(cid:6)\n\n(cid:10)\u22121/\u03be\u22121 \u00d7 exp\n\n\u2212t0\n\n(cid:9)\n\n(cid:8)\n\n(cid:10)\u22121/\u03be\n\n.\n\n1 + \u03be\n\nu \u2212 \u03b7\n\n\u03c4\n\nshow that this may be reparametrized to give (6.39) and that this is the log likelihood\ncorresponding to a decomposition\n\npr(n = n; \u03bb) \u00d7 n(cid:4)\n\ng(w j ; \u03be, \u03c3 ).\n\nj=1\n\ngive the distributions of n , of the w j , and of y = max(w1, . . . , wn ). surprised?\n\n(cid:29)\n\n\u2212\u03b2t )/\u03b2\n\n(cid:28)\u2212\u00b5(1 \u2212 e\n\n16 a computer program has an unknown number of bugs m. each bug causes the program\nto crash, and is then located and (instantaneously!) removed. if the times at which the m\nfailures occur are independent exponential variables with common mean \u03b2\u22121, and if m is\npoisson with mean \u00b5/\u03b2, then show that\npr{n (t) = 0} = exp\n(cid:6)\n\nt \u2265 0.\n(a) deduce that the times of crashes follow a poisson process of rate \u00b5e\nlikelihood when failures occur at times 0 \u2264 t1 < \u00b7\u00b7\u00b7 < tn \u2264 t0 is\n(cid:3)(cid:8)\n1 \u2212 e\n\u2212\u03b2t0\n\nn(cid:7)\nl(\u00b5, \u03b2) = \u00b5n exp\nj=1\nconditional distribution of s = (cid:5)\nand that this is an exponential family model.\n(b) reliability growth occurs if \u03b2 > 0. show that a test for this may be based on the\ntj given that n failures have occurred in [0, t0], and\nthat if \u03b2 = 0, e(s) = nt0/2 and var(s) = nt 2\n(c) we now treat m as a unknown parameter and aim to estimate it. show that\n\n/12. suggest how to perform such a test.\n\n\u2212\u03b2t . show that the\n\nt j \u2212 \u00b5\u03b2\u22121\n\n\u2212\u03b2\n\n(cid:2)\n\n,\n\n,\n\n0\n\nl(m, \u03b2) =\n\nm!\n\n(m \u2212 n)!\n\n\u03b2n exp{\u2212\u03b2t0(m + s/t0 \u2212 n)} ,\n\n\u03b2 > 0, m = n, n + 1, . . . ,\n\nand hence find the profile log likelihood (cid:5)p(m) for m.\n(d) the code below plots (cid:5)p(m) after the first r failures of the data in table 6.13. try\nvarying r up to 30, and observe the shapes taken by the profile log likelihood.\n\ny <-c(9,12,11,4,7,2,5,8,5,7,1,6,1,9,4,1,3,3,\n6,1,11,33,7,91,2,1,87,47,12,9,135,258,16,35)\nl <-function(m,n,s) lgamma(m+1)-lgamma(m-n+1) - n*log(m-n+s)\nr <- 20\ny <-cumsum(y[1:r])\ns <-sum(y)/y[r]\nx <-r:(r+100)\nplot(x,l(x,r,s))\n\n# just take data up to time of rth failure\n\n# plot log likelihood\n\nwhat problems do you see with likelihood inference for this model?\nthe software was released after 31 failures. give the maximum likelihood estimate of m\nat that point, and its confidence interval, if possible.\n(section 6.5.1, jelinski and moranda, 1972)\n\n "}, {"Page_number": 312, "text": "7\n\nestimation and hypothesis testing\n\nchapter 4 introduced likelihood and explored associated concepts such as likelihood\nratio statistics and maximum likelihood estimators, which were then extensively used\nfor inference in chapters 5 and 6. in this chapter we turn aside from the central theme\nof the book and discuss some more theoretical topics. estimation is a fundamental\nstatistical activity, and in section 7.1 we consider what properties a good estimator\nshould have, including a brief discussion of nonparametric density estimators and the\nmathematically appealing topic of minimum variance unbiased estimation. one of\nthe most important approaches to constructing estimators is as solutions to systems of\nestimating equations. in section 7.2 we discuss the implications of this, showing how\nit complements minimum variance unbiased estimation, and seeing its implications\nfor robust estimation and for stochastic processes. we then give an account of some of\nthe main ideas underlying another major statistical activity, the testing of hypotheses,\ndiscussing the construction of tests with good properties, and making the connection\nto estimation.\n\n7.1 estimation\n7.1.1 mean squared error\nsuppose that we wish to estimate some aspect of a probability model f (y). in principle\nwe might try and estimate almost any feature of f , but we largely confine ourselves to\nestimation of the unknown parameter \u03b8 or a function of it \u03c8(\u03b8) in aparametric model\nf (y; \u03b8). suppose that our data y comprise a random sample y1, . . . ,y n from f , and\nlet the statistic t = t(y ) be anestimator of \u03c8(\u03b8). we say that t is unbiased for \u03c8(\u03b8)\nif e(t ) = \u03c8(\u03b8) for all \u03b8, and define the bias of t to be e(t ) \u2212 \u03c8(\u03b8). large bias\nmeans that the long-run average value of t lies far from \u03c8(\u03b8), and this is undesirable.\nthe mean squared error of t is the expected squared distance between t and its\nestimand, which turns out to equal the sum of the variance and squared bias:\n\ne[{t \u2212 \u03c8(\u03b8)}2] = e[{t \u2212 e(t ) + e(t ) \u2212 \u03c8(\u03b8)}2]\n\n= var(t ) + {e(t ) \u2212 \u03c8(\u03b8)}2,\n\n(7.1)\n\n300\n\n "}, {"Page_number": 313, "text": "7.1 \u00b7 estimation\n301\nbecause e [{t \u2212 e(t )}{e(t ) \u2212 \u03c8(\u03b8)}] = 0. mean squared error is a common mea-\nsure of how well t estimates \u03c8(\u03b8). the decomposition (7.1) is useful because in\npractice it helps to know if a large mean squared error is due to a large bias or large\nvariance or both.\n\nexample 7.1 (normal variance) let y1, . . . ,y n be a random sample from the\nestimator (cid:1)\u03c3 2 = n\nnormal distribution with mean \u00b5 and variance \u03c3 2. then \u03c3 2 has maximum likelihood\nn\u22121 distribution, by\n(3.15). as e(v ) = (n \u2212 1) and var(v ) = 2(n \u2212 1),\n\n\u22121\u03c3 2v , where v has a \u03c7 2\n\nj (y j \u2212 y )2 d= n\n\n(cid:2)\n\n\u22121(n \u2212 1)\u03c3 2,\n\nvar((cid:1)\u03c3 2) = n\n\n\u221222(n \u2212 1)\u03c3 4.\n\n\u22121\ne((cid:1)\u03c3 2) = n\n\nhence(cid:1)\u03c3 2 has a negative bias of\n\ne((cid:1)\u03c3 2) \u2212 \u03c3 2 = n \u2212 1\n\n\u03c3 2 \u2212 \u03c3 2 = \u2212 \u03c3 2\nn\n\nn\n\nand mean squared error\n\nn \u2212 1\nn2\n\n2\n\n\u03c3 4 +\n\n(cid:3)\n\n(cid:4)2 = 2n \u2212 1\n\u2212 \u03c3 2\n(cid:2)\nn\nj (y j \u2212 y )2 d= (n \u2212 1)\n\n\u03c3 4.\n\nn2\n\n\u22121\n\n\u03c3 4.\n\nvar(s2) = 2(n \u2212 1)\u03c3 4(n \u2212 1)\n\n\u22121\u03c3 2v , so\n\u22122 = 2\nn \u2212 1\n\nthe usual estimator is s2 = (n \u2212 1)\ne(s2) = (n \u2212 1)\n\u22121\u03c3 2(n \u2212 1) = \u03c3 2,\nhence s2 is unbiased, but its mean squared error is greater than that of (cid:1)\u03c3 2 because\n(2n \u2212 1)/n2 < 2/(n \u2212 1) for all n > 1.\n(cid:1)\n\u22121) inthis example, so at least for large n the\ncontribution that bias squared makes to mean squared error is negligible compared\nto that of variance. this often occurs with parametric estimators, as the following\nargument suggests. suppose that the data are a random sample from a distribution with\nmean \u00b5, variance \u03c3 2, third cumulant \u03ba3, and well-behaved higher cumulants. then\n(2.32) implies that their average y has mean \u00b5, variance \u03c3 2/n, and third cumulant\n\u03ba3/n3/2. suppose also that the estimator of a scalar parameter \u03c8 = t(\u00b5) may be\nwritten as (cid:1)\u03c8 = t(y ), as is true for natural exponential family models, for example;\n\nboth bias and variance are o(n\n\nsee section 5.2. then under mild conditions, taylor series expansion of t(y ) about \u00b5\ngives\n\n(cid:1)\u03c8 = t(\u00b5) + (y \u2212 \u00b5)t\n\n(cid:1)\n\n(\u00b5) + 1\n2\n\n(y \u2212 \u00b5)2t\n\n(cid:1)(cid:1)\n\n(\u00b5) + 1\n6\n\n(y \u2212 \u00b5)3t\n\n(cid:1)(cid:1)(cid:1)\n\n(\u00b5) + \u00b7\u00b7\u00b7 ,\n\nso\n\ne((cid:1)\u03c8) = t(\u00b5) + 1\n\n(cid:1)(cid:1)\n\n\u22121\u03c3 2t\nshowing that the bias of (cid:1)\u03c8 is of order n\n\n(\u00b5) + 1\n6\n\u22121. thus for asymptotic purposes we can\ntypically compare two parametric estimators in terms of their variances, as squared\nbias is of smaller asymptotic order. in finite samples the issue is less clear, because it\n\n(\u00b5) + \u00b7\u00b7\u00b7 ,\n\n\u22123/2\u03ba3t\n\n(cid:1)(cid:1)(cid:1)\n\n2\n\nn\n\nn\n\n "}, {"Page_number": 314, "text": "harald cram\u00b4er\n(1893\u20131985) worked all\nhis life at the university of\nstockholm, researching in\nbiochemistry and number\ntheory before turning to\nprobability and statistics.\nhe was the first swedish\nprofessor of mathematical\nstatistics and actuarial\nscience, and made\nfundamental contributions\nto both subjects. his\nmasterpiece,\nmathematical methods of\nstatistics, was written\nduring world war ii,\nwhen he was largely\nscientifically isolated.\ncalyampudi\nradhakrishnan rao\n(1920\u2013) works at\npennsylvania state\nuniversity. see degroot\n(1987b) for an account of\nhis life and work.\n\n7 \u00b7 estimation and hypothesis testing\n302\nis not useful to be told that e(t ) \u2212 \u03c8\n.= b/n without some idea of\na and b: if a2/n2 (cid:2) b/n for all values of n likely to be met in a particular application,\nthen the bias term in (7.1) predominates and comparison purely in terms of variances\nis unhelpful. nonetheless it is natural to ask if there is a lower bound to the variance\nof estimators of \u03c8.\n\n.= a/n and var(t )\n\ncram\u00b4er\u2013rao lower bound\nsuppose that the density f (y; \u03b8) is regular for maximum likelihood estimation of the\nscalar parameter \u03b8. if t is an unbiased estimator of a scalar \u03c8 = \u03c8(\u03b8), then under\nmild conditions and for all \u03b8,\n\nvar(t ) \u2265 (d\u03c8/d\u03b8)2\n\ni (\u03b8)\n\n,\n\n(7.2)\n\nwhere i (\u03b8) isthe expected information in the sample. the right-hand side of (7.2)\nis the cram\u00b4er\u2013rao lower bound. itfollows that if we can find an unbiased estimator\nthat attains equality or near-equality in (7.2), we need search no further: no unbiased\nestimator could do better. we might also hope that when t has a small bias and its\nvariance is close to the lower bound, it will be difficult to ameliorate.\n\nto establish (7.2), note that as t is unbiased and f (y; \u03b8) is adensity,\nf (y; \u03b8) dy = 1,\n\nt(y) f (y; \u03b8) dy = \u03c8(\u03b8),\n\ne(t ) =\n\n(cid:5)\n\n(cid:5)\n\n(cid:5)\n\n(cid:5)\n\n0 =\n\nfor all \u03b8. ifthe order of integration and differentation can be interchanged, differenta-\ntion of these equations with respect to \u03b8 gives\n\n=\n\nd\u03c8\nd\u03b8\n\nd f (y; \u03b8)\n\ndy =\n\nt(y)\n\nd\u03b8\n\nd log f (y; \u03b8)\n\nt(y)\n\nf (y; \u03b8) dy = e(t u ),\n\nd\u03b8\n\nwhere u = d log f (y ; \u03b8)/d\u03b8 is the score statistic, and\n\n(cid:5)\n\n(cid:5)\n\nd f (y; \u03b8)\n\ndy =\n\nd\u03b8\n\nd log f (y; \u03b8)\n\nf (y; \u03b8) dy = e(u ).\n\nhence cov(t , u ) = e(t u ) \u2212 e(t )e(u ) = d\u03c8/d\u03b8. moreover\n\nd\u03b8\n\n(cid:6)\n\n(cid:7)\n\nvar(u ) = e(u 2) = e\n\n\u2212 d2 log f (y ; \u03b8)\n\nd\u03b8 2\n\n= i (\u03b8)\n\nby (4.33). but the cauchy\u2013schwarz inequality (exercise 2.2.3) gives\n\ncov(t , u )2 \u2264 var(t )var(u ),\n\nor (d\u03c8/d\u03b8)2 \u2264 var(t )i (\u03b8), which entails (7.2). this will apply if maximum likeli-\nhood estimation is regular, for example. equality in (7.2) only occurs when there is\nlinear dependence between t and u , sothat t = b1(\u03b8) + b2(\u03b8)u for all \u03b8 and some\nconstants b1(\u03b8) and b2(\u03b8) (cid:5)= 0.\n\u22121, which equals\nthe inverse fisher information for \u03c8 is i (\u03c8)\nthe right-hand side of (7.2) (problem 4.2). hence the cram\u00b4er\u2013rao lower bound for\n\u22121, for any sample size. however (4.26) implies that in regular\ncases, the large-sample distribution of the maximum likelihood estimator,(cid:1)\u03c8, isnormal\nestimation of \u03c8 is i (\u03c8)\n\n\u22121 = (d\u03c8/d\u03b8)2 i (\u03b8)\n\n "}, {"Page_number": 315, "text": "table 7.1 cram\u00b4er\u2013rao\nlower bound for\nestimation of mean of a\nlog-normal sample of size\nn, when \u00b5 = 0 and\n\u03c3 = 1.5, with properties\nof estimators t = y and\n(cid:1)\u03c8 = exp((cid:1)\u03c3 2/2).\n\n7.1 \u00b7 estimation\n\n303\n\nn\n\n5\n\n10\n\n20\n\n40\n\n80\n\n160\n\n320\n\ncram\u00b4er\u2013rao lower bound\nvar(t )\n\nvar((cid:1)\u03c8)\nbias of (cid:1)\u03c8 (\u00d710\nmean squared error of (cid:1)\u03c8\n\n\u22122)\n\n4.80\n16.11\n296.36\n137.73\n298.26\n\n2.40\n8.05\n7.08\n49.66\n7.32\n\n1.20\n4.03\n1.91\n21.83\n1.96\n\n0.60\n2.01\n0.75\n10.30\n0.76\n\n0.30\n1.01\n0.33\n5.01\n0.34\n\n0.15\n0.50\n0.16\n2.47\n0.16\n\n0.08\n0.25\n0.08\n1.23\n0.08\n\n\u22121. thus (cid:1)\u03c8 is asymptotically unbiased and attains\nwith mean \u03c8 and variance i (\u03c8)\nthe cram\u00b4er\u2013rao lower bound as n \u2192 \u221e: ithas asymptotically the smallest possible\nvariance among unbiased estimators. such an estimator is said to be efficient. this\nsuggests that the asymptotic relative efficiency of t be defined as\n\nvar((cid:1)\u03c8)\n\nvar(t )\n\n= (d\u03c8/d\u03b8)2\ni (\u03b8)var(t )\n\n,\n\ngeneralizing (4.19).\n\nin regular cases var((cid:1)\u03c8) = o(n\n\n\u22121), where n is sample size, and it is impossible to\nfind an unbiased estimator of \u03c8 with variance of smaller order than this. if in addition\nthe observations are identically distributed, the cram\u00b4er\u2013rao lower bound for \u03b8 is\n1/{ni(\u03c8)}, where i(\u03c8) isthe information in a single observation.\n\nexample 7.2 (log-normal mean) a log-normal random variable y may be ex-\npressed as exp(\u00b5 + \u03c3 z), where z is a standard normal variable; thus x = log y has\nthe n (\u00b5, \u03c3 2) distribution. the mean and variance of y are (problem 3.5)\n\ne2\u00b5+\u03c3 2(cid:8)\n(cid:2)\nif it is known that \u00b5 = 0, then the maximum likelihood estimator of \u03c3 2 based on a\nrandom sample y1, . . . ,y n is(cid:1)\u03c3 2\nj , where x j = log y j , and two possible\nestimators for \u03c8 are t = y and (cid:1)\u03c8 = exp((cid:1)\u03c3 2\n/2). now(cid:1)\u03c3 2\nx 2\nd= \u03c3 2v /n, where v has a\n\n\u03c8 = e\u00b5+\u03c3 2/2,\n\ne\u03c3 2 \u2212 1\n\n= n\n\n\u22121\n\n(cid:9)\n\n0\n\n.\n\n0\n\n0\n\nchi-squared distribution on n degrees of freedom, so\n\ne((cid:1)\u03c8r ) = e{exp(r \u03c3 2v /2n)} =(1 \u2212 r \u03c3 2/n)\n\u2212n/2,\nbecause v has moment-generating function (1 \u2212 2t)\n\u2212n/2. this enables exact calcu-\nlation of the bias, variance and mean squared error of (cid:1)\u03c8.\n\nn = 1, 2, . . . ,\n\nevidently t is an unbiased estimator of \u03c8 with variance and mean squared error\n\u03c8,\n\n\u22121 exp(\u03c3 2){exp(\u03c3 2) \u2212 1}. as i (\u03c3 2) = \u03c3 4/(2n) and d\u03c8/d\u03c3 2 = 1\nboth equal to n\nthe first three lines of table 7.1 give values of the lower bound, var(t ) and var((cid:1)\u03c8)\nthe cram\u00b4er\u2013rao lower bound for \u03c8 is \u03c3 4 exp(\u03c3 2)/(2n).\nfor various n when \u03c3 = 1.5, so \u03c8 = exp(\u03c3 2/2) = 9.49. for n \u2265 80 the bound is\neffectively attained by (cid:1)\u03c8, whose bias is always small compared to its variance. for\nn < 80 the bias and variance of (cid:1)\u03c8 decrease rather faster than their asymptotic rate\n\u22121. even when n = 5 the contribution to mean squared error from bias is very small.\nn\nthe unbiased estimator t is much more efficient when n = 5, but otherwise is beaten\n\n2\n\n "}, {"Page_number": 316, "text": "7 \u00b7 estimation and hypothesis testing\n\n304\n\nby (cid:1)\u03c8; its asymptotic relative efficiency is\n\nvar((cid:1)\u03c8)\n\nvar(t )\n\n=\n\n(cid:8)\n\u03c3 4e\u03c3 2\ne\u03c3 2 \u2212 1\n\n(cid:9) =\n\n(cid:8)\n\n2\n\n\u03c3 4\ne\u03c3 2 \u2212 1\n\n(cid:9) .= 0.3\n\n2e\u03c3 2\n\nwhen \u03c3 = 1.5, close to the variance ratio in the last columns.\nexample 7.3 (uniform distribution) let y1, . . . ,y n be a random sample from the\nuniform distribution on (0, \u03b8). the likelihood for \u03b8 is\n\n(cid:1)\n\nl(\u03b8) =\n\n0 \u2264 y1, . . . ,y n \u2264 \u03b8,\notherwise,\n\n(cid:6)\n\n\u03b8\u2212n,\n0,\n\n(cid:6)\n\nor equivalently and in terms of the largest order statistic y(n),\n\nl(\u03b8) =\n\n\u03b8\u2212n,\n0,\n\n0 \u2264 y(n) \u2264 \u03b8,\notherwise,\n\na sketch of which shows that(cid:1)\u03b8 = y(n). hence(cid:1)\u03b8 has distribution\nu < 0,\n0 \u2264 u < 1,\n1 \u2264 u,\n\npr((cid:1)\u03b8 \u2264 u) =\n\n\uf8f1\uf8f2\n\uf8f3 0,\n\n(u/\u03b8)n,\n1,\n\nand it is straightforward to see that\n\nvar((cid:1)\u03b8) =\n\ne((cid:1)\u03b8) = n\u03b8\nn + 1\n\u22122), suggesting a potential problem with the cram\u00b4er\u2013rao lower\n\n(n + 1)2(n + 2)\n\nn\u03b8 2\n\n,\n\n.\n\n0\n\n(cid:5)\n\n(cid:5)\n\n\u2212\n\nt(y)\n\n(cid:5) \u03b8\n\n(cid:5) \u03b8\n\nt(y)\n\u03b8\n\ndy = t(\u03b8)\n\nthis variance is o(n\nbound. in fact in this case\nt(y) f (y; \u03b8) dy = d\nd\nd\u03b8\nd\u03b8\nbecause the limit of the integral depends on \u03b8. here the model is non-regular and the\n(cid:1)\nlower bound does not apply.\nthe lower bound extends to the case where \u03b8 has dimension p and \u03c8 = \u03c8(\u03b8) has\ndimension q \u2264 p. suppose that the q \u00d7 1 statistic t is an unbiased estimator of \u03c8,\nwith q \u00d7 q covariance matrix var(t ). then e(t ) = \u03c8(\u03b8) for all \u03b8, so\u2202\u03c8/\u2202\u03b8 t is a\nq \u00d7 p matrix. an argument analogous to that on page 302 shows that the q \u00d7 q matrix\n\n\u03b8 2 dy (cid:5)=\n\nd f (y; \u03b8)\n\nt(y)\n\nd\u03b8\n\n\u03b8\n\n0\n\ndy,\n\n\u2202\u03b8\nis positive semi-definite. if \u03c8 is scalar, it follows that\n\n\u2202\u03b8 t\n\nvar(t ) \u2212 \u2202\u03c8\n\ni (\u03b8)\n\n\u2202\u03c8 t\n\n\u22121\n\nvar(t ) \u2265 \u2202\u03c8\n\n\u2202\u03b8 t\n\ni (\u03b8)\n\n\u2202\u03c8 t\n\n\u22121\n\n\u2202\u03b8\n\n,\n\n(7.3)\n\nwhich extends (7.2).\n\nexample 7.4 (log-normal mean) expression (4.18) implies that the inverse fisher\ninformation for the parameters of a normal distribution with parameters \u03b8 = (\u00b5, \u03c3 2)t\n\n "}, {"Page_number": 317, "text": "7.1 \u00b7 estimation\n\nfigure 7.1 unbiased\n(left) and maximum\nlikelihood estimates\n(right) of log-normal\nmean based on samples of\nsize n when \u00b5 = 0,\n\u03c3 = 1.5. the horizontal\nlines show the target\nparameter \u03c8. the white\nband in the centre of the\nboxplots indicates the\nmedian of the simulated\nvalues.\n\n0\n2\n\n5\n1\n\nt\n\n0\n1\n\n5\n\n0\n\n305\n\nt\n\na\nh\n\n \ni\ns\np\n\n0\n2\n\n5\n1\n\n0\n1\n\n5\n\n0\n\n5 10 20 40 80 160  320\n\n5 10 20 40 80 160  320\n\n(cid:3)\nbased on a random sample x1, . . . , xn is\n\ni (\u00b5, \u03c3 2)\n\n\u22121 =\n\n\u03c3 2/n\n\n0\n\n0\n\n2\u03c3 4/n\n\n(cid:4)\n\n.\n\n(cid:2)\n\n(cid:4)\n\n(cid:3)\n\nthe maximum likelihood estimators are(cid:1)\u00b5 = x and(cid:1)\u03c3 2 = n\nexp(\u00b5 + \u03c3 2/2) denote the mean of the log-normal variable y = exp(x). then\n\n(x j \u2212 x)2. let \u03c8 =\n\n\u22121\n\ne\u00b5+\u03c3 2/2\n2 e\u00b5+\u03c3 2/2\nand the cram\u00b4er\u2013rao lower bound (7.3) for \u03c8 is n\n\n=\n\n\u2202\u03c8\n\n\u2202\u03b8\n\n1\n\n\u22121(\u03c3 2 + \u03c3 4/2)e2\u00b5+\u03c3 2; this is also\nthe asymptotic variance of the maximum likelihood estimator (cid:1)\u03c8 = exp((cid:1)\u00b5 +(cid:1)\u03c3 2/2).\nas in example 7.2, the exact bias and variance of (cid:1)\u03c8 may be obtained explicitly\nfigure 7.1 shows 1000 simulated values of t and(cid:1)\u03c8 for various n. their appreciable\n\n(problem 7.1).\n\nskewness when n is small shows that their distributions are then far from normal and\ncalls into question our use of bias and variance to compare them. as \u03c8 > 0, a measure\nof relative error such as e{(t \u2212 \u03c8)2}/\u03c8 2 might be preferable.\n(cid:1)\n\n(cid:1)\n\nas an estimator of \u03c8, g(t ) need not be better than g(t\n\nmean squared error is a useful measure with which to compare estimators, but it\nhas the disadvantage of being tied to a particular scale. thus even if t is preferable\nto t\n) when estimating g(\u03c8).\nin particular, if t is unbiased, then g(t ) isunbiased only when g is linear. this is not\ncritical when interest focuses on a quantity for which transformations are irrelevant,\nbut is awkward otherwise.\n\n(cid:1)\n\n7.1.2 kernel density estimation\nthe examples above suggest that estimators in parametric problems can often be com-\npared in terms of their variances, bias being relatively unimportant. bias and variance\nplay more balanced roles in other contexts, as we now illustrate with a discussion\nof nonparametric density estimation. we also take the opportunity of introducing\ncross-validation, which plays a role later.\n\nthe elementary estimator of an unknown density f (y), the histogram, has a number\nof drawbacks. in addition to often looking rather rough due to the use of bins with\n\n "}, {"Page_number": 318, "text": "306\n\ny\nt\ni\ns\nn\ne\nd\n\n5\n1\n0\n\n.\n\n0\n\n0\n1\n0\n\n.\n\n0\n\n5\n0\n0\n\n.\n\n0\n\n0\n\n.\n\n0\n\n7 \u00b7 estimation and hypothesis testing\n\ny\nt\ni\ns\nn\ne\nd\n\n5\n1\n0\n\n.\n\n0\n\n0\n1\n0\n\n.\n\n0\n\n5\n0\n0\n\n.\n\n0\n\n0\n\n.\n\n0\n\nfigure 7.2 kernel\ndensity estimates for\nmaize data. left:\nconstruction of kernel\nestimate (heavy) as sum of\n15 scaled normal densities\ncentred at the y j , with\nh = 19.5. right: density\nestimates with h = 13.3\n(solid), h = 23.2 (dots)\nand h = 30 (dashes).\n\n-150\n\n-50\n\n0\n\ny\n\n50 100 150\n\n-150\n\n-50\n\n50 100 150\n\n0\n\ny\n\nsharp edges, its appearance depends heavily on the placing of the bin boundaries and\non bin width. slight changes to the boundaries can give strikingly different histograms\nfor the same data, particularly with small samples, and this is clearly undesirable.\n\nan alternative approach is based on a kernel function w(y), which is a symmetric\nprobability density with mean zero and unit variance, descending smoothly to zero\nas |y| \u2192 \u221e. an example is the standard normal density, but there are many other\npossibilities. the kernel density estimator based on a random sample y1, . . . ,y n\nfrom f is\n\n(cid:1)f h(y) = 1\n\nnh\n\n(cid:4)\n\n(cid:3)\n\nn(cid:13)\nj=1\n\nw\n\ny \u2212 y j\n\nh\n\n,\n\n(7.4)\n\nwhere h > 0 is abandwidth. figure 7.2 shows how this is constructed for the data\n\nin the rightmost column of table 1.1: the value of (cid:1)f h at y is obtained by summing\n\ncontributions from densities with standard deviations h and centred at each of the y j ,\nwith those closest to y contributing most. the estimate depends on both bandwidth\nand kernel, but the choice of h is much the more important. when n is large, a smaller\nbandwidth can be chosen, so that the contributions are more localized. such estimators\nare commonly available in statistical packages and are widely used in applications.\n\nto find the mean and variance of (cid:1)f h(y), note that\n(cid:6)\n\u22121w\n\ny \u2212 y j\n\n(cid:4)(cid:15)\n\n= e\n\n(cid:14)\n\n(cid:3)\n\nw\n\ne\n\nh\n\nn(cid:13)\nj=1\n\n1\nnh\n\n(cid:3)\n\ny \u2212 y\nh\n\n(cid:4)(cid:7)\n\n.\n\nh\n\n(cid:5)\n\n(cid:3)\n\n(cid:4)\n\n(cid:5)\n\n(cid:3)\n\nh\n\nw\n\ny \u2212 x\nh\n\nf (x) dx = h\n\nsymmetry of the kernel gives\n\u22121\n\nx \u2212 y\nh\nand taylor series expansion for small h gives\n(y) + \u00b7\u00b7\u00b7\n\nf (y) + hu f\n\nh2u2 f\n\nw (u)\n\n(cid:6)\n\n(cid:5)\n\n\u22121\n\nw\n\n(cid:1)(cid:1)\n\n(cid:1)\n\n(cid:7)\n\n(y) + 1\n2\n\n(cid:4)\n\n(cid:5)\n\nf (x) dx =\n\nw(u) f (y + hu) du,\n\n(cid:1)\n\n(y) = d f (y)/dy,\n\nhere f\nand so forth.\n\ndu = f (y) + 1\n2\n\nh2 f\n\n(cid:1)(cid:1)\n\n(y) + o(h4),\n(7.5)\n\n "}, {"Page_number": 319, "text": "7.1 \u00b7 estimation\nbecause of the symmetry and moment properties of w.thus (cid:1)f h(y) has bias 1\no(h4). the variance of (cid:1)f h(y) is\nnh2 var\n(cid:16)\n\n(cid:6)\n(cid:14)\n\n(cid:15)\n\n(cid:17)\n\nw\n\n(cid:6)\n\n(cid:3)\n\nvar{(cid:1)f h(y)} = 1\n= 1\nnh2\n\n(cid:4)(cid:7)\n(cid:4)2\n\n(cid:3)\ny \u2212 y\n(cid:3)\nh\ny \u2212 y\nh\n\n(cid:4)(cid:7)2\n\n,\n\ny \u2212 y\nh\n\ne\n\nw\n\n\u2212 e\n\nw\n\n307\n(y)+\n\n(cid:1)(cid:1)\n\n2 h2 f\n\nwhich depends on\n\n(cid:14)\n\n\u22122w\n\nh\n\ne\n\n(cid:15)\n\n(cid:4)2\n\n(cid:3)\n\ny \u2212 y\nh\n\n(cid:6)\n\n\u22121w\n\nh\n\n(cid:3)\n\ny \u2212 y\nh\n\n(cid:4)(cid:7)\n\n.\n\n, e\n\n(cid:5)\n\n(cid:5)\n\nan argument similar to that above shows that the first of these is\n\n\u22121\n\n\u22121 f (y)\n\nw(u)2 du + o(h),\n\nw (u)2 { f (y) + hu f\n\n(cid:1)\n\n(y) + \u00b7\u00b7\u00b7} du = h\n\nh\n\n\u22121.\n\n(7.6)\nwhile we have already seen that the second is o(1). thus as h \u2192 0, the variance of\n(cid:1)f h(y) is oforder (nh )\nif both the bias obtained from (7.5) and the variance obtained from (7.6) are to\nvanish as n \u2192 \u221e we must choose h so that h \u2192 0 and nh \u2192 \u221e, for example taking\nh \u221d n\n\u22121/2. but does this give the best bias-variance tradeoff? the leading terms of\nthe asymptotic mean squared error of (cid:1)f h(y) are\n(cid:5)\n\nf (y)\n\nw(u)2 du.\n\n(7.7)\n\ndifferentiation shows that this is minimized as a function of h at\n\nhopt (y, f ) = n\n\n\u22121/5\n\nw(u)2 du f (y)/ f\n\n(cid:1)(cid:1)\n\n(y)2\n\n(cid:7)1/5\n\n,\n\ngiving an optimal bandwidth that varies with y. choosing h \u221d n\n\u22121/5 gives bias of\n\u22124/5, and then bias squared and variance contribute\n\u22122/5 and variance of order n\norder n\nterms of equal order to (7.7). as far as variance is concerned, the effective local sample\n\u22124/5, so ifn = 1000, say, the number of observations contributing to a local\nsize is n\nestimator will be roughly 250, considerably fewer than the sample size applicable for\nparametric estimation; this is typical of local problems, for which larger samples are\nneeded.\n\nthe bandwidth plays a key role in kernel density estimation. in exploratory work a\ngood strategy is to try several values, in the hope that different amounts of smoothing\nwill reveal different interesting features of the data. however automatic rules for\nselection of h can be valuable, both to suggest starting-points for exploration and for\nuse when density estimation plays a minor role in a larger analysis. the key difficulty\nis that as we saw above, an optimal choice such as hopt (y, f ) depends on the unknown\nf as well as on y. dependence on y can be removed by minimizing an overall measure\n\nh4\n4\n\nf\n\n(cid:1)(cid:1)\n\n(y)2 + 1\nnh\n\n(cid:6)(cid:5)\n\n "}, {"Page_number": 320, "text": "308\n\nof the distance between (cid:1)f h and f , such as the integral of (7.7) over y,\n\n7 \u00b7 estimation and hypothesis testing\n\n(cid:5)\n\nh4\n4\n\n(cid:5)\n\nf\n\n(cid:1)(cid:1)\n\n(y)2 dy + 1\nnh\n\nw(u)2 du,\n\n(7.8)\n\nbut the dependence on f remains. many proposals have been made to sidestep it, of\nwhich we outline only two.\none simple approach is to find the bandwidth that would be optimal if f were\nknown. if it is normal with variance \u03c3 2, for example, and w(u) = \u03c6(u), then the\nchoice minimizing (7.8) is h = 1.06\u03c3 n\n\u22121/5. inpractice \u03c3 is replaced by its sample\ncounterpart s, but as this is sensitive to outliers it may be better to use a rule of thumb\nsuch as\n\n(cid:1)\nopt\n\nh\n\n= 0.9 min{s, iqr/1.35} n\n\n\u22121/5.\n\niqr is the sample\ninterquartile range.\n\nerror of (cid:1)f h(y),\n(cid:5)\n\nthis choice is very simple but tends to oversmooth for non-normal data, thereby\nobscuring multimodality and other features of potential interest.\n\na better approach is to minimize an estimate of the exact mean integrated squared\n\n{(cid:1)f h(y) \u2212 f (y)}2 dy = e\n\n(cid:5) (cid:1)f h(y)2 dy \u2212 2e\n\n(cid:5) (cid:1)f h(y) f (y) dy +\n\n(cid:5)\n\ne\n\nf (y)2 dy,\n(7.9)\nwhose true value involves expectation with respect to f and so is unknown. the third\nterm of this does not depend on h, so itsuffices to estimate the first two terms. the\nawkward term is the second, an obvious estimator of which may be expressed in terms\n\nof the empirical distribution function (cid:1)f as\n(cid:5) (cid:1)f h(y) d(cid:1)f (y) = 1\n\nn(cid:13)\nj=1\n\nn\n\n(cid:1)f h(y j ),\n\nbut this is biased because y j appears twice in (cid:1)f h(y j ), once implicitly. the bias can\n\nbe removed by using the cross-validation estimator given by\n\nn(cid:13)\nj=1\n\n1\nn\n\n(cid:1)f h,\u2212 j (y j ), where (cid:1)f h,\u2212 j (y) =\n\n(cid:4)\n\n(cid:3)\n\n(cid:13)\ni(cid:5)= j\n\nw\n\ny \u2212 yi\nh\n\n1\n\n(n \u2212 1)h\n\nis based on all observations except y j and so is called a leave-one-out estimator.\nthe term cross-validation means that each datum is compared with the rest. cross-\nvalidation is widely used and has many variants. it is related to aic; in both cases\nthe complexity of a set of models would ideally be assessed using the accuracy of\ntheir predictions for data sets like the original. as a new dataset is unavailable, we\nmanufacture one with just one datum, assess how well that datum can be predicted\nusing the remainder, and average all n possible such comparisons.\n\nit is easily shown that apart from a constant factor the first two terms of (7.9) are\n\nestimated unbiasedly by\n\ncv(h) = 1\nn\n\nn(cid:13)\nj=1\n\n(cid:5) (cid:1)f h,\u2212 j (y)2 dy \u2212 2\n\nn(cid:13)\nj=1\n\n(cid:1)f h,\u2212 j (y j ),\n\nn\n\n "}, {"Page_number": 321, "text": "309\n\n7.1 \u00b7 estimation\nso it seems reasonable to hope that the value(cid:1)hc v\n\nopt that minimizes this will be close\nto the value hopt ( f ) which minimizes (7.9). if the kernel is normal, then cv(h) is\nreadily computed using results on convolutions of normal densities. care is needed\nwhen minimizing cv(h), as it may have several local minima.\n\nalthough close to unbiased,(cid:1)hc v\n\nopt is rather variable. other formulations of cross-\nvalidation can be used to derive more stable bandwidth estimators, but typically at\nthe expense of increased bias. section 7.4 gives further reading.\nexample 7.5 (maize data) the maize data have iqr = 34 and s = 37.74, so\n\u22121/5 = 23.2 and h\n= 13.3. the effect of these choices may be seen in\n(cid:1)\n1.06sn\nopt\n(cid:1)\nfigure 7.2, where using h\nopt makes the two negative observations more promi-\nnent, perhaps too much so, and h = 30 seems rather large. cross-validation gives\n(cid:1)hc v\n= 19.5, again over-emphasizing the two negative y j .\nhere n = 15 and it is unreasonable to hope for a useful nonparametric density\n(cid:1)\n\nopt\n\nestimate.\n\n7.1.3 minimum variance unbiased estimation\nother things being equal, bias is undesirable. a badly biased estimator has expected\nvalue far from the parameter it is supposed to estimate, so it seems a good idea to\nminimize bias so far as possible. this has motivated a careful study of unbiased\nestimators, for which there is a rather complete theory in a limited class of models.\n\nrao\u2013blackwell theorem\nsuppose that data y arise from a statistical model f (y; \u03b8) and that we want to estimate a\nscalar function \u03c8 = \u03c8(\u03b8) of\u03b8 . suppose also that the statistic s = s(y ) issufficient for\n\u03b8, and that t = t(y ) is anunbiased estimator of \u03c8. then subject to suitable regularity\nconditions, w = w(s) = e(t | s) is anunbiased estimator of \u03c8 with variance no\nlarger than that of t :\n\nvar(t ) \u2265 var(w )\n\nfor all \u03b8.\n\n(7.10)\n\ndavid harold blackwell\n(1919\u2013) works at the\nuniversity of california,\nberkeley. see degroot\n(1986a) for an account of\nhis life and work.\n\nthis is the rao\u2013blackwell theorem. it is anon-asymptotic result, applicable to samples\nof any size.\n\nto establish (7.10), first note that w is indeed a statistic:\n\n(cid:5)\n\nw = w(s) = e(t | s) =\n\nt(y) f (y | s)dy\n\ndoes not depend on \u03b8 because of the sufficiency of s. therefore w can be calculated\nfrom y alone. secondly,\n\ne(w ) = es{e(t | s)} =e( t ) = \u03c8,\n\nso w is unbiased for \u03c8. thirdly,\n\nvar(t ) = e{(t \u2212 \u03c8)2}\n\n= e[{t \u2212 e(t | s) + e(t | s) \u2212 \u03c8}2]\n\n "}, {"Page_number": 322, "text": "310\n\n7 \u00b7 estimation and hypothesis testing\n\n= e[{t \u2212 e(t | s)}2] + 2e{(t \u2212 w )(w \u2212 \u03c8)} +e{( w \u2212 \u03c8)2}\n= e{(t \u2212 w )2} +var( w ),\n\n(7.11)\n\nbecause the middle term is\n\ney{(t \u2212 w )(w \u2212 \u03c8)} =e sey|s{(t \u2212 w )(w \u2212 \u03c8)}\n\n= es{(w \u2212 w )(w \u2212 \u03c8)}\n= 0.\n\nevidently (7.11) implies (7.10), giving e{(t \u2212 w )2} =0, that is, t = w with prob-\nability one, so t and w are effectively the same estimator. the process of replacing\nan unbiased estimator t with another e(t | s) with smaller variance is called rao\u2013\nblackwellization.\n\nexample 7.6 (poisson mean) let y1, . . . ,y n be a poisson random sample whose\nas the poisson density is an exponential family, s = (cid:2)\nmean \u03b8 we intend to estimate. now e(y1) = \u03b8, so y1 is an unbiased estimator of \u03b8.\ny j is minimal sufficient for \u03b8\n(section 5.2.3). therefore w = e(y1 | s) is anunbiased estimator for \u03b8 with variance\nat most that of y1.\nto find w , we argue by symmetry. evidently e(y1 | s) = \u00b7\u00b7\u00b7 = e(yn | s), because\nthe y j were independent and identically distributed unconditionally and the condi-\ntioning statistic s is symmetric in the y j . therefore\n\ne(y1 | s) = n\n\n\u22121\n\ne(y j | s) = e\n\n\u22121\n\nn\n\n= e(n\n\n\u22121s | s) = s/n.\n\n(cid:18)\n\n(cid:20)\n\n(cid:19)(cid:19)(cid:19)(cid:19)(cid:19) s\n\nn(cid:13)\nj=1\n\ny j\n\nn(cid:13)\nj=1\n\nthis estimator has variance \u03b8/n, whereas y1 has variance \u03b8.\n\n(cid:1)\n\nexample 7.7 (dirac comb) a random sample of pairs (x, y ) from a bivariate\ndensity f (x, y) is available, and we wish to estimate some feature of the unknown\nmarginal density f (y) of y . suppose that the conditional density f (y | x) is available.\nthis may seem unrealistic, but in fact this situation often arises when using simulation\nto estimate a density; see section 11.3.3.\n\nthe likelihood based on data (x1, y1), . . . ,( xn, yn) is\n\nand as f (y | x) isknown, x1, . . . , xn is sufficient for f (x, y) and hence for f (y).\nsuppose that we wish to estimate f (y) itself. we might use a kernel density esti-\nmator (7.4) with bandwidth h > 0, but this would be biased. we can remove the bias\nby letting h \u2192 0, thus obtaining the dirac comb\n\nn(cid:21)\nj=1\n\nf (y j | x j ) f (x j ),\n\n(cid:1)f (y) = 1\n\nn\n\nn(cid:13)\nj=1\n\n\u03b4(y \u2212 y j ).\n\nthe comb, which places an infinite spike at each y j and can be regarded as the\nderivative of the empirical distribution function (2.3), gives terrible estimates of f (y),\n\npaul adrien maurice\ndirac (1902\u20131984) was\nborn in bristol and studied\nthere and at cambridge,\nwhere he later held the\nprofessorship of\nmathematics once held by\nnewton. his unifying\nwork on the basis of\nquantum mechanics and\nrelativity led to his\nreceiving the 1933 nobel\nprize in physics. an\nintensely private man, he\nhad to be persuaded that\nthe publicity would be\ngreater if he refused it, as\nhe originally intended.\n\n\u03b4(u) isthe dirac delta\nfunction.\n\n "}, {"Page_number": 323, "text": "7.1 \u00b7 estimation\n\nbut it isunbiased because\n\n(cid:5)\n\n\u03b4(y \u2212 u)g(u) du = g(y)\n\nfor any function g. toapply the rao\u2013blackwell theorem, note that\n(cid:1) = 1\nn\n\ne((cid:1)f (y) | x1, . . . , xn) = 1\n\n(cid:1) | x j ) dy\n\n\u03b4(y \u2212 y\n\n) f (y\n\nn\n\n(cid:1)\n\nn(cid:13)\nj=1\n\nn(cid:13)\nj=1\n\n(cid:5)\n\n(7.12)\nthis is a kernel estimator for which the kernel is the conditional density of y given\nx; it ismuch smoother than the comb. as the first two moments of\n\nf (y | x j ) are\n\nex { f (y | x)} =\nex{ f (y | x)2} =\n\nf (y | x) f (x) dx = f (y),\nf (y | x)2 f (x) dx =\n\n(cid:5)\n\nf (y | x) f (x, y) dx,\n\n(7.13)\n\n(cid:5)\n(cid:5)\n\n311\n\nf (y | x j ).\n\nwe see that (7.12) is unbiased, with finite variance when (7.13) is finite, as will usually\nbe the case.\n\na similar argument applies to the cumulative distribution function of y and may\n(cid:1)\n\nbe preferred by those wary of delta functions (exercise 7.1.8).\n\ncompleteness\ngiven an unbiased estimator t of \u03c8 and a sufficient statistic, s, the rao\u2013blackwell\ntheorem enables us to find an unbiased estimator with variance at most that of t .\nhowever there may be many unbiased estimators, each of which could be rao\u2013\nblackwellized. is there one with lowest variance?\n\nto answer this question we need the notion of completeness. astatistic s is complete\n\nif for any function h,\n\ne{h(s)} = 0\n\nfor all \u03b8 implies that\n\nh \u2261 0,\n\n(7.14)\n\nand is boundedly complete if (7.14) is true provided h is bounded. evidently a complete\nstatistic is also boundedly complete. if s is complete, we say that its density f (s; \u03b8) is\ncomplete. as completeness must hold for all \u03b8, it is aproperty of a family of densities\nrather than of a single density.\n\ncompleteness of minimal sufficient statistics is used to establish uniqueness of min-\nimum variance unbiased estimators. note the qualification here: sufficient statistics\nthat are not minimal are not in general complete.\n\nstrictly speaking h = 0\nalmost everywhere with\nrespect to the density of s.\n\nexample 7.8 (poisson density) suppose that y is poisson with mean \u03b8 > 0, and that\nh(y ) satisfies e{h(y )} = 0 for every value of \u03b8. then its expectation is proportional\nto a power series which is identically zero on the positive half-line:\n\n\u221e(cid:13)\ny=0\nhence h(0) = h(1) = \u00b7\u00b7\u00b7 =0, and y is complete.\n\n0 = e{h(y )} =\n\n\u221e(cid:13)\ny=0\n\n\u2212\u03b8 \u221d\n\n\u03b8 y\ny!\n\nh(y)\n\ne\n\n\u03b8 y h(y)\ny!\n\n,\n\n\u03b8 > 0.\n\n "}, {"Page_number": 324, "text": "312\n\n7 \u00b7 estimation and hypothesis testing\nnow consider a poisson sample of size n. then s = (y1, . . . ,y n) issufficient for \u03b8,\n(cid:2)\nand h(s) = y1 \u2212 y2 has expectation zero for all \u03b8. this does not imply that y1 = y2,\nhowever, so s is not complete. the corresponding minimal sufficient statistic\ny j\n(cid:1)\nhas a poisson density, and is complete.\nexample 7.9 (uniform density) suppose that y is uniformly distributed on (\u2212\u03b8, \u03b8).\nthen e(y ) = 0 for every \u03b8 > 0, but as h(y) = y is not identically zero, y is not\n(cid:1)\ncomplete.\n\nexample 7.10 (exponential family) suppose that y belongs to an exponential\nfamily of order p,\n\nf (y; \u03c9) = exp{s(y)t\u03b8 \u2212 \u03ba(\u03b8)} f0(y),\n\ny \u2208 y, \u03b8 \u2208 n .\n\nif y is continuous and e{h(y )} =0, then provided that n contains an open set around\nthe origin,\n\n(cid:5)\n\ne{h(y )} =\n\nh(y) exp{s(y)t\u03b8 \u2212 \u03ba(\u03b8)} f0(y) dy = 0\n\nis proportional to the laplace transform of h(y) f0(y). then the uniqueness of laplace\ntransforms implies that h(y) f0(y) = 0 except on sets of measure zero and thus h(y ) \u2261\n0: y is complete. when y is discrete the corresponding argument involves series or\npolynomials, as in example 7.8.\n\nthe same argument applies to any subfamily whose parameter space contains an\nopen set around the origin, and in particular to all the standard exponential family\n(cid:1)\nmodels.\n\n(cid:1)\n\n(cid:1)\n\n(cid:1) = t\n(cid:1)\n\n(y ). let w = e(t | s) and w\n\n) = 0 for all \u03b8, and both w and w\n\nto see how completeness is used, suppose that we have a parametric model f (y; \u03b8)\nwith complete minimal sufficient statistic s, and two unbiased estimators of \u03c8 =\n\u03c8(\u03b8), namely t = t(y ) and t\n(cid:1) | s). now\ne(w \u2212 w\nare functions of the data only through\ns. but s is complete, so w = w\nare\nidentical for all practical purposes. thus rao\u2013blackwellization of an unbiased estima-\ntor using a complete sufficient statistic always leads to w , and no unbiased estimator\nof \u03c8 has smaller variance. for suppose t\nis an unbiased estimator of \u03c8 with smaller\nvariance than w . then by the rao\u2013blackwell theorem, w\n\nexcept on sets of measure zero, that is, w and w\n\n(cid:1) | s) satisfies\n\n(cid:1) = e(t\n\n(cid:1) = e(t\n\n(cid:1)\n\n(cid:1)\n\n(cid:1)\n\nvar(w\n\n) \u2264 var(t\n\n(cid:1)\n\n) < var(w ),\n\n(cid:1)\n(cid:1) \u2261 w .\n\n(cid:2)\n\nwhich is impossible because w\n\nexample 7.11 (normal density) let y1, . . . ,y n be a n (\u00b5, \u03c3 2) random sample,\nwhere n \u2265 2. we saw in example 5.14 that s = (y ,\n(y j \u2212 y )2) isminimal suf-\nficient, and as its density is an exponential family of order 2 in which we can take\n\u0001 = (\u2212\u221e,\u221e) \u00d7 (0,\u221e), s is complete.\n\nnow y is an unbiased estimator of \u00b5 that is a function of s, and therefore it is the\nminimum variance unbiased estimator of \u00b5. likewise the minimum variance unbiased\nestimator of \u03c3 2 is (n \u2212 1)\n(cid:1)\n\n(y j \u2212 y )2.\n\n(cid:2)\n\n\u22121\n\n "}, {"Page_number": 325, "text": "7.1 \u00b7 estimation\n\n313\n\nalthough of theoretical interest, minimum variance unbiased estimators are not\nwidely used in practice. one difficulty is that the restriction to exact unbiasedness can\nexclude every interesting estimator.\n\nexample 7.12 (poisson density) let y1, . . . ,y n be a poisson random sample with\nminimal sufficient statistic s = (cid:2)\nmean \u03bb, and let \u03c8 = exp(\u22122n\u03bb). then an unbiased estimator h(s) of\u03c8 based on the\n\ny j must satisfy\n\nexp(\u22122n\u03bb) =\n\nh(s)\n\n(n\u03bb)s\ns!\n\n\u2212n\u03bb,\n\ne\n\n\u221e(cid:13)\ns=0\n\n(cid:6)\u22121,\n\n1,\n\nh(s) =\n\ns odd,\ns even.\n\nand completeness of s implies that the unique minimum variance unbiased estimator\nof \u03c8 is the unacceptable\n\nthe maximum likelihood estimator exp(\u22122s) ispreferable despite its bias.\n\n(cid:1)\n\na further difficulty is that minimum variance unbiased estimators do not transform\nin a simple way. moreover, as will be evident from the discussion above, there is\nno easy recipe that gives unbiased estimators, and once found, it may be awkward to\nrao\u2013blackwellize them. for these and other reasons, maximum likelihood estimators\nare generally preferable.\n\n7.1.4 interval estimation\nour focus so far has been on point estimates of a parameter and their variances.\nalthough these are useful when estimator is approximately normal, their relevance\nis much less obvious when its distribution is non-normal or the sample size is small.\nfurthermore it is often valuable to express parameter uncertainty in terms of an\ninterval, or more generally a region. the notion of a pivot, which we met in section 3.1,\nthen moves to centre stage.\nconsider a model f (y; \u03b8) for data y . then a pivot z = z(y, \u03b8) is afunction of y\nand \u03b8 that has a known distribution independent of \u03b8, this distribution being invertible\nas a function of \u03b8 for each possible value of y . that is, given a region a such that\npr{z(y, \u03b8) \u2208 a} =1 \u2212 2\u03b1, wecan find a region r\u03b1(y,a) ofthe parameter space\nsuch that\n\n1 \u2212 2\u03b1 = pr{z(y, \u03b8) \u2208 a} = pr{\u03b8 \u2208 r\u03b1(y ; a)} .\n\nif \u03b8 is scalar then z(y,a) istypically a strictly monotonic function of \u03b8 for each\ny . given data y and a suitable pivot, we find a (1 \u2212 2\u03b1) confidence region for the\ntrue value of \u03b8 by arguing that under repeated sampling r\u03b1(y;a) isthe realization\nof a random region r\u03b1(y ;a) that contains the true \u03b8 with probability (1 \u2212 2\u03b1). an\nimportant exact pivot is the student t statistic, and we have extensively used an ap-\nproximate pivot, the likelihood ratio statistic. for reasons to be given in section 7.3.4,\npivots such as these based on the likelihood tend to be close to optimal in the sense\n\n "}, {"Page_number": 326, "text": "314\n\n7 \u00b7 estimation and hypothesis testing\n\nof providing the shortest possible confidence intervals for given \u03b1, atleast in large\nsamples.\nexample 7.13 (exponential density) suppose we wish to base a (1 \u2212 2\u03b1) confi-\n\u2212\u03bby, y > 0,\ndence interval for \u03bb on a single observation from the exponential density \u03bbe\n\u03bb >0. then z = y \u03bb is pivotal, since pr(\u03bby \u2264 z) = 1 \u2212 e\n\u2212z, z > 0, independent of\n\u03bb. its upper (1 \u2212 \u03b1) quantile is z1\u2212\u03b1 = \u2212 log \u03b1. as\n\n1 \u2212 \u03b1 = pr(z \u2264 z1\u2212\u03b1) = pr(\u03bby \u2264 z1\u2212\u03b1) = pr(\u03bb \u2264 z1\u2212\u03b1/y ),\n\nan upper (1 \u2212 \u03b1) confidence limit is \u2212 log \u03b1/y. similarly an \u03b1 lower confidence limit\nfor \u03bb is \u2212 log(1 \u2212 \u03b1)/y, and an equi-tailed (1 \u2212 2\u03b1) confidence interval is (\u2212 log(1 \u2212\n\u03b1)/y,\u2212 log \u03b1/y). this is not symmetric about the maximum likelihood estimate\n(cid:1)\u03bb = 1/y, nor is it the shortest possible such interval.\nto find the shortest (1 \u2212 2\u03b1) confidence interval for \u03bb based on y, wechoose the\nupper tail probability \u03b3 , 0 < \u03b3 \u2264 2\u03b1, tominimize the interval length\n\n\u22121{\u2212 log \u03b3 + log(1 \u2212 2\u03b1 + \u03b3 )},\n\ny\n\ngiving \u03b3 = 2\u03b1 and confidence interval (0,\u2212 log(2\u03b1)/y). this is obvious from the\n(cid:1)\nshape of the exponential density and, not coincidentally, the likelihood.\n\nexercises 7.1\n1 let r be binomial with probability \u03c0 and denominator m, and consider estimators of \u03c0\nof form t = (r + a)/(m + b), for a, b \u2265 0. find a condition under which t has lower\nmean squared error than the maximum likelihood estimator r/m, and discuss which is\npreferable when m = 5, 10.\n(y j \u2212 y )2 be an estimator of \u03c3 2 based on a normal random sample. find\n\n2 let t = a\n\n(cid:2)\n\nvalues of a that minimize the bias and mean squared error of t .\n\n3 when t is a biased estimator of the scalar \u03c8(\u03b8), with bias b(\u03b8), show that under the usual\n\nregularity conditions, the mean squared error of t is no smaller than\n\n{d\u03c8/d\u03b8 + db(\u03b8)/d\u03b8}2 /i (\u03b8) + b(\u03b8)2.\n\n5 consider a kernel density estimator (7.4).\n\nif b(\u03b8) = b1(\u03b8)/n + b2(\u03b8)/n3/2 + \u00b7\u00b7\u00b7, where bi (\u03b8) is o(1), then show that the cram\u00b4er\u2013\nrao lower bound applies, at least in large samples.\n4 suppose that t is a q \u00d7 1 unbiased estimator of \u03c8 = \u03c8(\u03b8). show that cov(t , u ) =\n\u22121u , where u is p \u00d7 1\nd\u03c8/d\u03b8 t, and compute the variance matrix of t \u2212 d\u03c8/d\u03b8 t i (\u03b8)\nscore vector. hence establish (7.3).\n(a) verify the choice of h that minimizes (7.7). if f (y) = \u03c3 \u22121\u03c6{(y \u2212 \u00b5)/\u03c3} and w(u) = note that\n\u03c6(u), find hopt . discuss.\n(b) show that h = 1.06\u03c3 n\n(c) instead of using a constant bandwidth, we might take\ny \u2212 y j\nh\u03bb j\n\nn(cid:13)\nj=1\nfor local bandwidth factors \u03bb j \u221d { \u02dcf (y j )}\u2212\u03b3 based on a pilot density estimate \u02dcf (y). show\nthat if the pilot estimate is exact and \u03b3 = \u2212 1\n\n\u22121/5 minimises (7.8) using the densities in (a).\n(cid:1)f (y) = 1\n\n2 , then (cid:1)f has bias o(h2).\n\n1\n\u03bb j\n\n(cid:3)\n\n(cid:4)\n\nnh\n\nw\n\n6 find the expected value of cv(h), and show to what extent it estimates (7.9).\n\n\u03c6(z)2 = (2\u03c0)\n\n\u22121/2\u03c6(\n\n\u221a\n\n2z).\n\n "}, {"Page_number": 327, "text": "7.2 \u00b7 estimating functions\n\n7 find minimum variance unbiased estimators of \u03bb2, e\u03bb, and e\n\n315\n\u2212n\u03bb based on a random sample\ny1, . . . ,y n from a poisson density with mean \u03bb. show that no unbiased estimator of log \u03bb\nexists.\n8 in example 7.1.3, suppose we wish to estimate \u03c8 = pr(y \u2264 y) using the empirical\ni (y j \u2264 y). show that this is unbiased and that its rao\u2013\n\n(cid:2)\n\n\u22121\n\ndistribution function n\nblackwellized form is\n\nn(cid:13)\nj=1\n\n1\nn\n\npr(y j \u2264 y | x j ).\n\nhence obtain an unbiased estimator of f (y).\n\n9 let y \u223c n (0, \u03b8). is y complete? what about y 2? and |y|?\n10 let r1, . . . , rn be a binomial random sample with parameters m and 0 < \u03c0 <1, where m\nis known. find a complete minimal sufficient statistic for \u03c0 and hence find the minimum\nvariance unbiased estimator of \u03c0(1 \u2212 \u03c0).\n\n11 let y be the average of a random sample from the uniform density on (0, \u03b8). show that\n2y is unbiased for \u03b8. find a sufficient statistic for \u03b8, and obtain an estimator based on it\nwhich has smaller variance. compare their mean squared errors.\n\n7.2 estimating functions\n7.2.1 basic notions\nour discussion of the maximum likelihood estimator in section 4.4.2 stressed its\nasymptotic properties but said little about its finite-sample behaviour. by contrast our\ntreatment of unbiased estimators showed their finite-sample optimality under certain\nconditions, but suggested that the class of such estimators is often too small to be of\nreal interest for applications. furthermore both types of estimator can behave poorly if\nthe data are contaminated or if the assumed model is incorrect, making it worthwhile to\nconsider other possibilities. in this section we explore some consequences of shifting\nemphasis away from estimators and towards the functions that often determine them.\nsuppose that we intend to estimate a p \u00d7 1 parameter \u03b8 based on a random sample\nthen in most cases the maximum likelihood estimator(cid:1)\u03b8 is defined implicitly as the\ny1, . . . ,y n from a density f (y; \u03b8), assumed to be regular for likelihood inference.\nsolution to the p \u00d7 1 score equation\nu (\u03b8) = u(y ; \u03b8) = n(cid:13)\n\n\u2202 log f (y j ; \u03b8)\n\n= 0.\n\nj=1\n\nu(y j ; \u03b8) = n(cid:13)\nj=1\n(cid:6)\n\n\u2202\u03b8\n\n(cid:7)\n\nkey properties of the score statistic u (\u03b8) are\n\ne{u (\u03b8)} = 0,\n\nvar{u (\u03b8)} = e\n\n= i (\u03b8),\nfor all \u03b8, where the p \u00d7 p fisher information matrix i (\u03b8) = ni(\u03b8) and\ni(\u03b8) = var{u(y j ; \u03b8)} =\n\u2202u(y; \u03b8)\n\nu(y; \u03b8)u(y; \u03b8)t f (y; \u03b8) dy = \u2212\n\n\u2212 du (\u03b8)\nd\u03b8 t\n\n(cid:5)\n\n(cid:5)\n\nf (y; \u03b8) dy.\n\n\u2202\u03b8 t\n\n "}, {"Page_number": 328, "text": "316\n\nn\no\n\ni\nt\nc\nn\nu\n\nf\n \n\ng\nn\n\ni\nt\n\na\nm\n\ni\nt\ns\ne\n\n3\n\n2\n\n1\n\n0\n\n1\n-\n\n2\n-\n\n3\n-\n\n7 \u00b7 estimation and hypothesis testing\n\nn\no\n\ni\nt\nc\nn\nu\n\nf\n \n\ng\nn\n\ni\nt\n\na\nm\n\ni\nt\ns\ne\n\n3\n\n2\n\n1\n\n0\n\n1\n-\n\n2\n-\n\n3\n-\n\n-3\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\n3\n\n-3\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\n3\n\ntheta\n\ntheta\n\nthe implicit definition of (cid:1)\u03b8 suggests that we study properties of estimators \u02dc\u03b8 that\nsolve a p \u00d7 1 system of estimating equations of form\ng(y j ; \u03b8) = 0.\n\ng(y ; \u03b8) = n(cid:13)\n\n(7.15)\n\nj=1\n\nwe call g(y; \u03b8) anestimating function and say it is unbiased if\n\n(cid:5)\n\ne{g(y ; \u03b8)} = n\n\ng(y; \u03b8) f (y; \u03b8) = 0\n\nfor all \u03b8 .\n\nfigure 7.3 estimating\nfunctions. left:\nconstruction of g(y; \u03b8)\n(heavy) as the sum of\ng(y j ; \u03b8) for a sample of\nsize n = 3 shown by the\nrug. the lines g = 0\n(dots) and \u03b8 = \u02dc\u03b8 (dashes)\nare also shown. right:\nestimating functions for\nthe mean (solid), the\nhuber estimator (dots)\nand a redescending\nm-estimator (dashes),\nslightly offset to avoid\noverplotting.\n\nor sometimes an\ninference function.\n\nthis formulation encompasses many possibilities.\nexample 7.14 (logistic density) the logistic density ey\u2212\u03b8 /(1 + ey\u2212\u03b8 )2 has score\nfunction\n\nu(y; \u03b8) = 2ey\u2212\u03b8 /(1 + ey\u2212\u03b8 ) \u2212 1, \u2212\u221e < y < \u221e,\u2212\u221e < \u03b8 <\u221e.\n\nthe left panel of figure 7.3 shows the construction of the corresponding estimating\n(cid:1)\nfunction based on a sample of size three.\nif g(y; \u00b5) = y \u2212 \u00b5, then the solution to (7.15)\nexample 7.15 (moment estimators)\nis the sample average \u02dc\u00b5 = y , which is an unbiased estimator of the mean of f , ifthis\nexists. the estimating function y \u2212 \u00b5 is shown in the right panel of figure 7.3, with\nother estimating functions discussed later.\n\nthis can be extended to several parameters. the moment estimators of the mean or method of moments\n\nand variance of y are found by simultaneous solution of\n\nestimators.\n\nn(cid:13)\nj=1\n\nn(cid:13)\nj=1\n\n\u22121\n\nn\n\ny j \u2212 \u00b5 = 0,\n\n\u22121\n\nn\n\n\u2212 \u00b52 \u2212 \u03c3 2 = 0,\n\ny 2\nj\n\nand these are of form (7.15) with g(y; \u03b8) = (y \u2212 \u00b5, y2 \u2212 \u00b52 \u2212 \u03c3 2)t and \u03b8 = (\u00b5, \u03c3 2)t.\nalthough themselves unbiased, these estimating equations produce the biased esti-\nmator n\n\n(y j \u2212 y )2 of \u03c3 2.\n\n(cid:2)\n\n\u22121\n\n "}, {"Page_number": 329, "text": "\u0001(u) is the gamma\nfunction.\n\n7.2 \u00b7 estimating functions\n\n317\n\nestimators of functions of the mean and variance may be defined similarly. for\n\nexample, the weibull density\n\nf (y; \u03b2, \u03ba) = \u03ba\u03b2\u22121(y/\u03b2)\u03ba\u22121 exp{\u2212(y/\u03b2)\u03ba},\n\ny > 0, \u03b2, \u03ba > 0,\n\nhas e(y r ) = \u03b2r \u0001(1 + r/\u03ba). hence the moment estimator of \u03b8 = (\u03b2, \u03ba)t can be de-\ntermined as the solution to (7.15) with\n\ng(y; \u03b8) = ( y \u2212 \u03b2\u0001 (1 + 1/\u03ba) ,\n\ny2 \u2212 \u03b22\u0001 (1 + 2/\u03ba) )t .\n\n(7.16)\n\nthe parameters \u00b5 and \u03c3 2 have the same interpretations for any model that possesses\n(cid:1)\ntwo moments, whereas (\u03b2, \u03ba) are specific to the weibull case.\n\nexample 7.16 (probability weighted moment estimators) moment estimators\nmay be poor or even useless with data from long-tailed densities, whose moments\nmay not exist. an alternative is use of probability weighted moment estimators, defined\nas solutions to equations of form\n\n\u22121\n\nn\n\ny r f(y ; \u03b8)s {1 \u2212 f(y ; \u03b8)}t \u2212\n\nyr f(y; \u03b8)s {1 \u2212 f(y; \u03b8)}t f (y; \u03b8) dy = 0.\n\n(cid:5)\n\nn(cid:13)\nj=1\n\neven if the ordinary moments, which correspond to taking s = t = 0, do not exist,\nthe integrals here may be finite for positive values of s or t or both.\nan example is the generalized pareto distribution (6.38), for which we set \u03b8 =\n(\u03be, \u03c3 )t. inthis case it is convenient to take r = 1 and s = 0, giving\n\ngt (y; \u03b8) = y(1 + \u03be y/\u03c3 )\n\n\u2212t /\u03be \u2212\n\n\u03c3\n\n(t + 1)(t + 1 \u2212 \u03be)\n\n,\n\nwhich has finite expectation provided \u03be < t + 1. estimators may be obtained by set-\nting g(y; \u03b8) = (g1(y; \u03b8), g2(y; \u03b8))t and solving (7.15) simultaneously, though equiv-\nalent more convenient forms of the equations are preferred in practice.\n\nas with moment estimators, the choice of r, s, and t introduces an arbitrary element,\n(cid:1)\n\nbecause different choices will lead to different estimators.\n\nexample 7.17 (linear model) the scalar \u03b2 in the simple linear model\n\ny j = \u03b2x j + \u03b5 j ,\n(cid:2)\n\nj = 1, . . . ,n ,\n\ng(y; \u03b8) = y \u2212 \u03b2x, giving \u02dc\u03b2 = (cid:2)\n\nwhere the \u03b5 j have mean zero, can be estimated by the solution to (7.15) with\nx j . this estimator is unbiased whatever the\ndistributions of the \u03b5 j ; inparticular we have made no assumptions about their vari-\nances, requiring the \u03b5 j only to have zero mean. in fact, they need not be independent,\n(cid:1)\nor even uncorrelated.\n\ny j /\n\nin general discussion we shall suppose that \u03b8 is scalar and that for every value\nof y, wedeal with an unbiased estimating function g(y; \u03b8) that is strictly monotone\ndecreasing in \u03b8. it isthen easy to show that \u02dc\u03b8 is consistent for \u03b8. note first that \u02dc\u03b8 \u2264 a\nif and only if g(y ; a) \u2264 0. as g(y; \u03b8) isdecreasing in \u03b8 for each y, n\n\u22121g(y ; \u03b8 \u2212 \u03b5)\n\n "}, {"Page_number": 330, "text": "318\n\nconverges to\n\n7 \u00b7 estimation and hypothesis testing\n\nn\n\n\u22121e{g(y ; \u03b8 \u2212 \u03b5)} = n\n\n\u22121e{g(y ; \u03b8 \u2212 \u03b5) \u2212 g(y ; \u03b8)} = c(\u03b8 \u2212 \u03b5) > 0\nas n \u2192 \u221e for any \u03b5 > 0, by virtue of the weak law of large numbers. hence\n\npr(\u02dc\u03b8 \u2264 \u03b8 \u2212 \u03b5) = pr{n\n\n\u22121g(y ; \u03b8 \u2212 \u03b5) \u2264 0} \u21920,\n\nas n \u2192 \u221e.\n\nlikewise pr(\u02dc\u03b8 > \u03b8 + \u03b5) \u2192 0, so pr(|\u02dc\u03b8 \u2212 \u03b8| \u2264\u03b5 ) \u2192 1: \u02dc\u03b8 is a consistent estimator.\ntechnical difficulties arise with non-monotone or discontinuous estimating func-\ntions, to which most of the discussion below does not apply directly. in such cases it\nis necessary to show that there is a consistent solution to the estimating equation, to\nwhich the arguments below can be applied.\n\noptimality\nhaving defined the class of unbiased estimating functions, the question naturally\narises which of them we should use. to answer this we must find a finite-sample\noptimality criterion analogous to mean squared error. to motivate a suitable criterion,\nsuppose that \u03b8 is scalar and consider its estimator \u02dc\u03b8. taylor series expansion of g(y ; \u02dc\u03b8)\ngives\n\nso\n\n\u02dc\u03b8 \u2212 \u03b8\n\n.=\n\n,\n\n0\n\ndg(y ; \u03b8)\n\nd\u03b8\n\n.= g(y ; \u03b8) + (\u02dc\u03b8 \u2212 \u03b8)\n(cid:2)\n(cid:22) \u2212 dg(y ;\u03b8)\n\nn\nj=1 g(y j ; \u03b8)\ndg(y j ;\u03b8)\nn\nj=1\n\n=\n\nn\nj=1 g(y j ; \u03b8)\n\ne\n\nd\u03b8\n\nd\u03b8\n\n(cid:2)\n\u2212(cid:2)\n\n(cid:23) + o p(n\n\n\u22121),\n\n(7.17)\n\nusing the same argument as applied to the maximum likelihood estimator. this implies\nthat \u02dc\u03b8 has asymptotic variance\n\nvar(\u02dc\u03b8)\n\n.= var{g(y ; \u03b8)}\n(cid:23)(cid:25)2\n(cid:22) \u2212 dg(y ;\u03b8)\n(cid:24)\n\ne\n\nd\u03b8\n\n= n\n\n\u22121\n\ng(y; \u03b8)2 f (y; \u03b8) dy\nf (y; \u03b8) dy\n\ndg(y;\u03b8)\n\nd\u03b8\n\n(cid:23)2\n\n.\n\n(cid:26)\n(cid:22) \u2212(cid:26)\n\na measure of finite-sample performance of g(y; \u03b8) should not conflict with asymptotic\nproperties of \u02dc\u03b8, suggesting that we regard an estimating function as optimal in the\nclass of unbiased estimating functions if it minimizes\n\n(cid:24)\n\nvar{g(y ; \u03b8)}\n(cid:23)(cid:25)2\n(cid:22) \u2212 dg(y ;\u03b8)\ne\n\nd\u03b8\n\n(7.18)\n\nfor all \u03b8. this quantity is unaffected by one-one reparametrization.\n\nanother motivation for (7.18) rests on noting that although variance is a natural\nbasis for comparing estimating functions, a g(y ; \u03b8) isalso unbiased, with variance\na2 times greater than that of g(y ; \u03b8). hence fair comparison is possible only after\nremoving this arbitrary scaling. multiplication of g(y ; \u03b8) by a changes the slope\nof the estimating function, so it is natural to choose a to ensure that the expected\nderivative of g(y ; \u03b8) equals one, leading to (7.18).\n\n "}, {"Page_number": 331, "text": "7.2 \u00b7 estimating functions\n\nit can be shown that any unbiased estimating function must satisfy\n\ni (\u03b8)\n\n\u22121 \u2264 var{g(y ; \u03b8)}\n(cid:23)(cid:25)2\n(cid:22) \u2212 dg(y ;\u03b8)\n\n(cid:24)\n\ne\n\n,\n\nd\u03b8\n\n319\n\n(7.19)\n\n(cid:6)\n\n(cid:7)\u22121\n\nso there is a lower bound on (7.18), analogous to the cram\u00b4er\u2013rao lower bound.\nif (7.18) is evaluated with g(y ; \u03b8) = u(y ; \u03b8), the result is i (\u03b8)\n\u22121. hence the score\nfunction minimizes (7.18), and is in this sense optimal in finite samples. this ties in\nwith asymptotic properties of the maximum likelihood estimator, and may be extended\nto the case where \u03b8 is a p \u00d7 1 vector. then\n(cid:6)\nvar{g(y ; \u03b8)} e\n\n(7.20)\nin the sense that the difference of these p \u00d7 p matrices is positive semi-definite,\nprovided e{\u2212\u2202g(y ; \u03b8)/\u2202\u03b8 t} is invertible. the left-hand side of this inequality is the\nasymptotic covariance matrix of \u02dc\u03b8, and its sandwich form generalizes that of a maxi-\nmum likelihood estimator under a wrong model; see section 4.6. standard errors for\n\u02dc\u03b8 are obtained by replacing the matrices in (7.20) by sample versions, giving\n\n(cid:7)\u22121 \u2265 i (\u03b8)\n\n\u2212 \u2202g(y ; \u03b8)t\n\n\u2212 \u2202g(y ; \u03b8)\n\n\u2202\u03b8 t\n\n\u22121\n\n\u2202\u03b8\n\ne\n\n(cid:14)\n\nn(cid:13)\nj=1\n\n(cid:15)\u22121\n\nn(cid:13)\nj=1\n\n\u2202g(y j ; \u02dc\u03b8)\n\n\u2202\u03b8 t\n\ng(y j ; \u02dc\u03b8)g(y j ; \u02dc\u03b8)t\n\n(cid:14)\n\nn(cid:13)\nj=1\n\n(cid:15)\u22121\n\n,\n\n\u2202g(y j ; \u02dc\u03b8)t\n\n\u2202\u03b8\n\nfrom which confidence sets for elements of \u03b8 may be obtained, generally by normal\napproximation.\n\nexample 7.18 (weibull model) an estimating function for the weibull parameters\n\u03b2 and \u03ba is given by (7.16), for which elementary calculations give\n\n(cid:4)\n\n(cid:6)\n\n(cid:7)\n\n(cid:3)\n\n\u2212 \u2202g(y ; \u03b8)t\n\n\u2202\u03b8\n\n= n\n\n2\u03b2\u0001(1 + 2/\u03ba)\n\n\u0001(1 + 1/\u03ba)\n(1 + 1/\u03ba)/\u03ba 2 \u22122\u03b22\u0001(cid:1)\n(1 + 2/\u03ba)/\u03ba 2\n(cid:4)\n\n\u2212\u03b2\u0001(cid:1)\n(cid:3)\n\n\u2212\u0001(cid:1)\n\u03ba 2/\u03b22\n\u2212\u0001(cid:1)\n(2)/\u03b2 {1 + \u0001(cid:1)(cid:1)\n\n(2)/\u03b2\n(2)}/\u03ba 2\n\n,\n\ni (\u03b8) = n\n\nwhile var{g(y ; \u03b8)} is easily found in terms of the moments e(y r ). in analogy to the\ndiscussion of efficiency on page 113, the overall efficiency of g(y ; \u03b8) relative to the\nscore is taken to be the square root of the ratio of the determinants of the matrices on\neither side of the inequality in (7.20), while the efficiency for estimation of \u03b2 is the\nratio of their (1, 1) coefficients, with (2, 2) coefficients used for \u03ba. these efficiencies,\nplotted in the left panel of figure 7.4, show that the moment estimating functions are\n(cid:1)\nfairly efficient when \u03ba > 2, but are poor when \u03ba is small.\n\ne\n\n(u) = d\u0001(u)/du, and so\n\u0001(cid:1)\nforth.\n\nand\n\n7.2.2 robustness\nfinite-sample optimality of the score function is not the whole story, for several\nreasons. first, we may be unwilling or unable to specify the model fully, and then\nthe score is unavailable. second, even if we can be fairly sure of f (y; \u03b8), there is\n\n "}, {"Page_number": 332, "text": "figure 7.4 efficiencies\nof estimating functions.\nleft: overall efficiency\n(solid), efficiency for \u03b2\n(dashes) and for \u03ba (dots)\nfor moment estimators of\nweibull distribution.\nright: finite-sample\nefficiency of huber\nestimating function gc\nrelative to g(y; \u03b8) = y \u2212 \u03b8\nfor normal (solid), t5\n(dots), normal mixture\n0.95n (0, 1) +\n0.05n (0, 9) (small\ndashes) and logistic data\n(long dashes).\n\n320\n\ny\nc\nn\ne\nc\ni\nf\nf\n\ni\n\ne\n\n0\n1\n\n.\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n0\n\n7 \u00b7 estimation and hypothesis testing\n\ny\nc\nn\ne\nc\ni\nf\nf\n\ni\n\ne\n\n2\n\n.\n\n1\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n0\n\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n0\n\n1\n\nkappa\n\n3\n\n4\n\n2\n\nc\n\nalways the possibility of bad data \u2014 tryping errors, wild observations and so forth.\nin principle all data should be carefully scrutinized for these, but with big or complex\ndatasets or where data are collected automatically this is impracticable. estimating\nfunctions that are robust, that is, perform well under a wide range of potential models\ncentred at an ideal model may be preferred, even if they are somewhat sub-optimal\nwhen that model itself holds.\n\nrobustness entails insensitivity to departures from assumptions, but this has many\naspects. perhaps the most common usage relates to contamination by outliers. if bad\nvalues are present then we might optimistically hope to identify and delete them,\nor more realistically aim to downweight them. thus we ignore or play down some\n\u2018bad\u2019 portion of the data and hope to extract useful information from the \u2018good\u2019\npart, even if we are unsure where the boundary lies. a related usage concerns the\nneed for procedures that perform well when assumptions underlying the ideal model\nare relaxed. an essential requirement is then that estimands have the same inter-\npretation under all the potential models. in example 7.15 the first and second mo-\nments \u00b5 and \u03c3 2 have this property of robustness of interpretation but the weibull\nparameters \u03ba and \u03b2 do not, because they are meaningless for models other than the\nweibull.\n\noutliers are perhaps the most obvious form of departure from the model, but the as-\nsumed dependence structure is usually more crucial in applications. in example 6.25,\nfor instance, a confidence interval was three times too short when dependence was un-\naccounted for. although independence is often assumed, not only is mild dependence\noften difficult to detect, but also it may be hard to formulate a suitable alternative.\nin applications independence may be assured by the design of the investigation, but\noften it must be checked empirically, for example using time series tools such as the\ncorrelogram.\n\none way to view an estimating function is that it defines a parameter t(f) implicitly\n\nas the solution to the population equation\n\n(cid:5)\n\ng {y; t(f)} d f(y) = 0,\n\n "}, {"Page_number": 333, "text": "7.2 \u00b7 estimating functions\n\n321\n\nwhere f is any member of the class of distributions under consideration. the require-\nment that t(f) berobust of interpretation imposes restrictions on g. if, for instance,\nthe density f (y) = d f(y)/dy is symmetric about \u03b8 and we require t(f) = \u03b8 for any\nsuch density, then g(y; \u03b8) must be odd as a function of y \u2212 \u03b8, with g(\u03b8; \u03b8) = 0. in\nmany cases the requirement of robustness of interpretation indicates taking t(f) to be\na moment or related quantity, which will retain its meaning for all models possessing\nthe necessary moments.\n\none approach to downweighting bad data stems from observing that (7.17) implies\nthat the effect of y j on \u02dc\u03b8 is proportional to g(y j ; \u03b8). if this is large, then \u02dc\u03b8 will tend\nto be far from its estimand \u03b8. this suggests that the sensitivity of \u02dc\u03b8 to an observation\ny be measured by the influence function of \u02dc\u03b8,\n\nl(y; \u03b8) =\n\n\u2212(cid:26)\n\ng(y; \u03b8)\n\ndg(u;\u03b8)\n\nd\u03b8\n\nf (u; \u03b8) du\n\n;\n\nthis is simply a rescaling of the estimating function. our earlier discussion implies\nthat var(\u02dc\u03b8)\n\n\u22121var{l(y ; \u03b8)} in terms of a single observation y .\n\n.= n\n\nexpression (7.17) suggests that the impact of outliers can be reduced by using esti-\nmating functions and hence influence functions that are bounded in y. one possibility\nis a redescending function such as (y \u2212 \u03b8)/{1 + (y \u2212 \u03b8)2}, which tends to zero as\n|y \u2212 \u03b8| \u2192 \u221e. another possibility is to truncate a standard function such as y \u2212 \u03b8,\nso that values of y distant from \u03b8 have limited impact on \u02dc\u03b8. see figure 7.3.\n\nexample 7.19 (huber estimator) the effect of outliers on the estimation of a mean\nmay be reduced by using\n\n\uf8f1\uf8f2\n\uf8f3\n\ngc(y; \u03b8) =\n\n\u2212c,\ny \u2212 \u03b8, \u2212c < y \u2212 \u03b8 < c,\nc,\n\ny \u2264 \u03b8 \u2212 c,\n\u03b8 + c \u2264 y,\n\nwhere the constant c > 0 ischosen to balance robustness and efficiency. robustness\nto outliers is increased but efficiency at the normal model is reduced by decreasing\nc; when c = \u221e we have g\u221e(y; \u03b8) = y \u2212 \u03b8 and \u02dc\u03b8 = y . the estimator corresponding\nto gc(y; \u03b8) issometimes called the huber estimator of location. the parameter t(f)\nis the centre of an underlying symmetric density and equals its mean when c = \u221e\nand its median when c = 0. these are not the same when the underlying density is\nasymmetric, and then t(f) has no simple direct interpretation, though it may depend\nonly weakly on c for certain choices of f.\n\nthe finite-sample efficiency of gc(y; \u03b8) as afunction of c for various symmetric\ndensities is shown in the right panel of figure 7.4. the quantity plotted is (7.18)\ndivided by the variance of g\u221e(y ; \u03b8) = y \u2212 \u03b8, asthis rather than the score function\nfor the true density would usually be used in practice. under the normal model the\nefficiency of gc is essentially one when c = 2, dropping to the value 2/\u03c0 = 0.637 for\nthe median when c \u2192 0. overall a good choice seems to be c = 1.345, which is often\nthe default in software packages; it has efficiency 0.95 for normal data, but beats g\u221e\n(cid:1)\nin the other cases shown.\n\npeter johann huber\n(1934\u2013) has been\nprofessor of statistics at\neth z\u00a8urich,\nmassachusetts institute of\ntechnology, and harvard\nand bayreuth universities,\nand is now retired.\n\nor huber\u2019s proposal 2.\n\n "}, {"Page_number": 334, "text": "322\n\n7 \u00b7 estimation and hypothesis testing\n\nthe discussion above presupposes that the scale of the underlying density is known,\neven if the location is not. in practice estimation of scale has little effect on the effi-\nciency of location estimators, and the results above apply with little change provided\nscale is estimated robustly, for example using the median absolute deviation.\n\nto illustrate optimality under weak conditions on the underlying model, suppose\nthat we intend to estimate \u03b8 using the weighted combination of unbiased linear esti-\nmating functions\n\nwhere var(y j ) = v j (\u03b8) may be a function of \u03b8. wesuppose that the mean and variance\nfunctions \u00b5 j (\u03b8) and v j (\u03b8) for each of the y j are known, but make no assumption about\ntheir distributions. notice that our argument for consistency of \u02dc\u03b8 will apply under mild\nconditions on the weights and the moments. suppose also that the y j are uncorrelated.\nthen (7.18) is\n\nm(cid:13)\nj=1\n\nw j (\u03b8){y j \u2212 \u00b5 j (\u03b8)},\n\n(cid:2)\n(cid:22)(cid:2)\nj w 2\nj w j (\u03b8)\u00b5(cid:1)\n\nj (\u03b8)v j (\u03b8)\nj (\u03b8)\n\n(cid:23)2\n\n,\n\nn(cid:13)\nj=1\n\nn(cid:13)\nj=1\n\nj (\u03b8) = d\u00b5 j (\u03b8)/d\u03b8, and our earlier discussion suggests that we seek the\n\nwhere \u00b5(cid:1)\nweights w j (\u03b8) that minimize this. this is equivalent to the problem\n= c,\n\nsubject to\n\nw 2\n\nw j \u00b5(cid:1)\n\nj v j\n\nj\n\nmin\nw1,...,wn\n\nn(cid:13)\nj=1\n\nfor some constant c. use of lagrange multipliers gives w j (\u03b8) \u221d \u00b5(cid:1)\noptimal estimating equation is\n\nj (\u03b8)/v j (\u03b8), so the\n\n\u00b5(cid:1)\nj (\u03b8)\n\n1\n\n{y j \u2212 \u00b5 j (\u03b8)} =0.\n\nv j (\u03b8)\n\nj (\u03b8), so \u00b5(cid:1)\n\nj (\u03b8)} =0, which is optimal.\n\n(cid:2){y j \u2212 \u03ba(cid:1)\nj (\u03b8) and variance \u03ba(cid:1)(cid:1)\n\n(7.21)\nan exponential family variable y j with log likelihood contribution y j \u03b8 \u2212 \u03ba j (\u03b8) has\nj (\u03b8) = v j (\u03b8) and (7.21) reduces to the score\nmean \u03ba(cid:1)\nequation,\nexample 7.20 (straight-line regression) let the y j have means \u00b5(\u03b2) = x j \u03b2,\nj (\u03b2) = x j , and g(y j , \u03b2) = y j \u2212 x j \u03b2. if var(y j ) = v j (\u03b2)\nwith x j known. then \u00b5(cid:1)\n\u02dc\u03b2 = (cid:2)\nx j (y j \u2212 \u03b2x j ), and the corresponding estimator is\nis constant, (7.21) becomes\nx 2\nj . this is the least squares estimator of \u03b2, corresponding to a\nnormal distribution for y j , but it has much wider validity.\n(cid:2)\nif var(y j ) = x j \u03b2, as would be the case if y j were poisson with mean x j \u03b2, then the\noptimal estimating function is\nx j . as inthe normal\n(cid:1)\ncase, \u02dc\u03b2 is optimal more widely.\n\n(y j \u2212 \u03b2x j ), and \u02dc\u03b2 = (cid:2)\n\ny j x j /\n\n(cid:2)\n\n(cid:2)\n\n(cid:2)\n\ny j /\n\nestimating equations of form similar to (7.21) are very important in the regression\n\nmodels encountered in chapters 8 and 10.\n\n "}, {"Page_number": 335, "text": "this may be omitted at a\nfirst reading.\n\n7.2 \u00b7 estimating functions\n\n323\n\n7.2.3 dependent data\nin earlier discussion, for example in section 6.1, we used the fact that standard likeli-\nhood asymptotics also apply to some types of dependent data. for some explanation\nof this, consider the more general context of unbiased estimating functions for a scalar\n\u03b8. suppose that \u02dc\u03b8 is defined as the solution to the equation\n\nn(cid:13)\nj=1\n\ng j (y ; \u03b8) = 0,\n\n(7.22)\n\nwhere g j (y ; \u03b8) depends only on y1, . . . ,y j and is such that for all \u03b8,\n\ne{g1(y )} =0,\n\ne{g j (y ; \u03b8) | y1, . . . ,y j\u22121} =0,\nso that the unconditional expectation e{g j (y ; \u03b8)} =0 for all\n\nj = 2, . . . ,n ,\n\nj. if j > k, then\n\ncov{g j (y ; \u03b8), gk(y ; \u03b8)} =e{ g j (y ; \u03b8)gk(y ; \u03b8)}\n\n= e[gk(y ; \u03b8)e{g j (y ; \u03b8) | y1, . . . ,y j\u22121}] = 0,\n\nso\n\n(cid:14)\nn(cid:13)\nj=1\n\nvar\n\n(cid:15)\n\n= n(cid:13)\n\nj=1\n\ng j (y ; \u03b8)\n\nvar{g j (y ; \u03b8)}.\n\nthe left of (7.22) is a zero-mean martingale, and under mild regularity conditions a\nmartingale central limit theorem as n \u2192 \u221e gives\n\n(cid:2)\nj=1 var{g j (y ; \u03b8) | y1, . . . ,y j\u22121}\n(cid:24)(cid:2)\nj=1 e{dg j (y ; \u03b8)/d\u03b8 | y1, . . . ,y j\u22121}(cid:25)2\n\nn\n\nn\n\n,\n\n\u22121/2(\u02dc\u03b8 \u2212 \u03b8) d\u2212\u2192 z , where v =\n\nv\n\n(7.23)\nand z is standard normal. thus provided the random variable v is used to estimate\nthe variance of \u02dc\u03b8, confidence intervals for \u03b8 can be set in the usual way.\ntwo main possibilities arise for the limiting behaviour of v . in anergodic\nmodel a deterministically rescaled version of v converges to a constant as n \u2192 \u221e,\nsuch as nv p\u2212\u2192 v > 0. this occurs, for instance, with independent data, ergodic\nmarkov chains, and many time series models. under regularity conditions the usual\narguments then apply to the rescaled estimator, whose limiting distribution is normal,\nand the argument starting from (7.17) yields (7.18). the second possibility is that\nwhen rescaled, v converges to a nondegenerate random variable d. the model is\nthen said to be non-ergodic, and as the limiting distribution of the rescaled estimator\nis d\n\n\u22121/2 z, standard large-sample theory does not apply.\nas with independent data, we can find the optimal finite-sample choice of weighting\n\nfunctions within the class of linear combinations of the g j (y ; \u03b8),\n\nn(cid:13)\nj=1\n\nw j (\u03b8)g j (y ; \u03b8),\n\nwhere the w j (\u03b8), now random variables, can depend on y1, . . . ,y j\u22121 and \u03b8. this\n\n "}, {"Page_number": 336, "text": "324\n\nturns out to be\n\n7 \u00b7 estimation and hypothesis testing\n\nw j (\u03b8) = \u2212e{dg j (y ; \u03b8)/d\u03b8 | y1, . . . ,y j\u22121}\nvar{g j (y ; \u03b8) | y1, . . . ,y j\u22121}\n\n.\n\n(7.24)\n\nthis finite-sample result is independent of the asymptotic properties of \u02dc\u03b8.\n\nexample 7.21 (branching process) the branching process was first used to model\nthe survival of surnames, it being supposed that a surname would die out if all every\nmale bearing it had no sons, but it has applications in epidemic modelling and else-\nrandom number of individuals, so y j = (cid:2)y j\u22121\nwhere. each of the y j\u22121 individuals in generation j \u2212 1 independently gives birth to a\ni=1 ni , where the ni are independent with\nmean \u03b8 and variance \u03c3 2. wetake y0 = 1. here g j (y ; \u03b8) = y j \u2212 \u03b8y j\u22121 is unbiased\n(cid:6)\nwhatever the distribution of the ni , while\nvar{g j (y ; \u03b8) | y1, . . . ,y j\u22121) = y j\u22121\u03c3 2, e\n= y j\u22121.\n(cid:2)\nthe optimal weights are w j (\u03b8) = 1/\u03c3 2, here non-random, and the corresponding\nthus \u02dc\u03b8 = (cid:2)\n(cid:2)\nj=2(y j \u2212 \u03b8y j\u22121) = 0, whatever the distribution of the ni .\nn\nestimating equation is\nj=1 y j is optimal and v = \u03c3 2/\nn\u22121\nn\u22121\nj=1 y j+1/\nextinction is certain if \u03b8 \u2264 1 butnot if \u03b8 > 1. if extinction occurs then no estimator\nof \u03b8 can be consistent. when \u03b8 > 1 and given that extinction does not occur, (7.23)\n\u22121/2(\u02dc\u03b8 \u2212 \u03b8) d\u2212\u2192 \u03c3 z. inthis case \u03b8\u2212nv converges to a nondegenerate\nimplies that v\nrandom variable and the asymptotics are nonstandard. confidence intervals for \u03b8 are\nbest constructed using v .\n\n(cid:19)(cid:19)(cid:19)(cid:19) y1, . . . ,y n\u22121\n(cid:2)\n\n\u2212 dg j (y ; \u03b8)\n\nn\nj=1 y j\u22121.\n\n(cid:7)\n\nd\u03b8\n\nother growth models such as birth processes and non-stationary diffusions can\nalso be non-ergodic. as the discussion above suggests, inference for \u03b8 is then best\n(cid:1)\nperformed using observed information or its generalization v\n\n\u22121.\n\n(cid:27)\n\nthe argument leading to (7.23) applies in particular to maximum likelihood estima-\ntors. we write f (y1, . . . , yn; \u03b8) = f (y1; \u03b8)\nj=2 f (y j | y1, . . . , y j\u22121; \u03b8) and express\nthe score as\n\nn\n\nd(cid:19)(\u03b8)\n\nd\u03b8\n\n= d log f (y1; \u03b8)\n\nd\u03b8\n\n+ n(cid:13)\n\nj=2\n\nd log f (y j | y1, . . . ,y j\u22121; \u03b8)\n\nd\u03b8\n\n= n(cid:13)\n\nj=1\n\ng j (y ; \u03b8).\n\nhere w j (\u03b8) \u2261 1, so the unweighted score is optimal in finite samples. in the ergodic\ncase, taylor series arguments establish the usual properties of maximum likelihood\nestimators and likelihood ratio statistics, subject to regularity conditions like those\nneeded for independent data.\n\nexercises 7.2\n1\n\nshow that if an estimating function undergoes a smooth 1\u20131 reparametrization by writing\ng(y; \u03b8) = g{y; \u03b8(\u03c8)} = g\n(y; \u03c8), then \u02dc\u03b8 = \u03b8( \u02dc\u03c8). establish also that (7.18) is unchanged.\nshow that the sample median of a continuous density solves (7.15) with\n\n(cid:1)\n\n2\n\ng(y; \u03b8) = h(y \u2212 \u03b8) \u2212 h(\u03b8 \u2212 y),\n\nh(u) isthe heaviside\nfunction.\n\n "}, {"Page_number": 337, "text": "7.3 \u00b7 hypothesis tests\n\n325\n\ngiving g(y ; \u03b8) = (cid:2){i (\u03b8 \u2264 y j ) \u2212 i (y j \u2264 \u03b8)}, adescending staircase, with a unique so-\n\n\u22121/2\u03b5n\u03b6 \u22121(1 \u2212 n\n\n\u22121/2\u03b7n\u03b6 \u22121 + \u00b7\u00b7\u00b7), and hence find the desired result.\n\nlution only when n is odd. find (7.18). surprised?\nfind the form of estimating function for an exponential family model.\nto verify (7.17), show that the numerator and denominator in the first ratio may be written\nas n1/2\u03b5n and n\u03b6 + n1/2\u03b7n, where \u03b6 (cid:5)= 0 and \u03b5n and \u03b7n are o p(1) random variables. deduce\nthat the ratio is n\nreread the proof of the cram\u00b4er\u2013rao lower bound, and then establish (7.19).\nto establish (7.20), let c and g denote the p \u00d7 p matrix e{\u2212\u2202g(y ; \u03b8)t/\u2202\u03b8} and the p \u00d7 1\nvector g(y ; \u03b8), note that c = cov{g, u (\u03b8)} and, assuming that c is invertible, compute\nthe variance matrix of c\nlet f\u03bd represent the gamma distribution with unit mean and shape parameter \u03bd. investigate\nhow the quantity t(f\u03bd) determined by the huber estimating function gc(y; \u03b8) depends on\nc and \u03bd.\nto establish (7.24), note that (7.18) depends on\n\n\u22121g \u2212 i (\u03b8)\n\n\u22121u (\u03b8).\n\n(cid:14)\nn(cid:13)\nj=1\n\ne\n\n(cid:9)(cid:15)\n\n(cid:8)\n\n(cid:14)\nn(cid:13)\nj=1\n\nw 2\n\nj e j\u22121\n\ng2\nj\n\n, e\n\nw 2\n\nj e j\u22121\n\n(cid:4)(cid:15)\n\n,\n\n(cid:3)\n\ndg j\nd\u03b8\n\nwhere e j\u22121 denotes expectation conditional on y1, . . . ,y j\u22121 and g j = g j (y ; \u03b8). call the\nsums here a2 and b, sothat (7.18) has inverse {e(b)}2/e(a2).\n(a) use the fact that e{(b/a \u2212 c a)2} \u22650 toshow that e( b)2/e(a2) \u2264 e(b2/a2).\n(b) deduce that e(b2/a2) ismaximized by (7.24), and show that this choice gives\ne(b)2/e(a2) = e(b2/a2).\n(cid:2)\n(c) hence show that (7.18) is minimized among the class of estimating functions\n\nw j (\u03b8)g j (y ; \u03b8) bytaking (7.24).\n\n(godambe, 1985)\nfind the optimal estimating function based on dependent data y1, . . . ,y n with g j (y ; \u03b8) =\ny j \u2212 \u03b8y j\u22121 and var{g j (y ; \u03b8) | y1, . . . ,y j\u22121} =\u03c3 2. derive also the estimator \u02dc\u03b8. find the\nmaximum likelihood estimator of \u03b8 when the conditional density of y j given the past is\nn (\u03b8 y j\u22121, \u03c3 2). discuss.\n\n3\n4\n\n5\n6\n\n7\n\n8\n\n9\n\n7.3 hypothesis tests\n7.3.1 significance levels\na scientific theory or hypothesis leads to assertions that are testable using empirical\ndata. such data may discredit the hypothesis, as when the michelson\u2013morley exper-\niment demolished the nineteenth-century notion of an aether in which the earth and\nplanets move, or they may lead to elaboration or development of it, just as quantum\ntheory supercedes newtonian mechanics but does not make newton\u2019s laws of mo-\ntion useless for daily life. one way to investigate the extent to which an assertion is\nsupported by the data y is to choose a test statistic, t = t(y ), large values of which\ncast doubt on the assertion and hence on the underlying theory. this theory, the null\nhypothesis h0, places restrictions on the distribution of y and is used to calculate a\nsignificance level or p-value\n\npobs = pr0(t \u2265 tobs),\n\n(7.25)\n\n "}, {"Page_number": 338, "text": "326\n\n7 \u00b7 estimation and hypothesis testing\n\nwhere tobs is the value of t actually observed. a distribution computed under the\nassumption that h0 is true is called a null distribution, and then we use pr0, e0, . . .\nto indicate probability, expectation and so forth. small values of pobs correspond to\nvalues tobs unlikely to arise under h0, and signal that theory and data are inconsistent.\nthe rationale for calculating the probability that t \u2265 tobs in (7.25) is that any value t\n(cid:1) >\ntobs would cast even greater doubt on h0. ahypothesis that completely determines\nthe distribution of y is called simple; otherwise it is composite.\n\nif there is a precise idea what situation will hold if the null hypothesis is false,\nthen there is a clearly specified alternative hypothesis, h1, and we can choose a test\nstatistic that has high probability of detecting departures from h0 in the direction of\nh1. otherwise the alternative may be very vague. in either case calculation of (7.25)\ninvolves only h0.\n\nfor many standard tests the null distribution of t is tabulated, available in statisti-\ncal packages, or readily approximated. if not, (7.25) can be estimated by generating\n\u2217\nr independent sets of data y\nr from the null distribution of y , calculating the corre-\n\u2217\n\u2217\nsponding values t\nr ), and then setting\nr\n\n= t(y\n\n\u2265 tobs)\n\n\u2217\nr\nr=1 i (t\nr\n1 + r\nthe added 1s here arise because under h0 the original value tobs is a realization\nof t and trivially tobs \u2265 tobs. the indicators i (t\n\u2265 tobs) are independent bernoulli\nvariables with probability pobs under h0, and this enables a suitable r to be determined\n(exercise 7.3.1).\n\n(7.26)\n\n\u2217\nr\n\n;\n\n(cid:1)pobs = 1 +(cid:2)\n\nt\n\n(cid:14)\n\n(cid:2)\n(cid:2)\n\n(cid:1) = \u03bbn\n1 exp\n\u03bbn\n0 exp\n\n(cid:8) \u2212 \u03bb1\n(cid:8) \u2212 \u03bb0\n\nexample 7.22 (exponential density) consider an exponential random sample\ny1, . . . ,y n with parameter \u03bb. wewish to test \u03bb = \u03bb0 against the alternative \u03bb = \u03bb1,\nwith both \u03bb0 and \u03bb1 known, using the likelihood ratio\nn(cid:13)\n(\u03bb0 \u2212 \u03bb1)\nj=1\ny j > tobs), where tobs = (cid:2)\nor equivalently (\u03bb0 \u2212 \u03bb1)\n(cid:5) \u221e\n\ny j is large. if\ny j . under the null\ny j has a gamma distribution with index n and rate \u03bb0, so if\u03bb 1 < \u03bb0,\n\nwe declare that doubt is cast on \u03bb0 if t\n\u03bb1 < \u03bb0, the value of pobs is pr0(\nhypothesis,\nthe p-value is\npobs =\n\n(cid:9)\n(cid:9) = exp\n(cid:2)\n\n\u2212v dv = pr(v \u2265 \u03bb0tobs),\n\ny j + n log(\u03bb1/\u03bb0)\n\n\u2212\u03bb0u du =\n\n(cid:5) \u221e\n\n(cid:2)\n\n(cid:2)\n\ny j\ny j\n\n(cid:15)\n\ne\n\ne\n\n.\n\n(cid:1)\n\n0un\u22121\n\u03bbn\n\u0001(n)\n\ntobs\n\nv n\u22121\n\u0001(n)\n\n\u03bb0tobs\n\nwhere v has a gamma distribution with index n; pobs can be calculated exactly because\n(cid:1)\n\u03bb0 and tobs are known.\n\nexamples of situations with a vague alternative hypothesis are given below.\n\ninterpretation\nthe significance level may be written as pobs = 1 \u2212 f0(tobs), where f0 is the null\ndistribution function of t , supposed to be continuous. one interpretation of pobs\n\n "}, {"Page_number": 339, "text": "7.3 \u00b7 hypothesis tests\nstems from the corresponding random variable, p = 1 \u2212 f0(t ). for 0 \u2264 u \u2264 1, its\nnull distribution is\n\n327\n\npr0 {1 \u2212 f0(t ) \u2264 u} = pr0\n\n(cid:22)\n\u22121\n(cid:22)\nf\n0\n= 1 \u2212 f0\n\n(cid:23)\n(cid:23) = u,\n\n(1 \u2212 u) \u2264 t\n(1 \u2212 u)\n\u22121\nf\n0\n\nthat is, uniform on the unit interval. hence if we regard the observed tobs as being just\ndecisive evidence against h0, then this is equivalent to following a procedure which\nrejects h0 with error rate pobs: if wetested many different hypotheses and rejected\nthem all, the same tobs having arisen in each case, then a proportion pobs of our\ndecisions would be incorrect. this interpretation applies exactly if f0 is known, and\nthe test is then called exact; otherwise it will typically apply only as an approximation\nin large samples.\n\na common misinterpretation of the p-value is as the probability that the null hy-\npothesis is true. this cannot be the case, because alternative hypotheses play no\ndirect role in its calculation. bayesian p-values account for alternatives and do have\nthis more direct interpretation; see section 11.2.2.\n\nhypothesis testing is very useful in certain contexts but has important limitations.\na first is that statistical significance of a result may be quite different from its practical\nimportance, because even a very small pobs may correspond to an uninteresting depar-\nture from the null hypothesis. for example, a test for lack of fit of a parametric model\nmay be highly significant even though the model is satisfactory, simply because the\nfit is poor only in an unimportant part of the distribution or because the sample size\nis so large that no simple parametric model can be expected to fit well. on the other\nhand a large value of pobs may arise when effects of real importance are undetectable\nbecause the sample size is too small. computer models of climate change suggest that\nrare weather events may be occuring more frequently, for example, but most daily\ntemperature series are too short to detect such small changes.\n\na second limitation is that even a very small p-value may sometimes indicate\nmore support for the null than for an alternative hypothesis. a simple test of the\nnull hypothesis \u00b5 = 0 based on a single n (\u00b5, 1) random variable with value y = 3\nagainst the alternative hypothesis \u00b5 = 20 has significance level 1 \u2212 \u0001(y)\n.= 0.001,\nbut \u00b5 = 0 isclearly more plausible than \u00b5 = 20.\n\na third limitation is that a p-value simply gives evidence against the null hypothesis\nand does not indicate which of a family of alternatives is best supported by the data.\nfor this reason the use of confidence intervals for model parameters is generally\npreferable, when it is feasible.\n\ngoodness of fit tests\nin earlier chapters we used graphs such as probability plots to assess model fit. we\nnow briefly discuss how to supplement such informal procedures with more formal\nones. suppose initially that the null hypothesis is that a random sample y1, . . . ,y n\nhas issued from a known continuous distribution f(y). then we can compare f with\n\n "}, {"Page_number": 340, "text": "n(cid:13)\nj=1\n\n(cid:22)\n\n328\n\nthe empirical distribution function\n\n(cid:1)f(y) = n\n\n\u22121\n\n7 \u00b7 estimation and hypothesis testing\n\ni (y j \u2264 y),\n\nwhose mean and variance are f(y) and f(y){1 \u2212 f(y)}/n under h0.\nstandard measures of distance between f and (cid:1)f include the kolmogorov\u2013smirnov,\n\ncram\u00b4er\u2013von mises and anderson\u2013darling statistics\n\nj\n\nsup\ny\n\n|(cid:1)f(y) \u2212 f(y)| =max\n{(cid:1)f(y) \u2212 f(y)}2 d f(y) = 1\n{(cid:1)f(y) \u2212 f(y)}2\nf(y){1 \u2212 f(y)\n\nd f(y) = \u2212n \u2212 n(cid:13)\n\n+ 1\nn\n\n12n2\n\n(cid:5) \u221e\n(cid:5) \u221e\n\n\u2212\u221e\n\nn\n\n\u2212\u221e\n\nn(cid:13)\nj=1\n2 j \u2212 1\n\nj=1\n\nn\n\nj/n \u2212 u( j), u( j) \u2212 ( j \u2212 1)/n\n(cid:4)2\n\n(cid:3)\nu( j) \u2212 2 j \u2212 1\n\n,\n\n2n\n\n(cid:22)\nu( j)(1 \u2212 u(n+1\u2212 j))\n\n(cid:23)\n\n,\n\nlog\n\n(cid:23)\n\n,\n\nwhere the u j = f(y j ) have auniform null distribution and the u( j) are their or-\nder statistics; see section 2.3. the first of these is simple and widely used, while\nof the variance of (cid:1)f(y) on y, the third makes it easier to detect lack of fit for ex-\nthe second and third put more weight on the tails; by allowing for the dependence\ntreme values of y. all three statistics converge rapidly to their limiting distribu-\ntions as n \u2192 \u221e, but simulation can be used to estimate p-values if tables are not at\n\u22121/2 and\nhand. the kolmogorov\u2013smirnov statistic has 0.95 and 0.99 quantiles 1.358n\n\u22121/2 for large n; significance is declared if the empirical distribution function of\n1.628n\nthe u( j) passes confidence bands defined in terms of these quantiles. see figures 6.14\nand 6.20.\n\nexample 7.23 (danish fire data)\nin section 6.5.1 we saw that the rescaled times\nu1 = t1/t0, . . . ,u n = tn/t0 of the events of a homogeneous poisson process observed\ncase, therefore, we can take (cid:1)f(y) = n\non [0, t0] may be regarded as the order statistics of n uniform random variables. in this\nh(y \u2212 u j ) and f(y) = y, for 0 \u2264 y \u2264 1,\nthe lower right panel of figure 6.14 shows (cid:1)f(y) for the 254 largest danish fire\n\nand use the above tests to assess the adequacy of the poisson process.\n\n(cid:2)\n\n\u22121\n\nclaims, for which the kolmogorov\u2013smirnov, cram\u00b4er\u2013von mises, and anderson\u2013\ndarling statistics equal 0.095, 0.002, and 2.718 respectively. to assess the significance\nof these values we computed the three statistics for 10,000 samples of 254 independent\nvariables generated from the u (0, 1) distribution. just 207 of the simulated\nkolmogorov\u2013smirnov statistics exceeded the observed value, giving significance\n\nlevel 0.0208. the solid diagonal lines show the regions within which (cid:1)f would have\n\nto fall in order for significance not to be achieved at the 0.05 and 0.01 levels, the inner\n0.05 lines are breached but the outer 0.01 ones are not, consistent with significance\nat the 0.02 level. the significance levels for the cram\u00b4er\u2013von mises and anderson\u2013\ndarling statistics were 0.0348 and 0.0397, so the rate function for the claims does\nseem to vary. this illustrates one drawback of generic tests of fit such as these, which\n(cid:1)\ncan suggest that the model is inadequate, but not how.\n\nh(u) isthe heaviside\nfunction.\n\n "}, {"Page_number": 341, "text": "figure 7.5 analysis of\nmaize data. left:\nempirical distribution\nfunction for height\ndifferences, with fitted\nnormal distribution (dots).\nright: null density of\nanderson\u2013darling\nstatistic t for normal\nsamples of size n = 15\nwith location and scale\nestimated. the shaded part\nof the histogram shows\nvalues of t\nthe observed value tobs.\n\nin excess of\n\n\u2217\n\n7.3 \u00b7 hypothesis tests\n\n329\n\nn\no\n\ni\nt\nc\nn\nu\n\nf\n \n\nn\no\n\ni\nt\n\nu\nb\ni\nr\nt\ns\nd\n\ni\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n0\n\n0\n\n.\n\n3\n\n0\n\n.\n\n2\n\n0\n\n.\n\n1\n\n0\n\n.\n0\n\n-100 -50\n\n0\n\ny\n\n50\n\n100\n\n0.0\n\n0.5\n\n1.0\n\n1.5\n\nt*\n\nthis example is atypical, because f generally depends on unknown parameters.\nan exact test may be available anyway, for example using the maximal invariant of\na group transformation model. an observation from a location-scale model may be\nwritten as y = \u03b7 + \u03c4 \u03b5, where \u03b5 has known distribution g, and f(y) = g{(y \u2212 \u03b7)/\u03c4}.\nmost useful estimators are equivariant, with\n\n(cid:1)\u03b7(y1, . . . ,y n) = \u03b7 + \u03c4 h1(\u03b51, . . . , \u03b5n), (cid:1)\u03c4 (y1, . . . ,y n) = \u03c4 h2(\u03b51, . . . , \u03b5n).\ny j \u2212(cid:1)\u03b7(cid:1)\u03c4\n, j = 1, . . . ,n ,\nmay be tested by comparing the empirical and fitted distribution functions (cid:1)f(y) and\ng{(y \u2212(cid:1)\u03b7)/(cid:1)\u03c4}.\n\ndepends only on g, h1, and h2 and not on the parameters. thus the form of g\n\n= \u03b7 + \u03c4 \u03b5 j \u2212 \u03b7 + \u03c4 h1(\u03b51, . . . , \u03b5n)\n\n= \u03b5 j \u2212 h1(\u03b51, . . . , \u03b5n)\n\nthen the joint distribution of the residuals\n\n\u03c4 h2(\u03b51, . . . , \u03b5n)\n\nh2(\u03b51, . . . , \u03b5n)\n\nexample 7.24 (maize data) under the matched pair model for the maize data of\ntable 1.1, the pairs of plants are independent and their height differences y j have\nmean \u03b7 and variances \u03c4 = 2\u03c3 2. our discussion in section 3.2.2 presupposed that the\nbe the case. to assess this we take(cid:1)\u03b7 and(cid:1)\u03c4 2 to be the sample average and variance, and\ny j are normally distributed, but the left panel of figure 7.5 suggests that this may not\ncompute the anderson\u2013darling statistic based on the (y j \u2212(cid:1)\u03b7)/(cid:1)\u03c4 . its value is 0.618,\nwith significance level(cid:1)pobs = 0.0874 computed from the 10,000 simulations shown\n\nin the right panel of the figure. the assumption of normality seems reasonable.\n\n(cid:1)\n\nsimilar ideas can be applied to other group transformation models. among other\ngoodness of fit tests are those based on the chi-squared statistics described in sec-\ntion 4.5.3.\n\none- and two-sided tests\noften large and small values of t suggest different departures from the null hypothesis.\nlarge values of goodness of fit statistics, for instance, imply that the model fits badly,\nbut extremely small values might in some circumstances lead one to suspect that the\n\n "}, {"Page_number": 342, "text": "330\n\n7 \u00b7 estimation and hypothesis testing\n\ndata had been faked, the fit being too good to be true. with departures of two types it\nmay be appropriate to use t 2 or equivalently |t| as the test statistic, with significance\nlevel pr0(t 2 \u2265 t 2\nobs). this is not useful in a case like figure 7.5, however, owing to\nthe asymmetry of the null density of t , and then we regard the test as having two\npossible implications, measured by\n\np\n\np\n\n+\nobs\n\n\u2212\nobs\n+\nobs\n\n= pr0(t \u2265 tobs),\n\n= pr0(t \u2264 tobs),\n= 1 + pr0(t = tobs), which\n+ p\ncorresponding to one-sided tests. note that p\nequals unity if the distribution of t is continuous. let p\nrepresent the random\nand p\nvariables corresponding to these two-sided significance levels. if both large and small\nvalues of t may be regarded as evidence against h0 we use p = min(p\n) as the\nobs)} as the significance level. when\noverall test statistic, and take pr0{p \u2264 min( p\n\u2212\nthe test is exact and t is continuous the density of p is uniform on the interval (0, 1\n2 ),\n+\n\u2212\nand the two-sided significance level equals 2 min( p\nobs). this is the p-value for\nobs\na two-sided test.\n\n+, p\n\n\u2212\nobs\n+\n\n+\nobs\n\n, p\n\n, p\n\n\u2212\n\n\u2212\n\nexample 7.25 (student t test) let y1, . . . ,y n be a normal random sample with\nmean \u00b5 and variance \u03c3 2. suppose that the null hypothesis is \u00b5 = \u00b50, and the two-\nsided alternative is that \u00b5 takes any other real value, with no restriction on \u03c3 2 under\neither hypothesis. both hypotheses are composite.\nthe likelihood ratio statistic is (example 4.31)\n\nwp(\u00b50) = 2\n\n(cid:19)(\u00b5, \u03c3 2) \u2212 max\n\n\u03c3 2\n\nmax\n\u00b5,\u03c3 2\n\n(cid:19)(\u00b50, \u03c3 2)\n\n(cid:6)\n\n(cid:23) = n log\n\n(cid:7)\n\n,\n\n1 + t (\u00b50)2\nn \u2212 1\n\nwhere the null distribution of t (\u00b50) = (y \u2212 \u00b50)/(s2/n)1/2 is tn\u22121. as wp(\u00b50) is a\n(cid:22)\nmonotone function of t (\u00b50)2, the significance level is\n\n(cid:23)\n\npobs = pr0{wp(\u00b50) \u2265 wobs} =pr 0\n\nt 2(\u00b50) \u2265 t 2\n\nobs\n\n,\n\nwhere wobs and tobs are the observed values of wp(\u00b50) and t (\u00b50). large values of\nwobs arise when tobs is distant from zero, suggesting that the population mean is\nnot \u00b50.\n\nthe results of section 4.5 tell us that the null distribution of wp(\u00b50) isapproxi-\n1 . wecould use this to approximate to pobs, but an exact value is available,\n\npobs = pr0\n\nt (\u00b50)2 \u2265 t 2\n\nobs\n\n(cid:8)\n\n(cid:23) = pr\n\nt 2 \u2265 t 2\n\nobs\n\n(cid:9) = 2pr(t \u2265 |tobs|),\n\n(7.27)\n\nmately \u03c7 2\nbecause\n\nwhere t \u223c tn\u22121. this is the p-value for the two-sided test.\nif we suspect that \u00b5 > \u00b50 but not that \u00b5 < \u00b50, then large positive values of t (\u00b50)\n\nwill cast doubt on h0, and the corresponding one-sided p-value is\n\n+\nobs\n\np\n\n= pr0{t (\u00b50) \u2265 tobs} =pr( t \u2265 tobs),\n\n= pr(t \u2264 tobs) measures evidence against h0 in the direction \u00b5 < \u00b50.\nwhile p\nthese differ slightly from the p-values for the one-sided likelihood ratio tests. the\n\n\u2212\nobs\n\n(cid:22)\n\n(cid:22)\n\n "}, {"Page_number": 343, "text": "7.3 \u00b7 hypothesis tests\n\ntwo-sided significance level\n\n2 min( p\n\nequals (7.27).\n\n\u2212\nobs\n\n, p\n\nobs) = 2pr(|t| \u2265 |tobs|)\n+\n\n331\n\n(cid:1)\n\nnonparametric tests\nthe examples above concern tests in parametric models, where hypotheses typically\ndetermine values of the parameters, the form of the density being supposed known.\nnonparametric tests presuppose that the data are independently sampled from an\nunspecified underlying model.\n\nexample 7.26 (sign test) a random sample y1, . . . ,y n arises from an unknown\ndistribution f. the null hypothesis h0 asserts that f has median \u00b5 equal to \u00b50, while\nthe alternative is that \u00b5 > \u00b50. both hypotheses are composite, but neither specifies a\nparametric model, and we argue as follows.\n\nthat we base a test on s = (cid:2)\n\nif the median is \u00b50, the probability that an observation y falls on either side of \u00b50\nis 1/2, and if the median is greater than \u00b50, then pr(y > \u00b50) > 1/2. this suggests\nn\nj=1 i (y j > \u00b50), large values of which cast doubt on\nh0. under the null hypothesis, s has a binomial distribution with denominator n and\nprobability 1/2, so its mean and variance are n/2 and n/4. hence the p-value is\n\npobs = pr0(s \u2265 sobs) = n(cid:13)\n\nr=sobs\n\n(cid:3)\n\n(cid:4)\n\n(cid:6)\n\n(cid:7)\n\nn\nr\n\n.= 1 \u2212 \u0001\n\n1\n2n\n\n2(sobs \u2212 n/2)\n\nn1/2\n\nby normal approximation to the binomial null distribution of s.\n\n,\n\n(cid:1)\n\nexample 7.27 (wilcoxon signed-rank test) a random sample y1, . . . ,y n has been\ndrawn from a density that is symmetric about \u00b5 but otherwise unspecified. we wish\nto test the hypothesis that \u00b5 = 0. the sign test is one possibility, but as it does not\nuse the symmetry of the density, a better test can be found.\nwilcoxon signed-rank statistic is w = (cid:2)\nlet r j denote the rank of |y j| among |y1|, . . . ,|y n|, and let z j = sign(y j ). the\nj z j r j . large positive values of w suggest\n\u00b5 >0, while large negative values suggest \u00b5 <0.\nto find the null mean and variance of w , note that when \u00b5 = 0 the ranks, r j , are\nindependent of the signs, z j , bysymmetry about zero, and that\n\nvar0(z j ) = (\u22121)2 1\n2\n\n+ 12 1\n2\n\n= 1, e0(z j r j ) = n\n\n\u22121\n\nk\n\n1\n2\n\n+ (\u2212k)\n\n1\n2\n\n= 0,\n\n(cid:18)\n\n(cid:20)\n\nimplying that e0(w ) = 0. to find var0(w ), we argue conditionally on the ranks\nr1, . . . , rn, finding\nn(cid:13)\nj=1\n\nj var0(z j ) = n(cid:13)\n\n(cid:19)(cid:19)(cid:19)(cid:19)(cid:19) r1, . . . , rn\n\n= n(cid:13)\n\n= n(cid:13)\n\nand this equals n(n + 1)(2n + 1)/6. thus w has mean zero and variance n(n + 1)\n(2n + 1)/6 under the null hypothesis, and as its distribution is then symmetric, a\n(cid:1)\nnormal approximation to the exact p-value may be useful.\n\nz j r j\n\nvar0\n\nj=1\n\nj=1\n\nj=1\n\nr2\nj\n\nj 2,\n\nr2\n\n(cid:6)\n\nn(cid:13)\nk=1\n\n(cid:7)\n\n "}, {"Page_number": 344, "text": "332\n\ndifference d\nsign z\nrank r\n\n49 \u221267\n+\n\u2212\n14\n11\n\n7 \u00b7 estimation and hypothesis testing\n\n6\n\n23\n\n16\n\n8\n75\n+ + ++ + + + + + + + +\n13\n2\n\n41\n\n28\n\n14\n\n29\n\n56\n\n24\n\n15\n\n12\n\n15\n\n4\n\n7\n\n9\n\n3\n\n8\n\n6\n\ntable 7.2 analysis of\ndifferences for maize data.\n\n60\u221248\n\u2212\n10\n\nexample 7.28 (maize data) under the model for the maize data of table 1.1,\nthe height differences between cross- and self-fertilized plants may be written as\nd j = \u03b7 + \u03c3 (\u03b52 j \u2212 \u03b51 j ), where the \u03b5i j are independent random variables with mean\nzero and some common variance. if the \u03b5i j have the same distribution, the d j will be\nsymmetically distributed around \u03b7, while \u03b7 = 0 under the null hypothesis h0 of no dif-\nference between the effects of the different types of fertilization. if cross-fertilization\nincreases height, then \u03b7 > 0, as is suggested by the observed d j in table 7.2.\nif the d j were normally distributed, we would perform a student t test based on\nthe average and variance of the observed differences, d = 20.95 and s2 = 1424.6,\ngiving tobs = n1/2(d \u2212 0)/s = 2.15; see example 7.25. under h0 this is the realized\nvalue of a t14 variable, so pobs = pr(t \u2265 tobs) = 0.025, where t \u223c t14. though low,\nthis is not overwhelming evidence against the null hypothesis.\n\n15(cid:13)\nr=13\n\nif we wish to avoid the assumption of normality, a nonparametric test is preferable.\nunder the null hypothesis, the d j come from density symmetric about zero but not\nnecessarily normal. thirteen of them are positive, so the sign test statistic takes value\nsobs = 13, with exact significance level\n(cid:3)\n\n(cid:4)\n\npr0(s \u2265 sobs) = 1\n215\n\n15\nr\n\n= 1\n215 (1 + 15 + 105) = 0.0037;\n\n\u221a\n\n15} =0.0023. both give much\n\nnormal approximation gives 1 \u2212 \u0001{2(13 \u2212 15/2)/\nstronger evidence against h0 than does the t test.\nobserved value of w = (cid:2)\n\ntable 7.2 shows the quantities needed for the wilcoxon signed-rank test. the\nz j r j is 72, and its null distribution when n = 15 is ap-\nproximately normal with mean zero and variance 1240. therefore the p-value is\nroughly\n\npobs = pr0(w \u2265 57)\n\n.= 1 \u2212 \u0001\n\n57/12401/2\n\n(cid:8)\n\n(cid:9) = 0.053,\n\nto be compared with the values for the t and sign tests.\n\n(cid:1)\n\nwe shall see in section 7.3.2 that likelihood considerations lead to tests that are\n\u2018best\u2019 in a certain sense when there is a parametric model. but if the model is not\ncredible, nonparametric tests that make make fewer assumptions may be preferable,\nand often they perform nearly as well as parametric tests. some situations are so ill-\nspecified that parametric models are inappropriate, and the independence assumptions\nthat underlie most nonparametric tests are doubtful also. then only rough-and-ready\nmethods can be applied and conclusions are correspondingly weaker.\n\n "}, {"Page_number": 345, "text": "7.3 \u00b7 hypothesis tests\n\n333\n\n7.3.2 comparison of tests\nwe now consider how to compare different test statistics for the same problem. having\nchosen a test statistic t = t(y ) and a probability \u03b1, suppose we decide to reject the\nnull hypothesis h0 in favour of an alternative h1 at level \u03b1 if and only if the data y\nfall into the subset y\u03b1 = {y : t(y) \u2265 t\u03b1} of the sample space, where t\u03b1 is chosen so\nthat\n\npr0(t \u2265 t\u03b1) = pr0 (y \u2208 y\u03b1) = \u03b1.\n\nthe size of the test is the probability \u03b1 of rejecting h0 when it is actually true, and\ny\u03b1 is called a size \u03b1 critical region. this construction implies that as \u03b1 decreases,\n\u2282 y\u03b12 whenever \u03b11 \u2264 \u03b12, as isessential if we are to avoid\nt\u03b1 increases and that y\u03b11\nimbecilities such as \u2018h0 is rejected when \u03b1 = 0.01 but not when \u03b1 = 0.05\u2019. choosing\na test statistic and values of t\u03b1 is equivalent to specifying a system of critical regions\nfor the different values of \u03b1, so wecan discuss the test in terms of its critical regions\nif convenient.\n\nby using a fixed \u03b1 we have moved from regarding the significance level as a measure\nof evidence against h0 to using the test to decide which of the two hypotheses is better\nsupported by the data. two wrong decisions are then possible, committing a type i\nerror by rejecting h0 when it is true, or a type ii error by accepting h0 when h1 is\ntrue. the power of the test is the probability of detecting that h0 is false,\n\npr1(t \u2265 t\u03b1) = pr1(y \u2208 y\u03b1).\n\npr1, e1 and so forth\nindicate probability,\nexpectation and so forth\ncomputed under h1.\n\nexample 7.29 (normal mean) let y1, . . . ,y n be a random sample from the\nn (\u00b5, \u03c3 2) distribution with known \u03c3 2, and suppose that h0 specifies that \u00b5 = \u00b50,\nwhereas \u00b5 > \u00b50 under h1. suppose we decide to reject h0 if y exceeds some con-\nstant t\u03b1. under h0, y \u223c n (\u00b50, \u03c3 2/n), so this test has size\n\u2265 n1/2 (t\u03b1 \u2212 \u00b50)\n(cid:7)\n\npr0(y \u2265 t\u03b1) = pr0\n\n(cid:14)\n\n(cid:15)\n\n(cid:6)\n\n(cid:7)\n\n\u03c3\n\nn1/2 (y \u2212 \u00b50)\n(cid:6)\nn1/2(t\u03b1 \u2212 \u00b50)\n\n\u03c3\n\nn1/2(\u00b50 \u2212 t\u03b1)\n\n= \u0001\n\n= 1 \u2212 \u0001\n\n,\n\n\u03c3\n\n\u03c3\n\nz\u03b1 is the \u03b1 quantile of the\nn (0, 1) distribution.\n\nusing the symmetry of the normal distribution. for a test of size \u03b1, wemust choose\nt\u03b1 such that\n\nn1/2(\u00b50 \u2212 t\u03b1)\n\n\u03c3\n\n= z\u03b1,\n\ngiving t\u03b1 = \u00b50 \u2212 n\n\n\u22121/2\u03c3 z\u03b1. thus the size \u03b1 critical region is\ny\u03b1 = (cid:22)\n\u22121/2\u03c3 z\u03b1\n\n(y1, . . . , yn) : y \u2265 \u00b50 \u2212 n\n\n(cid:23)\n\n,\n\nand we can decide if y falls into this because \u03c3 2 and \u00b50 are known under h0.\n\n "}, {"Page_number": 346, "text": "334\n\nr\ne\nw\no\np\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n.\n\n0\n\n7 \u00b7 estimation and hypothesis testing\n\nfigure 7.6 power\nfunctions for a test of\nwhether the mean of a\nn (\u00b5, \u03c3 2) random sample\nof size n equals \u00b50 against\nthe alternative \u00b5 = \u00b51, as\na function of\n\u03b4 = n1/2(\u00b51 \u2212 \u00b50)/\u03c3 .\nthe test size is \u03b1 = 0.05.\nthe solid curve is the\npower function for a test\nof \u00b51 > \u00b50 based on y,\nand the dashed line is the\npower function for the\nsign test. both critical\nregions are of form\ny > t\u03b1. the dotted curve\nis the power function for y\nwhen the critical region is\ny < t\u03b1.\n\n-4\n\n-2\n\n0\n\ndelta\n\n2\n\n4\n\nif in fact \u00b5 equals \u00b51 > \u00b50, then y \u223c n (\u00b51, \u03c3 2/n), and the test has power\n(cid:20)\n\n(cid:18)\n\n(cid:28)\n\npr1\n\ny \u2265 \u00b50 \u2212 \u03c3 z\u03b1\nn1/2\n\n(cid:29)\n\nn1/2 (y \u2212 \u00b51)\n\n= pr1\n= 1 \u2212 \u0001(\u2212\u03b4 \u2212 z\u03b1) = \u0001(z\u03b1 + \u03b4),\n\n\u2265 n1/2 (\u00b50 \u2212 \u00b51)\n\n\u03c3\n\n\u03c3\n\n\u2212 z\u03b1\n\n(7.28)\nwhere \u03b4 = n1/2(\u00b51 \u2212 \u00b50)/\u03c3 measures the distance between the means under the two\nhypotheses, standardized by var(y )1/2 = \u03c3/n1/2. the power is plotted in figure 7.6,\nwith \u03b1 = 0.05. for fixed n, \u03c3 , and \u00b50, itincreases with \u00b51. when \u03c3 , \u00b50, and \u00b51 are\nfixed, the power increases with n.\n\npower can be used to choose the sample size when planning an experiment. suppose\nwe desire to perform a test of size \u03b1 and that power of at least \u03b2 is sought for detecting\nwhether \u00b51 = \u00b50 + \u03c3 \u03b3 , where \u03b3 is known. then we require \u0001(z\u03b1 + n1/2\u03b3 ) \u2265 \u03b2 and\nhence z\u03b1 + n1/2\u03b3 \u2265 \u0001\u22121(\u03b2) orequivalently n \u2265 (z\u03b2 \u2212 z\u03b1)2/\u03b3 2.\nif, for instance, \u00b50 = 0 and \u03c3 = 1, and we desire to detect whether a test of size\n0.05 could detect \u00b51 = 0.5 with power 0.8 ormore, then \u03b3 = 0.5, z\u03b1 = \u22121.645,\nz\u03b2 = 0.842 and hence we would need n \u2265 24.7\n.= 25.\n(cid:1)\ntribution to equal a specified value \u00b50, using s = (cid:2)\nexample 7.30 (sign test) example 7.26 describes a test for the median of a dis-\nn\nj=1 i (y j > \u00b50) astest statistic.\nunder h0 the distribution of s is binomial, and if a normal approximation applies, a\nsize \u03b1 critical region is determined by the value s\u03b1 such that pr0(s \u2265 s\u03b1) = \u03b1, giving\ns\u03b1 = n/2 \u2212 n1/2z\u03b1/2.\niid\u223c n (\u00b5, \u03c3 2), with\nfor anillustrative power calculation for this test, let y1, . . . ,y n\nnull hypothesis \u00b5 = \u00b50 and alternative h1 that \u00b5 = \u00b51 > \u00b50. the normal density is\n(cid:8)\nsymmetric, so its mean equals its median. now\n\npr1(y j \u2265 \u00b50) = pr1{(y j \u2212 \u00b51)/\u03c3 \u2265 (\u00b50 \u2212 \u00b51)/\u03c3} =\u0001\n\n\u22121/2\u03b4\n\nwhere again \u03b4 = n1/2(\u00b51 \u2212 \u00b50)/\u03c3 . under h1, therefore, s is approximately normal\n\u22121/2\u03b4)}, and the probability\nwith mean n\u0001(n\n\n\u22121/2\u03b4) and variance n\u0001(n\n\n\u22121/2\u03b4){1 \u2212 \u0001(n\n\n(cid:9)\n\nn\n\n,\n\n "}, {"Page_number": 347, "text": "7.3 \u00b7 hypothesis tests\n\nthat h0 is rejected is\n\n(cid:8)\npr1(s \u2265 s\u03b1) = pr1\n(cid:14)\n.= \u0001\n\n(cid:9)\n\ns \u2265 n/2 \u2212 n1/2z\u03b1/2\n(cid:24)\nn\u0001\n\n(cid:8)\n(cid:9) \u2212 n/2 + n1/2z\u03b1/2\n\u22121/2\u03b4\n(cid:9)(cid:22)\n(cid:9)(cid:23)(cid:25)1/2\n(cid:8)\nn\n1 \u2212 \u0001\nn\u22121/2\u03b4\nn\u22121/2\u03b4\n\nn\u0001\n\n(cid:8)\n\n(cid:15)\n\n,\n\n335\n\n\u22121/2\u03b4)\n\n.=\n\nusing the normal approximation to the binomial distribution. for n large, \u0001(n\n+ n\n1\n2\n\n\u22121/2\u03b4\u03c6(0) = 1\n\n+ (2\u03c0n)\n\n2\n\n\u22121/2\u03b4, and after simplifying,\nz\u03b1 + \u03b4(2/\u03c0)1/2\n\n.= \u0001\n\n(cid:22)\n\npr1(s \u2265 s\u03b1)\n\n(cid:23)\n\n.\n\n(7.29)\n\nas (2/\u03c0)1/2 < 1, the sign test has lower power than does the test using y in exam-\nple 7.29. that test has power \u0001(z\u03b1 + \u03b4), so it requires smaller samples to attain a\ngiven power than does the test based on s. figure 7.6 compares the power functions\nwith \u03b1 = 0.05. sign tests have rather low power, and better tests are almost always\n(cid:1)\npossible.\n\nalthough power is important in planning an experiment, in giving a basis for\nchoosing the sample size required, and in assessing the size of effects that could\nreasonably be detected from a given set of data, it plays no role in conducting the test\nitself, which simply requires a tail probability computed under the null distribution.\n\nneyman\u2013pearson lemma\nother things being equal, a test with high power is preferable to one with low power.\nbut in order for a comparison of two tests to be fair, they must compete on an equal\nfooting. this leads us to compare them in terms of their power for fixed size. that is,\nout of all possible tests with a given size, we aim to find the one with highest power.\nlet f0(y) and f1(y) denote the probability densities of y under the null and alter-\nnative hypotheses. then the neyman\u2013pearson lemma states that the most powerful\ntest of size \u03b1 has critical region\ny =\n\n(cid:6)\n\n(cid:7)\n\n,\n\nt\u03b1 \u2265 0,\n\n\u2265 t\u03b1\n\ny :\n\nf1(y)\nf0(y)\n\ndetermined by the likelihood ratio, if such a region exists. to explain this, suppose\nthat such a region does exist and let y(cid:1)\nbe any other critical region of size \u03b1 or less.\nthen for any density f ,\n\n(cid:5)\n\nf (y) dy \u2212\n\ny\n\nf (y) dy,\n\n(cid:5)\n\ny(cid:1)\n(cid:5)\n\negon sharpe pearson\n(1895\u20131980), the second\nchild of karl pearson, was\nvery unlike his combative\nfather. after school in\noxford and winchester\nhis studies in cambridge\nwere interrupted by illness\nand the 1914\u201318 war. he\ntook his degree in 1920\nand began work at\nuniversity college\nlondon, where he stayed\nthe rest of his life. apart\nfrom broad contributions\nto statistical theory, he\npioneered industrial\nquality control and was\neditor of the statistical\njournal biometrika from\n1936\u20131966.\n\ny is the complement of y\nin the sample space.\n\nequals\n\n(cid:5)\ny\u2229y(cid:1)\n\nf (y) dy +\n\nand this is\n\n(cid:5)\n\ny\u2229y(cid:1)\n(cid:5)\ny\u2229y(cid:1)\n\nf (y) dy \u2212\n\nf (y) dy \u2212\n\ny(cid:1)\u2229y\n(cid:5)\n\ny(cid:1)\u2229y\n\n(cid:5)\n\nf (y) dy \u2212\n\nf (y) dy,\n\ny(cid:1)\u2229y\n\nf (y) dy.\n\n(7.30)\n\n "}, {"Page_number": 348, "text": "336\n\n7 \u00b7 estimation and hypothesis testing\nif f = f0, this expression is non-negative, because y(cid:1)\nhas size at most that of y.\nsuppose that f = f1. if y \u2208 y, then t\u03b1 f0(y) > f1(y), while f1(y) \u2265 t\u03b1 f0(y) if y \u2208 y.\nhence when f = f1, (7.30) is no smaller than\n(cid:7)\n(cid:5)\n\n(cid:6)(cid:5)\n\nf0(y) dy \u2212\nthus the power of y is at least that of y(cid:1)\n\ny\u2229y(cid:1)\n\nt\u03b1\n\nf0(y) dy\n\ny(cid:1)\u2229y\n\n\u2265 0.\n\n, and the result is established.\n\nit may happen that h0 is simple and the alternative is composite, but that the\nlikelihood ratio critical region is most powerful for each component of the alternative\nhypothesis. then y is said to be uniformly most powerful.\nexample 7.31 (exponential family) consider testing the null hypothesis \u03b8 = \u03b80\nagainst the one-sided alternative \u03b8 = \u03b81 > \u03b80 based on a random sample y1, . . . ,y n\nfrom the one-parameter exponential family\n\nthe likelihood ratio is\n\nf (y; \u03b8) = exp{s(y)\u03b8 \u2212 \u03ba(\u03b8) + c(y)} .\n(cid:14)\n(cid:15)\n\n(\u03b81 \u2212 \u03b80)\n\nexp\n\ns(y j ) + \u03ba(\u03b80) \u2212 \u03ba(\u03b81)\n\n,\n\nn(cid:13)\nj=1\n\n(cid:31)\nso for each \u03b81 > \u03b80 the most powerful size \u03b1 critical region is\n\n(cid:30)\n\n(cid:13)\n\ny\u03b1 =\n\n(y1, . . . , yn) :\n\ns(y j ) \u2265 t\n\n(cid:1)\n\u03b1\n\n,\n\n\u03b1 can be found such that pr0(y \u2208 y\u03b1) = \u03b1. this test is therefore uniformly most\n(cid:1)\nif a t\npowerful against this one-sided alternative. when \u03b81 < \u03b80, the same argument shows\nthat a uniformly most powerful critical region is obtained by replacing \u2265 by \u2264 in the\nabove definition of y\u03b1.\n\na special case of this is the exponential density of example 7.22, where the uni-\nformly most powerful critical region of size \u03b1 against one-sided alternatives \u03bb1 < \u03bb0\n\u03b1 the (1 \u2212 \u03b1) quantile of the gamma dis-\nis y\u03b1 = {(y1, . . . , yn) :\n(cid:1)\ntribution with unit scale and shape parameter n.\n\n\u03b1}, with \u03bb0t\n(cid:1)\n\ny j > t\n\n(cid:2)\n\n(cid:20)\n\n(cid:18)\n\nin discrete models uniformly most powerful tests of every size do not exist. in the\npoisson case, for example, the null distribution of\ny j is poisson with\nmean n\u03b80, soy \u03b1 has possible sizes\n\u221e(cid:13)\n=\nu=t(cid:1)\n\nt\u03b1 = 0, 1, . . . .\n\nexp(\u2212n\u03b80),\n\nn(cid:13)\nj=1\n\ny j \u2265 t\n\nsetting n\u03b80 = 5, for example, gives sizes 1.00, 0.993, . . . ,0.068, 0.032, . . . , so a\nlikelihood ratio critical region of size 0.05 does not exist. this does not affect the\n(cid:1)\ncomputation of a significance level, whose value is not pre-specified.\n\n(n\u03b80)u\n\npr0\n\nu!\n\n(cid:1)\n\u03b1\n\n\u03b1\n\n(cid:2)\n\ns(y j ) = (cid:2)\n\nthis last example shows that construction of a likelihood ratio critical region of\nexact size \u03b1 may be impossible. if so, a randomized test may be used to obtain the\nexact size required. suppose that critical regions of size \u03b11 and \u03b12 are available,\n\n "}, {"Page_number": 349, "text": "337\n\n7.3 \u00b7 hypothesis tests\nwhere \u03b11 < \u03b1 < \u03b12. then if i is a bernoulli variable with success probability p =\n(\u03b12 \u2212 \u03b1)/(\u03b12 \u2212 \u03b11), the test with region\n(cid:6)y\u03b11\ny\u03b12\n\nhas size \u03b1. inthe previous example we might take \u03b1 = 0.05, \u03b11 = 0.032 and \u03b12 =\n0.068, giving p = 0.5. then each time the test was conducted, we would flip a coin\nto decide whether to use y\u03b11 or y\u03b12 as the critical region. although this trick is useful\nin theoretical calculations, it introduces a random element unrelated to the data. in\napplications it is preferable to compute a significance level and weigh the evidence\naccordingly.\n\ni = 1,\ni = 0\n\ny =\n\n,\n,\n\nin example 7.29 the likelihood ratio for testing\n\nexample 7.32 (normal mean)\n\u00b5 = \u00b50 against \u00b5 = \u00b51 with \u03c3 known is\n\u2212n/2 exp\n(2\u03c0 \u03c3 2)\u2212n/2 exp\n(cid:22)\n\n= (2\u03c0 \u03c3 2)\n!\n\nf1(y )\nf0(y )\n\n(cid:2)\n(cid:2)\n\n(cid:22) \u2212 1\n(cid:22) \u2212 1\n\n2\u03c3 2\n\n2\u03c3 2\n\n(cid:23)\n(cid:23)\n\nn\n\nn\n\nj=1(y j \u2212 \u00b51)2\nj=1(y j \u2212 \u00b50)2\n(cid:23)\"\n\n= exp\n\n1\n2\u03c3 2\n\n2ny (\u00b51 \u2212 \u00b50) \u2212 \u00b52\n\n1\n\n+ \u00b52\n\n0\n\n.\n\nif \u00b51 > \u00b50, this is monotone increasing in y for any fixed \u00b51 and \u00b50, and so the\ncritical region rejects h0 when y \u2265 t\n(cid:1)\n\u03b1 chosen to give a test of size \u03b1. hence\nthe size \u03b1 critical region is\n\n(cid:1)\n\u03b1, with t\n\ny+\n\n\u03b1 = (cid:22)\n\u03b1 = (cid:22)\n\ny\u2212\n\n(y1, . . . , yn) :n 1/2(y \u2212 \u00b50)/\u03c3 \u2265 z1\u2212\u03b1\n(cid:23)\n\n(y1, . . . , yn) :n 1/2(y \u2212 \u00b50)/\u03c3 \u2264 z\u03b1\n\nthis is most powerful for any \u00b51 > \u00b50 and so is uniformly most powerful. the region\n\nis likewise uniformly most powerful against alternatives \u00b51 < \u00b50.\nsuppose that we wish to test the same null hypothesis against the two-sided alter-\nnative that \u00b5 (cid:5)= \u00b50. the null distribution of y is symmetric about \u00b50, so it isnatural\nto use\n\n(y1, . . . , yn) :n 1/2|y \u2212 \u00b50|/\u03c3 \u2265 z\u03b1/2\n\n.\n\n(7.31)\n\ny\u03b1 = (cid:22)\n\nthis critical region has size \u03b1 but isnot uniformly most powerful against the two-sided\nalternative. when \u00b51 > \u00b50,y+\n\u03b1 has size \u03b1 and has higher power, while when \u00b51 < \u00b50,\ny\u2212\n\u03b1 has size \u03b1 and has higher power. the power of a uniformly most powerful two-\nsided critical region would equal those of y+\n\u03b1 for\n\u00b51 < \u00b50, but its size would have to be \u03b1, whereas y\u2212\n\u03b1 has size 2\u03b1. in fact no\nuniformly most powerful test exists for this two-sided alternative. this difficulty can\n(cid:1)\nalso arise in other contexts.\n\n\u03b1 for alternatives \u00b51 > \u00b50 and of y\u2212\n\n\u03b1 \u222a y+\n\nthis last example highlights a problem with two-sided tests. one approach to\n\ndealing with it is to say that a critical region y is unbiased if\n\npr1(y \u2208 y) \u2265 pr0(y \u2208 y)\n\n(cid:23)\n\n;\n\n(cid:23)\n\n "}, {"Page_number": 350, "text": "338\n\n7 \u00b7 estimation and hypothesis testing\n\nfor all alternative hypotheses under consideration. this implies that the probability\nof rejecting h0 is higher under any h1 than under h0, and would rule out using the\ncritical regionsy+\n\u03b1 andy\u2212\n\u03b1 for two-sided tests in the previous example. if \u00b51 < \u00b50, for\nexample, then pr1(y \u2208 y+\n\u03b1 ) = \u0001(z\u03b1 + \u03b4) < \u03b1 because \u03b4 < 0, and hence y+\n\u03b1 would\nbe biased. there is a well-developed mathematical theory of such tests, but they are of\nlittle practical interest. to see why, suppose that the two-sided unbiased region y\u03b1 had\nbeen used in the previous example, and that doubt had been cast on the null hypothesis\n\u00b5 = \u00b50. the test being two-sided, it would then be natural to ask whether the data\nsuggest that \u00b5 > \u00b50 or \u00b5 < \u00b50, leading to use of one-sided regions such as y\u2212\n\u03b1 and\ny+\n\u03b1 . itseems more sensible to perform two one-sided tests and obtain an overall\np-value by combining the individual significance levels, as outlined in section 7.3.1.\nthis amounts to using two one-sided tests each of size \u03b1, and in general this is not\nthe same as an unbiased test of size 2\u03b1.\n\nlocal power\nwe now consider how the likelihood ratio behaves under a local alternative, when the\nnull and alternative models f0(y) = f (y; \u03b80) and f1(y) = f (y; \u03b81) depend on a scalar\nparameter \u03b8, and \u03b81 = \u03b80 + \u0001 for some small \u0001. then\n\n= f (y ; \u03b80 + \u0001)\n\nf (y ; \u03b80)\n\nf1(y )\nf0(y )\n\n(cid:6)\n\n1\n\n=\n.= 1 + \u0001u (\u03b80),\n\nf (y ; \u03b80)\n\n(cid:7)\n\nf (y ; \u03b80) + \u0001\n\nd f (y ; \u03b80)\n\nd\u03b80\n\n+ \u00b7\u00b7\u00b7\n\nwhere u (\u03b8) = d log f (y ; \u03b8)/d\u03b8 is the score statistic. as \u0001 \u2192 0, this expansion shows\nthat the likelihood ratio and score statistics are equivalent, so the neyman\u2013pearson\nlemma implies that a locally most powerful test against h0 may be based on large\nvalues of the score statistic. this is a score test.\n\nin large samples from regular models the null distribution of u (\u03b80) isapproximately\nnormal with mean zero and variance equal to the fisher information i (\u03b80), so a locally\nmost powerful critical region has form\n\n(cid:22)\n\n(y1, . . . , yn) :u (\u03b80) \u2265 i (\u03b80)1/2z1\u2212\u03b1\n\n(cid:23)\n\n.\n\nunder the alternative hypothesis, u (\u03b80) has mean\n\n(cid:5)\n\n(cid:5)\n\nu(\u03b80) f (y; \u03b80 + \u0001) dy =\n.= \u0001\n\nu(\u03b80){ f (y; \u03b80) + \u0001u(\u03b80) f (y; \u03b80) + \u00b7\u00b7\u00b7} dy\n(cid:5)\nu(\u03b80)2 f (y; \u03b80) dy = \u0001 i (\u03b80),\n(cid:23) .= \u0001 (z\u03b1 + \u03b4) ,\n\n(cid:22)\nu (\u03b80) \u2265 i (\u03b80)1/2z1\u2212\u03b1\n\npr1\n\nwhile its variance is i (\u03b80) + o(n\u0001). hence the local power of the score test is\n\nanalogous to (7.28), with \u03b4 = i (\u03b80)1/2(\u03b81 \u2212 \u03b80) = n1/2(\u03b81 \u2212 \u03b80)/i(\u03b80)\n\u22121/2 playing the\nrole of n1/2(\u00b51 \u2212 \u00b50)/\u03c3 in example 7.29. thus the power of the test is increased when\nthe null fisher information per observation i(\u03b80) islarge, when n is large, or when \u03b81\nis distant from \u03b80.\n\n "}, {"Page_number": 351, "text": "7.3 \u00b7 hypothesis tests\n\n339\n\nexample 7.33 (gamma density) suppose that y1, . . . ,y n is a random sample from\nthe gamma density\n\nf (y; \u00b5, \u03bd) = \u03bd\u03bd y\u03bd\u22121\n\n\u0001(\u03bd)\u00b5\u03bd exp(\u2212\u03bdy/\u00b5),\n\ny > 0, \u03bd, \u00b5 > 0.\n\nwe consider testing if \u03bd = 1, that is, that the density is in fact exponential. initially we\nsuppose that \u00b5 is known. the log likelihood contribution from a single observation\nis \u03bd log \u03bd + (\u03bd \u2212 1) log y \u2212 \u03bd log \u00b5 \u2212 \u03bdy/\u00b5 \u2212 log \u0001(\u03bd), so\n\nu (\u03bd) = n(cid:13)\n(cid:6)\nj=1\ni (\u03bd) = n\n\n(cid:6)\n\nlog\n\n(cid:3)\n\n(cid:4)\n\ny j\n\u00b5\n\nd2 log \u0001(\u03bd)\n\nd\u03bd2\n\n\u2212 y j\n(cid:7)\n\u00b5\n\n\u2212 1\n\u03bd\n\n+ 1 \u2212 log \u03bd \u2212 d log \u0001(\u03bd)\n\nd\u03bd\n\n.\n\n(cid:7)\n\n,\n\nan asymptotic test of \u03bd = 1 therefore consists in comparing u (1)/i (1)1/2 with the\nstandard normal distribution.\nnull hypothesis, (cid:1)\u00b5 = y . then the large-sample distribution of the score is given by\nin practice an unknown \u00b5 is replaced by its maximum likelihood estimator under the\n(4.48) with \u03c8 = \u03bd and \u03bb = \u00b5. inthis case the off-diagonal element of the fisher\ninformation matrix is i\u03bb\u03c8 = e(\u2212\u2202 2(cid:19)/\u2202\u00b5\u2202\u03bd) = 0, so the test involves replacing\n(cid:1)\n\u00b5 by y .\n\n7.3.3 composite null hypotheses\nthus far we have supposed that the null hypothesis is simple, that is, it fully specifies\nthe null distribution of the test statistic. an exact significance level, perhaps esti-\nmated by simulation, is then in principle available. in practice exact tests are usually\nunobtainable because the null distribution of y depends on unknowns. in the most\ncommon setting there is a nuisance parameter \u03bb and a parameter of interest \u03c8, and\nthe null hypothesis imposes the constraint \u03c8 = \u03c80 but puts no restriction on \u03bb. most\nof the tests in preceding chapters were of this sort. the p-value may then be written\n\npr0(t \u2265 tobs) = pr(t \u2265 tobs; \u03c80, \u03bb) =\n\n(7.32)\nin general this depends on \u03bb, perhaps strongly, but sometimes a critical region y\u03b1 of\nsize \u03b1 can be found such that\n\nf (y; \u03c80, \u03bb) dy.\n\npr(y \u2208 y\u03b1; \u03c80, \u03bb) = \u03b1\n\nfor all \u03bb.\n\nsuch a y\u03b1 is called a similar region; it issimilar to the sample space, which satisfies\nthis equation with \u03b1 = 1. a test whose critical regions are similar is called a similar\ntest and is clearly desirable if it can be found. the two main approaches to finding\nexact tests are use of conditioning and appeal to invariance. before discussing these,\none simple idea is to replace \u03bb by(cid:1)\u03bb0, the maximum likelihood estimator of \u03bb when\nwe outline approximate ways to reduce the dependence of (7.32) on \u03bb.\n\u03c8 = \u03c80, but this is generally unsatisfactory because the result still depends on \u03bb, albeit\n\n(cid:5)\n{y:t(y)\u2265tobs}\n\n "}, {"Page_number": 352, "text": "340\n\n7 \u00b7 estimation and hypothesis testing\n\nto a lower order. it is better to base the test on a pivot, exact or approximate. we have\nalready extensively used an important example of this, the likelihood ratio statistic\n\nwp(\u03c80) = 2{(cid:19)((cid:1)\u03c8 ,(cid:1)\u03bb) \u2212 (cid:19)(\u03c80,(cid:1)\u03bb0)}. under regularity conditions its distribution for a\n\nlarge sample size n is \u03c7 2\n\npr{wp(\u03c80) \u2264 c p(\u03b1); \u03c80, \u03bb} =\u03b1 {1 + o(n\n\np, where p is the dimension of \u03c8, and in fact as\nfor all \u03bb,\n\n\u22121)}\n\nc p(\u03b1) is the\u03b1 quantile of\nthe \u03c7 2\n\np distribution.\n\n(7.33)\n\ntests based on wp(\u03c80) are approximately similar. in continuous models the error in\n(7.33) can be reduced by noting that e0{wp(\u03c80)} .= p{1 + b(\u03b80)/n}, where b(\u03b80) =\nb(\u03c80, \u03bb) conveys how much the null mean of wp(\u03c80) differs from its asymptotic\nvalue. tedious calculations establish that\n\npr{wp(\u03c80){1 + b((cid:1)\u03b80)}\u22121 \u2264 c p(\u03b1); \u03c80, \u03bb} =\u03b1 {1 + o(n\n\nwhere(cid:1)\u03b80 = (\u03c80,(cid:1)\u03bb0). thus division of the likelihood ratio statistic to make its mean\n\nfor all \u03bb,\n\n\u22122)}\n\nconditioning\nwhen there is a minimal sufficient statistic s0 for the unknown \u03bb in a null distribution,\nit may be removed by conditioning, giving p-value\n\ncloser to p improves the quality of the \u03c7 2 approximation to its entire distribution.\nbartlett adjustment of this sort can decrease substantially the error in (7.33), and may maurice stevenson\nbartlett (1910\u20132002)\nbe valuable if n is small or if the dimension of \u03bb is appreciable.\nworked at research\ninstitutes and the\nuniversities of london,\nmanchester, and oxford.\nstarting in the mid 1930s,\nhe made pioneering\ncontributions to likelihood\ninference, to multivariate\nanalysis and to stochastic\nprocesses, on which he\nwrote a highly influential\nbook.\n\nwhich is independent of \u03bb by sufficiency of s0. if s0 is boundedly complete, this is\nthe only way to construct a test statistic with p-values independent of \u03bb. tosee why,\nlet y\u03b1 be a critical region of size \u03b1 for all \u03bb. then\n\n(cid:5)\n{y:t(y)\u2265tobs}\n\npr0(t \u2265 tobs | s0; \u03c80) =\n\nf (y | s0; \u03c8o) dy,\n\n0 = pr0(y \u2208 y\u03b1; \u03c80, \u03bb) \u2212 \u03b1 = e{i (y \u2208 y\u03b1) \u2212 \u03b1; \u03c80, \u03bb}\n\n= es0 [e{i (y \u2208 y\u03b1) | s0; \u03c80} \u2212 \u03b1; \u03c80, \u03bb] ,\n\nfor all \u03bb, and the bounded completeness of s0 implies that\n\ne{i (y \u2208 y\u03b1) | s0; \u03c80} = pr (y \u2208 y\u03b1 | s0; \u03c80) = \u03b1.\n\nhence similar critical regions must be based on this conditional density.\n\nexample 7.34 (exponential family)\nthe statistic s2 associated with \u03bb in the full exponential family model\n\nin section 5.2.3 we saw that conditioning on\n\nf (s1, s2; \u03c8, \u03bb) = exp\n\n(cid:22)\n\ns t\n1\n\n(cid:23)\n\ng0(s1, s2),\n\ngives a density independent of \u03bb, namely\nf (s1 | s2; \u03c8) = exp\n\ngs2(s1).\n\n(7.34)\n\n2\n\n\u03c8 + s t\n(cid:22)\n\n\u03bb \u2212 \u03ba(\u03c8, \u03bb)\n(cid:23)\n\n\u03c8 \u2212 \u03bas2(\u03c8)\n\ns t\n1\n\nif a particular value \u03c80 of \u03c8 is fixed, then s2 is complete and minimal sufficient for\n\u03bb. hence similar critical regions for testing \u03c8 = \u03c80 must be based on (7.34).\nconsider two independent poisson variables with means \u00b51 and \u00b52, and sup-\nthe hypothesis \u00b51 = \u00b52. wemay equivalently set\n\npose that we wish to test\n\n "}, {"Page_number": 353, "text": "7.3 \u00b7 hypothesis tests\n\u00b51 = exp(\u03bb + \u03c8) and \u00b52 = exp(\u03bb) with \u2212\u221e < \u03c8, \u03bb <\u221e and test the hypothesis\n\u03c8 = 0 with no restriction on \u03bb. the corresponding exponential family model is\n\n341\n\n\u00b5y1\n1\ny1!\n\ne\n\n\u2212\u00b51 \u00d7 \u00b5y2\n2\ny2!\n\n\u2212\u00b52 = 1\n\ne\n\nexp{y1\u03c8 + (y1 + y2)\u03bb \u2212 e\u03bb+\u03c8 \u2212 e\u03bb},\n\nwhere y1, y2 \u2208 {0, 1, . . .}. here s2 = y1 + y2 has a poisson distribution with mean\n\u00b51 + \u00b52 = e\u03bb(1 + e\u03c8 ), so the conditional density of s1 = y1 is binomial,\n\ny1!y2!\n\n(cid:3)\n\ns2!\n\ns1!(s2 \u2212 s1)!\n\nf (s1 | s2; \u03c8) =\n\ns1 = 0, 1, . . . ,s 2.\nthis has denominator s2 = y1 + y2 and so treats the total for the two variables as\nfixed. when \u03c8 = 0 the probability equals 1/2, so the only similar critical regions for\na test of \u03c8 = 0 against \u03c8 > 0, that is, \u00b51 > \u00b52, have form\n\ne\u03c8\n1 + e\u03c8\n\n1 + e\u03c8\n\n1\n\n,\n\n(cid:4)s1\n\n(cid:3)\n\n(cid:4)s2\u2212s1\n\npr0(y1 \u2265 r\n\n(cid:1) | y1 + y2 = s2) = s2(cid:13)\n\n(cid:3)\n\n(cid:4)\n\ns2\nr\n\nr=r(cid:1)\n\n\u2212r ,\n\n2\n\n(cid:1) = 0, 1, . . . ,s 2.\n\nr\n\nthus y1, y2 show evidence for \u03c8 > 0 if y1 is too close to y1 + y2.\nsee also example 4.40.\n\nexample 7.35 (permutation test) let y1, . . . ,y m and ym+1, . . . ,y n be indepen-\ndent random samples with densities g(y) and g(y \u2212 \u03b8), where g is unknown. one\npossibility here is to base a test of \u03b8 = 0 onthe two-sample t statistic\n\nt =\n\n(cid:24)(cid:8)\n\n(cid:9)(cid:22)\n\n+ 1\nn\u2212m\n\n1\nm\n\ny 2 \u2212 y 1\n(m \u2212 1)s2\n\n1\n\n+ (n \u2212 m)s2\n\n2\n\n(cid:23)(cid:25)1/2\n\n,\n\nwhere y 2 and s2\ncorresponding quantities for y1, . . . ,y m.\n\n2 are the average and variance of ym+1, . . . ,y n and y 1 and s2\n\n1 are the\n\nunder the null hypothesis y1, . . . ,y n form a random sample with unknown density\ng, and the set of order statistics y(1), . . . ,y (n) is a minimal sufficient statistic. the\nconditional null distribution of y1, . . . ,y n given the observed values y(1), . . . , y(n) of\nthe order statistics puts equal mass on each of the n! permutations of y1, . . . , yn, so\nthe conditional p-value is\n\n(cid:13)\n\npr0(t \u2265 tobs | y(1), . . . ,y (n)) = 1\nn!\n\nh{t(yperm) \u2265 tobs}\n\nwhere the sum is over all permutations yperm of y1, . . . , yn.\n\n(cid:1)\n\n(cid:1)\n\ninvariance\nsection 5.3 describes models in which data y were transformed by the action of a\ngroupg on the sample space, thereby inducing a similar group action on the parameter\nspace. in many cases it is appropriate that tests be invariant to the subgroup g0 of\nsuch transformations that preserves the null hypothesis. when testing the hypothesis\n\u00b5 = 0 for a sample y from the n (\u00b5, \u03c3 2) distribution, for example, we might seek a test\nthat is unaffected by replacing y by \u03c4 y. the corresponding parameter transformation\nmaps \u03c3 2 to \u03c4 2\u03c3 2, thereby preserving the null hypothesis. to see some consequences of\n\n "}, {"Page_number": 354, "text": "342\n\n7 \u00b7 estimation and hypothesis testing\n\nrequiring such invariances, suppose that the null hypothesis splits the parameter space\n\u0001 into disjoint parts \u00010 and \u00011 corresponding to the null and alternative hypotheses.\nthe problem is then said to be invariant under g0 if\n\npr{g(y ) \u2208 a; \u03b8} = pr{y \u2208 a; g\n\n\u2217\n\n(\u03b8)}\n\n\u2217\n\n\u2217\n\nsatisfies g\n\n(\u0001) = \u0001, g\n\nfor all subsets a of the sample space and all g \u2208 g0 and corresponding g\n\u2217 \u2208 g\u2217\n0 ,\n(\u00011) = \u00011. thus the action of\nwhere g\ng\u2217\n0 on \u0001 leaves \u00010 and \u00011 unchanged: whatever transformation is applied to y , the\nnull hypothesis remains equally true or false. hence the evidence for or against the\nhypotheses is unaffected by observing g(y ) rather than y , for any g \u2208 g0. atest with\ncritical region y\u03b1 is then said to be invariant if\n\n(\u00010) = \u00010 and g\n\n\u2217\n\n\u2217\n\ny \u2208 y\u03b1 if and only if g(y ) \u2208 y\u03b1 for all g \u2208 g0,\n\n(7.35)\n\nimplying that its properties are unaffected by transformation. the hope is that appeal\nto invariance will simplify the problem by eliminating nuisance parameters. we can\nthen search among invariant tests for one with high power or other good properties.\nas every invariant statistic is a function of a maximal invariant, we start by seeking\na maximal invariant under g0.\nexample 7.36 (student t test) suppose that we wish to test \u00b5 = \u00b50 against the\nalternative \u00b5 (cid:5)= \u00b50, based on a normal random sample y1, . . . ,y n, with no restriction\non the variance \u03c3 2. wetake \u03b8 = (\u00b5, \u03c3 ), so \u00010 is {\u00b50} \u00d7ir + and\n\n\u00011 = {(\u2212\u221e, \u00b50) \u222a (\u00b50,\u221e)} \u00d7ir +.\nlet v = (n \u2212 1)\n(y j \u2212 y )2. the statistic (y , v 1/2) isminimal sufficient in the\n\u22121\nfull model and can form the basis of our discussion. as (y , v 1/2) takes values in the\nparameter space \u0001, example 5.21 implies that an element g(\u03b7,\u03c4 ) of the group g\u2217\nacting\non \u0001 transforms (y , v 1/2) to (\u03b7 + \u03c4 y , \u03c4 v 1/2). this reduction to a minimal sufficient\nstatistic taking values in \u0001 means that our discussion below may be expressed in\nterms of g\u2217\n\nrather than the group g acting on the original data y .\n\n(cid:2)\n\nthe subset of g\u2217\n\nthat preserves \u00010 must have\ng(\u03b7,\u03c4 )(\u00b50, \u03c3 ) = (\u03b7 + \u03c4 \u00b50, \u03c4 \u03c3 ) = (\u00b50, a)\n\nfor some a > 0, and this implies that \u03b7 = \u00b50 \u2212 \u03c4 \u00b50 but imposes no restriction on \u03c4 .\nhence the largest such subset is\ng\u2217\n\n(cid:23)\n\n= (cid:22)\n0 is a subgroup of g\u2217\n\n0\n\nto verify that g\u2217\n\ng(\u00b50\u2212\u03c4 \u00b50,\u03c4 ) : \u03c4 > 0\n, note that it is closed, because\n\n.\n\ng(\u00b50\u2212\u03c4 \u00b50,\u03c4 ) \u25e6 g(\u00b50\u2212\u03c3 \u00b50,\u03c3 ) = g(\u00b50\u2212\u03c4 \u00b5+\u03c4 (\u00b50\u2212\u03c3 \u00b50),\u03c4 \u03c3 ) = g(\u00b50\u2212\u03c4 \u03c3 \u00b50,\u03c4 \u03c3 )\n\nis also an element of g\u2217\ng(\u00b50\u2212\u03c4 \u00b50,\u03c4 ) has inverse g(\u00b50\u2212\u03c4\u22121\u00b50,\u03c4\u22121) also an element of g\u2217\n\u00011, because if \u00b5 (cid:5)= \u00b50, then\n\n0 , that setting \u03c4 = 1 gives the identity element g(0,1), and that\n0 preserves\n\n0 . moreover g\u2217\n\ng(\u00b50\u2212\u03c4 \u00b50,\u03c4 )(\u00b5, \u03c3 ) = (\u00b50 \u2212 \u03c4 \u00b50 + \u03c4 \u00b5, \u03c4 \u03c3 ) = (\u00b50 + \u03c4 (\u00b5 \u2212 \u00b50), \u03c4 \u03c3 ) \u2208 \u00011.\n\n "}, {"Page_number": 355, "text": "tn\u22121(\u03b1) is the\u03b1 quantile\nof the tn\u22121 distribution.\n\n343\n\n7.3 \u00b7 hypothesis tests\nnow g(\u00b50\u2212\u03c4 \u00b50,\u03c4 ) maps the student t pivot t (\u00b50) = n1/2(y \u2212 \u00b50)/v 1/2 to\n\nn1/2\n\n\u03c4 v 1/2\n\n= n1/2\n\n= t (\u00b50),\n\n\u00b50 \u2212 \u03c4 \u00b50 + \u03c4 y \u2212 \u00b50\n\n\u03c4 (y \u2212 \u00b50)\n\u03c4 v 1/2\nso t (\u00b50) is invariant under g0. to verify that it is a maximal invariant, we find an\n0 , such as s(y , v 1/2) = (\u00b50, v 1/2).\nestimator that lies in \u00010 and is equivariant under g\u2217\n(cid:9) = g\nthen a maximal invariant is (page 185)\n\u2217\n= (cid:8)\n(\u00b50\u2212\u00b50v \u22121/2,v \u22121/2)\n\u00b50 \u2212 \u00b50v\n= (cid:8)\n\u00b50 + (y \u2212 \u00b50)v\n\n(cid:8)\n\u22121/2 + v\n\n(cid:9)\n\u22121/2y , v\n\n\u2217\u22121\ng\n(\u00b50\u2212\u00b50v 1/2,v 1/2)\n\ny , v 1/2\n(cid:9)\n\n\u22121/2v 1/2\n\n\u22121/2, 1\n\ny , v 1/2\n\n(cid:8)\n\n(cid:9)\n\n,\n\nthe second component of which can obviously be discarded. under the null hypothesis\n\u00b50 is known, so t (\u00b50) isalso maximal invariant, as we had anticipated. hence any\ncritical region based on t (\u00b50) would be unaltered if a sample y was replaced by\n\u00b50 \u2212 \u03c4 \u00b50 + \u03c4 y, for any \u03c4 > 0, because\n\u2208 a if and only if\n\n\u00b50 \u2212 \u03c4 \u00b50 + \u03c4 y \u2212 \u00b50\n\nn1/2 y \u2212 \u00b50\n\n\u2208 a\n\nn1/2\n\nfor any set a \u2282 ir, thus verifying (7.35). thus any critical region based on t (\u00b50) is\ninvariant. an example is\n\n\u03c4 v 1/2\n\nv 1/2\n\n(cid:6)\n\n(y1, . . . , yn) :n 1/2\n\n(cid:19)(cid:19)(cid:19)(cid:19) y \u2212 \u00b50\n\nv 1/2\n\n(cid:19)(cid:19)(cid:19)(cid:19) \u2265 tn\u22121(1 \u2212 \u03b1)\n\n(cid:7)\n\n,\n\nwhich has size 2\u03b1 and is uniformly most powerful unbiased against two-sided alter-\n(cid:1)\nnatives, in addition to being invariant.\n\n7.3.4 link with confidence intervals\nthere is a close link between tests and the construction of confidence intervals. if the\ndensity of y depends on a scalar parameter \u03b8, wedefine a level \u03b1 upper confidence\nlimit to be a function t \u03b1 = t \u03b1(y ) of y such that\n\npr(\u03b8 \u2264 t \u03b1; \u03b8) = 1 \u2212 \u03b1 for all \u03b8,\n\n(7.36)\nand that t \u03b11 \u2264 t \u03b12 whenever \u03b11 > \u03b12. this requirement is similar to the nesting of\ncritical regions for tests and is imposed for the same reasons of consistency; it implies\nthat t \u03b1 is non-increasing in \u03b1. lower confidence limits may be defined analogously.\nthe random quantity in (7.36) is t \u03b1. anequi-tailed (1 \u2212 2\u03b1) confidence interval\nfor \u03b8 is (t 1\u2212\u03b1, t \u03b1). if the reparametrization \u03c8 = \u03c8(\u03b8) ismonotonic increasing, then\n\u03c8(t \u03b1) is anupper confidence limit for \u03c8.\n\nin many cases confidence limits are derived from a pivot z(\u03b8), a function of the\ndata and \u03b8 with the same distribution for all \u03b8. ifthis distribution is continuous, we\ncan find a z\u03b1 such that\n\npr{z(\u03b8) \u2264 z\u03b1; \u03b8} = \u03b1 for all \u03b8.\n\n "}, {"Page_number": 356, "text": "344\n\n7 \u00b7 estimation and hypothesis testing\n\nif z(\u03b8) isdecreasing in \u03b8 for every possible value of y , then the solution in \u03b8 to\nthe equation z(\u03b8) = z\u03b1 can be taken as an upper (1 \u2212 \u03b1) confidence limit for \u03b8. we\napplied this argument to approximate normal pivots and the signed likelihood ratio\nstatistic in sections 3.1.1 and 4.5.2; see figures 3.1 and 4.7.\nnow suppose that y\u03b1(\u03b80) is acritical region of size \u03b1 constructed for tests of \u03b8 = \u03b80\nagainst lower alternatives \u03b8 < \u03b80. as\u03b8 0 increases, the critical region will vary and we\ncan define the set\n\n{\u03b8 : y (cid:5)\u2208 y\u03b1(\u03b8)}\n\nof values of \u03b8 not rejected by the test and hence compatible with the data at level \u03b1.\nunder natural monotonicity conditions the supremum of this set can be taken as an\nupper (1 \u2212 \u03b1) confidence limit t \u03b1. this inversion of a collection of critical regions to\nobtain a confidence interval allows us to use good tests to construct good confidence\nintervals. for example, the neyman\u2013pearson lemma tells us that uniformly most\npowerful tests of simple hypotheses are commonly based on likelihood ratio statistics,\nwhich will therefore also be the basis for shortest confidence intervals.\n\nin many cases we can express the above argument as follows. let g(t; \u03b80) denote\nthe null distribution function of a continuous test statistic t when the null hypothesis\nis \u03b8 = \u03b80. then the p-value\n\npobs(\u03b80) = pr0(t \u2265 tobs) = 1 \u2212 g(tobs; \u03b80)\n\nis a realization of p(\u03b80) = 1 \u2212 g(t ; \u03b80), and the probability integral transform\n(section 2.3) implies that the null distribution of p(\u03b80) isuniform on (0, 1). if the test\nrejects when p(\u03b80) < \u03b1, then the set{\u03b8 : \u03b1 \u2264 p(\u03b8)} is a one-sided (1 \u2212 \u03b1) confidence\nset. in the two-sided case we take {\u03b8 : \u03b1 \u2264 p(\u03b8) \u2264 1 \u2212 \u03b1}.\n\nthis argument applies when we can eliminate parameters other than \u03b8 by appeal\nto similarity or invariance; otherwise it can be sometimes be applied approximately,\nas with the likelihood ratio statistic. minor complications arise when t is discrete;\nsee example 7.38.\n\nexample 7.37 (exponential density) let y1, . . . ,y n be a random sample from the\nexponential density with parameter \u03bb, and let a test of \u03bb = \u03bb0 be conducted against\nt = (cid:2)\nthe two-sided alternative \u03bb (cid:5)= \u03bb0. we saw in example 7.22 that the null density of\ny j is gamma with shape parameter n and scale \u03bb0, sothe null hypothesis is\nrejected at level (1 \u2212 2\u03b1) if\n\npobs(\u03bb0) = pr0 (t \u2265 tobs) =\n\n(cid:5) \u221e\n\n\u03bb0tobs\n\nv n\u22121\n\u0001(n)\n\n\u2212v dv\n\ne\n\nlies outside the interval (\u03b1, 1 \u2212 \u03b1). for a given value of tobs, this probability depends\non \u03bb0, asshown in figure 7.7, and a (1 \u2212 2\u03b1) confidence interval can be determined\nas the set of values of \u03bb for which \u03b1 \u2264 pobs(\u03bb) \u2264 1 \u2212 \u03b1.\n(cid:1)\nthe interpretation of two-sided confidence intervals as providing random upper and\nlower bounds is direct and useful for scalar parameters. confidence regions for vector\n\u03b8 require a shape. it is natural to base this on likelihood, insisting that a confidence\n\n "}, {"Page_number": 357, "text": "figure 7.7 inversion of\na two-sided test with level\n0.9 to form confidence\ninterval. left: significance\nlevels pobs(\u03bb0) for\n\u03bb0 = 0.1, 0.2, 0.5, 1, 2\n(top to bottom).\nhorizontal lines show\nprobabilities 0.05, 0.95\nand the vertical line shows\ntobs = 4. hypotheses\n\u03bb0 = 2, 0.1 are rejected,\nhypotheses \u03bb0 = 1, 0.5\nare not rejected, and\n\u03bb0 = 0.2 isjust rejected.\nright: significance level\npobs(\u03bb) as afunction of \u03bb.\nvalues of \u03bb for which\n0.05 \u2264 pobs(\u03bb) \u2264 0.95 are\ncontained in the 0.9\nconfidence interval.\n\notherwise they are called\nliberal.\n\n7.3 \u00b7 hypothesis tests\n\n345\n\n)\na\nd\nb\nm\na\n\nl\n;\nt\n(\n\ng\n-\n1\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n0\n\n)\na\nd\nb\nm\na\n\nl\n;\nt\n(\n\ng\n-\n1\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n0\n\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\n\nt\n\nlambda\n\nregion r\u03b1 be such that pr(\u03b8 \u2208 r\u03b1; \u03b8) = \u03b1 for all \u03b8 and that l(\u03b8) \u2265 l(\u03b8(cid:1)\n) for any\n\u03b8 \u2208 r\u03b1 and \u03b8(cid:1) (cid:5)\u2208 r\u03b1. this amounts to computing r\u03b1 by inverting the likelihood ratio\nstatistic, typically using its asymptotic distribution, perhaps with bartlett adjustment.\noften the test inverted to obtain limits of confidence intervals is not exact. then\nthere is coverage error, defined as the difference between the actual and nominal\nprobabilities that the confidence set contains the parameter,\n\npr(t \u03b11 < \u03b8 \u2264 t \u03b12; \u03b8) \u2212 (\u03b11 \u2212 \u03b12),\n\nfor \u03b11 > \u03b12.\n\n(7.37)\n\nit can be helpful to know where the error occurs. the limit t \u03b1 is said to be conservative\nif it tends to be too high, that is, pr(\u03b8 \u2264 t \u03b1; \u03b8) \u2265 1 \u2212 \u03b1; confidence intervals for which\n(7.37) is positive are called conservative.\nexample 7.38 (binomial density) an equitailed (1 \u2212 2\u03b1) confidence interval for\nthe probability \u03c0 of a binomial variable y with denominator m may be found in\nvarious ways. exact limits may be found by inverting tests based on y . having\nobserved y = y, the significance level for testing the null hypothesis \u03c0 = \u03c00 against\nthe one-sided alternative \u03c0 < \u03c00 is\n\npr0(y \u2264 y) = pr(y \u2264 y; \u03c00) = y(cid:13)\n\n(cid:3)\n\n(cid:4)\n\nm\nr\n\nr=0\n\n0 (1 \u2212 \u03c00)m\u2212r ,\n\n\u03c0 r\n\nso the upper \u03b1 limit \u03c0 \u03b1 is the solution to\n(cid:3)\n\npr(y \u2264 y; \u03c0) = y(cid:13)\n\n(cid:4)\n\nm\nr\n\n\u03c0 r (1 \u2212 \u03c0)m\u2212r = \u03b1,\n\nr=0\n\nand equals 1 if y = m. asimilar argument with alternative \u03c0 > \u03c00 shows that the\nlower \u03b1 limit \u03c0\u03b1 is the solution to\n\npr(y \u2265 y; \u03c0) = m(cid:13)\n\nr=y\n\n(cid:3)\n\n(cid:4)\n\nm\nr\n\n\u03c0 r (1 \u2212 \u03c0)m\u2212r = \u03b1,\n\n "}, {"Page_number": 358, "text": "figure 7.8 exact\ncoverages of equi-tailed\n0.95 confidence intervals\nfor the binomial\nparameter \u03c0, asfunctions\nof \u03c0, when m = 10. the\nhorizontal line shows the\ntarget coverage. left:\nexact (solid), score (dots)\nand maximum likelihood\nestimator (dashes). right:\nsigned likelihood ratio\nstatistic (solid), modified\nsigned likelihood ratio\nstatistic (dots) and\nmodified maximum\nlikelihood estimator\n(dashes), obtained by\nreplacing m and r by\nm + 2 and r + 1 (dashes).\n\nf\u03bd1 ,\u03bd2 (y) is the\ndistribution function of an\nf variable with \u03bd1, \u03bd2\ndegrees of freedom.\n\n346\n\ne\ne\ng\ng\na\na\nr\nr\ne\ne\nv\nv\no\no\nc\nc\n \n \nt\nt\nc\nc\na\na\nx\nx\ne\ne\n\n0\n0\n0\n0\n\n.\n.\n\n1\n1\n\n0\n0\n9\n9\n\n.\n.\n\n0\n0\n\n0\n0\n8\n8\n\n.\n.\n\n0\n0\n\n0\n0\n7\n7\n\n.\n.\n\n0\n0\n\n7 \u00b7 estimation and hypothesis testing\n\ne\ne\ng\ng\na\na\nr\nr\ne\ne\nv\nv\no\no\nc\nc\n \n \nt\nt\nc\nc\na\na\nx\nx\ne\ne\n\n0\n0\n0\n0\n\n.\n.\n\n1\n1\n\n0\n0\n9\n9\n\n.\n.\n\n0\n0\n\n0\n0\n8\n8\n\n.\n.\n\n0\n0\n\n0\n0\n7\n7\n\n.\n.\n\n0\n0\n\n0.0\n0.0\n\n0.2\n0.2\n\n0.4\n0.4\n\n0.6\n0.6\n\n0.8\n0.8\n\n1.0\n1.0\n\n0.0\n0.0\n\n0.2\n0.2\n\n0.4\n0.4\n\n0.6\n0.6\n\n0.8\n0.8\n\n1.0\n1.0\n\npi\npi\n\npi\npi\n\nbut equals 0 if y = 0. it turns out that \u03c0 \u03b1 and \u03c0\u03b1 are expressible using quantiles of\nthe f distribution, giving (1 \u2212 2\u03b1) confidence interval\n\n1 + m \u2212 y + 1\n\u22121\n2y,2(m\u2212y+1)(\u03b1)\n\ny f\n\nm \u2212 y\n2(y+1),2(m\u2212y)(1 \u2212 \u03b1)\n\u22121\n\n(y + 1)f\n\n(cid:15)\u22121\n\n(cid:14)\n\n1 +\n\n,\n\n(cid:15)\u22121\n\n\uf8f6\n\uf8f8 ,\n\n(cid:14)\n\n\uf8eb\n\uf8ed\n\nwith the changes mentioned above when y = 0 or y = m. this interval is exact in the\nsense that no approximation of binomial probabilities is involved.\nof the score statistic, the maximum likelihood estimator (cid:1)\u03c0 = y /m or the signed\n\napproximate intervals can be based on asymptotic standard normal distributions\n\nlikelihood ratio statistic,\n\n\u2217\n\nm\ny=0( m\n\nz1(\u03c0) = (y \u2212 m\u03c0)/{m\u03c0(1 \u2212 \u03c0)}1/2 ,\nz2(\u03c0) = ((cid:1)\u03c0 \u2212 \u03c0)/{(cid:1)\u03c0(1 \u2212(cid:1)\u03c0)/m}1/2 ,\nz3(\u03c0) = sign((cid:1)\u03c0 \u2212 \u03c0)\n\n(cid:8)\n(cid:24)\ny log((cid:1)\u03c0 /\u03c0) + (m \u2212 y ) log{(1 \u2212(cid:1)\u03c0)/(1 \u2212 \u03c0)}(cid:25)(cid:9)1/2 ,\n2\n\u22121 log{z2(\u03c0)/z3(\u03c0)} motivated in\n(\u03c0) = z3(\u03c0) + z3(\u03c0)\n\u2217\nas well as on a quantity z\nsection 12.3.3. the confidence interval based on each of these is the set of \u03c0 for\n(cid:2)\nwhich |z(\u03c0)| < z1\u2212\u03b1; this must be found numerically for z3(\u03c0) and z\n(\u03c0). any of\nthese intervals has coverage\nindicates that \u03c0 lies in an interval of nominal level (1 \u2212 2\u03b1) based on y.\n\ny )\u03c0 y(1 \u2212 \u03c0)m\u2212y i1\u22122\u03b1(\u03c0, y), where i1\u22122\u03b1(y, \u03c0)\nfigure 7.8 compares the coverages for \u03b1 = 0.025 and m = 10. that of the exact\ninterval always exceeds 0.975, so it is quite conservative, while that of the interval\nbased on z1(\u03c0) is fairly close to its nominal level overall. intervals based on z2(\u03c0)\nundercover for most \u03c0. the intervals based on z3(\u03c0) and z\n(\u03c0) have coverage close\nto nominal for 0.3 < \u03c0 <0.7, while perhaps the best overall performance is obtained\nfrom z2(\u03c0) with m and y replaced by m + 2 and y + 1.\n(cid:1)\nthis example suggests that in highly discrete situations approximate confidence\nintervals may be preferable to exact ones. moreover exact tests will inherit the con-\nservatism and tend to reject too rarely. the difference decreases as the sample size\nincreases, but even with m = 50 the mean exact coverage is about 0.97 in the binomial\ncase.\n\n\u2217\n\n "}, {"Page_number": 359, "text": "7.3 \u00b7 hypothesis tests\n\n347\n\nbased on the data 1.2, 3, 1.5, 0.3.\n\nexercises 7.3\n1 show that (7.26) has mean and variance roughly pobs and pobs(1 \u2212 pobs)/r. hence give\nminimum values of r for obtaining 5% relative error in estimation of pobs = 0.5, 0.2, 0.1,\n0.05, 0.01, 0.001. discuss.\n2 in example 7.22, calculate the significance level for testing h0 : \u03bb = 1 against h1 : \u03bb = 4,\n3 if u \u223c u (0, 1), show that min(u, 1 \u2212 u ) \u223c u (0, 1\n2 ). hence justify the computation of a\ntwo-sided significance level as 2 min(p\n4 consider testing the hypothesis that \u00b5 = \u00b50 based on a random sample y1, . . . ,y n from\nthe n (\u00b5, \u03c3 2) distribution, with two-sided alternative \u00b5 (cid:5)= \u00b50. show that the power of the\nregion (7.31) is \u0001(z\u03b1/2 + \u03b4) + \u0001(z\u03b1/2 \u2212 \u03b4), where \u03b4 = n1/2(\u00b5 \u2212 \u00b50)/\u03c3 . sketch this as a\nfunction of \u03b4 for \u03b1 = 0.025, and explain why it is invariant to the sign of \u00b5 \u2212 \u00b50.\n\n\u2212, p\n\n).\n\n+\n\n5 check the power calculation for the sign test in example 7.30.\n6 consider testing the hypothesis that a binomial random variable has probability \u03c0 = 1/2\nagainst the alternative that \u03c0 > 1/2. for what values of \u03b1 does a uniformly most powerful\ntest exist when the denominator is m = 5?\n7 in arandom sample y1, . . . ,y n from the gamma density with shape \u03ba and scale \u03bb, find a\nlocally most powerful test of the null hypothesis \u03ba = 1.\n8 if i is bernoulli with probability p = (\u03b12 \u2212 \u03b1)/(\u03b12 \u2212 \u03b11) and y\u03b11 and y\u03b12 are critical\nregions of sizes \u03b11, \u03b12, show that the critical region y = iy\u03b11\n+ (1 \u2212 i )y\u03b12 has size \u03b1.\n9 y1, y2 are independent gamma variables with known shape parameters \u03bd1, \u03bd2 and scale\nparameters \u03bb1, \u03bb2,and it is desired to test the null hypothesis h0 that \u03bb1 = \u03bb2 = \u03bb, with\n\u03bb unknown. show that a minimal sufficient statistic for \u03bb under h0 is y1 + y2, find\nits distribution, and show that it is complete. hence show that the test is based on the\nconditional distribution of y1 given y1 + y2 and that significance levels are computed\nfrom integrals of form\n\n\u0001(\u03bd1 + \u03bd2)\n\u0001(\u03bd1)\u0001(\u03bd2)\n\n(cid:5)\n\n0\n\ny1/(y1+y2)\n\nu\u03bd1\u22121(1 \u2212 u)\u03bd2\u22121 du.\n\nexplain how this argument is useful in comparison of the scale parameters of two inde-\npendent exponential samples.\n10 independent data pairs (x1, z1), . . . ,( xn, zn) arise from a joint density f (x, z). the null\nhypothesis is that x and z are independent, so f (x, z) = g(x)h(z) for some unknown den-\nsities g and h and all x and z. show that the order statistics x(1), . . . , x(n) and z(1), . . . , z(n)\nare minimal sufficient for g and h under the null hypothesis, and deduce that a similar test\nhas p-value\n\n(cid:13)\n\npobs = 1\nn!\n\nh{t(yperm) \u2265 tobs},\n\n\u22121\n\n(cid:2)\n\nx j z j \u2212 x z)/(s2\n\nwhere the sum is over all yperm = {(x1, z\u03c0(1)), . . . ,( xn, z\u03c0(n))} with the observed values of\nthe zs permuted, the xs being held fixed.\nif the test statistic is t = (n\nz being the sample\nvariances of the x j and the z j , show that it is equivalent to base the test on\n11 in a scale family, y = \u03c4 \u03b5, where \u03b5 has a known density and \u03c4 > 0. consider testing the\nnull hypothesis \u03c4 = \u03c40 against the alternative \u03c4 (cid:5)= \u03c40. show that the appropriate group for\nconstructing an invariant test has just one element (apart from permutations) and hence\nshow that the test may be based on the maximal invariant y(1)/\u03c40, . . . ,y (n)/\u03c40.\nwhen \u03b5 is exponential, show that the invariant test is based on y /\u03c40.\n12 one natural transformation of a binomial variable r is reversal of \u2018success\u2019 and\n\u2018failure\u2019. show that this maps r to m \u2212 r, where m is the denominator, and that\n\nx and s2\n\nz )1/2, s2\n\nx j z j .\n\n(cid:2)\n\nx s2\n\n "}, {"Page_number": 360, "text": "348\n\n7 \u00b7 estimation and hypothesis testing\nthe induced transformation on the parameter space maps \u03c0 to 1 \u2212 \u03c0. which of the\ncritical regions (a) y1 = {0, 1, 20}, (b) y2 = {0, 1, 19, 20}, (c) y3 = {0, 1, 10, 19, 20},\n2 when m = 20? which is prefer-\n(d) y4 = {8, 9, 10, 11, 12}, is invariant for testing \u03c0 = 1\nable and why?\n\n13 the incidence of a rare disease seems to be increasing. in successive years the numbers of\nnew cases have been y1, . . . , yn. these may be assumed to be independent observations\nfrom poisson distributions with means \u03bb\u03b8, . . . , \u03bb\u03b8 n. show that there is a family of tests\neach of which, for any given value of \u03bb, is auniformly most powerful test of its size for\ntesting \u03b8 = 1 against \u03b8 > 1.\n\n14 a random sample y1, . . . ,y n is available from the type i pareto distribution\n\n(cid:6)\n\nf(y; \u03c8) =\n\n\u2212\u03c8 ,\n\n1 \u2212 y\n0,\n\ny \u2265 1,\ny < 1.\n\nfind the likelihood ratio statistic to test that \u03c8 = \u03c80 against \u03c8 = \u03c81, where \u03c80, \u03c81 are\nknown, and show how to calculate a p-value when \u03c80 > \u03c81.\nhow does your answer change if the distribution is\n1 \u2212 (y/\u03bb)\n0,\n\nf(y; \u03c8, \u03bb) =\n\ny \u2265 \u03bb,\ny < \u03bb,\n\n\u2212\u03c8 ,\n\n(cid:6)\n\nwith \u03bb >0 unspecified?\n\n7.4 bibliographic notes\n\nthe main concepts described in this chapter belong to the core of statistical theory and\nwere developed in the first half of the twentieth century by fisher, neyman, pearson\nand others; other treatments are contained in most books on mathematical statistics.\nsee for example the treatments of estimation in silvey (1970), rice (1988), casella\nand berger (1990) and bickel and doksum (1977), or at a more advanced level cox\nand hinkley (1974), lehmann (1983) and shao (1999).\n\nkernel density estimation has been extensively studied since it was proposed in the\n1950s. among numerous excellent expositions are silverman (1986), scott (1992),\nwand and jones (1995), and bowman and azzalini (1997). the last of these is more\npractical in emphasis, while wand and jones (1995) contains a detailed discussion of\nthe choice of bandwidth, a topic on which there has been much progress in the 1990s.\nalthough cross-validation is an important paradigm for selection of bandwidths and\nrelated smoothing parameters in other non- and semi-parametric contexts, other ap-\nproaches to bandwidth selection give better results; see sheather and jones (1991).\nstone (1974) is a fundamental reference on cross-validation.\n\nestimators based on estimating functions are widely used in practice, but there\nare few general expositions of them at this level. godambe (1991) is an interesting\ncollection of papers on the topic, with many further references, while mcleish and\nsmall (1994) give a more abstract treatment. a fundamental reference for the role\nof the influence function in robust statistics is hampel et al. (1986). inference for\nstochastic processes is discussed in books by hall and heyde (1980), basawa and\nscott (1981), and guttorp (1991), while s\u00f8rensen (1999) reviews the asymptotic\ntheory for estimating functions.\n\n "}, {"Page_number": 361, "text": "7.5 \u00b7 problems\n\n349\n\nalthough the idea of significance testing goes back hundreds of years, the develop-\nment of underlying theory is more recent. r. a. fisher made extensive informal use\nof p-values, but resisted what he saw as the over-formalization due to neyman and\ne. s. pearson. they introduced the idea of testing as a choice between two hypotheses\nand introduced the notions of size, power and so forth in work that prefigured the\nlater development of decision theory. their joint papers are collected in neyman and\npearson (1967). the theory of testing is explained more fully in lehmann (1983) and\nin chapters 3\u20136 of cox and hinkley (1974). bartlett correction was first described\nby bartlett (1937). example 7.38 is based on agresti and coull (1998), agresti and\ncaffo (2000), and greenland (2001).\n\n7.5 problems\n1 in example 7.2 show that (cid:1)\u03c8 d= exp{\u00b5 + \u03c3 n\n\nexpression for e((cid:1)\u03c8 r ) and compute the analogue of table 7.1. discuss your results.\nor not y j lies in the interval (a \u2212 1\na binomial distribution with denominator n and probability\n\n\u22121/2 z + \u03c3 2v /(2n)}. hence give an explicit\n2 h], and consider r = (cid:2)\n\n2 let y1, . . . ,y n be a random sample from an unknown density f . let i j indicate whether\ni j . show that r has\n\n2 h, a + 1\n(cid:5)\na+ 1\n2 h\na\u2212 1\n2 h\n\nf (y) dy.\n\nhence show that r/(nh) has approximate mean and variance f (a) + 1\nf (a)/nh, where f\nwhat implications have these results for using the histogram to estimate f (a)?\n\nis the second derivative of f .\n\n2 h2 f\n\n(cid:1)(cid:1)\n\n(cid:1)(cid:1)\n\n(a) and\n\n3 suppose that the random variables y1, . . . ,y n are such that\n\ne(y j ) = \u00b5,\n\nwhere \u00b5 is unknown and the \u03c3 2\ngiving an unbiased estimator of \u00b5 with minimum variance is\n\nj\n\n,\n\nj (cid:5)= k,\n\ncov(y j , yk) = 0,\n\nvar(y j ) = \u03c3 2\nj are known. show that the linear combination of the y j \u2019s\nn(cid:13)\nj=1\n\n\u2019 n(cid:13)\nj=1\n\n\u03c3 \u22122\nj y j\n\n\u03c3 \u22122\n\n.\n\nj\n\nsuppose now that y j is normally distributed with mean \u03b2x j and unit variance, and that the\ny j are independent, with \u03b2 an unknown parameter and the x j known constants. which of\nthe estimators\n\nt1 = n\n\n\u22121\n\nis preferable and why?\n\nn(cid:13)\nj=1\n\nt2 = n(cid:13)\n\nj=1\n\n\u2019 n(cid:13)\nj=1\n\nx 2\nj\n\ny j /x j ,\n\ny j x j\n\n(cid:2)\n\n4 in n independent food samples the bacterial counts y1, . . . ,y n are presumed to be poisson\nrandom variables with mean \u03b8. it isrequired to estimate the probability that a given sample\nwould be uncontaminated, \u03c0 = pr(y j = 0).\nshow that u = n\ni (y j = 0), the proportion of the samples uncontaminated, is unbi-\n\u22121\nased for \u03c0, and find its variance. using the rao\u2013blackwell theorem or otherwise, show\n(cid:2)\nthat an unbiased estimator of \u03c0 having smaller variance than u is v = {(n \u2212 1)/n}ny ,\nwhere y = n\nfind var(v ) and hence give the asymptotic efficiency of u relative to v .\n\ny j . isthis a minimum variance unbiased estimator of \u03c0?\n\n\u22121\n\n "}, {"Page_number": 362, "text": "350\n\n7 \u00b7 estimation and hypothesis testing\n\n5 let y1, . . . ,y n be independent poisson variables with means x1\u03b2, . . . , xn\u03b2, where \u03b2 > 0\nx 2\nj is\n\nis an unknown scalar and the x j > 0 are known scalars. show that t = (cid:2)\n(cid:2)\nan unbiased estimator of \u03b2 and find its variance.\nfind a minimal sufficient statistic s for \u03b2, and show that the conditional distribution of y j\ngiven that s = s is multinomial with mean sx j /\ni xi . hence find the minimum variance\nunbiased estimator of \u03b2. is itunique?\nsr = (cid:2)\n6 given that there is a 1\u20131 mapping between x1 < \u00b7\u00b7\u00b7 < xn and the sums s1, . . . ,s n, where\nx r\nj , show that the order statistics of a random sample form a complete minimal\nsufficient statistic in the class of all continuous densities. you may find it useful to consider\nthe exponential family density\n\ny j x j /\n\n(cid:2)\n\nf (y; \u03b8) \u221d exp(\u2212x 2n + \u03b81x + \u00b7\u00b7\u00b7 + \u03b8n x n).\n\n8\n\n7 find the maximum likelihood estimator of \u03b2 based on a random sample from the shifted\n\n\u2212(y\u2212\u03b2) for y \u2265 \u03b2. show that(cid:1)\u03b2 is biased but consistent. does\nexponential density f (y) = e\nit satisfy the cram\u00b4er\u2013rao lower bound?\n\u2212\u03bby, y > 0, \u03bb >0.\n(a) let y1, . . . ,y n be a random sample from the exponential density \u03bbe\nsay why an unbiased estimator w for \u03bb should have form a/s, and hence find a. find\nthe fisher information for \u03bb and show that e(w 2) = (n \u2212 1)\u03bb2/(n \u2212 2). deduce that\nno unbiased estimator of \u03bb attains the cram\u00b4er\u2013rao lower bound, although w does so\nasymptotically.\n(b) let \u03c8 = pr(y > a) = e\n\n\u2212\u03bba, for some constant a. show that\ni (y1 > a) =\n\n1, y1 > a,\n0, otherwise,\n\n(cid:30)\n\nis an unbiased estimator of \u03c8, and hence obtain the minimum variance unbiased estimator.\ndoes this attain the cram\u00b4er\u2013rao lower bound for \u03c8?\n9 let x1, . . . , xn represent the times of the first n events in a poisson process of rate\n\u00b5\u22121 observed from time zero; thus 0 < x1 < \u00b7\u00b7\u00b7 < xn. show that w = 2(x1 + \u00b7\u00b7\u00b7 +\nxn)/{n(n + 1)} is an unbiased estimator of \u00b5, and establish that its rao\u2013blackwellized\nform is t = xn/n. find var(w ) and give the asymptotic efficiency of w relative to t .\n10 show that no unbiased estimator exists of \u03c8 = log{\u03c0/(1 \u2212 \u03c0)}, based on a binomial\n11 let y j = \u03b7 + \u03c4 \u03b5 j , where \u03b51, . . . , \u03b5n is a random sample from a known density. show that\nthe set of order statistics y(1), . . . ,y (n) is in general minimal sufficient for \u03b7, \u03c4 (exam-\nple 4.12). by considering (y(2) \u2212 y(1))/(y(n) \u2212 y(1)) show that it is not complete.\n\nvariable with probability \u03c0.\n\n12 show that when the data are normal, the efficiency of the huber estimating function\n\ngc(y; \u03b8) compared to the optimal function g\u221e(y; \u03b8) is\n{1 \u2212 2\u0001(\u2212c)}2\n\n1 + 2{c2\u0001(\u2212c) \u2212 \u0001(\u2212c) \u2212 c\u03c6(c)} .\n\nhence verify that the efficiency is 0.95 when c = 1.345.\n\n13 compare the performance of the estimating function\n\n(cid:30)\n\ng(y; \u03b8) =\n\ny \u2212 \u03b8,\n0,\n\n|y \u2212 \u03b8| < c,\notherwise,\n\nwith that of the huber function gc(y; \u03b8) for the distributions in example 7.19.\n\n14 show how (a) the poisson birth process in example 4.6, and (b) the markov chain likeli-\nhood in section 6.1.1, fall into the framework for dependent data outlined in section 7.2.3.\niid\u223c n (\u00b5, \u03c3 2), with both parameters unknown. suppose that we wish to test\n\u00b5 = \u00b50 against the one-sided alternative \u00b5 > \u00b50. byconsidering separately the cases\n\n15 let y1, . . . ,y n\n\n "}, {"Page_number": 363, "text": "7.5 \u00b7 problems\n\n351\n\ny \u2265 \u00b50 and y < \u00b50, show that the likelihood ratio statistic is\n\nwp(\u00b50) =\n\n(cid:14)\n\n(cid:30)\n\nn log\n0,\n\n(cid:31)\n\n1 + t (\u00b50)2\nn\u22121\n\n, y \u2265 \u00b50,\ny < \u00b50.\n\nk\n\nhence justify the one-tailed significance level described in example 7.25.\n\nr =\n\n16 independent random samples yi1, . . . ,y ini , where ni \u2265 2, are drawn from each of k nor-\nmal distributions with means \u00b51, . . . , \u00b5k and common unknown variance \u03c3 2. derive the\nlikelihood ratio statistic wp for the null hypothesis that the \u00b5i all equal an unknown \u00b5,\nand show that it is a monotone function of\n\n(cid:2)\ni=1 ni (y i\u00b7 \u2212 y \u00b7\u00b7)2\n(cid:2)\n(cid:2)\nj=1(yi j \u2212 y i\u00b7)2\nk\n(cid:2)\n(cid:2)\ni=1\nwhere y i\u00b7 = n\nj yi j and y \u00b7\u00b7 = (\n\u22121\ni, j yi j . what is the null distribution of r?\n17 let x1, . . . , xm and y1, . . . ,y n be independent random samples from continuous distri-\nbutions fx and fy . wewish to test the hypothesis h0 that fx = fy .\n(cid:2)\ndefine indicator variables ii j = i (xi < y j ) for i = 1, . . . ,m , j = 1, . . . ,n and let u =\ni, j ii j . assuming that h0 is true, (i) show that e(u ) = mn/2; (ii) find cov(ii j , iik) and\ncov(ii j , ikl), where i, j, k, l are distinct; and (iii) hence show that var(u ) = mn(m + n +\n1)/12. why is it important that the underlying distributions are continuous?\nhere are the weight gains (gms) of rats fed on low and high protein diets:\n\n(cid:2)\n\n\u22121\ni\n\nni )\n\n, ,\n\nni\n\nhigh\nlow\n\n83\n70\n\n97\n85\n\n104\n94\n\n107\n101\n\n113\n106\n\n119\n118\n\n123\n132\n\n124\n\n129\n\n134\n\n146\n\n161\n\nuse the approximate normality of u to test for a difference between diets.\n\n18 below are diastolic blood pressures (mm hg) of ten patients before and after treatment for\nhigh blood pressure. test the hypothesis that the treatment has no effect on blood pressure\nusing a wilcoxon signed-rank test, (a) using the exact significance level and (b) using a\nnormal approximation. discuss briefly.\n\nbefore\nafter\n\n94\n96\n\n105\n96\n\n101\n95\n\n106\n103\n\n118\n105\n\n107\n111\n\n96\n86\n\n102\n90\n\n114\n107\n\n95\n84\n\n19 (a) a random sample of size n = 2 istaken from f (y). for 0 < \u03b1 <1/2, find a critical\n\n(cid:6)\n\nregion of size \u03b1 for testing that f (y) is\nf0(y) =\n\n\u03b8\u22121, 0 < y < \u03b8,\n0,\notherwise,\nwhen \u03b8 = 1, against the alternative that\n\u2212y,\ny > 0. is there a best critical region for testing f = f0 against the composite hypothesis\nf (y) = \u03bb exp(\u2212\u03bby), y > 0, for some \u03bb >0?\n(b) show there is no best critical region when \u03b8 is unknown.\n(c) show that the largest order statistic y(2) is sufficient for \u03b8 under the null model,\nand deduce that there is a uniformly most powerful test based on the ratio of conditional\ndensities of y given y(2) under the two hypotheses. show that the most powerful conditional\ncritical region of size \u03b1 is y\u03b1 = {(y1, y2) : 0\u2264 y(1) \u2264 \u03b1y(2))}.\n(d) find the conditional critical region for general n.\n\nf (y) isthe exponential density\n\nf1(y) = e\n\n20 if\n\nf (x; \u03b8) =\n\n(cid:6)\n\n\u22121x \u03bb\u22121e\n\n\u03b8 \u03bb\u0001(\u03bb)\n0,\n\n\u2212\u03b8 x ,\n\nx > 0,\nelsewhere,\n\nwhere \u03bb is known and \u03b8 is positive, deduce that there exists a uniformly most powerful\ntest of size \u03b1 of the hypothesis \u03b8 = \u03b80 against the alternative \u03b8 > \u03b80, and show that when\n\u03bb = 1/n the power function of the test is 1 \u2212 (1 \u2212 \u03b1)\u03b8/\u03b80.\n\n "}, {"Page_number": 364, "text": "7 \u00b7 estimation and hypothesis testing\n352\n21 a source at location x = 0 pollutes the environment. are cases of a rare disease d later\n\nobserved at positions x1, . . . , xn linked to the source?\ncases of another rare disease d(cid:1)\nknown to be unrelated to the pollutant but with the same\nsusceptible population as d are observed at x\n(cid:1)\nm. ifthe probabilities of contracting\nd and d(cid:1)\n, and the population of susceptible individuals has\ndensity \u03bb(x), show that the probability of d at x, given that d or d(cid:1)\n\nare respectively \u03c8(x) and \u03c8(cid:1)\n\noccurs there, is\n\n, . . . , x\n\n(cid:1)\n1\n\n\u03c0(x) =\n\n\u03c8(x)\u03bb(x)\n\n\u03c8(x)\u03bb(x) + \u03c8(cid:1)\u03bb(x)\n\n.\n\ndeduce that the probability of the observed configuration of diseased persons, conditional\non their positions, is\n\nn(cid:21)\nj=1\n\nm(cid:21)\ni=1\n\n\u03c0(x j )\n\n{1 \u2212 \u03c0(x\n\ni )}.\n(cid:1)\n\nthe null hypothesis that d is unrelated to the pollutant asserts that \u03c8(x) isindependent of\nx. show that in this case the unknown parameters may be eliminated by conditioning on\nhaving observed n cases of d out of a total n + m cases. deduce that the null probability\nof the observed pattern is ( n+m\nif t is a statistic designed to detect decline of \u03c8(x) with x, explain how permutation of\ncase labels d, d(cid:1)\nsuch a test is typically only conducted after a suspicious pattern of cases of d has been\nobserved. how will this influence pobs?\n\nmay be used to obtain a significance level pobs.\n\n\u22121.\n)\n\nn\n\n "}, {"Page_number": 365, "text": "8\n\nlinear regression models\n\nregression models are used to describe how one or perhaps a few response variables\ndepend on other explanatory variables. the idea of regression is at the core of much\nstatistical modelling, because the question \u2018what happens to y when x varies?\u2019 is cen-\ntral to many investigations. it is often required to predict or control future responses by\nchanging the other variables, or to gain an understanding of the relation between them.\nthere is usually a single response, treated as random. often there are many explanatory\nvariables, which are treated as non-stochastic. the simplest models involve linear de-\npendence and are described in this chapter, while chapter 9 deals with more structured\nsituations in which the explanatory variables have been chosen by the experimenter\naccording to a design. chapter 10 describes some of the many extensions of regression\nto nonlinear dependence. throughout we simplify our previous notation by using y to\nrepresent both the response variable and the value it takes; no confusion should arise\nthereby.\n\n8.1 introduction\n\nif we denote the response by y and the explanatory variables by x, our concern is\nhow changes in x affect y. insection 5.1, for example, the key question was how the\nannual maximum sea level in venice depended on the passage of time. we fitted the\nstraight-line regression model\n\ny j = \u03b20 + \u03b21x j + \u03b5 j ,\n\nj = 1, . . . ,n ,\n\nwhere we took y j to be the jth annual maximum sea level and x j to be the year in\nwhich this occurred. the parameters \u03b20 and \u03b21 represent a baseline maximum sea\nlevel and the annual rate at which sea level increases, while \u03b5 j is a random variable\nthat represents the difference between the underlying level, \u03b20 + \u03b21x j , and the value\nobserved, y j .\n\n353\n\n "}, {"Page_number": 366, "text": "354\n\n8 \u00b7 linear regression models\n\nan immediate generalization is to increase the number of explanatory variables,\n\nsetting\n\ny j = \u03b21x j1 + \u00b7\u00b7\u00b7 +\u03b2 px j p + \u03b5 j = x t\n\nj\n\n\u03b2 + \u03b5 j ,\n\n= (x j1, . . . , x j p) is a 1 \u00d7 p vector of explanatory variables associated with\nwhere x t\nthe jth response, \u03b2 is a p \u00d7 1 vector of unknown parameters and \u03b5 j is an unobserved\nj\nerror accounting for the discrepancy between the observed response y j and x t\n\u03b2. in\nj\nmatrix notation,\n\ny = x\u03b2 + \u03b5,\n\n(8.1)\nwhere y is the n \u00d7 1 vector whose jth element is y j , x is an n \u00d7 p matrix whose\nj , and \u03b5 is the n \u00d7 1 vector whose jth element is \u03b5 j . the data on which\njth row is x t\nthe investigation is to be based are y and x, and the aim is to disentangle systematic\nchanges in y due to variation in x from the haphazard scatter added by the errors \u03b5.\nmodel (8.1) is known as a linear regression model with design matrix x.\n\nexample 8.1 (straight-line regression) for the straight-line regression model,\n(8.1) becomes\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\n\uf8eb\n\uf8f6\n\uf8ec\uf8ec\uf8ed\n\uf8f7\uf8f7\uf8f8 ,\n\u03b51\n\u03b52\n...\n\u03b5n\nso x is an n \u00d7 2 matrix and \u03b2 a 2 \u00d7 1 vector of parameters.\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8 =\n\nx1\nx2\n...\nxn\n\ny1\ny2\n...\nyn\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8\n\n1\n1\n...\n1\n\n\u03b20\n\u03b21\n\n(cid:7)\n\n(cid:8)\n\n+\n\n(cid:1)\n\nexample 8.2 (polynomial regression) suppose that the response is a polynomial\nfunction of a single covariate,\n\ny j = \u03b20 + \u03b21x j + \u00b7\u00b7\u00b7 +\u03b2 p\u22121x p\u22121\n\n+ \u03b5 j .\n\nj\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8 =\n\nfor example, we might wish to fit a quadratic or cubic trend in the venice sea level\ndata, in which case we would have p = 3 or p = 4 respectively. then\n\uf8f6\n\uf8eb\n\uf8f7\uf8f7\uf8f8 ,\n\uf8ec\uf8ec\uf8ed\n\u03b51\n\u03b52\n...\n\u03b5n\n\nx1\nx2\n...\nxn\nwhere x has dimension n \u00d7 p.\n\nx p\u22121\nx p\u22121\n...\nx p\u22121\n\n\u03b20\n\u03b21\n...\n\u03b2 p\u22121\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8 +\n\nx 2\n1\nx 2\n2\n...\nx 2\nn\n\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ed\n\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f8\n\ny1\ny2\n...\nyn\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n\n1\n1\n...\n1\n\n\u00b7\u00b7\u00b7\n\n(cid:1)\n\n1\n\n2\n\nn\n\na key point is that (8.1) is linear in the parameters \u03b2. polynomial regression can\n\nbe written in form (8.1) because of its linearity, not in x, butin \u03b2.\n\nexample 8.3 (cement data) table 8.1 contains data on the relationship between\nthe heat evolved in the setting of cement and its chemical composition. data on\nheat evolved, y, for each of n = 13 independent samples are available, and for each\n\n "}, {"Page_number": 367, "text": "table 8.1 cement data\n(woods et al., 1932): y is\nheat evolved in calories\nper gram of cement, and\nx1, x2, x3, and x4 are\npercentage weight of\nclinkers, with x1,\n3ca o.al2 o3, x2,\n3ca o.si o2, x3,\n4ca o.al2 o3.fe2 o3,\nand x4, 2cao.si o2.\n\nfigure 8.1 plots of\ncement data. the\nvariables are heat evolved\nin calories per gram, y,\npercentage weight in\nclinkers of x1,\n3ca o.al2 o3, x2,\n3ca o.si o2, x3,\n4ca o.al2 o3.fe2 o3,\nand x4, 2cao.si o2.\n\n8.1 \u00b7 introduction\n\n355\n\nx2\n\n26\n29\n56\n31\n52\n55\n71\n31\n54\n47\n40\n66\n68\n\n\u2022\n\ncase\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\nx1\n\n7\n1\n11\n11\n7\n11\n3\n1\n2\n21\n1\n11\n10\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n5\n\n10\n\n15\n\n20\n\nx3\n\n6\n15\n8\n8\n6\n9\n17\n22\n18\n4\n23\n9\n8\n\nx4\n\n60\n52\n20\n47\n33\n22\n6\n44\n22\n26\n34\n12\n12\n\ny\n\n78.5\n74.3\n104.3\n87.6\n95.9\n109.2\n102.7\n72.5\n93.1\n115.9\n83.8\n113.3\n109.4\n\n0\n1\n1\n\n0\n0\n1\n\n0\n9\n\n0\n8\n\nl\n\ny\n \nd\ne\nv\no\nv\ne\n \nt\na\ne\nh\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n30\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n40\n\n50\n\n60\n\n70\n\npercentage weight in clinkers, x1\n\npercentage weight in clinkers, x2\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n15\n\n5\n\n10\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n0\n1\n1\n\n0\n0\n1\n\n0\n9\n\n0\n8\n\ny\n \nd\ne\nv\no\nv\ne\n\nl\n\n \nt\n\na\ne\nh\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n20\n\n10\n\n20\n\n30\n\n40\n\n50\n\n60\n\npercentage weight in clinkers, x3\n\npercentage weight in clinkers, x4\n\n0\n1\n1\n\n0\n0\n1\n\n0\n9\n\n0\n8\n\n0\n1\n1\n\n0\n0\n1\n\n0\n9\n\n0\n8\n\nl\n\ny\n \nd\ne\nv\no\nv\ne\n \nt\na\ne\nh\n\ny\n \nd\ne\nv\no\nv\ne\n\nl\n\n \nt\n\na\ne\nh\n\nsample the percentage weight in clinkers of four chemicals, x1, 3ca o.al2 o3, x2,\n3cao.si o2, x3, 4cao.al2 o3.fe2 o3, and x4, 2ca o.si o2, isrecorded.\n\nfigure 8.1 shows that although the response y depends on each of the covariates\n\nx1, . . . , x4, the degrees and directions of the dependences differ.\n\n "}, {"Page_number": 368, "text": "356\n\nin this case we might fit the model\n\n8 \u00b7 linear regression models\n\ny j = \u03b20 + \u03b21x1 j + \u03b22x2 j + \u03b23x3 j + \u03b24x4 j + \u03b5 j ,\n\nwhere figure 8.1 suggests that \u03b21 and \u03b22 are positive, and that \u03b23 and \u03b24 are negative.\nthe design matrix has dimension 13 \u00d7 5, and is\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\nx =\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8 ;\n\n1\n7\n1\n1\n...\n...\n1 10\n\n6\n\n26\n60\n29 15 52\n...\n...\n12\n68\n\n...\n8\n\nthe vectors y and \u03b5 have dimension 13 \u00d7 1 and \u03b2 has dimension 5 \u00d7 1.\n\n(cid:1)\n\nin the examples above the explanatory variables consist of numerical quantities,\nsometimes called covariates. dummy variables that represent whether or not an effect\nis applied can also appear in the design matrix.\n\nexample 8.4 (cycling data) norman miller of the university of wisconsin wanted\nto see how seat height, tyre pressure and the use of a dynamo affected the time taken\nto ride his bicycle up a hill. he decided to collect data at each combination of two\nseat heights, 26 and 30 inches from the centre of the crank, two tyre pressures, 40\nand 55 pounds per square inch (psi) and with the dynamo on and off, giving eight\ncombinations in all. the times were expected to be quite variable, and in order to\nget more accurate results he decided to make two timings for each combination. he\nwrote each of the eight combinations on two pieces of card, and then drew the sixteen\nfrom a box in a random order. he planned to make four widely separated runs up the\nhill on each of four days, first adjusting his bicycle to the setups on the successive\npieces of card, but bad weather forced him to cancel the last run on the first day; he\nmade five on the third day to make up for this. table 8.2 gives timings obtained with\nhis wristwatch.\n\nthe lower part of table 8.2 shows how average time depends on experi-\nmental setup. there is a large reduction in the average time when the seat is\nraised and smaller reductions when the tyre pressure is increased and the dynamo\nis off.\n\nthe quantities that are varied in this experiment \u2014 seat height, tyre pressure, and\nthe state of the dynamo \u2014 are known as factors. each takes two possible values,\nknown as levels. here there are two types of factors: quantitative and qualitative. the\ntwo levels of seat height and tyre pressure are quantitative \u2014 other values might have\nbeen chosen, and more than two levels could have been used \u2014 but the dynamo factor\nhas only two possible levels and is qualitative.\n\nan experiment like this, in which data are collected at each combination of a\nnumber of factors, is known as a factorial experiment. such designs and their variants\n\n "}, {"Page_number": 369, "text": "table 8.2 data and\nexperimental setup for\nbicycle experiment (box\net al., 1978, pp. 368\u2013372).\nthe lower part of the table\nshows the average times\nfor each of the eight\ncombinations of settings\nof seat height, tyre\npressure, and dynamo,\nand the average times for\nthe eight observations at\neach setting, considered\nseparately.\n\n8.1 \u00b7 introduction\n\n357\n\nsetup\n\nday\n\nrun\n\nseat height\n\n(inches)\n\ntyre pressure\n\ndynamo\n\n(psi)\n\ntime\n(secs)\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n3\n4\n2\n2\n3\n2\n3\n4\n1\n4\n3\n4\n3\n1\n1\n2\n\n2\n1\n2\n3\n3\n1\n1\n3\n1\n4\n5\n2\n4\n3\n2\n4\n\n\u2212\n\u2212\n+\n+\n\u2212\n\u2212\n+\n+\n\u2212\n\u2212\n+\n+\n\u2212\n\u2212\n+\n+\n\n\u2212\n\u2212\n\u2212\n\u2212\n+\n+\n+\n+\n\u2212\n\u2212\n\u2212\n\u2212\n+\n+\n+\n+\n\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n+\n+\n+\n+\n+\n+\n+\n+\n\n51\n54\n41\n43\n54\n60\n44\n43\n50\n48\n39\n39\n53\n51\n41\n44\n\nseat height\n\ntyre pressure\n\n(inches from centre of crank)\n\ndynamo\n\n(psi)\n\n\u2212\n+\n\n26\n30\n\noff\non\n\n40\n55\n\ntyre pressure low\n\ntyre pressure high\n\ndynamo\n\nseat low\n\nseat high\n\nseat low\n\nseat high\n\noff\non\n\n52.5\n57.0\n\n42.0\n43.5\n\n49.0\n52.0\n\n39.0\n42.5\n\ndynamo\n\ntyre pressure\n\nseat\n\noff\n\non\n\nlow\n\nhigh\n\nlow\n\nhigh\n\n45.63\n\n48.75\n\n48.75\n\n45.63\n\n52.63\n\n41.75\n\nare widely used; see section 9.2.4. in this case an experimental setup with three factors\neach having two levels is applied twice: the design consists of two replicates of a\n23 factorial experiment.\n\none linear model for the data in table 8.2 is that at the lower seat height, with the\ndynamo off, and the lower tyre pressure, the mean time is \u00b5, and the three factors act\nseparately, changing the mean time by \u03b11, \u03b12, and \u03b13 respectively. this corresponds\n\n "}, {"Page_number": 370, "text": "358\n\n8 \u00b7 linear regression models\n\uf8eb\n\n\uf8f6\n\nto the linear regression model\n\n\uf8eb\n\n\uf8f6\n\n\uf8eb\n\n\uf8f6\n\n=\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8 +\n\n0\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n1\n1\n1\n1\n\n1 0\n1 0\n1 1\n1 1\n1 0\n1 0\n1 1\n1 1\n1 0\n1 0\n1 1\n1 1\n1 0\n1 0\n1 1\n1 1\n\ny1\ny2\ny3\ny4\ny5\ny6\ny7\ny8\ny9\ny10\ny11\ny12\ny13\ny14\ny15\ny16\n.= 52.5, that \u03b11 < 0, \u03b12 > 0, and \u03b13 < 0. the baseline time\ntable 8.2 suggests that \u00b5\nis \u00b5, which corresponds to the mean time at the lower level of all three factors, and\n\u03b13 + \u03b5, where \u03b5 is the average of\nthe overall average time is y = \u00b5 + 1\nthe unobserved errors.\n\n\u03b51\n\u03b52\n\u03b53\n\u03b54\n\u03b55\n\u03b56\n\u03b57\n\u03b58\n\u03b59\n\u03b510\n\u03b511\n\u03b512\n\u03b513\n\u03b514\n\u03b515\n\u03b516\n\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n\n\u03b11 + 1\n\n\u03b12 + 1\n\n\u00b5\n\u03b11\n\u03b12\n\u03b13\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n2\n\n2\n\n2\n\n.\n\na different formulation of the model would take the overall mean time as the\n\nbaseline, leading to\uf8eb\n\n\uf8f6\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\ny1\ny2\ny3\ny4\ny5\ny6\ny7\ny8\ny9\ny10\ny11\ny12\ny13\ny14\ny15\ny16\n\n\uf8eb\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n\uf8f6\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n1 \u22121 \u22121 \u22121\n1 \u22121 \u22121 \u22121\n\u22121\n1\u22121\n1\n\u22121\n1\u22121\n1\n1\u22121\n1 \u22121\n1\u22121\n1 \u22121\n1\u22121\n1\n1\n1\u22121\n1\n1\n1 \u22121 \u22121\n1 \u22121 \u22121\n1\u22121\n1\n1\u22121\n1\n1 \u22121\n1 \u22121\n1\n1\n1\n1\n\n1\n1\n1\n1\n\n1\n1\n1\n1\n\n1\n1\n\n1\n1\n\n=\n\n\uf8f6\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n\uf8eb\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n\u03b51\n\u03b52\n\u03b53\n\u03b54\n\u03b55\n\u03b56\n\u03b57\n\u03b58\n\u03b59\n\u03b510\n\u03b511\n\u03b512\n\u03b513\n\u03b514\n\u03b515\n\u03b516\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8 +\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\n\u03b20\n\u03b21\n\u03b22\n\u03b23\n\n.\n\n(8.2)\n\nin (8.2) the effect of increasing seat height from 26 to 30 inches is 2\u03b21, the effect\nof switching the dynamo on is 2\u03b22, and the effect of increasing tyre pressure is 2\u03b23.\nas each column of the design matrix apart from the first has sum zero, the overall\naverage time in this parametrization is \u03b20 + \u03b5. although the parameter \u03b20 is related\n\n "}, {"Page_number": 371, "text": "8.2 \u00b7 normal linear model\n\n359\n\nto the overall mean, it does not correspond to a combination of factors that can be\napplied to the bicycle \u2014 how can the dynamo be half on? despite this, we shall see\n(cid:1)\nbelow that (8.2) is convenient for some purposes.\n\noften it is better to apply a linear model to transformed data than to the original\n\nobservations.\n\nexample 8.5 (multiplicative model) suppose that the data consist of times to\nfailure that depend on positive covariates x1 and x2 according to\n\ny = \u03b30x \u03b31\nwhere \u03b7 is a positive random variable. then\n\n1 x \u03b32\n\n2\n\n\u03b7,\n\nlog y = log \u03b30 + \u03b31 log x1 + \u03b32 log x2 + log \u03b7,\n\nwhich is linear in log \u03b30, \u03b31, and \u03b32. the variance of the transformed response log y\ndoes not depend on its mean, whereas y has variance proportional to the square of its\nmean, so in addition to achieving linearity, the transformation equalizes the variances.\n(cid:1)\n\nexercises 8.1\n1 which of the following can be written as linear regression models, (i) as they are, (ii) when\na single parameter is held fixed, (iii) after transformation? for those that can be so written,\ngive the response variable and the form of the design matrix.\n(a) y = \u03b20 + \u03b21/x + \u03b22/x 2 + \u03b5;\n(b) y = \u03b20/(1 + \u03b21x) + \u03b5;\n(c) y = 1/(\u03b20 + \u03b21x + \u03b5);\n(d) y = \u03b20 + \u03b21x \u03b22 + \u03b5;\n(e) y = \u03b20 + \u03b21x \u03b22\n+ \u03b23x \u03b24\ndata are available on the weights of two groups of three rats at the beginning of a fortnight,\nx, and at its end, y. during the fortnight, one group was fed normally and the other group\nwas fed a growth inhibitor. consider a linear model for the weights,\n\n+ \u03b5;\n\n2\n\n1\n\n2\n\ny jg = \u03b1g + \u03b2gx jg + \u03b5 jg,\n\nj = 1, . . . ,3,\n\ng = 1, 2.\n\n(a) write down the design matrix for the model above.\n(b) the model is to be reparametrized in such a way that it can be specialized to (i) two\nparallel lines for the two groups, (ii) two lines with the same intercept, (iii) one common\nline for both groups, just by setting parameters to zero. give one design matrix which can\nbe made to correspond to (i), (ii), and (iii), just by dropping columns.\n\n8.2 normal linear model\n8.2.1 estimation\nsuppose that the errors \u03b5 j in (8.1) are independent normal random variables, with\nmeans zero and variances \u03c3 2. then the responses y j are independent normal random\n\u03b2 and variances \u03c3 2, and (8.1) is the normal linear model. the\nvariables with means x t\nj\n\n "}, {"Page_number": 372, "text": "360\n\n8 \u00b7 linear regression models\n\nlikelihood for \u03b2 and \u03c3 2 is\n\nl(\u03b2, \u03c3 2) = n(cid:9)\n\nj=1\n\n(cid:10)\n\n(cid:11)\n\n\u2212 1\n2\u03c3 2\n\n(cid:13)\n\n(cid:12)2\n\n,\n\ny j \u2212 x t\n\nj\n\n\u03b2\n\n1\n\n(2\u03c0 \u03c3 2)1/2 exp\n\nand the log likelihood is\n\n(cid:8)(\u03b2, \u03c3 2) \u2261 \u2212 1\n2\n\n(cid:14)\n\nn log \u03c3 2 + 1\n\n\u03c3 2\n\n(cid:16)\n\n(cid:12)2\n\n.\n\n(cid:11)\n\nn(cid:15)\nj=1\n\ny j \u2212 x t\n\nj\n\n\u03b2\n\nwhatever the value of \u03c3 2, the log likelihood is maximized with respect to \u03b2 at the\n\nvalue that minimizes the sum of squares\ny j \u2212 x t\n\nss(\u03b2) = n(cid:15)\n\n(cid:11)\n\n\u03b2\n\nj\n\nj=1\n\n(cid:12)2 = (y \u2212 x\u03b2)t(y \u2212 x\u03b2).\n\n(8.3)\n\nwe obtain the maximum likelihood estimate of \u03b2 by solving simultaneously the\nequations\n\n\u2202 ss(\u03b2)\n\n\u2202\u03b2r\n\n= 2\n\nn(cid:15)\nj=1\n\nx jr (y j \u2212 \u03b2 tx j ) = 0,\n\nr = 1, . . . , p.\n\nin matrix form these amount to the normal equations\nx t(y \u2212 x\u03b2) = 0,\n\n(8.4)\nwhich imply that the estimate satisfies (x t x)\u03b2 = x t y. provided the p \u00d7 p matrix\nx t x is of full rank it is invertible, and the least squares estimator of \u03b2 is\n\n(cid:17)\u03b2 = (x t x)\n\n\u22121 x t y.\n\nthe maximum likelihood estimator of \u03c3 2 may be obtained from the profile likeli-\n\nhood for \u03c3 2,\n\n(cid:8)p(\u03c3 2) = max\n\n\u03b2\n\n(cid:8)(\u03b2, \u03c3 2) = \u2212 1\n2\n\n(cid:10)\n\nn log \u03c3 2 + 1\n\n\u03c3 2 (y \u2212 x(cid:17)\u03b2)t(y \u2212 x(cid:17)\u03b2 )\n\n(cid:13)\n\n,\n\n(8.5)\n\nand it follows by differentiation that the maximum likelihood estimator of \u03c3 2 is\n\n(cid:17)\u03c3 2 = n\n\n\u22121(y \u2212 x(cid:17)\u03b2)t(y \u2212 x(cid:17)\u03b2) = n\n\n\u22121\n\n(cid:17)\u03b2\n\ny j \u2212 x t\n\nj\n\n(cid:12)2.\n\n(cid:11)\n\nn(cid:15)\nj=1\n\nwe shall see below that(cid:17)\u03c3 2 is biased and that an unbiased estimator of \u03c3 2 is\n(cid:12)2.\n\n(y \u2212 x(cid:17)\u03b2)t(y \u2212 x(cid:17)\u03b2) = 1\nn \u2212 p\n\ns2 = 1\nn \u2212 p\n\ny j \u2212 x t\n\n(cid:17)\u03b2\n\n(cid:11)\n\nj\n\nn(cid:15)\nj=1\n\n "}, {"Page_number": 373, "text": "8.2 \u00b7 normal linear model\n\n361\n\nexample 8.6 (straight-line regression) we write the straight-line regression model\n\n(5.3) in matrix form as\uf8eb\n\uf8ec\uf8ec\uf8ed\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8 =\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\nthe least squares estimates are\n\n(cid:17)\u03b2 =\n\n(cid:8)\n\n(cid:7)(cid:17)\u03b30(cid:17)\u03b31\n\ny1\ny2\n...\nyn\n\n(cid:7)\n(cid:7)\n(cid:7)\n\n=\n\n=\n\n=\n\n(cid:18)\nn\n(x j \u2212 x)\n\u22121\n0\nn\n1(cid:18)\n(cid:8)\n0\n(x j\u2212x)2\ny(cid:18)\n(cid:18)\n(x j\u2212x)y j\n(x j\u2212x)2\n\n.\n\n1\n1\n...\n1\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8\n\n(cid:8)\n\n...\n\n(cid:7)\n\n\u03b30\n\u03b31\n\nx1 \u2212 x\nx2 \u2212 x\nxn \u2212 x\n(cid:18)\n(cid:18)\n(x j \u2212 x)\n(x j \u2212 x)2\n(cid:8)(cid:7) (cid:18)\n(cid:18)\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\n+\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8 .\n\n\u03b51\n\u03b52\n...\n\u03b5n\n\n(cid:18)\n(cid:8)\n\ny j\n\n(x j \u2212 x)y j\n\n(cid:8)\u22121(cid:7) (cid:18)\n\n(cid:8)\n\ny j\n\n(x j \u2212 x)y j\n\nif all the x j are equal, x t x is not invertible, and (cid:17)\u03b31 is undetermined: any value is\n\npossible.\n\nthe unbiased estimator of \u03c3 2 is\n\n(cid:10)\n\nn(cid:15)\nj=1\n\n1\nn \u2212 2\n\ny j \u2212 y \u2212 (x j \u2212 x)\n\n(cid:18)\n(xk \u2212 x)yk\n(cid:18)\n(xk \u2212 x)2\n\n(cid:13)2\n\n.\n\n(cid:1)\n\n\uf8f6\n\uf8f8\n\n(cid:8)\n\nexample 8.7 (surveying a triangle) suppose that we want to estimate the angles\n\u03b1, \u03b2, and \u03b3 (radians) of a triangle abc based on a single independent measurement\nof the angle at each corner. although there are three angles, their sum is the constant\n\u03b1 + \u03b2 + \u03b3 = \u03c0, and so just two of them vary independently. in terms of \u03b1 and \u03b2,\nwe have ya = \u03b1 + \u03b5a, yb = \u03b2 + \u03b5b, and yc = \u03c0 \u2212 \u03b1 \u2212 \u03b2 + \u03b5c, and this gives the\nlinear model\n\n\uf8eb\n\uf8ed ya\nyb\nyc \u2212 \u03c0\n\n\uf8f6\n\uf8f8 =\n\n\uf8eb\n\uf8ed 1\n0\n1\n0\n\u22121 \u22121\n\n(cid:7)\n\n(cid:8)\n\n+\n\n\u03b1\n\u03b2\n\n\uf8f6\n\uf8f8 .\n\n\uf8eb\n\uf8ed \u03b5a\n\n\u03b5b\n\u03b5c\n\n(cid:8)\n\nhence(cid:7)(cid:17)\u03b1(cid:17)\u03b2\n\n(cid:7)\n\n(cid:8)(cid:7)\n\n(cid:7)\n\n= 1\n3\n\n2 \u22121\n\u22121\n2\n\n\u03c0 + ya \u2212 yc\n\u03c0 + yb \u2212 yc\n\n= 1\n3\n\n\u03c0 + 2ya \u2212 yb \u2212 yc\n\u03c0 + 2yb \u2212 ya \u2212 yc\n\nit is straightforward to show that s2 = (ya + yb + yc \u2212 \u03c0)2/3.\nthe sum of squares ss(\u03b2) plays a central role. its minimum value,\n(cid:12)2 = (y \u2212 x(cid:17)\u03b2)t(y \u2212 x(cid:17)\u03b2),\n(cid:17)\u03b2\n\nss((cid:17)\u03b2) = n(cid:15)\n\ny j \u2212 x t\n\n(cid:11)\n\nj\n\n(cid:8)\n\n.\n\n(cid:1)\n\nj=1\n\nis called the residual sum of squares because it is the residual squared discrepancy\n\nbetween the observations, y, and the fitted values,(cid:17)y = x(cid:17)\u03b2. the vector(cid:17)y is the linear\n\n "}, {"Page_number": 374, "text": "362\n\n8 \u00b7 linear regression models\n\ncombination of the columns of x that best accounts for the variation in y, inthe sense\nof minimizing the squared distance between them. note that\n\u22121 x t y = h y,\n\n(cid:17)y = x(cid:17)\u03b2 = x(x t x)\nsay, where the hat matrix h = x(x t x)\nprojection matrix; see section 8.2.2.\nthe unobservable error \u03b5 j = y j \u2212 x t\n(cid:17)y j = y j \u2212 x t\n\n(cid:17)\u03b2. in vector terms,\n\nj\n\nj\n\ne = y \u2212 x(cid:17)\u03b2 = y \u2212 h y = (in \u2212 h)y,\n\nwhere in is the n \u00d7 n identity matrix.\nexample 8.8 (cycling data) for model (8.2) we find that\n\n\u22121 x t \u201cputs hats\u201d on y. evidently h is a\n\u03b2 is estimated by the jth residual e j = y j\u2212 sometimes e j is called a\n\nraw residual.\n\n(x t x)\n\n\u22121 = 1\n16\n\u22121 x t y are\n\ni4,\n\nso the least squares estimates (x t x)\n\n\uf8eb\n\uf8ec\uf8ed y1 + y2 + y3 + y4 + y5 + y6 + y7 + y8 + y9 + y10 + y11 + y12 + y13 + y14 + y15 + y16\n\u2212y1 \u2212 y2 + y3 + y4 \u2212 y5 \u2212 y6 + y7 + y8 \u2212 y9 \u2212 y10 + y11 + y12 \u2212 y13 \u2212 y14 + y15 + y16\n\u2212y1 \u2212 y2 \u2212 y3 \u2212 y4 + y5 + y6 + y7 + y8 \u2212 y9 \u2212 y10 \u2212 y11 \u2212 y12 + y13 + y14 + y15 + y16\n\u2212y1 \u2212 y2 \u2212 y3 \u2212 y4 \u2212 y5 \u2212 y6 \u2212 y7 \u2212 y8 + y9 + y10 + y11 + y12 + y13 + y14 + y15 + y16\n\n1\n16\n\n\uf8f6\n\uf8f7\uf8f8 =\n\n\uf8f6\n\uf8f7\uf8f8 .\n\n\uf8eb\n\uf8ec\uf8ed 47.19\u22125.437\n1.563\u22121.563\n\nthus the overall average time is 47.19 seconds, putting the seat at height\n30 inches rather than 26 inches changes the time by an average of 2 \u00d7 (\u22125.437) =\n\u221210.87 seconds, putting the dynamo on rather than off changes the time by an aver-\nage of 2 \u00d7 1.563 = 3.13 seconds, and increasing the tyre pressure from 40 to 55 psi\nchanges the time by \u20133.13 seconds. the largest effect is due to increasing the seat\nheight. the model suggests that the fastest time is obtained with no dynamo, a high\nseat and tyres at 55 psi.\n\ny2\nj\n\nthe residual sum of squares for this model is 43.25 seconds squared, the overall\n= 36221 seconds squared, and therefore the sum of squares\nsum of squares is\nexplained by the model is 36221 \u2212 43.25 = 36177.75 seconds squared; this is the\nthe fitted values are (cid:17)y = x(cid:17)\u03b2, giving (cid:17)y1 =(cid:17)\u03b20 \u2212(cid:17)\u03b21 \u2212(cid:17)\u03b22 \u2212(cid:17)\u03b23 = 52.625, e1 =\namount of variation removed when x\u03b2 is fitted.\ny1 \u2212(cid:17)y1 = 51 \u2212 52.625 = \u22121.625, and so forth. table 8.3 gives the data, fitted values,\n\n(cid:18)\n\nresiduals and quantities discussed in examples 8.22 and 8.27.\n\n(cid:1)\n\n8.2.2 geometrical interpretation\nfigure 8.2 shows the geometry of least squares. the n-dimensional vector space in-\nhabited by the observation vector y is represented by the space spanned by all three\nplane through the origin. the least squares estimate(cid:17)\u03b2 minimizes (y \u2212 x\u03b2)t(y \u2212 x\u03b2),\naxes, and the p-dimensional subspace in which x\u03b2 lies is represented by the horizontal\nwhich is the squared distance between x\u03b2 and y. wesee that ( y \u2212 x\u03b2)t(y \u2212 x\u03b2) is\nminimized when the vector y \u2212 x\u03b2 is orthogonal to the horizontal plane spanned by\nthe columns of x, sothat for any column x of x we have x t(y \u2212 x\u03b2) = 0. equiv-\nalently the normal equations x t(y \u2212 x\u03b2) = 0 hold, and provided x t x is invertible\n\n "}, {"Page_number": 375, "text": "table 8.3 data from\nbicycle experiment,\ntogether with fitted values\n\n(cid:17)y, raw residuals e,\n\nstandardized residuals, r,\ndeletion residuals r\n,\nleverages h and cook\ndistances c.\n\n(cid:2)\n\n8.2 \u00b7 normal linear model\n\nseat\nheight dynamo\n\ntyre\n\npressure\n\ntime\n\ny\n\nsetup\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\u22121\n\u22121\n1\n1\n\u22121\n\u22121\n1\n1\n\u22121\n\u22121\n1\n1\n\u22121\n\u22121\n1\n1\n\n\u22121\n\u22121\n\u22121\n\u22121\n1\n1\n1\n1\n\u22121\n\u22121\n\u22121\n\u22121\n1\n1\n1\n1\n\n\u22121\n\u22121\n\u22121\n\u22121\n\u22121\n\u22121\n\u22121\n\u22121\n1\n1\n1\n1\n1\n1\n1\n1\n\n51\n54\n41\n43\n54\n60\n44\n43\n50\n48\n39\n39\n53\n51\n41\n44\n\n(cid:2)\n\nr\n\nr\n\ne\n\n0.76\n\n4.250\n\n1.250\n\n1.375 \u22120.84\n\n(cid:17)y\n52.62 \u22121.625 \u22120.99 \u22120.99\n0.83\n52.62\n41.75 \u22120.750 \u22120.46 \u22120.44\n0.75\n41.75\n55.75 \u22121.750 \u22121.06 \u22121.07\n55.75\n3.72\n44.87 \u22120.875 \u22120.53 \u22120.52\n44.87 \u22121.875 \u22121.14 \u22121.16\n0.29\n49.50\n49.50 \u22121.500 \u22120.91 \u22120.91\n0.22\n38.62\n38.62\n0.22\n52.62\n0.22\n52.62 \u22121.625 \u22120.99 \u22120.99\n41.75 \u22120.750 \u22120.46 \u22120.44\n1.43\n41.75\n\n0.375\n0.375\n0.375\n\n0.23\n0.23\n0.23\n\n2.250\n\n0.500\n\n2.59\n\n0.30\n\n1.37\n\n363\n\nh\n\nc\n\n0.25\n0.25\n0.25\n0.25\n0.25\n0.25\n0.25\n0.25\n0.25\n0.25\n0.25\n0.25\n0.25\n0.25\n0.25\n0.25\n\n0.08\n0.06\n0.02\n0.05\n0.09\n0.56\n0.02\n0.11\n0.01\n0.07\n0.00\n0.00\n0.00\n0.08\n0.02\n0.16\n\n\u22121 x t y. the fitted value (cid:17)y = x(cid:17)\u03b2 = x(x t x)\n\nwe obtain (cid:17)\u03b2 = (x t x)\n\u22121 x t y = h y is\nmatrix representing that projection is h. notice that(cid:17)y is unique whether or not x t x\nthe orthogonal projection of y onto the plane spanned by the columns of x, and the\nfigure 8.2 shows that the vector of residuals, e = y \u2212(cid:17)y = (in \u2212 h)y, and the\nvector of fitted values,(cid:17)y = h y, are orthogonal. to see this algebraically, note that\n\nis invertible.\n\n(cid:17)yte = yt h t(in \u2212 h)y = yt(h \u2212 h)y = 0,\n\n(8.6)\nbecause h t = h and h h = h, that is, the projection matrix h is symmetric and\nidempotent (exercise 8.2.5). the close link between orthogonality and independence\nfor normally distributed vectors means that (8.6) has important consequences, as we\nshall see in section 8.3. for now, notice that (8.6) implies that\n\nyt y = (y \u2212(cid:17)y +(cid:17)y)t(y \u2212(cid:17)y +(cid:17)y) = (e +(cid:17)y)t(e +(cid:17)y) = ete +(cid:17)yt(cid:17)y,\n(cid:18)\n\nas is clear from figure 8.2 by pythagoras\u2019 theorem. that is, the overall sum of squares\nof the data,\n\n= yt y, equals the sum of the residual sum of squares, ss((cid:17)\u03b2) =\n\n(y j \u2212(cid:17)y j )2 = ete, and the sum of squares for the fitted model,\n\n=(cid:17)yt(cid:17)y.\n\n(cid:18)(cid:17)y2\n\n(cid:18)\n\n(8.7)\n\ny2\nj\n\nj\n\nsuch decompositions are central to analysis of variance, discussed below.\n\n8.2.3 likelihood quantities\nchapter 4 shows how the observed and expected information matrices play a central\nrole in likelihood inference, by providing approximate variances for maximum like-\nlihood estimates. to obtain these matrices for the normal linear model, note that the\n\n "}, {"Page_number": 376, "text": "figure 8.2 the\ngeometry of least squares\nestimation. the space\nspanned by all three axes\nrepresents the\nn-dimensional observation\nspace in which y lies. the\nhorizontal plane through\no represents the\np-dimensional space in\nwhich the linear\ncombination x\u03b2 lies, and\nestimation by least\nsquares amounts to\nminimizing the squared\ndistance\n(y \u2212 x\u03b2)t(y \u2212 x\u03b2). in\nthe figure the value of x\u03b2\nthat gives the minimum\nlies vertically below y,\nwhich corresponds to\northogonal projection of y\ninto the p-dimensional\nsubspace spanned by the\ncolumns of x; the fitted\n\nvalue(cid:17)y = h y is the point\n\nclosest to y in that\nsubspace, and the\nprojection matrix is\nh = x(x t x)\n\u22121 x t. the\ne = y \u2212(cid:17)y is orthogonal to\nvector of residuals\nthe fitted value(cid:17)y. the line\nx = z = 0 represents the\nspace spanned by the\ncolumns of the reduced\nmodel matrix x1, with\ncorresponding fitted value\n\n(cid:17)y1. the orthogonality of\n(cid:17)y1,(cid:17)y \u2212(cid:17)y1, and y \u2212(cid:17)y\n\nimplies that when the data\nare normal the\ncorresponding sums of\nsquares are independent.\n\n364\n\n8 \u00b7 linear regression models\n\ny\n\n\b\n\n(cid:1)y1\n\n(cid:1)y\n\n\u0018\u0018\n\n0\n\n\u2202 2(cid:8)\n\ny j \u2212 x t\n\nj\n\n\u2202\u03b2r \u2202\u03c3 2\nn(cid:15)\n(cid:11)\nj=1\n(cid:7)\n\n\u2212 \u2202 2(cid:8)\n\u2202\u03b2r \u2202\u03c3 2\n\n(cid:8)\n\nlog likelihood has second derivatives\n\n\u2202 2(cid:8)\n\n\u2202\u03b2r \u2202\u03b2s\n\n= \u2212 1\n\u03c3 2\n\n\u2202 2(cid:8)\n\n\u2202(\u03c3 2)2\n\n= \u2212 1\n2\n\nn(cid:15)\n(cid:14)\nj=1\n\u2212 1\n\u03c3 4\n\nx jr x js ,\n\n+ 2\n\u03c3 6\n\nn(cid:15)\n(cid:16)\nj=1\n\n,\n\n(cid:11)\n\nx jr\n\ny j \u2212 x t\n\nj\n\n\u03b2\n\n(cid:12)\n\n,\n\nr, s = 1, . . . , p.\n\n= 1\n\u03c3 4\n(cid:12)2\n\n\u03b2\n\n(cid:8)\n\n(cid:7)\n\nthus elements of the expected information matrix are\n\n(cid:7)\n\ne\n\n\u2212 \u2202 2(cid:8)\n\u2202\u03b2r \u2202\u03b2s\n\n= 1\n\u03c3 2\n\nx jr x js , e\n\nn(cid:15)\nj=1\n\n(cid:13)\n\n(cid:10)\n\n\u2212 \u2202 2(cid:8)\n\u2202(\u03c3 2)2\n\n= n\n2\u03c3 4\n\n,\n\n= 0, e\n\nor in matrix form\n\n(cid:8)\n\n(cid:7)\n\n(cid:8)\n\n\u22121\n\n\u03c3 \u22122 x t x\n\n.\n\n1\n\n,\n\n0\n\n0\n\n0\n\n2\u03c3 4/n\n\n\u22121 =\n\ni (\u03b2, \u03c3 2)\n\n\u03c3 2(x t x)\n\n0\n2 n\u03c3 \u22124\n\n(exercise 8.2.7).\n\ni (\u03b2, \u03c3 2) =\nprovided that x has rank p, the matrices i (\u03b2, \u03c3 2) and j ((cid:17)\u03b2,(cid:17)\u03c3 2) are positive definite\ntheory of likelihood estimation implies that the asymptotic distribution of(cid:17)\u03b2 and \u03c3 2 is\nunder mild regularity conditions on the design matrix and the errors, the general\n\u22121, the block\ndiagonal structure of which implies that(cid:17)\u03b2 and(cid:17)\u03c3 2 are asymptotically independent. we\nthe estimates(cid:17)\u03b2 have an exact normal distribution and are independent of(cid:17)\u03c3 2 for every\nvalue of n, while(cid:17)\u03c3 2 has a distribution proportional to \u03c7 2\n\nshall see in the next section that stronger results are true: when the errors are normal\n\nnormal with means \u03b2 and \u03c3 2, and covariance matrix given by i (\u03b2, \u03c3 2)\n\nn\u2212 p provided that n > p.\n\n "}, {"Page_number": 377, "text": "8.2 \u00b7 normal linear model\nthe quantities (cid:17)\u03b2 and ss((cid:17)\u03b2) are minimal sufficient statistics for \u03b2 and \u03c3 2\n\n365\n\n(problem 8.7).\n\nexample 8.9 (two-sample model) suppose that we have two groups of normal\ndata, the first with mean \u03b20,\n\nand the second with mean \u03b20 + \u03b21,\n\ny0 j = \u03b20 + \u03b50 j ,\n\nj = 1, . . . ,n 0,\n\ny1 j = \u03b20 + \u03b21 + \u03b51 j ,\n\nj = 1, . . . ,n 1,\n\nwhere the \u03b5g j are independent with means zero and variances \u03c3 2. the matrix form of\nthis model is\n\n\uf8f6\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n\uf8eb\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n=\n\n\uf8f6\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 .\n\n\uf8eb\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n\u03b501\n...\n\u03b50n0\n\u03b511\n...\n\u03b51n1\n\n+\n\n\uf8f6\n\n(cid:8)\n\n(cid:7)\n\n\u03b20\n\u03b21\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n0\n...\n0\n1\n...\n1\n\n1\n...\n1\n1\n...\n1\n\u22121 x t y, that is,\n(cid:8)\u22121(cid:7)\n\n\uf8eb\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\ny01\n...\ny0n0\ny11\n...\ny1n1\n(cid:7)\n(cid:7)\n(cid:7)\n\n=\n\n(cid:8)\n\nthe estimator of \u03b2 is(cid:17)\u03b2 = (x t x)\nn0 + n1\n\n(cid:8)\n\n(cid:7)(cid:17)\u03b20(cid:17)\u03b21\n\nn0 y0\u00b7 + n1 y1\u00b7\n(cid:8)(cid:7)\n\n=\n\n(cid:8)\n\nn1 y1\u00b7\nn0 y0\u00b7 + n1 y1\u00b7\n\nn1\nn1\nn1\n\u2212n\n\u22121\n\u22121\nn\n+ n\n\u2212n\n\u22121\n\u22121\n0\n0\n(cid:8)\nn\n0\n0\ny0\u00b7\ny1\u00b7 \u2212 y0\u00b7\n(cid:18)\nwhere y0\u00b7 = n\ny0 j and y1\u00b7 = n\n\u22121\ny1 j are the group averages. one can verify\n1\n\u22121 give the variances and covariance of the least\ndirectly that the elements of \u03c3 2(x t x)\nin this example the fitted values are(cid:17)\u03b20 = y0\u00b7 for the first group and(cid:17)\u03b20 +(cid:17)\u03b21 = y1\u00b7\nsquares estimators.\n\nn1 y1\u00b7\n\n(cid:18)\n\n\u22121\n1\n\n\u22121\n0\n\n=\n\n,\n\nfor the second group, and the unbiased estimator of \u03c3 2 is\n\ns2 =\n\n1\n\nn0 + n1 \u2212 2\n\n(cid:14)\nn0(cid:15)\nj=1\n\n(y0 j \u2212 y0\u00b7)2 + n1(cid:15)\n\nj=1\n\n(cid:16)\n\n(y1 j \u2212 y1\u00b7)2\n\n.\n\n(cid:1)\n\na minimal sufficient statistic for (\u03b20, \u03b21, \u03c3 2) is (y0\u00b7, y1\u00b7, s2).\n\nexample 8.10 (maize data) the discussion in example 1.1 suggests that a model\nof matched pairs better describes the experimental setup for the maize data than the\ntwo-sample model of example 8.9. we parametrize the matched pair model so that\nthe jth pair of observations is\ny1 j = \u03b2 j \u2212 \u03b20 + \u03b51 j ,\n\ny2 j = \u03b2 j + \u03b20 + \u03b52 j ,\n\nj = 1, . . . ,m ,\n\n "}, {"Page_number": 378, "text": "366\n\n8 \u00b7 linear regression models\n\n\uf8f6\n\n\uf8eb\n\n\uf8f6\n\n\uf8eb\n\n\uf8f6\n\n\uf8eb\n\n\uf8f6\n\n\uf8eb\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\nwhere we assume that the \u03b5 ji are independent normal random variables with means\nzero and variances \u03c3 2. we havem = 15. the average difference between the heights\nof the crossed and self-fertilized plants in a pair is 2\u03b20, and the mean height of the\npair is \u03b2 j . the matrix form of this model is\n1 0\u00b7\u00b7\u00b7\n1 0\n0 1\u00b7\u00b7\u00b7\n0 1\n...\n...\n0 0\u00b7\u00b7\u00b7\n0 0\n\n0\n\u00b7\u00b7\u00b7 0\n0\n\u00b7\u00b7\u00b7 0\n...\n1\n\u00b7\u00b7\u00b7 1\n\n\u22121\n1\n\u22121\n1\n...\n\u22121\n1\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 +\n\nso \u03b2 has dimension (m + 1) \u00d7 1 and x t x = diag(2m, 2, . . . ,2) has dimension (m +\n1) \u00d7 (m + 1).\nwe see that(cid:17)\u03b20 = (y21 \u2212 y11 + y22 \u2212 y12 + \u00b7\u00b7\u00b7 + y2m \u2212 y1m)/(2m),\n\n\u03b511\n\u03b521\n\u03b512\n\u03b522\n...\n\u03b51m\n\u03b52m\n\ny11\ny21\ny12\ny22\n...\ny1m\ny2m\n\n\u03b20\n\u03b21\n\u03b22\n...\n\u03b2m\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n=\n\n,\n\nand that the estimators are independent. the unbiased estimator of \u03c3 2 is\n\nj = 1, . . . ,m ,\n\n(cid:17)\u03b2 j = 1\n\n2\n\n(y1 j + y2 j ),\nm(cid:15)\nj=1\n\n1\n\ns2 =\n\n2m \u2212 (m + 1)\n\n{(y1 j \u2212(cid:17)\u03b2 j +(cid:17)\u03b20)2 + (y2 j \u2212(cid:17)\u03b2 j \u2212(cid:17)\u03b20)2},\n\n(cid:18)\n\n\u22121\n\n(cid:18)\n\nd j is their average. note that(cid:17)\u03b20 equals 1\n\nwhich can be written as {2(m \u2212 1)}\u22121\n(d j \u2212 d)2, where d j = y2 j \u2212 y1 j is the dif-\nference between the heights of the crossed and self-fertilized plants in the jth pair,\nand d = m\n(cid:1)\nlikelihood ratio statistic\nthe likelihood ratio statistic is a standard tool for comparing nested models. in the\ncontext of the normal linear model, let\ny = x\u03b2 + \u03b5 = ( x1 x2 )\n\n+ \u03b5 = x1\u03b21 + x2\u03b22 + \u03b5,\n\n2 d.\n\n(cid:8)\n\n(cid:7)\n\nwhere x1 is an n \u00d7 q matrix, x2 is an n \u00d7 ( p \u2212 q) matrix, q < p, and \u03b21 and \u03b22 are\nvectors of parameters of lengths q and p \u2212 q. suppose that we wish to compare this\nwith the simpler model in which \u03b22 = 0, so the mean of y depends only on x1. under\nthe more general model the maximum likelihood estimators of \u03b2 and \u03c3 2 are (cid:17)\u03b2 and\n\u22121ss((cid:17)\u03b2), where ss(\u03b2) = (y \u2212 x\u03b2)t(y \u2212 x\u03b2), and it follows from (8.5) that\n(cid:17)\u03c3 2 = n\n\n\u03b21\n\u03b22\n\nthe maximized log likelihood is\n\n(cid:8)p((cid:17)\u03c3 2) = \u2212 1\n\n{n log ss((cid:17)\u03b2) + n \u2212 n log n},\n\n2\n\nwhere (cid:8)p(\u03c3 2) = max\u03b2 (cid:8)(\u03b2, \u03c3 2) isthe profile log likelihood for \u03c3 2. when \u03b22 = 0, the\nmaximum likelihood estimator of \u03c3 2 is\n\n(cid:17)\u03c3 2\n\n0\n\n\u22121ss((cid:17)\u03b21) = n\n\n= n\n\n\u22121(y \u2212 x1\n\n(cid:17)\u03b21)t(y \u2212 x1\n\n(cid:17)\u03b21),\n\n "}, {"Page_number": 379, "text": "8.2 \u00b7 normal linear model\nwhere(cid:17)\u03b21 is the estimator of \u03b21 when \u03b22 = 0. hence the likelihood ratio statistic for\n\n367\n\ncomparison of the models is\n\n(cid:19)\n\n2\n\n(cid:8)p((cid:17)\u03c3 2) \u2212 (cid:8)p\n\n(cid:11)(cid:17)\u03c3 2\n\n0\n\n(cid:12)(cid:20) = n log{ss((cid:17)\u03b2)/ss((cid:17)\u03b21)}\n(cid:21)\n{ss((cid:17)\u03b21) \u2212 ss((cid:17)\u03b2)}/( p \u2212 q)\n1 + p \u2212 q\n= n log\nn \u2212 p\n(cid:7)\n(cid:8)\n1 + p \u2212 q\nn \u2212 p\n\nss((cid:17)\u03b2)/(n \u2212 p)\n\n= n log\n\n,\n\n(cid:22)\n\nf\n\n(8.8)\nsay. here f \u2265 0, with equality only if the two sums of squares are equal. this event\ncan occur only if the columns of x2 are linearly dependent on those of x1. ifnot, the\nresults of section 4.5.2 imply that the likelihood ratio statistic has an approximate \u03c7 2\ndistribution, but as it is a monotonic function of f, large values of (8.8) correspond\nto large values of f. weshall see in section 8.5 that the exact distribution of f is\nknown and can be used to compare nested models, with no need for approximations.\nit is instructive to express f explicitly in terms of the least squares estima-\ntors. as (8.8) is a likelihood ratio statistic for testing \u03b22 = 0, it is invariant to 1\u20131\nreparametrizations that leave \u03b22 fixed, and we write e(y) as\n\n(cid:19)\n\n1 x1\n\n1 x1)\n\n1 x2\u03b22\n\n(cid:12)\u22121 x t\n\nx1\u03b21 + x2\u03b22 = x1\u03b21 + h1 x2\u03b22 + (i \u2212 h1)x2\u03b22\n(cid:20) + z2\u03b22\n\n\u03b21 +(cid:11)\n= x1\nx t\n= x1\u03bb + z2\u03c8,\n1 is the projection matrix for x1, z2 = (i \u2212 h1)x2\nsay, where h1 = x1(x t\n\u22121 x t\nis the matrix of residuals from regression of the columns of x2 on those of x1, and\nthe new parameters are \u03bb and \u03c8 = \u03b22. note that\n(cid:12)\u22121 x t\n(cid:19)\nx2 = 0,\n1 x1\n(cid:7)(cid:11)\n(cid:8)\n(cid:11)\n\n(cid:7)(cid:17)\u03bb(cid:17)\u03c8\n(cid:17)\u03c8)\n(cid:17)\u03c8)t(y \u2212 x1\n(cid:17)\u03bb \u2212 z2\n(cid:17)\u03bb \u2212 z2\nss((cid:17)\u03b2) = (y \u2212 x1\n(cid:17)\u03bb)t(y \u2212 x1\n(cid:17)\u03bb) \u2212 2(cid:17)\u03c8 t z t\n= (y \u2212 x1\n2(y \u2212 x1\n= ss((cid:17)\u03b21) \u2212 (cid:17)\u03c8 t z t\n(cid:17)\u03c8 ,\n\nwhile if \u03c8 = \u03b22 = 0, the least squares estimate of \u03bb remains(cid:17)\u03bb. consequently\n(cid:17)\u03c8\n\nand that h1 is idempotent. in this new parametrization the parameter estimates are\n\ni \u2212 x1\n(cid:8)\u22121(cid:7)\n\nx t\n1 z2\n1 x1 x t\nz t\nz t\n2 x1\n2 z t\n2\n\n(cid:17)\u03bb) + (cid:17)\u03c8 t z t\n\n(cid:12)\u22121 x t\n(cid:12)\u22121 z t\n\n1 z2 = x t\nx t\n\nx t\n1 x1\n2 z2\nz t\n\ny =\n\n1 y\n2 y\n\nx t\n1\nz t\n2\n\n2 z2\n\n(cid:8)\n\n(cid:8)\n\n(cid:7)\n\n=\n\n(cid:20)\n\nx t\n\n(cid:11)\n\n,\n\n1\n\n1\n\n2 z2\n\nsince\n\n(cid:17)\u03c8 t z t\n\n2(y \u2212 x1\n\n(cid:17)\u03bb) = (cid:17)\u03c8 t z t\n(cid:11)\n= (cid:17)\u03c8 t\n(cid:11)\n= (cid:17)\u03c8 t\n\n(cid:17)\u03bb\n(cid:12)\u22121 z t\n2 x1\n2 z2\nz t\n\n2 y \u2212 (cid:17)\u03c8 t z t\n(cid:12)(cid:11)\n(cid:12)(cid:17)\u03c8 .\n\n2 z2\nz t\nz t\n2 z2\n\n2 y\n\n "}, {"Page_number": 380, "text": "368\n\n8 \u00b7 linear regression models\n\nthus the f statistic in (8.8) may be written as\n\nf = n \u2212 p\np \u2212 q\n\n(cid:17)\u03b2 t\n2 x t\n\n(cid:17)\u03b22\n2(i \u2212 h1)x2\nss((cid:17)\u03b2)\n\nand this is large if(cid:17)\u03b22 differs greatly from zero.\nif \u03b22 is scalar, then p \u2212 q = 1, the matrix z t\nand f = t 2, where\n(cid:17)\u03b22 \u2212 \u03b22\n\nt =\n\n(v pps2)1/2\n\n2 z2 = x t\n\n2(i \u2212 h1)x2 = v\n\n\u22121\npp is scalar,\n\n(8.9)\n\nwith s2 = ss((cid:17)\u03b2)/(n \u2212 p) and \u03b22 = 0. thus f is a monotonic function of t 2. we\n\nshall see in section 8.3.2 that t has a tn\u2212 p distribution.\n\n8.2.4 weighted least squares\nsuppose that a normal linear model applies but that the responses have unequal\nvariances. if the variance of y j is \u03c3 2/w j , where \u03c3 2 is unknown but the w j are known\npositive quantities giving the relative precisions of the y j , the log likelihood can be\nwritten as\n\n(cid:13)\n\n(cid:8)(\u03b2, \u03c3 2) \u2261 \u2212 1\n2\n\nn log \u03c3 2 + 1\n\n\u03c3 2 (y \u2212 x\u03b2)tw (y \u2212 x\u03b2)\n\n,\n\nwhere w = diag{w1, . . . ,w n} is known as the matrix of weights. let w 1/2 =\ndiag{w 1/2\n(cid:2) = w 1/2 x. then the sum of squares\n, . . . ,w 1/2\n(cid:2)\u03b2). as this has the same form as (8.3), the es-\nmay be written as (y\ntimates of \u03b2 and \u03c3 2 are\n\n(cid:2) = w 1/2 y and x\n(cid:2) \u2212 x\n\n1\n\nn\n\n(cid:10)\n\n}, and set y\n(cid:2) \u2212 x\n(cid:2)\u03b2)t(y\n(cid:17)\u03b2 = (x\n\n(cid:2)t x\n\n(cid:2)\n\n\u22121 x\n\n(cid:2)t y\n\n)\n\n(cid:2) = (x tw x)\n\n\u22121 x tw y,\n\nand\n\ns2 = (n \u2212 p)\n= (n \u2212 p)\n\n(cid:2)\n\n(cid:2)t{i \u2212 x\n\n\u22121 y\n\u22121 x\n)\n\u22121 yt{w \u2212 w x(x tw x)\n\n(cid:2)t x\n\n(x\n\n(cid:2)t}y\n\u22121 x tw}y.\n\n(cid:2)\n\n(cid:2)\n\n(8.10)\n\n(8.11)\n\nthese are the weighted least squares estimates. this device of replacing y and x with\nw 1/2 y and w 1/2 x allows methods for unweighted least squares models to be applied\nwhen there are weights (exercise 8.2.9).\n\nexample 8.11 (grouped data) suppose that each y j is an average of a random\n\u03b2 and variance \u03c3 2, and that the\nsample of m j normal observations, each with mean x t\nj\n\u03b2 and variance \u03c3 2/m j ,\nsamples are independent of each other. then y j has mean x t\nj\nand the y j are independent. the estimates of \u03b2 and \u03c3 2 are given by (8.10) and (8.11)\nwith weights w j \u2261 m j .\n(cid:1)\n\n "}, {"Page_number": 381, "text": "8.2 \u00b7 normal linear model\n\n369\n\nweighted least squares can be extended to situations where the errors are correlated\nbut the relative correlations are known, that is, var(y) = \u03c3 2w\n\u22121, where w is known\nbut not necessarily diagonal. this is sometimes called generalized least squares. the\ncorresponding least squares estimates of \u03b2 and \u03c3 2 are given by (8.10) and (8.11).\n\nweighted least squares turns out to be of central importance in fitting nonlinear\n\nmodels, and is used extensively in chapter 10.\n\nexercises 8.2\n1 write down the linear model corresponding to a simple random sample y1, . . . , yn from\n\nthe n (\u00b5, \u03c3 2) distribution, and find the design matrix. verify that\n\n(cid:17)\u00b5 = (x t x)\n\n\u22121 x t y = y,\n\ns2 = ss((cid:17)\u03b2)/(n \u2212 p) = (n \u2212 1)\n\n(cid:15)\n\n\u22121\n\n(y j \u2212 y)2.\n\nrecall that: (i) if the\ntr(a) = (cid:18)\nmatrix a is square, then\naii ; (ii) if a\nand b are conformable,\nthen tr(ab) = tr(b a);\n(iii) \u03bb is an eigenvalue of\nthe square matrix a if\nthere exists a vector of\nunit length a such that\naa = \u03bba, and then a is an\neigenvector of a; and (iv)\na symmetric matrix a\nmay be written as e l e t,\nwhere l is a diagonal\nmatrix of the eigenvalues\nof a, and the columns of\ne are the corresponding\neigenvectors, having the\nproperty that e t = e\n\u22121.\nif the matrix is symmetric\nand positive definite, then\nall its eigenvalues are real\nand positive.\n\n2\n\n3\n\n4\n5\n\n6\n\n7\n\n8\n\n9\n\nverify the formula for s2 given in example 8.7, and show directly that its distribution is\n\u03c3 2\u03c7 2\n1 .\n\n(cid:18)\n\nthe angles of the triangle abc are measured with a and b each measured twice and c\nthree times. all the measurements are independent and unbiased with common variance\n\u03c3 2. find the least squares estimates of the angles a and b based on the seven measurements\nand calculate the variance of these estimates.\nin example 8.10, show that the unbiased estimator of \u03c3 2 is {2(m \u2212 1)}\u22121\n(d j \u2212 d)2.\nshow that if the n \u00d7 p design matrix x has rank p, the matrix h = x(x t x)\n\u22121 x t is\nsymmetric and idempotent, that is, h t = h and h 2 = h, and that tr(h) = p. show that\nin \u2212 h is symmetric and idempotent also. by considering h 2a, where a is an eigenvector\nof h, show that the eigenvalues of h equal zero or one. prove also that h has rank p.\ngive the elements of h for examples 8.9 and 8.10.\nin alinear model in which n \u2192 \u221e in such a way that (cid:17)\u03b2 p\u2212\u2192 \u03b2, show that e j\np\u2212\u2192 \u03b5 j .\nlet y j = \u03b20 + \u03b21x j + \u03b5 j with x1 = \u00b7\u00b7\u00b7 = xk = 0 and xk+1 = \u00b7\u00b7\u00b7 = xn = 1. is(cid:17)\u03b2 consis-\ngeneralize this to any finite subset of the residuals e. isthis true for the entire vector e?\ntent if n \u2192 \u221e and k = 1? if k = m, for some fixed m? ifk = n/2? which of the \u03b5 j can\nbe estimated consistently in each case?\nj ((cid:17)\u03b2,(cid:17)\u03c3 2) are positive definite.\nshow that in a normal linear model in which x has rank p, the matrices i (\u03b2, \u03c3 2) and\n(a) consider the two design matrices for example 8.4; call them x1 and x2. find the\n4 \u00d7 4 matrix a for which x1 = x2 a, and verify that it is invertible by finding its inverse.\n(b) consider the linear models y = x1\u03b2 + \u03b5 and y = x2\u03b3 + \u03b5, where x1 = x2 a, \u03b3 =\na\u03b2, and a is an invertible matrix. show that the hat matrices, fitted values, residuals, and\nsums of squares are the same for both models, and explain this in terms of the geometry\nof least squares.\n(a) consider a normal linear model y = x\u03b2 + \u03b5 where var(\u03b5) = \u03c3 2w\n\u22121, and w is a\nknown positive definite symmetric matrix. show that a inverse square root matrix w 1/2\nexists, and re-express the least squares problem in terms of y1 = w 1/2 y, x1 = w 1/2 x,\nand \u03b51 = w 1/2\u03b5. show that var(\u03b51) = \u03c3 2 in. hence find the least squares estimates, hat\nmatrix, and residual sum of squares for the weighted regression in terms of y, x, and\nw , and give the distributions of the least squares estimates of \u03b2 and the residual sum of\nsquares.\n(b) suppose that w depends on an unknown scalar parameter, \u03c1. find the profile log\nlikelihood for \u03c1, (cid:8)p(\u03c1) = max\u03b2,\u03c3 2 (cid:8)(\u03b2, \u03c3 2, \u03c1), and outline how to use a least squares\npackage to give a confidence interval for \u03c1.\n\n "}, {"Page_number": 382, "text": "370\n\n8.3 normal distribution theory\n\n8.3.1 distributions of(cid:17)\u03b2 and s2\n\n8 \u00b7 linear regression models\n\nthe derivation of the least squares estimators in the previous section rests on the\nassumption that the errors satisfy the second-order assumptions\n\ne(\u03b5 j ) = 0,\n\nvar(\u03b5 j ) = \u03c3 2,\n\ncov(\u03b5 j , \u03b5k) = 0,\n\nj (cid:6)= k,\n\n(8.12)\n\nand in addition are normal variables. as they are uncorrelated, their normality implies\nthey are independent. on setting \u03b5t = (\u03b51, . . . , \u03b5n), we have\ncov(\u03b5, \u03b5) = e(\u03b5\u03b5t) = \u03c3 2 in,\n\ne(\u03b5) = 0,\n\nwhere in is the n \u00d7 n identity matrix. the least squares estimator equals\n\u22121 x t\u03b5,\n\n\u22121 x t(x\u03b2 + \u03b5) = \u03b2 + (x t x)\n\n(cid:17)\u03b2 = (x t x)\n\n\u22121 x t y = (x t x)\n\nwhich is a linear combination of normal variables, and therefore its distribution is\nnormal. its mean vector and covariance matrix are\n\ne((cid:17)\u03b2) = \u03b2 + (x t x)\nvar((cid:17)\u03b2) = cov{\u03b2 + (x t x)\n\n\u22121 x te(\u03b5),\n\n\u22121 x t\u03b5, \u03b2 + (x t x)\n\u22121 x tcov(\u03b5, \u03b5)x(x t x)\n\u22121,\nvar((cid:17)\u03b2) = \u03c3 2(x t x)\n\n\u22121.\n\n= (x t x)\ne((cid:17)\u03b2) = \u03b2,\n\n\u22121 x t\u03b5}\n\nso\n\ntherefore(cid:17)\u03b2 is normally distributed with mean and covariance matrix given by (8.13).\nindependent of(cid:17)\u03b2. thus the key distributional results for the normal linear model are\n\nwe shall see below that the residual sum of squares has a chi-squared distribution,\n\n(8.13)\n\n(cid:17)\u03b2 \u223c n p{\u03b2, \u03c3 2(x t x)\n\n\u22121}\n\nindependent of\n\nn\u2212 p\n\n.\n\n(8.14)\n\nss((cid:17)\u03b2) \u223c \u03c3 2\u03c7 2\n\nto show that the least squares estimator and residual sum of squares are indepen-\n\ndent, note that the residuals can be written as\n\ne = (in \u2212 h)y = (in \u2212 h)(x\u03b2 + \u03b5) = (in \u2212 h)\u03b5,\n\n\u22121 x t x = x. therefore the vector e = (in \u2212 h)\u03b5 is a linear\nbecause h x = x(x t x)\ncombination of normal random variables and is itself normally distributed, with mean\nand variance matrix\n\ne(e) = e{(in \u2212 h)\u03b5} =0,\nvar(e) = var{(in \u2212 h)\u03b5} = (in \u2212 h)var(\u03b5)(in \u2212 h)t = \u03c3 2(in \u2212 h).\n\n(8.15)\n\nthe covariance between(cid:17)\u03b2 and e is\n= (x t x)\n= (x t x)\n\ncov((cid:17)\u03b2, e) = cov{\u03b2 + (x t x)\n\n\u22121 x t\u03b5, (in \u2212 h)\u03b5}\n\n\u22121 x tcov(\u03b5, \u03b5)(in \u2212 h)t\n\u22121 x t\u03c3 2 in(in \u2212 h)t = 0.\n\n "}, {"Page_number": 383, "text": "8.3 \u00b7 normal distribution theory\nas both e and(cid:17)\u03b2 are normally distributed and their covariance matrix is zero, they are\nindependent, which implies that(cid:17)\u03b2 and the residual sum of squares ss((cid:17)\u03b2) = ete are\nthe key to the distribution of ss((cid:17)\u03b2) isthe decomposition\n\nindependent.\n\n371\n\n\u03b5t\u03b5 = (y \u2212 x\u03b2)t(y \u2212 x\u03b2)\n\n= (y \u2212 x(cid:17)\u03b2 + x(cid:17)\u03b2 \u2212 x\u03b2)t(y \u2212 x(cid:17)\u03b2 + x(cid:17)\u03b2 \u2212 x\u03b2)\n= {e + x((cid:17)\u03b2 \u2212 \u03b2)}t{e + x((cid:17)\u03b2 \u2212 \u03b2)},\n\u03b5t\u03b5/\u03c3 2 = ete/\u03c3 2 + ((cid:17)\u03b2 \u2212 \u03b2)t x t x((cid:17)\u03b2 \u2212 \u03b2)/\u03c3 2,\n\nwhich leads to\n\n(8.16)\nbecause et x = yt(in \u2212 h)x = 0. the left-hand side of (8.16) is a sum of the n inde-\npendent chi-squared variables \u03b52\nn ; its moment-generating\nfunction is (1 \u2212 2t)\nj\ntion of(cid:17)\u03b2 in (8.14) that ((cid:17)\u03b2 \u2212 \u03b2)t x t x((cid:17)\u03b2 \u2212 \u03b2)/\u03c3 2 \u223c \u03c7 2\n2 . itfollows from applying (3.23) to the normal distribu-\np. ontaking moment-generating\n\n/\u03c3 2, soits distribution is \u03c7 2\n\n\u2212n/2, t < 1\n\nfunctions of both sides of (8.16) we therefore obtain\n\nt <\n\n,\n\n1\n2\n\n(1 \u2212 2t)\n\n\u2212n/2 = e{exp(tete/\u03c3 2)} \u00d7(1 \u2212 2t)\n\n\u2212 p/2,\n\nn\u2212 p. weneed only recall that\n\n\u2212(n\u2212 p)/2, showing that its distribution is \u03c7 2\n\nbecause e and (cid:17)\u03b2 are independent. therefore ete/\u03c3 2 has moment-generating func-\ntion (1 \u2212 2t)\nss((cid:17)\u03b2) = ete to establish the remaining result in (8.14): under the normal linear model,\nwe have ss((cid:17)\u03b2)/\u03c3 2 \u223c \u03c7 2\nas the distribution of ss((cid:17)\u03b2) is\u03c3 2\u03c7 2\nn\u2212 p, its mean is e{ss((cid:17)\u03b2)} =(n \u2212 p)\u03c3 2, and its\nvariance is var{ss((cid:17)\u03b2)} =2(n \u2212 p)\u03c3 4. thus\nn(cid:15)\n(y j \u2212 x j\nj=1\n\nss((cid:17)\u03b2)\nis an unbiased estimator of \u03c3 2, whereas(cid:17)\u03c3 2 = ss((cid:17)\u03b2)/n is biased.\n\n(cid:17)\u03b2)2 = 1\nn \u2212 p\n\ns2 = 1\nn \u2212 p\n\nn\u2212 p.\n\n8.3.2 confidence and prediction intervals\n\nconfidence intervals for components of \u03b2 are based on the distributions of(cid:17)\u03b2 and s2.\nunder the normal linear model the rth element of(cid:17)\u03b2 satisfies\n\nwhere vrr is the rth diagonal element of (x t x)\ndistribution is (n \u2212 p)\n\n\u22121\u03c3 2\u03c7 2\n\nn\u2212 p. therefore\n\n\u22121, and(cid:17)\u03b2 is independent of s2, whose\n\n(cid:17)\u03b2r \u223c n (\u03b2r , \u03c3 2vrr ),\n\nt =\n\n(cid:17)\u03b2r \u2212 \u03b2r\n(cid:23)\n\ns2vrr\n\n\u223c tn\u2212 p,\n\nwhich makes the connection with (8.9). a (1 \u2212 2\u03b1) confidence interval for \u03b2r is\n(cid:17)\u03b2r \u00b1 sv 1/2\nrr tn\u2212 p(\u03b1). when \u03c3 2 is known, we replace s by \u03c3 and tn\u2212 p(\u03b1) bythe normal\n\nt\u03bd (\u03b1) is the\u03b1 quantile of\nthe t\u03bd distribution.\n\nquantile z\u03b1.\n\n "}, {"Page_number": 384, "text": "372\n\n8 \u00b7 linear regression models\nimum likelihood estimator of the linear function x t+\u03b2 is x t+(cid:17)\u03b2, which has a normal\n\nsimilar reasoning gives confidence intervals for linear functions of \u03b2. the max-\n\ndistribution with mean x t+\u03b2 and variance\n\nvar(x t+(cid:17)\u03b2) = x t+var((cid:17)\u03b2)x+ = \u03c3 2x t+(x t x)\n\n\u22121x+.\n\nas s2 is independent of(cid:17)\u03b2, confidence regions for x t+\u03b2 can be based on\n\nx t+(cid:17)\u03b2 \u2212 x t+\u03b2\n\n{s2x t+(x t x)\u22121x+}1/2\n\n\u223c tn\u2212 p.\n\nj\n\nif \u03c3 2 is known, the observed s is replaced in the confidence interval by \u03c3 and quantiles\nof the t distribution are replaced by those of the normal. notice that the variance of a\n\u22121x j , and this equals \u03c3 2h j j , where h j j is the jth\n\nfitted value(cid:17)y j = x t\n\n(cid:17)\u03b2 is \u03c3 2x t\n\nj (x t x)\n\ndiagonal element of the hat matrix h.\na confidence interval for a function of parameters is different from a prediction\ninterval for a new observation, y+ = x t+\u03b2 + \u03b5+. the presence of \u03b5+ would introduce\nuncertainty about y+ even if \u03b2 was known, and a prediction interval must take this\nfrom which(cid:17)\u03b2 is estimated, we have\ninto account. if \u03b5+ is normal with mean zero and variance \u03c3 2, independent of the data\ne(x t+(cid:17)\u03b2 + \u03b5+) = x t+\u03b2,\nvar(x t+(cid:17)\u03b2 + \u03b5+) = var(x t+(cid:17)\u03b2) + var(\u03b5+) = \u03c3 2{x t+(x t x)\n\n\u22121x+ + 1}.\nwhen \u03c3 2 is unknown, therefore, a prediction interval for y+ can be based on\n\ny+ \u2212 x t+(cid:17)\u03b2\n\n[s2{1 + x t+(x t x)\u22121x+}]1/2\n\n\u223c tn\u2212 p,\n\nwith the appropriate changes if \u03c3 2 is known.\n\n16 i4. asthe residual sum of squares is ss((cid:17)\u03b2) = 43.25, n = 16 and\nexample 8.12 (cycling data) the covariance matrix for the parameter estimates\nin example 8.8 is \u03c3 2\np = 4, an estimate of \u03c3 2 is s2 = 43.25/12 = 3.604 on 12 degrees of freedom, and\neach estimate(cid:17)\u03b2r has standard error (s2/16)1/2 = 0.475.\na 0.95 confidence interval for the true value of \u03b21 is(cid:17)\u03b21 \u00b1 st12(0.025)/4, and this\nis \u22125.437 \u00b1 0.475 \u00d7 2.18 = (\u22126.47,\u22124.40) seconds, clear evidence that the time is\nshorter when the seat is higher. the change due to the effect of tyre pressure is 2(cid:17)\u03b23\nseconds, for which the standard error is 2 \u00d7 s/4 = 0.95 seconds.\na 0.95 prediction interval for a further timing y+ made with all three factors set at\ntheir higher levels would be 41.75 \u00b1 (1 + 4\n16 )1/2st12(0.025), which is (39.49, 46.01).\nthe variability introduced by \u03b5+ forms the bulk of the variability of y+, whose variance\n(cid:1)\nis five times that of the fitted value.\n\nexample 8.13 (maize data) consider the two-sample model applied to the data\nin table 1.1. if we assume that the heights of the cross-fertilized plants form a ran-\ndom sample with means \u03b20 + \u03b21, and that the heights of the self-fertilized plants\nform a random sample with height \u03b20, and that both have variance \u03c3 2, the results of\n\n "}, {"Page_number": 385, "text": "8.3 \u00b7 normal distribution theory\n\n373\n\nexample 8.9 establish that the estimates are\n\n(cid:17)\u03b20 = y0\u00b7 = 140.6, (cid:17)\u03b21 = y1\u00b7 \u2212 y0\u00b7 = 161.53 \u2212 140.6 = 20.93,\n\n+ n\n\nthat the unbiased estimate of \u03c3 2 is s2 = 553.19, and that the estimated variance of\n(cid:17)\u03b21 is s2(n\n1 ) = 73.78. as s2 has 28 degrees of freedom, a 0.95 confidence\n\u22121\n(cid:12)1/2t28(0.025) = 20.93 \u00b1 73.781/2 \u00d7 2.048 = 3.34, 38.52.\n(cid:17)\u03b21 \u00b1 s\n\ninterval for \u03b21 has limits\n\n\u22121\n0\n(cid:11)\n\n+ n\n\nn\n\n\u22121\n0\n\n\u22121\n1\n\nthis does not contain zero, and is evidence that the crossed plants are significantly\nfor the matched pairs model of example 8.10, there are m = 15 pairs, with(cid:17)\u03b20 =\ntaller than self-fertilized plants.\n10.48 and s2 = 712.36, on 2m \u2212 (m + 1) = 14 degrees of freedom. a 0.95 confidence\ninterval for \u03b20 based on this model has limits\n(cid:17)\u03b20 \u00b1 {s2/(2m)}1/2t14(0.025) = 10.48 \u00b1 (712.36/30)1/2 \u00d7 2.154 = 0.00, 20.96.\n\nthe corresponding interval for the height increase for crossed plants is an interval\nfor 2\u03b20, that is, (0.00, 41.91). this is wider than the interval for the two-sample model,\nand just contains the value zero, giving evidence that there may be no increase due to\ncross-fertilization. the increase in interval width has two causes. first, the estimate\nof \u03c3 2 for the matched pairs model equals 712.36, which is larger than the value\n553.19 for the two-sample model. second, there are only 14 degrees of freedom for\nthe matched pairs estimate of variance, and |t14(0.025)| > |t28(0.025)|, which slightly\ninflates the matched pairs confidence interval relative to the interval from the matched\n(cid:1)\nanalysis.\n\nexercises 8.3\n1\n\nthe following table gives the parameter estimates, standard errors and correlations, when\nthe model y = \u03b20 + \u03b21x1 + \u03b22x2 + \u03b23x3 + \u03b5 is fitted to the cement data of example 8.3.\nthe residual sum of squares is 48.11.\n\n(intercept)\nx1\nx2\nx3\n\nestimate\n48.19\n1.70\n0.66\n0.25\n\nse\n3.913\n0.205\n0.044\n0.185\n\ncorrelations of estimates\n\n(intercept)\n\nx1\n\nx2\n\nx1\nx2\nx3\n\n-0.736\n-0.416\n-0.828\n\n-0.203\n0.822\n\n-0.089\n\n2\n\non the assumption that this normal linear model applies, compute 0.95 confidence intervals\nfor \u03b20, \u03b21, \u03b22, and \u03b23, and test the hypothesis that \u03b23 = 0. compute a 0.90 confidence\ninterval for \u03b22 \u2212 \u03b23.\nlet(cid:17)\u03b2 be a least squares estimator, and suppose that \u03b5+ \u223c n (0, \u03c3 2) independent of(cid:17)\u03b2. ver-\nify that var(x t+(cid:17)\u03b2) = \u03c3 2x t+(x t x)\n\u22121x+}.\nassuming that a normal linear model is suitable for the cycling data, calculate a 0.90\nconfidence interval for the mean time to cycle up the hill when the three factors are at\ntheir lowest levels. obtain also a 0.90 prediction interval for a future observation made\nwith that setup.\n\n\u22121x+ and that var(x t+(cid:17)\u03b2 + \u03b5+) = \u03c3 2{1 + x t+(x t x)\n\n "}, {"Page_number": 386, "text": "374\n\n8 \u00b7 linear regression models\n\n8.4 least squares and robustness\nin section 8.2.1 we established that (cid:17)\u03b2 = (x t x)\n\u22121 x t y is the maximum likelihood\nestimator of the regression parameter \u03b2 under the assumption of normal responses.\n((cid:17)\u03b2, s2), and it follows that these are the unique minimum variance unbiased estimators\nthe model is a linear exponential family with complete minimal sufficient statistic\nshall see below that (cid:17)\u03b2 has minimum variance among all estimators linear in the\n\nof (\u03b2, \u03c3 2). it is natural to ask to what optimality properties hold more generally. we\n\nresponses y, under assumptions on the mean and variance structure of y alone. thus\nthe least squares estimator retains optimality properties even without full distributional\nassumptions. this has important generalizations, as we shall see in section 10.6.\n\nsuppose that the second-order assumptions (8.12) hold, but that the errors are\nnot necessarily normal. thus, although uncorrelated, they may be dependent. then\ne(y) = x\u03b2 and var(y) = \u03c3 2 in. let \u02dc\u03b2 denote any unbiased estimator of \u03b2 that is linear\nin y. then a p \u00d7 n matrix a exists such that \u02dc\u03b2 = ay, and unbiasedness implies that\ne( \u02dc\u03b2) = ax\u03b2 = \u03b2 for any parameter vector \u03b2; this entails ax = i p. now\n\nvar( \u02dc\u03b2) \u2212 var((cid:17)\u03b2) = a\u03c3 2 in at \u2212 \u03c3 2(x t x)\n= \u03c3 2{a at \u2212 ax(x t x)\n= \u03c3 2 a(in \u2212 h)at\n= \u03c3 2 a(in \u2212 h)(in \u2212 h)t at\n\n\u22121\n\u22121 x t at}\n\nsamples among all linear unbiased estimators of \u03b2, provided that the second-order\nassumptions hold. this result, the gauss\u2013markov theorem, gives further support for\n\nand this p \u00d7 p matrix is positive semidefinite. thus(cid:17)\u03b2 has smallest variance in finite\nusing (cid:17)\u03b2 if a linear estimator of \u03b2 is sought, though of course nonlinear estimators\nmay have smaller variance.\nexample 8.14 (student t density) suppose that y = x\u03b2 + \u03c3 \u03b5, where the \u03b5 j are\nindependent and have the student t density (3.11) with \u03bd degrees of freedom. now\nvar(\u03b5 j ) isfinite and equals \u03bd/(\u03bd \u2212 2) provided \u03bd > 2, and then the least squares\nestimator has variance matrix \u03c3 2\u03bd/(\u03bd \u2212 2) \u00d7 (x t x)\n\n\u22121.\n\nhow much efficiency is lost by using least squares rather than maximum likelihood\nestimation for \u03b2? tosee this we must compute the expected information matrix, which\ngives the inverse variance of the maximum likelihood estimator. the log likelihood\nassuming \u03bd and \u03c3 2 known is\n\nand differentiation with respect to \u03b2 gives\n\n(cid:8)(\u03b2) \u2261 \u2212 \u03bd + 1\n\n2\n\n\u2202(cid:8)(\u03b2)\n\n\u2202\u03b2\n\n\u2212 \u2202 2(cid:8)(\u03b2)\n\u2202\u03b2\u2202\u03b2 t\n\n\u03bd\u03c3 2\n\n= \u03bd + 1\n= \u03bd + 1\n\n\u03bd\u03c3 2\n\nn(cid:15)\nj=1\nn(cid:15)\nj=1\nn(cid:15)\nj=1\n\n(cid:19)\n\n1 +(cid:11)\n\nlog\n\ny j \u2212 x t\n\nj\n\n\u03b2\n\n(cid:12)2/(\u03bd\u03c3 2)\n\n(cid:20)\n\n,\n\nj\n\ny j \u2212 x t\n1 +(cid:11)\ny j \u2212 x t\n1 \u2212(cid:11)\ny j \u2212 x t\n(cid:19)\n1 +(cid:11)\ny j \u2212 x t\n\n\u03b2\n\nj\n\nj\n\n\u03b2\n\n(cid:12)2/(\u03bd\u03c3 2)\n(cid:12)2/(\u03bd\u03c3 2)\n(cid:12)2/(\u03bd\u03c3 2)\n\n\u03b2\n\n\u03b2\n\nj\n\nx j ,\n\n(cid:20)2 x j x t\n\nj\n\n.\n\nthe n \u00d7 n hat matrix\nh = x(x t x)\n\u22121 x t is\nsymmetric and idempotent\nand hence so is in \u2212 h.\n\njohann carl friedrich\ngauss (1777\u20131855) was\nborn and educated in\nbrunswick. he studied in\ng\u00a8ottingen and obtained a\ndoctorate from the\nuniversity of helmstedt.\nhis first book, published\nat the age of 24, contained\nthe largest advance in\ngeometry since the\ngreeks. he became\ndirector of the g\u00a8ottingen\nobservatory and invented\nleast squares estimation\nfor the combination of\nastronomical\nobservations, though his\nstatistical work was not\npublished until much later.\nhe also wrote treatises on\ntheoretical astronomy,\nsurveying, terrestial\nmagnetism, infinite series,\nintegration, number\ntheory, and differential\ngeometry.\n\n "}, {"Page_number": 387, "text": "8.4 \u00b7 least squares and robustness\n375\nnow e{(1 + \u03b52/\u03bd)\n\u2212r} =(\u03bd + 2r \u2212 2)\u00b7\u00b7\u00b7\u03bd/{(\u03bd + 2r \u2212 1)\u00b7\u00b7\u00b7(\u03bd + 1)}, sothe ex-\npected information for \u03b2 is \u03c3 \u22122(\u03bd + 1)/(\u03bd + 3) \u00d7 x t x. thus the maximum like-\nlihood estimator is a nonlinear function of y with large-sample variance matrix\n\u03c3 2(\u03bd + 3)/(\u03bd + 1) \u00d7 (x t x)\n\u22121. itfollows that the least squares estimator has asymp-\ntotic relative efficiency (\u03bd \u2212 2)(\u03bd + 3)/{\u03bd(\u03bd + 1)}, independent of the design matrix,\n\u03b2, or\u03c3 2. as\u03bd \u2192 \u221e, the efficiency tends to one; for \u03bd = 5, 10, and 20 it equals 0.8,\n0.95, and 0.99. maximum likelihood estimation of \u03b2 barely improves on least squares\nfor a wide range of \u03bd, because the t density is close to normal unless \u03bd is small. (cid:1)\n\nm-estimation\nthe least squares estimators have strong optimality properties, but because they are\nlinear in y, they are sensitive to outliers. when data are too extensive to be carefully\ninspected or when bad data are present, robust or resistant estimators are more ap-\npropriate. one approach to constructing them is to replace the sum of squares with a\nfunction\n\u03b2)/\u03c3 . the\nresulting estimators are called m-estimators because they are maximum-likelihood-\nlike: the function \u03c1 takes the place of a negative log likelihood. they may also be\ndefined as the solutions of the p \u00d7 1 estimating equation (section 7.2)\n\n\u03b2)/\u03c3} that downweights extreme values of (y j \u2212 x t\n\n\u03c1{(y j \u2212 x t\n\n(cid:18)\n\nj\n\nj\n\n\u03c3 \u22121\n\nx j \u03c1(cid:2)(cid:19)(cid:11)\n\nn(cid:15)\nj=1\n\n(cid:12)\n\n(cid:20) = 0,\n\ny j \u2212 x t\n\nj\n\n\u03b2\n\n/\u03c3\n\n(8.17)\n\n(8.18)\n\nwhere \u03c1(cid:2)\n\n(u) = d\u03c1(u)/du, which extends the least squares estimating equation\n\nx t(y \u2212 x\u03b2) = n(cid:15)\n\n(cid:11)\n\ny j \u2212 x t\n\nj\n\n\u03b2\n\nx j\n\n(cid:12) = 0.\n\nj=1\n\nmany functions \u03c1(u) have been proposed. setting \u03c1(u) = u2/2 gives least squares.\n\nother possibilities include \u03c1(u) = |u|, \u03c1(u) = \u03bd log(1 + u2/\u03bd)/2, and\n\n(cid:10)\n\n\u03c1(u) =\n\nu2,\nc(2|u| \u2212c ),\n\nif |u| < c,\notherwise,\n\ncorresponding to the median, a t\u03bd density, and a huber estimator (example 7.19).\nthese have the drawback that large outliers are not downweighted to zero. this can\nbe achieved with a redescending function such as the biweight,\n\n\u03c1(cid:2)\n\n(u) = u max[{1 \u2212 (u/c\n\n(cid:2)\n\n)2}2, 0];\n\n(cid:18)\n\n\u03c1{(y j \u2212 x t\n\n(cid:18)\n(cid:2) = 4.865 gives asymptotic efficiency 0.95 for normal data.\n\n\u03b2)/\u03c3} has second derivative \u03c3 \u22122\n\ntaking c\nnotice that\n\u03b2),\nwhose expectation is of form \u03c3 \u22122 x t x \u00d7 e{g\n(\u03b5)} under a model in which y j =\n\u03b2 + \u03c3 \u03b5 j and the \u03b5 j are independent and identically distributed with zero mean and\nx t\nj\nunit variance. the ideas of section 7.2 imply that the m-estimator has asymptotic\nvariance\n\n(y j \u2212 x t\n\nx j x t\nj g\n\n(cid:2)\n\n(cid:2)\n\nj\n\nj\n\n\u03c3 2(x t x)\n\n\u22121 \u00d7 e{g(\u03b5)2}/e{g\n\n(cid:2)\n\n(\u03b5)},\n\n "}, {"Page_number": 388, "text": "\u2022\u2022\n\n\u2022\u2022\n\n376\n\n4\n\n2\n\n0\n\n2\n-\n\n4\n-\n\ne\nm\n\ni\nt\n \nl\n\ni\n\na\nv\nv\nr\nu\ns\n \n\ng\no\nl\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\u2022\n\ny\n\n6\n\n4\n\n2\n\n0\n\n\u2022\n\n\u2022\n\n0.2\n\n0.6\n\n1.0\n\n1.4\n\ndose\n\n8 \u00b7 linear regression models\n\no\no\n\no\n\no\no\n\n\u2022\n\u2022\n\n\u2022\n\n2\n\n\u2022\n\u2022\n\u2022\n\n4\n\n\u2022\n\n\u2022\n\n0\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n6\n\n8 10 12 14\n\nx\n\n(\u03b5)}/e{g(\u03b5)2}. the huber\nso its efficiency relative to least squares is simply e{g\nestimator for regression has efficiencies given by the right panel of figure 7.4, for\ninstance.\n\n(cid:2)\n\nfigure 8.3 data for\nwhich least squares\nestimation fails. left: log\nsurvival proportions for\nrats given doses of\nradiation, with lines fitted\nby least squares with\n(solid) and without (dots)\nthe outlier, and a huber\nm-estimate for the entire\ndata (dashes) (efron,\n1988). right: simulated\ndata with a batch of\noutliers (circles), and fits\nby least squares to all data\n(solid), least squares to\ngood data only (large\ndash), huber (dot-dash),\nbiweight (dashes), and\nleast trimmed squares\n(medium dash). the\nhuber and biweight fits\nare the same to plotting\naccuracy.\n\nequation (8.17) may be solved using iterative versions of least squares described\nin section 10.2.2, though these may fail to converge if \u03c1 is not convex. in practice \u03c3\ntoo must be estimated, by the median absolute deviation of the residuals y j \u2212 x t\neach iteration, or using an m-estimator of scale.\n\n(cid:17)\u03b2 at\n\nj\n\ninitial values for these fits can be found by a highly resistant procedure such as least\ntrimmed squares, whereby \u03b2 is chosen to minimize\n(i); this is the sum\nof the smallest q = (cid:8)n/2(cid:9) + (cid:8)( p + 1)/2(cid:9) squared residuals, found by a monte carlo\nsearch. highly resistant procedures do not usually provide standard errors, which\ncan be obtained by a data-based simulation procedure such as the bootstrap; see the\nbibliographic notes.\n\n\u03b2)2\n\n(cid:18)q\ni=1(y j \u2212 x t\n\nj\n\nexample 8.15 (survival data) the left panel of figure 8.3 shows data on batches of\nrats given doses of radiation. they are well fit by a straight line, apart from an apparent\noutlier, which strongly affects the least squares fit \u2014 note what the pattern of residuals\nwill be. the least squares estimates of slope and its standard error with and without the\noutlier are\u22125.91 (1.05) and\u22127.79 (0.59), while huber estimation gives\u22127.02 (0.46).\ndownweighting the outlier using the robust estimator gives a result intermediate\nbetween keeping it and deleting it.\n\nthis sample is small and the outlier sticks out, so robust methods are not really\nneeded. they are more valuable for larger more complex data sets where visualization\n(cid:1)\nis difficult and outliers non-obvious.\n\nexample 8.16 (simulated data) to illustrate and compare some robust estimators,\nwe generated sets of 25 standard normal observations y with a single covariate x,\nand then added k outliers with mean 6, having the t5 distribution. the right panel\nof figure 8.3 shows one of these datasets, with k = 5. we then computed five esti-\nmates of slope, from least squares applied with and without the outliers, from huber\nand biweight m-estimators having efficiency 0.95 at the normal model, and from\n\n "}, {"Page_number": 389, "text": "8.4 \u00b7 least squares and robustness\n\n377\n\ntable 8.4 bias (standard\ndeviation) of estimators of\nslope in sample of 25\ngood data and k outliers,\nestimated from 200\nreplications.\n\nleast squares\n\nm-estimation\n\nleast trimmed\n\nk\n\nno outliers with outliers\n\nhuber\n\nbiweight\n\nsquares\n\n1\n2\n5\n10\n\n0.00 (0.07)\n0.00 (0.07)\n0.00 (0.07)\n0.00 (0.06)\n\n0.17 (0.06)\n0.26 (0.06)\n0.41 (0.05)\n0.48 (0.04)\n\n0.07 (0.07)\n0.13 (0.07)\n0.38 (0.06)\n0.48 (0.04)\n\n0.01 (0.07)\n0.02 (0.09)\n0.19 (0.19)\n0.46 (0.12)\n\n\u22120.01 (0.13)\n0.01 (0.14)\n0.01 (0.14)\n0.05 (0.20)\n\nleast trimmed squares. table 8.4 shows the bias and standard deviation of the slope\nestimators for various k, computed from 200 replicate data sets.\n\ninclusion of just one outlier ruins the least squares estimator, which is the bench-\nmark when outliers are excluded. the biweight gives the better of the m-estimators,\nbut with k \u2265 5 it isbadly biased. the m-estimators perform as badly as least squares\nwhen contamination is high. least trimmed squares is least biased overall, but is very\ninefficient even for k = 1. this suggests that a good practical data analysis strategy\nis to use an initial least trimmed squares fit to identify and delete outliers, and then\n(cid:1)\napply m-estimation to the remaining data.\n\nmisspecified variance\noutliers are just one of many possible problems in regression. suppose that although\n\ne(y) = x\u03b2, the variance is var(y) = v rather than the assumed \u03c3 2 in. then (cid:17)\u03b2 =\n\n(x t x)\n\n\u22121 x t y has variance\n\n\u22121.\n\n(x t x)\n\nif v = \u03c3 2 in, then var((cid:17)\u03b2) = \u03c3 2(x t x)\nnormal responses, while (7.20) implies that var((cid:17)\u03b2) isinflated otherwise.\n\n\u22121(x tv x)(x t x)\n(8.19)\n\u22121, which itself is the inverse fisher information\nfor \u03b2 under the normal model. thus if the variance of y is correctly supposed to equal\n\u03c3 2 in, the least squares estimator attains the cram\u00b4er\u2013rao lower bound appropriate to\n\u22121 and make no allowance for possible\nvariance misspecification. if plots such as those described in section 8.6 do not suggest\na particular variance to be fitted using weighted least squares, the weights being w =\n\u22121, then it may be better to apply least squares but to base confidence intervals on an\nestimate of (8.19). one simple possibility is to replace v with (cid:17)v = diag{r 2\nv\n},\nwhere r j = (y j \u2212(cid:17)y j )/(1 \u2212 h j j ).\n, . . . , r 2\nn\n\nmost packages use the formula \u03c3 2(x t x)\n\n1\n\nexercises 8.4\n1\n2\n\ncheck the details of example 8.14.\n\nshow that (cid:17)\u03b2 and s2 are unbiased estimators of \u03b2 and \u03c3 2 even when the errors are not\n\nnormal, provided that the second-order assumptions are satisfied.\nconsider a linear regression model (8.1) in which the errors \u03b5 j are independently\ndistributed with laplace density\n\u22121 exp\n\n, \u2212\u221e < u < \u221e, \u03c3 > 0.\n\nf (u; \u03c3 ) = (23/2\u03c3 )\n\n(cid:28)(cid:25)(cid:25)(cid:25)(cid:29)\n\n(cid:26)(cid:27)\n\n(cid:25)(cid:25)(cid:25)u\n\n21/2\u03c3\n\n\u2212\n\n(cid:24)\n\n3\n\n "}, {"Page_number": 390, "text": "378\n\n4\n\n5\n\n8 \u00b7 linear regression models\n\n(cid:18)|y j \u2212 x t\n\nj\n\n\u03b2| of y \u2212 x\u03b2.\n\nverify that this density has variance \u03c3 2. show that the maximum likelihood estimate of \u03b2\nis obtained by minimizing the l 1 norm\niid\u223c n (0, \u03c3 2), the asymptotic relative efficiency of the esti-\nshow that if in fact the \u03b5 j\nmators relative to least squares estimators is 2/\u03c0.\nconsider a linear model y j = x j \u03b2 + \u03b5 j , j = 1, . . . ,n in which the \u03b5 j are uncorrelated and\nhave means zero. find the minimum variance linear unbiased estimators of the scalar \u03b2\nwhen (i) var(\u03b5 j ) = x j \u03c3 2, and (ii) var(\u03b5 j ) = x 2\n\u03c3 2. generalize your results to the situation\nwhere var(\u03b5) = \u03c3 2/w j , where the weights w j are known but \u03c3 2 is not.\nuse (8.18) to establish that (7.20) takes form\n\u22121 x tv x(x t x)\n\n\u22121 \u2265 \u03c3 2(x t x)\nwhen var(y) iswrongly supposed equal to \u03b52 in instead of v .\n\n(x t x)\n\n\u22121\n\nj\n\n8.5 analysis of variance\n8.5.1 f statistics\nin most regression models a key question is whether or not the explanatory variables\naffect the response. for example, in the bicycle data, we were concerned how the\ntime to climb the hill depended on the seat height and other factors. ockham\u2019s razor\nsuggests that we use the simplest model we can. this poses the question: which\nexplanatory variables are needed? to be concrete, suppose that we fit a normal linear\nmodel\n\ny = x\u03b2 + \u03b5 = (x1, x2)\n\n+ \u03b5 = x1\u03b21 + x2\u03b22 + \u03b5,\n\n(8.20)\nwhere x1 is an n \u00d7 q matrix, x2 is an n \u00d7 ( p \u2212 q) matrix, q < p, and \u03b21 and \u03b22 are\nvectors with respective lengths q and p \u2212 q. wesuppose that x has rank p and x1\nhas rank q. the explanatory variables x2 are unnecessary if \u03b22 = 0, in which case\nthe simpler model y = x1\u03b21 + \u03b5 holds. how can we detect this?\nin figure 8.2, let the line x = 0 inthe horizontal plane through the origin rep-\nresent the linear subspace spanned by the columns of x1. the fitted value (cid:17)y1 =\nresiduals, y \u2212(cid:17)y1 = {in \u2212 x1(x t\n1 y is the orthogonal projection of y onto this subspace. the vector of\n}y, resolves into the two orthogonal vectors\ny \u2212(cid:17)y and(cid:17)y \u2212(cid:17)y1; that is,\n\n\u22121 x t\n\n\u22121 x t\n\nx1(x t\n\n1 x1)\n\n1 x1)\n\n1\n\n(cid:7)\n\n(cid:8)\n\n\u03b21\n\u03b22\n\ny \u2212(cid:17)y1 = (y \u2212(cid:17)y) + ((cid:17)y \u2212(cid:17)y1),\n\nwhere (y \u2212(cid:17)y)t((cid:17)y \u2212(cid:17)y1) = 0. these vectors are the residual from the more complex\nmodel, y \u2212(cid:17)y, and the change in fitted values when x2 is added to the design matrix,\n(cid:17)y \u2212(cid:17)y1. asthese vectors are orthogonal linear functions of the normally distributed\n\nvector y, they are independent. pythagoras\u2019 theorem implies that\n\n(y \u2212(cid:17)y1)t(y \u2212(cid:17)y1) = (y \u2212(cid:17)y)t(y \u2212(cid:17)y) + ((cid:17)y \u2212(cid:17)y1)t((cid:17)y \u2212(cid:17)y1),\n\nor equivalently\n\nss((cid:17)\u03b21) = ss((cid:17)\u03b2) + {ss((cid:17)\u03b21) \u2212 ss((cid:17)\u03b2)}.\n\n(8.21)\n\n "}, {"Page_number": 391, "text": "379\n\nthus the residual sum of squares for the simpler model is the sum of two independently\n\nand the reduction in sum of squares when the columns of x2 are added to the design\n\n8.5 \u00b7 analysis of variance\ndistributed parts: the residual sum of squares for the more elaborate model, ss((cid:17)\u03b2),\nmatrix, ss((cid:17)\u03b21) \u2212 ss((cid:17)\u03b2).\nparticular value zero. in this case ss((cid:17)\u03b21) has a \u03c3 2\u03c7 2\nthat when \u03b22 = 0, ss((cid:17)\u03b21) \u2212 ss((cid:17)\u03b2) has a \u03c3 2\u03c7 2\nss((cid:17)\u03b2)/(n \u2212 p)\n\nn\u2212q distribution, and ss((cid:17)\u03b2) has a\nn\u2212 p distribution. since ss((cid:17)\u03b21) \u2212 ss((cid:17)\u03b2) isindependent of ss((cid:17)\u03b2), (8.21) implies\n\nf = {ss((cid:17)\u03b21) \u2212 ss((cid:17)\u03b2)}/( p \u2212 q)\n\nif the submodel is correct, so too is the more elaborate model, because \u03b22 takes the\n\np\u2212q distribution, and that\n\n\u223c fp\u2212q,n\u2212 p;\n\n\u03c3 2\u03c7 2\n\nrecall (8.8). if \u03b22 is non-zero, the reduction in sum of squares due to including the\ncolumns of x2 in the design matrix will be larger on average than if \u03b22 = 0. thus if\n\u03b22 (cid:6)= 0, f will tend to be large relative to the fp\u2212q,n\u2212 p distribution. we can therefore\ntest the adequacy of the simpler model using the statistic f, large values of which\nsuggest that \u03b22 (cid:6)= 0.\nexercise 8.5.3 gives the algebraic equivalent of the geometric argument above. as\nwe saw in section 8.2.3, f arises from the likelihood ratio statistic for comparison\nof the two models. when x2 consists of a single covariate, \u03b22 is scalar, and tests\nand confidence intervals for it may be obtained by fitting the more elaborate model\nrr ). here s2 is the estimate of \u03c3 2 from the\nmore elaborate model, and the null distribution of t is tn\u2212 p. inthis situation there is\n\n(8.20) and calculating t = ((cid:17)\u03b22 \u2212 \u03b22)/(sv 1/2\na simple connection to f: when testing \u03b22 = 0, f = t 2 =(cid:17)\u03b22\nsuppose that we want to compare the models y =\nexample 8.17 (cement data)\n\u03b20 + x1\u03b21 + \u03b5 and y = \u03b20 + x1\u03b21 + x2\u03b22 + x3\u03b23 + x4\u03b24 + \u03b5. this corresponds to\nasking if is there any effect on y of x2, x3, or x4, after allowing for the effect of x1.\nhere x1 is a 13 \u00d7 2 matrix whose columns are a vector of ones and x1, and x2 is a\n13 \u00d7 3 matrix whose columns are x2, x3, and x4; both matrices have full rank.\nfor the full model p = 5 and the residual sum of squares is ss((cid:17)\u03b2) = 47.86, and\nfor the simpler model q = 2 and the residual sum of squares is ss((cid:17)\u03b21) = 1265.7.\nthus the reduction in sum of squares due to the columns of x2 after fitting x1 is\n1265.7 \u2212 47.86 = 1217.84 on three degrees of freedom. to test whether this is a\nsignificant reduction, we compute\n\n/(s2vrr ).\n\n2\n\nf = (1265.7 \u2212 47.86)/(5 \u2212 2)\n\n47.86/(13 \u2212 5)\n\n= 67.86,\n\nf\u03bd1 ,\u03bd2 (\u03b1) is the\u03b1 quantile\nof the f distribution with\n\u03bd1 and \u03bd2 degrees of\nfreedom.\n\nwhich would be consistent with an f3,8 distribution if the simpler model was adequate.\nas f greatly exceeds f3,8(0.95) = 4.066, there is strong evidence that there are effects\nof the added covariates.\n\nhaving established that adding extra covariates helps to explain the overall varia-\ntion, it is natural to ask whether this is due to a subset of them rather than to all three.\nis there a more informative decomposition of the sum of squares due to adding x2?\n(cid:1)\n\n "}, {"Page_number": 392, "text": "380\n\n8 \u00b7 linear regression models\n\n8.5.2 sums of squares\nthe interpretation of sums of squares is most useful if they can be decomposed into\nthe reductions from successively adding different explanatory variables to the design\nmatrix.\n\nsuppose that we have a normal linear model\n\ny = 1n\u03b20 + x1\u03b21 + x2\u03b22 + \u00b7\u00b7\u00b7 + xm \u03b2m + \u03b5,\n\n(8.22)\n\nwhere we call the matrices 1n, x1, x2, and so forth terms; the constant term 1n\nis a column of n ones. usually the simplest model that might be considered sets\n\ny = 1n\u03b20 + \u03b5, inwhich case the fitted value is (cid:17)y0 = 1n y, and the residual sum of\nsquares is ss0 = (cid:18)\nterms x1, x2, and so forth to the design matrix. let(cid:17)yr be the fitted value when the\n\n(y j \u2212 y)2 with \u03bd0 = n \u2212 1 degrees of freedom.\n\nwe now consider the successive reductions in sum of squares due to adding the\n\nterms x1, . . . , xr are included, and write\n\ny \u2212(cid:17)y0 = (y \u2212(cid:17)ym) + ((cid:17)ym \u2212(cid:17)ym\u22121) + \u00b7\u00b7\u00b7 +((cid:17)y1 \u2212(cid:17)y0).\n\nthis decomposition extends that leading to (8.21) and shown in figure 8.2. the\ngeometry of least squares implies that the quantities in parentheses on the right are\n\nmutually orthogonal. pythagoras\u2019 theorem tells us that (y \u2212(cid:17)y0)t(y \u2212(cid:17)y0) equals\n(y \u2212(cid:17)ym)t(y \u2212(cid:17)ym) + ((cid:17)ym \u2212(cid:17)ym\u22121)t((cid:17)ym \u2212(cid:17)ym\u22121) + \u00b7\u00b7\u00b7 + ((cid:17)y1 \u2212(cid:17)y0)t((cid:17)y1 \u2212(cid:17)y0),\n\nor equivalently\n\nss0 = ssm + (ssm\u22121 \u2212 ssm) + \u00b7\u00b7\u00b7 +( ss0 \u2212 ss1),\n\n(8.23)\n\nwhere ssr denotes the residual sum of squares that corresponds to the fitted value\n\nresidual sum of squares due to adding the term xr when the model already contains\n\n(cid:17)yr , on\u03bd r degrees of freedom. in (8.23) the difference ssr\u22121 \u2212 ssr is the reduction in\n1n, x1, . . . , xr\u22121. as y is normal and the vectors(cid:17)yr \u2212(cid:17)yr\u22121 and y \u2212(cid:17)ym are all linear\nfunctions of the data, the geometry of least squares implies that ssm and all the\nssr\u22121 \u2212 ssr are mutually independent.\nas more terms are successively added to the model, the degrees of freedom of\nthe residual sums of squares decrease, that is, \u03bd0 \u2265 \u03bd1 \u2265 \u00b7\u00b7\u00b7 \u2265\u03bd m, with \u03bdr = \u03bdr+1\n1n, x1, . . . , xr . if\u03bd r = \u03bdr+1, (cid:17)yr =(cid:17)yr+1, and ssr = ssr+1. the term xr+1 is then\nwhen the columns of xr+1 are a linear combination of the columns of the matrices\n\nredundant, because its inclusion does not change the fitted model.\n\nanalysis of variance\nthe sums of squares can be laid out in an analysis of variance table. the prototype\nis table 8.5. the residual sums of squares decrease as terms are added successively\nto the model. often the three leftmost columns are omitted and their bottom row is\nplaced under the right-hand columns; ssm is used to compute the denominator for\nthe f statistics for inclusion of x1, x2 and so forth, and these may be included also,\nas in the examples below.\n\n "}, {"Page_number": 393, "text": "table 8.5 analysis of\nvariance table.\n\ntable 8.6 analysis of\nvariance table for the\ncement data, showing\nreductions in overall sum\nof squares when terms are\nentered in the order given.\n\ntable 8.7 models for\nthe means of the crossed\nand self-fertilized plants\nin the pth pot and jth pair\nfor the maize data.\n\n8.5 \u00b7 analysis of variance\n\n381\n\nterms\n\ndf\n\nsum of squares\n\nresidual\n\nterms\nadded\n\ndf\n\nreduction in\nsum of squares mean square\n\n1n\n\n1n , x1\n\n1n , x1, x2\n\n...\n\n1n , x1, . . . , xm\n\nn \u2212 1\n\u03bd1\n\u03bd2\n...\n\u03bdm\n\nss0\nss1\nss2\n...\nssm\n\nx1\nx2\n...\nxm\n\nn \u2212 1 \u2212 \u03bd1\n\u03bd1 \u2212 \u03bd2\n\n...\n\n\u03bdm\u22121 \u2212 \u03bdm\n\nss0 \u2212 ss1\nss1 \u2212 ss2\n\n...\n\nssm\u22121 \u2212 ssm\n\nss0\u2212ss1\nn\u22121\u2212\u03bd1\nss1\u2212ss2\n\u03bd1\u2212\u03bd2\n...\n\nssm\u22121\u2212ssm\n\u03bdm\u22121\u2212\u03bdm\n\nterm\n\ndf\n\nreduction in\nsum of squares mean square\n\nf\n\nx1\nx2\nx3\nx4\n\nresidual\n\n1\n1\n1\n1\n\n8\n\n1450.1\n1207.8\n9.79\n0.25\n\n1450.1\n1207.8\n9.79\n0.25\n\n242.5\n202.0\n1.64\n0.04\n\n47.86\n\n5.98\n\nterms\n\ncrossed\n\nself-fertilized\n\n1\n1+fertilization\n1+fertilization+pot\n1+fertilization+pot+pair\n\n\u00b5\n\u00b5 + \u03b1\n\u00b5 + \u03b1 + \u03b2 p\n\u00b5 + \u03b1 + \u03b2 p + \u03b3 j\n\n\u00b5\n\u00b5\n\n\u00b5 + \u03b2 p\n\u00b5 + \u03b2 p + \u03b3 j\n\nexample 8.18 (cement data) table 8.6 gives the analysis of variance when the\ncovariates x1, x2, x3, and x4 are successively included in the design matrix. there are\nvery large reductions due to fitting x1 and x2, but those due to x3 and x4 are smaller.\nthe f statistics for testing the effects of x1 and x2 are highly significant, but once x1\nand x2 are included the f statistic for x3 is not large compared to the f1,8 distribution.\na similar conclusion holds for x4. thus once x1 and x2 are included, x3 and x4 are\n(cid:1)\nunnecessary in accounting for the response variation.\n\nexample 8.19 (maize data) consider models for the maize data with means as in\ntable 8.7. in order, these correspond to: no differences among pairs and no difference\nbetween cross-fertilization and self-fertilization; no differences among pairs but an\neffect of fertilization type; differences among the pots and an effect of fertilization\ntype; and differences among the pots and among the pairs and an effect of fertil-\nization type. table 8.8 gives the analysis of variance when these models are fitted\nsuccessively.\n\n "}, {"Page_number": 394, "text": "382\n\n8 \u00b7 linear regression models\n\nterm\n\nfertilization\npot\npair\n\nresidual\n\ndf\n\n1\n3\n11\n\n14\n\nreduction in\nsum of squares mean square\n\n3286.5\n1053.6\n4467.3\n\n9972.5\n\n3286.5\n351.2\n406.1\n\n712.3\n\nf\n\n4.61\n0.49\n0.57\n\ntable 8.8 analysis of\nvariance table for linear\nmodels fitted to the maize\ndata.\n\nthere are four pot parameters \u03b2 p, but the reduction in degrees of freedom when the\npots term is included is three because although the corresponding 30 \u00d7 4 matrix has\nrank four, its columns sum to a column of ones. as the design matrix already contains\na column of ones, including the four columns for the pots term increases the rank of\nthe design matrix by only three. likewise only 11 columns of the 30 \u00d7 15 matrix of\nterms for pairs increase the rank of a design matrix that already contains the overall\nmean and the pots term: the remaining four columns are linear combinations of those\nalready present.\nthe residual sum of squares for the eventual model is 9972.5 on 14 degrees of\nfreedom, so the denominator for f statistics is 9972.5/14 = 712.3. the f statistic\nfor fertilization is just significant at the 5% level, but there seem to be no differences\namong pots or pairs. we can attribute to random variation the reduction in sum of\nsquares when the pots and pairs terms are added, and obtain a better estimate of \u03c3 2,\nnamely\n\n(9972.5 + 1053.6 + 4467.3)/(14 + 3 + 11) = 553.3\n\non 28 degrees of freedom. the f statistic for fertilization with this pooled estimate of\n\u03c3 2 as denominator is 5.94 on 1 and 28 degrees of freedom and its significance level\nis 0.02, so the addition of the sums of squares for pots and pairs to the residual has\n(cid:1)\nresulted in a more sensitive analysis.\n\n8.5.3 orthogonality\nthe reduction in sum of squares when a term is added depends on the terms already\nin the model. this can obscure the interpretation of an analysis of variance, if a term\nthat gives a large reduction early in a sequence of fits gives a small reduction if fitted\nlater in the sequence instead.\n\nsuppose that a normal linear model (8.22) applies. the reductions in sum of squares\ndue to the terms xr are unique only if the vector spaces spanned by the columns of\nr xs = 0 when r (cid:6)= s. suppose that this\nthe xr are all mutually orthogonal, that is, x t\nis true, that in addition x t\n\nr 1n = 0, and that\ny = 1n\u03b20 + x1\u03b21 + x2\u03b22 + \u03b5.\n\n(8.24)\n\n "}, {"Page_number": 395, "text": "then the orthogonality of 1n, x1, and x2 implies that the least squares estimators are\n\n383\n\n\uf8eb\n\uf8ed\n\n\uf8eb\n\uf8ed 1t1\n\n8.5 \u00b7 analysis of variance\n\uf8f6\n\uf8f6\n(cid:17)\u03b20(cid:17)\u03b21(cid:17)\u03b22\n\uf8f8\u22121\n\uf8f8 =\nso that (cid:17)\u03b20 = y, (cid:17)\u03b21 = (x t\n1 y, and (cid:17)\u03b22 = (x t\nyt y \u2212(cid:17)\u03b2 t x t x(cid:17)\u03b2 = yt y \u2212 ny2 \u2212(cid:17)\u03b2 t\n\n0\n0\nx t\n2 x2\n\n0\nx t\n1 x1\n0\n\n\u22121 x t\n\nsquares\n\n1 x1)\n\n0\n0\n\n2 x2)\n\n2 y, with residual sum of\n\n( 1 x1 x2 )t y,\n\n\u22121 x t\n(cid:17)\u03b21 \u2212(cid:17)\u03b2 t\n\n(cid:17)\u03b22.\n\n1 x t\n\n1 x1\n\n2 x t\n\n2 x2\n\n(8.25)\n\nfor the simpler models\ny = 1n\u03b20 + \u03b5,\n\ny = 1n\u03b20 + x1\u03b21 + \u03b5\na similar calculation gives residual sums of squares\n(cid:17)\u03b21,\n\ny = 1n\u03b20 + x2\u03b22 + \u03b5,\n(cid:17)\u03b22,\nyt y \u2212 ny2 \u2212(cid:17)\u03b2 t\nand comparison with (8.25) shows that the reductions due to x1 and x2 are(cid:17)\u03b2 t\nand (cid:17)\u03b2 t\n\n(cid:17)\u03b21\n(cid:17)\u03b22 whether or not the other has been included in the design matrix.\n\nyt y \u2212 ny2 \u2212(cid:17)\u03b2 t\n\nyt y \u2212 ny2,\n\n1 x t\n\n1 x1\n\n2 x t\n\n1 x t\n\n1 x1\n\n2 x2\n\n2 x t\n\n2 x2\n\nconsequently the reductions in sums of squares due to x1 and x2 are unique. this\nargument readily extends to models with more than two mutually orthogonal terms\nxr . in fact (8.24) has three, as we see by writing 1n = x0.\nexample 8.20 (orthogonal polynomials) consider a normal linear model with\ndesign matrix\n\nx = (1n, x1, x2, x3, x4) =\n\n\uf8eb\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n2 \u22121\n\n1 \u22122\n1\n1 \u22121 \u22121\n2 \u22124\n0 \u22122\n1\n6\n0\n1 \u22121 \u22122 \u22124\n1\n1\n2\n1\n\n2\n\n1\n\n\uf8f6\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 ,\n\nthe last four columns of which correspond to linear, quadratic, cubic, and quartic\npolynomials in a covariate with five values equally spaced one unit apart. the columns\nof x are mutually orthogonal, and it follows that the reduction due to any of them\ndoes not depend on which of the others have already been fitted.\nif the values had been equally-spaced but \u03b4 units apart, the model would be\ny = 1n\u03b20 + \u03b4x1\u03b21 + \u00b7\u00b7\u00b7 +\u03b4 4x4\u03b24 + \u03b5, and the orthogonality of the terms would be\n(cid:1)\nunaffected.\n\nthe argument leading to (8.25) rarely applies directly, but it may do so if an\noverall mean, corresponding to a column of ones in the design matrix, is fitted first.\nsuppose that the matrices x1 and x2 in (8.24) are not mutually orthogonal and are\nnot orthogonal to 1n, but that we rewrite the model as\n\n(cid:11)\n\n(cid:12) +(cid:11)\ny = 1n\n= 1n\u03b30 + z1\u03b21 + z2\u03b22 + \u03b5,\n\n\u03b20 + x t\n\n\u03b21 + x t\n\n\u03b22\n\n1\n\n2\n\n(cid:12)\n\n\u03b21 +(cid:11)\n\nx1 \u2212 1nx t\n\n1\n\n(cid:12)\n\nx2 \u2212 1nx t\n\n2\n\n\u03b22 + \u03b5\n\n "}, {"Page_number": 396, "text": "384\n\n8 \u00b7 linear regression models\n\n1 and x t\n\nsay, where x t\n2 are the averages of the rows of x1 and x2. then z1 and z2\n11n = z t\n21n = 0. this rearrangement of the model changes the\nare centred and z t\nintercept but leaves \u03b21 and \u03b22 unaffected. if the original matrices x1 and x2 are such\n1 z2 = 0, we can apply the argument leading to (8.25) to our new model, to\nthat z t\nobtain the successive residual sums of squares\n\nss0 = yt y \u2212 ny2,\nss1 = yt y \u2212 ny2 \u2212(cid:17)\u03b2 t\nss2 = yt y \u2212 ny2 \u2212(cid:17)\u03b2 t\n\n(cid:17)\u03b21,\n(cid:17)\u03b21 \u2212(cid:17)\u03b2 t\n\n1 z t\n1 z t\n\n1 z1\n1 z1\n\n(cid:17)\u03b22,\n\n2 z t\n\n2 z2\n\nas the terms z1 and z2, orequivalently x1 and x2, are added to the design matrix.\nsince z1 is defined purely in terms of x1 and 1n, and z2 is defined purely in terms\nof x2 and 1n, the reduction in sum of squares due to adding x1 after including the\nconstant column 1n in the design matrix is the same whether or not x2 is present.\nhence provided the constant is fitted first, the reductions in sum of squares due to\nx1 and x2 are independent of the order in which they are included. this argument\nextends to models with more than two xr , provided that the centred matrices zr are\nmutually orthogonal.\nexample 8.21 (3 \u00d7 2 layout)\nand their means can be written\ny12\ny22\ny32\n\nin a 3 \u00d7 2 layout with no interaction the observations\n\n\u00b5 + \u03b41 \u00b5 + \u03b41 + \u03b1\n\u00b5 + \u03b42 \u00b5 + \u03b42 + \u03b1\n\n\u00b5 + \u03b1\n\ny11\ny21\ny31\n\n\u00b5\n\n,\n\n.\n\nin terms of the parameter vector (\u00b5, \u03b1, \u03b42, \u03b43)t, the design matrix is\n\n\uf8eb\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\nx =\n\n\uf8f6\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 ,\n\n1 0\n1 1\n1 0\n1 1\n1 0\n1 1\n\n0\n0\n1\n1\n0\n0\n\n0\n0\n0\n0\n1\n1\n\nwith x1 the second column of x, and x2 the third and fourth columns of x. evidently\nx1 and x2 are not orthogonal and they are not orthogonal to the constant. on the\nother hand z1 and z2 in the corresponding centred matrix,\n\n\uf8eb\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n2\n1\n2\n\n1 \u2212 1\n1\n1 \u2212 1\n1\n1 \u2212 1\n1\n\n2\n1\n2\n\n2\n1\n2\n\n\uf8f6\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 ,\n\n3\n\n\u2212 1\n\u2212 1\n\u2212 1\n\u2212 1\n\n3\n\n3\n\n3\n2\n3\n2\n3\n\n\u2212 1\n\u2212 1\n\n3\n\n3\n2\n3\n2\n3\n\n\u2212 1\n\u2212 1\n\n3\n\n3\n\nare orthogonal to the constant by construction and to each other because the design is\nbalanced: \u03b42 and \u03b43 each occur equally often with \u03b1 and without \u03b1. this balance has\nthe consequence that provided that \u00b5 is fitted first, the reductions in sums of squares\n(cid:1)\ndue to x1 and x2, orequivalently z1 and z2, are unique.\n\n "}, {"Page_number": 397, "text": "8.5 \u00b7 analysis of variance\n\n385\n\na designed experiment such as example 8.21 can often be balanced, so that or-\nthogonality is arranged, at least approximately, and the interpretation of its analysis of\nvariance is relatively clear-cut. even if the terms are not orthogonal, however, it may\nbe possible to order them unambiguously. one example is polynomial dependence of\ny on x, where terms of increasing degree are added successively. another example\nis when some terms represent classifications that are known to affect y but which are\nof secondary importance, and others correspond to the question of primary interest.\nfor instance, it would be natural to assess the effects of different treatments on the\nincidence of heart disease after taking into account the effects of classifying variables\nsuch as age, sex, and previous medical history.\n\nexercises 8.5\n1\n\nconsider the cement data of example 8.3, where n = 13. the residual sums of squares\nfor all models that include an intercept are given below.\n\nmodel\n\nss model\n\nss\n\nmodel\n\nss\n\n\u2013 \u2013 \u2013 \u2013\n1 \u2013 \u2013 \u2013\n\u2013 2 \u2013 \u2013\n\u2013 \u2013 3 \u2013\n\u2013 \u2013 \u2013 4\n\n2715.8\n1265.7\n906.3\n1939.4\n883.9\n\n1 2 \u2013 \u2013\n1 \u2013 3 \u2013\n1 \u2013 \u2013 4\n\u2013 2 3 \u2013\n\u2013 2 \u2013 4\n\u2013 \u2013 3 4\n\n57.9\n1227.1\n74.8\n415.4\n868.9\n175.7\n\n1 2 3 \u2013\n1 2 \u2013 4\n1 \u2013 3 4\n\u2013 2 3 4\n\n48.11\n47.97\n50.84\n73.81\n\n1 2 3 4\n\n47.86\n\ncompute the analysis of variance table when x4, x3, x2, and x1 are fitted in that order, and\ntest which of them should be included in the model. are your conclusions the same as in\nexample 8.18?\n(a) let a, b, c, and d represent p \u00d7 p, p \u00d7 q, q \u00d7 q, and q \u00d7 p matrices respectively.\nshow that provided that the necessary inverses exist\n\n(a + bc d)\n\n\u22121 = a\n\n(b) if the matrix a is partitioned as\n\n\u22121 b(c\n\n\u22121 b)\n\n\u22121 d a\n\n\u22121.\n\n\u22121 \u2212 a\n(cid:27)\n\na =\n\n\u22121 + d a\n(cid:28)\n\na11 a12\na21 a22\n\n,\n\nand the necessary inverses exist, show that the elements of the corresponding partition of\n\u22121 are\na\n\na11 = (cid:11)\na12 = \u2212a\n\na11 \u2212 a12 a\n\u22121\n11 a12 a22,\n\n\u22121\n22 a21\n\na22 = (cid:11)\n\n(cid:12)\u22121 ,\na21 = \u2212a\n\n\u22121\n22 a21 a11.\n\na22 \u2212 a21 a\n\n\u22121\n11 a12\n\n(cid:12)\u22121 ,\n\n\u22121 x t, p = in \u2212 h, h1 = x1(x t\n\nin (8.20), suppose that x1 and x2 have ranks q and p \u2212 q respectively, and define h =\n1 and p1 = in \u2212 h1. let(cid:17)y = h y, and\n(cid:17)y1 = h1 y.\nx(x t x)\n(a) show that (y \u2212(cid:17)y)t((cid:17)y \u2212(cid:17)y1) = 0 ifand only if h h1 = h1, and show that h1 h = h h1.\ngive a geometrical interpretation of the equations h1 h = h h1 = h1.\n\n\u22121 x t\n\n1 x1)\n\n2\n\n3\n\nuse the previous exercise.\n\n "}, {"Page_number": 398, "text": "386\n\n8 \u00b7 linear regression models\n\nmodel\n\nss\n\nmodel\n\nss\n\nmodel\n\nss\n\nmodel\n\nss\n\ntable 8.9 sums of\nsquares for models fitted\nto maize data.\n\n\u2014 \u2014 \u2014 18780 \u2014 po \u2014 17726\nf \u2014 \u2014 15493 \u2014 \u2014 pa\n\n13259 \u2014 po pa\n\nf po \u2014 14440\n13259\n\nf \u2014 pa\nf po pa\n\n9972\n9972\n\n(b) show that(cid:11)\n\nx t\n\n(cid:12)\u22121 = (cid:11)\n(cid:12)\u22121 x t\n\nx t\n\n1 p2 x1\n(cid:11)\n\n(cid:12)\u22121 \u2212 h1 x2\n\n(cid:11)\n\nx t\n\n2 p1 x2\n\n(cid:12)\u22121 x t\n\n1 x1\n\n(cid:11)\n\nx t\n\n1 x1\n\n2 x1\n(cid:11)\n\n(cid:12)\u22121 .\n(cid:12)\u22121 x t\n\n1\n\nx t\n\n1 p2 x1\n\n\u2212 h1 x2\n\n(cid:12)\u22121 x t\n\n(c) show that\nh = x1\n\n(cid:11)\nx t\n(d) use (b) and (c) to show that h h1 = h1.\nunder what two circumstances might one of the reductions in residual sum of squares\nssr \u2212 ssr+1 in an analysis of variance table for a normal linear model equal zero? does\nthe more probable of these occur when the columns of either of the design matrices below\nare included successively in their models:\n\n+ x2\n\n2 p1 x2\n\n2 p1 x2\n\n2 p1.\n\nx t\n\n2\n\n\uf8eb\n\uf8ec\uf8ed 1 1 0 0\n\n1 1 0 1\n1 0 1 0\n1 0 1 1\n\n\uf8f6\n\uf8f7\uf8f8 ,\n\n(a)\n\n(b)\n\n\uf8eb\n\uf8ec\uf8ed 1 1\n\n1 1\n1 0\n1 0\n\n\uf8f6\n\uf8f7\uf8f8?\n\n1 1\n0 0\n1 0\n0 1\n\nsuppose that the maize data consisted of three pots each containing two pairs of plants,\n12 plants in all. using the parametrization in example 8.19, write out the 12 \u00d7 11 design\nmatrix whose first two columns are terms for the overall mean and for cross-fertilization,\nwhose next three columns are the pots term, and whose last six columns are the pairs term.\nsay what the degrees of freedom for the four models in example 8.19 would then be, and\nhence give the degrees of freedom in the analysis of variance table.\nthe residual sums of squares in example 8.19 are given in table 8.9. for which of the\nterms are the reductions in residual sum of squares independent of the order of fitting?\nexplain why adding the pots term to a model that already contains the pairs term does\nnot reduce the sum of squares, even if fertilization is not included.\nverify that the columns of the design matrix in example 8.20 are orthogonal. use gram\u2013\nschmidt orthogonalization to derive the corresponding matrices for two, three, and four\nobservations.\nverify that 1n, z1, and z2 in example 8.21 are orthogonal. show that if one of the rows\nof the original design matrix is missing, the zr are not orthogonal.\n\n4\n\n5\n\n6\n\n7\n\n8\n\n8.6 model checking\n8.6.1 residuals\ndiscrepancies between data and a regression model may be isolated or systematic, or\nboth. one type of isolated discrepancy is when there are outliers: a few observations\nthat are unusual relative to the rest. systematic discrepancies arise, for example, when\na transformation of the response or a covariate is needed, when correlated errors\nare supposed independent, or when a term is incorrectly omitted. there are many\n\n "}, {"Page_number": 399, "text": "8.6 \u00b7 model checking\n\n387\n\ntechniques for detecting such problems. graphs are widely used, often supplemented\nby more formal methods that sharpen their interpretation.\n\nthe assumptions underlying the linear regression model (8.1) are:\n\nr linearity \u2014 the response depends linearly on each explanatory variable and on\n\nthe error, with no systematic dependence on any omitted terms;\n\nr constant variance \u2014 the responses have equal variances, which in particular\n\ndo not depend on the level of the response;\n\nr independence \u2014 the errors are uncorrelated, and independent if normal; and\n\nsometimes\n\nr normality \u2014 inthe normal linear model the errors are normally distributed.\n\nmany graphical methods for checking these assumptions are based on the raw resid-\n\nuals, e = y \u2212(cid:17)y. these are estimates of the unobserved errors \u03b5, with mean vector\n\nand variance matrix\n\ne(e) = 0,\n\nvar(e) = \u03c3 2(in \u2212 h),\n\u22121 x t. the covariance of two different residuals,\nwhere h is the hat matrix x(x t x)\ne j and ek, equals \u2212\u03c3 2h jk, so ingeneral the residuals are correlated.\na difficulty in direct comparison of the e j is that their variances, \u03c3 2(1 \u2212 h j j ), are\nusually unequal. we therefore construct standardized residuals\n\nr j =\n\ne j\n\ns(1 \u2212 h j j )1/2\n\n(cid:17)\u03b2\n= y j \u2212 x t\ns(1 \u2212 h j j )1/2\n\nj\n\n,\n\n(8.26)\n\n(cid:17)\u03b2 =(cid:17)y j is the jth fitted value and s2 is the unbiased estimate of \u03c3 2 based on\n\nwhere x t\nj\nthe model. the r j have means zero and approximately unit variances, and hence are\ncomparable with standard normal variables.\n\nthe simplest check on linearity is to plot the response vector y against each column\nof the design matrix x. it isalso useful to plot the standardized residuals r against each\nvariable, whether or not it has been used in the model. incorrect form of dependence on\nan explanatory variable, or omission of one, will show as a pattern in the corresponding\nplot. more formal techniques designed to detect wholesale nonlinearity are discussed\nbelow.\nconstancy of variance is usually checked by a plot of the r j or |r j| against fitted\nvalues. a common failure of this assumption occurs when the error variance increases\nresiduals e and the fitted values(cid:17)y are uncorrelated, we would expect random scatter\nwith the level of the response; this shows as a trumpet-shaped plot. since the raw\n\nif the model fitted adequately. this plot can also help to detect a nonlinear relation\nbetween the response and fitted value, as in example 8.24 below.\n\nnon-independence of the errors can be hard to detect and can have a serious effect\non the standard errors of estimates, but serial correlation of time-ordered observations\nmay show up in scatterplots of lagged r j , or intheir correlogram.\n\nassumptions about the distribution of the errors can be checked by probability\n\nplots of the r j . inparticular, normal scores plots are widely used.\n\n "}, {"Page_number": 400, "text": "388\n\n8 \u00b7 linear regression models\n\nfigure 8.4 residual\nplots for data on cycling\nup a hill. the panels\nshowing residuals plotted\nagainst levels of day and\nrun, and against fitted\nvalues, would show\nrandom variation if the\nmodel is adequate, as\nseems to be the case. the\nnormal scores plot shows\nthat the errors appear\nclose to normal.\n\nsingle outliers \u2014 maybe due to mistakes in data recording, transcription, or entry \u2014\nare likely to show up on any of the plots described above, while multiple outliers may\nlead to masking where each outlier is concealed by the presence of others.\n\nexample 8.22 (cycling data) figure 8.4 shows plots of the r j for the model that\nincludes effects of seat height, dynamo and tyre pressure. the top panels show the r j\nplotted against the day on which the run took place, and the order of the run within each\nday. there is slight evidence of dependence on these, but we must beware of spurious\npatterns when there are only sixteen observations. to check whether these patterns\nmight be genuine, we construct the f statistic for inclusion of factors corresponding\nto day and run after including seat height, dynamo, and tyre pressure in the model.\nits value is 3.99, to be compared to f7,5(0.95) = 4.88. any evidence of differences\namong days and runs is weak, and we discount it.\n\nthe lower left panel of the figure shows residuals plotted against fitted values.\nthere is a slight suggestion that the error variance increases as the fitted value does,\nbut this is mostly due to the largest observation at the right of the plot.\n\nthe lower right panel of the figure shows a normal probability plot of the residuals.\n\nthis is slightly upwardly curved, but not remarkably so in so small a set of data.\n\n "}, {"Page_number": 401, "text": "8.6 \u00b7 model checking\n\n389\n\ninspection of table 8.3 shows that the largest residual is for the sixth setup, of\n\nwhich the experimenter writes:\n\nits comparison run (setup 5) was only 54 seconds. this is the largest amount of\nvariation in the whole table. i suspect that the correct reading for setup 6 was\n55 seconds, that is, i glanced at my watch and thought that it said 60 instead of\n55 seconds. since i am not sure, however, i have not changed it for the analysis.\nthe conclusions would be the same in any case.\n\none reason that the conclusions would be unchanged is that a well-designed experi-\nment like this is relatively robust to a single bad value.\n\nto sum up: the linear model (8.2) seems to fit these data adequately.\n\n(cid:1)\n\n8.6.2 nonlinearity\nlinearity is usually a convenient fiction for describing how a response depends on\nthe explanatory variables, and there are many ways it can fail. for example, a linear\nmodel may be appropriate for a transformation of the original response, so that a(y) =\nx t\u03b2 + \u03b5 for some function a(\u00b7); then y = a\n\u22121(x t\u03b2 + \u03b5) and error is not additive on\nthe original scale. another possibility is that the response is a nonlinear function of\nx t\u03b2 but the error is additive, that is, y = b(x t\u03b2) + \u03b5 for some b(\u00b7). more generally\nwe could put a(y) = b(x t\u03b2) + c(\u03b5) for fairly arbitrary functions a(\u00b7), b(\u00b7) and c(\u00b7).\nsuch models can be fitted, but they are beyond our scope.\nfor asimpler approach, we consider parametric transformation of the response, in\nwhich we assume that for some family of transformations a(\u00b7) indexed by a parameter\n\u03bb, there is a transformation such that a(y) = x t\u03b2 + \u03b5. inprinciple we might consider\nmany possible transformations, but practical experience suggests that power and log-\narithmic transformations are among the most fruitful. the following example gives a\ngeneral approach.\n\nexample 8.23 (box\u2013cox transformation) suppose that a normal linear model\napplies not to y, butto\n\n(cid:10)\n\ny(\u03bb) =\n\n,\n\n\u03bb (cid:6)= 0,\ny\u03bb\u22121\nlog y, \u03bb = 0.\n\n\u03bb\n\nas \u03bb varies in the range (\u22122, 2) this encompasses the inverse transformation (\u03bb = \u22121),\nlog (\u03bb = 0), cube and square roots (\u03bb = 1\n2 ), and the original scale (\u03bb = 1), as well\nas the square transformation (\u03bb = 2). we assume below that all the y j are positive.\nif not, the transformation must be applied to y j + \u03be, with \u03be chosen large enough to\nmake all the y j + \u03be positive.\nnow let y(\u03bb) denote the n \u00d7 1 vector of transformed responses, and assume that a\n\n, 1\n\n3\n\nnormal linear model\n\ny(\u03bb) = x\u03b2 + \u03b5\n\napplies for some values of \u03bb, \u03b2, and error variance \u03c3 2. weassume that the design\nmatrix contains a column of ones, so that using y(\u03bb) rather than y\u03bb leaves the fit\nunchanged; it merely changes the intercept and rescales \u03b2.\n\nsuggested by box and\ncox (1964). george e. p.\nbox (1919\u2013) was educated\nat london university and\nhas held posts in industry\nand at princeton and the\nuniversity of wisconsin.\nhe has made important\ncontributions to robust\nand bayesian statistics,\nexperimental design, time\nseries, and to industrial\nstatistics. sir david\nroxbee cox (1924\u2013) was\nborn in birmingham and\neducated in cambridge\nand leeds. he has held\nposts at imperial college\nlondon, cambridge, and\noxford where he nows\nworks. he has made\nhighly influential\ncontributions across the\nwhole of statistical theory\nand methods. see\ndegroot (1987a) and\nreid (1994).\n\n "}, {"Page_number": 402, "text": "390\n\n8 \u00b7 linear regression models\n\nto obtain the likelihood for \u03b2, \u03c3 2, and \u03bb, note that on taking into account the\n\njacobian of the transformation from y(\u03bb) to y, the density of y j is\n\nf (y j ; \u03b2, \u03c3 2, \u03bb) = y\u03bb\u22121\n\nj\n\n(2\u03c0 \u03c3 2)1/2 exp\n\n\u2212 1\n2\u03c3 2\n\n\u2212 x t\n\nj\n\n\u03b2\n\ny(\u03bb)\nj\n\n(cid:10)\n\n(cid:11)\n\n(cid:13)\n\n(cid:12)2\n\n.\n\n(cid:14)\n\nconsequently the log likelihood based on independent y1, . . . , yn is\n+ (\u03bb \u2212 1)\n\nn log \u03c3 2 + 1\n\n\u2212 x t\n\n(cid:12)2\n\n(cid:11)\n\n\u03b2\n\n(cid:8)(\u03b2, \u03c3 2, \u03bb) \u2261 \u2212 1\n2\n\ny(\u03bb)\nj\n\n\u03c3 2\n\nj\n\nn(cid:15)\nj=1\n\nn(cid:15)\nj=1\n\n(cid:16)\n\nif \u03bb is regarded as fixed, the maximum likelihood estimates of \u03b2 and \u03c3 2 are (cid:17)\u03b2\u03bb =\n\u22121 x t y(\u03bb) and ss((cid:17)\u03b2\u03bb)/n, where ss((cid:17)\u03b2\u03bb) isthe residual sum of squares for the\n\n(x t x)\nregression of y(\u03bb) on the columns of x. thus the profile log likelihood for \u03bb is\n\nlog y j .\n\n(cid:8)p(\u03bb) = max\n(cid:30)\n\n\u03b2,\u03c3 2\n\n(cid:8)(\u03b2, \u03c3 2, \u03bb) \u2261 \u2212 n\n2\n\n(cid:19)\n\nlog ss((cid:17)\u03b2\u03bb) \u2212 log g2(\u03bb\u22121)\n\n(cid:20)\n\n,\n\ny j )1/n is the geometric average of y1, . . . , yn. equivalently (cid:8)p(\u03bb) =\nwhere g = (\n2 n log ssg((cid:17)\u03b2\u03bb), where ssg((cid:17)\u03b2\u03bb) isthe residual sum of squares for the regression of\n\u2212 1\ny(\u03bb)/g on the columns of x. exercise 8.6.3 invites you to provide the details.\n\u03bb; a (1 \u2212 2\u03b1) confidence interval is the set for which (cid:8)p(\u03bb) \u2265 (cid:8)p((cid:17)\u03bb) \u2212 1\n\na plot of the profile log likelihood (cid:8)p(\u03bb) summarizes the information concerning\n2 c1(1 \u2212 2\u03b1).\nthe exact maximum likelihood estimate of \u03bb is rarely used, since a nearby value is\n(cid:1)\nusually more easily interpreted.\na different approach is to consider whether the model y = b(x t\u03b2) + \u03b5 might apply.\nthis cannot be linearized by a response transformation and if there is evidence that\nb(\u00b7) issubstantially nonlinear but the variance is constant it may be necessary to fit a\nnonlinear normal model. the following example gives one method for detecting this\nsort of nonlinearity.\nexample 8.24 (non-additivity) suppose that it is feared that y = b(x t\u03b2) + \u03b5,\nwhere b(\u00b7) is asmooth nonlinear function. taylor series expansion of b(\u00b7) about a\ntypical value of x t\u03b2, \u03b7, say, gives\n\n.= b(\u03b7) + b\n\n(cid:2)\n\ny\n\n(\u03b7)(x t\u03b2 \u2212 \u03b7) + 1\n2\n\n(cid:2)(cid:2)\n\n(\u03b7)(x t\u03b2 \u2212 \u03b7)2 + \u03b5.\n\nb\n\nif the model contains a constant, so that x t\u03b2 = \u03b20 + x1\u03b21 + \u00b7\u00b7\u00b7, then y\n\u03b4(x t\u03b3 )2 + \u03b5, where \u03b3 is just a reparametrization of \u03b2, and \u03b4 \u221d b\nof \u03b4 corresponds to strong nonlinear dependence of y on x t\u03b2.\nlet us fit the model y = x\u03b2 + \u03b5, giving fitted values x t\nsquares ss((cid:17)\u03b2). then as y \u2212 x t\u03b3\n\n.= x t\u03b3 +\n(\u03b7). a large value\n(cid:17)\u03b2 and residual sum of\n.= \u03b4(x t\u03b3 )2 + \u03b5, non-additivity should show up as\n(cid:17)\u03b2)2\n\na formal test for non-zero \u03b4 is based on refitting the model with the column (x t\nj\n\ncurvature in a plot of standardized residuals against fitted values.\n\nadded to the design matrix. although the residual sum of squares for this model,\n\n(cid:2)(cid:2)\n\nj\n\nc\u03bd (\u03b1) is the\u03b1 quantile of\nthe \u03c7 2\n\n\u03bd distribution.\n\n "}, {"Page_number": 403, "text": "8.6 \u00b7 model checking\n\n391\n\ntable 8.10 poison data\n(box and cox, 1964).\nsurvival times in 10-hour\nunits of animals in a 3 \u00d7 4\nfactorial experiment with\nfour replicates. the table\nunderneath gives average\n(standard deviation) for\nthe poison \u00d7 treatment\ncombinations.\n\ntreatment\n\npoison 1\n\npoison 2\n\npoison 3\n\na\nb\nc\nd\n\n0.31, 0.45, 0.46, 0.43\n0.82, 1.10, 0.88, 0.72\n0.43, 0.45, 0.63, 0.76\n0.45, 0.71, 0.66, 0.62\n\n0.36, 0.29, 0.40, 0.23\n0.92, 0.61, 0.49, 1.24\n0.44, 0.35, 0.31, 0.40\n0.56, 1.02, 0.71, 0.38\n\n0.22, 0.21, 0.18, 0.23\n0.30, 0.37, 0.38, 0.29\n0.23, 0.25, 0.24, 0.22\n0.30, 0.36, 0.31, 0.33\n\ntreatment\n\npoison 1\n\npoison 2\n\npoison 3\n\naverage\n\na\nb\nc\nd\n\n0.41 (0.07)\n0.88 (0.16)\n0.57 (0.16)\n0.61 (0.11)\n\n0.32 (0.08)\n0.82 (0.34)\n0.38 (0.06)\n0.67 (0.27)\n\n0.21 (0.02)\n0.34 (0.05)\n0.24 (0.01)\n0.33 (0.03)\n\naverage\n\n0.62\n\n0.55\n\n0.28\n\n0.31\n0.68\n0.39\n0.53\n\n0.48\n\nss\u03b4, depends upon the fitted values for the previous fit, the f statistic for inclusion\nof (x t\nj\n\n(cid:17)\u03b2)2,\n\nss((cid:17)\u03b2) \u2212 ss\u03b4\nss\u03b4/(n \u2212 p \u2212 1)\n\n,\n\n(8.27)\n\nsee tukey (1949).\n\nhas an f1,n\u2212 p\u22121 distribution; this is known as tukey\u2019s one degree of freedom for\n(cid:1)\nnon-additivity.\n\n(cid:17)\u03b2)2 in\n\ncovariates that are artificially created to help assess model fit, such as (x t\nj\n\nexample 8.24, are known as constructed variables.\n\nexample 8.25 (poisons data) table 8.10 contains data from a completely random-\nized experiment on the survival times of 48 animals. the animals were divided at\nrandom into groups of size four, and then each group was given one of three poisons\nand one of four treatments. thus there are two factors, one with three and the other\nwith four levels. the lower part of table 8.10 and the upper panels of figure 8.5 both\nshow strong effects of treatment and poison: poison 3 is most potent, and treatments b\nand d are more efficacious than a and c. there is also evidence that the response vari-\nance depends on the mean: the standard deviations are smaller for poison \u00d7 treatment\ncombinations with smaller average response.\n\none model for these data is\nyt pj = \u00b5 + \u03b1t + \u03b2 p + \u03b5t pj ,\n\nt = 1, 2, 3, 4, p = 1, 2, 3, j = 1, 2, 3, 4.\n\n(8.28)\n\nhere \u00b5 represents a baseline average response in the absence of treatments or poisons,\n\u03b1t represents the effect of the tth treatment, \u03b2 p the effect of the pth poison and \u03b5t pj is\nthe unobserved error for the jth replicate given the tth treatment and pth poison. we\nassess the fit of (8.28) initially through the plot of standardized residuals against fitted\nvalues in the upper left panel of figure 8.6, which shows a striking increase of error\n\n "}, {"Page_number": 404, "text": "392\n\ne\nm\n\ni\nt\n\ne\nc\nn\na\ni\nr\na\nv\n\n2\n\n.\n\n1\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n5\n\n.\n\n0\n\n4\n\n.\n\n0\n\n3\n\n.\n\n0\n\n2\n\n.\n\n0\n\n1\n\n.\n\n0\n\n0\n\n.\n\n0\n\n2\n\n.\n\n1\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n2\n\n5\n\n.\n\n1\n\n0\n\n.\n\n1\n\n5\n0\n\n.\n\n0\n\n.\n\n0\n\ne\nm\n\ni\nt\n\n1\n\n2\n\n3\n\npoison\n\n\u2022\n\n\u2022\n\ne\nc\nn\na\ni\nr\na\nv\n\n\u2022\n\n\u2022\n\n\u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022\n\n8 \u00b7 linear regression models\n\nfigure 8.5 poison data.\nthe upper panels show\nhow the responses depend\non the factor levels. the\nlower left panel shows a\n\u03c7 2\n3 probability plots of the\n3s2\npt , where s2\npt is the\nsample variance of the\nfour replicates ypt j given\nthe pth poison and tth\ntreatment. the lower right\npanel shows the same plot\nfor the y\n\n\u22121\npt j .\n\na\n\nb\n\nc\n\nd\n\ntreat\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022 \u2022\n\n\u2022 \u2022\n\n\u2022\n\n\u2022 \u2022 \u2022\n\n0\n\n2\n\n4\n\n6\n\n0\n\n2\n\n4\n\n6\n\nquantiles of chi-squared\n\nquantiles of chi-squared\n\nvariance with the mean response. the model underpredicts for the lowest responses,\n\nwhere r j > 0 and therefore y j >(cid:17)y j , and overpredicts for the middle responses, where\n\nthe residuals are mostly negative. following example 8.24, this suggests that the\npoison and treatment effects are not additive. the neighbouring panel shows that the\nerrors are somewhat positively skewed relative to the normal distribution. the model\nfits the data poorly, not owing to a few bad observations, but in a systematic way, as\nwas also suggested by the lower left panel of figure 8.5.\n\nignoring for a moment the nonconstancy of variance, we explore whether the\nexplanatory variables act additively. the f statistic for non-additivity, (8.27), equals\n14.03. this is large compared with the 0.95 quantile of the f1,41 distribution and gives\nstrong evidence of non-additivity.\nthe lower right panel of figure 8.6 shows the profile log likelihood for the transfor-\nmation parameter, \u03bb. there is strong evidence that the original scale (\u03bb = 1) is poor;\nlog transformation (\u03bb = 0) also seems inappropriate. the most readily interpretable\nvalue of \u03bb in the 95% confidence interval seems to be \u22121, corresponding to fitting a\nlinear model to the inverse response 1/y. this can be interpreted in terms of the rate\n\u22121. the lower left panel of the figure suggests that the\nof dying, whose units are time\nevidence for non-additivity has gone, and that the inverse transformation has roughly\n\n "}, {"Page_number": 405, "text": "8.6 \u00b7 model checking\n\n393\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nfigure 8.6 diagnostic\nplots for the two-way\nlayout model for the\npoisons data. the upper\nleft panel a plot of\nstandardized residuals for\nthe fit of the two-way\nlayout model to the\noriginal data against the\nfitted value, while its\nneighbour shows the\nnormal probability plot of\nthese residuals. the lower\nright panel shows the\nprofile log likelihood for\nthe box\u2013cox parameter \u03bb\nand suggests that a linear\nmodel should be fitted to\nthe inverse response, 1/y.\nthe lower left panel\nshows the residuals for the\ntwo-way layout model\nwith response 1/y plotted\nagainst its fitted values;\nthis does not display the\nnon-linearity and\nsystematic increase of\nvariance of the panel\nabove.\n\nl\n\ni\n\na\nu\nd\ns\ne\nr\n\nl\n\ni\n\na\nu\nd\ns\ne\nr\n\n4\n\n2\n\n0\n\n2\n-\n\n4\n-\n\n4\n\n2\n\n0\n\n2\n-\n\n4\n-\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\u2022\n\n\u2022\u2022\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\u2022\u2022\u2022\n\n\u2022\u2022\u2022\u2022\n\n\u2022\u2022\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\nl\n\na\nu\nd\ns\ne\nr\n \n\ni\n\nd\ne\nr\ne\nd\nr\no\n\n4\n\n2\n\n0\n\n2\n-\n\n4\n-\n\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\nfitted value\n\nquantiles of standard normal\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\u2022\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\u2022\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n 95%\n\nd\no\no\nh\n\ni\nl\n\ne\nk\n\ni\nl\n \n\ng\no\n\nl\n \n\ne\n\nl\ni\nf\n\no\nr\np\n\n0\n2\n\n0\n1\n\n0\n\n0\n1\n-\n\n0\n2\n-\n\n0\n3\n-\n\n1\n\n2\n\n3\n\n4\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\nfitted value\n\nlambda\n\nequalized the error variances. a probability plot shows that the residuals on this scale\nare close to normal.\n\u22121 = \u00b5 + \u03b1t + \u03b2 p + \u03b5t pj seems to fit the data adequately,\nand has a direct interpretation as a linear model for the effect of poisons and treatments\non the speed of dying.\n\nto sum up, the model y\n\nwe return to these data in examples 9.6 and 9.8.\n\n(cid:1)\n\n8.6.3 leverage, influence, and case deletion\nwe call the explanatory and response variables (x j , y j ) the jth case. we have already\n(cid:17)\u03b2) = \u03c3 2(1 \u2212 h j j ), and notice that if h j j\nseen how an odd y j can arise, but there can also be effects due to unusual explanatory\nvariables. to see how, recall that var(y j \u2212 x t\nis close to one the jth fitted value must lie very close to y j itself. indeed, if h j j = 1,\n(cid:17)\u03b2 = y j . this is undesirable because in effect a\nthe model is constrained so that x t\nj\nexactly. the effect on(cid:17)\u03b2 could be catastrophic if y j were outlying.\ndegree of freedom, the equivalent of one parameter, is used to fit one response value\nthe argument above suggests that low leverage is good. but tr(h) = (cid:18)\n\nthe quantity h j j is called the leverage of the jth case. other things being equal,\nh j j = p\n\nj\n\n "}, {"Page_number": 406, "text": "394\n\n8 \u00b7 linear regression models\n\n(exercise 8.2.5), so the average leverage cannot be reduced below p/n. approximate\nequalization of leverage is one attribute of good design. in the factorial experiment\nin table 8.3, for example, h j j = 1\n4 for each case. a general guideline is that cases for\nwhich h j j > 2 p/n deserve closer inspection; it may be worthwhile to repeat an anal-\nysis without them in order to assess their effect on both the values and the precision\nof the estimates. in itself, however, high leverage is not sufficient reason to delete a\ncase, which if not outlying may be very informative.\n\nexample 8.26 (straight-line regression) the matrix formulation of\n\ny j = \u03b30 + (x j \u2212 x)\u03b31 + \u03b5 j ,\n\nj = 1, . . . ,n ,\n\nis given in example 8.6, and it is easily deduced that the jth leverage is\n\nh j j = 1\nn\n\n+ (x j \u2212 x)2\n(cid:18)\nk(xk \u2212 x)2\n\n.\n\n(cid:18)\nk(xk \u2212 x)2, and when\nwhen the constant is dropped the leverage is (x j \u2212 x)2/\n\u22121. thus h j j can be interpreted as a\nthe covariate x j is dropped the leverage is n\nsum of contributions for each parameter. as the contribution corresponding to \u03b31 is\nquadratic in x j \u2212 x, responses with large values of |x j \u2212 x| will strongly affect the\nslope of the fitted line. all the responses have equal weight in estimating the intercept.\nthese effects do not depend on the response values and depend purely on the design\n(cid:1)\nmatrix.\n\nhaving seen that an individual case may substantially affect least squares estimates,\nit is natural to ask how to measure this. one overall influence measure for the jth case\nis cook\u2019s distance, defined as\n\nc j = 1\n\nps2 ((cid:17)y \u2212(cid:17)y\u2212 j )t((cid:17)y \u2212(cid:17)y\u2212 j ),\n\nwhere(cid:17)y\u2212 j = x(cid:17)\u03b2\u2212 j , and subscript \u2212 j denotes a quantity calculated with the jth case\n\ndeleted from the model. cook\u2019s distance measures the overall change in the fitted\nvalues when the jth case is deleted from the model, standardized by the dimension of\n\u03b2 and the estimate of \u03c3 2. itcan be revealing to refit a model without the cases whose\nvalues of c j are largest.\n\nto gain some insight into c j , note that the least squares estimate of \u03b2 calculated\n\nwithout the jth case is(cid:17)\u03b2\u2212 j = (cid:11)\n\nsome linear algebra shows that\n\nj\n\nx t x \u2212 x j x t\n(cid:17)\u03b2\u2212 j =(cid:17)\u03b2 \u2212 (x t x)\n\n(cid:12)\u22121(x t y \u2212 x j y j ).\n\ny j \u2212(cid:17)y j\n1 \u2212 h j j\n\n,\n\n\u22121x j\n\nand it follows that (exercise 8.6.5)\n\nc j = r 2\n\nj h j j\n\np(1 \u2212 h j j )\n\n,\n\n(8.29)\n\n(8.30)\n\nsee cook (1977).\nr. dennis cook is a\nprofessor of statistics at\nthe university of\nminnesota.\n\n "}, {"Page_number": 407, "text": "8.6 \u00b7 model checking\n\n395\n\nwhere r j is the standardized residual. therefore large values of c j arise if a case\nhas high leverage or a large standardized residual, or both. a plot of c j against\nh j j /(1 \u2212 h j j ) helps to distinguish between these possibilities. a crude rule is that as\na residual with |r j| > 2 or acase with leverage h j j > 2 p/n deserve attention, a value\nof c j greater than 8/(n \u2212 2 p) is worth a closer look. it is possible for the model to\ndepend on a case whose cook\u2019s distance is zero (exercise 8.6.6), however, and there\nis no substitute for careful inspection of the data, residuals, and leverages.\n\nas an observation with a large standardized residual can have a big effect on a fitted\nmodel, it is natural to ask whether an outlier is more easily detected by comparing y j\nwith its predicted value based on the other observations, x t\nj\n\n(cid:17)\u03b2\u2212 j . after all, if the model\nis correct and y j is not an outlier, we expect that e((cid:17)\u03b2) = e((cid:17)\u03b2\u2212 j ) = x t\nof course (cid:17)\u03b2\u2212 j will be a less precise estimate of \u03b2 than (cid:17)\u03b2. onthe other hand, an\n(cid:17)\u03b2\u2212 j , so any discrepancy between them should\n\noutlying response y j does not affect x t\nj\nbe more obvious. there is a close connection to the idea of cross-validation. now\n(8.29) implies that\n\n\u03b2, although\n\nj\n\n(cid:17)\u03b2\u2212 j = yk \u2212(cid:17)yk + x t\n\nyk \u2212 x t\n\u22121x j\n\u22121x j = h jk, wefind that var( y j \u2212 x t\n\nk(x t x)\n\nk\n\nj\n\n,\n\ny j \u2212(cid:17)y j\n1 \u2212 h j j\n(cid:17)\u03b2\u2212 j ) = \u03c3 2/(1 \u2212 h j j ). this\n\nand since x t\nsuggests that deletion residuals be defined as\n\nk(x t x)\n\n=\n\n(cid:2)\nj\n\nr\n\n(cid:17)\u03b2\u2212 j\ny j \u2212 x t\n(cid:11)\n(cid:17)\u03b2\u2212 j\ny j \u2212 x t\n\nj\n\nj\n\n(cid:12)1/2\n\nvar\n\n= y j \u2212(cid:17)y\u2212 j, j\ns\u2212 j (1 \u2212 h j j )1/2\n\n,\n\nwhere(cid:17)y\u2212 j, j is the jth element of the vector(cid:17)y\u2212 j and the estimate of \u03c3 2 based on the\n\ndata with the jth case deleted equals\n\n(cid:31)\n\n=\n\ns2\u2212 j\n\n1\n\nn \u2212 1 \u2212 p\n\n(y \u2212(cid:17)y\u2212 j )t(y \u2212(cid:17)y\u2212 j ) \u2212\n\n(cid:10)\n\ny j \u2212(cid:17)y j + h j j (y j \u2212(cid:17)y j )\n1 \u2212 h j j\n\n!\n\n(cid:13)2\n\n.\n\nyet more algebra shows that the deletion residual can be expressed as\n\n\"\n\n=\n\n(cid:2)\nj\n\nr\n\nn \u2212 p \u2212 1\nn \u2212 p \u2212 r 2\n\nj\n\n#1/2\n\nr j ,\n\nwhich is a monotonic function of r j that exaggerates values for which|r j| > 1. as their\nderivation suggests, deletion residuals for outlying observations are more prominent\nthan are the corresponding r j .\n\nexample 8.27 (cycling data) table 8.3 gives standardized residuals, deletion resid-\nuals, and measures of leverage and influence for the model with an intercept and three\nmain effects fitted to these data. the design is balanced, and since (x t x)\n16 i4,\nall the leverages equal 1\n4 ; consequently the standardized residuals are a simple mul-\ntiple of the raw residuals. as remarked in example 8.22, the only unusual residual is\n\n\u22121 = 1\n\n "}, {"Page_number": 408, "text": "table 8.11 simulated\ndata and case diagnostics.\n\n396\n\n8 \u00b7 linear regression models\n\ncase\n\nx1\n\nx2\n\ny\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n0.02\n0.36\n7.12\n\u20131.54\n0.24\n0.26\n\u20130.16\n0.43\n\u20130.02\n4.58\n\n\u20136.31\n0.39\n\u20130.64\n1.13\n\u20131.90\n\u20130.06\n0.13\n0.80\n0.59\n0.29\n\n0.95\n0.44\n0.27\n0.09\n\u20130.82\n0.03\n\u20130.22\n0.13\n3.57\n0.57\n\n(cid:17)y\n\n0.41\n0.53\n0.38\n0.59\n0.49\n0.53\n0.54\n0.54\n0.55\n0.45\n\nr\n\n1.16\n\u20130.08\n\u20130.14\n\u20130.45\n\u20131.07\n\u20130.40\n\u20130.61\n\u20130.33\n2.47\n0.11\n\n(cid:2)\n\nr\n\n1.20\n\u20130.07\n\u20130.13\n\u20130.42\n\u20131.08\n\u20130.37\n\u20130.59\n\u20130.31\n6.31\n0.10\n\nh\n\nc\n\n0.88\n0.13\n0.68\n0.29\n0.15\n0.12\n0.14\n0.15\n0.15\n0.31\n\n3.28\n0.00\n0.01\n0.03\n0.07\n0.01\n0.02\n0.01\n0.37\n0.00\n\nfor setup 6, whose deletion residual is strikingly large: there is strong evidence that\nthis is an outlier. the corresponding cook statistic, 0.56, is by far the largest, but it\nis unremarkable relative to 8/(n \u2212 2 p) = 1. the belt-and-braces statistician might\n(cid:1)\nrepeat the analysis without this datum, but it makes little difference.\n\nexercises 8.6\n1\n\n2\n\n3\n4\n5\n\n6\n\nshow that the standardized residuals r j have means zero and variances (n \u2212 p)/(n \u2212 p \u2212\n2). what can you say about their joint distribution?\ntable 8.11 shows simulated data on the dependence of y = \u03b20 + \u03b21x1 + \u03b22x2 + \u03b5 on\n(a) choose a case and check the relationships between(cid:17)y, r, r\ncovariates x1 and x2. the residual sum of squares was 12.43.\n\n, h, and c.\n\n(cid:2)\n\n(b) discuss the fit. if it is not adequate, explain what further steps you would take in\nanalyzing the data.\nprovide the details for example 8.23.\ncompute and interpret the leverages for examples 8.9 and 8.20.\nuse exercise 8.5.2(a) with c = \u22121 toshow that\n\n(cid:11)\n\nx t x \u2212 x j x t\n\nj\n\n(cid:12)\u22121 = (x t x)\n\n\u22121(x t x)\n\n\u22121 + (1 \u2212 h j j )\n\u22121x j . hence show that\n(cid:12)\u22121 (x t y \u2212 x j y j ) =(cid:17)\u03b2 \u2212 (1 \u2212 h j j )\n\nj (x t x)\n\nit may help to note that h j j = x t\n\n(cid:17)\u03b2\u2212 j = (cid:11)\n\nx t x \u2212 x j x t\n\nj\n\n\u22121x j x t\n\nj (x t x)\n\n\u22121;\n\n\u22121(x t x)\n\n\u22121x j (y j \u2212(cid:17)y j ),\n\ndeduce that(cid:17)y \u2212(cid:17)y\u2212 j = (1 \u2212 h j j )\n\n\u22121 x(x t x)\n\n\u22121x j (y j \u2212(cid:17)y j ), and finally that\n\nc j = ((cid:17)y \u2212(cid:17)y\u2212 j )t((cid:17)y \u2212(cid:17)y\u2212 j )\n\nps2\n\n= r 2\n\nj h j j\n\np(1 \u2212 h j j )\n\n.\n\nsuppose that the straight-line regression model y = \u03b20 + \u03b21x + \u03b5 is fitted to data in which\nx1 = \u00b7\u00b7\u00b7 = xn\u22121 = \u2212a and xn = (n \u2212 1)a, for some positive a. show that although yn\ncompletely determines the estimate of \u03b21, cn = 0. is cook\u2019s distance an effective measure\nof influence in this situation?\n\n "}, {"Page_number": 409, "text": "8.7 \u00b7 model building\n\n397\n\n8.7 model building\n8.7.1 general\nonce the context for a regression problem is known and the data have been scrutinized\nfor outliers, missing values, and so forth, a model must be built. related investigations\nwill often suggest a form for it, the main initial questions concerning the choice of\nresponse and explanatory variables.\n\nthe purpose of the analysis determines one or perhaps more responses, which may\ncombine several of the original variables. once it is chosen, questions arise about\nwhether individual responses are correlated, and if their variance is constant. if not, it\nmay be necessary to use weighted or generalized least squares (section 8.2.4), or to\nconsider transformations. these may also be suggested by constraints, for example\nthat the response is positive, but it is then also good to consider more general classes\nof models discussed in chapter 10.\n\nscatterplots of the response against potential explanatory variables and of these\nvariables against each another are needed to screen out bad data, to suggest which\ncovariates are likely to be important, and perhaps also to indicate suitable transfor-\nmations. dimensional considerations or subject-matter arguments, for example that\ncertain regression coefficients should be positive, may suggest fruitful combinations\nof covariates or particular relations between them and the response.\n\nit may be clear that the response depends on a few variables, and that possible\nmodels can be fitted and compared using f and related tests. once some suitable\nmodels have been found, the techniques of model checking outlined in section 8.6\ncan be applied. often unexpected discrepancies between a fitted model and data\nwill lead to further thought, and then to more cycles of model-fitting, checking, and\ninterpretation, iterated until a broadly satisfactory model has been found.\n\nif p is much larger than n, then the design matrix must be cut down to size. one\npossibility is to use principal components regression. the basis of this is the spectral\ndecomposition, which enables us to write x t x = u du t, where d is the diagonal\nmatrix diag(d1, . . . ,d p) containing the ordered eigenvalues d p \u2265 \u00b7\u00b7\u00b7 \u2265d 1 \u2265 0 of\nx t x, and the columns of u are the corresponding eigenvectors. the matrix u can\nbe chosen so that uu t = u tu = i . the idea is to form the design matrix from\nthe columns of z = xu , which are called principal components. the first principal\ncomponent, z1, isthe linear combination z = xu of the columns of x for which ztz is\n1z2 = 0,\nlargest, the next, z2, isthe linear combination that maximizes zt\n1z2 = 0, and so forth. the hope is\nthe third, z3, maximizes zt\nthat much of the dependence of the response on the columns of x will be concentrated\nin these first few zr s, in which case a good low-dimensional regression model may\nbe obtainable. sometimes it is useful to centre the columns of x by subtracting their\naverages, or to scale them by dividing centred columns by their standard deviations.\nthe resulting principal components do not equal those for x.\n\n3z3 subject to zt\n\n2z2 subject to zt\n\n1z2 = zt\n\nprincipal components and corresponding parameter estimates may be uninter-\npretable in terms of the original covariates, though this drawback is less critical\nwhen the goal of analysis is prediction.\n\n "}, {"Page_number": 410, "text": "398\n\n8 \u00b7 linear regression models\n\n8.7.2 collinearity\nif there is a nonzero vector c such that xc = 0, the columns of the design matrix are\nsaid to be collinear. then x has rank less than p and x t x has no unique inverse.\nthe simplest example of this arises in straight-line regression: if all the x j are equal,\nit is impossible to find unique parameter estimates (example 8.6). this difficulty\narises more generally, because linear dependence among the columns of the design\nmatrix means that some combinations of parameters cannot be estimated from the\ndata; collinearity leads to indeterminable estimates with infinite variances. related\ndifficulties arise if the columns of x are almost collinear.\nthe matrix x t x is invertible only if all its eigenvalues d p \u2265 \u00b7\u00b7\u00b7 \u2265d 1 \u2265 0 are\nsquared distance between(cid:17)\u03b2 and \u03b2 is expressible as\npositive. even if x t x is invertible, however, the estimators can be very poor. the\n\n((cid:17)\u03b2 \u2212 \u03b2)t((cid:17)\u03b2 \u2212 \u03b2) d= \u03c3 2\n\nz 2\nr\n\n/dr , where z1, . . . , z p\n\niid\u223c n (0, 1).\n\nthus ((cid:17)\u03b2 \u2212 \u03b2)t((cid:17)\u03b2 \u2212 \u03b2) has mean and variance\n\np(cid:15)\nr=1\n\np(cid:15)\nr=1\n\n\u03c3 2\n\n\u22121\nd\nr\n\n,\n\n2\u03c3 4\n\n\u22122\nd\nr\n\n,\n\np(cid:15)\nr=1\n\n1 , and(cid:17)\u03b2 may be far distant from \u03b2\n\nbounded below respectively by \u03c3 2/d1 and 2\u03c3 4/d2\nfor small d1. the practical implication is that parameter estimates from different but\nrelated datasets may vary greatly, giving apparently contradictory interpretations of\nthe same phenomenon.\n\ndiagnostics to warn of collinearity can be based on functions of the dr such as\nthe condition number (d p/d1)1/2, but its statistical interpretation is not clear-cut. the\ncondition number is sometimes reduced by replacing x with the matrix obtained on\ndropping the column of ones if any and centering the remaining columns, or by using\nthe corresponding correlation matrix.\n\nthe most straightforward solution to collinearity or near collinearity is to drop\n\ncolumns from the design matrix until the estimates are better behaved.\na more systematic approach to dealing with weak design matrices is ridge\nregression, which starts by rewriting the original model y = 1\u03b20 + x1\u03b21 + \u03b5 as\ny = 1\u03b20 + z \u03b3 + \u03b5, where z t1 = 0 and the diagonal of z t z consists of ns. this in-\nvolves centring each column of x1 by subtracting its average, then dividing by its stan-\ndard deviation, and multiplying by n1/2. this centring and rescaling ensures that the\nlike with principal components regression. then the least squares estimates are(cid:17)\u03b20 = y\nelements of \u03b3 and of \u03b2 have the same interpretations apart from a change of scale, un-\nand(cid:17)\u03b3 = (z t z)\n\u22121 z t y. the idea is to replace z t z by z t z + \u03bbi p\u22121, where \u03bb \u2265 0 is\ncalled the ridge parameter. the corresponding estimates,(cid:17)\u03b3\u03bb = (z t z + \u03bbi p\u22121)\n\u22121 z t y,\nare biased unless \u03bb = 0, when they are the least squares estimates of \u03b3 . large values\nof \u03bb increase the bias by shrinking the estimates towards the origin, but this decreases\ntheir variance. the value of \u03bb is chosen empirically by minimization of a criterion\n\n "}, {"Page_number": 411, "text": "8.7 \u00b7 model building\n\n399\n\ntable 8.12 parameter\nestimates and their\nstandard errors for the full\nmodel and a reduced\nmodel fitted to the cement\ndata.\n\nfull model\n\nreduced model\n\nparameter\n\nestimate\n\nstandard error\n\nestimate\n\nstandard error\n\n\u03b20\n\u03b21\n\u03b22\n\u03b23\n\u03b24\n\n62.41\n1.55\n0.51\n0.10\n\u22120.14\n\n70.07\n0.74\n0.72\n0.75\n0.71\n\n71.64\n1.45\n0.42\n\u22120.24\n\n14.14\n0.12\n0.19\n\n0.17\n\nsuch as the cross-validation sum of squares\n\ncv(\u03bb) = n(cid:15)\n\nj=1\n\n(y j \u2212(cid:17)y\n\n\u2212\nj )2,\n\nwhere(cid:17)y\n\n\u2212\nj is the fitted value for y j predicted from the ridge regression model obtained\nwhen the jth case is deleted. cross-validation, introduced in section 7.1.2, is here used\nto assess how well the ridge regression fit would predict a new set of independent\ndata like the original observations. a variant approach chooses \u03bb to minimize the\ngeneralized cross-validation sum of squares,\n\ngcv(\u03bb) = n(cid:15)\nj=1\n+ z(z t z + \u03bbi p\u22121)\n\n(y j \u2212(cid:17)y j )2\n{1 \u2212 tr(h\u03bb)/n}2\nwhere h\u03bb = n\n\u22121 z t is the hat matrix corresponding to the\nridge regression, and the vector of fitted values(cid:17)y = h\u03bb y depends on \u03bb. wediscuss\nestimates such as (cid:17)\u03b3\u03bb that shrink towards a common value, here \u03b3 = 0, may also\n\nthese in more detail on page 523, though in another context.\n\n\u221211n1t\n\n,\n\nn\n\nbe derived by bayesian arguments (chapter 11).\n\nexample 8.28 (cement data) the astute reader will have realized that if the middle\nfour columns of table 8.1 are percentages, they may sum to 100. in fact they sum to\n(99, 97, 95, 97, 98, 97, 97, 98, 96, 98, 98, 98, 98). as there is a column of ones in the\ndesign matrix for the full model, its columns are nearly dependent: estimation of five\nparameters is almost impossible. this is reflected by the standard errors in table 8.12.\n\nthe standard error for (cid:17)\u03b20 is vastly inflated by inclusion of x3 because \u03b20 is almost\n\nimpossible to estimate, whereas the other estimates are less badly affected.\n\nthe residual sum of squares for model without x3 is 47.97, only slightly larger than\nthat for the full model, 47.86. thus inclusion of x3 changes the fit of the model very\nlittle, but has a drastic effect on the precision of parameter estimation.\n\nthe eigenvalues of x t x with all five columns of x are 44676, 5965.4, 810.0,\n105.4 and 0.00012. the condition number of 6056 indicates strong ill-conditioning,\nand\n\n(cid:18)\nthe left panel of figure 8.7 shows how the parameter estimates (cid:17)\u03b3\u03bb depend on\n\n= 821 seems very large.\n\n\u22121\nd\nr\n\nthe ridge parameter \u03bb. all change fairly sharply as \u03bb increases from zero, and are\nmore stable for \u03bb >0.2. the right panel shows that gcv(\u03bb) decreases sharply when\n\n "}, {"Page_number": 412, "text": "400\n\n1\n2\n\n3\n\n4\n\n)\na\nd\nb\nm\na\nl\n(\n \nt\n\na\nh\n\n \n\na\nm\nm\na\ng\n\n8\n\n6\n\n4\n\n2\n\n0\n\n2\n-\n\n4\n-\n\n6\n-\n\n1\n\n2\n\n4\n\nv\nc\ng\n\n0\n\n.\n\n8\n\n5\n\n.\n\n7\n\n0\n\n.\n\n7\n\n5\n\n.\n\n6\n\n0\n\n.\n\n6\n\n5\n\n.\n\n5\n\n0\n\n.\n\n5\n\n8 \u00b7 linear regression models\n\nfigure 8.7 ridge\nregression analysis of\ncement data. left:\nvariation of elements of\n\n(cid:17)\u03b3\u03bb as a function of \u03bb, for\n\nmodels with all four\ncovariates (solid) and with\nx1, x2, and x4 only (dots).\nright: generalized\ncross-validation criterion\ngcv(\u03bb) for these models.\n\n0.0\n\n0.5\n\n1.0\n\n1.5\n\n2.0\n\n0.0\n\n0.5\n\n1.0\n\n1.5\n\n2.0\n\nlambda\n\nlambda\n\n\u03bb increases from zero, and is minimized when \u03bb\n\n.= 0.3. the dotted lines show that\nwhen x3 is dropped both the(cid:17)\u03b3\u03bb and gcv(\u03bb) depend much less on \u03bb, consistent with\n\nthe discussion above.\n\n(cid:1)\n\n8.7.3 automatic variable selection\nthe screening and selection of many explanatory variables may be onerous. with p\ncovariates, each to be included or not, at least 2 p possible design matrices must be\nfitted even before accounting for transformations, combinations of covariates, and so\nforth. consequently automatic procedures for variable selection are widely used if p\nis large. while valuable as screening procedures, they are no substitute for careful\nmodel-building incorporating knowledge of the system under study and should be\ntreated as a backstop; their output should always be considered critically.\n\nstepwise methods\nforward selection takes as baseline the model with an intercept only. each term is\nadded separately to this, and the base model for the next stage is taken to be the\nmodel with the intercept and the term that most reduces the sum of squares. each\nof the remaining terms is added to the new base model, and the process continued,\nstopping if at any stage the f statistic for the largest reduction in sum of squares is\nnot significant or if the design matrix is rank deficient.\n\nbackward elimination starts from the model containing all terms, and then suc-\ncessively drops the least significant term at each stage. it stops when no term can be\ndeleted without increasing the sum of squares significantly.\n\nbackward elimination is generally the preferable of the two because its initial\nestimate of \u03c3 2 will usually be better than that for forward selection, though at the\npossible expense of an unstable initial model. they may yield different final models.\nin stepwise regression four options are considered at each stage: add a term, delete\na term, swap a term in the model for one not in the model, or stop. this algorithm is\noften used in practice.\n\nthese three procedures have been shown to fit complicated models to com-\npletely random data, and although widely used they have no theoretical basis. this\n\n "}, {"Page_number": 413, "text": "8.7 \u00b7 model building\n\n401\n\ntable 8.13 data on light\nwater reactors (lwr)\nconstructed in the usa\n(cox and snell, 1981,\np. 81). the covariates are\ndate (date construction\npermit issued), t1 (time\nbetween application for\nand issue of permit), t2\n(time between issue of\noperating license and\nconstruction permit),\ncapacity (power plant\ncapacity in mwe), pr (=1\nif lwr already present on\nsite), ne (=1 if constructed\nin north-east region of\nusa), ct (=1 if cooling\ntower used), bw (=1 if\nnuclear steam supply\nsystem manufactured by\nbabcock\u2013wilcox), n\n(cumulative number of\npower plants constructed\nby each\narchitect-engineer), pt\n(=1 if partial turnkey\nplant).\n\ncost\n\ndate\n\nt1\n\nt2\n\ncapacity\n\npr\n\nne\n\nct\n\nbw\n\nn\n\npt\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n460.05\n452.99\n443.22\n652.32\n642.23\n345.39\n272.37\n317.21\n457.12\n690.19\n350.63\n402.59\n412.18\n495.58\n394.36\n423.32\n712.27\n289.66\n881.24\n490.88\n567.79\n665.99\n621.45\n608.80\n473.64\n697.14\n207.51\n288.48\n284.88\n280.36\n217.38\n270.71\n\n68.58\n67.33\n67.33\n68.00\n68.00\n67.92\n68.17\n68.42\n68.42\n68.33\n68.58\n68.75\n68.42\n68.92\n68.92\n68.42\n69.50\n68.42\n69.17\n68.92\n68.75\n70.92\n69.67\n70.08\n70.42\n71.08\n67.25\n67.17\n67.83\n67.83\n67.25\n67.83\n\n14\n10\n10\n11\n11\n13\n12\n14\n15\n12\n12\n13\n15\n17\n13\n11\n18\n15\n15\n16\n11\n22\n16\n19\n19\n20\n13\n9\n12\n12\n13\n7\n\n46\n73\n85\n67\n78\n51\n50\n59\n55\n71\n64\n47\n62\n52\n65\n67\n60\n76\n67\n59\n70\n57\n59\n58\n44\n57\n63\n48\n63\n71\n72\n80\n\n687\n1065\n1065\n1065\n1065\n514\n822\n457\n822\n792\n560\n790\n530\n1050\n850\n778\n845\n530\n1090\n1050\n913\n828\n786\n821\n538\n1130\n745\n821\n886\n886\n745\n886\n\n0\n0\n1\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n0\n0\n1\n1\n1\n\n1\n0\n0\n1\n1\n1\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n0\n1\n1\n1\n1\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n1\n0\n1\n1\n0\n1\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n1\n\n14\n1\n1\n12\n12\n3\n5\n1\n5\n2\n3\n6\n2\n7\n16\n3\n17\n2\n1\n8\n15\n20\n18\n3\n19\n21\n8\n7\n11\n11\n8\n11\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n\narbitrariness is reflected in rules for deciding which terms to include, some of which\nuse tables of the f or t distributions. others simply drop a term from the model if its\nf statistic is less than a number such as 4, and otherwise include the term. sometimes\na theoretically-motivated criterion such as aic is used.\n\nexample 8.29 (nuclear plant data) table 8.13 contains data on the cost of 32 light\nwater reactors. the cost (in dollars \u00d710\n\u22126 adjusted to a 1976 base) is the quantity of\ninterest, and the others are explanatory variables.\n\ncosts are typically relative. moreover large costs are likely to vary more than small\nones, so it seems sensible to take log(cost) asthe response y. for consistency we also\ntake logs of the other quantitative covariates, fitting linear models using date, log(t1),\nlog(t2), log(capacity), pr, ne, ct, log(n), and pt. the last of these indicates six\nplants for which there were partial turnkey guarantees, and some subsidies may be\nhidden in their costs.\n\n "}, {"Page_number": 414, "text": "table 8.14 parameter\nestimates and standard\nerrors for linear models\nfitted to nuclear plants\ndata; forward and\nbackward indicate models\nfitted by forward selection\nand backward elimination.\n\n402\n\nconstant\ndate\nlog(t1)\nlog(t2)\nlog(cap)\npr\nne\nct\nbw\nlog(n)\npt\n\n8 \u00b7 linear regression models\n\nfull model\n\nbackward\n\nforward\n\nest (se)\n\nt\n\nest (se)\n\nt\n\nest (se)\n\nt\n\n\u221214.24 (4.229) \u22123.37\n3.21\n0.209 (0.065)\n0.092 (0.244)\n0.38\n0.290 (0.273)\n1.05\n0.694 (0.136)\n5.10\n\u22120.092 (0.077) \u22121.20\n3.35\n0.258 (0.077)\n0.120 (0.066)\n1.82\n0.033 (0.101)\n0.33\n\u22120.080 (0.046) \u22121.74\n\u22120.224 (0.123) \u22121.83\n\n\u221213.26 (3.140) \u22124.22\n4.91\n0.212 (0.043)\n\n\u22127.627 (2.875) \u22122.66\n3.38\n0.136 (0.040)\n\n0.723 (0.119)\n\n6.09\n\n0.671 (0.141)\n\n4.75\n\n0.249 (0.074)\n3.36\n0.140 (0.060)\n2.32\n\u22120.088 (0.042) \u22122.11\n\u22120.226 (0.114) \u22121.99\n\n\u22120.490 (0.103) \u22124.77\n\nresidual se (df)\n\n0.164 (21)\n\n0.159 (25)\n\n0.195 (28)\n\nestimates and standard errors for the full model and those found by backward\nelimination and forward selection are given in table 8.14. backward elimination\nstarts by refitting the model without bw and then considering the t statistics for the\nremaining variables, dropping the next least significant, here log(t1), and so forth.\nthe effects for the variables retained are strengthened; most are highly significant.\nforward selection chooses a smaller model with larger residual sum of squares, and\nthis results in smaller t statistics. stepwise selection starting from this model yields the\nmodel chosen by backward elimination. examination of residuals for this suggests no\ndifficulty, and we are left with a model in which cost increases with capacity, though\nnot proportionally, with presence of a cooling tower, with date, and in the north-east\nregion of the usa, but is decreased by a partial turnkey guarantee, and with architect\u2019s\n(cid:1)\nexperience.\n\nlikelihood criteria\na more satisfactory approach is to fit all reasonable models and adopt the one that\nminimizes some overall measure of discrepancy. one such measure is the residual sum\nof squares, but this continues to decrease as the number of parameters increases and\nalways yields the model with all possible terms. this suggests that model complexity\nbe penalized by balancing it against a measure of fit. we now discuss one approach\nto this.\nsuppose that the data were generated by a true model g under which the responses\ny j are independent normal variables with means \u00b5 j and variances \u03c3 2 and let eg(\u00b7)\ndenote expectation with respect to this model. following the discussion in section 4.7,\nour ideal would be to choose the candidate model f (y; \u03b8) tominimize the loss when\npredicting a new sample like the old one,\n\n\"\n\n(cid:31)\n\neg\n\n+\ng\n\ne\n\n2\n\n(cid:14)\n\nn(cid:15)\nj=1\n\nlog\n\n+\nj )\ng(y\n+\nf (y\n\nj ;(cid:17)\u03b8)\n\n(cid:16)!#\n\nthe scaling factor 2 is\nincluded for comparability\nwith aic.\n\n.\n\n(8.31)\n\n "}, {"Page_number": 415, "text": "+\n1\n\n, . . . ,y\n\n8.7 \u00b7 model building\n+\nn is another sample independent of y1, . . . ,y n but with the same dis-\n\nn , and(cid:17)\u03b8 is the maximum likelihood\nand \u03c3 2, with maximum likelihood estimators (cid:17)\u00b51, . . . ,(cid:17)\u00b5n and (cid:17)\u03c3 2. then the sum in\n\nhere y\ntribution, e\nestimator of \u03b8 based on y1, . . . ,y n.\n\nif the candidate model is normal, then \u03b8 comprises the mean responses \u00b51, . . . , \u00b5n\n\n+\ng denotes expectation over y\n\n, . . . ,y\n\n+\n1\n\n403\n\n+\n\n(8.31) equals\n\n1\n2\n\n(cid:14)\n\nn(cid:15)\nj=1\nn(cid:15)\nj=1\n\n+\nj\n\nlog(cid:17)\u03c3 2 + (y\n(cid:10)\n\nlog(cid:17)\u03c3 2 + \u03c3 2(cid:17)\u03c3 2\n\n\u2212(cid:17)\u00b5 j )2\n(cid:17)\u03c3 2\n+ (\u00b5 j \u2212(cid:17)\u00b5 j )2\n\n(cid:17)\u03c3 2\n\n\u2212 log \u03c3 2 \u2212 (y\n\n(cid:16)\n\n,\n\n+\nj\n\n\u2212 \u00b5 j )2\n\n\u03c3 2\n\n(cid:13)\n\n.\n\n\u2212 log \u03c3 2 \u2212 1\n\nand hence the inner expectation is\n\nsuppose that in our earlier terminology a candidate linear model with full-rank n \u00d7\np design matrix x is correct, that is, the true model is nested within it. then the\nvector \u00b5 = (\u00b51, . . . , \u00b5n)t of true means lies in the column space of x and there is a\np \u00d7 1 vector \u03b2 such that \u00b5 = x\u03b2. hence(cid:17)\u00b5 = ((cid:17)\u00b51, . . . ,(cid:17)\u00b5n)t is normal with mean \u00b5,\nof n(cid:17)\u03c3 2 \u223c \u03c3 2\u03c7 2\nfrom which it follows that\nand (\u03bd \u2212 2)\n\n(\u00b5 j \u2212(cid:17)\u00b5 j )2 = ((cid:17)\u00b5 \u2212 \u00b5)t((cid:17)\u00b5 \u2212 \u00b5) \u223c \u03c3 2\u03c7 2\n\np independent\n\u03bd variable and of its inverse are \u03bd\n\n(cid:18)\n\nn\u2212 p. now the expected values of a \u03c7 2\n\u22121, provided \u03bd > 2, and so (8.31) equals\nneg(log(cid:17)\u03c3 2) +\n\n+\n\nnp\n\nn2\n\nn \u2212 p \u2212 2\nn \u2212 p \u2212 2\nneg(log(cid:17)\u03c3 2) + n(n + p)\nn \u2212 p \u2212 2\n\n\u2212 n log \u03c3 2 \u2212 n,\n\n.\n\nor equivalently for our purposes,\n\nthis is estimated unbiasedly by the corrected information criterion\n\naicc = n log(cid:17)\u03c3 2 + n\n.= n log(cid:17)\u03c3 2 + n + 2( p + 1) + o( p2/n), and for large n and\nfixed p this will select the same model as aic = n log(cid:17)\u03c3 2 + 2 p. when p is comparable\n\nand the \u2018best\u2019 candidate model is taken to be that which minimizes this. taylor\nexpansion gives aicc\n\n1 \u2212 ( p + 2)/n\n\n1 + p/n\n\n,\n\nwith n, aicc penalizes model dimension more severely.\n\na widely used related criterion is\n\nc p = ssp\ns2\n\n+ 2 p \u2212 n,\n\nwhere ssp is the residual sum of squares for the fitted model and s2 is an estimate of\n\u03c3 2; c p can be derived as an approximation to aic (problem 8.16), though its original\nmotivation was different. in some cases \u03c3 2 can be estimated from the full model, but\ncare is needed because the choice of s2 is critical to successful use of c p.\nexample 8.30 (simulation study) twenty different n \u00d7 7 design matrices x\nwere constructed using standard normal variables, centered and scaled so that\n\n "}, {"Page_number": 416, "text": "404\n\n8 \u00b7 linear regression models\n\ntable 8.15 number of\ntimes models were\nselected using various\nmodel selection criteria in\n50 repetitions using\nsimulated normal data for\neach of 20 design\nmatrices. the true model\nhas p = 3.\n\nnumber of covariates\n\n1\n\n2\n\n3\n\n15\n\n131\n72\n52\n398\n\n4\n6\n2\n8\n\n504\n373\n329\n565\n\n673\n781\n577\n859\n\n712\n904\n673\n786\n\n4\n\n91\n97\n97\n18\n\n121\n104\n144\n94\n\n107\n56\n114\n105\n\n5\n\n63\n83\n91\n4\n\n88\n52\n104\n30\n\n73\n20\n90\n52\n\n6\n\n7\n\n83\n109\n125\n\n128\n266\n306\n\n61\n30\n76\n8\n\n66\n15\n69\n41\n\n53\n27\n97\n1\n\n42\n5\n54\n16\n\nn\n\n10\n\n20\n\n40\n\nc p\nbic\naic\naicc\n\nc p\nbic\naic\naicc\n\nc p\nbic\naic\naicc\n\neach column of x had mean zero and unit variance. the parameter vector was\n\u03b2 = (3, 2, 1, 0, 0, 0, 0)t, sothe true model had three covariates, and the errors were\ntaken to be independent standard normal variables. then the models with the first p\ncolumns of x were fitted for p = 1, . . . ,7, and the best of these was selected using\naic, aicc, the bayesian criterion bic, and c p. this procedure was performed\n50 times for each design matrix.\ntable 8.15 shows the results of this experiment. for n = 10 and 20, aicc has\nthe highest chance of selecting the true model, and moreover the models selected\nusing it are the least dispersed because of the stronger penalty applied, at least for p\ncomparable with n. forn = 40 the consistent criterion bic is most likely to select\nthe true model. in practice, however, the true model would rarely be among those\nfitted, and so aicc seems the best of the criteria considered, particularly when p is\n(cid:1)\ncomparable with n.\n\nexample 8.31 (nuclear plant data) when aicc is computed for the 210 possible\nmodels in example 8.29, the model chosen by backward elimination is selected, with\naicc = \u221271.24. two nearby models have aicc within 2 of the minimum, namely\nthose without log(n) and without pt, but dropping these covariates together increases\naicc sharply. the interpretation and overall fit are changed little by dropping them\n(cid:1)\nsingly, so we retain them.\n\nplots of the contributions to these criteria from individual observations can be\n\nuseful in diagnosing whether particular cases strongly influence model choice.\n\nthere may be several different models whose values of aicc are similarly low.\nif a single model is needed the choice among them should if possible be based on\nsubject-matter considerations. if there are several equally plausible models with quite\ndifferent interpretations, then it is important to say so.\n\n "}, {"Page_number": 417, "text": "8.7 \u00b7 model building\n\n405\n\nfigure 8.8 distribution\nof the supposed pivot z\nfor inference on the slope\nparameter in a\nstraight-line regression\nmodel, conditional on\ninclusion of slope in the\nmodel, for \u03b4 = 0, 1, . . . ,5\n(left to right) and testing\nfor inclusion at the 5%\nlevel. conditional on\ninclusion, z is\nnear-pivotal only if\n|\u03b4| (cid:11)0.\n\nz\n\n \nf\n\no\n\n \ny\nt\ni\ns\nn\ne\nd\n\n5\n\n.\n\n1\n\n0\n\n.\n\n1\n\n5\n\n.\n\n0\n\n0\n\n.\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\nz\n\ninference after model selection\none reason that automatic variable selection should if possible be avoided is its conse-\n(cid:18)\nquence for subsequent inference. to illustrate this, consider a straight-line regression\nmodel y = \u03b20 + x\u03b21 + \u03b5, based on n pairs (x j , y j ) with\nx j = 0 and independent\n(cid:17)\u03b21 is normally distributed with mean \u03b21 and variance v = \u03c3 2/\nnormal errors with mean zero and known variance \u03c3 2. then the least squares estimate\nthe discussion in section 8.3.2 we would base inference for \u03b21 on z = ((cid:17)\u03b21 \u2212 \u03b21)/v 1/2,\nx j , and following\n\n(cid:18)\n\nwhose distribution is standard normal when model selection is not taken into account.\nsuppose, however, that before attempting to construct a confidence interval for \u03b21,\nwe test for inclusion of the covariate x in the model, declaring that it should be in-\n\ncluded if |(cid:17)\u03b21/v 1/2| > z1\u2212\u03b1. ifnot, we declare that \u03b21 = 0 and use the simpler model\ny = \u03b20 + \u03b5. now as (cid:17)\u03b21 = \u03b21 + v 1/2 z, post-model selection inference for \u03b21 given\nthat x has been included will be based on the conditional density of z given that\n|z + \u03b21/v 1/2| > z1\u2212\u03b1, which is\n\n\u03c6\u03b4(z) = \u03c6(z){h(z < z\u03b1 \u2212 \u03b4) + 1 \u2212 h(z < \u2212z\u03b1 \u2212 \u03b4)}\n\n\u0001(z\u03b1 \u2212 \u03b4) + \u0001(z\u03b1 + \u03b4)\n\n, \u2212\u221e < z < \u221e,\n\nwhere \u03b4 = \u03b21/v 1/2 is the standardized slope. figure 8.8 displays \u03c6\u03b4(z) for \u03b4 =\n0, 1, . . . ,5 and \u03b1 = 0.025, corresponding to two-sided testing at the 5% level. when\n\u03b21 = 0, for example, z considered conditionally takes values in the tails of the stan-\ndard normal distribution but not in its centre. conditional on variable selection, z is\nclearly far from pivotal unless |\u03b4| (cid:11)0. hence it is only a sensible basis for inference\non \u03b21 if the regression on x is very strong.\n\nin practice there are three complications: the error variance \u03c3 2 is unknown, there are\ntypically many covariates, and the true model is not among those fitted. however the\nbroad conclusion applies: if variables are selected automatically, the only covariates\nfor which subsequent inference using the standard confidence intervals is reliable\nare those for which the evidence for inclusion is overwhelming, that is, for which it\nis clear that |\u03b4| (cid:11)0. other covariates should be considered in the light of previous\nknowledge and the context of the model.\n\nz1\u2212\u03b1 is the 1 \u2212 \u03b1 quantile\nof the standard normal\ndistribution.\n\nh(u) isthe heaviside\nfunction.\n\n "}, {"Page_number": 418, "text": "406\n\n8 \u00b7 linear regression models\n\nmodel uncertainty\ninference is often performed after comparing different competing models, and the\nquestions arise if, when, and how one should allow for this. consider for example the\nquantity \u03b20 in the two models m0 and m1 in which y = \u03b20 + \u03b5 and y = \u03b20 + x\u03b21 + \u03b5,\nwhere e(\u03b5) = 0. it is sometimes suggested that one should somehow average the\nvariances of the estimators (cid:17)\u03b20 across the models, but this is inappropriate because\n\nthe interpretation of \u03b20 is model-dependent. although the same symbol is used,\n\u03b20 represents the unconditional response mean e(y ) under m0, while under m1 it\nrepresents the conditional mean e(y | x = 0). hence the meaning of \u03b20 depends on\nthe context and inference for it must be conditioned on the model in which it appears:\naveraging is meaningless unless the quantity of interest has the same interpretation\nfor all models considered. in particular, the interpretation of regression coefficients\ntypically depends on the model in which they appear. having said this, one situation\nin which the quantity of interest has a model-free interpretation is prediction, and\nbelow we treat the simplest example of this.\nconsider using the fits of m0 and m1 to estimate the mean \u00b5+ = \u03b20 + x+\u03b21 of\na future variable y+ with covariate x+ (cid:6)= 0, assuming the error \u03b5 to be normal with\nmean zero and known variance \u03c3 2; note that \u00b5+ has the same interpretation under both\nx j = 0, so\nthat(cid:17)\u03b20 = y with variance \u03c3 2/n under either model, independent of the slope estimate\nmodels. suppose that n independent pairs (x j , y j ) are available and that\n(cid:17)\u03b21 with variance v = \u03c3 2/\nj . the estimators of \u00b5+ and their biases, variances, and\nx 2\n\n(cid:18)\n\n(cid:18)\n\nmean squared errors are\n\nmse\n\nmodel\nm0 :\n\nestimator\n\n(cid:17)\u00b50+ =(cid:17)\u03b20,\n\nbias\nx+\u03b21,\n\nvariance\n\u03c3 2/n,\n\n1\n\n,\n\n0,\n\n\u03c3 2/n + x 2+v,\n\nm1 : (cid:17)\u00b51+ =(cid:17)\u03b20 + x+(cid:17)\u03b21,\n\n\u03c3 2/n + x 2+\u03b22\n\u03c3 2/n + x 2+v,\nso(cid:17)\u00b50+ improves on(cid:17)\u00b51+ if |\u03b4| < 1, where \u03b4 = \u03b21/v 1/2 is the standardized slope.\nchoosing(cid:17)\u00b50+ if an estimator of \u03b4 is close enough to zero, and otherwise taking(cid:17)\u00b51+. if\nwe decide between the models on the basis that m1 is indicated when|(cid:17)\u03b21|/v 1/2 > z1\u2212\u03b1,\ncorresponding to a two-sided test of the hypothesis that \u03b21 = 0 at level (1 \u2212 2\u03b1), then\nthe overall estimator is\n\nthis suggests that it may be possible to construct a better estimator of \u00b5+ by\n\n(cid:17)\u00b5+ = (cid:17)\u03b20 + x+(cid:17)\u03b21\n\n(cid:12)(cid:20)\n= (cid:17)\u03b20 + x+v 1/2(\u03b4 + z){i (z < z\u03b1 \u2212 \u03b4) + i (z > z1\u2212\u03b1 \u2212 \u03b4)} ,\n\n(cid:11)(cid:17)\u03b21/v 1/2 < \u2212z1\u2212\u03b1\n\n(cid:11)(cid:17)\u03b21/v 1/2 > z1\u2212\u03b1\n\n(cid:12) + i\n\nwhere we have written(cid:17)\u03b21 = v 1/2(\u03b4 + z), with z = ((cid:17)\u03b21 \u2212 \u03b21)/v 1/2 a standard normal\nvariable; note that \u2212z1\u2212\u03b1 = z\u03b1. the bias and variance of(cid:17)\u00b5+ are\n\ne ((cid:17)\u00b5+ \u2212 \u00b5+) = x+v 1/2 e(q),\n\n+ x 2+v var(q),\nwhere q = (\u03b4 + z){i (z < z\u03b1 \u2212 \u03b4) + i (z > z1\u2212\u03b1 \u2212 \u03b4)} \u2212 \u03b4. asv = \u03c3 2/\nx 2\nj , the\n\u22121), while the mean squared error is \u03c3 2/n +\nbias is o(n\nx 2+v{e(q)2 + var(q)}. elementary calculations give the functions e(q), var(q), and\n\n\u22121/2) and the variance is o(n\n\nvar ((cid:17)\u00b5+) = \u03c3 2\n\n(cid:18)\n\n(cid:19)\n\nn\n\ni\n\ni (\u00b7) isthe indicator\nrandom variable of its\nevent.\n\n "}, {"Page_number": 419, "text": "8.7 \u00b7 model building\n\n407\n\nfigure 8.9 properties of\nestimators of \u03b20 + x+\u03b21\nin the straight-line\nregression model. left:\nbias (dots), variance\n(solid) and mean squared\nerror (dashes) for\n\nweighted estimator(cid:17)\u00b5w+.\n\nright: corresponding\nquantities for\nmodel-choice estimator\n\n(cid:17)\u00b5+. the weighted\n\nestimator improves\nconsiderably on the\nmodel-choice estimator.\nthe upper panels are for\ntheoretical calculations,\nand the lower ones for the\nsimulation experiment\ndescribed in\nexample 8.32.\n\ne\ns\nm\n\n \n,\n\ne\nc\nn\na\ni\nr\na\nv\n \n,\ns\na\nb\n\ni\n\ne\ns\nm\n\n \n,\n\ne\nc\nn\na\ni\nr\na\nv\n \n,\ns\na\nb\n\ni\n\n3\n\n2\n\n1\n\n0\n\n1\n-\n\n5\n1\n\n.\n\n0\n1\n\n.\n\n5\n\n.\n\n0\n\n0\n\n.\n\n0\n\n.\n\n5\n0\n-\n\n-4\n\n-2\n\n0\n\n2\n\n4\n\ndelta\n\ne\ns\nm\n\n \n,\n\ne\nc\nn\na\ni\nr\na\nv\n \n,\ns\na\nb\n\ni\n\ne\ns\nm\n\n \n,\n\ne\nc\nn\na\ni\nr\na\nv\n \n,\ns\na\nb\n\ni\n\n-4\n\n-2\n\n0\n\n2\n\n4\n\ndelta\n\n3\n\n2\n\n1\n\n0\n\n1\n-\n\n5\n1\n\n.\n\n0\n1\n\n.\n\n5\n\n.\n\n0\n\n0\n\n.\n\n0\n\n.\n\n5\n0\n-\n\n0.0\n\n0.4\n\n0.8\n\n1.2\n\n0.0\n\n0.4\n\n0.8\n\n1.2\n\ntau\n\ntau\n\ne(q)2 + var(q), which are shown in the upper right panel of figure 8.9 for \u03b1 =\nas we might have anticipated, (cid:17)\u00b5+ is generally biased towards zero because of the\n0.025, corresponding to choosing between the models at the two-sided 95% level.\npossibility of using the simpler estimator (cid:17)\u00b50+ even if \u03b21 (cid:6)= 0; its bias tends to zero\nwhen |\u03b4| (cid:11)0. the variance of (cid:17)\u00b5+ is largest when |\u03b4| .= 2, and then decreases to the\nlimit corresponding to use of(cid:17)\u00b51+.\none difficulty with (cid:17)\u00b5+ is that the indicator variables badly inflate its bias and\nvariance. a simple way to avoid this is to use a weighted combination of(cid:17)\u00b50+ and(cid:17)\u00b51+.\n\ntake for example the estimator\n\n(cid:17)\u00b5w+ = (1 \u2212 w )(cid:17)\u00b50+ + w(cid:17)\u00b51+ = (1 \u2212 w )(cid:17)\u03b20 + w ((cid:17)\u03b20 + x+(cid:17)\u03b21),\n\nwhere the weight\n\nw =\n\nexp(\u2212aic1/2)\n\nexp(\u2212aic1/2) + exp(\u2212aic0/2)\n\ndepends on the information criteria aic0 and aic1 for the two models. if aic1 (cid:12)\n.= (cid:17)\u00b51+. if on the\naic0, then w\nother hand \u03b21 = 0, then w slightly favours m0 but the estimators under both models\nare unbiased.\n\n.= 1, the data give a strong preference for m1, and(cid:17)\u00b5w+\n\n "}, {"Page_number": 420, "text": "408\n\n8 \u00b7 linear regression models\nunder our simplifying assumptions, aic0 \u2212 aic1 =(cid:17)\u03b22\n/v \u2212 2 = (\u03b4 + z)2 \u2212 2,\nand as (cid:17)\u00b5w+ =(cid:17)\u03b20 + x+w(cid:17)\u03b21, the quantity that corresponds to q above is qw =\n(\u03b4 + z)g{(\u03b4 + z)2/2 \u2212 1} \u2212\u03b4 , where g(u) = exp(u)/{1 + exp(u)}. the bias and\nvariance of (cid:17)\u00b5w+ depend on those of qw , which are shown in the upper left panel of\nfigure 8.9. both are smaller than the values for (cid:17)\u00b5+, and the mean squared error is\nconsiderably reduced. evidently(cid:17)\u00b5w+ improves on(cid:17)\u00b51+ over awide range of values of\n\u03b4, while its mean squared error is smaller than that of (cid:17)\u00b5+. the weighted estimator\n(cid:17)\u00b5w+ clearly improves on the model-choice estimator(cid:17)\u00b5+.\n\n1\n\nexample 8.32 (simulation study) to assess how this approach performs in a\nslightly more realistic setting, we performed a small simulation study with linear\nmodel data simulated in the same way as in example 8.30, now with n = 15 and\n\u03b2 t = \u03c4 (0, 4, 3, 2, 1, 1, 0, 0); thus p = 8 including a constant vector. we then fitted\nthe eight models with a constant only, constant plus the first covariate, constant plus\nand aic-based weights, to obtain a weighted estimator(cid:17)\u03b8 of \u03b8 = 1t\nfirst and second covariates, and so forth, and combined the corresponding estimators\nthis with the estimator(cid:17)\u03b8+ obtained from the \u2018best\u2019 model, this being chosen as the\n\u03b2. wecompared\nmodel minimizing\u22122(cid:17)(cid:8)q + 3.84q, where(cid:17)(cid:8)q is the log likelihood obtained when fitting\n\n8\n\nthe model with q parameters. this information criterion is constructed to give prob-\nability 0.05 of selecting the more complex of two nested models differing by one pa-\nrameter, when in fact the simpler model is correct. this criterion is intended to mimic\nhypothesis testing procedures for model selection, such as backward elimination.\nthis experiment was repeated with 20 different response vectors for each of\n250 design matrices: 5000 datasets, for \u03c4 = 0, 0.05, 0.1, 0.2, 0.4, . . . ,1.2. the lower\npanels of figure 8.9 show the bias, variance, and mean squared error of(cid:17)\u03b8 and(cid:17)\u03b8+. the\n\nresults bear out the preceding toy analysis: the weighted estimator has lower mean\n(cid:1)\nsquared error except when the regression effects are small.\n\nalthough we have only considered the simplest situation, our broad conclusion\ngeneralizes to more complex settings: sharp choices among estimators from different\nmodels tends to give worse predictions than do estimators interpolating smoothly\namong them.\n\nexercises 8.7\n1\n\nconsider the cement data of example 8.3, where n = 13. the residual sums of squares\nfor all models that include an intercept are given in exercise 8.5.1.\n(a) use forward selection, backward elimination, and stepwise selection to select models\nfor these data, including variables significant at the 5% level.\n(b) use c p to select a model for these data.\nanother criterion for model selection is to choose the covariates that minimize the cross-\nvalidated sum of squares\nthe jth case is deleted. show this is equivalent to minimizing\nand compare computational aspects of this approach with those based on aic.\n\n(cid:17)\u03b2\u2212 j )2, where(cid:17)\u03b2\u2212 j is the estimate of \u03b2 obtained when\n(cid:17)\u03b2)2/(1 \u2212 h j j )2,\n\n(y j \u2212 x t\n\n(y j \u2212 x t\n\n(cid:18)\n\n(cid:18)\n\nj\n\nj\n\n2\n\n "}, {"Page_number": 421, "text": "8.9 \u00b7 problems\n\n8.8 bibliographic notes\n\n409\n\nthere are books on all aspects of the linear model. seber (1977) and searle (1971) give\na thorough discussion of the theory, while draper and smith (1981), weisberg (1985),\nwetherill (1986) and rawlings (1988) have somwhat more practical emphases; see\nalso sen and srivastava (1990) and j\u00f8rgensen (1997a). most of these books cover the\ncentral topics of this chapter in more detail. scheff\u00b4e (1959) is a classic account of the\nanalysis of variance.\n\nrobust approaches to regression are described by li (1985), and in more detail in\n\nhuber (1981), hampel et al. (1986), and rousseeuw and leroy (1987).\n\ndavison and hinkley (1997) and efron and tibshirani (1993) give accounts of boot-\nstrap methods, which are simulation approaches to finding standard errors, confidence\nlimits and so forth, for use with awkward estimators.\n\nthe formal analysis of transformations was discussed by box and cox (1964) and\nfurther developed by many others; for book-length discussions see atkinson (1985)\nand carroll and ruppert (1988). the test for non-additivity was suggested by tukey\n(1949); see also hinkley (1985). books on general regression diagnostics include\ncook and weisberg (1982), belsley et al. (1980) and chatterjee and hadi (1988).\nbelsley (1991) focuses on problems of collinearity. shorter accounts of aspects of\nmodel-checking are davison and snell (1991) and davison and tsai (1992). atkinson\nand riani (2000) describe how diagnostic procedures may be used to give reliable\nstrategies for data analysis.\n\nstone and brooks (1990) and their discussants give numerous references and com-\nparison of various approaches to regression situations with fewer observations than\ncovariates, such as principal components regression and partial least squares. perhaps\nthe most widespread of these is ridge regression (hoerl and kennard, 1970a,b; hoerl\net al., 1985). brown (1993) is a book-length treatment of these and related methods.\nvariable selection for the linear model has been intensively studied. linhart and\nzucchini (1986) and miller (1990) give useful surveys, now somewhat dated owing\nto the considerable amount of work in the 1990s. model selection based on aic was\nsuggested by akaike (1973) in a much-cited paper, though related criteria such as\nc p were already in use (mallows, 1973). schwartz (1978) proposed use of bic, and\nhurvich and tsai (1989, 1991) derive the modified aic with improved small-sample\nproperties. mcquarrie and tsai (1998) give a comprehensive discussion of these and\nrelated criteria. p\u00a8otscher (1991) and hurvich and tsai (1990) give theoretical and\nnumerical results on inference after model selection in linear models. more general\ndiscussion and many further references may be found in chatfield (1995) and burnham\nand anderson (2002).\n\n8.9 problems\n1 consider table 8.16. formulate the design matrix x for the model in which e(yield) =\n\n\u03b2i + \u03b23(z \u2212 2), estimate the parameters and test whether \u03b21 = \u03b22.\n\n "}, {"Page_number": 422, "text": "410\n\n8 \u00b7 linear regression models\n\nlevel of fertilizer, z\n\nvariety\n\n0\n\n1\n\n2\n\n3\n\n4\n\n1\n2\n\n0.2\n0.1\n\n0.6\n0.2\n\n0.5\n0.4\n\n0.8\n0.6\n\n0.9\n0.7\n\ntable 8.16 rescaled\nyields (tonnes/ha) when\ntwo varieties of corn were\ntreated with five levels of\nfertiliser.\n\n2 suppose that random variables yg j , j = 1, . . . ,n g, g = 1, . . . , g, are independent and that\n\u03b2 + \u03b5g j . write down the covariate matrix for\nthey satisfy the normal linear model yg j = x t\n1 w z,\nthis model, and show that the least squares estimates can be written as (x t\nwhere w = diag{n1, . . . ,n g}, and the gth element of z is n\nj yg j . hence show that\nweighted least squares based on z and unweighted least squares based on y give the same\nparameter estimates and confidence intervals, when \u03c3 2 is known. why do they differ if\n\u03c3 2 is unknown, unless ng \u2261 1?\ndiscuss how the residuals for the two setups differ, and say which is preferable for model-\nchecking.\n\n1 w x1)\n\n\u22121 x t\n\n(cid:18)\n\n\u22121\ng\n\ng\n\n= \u03c3 2\nmodels is large when the ratio t = (cid:18)\n\n3 let y1, . . . ,y n and z1, . . . , zm be two independent random samples from the n (\u00b51, \u03c3 2\n1 )\nand n (\u00b52, \u03c3 2\n2 ) distributions respectively. consider comparison of the model in which\n\u03c3 2\n2 and the model in which no restriction is placed on the variances, with no restriction\n1\non the means in either case. show that the likelihood ratio statistic wp to compare these\n(z j \u2212 z)2 is large or small. show\nthat t is proportional to a random variable with the f distribution, and discuss whether\nthe model of equal variances is plausible for the maize data of example 1.1.\n\n(y j \u2212 y )2/\n\n(cid:18)\n\nline regression model (5.2).\n\n4 find the expected information matrix for the parameters (\u03b20, \u03b21, \u03c3 2) ofthe normal straight-\n5 the usual linear model y = x\u03b2 + \u03b5 is thought to apply to a set of data, and it is assumed that\nin terms of the usual least squares estimates and estimate of \u03c3 2,(cid:17)\u03b2 and s2. unknown to the\nthe \u03b5 j are independent with means zero and variances \u03c3 2, sothat the data are summarized\nunfortunate investigator, in fact var(\u03b5 j ) = v j \u03c3 2, and v1, . . . ,v n are unequal. show that(cid:17)\u03b2\n\nremains unbiased for \u03b2 and find its actual covariance matrix.\n\n6 suppose that y satisfies a quadratic regression, that is,\n\ny = \u03b20 + x\u03b21 + x 2\u03b22 + \u03b5,\n\nand that we can control the values of x. it isdecided to choose x = \u00b1a r times each and\nx = 0 n \u2212 2r times.\n(a) derive explicit expressions for the least squares estimates. are they uncorrelated? if\nnot, can they easily be made so?\n(b) what value of r is best if we intend to test for the adequacy of a linear regression?\n(c) what value of r is best if we intend to predict y at x = a/2?\n\n7 by rewriting y \u2212 x\u03b2 as e + x(cid:17)\u03b2 \u2212 x\u03b2 and that et x = 0, show that\n\nhence show that that the likelihood for the normal linear model equals\n\n(y \u2212 x\u03b2)t(y \u2212 x\u03b2) = ss((cid:17)\u03b2) + ((cid:17)\u03b2 \u2212 \u03b2)t x t x((cid:17)\u03b2 \u2212 \u03b2).\n(cid:13)\n2\u03c3 2 ((cid:17)\u03b2 \u2212 \u03b2)t x t x((cid:17)\u03b2 \u2212 \u03b2)\n\n\u2212 ss((cid:17)\u03b2)\n\n(2\u03c0)n/2\u03c3 n exp\n\n\u2212 1\n\n2\u03c3 2\n\n(cid:10)\n\n1\n\n,\n\nand use the factorization criterion to establish that ((cid:17)\u03b2, ss((cid:17)\u03b2)) is a minimal sufficient\n\nstatistic for (\u03b2, \u03c3 2). the sample size n and the covariate matrix x are also needed to\ncalculate the likelihood, so why are they not regarded as part of the minimal sufficient\nstatistic?\n\n "}, {"Page_number": 423, "text": "8.9 \u00b7 problems\n411\n8 consider a normal linear regression y = \u03b20 + \u03b21x + \u03b5 in which the parameter of interest\nis \u03c8 = \u03b20/\u03b21, to beestimated by (cid:17)\u03c8 =(cid:17)\u03b20/(cid:17)\u03b21; let var((cid:17)\u03b20) = \u03c3 2v00, cov((cid:17)\u03b20,(cid:17)\u03b21) = \u03c3 2v01\nand var((cid:17)\u03b21) = \u03c3 2v11.\n\n(a) show that\n\n(cid:17)\u03b20 \u2212 \u03c8(cid:17)\u03b21\n\nand hence deduce that a (1 \u2212 2\u03b1) confidence interval for \u03c8 is the set of values of \u03c8\n(cid:20) \u2264 0.\nsatisfying the inequality\n\n{s2(v00 \u2212 2\u03c8v01 + \u03c8 2v11)}1/2\n(cid:20) + \u03c8 2\n\nn\u2212 p(\u03b1)v01 \u2212 \u03b20\u03b21\n\nn\u2212 p(\u03b1)v00 + 2\u03c8\n\n\u2212 s2t 2\n\n\u2212 s2t 2\n\nn\u2212 p(\u03b1)v11\n\n(cid:19)(cid:17)\u03b22\n\n(cid:17)\u03b22\n\ns2t 2\n\n(cid:19)\n\n1\n\n0\n\n\u223c tn\u2212 p,\n\nhow would this change if the value of \u03c3 was known?\n(b) by considering the coefficients on the left-hand-side of the inequality in (a), show that\nthe confidence set can be empty, a finite interval, semi-infinite intervals stretching to \u00b1\u221e,\nthe entire real line, two disjoint semi-infinite intervals \u2014 six possibilities in all. in each\ncase illustrate how the set could arise by sketching a set of data that might have given rise\nto it.\n(c) a government department of fisheries needed to estimate how many of a certain\nspecies of fish there were in the sea, in order to know whether to continue to license\ncommercial fishing. each year an extensive sampling exercise was based on the numbers\nof fish caught, and this resulted in three numbers, y, x, and a standard deviation for y, \u03c3 .\na simple model of fish population dynamics suggested that y = \u03b20 + \u03b21x + \u03b5, where the\nerrors \u03b5 are independent, and the original population size was \u03c8 = \u03b20/\u03b21. tosimplify the\ncalculations, suppose that in each year \u03c3 equalled 25. if the values of y and x had been\n\ny : 160 150 100\nx : 140 170 200\n\n80\n100\n230 260\n\nafter five years, give a 95% confidence interval for \u03c8. doyou find it plausible that \u03c3 = 25?\nif not, give an appropriate interval for \u03c8.\n9 over aperiod of 2m + 1 years the quarterly gas consumption of a particular household\n\nmay be represented by the model\n\ni = 1, . . . ,4, j = \u2212m,\u2212m + 1, . . . ,m \u2212 1, m,\n\nyi j = \u03b2i + \u03b3 j + \u03b5i j ,\n(cid:18)\nwhere the parameters \u03b2i and \u03b3 are unknown, and \u03b5i j\nsquares estimators and show that they are independent with variances (2m + 1)\nm\n\u03c3 2/(8\ni=1 i 2).\n\uf8f9\nshow also that\n\uf8fb\n\n\uf8ee\n\uf8f0 4(cid:15)\n\niid\u223c n (0, \u03c3 2). find the least\n\u22121\u03c3 2 and\n\n. j\n\n(8m \u2212 1)\n\u22121\n\ni\u00b7 \u2212 2\ny 2\n\nm(cid:15)\nj=\u2212m\n\ny 2\ni j\n\ni=1\n\n4(cid:15)\ni=1\n\n\u2212 (2m + 1)\n(cid:18)\n\nis unbiased for \u03c3 2, where y i\u00b7 = (2m + 1)\n\n\u22121\n\nm\n\nj=\u2212m yi j and y \u00b7 j = 1\n\nm\n\n(cid:18)\n(cid:18)\nj=\u2212m jy 2\nm\ni=1 i 2\n(cid:18)\n4\ni=1 yi j .\n\n4\n\n10 a statistician travels regularly from a to b by one of four possible routes, each route\ncrossing a river bridge at r. the times taken for the possible segments of the journey are\nindependent random variables with means as shown in the figure, each having variance\n\u03c3 2/2.\n\na\n\n\u03b12\n\n\u03b11\n\nr\n\n\u03b22\n\n\u03b21\n\nb\n\n "}, {"Page_number": 424, "text": "412\n\n8 \u00b7 linear regression models\n\nmodel\n\nss\n\nmodel\n\nss\n\nmodel\n\nss\n\n- - - -\n1 - - -\n- 2 - -\n- - 3 -\n- - - 4\n\n11.06\n5.96\n10.19\n9.96\n9.09\n\n1 2 - -\n1 - 3 -\n1 - - 4\n- 2 3 -\n- 2 - 4\n- - 3 4\n\n5.56\n4.78\n1.34\n8.09\n7.94\n6.51\n\n1 2 3 -\n1 2 - 4\n1 - 3 4\n- 2 3 4\n\n4.75\n0.74\n0.83\n3.05\n\n1 2 3 4\n\n0.69\n\ntable 8.17 residual\nsums of squares for fits of\nlinear models to output\nfrom n = 10 runs of a\nhydrological model.\n\nhe times the complete journey once by each route, obtaining observations yi j distributed\nas random variables yi j having means e(yi j ) = \u03b1i + \u03b2 j , for i, j = 1, 2. why it is not\npossible to estimate all the parameters from these observations?\nnow define \u00b5 = \u03b11 + \u03b21, \u03b3 = \u03b12 \u2212 \u03b11 and \u03b4 = \u03b22 \u2212 \u03b21. obtain expressions for the least\nsquares estimates of \u00b5, \u03b3 and \u03b4 and also for their variance matrix.\nif the observed vector of times is (y11, y21, y12, y22) = (124, 120, 128, 136) minutes, de-\ntermine which route has the smallest estimated mean time. obtain a 90% confidence\ninterval for the mean on the assumption that the times are normally distributed.\n11 suppose that we wish to construct the likelihood ratio statistic for comparison of the two\nlinear models y = x1\u03b21 + \u03b5 and y = x1\u03b21 + x2\u03b22 + \u03b5, where the components of \u03b5 are\nindependent normal variables with mean zero and variance \u03c3 2; call the corresponding\nresidual sums of squares ss1 and ss on \u03bd1 and \u03bd degrees of freedom.\n(a) show that the maximum value of the log likelihood is \u2212 1\n2 n(log ss + 1 \u2212 log n) for a\nmodel whose residual sum of squares is ss, and deduce that the likelihood ratio statistic\nfor comparison of the models above is w = n log(ss1/ss).\n(b) by writing ss1 = ss + (ss1 \u2212 ss), show that w is a monotonic function of the f\nstatistic for comparison of the models.\n.= (\u03bd1 \u2212 \u03bd)f when n is large and \u03bd is close to n, and say why f would\n(c) show that w\n12 suppose that the denominator in the f statistic was replaced by ss((cid:17)\u03b21)/(n \u2212 q), giving f\nusually be preferred to w .\n(cid:2)\n,\ndoes not have an f distribution,\nn\u2212q. show that f\nis a monotone\nincreasing function of f, that tends to be less than f if the simpler model is not adequate.\n13 table 8.17 gives results from n = 10 runs of a computer experiment to assess the accuracy\nof a hydrological model. the response y is the relative accuracy of predictions, and the\ncovariates x1, x2, x3, and x4 represent parameters input to the model. the table gives the\nresidual sums of squares for all normal linear models that include an intercept and the x j .\ntaking the level of significance to be 5%, select models for the data using (a) forward\nselection, (b) backward elimination, (c) stepwise model selection starting from the full\nmodel, and (d) c p. comment briefly.\nin the normal straight-line regression model it is thought that a power transformation of\nthe covariate may be needed, that is, the model\n\neven if the simpler model is correct so that ss((cid:17)\u03b21) \u223c \u03c3 2\u03c7 2\n\nsay. use the geometry of least squares to explain why f\n\n14\n\n(cid:2)\n\n(cid:2)\n\ny = \u03b20 + \u03b21x (\u03bb) + \u03b5\nmay be suitable, where x (\u03bb) is the power transformation\n\u03bb (cid:6)= 0,\nx \u03bb\u22121\nlog x, \u03bb = 0.\n\nx (\u03bb) =\n\n(cid:10)\n\n,\n\n\u03bb\n\n(a) show by taylor series expansion of x (\u03bb) at \u03bb = 1 that a test for power transformation\ncan be based on the reduction in sum of squares when the constructed variable x log x is\nadded to the model with linear predictor \u03b20 + \u03b21x.\n\n "}, {"Page_number": 425, "text": "8.9 \u00b7 problems\n\n2 log ss((cid:17)\u03b2\u03bb),\n(b) show that the profile log likelihood for \u03bb is equivalent to (cid:8)p(\u03bb) \u2261 \u2212 n\nwhere ss((cid:17)\u03b2\u03bb) isthe residual sum of squares for regression of y on the n \u00d7 2 design matrix\n\nwith a column of ones and the column consisting of the x (\u03bb)\nj\ntransformation not needed in this case, unlike in example 8.23?\n(box and tidwell, 1962)\n\n15 consider model y = x1\u03b21 + x2\u03b22 + \u03b5, which leads to least squares estimates\n\n. why is a jacobian for the\n\n413\n\n(cid:7)\n\n(cid:28)\n\n(cid:27)(cid:17)\u03b21(cid:17)\u03b22\n\n=\n\n(cid:8)\u22121(cid:7)\n\n(cid:8)\n\n.\n\n1 y\nx t\nx t\n2 y\n\nx t\nx t\n\n1 x1 x t\n2 x1 x t\n\n1 x2\n2 x2\n\n(cid:11)\n\nx t\n\nx t\n\nx t\n\n2 x1\n\n2 x2\n\n2 p1 x2\n\n1 p2 x1\n\n\u22121 x t\n\n2 p1 x2)\n\n2 p1 x2)\n\n2 p1 y, with vari-\n\n(cid:12)\u22121 x t\n\n(cid:12)\u22121 x t\n1 y,\n\u22121 x t\n\nlet h1 = x1(x t\n1 x1)\n(a) show that(cid:17)\u03b22 can be expressed as\nprojection matrices are symmetric and idempotent.\n(cid:11)\n2 y \u2212(cid:11)\nand use the result from exercise 8.5.3 to deduce that(cid:17)\u03b22 = (x t\n\n1, p1 = in \u2212 h1, and define h2 and p2 similarly; notice that these\n(cid:12)\u22121 x t\n\u22121. note that(cid:17)\u03b22 is the parameter estimate from the regression of\n\n16 (a) show that aic for a normal linear model with n responses, p covariates and unknown\n\nance matrix \u03c3 2(x t\np1 y on the columns of p1 x2.\n(b) use the geometry of least squares to show that the residual sums of squares for\nregression of y on x1 and x2 is the same as for the regression of p1 y on x1 and x2.\n(c) suppose that in a normal linear model, x2 is a single column that depends on y only\nthrough the fitted values from regression of y on x1, sothat x2 is itself random. noting\nthat the residuals p1 y are independent of the fitted values, h1 y, and arguing conditionally\n\non h1 y, show that the t statistic for(cid:17)\u03b22 has a distribution that is independent of x2. hence\n\u03c3 2 may be written as n log(cid:17)\u03c3 2 + 2 p, where(cid:17)\u03c3 2 = ssp/n is the maximum likelihood esti-\nmate of \u03c3 2. if(cid:17)\u03c3 2\nshow that use of aic is equivalent to use of n log{1 + ((cid:17)\u03c3 2 \u2212(cid:17)\u03c3 2\n0 is the unbiased estimate under some fixed correct model with q covariates,\n} +2 p, and that this is\nroughly equal to n((cid:17)\u03c3 2/(cid:17)\u03c3 2\n\u2212 1) + 2 p. deduce that model selection using c p approximates\nthat using aic.\n(b) show that c p = (q \u2212 p)(f \u2212 1) + p, where f is the f statistic for comparison of the\nmodels with p and q > p covariates, and deduce that if the model with p covariates is\ncorrect, then e(c p)\n17 consider the straight-line regression model y j = \u03b1 + \u03b2x j + \u03c3 \u03b5 j , j = 1, . . . ,n . suppose\nx j = 0 and that the \u03b5 j are independent with means zero, variances \u03b5, and common\n\n.= q, but that otherwise e(c p) > q.\n\ngive the unconditional distribution of (8.27).\n\n0 )/(cid:17)\u03c3 2\n\n(cid:18)\n\n0\n\n0\n\nthat\ndensity f (\u00b7).\n(a) write down the variance of the least squares estimate of \u03b2.\n(b) show that if \u03c3 is known, the log likelihood for the data is\n\n(cid:8)(\u03b1, \u03b2) = \u2212n log \u03c3 + n(cid:15)\n\nlog f\n\nj=1\n\n(cid:7)\n\n(cid:8)\n\n,\n\ny j \u2212 \u03b1 \u2212 \u03b2x j\n(cid:18)\n\n\u03c3\n\n(cid:10)\n\ni = e\n\n\u2212 d 2 log f (\u03b5)\n\nd\u03b52\n\n(cid:13)\n\n.\n\nderive the expected information matrix for \u03b1 and \u03b2, and show that the asymptotic variance\nof the maximum likelihood estimate of \u03b2 can be written as \u03c3 2/(i\n\nx 2\nj ), where\n\nrecall that a model is\ncalled correct if it contains\nall covariates with\nnon-zero coefficients, and\ncalled true if it contains\nprecisely these covariates.\n\n\u0001(t) = ( \u221e\nwith\n(1) \u2212 \u0001(cid:2)\n\u0001(cid:2)(cid:2)\n1.64493.\n\n\u2212u du,\n\n0 ut\u22121e\n(1)2 .=\n\nhence show that the the least squares estimate of \u03b2 has asymptotic relative efficiency\ni /v \u00d7 100%.\nf (u) =\n(c) show that the cumulant-generating function of the gumbel distribution,\nexp{\u2212u \u2212 exp(\u2212u)}, \u2212\u221e < u < \u221e, islog \u0001(1 \u2212 t), and deduce that its variance is\nroughly 1.65. find i for this distribution, and show that the asymptotic relative efficiency\nof least squares is about 61%.\n\n "}, {"Page_number": 426, "text": "414\n\n8 \u00b7 linear regression models\n\n18 over a period of 90 days a study was carried out on 1500 women. its purpose was to\ninvestigate the relation between obstetrical practices and the time spent in the delivery\nsuite by women giving birth. one thing that greatly affects this time is whether or not a\nwoman has previously given birth. unfortunately this vital information was lost, giving\nthe researchers three options: (a) abandon the study; (b) go back to the medical records\nand find which women had previously given birth (very time-consuming); or (c) for each\nday check how many women had previously given birth (relatively quick). the statistical\nquestion arising was whether (c) would recover enough information about the parameter\nof interest.\nsuppose that a linear model is appropriate for log time in delivery suite, and that the\nlog time for a first delivery is normally distributed with mean \u00b5 + \u03b1 and variance \u03c3 2,\nwhereas for subsequent deliveries the mean time is \u00b5. suppose that the times for all\nthe women are independent, and that for each there is a probability \u03c0 that the labour\nis her first, independent of the others. further suppose that the women are divided into\nk groups corresponding to days and that each group has size m; the overall number is\nn = mk. under (c), show that the average log time on day j, z j , isnormally distributed\nwith mean \u00b5 + r j \u03b1/m and variance \u03c3 2/m, where r j is binomial with probability \u03c0 and\ndenominator m. hence show that the overall log likelihood is\n\n(cid:8)(\u00b5, \u03b1) = \u2212 1\n2\n\nk log(2\u03c0 \u03c3 2/m) \u2212 m\n2\u03c3 2\n\n(z j \u2212 \u00b5 \u2212 r j \u03b1/m)2,\n\nk(cid:15)\nj=1\n\nwhere z j and r j are the observed values of z j and r j and we take \u03c0 and \u03c3 2 to be\nknown. if r j has mean m\u03c0 and variance m\u03c4 2, show that the inverse expected information\nmatrix is\n\n(cid:27)\n\ni (\u00b5, \u03b1)\n\n\u22121 = \u03c3 2\nn\u03c4 2\n\nm\u03c0 2 + \u03c4 2 \u2212m\u03c0\n\u2212m\u03c0\nm\n\n(cid:28)\n\n.\n\n(i) if m = 1, \u03c4 2 = \u03c0(1 \u2212 \u03c0), and \u03c0 = n1/n, where n = n0 + n1, show that i (\u00b5, \u03b1)\n\u22121\nequals the variance matrix for the two-sample regression model. explain why.\n(ii) if \u03c4 2 = 0, show that neither \u00b5 nor \u03b1 is estimable; explain why.\n(iii) if \u03c4 2 = \u03c0(1 \u2212 \u03c0), show that \u00b5 is not estimable when \u03c0 = 1, and that \u03b1 is not estimable\nwhen \u03c0 = 0 or\u03c0 = 1. explain why the conditions for these two parameters to be estimable\n(iv) show that the effect of grouping, (m > 1), is that var((cid:17)\u03b1) isincreased by a factor m\ndiffer in form.\n.= 0.3. calculate the standard error for(cid:17)\u03b1.\nregardless of \u03c0 and \u03c3 2.\n(v) it was known that \u03c3 2 .= 0.2, m\nit was known from other studies that first deliveries are typically 20\u201325% longer than\nsubsequent ones. show that an effect of size \u03b1 = log(1.25) would be very likely to be\ndetected based on the grouped data, but that an effect of size \u03b1 = log(1.20) would be less\ncertain to be detected, and discuss the implications.\n19 suppose that model y = x\u03b2 + z \u03b3 + \u03b5 holds, but that model y = x\u03b2 + \u03b5 is fitted, giving\n(cid:17)\u03b2 = (x t x)\n\u22121 x t y with hat matrix h = x(x t x)\n\n\u22121 x t and residuals e = y \u2212 x(cid:17)\u03b2.\n\n.= 1500/90, \u03c0\n\n(a) show that\n\ne = (i \u2212 h)y = (i \u2212 h)z \u03b3 + (i \u2212 h)\u03b5,\n\nand hence that e(e) = (i \u2212 h)z \u03b3 . what happens if z lies in the space spanned by the\ncolumns of x?\n(b) now suppose that z is a single column z. explain how an added variable plot of the\nresiduals from the regression of y on x against the residuals from the regression of z on\nx can help in deciding whether or not to add z to the design matrix.\n(c) discuss the interpretation of the added variable plots in figure 8.10, bearing in mind\nthe possibility of outliers and of a need to transform z before including it in the design\nmatrix.\n\n "}, {"Page_number": 427, "text": "8.9 \u00b7 problems\n\n415\n\nfigure 8.10 added\nvariable plots for four\nnormal linear models.\n\ny\n \nm\no\nr\nf\n \nl\na\nu\nd\ns\ne\nr\n\ni\n\ny\n \nm\no\nr\nf\n \nl\n\ni\n\na\nu\nd\ns\ne\nr\n\n4\n\n2\n\n0\n\n2\n-\n\n4\n-\n\n4\n\n2\n\na\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\nb\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\u2022\n\n\u2022\n\n\u2022\n\ny\n \nm\no\nr\nf\n \nl\na\nu\nd\ns\ne\nr\n\ni\n\n4\n\n2\n\n0\n\n2\n-\n\n4\n-\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n-15 -10 -5\n\n0\n\n5\n\n10\n\n15\n\n-15 -10 -5\n\n0\n\n5\n\n10\n\n15\n\nresidual from z\n\nresidual from z\n\nc\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\ny\n \nm\no\nr\nf\n \nl\n\ni\n\na\nu\nd\ns\ne\nr\n\n4\n\n2\n\n0\n\n2\n-\n\n4\n-\n\nd\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n0\n\n\u2022\n\n2\n-\n\n4\n-\n\n-15\n\n-5\n\n0\n\n5\n\n10 15\n\n-2\n\n-1\n\n0\n\n1\n\nresidual from z\n\nresidual from z\n\n20 figure 8.11 shows standardized residuals plotted against fitted values for linear models\nfitted to four different sets of data. in each case discuss the fit and explain briefly how you\nwould try to remedy any deficiencies.\n\n21 data (x1, y1), . . . ,( xn, yn) satisfy the straight-line regression model (5.3). in a calibration\nproblem the value y+ of a new response independent of the existing data has been observed,\nand inference is required for the unknown corresponding value x+ of x.\n(x j \u2212 x)2 and let s2 be the unbiased estimator of the error variance \u03c3 2.\n(a) let s2\nx\nshow that\n\n= (cid:18)\n\nt (x+) =\n\n(cid:19)\n\ny+ \u2212(cid:17)\u03b30 \u2212(cid:17)\u03b31(x+ \u2212 x)\n1 + n\u22121 + (x+ \u2212 x)2/s2\n\nx\n\n)\n\ns2\n\n(cid:20)*1/2\n\nis a pivot, and explain why the set\n\nx1\u22122\u03b1 = {x+ : tn\u22122(\u03b1) \u2264 t (x+) \u2264 tn\u22122(1 \u2212 \u03b1)}\n\ncontains x+ with probability 1 \u2212 2\u03b1.\n(b) show that the function g(u) = (a + bu)/(c + u2)1/2, c > 0, a, b (cid:6)= 0, has exactly one\nstationary point, at \u02dcu = \u2212bc/a, that sign g(\u02dcu) = sign a, that g(\u02dcu) is alocal maximum if\n\n "}, {"Page_number": 428, "text": "figure 8.11\nstandardized residuals\nplotted against fitted\nvalues for four normal\nlinear models.\n\n416\n\nl\n\ni\n\na\nu\nd\ns\ne\nr\n \nd\ne\nz\nd\nr\na\nd\nn\na\nt\ns\n\ni\n\nl\n\na\nu\nd\ns\ne\nr\n \n\ni\n\ni\n\nd\ne\nz\nd\nr\na\nd\nn\na\ns\n\nt\n\n4\n\n2\n\n0\n\n2\n-\n\n4\n-\n\n4\n\n2\n\n0\n\n2\n-\n\n4\n-\n\n8 \u00b7 linear regression models\n\na\n\nb\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\n\nl\n\ni\n\na\nu\nd\ns\ne\nr\n \nd\ne\nz\nd\nr\na\nd\nn\na\nt\ns\n\ni\n\n4\n\n2\n\n0\n\n2\n-\n\n4\n-\n\n\u2022\n\n\u2022\u2022\n\n\u2022\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n1.0\n\n1.5\n\n2.0\n\n2.5\n\n0.5\n\n1.0\n\n1.5\n\n2.0\n\n2.5\n\nfitted value\n\nc\n\nfitted value\n\nd\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\u2022\u2022\u2022\n\n\u2022\u2022\u2022\u2022\n\n\u2022\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nl\n\na\nu\nd\ns\ne\nr\n \n\ni\n\ni\n\nd\ne\nz\nd\nr\na\nd\nn\na\ns\n\nt\n\n4\n\n2\n\n0\n\n2\n-\n\n4\n-\n\n\u2022\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n2\n\n3\n\n4\n\n5\n\n6\n\n0\n\n1\n\n2\n\n3\n\nfitted value\n\nfitted value\n\na > 0 and a local minimum if a < 0, and that limu\u2192\u00b1\u221e g(u) = \u2213b. hence sketch g(u) in\nthe four possible cases a, b < 0, a, b > 0, a < 0 < b and b < 0 < a.\n(c) by setting u = s(x+ \u2212 x)/sx , show that t (x+) can be written in form g(u). deduce that\nx1\u22122\u03b1 can be a finite interval, two semi-infinite intervals or the entire real line. discuss.\n(d) show that if in fact \u03b31 = 0, x1\u22122\u03b1 has infinite length with probability 1 \u2212 2\u03b1.\n(e) a different approach considers x+ to be an unknown parameter, and constructs the\nlikelihood for \u03b2, \u03c3 2 and x+ based on the pairs (x j , y j ) and y+. does the resulting profile\nlog likelihood (cid:8)p(x+) result in confidence sets such as those in (c)?\n\n "}, {"Page_number": 429, "text": "9\n\ndesigned experiments\n\na carefully planned investigation can give much more insight into the question at\nhand than a haphazard one, data from which may be useless. experimental de-\nsign is a highly developed subject, though its principles are not universally appreci-\nated. in this chapter we outline some basic ideas and describe some simple designs\nand associated analyses. the first section discusses the importance of randomiza-\ntion, and shows how it can be used to justify standard linear models and how it\nstrengthens inferences. section 9.2 then describes some common designs and analy-\nses. interaction, contrasts and analysis of covariance are discussed in section 9.3.\nsection 9.4 then outlines the consequences of having more than one level of\nvariability.\n\n9.1 randomization\n9.1.1 randomization\nthe purpose of a designed experiment is to compare how treatments affect a re-\nsponse, by applying them to experimental units, oneach of which the response is\nto be measured. the units are the raw material of the investigation; formally a unit\nis the smallest subdivision of this such that any two different units might receive\ndifferent treatments. the treatments are clearly defined procedures one of which is\nto be applied to each experimental unit. in an agricultural field trial the treatments\nmight be different amounts of nitrogen and potash, while a unit is a plot of land. in a\nmedical setting, treatments might be types of operation and different therapies, with\nunits being patients who are operated upon and then given therapy to aid recovery. in\neach case our concern is how the response depends on the treatment combinations and\nother measurable quantities. the response must be carefully defined and measured in\na consistent way for every unit.\nsuppose for illustration that we wish to assess the effect of a drug in reducing blood\npressure, and that n = 2m individuals are available. we plan to administer the drug to\nm of the individuals, the treatment group, and to give a placebo to the remaining m,\n\n417\n\n "}, {"Page_number": 430, "text": "418\n\n9 \u00b7 designed experiments\n\nthe control group. the response is to be the blood pressure of an individual measured a\nfixed time after the drug has first been administered. we calculate the average changes\nfor the treated and control groups, y1 and y0, observe that y1 \u2212 y0 is significantly less\nthan zero, and declare that the drug plays an effect in reducing blood pressure. is this\nheadline news? no!\n\nfigure 9.1 directed\nacyclic graphs showing\nconsequences of\nrandomization. an arrow\nfrom t to y indicates\ndependence of y on t ,\nand so forth. in general\nboth response y and\ntreatment t may depend\non properties u of units\n(upper left).\nrandomization (lower\nleft) makes treatments and\nunits independent, so any\nobserved dependence of y\non t cannot be ascribed to\njoint dependence on u .\nthe upper right graph\nshows the general\ndependence of y , t , and\ncovariates x on u .\nrandomization makes t\nand u independent,\nconditional on x (lower\nright), so any influence of\nu on t is mediated\nthrough x, for which\nadjustment is possible in\nprinciple. thus having\nadjusted for x,\ndependence of y on t\ncannot be due to u .\n\na key difficulty is that the procedure does not avoid biased allocation of treatments\nto units. for example, if the control group mostly consisted of those patients with\nhigher blood pressures at the start of the study, y1 and y0 might differ greatly even if\nthe treatment had been ineffective. this particular source of bias could be avoided if\nthe experimenter measured the initial blood pressures and deliberately balanced the\ngroups with respect to them, but unknown causes of bias could not be removed in this\nway, and the interpretation of the results would rely on the uncheckable assertion that\nthe experiment was also balanced with respect to these unknown factors. any deter-\nministic allocation scheme will have this flaw, and we turn instead to randomization.\nby allocating treatments to patients at random, we expect to equalize the effect of any allocation at random\nfactors that might affect the response, other than the treatment itself. we can then be\nsurer that a significant difference between the groups is related to the treatment itself.\nto explain randomization differently, let t represent the treatment, y the response,\nand u properties of units \u2014 potential sources of bias. for example, left to their\nown devices physicians might be tempted to allocate a promising but untested new\ntreatment to patients most severely affected by a disease, and an existing treatment\nto less severe cases. then treatment t would depend on an attribute of the units,\ndisease severity u ; the response y might depend on both t and u . this is shown\nby the directed acyclic graph in the upper left part of figure 9.1. in general both\nt and y depend on u , so any apparent relation between y and t may be ascribed\nto u . randomization induces independence between properties of the unit and any\ntreatment allocation, making t independent of u and the lower left graph appropriate:\nalthough u may influence the response y , itcannot entirely explain any dependence\non t unless the randomization is compromised, for example by allocating all men\nto one group and all women to the other purely by chance. if this has not happened,\n\nmeans that some physical\ndevice has been used, not\nthat the experimenter has\nmade a choice that\nappears haphazard.\n\n "}, {"Page_number": 431, "text": "9.1 \u00b7 randomization\n\n419\n\nthen a highly significant effect of t implies either that treatment works, or that a rare\nevent has occurred.\n\nif randomization had been used and if a normal linear model was suitable, inference\n\ncould be based on the two-sample model of example 8.9, using\n\nz = y1 \u2212 y0\n\n(9.1)\n(cid:1)\nt, j (yt j \u2212 yt )2 is the pooled estimate of error and yt j is the\nwhere s2 = (2m \u2212 2)\nresponse for the jth individual in treatment group t. in fact randomization gives a\nbasis for the use of this and other linear models, as we shall see below.\n\n(2s2/m)1/2\n\n\u22121\n\n,\n\nblocking\nthe design outlined above presupposes that the units are fairly homogeneous, that\nis, any variation among blood pressures of different patients is small enough for\nthe design to be completely randomized. however, if the treatment effect was small\nrelative to this variation, s2 would be inflated because the division into groups made\nno allowance for it. the larger is s2, the smaller is z for given y1 \u2212 y0, and this\nmakes it harder to detect any treatment effect. this suggests that we should subdivide\nthe patients into groups whose initial blood pressures are as alike as possible, and\nallocate the treatment randomly within these groups, a procedure known as blocking.\nas the purpose of our experiment is to compare one treatment with the control,\nwe divide the patients into m blocks of two individuals with similar initial blood\npressures, and randomly allocate one of each pair to the treatment and the other to the\ncontrol, in a paired comparison. inthe corresponding normal linear model, discussed\nin example 8.10, analysis is based on the differences d j between the treated and control\nindividuals in the jth block, leading to confidence statements using the standardized\ndifference given by\n\nzd =\n\n(cid:2)\n\n(cid:3)1/2\n\n,\n\nd\n\n/m\n\n= (m \u2212 1)\n\n\u22121\n\ns2\nd\n\ns2\nd\n\nm(cid:4)\nj=1\n\n(d j \u2212 d)2,\n\n(9.2)\n\nwhere d = y1 \u2212 y0 is the average difference between pairs. the numerator of zd is\nthe same as that of (9.1), but the denominator may be substantially smaller if the\nblocking has been effective in increasing the precision of the experiment. although\nhere the matching is performed deliberately, randomization is still involved in the\ntreatment allocations.\n\nthis line of reasoning suggests taking as response for each patient the difference\nbetween his initial blood pressure and that after treatment, so the comparisons are\nmade entirely within individuals, allocated randomly to treatment or control. we\nignore this design below, however, purely for purposes of exposition.\n\nthe right half of figure 9.1 shows the effect of randomization when treatment\nallocation can depend on a covariate, x. for example, randomization might take into\naccount knowledge that certain treatments should not be given to patients taking other\nmedication. in general t might depend on unknown properties u of the unit as well\nas on x, sothat y and t depend on both x and u . randomization breaks the direct\n\n "}, {"Page_number": 432, "text": "figure 9.2 simulated\nresults from experiments\nto compare the effect of a\ntreatment t on a response\ny that varies with a\ncovariate x. the lines\nshow the mean response\nfor t = 0 (solid) and\nt = 1 (dots). left: the\neffect of t is confounded\nwith dependence on x.\nright: the experiment is\nbalanced, with random\nallocation of t dependent\non x.\n\n420\n\n4\n\n3\n\n2\n\n1\n\ny\n\n 0\n\n0\n\n 0\n\n1\n-\n\n 0\n\n 0\n\n 0\n\n 0\n 0\n\n1\n\n 0\n 0 0\n\n 0 0\n\n 0\n 0\n 0 0 0\n 0\n 0\n\n 0\n\n1\n\n1\n1\n\n1\n\n1\n\n11\n1\n\n1\n\n1\n\n1\n\n1\n1 1\n1\n1\n11\n\n1\n\n9 \u00b7 designed experiments\n\n4\n\n3\n\n2\n\n1\n\ny\n\n 0\n\n 0\n\n 0\n\n 01\n\n0\n\n 0\n\n1 0\n 0\n1\n1\n\n1\n\n1\n\n1\n-\n\n1\n\n1\n\n 0\n\n 0\n 01\n 01\n\n 0\n\n1\n\n 01\n\n 0\n\n1\n1\n 01\n1\n\n 0\n 0\n\n1\n\n1\n 0\n 0\n\n1\n\n 0\n\n1\n\n0.0\n\n1.0\n\n2.0\n\n3.0\n\n0.0\n\n1.0\n\n2.0\n\n3.0\n\nx\n\nx\n\nlink between u and t , so any effect of x on t is mediated through the observed x,\nfor which adjustment is in principle possible.\n\nto illustrate this, figure 9.2 shows results from two simulated attempts to assess\nthe effect of a treatment t on a response y . unnoticed by the virtual experimenter\nwho obtained the data in the left panel, the mean of y increases with a covariate x, as\nshown by the lines. however because all the units for which t = 1 also have the largest\nvalues of x, there appears to be no difference between the treatment group averages.\nthe true treatment effect is \u03b4 = \u22121, but the observed difference of averages is 0.2 with\nstandard error 0.2. the 0.95 confidence interval (\u22120.2, 0.6) does not include the true\n\u03b4 because of confounding between the effects of x and t . inpractice such serious\nconfounding would be most likely to arise due to lack of randomization, but lack of\nbalance could occur by accident even if the treatments had been allocated at random.\nif so, randomization would fail to remove all possible biases due to confounders\nsuch as x.\n\na cannier experimenter might have formed pairs of units using values of x mea-\nsured before the experiment and then randomized the treatment within pairs, leading\nto results like those in the right panel, where the difference of averages is \u22121.2 with\nstandard error 0.3; the 0.95 confidence interval now contains \u03b4.\nin both cases the observed values of x can be used to obtain more precise estimates\nof \u03b4, byfitting the model y = \u03b20 + \u03b21x + \u03b4t + \u03b5 to the observed triples (x, t, y),\nwhere t = 0 or 1.the left panel has (cid:5)\u03b4 = \u22120.7 with standard error 0.3 and corre-\nlation corr((cid:5)\u03b4,(cid:5)\u03b21) = \u22120.82, while the right has(cid:5)\u03b4 = \u22121.25, standard error 0.16 and\ncorr((cid:5)\u03b4,(cid:5)\u03b21) = \u22120.04. one effect of the blocking has been to reduce the confounding\n\nof t and x by making the corresponding columns of the design matrix almost orthog-\nonal; their parameters can then be estimated without ambiguity. there is a relation\nhere to the discussion of collinearity in section 8.7.2.\n\nalthough regression on x reduces the confounding between x and t in the first\nexperiment, the lack of overlap in the values of x for the two treatment groups means\nthat the model must be used to interpolate between them. this makes the estimate\nless precise and the inference less secure: an act of faith in the linearity of the model\nis needed, because neither of the groups has x values over the entire range.\n\n "}, {"Page_number": 433, "text": "9.1 \u00b7 randomization\nx, though the precision of(cid:5)\u03b4 is increased by making the adjustment, known as analysis\n\nthe second experiment gives similar estimates of \u03b4 with or without adjustment for\n\n421\n\nof covariance; see section 9.3.3. moreover the data can be used to check whether the\ntreatment effect is constant over x.\n\nrandomization inference\nin this chapter we shall assume that normal linear models are applicable. in fact the act\nof randomization provides a basis for inference without appealing to specific paramet-\nric assumptions, but for which the normal model often provides a good approximation.\nsuppose that m observations have been randomly allocated to a treatment and a fur-\nther m to a control. suppose also that unit-treatment additivity holds, that is there\nexist constants \u03b31, . . . , \u03b32m, one for each unit, and \u03b4 for the treatment, such that the\nresponse on the jth unit is \u03b3 j + \u03b4 when it is allocated to the treatment, and \u03b3 j \u2212 \u03b4 if it\nis allocated to the control group, regardless of the allocation of treatments to the other\nunits. thus the effect of treatment is to increase the response by \u0001 = 2\u03b4 relative to\nthe control, for each unit in the experiment. under this model the responses from the\njth unit when it is allocated to treatment and to control are\n(1 \u2212 tj )(\u03b3 j \u2212 \u03b4),\n\ntj (\u03b3 j + \u03b4),\n\n2m(cid:4)\nj=1\n\n2m(cid:4)\nj=1\n\n2m(cid:4)\nj=1\n\n(2tj \u2212 1)\u03b3 j .\n\ne(tj ) = 1\n2\n\ntj (\u03b3 j + \u03b4) \u2212 1\nm\n\nwhere tj is an indicator of whether it has been allocated to the treatment. therefore\nthe difference between treatment and control averages is\ny 1 \u2212 y 0 = 1\nm\n\n(1 \u2212 tj )(\u03b3 j \u2212 \u03b4) = 2\u03b4 + 1\nm\nthe properties of y 1 \u2212 y 0 stem from the moments of t1, . . . , t2m,\n, e(tj tk) = m \u2212 1\nj (cid:1)= k.\n2(2m \u2212 1)\n(9.3)\n(cid:1)\nthus y 1 \u2212 y 0 has mean \u0001 and variance 2{m(2m \u2212 1)}\u22121\nj=1(\u03b3 j \u2212 \u03b3 )2. moreover\nthe strong symmetry induced by the tj , allied to the weak dependence among them,\nmeans that the randomization distribution of y 1 \u2212 y 0 is close to normal.\nexample 9.1 (shoe data) table 9.1 shows the amount of wear in a paired compar-\nison of materials a and b used to sole shoes. material b is cheaper and the aim of the\nexperiment was to see if it was less durable than a. ten boys were chosen, material a\nallocated at random to one of their shoes, and material b to the other. all but two of\nthe differences d j are positive, suggesting that shoes soled with b wear more quickly\nthan those with a. the average difference is d = 0.41.\n\n2m\n\n,\n\nsuppose that there was no difference between the materials. then a and b\nwould simply be labels attached randomly to the shoes, and each difference might\nequally well have had the opposite sign. that is, each of the 210 = 1024 outcomes\n\u00b10.8,\u00b10.6, . . . ,\u00b10.3 w ould have been equally as likely as that actually observed.\nthus the average difference d would be the observed value of d = m\nj d j ,\nwhere d j = i j d j , and i1, . . . , im are independent variables taking values \u00b11 with\n\n(cid:1)\n\n\u22121\n\n "}, {"Page_number": 434, "text": "table 9.1 shoe wear\ndata (box et al., 1978,\np. 100). the table shows\nthe amount of shoe wear\nin an paired comparison\nexperiment in which two\nmaterials a and b were\nrandomly assigned to the\nsoles of the left (l) or\nright (r) shoe of each of\nten boys.\n\nfigure 9.3\nrandomization\ndistribution of the t\nstatistic for the shoes data,\ntogether with its\napproximating t9\ndistribution. the left panel\nshows a histogram and rug\nfor the randomized values\nof z, with the t9 density\noverlaid; the observed\nvalue is given by the\nvertical dotted line. the\nright panel shows a\nprobability plot of the\nrandomization distribution\nagainst t9 quantiles.\n\n422\n\n9 \u00b7 designed experiments\n\nmaterial\n\ndifference\n\nboy\n\na\n\nb\n\nd\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n13.2 (l)\n8.2 (l)\n10.9 (r)\n14.3 (l)\n10.7 (r)\n6.6 (l)\n9.5 (l)\n10.8 (l)\n8.8 (r)\n13.3 (l)\n\n14.0 (r)\n8.8 (r)\n11.2 (l)\n14.2 (r)\n11.8 (l)\n6.4 (r)\n9.8 (r)\n11.3 (r)\n9.3 (l)\n13.6 (r)\n\n0.8\n0.6\n0.3\n\u20130.1\n1.1\n\u20130.2\n0.3\n0.5\n0.5\n0.3\n\n4\n\n.\n\n0\n\n3\n\n.\n\n0\n\n2\n\n.\n\n0\n\n1\n\n.\n\n0\n\n0\n\n.\n\n0\n\n-4\n\n-2\n\n2\n\n4\n\n0\n\nt\n\nn\no\n\ni\nt\n\nu\nb\ni\nr\nt\ns\nd\n\ni\n\n \n\nn\no\n\ni\nt\n\ni\n\na\nz\nm\no\nd\nn\na\nr\n \nf\n\no\n\n \ns\ne\n\nl\ni\nt\n\nn\na\nu\nq\n\n4\n\n2\n\n0\n\n2\n-\n\n4\n-\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n-4\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\u2022\u2022\u2022\u2022\n\n\u2022\u2022\u2022\n\n\u2022\u2022\u2022\u2022\u2022\n\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\n\u2022 \u2022 \u2022 \u2022\n\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\n\u2022 \u2022 \u2022 \u2022\n\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\n\u2022\u2022\u2022\u2022\u2022\n\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\n\u2022\u2022\u2022\n\n\u2022\u2022\u2022\u2022\u2022\u2022\n\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\n\u2022\n\n-2\n\n0\n\n2\n\n4\n\nquantiles of t distribution\n\n(cid:1)\n\nthe studentized version of d, z = d/[{m(m \u2212 1)}\u22121\n\nprobability 1\nthan d, and four values equal to it, so the exact p-value based on d is 7/1024\n\n2 ; here m = 10. in fact there are precisely three values of d that are larger\n.= 0.007.\n(d j \u2212 d)2]1/2, is amono-\ntonic function of d, soboth z and d give the same p-values under randomization.\nfigure 9.3 shows the randomization distribution of z, with the t distribution on\nm \u2212 1 = 9 degrees of freedom that would be used under a normal model. the agree-\nment between the randomization distribution and the normal approximation is excel-\nlent. the observed value of z is 3.35, with significance level 0.004 when compared\nto the t9 distribution.\n\nthe pairing in this experiment could have been used to extend the validity of the\nresults, by taking boys of different ages, with different types of shoes and so forth.\nas the comparisons are based only on differences between feet of the same boy,\nthat is, within blocks, the heterogeneity of the boys themselves does not affect the\ncomparison of a and b. if the same difference between materials was seen on a wide\nvariety of blocks, one could be more confident that the difference in durability was\ngeneral. as previously mentioned, blocking is used to ensure a generalizable result\n\n "}, {"Page_number": 435, "text": "9.1 \u00b7 randomization\n\n423\n\nby taking blocks that are heterogeneous, while eliminating block effects by ensuring\n(cid:1)\nthat treatment comparisons are made within blocks.\n\nalthough described above only in the simplest cases, the normal linear model\nprovides approximations to randomization distributions in other settings also. below\nwe continue to talk of normal errors, with the understanding that these often generate\napproximations to randomization distributions.\n\n9.1.2 causal inference\nin many investigations the key question is causal. does passive smoking cause lung\ncancer? does exposure to air pollution increases levels of asthma? does applying\ntreatment t to a unit increase its response y by amount \u03b4? the extensive philosophical\ndiscussion of causality is largely irrelevant here, because of its focus on deterministic\nrelations between cause and effect. the best we can usually hope for is statements\nsuch as \u2018if applied to a large sample of units, t would give an average increase \u03b4,\ncompared with what would have been observed had they remained untreated\u2019. this\ntranslates into probability statements for individual units.\n\nit is important to appreciate that potential causes are aspects of units that could\nin principle be manipulated in the context in question. in a study of the effects of\nlifestyle on longevity, we can conceive of altering individuals\u2019 dietary and exercise\nhabits, for example, but not their genders. we can imagine comparing the survival of\nflabby burger-loving mr jones with his survival as fit or vegetarian or both, but not\nwith that of mr jones as female rather than male; were he a woman, he would not\nbe mr jones. here diet and exercise are potential causes, but gender is not. intrinsic\nattributes of units cannot be regarded as potential causes, because to speak of a causal\neffect of t on y , intervention to change the value of t must be possible.\n\nthree types of causal statement are as follows. first, and strongest, there may be\na well-understood evidence-based mechanism or set of mechanisms \u2014 biological,\nphysical or whatever \u2014 that links a cause to its effect. this is the usual meaning of\ncausality in so-called hard science, even though knowledge about the mechanism is\ninvariably subject to improvement.\n\nsecond, and much weaker, is the observation that two phenomena are linked by\na stable association, whose direction is established and which cannot be explained\nby mutual dependence on some other allowable variable. in example 6.18, for ex-\nample, ignoring age induced an apparently positive association between survival and\nsmoking, whose direction was reversed once age was taken into account. to see this\ndifferently, consider a population of units on each of which (t , y ) may be observed,\nand let the association between t and y be measured by\n\n\u03b3 = e(y | t = 1) \u2212 e(y | t = 0) > 0,\n\nsay. this can be estimated by the difference in averages y 1 \u2212 y 0 for samples with\nt = 1 and t = 0. to say that the association cannot be explained away amounts to\n\n "}, {"Page_number": 436, "text": "424\n\n9 \u00b7 designed experiments\n\nasserting that no confounding variable x exists for which\n\n\u03b3 (x) = e(y | t = 1, x = x) \u2212 e(y | t = 0, x = x) \u2261 0.\n\nin practice this will need to be bolstered by careful study design, often consider-\ning together studies that account for different possible confounders. the restric-\ntion to allowable variables means amongst other things that x cannot itself be a\nresponse to t .\n\na third interpretation of causality, intermediate between the first two, is related to\nexperimentation and relies on the notion of a counterfactual. consider a unit, and let\nr0 and r1 represent its responses on setting t = 0 and t = 1; these three variables\nare assumed to have a joint distribution. if in fact t = 0, then r0 is observed and r1\nis counterfactual; it is the response that would have been observed had the treatment\nbeen different. conversely r0 is counterfactual if t = 1. the central difficulty of\ncausal inference is that it is impossible to compare values of r0 and r1 from the\nsame unit. thus an assumption of homogeneity of treatment effects over units, that is,\nunit-treatment additivity, is essential. if unit-treatment additivity holds then the effect\nof t is measured by the difference of mean responses\n\u03b4 = e(r1) \u2212 e(r0),\n\nbut unlike \u03b3 this is not observable. in general \u03b4 (cid:1)= \u03b3 , but if the treatment allocation\nis randomized, then t is independent of any property of the unit, and the consistency\nequation y = r0(1 \u2212 t ) + r1t relating the counterfactuals to the response y entails\n\n\u03b4 = e(r1) \u2212 e(r0) = e(r1 | t = 1) \u2212 e(r0 | t = 0)\n\n= e(y | t = 1) \u2212 e(y | t = 0) = \u03b3 .\n\nhence unit-treatment additivity and randomization ensure that the quantity \u03b4 we want\nto estimate equals \u03b3 , which we can estimate.\n\nthis argument presumes there to be no relation between treatment allocation and\nany property of the unit, and this is typically true only in completely randomized\nexperiments. suppose however that (r1, r0) and t are independent conditional on\nthe value of another variable x, as inthe lower right of figure 9.1. then\n\ne(r1) = ex{e(r1 | x)} =e x{e(r1 | x, t = 1)} =e x{e(y | x, t = 1)},\n\nand with a parallel argument for e(r0) we have\n\n\u03b4 = e(r1) \u2212 e(r0) = ex{e(y | x, t = 1) \u2212 e(y | x, t = 0)} =\u03b3 ,\n\nsay. the observable effect \u03b3 , afunction of the joint distribution of y , x, and t , is\nnow averaged over the possible values of x for the unit. the interpretation of \u03b3 and\nthe case for a causal effect are both strengthened if in fact\n\ne(y | x, t = 1) \u2212 e(y | x, t = 0) = \u03b3 (x) \u2261 \u03b3 ,\n\nthat is, association with t does not depend on x, as infigure 9.2. otherwise there is\ninteraction between t and y ; see section 9.3.1.\n\nthe assumption need not\napply on the original\nscale; it might apply to a\ntransformed response, in\nwhich case the argument\nbelow is applied on the\ntransformed scale.\n\n "}, {"Page_number": 437, "text": "9.1 \u00b7 randomization\n\n425\n\nthe use of randomization to eliminate confounding variables is a powerful tool,\nbut it isnot sufficient for causal inference. an obvious counter-example is the left\npanel of figure 9.2, where it would be foolhardy to talk of a causal effect of t on\ny even if the appropriate linear model had been fitted, because the observed triples\n(x, t, y) give no way to assess whether confounding between x and t is present\ndespite randomization.\n\neven if experimentation has established that t changes the distribution of y , it\nseems rash to assert causality with no idea of an underlying mechanism. in practice\na combination of evidence from physical mechanisms, direct experiment, and large-\nscale observational data will be most compelling.\n\nexercises 9.1\n1\n\n\u2212 y1,\n(a) show that under the two-sample model, the difference of the sample averages, y2\nhas variance (n1 + n2)\u03c3 2/(n1n2). show that subject to n1 + n2 = n, this is minimized\nwhen n1 and n2 are as nearly equal as possible.\n(b) suppose that n units are split into k blocks of size m + 1, and that one unit in each\nblock is chosen at random to be treated, while the remaining m are controls. suppose\nthat the responses in the jth block are y j1 and y j2, . . ., y j(m+1), and let d j represent the\ndifference between the treated individual and the average of the controls. show that the\naverage of these differences has variance (m + 1)\u03c3 2/(km), and show that for fixed n this\nis minimized when m = 1.\nsuppose a paired comparison experiment is performed, in which the jth pair satisfies the\nnormal linear model\n\ny0 j = \u00b5 j \u2212 \u03b4 + \u03b50 j ,\n\ny1 j = \u00b5 j + \u03b4 + \u03b51 j ,\n\nj = 1, . . . ,m ,\n\nbut that data analysis is performed using the two-sample model. show that the variance\nestimator can be written as\ns2 =\n\n(\u00b5 j \u2212 \u00b5 + \u03b5t j \u2212 \u03b5\u00b7\u00b7)2.\n\n(cid:4)\n\n1\n\n\u22121\n\ndeduce that this has expected value \u03c3 2 + (m \u2212 1)\nj (\u00b5 j \u2212 \u00b5\u00b7)2 conditional on the \u00b5 j ,\nand hence show that if the \u00b5 j are normally distributed with variance \u03c4 2, then e(s2) =\n\u03c3 2 + \u03c4 2.\nshow that if the two-sample model is used in this situation, the length of a 95% con-\nfidence interval for 2\u03b4 is roughly 2(\u03c3 2 + \u03c4 2)1/2t2(m\u22121)(0.025), whereas under the paired\ncomparisons model the length is about 2\u03c3 tm\u22121(0.025). for what values of \u03c4 2/\u03c3 2 are the\ntwo-sample intervals shorter when (a) m = 3, (b) m = 11? discuss your results.\ncheck (9.3), find var(tj ) and cov(tj , tk) and hence verify the given formulae for\nthe mean and variance of y 1 \u2212 y 0.\nin example 9.1, show that z is a monotonic function of d.\nto what extent can gender be regarded as a cause in studies (a) relating longevity and\nlifestyle and (b) of salary differentials in employment?\nlet t = 0 with probability 1 \u2212 \u03b1 and t = 1 otherwise, and suppose that conditional on\nt = 0, r0 is normal with mean zero and r1 is normal with mean \u03b4, while conditional\non t = 1, the corresponding means are \u03b7 and \u03b7 + \u03b4; ineach case the variables have unit\nvariances. let y = r0(1 \u2212 t ) + r1t denote the observed response variable. show that\n\u03b3 = e(y | t = 1) \u2212 e(y | t = 0) = \u03b7 + \u03b4, and deduce that \u03b4 = e(r1) \u2212 e(r0) cannot\nbe estimated unless (r0, r1) and t are independent.\n\n2(m \u2212 1)\n\nj,t\n\n(cid:1)\n\n2\n\n3\n\n4\n5\n\n6\n\n "}, {"Page_number": 438, "text": "426\n\n9 \u00b7 designed experiments\n\nterm\n\ngroups\n\ndf\n\nt \u2212 1\n\nresidual\n\nt (r \u2212 1)\n\n(cid:1)\n(cid:1)\n\nsum of squares\nt,r (yt\u00b7 \u2212 y\u00b7\u00b7)2\nt,r (ytr \u2212 yt\u00b7)2\n\nmean square\n\n(t \u2212 1)\n\n\u22121\n\n{t (r \u2212 1)}\u22121\n\n(cid:1)\nt (yt\u00b7 \u2212 y\u00b7\u00b7)2\n(cid:1)\nt,r (ytr \u2212 yt\u00b7)2\n\ntable 9.2 analysis of\nvariance table for one-way\nlayout.\n\n9.2 some standard designs\n9.2.1 one-way layout\nif more than two treatments are to be compared and the population is relatively\nhomogeneous, the two-group model may be extended to a completely randomized\ndesign, known as a one-way layout. henceforth we let t denote the number of\ntreatments in the model under consideration.\nsuppose that we wish to compare the effects of t treatments and that we have\navailable n = rt units. we divide the units at random into t groups each of size r,\nand apply a single treatment to all the units in each group. the corresponding linear\nmodel is\n\nytr = \u03b2t + \u03b5tr ,\n\nt = 1, . . . , t ,\n\nr = 1, . . . , r,\n\n(9.4)\niid\u223c n (0, \u03c3 2). this assumes that the only effect of the treatment is to alter\nwhere \u03b5tr\nthe mean response, as would be the case under a randomization distribution. thus\nthe observations within each group are random samples, but the groups may have\ndifferent means. this explains the term one-way layout: laid out as a t \u00d7 r array,\nonly the treatment index is meaningful. in matrix terms this model is\n\n\uf8f6\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n\uf8eb\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\ny11\n.\n.\n.\ny1r\ny21\n.\n.\n.\ny2r\n.\n.\n.\nyt 1\n.\n.\n.\nyt r\n\n\uf8eb\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n1\n.\n.\n.\n1\n0\n.\n.\n.\n0\n.\n.\n.\n0\n.\n.\n.\n0\n\n=\n\n\uf8f6\n\n\uf8eb\n\n\uf8f6\n\n.\n\n(9.5)\n\n\u03b22\n.\n.\n.\n\u03b2t\n\n0\n.\n.\n.\n0\n0\n.\n.\n.\n0\n.\n.\n.\n1\n.\n.\n.\n1\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n\uf8eb\n\uf8ec\uf8ed \u03b21\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n\uf8f6\n\uf8f7\uf8f8 +\n\n\u03b511\n.\n.\n.\n\u03b51r\n\u03b521\n.\n.\n.\n\u03b52r\n.\n.\n.\n\u03b5t 1\n.\n.\n.\n\u03b5t r\n\n0\u00b7\u00b7\u00b7\n.\n.\n.\n0\u00b7\u00b7\u00b7\n1\u00b7\u00b7\u00b7\n.\n.\n.\n1\u00b7\u00b7\u00b7\n.\n.\n.\n0\u00b7\u00b7\u00b7\n.\n.\n.\n0\u00b7\u00b7\u00b7\n(cid:1)\nr ytr . ifthe \u03b2t are all equal, corresponding to here and below\nreplacement of a subscript\nby a dot indicates\naveraging over the values\nof that subscript.\n\n(cid:4)\n\n(cid:4)\n\nthis design matrix has full rank t and the least squares estimator of \u03b2t it yields is the\naverage for the tth group, yt\u00b7 = r\nthe model y = 1n\u03b20 + \u03b5 in our general notation, the fitted value for the entire set of\ndata is the overall average y\u00b7\u00b7. the sum of squares then decomposes as\n\n\u22121\n\n(ytr \u2212 y\u00b7\u00b7)2 =\n\n(ytr \u2212 yt\u00b7)2 +\n\n(yt\u00b7 \u2212 y\u00b7\u00b7)2,\n\nt,r\n\nt,r\n\nt,r\n\ncorresponding to (8.23), and the analysis of variance is shown in table 9.2.\n\n(cid:4)\n\n "}, {"Page_number": 439, "text": "9.2 \u00b7 some standard designs\n\n427\n\ntable 9.3 data on the\nteaching of arithmetic.\n\ngroup\n\ntest result y\n\naverage\n\nvariance\n\na (usual)\nb (usual)\nc (praised)\nd (reproved)\ne (ignored)\n\n17\n21\n28\n19\n21\n\n14\n23\n30\n28\n14\n\n24\n13\n29\n26\n13\n\n20\n19\n24\n26\n19\n\n24\n13\n27\n19\n15\n\n23\n19\n30\n24\n15\n\n16\n20\n28\n24\n10\n\n15\n21\n28\n23\n18\n\n24\n16\n23\n22\n20\n\n19.67\n18.33\n27.44\n23.44\n16.11\n\n17.75\n12.75\n6.03\n9.53\n13.11\n\nthe unbiased estimator of \u03c3 2 when (9.4) is fitted is\n\ns2 = 1\nn \u2212 p\n\n(y \u2212(cid:5)y)t(y \u2212(cid:5)y) =\n\n1\n\nt (r \u2212 1)\n\n(cid:4)\n\nt,r\n\n(ytr \u2212 yt\u00b7)2,\n\nwith t (r \u2212 1) degrees of freedom. if r = 1 it isimpossible to estimate \u03c3 2, for then\nthere is only one observation with which to estimate \u03b2t , and ytr \u2261 yt\u00b7. thus replication\nof the responses for each treatment is essential unless an external estimate of \u03c3 2 is\navailable, for example from another experiment. a further benefit of replication is the\ncapacity to check model assumptions, as we shall see in examples 9.2 and 9.6.\n\nthe f statistic for assessing significance of differences among treatments,\n\nf = (t \u2212 1)\n\u22121\n\n(cid:1)\nt (yt\u00b7 \u2212 y\u00b7\u00b7)2\ns2\n\n\u223c ft\u22121,t (r\u22121),\n\nwhen \u03b21 = \u00b7\u00b7\u00b7 =\u03b2 t . inapplications interest generally focuses on estimation of par-\nticular differences among the \u03b2t , however, rather than on testing for overall differences,\nthis being merely an initial screening device.\n\nanother possible linear model for the data is\n\nytr = \u03b1 + \u03b3t + \u03b5tr ,\n\nt = 1, . . . , t ,\n\nr = 1, . . . , r,\n\nin which the overall mean is represented by \u03b1, and \u03b3t represents the difference between\nthe mean for treatment t and the overall mean. the design matrix for this model has\nt + 1 columns, namely the t columns of the matrix in (9.5) and a column of ones,\nand has rank t : the t + 1 parameters cannot be estimated from t groups. although\nthe t linear combinations \u03b1 + \u03b31, . . . , \u03b1 + \u03b3t corresponding to the group means are\nestimable, the t + 1 parameters \u03b1, \u03b31, . . . , \u03b3t are not.\nexample 9.2 (teaching methods data)\nin an investigation on the teaching of arith-\nmetic, 45 pupils were divided at random into five groups of nine. groups a and b\nwere taught in separate classes by the usual method. groups c, d, and e were taught\ntogether for a number of days. on each day c were praised publicly for their work, d\nwere publicly reproved and e were ignored. at the end of the period all pupils took\na standard test, with the results given in table 9.3 and displayed in the left panel of\nfigure 9.4. groups a and b seem to have performed similarly, but the other groups\nhave responded differently to the regimes imposed, as we see from the averages and\nvariances in the final columns of the table. if the only differences among groups were\nin their means, the group variances could be expected to be independently distributed\n\n "}, {"Page_number": 440, "text": "428\n\n9 \u00b7 designed experiments\n\nterm\n\ndf\n\nsum of squares mean square\n\nf\n\ngroups\n\n4\n\n722.67\n\n180.67\n\n15.3\n\nresidual\n\n40\n\n473.33\n\n11.83\n\ntable 9.4 analysis of\nvariance for data on the\nteaching of arithmetic.\n\n0\n3\n\n5\n2\n\ny\n\n0\n2\n\n5\n1\n\n0\n1\n\n0\n2\n\n5\n1\n\n0\n1\n\n5\n\ne\nc\nn\na\ni\nr\na\nv\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nfigure 9.4 data on\nteaching of arithmetic.\nthe left panel shows the\noriginal data, and the right\npanel shows the ordered\nvariances for each group\nplotted against plotting\npositions for the \u03c7 2\n8\ndistribution.\n\na\n\nb\n\nc\n\nd\n\ne\n\ngroup\n\n2\n\n4\n\n6\n\n8\n\n10\n\n12\n\nquantile of chi-squared distribution\n\nas \u03c3 2\u03c7 2\n/8. no doubt is cast on this by the corresponding probability plot, shown in\n8\nthe right panel of figure 9.4; this is only available because of the replication within\neach group.\n\nthe analysis of variance, shown in table 9.4, shows very strong evidence of dif-\nferences among the groups, as we would expect from inspecting the data. the cor-\nresponding f statistic is 15.3, to be considered as f4,40 under the hypothesis of no\ngroup differences, in which case the significance level is zero.\nas a group average is an average of r = 9 observations, its variance is \u03c3 2/r,\nand consequently the estimated variance for the difference between the averages for\ngroups a and b, y a \u2212 y b = 1.33, is 2s2/9 = 2.63. the corresponding t statistic,\n1.33/2.631/2, shows no evidence of differences between the control groups, and the\npooled estimate of the mean using the usual teaching method, \u03b2u , isaccordingly\nyu = 1\ncomparisons of the usual and other methods are of interest here, and they are based\non statistics such as y c \u2212 y u , each having estimated variance s2/18 + s2/9 = 1.97.\nconfidence intervals for the underlying differences are based on the quantities\n{y c \u2212 y u \u2212 (\u03b2c \u2212 \u03b2u )}/{s2/18 + s2/9}1/2, each having a t40 distribution. thus\n95% confidence intervals are (5.7, 11.2) for \u03b2c \u2212 \u03b2u , (1.7, 7.2) for \u03b2d \u2212 \u03b2u , and\n(\u22125.6,\u22120.14) for \u03b2e \u2212 \u03b2u . giving approval and reproval improves test performance\nrelative to the usual method, with approval working best, while ignoring pupils de-\ncreases their test scores, though by less. these conclusions are necessarily highly\n(cid:1)\ntentative because of the very limited scale of the experiment.\n\n2 (19.67 + 18.33) = 19, with estimated variance s2/18.\n\n "}, {"Page_number": 441, "text": "9.2 \u00b7 some standard designs\n\n429\n\n9.2.2 randomized block design\nsuppose that t treatments are to be compared, and that n = t b units are available.\nthe analogue of the paired comparisons experiment when there are more than two\ntreatments is the randomized block design. the units are divided into b blocks of t\nunits so that similar units are so far as possible in the same block. the t treatments\nare then applied randomly to the units, each treatment appearing precisely once in\neach block. a simple linear model here is that the response of the unit in block b\ngiven treatment t is\n\nytb = \u00b5 + \u03b1t + \u03b2b + \u03b5tb,\n\nt = 1, . . . , t , b = 1, . . . , b,\n\n(9.6)\n\n\uf8f6\n\n\uf8eb\n\nwhere the \u03b5tb are a random sample of n (0, \u03c3 2) variables. this is the two-way layout\nmodel, so-called because the ytb can be laid out as an array with t rows and b columns,\nwith \u03b1t the treatment effect for the tth row and \u03b2b the block effect for the bth column;\nsee table 9.6. with t = 4 and b = 3 for definiteness, and with parameter vector\n(\u00b5, \u03b11, \u03b12, \u03b13, \u03b14, \u03b21, \u03b22, \u03b23)t, the 12 \u00d7 8 design matrix\n0\n1\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0\n\n1 1\n1 1\n1 1\n1 0\n1 0\n1 0\n1 0\n1 0\n1 0\n1 0\n1 0\n1 0\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n\n1\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0\n0\n\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0\n0\n1\n\nx =\n\nhas rank 1 + (t \u2212 1) + (b \u2212 1) = 6: all eight parameters cannot be estimated. the\nterms corresponding to the treatment and block effects are columns 2\u20135 and 6\u20138 of\nthis matrix respectively. dropping the second and sixth columns of x is equivalent\nto setting \u03b11 = \u03b21 = 0, in which case \u03b1t and \u03b2b represent the mean differences in\nresponse between treatment t and treatment 1 and between block b and block 1. in\nthis, the corner-point parametrization, \u00b5 is the mean response of the unit in block\n1 given treatment 1, that is, in the top left corner of the two-way layout. the least\nsquares estimates in this parametrization can be obtained from the usual formula, but\ntheir derivation is unenlightening.\n\ninstead, let us use the original parametrization with the least squares estimates\n\n(cid:5)\u03b2c = 0. these are constraints on the estimates, not\n\nconstrained so that\non the parameters: nature is free to use as many parameters as she likes, but only certain\nlinear combinations of them are estimable. these two linear restrictions ensure that\nour fitted model is not overparametrized, and we can use symmetry to avoid inverting\n\na rank-deficient matrix x t x. weuse lagrange multipliers to find the values of (cid:5)\u00b5,(cid:5)\u03b1t ,\n\n(cid:5)\u03b1r = (cid:1)\n\n(cid:1)\n\nc\n\nr\n\n "}, {"Page_number": 442, "text": "430\n\n(cid:5)\u03b2b, \u03b7 and \u03b6 that minimize\n\n(cid:4)\n\nt,b\n\n(ytb \u2212(cid:5)\u00b5 \u2212(cid:5)\u03b1t \u2212(cid:5)\u03b2b)2 + \u03b7\n\n9 \u00b7 designed experiments\n(cid:12)(cid:4)\n\n(cid:13)\n\n(cid:5)\u03b2b \u2212 0\n\n.\n\nb\n\n(cid:13)\n\n+ \u03b6\n\n(cid:5)\u03b1t \u2212 0\n\n(cid:12)(cid:4)\n\nt\n\non differentiating, we see that we should solve the equations\n\nt,b\n\n(cid:4)\n(ytb \u2212(cid:5)\u00b5 \u2212(cid:5)\u03b1t \u2212(cid:5)\u03b2b),\n(cid:4)\n(ytb \u2212(cid:5)\u00b5 \u2212(cid:5)\u03b1t \u2212(cid:5)\u03b2b) \u2212 \u03b7,\n(cid:4)\n(ytb \u2212(cid:5)\u00b5 \u2212(cid:5)\u03b1t \u2212(cid:5)\u03b2b) \u2212 \u03b6,\n(cid:4)\n(cid:5)\u03b1t ,\n\n(cid:5)\u03b2b,\n\n(cid:4)\n\n0 =\n\nb\n\nt\n\n0 =\n0 =\n0 =\n0 =\n\nt = 1, . . . , t ,\nb = 1, . . . , b,\n\nt\n\nb\n\n\u22121\n\n(cid:1)\n\n(cid:1)\n\nb ytb, and so forth.\n\nt,b ytb, yt\u00b7 = b\n\ngiving (cid:5)\u00b5 = y\u00b7\u00b7, (cid:5)\u03b1t = yt\u00b7 \u2212 y\u00b7\u00b7, and (cid:5)\u03b2b = y\u00b7b \u2212 y\u00b7\u00b7, where as before we use a dot\nin a subscript to indicate averaging over the corresponding index. thus we have\ny\u00b7\u00b7 = (t b)\n\u22121\nthe fitted values are(cid:5)\u00b5 +(cid:5)\u03b1t +(cid:5)\u03b2b = y\u00b7b + yt\u00b7 \u2212 y\u00b7\u00b7, and hence the residual sum of\n(cid:1)\nt,b(ytb \u2212 yt\u00b7 \u2212 y\u00b7b + y\u00b7\u00b7)2; these would be the same in the corner-point\nsquares is\nparametrization, because the same subspace is spanned by the columns of the design\ndepend only on the column space of the design matrix; recall figure 8.2. if the (cid:5)\u03b2b\nmatrix in both cases, and the fitted values \u2014 though not the parameter estimates \u2014\nwere not in the model,(cid:5)\u00b5 and(cid:5)\u03b1t would remain the same, and the fitted values would\nbe(cid:5)\u00b5 +(cid:5)\u03b1t = yt\u00b7.\ndifference(cid:5)\u03b1r \u2212(cid:5)\u03b1s = yr\u00b7 \u2212 ys\u00b7. if wewrite this estimate in terms of the underlying\nthe mean difference in response between treatments r and s is estimated by the\nparameters, by replacing yrb by \u00b5 + \u03b1r + \u03b2b + \u03b5rb and so forth, we obtain\n\nb\n\n\u22121(b\u00b5 + b\u03b1r + \u03b21 + \u00b7\u00b7\u00b7 +\u03b2 b + \u03b5r1 + \u00b7\u00b7\u00b7 + \u03b5r b)\n\u2212b\n\n\u22121(b\u00b5 + b\u03b1s + \u03b21 + \u00b7\u00b7\u00b7 +\u03b2 b + \u03b5s1 + \u00b7\u00b7\u00b7 + \u03b5s b),\n\nwhich equals \u03b1r \u2212 \u03b1s + \u03b5r\u00b7 \u2212 \u03b5s\u00b7, and this is independent of the block effects, which\nappeared equally often in yr\u00b7 and ys\u00b7. thus because the design is balanced, com-\nparisons among treatments are essentially made within blocks, and this increases\nprecision if there are substantial block effects.\n\nthe difference between an observation and the overall average equals\nytb \u2212 y\u00b7\u00b7 = (ytb \u2212 yt\u00b7 \u2212 y\u00b7b + y\u00b7\u00b7) + (yt\u00b7 \u2212 y\u00b7\u00b7) + (y\u00b7b \u2212 y\u00b7\u00b7),\n(cid:1)\n\nt,b(ytb \u2212 y\u00b7\u00b7)(yt\u00b7 \u2212 y\u00b7\u00b7) = 0,\n\n(cid:1)\nt,b(yt\u00b7 \u2212 y\u00b7\u00b7)(y\u00b7b \u2212 y\u00b7\u00b7) = 0, and so\n(cid:4)\n\n(cid:4)\n\n(y\u00b7b \u2212 y\u00b7\u00b7)2,\nwhich echoes (8.23). if we had set (cid:5)\u03b2b \u2261 0, the corresponding sum of squares\n\n(ytb \u2212 yt\u00b7 \u2212 y\u00b7b + y\u00b7\u00b7)2 +\n\n(yt\u00b7 \u2212 y\u00b7\u00b7)2 +\n\nt,b\n\nt,b\n\nt,b\n\nt,b\n\n(cid:4)\n\nand because\nforth, we see that\n(ytb \u2212 y\u00b7\u00b7)2 =\n\n(cid:4)\n\n "}, {"Page_number": 443, "text": "9.2 \u00b7 some standard designs\n\n431\n\ntable 9.5 analysis of\nvariance table for\ntwo-way layout model.\n\nterm\n\ntreatments\nblocks\n\ndf\n\nt \u2212 1\nb \u2212 1\n\nresidual\n\n(t \u2212 1)(b \u2212 1)\n\n(cid:1)\n\nsum of squares\n(cid:1)\n(cid:1)\nt,b(yt\u00b7 \u2212 y\u00b7\u00b7)2\nt,b(y\u00b7b \u2212 y\u00b7\u00b7)2\n\nt,b(ytb \u2212 yt\u00b7 \u2212 y\u00b7b + y\u00b7\u00b7)2\n\ntable 9.6 data on\nweight gains in pigs.\n\ngroup\n\ndiet\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\naverage\n\ni\nii\niii\niv\n\n1.40\n1.31\n1.40\n1.96\n\n1.79\n1.30\n1.47\n1.77\n\n1.72\n1.21\n1.37\n1.62\n\n1.47\n1.08\n1.15\n1.76\n\n1.26\n1.45\n1.22\n1.88\n\n1.28\n0.95\n1.48\n1.50\n\n1.34\n1.26\n1.31\n1.60\n\n1.55\n1.14\n1.27\n1.49\n\naverage\n\n1.52\n\n1.58\n\n1.48\n\n1.37\n\n1.45\n\n1.30\n\n1.38\n\n1.36\n\n1.48\n1.21\n1.33\n1.70\n\n1.43\n\ndecomposition would have been\n(ytb \u2212 y\u00b7\u00b7)2 =\n\n(cid:4)\n\n(cid:4)\n\n(ytb \u2212 yt\u00b7)2 +\n\nt,b\n\nt,b\n\n(cid:4)\n\nt,b\n\n(yt\u00b7 \u2212 y\u00b7\u00b7)2,\n\n(cid:1)\n\n(cid:1)\n\nand it follows by symmetry that once the constant term has been fitted, the reductions\n(cid:1)\nt,b(yt\u00b7 \u2212 y\u00b7\u00b7)2\nin sum of squares due to treatment and block terms are respectively\nt,b(yt\u00b7 \u2212 y\u00b7\u00b7)(y\u00b7b \u2212 y\u00b7\u00b7) = 0, these sums of squares are\nand\nindependent if the errors are normal.\n\nt,b(y\u00b7b \u2212 y\u00b7\u00b7)2. as\n\nthe analysis of variance table for a two-way layout with t rows and b columns is\n\nin table 9.5. the residual degrees of freedom are\n\nt b \u2212 1 \u2212 (t \u2212 1) \u2212 (b \u2212 1) = (t \u2212 1)(b \u2212 1),\n\nand the sums of squares are independent of the order in which terms are fitted.\n\nexample 9.3 (pig diet data) twelve pigs were divided into eight groups of four,\nin such a way that the pigs in any one group were expected to gain weight at equal\nrates if fed in the same way. four diets were compared by randomly assigning them\nto pigs, subject to each diet occurring once in each group. the average daily weight\ngains of the pigs are given in table 9.6. the diet averages suggest that pigs on diet\niv gain more weight than the others, and that any differences between ii and iii are\nsmall. differences among the groups are less marked.\nthe analysis of variance in table 9.7 shows strong differences among diets, but little\neffect of blocking into groups. the estimate of \u03c3 2 is s2 = 0.024. the diet averages\nare 1.48, 1.21, 1.33, and 1.70, and as the standard error for a difference of two of them\nis (2s2/8)1/2 = 0.077, it is clear that diet iv leads to the fastest weight gain, with diet\n(cid:1)\ni second and better than diet iii; it is less clear that ii is worse than iii.\n\n "}, {"Page_number": 444, "text": "table 9.7 analysis of\nvariance table for two-way\nlayout model applied to\nthe data of table 9.6.\n\ntable 9.8 log10 dry\nweight y (\u00b5g) of chick\nbones after cultivation\nover anutrient chemical\nmedium, either complete\n(\u2014), or with single amino\nacids missing (cox and\nsnell, 1981, p. 95). the\norder of treatment pairs\nwas randomized, but the\ntable shows them\nsystematically.\n\n432\n\n9 \u00b7 designed experiments\n\nterm\n\ndf\n\nsum of squares mean square\n\nf statistic\n\ndiet\ngroup\n\n3\n7\n\n1.042\n0.247\n\nresidual\n\n21\n\n0.500\n\n0.347\n0.035\n\n0.024\n\n14.6\n1.48\n\nembryo\n\ntreat\n\ny\n\ntreat\n\ny\n\nembryo\n\ntreat\n\ny\n\ntreat\n\ny\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\nhis\u2013\nhis\u2013\nhis\u2013\n\n2.51\n2.49\n2.54\n2.58\n2.65\n2.11\n2.28\n2.15\n\nhis\u2013\narg\u2013\nthr\u2013\nval\u2013\nlys\u2013\narg\u2013\nthr\u2013\nval\u2013\n\n2.15\n2.23\n2.26\n2.15\n2.41\n1.90\n2.11\n1.70\n\n9\n10\n11\n12\n13\n14\n15\n\nhis\u2013\narg\u2013\narg\u2013\narg\u2013\nthr\u2013\nthr\u2013\nval\u2013\n\n2.32\n2.15\n2.34\n2.30\n2.20\n2.26\n2.28\n\nlys\u2013\nthr\u2013\nval\u2013\nlys\u2013\nval\u2013\nlys\u2013\nlys\u2013\n\n2.53\n2.23\n2.15\n2.49\n2.18\n2.43\n2.56\n\nbalanced incomplete block design\nsometimes variation among units is large enough for blocking to be required, but a\nrandomized block design cannot be used because the block size is smaller than the\nnumber of treatments, t . insuch circumstances it may be possible to use a balanced\nincomplete block design. suppose that there are b blocks each with k units, and that\nr = b k/ t is an integer. in the simplest such design, each treatment appears exactly\nonce in a block, and each pair of treatments appears together \u03bb times, in which case\nr(k \u2212 1) = (t \u2212 1)\u03bb.\nexample 9.4 (chick bone data) table 9.8 gives data on the growth of chick\nbones. bones from 7-day-old chick embryos were cultivated over a nutrient chemical\nmedium. two bones were available from each chick, and the experiment was set out\nin a balanced incomplete block design with two units per block. the treatments were\ngrowth in the complete medium, with about 30 nutrients in carefully controlled quan-\ntities, and growth in five other media, each with a single amino acid omitted. thus\nhis\u2013, arg\u2013, and so forth denote media without particular amino acids. this balanced\nincomplete block design has t = 6, b = 15, k = 2, r = 5, and \u03bb = 1.\none way to proceed here is to let \u03b2h , \u03b2a, . . . denote the effect of the absence of\nhis, arg, . . ., and then regard the first pair of responses as having means \u00b51, \u00b51 + \u03b2h ,\nthe sixth as having means \u00b56 + \u03b2h , \u00b56 + \u03b2a, and so forth. we then perform a linear\nregression with response the differences of the responses for each of the embryos,\nparameter vector (\u03b2h , \u03b2a, \u03b2t , \u03b2v , \u03b2l)t, and a 15 \u00d7 5 design matrix whose first and\nsixth rows are (1, 0, 0, 0, 0) and (\u22121, 1, 0, 0, 0). this avoids the necessity to estimate\n\u00b51, . . . , \u00b515, but gives the same estimates of the \u03b2s, shown in the first line of table 9.9.\nthe estimate of error variance is s2 = 0.013. the initial sum of squares of 1.024 re-\nduces to 0.132, giving overall f statistic 13.6 on 5 and 10 degrees of freedom: a\n\n "}, {"Page_number": 445, "text": "9.2 \u00b7 some standard designs\n\n433\n\ntable 9.9 parameter\nestimates and standard\nerrors for intra-block,\ninter-block and pooled\nanalyses of chick data.\n\namino acid\n\nanalysis\n\nintra-block(cid:5)\u03b2\ninter-block \u02dc\u03b2\npooled \u03b2\u2217\n\nhis\n\narg\n\nthy\n\nval\n\nlys\n\nse\n\n\u22120.22 \u22120.35 \u22120.35 \u22120.49 \u22120.16\n\u22120.55 \u22120.40 \u22120.33 \u22120.42\n0.07\n\u22120.29 \u22120.36 \u22120.34 \u22120.47 \u22120.11\n\n0.066\n0.124\n0.058\n\nhighly significant reduction. lack of each amino acid reduces growth; lys has the\nsmallest effect, but even this has the large t value of \u22120.16/0.066 = \u22122.42 on\n10 degrees of freedom.\n\nin this regression the terms for individual amino acids are not orthogonal, and the\nanalysis of variance is not unique. for example, if acids are fitted in the order his,\narg, thr, val, lys, the reductions in sum of squares are 0.014, 0.052, 0.078, 0.67,\n0.077, while the reductions for the order lys, thr, val, his, arg are 0.074, 0.033,\n0.40, 0.01, 0.38. here balance gives equal precision for estimation of each of the \u03b2s,\nrather than orthogonal sums of squares.\n\nj2\n\nj1\n\n\u03b2, \u00b5 j + x t\n\nthough simple, this so-called intra-block analysis uses a degree of freedom to\nestimate each block parameter \u00b5 j , and if these vary little then information may be\nlost. to outline how inter-block analysis can retrieve this, we denote the responses\nfrom the jth block as y j1, y j2 and treat these as independent normal variables with\nmeans \u00b5 j + x t\n\u03b2 and variances \u03c3 2. the previous analysis was based\non y j1 \u2212 y j2, and this is independent of the block sum y j1 + y j2. the inter-block\nanalysis treats the \u00b5 j as random variables with mean \u00b5, say, and variance \u03c3 2\n\u00b5, so\n\u00b5 + \u03c3 2), perhaps much larger than the variance\nthe block sums have variance 2(2\u03c3 2\n2\u03c3 2 of the differences. we then fit to the 15 block sums a linear model with means\n\u00b5115 + x\u03b2, the jth row of x being x t\nj2, thereby obtaining estimates \u02dc\u03b2 of\nthe amino acid effects. these estimates are independent of those obtained from the\nintrablock analysis; their values are given in table 9.9, along with the standard error\ninflated by \u03c3 2\n\u00b5. both sets of estimates are unbiased, so approximate minimum variance\n\nunbiased estimates are formed as a weighted combination \u03b2\u2217 = w(cid:5)\u03b2 + (1 \u2212 w) \u02dc\u03b2,\nwhere w =(cid:5)v\n\u22121),(cid:5)v and \u02dcv being the estimated variances for the intra- and\ninter-block estimates. as w = 0.78, these pooled estimates are close to the (cid:5)\u03b2, with\n\n\u22121/((cid:5)v\n\n\u22121 + \u02dcv\n\n+ x t\n\nj1\n\nt13.8(0.975) = 2.15\n\na slightly smaller standard error. this standard error combines independent standard\nerrors from the intra- and inter-block analyses and has approximately 13.8 degrees of\nfreedom; see exercise 9.2.3.\n\nthe response is log10 dry weight, so the effect of eliminating an amino acid is\nmultiplicative on the original scale. a 0.95 confidence interval for the median effect\nof eliminating his is 10\ncorresponding to a 50% reduction in growth. see practical 9.1.\n\n(cid:5)\u03b2h\u00b12.15\u00d70.058 = (0.38, 0.68), with the estimate 10\n\n(cid:5)\u03b2h = 0.51\n(cid:1)\n\nthere are many generalizations of incomplete block designs. their key purpose is\nto give good precision for estimation of the effects of interest \u2014 usually treatment\neffects, or perhaps a subset of them \u2014 when there are more treatments than blocks.\n\n "}, {"Page_number": 446, "text": "434\n\n9 \u00b7 designed experiments\n\nhigher degrees of balance are possible, in which all triples of treatments appear\nequally often, and sometimes constraints on the numbers of units lead to the use of\npartially balanced designs, which give increased precision on treatments of primary\ninterest while sacrificing precision on those of less importance.\n\nr\n\n9.2.3 latin square\nwhen there are two possible blocking factors, a three-way layout could be used. in\nmany circumstances, however, a design that requires fewer units is required, and one\npossibility may be a latin square. suppose that the blocking factors and the treatment\nhave the the same number of levels, q. then a latin square design is constructed by\nlaying out units in a q \u00d7 q array with the blocking factors corresponding to rows and\ncolumns, and applying each treatment precisely once in each row and in each column.\nan example is shown in the upper left part of table 9.11. this balanced application\nof treatments leads to an orthogonal decomposition of the total sum of squares. many\nsuch layouts are possible, with randomization by choice of design and permutation\nof row, column and treatment labels.\nthe corresponding linear model treats the response in the rth row and cth column\nas yrc = \u00b5 + \u03b1r + \u03b2c + \u03b3t(r,c) + \u03b5rc, where t(r, c) isthe treatment applied to that\niid\u223c n (0, \u03c3 2). as it stands this model contains 1 + 3q parameters but the\nunit, and \u03b5rc\ndesign matrix would have rank 1 + 3(q \u2212 1).\n(cid:1)\nr,c(yrc \u2212(cid:5)\u00b5 \u2212(cid:5)\u03b1r \u2212(cid:5)\u03b2c \u2212(cid:5)\u03b3t(r,c))2 subject to\n(cid:5)\u03b3t = 0. this yields\n\n(cid:1)\n(cid:5)\u00b5 = y, (cid:5)\u03b1r = yr\u00b7 \u2212 y\u00b7\u00b7, (cid:5)\u03b2c = y\u00b7c \u2212 y\u00b7\u00b7, (cid:5)\u03b3t = yt \u2212 y\u00b7\u00b7,\n\n(cid:5)\u03b2c = (cid:1)\n(cid:1)\nr,c(yrc \u2212 yr\u00b7 \u2212 y\u00b7c \u2212 yt + 2y\u00b7\u00b7)2.\n\nargument on page 429. we minimize\nthe constraints\n\nleast squares estimates may be obtained by extending the lagrange multiplier\n\nwith residual sum of squares\n\n(cid:5)\u03b1r = (cid:1)\n\nto see this another way, suppose that we had ignored the treatment classification in\nthe latin square, and obtained the analysis of variance table for the row and column\nclassifications. these would be the same as in table 9.5, though the residual sum\nof squares would also contain the variation due to treatments. however, we could\nrewrite the table so that treatments appeared as the row classification, in which case\nthe current row classification would take on the role of treatments and appear inside the\ntable. the two-way analysis of variance table for the rearranged data, ignoring the\nnew treatments (old rows), would contain sums of squares due to treatments and\nto columns, and its residual sum of squares would also contain the sum of squares\nfor rows. since the sums of squares for both two-way analyses are orthogonal, the\nanalysis of variance table for a q \u00d7 q latin square must be as shown in table 9.10.\nexample 9.5 (field concrete mixer data) a field concrete mixer lays down a\nconcrete road surface while moving forward. its efficiency is measured by the hardness\nof the surface it produces, as a percentage of the corresponding hardness produced\nunder laboratory conditions. it is thought that efficiency may fall off as the speed at\nwhich the machine moves increases, and trials were performed to investigate this.\non each of four days, the machine was run at four different speeds, 4, 8, 12, and\n\nc\n\nt\n\n "}, {"Page_number": 447, "text": "9.2 \u00b7 some standard designs\n\n435\n\ntable 9.10 analysis of\nvariance table for a latin\nsquare.\n\ntable 9.11 field\nconcrete mixer data. latin\nsquare experiment,\nshowing application of\ntreatments \u2014 speed in\nmiles per hour (left) \u2014\nand observed responses \u2014\nmachine efficiency (%)\n(right) \u2014 for 16\ncombinations of day and\nrun. below are average\nefficiencies for day, run,\nand speed.\n\nfigure 9.5 field\nconcrete mixer data. left\npanel: efficiencies as a\nfunction of speed, with\nplotting symbol giving the\nday. right panel: average\nefficiencies, with fitted\nquadratic curve\ncorresponding to day 1\nand run 1.\n\ny\nc\nn\ne\nc\ni\nf\nf\n\ni\n\ne\n\n0\n7\n\n5\n6\n\n0\n6\n\n5\n5\n\n0\n5\n\n5\n4\n\nterm\n\nrows\ncolumns\ntreatments\n\ndf\n\nq \u2212 1\nq \u2212 1\nq \u2212 1\n\nresidual\n\n(q \u2212 1)(q \u2212 2)\n\nsum of squares\n(cid:1)\n(cid:1)\nr,c(yr\u00b7 \u2212 y\u00b7\u00b7)2\n(cid:1)\nr,c(y\u00b7c \u2212 y\u00b7\u00b7)2\nr,c(yt(r,c) \u2212 y\u00b7\u00b7)2\n\n(cid:1)\nr,c(yrc \u2212 yr\u00b7 \u2212 y\u00b7c \u2212 yt(r,c) + 2y\u00b7\u00b7)2\n\nrun\n\nrun\n\nday\n\n1\n\n2\n\n3\n\n4\n\nday\n\n1\n\n2\n\n3\n\n4\n\n1\n2\n3\n4\n\n8\n16\n4\n12\n\n16\n12\n8\n4\n\n4\n8\n12\n16\n\n12\n4\n16\n8\n\n1\n2\n3\n4\n\n64.2\n47.5\n54.2\n60.1\n\n59.8\n57.3\n59.9\n68.4\n\n66.2\n67.7\n57.1\n58.7\n\n63.6\n58.6\n54.1\n63.7\n\nday\n\naverage\n\nrun\n\naverage\n\nspeed\n\naverage\n\n1\n2\n3\n4\n\n4\n1\n\n2\n\n3\n\n63.45\n57.78\n56.33\n62.73\n\n1\n2\n3\n4\n\n56.50\n61.35\n62.43\n60.00\n\n4\n8\n12\n16\n\n61.85\n63.88\n59.53\n55.03\n\n2\n\n1\n4\n\n3\n\n1\n\n4\n2\n3\n\n1\n4\n\n3\n\n2\n\ny\ny\nc\nc\nn\nn\ne\ne\nc\nc\ni\ni\nf\nf\nf\nf\n\ni\ni\n\ne\ne\n\n0\n0\n7\n7\n\n5\n5\n6\n6\n\n0\n0\n6\n6\n\n5\n5\n5\n5\n\n0\n0\n5\n5\n\n5\n5\n4\n4\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n0\n\n5\n\n10\n\n15\n\n20\n\n0\n0\n\n5\n5\n\n10\n10\n\n15\n15\n\n20\n20\n\nspeed\n\nspeed\nspeed\n\n16 miles per hour, these being taken in a different order on each day. the layout\nin table 9.11 gives the speeds and the response, machine efficiency. there are two\nblocking factors, day and run, and a quantitative treatment with four levels, speed.\nthe averages, also in table 9.11, show large differences among days and speeds, and\nsmaller ones among runs, while they and the left panel of figure 9.5 show a systematic\nvariation of efficiency with speed.\n\n "}, {"Page_number": 448, "text": "436\n\n9 \u00b7 designed experiments\n\nterm\n\ndf\n\nsum of squares mean square\n\nf statistic\n\ndays\nruns\nspeeds\n\nresidual\n\n3\n3\n3\n\n6\n\n151.06\n79.74\n173.58\n\n52.23\n\n50.35\n26.58\n57.86\n\n8.71\n\n5.78\n3.05\n6.64\n\ntable 9.12 analysis of\nvariance for latin square\nfitted to field concrete\nmixer data.\n\nthe analysis of variance in table 9.12 shows evidence of day and speed differences,\nwith significance levels for their f statistics respectively 0.03 and 0.02, and weak\nevidence of differences among runs, at significance level 0.11.\n\nthe estimated mean response for the tth treatment is(cid:5)\u00b5 +(cid:5)\u03b3t = yt , and the average\n\nefficiencies for the speeds are 61.85, 63.88, 59.53, and 55.03%, suggesting that the\nbest speed is 8 mph or so. a 95% confidence interval for the true efficiency at 8 mph is\n63.88 \u00b1 t6(0.025)(s2/4)1/2, and since s2 = 8.71 and t6(0.025) = \u22122.45, this interval\n(cid:1)\nis (60.26, 67.50)%. we return to these data in example 9.12.\n\n9.2.4 factorial design\na factorial design involves a number of treatments, each with several levels, and every\ncombination of levels of the different factors appears together. thus if there are two\nfactors with two levels and one with three levels, there are 2 \u00d7 2 \u00d7 3 = 12 possible\ntreatment combinations, each of which is applied to at least one unit. a 23 factorial\ndesign was used in example 8.4.\nexample 9.6 (poisons data) the data in table 8.10 are from a 3 \u00d7 4 factorial\nexperiment with four replicates. the model\n\nyt pj = \u00b5 + \u03b1t + \u03b2 p + \u03b3t p + \u03b5t pj ,\n\nt = 1, 2, 3, 4, p = 1, 2, 3, j = 1, 2, 3, 4,\n\n(9.7)\nmodifies (8.28) by adding terms \u03b3t p representing the interaction of poisons and treat-\nments; see section 9.3.1. if the \u03b3t p are all equal, (9.7) is the two-way layout model\n(9.6), except that there are four replicates at each combination of the two factors. the\nmodel (9.7) has 20 parameters, which evidently cannot be estimated separately from\nthe 12 groups of times available. if we use lagrange multipliers to minimize the sum\nof squares subject to the constraints\n\n(cid:4)\n\n(cid:4)\n\n(cid:5)\u03b2 p =\n\n(cid:4)\n\n(cid:5)\u03b3t p =\n\n(cid:4)\n\n(cid:5)\u03b1t =\n\n(cid:5)\u03b3t p = 0,\n\nt\n\np\n\nt\n\np\n\nthen we find\n\n(cid:5)\u00b5 = y\u00b7\u00b7\u00b7, (cid:5)\u03b1t = yt\u00b7\u00b7 \u2212 y\u00b7\u00b7\u00b7, (cid:5)\u03b2 p = y\u00b7 p\u00b7 \u2212 y\u00b7\u00b7\u00b7, (cid:5)\u03b3t p = yt p\u00b7 \u2212 yt\u00b7\u00b7 \u2212 y\u00b7 p\u00b7 + yt p\u00b7,\n\nwith corresponding orthogonal decomposition\nyt pj \u2212 y\u00b7\u00b7\u00b7 = (yt pj \u2212 yt p\u00b7) + (yt p\u00b7 \u2212 yt\u00b7\u00b7 \u2212 y\u00b7 p\u00b7 + yt p\u00b7) + (yt\u00b7\u00b7 \u2212 y\u00b7\u00b7\u00b7) + (y\u00b7 p\u00b7 \u2212 y\u00b7\u00b7\u00b7).\n\n "}, {"Page_number": 449, "text": "table 9.13 sums of\nsquares for two-way\nlayout with replication,\nassuming t rows, p\ncolumns, and j replicates\nin each cell. for the\npoisons data, t = 4,\np = 3, and j = 4.\n\ntable 9.14 analyses of\nvariance for the poisons\ndata, with responses y and\n\u22121. forms and f read\ny\n\u2018mean square\u2019 and \u2018f\nstatistic\u2019.\n\n9.2 \u00b7 some standard designs\n\n437\n\nterm\n\ndf\n\nrows\ncolumns\nrows\u00d7columns\n\nt \u2212 1\np \u2212 1\n\n(t \u2212 1)(p \u2212 1)\n\n(cid:1)\n\nresidual\n\nt p(j \u2212 1)\n\nsum of squares\n(cid:1)\n(cid:1)\nt, p, j (yt\u00b7\u00b7 \u2212 y\u00b7\u00b7\u00b7)2\nt, p, j (y\u00b7 p\u00b7 \u2212 y\u00b7\u00b7\u00b7)2\n\n(cid:1)\n\nt, p, j (yt pj \u2212 yt p\u00b7)2\n\nt, p, j (yt p\u00b7 \u2212 yt\u00b7\u00b7 \u2212 y\u00b7 p\u00b7 + yt p\u00b7)2\n\nresponse y\n\nresponse y\n\n\u22121\n\nterm\n\ndf\n\nss\n\nms\n\nf\n\nss\n\nms\n\nf\n\npoisons\ntreatments\ntreatments \u00d7 poisons\n\n2\n3\n6\n\n1.033\n0.921\n0.250\n\n0.517\n0.307\n0.042\n\n23.22\n13.81\n1.87\n\n34.88\n20.41\n1.57\n\n17.44\n6.80\n0.26\n\n72.63\n28.34\n1.09\n\nresidual\n\n36\n\n0.801\n\n0.022\n\n8.64\n\n0.24\n\nthe sums of squares and their degrees of freedom are given in table 9.13. notice that\nif j = 1, the residual sum of squares is zero, because yt pj \u2261 yt p\u00b7, and the analysis\nof variance reduces to that in table 9.5, with the interaction sum of squares used\nto estimate the error variance \u03c3 2. if it was known a priori that an interaction was\nlikely to be present, replication would be essential rather than merely desirable, and\nif replication was for some reason impossible, an external estimate of \u03c3 2 would be\nrequired.\n\nthe left part of table 9.14 shows the analysis of variance. there are the expected\nstrong effects of poisons and treatments, but the interaction is less important, with a\nsignificance level of about 0.11 when treated as an f6,36 variable. the estimate of \u03c3 2\nis s2 = 0.022.\n\nunder the model (9.7) the fitted values for each cell are yt p\u00b7. this suggests a check\non the adequacy of the model. the four observations within each combination of\npoison and treatment should form a random sample from the normal distribution\nwith mean \u00b5 + \u03b1t + \u03b2 p + \u03b3t p and variance \u03c3 2, and therefore their sample variance\nt p should have the \u03c3 2\u03c7 2\ns2\n/3 distribution if the error assumption is correct. the lower\n3\nleft panel of figure 8.5 shows a systematic departure from linearity, suggesting that\nthe assumption of normal errors with constant variance is untenable. the lower right\npanel shows that the inverse survival times follow the normal error model more closely,\n\u22121\nsuggesting that it is better to replace yt pj with y\nt pj . the corresponding analysis of\nvariance, shown in the right part of table 9.14, shows that the poison and treatment\neffects explain a higher proportion of the response variability on the inverse scale,\nand that the interaction is reduced.\n\u22121, the parameter estimates for the model with no interac-\ntion are (cid:5)\u00b5 = 2.62, (cid:5)\u03b11 = 0.90, (cid:5)\u03b12 = \u22120.76, (cid:5)\u03b13 = 0.32, (cid:5)\u03b14 = \u22120.46, (cid:5)\u03b21 = \u22120.82,\n\nwith response y\n\n "}, {"Page_number": 450, "text": "438\n\n9 \u00b7 designed experiments\n(cid:5)\u03b22 = \u22120.35,(cid:5)\u03b23 = 1.17. the standard errors are 0.07 for(cid:5)\u00b5, 0.12 for the(cid:5)\u03b1s, and 0.10\nfor the(cid:5)\u03b2s, all with units of (10-hours)\n\u22121. assuggested by the panels of figure 8.5,\ntreatments b and d prolong life best, and poison 3 shortens it most. we reconsider\n(cid:1)\nthese data in example 9.8.\n\nexercises 9.2\n1\n\n(cid:1)\nconsider the one-way layout. show that when the model ytr = \u00b5 + \u03b5tr is fitted, the residual\nr,t (ytr \u2212 y\u00b7\u00b7)2 on t r \u2212 1 degrees of freedom, where y\u00b7\u00b7 is the overall\nsum of squares is\naverage. show that(cid:4)\n\n(cid:4)\n\n(cid:4)\n\n(ytr \u2212 y\u00b7\u00b7)2 =\n\n(ytr \u2212 yt.)2 +\n\n(yt. \u2212 y\u00b7\u00b7)2,\n\n2\n\n3\n\n4\n\n5\n\n6\n\nr,t\n\nr,t\n\nr,t\n\nand hence verify the contents of table 9.2.\nhow would you form a confidence interval for \u03b21 \u2212 \u03b22?\ncalculate the analysis of variance table for the data of example 9.3, and test whether there\nare differences between the diets and the groups. find the standard error of a difference\nbetween diets, and use it to give a 95% confidence interval for the mean difference in\nweight gain between diets iv and i.\nsuppose that t1 and t2 have common mean \u00b5 and variances \u03c3 2\nbe estimators of \u03c3 2\n\u03bd1, \u03c7 2\n\u03bd2.\n(a) show that if \u03c3 2\n\n2 , independently distributed as \u03c7 2\n2 are known, then \u00b5 has minimum variance unbiased estimator\n\n2 , and let s2\n\n1 , \u03c3 2\n1 , \u03c3 2\n\n1 and \u03c3 2\n\n1 and s2\n2\n\nt = t1/\u03c3 2\n1\n1/\u03c3 2\n1\n\n+ t2/\u03c3 2\n+ 1/\u03c3 2\n\n2\n\n2\n\n,\n\nwith variance \u03c3 2\n1\n(b) suppose that var(t ) isestimated by v in which \u03c3 2\nestimates. show that v has approximate mean and variance\n\n/(\u03c3 2\n1\n\n\u03c3 2\n2\n\n+ \u03c3 2\n2 ).\n\n1 and \u03c3 2\n\n2 are replaced by their\n\n\u03c3 2\n\u03c3 2\n1\n2\n+ \u03c3 2\n\n2\n\n\u03c3 2\n1\n\n,\n\n(cid:2)\n\n2\u03c3 4\n\u03c3 4\n1\n2\n+ \u03c3 2\n\u03c3 2\n1\n\n2\n\n(cid:3)4\n\n\u03c3 4\n1\n\u03bd1\n\n+ \u03c3 4\n2\n\u03bd2\n\n(cid:14)\n\n(cid:15)\n\n.\n\n2\n\n/\u03bd2).\n\n+ \u03c3 2\n\n2 )2/(\u03c3 4\n1\n\n/\u03bd1 + \u03c3 4\n\nhence show that if v is regarded as approximately \u03c7 2, then its degrees of freedom are\n(\u03c3 2\n1\n(c) compute the degrees of freedom for this approximation in example 9.4.\ngive the analysis of variance table for a two-way layout with replication, when the numbers\nof replicates in the tth row and pth column, jt p, are unequal.\nuse lagrange multipliers to verify the formulae for the estimates and fitted values given\nin example 9.5, and hence check the contents of the analysis of variance table for a latin\nsquare.\nin example 9.5, suppose that a confidence interval is required for the difference of the\nmean efficiencies between 8 and 4 mph. show that owing to the balance of the experiment,\na point estimate of this is just the difference between the average efficiencies for these\nspeeds, and that its variance is 1\n\u03c3 2. give the estimate of \u03c3 2 ignoring day and run effects,\n2\nthat is, treating the data as a one-way layout with the four levels of speed as groups. how\nmuch longer is the corresponding confidence interval than when day and run effects are\ntaken into account?\n\n "}, {"Page_number": 451, "text": "9.3 \u00b7 further notions\n\n439\n\n9.3 further notions\n9.3.1 interaction\nterms that do not act additively in a linear model are said to interact. aneasy way to\nunderstand this is by example.\n\nexample 9.7 (22 factorial experiment) a 22 factorial experiment involves a re-\nsponse measured at each combination of two factors each with two levels. as an\nillustration, consider an experiment to assess the effects of two fertilizers, in which\nthe factors are addition or not of potash and nitrogen. if the cell means are\n\nno nitrogen\n\nnitrogen\n\nno potash\n\u00b5 + \u03b1\n\n\u00b5\n\npotash\n\u00b5 + \u03b2\n\n\u00b5 + \u03b1 + \u03b2 + \u03b3\n\n,\n\nand \u03b3 = 0, the effects act additively because the addition of potash increases the mean\nresponse by \u03b2 whether or not nitrogen is present. similarly the effect of nitrogen does\nnot depend on the presence of potash.\n\nthe two treatments interact if \u03b3 is non-zero, because the effect of both treatments\ntogether is not the sum of the effects of adding them separately. the difference between\nthe effects of adding potash when there is no nitrogen present and when there is\nnitrogen present is\n\n{(\u00b5 + \u03b1 + \u03b2 + \u03b3 ) \u2212 (\u00b5 + \u03b1)} \u2212 {(\u00b5 + \u03b2) \u2212 \u00b5)} =\u03b3 ,\n\nso we can view a non-zero interaction of the two fertilizers as a differential effect of\nadding potash depending on the presence or not of nitrogen. the average effect of\nadding potash, taken over both rows of the table, is then\n\n1\n2\n\n{(\u00b5 + \u03b1 + \u03b2 + \u03b3 ) \u2212 (\u00b5 + \u03b1) + (\u00b5 + \u03b2) \u2212 \u00b5} =\u03b2 + 1\n2\n\n\u03b3 .\n\nthus if there is no interaction, \u03b2 represents the average effect of adding potash\nwhatever the level of nitrogen, but it loses this interpretation if \u03b3 is non-zero.\n\nif the model is reparametized to have cell means\n\nno potash\n\nno nitrogen \u03b20 \u2212 \u03b21 \u2212 \u03b22 + \u03b23 \u03b20 \u2212 \u03b21 + \u03b22 \u2212 \u03b23\n\u03b20 + \u03b21 \u2212 \u03b22 \u2212 \u03b23 \u03b20 + \u03b21 + \u03b22 + \u03b23\nnitrogen\n\n,\n\npotash\n\nthe overall mean is \u03b20, the average effect of adding potash is\n\n1\n2\n\n{(\u03b20 + \u03b21 + \u03b22 + \u03b23) \u2212 (\u03b20 + \u03b21 \u2212 \u03b22 \u2212 \u03b23) + (\u03b20 \u2212 \u03b21 + \u03b22 \u2212 \u03b23)\n\u2212 (\u03b20 \u2212 \u03b21 \u2212 \u03b22 + \u03b23)} =2\u03b2 2,\n\nand likewise the average effect of adding nitrogen is 2\u03b21. the difference between the\neffects of adding potash when there is no nitrogen present and when it is present is\n\n1\n2\n\n[{(\u03b20 + \u03b21 + \u03b22 + \u03b23) \u2212 (\u03b20 + \u03b21 \u2212 \u03b22 \u2212 \u03b23)} \u2212 {(\u03b20 \u2212 \u03b21 + \u03b22 \u2212 \u03b23)\n\u2212 (\u03b20 \u2212 \u03b21 \u2212 \u03b22 + \u03b23)}] = 2\u03b23.\n\n "}, {"Page_number": 452, "text": "440\n\ne\ns\nn\no\np\ns\ne\nr\n \n\nd\ne\n\nt\nt\ni\n\nf\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n0\n\n1\n2\n\n3\n\n9 \u00b7 designed experiments\n\ne\ns\nn\no\np\ns\ne\nr\n \n\nd\ne\n\nt\nt\ni\n\nf\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n0\n\n1\n2\n3\n\na\n\nb\n\nc\n\nd\n\na\n\nb\n\nc\n\nd\n\ntreatment\n\ntreatment\n\nfigure 9.6 poison data.\nthe left panel shows how\nthe fitted values under the\nmodel of no interaction,\n\n(cid:5)\u00b5 +(cid:5)\u03b1t +(cid:5)\u03b2 p, for\n\ntreatments a\u2013d depend\non poisons 1\u20133. the right\npanel shows the\ncorresponding fitted\nvalues under the model of\ninteraction,\n\n(cid:5)\u00b5 +(cid:5)\u03b1t +(cid:5)\u03b2 p +(cid:5)\u03b3t p. the\n\nvertical line in each panel\nhas length four times the\nstandard error of a fitted\nvalue.\n\nthe difference between the two parametrizations is clear from the design matrices,\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8 ,\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8 .\n\n1 \u22121 \u22121\n1\n1 \u22121\n1\n1\n\n1\n1 \u22121 \u22121\n1 \u22121\n1\n1\n\n1 0\n1 1\n1 0\n1 1\n\n0\n0\n1\n1\n\n0\n0\n0\n1\n\nthe first parametrization, in terms of \u00b5, \u03b1, \u03b2 and \u03b3 , represents changes in the mean\nresponse relative to the top left cell; this is the corner-point parametrization. in the\nparametrization using \u03b20, \u03b21, \u03b22 and \u03b23, the parameters can be interpreted as the\noverall mean, the mean effects of adding potash, nitrogen, and the effect of adding\nboth, regardless of the other parameters. the first column corresponds to the overall\nmean, the middle two columns to main effects of potash and nitrogen, and the final\ncolumn to the first-order or two-factor interaction between the two main effects.\nnotice that the interaction term is the product of the columns for the main effects\nand that the second parametrization is orthogonal, but the first is not. in practice the\nparametrization used would depend on the purpose of the analysis.\n\nif interaction is present, four parameters must be estimated from four observations.\n(cid:1)\n\nin this case \u03c3 2 would usually be estimated by replicating the experiment.\n\nthe same ideas generalize, as the next example shows.\n\nthe order of an\ninteraction is one less than\nthe number of effects it\ninvolves.\n\nexample 9.8 (poisons data) the linear model corresponding to the data discussed\nin example 9.6 is given at (9.7). if the first-order interaction parameters \u03b3 pt \u2261 0,\nthe profile of fitted values for poison p and treatments a\u2013d may be written ((cid:5)\u00b5 +\n(cid:5)\u03b11 +(cid:5)\u03b2 p, . . . ,(cid:5)\u00b5 +(cid:5)\u03b14 +(cid:5)\u03b2 p), and the effect of applying poison r instead of poison p\nis a translation of the fitted values by (cid:5)\u03b2r \u2212(cid:5)\u03b2 p. the left panel of figure 9.6 shows\nas would also be the case for the poison profiles ((cid:5)\u00b5 +(cid:5)\u03b1t +(cid:5)\u03b21, . . . ,(cid:5)\u00b5 +(cid:5)\u03b1t +(cid:5)\u03b23),\n(cid:5)\u00b5 +(cid:5)\u03b1t +(cid:5)\u03b2 p +(cid:5)\u03b3t p = yt p\u00b7, and the profiles are (y1 p, . . . , y4 p); these are shown in the\n\nunder the model with interaction, that is, including the \u03b3t p, the fitted values are\n\nthe profile of fitted values for the three poisons; of course the profiles are parallel,\n\nbecause no interaction has been fitted.\n\nright panel of the figure, and are not parallel, because under the model with interaction\n\n "}, {"Page_number": 453, "text": "table 9.15 interactions\nfor 23 factorial design.\n\n9.3 \u00b7 further notions\n\n441\n\nmain effects\n\nintercept\n\ntwo-factor\ninteractions\n\nthree-factor\ninteraction\n\nunit\n\ntreatment\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n1\na\nb\nab\nc\nac\nbc\nabc\n\ni\n\n+\n+\n+\n+\n+\n+\n+\n+\n\na\n\nb\n\nc\n\nab\n\nac\n\nbc\n\nabc\n\n\u2212 \u2212 \u2212\n+ \u2212 \u2212\n\u2212 + \u2212\n+ + \u2212\n\u2212 \u2212 +\n+ \u2212 +\n\u2212 + +\n+ + +\n\n+\n\u2212\n\u2212\n+\n+\n\u2212\n\u2212\n+\n\n+\n\u2212\n+\n\u2212\n\u2212\n+\n\u2212\n+\n\n+\n+\n\u2212\n\u2212\n\u2212\n\u2212\n+\n+\n\n\u2212\n+\n+\n\u2212\n+\n\u2212\n\u2212\n+\n\nthe effect of changing poisons is more complex than a simple translation. the profiles\nare broadly similar to those in the left panel, and the evidence for interaction is weak,\nas shown by the f statistic for treatments \u00d7 poisons in table 9.14, which gives an\n(cid:1)\noverall test for departures from the simple pattern in the left panel.\n\ntwo-factor interactions are relatively common in applications. higher-order inter-\n\nactions are rarer and can indicate outliers or a poorly fitting model.\n\nexample 9.9 (23 factorial experiment) a 23 factorial experiment has three factors,\na, b, and c, each with two levels, denoted \u22121 and +1. each of the 23 possible\ncombinations of the factor levels is applied to a unit, as in the main effects columns\nof table 9.15, where the signs only are given. this design, replicated twice, is used\nin example 8.4.\n\nthe second column of the table shows which treatments have been applied to each\nunit. under the model with main effects of a, b, and c only, no treatment is applied\nto unit 1 and its mean response is \u03b20 \u2212 \u03b2a \u2212 \u03b2b \u2212 \u03b2c, treatment a alone is applied\nto unit 2 and its mean is \u03b20 + \u03b2a \u2212 \u03b2b \u2212 \u03b2c, and so forth. the design matrix then\ncorresponds to the intercept and main effects columns, and\n\n(ya \u2212 y1 + yab \u2212 yb + yac \u2212 yc + yabc \u2212 ybc)\n\nwhere ya is the response for unit 2, yab is the response for unit 4, and so forth. thus\nthe estimate of \u03b2a is based on contrasting the responses for units to which a was\napplied with those to which it was not applied. likewise\n\n(yb \u2212 y1 + yab \u2212 ya + ybc \u2212 yc + yabc \u2212 yac),\n(yc \u2212 y1 + yac \u2212 ya + ybc \u2212 yb + yabc \u2212 yab).\n\n(cid:5)\u03b2a = 1\n\n8\n\n(cid:5)\u03b2b = 1\n(cid:5)\u03b2c = 1\n\n8\n\n8\n\nunder the model that includes the two-factor interactions \u03b2ab, \u03b2ac, and \u03b2bc as\nwell as the intercept and main effects, the mean response for unit 1 is \u03b20 \u2212 \u03b2a \u2212\n\u03b2b \u2212 \u03b2c + \u03b2ab + \u03b2ac + \u03b2bc. onincluding the two-factor interaction columns in\n\n "}, {"Page_number": 454, "text": "442\n\n9 \u00b7 designed experiments\n\nthe design matrix, we obtain\n\n(yab \u2212 ya \u2212 yb + y1 + yabc \u2212 yac \u2212 ybc + yc),\n\nwhich is based on contrasting responses for which the levels of a and b are the\nsame with those for which the levels of a and b are different. there are similar\ninterpretations of the other estimated two-factor interactions\n\nthe estimated three-factor interaction is\n\n(yac \u2212 ya \u2212 yc + y1 + yabc \u2212 yab \u2212 ybc + yb),\n(ybc \u2212 yb \u2212 yc + y1 + yabc \u2212 yab \u2212 yac + ya).\n\n(yabc + ya + yb + yc \u2212 yab \u2212 yac \u2212 ybc \u2212 y1)\n{(yabc \u2212 yac \u2212 ybc + yc) \u2212 (yab \u2212 ya \u2212 yb + y1)},\n\n(cid:5)\u03b2ab = 1\n\n8\n\n(cid:5)\u03b2ac = 1\n(cid:5)\u03b2bc = 1\n\n8\n\n8\n\n(cid:5)\u03b2abc = 1\n8\n= 1\n8\n\nwhich contrasts the contributions to the ab interaction when c is applied and when\nit is not applied. whichever of these models is fitted, the design matrix is orthogonal,\n8 i , sothe variance of each of the estimates above is \u03c3 2/8, as is\nand (x t x)\n(cid:1)\nreadily verified directly.\n\n\u22121 = 1\n\nwhen strong two-factor interaction is present, interpretation is often simplified by\nconsidering how responses behave separately for each level of one factor, and likewise\nfor higher-order interactions.\n\nconfounding\nfactorial designs make it possible to assess the effects of many treatments and their in-\nteractions in a single experiment, but when there are many factors many homogeneous\nunits must be found, and this can pose practical problems.\n\nexample 9.10 (22 factorial experiment) an experiment with two two-level factors\na and b is performed using two blocks each having two units. the possible treatments\nare 1, a, b, and ab, and there are three possible designs depending on which treatment\nappears in the same block as 1. suppose that the first block has treatments 1 and a,\nand the second has b and ab. the model with an intercept, a block effect, and the\nmain effects of a and b is\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8 =\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\ny1\nya\nyb\nyab\n\n1 0\u22121\n1 0\n1 1\u22121\n1 1\n\n\u22121\n\n1\u22121\n1\n\n1\n\n1\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8 + \u03b5,\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\n\u03b20\n\u03b1\n\u03b2a\n\u03b2b\n\nin which the design matrix has rank three because the column for b is a linear\ncombination of the first two columns. the design makes it impossible to distinguish\nthese effects, which are said to be confounded. evidently a would be confounded\nwith blocks if the first block contained treatments 1 and b and the second a and ab.\n\n "}, {"Page_number": 455, "text": "9.3 \u00b7 further notions\n\n443\n\nsuppose instead that the experiment is set up to have 1 and ab in the same block.\n\nthen the design matrix is\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\nwhich has rank four. then (cid:5)\u03b20 = 1\nthe estimates(cid:5)\u03b2a = 1\n\n\uf8f6\n0\u22121\n\uf8f7\uf8f7\uf8f8 ,\n0\n1\u22121\n1\n2 (y1 + yab), (cid:5)\u03b1 = 1\n4 (yab \u2212 y1 + ya \u2212 yb) and(cid:5)\u03b2b = 1\n\n1\n1\n1\u22121\n\n\u22121\n1\n\n1\n1\n1\n1\n\n2 (ya + yb \u2212 y1 \u2212 yab), while\n4 (yab \u2212 y1 \u2212 ya + yb) corre-\nspond to comparisons made within blocks. thus this design does allow the estimation\nof the main effects of interest, though an external estimate of the error variance \u03c3 2 is\nrequired.\n\nthe interaction between a and b would usually be estimated by\n\n(cid:5)\u03b2ab = 1\n\n(ya + yb \u2212 y1 \u2212 yab) = 1\n2\n\n4\n\n(cid:5)\u03b1,\n\nso use of this design entails sacrificing any information about this interaction.\nin examples with many factors, interactions known to be unimportant are often\ndeliberately confounded with blocks in such a way that the effects of interest\nare estimated with maximum precision in the resulting fractional factorial design.\n(cid:1)\n\nif effects are confounded by accident, then further experimentation will be needed\nto identify the parameters, unless there is external information about their values.\nsome models are intrinsically non-identifiable, however; see section 4.6.\n\n9.3.2 contrasts\nsuppose that a model has n \u00d7 p design matrix x of full rank and that we have a p \u00d7 p\ninvertible matrix a for which x a = c, where the first column of c is a column of\nones, and the remaining columns, c1, . . . ,c p\u22121, are orthogonal to the first and to\neach other. in this case c tc = diag(n, ct\np\u22121c p\u22121). let us reparametrize the\noriginal model, y = x\u03b2 + \u03b5, byletting \u03b3 = a\n\u22121\u03b2, thereby obtaining y = c\u03b3 + \u03b5.\nthe least squares estimators for the model y = c\u03b3 + \u03b5 are (cid:5)\u03b3 = (c tc)\nthe columns of c are known as orthogonal contrasts.\n\u22121c t y,\n\u22121. as c tc is a diagonal matrix, the estimate of\n\u03b3r is(cid:5)\u03b3r = ct\nr cr , and different estimates(cid:5)\u03b3r are\nsquares ss((cid:5)\u03b3 ) equals\nyt{i \u2212 c(c tc)\n\nr cr , with variance var((cid:5)\u03b3r ) = \u03c3 2/ct\n\nuncorrelated with each other and with the overall average, y. the residual sum of\n\nwith covariance matrix \u03c3 2(c tc)\n\n1c1, . . . ,c t\n\nr y/ct\n\n\u22121c t}y = yt y \u2212(cid:5)\u03b3 tc tc(cid:5)\u03b3\n\u2212 ny2 \u2212(cid:5)\u03b3 2\n1 ct\n\n= n(cid:4)\n\ny2\nj\n\n1c1 \u2212 \u00b7\u00b7\u00b7 \u2212(cid:5)\u03b3 2\n\np\u22121c p\u22121.\nas the reduction in sum of squares due to adding cr to the design matrix is(cid:5)\u03b3 2\n\nr cr , the\ntotal sum of squares can be split into the contributions from each of the columns of\n\np\u22121ct\n\nr ct\n\nj=1\n\n "}, {"Page_number": 456, "text": "444\n\n9 \u00b7 designed experiments\n\nterm\n\ndf\n\nsum of squares mean square\n\nf statistic\n\nseat\ndynamo\ntyre\nseat \u00d7 dynamo\nseat \u00d7 tyre\ndynamo \u00d7 tyre\nseat \u00d7 dynamo \u00d7 tyre\n\nresidual\n\n1\n1\n1\n1\n1\n1\n1\n\n8\n\n473.06\n39.06\n39.06\n1.56\n5.06\n0.06\n3.06\n\n33.50\n\n112.9\n9.32\n9.32\n0.37\n1.21\n0.01\n0.73\n\n473.06\n39.06\n39.06\n1.56\n5.06\n0.06\n3.06\n\n4.19\n\n\u2022\nseat\n\nt\ns\na\nr\nt\n\nn\no\nc\n \nd\ne\nz\n\ni\nl\n\na\nm\nr\no\nn\n\n0\n2\n\n5\n1\n\n0\n1\n\n5\n\n0\n\ndynamo\n\n\u2022\n\ntyre\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nt\ns\na\nr\nt\n\nn\no\nc\n \nd\ne\nz\n\ni\nl\n\na\nm\nr\no\nn\n\n0\n2\n\n5\n1\n\n0\n1\n\n5\n\n0\n\n\u2022\nseat\n\ndynamo\n\n\u2022\n\ntyre\n\u2022\n\n\u2022\n\n\u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022\n\n0.0\n\n0.5\n\n1.0\n\n1.5\n\n2.0\n\n0.0\n\n0.5\n\n1.0\n\n1.5\n\n2.0\n\nhalf-normal quantile\n\nhalf-normal quantile\n\nc, namely ny2,(cid:5)\u03b3 2\nr cr , and if the errors are normal, (cid:5)\u03b3 2\n1 ct\n(cid:5)\u03b3r (ct\nr cr )1/2, aplot of the ordered (cid:5)\u03b3 2\nplot (practical 2.1) of the |(cid:5)\u03b3r|(ct\n\n\u03c3 2/ct\n\n1c1, and so forth. if \u03b3r equals zero,(cid:5)\u03b3r has mean zero and variance\n\nr cr \u223c \u03c3 2\u03c7 2\nr ct\n\n1 . anormal scores plot of the\n1 plotting positions, or a half-normal\nr cr )1/2, helps to show which of the \u03b3r may be non-zero.\n\nr cr against \u03c7 2\n\nr ct\n\ntable 9.16 analysis of\nvariance for the cycling\ndata.\n\nfigure 9.7 half-normal\nplots of normalized\n\ncontrasts |(cid:5)\u03b3r|(ct\nthe |(cid:5)\u03b3r|(ct\n\nthe data on cycling up a\nhill. the left panel shows\nr cr )1/2 for the\n\nr cr )1/2 for\n\nlast seven columns of c1,\nwith the dotted line having\nslope s = 4.191/2, the\nresidual standard error.\n|(cid:5)\u03b3r|(ct\nthe right panel shows the\nr cr )1/2 for the last\n15 columns of c2. ineach\ncase, only contrasts\ncorresponding to main\neffects seem to be\nnon-null. see text for\ndetails.\n\nexample 9.11 (cycling data) the data on cycling up a hill are from a 23 factorial\nexperiment, replicated twice. the design matrix d for a 23 experiment, in which the\nthree main effects, the three second-order interactions, and the third-order interaction\nare all fitted, is obtained by adding ones to the pluses and minuses in table 9.15. this\nmatrix has the property that dt d = 8i8, soits columns are orthogonal contrasts. the\n16 \u00d7 8 design matrix for the replicated experiment may be written as\n\n(cid:14)\n\n(cid:15)\n\n;\n\nd\nd\n\nc1 =\n\n1c1 = 16i8, the columns of c1 are also orthogonal contrasts. table 9.16 shows\nas c t\nthe analysis of variance when this model is fitted. there are eight residual degrees of\nfreedom, and the estimate of error is s2 = 4.19. the main effects are significant, but\nthe interactions, denoted seat\u00d7dynamo and so forth, are not.\nthe left panel of figure 9.7 shows a half-normal plot of the quantities |(cid:5)\u03b3r|(ct\n\nr cr )1/2\ncorresponding to the contrasts in the last seven columns of c1; the dotted line has\nslope s. the plot confirms our impression from the analysis of variance table: only\nthe three main effects seem to be non-zero.\n\n "}, {"Page_number": 457, "text": "9.3 \u00b7 further notions\n\n445\n\nthe residual sum of squares can also be decomposed into its component degrees\n\nof freedom. to see how, we add eight columns to c1, giving\n\n(cid:14)\n\nc2 =\n\nd \u2212d\nd\nd\n\n(cid:15)\n\n,\n\n2c2 = 16i16. the right\nthe last 15 columns of which are orthogonal contrasts, as c t\npanel of figure 9.7 shows a half-normal plot of the contrasts corresponding to these\ncolumns; the eight contrasts comprising s2 \u2014 corresponding to the columns of c2 not\nin c1 \u2014 have been added to the previous seven. these eight contrast the main effects,\nthe second- and third-order interactions between the two replicates. as no degrees\nof freedom remain with which to estimate \u03c3 2, itmay be estimated by pooling those\ncontrasts that lie roughly on a straight line in the lower left corner of the graph. here\nthere seem to be about 12 such contrasts, the pooling of which gives error estimate\n(cid:1)\n3.60 on 12 degrees of freedom.\n\nexample 9.12 (field concrete mixer data)\nin example 9.5 we found evidence that\nthe efficiency of the mixer depended strongly on its speed, with the best concrete\nproduced at about 8 mph. an estimated best speed can be obtained by decomposing\nthe sum of squares due to speed using contrasts based on orthogonal polynomi-\nals. there are three degrees of freedom for speeds. one parametrization for them\ngives three columns in which the rows corresponding to the four speeds 4, 8, 12, and\n16 miles per hour are\n\n4\n8\n12\n16\n\n0\n1\n0\n0\n\n0\n0\n1\n0\n\n0\n0\n0\n1\n\n,\n\nwhereas a parametrization in terms of orthogonal polynomials gives\n\n4 \u22123\n8 \u22121 \u22121\n12\n16\n\n1 \u22121\n3\n1 \u22121 \u22123\n1\n3\n\n1\n\n,\n\nwhere the first column is linear in speed and the other two columns are obtained by\ngram\u2013schmidt orthogonalization of the square and cube of the first. the correspond-\ning columns of the design matrix, s1, s2, and s3, are orthogonal to the grand mean, to\neach other, and to the day and run effects. the parameter estimates, standard errors,\nr sr for these contrasts are given in table 9.17; note that the\n\nand sums of squares(cid:5)\u03b3 2\n\nr s t\n\ntotal sum of squares equals that for speeds in table 9.12.\n\nthe t statistics for the linear, quadratic, and cubic effects are significant at lev-\nefficiency may be summarized as (cid:5)\u00b5 +(cid:5)\u03b31s1 +(cid:5)\u03b32s2, where (cid:5)\u00b5 is the estimated grand\nels about 0.01, 0.07, and 0.38 respectively, suggesting that the effect of speed on\nmean in this parametrization. in terms of speed, x, s1 = (x \u2212 10)/2 and s2 = {(x \u2212\n10)2 \u2212 20}/16. since (cid:5)\u03b32 < 0, efficiency is maximized as a function of speed when\n(cid:5)\u03b31ds1/dx +(cid:5)\u03b32ds2/dx = 0 and the estimated best speed is 10 \u2212 4(cid:5)\u03b31/(cid:5)\u03b32 = 6.96 mph.\n\n "}, {"Page_number": 458, "text": "446\n\n9 \u00b7 designed experiments\n\nterm\n\nestimate\n\nstandard error\n\nsum of squares\n\n\u22121.24\n\u22121.63\n0.31\n\n0.329\n0.737\n0.330\n\nlinear, s1\nquadratic, s2\ncubic, s3\n\ntotal\n\n123.26\n42.58\n7.75\n\n173.58\n\ntable 9.17 field\nconcrete mixer data:\northogonal decomposition\nof sum of squares for\nspeed into linear,\nquadratic, and cubic\neffects.\n\na confidence interval for the true best speed may be obtained by the delta method or\n\nusing an exact argument (exercise 9.3.3), but the t statistic for(cid:5)\u03b32 suggests that such\n\na confidence interval will be imprecise. the right panel of figure 9.5 shows the fitted\n(cid:1)\nquadratic curve as a function of speed.\n\n9.3.3 analysis of covariance\nanalysis of covariance is intended to reduce bias or increase precision when some\nvariables cannot be controlled by design. this may arise because the importance of\nthese variables has been recognized only after randomization, because the random-\nization took them partially but not fully into account, or because their values only\nbecame available after randomization.\n\nsuppose that a model with a design matrix x from a balanced experimental setup\nis to be fitted, but that additional explanatory variables contained in the matrix z have\nbeen measured that might affect the response. the design leads us to fit the model\ny = x\u03b2 + \u03b5, but instead we fit y = x\u03b2 + z \u03b3 + \u03b5, inthe hope that inclusion of z\nwill increase the precision of(cid:5)\u03b2. however adding z removes balance and complicates\n\nthe analysis of variance. our interest is in the treatment effects after adjusting for z,\nso for analysis of variance we fit z before treatments and their interactions.\n\nexample 9.13 (cat heart data) table 9.18 shows the results from an experiment to\ndetermine the relative potencies of eight similar cardiac drugs, labelled a\u2013h, where\na is astandard. the method used was to infuse slowly a suitable dilution of the drug\ninto an anaesthetized cat. the dose at which death occurred and the weight of the\ncat\u2019s heart were recorded. the table shows y = 100 \u00d7 log dose in \u00b5gm, and, below,\nz = 100 \u00d7 log heart weight in gm. four observers each made two determinations on\neach of eight days, with a latin square design used to eliminate observer and time\ndifferences. here z cannot be known at the start of the experiment, but might be\nexpected to affect comparisons among the treatments; it is assumed that heart weight\nis unaffected by the treatments.\n\nthe left part of table 9.19 gives the analysis of variance without adjustment for\nheart weight. the seven degrees of freedom for the sum of squares between rows have\nbeen decomposed into the main effects of observer and time, and their interaction.\nthere are clearly large differences among observers, and between times, and a\nsmaller but substantial interaction between these terms, but there is little evidence\nof day-to-day variation.\n\n "}, {"Page_number": 459, "text": "table 9.18 data from\nlatin square experiment\non the potencies of eight\ncardiac drugs given to\nanaesthetized cats. the\ntable shows\ny = 100 \u00d7 log dose in\n\u00b5gm at which death\noccurred, and, below,\nz = 100 \u00d7 log heart\nweight in gm.\n\n9.3 \u00b7 further notions\n\n447\n\nobserver time\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\nday\n\n1\n\n1\n\n2\n\n2\n\n3\n\n3\n\n4\n\n4\n\nam g 75\n91\n\nf\n\npm\n\ne 81 d\n\n76\n\nam h 94 g\n\n90\n\npm b 73 a\n\n88\n\nam a 22 c\n\n90\n\npm c 46 h\n\n90\n\nam\n\nf 39 e\n\n83\n\npm d 87 b\n\n96\n\nf\n\nf 104 c\n\n52 e\n102\n74 a\n116\n\n77 a\n77\n58 g\n90\n86\n100\n102\n59 e 103\n82\n94\n39 d\n36 b\n83\n81\n52 g\n25 d\n91\n81\n56 h\n56 b\n95\n88\n72 h\n82 c\n93\n87\n\n97\n\n93\n\n84\n\nf 62 h\n\n71 c 65 d\n102\n54\n87\n66 e 94 b\n108\n86 g 84 c\n77\n32 h 43 e\n94\n59 b 42 a\n99\n28 d 52 g\n79\n87\n92 a 58\n89\n87\n\n88\n\n95\n\n90\n\nf\n\n47 b\n85\n69 c\n79\n72 d\n90\n82 h\n106\n67 g\n101\n54\n98\n86 a\n100\n92 e\n92\n\nf\n\n37 h\n73\n59 b\n105\n82 a\n96\n95 d\n89\n52\n97\n77 e\n106\n45 c\n84\n99 g\n90\n\nf\n\n63\n84\n59\n71\n58\n90\n65\n83\n34\n66\n69\n101\n70\n117\n89\n106\n\ntable 9.19 analysis of\nvariance for cats data, with\nand without adjustment\nfor heart weight.\n\nwithout adjustment\n\nwith adjustment\n\nterm\n\ndf\n\nsum of squares mean square\n\ndf\n\nsum of squares mean square\n\ntable 9.20 estimated\ndifferences between\nstandard drug a and the\ntreatments b\u2013h, without\nand with adjustment for\nthe covariate heart weight.\n\nheart weight\nobserver\ntime\nobserver \u00d7 time\nday\ndrug\n\n3\n1\n3\n7\n7\n\n9949\n2003\n2238\n922.9\n6098\n\n3316\n2003\n746\n131.8\n871.1\n\n1\n3\n1\n3\n7\n7\n\n3058\n9452\n1939\n1890\n463\n5051\n\nresidual\n\n42\n\n4874\n\n116.0\n\n41\n\n4228\n\n3058\n3151\n1939\n630\n66.3\n721.6\n\n103.1\n\nb\n\nc\n\nd\n\ne\n\nf\n\ng\n\nh\n\nunadjusted 3.75 (5.4) 11.75 (5.4) 9.13 (5.4) 29.75 (5.4) 21.12 (5.4) 25.37 (5.4) 16.87 (5.4)\nadjusted\n22.38 (5.1) 21.34 (5.3) 17.82 (5.1)\n\n8.71 (5.2) 9.02 (5.1) 28.2 (5.1)\n\n6.53 (5.2)\n\nthe estimates of the differences between the drugs and the standard, unadjusted\nfor heart weight, are given in the upper row of table 9.20. their standard errors are\nequal because of the balance. the dose of drug b needed to cause death appears not\nto differ from the standard, those for c, d and h are rather larger, and those required\nfor e, f, and g are substantially larger.\n\nthe analysis of variance with z is given in the right part of table 9.19. since interest\ncentres on the drug effects, heart weight must be fitted before the term for drugs, but\n\n "}, {"Page_number": 460, "text": "448\n\n9 \u00b7 designed experiments\n\na\n\nb\n\nc\n\nx\ny\n\n1.5, 2.2, 2.9, 4.1, 4.1\n\n9.6, 11.3, 10.3, 12.5, 12.6\n\n2.7, 3.8, 5.6, 6.4, 6.8\n8.6, 7.2, 8.9, 11.6, 11.5\n\n2.2, 3.5, 4.6, 5.5, 6.6\n4.8, 5.6, 6.2, 7.5, 6.8\n\ntable 9.21 data from a\ncompletely randomized\nexperiment on the\ncomparison of diets, with\ninitial weight x and final\nweight y.\n\notherwise it is immaterial when it is fitted; here we fit it before allowing for the\nexperimental conditions. this results in a non-unique analysis of variance: the order\nof fitting is irrelevant in the left part of the table, but matters in the right part. the\nadjustment reduces the sums of squares for the other terms, with the reduction for\ndays being largest. the estimate of \u03c3 2 adjusted for heart weight, 103.1, is somewhat\nsmaller than the unadjusted estimate, 116.0, and the precision of the comparisons\nbetween the drugs and the standard is slightly increased. in particular the adjusted\nestimates for b, c, and d, and for f, g, and h, are more similar \u2014 some of the\n(cid:1)\nvariation in the unadjusted comparisons is due to heart weights.\n\nexercises 9.3\n1\n\nsuppose that a 22 factorial experiment is to be performed using eight units in four blocks\nof two units each. show that the intercept, three block effects, and the main effects and\ninteraction between the treatments can be estimated if the treatments are allocated to blocks\nas follows: (1, a), (b, ab), (1, ab), (a, b). can they all still be estimated if an observation\nfrom the last block is lost?\ntable 9.21 gives results from a completely randomized experiment in which five individ-\nuals were allocated at random to each of three diets.\n(a) calculate the group averages and variances, and hence obtain the analysis of variance\nof the final weights, unadjusted for initial weights. give the standard errors for differences\nbetween averages for diet a and the other two diets.\n(b) use analysis of covariance to adjust for the initial weights. give the new analysis of\nvariance table, and adjusted standard errors for the differences in (a). comment.\nconsider the calculation of a 95% confidence interval for the speed that gives the maximum\nefficiency for the field concrete mixer of example 9.12.\n(a) use the delta method to show that\n\n2\n\n3\n\n(cid:14)\n\n(cid:15)\n\n(cid:14)(cid:5)\u03b31(cid:5)\u03b32\n\nvar\n\n.= \u03b3 2\n1\n\u03b3 2\n2\n\nvar((cid:5)\u03b31)\n\n+ var((cid:5)\u03b32)\n\n\u03b3 2\n1\n\n\u03b3 2\n2\n\n(cid:15)\n\n.\n\nuse this to show that(cid:5)\u03b31/(cid:5)\u03b32 has standard error 1.595, and hence give an approximate 95%\nconfidence interval for 10 \u2212 4\u03b31/\u03b32.\n(b) if \u03c8 = \u2212\u03b31/\u03b32, show that the distribution of (cid:5)\u03b32\u03c8 +(cid:5)\u03b31 is normal with mean zero\nand variance \u03c3 2(\u03c8 2v22 + v11), where v11 and v22 are the diagonal elements of the matrix\n\u22121 that correspond to \u03b31 and \u03b32. deduce that as ((cid:5)\u03b32\u03c8 +(cid:5)\u03b31)2/{s2(\u03c8 2v22 + v11)}\n(x t x)\n((cid:5)\u03b32\u03c8 +(cid:5)\u03b31)2/{s2(\u03c8 2v22 + v11)} \u2264 f1,\u03bd(1 \u2212 \u03b1).\nhas an f1,\u03bd distribution, an exact confidence region for \u03c8 is the set of values such that\na 95% confidence set for 10 + 4\u03c8 based on the calculations in example 9.12 is\n(\u2212\u221e, 9.15), (38.03,\u221e). on the same graph, plot this confidence set, the average ef-\nficiencies for the different speeds and the fitted efficiency from example 9.12 against\nspeed. do you find the exact confidence set surprising?\n(c) use part (b) to calculate the exact coverage of your delta method confidence interval.\n\n "}, {"Page_number": 461, "text": "9.4 \u00b7 components of variance\n\n449\n\n9.4 components of variance\n9.4.1 basic ideas\nour models so far have involved just one level of random variation, with all the\nresponses independent. sometimes a more complex error structure is required.\n\nthe simplest example is the one-way layout with r units in each of t blocks.\nsuppose the blocking factors are of no intrinsic interest, and the block effects may be\nthought of as being sampled at random from a population, block means being a random\nsample from a normal distribution with mean \u00b5 and variance \u03c3 2\nb . conditional on the\nblock mean, the responses for units within a block are independent normal variables\nwith mean zero and variance \u03c3 2. thus the response for the rth unit in block t is\n\nytr = \u00b5 + bt + \u03b5tr ,\n\n(9.8)\n\nwhere the bt have zero means and variances \u03c3 2\nb , the \u03b5tr have zero means and variances\n\u03c3 2, and the bt and \u03b5tr are all mutually independent. responses from different blocks\nare independent, but those within the same block are not, as cov(ytr , yts) = \u03c3 2\nb for\nr (cid:1)= s. thus the covariance matrix for the responses is block diagonal. this is called\na random effects model, asthe block effects are regarded as random variables rather\nthan fixed parameters.\n\nthe analysis of variance for the one-way layout involves the sums of squares within\n\nand between blocks\n\nssw =\n\n(cid:4)\n\nt,r\n\n(ytr \u2212 yt\u00b7)2,\n\nssb =\n\n(yt\u00b7 \u2212 y\u00b7\u00b7)2.\n\n(cid:4)\n\nt,r\n\nunder the random effects model, ytr \u2212 yt\u00b7 = \u03b5tr \u2212 \u03b5t\u00b7, and as this does not depend\non the presence of the random effects, ssw has its usual \u03c3 2\u03c7 2\nt (r\u22121) distribution.\nnow yt\u00b7 = \u00b5 + bt + \u03b5t\u00b7 \u223c n (\u00b5, \u03c3 2\n+ \u03c3 2/r), and as the yt\u00b7 are independent, the\n+ \u03c3 2/r)\u03c7 2\ndistribution of ssb is r(\u03c3 2\nt\u22121. furthermore,\nb\n\nb\n\ncov(ytr \u2212 yt\u00b7, yt\u00b7 \u2212 y\u00b7\u00b7) = cov(bt + \u03b5tr \u2212 bt \u2212 \u03b5t\u00b7, bt + \u03b5t\u00b7 \u2212 b\u00b7 \u2212 \u03b5\u00b7\u00b7) = 0,\n\nand hence the linear combinations of normal variables ytr \u2212 yt\u00b7 and yt\u00b7 \u2212 y\u00b7\u00b7 must be\nindependent. thus the sums of squares ssw and ssb have independent chi-squared\ndistributions with scale parameters \u03c3 2 and \u03c3 2 + r\u03c3 2\nb respectively. tests and confi-\ndence intervals for the ratio \u03c3 2\nb\n\n/\u03c3 2 can be based on the ft\u22121,t (r\u22121) distribution of\n\u03c3 2\n\n\u00d7 ssb/(t \u2212 1)\nssw /{t (r \u2212 1)} .\n\n\u03c3 2 + r\u03c3 2\n\nb\n\n(9.9)\n\nan alternative derivation of the independence of ssw and ssb under the random\neffects model is to argue conditionally on the values of the bt . conditional on the bt ,\nthe model is just the one-way layout described in section 9.2.1, under which ssw and\nssb are independent, and only the distribution of ssb depends on the bt . hence ssw\nand ssb are unconditionally independent.\none aspect of interest may be statements of uncertainty for the population mean\n\u00b5, which is estimated by the overall sample average, y\u00b7\u00b7 = \u00b5 + b\u00b7 + \u03b5\u00b7\u00b7. this has\n\n "}, {"Page_number": 462, "text": "450\n\n9 \u00b7 designed experiments\n\ntable 9.22 blood data:\nseven measurements from\neach of six subjects on a\nproperty related to the\nstickiness of their blood.\n\nsubject\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n68\n42\n69\n64\n39\n66\n29\n\n49\n52\n41\n56\n40\n43\n20\n\n41\n40\n26\n33\n42\n27\n35\n\n33\n27\n48\n54\n42\n56\n19\n\n40\n45\n50\n41\n37\n34\n42\n\n30\n42\n35\n44\n49\n25\n45\n\n/t + \u03c3 2/(t r) = (\u03c3 2 + r\u03c3 2\n\nvariance \u03c3 2\nb )/(t r), which is estimated unbiasedly by\nssb/{(t \u2212 1)t r}, independent of y\u00b7\u00b7, and confidence intervals are based on the tt\u22121\nb\ndistribution of (y\u00b7\u00b7 \u2212 \u00b5)/[ssb/{(t \u2212 1)t r}]1/2.\nthe assumptions of homogeneous variance across all blocks and of normality can\n\nbe checked using probability plots.\n\nexample 9.14 (blood data) six subjects were selected at random from a large\npopulation, and a property related to stickiness of samples of blood was measured\nseven times on each subject. the data are given in table 9.22.\nfor these data, ssw = 4549.7 and ssb = 1466.0 on 36and 5 degrees of freedom\nrespectively. a point estimate of the variance for different measurements on the same\nsubject is ssw /36 = 126.4. and a point estimate of the variance of mean stickiness\nbetween subjects is (ssb/5 \u2212 ssw /36)/7 = 23.83. an equi-tailed 90% confidence\n/\u03c3 2 based on (9.9) is (\u22120.01, 1.34); this overlaps the negative\ninterval for the ratio \u03c3 2\nb\n(cid:1)\nhalf-axis and would not usually be appropriate.\n\nnested variation\nthe previous example had two levels of nested variation, for subjects and for mea-\nsurements. in practice data with several levels of variation arise. consider for example\ncomparison of the success of a surgical procedure, measured on a continuous scale.\ndata are available on patients, p of whom are treated by each surgeon and with s\nsurgeons working at h hospitals. we suppose that surgeons at different hospitals\nare independent, and likewise for the patients, so patients are nested within surgeons\nwithin hospitals \u2014 there is no relation between the first patient of surgeon 1 at\nhospital 1 and the first patient of surgeon 2 at hospital 1, nor between surgeon 1 at\nhospital 1 and surgeon 1 at hospital 2. put another way, labels for patients can be\npermuted independently within each surgeon without changing the data structure,\nand likewise for surgeons within each hospital. a simple model for the outcome yhsp\nfor the pth patient of the sth surgeon at the hth hospital is\n\nyhsp = \u00b5 + bh + ehs + \u03b5hsp,\n\nh = 1, . . . , h, s = 1, . . . , s, p = 1, . . . , p,\n\n(9.10)\n\n "}, {"Page_number": 463, "text": "9.4 \u00b7 components of variance\n\n451\n\nterm\n\nbetween hospitals\n\nbetween surgeons\nwithin hospitals\nbetween patients\nwithin surgeons\n\ndf\n\nh \u2212 1\nh(s \u2212 1)\nh s(p \u2212 1)\n\n(cid:1)\n(cid:1)\n(cid:1)\n\nsum of squares\n(yh\u00b7\u00b7 \u2212 y\u00b7\u00b7\u00b7)2\n(yhs\u00b7 \u2212 yh\u00b7\u00b7)2\n(yhsp \u2212 yhs\u00b7)2\n\ne(mean square) when terms below random\n\n\u03b5\n\n\u03b5, e\n\n\u03b5, e, b\n\np s\u03b42\nb\n\n+ \u03c3 2\n\n+ p\u03b42\n+ \u03c3 2\n\ne\n\np\u03b42\ne\n\np s\u03b42\nb\n\n+ \u03c3 2\n\n+ p\u03c3 2\n+ \u03c3 2\n\ne\n\np\u03c3 2\ne\n\np s\u03c3 2\nb\n\n+ \u03c3 2\n\n+ p\u03c3 2\n+ \u03c3 2\n\ne\n\np\u03c3 2\ne\n\n\u03c3 2\n\n\u03c3 2\n\n\u03c3 2\n\ntable 9.23 analysis of\nvariance table for nested\nmodel. each sum of\nsquares is summed over h,\ns and p. mean squares are\nformed by dividing sums\nof squares by their degrees\nof freedom. \u03b42\ne are\nnon-centrality parameters\nmeasuring differences\namong the bh and ehs\nwhen they are treated as\nfixed.\n\nb and \u03b42\n\nb and \u03c3 2\n\nwhere \u00b5 is the mean success level in a population of hospitals, from which the hth\nhospital departs by bh, the ehs represent surgeon effects, and the \u03b5hsp are independent\nnormal variables with means zero and variance \u03c3 2 corresponding to the pth patient\ntreated by the sth surgeon at hospital h. ifrandom, we suppose the bh and ehs to\nbe independent normal variables with means zero and variances \u03c3 2\ne , butthe\ndecision whether they should be treated as random or as fixed depends on the context.\na potential patient able to choose his surgeon would treat bh and ehs as fixed, and\nhope to choose h and s to optimize his prospects. if on the other hand he could choose\nhis hospital but not his surgeon, he might treat the ehs as random \u2014 in effect he\nwill be operated upon by a randomly selected surgeon \u2014 but try and choose among\nhospitals, treated as fixed. a health service official hoping to estimate the national\nsuccess rate for the procedure from a sample of such data would treat the bh and the\nehs as random. the quantities of interest in the three cases are \u00b5 + bh + ehs, \u00b5 + bh,\n/s + \u03c3 2/(s p),\nand \u00b5, estimated by yhs\u00b7, yh\u00b7\u00b7, and y\u00b7\u00b7\u00b7, whose variances are \u03c3 2/p, \u03c3 2\n/(h s) + \u03c3 2/(h s p). in each case the analysis of variance is given\ne\nand \u03c3 2\nb\nby table 9.23 and depends on\n\n/h + \u03c3 2\n\ne\n\nyh\u00b7\u00b7 \u2212 y\u00b7\u00b7\u00b7 = bh \u2212 b\u00b7 + eh\u00b7 \u2212 e\u00b7\u00b7 + \u03b5h\u00b7\u00b7 \u2212 \u03b5\u00b7\u00b7\u00b7,\nyhs\u00b7 \u2212 yh\u00b7\u00b7 = ehs \u2212 eh\u00b7 + \u03b5hs\u00b7 \u2212 \u03b5h\u00b7\u00b7,\nyhsp \u2212 yhs\u00b7 = \u03b5hsp \u2212 \u03b5hs\u00b7.\n\nif all the quantities contributing to it are regarded as random, then each sum of\nsquares has a chi-squared distribution. for example, the sum of squares between\nsurgeons within hospitals is\n\n(cid:4)\n\nsss =\n\n(yhs\u00b7 \u2212 yh\u00b7\u00b7)2 = p\n\n(ehs \u2212 eh\u00b7 + \u03b5hs\u00b7 \u2212 \u03b5h\u00b7\u00b7)2,\n\nh,s, p\n\nh,s\n\nand if ehs and \u03b5hsp are random, then ehs + \u03b5hs\u00b7 is normal with mean zero and variance\n\n+ \u03c3 2/p. hence\n\n\u03c3 2\ne\n\n(cid:4)\n\np\n\nh,s\n\n(ehs \u2212 eh\u00b7 + \u03b5hs\u00b7 \u2212 \u03b5h\u00b7\u00b7)2 d= p\n\u223c (cid:2)\n\n+ \u03c3 2/p\n(cid:3)\n+ \u03c3 2\n\n\u03c7 2\nh(s\u22121)\n\n,\n\n(cid:3)\n\n(w1 + \u00b7\u00b7\u00b7 + wh )\n\n(cid:4)\n\n(cid:2)\n\n\u03c3 2\ne\n\np\u03c3 2\ne\n\n "}, {"Page_number": 464, "text": "452\n\n9 \u00b7 designed experiments\n\nwhere the wh are a random sample from the \u03c7 2\ns\u22121 distribution. if the ehs are fixed,\nthen ehs + \u03b5hs\u00b7 is normal with mean ehs and variance \u03c3 2/p and hence sss has a non-\ncentral chi-squared distribution with h(s \u2212 1) degrees of freedom and non-centrality\nparameter h(s \u2212 1)\u03b42\nh,s(ehs \u2212 eh\u00b7)2 (problem 2.12). such calculations give\nthe entries in table 9.23, in which (h \u2212 1)\u03b42\ne ) = \u03c3 2\nand e(\u03b42\n\nh(bh \u2212 b\u00b7)2. note that e(\u03b42\n\n= (cid:1)\n\n= p\n\n(cid:1)\n\nb) = \u03c3 2\nb .\n\nunder the model with bh and ehs fixed, ratios of mean squares can be used to test for\ndifferences among surgeons and hospitals, for example comparing the ratio of mean\nsquares for the last two lines of table 9.23 with the fh(s\u22121),h s(p\u22121) distribution.\n\nb\n\ne\n\ne\n\nthe assumptions underlying this model would need careful scrutiny in applications:\nfrom what populations are patients, surgeons, and hospitals drawn, and in what sense\ncan they be treated as random samples?\n\nnesting is fundamentally different from the type of classification described earlier.\nconsider a two-way layout in which factors a and b with t and r levels respectively\nare applied to t r units. then if the levels of b among y11, . . . , y1r were permuted,\nthe same permutation would have to be applied to those of yt1, . . . , yt r for each t,\nbecause the second subscript corresponds to the same treatment for y2r and y1r , for\nexample. the two classifications are then said to be crossed. inthe random effects\nmodel described at the start of this section, however, the labelling is essentially ar-\nbitrary, yt1, . . . , yt r being simply replicate observations; here permutation of any or\nall of these groups of observations should not affect analysis. compare examples 9.3\nand 9.14.\n\nit is crucial that crossed and nested effects be distinguished. typically the levels of\ncrossed effects are of intrinsic interest and are represented by fixed parameters, while\nparameters associated with nested suffixes are treated as random. different levels of\nnesting then correspond to different variance components. however it may be hard\nto write down the model appropriate to a complex design.\n\nsplit-unit experiments\nsome experiments are performed with certain treatments applied to entire units and\nothers to sub-units. as such designs originally arose in agriculture, with units being\nfor instance plots of land sown with plant varieties, sub-plots of which were treated\nwith different fertilisers, they are often called split-plot experiments. they also arise in\nindustrial applications, where certain aspects of a manufacturing process may be more\neasily varied than others, and in medical settings where units are often patients, each\nwith measurements taken in succession over a period, giving a series of correlated\nresponses. such designs are useful if it is already known that whole-unit treatments\ndiffer substantially and interest centres on sub-unit treatments and their interactions,\nor if physical constraints impose them; they can also arise by accident. the key\nidea is that there is variation within units (that is, between sub-units) as well as\nbetween units. analysis of variance is effectively performed at two levels, as discussed\nbelow.\n\nsuppose there are b blocks of w units, to each of which a whole-unit treatment is\napplied according to a randomized block design, for example. units themselves are\n\n "}, {"Page_number": 465, "text": "9.4 \u00b7 components of variance\n\n453\n\nsplit into s sub-units, with a sub-unit treatment randomized to each. the corresponding\nlinear model is\n\nybws = \u00b5 + \u03b2b + \u03b3w + ubw + \u03b6s + \u03c4ws + \u03b5bws , b = 1, . . . , b, w = 1, . . . , w,\n\ns = 1, . . . , s,\n\nwe use the roman letter u\nto indicate that the\nwhole-unit effects are\nregarded as random\nvariables rather than as\nparameters.\n\nwhere the whole-unit effects are the overall mean \u00b5, the block and whole-unit treat-\nment parameters \u03b2b and \u03b3w , and the whole-unit errors ubw , taken to be independent\nnormal variables with mean zero and variance \u03c3 2\nu . the sub-unit treatment effects are\n\u03b6s, the interactions between sub- and whole-unit treatments \u03c4ws and the sub-unit errors\n\u03b5bws, taken to be normal with mean zero and variance \u03c3 2 independent of each other\nand of the ubw . terms \u03bebs are not included because interaction between blocks and\nsub-units makes no sense.\n\nunder this model different treatments are analyzed at different levels. whole-unit\n+ \u03c3 2/s estimated by the residual mean square from a ran-\naverages have variance \u03c3 2\ndomized block analysis of these averages, with (b \u2212 1)(w \u2212 1) degrees of freedom.\nu\nwhole-unit treatments are compared using contrasts of these averages such as\n\ny\u00b72\u00b7 \u2212 y\u00b71\u00b7 = \u03b32 \u2212 \u03b31 + u\u00b72 \u2212 u\u00b71 + \u03b5\u00b72\u00b7 \u2212 \u03b5\u00b71\u00b7,\n\n/b + 2\u03c3 2/(b s). comparisons of sub-unit treatments and their\nwhose variance is 2\u03c3 2\ninteractions with whole-unit treatments use the bw (s \u2212 1) remaining degrees of\nu\nfreedom and involve quantities such as\n\ny\u00b7\u00b72 \u2212 y\u00b7\u00b71 = \u03b62 \u2212 \u03b61 + \u03c4\u00b72 \u2212 \u03c4\u00b71 + \u03b5\u00b7\u00b72 \u2212 \u03b5\u00b7\u00b71,\n\n(y\u00b722 \u2212 y\u00b721) \u2212 (y\u00b712 \u2212 y\u00b711) = \u03c422 \u2212 \u03c421 \u2212 \u03c412 + \u03c411 + \u03b5\u00b722 \u2212 \u03b5\u00b721 \u2212 \u03b5\u00b712 + \u03b5\u00b711,\nwith variances respectively 2\u03c3 2/(bw ) and 4\u03c3 2/b. asthere are s \u2212 1 degrees of\nfreedom for sub-unit treatments and (s \u2212 1)(w \u2212 1) degrees of freedom for their\ninteractions with whole-unit treatments,\n\nbw (s \u2212 1) \u2212 (s \u2212 1) \u2212 (s \u2212 1)(w \u2212 1) = w (b \u2212 1)(s \u2212 1)\n\ndegrees of freedom remain for estimation of \u03c3 2. ifthe variability between whole units\nis larger than that within them, that is, \u03c3 2 < \u03c3 2\nu , then comparisons among sub-unit\ntreatments and their interactions with whole-unit treatments will be more precise than\namong whole-unit treatments themselves.\n\n\u25e6\n\nexample 9.15 (cake data) table 9.24 gives data from an experiment in which six\ndifferent temperatures for cooking three recipes for chocolate cake were compared.\neach time a mix was made using one of the recipes, enough batter was prepared\nfor six cakes, which were then randomly allocated to be cooked at temperatures\n175, 185, . . . ,225\nc. thus mixes correspond to blocks, recipes are the whole-unit\ntreatments and baking temperatures the sub-unit treatments. we suppose that the 15\nmixes of each recipe were made in order 1, . . . ,15, so that mix is a surrogate for time.\nthe response is the breaking angle, found by fixing one half of a slab of cake, then\npivoting the other half about the middle until breakage occurs. let yrmt denote the\nresponse for the rth recipe, mth mixture and tth temperature, where r = 1, . . . ,3,\n\n "}, {"Page_number": 466, "text": "454\n\nrecipe\n\ntemp\n\u25e6\n\nc\n\n9 \u00b7 designed experiments\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\nmix\n\n\u25e6\n\ntable 9.24 data on\nbreaking angles (\n) of\nchocolate cakes (cochran\nand cox, 1959, p. 300).\n\n1\n\n2\n\n3\n\n175\n185\n195\n205\n215\n225\n\n175\n185\n195\n205\n215\n225\n\n175\n185\n195\n205\n215\n225\n\n42\n46\n47\n39\n53\n42\n\n39\n46\n51\n49\n55\n42\n\n46\n44\n45\n46\n48\n63\n\n47\n29\n35\n47\n57\n45\n\n35\n46\n47\n39\n52\n61\n\n43\n43\n43\n46\n47\n58\n\n32\n32\n37\n43\n45\n45\n\n34\n30\n42\n35\n42\n35\n\n33\n24\n40\n37\n41\n38\n\n26\n32\n35\n24\n39\n26\n\n25\n26\n28\n46\n37\n37\n\n38\n41\n38\n30\n36\n35\n\n28\n30\n31\n37\n41\n47\n\n31\n30\n29\n35\n40\n36\n\n21\n25\n31\n35\n33\n23\n\n24\n22\n22\n29\n35\n26\n\n24\n29\n29\n29\n24\n35\n\n24\n33\n30\n30\n37\n35\n\nl\n\ni\n\na\nu\nd\ns\ne\nr\n\n0\n2\n\n0\n1\n\n0\n\n0\n1\n-\n\n24\n27\n28\n33\n34\n23\n\n27\n26\n32\n28\n32\n33\n\n24\n18\n21\n26\n28\n28\n\n24\n33\n27\n31\n30\n33\n\n21\n24\n24\n27\n37\n30\n\n26\n28\n27\n27\n35\n35\n\n33\n39\n33\n28\n33\n30\n\n20\n27\n33\n31\n28\n33\n\n28\n25\n26\n25\n38\n28\n\n28\n31\n27\n39\n35\n43\n\n23\n28\n31\n34\n31\n29\n\n24\n30\n28\n35\n33\n28\n\n29\n28\n31\n29\n37\n33\n\n32\n35\n30\n27\n35\n30\n\n28\n29\n43\n28\n33\n37\n\n24\n40\n29\n40\n40\n31\n\n23\n25\n22\n19\n21\n35\n\n19\n22\n27\n25\n25\n35\n\n26\n28\n32\n25\n37\n33\n\n21\n21\n28\n26\n27\n20\n\n21\n28\n25\n25\n31\n25\n\n26\n23\n25\n27\n33\n35\n\n22\n25\n26\n26\n29\n36\n\n20\n21\n31\n24\n30\n33\n\n24\n33\n23\n32\n31\n34\n\n26\n23\n24\n31\n27\n37\n\n24\n23\n21\n24\n21\n35\n\nl\n\ni\n\na\nu\nd\ns\ne\nr\n\n0\n1\n\n0\n\n0\n1\n-\n\n5 10 15\n\n5 10 15\n\n5 10 15\n\n175 185 195 205 215 225\n\nrecipe/mixture\n\ntemperature (c)\n\nfigure 9.8 cake data.\nleft: variation of\nyrmt \u2212 yrm\u00b7 across mixes\nfor the three recipes. the\nvertical lines demarcate\nresults for the three\nrecipes. right:\ndependence of yrmt \u2212 y\u00b7\u00b7t\non temperature.\n\nm = 1, . . . ,15 and t = 1, . . . ,6. the model we consider is\n\nyrmt = \u00b5 + \u03b2r + \u03b3m + urm + \u03b6t + \u03c4rt + \u03b5rmt ,\n\niid\u223c n (0, \u03c3 2\n\nwhere the urm\nu ) represent the whole-unit errors corresponding to the mixes\niid\u223c n (0, \u03c3 2) denote the sub-unit errors. we treat the\nmade for each recipe, and the \u03b5rmt\n\u03b2r and \u03b3m as parameters and the urm as random variables because if the experiment\nwas repeated, the recipes would be unchanged and the time ordering would still arise,\nbut the mixes would be different.\nthe left panel of figure 9.8 shows how yrmt \u2212 yrm\u00b7 varies across mixes for the three\nrecipes. there is evidently a systematic effect of mix, with responses for the first few\n\n(9.11)\n\n "}, {"Page_number": 467, "text": "9.4 \u00b7 components of variance\n\n455\n\ntable 9.25 analysis of\nvariance on a split-unit\nbasis for cakes data. the\nf statistics for the upper\npart are computed using\nthe residual sum of\nsquares (a) for contrasts\namong whole units. those\nin the lower part are\ncomputed using (b), the\nresidual sum of squares\nfor contrasts among split\nunits. there are large\ndifferences among mixes\nand temperatures, but not\namong recipes. the\ntemperature effect is\nessentially linear.\n\ntable 9.26 average log\nbreaking angle (degrees)\nof cakes by recipe and\ntemperature.\n\nsource of variation\n\ndf\n\nsum of squares mean square\n\nf\n\nmixes\nrecipes\nresidual (a)\n\ntemperatures\n\nlinear\nquadratic\ncubic, quartic, quintic\nrecipes \u00d7 temperatures\nresidual (b)\n\n14\n2\n28\n\n5\n1\n1\n3\n10\n210\n\n8.159\n0.186\n1.343\n\n2.051\n1.925\n0.021\n0.105\n0.176\n4.040\n\n0.583\n0.093\n0.048\n\n0.410\n1.925\n0.021\n0.035\n0.018\n0.019\n\n12.15\n1.93\n\n21.32\n100.08\n1.08\n1.82\n0.91\n\ntemperature (\n\n\u25e6\n\nc)\n\nrecipe\n\n175\n\n185\n\n195\n\n205\n\n215\n\n225\n\naverage\n\n1\n2\n3\n\n3.350\n3.270\n3.293\n\n3.433\n3.355\n3.331\n\n3.409\n3.428\n3.428\n\n3.493\n3.443\n3.405\n\n3.638\n3.505\n3.516\n\n3.535\n3.537\n3.538\n\n3.476\n3.423\n3.419\n\naverage\n\n3.304\n\n3.373\n\n3.422\n\n3.447\n\n3.553\n\n3.537\n\n3.439\n\nmixes for each recipe substantially greater than for later ones, but any recipe differ-\nences seem small. the right panel shows roughly linear dependence on temperature\nof the differences yrmt \u2212 y\u00b7\u00b7t , from which whole-unit variation has been eliminated,\nbut there is a perceptible increase in variance with mean. this suggests use of log-\ntransformed responses, which is confirmed by a box\u2013cox analysis (example 8.23).\ntable 9.25 shows the analysis of variance for the model fitted to the log responses.\nthere are 3 \u00d7 15 = 45 whole units, from which a grand mean and 44 contrasts may be\ncomputed. the component of variance for these 44 degrees of freedom is shown in the\nupper part of the table, split into 14 degrees of freedom among mixes, 2 among recipes\nand 28 residuals. the mean square at (a) estimates \u03c3 2 + 6\u03c3 2\nu and is the appropriate\nbasis for comparison of recipes and mixes. there are large differences among mixes\nbut not among recipes.\nthe lower part of the table shows the 3 \u00d7 15 \u00d7 (6 \u2212 1) = 225 degrees of freedom\nfor contrasts within whole units, of which there are 5 among temperatures and\n(3 \u2212 1) \u00d7 (6 \u2212 1) for the recipe\u00d7 temperature interaction. the mean square for resid-\nual (b) estimates \u03c3 2, and comparison with (a) gives estimate (0.048 \u2212 0.019)/6 =\n0.0035 of \u03c3 2\nu , rather small variation among mixes. a split of the overall temperature\neffect into linear, quadratic and remaining effects confirms the linearity of the effect\nof temperature on the response.\n\ntable 9.26 shows average log breaking angles by recipe and temperature. each\n+ \u03c3 2/15, but while\n\naverage is based on 15 raw observations and has variance \u03c3 2\nu\n\n "}, {"Page_number": 468, "text": "456\n\n9 \u00b7 designed experiments\n\ndifferences between rows involve the u\u2019s, those between columns do not. differ-\nences between two recipe averages and between two temperature averages have vari-\n+ \u03c3 2/6)/15 and 2(\u03c3 2/15)/3, estimated by 2 \u00d7 0.048/90 = 0.0333 and\nances 2(\u03c3 2\n2 \u00d7 0.019/45 = 0.0292. the difference between two temperature averages for one\nu\n\u2212 yr\u00b7t2, does not depend on the urm, soits variance is 2\u03c3 2/15, while the\nrecipe, yr\u00b7t1\ndifference of two recipe averages for a given temperature, yr1\u00b7t \u2212 yr2\u00b7t , involves both\n+ \u03c3 2)/15; these variances are estimated respectively\nu\u2019s and \u03b5\u2019s and has variance 2(\u03c3 2\nby 2 \u00d7 0.019/15 = 0.0502 and 2 \u00d7 {5 \u00d7 0.019 + 0.048)/90 = 0.0562.\nu\n\nthe best summary of the results here is table 9.26, supplemented by the standard\n(cid:1)\n\nerrors for comparisons among the averages.\n\n9.4.2 linear mixed models\nin many situations the comparison of treatments is complicated by correlations among\nthe responses. in medical settings, for example, a common design involves repeated\nmeasures on the same individual, leading to repeated measures or longitudinal data.\nrelated designs arise in many types of investigation. although the notion of levels of\nvariation underlying the classical split-plot experiment remains very useful, such data\nare rarely neatly balanced and their analysis and interpretation is less straightforward.\nin this section we briefly put such experiments in a more general context.\n\nwhen confronted with a complex experiment, it is helpful to ask if it is reasonable\nto assume that the levels of certain factors have been selected from a population. if\nso, we ask if interest resides purely in the population, or also in the realized values of\nrandom variables sampled from it. when this latter is the case, then we must estimate\nnot only properties of the population but also the underlying variables. in dairy herd\nbreeding experiments, for example, bulls and cows are mated and the milk yield of\ntheir daughters is treated as the response. as any repetition of the experiment would\ninvolve different animals, they are regarded as randomly sampled from a population.\nit is useful to estimate effects for individual animals, however, in order to retain for\nfuture breeding those bulls whose daughters give the best yield. thus although a\nrandom effects model is appropriate, estimates of the random effects are required.\nsimilar considerations arise in many other contexts, and we now discuss inference\nfor random effects.\n\nwe consider normal linear models of form\n\ny = x\u03b2 + zb + \u03b5,\n\n(9.12)\nwhere in addition to the usual setup the n \u00d7 q matrix z indicates how the response\nvector y depends on the q \u00d7 1 vector of unobserved random variables b. this is called\na mixed model because the response depends on random variables b as well as on\nfixed parameters \u03b2. ifb is normal with mean zero and covariance matrix \u0001b, then we\nmay write\n\ny | b \u223c nn(x\u03b2 + zb, \u0001)\n\nand\n\nb \u223c nq(0, \u0001b).\n\n(9.13)\n\n "}, {"Page_number": 469, "text": "9.4 \u00b7 components of variance\nthus the marginal density of y is normal with mean x\u03b2 and variance matrix z \u0001b z t +\n\u0001, which does not depend on \u03b2. inmost cases \u0001 = \u03c3 2 in, where \u03c3 2 = var(\u03b5 j ), and\nlater it will be useful to write z \u0001b z t + \u0001 = \u03c3 2\u03d2\u22121, say. we use \u03c8 to denote the\nvector of distinct variance ratios appearing in \u03d2\u22121.\n\n457\n\nj = 1, 2.\n\nexample 9.16 (longitudinal data) a short longitudinal study has one individual\nallocated to the treatment and two to the control, with observations\ny1 j = \u03b20 + b1 + \u03b51 j , y21 = \u03b20 + b2 + \u03b521, y3 j = \u03b20 + \u03b21 + b3 + \u03b53 j ,\nthus there are two measurements on the first and third individuals, and just one on the\nsecond. the b j represent variation among individuals and the \u03b5i j variation between\nmeasures on the same individuals. if the b\u2019s and \u03b5\u2019s are all mutually independent with\n\uf8eb\n\uf8eb\nvariances \u03c3 2\nb and \u03c3 2, then\n1\ny11\n1\ny12\n1\ny21\n1\ny31\n1\ny32\n\n0\n0\n1\n0\n0\nand this fits into formulation (9.12) with \u0001b = \u03c3 2\nb i3 and \u0001 = \u03c3 2 i5. here \u03c8 comprises\nthe scalar \u03c3 2\n/\u03c3 2, and hence the variance matrix\nb\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 =\n\n\uf8f6\n\uf8f8 +\n\n\uf8eb\n\uf8ed b1\n\n\u03b511\n\u03b512\n\u03b521\n\u03b531\n\u03b532\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 ,\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n0\n0\n0\n1\n1\n\n1\n1\n0\n0\n0\n\n0\n0\n0\n1\n1\n\n\u03b20\n\u03b21\n\nb2\nb3\n\n\uf8f6\n\n\uf8f6\n\n\uf8eb\n\n\uf8f6\n\n\uf8eb\n\n\uf8f6\n\n(cid:15)\n\n(cid:14)\n\n+\n\n\u0001 + z \u0001b z t =\n\nmay be written as\n\n\u03c3 2\u03d2\u22121 = \u03c3 2\n\n\u03c3 2\nb\n\n\uf8eb\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n+ \u03c3 2\n\u03c3 2\nb\n0\n0\n0\n\n\u03c3 2\nb\n\n\u03c3 2\n+ \u03c3 2\nb\n0\n0\n0\n\n\u03c3 2\nb\n\n0\n0\n+ \u03c3 2\n0\n0\n\n0\n0\n0\n+ \u03c3 2\n\u03c3 2\nb\n\n\u03c3 2\nb\n\n\uf8eb\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n1 + \u03c8\n\n\u03c8\n0\n0\n0\n\n\u03c8\n\n1 + \u03c8\n0\n0\n0\n\n0\n0\n1 + \u03c8\n0\n0\n\n0\n0\n0\n1 + \u03c8\n\n\u03c8\n\n0\n0\n0\n\u03c8\n1 + \u03c8\n\n\uf8f6\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n\u03c3 2\nb\n\n0\n0\n0\n\u03c3 2\n+ \u03c3 2\nb\n\uf8f6\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 ,\n\nof block diagonal form.\n\n(cid:1)\n\nin principle likelihood inference for the parameters of this model may be based on\n\nthe marginal normal density of y, which gives log likelihood\n\n(cid:18)(\u03b2, \u03c3 2, \u03c8) \u2261 \u2212 1\n\n2\u03c3 2 (y \u2212 x\u03b2)t \u03d2(y \u2212 x\u03b2) \u2212 n\n\n2\n\nlog \u03c3 2 + 1\n2\n\nlog|\u03d2|,\n\nwhere \u03d2 depends on \u03c8. forknown \u03c8 the maximum likelihood estimators of \u03b2 and\n\u03c3 2 are\n\n(cid:5)\u03b2\u03c8 = (x t\u03d2 x)\n\n\u22121 x t\u03d2 y, (cid:5)\u03c3 2\n\n\u03c8 = n\n\n\u22121(y \u2212 x(cid:5)\u03b2)t\u03d2(y \u2212 x(cid:5)\u03b2),\n\n "}, {"Page_number": 470, "text": "458\n\n9 \u00b7 designed experiments\nso the profile log likelihood for \u03c8 is (cid:18)p(\u03c8) \u2261 \u2212 1\n2 log|\u03d2|. wemaximize\nthis to estimate \u03c8, and then obtain maximum likelihood estimates(cid:5)\u03b2(cid:5)\u03c8 and(cid:5)\u03c3 2(cid:5)\u03c8 . thus\n\n2 n log(cid:5)\u03c3 2\n\n\u03c8 + 1\n\ninference boils down to maximization of (cid:18)p(\u03c8) .\n\nunfortunately life is not so simple. one difficulty is that the maximum likelihood\nvariance estimators can have large downward bias because no adjustment is made for\nthe degrees of freedom lost in estimating the p \u00d7 1 vector \u03b2. insuch models p can\nbe large, and then it is important to replace the divisor n in(cid:5)\u03c3 2 by the true degrees of\nfreedom n \u2212 p. adjustment both for this and for estimation of the elements of \u03c8 can\nbe performed by maximizing the modified log likelihood\n\n(cid:18)(\u03b2, \u03c3 2, \u03c8) + p\n2\n\nlog \u03c3 2 \u2212 1\n2\n\nlog|x t\u03d2 x|.\n\nthis procedure, known as reml or restricted maximum likelihood estimation, is\njustified in section 12.2. it turns out to be equivalent to use of a marginal likelihood,\nthat is, a likelihood formed from a cunningly chosen marginal density rather than the\nfull density of the data.\na second difficulty is that the domain for \u03c8 is [0,\u221e)dim \u03c8 . ifthe maximum occurs on\nthe boundary of this set, then standard likelihood theory does not apply to confidence\nfrom the boundary. if so, standard errors for (cid:5)\u03b2 are found from \u03c3 2(x t\u03d2 x)\nintervals and so forth. care must anyway be used unless the maximum lies well away\n\u22121 with\n\nparameters replaced by estimates.\n\na third difficulty is computational: in realistic problems the matrices involved in\nsuch models can be large enough that even specially designed optimization routines\nconverge only slowly.\n\nprediction of random effects\nonce estimates of \u03b2, \u03c3 2, and \u03c8 have been obtained, the question arises how to perform\ninference for the random variables b. weprefer to reserve the term estimation for\nunknown parameters and to speak of prediction of unobserved random variables. in\nnormal models it is natural to choose the predictor \u02dcb = \u02dcb(y) to bethe function of y\nthat minimizes the mean squared prediction error\n\ne[{\u02dcb(y) \u2212 b}t{\u02dcb(y) \u2212 b}],\n\ndim \u03c8 is the dimension of\n\u03c8.\n\nwhere the expectation is over both b and y. it isstraightforward to show that this is\nachieved by taking \u02dcb(y) = e(b | y), the conditional mean of b given y. asb and y\nhave a joint normal distribution, we obtain (exercise 9.4.5)\n\ne(b | y) = (cid:2)\nvar(b | y) = (cid:2)\n\nz t\u0001\u22121 z + \u0001\u22121\nz t\u0001\u22121 z + \u0001\u22121\n\nb\n\nb\n\n(cid:3)\u22121 z t\u0001\u22121 (y \u2212 x\u03b2) ,\n(cid:3)\u22121 .\n\n(9.14)\n\n(9.15)\n\nreplacement of the unknown parameters by estimates results in the predictions \u02dcb and\ntheir estimated variance. it turns out that the \u02dcb are best linear unbiased predictors\n(problem 9.6). if \u0001\u22121\nb was absent, then (9.14) would be the weighted least squares\nestimator from regressing (y \u2212 x\u03b2) onthe columns of z with weight matrix \u0001\u22121.\n\nthey are often called\nblups.\n\n "}, {"Page_number": 471, "text": "9.4 \u00b7 components of variance\nthe presence of \u0001\u22121\nleast squares estimator, and for this reason \u02dcb is known as a shrinkage estimator.\n\nb means that \u02dcb is shrunk towards zero compared to the weighted\n\n459\n\nthe residuals too are modified due to shrinkage. as\n\ny \u2212 x(cid:5)\u03b2 = z \u02dcb + y \u2212 x(cid:5)\u03b2 \u2212 z \u02dcb\n\n= z \u02dcb +(cid:16)\n\nin \u2212 z\n\n(cid:2)\n\nz t(cid:5)\u0001\u22121 z +(cid:5)\u0001\u22121\n\n(cid:3)\u22121 z t(cid:5)\u0001\u22121\n\n(cid:17)\n\n(y \u2212 x(cid:5)\u03b2),\n\nthe residuals y \u2212 x(cid:5)\u03b2 split into two parts, the first z \u02dcb being attributable to the predicted\nrandom effects, and the second being the usual residual y \u2212 x(cid:5)\u03b2 shrunk towards zero;\n\nb\n\nthis estimates \u03b5.\n\nexample 9.17 (one-way layout) consider the unbalanced one-way layout model\n\nyi j = \u00b5 + bi + \u03b5i j ,\n\nj = 1, . . . ,n i ,\n\ni = 1, . . . , q,\n\nb ) independently of the individual errors\n\niid\u223c n (0, \u03c3 2\n\nin which the group effects bi\n\u03b5i j\n\niid\u223c n (0, \u03c3 2). this generalizes (9.8). in terms of (9.12),\n1n1\n0\n...\n0\n\n\u0001 = \u03c3 2 in, \u0001b = \u03c3 2\n\nx = 1n,\n\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ed\n\nz =\n\nb iq ,\n\n0\n1n2\n...\n0\n\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f8 ,\n\n\u00b7\u00b7\u00b7\n0\n\u00b7\u00b7\u00b7\n0\n...\n...\n\u00b7\u00b7\u00b7 1nq\n\nwhere n = n1 + \u00b7\u00b7\u00b7 +n q. substitution into (9.14) and (9.15) reveals that the ith\nelement of \u02dcb and its estimated variance are\n(cid:3) ,\n\n1\n\n,\n\n\u02dcbi =\n\nyi\u00b7 \u2212 y\u00b7\u00b7\n(cid:2)\nni(cid:5)\u03c3 2\n\n1 +(cid:5)\u03c3 2/\n\nb\n\n1/(cid:5)\u03c3 2\n\nb\n\n+ ni /(cid:5)\u03c3 2\n\nso the fixed-effects estimator yi\u00b7 \u2212 y\u00b7\u00b7 is shrunk towards zero by an amount that\n(cid:5)\u03c3 2/ni (cid:8)(cid:5)\u03c3 2\ndepends on the estimated variance ratio. the shrinkage will be considerable if\nb , corresponding to large variation in the group averages owing to in-\ndividual variances compared to the variation between groups, as in example 9.14.\nthe data are then almost a simple random sample of size n, sostrong shrinkage is not\nsurprising.\n\u2192 0, \u03c3 2 \u2192 0,\nthe variance formula is also instructive, as var(\u02dcbi | y) \u2192 0 when \u03c3 2\nor ni \u2192 \u221e. inthe first case, there is no variation between groups, and hence bi = 0\nwith probability one. in the second two cases, the value of bi is known exactly, because\nvariation around it is negligible. the practical implication is that consistent inference\nb and \u03c3 2 take positive values: even if q \u2192 \u221e, the amount\nfor bi is impossible when \u03c3 2\nof information on any given bi does not accumulate unless ni \u2192 \u221e, and this is rarely\n(cid:1)\nthe case. this applies to estimation of random effects more generally.\nexample 9.18 (rat growth data) table 9.27 gives the weights of n = 30 young\nrats measured for five weeks. the left panel of figure 9.9 shows that although the\nweight of each rat grows roughly linearly, neither slope nor intercept appears to be\ncommon to all the animals. this is confirmed by the analysis of variance from fitting\nstandard linear models with common intercept and slope, different intercepts, and\nboth intercepts and slopes different: the f tests are all highly significant.\n\nb\n\n "}, {"Page_number": 472, "text": "460\n\n9 \u00b7 designed experiments\n\nweek\n\nweek\n\n1\n\n2\n\n3\n\n4\n\n5\n\n1\n\n2\n\n3\n\n4\n\n5\n\ntable 9.27 weights\n(units unknown) of\n30 young rats over a\nfive-week period (gelfand\net al., 1990).\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n151\n145\n147\n155\n135\n159\n141\n159\n177\n134\n160\n143\n154\n171\n163\n\n199\n199\n214\n200\n188\n210\n189\n201\n236\n182\n208\n188\n200\n221\n216\n\n246\n249\n263\n237\n230\n252\n231\n248\n285\n220\n261\n220\n244\n270\n242\n\n283\n293\n312\n272\n280\n298\n275\n297\n340\n260\n313\n273\n289\n326\n281\n\n320\n354\n328\n297\n323\n331\n305\n338\n376\n296\n352\n314\n325\n358\n312\n\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\nl\n\n)\n?\n(\n \nt\n\ni\n\nh\ng\ne\nw\n\n0\n5\n3\n\n0\n0\n3\n\n0\n5\n2\n\n0\n0\n2\n\n0\n5\n1\n\n \n\ne\nd\no\nm\nd\ne\nx\nm\nm\no\nr\nf\n \n\n \n\ni\n\ne\n\nt\n\na\nm\n\n \n\ni\nt\ns\ne\ne\np\no\ns\n\nl\n\n160\n142\n156\n157\n152\n154\n139\n146\n157\n132\n160\n169\n157\n137\n153\n\n207\n187\n203\n212\n203\n205\n190\n191\n211\n185\n207\n216\n205\n180\n200\n\n248\n234\n243\n259\n246\n253\n225\n229\n250\n237\n257\n261\n248\n219\n244\n\n288\n280\n283\n307\n286\n298\n267\n272\n285\n286\n303\n295\n289\n258\n286\n\n324\n316\n317\n336\n321\n334\n302\n302\n323\n331\n345\n333\n316\n291\n324\n\n9\n\n14\n27\n\n15\n\n19\n11\n6\n26\n24\n16\n3\n28\n21\n8\n18\n4\n20\n13\n30\n1\n\n23\n\n2\n7\n17\n22\n12\n\n5\n29\n10\n\n25\n\nfigure 9.9 rat growth\ndata. left: weekly weights\nof 30 young rats. right:\nshrinkage of individual\nslope estimates towards\noverall slope estimate; the\nsolid line has unit slope,\nand the estimates from the\nmixed model lie slightly\ncloser to zero than the\nindividual estimates.\n\n0\n3\n\n0\n2\n\n0\n1\n\n0\n\n0\n1\n-\n\n0\n2\n-\n\n0\n3\n-\n\n1\n\n2\n\n3\n\n4\n\n5\n\n-30 -20 -10\n\n0\n\n10\n\n20\n\n30\n\nweek\n\nseparate intercept estimate\n\nwe treat the rats as a sample from a population of similar creatures, with different\ninitial weights and growing at different rates. to model this we express the data from\nthe jth rat as\n\ny jt = \u03b20 + b j0 + (\u03b21 + b j1)x jt + \u03b5 jt ,\n\nt = 1, . . .5,\n\n\uf8f6\n\uf8f7\uf8f8 =\n\nwhere the random variables (b j0, b j1) have ajoint normal distribution with mean\n\uf8eb\nvector zero and unknown variance matrix. in matrix terms we have\n\uf8ec\uf8ed y j1\n...\ny j5\n\n\uf8eb\n\uf8ec\uf8ed \u03b5 j1\n...\n\u03b5 j5\n\nj = 1, . . . ,n ,\n\n\uf8eb\n\uf8ec\uf8ed 1\n...\n1\n\n\uf8eb\n\uf8ec\uf8ed 1\n...\n1\n\n\uf8f6\n\uf8f7\uf8f8 ,\n\nx j1\n...\nx j5\n\nx j1\n...\nx j5\n\n\uf8f6\n\uf8f7\uf8f8\n\n\uf8f6\n\uf8f7\uf8f8\n\nb j0\nb j1\n\n\u03b20\n\u03b21\n\n(cid:15)\n\n(cid:14)\n\n(cid:15)\n\n(cid:14)\n\n+\n\n+\n\nand the overall model is obtained by stacking these expressions. below we take\n(x j1, . . . , x j5) = (0, . . . ,4), so that the intercept \u03b20 corresponds to the weight in\nweek 1. there are just p = 2 population parameters \u03b20 and \u03b21, but q = 60 because\n\n "}, {"Page_number": 473, "text": "9.4 \u00b7 components of variance\n\n461\n\ntable 9.28 results from\nfit of mixed model to rat\ngrowth data, using reml.\nvalues in parentheses are\nfor maximum likelihood\nfit. in each case\n\n(cid:5)\u03c3 2 = 5.822.\n\nfixed\n\nrandom\n\nparameter\n\nestimate\n\nstandard error\n\nvariance\n\ncorrelation\n\nintercept\n\nslope\n\n156.05\n43.27\n\n2.16 (2.13)\n0.73 (0.72)\n\n10.932 (10.712)\n3.532 (3.462)\n\n0.18 (0.19)\n\nthere are two random variables per rat. we assume that the within-rat errors \u03b5 jt are\nindependent normal variables with variances \u03c3 2, independent of the b\u2019s.\n\ntable 9.28 gives estimates from reml and maximum likelihood fits of this model.\nas expected, the maximum likelihood estimates for variances are smaller than the\nreml estimates, but here p is small and the difference is minimal. the estimated\nmean weight in week 1 is 156, but the variability from rat to rat has estimated standard\ndeviation of about 11 about this. the slopes show similarly large variation. correlation\nbetween the slope and intercept variables is small, however. the measurement error\n\nvariance(cid:5)\u03c3 2 = 5.822 is smaller than is the inter-rat variation in intercepts, but exceeds\n\nthat for slopes.\n\ndepends on the intercepts it is not uniform.\n\nthe right panel of figure 9.9 shows how the slope estimates from fitting separate\nmodels to each rat are shrunk towards the overall value. the amount of shrinkage is\n\nsmall, owing to the relatively large variation among the rats relative to (cid:5)\u03c3 2, and as it\nprobability plots show that the residuals and random effects \u02dcb are reasonably\nclose to normal, but a plot of residuals against week suggests adding quadratic terms\n(\u03b22 + b j2)x 2\njt . their inclusion reduces aic for reml from 1096.58 to 1013.36, a\nlarge improvement, but the resulting model involves predicting 90 b\u2019s from 150 obser-\nvations, leaving only about two observations per rat for model checking. fortunately\n(cid:1)\ncubic terms do not seem to be necessary as well.\n\nsometimes it is helpful to separate b into sub-vectors corresponding to different\nlevels of variation. for example, educational studies may involve classes of students\nin different schools belonging to different educational authorities, so that comparisons\nof outcomes must take into account different levels of random effects as well as fixed\neffects corresponding to types of school, socio-economic background of students,\nand so forth. for data with l levels of variation it may be useful to write this as a\nmulti-level model\n\ny = x\u03b2 + z lbl + \u00b7\u00b7\u00b7 + z0b0,\n\n(9.16)\n(cid:1)\nwhere the ql \u00d7 1 vectors bl are all mutually independent with means zero and variance\nl\nl=0 zl \u0001l z t\nmatrices \u0001l. then the marginal mean of y is x\u03b2, while its variance is\nl .\nfor consistency we set z0 = in and let b0 contain the errors, so b0 = \u03b5 and \u00010 = \u03c3 2 in.\nthe examples above have l = 1, so in addition to measurement error there is just\none other level of variation, corresponding to individuals. more generally l > 1, and\nvar(y) isformed by adding block diagonal matrices. references to fuller discussions\ncan be found in section 9.5.\n\n "}, {"Page_number": 474, "text": "462\n\n9 \u00b7 designed experiments\n\npipette and counting chamber\n\ndoctor\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\na\nb\nc\nd\ne\n\n427\n434\n480\n451\n462\n\n372\n420\n421\n369\n453\n\n418\n385\n473\n500\n450\n\n440\n472\n496\n464\n520\n\n349\n415\n474\n444\n489\n\n484\n420\n411\n410\n409\n\n430\n415\n472\n422\n508\n\n416\n396\n423\n396\n347\n\n449\n439\n502\n459\n440\n\n464\n424\n488\n471\n391\n\ntable 9.29 the numbers\nof red blood cells counted\nby five doctors using ten\nsets of apparatus.\n\nexercises 9.4\n1\n\n2\n\n3\n\nconsider (9.8).\n(a) show that a confidence interval for the mean of the tth group, \u03b1t = \u00b5 + bt , may be\nbased on t = (yt\u00b7 \u2212 \u03b1t )/[ssw /{t r(r \u2212 1)}]1/2, and give its distribution.\n(b) suppose we take a single observation on a randomly selected block. show that its\nvariance is \u03c3 2 + \u03c3 2\nb , and that this is estimated unbiasedly by {t ssb + (t \u2212 1)ssw}/\n{rt (t \u2212 1)}.\n(c) suppose a fixed number n = rt of units is available, and that it is required to minimize\nthe variance of the population mean estimator, y\u00b7\u00b7. show that we should take t as large\nas possible, that is, r = 2.\n(d) suppose there is a cost c0 for measuring the response on each unit, and a cost c1\nfor each group. show that the total cost is rt c0 + t c1, and find r to minimize var(y\u00b7\u00b7)\nsubject to a fixed total cost.\n(cid:1)\ndiscuss how to check the assumptions of the components of variance model (9.8) using\n(i) normal probability plots of the ytr \u2212 yt\u00b7 and of the yt\u00b7 and (ii) chi-squared probability\nr (ytr \u2212 yt.)2. ineach case give the expected slope\nplots of the group sums of squares\nand intercept of the plot.\nshow that if r is small a normal scores plot of the (ytr \u2212 yt\u00b7)/{(r \u2212 1)/r}1/2 is preferable\nto one based on the ytr \u2212 yt\u00b7.\ndiscuss whether (9.8) is appropriate for the data of example 9.14.\ntable 9.29 gives the numbers of red blood cells counted by five doctors using ten sets of\napparatus. suppose that both doctors and sets of apparatus are thought of as randomly\nselected from suitable populations, and that the response for the rth doctor and cth set of\napparatus is\n\nyrc = \u00b5 + dr + ac + \u03b5rc,\n\n(cid:1)\nr,c(y\u00b7c\n\n\u2212 y\u00b7\u00b7)2, and residuals,\n\nwhere dr , ac, and \u03b5rc are independent normal variables with zero means and variances \u03c3 2\nd ,\na , and \u03c3 2.\n\u03c3 2\n(a) show that if the means of the dr , ac, and \u03b5rc were in fact non-zero, they could not be\n(cid:1)\n(cid:1)\ndistinguished from \u00b5. give acareful interpretation of \u00b5.\n(b) by arguing conditionally on the values of the dr and ac, show that the sums of squares\nr,c(yr\u00b7 \u2212 y\u00b7\u00b7)2, columns,\n+\nr,c(yrc \u2212 yr\u00b7 \u2212 y\u00b7c\nfor rows,\ny\u00b7\u00b7)2, are independent. obtain their distributions, and hence give formulae for unbiased\nestimates of the variances.\n(c) the sums of squares for the analysis of variance table are 2969 for pipettes on 9 df,\n2938 for doctors on 4 df, and 1176 for residual on 36 df. obtain estimates of \u03c3 2\nd , \u03c3 2\na ,\nand \u03c3 2.\n(d) now suppose that general practitioners are to perform these measurements on a routine\nbasis, with results referred to a central laboratory. under the assumption that the data are\nnormal, give the standard error for a measurement taken by a particular gp (i) if the\napparatus is reusable, (ii) if a new set of apparatus must be used for each measurement.\nwhat standard error is appropriate if the measurements are rarely made, so that in effect\n\n "}, {"Page_number": 475, "text": "9.5 \u00b7 bibliographic notes\n\n463\n\nboth gp and apparatus are new? what if the average of k measurements is recorded, and\n(i) apparatus is reusable, (ii) apparatus is not reusable?\n\n4 write down the linear mixed models corresponding to (9.8) and (9.11).\n5\n6\n\nuse (3.21) and exercise 8.5.2 to obtain (9.14) and (9.15).\non page 458, let b\n\nbe any predictor of b based on y. show that\n\n\u2020\n\ncov(b, \u02dcb) = var(\u02dcb),\n\ncov(\u02dcb, y) = cov(b, y),\n\n\u2020, b) = cov(b\n\n\u2020, \u02dcb),\n\ncov(b\n\n7\n\n8\n\nand deduce that\n\ncorr(b\n\n\u2020, b).\n\n\u2212 1\n2\n\n\u2020, b)2 = corr(b\n\n\u2020, \u02dcb)2corr(\u02dcb, b)2.\nhence show that \u02dcb is the predictor of b that maximizes corr(b\nconsider applying the em algorithm (section 5.5.2) for estimation in a normal mixed\nmodel. show that if the random effects b are treated as unobserved data, then the complete-\ndata log likelihood is\nlog|\u0001| \u2212 1\n2\nb b | y; \u03b8(cid:10)(cid:3)\n(cid:2)\nand show that the only quantity needed for the m-step for estimation of components of\n\u0001b is e\nin the special case \u0001b = \u03c3 2\nb iq \u2212 (\u03c3 (cid:10)\n/\u03c3 (cid:10)\n\u03c3 (cid:10)2\n\nb iq, \u0001 = \u03c3 2 in, show that e(btb | y; \u03b8(cid:10)\n)2 z t\u03d2(cid:10)\n\n(y \u2212 x\u03b2 \u2212 zb)t\u0001\u22121(y \u2212 x\u03b2 \u2212 zb) \u2212 1\n2\n\n)t\u03d2(cid:10)\nhence write down the form of the em algorithm for this model.\n(searle et al., 1992, section 8.3)\nanother approach to estimation in mixed models starts from noticing that\n\nlog|\u0001b| \u2212 1\n2\n\nbt\u0001\u22121\n(cid:16)\n\n(cid:17) + (\u03c3 (cid:10)\n\n)4(y \u2212 x\u03b2(cid:10)\n\n(y \u2212 x\u03b2(cid:10)\n\nz z t\u03d2(cid:10)\n\nbt\u0001\u22121\n\n) equals\n\nb b,\n\n/\u03c3 (cid:10)\n\ntr\n\n).\n\nz\n\n.\n\nb\n\nb\n\ne{(y \u2212 x\u03b2)(y \u2212 x\u03b2)t} =\u0001 + z \u0001b z t\n\nis linear in the variance parameters. thus given an estimate(cid:5)\u03b2, wecould stack the unique\nelements of (y \u2212 x(cid:5)\u03b2)(y \u2212 x(cid:5)\u03b2)t as a vector, v, say, and estimate the variance param-\n(cid:5)\u03b2 = (x t\u03d2 x)\neters by least squares regression of v on the appropriate design matrix. we then take\n\u22121 x \u03d2 y, where \u03d2 is formed using the variance estimates, and iterate the\n\nprocedure.\ngive the details of this for example 9.16, using as initial value \u03c3 2\nb\nwhat difficulties do you see with this approach in general? say how they might be over-\ncome.\n\n= 0.\n\n9.5 bibliographic notes\n\ndesigned experiments were used in the nineteenth century and earlier, but r. a. fisher\nwas the first to realise the importance of randomization, and his ideas had a strong\nimpact from the 1920s onwards. his 1935 book on design of experiments, re-issued\nas part of fisher (1990), is fundamental reading. important further developments,\nparticularly in agricultural experimentation, were due to f. yates, with yates (1937)\nhighly influential. an excellent recent account is cox and reid (2000), which con-\ntains a full treatment of the topics of this chapter and other topics not mentioned\nhere, with many further references. a more elementary discussion is cobb (1998).\nolder standard texts are cochran and cox (1959) and the excellent non-mathematical\ntreatment of cox (1958).\n\nthis is sometimes called\niterative generalized least\nsquares or igls\nestimation.\n\nfrank yates (1902\u20131994)\nwas born in manchester\nand educated there and in\ncambridge. after working\non a survey in ghana he\nbecame fisher\u2019s assistant\nat rothamsted\nexperimental station,\nwhere he rapidly became\nhead of the statistics\ndepartment. he made\nfundamental contributions\nto the design and analysis\nof experiments and to\nsample surveys. he\nquickly saw the\nimportance of computing:\nin the 1950s he and his\ncolleagues wrote machine\ncode programs for\nanalysis of variance and\nfor survey analysis.\n\n "}, {"Page_number": 476, "text": "464\n\n9 \u00b7 designed experiments\n\nthe study of causality is central to scientific thought, but has been little discussed by\nstatisticians until fairly recently. a valuable account and excellent starting-point for\nfurther reading is chapter 8 of edwards (2000), while holland (1986) is a good review\nmaking links to the philosophical study of causation. cox (1992) and section 8.7 of\ncox and wermuth (1996) give a somewhat different perspective. contrasting views\non the usefulness of counterfactuals are held by dawid (2000), lauritzen (2001), and\npearl (2000).\n\nscheff\u00b4e (1959) is a standard account of the analysis of variance.\nbox et al. (1978) and fleiss (1986) respectively discuss industrial experimentation\nand medical studies. atkinson and donev (1992) give a clear discussion of optimal\nexperimental design; see also silvey (1980) for a more theoretical account.\n\ncomponents of variance models originated in astronomy in the 1860s and have\nbeen rediscovered and renamed many times since, being also known as hierarchical\nor multilevel models. chapter 2 of searle et al. (1992) gives a brief history oriented\ntowards biometry and agriculture, while goldstein (1995) describes their use in the so-\ncial sciences, using slightly different estimation techniques and with a largely disjoint\nset of references! although r. a. fisher had discussed components of variance in the\n1920s and 1930s, important work by henderson (1953) and hartley and rao (1967)\nwas key in a more general reformulation, while patterson and thompson (1971) built\non earlier work to give a general discussion of reml estimation. robinson (1991)\nis a passionate advocate of best linear unbiased prediction, with an interesting and\nwide-ranging discussion; see particularly the contribution by t. p. speed. mcculloch\nand searle (2001) give a recent account of variance components estimation in linear\nand generalized linear models.\n\n9.6 problems\n\n1\n\niid\u223c n (0, \u03c3 2).\n\nc = 1, . . . , c,\n\nr = 1, . . . , r,\n\nexample 9.6 is a two-way layout with replication, inwhich the jth replicate in row r and\ncolumn c is\nyrcj = \u00b5 + \u03b1r + \u03b2c + \u03b3rc + \u03b5rcj ,\nj = 1, . . . ,k .\nthe \u03b1r and \u03b2c represent the main effects of rows and columns; the \u03b3rc are row\u00d7column\ninteractions; and \u03b5rcj\n(a) explain why an external estimate of \u03c3 2 is needed if the \u03b3rc are known not to be constant,\nand k = 1.\n(cid:1)\n(b) a first step in the analysis of such data is to calculate the cell mean and sums of squares\nj (yrcj \u2212 yrc\u00b7)2. show that the distribution of each cell sum of squares is \u03c3 2\u03c7 2\nyrc\u00b7 and\nk\u22121,\nj (yrcj \u2212 yrc\u00b7)2 against\nand explain what you might expect to learn from a plot of log\nlog yrc\u00b7. what does this plot show for the poisons data?\n(c) the analysis of variance for this design is in table 9.30. show that\n(cid:4)\n\n(cid:1)\n\n(cid:19)\n\n= (r \u2212 1)\u03c3 2 + kc\n\n(\u03b1r \u2212 \u03b1\u00b7 + \u03b3\n\nr\u00b7 \u2212 \u03b3 \u00b7\u00b7)2,\n\nr,c, j (y\u00b7c\u00b7 \u2212 y\u00b7\u00b7\u00b7)2}. explain why these depend on the \u03b1r and \u03b2c only\n\nr\n\ne\n\n(cid:18)(cid:4)\nand write down e{(cid:1)\nthrough \u03b1r \u2212 \u03b1\u00b7 and \u03b2c \u2212 \u03b2\u00b7.\n\n(yr\u00b7\u00b7 \u2212 y\u00b7\u00b7\u00b7)2\n\nr,c, j\n\n "}, {"Page_number": 477, "text": "9.6 \u00b7 problems\n\n465\n\ntable 9.30 analysis of\nvariance for two-way\nlayout with replication.\n\nterms\n\ndf\n\nrows\ncolumns\nrows \u00d7 columns\n\nresidual\n\nr \u2212 1\nc \u2212 1\n\n(r \u2212 1)(c \u2212 1)\n\nrc(k \u2212 1)\n\nsum of squares\n(cid:1)\n(cid:1)\nr,c, j (yr\u00b7\u00b7 \u2212 y\u00b7\u00b7\u00b7)2\nr,c, j (y\u00b7c\u00b7 \u2212 y\u00b7\u00b7\u00b7)2\n\n(cid:1)\nr,c, j (yrc\u00b7 \u2212 yr\u00b7\u00b7 \u2212 y\u00b7c\u00b7 + y\u00b7\u00b7\u00b7)2\n\n(cid:1)\nr,c, j (yrcj \u2212 yrc\u00b7)2\n\n2\n\n3\n\n(cid:1)\nfind the distribution of\nr=1(ygr \u2212 yg\u00b7)2.\n\nr\n\nf3,16(0.95) = 3.24.\n\n(d) show that\n\n(cid:18)(cid:4)\n\n(cid:19)\n\n(cid:4)\n\nr,c, j\n\n(\u03b3rc \u2212 \u03b3\n\n(yrc\u00b7 \u2212 yr\u00b7\u00b7 \u2212 y\u00b7c\u00b7 + y\u00b7\u00b7\u00b7)2\n\n= (r \u2212 1)(c \u2212 1)\u03c3 2 + k\ne\nunder what circumstances does this equal (r \u2212 1)(c \u2212 1)\u03c3 2?\nlet ygr , g = 1, . . . , g, r = 1, . . . , r, beindependent normal random variables with means\n\u00b5gr and common variance \u03c3 2.\n(a) assume the one-way analysis of variance model, namely that \u00b5gr = \u00b5g, sothat the\nygr are replicate measurements with the same mean, and find the sufficient statistics for\nthe \u00b5s and \u03c3 2. show that these are equivalent to\n\nr\u00b7 \u2212 \u03b3 \u00b7c\n\n+ \u03b3 \u00b7\u00b7)2.\n\nrc\n\nwhere yg\u00b7 = r\n\n\u22121\n\ny1\u00b7, . . . , yg\u00b7,\n\n(cid:1)\n(cid:4)\nr\nr=1 ygr ; note that\n(ygr \u2212 \u00b5g)2 =\n\n(ygr \u2212 yg\u00b7)2,\n\nr(cid:4)\nr=1\n\nss = g(cid:4)\n(cid:4)\n\ng=1\n\n(ygr \u2212 yg\u00b7)2 + r(yg\u00b7 \u2212 \u00b5g)2.\n\nr\n\nr\n\n(cid:1)\n(b) prove that ss is independent of the group means, and that it is proportional to a\nchi-squared random variable on g(r \u2212 1) degrees of freedom.\n(cid:1)\n(c) let y\u00b7\u00b7 = g\ng yg\u00b7 denote the overall mean. if \u00b51 = \u00b7\u00b7\u00b7 = \u00b5g, show that the distri-\n\u22121\nbution of ssg = r\ng=1(yg\u00b7 \u2212 y\u00b7\u00b7)2 is proportional to a chi-squared distribution on g \u2212 1\ndegrees of freedom. hence find the distribution of g(r \u2212 1)ssg /(g \u2212 1)s2, when the\nmeans are equal.\n(d) samples of the same material are sent to four laboratories for chemical analysis as\npart of a study to determine whether laboratories give the same results. the results for\nlaboratories a\u2013d are:\n\ng\n\na 58.7\nb 62.7\nc 55.9\nd 60.7\n\n58.2\n60.3\n58.1\n62.3\ntest the hypothesis that the means are different and comment.\n(a) for n = 2m + 1 and positive integer m, suppose that y1, . . . , yn follow the normal\nlinear model\n\n61.4\n64.5\n56.1\n60.3\n\n60.9\n63.1\n57.3\n60.9\n\n59.1\n59.2\n55.2\n61.4\n\ny j = \u03b20 + m(cid:4)\n\nk=1\n\n{\u03b2k cos(2\u03c0k j/n) + \u03b3k sin(2\u03c0k j/n)} + \u03b5 j .\n\nshow that the last 2m columns of the design matrix for this model are orthogonal contrasts,\nand find the least squares estimators of the parameters.\nresponding to the grand mean, and m components i j = n((cid:5)\u03b22\nj may be split into a component ny2 cor-\ny2\n(b) show that the overall sum of squares\nj )/2 corresponding to\nvariation with frequency 2\u03c0 j/n, j = 1, . . . ,m . show that the i j are independent, and\n\n+(cid:5)\u03b3 2\n\n(cid:1)\n\nj\n\n "}, {"Page_number": 478, "text": "466\n\n9 \u00b7 designed experiments\n\n2 i j has an exponential distribution with mean \u03c3 2,\n\nthat if there is no cyclical variation, 1\nwhatever the value of n.\n(c) dataframe venice contains the annual maximum tides at venice for the 51 years\n1931\u20131981. it has been suggested that they may vary according to the astronomical tidal\ncycle, which has period 18.62 years, and that they may also be affected by the sunspot\ncycle, whose period is 11 years. to assess this:\n\nattach(venice)\nsplit.screen(c(1,2))\nscreen(1); plot(year,sea,ylab=\"sea level (cm)\")\nn <- 51;\nk1 <- 19; omega1 <- 2*pi*k1/n # roughly 18.62 years\nk2 <- 11; omega2 <- 2*pi*k2/n\nx19 <- cbind(sin(year*omega1),cos(year*omega1))\nx11 <- cbind(sin(year*omega2),cos(year*omega2))\ncrossprod(cbind(rep(1,n), x11,x19)) # matrix x^\\t x\nvenice.lm <- lm(sea~x19+x11)\nanova(venice.lm,test=\"f\")\n\ndo these cycles seem to be present? how would the knowledge that the errors are non-\nnormal affect your conclusion?\nhint: if \u03c9 = 2\u03c0 p/n, where p is an integer in the range 1, . . . ,[n /2], and k1, k2 are integers\nin the range 1, . . . ,n , then\n\nn(cid:4)\nj=1\n\ncos(k1\u03c9j) = n(cid:4)\ncos(k1\u03c9j) cos(k2\u03c9j) = n(cid:4)\n\nj=1\n\nsin(k1\u03c9j) = n(cid:4)\nsin(k1\u03c9j) sin(k2\u03c9j) =\n\nj=1\n\ncos(k1\u03c9j) sin(k2\u03c9j) = 0,\nk1 = k2,\notherwise.\n\nn/2,\n0,\n\n(cid:20)\n\nj=1\n\nn(cid:4)\nj=1\n\n4\n\n5\n\n(a) suppose that times had been obtained only for the odd-numbered setups example 8.4.\nfurther suppose that two people had been involved, and that one had ridden the bike for\nthose setups with the seat in the low position, while the other had done so for those with\nthe seat in the high position. show that in this case the estimated seat and person effects\nare the same, and discuss what this implies about what the seat height should be.\n(b) now suppose that one person had ridden the bike for setups 1, 7, 9, and 15, while\nthe other person had done so for setups 3, 5, 11, and 13. show that provided there are no\nsecond- and higher-order interactions, the seat, tyre, dynamo and person effects could all\nbe estimated from this experiment. what problem would arise if there was known to be\nan interaction of seat and dynamo? explain how to modify the design to overcome this if\nit is known that there are no other second-order interactions.\nthe 3 \u00d7 3 latin square laid out as\n\nc\na b\nb\nc a\nc a b\n\nhas nine observations classified by rows, columns, and the treatments a, b, and c. the\ncorner-point parametrization for the means is\n\u00b5 + \u03b11 + \u03b31\n\u00b5 + \u03b41 + \u03b31 \u00b5 + \u03b11 + \u03b41 + \u03b32\n\u00b5 + \u03b11 + \u03b42\n\u00b5 + \u03b42 + \u03b32\n\n\u00b5 + \u03b12 + \u03b32\n\u00b5 + \u03b12 + \u03b41\n\u00b5 + \u03b12 + \u03b42 + \u03b31\n\n\u00b5\n\n;\n\n\u03b11, \u03b12 are column effects, \u03b41, \u03b42 are row effects and \u03b31, \u03b32 are treatment effects.\nwrite out the corresponding design matrix and verify that the effects of rows, columns,\nand treatments are not orthogonal. check that the matrix is orthogonal when the terms for\nrows, columns and treatments are centred. without doing any calculations, say whether\n\nto verify this, consider\nthe real and imaginary\nparts of the sums\neik1 \u03c9 + \u00b7\u00b7\u00b7 +e ik1 \u03c9n and\nei(k1+k2)\u03c9 + \u00b7\u00b7\u00b7 +\nei(k1+k2)\u03c9n.\n\n "}, {"Page_number": 479, "text": "9.6 \u00b7 problems\n\nthe same is true for the graeco-latin square\n\n467\n\na\u03b1\nb\u03b3\nc\u03b2 a\u03b3\n\nc\u03b3\nb\u03b2\nc\u03b1 a\u03b2\nb\u03b1\n\n6\n\n7\n\nyou may like to know that\n|cik + d1k 1t\nck\u22121(c + kd).\n\n| =\n\nk\n\n\u2020\n\nin which there is the further set of treatments \u03b1, \u03b2, \u03b3 .\n(a) let a be a q \u00d7 q positive definite symmetric matrix, and let b and y be two random\n\u2020 = b\nvariables with a joint distribution. let b\n(y) denote any predictor of b based on y,\nand let \u02dcb = e(b | y), assuming this is finite for all y. bywriting\n\u2020 \u2212 \u02dcb + \u02dcb \u2212 b)t a(b\n\u2020 \u2212 b)} =e yeb|y{(b\ndeduce that this is minimized when b\ne(\u02dcb) = e(b), so in this sense \u02dcb is unbiased.\n(b) now consider the class of linear predictors b\nstants of dimensions q \u00d7 1 and q \u00d7 n. let w = b \u2212 by, and show that\n\n\u2020 = \u02dcb, for any a and any joint distribution. note that\n(y) = a + by, where a and b are con-\n\n\u2020 \u2212 \u02dcb + \u02dcb \u2212 b) | y},\n\n\u2020 \u2212 b)t a(b\n\ne{(b\n\n\u2020\n\ne{(b\n\n\u2020 \u2212 b)t a(b\n\n\u2020 \u2212 b)} = {a \u2212 e(w )}t a{a \u2212 e(w )} +tr{ avar(w )}.\n\n(cid:18)\n\ndeduce that this is minimized by taking a = \u2212b x\u03b2 and b = \u03c3 \u22122\u0001b z t\u03d2. hence show\nthat \u03c3 \u22122\u0001b z t\u03d2(y \u2212 x\u03b2) isthe best linear predictor of b whatever the distributions of b\nand y.\n(a) show that the log likelihood in example 9.17 is\nni (yi\u00b7 \u2212 \u00b5)2\n(cid:18)(\u00b5, \u03c3 2, \u03c8) \u2261 \u2212 1\n1 + ni \u03c8\n2\u03c3 2\ni=1\ni, j (yi j \u2212 yi\u00b7)2.\nwhere \u03c8 = \u03c3 2\n(cid:18)\n(b) show that using reml increases the log likelihood by\nq(cid:4)\ni=1\n\nssw + q(cid:4)\n/\u03c3 2 and ssw = (cid:1)\n\nlog \u03c3 2 \u2212 1\n2\n\nlog(1 + ni \u03c8),\n\nq(cid:4)\ni=1\n\n\u2212 n\n2\n\n(cid:19)\n\n(cid:19)\n\n1\n2\n\n.\n\nb\n\n(c) show that if \u03c8 is known, then\n\nni\n\nlog\n\n1 + ni \u03c8\n\nlog \u03c3 2 \u2212 1\n2\n(cid:1)\ni=1 ni yi\u00b7/(1 + \u03c8ni )\n(cid:1)\ni=1 ni /(1 + \u03c8ni )\n\n(cid:5)\u00b5(\u03c8) =\n\nq\n\nq\n\n,\n\nand deduce that(cid:5)\u00b5 = y\u00b7\u00b7 if the design is balanced, that is, n1 = \u00b7\u00b7\u00b7 = nq. inthis case obtain\n(cid:5)\u03c3 2(\u03c8) and (cid:5)\u03c8 and compare with the results for (9.8). what is the effect of using reml?\n\n "}, {"Page_number": 480, "text": "10\n\nnonlinear regression models\n\n10.1 introduction\n\nthe regression models of chapters 8 and 9 involve a continuous response that depends\nlinearly on the parameters. linear models remain the backbone of most statistical data\nanalysis, but they have their deficiencies. in many applications, response variables are\ndiscrete, or statistical or substantive considerations suggest that covariates will appear\nnonlinearly. models of this sort appeared on a somewhat ad hoc basis in the literature\nup to about 1970, since when there has been an explosion of generalizations to the\nlinear model. two important developments were the use of iterative weighted least\nsquares for fitting, and the systematic use of exponential family response distributions.\nthe iterative weighted least squares algorithm has wide applicability in nonlinear\nmodels and we outline its properties in section 10.2, giving also a discussion of\nlikelihood inference in this context. exponential family response densities play a\ncentral role in generalized linear models, which we describe in section 10.3, turning\nto the important special cases of binomial and poisson responses in sections 10.4 and\n10.5. these models are widely used, but real data often display too much variation\nfor them to be taken at face value. in section 10.6 we outline remedies for this, based\non the discussion of estimating functions in section 7.2.\n\nin each of these generalizations of the linear model our key notion that a few\nparameters summarize the entire model is retained. section 10.7 branches out in\na different direction, taking the viewpoint that the regression curve itself is more\ncentral than the parameters that summarize it. this leads to the idea of semiparametric\nmodelling, particularly useful in exploratory analysis and assessment of model fit.\nfinally section 10.8 outlines how the special features of survival data described in\nsection 5.4 may be dealt with in regression settings. the remainder of this section\nbriefly motivates later developments.\n\nbelow we mostly assume that the responses y1, . . . , yn are independent, and that\nthe density of y j depends on a parameter \u03b7 j , through which systematic variation\nenters. the \u03b7 j depend on parameters \u03b2 and explanatory variables, so we can write\n\u03b7 j = \u03b7 j (\u03b2). in some models there is an incidental parameter \u03c6 which controls the\n\n468\n\n "}, {"Page_number": 481, "text": "table 10.1 calcium\nuptake (nmoles/mg) of\ncells suspended in a\nsolution of radioactive\ncalcium, as a function of\ntime suspended (minutes)\n(rawlings, 1988, p. 403).\n\nfigure 10.1 calcium\nuptake (nmoles/mg) of\ncells suspended in a\nsolution of radioactive\ncalcium, as a function of\ntime suspended (minutes).\n\n10.1 \u00b7 introduction\n\n469\n\ntime (minutes)\n\ncalcium uptake (nmoles/mg)\n\n0.45\n1.30\n2.40\n4.00\n6.10\n8.05\n11.15\n13.15\n15.00\n\n0.34170 \u22120.00438\n0.95384\n1.77967\n1.27497\n1.75136\n2.60958\n3.12273\n3.17881\n3.00782\n3.94321\n3.05959\n3.35583\n4.80735\n4.70274\n5.13825\n3.60407\n4.15029\n\n0.82531\n0.64080\n1.17332\n2.57429\n2.67061\n3.43726\n2.78309\n4.25702\n3.42484\n\n)\ng\nm\n/\ns\ne\no\nm\nn\n(\n \n\nl\n\ne\nk\na\n\nt\n\n \n\np\nu\nm\nu\nc\na\nc\n\nl\n\ni\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\n\u2022\n\u2022\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n0\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\u2022\n\n\u2022\u2022\n\u2022\n\n5\n\n10\n\n15\n\ntime (minutes)\n\nshape or spread of the distribution. usually the aim is inference about the parameters\n\u03b2 or prediction of future responses. for the normal linear model y j is normally\ndistributed with mean \u03b7 j and variance \u03c6 = \u03c3 2 and \u03b7 j = x t\n\u03b2, but much wider classes\nof models can be put into this framework.\n\nj\n\nexample 10.1 (calcium data) table 10.1 contains data on the uptake of calcium\nby cells that had been in \u201chot\u201d calcium suspension. there are three observations at\neach of nine times after the start of the experiment. the data, plotted in figure 10.1,\nshow nonlinear dependence of calcium uptake on time.\n\nlet y denote calcium uptake at time x after the start of the experiment. then a\n\ndifferential equation that might describe how y depends on x is\n\n= (\u03b20 \u2212 y)/\u03b21,\n\ndy\ndx\n\nwith initial condition y = 0 when x = 0 and solution y = \u03b20{1 \u2212 exp(\u2212x/\u03b21)}. al-\nlowing for measurement error, which seems to be similar at all levels of y, wemight\nwrite\n\ny = \u03b20{1 \u2212 exp(\u2212x/\u03b21)} +\u03b5,\n\n "}, {"Page_number": 482, "text": "table 10.2 response of\na rufous-tailed jacamar to\nindividuals of seven\nspecies of palatable\nbutterflies with artifically\ncoloured wing undersides.\n(n=not sampled, s =\nsampled and rejected, e =\neaten)\n\nfigure 10.2 proportion\nof butterflies eaten\n(\u00b12s e) for diffferent\nspecies and wing colour.\n\n470\n\n10 \u00b7 nonlinear regression models\n\naphrissa\nboisduvalli\n\nn/s/e\n\nphoebis\nargante\nn/s/e\n\ndryas\niulia\nn/s/e\n\npierella\n\nluna\nn/s/e\n\nconsul\nfabius\nn/s/e\n\nsiproeta\nstelenes\u2020\nn/s/e\n\nunpainted\nbrown\nyellow\nblue\ngreen\nred\norange\nblack\n\u2020 includes philaethria dido also.\n\n0/0/14\n7/1/2\n7/2/1\n6/0/0\n3/0/1\n4/0/0\n4/2/0\n4/0/0\n\n6/1/0\n2/1/0\n4/0/2\n0/0/0\n1/1/0\n0/0/0\n6/0/0\n0/0/0\n\n1/0/2\n1/0/1\n5/0/1\n0/0/1\n5/0/0\n6/0/0\n4/1/1\n1/0/1\n\n4/1/5\n2/2/4\n2/0/5\n4/0/3\n6/0/2\n4/0/2\n7/0/1\n4/2/2\n\n0/0/0\n0/0/3\n0/0/1\n0/0/1\n0/0/1\n0/0/1\n0/0/2\n7/1/0\n\n0/0/1\n0/0/1\n0/0/3\n0/1/1\n0/0/3\n3/0/1\n1/1/1\n0/1/0\n\nn\ne\n\nt\n\na\ne\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nab pa di pl cf ss\n\nn\ne\n\nt\n\na\ne\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nu br y bl g r oblack\n\nspecies\n\ncolour\n\nwhere \u03b5 is normal with mean zero and variance \u03c3 2. this fits into the general framework\nwith \u03b7(\u03b2) = \u03b20{1 \u2212 exp(\u2212x/\u03b21)}, if y is normal with mean \u03b7 and variance \u03c3 2. (cid:1)\nexample 10.2 (jacamar data) as part of a study of the learning ability of tropical\nbirds, peng chai of the university of texas at austin collected data on the response\nof a rufous-tailed jacamar, galbula ruficauda, to butterflies. he used marker pens\nto paint the underside of the wings of eight species of butterflies, and then released\neach butterfly in the cage where the bird was confined. the bird responded in three\nways: by not attacking the butterfly (n); by attacking the butterfly, then sampling but\nrejecting it (s); or by attacking and eating the butterfly, usually after removing some\nor all of the wings (e).\ntable 10.2 gives the data in the form of an 8 \u00d7 6 layout, but the response is a triplet\nof counts. figure 10.2 shows strong variation in the proportion of the different species\nand colours eaten.\n\nif we take the number of butterflies eaten as the response, we might consider a\nmodel where the probability of the jacamar eating a butterfly would depend on its\nspecies and on its wing colour. however the low numbers in most cells make it\nlikely that a linear model would give negative fitted probabilities, and this is clearly\n\n "}, {"Page_number": 483, "text": "10.2 \u00b7 inference and estimation\n\n471\n\nundesirable. in the better models described in section 10.3 the probabilities have\nlogistic form exp(\u03b7)/{1 + exp(\u03b7)}, where \u03b7(\u03b2) = x t\u03b2, soonce again we have a model\n(cid:1)\nthat is nonlinear in the covariates.\n\none time-honoured way to treat examples like these is to transform the responses\nso that a linear model can be applied. the transformation may be chosen to stabilise\nthe variance of the response, or better, for simplicity of interpretation. although trans-\nformations remain useful for exploratory analysis and for plotting, they have largely\nbeen superseded by the use of likelihood estimation and more realistic modelling. in\nthe next section we outline how this may be applied in nonlinear regression.\n\n10.2 inference and estimation\n10.2.1 likelihood inference\ninference for nonlinear models is usually based on the large-sample likelihood theory\ndescribed in chapter 4. under mild regularity conditions the asymptotic chi-squared\ndistributions of likelihood ratio statistics and the joint normal distribution of the\nmaximum likelihood estimates of \u03b2 and \u03c6 form the basis for tests and confidence\nintervals, though the adequacy of these approximations needs to be considered in\napplications.\n\ncomparisons between nested models are often based on a form of likelihood ratio\nstatistic known as the deviance. suppose that the log likelihood for a model for\nindependent observations y1, . . . , yn is\n\n(cid:6)(\u03b2, \u03c6) = n(cid:1)\n\nj=1\n\nlog f {y j ; \u03b7 j (\u03b2), \u03c6},\n\n(10.1)\n\nand that \u03c6 is known; for now we suppress \u03c6. ifthe maximum likelihood estimate of\n\nthe p \u00d7 1 parameter vector \u03b2 is (cid:2)\u03b2, then the maximum likelihood estimate of \u03b7 j is\n(cid:2)\u03b7 j = \u03b7 j ((cid:2)\u03b2) and the maximized log likelihood is (cid:6)((cid:2)\u03b2). this is obtained by maximizing\n\nover p parameters, which connect the \u03b7 j for the different observations y j , and thereby\nconstrain them: larger p would lead to a larger log likelihood. the saturated model\nwith no constraints on the \u03b7 j gives the largest possible log likelihood. then \u03b2 has\ndimension n and if the map between \u03b2 and \u03b7 is 1\u20131, the maximized log likelihood is\nsimply the sum of the maxima of the individual terms on the right of (10.1). let \u02dc\u03b7 j be\nthe value of \u03b7 j that maximizes log f {y j ; \u03b7 j}. then the scaled deviance is defined as\n\nd = 2\n\n{log f (y j ; \u02dc\u03b7 j ) \u2212 log f (y j ;(cid:2)\u03b7 j )},\n\n(10.2)\n\nwhich is always non-negative. this will be small when the (cid:2)\u03b7 j and \u02dc\u03b7 j are close,\n\nsuggesting that the model fits well. large d suggests poor fit, analogous to the sum\nof squares in a linear model.\n\nsuppose we want to test whether \u03b2q+1, . . . , \u03b2 p take specified values. let a denote\nthe model in which all p components of \u03b2 vary freely and let b denote the model\n\nn(cid:1)\nj=1\n\n "}, {"Page_number": 484, "text": "472\n\n10 \u00b7 nonlinear regression models\nwith q < p parameters nested within a, with maximum likelihood estimates(cid:2)\u03b7 a and\n(cid:2)\u03b7b and deviances da and db. then the likelihood ratio statistic for comparing the\n\nmodels is\n\n(cid:3)\n\nn(cid:1)\nj=1\n\n2\n\n(cid:4)\n\ny j ;(cid:2)\u03b7 a\n\nj\n\n(cid:5) \u2212 log f\n\n(cid:4)\n\ny j ;(cid:2)\u03b7b\n\nj\n\n(cid:5)(cid:6) = db \u2212 da,\n\nlog f\n\n(10.3)\n\nand provided the models are regular, this has an approximate \u03c7 2\np\u2212q distribution when\nmodel b is correct. hence differences between scaled deviances are often used to\ncompare nested models.\n\nexample 10.3 (normal deviance) suppose that the y j are normal with means \u03b7 j\nand known variance \u03c6. then\n\nlog f (y j ; \u03b7 j , \u03c6) = \u2212 1\n2\n\nlog(2\u03c0 \u03c6) \u2212 (y j \u2212 \u03b7 j )2/\u03c6\n\n2 log(2\u03c0 \u03c6). therefore the scaled deviance for a model with fitted means(cid:2)\u03b7 j is\nis maximized with respect\n\u2212 1\n\nto \u03b7 j when \u02dc\u03b7 j = y j , giving log f (y j ; \u02dc\u03b7 j , \u03c6) =\n\nd = \u03c6\u22121\n\n(y j \u2212(cid:2)\u03b7 j )2,\n\nn(cid:1)\nj=1\n\nwhich is just the residual sum of squares for the model, divided by \u03c6. if\u03b7 j = x t\n\u03b2\nis the correct normal linear model, we saw in section 8.3 that the distribution of\nthe residual sum of squares is \u03c6\u03c7 2\nn\u2212 p\ndistribution call the model into question.\n\nn\u2212 p, so values of d extreme relative to the \u03c7 2\n\nj\n\nthe difference between deviances for nested models a and b in which \u03b2 has\n\ndimensions p and q < p,\n\ndb \u2212 da = \u03c6\u22121\n\n(cid:3)(cid:4)\n\nn(cid:1)\nj=1\n\ny j \u2212(cid:2)\u03b7b\n\nj\n\n(cid:5)2 \u2212(cid:4)\n\ny j \u2212(cid:2)\u03b7 a\n\nj\n\n(cid:5)2(cid:6) .\u223c \u03c7 2\n\np\u2212q\n\nwhen model b is correct. results from section 8.5.1 show that this distribution is\n(cid:1)\nexact for linear models.\n\nif \u03c6 is unknown, it is replaced by an estimate. the large-sample properties of\ndeviance differences outlined above still apply, though in small samples it may be\nbetter to replace the approximating \u03c7 2 distribution by an f distribution with numerator\ndegrees of freedom equal to the degrees of freedom for estimation of \u03c6.\n\n10.2.2 iterative weighted least squares\nin order to apply large-sample likelihood approximations we must obtain the max-\n\nimum likelihood estimates(cid:2)\u03b2 and their standard errors. a general procedure for this\n\nmay be obtained by a variant newton\u2013raphson method, an iterative weighted least\nsquares algorithm widely used for nonlinear estimation. we now discuss this and\nsome of its ramifications.\n\n "}, {"Page_number": 485, "text": "\u2202g/\u2202\u03b2 denotes the p \u00d7 1\nvector whose rth element\nis \u2202g/\u2202\u03b2r , \u2202 2g/\u2202\u03b2\u2202\u03b2t\ndenotes the p \u00d7 p matrix\nwhose (r, s) element is\n\u2202 2g/\u2202\u03b2r \u2202\u03b2s , \u2202\u03b7/\u2202\u03b2t\ndenotes the n \u00d7 p matrix\nwith ( j, r) element is\n\u2202\u03b7 j /\u2202\u03b2r , and so forth.\nnote that\n\u2202\u03b7t/\u2202\u03b2 = (\u2202\u03b7/\u2202\u03b2t)t.\n\n10.2 \u00b7 inference and estimation\n\n473\n\n(cid:6)(\u03b2) = n(cid:1)\n\nassuming for now that \u03c6 is fixed, we write the log likelihood for \u03b2 as\n\n(cid:6) j{\u03b7 j (\u03b2), \u03c6},\nwhere (cid:6) j{\u03b7 j (\u03b2), \u03c6} is the contribution made by the jth observation. the maximum\nlikelihood estimates(cid:2)\u03b2 usually satisfy the equations\n\nj=1\n\n\u2202(cid:6)((cid:2)\u03b2)\n\n\u2202\u03b2r\n\n= 0,\n\u2202(cid:6)((cid:2)\u03b2)\n\nr = 1, . . . , p,\n\nu((cid:2)\u03b2) = 0,\n\n= \u2202\u03b7t\n\u2202\u03b2\n\nwhich we put in matrix form to give the likelihood equation\n\n(cid:7)\n\nn(cid:1)\nj=1\n\n(10.4)\nwhere u(\u03b2) is then \u00d7 1 vector whose jth element is \u2202(cid:6)(\u03b2)/\u2202\u03b7 j because \u03b7 j only enters\nto find the maximum likelihood estimate(cid:2)\u03b2 starting from a trial value \u03b2, wemake\nthe log likelihood through the contribution made by the jth observation.\n\n\u2202\u03b2\n\na taylor series expansion in (10.4), to obtain\n\n\u2202\u03b7t(\u03b2)\n\n\u2202\u03b2\n\nu(\u03b2) +\n\n\u2202\u03b7 j (\u03b2)\n\n\u2202 2(cid:6) j (\u03b2)\n\n\u2202\u03b7 j (\u03b2)\n\n\u2202\u03b2\n\n\u2202\u03b72\nj\n\n\u2202\u03b2 t\n\n\u2202 2\u03b7 j (\u03b2)\n\u2202\u03b2\u2202\u03b2 t\n\nu j (\u03b2)\n\n(10.5)\nif we denote the p \u00d7 p matrix in braces on the left by the p \u00d7 p matrix \u2212j (\u03b2),\nassumed invertible, we can rearrange (10.5) to obtain\n\n(cid:2)\u03b2\n\n.= \u03b2 + j (\u03b2)\n\n\u22121\n\n\u2202\u03b7t(\u03b2)\n\n\u2202\u03b2\n\nu(\u03b2).\n\n(10.6)\n\nthis suggests that maximum likelihood estimates may be obtained by starting from\n\na particular \u03b2, using (10.6) to obtain(cid:2)\u03b2, then setting \u03b2 equal to(cid:2)\u03b2, and iterating (10.6)\n\nuntil convergence. this is the newton\u2013raphson algorithm applied to our particular\nsetting. in practice it can be more convenient to replace j (\u03b2) byits expected value\n\n(cid:8)\n\n((cid:2)\u03b2 \u2212 \u03b2)\n\n.= 0.\n\n+ n(cid:1)\n\nj=1\n\ni (\u03b2) = n(cid:1)\n\nj=1\n\n\u2202\u03b7 j (\u03b2)\n\n\u2202\u03b2\n\ne\n\n(cid:10)\n\n(cid:9)\n\u2212 \u2202 2(cid:6) j\n\u2202\u03b72\nj\n\n\u2202\u03b7 j (\u03b2)\n\n\u2202\u03b2 t\n\n;\n\nthe other term vanishes because e{u j (\u03b2)} =0. we write\ni (\u03b2) = x(\u03b2)tw (\u03b2)x(\u03b2),\n\n(10.7)\nwhere x(\u03b2) isthe n \u00d7 p matrix \u2202\u03b7(\u03b2)/\u2202\u03b2 t and w (\u03b2) isthe n \u00d7 n diagonal matrix\nwhose jth diagonal element is e(\u2212\u2202 2(cid:6) j /\u2202\u03b72\nj ).\nif we replace j (\u03b2) by x(\u03b2)tw (\u03b2)x(\u03b2) and reorganize (10.6), we obtain\n\n(cid:2)\u03b2 = (x tw x)\n\n\u22121 x tw (x\u03b2 + w\n\n\u22121u) = (x tw x)\n\n\u22121 x tw z,\n\n(10.8)\n\nstarting from \u03b2, the updated estimate(cid:2)\u03b2 is obtained by weighted linear regression of\nsay, where the dependence of the terms on the right on \u03b2 has been suppressed. that is,\nthe vector z = x(\u03b2)\u03b2 + w (\u03b2)\n\u22121u(\u03b2) onthe columns of x(\u03b2), using weight matrix\nw (\u03b2). the maximum likelihood estimates are obtained by repeating this step until\n\n "}, {"Page_number": 486, "text": "474\n\n10 \u00b7 nonlinear regression models\n\nthe log likelihood, the estimates, or more often both are essentially unchanged. the\nvariable z plays the role of the response or dependent variable in the weighted least\nsquares step and is sometimes called the adjusted dependent variable.\n\nstep (10.6), or from the profile log likelihood (cid:6)p(\u03c6).\n\noften the structure of a model simplifies the estimation of an unknown value of \u03c6.\n\nit may be estimated by a separate step between iterations of(cid:2)\u03b2, byincluding it in the\nin the normal linear model, we write \u03b7 j =\nexample 10.4 (normal linear model)\n\u03b2. ifthe y j are independently normally distributed with means \u03b7 j and variances\nx t\n\u03c6 = \u03c3 2, we have\nj\n\n(cid:11)\n\n(cid:6) j (\u03b7 j , \u03c3 2) \u2261 \u2212 1\n2\n\nso\n\nlog \u03c3 2 + 1\n\n\u03c3 2 (y j \u2212 \u03b7 j )2\n\n(cid:12)\n\n,\n\nu j (\u03b7 j ) = \u2202(cid:6) j\n\u2202\u03b7 j\n\n= 1\n\u03c3 2 (y j \u2212 \u03b7 j ),\n\n\u2202 2(cid:6) j\n\u2202\u03b72\nj\n\n= \u2212 1\n\u03c3 2 ;\n\nthe jth element on the diagonal of w is the constant \u03c3 \u22122. the ( j, r) element of the\nmatrix \u2202\u03b7/\u2202\u03b2 t is \u2202\u03b7 j /\u2202\u03b2r = x jr , so x(\u03b2) issimply the n \u00d7 p design matrix x. we\nsee that z = x(\u03b2)\u03b2 + w\n\u22121(\u03b2)u(\u03b2) = y, because in this situation x(\u03b2)\u03b2 = x\u03b2 and\n\u22121(\u03b2)u(\u03b2) = \u03c3 2(y \u2212 x\u03b2)/\u03c3 2.\nw\nhere iterative weighted least squares converges in a single step.\n(cid:2)\u03c3 2 = ss((cid:2)\u03b2)/n, where ss(\u03b2) isthe sum of squares ( y \u2212 x\u03b2)t(y \u2212 x\u03b2).\nthe maximum likelihood estimate of \u03c3 2 is obtained as in section 8.2.1, and is\n(cid:1)\n\nexample 10.5 (normal nonlinear model) here the mean of the jth observation\nis \u03b7 j = \u03b7 j (\u03b2). the log likelihood contribution (cid:6) j (\u03b7 j ) isthe same as in the previous\nexample, so u and w are the same also. however, the jth row of the matrix x =\ndepends on \u03b2. after some simplification, we see that the new value for (cid:2)\u03b2 given by\n\u2202\u03b7/\u2202\u03b2 t is (\u2202\u03b7 j /\u2202\u03b20, . . . , \u2202\u03b7 j /\u2202\u03b2 p\u22121), and as \u03b7 j is nonlinear as a function of \u03b2, x\n\n(10.8) is\n\n(cid:2)\u03b2\n\n.= (x t x)\n\n\u22121 x t(x\u03b2 + y \u2212 \u03b7),\n\n(10.9)\nwhere x and \u03b7 are evaluated at the current \u03b2. here \u03b7 (cid:3)= x\u03b2 and (10.9) must be iterated.\n(cid:13)\nthe log likelihood is a function of \u03b2 only through the sum of squares, ss(\u03b2) =\nn\nj=1\n\n{y j \u2212 \u03b7 j (\u03b2)}2. the profile log likelihood for \u03c3 2 is\n\n(cid:6)p(\u03c3 2) = max\n\n(cid:6)(\u03b2, \u03c3 2) \u2261 \u2212 1\n2\n\n{n log \u03c3 2 + ss((cid:2)\u03b2)/\u03c3 2},\n\n\u03b2\n\nso the maximum likelihood estimator of \u03c3 2 is (cid:2)\u03c3 2 = ss((cid:2)\u03b2)/n. although s2 =\nss((cid:2)\u03b2)/(n \u2212 p) isnot unbiased when the model is nonlinear, it turns out to have\nsmaller bias than(cid:2)\u03c3 2, and is preferable in applications.\n\nin some cases the error variance depends on covariates, and we write the variance\n= \u03c3 2(x j , \u03b3 ). such models may be fitted by alternating\nof the jth response as \u03c3 2\nj\niterative weighted least squares updates for \u03b2 treating \u03b3 as fixed at a current value\n\n "}, {"Page_number": 487, "text": "10.2 \u00b7 inference and estimation\n\n475\n\nwith those for \u03b3 with \u03b2 fixed, convergence being attained when neither estimates nor\n(cid:1)\nlog likelihood change materially.\n\niterative weighted least squares can be used for maximum likelihood estimation\nin linear models with non-normal errors and extends to situations with dependent\nresponses.\n\n(cid:14)\n\n(cid:15)\n\nexample 10.6 (venice sea level data)\nin section 5.1 the straight-line regression\nequation y j = \u03b30 + \u03b31(x j \u2212 x) + \u03b5 j was fitted to data on annual maximum sea levels\nat venice from 1931\u20131981. fitting was by least squares, as is appropriate for normal\nresponses, but the right-hand panel of figure 5.2 suggests that the errors are non-\nnormal. as the data are annual maxima, it is more appropriate to suppose that y j has\nthe gumbel density\n\nf (y j ; \u03b7 j , \u03c4 ) = \u03c4\u22121 exp\n\n\u2212 y j \u2212 \u03b7 j\n\n\u2212 exp\n\n\u2212 y j \u2212 \u03b7 j\n\n(10.10)\nwhere \u03c4 is a scale parameter and \u03b7 j = \u03b20 + \u03b21(x j \u2212 x); here we have replaced the\n\u03b3 s with \u03b2s for continuity with the general discussion above. use of this density is\njustified by the arguments leading to (6.34).\n\n\u03c4\n\n\u03c4\n\n,\n\n(cid:15)(cid:12)\n\n(cid:14)\n\n(cid:11)\n\nin this case\n\n(cid:6) j (\u03b7 j , \u03c4 ) = \u2212 log \u03c4 \u2212 y j \u2212 \u03b7 j\n\u2212 exp\n(cid:15)(cid:12)\n\n(cid:14)\n\n(cid:11)\n\n\u03c4\n\n\u2212 y j \u2212 \u03b7 j\n(cid:7)\n\n\u03c4\n\nand it is straightforward to establish that\n\n\u2202(cid:6) j (\u03b7 j , \u03c4 )\n\n\u2202\u03b7 j\n\n= \u03c4\u22121\n\n1 \u2212 exp\n\n\u2212 y j \u2212 \u03b7 j\n\n\u03c4\n\n, e\n\n\u2212 \u2202 2(cid:6) j (\u03b7 j , \u03c4 )\n\n\u2202\u03b72\nj\n\n,\n\n(10.11)\n\n(cid:8)\n\n= \u03c4\u22122,\n\nthat \u2202\u03b7/\u2202\u03b2 t = x is the n \u00d7 2 matrix whose jth row is (1, x j \u2212 x), and w = \u03c4\u22122 in.\nhence (10.8) becomes (cid:2)\u03b2\n\u22121(x\u03b2 + \u03c4 2u), where the jth element of u is\n\u03c4\u22121[1 \u2212 exp{\u2212(y j \u2212 \u03b7 j )/\u03c4}].\nhere it is simplest to fix \u03c4 , toobtain (cid:2)\u03b2 by iterating (10.8) for each fixed value of\n\n.= (x t x)\n\n\u03c4 , and then to repeat this over a range of values of \u03c4 , giving the profile log likelihood\n(cid:6)p(\u03c4 ) and hence confidence intervals for \u03c4 . confidence intervals for \u03b20 and \u03b21 are\nobtained from the information matrix.\nwith starting value chosen to be the least squares estimates of \u03b2, and with \u03c4 = 5,\n19 iterations of (10.8) were required to give estimates and a maximized log likelihood\n\u22126 between successive iterations. we then took\n\u03c4 = 5.5, . . . ,40, using (cid:2)\u03b2 from the preceding iteration as starting-value for the next;\nwhose relative change was less than 10\na close-up of (cid:6)p(\u03c4 ); its maximum is at (cid:2)\u03c4 = 14.5, and the 95% confidence interval\n\nin most cases just three iterations were needed. the left panel of figure 10.3 shows\n\nfor \u03c4 is (11.9, 18.1). the maximum likelihood estimates of \u03b20 and \u03b21 are 111.4 and\n0.563, with standard errors 2.14 and 0.137; these compare with standard errors 2.61\nand 0.177 for the least squares estimates. there is some gain in precision in using the\n(cid:1)\nmore appropriate model.\n\nexample 10.7 (abo blood group system)\nin the usual model for the abo blood-\ngroup system (examples 4.38, 5.12), the probabilities for the observed blood groups\n\n "}, {"Page_number": 488, "text": "476\n\nd\no\no\nh\n\ni\nl\n\ne\nk\n\ni\nl\n \n\ng\no\n\nl\n \n\ne\n\nl\ni\nf\n\no\nr\np\n\n0\n\n2\n-\n\n4\n-\n\n6\n-\n\n8\n-\n\n0\n1\n-\n\n10 \u00b7 nonlinear regression models\n\n*\nr\n\n3\n\n \n \n \n \n \n \n\n2\n\n \n \n \n \n \n \n\n1\n\n \n \n \n \n \n \n \n\n0\n\n1\n-\n\n2\n-\n\n3\n-\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022 \u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nfigure 10.3 gumbel\nanalysis of venice data.\nleft panel: profile log\nlikelihood (cid:6)p(\u03c4 ), with\n95% confidence interval.\nright panel: normal\nprobability plot of\nresiduals r\n\n\u2217\nj .\n\n10\n\n15\n\n20\n\n25\n\ntau\n\n-3\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\n3\n\nquantiles of standard normal\n\na\n\n+ 2\u03bba\u03bbo, \u03b7b = \u03bb2\n\na, b, ab and o are \u03b7a = \u03bb2\n+ 2\u03bbb \u03bbo, \u03b7ab = 2\u03bba\u03bbb, and\n\u03b7o = \u03bb2\no, assuming random mating of a population in which \u03bba, \u03bbb, and \u03bbo are\nthe frequencies of alleles a, b, and o; here \u03bba + \u03bbb + \u03bbo = 1. given n inde-\npendent individuals in which the groups appear with frequencies ya, yb, yab, and\nyo = n \u2212 ya \u2212 yb \u2212 yab, the log likelihood is\nya log \u03b7a + yb log \u03b7b + yab log \u03b7ab\n\nb\n\n+ (n \u2212 ya \u2212 yb \u2212 yab) log(1 \u2212 \u03b7a \u2212 \u03b7b \u2212 \u03b7ab).\n\none of the \u03b7s isredundant and we have replaced \u03b7o with 1 \u2212 \u03b7a \u2212 \u03b7b \u2212 \u03b7ab. if we\nset \u03b21 = log \u03bba and \u03b22 = log \u03bbb, we have\n\uf8f6\n\uf8f8 ,\n\n\uf8f6\n\uf8f8\n\n=\n\n\u2202\u03b7\n\n\u2202(cid:6)\n\n\uf8eb\n\uf8ed \u03bba\u03bbo \u2212\u03bba\u03bbb\n\u2212\u03bba\u03bbb\n\u03bba\u03bbb\n\n\u03bbb \u03bbo\n\u03bba\u03bbb\n\n= 2\n\n\u2202\u03b2 t\n\n\u2202\u03b7\n\n\uf8eb\n\uf8ed ya/\u03b7a \u2212 yo /\u03b7o\nyb /\u03b7b \u2212 yo /\u03b7o\nyab /\u03b7ab \u2212 yo /\u03b7o\n\uf8f6\n\uf8f8 .\n\n\u22121/\u03b7o\n\u22121/\u03b7o\n\n1/\u03b7ab \u2212 1/\u03b7o\n\nand\n\n\uf8eb\n\uf8ed 1/\u03b7a \u2212 1/\u03b7o\n\u22121/\u03b7o\n\u22121/\u03b7o\n\nw = n\n\n1/\u03b7b \u2212 1/\u03b7o\n\n\u22121/\u03b7o\n\u22121/\u03b7o\n\nonce again iterative weighted least squares can be used for maximization, although\nthe weight matrix is not diagonal because the log likelihood contribution from yo\n(cid:1)\ndepends on \u03b7a, \u03b7b and \u03b7ab.\n\n10.2.3 model checking\nthe fit of a linear model is checked using residuals and other diagnostics and by\nembedding it into more complex models chosen to capture particular departures of\ninterest. these ideas extend to nonlinear models through the components of the itera-\ntive weighted least squares algorithm. for example, the leverage of the jth case, h j j ,\nis defined as the jth diagonal element of the matrix h = w 1/2 x(x tw x)\n\u22121 x tw 1/2,\n\n "}, {"Page_number": 489, "text": "477\n\n10.2 \u00b7 inference and estimation\nevaluated at(cid:2)\u03b2. asthe weight matrix w generally depends on (cid:2)\u03b2, the leverages de-\npend on the responses as well as the covariates, but otherwise the h j j have the same\n\u22121 x t.\nproperties as in a linear model, where h j j is the jth diagonal element of x(x t x)\nthere are several types of residual for nonlinear models, because no single definition\nplays all the roles of the standardized residual r j defined at (8.26) for the linear model.\ne2\nj , sofor more\nd2\nj , where\nd j is the signed square root of the contribution y j makes to the scaled deviance. this\ngives the deviance residual\n\ngeneral models the analogy with the deviance suggests that we write d = (cid:13)\n\nthe residual sum of squares for a linear model can be written\n\n(cid:13)\n\nd j = sign(\u02dc\u03b7 j \u2212(cid:2)\u03b7 j )[2{(cid:6) j (\u02dc\u03b7 j ; \u03c6) \u2212 (cid:6) j ((cid:2)\u03b7 j ; \u03c6)}]1/2.\n\nanalogy with the linear model suggests that the standardized deviance residuals\nrdj = d j /(1 \u2212 h j j )1/2 will be more homogeneous, and detailed calculations confirm\nthat usually the rdj have roughly unit variances and distributions close to normal,\nthough possibly with non-zero mean. exceptions to this are binary data and poisson\ndata with small responses, whose residuals are essentially discrete.\n\na better general definition of residual combines the standardized deviance residual\n\nwith the standardized pearson residual\n\nu j ((cid:2)\u03b2)\n\nr p j =\n\n{w j ((cid:2)\u03b2)(1 \u2212 h j j )}1/2\n\n,\n\n\u2217\nj\n\n= rdj + r\n\nwhich is a standardized score statistic. detailed calculations show that the distributions\n\u22121\nof the quantities r\ndj log(r p j /rdj ) are close to normal for a wide range of\nmodels. for a normal linear model, r p j = rdj = r\n\u2217\nj or rdj may be used\nin the plots described in section 8.6.1.\n((cid:2)y \u2212 x(cid:2)\u03b2\u2212 j )t((cid:2)y \u2212 x(cid:2)\u03b2\u2212 j ), where (cid:2)\u03b2\u2212 j is the estimate when the case is deleted from\n\nin a linear model, the influence of the jth case, (x j , y j ), is proportional to\n\n= r j . the r\n\n\u2217\nj\n\nthe model. for more general models a better measure is\n\n\u22121{(cid:6)((cid:2)\u03b2) \u2212 (cid:6)((cid:2)\u03b2\u2212 j )},\n\n2 p\n\n(10.12)\n\nwhere p is the dimension of \u03b2. calculation of all n of these requires n additional fits,\nand it is more convenient to use the approximate cook statistic\n\nc j =\n\nh j j\n\np(1 \u2212 h j j )\n\nr 2\np j\n\n.\n\n(10.13)\n\nthis is derived by taylor series expansion of (10.12) and reduces to (8.30) in the case\nof the normal linear model.\n\nexample 10.8 (venice sea level data) here the weight matrix w is proportional to\nin and the matrix \u2202\u03b7/\u2202\u03b2 t = x is constant. it follows that leverages h j j are simply the\n\u22121 x t. if we set z j = (y j \u2212(cid:2)\u03b7 j )/(cid:2)\u03c4 , it iseasy to check\nthat the jth deviance residual is sign(y j \u2212(cid:2)\u03b7 j )[2{z j + exp(\u2212z j ) \u2212 1}]1/2. the r\ndiagonal elements of x(x t x)\n\n\u2217\nj for\nthe fitted model are shown in the right panel of figure 10.3. they are close to standard\n(cid:1)\nnormal, and cast no doubt on the adequacy of the model.\n\n "}, {"Page_number": 490, "text": "478\n\n8\n\n6\n\n4\n\n1\na\n\nt\n\ne\nb\n\n-60\n\n-35\n\n-30\n-29\n\n-28\n\n2\n\n-50\n\n-40\n\n-40\n\n-50\n\n-60\n\n-70\n\n3\n\n4\n\n5\n\n6\n\n7\n\nbeta0\n\n\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n2\n\n1\n\n0\n\n1\n-\n\n2\n-\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n0\n\n5\n\n10\n\n15\n\nl\n\ni\n\na\nu\nd\ns\ne\nr\n\nfigure 10.4 fit of a\nnonlinear model to the\ncalcium data. upper left:\ncontours for (cid:6)p(\u03b20, \u03b21).\nupper right: contours for\n(cid:6)p(\u03b20, \u03b31), where\n\u03b31 = 1/\u03b21. lower left:\nstandardized residuals\nplotted against time.\nlower right: plot of cook\nstatistics against\nh/(1 \u2212 h), where h is\nleverage.\n\n10 \u00b7 nonlinear regression models\n\n-40\n\n-35\n\n-50\n\n-40\n-50-50\n-70\n\n-60\n\n-35\n\n-30\n\n-70\n-60\n-50\n-35\n-40\n-50\n-60\n\n3        4\n\n5        6       7\n\nbeta0\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\n\u2022\n\u2022\u2022\n0.05 0.10 0.15 0.20\n\n\u2022\n\u2022\u2022\n\n\u2022\n\n\u2022\u2022\u2022\n\n0.0\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n0\n\n.\n\n4\n0\n\n.\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n0\n2\n\n.\n\n0\n\n0\n1\n0\n\n.\n\n0\n\n.\n\n0\n\n1\na\n\nt\n\ne\nb\n\n/\n\n1\n\nc\ni\nt\ns\ni\nt\n\na\nt\ns\n \nk\no\no\nc\n\ntime\n\nh/(1-h)\n\nthe deviance can be used to check the fit of some types of models, but in moderate\nsamples its distribution can be far from \u03c7 2 and plots of deviances from simulated data\ncan be useful.\n\nexample 10.9 (calcium data) the model for the calcium data of example 10.1\nsets \u03b7 j (\u03b2) = \u03b20{1 \u2212 exp(\u2212x j /\u03b21)}, so x = \u2202\u03b7/\u2202\u03b2 t has jth row\n\n(\u2202\u03b7 j /\u2202\u03b20, \u2202\u03b7 j /\u2202\u03b21) = (cid:4)\n\n1 \u2212 exp(\u2212x j /\u03b21), \u2212x j \u03b20 exp(\u2212x j /\u03b21)/\u03b22\n\n1\n\n(cid:5)\n\n.\n\n.= 5, and that \u03b20/\u03b21\n\nas the initial slope of \u03b7 as a function of x is \u03b20/\u03b21, and as \u03b7 has asymptote\n.= 1. this suggests\n\u03b20 for large x, we expect roughly that \u03b20\ntaking \u03b20 = 5 and \u03b21 = 5 asinitial values for the iterative weighted least squares\nalgorithm, which then converges rapidly to(cid:2)\u03b20 = 4.31 and(cid:2)\u03b21 = 4.80, with standard\nmatrix is 0.237, corresponding to corr((cid:2)\u03b20,(cid:2)\u03b21)\nerrors 0.303 and 0.905. the off-diagonal element of the inverse expected information\n.= 0.45. the residual sum of squares is\nss((cid:2)\u03b2) = 7.465, and the estimate of \u03c3 2 is s2 = 7.465/(27 \u2212 2) = 0.299.\nlarge-sample likelihood theory suggests that(cid:2)\u03b20 and(cid:2)\u03b21 have approximately a bivari-\n\nate normal distribution. this rests on quadratic approximation to the log likelihood,\nwhich seems reasonable from the upper left panel of figure 10.4. the upper right\n\n "}, {"Page_number": 491, "text": "10.2 \u00b7 inference and estimation\npanel contains contours of (cid:6)p(\u03b20, \u03b31) = max\u03c3 2 (cid:6)(\u03b20, \u03b31, \u03c3 2), where \u03b31 = 1/\u03b20, for\nwhich quadratic approximation would be poor.\n\n479\n\nthe standardized pearson residuals in the lower left panel suggests possible poor\nfit at the three longest times, although the normal scores plot is good. the lower right\npanel shows that one of the approximate cook statistics is somewhat large, but due\nto an outlier rather than a high leverage point.\n\nto check model fit more formally, we allow unconnected means at each time, giving\nnine parameters and residual sum of squares 4.797 on 18 degrees of freedom. the\nf statistic that compares this and the fitted nonlinear model is equal to {(7.645 \u2212\n4.797)/7}/(4.797/18) = 1.53, and as the 0.95 quantile of the f7,18 distribution is\n2.58, there is no evidence of poor fit overall. under the unconnected model the sums\nof squares between the three observations at each time have distribution \u03c3 2\u03c7 2\n2 , so\na chi-squared probability plot of the ordered sums of squares should be straight.\nthis plot suggests difficulties at x = 11.15 minutes, as was evident from the original\n(cid:1)\ndata.\n\nexercises 10.2\n1\n\nshow that the scaled deviance contribution for a binomial response with probability density\nr )\u03c0 r (1 \u2212 \u03c0)m\u2212r , 0 < \u03c0 <1, r = 0, . . . ,m , and \u03b7 = \u03c0 is\n( m\n\n2{r log{r/(m(cid:2)\u03c0)} +(m \u2212 r) log[(m \u2212 r)/{m(1 \u2212(cid:2)\u03c0)}]}.\n\n\u2212\u03b7/y!,\n\u03b7 > 0, y = 0, 1, . . ., is 2{y log(y/(cid:2)\u03b7) \u2212 y +(cid:2)\u03b7}.\nshow that the scaled deviance contribution for a poisson response with density \u03b7ye\nconsider a linear model with non-normal errors, in which the jth response is y j = \u03b7 j +\n\u03c4 \u03b5 j , where \u03b7 j = x t\n(a) show that the log likelihood contribution from y j may be written as u(z j ) \u2212 log \u03c4 ,\nand hence express in terms of u(\u00b7) and \u03c4 the quantities needed to obtain the maximum\nlikelihood estimates of \u03b2 by iterative weighted least squares.\nn z = 0. show\n(b) let x t\nj\nthat the expected information matrix may be written\n\nj ), so that the covariate matrix equals x = (1n, z) with 1t\n\n\u03b2, and \u03b5 j has density exp u(\u03b5), for j = 1, . . . ,n .\n\n= (1, zt\n\nj\n\n(cid:20)\n\n\u03c4 \u22122\n\nz t z\n\n0\n\n0\nn a\n\n(cid:21)\n\n,\n\nwhere a is a 2 \u00d7 2 matrix that does not depend on the parameters. show further that a is\ndiagonal if the density of \u03b5 j is symmetric about zero.\n(c) give the matrix a for the t density (3.11) and for the gumbel density (10.10).\nsuppose that n years of daily data are available, for each of which the r largest observations\nare known. find the score and observed information when the model (6.37) is fitted to\nthese, with \u03b7 = x t\n\u03b2 but constant \u03c4 and \u03be. hence write down the steps needed to apply\nweighted least squares to estimation of \u03b2, when \u03c4 and \u03be are known. how would you\nshow that for a normal linear model in which \u03c6 is replaced by(cid:2)\u03c6 = ss((cid:2)\u03b2)/(n \u2212 p), the\nestimate \u03c4 and \u03be?\n\nj\n\nstandardized deviance and pearson residuals both equal the usual standardized residual\nr j , and hence verify that (10.13) reduces to (8.30).\nverify the contents of the matrices x and w in example 10.7.\nin anonlinear normal regression model, suppose that \u03b7 = \u03b20(1 \u2212 e\nthe residual sum of squares when \u03b21 is known, that is, when the single covariate 1 \u2212 e\n\n\u2212\u03b21 x ). let ss(\u03b21) be\n\u2212\u03b21 x\n\n2\n\n3\n\n4\n\n5\n\n6\n7\n\n "}, {"Page_number": 492, "text": "480\n\n10 \u00b7 nonlinear regression models\n\nis fitted. show that the profile log likelihood for \u03b21 can be written as\n\n(cid:6)p(\u03b21) = max\n\n\u03b20,\u03c3 2\n\n(cid:6)(\u03b20, \u03b21, \u03c3 2) \u2261 \u2212 n\n2\n\nlog s2(\u03b21),\n\nand give the form of a (1 \u2212 2\u03b1) confidence interval for \u03b21 based on (cid:6)p.\n\n10.3 generalized linear models\n10.3.1 density and link functions\nlinear models play a central role in regression. in their simplest form they apply when\nthe response variable is continuous, takes values on the real line, and has constant\nvariance. transformations and weighting broaden their applicability but can give fits\nthat are awkward to interpret, as well as being unsatisfactory with discrete responses.\nin this section we describe how the key features of the linear model may be extended\nto situations where the response comes from any of a wide class of distributions.\nsuch models are widely used in practice and the main ideas form the basis for much\nfurther development, in which the iterative weighted least squares algorithm plays an\nimportant role.\n\nthree aspects of the normal linear model for a continuous response y are:\nr the linear predictor \u03b7 = x t\u03b2 through which \u00b5 = e(y) depends on the p \u00d7 1\n\nvectors x of explanatory variables and \u03b2 of parameters;\n\nr the density of the response y, which is normal with mean \u00b5 and variance \u03c3 2;\nr the fact that the mean response equals the linear predictor, \u00b5 = \u03b7.\n\nand\n\nin a generalized linear model the second and third of these are extended to:\n\nr the response y has density\n\nf (y; \u03b8, \u03c6) = exp\n\n(cid:11)\n\ny\u03b8 \u2212 b(\u03b8)\n\n\u03c6\n\n+ c(y; \u03c6)\n\n(cid:12)\n\n,\n\n(10.14)\n\nwhere \u03b8 depends on the linear predictor, and the dispersion parameter \u03c6 is\noften known; and\n\nr the linear predictor and the mean of y are related by a monotone link function\n\ng, with\n\n\u03b7 = g(\u00b5).\n\n(10.15)\n\nif \u03c6 is known then (10.14) is a linear exponential family with natural parameter\n\u03b8/\u03c6. thus it contains old friends such as the normal, gamma, binomial, and poisson\ndensities, but it includes also more casual acquaintances such as the inverse gaussian\nand negative binomial models. this broadens the applicability of linear model ideas\nto data where the responses are positive, counts or proportions, without the need for\ntransformations.\n\n "}, {"Page_number": 493, "text": "(cid:5)\n\ndenotes differentiation\nwith respect to \u03b8.\n\n10.3 \u00b7 generalized linear models\n\n481\n\nif y has density (10.14), its moment-generating function is (exercise 10.3.3)\n\nm(t) = exp{b(\u03b8 + t \u03c6) \u2212 b(\u03b8)},\n\n(cid:5)\n\n(\u03b8) = \u00b5,\n\ne(y ) = b\n\nso the response has mean and variance\nvar(y ) = \u03c6b\nsay, provided that a function inverse to b\nv (\u00b5) insection 5.2.1.\n\n(cid:5)(cid:5)\n\n(cid:5)(cid:5){b\n\n(cid:5)\u22121(\u00b5)} =\u03c6 v (\u00b5),\n\n(\u03b8) = \u03c6b\n(cid:5)\n(\u03b8) exists. we met the variance function\n\n(10.16)\n\nexample 10.10 (poisson density) the poisson density may be written as\ny = 0, 1, . . . , \u00b5 > 0,\n\nf (y; \u00b5) = exp (y log \u00b5 \u2212 \u00b5 \u2212 log y!) ,\n\n(cid:5)\n\nwhich has form (10.14) with \u03b8 = log \u00b5, b(\u03b8) = e\u03b8 , \u03c6 = 1, and c(y; \u03c6) = \u2212 log y!.\nthe mean of y is \u00b5 = b\n(\u03b8) = e\u03b8 = \u00b5, sothe\nvariance function is linear: v (\u00b5) = \u00b5; see example 5.9.\n(cid:1)\nexample 10.11 (normal density) the normal density with mean \u00b5 and variance\n\u03c3 2 may be written\n\n(\u03b8) = e\u03b8 = \u00b5, and its variance is b\n\n(cid:5)(cid:5)\n\n(cid:11)\n\n(cid:12)\n\nf (y; \u00b5, \u03c3 2) = exp\n\n\u2212 (y2 \u2212 2y\u00b5 + \u00b52)\n\n2\u03c3 2\n\n\u2212 1\n2\n\nlog(2\u03c0 \u03c3 2)\n\n,\n\nso\n\n\u03b8 = \u00b5,\n\n\u03c6 = \u03c3 2,\n\nb(\u03b8) = 1\n2\n\n\u03b8 2,\n\nc(y; \u03c6) = \u2212 1\n2\u03c6\n\ny2 \u2212 1\n2\n\nlog(2\u03c0 \u03c6).\n\nas the first and second derivatives of b(\u03b8) are \u03b8 and 1, we have v (\u00b5) = 1; the variance\n(cid:1)\nfunction is constant.\n\n(cid:14)\n\n(cid:15)\n\nexample 10.12 (binomial density) we write the binomial density\n\nf (r; \u03c0) =\n(cid:22)\n\nin the form\n\nexp\n\nm\n\nm\nr\n\n(cid:11)\n\n\u03c0 r (1 \u2212 \u03c0)m\u2212r ,\n(cid:15)\n\n(cid:14)\n\n0 < \u03c0 <1,\n\n(cid:12)\n\n+ log(1 \u2212 \u03c0)\n\n+ log\n\nr = 0, . . . ,m ,\n(cid:14)\n\n(cid:15)(cid:23)\n\nm\nr\n\n,\n\nso\ny = r\nm\n\n, \u03c6 = 1\nm\n\n, \u03b8 = log\n\nr\nm\n\nlog\n\n(cid:14)\n\n\u03c0\n\n1 \u2212 \u03c0\n(cid:15)\n\n\u03c0\n\n1 \u2212 \u03c0\n\n, b(\u03b8) = log(1 + e\u03b8 ), c(y; \u03c6) = log\n\nthe mean and variance of y are\n\n\u00b5 = b\n\n(cid:5)\n\n(\u03b8) = e\u03b8\n1 + e\u03b8\n\n(cid:5)(cid:5)\n\n(\u03b8) =\n\n\u03c6b\n\n,\n\nthe variance function is v (\u00b5) = \u00b5(1 \u2212 \u00b5).\n\ne\u03b8\n\nm(1 + e\u03b8 )2 ;\n\n(cid:14)\n\n(cid:15)\n\n.\n\nm\nr\n\n(cid:1)\n\nby allowing nonlinear relations between the mean response and the covariates, the\nlink function (10.15) permits the relationship between the linear part of the model and\n\n "}, {"Page_number": 494, "text": "482\n\n10 \u00b7 nonlinear regression models\n\nthe mean response to be chosen on subject-matter or statistical grounds. restrictions\non \u00b5 can be imposed through the choice of g, which maps the domain of \u00b5 to the\nset inhabited by \u03b7, usually the real line. one particular choice is the canonical link,\nwhich is obtained when \u03b7 = \u03b8 = b\n(cid:5)\u22121(\u00b5). when \u03c6 is known and the canonical link is\nused, the model is a natural exponential family and there is a p-dimensional minimal\nsufficient statistic for \u03b2. this is attractive from the vantage of statistical theory, but\nsubstantive considerations are more important.\n\nexample 10.13 (poisson link functions) the poisson mean is positive, so the most\ncommon link function is the log, for which log \u00b5 = \u03b7. as\u03b8 = log \u00b5, this is the\ncanonical link.\n\nsuppose that data n1 and n2 are gathered from two independent poisson processes,\nwith means t1\u03b31 and t2\u03b32, where t1 and t2 are known, but that the aggregated count\ny = n1 + n2 only is known. then e(y) = t1\u03b31 + t2\u03b32 and the identity link function\n(cid:1)\nwould be appropriate.\n\nexample 10.14 (normal link functions) the usual link function for the normal\ndensity is the identity, yielding the normal linear model. suppose instead that y =\n\u00b5 + \u03b5, but\n\n\u00b5 = \u03b10\n\nx\n\n\u03b11 + x\n\n.\n\non rewriting \u00b5\u22121 as \u03b7 = \u03b20 + \u03b21z, where \u03b20 = \u03b1\u22121\n\u22121, we\nsee that this model fits into our general setup with normal distribution for the response\nand the inverse link function, \u03b7 = \u00b5\u22121.\n(cid:1)\n\n0 , \u03b21 = \u03b11/\u03b10, and z = x\n\n(cid:11)\n\n10.3.2 estimation and inference\nthe log likelihood of independent responses y1, . . . , yn from density (10.14) is\n\n(cid:6)(\u03b2) = n(cid:1)\nj=1\nwhere \u03b8 j = \u03b8(\u03b7 j ) with \u03b7 j = x t\nsolving the score equation (10.4) by iterative weighted least squares. some differen-\ntiation shows that\n\n\u03b2. maximum likelihood estimates(cid:2)\u03b2 are obtained by\n\ny j \u03b8 j \u2212 b(\u03b8 j )\n\n+ c(y j ; \u03c6 j )\n\n(10.17)\n\n(cid:12)\n\n\u03c6 j\n\n,\n\nj\n\n\u2202(cid:6)(\u03b2)\n\n= \u2202\u03b7t\n\u2202\u03b2\n\n\u2202\u03b8\n\n\u2202(cid:6)\n\n= x tu(\u03b2),\n\n\u2202\u03b2\n\n\u2202\u03b8 t\n\n\u2202\u03b7t\n\n(10.18)\nwhere the design matrix \u2202\u03b7/\u2202\u03b2 t = x does not depend on \u03b2. the components of\nthe score statistic u(\u03b2) and the weight matrix w (\u03b2) may be expressed in terms of\ncomponents \u00b5 j of the mean vector \u00b5 as\n=\n(cid:15)2 \u2202 2(cid:6) j (\u03b8 j )\n\ng(cid:5)(\u00b5 j )\u03c6 j v (\u00b5 j )\n1\n\nu j = \u2202\u03b8 j\n(cid:14)\n\u2202\u03b7 j\n\ny j \u2212 \u00b5 j\n\n\u2202(cid:6) j (\u03b8 j )\n\n\u2202\u03b8 j\n\n,\n\nw j =\n\n\u2202\u03b8 j\n\u2202\u03b7 j\n\n=\n\n\u2202\u03b8 2\nj\n\ng(cid:5)(\u00b5 j )2\u03c6 j v (\u00b5 j )\n\n,\n\n(10.19)\n\n "}, {"Page_number": 495, "text": "10.3 \u00b7 generalized linear models\n\n483\n\n(cid:5)\n\n(cid:5)\n\n(\u00b5 j ) = dg(\u00b5 j )/d\u00b5 j . thus(cid:2)\u03b2 is obtained by iterative weighted least squares\nwhere g\nregression of response z = x\u03b2 + g\n(\u00b5)(y \u2212 \u00b5) onthe columns of x using weights\n(10.19). by using y as an initial value for \u00b5 and g(y) as aninitial value for \u03b7 = x\u03b2,\nwe avoid needing an initial value for \u03b2.\nmost generalized linear models in which the \u03c6 j are unknown have \u03c6 j = \u03c6a j ,\nwhere the a j are known and only \u03c6 must be estimated; this is analogous to weighted\nleast squares, with a\nplaying the role of the weight attached to y j . when \u03c6 is\nunknown, the scaled deviance is replaced by the deviance, defined as \u03c6 times the scaled\ndeviance.\n\nunder the usual regularity conditions, (cid:2)\u03b2 has a large-sample normal distribution\n\n\u22121\nj\n\nwith mean \u03b2 and variance matrix x tw x)\n\n\u22121.\n\nthe maximum likelihood estimator of \u03c6 can behave poorly, but another estimator\nis suggested by noting that if the regression parameters \u03b2 were known, an unbiased\nestimator of \u03c6 = var(y j )/{a j v (\u00b5 j )} would be\n\nn(cid:1)\nj=1\n\n1\nn\n\n(y j \u2212 \u00b5 j )2\na j v (\u00b5 j )\n\n.\n\nthis motivates the use of\n\n(cid:2)\u03c6 = 1\nn \u2212 p\n\nn(cid:1)\nj=1\n\n(y j \u2212(cid:2)\u00b5 j )2\na j v ((cid:2)\u00b5 j )\n\n(10.20)\nwhere the divisor n \u2212 p allows for estimation of \u03b2, analogous to the unbiased estimate\nof variance in a normal linear model. when \u03c6 is known, fit can be measured using\npearson\u2019s statistic,\n\n,\n\nn(cid:1)\nj=1\n\n(y j \u2212(cid:2)\u00b5 j )2\nv ((cid:2)\u00b5 j )/a j\n\np = 1\n\n\u03c6\n\n,\n\n(10.21)\n\nanalogous to the scaled residual sum of squares in a normal model.\n\nwhen the dispersion parameter is known, overall tests of fit are provided by\npearson\u2019s statistic and the scaled deviance, but their distributions depend on the\nsituation. for gamma or binomial data with small dispersion, that is small \u03c6, the\ndistribution of d or p when the model fits is roughly \u03c7 2\nn\u2212 p. this corresponds to large\ntions are useful unless all the fitted means are small,(cid:2)\u00b5 <5, say. empirical evidence\n\u03bd for gamma data and to large m for binomial data. for poisson data \u03c7 2 approxima-\n\nsuggests that although such approximations are better for p than for d, they are poor\nif the data are sparse. the problem is most acute for the deviance of binary data, for\nwhich the large-sample approximation is useless (exercise 10.4.1).\n\nexample 10.15 (jacamar data) for the data of example 10.2, we treat the number\nof butterflies of species s painted the cth colour and eaten, rcs, asbinomial with\ndenominator mcs and probability\n\n\u03c0cs = exp(\u03b1c + \u03b3s)\n1 + exp(\u03b1c + \u03b3s)\n\nc = 1, . . . ,8, s = 1, . . . ,6.\n\n,\n\n "}, {"Page_number": 496, "text": "484\n\n10 \u00b7 nonlinear regression models\n\nterms\n\ndf\n\ndeviance\n\n1\n1+species\n1+colour\n1+species+colour\n\n43\n38\n36\n31\n\n134.24\n114.59\n108.46\n67.28\n\ntable 10.3 deviances\nand analysis of deviance\nfor models fitted to\njacamar data. the lower\npart of the analysis of\ndeviance table shows\nresults for the reduced\ndata, without two outliers.\n\nterms\n\nspecies (unadj. for colour)\ncolour (adj. for species)\n\nspecies (unadj. for colour)\ncolour (adj. for species)\n\ndeviance\nreduction\n\n19.64\n47.31\n\n27.63\n18.03\n\ndf\n\n5\n7\n\n4\n7\n\nterms\n\nspecies (adj. for colour)\ncolour (unadj. for species)\n\nspecies (adj. for colour)\ncolour (unadj. for species)\n\ndeviance\nreduction\n\n41.18\n25.78\n\n35.18\n10.48\n\ndf\n\n5\n7\n\n4\n7\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n6\n\n \n \n \n \n \n \n \n\n4\n\n \n \n \n \n \n \n\n2\n\nd\n\nr\n\n0\n\n2\n-\n\n4\n-\n\n6\n-\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n6\n\n \n \n \n \n \n \n \n\n4\n\n \n \n \n \n \n \n\n2\n\nd\n\nr\n\n0\n\n2\n-\n\n4\n-\n\n6\n-\n\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\u2022\n\n\u2022\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\u2022\n\n\u2022\n\u2022\n\n\u2022\n\nab\n\npa\n\ndi\n\npl\n\ncf\n\nss\n\nu br y bl g r o black\n\nspecies\n\ncolour\n\nfigure 10.5\nstandardized deviance\nresiduals rd for binomial\ntwo-way layout fitted to\njacamar data.\n\nfrom example 10.12 we see that this is equivalent to a generalized linear model with\nbinomial errors and response ycs = rcs /mcs whose mean \u00b5cs = \u03c0cs is related to the\nlinear predictor \u03b7cs = \u03b1c + \u03b3s by the logit link function\n\n(cid:14)\n\n(cid:15)\n\n.\n\n\u03b7 = log\n\n\u03c0\n\n1 \u2212 \u03c0\n\ncolour and species effects are represented by the \u03b1c and \u03b3s respectively.\n\ntable 10.3 contains the analysis of deviance. as there are four cells with zero counts\nthere are 44 degrees of freedom in total. the reductions in deviance depend on the\norder of fitting, but are highly significant compared to the appropriate chi-squared\ndistributions. the residual deviance, 67.28, is large compared to the \u03c7 2\n31 distribution\nand suggests poor fit.\n\nfigure 10.5 shows at least two outliers, corresponding to the first cell in the top\nrow and the penultimate cell in the last row of table 10.2. deletion of the second of\n\n "}, {"Page_number": 497, "text": "10.3 \u00b7 generalized linear models\n\n485\n\ntable 10.4 estimated\nparameters and standard\nerrors for the reduced\njacamar data.\n\naphrissa\nboisduvalli\n\nphoebis\nargante\n\ndryas\niulia\n\npierella\n\nluna\n\nconsul\nfabius\n\nsiproeta\nstelenes\n\n\u22121.99 (0.79) \u22122.22 (0.85) \u22120.56 (0.67)\n\n0.16 (0.54)\n\n\u2212\n\n1.50 (0.78)\n\ntable 10.5 times in\nminutes taken by four\nchimpanzees to learn ten\nwords (brown and\nhollander, 1977, p. 257).\n\nincidentally when only\nthe last cell in this column\nwas dropped, neither of\nthe two packages used\n\nsignalled a failure of(cid:2)\u03b3s to\n\nconverge.\n\nbrown\n\nyellow\n\nblue\n\ngreen\n\nred\n\norange\n\nblack\n\n0.16 (0.73)\n\n0.33 (0.68) \u22120.53 (0.81) \u22120.83 (0.75) \u22121.93 (0.88) \u22121.94 (0.85) \u22121.26 (0.86)\n\nword\n\nchimpanzee\n\n1\n\n1\n2\n3\n4\n\n178\n78\n99\n297\n\n2\n\n60\n14\n18\n20\n\n3\n\n177\n80\n20\n195\n\n4\n\n36\n15\n25\n18\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n225\n10\n15\n24\n\n345\n11510\n54\n420\n\n40\n\n25\n40\n\n2\n\n287\n\n14\n\n12\n\n129\n\n80\n\n10\n15\n\n476\n372\n\n55\n190\n\nthese necessitates dropping its entire column, because all the remaining consul fabius\n\nbutterflies were eaten and the corresponding(cid:2)\u03b3s is infinite. in this context a case is a\n\nsingle butterfly, so deletion of a cell involves deleting a number of cases.\n\nthe lower part of table 10.3 shows the analysis of deviance for the reduced\ndata. the overall deviance drops from 73.68 on 35 degrees of freedom to 28.02\non 24 degrees of freedom. the significance of the colours depends on whether or not\nspecies is fitted first. overall significance for each term is assessed by its significance\nafter adjusting for the other. thus colour is significant at about the 0.01 level, when\ntreated as \u03c7 2\n24 distribution, and\ncasts no doubt on the model.\n\n7 . the residual deviance is not large compared to the \u03c7 2\n\ntable 10.4 shows the estimates and standard errors. the odds that an unpainted\n\u22121.99 = 0.14 and the correponding probability is\naphrissa boisduvalli is eaten are e\n\u22121.99) = 0.12. painting the underside of its wings green multiplies the\n\u22121.99/(1 + e\ne\n\u22120.83 = 0.44. the most marked reductions are when red,\nodds of its being eaten by e\norange, or black paint is used; locally these colours are associated with unpalatable\nbutterflies.\npearson\u2019s statistic is 25.58 on 24 degrees of freedom. the standardized deviance\nresiduals lie between \u22122.03 and 1.96, and these and other case diagnostics show that\n(cid:1)\nthe model fits the reduced data well.\n\nexample 10.16 (chimpanzee learning data) table 10.5 gives times in minutes\ntaken by four chimpanzees to learn each of ten words. the data are a two-way layout,\nbut the responses are positive and vary over two orders of magnitude, suggesting that\na linear model is inappropriate. when a linear model with mean \u03b1c + \u03b3w is fitted,\n\n "}, {"Page_number": 498, "text": "486\n\n10 \u00b7 nonlinear regression models\n\nterm\n\nchimp (unadj. for word)\nword (adj. for chimp)\n\ndf\n\n3\n9\n\ndeviance\nreduction\n\nterm\n\n6.95\n38.46\n\nchimp (adj. for word)\nword (unadj. for chimp)\n\ndeviance\nreduction\n\n6.22\n39.19\n\ndf\n\n3\n9\n\ntable 10.6 analysis of\ndeviance for models fitted\nto chimpanzee data.\n\n(cid:14)\n\n\u03bd\n\n(cid:15)\u03bd\n\nwhere \u03b1c and \u03b3w correspond to chimpanzee and words effect, the f statistic (8.27)\nfor non-additivity strongly indicates a change of scale.\nwe fit a model with gamma errors and the log link function, in which the mean time\ntaken by the cth chimpanzee to learn the wth word is \u00b5cw , where log \u00b5cw = \u03b7cw =\n\u03b1c + \u03b3w . ifthe gamma density with mean \u00b5 and shape parameter \u03bd is written in form\n\nf (y; \u00b5, \u03bd) = 1\n\u0001(\u03bd)\n\ny\u03bd\u22121\n\n(10.22)\nwe see (exercise 10.3.4) that the dispersion parameter \u03c6 = 1/\u03bd, which must be esti-\nmated.\n\n\u03bd, \u00b5 > 0,\n\ny > 0,\n\n\u00b5\n\nexp(\u2212\u03bdy/\u00b5),\n\nthe deviances for models 1, 1+chimp, 1+word, and 1+chimp+word are 60.38,\n53.43, 21.19, and 14.97, with 39, 36, 30, and 27 degrees of freedom. table 10.6\nshows the analysis of deviance. the two-way layout is balanced and the order of\n\nfitting matters less than in example 10.15. use of (10.20) gives (cid:2)\u03c6 = 0.432, and\n(cid:2)\u03bd =(cid:2)\u03c6\u22121 = 2.31. the significance of the deviance reductions for chimps and words\nis gauged by f tests using(cid:2)\u03c6 as the denominator. for example, we test for differences\nbetween chimps by comparing (6.22/3)/0.432 = 4.78 with the f3,27 distribution,\ngiving significance level about 0.01.\n\nan alternative is to fit a normal two-way layout to the log data, but the residuals\nsuggest that the gamma model is preferable. the largest residual is for the first chim-\npanzee and fifth word, which is very large compared to the other times for that word.\nthis is also the most influential value for the gamma model, but we shall not pursue\nthis.\na different approach uses gamma errors and the inverse link \u03b7cw = 1/\u00b5cw , giving\na linear model for the speed with which a word is learnt. the residual deviance for this\noutlined in example 8.24 by adding the constructed variable(cid:2)\u03b72 to the linear predictor,\nmodel is 17.08. to test whether this link is suitable we extend the nonadditivity test\nwhere(cid:2)\u03b7 is the fitted linear predictor for the model with inverse link. the deviance\ndrops by 1.26 on one degree of freedom, and since(cid:2)\u03c6 = 0.47 for the extended model,\nthe test statistic is (1.26/1)/0.47 = 2.68, to be compared to the f1,26 distribution.\n(cid:1)\nthe significance level, 0.11, gives only weak evidence against the inverse link.\n\nexercises 10.3\n1\n\nsuppose that y is the number of events in a poisson process of rate \u03bb observed for a period\nof length t . show that y has a generalized linear model density and give \u03b8, b(\u03b8), \u03c6 and\nc(y; \u03c6).\nuse the identities e(\u2202(cid:6)/\u2202\u03b8) = 0 and var(\u2202(cid:6)/\u2202\u03b8) = e(\u2212\u2202 2(cid:6)/\u2202\u03b8 2) toderive the mean and\nvariance of the density (10.14).\n\n2\n\n "}, {"Page_number": 499, "text": "10.4 \u00b7 proportion data\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n487\ncheck that the moment-generating function that corresponds to (10.14) is exp{b(\u03b8 +\nt \u03c6) \u2212 b(\u03b8)}, give the corresponding cumulant-generating function, and hence verify the\nmean and variance in (10.16).\nshow that the gamma density (10.22) can be put in form (10.14) with canonical parameter\n\u03b8 = \u2212\u00b5\u22121, b(\u03b8) = \u2212 log(\u2212\u03b8) and dispersion parameter \u03c6 = 1/\u03bd. give the canonical link\nfunction, and use b(\u03b8) toshow that (10.22) has mean \u00b5, variance function v (\u00b5) = \u00b52,\nand variance \u00b52/\u03bd.\nverify that the inverse gaussian density\n\nf (y; \u03bb, \u00b5) =\n\n(cid:14)\n\n\u03bb\n\n2\u03c0 y3\n\n(cid:15)1/2\n\n(cid:11)\n\nexp\n\n\u2212 \u03bb(y \u2212 \u00b5)2\n\n2\u00b52 y\n\n(cid:12)\n\n,\n\ny > 0,\n\n\u03bb > 0, \u00b5 > 0,\n\n(cid:12)\n\n(cid:2)\u03b8 j \u2212 b((cid:2)\u03b8 j )\n\n(cid:11)\n\nn(cid:1)\nj=1\n\ncan be written in form (10.14) by giving \u03b8, b(\u03b8), \u03c6, and c(y; \u03c6), and show that its variance\nfunction is v (\u00b5) = \u00b53.\nin (10.17), suppose that \u03c6 j = \u03c6a j , where the a j are known constants, and that \u03c6 is\nfunctionally independent of \u03b2. show that the likelihood equations for \u03b2 are independent\nof \u03c6, and deduce that the profile log likelihood for \u03c6 is\n\n.\n\ny j\n\na j\n\n\u22121\n\n+ c(y j ; \u03c6a j )\n\n(cid:6)p(\u03c6) = \u03c6\u22121\n(cid:13)\n( z j \u2212 log z j \u2212 1), where z j = y j /(cid:2)\u00b5 j and \u03c8(\u03bd) isthe digamma\nhence show that for gamma data the maximum likelihood estimate of \u03bd solves the equation\nlog \u03bd \u2212 \u03c8(\u03bd) = n\nfunction d log \u0001(\u03bd)/d\u03bd.\nsuppose that the canonical link is used with the log likelihood (10.17), so that \u03b8 j = \u03b7 j ,\nand that it is required to check this link. let \u03b8 j = \u03b8(\u03b7 j ), where \u03b8(\u00b7) is apotentially\nnonlinear function. by quadratic taylor series expansion of \u03b8(.) about a suitable \u03b70, show\n.= \u03b7(cid:5) + \u03b4\u03b7(cid:5)2, where \u03b4 is\nthat provided \u03b7 contains an intercept term, we can write \u03b8(\u03b7)\nmodel with the canonical link has given linear predictor(cid:2)\u03b7, aconstructed variable test of\nproportional to b\nis a linear function of \u03b7. hence verify that when the fit of a\nthe canonical link is based on the change in fit when(cid:2)\u03b72 is added to the linear predictor.\ndiscuss example 8.24 in this light.\nshow that in any generalized linear model, the fisher information e(\u2212\u2202 2l/\u2202\u03b2r \u2202\u03c6) for the\northogonal. what is the implication for the maximum likelihood estimates(cid:2)\u03c6 and(cid:2)\u03b2r ?\ndispersion parameter \u03c6 and any regression parameter \u03b2r is zero; \u03c6 and \u03b2 are said to be\n\u2212(cid:13)\n\n\u2202 2 log f (y j ; \u03b2, \u03c6)/\u2202\u03c6\u2202\u03b2 is zero, evaluated at(cid:2)\u03b2.\n\nprove also the stronger result that, whatever the value of \u03c6, the joint observed information\n\n(\u03b70) and \u03b7(cid:5)\n\n(cid:5)(cid:5)\n\n10.4 proportion data\n10.4.1 binary data\na binary response y takes values 1 and 0 with probabilities \u03c0 and 1 \u2212 \u03c0, denoting\na dichotomous outcome such as success/failure, won/lost, or well/ill. such data are\ncommon in applications. the simplest relation between e(y ) = \u03c0 and a linear pre-\ndictor is \u03c0 = x t\u03b2. this is unsuitable for general use because \u03c0 may then lie outside\nthe unit interval. it is usually better to force 0 < \u03c0 <1 bytaking it to be a nonlinear\nmonotone function of x t\u03b2, whose inverse is the link function of the corresponding\ngeneralized linear model.\none way to derive link functions for dichotomous variables is to suppose that y\nis a binary version of an underlying continuous response z. let z = x t\u03b3 + \u03c3 \u03b5, and\n\n "}, {"Page_number": 500, "text": "488\n\n10 \u00b7 nonlinear regression models\n\ndistribution\n\nlink function\n\nlogistic\nnormal\nlog weibull\ngumbel\n\neu /(1 + eu)\n\n\u0001(u)\n\n1 \u2212 exp(\u2212 exp(u)}\nexp{\u2212 exp(\u2212u)}\n\nlogit\nprobit\nlog-log\ncomplementary log-log\n\n\u03b7 = \u0001\u22121(\u03c0)\n\n\u03b7 = log{d\u03c0/(1 \u2212 \u03c0)}\n\u03b7 = \u2212 log{\u2212 log(\u03c0)}\n\u03b7 = log{\u2212 log(1 \u2212 \u03c0)}\n\ntable 10.7 tolerance\ndistributions and\ncorresponding link\nfunctions for binary data.\n\nsuppose that \u03b5 has continuous distribution function f. the mean of z increases with\nx t\u03b3 , and y = 1 if z > 0, with probability\n\n\u03c0 = pr(y = 1) = 1 \u2212 f(\u2212x t\u03b3 /\u03c3 ) = 1 \u2212 f(\u2212x t\u03b2),\n\nsay. the ratio \u03b2 = \u03b3 /\u03c3 is estimable from the binary data, but \u03b3 and \u03c3 are not. if f\nis symmetric about zero, then \u03c0 equals f(x t\u03b2) and the corresponding link function\n(10.15) is \u03b7 = x t\u03b2 = f\n\u22121(\u03c0). some standard choices of the so-called tolerance dis-\ntribution f and corresponding link functions are shown in table 10.7. the logit and\nprobit functions are symmetric and usually hard to distinguish in practice, while the\nlog-log and complementary log-log functions are asymmetric in opposite directions.\nnumerous other links have been proposed, but those in the table usually suffice in\napplications.\n\nj and w = k in, where k = \u2212(cid:24)\n\nmuch information may be lost by splitting and it is generally better to work with\nthe original responses if they are available. otherwise less information is lost by\ntaking several categories. difficulties in the binary case are illustrated in the following\nexample.\nexample 10.17 (dichotomization) suppose independent observations z j =\n\u03b2 + \u03b5 j are dichotomized by setting y j = 1 if z j > 0 and y j = 0 otherwise, and\nx t\nj\nlet f and f denote the distribution and density of the \u03b5 j . ifthe original z j were\navailable, the jth log likelihood contribution would be log f (z j \u2212 x t\n\u03b2) and the ex-\npected information matrix would be (10.7), with x the constant matrix whose jth\navailable the asymptotic covariance matrix of the maximum likelihood estimator(cid:2)\u03b2z\nd2 log f (\u03b5)/d\u03b52 f (\u03b5) d\u03b5. thus if the z j are\nrow is x t\n\u22121(x t x)\nis k\nsuppose now that only the binary variables y1, . . . ,y n are known. as y j has suc-\ncess probability \u03c0 j = 1 \u2212 f(\u2212\u03b7 j ), where \u03b7 j = x t\n\u03b2, its log likelihood contribution\nis (cid:6) j = y j log \u03c0 j + (1 \u2212 y j ) log(1 \u2212 \u03c0 j ), and the fisher information matrix is (10.7)\nwith the same x as before but with w the diagonal matrix whose jth element is\ne(\u2212d2(cid:6) j /d\u03b72\nj ) = f (\u2212\u03b7 j )2/[f(\u2212\u03b7 j ){1 \u2212 f(\u2212\u03b7 j )}]. the asymptotic variance of the\nmaximum likelihood estimator(cid:2)\u03b2y based on y1, . . . ,y n is thus (x tw x)\n\u22121 and (x tw x)\n\nthe efficiency of large-sample inferences based on z and y may be compared\n\u22121 of the corre-\nthrough the asymptotic variance matrices k\nsponding maximum likelihood estimators. rather than attempt a general discussion,\nwe illustrate this numerically. let \u03b7 j = \u03b20 + \u03b21x j , with x j taking n = 21 values\nequally spaced from \u22121 to 1.the left panel of figure 10.6 shows data simulated\n\n\u22121(x t x)\n\n\u22121.\n\n\u22121.\n\nj\n\nj\n\nthe asymptotics here\narise if we imagine m\nreplicate observations at\neach x j , and let m \u2192 \u221e.\n\n "}, {"Page_number": 501, "text": "10.4 \u00b7 proportion data\n\n489\n\nfigure 10.6 efficiency\nloss due to reducing\ncontinuous variables to\nbinary ones. left panel:\nsimulated data. blobs\nabove the dotted line are\ncounted as successes, with\nzeros below it as failures;\nthe solid line is 0.5 + 2x.\nright panel: comparison\nof asymptotic t statistics\nwhen continuous data are\ndichotomized, for normal\nerror distribution, when\n\u03b20 = 0.5, 1, 1.5 (solid,\ndots, dashes).\n\n4\n\n2\n\nz\n\n0\n\n2\n-\n\n4\n-\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022 \u2022\n\no\n\n\u2022\n\no\n\no\n\no\n\no\n\no\n\no\n\noo\n\no\n\no\n\n)\ny\nr\na\nn\nb\n(\n \n\ni\n\nl\n\ne\np\no\ns\n \nd\ne\nz\nd\nr\na\nd\nn\na\ns\n\nt\n\ni\n\n0\n\n.\n\n3\n\n5\n\n.\n\n2\n\n0\n2\n\n.\n\n5\n\n.\n\n1\n\n0\n\n.\n\n1\n\n5\n\n.\n\n0\n\n0\n\n.\n\n0\n\n-1.0\n\n-0.5\n\n0.0\n\nx\n\n0.5\n\n1.0\n\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n12\n\nstandardized slope (continuous)\n\nz\n\ny\n\nagainst \u03b21/v 1/2\n\nfrom this model, with \u03b20 = 0.5, \u03b21 = 2, and standard normal errors. the right panel\nplots \u03b21/v 1/2\nz , where vy and v z are the large-sample variances of the\ny\nmaximum likelihood estimates of \u03b21 based on the y s and the zs, for three values\nof \u03b20. the quantity \u03b21/v 1/2\nis the limiting value of the t statistic for testing whether\n\u03b21 = 0, based on the full data, while \u03b21/v 1/2\nz\nis the corresponding quantity for the\nbinary data. the ratio v z /vy is the asymptotic efficiency for estimating \u03b21 from the\nbinary data, relative to the original data, and in the graph this has largest value of about\n(3/4)2 .= 0.56 when \u03b21 = 0, decreasing to about (2/12)2 .= 0.03. for the data in the\nleft panel, the t-statistic is (cid:2)\u03b21/v 1/2\n= 2.39/0.36 = 6.6, whereas the corresponding\nquantity for binary data is 3.15/1.20 = 2.6, which is much weaker \u2014 though still\nstrong \u2014 evidence of non-zero slope.\n\u03b1 two-sided test of \u03b21 = 0 using the asympototic normal distribution of (cid:2)\u03b21/v 1/2\nan argument analogous to that giving (7.28) shows that the power of a size\nis replaced by \u03b4z = \u03b21/v 1/2\nis \u0001(z\u03b1/2 + \u03b4y ) + \u0001(z\u03b1/2 \u2212 \u03b4y ), where \u03b4y = \u03b21/v 1/2\ny\nin\nthe corresponding power from the full data. use of the binary data can sharply reduce\n.= 1.4, and with \u03b1 = 0.025 the\n.= 2, for example, \u03b21/v 1/2\nthe power. when \u03b21/v 1/2\nz\npower is reduced from 0.52 to 0.29.\ny as \u03b21 \u2192 \u221e, because the\na peculiarity of binary regression is the decrease in \u03b21/v 1/2\ninformation f (\u2212\u03b7)2/[f(\u2212\u03b7){1 \u2212 f(\u2212\u03b7)}] tends to zero so quickly that v 1/2\n\u2192 \u221e\nfaster than \u03b21 \u2192 \u221e. thus the reduced efficiency for estimating \u03b21 becomes extreme\nwhen |\u03b21| is large; to put this another way, as |\u03b21| \u2192 \u221e, the power for testing for zero\nslope based on(cid:2)\u03b21 tends to zero. the explanation for this is that most information in\nbinary data is contributed by those responses whose variances are largest, for which \u03c0\nis not too close to zero or one, but as \u03b21 \u2192 \u00b1\u221e, the variances of all the observations\ntend to zero and \u03b21 cannot be reliably estimated.\n\nz\n\ny\n\ny\n\ny\n\ncomplete separation of successes and failures can occur. to see how, note that the\nestimate of \u03b21 from the binary data in the left panel of figure 10.6 depends crucially\non the value y7 at x7 = \u22120.4. if y7 had equalled zero, then a perfect fit would have\nbeen obtained by setting(cid:2)\u03b21 = +\u221e and choosing(cid:2)\u03b20 so that (cid:2)\u03c0 = 0 for x \u2264 0.1 and\n(cid:2)\u03c0 = 1 otherwise. this is harder to spot when there are several covariates, but a good\n\n "}, {"Page_number": 502, "text": "490\n\n10 \u00b7 nonlinear regression models\nmodel-fitting routine will signal convergence problems as |(cid:2)\u03b2| \u2192 \u221e. near-complete\n\nseparation will be indicated by regression diagnostics, which here suggest that the\n(cid:1)\npair (y7, x7) is anoutlier, as it has a large residual and is highly influential.\n\nlogistic regression\nthe most common choice of function f is the logistic distribution, which gives the\ncanonical, logit, link function. then\npr(y = 1) = \u03c0 = exp(x t\u03b2)\n1 + exp(x t\u03b2)\n\npr(y = 0) = 1 \u2212 \u03c0 =\n\n1 + exp(x t\u03b2)\n\n1\n\n,\n\n,\n\nand the resulting logistic regression model is a linear model for the logarithm of the\nodds of success,\n\n= \u03c0\n1 \u2212 \u03c0\n\n= exp(x t\u03b2).\n\npr(y = 1)\npr(y = 0)\n(cid:8)y j\n\n(cid:7)\n\n(cid:5)\n\nj\n\n\u03b2\n\n\u03b2\n\n\u03b2\n\n\u03b2\n\n\u03b2\n\n(cid:5)\n\n(cid:4)\n\n(cid:5)\n\n(cid:4)\n\nx t\nj\n\nx t\nj\n\n(cid:7)\n\n(cid:5)\n(cid:5)(cid:28) .\n\nvectors x1, . . . , xn is\n\nthe likelihood for independent binary observations y1, . . . , yn with covariate\n\n(cid:4)(cid:13)\n1 + exp\n\n(cid:4)\nexp\nx t\nj\n1 + exp\n\n(cid:8)1\u2212y j = exp\n(cid:27)\n(cid:26)\n\nj=1\n\n(10.23)\n\n1\n1 + exp\n\n(cid:4)\nj y j x t\nj\nx t\nj\n\nfor \u03b2. if any of the covariate vectors are repeated, s may be written as\n\nl(\u03b2) = n(cid:25)\nthis is a linear exponential family model in which s = (cid:13)\nthe distinct covariate vectors are labelled xd and rd = (cid:13)\n\n(cid:13)\ny j x j is minimal sufficient\nd xd rd, where\nj yd j , the total number of\nsuccesses for responses with covariates xd, is abinomial variable. apart from a con-\nstant, (10.23) is the same likelihood as would be obtained from responses aggregated\nby covariate vectors.\nempirical logistic transform log{(r + 1\n2 )}, whose estimated variance\nis (r + 1\n2 )\na given fitted probability (cid:2)\u03c0, aresidual takes just two values, so comparison with a\nis a function of the data through(cid:2)\u03b2 alone, and hence it provides no information about\n\nnormal distribution is not useful. moreover the deviance for a binary logistic model\n\nif r is binomial with denominator m, then the log odds may be estimated by the\n\nmany model-checking procedures break down for unaggregated binary data. for\n\n\u22121, and this is sometimes useful for plotting.\n\n\u22121 + (m \u2212 r + 1\n2 )\n\n2 )/(m \u2212 r + 1\n\nfit in any absolute sense (exercise 10.4.1). pearson\u2019s statistic is strongly correlated\nwith the deviance and shares this difficulty.\n\nexample 10.18 (nodal involvement data) table 10.8 summarizes data on 53 pa-\ntients with prostate cancer. there are five binary explanatory variables: age in years\n(0 = less than 60, 1 = 60 or more); stage, ameasure of the seriousness of the tumour\n(0 = less serious, 1 = more serious); grade, ameasure of the pathology of the tumour\n(0 = less serious, 1 = more serious); xray (0 = less serious, 1 = more serious); and\nacid, the level of serum acid phosphatase (0 = less than 0.6, 1 = 0.6 or more). the\nresponse, nodal involvement, indicates whether the cancer has spread to neighbour-\ning lymph nodes. the first row of the table shows that for five out of six patients\n\n "}, {"Page_number": 503, "text": "10.4 \u00b7 proportion data\n\n491\n\ntable 10.8 data on\nnodal involvement\n(brown, 1980).\n\nm\n\n6\n6\n4\n4\n4\n3\n3\n3\n3\n2\n\n2\n2\n1\n1\n1\n1\n1\n1\n1\n1\n\n1\n1\n1\n\nr\n\n5\n1\n0\n2\n0\n2\n1\n0\n0\n0\n\n1\n1\n1\n1\n1\n1\n0\n1\n0\n1\n\n1\n0\n0\n\nage\n\nstage\n\ngrade\n\nxray\n\nacid\n\n0\n0\n1\n1\n0\n0\n1\n1\n1\n1\n\n0\n0\n1\n1\n1\n1\n1\n0\n0\n0\n\n0\n0\n0\n\n1\n0\n1\n1\n0\n1\n1\n0\n0\n0\n\n1\n0\n1\n1\n0\n0\n0\n1\n1\n1\n\n0\n0\n0\n\n1\n0\n1\n0\n0\n1\n0\n0\n0\n0\n\n0\n1\n1\n0\n1\n0\n1\n1\n1\n0\n\n1\n0\n0\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n0\n0\n1\n1\n1\n1\n0\n1\n0\n1\n\n0\n1\n1\n\n1\n1\n0\n1\n0\n1\n0\n1\n0\n0\n\n1\n0\n1\n1\n1\n1\n0\n0\n0\n0\n\n1\n1\n0\n\naged less than 60 and with high levels of the other explanatory variables, there was\nnodal involvement. a case is an individual patient rather than a row of the table. the\nexplanatory variables are relatively easily collected and the aim of analysis was to\npredict nodal involvement from them.\n\ntable 10.9 contains the deviances for all 25 combinations of explanatory variables\nwhen a binary logistic model is fitted to the data. the model with terms for stage,\nxray, and acid has deviance 19.64 on 49 degrees of freedom and the smallest aic; it\nseems best overall, though it has several close competitors. the fitted linear predictor\nfor this model is \u22123.05 + 1.65istage + 1.91ixray + 1.64iacid, where istage\nindicates that stage takes its higher level, and so forth. the fitted odds of nodal\ninvolvement when all the explanatory variables take their lower levels are a low\n\u22123.05 .= 0.047, though this must be viewed with caution as there are no such cases in\ne\nthe data. the odds increase by a factor e1.91 .= 6.75 when acid takes its higher level,\nand are e\n\n\u22123.05+1.91+1.65+1.64 .= 8.6 atthe higher levels of stage, acid, and xray.\n\nthe residual scaled deviance of 19.64 on 49 degrees of freedom suggests that the\nmodel fits well, but the binomial denominators are too small for confidence in \u03c7 2\nasymptotics. the deviance does not measure model fit for binary data: it is a function\n\nof (cid:2)\u03b2 alone and hence does not contrast the data with the fitted model. if the data\n\nin table 10.8 had been analyzed as written there, that is as 23 binomial rather than\n53 binary observations, the degrees of freedom for the best-fitting model would be\n\n "}, {"Page_number": 504, "text": "492\n\n10 \u00b7 nonlinear regression models\n\nage\n\nstage\n\ngrade\n\nxray\n\nacid\n\ndf\n\ndeviance\n\nage\n\nstage\n\ngrade\n\nxray\n\nacid\n\ndf\n\ndeviance\n\n+\n\n+\n+\n+\n+\n\n+\n\n+\n\n+\n+\n+\n\n52\n51\n51\n51\n51\n51\n50\n50\n50\n50\n50\n50\n50\n50\n50\n50\n\n40.71\n39.32\n33.01\n35.13\n31.39\n33.17\n30.90\n34.54\n30.48\n32.67\n31.00\n24.92\n26.37\n27.91\n26.72\n25.25\n\n+\n+\n+\n+\n+\n+\n\n+\n+\n+\n+\n+\n\n+\n+\n+\n\n+\n+\n+\n+\n+\n+\n+\n+\n\n+\n\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n\n49\n49\n49\n49\n49\n49\n49\n49\n49\n49\n48\n48\n48\n48\n48\n47\n\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n\n29.76\n23.67\n25.54\n27.50\n26.70\n24.92\n23.98\n23.62\n19.64\n21.28\n23.12\n23.38\n19.22\n21.27\n18.22\n18.07\n\n+\n\n+\n\n+\n\n+\n+\n\n+\n\n+\n\n+\n+\n+\n\n+\n\n+\n\n+\n+\n+\n\n23 \u2212 4 = 19 rather than 49, but the deviance of 19.64 would be unchanged. this\nambiguity is another reason not to rely on the deviance to measure fit for binary data.\nfigure 10.7 illustrates difficulties with binary residuals. the left panel shows the\n53 residuals for the unaggregated data. the linear predictors are slightly jittered to\nprevent over-plotting. the upper and lower bands, corresponding to ones and zeros\nrespectively, are typical of data with only a few response values. the right panel\nshows the 23 residuals for the aggregated data; the fitted values are the same as on the\nleft. banding remains but is much less obvious, and the apparent outliers are gone.\nthere is little useful information in either plot.\nwe reconsider these data in example 12.18.\n\n(cid:1)\n\ntable 10.9 scaled\ndeviances for 32 logistic\nregression models for\nnodal involvement data. a\nplus denotes a term\nincluded in the model.\n\n10.4.2 2 \u00d7 2 table\n\na very common data structure classifies individuals by two sets of binary categories.\nin a medical setting, for example, we may observe success or failure for patients\nrandomly allocated to be either a case \u2014 receiving some treatment \u2014 or a control.\nthe resulting data may be laid out as in table 10.10. the simplest model for this\nregards the numbers of successes r1 and r0 as independent binomial variables with\nprobabilities\n\n, \u03c00 = e\u03bb\n1 + e\u03bb\n\n\u03c01 = e\u03bb+\u03c8\n1 + e\u03bb+\u03c8\n(cid:15)(cid:14)\n(cid:14)\n\n(cid:15)\n\nand denominators m1 and m0. then the joint density of r1 and r0 is\n\npr(r1 = r1, r0 = r0; \u03c8, \u03bb) =\n\nm1\nr1\n\nm0\nr0\n\ne(\u03bb+\u03c8)r1\n\n(1 + e\u03bb+\u03c8 )m1\n\ne\u03bbr0\n\n(1 + e\u03bb)m0\n\n,\n\n(10.24)\n\n "}, {"Page_number": 505, "text": "10.4 \u00b7 proportion data\n\n493\n\ntable 10.10 notation\nfor 2 \u00d7 2 table.\n\nfigure 10.7\nstandardized deviance\nresiduals for nodal\ninvolvement data, for\nungrouped responses (left)\nand grouped responses\n(right).\n\nl\n\na\nu\nd\ns\ne\nr\n \n\ni\n\ne\nc\nn\na\nv\ne\nd\n\ni\n\n \n\ni\n\nd\ne\nz\nd\nr\na\nd\nn\na\ns\n\nt\n\nsuccess\n\nr1\nr0\n\nfailure\nm1 \u2212 r1\nm0 \u2212 r0\n\ntotal\n\nm1\nm0\n\nr1 + r0\n\nm1 + m0 \u2212 r1 \u2212 r0\n\nm1 + m0\n\ncase\ncontrol\n\ntotal\n\n\u2022\n\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\n3\n\n2\n\n1\n\n0\n\n1\n-\n\n2\n-\n\n3\n-\n\n\u2022\n\u2022\u2022\n\u2022\u2022\n\n\u2022\n\u2022\n\u2022\u2022\n\n\u2022\n\u2022\u2022\n\u2022\n\n\u2022\n\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\n\u2022\n\nl\n\na\nu\nd\ns\ne\nr\n \n\ni\n\ne\nc\nn\na\nv\ne\nd\n\ni\n\n \n\ni\n\nd\ne\nz\nd\nr\na\nd\nn\na\ns\n\nt\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n3\n\n2\n\n1\n\n0\n\n1\n-\n\n2\n-\n\n3\n-\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\nlinear predictor\n\nlinear predictor\n\nwhich is an exponential family of order two with natural parameter (\u03c8, \u03bb) and natural\nobservation (r1, r1 + r0) (section 5.2.2). this is a generalized linear model with\nbinomial errors and logit link function.\nthe usual purpose of analysis is to compare \u03c01 with \u03c00. although quantities such\nas the difference \u03c01 \u2212 \u03c00 are sometimes of interest, we focus here on the difference\nin log odds,\n\n(cid:14)\n\n(cid:15)\n\n(cid:14)\n\n\u03c8 = log\n\n\u03c01\n1 \u2212 \u03c01\n\n\u2212 log\n\n\u03c00\n1 \u2212 \u03c00\n\n(cid:15)\n\n.\n\nthis is a natural parameter of the exponential family, but more importantly its inter-\npretation does not depend on whether the data are obtained prospectively or retrospec-\ntively. to appreciate this, suppose that a prospective study is performed: an individual\nis allocated randomly to cases (t = 1) or controls (t = 0) and then followed until a\nbinary outcome y is observed. then\n\npr(y = 1 | t = 1) = e\u03bb+\u03c8\n1 + e\u03bb+\u03c8\n\n,\n\npr(y = 1 | t = 0) = e\u03bb\n1 + e\u03bb\n\n(10.25)\n\nand as t is allocated and then y observed, the scheme fixes the vertical margin in\ntable 10.10. the drawback is that it may be costly and difficult to follow up enough\nindividuals to obtain a precise estimate of \u03c8.\n\nin a retrospective study the treatment status t is determined only after the out-\ncomes y are known; the scheme fixes the horizontal margin in table 10.10. often the\ntreatment undergone can be ascertained from medical records, so large samples can\n\n "}, {"Page_number": 506, "text": "494\n\n10 \u00b7 nonlinear regression models\n\nbe assembled more easily and cheaply than a prospective study, though the lack of\nrandomization weakens subsequent inferences. let z = 1 indicate that an individual\nis chosen for the retrospective study, and suppose that this occurs with probabilities\n\npr(z = 1 | y = 1) = p1,\n\npr(z = 1 | y = 0) = p0,\n\nindependent of treatment status t . then the success probability for an individual\nwho was treated, conditional on their being chosen for inclusion in the study is\npr(y = 1 | z = 1, t = 1). this equals\n\npr(z = 1 | y = 1)pr(y = 1 | t = 1)\n\npr(z = 1 | y = 1)pr(y = 1 | t = 1) + pr(z = 1 | y = 0)pr(y = 0 | t = 1)\n\nby bayes\u2019 theorem, so\n\npr(y = 1 | z = 1, t = 1) =\n\np1e\u03bb+\u03c8 + p0\nwhere \u03bb(cid:5) = \u03bb + log( p1/ p0). a similar argument gives\n\np1e\u03bb+\u03c8\n\n= e\u03bb(cid:5)+\u03c8\n1 + e\u03bb(cid:5)+\u03c8\n\n,\n\npr(y = 1 | z = 1, t = 0) = e\u03bb(cid:5)\n\n1 + e\u03bb(cid:5) ,\n\nso although retrospective sampling alters \u03bb, the difference of log odds \u03c8 is unchanged.\nthis gives a strong motivation for using \u03c8 to summarize the treatment effect, partic-\nularly if estimates from both types of study will ultimately be combined.\n\nthis argument applies also if \u03c8 is replaced by x t\u03b2, where x contains covariates as\nwell as an indicator of treatment status. the key point is that the selection probabilities\np1 and p0 must be independent of x.\nexample 10.19 (smoking and the grim reaper) table 6.8 contains seven 2 \u00d7\n2 tables, containing a prospective observational, that is, non-randomized, study on\noutcomes for women smokers and non-smokers. the simplest model ignores age by\nusing only the overall data in the first line of table 6.8, and gives parameter estimates\n\nfor (10.25) of(cid:2)\u03bb = 0.78 (0.08) and (cid:2)\u03c8 = 0.38 (0.13). the significant positive value\nof (cid:2)\u03c8 shows an unlikely preservative effect of smoking. the deviance is 632.3 on\n6 degrees of freedom, and (cid:2)\u03c8 = \u22120.43 (0.18): smoking significantly increases the\n\nwhen different values of \u03bb are fitted to each table, the deviance drops to 2.38 on\n\n12 degrees of freedom, however, so the model is evidently inadequate.\n\ndeath rate. there are 14 residuals, but as they arise in negatively correlated pairs\nand have only 6 degrees of freedom, it is better to examine one residual for each\n2 \u00d7 2 table. they show nothing untoward.\n(cid:1)\n\nsmall sample analysis\nthe discussion above relies on large-sample likelihood results. special techniques are\nneeded for 2 \u00d7 2 tables with small counts. as (10.24) is an exponential family, the\n\n "}, {"Page_number": 507, "text": "10.4 \u00b7 proportion data\n\n495\n\nr+(cid:1)\nu=r\u2212\n\nnuisance parameter \u03bb may be eliminated by conditioning on its associated statistic\na = r1 + r0, whose density is\ne\u03bba\n\n(cid:15)(cid:14)\n\ne\u03c8u\n\n(cid:14)\n\n(cid:15)\n\nm1\nu\n\nm0\na \u2212 u\n\n(1 + e\u03bb)m0\n\n(1 + e\u03bb+\u03c8 )m1\n\n,\n\na = 0, . . . ,m 1 + m0,\n\nwhere r\u2212 = max(0, a \u2212 m0), r+ = min(m1, a). the conditional density of r1 given\na = a is the non-central hypergeometric density\n\nf (r | a; \u03c8) =\n\n(cid:4)\n\n(cid:13)\n\nm1\nr\nr+\nu=r\u2212\n\n(cid:5)(cid:4)\n(cid:5)\n(cid:4)\n(cid:5)(cid:4)\nm0\ne\u03c8r\na\u2212r\nm1\nm0\na\u2212u\nu\n\n(cid:5)\n\nr = r\u2212, . . . , r+,\n\n,\n\ne\u03c8u\n\n(10.26)\n\non which exact inferences for \u03c8 may be based; this amounts to conditioning on both\nmargins of table 10.10. tests of \u03c01 = \u03c00 compare the observed value of r1 with\nits null distribution, obtained by setting \u03c8 = 0 in(10.26). to test \u03c8 = 0 against the\none-sided alternative \u03c8 > 0 weuse the p-value\n\npr(r1 \u2265 r1 | a = a; 0) = r+(cid:1)\n\nf (r | a; 0),\n\n(10.27)\nand take pr(r1 \u2264 r1 | a = a; 0) when testing \u03c8 < 0. exact confidence intervals are\nobtained by inverting these tests, solving for \u03c8\u03b1, \u03c8 \u03b1 the equations\n\nr=r1\n\npr(r1 \u2265 r1 | a = a; \u03c8\u03b1) = \u03b1,\n\npr(r1 \u2264 r1 | a = a; \u03c8 \u03b1) = \u03b1.\n\nwhen the margins of the table are small, the conditional distribution of r1 is very\ndiscrete and the difficulties seen in example 7.38 arise: exact conditional confidence\nintervals are quite conservative and it is preferable to replace (10.27) by the mid- p\nsignificance level\n\n(10.28)\nwhen exact significance levels for testing \u03c8 = 0 are unavailable, approximate ones\n\npr(r1 = r1 | a; 0) + pr(r1 > r1 | a; 0).\n\np+,mid = 1\n2\n\nmay be obtained by treating\n\nz = r1 \u2212 1\n\n2\n\n\u2212 e(r1 | a = a; 0)\n\nvar(r1 | a = a; 0)1/2\n\nas standard normal, where the 1\n2 is a continuity correction, and\n, var(r1 | a = a; 0) = m0m1a(m0 + m1 \u2212 a)\ne(r1 | a = a; 0) = m1a\nm0 + m1\n(m0 + m1)2(m0 + m1 \u2212 1)\nexample 10.20 (ulcer data)\nin a trial to compare two treatments for stomach ulcer,\n28 persons with ulcers were divided randomly into two groups, one of size m1 = 15\nwho were given a new surgical treatment, and the other of size m0 = 13 who were\ngiven an existing one; see table 10.11, which also contains data from other trials. the\nnumbers in these groups without an adverse outcome, recurrent bleeding, were r1 = 8\nand r0 = 2. does the new treatment reduce the number of adverse outcomes? here the\nnull hypothesis is \u03c8 = 0, with alternative \u03c8 > 0. the attainable significance levels\nfor the conditional test are in table 10.12, and p+ = 0.0434 and p+,mid = 0.0243.\nthere is some evidence that the new treatment improves on the old.\n\n.\n\n "}, {"Page_number": 508, "text": "table 10.11 data from\n40 independent\nexperiments to compare a\nnew surgery for stomach\nulcer with an older\nsurgery; data from efron\n(1996) corrected from\noriginal articles. shown\nare the number of persons\ngiven the new treatment,\nm1, of whom r1 did not\nhave recurrent bleeding,\nand the number given the\nold treatment, m0, of\nwhom r0 did not have\nrecurrent bleeding.\n\n(cid:5)\n\ntable 10.12\nsignificance probabilities\nfor a test of no treatment\neffect in the first 2 \u00d7 2\ntable of the ulcer data.\nhere p+ = pr(r1 \u2265 r1 |\na = a; 0), and z\nand z are\nthe standardized forms of\nr1 without and with\ncontinuity correction.\nnote how closely\n1 \u2212 \u0001(z) and 1 \u2212 \u0001(z\nmatch p+ and p+,mid\nrespectively.\n\n)\n\n(cid:5)\n\n496\n\n10 \u00b7 nonlinear regression models\n\nr1\n\nm1\n\nr0\n\nm0\n\nr1\n\nm1\n\nr0\n\nm0\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n8\n11\n29\n14\n9\n3\n13\n15\n11\n36\n6\n5\n12\n14\n22\n7\n8\n30\n24\n36\n\n15\n19\n34\n20\n12\n7\n17\n16\n14\n38\n12\n7\n21\n21\n25\n11\n10\n31\n28\n43\n\n2\n8\n35\n13\n12\n0\n11\n3\n15\n20\n0\n2\n17\n20\n21\n4\n2\n23\n16\n27\n\n13\n16\n39\n21\n12\n4\n24\n16\n22\n32\n8\n9\n24\n25\n32\n10\n10\n27\n31\n43\n\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n34\n14\n54\n20\n6\n9\n12\n10\n22\n16\n14\n9\n20\n13\n30\n13\n30\n31\n34\n9\n\n40\n18\n68\n24\n6\n10\n17\n10\n22\n18\n15\n12\n20\n17\n40\n16\n34\n38\n34\n9\n\n8\n34\n61\n19\n0\n10\n10\n2\n16\n11\n6\n2\n18\n14\n8\n14\n14\n22\n0\n16\n\n21\n39\n74\n27\n6\n15\n15\n14\n24\n21\n13\n9\n23\n16\n20\n16\n19\n37\n34\n16\n\n0\n\n1\n1\n1\n1\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n10.999\n10.999\n10.994\n10.995\n\n0.989\n0.987\n0.959\n0.966\n\n0.929\n0.925\n0.840\n0.854\n\n0.751\n0.747\n0.604\n0.609\n\n0.456\n0.456\n0.320\n0.309\n\n0.184\n0.187\n0.114\n0.101\n\n0.043\n0.048\n0.024\n0.020\n\n0.005\n0.007\n0.003\n0.002\n\n0\n0\n0\n0\n\nr1\n\np+\n\n1 \u2212 \u0001(z)\np+,mid\n1 \u2212 \u0001(z\n(cid:5)\n\n)\n\nthe left panel of figure 10.8 shows the conditional and unconditional distribu-\ntions of z; for the unconditional distribution \u03bb = 0. though both are discrete, the\nunconditional distribution is much more nearly continuous.\n\nthe right panel shows summaries for likelihood analysis of the data. the difference\nbetween the conditional likelihood based on (10.26) and the profile likelihood for \u03c8\nis small, but we should be wary of using large-sample likelihood approximations,\nbecause the sample is rather small. the panel also shows slices through the likelihood\ncorresponding to various values of \u03bb; evidently the likelihood depends strongly on\n(cid:1)\nboth parameters.\nthe history of the 2 \u00d7 2 table has been dogged by controversy, partly because of the\neffect of the discreteness of the conditional distribution of r1 on confidence intervals\nfor \u03c8. the unconditional distribution is more nearly continuous, so it yields shorter\nconfidence intervals and more powerful tests. hence some authors believe that infer-\nence should be based on the unconditional rather than on the conditional distribution.\nthe drawback is that as the unconditional distribution depends on \u03bb it does not give\nexact tests and confidence intervals, whereas the conditional approach does.\n\n "}, {"Page_number": 509, "text": "10.4 \u00b7 proportion data\n\n497\n\nfigure 10.8 analysis\nfor first 2 \u00d7 2 table of\nulcer data. left:\nconditional (bold) and\nunconditional distribution\nof standardized r1. right:\nrelative likelihoods based\non conditional distribution\nof r1 given a (heavy),\nprofile likelihood (solid),\nand slices through\nlikelihood based on r1\nand r2 for fixed values of\n\u03bb, equal to\n\u22120.5,\u22121,\u22121.5, 2,\u22122.5,\nfrom left to right (dots).\n\ny\nt\ni\nl\ni\n\nb\na\nb\no\nr\np\n \ne\nv\ni\nt\na\nu\nm\nu\nc\n\nl\n\n0\n.\n1\n\n8\n.\n0\n\n6\n.\n0\n\n4\n.\n0\n\n2\n.\n0\n\n0\n\n.\n\n0\n\nd\no\no\nh\n\ni\nl\n\ne\nk\n\ni\nl\n \ne\nv\ni\nt\na\ne\nr\n\nl\n\n0\n.\n1\n\n8\n.\n0\n\n6\n.\n0\n\n4\n.\n0\n\n2\n.\n0\n\n0\n\n.\n\n0\n\n-4\n\n-2\n\n0\n\n2\n\n4\n\nstandardized test statistic\n\n-1\n\n0\n\n1\n\n2\n\n3\n\n4\n\npsi\n\nexercises 10.4\n1\n\ndata y1, . . . , yn are assumed to follow a binary logistic model in which y j takes value 1\nwith probability \u03c0 j = exp(x t\n\u03b2)} and value 0 otherwise, for j = 1, . . . ,n .\nthe deviance for a model with fitted probabilities (cid:2)\u03c0 j can be\n(a) show that\nwritten as\n\n\u03b2)/{1 + exp(x t\n\nj\n\nj\n\n(cid:7)\n\nyt x(cid:2)\u03b2 + n(cid:1)\n\nj=1\n\n(cid:8)\n\nlog(1 \u2212(cid:2)\u03c0 j )\n\nd = \u22122\n\nand that the likelihood equation is x t y = x t(cid:2)\u03c0. hence show that the deviance is a function\nof the(cid:2)\u03c0 j alone.\n(b) if \u03c01 = \u00b7\u00b7\u00b7 = \u03c0n = \u03c0, then show that(cid:2)\u03c0 = y, and verify that\nd = \u22122n {y log y + (1 \u2212 y) log(1 \u2212 y)} .\n\ncomment on the implications for using d to measure the discrepancy between the data\nand fitted model.\n(c) in (b), show that pearson\u2019s statistic (10.21) is identically equal to n. comment.\n(a) show that the parametric link function\n\ng(\u03c0; \u03b3 ) = log [\u03b3 \u22121{(1 \u2212 \u03c0)\n\n\u2212\u03b3 \u2212 1}],\n\n\u03b3 (cid:3)= 0,\n\ngives the logit and complementary log-log links when \u03b3 = 1 and when \u03b3 \u2192 0.\ngive a similar function containing the logit and log-log link functions.\n(b) show that the link function\n\ng(\u03c0; \u03b3 ) = 2\u03b3 \u22121\n\n\u03c0 \u03b3 \u2212 (1 \u2212 \u03c0)\u03b3\n\u03c0 \u03b3 + (1 \u2212 \u03c0)\u03b3\n\n,\n\n\u03b3 (cid:3)= 0,\n\nis symmetric for all \u03b3 and gives the logit and identity functions when \u03b3 \u2192 0 and when\n\u03b3 = 1.\nif x is a poisson variable with mean \u00b5 = exp(x t\u03b2) and y is a binary variable indicating\nthe event x > 0, find the link function between e(y ) and x t\u03b2.\n\n2\n\n3\n\n "}, {"Page_number": 510, "text": "498\n\n10 \u00b7 nonlinear regression models\n\n10.5 count data\n10.5.1 log-linear models\nthe basic model for count data treats the response y as a poisson variable with\nmean \u00b5. with the canonical, log, link, \u00b5 = exp(x t\u03b2); this is a log-linear model.\nin certain applications y may be thought of as the number of events in a poisson\nprocess of rate exp(x t\u03b2) observed for a period t , inwhich case \u00b5 = t exp(x t\u03b2) =\nexp(x t\u03b2 + log t ). this is a log-linear model with linear predictor \u03b7(cid:5) = x t\u03b2 + log t ;\nthe offset term log t is a fixed part of the linear predictor.\n\nthe connection between the poisson and binomial distributions induces a rela-\ntionship between log-linear and logistic models. let y1 and y2 be independent pois-\nson variables with means \u00b51 and \u00b52. then the conditional distribution of y2 given\nthat y1 + y2 = m is binomial with probability \u03c0 = \u00b52/(\u00b51 + \u00b52) and denominator\nm. if\u00b5 1 = exp(\u03b3 + x t\n\u03b2), then \u03c0 = exp{(x2 \u2212 x1)t\u03b2}/[1 +\nexp{(x2 \u2212 x1)t\u03b2}], so \u03b2 may be estimated either by a log-linear model based on both\nobservations, or by a logistic model using the conditional distribution of the second\ngiven their sum; in this second case \u03b3 cannot be estimated.\n\n\u03b2) and \u00b52 = exp(\u03b3 + x t\n\n1\n\n2\n\nexample 10.21 (premier league data) we consider the numbers of goals scored\nin the 380 soccer matches played in the english premier league in the 2000\u20132001\nseason. the data are the home and away scores, yh\ni j , when team i is at home\nto team j, treated as independent poisson variables with means\n\ni j and ya\n\n= exp(\u0001 + \u03b1i \u2212 \u03b2 j ), \u00b5a\n\ni j\n\n= exp(\u03b1 j \u2212 \u03b2i ),\n\n\u00b5h\ni j\n\nwhere \u0001 represents the home advantage and \u03b1i and \u03b2i the offensive and defensive\nstrengths of team i. we expect to find \u0001 >0, corresponding to better performance\nfor teams playing at home.\n\ntable 10.13 contains the analysis of deviance for this log-linear model. there are\nlarge home and offensive effects and weaker but still very significant defensive effects.\nalthough the residual deviance is substantially larger than its degrees of freedom,\nonly 36 of the individual scores exceed three goals, so asymptotics based on large\ncounts are suspect. for the same reason residual analysis is not very useful, and we\nassess model fit by monte carlo methods. simulation from the fitted model gave 999\ndeviances with average value of 826, of which 748 exceeded the observed value. thus\nthe observed residual deviance is not unusual, suggesting that the model is broadly\n\nadequate. under this model, (cid:2)\u0001 = 0.37 (0.07), so the mean score of a team playing\nat home is increased by a substantial multiplier of exp((cid:2)\u0001) = 1.45. estimates of the\n\nother parameters are given in the lower part of table 10.13. the fitted mean scores are\nreadily computed; for example when manchester united is at home to coventry the\nfitted means are exp{0.37 + 0.22 \u2212 (\u22120.52)} =3.03 and exp(\u22120.53 \u2212 0.15) = 0.51.\nin fact this match was a 4\u20132 win for the home team, coventry doing better than\nexpected but losing anyway.\n\na different analysis models the home score, given the total score m for each match.\nthe paragraph preceding this example shows that if the log-linear model is correct,\n\n "}, {"Page_number": 511, "text": "10.5 \u00b7 count data\n\n499\n\ntable 10.13 log-linear\nand logistic models fitted\nto premier league data.\nthe upper part shows the\nanalysis of deviance for\nlog-linear models with\nparameters for home\nadvantage, offense and\ndefense. the lower part\nshows a league table\nbased on the overall\nstrengths estimated from\nthe binomial model, with\nestimated offensive and\ndefensive capabilities\nfrom the log-linear model.\nthe baseline team is\narsenal, some of whose\nparameters are aliased.\nindividual standard errors\nare not shown, but they are\nwithin \u00b10.02 of the values\nat the foot of the table.\n\nlog-linear model\n\nlogistic model\n\nterms\n\nhome\ndefense\noffense\n\ndeviance\nreduction\n\n33.58\n39.21\n58.85\n\ndf\n\n1\n19\n19\n\nterms\n\nhome\nteam\n\ndeviance\nreduction\n\n33.58\n79.63\n\ndf\n\n1\n19\n\nresidual\n\n720\n\n801.08\n\nresidual\n\n332\n\n410.65\n\noverall (\u03b4)\n\noffensive (\u03b1)\n\ndefensive (\u03b2)\n\nmanchester united\nliverpool\narsenal\nchelsea\nleeds\nipswich\nsunderland\naston villa\nwest ham\nmiddlesborough\ncharlton\ntottenham\nnewcastle\nsouthampton\neverton\nleicester\nmanchester city\ncoventry\nderby\nbradford\n\nses\n\n0.39\n0.13\n\u2014\n\u22120.09\n\u22120.10\n\u22120.16\n\u22120.33\n\u22120.48\n\u22120.53\n\u22120.53\n\u22120.55\n\u22120.58\n\u22120.59\n\u22120.60\n\u22120.75\n\u22120.77\n\u22120.90\n\u22120.93\n\u22120.93\n\u22121.29\n\n0.29\n\n0.22\n0.12\n0.04\n0.08\n0.02\n\u22120.10\n\u22120.31\n\u22120.31\n\u22120.33\n\u22120.35\n\u22120.21\n\u22120.28\n\u22120.35\n\u22120.45\n\u22120.32\n\u22120.47\n\u22120.40\n\u22120.53\n\u22120.51\n\u22120.71\n\n0.20\n\n0.15\n\u22120.08\n\u2014\n\u22120.22\n\u22120.17\n\u22120.13\n\u22120.10\n\u22120.15\n\u22120.30\n\u22120.17\n\u22120.43\n\u22120.38\n\u22120.30\n\u22120.25\n\u22120.46\n\u22120.31\n\u22120.56\n\u22120.52\n\u22120.45\n\u22120.62\n\n0.20\n\nthe distribution of the number of goals scored when team i plays at home to team j\nis binomial with denominator m and probability\n\n\u00b5h\ni j\n+ \u00b5a\n\ni j\n\n\u00b5h\ni j\n\nexp(\u0001 + \u03b1i \u2212 \u03b2 j )\n\n=\nexp(\u0001 + \u03b1i \u2212 \u03b2 j ) + exp(\u03b1 j \u2212 \u03b2i )\n= exp(\u0001 + \u03b4i \u2212 \u03b4 j )\n1 + exp(\u0001 + \u03b4i \u2212 \u03b4 j )\n\n,\n\n(10.29)\nwhere \u03b4i = \u03b1i + \u03b2i represents the overall strength of team i. under this logistic\nmodel, no-score draws contribute no information, as the conditional distribution of\ny2 is degenerate when m = 0, and if there was no home advantage and no differences\namong the teams, the number of goals scored by the home side would be binomial\nwith denominator m and probability 1\n2 . this analysis will give no information on the\n\n "}, {"Page_number": 512, "text": "500\n\n10 \u00b7 nonlinear regression models\n\nabsolute goal-scoring abilities of the teams, merely their relative strengths. as an\narbitrary constant may be added to the \u03b4i , they cannot all be estimated; we deal with\nthis by declaring that \u03b4 = 0 for arsenal.\nthe teams. the home advantage remains (cid:2)\u0001 = 0.37 (0.07), and the estimated \u03b4s are\n\nwhen this model is fitted to the 352 matches with at least one goal scored, we obtain\nthe deviances in the right part of table 10.13. there are strong differences among\n\ngiven in the lower part of the table. the broad pattern is the same as in the log-linear\nmodel, though the ordering of clubs in the middle of the league is different. however,\nthe standard errors for the team effects are larger than those for the log-linear model,\nbecause information is lost when the logistic model is fitted; see exercise 10.5.2.\n\nthe logistic model gives an overall ranking of the teams similar to the official as the lowest three sides\nwere relegated to the first\ndivision, their supporters\nmight not regard this as a\nmatter of detail!\n\nranking, though with differences of detail: arsenal and liverpool are interchanged,\nand so are derby and manchester city. the centre of the table has further differences,\n\nbut asthe standard error for each (cid:2)\u03b4 j \u2212(cid:2)\u03b4i is about 0.3, it is dangerous to read much\n\ninto them. one reason for the differences is that the ranking here is based on numbers\nof goals, while the official ranking gives 2 for a win, 1 for a draw, and 0 for a\n(cid:1)\nloss.\n\nthis would not be a good\nway toproceed because of\nlikely bias due to\nnon-random sampling.\n\n10.5.2 contingency tables\ncount data often arise in the form of contingency tables that cross-classify individuals\naccording to their attributes. the appropriate class of models for such a table depends\non the sampling scheme. suppose that an r \u00d7 c table arises by randomly sampling\na population over a fixed period and then classifying the resulting individuals. for\nexample, a researcher interested in the association of gender (rows) and voting inten-\ntions (columns) might stand on a street corner for an hour recording data from anyone\nwilling to talk to him. there are then no constraints on the row and column totals,\nand a simple model is that the count in the (r, c) cell, yrc, has a poisson distribution\n\nwith mean \u00b5rc. the resulting likelihood is(cid:25)\n\n(cid:11)\n\n(cid:12)\n\n\u2212\u00b5rc\n\ne\n\n;\n\n\u00b5yrc\nrc\nyrc!\n\nr,c\n\nthis is simply the poisson likelihood for the counts in the rc groups.\n\n(cid:13)\nour hapless researcher may set out with the intention of interviewing a fixed num-\nrc yrc = m. inthis case the data are\n(cid:1)\n\nber m of individuals, stopping only when\nmultinomially distributed, with likelihood\n\n(cid:25)\n\nm!(cid:26)\nr,c yrc!\n\n\u03c0 yrc\nrc\n\n,\n\nr,c\n\nr,c\n\n\u03c0rc = 1,\n\n\u00b5st the probability of falling into the (r, c) cell.\n\nwith \u03c0rc = \u00b5rc/\nthe row totals mr = (cid:13)\na third scheme is to interview fixed numbers of men and of women, thus fixing\nc yrc in advance. in effect this treats the row categories as\nsubpopulations, and the column categories as the response. this yields independent\n\ns,t\n\n(cid:13)\n\n "}, {"Page_number": 513, "text": "10.5 \u00b7 count data\n\n501\n\nmultinomial distributions for each row, and product multinomial likelihood\n\n(cid:7)\n\n(cid:25)\n\nr\n\n(cid:25)\n\nc\n\nmr !(cid:26)\n(cid:13)\nc yrc!\n\n(cid:8)\n\n\u03c0 yrc\nrc\n\n,\n\n(cid:1)\n\nc\n\n\u03c01c = \u00b7\u00b7\u00b7 =\n\n\u03c0rc = 1,\n\n(cid:1)\n\nc\n\nin which \u03c0rc = \u00b5rc/\n\u00b5rt . see table 10.2, in which the response is the fate of a\nfixed number of butterflies for each combination of species and colour; the appropriate\nproduct multinomial model fixes the total for each triplet.\n\nt\n\nthese three set-ups can all be fitted as log-linear models, provided the appropriate\nbaseline terms are included in the linear predictor. to see this, we arrange our data\nas a two-way layout, with row totals fixed: the multinomial sampling scheme gives\njust one row, whereas we would arrange the data in table 10.2 as a 48 \u00d7 3 table.\nsuppose that the cell counts yrc are independent poisson variables with means \u00b5rc =\nexp(\u03b3r + x t\n\u03b2), where \u03b3r corresponds to the overall count in the rth row; interest\nc yrc = mr\nfocuses on the parameter \u03b2. the multinomial model has fixed row totals\nand probabilities\n\n(cid:13)\n\nrc\n\n(cid:4)\n\n(cid:5)\n\n\u03b3r + x t\n(cid:4)\n\u03b3r + x t\n\nrc\n\n\u03b2\n\nrd\n\n(cid:13)\n\n(cid:5) = exp\nx t\nrc\nd exp\nx t\nrd\n\n(cid:4)\n\n\u03b2\n\n\u03b2\n\n\u03b2\n\n(cid:5) ,\n\n(cid:4)\n\n(cid:5)\n\n\u03c0rc = \u00b5rc(cid:13)\n\n(cid:13)\n= exp\nd exp\nso the corresponding log likelihood is\n\n\u00b5rd\n\nd\n\n(cid:1)\n(cid:1)\n\nrc\n\n(cid:6)mult(\u03b2; y | m) \u2261\n\n=\n\nyrc log \u03c0rc\n\n(cid:7)(cid:1)\n\nyrcx t\nrc\n\n\u03b2 \u2212 mr log\n\n(cid:10)(cid:8)\n\n\u03b2\n\nex t\n\nrc\n\n,\n\n(10.30)\n\n(cid:9)(cid:1)\n\nc\n\nr\n\nc\n\n(cid:1)\n(cid:1)\n\nr,c\n\nwhere we have emphasized the fact that the likelihood is based on the conditional\ndistribution of the counts y given the row totals m.\n\nfor the poisson model there is no conditioning, so the log likelihood is\n\nr\n\n=\n\n(cid:10)\n\nyrcx t\nrc\n\n(cid:1)\n\n(cid:1)\n\n\u03b2 \u2212 e\u03b3r\n\n(cid:6)poiss(\u03b2, \u03b3 ) \u2261\n\n(yrc log \u00b5rc \u2212 \u00b5rc)\n(cid:9)\nmr \u03b3r +\n(cid:13)\nthe row totals \u03c4r = (cid:13)\nas the \u03b3r are not of central concern, we express this log likelihood as a function of\nof \u03b2 and the \u03c4r , we have\u03b3 r = log \u03c4r \u2212 log{(cid:13)\nc ex t\n\u03b2 and the parameter of interest, \u03b2. interms\n(cid:7)(cid:1)\n(cid:1)\n(cid:1)\n(cid:6)poiss(\u03b2, \u03c4 ) \u2261\n= (cid:6)poiss(\u03c4 ; m) + (cid:6)mult(\u03b2; y | m),\n\n\u03b2)}, giving\n\u03b2 \u2212 mr log\n\n(mr log \u03c4r \u2212 \u03c4r ) +\n\n\u00b5rc = e\u03b3r\n\n(cid:9)(cid:1)\n\nc exp(x t\nrc\n\n(cid:10)(cid:8)\n\nyrcx t\nrc\n\nex t\n\nex t\n\n,\n\nrc\n\nrc\n\nrc\n\n\u03b2\n\n\u03b2\n\n.\n\nc\n\nc\n\nc\n\nc\n\nc\n\nr\n\nr\n\nsay. the first term on the right of this decomposition is the log likelihood that cor-\nresponds to the poisson distribution of the row total mr \u2014 asum of independent\npoisson variables \u2014 while the second is the multinomial log likelihood (10.30).\nthus the mr form a cut, and the maximum likelihood estimates of \u03b2 and \u03c4 based\n\n "}, {"Page_number": 514, "text": "502\n\non (cid:6)poiss(\u03b2, \u03c4 ) are the same as those based on separate maximizations of (cid:6)poiss(\u03c4 ; m)\n\n10 \u00b7 nonlinear regression models\nand (cid:6)mult(\u03b2; y | m); see (5.21). hence(cid:2)\u03b2 equals the maximum likelihood estimate for\nthe multinomial log likelihood (10.30), and (cid:2)\u03c4r = mr . moreover, the observed and\nexpected information matrices for the model are block diagonal, with blocks corre-\nsponding to \u2212\u2202 2(cid:6)poiss(\u03c4 ; m)/\u2202\u03c4 \u2202\u03c4 t and \u2212\u2202 2(cid:6)mult(\u03b2; y | m)/\u2202\u03b2\u2202\u03b2 t.\nto see that the standard errors for(cid:2)\u03b2 based on the multinomial and poisson models\n\nare equal, note that \u2202 2(cid:6)poiss(\u03b2, \u03c4 )/\u2202\u03b2\u2202\u03b2 t depends on the data only through the mr .\ntherefore the expected information for \u03b2 under the multinomial model, in which the\nmr are fixed, equals the observed information for \u03b2 under the poisson model. under\nthe poisson model, the expected information for \u03b2 is\n\n\u2202 2 log\n\ne(mr )\n\n\u03b2\n\nc ex t\n\nrc\n\n=\n\n\u2202\u03b2\u2202\u03b2 t\n\n\u2202 2 log\n\n\u03b2\n\nc ex t\n\nrc\n\n\u2202\u03b2\u2202\u03b2 t\n\n(cid:4)(cid:13)\n\n(cid:5)\n\n(cid:1)\n\n\u03c4r\n\nr\n\n(cid:4)(cid:13)\n\n(cid:5)\n\n,\n\n(cid:1)\n\nr\n\nand the standard errors for(cid:2)\u03b2 are obtained by replacing \u03c4r and \u03b2 with their estimates,\nand inverting the resulting matrix. but as(cid:2)\u03c4r = mr , the resulting standard errors will\n\nequal those obtained by inverting the expected information matrix obtained from\n(10.30). it follows that the numerical values of standard errors and maximum like-\nlihood estimates for \u03b2 under the poisson model are the same as those under the\nmultinomial model, provided that the parameters associated with the margin fixed\nunder the multinomial model, the \u03b3r , are included in the fit. the log linearity is im-\nportant here, as it ensures that second derivatives of both log likelihoods with respect\nto \u03b2 involve the counts yrc only through their row totals mr .\n\nexample 10.22 (jacamar data) let ycs f denote the number of butterflies of the\ncth colour and sth species suffering the f th fate, where c = 1, . . . ,8, s = 1, . . . ,6,\nand f = 1, 2, 3. if we treat fate as the response, any model should fix the total count\nfor each of the 48 combinations of species and colour, giving 48 trinomial variables.\nany poisson model should have a term \u03b1cs in the linear predictor. for example,\nlog \u00b5cs f = \u03b1cs corresponds to equal probability of each of the three fates, whatever\nthe colour and species, because\n\n(cid:9)\n\n(\u03c0cs1, \u03c0cs2, \u03c0cs3) =\n\n\u00b5cs1(cid:13)\n\n\u00b5cs f\n\nf\n\n\u00b5cs2(cid:13)\n\n\u00b5cs f\n\nf\n\n\u00b5cs3(cid:13)\n\n\u00b5cs f\n\nf\n\n,\n\n,\n\n(cid:10)\n\n(cid:14)\n\n=\n\n(cid:15)\n\n,\n\n,\n\n1\n3\n\n,\n\n1\n3\n\n1\n3\n\nand this is independent of c and s, while log \u00b5cs f = \u03b1cs + \u03b3 f corresponds to proba-\nbilities\n\n(cid:9)\n\n(cid:10)\n\n(\u03c0cs1, \u03c0cs2, \u03c0cs3) =\n\n\u00b5cs1(cid:13)\n\n\u00b5cs f\n\nf\n\n,\n\n\u00b5cs2(cid:13)\n\n\u00b5cs f\n\nf\n\n\u00b5cs3(cid:13)\n\n\u00b5cs f\n\nf\n\n,\n\n=\n\n1\n\ne\u03b31 + e\u03b32 + e\u03b33\n\n(e\u03b31 , e\u03b32 , e\u03b33) ,\n\nalso independent of colour and species. linear predictor log \u00b5cs f = \u03b1cs + \u03b3c f\n\n "}, {"Page_number": 515, "text": "10.5 \u00b7 count data\n\n503\n\ntable 10.14 deviances\nfor log-linear models\nfitted to jacamar data.\n\nterms\n\ndf\n\ndeviance\n\nc(cid:22)s\nc(cid:22)s+f\nc(cid:22)s+c(cid:22)f\nc(cid:22)s+s(cid:22)f\nc(cid:22)s+c(cid:22)f+s(cid:22)f\nc(cid:22)s(cid:22)f\n\n22\n86\n72\n76\n62\n0\n\n259.42\n173.86\n139.62\n148.23\n90.66\n0\n\ncorresponds to probability vector\n(\u03c0cs1, \u03c0cs2, \u03c0cs3) =\n=\n\n(cid:9)\n\n\u00b5cs1(cid:13)\n\nf\n\n\u00b5cs f\n1\n\n(cid:10)\n\n\u00b5cs2(cid:13)\n\n\u00b5cs f\n\nf\n\n,\n\n,\n\n\u00b5cs3(cid:13)\n\n\u00b5cs f\n\nf\n\ne\u03b3c1 + e\u03b3c2 + e\u03b3c3\n\n(e\u03b3c1 , e\u03b3c2 , e\u03b3c3) ,\n\nin which the probabilities of the different fates depend on colour, but not on species.\nlet c(cid:22)s+f denote the terms of the linear predictor \u03b1cs + \u03b3 f . then the terms for the\nthree models above are c(cid:22)s, c(cid:22)s+f, and c(cid:22)s+c(cid:22)f. any model that treats f as the\nresponse must contain a term c(cid:22)s, which fixes the row totals. the term c(cid:22)f indicates\nthat the response probabilities depend on colour.\n\ntable 10.14 contains the deviances for the models with c(cid:22)s, with degrees of freedom\nadjusted for triplets with zero totals. the best-fitting model is the full model c(cid:22)s(cid:22)f.\nthe best reasonable model is c(cid:22)s+c(cid:22)f+s(cid:22)f, which extends to trinomial responses the\nbinomial two-way layout model of example 10.15, but its deviance is large compared\nto its asymptotic \u03c7 2\nif categories n and s were merged there would be 96 observations and two possible\nfates. in this case the linear predictor log \u00b5cs f = \u03b1cs + \u03b3c f corresponds to probabilities\n\n62 distribution.\n\n(\u03c0cs1, \u03c0cs2) =\n\n1\n\ne\u03b3c1 + e\u03b3c2\n\n(e\u03b3c1 , e\u03b3c2) =\n\n1\n\n1 + e\u03b3c2\u2212\u03b3c1\n\n(1, e\u03b3c2\u2212\u03b3c1),\n\nwhich is the binomial logistic model with terms 1+colour fitted in example 10.15.\nwhen the response classification has two categories, it simplifies matters to fit the\nmodel as binomial rather than poisson, although identical inferences are drawn about\n(cid:1)\nits parameters.\n\nexample 10.23 (lung cancer data) example 1.4 gives data on the lung cancer\nmortality of cigarette smokers among british male physicians. the response is the\nnumber of deaths in each cell of the table, which also gives the total number of\nman-years of exposure t in each category.\nwe initially fit a log-linear model with factors for both margins and offset log man-\nyears at risk in each cell. thus trc exp(\u03b1r + \u03b2c) isthe mean number of deaths in the\n(r, c) cell. this model has deviance 51.47 on 48 degrees of freedom and appears to fit\nwell. figure 10.9 shows the coefficients for this model; the first level of each factor is\ntaken to have coefficient zero. the figure suggests that there is a linear effect of dose\nd on the cancer rate, but that the increase with age is faster. however the standard\n\n "}, {"Page_number": 516, "text": "10 \u00b7 nonlinear regression models\n\n504\n\n0\n0\n5\n1\n\n0\n0\n0\n1\n\n0\n0\n5\n\nr\ne\n\nt\n\ne\nm\na\nr\na\np\n\n \n\ne\nm\nt\n\ni\n\n0\n\n-\n-\n\u2022\n\n-\n\u2022\n-\n\n-\n\u2022\n-\n\n30\n\n-\n-\n\u2022\n\n20\n\n-\n\n\u2022\n-\n\nt\n\nr\ne\ne\nm\na\nr\na\np\n\n \n\ne\ns\no\nd\n\n0\n0\n1\n\n0\n8\n\n0\n6\n\n0\n4\n\n0\n2\n\n0\n\n-\n\n\u2022\n-\n\n-\n\n\u2022\n-\n\n-\n\n\u2022\n-\n\n40\n\n50\n\n-\n\u2022\n-\n\n\u2022\n\n0\n\nfigure 10.9 results for\ntwo-way layout model,\nplotted against age and\ncigarette consumption.\nshown are exponentials of\ncoefficients, plus/minus\ntwo standard errors.\n\n-\n\n\u2022\n\n-\n\n-\n\n\u2022\n-\n\n-\n\n\u2022\n-\n\n-\n\n\u2022\n-\n\n-\n\n\u2022\n-\n\n10\n\n20\n\n30\n\n40\n\nnumber of cigarettes\n\nyears smoking\n\nerrors for the individual parameters are very large, reflecting the small numbers of\ndeaths in most cells.\n\nfor amore concise model that is not log-linear, let the death rate for those smoking\n\nd cigarettes per day after t years of smoking be\n\n\u03bb(d, t) = (cid:4)\n\n\u03b20 + \u03b21d \u03b22\n\n(cid:5)\n\nt \u03b23 ,\n\n(10.31)\n\ndeaths per 100,000 man-years at risk; here \u03b20 and \u03b21 are non-negative, and \u03b22 and\n\u03b23 are real. we take t to be the midpoint of each group, divided by 42.5, so that \u03b20\nrepresents the background rate of cancer for non-smokers aged 62.5 years, for whom\nthe rescaled t = 1. the broadly exponential pattern in the left panel of figure 10.9\nsuggests that \u03b23 > 1. the term \u03b21d \u03b22 describes the effect of smoking on death rates;\n.= 1, corresponding to the linear increase seen in the right panel of\nwe expect \u03b22\nfigure 10.9.\na likelihood ratio test of the effect of smoking on death rate would be non-regular,\nbecause setting either \u03b21 = 0 or\u03b2 2 = 0 eliminates both of these parameters; moreover\n\u03b21 = 0 is aboundary hypothesis. in either case the resulting model is log-linear, with\ndeviance 180.8 on 61 degrees of freedom; the fit seems poor, despite the low counts\nand hence the likely inapplicability of chi-squared deviance asymptotics.\n\nto fit the full model it is better to recast (10.31) as\n\n\u03bb(d, t) = {e\u03b30 + exp(\u03b31 + \u03b22 log d)} exp(\u03b23 log t),\n\nso that all the parameters are unconstrained; the term exp(\u03b31 + \u03b22 log d) isomitted\nfor the non-smokers. in this form it is straightforward to maximize the log likelihood\nby iterative weighted least squares, giving deviance 59.58 on 59 degrees of freedom,\nso marked an improvement on the model without smoking that the non-regularity of\nthe asymptotics is immaterial.\n\ntable 10.15 shows that the precision of (cid:2)\u03b30 depends heavily on the data for non-\n(cid:2)\u03b30 = 18.9\nsmokers. the background non-smoker death-rate from cancer at age 62.5 is e\nper 100,000 years at risk.\nwith the restriction \u03b22 = 1 the deviance increases to 61.84 on 60 degrees of free-\ndom, a deviance difference of 2.26 on 1 degree of freedom. linear dependence of\n\n "}, {"Page_number": 517, "text": "table 10.15 parameter\nestimates (standard errors)\nfor lung cancer data.\n\ntable 10.16 joint\ndistribution of visual\nimpairment on both eyes\nby race and age liang\net al. (1992). combination\n(0, 0) means neither eye is\nvisually impaired.\n\n10.5 \u00b7 count data\n\n505\n\n\u03b30\n\n\u03b31\n\n\u03b22\n\n\u03b23\n\nsmokers only\nall data\nall data (\u03b22 = 1)\n\n0.96 (25.4)\n2.94 (0.58)\n2.75 (0.56)\n\n2.15 (1.45)\n1.82 (0.66)\n2.72 (0.09)\n\n1.20 (0.40)\n1.29 (0.20)\n\n\u2014\n\n4.50 (0.34)\n4.46 (0.33)\n4.43 (0.33)\n\neye\n\nprevalence for whites aged\n\nprevalence for blacks aged\n\nleft\n\nright\n\n40\u201350\n\n51\u201360\n\n61\u201370\n\n0\n1\n0\n1\n\n0\n0\n1\n1\n\n602\n11\n15\n4\n\n541\n15\n16\n9\n\n752\n31\n37\n11\n\n70+\n\n606\n60\n67\n79\n\n40\u201350\n\n51\u201360\n\n61\u201370\n\n729\n19\n21\n10\n\n551\n24\n23\n14\n\n452\n22\n21\n28\n\n70+\n\n307\n29\n37\n56\n\ndeath rate on d appears plausible, in which case the background death rate drops\nsomewhat to 15.6 deaths per 100,000 man-years at risk, but rises by an additional\n(cid:2)\u03b31\n.= 15.2 for every cigarette smoked daily. case analysis shows no residuals out of\ne\nline, and the model appears to fit well. it is both more parsimonious than the log-linear\n(cid:1)\nmodel and motivated by substantive considerations, so it seems preferable.\n\nmarginal models\nalthough mathematically elegant and simple to fit, log-linear models have some awk-\nward statistical properties because their parameters have interpretations that depend\non other terms in the model, as we shall now see.\n\nexample 10.24 (eye data) table 10.16 gives data from the baltimore eye study\nsurvey. drivers are classified by age, race and visual impairment, defined as vision\nless than 20/60; in the original data their level of education is also available, and is\ntreated as a surrogate for socioeconomic status. the aim of the original analysis was\nto see how visual impairment depends on age and race, controlling for education, but\nwe shall simply consider dependence on age and race.\nwe treat the data as eight 2 \u00d7 2 tables corresponding to columns 3\u201310 of the table,\nthat is one for each different combination of race and age, given by the covariate\nvector x. each table has elements (y00, y01; y10, y11), where y00 is the number of\nmen without visual impairment, y01 is the number whose right eye only is poor, and\nso forth. the total number of men with covariate combination x is m = y00 + y01 +\ny10 + y11, which we treat as fixed. the corresponding probabilities (\u03c000, \u03c001; \u03c010, \u03c011)\ndepend on x.\n\na natural preliminary to joint analysis of data for both eyes is to fit logistic regres-\nsion models and estimate the probability of impairment in each eye separately. for\nthe left eye we would treat rl = y10 + y11 as a binomial response with denominator\nm and probability\n\n\u03c010 + \u03c011 = \u03c0l = exp(x t\u03b2l)/{1 + exp(x t\u03b2l)},\n\n "}, {"Page_number": 518, "text": "506\n\n10 \u00b7 nonlinear regression models\nsay. for the right eye the response would be r r = y01 + y11 with denominator m and\nprobability \u03c001 + \u03c011 = \u03c0r = exp(x t\u03b2r)/{1 + exp(x t\u03b2r)}. here \u03b2l and \u03b2r summa-\nrize the effect of x on the marginal distributions of rl and r r. our earlier arguments\nshow that these logistic models are also log-linear.\n\nin generalizing these marginal models to allow for the anticipated dependence\nbetween the eyes, it is natural to augment \u03c0l and \u03c0r by adding further parameters.\none possibility is to write the odds ratio as\n\n\u03c011\u03c000\n\u03c010\u03c001\n\n= \u03c011(1 \u2212 \u03c0l \u2212 \u03c0r + \u03c011)\n(\u03c0l \u2212 \u03c011)(\u03c0r \u2212 \u03c011)\n\n= exp(x t\u03b2l r).\n\nif x t\u03b2l r = \u03b3 was independent of x, there would be constant association between the\neyes after adjusting for marginal effects of age and race, with more complicated models\nindicating more complex patterns of association. as 0 < \u03c000, \u03c001, \u03c010, \u03c011 < 1, the\nprobability \u03c011 must lie in the interval (max(0, \u03c0l + \u03c0r \u2212 1), min(\u03c0l , \u03c0r)), and a\nlittle algebra shows that \u03c011 may be expressed as the root of a quadratic equation\nwhose coefficients depends on \u03c0l, \u03c0r, and x t\u03b2l r, thereby enabling us to express the\nprobabilities in each 2 \u00d7 2 table in terms of the marginal probabilities and the odds\nratio.\n\nthe log-linear model for the joint density of (y00, y01; y10, y11) has probabilities\n(\u03c000, \u03c001; \u03c010, \u03c011) =\n\n(1, e\u03b3r , e\u03b3l , e\u03b3r+\u03b3l+\u03b3l r ),\n\n1 + e\u03b3r + e\u03b3l + e\u03b3r+\u03b3l+\u03b3l r\n\nwhere \u03b3l = x t\u03b4l, \u03b3r = x t\u03b4r, \u03b3l r = x t\u03b4l r. under this model the marginal proba-\nbility of an unimpaired left eye is\n\n1\n\n=\n\n\u03c0(cid:5)\n\nl\n\ne\u03b3l + e\u03b3r+\u03b3l+\u03b3l r\n\n1 + e\u03b3r + e\u03b3l + e\u03b3r+\u03b3l+\u03b3l r\n\n,\n\nwhich has logistic form e\u03b3l /(1 + e\u03b3l ) only when \u03b3l r = 0, that is conditional on x\nvisual impairment occurs independently in each eye. otherwise the marginal proba-\nbility of an impaired left eye depends on \u03b3r and \u03b3l r, implying that the initial logistic\nfits shed no light on \u03b3l.\n\nto put this another way, note that \u03b3l may be written as\n\n(cid:14)\n\n(cid:15)\n\n\u03c010\n\u03c000\n\nlog\n\n= log\n\npr(l = 1 | r = 0, x)\npr(l = 0 | r = 0, x)\n(cid:12)\n\n(cid:11)\n\n(cid:11)\n\nwith a similar expression for \u03b3r, and that\npr(l = 1 | r = 1, x)\npr(l = 0 | r = 1, x)\n\n\u03b3l r = log\n\n\u2212 log\n\npr(l = 1 | r = 0, x)\npr(l = 0 | r = 0, x)\n\n(cid:12)\n\n.\n\nthus the parameters of the log-linear model have interpretations in terms of contrasts\nof log odds for one eye conditional on the state of the other, and these do not yield\nmarginal probabilities with simple interpretations. therefore the log-linear model\nfor the joint outcomes is not upwardly compatible with the logistic models for the\nmarginal outcomes. this poses problems in applications where marginal properties\n(cid:1)\nof the variables are of interest.\n\n,\n\nl = 1 denotes visual\nimpairment in the left eye,\netc.\n\n "}, {"Page_number": 519, "text": "10.5 \u00b7 count data\n\n507\n\ninference for marginal models is awkward because complete specification of their\nlikelihoods is ordinarily neither possible nor desirable. an alternative is to base in-\nference on systems of estimating equations, and we now sketch how this is done.\nsuppose that the jth of n individuals contributes a q \u00d7 1 response vector y j and a\np \u00d7 1 vector of explanatory variables x j , and let z j denote the q(q \u2212 1)/2 \u00d7 1 vector\ncontaining the distinct products of pairs of elements of y j . now e(y j ) = \u00b5(x t\n\u03b2) is\nspecified by the marginal model, while the covariance structure among the responses\nis given by e(z j ) = \u03be(x t\n\u03b2, \u03b3 ), where \u03b2 represents the parameters of the marginal\nmodel, and \u03b3 additional parameters that account for association among elements of y j .\nin the preceding example q = 2 and y t\nj equals (0, 0), (0, 1), (1, 0), or (1, 1), indi-\n(cid:14)\n\ncating the state of the left and right eyes, while\n\n(cid:15)\n\nj\n\nj\n\n(cid:4)\n\ne\n\ny t\nj\n\n(cid:5) = \u00b5\n\n(cid:4)\n\n(cid:5)\n\nt = (\u03c0l , \u03c0r) =\n\n\u03b2\n\nx t\nj\n\nexp(x t\u03b2l)\n1 + exp(x t\u03b2l)\n\n,\n\nexp(x t\u03b2r)\n1 + exp(x t\u03b2r)\n\n,\n\n\u03b2, \u03b3 ), the probability of visual impairment in both eyes for individual j,\n\nand \u03be(x t\nj\ndepends both on x j and on the degree of association between the eyes.\nideas from section 7.2 suggest that consistent estimators of \u03b2 and \u03b3 may be obtained\nby combining the unbiased estimating functions y j \u2212 \u00b5(x t\n\u03b2, \u03b3 ),\nand the form of (7.21) suggests that the estimators that solve the generalized estimating\nequations\n\n\u03b2) and z j \u2212 \u03be(x t\n\nj\n\nj\n\n(cid:3)\n\n(cid:4)\n\n(cid:5)\n\n(cid:4)\n\n\u2202\n\n\u00b5\n\n\u03b2\n\nx t\nj\n\n\u03b2, \u03b3\n\nt, \u03be\n\nx t\nj\n\u2202(\u03b2, \u03b3 )\n\nn(cid:1)\nj=1\n\n(cid:6)\n\n(cid:5)\n\nt\n\n(cid:14)\n(cid:14)\n\n\u00d7\n\n(cid:15)\u22121\n\nvar(y j )\n\n(cid:4)\n\ncov(z j , y j )\n(cid:4)\ny j \u2212 \u00b5\nz j \u2212 \u03be\nx t\nj\n\n\u03b2\n\nx t\nj\n\u03b2, \u03b3\n\ncov(y j , z j )\n\n(cid:5)\n\n(cid:15)\n\nvar(z j )\n= 0\n\n(cid:5)\n\nwill have smallest asymptotic variance. the presence of cov(y j , z j ) means that third-\norder moments of y j must in principle be specified, and one way to avoid this is to\n\nreplace this term by a zero matrix. the resulting estimators (cid:2)\u03b2 and (cid:2)\u03b3 are consistent\nbut the variance of(cid:2)\u03b3 can be much larger than when the correct covariance matrix is\na simple form for cov(y j , z j ). standard errors for(cid:2)\u03b2 and(cid:2)\u03b3 are based on a sandwich\n\nused. if \u03b3 is of interest then some of the lost efficiency can be retrieved by assuming\n\ncovariance matrix; see section 7.2. in many applications it is important to be able to\naccomodate missing data, and this is achieved by allowing the length of y j to vary\nwith the individual; no essentially new points arise.\n\n10.5.3 ordinal responses\ndiscrete data often arise in which the response comprises numbers in ordered cat-\negories that may be labelled 1, . . . ,k . examples are individuals undergoing some\ntreatment and asked to say if they experience one of {no pain, slight pain, moderate\npain, extreme pain}, orwhere curries are classified as {bland, mild, . . . , volcanic}.\nthe goal is then typically to assess how these ordinal responses depend on explana-\ntory variables x. sometimes the response is a discretized version of an underlying\ncontinuous variable, though this interpretation is not always plausible. in either case\nsuitable models are based on the multinomial distribution. if there are n independent\n\n "}, {"Page_number": 520, "text": "508\n\n10 \u00b7 nonlinear regression models\nindividuals whose responses are i1, . . . , in, and i j = l indicates that the jth re-\nsponse falls in category l, then pr(i j = l) = \u03c0l for l = 1, . . . ,k , and the correspond-\ning cumulative probabilities are \u03b3l = pr(i j \u2264 l) = \u03c01 + \u00b7\u00b7\u00b7 +\u03c0 l for l = 1, . . . ,k ; of\ncourse \u03b3k = 1. individual responses with common explanatory variables x can be\nmerged to give a multinomial variable (y1, . . . ,y k), where yl represents the number\nin category l; thus y1 + \u00b7\u00b7\u00b7 + yk = n. typically the joint distribution of (y1, . . . ,y k)\ndepends on x through a linear predictor x t\u03b2.\n\nin many applications it is appropriate to require that the interpretation of the model\nparameters remains unchanged when adjacent categories are merged. one class of\nmodels with this property may be motivated by positing the existence of an underlying\ncontinuous variable \u03b5 with distribution function f, with i indicating into which of\nthe k intervals\n\n(\u2212\u221e, \u03b61], (\u03b61, \u03b62], . . . , (\u03b6k\u22122, \u03b6k\u22121], (\u03b6k\u22121,\u221e),\n\n\u03b61 < \u00b7\u00b7\u00b7 < \u03b6k\u22121,\n\nx t\u03b2 + \u03b5 falls. for convenience let \u03b60 = \u2212\u221e and \u03b6k = \u221e. then\n\u03c0l(x t\u03b2) = pr(i = l; x t\u03b2) = pr(\u03b6l\u22121 < x t\u03b2 + \u03b5 \u2264 \u03b6l)\n\n= f(\u03b6l \u2212 x t\u03b2) \u2212 f(\u03b6l\u22121 \u2212 x t\u03b2),\n\nand \u03b3l(x t\u03b2) = f(\u03b6l \u2212 x t\u03b2), for l = 1, . . . ,k . thus large x t\u03b2 leads to higher proba-\nbilities for the higher categories. a natural choice is the logistic distribution function\nf(u) = exp(u)/{1 + exp(u)}, which leads to the proportional odds model, so-called\nbecause the odds ratio of appearing in category l or lower for two individuals with\nexplanatory variables x1 and x2,\npr(i \u2264 l; x2)/pr(i > l; x2)\npr(i \u2264 l; x1)/pr(i > l; x1)\n\n(cid:3)\u2212(x2 \u2212 x1)t\u03b2\n\n(cid:4)\n\u03b6l \u2212 x t\n(cid:4)\n\u03b6l \u2212 x t\n\n(cid:5)\n(cid:5) = exp\n\n= exp\nexp\n\nis independent of l. another possibility that often works well in practice is f(u) =\n1 \u2212 exp{\u2212 exp(u)}. whatever the choice of f, interest focuses on how the response\ndepends on the covariates, summarized in \u03b2; typically \u03b61, . . . , \u03b6k\u22121 are of little con-\ncern. any overall intercept term in x t\u03b2 is aliased with the \u03b6l.\n\n(cid:6)\n\n\u03b2\n\n\u03b2\n\n,\n\n2\n\n1\n\nalthough this model is motivated by arguing as if an underlying continuous variable\nexists, this is not essential in order for it to be applied \u2014 the model may be useful\neven when \u03b5 clearly does not exist. as examples 4.21 and 10.17 show, the loss of\ninformation due to categorizing continuous data can be substantial if the number of\ncategories is very small.\n\nthe log likelihood based on independent responses i1, . . . , in with corresponding\n\nvectors of explanatory variables x1, . . . , xn may be written as\n\n(cid:6)(\u03b2, \u03b6 ) = n(cid:1)\n\nj=1\n\nk(cid:1)\nl=1\n\ni (i j = l) log \u03c0l(x t\n\nj\n\n\u03b2),\n\na multinomial log likelihood to which by now familiar methods can be applied.\n\nexample 10.25 (pneumoconiosis data) the data in table 10.17 concern the period\nx in years of work at a coalface and the degree of pneumoconiosis in a group of\nminers. the response consists of counts {y1, y2, y3} in k = 3 categories {normal,\n\n "}, {"Page_number": 521, "text": "10.5 \u00b7 count data\n\n509\n\ntable 10.17 period of\nexposure x and prevalence\nof pneumoconiosis\namongst coalminers\n(ashford, 1959).\n\nperiod of exposure (years)\n\n5.8\n\n15\n\n21.5\n\n27.5\n\n33.5\n\n39.5\n\n46\n\n51.5\n\nnormal\npresent\nsevere\n\n98\n0\n0\n\n51\n2\n1\n\n34\n6\n3\n\n35\n5\n8\n\n32\n10\n9\n\n23\n7\n8\n\n12\n6\n10\n\n4\n2\n5\n\nfigure 10.10\npneumoconiosis data\nanalysis. the left panel\nshows how empirical\nlogistic transformations z2\nand z3 depend on\nexposure x. the right\npanel shows how the\nimplied fitted logistic\ndistributions depend on x.\n\nthe vertical lines show(cid:2)\u03b61\nand(cid:2)\u03b62; the areas lying left\nprobabilities(cid:2)\u03c01(x),(cid:2)\u03c02(x),\nand(cid:2)\u03c03(x).\n\nof, between, and right of\nthem equal the fitted\n\n3 3\n2\n2\n\n3\n\n2\n\n3\n2\n\n3\n2\n\nm\nr\no\n\nf\ns\nn\na\nr\nt\n \nc\ni\nt\ns\ng\no\n\ni\n\nl\n \nl\n\na\nc\ni\nr\ni\np\nm\ne\n\n0\n\n2\n-\n\n4\n-\n\n6\n-\n\n3\n2\n\n3\n\n2\n\n3\n2\n\nx= 51.5\n\nx= 46\n\nx= 39.5\n\nx= 33.5\n\nx= 27.5\n\nx= 21.5\n\nx= 15\n\nx= 5.8\n\n10\n\n20\n\n30\n\n40\n\n50\n\n0\n\n5\n\n10\n\n15\n\nexposure x\n\nlinear predictor\n\npresent, severe} assessed radiologically and is qualitative. as the period of exposure\nincreases, the proportion of miners with the disease present or in severe form increases\nsharply.\na simple analysis starts by combining categories, either as {normal or present,\nsevere} or as {normal, present or severe}, towhich models for binomial responses\nmay be fitted. the plot of the empirical logistic transforms\n\n(cid:9)\n\n(cid:10)\n\nz2 = log\n\ny3 + 1\n\n2\n\ny1 + y2 + 1\n\n2\n\nz3 = log\n\n,\n\n(cid:9)\n\n(cid:10)\n\n,\n\ny2 + y3 + 1\n\n2\n\ny1 + 1\n\n2\n\nin the left panel of figure 10.10 shows that the linear predictor should contain log x\nrather than x. the logistic regression model with linear predictor \u03b20 + \u03b21 log x and\nresponse y2 + y3 gives(cid:2)\u03b20 = \u22129.6 and(cid:2)\u03b21 = 2.58. the corresponding model with re-\nsponse y3 yields(cid:2)\u03b20 = \u221210.9 and(cid:2)\u03b21 = 2.69. both models fit well, and the similarity\n\nof the slope estimates suggests that fitting the proportional odds model will be worth-\nwhile.\n\nmaximum likelihood fitting of the proportional odds model with linear predictor\n\n\u03b21 log x gives (cid:2)\u03b21 = 2.60 (0.38), (cid:2)\u03b61 = 9.68 (1.32), and (cid:2)\u03b62 = 10.58 (1.34), entirely\nso the fit seems good. the interpretation of (cid:2)\u03b21 is that every doubling of exposure\nconsistent with the binomial fits. pearson\u2019s statistic is 4.7 on 13 degrees of freedom,\n.= 1.8 and hence the odds of having the\nincreases the linear predictor by 2.6 \u00d7 log 2\ndisease by a factor 6 or so. the same increase applies to the odds of having the\ndisease in severe form. the right panel of figure 10.10 illustrates how the fitted\n(cid:1)\nlogistic distribution implied by the model changes with x.\n\n "}, {"Page_number": 522, "text": "510\n\n10 \u00b7 nonlinear regression models\nsuch models can be broadened by taking an underlying variable x t\u03b2 + \u03c3 \u03b5, with\n\n\u03c3 dependent on explanatory variables.\n\ncontinuation ratio models may be based on the decomposition of the multinomial\n\ndistribution of (y1, . . . ,y k) as\n(cid:14)\n\ny1 \u223c b(n, \u03c01),\nn \u2212 y1,\n\ny2 | y1 \u223c b\n\n(cid:15)\n\n,\n\n\u03c02\n1 \u2212 \u03c01\n\n...\n\nyk\u22121 | y1, . . . ,y k\u22122 \u223c b\n\n(cid:14)\n\nn \u2212 y1 \u2212 \u00b7\u00b7\u00b7 \u2212 yk\u22122,\n\n\u03c0k\u22121\n\n1 \u2212 \u03c01 \u2212 \u00b7\u00b7\u00b7 \u2212\u03c0 k\u22122\n\n(10.32)\n\n(cid:15)\n\n;\n\nof course yk is constant conditional on y1, . . . ,y k\u22121. ateach stage the number of\nindividuals in category l, given the numbers in categories 1, . . . , l \u2212 1, is treated\nas a binomial variable with response probability \u03c0l /(1 \u2212 \u03b3l\u22121), to which a logistic\nregression or other suitable binomial response model may be fitted. thus the original\nk-nomial response is broken into k \u2212 1 separate binomial responses. unlike in the\nproportional odds model there is no necessity that the same explanatory variables be\nused in each of the k \u2212 1 fits, nor that their link functions be the same; this would\ndepend on the scientific context.\n\nexercises 10.5\n1\n\nconsider the 2 \u00d7 n table of independent poisson variables\n\u00b7\u00b7\u00b7 y1n\n\u00b7\u00b7\u00b7 y2n\n\n\u00b7\u00b7\u00b7 y1 j\n\u00b7\u00b7\u00b7 y2 j\n\ny11\ny21\n\n,\n\nwhere\n\nj\n\n2 j\n\n1 j\n\n\u03b2.\n\n\u03b2,\n\n\u03b71 j = log e(y1 j ) = x t\n\n\u03b72 j = log e(y2 j ) = x t\nshow that the conditional density of y1 j given that y1 j + y2 j = m j is binomial with\ndenominator m j and probability \u03c0 j satisfying log{\u03c0 j /(1 \u2212 \u03c0 j )} = x t\n\u03b2, where x j = x1 j \u2212\nx2 j . this implies that a contingency table in which a single, binary, classification is\nregarded as the response can be analyzed using logistic regression. what advantages are\nthere to doing so in terms of model-fitting and the examination of residuals?\nin light of the preceding exercise and the discussion on page 501, reconsider the models\nfitted in example 10.21. say why table 10.13 contains much larger standard errors for\nthe logistic than for the log-linear model.\nfor a 2 \u00d7 2 contingency table with probabilities\n\u03c000 \u03c001\n\u03c010 \u03c011\nthe maximal log-linear model may be written as\n\n\u03b700 = \u03b1 + \u03b2 + \u03b3 + (\u03b2\u03b3 ), \u03b701 = \u03b1 + \u03b2 \u2212 \u03b3 \u2212 (\u03b2\u03b3 ),\n\u03b710 = \u03b1 \u2212 \u03b2 + \u03b3 \u2212 (\u03b2\u03b3 ), \u03b711 = \u03b1 \u2212 \u03b2 \u2212 \u03b3 + (\u03b2\u03b3 ),\n\nwhere \u03b7 jk = log e(y jk) = log(m\u03c0 jk) and m = (cid:13)\n(\u03b2\u03b3 ) may be written (\u03b2\u03b3 ) = 1\n(\u03b2\u03b3 ) = 0 isequivalent to \u0001 = 1.\ngive the matrices needed for iterative weighted least squares for the nonlinear model\n(10.31) in example 10.23. how might starting-values be obtained?\n\nj,k y jk. show that the \u2018interaction\u2019 term\n4 log \u0001, where \u0001 is the odds ratio (\u03c000\u03c011)/(\u03c001\u03c010), so that\n\n,\n\n2\n\n3\n\n4\n\n "}, {"Page_number": 523, "text": "10.6 \u00b7 overdispersion\n\n511\n\n5\n\n6\n\n7\n\nin example 10.24, discuss whether a marginal model or a log-linear model is preferable\nfor (a) a white man aged 43 with a visually impaired left eye, who wants to assess his\nprobability of having visual impairment in the other eye at the age of 65, and (b) a scientist\ncomparing how visual impairment deveops with age for men of different races.\ngive the form of the proportional odds model obtained when an underlying continuous\nvariable x t\u03b2 + exp(x t\u03b3 )\u03b5 is categorized; \u03b5 has the logistic density eu /(1 + eu)2, \u2212\u221e <\nu < \u221e. derive the iterative weighted least squares algorithm for estimation of \u03b2 when it\nis known that \u03b3 = 0. explain how you would need to change your algorithm to deal with\n\u03b3 (cid:3)= 0.\nestablish (10.32).\n\n10.6 overdispersion\n\nthus far we have supposed that our data are well-described by a model with a simple\nerror distribution. nature is not usually so obliging, however, and in practice it is\ncommon to find that count and proportion data are more variable than would be\nexpected under the poisson and binomial models. other types of data may also exhibit\nsuch overdispersion, manifested by models with over-large deviances and residuals,\nbut otherwise showing no systematic lack of fit. structure in the data is obscured by\nadditional noise, so overdispersion increases uncertainty. underdispersion also arises\nbut ismuch rarer.\n\ntwo approaches to dealing with overdispersion are explicit parametric modelling of\nthe heterogeneity, and the use of quasi-likelihood and associated estimating functions.\n\nparametric models\nsuppose that the response y has a standard distribution conditional on the unob-\nserved variable \u03b5, but that \u03b5 induces extra variation in y . here \u03b5 might represent\nunobserved \u2014 perhaps unobserveable \u2014 covariates that affect the response. let \u03b5\nhave unit mean and variance \u03be > 0, and to be concrete suppose that conditional on \u03b5,\ny has the poisson distribution with mean \u00b5\u03b5. then (3.12) and (3.13) give\n\ne(y ) = e\u03b5 {e(y | \u03b5)} ,\n\nvar(y ) = var\u03b5 {e(y | \u03b5)} + e\u03b5 {var(y | \u03b5)} ,\n\nso the response has mean and variance\n\ne(y ) = e\u03b5(\u00b5\u03b5) = \u00b5,\n\nvar(y ) = var\u03b5(\u00b5\u03b5) + e\u03b5(\u00b5\u03b5) = \u00b5(1 + \u03be \u00b5).\n\nif on the other hand the variance of \u03b5 is \u03be /\u00b5, then var(y ) = (1 + \u03be)\u00b5. inboth cases\nthe variance of y is greater than its value under the standard poisson model, for\nwhich \u03be = 0. in the first case the variance function is quadratic, and in the second it is\nlinear.\n\ntable 10.18 illustrates the difference between these variance functions under mod-\nest overdispersion. large amounts of data will be needed to detect overdispersion\nwhen the counts are small. the variances are equal when \u00b5 = 15, but evidently a\nlot of data over a limited range of values of \u00b5 or alternatively a large range of mean\nresponses would be needed to discriminate well between the two variance functions.\nthis is one reason to consider a more robust approach, rather than to model the\n\n "}, {"Page_number": 524, "text": "512\n\n10 \u00b7 nonlinear regression models\n\n\u00b5\nlinear\nquadratic\n\n1\n1.5\n1.0\n\n2\n3.0\n2.1\n\n5\n7.5\n5.8\n\n10\n15.0\n13.3\n\n15\n22.5\n22.5\n\n20\n30\n33\n\n30\n45\n60\n\n40\n60\n93\n\n60\n90\n180\n\noverdispersion in detail. if a full likelihood analysis is desired regardless, one can\nproceed as in the following example.\n\nexample 10.26 (negative binomial model)\nin the discussion above, suppose that \u03b5\nhas the gamma distribution with unit mean and variance 1/\u03bd. then y has the negative\nbinomial density (exercise 10.6.1)\n\ntable 10.18\ncomparison of variance\nfunctions for\noverdispersed count data.\nthe linear and quadratic\nvariance functions are\nvl (\u00b5) = (1 + \u03bel )\u00b5 and\nvq(\u00b5) = \u00b5(1 + \u03beq \u00b5),\nwith \u03bel = 0.5 and \u03beq\nchosen so that\nvl (15) = vq(15).\n\nf (y; \u00b5, \u03bd) = \u0001(y + \u03bd)\n\n\u03bd\u03bd \u00b5y\n\ny = 0, 1, . . . , \u00b5, \u03bd > 0,\n\n,\n\n\u0001(\u03bd)y!\n\n(\u03bd + \u00b5)\u03bd+y\n\n(10.33)\nand quadratic and linear variance functions are obtained on setting \u03bd = 1/\u03be and \u03bd =\n\u00b5/\u03be respectively. the first leads to simpler likelihood equations and so is preferable in\npurely numerical terms. when independent responses y j have associated covariates\nx j , it isnatural to take the log link, giving means \u00b5 j = exp(x t\n\u03b2). the value of \u03be may\nbe estimated from its profile log likelihood or by equating the pearson statistic and\n(cid:1)\nits expected value; see example 10.28.\na similar analysis applies to proportions. suppose that conditional on \u03b5, r = my\nis binomial with denominator m and success probability \u03c0 \u03b5, and that \u03b5 has unit mean\nand variance \u03be. then calculations like those above give\n\nj\n\ne(y ) = \u03c0,\n\nvar(y ) = m\n\n\u22121 {\u03c0(1 \u2212 \u03c0) + \u03be \u03c0 2(m \u2212 1)}.\n\n(10.34)\n\nhence overdispersion increases with m if \u03be is constant. heterogeneity is unde-\ntectable in pure binary data, for which m = 1. when m > 1 and \u03b3 > 0, the choice\n\u03be = \u03b3 (1 \u2212 \u03c0)/{\u03c0(m \u2212 1)} gives var(y ) = (1 + \u03b3 )\u03c0(1 \u2212 \u03c0)/m, corresponding to\nuniform overdispersion. this is explored further in exercise 10.6.4.\n\nquasi-likelihood\nin all but the simplest cases the modelling of overdispersion by integrating out an\nunobserved variable leads to use of numerical integration. this can be awkward, but\na more serious difficulty is that inferences might depend strongly on the unobserved\ncomponent, which can be validated only indirectly. hence it is often preferable to\nmodify standard methods to accommodate overdispersion, in analogy with the use\nof least squares estimation when responses are non-normal (section 8.4). we shall\nsee below that provided the mean and variance functions are correctly specified,\nthe estimators obtained by fitting standard models retain their large-sample normal\ndistributions, but with an inflated variance matrix. this is very convenient, because\nstandard software can then be used for fitting, with minor modification to the output.\nunrecognised overdispersion is a form of model misspecification, so one starting-\npoint is to apply the ideas of section 7.2, treating the generalized linear model score\n\u02dc\u03b2 is obtained\nstatistic (10.18) as an estimating function g(y ; \u03b2) for \u03b2. anestimator\n\n "}, {"Page_number": 525, "text": "x is the n \u00d7 p matrix\nwhose jth row is x t\nj .\n\n10.6 \u00b7 overdispersion\n\n513\n\nby solving the quasi-likelihood equation\n\ng(y ; \u03b2) = x tu(\u03b2) = n(cid:1)\n\nx j u j (\u03b2) = n(cid:1)\nj=1\nwhere the link function gives g(\u00b5 j ) = \u03b7 j = x t\n\u03b2. now if the mean structure has been\nchosen correctly, then e(y j ) = \u00b5 j and the estimating function is unbiased, that is\ne{g(y ; \u03b2)} =0 for all \u03b2. then the quasi-likelihood estimator \u02dc\u03b2 is consistent under\nmild regularity conditions.\n\ng(cid:5)(\u00b5 j )\u03c6 j v (\u00b5 j )\n\ny j \u2212 \u00b5 j\n\n= 0,\n\n(10.35)\n\nj=1\n\nx j\n\nj\n\nin large samples \u02dc\u03b2 is normal with variance matrix (section 7.2.1)\n\n(cid:11)\n\n(cid:12)\u22121\n\n(cid:11)\n\n(cid:12)\u22121\n\ne\n\n\u2212 \u2202g(y ; \u03b2)\n\n\u2202\u03b2 t\n\nvar{g(y ; \u03b2)} e\n\n\u2212 \u2202g(y ; \u03b2)t\n\n\u2202\u03b2\n\n.\n\n(10.36)\n\nin order to compute this we require e{\u2212\u2202g(y ; \u03b2)/\u2202\u03b2 t} and var{g(y ; \u03b2)}. now\n\n\u2202u j (\u03b2)\n\n\u2202\u03b2 t\n\n= \u2202\u03b7 j\n\u2202\u03b2 t\n\n\u2202\u00b5 j\n\u2202\u03b7 j\n1\n\n(cid:11)\n\n\u2202u j (\u03b2)\n\u2202\u00b5 j\n\u2212 g\n\nj\n\n= x t\n\ng(cid:5)(\u00b5 j )\n\n(cid:5)(cid:5)\n(\u00b5 j )\ng(cid:5)(\u00b5 j )\nand as e{u j (\u03b2)} =0, it follows that\n(cid:12)\n= \u2212 n(cid:1)\n= n(cid:1)\n\n\u2212 \u2202g(y ; \u03b2)\n\nj=1\n\n(cid:11)\n\n\u2202\u03b2 t\n\ne\n\nu j (\u03b2) \u2212 v\n\n(cid:5)\n\n(\u00b5 j )\nv (\u00b5 j )\n\nu j (\u03b2) \u2212\n\n1\n\ng(cid:5)(\u00b5 j )\u03c6 j v (\u00b5 j )\n\n(cid:12)\n\n,\n\n(cid:12)\n\n(cid:11)\n\nx j e\n\n\u2202u j (\u03b2)\n\n\u2202\u03b2 t\n\nx j x t\nj\n\nj=1\n\n1\n\ng(cid:5)(\u00b5 j )2\u03c6 j v (\u00b5 j )\n\n= x tw x,\n\nwhere w is the n \u00d7 n diagonal matrix with jth element {g\n(\u00b5 j )2\u03c6 j v (\u00b5 j )}\u22121.\nmoreover if in addition the variance function has been correctly specified, then\nvar(y j ) = \u03c6 j v (\u00b5 j ), and hence\n\n(cid:5)\n\nvar{g(y ; \u03b2)} = x tvar{u(\u03b2)}x = n(cid:1)\n\nx j x t\nj\n\nvar(y j )\n\ng(cid:5)(\u00b5 j )2\u03c62\n\nj v (\u00b5 j )2\n\n= x tw x.\n\nj=1\n\n(cid:5)\n\nx)(x tw x)\n\n\u22121. had the variance function been wrongly spec-\nthus (10.36) equals (x tw x)\n\u22121\n\u02dc\u03b2 would have been of sandwich form (x tw x)\nified, the variance matrix of\n(cid:5)\n\u22121, where w\n(x tw\nis a diagonal matrix involving the true and assumed\nvariance functions. only if the variance function has been chosen very badly will\n\u22121, which therefore provides useful\nthis sandwich matrix differ greatly from (x tw x)\nstandard errors unless a plot of absolute residuals against fitted means is markedly\nnon-random. in that case the choice of variance function should be reconsidered.\nquasi-likelihood estimates and standard errors are easily obtained using software\nthat fits generalized linear models. usually \u03c6 j = a j \u03c6, where the a j are known con-\nstants and \u03c6 = 1 corresponds to a model such as the poisson or binomial, for which\nthe software finds estimates and standard errors by solving (10.35) with \u03c6 = 1. as \u03c6\ncancels from (10.35), the quasi-likelihood estimate \u02dc\u03b2 equals the maximum likelihood\nestimate. software that sets \u03c6 = 1 will yield a variance matrix that is too small by a\n\n "}, {"Page_number": 526, "text": "strictly q(\u03b2; y ) is a\nquasi-log likelihood.\n\n514\n\n10 \u00b7 nonlinear regression models\nfactor \u03c6, however, so the usual standard errors must be multiplied by(cid:2)\u03c61/2, where(cid:2)\u03c6\n\nis defined at (10.20).\n\nunder an exponential family model, the quantity g(y ; \u03b2) in(10.35) is the score\nstatistic, so estimators based upon it are asymptotically optimal. even if that model\nis false, inference based on g(y ; \u03b2) is valid provided the mean and its relation with\nthe variance v (\u00b5) have been correctly specified. moreover the argument on page 322\nshows that \u02dc\u03b2 is optimal among estimators based on linear combinations of the y j \u2212 \u00b5 j ,\nin analogy with the gauss\u2013markov theorem. the essential requirement for this is that\nthe u j (\u03b2) satisfy the two key properties\n\ne(\u2202(cid:6)/\u2202\u00b5) = 0,\n\nvar(\u2202(cid:6)/\u2202\u00b5) = e(\u2212\u2202 2(cid:6)/\u2202\u00b52)\n\nof a log likelihood derivative. in fact, g(y ; \u03b2) isthe derivative with respect to \u03b2 of the\nquasi-likelihood function\n\nq(\u03b2; y ) = n(cid:1)\n\n(cid:29) \u00b5 j\n\nj=1\n\ny j\n\ny j \u2212 u\n\u03c6a j v (u)\n\ndu,\n\nand we can define a deviance as \u22122\u03c6 q(\u03b2; y ). this is positive by construction and\ncan be used to compare nested models under overdispersion.\n\nexample 10.27 (weighted least squares) the simplest example of quasi-likelihood\nestimation arises when v (\u00b5) = 1, \u03c6 j = \u03c6a j , and the mean of y j is \u00b5 j = x t\n\u03b2. then\n(10.35) becomes\n\nj\n\nn(cid:1)\nj=1\n\ny j \u2212 x t\n\u03c6a j\n\nj\n\nx j\n\n\u03b2\n\n= x tw (y \u2212 x\u03b2) = 0,\n\nwhere w is the diagonal matrix \u03c6\u22121diag(1/a1, . . . ,1/ an), and\n\n\u02dc\u03b2 = (x tw x)\n\n\u22121 x tw y\n\n. this estimator is\nis the weighted least squares estimator of \u03b2, found using weights a\nthe maximum likelihood estimator only if the y j are independent and normal, but even\nif not, \u02dc\u03b2 is the minimum variance unbiased estimator linear in the y j (section 8.4).\nintegration shows that the deviance q(\u03b2; y ) equals the weighted sum of squares\n(y \u2212 x\u03b2)tw (y \u2212 x\u03b2), while\n(cid:4)\n\n\u22121\nj\n\n(cid:2)\u03c6 = 1\nn \u2212 p\n\nn(cid:1)\nj=1\n\n\u02dc\u03b2\n\ny j \u2212 x t\na j\n\nj\n\n(cid:5)2\n\n= 1\nn \u2212 p\n\n(y \u2212 x \u02dc\u03b2)tw (y \u2212 x \u02dc\u03b2);\n\nsee section 8.2.4.\n\n(cid:1)\n\nexample 10.28 (cloth fault data) the left panel of figure 10.11 shows the numbers\nof flaws in n = 32 cloth samples of various lengths. a plausible model is that the\na maximum likelihood fit of this model gives (cid:2)\u03b2 = 1.51 with standard error 0.09.\nnumber of faults y in a sample of length x has a poisson distribution with mean \u03b2x.\n\nhowever the deviance of 64.5 on 31 degrees of freedom and the right panel of the\nfigure suggest that the data are more variable than the poisson model might indicate.\n\n "}, {"Page_number": 527, "text": "10.6 \u00b7 overdispersion\n\n515\n\nfigure 10.11 cloth data\nanalysis (bissell, 1972).\nthe left panel shows the\nnumbers of flaws in 32\ncloth samples of various\nlengths (m). the dotted\nline shows the fitted mean\nnumber of faults under the\nmodel. the right panel\nshows that absolute\nresiduals for the fit are\noverdispersed relative to\nthe standard normal\ndistribution appropriate\nunder the poisson\nassumption.\n\ns\nw\na\n\nl\nf\n \nf\n\no\n\n \nr\ne\nb\nm\nu\nn\n\n0\n3\n\n5\n2\n\n0\n2\n\n5\n1\n\n0\n1\n\n5\n\n0\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n4\n\n3\n\n|\n*\nr\n|\n\n2\n\n\u2022\n\n\u2022\n\n\u2022\n\n1\n\n0\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\nlength (m)\n\nlength (m)\n\non reflection this is not surprising, as the rate \u03b2 is likely to vary from one sample to\nanother.\n\nfor quasi-likelihood estimation with var(y ) = \u03c6\u00b5, (10.35) is given by\n\nn(cid:1)\nj=1\n\ny j \u2212 \u00b5 j\n\u03c6\u00b5 j\n\nx j\n\n= 0, \u00b5 j = x j \u03b2,\n\n(cid:13)\n\nand as\n\npoisson model.\n\n(y j \u2212(cid:2)\u00b5 j )2/(cid:2)\u00b5 j = 68.03 on 31 degrees of freedom,(cid:2)\u03c6 = 68.03/31 = 2.19.\nthe standard error for(cid:2)\u03b2 is then 0.09(cid:2)\u03c61/2 = 0.13, appreciably larger than under the\nwhen the negative binomial model with variance function \u00b5(1 + \u03be \u00b5) isfitted, the\n(y j \u2212(cid:2)\u00b5 j )2/{(cid:2)\u00b5 j (1 +\nmaximum likelihood estimates of \u03b2 and \u03be are 1.51 and 0.115 with standard errors\n0.13 and 0.056. the maximized log likelihood is \u221287.73, and\n(cid:2)\u03be(cid:2)\u00b5 j )} =32.57 on 31 degrees of freedom, giving no evidence of poor fit. for the\nalternative negative binomial model with variance function (1 + \u03be)\u00b5, the maximized\nlog likelihood is \u221288.63, so the fit is slightly worse. this is borne out by the right\npanel of figure 10.11, which suggests that the variance of the residuals increases with\nx, as would be the case if the linear variance function was fitted when the quadratic\n(cid:1)\nwas more appropriate.\n\n(cid:13)\n\nthe discussion above shows how standard errors should be modified in the presence\nof overdispersion. a similar adjustment applies when using deviance differences for\nmodel selection. let model a be nested within a more complicated model b, with\ndeviances da > db and parameters pa < pb. for binomial and poisson data, the\nusual approach is to compare da \u2212 db with the \u03c7 2\npb\u2212 pa distribution. in the presence\n(cid:2)\u03c6b is the estimate of \u03c6 under the more complex model, then the adequacy of model\nof overdispersion this is modified by analogy with f tests in linear regression: if\na relative to model b is assessed by referring {(da \u2212 db)/( pb \u2212 pa)}/(cid:2)\u03c6b to the\n\nfpb\u2212 pa,n\u2212 pb distribution.\n\nexample 10.29 (toxoplasmosis data) table 10.19 gives data on the relation be-\ntween rainfall and the proportions of people with toxoplasmosis for 34 cities in\n\n "}, {"Page_number": 528, "text": "table 10.19\ntoxoplamosis data:\nrainfall (mm) and the\nnumbers of people testing\npositive for\ntoxoplasmosis, r, our of m\npeople tested, for 34 cities\nin el salvador (efron,\n1986).\n\ntable 10.20 analysis of\ndeviance for polynomial\nlogistic models fitted to\nthe toxoplasmosis data.\n\nfigure 10.12\ntoxoplasmosis data. the\nleft panel shows the\nproportion of people\ntesting positive, r/m,\nplus/minus\n2{r(m \u2212 r)/m3}1/2, as a\nfunction of rainfall in 34\ncities in el salvador. the\nright panel shows the data\nand linear (solid),\nquadratic (dots, almost\nidentical to the linear fit),\nand cubic (dashes)\npolynomial models fitted\non the logistic scale.\n\n516\n\n10 \u00b7 nonlinear regression models\n\ncity rain\n\nr/m\n\ncity rain\n\nr/m\n\ncity rain\n\nr/m\n\ncity rain\n\nr/m\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n1735\n1936\n2000\n1973\n1750\n1800\n1750\n2077\n1920\n1800\n\n2/4\n3/10\n1/5\n3/10\n2/2\n3/5\n2/8\n7/19\n3/6\n8/10\n\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n2050\n1830\n1650\n2200\n2000\n1770\n1920\n1770\n2240\n1620\n\n7/24\n0/1\n15/30\n4/22\n0/1\n6/11\n0/1\n33/54\n4/9\n5/18\n\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n1756\n1650\n2250\n1796\n1890\n1871\n2063\n2100\n1918\n1834\n\n2/12\n0/1\n8/11\n41/77\n24/51\n7/16\n46/82\n9/13\n23/43\n53/75\n\n31\n32\n33\n34\n\n1780\n1900\n1976\n2292\n\n8/13\n3/10\n1/6\n23/37\n\nterms\n\ndf\n\ndeviance\n\nconstant\nlinear\nquadratic\ncubic\n\n33\n32\n31\n30\n\n74.21\n74.09\n74.09\n62.63\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\ne\nv\ni\nt\ni\ns\no\np\n\n \n\nn\no\n\ni\nt\nr\no\np\no\nr\np\n\n0\n\n.\n\n1\n\n8\n0\n\n.\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\ne\nv\ni\nt\ni\ns\no\np\n\n \n\nn\no\n\ni\nt\nr\no\np\no\nr\np\n\n0\n\n.\n\n1\n\n8\n0\n\n.\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\u2022\n\n\u2022\u2022\n\u2022\n\n\u2022 \u2022\n\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\u2022\n\u2022\n\n\u2022 \u2022\n\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n1600 1800 2000 2200 2400\n\n1600 1800 2000 2200 2400\n\nrainfall (mm)\n\nrainfall (mm)\n\nel salvador. there is wide variation in the numbers tested, as well as in the propor-\ntions testing positive, and the left panel of figure 10.12 indicates a possible nonlinear\nrelation between rainfall and toxoplasmosis incidence.\n\nthe right panel shows fitted proportions for logistic regression models in which the\nlinear predictor contains terms linear, quadratic, and cubic in rainfall. table 10.20 con-\ntains the analysis of deviance when the polynomial terms are included successively.\nthe residual deviance of 62.63 on 30 degrees of freedom indicates overdispersion by\na factor of roughly two.\nunder the binomial assumption, the cubic model is tested against the constant\nmodel by comparing the deviance difference 74.21 \u2212 62.63 = 11.58 with the \u03c7 2\n\n3\n\n "}, {"Page_number": 529, "text": "10.6 \u00b7 overdispersion\n\n517\n\ndistribution, giving significance level 0.009. this overstates the significance of the\ntest because it makes no allowance for overdispersion. under quasi-likelihood with\nvar(r) = \u03c6m\u03c0(1 \u2212 \u03c0) weobtain \u02dc\u03c6 = 1.94, and our general discussion suggests that\nwe should compare the f statistic (11.58/3)/ \u02dc\u03c6 = 1.99 with the f3,30 distribution.\nthis gives significance level 0.14, only weak evidence of a relationship between\n(cid:1)\nrainfall and incidence. we return to these data in example 10.32.\n\nif the responses are dependent, the above discussion can be extended by taking as\n\u22121(y \u2212 \u00b5), where v (\u00b5) is ann \u00d7 n covariance matrix\nestimating function x tv (\u00b5)\nfor y ; see page 507. this is a common technique for modelling longitudinal data,\nin which short, often irregular time series are available on independent individuals.\nin such cases there may be no function whose derivatives with respect to \u03b2 give the\nestimating function, and then no quasi-likelihood exists.\nin some cases the response variance may be expressed as \u03c6(\u03b3 )v (\u00b5; \u03be), with \u03b3 and \u03be\nunknown. an example is the quadratic variance function \u00b5 + \u03be \u00b52 in example 10.26.\nthe definition of the deviance depends on \u03be, somodels with different values of \u03be\ncannot be compared using differences of deviances. an extended quasi-likelihood\ncan be defined as the sum of the contributions\nlog{\u03c6 j (\u03b3 )v (\u00b5 j ; \u03be)} \u2212 1\n2\n\ny j \u2212 u\n\n\u03c6 j (\u03b3 )a j v (u; \u03be)\n\n(cid:29) \u00b5 j\n\n\u2212 1\n2\n\ndu,\n\ny j\n\nhowever, and used for inference about the unknown parameters. unfortunately this\ndefinition is ambiguous: for example \u00b5 + \u03be \u00b52 can be written as \u03c6(\u03b3 ) = 1, v (\u00b5; \u03be) =\n\u00b5 + \u03be \u00b52 or as \u03c6(\u03b3 ) = \u00b5, v (\u00b5; \u03be) = 1 + \u03be \u00b5, and these give different extended quasi-\nlikelihoods. uniqueness can be imposed by insisting that \u03c6(\u03b3 ) not involve \u00b5 or that\nv (\u00b5) = 1, leading to two different systems of estimating equations. the first system\ngives inconsistent estimators and the second gives consistent estimators. however\nsimulation shows that for sample sizes of most interest the second estimators are\nworse than the first. thus in practice the solutions to the first system are preferable,\nthough neither is really satisfactory.\n\nexercises 10.6\n1\n\nuse (2.8) to establish (10.33). give formulae for the corresponding deviance residuals\nwhen \u03bd = 1/\u03be and when \u03bd = \u00b5/\u03be.\nsuppose that independent counts y1, . . . , yn arise with means \u00b5 j = exp(x t\n\u03b2). under the\nmodel with constant \u03bd = 1/\u03be, write down the negative binomial log likelihood for \u03b2 and\n\u03be. explain why the likelihood equations become more complicated if the shape parameter\nchanges for each observation, so \u03bd j = \u00b5 j /\u03be.\n(y j \u2212(cid:2)\u00b5 j )2/v ((cid:2)\u00b5 j ) ton \u2212 p, where\nif we estimate \u03be by equating the pearson statistic\nv (\u00b5 j ) = var(y j ), discuss how to obtain the estimate under the above two variance\nfunctions.\nlet i be a binary variable with success probability \u03c0, and suppose that \u03c0 is given a density\nh. show that i remains a binary variable whatever the choice of h, and hence explain the\nform of the variance in (10.34).\nagainst what variable should the squared pearson residual be plotted if it is desired to\nassess if (10.34) gives a suitable fit to data?\n\n(cid:13)\n\nj\n\n2\n\n "}, {"Page_number": 530, "text": "\u0001(a) is the gamma\nfunction.\n\n518\n\n3\n\n4\n\n5\n\n10 \u00b7 nonlinear regression models\n(\u00b5 j )v (\u00b5 j )} and v (\u00b5) equals \u00b5, \u00b5(1 \u2212 \u00b5),\n\nfind q(\u03b2; y ) when u j (\u03b2) = (y j \u2212 \u00b5 j )/{\u03c6g\nand \u00b52.\none standard model for over-dispersed binomial data assumes that r is binomial with\ndenominator m and probability \u03c0, where \u03c0 has the beta density\n\n(cid:5)\n\nf (\u03c0; a, b) = \u0001(a + b)\n\n\u0001(a)\u0001(b)\n\n\u03c0 a\u22121(1 \u2212 \u03c0)b\u22121,\n\n0 < \u03c0 <1, a, b > 0.\n\n(a) show that this yields the beta-binomial density\n\npr(r = r; a, b) = \u0001(m + 1)\u0001(r + a)\u0001(m \u2212 r + b)\u0001(a + b)\n\u0001(r + 1)\u0001(m \u2212 r + 1)\u0001(a)\u0001(b)\u0001(m + a + b)\n\nr = 0, . . . ,m .\n\n,\n\n(b) let \u00b5 and \u03c3 2 denote the mean and variance of \u03c0. show that in general,\n\ne(r) = m\u00b5,\n\nvar(r) = m\u00b5(1 \u2212 \u00b5) + m(m \u2212 1)\u03c3 2,\n\nvar(r) = m\u00b5(1 \u2212 \u00b5){1 + (m \u2212 1)\u03b4},\n\nand that the beta density has \u00b5 = a/(a + b) and s2 = ab/{(a + b)(a + b + 1)}. deduce\nthat the beta-binomial density has mean and variance\ne(r) = ma/(a + b),\n\u22121.\nhence re-express pr(r = r; a, b) as afunction of \u00b5 and \u03b4. what is the condition for\nuniform overdispersion?\nconditional on \u03b5, the observation y has a generalized linear model density with canonical\nparameter \u03b7 + \u03c4 \u03b5, where \u03c4 > 0. if \u03b5 is standard normal, show that the marginal density\nof y can be written\nf (y; \u03b7, \u03c4 ) =\n\ny\u03b7 + y\u03c4 \u03b5 \u2212 b(\u03b7 + \u03c4 \u03b5)\n\n+ c(y; \u03c6) \u2212 \u03b52/2\n\n\u03b4 = (a + b + 1)\n\n(cid:29) \u221e\n\nexp\n\n(cid:12)\n\n(cid:11)\n\nd\u03b5.\n\n1\n\nby second-order taylor series expansion of b(\u03b7 + \u03c4 \u03b5) for small \u03c4 , orotherwise, show that\nf (y; \u03b7, \u03c4 ) equals\n\n(2\u03c0)1/2\n\n\u2212\u221e\n\na(\u03c6)\n\n(cid:23)\n\n(cid:22)\n\n\u03c4 2\n2\n\n{y \u2212 b\n\n(cid:5)\n\n(\u03b7)}2\n\na(\u03c6)2{1 + \u03c4 2b(cid:5)(cid:5)(\u03b7)/a(\u03c6)}\n\nf (y; \u03b7, 0) exp\n\n{1 + \u03c4 2b\n\n(cid:5)(cid:5)\n\n(\u03b7)/a(\u03c6)}\u22121/2 + o p(\u03c4 2).\n\nprove that this approximation is exact when the conditional density of y given \u03b5 is normal,\nand then find the unconditional mean and variance of y .\n\n10.7 semiparametric regression\n\nour earlier regression models have involved responses that depend on explanatory\nvariables x through simple parametric functions such as \u03b20 + \u03b21x + \u03b22x 2. their con-\nciseness and direct interpretation gives such formulations great appeal, but they are\nnot flexible enough to cater for all the situations met in practice and more general\napproaches are desirable, especially for exploratory analysis, model-checking, and\nother situations where the data should be allowed to \u2018speak for themselves\u2019. many\nways to do this have been proposed in recent years, under the heading of nonparamet-\nric or semiparametric models, the aim typically being to extract a smooth curve from\nthe data. an algorithm that does this is often termed a smoother. in fact smoothing the adjective \u2018smoother\u2019\nhas become a noun in this\noperations typically do involve parameters, but in less prescriptive ways than before,\ncontext.\nand the results are best understood graphically. there are many approaches to semi-\nparametric modelling, and below we merely sketch the possibilities by extending our\nprevious discussion in two directions.\n\n "}, {"Page_number": 531, "text": "10.7 \u00b7 semiparametric regression\n\nfigure 10.13\nearthquake magnitudes\nplotted against fitted\nintensity just before the\nearthquake shock and time\nsince the preceding shock.\nnote the log scales. the\nmagnitudes have been\njittered to reduce\noverplotting.\n\ne\nd\nu\n\nt\ni\n\nn\ng\na\nm\n\n0\n\n.\n\n9\n\n5\n8\n\n.\n\n0\n\n.\n\n8\n\n5\n\n.\n\n7\n\n0\n\n.\n\n7\n\n5\n\n.\n\n6\n\n0\n\n.\n\n6\n\n.\n\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.. .\n.\n.\n.\n.\n...\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n...\n.\n..\n.\n.\n.\n.\n.\n.\n.\n...\n.\n...\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n. .\n..\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n..\n.\n.\n.\n.\n.\n..\n.\n..\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n..\n.\n.\n.\n..\n. ..\n.\n..\n.\n..\n..\n.\n..\n.\n..\n.\n.\n.\n..\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n..\n.....\n.\n.\n.\n.\n..\n.\n.\n.\n.\n..\n.\n..\n.\n. .\n.\n.\n.\n.\n.\n.\n.. .\n.\n.\n..\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n..\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n. ...\n......\n..\n...\n.\n.\n.\n.\n.\n.\n... .\n.\n..\n..\n..\n.\n.\n.\n..\n.\n.\n..\n.\n.\n.\n..\n.\n..\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n..\n\n.\n\n.\n\n..\n.\n.\n.\n\n0.005\n\n0.050\n\n0.500\n\n5.000\n\ne\nd\nu\n\nt\ni\n\nn\ng\na\nm\n\n0\n\n.\n\n9\n\n5\n8\n\n.\n\n0\n\n.\n\n8\n\n5\n\n.\n\n7\n\n0\n\n.\n\n7\n\n5\n\n.\n\n6\n\n0\n\n.\n\n6\n\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n\n1\n\n519\n\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n.\n\n.\n.\n\n.\n\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n..\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n..\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n..\n. .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n\n.\n\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n\n.\n.\n.\n\n5 10\n\n50\n\n500\n\nintensity just before quake (1/days)\n\ntime since last quake (days)\n\nexample 10.30 (japanese earthquake data) figure 6.19 shows data on 483 earth-\nquake shocks, of magnitude at least 6 on the richter scale, offshore from japan from\n1885\u20131980. in example 6.38 a self-exciting point process model was fitted to the\ndata, in which the intensity at time t was given by\n\n\u03bbh(t) = \u00b5 + \u03ba\n\n(cid:1)\n\nj:t j <t\n\ne\u03b2(m j\u22126)\nt \u2212 t j + \u03b3\n\n,\n\nwhere m j is the magnitude and t j the time of the jth earthquake. this fits the times\nadequately, but \u03bbh(t) models only the time of the next shock and not its its magnitude.\nit is natural to ask if the magnitude of a shock depends on the past, for example on the\nvalue of \u03bbh(t) just before the shock occurs, or on the time elapsed since the previous\n(cid:2)\u03bbh(ti \u2212 \u03b4) and against\nshock.\nfigure 10.13 contains scatterplots of mi against lim\u03b4\u21920\nti \u2212 ti\u22121. the lack of pattern in both panels suggests that magnitude is unrelated to\nthese other quantities, but the clustering of points at the left of the left panel makes it\nhard to be sure, and a smooth curve would sharpen our judgement. fitting a particular\nparametric model seems difficult to justify, so the curve should be defined flexibly. (cid:1)\n\n10.7.1 local polynomial models\nsuppose that the response y equals g(x) + \u03b5, where g is a smooth function of the\nscalar x, and \u03b5 has mean zero and variance \u03c3 2. although we assume that g has\nas many derivatives as we need, it will not usually have a simple form. the data\nconsist of pairs (x1, y1), . . . ,( xn, yn), and initially we shall suppose that we wish to\nmake inferences about g(x) at asingle point x = x0 in the interval spanned by the\ndesign points x j . one approach is to fit a polynomial to all n pairs and then read off\nits value at x0. however, such fits are often unconvincing \u2014 see the right panel of\nfigure 10.12, for example \u2014 and furthermore they can be sensitive to observations\ndistant from x0. rather than treat all data pairs equally, it seems natural to attach\nmore importance to observations close to x0. this is closely related to kernel density\nestimation (section 7.1.2).\n\n "}, {"Page_number": 532, "text": "figure 10.14\nconstruction of a local\nlinear smoother. left\npanel: observations in the\nshaded part of the panel\nare weighted using the\nkernel shown at the foot,\nwith h = 0.8, and the\nsolid straight line is fitted\nby weighted least squares.\nthe local estimate is the\nfitted value when x = x0,\nshown by the vertical line.\ntwo hundred local\nestimates formed using\nequi-spaced x0 were\ninterpolated to give the\ndotted line, which is the\nestimate of g(x). right\npanel: local linear\nsmoothers with h = 0.2\n(solid) and h = 5 (dots).\n\n520\n\n2\n1\n\n0\n1\n\n8\n\n10 \u00b7 nonlinear regression models\n\n2\n1\n\n0\n1\n\n8\n\ny\n\n6\n\ny\n\n6\n\n4\n\n2\n\n0\n\n4\n\n2\n\n0\n\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\nx\n\nx\n\nrecall that a kernel function w(u) is aunimodal density function symmetric about\nu = 0 and with unit variance. one choice of w is the standard normal density. another\nis a rescaled form of the tricube function\n\n(cid:11)\n\nw(u) =\n\n(1 \u2212 |u|3)3,\n0,\n\n|u| \u22641,\notherwise,\n\n(10.37)\n\n\u22121w{h\n\n\u22121(x j \u2212 x0)} to\n\nand there are many others.\nwhen estimating g(x) at x = x0, weattach weight w j = h\n(x j , y j ), where h is a bandwidth, and fit the polynomial\n\n\u03b20 + \u03b21(x \u2212 x0) + \u00b7\u00b7\u00b7 +\u03b2 k(x \u2212 x0)k\n\nto the responses by weighted least squares (section 8.2.4). the weights decrease the\ninfluence of points for which |x j \u2212 x0|/ h is big: for large h points far from x0 will\naffect the fit, whereas as h \u2192 0 the regression becomes ever more local. the number\nof data included will vary as x0 changes, with fewer points when x0 is near the limits\nof its range or where the x j are sparse.\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8 =\n\n\uf8f6\nthe estimate of g(x0) isobtained by fitting the linear model\n\uf8f7\uf8f7\uf8f8 +\n\u03b20\n\u03b21\n...\n\u03b2k\n\n\uf8eb\n\uf8ed 1\n...\n1 (xn \u2212 x0)\n\n(xn \u2212 x0)k\n\n(x1 \u2212 x0)k\n\n(x1 \u2212 x0)\n\ny1\ny2\n...\nyn\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\n\uf8f6\n\uf8f8\n\n\u00b7\u00b7\u00b7\n\n\u00b7\u00b7\u00b7\n\n...\n\n...\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\n\u03b51\n\u03b52\n...\n\u03b5n\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8 ,\n\nthat is y = x\u03b2 + \u03b5, with weight matrix w = diag(w1, . . . ,w n). this results in the\nweighted least squares estimate(cid:2)\u03b2 = (x tw x)\n\u22121 x tw y, ofwhich the component of\ninterest is(cid:2)\u03b20, the value of the fitted polynomial when x = x0.\nin practice a smooth estimate(cid:2)g(x) ofthe entire function g(x) isusually required.\nit is obtained by interpolating the estimates(cid:2)\u03b20 for different values of x0, as inthe left\nwhen h is large,(cid:2)g is too smooth to capture the pattern of the data, and hence it will\nbe biased. when h is small,(cid:2)g follows the data better but wiggles implausibly and\n\npanel of figure 10.14. the right panel shows how the curve estimate depends on h.\n\nso has a high variance. the intermediate estimate shown in the left panel balances\n\n "}, {"Page_number": 533, "text": "10.7 \u00b7 semiparametric regression\n\n521\n\nbias and variance more satisfactorily. the choice of h is important and we discuss it\nbelow.\n\nthe dips in(cid:2)g towards the right of both panels suggest that local polynomial fits\n\nare sensitive to outliers. this is no surprise because they involve least squares estima-\ntion, which is non-robust. some implementations robustify the fit, for example using\nthe huber estimator or discounting outliers by making the weights depend on the\n\nresiduals y j \u2212(cid:2)g(x j ) from an initial fit. the popular iterative algorithm lowess takes\nthis approach, though it uses nearest neighbours, weighting the proportion p of the\ndata nearest to x0 instead of using a fixed bandwidth. typically p = 2/3 instandard\nimplementations. nearest neighbour fitting automatically allows for changes in the\ndensity of the x j and hence reduces fluctuations like those at x = 2 inthe right panel\nof figure 10.14; it amounts to taking a locally-varying bandwidth.\n\nlocal polynomial estimators can be studied using ideas from least squares. the\n\nlowess stands for locally\nweighted scatterplot\nsmoother.\n\nestimator of g(x0) is aweighted average of the y j , that is\n\n(cid:2)g(x0) =(cid:2)\u03b20 = n(cid:1)\n\nj=1\n\ns(x0; x j , h)y j ,\n\nwhere the elements of the effective kernel s(x0; x1, h), . . . , s(x0; xn, h) form the first\nrow of the (k + 1) \u00d7 n matrix (x tw x)\n\u22121 x tw . this depends on the design matrix,\nthe bandwidth, and x0, butnot on y, so\ne{(cid:2)g(x0)} = n(cid:1)\nthe second expression here gives a finite-sample variance for(cid:2)g(x0), provided that \u03c3 2\n\nvar{(cid:2)g(x0)} = \u03c3 2\n\ns(x0; x j , h)g(x j ),\n\ns(x0; x j , h)2.\n\nn(cid:1)\nj=1\n\n(10.38)\n\nj=1\n\nis replaced by an estimate. one natural choice is\n\ns2(h) =\n\n1\n\nn \u2212 2\u03bd1 + \u03bd2\n\nn(cid:1)\nj=1\n\n{y j \u2212(cid:2)g(x j )}2,\n\n(10.39)\n\nwhere \u03bd1 and \u03bd2 are defined below. the corresponding estimator is unbiased when g\nis a polynomial of degree k, but otherwise is biased upwards. a simple way to reduce\nthe bias is to construct s2(h) using a smaller h than that used for the curve estimate.\nfor theoretical and practical purposes it is fruitful to represent the smoothing opera-\n\ntion in matrix form. the fitted values(cid:2)g(xi ) may be obtained by setting x0 = x1, . . . , xn,\nand to each of these corresponds an effective kernel si1(h), . . . , sin(h), where si j (h)\nis an abbreviation of s(xi ; x j , h). let us stack these as an n \u00d7 n smoothing matrix sh.\nthen the vector of fitted values(cid:2)g = ((cid:2)g(x1), . . . ,(cid:2)g(xn))t may be written as(cid:2)g = sh y.\nthere is an analogy with the hat matrix h = x(x t x)\n\u22121 x t for a linear regression with\nfitted values(cid:2)y = h y. unlike a hat matrix, however, sh is not idempotent and it may\nbe asymmetric. any sensible smoothing operation will leave a constant unchanged,\nso sh1n = 1n. equivalently any effective kernel sums to one.\nalthough kernel smoothers are most conveniently defined in terms of their band-\nwidth, it is useful to know their degrees of freedom when comparing their output.\n\n "}, {"Page_number": 534, "text": "522\n\n10 \u00b7 nonlinear regression models\n\nin a standard linear model the trace of the hat matrix equals the number of param-\neters, suggesting by analogy that we define the degrees of freedom of the smoother\nto be \u03bd1 = tr(sh). the analogy is not perfect, however, and alternatives such as\n\u03bd2 = tr(st\nh sh) have been proposed. both \u03bd1 and \u03bd2 decrease as h increases, and\nk \u2264 \u03bd2 \u2264 \u03bd1 \u2264 n.\nthree things must be chosen when implementing a local polynomial fit: the kernel\nfunction w, the degree k of the polynomial, and the bandwidth h. experience shows\nthat the choice of w is rarely important, though the best-looking curves are obtained\nwhen w descends smoothly to zero. sharp-edged kernels such as the uniform density\ngive rough and visually unappealing fits.\n\nchoice of polynomial\nit turns out to be sensible to take k odd, at least in theory. to see this in qualitative\nterms, consider the simplest form of local smoothing, with k = 0. then the local\npolynomial is a constant and the least squares estimator of g(x0) is aweighted average,\nthe nadaraya\u2013watson estimator\n\n(cid:2)\u03b20,n w =\n\n(cid:13)\n(cid:13)\nn\nj=1 w j y j\nn\nj=1 w j\n\n.\n\n(10.40)\n\nas(cid:2)\u03b20,n w does not allow for g\n\nthis is a simple estimator, but not a very good one, because of its bias at the ends of the\ndata. figure 10.15 shows what happens when local constant and local linear estimators\nare fitted to data, shown noiseless for clarity. the left panels show that local linear\nand local constant fits give similar estimates when x0 is central; the effective kernel\nis almost the same. in the right panels x0 lies at the right-hand edge of the data, and\n(x0) it isbadly biased upwards. the effective kernel for\nthe local linear fit is smaller away from the boundary values of x j , however, and so its\nbias is smaller. the fact that local polynomial fits adapt automatically to the presence\nof a boundary gives them a large practical advantage over many other approaches.\nan asymptotic argument given at the end of this section, under which n \u2192 \u221e\nand h \u2192 0 with nh \u2192 \u221e, shows that when x0 is away from a boundary, both local\nconstant and local linear fits have approximate bias and variance\n\n(cid:5)\n\n(cid:29)\n\n(cid:5)(cid:5)\n\nh2g\n\n(x0),\n\n1\n2\n\n\u03c3 2\n\nnh f (x0)\n\nw (u)2 du,\n\n(10.41)\n\nwhere f (x) represents the limiting density of design points. the bias increases if\ng(x) has high curvature or if h is large; see figure 10.14. the bias has the desirable\nproperty of not depending on the pattern of design points, at least asymptotically. one\ncontribute to the weighted average(cid:2)\u03b20 is nh f (x0)/\ninterpretation of the variance formula is that the effective number of observations that\nw(u)2 du, and this explains why\nwe must have nh \u2192 \u221e for a sensible asymptotic framework, while a small value of\nf (x0) \u2014that is, few points near x0 \u2014 decreases the precision with which g(x0) may\nbe estimated.\n\n(cid:24)\n\nwhen x0 is near a boundary it may be shown that the bias of the local constant\nestimator increases to o(h). the local linear estimator retains its o(h2) bias, however,\n\ngeoffrey stuart watson\n(1921\u20131998) was\neducated in melbourne,\nnorth carolina and\ncambridge, and held posts\nin australia and north\namerica, the last being at\nprinceton. he made\nimportant contributions to\ntime series, to directional\ndata analysis, and to\nmathematical biology. see\nberan and fisher (1998).\ne. a. nadaraya (1935\u2013)\nworked at tbilisi state\nuniversity. both men\npublished papers\ndescribing this estimator\nin 1964.\n\n "}, {"Page_number": 535, "text": "figure 10.15 local\npolynomial fitting by least\nsquares. in each panel the\nfunction g(x) is shown by\na line joining the solid\nblobs (x j , y j ), shown\nwithout error for clarity,\nand the target value x0 at\nwhich g is to be estimated\nis given by the vertical\nline; x0 = 1.5 for the left\npanels and x0 = 2 for the\nright panels. only\nobservations falling inside\nthe shaded region\ncontribute to the fit, and\nthe effective kernel is\nshown by the circles; in\nthe right panels the\neffective kernel has been\nshifted upwards by 0.8.\nthe heavy solid lines\nshow the local polynomial\nfits, which are constant in\nthe upper panels and\nlinear in the lower panels.\nthe local constant fit is\nmore biased than the local\nlinear fit, especially at the\nedge x0 = 2.\n\n10.7 \u00b7 semiparametric regression\n\n523\n\n.\n\n1\n\n0 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\n\n.\n\n1\n\n0 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\n\n)\nx\n(\ng\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n\u2022\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\n\n)\nx\n(\ng\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n\u2022\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\n\n1.0 1.2 1.4 1.6 1.8 2.0\n\n1.0 1.2 1.4 1.6 1.8 2.0\n\nx\n\n\u2022\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\n\n.\n\n1\n\n0 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\n\n)\nx\n(\ng\n\n8\n0\n\n.\n\n6\n0\n\n.\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\nx\n\n\u2022\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\n\n.\n\n1\n\n0 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\n\n)\nx\n(\ng\n\n8\n0\n\n.\n\n6\n0\n\n.\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n1.0 1.2 1.4 1.6 1.8 2.0\n\n1.0 1.2 1.4 1.6 1.8 2.0\n\nx\n\nx\n\nconfirming what is suggested by figure 10.15. near a boundary both variances remain\n\u22121. thus in asymptotic terms the variance of the local linear estimator is\nof order (nh)\nthe same argument applies in general: whenever k is even, the variance of(cid:2)\u03b20 is not\nno worse than that of the local constant estimator, while its bias is of smaller order.\nincreased asymptotically by fitting a polynomial of order k + 1, and the bias of(cid:2)\u03b20 is\n\nreduced thereby.\n\nasymptotic arguments are useful backstops, but the theoretical benefits of higher-\norder fitting are outweighed by a finite-sample increase in variance. in practice local\nlinear and quadratic polynomials are commonest, but local cubic curves are also\nsometimes fitted, particularly to data showing high-frequency variation.\n\nchoice of smoothing parameter\none possibility is to choose the bandwidth h to minimize the asymptotic mean squared\nerror\n\n(cid:5)(cid:5)\n\n(x0)2 + \u03c3 2\n\ng\n\nh4\n4\n\nw(u)2 du\n\nof(cid:2)g(x0) with respect to h, giving a local bandwidth at x0. anoverall bandwidth could\n\nnh f (x0)\n\nbe found using the integrated asymptotic mean squared error; see (7.7) and (7.8).\nunfortunately both local and overall choices of h involve the unknowns g\nand f ,\n\n(cid:5)(cid:5)\n\n(cid:29)\n\n "}, {"Page_number": 536, "text": "524\n\n10 \u00b7 nonlinear regression models\n\nand further bandwidths are needed to estimate them. a quagmire stands before us,\nand we must skirt it if we can.\n\na better, finite-sample, approach is to trade off the bias and variance when each\nobservation is predicted from the rest, choosing h to minimize the cross-validation\nsum of squares\n\nwhere(cid:2)g\u2212 j (x) isobtained by applying the smoother with bandwidth h but dropping\n\nthe jth case from the data. at first sight it appears that n fits are needed to compute\ncv(h), but as (exercise 10.7.5)\n\n(10.42)\n\nit turns out that\n\ncv(h) = n(cid:1)\n\nj=1\n\n{y j \u2212(cid:2)g\u2212 j (x j )}2,\n\n,\n\ny j \u2212(cid:2)g\u2212 j (x j ) = y j \u2212(cid:2)g(x j )\n1 \u2212 s j j (h)\n(cid:12)2\n\ncv(h) = n(cid:1)\n\ny j \u2212(cid:2)g(x j )\n1 \u2212 s j j (h)\n\n(cid:11)\n\nj=1\n\nmay be computed from components of the overall fit.\n\nin practice cv(h) must be minimized over a grid of values of h, so fast computation\nof the s j j (h) isessential. an alternative is to minimize the generalized cross-validation\ncriterion\n\ngcv(h) = n(cid:1)\n\n(cid:11)\n\nj=1\n\ny j \u2212(cid:2)g(x j )\n1 \u2212 tr(sh)/n\n\n(cid:12)2\n\ninstead. this replaces the individual s j j (h) bytheir average, which is found more\nspeedily.\n\ncross-validation and information criteria are closely related, so it is not surprising\nthat the values of h found by minimizing cv(h) and gcv(h) tend to be too small,\nanalogous to the overfitting seen when aic is used. in section 8.7.3 we saw that for\nlinear models this may be remedied by using the corrected information criterion aicc.\nfor semiparametric models this suggests that we choose h to minimize aicc(h), given\nby\n\naicc(h) = n log(cid:2)\u03c3 2(h) + n\n\n1 + tr(sh)/n\n\n1 \u2212 {tr(sh) + 2}/n\n\n, (cid:2)\u03c3 2(h) = n\n\n\u22121\n\n(cid:1)\n\n{y j \u2212(cid:2)g(x j )}2,\n\nwhere the number of regressors p in the linear model is replaced by tr(sh). simulation\nshows that this procedure reduces the overfitting.\n\nsemiparametric versions of other criteria also exist, but in practice an automatic\nchoice of h should typically be used as a starting-point for investigation, rather than\nas a black box procedure.\n\n "}, {"Page_number": 537, "text": "10.7 \u00b7 semiparametric regression\n\n525\n\ninference\nat first sight it seems that approximate pointwise confidence intervals for g(x0) can\nbe constructed using the mean and variance (10.38), simply by supposing that\n\n(cid:2)g(x0) \u2212 e{(cid:2)g(x0)}\n!var{(cid:2)g(x0)}1/2\n\n.\u223c n (0, 1).\n\nconfidence interval may be centred in quite the wrong place. one way to deal with\nthis is to correct the interval using a bias estimate, for example taken from (10.41)\n\nunfortunately, however, this leads to confidence statements for e{(cid:2)g(x0)} rather than\nfor the usual quantity of interest g(x0). if the bias e{(cid:2)g(x0)} \u2212 g(x0) issubstantial, the\nor found by comparing (cid:2)g(x0) and a less biased estimate obtained using a smaller\nbandwidth or higher k. these solutions tend to be complicated and can work poorly\nin small samples. a simpler approach ignores the bias issue and constructs a (1 \u2212 2\u03b1)\nvariability band, inwhich \u00b1z\u03b1!var{(cid:2)g(x0)}1/2 is added to(cid:2)g(x0) to give anidea of its\nvariability. in effect this is a confidence band for e{(cid:2)g(x0)}.\nsometimes it is useful to construct an overall (1 \u2212 2\u03b1) confidence band for g over\nthe set a of x-values. this requires two curves l(x) and u (x) such that\n\npr{l(x) \u2264 g(x) \u2264 u (x), x \u2208 a} = 1 \u2212 2\u03b1,\n\nwhich may be found by probability approximation to the distribution of\n\nsupx\u2208a |(cid:2)g(x) \u2212 g(x)|/!var{(cid:2)g(x)}1/2. references to this are given in the bibliographic\n\nnotes.\n\nit is useful to have a means of testing the overall significance of an apparent\ndeparture from a given parametric form. suppose that a non-local linear model has\n\nyielded the fitted values(cid:2)y = h y, where h is the corresponding hat matrix, and that\n\na more comprehensive, smooth, model has smoothing matrix sh. then it is natural\nto compute a p-value for departures from the non-local model using the ratio of the\ncorresponding residual sums of squares\n\n(cid:13)\n(cid:13)\n\nn\n\nj=1(y j \u2212(cid:2)y j )2\n{y j \u2212(cid:2)g(x j )}2\n\nn\nj=1\n\nr(h) =\n\n=\n\nyt(in \u2212 h)y\n\nyt(in \u2212 sh)t(in \u2212 sh)y\n\n.\n\nif the observations are normal it turns out to be possible to compute good approxi-\nmations to the corresponding p-value\n\npobs(h) = pr0 {r(h) \u2265 robs} ,\n\n(10.43)\n\nwhere robs is the observed value of r(h) (problem 10.13). it is useful to plot a signif-\nicance trace of pobs(h) as afunction of h, toassess the strength of evidence against\nthe parametric model at different bandwidths.\n\nexample 10.31 (japanese earthquake data) the left panel of figure 10.13 sug-\ngests that the magnitude of an earthquake might depend on the conditional intensity\n\n(cid:2)\u03bbh(t) just before it, with a larger release of energy and hence a bigger shock when\nthe(cid:2)\u03bbh(t) issmall. to assess this we fit a local linear smoother with tricube kernel\n\n "}, {"Page_number": 538, "text": "526\n\n0\n9\n\n.\n\n0\n\n.\n\n8\n\n0\n7\n\n.\n\n0\n6\n\n.\n\n0\n2\n\n5\n1\n\n0\n1\n\n5\n\ne\nd\nu\n\nt\ni\n\nn\ng\na\nm\n\nm\no\nd\ne\ne\nr\nf\n \nf\n\no\n\n \ns\ne\ne\nr\ng\ne\nd\n\n.\n\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n...\n.\n.\n.. .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.....\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n...\n.\n.\n.\n.\n..\n.\n.\n..\n.\n. .\n.\n.\n..\n.\n.\n.\n.\n..\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n..\n.\n.\n..\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n..\n..\n...\n.\n.\n..\n..\n.\n.\n.\n.\n..\n.\n.\n..\n.\n.\n.\n.\n.\n.\n..\n..\n.\n.\n.\n.\n.\n.\n.. .\n.\n.\n.\n.\n....\n...\n..\n.\n.\n.\n...\n.\n.\n..\n..\n. .\n.\n.\n.\n..\n.\n..\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n......\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n..\n.\n.\n.\n.\n..\n.\n.\n.\n..\n..\n..\n..\n.\n..\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n..\n.\n.\n...\n.\n.\n.\n.\n.\n... .\n.\n.\n.\n.\n.\n..\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n. ..\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n..\n\n.\n\n.\n\n.\n\n.\n\n..\n.\n\n.\n\n-4\n\n-2\n\n0\n\nlog intensity\n\n10 \u00b7 nonlinear regression models\n\n0\n\n2\n\n4\n\n6\n\n8 10 12 14\n\nbandwidth h\n\nfigure 10.16 smooth\nanalysis of earthquake\ndata. upper left: local\nlinear regression of\nmagnitude on log intensity\njust before quake (solid),\nwith 0.95 pointwise\nconfidence bands (dots).\nupper right: generalized\ncross-validation criterion\ngcv(h) as afunction of\nbandwidth h. lower left:\nrelation between degrees\nof freedom \u03bd1 (solid), \u03bd2\n(dots), and h. lower right:\nsignificance traces for test\nof no relation between\nmagnitude and log\nintensity, based on\nchi-squared\napproximation (dots) and\nsaddlepoint\napproximation (solid).\nthe horizontal line shows\nthe conventional 0.05\nsignificance level.\n\n.\n\n..\n.\n.\n.\n\n2\n\n)\nh\n(\nv\nc\ng\n\nl\n\ne\nu\na\nv\n-\np\n\n8\n6\n1\n0\n\n.\n\n6\n6\n1\n0\n\n.\n\n4\n6\n1\n\n.\n\n0\n\n0\n1\n\n.\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n0\n\n.\n\n0\n\n2\n\n4\n\n6\n\n8 10 12 14\n\n0\n\n2\n\n4\n\n6\n\n8 10 12 14\n\nbandwidth h\n\nbandwidth h\n\n(10.37) and bandwidth h = 2. the upper left panel of figure 10.16 shows this fit,\nwith a 0.95 pointwise confidence interval. the width of the interval increases at the\nboundaries and at the right of the panel, owing to the lower density of design points\nx j . there is a suggestion of an increased magnitude at very low and high intensities,\nbut the evidence is not compelling.\n\nthe upper right panel shows that although the cross-validation criterion gcv(h)\n.= 5.5, the minimum changes little for h > 2. in the lower left\nis minimized when h\npanel we see that h > 2 corresponds to fitting curves with at most 4 or so equivalent\ndegrees of freedom, while for h > 8 there are essentially two degrees of freedom,\ncorresponding to straight-line regression. the corresponding plots for aic(h) and\naicc(h) decrease sharply and then tail off slowly, and also suggest that large band-\nwidths are appropriate.\n\nthe lower right panel of the figure shows two approximations to the significance\ntrace for an overall test of no relation between log intensity and magnitude. the\nvalues of pobs(h) suggest that the evidence for such a relation varies from weak to\nnon-existent. the approximations rest on the assumption that the data are normal. the\nlarge number of observations should mitigate the fact that this assumption is plainly\nincorrect, and it is unlikely to be critical, at least in this case.\n\n "}, {"Page_number": 539, "text": "10.7 \u00b7 semiparametric regression\n\n527\n\nthese data show no relation between the fitted intensity just prior to an earthquake\nand its magnitude. this conclusion is of course very tentative, because seismological\n(cid:1)\nknowledge has not been incorporated.\n\nextensions\nthe locally weighted polynomial fit arises naturally from a modified form of likeli-\nhood. for if the \u03b5 j were independent and normal, the contribution from (x j , y j ) to\nthe overall log likelihood for a polynomial fit of degree k centred at x0 would be\n(cid:6) j (\u03b2, \u03c3 ; x0) \u2261 \u2212 1\n2\u03c3 2\nand(cid:2)\u03b2 maximizes the local log likelihood\n(cid:14)\n\n{y j \u2212 \u03b20 \u2212 \u03b21(x j \u2212 x0) \u2212 \u00b7\u00b7\u00b7 \u2212\u03b2 k(x j \u2212 x0)k}2 \u2212 1\n2\n\nlog \u03c3 2,\n\n(cid:15)\n\n(cid:6)(\u03b2, \u03c3 ; x0) = n(cid:1)\n\n1\nh\n\nw\n\nj=1\n\nx j \u2212 x0\n\nh\n\n(cid:6) j (\u03b2, \u03c3 ; x0).\n\nthis idea extends fairly directly, for example to generalized linear and stochastic\n\nprocess models, using the appropriate log likelihood contribution and estimating (cid:2)\u03b2\n\nby iterative weighted least squares. the ideas described above then go through largely\nunchanged, though for a generalized linear model aicc(h) must be changed to\n\naicc(h) = n(cid:1)\n\nj=1\n\nd j{y j ;(cid:2)\u00b5 j (h)} +n\n\n1 + tr(sh)/n\n\n1 \u2212 {tr(sh) + 2}/n\n\n,\n\n(10.44)\n\nwhere d j{y j ;(cid:2)\u00b5 j (h)} is the deviance contribution from y j when the fitted value is\n(cid:2)\u00b5 j (h). this is a large topic, to which the bibliographic notes give some entry points.\n\nthe key ideas are summarized in the following example.\n\nexample 10.32 (toxoplasmosis data) example 10.29 described how allowance\nmight be made for the overdispersion of the data in table 10.19,\nto which\na logistic regression model with cubic dependence on rainfall was fitted. in\nview of the implausibility of the cubic model shown in the right panel of fig-\nure 10.12, we consider local fitting with binomial probability \u03c0(x) = exp{\u03b8(x)}/[1 +\nexp{\u03b8(x)}] depending on a local log odds \u03b8(x). we fit a taylor series expan-\n.= \u03b20 + \u03b21(x \u2212 x0) + \u00b7\u00b7\u00b7 +\u03b2 k(x \u2212 x0)k /k!, and take (cid:2)\u03b20 as the estimate\nsion, \u03b8(x)\nof \u03b8(x0).\n\n(cid:3)\n\n(cid:4)\n\n(cid:5)(cid:6)\n\nthe local log likelihood is\n\n(cid:6)(\u03b2; x0, h) \u2261 n(cid:1)\n\n\u03c6\n\nj=1\n\ny j x t\nj\n\nw j m j\n\n1 + ex t\n\n\u03b2 \u2212 log\nis the binomial denominator, y j = r j /m j\nis the observed proportion\nwhere m j\n= (1, x j \u2212 x0, . . . ,( x j \u2212 x0)k), and taking \u03c6 > 0 will allow for overdis-\npositive, x t\nj\npersion relative to the binomial model. the kernel function reduces the effective\nvalue of m j to m j w j , so towns whose rainfall x j is far from x0 count for less in the\nestimation of \u03b2.\n\n,\n\n\u03b2\n\nj\n\n "}, {"Page_number": 540, "text": "figure 10.17 local fits\nto the toxoplasmosis data.\nthe left panel shows fitted\n\nprobabilities(cid:2)\u03c0(x), with\n\nthe fit of local linear\nlogistic model with\nh = 400 (solid) and 0.95\npointwise confidence\nbands (dots). also shown\nis the local linear fit with\nh = 300 (dashes). the\nright panel shows the local\nquadratic fit with h = 400\nand its 0.95 confidence\nband. note the increased\nvariability due to the\nquadratic fit, and its\nstronger curvature at the\nboundaries.\n\n528\n\ne\nv\ni\nt\ni\ns\no\np\n\n \n\nn\no\n\ni\nt\nr\no\np\no\nr\np\n\n0\n1\n\n.\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n0\n\n10 \u00b7 nonlinear regression models\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\ne\nv\ni\nt\ni\ns\no\np\n\n \n\nn\no\n\ni\nt\nr\no\np\no\nr\np\n\n0\n1\n\n.\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n0\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\n\u2022\u2022\n\u2022\n\n\u2022 \u2022\n\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\u2022\n\n\u2022\u2022\n\u2022\n\n\u2022 \u2022\n\u2022\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n1600\n\n1800\n\n2000\n\n2200\n\n2400\n\n1600\n\n1800\n\n2000\n\n2200\n\n2400\n\nrainfall (mm)\n\nrainfall (mm)\n\nto be compared with\n\n\u22121 x tw2 x(x tw1 x)\n\nthe local score function may be written x tw u(\u03b2),\n\n(10.18), and (cid:2)\u03b2 is obtained by applying iterative weighted least squares to the\nbinomial model with artificial denominators w j m j . asandwich variance matrix\n\u22121 is required, where the jth elements of the diago-\nnal matrices w1 and w2 are w j m j(cid:2)\u03c0 j (1 \u2212(cid:2)\u03c0 j ) and w 2\nj m j(cid:2)\u03c0 j (1 \u2212(cid:2)\u03c0 j ), with(cid:2)\u03c0 j the fitted\n(x tw1 x)\nprobabilities. the dispersion parameter \u03c6 does not appear in the local score equation,\nmatrix or the degrees of freedom. an estimator(cid:2)\u03c6 is obtained by replacing the divi-\nand plays no role in the estimation of \u03b2, inthe effective kernel, in the smoothing\nsor n \u2212 p in (10.20) by its counterpart n \u2212 2\u03bd1 + \u03bd2, hence generalizing (10.39) to\naccommodate the binomial variance function.\nfigure 10.17 shows linear and quadratic local fits and their 0.95 pointwise confi-\ndence bands, obtained with h = 400; the left panel also shows the fit with h = 300.\nthe confidence bands for the quadratic fit are appreciably wider, and the fit itself is\nmore curved, particularly at the boundaries. as might be expected, taking h = 300\ngives a more locally adapted fit, whose effect is similar to increasing the order of the\npolynomial. all the fits are more plausible than the polynomial shown in figure 10.12.\nthe confidence bands are appreciably narrower when no allowance is made for the\noverdispersion, and they suggest that the probability depends on rainfall. overdis-\npersion makes this much less plausible, and indeed a horizontal line would lie inside\nthe bands in both panels of figure 10.17. any evidence for a relation between the\nprobability and rainfall seems weak, though an analogue of (10.43) would be required\n(cid:1)\nfor a more definite conclusion.\n\nin some applications trigonometric or other expansions may be more appropriate\nthan polynomial expansions; they too may be fitted locally using kernel or nearest\nneighbour weighting.\n\nsimilar ideas may be applied for smoothing in several dimensions, though the curse\nof dimensionality can then become heavy. it is useful to scale the covariates so that\na common bandwidth can be used for them all, for example by using bandwidth hsr\non the rth axis, where sr is the standard deviation of the rth covariate.\n\n "}, {"Page_number": 541, "text": "10.7 \u00b7 semiparametric regression\n\n529\n\nthis can be omitted on a\nfirst reading.\n\ncomputation of bias and variance\nto express the lessons of figure 10.15 in algebraic terms, we compute the mean and\n\n(x \u2212 x0)k g(k)(x0)\n1\n\nvariance of(cid:2)\u03b2. taylor series expansion gives\n(x0) + \u00b7\u00b7\u00b7 + 1\nk!\n+\n(k + 1)!\n\ng(x) = g(x0) + (x \u2212 x0)g\n\n(cid:5)\n\n= \u03b20 + (x \u2212 x0)\u03b21 + \u00b7\u00b7\u00b7 +( x \u2212 x0)k \u03b2k + b(x),\n\uf8eb\nsay, where the final term is the remainder. consequently\n\uf8ec\uf8ec\uf8ed\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8 =\n\n(x1 \u2212 x0)k\n\n(x1 \u2212 x0)\n\n\uf8eb\n\uf8ed 1\n...\n1 (xn \u2212 x0)\n\n(xn \u2212 x0)k\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\n\uf8f6\n\uf8f8\n\n\u00b7\u00b7\u00b7\n\n\u00b7\u00b7\u00b7\n\n...\n\n...\n\ng(x1)\ng(x2)\n...\ng(xn)\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8 +\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8 ,\n\nb(x1)\nb(x2)\n...\nb(xn)\n\n\u03b20\n\u03b21\n...\n\u03b2k\n\n(x \u2212 x0)k+1g(k+1)(x) + \u00b7\u00b7\u00b7\n\nor equivalently g = x\u03b2 + b, where b is the n \u00d7 1 vector whose jth element is b(x j ).\nlet y, g, and \u03b5 represent the n \u00d7 1 vectors whose jth elements are y j , g(x j ), and\n\u03b5 j , and recall that the \u03b5 j are independent with mean zero and variance \u03c3 2. then\ny = g + \u03b5 = x\u03b2 + b + \u03b5, giving\n\ne((cid:2)\u03b2) = e{(x tw x)\nvar((cid:2)\u03b2) = \u03c3 2(x tw x)\n\n\u22121 x tw (x\u03b2 + b + \u03b5)} =\u03b2 + (x tw x)\n\u22121 x tw 2 x(x tw x)\n\n\u22121,\n\n\u22121 x tw b,\n\nhence(cid:2)\u03b2 has a bias that depends on the polynomial terms of degree k + 1 and higher.\nif g(x) isindeed a polynomial of degree k or lower then b = 0 and(cid:2)\u03b2 is unbiased.\nfor the local linear fit, k = 1, and the bias of(cid:2)\u03b2 is\n(cid:15)\n\n(cid:15)\u22121(cid:14) (cid:13)\n\nw j\n\n(x tw x)\n\n\u22121 x tw b =\nhence the bias of(cid:2)\u03b20 is\n(cid:13)\n(cid:13)\n\n(cid:14) (cid:13)\n(cid:13)\n\n(cid:13)\n(cid:13)\nw j (x j \u2212 x0)\n(cid:13)\nw j b j \u2212(cid:13)\nw j (x j \u2212 x0)2\nw j (x j \u2212 x0)2 \u2212(cid:3)(cid:13)\n(cid:13)\nw j\n\nw j (x j \u2212 x0)\nw j (x j \u2212 x0)2\n(cid:13)\n\nw j (x j \u2212 x0)\n\nw j (x j \u2212 x0)b j\n\n(cid:6)2\n\n.\n\nw j (x j \u2212 x0)\n\n(cid:13)\n\nw j b j\n\nw j (x j \u2212 x0)b j\n\n.\n\nto approximate this, we suppose that the x j are sufficiently dense to have a well-\nbehaved smooth density, f (x), let n \u2192 \u221e and h \u2192 0 insuch a way that nh \u2192 \u221e,\nand replace the sums by integrals. we then see, for example, that\n\n(x \u2212 x0)2 f (x) dx\n\n(cid:1)\n\n(cid:29)\n\n(cid:14)\n\nx \u2212 x0\n\n(cid:15)\n\nh\n\n1\nh\n\n(cid:29)\n(cid:29)\n\nw j (x j \u2212 x0)2 .= n\n= nh2\n.= nh2\n.= nh2 f (x0) + o(nh4),\n\nw\nw(u)u2 f (x0 + hu) du\nw(u)u2{ f (x0) + hu f\n(cid:5)\n\n(x0) + \u00b7\u00b7\u00b7}du\n\n "}, {"Page_number": 542, "text": "530\n\n10 \u00b7 nonlinear regression models\non changing the variable of integration to u = (x \u2212 x0)/ h and recalling that w has\nunit variance and is symmetric. this calculation presupposes that x0 is sufficiently\n(cid:24)\nfar from the boundary relative to h that the range of integration for integrals such as\nw(u)u du is effectively infinite; otherwise odd powers of h do not vanish and the\nresult is anh2 f (x0) + o(nh3), with 0 < a < 1. provided the odd terms do cancel,\nsimilar calculations give(cid:1)\nw j b j\nw j (x j \u2212 x0)b j\n\nand on putting the pieces together we find that (cid:2)\u03b20 has bias whose leading term is\n1\n2 h2g\n(x0). it turns out that the bias has order h2 even when x0 is near the boundary,\nbut asimilar calculation for the nadaraya\u2013watson estimator (10.40) shows that its\nto get a handle on the variance of(cid:2)\u03b20, it issimplest to orthogonalize x by replacing\nbias near the boundary is o(h) (exercise 10.7.6).\nthe jth element of its second column with x j \u2212 x w , where x w = (cid:13)\n\n(cid:1)\n.= n f (x0)\n\n.= 1\n2\n.= o(nh4),\n\nw j (x j \u2212 x0)\n\n.= nh2 f\n\nnh2 f (x0)g\n\n(cid:1)\n(x0),\n\n(cid:1)\n\n(10.45)\n\nw j x j /\n\nw j . in\n\n(cid:13)\n\n(x0),\n\nw j\n\n(cid:5)(cid:5)\n\n(cid:5)(cid:5)\n\n(cid:5)\n\nthis parametrization the weighted least squares estimators are\n\n(cid:14)(cid:13)\n\n(cid:2)\u03b3 =\n\n(cid:13)\n\nw j\n0\n\n0\n\nw j (x j \u2212 x w )2\n\n(cid:15)\u22121(cid:14)\n\n(cid:13)\n\n(cid:13)\nw j (x j \u2212 x w )y j\n\nw j y j\n\n(cid:15)\n\n,\n\nand (cid:2)\u03b20 =(cid:2)\u03b30 + (x0 \u2212 x w )(cid:2)\u03b31. this gives a simple explicit formula for (cid:2)\u03b20, useful for\n\nnumerical work. its variance is\n\nvar((cid:2)\u03b20) = \u03c3 2\n\n\" (cid:13)\n(cid:4)(cid:13)\n\nw 2\nj\n\n(cid:5)2\n\nw j\n\n+ (x0 \u2212 x w )2\n\n(cid:13)\nj (x j \u2212 x w )2\n(cid:3)(cid:13)\n(cid:6)2\nw 2\nw j (x j \u2212 x w )2\n\n#\n\n,\n\nthe first term of which equals the variance of the local constant estimator (10.40).\nit turns out that x0 \u2212 x w\n(x0)/ f (x0) away from the boundary, and is o(h)\notherwise, and calculations like those above show that away from the boundary, both\nlocal linear and constant estimators have the approximate variance given in (10.41).\n\n.= h2 f\n\n(cid:5)\n\n10.7.2 roughness penalty methods\nlocal polynomial fitting is brought under the likelihood umbrella by local weighting\nof the log likelihood contributions. a different approach to curve estimation is based\non fitting a family of flexible functions to the data, with the most appropriate of these\nspecified indirectly by penalizing the roughness of the result. the idea is to fit a model\nwith potentially as many parameters as there are observations, but to constrain these\nparameters to the extent desired.\nto see how this might be done, we first consider suitably parametrized families of\nsmooth functions. let the data consist of pairs (t1, y1), . . . ,(t n, yn), where a = t0 <\nt1 < \u00b7\u00b7\u00b7 < tn < tn+1 = b. weseek a smooth summary g(t) of how the response y\ndepends on t over the interval [a, b].\n\none approach is to use a natural cubic spline g(t) with knots t1, . . . ,t n. such a\nfunction consists of separate cubic polynomials on each of the intervals [t1, t2], . . . ,\n\nwe denote the covariate\nby t rather than x for ease\nof generalization below.\n\n "}, {"Page_number": 543, "text": "10.7 \u00b7 semiparametric regression\n\n531\n\nfigure 10.18 natural\ncubic spline fits to n = 15\ndata pairs simulated from\nthe model y = 8x 2 + \u03b5.\nleft panel: fit with 15\ndegrees of freedom (solid)\nthat interpolates the data,\nwith values of t j shown by\nthe vertical dashed lines.\nright panel: fits with\ndegrees of freedom 2\n(solid), 7 (dashes), and 3.7\n(dots); the latter is chosen\nby cross-validation.\n\ny\n\n0\n1\n\n5\n\n0\n\n5\n-\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\u2022\n\u2022\n\u2022\n\ny\n\n0\n1\n\n5\n\n0\n\n5\n-\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\n-1.0\n\n-0.5\n\n0.0\n\n0.5\n\n1.0\n\n-1.0\n\n-0.5\n\n0.0\n\n0.5\n\n1.0\n\nt\n\nt\n\n(cid:5)(cid:5)\n\nhere and below, g\n(t) is\nthe second derivative of\ng(t).\n\n[tn\u22121, tn], constrained to be continuous and to have continuous first and second deriva-\ntives at each knot. the spline is linear on the extreme intervals [a, t1] and [tn, b].\nas there are 2 + 4(n \u2212 1) + 2 coefficients for these polynomial pieces and 3n con-\nstraints, just n numbers specify the spline. it turns out to be convenient to express\nit both in terms of its values gt = (g1, . . . , gn) atthe knots and the second deriva-\ntives \u03b3 t = (\u03b32, . . . , \u03b3n\u22121) att 2, . . . ,t n\u22121, where g j = g(t j ) and \u03b3 j = g\n(t j ). the sec-\nond derivatives at t1 and tn are zero, so \u03b31 = \u03b3n = 0. in fact there exist n \u00d7 (n \u2212 2)\nand (n \u2212 2) \u00d7 (n \u2212 2) matrices q and r, depending only on t1, . . . ,t n, such that\nqtg = r\u03b3 ; r is positive definite and hence invertible, and both q and r have simple\nstructure that makes numerical work with them very efficient (problem 10.14).\nnote that \u03b3 = r\n\u22121 qtg, so g(t) iscompletely determined by its values at the knots.\nan example is shown in the left panel of figure 10.18. as outlined above, the\nspline g(t) islinear outside (t 1, tn) and cubic between the vertical lines that show\nthe t j , with smooth joins between cubic portions. one way to imagine this is that\nthe spline adjusts to pass smoothly through beads \u2014 the y j \u2014 that move on vertical\nwires fixed at the t j .\n\n(cid:5)(cid:5)\n\npenalized log likelihood\nalthough perhaps a useful numerical summary of the data in figure 10.18, the spline\nin the left panel is a poor statistical summary: we need a smoother fit. suppose that\ny j = g(t j ) + \u03b5 j , where the \u03b5 j are independent normal errors with common variance\n\u03c3 2, and g(t) is anatural cubic spline with its n parameters gt = (g1, . . . , gn), where\ng j = g(t j ); let y denote the n \u00d7 1 vector of observed responses. maximization of the\nlikelihood over g boils down to minimization of the sum of squares (y \u2212 g)t(y \u2212 g).\nthis is achieved when g j = y j , clearly overfitted.\nwhen a similar difficulty arose in our discussion of model selection (section 4.7),\nwe dealt with it by penalizing the log likelihood to account for model complexity, and\nwe apply this idea here as well. if we judge that a straight line is the acme of simplicity,\n(t)}2 dt,\nthen one measure of the complexity of g(t) over the interval [a, b] is\nwhich would be zero for a linear fit. rather than maximize the usual log likelihood,\n\n{g\n\n(cid:24)\n\nb\na\n\n(cid:5)(cid:5)\n\n "}, {"Page_number": 544, "text": "532\n\n10 \u00b7 nonlinear regression models\n\ntherefore, we maximize the penalized log likelihood\n{g\n\n(cid:6)\u03bb(g, \u03c3 2) = n(cid:1)\n\nlog f {y j ; g(t j ), \u03c3 2} \u2212 \u03bb\n2\u03c3 2\n\nb\n\n(cid:29)\n\na\n\nj=1\n\n(cid:5)(cid:5)\n\n(t)}2 dt,\n\n\u03bb \u2265 0.\n\n(10.46)\n\nthis trades off the increase in the log likelihood term for more complex g against the\nsecond term, which penalizes nonlinearity. the extent of the trade-off is controlled\ncurve(cid:2)g\u03bb(t). when \u03bb = 0, no penalty is applied and there are n degrees of freedom,\nby \u03bb, adimensionless quantity related to the degrees of freedom of the maximizing\ncorresponding to unconstrained variation of each element of the vector g. as\u03bb \u2192 \u221e,\nthe penalty becomes so large that g(t) becomes a straight line, which has two degrees\nof freedom. intermediate values give curves lying between these extremes. for now\nwe suppose that \u03bb is fixed, deferring discussion of how to choose it.\nit turns out that when g(t) is anatural cubic spline, the integral in (10.46) may\nbe expressed as \u03b3 t qtg = gt q r\n\u22121 qtg = gt k g, say, where k is a n \u00d7 n symmet-\nric matrix of rank n \u2212 2 (problem 10.14). for normal errors the jth log likelihood\ncontribution is\n\nlog f {y j ; g(t j ), \u03c3 2} \u2261 \u2212 1\n2\n\n\u03c3 \u22122{y j \u2212 g(t j )}2 \u2212 1\n2\n\nlog \u03c3 2,\n\nso(cid:2)g\u03bb(t) isdetermined by the vector (cid:2)g that minimizes the penalized sum of squares\n\n(y \u2212 g)t(y \u2212 g) + \u03bbgt k g\n\nwith respect to g. oncompleting the square we find that (cid:2)g minimizes\n(cid:6)\n\n(cid:6)\n\n(cid:3)\n\n(cid:3)\n\n(i + \u03bbk )\n\n\u22121 y \u2212 g\n\nt (i + \u03bbk )\n\n(i + \u03bbk )\n\n\u22121 y \u2212 g\n\n,\n\n(10.47)\n\n(10.48)\n\nthe structure of the matrix k can be exploited to give a fast algorithm for\n\nsee that(cid:2)g = (i + \u03bbk )\nwhich differs from (10.47) only by a constant independent of g. it isstraightforward to\n\u22121 y is the unique natural cubic spline that minimizes (10.48);\non [a, b] and have absolutely continuous first derivative,(cid:2)g is optimal in a large class\nfurthermore, as it turns out that it does so among all functions that are differentiable\nof smooth functions.\n\u22121 qt. hence(cid:2)g is the solu-\nfitting the spline. recall that \u03b3 = r\n\u22121 qt)g = y. equivalently the corresponding (cid:2)\u03b3 solves the system\ntion to (i + \u03bbq r\n(r + \u03bbqt q)\u03b3 = qt y, which can be solved in o(n) operations because both r and\nq are band matrices; their only non-zero elements lie on the diagonal or just above\nand below it.\nlike a local polynomial fit, the spline is a linear smoother. its smoothing\nmatrix is s\u03bb = (i + \u03bbk )\n\u22121. note that k g = 0 for vectors of form g = \u03b201n +\n\u03b21(t1, . . . ,t n)t,because the roughness penalty is zero when g(t) islinear, and it follows\nthat s\u03bbg = g for such vectors. once again there are several definitions of the degrees\nof freedom for the smooth fit, the most obvious being tr(s\u03bb).\n\n\u22121 qtg and k = q r\n\nthe right panel of figure 10.18 shows three fits, of which the linear fit is evidently\ntoo smooth, and the one with seven degrees of freedom too rough. the fit with\n\n "}, {"Page_number": 545, "text": "10.7 \u00b7 semiparametric regression\n\n533\n\n3.7 degrees of freedom, chosen by cross-validation as described below, seems more\nplausible.\nfor later development we must deal with two complications. the first arises when\nweights w j are attached to the cases (t j , y j ). then (y \u2212 g)t(y \u2212 g) must be replaced\nby (y \u2212 g)tw (y \u2212 g), where w = diag{w1, . . . ,w n}; see section 8.2.4. the second\noccurs when some of the t j are tied. if so, we let s1 < \u00b7\u00b7\u00b7 < sq denote the ordered\ndistinct values among t1, . . . ,t n and denote by n the n \u00d7 q incidence matrix whose\n( j, k) element indicates whether t j = sk; obviously q \u2265 2, and n = i if the t j are\ndistinct and ordered. with these changes (10.47) alters to\n(y \u2212 ng)tw (y \u2212 ng) + \u03bbgt k g,\n\nwhich is minimized at(cid:2)g = n (n tw n + \u03bbk )\nn (n tw n + \u03bbk )\n\n\u22121 n tw reduces to the previous expression when w = n = i .\n\n\u22121 n tw y. the smoothing matrix s\u03bb =\n\nhow much smoothing?\nthe smoothing parameter \u03bb plays the same role as the bandwidth in a local polynomial\nmodel, and it too is typically chosen by minimizing information or cross-validation\ncriteria such as\n\n(cid:11)\n\ncv(\u03bb) = n(cid:1)\n\nj=1\n\ny j \u2212(cid:2)g(t j )\n1 \u2212 s j j (\u03bb)\n\n(cid:12)2\n\n, gcv(\u03bb) = n(cid:1)\n\n(cid:11)\n\ny j \u2212(cid:2)g(t j )\n1 \u2212 tr(s\u03bb)/n\n\n(cid:12)2\n\n,\n\nj=1\n\nwhere both the diagonal elements s j j (\u03bb) ofthe smoothing matrix s\u03bb and the fitted\nvalues g(t j ) depend on \u03bb. aswith other applications of smoothing, the goal is to trade\noff fidelity to the data against smoothness of the fit. once again a caveat is needed: the\nresults from an automatic procedure cannot always be trusted, and it is often valuable\nto apply different levels of smoothing. as mentioned above, it is useful to know the\ndegrees of freedom of a smooth fit.\n\nexample 10.33 (spring barley data) table 10.21 gives standardized yields from\nan agricultural field trial in which three blocks of long narrow plots were sown with\n75 varieties of spring barley in a random order within each block. the yield from\nvariety 27 in the third block is missing, but otherwise there are three replicates for\neach variety. the plot of the yields in the left panel of figure 10.19 shows strong\nspatial patterns owing to fertility trends within each block, in addition to the variety\neffects. for the moment we ignore differences among the varieties, and illustrate\nhow fitting a natural cubic spline can account for the fertility gradient in the first\nblock.\n\nthe left panel of figure 10.19 shows some of the disadvantages of polynomial\nfitting. the lower curve, for example, wiggles implausibly compared to a spline fit\nwith the same degrees of freedom, shown in the upper right panel. the lower right\npanel shows how cv(\u03bb) and gcv(\u03bb) vary with the equivalent degrees of freedom,\nfor the three blocks. the fit to block 2 seems fairly reasonable, but block 3 is evidently\noverfitted with 40 degrees of freedom, and block 1 is probably also overfitted. we\n(cid:1)\nreconsider these data in example 10.35.\n\n "}, {"Page_number": 546, "text": "table 10.21 spring\nbarley data (besag et al.,\n1995). spatial layout and\nplot yield at harvest y\n(standardized to have unit\ncrude variance) in a final\nassessment trial of\n75 varieties of spring\nbarley. the varieties are\nsown in three blocks, with\neach variety replicated\nthrice in the design. the\nyield for variety 27 is\nmissing in the third block.\n\n534\n\n10 \u00b7 nonlinear regression models\n\nblock 1\n\nblock 2\n\nblock 3\n\nlocation t\n\nvariety\n\nyield y\n\nvariety\n\nyield y\n\nvariety\n\nyield y\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n57\n39\n3\n48\n75\n21\n66\n12\n30\n32\n59\n50\n5\n23\n14\n68\n41\n1\n64\n28\n46\n73\n37\n55\n19\n10\n35\n26\n17\n71\n8\n62\n44\n53\n74\n20\n56\n29\n2\n47\n11\n38\n65\n13\n31\n40\n4\n67\n22\n49\n58\n43\n\n9.29\n8.16\n8.97\n8.33\n8.66\n9.05\n9.01\n9.40\n10.16\n10.30\n10.73\n9.69\n11.49\n10.73\n10.71\n10.21\n10.52\n11.09\n11.39\n11.24\n10.65\n10.77\n10.92\n12.07\n11.03\n11.64\n11.37\n10.34\n9.52\n8.99\n8.34\n9.25\n9.86\n9.90\n11.04\n10.30\n11.56\n9.69\n10.68\n10.91\n10.05\n10.80\n10.06\n10.04\n10.50\n9.51\n9.20\n9.74\n8.84\n9.33\n9.51\n9.35\n\n49\n18\n8\n69\n29\n59\n19\n39\n67\n57\n37\n26\n16\n6\n47\n36\n64\n63\n33\n74\n13\n43\n3\n53\n23\n62\n52\n12\n2\n32\n22\n42\n72\n73\n25\n45\n15\n35\n66\n5\n56\n46\n71\n51\n21\n1\n31\n11\n41\n61\n55\n14\n\n7.99\n9.56\n9.02\n8.91\n9.17\n9.49\n9.73\n9.38\n8.80\n9.72\n10.24\n10.85\n9.67\n10.17\n11.46\n10.05\n11.47\n10.63\n11.03\n10.85\n11.35\n10.25\n10.08\n10.25\n9.57\n11.34\n10.19\n10.80\n10.04\n9.69\n9.36\n9.43\n11.46\n9.29\n10.10\n9.53\n10.55\n11.34\n11.36\n10.88\n11.61\n10.33\n10.53\n8.67\n9.56\n9.95\n11.10\n10.11\n9.36\n10.23\n11.38\n11.30\n\n63\n38\n14\n71\n22\n46\n6\n30\n16\n24\n40\n64\n8\n56\n32\n48\n54\n37\n21\n29\n62\n5\n70\n13\n11\n44\n36\n52\n60\n68\n3\n19\n67\n59\n2\n75\n27\n43\n51\n10\n35\n74\n66\n34\n18\n50\n42\n1\n58\n26\n41\n25\n\n11.77\n12.05\n12.25\n10.96\n9.94\n9.27\n11.05\n11.40\n10.78\n10.30\n11.27\n11.13\n10.55\n12.82\n10.95\n10.92\n10.77\n11.08\n10.22\n10.59\n11.35\n11.39\n10.59\n11.26\n11.79\n12.25\n12.23\n10.84\n10.92\n10.41\n10.96\n9.94\n11.27\n11.79\n11.51\n11.64\n\n\u2014\n9.78\n8.86\n10.28\n12.15\n10.36\n9.59\n10.53\n11.26\n10.37\n10.10\n9.95\n9.80\n10.58\n9.31\n9.29\n\n "}, {"Page_number": 547, "text": "10.7 \u00b7 semiparametric regression\n\n535\n\ntable 10.21 (cont.)\n\nblock 1\n\nblock 2\n\nblock 3\n\nlocation t\n\nvariety\n\nyield y\n\nvariety\n\nyield y\n\nvariety\n\nyield y\n\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n\n7\n25\n61\n16\n52\n70\n34\n42\n24\n33\n51\n60\n69\n15\n6\n63\n54\n18\n45\n72\n9\n36\n27\n\n9.01\n10.58\n11.03\n9.89\n11.39\n11.24\n12.18\n10.21\n11.08\n11.05\n10.29\n10.57\n10.42\n10.49\n10.00\n9.23\n10.57\n10.27\n8.86\n9.45\n8.03\n9.22\n8.70\n\n44\n34\n54\n24\n4\n65\n75\n38\n17\n68\n7\n27\n58\n48\n28\n60\n30\n70\n20\n9\n40\n50\n10\n\n10.90\n10.97\n12.22\n10.10\n11.22\n10.01\n10.29\n10.95\n9.66\n9.31\n8.84\n10.64\n9.45\n9.66\n9.85\n9.24\n10.11\n9.63\n9.04\n8.43\n10.97\n8.98\n9.88\n\n33\n9\n17\n57\n65\n49\n73\n7\n23\n72\n55\n31\n39\n47\n15\n20\n61\n28\n53\n69\n45\n12\n4\n\n10.03\n9.49\n11.52\n12.24\n11.64\n10.74\n10.29\n10.25\n11.39\n13.34\n12.73\n12.62\n10.19\n11.61\n10.52\n9.07\n10.76\n9.91\n10.17\n8.68\n8.74\n9.15\n9.39\n\n(cid:6)\u03bb(\u03b2, g) = n(cid:1)\n\n10.7.3 more general models\nwe now consider how the discussion above should be modified when there are ex-\nplanatory variables as well as a smooth variable, treating certain covariates nonpara-\nmetrically and others not, and allowing the response to have a density other than the\nnormal.\nlet the data consist of independent triples (x1, t1, y1), . . . ,( xn, tn, yn), with jth log\nlikelihood contribution (cid:6) j (\u03b7 j , \u03ba), where \u03b7 j = x t\n\u03b2 + g(t j ); for now we suppress\ndependence on \u03ba. then the analogue of (10.47) is the penalized log likelihood\n\nj\n\n(cid:6) j (\u03b7 j ) \u2212 1\n2\n\nb\n\n(cid:5)(cid:5)\n\n{g\n\n(t)}2 dt,\n\na\n\n\u03bb\n\nj=1\n\n\u03bb > 0,\n\n(10.49)\nwhere a and b are chosen so that a < t1, . . . ,t n < b. ifall the t j are distinct and \u03bb = 0,\nthe maximum is obtained by choosing g j = g(t j ) tomaximise the jth log likelihood\ncontribution, but this is not useful because the resulting model has n parameters and\nis too rough. the integral in (10.49) penalizes roughness of g(t), so \u03bb has the same\ninterpretation as before.\nif the ordered distinct values of t1, . . . ,t n are s1 < \u00b7\u00b7\u00b7 < sq and if g(t) is anatural\ncubic spline with knots at the si , then the integral in (10.49) may be written gt k g,\nwhere the q \u00d7 1 vector g has ith element gi = g(si ). given a value of \u03bb, our aim\n\n(cid:29)\n\n "}, {"Page_number": 548, "text": "figure 10.19 spring\nbarley data analysis. left\npanel: yield y as a\nfunction of location x for\nthe three blocks. yields\nfor blocks 2, 3 have been\noffset by adding 4, 8\nrespectively. the smooth\nsolid lines are the fits of\npolynomials of degree 20,\n10 and 40 to the data from\nblocks 1, 2 and 3. upper\nright: yields for block 1,\nwith smoothing spline fit\nwith 18 degrees of\nfreedom. lower right:\ncross-validation (solid)\nand generalized\ncross-validation (dots)\ncriteria for smoothing\nspline fits to blocks 1, 2\nand 3, with minima at\nroughly 20, 10 and\n40 equivalent degrees of\nfreedom.\n\n536\n\ny\n \n\nl\n\nd\ne\ny\n\ni\n\n2\n2\n\n0\n2\n\n8\n1\n\n6\n1\n\n4\n1\n\n2\n1\n\n0\n1\n\n8\n\ny\n \n\nl\n\nd\ne\ny\n\ni\n\n10 \u00b7 nonlinear regression models\n\n2\n1\n\n1\n1\n\n0\n1\n\n9\n\n8\n\n0\n\n20\n\n40\n\n60\n\nlocation x\n\nn\no\ni\nr\ne\n\nt\ni\nr\nc\n \n\nn\no\n\ni\nt\n\na\nd\n\ni\nl\n\na\nv\n-\ns\ns\no\nr\nc\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n0\n\n20\n\n40\n\n60\n\nlocation x\n\n10\n\n20\n\n30\n\n40\n\n50\n\ndegrees of freedom\n\nis to find values of \u03b2 and g that maximize (cid:6)\u03bb(\u03b2, g). as the n \u00d7 1 vector of linear\npredictors may be written \u03b7 = x\u03b2 + ng, where n is the n \u00d7 q incidence matrix for\nthe elements of g and the n \u00d7 p matrix x has jth row x t\nn tu((cid:2)\u03b7) \u2212 \u03bbk(cid:2)g\n\nj , the score equations are\n(cid:15)\n\n\u2202(cid:6)\u03bb((cid:2)\u03b2,(cid:2)g)/\u2202\u03b2\n\u2202(cid:6)\u03bb((cid:2)\u03b2,(cid:2)g)/\u2202g\n\nx tu((cid:2)\u03b7)\n\n= 0,\n\n(10.50)\n\n(cid:14)\n\n(cid:15)\n\n(cid:14)\n\n=\n\n(cid:14)\n\nwhere the n \u00d7 1 vector u(\u03b7) has jth element \u2202(cid:6) j (\u03b7 j )/\u2202\u03b7 j . the usual taylor series\nexpansion (sections 4.4.1, 10.2.2) then gives\n\n(cid:14)\n\n(cid:15)\n\n(cid:15)(cid:14)(cid:2)\u03b2(cid:2)g\n\n.=\n\n(cid:15)\n\nx tw n\n\nx tw x\nn tw x n tw n + \u03bbk\n\nx tw z\nn tw z\nwhere w = diag{w1, . . . ,w n}, with w j = e{\u2212\u2202 2(cid:6) j (\u03b7 j )/\u2202\u03b72\n} and where z =\n\u22121u(\u03b7) + \u03b7 is the n \u00d7 1 adjusted dependent variable. fisher scoring would solve\nw\n(10.51) iteratively starting from suitable initial values of \u03b2 and g, but here there are\np + q regressors, where q is typically comparable to n, and an approach known as\nbackfitting is generally used instead. the idea is to alternate between the two matrix\n\n(10.51)\n\n,\n\nj\n\n "}, {"Page_number": 549, "text": "10.7 \u00b7 semiparametric regression\n\n537\n\nequations in (10.51), rewritten as\n\n(x tw x)(cid:2)\u03b2\n(n tw n + \u03bbk )(cid:2)g\n\n.= x tw (z \u2212 n(cid:2)g),\n.= n tw (z \u2212 x(cid:2)\u03b2).\n\n(10.52)\n(10.53)\n\ngiven initial values \u03b20 and g0 of \u03b2 and g, wecalculate \u03b7, w , and z, replace (cid:2)g in\n(10.52) by g0, and then obtain an approximate value of(cid:2)\u03b2 by regressing z \u2212 ng0 =\n\u22121u + x\u03b20 on the columns of x with weights w . wethen recalculate \u03b7 = x(cid:2)\u03b2 +\nw\nng0, w , and z, and solve (10.53) by applying the matrix (n tw n + \u03bbk )\n\u22121 n tw to\nz \u2212 x(cid:2)\u03b2 = w\n\u22121u + ng0, thus obtaining an approximate value of(cid:2)g. wethen set \u03b20\nand g0 equal to(cid:2)\u03b2 and(cid:2)g and iterate the cycle to convergence.\nexample 10.34 (partial spline model) the case where y j is normal with mean\n\u03b7 j and variance \u03c3 2 is known as a partial spline model. here u(\u03b7) = \u03c3 \u22122(y \u2212 \u03b7) and\nw = \u03c3 \u22122 in, so z = y (example 10.4).\nthe first backfitting step is least squares regression of y \u2212 ng0 on the columns\nof x. the second applies the linear smoother (n t n + \u03c3 2\u03bbk )\n\u22121 n t to the residual\ny \u2212 x(cid:2)\u03b2 from the first step; the effective penalty is thus \u03bb(cid:5) = \u03c3 2\u03bb. ateach step of the\nboth y \u2212 x(cid:2)\u03b2 and y \u2212 n(cid:2)g.\n\niteration either least squares or linear smoothing is applied to the residual from the\nprevious operation, continuing until any systematic structure has been removed from\n(cid:1)\n\nexample 10.36 gives a further illustration of such fitting.\nbackfitting yields parameter estimates, but unless the fit is purely exploratory other\nquantities are required for inference. the deviance is defined in the usual way, and\nthe error degrees of freedom of a fit are\n\nn \u2212 \u03bd\u03bb = n \u2212 tr(s\u03bb) \u2212 tr[{x tw (i \u2212 s\u03bb)x}\u22121 x tw (i \u2212 s\u03bb)2 x],\n\nwhere this and the smoothing matrix\n\ns\u03bb = n (n tw n + \u03bbk )\n\n\u22121 n tw\n\nare computed at convergence. the usual chi-squared theory is often used for approx-\nimate comparison of nested models, even though it has no firm theoretical basis.\n\nstandard errors for elements of(cid:2)\u03b2 and the fitted(cid:2)g(t) are generally obtained using the\n\napproximate linearization entailed by (10.51).\n\nas usual in semiparametric modelling, the degree of smoothing is critical. various\nforms of the cross-validation and generalized cross-validation criteria for choice of \u03bb\nhave been suggested. one approach takes\n\ncv(\u03bb) = n(cid:1)\n\nj=1\n\n(y j \u2212(cid:2)y j )2\nv j{1 \u2212 h j j (\u03bb)}2\n\n, gcv(\u03bb) = n(cid:1)\n\n(y j \u2212(cid:2)y j )2\n\nv j{1 \u2212 tr(h\u03bb)/n}2\n\n,\n\nj=1\n\nwhere v j is the estimated variance of y j ,(cid:2)y j is the jth fitted value, and h j j (\u03bb) is the\npartial derivative of (cid:2)y j with respect to y j , analogous to the earlier role of s j j (h).\n\nanother possibility is to cross-validate the approximating linear problem obtained at\neach step of the fitting algorithm.\n\n "}, {"Page_number": 550, "text": "538\n\n10 \u00b7 nonlinear regression models\n\nwhen several covariates might be treated nonparametrically, say t, u, and v, we can\ntake the linear predictor to be x t\u03b2 + g1(t) + g2(u) + g3(v) and fit smoothing splines\nor other nonparametric curves with a version of backfitting in which \u03b2 and the gs are\niteratively estimated in succession. such a setup is known as a generalized additive\nmodel, towhich the same ideas apply as outlined above. the degrees of smoothing\nare controlled by separate penalties for each of the gs, and the corresponding \u03bbs may\nbe estimated by minimizing a cross-validation or similar criterion. surfaces g(t, u)\ncan also be fitted using similar ideas.\n\nin some cases it is necessary to diminish the computational burden by reducing the\nnumber of knots and hence the number of parameters q that specify the fitted curve.\nalthough the resulting fit no longer has the optimality properties of the natural cubic\nspline, this is typically unimportant in practice.\n\nexample 10.35 (spring barley data)\nin addition to their strong spatial dependence,\nthe spring barley yields in table 10.21 depend on variety effects. the simplest model\nthat would accomodate these is a two-way layout with variety and block effects, in\nwhich the response is\nyvb = \u03c4b + \u03b2v + \u03b5vb,\niid\u223c n (0, \u03c3 2).\nv = 1, . . . ,75, b = 1, 2, 3, where\nthis has residual sum of squares 94.87 on 147 degrees of freedom, giving(cid:2)\u03c3 2 = 0.645,\nwhile the standard error for a difference of variety effects(cid:2)\u03b2v1\n\n\u2212(cid:2)\u03b2v2 is 0.655.\n\nas the two-way layout ignores the spatial variation, it greatly overestimates \u03c3 2,\nthereby decreasing the sensitivity of comparisons among the varieties. moreover the\nvariety effect estimators may be biased if all three replicates of a particular variety\nhappen to fall where the fertilities are higher than average. it seems more sensible\nto fit a model in which the yield for the vth variety in the bth block depends on its\nlocation tvb through\n\n\u03b5vb\n\nyvb = gb(tvb) + \u03b2v + \u03b5vb,\n\nv = 1, . . . ,75, b = 1, 2, 3,\n\nwhere gb(t) is asmooth function that determines how the fertility pattern in block b\ndepends on location t. when this model is fitted using smoothing splines with 40 knots\nfor each of the gb, 77 degrees of freedom are needed to account for the variety and\nblock effects, the degrees of freedom that minimize the generalized cross-validation\ncriterion are 16.4, 8.3, and 25.2 for b = 1, 2, and 3, the residual sum of squares is 16.85\non 224 \u2212 77 \u2212 16.4 \u2212 8.3 \u2212 25.3 = 97 degrees of freedom, and (cid:2)\u03c3 2 = 0.174, about\n\none quarter of the value for the two-way layout. the standard errors for differences\nof variety effects are roughly 0.41, so more precise comparisons are possible than in\nthe simpler fit. fewer degrees of freedom are needed to model the spatial variation\nhere than the 20, 10, and 40 required for the three blocks in example 10.33, because\nallowance for variety effects enables smoother fertility trends to be used.\n\nfigure 10.20 shows how this model decomposes the original data into fertility\ntrends, variety effects, and residuals. as their degrees of freedom would suggest, the\n\nestimate(cid:2)g2(t) isappreciably smoother than the fertility trends in blocks 1 and 3. the\nbest-yielding varieties in decreasing order of (cid:2)\u03b2v are 35, 56, 31, 54, 72, 55, 47, 18,\n\n "}, {"Page_number": 551, "text": "10.7 \u00b7 semiparametric regression\n\n539\n\nfigure 10.20 spring\nbarley data analysis.\nblock 1 is shown on the\nleft and block 3 on the\nright. the panel shows,\nfrom the top, the original\nyields y, the fertility trend\nand variety effect\n\nestimates(cid:2)gb(t) and(cid:2)\u03b2v ,\n\nboth offset for display,\nand the crude residuals.\nthe varieties with the ten\n\nlargest(cid:2)\u03b2v are marked.\n\nl\n\nd\ne\ny\n\ni\n\n2\n1\n\n0\n1\n\n8\n\n6\n\n4\n\n2\n\n0\n\n5\n5\n\n5\n3\n\n6\n2\n\n6\n5\n\n7\n4\n\n1\n3\n\n0\n4\n\n4\n5\n\n8\n1\n\n2\n7\n\n8\n1\n\n6\n2\n\n7\n4\n\n2\n7\n\n5\n3\n\n6\n5\n\n1\n3\n\n5\n5\n\n4\n5\n\n0\n4\n\nlocation\n\n0\n4\n\n6\n5\n\n4\n5\n\n5\n3\n\n8\n1\n\n6\n2\n\n2\n7\n\n5\n5\n\n1\n3\n\n7\n4\n\n40, and 26, but as a 0.95 confidence interval for differences of two variety effects has\nwidth 1.61, there is no clear-cut best variety.\n\na probability plot of the residuals shows nothing to undermine normality of the\nerrors, but the correlograms and partial correlograms of residuals from blocks 1 and 3\n\nshow slight negative correlations, suggesting that(cid:2)g1(t) and(cid:2)g3(t) may be overfitted. a\n\nmore complete analysis would try and remedy this by refitting the model with fewer\n(cid:1)\ndegrees of freedom for the fertility trends in blocks 1 and 3.\n\nexercises 10.7\n1\n\n(cid:5)\n\nexplain how the derivatives g\n\n(x0), . . . , g(k)(x0) may be estimated using the least squares\n\nestimator(cid:2)\u03b2 from a local polynomial fit of degree k.\n2 what is the bias of the local fit of a polynomial of degree k to a function that is polynomial of\ndegree l \u2264 k? how would you measure the disadvantage of this relative to an unweighted\n(cid:13){y j \u2212(cid:2)g(x j )}2 = (y \u2212(cid:2)g)t(y \u2212(cid:2)g) and recalling that y = g + \u03b5 and(cid:2)g = sy,\nfit?\nby writing\n\"\nwhere s is a smoothing matrix, show that\nn(cid:1)\nj=1\n\n= \u03c3 2(n \u2212 2\u03bd1 + \u03bd2) + gt(i \u2212 s)t(i \u2212 s)g.\n\n{y j \u2212(cid:2)g(x j )}2\n\n#\n\ne\n\n3\n\nhence explain the use of s2(h) as anestimator of \u03c3 2. under what circumstances is it\nunbiased?\n(a) if s11(h), . . . , snn(h) are the leverages of a smoothing matrix sh, establish\n\n4\n\n\u03bd1 = n(cid:1)\n\nj=1\n\ns j j (h),\n\n\u03bd2 = n(cid:1)\n\ni, j=1\n\nsi j (h)2 = \u03c3 \u22122\n\n(cid:6)\n\n(cid:3)(cid:2)g(x j )\n\n.\n\nvar\n\nn(cid:1)\nj=1\n\n(b) show that s(x j ; x j , h) isproportional to the (1, 1) element of (x tw x)\nfluence function of the smoother be i (x) = w(0)et\n\u22121e1, where et\nhas length k + 1. show that\n1\n\n1(x tw x)\n\n\u03c3 \u22122var{(cid:2)g(x)} \u2264 i (x),\n\n\u22121, and let the in-\n= (1, 0, . . . ,0)\n\n "}, {"Page_number": 552, "text": "540\n\n5\n\n6\n\n7\n\n10 \u00b7 nonlinear regression models\n\na j (u) =\n\nu = y j ,\nu =(cid:2)g\u2212 j (x j ).\n\nand deduce that \u03bd2 \u2264 \u03bd1.\n(c) let i1(x) and i2(x) bethe influence functions corresponding to bandwidths h1 < h2,\nand let the corresponding weight matrices be w1 and w2. show that w1 \u2264 w2 compo-\nnentwise and deduce that i2(x) \u2265 i1(x).\nconsider a linear smoother with n \u00d7 n smoothing matrix sh, so(cid:2)g = sh y, and show that\n(cid:11)(cid:2)g(x j ),\n(cid:2)g\u2212 j (x j ),\n\nthe function a j (u) giving the fitted value at x j as a function of the response u there satisfies\n\nexplain why this implies that s j j (h){y j \u2212(cid:2)g\u2212 j (x j )} =(cid:2)g(x j ) \u2212(cid:2)g\u2212 j (x j ), and hence obtain\n(a) check (10.45), and hence verify that e((cid:2)\u03b20) \u2212 g(x0)\n\n.= 1\ndo the corresponding calculation for x0 near a boundary.\n(x0) + \u00b7\u00b7\u00b7(cid:6)\n(b) show that the bias of the nadarayah\u2013watson estimator (10.40) may be expressed as\n(x0) + 1\n2 (x j \u2212 x0)2g\n(cid:13)\nw j\n(x0)a near a boundary, where a (cid:3)= 0, and\n(cid:5)\n\n2 h2{g\ndevelop the details of local likelihood smoothing when a linear polynomial is fitted to\npoisson data, using link function log \u00b5 = \u03b20 + \u03b21(x \u2212 x0).\n\nand deduce that this is approximately hg\n1\n\n(x0)} elsewhere.\n\n(x0) far from a boundary.\n\n(x j \u2212 x0)g\n\n(x0) + f\n\n(10.42).\n\n2 h2g\n\n(cid:13)\n\n(x0)g\n\nw j\n\n(cid:3)\n\n(cid:5)(cid:5)\n\n(cid:5)(cid:5)\n\n(cid:5)(cid:5)\n\n,\n\n(cid:5)\n\n(cid:5)\n\n(cid:5)\n\n10.8 survival data\n10.8.1 introduction\nsurvival or event history analysis concerns the times of events. such data are par-\nticularly common in medicine and the social sciences, but also arise in many other\ndomains. as we saw in section 5.4, the responses may be incompletely observed\nowing to censoring or truncation. here we give an introduction to regression analy-\nsis of such data in the simplest and most common situation, with just one event per\nindividual. throughout we use the term \u2018failure\u2019 to describe the event of interest, and\nrefer to the time to failure as a survival time.\nlet the data available on the jth of n independent individuals be (x j , y j , d j ), where\nx j is a p \u00d7 1 vector of explanatory variables, d j = 1 indicates that y j is an observed\nsurvival time, and d j = 0 indicates that the survival time is right-censored at y j .\nconsider a parametric model under which the survival time has density f (y; x, \u03b2),\nsurvivor function f(y; x, \u03b2) = 1 \u2212 f(y; x, \u03b2), and hazard and cumulative hazard\nfunctions h(y; x, \u03b2) and h(y; x, \u03b2) = \u2212 logf(y; x, \u03b2). assume that the censoring\nmechanism is uninformative, that is, independent of the failure time and uninformative\nabout \u03b2. then the discussion in section 5.4 implies that the log likelihood may be\nwritten as\n\n{d j log h(y j ; x j , \u03b2) \u2212 h(y j ; x j , \u03b2)},\n\n(10.54)\n\nfrom which maximum likelihood estimates(cid:2)\u03b2 may be obtained by iterative weighted\n\n(cid:6)(\u03b2) = n(cid:1)\n\nj=1\n\n "}, {"Page_number": 553, "text": "after cox and snell\n(1968).\n\n541\n\nleast squares. any additional parameters \u03c6 may be estimated by interleaving updates\n\n10.8 \u00b7 survival data\nto(cid:2)\u03b2 and(cid:2)\u03c6. asusual with incomplete data, confidence intervals should be based on\nobserved information, as in example 4.47.\nresiduals are important for model checking. the relation f(y; x, \u03b2) = 1 \u2212\nexp{\u2212h(y; x, \u03b2)} implies that if y is continuous and uncensored with distribution\nthis suggests that for diagnostic purposes the cox\u2013snell residuals h(y j ; x,(cid:2)\u03b2) can\nfunction f(y; x, \u03b2), then h(y ; x, \u03b2) is exponentially distributed with unit mean.\nbe regarded as an exponential random sample. for observations censored at c, we\nargue that as e{h(y ; x, \u03b2) | y > c} = h(c; x, \u03b2) + 1, an appropriate residual is\nh(c; x,(cid:2)\u03b2) + 1. this yields modified cox\u2013snell residuals\nr j = h(y j ; x,(cid:2)\u03b2) + 1 \u2212 d j .\n\n(10.55)\n\nother residuals may be defined for the proportional hazards model, described below.\ncase diagnostics discussed in section 10.2.3 may also be useful.\n\naccelerated life models\none notion used particularly in reliability studies is that time to failure may be ac-\ncelerated or retarded relative to some baseline. let y and y0 denote failure times\nfor individuals with covariates x and x = 0. then the accelerated life model posits\nthe existence of a positive function \u03c4 (\u03b2; x) such that y and \u03c4 (\u03b2; x)y0 have the same\ndistribution; equivalently y /\u03c4 (\u03b2; x) d= y0, abaseline random variable. an individual\nwith \u03c4 (\u03b2; x) < 1 will \u2018wear out\u2019 at a faster rate than the baseline, and conversely. if y0\nhas survivor, density, and hazard functions f0(y), f0(y), and h0(y), the corresponding\nfunctions for y are\n\nf0 {y/\u03c4 (\u03b2; x)} , \u03c4 (\u03b2; x)\n\n\u22121 f0 {y/\u03c4 (\u03b2; x)} , \u03c4 (\u03b2; x)\n\n\u22121h0 {y/\u03c4 (\u03b2; x)} .\n\n(10.56)\n\nthis is a scale model, so obvious possibilities are to let y0 be an exponen-\ntial, gamma, weibull, log-normal, or log-logistic variable. if \u03c4 (\u03b2; x) = exp(x t\u03b2),\nthen log y d= x t\u03b2 + \u03b5, where \u03b5 = log y0. the regression-scale model x t\u03b2 + \u03c3 \u03b5 is\nequivalent to taking (y /\u03c4 )1/\u03c3 d= y0. any of these gives a linear model for the log\nresponses, and if there is no censoring this can be fitted using least squares, though\ntypically information will be lost by doing so. however there is no special difficulty\nwith maximum likelihood estimation by iterative weighted least squares, if the density\nof \u03b5 or equivalently of y0 is known.\n\nexample 10.36 (leukaemia data) table 10.22 contains data on the survival of\nacute leukaemia victims. the covariate x is log10 white blood cell count at time of\ndiagnosis, and the patients are grouped according to the presence or not of a morpho-\nlogic characteristic of their white blood cells. within each group suppose that survival\ntime y is exponential with mean \u03c4 = exp(\u03b7), where \u03b7 = \u03b20 + \u03b21 i (group = 1) + \u03b22x.\nthis is a generalized linear model with gamma errors, log link function, and dispersion\nparameter \u03c6 = 1.\n\n "}, {"Page_number": 554, "text": "table 10.22 survival\ntimes y (weeks) for two\ngroups of acute leukaemia\npatients, together with\nx = log10 white blood cell\ncount at time of diagnosis\n(feigl and zelen, 1965).\npatients in group 1 had\nauer rods and/or\nsignificant granulation of\nthe leukaemic cells in the\nbone marrow at the time\nof diagnosis; those in\ngroup 2 did not.\n\nfigure 10.21 plots of\ndata and fitted means for\ngeneralized linear (left)\nand generalized additive\n(right) models fitted to\ntwo groups of survival\ntimes for leukaemia\npatients: group 1 (solid);\ngroup 2 (dashed).\n\n542\n\n10 \u00b7 nonlinear regression models\n\ngroup 1\n\ngroup 2\n\nx\n\ny\n\nx\n\ny\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n3.36\n2.88\n3.63\n3.41\n3.78\n4.02\n4.00\n4.23\n3.73\n\n65\n156\n100\n134\n16\n108\n121\n4\n39\n\n10\n11\n12\n13\n14\n15\n16\n17\n\n3.85\n3.97\n4.51\n4.54\n5.00\n5.00\n4.72\n5.00\n\n143\n56\n26\n22\n1\n1\n5\n65\n\nx\n\n3.64\n3.48\n3.60\n3.18\n3.95\n3.72\n4.00\n4.28\n4.43\n\ny\n\n56\n65\n17\n7\n16\n22\n3\n4\n2\n\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\nx\n\n4.45\n4.49\n4.41\n4.32\n4.90\n5.00\n5.00\n\ny\n\n3\n8\n4\n3\n30\n4\n43\n\n27\n28\n29\n30\n31\n32\n33\n\n1\n\n0\n5\n1\n\n)\ns\nk\ne\ne\nw\n\n(\n \n\ne\nm\n\ni\nt\n \n\ne\nr\nu\n\nl\ni\n\na\nf\n\n0\n0\n1\n\n0\n5\n\n0\n\n1\n\n1\n1\n\n1\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n2\n\n11\n2 22222\n1\n\n1\n\n1\n\n2\n\n2\n1\n\n2\n\n1\n\n2\n1\n\n2\n\n2\n\n1\n\n0\n5\n1\n\n)\ns\nk\ne\ne\nw\n\n(\n \n\ne\nm\n\ni\nt\n \n\ne\nr\nu\n\nl\ni\n\na\nf\n\n0\n0\n1\n\n0\n5\n\n0\n\n1\n\n1\n1\n\n1\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n2\n\n11\n2 22222\n1\n\n1\n\n1\n\n2\n\n2\n1\n\n2\n\n1\n\n2\n1\n\n2\n\n2\n\n3.0\n\n3.5\n\n4.0\n\n4.5\n\n5.0\n\n3.0\n\n3.5\n\n4.0\n\n4.5\n\n5.0\n\nlog10(white blood cell count)\n\nlog10(white blood cell count)\n\nwhen this model is fitted the deviance drops by 17.82 to 40.32, and the degrees\nof freedom drop from 32 to 30. the parameter estimates and standard errors are\n\n(cid:2)\u03b20 = 5.81 (1.29),(cid:2)\u03b21 = 1.02 (0.35), and(cid:2)\u03b22 = \u22120.70 (0.30). the mean survival time\nfitted means(cid:2)\u03c4 and data are shown in the left panel of figure 10.21. an exponential\nprobability plot of the residuals y/(cid:2)\u03c4 casts no doubt on the model. this can be verified\n\ndrops rapidly with x, but is increased in group 1; both effects are significant. the\n\nmore formally by fitting gamma, weibull, and log-logistic distributions, none of which\nimproves on the exponential.\n\ninspection of the left panel of figure 10.21 suggests some lack of fit of the system-\natic part of the model and we use this to illustrate the fitting of a generalized addi-\ntive model with linear predictor \u03b7 = \u03b20 + \u03b21 i (group = 1) + s(x), where the smooth\nfunction s(x) is anatural cubic spline. the right panel shows smooth dependence\non x with 3.36 degrees of freedom, found by generalized cross-validation starting\nfrom a cubic spline with 6 knots equi-spaced along the range of x. with this model\n\n(cid:2)\u03b20 = 2.80 (0.24) and (cid:2)\u03b21 = 1.15 (0.31); the value of (cid:2)\u03b20 changes because s(x) is\n\nparametrized to be orthogonal to a constant. the deviance is 31.68 with 27.63 equiv-\nalent degrees of freedom, so the f statistic for comparison of this with the fully\nparametric model is (40.32 \u2212 31.68363)/2.37/(31.68/27.63) = 3.18 on 2.37 and\n\n "}, {"Page_number": 555, "text": "often called the cox\nmodel, because\nintroduced by cox (1972).\n\nwe can take x0 = 0\nwithout loss of generality.\n\n10.8 \u00b7 survival data\n\n543\n\n27.63 degrees of freedom, giving significance level 0.05. here chi-squared asymp-\ntotics are of dubious relevance, and as simulation from the parametric model gives a\nrather larger significance level, there is no reason to choose the more complex gen-\neralized additive model, particularly as increased mean survival time at the highest\n(cid:1)\nwhite blood cell counts seems implausible.\n\n10.8.2 proportional hazards model\nin medical applications the focus of interest is typically on how treatments or clas-\nsifications of the units affect survival, the form of the survival distribution being of\nsecondary importance. this suggests that we seek inferences that will be valid for\nany such distribution. this is difficult for accelerated life models, and instead we let\nthe covariates act directly on the hazard. suppose that an individual with baseline\ncovariate x0 has hazard function h0(y) after a time y on trial, while an individual with\ncovariate x has hazard function h(y) = \u03be(\u03b2; x)h0(y), where \u03be(\u03b2; x) is apositive func-\ntion sometimes called a risk score; usually \u03be(\u03b2; x) = exp(x t\u03b2). the ratio h(y)/ h0(y)\ndoes not involve h0. this proportional hazards assumption turns out to be crucial,\nbut it isstrong and must be checked in practice.\nexp{\u2212h(y)}, where h(y) = (cid:24)\nthe basic relationship between the survivor and hazard functions, f(y) =\ny\n0 h(u) du, implies that the survival time for an in-\n\ndividual with covariate x has survivor and density functions\n\u03be(\u03b2; x)h0(y)f0(y)\u03be(\u03b2;x).\n\nf0(y)\u03be(\u03b2;x),\n\nthus whereas accelerated life models scale the axis of a baseline survivor function,\nf0, proportional hazards raise the baseline survivor function to a power.\nthe action of the covariates being of primary interest, we seek a likelihood on which\nto base inference for \u03b2, regardless of h0(y). to motivate the argument below, note\nthat if the hazard function was entirely arbitrary, then inference could only be based\non events where failures actually occurred, because the hazard might in principle\nbe zero at every other time. thus it suffices to estimate the baseline cumulative\n\nhazard function by a step function h0(y) = (cid:13)\nj:y j\u2264y h j , where h j = h0(y j ) > 0 only\nat observed failure times.\nsuppose there are no ties, take 0 < y1 < \u00b7\u00b7\u00b7 < yn without loss of generality, and\nlet r j denote the risk set of individuals still available to fail at the instant before y j ,\nthat is, all except those who have previously failed or been censored; see figure 5.8.\nfor brevity set \u03be j = \u03be(\u03b2; x j ). then the log likelihood is\n\nn(cid:1)\nj=1\n\n{d j log(\u03be j h j ) \u2212 \u03be j h0(y j )} = n(cid:1)\n= n(cid:1)\n\nj=1\n\n(cid:11)\nd j log(\u03be j h j ) \u2212 \u03be j\n(cid:14)\n\nj(cid:1)\ni=1\n\n(cid:12)\n\nhi\n\nd j log \u03be j + d j log h j \u2212 h j\n(cid:13)\ni\u2208r j\n\n(cid:15)\n\n\u03bei\n\n.\n\n(cid:1)\ni\u2208r j\n\nj=1\n\nwith \u03b2 fixed the h j have maximum likelihood estimators(cid:2)h j = d j /\n\n\u03bei , positive\n\n "}, {"Page_number": 556, "text": "544\n\n10 \u00b7 nonlinear regression models\nonly when d j = 1, so the profile log likelihood for \u03b2 is\n(cid:9)\n\n(cid:10)\n\n(cid:6)p(\u03b2) = max\n\nh1,...,hn\n\n(cid:6)(\u03b2, h1, . . . , hn) \u2261 n(cid:1)\n(cid:8)d j =\n\nj=1\n\n(cid:13)\n\u03be(\u03b2; x j )\ni\u2208r j\n\n\u03be(\u03b2; xi )\n\n(cid:25)\n\nfailures\n\n(cid:7)\n\nn(cid:25)\nj=1\n\nthe corresponding profile likelihood is\n\n\u03be j(cid:13)\ni\u2208r j\n\nd j log\n\n.\n\n\u03bei\n\n(10.57)\n\n(cid:13)\n\u03be(\u03b2; x j )\ni\u2208r j\n\n\u03be(\u03b2; xi )\n\n.\n\n(10.58)\n\nalternatively we may reason that the probability of the particular failure observed to\noccur at y j , conditional on a failure occurring then, is\n\n(cid:13)\n\u03be j h0(y j )\ni\u2208r j\n\n\u03bei h0(y j )\n\n=\n\n\u03be j(cid:13)\ni\u2208r j\n\n,\n\n\u03bei\n\n(10.59)\n\nand hence (10.58) is the probability that failures occur in the observed order, condi-\ntional on their occurrence times and margining over times of censoring. thus (10.58)\nis the product of a nested sequence of multinomial variables. there is a close con-\nnection to the discussion of poisson variables and log-linear models on page 501.\n\nexpression (10.58) is known as a partial likelihood. insection 12.2 it is derived\nas a marginal likelihood based on the observed ranking of failure times. although\na mathematically complete derivation is beyond our scope, it turns out that despite\nthe maximization over n nuisance parameters, (10.58) can be treated as an ordinary\n\nlikelihood: the maximum partial likelihood estimator(cid:2)\u03b2 is consistent for \u03b2 under mild\n\nconditions, and standard errors can be based on the inverse observed information\nmatrix.\n\ninformation contained in the failure times is lost, because they are treated as fixed\nin constructing the partial likelihood. the loss of information compared to using the\ncorrect parametric model turns out to be small in most cases, however, so standard\nerrors from partial likelihood are close to those obtained under the true model. partial\nlikelihood inferences make essentially no assumptions about h0, and are in this sense\nsemiparametric.\n\ntied failure times have probability zero for continuous distributions, but neverthe-\nless they arise in data due to rounding. three possible modifications of the partial\nlikelihood to adjust for the simultaneous failure of a elements of the risk set r j at\ntime y j are to include a term corresponding to each failure occurring first, to compute\nthe exact probability of a failures, and to use an approximation to this. thus (10.59)\nis replaced by one of\n\na(cid:25)\ni=1\n\n\u03bei(cid:13)\nk\u2208r j\n\n(cid:26)\n(cid:13)(cid:26)\na\ni=1\n\u03bei\na\nk=1\n\u03belk\n\na(cid:25)\ni=1\n\n(cid:13)\n\n,\n\n,\n\n\u03bek\n\n(10.60)\nwhere the sum in the exact central formula is over all subsets of r j of size a. the\nfirst of these arises from applying the profile likelihood argument above to tied data.\nin practice these corrections often give similar results.\n\n\u03bek \u2212 i\u22121\n\nk\u2208r j\n\na\nk=1\n\n\u03bek\n\n,\n\na\n\n\u03bei\n\n(cid:13)\n\n "}, {"Page_number": 557, "text": "10.8 \u00b7 survival data\n545\nexample 10.37 (leukaemia data) consider the data in table 10.22 with \u03be =\nexp{\u03b20 + \u03b21 i (group = 1) + \u03b22x}. now\u03b2 0 cancels from the partial likelihood, max-\nimization of which gives(cid:2)\u03b21 = \u22121.07 (0.43) and(cid:2)\u03b22 = 0.85 (0.31). these are similar\n\nto the values for the exponential model, apart from the sign change because the hazard\nand mean survival time are inversely related. note in particular that the standard er-\nrors barely differ, confirming our comments about the efficiency of partial likelihood\nestimation.\n\nthese data have 17 ties. the estimates above result from using the third, approxi-\n\nmate, term in (10.60), while the second, exact, formula gives(cid:2)\u03b21 = \u22121.08 (0.45) and\n(cid:2)\u03b22 = 0.90 (0.34), and the simple first approximation gives (cid:2)\u03b21 = \u22121.02 (0.42) and\n(cid:2)\u03b22 = 0.83 (0.31). there is little to choose among these, but rather more to choose\n\namong the likelihood ratio statistics for inclusion of the two covariates, which are 15.6\nusing the second and third terms, and 14.6 using the first. the third term in (10.60)\nthus seems preferable to the first, as both require the same computational effort.\n\nit may be useful to contrast two types of semiparametric procedure. partial like-\nlihood inference requires no assumptions about the baseline hazard and distribution\nfunction, and in a sense relaxes the vertical axis of figure 10.21. use of splines or\nother smoothing procedures can also be described as semiparametric, but it relaxes\nthe horizontal axis of the figure, which relates the covariate and response. spline\nterms can be introduced into the linear predictor of the proportional hazards model,\nreplacing \u03b22x by s(x), but this does not improve significantly on the exponential\n(cid:1)\nmodel.\n\n,\n\n(10.61)\n\nthe baseline cumulative hazard and survivor functions may be estimated by\n\n(cid:2)h0(y) =\n\n(cid:1)\nj:y j\u2264y\n\nd j(cid:13)\ni\u2208r j\n\n(cid:2)\u03bei\n\n, (cid:2)f0(y) =\n\n(cid:10)\n\n(cid:9)\n1 \u2212\n\n(cid:25)\nj:y j\u2264y\n\nd j(cid:13)\ni\u2208r j\n\n(cid:2)\u03bei\n\nwhere(cid:2)\u03be j = \u03be((cid:2)\u03b2; x j ). these are needed to assess fit and to predict survival probabilities\nwith covariates x+ is f+(y) = exp{\u2212\u03be((cid:2)\u03b2; x+)(cid:2)h0(y)}, from which the probability of\n\nfor individuals from a fitted model. the estimated survivor function for an individual\n\nsurvival beyond a given point can be read off, with standard errors found using the\ndelta method.\n\nthe construction above extends to stratified data, with the baseline hazard varying\nbetween strata but the parameter being common to all strata. this is useful in checking\nproportionality of hazards.\n\nlog rank test\nwe now briefly discuss use of the proportional hazards model to construct tests for\nequality of survival distributions. when \u03be(\u03b2; x) = exp(x t\u03b2), the log partial likelihood\n(10.57) equals\n(cid:6)p(\u03b2) =\n\n\u03b2 \u2212 log a j (\u03b2)\n\n\u03b2 \u2212 log\n\n(cid:5)(cid:12)(cid:23)\n\n(cid:1)\n\n(cid:1)\n\nexp\n\n=\n\n(cid:22)\n\n(cid:3)\n\n(cid:6)\n\n(cid:4)\n\n\u03b2\n\n,\n\nx t\nj\n\nx t\nj\n\nx t\ni\n\nfailures\n\nfailures\n\n(cid:11) (cid:1)\ni\u2208r j\n\n "}, {"Page_number": 558, "text": "546\n\n10 \u00b7 nonlinear regression models\n\nsay, with first derivative\nx j \u2212\n\nu (\u03b2) =\n\n(cid:1)\n\n(cid:7)\n\nfailures\n\n(cid:13)\ni\u2208r j\n\n(cid:5)\n\n(cid:8)\n\n(cid:4)\n\n\u03b2\n\nx t\ni\n\n=\n\n(cid:11)\n\n(cid:1)\n\nfailures\n\nxi exp\na j (\u03b2)\n\n(cid:12)\n\nx j \u2212 b j (\u03b2)\na j (\u03b2)\n\n,\n\n(10.62)\n\nsay, and negative second derivative\n\n(cid:7)(cid:13)\n\n(cid:1)\n\nfailures\n\nj (\u03b2) =\n\n(cid:5)\n\n(cid:4)\n\n\u03b2\n\nx t\ni\n\n(cid:8)\n\n.\n\n\u2212 b j (\u03b2)b j (\u03b2)t\n\na j (\u03b2)2\n\n(10.63)\n\ni\u2208r j\n\ni exp\nxi x t\na j (\u03b2)\n\nsuppose the data fall into two groups with respective hazard functions h0(y) and\nh(y) = e\u03b2h0(y). then a score test for \u03b2 = 0, that is, equality of survival distributions,\nis obtained by letting x j be scalar, with x j = 1 or 0indicating that failure j belongs to\ngroups 1 or 0, and taking u (0)\n1 . this\nis known as the log rank test.\n\n.\u223c n{0, j (0)} or equivalently u (0)2/j (0)\n\n.\u223c \u03c7 2\n\nnow a j (0) and b j (0) respectively equal\n\n= m0 j + m1 j ,\n\n\u03b2=0\n\n(cid:5)\n\n(cid:4)\n\n\u03b2\n\nx t\ni\n\nxi exp\n\n(cid:1)\ni\u2208r j\n\n$$$$$$\n\n= m1 j ,\n\n\u03b2=0\n\nthe total number of individuals and the number of group 1 individuals available to\nfail at time y j . thus\n\nr j \u2212\n\nm1 j\n\nm0 j + m1 j\n\nj (0) =\n\n,\n\nm0 j m1 j\n\n(m0 j + m1 j )2\n\n,\n\n(cid:15)\n\n(cid:1)\n\nfailures\n\nwhere r j = 1 ifthe individual failing at time y j belongs to group 1 and r j = 0\notherwise. hence the score statistic is a sum of centred binary variables. these are\nnot independent but under mild conditions the normal limiting distribution above will\nnonetheless hold.\n\nan alternative argument proceeds by cross-classifying the risk set at each failure\ntime by group membership and failure/survival, and using the hypergeometric distri-\nbution for the number of group 1 failures conditional on the row and column totals in\nthe resulting 2 \u00d7 2 table. this applies also when there are ties, and yields\n\n(cid:4)\n\nexp\n\n\u03b2\n\nx t\ni\n\n(cid:1)\ni\u2208r j\n\n(cid:5)\n\n$$$$$$\n\nu (0) =\n\n(cid:14)\n\n(cid:1)\n\nfailures\n\nu (0) =\n\nj (0) =\n\n(cid:14)\n\n(cid:1)\n(cid:1)\n\nfailures\n\nfailures\n\n(cid:15)\n\n,\n\nr j \u2212 m1 j a j\nm0 j + m1 j\nm0 j m1 j a j (m0 j + m1 j \u2212 a j )\n(m0 j + m1 j )2(m0 j + m1 j \u2212 1)\n\n,\n\nwith r j now the number of group 1 failures at y j among the total number of failures\na j at the jth failure time; see the discussion after (10.28). this reduces to the previous\nversion when there are no ties, that is, a j \u2261 1.\nexample 10.38 (mouse data) figure 5.11 compares cumulative hazard functions\nfor subsets of the data of table 5.7. the values of u (0)2/j (0) for the left and right\npanels are 3.3 and 40.1, each to be treated as \u03c7 2\n1 . the first has significance level 0.07,\n\n "}, {"Page_number": 559, "text": "10.8 \u00b7 survival data\n\n547\n\nweak evidence that the distributions differ. the second strongly supports the visual\n(cid:1)\nimpression of quite different distributions.\n\nthe log rank test generalizes to quantitative covariates x, tomultiple survival\n\ndistributions, and to weighted sums(cid:1)\n\n(cid:14)\n\nw j\n\nfailures\n\n(cid:15)\n\n,\n\nr j \u2212 m1 j a j\nm0 j + m1 j\n\nwhere the w j can depend on the failure times, on m0 j , m1 j , and on a j . such statis-\ntics can give better power against alternatives other than proportional hazards. their\nvariances may be found using ideas from section 7.2.3.\n\ntime-dependent covariates\nthus far we have supposed that the covariate vector x j takes the same value throughout\nthe period over which the jth individual is observed. this is appropriate for variables\nsuch as age on entry, sex, and summaries of medical history prior to entry to the\nstudy, but it is also necessary to be able to accommodate explanatory variables that\nvary during the study. quantities such as a patient\u2019s blood pressure may be available\nat various points over the observation period, for example, or a treatment may be not\nallocated until well after the study has begun, or changed during the trial. in reliability\ntrials the key explanatory variable may be cumulative stress, or perhaps instantaneous\nstress, both of which may change during the experiment.\n\nthe interpretation of effects of covariates that may be influenced by the treatments\ndemands careful thought. consider for example a study in which treatments for hy-\npertension are compared, blood pressure being an explanatory variable. use of initial\nblood pressure as a covariate should increase the precision with which the treatment\neffects can be estimated, but interest would focus on the treatments, the estimated\neffect of blood pressure being of little direct interest. use of blood pressure monitored\nafter treatment allocation, by contrast, would allow the analyst to assess the extent\nto which treatments affect survival by influencing blood pressure; the estimate might\nthen be of prime concern.\n\ntime-varying covariates may also be constructed for technical reasons, for instance\nto check adequacy of the proportional hazards assumption by including y or log y in\nthe linear predictor.\nwhatever the interpretation, use of time-dependent covariates leads to replacement\nof the p elements x j1, . . . , x j p of x j by functions x j1(y), . . . , x j p(y), 0 \u2264 y \u2264 y j .\nthese may be indicator variables, for example showing the treatment being applied\nat time y. the covariates are typically measured only at certain times, so the func-\ntion x jr (y) isusually obtained by interpolation. let x j (y) denote the p \u00d7 1 vector\n(x j1(y), . . . , x j p(y))t. the hazard function \u03be{\u03b2; x j )h0(y) becomes \u03be{\u03b2; x j (y)}h0(y),\nand our previous argument shows that the log partial likelihood is\n\u03be{\u03b2; xi (y j )}\n\n(cid:6)p(\u03b2) = n(cid:1)\n\n\u03be{\u03b2; x j (y j )} \u2212log\n\n(cid:23)(cid:15)\n\n(cid:14)\n\nd j\n\n,\n\n(cid:22) (cid:1)\ni\u2208r j\n\nj=1\n\n "}, {"Page_number": 560, "text": "548\n\n10 \u00b7 nonlinear regression models\n\nthe outer sum being over failure times y j . thus rather than x j , the covariates needed\nfor the jth individual are {x j (yi ) : yi \u2264 y j}, where the failure times yi are those at\nwhich case j lies in the risk set.\n\nstandard large-sample likelihood results may be used for inference on \u03b2.\n\nmodel checking\nwhen the data contain two groups, a graphical check on proportional hazards may be\nbased on their estimated cumulative hazard functions. if the cumulative hazard func-\ntions for the two groups are h0(y) and h(y), then proportional hazards asserts that\n\nh(y) = \u03be(\u03b2; x)h0(y). thus log (cid:2)h(y) \u2212 log (cid:2)h0(y) should appear independent of y.\nequal r j = (cid:2)h j (y j ) + 1 \u2212 d j , where the estimated cumulative hazard function for\n(cid:2)h j (y) = \u03be((cid:2)\u03b2; x j )(cid:2)h0(y j ) and (cid:2)h0 is given at (10.61). plots of the r j for subsets of the\n\nthe jth individual under a proportional hazards model with constant covariates is\n\nvarious residuals can be defined. the modified cox\u2013snell residuals (10.55)\n\nobservations may cast light on interactions, but are not useful for assessing distribu-\ntional assumptions in the proportional hazards model because h0(y) isnot specified\nparametrically.\nif y j is a continuous random variable with censoring indicator d j and cumulative\nhazard function h j (y), then i (y j \u2264 y, d j = 1) \u2212 h j{min(y, y j )} is a zero-mean\ncontinuous-time martingale; see page 552. with y = \u221e this gives d j \u2212 h j (y j ), and\nimplies that a martingale residual may be constructed as d j \u2212 (cid:2)h j (y j ) = 1 \u2212 r j , just\nthe residual above apart from a location and sign change. the functional form of a\ncovariate in a proportional hazards model with \u03be(\u03b2; x) = exp(x t\u03b2) can be checked\nby plotting 1 \u2212 r j computed with the covariate omitted against the covariate itself.\nthe strong negative skewness of martingale residuals can be reduced by transfor-\n\nmation, giving deviance residuals\n\nsign{d j \u2212 (cid:2)h j (y j )}[2{(cid:2)h j (y j ) \u2212 d j \u2212 d j log (cid:2)h j (y j )}]1/2,\n\nwhich are useful for checking for outliers; they are formally equivalent to treating the\nd j as poisson variables with means h j (y j ).\n\nan approach based on (10.62) uses components of the contributions\n\n(cid:13)\n(cid:13)\ni\u2208r j\ni\u2208r j\n\n(cid:4)\n\nxi exp\nexp\n\n(cid:4)\n\n(cid:5)\n(cid:2)\u03b2\n(cid:5) ,\n(cid:2)\u03b2\n\nx t\ni\n\nx t\ni\n\nx j \u2212\n\nseen to fail. these p \u00d7 1 vectors can be scaled by pre-multiplication by j j ((cid:2)\u03b2), where\nto the score vector, thus giving a residual for each covariate and for each individual\nthe p \u00d7 p matrix j j (\u03b2) isthe contribution to (10.63) from the\njth failure. they\nare closely related to the influence measures (8.29) and (10.13), and plots of their\nof(cid:2)\u03b2. they are also useful for assessing adequacy of proportional hazards. a natural\ncomponents help to determine which of the observations are influential for elements\nway inwhich hazards might not be proportional is h(y) = h0(y) exp{x t\u03b2(y)}, that is,\nthen e(s j ) +(cid:2)\u03b2\nthe coefficient of x depends on time. if this is the case, and there are no tied failures,\n.= \u03b2(y j ), where s j is a standardized version of the score contributions\n\n "}, {"Page_number": 561, "text": "edema is the\naccumulation of fluids in\nbody tissues.\n\n10.8 \u00b7 survival data\n\n549\n\ncomputed using only the risk set at time y j . anon-constant plot of observed s j against\ny j suggests this type of model failure.\n\nthese and other diagnostics for the proportional hazards model can be extended to\n\ntime-dependent covariates.\n\nexample 10.39 (pbc data) primary biliary cirrhosis (pbc) is a chronic fatal dis-\nease of the liver, with an incidence of about 50 cases per million. controlled clinic\ntrials are hard to perform with very rare diseases, so the double-blinded randomized\ntrial conducted at the mayo clinic from 1974\u20131984 is a valuable resource for liver\nspecialists. a total of 424 patients were eligible for the trial, and the 312 who con-\nsented to take part were randomized to be treated either with the drug d-penicillamine\nor with a placebo. although basic data are available on all 424 patients, we consider\nonly these 312 individuals. covariates available on each of them at recruitment in-\nclude the demographic variables sex and age; clinical variables, namely presence or\nabsence of ascites, hepatomegaly, spiders, and a ternary varable edtrt whose\nvalues 0, 1/2, 1 indicate no, mild, and severe edema; and biochemical variables,\nnamely levels of serum bilirubin (mg/dl), serum cholesterol (mg/dl), albumin\n(gm/dl), urine copper (\u00b5g/day), alkaline phosphatase (u/ml), sgot (u/ml), and\ntriglycerides (mg/dl), platelet count (coded), prothombin time (seconds), and the\nhistologic stage of the disease (1\u20134). there are 28 missing values of serum choles-\nterol and 30 of triglycerides, and we ignore these covariates. four missing values\nof platelets and two of urine copper were replaced by the medians of the remain-\ning values; this should have little effect on the analysis. at the time at which the\ndata considered here became available, 125 patients had died, with just 11 deaths\nnot due to pbc, eight patients had been lost to follow-up, and 19 had undergone\na liver transplant. as the response is time to death, these patients are regarded as\ncensored.\n\nthe upper left panel of figure 10.22 shows that estimated survivor functions for\nthe patients with the drug and the placebo are very close, and it is no surprise that the\nlog-rank statistic has value 0.1, insignificant when treated as \u03c7 2\n1 . this is borne out by\nthe estimated treatment effect of \u22120.057 (0.179) for a fit of the proportional hazards\nmodel with treatment effect only. analysis stratified by sex gives an estimate of\n\u22120.045 (0.179). neither differs significantly from zero. the corresponding baseline\nsurvival function estimates in the upper right panel of figure 10.22 suggest no need\nto stratify.\n\nsimilar analyses for subgroups of the data and the corresponding log-rank statistics\n\nalso show no significant treatment effects.\n\nhaving established that treatment has no effect on survival, we try constructing a\nmodel for prediction of survivor functions for new patients. this should be useful in\nassessing for whom liver transplant is a priority. the first step is to see which readily\naccessible covariates are highly predictive of survival. we exclude histologic stage,\nwhich requires a liver biopsy, and urine copper and sgot, which are frequently un-\nmeasured. the product-limit estimates and log rank statistics show strong dependence\nof failure on the other variables individually, so we fit a proportional hazards model\n\n "}, {"Page_number": 562, "text": "550\n\n10 \u00b7 nonlinear regression models\n\nvariable\n\nfull\n\nreduced\n\ntransformed\n\nfinal\n\nestimate (se)\n\nage\nalb\nalkphos\nascites\nbili\nedtrt\nhepmeg\n\n0.028 (0.009)\n\u22120.97 (0.027)\n0.015 (0.035)\n0.29 (0.31)\n0.11 (0.02)\n0.69 (0.32)\n0.49 (0.22)\nplatelet \u22120.61 (1.02)\n0.24 (0.08)\n\u22120.48 (0.26)\n0.29 (0.21)\n\nprotime\nsex\nspiders\n\n0.030 (0.009)\n\u22121.09 (0.24)\n\n0.033 (0.009)\n\u22123.06 (0.72)\n\n0.041 (0.009)\n\u22123.07 (0.72)\n\n0.11 (0.02)\n0.77 (0.31)\n0.50 (0.22)\n\n0.25 (0.08)\n\u22120.55 (0.25)\n0.30 (0.21)\n\n0.88 (0.10)\n0.79 (0.30)\n0.25 (0.22)\n\n0.88 (0.10)\n0.69 (0.30)\n\n3.01 (1.02)\n\n3.57 (1.13)\n\ny\nt\ni\nl\ni\n\nb\na\nb\no\nr\np\n\n \nl\n\ni\n\na\nv\nv\nr\nu\ns\n\n0\n\n1000 2000 3000 4000\n\ntime (days)\n\nl\n\ni\n\na\nu\nd\ns\ne\nr\n \ne\na\ng\nn\n\nl\n\ni\nt\nr\na\nm\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n1\n\n0\n\n1\n-\n\n2\n-\n\n3\n-\n\ny\nt\ni\nl\ni\n\nb\na\nb\no\nr\np\n\n \nl\n\ni\n\na\nv\nv\nr\nu\ns\n\nd\no\no\nh\n\ni\nl\n\ne\nk\n\ni\nl\n \n\ng\no\n\nl\n \n\ne\n\nl\ni\nf\n\no\nr\np\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n0\n\n1\n-\n\n2\n-\n\n3\n-\n\n4\n-\n\n0\n\n1000 2000 3000 4000\n\ntime (days)\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n\n.\n.\n\n.\n\n.\n\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n\n.\n\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.. .\n.\n.\n.\n..\n.\n..\n. .\n.\n.\n.\n.\n. .\n...\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n..\n. .\n.\n.\n..\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n-1\n\n0\n\n1\n\nlambda\n\n-1\n\n0\n\n1\n\n2\n\n3\n\nlog(bili)\n\nwith all but the excluded covariates. table 10.23 suggests that serum bilirubin is\nmost significant and that several other covariates can be dropped. backward selection\nbased on aic leads to the reduced model in the table. the likelihood ratio statistic\nfor comparison of the two models is 1.22, plainly insignificant. dropping sex and\n\ntable 10.23 parameter\nestimates and standard\nerrors for proportional\nhazards models fitted to\nthe pbc data. the full fit\nis reduced by backwards\nelimination. in the last\ntwo columns log\ntransformation is applied\nto alb, bili, and\nprotime.\n\nfigure 10.22 pbc data\nanalysis (fleming and\nharrington, 1991). top\nleft: product-limit\nestimates for control\n(solid) and treatment\n(dots) groups. top right:\nestimates of baseline\nsurvivor function for data\nstratified by sex, men\n(dots), women (solid). the\nheavy line shows the\nunstratified estimate.\nlower left: profile\nlikelihood for box\u2013cox\ntransformations of\nbilirubin (solid), albumin\n(dots), and prothrombin\ntime (dashes); the\nhorizontal line indicates\n95% confidence limits for\nthe transformation\nparameter. lower right:\nmartingale residuals from\nthe model with terms age,\nlog(alb), edtrt,\nlog(protime) against\nlog bilirubin, and lowess\nsmooth with p = 2/3.\n\n "}, {"Page_number": 563, "text": "10.8 \u00b7 survival data\n\n551\n\nspiders also leads to a likelihood ratio statistic of 7.29, with significance level 0.20\nwhen treated as \u03c7 2\n5 . bearing in mind the tendency of aic to overfit, we now ignore\nthese covariates.\n\nto investigate whether transformation is worthwhile we apply the box\u2013cox\napproach (example 8.23) to alb, bili, and protime. the lower left panel of\nfigure 10.22 clearly indicates log transformation of bili, but not of the other vari-\nables. the need for transformation of bili can also be assessed through the plot of\nmartingale residuals obtained when it is dropped, given in the lower right panel of the\nfigure. note the strong negative skewness of the residuals. the near-linearity of the\nlowess smooth shows the appropriateness of the transformation. the corresponding\nplot against bili itself is harder to read because the points are bunched towards zero.\nthe plots for alb and protime are more ambiguous. if we take logs of all three\nvariables, then the maximized log partial likelihood increases by 13.8 and hepmeg\ncan be dropped; see table 10.23.\n\na model with terms age+log(alb)+log(bili)+edtrt+log(protime) is med-\nically plausible. as the disease progresses, the liver\u2019s ability to produce albumin\ndecreases, leading to the negative coefficient for alb, while damage to the bile ducts\nreduces excretion of bilirubin and so increases its level in the body. edema is of-\nten associated with the later stages of the disease, while prothrombin is decreased,\nleading to slower clotting of the blood. finally and unsurprisingly, risk increases\nwith age.\n\nthe upper panels of figure 10.23 show deviance residuals plotted against age and\nprothrombin time. inspection of those in the left panel lying outside the 0.01 and 0.99\nnormal quantiles reveals an error in the data coding; case 253 has residual \u22122.55\nbut his age should be 54.4 rather than 78.4. the right panel shows an unusually high\nprothrombin time of 17.1, which should have been 10.7. the estimates after these\ncorrections are shown in the final column of table 10.23.\n\nthe lower left panel of figure 10.23 shows the scaled scores plotted against pro-\nthrombin time. there is some suggestion of non-proportionality, but it is too limited\nto suggest model failure. such plots for the other variables cast no doubt on propor-\ntionality of hazards, and we accept the model.\n\nto illustrate prediction, consider an individual with age=60, alb=4, bili=1,\n\nedtrt=0, and protime=8, for whom x t(cid:2)\u03b2 = \u22121.618 and whose hazard is reduced by\na factor exp(x t(cid:2)\u03b2) = 0.20 compared to baseline. setting edtrt=1 and bili=20 gives\nfound by solving for y the equation (cid:2)f0(y)exp(x t(cid:2)\u03b2) = 0.5.\n\nestimated risk scores of 0.4 and 2.8. the lower right panel of figure 10.23 shows how\nthe survivor functions then vary. the median estimated lifetime in each case can be\n(cid:1)\n\nthe proportional hazards model has been broadened in many directions. suppose,\nfor instance, that individuals move between states 1 and 2 and back again, baseline\ntime-dependent transition rates \u03b312(y) and \u03b321(y) being modified to \u03b312(y)\u03be12(\u03b2; x)\nand \u03b321(y)\u03be21(\u03b2; x) for an individual with explanatory variables x. the partial like-\nlihood for \u03b2 is a product of terms corresponding to each of the observed transitions\nbetween states. for instance, the contribution from transition 1 \u2192 2 attime y by an\n\n "}, {"Page_number": 564, "text": "552\n\n10 \u00b7 nonlinear regression models\n\n4\n\n \n \n \n \n \n \n \n \n \n \n\n2\n\n \n \n \n \n \n \n \n \n \n\n0\n\n2\n-\n\n4\n-\n\n0\n4\n\n0\n2\n\n0\n\n0\n2\n-\n\nl\n\na\nu\nd\ns\ne\nr\n \n\ni\n\ne\nc\nn\na\nv\ne\nd\n\ni\n\n)\ne\nm\n\ni\nt\n\no\nr\np\n(\ng\no\n\nl\n \nr\no\n\nf\n \n)\nt\n(\na\n\nt\n\ne\nb\n\n.\n\n.\n\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n..\n.\n..\n.\n..\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.. .\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.. .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n\n.\n\n.\n.\n\n.\n\n.\n.\n\n.\n\n30\n\n40\n\n50\n\n60\n\n70\n\n80\n\nage\n\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\u2022\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n\n.\n\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n.\n.\n..\n.\n.\n.\n...\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n........ .\n..\n. .\n......\n..\n. .\n.\n....\n... ..\n.\n.\n. ...\n.\n.\n... ....\n..\n. ..\n.... .\n.\n. . .\n..\n.\n. . .. .\n..\n..\n.\n. .\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n.\n.\n..\n..\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n..\n.\n..\n. .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n.\n.\n\n.\n\n.\n\n.\n.\n.\n\n.\n.\n.\n..\n\n.\n\n.\n\n10\n\n12\n\n14\n\n16\n\nprothrombin time\n\n\u2217\nj\n\nfigure 10.23 pbc data\nanalysis. upper panels:\ndeviance residuals plotted\nagainst age and\nprothrombin time, with\nhorizontal lines showing\n0.01 and 0.99 standard\nnormal quantiles. lower\nleft: scaled scores s\nplotted against\nprothrombin time, with\nlowess smooth and\napproximate 0.95\npointwise confidence\nbands (curved lines). also\nshown are overall estimate\nand 0.95 confidence\ninterval (horizontal lines).\nlower right: baseline\nsurvivor function estimate\n(heavy), with predicted\nsurvivor functions for\nindividuals with risk\nfactors 0.2, 0.4, and 2.8\n(top to bottom).\n\n.\n.\n.\n\nl\n\na\nu\nd\ns\ne\nr\n \n\ni\n\ne\nc\nn\na\nv\ne\nd\n\ni\n\ny\nt\ni\nl\ni\n\nb\na\nb\no\nr\np\n\n4\n\n \n \n \n \n \n \n \n \n \n\n2\n\n \n \n \n \n \n \n \n \n \n\n0\n\n2\n-\n\n4\n-\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n210\n\n1200 2400 3400\n\n0\n\n1000 2000 3000 4000\n\ntime\n\nsurvival time (days)\n\nindividual with covariates x j is\n\n(cid:13)\n\u03b312(y)\u03be12(\u03b2; x j )\n\u03b312(y)\u03be12(\u03b2; xk)\n\n(cid:13)\n= \u03be12(\u03b2; x j )\n\u03be12(\u03b2; xk)\n\n,\n\nthe sum being over individuals in state 1 at time y. individuals unobserved at y, or\nnot in state 1, do not appear in the sum. such extensions of partial likelihood enable\ninference for many types of partially observed and censored multi-state data, but\ndetails cannot be given here.\n\ncounting processes and martingale residuals\nconsider a random variable y with censoring indicator d and hazard function h(y),\nand let\n\nthis can be skipped at a\nfirst reading.\n\nv (y) = i (y \u2265 y), n (y) = i (y \u2264 y, d = 1),\n\nbe random variables that indicate whether y is in view at time y, and whether failure\nhas been observed by y. as v (y) isleft-continuous, its value at time y can be predicted\nthe moment before, y\n, whereas the counting process n (y) isright-continuous and\nso is not predictable. let {hy : y \u2265 0} denote the history of the process up to time y.\nthis is known as a filtration or increasing collection of sigma-algebras: hx \u2282 hy for\n\n\u2212\n\n "}, {"Page_number": 565, "text": "10.8 \u00b7 survival data\nx \u2264 y; knowledge accumulates. define also d n (y) = n{(y + dy)\nequals 1 if failure is observed to occur at y, and otherwise equals 0. then\ny \u2265 0;\n\u2212\n\ne{d n (y) | hy\u2212} =pr{d n (y) = 1 | hy\u2212} = h(y)v (y),\n\n553\n\u2212} \u2212 n (y), which\n\nthe mean failure rate at y can be predicted from the history to y\n. however potential\ndependence on hy\u2212 makes h(y)v (y) arandom variable.\nnow (10.64) implies that d m(y) = d n (y) \u2212 h(y)v (y) is azero-mean continuous-\ntime martingale with respect to hy\u2212, for all y > 0. this implies that\n\n(10.64)\n\n(cid:29)\n\n(cid:29)\n\ny\n\nm(y) =\n\nd m(y) = n (y) \u2212\nhas the property that for any y \u2265 x,\n\n0\n\ny\n\nh(u)v (u) du = n (y) \u2212 h{min(y, y )},\n\n0\n\ne{m(y) | hx} \u2212 m(x) = e\n(cid:29)\n=\n\n(cid:11)(cid:29)\n\ny\n\nx\n\n(cid:12)\n\n$$$$hx\n\nd m(u)\n\ny\n\ne[e{d m(u) | hu\u2212} |h x ] = 0,\n\nx\n\nand is therefore a martingale. thus e{m(y)} =0 for all y, and in particular\ne{m(\u221e)} =e{ d \u2212 h(y )} =0.\nlet independent variables y1, . . . ,y n with cumulative hazard functions h j (y)\nand censoring indicators d1, . . . , dn be observed, and set v j (y) = i (y j \u2265 y)\nand n j (y) = i (y j \u2264 y, d j = 1). the corresponding continuous-time martingale is\nm j (y) = n j (y) \u2212 h j{min(y, y j )}, from which martingale residuals are obtained by\nsetting y = \u221e and replacing unknowns with estimates.\n\ndevelopments of the above formulation are central to mathematical treatments of\n\ntime-to-event data, references to which are given in section 10.9.\n\nexercises 10.8\n1\n\n2\n\nshow that if y is continuous with cumulative hazard function h(y), then h(y ) has the unit\nexponential distribution. hence establish that e{h(y ) | y > c} =1 + h(c), and explain\nthe reasoning behind (10.55).\nlet y be a positive continuous random variable with survivor and hazard functions f(y)\nand h(y). let \u03c8(x) and \u03c7(x) bearbitrary continuous positive functions of the covariate x,\nwith \u03c8(0) = \u03c7(0) = 1. in a proportional hazards model, the effect of a non-zero covariate\nis that the hazard function becomes h(y)\u03c8(x), whereas in an accelerated life model, the\nsurvivor function becomes f{y\u03c7(x)}. show that the survivor function for the proportional\nhazards model is f(y)\u03c8(x), and deduce that this model is also an accelerated life model if\nand only if\n\nlog \u03c8(x) + g(\u03c4 ) = g{\u03c4 + log \u03c7(x)},\n\nwhere g(\u03c4 ) = log{\u2212 logf(e\u03c4 )}. show that if this holds for all \u03c4 and some non-unit \u03c7(x),\nwe must have g(\u03c4 ) = \u03ba\u03c4 + \u03b1, for constants \u03ba and \u03b1, and find an expression for \u03c7(x)\nin terms of \u03c8(x). hence or otherwise show that the classes of proportional hazards and\naccelerated life models coincide if and only if y has a weibull distribution.\n\n "}, {"Page_number": 566, "text": "10 \u00b7 nonlinear regression models\nin the usual notation for a linear regression model, x t(y \u2212 x(cid:2)\u03b2) = 0. by writing the partial\n\n(cid:13)\n\n554\n\n3\n\nlikelihood corresponding to (10.62) as\n\n(cid:27)\n\nn(cid:1)\nj=1\n\nd j \u2212 exp\n\nx j\n\nn\n\nj\n\nj=1 d j{x t\n\u03b2 \u2212 log a j (\u03b2)}, show that\n(cid:20)\n(cid:21)(cid:2)h0(y j )\n(cid:2)\u03b2\n\n= 0.\n\n(cid:28)\n\nx t\nj\n\n4\n\nwhich type of residual for a proportional hazards model is analogous to the raw residual\nin a linear model?\nsuppose that survival data data consist of independent observations (y j , c j ), j = 1 . . . ,n ,\nwhere y j is an exponential random variable with mean exp(x t\n\u03b2), censored at random,\nj\nand the censoring indicator is c j , which equals 0 if y j is censored and equals 1 otherwise.\nshow that the likelihood for these data is the same as if the counts c j had poisson\ndistributions with means y j exp(\u2212x t\n\u03b2). hence show that maximum likelihood estimates\nfor the censored data model, and their standard errors based on observed information, can\nbe obtained by regarding the censoring variable as having the poisson distribution with\nlog link function and offset log y j .\n5 write down the partial likelihood contributions from failure times y = 1, 2, for the data\nin table 10.22, using the model \u03be = exp{\u03b20 + \u03b21 i (group = 1) + \u03b22x}.\nsuppose that the continuous-time proportional hazards model holds, but that the failure\ntimes are grouped into intervals 0 = u0 < u1 < \u00b7\u00b7\u00b7 < um = \u221e. show that the correspond-\ning grouped hazards\n\n6\n\nj\n\nhi (x) = pr(y < ui | y \u2265 ui\u22121; x),\n\ni = 1, . . . ,m ,\n\nsatisfy\n\nlog{1 \u2212 hi (x)} =\u03be (\u03b2; x) log{1 \u2212 hi (0)},\n\nand write down the corresponding log likelihood when \u03be(\u03b2; x j ) = exp(x t\n\u03b2). hence find\nthe maximum likelihood estimator of \u03b2 when the hi (0) are treated as nuisance parameters.\ndoes this have the usual properties if n \u2192 \u221e and m is fixed?\n(prentice and gloeckler, 1978)\n\nj\n\n10.9 bibliographic notes\n\ndriven by the needs of applications, the literature on regression models has expanded\nhugely over the last 30 years, and most of the development has been in nonlin-\near modelling. generalized linear models were first explicitly formulated by nelder\nand wedderburn (1972), though others had previously suggested special cases. the\nresulting conceptual unification of apparently disparate models has had a major influ-\nence on subsequent developments, not least because of the part played by the iterative\nweighted least squares algorithm, for which green (1984) is a standard reference.\nmccullagh and nelder (1989) give an excellent account of generalized linear models\nand their ramifications, while dobson (1990) is more elementary. shorter accounts\nof generalized linear models and corresponding diagnostics are firth (1991), ?), and\ndavison and tsai (1992). j\u00f8rgensen (1997b) describes more general classes of expo-\nnential family-like distributions.\n\ndata with binary responses are discussed by collett (1991) and by cox and snell\n\n(1989).\n\nbishop et al. (1975) and fienberg (1980) are standard references to log-linear\nmodels, though their approach is rather different to that adopted here. log-linear and\n\n "}, {"Page_number": 567, "text": "10.10 \u00b7 problems\n\n555\n\nmarginal models are discussed in chapter 6 of mccullagh and nelder (1989), with\nmore recent work by liang et al. (1992), glonek and mccullagh (1995), and others.\ngeneralized estimating equations and marginal modelling are of great importance in\nlongitudinal data, a good discussion of which is given in diggle et al. (1994). agresti\n(1984) discusses models for ordinal data.\n\nquasi-likelihood was introduced by wedderburn (1974) in a seminal article. for\nsubsequent developments see mccullagh (1991), the useful survey by firth (1993),\nand davison (2001). heyde (1997) gives a longer more theoretical account. see also\nthe bibliographic notes for chapter 7.\n\nthere are now many books on semiparametric regression. bowman and azzalini\n(1997) give an elementary account of kernel methods, with an applied emphasis, while\nwand and jones (1995) contains a more theoretical treatment, and simonoff (1996)\ngives an excellent general discussion. fan and gijbels (1996) describe the theory of\nlocal polynomial modelling in detail, while the more practical account by loader\n(1999) includes references to purpose-written software for local fitting. hastie and\nloader (1993) give a shorter more intuitive account of the properties of these methods.\nthe account of spline methods in section 10.7.2 is based on green and silverman\n(1994). hastie and tibshirani (1990) give a book-length account of generalized ad-\nditive models. wood (2000) gives a recent account of smoothing parameter selection\nfor penalized likelihood procedures.\n\nsurvival data analysis has developed very rapidly over the last three decades. a\nmajor impetus was given by the introduction of the proportional hazards model by\ncox (1972), which led to greatly increased interest in the area, the use of point process\nmethods by aalen (1978), and a flood of subsequent work. fleming and harrington\n(1991) and andersen et al. (1993) are standard references to this topic, the latter also\ntreating event history analysis in other areas. therneau and grambsch (2000) is an\nexcellent recent book highlighting computation for proportional hazards models and\ntheir extensions, while hougaard (2000) is an account of more advanced topics such\nas frailty and multistate models. see also klein and moeschberger (1997). although\nmost developments have centred on the proportional hazards model, it is not always\nsuitable in practice, and many other possibilities have been suggested. see also the\nbibliographic notes to chapter 5.\n\n10.10 problems\n\n1 suppose that y has a density with generalized linear model form\n+ c(y; \u03c6)\n\nf (y; \u03b8, \u03c6) = exp\n\ny\u03b8 \u2212 b(\u03b8)\n\n,\n\n(cid:12)\n\n(cid:11)\n\na(\u03c6)\n\nwhere \u03b8 = \u03b8(\u03b7) and \u03b7 = \u03b2 tx.\n(a) show that the weight for iterative weighted least squares based on expected information\nis\n\nw = b\n\n(cid:5)(cid:5)\n\n(\u03b8)(d\u03b8/d\u03b7)2/a(\u03c6),\n\nand deduce that w\nthat the adjusted dependent variable is \u03b7 + (y \u2212 \u00b5)dg(\u00b5)/d\u00b5.\n\n\u22121 = v (\u00b5)a(\u03c6){dg(\u00b5)/d\u00b5}2, where v (\u00b5) isthe variance function, and\n\n "}, {"Page_number": 568, "text": "556\n\n10 \u00b7 nonlinear regression models\n\nnote that initial values are not required for \u03b2, since w and z can be determined in terms\nof \u03b7 and \u00b5; initial values can be found from y as \u00b51 = y and \u03b71 = g(y).\n(b) give explicit formulae for the weight and adjusted dependent variable when r = my\nis binomial with denominator m and probability \u03c0 = e\u03b7/(1 + e\u03b7).\n2 the independent observations y j , j = 1, . . . ,n , have poisson distributions with means\n\u00b5 j , where g(\u00b5 j ) = \u03b7 j , g(\u00b7) isthe link function, and \u03b7 j is the linear predictor x t\n\u03b2. the\nx j are p \u00d7 1 vectors of known covariates such that the matrix x whose jth row is x t\nhas rank p. show that the likelihood equation for the maximum likelihood estimator(cid:2)\u03b2 of\n\nj\n\nj\n\n\u03b2 can be written\n\nx ts((cid:2)\u03b2) = 0,\n\nand hence derive the iterative weighted least squares algorithm for estimation of \u03b2, giving\nexplicit formulae for the weight matrix and the adjusted dependent variable.\nin a set of data on faults in lengths of textile, there were y faults in independent samples of\nlength x. five pairs (y, x) were (6, 5.5), (4, 6.5), (17,8.3), (9, 3.8), and (14, 7.2). suppose\nthat the y j are independent poisson variables with means \u03b7 j , and \u03b7 j = \u03b20 + \u03b21x j . givethe\nlink function for this model, verify that the maximum likelihood estimates are(cid:2)\u03b20 = 1.006\nand(cid:2)\u03b21 = 1.437, and calculate their asymptotic covariance matrix. is there evidence that\n\u03b20 (cid:3)= 0?\n\n3 for a generalized linear model with known dispersion parameter \u03c6 and canonical link\nj is the contribution from the jth obser-\n\nj , where d 2\n\nn\nj=1 d 2\n\nfunction, write the deviance as\nvation. also let\n\nu j (\u03b2) = \u2202 log f (y j ; \u03b7 j , \u03c6)/\u2202\u03b7 j , w j = \u2212\u2202 2 log f (y j ; \u03b7 j , \u03c6)/\u2202\u03b72\n\n,\n\nj\n\n(cid:13)\n\n.\n\nx j u j\n\n(cid:4)(cid:2)\u03b2(k)\n\n\u22121 x tw 1/2, where w = diag{w1, . . . ,w n}.\n\ndenote the elements of the score vector and observed information, let x denote the\nn \u00d7 p matrix whose jth row is x t\nj , where \u03b7 j = \u03b2 tx j , and let h denote the matrix\n(a) let(cid:2)\u03b2(k) be the solution of the likelihood equation when case k is deleted,\nw 1/2 x(x tw x)\n(cid:1)\nj(cid:3)=k\n\n(cid:5) = 0,\nand let(cid:2)\u03b2 be the maximum likelihood estimate based on all n observations. use first-order\ntaylor series expansion of (10.65) about(cid:2)\u03b2 to show that\n\u22121xk\n\n.=(cid:2)\u03b2 \u2212 (x tw x)\n\nexpress(cid:2)\u03b2(k) in terms of the standardized pearson residual r pk = uk /{wk(1 \u2212 hkk)}1/2.\n\nuk((cid:2)\u03b2)\n1 \u2212 hkk\n\n(cid:2)\u03b2(k)\n\n(10.65)\n\n(b) use a second order taylor series expansion of the deviance to show that the change in\nthe deviance when the kth case is deleted is approximately\n+ hkkr 2\n\n= (1 \u2212 hkk)r 2\n\nwhere rdk is the standardized deviance residual dk /(1 \u2212 hkk)1/2.\n(c) suppose models a and b have deviances da and db. use (b) to find an expression\nfor the change in the likelihood ratio statistic da \u2212 db, when the kth case is deleted.\n(d) show that your results (a)\u2013(c) are exact in models with normal errors.\nin astudy on the relation between social class, education, and income, m independently\nsampled individuals are classified according to the social class of their parents, their income\ngroup, and their level of education. m is fixed in advance. the number of individuals with\nparents in class j, income group k, and with educational level l is y jkl, where j = 1, . . . , j ,\nk = 1, . . . , k and l = 1, . . . , l. show that the joint multinomial distribution for the y jkl\n(cid:13)\nwhich is appropriate to this sampling scheme is equivalent to that derived by treating the\nk jl y jkl =\ny jkl as independent poisson random variables with means \u00b5 jkl, conditional on\nm, and give the multinomial probabilities in terms of the \u00b5 jkl.\n\nr 2\ngk\n\ndk\n\npk\n\n,\n\n4\n\nrecall exercise 8.5.2.\n\nrgk = sign(yk \u2212(cid:2)\u00b5k )\n\n%\nr 2\ngk\n\nis called a jackknifed\ndeviance residual.\n\n "}, {"Page_number": 569, "text": "10.10 \u00b7 problems\n\n557\n\n(cid:13)\n\n\u03b1 j = (cid:13)\n\none possible model for such data would be that the multinomial probabilities \u03c0 jkl may be\nkl(\u03b2\u03b3 )kl = 1. show that the maximum\nwritten in the form \u03b1 j (\u03b2\u03b3 )kl, where\nlikelihood estimate for \u03b1 j is then y j\u00b7\u00b7/m, where a dot indicates summation over the cor-\nresponding subscript, and find the maximum likelihood estimates of the (\u03b2\u03b3 )kl. derive\nthe deviance statistic to test the adequacy of this model, and show that for large m it is\nequivalent to\n\nj\n\n1\nm\n\nwhen the model is correct.\n\n(cid:1)\n\n(my jkl \u2212 y j\u00b7\u00b7 y\u00b7kl)2\n\n,\n\njkl\n\ny j\u00b7\u00b7 y\u00b7kl\n\n5 the rate of growth of an epidemic such as aids for a large population can be estimated\nfairly accurately and treated as a known function g(t) oftime t. in asmaller area where\nfew cases have been observed the rate is hard to estimate because data are scarce. however\npredictions of the numbers of future cases in such an area must be made in order to allocate\nresources such as hospital beds. a simple assumption is that cases in the area arise in a\nnon-homogeneous poisson process with rate \u03bbg(t), for which the mean number of cases\ng(t)dt. suppose that n1 = n1 individuals with the disease have\nin period (t1, t2) is\u03bb\nbeen observed in the period (\u2212\u221e, 0), and that predictions are required for the number n2\nof cases to be observed in a future period (t1, t2).\n(a) find the conditional distribution of n2 given n1 + n2, and show it to be free of \u03bb. de-\nduce that a (1 \u2212 2\u03b1) prediction interval (n\u2212, n+) for n2 is found by solving approximately\nthe equations\n\nt2\nt1\n\n(cid:24)\n\n\u03b1 = pr(n2 \u2264 n\u2212|n1 + n2 = n1 + n\u2212),\n\u03b1 = pr(n2 \u2265 n+|n1 + n2 = n1 + n+).\n\n(b) use a normal approximation to the conditional distribution in (a) to show that for\nmoderate to large n1, n\u2212 and n+ are the solutions to the quadratic equation\n\n(1 \u2212 p)2n2 + p( p \u2212 1)\n\nn + n1 p\n\nn1 p \u2212 (1 \u2212 p)z2\n\n\u03b1\n\n(cid:6) = 0,\n\n(cid:3)\n\n(cid:5)\n\n(cid:4)\n\n\u03b1\n\n2n1 + z2\n(cid:11)(cid:29)\n\nt2\n\ng(t)dt /\n\nt1\n\n(cid:29)\n\nt2\n\nt1\n\np =\n\n(cid:12)\n\ng(t)dt\n\n.\n\n(cid:29)\n\n0\n\n\u2212\u221e\n\ng(t)dt +\n\nwhere \u0001(z\u03b1) = \u03b1 and\n\n(c) find approximate 0.90 prediction intervals for the special case where g(t) = 2t /2, so\nthat the doubling time for the epidemic is two years, n1 = 10 cases have been observed\nuntil time 0, and t1 = 0, t2 = 1 (next year) (cox and davison, 1989).\n6 let r0 and r1 be independent binomial variables with denominators m0 and m1 and\nprobabilities \u03c00 and \u03c01, and let \u0001 = {\u03c01(1 \u2212 \u03c00)}/{\u03c00(1 \u2212 \u03c01)} be the odds ratio for the\n2 \u00d7 2 table (r0, m0 \u2212 r0; r1, m1 \u2212 r1). let a = r0 + r1, and let y (s) = y (y \u2212 1)\u00b7\u00b7\u00b7\n(y \u2212 s + 1) = y !/(y \u2212 s)!, with y (s) = 0 if y + 1 \u2264 s.\n1 (m0 \u2212 r0)(s) | a = a} =\u0001 se{r(s)\n0 (m1 \u2212 r1)(s) | a = a}, and that\n(a) show that e{r(s)\n1 a(s)/(m0 + m1)(s).\n| a = a) = m(s)\nwhen \u0001 = 1, e(r(s)\n(b) when \u0001 = 1, show that\nvar(r1 | a = a) = m0m1a(m0 + m1 \u2212 a)\ne(r1 | a = a) = m1a\nm0 + m1\n(m0 + m1)2(m0 + m1 \u2212 1)\n\n(c) show that unconditionally {e(r1)e(m0 \u2212 r0)}/{e(r0)e(m1 \u2212 r1)} =\u0001, whereas\nconditionally on a,\n\n,\n\n1\n\n.\n\n{e(r1)e(m0 \u2212 r0) + var(r1)}/{e(r0)e(m1 \u2212 r1) + var(r1)} =\u0001.\n\nwhat does this indicate about the conditional maximum likelihood estimate of \u0001 relative\nto the unconditional one?\n\n "}, {"Page_number": 570, "text": "558\n\n10 \u00b7 nonlinear regression models\n\n(d) show that conditional on a, r1 has a generalized linear model density with\n\nb(\u03b8) = log\n\n(cid:7)\n\nu+(cid:1)\nu=u\u2212\n\n(cid:14)\n\n(cid:15)(cid:14)\n\nm1\nu\n\nm0\na \u2212 u\n\n(cid:15)\n\n(cid:8)\n\neu\u03b8\n\n, u\u2212 = max{0, a \u2212 m0}, u+ = min{m1, a}.\n\ndeduce that a score test of \u0001 = 1 based on data from n independent 2 \u00d7 2 tables\n(r0 j , m0 j \u2212 r0 j ; r1 j , m1 j \u2212 r1 j ) isobtained by treating\nr1 j as approximately nor-\nmal with mean and variance\nm1 j a j\nm0 j + m1 j\n\nm0 j m1 j a j (m0 j + m0 j \u2212 a j )\n(m0 j + m1 j )2(m0 j + m1 j \u2212 1)\n\n;\n\n,\n\nn(cid:1)\nj=1\n\nn(cid:1)\nj=1\n\n(cid:13)\n\nwhen continuity-corrected this is the mantel\u2013haenszel test.\n(mantel and haenszel, 1959)\n7 suppose that the cumulant-generating function of x can be written in the form m{b(\u03b8 +\nt) \u2212 b(\u03b8)}. let e(x) = \u00b5 = mb\n(\u03b8) and let \u03ba2(\u00b5) and \u03ba3(\u00b5) bethe variance and third\ncumulant respectively of x, expressed in terms of \u00b5; \u03ba2(\u00b5) isthe variance function v (\u00b5).\n(a) show that\n\n(cid:5)\n\n\u03ba3(\u00b5) = \u03ba2(\u00b5)\u03ba(cid:5)\n\n2(\u00b5)\n\nand\n\n\u03ba3\n\u03ba 2\n2\n\n= d\nd\u00b5\n\nlog \u03ba2(\u00b5).\n\nverify that the binomial cumulants have this form with b(\u03b8) = log(1 + e\u03b8 ).\n(b) show that if the derivatives of b(\u03b8) are all o(1), then y = g(x) isapproximately\nsymmetrically distributed if g satisfies the second-order differential equation\n\nshow that if \u03ba2(\u00b5) and \u03ba3(\u00b5) are related as in (a), then\n\n3\u03ba 2\n\n2 (\u00b5)g\n\n(cid:5)\n\n(\u00b5)\u03ba3(\u00b5) = 0.\n\n(cid:5)(cid:5)\n\n(\u00b5) + g\n(cid:29)\n\nx\n\ng(x) =\n\n\u22121/3\n2\n\n\u03ba\n\n(\u00b5)d\u00b5.\n\n(c) hence find symmetrizing transformations for poisson and binomial variables.\n(mccullagh and nelder, 1989, section 4.8)\n\n8 show that the chi-squared density with known degrees of freedom \u03bd,\n\n(cid:20)\n\n(cid:21)\n\ny\u03bd/2\u22121\n\n2\u03bd/2\u03c3 \u03bd \u0001(\u03bd/2)\n\nexp\n\n\u2212 y\n2\u03c3 2\n\ny > 0, \u03c3 > 0, \u03bd = 1, 2, . . . ,\n\n,\n\ncan be written in generalized linear model form (10.14) , where \u03b8 and \u03c6 are functions, to\nbe found, of \u03bd and \u03c3 2. hence derive an expression for its rth cumulant, r \u2265 1.\nthe yield of an industrial process was measured ri times independently at m different\ntemperatures ti . the resulting yields zi j , i = 1, . . . ,m , j = 1, . . . , ri may be assumed to\non ti . explain how the sums of squares yi = (cid:13)\nbe independent and normally distributed with both means \u03b6i and variances \u03c4i dependent\nri\nj=1 zi j ,\nmay be used to assess the dependence of variance on temperature in a suitable generalized\nlinear model. briefly discuss the advantages and disadvantages of the canonical link\nfunction of your model.\n9 at each of the doses x1 < x2 < \u00b7\u00b7\u00b7 < xn of a drug, m animals are tested. at dose xi , ri\nanimals respond. derive the maximum likelihood equation when the linear predictor takes\nthe form \u03b7 = \u03b2x when a probit link function is used. if only one dosage x0 > 0 isused,\nshow that\n\nj=1(zi j \u2212 z i )2, where z i = r\n\n(cid:13)\n\n\u22121\ni\n\nri\n\n(cid:2)\u03b2 = 1\n\nx0\n\n\u0001\u22121(r/m),\n\nvar((cid:2)\u03b2)\n\n.= \u0001(\u03b2x0){1 \u2212 \u0001(\u03b2x0)}\n\n,\n\n{\u03c6(\u03b2x0)}2\n\nmx 2\n0\n\nwhere \u03c6 and \u0001 are the standard normal density and distribution functions. plot the function\n\u0001(\u03b7){1 \u2212 \u0001(\u03b7)}/\u03c6(\u03b7)2 for \u03b7 in the range \u22123 \u2264 \u03b7 \u2264 3, and comment on the implications\nfor the choice of x0 if there is some prior knowledge of the likely value of \u03b2.\n\n "}, {"Page_number": 571, "text": "table 10.24 simulated\ndata with two covariates,\nbinary response, and fitted\nvalues.\n\nhe usually comes\ndisguised as elvis, but\nattends statistical\ncongresses in the guise of\nan eminent statistician;\nthis may account for the\nother-worldly discussion.\n\n10.10 \u00b7 problems\n\n559\n\ncase\n\nx1\n\nx2\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n3.7\n3.5\n1.25\n0.75\n0.8\n0.7\n0.6\n1.1\n0.9\n0.9\n\n0.83\n1.09\n2.50\n1.50\n3.2\n3.5\n0.75\n1.70\n0.75\n0.45\n\ny\n\n1\n1\n1\n1\n1\n1\n0\n0\n0\n0\n\n(cid:2)y\n\n0.999\n0.999\n0.875\n0.066\n0.886\n0.921\n0.005\n0.320\n0.017\n0.008\n\n10 let y be binomial with probability \u03c0 = e\u03bb/(1 + e\u03bb) and denominator m.\n\n(a) show that m \u2212 y is binomial with \u03bb(cid:5) = \u2212\u03bb. consider\n(cid:15)\n\n(cid:14)\n\n\u02dc\u03bb = log\n\ny + c1\nm \u2212 y + c2\n\nas an estimator of \u03bb. show that in order to achieve consistency under the transformation\ny \u2192 m \u2212 y , wemust have c1 = c2.\n(b) write y = m\u03c0 + \u221a\n\nm\u03c0(1 \u2212 \u03c0)z, where z = o p(1) for large m. show that\n\ne{log(y + c)} =log(m \u03c0) + c\nm\u03c0\n\n\u2212 1 \u2212 \u03c0\n\n2m\u03c0\n\n+ o(m\n\n\u22123/2).\n\nfind the corresponding expansion for e{log(m \u2212 y + c)}, and with c1 = c2 = c find the\nvalue of c for which \u02dc\u03bb is unbiased for \u03bb to order m\nwhat is the connection to the empirical logistic transform?\n(cox, 1970, section 3.2)\n\n\u22121.\n\n11 arcturian society is surprisingly similar to ours, the main differences being that arcturians\nhave three eyes (left, centre, and right) and are better at quantum physics. their statis-\ntics is relatively rudimentary. on a recent study visit to our planet an arcturian statis-\ntician encountered marginal models and decided to use one for visual impairment\ndata similar to those in table 10.16. he set up a 2 \u00d7 2 \u00d7 2 table of probabilities\n(\u03c0000, \u03c0001; \u03c0010, \u03c0011; \u03c0100, \u03c0101; \u03c0110, \u03c0111) and used logistic models with marginal prob-\nabilities \u03c0l = \u03c0100 + \u03c0101 + \u03c0110 + \u03c0111 and so forth, and odds ratios\n\n\u03b3lc = pr(l = c = 1)pr(l = c = 0)\npr(l = 1, c = 0)pr(l = 0, c = 1)\n\u03b3l r = pr(l = r = 1)pr(l = r = 0)\npr(l = 1, r = 0)pr(l = 0, r = 1)\nshow that the corresponding odds ratio \u03b3c r may be expressed as\n\n,\n\n.\n\npr(c = r = 1){1 \u2212 \u03c0c \u2212 \u03c0r + pr(c = r = 1)}\n{\u03c0c \u2212 pr(c = r = 1)}{\u03c0r \u2212 pr(c = r = 1)} ,\n\nand that pr(c = r = 1) lies between min(\u03c0c , \u03c0r) and max(0, pr(l = c = 1) + pr(l =\nr = 1) \u2212 \u03c0l). deduce that if \u03c0l, \u03c0c , \u03c0r, \u03b3lc and \u03b3l r are fixed, then the range of values\nthat \u03b3c r can take is limited by the other parameters. what problems of fitting and inter-\npretation might be encountered with such a model? compare this with the corresponding\nlog-linear model.\n\n12 the data in table 10.24 are from an experiment with a binary response in which two covari-\n\nates are fitted. the parameter estimates and their standard errors are(cid:2)\u03b20 = \u22129.530(3.224),\n(cid:2)\u03b21 = 3.882(1.425), and(cid:2)\u03b22 = 2.649(0.9121). the table gives the data and fitted values for\n\nthe first ten of the 39 observations. the overall deviance for the model is 29.77.\n\n "}, {"Page_number": 572, "text": "560\n\n13\n\n14\n\n10 \u00b7 nonlinear regression models\n\nn\nj=1\n\n\u03bb ju 2\n\nj , where u1, . . . , un\n\ngive a careful interpretation of the effect of the covariates on the response.\nverify that the fitted value for case 8 is correct.\nis the deviance a useful guide to the fit of the model?\n(a) in (10.43), show that pobs(h) = pr0(yt ay \u2265 0), where a is a n \u00d7 n real symmetric\nmatrix.\n(b) let y \u223c nn(\u00b5, \u0001), where \u0001 = l l t and l is a non-singular lower triangular matrix.\nif \u00b5 = 0, then show that yt ay d= (cid:13)\niid\u223c n (0, 1) and \u03bb1 \u2265\n\u00b7\u00b7\u00b7 \u2265 \u03bbn are the eigenvalues of l t al. deduce that the rth cumulant of yt ay equals\n\u03bar = 2r\u22121(r \u2212 1)!tr{(\u0001a)r}. show that the same is true whenever \u00b5 lies in the null space\nof a.\n(c) for a simple approximation to the distribution of yt ay, wematch its frst three cu-\nmulants with those of the random variable aw + c,where w \u223c \u03c7 2\nb . show that this gives\na = |\u03ba3|/(4\u03ba2), b = 8\u03ba 3\n3 and c = \u03ba1 \u2212 ab. outline how this can be used to approximate\npobs(h).\n(d) compute the cumulant-generating function of yt ay, and develop a saddlepoint ap-\nproximation to its distribution.\n(azzalini et al., 1989; azzalini and bowman, 1993; kuonen, 1999)\nin the penalized least squares setup of section 10.7.2, with t0 < t1 < \u00b7\u00b7\u00b7 < tn < tn+1, set\nh j = t j+1 \u2212 t j for each j = 1, . . . ,n \u2212 1, let g1, . . . , gn and \u03b31, . . . , \u03b3n be arbitrary real\nnumbers, and define\n\n/\u03ba 2\n\n2\n\ng(t) = (t \u2212 t j )g j+1 + (t j+1 \u2212 t)g j\n(cid:11)(cid:14)\n\nh j\n\n(cid:15)\n\n(cid:14)\n\n(cid:15)\n\n(cid:12)\n\n\u2212 1\n6\n\n(t \u2212 t j )(t j+1 \u2212 t)\n\n1 + t \u2212 t j\non each interval t j \u2264 t \u2264 t j+1, j = 1, . . . ,n \u2212 1.\n(a) show that on each such interval g(t) is acubic function with g(t j ) = g j .\n(b) show that\n\n1 + t j+1 \u2212 t\n\n\u03b3 j+1 +\n\nh j\n\nh j\n\n\u03b3 j\n\n(t) = g j+1 \u2212 g j\n\nh j (2\u03b3 j + \u03b3 j+1),\n\n\u2212 1\n6\n\nh j\n(cid:5)\n\n(t) = g j+1 \u2212 g j\n\nh j\n\nlim\nt\u2191t j+1\n\ng\n\n+ 1\n6\n\nh j (\u03b3 j + 2\u03b3 j+1),\n\n(cid:5)\n\ng\n\nlim\nt\u2193t j\n\nand\n\n(cid:5)(cid:5)\n\ng\n\n(t) = (t \u2212 t j )\u03b3 j+1 + (t j+1 \u2212 t)\u03b3 j\n\n,\n\n(cid:5)(cid:5)(cid:5)\n\n(t) = h\n\ng\n\nj (\u03b3 j+1 \u2212 \u03b3 j ),\n\u22121\n\nt j \u2264 t \u2264 t j+1,\n\nand hence deduce that g\n(c) show that\n\nh j\n(t j ) = \u03b3 j .\n(cid:5)(cid:5)\n\n(cid:5)\n\ng\n\n(t1) = g2 \u2212 g1\nt2 \u2212 t1\n\n\u2212 1\n6\n\n(t2 \u2212 t1)\u03b32,\n\n(cid:5)\n\ng\n\n(tn) = gn \u2212 gn\u22121\ntn \u2212 tn\u22121\n\n\u2212 1\n6\n\n(tn \u2212 tn\u22121)\u03b3n\u22121,\n\nand deduce that if g(t) is to be anatural cubic spline, then we must define\n\n(cid:11)\n\ng(t) =\n\ng1 \u2212 (t1 \u2212 t)g\n(cid:5)\n(t1),\ngn + (t \u2212 tn)g\n(cid:5)\n(tn),\n\nt \u2264 t1,\nt \u2265 tn,\n\nindependent of the values of t0 and tn+1. deduce that \u03b31 = \u03b3n = 0.\n(d) we have seen that g(t) iscontinuous, with continuous first derivative at t1 and tn, and\n(t) = limt\u2193t j g\n(t) = \u03b3 j for each j. if g(t) is to be anatural cubic spline,\n(cid:5)(cid:5)\nthat limt\u2191t j g\n(t) = limt\u2193t j g\n(t) for each j = 2, . . . ,n \u2212 1. show that this\n(cid:5)\nit must also satisfy limt\u2191t j g\n\n(cid:5)(cid:5)\n\n(cid:5)\n\n "}, {"Page_number": 573, "text": "recall that \u03b31 = \u03b3n = 0.\n\n10.10 \u00b7 problems\n\n561\n\nimplies that\n\ng j+1 \u2212 g j\n\u2212 g j \u2212 g j\u22121\nh j\u22121\nh j\nj = 2, . . . ,n \u2212 1,\n\n= 1\n6\n\nh j\u22121\u03b3 j\u22121 + 1\n3\n\n(h j\u22121 + h j )\u03b3 j + 1\n6\n\nh j \u03b3 j+1,\n\nand that this system of equations may be rewritten as qtg = r\u03b3 , where gt = (g1, . . . , gn),\n\u03b3 t = (\u03b32, . . . , \u03b3n\u22121), and q and r have dimensions n \u00d7 (n \u2212 2) and (n \u2212 2) \u00d7 (n \u2212 2);\nit is necessary to label the columns of q from 2 to n \u2212 2 and both rows and columns of\nr likewise, so their top left elements are respectively q12 and r22.\n(e) use integration by parts to show that the integral in (10.46) may be written\n\n(t)}2 dt = n\u22121(cid:1)\n\ntn+1\n\n(cid:5)(cid:5)\n\n{g\n\n(cid:29)\n\nt0\n\n\u03b3 j+1 \u2212 \u03b3 j\n\nh j\n\n(g j \u2212 g j+1),\n\nj=1\n\nand deduce that the integral may be written \u03b3 t qtg = gt k g.\n(f) write down q and r when n = 5 and h1 = \u00b7\u00b7\u00b7 = hn\u22121 = 1. show that r is then\ninvertible, and give k = q r\n(green and silverman, 1994, pp. 22\u201325)\n\n\u22121 qt.\n\n15 (a) let u1, . . . , un be independent exponential variables with parameters \u03bb1, . . . , \u03bbn, and\nlet h0(u) be adifferentiable monotone increasing function of u > 0, with derivative h0(u).\nshow that y1 = h0(u1), . . . ,y n = h0(un) have joint density\n\u03bb j h0(y j ) exp{\u2212\u03bb j h0(y j )}.\n\nn(cid:25)\nj=1\n\n(cid:10)\n\n(cid:9)\nn(cid:1)\ni= j\n\nn(cid:25)\nj=1\n\n\u03bb( j)(cid:13)\nn\ni= j\n\n(b) show that the joint density of u1, . . . , un may be written as\nexp(\u2212\u03bb( j)u( j)),\n\n\u00d7 n(cid:25)\n\n(10.66)\nwhere the elements of the rank statistic r = {(1), . . . ,(n )} are determined by the ordering\non the failure times, u(1) < \u00b7\u00b7\u00b7 < u(n). establish that the first term of this product is\ninvariant to transformations y = h0(u ) but that the second is not.\n(c) suppose that \u03bb j = exp(x t\non the first term of (10.66) only.\n\n\u03b2). give an argument why inference for \u03b2 should be based\n\n\u03bb(i)\n\n\u03bb(i)\n\nj=1\n\nj\n\n16 in figure 5.8, let y represent time on trial and t calendar time, and suppose that the hazard\n\u2020\n0(t) exp(x t\u03b2), where h0(y)\nfunction for an individual with covariates x has form h0(y)h\n\u2020\n0(t) abaseline hazard for calendar time.\nrepresents a baseline hazard for time on trial and h\ndiscuss how partial likelihood inference might be generalized to account for inclusion of\n\u2020\n0(t), which is included to allow for changes in medical practice during the course of the\nh\ntrial.\n17 consider independent exponential variables y j with densities \u03bb j exp(\u2212\u03bb j y j ), where \u03bb j =\nexp(\u03b20 + \u03b21x j ), j = 1, . . . ,n , where x j is scalar and\nx j = 0 without loss of generality.\nestimator(cid:2)\u03b21 has asymptotic variance (nm2)\n(a) find the expected information for \u03b20, \u03b21 and show that the maximum likelihood\n\u22121, where m2 = n\n(cid:7)\n(cid:5)(cid:8)\nn(cid:1)\ni= j\n\n(b) under no censoring, show that the partial log likelihood for \u03b21 equals\n\n\u2212 n(cid:1)\n\n\u03b21x(i)\n\n(cid:13)\n\n(cid:13)\n\nwhere the elements of the rank statistic r = {(1), . . . ,(n )} are determined by the ordering\non the failure times, y(1) < \u00b7\u00b7\u00b7 < y(n). deduce the information in the partial likelihood is\n\nx 2\nj .\n\nexp\n\nj=1\n\nlog\n\n\u22121\n\n(cid:4)\n\n,\n\nir(\u03b21) = n(cid:1)\n\nj=1\n\ner{m2, j (\u03b21) \u2212 m1, j (\u03b21)2},\n\n "}, {"Page_number": 574, "text": "562\n\n10 \u00b7 nonlinear regression models\n\nwhere the expectation is over the distribution of r and\n\nmk, j (\u03b21) =\n\nshow that when \u03b21 = 0,\n\n(cid:4)\n\n(cid:13)\n(cid:13)\nn\ni= j x k\nn\ni= j exp\n\n(cid:4)\n(i) exp\n\n\u03b21x(i)\n\n(cid:5)\n\n\u03b21x(i)\n\n(cid:5)\n\n.\n\ner{m2, j (\u03b21)} =m 2, er{m1, j (\u03b21)2} = m2\nn \u2212 1\n\nn(cid:1)\ni=1\n\ni \u2212 1\nn \u2212 i + 1\n\n,\n\nand hence find the efficiency of partial likelihood estimation of \u03b21 relative to maximum\nlikelihood estimation. compute this for n = 2, 5, 10, 20, 50, 100, and comment.\n(c) it can be shown that as n \u2192 \u221e for small \u03b21, the relative efficiency equals exp(\u2212m2\u03b22\n1 ).\nshow that in the two-sample problem with equal numbers of observations in each group\nand x j = \u00b11/2, the relative efficiency exceeds 0.75 when |\u03b21| < 1.07, corresponding to\na ratio of failure rates between the two groups in the range (1/3, 3). discuss.\n(kalbfleisch, 1974)\nand that the m events occur at 0 < y1 < \u00b7\u00b7\u00b7 < ym < y0, inprocesses j1, . . . , jm.\n(a) show that the probabilities that the first event occurs at y1 and that given this it has\ntype j1 are respectively\n\n18 suppose that n independent poisson processes of rates \u03bb j (y) are observed simultaneously,\n\n(cid:8)\n\n\u03bb j (y1)\n\nexp\n\n(cid:7)\n\nn(cid:1)\nj=1\n(cid:29)\n\ny0\n\n(cid:7)\n\n\u2212 n(cid:1)\n\n(cid:8)\n\ny1\n\n\u03bb j (u) du\n\n,\n\n0\n\n(cid:8)\n\n(cid:29)\n\n(cid:7)\n\n\u2212 n(cid:1)\nj=1\n(cid:7)\nn(cid:1)\nj=1\n\nm(cid:25)\ni=1\n\n(cid:8)\n\nm(cid:25)\ni=1\n\n(cid:13)\n\n\u03bb ji (yi )\nn\nj=1\n\n(cid:13)\n\n\u03bb j1(y1)\nn\nj=1\n\n\u03bb j (y1)\n\n.\n\nhence interpret the quantities\n\n0\n\n,\n\nj=1\n\nexp\n\n\u03bb j (yi )\n\n\u03bb j (u) du\n\n(10.67)\n(b) now suppose that \u03bb j (y) = h0(y)\u03be{\u03b2; x j (y)}v j (y), where h0(y) is abaseline hazard\nfunction, \u03be{\u03b2; x j (y)} depends on parameters \u03b2 and time-varying covariates x j (y), and\nv j (y) is apredictable process, with v j (y) = 1 if the jth process is in view at time y, and\nv j (y) = 0 ifnot. thus if the jth process is censored at time c j , v j (y) = 0, y > c j . ifr i\nis the set { j : v j (yi ) = 1}, show that the second term in (10.67) equals\n\n\u03bb j (yi )\n\n.\n\nm(cid:25)\ni=1\n\n(cid:13)\n\n\u03be{\u03b2; x ji (yi )}\nj\u2208ri\n\n\u03be{\u03b2; x j (yi )} .\n\nhow does this specialize for time-varying explanatory variables in the proportional hazards\nmodel?\n\n19 two individuals with cumulative hazard functions u h1(y1) and u h2(y2) are independent\n\nconditional on the value u of a frailty u whose density is f (u).\n(a) for this shared frailty model, show that\nf(y1, y2) = pr(y1 > y1, y2 > y2) =\n\n(cid:29) \u221e\n\nexp{\u2212u h1(y1) \u2212 u h2(y2)} f (u) du.\n\nif f (u) = \u03bb\u03b1u\u03b1\u22121 exp(\u2212\u03bbu)/ \u0001(\u03b1), for u > 0 is a gamma density, then show that\n\n0\n\nf(y1, y2) =\n\n\u03bb\u03b1\n\n{\u03bb + h1(y1) + h2(y2)}\u03b1 ,\n\ny1, y2 > 0,\n\nand deduce that in terms of the marginal survivor functions f1(y1) and f2(y2) ofy 1 and\ny2,\n\n(cid:6)\u2212\u03b1\n\u22121/\u03b1 \u2212 1\nwhat happens to this joint survivor function as \u03b1 \u2192 \u221e?\n\nf(y1, y2) = (cid:3)f1(y1)\n\n\u22121/\u03b1 + f2(y2)\n\n,\n\ny1, y2 > 0.\n\n "}, {"Page_number": 575, "text": "10.10 \u00b7 problems\n\n563\n\n(b) find the likelihood contributions when both individuals are observed to fail, when one\nis censored, and when both are censored.\n(c) extend this to k individuals with parametric regression models for survival.\n\u2212su ) = exp(\u2212\u03b4s \u03b1/\u03b1), 0 < \u03b1 \u2264 1.\n\n20 a positive stable random variable u has e(e\n\n(a) show that if y follows a proportional hazards model with cumulative hazard function\nu exp(x t\u03b2)h0(y), conditional on u = u, then y also follows a proportional hazards model\nunconditionally. are \u03b2, \u03b1, and \u03b4 estimable from data with single individuals only?\n(b) consider a shared frailty model, as in the previous question, with positive stable u .\nshow that the joint survivor function may be written as\n\n(cid:4)\u2212&{\u2212 logf1(y1)}1/\u03b1 + {\u2212 logf2(y2)}1/\u03b1\n\n\u2019\u03b1(cid:5)\n\n,\n\nin terms of the marginal survivor functions f1 and f2. show that if the conditional cumu-\nlative hazard functions are weibull, u hr (y) = u\u03ber y\u03b3 , \u03b3 > 0, r = 1, 2, then the marginal\nsurvivor functions are also weibull. show also that the time to the first event has a weibull\ndistribution.\n\nf(y1, y2) = exp\n\ny1, y2 > 0,\n\n21 consider individuals arising in k independent clusters of sizes n1, . . . ,n k, and such that\nconditional on the values u1, . . . ,u k of unobserved frailties u1, . . . , uk, the individuals in\nthe ith cluster have survival times independently distributed according to a proportional\nhazards model with cumulative hazards ui \u03bei j h0(yi j ), for j = 1, . . . ,n i , where \u03bei j is short-\nhand for \u03be(\u03b2; xi j ), xi j being a vector of explanatory variables. let h0(y) be thederivative\nof h0(y), and suppose that the ui are independent gamma variables with unit means and\nshape parameter \u03b8.\n(a) if the survival times are subject to non-informative censoring, show that the joint\ndensity of ui and the (survival time, censoring indicator) pairs (yi j , di j ) for the ith cluster\nis\n\n(cid:8)\n\n{ui \u03bei j h0(yi j )}di j \u00d7 exp\n\nui \u03bei j h0(yi j )\n\n\u00d7 \u03b8 \u03b8 u\u03b8\u22121\n\ni\n\n\u0001(\u03b8)\n\nexp(\u2212\u03b8ui ),\n\n(cid:7)\n\n\u2212 n(cid:1)\n\nj=1\n\nni(cid:25)\nj=1\n\nj=1\n\nj=1\n\n\u03bei j h0(yi j ),\n\ndi\u00b7 = ni(cid:1)\n\nbi = \u03b8 + ni(cid:1)\n\nand deduce that the conditional means of ui and of log ui given the observed data are\nwi (\u03b8, \u03b2) = ai /bi and \u03c8(ai ) \u2212 log bi , where\ndi j , \u03c8(\u03b1) = d log \u0001(\u03b1)/d\u03b1.\nai = \u03b8 + di\u00b7,\ndiscuss the merits and demerits (if any) of inference in terms of \u03c8 = \u03b8\u22121: what happens\nas \u03c8 \u2192 0?\n(b) show that a step of the em algorithm for estimation of (\u03b8, \u03b2) involves updating (\u03b8(cid:5), \u03b2(cid:5)\nby maximization of (cid:6)1(\u03b2, h0) + (cid:6)2(\u03b8) over \u03b2, h0, and \u03b8, where\n(cid:6)1(\u03b2, h0) = k(cid:1)\n(cid:6)2(\u03b8) = k(cid:1)\ni=1\n= wi (\u03b8(cid:5), \u03b2(cid:5)\n) and a\n\nni(cid:1)\nj=1\n{(\u03b8 + di\u00b7 \u2212 1)(\u03c8(a\n\n). extend the argument leading to\nw\n(10.57) to establish that the step for \u03b2 involves maximizing the partial likelihood that is a\n\n[di j{log \u03bei j + log h0(yi j )} \u2212w\n\n} +k {\u03b8 log \u03b8 \u2212 log \u0001(\u03b8)},\n\ni ) \u2212 ai \u03b8/b\n(cid:5)\n\ni ) \u2212 log b\n(cid:5)\n\n(cid:5)\ni and b\n\n\u03bei j h0(yi j )],\n\ni=1\n\n(cid:5)\ni\n\n(cid:5)\ni\n\n(cid:5)\ni\n\n)\n\nproduct over individuals of terms(cid:7)\n\ni are evaluated at (\u03b8(cid:5), \u03b2(cid:5)\n(cid:5)\n(cid:8)di j\n\n(cid:13)\n\n(cid:5)\nw\ni\nk\u2208ri j\n\n\u03be(\u03b2; xi j )\n\n(cid:5)\ni\n\nw\n\n\u03be(\u03b2; xk)\n\n,\n\nwith the risk set ri j containing those individuals from every cluster available to fail at\nfailure time yi j . when \u03be(\u03b2; x) = exp(x t\u03b2), show that this amounts to using an offset in the\nproportional hazards model. find the form of (cid:2)h0(y), and give an algorithm for estimation\n\nof \u03b2 and \u03b8.\n\n "}, {"Page_number": 576, "text": "564\n\n(c) show that the joint survivor function for the individuals in a cluster is\n\npr(yi1 > yi1, . . . ,y ini\n\n> yini ) = \u03b8 \u03b8\n\n10 \u00b7 nonlinear regression models\n\u03b8 + ni(cid:1)\n\n(cid:12)\u2212\u03b8\n\n,\n\n\u03bei j h0(y j )\n\n(cid:11)\n\nj=1\n\nand hence give the log likelihood contribution from (yi j , di j ), for j = 1, . . . ,n i . explain\nhow to use this to obtain the observed information matrix for \u03b8 and \u03b2 based on the estimates\nobtained in (b).\n(klein, 1992)\n\n22 let y1, . . . ,y n be independent exponential variables with hazards \u03bb j = exp(\u03b2 tx j ).\n\n(a) show that the expected information for \u03b2 is x t x, inthe usual notation.\n(b) now suppose that y j is subject to uninformative right censoring at time c j , sothat y j\nis a censoring time or a failure time as the case may be. show that the log likelihood is\n\n(cid:6)u (\u03b2) =\n\n(cid:1)\n\n\u03b2 tx j \u2212 n(cid:1)\n\nj=1\n\nf\n\nexp(\u03b2 tx j )y j ,\n\n(cid:13)\n\nf denotes a sum over observations seen to fail. if the jth censoring-time is\nwhere\nexponentially distributed with rate \u03ba j , show that the expected information for \u03b2 is x t x \u2212\nx tc x, where c = diag{c1, . . . ,c n}, and c j = \u03ba j /(\u03ba j + \u03bb j ) isthe probability that the jth\nobservation is censored. what is the implication for estimation of \u03b2 if the c j are constant?\n(c) sometimes a variable w j has been measured which can act as a surrogate response\nvariable for censored individuals. we formulate this as w j = z j /u j , where z j is the\nunobserved remaining life-time of the jth individual from the moment of censoring, and\nu j is a noise component which has a fixed distribution independent of the censoring time\nand of x j . owing to the exponential assumption, the excess life z j is independent of y j\nif censoring occurred. if u j has gamma density\n\u03b1\u03bau\u03ba\u22121 exp(\u2212\u03b1u)/ \u0001(\u03ba),\n\n\u03b1, \u03ba > 0, u > 0,\n\nshow that w j has density\n\n\u03bb j \u03ba\u03b1\u03ba /(\u03b1 + \u03bb j w)\u03ba+1, w > 0.\n\n(cid:1)\n\n(cid:3)\n\nshow that the log likelihood for the data, including the additional information in\nthe w j , is\n\n(cid:6)(\u03b2) = lu (\u03b2) +\n(cid:13)\nc denotes a sum over censored individuals, and we have assumed that \u03b1 and \u03ba\n\n\u03b2 tx j + log \u03ba + \u03ba log \u03b1 \u2212 (\u03ba + 1) log\n\n\u03b1 + e\u03b2t x j w j\n\n,\n\nc\n\n(cid:5)(cid:6)\n\n(cid:4)\n\nwhere\nare known. show that the expected information for \u03b2 is\nx t x \u2212 2/(\u03ba + 2)x tc x,\n\nand compare this with (b). explain qualitatively in terms of the variability of the distribution\nof u why the loss of information decreases as \u03ba increases.\n(cox, 1983)\n\n "}, {"Page_number": 577, "text": "11\n\nbayesian models\n\nevery statistical investigation takes place in a context. information about what ques-\ntion is to be addressed will suggest what data are needed to give useful answers. before\nthe data are available, one role for this information is to suggest suitable probability\nmodels. there may also be information about the values of unknown parameters,\nand if this can be expressed as a probability density, an approach to inference based\non bayes\u2019 theorem is possible. many statisticians make the stronger claim that this\ntheorem provides the only entirely consistent basis for inference, and insist on its use.\nthis chapter outlines some aspects of the bayesian approach to modelling. we first\ngive an account of basic uses of bayes\u2019 theorem and of the role and construction\nof prior densities. we then turn to inference, dealing with analogues of confidence\nintervals, tests, approaches to model criticism, and model uncertainty. until recently\ncomputational difficulties placed realistic bayesian modelling largely out of reach,\nbut over the last 20 years there has been rapid progress and complex models can\nnow be fitted routinely. section 11.3 gives an account of bayesian computation, first\nof analytical approaches based on integral approximations, and then of monte carlo\nmethods. the chapter concludes with brief introductions to hierarchical and empirical\nbayesian procedures.\n\n11.1 introduction\n11.1.1 bayes\u2019 theorem\nlet a1, . . . , ak be events that partition a sample space, and let b be an arbitrary event\non that space for which pr(b) > 0. then bayes\u2019 theorem is\n\npr(a j | b) = pr(b | a j )pr(a j )\n(cid:1)\ni=1 pr(b | ai )pr(ai )\n\nk\n\n.\n\nthis reverses the order of conditioning by expressing pr(a j | b) interms of\npr(b | a j ) and the marginal probability pr(b) inthe denominator. for continuous\n\n565\n\n "}, {"Page_number": 578, "text": "566\n\nrandom variables y and z,\n\nf z|y (z | y) =\n\n11 \u00b7 bayesian models\n\n(cid:2)\n\nfy|z (y | z) f z (z)\nfy|z (y | z) f z (z) dz\n\n,\n\n(11.1)\n\nprovided the marginal density f (y) > 0, with integration replaced by summation for\ndiscrete variables.\n\ninference\nto see how bayes\u2019 theorem is used for inference, suppose that there is a probability\nf (y | \u03b8) for data y. inearlier chapters we have written f (y | \u03b8) = f (y; \u03b8),\nmodel\nbut here we use the conditional notation to emphasize that the probability model is a\ndensity for the data given the value of \u03b8. suppose also that we are able to summarize\nour beliefs about \u03b8 in a prior density, \u03c0(\u03b8), constructed separately from the data y.\nthis implies that we think of the unknown value \u03b8 that underlies our data as the\noutcome of a random variable whose density is \u03c0(\u03b8), just as our probability model is\nthat the data y are the observed value of a random variable y with density f (y | \u03b8).\nonce the data have been observed, our beliefs about \u03b8 are contained in its conditional\ndensity given that y = y,\n\n\u03c0(\u03b8 | y) = \u03c0(\u03b8) f (y | \u03b8)\n\u03c0(\u03b8) f (y | \u03b8) d\u03b8\n\n(cid:2)\n\n(11.2)\nthis is the posterior density for \u03b8 given y. note that f (y | \u03b8) isthe likelihood for \u03b8\nbased on y, sothat in terms of \u03b8, we haveposterior \u221d prior \u00d7 likelihood.\n\n.\n\nfrequentist inference treats \u03b8 as an unknown constant, whereas the bayesian ap-\nproach treats it as a random variable. we make this distinction explicit by using \u03c0 to\ndenote a density for \u03b8, which thus has prior and posterior densities \u03c0(\u03b8) and \u03c0(\u03b8 | y),\nrather than f (\u03b8) and f (\u03b8 | y).\n\nit is useful to note that any quantity that does not depend on \u03b8 cancels from the\ndenominator and numerator of (11.2). this implies that if we can recognise which\ndensity is proportional to (11.2), regarded solely as a function of \u03b8, wecan read off the\nposterior density of \u03b8. furthermore, the factorization criterion (4.15) implies that the\nposterior density depends on the data solely through any minimal sufficient statistic\nfor \u03b8.\n\nexample 11.1 (bernoulli trials) suppose that conditional on \u03b8, the data y1, . . . , yn\nare a random sample from the bernoulli distribution, for which pr(y j = 1) = \u03b8 and\n1 \u2212 pr(y j = 0) = \u2212\u03b8, where 0 < \u03b8 <1. the likelihood is\nwhere r = (cid:1)\n\nl(\u03b8) = f (y | \u03b8) = n(cid:3)\n\n\u03b8 y j (1 \u2212 \u03b8)1\u2212y j = \u03b8 r (1 \u2212 \u03b8)n\u2212r ,\n\n0 < \u03b8 <1,\n\nj=1\n\ny j .\n\na natural prior here is the beta density with parameters a and b,\n\n\u03c0(\u03b8) =\n\n1\n\n\u03b8 a\u22121(1 \u2212 \u03b8)b\u22121,\n\n(11.3)\nwhere b(a, b) isthe beta function \u0001(a)\u0001(b)/ \u0001(a + b). figure 5.4 shows (11.3) for\nvarious values of a and b.\n\n0 < \u03b8 <1,\n\na, b > 0,\n\nb(a, b)\n\n\u0001(a) = (cid:2) \u221e\n\n0 ua\u22121e\n\n\u2212u du is\nthe gamma function; see\nexercise 2.1.3.\n\n "}, {"Page_number": 579, "text": "11.1 \u00b7 introduction\n\nthe posterior density of \u03b8 conditional on the data is given by (11.2), and is\n\n\u03c0(\u03b8 | y) =\n\n(cid:2)\n\u221d \u03b8 r+a\u22121(1 \u2212 \u03b8)n\u2212r+b\u22121,\n\n\u03b8 r+a\u22121(1 \u2212 \u03b8)n\u2212r+b\u22121/b(a, b)\n\u03b8 r+a\u22121(1 \u2212 \u03b8)n\u2212r+b\u22121 d\u03b8/b(a, b)\n0 < \u03b8 <1.\n\n1\n0\n\n567\n\n(11.4)\n\nas (11.3) has unit integral for all positive a and b, the constant normalizing (11.4)\nmust be b(a + r, b + n \u2212 r). therefore\nb(a + r, b + n \u2212 r)\n\n\u03b8 r+a\u22121(1 \u2212 \u03b8)n\u2212r+b\u22121,\n\n\u03c0(\u03b8 | y) =\n\n0 < \u03b8 <1.\n\n1\n\nthus the posterior density of \u03b8 has the same form as the prior: acquiring data has\nthe effect of updating (a, b) to (a + r, b + n \u2212 r). as the mean of the b(a, b) density\nis a/(a + b), the posterior mean is (r + a)/(n + a + b), and this is roughly r/n in\nlarge samples. hence the prior density inserts information equivalent to having seen\na sample of a + b observations, of which a were successes. if we were very sure\n.= 1/2, for example, we might take a = b very large, giving a prior density\nthat \u03b8\ntightly concentrated around \u03b8 = 1/2, whereas taking smaller values of a and b would\nincrease the prior uncertainty.\nto illustrate this, suppose that a = b = 1, so that the initial density of \u03b8 is the\nabout \u03b8. then data with n = 23 and r = (cid:1)\nuniform prior shown in the upper right panel of figure 5.4, representing ignorance\ny j = 14 update the prior density to the\n(cid:1)\n\nposterior density in the lower right panel.\n\nthe use of the beta density as prior for a model whose likelihood is proportional\nto \u03b8 r (1 \u2212 \u03b8)s leads to a posterior density that is also beta. this is an example of a\nconjugate prior, anidea discussed in section 11.1.3.\n\nwhen the parameter takes one of a finite number of values, labelled 1, . . . ,k , with\nprior probabilities \u03c01, . . . , \u03c0k, the posterior density is the probability mass function\n\npr(\u03b8 = j | y) = \u03c0 j f (y | \u03b8 = j)\n\u03c0i f (y | \u03b8 = i)\n\n(cid:1)\nk\ni=1\n\n.\n\n(11.5)\n\nexample 11.2 (diagnostic tests) a disease occurs with prevalence \u03b3 in a pop-\nulation, and \u03b8 indicates that an individual has the disease. hence pr(\u03b8 = 1) = \u03b3 ,\npr(\u03b8 = 0) = 1 \u2212 \u03b3 . adiagnostic test gives a result y , whose distribution is f1(y) for\na diseased individual and f0(y) otherwise. the commonest type of test declares that\na person is diseased if y > y0, say, where y0 is fixed on the basis of past data. the\nprobability that a person is diseased, given a positive test result, is\n\npr(\u03b8 = 1 | y > y0) =\n\n\u03b3{1 \u2212 f1(y0)}\n\n\u03b3{1 \u2212 f1(y0)} +(1 \u2212 \u03b3 ){1 \u2212 f0(y0)} ;\n\nthis is sometimes called the positive predictive value of the test. its sensitivity and\nspecificity are 1 \u2212 f1(y0) and f0(y0). these are the probabilities of correct classifica-\ntion of diseased and non-diseased persons, while the false negative and false positive\nratios are f1(y0) and 1 \u2212 f0(y0). one aims to construct tests whose sensitivity and\n(cid:1)\nspecificity are as high as possible.\n\n "}, {"Page_number": 580, "text": "568\n\n11 \u00b7 bayesian models\n\nprediction\nprediction of the value of a future random variable, z, isstraightforward when there\nis a prior density for the parameters. the joint density of z and the data y may be\nwritten\n\n(cid:4)\n\nf (y, z) =\n\nf (z | y, \u03b8) f (y | \u03b8)\u03c0(\u03b8) d\u03b8,\n\nand hence once y has taken the value y, inference for z is based on its posterior\npredictive density,\n\n(cid:4)\n\nf (z | y) =\n\nf (z | y, \u03b8)\u03c0(\u03b8 | y) d\u03b8 =\n\n(cid:2)\n\n(cid:2)\nf (z | y, \u03b8) f (y | \u03b8)\u03c0(\u03b8) d\u03b8\n\nf (y | \u03b8)\u03c0(\u03b8) d\u03b8\n\n.\n\n(11.6)\n\nthis is (11.1) expanded to make explicit\ndensity of \u03b8.\n\nthe integration over the posterior\n\nexample 11.3 (bernoulli trials) heads occurs r times among the first n tosses in\na sequence of independent throws of a coin. what is the probability of a head on the\nnext throw?\nlet \u03b8 be the unknown probability of a head and let z = 1 indicate the event that\nthe next toss yields a head. conditional on \u03b8, pr(z = 1 | y, \u03b8) = \u03b8 independent of\nthe data y so far. if the prior density for \u03b8 is beta with parameters a and b, then\n\npr(z = 1 | y) =\n\n(cid:4)\n(cid:4)\n\n1\n\n0\n\n1\n\npr(z = 1 | \u03b8, y)\u03c0(\u03b8 | y) d\u03b8\n\u03b8 a+r\u22121(1 \u2212 \u03b8)b+n\u2212r\u22121\n=\nb(a + r, b + n \u2212 r)\n= b(a + r + 1, b + n \u2212 r)\nb(a + r, b + n \u2212 r)\n\nd\u03b8\n\n\u03b8\n\n0\n\n= a + r\na + b + n\n\n,\n\non using results for beta functions; see example 11.1 and exercise 2.1.3. as n, r \u2192\n\u221e, this tends to the sample proportion of heads r/n, sothe prior information is\n(cid:1)\ndrowned by the sample.\n\n11.1.2 likelihood principle\nthere have been many attempts to justify the use of bayes\u2019 theorem as a basis for\ninference. one line of argument rests on axioms that individuals can use to make\noptimal decisions in the face of uncertain events, and leads to the view that probability\nis a measure of personal belief about the world, to be updated by additional knowledge\nusing bayes\u2019 theorem. an account of this would take us too far afield, and instead we\noutline another argument, which centres on principles intended to guide inference.\nthe force of this is that two basic principles \u2014 the sufficiency and conditionality\nprinciples \u2014 together imply a third \u2014 the likelihood principle \u2014 which is difficult\nto apply except through bayes\u2019 theorem. many statisticians do subscribe to the first\ntwo, at least implicitly, thus setting them on the path to bayesian inference.\n\n "}, {"Page_number": 581, "text": "11.1 \u00b7 introduction\n\n569\n\nwe begin by introducing the notion of an experiment e, which yields data y, on\nwhich we wish to base inference about \u03b8 through the evidence ev(e , y). the form of\nthis function need not be specified; we merely suppose that it exists and contains all\nthe information about \u03b8 based on e and y.\n\nsufficiency and conditionality principles\nthe form of the sufficiency principle we shall use is that if an experiment e could give\nrise to y1 and y2, but that there is a statistic s(\u00b7) sufficient for \u03b8 such that s(y1) = s(y2),\nthen any inference for \u03b8 should be the same whether y1 or y2 is observed, that is\nev(e , y1) = ev(e , y2). this is widely accepted, as the factorization criterion (4.15)\nimplies that given the sufficient statistic, the data contain no further information\nabout \u03b8.\n\na second principle can be motivated by the following classic example.\n\nexample 11.4 (measuring machines) suppose that a physical quantity \u03b8 can be\nmeasured by two machines, both giving normal measurements y with mean \u03b8. a\nmeasurement from the first machine has unit variance, but one from the second\nhas variance 100. the more precise machine is often busy, while the second is\nused only if the first is unavailable; the upshot is that each is equally likely to be\nused. thus if a takes value 1 or 2 depending on the machine used, pr(a = 1) =\npr(a = 2) = 1\n2 .\n\nsuppose that an observation obtained is from machine 1. then clearly any inference\nabout \u03b8 should not take into account that machine 2 might have been used, when it\nis known that it was not. mathematically this is expressed by saying that the revelant\ndistribution for inference about \u03b8 is the conditional distribution of y given a, rather\nthan the unconditional distribution of y . for example, the conditional 95% confidence\ninterval for \u03b8 given that a = 1 is y \u00b1 1.96, whereas the unconditional interval is\ny \u00b1 16.45, which is clearly much too long if it is known that y came from the n (\u03b8, 1)\n(cid:1)\ndistribution.\n\nthe lesson of this is formalized as follows. suppose that an experiment e can\nbe thought of as arising in two stages. in the first stage we observe that a random\nvariable a with known distribution independent of \u03b8 takes value a, and in the second\nstage we observe ya from a component experiment ea. this is a mixture experiment,\nfor which the data are (a, ya). then one form of the conditionality principle says\nthat ev{e , (a, ya)} =ev( ea, ya): the evidence concerning \u03b8 based on the compound\nexperiment e is equal to the evidence from the component experiment ea actually\nperformed, the results of other possible components being irrelevant. the key point is\nthat since the distribution of a does not depend on \u03b8, conditioning on a does not lead\nto a loss of information about \u03b8, but selects the relevant component of the mixture\nexperiment. this principle is widely, even if sometimes unconsciously, accepted; we\ndiscuss its implications in more detail in chapter 12.\n\n "}, {"Page_number": 582, "text": "570\n\n11 \u00b7 bayesian models\n\nlikelihood principle\nsuppose that two experiments relating to \u03b8, e1 and e2, give rise to data y1 and y2\nsuch that the corresponding likelihoods are proportional, that is, for all \u03b8,\n\nl(\u03b8; y1, e1) = cl(\u03b8; y2, e2).\n\nthen according to one expression of the likelihood principle, ev(e1, y1) =\nev(e2, y2): inference should be based on the observed likelihood alone. full ac-\nceptance of this means rejecting frequentist tools such as significance tests, as the\nfollowing example shows.\n\nexample 11.5 (bernoulli trials) suppose that e1 consists of observing the number\ny1 of successes in a fixed number n1 of independent bernoulli trials. the likelihood\nis then\n\nl1(\u03b8) =\n\n\u03b8 y1(1 \u2212 \u03b8)n1\u2212y1 ,\n\n0 < \u03b8 <1,\n\ncorresponding to the binomial number of successful trials.\n\nexperiment e2 consists of conducting bernoulli trials independently until y2 suc-\n\ncesses occur, at which point there have been n2 trials. here the likelihood,\n\nl2(\u03b8) =\n\n\u03b8 y2(1 \u2212 \u03b8)n2\u2212y2 ,\n\n0 < \u03b8 <1,\n\n(cid:5)\n\n(cid:6)\n\nn1\ny1\n\n(cid:5)\n\n(cid:6)\nn2 \u2212 1\ny2 \u2212 1\n\ncorresponds to the negative binomial number of trials up to y2 successes.\nnow suppose that it happens that n1 = n2 = n and y1 = y2 = y, giving l1(\u03b8) \u221d\nl2(\u03b8). then according to the likelihood principle, inferences based on the two exper-\niments should be the same. but consider testing the hypothesis h0 : \u03b8 = 1\n2 against\nthe alternative that \u03b8 < 1\n2 . in e1, the test statistic would be the random number of\n(cid:5)\nsuccesses, y , and the p-value would be\n\n(cid:6)\n\n(cid:6)\n\n(cid:5)\n\ny \u2264 y | \u03b8 = 1\n2\n(cid:6)\n\npr\n\n(cid:5)\n\nn \u2265 n | \u03b8 = 1\n2\n\n=\n\n= y(cid:7)\nr=0\n(cid:5)\n\u221e(cid:7)\nm=n\n\n\u2212n,\n\n2\n\nn\nr\n\n(cid:6)\nm \u2212 1\ny \u2212 1\n\n\u2212m .\n\nwhile in e2 the test statistic would be the total number of trials, n , with p-value\n\n2\n\npr\n\n(11.8)\nthe catch is that (11.7) and (11.8) need not be equal. for example, if y = 3 and n = 12,\nthe p-values are respectively 0.073 and 0.033, conveying different evidence against\nh0. inparticular, use of the fixed significance level 0.05 would lead to acceptance or\nrejection of h0 depending on the experiment performed. the reason for this is that\n(11.7) and (11.8) involve summation over portions of two different sample spaces.\nthis conflicts with the likelihood principle, according to which only the data actually\n(cid:1)\nobserved should contribute to the inference.\n\nconstruction of tail probabilities such as (11.7) or (11.8), or of confidence inter-\nvals, involves consideration of data not actually observed, and thereby disobeys the\nlikelihood principle. this poses a problem for frequentist procedures, because a ra-\ntional statistician who rejects the likelihood principle should also reject one of the\n\n(11.7)\n\n "}, {"Page_number": 583, "text": "11.1 \u00b7 introduction\n\n571\n\napparently reasonable sufficiency and conditionality principles, which together entail\nthe likelihood principle.\nto see this, suppose that we accept the sufficiency and conditionality principles,\nand that experiments e1 and e2 have yielded data y1 and y2 such that l(\u03b8; y1, e1) =\ncl(\u03b8; y2, e2) for some c > 0 and all \u03b8. consider the mixture experiment e that\nconsists of observing (ea, ya), where a is the observed value of the binary random\nvariable such that\n\npr(a = 1) = 1\nc + 1\n\n,\n\npr(a = 2) = c\nc + 1\n\n;\n\nthe distribution of a is independent of \u03b8. the outcomes for e are (e1, y1) and (e2, y2),\nand the decomposition pr(ea, ya; \u03b8) = pr(ya | ea; \u03b8)pr(ea) shows that the corre-\nsponding likelihoods,\n\n1\nc + 1\n\nl(\u03b8; y1, e1),\n\nc\nc + 1\n\nl(\u03b8; y2, e2),\n\nare equal for all \u03b8. since the likelihood function is itself a minimal sufficient statistic\nfor \u03b8 (exercise 4.2.11), the sufficiency principle implies\n\nev{e , (e1, y1)} =ev{ e , (e2, y2)}.\n\n(11.9)\n\nbut the conditionality principle implies\n\nev{e , (e1, y1)} =ev( e1, y1), ev{e , (e2, y2)} =ev( e2, y2),\n\nand combined with (11.9) we get ev(e1, y1) = ev(e2, y2). thus acceptance of the\nsufficiency and conditionality principles implies acceptance of the likelihood princi-\nple. the converse is also true (problem 11.6). in fact it can be shown that a stronger\nversion of the conditionality principle on its own implies the likelihood principle.\n\nstatisticians attempting to weaken the force of this argument have criticized its cen-\ntral notions of evidence and mixture experiments, or have insisted that the sufficiency\nand conditionality principles apply only in a more limited way. they can then accept\nsome form of these principles but not the conclusion of the argument, and continue\nto use such tools as confidence intervals and p-values. others deny the validity of the\nargument on the grounds that it applies only to models known to be true, and this is\nrare in practice.\n\nstatisticians who embrace the likelihood principle find themselves in an awkward\nposition: their inference should be based on the observed likelihood, l(\u03b8), but how\nshould it be expressed? in particular, what can be inferred about a scalar component\nof vector \u03b8? the obvious solution of profiling over the other components of \u03b8 can go\nbadly awry, as we shall see in chapter 12, and the alternative of integrating them out\ndoes not give a unique answer (problem 11.7). thus the idea of multiplying l(\u03b8) by\na prior density and applying the simple recipe of bayes\u2019 theorem starts to appear very\nattactive. moreover, we see from (11.2) that given a particular prior \u03c0(\u03b8), bayesian\ninference for \u03b8 does conform to the likelihood principle, because any constants in\nf (y | \u03b8) donot appear in the posterior density.\n\n "}, {"Page_number": 584, "text": "572\n\n11 \u00b7 bayesian models\n\n11.1.3 prior information\ndespite its conformity to the likelihood principle, inference based on bayes\u2019 theorem\nhas often been seen as controversial. this is not due to the result itself, which\nsimply states mathematically how the probability density of one random variable\nchanges when another has been observed, but because its use in statistical inference\nfor \u03b8 requires the investigator to treat \u03b8 as a random variable, and to specify a\nprior density \u03c0(\u03b8) separate from the data. a key issue is the interpretation and\nchoice of \u03c0.\n\nin some circumstances it is uncontroversial to treat \u03b8 as random. at one extreme\nthe data at hand may be the latest in a stream of similar datasets, each having an\nunderlying parameter that may be supposed to be drawn from a distribution. for\nexample, an accountant may wish to estimate the level of errors in a company\u2019s\nbooks, \u03b8, based on a sample of transactions that reveals y errors. it will be sensible\nto treat \u03b8 as randomly chosen from a density \u03c0(\u03b8) oferror rates based on experience\nwith previous firms. then inference on \u03b8 will use both y and \u03c0(\u03b8). an example in despite this, the london\ncourt of appeal (regina\nthe use of forensic evidence is when there is a close match between dna profile data\nvs. adams, 1996, 1997)\nfrom the scene of a crime and a suspect. then a database of prior profiles may help\nruled that \u2018introducing\nbayes\u2019 theorem . . . into a\nto establish whether dna found at the scene of the crime could plausibly have come\ncriminal trial plunges the\nfrom someone else. in these applications the prior information has a frequentist basis,\njury into inappropriate\nand unnecessary realms of\nso new issues of interpretation do not arise.\ncomplexity, deflecting\nthem from their proper\ntask\u2019.\n\nat the other end of the range of possibilities is the situation where the data are to\nbe used to make subjective decisions such as \u2018should i bet on the outcome of this\nrace?\u2019 although likely to depend on how facts such as \u2018flatfoot has not won a race\nthis season\u2019 are viewed, both model and prior information here reflect a personal\njudgement. here bayes\u2019 theorem provides the mechanism for updating prior beliefs\nin the light of whatever data is available, but the inference is a personal assessment\nof the evidence and has no claim to objective force.\n\nthe debate arises when the prior information does not have a frequency interpre-\ntation, but the inference required is not purely personal. many statisticians regard\nthe information in data as being qualitatively different from their prior beliefs about\nmodel parameters, and hence find it unacceptable to use bayes\u2019 theorem to com-\nbine the two. they argue that although the choice of probability model is usually\na matter of individual judgement, that judgement can be checked by comparing the\ndata and fitted model, while by definition prior information cannot be checked di-\nrectly. to which a bayesian might reply that the epistemological distinction between\ndata, model, and prior is unclear, because collection of any data must be based on\nsome prior belief, which will often include information about possible models and\nthe likely values of their parameters. furthermore bayes\u2019 theorem provides a single\nrecipe for inference about unknowns, while frequentist notions such as confidence\nintervals can violate what seem reasonable principles of inference. much has been\nwritten on this, but we shall avoid getting embroiled, simply noting that in many situ-\nations the bayesian approach is simpler and more direct than frequentist alternatives,\nand that when they can be compared, the inferences produced by bayesian and good\n\n "}, {"Page_number": 585, "text": "11.1 \u00b7 introduction\n\n573\n\nfrequentist procedures are often rather similar, so that the practical consequences of\nchoosing between them are usually not critical. when a frequentist inference dif-\nfers strongly from any conceivable bayesian one, it seems wise to pause and reflect\nawhile.\n\nwhatever its interpretation, a prior must be specified in order for bayesian analysis\n\nto proceed. we now consider aspects of this.\n\nconjugate densities\nin example 11.1 the combination of a beta prior density for a probability and the\nlikelihood for several bernoulli trials led to a beta posterior density. although too\ninflexible to encompass the range of prior knowledge that arises in applications, such\nconjugate combinations of prior and likelihood are useful because of their simple\nclosed forms. they are closely tied to exponential family models.\n\nexample 11.6 (exponential family) suppose that y1, . . . , yn is a random sample\nfrom the exponential family (5.12)\n\nso that in terms of s = (cid:1)\n\nf (y | \u03c9) = exp{s(y)t\u03b8(\u03c9) \u2212 b(\u03c9)} f0(y),\ns(y j ), the likelihood is proportional to\n\nexp{s t\u03b8(\u03c9) \u2212 nb(\u03c9)}.\n\n(11.10)\n\nif the prior density for \u03c9 depends on the quantities \u03be and \u03bd and has form\n\n\u03c0(\u03c9) = exp{\u03be t\u03b8(\u03c9) \u2212 \u03bdb(\u03c9) + c(\u03be, \u03bd)},\n\nthen the posterior density is proportional to\n\nexp{(\u03be + s)t\u03b8(\u03c9) \u2212 (\u03bd + n)b(\u03c9)}.\n\nprovided this is integrable the posterior density therefore must be\n\n\u03c0(\u03c9 | y) = exp{(\u03be + s)t\u03b8(\u03c9) \u2212 (\u03bd + n)b(\u03c9) + c(\u03be + s, \u03bd + n)}.\n\nthus the prior parameters (\u03be, \u03bd) are updated to (\u03be + s, \u03bd + n) bythe data. one inter-\npretation of the hyperparameters \u03be and \u03bd is that the prior information is equivalent to\n\u03bd prior observations summing to \u03be.\nfor example, the poisson density with mean \u03c9 has kernel exp(y log \u03c9 \u2212 \u03c9), so the\nconjugate prior must have kernel exp(\u03be log \u03c9 \u2212 \u03bd\u03c9). for \u03be, \u03bd > 0, this is proportional\nto the gamma density with mean \u03be /\u03bd, whose density is\n\n\u03c0(\u03c9) = \u03bd\u03be \u03c9\u03be\u22121\n\n\u0001(\u03be)\n\n\u2212\u03bd\u03c9, \u03c9 > 0,\n\ne\n\nand which is therefore the conjugate prior for the poisson mean. as the data update\n(\u03be, \u03bd) to (\u03be + s, \u03bd + n), the posterior density\n\u03c0(\u03c9 | y) = (\u03bd + n)\u03be+s \u03c9\u03be+s\u22121\n\n\u2212(\u03bd+n)\u03c9, \u03c9 > 0,\n\ne\n\n\u0001(\u03be + s)\n\nalso has gamma form.\n\n(cid:1)\n\n "}, {"Page_number": 586, "text": "(cid:1)\n\ny is the sample average\n\u22121\nn\n\ny j .\n\n574\n\n11 \u00b7 bayesian models\n\nexample 11.7 (normal distribution) let y1, . . . , yn be a normal random sample\n(cid:5)\nwith mean \u00b5 and known variance \u03c3 2. the likelihood is\n\u221d exp\n\n(y j \u2212 \u00b5)2\n\n(cid:8)\n\n(cid:9)\n\n(cid:6)\n\n\u00b52\n\n\u00b5\n\n1\n\n,\n\nny\n\u03c3 2\n\n\u2212 n\n\u03c3 2\n\n1\n2\n\n(2\u03c0 \u03c3 2)n/2 exp\n\nn(cid:7)\nj=1\n\n\u2212 1\n2\u03c3 2\n\nwhich is of form (11.10) with s = ny/\u03c3 2, k = n/\u03c3 2, a(\u00b5) = \u00b5, and \u03ba(\u00b5) = 1\ntherefore the conjugate prior is proportional to\n\u2212 1\n\u03c4 2\n\n\u00b50\n\u03c4 2\n\n(cid:5)\n\n(cid:6)\n\nexp\n\n\u00b52\n\n\u00b5\n\n,\n\n2\n\n1\n2\n\n\u00b52.\n\nand must be the normal density with mean \u00b50 and variance \u03c4 2. the effect of the data\nis to update (\u00b50\u03c4\u22122, \u03c4\u22122) to (\u00b50\u03c4\u22122 + s\u03c3 \u22122, \u03c4\u22122 + n\u03c3 \u22122), so the posterior density\nfor \u00b5 is normal with mean and variance\nny/\u03c3 2 + \u00b50/\u03c4 2\nn/\u03c3 2 + 1/\u03c4 2\n\nn/\u03c3 2 + 1/\u03c4 2\n\n(11.11)\n\n1\n\n,\n\n.\n\non writing the mean in (11.11) as\n\nny + (\u03c3 2/\u03c4 2)\u00b50\n\nn + \u03c3 2/\u03c4 2\n\n,\n\nwe see that the prior injects information equivalent to \u03c3 2/\u03c4 2 observations with mean\n\u00b50, and shrinks the sample average, y, towards the prior mean by an amount that\ndepends on the ratio of \u03c4 2 to \u03c3 2/n. asn \u2192 \u221e or \u03c4 2 \u2192 \u221e, corresponding to in-\ncreasing information in the data relative to the prior, the posterior density becomes\nnormal with mean y and variance \u03c3 2/n, sothe effect of the prior withers away. as\n\u03c4 2 \u2192 0, corresponding to more definite prior knowledge, the posterior approaches\n(cid:1)\nthe normal density with mean \u00b50 and variance \u03c4 2, which is the prior.\n\nconjugate priors are often too restrictive for expression of realistic prior infor-\nmation, but it is straightforward to establish that mixtures of conjugate densities are\nalso conjugate, and this considerably broadens the class of priors with closed-form\nposterior densities (problem 11.3).\n\nignorance\nsometimes the prior density must express prior ignorance about a parameter. one\nreason for this may be the need for a \u2018baseline\u2019 analysis as a basis for discussion.\nanother is the belief that a non-informative prior will allow the data \u2018to speak for\nthemselves\u2019, though it seems optimistic to think that they will spill their secrets without\ncareful interrogation. nevertheless it is important to weigh how much an inference\ndepends on the prior compared to the data. one way to do this is to contrast inferences\nfrom a minimally informative prior with those from the prior actually used.\nwhen \u03b8 has bounded support, as in example 11.1, a uniform prior density, with\n\u03c0(\u03b8) \u221d 1, seems an obvious choice. when the support of \u03b8 is unbounded, such a\nprior has infinite integral and so is improper. animproper prior may nevertheless\nlead to a proper posterior density. in example 11.7, for example, we can represent\n\n "}, {"Page_number": 587, "text": "11.1 \u00b7 introduction\n575\ncomplete ignorance about the prior value of \u00b5 by letting \u03c4 2 \u2192 \u221e, inwhich case the\nprior is \u03c0(\u00b5) \u221d 1 with support on the entire real line, and the posterior density of \u00b5 is\nnormal with mean y and variance \u03c3 2/n, which is proper. prior ignorance about \u03c3 in\nmodels where the density of the data is of form \u03c3 \u22121g(u/\u03c3 ), u > 0, \u03c3 > 0, is usually\nrepresented by the improper prior \u03c0(\u03c3 ) \u221d \u03c3 \u22121, \u03c3 > 0. non-informative priors of\nthis sort exist for more general situations, but there is a fundamental difficulty in\nrepresenting ignorance in a way that is independent both of the data to be collected\nand the parametrization of the model (problem 11.4). the key question is: ignorance\nabout what? the following classic example illustrates this.\n\nexample 11.8 (bernoulli probability) the probability of success in a bernoulli\ntrial lies in the interval [0, 1], so if we are completely ignorant of its true value, the\nobvious prior to use is uniform on the unit interval: \u03c0(\u03b8) = 1, 0 \u2264 \u03b8 \u2264 1. but if we\nare completely ignorant of \u03b8, weare also completely ignorant of \u03c8 = log{\u03b8/(1 \u2212 \u03b8)},\nwhich takes values in the real line. the density implied for \u03c8 by the uniform prior\nfor \u03b8 is\n\n\u03c0(\u03c8) = \u03c0{\u03c8(\u03b8)} \u00d7\n\ne\u03c8\n\n(1 + e\u03c8 )2\n\n, \u2212\u221e < \u03c8 <\u221e :\n\n(cid:10)(cid:10)(cid:10)(cid:10) d\u03b8\n\nd\u03c8\n\n(cid:10)(cid:10)(cid:10)(cid:10) =\n\nsir harold jeffreys\n(1891\u20131989) studied first\nin newcastle and then in\ncambridge, where he\nremained for the rest of\nhis life, becoming\nplumian professor of\nastronomy. during world\nwar i he worked in the\ncavendish laboratory,\nand thereafter studied and\ntaught hydrodynamics and\ngeophysics, being the first\nto claim that the core of\nthe earth is liquid. in an\nimportant series of books\nhe championed objective\nbayesian inference long\nbefore it became popular\n(jeffreys, 1961), and also\nwrote important works on\ngeophysics and\nmathematical physics. his\nunassuming character\ninspired deep affection.\n\nthe standard logistic density. far from expressing ignorance about \u03c8, this density\nasserts that the prior probability of |\u03c8| < 3 isabout 0.9.\n(cid:1)\n\njeffreys priors\napparent paradoxes like that of example 11.8 led to a widespread rejection of\nbayesian inference in the early twentieth century. the key difficulty is that the repre-\nsentation of ignorance is not invariant under reparametrization. a solution to this is\nto seek invariant priors. for scalar \u03b8 the best-known of these is the jeffreys prior\n\n\u03c0(\u03b8) \u221d |i(\u03b8)|1/2,\n\n(11.12)\nwhere i(\u03b8) = \u2212e{d2(cid:12)(\u03b8)/d\u03b8 2} is the expected information for \u03b8 based on the log like-\nlihood (cid:12)(\u03b8); i(\u03b8) ispositive in a regular statistical model. for a smooth reparametriza-\ntion \u03b8 = \u03b8(\u03c8) interms of \u03c8, the expected information for \u03c8 is\n\n(cid:11)\n\nd2(cid:12){\u03b8(\u03c8)}\n\nd\u03c8 2\n\n(cid:12)\n\n(cid:13)\n\n= \u2212e\n\nd2(cid:12)(\u03b8)\nd\u03b8 2\n\n(cid:14)\n\n\u00d7\n\n(cid:10)(cid:10)(cid:10)(cid:10) d\u03b8\n\nd\u03c8\n\n(cid:10)(cid:10)(cid:10)(cid:10)2 = i(\u03b8) \u00d7\n\n(cid:10)(cid:10)(cid:10)(cid:10) d\u03b8\n\nd\u03c8\n\n(cid:10)(cid:10)(cid:10)(cid:10)2\n\n.\n\ni(\u03c8) = \u2212e\n\nconsequently |i(\u03b8)|1/2d\u03b8 = |i(\u03c8)|1/2d\u03c8: with the choice (11.12), prior informa-\ntion does behave consistently under reparametrization; furthermore such priors give\nwidely-accepted solutions in some standard problems. when \u03b8 is vector, |i(\u03b8)| is\ntaken to be the determinant of i(\u03b8).\n\nthis prior was initially proposed with the aim of giving an \u2018objective\u2019 basis for\ninference, but after further paradoxes emerged its use was suggested for convenience,\na matter of scientific convention rather than as a logically unassailable expression of\nignorance about the parameter.\n\n "}, {"Page_number": 588, "text": "576\n\n11 \u00b7 bayesian models\n\n\u22121. thus the jeffreys prior is proportional to \u03b8\u22121/2(1 \u2212 \u03b8)\n\nexample 11.9 (bernoulli probability) the log likelihood for a single bernoulli trial\nwith success probability \u03b8 is y log \u03b8 + (1 \u2212 y) log(1 \u2212 \u03b8), and the fisher information\nis i(\u03b8) = \u03b8\u22121(1 \u2212 \u03b8)\n\u22121/2,\nand so equals the beta density (11.3) shown in the top left panel of figure 5.4,\nwhich while proper does not look uninformative. it can be interpreted as carrying\ninformation equivalent to one trial, in which one-half of a success was observed. as\nthe prior information for n independent trials is ni(\u03b8), the jeffreys prior is the same\n(cid:1)\nbecause the constant of proportionality is independent of \u03b8.\n\nexample 11.10 (location-scale model) suppose that y1, . . . , yn is a random sam-\nis (cid:12)(\u03b7) = (cid:1)\nple from a location model f (y; \u03b7) = g(y \u2212 \u03b7), for real y and \u03b7. then the log likelihood\n\nlog g(y j \u2212 \u03b7), so\ni(\u03b7) = \u2212n\n\n(cid:4) \u221e\n\n\u2212\u221e\n\nd2 log g(y \u2212 \u03b7)\n\nd\u03b72\n\ng(y \u2212 \u03b7) dy.\n\nthe substitution u = y \u2212 \u03b7 shows that i(\u03b7) isindependent of \u03b7, and therefore the\njeffreys prior is the constant non-informative prior \u03c0(\u03b7) \u221d 1 for all \u03b7.\na modification of this argument (problem 11.2) shows that the jeffreys prior for\nf (y; \u03c4 ) = \u03c4\u22121g(y/\u03c4 ), y, \u03c4 > 0, is \u03c0(\u03c4 ) \u221d \u03c4\u22121, which is also widely accepted as\nnon-informative. both \u03c0(\u03c4 ) and \u03c0(\u03b7) are improper.\na difficulty with this approach appears when we consider the location-scale model\nf (y; \u03b7, \u03c4 ) = \u03c4\u22121g{(y \u2212 \u03b7)/\u03c4}. its information matrix has form i(\u03b7, \u03c4 ) = n\u03c4\u22122 a,\nwhere the 2 \u00d7 2 matrix a is free of parameters, so \u03c0(\u03b7, \u03c4 ) = |i(\u03b7, \u03c4 )|1/2 \u221d \u03c4\u22122. this\ndoes not equal the prior \u03c4\u22121 arising from taking independent jeffreys priors for \u03b7 and\n\u03c4 separately.\nthe approach is here unsatisfactory because the prior \u03c4\u22122 is not widely accepted\nas a non-informative statement of uncertainty about \u03c4 . more generally this exam-\nple shows that a non-informative inference for a parameter of interest, \u03b7, say, may\ndepend on the model in which \u03b7 is embedded, in the sense that the inference may\ndepend on the prior chosen for nuisance parameters, even when these are a priori\n(cid:1)\nindependent of \u03b7.\n\njeffreys\u2019 general solution to the difficulty raised in example 11.10 was to treat\nlocation parameters as fixed when computing i(\u03b8). let \u03b8 = (\u00b51, . . . , \u00b5 p, \u03c8), where\nthe \u00b5r are location parameters and \u03c8 contains all other parameters in the problem.\nthen the prior he recommended is\n\u03c0(\u00b51, . . . , \u00b5 p, \u03c8) \u221d\n\n\u2212 \u2202 2(cid:12)(\u00b51, . . . , \u00b5 p, \u03c8)\n\n(cid:14)(cid:10)(cid:10)(cid:10)(cid:10)1/2\n\n(cid:10)(cid:10)(cid:10)(cid:10)e\n\n(cid:13)\n\n,\n\n\u2202\u03c8 \u2202\u03c8 t\n\nwhich produces \u03c0(\u03b8) \u221d \u03c4\u22121 in the location-scale model.\n\nnumerous other approaches to representing prior ignorance have been proposed,\nbased for example on notions of invariance, of minimal information, or of matching\nthe coverage of bayesian and frequentist confidence intervals. to a large extent these\nare regarded as useful to the extent that they yield jeffreys priors, and we shall not\nconsider them in detail. to be more explicit about links with the frequentist approach,\n\n "}, {"Page_number": 589, "text": "11.1 \u00b7 introduction\n577\nhowever, note that if a uniform prior is taken in (11.11), corresponding to \u03c4 \u2192 \u221e,\nand we define ay to be the interval with limits y \u00b1 z\u03b1n\n\u22121/2\u03c3 , then the posterior\nprobability pr(\u03b8 \u2208 ay | y) = 1 \u2212 2\u03b1. thus ay has posterior coverage (1 \u2212 2\u03b1). but\nay also has the same coverage for any fixed \u03b8 unconditional on y, sothe uniform prior\nyields an interval justifiable from both bayesian and frequentist viewpoints. exact\nresults such as this are unobtainable in more general settings, but nonetheless it can\nbe helpful to consider the extent to which bayesian and frequentist procedures agree.\n\nsome further aspects of jeffreys priors are outlined in problem 11.4.\n\nexercises 11.1\n1\n\nin example 11.3, calculate the predictive probability for k future heads out of m tosses\nbased on r heads observed in n tosses, using a beta prior density.\nshow that the limits of an unconditional confidence interval of level (1 \u2212 2\u03b1) in\nexample 11.4 involve the solutions to the equation\n\n1\n2\n\n\u0001{(y \u2212 \u03b8)/10} + 1\n2\n\n\u0001(y \u2212 \u03b8) = \u03b1, 1 \u2212 \u03b1.\n\nhence justify the approximate 0.95 interval given in the example.\n(a) let y1, . . . , yn be a poisson random sample with mean \u03b8, and suppose that the prior\ndensity for \u03b8 is gamma,\n\n(cid:2)\n\n\u0001(\u03b1)\n\n\u03b8 > 0, \u03bb, \u03b1 > 0.\n\n\u03c0(\u03b8) = g(\u03b8; \u03b1, \u03bb) = \u03bb\u03b1\u03b8 \u03b1\u22121\n\nexp(\u2212\u03bb\u03b8),\nshow that the posterior density of \u03b8 is g(\u03b8; \u03b1 +(cid:1)\ny j , \u03bb + n), and find conditions under\nwhich the posterior density remains proper as \u03b1 \u2193 0 even though the prior density becomes\nimproper in the limit.\n\u03b8g(\u03b8; \u03b1, \u03bb) d\u03b8 = \u03b1/\u03bb. find the prior and posterior means e(\u03b8) and e(\u03b8 |\n(b) show that\ny), and hence give an interpretation of the prior parameters.\n(c) let z be a new poisson variable independent of y1, . . . ,y n, also with mean \u03b8. find\nits posterior predictive density. to what density does this converge as n \u2192 \u221e? does this\nmake sense?\nhow would you express prior ignorance about an angle? about the position of a star in\nthe firmament?\nif yi j \u223c n (\u00b5i , \u03c3 2) independently for i = 1, . . . ,k and j = 1, . . . ,m , show that the\njeffreys prior for \u00b51, . . . , \u00b5k , \u03c3 equals \u03c3 \u2212(k+1). discuss the form of posterior inferences\non \u03c3 2 when m = 2. is this prior reasonable? if not, suggest a better alternative.\naccording to the principle of insufficient reason probabilities should be ascribed uniformly\nto finite sets unless there is some definite reason to do otherwise. thus the most natural\nway to express prior ignorance for a parameter \u03b8 that inhabits a finite parameter space\n\u03b81, . . . , \u03b8k is to set \u03c0(\u03b81) = \u00b7\u00b7\u00b7 = \u03c0(\u03b8k) = 1/k. let \u03c0i = \u03c0(\u03b8i ).\nconsider a parameter space {\u03b81, \u03b82}, where \u03b81 denotes that there is life in orbit around the\nstar sirius and \u03b82 that there is not. can you see any reason not to take \u03c01 = \u03c02 = 1/2?\nnow consider the parameter space {\u03c91, \u03c92, \u03c93}, where \u03c91, \u03c92, and \u03c93 denote the\nevents that there is life around sirius, that there are planets but no life, and that there\nare no planets. with this parameter space the principle of insufficient reason gives\npr(life around sirius) = 1/3.\ndiscuss this partitioning paradox. what solutions do you see?\n(schafer, 1976, pp. 23\u201324)\ncompute the prior and posterior means and variances for exponential family data with the\nconjugate prior distribution, and discuss their interpretation.\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n "}, {"Page_number": 590, "text": "578\n\n8\n9\n\n11 \u00b7 bayesian models\n\nf (y | \u03b8)\n\nparameter\n\nprior\n\nbinomial\npoisson\nexponential\nnormal\nnormal\nmultinomial\n\nsuccess probability\nmean\nmean\nmean (known variance)\nvariance (known mean)\nprobabilities\n\nbeta\ngamma\ngamma\nnormal\ninverse gamma\ndirichlet\n\ntable 11.1 conjugate\nprior densities for\nexponential family\nsamling distributions.\n\nuse example 11.6 to verify the contents of table 11.1.\nlet \u03b8 be a randomly chosen physical constant. such constants are measured on an arbitrary\nscale, so transformations from \u03b8 to \u03c8 = c\u03b8 for some constant c should leave the density\n\u03c0(\u03b8) of\u03b8 unchanged. show that this entails \u03c0(c\u03b8) = c\n\u22121\u03c0(\u03b8) for all c, \u03b8 > 0, and deduce\nthat \u03c0(\u03b8) \u221d \u03b8\u22121.\nlet \u02dc\u03b8 be the first significant digit of \u03b8 in some arbitrary units. show that\n\npr(\u02dc\u03b8 = d) \u221d\n\nu\nand hence verify that pr(\u02dc\u03b8 = d) = log10(1 + d\n\u2018constants\u2019 (e.g. sizes of countries or of lakes) fits this distribution.\n\nd10a\n\n\u22121 du,\n\nd = 1, . . . ,9,\n\n\u22121). check whether some set of physical\n\n(cid:4)\n\n(d+1)10a\n\n11.2 inference\n11.2.1 posterior summaries\nif the information regarding \u03b8 is contained in its posterior density given the data y,\n\u03c0(\u03b8 | y), how do we get at it? in principle this is easy: we simply use the posterior\ndensity to calculate the probability of any event of interest. but some summary quan-\ntities may be useful. for example, if \u03b8 = (\u03c8, \u03bb) is a vector, and we are interested in\n\u03c8, the marginal posterior density\n\n(cid:4)\n\n\u03c0(\u03c8 | y) =\n\n\u03c0(\u03c8, \u03bb | y) d\u03bb,\n\ncontains the marginal information in the model and prior concerning \u03c8. it ismost\nuseful when \u03c8 has dimension one or two, in which case it can be plotted. it condenses\nfurther to moments, quantiles, or the mode of \u03c0(\u03c8 | y).\n\nnormal approximation\none simple approximate summary of a unimodal posterior rests on quadratic series\nexpansion of the log posterior density, analogous to expansion of the log likelihood.\nin terms of \u02dc(cid:12)(\u03b8) = log l(\u03b8) + log \u03c0(\u03b8) and the posterior mode \u02dc\u03b8, we have\n\n\u02dc(cid:12)(\u03b8)\n\n.= \u02dc(cid:12)(\u02dc\u03b8) + (\u03b8 \u2212 \u02dc\u03b8)t\n= \u02dc(cid:12)(\u02dc\u03b8) \u2212 1\n2\n\n(\u03b8 \u2212 \u02dc\u03b8)t \u02dcj (\u02dc\u03b8)(\u03b8 \u2212 \u02dc\u03b8),\n\n\u2202 \u02dc(cid:12)(\u02dc\u03b8)\n\u2202\u03b8\n\n+ 1\n2\n\n(\u03b8 \u2212 \u02dc\u03b8)t\n\n\u2202 2 \u02dc(cid:12)(\u02dc\u03b8)\n\u2202\u03b8 \u2202\u03b8 t\n\n(\u03b8 \u2212 \u02dc\u03b8)\n\n "}, {"Page_number": 591, "text": "table 11.2 mortality\nrates r/m from cardiac\nsurgery in 12 hospitals\n(spiegelhalter et al.,\n1996b, p. 15). shown are\nthe numbers of deaths r\nout of m operations.\n\n11.2 \u00b7 inference\n\n579\n\na\ng\n\n0/47\n9/148\n\nb\nh\n\n18/148\n31/215\n\nc\ni\n\n8/119\n14/207\n\nd\nj\n\n46/810\n8/97\n\ne\nk\n\n8/211\n29/256\n\nf\nl\n\n13/196\n24/360\n\nprovided the mode lies inside the parameter space. here \u02dcj (\u03b8) isthe second deriva-\ntive matrix of \u2212 \u02dc(cid:12)(\u03b8). this expansion corresponds to a posterior multivariate normal\n\u22121, based on which an equitailed\ndensity for \u03b8, with mean \u02dc\u03b8 and variance matrix \u02dcj ( \u02dc\u03b8)\n(1 \u2212 2\u03b1) confidence interval for the rth component \u03b8r of \u03b8 is \u02dc\u03b8 r \u00b1 z\u03b1 \u02dcv1/2\nrr , where \u02dcvrr\nis the rth diagonal element of \u02dcj ( \u02dc\u03b8)\n\n\u22121.\n\nin large samples the log likelihood contribution is typically much greater than that\nfrom the prior, so \u02dc\u03b8 and \u02dcj ( \u02dc\u03b8) are essentially indistinguishable from the maximum like-\n\nlihood estimate(cid:15)\u03b8 and observed information j ((cid:15)\u03b8). thus likelihood-based confidence\n\nintervals may be interpreted as giving approximate bayesian inferences, if the sample\nis large. this approximation will usually be better if applied to the marginal posterior\nof a low-dimensional subset of \u03b8, because of the averaging effect of integration over\nthe other parameters. the same caveats apply when using this approximation as to use\nof normal approximations for the maximum likelihood estimator; in particular, it may\nbe more suitable for a transformed parameter. we describe a more refined approach\nin section 11.3.1.\n\nother distributions may be used to approximate posterior densities, for example by\n\nmatching first and second moments.\n\nposterior confidence sets\nthe mean and mode of the posterior density are point summaries of \u03c0(\u03b8 | y), but\nconfidence regions or intervals are usually more useful. the bayesian analogue of a\n(1 \u2212 2\u03b1) confidence interval is a (1 \u2212 2\u03b1) credible set, defined to be a set, c, of values\nof \u03b8, whose posterior probability content is at least 1 \u2212 2\u03b1. when \u03b8 is continuous this is\n\n(cid:4)\n1 \u2212 2\u03b1 = pr(\u03b8 \u2208 c | y) =\n(cid:1)\nc\n\u03b8\u2208c\n\n\u03c0(\u03b8 | y) d\u03b8.\n\u03c0(\u03b8 | y). for scalar \u03b8, such a set\nwhen \u03b8 is discrete, the integral is replaced by\nis equi-tailed if it has form (\u03b8l , \u03b8u ), where \u03b8l and \u03b8u are the posterior \u03b1 and 1 \u2212 \u03b1\nquantiles of \u03b8, that is, pr(\u03b8 < \u03b8l | y) = pr(\u03b8 > \u03b8u | y) = \u03b1.\noften c is chosen so that the posterior density for any \u03b8 in c is higher than\nfor any \u03b8 not in c. that is, if \u03b8 \u2208 c, \u03c0(\u03b8 | y) \u2265 \u03c0(\u03b8(cid:9) | y) for any \u03b8(cid:9) /\u2208 c. such\na region is called a highest posterior density credible set, ormore concisely a\nhpd credible set.\n\nexample 11.11 (cardiac surgery data) table 11.2 contains data on the mortality\nlevels for cardiac surgery on babies at 12 hospitals. a simple model treats the number\nof deaths r as binomial with mortality rate \u03b8 and denominator m. athospital a, for\n\nexample, m = 47 and r = 0, giving maximum likelihood estimate(cid:15)\u03b8a = 0/47 = 0,\nbut itseems too optimistic to suppose that \u03b8a could be so small when the other rates\nare evidently larger. if we take a beta prior density with a = b = 1, the posterior\ndensity is beta with parameters a + r = 1 and b + m \u2212 r = 48, as shown in the\n\n "}, {"Page_number": 592, "text": "580\n\nf\nd\np\n\n4\n\n.\n\n0\n\n3\n\n.\n\n0\n\n2\n\n.\n\n0\n\n1\n\n.\n\n0\n\n0\n\n.\n0\n\n11 \u00b7 bayesian models\n\nfigure 11.1 cardiac\nsurgery data. left panel:\nposterior density for \u03b8a,\nshowing boundaries of\n0.95 highest posterior\ncredible interval (vertical\nlines) and region between\nposterior 0.025 and 0.975\nquantiles of \u03c0(\u03b8a | y)\n(shaded). right panel:\nexact posterior beta\ndensity for overall\nmortality rate \u03b8 (solid)\nand normal approximation\n(dots).\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\nf\nd\np\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n0\n\n0\n\n5\n\n10\n\n15\n\n20\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\ntheta (%)\n\ntheta (%)\n\nleft panel of figure 11.1. the 0.95 hpd credible interval is (0, 6.05)%, while the\nequitailed credible interval uses the 0.025 and 0.975 quantiles of \u03c0(\u03b8a | y) and is\n(0.05, 7.40)%.\nthe right panel of figure 11.1 shows the posterior density for the overall mortality\nrate \u03b8, obtaining by merging all the data, giving r = 208 deaths in m = 2814 oper-\nations. here the prior parameters a and b have essentially no effect on the posterior,\nand hence\n\n\u22121 = (a + r \u2212 1)(b + m \u2212 r \u2212 1)\n\n.= r(m \u2212 r)\n\u02dc\u03b8 = a + r \u2212 1\na + b + m \u2212 2\nthe figure shows the corresponding normal approximation to \u03c0(\u03b8 | y). evidently\ninferences from exact and approximate posterior densities will be equivalent for\npractical purposes.\n\n(a + b + m \u2212 2)3\n\n.= r\nm\n\n\u02dcj ( \u02dc\u03b8)\n\nm3\n\n,\n\n.\n\nboth separate and pooled analyses of mortality rates seem unsatisfactory, because\nalthough some variation among hospitals is plausible they are likely also to have\nelements in common. example 11.26 describes an approach intermediate between\n(cid:1)\nthose used here.\n\nexample 11.12 (normal distribution) consider a normal\nrandom sample\ny1, . . . , yn with mean \u00b5 and variance \u03c3 2 both unknown. we shall give them in-\ndependent prior densities. as the posterior for (\u00b5, \u03c3 2) depends on y only through the\nminimal sufficient statistic (y, s2), we have\n\n\u03c0(\u00b5, \u03c3 2 | y, s2) \u221d f (y, s2 | \u00b5, \u03c3 2)\u03c0(\u00b5, \u03c3 2)\n\n= f (y | \u00b5, \u03c3 2) f (s2 | \u00b5, \u03c3 2)\u03c0(\u00b5, \u03c3 2)\n= f (y | \u00b5, \u03c3 2) f (s2 | \u03c3 2)\u03c0(\u00b5)\u03c0(\u03c3 2)\n\u221d \u03c0(\u00b5 | y, \u03c3 2) f (s2 | \u03c3 2)\u03c0(\u03c3 2),\n\n(11.13)\n\nwhere the first step follows from bayes\u2019 theorem, the second from the conditional\nindependence of y and \u03c3 2 given \u00b5 and \u03c3 2, the third from the prior independence of\n\u00b5 and \u03c3 2 and the independence of s2 and \u00b5, and the fourth on using bayes\u2019 theorem\n\n(cid:1)\n(cid:1)\ny j and s2 =\ny = n\n\u22121\n(n \u2212 1)\n(y j \u2212 y)2 are\n\u22121\nthe sample average and\nvariance.\n\n "}, {"Page_number": 593, "text": "11.2 \u00b7 inference\n\n581\n\nto get the posterior density for \u00b5 conditional on y and \u03c3 2. integration of (11.13)\nwith respect to \u00b5 shows that \u03c0(\u03c3 2 | y, s2) \u221d f (s2 | \u03c3 2)\u03c0(\u03c3 2): the marginal posterior\ndensity of \u03c3 2 depends only on s2. however, as\u03c3 2 appears in all three terms, integration\nof (11.13) with respect to \u03c3 2 shows that the marginal posterior for \u00b5 depends on both\ny and s2.\nlet us use the improper priors \u03c0(\u00b5) \u221d 1, \u03c0(\u03c3 2) \u221d \u03c3 \u22122. example 11.7 shows that\nthe posterior density for \u00b5 when \u03c3 2 is known is n (y, \u03c3 2/n). conditional on \u03c3 2, the\ndistribution of (n \u2212 1)s2 is \u03c3 2\u03c7 2\n\nn\u22121, soour choice of prior gives\n\n\u03c0(\u03c3 2 | s2) \u221d \u03c0(\u03c3 2) f (s2 | \u03c3 2)\n\n(cid:13)\n\n(cid:14)\n\n\u221d (\u03c3 2)\n\n\u22121(\u03c3 2)\n\n\u2212(n\u22121)/2 exp\n\n(n \u2212 1)s2/\u03c3 2\n\n\u2212 1\n2\n\n,\n\n\u03c3 2 > 0.\n\nthus the marginal posterior density of \u03c3 2 is inverse gamma,\n\n\u03b2 \u03b1\n\n\u0001(\u03b1)x \u03b1+1 exp(\u2212\u03b2/x),\n2 (n \u2212 1) and \u03b2 = 1\n\nx > 0,\n\n\u03b1, \u03b2 > 0,\n\n(11.14)\n\nwith x = \u03c3 2, \u03b1 = 1\n2 (n \u2212 1)s2; auseful shorthand for (11.14) is\ni g(\u03b1, \u03b2). its mean and variance are \u03b2/(\u03b1 \u2212 1) and \u03b22/{(\u03b1 \u2212 1)2(\u03b1 \u2212 2)}, provided\nthat \u03b1 > 2. equivalently, the posterior distribution of \u03c3 2 given s2 is that of (n \u2212\n1)s2/v , where v \u223c \u03c7 2\n\nn\u22121. the joint posterior density for (\u00b5, \u03c3 2),\n\n\u03c0(\u00b5, \u03c3 2 | y, s2) \u221d \u03c0(\u00b5 | y, \u03c3 2)\u03c0(\u03c3 2 | s2).\n\nis proportional to\n\u22121/2 exp\n\n(\u03c3 2)\n\n(cid:16)\n\n(cid:17)\n\n\u00d7 (\u03c3 2)\n\n\u2212 n\n2\u03c3 2 (\u00b5 \u2212 y)2\n(cid:19)\n(cid:18)\nn\n2\nn\u22121\n2\n\n(cid:19)\n\n\u0001\n\n(cid:13)\n\n(cid:18)\n\u03c0(\u00b5 | y, s2) = \u0001\n\n\u2212(n\u22121)/2\u22121 exp\n(cid:14)1/2(cid:13)\n\nn\n\n(n \u2212 1)s2\u03c0\n\n(cid:14)\n\n(cid:13)\n\n\u2212 (n \u2212 1)s2\n\n2\u03c3 2\n\n,\n\n(11.15)\n\n(cid:14)\u2212n/2\n\n.\n\n1 + n(\u00b5 \u2212 y)2\n(n \u2212 1)s2\n\nintegration of which over \u03c3 2 yields the marginal posterior density for \u00b5,\n\ntherefore n1/2(\u00b5 \u2212 y)/s \u223c tn\u22121 a posteriori. the corresponding frequentist result\ntreats y and s2 as random and \u00b5 as fixed; here the random variable is \u00b5, with y and\ns2 regarded as constants.\nfigure 11.2 shows posterior densities for \u00b5 and \u03c3 2 based on the height differences\nfor the 15 pairs of plants in table 1.1; here y = 20.93 and s2 = 1424.64. evidently\nthe posterior densities are not independent. while the hpd credible set for \u00b5 is\n(cid:1)\nequi-tailed, that for \u03c3 2 is not.\n\na credible set may contain the same values of \u03b8 as a confidence interval, but its in-\nterpretation is different. in the bayesian framework the data are regarded as fixed and\nthe parameter as random, so the endpoints of the credible set are fixed and the probabil-\nity statement concerns the parameter, regarded as a random variable. the frequentist\napproach treats the parameter as an unknown constant and the confidence interval\nendpoints as random variables; the probability statement concerns their behaviour in\nrepeated sampling from the model.\n\n "}, {"Page_number": 594, "text": "582\n\n2\na\nm\ng\ns\n\ni\n\n0\n0\n0\n5\n\n0\n0\n0\n4\n\n0\n0\n0\n3\n\n0\n0\n0\n2\n\n0\n0\n0\n1\n\n0\n\n-6\n\n-4\n\n-0.5-1 -2\n\n-2\n\n 0\n\ny\nt\ni\ns\nn\ne\nd\n\n \nr\no\ni\nr\ne\n\nt\ns\no\np\n\n4\n0\n\n.\n\n0\n\n3\n0\n\n.\n\n0\n\n2\n0\n\n.\n\n0\n\n1\n0\n\n.\n\n0\n\n0\n\n.\n\n0\n\n11 \u00b7 bayesian models\n\nfigure 11.2 posterior\ndensities of (\u00b5, \u03c3 2) of\nnormal model for maize\ndata. left: contours of the\nnormalized log joint\nposterior density. right:\nmarginal posterior density\nfor \u00b5, showing 95% hpd\ncredible set, which is the\nset of values of \u00b5 whose\nvalues of the posterior\ndensity \u03c0(\u00b5 | y) lie above\nthe dashed line. the\nshaded region has area\n0.05.\n\n-20\n\n0\n\n20\n\nmu\n\n40\n\n60\n\n-20\n\n0\n\n40\n\n60\n\n20\n\nmu\n\n11.2.2 bayes factors\nthe frequentist approach to hypothesis testing compares a null hypothesis h0 with an\nalternative h1 through a test statistic t that tends to be larger under h1 than under h0,\nand rejects h0 for small values of the significance probability pobs = pr0(t \u2265 tobs),\nwhere tobs is the value of t actually observed and the probability is computed as if\nh0 were true.\n\nthe bayesian approach attaches prior probabilities to the models corresponding to\n\nh0 and h1 and compares their posterior probabilities\npr(y | hi )pr(hi )\n\npr(hi | y) =\n\npr(y | h0)pr(h0) + pr(y | h1)pr(h1)\n\ni = 0, 1.\n\n,\n\nan obvious distinction between this and the frequentist approach is that pr(h0 | y)\nis the probability of h0 conditional on the data, whereas the p-value may not be\ninterpreted in this way. in bayesian settings increasing amounts of data may lead to\nincreasing support for one hypothesis relative to the alternatives. this differs from the\nfrequentist approach, where non-rejection of h0 does not indicate increasing support\nfor it in large samples. a further important difference is that the p-value does not\ndepend on the particular alternative h1 under discussion. indeed, whereas frequentist\ntesting does not require h1 to be fully specified, this is essential for bayesian testing,\nwhich is in this sense more restrictive.\n\nfor some purposes it is valuable to use the odds in favour of h1,\n\npr(h1 | y)\npr(h0 | y)\n\n= pr(y | h1)\npr(y | h0)\n\n\u00d7 pr(h1)\npr(h0)\n\n.\n\n(11.16)\n\nthe change in prior to posterior odds for h1 relative to h0 depends on data only\nthrough the bayes factor\n\nb10 = pr(y | h1)\npr(y | h0)\n\n.\n\n(11.17)\n\nthus analogous to the updating rule for inference on \u03b8, weupdate evidence comparing\nthe models by the rule posterior odds = bayes factor \u00d7 prior odds.\n\n "}, {"Page_number": 595, "text": "11.2 \u00b7 inference\n\n583\n\ntable 11.3\ninterpretation of bayes\nfactor b10 in favour of h1\nover h0. since\nb10 = b\n\u22121\n01 , negating the\nvalues of 2 log b10 gives\nthe evidence against h1.\n\nb10\n\n2 log b10\n\nevidence against h0\n\n1\u20133\n3\u201320\n20\u2013150\n> 150\n\n0\u20132\n2\u20136\n6\u201310\n> 10\n\nhardly worth a mention\npositive\nstrong\nvery strong\n\nthe simplest situation is when both hypotheses are simple, in which case b10\nequals the likelihood ratio in favour of h1. usually, however, both hypotheses involve\n(cid:4)\nparameters, say \u03b80 and \u03b81, and\n\npr(y | hi ) =\n\nf (y | hi , \u03b8i )\u03c0(\u03b8i | hi ) d\u03b8i ,\n\ni = 0, 1,\n\nwhere \u03c0(\u03b8i | hi ) isthe prior for \u03b8i under hi . inthis case the bayes factor is a ratio\nof weighted likelihoods. by analogy with the likelihood ratio statistic, the quantity\n2 log b10 is often used to summarize the evidence for h1 compared to h0, with\nthe rough interpretation shown in table 11.3. this contrasts with the interpretation\nof a likelihood ratio statistic, whose null \u03c7 2 distribution for nested models would\ndepend on the difference in their degrees of freedom. the log bayes factor log b10 is\nsometimes called the weight of evidence.\n\nexample 11.13 (hus data) example 4.40 introduced data on the numbers of cases\nof haemolytic uraemic syndrome (hus) treated at a clinic in birmingham from 1970\nto 1989. the data suggest a sharp rise in incidence around 1980. in that example\nit was supposed that the annual counts y1, . . . , yn are realizations of independent\npoisson variables with means e(y j ) = \u03bb1 for j = 1, . . . , \u03c4 and e(y j ) = \u03bb2 for j =\n\u03c4 + 1, . . . ,n . here the changepoint \u03c4 can take values 1, . . . ,n \u2212 1.\nsuppose that our baseline model h0 is that \u03bb1 = \u03bb2 = \u03bb, that is, no change, and\nconsider the alternative h\u03c4 of change after year \u03c4 . under h\u03c4 we suppose that \u03bb1 and\n\u03bb2 have independent gamma prior densities with parameters \u03b3 and \u03b4. this density\nhas mean \u03b3 /\u03b4 and variance \u03b3 /\u03b42. then pr(y | h\u03c4 ) equals\n(cid:4) \u221e\n\n(cid:4) \u221e\n\n\u03bby j\n1\ny j !\n\ne\n\n\u2212\u03bb1 \u00d7 \u03b4\u03b3 \u03bb\u03b3\u22121\n\n1\n\n\u0001(\u03b3 )\n\n\u2212\u03b4\u03bb1 d\u03bb1\n\ne\n\n0\n\n\u03bby j\n2\ny j !\n\ne\n\n\u2212\u03bb2 \u00d7 \u03b4\u03b3 \u03bb\u03b3\u22121\n\n2\n\n\u0001(\u03b3 )\n\n\u2212\u03b4\u03bb2 d\u03bb2,\n\ne\n\n\u03c4(cid:3)\nj=1\n\n0\n\nn(cid:3)\nj=\u03c4+1\n\nor equivalently\n\n(cid:20)\n\u0001(\u03b3 )2\nwhere s\u03c4 = y1 + \u00b7\u00b7\u00b7 + y\u03c4 .\nunder h0 we assume that \u03bb also has the gamma density with parameters \u03b3 and \u03b4.\n\n\u0001 (\u03b3 + s\u03c4 ) \u0001 (\u03b3 + sn \u2212 s\u03c4 )\n(\u03b4 + \u03c4 )\u03b3+s\u03c4 (\u03b4 + n \u2212 \u03c4 )\u03b3+sn\u2212s\u03c4\n\n\u03b42\u03b3\nn\nj=1 y j !\n\n,\n\nthen the bayes factor for a changepoint in year \u03c4 is\n\nb\u03c4 0 = \u0001 (\u03b3 + s\u03c4 ) \u0001 (\u03b3 + sn \u2212 s\u03c4 ) \u03b4\u03b3 (\u03b4 + n)\u03b3+sn\n\u0001(\u03b3 )\u0001(\u03b3 + sn)(\u03b4 + \u03c4 )\u03b3+s\u03c4 (\u03b4 + n \u2212 \u03c4 )\u03b3+sn\u2212s\u03c4\n\nfor completeness we set bn0 = 1.\n\n\u03c4 = 1, . . . ,n \u2212 1.\n\n,\n\n "}, {"Page_number": 596, "text": "584\n\n11 \u00b7 bayesian models\n\n1970\n\n1971\n\n1972\n\n1973\n\n1974\n\n1975\n\n1976\n\n1977\n\n1978\n\n1979\n\ny\n2 log b\u03c4 0, \u03b3 = \u03b4 = 1\n2 log b\u03c4 0, \u03b3 = \u03b4 = 0.01\n2 log b\u03c4 0, \u03b3 = \u03b4 = 0.0001\n\n1\n4.9\n\u22121.3\n\u221210\n\n5\n\u22120.5\n\u22125.9\n\u221215\n\n3\n0.6\n\u22124.5\n\u221214\n\n2\n3.9\n\u22121.0\n\u221210\n\n2\n7.5\n3.0\n\u22126.1\n\n1\n13\n9.7\n0.6\n\n0\n24\n20\n11\n\n0\n35\n32\n23\n\n2\n41\n39\n30\n\n1\n51\n51\n42\n\n1980\n\n1981\n\n1982\n\n1983\n\n1984\n\n1985\n\n1986\n\n1987\n\n1988\n\n1989\n\ny\n2 log b\u03c4 0, \u03b3 = \u03b4 = 1\n2 log b\u03c4 0, \u03b3 = \u03b4 = 0.01\n2 log b\u03c4 0, \u03b3 = \u03b4 = 0.0001\n\n1\n63\n64\n55\n\n7\n55\n57\n48\n\n11\n38\n40\n31\n\n4\n42\n47\n38\n\n7\n40\n46\n37\n\n10\n31\n38\n29\n\n16\n11\n18\n8.8\n\n16\n\u22122.9\n1.8\n\u22127.1\n\n9\n\u22125.3\n1.2\n\u22127.7\n\n15\n0\n0\n0\n\ntable 11.4 gives 2 log b\u03c4 0 for \u03c4 = 1, . . . ,19, for values of \u03b3 and \u03b4 such that the\nprior density for \u03bb has unit mean and variances respectively 1, 102, 104, corresponding\nto increasing prior uncertainty. negative values of 2 log b\u03c4 0 correspond to evidence\nin favour of h0. there is very strong evidence for change in any year from 1976 to\n1986, but the most plausible changepoint is just after 1980. the evidence for change\n(cid:1)\nis overwhelming for all the priors chosen. see practical 11.6.\n\ntable 11.4 bayes\nfactors for comparison of\nmodel of change in\npoisson parameter after \u03c4\nyears, h\u03c4 , with model of\nno change h0, for hus\ndata y. there is very\nstrong evidence of change\nin any year from 1976\u201386.\n\nexample 11.14 (forensic evidence) the following situation can arise when foren-\nsic evidence is used in criminal trials: material y found on a suspect is similar to other\nmaterial, x, atthe scene of the crime, and it is desired to know how this affects our\nview of the case. for simplicity we shall suppose that if x and y come from the same\nsource, the suspect is guilty, an event we shall denote by g. let e denote any other\nevidence. then the odds of guilt, conditional on e and the data, are\n\npr(g | x, y, e)\npr(g | x, y, e)\n\npr(g | e)\npr(g | e)\n\n= pr(x, y | g, e)\npr(x, y | g, e)\npr(x, y | g)\n\n=\n\npr(x | g)pr(y | g)\n\n\u00d7 pr(g | e)\npr(g | e)\n\n,\n\n(11.18)\n\nwhere we have supposed that x and y are independent of e, and that they are inde-\npendent given the event g that the suspect is not guilty. the first ratio on the right of\n(11.18) is the bayes factor due to the forensic evidence.\n\nlet y and x represent single measurements on the refractive index of glass fragments\nfound on a suspect and at the scene of a burglary. we model the corresponding random\nvariables as\n\nx | \u03b81 \u223c n (\u03b81, \u03c3 2),\n\ny | \u03b82 \u223c n (\u03b82, \u03c3 2),\n\nwhere \u03b81 and \u03b82 are the true refractive indexes and \u03c3 2 is known. if the suspect is\nguilty, then \u03b81 = \u03b82 = \u03b8, say. we model natural variation among refractive indexes\n\n "}, {"Page_number": 597, "text": "11.2 \u00b7 inference\n\n585\n\nby supposing that \u03b8 is drawn from a population of types of glass whose true refractive\nindexes are n (\u00b5, \u03c4 2), where \u00b5 and \u03c4 2 (cid:10) \u03c3 2 both known. thus under g,\n\nx, y | \u03b8 iid\u223c n (\u03b8, \u03c3 2),\n\n\u03b8 \u223c n (\u00b5, \u03c4 2),\n\nwhile under g, the true indexes \u03b81 and \u03b82 are independent, giving\n\nx | \u03b81 \u223c n (\u03b81, \u03c3 2),\n\ny | \u03b82 \u223c n (\u03b82, \u03c3 2),\n\niid\u223c n (\u00b5, \u03c4 2).\n\n\u03b81, \u03b82\n\nit turns out to be easier to work in terms of transformed observations u = x \u2212 y and\nz = 1\n\n2 (x + y), and to write the corresponding random variables as\nu = \u03b81 \u2212 \u03b82 + \u03b51 \u2212 \u03b52,\n\n(\u03b81 + \u03b82 + \u03b51 + \u03b52),\n\niid\u223c n (0, \u03c3 2).\n\n\u03b51, \u03b52\n\nz = 1\n2\n\nthen u and z are independent and normal both conditionally on \u03b81, \u03b82 and uncondi-\ntionally. under g, \u03b81 = \u03b82, so\n\n(cid:5)\n\n(cid:6)\n\nu \u223c n (0, 2\u03c3 2),\n\nz \u223c n\n\n\u00b5, \u03c4 2 + 1\n2\n\n\u03c3 2\n\n,\n\nwhile under g,\n\nu \u223c n (0, 2\u03c4 2 + 2\u03c3 2),\n\nz \u223c n\n\n(cid:5)\n\n\u00b5,\n\n1\n2\n\n\u03c4 2 + 1\n2\n\n(cid:6)\n\n\u03c3 2\n\n.\n\nas the jacobian of the transform from (x, y) to (u, z) equals one under both g and\ng, and \u03c4 2 (cid:10) \u03c3 2, the bayes factor is roughly\n\u22121/2 exp{\u2212u2/(4\u03c3 2)}(\u03c4 2)\n\n\u22121/2 exp{\u2212(z \u2212 \u00b5)2/(4\u03c4 2)}\n(2\u03c3 2)\n(2\u03c4 2)\u22121/2 exp{\u2212u2/(4\u03c4 2)}(\u03c4 2/2)\u22121/2 exp{\u2212(z \u2212 \u00b5)2/\u03c4 2} ,\n\nwhich equals\n\n(cid:5)\n\n(cid:5)\n\n(cid:6)1/2 \u00d7 exp\n\n(cid:6)\n\n\u00d7 exp\n\n(cid:13)\n\n(z \u2212 \u00b5)2\n\n(cid:14)\n\n.\n\n2\u03c4 2\n\n\u2212 u2\n4\u03c3 2\n\n\u03c4 2\n2\u03c3 2\n\nthe interpretation of the second term is that if the difference u = x \u2212 y is large\nrelative to its variance 2\u03c3 2, there is strong evidence that \u03b81 and \u03b82 differ, and this\nfavours g. the third term measures how typical x and y are. if z = 1\n2 (x + y) is far\nfrom its mean, \u00b5, compared to its variance 1\n\u03c4 2 under g, both x and y have similar but\nunusual refractive indexes, and this strengthens the evidence for g. with \u03c4/\u03c3 = 100,\n2\nu/(2\u03c3 2)1/2 = 2, and (z \u2212 \u00b5)/( 1\n\u03c4 2)1/2 = 2, for example, these factors are respectively\n0.135 and 2.718, and the overall bayes factor is 26.01. under g a frequentist test for\na difference between \u03b81 and \u03b82 based on u would suggest that \u03b81 (cid:11)= \u03b82 at the 5% level,\nbut the bayes factor gives strong evidence in favour of guilt, as the values of x and y\ncorrespond to similar, unusual, types of glass.\n\n2\n\na more realistic model would account for non-normality of the distribution of \u03b8.\nother forms of evidence, such as dna fingerprints or cloth samples, require more\ncomplex likelihoods in the bayes factor and use prior information from specially\ntailored databases. moreover when the probabilities being modelled are very small,\n\n "}, {"Page_number": 598, "text": "586\n\n11 \u00b7 bayesian models\n\nit is important to allow for the possibility of events such as mistakes at the forensic\n(cid:1)\nlaboratory.\nwe often wish to test nested hypotheses. in a typical example \u03b8 = (\u03c8, \u03bb) for real\n\u03c8, and \u03bb varies in an open subset of ir p, with h0 : \u03c8 = \u03c80 and h1 : \u03c8 (cid:11)= \u03c80. then\nif the same proper continuous prior \u03c0(\u03c8, \u03bb) isused under both hypotheses, the prior\n(cid:4)\nodds in favour of h1 are infinite because\n\npr(h0) =\n\n\u03c0(\u03c80, \u03bb) d\u03bb = 0\n\nis an integral over a set of prior probability zero. thus the posterior odds in favour of\nh1 are infinite, whatever the data. this vexation can be eliminated by using different\nprior densities, weighted according to prior belief in h0 and h1, giving overall prior\n\n\u03c0(\u03c8, \u03bb) = \u03b4(\u03c8 \u2212 \u03c80)\u03c0(\u03c80, \u03bb | h0)pr(h0) + \u03c0(\u03c8, \u03bb | h1)pr(h1),\n\nwhere\n\n(cid:4)\n\n(cid:4)\n\n\u03c0(\u03c80, \u03bb | h0) d\u03bb =\n\n\u03c0(\u03c8, \u03bb | h1) d\u03c8d\u03bb = 1.\n\none result of this is that bayes factors are more sensitive to the prior than are posterior\ndensities. in particular, improper priors cannot be used, as the bayes factor depends\non the ratio of the two arbitrary constants of proportionality that appear in the priors.\none way to remove the arbitrariness is to fix the ratio of these constants using some\nexternal argument.\n\n\u03b4(\u00b7) isthe dirac delta\nfunction.\n\na further difficulty is that when a large number of models must be compared, prior\nprobabilities and proper priors must be assigned to each. this can be hard in practice,\nand the results may depend strongly on how it is done. this contrasts with frequentist\nhypothesis testing, where such difficulties do not arise. an apparently even more\nstriking contrast is provided by the following example.\nexample 11.15 (jeffreys\u2013lindley paradox) consider testing h0 : \u00b5 = 0 against dennis victor lindley\nh1 : \u00b5 (cid:11)= 0 based on a normal random sample y1, . . . , yn with mean \u00b5 and known\nvariance \u03c3 2. the usual test is based on the normal distribution of n1/2y /\u03c3 under\nh0, and gives p-value p = \u0001(\u2212n1/2|y|/\u03c3 ). in the bayesian framework, we write\n\u03c00 = pr(h0), and suppose that under h1, \u00b5 is normal with mean zero and variance\n\u03c4 2. then the posterior probabilities are\n\u2212 1\n(cid:4)\n2\u03c3 2\n\n(1923\u2013) was educated at\ncambridge, and held\nacademic posts there, in\naberwystwyth, and in\nlondon. he is a strong\nadvocate of bayesian\nstatistics. see smith\n(1995).\n\ny is the average of the\nrandom variables\ny1, . . . ,y n; its observed\nvalue is y.\n\npr(h0 | y) =\n\nn(cid:7)\nj=1\n\ny2\n(cid:8)\nj\n\n(cid:21)\n\n(cid:22)\n\n(cid:9)\n\n\u03c00\n\n,\n\n(2\u03c0 \u03c3 2)n/2 exp\n1 \u2212 \u03c00\n\n(2\u03c0 \u03c3 2)n/2(2\u03c0 \u03c4 2)1/2\n\npr(h1 | y) =\n\nexp\n\n\u2212 1\n2\u03c3 2\n\n(y j \u2212 \u00b5)2 \u2212 \u00b52\n2\u03c4 2\n\nd\u00b5,\n\nn(cid:7)\nj=1\n\nleading to bayes factor\nb01 =\n\n(cid:5)\n\n1 + n\n\n\u03c4 2\n\u03c3 2\n\n(cid:6)1/2\n\n(cid:8)\n\n\u2212\n\nexp\n\n(cid:9)\n\nny2\n\n2\u03c3 2(1 + n\u22121\u03c3 2/\u03c4 2)\n\n "}, {"Page_number": 599, "text": "11.2 \u00b7 inference\n\n587\n\ntable 11.5 dependence\nof bayes factor b01 on\nsample size n for a test\nwith significance level\n0.01.\n\nn\nb01\n\n1\n0.269\n\n10\n0.163\n\n100\n\n1000\n\n0.376\n\n1.15\n\n104\n3.63\n\n106\n36.2\n\n108\n362\n\n\u03b1/2\n\n.= n1/2\u03c4 \u03c3 \u22121 exp(\u2212z2\n\nin favour of h0. now suppose that ny2/\u03c3 2 = z2\n\u03b1/2. the significance level of the\nconventional test is \u03b1, butas n \u2192 \u221e we see that b01\n/2),\ngiving increasingly strong evidence in favour of h0. hence the paradox: although with\ny corresponding to \u03b1 = 10\n\u22126 we would reject h0 decisively, the bayes factor gives\nincreasingly strong support for h0, because as n \u2192 \u221e, the weight of the alternative\ndistribution is more and more widely spread compared to the distance from y to the\nnull hypothesis value of \u00b5. table 11.5 gives some values of b01 when \u03c4 2 = \u03c3 2.\none resolution of this hinges on noticing that a fixed alternative is not appropriate\nas n \u2192 \u221e. atest is used when there is doubt as to its outcome \u2014 when the data do\nnot evidently contradict the null hypothesis. mathematically, this means that sensible\n\u22121/2) distant from the null hypothesis. in this case we take \u03c4 2 =\nalternatives are o(n\n\u22121\u03b4\u03c3 2, sothat as n \u2192 \u221e the range of alternatives is fixed relative to the null; sensible\nn\nvalues for \u03b4 might be in the range 5\u201320. then the bayes factor corresponding to\nsignificance level \u03b1, b01 = (1 + \u03b4)1/2 exp{\u2212 1\n/(1 + \u03b4\u22121)}, does not increase with\n2 z2\nn. if wetake \u03b4 = 10 and \u03b1 = 0.05, 0.01, 0.001, and 0.0001, b10 equals 1.73, 6.2,\n\u03b1/2\n41.4, and 293. according to table 11.3 these correspond respectively to evidence\nagainst h0 that is hardly worth mentioning, positive, strong, and very strong, broadly\n(cid:1)\nagreeing with the usual interpretation of the p-values.\n\n11.2.3 model criticism\nthe prior density \u03c0(\u03b8) introduces further information into the model, with the benefit\nof directness of inference for \u03b8. the corresponding disbenefit is the need to assess\nthe appropriateness of \u03c0(\u03b8) and the sensitivity of posterior conclusions to the prior,\nadded to the usual concerns about the sampling model f (y | \u03b8). sensitivity analysis\nis generally performed simply by comparing posterior inferences based on a range of\npriors and models. the problems this poses are mainly computational, and we discuss\nthem briefly in section 11.3.\n\nwhen just a few parametrized alternative models are in view, the ideas for model\ncomparison outlined in section 11.2.2 can be applied, supplemented with suitable\ngraphs. in practice, however, consideration of all possible models is usually infeasible,\nnot least because data can spring surprises on the investigator, and so we turn to model-\nchecking when the alternatives are not explicit.\n\nmarginal inference\nfrom a bayesian viewpoint all information concerning the data and model is contained\nin the joint density\n\nf (y, \u03b8) = \u03c0(\u03b8 | y) f (y).\n\n(11.19)\n\n "}, {"Page_number": 600, "text": "588\n\n11 \u00b7 bayesian models\n\nand this suggests that f (y) should be used to check the model. it is relatively clear how\nto do this when there is a sufficient statistic s and s = (t, a), where a is a function of s\nwhose distribution does not depend on \u03b8; a is an ancillary statistic, anotion explored\nin section 12.1. then we can write\n\n(cid:4)\n\nf (y) = f (y | s) f (a)\n\nf (t | a, \u03b8)\u03c0(\u03b8) d\u03b8,\n\n(11.20)\n\nthe first two components of which do not depend on the prior, and hence can be used to\ngive information about the sampling model. the third component of (11.20), f (t | a),\ncan be regarded as carrying information about agreement between data and prior. in\nsimple models, consideration of the first two terms can yield standard model-checking\ntools.\n\nexample 11.16 (location-scale model) let y1, . . . , yn be a random sample from\nthe location-scale model y j = \u03b7 + \u03c4 \u03b5 j , where the \u03b5 j have density g. ingeneral, the\norder statistics s = (y(1), . . . , y(n)) form a minimal sufficient statistic for \u03b8 = (\u03b7, \u03c4 )\nbased on y1, . . . , yn. they may be re-expressed as\n\nt =(cid:15)\u03b8 = ((cid:15)\u03b7,(cid:15)\u03c4 ),\n\na =\n\n(cid:5)\n\ny(1) \u2212(cid:15)\u03b7(cid:15)\u03c4\n\n, . . . ,\n\n(cid:6)\n\n,\n\ny(n) \u2212(cid:15)\u03b7(cid:15)\u03c4\n\nwhere t consists of the maximum likelihood estimators of \u03b8, and the joint distribution\nof the maximal invariant a is degenerate but independent of \u03b7 and \u03c4 . the suitability\nof g can be checked by probability plots of a against quantiles of g. similar ideas\nextend to regression models.\n\ngiven a particular choice of g, agreement between the prior and data would be\n\nassessed through the conditional density of(cid:15)\u03b8 given a.\n\nwhen g is normal, the minimal sufficient statistic is (y, s2) and the assumption\nof normality is checked using the distribution of y given y and s2. example 5.14\nestablished that the raw residuals ((y1 \u2212 y)/s, . . . ,( yn \u2212 y)/s) are independent of y\nand s2.\n\nthe marginal joint distribution of y and s2 enables the prior to be criticized. for\n\n(cid:6)\ninstance, suppose that a joint conjugate prior is used for \u00b5 and \u03c3 2, with\n\n(cid:5)\n\n\u00b5 | \u03c3 2 \u223c n (\u00b50, \u03c3 2/k0),\n\n\u03c3 2 \u223c i g\n\n1\n2\n\n1\n2\n\n\u03bd0,\n\n\u03bd0\u03c3 2\n0\n\n.\n\nthen integration shows that the marginal densities of y and s2 are given by\n\nd1 =\n\n(cid:18)\n\ny \u2212 \u00b50\nn\u22121 + k\n\u22121\n0\n\n(cid:19)1/2\n\n\u03c30\n\n\u223c t\u03bd0\n\n,\n\nd2 = s2\n\n\u03c3 2\n0\n\n\u223c fn\u22121,\u03bd0\n\n.\n\nvalues of d1 and d2 that are unusual relative to the distributions of the corresponding\nrandom variables d1 and d2 can cast doubt on both prior and sampling models.\nfor example, if a probability plot cast no doubt on the assumption of normality, and\nd1 = 100 nevertheless, the relevance of the prior values \u00b50 and \u03c3 2\n0 would be called\ninto question. but if the data were not normal but cauchy, then y would have the\n\ni g(\u00b7,\u00b7) denotes the\ninverse gamma\ndistribution.\n\n "}, {"Page_number": 601, "text": "11.2 \u00b7 inference\n\n589\n\nsame distribution as y1 and very large values of d1 could arise even if the prior and\ndata agreed about \u00b5.\nconsider again the data of example 11.12, for which the model was normal. sup-\npose that our prior is that conditional on \u03c3 2, \u00b5 \u223c n (0, \u03c3 2), and that the prior distribu-\ntion for \u03c3 2 is i g(3, 3 \u00d7 1002). then d1 = 0.202 and d2 = 0.1424. the first is close\nenough to zero to cast no doubt on the prior mean, but d2 is rather small relative to\nthe f14,6 distribution, and casts some doubt on the prior variance. the corresponding\nbayesian p-values are pr(|d1| > |d1|) = 0.75 and pr(d2 < d2) = 0.045; the data are\n(cid:1)\nrather more precise than our prior information would suggest.\n\none overall measure of the plausibility of the data under the model is the probability\npr{ f (y+) \u2264 f (y)}, where f (y) isthe marginal density of the data actually observed,\nand y+ is a set of data that might have been observed (problem 11.12). some contro-\nversy surrounds this test and the p-values calculated in the previous example, as they\nflout the likelihood principle. one view is that the essence of bayesian inference is\nto use bayes\u2019 theorem to update prior belief in light of the data. this entails using\nposterior probabilities or equivalently bayes factors to compare competing models,\nand leaves no place for tail probability calculations. a contrary argument is that a\nbayes factor measures the relative support for two hypotheses and therefore requires\nprior specification of each, while some model-checking techniques do not require\niid\u223c n (0, 1), i am surprised\nexplicit alternatives: if my prior belief is that y1, . . . , y20\nto learn that the smallest value is \u221210, even before considering how this could have\narisen. furthermore, a strict interpretation of the argument for bayes factors requires\nthe specification of a proper prior distribution over all reasonable alternatives, which\nseems infeasible in practice. finally, the argument for the likelihood principle as-\nsumes that the model is correct and the case for strict adherence to the principle\nseems weaker when assessing fit than when performing inference for a parameter.\n\nprediction diagnostics\nmost models do not have a useful reduction in terms of exact minimal sufficient or\nancillary statistics, so the ideas outlined above cannot usually be applied. moreover,\n\u03c0(\u03b8) isoften improper in practice and then f (y) istypically improper also, though\nthis need not undercut diagnostic use of f (y | s) f (a) ifthere is a useful sufficient\nreduction. when \u03c0(\u03b8) isimproper, posterior predictive distributions can be used to\ndiagnose both problems with individual cases and more general model failures. the\nidea is to assess the posterior plausibility of suitable functions of the data.\none way to detect single outliers compares observations with their predicted values\nconditional on the remaining data through the conditional predictive ordinates f (y j |\ny\u2212 j ), where y\u2212 j consists of all the data except y j . since these quantities may be\nwritten in terms of ratios of densities, they depend less on the propriety of priors.\nthere is a close link to cross-validation.\n\nexample 11.17 (normal linear model)\nin the normal linear model with known\nn \u00d7 p design matrix x of rank p < n, the distribution of the n \u00d7 1 response vector y\nconditional on the p \u00d7 1 vector of parameters \u03b2 and the error variance \u03c3 2 is normal\n\n "}, {"Page_number": 602, "text": "590\n\n11 \u00b7 bayesian models\n\nwith mean x\u03b2 and covariance matrix \u03c3 2 in, and the least squares estimates and residual\nestimate of error\n\n(cid:15)\u03b2 = (x t x)\n\n\u22121 x t y,\n\ns2 = (n \u2212 p)\n\n\u22121 yt{i \u2212 x(x t x)\n\n\u22121 x t}y,\n\nare independent and minimal sufficient for \u03b2 and \u03c3 2.\n\nit would be alarming if the usual standardized residuals r j had no bayesian jus-\ntification. fortunately they do, as we now see. the simplest argument is that the\njoint distribution of a = (r1, . . . , rn) isfree of the parameters \u03b8 = (\u03b2, \u03c3 2), for which\n(cid:15)\u03b8 = ((cid:15)\u03b2, s2) form a complete minimal sufficient statistic. basu\u2019s theorem (page 649)\nimplies that a is independent of(cid:15)\u03b8, so weinfer from (11.20) that the sampling model\n\ncan be checked by comparing a to its joint distribution. this justifies residual plots\nand other tricks of the trade.\nfor alonger more tedious argument for bayesian use of deletion residuals and\nhence of the r j , wecompute the conditional predictive ordinate f (y j | y\u2212 j ) under the\nconjugate prior distribution for \u03b2 and \u03c3 2,\n\u03b2 | \u03c3 2 \u223c n (\u03b3 , \u03c3 2v ),\n\n\u03c3 2 \u223c i g\n\n(cid:6)\n\n(cid:5)\n\n\u03bd\u03c4 2\n\n\u03bd,\n\n,\n\n1\n2\n\n1\n2\n\nconcentrationally-\nchallenged readers may\nwant to jump to (11.23).\n\nwhere the hyperparameters are the p \u00d7 1 vector \u03b3 , the p \u00d7 p positive definite sym-\nmetric matrix v , and the scalars \u03bd and \u03c4 2; these are all regarded as known. an\nargument analogous to that leading to (11.13) gives\n\n\u03c0(\u03b2, \u03c3 2 | y) \u221d \u03c0(\u03b2 |(cid:15)\u03b2, \u03c3 2)\u03c0(\u03c3 2 | s2),\n\nso we need only find the posterior distributions of \u03b2 given(cid:15)\u03b2 and \u03c3 2 and of \u03c3 2 given\ns2. asthe joint distribution of (\u03b2 t,(cid:15)\u03b2 t)t given \u03c3 2 is\n\n(cid:13)(cid:5)\n\n(cid:6)\n\n(cid:5)\n\nn2 p\n\n\u03b3\n\u03b3\n\n, \u03c3 2\n\nv\nv v + (x t x)\n\nv\n\n\u22121\n\n(cid:6)(cid:14)\n\n,\n\n(3.21) and exercise 8.5.2 shows that the posterior distribution of \u03b2 given(cid:15)\u03b2 and \u03c3 2 is\n\nnormal with mean and variance matrix\n\n(x t x + v\n\n\u22121)\n\n\u22121(x t x(cid:15)\u03b2 + v\n\n\u22121\u03b3 ),\n\n\u03c3 2(x t x + v\n\n\u22121)\n\n\u22121,\n\nwhich generalizes (11.11). as prior uncertainty about \u03b3 increases, v\n\n(11.21)\n\u22121 \u2192 0, and\nthen we see from (11.21) that the posterior mean and variance of \u03b2 approach(cid:15)\u03b2 and\n\u22121. direct calculation shows that the posterior distribution of \u03c3 2 given s2 is\n\u03c3 2(x t x)\ni g[(\u03bd + n)/2,{\u03bd\u03c4 2 + (n \u2212 p)s2}/2]. if the constant prior \u03c0(\u03b2) \u221d 1 isused, then the\nposterior mean and variance of \u03b2 given \u03c3 2 are (cid:15)\u03b2 and \u03c3 2(x t x)\n\u22121, but the posterior\ndensity for \u03c3 2 is i g[(\u03bd + n \u2212 p)/2,{\u03bd\u03c4 2 + (n \u2212 p)s2}/2]; letting \u03bd \u2192 0 gives the\neffect of taking \u03c0(\u03b2, \u03c3 2) \u221d \u03c3 \u22122.\nfor future reference we note that the distribution of y conditional on \u03c3 2 is nor-\nmal with mean x \u03b3 and variance \u03c3 2(i + x v x t), and that on integrating over the\n\n "}, {"Page_number": 603, "text": "11.2 \u00b7 inference\n\n591\n\n(cid:18)\n(cid:18)\n\n(cid:19)\n(cid:19)|i + x v x t|1/2\n\n(\u03bd\u03c4 2)\u03bd/2\n\nn+\u03bd\n2\n\nprior distribution for \u03c3 2, wefind that the marginal density f (y) has a multivariate\nt form\n\n\u0001\n\n\u03c0 n/2\u0001\n\n\u22121(y \u2212 x \u03b3 )}\u2212(n+\u03bd)/2.\n(11.22)\nto find the posterior predictive density of another observation y+ with p \u00d7 1 co-\n\n{\u03bd\u03c4 2 + (y \u2212 x \u03b3 )t(i + x v x t)\n\n\u03bd\n2\n\nvariate vector x+, assumed independent of y conditional on \u03b2 and \u03c3 2, wewrite\n\n(cid:4)\n(cid:4) (cid:4)\n(cid:4)\n\nf (y+ | y) =\n=\n\n=\n\nf (y+ | \u03b8)\u03c0(\u03b8 | y) d\u03b8\nf (y+ | \u03b2, \u03c3 2)\u03c0(\u03b2 |(cid:15)\u03b2, \u03c3 2)\u03c0(\u03c3 2 | s2) d\u03b2 d\u03c3 2\nf (y+ | \u03b2, \u03c3 2)\u03c0(\u03b2 |(cid:15)\u03b2, \u03c3 2) d\u03b2 d\u03c3 2.\n\u03c0(\u03c3 2 | s2)\n\n(cid:4)\n\nnow\n\ny+ | \u03b2, \u03c3 2 \u223c n (x t+\u03b2, \u03c3 2),\n\u03b2 |(cid:15)\u03b2, \u03c3 2 \u223c n{(x t x + v\n\n\u22121(x t x(cid:15)\u03b2 + v\n\n\u22121)\n\nfrom which it follows that conditional on(cid:15)\u03b2 and \u03c3 2, the distribution of y+ is normal\n\nwith mean and variance\n\u22121)\n\nx t+(x t x + v\n\n\u22121(x t x(cid:15)\u03b2 + v\n\n\u22121\u03b3 ),\n\n\u03c3 2{1 + x t+(x t x + v\n\n\u22121)\n\n\u22121x+}.\n\n\u22121\u03b3 ), \u03c3 2(x t x + v\n\n\u22121)\n\n\u22121},\n\n\u22121(x t x(cid:15)\u03b2 + v\n\u22121)\n\nintegration over the posterior distribution of \u03c3 2 shows that the posterior predictive\ndistribution of y+ conditional on y is given by\n\ny+ \u2212 x t+(x t x + v\n(cid:23)(cid:24)\n(n\u2212 p)s2+\u03bd\u03c4 2\n\n\u22121\u03b3 )\n(cid:25){1 + x t+(x t x + v \u22121)\u22121x+}(cid:26)1/2\nfor prediction of y j given the other observations y\u2212 j , based on the improper prior\n\u03c0(\u03b2, \u03c3 2) \u221d \u03c3 \u22122, we set v\n\u22121 = 0 and \u03bd = 0 and replace y+ with y j , x+ with x j , n + \u03bd\nwith n \u2212 p \u2212 1, and(cid:15)\u03b2, s2 and x with the corresponding quantities(cid:15)\u03b2\u2212 j , s2\u2212 j and x\u2212 j\n\n\u223c tn+\u03bd .\n\n(11.23)\n\nn+\u03bd\n\nbased on y\u2212 j . then (11.23) becomes\ny j \u2212 x t\n(cid:18)\n\n(cid:24)\n\n(cid:23)\n\n1 + x t\n\nj\n\ns2\u2212 j\n\n(cid:15)\u03b2\u2212 j\nx t\u2212 j x\u2212 j\n\nj\n\n(cid:19)\u22121x j\n\n(cid:25)(cid:26)1/2\n\n\u223c tn\u2212 p\u22121.\n\na straightforward calculation reveals that the term in braces in the denominator here\nis (1 \u2212 h j )\n\u22121, where h j is the jth leverage based on the full model. hence prediction\nof y j given y\u2212 j may be based on the tn\u2212 p\u22121 distribution of the deletion residual\n\n(cid:18)\n\ny j \u2212 x t\n\nj\n\n(cid:19)\n\n(cid:15)\u03b2\u2212 j\n\ns\u2212 j\n\n=\n\n\u2217\nj\n\nr\n\n(1 \u2212 h j )1/2\n\n.\n\nthus outlier detection based on the conditional predictive ordinate is conducted using\n\u2217\nj . asthese are monotonic functions of the standardized\nthe usual deletion residuals r\n(cid:1)\nresiduals r j , this supports bayesian use of the r j .\n\n "}, {"Page_number": 604, "text": "592\n\n11 \u00b7 bayesian models\n\nmore general diagnostics can be based on measures of discrepancy between data\nand the model, d = d(y, \u03b8), compared to data y+ that might have been generated by\nthe model. posterior predictive checks are based on comparison of d+ = d(y+, \u03b8)\nwith its predictive distribution, via\n\npr{d(y+, \u03b8) \u2265 d(y, \u03b8) | y} ,\n\n(11.24)\n\nwhere the averaging is over both y+ and the posterior distribution of \u03b8. since y+ is\n(cid:4)\nindependent of y given \u03b8, wecan write\n\npr{d+ \u2265 d(y, \u03b8) | y, \u03b8} \u03c0(\u03b8 | y) d\u03b8 =\n\npr{d+ \u2265 d(y, \u03b8) | \u03b8} \u03c0(\u03b8 | y) d\u03b8.\nthus a simple way to evaluate (11.24) is to calculate pr{d+ \u2265 d(y, \u03b8) | \u03b8} for fixed\n\u03b8, and then to average this probability over the posterior density of \u03b8. one omnibus\nmeasure of discrepancy is the analogue of pearson\u2019s statistic,\n\n(cid:4)\n\nd(y, \u03b8) = n(cid:7)\n\nj=1\n\n{y j \u2212 e(y j | \u03b8)}2\n\nvar(y j | \u03b8)\n\n,\n\nbut this may be inappropriate, and typically d+ is chosen with key aspects of the\nmodel in mind. as mentioned above, authors differ over whether (11.24) should be\nused, though unlike the use of the marginal density of y, inference based on (11.24)\ndoes condition on the data.\n\n11.2.4 prediction and model averaging\nin the bayesian framework prediction is performed through the posterior predictive\ndensity (11.6). in practice this is not as simple as it appears, because there may be a\nnumber of possible models m1, . . . , mk on which to the base the prediction. condi-\ntional on mi , the predictive density for z based on y is f (z | y, mi ), but this ignores\nany uncertainty concerning the selection of mi . this uncertainty can be incorpo-\nrated by averaging over the posterior distribution of the model selected, to give the\nmodel-averaged prediction\n\nf (z | y) = k(cid:7)\n\ni=1\n\nf (z | y, mi )pr(mi | y)\n\n(11.25)\n\nwhich is an average of the posterior distributions of z under the different models,\nweighted according to their posterior probabilities\n\npr(mi | y) =\n\nf (y | mi )pr(mi )\n(cid:1)\nl=1 f (y | ml)pr(ml)\n\nk\n\n,\n\n(11.26)\n\nwhere\n\n(cid:4)\n(cid:2)\n\nf (y | mi ) =\nf (z | mi , y) =\n\nf (y | \u03b8i , mi )\u03c0(\u03b8i | mi ) d\u03b8i ,\nf (z | y, \u03b8i , mi ) f (y | \u03b8i , mi )\u03c0(\u03b8i | mi ) d\u03b8i\n\nf (y | mi )\n\n.\n\n "}, {"Page_number": 605, "text": "11.2 \u00b7 inference\nhere \u03b8i is the parameter for model mi , under which the prior is \u03c0(\u03b8i | mi ) and the\nprior probability of mi is pr(mi ). formally, (11.25) is just a re-expression of (11.6)\nin which the parameter splits into two parts, one a model indicator, mi , and the other\nthe parameters conditional on mi . inusing (11.25) it is crucial that z is the same\nquantity under all models considered, rather than one whose interpretation depends\non the model.\n\n593\n\nin practice the main obstacle to model averaging is computational. for each model,\nthe integrations involved must usually be done numerically using ideas described in\nsection 11.3. furthermore there can be many models in some applications \u2014 for\nexample, selecting among 15 covariates in a regression problem gives 215 = 32, 768\nmodels, corresponding to inclusion or exclusion of each covariate separately, without\nconsidering outliers, transformations, and so forth. thus it may be difficult to find the\nmost plausible models, quite apart from the calculations conditional on each model\nand the difficulties of specifing a prior over model space \u2014 giving the same weight\nto all combinations of covariates will rarely be sensible.\n\nexample 11.18 (cement data) we fit linear models to the data in table 8.1 with\nn = 13 observations and four covariates. there are 24 possible subsets of the co-\nvariates, giving us models m1, . . . , m16, which for sake of illustration we regard as\nequally probable a priori, though in practice we should hope that a small number\nof covariates is more likely than a large number. the models are on different pa-\nrameter spaces, so the discussion in section 11.2.2 implies that proper, preferably\nweak, priors should be used. we use the conjugate prior described in example 11.17,\nand without loss of generality centre and scale each covariate vector to have average\nzero and unit variance. we then set v to be the 5 \u00d7 5 matrix with diagonal elements\n\u03c62(v, 1, 1, 1, 1), where v is the sample variance of y, \u03b3 t = (y, 0, 0, 0, 0), \u03bd = 2.58,\n\u03c4 2 = 0.28, and \u03c6 = 2.85. this choice implies that the elements of \u03b2 are independent\na priori, and should give a weak but proper prior that is consistent between different\nmodels and invariant to location and scale changes of the response and explanatory\nvariables.\n\nthe marginal density of y under this model is (11.22); for each subset of covariates\nwe use the corresponding submatrix of v . table 11.6 shows the quantities 2 log b10,\nwhere b10 = pr(y | m1)/pr(y | m0) isthe bayes factor in favour of a subset of co-\nvariates relative to the model with none, the posterior probabilities of each subset, and,\nfor comparison, the residual sums of squares under the usual linear models, which\nare broadly in line with the probabilities.\nlet us try and predict the value of a new response y+ with covariates x t+ =\n(1, 10, 40, 20, 30). conditional on a particular subset of covariate vectors, the predic-\ntive distribution for y+ is given by (11.23). figure 11.3 shows these densities for the six\nmodels shown in table 11.6 to have non-negligible support, and the model-averaged\n(cid:1)\npredictive density.\n\na different approach to dealing with model uncertainty is to find a plausible model,\nf (y | \u03c8)\u03c0(\u03c8), and then add further parameters \u03bb whose variation allows for the most\nuncertain aspects of the model, together with a prior that expresses belief about them.\n\n "}, {"Page_number": 606, "text": "table 11.6 bayesian\nprediction using model\naveraging for the cement\ndata. for each of the 16\npossible subsets of\ncovariates, the table shows\nthe log bayes factor in\nfavour of that subset\ncompared to the model\nwith no covariates and\ngives the posterior\nprobability of each model.\nthe values of the posterior\nmean and scale\nparameters a and b are\nalso shown for the six\nmost plausible models;\n(y+ \u2212 a)/b has a posterior\nt density. for comparison,\nthe residual sums of\nsquares are also given.\n\nfigure 11.3 posterior\npredictive densities for\ncement data. predictive\ndensities for y+ based on\nindividual models are\ngiven as dotted curves,\nand the heavy curve is the\naveraged prediction from\nall 16 models.\n\n594\n\n11 \u00b7 bayesian models\n\nmodel\n\nrss\n\n2 log b10\n\npr(m | y)\n\na\n\nb\n\n\u2013 \u2013 \u2013 \u2013\n1 \u2013 \u2013 \u2013\n\u2013 2 \u2013 \u2013\n\u2013 \u2013 3 \u2013\n\u2013 \u2013 \u2013 4\n1 2 \u2013 \u2013\n1 \u2013 3 \u2013\n1 \u2013 \u2013 4\n\u2013 2 3 \u2013\n\u2013 2 \u2013 4\n\u2013 \u2013 3 4\n1 2 3 \u2013\n1 2 \u2013 4\n1 \u2013 3 4\n\u2013 2 3 4\n1 2 3 4\n\n2715.8\n1265.7\n906.3\n1939.4\n883.9\n57.9\n1227.1\n74.8\n415.4\n868.9\n175.7\n48.11\n47.97\n50.84\n73.81\n47.86\n\n0.0\n7.1\n12.2\n0.6\n12.6\n45.7\n4.0\n42.8\n19.3\n11.0\n31.3\n43.6\n47.2\n44.2\n33.2\n45.0\n\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.2027\n0.0000\n0.0480\n0.0000\n0.0000\n0.0002\n0.0716\n0.4344\n0.0986\n0.0004\n0.1441\n\n93.77\n\n2.31\n\n99.05\n\n2.58\n\n95.96\n95.88\n94.66\n\n2.80\n2.45\n2.89\n\n95.20\n\n2.97\n\ny\nt\ni\ns\nn\ne\nd\n\n \n\ne\nv\ni\nt\nc\nd\ne\nr\np\n\ni\n\n \nr\no\ni\nr\ne\n\nt\ns\no\np\n\n0\n2\n\n.\n\n0\n\n5\n1\n\n.\n\n0\n\n0\n1\n\n.\n\n0\n\n5\n0\n\n.\n\n0\n\n0\n\n.\n\n0\n\n80\n\n85\n\n90\n\n95\n\ny+\n\n100\n\n105\n\n110\n\nthis gives an expanded model f (y | \u03c8, \u03bb)\u03c0(\u03c8, \u03bb), to which (11.6) is then applied\nwith \u03b8 = (\u03c8, \u03bb).\n\nexercises 11.2\n1 find elements \u02dc\u03b8 and \u02dcj ( \u02dc\u03b8) ofthe normal approximation to a beta density, and hence check\nthe formulae in example 11.11. find also the posterior mean and variance of \u03b8. give an\napproximate 0.95 credible interval for \u03b8. how does this differ from a 0.95 confidence\ninterval? comment.\n\n2 let y1, . . . ,y n be a random sample from the uniform distribution on (0, \u03b8), and take as\n\nprior the pareto density with parameters \u03b2 and \u03bb,\n\n\u03c0(\u03b8) = \u03b2\u03bb\u03b2 \u03b8\u2212\u03b2\u22121,\n\n\u03b8 > \u03bb,\n\n\u03b2, \u03bb > 0.\n\n(a) find the prior distribution function and quantiles for \u03b8, and hence give prior one- and\ntwo-sided credible intervals for \u03b8. if\u03b2 > 1, find the prior mean of \u03b8.\n\n "}, {"Page_number": 607, "text": "11.2 \u00b7 inference\n\n595\nthe posterior density of \u03b8 is pareto with parameters n + \u03b2 and\n(b) show that\nmax{y1, . . . ,y n, \u03bb}, and hence give posterior credible intervals and the posterior mean\nfor \u03b8.\n(c) interpret \u03bb and \u03b2 in terms of a prior sample from the uniform density.\n\n3 check the details of example 11.7.\n\n4 two independent samples y1, . . . ,y n\n\niid\u223c n (\u00b5, \u03c3 2) and x1, . . . , xm\n\niid\u223c n (\u00b5, c\u03c3 2) are\navailable, where c > 0 isknown. find posterior densities for \u00b5 and \u03c3 based on prior\n\u03c0(\u00b5, \u03c3 ) \u221d 1/\u03c3 .\n5 verify (11.21), (11.22), and (11.23). how do (11.21) and (11.22) change when var(y j |\n\u03b2, \u03c3 2) = \u03c3 2/w j , the w j being known weights?\n6 travelling in a foreign country, you arrive at midnight in a town you have never heard of.\nyou have noidea of its size. the first thing you see is a bus with the number y = 100.\nwhat is a reasonable estimate of the total number \u03b8 of buses in the town, assuming that\nthey are numbered 1, . . . , \u03b8?\n(a) explain why it is sensible to use the improper prior \u03c0(\u03b8) \u221d \u03b8\u22121, \u03b8 = 1, 2, . . . . as-\nsuming that f (y | \u03b8) isuniform on 1, . . . , \u03b8 , show that \u03b8 has posterior density\n\n\u03c0(\u03b8 | y) =\n\n\u03b8\u22122(cid:1)\u221e\nu=y u\u22122\n\n\u03b8 = y, y + 1, . . . .\n\n,\n\n(b) show that the posterior mean of \u03b8 is infinite. show also that the posterior distribution\nfunction is approximately\n\npr(\u03b8 \u2264 v | y)\n\n.=\n\n(cid:2) v+1/2\n(cid:2) \u221e\n\u22122 du\ny\u22121/2 u\ny\u22121/2 u\u22122 du\n\n,\n\nand that the posterior median is approximately 2y \u2212 3/2. give an equi-tailed 95% posterior\nconfidence interval and a 95% hpd interval for \u03b8.\n(c) what would you conclude if you saw two buses, numbered 100 and 30?\n7 in example 11.12, calculate the bayes factor for h0 : \u00b5 \u2264 0 and h1 : \u00b5 >0.\n8 a forensic laboratory assesses if the dna profile from a specimen found at a crime scene\nmatches the dna profile of a suspect. the technology is not perfect, as there is a (small)\nprobability \u03c1 that a match occurs by chance even if the suspect was not present at the\nscene, and a (larger) probability \u03b3 that a match is reported even if the profiles are different;\nthis can arise due to laboratory error such as cross-contamination or accidental switching\nof profiles.\n(a) let r, s, and m denotes the events that a match is reported, that the specimen does\nindeed come from the suspect, and that there is a match between the profiles, and suppose\nthat\npr(r | m \u2229 s) = pr(r | m \u2229 s) = pr(r | m) = 1, pr(m | s) = 0, pr(r | s) = 1.\nshow that the posterior odds of the profiles matching, given that a match has been reported,\ndepend on\n\npr(r | s)\npr(r | s)\n\n= pr(r | m \u2229 s)pr(m | s) + pr(r | m \u2229 s)pr(m | s)\npr(r | m \u2229 s)pr(m | s) + pr(r | m \u2229 s)pr(m | s)\n\n,\n\n\u22122.\n\n\u22123, 10\n\nand establish that this equals {\u03c1 + \u03b3 (1 \u2212 \u03c1)}\u22121.\n(b) tabulate pr(r | s)/pr(r | s) when \u03c1 = 0, 10\n10\n(c) at what level of posterior odds would you be willing to convict the suspect, if the only\nevidence against them was the dna analysis, and you should only convict if convinced\nof their guilt \u2018beyond reasonable doubt\u2019? would your chosen odds level depend on the\nlikely sentence, if they are found guilty? how does your answer depend on the prior odds\nof the profiles matching, pr(s)/pr(s)?\n\n\u22123 and \u03b3 = 0, 10\n\n\u22126, 10\n\n\u22129, 10\n\n\u22124,\n\nm denotes the\ncomplement of m, and \u2229\nmeans \u2018and\u2019.\n\n "}, {"Page_number": 608, "text": "596\n\n11 \u00b7 bayesian models\n\n9 one way to set the ratio of arbitrary constants that appears when two models are compared\nusing bayes factors and improper priors is by imaginary observations: weimagine the\nsmallest experiment that would enable the models to be discriminated but maximizes\nevidence in favour of h0, and then choose the constants so that the bayes factor equals\none for these data.\nconsider data from a poisson process observed on [0, t0], and let h0 and h1 represent the\nmodels with rates \u03bb(t) = \u03c1 and \u03bb(t) = \u00b5\u03b2\u22121{1 \u2212 exp(\u2212\u03b2t)}, where \u03c1, \u00b5, \u03b2 > 0. take\nimproper priors \u03c0(\u03c1) = c0\u03c1\u22121 and \u03c0(\u00b5, \u03b2) = c1\u00b5\u22122, with c1, c0 > 0.\n(a) explain why the smallest experiment that enables the models to be discriminated must\nhave two events, and show that it gives pr(y | h0) = c0/t 2\n0 . find pr(y | h1) and show that\nit is minimized when both events occur at t0, with\n\n(cid:5)\n\n(cid:6)\n\npr(y | h1) = c1\n\n\u22122\u03b2t0\n\u03b2e\n1 \u2212 e\u2212\u03b2t0\n\nd\u03b2 = c1t\n\n\u22122\n0\n\n\u2212 1\n\n.\n\n\u03c0 2\n6\n\ndeduce that the device of imaginary observations gives c0/c1 = \u03c0 2/6 \u2212 1.\n(b) compute the bayes factor when these two models are compared using the data in\ntable 6.13. discuss.\n(section 6.5.1; raftery, 1988; spiegelhalter and smith, 1982)\n\n10 a random sample y1, . . . , yn arises either from a log-normal density, with log y j \u223c\nn (\u00b5, \u03c3 2), or from an exponential density \u03c1\u22121e\n\u2212y/\u03c1. the improper priors chosen are\n\u03c0(\u03c1) = c0/\u03c1 and \u03c0(\u00b5, \u03c3 ) = c1/\u03c3 , for \u03c1, \u03c3 > 0 and c0, c1 > 0. use imaginary obser-\nvations to give a value for c1/c0.\n\n(cid:4) \u221e\n\n0\n\n11.3 bayesian computation\n11.3.1 laplace approximation\nthe goal of bayesian data analysis is posterior inference for quantities of interest,\nand this involves integration over one or more of the parameters. usually the inte-\ngrals cannot be obtained in closed form and numerical approximations must be used.\ndeterministic integration procedures such as gaussian quadrature can sometimes be\napplied, but they are typically useful only for low-dimensional integrals, and have\nthe drawback of requiring information about the position and width of any modes of\nthe integrand that unavailable in practice. the most powerful tool for approximate\ncalculation of posterior densities is numerical integration by monte carlo simulation,\nto which we turn after describing an analytical approach known as laplace\u2019s method.\n\nconsider the one-dimensional integral\n\n(cid:4) \u221e\n\nin =\n\n\u2212nh(u) du,\n\ne\n\n(11.27)\nwhere h(u) is asmooth convex function with minimum at u = \u02dcu, atwhich point\ndh(\u02dcu)/du = 0 and d2h(\u02dcu)/du2 > 0. for compactness of notation we write h2 =\nd2h(\u02dcu)/du2, h3 = d3h(\u02dcu)/du3, and so forth. close to \u02dcu a taylor series expansion\ngives h(u)\n\n.= h(\u02dcu) + 1\n\n\u2212\u221e\n\n2 h2(u \u2212 \u02dcu)2, so\n\u2212nh(\u02dcu)\n\n.= e\n\nin\n\n= e\n\u2212nh(\u02dcu)\n(cid:5)\n\n=\n\n2\u03c0\nnh2\n\n\u2212\u221e\n\n(cid:4) \u221e\n(cid:4) \u221e\n(cid:6)1/2\n\n\u2212\u221e\n\ne\n\n\u2212nh2(u\u2212\u02dcu)2/2 du\n\ne\n\ne\n\n\u2212z2/2 du\ndz\n\ndz\n\n\u2212nh(\u02dcu),\n\n "}, {"Page_number": 609, "text": "11.3 \u00b7 bayesian computation\nwhere the first and second equalities use the substitution z = (nh2)1/2(u \u2212 \u02dcu) and\nthe fact that the normal density has unit integral. a more detailed accounting\n(exercise 11.3.2) gives\n\n597\n\n(cid:5)\n\n(cid:6)1/2\n\nin =\n\n2\u03c0\nnh2\n\n(cid:13)\n\n(cid:5)\n\n\u2212nh(\u02dcu) \u00d7\n\ne\n\n1 + n\n\n\u22121\n\n5h2\n3\n24h3\n2\n\n\u2212 h4\n8h2\n2\n\n(cid:6)\n\n(cid:14)\n\n+ o(n\n\n\u22122)\n\n.\n\n(11.28)\n\nthe leading term on the right of (11.28) is known as the laplace approximation to\nin, and we denote it by \u02dcin.\nthere are several points to note about (11.28). first, as in/\u02dcin = 1 + o(n\n\u22121), the\nerror is relative, and \u02dcin is often remarkably accurate. second, \u02dcin involves only h and\nits second derivative at \u02dcu, so it isrelatively easy to obtain, numerically if necessary.\nthird, the right-hand side of (11.28) is an asymptotic series for in, implying that its\npartial sums need not converge, and that the approximation may not be improved\nby including further terms of the series. and fourth, because the bulk of the normal\nprobability integral lies within three standard deviations of its centre, the limits of\nthe integral will not affect \u02dcin provided they lie outside the interval with endpoints\n\u02dcu \u00b1 3(nh2)\nin the multivariate case, with h(u) again a smooth convex function but u a vector\nof length p, the same argument but using the multivariate normal density shows that\nthe laplace approximation to (11.27) is\n\n\u22121/2 or so.\n\n(cid:5)\n\n2\u03c0\nn\n\n(cid:6) p/2 |h2|\u22121/2e\n\n\u2212nh(\u02dcu),\n\n(11.29)\nwhere \u02dcu solves the p \u00d7 1 system of equations \u2202h(u)/\u2202u = 0 and |h2| is the determi-\nnant of the p \u00d7 p matrix of second derivatives \u2202 2h(u)/\u2202u\u2202ut, evaluated at u = \u02dcu, at\nwhich point the matrix is positive definite.\n\nin applications an approximation is often required to an integral of form\n\n(cid:4)\n\n(cid:28)1/2\n\n(cid:27)\n\nn\n2\u03c0\n\njn(u0) =\n\n\u2212ng(u){1 + o(n\n\n\u22121)} du,\n\na(u)e\n\nu0\n\u2212\u221e\n\n(11.30)\n\nwhere u is scalar, a(u) > 0, and in addition to possessing the properties of h(u) above,\ng is such that g(\u02dcu) = 0. the first step in approximating (11.30) is to change the\nvariable of integration from u to r(u) = sign(u \u2212 \u02dcu){2g(u)}1/2; that is, r 2/2 = g(u).\nthen g\n\n(u) = dg(u)/du and r(u) have the same sign, and rdr/du = g\n\u22121)} dr\n\n\u2212nr 2/2{1 + o(n\n\n(u), so\n\na(u)\n\ne\n\n(cid:9)\n\n(cid:9)\n\nr\ng(cid:9)(u)\n\njn(u0) =\n=\n\n(cid:4)\n(cid:4)\n\n(cid:28)1/2\n(cid:28)1/2\n\n(cid:27)\n(cid:27)\n\nn\n2\u03c0\nn\n2\u03c0\n\nr0\n\u2212\u221e\nr0\n\u2212\u221e\n\n\u2212nr 2/2+log b(r){1 + o(n\n\n\u22121)} dr,\n\ne\n\nwhere the positive quantity b(r) = a(u)r/g\n\n(cid:9)\n\nwe now change variables again, from r to r\n\n(u) is regarded as a function of r.\n\u2217 = r \u2212 (rn)\n\n\u22121 log b(r), so\n\n\u2212nr\n\n\u22172 = \u2212nr 2 + 2 log b(r) \u2212 n\n\n\u22121r\n\n\u22122{log b(r)}2.\n\n "}, {"Page_number": 610, "text": "jn(u0) =\n\nr\n\n\u2217\n0\n\n(cid:4)\n(cid:19) + o(n\n\u2212\u221e\n\ne\n\n(cid:27)\n\n(cid:28)1/2\n\n(cid:18)\n\nn\n2\u03c0\nn1/2r\n(cid:6)\n\n\u2217\n0\n\n,\n\n= \u0001\n(cid:5)\n\nv0\nr0\n\n598\nthe jacobian of the transformation and the third term in \u2212nr\nerror of jn(u0), so\n\n11 \u00b7 bayesian models\n\u22172 contribute only to the\n\n\u22172/2{1 + o(n\n\u2212nr\n\u22121),\n\n\u22121)} dr\n\n\u2217\n\n(11.31)\n\nwhere\n\u2217\n0\n\nr\n\n= r0 + (r0n)\n\n\u22121 log\n\nr0 = sign(u0 \u2212 \u02dcu){2g(u0)}1/2,\n\n(cid:9)\n\nv0 = g\n\n(u0)\na(u0)\n\n.\n\nvariants on this expression play an important role in chapter 12.\nhere is a further approximation for later use. let u = (u1, u2), where u1 is scalar\nand u2 a p \u00d7 1 vector, and consider\n\n\u2212( p+1)/2c\n\n(2\u03c0)\n\ndu1\n\ndu2 exp{\u2212nh(u1, u2)} ,\n\n(11.32)\n\n(cid:4)\n\n(cid:4)\n\u2212\u221e\n\nu0\n1\n\n(cid:4)\n\u2212\u221e\n\nu0\n1\n\nwhere c is constant, the inner integral being over ir p. here h has its previous smooth-\nness properties, is maximized at (\u02dcu1, \u02dcu2), and in addition h(\u02dcu1, \u02dcu2) = 0. we fix u1\nand apply laplace approximation to the inner integral, obtaining\n\n\u22121/2c\n\n(2\u03c0)\n\n|nh22(u1, \u02dcu21)|\u22121/2 exp{\u2212nh(u1, \u02dcu21)}{1 + o(n\n\n\u22121)} du1,\n\n(cid:18)\n\n(cid:18)\n\n(cid:19)\n\n(cid:18)\n\n(cid:18)\n\n2h\n\n\u2202h\n\n\u22121\n\nu0\n1\n\nu0\n1\n\n, \u02dcu20\n\n, \u02dcu20\n\n(cid:19)(cid:24)\n\n\u2212 \u02dcu1\n\nv0 = c\n\n(cid:10)(cid:10)h22\n\n(cid:19)(cid:25)1/2 ,\n\nwhere \u02dcu21 = \u02dcu2(u1) maximizes h(u1, u2) with respect to u2 when u1 is fixed, and\nh22(u1, u2) = \u2202 2h(u1, u2)/\u2202u2\u2202ut\n2 is the p \u00d7 p hessian matrix of h with respect to\nu2. apart from multiplicative constants, this integral has form (11.30), and so (11.31)\nmay be used to approximate to (11.32), with\nr0 = sign\nwhere \u02dcu20 is the maximizing value of u2 when u1 = u0\n1.\nalthough the formulation of (11.27), (11.30), and (11.32) in terms of n and the\no(1) functions h and g simplifies the derivation of (11.29) and (11.31) by clarifying\nthe orders of the various terms, for applications it is equivalent and usually simpler\nto set n = 1 and allow h and g and their derivatives to be o(n).\ninference\nof the hypotheses we write pr(y) = (cid:2)\none application of laplace approximation is to the bayes factor (11.17). for one\nf (y | \u03b8)\u03c0(\u03b8) d\u03b8, with integrand expressed as\nexp{\u2212h(\u03b8)}, where h(\u03b8) = \u2212(cid:12)m(\u03b8) and\n\n(cid:19)(cid:10)(cid:10)1/2,\n\nu0\n1\n\u2202u1\n\n, \u02dcu20\n\nu0\n1\n\n(cid:12)m(\u03b8) = log f (y | \u03b8) + log \u03c0(\u03b8)\n\nis the log likelihood modified by addition of the log prior. typically the first term of\n(cid:12)m is o(n), and the second is o(1). the value \u02dc\u03b8 that minimizes h(\u03b8) isthe maximum\n\n "}, {"Page_number": 611, "text": "11.3 \u00b7 bayesian computation\n\n599\n\nlog pr(y)\n\na posteriori estimate of \u03b8 \u2014 the value that maximizes the modified log likelihood \u2014\nand we can apply (11.29). the result is\n.= log f (y | \u02dc\u03b8) + log \u03c0( \u02dc\u03b8) \u2212 1\n2\n\n(cid:10)(cid:10)(cid:10)(cid:10),\n(cid:10)(cid:10)(cid:10)(cid:10)\u2212 \u2202 2(cid:12)m( \u02dc\u03b8)\nlikelihood estimate (cid:15)\u03b8, and if p is fixed we can drop terms that are o(1). crudely\n\nwhere p is the dimension of \u03b8. tofurther simplify this, note that in large samples the\nlog prior is negligible relative to the log likelihood and \u02dc\u03b8 is roughly the maximum\n\np log(2\u03c0) \u2212 1\n2\n\np log n + 1\n2\n\n\u2202\u03b8 \u2202\u03b8 t\n\nlog\n\nspeaking, therefore,\n\n\u22122 log pr(y)\n\n.= bic = \u22122 log f (y |(cid:15)\u03b8) + p log n.\n\nthis bayes information criterion, which we met in section 4.7, is used for rough\ncomparison of competing models.\n\nfor amore sophisticated application we write a vector parameter \u03b8 as (\u03c8, \u03bbt)t and\n\n(cid:2)\napproximate the marginal posterior density for the scalar \u03c8,\nf (y | \u03c8, \u03bb)\u03c0(\u03c8, \u03bb) d\u03bb\nf (y | \u03c8, \u03bb)\u03c0(\u03c8, \u03bb) d\u03bbd\u03c8\n\n\u03c0(\u03c8 | y) =\n\n(cid:2)\n\n,\n\n(11.33)\n\nby applying laplace\u2019s method to each integral. the discussion above gives the approx-\nimation to the denominator. for the numerator we take h\u03c8 (\u03bb) = \u2212(cid:12)m(\u03c8, \u03bb), where\nthe notation emphasises that the approximation is applied only to the integral over \u03bb,\nfor a fixed value of \u03c8. the resulting approximation may be written as\nf (y | \u03c8, \u02dc\u03bb\u03c8 )\u03c0(\u03c8, \u02dc\u03bb\u03c8 )\nf (y | \u02dc\u03c8 , \u02dc\u03bb)\u03c0( \u02dc\u03c8 , \u02dc\u03bb)\n\n(cid:10)(cid:10)(cid:10)\n(cid:10)(cid:10)(cid:10)\u2212 \u2202 2(cid:12)m ( \u02dc\u03c8 ,\u02dc\u03bb)\n(cid:10)(cid:10)(cid:10)\u2212 \u2202 2(cid:12)m (\u03c8,\u02dc\u03bb\u03c8 )\n(cid:10)(cid:10)(cid:10)\n\n\u03c0(\u03c8 | y)\n\n(cid:28)1/2\n\n\uf8f1\uf8f2\n\uf8f3\n\n\uf8fc\uf8fd\n\uf8fe\n\n(11.34)\n\nn\n2\u03c0\n\n.=\n\n(cid:27)\n\n\u2202\u03b8 \u2202\u03b8 t\n\n1/2\n\n,\n\n\u2202\u03bb\u2202\u03bbt\n\nwhere \u02dc\u03bb\u03c8 is the maximum a posteriori estimate of \u03bb for fixed \u03c8 and the denom-\ninator and numerator determinants are of hessian matrices of sides ( p \u2212 1) and\np respectively.\nthe posterior marginal cumulative distribution for \u03c8 may be approximated by\napplying (11.31) to the integral of (11.34) over the range (\u221e, \u03c80). we take u0 = \u03c80,\n\ng(\u03c8) = (cid:12)m( \u02dc\u03c8 , \u02dc\u03bb) \u2212 (cid:12)m(\u03c8, \u02dc\u03bb\u03c8 ),\n\na(\u03c8) =\n\n\uf8f1\uf8f2\n\uf8f3\n\n(cid:10)(cid:10)(cid:10)\n(cid:10)(cid:10)(cid:10)\u2212 \u2202 2(cid:12)m ( \u02dc\u03c8 ,\u02dc\u03bb)\n(cid:10)(cid:10)(cid:10)\n(cid:10)(cid:10)(cid:10)\u2212 \u2202 2(cid:12)m (\u03c8,\u02dc\u03bb\u03c8 )\n\n\u2202\u03b8 \u2202\u03b8 t\n\n\u2202\u03bb\u2202\u03bbt\n\n\uf8fc\uf8fd\n\uf8fe\n\n1/2\n\n,\n\nand set r\n\n\u2217\n0\n\n= r0 + r\n\n\u22121\n0\n\nlog(v0/r0), where\n\nr0 = sign(\u03c80 \u2212 \u02dc\u03c8)[2{(cid:12)m( \u02dc\u03c8 , \u02dc\u03bb) \u2212 (cid:12)m(\u03c80, \u02dc\u03bb\u03c80)}]1/2,\nv0 = \u2212 \u2202(cid:12)m(\u03c80, \u02dc\u03bb\u03c80)\n\n(cid:10)(cid:10) \u2212 \u2202 2(cid:12)m (\u03c80,\u02dc\u03bb\u03c80 )\n(cid:10)(cid:10) \u2212 \u2202 2(cid:12)m ( \u02dc\u03c8 ,\u02dc\u03bb)\n(cid:10)(cid:10)\n\n\uf8f1\uf8f2\n\uf8f3\n\n\uf8fc\uf8fd\n\uf8fe\n\n\u2202\u03bb\u2202\u03bbt\n\n\u2202\u03c8\n\n(cid:10)(cid:10)\n\n1/2\n\n;\n\n\u2202\u03b8 \u2202\u03b8 t\n\nhere \u02dc\u03bb\u03c80 is the maximum a posteriori estimate of \u03bb when \u03c8 is fixed at \u03c80. it isoften\nconvenient to find the derivatives numerically.\n\n "}, {"Page_number": 612, "text": "600\n\n11 \u00b7 bayesian models\n\nrate estimate (\u00d7102)\n\ncase\n\nx\n\ny\n\ncrude\n\nempirical bayes\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n94.320\n15.720\n62.880\n125.760\n5.240\n31.440\n1.048\n1.048\n2.096\n10.480\n\n5\n1\n5\n14\n3\n19\n1\n1\n4\n22\n\n5.3\n6.4\n8.0\n11.1\n57.3\n60.4\n95.4\n95.4\n190.8\n209.9\n\n6.1\n10.7\n9.1\n11.7\n58.8\n60.6\n80.0\n80.0\n143.7\n194.4\n\ntable 11.7 numbers of\nfailures y of ten pumps in\nx thousand operating\nhours, with the crude rate\nestimate y/x (gaver and\no\u2019muircheartaigh, 1987).\nthe final column gives\nempirical bayes rate\nestimates derived in\nproblem 11.26.\n\nnumerous variant approaches are possible. for example, the ratio of priors in the\nintegral of (11.34) may be included in the function a(u) of(11.30), which case (cid:12)m is\nsimply the log likelihood, \u02dc\u03b8 and \u02dc\u03bb\u03c8 are maximum likelihood estimates, the hessians\nare observed information matrices, and r0 is the directed likelihood ratio statistic\nfor testing the hypothesis \u03c8 = \u03c80. the prior then appears only in v0. the resulting\napproximation is generally poorer than that described above, but this idea does suggest\na quick way to assess sensitivity to the prior density. the key is to notice that the\napproximate effect on (11.34) of taking a different prior, \u03c01(\u03c8, \u03bb), say, would be\nto multiply (11.34) by the ratio c(\u03c8) = {\u03c01(\u03c8, \u02dc\u03bb\u03c8 )/\u03c0(\u03c8, \u02dc\u03bb\u03c8 )}/{\u03c01( \u02dc\u03c8 , \u02dc\u03bb)/\u03c0( \u02dc\u03c8 , \u02dc\u03bb)};\nthe effect is approximate because laplace approximation based on \u03c01 would not\nlead to integrals maximized at \u02dc\u03bb\u03c8 and ( \u02dc\u03c8 , \u02dc\u03bb). on the other hand, the effect on these\nmaximizing values of changing the prior is often relatively small. thus the effect\nof modifying the prior from \u03c0 to \u03c01 may be gauged by changing v0 to v0/c(\u03c80),\n\u2217\nand recalculating r\n0 ). this involves no further maximization or numerical\ndifferentation.\n\n\u2217\n0 and \u0001(r\n\nexample 11.19 (pump failure data) table 11.7 contains the numbers of failures y j\nof n = 10 pumps in operating periods of x j thousands of hours. the pumps are from\nseveral systems in the nuclear plant farley 1; pumps 1, 3, 4, and 6 operate continuously,\nwhile the rest operate only intermittantly or on standby. for now we suppose that the\npumps may be expected to have similar rates of failure, with the jth pump having\nfailure rate \u03bb j , and that conditional on \u03bb j , the numbers of failures y j have independent\npoisson distributions with means \u03bb j x j . wefurther suppose that the \u03bb j are independent\nrealizations of a gamma variable with parameters \u03b1 and \u03b2, and that \u03b2 itself has a prior\ngamma distribution with parameters \u03bd and \u03c6. thus\n\nf (y | \u03bb) = n(cid:3)\n\n(x j \u03bb j )y j\n\ny j !\n\ne\n\nj=1\n\n\u2212x j \u03bb j , \u03c0(\u03bb | \u03b2) = n(cid:3)\n\n\u03b2 \u03b1\u03bb\u03b1\u22121\nj\n\u0001(\u03b1)\n\n\u2212\u03b2\u03bb j ,\n\ne\n\n(11.35)\n\nj=1\n\n\u03c0(\u03b2) = \u03c6\u03bd \u03b2 \u03bd\u22121\n\n\u0001(\u03bd)\n\n\u2212\u03c6\u03b2 ,\n\ne\n\n "}, {"Page_number": 613, "text": "table 11.8 integrals of\ntwo approximate posterior\ndensities for \u03b2 for the\npumps data. the first, \u02dci1,\ninvolves a\none-dimensional laplace\napproximation to (11.36),\nwhile \u02dci10 involves\nten-dimensional laplace\napproximation. the table\nshows how the integral\nchanges when the\ncurvature of the likelihood\nis increased by a.\n\n11.3 \u00b7 bayesian computation\n\n601\n\na\n\u02dci1\n\u02dci10\n\n1\n1.022\n1.782\n\n2\n1.017\n1.309\n\n3\n1.014\n1.183\n\n4\n1.012\n1.127\n\n5\n1.011\n1.096\n\n10\n1.009\n1.042\n\n20\n1.007\n1.019\n\n(cid:24)\n\nn(cid:3)\nj=1\n\nn(cid:3)\nj=1\n\nso that the joint density of the data y, the rates \u03bb, and \u03b2 is\n\nf (y | \u03bb) f (\u03bb | \u03b2)\u03c0(\u03b2) = c\n\n\u03bby j+\u03b1\u22121\n\nj\n\ne\n\n\u2212\u03bb j (x j+\u03b2)\n\n(cid:25) \u00d7 \u03b2n\u03b1+\u03bd\u22121e\n\n\u2212\u03c6\u03b2 ,\n\n(11.36)\n\n(cid:4) \u221e\n\n0\n\n\u2212h(\u03b2) d\u03b2,\n\ne\n\nwhere c is a constant of proportionality.\n\nto find the conditional density of \u03b2, weintegrate over the \u03bb j , toobtain\n\nf (y, \u03b2) = c\n\n(x j + \u03b2)\n\n\u2212(y j+\u03b1)\u0001(y j + \u03b1)\n\n(cid:25) \u00d7 \u03b2n\u03b1+\u03bd\u22121e\n\n\u2212\u03c6\u03b2 ,\n\n(11.37)\n\n(cid:24)\n\nn(cid:3)\nj=1\n\nfrom which the marginal density of y is obtained by further integration to give\n\nf (y) = c\n\n\u0001(y j + \u03b1) \u00d7\nwhere h(\u03b2) = \u03c6\u03b2 \u2212 (n\u03b1 + \u03bd \u2212 1) log \u03b2 +(cid:1)\n\n(y j + \u03b1) log(x j + \u03b2); we use i to de-\n\nnote the integral in this expression.\nfor sake of illustration we take a proper but fairly uninformative prior for \u03b2,\nwith \u03bd = 0.1 and \u03c6 = 1, and take \u03b1 = 1.8. application of laplace\u2019s method to i\nthen results in the approximate posterior density for \u03b2, \u02dc\u03c0(\u03b2 | y) = \u02dci\n\u22121 exp{\u2212h(\u03b2)},\nwhich has integral 1.022.\n\nthe accuracy of laplace\u2019s method can be tested by taking a different approach,\nin which we first integrate (11.36) over \u03b2, and then apply the multivariate version of\nlaplace\u2019s method to the resulting ten-dimensional integral with respect to the \u03bb j . in\nthis case the density approximation has integral 1.782, because the ten-dimensional\nintegral approximation, \u02dci10, isless accurate than \u02dci1. tocompare the two approaches we\nrecalculate the approximations for data (ax j , ay j ) and various values of a. this leaves\nunchanged the failure rates y j /x j , but increases by a factor a the fisher information for\neach of the \u03bb j , thereby increasing the curvature of the log likelihood and the accuracy\nof the approximation. the results in table 11.8 show that \u02dci10 rapidly improves as a\nincreases, and that with counts about 4\u20135 times as large as those observed, laplace\u2019s\nmethod gives adequately accurate answers, even in ten dimensions. in practice, of\ncourse, \u02dci1 would be used.\nto calculate approximate posterior densities for \u03bb j , weintegrate (11.36) over\n\u03bbi , i (cid:11)= j, and then apply laplace\u2019s method to the numerator and denominator\nintegrals of\n\n\u03c0(\u03bb j | y) = \u03bby j+\u03b1\u22121\n\nj\n\n(cid:2) \u221e\n(cid:2) \u221e\n\u2212\u03bb j x j\n\u2212h j (\u03b2) d\u03bb\n0 e\n\u0001(y j + \u03b1)\n0 e\u2212h(\u03b2) d\u03b2\n\ne\n\n,\n\n "}, {"Page_number": 614, "text": "11 \u00b7 bayesian models\n\nfigure 11.4\napproximate posterior\ndensities for \u03b2 and \u03bb2 for\nthe pumps data, based on\nlaplace approximation.\n\ny\nt\ni\ns\nn\ne\nd\n\n \nr\no\ni\nr\ne\n\nt\ns\no\np\n\n5\n\n \n \n \n \n \n \n \n \n\n4\n\n \n \n \n \n \n \n \n \n \n\n3\n\n \n \n \n \n \n \n \n \n\n2\n\n \n \n \n \n \n \n \n \n\n1\n\n \n \n \n \n \n \n \n \n \n\n0\n\n0\n\n2\n\n4\n\n6\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\nbeta\n\nlambda2\n\n602\n\ny\nt\ni\ns\nn\ne\nd\n\n \nr\no\ni\nr\ne\n\nt\ns\no\np\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\nwhere\n\nh j (\u03b2) = (\u03c6 + \u03bb j )\u03b2 \u2212 (n\u03b1 + \u03bd \u2212 1) log \u03b2 +\n\n(cid:7)\ni(cid:11)= j\n\n(yi + \u03b1) log(xi + \u03b2).\n\nthe resulting denominator is again \u02dci1, while the numerator must be recalculated at\neach of a range of values of \u03bb j . figure 11.4 shows these approximate densities for\n\u03b2 and for \u03bb2. that for \u03bb2 has integral 1.0004 and is presumably closer to one because\n(cid:1)\nit is based on a ratio of laplace approximations.\n\nthe ideal situation for laplace approximation is when the posterior density is\nstrongly unimodal. when the posterior is multimodal, the approximation can be ap-\nplied separately to each mode \u2014 provided they can all be found. different approxi-\nmations apply when the posterior is peaked at the end of its range (exercise 11.3.5).\n\n11.3.2 importance sampling\nmany monte carlo techniques may be applied in bayesian computation. in this section\nwe discuss ideas based on importance sampling, and in the next section we turn to\niterative methods based on simulating markov chains. importance sampling gives\nindependent samples, and so measures of uncertainty for estimators are usually fairly\nreadily obtained, but it applies to a limited range of problems. iterative methods are\nmore widely applicable but it can be difficult to assess their convergence and to give\nstatements of uncertainty for their output.\n\nsuppose we wish to calculate an integral of form\n\n(cid:4)\n\n\u00b5 =\n\nm(\u03b8, y, z)\u03c0(\u03b8 | y) d\u03b8.\n\nif we take m(\u03b8, y, z) = i (\u03b8 \u2264 a), for example, then \u00b5 = pr(\u03b8 \u2264 a | y), while tak-\ning m(\u03b8, y, z) = f (z | y, \u03b8) gives\u00b5 = f (z | y), the posterior predictive density for\nz given the data. suppose that direct computation of \u00b5 is awkward, but that it is\n\n "}, {"Page_number": 615, "text": "11.3 \u00b7 bayesian computation\n\n603\n\nstraightforward both to generate a sample \u03b81, . . . , \u03b8s from a density h(\u03b8) whose sup-\nport includes that of \u03c0(\u03b8 | y), and to calculate m(\u03b8, y, z) and f (y | \u03b8). we can then\napply importance sampling for estimation of \u00b5, obtaining the unbiased estimator\n(section 3.3.2)\n\n(cid:15)\u00b5 = s\n\n\u22121\n\ns(cid:7)\ns=1\n\n\u03c0(\u03b8s | y)\nh(\u03b8s)\n\n= s\n\n\u22121\n\ns(cid:7)\ns=1\n\nm(\u03b8s , y, z)\n\n(11.38)\nsay, where w(\u03b8) = \u03c0(\u03b8 | y)/ h(\u03b8) is animportance sampling weight. an important\nadvantage of(cid:15)\u00b5 over the iterative procedures to be disussed later is that its variance is\n\nm(\u03b8s , y, z)w(\u03b8s),\n\nreadily obtained (exercise 11.3.6).\n\nin practice the importance sampling ratio estimator of \u00b5,\n\n(cid:15)\u00b5rat =\n\n(cid:1)\n(cid:1)\ns\ns=1 m(\u03b8s , y, z)w(\u03b8s)\n\n,\n\ns\ns=1 w(\u03b8s)\n\nis more commonly used. this is typically less variable than (cid:15)\u00b5; indeed it performs\n\nperfectly if m(\u03b8, y, z) isconstant, as is clear from its variance, given by (example 2.25)\n\n$var((cid:15)\u00b5rat) =\n\n1\n\ns(s \u2212 1)\n\ns(cid:7)\ns=1\n\n{m(\u03b8s , y, z) \u2212(cid:15)\u00b5rat}2w(\u03b8s)2\n\nw 2\n\n, w = s\n\n\u22121\n\nw(\u03b8s).\n\ns(cid:7)\ns=1\n\n(cid:25)\n\n(cid:24)(cid:15)\u03b8 , j ((cid:15)\u03b8)\n\nas usual with importance sampling, a good choice of h(\u03b8) iscrucial if the simula-\n, where(cid:15)\u03b8 and j ((cid:15)\u03b8) are the maximum likelihood\ntion is to be useful. one possibility is a normal approximation to the posterior density\nof \u03b8, taking h(\u03b8) to be n\nestimate and the observed information. normal approximation may be better if ap-\nplied to a transformed parameter \u03c8 = \u03c8(\u03b8), however, while the light-tailed normal\ndistribution typically gives too few simulations in the tail of the posterior density.\nhence it is usually better to generate the \u03b8s from a shifted and rescaled t\u03bd density.\n\n\u22121\n\nexample 11.20 (challenger data) table 1.3 gives data on launches of the space\nshuttle, including the ill-fated challenger launch. in examples 1.3, 4.5 and 4.33\nwe saw how these data may be modelled using a logistic regression model, under\nwhich the number of o-rings suffering thermal distress when a launch takes place at\n1f isbinomial with denominator m = 6 and probability \u03c0(\u03b2 + \u03b21x1) =\n\u25e6\ntemperature x\nexp(\u03b20 + \u03b21x1)/{1 + exp(\u03b20 + \u03b21x1)}. the likelihood (4.6) for this model is shown\nin figure 4.3. let us represent the data for the 23 successful launches by y, with\nlikelihood f (y | \u03b8); here \u03b8 = (\u03b20, \u03b21).\none aspect of interest when deciding whether to launch the challenger should\nhave been the number z of distressed o-rings at its launch temperature of x1 = 31\n\u25e6\nf.\nf (z | \u03b8) isbinomial with denominator m = 6\nwe suppose that, conditional on \u03b8,\nand probability \u03c0(\u03b20 + 31\u03b21), independent of other launches. then in the bayesian\nframework we should calculate the posterior predictive density for z,\n\n(cid:2)\n\n(cid:2)\nf (z | \u03b8) f (y | \u03b8)\u03c0(\u03b8) d\u03b8\n\nf (y | \u03b8)\u03c0(\u03b8) d\u03b8\n\n,\n\nwhere \u03c0(\u03b8) isthe prior density on (\u03b2 0, \u03b21).\n\n "}, {"Page_number": 616, "text": "importance\n\nfigure 11.5\nsampling applied to\nshuttle data. left: pairs\n(\u03b20, \u03b21) simulated from a\nprior density, with log\nlikelihood contours\nsuperimposed. pairs\nwhose weight ws exceeds\n\u22121 are shown as\n(100s)\nblobs. the other pairs\nhave very low likelihoods\nand hence essentially zero\nposterior probabilities ws .\nright: posterior predictive\ndensity for the number of\ndistressed o-rings for a\n\u25e6\nlaunch at 31\nf, using beta\nprior with a = b = 0.5\n(blobs), a = b = 1 (1) and\na = 1, b = 4 (2),\nestimated by importance\nsampling with\ns = 10, 000.\n\n604\n\n1\na\n\nt\n\ne\nb\n\n5\n0\n0\n\n.\n\n5\n0\n\n.\n\n0\n-\n\n5\n1\n\n.\n\n0\n-\n\n5\n2\n0\n-\n\n.\n\n..\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\u2022\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n-40\n..\n.\n.\n.\n\u2022\n..\n.\n.\n.\n\u2022\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\u2022\n\u2022\n.\n-200\n\u2022\n\u2022\n.\n.\n.\n.\n.\n\u2022\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\u2022\n\u2022\n.\n.\n.\n\u2022\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n\u2022 \u2022\n\u2022\n.\n. .\n.\n\u2022\n.\n.\n\u2022\n.\n.\n.\n..\n\u2022\n. .\n.\n\u2022\n.\n-20\n. .\n.\n.\n.\n.\n.\n.\n.\n.\n. .\n\u2022\n.\n.\n.\n\u2022\n.\n.\n.\n.\n.\n..\n.\n.\n.\n\u2022\u2022\n\u2022\n.\n.\n.\n.\n\u2022\n.\n.\n.\n.\n\u2022\n.\n.\n.\n.\n.\n.\n\u2022\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n...\n.\n.\n.\n.\n.\n\u2022\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\u2022\n\u2022\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\u2022\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n.\n.\n.\n\u2022\n.\n.\n.\n\u2022\n\u2022\n.\n.\n.\n.\n.\n.\n.\n.\n.\n..\n.\n.\n.\n-20\n..\n..\n.\n.\n.\n\u2022\n.\n.\n.\n.\n\u2022\n.\n.\n\u2022\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\u2022\n.\n.\n.\n\u2022\n.\n.\n.\n\u2022 \u2022\n\u2022\n\u2022\n.\n.\n.\n.\n.\n.\n.\n\u2022\n.\n.\n.\n.\n\u2022\n\u2022\n\u2022\u2022\n..\n.\n.\n\u2022\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\u2022\n\u2022\n.\n.\n..\n.\n.\n..\n.\n\u2022\n.\n.\n\u2022\n.\n.\n\u2022\n\u2022\n\u2022\n.\n.\n..\n.\n.\n\u2022\n.\n.\n\u2022\n.\n.\n.\n.\n.\n\u2022\n.\n.\n.\n.\n.\n.\n\u2022\n.\n.\n.\n.\n\u2022\n.\n.\n.\n.\n\u2022\n.\n.\n\u2022\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\u2022\n.\n\u2022\n.\n\u2022\n.\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\n\u2022\n.\n.\n.\n.\n.\n.\n\u2022\n.\n.\n\u2022\n.\n.\n.\n.\n.\n\u2022\n.\n.\n.\n.\n.\n.\n\u2022\n.\n.\n.\n.\n.\n.\n\u2022\n.\n.\n.\n.\n.\n\u2022\n.\n\u2022\n\u2022\n.\n-200\n.\n.\n.\n\u2022\n.\n.\n.\n.\n.\n.\n.\n.\n.\n-100\n.\n\u2022\n\u2022\n.\n.\n.\n-40\n.\n\u2022\n-20\n.\n\u2022\n\n-100\n.\n\n-100\n\n-40\n\n-5\n\n0\n\n5\n\n10\n\n15\n\n11 \u00b7 bayesian models\n\ny\nt\ni\nl\ni\n\nb\na\nb\no\nr\np\n\n \nr\no\ni\nr\ne\n\nt\ns\no\np\n\n5\n0\n\n.\n\n4\n\n.\n\n0\n\n3\n\n.\n\n0\n\n2\n\n.\n\n0\n\n1\n\n.\n\n0\n\n0\n0\n\n.\n\n\u2022\n1\n2\n\n\u2022\n2\n1\n\n2\n\u2022\n1\n\n2\n1\n\u2022\n\n2\n1\n\u2022\n\n2\n\n3\n\n4      5       6\n\n2\n1\n\u2022\n\n1\n\n2\n1\n\u2022\n\n0\n\nbeta0\n\nnumber of distressed o-rings\n\n\u25e6\n\nthe parameters \u03b20 and \u03b21 are difficult to interpret directly, and instead we consider\nthe probabilities \u03c01 = \u03c0(\u03b2 + 60\u03b21) and \u03c02 = \u03c0(\u03b2 + 80\u03b21) that a single o-ring will\nbe distressed at 60 and 80\nf. in practice specification of the joint prior density of \u03c01\nand \u03c02 would require engineering expertise, but in default of this we simply suppose\nthat they have independent beta densities (11.3) with a = b = 1/2. for the initial step\nof the importance sampling algorithm we generate 10,000 independent pairs (\u03c01, \u03c02)\nand then set\n\u03b21 =\n\n\u03b20 = log\n\n\u2212 60\u03b21.\n\n(cid:13)\n\n(cid:14)\n\n(cid:13)\n\n(cid:14)\n\nlog\n\n1\n\n,\n\n\u03c02(1 \u2212 \u03c01)\n\u03c02(1 \u2212 \u03c01)\n\n\u03c01\n1 \u2212 \u03c01\n\n80 \u2212 60\n\n(cid:1)\n\nthe left panel of figure 11.5 shows some of the resulting pairs \u03b8s = (\u03b20, \u03b21),\nsuperimposed on contours of the log likelihood. pairs whose weight ws exceeds one-\nhundredth of its average are shown by blobs. about 30% of the simulated values\nws = 0.9996, so just 4/10,000ths of the posterior\nfall into this category, for which\nprobability is placed on the other 7000 pairs. this occurs both because the prior is\nmuch more dispersed than the likelihood, and because they are mismatched, in the\nsense that the prior value of \u03b21 for a given \u03b20 is generally too large \u2014 the mode of\nf (\u03b21 | \u03b20) lies to the right of that of f (y | \u03b21, \u03b20), considered as a function of \u03b21 for\nfixed \u03b20.\nthe right panel of figure 11.5 shows the posterior probabilities of z = 0, . . . ,\n6 distressed rings. there is appreciable probability of damage to most of the rings, as\npr(z \u2265 4 | y)\n(cid:1)\n\n.= 0.65, with little dependence on the prior.\n\nthis examples show both the strengths and weaknesses of importance sampling.\nit is simple to apply, and because \u03b81, . . . , \u03b8s are independent it is easy to obtain a\n\nstandard error for(cid:15)\u00b5, and then to increase s if necessary. on the other hand the prior\n\nis sometimes so overdispersed relative to the likelihood that s must be huge before\nan appreciable number of the ws are non-zero, and a better importance sampling\ndistribution must be found. this problem becomes acute when the dimension of \u03b8\nis large and the curse of dimensionality bites. there are clever ways to improve\n\n "}, {"Page_number": 617, "text": "11.3 \u00b7 bayesian computation\n\n605\n\nimportance sampling in such situations, but markov chain methods apply readily to\nmany high-dimensional problems, and to these we now turn.\n\n11.3.3 markov chain monte carlo\nthe idea of markov chain monte carlo simulation is to construct a markov chain that\nwill, if run for an infinitely long period, generate samples from a posterior distribution\n\u03c0, specified implicitly and known only up to a normalizing constant. although it\nhas roots in areas such as statistical physics, its application in mainstream bayesian\nstatistics is relatively recent and the discussion below is merely a snapshot of a topic\nin full spate of development. the reader whose memory of markov chains is hazy\nmay find it useful to review the early pages of section 6.1.1.\n\ngibbs sampler\nlet u = (u1, . . . , uk) be arandom variable of dimension k whose joint density \u03c0(u) is\nunknown. our goal is to estimate aspects of \u03c0(u), such as joint or marginal densities\nand their quantiles, moments such as e(u1) and var(u1), and so forth. although\n\u03c0(u) itself is unknown, we suppose that we can simulate observations from the full\nconditional densities \u03c0(ui | u\u2212i ), where u\u2212i = (u1, . . . ,u i\u22121, ui+1, . . . ,u k). often in\npractice the constant normalizing \u03c0(u) isunknown, but as it does not appear in the\n\u03c0(ui | u\u2212i ), this causes no difficulty. if \u03c0(u) isproper, then the hammersley\u2013clifford\ntheorem implies that under mild conditions \u03c0(u) isdetermined by these densities;\nthis does not imply that any set of full conditional densities determines a proper joint\ndensity. gibbs sampling is successive simulation from the \u03c0(ui | u\u2212i ) according to\nthe algorithm:\n\nthe term gibbs sampling\ncomes from an analogy\nwith statistical physics,\nwhere similar methods are\nused to generate states\nfrom gibbs distributions.\nin that context it is called\nthe heat bath algorithm.\n\n1.\n2. then for i = 1, . . . , i ,\n\ninitialize by taking arbitrary values of u (0)\n1\nu1 | u2 = u (i\u22121)\nu2 | u1 = u (i)\n\n, . . . , u (0)\nk\n, . . . ,u k = u (i\u22121)\n, u3 = u (i\u22121)\n\n(cid:18)\n(cid:18)\n\n.\n\n2\n\nk\n\n(cid:19)\n\n,\n\n(a) generate u (i)\n1\n(b) generate u (i)\n2\n(c) generate u (i)\n3\n\nfrom \u03c0\nfrom \u03c0\nfrom\n\n1\n\n3\n\n(cid:18)\n\nu3 | u1 = u (i)\n\n1\n\n, u2 = u (i)\n\n2\n\n, u4 = u (i\u22121)\n\n4\n\n\u03c0\n\nk\n\n, . . . ,u k = u (i\u22121)\n(cid:19)\n\n, . . . ,u k = u (i\u22121)\n\nk\n\n(cid:19)\n\n,\n\n,\n\n...\n(d) generate u (i)\nk\n\n(cid:18)\n\nfrom \u03c0\n\nuk | u1 = u (i)\n\n1\n\n, . . . ,u k\u22121 = u (i)\nk\u22121\n\n(cid:19)\n\n.\n\nhere we update each of the u j in turn, basing each value generated on the k \u2212 1\nprevious simulations. this gives a stream of random variables\n, . . . , u (i\u22121)\n\n. . . , u (i\u22121)\n\n,\n\n,\n\n, . . . , u (1)\nk\n\n, u (2)\n1\n\n, . . . , u (i )\nk\n\n, . . . , u (2)\nk\n\n, u (i )\n1\n\nu (1)\n1\n\n1\n\nk\n\nso for the jth component of u we have a sequence u (1)\nj\n\n, . . . , u (i )\nj\n\n.\n\n "}, {"Page_number": 618, "text": "606\n\n11 \u00b7 bayesian models\n\nto see why we might hope that (u (i )\n) isapproximately a sample from\n\u03c0(u), suppose that k = 2 and that u1 and u2 take values in the finite sets {1, . . . ,n }\n1\nand {1, . . . ,m }. wewrite their joint and marginal densities as\n\n, . . . , u (i )\nk\n\npr(u1 = r, u2 = s) = \u03c0(r, s),\npr(u1 = r) = \u03c01(r) = m(cid:7)\npr(u2 = s) = \u03c02(s) = n(cid:7)\n\ns=1\n\n\u03c0(r, s),\n\n\u03c0(r, s),\n\nr=1\n\nr = 1, . . . ,n ,\n\ns = 1, . . . ,m ,\n\n,\n\n,\n\nqrs = pr(u2 = s | u1 = r) = \u03c0(r, s)\n\u03c01(r)\n\nwith \u03c01(r), \u03c02(s) > 0 for all r and s. the conditional densities are\npsr = pr(u1 = r | u2 = s) = \u03c0(r, s)\n\u03c02(s)\nwhich we express as an m \u00d7 n matrix p21 with (s, r) element psr and an n \u00d7 m matrix\np12 with (r, s) element qrs. these transition matrices give the probabilities of going\nfrom the m possible values of u2 to the n possible values of u1 and back again. as\nthey are ratios, prs and qrs do not involve the normalizing constant for \u03c0.\nif f0 is an m \u00d7 1 vector containing the distribution of u (0)\n2 , the distributions of\nu (1)\n, u (1)\n0 p21 p12 p21, . . . . thus each iteration of\n0 p21 p12, f t\n2\n1\nstep 2 of the algorithm corresponds to postmultiplying the current distribution of u (i)\nby the m \u00d7 m matrix h = p21 p12. hence u (i )\n2\n0 h i . conditional\n2 has distribution f t\non u (i)\n, . . . , u (i )\nis independent of earlier values, so the sequence u (1)\nis a\n2\n2\nmarkov chain with transition matrix h. ifthe chain is ergodic, then u (i )\n2 has a unique\nlimiting distribution f as i \u2192 \u221e, satisfying the equation f t h = f t. asthis limit\nis unique, we need only show that f is the marginal distribution of u2 to see that\nthe algorithm ultimately produces a variable with density \u03c02. nowthe rth element of\n2 h = \u03c0 t\n\u03c0 t\n\n2 , u (i+1)\n\n, . . ., are f t\n\n0 p21, f t\n\n, u (2)\n1\n\n2\n\n2 p21 p12 equals\nn(cid:7)\nt=1\n\nm(cid:7)\ns=1\n\n\u03c02(t) ptsqsr = n(cid:7)\n\nt=1\n\nm(cid:7)\ns=1\n\n\u03c02(t)\n\n\u03c0(s, t)\n\u03c02(t)\n\n\u03c0(r, s)\n\u03c01(s)\n\n= \u03c02(r),\n\n, . . . , u (i )\n1\n\nso \u03c02 is indeed the unique solution to the equation f t h = f t. bysymmetry,\nu (1)\nis a markov chain with transition matrix p12 p21 and limiting dis-\n1\ntribution \u03c01. moreover the fact that \u03c0 t\n1 ensures that the joint distribution\n2 ) converges to \u03c0(r, s) as i \u2192 \u221e. generalization to k > 2 works in an\nof (u (i )\n, u (i )\n1\nobvious way.\n\n2 p21 = \u03c0 t\n\nmost of the densities \u03c0(u) met in applications are continuous, so this argument is\nnot directly applicable. however any continuous density can be closely approximated\nby one with countable support, for which essentially the same results hold, so it is not\nsurprising that the ideas apply more widely, and from now on we shall assume that\nthey are applicable to our problems.\n\nsuch a simulation will only be useful if convergence to the stationary distri-\nbution is not too slow. in discrete cases like that above, the convergence rate\nis determined by the modulus of the second largest eigenvalue l2 of h, where\n1 = l1 \u2265 |l2| \u2265 \u00b7\u00b7\u00b7. if| l2| < 1, then convergence is geometrically ergodic; see (6.4).\n\n "}, {"Page_number": 619, "text": "11.3 \u00b7 bayesian computation\nin the continuous case it can occur that |l2| =1 orthat\nl2 does not exist, either\nof which will spell trouble. a reversible chain has real eigenvalues and satisfies\nthe detailed balance condition (6.5). hence it can be useful to make the chain re-\nversible, for example by generating variables in order 1, . . . ,k , k \u2212 1, . . . ,2, . . . or\nby choosing the next update at random. either involves modifying step 2 of the\nalgorithm.\n\n607\n\noutput analysis\nthe only sure way to know how long a markov chain simulation algorithm should\nbe run is by theoretical analysis to determine its rate of convergence. this requires\nknowledge of the stationary distribution being estimated, however, and is possible\nonly in very special cases. a more pragmatic approach is to declare that the algo-\nrithm has converged when its output satisfies tests of some sort. such convergence\ndiagnostics can at best detect non-convergence, however; they cannot guarantee that\nthe output will be useful. both empirically- and theoretically-based diagnostics have\nbeen proposed, and references to them are given in the bibliographic notes. empirical\napproaches include contrasting output from the start and the end of a run, and compar-\ning results from parallel independent runs whose initial values have been chosen to\nbe overdispersed relative to the target distribution. theoretical approaches generally\nassess whether the output satisfies known properties of stationary chains. in practice\nit is sensible to use several diagnostics but also to scrutinize time series plots of the\noutput. as different parameters may converge at different rates, it is important to\nexamine all parameters of interest and also global quantities such as the current log\nlikelihood, prior, and posterior.\n\nif stationarity seems to have been attained, then it is useful to examine correlograms\nand partial correlograms of output. if the autocorrelations are high, then the statisti-\ncal efficiency of the algorithm will be low. a chain with low correlations will yield\nestimators with smaller variance, and is more likely to visit all regions of significant\nprobability mass. the algorithm may need modification to reduce high autocorrela-\ntions, for example by reparametrization; see example 11.24.\n\nmultimodal target densities are awkward because it can be hard to know if all\nsignificant modes have been visited. use of widely separated starting values may\nthen be useful, and so too may be occasional insertion of large random jumps into\nthe algorithm, so that it effectively restarts from a location unrelated to its previous\nposition.\nsuppose that the chain seems to have converged after b iterations and is run for a\ntotal of i (cid:10) b iterations. in general discussion below we suppose that i is so much\nlarger than b that inference can safely be based on all i iterations, but in practice\n(cid:2)\n(cid:2) |m(u)|\u03c0(u) du < \u221e. unless there is qualitative knowledge\nwe use only output from iterations b + 1, . . . , i . let the quantity of interest be \u00b5 =\nm(u)\u03c0(u) du, where\nabout \u03c0(u) this may involve an act of faith. for example, taking m(u) = u1 gives\n\u00b5 = e(u1), which could be infinite although \u03c0(u) isproper. hence unless properties\nof the posterior density are known it is safer to base inferences on density and quantile\n\n "}, {"Page_number": 620, "text": "608\n\n11 \u00b7 bayesian models\n\n(11.40)\n\nestimates than on moments. if \u00b5 is finite then it can be estimated by the ergodic\naverage\n\n(cid:19)\n\n(cid:18)\nu (i)\n\ni(cid:7)\ni=1\n\nm\n\n\u22121\n\n(cid:15)\u00b5 = i\nk ). the ergodic theorem (6.2) implies that (cid:15)\u00b5 con-\n, . . . , u (i)\n(cid:18)\n\n(11.39)\n\n,\n\nwhere u (i) denotes (u (i)\nverges almost surely to \u00b5 as i \u2192 \u221e, and under further conditions\n1\ni 1/2 ((cid:15)\u00b5 \u2212 \u00b5)\n< \u221e,\nso(cid:15)\u00b5 is approximately normal for large i . inthat case\n=\n(i \u2212 |i|) \u03b3i \u223c \u03c3 2\n\ni \u00d7 var((cid:15)\u00b5) = i\n\n, where\n\n\u03b3i = \u03b30\n\n0, \u03c3 2\nm\n\n\u22121\n\n(cid:19)\n\nd\u2212\u2192 n\ni\u22121(cid:7)\ni=\u2212i+1\n\n0 < \u03c3 2\nm\n\u221e(cid:7)\ni=\u2212\u221e\n\nm\n\n\u221e(cid:7)\ni=\u2212\u221e\n\n\u03c1i ,\n\nwhere \u03b3i = cov{m(u (0)), m(u (i))} depends on \u03c0 and on the construction of the chain,\nand \u03c1i = \u03b3i /\u03b30 is the ith autocorrelation. the marginal variance of m(u ) is\u03b3 0 =\nto inflate var((cid:15)\u00b5) by a factor \u03c4 = (cid:1)\u221e\nvar\u03c0{m(u )}, which depends only on m and \u03c0. the effect of using correlated output is\nso an estimate(cid:15)\u03c4 from a pilot run may suggest how large i should be. the obvious\n\u2212\u221e \u03c1i relative to an independent sample of size i ,\nsimple possibility is(cid:15)\u03c4 = (cid:1)\n0.05, say, and gives the standard error for (cid:15)\u00b5 as if the block averages were a simple\n\nanother approach splits the output into b blocks of k successive iterations, with\nk taken so large that the block averages of the m(u (i)) have correlations lower than\n\n(cid:15)\u03c1i , where m = (cid:15)3(cid:15)\u03c4(cid:16) is found by iteration.\n\nestimator of \u03c4 based on the correlogram is inconsistent, but better ones exist. one\n\nm\ni=\u2212m\n\nrandom sample.\n\nthe density of u1 at u1 may be estimated by a kernel method (section 7.1.2), or\n\nby the unbiased estimator (7.12), written in this context as\n\n(cid:18)\n\ni(cid:7)\ni=1\n\n\u22121\n\ni\n\nu1 | u (i)\u22121\n\n\u03c0\n\n(cid:19)\n\n.\n\n(11.41)\n\nthe discussion above presupposes a single long run of the chain. an alternative is\ns independent parallel runs of length i , leading ultimately to s independent values\nu (i ) from \u03c0(u). an estimate based on these may be less variable than one based on\ns i dependent samples from a single chain, and its variance is more easily estimated.\nroughly s b iterations must be disregarded, however, compared to b when there is\nonly one chain. from this viewpoint a single run is preferable, but it is then harder to\ndetect lack of convergence.\n\nexample 11.21 (bivariate normal density)\nmeans zero, variances one and correlation \u03c1, then\n\nif (u1, u2) are bivariate normal with\n(cid:13)\n\n(cid:14)\n\n\u03c0(u1 | u2) =\n\n1\n\n(1 \u2212 \u03c12)1/2\n\n\u03c6\n\nu1 \u2212 \u03c1u2\n(1 \u2212 \u03c12)1/2\n\n,\n\nwith a symmetric result for \u03c0(u2 | u1), and we can use the marginal standard\nnormal densities of u1 and u2 to assess convergence. the upper left panel of\nfigure 11.6 shows the contours of the joint density when \u03c1 = 0.75, together with a\nsample path of the process starting from an initial value generated uniformly on the\n\n(cid:15)x(cid:16) is the smallest integer\ngreater than or equal to x.\n\n\u03c6 denotes the standard\nnormal density.\n\n "}, {"Page_number": 621, "text": ", u(i)\n\n2 ), for\n\nfigure 11.6 gibbs\nsampler for bivariate\nnormal density. top left:\ncontours of the bivariate\nnormal density with\n\u03c1 = 0.75, with the first\nfive iterations of a gibbs\nsampler; the blobs are at\n(u(i)\ni = 0, . . . ,5, starting\n1\nfrom the top left of the\npanel. top right: sample\n1 and u (i)\npaths of u (i)\nfor\ni = 1, . . . ,100. bottom\n2\nleft: kernel density\nestimates of \u03c01(u1) (heavy\nsolid) based on 100\nparallel chains after i\niterations, with i = 0\n(solid), 2 (dots), 5\n(dashes), 10 (large\ndashes), and 100 (largest\ndashes); the bandwidth is\nchosen by uniform\ncross-validation. bottom\nright: estimates (dots) of\n\u03c01(u1) (heavy solid) after\n100 iterations of 5\nreplicate chains, based on\n(11.41).\n\n11.3 \u00b7 bayesian computation\n\n609\n\n\u2022\n\n4\n\n2\n\n2\nu\n\n0\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n-4\n\n-2\n\n2\n\n4\n\n0\n\nu1\n\n2\n-\n\n4\n-\n\n4\n4\n\n.\n.\n\n0\n0\n\n3\n3\n\n.\n.\n\n0\n0\n\n2\n2\n\n.\n.\n\n0\n0\n\n.\n.\n\n1\n1\n0\n0\n\n0\n0\n\n.\n.\n\n0\n0\n\nf\nf\nd\nd\np\np\n\n1\nu\n\n4\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n2\n\n \n \n \n \n \n \n \n \n \n \n \n\n0\n\n2\n-\n\n4\n-\n\n4\n\n2\n\n2\nu\n\n0\n\n2\n-\n\n4\n-\n\nf\nd\np\n\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n\niteration\n\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n\niteration\n\n4\n\n.\n\n0\n\n3\n\n.\n\n0\n\n2\n\n.\n\n0\n\n.\n\n1\n0\n\n0\n\n.\n\n0\n\n-4\n-4\n\n-2\n-2\n\n0\n0\n\nu1\nu1\n\n2\n2\n\n4\n4\n\n-4\n\n-2\n\n2\n\n4\n\n0\n\nu1\n\nsquare (\u22124, 4) \u00d7 (\u22124, 4). the updating scheme forces the sample path to consist of\nsteps parallel to the coordinate axes. the upper right panel shows that the sample\npaths of the markov chains appear to converge rapidly to their limit distributions,\nthe estimated variance inflation factor (cid:15)\u03c4\nas the calculations in problem 11.20 show will be the case. this is confirmed by\n.= 3. the lower left panel shows rapid\nconvergence of the kernel density estimates to their target, based on s = 100 parallel\nchains. the lower right panel illustrates the variability of (11.41), which here\n(cid:1)\nperforms better than the kernel estimator.\n\nbayesian application\nthe essence of bayesian inference is to treat all unknowns as random variables, and to\ncompute their posterior distributions given the data y. the gibbs sampler is applied by\ntaking u1, . . . , uk to be the unknowns, usually parameters, and simulating conditional\non y. the full conditional densities \u03c0(ui | u\u2212i ) are typically of form \u03c0(\u03b8i | \u03b8\u2212i , y)\n\n "}, {"Page_number": 622, "text": "610\n\n11 \u00b7 bayesian models\n\nand must be obtained before the algorithm can be applied. fortunately this is often\npossible for \u2018nice\u2019 models, where the full conditional densities have conjugate forms.\n\nexample 11.22 (random effects model) the sampling model in the simplest nor-\nmal one-way layout is\n\nytr = \u03b8t + \u03b5tr ,\niid\u223c n (\u03bd, \u03c3 2\nwhere \u03b81, . . . , \u03b8t\n\u03b8 ) and independent of this \u03b5tr\ninterest is usually \u03c3 2 and \u03c3 2\n\u03b8 .\n\nt = 1, . . . , t , r = 1, . . . , r,\n\niid\u223c n (0, \u03c3 2). the focus of\n\nbayesian analysis requires prior information, which we suppose to be expressed\n\nthrough the conjugate densities\n\u00b5 \u223c n (\u00b50, \u03c4 2),\n\n\u03c3 2 \u223c i g(\u03b1, \u03b2),\n\n\u03b8 \u223c i g(\u03b1\u03b8 , \u03b2\u03b8 ).\n\u03c3 2\n(cid:19)\n\n(cid:18)\n\n\u03c0\n\n(cid:18)\n\n\u03b8 | y\n\n\u00b5, \u03b8, \u03c3 2, \u03c3 2\n\nthe full posterior density is then\n\n(cid:19) \u221d f (y | \u03b8, \u03c3 2) f\n(11.42)\nwe now take (u1, u2, u3, u4) = (\u03c3 2\n\u03b8 , \u03c3 2, \u00b5, \u03b8), and calculate the full conditional\ndensities needed for gibbs sampling, always treating the data y as fixed. each calcu-\nlation requires integration over just one parameter. for example,\n\n\u03c0(\u00b5)\u03c0(\u03c3 2)\u03c0\n\n\u03b8 | \u00b5, \u03c3 2\n\n\u03c3 2\n\u03b8\n\n(cid:18)\n\n(cid:19)\n\n.\n\n\u03b8\n\n\u03c0\n\n(cid:18)\n\n\u03b8 | \u03c3 2, \u00b5, \u03b8, y\n\u03c3 2\n\n(cid:19) =\nf (y | \u03b8, \u03c3 2) f\n(cid:2)\n(cid:18)\nf (y | \u03b8, \u03c3 2) f\n(cid:19)\n(cid:18)\n\u03b8 | \u00b5, \u03c3 2\n(cid:2)\n(cid:18)\n(cid:19)\n=\nf\n\u03b8 | \u00b5, \u03c3 2\n(cid:19)\nf\n\u03b8\n= \u03c0\n\u03b8 | \u00b5, \u03b8\n\u03c3 2\n.\nsimilar calculations reveal that \u03c0(\u03b8 | \u03c3 2\n(cid:19) = \u03c0(\u03c3 2 | \u03b8, y), \u03c0\n\n\u03c3 2 | \u03c3 2\n\n\u03b8 , \u00b5, \u03b8, y\n\n(cid:18)\n\n(cid:18)\n\n\u03c0\n\n\u03b8\n\n(cid:18)\n\n(cid:18)\n\u03c0(\u00b5)\u03c0(\u03c3 2)\u03c0\n\u03c0(\u00b5)\u03c0(\u03c3 2)\u03c0\n\n(cid:19)\n\n(cid:18)\n\n(cid:19)\n\n\u03c3 2\n\u03b8\n\n\u03c3 2\n\u03b8\n\nd\u03c3 2\n\u03b8\n\n\u03b8\n\n(cid:19)\n(cid:18)\n\u03b8 | \u00b5, \u03c3 2\n(cid:19)\n\u03b8 | \u00b5, \u03c3 2\n(cid:19)\n(cid:18)\n\u03b8\n(cid:19)\n\u03c3 2\n\u03c0(\u00b5)\u03c0\n\u03b8\n\u03c0(\u00b5)\u03c0\n\n(cid:18)\n\n\u03c3 2\n\u03b8\n\nd\u03c3 2\n\u03b8\n\n\u03b8 , \u03c3 2, \u00b5, y) does not simplify, but that\n\n\u00b5 | \u03c3 2\n\n\u03b8 , \u03c3 2, \u03b8, y\n\n(cid:18)\n\n(cid:19) = \u03c0\n(cid:22)\n\n,\n\n(cid:19)\n\n.\n\n\u00b5 | \u03c3 2\n\n\u03b8 , \u03b8\n\n(11.43)\n\n(11.44)\n\n(cid:22)\n\n,\n\n(11.45)\n\n.\n\n(11.46)\n\narguments paralleling those in example 11.12 lead to\n\n\u03b8 | \u00b5, \u03b8 \u223c i g\n\u03c3 2\n\n\u03c3 2 | \u03b8, y \u223c i g\n(cid:5)\n\n\u00b5 | \u03c3 2\n\n\u03b8 , \u03b8 \u223c n\n\n(cid:21)\n\n(cid:21)\n\n\u03b1\u03b8 + 1\nt , \u03b2\u03b8 + 1\n2\n2\nt r, \u03b2 + 1\n2\n\n\u03b1 + 1\n2\n\u03b8 \u00b50 + \u03c4 2\n\n(cid:1)\nt=1\n\u03b8 + t \u03c4 2\n\n\u03c3 2\n\nt\n\n\u03c3 2\n\nt(cid:7)\nt=1\nt(cid:7)\nt=1\n\n\u03b8t\n\n,\n\n\u03c3 2\n\n(\u03b8t \u2212 \u00b5)2\nr(cid:7)\nr=1\n\u03c3 2\n\u03b8 \u03c4 2\n\u03b8 + t \u03c4 2\n\n(cid:6)\n\n(ytr \u2212 \u03b8t )2\n\nthe conditional density \u03c0(\u03b8 | \u03c3 2\ngiven \u00b5, \u03c3 2\nwhile the prior density for \u03b8t given \u03c3 2\ndensity for \u03b8t is\n\n\u03b8 , \u03c3 2, \u00b5, y) ismost readily calculated by noting that\n\u03b8 and \u03c3 2, the statistic yt is sufficient for \u03b8t , with distribution n (\u03b8t , \u03c3 2/r),\n\u03b8 ). hence the posterior\n\n\u03b8 , \u03c3 2, and \u00b5 is n (\u00b5, \u03c3 2\n\n(cid:6)\n\n(cid:5)\n\n\u03b8t | \u03c3 2\n\n\u03b8 , \u03c3 2, \u00b5, y \u223c n\n\n\u03b8 yt + \u03c3 2\u00b5\n\u03b8 + \u03c3 2\nr\u03c3 2\nand the \u03b8t are conditionally independent.\n\nr\u03c3 2\n\n,\n\n\u03c3 2\n\u03b8 \u03c3 2\n\u03b8 + \u03c3 2\nr\u03c3 2\n\nt = 1, . . . , t ,\n\n,\n\n(11.47)\n\n "}, {"Page_number": 623, "text": "11.3 \u00b7 bayesian computation\n\n611\n\ntable 11.9 estimated\nposterior means and\nstandard deviations for the\nmodel fitted to the blood\ndata, and simple\nfrequentist estimates from\nanalysis of variance.\n\nfigure 11.7 graphs for\nrandom effects model of\nexample 11.22. left:\ndirected acyclic graph\nshowing dependence of\nrandom variables (circles)\non themselves and on\nfixed quantities\n(rectangles). right:\nconditional independence\ngraph, formed by\nmoralizing the directed\nacyclic graph, that is,\njoining parents and\ndropping arrowheads.\n\n2a2\n\u03b8\n\n2b2\n\u03b8\n\n\u03c3 2\n\u03b8\n\n23.8\n17.1\n30.3\n\nestimate\nposterior mean\nposterior sd\n\n\u03c3 2\n\n\u00b5\n\n\u03b81\n\n\u03b82\n\n\u03b83\n\n\u03b84\n\n\u03b85\n\n\u03b86\n\n126.4\n138.0\n33.8\n\n41.9\n41.9\n2.4\n\n53.9\n45.8\n4.1\n\n43.0\n42.3\n2.9\n\n34.9\n39.6\n3.4\n\n39.9\n41.2\n2.9\n\n41.3\n41.7\n2.9\n\n38.6\n40.8\n3.0\n\n\u00b52\n0\n\n\u03c4 2\n\u03b8\n\na2\n\u03b8\n\nb2\n\u03b8\n\n2\u00b52\n\u03b8\n\n2\u03c32\n\u03b8\n\n2\u03b82\n\u03b8\n\n2\u03c322\n\u03b8\n\n2y2\n\u03b8\n\n2\u00b52\n\u03b8\n\n2\u03c32\n\u03b8\n\n2\u03b82\n\u03b8\n\n2\u03c322\n\u03b8\n\n2y2\n\u03b8\n\nexpressions (11.44)\u2013(11.47) give the steps required for an iteration of the gibbs\nsampler. as the t updates in (11.47) are independent, they may all be performed at\nonce, if the programming language used permits simultaneous generation of several\nnon-identically-distributed normal variates.\n\nideas from section 6.2.2 render the structure of the full conditional densities more\nintelligible. figure 11.7 shows the directed acyclic graph and the corresponding con-\nditional independence graph for the present model. each of \u00b5, \u03c3 2\n\u03b8 , and \u03c3 2 has two\nhyperparameters, considered fixed, and \u00b5 and \u03c3 2\n\u03b8 are parents of \u03b81, . . . , \u03b8t . each\niteration of the gibbs sampler traverses the parameter nodes in the conditional inde-\npendence graph, simulating from the full conditional distribution corresponding to\neach node with remaining parameters set at their current values. the data y are held\nfixed throughout.\nwe applied this algorithm to the data in table 9.22 on the stickiness of blood. for\nillustration we took \u03b1 = \u03b1\u03b8 = 0.5, \u03b2 = \u03b2\u03b8 = 1, \u00b5 = 0, and \u03c4 2 = 1000, and generated\nstarting-values for the parameters from the uniform distribution on (0, 100). we ran\n25 independent chains with i = 1000.\nfigure 11.8 shows simulated series for three parameters and estimates of their\nposterior densities. the burn-in period seems to last for about b = 100 iterations,\nafter which the chains seem stable. the chain for \u03c3 2\n\u03b8 makes some large positive\nexcursions, but the others seem fairly homogeneous, though they both show fairly\nstrong autocorrelations. estimated variance inflation factors are about 10 for \u03c3 2\n\u03b8 and \u00b5,\nbut only 1\u20132.5 for the other parameters, consistent with the top left panels of the figure.\ntable 11.9 shows the posterior means and standard deviations for the parameters,\nwith their frequentist estimates. the posterior mean for \u00b5 is essentially equal to the\noverall average y, but the posterior densities of the \u03b8t are strongly shrunk towards\nit, because there is evidence that \u03c3 2\n\u03b8 is small; its posterior 0.1, 0.5, and 0.9 quantiles\n\n "}, {"Page_number": 624, "text": "11 \u00b7 bayesian models\n\n30 35 40 45 50 55 60\n\ntheta1\n\nfigure 11.8 gibbs\nsampler for normal\ncomponents of variance\nmodel and blood data. top\nleft: time plots of \u03b81, \u03c3 2\n\u03b8 ,\nand \u03c3 2. the other panels\nshow estimated posterior\ndensities for these\nparameters, based on\napplying analogues of\n(11.41) to the last 200\nestimates from each of 25\nparallel chains of length\n1000. frequentist\nestimates are shown as the\ndotted vertical lines.\n\n612\n\n1\na\n\nt\n\ne\nh\n\nt\n\n0\n8\n\n0\n6\n\n0\n4\n\n2\na\n\nt\n\ne\nh\n\nt\n.\n\na\nm\ng\ns\n\ni\n\n0\n0\n5\n\n0\n0\n2\n\n0\n\n2\na\nm\ng\ns\n\ni\n\n0\n0\n3\n\n0\n0\n1\n\n0\n\n200\n\n400\n\n600\n\n800\n\n1000\n\niteration\n\n0\n\n200\n\n400\n\n600\n\n800\n\n1000\n\niteration\n\n0\n\n200\n\n400\n\n600\n\n800\n\n1000\n\niteration\n\nf\nd\np\n\n5\n1\n\n.\n\n0\n\n0\n1\n\n.\n\n0\n\n5\n0\n0\n\n.\n\n0\n\n.\n\n0\n\nf\nd\np\n\nf\nd\np\n\n8\n0\n\n.\n\n0\n\n4\n0\n\n.\n\n0\n\n0\n\n.\n0\n\n2\n1\n0\n0\n\n.\n\n8\n0\n0\n\n.\n\n0\n\n4\n0\n0\n\n.\n\n0\n\n0\n\n.\n\n0\n\n0\n\n20\n\n40\n\n60\n\n80 100\n\n50 100 150 200 250 300\n\nsigma2_theta\n\nsigma2\n\nare 0.46, 7.1, and 42.1. the variability mostly comes from measurement error, not\n(cid:1)\ninter-subject variation.\n\nmetropolis\u2013hastings algorithm\nthe gibbs sampler is easy to program, but if the full conditional densities it involves\nare unavailable or too nasty then a more general algorithm may be needed. a powerful\napproach known as the metropolis\u2013hastings algorithm works as follows. in order to\nupdate the current value u of a markov chain, a new value u\nis generated using\n(cid:9) | u) > 0 if\na proposal density q(u\nand only if q(u | u\n) > 0 and the resulting chain has the properties desired. having\ngenerated u\n\n(cid:9) | u). any density q can be used provided q(u\n\nis accepted with probability\n\n, a move from u to u\n\n(cid:9)\n\n(cid:9)\n\n(cid:9)\n\n(cid:9)\n\n(cid:9)\n\n) = min\n\na(u, u\n\n1,\n\n(cid:13)\n\n(cid:14)\n\n,\n\n(cid:9)\n\n)q(u | u\n(cid:9)\n)\n\u03c0(u\n\u03c0(u)q(u(cid:9) | u)\n\nbut otherwise the chain remains at u. hence the probability density for a move to u\ngiven that the chain has current value u, is\n\n(cid:9) | u) = q(u\n\n(cid:9) | u)a(u, u\n\n(cid:9)\n\n) + r(u)\u03b4(u \u2212 u\n\n(cid:9)\n\n),\n\np(u\n\n(cid:9)\n\n,\n\n\u03b4 denotes the dirac delta\nfunction.\n\n "}, {"Page_number": 625, "text": "11.3 \u00b7 bayesian computation\n\nwhere\n\n(cid:4)\n\nr(u) = 1 \u2212\n\nq(v | u)a(u, v) dv.\n\n613\n\n(cid:9) | u) are the probability density for a move from u\nbeing proposed and accepted, and the probability that a move away from u is\n\n(cid:9)\n\nthe first and second terms of p(u\nto u\nrejected.\n\nthe metropolis\u2013hastings update step satisfies the detailed balance condition (6.5),\n\nbecause\n\n\u03c0(u) p(u\n\n(cid:9) | u) = \u03c0(u)q(u\n\n(cid:9) | u) min\n\n(cid:9)\n\n= \u03c0(u\n= \u03c0(u\n\n(cid:9)\n\n(cid:9)\n\n)q(u | u\n) p(u | u\n\n) min\n(cid:9)\n\n).\n\n(cid:13)\n(cid:13)\n\n1,\n\n(cid:9)\n\n)q(u | u\n(cid:9)\n\u03c0(u\n)\n\u03c0(u)q(u(cid:9) | u)\n(cid:9) | u)\n\u03c0(u)q(u\n\u03c0(u(cid:9))q(u | u(cid:9))\n\n, 1\n\n(cid:14)\n+ \u03c0(u)r(u)\u03b4(u \u2212 u\n(cid:14)\n\n(cid:9)\n\n)\n\n+ \u03c0(u\n\n(cid:9)\n\n(cid:9)\n\n)r(u\n\n)\u03b4(u\n\n(cid:9) \u2212 u)\n\n(cid:13)\n\n(cid:9)\n\n(cid:24)\n\nhence the corresponding markov chain is reversible with equilibrium distribution \u03c0,\nprovided it is irreducible and aperiodic. as \u03c0 appears only in a ratio \u03c0(u\n)/\u03c0(u) inthe\nacceptance probability a(u, u\n), the algorithm requires no knowledge of the constant\nthat normalizes \u03c0.\n) =\n(cid:9) = u + \u03b5, where \u03b5 is symmet-\nmin\nric with density g; then q(u\n). this is called\nrandom walk metropolis sampling. it is often applied to transformations of u, or to\nsubsets of its elements, using a different proposal distribution for each subset.\n\n),\nthe kernel\n(cid:9) | u) = g(u\n\nis called symmetric, and a(u, u\n\n(cid:9) | u) = q(u | u\n(cid:9)\n\n. this occurs in particular if u\n\n(cid:9) \u2212 u) = g(u \u2212 u\n\n) = q(u | u\n\nif q(u\n\n)/\u03c0(u)\n\n1, \u03c0(u\n\n(cid:25)\n\n(cid:9)\n\n(cid:9)\n\n(cid:9)\n\n(cid:9)\n\n(cid:9)\n\nthe gibbs sampler is a form of metropolis\u2013hastings algorithm, the proposal den-\n\n(cid:9)\n\n(cid:9)\n\n(cid:9)\n\n(cid:9)\ni\n\n(cid:9)\ni\n\n(cid:9)\ni\n\n(cid:9)\n\u2212i\n\nq(u\n\n)/\u03c0(u\n\n)/\u03c0(u\n\n= \u03c0(u\n\n= \u03c0(u\n\n= \u03c0(u\n\n| u\u2212i ),\n\n(cid:9) | u) =\n\n\u03c0(u)/\u03c0(ui | u\n\nbecause u\ncepted, because a(u, u\n\n(cid:9)\n\u2212i )\n\u03c0(u)/\u03c0(ui | u\u2212i )\n\n= u\u2212i ,\n(cid:9)\n\u2212i\nu\notherwise.\n| u\n\nit then follows that\n)q(u | u\n(cid:9)\n)\n\u03c0(u\n\u03c0(u)q(u(cid:9) | u)\n\n= u\u2212i . here the proposals always have u\n\nsity at the ith step of an iteration being\n\u03c0(u\n0,\n| u\u2212i )\n(cid:9)\n\u2212i )\n) = min[1, \u03c0(u\n)q(u | u\nalthough there are few theoretical restrictions on the choice of q, practical con-\n(cid:9) | u) is sochosen that the acceptance probability\nstraints intervene. for example, if q(u\na(u, u\n) isessentially zero, the chain will spend long periods without moving and its\noutput will be useless, and if the acceptance probability is close to one at each step\nbut the chain barely moves, the state space will be traversed too slowly. hence it is\nimportant to balance a reasonably high acceptance probability a(u, u\n) with a chain\nthat moves around its state space quickly enough. this can demand creativity and\npatience from the programmer.\n\n= u\u2212i and are always ac-\n(cid:9) | u)}] = 1.\n\n(cid:9)\n\u2212i\n)/{\u03c0(u)q(u\n\n(cid:9)\n\u2212i )\n\u03c0(u\u2212i )\n\n= 1,\n\n(cid:9)\n\n(cid:9)\n\n(cid:9)\n\n(cid:9)\n\n(cid:9)\n\nexample 11.23 (normal density)\nfor illustration we take the toy problem of using\nthe metropolis\u2013hastings algorithm to simulate from the standard normal density\n\n "}, {"Page_number": 626, "text": "11 \u00b7 bayesian models\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n10\n\n2.4\n\n4\n\n2\n\n0\n\n-2\n\n-4\n\n-6\n\n-8\n\n-10\n\nfigure 11.9 sample\npaths for\nmetropolis\u2013hastings\nalgorithm. the stationary\ndensity is standard normal\nand the proposal density\n(cid:9) | u) is n (u, \u03c3 2), with\nq(u\n\u03c3 = 0.1, 0.5, 2.4 and 10.\nthe initial value is\nu0 = \u221210 and the same\nseed is used for the\nrandom number generator\nin each case. note the\ndependence of the\nacceptance rate and\nconvergence to\nstationarity on \u03c3 . the\nhorizontal dashed lines\nshow the \u2018usual\u2019 range for\nu.\n\n614\n\n0.1\n\nu\n\n0.5\n\n4\n\n2\n\n0\n\n-2\n\n-4\n\n-6\n\n-8\n\n-10\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\niteration\n\n(cid:9)\n\n), the acceptance probability is a(u, u\n\n(cid:9) | u) = \u03c3 \u22121\u03c6{(u\n(cid:9)\n\n(cid:9) \u2212 u)/\u03c3}, depends on \u03c3 .\n\u03c6(u) = \u03c0(u). the proposal density, q(u\nwe take initial value u0 = \u221210 far from the centre of the stationary distribution. as\n(cid:9) | u) = q(u | u\n) = min{1, \u03c6(u\n)/\u03c6(u)}.\nq(u\nfigure 11.9 shows sample paths u0, . . . ,u 500 for four values of \u03c3 . when \u03c3 =\n0.1, only small steps occur but they are accepted with high probability because\n.= 1. although u changes at almost every step, it moves so little that\n(cid:9)\n\u03c6(u\nthe chain has not reached equilibrium after 500 iterations. when \u03c3 = 0.5 it takes 100\nor so iterations to reach convergence and the chain then appears to mix fairly fast.\nwhen \u03c3 = 2.4 convergence is almost immediate but as the acceptance probability is\nlower the chain tends to get stuck for slightly longer. when \u03c3 = 10 the acceptance\nprobability is low and although the chain jumps to its stationary range almost at once,\nit spends long periods without moving.\n\n)/\u03c6(u)\n\n(cid:9)\n\nfor comparison the experiment above was repeated 50 times, and the estimated\nmeans of \u03c0(u) were compared. the estimator was the average of the last half of\nu0, . . . ,u i , with i = 500 iterations; that is, (11.39) with m(u) = u and b = 250.\neach of the 50 replicates used the same seed and initial value u0 for each \u03c3 ; the\nvalues of u0 were generated from the t5 density. the estimated values of \u03c3 2\nm in (11.40)\nwere 170, 17.7, 6.2, and 8.0 for \u03c3 = 0.1, 0.5, 2.4, and 10; the larger values of \u03c3\n= 1 for\nare preferable, but there is a large efficiency loss relative to the value \u03c3 2\nm\nindependent sampling. this is because of the serial correlations of u b+1, . . . ,u i ,\nwhich were roughly 0.97, 0.89, 0.62, and 0.83 for \u03c3 = 0.1, 0.5, 2.4, and 10.\n\nexercise 11.3.11 sheds more light on this example.\n\n(cid:1)\n\n "}, {"Page_number": 627, "text": "11.3 \u00b7 bayesian computation\n\n615\n\ntable 11.10 motorette\ndata (nelson and hahn,\n1972). censored failure\ntimes are denoted by +.\n\n\u25e6\n\nx (\n\nf)\n\n150\n170\n190\n220\n\nfailure time (hours)\n\n8064+ 8064+ 8064+ 8064+ 8064+ 8064+ 8064+ 8064+ 8064+ 8064+\n5448+ 5448+ 5448+\n4860\n1764\n1680+ 1680+ 1680+ 1680+ 1680+\n408\n528+ 528+ 528+ 528+ 528+\n408\n\n3444\n1344\n504\n\n3542\n1344\n504\n\n3780\n1440\n504\n\n2772\n408\n408\n\n5196\n\nexample 11.24 (motorette data) table 11.10 contains failure times yi j from an\naccelerated life trial in which ten motorettes were tested at each of four temperatures,\n\u25e6\nwith the objective of predicting lifetime at 130\nf. we analyse these data using a\nweibull model with\n\npr(yi j \u2264 y; xi ) = 1 \u2212 exp{(y/\u03b8i )\u03b3} ,\n\n\u03b8i = exp (\u03b20 + \u03b21xi ) ,\n\n(11.48)\nj = 1, . . . ,10, where failure time is taken in units of hundreds of\n\nfor i = 1, . . . ,4,\nhours and xi is log(temperature/100).\n\n1\n\n(cid:19)\n\n\u03b22\n0\n\nhere we describe a simple bayesian analysis using the metropolis\u2013hastings algo-\nrithm. for illustration we take independent priors on the parameters, n (0, 100) on \u03b20\nand \u03b21 and exponential with mean 2 on \u03b3 . then the log posterior is\n\n/200 \u2212 \u03b3 /2\ndi j{log \u03b3 + \u03b3 log(yi j /\u03b8i )} \u2212( yi j /\u03b8i )\u03b3 ,\n\n(cid:12)m(\u03b20, \u03b21, \u03b3 ) \u2261 \u2212(cid:18)\n+ \u03b22\n+ 4(cid:7)\n10(cid:7)\ni=1\nj=1\nwhere di j = 0 for uncensored yi j .\nfor proposal distribution we update all three parameters simultaneously, by taking\n) = (\u03b20, \u03b21, log \u03b3 ) + c(s1 z1, s2 z2, s3 z3), where the sr are the standard\n(\u03b2(cid:9)\n, \u03b2(cid:9)\niid\u223c n (0, 1), and c can\nerrors of the corresponding maximum likelihood estimates, zr\nbe chosen to balance the acceptance probability and the size of the move. the ratio\nq(u | u\n(cid:24)\n(\u03b2(cid:9)\n\n(cid:9) | u) reduces to \u03b3 (cid:9)/\u03b3 , sothe acceptance probability equals\n, \u03b2(cid:9)\n, \u03b3 (cid:9)\n\n(cid:12)m(\u03b20, \u03b21, \u03b3 ) \u2212 (cid:12)m(\u03b2(cid:9)\n\n(cid:25) = min\n\n)/q(u\n, \u03b2(cid:9)\n, \u03b3 (cid:9)\n\n), (\u03b20, \u03b21, \u03b3 )\n\n, log \u03b3 (cid:9)\n\n\u03b3 (cid:9)/\u03b3\n\n1, exp\n\n(cid:24)\n\n(cid:25)\n\n(cid:23)\n\n(cid:26)\n\na\n\n)\n\n0\n\n1\n\n.\n\n(cid:9)\n\n0\n\n1\n\n0\n\n1\n\nthe chain is clearly irreducible and aperiodic, so the ergodic theorem applies.\nwe take initial values near the maximum likelihood estimates, and run the chain\nfor 5000 iterations with c = 0.5. the sample path for \u03b21 in the upper left panel of\nfigure 11.10 shows that despite its acceptance probability of about 0.3, the chain\nis not moving well over the parameter space. this is confirmed by the correlogram\nand partial correlogram for successive values of \u03b21, which suggest that the chain is\n.= 0.99. in this case the variance inflation factor\nis(cid:15)\u03c4 = 199, so 5000 successive observations from the chain are worth about 25 inde-\nessentially an ar(1) process with \u03c11\n\npendent observations. sample paths for the other parameters are similar, and varying c\ndoes not improve matters. one reason for this is that \u03b20 and \u03b21 have correlation about\n\u22120.97 a posteriori, and the proposal distribution does not respect this. it is better to\n\n "}, {"Page_number": 628, "text": "figure 11.10 bayesian\nanalysis of motorette data\nusing\nmetropolis\u2013hastings\nalgorithm. upper panels:\nsample paths for \u03b21 using\ntwo parametrizations, the\nright one more nearly\northogonal. lower left:\nkernel density estimates of\n\u03c0(\u03b21 | y) and of\n\u03c0(y+ | y), where y+ is\nfailure time predicted for\n130\n\nf.\n\n\u25e6\n\n616\n\n11 \u00b7 bayesian models\n\nreduce this correlation by replacing x by x \u2212 x, after which corr(\u03b20, \u03b21 | y)\n.= \u22120.4.\nthe sample path for \u03b21 from a run of the algorithm starting near the new maximum\nlikelihood estimates, with the new sr and with c = 2, is shown in the upper right panel\nof figure 11.10. this chain mixes much better, though its acceptance probability is\n.= 0.9,\nabout 0.2. the usual plots suggest that \u03b21 follows an ar(1) process with \u03c1\nhere(cid:15)\u03c4 has the more acceptable value 19, though 5000 iterations would remain too\nand likewise for the other parameters, whose chains show similar good behaviour.\n\nsmall in practice.\n\nthe lower panels of the figure show kernel density estimates of the posterior densi-\nties for \u03b21 and for a predicted failure time y+ for temperature 130\nf. once convergence\nhas been verified, it is easy to obtain values for y+, simply by simulating a weibull\nvariable from (11.48) using the current parameter values at each iteration. quantiles\nof the simulated distributions may be used to obtain posterior confidence intervals for\nthe corresponding quantities.\n\n\u25e6\n\nthe metropolis\u2013hastings update described above changes all three parameters on\neach iteration, or none of them. alternatively we may attempt to update one parameter,\nchosen at random. the resulting chain is also ergodic, but it does not improve on the\n(cid:1)\nsecond approach described above.\n\n "}, {"Page_number": 629, "text": "11.3 \u00b7 bayesian computation\n\n617\n\ntable 11.11 accuracy\nof stirling\u2019s formula and\nrelated approximations.\n\n\u03b1\n\n0.5\n\n1\n\n2\n\n3\n\n4\n\n5\n\ni\u03b1+1\n\n\u02dci\u03b1+1/i\u03b1+1\n(cid:9)\n/i\u03b1+1\n\u02dci\n\u03b1+1\n\n0.8862\n0.8578\n0.9905\n\n1\n0.9221\n0.9960\n\n2\n0.9595\n0.9987\n\n6\n0.9727\n0.9994\n\n24\n0.9794\n0.9996\n\n120\n\n0.9834\n0.9998\n\nmetropolis\u2013hastings updates using an appropriate proposal distribution can be used\nwhen the full conditional densities needed for particular steps of the gibbs sampler\nare not available. generalizations can be constructed to jump between spaces of\ndiffering dimensions, and these are valuable in applications where averaging over\nvarious spaces or choosing among them is important. more details are given in the\nbibliographic notes.\n\nexercises 11.3\n1 show that laplace approximation to the gamma function\n\n(cid:4) \u221e\n\ni\u03b1+1 = \u0001(\u03b1 + 1) =\n\n\u2212u du\n.= \u02dci\u03b1+1 = (2\u03c0)1/2\u03b1\u03b1+1/2e\n\nu\u03b1e\n\n0\n\n= (2\u03c0)1/2(\u03b1 + 1\n\ngives stirling\u2019s formula, \u0001(\u03b1 + 1)\nterm in (11.28) is (12\u03b1)\n(cid:9)\n\u02dci\n\u03b1+1\n\n\u2212\u03b1, and verify that the o(\u03b1\u22121)\n\u22121. show that this can be incorporated by modifying \u02dci\u03b1+1 to\n2 use the facts that if z is a standard normal variable, e(z 4) = 3 and e(z 6) = 15, to check\n(11.28). use properties of normal moments to explain why (11.28) is an expansion with\nterms in increasing powers of n\n\n\u2212\u03b1, and check some of the numbers in table 11.11.\n\n6 )1/2\u03b1\u03b1e\n\n\u22121/2.\n\n(cid:2)\n\n\u22121 rather than n\n3 let f (y; \u03b8) be aunimodal density with mode at\ng(u) = log f ( \u02dcy\u03b8 ; \u03b8) \u2212 log f (u; \u03b8),\n\napproximated by (11.31), with\n\ny\u2212\u221e f (u; \u03b8) du may be\n\n\u02dcy\u03b8 . show that\na(u) = (2\u03c0)1/2 f ( \u02dcy\u03b8 ; \u03b8),\n\n(cid:2)\n\nand verify that the approximation is exact for the n (\u03b8, \u03c3 2) density. investigate its accuracy\nnumerically for the gamma density with shape parameter \u03b8 > 1, and for the t\u03bd density.\n4 consider predicting the outcome of a future random variable z on the basis of a random\n\u2212u/\u03bb, u > 0, \u03bb >0. show that \u03c0(\u03bb) \u221d \u03bb\u22121 gives\n\nsample y1, . . . ,y n from density \u03bb\u22121e\nposterior predictive density\n\n(cid:2)\n\nf (z, y | \u03bb)\u03c0(\u03bb) d\u03bb\nf (y | \u03bb)\u03c0(\u03bb) d\u03bb\n\n= nsn/(s + z)n+1,\n\nf (z | y) =\nwhere s = y1 + \u00b7\u00b7\u00b7 + yn.\nshow that when laplace\u2019s method is applied to each integral in the predictive density the\nresult is proportional to the exact answer, and assess how close the approximation is to a\ndensity when n = 5.\n5 consider the integral\n\nz > 0,\n\n(cid:4)\n\nu1\n\nin =\n\nu2\n\n\u2212nh(u) du,\n\ne\n\nwhere h(u) is asmooth increasing function with minimum at u1, atwhich point its deriva-\ntives are h1 = h\n\n(u1) > 0, h2 = h\n\n(cid:9)(cid:9)\n\n(cid:9)\n\n(cid:24)\n\n(u1) and so forth. show that\n\u2212nh1(u2\u2212u1) + o(n\n\n1 \u2212 e\n\n\u2212nh(u1)\n\n(cid:25)\n\n\u22121)\n\n,\n\nin = 1\nnh1\n\ne\n\n "}, {"Page_number": 630, "text": "618\n\n11 \u00b7 bayesian models\n\nand deduce that\n\n(cid:4)\n\n(cid:4) \u221e\n\nu2\n\n\u2212nh(u) du/\n\ne\n\n\u2212nh(u) du\n\ne\n\n.= 1 \u2212 e\n\n\u2212nh1(u2\u2212u1).\n\nu1\n\nu1\n\n6 give anapproximate variance for the importance sampling estimator (11.38), and verify\n\na posterior density has form \u03c0(\u03b8 | y) \u221d \u03b8\u2212m\u22121, for \u03b8 > \u03b81 (exercise 11.2.2). find the\napproximate and exact posterior density and distribution functions of \u03b8, and compare\nthem numerically when m = 5, 10, 20 and \u03b81 = 1. discuss.\ninvestigate how the approximation will change if h1 = 0.\nthe formula for var((cid:15)\u00b5rat).\nsampling-importance resampling (sir) works as follows: instead of using (11.38) as an\nq of size q (cid:18) s is taken from \u03b81, . . . , \u03b8s\nestimator of \u00b5, anindependent sample \u03b8\u2217\nwith probabilities proportional to w(\u03b81), . . . ,w (\u03b8s). the estimator of \u00b5 is(cid:15)\u00b5\u2217 = q\n\u03b8\u2217\nq .\n(a) discuss sir critically when the initial sample is taken from the prior \u03c0(\u03b8); this is\nsometimes called the bayesian bootstrap. give an explicit discussion in the case of an\nexponential family model and conjugate prior.\n(b) show that e\n\n) = (cid:15)\u00b5rat, and find its variance. use the rao\u2013blackwell theorem to\n\n, . . . \u03b8\u2217\n\n((cid:15)\u00b5\u2217\n\n(cid:1)\n\n\u22121\n\n7\n\n\u2217\n\n1\n\nshow that the variance of(cid:15)\u00b5\u2217\n\nexceeds that of(cid:15)\u00b5rat.\n\nunder what circumstances would it be sensible to use sir anyway?\n(rubin, 1987; smith and gelfand, 1992; ross, 1996)\n\n8 show that the gibbs sampler with k > 2 components updated in order\n\n1, . . . ,k , 1, . . . ,k , 1, . . . ,k , . . .\n\n(cid:9)\n\n(cid:9)\n\n(cid:9)\n\n.\n\n(cid:13)\n\nmin\n\n)/q(u\n\nwhen random walk metropolis\n\nis not reversible. are samplers updated in order 1, . . . ,k , k \u2212 1, . . . ,1, 2, . . ., or in a\nrandom order reversible?\n9 show that the acceptance probability for a move from u to u\nsampling is applied to a transformation v = v(u) ofu is\n(cid:14)\n)|dv/du|\n\u03c0(u\n\u03c0(u)|dv(cid:9)/du(cid:9)|\n1,\n(cid:9) | u) given in example 11.24.\n\nhence verify the form of q(u | u\nfind the acceptance probability when a component of u takes values in (a, b), and a\nrandom walk is proposed for v = log{(u \u2212 a)/(b \u2212 u)}.\nand correlation parameter \u03c1 such that |\u03c1| < 1. show that\nn\u22121(cid:7)\nj=1\n(cid:1)\n\nand deduce that as n \u2192 \u221e for any fixed \u03c1, nvar(y ) \u2192 \u03c3 2/(1 \u2212 \u03c1)2.\nwhat happens when |\u03c1| =1?\ndiscuss estimation of var(y ) based on (n \u2212 1)\nin example 11.23, show that the probability of acceptance of a move starting from u > 0\nequals\n\n10 suppose that y1, . . . ,y n are taken from an ar(1) process with innovation variance \u03c3 2\n\n(y j \u2212 y )2 and an estimate(cid:15)\u03c1.\n\nn2(1 \u2212 \u03c12)\n\n(n \u2212 j)\u03c1 j\n\nvar(y ) =\n\nn + 2\n\n(cid:9)\n\n(cid:8)\n\n\u22121\n\n11\n\n\u03c3 2\n\n,\n\n+ (1 + \u03c3 2)\n\n\u22121/2 exp(a2/2){\u0001(a) + \u0001 (b)} \u2212 \u0001 (\u22122u/\u03c3 ) ,\n\n1\n2\n\nwhere\n\na = \u2212 \u03c3 u\u221a\n\n1 + \u03c3 2\n\n,\n\nb = \u2212(2 + \u03c3 2)u\n%\n\u03c3 2(1 + \u03c3 2)\n\n.\n\n "}, {"Page_number": 631, "text": "11.4 \u00b7 bayesian hierarchical models\n\n619\n\nshow that the expected move size may be written as\n\u03c3 2u\n\n\u03c3\n\n(cid:5)\n\n(cid:6)(cid:11)\n\nexp\n\na2\n2\n\n1 + \u03c3 2\n\n{\u03c6 (a) \u2212 \u03c6 (b)} \u2212\n(cid:5)\u22122u\n\n(cid:13)\n\n+\u03c3\n\n\u03c6\n\n(cid:6)\n(1 + \u03c3 2)3/2\n\u2212 \u03c6(0)\n\n\u03c3\n\n(cid:12)\n\n{\u0001 (a) + \u0001 (b)}\n(cid:14)\n\n.\n\nplot these functions over the range 0 \u2264 u \u2264 15 for \u03c3 = 0.1, 1, 2.4, 10, and also with\n0 \u2264 \u03c3 \u2264 10 for u = 0, 1, 2, 3, 10. what light do these plots cast on the behaviour of the\nchains in figure 11.9?\n\n11.4 bayesian hierarchical models\n\nhierarchical models are useful when data have layers of variation. the incidence of\na disease may vary from region to region of a country, for instance, while within\nregions there is variation due to differences in poverty, pollution, or other factors.\nif the regional and local incidence rates are regarded as random, we can imagine a\nhierarchy in which the numbers of diseased persons depend on random local rates,\nwhich themselves depend on random regional rates. such models were discussed\nbriefly from a frequentist viewpoint in section 9.4. here we outline the bayesian\napproach, using the notion of exchangeability.\n\nthe random variables u1, . . . , un are called finitely exchangeable if their density\n\nhas the property\n\n(cid:18)\n\n(cid:19)\n\nf (u1, . . . ,u n) = f\n\nu\u03be(1), . . . ,u \u03be(n)\n\nfor any permutation \u03be of the set {1, . . . ,n }. then the density is completely symmetric\nin its arguments and in probabilistic terms the u1, . . . , un are indistinguishable; this\ndoes not mean that they are independent. an infinite sequence u1, u2, . . . , is called\ninfinitely exchangeable if every finite subset of it is finitely exchangeable.\n\na key result in this context is de finetti\u2019s theorem, whose simplest form says that if\nu1, u2, . . ., is aninfinitely exchangeable sequence of binary variables, taking values\nu j = 0, 1, then for any n there is a distribution g such that\n\nf (u1, . . . ,u n) =\n\n\u03b8 u j (1 \u2212 \u03b8)1\u2212u j dg(\u03b8)\n\n(11.49)\n\n(cid:4)\n\nn(cid:3)\nj=1\n\n1\n\n0\n\nwhere\ng(\u03b8) = lim\n\nm\u2192\u221e m\n\n\u03b8 = lim\n\nm\u2192\u221e pr{m\n\n\u22121(u1 + \u00b7\u00b7\u00b7 + um) \u2264 \u03b8},\n\n\u22121(u1 + \u00b7\u00b7\u00b7 + um).\nthis is justified at the end of this section. it implies that any set of exchangeable\nbinary variables u1, . . . , un may be modelled as if they were independent bernoulli\nvariables, conditional on their success probability \u03b8, this having distribution g and\nbeing interpretable as the long-run proportion of successes. more general versions\nof (11.49) hold for real u j , for example. the upshot is that a judgement that certain\nquantities are exchangeable implies that they may be represented as a random sample\nconditional on a variable that itself has a distribution. this provides the basis of a\n\nbruno de finetti\n(1906\u20131985) was born in\ninnsbruck and studied in\nmilan and rome, where\nhe eventually became\nprofessor, after working in\ntrieste as an actuary and\nat the university of\npadova. his main\ncontribution to statistics\nwas to develop\npersonalistic probability,\nteaching that \u2018probability\ndoes not exist\u2019. (you may\nthink this should have\nbeen made clear on page 1\nof the book!) he argued\nthat probability\ndistributions express a\nperson\u2019s view of the\nworld, with no objective\nforce. his ideas have\nstrongly influenced\nbayesian thought.\n\n "}, {"Page_number": 632, "text": "620\n\n11 \u00b7 bayesian models\n\ncase in favour of bayesian inference, because it implies that the conditional density\npr(un+1 | u1, . . . , un) for a future variable un+1 given the outcomes of u1, . . . , un,\nmay be represented as a ratio of two integrals of form (11.49), and this is formally\nequivalent to bayesian prediction using a prior density on \u03b8.\n\nthe essence of hierarchical modelling is to treat not data but particular sets of\nparameters as exchangeable. for if our model contains parameters \u03b81, . . . , \u03b8n, and if\nwe believe a priori that these are to be treated completely symmetrically, then they\nare exchangeable and may be thought of as a random sample from a distribution that\nis itself unknown. in principle that distribution might be anything, but in practice a\ntractable one is often chosen.\n\nexample 11.25 (normal hierarchical model) a prototypical case is the normal\nmodel under which y1, . . . , yn satisfy\n\ny j | \u03b8 j\n\nind\u223c n (\u03b8 j , v j ),\n\n\u03b81, . . . , \u03b8n | \u00b5 iid\u223c n (\u00b5, \u03c3 2), \u00b5 \u223c n (\u00b50, \u03c4 2),\n\nwhere v1, . . . , vn, \u03c3 2, \u00b50 and \u03c4 2 are known; the last two are hyperparameters that\ncontrol the uncertainty injected at the top level of the hierarchy. the y j have different\nvariances, but their means \u03b8 j are supposed indistinguishable and hence are modelled as\nexchangeable, being normal with unknown mean \u00b5. asthe joint density of (\u00b5, \u03b8 t, yt)t\nis multivariate normal of dimension 2n + 1, with mean vector and covariance matrix\n\n\uf8eb\n\uf8ed \u03c4 2\n\u03c4 21n\n\u03c4 21n\n\n\u00b5012n+1,\n\n\u03c4 21t\n\u03c4 21t\n+ \u03c3 2 in\n+ \u03c3 2 in\nn\nn\n\u03c4 21n1t\n+ \u03c3 2 in v + \u03c4 21n1t\n+ \u03c3 2 in\nn\n\nn\n\n\u03c4 21n1t\nn\n\u03c4 21n1t\nn\n\n\uf8f6\n\uf8f8 ,\n\n(11.50)\n\nwhere v = diag(v1, . . . , vn), the posterior density of (\u00b5, \u03b8 t)t given y is also normal.\ne(\u00b5 | y) = \u00b50/\u03c4 2 +(cid:1)\nunenlightening matrix calculations give\ny j /(\u03c3 2 + v j )\n1/\u03c4 2 +(cid:1)\n1/(\u03c3 2 + v j )\n\n1/\u03c4 2 +(cid:1)\n\n1\n1/(\u03c3 2 + v j )\n\n, var(\u00b5 | y) =\n\n,\n\nand\n\ne(\u03b8 j | y) = e(\u00b5 | y) + \u03c3 2\n\u03c3 2 + v j\n\n{y j \u2212 e(\u00b5 | y)}.\n\nthe posterior mean of \u00b5 is a weighted average of its prior mean \u00b50 and of the y j ,\nweighted according to their precisions conditional on \u00b5. typically \u03c4 2 is very large,\nand then e(\u00b5 | y) isessentially a weighted average of the data. even when v j \u2192 0\nfor all j there is still posterior uncertainty about \u00b5, whose variance is \u03c3 2/n because\ny1, . . . , yn is then a random sample from n (\u00b5, \u03c3 2).\nthe posterior mean of \u03b8 j is a weighted average of y j and e(\u00b5 | y), showing\nshrinkage of y j towards e(\u00b5 | y) by anamount that depends on v j . asv j \u2192 0,\ne(\u03b8 j | y) \u2192 y j , while as v j \u2192 \u221e, e(\u03b8 j | y) \u2192 e(\u00b5 | y). this is a characteristic\nfeature of hierarchical models, in which there is a \u2018borrowing of strength\u2019 whereby\nall the data combine to estimate common parameters such as \u00b5, while estimates of\nindividual parameters such as the \u03b8 j are shrunk towards common values by amounts\n\n "}, {"Page_number": 633, "text": "11.4 \u00b7 bayesian hierarchical models\n\n621\n\nthat depend on the precision of the corresponding observations, here represented by\n(cid:1)\nthe v j .\n\nj = a, . . . , l ,\n\nind\u223c b(m j , \u03b8 j ),\n\n\u03b8a, . . . , \u03b8l | \u03b6 iid\u223c f (\u03b8 | \u03b6 ),\n\nexample 11.26 (cardiac surgery data) table 11.2 contains data on mortality of\nbabies undergoing cardiac surgery at 12 hospitals. although the numbers of operations\nand the death rates vary, we have no further knowledge of the hospitals and hence no\nbasis for treating them other than entirely symmetrically, suggesting the hierarchical\nmodel\n\u03b6 \u223c \u03c0(\u03b6 ).\nr j | \u03b8 j\nconditional on \u03b8 j , the number of deaths r j at hospital j is binomial with probability\n\u03b8 j and denominator m j , the number of operations, which plays the same role as v\u22121\nin example 11.25: when m j is large then a death rate is relatively precisely known.\nconditional on \u03b6 , the \u03b8 j are a random sample from a distribution f (\u03b8 | \u03b6 ), and \u03b6 itself\nhas a prior distribution that depends on fixed hyperparameters.\none simple formulation is to let \u03b2 j = log{\u03b8 j /(1 \u2212 \u03b8 j )} \u223c n (\u00b5, \u03c3 2), conditional\non \u03b6 = (\u00b5, \u03c3 2), thereby supposing that the log odds of death have a normal distribu-\ntion, and to take \u00b5 \u223c n (0, c2) and \u03c3 2 \u223c i g(a, b), where a, b, and c express proper\nbut vague prior information. for sake of illustration we let a = b = 10\n\u22123, so\u03c3 2 has\nprior mean one but variance 103, and c = 103, giving \u00b5 prior variance 106. the joint\ndensity then has form\n\nj\n\n(cid:13)\n\n(cid:14)\n\nm j\nr j\n\ner j \u03b2 j\n\n(1 + e\u03b2 j )m j\n\n1\n\n(2\u03c0 \u03c3 2)1/2 exp\n\n\u2212 1\n2\u03c3 2 (\u03b2 j \u2212 \u00b5)2\n\n\u00d7 \u03c0(\u00b5)\u03c0(\u03c3 2),\n\n(cid:5)\n\n(cid:3)\n\n(cid:6)\n\nj\n\nso the full conditional densities for \u00b5 and \u03c3 2 are normal and inverse gamma. apart\nfrom a constant, the full conditional density for \u03b2 j has logarithm\n\nr j \u03b2 j \u2212 m j log(1 + e\u03b2 j ) \u2212 (\u03b2 j \u2212 \u00b5)2\n\n,\n\n2\u03c3 2\n\nand as this is a sum of two functions concave in \u03b2 j , adaptive rejection sampling may\nbe used to simulate \u03b2 j given \u00b5, \u03c3 2, and the data; see example 3.22.\n\nthis model was fitted using the gibbs sampler with 5500 iterations, of which the\n\nfirst 500 were discarded. convergence appeared rapid.\n\nfigure 11.11 compares results for the hierarchical model with the effect of treating\neach hospital separately using uniform prior densities for the \u03b8 j . shrinkage due to the\nhierarchical fit is strong, particularly for the smaller hospitals; the posterior mean of\n\u03b8a, for example, has changed from about 2% to over 5%. likewise the posterior means\nof \u03b8h and \u03b8b have decreased considerably towards the overall mean. by contrast, the\nposterior mean of \u03b8d barely changes because of the large value of m d. posterior\ncredible intervals for the hierarchical model are only slightly shorter but they are\ncentred quite differently. the posterior mean rate is about 7.3%, with 0.95 credible\n(cid:1)\ninterval (5.3, 9.4)%.\n\nin some cases the hierarchical element is merely a component of a more complex\n\nmodel, as the following example illustrates.\n\n "}, {"Page_number": 634, "text": "622\n\n11 \u00b7 bayesian models\n\nh (31/215)\nb (18/148)\nk (29/256)\nj (8/97)\nc (8/119)\ni (14/207)\nf (13/196)\nl (24/360)\ng (9/148)\nd (46/810)\ne (8/211)\na (0/47)\n\no\n\u2022\n\no\n\n\u2022\n\u2022\n\n5\n\no\n\n0\n\no\n\n\u2022\n\no\n\no\n\n\u2022\n\u2022\n\no\n\n\u2022\n\no\n\n\u2022\no\n\u2022\no\n\u2022\no\n\u2022\no\n\u2022\n\n10\n\n15\n\n20\n\ndeath rate (%)\n\nfigure 11.11 posterior\nsummaries for mortality\nrates for cardiac surgery\ndata. posterior means and\n0.95 equitailed credible\nintervals for separate\nanalyses for each hospital\nare shown by hollow\ncircles and dotted lines,\nwhile blobs and solid lines\nshow the corresponding\nquantities for a\nhierarchical model. note\nthe shrinkage of the\nestimates for the\nhierarchical model\ntowards the overall\nposterior mean rate,\nshown as the solid vertical\nline; the hierarchical\nintervals are slightly\nshorter than those for the\nsimpler model.\n\nexample 11.27 (spring barley data) table 10.21 contains data on a field trial\nintended to compare the yields of 75 varieties of spring barley allocated randomly to\nplots in three long narrow blocks. the data were analysed in example 10.35 using a\ngeneralized additive model to accommodate the strong fertility trends over the blocks.\nin the absence of detailed knowledge about the varieties it seems natural to treat them\nas exchangeable, and we outline a bayesian hierarchical approach. we also show how\nthe fertility patterns may be modelled using a simple markov random field.\nlet y = (y1, . . . , yn)t denote the yields in the n = 225 plots and let \u03c8 j denote the\nunknown fertility of plot j. let x denote the n \u00d7 p design matrix that shows which\nof the p = 75 variety parameters \u03b2 = (\u03b21, . . . , \u03b2 p)t have been allocated to the plots.\nthen a normal linear model for the yields is\n\ny | \u03b2, \u03c8, \u03bby \u223c nn(\u03c8 + x\u03b2, in/\u03bby),\n\n(11.51)\nwhere \u03c8 is the n \u00d7 1 vector containing the fertilities and \u03bby is the unknown precision\nof the ys.\n\nwe take the prior density of \u03bby to be gamma with shape and scale parameters a\nand b, g(a, b), so that its prior mean and variance are a/b and a/b2, where a and\nb are specified. as there is no special treatment structure, we take for the \u03b2r the\nexchangeable prior \u03b2 \u223c n p(0, i p/\u03bb\u22121\n\u03b2 ), with \u03bb\u03b2 \u223c g(c, d) and c, d specified. for\nthe fertilities we take the normal markov chain of example 6.13, for which\n\n\u03c0(\u03c8 | \u03bb\u03c8 ) \u221d \u03bbn/2\n\n\u03c8 exp\n\n(\u03c8i \u2212 \u03c8 j )2\n\n,\n\n\u03bb\u03c8 > 0,\n\n(11.52)\n\n(cid:8)\n\n(cid:7)\ni\u223c j\n\n\u2212 1\n2\n\n\u03bb\u03c8\n\n(cid:9)\n\nthe summation being over pairs of neighbouring plots and \u03bb\u22121\n\u03c8 being the variance of\ndifferences between fertilities. each \u03c8 j occurs in n j terms, where n j = 1 or 2is the\n\n "}, {"Page_number": 635, "text": "11.4 \u00b7 bayesian hierarchical models\n\n623\n\nnumber of plots adjacent to plot j. the sum in (11.52) equals \u03c8 tw \u03c8, where w is\nthe n \u00d7 n tridiagonal matrix with elements\n\uf8f1\uf8f2\n\uf8f3 ni ,\n\u22121,\n0,\n\ni = j,\ni \u223c j,\notherwise.\n\nwi j =\n\nthus w is block diagonal, with three blocks like the matrix v in example 6.13\nwith \u03c4 = 0, corresponding to the three physical blocks of the experiment. we take\n\u03bb\u03c8 \u223c g(g, h), with g and h specified.\n\n(cid:13)\n\n\u03c0(\u03b2, \u03c8, \u03bb) \u221d \u03bbn/2\n\nwith these conjugate prior densities, the joint posterior density is\n\u2212 1\n\u03bby(y \u2212 \u03c8 \u2212 x\u03b2)t(y \u2212 \u03c8 \u2212 x\u03b2)\n(cid:5)\n2\n\u2212 1\n2\n\n\u03c8 exp\n\n(cid:6)\n\n(cid:5)\n\nexp\n\ny\n\n\u03bb\u03c8 \u03c8 tw \u03c8\n\n(cid:6)\n\n(cid:14)\n\n\u03bb\u03b2 \u03b2 t\u03b2\n\nexp\nexp(\u2212b\u03bby) \u00d7 \u03bbc\u22121\n\n\u03b2\n\n\u2212 1\n2\n\n\u00d7 \u03bb p/2\nexp(\u2212c\u03bb\u03b2) \u00d7 \u03bbg\u22121\n(cid:25)\n\n\u03c8\n\nwhere \u03bb = (\u03bby, \u03bb\u03b2 , \u03bb\u03c8 )t. the full conditional densities turn out to be\n\n\u03b2\n\n,\n\n\u03bby q\n\n\u03bby q\n\n\u03b2 x t(y \u2212 \u03c8), q\n\u22121\n\u22121\n(cid:25)\n\u03c8 (y \u2212 x\u03b2), q\n\u22121\n\u22121\n\n\u03b2 | \u03c8, \u03bb, y \u223c n\n\u03c8 | \u03b2, \u03bb, y \u223c n\n\u03bby | \u03c8, \u03b2, y \u223c g{a + n/2, b + (y \u2212 x\u03b2 \u2212 \u03c8)t(y \u2212 x\u03b2 \u2212 \u03c8)/2},\n\u03bb\u03b2 | \u03c8, \u03b2, y \u223c g(c + p/2, d + \u03b2 t\u03b2/2),\n\u03bb\u03c8 | \u03c8, \u03b2, y \u223c g(g + n/2, h + \u03c8 tw \u03c8/2),\n\n\u03c8\n\n,\n\ny\n\n\u03b2\n\n\u00d7\u03bb p/2\n\u00d7\u03bba\u22121\n(cid:24)\n(cid:24)\n\nexp(\u2212h\u03bb\u03c8 ),\n\n(11.53)\n\n(11.54)\n\n(11.55)\n\n(11.56)\n\n(11.57)\n\nwhere\n\nq\u03b2 = \u03bby x t x + \u03bb\u03b2 i p, q\u03c8 = \u03bby in + \u03bb\u03c8 w.\n\nthe elements of \u03bb are independent conditional on the remaining variables. the\nrelatively simple form of the densities in (11.53)\u2013(11.57) suggests using a time-\nreversible gibbs sampler, in which \u03b2, \u03c8, and \u03bb are updated in a random order at\neach iteration. the most direct approach to simulation in (11.53) and (11.54) is\nthrough cholesky decomposition of q\u03b2 and q\u03c8 : in(11.53), for example, we find\nthe lower triangular matrix l such that l l t = q\n\u03b2 , generate \u03b5 \u223c n p(0, i p), and let\n\u22121\n\u03b2 = \u03bby q\n\u03b2 x t(y \u2212 \u03c8) + l\u03b5. the block diagonal structure of w means that the \u03c8s\n\u22121\nfor different blocks can be updated separately, so the largest cholesky decomposition\nneeded is that of a 75 \u00d7 75 matrix. an alternative is to update individual \u03c8 j s in a\nrandom order, but although the computational burden is smaller, the algorithm then\nconverges more slowly than with direct use of (11.54).\n\nnote the strong resemblance of (11.53) and (11.54) to the steps of the backfitting\n\nalgorithm for the corresponding generalized additive model.\n\nthe missing response in block 3 is simply a further unknown whose value may be\nsimulated using the relevant marginal density of (11.51). this adds a fourth component\nto the simulation in random order of \u03b2, \u03c8, and \u03bb at each iteration; there are no other\nchanges to the algorithm.\n\n "}, {"Page_number": 636, "text": "624\n\n11 \u00b7 bayesian models\n\nfigure 11.12 posterior\nsummaries for fertility\ntrend \u03c8 for the three\nblocks of spring barley\ndata, shown from left to\nright. above: median\ntrend (heavy) and overall\n0.9 posterior credible\nbands. below: 20\nsimulated trends from\ngibbs sampler output.\n\nif the matrix x t x is diagonal, then the full conditional density for the rth variety\n\neffect has form\n\n\u03b2r | \u03c8, \u03bb, y \u223c n\n\n(cid:5)\n\n\u03bbymr zr\n\u03bb\u03b2 + \u03bbymr\n\n1\n\n,\n\n\u03bb\u03b2 + \u03bbymr\n\n(cid:6)\n\n,\n\nwhere zr is the current average of y j \u2212 \u03c8 j for the mr plots receiving variety r. thus the\n\u03b2r are shrunk towards zero by an amount that depends on the ratio \u03bb\u03b2 /\u03bby; with \u03bb\u03b2 = 0\nthe mean for \u03b2 in (11.53) is the least squares estimate computed by regressing y \u2212 \u03c8\non the columns of x. unlike in example 11.25, however, the normal distributions of\nthe \u03b2r are here averaged over the posterior densities of \u03c8, \u03bby and \u03bb\u03b2.\n\nthe algorithm described above was run with random initial values for 10,500\niterations. time series plots of the parameters and log likelihood suggested that it had\nconverged after 500 iterations, and inferences below are based on the final 10,000\n\niterations. the variance inflation factors(cid:15)\u03c4 were less than 4 for \u03c8 and \u03b2, about 44, 6\n\nand 30 for \u03bby, \u03bb\u03c4 and \u03bb\u03c8 , and about 6 for y187. thus estimation for \u03bby is least reliable,\nbeing based on a sample equivalent to about 220 independent observations. a longer\nrun of the algorithm would seem wise in practice. based on this run, the posterior\n0.9 credible intervals for \u03bby, \u03bb\u03c8 and \u03bb\u03b2 were (5.2, 12.4), (5.0, 11.5) and (2.7, 5.7)\nrespectively, and differences of two variety effects have posterior densities very close\nto normal with typical standard deviation of 0.35. the corresponding standard error for\nthe generalized additive model was 0.41, so use of a hierarchical model and injection\nof prior information has increased the precision of these comparisons.\n\nfigure 11.12 shows some simulated values of \u03c8 and pointwise 0.90 credible en-\nvelopes for the true \u03c8. these envelopes are constructed by joining the 0.05 quantiles\nof the fertilities simulated from the posterior density, for each location, and likewise\nwith the 0.95 quantiles. by contrast with the analysis in example 10.35, the effective\ndegrees of freedom for \u03c8, controlled by \u03bb\u03c8 , are here equal for each block, lead-\ning to apparent overfitting of the fertilities for block 2 compared to the generalized\nadditive model. a difference between the models is that the current model corresponds\n\n "}, {"Page_number": 637, "text": "11.4 \u00b7 bayesian hierarchical models\n\n625\n\ntable 11.12 posterior\nprobabilities that a variety\nis ranked among the best r\nvarieties, estimated from\n10,000 iterations of gibbs\nsampler.\n\nvariety\n\nr\n\n1\n2\n5\n10\n\n56\n\n35\n\n72\n\n31\n\n55\n\n47\n\n54\n\n18\n\n38\n\n40\n\n0.327\n0.518\n0.814\n0.959\n\n0.182\n0.357\n0.690\n0.908\n\n0.149\n0.299\n0.643\n0.887\n\n0.129\n0.270\n0.621\n0.871\n\n0.075\n0.174\n0.486\n0.795\n\n0.055\n0.136\n0.416\n0.743\n\n0.019\n0.050\n0.234\n0.560\n\n0.015\n0.042\n0.183\n0.497\n\n0.012\n0.035\n0.153\n0.429\n\n0.006\n0.020\n0.106\n0.344\n\nto first differences of \u03c8 being a normal random sample, while in the earlier model\nthe second differences are a normal random sample, giving a smoother fit.\n\nthe posterior probabilities that certain varieties rank among the r best are given in\ntable 11.12. the ordering is somewhat different from that in example 10.35, perhaps\ndue to the slightly different treatment of fertility effects. as mentioned previously, no\nsingle variety strongly outperforms the rest, and future field experiments would have\nto include several of those included in this trial. this type of information is difficult to\nobtain using frequentist procedures, but is readily found by manipulating the output\nof the simulation algorithm described above.\n\nthis analysis is relatively easily modified when elements of the model are changed.\nindeed the priors and other components chosen largely for convenience should be\nvaried in order to assess the sensitivity of the conclusions to them; see exercise 11.3.6.\nmetropolis\u2013hastings steps would then typically replace the gibbs updates in the\n(cid:1)\nalgorithm.\n\nas mentioned above, more complicated hierarchies involve several layers of nested\nvariation. such models are widely used in certain applications, but their assessment\nand comparison can be difficult. for instance, shrinkage makes it unclear just how\nmany parameters a hierarchical model has. hierarchical modelling is an active area\nof current research.\n\njustification of (11.49)\nto establish (11.49), suppose that r lies in 0, . . . ,n and that m > n. then exchange-\nability of u1, . . . , um implies that the conditional probability\n\npr(u1 + \u00b7\u00b7\u00b7 + un = r | u1 + \u00b7\u00b7\u00b7 + um = s)\n\nequals the probability of seeing r 1\u2019s in n draws without replacement from an urn\nfor s = r, . . . ,m \u2212 (n \u2212 r)\ncontaining s 1\u2019s and m \u2212 s 0\u2019s, which is\nand zero otherwise. hence\n(cid:6)\npr(u1 + \u00b7\u00b7\u00b7 + un = r) = m\u2212(n\u2212r)(cid:7)\n\npr(u1 + \u00b7\u00b7\u00b7 + um = s)\n\nm\u2212s\nn\u2212r\n\ns\nr\n\n(cid:19)(cid:18)\n\nm\nn\n\n(cid:19)\u22121(cid:18)\n(cid:18)\n(cid:6)(cid:5)\n(cid:6)\u22121(cid:5)\n\n(cid:19)\n\n(cid:5)\n(cid:6) m\u2212(n\u2212r)(cid:7)\n\nm\nn\n\n(cid:5)\n\ns=r\nn\nr\n\n=\n\ns\nr\n\nm \u2212 s\nn \u2212 r\ns(r)(m \u2212 s)(n\u2212r)\n\ns=r\n\nm(n)\n\npr(u1 + \u00b7\u00b7\u00b7 + um = s),\n\n "}, {"Page_number": 638, "text": "626\n\n11 \u00b7 bayesian models\nwhere s(r) = s(s \u2212 1)\u00b7\u00b7\u00b7(s \u2212 r + 1) and so forth. if gm(\u03b8) denotes the distribution\nputting mass pr(u1 + \u00b7\u00b7\u00b7 + um = s) ats /m, for s = 0, . . . ,m , then\n(m\u03b8)(r){m(1 \u2212 \u03b8)}(n\u2212r)\n\n(cid:6)(cid:4)\n\n(cid:5)\n\n1\n\npr(u1 + \u00b7\u00b7\u00b7 + un = r) =\n\ndgm(\u03b8).\n\nn\nr\n\n0\n\nm(n)\n\nas m \u2192 \u221e,\n\n(m\u03b8)(r){m(1 \u2212 \u03b8)}(n\u2212r)\n\nm(n)\n\n\u2192 \u03b8 r (1 \u2212 \u03b8)n\u2212r ,\n\nand in fact there is an infinite subsequence of values of m such that gm converges to\na limit g that is a distribution function. to establish (11.49) we simply note that\n\n(cid:19) = pr(u1 + \u00b7\u00b7\u00b7 + un = r)\n\nf\n\nu\u03be(1), . . . ,u \u03be(n)\n\nfor any permutation \u03be of {1, . . . ,n } such that u\u03be(1) + \u00b7\u00b7\u00b7 + u\u03be(n) = r, giving\n\n(cid:5)\n\n(cid:6)\n\nn\nr\n\n(cid:18)\n\n(cid:4)\n\n1\n\n\u03b8 r (1 \u2212 \u03b8)n\u2212r dg(\u03b8) =\n\n0\n\n0\n\n\u03b8 u j (1 \u2212 \u03b8)1\u2212u j dg(\u03b8)\n\n(cid:4)\n\n1\n\nn(cid:3)\nj=1\n\nf (u1, . . . ,u n) =\n\nas desired.\n\nexercises 11.4\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\ntwo balls are drawn successively without replacement from an urn containing three white\nand two red balls. are the outcomes of the first and second draws independent? are they\nexchangeable?\nunder what conditions are the bernoulli random variables y1 and y2 = 1 \u2212 y1 exchange-\nable? what about y1, . . . ,y n given that y1 + \u00b7\u00b7\u00b7 + yn = m?\nestablish (11.50), and use it and (3.21) to verify the given formulae for the posterior mean\nand variance for \u00b5.\ndescribe how a metropolis\u2013hastings update could be used to avoid adaptive rejection\nsampling from the full conditional density for \u03b2 in example 11.26. compare and contrast\nthe two approaches.\nin a variant on the hierarchical poisson model in example 11.19, let y1, . . . ,y n be in-\ndependent poisson variables with means \u03b81, . . . , \u03b8n, let \u03b81, . . . , \u03b8n be a random sample\n\u2212\u03b8\u03b2, \u03b8 > 0, and let the prior density of \u03b2 be uniform on the positive\nfrom the density \u03b2e\nhalf-line. find e(\u03b8 j | y, \u03b2), and show that if ny > 1 then the posterior distribution of\n\u03b3 = 1/(1 + \u03b2) isbeta with parameters ny \u2212 1 and n + 1. hence show that the posterior\nmean of \u03b8 j is (y j + 1)(ny \u2212 1)/(ny + n). under what condition is this greater than the\nestimate(cid:15)\u03b8 j = y j obtained under the classical model with no link among the \u03b8s? explain.\n\n(a) give the directed acyclic and conditional independence graphs for the model in\nexample 11.27, and verify (11.53)\u2013(11.57).\n(b) what changes to the algorithm are needed if (11.52) is replaced by\n\n\u03c0(\u03c8 | \u03bb\u03c8 ) \u221d \u03bbn/2\n\n\u03c8 exp\n\n(cid:8)\n\n(cid:7)\ni\u223c j\n\n\u2212 1\n2\n\n\u03bb\u03c8\n\n(cid:9)\n\n(cid:10)(cid:10)\u03c8i \u2212 \u03c8 j\n\n(cid:10)(cid:10)\n\n,\n\n\u03bb\u03c8 > 0?\n\nwhat changes are needed if (11.51) specifies that the y j have independent t\u03bd densities, for\nsome known \u03bd?\n(c) how would you allow different degrees of smoothing for the different blocks?\n(besag et al., 1995)\n\n "}, {"Page_number": 639, "text": "11.5 \u00b7 empirical bayes inference\n\n627\n\n11.5 empirical bayes inference\n11.5.1 basic ideas\nthe borrowing of strength achieved by hierarchical bayes models increases the preci-\nsion of parameter estimation at the cost of specifying prior distributions at two levels.\nthis can be bothersome in practice, because priors on hyperparameters are difficult to\nverify and it is natural to worry about their effect on subsequent inferences. sensitivity\nanalysis, comparing results from different priors, is valuable, but another possibility\nin some cases is to estimate the hyperparameters from the data. many bayesians\ndeprecate this empirical bayes approach as essentially frequentist; we shall skirt this\nissue and simply sketch the main ideas.\n\nconsider the model\ny1, . . . , yn | \u03b81, . . . , \u03b8n\n\nind\u223c f (y1 | \u03b81), . . . , f (yn | \u03b8n),\n\n\u03b81, . . . , \u03b8n\n\niid\u223c \u03c0(\u03b8 | \u03b3 ).\n\na fully bayesian specification would add a prior density \u03c0(\u03b3 ) for \u03b3 , with inference\nfor the \u03b8 j based on the marginal posterior densities \u03c0(\u03b8 j | y). if we do not add this\nfurther level of complexity, then the data have marginal density\n\nf (y1, . . . , yn | \u03b3 ) = n(cid:3)\n\n(cid:4)\n\nj=1\n\nf (y j | \u03b8 j )\u03c0(\u03b8 j | \u03b3 ) d\u03b8 j\n\nfrom which we might estimate \u03b3 . an obvious approach is to use the maximum\n\nlikelihood estimator (cid:15)\u03b3 found from this density, and then to base inferences on the\nposterior densities \u03c0(\u03b8 j | y,(cid:15)\u03b3 ), for example computing posterior moments\n\n(cid:2)\n\n(cid:18)\n\ne\n\n\u03b8 r\nj\n\n(cid:19) =\n| y,(cid:15)\u03b3\n\n(cid:2)\n\n\u03b8 r\n\nj f (y j | \u03b8 j )\u03c0(\u03b8 j | \u03b3 ) d\u03b8 j\nf (y j | \u03b8 j )\u03c0(\u03b8 j | \u03b3 ) d\u03b8 j\n\n(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)\n\n.\n\n\u03b3=(cid:15)\u03b3\n\nnumerical methods are generally needed to evaluate the integrals. full bayesian\nanalysis would integrate out \u03b3 with respect to its prior density, thereby accounting\n\nfor uncertainty about \u03b3 rather than simply setting it to(cid:15)\u03b3 .\n\nexample 11.28 (normal distribution) consider the model\n\ny1, . . . , yn | \u03b81, . . . , \u03b8n\n\nind\u223c n (\u03b8 j , v j ),\n\n\u03b81, . . . , \u03b8n\n\niid\u223c n (\u00b5, \u03c4 2),\n\nwhere the v j are known positive constants, and suppose initially that \u03c4 2 > 0 isalso\nknown. the conditional distribution of \u03b8 j given y is\nn (\u03be j \u00b5 + (1 \u2212 \u03be j )y j , (1 \u2212 \u03be j )v j ), with \u03be j = v j\n\n(11.58)\nand the y j are marginally independent with n (\u00b5, v j + \u03c4 2) densities. the maximum\nlikelihood estimate of \u00b5 is therefore\n\nj = 1, . . . ,n ,\n\nv j + \u03c4 2\n\n,\n\n(cid:15)\u00b5 = (cid:15)\u00b5(\u03c4 2) =\n\n(cid:1)\nj=1 y j /(v j + \u03c4 2)\n(cid:1)\nj=1 1/(v j + \u03c4 2)\n\nn\n\nn\n\n,\n\n "}, {"Page_number": 640, "text": "628\n\n11 \u00b7 bayesian models\nand the empirical bayes estimate of \u03b8 j is found by substituting this into e(\u03b8 j | y), to\ngive\n\n\u02dc\u03b8 j = \u03be j(cid:15)\u00b5 + (1 \u2212 \u03be j )y j = (cid:15)\u00b5 + (1 \u2212 \u03be j )(y j \u2212(cid:15)\u00b5).\n\n(11.59)\nwhen \u03be j = 0 then \u02dc\u03b8 j = y j is unbiased for \u03b8 j . taking \u03be j > 0 gives non-zero shrinkage\nand biased estimation of \u02dc\u03b8 j , but the hope is that the borrowing of strength induced by\nof shrinkage towards(cid:15)\u00b5 depends on v j /\u03c4 2. this is disquieting because the amount of\nshrinkage towards a common mean will reduce overall mean squared error. the degree\n\nshrinkage bears no relation to the data. thus if the y j were very different doubt would\nbe cast on the model, but the formulation pays no heed to this.\n\nwhen \u03c4 2 is unknown, its profile log likelihood is\n(cid:12)p(\u03c4 2) \u2261 \u2212 1\n2\n\nlog(v j + \u03c4 2) \u2212 1\n2\n\nn(cid:7)\nj=1\n\nn(cid:7)\nj=1\n\n{y j \u2212(cid:15)\u00b5(\u03c4 2)}2/(v j + \u03c4 2),\n\n\u03c4 2 \u2265 0,\n\nthe data give no evidence of variation in the \u03b8 j , all the y j have mean \u00b5, and all the\n\nfrom which the maximum likelihood estimate (cid:15)\u03c4 2 can be obtained. if (cid:15)\u03c4 2 = 0 then\n\u02dc\u03b8 j are shrunk to (cid:15)\u00b5. if (cid:15)\u03c4 2 > 0, then \u03be j is replaced by v j /(v j +(cid:15)\u03c4 2) in(11.59). as\n0 \u2264 v j /(v j +(cid:15)\u03c4 2) \u2264 1, \u02dc\u03b8 j lies between y j and(cid:15)\u00b5.\nvariability of(cid:15)\u00b5 and(cid:15)\u03c4 2 is unaccounted for. approaches to overcoming this have been\n(cid:1)\n\nconfidence intervals for the \u03b8 j may be computed by replacing \u00b5 and \u03c4 2 in (11.58)\nby estimates, but their coverage will be lower than the nominal level because the\n\nproposed, but we shall not treat them here.\n\nexample 11.29 (toxoplasmosis data) example 10.29 discusses estimation of lev-\nels of toxoplasmosis in 34 cities in el salvador. for a simple analysis of these data,\nwe let y j = log{(r j + 1/2)/(m j \u2212 r j + 1/2)} represent empirical logistic transforma-\ntions of the binomial responses giving the level of toxoplasmosis, with approximate\n\u22121 + (m j \u2212 r j + 1/2)\nvariances v j = (r j + 1/2)\n\u22121 treated as known. we generalize\nexample 11.28 to encompass regression by taking\n\nind\u223c n (\u03b8 j , v j ),\n\n\u03b8 j | \u03b2 ind\u223c n (x t\n\nj\n\n\u03b2, v(cid:9)\n\nj ),\n\nj = 1, . . . ,n ,\n\nj\n\nj\n\nj\n\n,\n\n(cid:25)\n\n\u03b2, v j (1 \u2212 \u03be j )\n\n\u03b2. then\n(1 \u2212 \u03be j )y j + \u03be j x t\n\u03b2, v j + v(cid:9)\n\nind\u223c n\nind\u223c n (x t\n(cid:15)\u03b2 of the \u03b8 j , with estimated variances v j (1 \u2212 \u03be j ).\n\nso that the \u03b8 j vary around means x t\nj\n\u03b8 j | y, \u03b2, v(cid:9)\n\u03be j = v j /(v j + v(cid:9)\nj ),\nj ), for j = 1, . . . ,n . maximum likelihood yields\nthe weighted least squares estimator (cid:15)\u03b2 = (x tw x)\nand marginally y j\n\u22121 x tw y, where w is the diag-\nonal matrix with elements w j = (v j + v(cid:9)\n\u22121, leading to shrinkage estimators \u02dc\u03b8 j =\nj )\n(1 \u2212 \u03be j )y j + \u03be j x t\nthe v(cid:9)\nj typically depend on unknown parameters that may be estimated from the\n= \u03c4 2. if x t\u03b2 equals a constant, then\nprofile likelihood. here we take v(cid:9)\n(cid:15)\u03c4 2 = 0.17, but it is better to let x t\u03b2 be a cubic function of rainfall, leading to(cid:15)\u03c4 2 = 0.1.\n(cid:15)\u03b2. the average variance reduces by a factor of almost ten,\n\nfigure 11.13 shows strong shrinkage of the individual estimates y j towards their\nregression counterparts x j\n\n= \u00b7\u00b7\u00b7 = v(cid:9)\n\n1\n\nn\n\nj\n\ny1, . . . , yn | \u03b81, . . . , \u03b8n\n(cid:24)\n\n "}, {"Page_number": 641, "text": "table 11.13\nshakespeare\u2019s word type\nfrequencies (efron and\nthisted, 1976; thisted\nand efron, 1987). entry r\nis nr , the number of word\ntypes used exactly r times.\nthere are 846 word types\nwhich appear more than\n100 times, for a total of\n31,534 word types.\n\nfigure 11.13 shrinkage\nof individual estimates\n(lower blobs) towards\nregession estimates (upper\nblobs) for toxoplasmosis\ndata.\n\n11.5 \u00b7 empirical bayes inference\n\n629\n\nr\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\ntotal\n\n0+ 14376\n10+\n305\n20+\n104\n30+\n73\n40+\n49\n50+\n25\n60+\n30\n70+\n13\n80+\n13\n90+\n4\n\n4343\n259\n105\n47\n41\n19\n19\n12\n12\n7\n\n2292\n242\n99\n56\n30\n28\n21\n10\n11\n6\n\n1463\n223\n112\n59\n35\n27\n18\n16\n8\n7\n\n1043\n187\n93\n53\n37\n31\n15\n18\n10\n10\n\n837\n181\n74\n45\n21\n19\n10\n11\n11\n10\n\n638\n179\n83\n34\n41\n19\n15\n8\n7\n15\n\n519\n130\n76\n49\n30\n22\n14\n15\n12\n7\n\n430\n127\n72\n45\n28\n23\n11\n12\n9\n7\n\n364\n128\n63\n52\n19\n14\n16\n7\n8\n5\n\n26305\n1961\n881\n513\n331\n227\n169\n122\n101\n78\n\n\u2022\n\n\u2022\n\u2022\n\u2022\n-1.0\n\n-1.5\n\n-0.5\n\n\u2022\n\u2022\n0.0\n\nestimate\n\n\u2022\u2022\n\u2022\n0.5\n\n\u2022\n\n\u2022\n\n1.0\n\n\u2022\n\n1.5\n\nfrom v = 0.68 to v(1 \u2212(cid:15)\u03be) = 0.07, and one would expect a large decrease in overall\n\nmean squared error.\n\nthe empirical bayes estimates of the toxoplasmosis levels themselves are obtained\nby inverse logistic transformation, with standard errors from the delta method. a more\n\ndetailed analysis, or simulation, would be needed to account for the uncertainty in(cid:15)\u03b2\nand(cid:15)\u03c4 2.\n\n(cid:1)\n\nthe previous examples illustrate parametric empirical bayes inference, in which\nthe prior for \u03b8 is taken from a parametrized family of distributions. in practice an\nalternative is to try and estimate the prior nonparametrically. the resulting estimators\nare generally unstable if the data are not extensive, and some form of smoothing may\nbe needed.\n\nexample 11.30 (shakespeare\u2019s vocabulary data) the canon of shakespeare\u2019s\naccepted works contains 884,647 words, with 31,534 distinct word types. a word\ntype is a distinguishable arrangement of letters, so \u2018king\u2019 is different from \u2018kings\u2019\nand \u2018alehouse\u2019 different from both \u2018ale\u2019 and \u2018house\u2019. table 11.13 shows how many\nword types occurred once, twice, and so on in the canon: 14,376 appear just once,\n4343 appear twice, and so forth. if nr is the number of word types appearing r times,\nthen\nif a new body of work containing 884,647t words was found, how many new word\ntypes might it contain? taking t = 1 corresponds to finding a new set of works the\nsame size as the canon, while setting t = \u221e enables us to estimate shakespeare\u2019s\ntotal vocabulary.\n\n(cid:1)\u221e\nr=1 nr = 31,534.\n\n "}, {"Page_number": 642, "text": "630\n\n11 \u00b7 bayesian models\n\nfinding a new word type in a body of work is analogous to finding a new species\nof animal among those caught in a trap. suppose that there are s species in total,\nand that after trapping over the period [\u22121, 0] we have ys members of species s. we\nof time, so ys is poisson with mean \u03bbs, and let nr = (cid:1)\nassume that they enter the trap according to a poisson process of rate \u03bbs per unit\ns i (ys = r) bethe number\nof species observed exactly r times in the trapping period [\u22121, 0]. let g(\u03bb) bethe\nunknown distribution function of \u03bb1, . . . , \u03bbs. then the expected number of species\nseen in (0, t] that were seen exactly r times in the previous interval [\u22121, 0] is\n\n0\n\n(cid:4) \u221e\n\u03bdr (t) = s\n(cid:4) \u221e\n= s\n\u221e(cid:7)\nk=1\n\n=\n\n0\n\n\u2212\u03bbt )dg(\u03bb)\n\ne\n\n(1 \u2212 e\n(cid:13)\n\n\u2212\u03bb \u03bbr\nr!\n\u03bbt \u2212 (\u03bbt)2\n\u2212\u03bb \u03bbr\n(cid:5)\nr!\n2!\nr + k\n(\u22121)k+1\nk\n\n(cid:6)\n\ne\n\nt k \u03b7r+k ,\n\n+ (\u03bbt)3\n3!\n\n\u2212 \u00b7\u00b7\u00b7\n\n(cid:14)\n\ndg(\u03bb)\n\n(11.60)\n\nwhere\n\n\u03b7r = e(nr ) = s\n\n(cid:4) \u221e\n\n0\n\n\u03bbr\nr!\n\n\u2212\u03bb dg(\u03bb),\n\ne\n\nr = 1, 2, . . . .\n\nthe convergence of (11.60) will depend on t, but if it does converge, then an unbiased\nnonparametric empirical bayes estimator \u02dc\u03bdr (t) isobtained by replacing the \u03b7r by\nestimates \u02dc\u03b7r = nr obtained from the marginal distribution across the species. if the\ns poisson processes are independent, then the nr will be approximately independent\npoisson variables with means \u03b7r . thus for example,\nvar{\u02dc\u03bd0(t)} = var(n1t \u2212 n2t 2 + n3t 3 \u2212 \u00b7\u00b7\u00b7)\n.=\n\nnr t 2r\n\n\u03b7r t 2r\n\n.=\n\n\u221e(cid:7)\nr=1\n\n\u221e(cid:7)\nr=1\n\nprovides a standard error for \u02dc\u03bd0(t).\nfor the data in table 11.13, \u02dc\u03bd0(1) = 11,430 with standard error 178. it turns out\nnot to be possible to give an upper bound for the size of shakepeare\u2019s vocabulary, but\na fairly realistic lower bound can be established of about 35,000 word types that he\nknew but which do not appear in the canon.\n\nparametric empirical bayes models employ parametric distributions for g, one\n\ncandidate being gamma with mean and variance \u03be /\u03b2 and \u03be /\u03b22. then\nr = 1, 2, . . . ,\n\n\u03b7r = \u03b71\n\n\u03b2\n\n(cid:6)r\u22121\n\n(cid:5)\n\n,\n\n\u0001(r + \u03be)\nr!\u0001(1 + \u03be)\n\n1 + \u03b2\n\nproportional to the negative binomial density truncated so that r > 0. in the negative\nbinomial case \u03be > 0, but here any value of \u03be > \u22121 ispossible; \u03be = 0 gives the\nlogarithmic series distribution, the first to be fitted to species abundance data. the\ntribution of n1, . . . ,n r0, for some suitable r0. taking r0 = 40 yields (cid:15)\u03b71 = 14,376,\nparameters can be estimated by maximum likelihood fitting of the multinomial dis-\n(cid:15)\u03be = \u22120.3954 and(cid:15)\u03b2 = 104.3. the fit to table 11.13 is then remarkably good, giving\n\n.= 11,483, very close to the nonparametric empirical bayes estimate.\n\n\u02dc\u03bd0(1)\n\n "}, {"Page_number": 643, "text": "11.5 \u00b7 empirical bayes inference\n\n631\n\nin 1985 a previously unknown nine-stanza poem was found in the bodleian library\nin oxford. it consists of 429 words with 258 word types, of which nine do not\nappear in the canon. the empirical counts can be compared with the values \u02dc\u03bdr (t) with\nt = 429/884,647; for example \u02dc\u03bd0(t) = 6.97 is in fair agreement with the observed\nnumber of nine new words. detailed work suggests that at least on the basis of the word\ncounts, the poem might be attributable to shakespeare. scholarly debate continues,\n(cid:1)\nhowever, as word usage in the new poem differs from that in the canon.\n\nshrinkage improves estimators in many models. before discussing an unexpected\n\nconsequence of this, we outline some key notions of decision theory.\n\n11.5.2 decision theory\nsometimes data are gathered in order to decide among decisions whose payoffs are\nknown explicitly. the decision chosen will depend on the data y, and the choice is\nmade according to a decision rule \u03b4(y), which takes a value in a decision space d.\nthus \u03b4 is a mapping from the sample space y to d.\n\nthe fact that some decisions have better consequences than others is quantified\nthrough a loss function l(d, \u03b8), which represents the loss due to making decision d\nwhen the true state of nature is \u03b8. abad decision incurs a big loss, a better decision\na smaller one.\n\nat the time a decision is taken its loss is unknown because of uncertainty about \u03b8.\nnevertheless, provided we have prior information on \u03b8, wecan calculate the posterior\nexpected loss,\n\n(cid:4)\n\ne{l(d, \u03b8) | y} =\n\nl(d, \u03b8)\u03c0(\u03b8 | y) d\u03b8 =\n\n(cid:2)\n\n(cid:2)\nl(d, \u03b8) f (y | \u03b8)\u03c0(\u03b8) d\u03b8\n\nf (y | \u03b8)\u03c0(\u03b8) d\u03b8\n\n.\n\nthis is a function of d and y. if we want to make a decision leading to as small a\nloss as possible, one strategy is to choose the decision d that minimizes the posterior\nexpected loss for the particular y that has been observed. thus \u03b4(y) = d, where\n(cid:9) \u2208 d. this is called the bayes rule for\ne{l(d\nloss function l with respect to prior \u03c0.\n\n(cid:9), \u03b8) | y} \u2265e {l(d, \u03b8) | y} for every d\n\nexample 11.31 (discrimination) suppose we must decide whether or not a patient\nwith measurements y has a disease that has prevalence \u03b3 in the population. let \u03b8 = 1\nindicate the event that he is diseased. then\n\npr(\u03b8 = 1) = \u03b3 ,\n\npr(\u03b8 = 0) = 1 \u2212 \u03b3 ,\n\nand y has densities f1(y) and f0(y) according to the unknown value of \u03b8, which\nrepresents the state of nature. the possible decisions are\n\nd0 = \u2018patient is not diseased\u2019,\n\nd1 = \u2018patient is diseased\u2019,\n\nand a decision rule \u03b4(y) is aprocedure that chooses one of these.\n\n "}, {"Page_number": 644, "text": "632\n\n11 \u00b7 bayesian models\nlet li j denote the loss made when \u03b8 = i and decision d j is made. we set l00 =\nl11 = 0, so there is no loss when a decision is correct, and assume that l10, l01 > 0.\nthe posterior expected losses associated with d0 and d1 are\n\ne{l(d0, \u03b8) | y} = l00(1 \u2212 \u03b3 ) f0(y) + l10\u03b3 f1(y)\n(1 \u2212 \u03b3 ) f0(y) + \u03b3 f1(y)\ne{l(d1, \u03b8) | y} = l01(1 \u2212 \u03b3 ) f0(y) + l11\u03b3 f1(y)\n(1 \u2212 \u03b3 ) f0(y) + \u03b3 f1(y)\n\nand\n\n=\n\nl10\u03b3 f1(y)\n\n(1 \u2212 \u03b3 ) f0(y) + \u03b3 f1(y)\n\n=\n\nl01(1 \u2212 \u03b3 ) f0(y)\n\n(1 \u2212 \u03b3 ) f0(y) + \u03b3 f1(y)\n\n.\n\nthe posterior expected loss is minimized by d0 if l10\u03b3 f1(y) < l01(1 \u2212 \u03b3 ) f0(y) and\notherwise by d1; weare indifferent if l10\u03b3 f1(y) = l01(1 \u2212 \u03b3 ) f0(y).\nthis bayes rule can be expressed in more familiar terms: choose d0 if\n\nf0(y)\nf1(y)\n\n>\n\nl10\u03b3\n\nl01(1 \u2212 \u03b3 )\n\n,\n\nand otherwise choose d1. this is reminiscent of the neyman\u2013pearson lemma, though\nhere the value determining the decision involves \u03b3 and the loss function rather than\n(cid:1)\na null distribution for y.\n\nthe set-up described thus far applies to decisions to be made once the data are\nknown. but actions must sometimes be taken before any data are available \u2014 for\nexample, an experimental design should be chosen to maximize the information in\nfuture data. it then seems wise to average the loss incurred over the future data. the\nexpected loss due to using decision rule \u03b4(y) when the true state of nature is \u03b8 is called\nthe risk function of \u03b4,\n\nr\u03b4(\u03b8) =\n\nl{\u03b4(y), \u03b8} f (y | \u03b8) dy.\n\nif we have prior density \u03c0(\u03b8) for \u03b8, the overall expected loss due to using \u03b4 is the\nbayes risk,\n\n(cid:4)\n\nr\u03b4(\u03b8)\u03c0(\u03b8) d\u03b8 =\n=\n\n(cid:4)\n(cid:4)\n\n\u03c0(\u03b8)\n\nf (y)\n\nl{\u03b4(y), \u03b8} f (y | \u03b8) dy d\u03b8\nl{\u03b4(y), \u03b8}\u03c0(\u03b8 | y) d\u03b8 dy.\n\nfor any given y this is minimized by the decision \u03b4(y) minimizing the inner integral,\nand this choice of \u03b4 is the bayes rule for the prior \u03c0(\u03b8). thus the bayes rule minimizes\nexpected loss for both post-data and pre-data decisions.\n\nif we view estimation as a decision problem, then a decision is a choice of the\nvalue \u02dc\u03b8 to be used to estimate \u03b8, and the loss depends on \u03b8 and \u02dc\u03b8. acommon choice\nis squared error loss, l( \u02dc\u03b8 , \u03b8) = ( \u02dc\u03b8 \u2212 \u03b8)2. the bayes rule then uses as estimator the\nposterior mean of \u03b8,\n\n(cid:4)\n\nm(y) =\n\n\u03b8 \u03c0(\u03b8 | y) d\u03b8.\n\n(cid:4)\n\n(cid:4)\n(cid:4)\n\n "}, {"Page_number": 645, "text": "11.5 \u00b7 empirical bayes inference\n\n633\n\nto see why, let \u02dc\u03b8(y) be any other estimator, and note that as\n\n{ \u02dc\u03b8(y) \u2212 \u03b8}2 = { \u02dc\u03b8(y) \u2212 m(y)}2 + 2{ \u02dc\u03b8(y) \u2212 m(y)}{m(y) \u2212 \u03b8} + {m(y) \u2212 \u03b8}2 ,\n(cid:4)\n\nthe posterior expected loss\n\n(cid:4)\n\n{ \u02dc\u03b8(y) \u2212 \u03b8}2\u03c0(\u03b8 | y) d\u03b8 = { \u02dc\u03b8(y) \u2212 m(y)}2 +\n\n{m(y) \u2212 \u03b8}2 \u03c0(\u03b8 | y) d\u03b8\n\n(11.61)\n\nis minimized by choosing \u02dc\u03b8(y) = m(y).\nadmissible decision rules\nwe saw above that if a prior density for \u03b8 is available, one should choose the decision\nthat minimizes the posterior expected loss with respect to that prior. but if no prior is\navailable then we must attempt to make a good decision whatever the value of \u03b8. we\nthrough their risk functions. if r\u03b4(cid:9)(\u03b8) \u2265 r\u03b4(\u03b8)\ncan compare two decision rules \u03b4 and \u03b4(cid:9)\nfor all \u03b8, with strict inequality for some \u03b8, then we say that \u03b4(cid:9)\nis inadmissible \u2014 it\nis beaten by another rule. if no such rule can be found, \u03b4(cid:9)\nis said to be admissible.\nprovided the decision formulation is accepted and considerations such as robustness\nmay be ignored, we should clearly restrict attention to admissible decision rules.\nthe bayes rule \u03b4b corresponding to a proper prior \u03c0(\u03b8) is always admissible. for if\nsuch that r\u03b4(cid:9)(\u03b8) \u2264 r\u03b4b (\u03b8), with strict inequality for some set of\nnot, there is a rule \u03b4(cid:9)\nvalues of \u03b8 to which \u03c0 attaches positive probability. the corresponding bayes risks\n(cid:4)\nsatisfy\n\n(cid:4)\n\n\u03c0(\u03b8)r\u03b4(cid:9)(\u03b8) d\u03b8 <\n\n\u03c0(\u03b8)r\u03b4b (\u03b8) d\u03b8,\n\ncontradicting the fact that \u03b4b minimizes the bayes risk with respect to \u03c0(\u03b8).\n\nin a particular setting there may be many admissible decision rules. we can choose\namong them by minimizing sup\u03b8 r\u03b4(\u03b8). this generally very conservative choice is\ncalled a minimax rule. anadmissible decision rule \u03b4 with constant risk is minimax.\nfor otherwise there exists a rule \u03b4(cid:9)\n\nsuch that for all \u03b8,\n\nr\u03b4(cid:9)(\u03b8) \u2264 sup\n\nr\u03b4(cid:9)(\u03b8) < sup\n\nr\u03b4(\u03b8).\n\nbut if \u03b4 has constant risk, then the right-hand side of this expression is constant, and\n\u03b4 must be inadmissible, which is a contradiction.\n\n\u03b8\n\n\u03b8\n\nexample 11.32 (normal distribution) suppose that y1, . . . ,y n is a random sample\nfrom the n (\u00b5, \u03c3 2) distribution with known \u03c3 2 and that we wish to choose an estimator\n\u02dc\u00b5 of \u00b5 among\n\n1.\n2.\n3.\n\n\u03b41(y ) = y , the sample average;\n\u03b42(y ) isthe median of y1, . . . ,y n; and\n\u03b43(y ) = (ny /\u03c3 2 + \u00b50/\u03c4 2)/(n/\u03c3 2 + 1/\u03c4 2), the posterior mean for \u00b5 under the\nprior n (\u00b50, \u03c4 2); see (11.11).\n\nwe take loss function ( \u02dc\u00b5 \u2212 \u00b5)2, so\u03b4 (y ) has risk r\u03b4(\u00b5) equal to its mean squared\nerror, e[{\u03b4(y ) \u2212 \u00b5}2], the expectation being over y for fixed \u00b5.\n\n "}, {"Page_number": 646, "text": "634\n\n11 \u00b7 bayesian models\n\nthe average \u03b41(y ) has mean and variance \u00b5 and \u03c3 2/n, while the median \u03b42(y ) has\n\napproximate mean and variance \u00b5 and \u03c0 \u03c3 2/(2n). their risks are\n\nr\u03b41(\u00b5) = \u03c3 2/n,\n\n.= \u03c0 \u03c3 2/(2n).\n\nr\u03b42(\u00b5)\n\nthe posterior mean \u03b43(y ) has bias and variance\n\nand so\n\nn/\u03c3 2\n\n\u2212 \u00b5,\n\nn\u00b5/\u03c3 2 + \u00b50/\u03c4 2\nn/\u03c3 2 + 1/\u03c4 2\nr\u03b43(\u00b5) = n/\u03c3 2 + (\u00b5 \u2212 \u00b50)2/\u03c4 2\n(n/\u03c3 2 + 1/\u03c4 2)2\n\n.\n\n(n/\u03c3 2 + 1/\u03c4 2)2\n\n,\n\nas r\u03b42(\u00b5) > r\u03b41(\u00b5) for all \u00b5, \u03b42 is inadmissible. it can be shown that \u03b41 is ad-\nmissible, and as it has constant risk it is minimax. the rule \u03b43 is bayes and hence\nadmissible. if \u03c4 2 is small, \u03b43 will be greatly preferable to \u03b41 for values of \u00b5 close to\nthe prior mean \u00b50. contrariwise if \u03c4 2 is large, corresponding to weak prior informa-\ntion, then r\u03b43(\u00b5) < r\u03b41(\u00b5) over awide range, but the improvement is small. when\n\u03c4 \u2192 \u221e, wesee that \u03b43 \u2192 \u03b41.\n(cid:1)\nshrinkage and squared error loss\nhaving set up machinery for the comparison of estimators using risk, we investigate\nthe gains due to shrinkage when using empirical bayes estimation.\n\nlet y1, . . . ,y n be independent normal variables with means \u03b81, . . . , \u03b8n and unit\nvariance. we consider estimation of \u03b81, . . . , \u03b8n by \u02dc\u03b8 1, . . . , \u02dc\u03b8 n using as risk function\nthe sum of squared errors\n\nr \u02dc\u03b8 (\u03b8) = e\n\n( \u02dc\u03b8 j \u2212 \u03b8 j )2\n\n,\n\n(11.62)\n\n(cid:9)\n\n(cid:8)\nn(cid:7)\nj=1\n\nthe expectation being over y with \u03b8 fixed. at first sight this formulation seems highly\nartificial, but in fact it is paradigmatic of many situations, one being the semiparametric\nmodels discussed in section 10.7. the maximum likelihood estimators arise when\n\u02dc\u03b8 j = y j and have risk r \u02dc\u03b8 (\u03b8) = n. are better estimators available?\none possibility stems from taking (11.59) when v1 = \u00b7\u00b7\u00b7 = vn. then(cid:15)\u00b5 = y does\n\nnot depend on \u03c4 2, whose maximum likelihood estimator is given by\n(y j \u2212 y )2.\n\n\u22121w \u2212 1, 0), w = n(cid:7)\n\n(cid:15)\u03c4 2+ = max(n\n\nj=1\n\nthe eventual conclusion is unchanged but the computations below simplify if we\n\nreplace(cid:15)\u03c4 2+ by w/b, where we choose b to minimize the risk. substitution into (11.59)\n\ngives the shrinkage estimators\n\n\u02dc\u03b8 j = y + (1 \u2212 b/w )(y j \u2212 y ),\n\nj = 1, . . . ,n .\n\n(11.63)\n\nthese are more appealing than (11.59), because the degree of shrinkage depends on\nthe data, being small if the y j are widely separated and w is large. \u2018overshrinkage\u2019\n\n "}, {"Page_number": 647, "text": "635\n\n11.5 \u00b7 empirical bayes inference\nas(cid:15)\u03c4+.\n\noccurs if b/w > 1, so in practice one would use a non-negative estimator such\n\ncharles stein (1920\u2013)\nstudied at chicago and\ncolumbia universities and\nsince 1953 has worked at\nstanford university. he\nhas made important\ncontributions to\nmathematical statistics.\nsee degroot (1986b).\n\nwe show below that the risk of (11.63) using squared error loss is\n\nr \u02dc\u03b8 (\u03b8) = n + b {b \u2212 2(n \u2212 3)} e(w\n\n\u22121).\n\n(11.64)\nthis has minimum value n \u2212 (n \u2212 3)2e(w\n\u22121) > 0\nthis risk is uniformly less than n when n > 3. that is, when means of four or more\nnormal variables are estimated simultaneously using (11.63) and squared error loss,\nthe maximum likelihood estimator is inadmissible: the paragon of point estimation\nshould not be used. this risk improvement is often called the stein effect after its\nchief discoverer.\n\n\u22121) when b = n \u2212 3, and as e(w\n\n\u22121) = (n \u2212 3)\n\ncentral chi-squared with non-centrality parameter \u03c1 = (cid:1)\n\nthis striking result rests on the cumulation of risk across observations; the chosen\nrisk function would not be sensible if interest focused on a single \u03b8 j . the extent to\nwhich shrinkage reduces the risk depends on the distribution of w , which is non-\n(\u03b8 j \u2212 \u03b8)2. if\u03c1 = 0, that is,\n\u22121 and r \u02dc\u03b8 (\u03b8) = 3 independent of n. inthis\nall the \u03b8 j are equal, then e(w\ncase shrinkage yields a dramatically improved estimator. if \u03c1 is large, then the means\n\u22121) issmall, so r \u02dc\u03b8 (\u03b8) isonly slightly less than\nof the y j are widely separated and e(w\nn: the gain from shrinkage is then small. when y in (11.63) and in w is replaced\nby a fixed prior value \u00b5, then essentially the same result applies, with the maximum\nlikelihood estimator then inadmissible when n > 2. the amount of shrinkage then\ndepends on the distance from \u03b8 to the prior mean \u00b5, and is large if this distance is\nsmall.\n\nsimilar results apply more generally, for example to regression and to multivariate\nsituations. the broad lesson is that frequentist estimation of related quantities may\nbe improved by using shrinkage procedures.\n\nn(cid:7)\nj=1\nn(cid:7)\nj=1\n\nderivation of (11.64)\nnote first that with \u02dc\u03b8 j given in (11.63),\n\n{y + (1 \u2212 b/w )(y j \u2212 y ) \u2212 \u03b8 j}2 = n(cid:7)\n\n( \u02dc\u03b8 j \u2212 \u03b8 j )2 equals\n\n(cid:1)\n\n{y j \u2212 \u03b8 j \u2212 b(y j \u2212 y )/w}2\n\nj=1\n\nand this equals\n\n(y j \u2212 \u03b8 j )2 \u2212 2bw\n\n\u22121\n\nn(cid:7)\nj=1\n\n(y j \u2212 \u03b8 j )(y j \u2212 y ) + b2w\n\n\u22121.\n\n(11.65)\n\nthe first term has expectation n and the last appears in (11.64), so we must deal with\nthe middle term.\n\n(cid:24)\n\n(cid:25)\n\n, where h j (y) is asufficiently well-behaved func-\nind\u223c n (\u03b8 j , 1), and that d\u03c6(z)/dz = \u2212z\u03c6(z),\n\n(y j \u2212 \u03b8 j )h j (y )\n\nconsider e\ntion. integration by parts, recalling that y j\nimplies that e{(y j \u2212 \u03b8 j )h j (y )} =e{\u2202 h j (y )/\u2202y j}. setting\n= y j \u2212 y\n(cid:1)\ni (yi \u2212 y )2\n\nh j (y ) = y j \u2212 y\n\nw\n\n "}, {"Page_number": 648, "text": "636\n\nyields\n\n\u2202h j (y )\n\n\u2202y j\n\n\u22121\n\n= 1 \u2212 n\n\nw\n\n\u2212 2\n\n(y j \u2212 y )2\n\nw 2\n\n,\n\n11 \u00b7 bayesian models\n\nand a little algebra establishes that the central term in (11.65) has expectation\n\u22122b(n \u2212 3)e(w\n\n\u22121). expression (11.64) follows directly.\n\nexercises 11.5\n1\n\n2\n\n3\n\n4\n\nj\n\n= \u03c4 2v j . show that an unbiased estimator of \u03c4 2 is then\nin example 11.29, suppose that v(cid:9)\nss/(n \u2212 p) \u2212 1, where ss is the residual sum of squares and p is the dimension of \u03b2,\nand explain why a better estimator is max{ss/(n \u2212 p) \u2212 1, 0}.\nfind also the profile log likelihood when v(cid:9)\nconsider estimating the success probability \u03b8 for a binomial variable r with denominator\nm, using a beta prior distribution with parameters a, b > 0.\n(a) show that the marginal probability pr(r = r | \u00b5, \u03bd) has beta-binomial form\n\n= \u03c4 2.\n\nj\n\n\u0001(r + \u03bd\u00b5)\u0001{m \u2212 r + \u03bd(1 \u2212 \u00b5)}\n\n,\n\n\u0001(\u03bd)\n\n\u0001(\u03bd\u00b5)\u0001{\u03bd(1 \u2212 \u00b5)}\n\n\u0001(m + \u03bd)\nwhere \u00b5 = a/(a + b) and \u03bd = a + b, and deduce that\nvar(r/m) = \u00b5(1 \u2212 \u00b5)\n\ne(r/m) = \u00b5,\n\n(cid:5)\n\n1 + m \u2212 1\n\u03bd + 1\n\n.\n\nm\n\nr = 0, . . . ,m ,\n(cid:6)\n\n(cid:5)\n\n(cid:6)\n\nm\nr\n\n(b) show that methods of moments estimators based on a random sample r1, . . . , rn all\nwith denominator m are\n\n(cid:15)\u00b5 = r, (cid:15)\u03bd = (cid:15)\u00b5(1 \u2212(cid:15)\u00b5) \u2212 s2\ns2 \u2212(cid:15)\u00b5(1 \u2212(cid:15)\u00b5)/m\n\n,\n\nwhere r and s2 are the sample average and variance of the r j .\n(c) find the mean and variance of the conditional distribution of \u03b8 given r, and show\nthat the mean can be written as a shrinking of r/m towards \u00b5. hence give the empirical\nbayes estimates of the \u03b8 j .\nconsider a logistic regression model for example 11.29. show that the marginal log\nlikelihood for \u03b2, \u03c4 2 may be written as\n\n(cid:4)\n\nlog\n\nn(cid:7)\nj=1\n\n(cid:6)\n\n(cid:5)\n\n\u03b8 \u2212 x t\n\nj\n\n\u03b2\n\n\u03c4\n\ner j \u03b8\n\n(1 + e\u03b8 )m j\n\n\u03c6\n\nd\u03b8 \u2212 log \u03c4.\n\nuse laplace approximation to remove the integrals, and outline how you would then\nestimate \u03b2 and \u03c4 2. give also a laplace approximation for the posterior mean of \u03b8 j given\nthe data, \u03b2 and \u03c4 .\nconsider the exponential family density f (y | \u03b8) = \u03b8 ye\nf0(y) is known. if \u03c0(\u03b8) is any prior on \u03b8, show that\n\n\u2212\u03ba(\u03b8) f0(y) for integer y, where\n\ne(\u03b8 | y) =\n\n(cid:2)\n(cid:2)\n\u03b8 y+1e\n\u2212\u03ba(\u03b8)\u03c0(\u03b8) d\u03b8\n\u03b8 ye\u2212\u03ba(\u03b8)\u03c0(\u03b8) d\u03b8\n\n= pr\u03c0 (y = y + 1) f0(y)\npr\u03c0 (y = y) f0(y + 1)\n\n,\n\nwhere pr\u03c0 (y = y) isthe marginal probability that y = y, averaged over \u03c0. given a sample\ny1, . . . , yn from the corresponding empirical bayes model, explain why e(\u03b8 j | y j ) may\nbe estimated by\n\n(cid:1)\ni=1 i (yi = y j + 1)\n(cid:1)\nf0(y j )\nf0(y j + 1)\ni=1 i (yi = y j )\n\nn\n\nn\n\n.\n\n "}, {"Page_number": 649, "text": "11.6 \u00b7 bibliographic notes\n\n637\n\n5\n\n6\n\n7\n\n8\n\n9\n\ndo you think this estimator will be numerically stable? check by simulating some data\nand trying it out.\nlet x1, . . . , xn be a poisson random sample with mean \u00b5. previous experience suggests\nprior density\n\n\u03c0(\u00b5) = 1\n\u0001(\u03bd)\n\n\u00b5\u03bd\u22121e\n\n\u2212\u00b5,\n\n0 < \u00b5 <\u221e, \u03bd > 0.\n\nif the loss function for an estimator \u02dc\u00b5 of \u00b5 is ( \u02dc\u00b5 \u2212 \u00b5)2, determine an estimator that\nminimizes the expected loss and compare its bias and variance with those of the maximum\nlikelihood estimator.\nthe proportion \u03b8 of defective items from a production process varies because of fluctua-\ntions in the the raw material. records show that the prior density for \u03b8 is proportional to\n\u03b8(1 \u2212 \u03b8)4. a hundred items are inspected from a large batch all made from a homogeneous\nbatch of raw material, and six are found to be defective.\nof estimating \u03b8 by(cid:15)\u03b8 is \u03b8 2((cid:15)\u03b8 \u2212 \u03b8)2. find also the value of(cid:15)\u03b8 which minimizes the expected\nfind the posterior density function for the proportion \u03b8 of defectives in the batch. the cost\ncost, and the value of the minimum expected cost.\nthe loss when the success probability \u03b8 in bernoulli trials is estimated by \u02dc\u03b8 is ( \u02dc\u03b8 \u2212\n\u03b8)2\u03b8\u22121(1 \u2212 \u03b8)\n\u22121. show that if the prior distribution for \u03b8 is uniform and m trials result in\nr successes then the corresponding bayes estimator for \u03b8 is r/m. hence show that r/m\nis also a minimax estimator for \u03b8.\na population consists of k classes \u03b81, . . . , \u03b8k and it is required to classify an individual on\nthe basis of an observation y having density fi (y | \u03b8i ) when the individual belongs to class\ni = 1, . . . ,k . the classes have prior probabilities \u03c01, . . . , \u03c0k and the loss in classifying\nan individual from class i into class j is li j .\n(a) find the posterior probability \u03c0i (y) = pr(class i | y) and the posterior risk of allocating\nthe individual to class i.\n(b) now consider the case of 0\u20131 loss, that is, li j = 0 if i = j and li j = 1 otherwise. show\nthat the risk is the probability of misclassification.\n(b) suppose that k = 3, that \u03c01 = \u03c02 = \u03c03 = 1/3 and that y is normally distributed with\nmean i and variance 1 in class i. find the bayes rule for classifying an observation. use\nit to classify the observation y = 2.2.\nlet y j\nthe estimator of \u03b81, . . . , \u03b8n given by\n\nind\u223c n (\u03b8 j , 1), j = 1, . . . ,n , let \u00b5t = (\u00b51, . . . , \u00b5n) be aconstant vector, and consider\n\n(cid:16)\n\n*(cid:7)\n\n(cid:17)\n\n\u02dc\u03b8 j = \u00b5 +\n\n1 \u2212 b\n\n(yi \u2212 \u00b5i )2\n\n(y j \u2212 \u00b5),\n\nj = 1, . . . ,n .\n\nshow that the risk under squared error loss, (11.62), reduces to (11.64) with n \u2212 3 replaced\nby n \u2212 2. discuss the consequences of this.\n\n11.6 bibliographic notes\n\nthe bayesian approach to statistics, then called the inverse probability approach,\nplayed a central role in the early and middle parts of the nineteenth century, and was\ncentral to laplace\u2019s work. it then fell into disrepute after strong attacks were made\non the principle of insufficient reason and remained there for many years. during\nthe 1920s and 1930s r. a. fisher strongly criticised the use of prior distributions\nto represent ignorance. the publication in 1939 of the first edition of the influential\njeffreys (1961) marked the start of a resurgence of interest in bayesian inference,\nwhich was consolidated by further important advocacy in the 1950s, particularly\n\n "}, {"Page_number": 650, "text": "638\n\n11 \u00b7 bayesian models\n\nafter difficulties with frequentist procedures emerged. interest has mounted especially\nstrongly since serious bayesian computation became routinely possible.\n\nintroductory books on the bayesian approach are o\u2019hagan (1988), lee (1997),\nand robert (2001), while the excellent carlin and louis (2000) and gelman et al.\n(1995) are more oriented towards applications; see also box and tiao (1973), and\nleonard and hsu (1999). more advanced accounts are berger (1985) and bernardo\nand smith (1994), while de finetti (1974, 1975) is de rigeur for the serious reader. the\nlikelihood principle and its relation to the bayesian approach is discussed at length\nby berger and wolpert (1988). bayesian model averaging is described by hoeting\net al. (1999), who give other references to the topic.\n\nthe role and derivation of prior information has been much debated. for some\nflavour of this, see lindley (2000) and its discussion. a valuable review of arguments\nfor non-subjective representations of prior ignorance is given by kass and wasserman\n(1996). the elicitation of priors is extensively discussed by kadane and wolfson\n(1998), o\u2019hagan (1998), and craig et al. (1998).\n\nlaplace approximation is a standard tool in asymptotics, with close links to sad-\ndlepoint approximation. a statistical account is given by barndorff-nielsen and cox\n(1989), which gives further references. it has been used sporadically in bayesian con-\ntexts at least since the 1960s. tierney and kadane (1986) and tierney et al. (1989)\nraised its profile for modern readers. the same idea can be applied to other distribu-\ntions; see for example leonard et al. (1994).\n\nmarkov chain monte carlo methods originated in statistical physics. the origi-\nnal algorithm of metropolis et al. (1953) was broadened to what is now called the\nmetropolis\u2013hastings algorithm by hastings (1970), a paper astonishingly overlooked\nfor two decades, though known to researchers in spatial statistics and image analysis\n(geman and geman, 1984; ripley, 1987, 1988). the last decade has made up for this\noversight, with rapid progress being made in the 1990s following gelfand and smith\n(1990)\u2019s adoption of the gibbs sampler for mainstream bayesian application. valu-\nable books on bayesian use of such procedures are gilks et al. (1996), gamerman\n(1997), and robert and casella (1999), while brooks (1998) and green (2001) give\nexcellent shorter accounts. example 11.27 is taken from besag et al. (1995), while fur-\nther interesting applications are contained in besag et al. (1991) and besag and green\n(1993). tanner (1996) describes a number of related algorithms, including variants on\nthe em algorithm and data augmentation. green (1995) and stephens (2000) describe\nprocedures that may be applied when the parameter space has varying dimension.\n\nspiegelhalter et al. (1996a) describe software for bayesian use of gibbs sampling\nalgorithms, with many examples in the accompanying manuals (spiegelhalter et al.,\n1996b,c). cowles and carlin (1996) and brooks and gelman (1998) review numerous\nconvergence diagnostics for markov chain monte carlo output.\n\ndecision theory is treated by lindley (1985), smith (1988), raiffa and schlaifer\n(1961), and ferguson (1967). hierarchical modelling is discussed in many of the\nabove references. carlin and louis (2000) give a modern account of empirical\nbayes methods, while the more theoretical maritz and lwin (1989) predates modern\ncomputational developments. the discovery of the inadmissibility of the maximum\n\n "}, {"Page_number": 651, "text": "11.7 \u00b7 problems\n\n639\n\nlikelihood estimator by stein (1956) and the effects of shrinkage spurred much work;\nsee morris (1983) for a review.\n\n11.7 problems\n\n1 show that the integration in (11.6) is avoided by rewriting it as\n\nf (z | y) = f (z | y, \u03b8)\u03c0(\u03b8 | y)\n\n\u03c0(\u03b8 | y, z)\n\n.\n\nnote that the terms on the right need be calculated only for a single \u03b8.\nuse this formula to give a general expression for the density of a future observation in an\nexponential family with a conjugate prior, and check your result using example 11.3.\n(besag, 1989)\n\n2 (a) consider a scale model with density f (y) = \u03c4 \u22121g(y/\u03c4 ), y > 0, depending on a positive\nparameter \u03c4 . show that this can be written as a location model in terms of log y and log \u03c4 ,\nand infer that the non-informative prior for \u03c4 is \u03c0(\u03c4 ) \u221d \u03c4 \u22121, for \u03c4 > 0.\n(b) verify that the expected information matrix for the location-scale model f (y; \u03b7, \u03c4 ) =\n\u03c4 \u22121g{(y \u2212 \u03b7)/\u03c4}, for real \u03b7 and positive \u03c4 , has the form given in example 11.10, and\nhence check the jeffreys prior for \u03b7 and \u03c4 given there.\nprior \u03c0(\u03b8 | \u03bb, m), any finite mixture of conjugate priors,\n\n3 show that if y1, . . . , yn is a random sample from an exponential family with conjugate\n\nk(cid:7)\nj=1\n\np j \u03c0(\u03b8, \u03bb j , m j ),\n\nj\n\n(cid:7)\n\np j = 1, p j \u2265 0,\n\nis also conjugate. check the details when y1, . . . , yn is a random sample from the bernoulli\ndistribution with probability \u03b8.\n\n4 inference for a probability \u03b8 proceeds either by observing a single bernoulli trial, x,\nwith probability \u03b8, or byobserving the outcome of a geometric random variable, y ,\nwith density \u03b8(1 \u2212 \u03b8)y\u22121, y = 1, 2, . . .,. show that the corresponding jeffreys priors are\n\u03b8\u22121/2(1 \u2212 \u03b8)\n\u22121/2, and deduce that although the likelihoods for x and\ny are equal, subsequent inferences may differ. does this make sense to you?\n\n\u22121/2 and \u03b8\u22121(1 \u2212 \u03b8)\n\n5 let y1, y2 be the observed value of a random variable from the bivariate density\n, \u2212\u221e < y1, y2, \u03b8 < \u221e.\n\nf (y1, y2; \u03b8) = \u03c0\u22123/2 exp{\u2212(y1 + y2 \u2212 2\u03b8)2/4}\n\n1 + (y1 \u2212 y2)2\n\nshow that the likelihood for \u03b8 is the same as for two independent observations from the\nn (\u03b8, 1) density, but that confidence intervals for \u03b8 based the average y are not the same\nunder both models, in contravention of the likelihood principle.\n\n6 show that acceptance of the likelihood principle implies acceptance of the sufficiency and\n\nconditionality principles.\n\n7 consider a likelihood l(\u03c8, \u03bb), and suppose that in order to respect the likelihood principle\n\nwe base inferences for \u03c8 on the integrated likelihood\n\n(cid:4)\n\nl(\u03c8, \u03bb) d\u03bb.\n\n(a) compare what happens when x and y have independent exponential distributions\nwith means (i) \u03bb\u22121 and (\u03bb\u03c8)\n(b) suppose that the parameters in (i) are given prior density \u03c0(\u03c8, \u03bb) and that we compute\nthe marginal posterior density for \u03c8. establish that if the corresponding prior density is\nused in the parametrization in (ii), the problems in (a) do not arise.\n\n\u22121, (ii) \u03bb and \u03bb/\u03c8. discuss.\n\n "}, {"Page_number": 652, "text": "you may like to check that\nfor b > 0, the function\ng(u) = au \u2212 beu is\nconcave with a maximum\nat a finite u if a > 0, but\nthat if a < 0, it is\nmonotonic decreasing.\n\n640\n\n11 \u00b7 bayesian models\n\n9\n\n8 obtain expressions for the mean, variance, and mode of the inverse gamma density (11.14),\nand express its quantiles in terms of those of the gamma density. use your results to\nsummarize the posterior density of \u03c3 2 in example 11.12. calculate also 95% hpd and\nequi-tailed credible sets for \u03c3 2.\n(a) let y be poisson with mean \u03b8 and gamma prior \u03bb\u03bd \u03b8 \u03bd\u22121 exp(\u2212\u03bb\u03b8)/ \u0001(\u03bd), for \u03b8 > 0.\nshow that if \u03bd = 1\n2 and y = 0, the posterior density for \u03b8 has mode zero, and that a hpd\ncredible set for \u03b8 has form (0, \u03b8u ).\n(b) show that a hpd credible set for \u03c6 = log \u03b8 has form(\u03c6l , \u03c6u ), with both endpoints\nfinite. how does this compare to the interval transformed from (a)? why does the difference\narise?\n(c) compare the intervals in (a) and (b) with the use of quantiles of \u03c0(\u03b8 | y) toconstruct\nan equi-tailed credible set for \u03b8, and with confidence intervals based on the likelihood\nratio statistic.\n10 use (11.15) to show that the joint conjugate density for the normal mean and variance has\n\u00b5 \u223c n (\u00b50, \u03c3 2/k) conditional on \u03c3 2, with \u03c3 2 having an inverse gamma density. give in-\nterpretations of the hyperparameters, and investigate under what conditions the conjugate\nprior approaches the improper prior in which \u03c0(\u00b5, \u03c3 2) \u221d \u03c3 \u22122.\nconsider instead replacing the prior variance \u03c3 2/k of \u00b5 by a known quantity \u03c4 2. is the\nresulting joint prior conjugate?\n\n11 two competing models for a random sample of count data y1, . . . , yn are that they are\nindependent poisson variables with mean \u03b8, orindependent geometric variables with\ndensity \u03b8(1 \u2212 \u03b8)y\u22121, for y = 0, 1, . . ., with 0 < \u03b8 <1; this density has mean \u03b8\u22121. give\nthe posterior odds and bayes factor for comparison of these models, using conjugate priors\nfor \u03b8 in both cases.\nwhat are your prior mean and variance for the numbers of seedlings per five foot square\nquadrat in a fir plantation? use them to deduce the corresponding parameters of the con-\njugate priors for the poisson and geometric models. calculate your prior odds and bayes\nfactor for comparison of the two models applied to the data in table 11.14. investigate\ntheir sensitivity to other choices of prior mean and variance.\n\n12 consider a random sample y1, . . . , yn from the n (\u00b5, \u03c3 2) distribution, with conjugate prior\nn (\u00b50, \u03c3 2/k) for \u00b5; here \u03c3 2 and the hyperparameters \u00b50 and k are known. show that the\nmarginal density of the data\n\nf (y) \u221d \u03c3 \u2212(n+1)(\u03c3 2n\n\n(cid:13)\n\n\u22121 + \u03c3 2k\n(cid:14)\n\n\u22121)1/2 exp\n\n\u2212 1\n2\n\n\u221d exp\n\n\u2212 1\n2\n\nd(y)\n\n,\n\n(cid:11)\n\n(cid:13)\n\n(cid:14)(cid:12)\n\n(n \u2212 1)s2\n\n\u03c3 2\n\n+ (y \u2212 \u00b50)2\n\u03c3 2/n + \u03c3 2/k\n\nn\n\nsay. hence show that if y+ is a set of data from this marginal density, pr{ f (y+) \u2264 f (y)} =\npr{\u03c7 2\n\u2265 d(y)}. evaluate this for the sample 77, 74, 75, 78, with \u00b50 = 70, \u03c3 2 = 1, and\nk0 = 1\n2 . what do you conclude about the model?\ndo the corresponding development when \u03c3 2 has an inverse gamma prior.\n(box, 1980)\n\n13 suppose that y1, . . . , yn is a random sample from the poisson distribution with mean \u03b8,\nand that the prior information for \u03b8 is gamma with scale and shape parameters \u03bb and \u03bd.\nshow that the marginal density of y is\n\nf (y) =\n\ns!(cid:20)\nn\nj=1 y j !\n\nn\n\n\u2212s \u00d7 \u0001(s + \u03bd)\n\n\u03bb\u03bdns\n\n(\u03bb + n)\u03bd+s\n\n,\n\n\u0001(\u03bd)s!\n\nwhere s = (cid:1)\nsuppose that the data in table 11.14 are treated as poisson variables, and that prior\ninformation suggests that \u03bb = 1 and \u03bd = 1\n2 . isthis compatible with the data? do the data\nseem poisson, regardless of the prior?\n\nj y j , and give an interpretation of it.\n\ny1, . . . , yn \u2265 0,\n\n "}, {"Page_number": 653, "text": "11.7 \u00b7 problems\n\n641\n\ntable 11.14 counts of\nof balsam-fir seedlings in\nfive feet square quadrats.\n\n0\n0\n1\n4\n3\n\n1\n2\n1\n1\n1\n\n2\n0\n1\n2\n4\n\n3\n2\n1\n5\n3\n\n4\n4\n4\n2\n1\n\n3\n2\n1\n0\n0\n\n4\n3\n5\n3\n0\n\n2\n3\n2\n2\n2\n\n2\n4\n2\n1\n7\n\n1\n2\n3\n1\n0\n\n14 in the usual normal linear regression model, y = x\u03b2 + \u03b5, suppose that \u03c3 2 is known and\n\nthat \u03b2 has prior density\n\u03c0(\u03b2) =\n\n1\n\n|\u0001|1/2(2\u03c0) p/2 exp{\u2212(\u03b2 \u2212 \u03b20)t\u0001\u22121(\u03b2 \u2212 \u03b20)/2},\n\n15 show that the (1 \u2212 2\u03b1) hpd credible interval for a continuous unimodal posterior density\n\nwhere \u0001 and \u03b20 are known. find the posterior density of \u03b2.\n\u03c0(\u03b8 | y) isthe shortest credible interval with level (1 \u2212 2\u03b1).\n16 an autoregressive process of order one with correlation parameter \u03c1 is stationary only\nif |\u03c1| < 1. discuss bayesian inference for such a process. how might you (a) impose\nstationarity through the prior, (b) compute the probability that the process underlying data\ny is non-stationary, (c) compare the models of stationarity and non-stationarity?\n\n17 study the derivation of bic for a random sample of size n. investigate the sizes of the\nneglected terms for nested normal linear models with known variance. suggest a better\nmodel comparison criterion that is almost equally simple.\n18 the lifetime in months, y, of anindividual with a certain disease is thought to be expo-\nnential with mean 1/(\u03b1 + \u03b2x), where \u03b1, \u03b2 > 0 are unknown parameters and x a known\ncovariate. data (x j , y j ) are observed for n independent individuals, some of the lifetimes\nbeing right-censored. the prior density for \u03b1 and \u03b2 is\n\u03c0(\u03b1, \u03b2) = ab exp(\u2212\u03b1a \u2212 \u03b2b),\n\n\u03b1, \u03b2 > 0,\n\nwhere a, b > 0 are specified. show that an approximate predictive density for the uncen-\nsored lifetime, z, of afuture individual with covariate t is\n\nwhere(cid:15)\u03b1 and(cid:15)\u03b2 satisfy the equations\n(cid:7)\nj\u2208u\n\n(cid:15)f (z|t, y1, . . . , yn) = ((cid:15)\u03b1 +(cid:15)\u03b2t) exp{\u2212((cid:15)\u03b1 +(cid:15)\u03b2t)z},\nb + n(cid:7)\n(cid:7)\nj\u2208u\n\na + n(cid:7)\n\n\u03b1 + \u03b2x j\n\nx j y j =\n\ny j =\n\nj=1\n\nj=1\n\nx j\n\n,\n\nand u denotes the set of uncensored individuals.\n\n19 suppose that (u1, u2) lies in a product space, of form u1 \u00d7 u2.\n\n(a) show that\n\nz > 0,\n\n1\n\n\u03b1 + \u03b2x j\n\n,\n\n\u03c0(u1) = \u03c0(u1 | u2)\n\u03c0(u2 | u1)\n(cid:13)(cid:4)\n\n\u03c0(u2),\n\nfor any u1 \u2208 u1, u2 \u2208 u2,\n\n(cid:9)\n1\n\n\u03c0(u2) =\n\n\u2208 u1,\nand deduce that for each u2 \u2208 u2 and an arbitrary u\n(cid:13)(cid:4)\n(cid:14)\u22121 = \u03c0(u2 | u\n(cid:9)\n1)\n| u2)\n(cid:9)\n1\n2 is a random sample from \u03c0(u2 | u\n(cid:9)\n(cid:9)\u22121\n1), show that\n(cid:19)\u22121\n\n\u03c0(u1 | u2)\n\u03c0(u2 | u1)\n(cid:8)\n\n(b) if u 1\n2\n\n, . . . , u s\n\ndu1\n\n\u03c0(u\n\n(cid:18)\n\n(cid:9)\n1\n\n\u03c0(u2 | u\n(cid:9)\n1)\n| u2)\n\u03c0(u\n\n\u22121\n\ns\n\n(cid:9)\n1\n\nu\n\n| u s\n\n2\n\n\u03c0\n\n(cid:15)\u03c0(u2) = \u03c0(u2 | u\n(cid:9)\n1)\n| u2)\n\n\u03c0(u\n\n(cid:9)\n1\n\ns(cid:7)\ns=1\n\np\u2212\u2192 \u03c0(u2) as s \u2192 \u221e.\n\n(cid:14)\u22121\n\ndu2\n\n.\n\n(c) verify that the code below applies this approach to the bivariate normal model in\nexample 11.21.\n\n "}, {"Page_number": 654, "text": "642\n\n20\n\n11 \u00b7 bayesian models\n\ns <-1000; rho <- 0.75; u1p <- -2\nz <-seq(from=-4,to=4,length=200)\nplot(z,dnorm(z),type=\"l\",ylim=c(0,1.5))\nfor (r in 1:20)\n{ u2.sim <- rnorm(s, rho*u1p, sqrt(1-rho^2))\n\n# u1p is u1prime\n\n# 20 replicates of the simulation\n\nif (r==1) rug(u2.sim)\nconst <- mean( 1/dnorm(u1p,rho*u2.sim,sqrt(1-rho^2)) )\ndz <- dnorm(z,rho*u1p,sqrt(1-rho^2))/dnorm(u1p,rho*z,sqrt(1-rho^2))\nlines(z, dz/const) }\n\n# rug with one of the u2 samples\n\ndoes this work well? why not? try with u\nwhat lesson does this example suggest for the use of this approach in general?\n(a) let (u1, u2) have ajoint density \u03c0, marginal densities \u03c01 and \u03c02, and conditional\ndensities \u03c01|2 and \u03c02|1. show that \u03c01 satisfies the integral equation\n\n(cid:4)\n\n(cid:4)\n\n= \u22122, \u22121, 0.\n\n(cid:9)\n1\n\n\u03c01(u) =\n\nh(u, v)\u03c01(v) dv, where\n\nh(u, v) =\n\n\u03c01|2(u | w)\u03c02|1(w | v) dw .\n| u (i)\n\n= v,\n\n1\n\n1\n\n2\n\n1\n\n1\n\n2\n\n| u (1)\n\n| u (i+1)\n\n= v, i = 1, . . . , i \u2212 1, are those of\n\n| u (i)\n\u03c1w + (1 \u2212 \u03c12)1/2\u03b52,\n\n(b) in example 11.21, establish that the conditional distributions of u (i+1)\nu (i+1)\n\u03c12v + (1 \u2212 \u03c14)1/2\u03b53,\n\n= w, and u (i+1)\n\u03c1v + (1 \u2212 \u03c12)1/2\u03b51,\niid\u223c n (0, 1). hence write down h(u, v) for this problem.\nwhere \u03b5 j\n(c) show by induction that the conditional distribution of u (i+1)\nas that of \u03c12i v + (1 \u2212 \u03c14i )1/2\u03b54, and hence show that (i) the markov chain u (1)\nis in equilibrium when u (0)\nequilibrium provided u (0)\ndensity with several widely separated modes. let u = (u1, u2)t and consider\nwhere u = (u1, u2)t, 0 < \u03b3 <1 and \u03b4 > 0; this is a mixture of two bivariate normal\ndensities whose separation depends on \u03b4 and whose relative sizes depend on \u03b3 .\n(a) when \u03b3 = 1/2, sketch contours of \u03c0 and the conditional density of u1 given u2 = u2\nfor u2 = \u22122\u03b4, \u03b4, 0, \u03b4,2\u03b4 . sketch also some sample paths for a gibbs sampling algorithm.\nwhat problem do you foresee if \u03b4 > 4, say?\n(b) show that the conditional density of u1 given u2 = u2 may be written\n\n, . . .\n2 has the standard normal density, and (ii) the chain will reach\n2 may not equal \u00b1\u221e.\n\n\u03c0(u) = \u03b3 \u03c6(u1 \u2212 \u03b4)\u03c6(u2 \u2212 \u03b4) + (1 \u2212 \u03b3 )\u03c6(u1 + \u03b4)\u03c6(u2 + \u03b4),\n\n= v is the same\n\n, u (2)\n1\n\n1\n\n1\n\n1\n\n21 the unmodified gibbs sampler can be a poor way to generate values from a posterior\n\n\u03b1(u2)\u03c6(u1 \u2212 \u03b4) + {1 \u2212 \u03b1(u2)}\u03c6(u1 + \u03b4), where \u03b1(u2) =\n\n\u03b3 e2\u03b4u2\n\n1 \u2212 \u03b3 + \u03b3 e2\u03b4u2\n\n,\n\nand write down a gibbs sampling algorithm for \u03c0.\n(c) if c > 0 islarge enough that \u0001(\u2212c) is negligible, show that the probability that the\nsampler stays in the same mode during r iterations of the sampler is bounded below by\n\nexp{\u22122r(\u03b3 \u22121 \u2212 1)e\n\u22122\u03b4c},\nand compute this for \u03b4 = 2, 3 and some suitable values of c. comment.\n(d) find the joint distribution of v = (v1, v2)t = 2\n\u22121/2(u1 + u2, u1 \u2212 u2)t and show\nthat if simulation is performed in terms of v , convergence is immediate. comment on the\nimplications for implementing the gibbs sampler.\n22 table 5.9 gives data from k clinical trials as 2 \u00d7 2 tables (rt j , m t j ; rc j , mc j ), where rt j\nis the number of deaths in the treatment group of m t j patients and similarly in the control\ngroup, for j = 1, . . . ,k . as amodel for such data, ignoring publication bias, assume that\nrc j and rt j are independent binomial variables with denominators mc j and m t j and\nprobabilities\n\nexp(\u00b5 j )\n1 + exp(\u00b5 j )\n\n,\n\nexp(\u00b5 j + \u03b4 j )\n1 + exp(\u00b5 j + \u03b4 j )\n\nj = 1, . . . ,k ,\n\n,\n\n "}, {"Page_number": 655, "text": "11.7 \u00b7 problems\n\n643\niid\u223c n (\u03b3 , \u03c4 2) represent the treatment effects. suitable prior densities are assumed\n\nwhere \u03b4 j\nfor \u00b51, . . . , \u00b5k, \u03b3 and \u03c4 2.\n(a) write down the directed acyclic graph for this model, derive its conditional indepen-\ndence graph, and hence give steps of a markov chain monte carlo algorithm to sample from\nthe posterior density of \u00b51, . . . , \u00b5k, \u03b3 and \u03c4 2. if any steps require metropolis\u2013hastings\nsampling, suggest how you would implement it and give the acceptance probabilities.\n(b) how does your sampler change if one of the rc s ismissing?\n(c) how should your sampler be modified to generate from the posterior predictive density\nof \u03b4+, the value of \u03b4 for a new trial?\n(d) how should your algorithm be modified if an hierarchical model is used for the \u00b5 j ?\n\n23 a poisson process with rate\n\n\u03bb(t) =\n\n(cid:16)\n\n\u03bb0, 0 < t \u2264 \u03c4 ,\n\u03c4 < t \u2264 t0,\n\u03bb1,\n\nwhere \u03c4 is known, is observed on the interval (0, t0]. let n0 and n1 denote the numbers\nof events seen before and after \u03c4 , and suppose that \u03bb0 and \u03bb1 are independent gamma\nvariables with parameters \u03bd and \u03b2, where \u03bd is specified and \u03b2 has a gamma prior density\nwith specified parameters a and b.\n(a) check that the joint density of n0, n1, \u03bb0, \u03bb1, and \u03b2 is\n\n(\u03bb0\u03c4 )n0\n\nn0!\n\ne\n\n\u2212\u03bb0\u03c4 {\u03bb1(t0 \u2212 \u03c4 )}n1\n\nn1!\n\n\u2212\u03bb1(t0\u2212\u03c4 )\n\ne\n\n\u03bb\u03bd\u22121\n\u03b2 \u03bd\n0\n\u0001(\u03bd)\n\n\u2212\u03bb0\u03b2 \u03bb\u03bd\u22121\n\u03b2 \u03bd\n1\n\u0001(\u03bd)\n\ne\n\n\u2212\u03bb1\u03b2 \u03b2a\u22121ba\n\u0001(a)\n\ne\n\n\u2212b\u03b2 .\n\ne\n\nshow that \u03bb0, \u03bb1, and \u03b2 have gamma full conditional densities, and hence give a reversible\ngibbs sampler algorithm for simulating from their joint posterior density. extend this to\na process with known multiple change points \u03c41, . . . , \u03c4k, for which\n\n\uf8f1\uf8f4\uf8f2\n\uf8f4\uf8f3\n\n\u03bb0,\n\u03bb1,\n\u00b7\u00b7\u00b7\n\u03bbk ,\n\n0 < t \u2264 \u03c41,\n\u03c41 < t \u2264 \u03c42,\n\u00b7\u00b7\u00b7\n\u03c4k < t \u2264 t0.\n\n\u03bb(t) =\n\n(b) now suppose that \u03bd is unknown, with prior gamma density with specified parameters\nc and d. show that a random walk metropolis\u2013hastings move to update log \u03bd to log \u03bd(cid:9)\nhas\nacceptance probability\n\n,\n\n(cid:13)\n\nmin\n\n1,\n\n\u0001(\u03bd)\n\u0001(\u03bd(cid:9))\n\n(cid:14)k+1(cid:5)\n\n(cid:6)c(cid:27)\n\n\u03bd(cid:9)\n\u03bd\n\n\u2212d \u03b2k+1\n\ne\n\n(cid:3)\n\n-\n\n(cid:28)\u03bd(cid:9)\u2212\u03bd\n\n\u03bb j\n\n.\n\nhow would you add this to the algorithm in (a) to retain reversibility?\n(c) now suppose that although k is known, \u03c41, . . . , \u03c4k are not. show that the joint density\nof the even order statistics from a random sample of size 2k + 1 from the uniform density\non (0, t0) isproportional to\n\n\u03c41(\u03c42 \u2212 \u03c41)\u00b7\u00b7\u00b7(\u03c4 k \u2212 \u03c4k\u22121)(t0 \u2212 \u03c4l),\n\n0 < \u03c41 < \u00b7\u00b7\u00b7 < \u03c4k < t0.\n\nsuppose that this is taken as the prior for the positions of the k changepoints, and that\nthese are updated singly with proposals in which \u03c4 (cid:9)\ni is drawn uniformly from (\u03c4i\u22121, \u03c4i+1),\nwith obvious changes for \u03c41 and \u03c4k. find the acceptance probabilities for these moves.\n\n24 in a bayesian formulation of problem 6.16, we suppose that the computer program is one\nof many to be debugged, and that the mean number of bugs per program has a poisson\ndistribution with mean \u00b5/\u03b2, where \u00b5, \u03b2 > 0. the actual number of bugs in a particular\nprogram is m, and each gives rise to a failure after an exponential time with mean \u03b2\u22121,\nindependent of the others. on failure, the corresponding bug is found and removed at\nonce.\n(a) debugging takes place over the interval [0, t0] and failures are seen to occur at times\n0 < t1 < \u00b7\u00b7\u00b7 < tn < t0. show that\nf (y | m, \u03b2) =\n\n\u03b2n exp{\u2212\u03b2t0(m + s/t0 \u2212 n)} ,\n\n\u03b2 > 0, m = n, n + 1, . . . ,\n\nm!\n\n(m \u2212 n)!\n\n "}, {"Page_number": 656, "text": "11 \u00b7 bayesian models\n\n644\n\nwhere y represents the failure times and s = (cid:1)\n(b) we take prior \u03c0(\u00b5, \u03b2) \u221d \u00b5\u22122, \u00b5 >0. show that\n\nn\nj=1 t j .\n\n(cid:4) \u221e\n\n(cid:4) \u221e\n\n\u03c0(y, m) =\n\nf (y | m, \u03b2) f (m | \u03b2, \u00b5)\u03c0(\u03b2, \u00b5) d\u03b2d\u00b5\n\n0\n\n0\n\n\u221d (m \u2212 n + s/t0)\n\n\u2212n\n\nn\u22122(cid:3)\ni=1\n\n(m \u2212 n + i), m = n, n + 1, . . . ,\n\n\u03b4 is the dirac delta\nfunction.\n\nand give expressions for the posterior probabilities (i) that the program has been entirely\ndebugged and (ii) that there are no failures in [t0, t0 + u].\n(c) use the data in table 6.13 to give a 95% hpd credible interval for the number of bugs\nremaining after 31 failures. compute the probability that the program had been entirely\ndebugged (i) after 31 failures and (ii) after 34 failures. should the program have been\nreleased when it was?\n(d) discuss how the appropriateness of the model might be checked.\n(example 2.28; raftery, 1988)\n\n25 let y1, . . . ,y n be independent normal variables with means \u00b51, . . . , \u00b5n and common\n\nvariance \u03c3 2. show that if the prior density for \u00b5 j is\n\u03c0(\u00b5 j ) = \u03b3 \u03c4 \u22121\u03c6(\u00b5 j /\u03c4 ) + (1 \u2212 \u03b3 )\u03b4(\u00b5 j ),\n\n\u03c4 > 0, 0 < \u03b3 <1,\n\nwith all the \u00b5 j independent a priori, then \u03c0(\u00b5 j | y j ) isalso a mixture of a point mass and\na normal density, and give an interpretation of its parameters.\n(a) find the posterior mean and median of \u00b5 j when \u03c3 is known, and sketch how they vary\nas functions of y j . which would you prefer if the signal is sparse, that is, many of the \u00b5 j\nare known a priori to equal zero but it is not known which?\n(b) how would you find empirical bayes estimates of \u03c4 , \u03b3 , and \u03c3 ?\n(c) in applications of the tails of the normal density might be too light to represent the\ndistribution of non-zero \u00b5 j well. how could you modify \u03c0 to allow for this?\n\n26 suppose that y1, . . . , yn are independent poisson variables with means \u03bb j x j , where the x j\nare known constants, and that the \u03bb j are a random sample from the gamma density with\nmean \u03be /\u03bd.\n(a) show that the marginal density of y j is\nx y j\nj\n\n\u03bd \u03be\n\nf (y j ; \u03be, \u03bd) = \u0001(y j + \u03be)\n\ny j = 0, 1, . . . ,\n\n\u03be, \u03bd > 0,\n\n,\n\n\u0001(\u03be)y j !\n\n(x j + \u03be)y j+\u03be\n\nand give its mean. say how you would estimate \u03be and \u03bd based on y1, . . . , yn.\n(b) establish that\n\ne(\u03bb j | y, \u03be, \u03bd) = y j + \u03be\nx j + \u03bd\n\n,\n\nvar(\u03bb j | y, \u03be, \u03bd) = y j + \u03be\n(x j + \u03bd)2\n\n,\n\nand give an interpretation of this.\n\n(c) check that the code below computes the maximum likelihood estimates(cid:15)\u03be and(cid:15)\u03bd, and\n\nyields the empirical bayes estimates in table 11.7. discuss.\n\nx <-c(94.32,15.72,62.88,125.76,5.24,31.44,1.048,1.048,2.096,10.48)\ny <-c(5,1,5,14,3,19,1,1,4,22)\nl <-function(p, y, x)\n\n-sum(dnbinom(y, size=p[1], prob=p[2]/(x+p[2]), log=t))\n\nfit <- nlm(l, p=c(1,1), y=y, x=x) # marginal maximum likelihood\nxi <- fit$estimate[1]\nnu <- fit$estimate[2]\nests <- (y+xi)/(x+nu)\nvars <- (y+xi)/(x+nu)^2\ncbind(ests,vars)\n\n "}, {"Page_number": 657, "text": "12\n\nconditional and marginal inference\n\nin most models the parameter vector can be split into two parts, \u03b8 = (\u03c8, \u03bb), where\n\u03c8 is the interest parameter or parameter of interest and \u03bb is a nuisance or incidental\nparameter. the former is the focus of enquiry, while the latter summarizes aspects\nof the model that are not of central concern, but which are nevertheless essential to\nrealistic modelling. usually \u03c8 has small dimension, often being scalar, while \u03bb may\nbe of high dimension. different elements of \u03b8 may be nuisance or interest parameters\nat different stages of an investigation. we suppose throughout the discussion below\nthat \u03c8 and \u03bb are variation independent, that is, the parameter space has form \u0001 \u00d7 \u0001,\nso knowledge about the range of \u03bb imparts no information about \u03c8. inmany cases\nit is desirable that inferences be invariant to interest-preserving reparametrizations,\nthat is, one-one maps between (\u03c8, \u03bb) and (\u03b7(\u03c8), \u03b6 (\u03c8, \u03bb)).\n\nif x \u223c n (\u00b5, \u03c3 2), then the log-normal variable\n\nexample 12.1 (log-normal mean)\ny = exp(x) has mean and variance\ne(y ) = exp(\u00b5 + \u03c3 2/2) = \u03c8,\n\nvar(y ) = exp(2\u00b5 + \u03c3 2){exp(\u03c3 2) \u2212 1} =\u03bb,\n\nsay. a confidence interval (\u03c8\u2212, \u03c8+) for \u03c8 should transform to (log \u03c8\u2212, log \u03c8+) if the\nmodel is expressed in terms of \u03b7 = log \u03c8 and \u03b6 = \u03b6 (\u03c8, \u03bb) = \u03c3 2.\n(cid:1)\n\nin many important cases the density of data y may be factorized as\n\nf (y; \u03c8, \u03bb) \u221d f (t1 | a; \u03c8) f (t2 | t1, a; \u03c8, \u03bb),\n\nor as\n\nf (y; \u03c8, \u03bb) \u221d f (t1 | t2, a; \u03c8) f (t2 | a; \u03c8, \u03bb),\n\nwhere terms free of the parameters have been neglected. the quantity a may not be\npresent, but if it is, it is usually chosen to be an ancillary statistic, a notion discussed in\nsection 12.1. if such a factorization holds, it is natural to base inference for \u03c8 on the\nleading term on its right side. information about \u03c8 can then be extracted from the data\nwithout needing to estimate or otherwise account for \u03bb. leaving aside the presence\nof a, these terms are respectively marginal and conditional densities of t1, and can be\n\n645\n\n "}, {"Page_number": 658, "text": "so-called because first\npointed out by neyman\nand scott (1948).\n\n646\n\n12 \u00b7 conditional and marginal inference\n\nviewed as a marginal likelihood and a conditional likelihood for \u03c8. one reason for\nconsidering them is that when \u03bb is high-dimensional, the standard likelihood methods\ndescribed in chapter 4 and used throughout the book can become unreliable or fail\nutterly, as in the following classic example.\n\n2\n\n\u03c3 2\n\n(cid:2)\n\n(cid:3)\n\n(cid:2)\n\n= \u2212 1\n\n(yi j \u2212 \u00b5i )2\n\nk log \u03c3 2 + 1\n\nexample 12.2 (neyman\u2013scott problem)\nthe log likelihood based on data yi j ,\ni = 1, . . . ,m , j = 1, . . . ,k , supposed to be realized values of independent normal\nvariables with means \u00b5i and common variance \u03c3 2, is\nk(cid:1)\n(cid:9)(\u00b51, . . . , \u00b5m , \u03c3 2) \u2261 \u2212 1\nj=1\nk(cid:1)\nk log \u03c3 2 + 1\nj=1\n(cid:4)\nis yi\u00b7 = k\nj yi j , the maximum\ni j (yi j \u2212 yi\u00b7)2. the sums of squares\n\nlikelihood estimate of \u03c3 2 is (cid:5)\u03c3 2 = (mk)\n(cid:4)\n\u22121\nj (yi j \u2212 yi\u00b7)2 are independently distributed as \u03c3 2\u03c7 2\n\nas the maximum likelihood estimate of \u00b5i\n\n(yi j \u2212 yi\u00b7)2 + k\n\n\u03c3 2 (yi\u00b7 \u2212 \u00b5i )2\n\nm(cid:1)\ni=1\nm(cid:1)\ni=1\n\ne((cid:5)\u03c3 2) = \u03c3 2 (k \u2212 1)\nthus if k is fixed and m increases, (cid:5)\u03c3 2 converges to the wrong value. when k = 2,\nfor example,(cid:5)\u03c3 2\np\u2212\u2192 \u03c3 2/2, so(cid:5)\u03c3 2 is both biased in small samples and inconsistent as\nm \u2192 \u221e.\n\nvar((cid:5)\u03c3 2) = 2\u03c3 4 (k \u2212 1)\n\nk\u22121, so\n\n(cid:4)\n\nmk2\n\n(cid:3)\n\n\u22121\n\n\u03c3 2\n\nk\n\n,\n\n2\n\n.\n\n.\n\nthe problem here is that the number of parameters increases with the sample\nsize, but the information about each of the \u00b5i stays fixed. moreover, even though the\nexpected information for \u03c3 2 does increase with m, it gives a misleading impression of\nthe precision with which \u03c3 2 can be estimated. this is an extreme situation, but similar\ndifficulties arise in many other models. here they could be eliminated by replacing k\n\nby k \u2212 1 inthe denominator of (cid:5)\u03c3 2, but such a repair is impossible in general. it turns\n(cid:1)\n\nout, however, that use of marginal likelihood rescues the situation.\n\nthis chapter gives an introduction to conditional and marginal inference. in sec-\ntion 12.1 we define ancillary statistics and discuss their properties, and in sections 12.2\nand 12.3 describe marginal and conditional likelihoods, which have their major ap-\nplications for group transformation and exponential family models respectively. in\nmany cases these likelihoods are difficult to construct exactly. highly accurate ap-\nproximations to them may be based on saddlepoint procedures, which are closely\nrelated to the bayesian use of laplace approximation, and these are described in\nsection 12.3. finally section 12.4 gives a brief account of modifications intended to\nimprove inferences based on the profile likelihood.\n\n12.1 ancillary statistics\n\nconditioning arguments have several important roles in statistics. one is to re-\nstrict the sample space under consideration, thereby ensuring that repeated sampling\n\n "}, {"Page_number": 659, "text": "12.1 \u00b7 ancillary statistics\n\n647\n\ncomparisons are made within a so-called relevant subset of the full sample space. the\nconditionality principle (section 11.1.2) suggests that this will often result in infer-\nences that are close to bayesian ones, but without the need to specify a prior density\nthat may be difficult to justify. a second role of conditioning is the elimination of\nnuisance parameters; see sections 5.2.3, 7.3.3, and 12.3. in this section we focus on\nthe use of conditioning for ensuring relevance of tests and confidence intervals.\nconsider a statistical model for data y that depends on a parameter \u03b8, and suppose\nthat the minimal sufficient statistic for \u03b8 is s = s(y ). the factorization theorem\nimplies that inference for \u03b8 may be based on the density of s. if wecan write s =\n(t , a), where the distribution of a does not depend on \u03b8, then a is said to be ancillary\nfor \u03b8, or, more loosely, an ancillary statistic. note our requirement that a be a function\nof the minimal sufficient statistic. some authors do not impose this, and say that any\nfunction of y whose distribution does not depend on \u03b8 is ancillary, but we reserve\nthe term distribution constant for such statistics; see section 5.3. we shall see below\nthat in a particular sense ancillary statistics determine the amount of information in the\ndata, and as a result they play a central role in conditional inference.\n\nthe likelihood for \u03b8 may be written\n\nl(\u03b8) \u221d f (s; \u03b8) = f (t, a; \u03b8) = f (a) f (t | a; \u03b8),\n\nsuggesting that inference for \u03b8 be based on the conditional distribution of t given\nthe observed value a of a. one argument for this is that, in principle at least, the\nexperiment that generates s may be regarded as having two stages. the first stage\nconsists of observing a value a from the marginal density of a. inthe second stage a\nvalue for t is observed from the conditional density of t given that a = a. asthe\ndistribution of a does not depend upon \u03b8, the conditionality principle implies that\ninference for \u03b8 should be based only on the second stage, which contributes f (t | a; \u03b8)\nto the likelihood. according to this argument the subset of the sample space relevant to\ninference for \u03b8 is the set {y : a(y) = aobs}, where aobs denotes the value of a actually\nobserved.\n\nexample 12.3 (uniform density) let y1, . . . ,y n be a random sample from the\ndensity uniform on (\u03b8 \u2212 1/2, \u03b8 + 1/2), with \u03b8 unknown. then the likelihood,\n\n(cid:6)\n\nl(\u03b8) =\n\n\u2264 y1, . . . , yn \u2264 \u03b8 + 1\n2 ,\n\n\u03b8 \u2212 1\notherwise,\n\n2\n\n1,\n0,\n\ncan be re-expressed as\n\nl(\u03b8) =\n\n(cid:6)\n\n1, max(y j ) \u2212 1\notherwise,\n0,\n\n2\n\n\u2264 \u03b8 \u2264 min(y j ) + 1\n2 ,\n\nso the minimal sufficient statistic consists of the smallest and largest order statistics,\ns = (y(1), y(n)). the joint density of v = y(n) and u = y(1) is\n\nf (u, v) = n(n \u2212 1)(v \u2212 u)n\u22122,\n\n\u03b8 \u2212 1\n\n2\n\n\u2264 u < v < \u03b8 + 1\n\n2\n\n,\n\n "}, {"Page_number": 660, "text": "648\n\n12 \u00b7 conditional and marginal inference\nand on changing variables from (v, u) to (a, t), where a = v \u2212 u = y(n) \u2212 y(1) and\nt = (y(1) + y(n))/2, we obtain\n\nf (t, a) = n(n \u2212 1)an\u22122,\n\n\u03b8 \u2212 1\u2212a\n\n2\n\n\u2264 t \u2264 \u03b8 + 1\u2212a\n\n, 0 \u2264 a \u2264 1;\n\n2\n\nthe random variable t = (y(1) + y(n))/2 is anunbiased estimator of \u03b8. the support of\nthe joint density for (t, a) is atriangle with vertices (\u03b8, 1) and (\u03b8 \u00b1 1/2, 0). straight-\nforward calculations give\n\n,\n\n(12.1)\n\nf (t) = n (1 \u2212 2|t \u2212 \u03b8|)n\u22121 ,\nf (a) = n(n \u2212 1)(1 \u2212 a)an\u22122,\n\n\u2264 t \u2264 \u03b8 + 1\n\n2\n\n2\n\n\u03b8 \u2212 1\n0 \u2264 a \u2264 1,\n\u2264 t \u2264 \u03b8 + 1\u2212a\n\nf (t | a; \u03b8) = 1\n1 \u2212 a\n\n\u03b8 \u2212 1\u2212a\n\n.\n\n2\n\n2\n\n,\n\n(12.2)\nthus a is ancillary, as is clear from its location-invariance. conditional on a = a, the\ndensity of t is uniform on an interval of length 1 \u2212 a centred at \u03b8. from (12.2) we see\nthat the conditional likelihood for \u03b8 equals (1 \u2212 a)\n\u22121 on the interval with endpoints\nt \u00b1 (1 \u2212 a)/2 and is zero elsewhere. thus a\n.= 0 conveys little information about \u03b8,\n.= 1 pins down \u03b8 very precisely.\nwhereas a\nfor example, a sample of size n = 2 with y(1) = \u22120.25 and y(2) = 0.45 has t = 0.1\nand a = 0.7. the unconditional equi-tailed 0.9 confidence interval for \u03b8 based on\n(12.1) is (\u22120.242, 0.442) (exercise 12.1.2), while the 0.9 conditional interval based on\n(12.2) is (\u22120.035, 0.235); their respective lengths are 0.68 and 0.27. the unconditional\ninterval is logically inconsistent with the data, because it includes values such as\n\u03b8 = 0.3 for which y(1) = \u22120.25 could not be observed. the conditional interval takes\n(cid:1)\nthe observed value of a into account and eliminates such absurdities.\n\nexample 12.4 (regression model)\nin many regressions the observed explanatory\nvariables x and responses y could both be regarded as realizations of random variables\nx and y , inthe sense that different values ( x, y) might have occurred. this is clearest\nin an observational study, where individuals each have a number of variables recorded,\nsome being later regarded as explanatory and others as responses. unless the values\nof the xs are restricted, the data collection scheme implies that they may be modelled\nas random variables. if, however, the joint density of (x, y ) factorizes as\n\nf (y | x; \u03c8) f (x),\n\nand properties of the marginal distribution of x are not of interest, it will be appro-\npriate to treat x as fixed throughout the analysis, and this is what is generally done\nin practice. notice that \u03c8 is a parameter of the conditional density of y given x;\nanalysis based on f (y | x; \u03c8) alone would not be appropriate if \u03c8 also entered the\ndistribution of x.\nconsider the linear model y = x\u03b2 + \u03c3 \u03b5, where y and x are the vector of responses\nand the observed design matrix, respectively. the least squares estimator of \u03b2 is\n\u22121. thus the estimator\n(x tx)\nand its variance both depend on the explanatory variables actually observed, and not\non those that might have been observed but in fact were not. for example, if certain\n\n\u22121x ty , and conditional on x = x, ithas variance \u03c3 2(x tx)\n\nin this example (only) we\ndepart from our\nconvention that x is the\nobserved design matrix.\n\n "}, {"Page_number": 661, "text": "see basu (1955, 1958).\n\n12.1 \u00b7 ancillary statistics\n\n649\n\nparameters cannot be estimated from the design matrix actually used, it is irrelevant\n(cid:1)\nthat with another design they might have been estimable.\n\nbasu\u2019s theorem\nif s is a complete minimal sufficient statistic, then it is independent of any distribution\nconstant statistic c. tosee this in the discrete case, note that for any c and \u03b8 the\nmarginal density of c may be written as\n\nfc(c) =\n\nfc|s(c | s) fs(s; \u03b8),\n\n(cid:1)\n\ns\n\nwhere the sum is over all possible values of s. this implies that for all \u03b8\n\n(cid:1)\n\n(cid:7)\n\n(cid:8)\n\nfc(c) \u2212 fc|s(c | s)\n\nfs(s; \u03b8) = 0,\n\ns\n\nand completeness of s gives fc(c) = fc|s(c | s) for every c and s. hence c and s are\nindependent. this result is useful, because it assures independence without the effort\nof computing the joint density of c and s. the argument in the continuous case is\nanalogous.\n\nmal linear model y = x\u03b2 + \u03b5 consists of(cid:5)\u03b2 = (x t x)\nexample 12.5 (normal linear model) the minimal sufficient statistic in the nor-\n\u22121 x t y and the rescaled sum of\nsquares s2 = (y \u2212 x(cid:5)\u03b2)t(y \u2212 x(cid:5)\u03b2)/(n \u2212 p). that the pair ((cid:5)\u03b2, s2) iscomplete and min-\n\nimal sufficient follows from properties of the exponential family; see example 7.10.\nthe standardized residuals\n\n(cid:5)\u03b2\ny j \u2212 x t\ns(1 \u2212 h j )1/2\n\nj\n\nj = 1, . . . ,n ,\n\n,\n\nimplies that this distribution is independent of(cid:5)\u03b2 and s2; see example 5.14.\n\nhave a (degenerate) joint distribution that does not depend on \u03b2 and \u03c3 2. basu\u2019s theorem\n(cid:1)\n\nlocation model\nsuppose that y1, . . . , yn is a random sample from the continuous density f (y; \u03b8) =\ng(y \u2212 \u03b8), where \u2212\u221e < \u03b8 <\u221e and the density g is known. here \u03b8 determines the\nlocation of f , and we can write a random variable from f as y = \u03b8 + \u03b5, where\n\u03b5 has density g. except for certain special choices of g, such as the normal and\nuniform densities, the minimal sufficient statistic for \u03b8 consists of the order statistics\ny(1) < \u00b7\u00b7\u00b7 < y(n), whose joint density is\nn(cid:9)\nj=1\n\nf (y(1), . . . , y(n); \u03b8) = n!\n\ny(1) < \u00b7\u00b7\u00b7 < y(n).\n\ng(y( j) \u2212 \u03b8),\n\nwe now discuss conditional inference for \u03b8 based on the maximum likelihood es-\n\ntimator (cid:5)\u03b8. although rarely of interest in applications, the simplicity of this model\n\nbrings out the main ideas without unnecessary complication, and helps to motivate\nlater developments.\n\n "}, {"Page_number": 662, "text": "650\n\nwe assume that(cid:5)\u03b8 is the sole solution to the score equation\n\n0 = n(cid:1)\n\nj=1\n\n\u2202 log g(y( j) \u2212(cid:5)\u03b8)\n\n\u2202\u03b8\n\n12 \u00b7 conditional and marginal inference\n= n(cid:1)\n\n\u2202 log g(a j )\n\n,\n\n(12.3)\n\nj=1\n\n\u2202\u03b8\n\nwhere a j = y( j) \u2212(cid:5)\u03b8. as(cid:5)\u03b8 is equivariant, the random variables a1, . . . , an form a\nmaximal invariant, which is of course distribution constant. moreover the configu-\nration a = (a1, . . . , an) is afunction of the minimal sufficient statistic. hence it is\ndistribution of(cid:5)\u03b8 given the value a of a. equivariance of(cid:5)\u03b8 means that z(\u03b8) =(cid:5)\u03b8 \u2212 \u03b8\nancillary. the discussion above suggests that inference be based on the conditional\nis a pivot, so if we let z\u03b1(a) denote the \u03b1 quantile of the conditional distribution of\nz(\u03b8), a (1 \u2212 2\u03b1) conditional confidence interval for \u03b8 will have limits\n\n(cid:5)\u03b8 \u2212 z1\u2212\u03b1(a), (cid:5)\u03b8 \u2212 z\u03b1(a)\n\nthat depend on the observed a.\n\nanother possibility is to use the unconditional distribution of z(\u03b8), whose quantiles\nz\u03b1 and z1\u2212\u03b1 do not depend on a. these quantiles are readily estimated by simulation,\n\nbut the z\u03b1(a) are not. in fact the conditional distributions of(cid:5)\u03b8 and hence of z(\u03b8) are\n\nfairly easily found, as we now see.\nowing to the constraint (12.3), the density of a is degenerate, and there exists a\nfunction h such that a1 = h(a2, . . . , an); this is assumed sufficiently smooth for the\ndevelopment below. thus\n\ny(1) =(cid:5)\u03b8 + h(a2, . . . , an),\n\nj = 2, . . . ,n .\nthe jacobian for transformation from (y(1), . . . , y(n)) to ((cid:5)\u03b8 , a2, . . . , an) is\n\ny( j) =(cid:5)\u03b8 + a j ,\n\n(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)\n\n1\nh1\n...\nhn\n\n(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)\n\n1 \u00b7\u00b7\u00b7 1\n\u00b7\u00b7\u00b7 0\n1\n...\n...\n...\n\u00b7\u00b7\u00b7 1\n0\n\n= h(a),\n\n(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)\n\n...\n\n...\n\n\u2202y(2)\n\n\u2202y(1)\n\n\u2202(cid:5)\u03b8\n\n\u2202(cid:5)\u03b8\n\n\u2202y(2)\n\u2202a2\n\n\u2202y(1)\n\u2202a2\n\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n...\nn(cid:9)\nj=1\nso for a1 \u2264 \u00b7\u00b7\u00b7 \u2264 an and(cid:5)\u03b8 real,\n\nf ((cid:5)\u03b8 , a; \u03b8) = n!h(a)\n\n\u2202y(2)\n\u2202an\n\n\u2202y(1)\n\u2202an\n\n(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)\n\n\u2202y(n)\n\n\u2202(cid:5)\u03b8\n\n\u2202y(n)\n\u2202a2\n\n...\n\n\u2202y(n)\n\u2202an\n\n=\n\n(cid:10)(cid:10)\n\nsay, where h j = \u2202h(a2, . . . , an)/\u2202a j . hence the joint density of(cid:5)\u03b8 and a is\n\ng(y( j) \u2212 \u03b8)\n\ny( j)=(cid:5)\u03b8+a j\n\n= n!h(a)\n\nn(cid:9)\nj=1\n\ng(a j +(cid:5)\u03b8 \u2212 \u03b8),\n\nf ((cid:5)\u03b8 | a; \u03b8) = f ((cid:5)\u03b8 , a; \u03b8)\n\nf (a)\n\n(cid:11)\n(cid:12) \u221e\n\n\u2212\u221e\n\nn\n\nj=1 g(a j +(cid:5)\u03b8 \u2212 \u03b8)\n(cid:11)\nj=1 g(a j + u) du\n\nn\n\n=\n\n.\n\n(12.4)\n\nthis conditional density contains all the information in the data concerning \u03b8, appro-\n\npriately conditioned. changing variables to z(\u03b8) =(cid:5)\u03b8 \u2212 \u03b8 and integrating gives\n\npr{z(\u03b8) \u2264 z | a = a} =\n\n(cid:11)\n(cid:11)\n\n(cid:12)\n(cid:12) \u221e\nz\u2212\u221e\n\u2212\u221e\n\nn\n\nj=1 g(a j + u) du\nj=1 g(a j + u) du\n\nn\n\n.\n\n "}, {"Page_number": 663, "text": "12.1 \u00b7 ancillary statistics\n\n651\n\nfigure 12.1 conditional\ninference for location in\ntwo samples of size n = 5\nfrom the t3 density.\npivot z(\u03b8) =(cid:5)\u03b8 \u2212 \u03b8 given\nconditional density of\n\nconfiguration ancillary\n(solid), and unconditional\ndensity of z(\u03b8) (dots).\nthe blobs show the\nobserved configuration,\nand the rug the expected\nconfiguration.\n\ny\nt\ni\ns\nn\ne\nd\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\ny\nt\ni\ns\nn\ne\nd\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n-4\n\n-2\n\n\u2022 \u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n0\n\nz\n\n2\n\n4\n\n-4\n\n-2\n\n\u2022\n\n\u2022\n\n2\n\n4\n\n\u2022\u2022\u2022\n\n0\n\nz\n\nthis function can be obtained by evaluating the integrand on a grid of values, or by\nmore sophisticated methods. once it has been approximated the quantiles z\u03b1(a) are\neasily found.\n\nexample 12.6 (student t distribution) figure 12.1 shows the conditional densi-\nties of z(\u03b8) for two samples of size five from the t3 density. in the left panel the\n\nconfiguration is very asymmetric, and hence so is the conditional density of (cid:5)\u03b8. in\n\nthe right panel the configuration is quite symmetric but underdispersed relative to a\ntypical configuration. hence the conditional density of z(\u03b8) ismore peaked than the\nunconditional density, thereby giving more precise inferences.\nin the left panel the conditional 0.025 and 0.975 quantiles are \u22122.11 and 1.03, so if\n(cid:5)\u03b8 = 0 the 0.95 confidence interval, (\u22121.03, 2.11), is asymmetric in the same direction\nas the configuration. the corresponding confidence interval for the right panel would\nbe (\u22121.13, 1.18), to be compared to the unconditional interval (\u22121.61, 1.61).\n(cid:1)\nit is suggestive to express f ((cid:5)\u03b8 | a; \u03b8) using the log likelihood for the model, which\nwe write as (cid:9)(\u03b8;(cid:5)\u03b8 , a) tostress its dependence on the data through (cid:5)\u03b8 and a. setting\nu = \u03b8 +(cid:5)\u03b8 \u2212 v we see that (12.4) becomes\n(cid:12) \u221e\n\nf ((cid:5)\u03b8 | a; \u03b8) =\n\n(12.5)\n\n.\n\n\u22121/2|j ((cid:5)\u03b8;(cid:5)\u03b8 , a)|1/2 exp\n\nlaplace approximation to the integral (section 11.3.1) gives\n\n(cid:8)\nf ((cid:5)\u03b8 | a; \u03b8) = (2\u03c0)\nwhere j ((cid:5)\u03b8;(cid:5)\u03b8 , a) equals \u2212\u2202 2(cid:9)(\u03b8;(cid:5)\u03b8 , a)/\u2202\u03b8 2 evaluated at \u03b8 =(cid:5)\u03b8. in fact the quantity\nj ((cid:5)\u03b8;(cid:5)\u03b8 , a) does not depend on(cid:5)\u03b8, because the log likelihood may be written as (cid:9)((cid:5)\u03b8 \u2212 \u03b8).\n\n\u22121)\n(12.6)\n\n1 + o(n\n\n(cid:8)(cid:7)\n\n.\n\n(cid:8)\n(cid:7)\n(cid:9)(\u03b8;(cid:5)\u03b8 , a)\n(cid:7)\n(cid:8)\n(cid:9)(v;(cid:5)\u03b8 , a)\n(cid:9)(\u03b8;(cid:5)\u03b8 , a) \u2212 (cid:9)((cid:5)\u03b8;(cid:5)\u03b8 , a)\n\ndv\n\nexp\n\u2212\u221e exp\n(cid:7)\n\n\u2217\n\nthis is called the p\nformula or\nbarndorff-nielsen\u2019s\nformula.\n\nby (12.5) we see that renormalizing (12.6) to have unit integral will recover the exact\nexpression, giving\n\nf ((cid:5)\u03b8 | a; \u03b8) = (2\u03c0)\n\n(cid:9)(\u03b8;(cid:5)\u03b8 , a) \u2212 (cid:9)((cid:5)\u03b8;(cid:5)\u03b8 , a)\ndetailed analysis of the laplace expansion shows that c(a) = 1 + o(n\n\u22121).\n\n\u22121/2c(a)|j ((cid:5)\u03b8;(cid:5)\u03b8 , a)|1/2 exp\n\n(12.7)\n\n(cid:7)\n\n(cid:8)\n\n.\n\n "}, {"Page_number": 664, "text": "652\n\n12 \u00b7 conditional and marginal inference\nformula (12.7) expresses the conditional density of(cid:5)\u03b8 in terms of the log likelihood\n\nand its derivatives and is the basis for much further development. it is exact for group\ntransformation models and approximately true more generally, though we shall prove\nneither of these assertions; see instead the bibliographic notes.\n\nif we accept that (12.7) is correct, how may it be used for inference? one possibility\n\nis to obtain conditional confidence intervals for \u03b8 by transforming(cid:5)\u03b8 to a normally-\ndistributed pivot. to do this, we write the integral of (12.6) in form (11.30), taking\na(u) = |j (u; u, a)|1/2 and g(u) = (cid:9)(u; u, a) \u2212 (cid:9)(\u03b8; u, a). then the argument leading\nto (11.31) gives\n\npr((cid:5)\u03b8 \u2264 t | a = a; \u03b8) = \u0001\n\n\u2217\n(\u03b8)\nr\n\u22121 log{v(\u03b8)/r(\u03b8)}, with\n\n\u2217\n\n(\u03b8) = r(\u03b8) + r(\u03b8)\n\nwhere r\nr(\u03b8) = sign(t \u2212 \u03b8) [2{(cid:9)(t; t, a) \u2212 (cid:9)(\u03b8; t, a)}]1/2 , v(\u03b8) = (cid:9)\n\n(cid:8) + o(n\n\n(cid:7)\n\n\u22121),\n\n(12.8)\n\n;(cid:5)\u03b8 (t; t, a) \u2212 (cid:9)\n\n;(cid:5)\u03b8 (\u03b8; t, a)\n\n|j (t; t, a)|1/2\n\n.\n\nthe last of these involves a sample space derivative of (cid:9),\n\n;(cid:5)\u03b8 (\u03b8;(cid:5)\u03b8 , a) = \u2202(cid:9)(\u03b8;(cid:5)\u03b8 , a)\n\u2202(cid:5)\u03b8\n\n.\n\n(cid:9)\n\nin order to obtain this, (cid:9) must be expressed as a function of(cid:5)\u03b8 and a so that it can be\ndifferentiated partially with respect to(cid:5)\u03b8, holding a fixed. usually this re-expression is\n\n(cid:9)\n\n\u2217\n\n\u2202\u03b8\n\n(cid:8)\n\n\u22123/2)\n\n1 + o(n\n\n(\u03b8)}(cid:7)\n\ndifficult, and approximations to such derivatives are needed. for the location model,\nhowever, (cid:9)\n\n;(cid:5)\u03b8 (t; t, a) = 0 for any t, and\n\n;(cid:5)\u03b8 (\u03b8;(cid:5)\u03b8 , a) = \u2212 \u2202(cid:9)(\u03b8;(cid:5)\u03b8 , a)\n\n= \u2212(cid:9)\u03b8;(\u03b8;(cid:5)\u03b8 , a),\nsay. thus v(\u03b8) = (cid:9)\u03b8;(\u03b8; t, a)/|j (t; t, a)|1/2 is the score statistic.\nn1/2(t \u2212(cid:5)\u03b8) = o(1), the error in (12.8) is relative and of order o(n\nmore detailed computations show that in a moderate deviation region, where\n\u22123/2): the right-\nhand side is in fact \u0001{r\nt \u2212(cid:5)\u03b8 = o(1), the error remains relative but becomes o(n\n. in alarge deviation region, for which\n\u22121). the relative error\npersists far into the tails of the distribution of(cid:5)\u03b8.\nproperty helps explain why the extraordinary accuracy of this type of approximation\n(\u03b8)\nis an approximate pivot, conditional on a = a, because its distribution is almost\nstandard normal. as this distribution is essentially independent of a, r\n(\u03b8) isalso\napproximately normal unconditionally. the limits of a (1 \u2212 2\u03b1) confidence interval\nfor \u03b8 may be found as those \u03b8 for which \u0001{r\n(\u03b8)} =\u03b1, 1 \u2212 \u03b1; equivalently we may\n\u2217\n(\u03b8) = \u0001\u22121(\u03b1) and r\n(\u03b8) = \u0001\u22121(1 \u2212 \u03b1).\n\u2217\nsolve for \u03b8 the equations r\nz(\u03b8) =(cid:5)\u03b8 \u2212 \u03b8, soboth z(\u03b8) and r\n(\u03b8) can be regarded as a transformation of(cid:5)\u03b8 \u2212 \u03b8 to normality. such transformations\n\n(\u03b8) is amonotone function of\n(\u03b8) yield the same conditional confidence intervals;\n\nr\nhave the same effect in much greater generality. there is a close link to the bayesian\napproximations of section 11.3.1.\n\nexpression (12.8) shows that the random quantity r\n\nowing to the structure of the location model, r\n\n(\u03b8) corresponding to r\n\n\u2217\n\n\u2217\n\n\u2217\n\n\u2217\n\n\u2217\n\n\u2217\n\n\u2217\n\n "}, {"Page_number": 665, "text": "(\u03b8)\n\nfigure 12.2 use of\nnormal approximation to\n\u2217\npivotal quantities r\n(solid), r(\u03b8) (large\ndashes), and z1(\u03b8) (small\ndashes) to construct 0.95\nconfidence intervals for\nlocation based on two\nsamples of size 5. the\ndotted horizontal lines\nshows the 0.025 and 0.975\nquantiles of the standard\nnormal distribution, and\nthe dotted vertical lines\nmark the limits of the\nconfidence interval based\non r\nconfidence intervals based\non r\naccount of the skewness\nof the configuration\nancillary.\n\n(\u03b8). note how the\n\n(\u03b8) and r(\u03b8) take\n\n\u2217\n\n\u2217\n\n12.1 \u00b7 ancillary statistics\n\n653\n\n)\na\n\nt\n\ne\nh\n\nt\n(\nt\n\no\nv\np\n\ni\n\n3\n\n2\n\n1\n\n0\n\n1\n-\n\n2\n-\n\n3\n-\n\n)\na\n\nt\n\ne\nh\n\nt\n(\nt\n\no\nv\np\n\ni\n\n3\n\n2\n\n1\n\n0\n\n1\n-\n\n2\n-\n\n3\n-\n\n-3\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\n3\n\n-3\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\n3\n\ntheta\n\ntheta\n\nthe approximate pivot r\n\n\u2217\n\n(\u03b8) = r(\u03b8) + rinf(\u03b8), where\n\u22121 log{v (\u03b8)/r(\u03b8)} ,\nrinf(\u03b8) = r(\u03b8)\n(cid:7)\n(cid:13)\n(cid:9)((cid:5)\u03b8;(cid:5)\u03b8 , a) \u2212 (cid:9)(\u03b8;(cid:5)\u03b8 , a)\n\n2\n\nr(\u03b8) = sign((cid:5)\u03b8 \u2212 \u03b8)\n\n(12.9)\n\n(cid:8)(cid:14)1/2 .\n\nis a modified form of the signed likelihood ratio statistic\n\n\u22121/2)\nas r(\u03b8) has an approximate normal distribution with absolute error of o(n\n\u22123/2), rinf(\u03b8) can be regarded as a correction to\nrather than relative error of o(n\nr(\u03b8) that gives an improved normal approximation; although r\n(\u03b8) isslightly more\ncomplicated, it yields inferences that are much more accurate. it can be useful to\ncompute values rinf(\u03b8) of rinf(\u03b8) for the plausible range of values of \u03b8.\n\n\u2217\n\nexample 12.7 (student t distribution) the log likelihood contribution for the t\u03bd\ndensity is g(u) = \u2212 1\n2 (\u03bd + 1) log(1 + u2/\u03bd), so the log likelihood and its derivatives\nare\n\n1 + (a j +(cid:5)\u03b8 \u2212 \u03b8)2/\u03bd\n\n(cid:8)\n\n,\n\n(cid:7)\n\nlog\n\n2\n(cid:9)\u03b8;(\u03b8; t, a) = (\u03bd + 1)\n\nn(cid:1)\n(cid:9)(\u03b8;(cid:5)\u03b8 , a) = \u2212 \u03bd + 1\nj=1\nn(cid:1)\n(a j + t \u2212 \u03b8)/\nj=1\nn(cid:1)\nj=1\n\nj (t; t, a) = (\u03bd + 1)\n\n(\u03bd \u2212 a2\n\nj )/(\u03bd + a2\n\nj )2,\n\n(cid:7)\n\n\u03bd + (a j + t \u2212 \u03b8)2\n\n(cid:8)\n\n,\n\n\u2217\n\n(\u03b8) may be found.\n\nthe data used in the left panel of figure 12.1 have n = 5,(cid:5)\u03b8 = 0.22, and configura-\nfrom which r(\u03b8), v(\u03b8), and r\ntion a = (\u22120.71,\u22120.54,\u22120.47, 2.39, 3.28). the left panel of figure 12.2 compares\n(\u03b8), r(\u03b8) and the approximate pivot z1(\u03b8) = j ((cid:5)\u03b8;(cid:5)\u03b8 , a)1/2((cid:5)\u03b8 \u2212 \u03b8)\nderived from the large-sample n (\u03b8, j ((cid:5)\u03b8;(cid:5)\u03b8 , a)\n\u22121) distribution of(cid:5)\u03b8. all three decrease\nthe values of r\n\n\u2217\n\nas functions of \u03b8, but r\n(\u03b8) and r(\u03b8) are curved for positive \u03b8 and shifted upwards rel-\native to z1(\u03b8), reflecting the asymmetry of the configuration. the limits of confidence\n\n\u2217\n\n "}, {"Page_number": 666, "text": "\u2217\n\n654\n\n12 \u00b7 conditional and marginal inference\n(\u03b8) = z\u03b1, z1\u2212\u03b1, and\nintervals based on r\nlikewise with r(\u03b8) and z1(\u03b8). thus 0.95 confidence intervals based on r\n(\u03b8), r(\u03b8),\nand z1(\u03b8) are (\u22120.83, 2.45), (\u22120.87, 2.08), and (\u22121.00, 1.44). these intervals reflect\nthe upward shift and curvature in r\n(\u03b8), as the corresponding interval is moved to the\nright and quite asymmetric.\n\n(\u03b8) are read off as those values for which r\n\n\u2217\n\n\u2217\n\n\u2217\n\nthe right panel of figure 12.2 shows that for the symmetric configuration in the right\n(\u03b8) are essentially\n(cid:1)\n\nof figure 12.1, confidence intervals based on z(\u03b8), r(\u03b8), and r\nidentical.\n\n\u2217\n\napplications are usually much more complicated than this, and though it nicely\nillustrates how conditioning can affect inference, one might wonder how relevant the\nabove toy example is to a real world in which models contain nuisance parameters\nand have no exact ancillary statistics. in practice a key aspect is not the number of\nobservations, but the information they contain. if ten parameters are estimated from\n50 observations, say, then one may worry that inference for one key parameter is\nweakened by having to deal with the other nine: the proportion of parameters to\nobservations is the same as in the example above. then small-sample procedures\nmay in some cases provide reassurance that standard inferences are adequate, while\ngiving improvements when they are not.\n\ndifficulties with ancillaries\none difficulty in a general treatment of conditional inference is that some statistical\nmodels admit no exact ancillary statistic, while others have several. an instance of\nthis is the location model, where any subset of the configuration is ancillary, but there\nit is clear that the sample space is partitioned most finely by conditioning on the entire\nconfiguration, which is a maximal ancillary statistic. such arguments may not always\nbe applied, however.\n\n6 (2 + \u03b8), \u03c04 = 1\n\n6 (1 \u2212 \u03b8), \u03c03 = 1\n\n6 (1 + \u03b8), \u03c02 = 1\n\nexample 12.8 (multinomial distribution) consider the multinomial distribu-\ntion with denominator m and responses r1, . . . , r4 corresponding to cells with\nprobabilities\n\u03c01 = 1\n6 (2 \u2212 \u03b8), \u22121 < \u03b8 <1.\nthe minimal sufficient statistic is s = (r1, r2, r3), but a1 = r1 + r2 and\na2 = r2 + r3 are binomial with success probabilities 1\n2 . as a1 and a2 are\nfunctions of s whose distributions are free of \u03b8, both are ancillary. they are not\njointly ancillary, however (problem 12.6).\nconditioning on a1 = a1 splits the multinomial likelihood into two binomial\n2 (1 + \u03b8), and\ncontributions, one from r1 with denominator a1 and probability 1\nthe other from r3 with denominator m \u2212 a1 and probability 1\n4 (2 + \u03b8), so the ex-\npected information is the sum of the information from the two binomials, that is\ni1(\u03b8) = 4a1/(1 \u2212 \u03b8 2) + 16(m \u2212 a1)/(2 \u2212 \u03b8 2). conditioning on a2 gives a similar\n\n3 and 1\n\n "}, {"Page_number": 667, "text": "12.1 \u00b7 ancillary statistics\n\n655\n\nsplit, but the conditional expected information is\n\ni2(\u03b8) = 9a2/{(1 \u2212 \u03b8)(2 + \u03b8)} +9(m \u2212 a2)/{(1 + \u03b8)(2 \u2212 \u03b8)}.\n\nevidently there is no reason that i1(\u03b8) should equal i2(\u03b8), and therefore conditional\nlarge-sample confidence intervals for \u03b8 would depend on whether a1 or a2 was\n(cid:1)\nused.\n\nthis lack of uniqueness is annoying in principle but turns out not to be crucial\nin applications. more awkward difficulties arise when there is no exact ancillary.\nthen it is necessary to construct approximate ancillaries, and several approaches\nto this have been proposed. one conceptually simple notion can be used when\nthe model under study can be regarded as nested within a density f (y; \u03b8, \u03bb), with\n\u03bb = \u03bb0 corresponding to the model under study. suppose that the likelihood for the\nlarger model may be expressed as a function of the data through ((cid:5)\u03b8 ,(cid:5)\u03bb, a0), where\n(cid:5)\u03b8 and (cid:5)\u03bb are maximum likelihood estimators and a0 is ancillary for (\u03b8, \u03bb). let (cid:5)\u03b80\nbe the maximum likelihood estimator of \u03b8 when \u03bb = \u03bb0. then if the model under\nstudy is correct, the likelihood ratio statistic 2\nhas an\napproximate chi-squared distribution whatever the value of \u03b8 and so is approximately\nancillary. more elaborate variants of this argument are required to produce ancillary\nstatistics on which conditioning can readily be performed in practice, for example\nby decomposing the ancillary into a sum of squared approximate standard normal\nvariables, and conditioning on these. it may also be necessary to modify these\nvariables to have a joint normal distribution to higher order. the details are tedious\nand since explicit forms of these ancillaries are not needed below we shall not delve\ninto them. although there are different forms of approximate ancillary, most of the\napproximations in later sections do not depend on the particular one used.\n\n(cid:9)((cid:5)\u03b8 ,(cid:5)\u03bb; a0) \u2212 (cid:9)((cid:5)\u03b80, \u03bb0; a0)\n\n(cid:7)\n\n(cid:8)\n\nexercises 12.1\n\niid\u223c n (\u00b5, c2\u00b52), with c known. show that y /s is ancillary for \u00b5.\n\nlet y1, . . . ,y n\nin example 12.3, show that (1 \u2212 2\u03b1) equi-tailed conditional and unconditional confidence\nintervals are t \u00b1 (1/2 \u2212 \u03b1)(1 \u2212 a) and t \u00b1 {(2\u03b1)1/n \u2212 1}/2. hence verify the intervals\ngiven in the example.\ny1/s, . . . ,y n/s is independent of that of s = (cid:4)\nif y1, . . . ,y n is an exponential random sample, show that the joint distribution of\ny j , without computing it. is this true\n\nwhen s is replaced by any other equivariant estimator of scale?\n\n4\n\nlet y1, . . . ,y n\ntion constant, and deduce that y and\n\niid\u223c n (\u00b5, \u03c3 2) with \u03c3 2 known. show that (y1 \u2212 y , . . . ,y n \u2212 y ) isdistribu-\n5 when is the configuration (y1 \u2212(cid:5)\u03b7)/(cid:5)\u03c4 , . . . ,(y n \u2212(cid:5)\u03b7)/(cid:5)\u03c4 in a location-scale model ancillary\n\n(y j \u2212 y )2 are independent.\n\n(cid:4)\n\nas well as maximal invariant?\nshow that conditional and unconditional inference for the mean of the normal location-\nscale model are the same.\ngive expressions for the quantities needed to compute r\nthe data have the gumbel density, so g(u) = \u2212u \u2212 e\nbased on r(\u03b8), r\nexample 12.6.\n\n(\u03b8) for the location model when\n\u2212u. hence find confidence intervals\n(\u03b8), and z1(\u03b8) with the data \u22120.32, \u22120.49, \u22120.25, 2.61, 3.50 from\n\n\u2217\n\n\u2217\n\n1\n2\n\n3\n\n6\n\n7\n\n "}, {"Page_number": 668, "text": "656\n\n12.2 marginal inference\n\n12 \u00b7 conditional and marginal inference\n\nconsider a model for data with minimal sufficient statistic (t1, t2, a), where a is\nancillary, and for which\n\nf (y; \u03c8, \u03bb) \u221d f (t1, t2, a; \u03c8, \u03bb) = f (a) f (t1 | a; \u03c8) f (t2 | t1, a; \u03c8, \u03bb).\n\nas the density of t1 conditional on a depends only on \u03c8, inference for \u03c8 may be\nbased on the marginal likelihood l m(\u03c8) = f (t1 | a; \u03c8). in many cases there is no\nancillary, and then lm(\u03c8) issimply the marginal density of t1. there may be some\nloss of information about \u03c8 due to neglecting the third term in the decomposition\nabove, but the complications in retrieving it are presumed to outweigh the benefits.\nwe have already encountered marginal likelihood and maximum marginal likeli-\nhood estimators in several contexts. such estimators and derived statistics will have\nthe usual large-sample distributions provided that the usual regularity conditions apply\nto f (t1 | a; \u03c8).\nexample 12.9 (normal linear model) consider the usual normal linear model\ny = x\u03b2 + \u03c3 \u03b5, where the \u03b5 j are independent standard normal variables and \u03b2 has\ndimension p \u00d7 1. the minimal sufficient statistics for (\u03b2, \u03c3 2) are\n\nt1 = s2 = (n \u2212 p)\n\n\u22121(y \u2212 x(cid:5)\u03b2)t(y \u2212 x(cid:5)\u03b2),\n\nt2 =(cid:5)\u03b2 = (x t x)\n\n\u22121 x t y,\n\nand these are independent, with (n \u2212 p)s2/\u03c3 2 having a \u03c7 2\ninterest parameter is \u03c3 2, and \u03b2 is the nuisance parameter, we have\n\nf (y; \u03c3 2, \u03b2) = f (y |(cid:5)\u03b2, s2) f ((cid:5)\u03b2; \u03b2, \u03c3 2) f (s2; \u03c3 2).\n\nn\u2212 p distribution. if the\n\nas the density of s2 depends only on \u03c3 2, wemay base inference for \u03c3 2 on the\ncorresponding marginal likelihood\nn \u2212 p\n2\u03c3 2\n\n(cid:16)(n\u2212 p)/2 (s2)(n\u2212 p)/2\u22121\n(cid:18)\n\nlm(\u03c3 2) = f (s2; \u03c3 2) =\n\n\u2212 (n \u2212 p)s2\n\n2\u03c3 2\n\n(cid:15)\n\nexp\n\n(cid:6)\n\n(cid:19)\n\n(cid:17)\n\n\u0001\n\n,\n\nn\u2212 p\n2\n\nwhere \u03c3 2 > 0, s2 > 0. the marginal maximum likelihood estimate of \u03c3 2 is s2, with\nexpected information (n \u2212 p)/(2\u03c3 4) from the marginal likelihood. these give infer-\nences for \u03c3 2 essentially equivalent to those described in chapter 8.\nple 12.2, where p = k and\n\nthis argument applies to any normal linear model, and in particular to exam-\n\n(y \u2212 x(cid:5)\u03b2)t(y \u2212 x(cid:5)\u03b2)\n\ns2 = 1\nn \u2212 p\n1\n\n=\n\nm(k \u2212 1)\n\nm(cid:1)\ni=1\n\nk(cid:1)\nj=1\n\n(yi j \u2212 yi )2 \u223c\n\n\u03c3 2\n\nm(k \u2212 1)\n\n\u03c7 2\nm(k\u22121)\n\n.\n\nhere marginal likelihood automatically produces corrected inferences for \u03c3 2.\n\n(cid:1)\n\nexample 12.10 (partial likelihood) the partial likelihood used with the propor-\ntional hazards model (section 10.8.2) is a marginal likelihood. suppose that survival\n\n "}, {"Page_number": 669, "text": "12.2 \u00b7 marginal inference\n657\nh(y) = (cid:12)\ntime y has hazard \u03be h(y), so its density may be written \u03be h(y) exp{\u2212\u03be h(y)}, where\n\ny\n0 h(u) du is the baseline cumulative hazard. observe that\n\n(cid:20) \u221e\n\nu\n\n\u2212\u03b3 h(s) ds = \u03b3 \u22121e\n\n\u2212\u03b3 h(u).\n\nh(s)e\n\n+\n3\n\n< y1 < y4, where y\n\nto illustrate the argument, consider n = 4 continuous observations that fall in the\n+\nis right-censored. with nothing known\norder 0 < y2 < y\n3\nabout the baseline hazard h(y), a minimal sufficient statistic is the set of failure times\nand censoring indicators (y1, 1), (y2, 1), (y3, 0), (y4, 1); these are in 1\u20131 correspon-\n+, 1, 4), the\ndence with the order statistics (y(1), y(2), y(3), y(4)) and inverse ranks (2, 3\njth of the inverse ranks giving the original index of y( j) and its censoring status. we\nshall compute the probability of seeing this particular realization of inverse ranks. if\ny3 had been observed, the joint density of the y j would be\n\n\u03be2h(y2)e\n\n\u2212\u03be2 h(y2) \u00d7 \u03be3h(y3)e\n\n\u2212\u03be3 h(y3) \u00d7 \u03be1h(y1)e\n\n\u2212\u03be1 h(y1) \u00d7 \u03be4h(y4)e\n\n\u2212\u03be4 h(y4),\n\nso the probability that 0 < y2 < y1 < y4 with y3 censored somewhere to the right of\ny2 is\n\n\u03be2h(y2)e\n\n\u2212\u03be2 h(y2) \u00d7 e\n\n\u2212\u03be3 h(y2) \u00d7 \u03be1h(y1)e\n\n\u2212\u03be1 h(y1) \u00d7 \u03be4h(y4)e\n\n\u2212\u03be4 h(y4).\n\nhence the probability that the uncensored observations fail in the order observed,\nwith y3 censored to the right of y2, is\n\n(cid:20) \u221e\n\n(cid:20) \u221e\n\n(cid:20) \u221e\n\n\u2212(\u03be2+\u03be3)h(y2)\u2212\u03be1 h(y1)\u2212\u03be4 h(y4),\n\n\u03be2\u03be1\u03be4\n\ndy2\n\ndy1\n\ny2\n\ny1\n\n0\n\nand this equals\n\ndy4 h(y2)h(y1)h(y4)e\n(cid:9)\n\n\u00d7 \u03be1\n\u03be1 + \u03be4\n\n=\n\nj\n\n\u03be j(cid:4)\ni\u2208r j\n\n,\n\n\u03bei\n\n\u03be2\n\n\u03be1 + \u03be2 + \u03be3 + \u03be4\n\nwhere the product is over those j for which y j is uncensored and r j denotes the\nrisk set of individuals available to fail at the jth failure time. this last expression is\nsimply the partial likelihood (10.58) in this case. plainly this argument generalizes to n\nfailures, the only complication being notational. thus partial likelihood is a marginal\n(cid:1)\nlikelihood based on the inverse ranks of the failure and censoring times.\n\nrestricted maximum likelihood\nan important application of marginal likelihood is to normal mixed models, which\nwe met in section 9.4.2. there the response vector may be written\n\ny = x\u03b2 + zb + \u03b5,\n\n(12.10)\nwhere the n \u00d7 p and n \u00d7 q matrices x and z are known, \u03b2 is an unknown p \u00d7 1\nparameter vector, and the q \u00d7 1 and n \u00d7 1 random vectors b and \u03b5 are independent\nwith respective nq(0, \u0001b) and nn(0, \u03c3 2 in) distributions. suppose that the variance\nmatrix \u03c3 2\u03d2\u22121 = \u03c3 2 in + z \u0001b z t exists and that \u03d2 depends on parameters \u03c8 but not\non \u03b2. weaim to construct a marginal likelihood for \u03c3 2 and \u03c8, eliminating the nuisance\nparameter \u03b2.\n\n "}, {"Page_number": 670, "text": "658\n\n12 \u00b7 conditional and marginal inference\nwe can write y = x\u03b2 + \u03b6 , where \u03b6 \u223c nn(0, \u03c3 2\u03d2\u22121). if \u03c8 and hence \u03d2 were\nknown, the maximum likelihood estimator of \u03b2 would be\n\u22121 x t\u03d2 y = \u03b2 + (x t\u03d2 x)\n\n(cid:5)\u03b2\u03c8 = (x t\u03d2 x)\n\n\u22121 x t\u03d2 \u03b6,\n\n\u22121).\nwhose distribution is n p(\u03b2, \u03c3 2(x t\u03d2 x)\n\u22121 x t; itsatisfies h x = x. anatural ba-\nlet h denote the n \u00d7 n matrix x(x t x)\nsis for building a marginal likelihood is the vector of residuals (in \u2212 h)y, whose\ndistribution does not depend on \u03b2, butas in \u2212 h has rank (n \u2212 p) this distribution\nis degenerate and it seems better to take just (n \u2212 p) linearly independent residu-\nals. consider therefore the (n \u2212 p) \u00d7 1 random variable u = b t y, where b is any\nn \u00d7 (n \u2212 p) matrix with b b t = in \u2212 h and b t b = in\u2212 p (exercise 12.2.3); b has\nrank (n \u2212 p). now b t y = b t b b t y = b t(in \u2212 h)y is a linear combination of the\nresiduals. furthermore\n\nb t x = b t b b t x = b t(in \u2212 h)x = 0,\n\nwhose distribution does not depend on \u03b2. moreover u and(cid:5)\u03b2\u03c8 are normal variables,\n\ngiving\n\ne\n\nwith covariance\n\n(cid:7)\nu ((cid:5)\u03b2\u03c8 \u2212 \u03b2)\n\nu = b t y = b t(x\u03b2 + \u03b6 ) = b t\u03b6,\n(cid:8) = b te(\u03b6 \u03b6 t)\u03d2 x(x t\u03d2 x)\n\u22121\n= \u03c3 2 b t\u03d2\u22121\u03d2 x(x t\u03d2 x)\n\u22121 = 0,\nbecause b t x = 0. hence u and(cid:5)\u03b2\u03c8 are independent, and therefore\n= f (u,(cid:5)\u03b2\u03c8 ; \u03b2, \u03c8)\nf (u; \u03c8) = f (u; \u03c8) f ((cid:5)\u03b2\u03c8 ; \u03b2, \u03c8)\nf ((cid:5)\u03b2\u03c8 ; \u03b2, \u03c8)\nf ((cid:5)\u03b2\u03c8 ; \u03b2, \u03c8)\nf ((cid:5)\u03b2\u03c8 ; \u03b2, \u03c8)\n= f (y; \u03b2, \u03c8)\nwhile the jacobian for the change of variable from (u,(cid:5)\u03b2\u03c8 ) to y is\n(cid:10)(cid:10)(cid:10)(cid:10) \u2202(u,(cid:5)\u03b2\u03c8 )\n(cid:10)(cid:10)(cid:10)(cid:10)1/2\n(cid:16)(cid:10)(cid:10)(cid:10)(cid:10)1/2\n(cid:16)(cid:10)(cid:10)(cid:10)(cid:10)1/2 = |x t x|\u22121/2.\n\n(cid:10)(cid:10)(cid:10)(cid:10) = | b x(x t x)\n(cid:10)(cid:10)(cid:10)(cid:10)\n(cid:15)\n(cid:10)(cid:10)(cid:10)(cid:10)\n(cid:15)\n(cid:10)(cid:10)(cid:10)(cid:10)\n(cid:15)\n\nb t\n\u22121 x t\n(x t x)\nb t b\n\u22121 x t b\n0\n\nb t x(x t x)\n\u22121\n\n\u22121 )\n\u22121\n\n( b x(x t x)\n\n\u22121 |\n(cid:16)\n\n(x t x)\n\n=\n\n=\n\n=\n\n\u2202y\n\n\u22121\n\n(x t x)\nin\u2212 p\n0\n\n(x t x)\n\n(cid:10)(cid:10)(cid:10)(cid:10)\n\n\u2202y\n\n\u2202(u,(cid:5)\u03b2\u03c8 )\n\n(cid:10)(cid:10)(cid:10)(cid:10) ,\n\n(12.11)\n\non substituting this and the normal densities of y and(cid:5)\u03b2\u03c8 into (12.11), we find\n(cid:19)\n2\u03c3 2 (y \u2212 x(cid:5)\u03b2\u03c8 )t\u03d2(y \u2212 x(cid:5)\u03b2\u03c8 )\nf (u; \u03c8) =\n\n(2\u03c0 \u03c3 2)(n\u2212 p)/2|x t\u03d2 x|1/2 exp\n\n|x t x|1/2|\u03d2|1/2\n\n\u2212 1\n\n(cid:6)\n\n,\n\n(12.12)\n\n "}, {"Page_number": 671, "text": "it is also called the\nresidual likelihood.\nrestricted maximum\nlikelihood estimation is\noften abbreviated to\nreml.\n\nor rigls algorithm.\n\n12.2 \u00b7 marginal inference\n\n659\n\nin which b does not appear. hence (12.12) is independent of the choice of b and\ntherefore of the linear combination of elements of (in \u2212 h)y used.\nexpression (12.12) is the marginal likelihood on which inference for \u03c8 is based.\nit is also known as the restricted likelihood, because its parameter space involves \u03c3 2\nand \u03c8 alone. the log restricted likelihood is\n\n2\u03c3 2 (y \u2212 x(cid:5)\u03b2\u03c8 )t\u03d2(y \u2212 x(cid:5)\u03b2\u03c8 )\n\n(cid:9)m(\u03c8, \u03c3 2) \u2261 1\n\n2 log|\u03d2| \u2212 1\n\u2212 n \u2212 p\n\n2 log|x t\u03d2 x| \u2212 1\nlog \u03c3 2,\n\nwhere \u03d2 and (cid:5)\u03b2\u03c8 depend on \u03c8. this can be maximized with respect to \u03c8 by a\n\n2\n\nnewton\u2013raphson procedure, leading to a restricted iterative generalized least squares\nalgorithm, or using the slower but more stable em algorithm. computational consid-\nerations are often important in practice, because the matrices involved can be very\nlarge.\n\nthe profile log likelihood for \u03c8 and \u03c3 2 based directly on the density of y is\n\n(12.13)\n\n(cid:9)p(\u03c8, \u03c3 2) \u2261 1\n2\n\nlog|\u03d2| \u2212 1\n\n2\u03c3 2 (y \u2212 x(cid:5)\u03b2\u03c8 )t\u03d2(y \u2212 x(cid:5)\u03b2\u03c8 ) \u2212 n\n2 log \u03c3 2 \u2212 1\n\n2\n\nlog \u03c3 2,\n2 log|x t\u03d2 x|, of\n\nfrom which (cid:9)m(\u03c8) differs by the addition of the term p\norder p. thus if the dimension of \u03b2 is large, the residual maximum likelihood estima-\ntor obtained from (cid:9)m(\u03c8) may differ substantially from the usual maximum likelihood\nestimator from (cid:9)p(\u03c8). when the term zb does not appear in (12.10), \u03d2 = in and the\nestimator of \u03c3 2 obtained by maximizing (12.13) is the usual unbiased quantity (exer-\ncise 12.2.4). thus the argument leading to (12.13) generalizes that in example 12.9.\nwe now illustrate how restricted likelihood can lead to modified inferences; see\n\nalso example 9.18.\n\n\uf8f1\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\n\n\uf8f6\n\uf8f8 ,\n\n\uf8eb\n\uf8ed yi1\n...\nyik\n\n\uf8f6\n\uf8f8 \u223c nk\n\nexample 12.11 (short time series) consider m independent individuals on each\nof which k measurements yi j are available from the distribution\n\u00b7\u00b7\u00b7 \u03c1k\u22121\n\u00b7\u00b7\u00b7 \u03c1k\u22122\n...\n...\n\u00b7\u00b7\u00b7\n1\n\n\uf8fc\uf8f4\uf8f4\uf8fd\n\uf8f4\uf8f4\uf8fe , i = 1, . . . ,m .\ndenote the matrix here by \u00010. its determinant is (1 \u2212 \u03c12)k\u22121 and its inverse has\ntridiagonal form\n\n\u03c1\n1\n...\n...\n\u03c1k\u22121 \u03c1k\u22122\n\n\uf8eb\n\uf8ed \u00b5i\n...\n\u00b5i\n\n\u03c3 2\n1 \u2212 \u03c12\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8\n\n1\n\u03c1\n\n\uf8eb\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\n\u2212\u03c1\n\n\u00b7\u00b7\u00b7\n...\n...\n...\n\n0\n\n0\n\n1\n0\n\u2212\u03c1 1 + \u03c12 \u2212\u03c1\n0\n...\n...\n0\n...\n...\n0\n... \u2212\u03c1 1 + \u03c12 \u2212\u03c1\n0\n\u00b7\u00b7\u00b7\n1\n0\n\n\u2212\u03c1\n...\n\n...\n\u2212\u03c1\n\n\u2212\u03c1\n\n0\n\n0\n\n\uf8f6\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n.\n\n1\n\n1 \u2212 \u03c12\n\n "}, {"Page_number": 672, "text": "660\n\n12 \u00b7 conditional and marginal inference\n\nthis formulation is typical of that arising in longitudinal data, where individuals are\nmeasured at successive time points. correlation among measurements on a single\nindividual is represented by \u00010, here corresponding to a first-order markov chain; see\nexample 6.13. to illustrate the argument above, we construct the profile and marginal\nlog likelihoods for the correlation parameter \u03c1, with m + 1 nuisance parameters\n\u00b51, . . . , \u00b5m , \u03c3 2.\n\nthe matrices x and \u03c3 2\u03d2\u22121 are of side mk \u00d7 m and mk \u00d7 mk respectively and\n\n\u03c3 2\u03d2\u22121 = \u03c3 2(1 \u2212 \u03c12)\n\n\u22121diag{\u00010, . . . , \u00010} ,\n\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\nx =\n\n1k\n0\n...\n0\n\n0\n1k\n...\n0\n\n\uf8f6\n\uf8f7\uf8f7\uf8f8 ,\n\n\u00b7\u00b7\u00b7\n0\n\u00b7\u00b7\u00b7\n0\n...\n...\n\u00b7\u00b7\u00b7 1k\n\nwhile the marginal log likelihood (12.13) when profiled gives\n\nand some algebra gives\n\n1\n\n2\n\nlog(1 \u2212 \u03c12), \u2212 1\n\n2 log|\u03d2| = m\nmore algebra establishes that(cid:5)\u00b5i = yi , so\n\n2 log|x t\u03d2 x| = \u2212 m\n(y \u2212 x(cid:5)\u03b2\u03c8 )t\u03d2(y \u2212 x(cid:5)\u03b2\u03c8 ) = m(cid:1)\n\n2\n\nti (yi , \u03c1),\n\ni=1\n\nlog [(1 \u2212 \u03c1){k \u2212 (k \u2212 2)\u03c1}] .\n\nwhere\n\nti (\u00b5, \u03c1) = k(cid:1)\n\nj=1\n\n(yi j \u2212 \u00b5)2 + \u03c12\n\nk\u22121(cid:1)\nj=2\n\n(yi j \u2212 \u00b5)2 \u2212 2\u03c1\n\nk\u22121(cid:1)\nj=1\n\n(yi j \u2212 \u00b5)(yi, j+1 \u2212 \u00b5).\n\nthus the profile log likelihood for \u03c1 is given by\n\n(cid:9)p(\u03c1) = m\n2\n\nlog(1 \u2212 \u03c12) \u2212 mk\n2\n(cid:6)\n\n(cid:9)m(\u03c1, \u03c3 2) = \u2212 m\n2\n\nmax\n\u03c3 2\n\nlog\n\np (\u03c1) = 1\nmk\n\nm(cid:1)\ni=1\n\nti (yi , \u03c1),\n\n\u2212 m(k \u2212 1)\n\nlog(cid:5)\u03c3 2\n\nm(\u03c1),\n\n2\n\nlog(cid:5)\u03c3 2\n\np (\u03c1), (cid:5)\u03c3 2\n(cid:19)\n\nk \u2212 (k \u2212 2)\u03c1\n1 + \u03c1\nm(\u03c1) >(cid:5)\u03c3 2\n\nm(\u03c1) = k(cid:5)\u03c3 2\n\nwhere(cid:5)\u03c3 2\nlike that in example 12.9. let(cid:5)\u03c1 and(cid:5)\u03c1m denote the estimators from maximizing the\n\np (\u03c1)/(k \u2212 1). as(cid:5)\u03c3 2\n\np (\u03c1), there is an upward bias correction\n\nusual and marginal likelihoods for \u03c1.\nfigure 12.3 shows the profile log likelihood and the profile marginal log likelihood\nfor 20 samples with m = 10, k = 5 and \u03c1 = 0.7. most of the ordinary maximum\nlikelihood estimates(cid:5)\u03c1 are much smaller than \u03c1, but the marginal maximum likelihood\nestimator(cid:5)\u03c1m is much less downwardly biased, as is confirmed by further simulation.\nsome values of(cid:5)\u03c1m equal the upper limit of \u03c1 = 1, so the small-sample distributions\nof (cid:5)\u03c1m and the marginal likelihood ratio statistic will be poorly approximated by\n\nasymptotic results.\n\n(cid:1)\n\ndespite its artificiality, this example illustrates how estimation using restricted like-\nlihood can give substantial bias corrections, and it is generally preferable to ordinary\nlikelihood for variance estimation in complex models.\n\n "}, {"Page_number": 673, "text": "12.2 \u00b7 marginal inference\n\n661\n\nfigure 12.3 inference\nfor correlation parameter\nin short time series with\n\u03c1 = 0.7. left: profile log\nlikelihoods for 20 samples\nconsisting of m = 10\nseries of length k = 5.\nright: profile marginal log\nlikelihoods for \u03c1, for the\nsame datasets. maximum\nmarginal likelihood\nestimators are much less\nbiased than are ordinary\nmaximum likelihood\nestimators.\n\nd\no\no\nh\n\ni\nl\n\ne\nk\n\ni\nl\n \n\ng\no\n\nl\n \n\ne\n\nl\ni\nf\n\no\nr\np\n\n0\n\n1\n-\n\n2\n-\n\n3\n-\n\n4\n-\n\n5\n-\n\n6\n-\n\nd\no\no\nh\n\ni\nl\n\ne\nk\n\ni\nl\n \n\ng\no\n\nl\n \nl\n\ni\n\na\nn\ng\nr\na\nm\ne\n\n \n\nl\ni\nf\n\no\nr\np\n\n0\n\n1\n-\n\n2\n-\n\n3\n-\n\n4\n-\n\n5\n-\n\n6\n-\n\n-1.0\n\n-0.5\n\n0.0\n\nrho\n\n0.5\n\n1.0\n\n-1.0\n\n-0.5\n\n0.5\n\n1.0\n\n0.0\n\nrho\n\nregression-scale model\nconsider the linear regression model y = x\u03b2 + e\u03c4 \u03b5, where the errors \u03b51, . . . , \u03b5n are\nindependent, the n \u00d7 p design matrix x has rank p and rows x t\nn, \u03c4 is scalar\nand \u03b2 is a p \u00d7 1 vector of parameters. exact results for this linear regression model\nwere discussed in chapters 8 and 9 for normal errors, but are typically unavailable\notherwise. accurate approximations to distributions needed for inference may be\nfound, however. let d be the negative log density of \u03b5, and note that the log likeli-\nhood is\n\n, . . . , x t\n\n1\n\n(cid:9)(\u03b2, \u03c4 ; y) = \u2212n\u03c4 \u2212 n(cid:1)\n\nd\n\n(cid:7)\n\n\u2212\u03c4 (y j \u2212 x t\n\nj\n\ne\n\n\u03b2)\n\n(cid:8)\n\n.\n\nj=1\n\nthe minimal sufficient statistic is typically the full set of responses y1, . . . ,y n. dif-\n\nferentiation of the log likelihood shows that the maximum likelihood estimators (cid:5)\u03b2\nand(cid:5)\u03c4 are determined by the score equations\n\n(cid:9)(cid:17)\n\n(cid:18) = 0,\n\nx j d\n\na j\n\nn(cid:1)\nj=1\n\nn \u2212 n(cid:1)\n\nj=1\n\n(cid:9)(cid:17)\n\n(cid:18) = 0,\n\na j d\n\na j\n\n(12.14)\n\n(cid:9)\n\nwhere d\n\n(u) isthe derivative of d(u) with respect to u, and a j = e\n\n(cid:5)\u03b2),\n\u2212(cid:5)\u03c4 (y j \u2212 x t\nfor j = 1, . . . ,n . evidently a = (a1, . . . , an)t is distribution constant, and as (cid:5)\u03b2\nand(cid:5)\u03c4 are functions of the minimal sufficient statistic, a is in general also ancillary.\nits distribution is degenerate, with p + 1 constraints imposed by (12.14), so just\nn \u2212 p \u2212 1 of the a j are needed to determine them all. let a0 denote such a subset,\nsay, a0 = (a1, . . . , an\u2212 p\u22121).\nwe now consider how conditional tests and confidence intervals may be derived for\ndensity that depends only on \u03b21, byusing the joint density of (cid:5)\u03b2 and(cid:5)\u03c4 conditional on\nan element of \u03b2, say \u03b21 without loss of generality. the idea is to construct a marginal\n\nj\n\na to obtain\n\nf (u1, u2 | a; \u03b2, \u03c4 ) = f ((cid:5)\u03b2,(cid:5)\u03c4 | a; \u03b2, \u03c4 )\n\u2212(cid:5)\u03c4 ((cid:5)\u03b21 \u2212 \u03b21), u2 denotes the p \u00d7 1 vector with elements (cid:5)\u03c4 \u2212 \u03c4 and\nwhere u1 = e\n\u2212(cid:5)\u03c4 ((cid:5)\u03b2\u22121 \u2212 \u03b2\u22121), and \u03b2\u22121 denotes (\u03b22, . . . , \u03b2 p)t. weshall see that u1 and u2 are\ne\n\n\u2202(u1, u2)\n\n(cid:10)(cid:10)(cid:10)(cid:10) \u2202((cid:5)\u03b2,(cid:5)\u03c4 )\n\n(cid:10)(cid:10)(cid:10)(cid:10) ,\n\n "}, {"Page_number": 674, "text": "662\n\n12 \u00b7 conditional and marginal inference\n\npivots, and that the distribution of u1 conditional on the ancillary statistic a forms\nthe basis of conditional tests and confidence intervals for \u03b21. these are readily obtained\nusing p\n\n-type approximations.\n\n\u2217\n\n(cid:7)\n\n\u2212(cid:5)\u03c4 (\u03b2 \u2212(cid:5)\u03b2), \u03c4 \u2212(cid:5)\u03c4 ; a\n\n(cid:8) \u2212 n(cid:5)\u03c4 ;\n\nas a preliminary step, we write the log likelihood as\n\n(cid:9)(\u03b2, \u03c4 ; y) = (cid:9)\n\nits maximized value is (cid:9)((cid:5)\u03b2,(cid:5)\u03c4 ; a) = \u2212n(cid:5)\u03c4 \u2212(cid:4)\n(cid:7)\n\nha(u1, u2) = (cid:9)(0, 0; a) \u2212 (cid:9) (u1, u2; a)\n\ne\n\n\u2212(cid:5)\u03c4 (\u03b2 \u2212(cid:5)\u03b2), \u03c4 \u2212(cid:5)\u03c4 ; a\n\ne\n\n(cid:8)\n\n,\n\n= (cid:9)(0, 0; a) \u2212 (cid:9)\n\nn\nj=1 d(a j ). for later use define\n\n(12.15)\n\nand observe that its second derivative matrix with respect to (u1, u2) has determinant\nof form e\n\n\u22122(cid:5)\u03c4 ha(u1, u2), and\n\n\u2202ha(u1, u2)\n\n= \u2202(cid:9)(\u03b2, \u03c4 ; y)\n\n.\n\n\u2202\u03b21\n\n\u2202u1\n\nwe now compute the density of ((cid:5)\u03b2,(cid:5)\u03c4 ) conditional on a. an extension of the\nargument giving (5.19) shows that the jacobian for the transformation y (cid:10)\u2192 ((cid:5)\u03b2,(cid:5)\u03c4 , a0)\nmay be written e(n\u2212 p)(cid:5)\u03c4 j (a), so\n\nf ((cid:5)\u03b2,(cid:5)\u03c4 , a0; \u03b2, \u03c4 ) = j (a) exp{(n \u2212 p)(cid:5)\u03c4 + (cid:9)(\u03b2, \u03c4 ; y)}\n\n(cid:13)\u2212 p(cid:5)\u03c4 + (cid:9)\n\u2212(cid:5)\u03c4 (\u03b2 \u2212(cid:5)\u03b2), \u03c4 \u2212(cid:5)\u03c4 ; a\n(cid:8)(cid:14)\n(cid:7)\n(cid:13)\u2212 p(cid:5)\u03c4 + (cid:9)\n\u2212(cid:5)\u03c4 (\u03b2 \u2212(cid:5)\u03b2), \u03c4 \u2212(cid:5)\u03c4 ; a\n(cid:13)\u2212 p(cid:5)\u03c4 + (cid:9)\n(cid:7)\n(cid:8)(cid:14)\ne\u2212(cid:5)\u03c4 (\u03b2 \u2212(cid:5)\u03b2), \u03c4 \u2212(cid:5)\u03c4 ; a\nd(cid:5)\u03b2d(cid:5)\u03c4\nthe domain of integration being ir p+1. we now change variables in the integral from\n((cid:5)\u03b2,(cid:5)\u03c4 ) to (u1, u2), with jacobian exp( p(cid:5)\u03c4 ). thus the integral may be written\n\nf ((cid:5)\u03b2,(cid:5)\u03c4 | a; \u03b2, \u03c4 ) =\n\n= j (a) exp\n\nhence\n\n(cid:8)(cid:14)\n\nexp\n\nexp\n\n(cid:7)\n\n(cid:12)\n\ne\n\ne\n\n,\n\n.\n\n(cid:20)\n\nexp{(cid:9)(0, 0; a) \u2212 ha(u1, u2)} du1du2\n\nand this equals\n\nby laplace approximation, the o(n\n\n(2\u03c0)( p+1)/2\ndensity f ((cid:5)\u03b2,(cid:5)\u03c4 | a; \u03b2, \u03c4 ) equals\n(cid:10)(cid:10)(cid:10)(cid:10) \u2202 2ha(0, 0)\n\n\u2212( p+1)/2\n\nc(a)(2\u03c0)\nwhere c(a) = 1 + o(n\nwe have\n\n(cid:10)(cid:10)(cid:10)(cid:10) \u2202 2ha(0, 0)\n\n\u2202u\u2202ut\n\n(cid:10)(cid:10)(cid:10)(cid:10)\u22121/2\n\n(cid:7)\n\ne(cid:9)(0,0;a)\n\n(cid:8)\n\n1 + o(n\n\n\u22121)\n\n\u22121) term depending only on a, and the conditional\n(cid:14)\n(cid:8) \u2212 (cid:9)(0, 0; a)\n\u2202u\u2202ut\n\u22121) depends only on a. onchanging variables to u1 and u2,\n\n\u2212(cid:5)\u03c4 (\u03b2 \u2212(cid:5)\u03b2), \u03c4 \u2212(cid:5)\u03c4 ; a\n\n(cid:10)(cid:10)(cid:10)(cid:10)1/2(cid:5)\u03c4\u2212 p exp\n\n(cid:7)\n\n(cid:13)\n\ne\n\n(cid:9)\n\n,\n\nf (u1, u2 | a; \u03b2, \u03c4 ) = c(a)(2\u03c0)\n\n\u2212( p+1)/2\n\nexp{\u2212ha(u1, u2)} ,\n\n(cid:10)(cid:10)(cid:10)(cid:10) \u2202 2ha(0, 0)\n\n\u2202u\u2202ut\n\n(cid:10)(cid:10)(cid:10)(cid:10)1/2\n\n "}, {"Page_number": 675, "text": "12.2 \u00b7 marginal inference\n\n663\n\n\u2217\n\nwhere r\n\n(cid:10)(cid:10)(cid:10)(cid:10)\u22121/2\n\u2212(cid:5)\u03c4 ((cid:5)\u03b21 \u2212 \u03b20\n\n,\n\nand as the the right-hand side does not depend on the parameters, u1(\u03b21) and\nu2(\u03b2\u22121, \u03c4 ) are pivots. note that the inferences below are not parametrization in-\nh to quadratic; taking(cid:5)\u03c4 \u2212 \u03c4 should give better results than the more obvious pivot\nvariant, because the accuracy of laplace approximation depends on the closeness of\n(cid:5)\u03c3 /\u03c3.\n\nconsider testing the hypothesis that \u03b21 takes value \u03b20\n1 . ifso, the observed value of\n= e\nu1 is u0\n(cid:20)\n1\n(cid:17)\nu1 \u2264 u0\n\u2212\u221e\n\n(cid:10)(cid:10)\u2202 2ha(0, 0)/\u2202u\u2202ut\n\n1 ), with corresponding tail probability\n\n\u2212(cid:5)\u03c4 ((cid:5)\u03b21 \u2212 \u03b20\n| a = a\n\n(cid:18) = c(a)\n\n\u2212ha(u1,u2),\n\n(cid:10)(cid:10)1/2\n\ndu2e\n\ndu1\n\n(cid:20)\n\npr\n\nu0\n1\n\n1\n\nthe inner integral being over ir p. this expression has form (11.32), so\n\n1\n\npr\n\n(cid:7)\n\n(2\u03c0)( p+1)/2\n(cid:18) = \u0001\n(cid:17)\nu1 \u2264 u0\n| a = a\n1 ) + r(\u03b20\n1 ) = r(\u03b20\n\u22121 log{v(\u03b20\n(cid:7)\n(\u03b20\n1 )\n1 ) = sign(u0\n2ha(u0\nr(\u03b20\n1)\n1\n1 ) = \u2202ha(u0\n1\n\u2202u1\n\n(cid:10)(cid:10)(cid:10)(cid:10) \u2202 2ha(u0\n\n, \u02dcu20)\n\n, \u02dcu20)\n\nv(\u03b20\n\n1 )/r(\u03b20\n\n\u2202u2\u2202ut\n2\n\n1 + o(n\n\n(cid:8)(cid:7)\n(\u03b20\n1 )\n1 )}, and\n(cid:8)1/2 ,\n(cid:10)(cid:10)(cid:10)(cid:10)1/2(cid:10)(cid:10)(cid:10)(cid:10) \u2202 2ha(0, 0)\n\n, \u02dcu20)\n\n\u2202u\u2202ut\n\n\u2217\n\nr\n\n1\n\n\u22121)\n\n(cid:8)\n\n,\n\n(12.16)\n\n,\n\n(12.17)\n\nwhere \u02dcu20 is the value of u2 that maximizes ha(u0\n1\nit is straightforward to check that in terms of the log likelihood and its derivatives,\n\n, u2) with u0\n1\n\n1 ) fixed.\n\n1 ) = sign((cid:5)\u03b21 \u2212 \u03b20\n1 ) = \u2202(cid:9)((cid:5)\u03b20,(cid:5)\u03c4 0; y)\n\n1 )\n\nr(\u03b20\n\nv(\u03b20\n\n\u2202\u03b21\n\n(cid:13)\n(cid:7)\n(cid:9)((cid:5)\u03b2,(cid:5)\u03c4 ; y) \u2212 (cid:9)((cid:5)\u03b20,(cid:5)\u03c4 0; y)\n(cid:10)(cid:10)j\u03bb\u03bb((cid:5)\u03b20,(cid:5)\u03c4 0; y)\n(cid:10)(cid:10)1/2\n(cid:10)(cid:10)j ((cid:5)\u03b20,(cid:5)\u03c4 0; y)\n(cid:10)(cid:10)1/2\n\n= e\n(cid:8)(cid:14)1/2 ,\n(cid:10)(cid:10)1/2\n(cid:10)(cid:10)j ((cid:5)\u03b20,(cid:5)\u03c4 0; y)\n(cid:10)(cid:10)j ((cid:5)\u03b2,(cid:5)\u03c4 ; y)\n(cid:10)(cid:10)1/2\n\n\u00d7\n\n2\n\nsay, where(cid:5)\u03b20 = (\u03b20\n,(cid:5)\u03b20\u22121),(cid:5)\u03b20\u22121 and(cid:5)\u03c4 0 are the maximum likelihood estimates of \u03b2\u22121\nand \u03c4 when \u03b21 = \u03b20\n1\n1 , j (\u03b2, \u03c4 ; y) isthe observed information matrix for \u03b2 and \u03c4 , and\nj\u03bb\u03bb(\u03b2, \u03c4 ; y) isthe observed information matrix corresponding to \u03bb = (\u03b2\u22121, \u03c4 ) only.\n1 ) = 0 when \u03b20\n1 ) = v(\u03b20\nnote that r(\u03b20\n1 ) has a finite limit\nas \u03b20\n1\n\n=(cid:5)\u03b21; itturns out that r\n\n\u2192 0.\n\n(\u03b20\n\nour argument has established that the conditional tail probability (12.16) depends\non the signed likelihood ratio statistic r(\u03b20\n1 ) for\ntesting the hypothesis \u03b21 = \u03b20\n1 . expression (12.17) may be written as a product, \u03b3 c,\nsay, where the first term is a score statistic standardized by its standard error and the\nsecond contains information matrix determinants. then r\n1 ) may be decomposed as\n(12.18)\n\n1 ) and a modified score statistic v(\u03b20\n\n\u22121 log c = r + rinf + rnp,\n\n\u22121 log(\u03b3 /r) + r\n\n\u2217 = r + r\n\n(\u03b20\n\n\u2217\n\n\u2217\n\nr\n\n1\n\nsay, where dependence on \u03b20\n1 has been suppressed. the discussion around (12.9)\nsuggests that rinf should be regarded as a correction that improves the normal ap-\nproximation to the distribution of the signed likelihood ratio statistic. the presence\nof \u03bb introduces the term rnp, large values of which indicate that strong correction for\nnuisance parameters is required. although rinf is typically small in applications, the\nnuisance parameter correction rnp can be substantial.\n\n "}, {"Page_number": 676, "text": "figure 12.4\nsmall-sample inference\non parameter \u03b21 for\npartial turnkey guarantee\nfor nuclear plant data,\nusing regression-scale\nmodel with t5 errors. left:\napproximate pivots r(\u03b21)\n(solid), r\n\nand ((cid:5)\u03b21 \u2212 \u03b21)/v 1/2\n\n(\u03b21) (heavy),\n\n\u2217\n\n11\n\n(dashes) as functions of\n\u03b21. horizontal dotted lines\nare at 0 and \u00b11.96, so\ntheir intersections with the\ndiagonal lines show the\nlimits of approximate 0.95\nconfidence intervals.\nright: small-sample\ncorrections rnp (solid) and\nrinf (dashes) as functions\nof \u03b21. the horizontal lines\nat \u00b10.2 give acrude rule\nof thumb for \u2018large\u2019\ncorrections.\n\n664\n\nt\n\no\nv\np\n\ni\n\n \n\ni\n\nt\n\ne\na\nm\nx\no\nr\np\np\na\n\n12 \u00b7 conditional and marginal inference\n\n3\n\n2\n\n1\n\n0\n\n1\n-\n\n2\n-\n\n3\n-\n\nn\no\n\ni\nt\nc\ne\nr\nr\no\nc\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n2\n\n.\n\n0\n-\n\n.\n\n4\n0\n-\n\n-0.6\n\n-0.4\n\n-0.2\n\n0.0\n\n-0.6\n\n-0.4\n\n-0.2\n\n0.0\n\nbeta1\n\nbeta1\n\nthe limits of an equi-tailed (1 \u2212 2\u03b1) conditional confidence interval for \u03b21 are the\nvalues \u03b2+\n\n1 , \u03b2\u2212\n\n(cid:7)\n\n1 that satisfy\nu1 \u2264 e\n\npr\n\n\u2212(cid:5)\u03c4 ((cid:5)\u03b21 \u2212 \u03b21)\n\n(cid:10)(cid:10) a\n\n(cid:8) .= \u0001\n\n(cid:7)\n\n(cid:8) = \u03b1, 1 \u2212 \u03b1.\n\n\u2217\n\n(\u03b21)\n\nr\n\n(\u03b21) with r(\u03b21)\nthe likelihood ratio confidence limits are obtained by replacing r\nin this expression, so computation of v(\u03b21) for use in r\n(\u03b21) isthe only addition\nneeded for small-sample conditional inference. routines for large-sample likeli-\nhood inference on the model using r(\u03b21) compute most of the quantities involved\nfor small-sample conditional inferences using r\n(\u03b21), so rather little extra work is\nneeded.\n\n\u2217\n\n\u2217\n\n\u2217\n\nexample 12.12 (nuclear plant data) to illustrate the effect of these small-sample\nmodifications, we fit the model with six covariates chosen in example 8.31 to the\ndata in table 8.13. we focus on the effect of the partial turnkey guarantee, pt, the\ncoefficient of which under the normal model is \u22120.266 with standard error 0.144,\ngiving an exact 0.95 confidence interval (\u22120.46, 0.01). the evidence for inclusion\nof this effect is somewhat marginal under the normal model, and it is interesting to\nsee the effect of using a t5 error distribution instead. computations that generalize\nthose in example 12.7 lead to the results shown in figure 12.4, the left panel of which\nshows how r(\u03b21) and r\n(\u03b21) depend on the parameter \u03b21 for the partial turnkey effect.\nthe 0.95 confidence intervals based on these pivots are respectively (\u22120.47,\u22120.07)\nand (\u22120.48,\u22120.03), while that based on normal approximation to the maximum\nlikelihood estimator(cid:5)\u03b21 is (\u22120.47,\u22120.08). the more accurate interval based on the\n\n\u2217\n\nsmall-sample approximation is rather wider than the others, and the right panel of the\nfigure shows that this is primarily due to the nuisance parameter adjustment.\n\nnone of the intervals differs much from that found with normal errors, however,\ngiving some reassurance that conclusions found using the normal model are valid\n(cid:1)\nmore broadly, at least for this example.\n\n "}, {"Page_number": 677, "text": "12.3 \u00b7 conditional inference\n\n665\n\nexercises 12.2\n(cid:4)\n1\n\nsuppose that y1, . . . ,y n are independent poisson variables with means \u03c8 \u03c0 j , where 0 <\n\u03c0 j = 1. find a marginal likelihood for \u03c8 based on y1, . . . ,y n, and show\n\u03c0 j < 1 and\nthat no information about \u03c8 is lost by using the marginal likelihood rather than the full\nlikelihood.\nverify the argument in example 12.10 by writing down the partial likelihood for obser-\n+\nfrom the proportional hazards model, and then\nvations 0 < y4 < y\n1\nderiving it by the reasoning in the example.\nuse the spectral decomposition to show that an n \u00d7 (n \u2212 p) matrix b exists such that\nb t b = in \u2212 h, and b b t = in\u2212 p, where h = x(x t x)\n\u22121 x t is the projection matrix for\na linear model with n \u00d7 p design matrix x of rank p.\nshow that the maximum likelihood estimator of \u03c3 2 based on (12.13) is given by\n\n+\n< y5 < y\n2\n\n+\n< y\n3\n\n(y \u2212 x(cid:5)\u03b2\u03c8 )t\u03d2(y \u2212 x(cid:5)\u03b2\u03c8 ), (cid:5)\u03b2\u03c8 = (cid:17)\n\nx t\u03d2 x\n\n(cid:18)\u22121 x t\u03d2 y.\n\n(cid:5)\u03c3 2\n\nm\n\n= 1\nn \u2212 p\n\nestimator of \u03c3 2 is (y \u2212 x(cid:5)\u03b2)t(y \u2212 x(cid:5)\u03b2)/(n \u2212 p).\ndeduce that when zb does not appear in (12.10), the restricted maximum likelihood\nfind marginal likelihoods for \u03c3 2 and for \u03c8 = (\u03b20, \u03c3 2) inexample 8.10.\ncheck the details of example 12.11.\nverify the p\n\u2212( p+1)/2.\n(2\u03c0)\n(a) express the quantities r(\u03b20\nhood and its derivatives.\n(b) compute the modified signed likelihood ratio statistic for inference on \u03c4 .\n(c) find r\n\n(\u03b21) for the regression-scale model when d(u) = u2/2.\n\nformula (12.7) for the regression-scale model, replacing (2\u03c0)\n\n1 ) and v(\u03b20\n\n1 ) defined after (12.16) in terms of the log likeli-\n\n\u22121/2 by\n\n\u2217\n\n\u2217\n\n2\n\n3\n\n4\n\n5\n6\n7\n\n8\n\n12.3 conditional inference\n12.3.1 exact conditioning\nthus far this chapter has focused on the use of conditioning to ensure the relevance\nof tests and confidence intervals. in exponential family models there is typically no\nancillary statistic and conditioning has the somewhat different purpose of removing\na nuisance parameter from consideration; see sections 10.4.2, 10.5, and 10.8.2. if the\nmodel density may be factorized as\n\nf (y; \u03c8, \u03bb) \u221d f (t1 | t2; \u03c8) f (t2; \u03c8, \u03bb),\n\n(12.19)\n\nthen although both terms on the right contain information on \u03c8, itmay not be worth-\nwhile to try and extract it from the second term. moreover in section 7.3.3 we saw\nthat similar critical regions for tests on \u03c8 must be based on the conditional density\nof t1 given t2. these considerations suggest that we restrict consideration to this\ndensity, which we treat as a conditional likelihood for \u03c8. exact inference is typically\ninfeasible, however.\n\nexample 12.13 (logistic regression) let y1, . . . ,y n be independent binary vari-\nables having p \u00d7 1 covariate vectors x j and satisfying a logistic regression model.\n\n "}, {"Page_number": 678, "text": "12 \u00b7 conditional and marginal inference\n\n666\n\nthe minimal sufficient statistic\n\n(cid:4)\n\n#\npr(s1 = s1, . . . , sp = s p; \u03b2) = c(s1, . . . ,s p) exp\n\n(cid:4)\n\nx j y j = s = (s1, . . . , sp)t has joint density\n(cid:18)\n\n(cid:17)\n1 + exp(x t\n\n\u03b2)\n\nj\n\ns1\u03b21 + \u00b7\u00b7\u00b7 +s p\u03b2 p\n\n$\n\n,\n\nn\nj=1\n\nfound by summing the joint density (10.23) of the y j over all c(s1, . . . ,s p) binary\nsequences of length n that yield the same value of s as did the data. if \u03b2 p is taken\nas the parameter of interest and the other \u03b2 j are treated as nuisance parameters, then\nthey do not appear in the conditional density\n\npr(sp = s p | s1 = s1, . . . , sp\u22121 = s p\u22121; \u03b2 p) =\n\n(cid:4)\n\nc(s1, . . . ,s p)es p \u03b2 p\nc(s1, . . . ,s p\u22121, u)eu\u03b2 p\n\n,\n\nwhere the sum is over the possible values of u for which s1, . . . ,s p\u22121 are fixed.\nexact tests and confidence intervals may be obtained by adapting the argument on\npage 495, but rely on ready computation of the combinatorial coefficients. in principle\nthese may be obtained by considering the coefficients of w s1\np in the expansion\n1\nof the generating function\n\n\u00b7\u00b7\u00b7w s p\n\n1 + w x j1\n\n1\n\n\u00b7\u00b7\u00b7w x j p\n\np\n\n(cid:18)\n\n,\n\n(cid:17)\n\nn(cid:9)\nj=1\n\nwhere x jr is the rth element of x j . recourse to computer algebra or special algorithms\nis needed for all but the simplest problems, however, and typically the computational\n(cid:1)\nburden puts exact inference out of reach.\n\nin subsequent sections we discuss accurate analytical approximations to exact con-\nditional inferences, but we first illustrate how markov chain monte carlo simulation\n(section 11.3.3) may be used to explore a conditional sample space.\nexample 12.14 (several 2 \u00d7 2 tables) consider n independent 2 \u00d7 2 tables con-\ntaining the numbers of successes r1 j and r0 j in m1 j and m0 j independent bernoulli\ntrials with success probabilities\n\nthe overall joint density is\n\n\u03c01 j = e\u03bb j+\u03c8 j\n1 + e\u03bb j+\u03c8 j\n(cid:16)(cid:15)\n(cid:15)\nn(cid:9)\nj=1\n\nm1 j\nr1 j\n\nand \u03c00 j = e\u03bb j\n1 + e\u03bb j\n(cid:16)\n\n(cid:18)m1 j\ner1 j (\u03bb j+\u03c8 j )\n1 + e\u03bb j+\u03c8 j\n\n(cid:17)\n\n(cid:17)\n\nm0 j\nr0 j\n\nj = 1, . . . ,n .\n\n,\n\n(cid:18)m0 j\n\n.\n\ner0 j \u03bb j\n1 + e\u03bb j\n\nsuppose we wish to test whether the log odds ratios are equal for each table, that is\n\u03c81 = \u00b7\u00b7\u00b7 = \u03c8n = \u03c8, where \u03c8 is unknown. let w = w(r) be astatistic constructed\nto test this against the alternative of unequal \u03c8 j . under the null hypothesis the statis-\ntics s j = r1 j + r0 j and t = r11 + \u00b7\u00b7\u00b7 + r1n are associated with \u03bb j and \u03c8, and\nas we wish to test homogeneity of the log odds ratios regardless of the values of\n\u03bb1, . . . , \u03bbn, \u03c8, weshould use the conditional distribution of w given the values of\n\n "}, {"Page_number": 679, "text": "12.3 \u00b7 conditional inference\n\n667\n\ns1, . . . , sn, t that were actually observed, namely s1, . . . ,s n, t. it isstraightforward\nto establish that\n\npr(r | s, t ) =\n\n(cid:17)\n\n(cid:11)\n(cid:4)(cid:11)\nn\nj=1\nn\nj=1\n\n(cid:18)\n\n(cid:18)(cid:17)\n(cid:18)(cid:17)\nm0 j\ns j\u2212r1 j\nm2 j\ns j\u2212u j\n\nm1 j\nu j\n\nm1 j\nr1 j\n\n(cid:17)\n\n(cid:18) = c\n\n\u22121\n\nn(cid:9)\nj=1\n\nc j (r1 j ),\n\n(12.20)\n\nsay, where the sum is over the set\n\nu = (cid:7)\n\n(u1, . . . ,u n) :u j,\u2212 \u2264 u j \u2264 u j,+, u j \u2208 zz, u1 + \u00b7\u00b7\u00b7 +u n = t\n\n(cid:8)\n\n,\n\nwith u j,\u2212 = max(0, s j \u2212 m0 j ) and u j,+ = min(m1 j , s j ). if the 2 \u00d7 2 tables were\nstacked on top of one another, then this conditioning would amount to fixing all three\nmargins of the stack. we would like to use (12.20) to calculate the tail probability\npr{w(r) \u2265 w | s = s, t = t}, but enumeration of u in order to find the normalising\nconstant in (12.20) is typically difficult. a simplifying feature is that any 2 \u00d7 2 table\nfor which s j equals zero or m0 j + m1 j is conditionally constant and can be ignored.\none possibility is to use the metropolis\u2013hastings algorithm. the idea is to update\nthe stack of 2 \u00d7 2 tables with contents r to have contents r\n, while holding its margins\nconstant. although it is hard to simulate directly from (12.20), it is straightforward to\ngenerate from the conditional distribution of r1i given r1 j + r1i , which is univariate\nwith density\n\n(cid:9)\n\npr(r1i = r | r1 j + r1i = v) = ci (r)c j (v \u2212 r)\nci (u)c j (v \u2212 u)\n\n(cid:4)\n\n,\n\nwhere the sum is over values of r1i = u for which r1 j + r1i = v is possible. the\nalgorithm starts with original data r and repeats the following steps for k = 1, . . . , k .\n1. for l = 1, . . . , l,\n\n(cid:9)\n\nr select two tables in the current stack at random, say i and j;\nr generate proposal data r\n= r1i + r1 j \u2212 r\nwith probability\n\nby leaving the remaining 2 \u00d7 2 tables intact and\n1i from the conditional distribution of r1i given r1i + r1 j ;\n(cid:9)\n(cid:9)\n1i .\n\ngenerating a value r\n(cid:9)\nthen set r\nr set r \u2190 r\n1 j\n(cid:9)\n\n(cid:6)\n\n(cid:19)\n\np = min\nthe estimated p-value is then {(cid:4)\n\n2. calculate w\n\n\u2217\nk\n\ni (w\n\n= w(r) based on the current data r.\n\n(cid:9)\n\n)q(r | r\n(cid:9)\n)\n\u03c0(r\n\u03c0(r)q(r(cid:9) | r)\n\n1,\n\nobserved value of w(r).\n\nthe target density is\n\n\u2217\nk\n\n\u2265 wobs) + 1}/(k + 1), where wobs is the\n(cid:9)\n\n.\n\n(12.21)\n\nand the transition density for the markov chain is\n\n(cid:9) | r) \u221d ci (r\n\n1i )c j (r1i + r1 j \u2212 r\n(cid:9)\n\n(cid:9)\n1i )\n\nq(r\n\nci (u)c j (r1i + r1 j \u2212 u),\n\n\u03c0(r) \u221d\n\nj\n\nc j (r1 j )\n(cid:1)\n\nu\n\n "}, {"Page_number": 680, "text": "668\n\nf\nd\np\n\n4\n0\n\n.\n\n0\n\n3\n0\n\n.\n\n0\n\n2\n0\n\n.\n\n0\n\n1\n0\n\n.\n\n0\n\n0\n\n.\n\n0\n\n12 \u00b7 conditional and marginal inference\n\nfigure 12.5 simulation\napproximation to\nconditional distribution of\ntest statistic w for\nhomogeneity of\nparameters \u03c8 j for the\nulcer data (table 10.11).\nthe line is the asymptotic\n\u03c7 2\n39 density.\n\n20\n\n40\n\n60\n\n80\n\nlikelihood ratio statistic\n\nso the ratio in (12.21) is\n\n(cid:9)\n\n)q(r | r\n(cid:9)\n)\n\u03c0(r\n\u03c0(r)q(r(cid:9) | r)\n(cid:9)\n\n(cid:9)\n1i )c j (r\n\n= ci (r\n\n(cid:9)\n1 j )ci (r1i )c j (r1 j )\n(cid:9)\n1 j )\n\n(cid:9)\n1i )c j (r\n\n= 1.\n\nci (r1i )c j (r1 j )ci (r\n\nthus the proposal r\n\nis always accepted.\n\n\u2217\n\nin order to assess whether the parameters \u03c8 j for recurrent bleeding vary across the\n41 studies in table 10.11, we apply this algorithm to the 40 non-degenerate tables;\nthe last one is conditionally constant. we take w to be the likelihood ratio statistic for\ntesting the null hypothesis. its observed value is w = 177.6 on 39 degrees of freedom,\nwhich is highly significant relative to its asymptotic \u03c7 2\nof\ncalculated every l = 100 steps using the algorithm above were uncorrelated, and\nw\nare compared with the asymptotic distribution in figure 12.5. the evidence that the\n\u03c8 j vary across the tables is overwhelming relative to either distribution, but as the\ndistributions are rather different, conflict between asymptotic and exact p-values could\narise for other such sets of data; then the p-value based on w\nwould be preferred.\nsuch a p-value is called monte carlo exact: it would be exact if the number of monte\n(cid:1)\ncarlo replicates was infinite.\n\n39 distribution. values w\n\n\u2217\n\n\u2217\n\nalgorithms of this type provide powerful tools for testing hypotheses in contingency\ntables with low counts, and can be adapted to provide monte carlo approximations to\nconditional likelihoods. a difficulty with exact conditioning, however, is that in some\ncases the inference depends very sharply on the conditioning event, giving likelihoods,\nsignificance levels and so forth that are highly sensitive to the exact data values. this\nraises the question whether some form of approximate conditioning would be more\nstable. below we outline how conditional inferences in exponential families may be\napproximated using saddlepoint methods.\n\n12.3.2 saddlepoint approximation\nsaddlepoint approximation to density and distribution functions is the basis of many\nhighly accurate small-sample procedures. in this section we describe informally the\n\n "}, {"Page_number": 681, "text": "12.3 \u00b7 conditional inference\n\n669\n\nfigure 12.6 saddlepoint\napproximations to the\ndensity and distribution\nfunctions of an average of\nn u (\u22121, 1) variables. left\npanel: exact density\nfunctions (solid) and their\nbasic (dots) and\nnormalized (dashes)\napproximations for\nn = 2, 5; the peakier\ncurves are for n = 5.\nright panel: exact\ndistribution function\n(solid) and its\napproximation (dots) for\nn = 2.\n\nf\nd\np\n\n5\n.\n1\n\n0\n.\n1\n\n5\n.\n0\n\n0\n0\n\n.\n\nf\nd\nc\n\n0\n.\n1\n\n8\n.\n0\n\n6\n.\n0\n\n4\n.\n0\n\n2\n.\n0\n\n0\n0\n\n.\n\n-1.0\n\n-0.5\n\n0.0\n\n0.5\n\n1.0\n\n-1.0\n\n-0.5\n\nx\n\n0.5\n\n1.0\n\n0.0\n\nx\n\nunderlying ideas. initially we state the basic approximations, leaving their justification\nto the end of the section.\n\n(cid:6)\n\n.=\n\n(cid:19)1/2\n\nlet x denote the average of a random sample of continuous scalar random variables\nx1, . . . , xn, each having cumulant-generating function k (u). then the saddlepoint\napproximation to the density of x at x is\n\nn\n\nexp [n {k (\u02dcu) \u2212 \u02dcux}] ,\n\nf x (x)\n\n2\u03c0 k (cid:9)(cid:9)(\u02dcu)\n\n(12.22)\nwhere \u02dcu = \u02dcu(x), known as the saddlepoint, is the unique value of u satisfying the sad-\n(u) are the first and second derivatives\ndlepoint equation k\nof k (u) with respect to u. there is a corresponding approximation to the cumulative\ndistribution function of x, namely\n\n(u) and k\n\n(u) = x, and k\n(cid:17)\n\n(cid:7)\n\n(cid:9)(cid:9)\n\n(cid:9)\n\n(cid:9)\n\n(cid:18) .= \u0001\n\n(cid:8)\npr\n\u22121 log{v(x)/r(x)} depends on\n\nx \u2264 x\n\n(x)\n\n\u2217\n\nr\n\n,\n\n\u2217\n\nwhere r\n\n(x) = r(x) + r(x)\nr(x) = sign(\u02dcu) [2n {\u02dcux \u2212 k (\u02dcu)}]1/2 ,\n\nv(x) = \u02dcu\n\n(12.23)\n\n(cid:7)\n\n(cid:8)1/2 .\n\n(cid:9)(cid:9)\n\n(\u02dcu)\n\nnk\n\ncalculation of (12.22) and (12.23) requires knowledge of k (u) and computation of \u02dcu\nfor each x of interest, but such approximations are often extremely accurate far into\nthe tails of the density of x. amore accurate density approximation can be obtained\nby renormalizing (12.22), that is, dividing by its integral, which is usually obtained\nnumerically.\n\nexample 12.15 (uniform distribution)\nif x has the uniform distribution on\n(\u22121, 1), then k (u) = log{sinh(u)/u}. the left panel of figure 12.6 compares the\nexact density function of x with its saddlepoint approximations when n = 2 and\nn = 5. numerical integrals of the density approximations are 1.097 and 1.034, and\nthe renormalized densities are also shown; that for n = 5 seems essentially exact.\nthe distribution function approximation (12.23) is very accurate even in the ex-\ntreme situation n = 2, although (12.22) fails to capture the cusp in the density at the\n(cid:1)\norigin.\n\n "}, {"Page_number": 682, "text": "670\n\n12 \u00b7 conditional and marginal inference\n\nthese basic approximations can be generalized in various ways. for our purpose\nthe most useful is to the situation where the x j are vectors of length p, inwhich case\nu is a p \u00d7 1 vector. then (12.22) extends to\n(cid:19)1/2\n\n(cid:6)\n\n(cid:13)\n\n(cid:7)\n\n(cid:8)(cid:14)\n\nexp\n\nn\n\nk (\u02dcu) \u2212 \u02dcutx\n\n,\n\n(12.24)\n\n.=\n\nf x (x)\n\nn p\n\n(2\u03c0) p|k (cid:9)(cid:9)(\u02dcu)|\n\nwhere now the p \u00d7 1 saddlepoint \u02dcu solves the p \u00d7 1 system of equations\n\n(cid:9)\n\nk\n\n(\u02dcu) = \u2202 k (\u02dcu)\n\u2202u\n\n= x,\n\n(cid:9)(cid:9)\n\n(cid:6)\n\n(u) = \u2202 2 k (u)/\u2202u\u2202ut is the p \u00d7 p matrix of second derivatives of k (u).\n\nand k\n2), where x2 has dimension ( p \u2212 1) \u00d7 1, and split ut into (u1, ut\nlet x t = (x1, x t\n2)\nconformably with x t; both x1 and u1 are scalar. then the marginal density of x 2 at\nx2 is obtained by saddlepoint approximation using the marginal cumulant-generating\nfunction k (0, u2) of x2, and is\nn p\u22121\n(2\u03c0) p\u22121|k\n22(\u02dcu0)|\n(cid:9)(cid:9)\n= (0, \u02dcut\n2)\n22 is the ( p \u2212 1) \u00d7 ( p \u2212 1) corner of k\n(cid:9)(cid:9)\n\n(12.25)\nisthe solution to the ( p \u2212 1) \u00d7 1 system of equations\nwhere \u02dcut\n\u2202 k (0, u2)/\u2202u2 = x2, and k\n0\ncorrespond-\ning to u2. division of (12.24) by (12.25) gives an approximation to the conditional\ndensity f x 1|x 2(x1 | x2), the double saddlepoint approximation\n\nk (\u02dcu0) \u2212 (0, x t\n\n(cid:19)1/2\n\nf x 2(x2)\n\n2)\u02dcu0\n\n(cid:8)(cid:14)\n\nexp\n\n.=\n\n(cid:7)\n\n(cid:13)\n\nn\n\n(cid:9)(cid:9)\n\n,\n\n&1/2 |k\n\n%\n\nn\n2\u03c0\n\n22(\u02dcu0)|1/2\n(cid:9)(cid:9)\n|k (cid:9)(cid:9)(\u02dcu)|1/2 exp\n\n(cid:7)\n\n(cid:13)\n\nn\n\nk (\u02dcu) \u2212 \u02dcutx \u2212 k (\u02dcu0) + (0, x t\n\n2)\u02dcu0\n\n,\n\n(12.26)\n\n(cid:8)(cid:14)\n\n(12.27)\n\ncorresponding to which is the distribution function approximation\n\npr(x 1 \u2264 x1 | x 2 = x2)\n\n\u2217\n\n(x1) equals r(x1) + r(x1)\n\n(cid:7)\n(cid:13)\nr(x1) = sign(\u02dcu1)\n2n\n(\u02dcu)|1/2/|k\nv(x1) = \u02dcu1n1/2|k\n(cid:9)(cid:9)\n\nk (\u02dcu0) \u2212 (0, x t\n2)\u02dcu0\n22(\u02dcu0)|1/2.\n(cid:9)(cid:9)\n\n(cid:8)\n\n,\n\nr\n\n\u2217\n\n(cid:7)\n.= \u0001\n(cid:8) \u2212 n\n\n(x1)\n(cid:7)\n\n\u22121 log{v(x1)/r(x1)}, but now with\n(cid:8)(cid:14)1/2 ,\n\nk (\u02dcu) \u2212 \u02dcutx\n\nwhere again r\n\nthese formulae break down when x1 = e(x1) and a different more complicated\napproximation is then available. in practice it is simplest to evaluate r\n(x1) for at a\ngrid of values of x1 excluding any too close to e(x1), and then to interpolate them\nusing a spline or other numerical method.\n\n\u2217\n\nexample 12.16 (poisson distribution) let v1 and v2 be independent poisson vari-\nables with means \u03bb1 and \u03bb2, and set x1 = v1, x2 = v1 + v2. then the joint cumulant-\ngenerating function of x1 and x2 is k (u) = \u03bb1 exp(u1 + u2) + \u03bb2 exp(u2), and the\nexact density of x1 given x2 = x2 is binomial with probability \u03c0 = \u03bb1/(\u03bb1 + \u03bb2)\nand denominator x2.\nfigure 12.7 shows the conditional density and distribution of x1, when \u03bb1 = 1,\n\u03bb2 = 2.5, and x2 = 15. the saddlepoint density gives a continuous approximation to\nthe discrete binomial density of x1 given x2, which it closely matches on the support\n\n "}, {"Page_number": 683, "text": "12.3 \u00b7 conditional inference\n\n671\n\nfigure 12.7 double\nsaddlepoint\napproximations to the\ndensity and distribution\nfunctions of x1 = v1\ngiven\nx2 = v1 + v2 = 15,\nwhere v1 and v2 are\npoisson with means 1 and\n2.5. left panel: exact\ndensity function and its\napproximation. right\npanel: exact distribution\nfunction (heavy) and its\napproximation without\n(solid) and with (dots) a\ncontinuity correction.\n\nf\nd\np\n\n0\n3\n\n.\n\n0\n\n0\n2\n0\n\n.\n\n0\n1\n0\n\n.\n\n0\n\n.\n0\n\nf\nd\nc\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n0\n\n0\n\n5\n\n10\n\n15\n\n0\n\n5\n\n10\n\n15\n\nx1\n\nx1\n\nof x1. for x1 = 0, . . . ,15, the distribution function approximation \u0001{r\nclosely with\n\n\u2217\n\n(x1)} agrees\n\npr(x1 \u2264 x1 \u2212 1 | x2) + 1\n\n2 pr(x1 = x1 | x2),\n\na quantity akin to the mid- p significance level (10.28). continuity correction applied\nby taking \u0001{r\n(cid:1)\n\n(x1 + 1/2)} approximates pr(x1 \u2264 x1 | x2) well.\n\n\u2217\n\nin applications saddlepoint formulae are sometimes used simply by taking the\ncumulant-generating function for a variable of interest, and formally setting n = 1,\nas we shall do below.\n\nthe rest of this section sketches derivations of the approximations above and can be\nskipped on a first reading. no attempt is made at a rigorous treatment; the intention is\nto give some justification for the approximations, and the flavour of the manipulations\ninvolved.\n\nedgeworth series\nalthough they have practical disadvantages compared to saddlepoint approximations,\nedgeworth series play a central role in theoretical discussions of small-sample infer-\nence. as we shall see below, they allow a relatively simple derivation of saddlepoint\napproximations.\nlet x1, . . . , xn be a random sample of continuous variables with cumulant-\ngenerating function k (u) and finite cumulants \u03bar , let \u03c1r = \u03bar /\u03bar/2\ndenote the rth\nstandardized cumulant, and let zn = (sn \u2212 n\u03ba1)/(n\u03ba2)1/2 denote the standardized\nversion of sn = x1 + \u00b7\u00b7\u00b7 + xn. the large-sample distribution of zn may be found\nby noting that its cumulant-generating function is\n\n2\n\nthus its moment-generating function is\n\nu2/2 + n\n\n\u22121/2\u03c13u3/3! + n\n\n\u22121\u03c14u4/4! + \u00b7\u00b7\u00b7 .\n(cid:16)\n\n(cid:15)\n\n(cid:6)\n\nexp(u2/2)\n\n1 + 1\n\n6\n\n\u22121/2u3 +\n\n\u03c13n\n\n\u03c12\n3\n72\n\n+ \u03c14\n24\n\n\u22121u4 + o(n\n\n\u22123/2)\n\nn\n\n(cid:19)\n\n.\n\n(12.28)\n\nfrancis ysidro edgeworth\n(1845\u20131926) was born in\nlongford, ireland, studied\nin dublin and oxford,\nwhere he took a first-class\ndegree in classics, and\nthen became a london\nbarrister. in 1880 he began\na university career as a\nmathematician, eventually\nholding professorships of\npolitical economy in\nlondon and oxford. his\npioneering articles\nsystematically applied\nideas from the theory of\nerrors to social science\nand economics.\n\n "}, {"Page_number": 684, "text": "(cid:20) \u221e\n\n\u2212\u221e\n\n12 \u00b7 conditional and marginal inference\n672\nall that remains when n \u2192 \u221e is the leading term, which is the moment-generating\nfunction of the standard normal density. the continuity theorem and uniqueness of\nmoment-generating functions then yield the limiting standard normal distribution\nof zn.\n\nto find better finite-sample approximations to the density and distribution functions\n\nof zn, note that integration by parts gives\n\neuz(\u22121)r dr \u03c6(z)\ndzr\n\ndz = ur eu2/2,\n\nr = 0, 1, . . . .\n\nletting (\u22121)r dr \u03c6(z)/dzr = \u03c6(z)hr (z) determine the rth-order hermite polynomial\nhr (z) gives\n\nh1(z) = z, h2(z) = z2 \u2212 1, h3(z) = z3 \u2212 3z, h4(z) = z4 \u2212 6z2 + 3,\nh5(z) = z5 \u2212 10z3 + 15z, h6(z) = z6 \u2212 15z4 + 45z2 \u2212 15.\n(cid:19)\n\nterm by term inversion of (12.28) shows that the corresponding density is\n+ o(n\n\nf zn (z) = \u03c6(z)\n\n1 + \u03c13\n\n\u22123/2)\n\nh6(z)\n\n(cid:6)\n\n\u2019\n\n6n1/2 h3(z) + 1\n\nn\n\nh4(z) + \u03c12\n3\n72\n\n\u03c14\n24\n\n(\n\n,\n\n\u2019\n\n(cid:6)\n\n(12.29)\ninversion of which gives the first three terms of the cumulant-generating function of\nzn. integration of (12.29) gives the corresponding distribution function,\nfzn (z) = \u0001(z) \u2212 \u03c6(z)\n\n\u22123/2)\n(12.30)\nthe leading terms of the edgeworth expansions (12.29) and (12.30) give the standard\nnormal approximation for zn, with subsequent terms giving improvements, based\nrespectively on the skewness of zn and on a combination of its skewness and kurtosis.\nhigher-order terms depend on further cumulants of the x j .\n\n6n1/2 h2(z) + 1\n\nh3(z) + \u03c12\n3\n72\n\n+ o(n\n\nh5(z)\n\n\u03c14\n24\n\n(cid:19)\n\n(\n\n\u03c13\n\nn\n\n.\n\nthe expansions (12.29) and (12.30) are more useful for theoretical development\nthan for applications, because there is no reason for (12.29) to remain positive or for\n(12.30) to be increasing in z, orindeed even for it to lie between zero and one, whereas\nthe saddlepoint formula (12.22) is guaranteed to give a positive density approximation.\non the other hand saddlepoint approximation requires the entire cumulant-generating\nfunction, while (12.29) and (12.30) use only first few cumulants and so are more\nreadily calculated.\nwhen the density for zn is evaluated at z = 0, the series in (12.29) contains only\n\u22121/2 depend on odd hermite polynomials,\npowers of n\nall of which vanish at z = 0.\nderivation of saddlepoint approximation\nto derive (12.22) we first embed the density f x of x in the exponential family\n\n\u22121, because the odd powers of n\n\nf x (x; u) = exp{xu \u2212 k (u)} f x (x),\n\nwhere k is the cumulant-generating function of x. here u plays the role of a param-\neter, and the new density f x (x; u) is an exponential tilt of f x . under this new model\n\n "}, {"Page_number": 685, "text": "12.3 \u00b7 conditional inference\n\nthe density of sn may be written\n\n673\n\nfsn (s; u) = exp{su \u2212 nk (u)} fsn (s),\n\n(12.31)\nand its cumulant-generating function as k sn (t) = n{k (t + u) \u2212 k (u)}. thus with\nparameter u the mean and variance of sn under the new density are nk\n(u) and\nnk\n\n(u). expression (12.31) gives\n\n(cid:9)(cid:9)\n\n(cid:9)\n\nfsn (s) = exp{nk (u) \u2212 su} fsn (s; u)\n\n(cid:9)\n\n(\u02dcu) = s, and the first term of the edgeworth series is {2\u03c0nk\n\nfor all u. we now replace fsn (s; u) inthis expression by its edgeworth expansion,\nbut with the value of u chosen so that the tilted density fsn (s; u) has mean s. then\n(\u02dcu)}\u22121/2. the re-\nnk\n(cid:8)\nsulting approximation to the density of sn is\n\n(\u02dcu)}\u22121/2 exp{nk (\u02dcu) \u2212 s \u02dcu}(cid:7)\nwhere the order of the error follows from the fact that at its mean the edgeworth\n\u22121. achange of variables s (cid:10)\u2192 x = s/n gives the\nseries contains only powers of n\nleading term of (12.22), with subsequent terms found by retaining more terms of the\nedgeworth series.\n\nfsn (s) = {2\u03c0nk\n\n1 + o(n\n\n\u22121)\n\n(cid:9)(cid:9)\n\n(cid:9)(cid:9)\n\n,\n\nthe argument leading to (12.23) starts by integrating (12.22), giving\nexp [n {k (\u02dcu) \u2212 \u02dcux}] dx,\n\npr(x \u2264 x0)\n\n(cid:19)1/2\n\n(cid:6)\n\n.=\n\n(cid:20)\n\nn\n\nx0\n\u2212\u221e\n\n2\u03c0 k (cid:9)(cid:9)(\u02dcu)\n\n(cid:9)\n\n(\u02dcu) = x. achange of\n\nwhere \u02dcu is a function of the variable of integration through k\nvariable from x to \u02dcu, using dx/d \u02dcu = k\n\n(\u02dcu), gives\n\n(cid:9)(cid:9)\n\npr(x \u2264 x0)\n\n.=\n\n(cid:19)1/2\n\n(cid:9)(cid:9)\n\nnk\n\n(\u02dcu)\n\n(cid:20)\n\n(cid:6)\n\nu0\n\u2212\u221e\n\n(cid:13)\n\n(cid:7)\n\nk (\u02dcu) \u2212 \u02dcu k\n\n(cid:9)\n\n(cid:8)(cid:14)\n\n(cid:9)\n\nn\n\n2\u03c0\n\n(\u02dcu)\n\nd \u02dcu,\n\nexp\n(u0) = x0. inthis case the approximation\nwhich is of form (11.30); here u0 satisfies k\n(11.31) takes the form (12.23). detailed accounting shows that under broad condi-\n\u22121) inlarge deviation regions, where\ntions the error in (12.23) is relative, of size o(n\nx0 \u2212 e(x) is o(1), and is only o(n\n\u22123/2) for moderate deviation regions, in which\nx0 \u2212 e(x) is o(n\n\u22121/2). the large-deviation property leads to the extraordinary ac-\ncuracy of the approximations, which have low relative error far into the tails of the\ndistribution of x.\ndiscrete distributions are also important in practice. suppose that the x j take val-\nues in the lattice a + bk, where a and b are constants and k = 0, 1, . . .. then x takes\nvalues in the lattice a + bk/n. the saddlepoint approximation for the density of x at\n(cid:8)1/2 is replaced by\n\u22121), but in the\nthese values is unchanged, and its error remains relative and of o(n\ncumulative distribution function approximation v(x) = \u02dcu\n(cid:8)1/2; note that this produces the continuous version when\n\u22121{1 \u2212 exp(\u2212b \u02dcu)}(cid:7)\nb\nb \u2192 0. the error in this approximation is o(n\n\u22121). a continuity-corrected approxi-\nmation to the quantity pr{x \u2264 x0 + 1/(2n)} replaces 1 \u2212 exp(\u2212b \u02dcu) by 2 sinh(\u02dcu/2).\n\nnk\n\nnk\n\n(\u02dcu)\n\n(\u02dcu)\n\n(cid:7)\n\n(cid:9)(cid:9)\n\n(cid:9)(cid:9)\n\n "}, {"Page_number": 686, "text": "674\n\n12 \u00b7 conditional and marginal inference\n\n12.3.3 approximate conditional inference\nto see how saddlepoint approximation may be applied for inference, we consider\ninitially the case where a random sample y1, . . . ,y n from the continuous exponential\nfamily exp{\u03b8 y \u2212 \u03ba(\u03b8)} f0(y) depends on a scalar \u03b8. the maximum likelihood estima-\ntor(cid:5)\u03b8 solves the likelihood equation y = \u03ba(cid:9)\n((cid:5)\u03b8), so the log likelihood and observed\n\ninformation may be expressed as\n\n(cid:9)(\u03b8;(cid:5)\u03b8) \u2261 n\n\n((cid:5)\u03b8) \u2212 \u03ba(\u03b8)\n\n\u03b8 \u03ba(cid:9)\n\nj (\u03b8;(cid:5)\u03b8) = \u2212\u2202 2(cid:9)(\u03b8;(cid:5)\u03b8)/\u2202\u03b8 2 = n\u03ba(cid:9)(cid:9)\n\n(\u03b8).\n\n,\n\n(cid:7)\n\n(cid:8)\n\nits density or equivalently that of (cid:5)\u03b8, which is a 1\u20131 function of y . the cumulant-\nnow y is a minimal sufficient statistic for \u03b8, so noinformation is lost by considering\ngenerating function of y j is k (u) = \u03ba(\u03b8 + u) \u2212 \u03ba(\u03b8), so the saddlepoint equation\n(\u03b8 + \u02dcu) = y. this implies that \u02dcu =(cid:5)\u03b8 \u2212 \u03b8; furthermore the second derivative\nis \u03ba(cid:9)\n\u22121 j ((cid:5)\u03b8;(cid:5)\u03b8). thus the density approximation (12.22) for y is\n(\u02dcu) = \u03ba(cid:9)(cid:9)\n(cid:9)(cid:9)\nk\n\nn\n\nexp [n {k (\u02dcu) \u2212 \u02dcu y}]\n\nn\n\n(cid:13)\n\n(cid:7)\n\n2\u03c0 k (cid:9)(cid:9)(\u02dcu)\n2\u03c0 \u03ba(cid:9)(cid:9)((cid:5)\u03b8)\nf ((cid:5)\u03b8; \u03b8) = c|j ((cid:5)\u03b8;(cid:5)\u03b8)|1/2 exp\n\n(cid:8)(cid:14)\n\u03ba((cid:5)\u03b8) \u2212 \u03ba(\u03b8) \u2212 ((cid:5)\u03b8 \u2212 \u03b8)y\n=\n\u22121 j ((cid:5)\u03b8;(cid:5)\u03b8), and hence the density of(cid:5)\u03b8 may be written as\n(cid:8)\n\n(cid:9)(\u03b8;(cid:5)\u03b8) \u2212 (cid:9)((cid:5)\u03b8;(cid:5)\u03b8)\n\n1 + o(n\n\n(cid:8)(cid:7)\n\n\u22121)\n\nexp\n\n(cid:7)\n\nn\n\n,\n\n.\n\nnow \u2202 y/\u2202(cid:5)\u03b8 = n\n\n(12.32)\nwhere c = (2\u03c0)\n\u22121/2. thus (12.7) again arises as an approximation to the density of a\nsaddlepoint approximation to the cumulative distribution function of(cid:5)\u03b8 or equiva-\nmaximum likelihood estimator, here with no ancillary statistic.\n\n(\u03b8 + \u02dcu) = n\n(cid:6)\n.=\n(cid:6)\n\nfy (y; \u03b8)\n\n(cid:19)1/2\n(cid:19)1/2\n\nlently of y gives\n\n(cid:7)\npr(y \u2264 y; \u03b8)\n\u2217\nr\n\u22121 log{v(\u03b8)/r(\u03b8)}, and\n\n.= \u0001\n\n(cid:8)\n\n(\u03b8)\n\n,\n\n(12.33)\n\nwhere r\n\n\u2217\n\n(\u03b8) = r(\u03b8) + r(\u03b8)\nr(\u03b8) = sign((cid:5)\u03b8 \u2212 \u03b8)[2{(cid:9)((cid:5)\u03b8) \u2212 (cid:9)(\u03b8)}]1/2,\n\nv(\u03b8) = ((cid:5)\u03b8 \u2212 \u03b8)j ((cid:5)\u03b8)1/2;\n\n(12.34)\n\n\u2217\n\n((cid:5)\u03b8) = y. anapproximate confidence interval for \u03b8 may be obtained by\nrecall that \u03ba(cid:9)\nfinding the values of \u03b8 for which \u0001{r\n(\u03b8)} = \u03b1, 1 \u2212 \u03b1 or equivalently for which\n(\u03b8) = z\u03b1, z1\u2212\u03b1. this improves on the likelihood ratio limits, which are found by\n\u2217\nr\nsolving r(\u03b8) = z\u03b1, z1\u2212\u03b1.\nexample 12.17 (gamma distribution) let y1, . . . ,y n be a gamma random\nsample with mean \u03b8 and known shape parameter \u03bd. the log likelihood is then\n\n(cid:9)(\u03b8;(cid:5)\u03b8) \u2261 \u2212n\u03bd(log \u03b8 +(cid:5)\u03b8 /\u03b8), with(cid:5)\u03b8 = y and observed information j (\u03b8;(cid:5)\u03b8) = n\u03bd\u03b8\u22122.\nthus the approximate density for(cid:5)\u03b8 is\n(cid:9)(\u03b8;(cid:5)\u03b8) \u2212 (cid:9)((cid:5)\u03b8;(cid:5)\u03b8)\n\nexp(n\u03bd \u2212 n\u03bd(cid:5)\u03b8 /\u03b8).\n\nc|j ((cid:5)\u03b8;(cid:5)\u03b8)|1/2 exp\n\n(cid:8) =\n\n(cid:15)(cid:5)\u03b8\n\n(cid:16)n\u03bd\n\n%\n\n(cid:7)\n\n&1/2 1(cid:5)\u03b8\n\nn\u03bd\n2\u03c0\n\n\u03b8\n\n "}, {"Page_number": 687, "text": "table 12.1 approximate\ntail probabilities (\u00d7102)\ncorresponding to quantiles\nof the maximum\nlikelihood estimator of the\nmean of an exponential\nvariable.\n\nfor brevity we let \u03ba\u03bb\ndenote \u2202\u03ba/\u2202\u03bb, \u03ba\u03c8 \u03bb denote\n\u22022\u03ba/\u2202\u03c8 \u2202\u03bbt, and so forth.\n\n12.3 \u00b7 conditional inference\n\n675\n\nx p\n\n0.001001\n\n0.01005\n\n0.02532\n\n0.05129\n\n2.996\n\n3.689\n\n4.605\n\n6.908\n\n\u2217\n\n1\n1.02937\n0.36040\n16.1099\n\n0.1\n0.104188\n0.029354\n15.88975\n\nexact, 100 p\n(\u03b8)}\n100\u0001{r\n100\u0001{r(\u03b8)}\n100\u0001{v(\u03b8)}\nthe exact density of (cid:5)\u03b8 is gamma with mean \u03b8 and shape parameter n\u03bd, so the\n\n2.5\n2.55549\n1.00503\n16.4859\n\n5\n5.08002\n2.21778\n17.1385\n\n99.9\n99.897\n99.760\n100.000\n\n95\n94.923\n90.997\n97.702\n\n97.5\n97.454\n95.189\n99.642\n\n99\n98.978\n97.926\n99.984\n\napproximation is here exact after renormalization; it merely substitutes stirling\u2019s\nformula for the gamma function that appears in the exact density.\n\nin this example,\n\nr(\u03b8) = sign((cid:5)\u03b8 \u2212 \u03b8)[2n\u03bd{(cid:5)\u03b8 /\u03b8 \u2212 1 \u2212 log((cid:5)\u03b8 /\u03b8)}] 1/2,\nv(\u03b8) = (n\u03bd)1/2((cid:5)\u03b8 \u2212 \u03b8)/(cid:5)\u03b8 ,\nand table 12.1 shows that (12.33) is essentially exact for this model when n\u03bd = 1.\nthe tail probability approximation based on the signed likelihood ratio statistic r(\u03b8)\nis also fairly accurate, while that based on the standardized maximum likelihood\nestimate v(\u03b8) is very poor. the accuracy of the tail approximation for r\n(\u03b8) should\n(cid:1)\ncarry over to the corresponding confidence intervals.\n\n\u2217\n\nconditional inference\nconsider now conditional inference for the scalar parameter \u03c8 in the exponential\nfamily\n\nf (t1, t2; \u03c8, \u03bb) = exp\n\nt1\u03c8 + t t\n\n\u03bb \u2212 \u03ba(\u03c8, \u03bb)\n\n(12.35)\nwhere t1 is scalar and t2 has dimension ( p \u2212 1) \u00d7 1, and \u03bb is treated as a ( p \u2212 1) \u00d7 1\nnuisance parameter. we obtain an approximate conditional density for t1 given t2 by\ndouble saddlepoint approximation of\n\nm(t1, t2),\n\n2\n\n(cid:7)\n\n(cid:8)\n\nf (t1 | t2; \u03c8) = f (t1, t2; \u03c8, \u03bb)\nf (t2; \u03c8, \u03bb)\n\n.\n\n(12.36)\n\nthe cumulant-generating function of (t1, t2) is\n\nk (u) = \u03ba(\u03c8 + u1, \u03bb + u2) \u2212 \u03ba(\u03c8, \u03bb),\n\nwhere ut = (u1, ut\nequation corresponding to the numerator of (12.36) is\n\n2), u1 is scalar, and u2 is a ( p \u2212 1) \u00d7 1 vector. the saddlepoint\nt2 = \u2202 k (\u03c8, \u03bb + \u02dcu2)\n\n= \u2202\u03ba(\u03c8,(cid:5)\u03bb\u03c8 )\n\nsay, where(cid:5)\u03b8 t\nfixed, based on t2; note that \u02dcu2 =(cid:5)\u03bb\u03c8 \u2212 \u03bb. the matrix of second derivatives\n\n\u03c8 ) and(cid:5)\u03bb\u03c8 is the maximum likelihood estimate of \u03bb with \u03c8\n\n= \u03ba\u03bb((cid:5)\u03b8\u03c8 ),\n\n\u2202u2\n\n\u2202\u03bb\n\n= \u2202 2\u03ba(\u03c8,(cid:5)\u03bb\u03c8 )\n\n\u2202\u03bb\u2202\u03bbt\n\n= \u03ba\u03bb\u03bb((cid:5)\u03b8\u03c8 ) = j\u03bb\u03bb((cid:5)\u03b8\u03c8 )\n\n\u03c8 = (\u03c8,(cid:5)\u03bbt\n\u2202 2 k (\u03c8, \u03bb + \u02dcu2)\n\n\u2202u2\u2202ut\n2\n\n "}, {"Page_number": 688, "text": "676\n\n12 \u00b7 conditional and marginal inference\n\nis the observed information for \u03bb when \u03c8 is fixed. the saddlepoint equation\ncorresponding to the denominator of (12.36) is\n\n(cid:15)\n\n(cid:16)\n\nt1\nt2\n\n=\n\n(cid:16)\n\n,\n\n(cid:15) \u2202 k (\u03c8+\u02dcu1,\u03bb+\u02dcu2)\n\u2202 k (\u03c8+\u02dcu1,\u03bb+\u02dcu2)\n(cid:16)\n\n\u2202u1\n\n(cid:16)\n\n(cid:15)\n\n=\n\n(cid:15)\n\n=\n\n\u03ba\u03c8 ((cid:5)\u03b8)\n\u03ba\u03bb((cid:5)\u03b8)\n\u03ba\u03c8 \u03c8 ((cid:5)\u03b8)\n\u03ba\u03bb\u03c8 ((cid:5)\u03b8)\n\n(cid:16)\n\n= j ((cid:5)\u03b8),\n\n\u03ba\u03c8 \u03bb((cid:5)\u03b8)\n\u03ba\u03bb\u03bb((cid:5)\u03b8)\n\n\u2202u2\nwhile the matrix of second derivatives is\n\u2202 2 k (\u03c8+\u02dcu1,\u03bb+\u02dcu2)\n\u2202 2 k (\u03c8+\u02dcu1,\u03bb+\u02dcu2)\n\n(cid:15) \u2202 2 k (\u03c8+\u02dcu1,\u03bb+\u02dcu2)\n\u2202 2 k (\u03c8+\u02dcu1,\u03bb+\u02dcu2)\n\n\u2202u1\u2202ut\n2\n\n\u2202u2\n1\n\nwhere(cid:5)\u03b8 t = ((cid:5)\u03c8 ,(cid:5)\u03bbt) isthe overall maximum likelihood estimate. substitution of these\n\n\u2202u2\u2202ut\n2\n\n\u2202u1\u2202u2\n\nformulae into the double saddlepoint approximation (12.26) and then reorganization\nalong the lines that leads to (12.32) gives\n\nf (t1 | t2; \u03c8)\n\n.=\n\n(cid:19)1/2\n\n(cid:6)|j\u03bb\u03bb((cid:5)\u03b8\u03c8 )|\n2\u03c0|j ((cid:5)\u03b8)|\n\n(cid:7)\n\n(cid:9)((cid:5)\u03b8\u03c8 ) \u2212 (cid:9)((cid:5)\u03b8)\n\n(cid:8)\n\nexp\n\nwhere (cid:9)(\u03b8) = (cid:9)(\u03c8, \u03bb) = log f (t1, t2; \u03c8, \u03bb) isthe overall log likelihood.\napproximate tail probabilities associated with particular values of \u03c8 can be formed\nin the way outlined after (12.33), and are used to construct confidence intervals. the\nbasis of the tail probability approximation is (12.27), which here depends on \u03c8,\nand is\n\n,\n\n(12.37)\n\nwhere r\n\n\u2217\n\n(\u03c8) = r(\u03c8) + r(\u03c8)\n\npr(t1 \u2264 t1 | t2 = t2; \u03c8)\n\n.= \u0001{r\n\u22121 log{r(\u03c8)/v(\u03c8)}, with\n\n\u2217\n\n(\u03c8)},\n\nr(\u03c8) = sign((cid:5)\u03c8 \u2212 \u03c8)[2{(cid:9)((cid:5)\u03b8) \u2212 (cid:9)((cid:5)\u03b8\u03c8 )}]1/2, v(\u03c8) = ((cid:5)\u03c8 \u2212 \u03c8)\n\n(12.38)\n\n(cid:6) |j ((cid:5)\u03b8)|\n|j\u03bb\u03bb((cid:5)\u03b8\u03c8 )|\n\n(cid:19)1/2\n\n.\n\nthus the improved approximation again involves modifying the signed likelihood\nratio statistic, here using a standardized maximum likelihood estimate rather than the\nscore statistic that appeared with the regression-scale model. we can write\n\nv(\u03c8) = ((cid:5)\u03c8 \u2212 \u03c8)\n\n(cid:6) |j ((cid:5)\u03b8\u03c8 )|\n|j\u03bb\u03bb((cid:5)\u03b8\u03c8 )|\n\n(cid:19)1/2 \u00d7\n\n(cid:6) |j ((cid:5)\u03b8)|\n|j ((cid:5)\u03b8\u03c8 )|\n\n(cid:19)1/2 = \u03b3 \u00d7 c,\n\n(12.39)\n\nsay, yielding a three-part decomposition of r\nshow that r\n\n(\u03c8) is invariant to interest-preserving reparametrization.\n\n(\u03c8) like (12.18). it is an exercise to\n\n\u2217\n\n\u2217\n\nexample 12.18 (nodal involvement data) a central issue for the data in table 10.8\nis how nodal involvement depends on the five binary covariates. for purpose of il-\nlustration we consider setting confidence intervals for the parameter \u03c8 associated\nwith acid, and regard the parameters for other covariates as incidental. the left\npanel of figure 12.8 shows r(\u03c8) and r\n(\u03c8) asfunctions of \u03c8, when acid only is\nincluded, and when all five covariates are included. the small-sample modification is\nappreciably larger when several nuisance parameters are eliminated, and the standard\nerror, corresponding roughly to the slope, is larger. the right panel shows infor-\nmation and nuisance parameter corrections rinf and rnp for models with 1+acid,\n1+stage+xray+acid, and with all five covariates. all three information corrections\n\n\u2217\n\n "}, {"Page_number": 689, "text": "table 12.2 estimate,\nstandard errors, and 0.95\nconfidence intervals for\nthe coefficient of acid, \u03c8,\nfor the nodal involvement\ndata with all the other\ncovariates fitted. the\ncontinuity corrected\nversion of r\n(\u03c8) is\nobtained by multiplying\nv(\u03c8) by\n(e\n\n(cid:5)\u03c8\u2212\u03c8 \u2212 1)/((cid:5)\u03c8 \u2212 \u03c8).\n\n\u2217\n\n\u2217\n\nfigure 12.8 conditional\ninference for coefficient of\nacid, \u03c8, for nodal\ninvolvement data. left:\nsigned likelihood ratio\nstatistic r(\u03c8) (dashes) and\nmodified version r\n(\u03c8)\n(solid) for models with\nfive nuisance parameters\n(left pair of curves) and\nwith one nuisance\nparameter (right pair of\ncurves, shifted right by\none unit). the dotted\nhorizontal lines are at\n0,\u00b11.96. right:\ninformation corrections\nrinf (dashes) and nuisance\nparameter corrections rnp\n(solid) for models with\none, three, and five\nnuisance parameters. the\nrnp increase in size with\nthe number of nuisance\nparameters eliminated.\nthe dotted horizontal\nlines at \u00b10.2 show a rule\nof thumb for substantial\ncorrections.\n\n12.3 \u00b7 conditional inference\n\n677\n\nmethod\n\nnormal approximation to (cid:5)\u03c8\nnormal approximation to (cid:5)\u03c8a\n\ndirected deviance r(\u03c8)\nmodified directed deviance r\nmodified directed deviance r\nwith continuity correction\n\n\u2217\n\u2217\n\n(\u03c8)\n(\u03c8)\n\nestimate (se)\n\ninterval\n\n(0.136, 3.232)\n(0.048, 2.930)\n(0.209, 3.378)\n(0.086, 2.998)\n(\u22120.131, 3.330)\n\n1.68 (0.79)\n1.49 (0.74)\n\n\u2014\n\u2014\n\u2014\n\n4\n0\n\n.\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n.\n\n2\n0\n-\n\n.\n\n4\n0\n-\n\nn\no\n\ni\nt\nc\ne\nr\nr\no\nc\n\nt\n\no\nv\np\n\ni\n\n \n\ne\n\nt\n\ni\n\na\nm\nx\no\nr\np\np\na\n\n2\n\n0\n\n2\n-\n\n-1\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n0\n\n1\n\n2\n\n3\n\n4\n\npsi\n\npsi\n\nare small, but the nuisance parameter correction increases sharply as more parameters\nare eliminated.\n\nan approximate conditional maximum likelihood estimate (cid:5)\u03c8a is obtained as the\ntable 12.2 shows that (cid:5)\u03c8a has smaller magnitude and standard error than has the ordi-\nnary maximum likelihood estimate (cid:5)\u03c8. small-sample adjustment shortens confidence\n\nsolution to r\n(\u03c8) atthat point.\ntogether these yield a confidence interval for \u03c8 based on normal approximation.\n\n(\u03c8) = 0, with standard error given by the slope of r\n\n\u2217\n\n\u2217\n\nintervals and moves them closer to zero. in such cases a continuity correction can be\napplied to give results closer to those from exact conditioning. this gives appreciably\n(cid:1)\nwider confidence intervals, as the discussion in example 7.38 would suggest.\n\ncurved exponential family\nour previous discussion has applied to linear exponential families, but curved expo-\nnential family models are also important in applications. the key difficulty in devel-\noping small-sample inferences is then to find an analogue of the quantities (12.17)\nand (12.39) that modify the signed likelihood ratio statistic. an exact expression is\nunavailable but one of several possible approximations is\n\nwith i (\u03b8) the expected information and c(\u03b8, \u03b80) denoting the p \u00d7 p matrix\n\nv(\u03c8)\n\n.= |i ((cid:5)\u03b8)|\u22121|c((cid:5)\u03b8 ,(cid:5)\u03b8\u03c8 )|(cid:7)|j ((cid:5)\u03b8)|/|j\u03bb\u03bb((cid:5)\u03b8\u03c8 )|(cid:8)1/2 ,\n(cid:15)\n\ncov0 {(cid:9)(\u03b80) \u2212 (cid:9)(\u03b8), (cid:9)\u03b8 (\u03b80)}\n\n(cid:16)\n\ncov0 {(cid:9)\u03bb(\u03b8), (cid:9)\u03b8 (\u03b80)}\n\n.\n\n(12.40)\n\n "}, {"Page_number": 690, "text": "678\n\n\u22123/2) inmoderate deviation regions and o(n\n\n12 \u00b7 conditional and marginal inference\nhere (cid:9)\u03b8 (\u03b8) = \u2202(cid:9)(\u03b8)/\u2202\u03b8, and so forth, and cov0 denotes covariance taken with respect\nto the model with parameter value \u03b80. expression (12.40) admits a decomposition\ninto two parts like that at (12.39). it requires neither that an ancillary statistic be\nspecified nor does it involve sample space derivatives, but it does entail a loss of\nprecision relative to (12.39), use of which in continuous models gives relative error of\n\u22121/2) inlarge deviation regions.\nsizes o(n\nby contrast use of (12.40) reduces the relative error in moderate deviation regions\n\u22121). the key point, however, is that the relative error properties, which give\nto o(n\nhighly accurate approximations far into the tails of the distribution of the modified\nsigned likelihood ratio statistic r\nexample 12.19 (nonlinear model) consider a model in which the n \u00d7 1 vector of\nresponses y \u223c nn(\u03b7, \u03c3 2 in) and the n \u00d7 1 mean vector \u03b7 = \u03b7(\u03b2) depends on a p \u00d7 1\nparameter vector \u03b2. let the parameter of interest \u03c8 be an element of \u03b2, \u03b21, say, let\n\u03bb denote the p \u00d7 1 vector comprising the remaining elements of \u03b2 and \u03c3 2, and let\n\u03b8 t = (\u03c8, \u03bbt). the log likelihood,\n(cid:7)\n\n(\u03c8), are preserved.\n\n(cid:8)\n\n\u2217\n\n(cid:9)(\u03b8) \u2261 \u2212 1\n\n2\n\nn log \u03c3 2 + (y \u2212 \u03b7)t(y \u2212 \u03b7)/\u03c3 2\n\n,\n\nhas score vector given by\n\n(cid:8)\nwhere \u03b7\u03b2 is the n \u00d7 p matrix \u2202\u03b7/\u2202\u03b2 t. onusing the decomposition\n\n(y \u2212 \u03b7)t(y \u2212 \u03b7) \u2212 n\u03c3 2\n\n\u03c3 \u22122(y \u2212 \u03b7)t\u03b7\u03b2 ,\n\n(cid:7)\n\n(cid:18)\n\n/(2\u03c3 4)\n\nn log \u03c3 2 + (y \u2212 \u03b70 + \u03b70 \u2212 \u03b7)t(y \u2212 \u03b70 + \u03b70 \u2212 \u03b7)/\u03c3 2\n(y \u2212 \u03b70)t(y \u2212 \u03b70) + 2(y \u2212 \u03b70)t(\u03b70 \u2212 \u03b7)\n/\u03c3 2 + d,\n\n(cid:8)\n\n(cid:8)\n\n(cid:9)\u03b8 (\u03b8)t = (cid:17)\n(cid:7)\n(cid:7)\n\n(cid:9)(\u03b8) = \u2212 1\n= \u2212 1\n\n2\n\n2\n\nwhere d depends only on the parameters and subscript 0 indicates that a quantity is\nevaluated at \u03b80 = (\u03b20, \u03c3 2\n\n0 ), we find\n\n(cid:17)\n\n(cid:18)\n\ncov0 {(cid:9)(\u03b8), (cid:9)\u03b8 (\u03b80)} = \u2212\u03c3 \u22122\n\n(\u03b70 \u2212 \u03b7)t\u03b7\u03b20, n/2\n\n,\n\nfrom which we see that the first row of c(\u03b8, \u03b80) equals\n(\u03b70 \u2212 \u03b7)t\u03b7\u03b20/\u03c3 2, n(\u03c3 \u22122 \u2212 \u03c3 \u22122\n\n0\n\n(cid:17)\n\n(cid:18)\n\n,\n\n)/2\n\n(cid:15)\n\nand a similar calculation shows that the remaining p \u00d7 ( p + 1) submatrix is\n\ncov0 {(cid:9)\u03bb(\u03b8), (cid:9)\u03b8 (\u03b80)} =\n\n\u03c3 \u22122\u03b7t\n\u03b22\n\n\u03b7\u03b20\n\n\u03c3 \u22124(\u03b70 \u2212 \u03b7)t\u03b7\u03b20\n\n0\n\nn/(2\u03c3 4)\n\nthus the approximation to v(\u03c8) involves this matrix, evaluated with \u03b80 =(cid:5)\u03b8 and\n\u03b8 =(cid:5)\u03b8\u03c8 , and the other information matrices. in this case i ((cid:5)\u03b8) = j ((cid:5)\u03b8).\n\n(cid:1)\n\n(cid:16)\n\n.\n\nexample 12.20 (calcium data) for numerical illustration we consider the data of\nexample 10.1, to which we fit a normal model with constant variance \u03c3 2 and mean\n\u03b20{1 \u2212 exp(\u2212x/\u03b21)}, where x represents time in minutes. first-order inference was\ndiscussed in example 10.9.\n\n "}, {"Page_number": 691, "text": "12.3 \u00b7 conditional inference\n\n679\n\n2\n\n1\n\nt\n\no\nv\np\n\ni\n\n0\n\n1\n-\n\n2\n-\n\n2\n\n1\n\nt\n\no\nv\np\n\ni\n\n0\n\n1\n-\n\n2\n-\n\n3.0\n\n3.5\n\n4.0\n\n4.5\n\n5.0\n\n5.5\n\n6.0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n12\n\n-3\n\n-2\n\n-1\n\n0\n\nbeta0\n\nbeta1\n\nlog sigma\n\n2\n\n1\n\nt\n\no\nv\np\n\ni\n\n0\n\n1\n-\n\n2\n-\n\n2\n\n1\n\nt\n\no\nv\np\n\ni\n\n0\n\n1\n-\n\n2\n-\n\n3.0\n\n3.5\n\n4.0\n\n4.5\n\n5.0\n\n5.5\n\n6.0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n12\n\n-3\n\n-2\n\n-1\n\n0\n\nbeta0\n\nbeta1\n\nlog sigma\n\n2\n\n1\n\nt\n\no\nv\np\n\ni\n\n0\n\n1\n-\n\n2\n-\n\n2\n\n1\n\nt\n\no\nv\np\n\ni\n\n0\n\n1\n-\n\n2\n-\n\nfigure 12.9 conditional\ninference for parameters\nof nonlinear regression\nmodel for calcium data.\neach panel shows how the\nsigned likelihood ratio\nstatistic (solid), the\nmodified signed\nlikelihood ratio statistic\n(heavy), and the\nstandardized maximum\nlikelihood estimate\n(dashes) depend on the\ncorresponding parameter.\nthe dotted horizontal\nlines are at 0,\u00b11.96.\nupper panels: full data;\nlower panels: just one\nobservation at each time.\n\nwhich shows, for each parameter of interest \u03c8, the quantities r(\u03c8), r\n\nthe result of applying the computations in example 12.19 are given in figure 12.9,\n(\u03c8) and the\n\npivot ((cid:5)\u03c8 \u2212 \u03c8)/s\u03c8 from normal approximation to the maximum likelihood estimator;\ns\u03c8 is the standard error for (cid:5)\u03c8 based on the observed information matrix. results are\n\n\u2217\n\ngiven for the full dataset, and for a reduced dataset comprising just the nine responses\nin the final column of table 10.1. the shallower slopes in the lower panels show the\neffect of reducing the sample size.\n\nnormal approximation is clearly inadequate, particularly at the upper limits of con-\nfidence intervals, but the ordinary unmodified signed likelihood ratio statistic yields\nintervals for \u03b20 and \u03b21 that are reasonably close to those from the modified version,\nat least for the full dataset. the unmodified estimates of log \u03c3 are strongly biased\ndownwards, and the higher-order procedures give worthwhile improvements. much\nof this bias may be removed by dividing the residual sum of squares by denominator\nn \u2212 p rather than n.\n\nit seems wise to use the modified statistics for all three parameters with the smaller\ndataset. the 0.95 confidence intervals based on r(\u03b21) and r\n(\u03b21) are (2.63, 9.75) and\n(2.31, 12.02), for instance, while the corresponding intervals for \u03c3 are (0.08, 0.55)\n(cid:1)\nand (0.10, 0.94), whose right tails differ substantially.\n\n\u2217\n\n "}, {"Page_number": 692, "text": "680\n\n12 \u00b7 conditional and marginal inference\n\nvariants have been proposed that replace the expectations by averages in order to\n\ngive a purely empirical version of (12.40); see the bibliographic notes.\n\nexercises 12.3\n1\n\n(cid:8)\n\n(cid:7)\n\nlet y and x be independent exponential variables with means 1/(\u03bb + \u03c8) and 1/\u03bb. find\nthe distribution of y given x + y and show that when \u03c8 = 0 ithas mean s/2 and variance\ns2/12. construct an exact conditional test of the hypothesis e(y ) = e(x).\na discrete exponential family for independent pairs (s1, t1), . . . ,( sn, tn) has form\n\ns j \u03bb j + t j \u03c8 j + \u03ba(\u03bb j , \u03c8 j )\n\nexp\n\nc j (s j , t j ),\n\nj = 1, . . . ,n .\n\nit is intended to test the hypothesis \u03c81 = \u00b7\u00b7\u00b7 = \u03c8n regardless of the values of the \u03bb j .\ngeneralize example 12.14 to explain how to perform monte carlo exact significance\ntests. give the acceptance probability for the metropolis\u2013hastings algorithm, and write\nout the algorithm when\n\nc j (s j , t j ) = {(s j \u2212 t j )!t j !}\u22121,\n\ns j = t j , t j + 1, . . . ,\nhow does the argument change if the variables are continuous?\nshow that the saddlepoint approximations for the density and distribution of the average\nof a normal random sample are exact.\nshow that saddlepoint approximation for the inverse gaussian density\n\nt j = 0, 1, . . . .\n\nf (y; \u00b5, \u03bb) =\n\n(cid:16)1/2\n\n(cid:15)\n\n\u03bb\n\n2\u03c0 y3\n\n(cid:7)\u2212\u03bb(y \u2212 \u00b5)2/(2\u00b52 y)\n\n(cid:8)\n\nexp\n\n,\n\ny > 0, \u03bb, \u00b5 > 0,\n\nis exact after renormalization. investigate the accuracy of the distribution function ap-\nproximation (12.23) when n = 1.\n\u22121, for j = 1, . . . ,n ,\nconsider independent exponential observations y j with means (x t\n\u03b2)\nwhere the x j are p \u00d7 1 vectors of explanatory variables and the p \u00d7 1 parameter vector\nj\n\u03b2 is unknown. find the ingredients of (12.37) and (12.38) when inference is required for\n\u03c8 = \u03b21, with \u03bb = (\u03b22, . . . , \u03b2 p) treated as incidental.\ngive the exact conditional likelihood for \u03c8 when y1, . . . ,y n\u22121 have mean \u03bb\u22121 and yn has\nmean (\u03bb + \u03c8)\nshow that (12.40) retrieves (12.39) for a linear exponential family (12.35).\ncheck the details of examples 12.19 and 12.20. find (12.40) when \u03c3 2 is of interest.\n\n\u22121, and investigate the accuracy of (12.38) when n = 2.\n\n2\n\n3\n\n4\n\n5\n\n6\n7\n\n12.4 modified profile likelihood\n12.4.1 likelihood adjustment\nthe profile log likelihood is a standard tool for inference in large-sample situations,\nand it is natural to consider if and how it may be modified for use in small-sample\nproblems. we saw at (12.13), for instance, that a log marginal likelihood for parameters\n\u03c8 controlling the variance of a normal distribution could be obtained by adding a term\nto the profile log likelihood, while (12.37) suggests that an approximate conditional\nlikelihood for a linear exponential family model has form\n\n(cid:9)(\u03c8,(cid:5)\u03bb\u03c8 ) + 1\n\n2 log|j\u03bb\u03bb(\u03c8,(cid:5)\u03bb\u03c8 )|,\n\n "}, {"Page_number": 693, "text": "12.4 \u00b7 modified profile likelihood\nwhere (cid:5)\u03bb\u03c8 is the maximum likelihood estimate of \u03bb for fixed \u03c8 and j\u03bb\u03bb(\u03c8, \u03bb) is\n\n681\n\nthe corner of the observed information matrix corresponding to \u03bb. this amounts to\na penalization of the log likelihood by an amount that depends on the information\navailable for \u03bb; when this is large, the profile log likelihood is more strongly penalized\nthan when it is small.\n\nthe form of these expressions suggests that in general settings we multiply the\n\nprofile likelihood\n\nlp(\u03c8) = exp\n\n(cid:7)\n\n(cid:7)\n\n(cid:8) = exp\n(cid:8)\n(cid:9)(\u03c8,(cid:5)\u03bb\u03c8 )\n(cid:8) = m(\u03c8)lp(\u03c8).\n\n(cid:9)p(\u03c8)\n(cid:7)\n\nlmp(\u03c8) = exp\n\n(cid:9)mp(\u03c8)\n\nby a cunningly chosen function of \u03c8, giving a modified profile likelihood\n\nit is natural to try and choose m(\u03c8) sothat lmp(\u03c8) gives inferences equivalent to\nusing a marginal or conditional likelihood for \u03c8, ifsuch is available. it is a remarkable\nfact that taking\n\nm(\u03c8) = (cid:10)(cid:10)j\u03bb\u03bb(\u03c8,(cid:5)\u03bb\u03c8 )\n\n(cid:10)(cid:10)\u22121/2\n\n(cid:10)(cid:10)(cid:10)(cid:10)(cid:10) \u2202(cid:5)\u03bb\n\u2202(cid:5)\u03bbt\n\n\u03c8\n\n(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)\n\n(12.41)\n\n(12.42)\n\nachieves this to a high degree of accuracy in some generality. the first term of (12.42)\ninvolves the part of the observed information matrix mentioned above, while the\nsecond term is a jacobian needed if the modified profile likelihood is to be invariant\nto interest-preserving transformations: typically this term depends on the parameter\n\u03c8. aderivation of (12.42) is given after example 12.18. here is a toy example.\n\nexample 12.21 (normal linear model) let the parameter of interest in the normal\nlinear model be the variance \u03c3 2, with the p \u00d7 1 vector \u03b2 treated as incidental. the\nlog likelihood is\n\n(cid:9)(\u03b2, \u03c3 2) \u2261 \u2212 n\n2\n\nlog \u03c3 2 \u2212 1\n\n2\u03c3 2 (y \u2212 x\u03b2)t(y \u2212 x\u03b2),\n\nand the maximum likelihood estimator of \u03b2 for \u03c3 2 fixed is(cid:5)\u03b2\u03c3 2 = (x t x)\nthis is independent of \u03c3 2, we have(cid:5)\u03b2 =(cid:5)\u03b2\u03c3 2. thus\n\n\u22121 x t y. as\n\nj\u03b2\u03b2(\u03b2, \u03c3 2) = \u03c3 \u22122 x t x,\n\n= i p, m(\u03c3 2) = (\u03c3 2) p/2|x t x|\u22121/2,\n\n\u2202(cid:5)\u03b2 t\n\u2202(cid:5)\u03b2\n\n\u03c3 2\n\nand the modified profile log likelihood is\n\n(cid:9)mp(\u03c3 2) \u2261 \u2212 n \u2212 p\n= \u2212 n \u2212 p\n\n2\n\n2\u03c3 2 (y \u2212 x(cid:5)\u03b2)t(y \u2212 x(cid:5)\u03b2)\n\nlog \u03c3 2 \u2212 1\n(cid:17)\nlog \u03c3 2 \u2212 s2/\u03c3 2\n\n(cid:18)\n\n,\n\n2\n\nwhere s2 is the unbiased estimator of \u03c3 2. hence (12.41) produces inferences iden-\ntical to those based on the marginal distribution of the residual sum of squares, or\nequivalently the marginal likelihood for \u03c3 2; see example 12.9.\n\n\u03c3 2 /\u2202(cid:5)\u03b2| =1 inall regression-scale models (exercise 12.4.2).\n\nin fact |\u2202(cid:5)\u03b2 t\n\n(cid:1)\n\n "}, {"Page_number": 694, "text": "682\n\n12 \u00b7 conditional and marginal inference\n\ncomputation of (12.41) demands knowledge of the two terms of (12.42). in appli-\ncations the first is easily found by numerical or analytical differentation of (cid:9)(\u03c8, \u03bb), but\nthe second is a sample space derivative and approximations to it are usually needed.\nwhen the log likelihood can be written in terms of the maximum likelihood estimates\n\n(cid:5)\u03c8 and(cid:5)\u03bb and an ancillary statistic a, however, the equation determining(cid:5)\u03bb\u03c8 may be\n\nexpressed as\n\n\u2202(cid:9)(\u03c8,(cid:5)\u03bb\u03c8 ;(cid:5)\u03c8 ,(cid:5)\u03bb, a)\n\n= 0,\n\n(12.43)\n\nand partial differentiation with respect to(cid:5)\u03bb, holding \u03c8, (cid:5)\u03c8, and a fixed, yields\n\n\u2202\u03bb\n\n\u03c8\n\n\u2202\u03bb\u2202\u03bbt\n\n\u2202(cid:5)\u03bbt\n\u2202 2(cid:9)(\u03c8,(cid:5)\u03bb\u03c8 ;(cid:5)\u03c8 ,(cid:5)\u03bb, a)\n\u2202(cid:5)\u03bb\n(cid:10)(cid:10)(cid:10)(cid:10)(cid:10) = (cid:10)(cid:10)j\u03bb\u03bb(\u03c8,(cid:5)\u03bb\u03c8 ;(cid:5)\u03c8 ,(cid:5)\u03bb, a)\n\n\u2202\u03bb\u2202(cid:5)\u03bbt\n\n+ \u2202 2(cid:9)(\u03c8,(cid:5)\u03bb\u03c8 ;(cid:5)\u03c8 ,(cid:5)\u03bb, a)\n(cid:10)(cid:10)(cid:10)(cid:10) \u2202 2(cid:9)(\u03c8,(cid:5)\u03bb\u03c8 ;(cid:5)\u03c8 ,(cid:5)\u03bb, a)\n\n\u2202\u03bb\u2202(cid:5)\u03bbt\n\n(cid:10)(cid:10)\u22121\n\n(cid:10)(cid:10)(cid:10)(cid:10) .\n\n= 0.\n\n(cid:10)(cid:10)(cid:10)(cid:10)(cid:10) \u2202(cid:5)\u03bbt\n\u2202(cid:5)\u03bb\n\n\u03c8\n\nthis gives the alternative and often more convenient expression\n\nalthough the ancillary must be held fixed when differentiating with respect to(cid:5)\u03bb, it is\n\nnot needed explicitly, and in some cases (12.43) can be obtained without the burden\nof specifying a.\n\nexample 12.22 (linear exponential family)\nlog likelihood expressed as\n\nin a linear exponential family with\n\nthere is no ancillary statistic and the maximum likelihood estimates (cid:5)\u03c8 and (cid:5)\u03bb are\n\n1\n\n2\n\n(cid:9)(\u03c8, \u03bb) \u2261 t t\n\n\u03c8 + t t\n\n\u03bb \u2212 \u03ba(\u03c8, \u03bb)\n\nsolutions of the equations\n\nt1 = \u03ba\u03c8 ((cid:5)\u03c8 ,(cid:5)\u03bb),\n\nt2 = \u03ba\u03bb((cid:5)\u03c8 ,(cid:5)\u03bb).\n\nthus the log likelihood may be written\n\n(cid:9)(\u03c8, \u03bb;(cid:5)\u03c8 ,(cid:5)\u03bb) = \u03ba\u03c8 ((cid:5)\u03c8 ,(cid:5)\u03bb)t\u03c8 + \u03ba\u03bb((cid:5)\u03c8 ,(cid:5)\u03bb)t\u03bb \u2212 \u03ba(\u03c8, \u03bb),\n\nand (12.43) implies that\n\n(cid:10)(cid:10)(cid:10)(cid:10) = (cid:10)(cid:10)\u03ba\u03bb\u03bb(\u03c8,(cid:5)\u03bb\u03c8 )\n\n(cid:10)(cid:10)(cid:10)(cid:10) \u2202(cid:5)\u03bb\u03c8\n\u2202(cid:5)\u03bb\n\n(cid:10)(cid:10)\u22121\n\n(cid:10)(cid:10)\u03ba\u03bb\u03bb((cid:5)\u03c8 ,(cid:5)\u03bb)\n\nthus the modified profile log likelihood for \u03c8 is\n\n(cid:9)mp(\u03c8) \u2261 (cid:9)p(\u03c8) + 1\n\n(cid:10)(cid:10)j\u03bb\u03bb((cid:5)\u03c8 ,(cid:5)\u03bb;(cid:5)\u03c8 ,(cid:5)\u03bb)\n(cid:10)(cid:10)\u22121\n(cid:10)(cid:10) = (cid:10)(cid:10)j\u03bb\u03bb(\u03c8,(cid:5)\u03bb\u03c8 ;(cid:5)\u03c8 ,(cid:5)\u03bb)\n(cid:10)(cid:10) ,\n(cid:10)(cid:10)j\u03bb\u03bb(\u03c8,(cid:5)\u03bb\u03c8 ;(cid:5)\u03c8 ,(cid:5)\u03bb)\n\n2 log\n\n(cid:10)(cid:10) .\n\nwhere terms independent of \u03c8 have been neglected. hence (12.41) and (12.42) do\nindeed retrieve the approximate conditional likelihood (12.37), apart from constants\n(cid:1)\nof proportionality.\n\ninference on \u03c8 is performed by treating (12.41) as a likelihood. its maximizing\n\nvalue (cid:5)\u03c8mp yields a confidence interval for \u03c8, for example by normal approximation\n\n "}, {"Page_number": 695, "text": "12.4 \u00b7 modified profile likelihood\nwith variance based on the second derivative of (cid:9)mp(\u03c8) at (cid:5)\u03c8mp, or bychi-squared ap-\nproximation to the distribution of 2{(cid:9)mp((cid:5)\u03c8mp) \u2212 (cid:9)mp(\u03c8)}. the maximum likelihood\nestimator based on lmp typically has better properties than(cid:5)\u03c8, as isthe case in exam-\n\n683\n\nple 12.21, but likelihood modification is not a universal panacea. one difficulty is that\nthe underlying inferential basis is first-order distributional results such as normal ap-\nproximation to the distribution of the maximum modified likelihood estimator rather\nthan more accurate forms such as (12.38). another difficulty is that modification may\nnot be enough to remove inconsistency of maximum likelihood estimators.\n\nexample 12.23 (binary matched pairs) consider matched pairs of binary obser-\nvations r0 j , r1 j , with success probabilities\n\n\u03c00 j = e\u03bb j\n1 + e\u03bb j\n\n, \u03c01 j = e\u03bb j+\u03c8\n1 + e\u03bb j+\u03c8\n\nj = 1, . . . ,n .\n\n,\n\nstatistic s1, . . . , sn, t , where s j = r0 j + r1 j is associated with \u03bb j and t = (cid:4)\nthis logistic regression model is a linear exponential family with minimal sufficient\nr1 j\nwith \u03c8. wesuppose that \u03c8 is finite, and compare its maximum likelihood estimators\nbased on the conditional, modified profile, and usual likelihoods as n \u2192 \u221e.\npairs for which s j = 0 or 2are uninformative (problem 12.11), and we suppose\nthat they have already been dropped from the analysis, so all n pairs are discordant,\nthat is, s1 = \u00b7\u00b7\u00b7 = sn = 1.\nt = (cid:4)\nthe exact conditional likelihood for \u03c8 is obtained from the conditional density of\nr1 j given that s1 = \u00b7\u00b7\u00b7 = sn = 1. conditional on s j = 1, r1 j is a bernoulli\nvariable with success probability \u03c0 = e\u03c8 /(1 + e\u03c8 ), so t has a binomial distribution\nwith denominator n and probability \u03c0. thus the conditional log likelihood is (cid:9)c(\u03c8) \u2261\nt \u03c8 \u2212 n log(1 + e\u03c8 ), which is maximized at (cid:5)\u03c8c = log{t /(n \u2212 t )}\np\u2212\u2192 \u03c8 as n \u2192\n\u221e: hence (cid:5)\u03c8c is consistent.\n\nthe overall log likelihood is\n\n(cid:9)(\u03c8, \u03bb) \u2261 t \u03c8 + n(cid:1)\n\n(cid:7)\n\n\u03bb j \u2212 log(1 + e\u03bb j ) \u2212 log(1 + e\u03c8+\u03bb j )\n\n(cid:8)\n\n,\n\nj=1\n\nand the values of \u03bb j that maximise (cid:9)(\u03c8, \u03bb) for fixed \u03c8 all equal(cid:5)\u03bb\u03c8 = \u2212 1\n\n\u03c8. thus\n\n2\n\nthe profile log likelihood is\n\n(cid:9)p(\u03c8) = t \u03c8 \u2212 2n log(1 + e\u03c8/2).\n\nit is straightforward to see that the maximum likelihood estimator (cid:5)\u03c8 p\u2212\u2192 2\u03c8 as\nn \u2192 \u221e (problem 12.11). thus (cid:5)\u03c8 is inconsistent.\ndifferentiation of (cid:9)(\u03c8, \u03bb) establishes that j\u03bb\u03bb(\u03c8,(cid:5)\u03bb\u03c8 ) is an \u00d7 n matrix with ele-\nments 2\u03b3 /(1 + \u03b3 )2 on the diagonal and zeros elsewhere, where \u03b3 = exp(\u03c8/2). hence\nthe modified profile log likelihood is\n\n(cid:9)mp(\u03c8) \u2261 1\n\n4 (n + 4t )\u03c8 \u2212 3n log(1 + e\u03c8/2),\n\n "}, {"Page_number": 696, "text": "684\n\nd\no\no\nh\n\ni\nl\n\ne\nk\n\ni\nl\n \n\ng\no\nl\n\n.\n\n0\n0\n\n5\n\n.\n\n0\n-\n\n0\n\n.\n\n1\n-\n\n5\n\n.\n\n1\n-\n\n0\n\n.\n\n2\n-\n\n.\n\n5\n2\n-\n\n.\n\n0\n3\n-\n\n12 \u00b7 conditional and marginal inference\n\n\u03c8\n\nlimit of (cid:5)\u03c8c\nlimit of (cid:5)\u03c8mp\nlimit of (cid:5)\u03c8\n\n0\n\n0\n0\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\n4\n\n5\n\n0.5\n0.66\n1\n\n1\n1.27\n2\n\n1.5\n1.81\n3\n\n2\n2.23\n4\n\n2.5\n2.56\n5\n\n3\n2.79\n6\n\n4\n3.05\n8\n\n5\n3.16\n10\n\ntable 12.3 probability\nlimits of ordinary,\nconditional, and\nmaximum modified\nlikelihood estimators of\nlog odds ratio \u03c8 in binary\nmatched pairs.\n\nd\no\no\nh\n\ni\nl\n\ne\nk\n\ni\nl\n \n\ng\no\nl\n\n.\n\n0\n0\n\n5\n\n.\n\n0\n-\n\n0\n\n.\n\n1\n-\n\n5\n\n.\n\n1\n-\n\n0\n\n.\n\n2\n-\n\n.\n\n5\n2\n-\n\n.\n\n0\n3\n-\n\nd\no\no\nh\n\ni\nl\n\ne\nk\n\ni\nl\n \n\ng\no\nl\n\n.\n\n0\n0\n\n5\n\n.\n\n0\n-\n\n0\n\n.\n\n1\n-\n\n5\n\n.\n\n1\n-\n\n0\n\n.\n\n2\n-\n\n.\n\n5\n2\n-\n\n.\n\n0\n3\n-\n\n-2\n\n0\n\n2\n\n4\n\n6\n\n8\n\n-2\n\n0\n\n2\n\n4\n\n6\n\n8\n\n-2\n\n0\n\n2\n\n4\n\n6\n\n8\n\npsi\n\npsi\n\npsi\n\nfigure 12.10\nlikelihood inference for\nnodal involvement data.\nleft panel: conditional\n(heavy), modified profile\n(solid) and profile log\nlikelihoods (dots) for\nmodel with terms\n1+acid. centre and right\npanels: corresponding log\nlikelihoods with terms\n1+stage+xray+acid\nand with all five\ncovariates.\n\nwhich is maximized at (cid:5)\u03c8mp = 2 log{(1 + 4t /n)/(5 \u2212 4t /n)}. now t /n p\u2212\u2192 \u03c0 as\nn \u2192 \u221e, so (cid:5)\u03c8mp\ntable 12.3 compares these limiting values. although (cid:5)\u03c8mp is inconsistent, it is not\nhorribly biased in the range |\u03c8| < 3 usually met in applications: the modification is\n(cid:1)\npartly but not wholly successful in eliminating bias.\n\np\u2212\u2192 2 log{(1 + 5e\u03c8 )/(5 + e\u03c8 )}.\n\nexample 12.24 (nodal involvement data) for numerical illustration of likelihood\nmodification we fit logistic regression models to the rows of table 10.8 with m = 2 or\nm = 1: this gives 17 binary responses in all. we let \u03c8 be the parameter corresponding\nto acid, and fit models with terms 1+acid, 1+stage+xray+acid, and with all five\ncovariates; thus the nuisance parameter \u03bb has dimensions q = 1, 3, and 5.\n\nfigure 12.10 shows the conditional, modified profile, and profile log likelihoods\nfor these three models. the conditional likelihoods were obtained by symbolic com-\nputation of the necessary generating functions; see example 12.13. the profile log\nlikelihoods are quite different from the conditional and modified profile log likeli-\nhoods. the effect of modification depends on the number of nuisance parameters;\nit works almost perfectly when q = 1 but less well otherwise. in the right panel\nfive nuisance parameters are eliminated from a likelihood based on just 17 binary\nobservations, and it is perhaps surprising that modification works so well.\n\ntable 12.4 compares the corresponding estimates and confidence intervals. the\ndifference between results based on the profile likelihood and conditional and modified\nprofile likelihoods increases sharply with the number of nuisance parameters, but the\n(cid:1)\ndifference between these two last sets of results remains modest.\n\n "}, {"Page_number": 697, "text": "12.4 \u00b7 modified profile likelihood\n\n685\n\ntable 12.4 estimates,\nstandard errors, and 0.95\nconfidence intervals based\non profile, modified\nprofile and conditional\nlikelihoods for reduced\nnodal data having 17\nbinary responses, with\nq = 1, 3 and 5 nuisance\nparameters.\n\nestimate (se)\n\nconfidence interval\n\nprofile\n\nmodified\n\nprofile\n\nmodified\n\nconditional\n\n1.79 (1.08)\n1.72 (1.13)\n2.83 (1.61)\n\n1.68 (1.04)\n1.43 (1.02)\n1.91 (1.15)\n\n(\u22120.21, 4.15)\n(\u22120.41, 4.21)\n(0.13, 6.90)\n\n(\u22120.28, 3.86)\n(\u22120.54, 3.51)\n(\u22120.39, 4.57)\n\n(\u22120.26, 3.92)\n(\u22120.53, 3.48)\n(\u22120.44, 4.71)\n\nq\n\n1\n3\n5\n\nderivation of (12.42)\nconsider a model with interest and nuisance parameters \u03c8 and \u03bb, and for which the\n\ndata y (cid:10)\u2192 ((cid:5)\u03c8 ,(cid:5)\u03bb, a), where a is ancillary. if a factorization\n\nf ((cid:5)\u03c8 ,(cid:5)\u03bb | a; \u03c8, \u03bb) = f ((cid:5)\u03c8 | a; \u03c8) f ((cid:5)\u03bb | (cid:5)\u03c8 , a; \u03c8, \u03bb)\n\n(12.44)\n\n(12.45)\n\u22121). to approximate to\n\nholds, then a marginal likelihood for \u03c8 may be based on the first term on the right.\nwe now apply the p\n\n\u2217\n\n,\n\n\u2217\n\n(cid:7)\n\n(2\u03c0)\n\nand apply the p\n\n(cid:10)(cid:10)1/2 exp\n\n(cid:10)(cid:10)j ((cid:5)\u03c8 ,(cid:5)\u03bb)\n\n\u2212 p/2c1(\u03c8, \u03bb, a)\n\nformula to the left-hand density, giving\n\nformula to the density on the right, giving\n\n(cid:8)\n(cid:9)(\u03c8, \u03bb) \u2212 (cid:9)((cid:5)\u03c8 ,(cid:5)\u03bb)\nwhere (\u03c8, \u03bb) has dimension p and c1(\u03c8, \u03bb, a) = 1 + o p(n\nf ((cid:5)\u03bb | (cid:5)\u03c8 , a; \u03c8, \u03bb), we note that with \u03c8, (cid:5)\u03c8, and a held fixed we have\n(cid:10)(cid:10)(cid:10)(cid:10) ,\n(cid:10)(cid:10)(cid:10)(cid:10) \u2202(cid:5)\u03bb\u03c8\nf ((cid:5)\u03bb | (cid:5)\u03c8 , a; \u03c8, \u03bb) = f ((cid:5)\u03bb\u03c8 | (cid:5)\u03c8 , a; \u03c8, \u03bb)\n\u2202(cid:5)\u03bb\n(cid:10)(cid:10)(cid:10)(cid:10) ,\n(cid:8)(cid:10)(cid:10)(cid:10)(cid:10) \u2202(cid:5)\u03bb\u03c8\n(cid:9)(\u03c8, \u03bb) \u2212 (cid:9)(\u03c8,(cid:5)\u03bb\u03c8 )\n\u2202(cid:5)\u03bb\nwhere q is the dimension of \u03bb and c2(\u03c8, \u03bb, a) = 1 + o p(n\nand (12.46) into (12.44) and rearranging we find that f ((cid:5)\u03c8 | a; \u03c8) equals\n(cid:8)(cid:10)(cid:10)(cid:10)(cid:10) \u2202(cid:5)\u03bb\n(cid:9)(\u03c8,(cid:5)\u03bb\u03c8 ) \u2212 (cid:9)((cid:5)\u03c8 ,(cid:5)\u03bb)\n\u2202(cid:5)\u03bb\u03c8\nwhere c(\u03c8, \u03bb, a) = c1(\u03c8, \u03bb, a)/c2(\u03c8, \u03bb, a) = 1 + o(n\n\u22121). thus\n(cid:8)\n(cid:7)\n\u22121)\n\n(cid:10)(cid:10)j ((cid:5)\u03c8 ,(cid:5)\u03bb)\n(cid:10)(cid:10)1/2\n(cid:10)(cid:10)1/2 exp\n(cid:10)(cid:10)j\u03bb\u03bb(\u03c8,(cid:5)\u03bb\u03c8 )\n(cid:7)\n(cid:8)\n\nf ((cid:5)\u03c8 | a; \u03c8) \u221d exp\n\n(cid:10)(cid:10)j\u03bb\u03bb(\u03c8,(cid:5)\u03bb\u03c8 )\n\n\u2212( p\u2212q)/2c(\u03c8, \u03bb, a)\n\n\u2212q/2c2(\u03c8, \u03bb, a)\n\n(cid:10)(cid:10)1/2 exp\n\n1 + o(n\n\nm(\u03c8)\n\n(2\u03c0)\n\n(2\u03c0)\n\n(cid:7)\n\n(cid:7)\n\n,\n\n(cid:9)p(\u03c8)\n\n(12.46)\n\u22121). on substituting (12.45)\n\n(cid:10)(cid:10)(cid:10)(cid:10) ,\n\n(12.47)\n\n\u22123/2) in amoderate deviation region, that is, when (cid:5)\u03c8 differs from the true \u03c8\n\nas a function of \u03c8, with m(\u03c8) given by (12.42). it can be shown that the error term\nis o(n\nby only o(n\n\n\u22121/2).\n\nexercise 12.4.4 derives (12.47) as an approximate conditional likelihood.\n\nthis section demands a\nnodding acquaintance\nwith partial differential\nequations.\n\n12.4.2 parameter orthogonality\nin applications it is rarely possible to find an explicit expression for the second term\nof (12.42). approximations to it are available in certain cases, but when they are not,\n\n "}, {"Page_number": 698, "text": "686\n\nalthough this occurs only in particular models, it suggests that we seek to reduce the\n\n12 \u00b7 conditional and marginal inference\nit is natural to seek to reduce the importance of that term. if(cid:5)\u03bb\u03c8 is independent of\n\u03c8, then(cid:5)\u03bb\u03c8 =(cid:5)\u03bb for all \u03c8, and |\u2202(cid:5)\u03bb/\u2202(cid:5)\u03bb\u03c8| plays no part in the likelihood modification.\ndependence of(cid:5)\u03bb\u03c8 on \u03c8 more generally. one approach to this is through orthogonal\nto motivate subsequent discussion, let (cid:9) = n\n\u22121(cid:9) denote the log likelihood, stan-\ndardized to be of order one, and note that (cid:5)\u03bb\u03c8 is determined by the equation\n(cid:9)\u03bb(\u03c8,(cid:5)\u03bb\u03c8 ) = 0. if we suppose that (cid:5)\u03c8 \u2212 \u03c8 = o(n\n\u22121/2), then taylor series expansion\n\nparameters.\n\naround the overall maximum likelihood estimator gives\n\n0 = (cid:9)\u03bb((cid:5)\u03c8 ,(cid:5)\u03bb) + (cid:9)\u03bb\u03c8 ((cid:5)\u03c8 ,(cid:5)\u03bb)(\u03c8 \u2212 (cid:5)\u03c8) + (cid:9)\u03bb\u03bb((cid:5)\u03c8 ,(cid:5)\u03bb)((cid:5)\u03bb\u03c8 \u2212(cid:5)\u03bb) + o p(n\n\u22121),\nwhere second and higher derivatives such as (cid:9)\u03bb\u03c8 ((cid:5)\u03c8 ,(cid:5)\u03bb) are o p(1). hence\n\n(cid:5)\u03bb\u03c8 \u2212(cid:5)\u03bb = (cid:9)\u03bb\u03bb((cid:5)\u03c8 ,(cid:5)\u03bb)\n= j\u03bb\u03bb((cid:5)\u03c8 ,(cid:5)\u03bb)\n= i\u03bb\u03bb(\u03c8, \u03bb)\n\n\u22121(cid:9)\u03bb\u03c8 ((cid:5)\u03c8 ,(cid:5)\u03bb)((cid:5)\u03c8 \u2212 \u03c8) + o p(n\n\u22121)\n\u22121 j\u03bb\u03c8 ((cid:5)\u03c8 ,(cid:5)\u03bb)((cid:5)\u03c8 \u2212 \u03c8) + o p(n\n\u22121)\n\u22121 i\u03bb\u03c8 (\u03c8, \u03bb)((cid:5)\u03c8 \u2212 \u03c8) + o p(n\n\u22121),\n\nwhere i\u03bb\u03bb and i\u03bb\u03c8 are components of the expected information matrix. in regular\nmodels these and the corresponding observed information quantities are of order n,\n\nso(cid:5)\u03bb\u03c8 \u2212(cid:5)\u03bb will be of precise order n\n\u22121/2 unless the model is set up so that j\u03bb\u03c8 ((cid:5)\u03c8 ,(cid:5)\u03bb)\nor i\u03bb\u03c8 (\u03c8, \u03bb) vanishes. if this can be arranged, then(cid:5)\u03bb\u03c8 differs from(cid:5)\u03bb by less than\n\u22121/2) and the asymptotic dependence of(cid:5)\u03bb\u03c8 on \u03c8 is reduced.\no p(n\nto be more explicit, suppose i\u03bb\u03c8 (\u03c8, \u03bb) = 0 atthe true parameter value. then\n(cid:5)\u03bb =(cid:5)\u03bb\u03c8 + o p(n\n\u03c8| that appears in (12.42)\nequals 1 + o p(n\n\u22121/2) typically obtained. it then\nseems reasonable to hope that little damage will be done by dropping the jacobian\n(cid:10)(cid:10) .\nterm from (12.42) and approximating (cid:9)mp(\u03c8) by\n\n\u22121), and it follows that the term |\u2202(cid:5)\u03bb/\u2202(cid:5)\u03bbt\n\u22121), in contrast to the value 1 + o p(n\n(cid:10)(cid:10)j\u03bb\u03bb(\u03c8,(cid:5)\u03bb\u03c8 )\n(12.48)\n\u22121)\nthis argument has a serious drawback, because knowledge that a term is o p(n\n\u22121) both when c = 100 and c = 0.01, but\ngives no notion of its actual size: c\u03c8/n is o(n\nthe numerical values are very different. asymptotic arguments are valuable heuristics\nbut cannot ensure accuracy in applications. with this in mind, we nevertheless press\nforward with vigour.\nif i\u03bb\u03c8 (\u03c8, \u03bb) = 0 for all (\u03c8, \u03bb), then the parameters \u03bb and \u03c8 are said to be or-\nblock diagonal, so the maximum likelihood estimators (cid:5)\u03c8 and(cid:5)\u03bb are asymptotically\nthogonal. among the consequences of this is that the inverse information matrix is\nindependent, and the asymptotic standard error for (cid:5)\u03c8 when \u03bb is unknown is the same\n\n(cid:9)(\u03c8,(cid:5)\u03bb\u03c8 ) \u2212 1\n\n2 log\n\nas when it is known. another advantage of is that likelihood maximization may be\nnumerically more stable in the orthogonal parametrization. below we briefly consider\nthe implications of parameter orthogonality for likelihood modification, first outlining\nhow to obtain parameters orthogonal to a given interest parameter.\nconsider a model with log likelihood (cid:9)\u2217\n(\u03c8, \u03b3 ) interms of the scalar interest param-\neter \u03c8 and an arbitrary nuisance parameter \u03b3 = (\u03b31, . . . , \u03b3q)t. weseek \u03bb = \u03bb(\u03c8, \u03b3 )\n\n "}, {"Page_number": 699, "text": "12.4 \u00b7 modified profile likelihood\nsuch that \u03bb is orthogonal to \u03c8. writing \u03b3 = \u03b3 (\u03c8, \u03bb), we have\n\n(cid:9)(\u03c8, \u03bb) = (cid:9)\u2217 {\u03c8, \u03b3 (\u03c8, \u03bb)} ,\n\n687\n\nand differentiation with respect to \u03c8 and \u03bb yields\n\u2202 2(cid:9)\u2217\n\u2202\u03b3 \u2202\u03b3 t\n\n\u2202 2(cid:9)\u2217\n\u2202\u03b3 \u2202\u03c8\n\n= \u2202\u03b3 t\n\u2202\u03bb\n\n+ \u2202\u03b3 t\n\u2202\u03bb\n\n\u2202\u03bb\u2202\u03c8\n\n\u2202 2(cid:9)\n\n\u2202\u03b3\n\n\u2202\u03c8\n\n+ \u2202 2\u03b3 t\n\u2202\u03bb\u2202\u03c8\n\n\u2202(cid:9)\u2217\n\u2202\u03b3\n\n.\n\nif \u03bb and \u03c8 are to be orthogonal, this expression must have expectation zero. hence\n\n0 = \u2202\u03b3 t\n\n\u2202\u03bb\n\n\u03b3 \u03c8 + \u2202\u03b3 t\n\u2217\n\n\u2202\u03bb\n\ni\n\n\u2217\n\u03b3 \u03b3\n\ni\n\n\u2202\u03b3\n\n\u2202\u03c8\n\n,\n\n\u2217\n\u03b3 \u03c8 and i\n\n\u2217\nwhere i\n\u03b3 \u03b3 are components of the expected information matrix in the non-\northogonal parametrization. thus provided that the q \u00d7 q matrix \u2202\u03b3 t/\u2202\u03bb is invertible\nand taking for granted the necessary regularity conditions, we see that \u03bb(\u03c8, \u03b3 ) is a\nsolution of the system of q partial differential equations\n\n= \u2212i\n\n\u2217\u22121\n\u03b3 \u03b3 (\u03c8, \u03b3 )i\n\n\u2217\n\u03b3 \u03c8 (\u03c8, \u03b3 ).\n\n\u2202\u03b3\n\n\u2202\u03c8\n\n(12.49)\n\n\u2217\n\nthere is latitude in the choice of \u03bb, because if \u03bb and \u03c8 are orthogonal, then any\nsmooth functions of \u03c8 and of \u03bb are orthogonal. thus \u03bb can in some cases be chosen\nto have a desirable property such as directness of interpretation.\nwhen the data are a random sample of size n, the expected information equals\n(\u03c8, \u03b3 ) = ni\n(\u03c8, \u03b3 ), and the partial differential equation (12.49) may be expressed\ni\nusing the information matrix i\nthis, write \u03b3 = \u03b3 (\u03c8, \u03bb) and note that\n\u2202(cid:9)\u2217\n\nit is not necessary to find \u03bb in terms of \u03c8 and \u03b3 in order to obtain (12.48). to see\n\n(\u03c8, \u03b3 ) for a single observation.\n\n\u2217\n\n\u2217\n\n\u2202(cid:9)(\u03c8, \u03bb)\n\n\u2202\u03bb\n\n\u2202 2(cid:9)(\u03c8, \u03bb)\n\n\u2202\u03bb\u2202\u03bbt\n\n= \u2202\u03b3 t\n\u2202\u03bb\n= \u2202 2\u03b3 t\n\u2202\u03bb\u2202\u03bbt\n\n,\n\n(\u03c8, \u03b3 )\n\u2202\u03b3\n\u2202(cid:9)\u2217\n\n(\u03c8, \u03b3 )\n\u2202\u03b3\n\n+ \u2202\u03b3 t\n\u2202\u03bb\n\n\u2202 2(cid:9)\u2217\n\n(\u03c8, \u03b3 )\n\n\u2202\u03b3\n\n\u2202\u03b3 \u2202\u03b3 t\n\n\u2202\u03bbt\n\n.\n\nif the maximum likelihood estimates of \u03bb and \u03b3 for fixed \u03c8 are(cid:5)\u03bb\u03c8 and(cid:5)\u03b3\u03c8 , then\n\nprovided |\u2202\u03b3 /\u2202\u03bbt| (cid:13)= 0, we have\n\nand so (12.48) equals\n\n.\n\n\u2217\n\nj\n\n\u2202\u03bb\n\n\u2202\u03bbt\n\nj\u03bb\u03bb(\u03c8,(cid:5)\u03bb\u03c8 ) = \u2202\u03b3 (\u03c8,(cid:5)\u03bb\u03c8 )t\n(cid:10)(cid:10)(cid:10)(cid:10) =\n\n(cid:10)(cid:10)(cid:10)(cid:10) \u2202\u03b3 (\u03c8,(cid:5)\u03bb\u03c8 )\n(cid:10)(cid:10)j\n\u03b3 \u03b3 (\u03c8,(cid:5)\u03b3\u03c8 )\n\n\u2202\u03b3 (\u03c8,(cid:5)\u03bb\u03c8 )\n\u03b3 \u03b3 (\u03c8,(cid:5)\u03b3\u03c8 )\n(cid:10)(cid:10)(cid:10)(cid:10)\u22121\n(cid:10)(cid:10)(cid:10)(cid:10) \u2202\u03bb(\u03c8,(cid:5)\u03b3\u03c8 )\n(cid:10)(cid:10)(cid:10)(cid:10) \u2202\u03bb(\u03c8,(cid:5)\u03b3\u03c8 )\n(cid:10)(cid:10) + log\n\n(\u03c8,(cid:5)\u03b3\u03c8 ) \u2212 1\n\n\u2202\u03b3 t\n\n\u2202\u03bbt\n\n\u2217\n\n,\n\n2 log\n\n\u2202\u03b3 t\n\n(cid:10)(cid:10)(cid:10)(cid:10) ,\n\n(cid:9)\u2217\n\nwhich can be computed without writing \u03bb explicitly in terms of \u03c8 and \u03b3 .\n\n(12.50)\n\n "}, {"Page_number": 700, "text": "688\n\nr\ne\n\nt\n\ne\nm\na\nr\na\np\n\n5\n1\n\n0\n1\n\n5\n\n0\n\n\u2022\n\n\u2022\n\n12 \u00b7 conditional and marginal inference\n\nd\no\no\nh\n\ni\nl\n\ne\nk\n\ni\nl\n \n\ng\no\nl\n\n0\n\n1\n-\n\n2\n-\n\n3\n-\n\n4\n-\n\n0.0\n\n0.5\n\n1.0\n\n1.5\n\n200\n\n400\n\n600\n\n800 1000\n\nxi\n\npsi\n\nexample 12.25 (generalized pareto distribution) the expected information ma-\ntrix for an observation with distribution function (6.38) is\n\n\u2217\n\n(\u03be, \u03c3 ) =\n\ni\n\n1\n\n\u03c3 2(1 + \u03be)(1 + 2\u03be)\n\n2\u03c3 2\n\u03c3\n\n\u03c3\n\n1 + \u03be\n\n(cid:15)\n\n(cid:16)\n\n.\n\nhence a parameter \u03bb = \u03bb(\u03be, \u03c3 ) orthogonal to the shape parameter \u03be satisfies the\npartial differential equation\n\n\u2202\u03c3\n\n= \u2212i\n\n\u03c3 \u03be (\u03be, \u03c3 ) = \u2212 \u03c3\n\u2217\n1 + \u03be\nit is straightforward to check that this implies that g(\u03bb) = \u03c3 (1 + \u03be) for a suitably\nsmooth function g. with the choice \u03bb = \u03c3 (1 + \u03be) wefind that\n\n\u2217\u22121\n\u03c3 \u03c3 (\u03be, \u03c3 )i\n\n\u2202\u03be\n\n.\n\n(cid:15)\n\ni(\u03be, \u03bb) =\n\n1\n\n\u03bb2(1 + \u03be)2(1 + 2\u03be)\n\n\u03bb2(1 + 2\u03be)\n\n0\n\n0\n\n(1 + \u03be)2\n\n(cid:16)\n\n.\n\nfigure 12.11\nlikelihood analysis of\ndanish fire data. left:\n\nvariation of(cid:5)\u03c3\u03be (solid) and\n(cid:5)\u03bb\u03be (dashes) as a function\n\nof the shape parameter \u03be.\nthe blobs show the\nmaximum likelihood\nestimates. the dotted line\nis the profile log\nlikelihood (cid:9)p(\u03be) and the\nvertical lines mark the\nlimits of a 0.99 confidence\ninterval for \u03be. the\n\northogonal parameter(cid:5)\u03bb\u03be\nparameter(cid:5)\u03c3\u03be over the\n\nvaries much less than does\nthe non-orthogonal\n\nrange of \u03c8 considered.\nright: profile log\nlikelihood (solid) and\nmodified profile log\nlikelihood (dashes) for\n0.99 quantile \u03c8 of\ngeneralized pareto\ndistribution. the\nhorizontal line determines\nthe limits of a 0.95\nconfidence interval for \u03c8.\n\nwe illustrate numerically the effect of parameter orthogonality using the danish\nfire insurance claim data described in examples 6.31 and 6.34. we apply a threshold\nu = 15 to the claim sizes and fit the generalized pareto distribution to the result-\ning 60 exceedances. the left panel of figure 12.11 shows that (cid:5)\u03c3\u03be varies more over\nthe range of appreciable likelihood for \u03be than does(cid:5)\u03bb\u03be =(cid:5)\u03c3\u03be (1 + \u03be), as our general\nnow suppose that interest focuses on the (1 \u2212 p) quantile of the distribution,\n\ndiscussion anticipates.\n\na return level, to which we seek an orthogonal parameter \u03bb = \u03bb(\u03c8, \u03be). as\n\n(cid:6)\n\n\u03c8 =\n\n\u2212\u03be \u2212 1)/\u03be,\n\n\u03c3 ( p\n\u2212\u03c3 log p,\n\n\u03be (cid:13)= 1,\n\u03be = 0,\n\n\u2217\n\ni\n\n(\u03be, \u03c8) = \u2202(\u03be, \u03c3 )t\n\u2202(\u03be, \u03c8)\n\n\u2217\n\ni\n\n(\u03be, \u03c3 )\n\n\u2202(\u03be, \u03c3 )\n\u2202(\u03be, \u03c8)t\n\n(cid:15)\n\n\u2217\n\n(\u03be, \u03c8) may be written as\n\u2212\u03c8a(\u03be)/b(\u03be)\n\n\u03c8 2a(\u03be)\n\n\u2212\u03c8a(\u03be)/b(\u03be)\n\nc(\u03be)\n\n(cid:10)(cid:10)(cid:10)(cid:10)\n\n,\n\n\u03c3=\u03c3 (\u03be,\u03c8)\n(cid:16)\n\n,\n\na tedious calculation shows that i\n\n "}, {"Page_number": 701, "text": "12.4 \u00b7 modified profile likelihood\nand so \u03bb = \u03bb(\u03c8, \u03be) solves the equation \u03c8b(\u03be)\u2202\u03be /\u2202\u03c8 = 1. thus we may take\n\n(cid:6)\n\n(cid:20) \u03be\n\n(cid:19)\n\n\u03bb = g\n\nlog \u03c8 +\n\nb(u) du\n\n689\n\nfor any suitably smooth function g. with the choice g(u) = u, we have\u2202\u03bb/\u2202\u03be = b(\u03be),\nand this may be used in (12.50).\n\nthe right panel of figure 12.11 shows the effect of likelihood modification when\nsetting a confidence interval for the 0.99 quantile of the generalized pareto model using\nthe 60 exceedances. application of standard chi-squared asymptotics to the profile log\nlikelihood yields the highly asymmetric 0.95 confidence interval (86.1, 723.8), while\nthe corresponding confidence interval based on the modified profile log likelihood is\n(87.6, 757.4).\n\nalthough modification changes the right-hand limit of the confidence interval ap-\npreciably, of overwhelmingly greater concern in applications would be the represen-\ntativeness of the largest few observations in the sample, on which inferences will\nhinge. moreover in practice the quantile also depends on the poisson rate of ex-\nceedance times, and hence must be orthogonalized with respect to two parameters;\n(cid:1)\nsee section 6.5.2.\n\nalthough (12.49) gives a basis for orthogonalizing \u03b3 with respect to a scalar \u03c8,\nparameters orthogonal to a vector \u03c8 cannot be found in general. for if \u03c8 contains \u03c81\nand \u03c82, say, then \u03bb must simultaneously satisfy both systems of equations\n\n= \u2212i\n\n\u2217\u22121\n\u03b3 \u03b3 (\u03c8, \u03b3 )i\n\n\u2217\n\u03b3 \u03c81(\u03c8, \u03b3 ),\n\n\u2202\u03b3\n\u2202\u03c81\n\n= \u2212i\n\n\u2217\u22121\n\u03b3 \u03b3 (\u03c8, \u03b3 )i\n\n\u2217\n\u03b3 \u03c82(\u03c8, \u03b3 ),\n\n\u2202\u03b3\n\u2202\u03c82\n\nfor all \u03b3 , \u03c81 and \u03c82. however there is no guarantee that the compatibility condition\n\u2202 2\u03b3 /\u2202\u03c81\u2202\u03c82 = \u2202 2\u03b3 /\u2202\u03c82\u2202\u03c81 will hold; if not, a simultaneous joint solution does not\nexist. when a solution exists, it can produce familiar results.\n\nexample 12.26 (linear exponential family)\nlog likelihood\n\nin a linear exponential family with\n\n(cid:9)\u2217\n\n(\u03c8, \u03b3 ) \u2261 t t\n\n\u03c8 + t t\n\n\u03b3 \u2212 \u03ba(\u03c8, \u03b3 ),\n\n1\n\n2\n\nthe parameters \u03bb = \u03bb(\u03c8, \u03b3 ) orthogonal to \u03c8 are determined by\n\n\u2202\u03b3\n\n= \u2212\u03ba\u22121\n\n(12.51)\nif we reparametrize in terms of \u03c8 and \u03bb = \u03ba\u03b3 (\u03c8, \u03b3 ) = \u2202\u03ba(\u03c8, \u03b3 )/\u2202\u03b3 , then in this new\nparametrization, \u03b3 is a function of \u03c8 and \u03bb, and\n\n\u03b3 \u03b3 (\u03c8, \u03b3 )\u03ba\u03b3 \u03c8 (\u03c8, \u03b3 ).\n\n\u2202\u03c8 t\n\n0 = \u2202\u03bbt\n\n\u2202\u03c8\n\n= \u2202\u03b3 t\n\u2202\u03c8\n\n\u03ba\u03b3 \u03b3 (\u03c8, \u03b3 ) + \u03ba\u03c8 \u03b3 (\u03c8, \u03b3 ),\n\nso \u03bb = \u03ba\u03b3 (\u03c8, \u03b3 ) is asolution to (12.51). that is, the parameter orthogonal to \u03c8 is\nthe so-called complementary mean parameter \u03bb(\u03c8, \u03b3 ) = e(t2; \u03c8, \u03b3 ). by symmetry,\ne(t1; \u03c8, \u03b3 ) isorthogonal to \u03b3 .\n\n "}, {"Page_number": 702, "text": "690\n\n12 \u00b7 conditional and marginal inference\n\nthe normal distribution with mean \u00b5 and variance \u03c3 2 has canonical parameter\n(\u00b5/\u03c3 2,\u22121/(2\u03c3 2)). the canonical statistic (y, y 2) has expectation (\u00b5, \u00b52 + \u03c3 2), so\n\u00b5 is orthogonal to \u22121/(2\u03c3 2), and hence to \u03c3 2, while \u00b5/\u03c3 2 is orthogonal to \u00b52 + \u03c3 2.\nindependent poisson variables y1 and y2 with means exp(\u03b3 ) and exp(\u03b3 + \u03c8) have\nlog likelihood\n\n(cid:9)\u2217\n\n(\u03c8, \u03b3 ) \u2261 (y1 + y2)\u03b3 + y2\u03c8 \u2212 e\u03b3 \u2212 e\u03b3+\u03c8 .\n\nthe discussion above suggests that\n\n\u03bb = e(y1 + y2) = exp(\u03b3 ) + exp(\u03b3 + \u03c8) = e\u03b3 (1 + e\u03c8 )\n\nis orthogonal to \u03c8, so\u03b3 = log \u03bb \u2212 log(1 + e\u03c8 ) and\n\n(cid:9)(\u03c8, \u03bb) \u2261 y2\u03c8 \u2212 (y1 + y2) log(1 + e\u03c8 ) + (y1 + y2) log \u03bb \u2212 \u03bb.\n\nthe separation of \u03c8 and \u03bb implies that the profile and modified profile likelihoods\nfor \u03c8 are proportional. they correspond to the conditional likelihood obtained from\nthe density of y2 given y1 + y2.\n(cid:1)\nif y \u223c nn(x\u03b2, \u03c3 2\u03d2\u22121), where the\nexample 12.27 (restricted likelihood)\nparameter of interest \u03c8 appears in the n \u00d7 n matrix \u03d2 but not in \u03b2, the log like-\nlihood is\n\n(cid:9)(\u03b2, \u03c3 2, \u03c8) \u2261 \u2212 n\n2\n\nlog \u03c3 2 + 1\n\n2 log|\u03d2| \u2212 1\n\n2\u03c3 2 (y \u2212 x\u03b2)t\u03d2(y \u2212 x\u03b2),\n\ndifferentiation of which yields \u2202(cid:9)/\u2202\u03b2 = \u03c3 \u22122\u03d2(y \u2212 x\u03b2). it follows that \u03b2 is orthog-\nonal to both \u03c3 2 and \u03c8. now j\u03b2\u03b2(\u03b2, \u03c3 2, \u03c8) = \u03c3 \u22122 x t\u03d2 x, soapart from the term\n|\u2202(cid:5)\u03b2\u03c8,\u03c3 2 /\u2202(cid:5)\u03b2|\u22121, the modified profile log likelihood for \u03c8 and \u03c3 2 equals the marginal\nlog likelihood (12.13). note that (cid:5)\u03b2\u03c8,\u03c3 2 = (x t\u03d2 x)\n\u22121 x t\u03d2 y depends on \u03c8 but not\n\non \u03c3 2.\n\nthis argument also applies when the mean of y is a nonlinear function of \u03b2,\n(cid:1)\n\nprovided that no parameter appears in both mean and variance.\n\nthe notion of parameter orthogonality is useful, but the resulting modified likeli-\nhoods can be viewed as unsatisfactory, partly because the arbitrariness of the choice\nof orthogonal parameter results in a lack of uniqueness. a second difficulty is that\nthe partial differential equation and hence its solution will change if there is a minor\nchange to the model, such as the introduction of censoring, so any statistical inter-\npretation of the orthogonal parameter is then compromised. a third is that inferences\nare typically based on first-order distributional approximations, as mentioned above.\n\nexercises 12.4\n\n1\n\nlet y1, . . . ,y n\n\niid\u223c n (\u00b5, \u03c3 2) and let \u00b5 be the interest parameter. show that\n(cid:9)(\u00b5, \u03c3 2;(cid:5)\u00b5,(cid:5)\u03c3 2) \u2261 \u2212 n\n\nlog \u03c3 2 + (cid:5)\u03c3 2 + ((cid:5)\u00b5 \u2212 \u00b5)2\n\n(cid:19)\n\n(cid:6)\n\n,\n\n2\n\n\u03c3 2\n\n "}, {"Page_number": 703, "text": "12.5 \u00b7 bibliographic notes\n\n691\n\nwhere (cid:5)\u00b5 and (cid:5)\u03c3 2 are the maximum likelihood parameter estimators, and hence find the\nmodified profile likelihood for \u00b5. compare this with the marginal likelihood based on the\ntn\u22121 density of (y \u2212 \u00b5)/(s2/n)1/2, where y and s2 are the unbiased estimators of \u00b5 and\n\u03c3 2. discuss.\ncompute (12.43) for (12.15). hence show that m(\u03c4 ) = |j\u03b2\u03b2(\u03c4,(cid:5)\u03b2\u03c4 )|\u22121/2 for any regression-\nin example 12.23, find the asymptotic relative efficiencies of (cid:5)\u03c8 and (cid:5)\u03c8mp when \u03c8 = 0.\nsuppose that y (cid:10)\u2192 ((cid:5)\u03c8 ,(cid:5)\u03bb, a), that a is ancillary, and that\n\nscale model.\n\nf ((cid:5)\u03c8 ,(cid:5)\u03bb | a; \u03c8, \u03bb) = f ((cid:5)\u03c8 |(cid:5)\u03bb, a; \u03c8) f ((cid:5)\u03bb | a; \u03c8, \u03bb).\n\nby modifying the argument on page 685 show that the first term on the right is proportional\nto m(\u03c8) exp{(cid:9)p(\u03c8)}, with m(\u03c8) given by (12.42), apart from a relative error of size n\n\u22121.\nindependent poisson variables y1 and y2 have means exp(\u03b3 ) and exp(\u03b3 + \u03c8). find the\nprofile and modified profile log likelihoods for \u03c8 in this parametrization. comment.\na poisson variable y has mean \u00b5, which is itself a gamma random variable with mean \u03b8\nand shape parameter \u03bd. find the marginal density of y , and show that var(y ) = \u03b8 + \u03b8 2/\u03bd\nand that \u03bd and \u03b8 are orthogonal. hence show that \u03bd is orthogonal to \u03b2 for any model in\nwhich \u03b8 = \u03b8(x t\u03b2), x being a covariate vector. is the same true for the model in which\n\u03bd = \u03b8/\u03ba, sothat var( y ) = (1 + \u03ba)\u00b5? discuss the implications for inference on \u03b2 when\nthe variance function is unknown.\nlet x and y be independent exponential variables with means \u03b3 \u22121 and (\u03b3 \u03c8)\n\u22121. show that\nthe parameter \u03bb(\u03b3 , \u03c8) orthogonal to \u03c8 is the solution to the equation \u2202\u03b3 /\u2202\u03c8 = \u2212\u03b3 /(2\u03c8),\nand verify that taking \u03bb = \u03b3 /\u03c8\u22121/2 yields an orthogonal parametrization.\ninvestigate how this solution changes when x and y are subject to type i censoring at c.\nconsider n pairs of independent binomial variables with denominators m0 j and m1 j and\nsuccess probabilities\n\nexp(\u03bb j )\n1 + exp(\u03bb j )\n\n,\n\nexp(\u03bb j + \u03c8)\n1 + exp(\u03bb j + \u03c8)\n\nj = 1, . . . ,n .\n\n,\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\nfind a parameter orthogonal to \u03c8 and hence obtain a modified profile likelihood for \u03c8.\nhow does it compare to that in example 12.23?\n\n12.5 bibliographic notes\n\nr. a. fisher (1934) suggested that conditioning plays a central role in inference,\nbuilding on earlier work, and subsequently developed this notion largely through\nexamples; see fisher (1922, 1925, 1935b, 1956, 1990). although many of these ex-\namples are convincing, a fully satisfactory systematic development remains elusive,\nowing among other things to the non-existence of exact ancillary statistics in some im-\nportant models and their non-uniqueness in others. however very substantial progress\nhas been made over the past two decades, initially stemming from efron and hinkley\n(1978) and barndorff-nielsen and cox (1979). the p\nformula was crystallized as\nbeing central by barndorff-nielsen (1983), building on earlier work going back to\nfisher (1934), though a fully general proof is hard to establish; perhaps the most sat-\nisfactory is given by skovgaard (1990). reid (1995, 2003) gives excellent reviews of\nthe roles in inference of conditioning and asymptotics, while barndorff-nielsen and\ncox (1994) and severini (2000) give more extended accounts of modern likelihood\nasymptotics and many further references.\n\n\u2217\n\n "}, {"Page_number": 704, "text": "692\n\n12 \u00b7 conditional and marginal inference\n\nconditional and marginal likelihoods were introduced by bartlett (1937) and are\nnow widely used in applications. restricted maximum likelihood estimation was first\ndescribed by patterson and thompson (1971), though restricted likelihood itself had\nbeen obtained by hartley and rao (1967) and has earlier roots. diggle et al. (1994)\ndiscuss its use in the context of longitudinal data. example 12.11 is based on cruddas\net al. (1989). kalbfleisch and prentice (1973) show the link between marginal and\npartial likelihoods for the proportional hazards model.\n\nthe use of monte carlo simulation for conditional testing is described by besag\nand clifford (1989, 1991), and applied extensively in contingency tables by forster\net al. (1996) and smith et al. (1996). see also section 4.2 of davison and hinkley\n(1997).\n\nsaddlepoint approximation was introduced into statistics in the pioneering paper of\ndaniels (1954), which was largely ignored until interest in small-sample asymptotics\nrevived in the 1970s, focussed particularly by barndorff-nielsen and cox (1979).\nreid (1988) surveys statistical aspects of saddlepoint approximation, while barndorff-\nnielsen and cox (1989) is a standard reference to these and related procedures. jensen\n(1995) is a thorough mathematical treatment. distribution function approximations\nare described by daniels (1987), skovgaard (1987), and barndorff-nielsen (1986).\nthe approximation (12.40) was proposed by skovgaard (1996) and has itself been\napproximated by severini (1999). numerous other approaches have been suggested;\nsee for example the account of likelihoods for component parameters in fraser (2002)\nor chapter 7 of severini (2000).\n\ncox and reid (1987) and its discussion give a systematic treatment of parameter\n\northogonality and its consequences for conditional inference.\n\nthe complexity of many likelihood expressions has led authors such as\nmccullagh (1987) and andrews and stafford (2000) to develop powerful tools for\nanalytic work with or symbolic computation applied to asymptotic expansions.\n\nbrazzale (1999, 2000), and bellio (1999) describe systematic attempts to imple-\n\nment small-sample likelihood asymptotics for practical use.\n\n12.6 problems\n\n1 find the fisher information matrix for example 12.2, and show that it gives the wrong\n\nasymptotic variances for all the parameters.\n\n2 does the argument of example 12.5 apply to the location-scale model with any error\n\ndistribution?\n\n3 show that a missingness indicator (section 5.5.1) is not generally an ancillary statistic,\n\nand discuss the implications for inference on \u03b8.\n\n4 a normal random variable y has mean \u03b8 and variance determined by table 12.5, where\nk1 and k2 are chosen so that the values of \u03c0 lie in the unit interval. show that a1 is exactly\nancillary but is not informative about the precision of y , while a2 is not exactly ancillary\nbut isindicative of the precision of y .\ndiscuss briefly the merits of exact and approximate conditional inference here. (lloyd,\n1992)\n\n "}, {"Page_number": 705, "text": "table 12.5 conditional\ndistributions of normal\nvariables.\n\n12.6 \u00b7 problems\n\nvar(y )\n\n\u03c0\na1\na2\n\n100\n\n1\n\n2 (1 \u2212 k1\u03b8)\n\n1\n0\n\n100\n\n1\n\n2 (1 + k2\u03b8)\n\n0\n0\n\n693\n\n1\n\n2 (1 \u2212 k2\u03b8)\n\n1\n\n0\n1\n\n1\n\n2 (1 + k1\u03b8)\n\n1\n\n1\n1\n\n5 show that if y has the cauchy density\n\nf (y; \u00b5, \u03c3 ) =\n\n\u03c0\n\n\u03c3\n\n\u03c3 2 + (y \u2212 \u00b5)2\n\n(cid:7)\n\n(cid:8) , \u2212\u221e < y, \u00b5 < \u221e,\n\n\u03c3 > 0,\n\nthen 1/y has density f (y; \u00b5(cid:9), \u03c3 (cid:9)\ndeduce that a random sample y1, . . . ,y n of cauchy variables yields two distinct sets of\nmaximal ancillary statistics. discuss inference for \u00b5, \u03c3 , and for \u00b5/\u03c3 .\n(mccullagh, 1992)\n\n), where \u00b5(cid:9) = \u00b5/(\u00b52 + \u03c3 2) and \u03c3 (cid:9) = \u03c3/(\u00b52 + \u03c3 2).\n\n6 in example 12.8, show that a1 and a2 are not jointly ancillary for \u03b8.\n\nconditioning on an ancillary statistic is intended to divide the sample space into relevant\nsubsets according to their information content, so one basis for choice among competing\nancillaries is to take that whose the conditional fisher information has largest variance.\nshow that e{i1(\u03b8)} =e{ i2(\u03b8)}, but that var{i1(\u03b8)} > var{i2(\u03b8)} for 0 < \u03b8 <1, and deduce\nthat a1 is preferable.\ndiscuss critically this idea.\n(cox, 1971)\n\n7 consider two independent exponential random samples, y1, . . . ,y n having rate \u03c8 > 0\nand x1, . . . , xn having rate \u03bb/\u03c8, where \u03bb >0.\n(a) suppose it is required to find an approximate ancillary for \u03c8 when \u03bb = 1. find the\nlikelihood ratio statistic for testing \u03bb = 1 against the alternative putting no restriction on\n\u03bb, and show that it is a function only of x y . hence give an exact ancillary statistic for \u03c8.\n(\u03c8) when \u03bb = 1. inves-\n(b) give explicit expressions for the p\ntigate the numerical accuracy of (12.8) when n = 1.\nat times 0 < t1 < \u00b7\u00b7\u00b7 < tn < 1. show that the likelihood is\n\n8 a poisson process of rate \u03bbe\u03c8t is observed on the interval [0, 1], over which events occur\n\nformula (12.7) and for r\n\n\u2217\n\n\u2217\n\n(cid:6)\n\n(cid:20)\n\n\u2212\u03bb\n\n1\n\ne\u03c8u du\n\nexp\n\n\u03bbe\u03c8t j ,\n\n(cid:19)\n\nn(cid:9)\nj=1\n\n0\n\nn(cid:9)\nj=1\n\nn!\n\n(cid:12)\n\ne\u03c8t j\n\n1\n0 e\u03c8u du\n\nand deduce that inference for \u03c8 may be based on the conditional density\n\n(cid:12)\nof the times of events t1 < \u00b7\u00b7\u00b7 < tn given that n = n. show that this is the joint density\n1\n0 e\u03c8u du on\nof the order statistics of a random sample of n variables with density e\u03c8t /\n(0, 1), and derive a conditional test of the hypothesis \u03c8 = 0 against \u03c8 > 0, giving the\nnull mean and variance of your test statistic.\nwhen \u03c8 = 0, how might you test the hypothesis that the tj are clustered relative to a\npoisson process?\n(cox and lewis, 1966, pp. 45\u201351)\n\n9 under the model of example 1.4, the number of deaths due to lung cancer in the (i, j)\n\ncell of table 1.4, yi j , is apoisson variable with mean expressible as\n\nxi j g(ti , \u03c6)(1 + \u03c81d \u03c82\nj ),\n\nwhere the notation reflects our interest in the effect of smoking.\n\n "}, {"Page_number": 706, "text": "694\n\n10\n\n11\n\n12 \u00b7 conditional and marginal inference\n\nshow that the marginal density of mi = yi1 + \u00b7\u00b7\u00b7 + yic is poisson with mean\n\n\u03bbi = g(ti ; \u03c6)\n\nxi j (1 + \u03c81d \u03c82\nj ),\n\ni = 1, . . . , r,\n\nc(cid:1)\nj=1\n\nwhile the conditional distribution of yi1, . . . ,y ic given mi = mi is multinomial with\ndenominator mi = yi1 + \u00b7\u00b7\u00b7 + yic and probabilities\n\n\u03c0i j =\n\n(cid:4)\n\nxi j (1 + \u03c81d \u03c82\nj )\nk=1 xik(1 + \u03c81d \u03c82\nk )\n\nc\n\nj = 1, . . . ,c .\n\n,\n\noutline how this computation may be used as a basis for inference on \u03c8, and in particular\nhow evidence for \u03c82 = 1 may be assessed. do the usual likelihood asymptotics apply\nwhen testing the hypothesis that \u03c81 = 0, regardless of \u03c82?\nin an exponential family density\n\nf (t1, t2; \u03c8, \u03bb) = exp\n\n\u03c8 + t t\n\n2\n\nt t\n1\n\n\u03bb \u2212 \u03ba(\u03c8, \u03bb) + c(t1, t2)\n\n(cid:8)\n\n,\n\n(cid:7)\n\nshow that the conditional distribution of t1 given t2 is unchanged if \u03bb is randomly taken\nfrom a density g(\u03bb).\nindependent pairs of observations (x1, y1), . . . ,( xn, yn) are supposed to have independent\npoisson distributions with means (\u00b5 j , \u03b2\u00b5 j ), for j = 1, . . . ,n . does your inference for \u03b2\ndepend on the knowledge that \u00b51, . . . , \u00b5n\nif the density g(\u00b5) = g(\u00b5; \u03b3 ) isknown up to the value of a parameter \u03b3 , say, suggest how\nto retrieve any information on \u03c8 in the marginal density of the y j .\nindependent pairs of binary observations (r01, r11), . . . ,( r0n, r1n) have success proba-\nbilities (e\u03bb j /(1 + e\u03bb j ), e\u03c8+\u03bb j /(1 + e\u03c8+\u03bb j )), for j = 1, . . . ,n .\nis (cid:5)\u03c8c = log(r01/r10), where r01 and r10 are respectively the numbers of (0,1) and (1,0)\n(a) show that the maximum likelihood estimator of \u03c8 based on the conditional likelihood\npairs. does (cid:5)\u03c8c tend to \u03c8 as n \u2192 \u221e?\n\niid\u223c g?\n\n(b) write down the unconditional likelihood for \u03c8 and \u03bb, and show that the likelihood\nequations are equivalent to\n\nj = 1, . . . ,n ,\n\n,\n\n(12.52)\n\n(cid:5)\u03bb j+(cid:5)\u03c8\n(cid:5)\u03bb j\n+ e\nr0 j + r1 j = e\n(cid:5)\u03bb j+(cid:5)\u03c8\n(cid:5)\u03bb j\n1 + e\n1 + e\nn(cid:1)\nr1 j = n(cid:1)\n(cid:5)\u03bb j+(cid:5)\u03c8\ne\n(cid:5)\u03bb j+(cid:5)\u03c8\n1 + e\nj=1\n\nj=1\n\n.\n\n(i) show that the maximum likelihood estimator of \u03bb j is \u221e if r0 j = r1 j = 1 and \u2212\u221e if\nr0 j = r1 j = 0; such pairs are not informative. (ii) use (12.52) to show that(cid:5)\u03bb j = \u2212(cid:5)\u03c8 /2 for\nthose pairs for which r0 j + r1 j = 1. (iii) hence deduce that the unconditional maximum\nlikelihood estimator of \u03c8 is (cid:5)\u03c8u = 2 log(r01/r10). what is the implication for uncondi-\n\ntional estimation of \u03c8?\n\n12 consider two independent poisson random samples x1, . . . , xn and y1, . . . ,y n, the first\nhaving mean \u03bb and the second having mean \u03bb\u03c8 , where \u03bb, \u03c8 > 0.\n(a) show that (t1, t2) = (x1 + \u00b7\u00b7\u00b7 + xn, y1 + \u00b7\u00b7\u00b7 + yn) isminimal sufficient, and for any\nfixed value of \u03c8 establish that \u03bb may be eliminated by conditioning on t\u03c8 = t1 + \u03c8 t2.\n(b) let (t1,obs, t2,obs) denote the observed value of (t1, t2). sketch the sample space for\n(t1, t2), and consider how the relevant subset\n\n(cid:7)\n\n(t1, t2) :t 1 + \u03c8t2 = t1,obs + \u03c8t2,obs\n\n(cid:8)\n\nvaries with \u03c8. hence explain how an exact significance level for a test of \u03c8 = \u03c80 against\nthe alternative \u03c8 > \u03c80 will depend on \u03c80. doyou find this satisfactory?\n13 adapt the argument giving inference for the regression-scale model to the location-scale\nmodel y = \u00b5 + e\u03c4 \u03b5, and outline how to make small-sample inferences for \u00b5 and for \u03c4 .\n\nwhat happens if \u03c8 is\nrational? what if \u03c8 is\nirrational?\n\n "}, {"Page_number": 707, "text": "12.6 \u00b7 problems\n\n695\n\ncompare the resulting confidence intervals for \u00b5 with the the posterior credible intervals\nfound by bayesian inference using prior density \u03c0(\u00b5, \u03c3 ) \u221d \u03c3 \u22121 and distribution function\napproximation (11.31). discuss.\n14 (a) consider a location-scale model in which y = \u03b7 + \u03c3 \u03b5, where \u03b5 has a known density\ng. find parameters orthogonal to \u03b7 and to \u03c3 , and give conditions under which \u03b7 and \u03c3 are\nthemselves orthogonal.\n(b) consider a regression model y = x t\u03b2 + \u03c3 \u03b5, where \u03b5 again has density g. find a\nparameter orthogonal to the first component of \u03b2, and compare your result with the\ndiscussion following (8.8).\nx j =\nex j \u03c8 , and deduce that the likelihood ratio statistic for testing\n\n(a) show that(cid:5)\u03bb\u03c8 = n\n\u03c8 = 0 can be written as w p(0) = 2n log((cid:5)\u03bb0/(cid:5)\u03bb(cid:5)\u03c8 ).\n(b) let \u03b3 = log \u03bb. bywriting the model in linear regression form, show that \u2202(cid:5)\u03b3 /\u2202(cid:5)\u03b3\u03c8 = 1,\nand deduce that \u2202(cid:5)\u03bb/\u2202(cid:5)\u03bb\u03c8 =(cid:5)\u03bb/(cid:5)\u03bb\u03c8 . hence find the modified profile likelihood both with\nand without this term, and compare the resulting likelihood ratio statistics with w p(0).\n(cox and reid, 1987)\n\n15 independent exponential variables y1, . . . ,y n have means e(y j ) = \u03bbex j \u03c8 , where\n\n0. show that \u03bb and \u03c8 are orthogonal parameters.\n\n(cid:4)\n\n(cid:4)\n\n\u22121\n\n16 the michaelis\u2013menton model of nonlinear regression is usually specified as\n\ny j = \u03b20x j\n\u03b21 + x j\n\n+ \u03b5 j ,\n\n\u03b51, . . . , \u03b5n\n\niid\u223c n (0, \u03c3 2);\n(cid:8)2 ,\n\nwe assume that \u03c3 2 is known. show that the log likelihood is\n\nn(cid:1)\nj=1\nand find the expected information matrix.\n(a) show that the parameter \u03bb = \u03bb(\u03b21, \u03b20) orthogonal to \u03b21 is determined by\n\n(\u03b20, \u03b21) = \u2212 1\n2\u03c3 2\n\ny j \u2212 \u03b20x j /(\u03b21 + x j )\n\n(cid:9)\u2217\n\n(cid:7)\n\ng(\u03bb) = \u03b22\n\n0\n\nx 2\nj\n\n(\u03b21 + x j )2\n\nn(cid:1)\nj=1\n\nfor an appropriate smooth function g. choose g suitably, and write the log likelihood\nexplicitly in terms of \u03bb and \u03b21.\n(b) show that the parameter \u03bb = \u03bb(\u03b21, \u03b20) orthogonal to \u03b20 is determined by\n\nand check that its solution is\n\nn(cid:1)\nj=1\n\n\u03b20\n\nx 2\nj\n\n(\u03b21 + x j )4\n\ng1(\u03bb) = \u03b23\n\n0\n\n\u2202\u03b21\n\u2202\u03b20\n\nn(cid:1)\nj=1\n\n= n(cid:1)\n\nj=1\n\nx 2\nj\n\n(\u03b21 + x j )3\n\nx 2\nj\n\n(\u03b21 + x j )3\n\n.\n\ncan the log likelihood be expressed explicitly in terms of \u03bb and \u03b20?\n(c) the orthogonal parametrizations above depend on the design points x j . doyou find\nthis satisfactory?\n(hills, 1987)\n\n "}, {"Page_number": 708, "text": "a p p e n d i x a\n\npracticals\n\nthe list below gives key words for practicals written in the statistical language s and\nintended to accompany the chapters of the book. the practicals themselves may be\ndownloaded from\n\nhttp://statwww.epfl.ch/people/~davison/sm\n\ntogether with a library of functions and data.\n\n2. variation\n\n1. speed of light data. exploratory data analysis.\n2. maths marks data. brush and spin plots.\n3. probability plots for simulated data.\n4. illustration of central limit theorem using simulated data.\n5. data on air-conditioning failures. exponential probability plots.\n\n3. uncertainty\n\n1. properties of half-normal distribution. half-normal plot.\n2. simulation of student t statistic, following original derivation.\n3. simulation of wiener process and brownian bridge.\n4. normal random number generation by summing uniform variables.\n5. implementation and assessment of a linear congruential generator.\n6. coverage of student t confidence interval under various scenarios.\n\n4. likelihood\n\n1. loss of information due to rounding of normal data.\n2. birth data. maximum likelihood estimation for poisson and gamma models. assessment\n\nof fit.\n\n3. data on sizes of groups of people. maximum likelihood fit of truncated poisson distri-\n\nbution. pearson\u2019s statistic.\n\n4. \u03b1-particle data. maximum likelihood fit of poisson process model.\n\n696\n\n "}, {"Page_number": 709, "text": "a \u00b7 practicals\n\n697\n\n5. blood group data. maximum likelihood fit of multinomial model.\n6. generalized pareto distribution. nonregular estimation of endpoint.\n\n5. models\n\n1. boiling point of water data. straight-line regression.\n2. survival data on leukaemia. exponential and weibull models.\n3. hus data. em algorithm for mixture of poisson distributions.\n4. em algorithm for mixture of normal distributions.\n\n6. stochastic models\n\n1. markov chain fitting to alofi rainfall data. assessment of fit.\n2. multivariate normal fit to data on head sizes.\n3. time series analysis of manaus river height data. arma modelling.\n4. inhomogeneous poisson process fitted to freezes of lake constance.\n5. extreme-value analysis of ftse return data.\n\n7. theory\n\n1. neurological data. kernel density estimation. test of unimodality.\n2. mean integrated squared error of kernel density estimator applied to mixtures of normal\n\ndensities.\n\n3. test for spatial poisson process. beetle data.\n4. coverage of confidence intervals for poisson mean.\n\n8. linear models\n\n1. cherry tree data. linear model.\n2. salinity data. linear model.\n3. data on iqs of identical twins. linear model.\n4. cement data. simulation of collinear data.\n5. simulation to assess properties of stepwise model selection procedures.\n6. data on pollution and mortality. linear model. ridge regression.\n\n9. designed experiments\n\n1. chick bone data. inter- and intra-block recovery of information.\n2. millet plant data. latin square. outliers. orthogonal polynomials.\n3. data on marking of examination scripts. analysis of variance.\n4. teak plant data. 2 \u00d7 3 factorial experiment.\n\n10. nonlinear models\n\n1. space shuttle data. logistic regression model.\n2. beetle data. regression models for binary data.\n3. stomach ulcer data. logistic regression for 2 \u00d7 2 tables. overdispersion.\n4. speed limit data. log-linear model. logistic regression model.\n5. lizards data. log-linear model. logistic regression model.\n6. titanic survivor data. log-linear model.\n\n "}, {"Page_number": 710, "text": "698\n\na \u00b7 practicals\n\n7. seed germination data. overdispersion. quasi-likelihood. beta-binomial model.\n8. coal-mining disaster data. inhomogeneous poisson process. generalized additive\n\nmodel.\n\n9. urine crystal data. logistic regression.\n10. survival data on leukaemia. proportional hazards model.\n11. motorette data. survival data analysis.\n12. pbc data. survival data analysis. proportional hazards model.\n\n11. bayesian models\n\n1. coin spun on edge. updating individual and group priors.\n2. cloth data. hierarchical poisson model. laplace approximation.\n3. gibbs sampler for bivariate truncated exponential distribution.\n4. random walk metropolis\u2013hastings algorithm with cauchy proposals.\n5. pump failure data. gibbs sampler for hierarchical poisson model.\n6. hus data. gibbs sampler. changepoint in poisson variables.\n7. beaver body temperature data. gibbs sampler. changepoint in normal variables.\n8. data augmentation algorithm with multinomial data.\n\n12. marginal and conditional likelihood\n\n1. ancillary statistic. simulation with cauchy data.\n2. saddlepoint approximation. laplace distribution.\n3. urine data. logistic regression. approximate conditional inference.\n\n "}, {"Page_number": 711, "text": "bibliography\n\naalen, o. o. (1978) nonparametric inference for a\nfamily of counting processes. annals of statistics 6,\n701\u2013726.\n\naalen, o. o. (1994) effects of frailty in survival analysis.\nstatistical methods in medical research 3, 227\u2013243.\n\nagresti, a. (1984) analysis of ordinal categorical\ndata. new york: wiley.\n\nagresti, a. and caffo, b. (2000) simple and effective\nconfidence intervals for proportions and differences of\nproportions result from adding two successes and two\nfailures. the american statistician 54, 280\u2013288.\n\nagresti, a. and coull, b. a. (1998) approximate is\nbetter than \u201cexact\u201d for interval estimation of binomial\nproportions. the american statistician 52, 119\u2013126.\n\nakaike, h. (1973) information theory and an extension\nof the maximum likelihood principle. in second\ninternational symposium on information theory, eds\nb. n. petrov and f. cz\u00b4aki, pp. 267\u2013281. budapest:\nakademiai kiad\u00b4o. reprinted in breakthroughs in\nstatistics, volume 1, eds s. kotz and n. l. johnson,\npp. 610\u2013624. new york: springer.\n\nalmond, r. (1995) graphical belief modelling.\nnew york: chapman & hall.\n\nandersen, p. k., borgan, \u00f8., gill, r. d. and keiding, n.\n(1993) statistical models based on counting processes.\nnew york: springer.\n\nanderson, t. w. (1958) introduction to multivariate\nstatistical analysis. new york: wiley.\n\nandrews, d. f. and stafford, j. e. (2000) symbolic\ncomputation for statistical inference. oxford:\nclarendon press.\n\narnold, b. c., balakrishnan, n. and nagaraja, h. n.\n(1992) a first course in order statistics. new york:\nwiley.\n\nartes, r. (1997) extens\u02dcoes da teoria das equac\u00b8 \u02dcoes de\nestimac\u00b8 \u02dcao generalizadas a dados circulares e modelos\nde dispers\u02dcao. ph.d. thesis, university of s\u02dcao paulo.\n\nashford, j. r. (1959) an approach to the analysis of\ndata for semi-quantal responses in biological assay.\nbiometrics 15, 573\u2013581.\n\natkinson, a. c. (1985) plots, transformations, and\nregression. oxford: clarendon press.\n\natkinson, a. c. and donev, a. n. (1992) optimum\nexperimental designs. oxford: clarendon press.\n\natkinson, a. c. and riani, m. (2000) robust diagnostic\nregression analysis. new york: springer.\n\navery, p. j. and henderson, d. a. (1999) fitting markov\nchain models to discrete state series such as dna\nsequences. applied statistics 48, 53\u201361.\n\nazzalini, a. and bowman, a. w. (1993) on the use of\nnonparametric regression for checking linear\nrelationships. journal of the royal statistical society\nseries b 55, 549\u2013557.\n\nazzalini, a., bowman, a. w. and h\u00a8ardle, w. (1989) on\nthe use of nonparametric regression for model-checking.\nbiometrika 76, 1\u201311.\n\nbarndorff-nielsen, o. e. (1978) information and\nexponential families in statistical theory. new york:\nwiley.\n\nbarndorff-nielsen, o. e. (1983) on a formula for the\ndistribution of the maximum likelihood estimator.\nbiometrika 70, 343\u2013365.\n\nappleton, d. r., french, j. m. and vanderpump, m. p. j.\n(1996) ignoring a covariate: an example of simpson\u2019s\nparadox. the american statistician 50, 340\u2013341.\n\nbarndorff-nielsen, o. e. (1986) inference on full or\npartial parameters based on the standardized signed log\nlikelihood ratio. biometrika 73, 307\u2013322.\n\n699\n\n "}, {"Page_number": 712, "text": "700\n\nbibliography\n\nbarndorff-nielsen, o. e. and cox, d. r. (1979)\nedgeworth and saddle-point approximations with\nstatistical applications (with discussion). journal of the\nroyal statistical society series b 41, 279\u2013312.\n\nbarndorff-nielsen, o. e. and cox, d. r. (1989)\nasymptotic techniques for use in statistics. london:\nchapman & hall.\n\nbarndorff-nielsen, o. e. and cox, d. r. (1994)\ninference and asymptotics. london: chapman & hall.\n\nbartlett, m. s. (1936a) statistical information and\nproperties of sufficiency. proceedings of the royal\nsociety of london, series a 154, 124\u2013137.\n\nbartlett, m. s. (1936b) the information available in\nsmall samples. proceedings of the cambridge\nphilosophical society 32, 560\u2013566.\n\nbartlett, m. s. (1937) properties of sufficiency and\nstatistical tests. proceedings of the royal society of\nlondon, series a 160, 268\u2013282.\n\nbasawa, i. v. and scott, d. j. (1981) asymptotic optimal\ninference for non-ergodic models. volume 17 of\nlecture notes in statistics. new york: springer.\n\nbasu, d. (1955) on statistics independent of a complete\nsufficient statistic. sankhy\u00afa 15, 377\u2013380.\n\nbasu, d. (1958) on statistics independent of sufficient\nstatistics. sankhy\u00afa 20, 223\u2013226.\n\nbellio, r. (1999) likelihood asymptotics: applications\nin biostatistics. ph.d. thesis, department of statistical\nscience, university of padova.\n\nbelsley, d. a. (1991) conditioning diagnostics:\ncollinearity and weak data in regression. new york:\nwiley.\n\nbelsley, d. a., kuh, e. and welsch, r. e. (1980)\nregression diagnostics: identifying influential data and\nsources of collinearity. new york: wiley.\n\nberan, j. (1994) statistics for long-memory processes.\nlondon: chapman & hall.\n\nberan, r. j. and fisher, n. i. (1998) a conversation with\ngeoff watson. statistical science 13, 75\u201393.\n\nberger, j. o. (1985) statistical decision theory and\nbayesian analysis. second edition. new york: springer.\n\nberger, j. o. and wolpert, r. l. (1988) the likelihood\nprinciple. second edition, volume 6 of lecture notes \u2014\nmonograph series. hayward, california: institute of\nmathematical statistics.\n\nbernardo, j. m. and smith, a. f. m. (1994) bayesian\ntheory. new york: wiley.\n\nbesag, j. e. (1974) spatial interaction and the statistical\nanalysis of lattice systems (with discussion). journal of\nthe royal statistical society series b 34, 192\u2013236.\n\nbesag, j. e. (1986) on the statistical analysis of dirty\npictures (with discussion). journal of the royal\nstatistical society series b 48, 259\u2013302.\n\nbesag, j. e. (1989) a candidate\u2019s formula: a curious\nresult in bayesian prediction. biometrika 76, 183.\n\nbesag, j. e. and clifford, p. (1989) generalized monte\ncarlo significance tests. biometrika 76, 633\u2013642.\n\nbesag, j. e. and clifford, p. (1991) sequential monte\ncarlo p-values. biometrika 78, 301\u2013304.\n\nbesag, j. e. and green, p. j. (1993) spatial statistics and\nbayesian computation. journal of the royal statistical\nsociety series b 55, 25\u201337.\n\nbesag, j. e., green, p. j., higdon, d. and mengersen, k.\n(1995) bayesian computation and stochastic systems\n(with discussion). statistical science 10, 3\u201366.\n\nbesag, j. e., york, j. and molli\u00b4e, a. (1991) bayesian\nimage restoration, with two applications in spatial\nstatistics (with discussion). annals of the institute of\nstatistical mathematics 43, 1\u201359.\n\nbickel, p. j. and doksum, k. a. (1977) mathematical\nstatistics: basic ideas and selected topics. san\nfrancisco: holden-day.\n\nbillingsley, p. (1961) statistical inference for markov\nprocesses. chicago: chicago university press.\n\nbishop, y. m., fienberg, s. e. and holland, p. w. (1975)\ndiscrete multivariate analysis. cambridge,\nmassachussetts: mit press.\n\nbissell, a. f. (1972) a negative binomial model with\nvarying element sizes. biometrika 59, 435\u2013441.\n\nbloomfield, p. (1976) fourier analysis of time series:\nan introduction. new york: wiley.\n\nbowman, a. w. and azzalini, a. (1997) applied\nsmoothing techniques for data analysis: the kernel\napproach with s-plus illustrations. oxford: clarendon\npress.\n\nbox, g. e. p. (1980) sampling and bayes inference in\nscientific modelling and robustness (with discussion).\njournal of the royal statistical society series a 143,\n383\u2013430.\n\nbox, g. e. p. and cox, d. r. (1964) an analysis of\ntransformations (with discussion). journal of the royal\nstatistical society series b 26, 211\u2013246.\n\nbox, g. e. p., hunter, w. g. and hunter, j. s. (1978)\nstatistics for experimenters. new york: wiley.\n\nbox, g. e. p. and tiao, g. c. (1973) bayesian inference\nin statistical analysis. second edition. reading,\nmassachussetts: addison\u2013wesley.\n\nbox, g. e. p. and tidwell, p. w. (1962) transformation\nof the independent variables. technometrics 4, 531\u2013550.\n\n "}, {"Page_number": 713, "text": "bibliography\n\n701\n\nbrazzale, a. r. (1999) approximate conditional\ninference in logistic and loglinear models. journal of\ncomputational and graphical statistics 8, 653\u2013661.\nbrazzale, a. r. (2000) practical small-sample\nparametric inference. ph.d. thesis, department of\nmathematics, swiss federal institute of technology,\nlausanne, switzerland.\nbr\u00b4emaud, p. (1999) markov chains: gibbs fields, monte\ncarlo simulation, and queues. new york: springer.\nbrillinger, d. r. (1981) time series: data analysis and\ntheory. expanded edition. san francisco: holden-day.\nbrockwell, p. j. and davis, r. a. (1991) time series:\ntheory and methods. second edition. new york:\nspringer.\nbrockwell, p. j. and davis, r. a. (1996) introduction to\ntime series and forecasting. new york: springer.\nbrooks, s. p. (1998) markov chain monte carlo and its\napplication. the statistician 47, 69\u2013100.\nbrooks, s. p. and gelman, a. (1998) general methods\nfor monitoring convergence of iterative simulations.\njournal of computational and graphical statistics 7,\n434\u2013455.\nbrown, b. w. (1980) prediction analysis for binary data.\nin biostatistics casebook, eds r. g. miller, b. efron,\nb. w. brown and l. e. moses, pp. 3\u201318. new york:\nwiley.\nbrown, b. w. and hollander, m. (1977) statistics: a\nbiomedical introduction. new york: wiley.\nbrown, l. d. (1986) fundamentals of statistical\nexponential families, with applications in statistical\ndecision theory. volume 9 of lecture notes \u2014\nmonograph series. hayward, california: institute of\nmathematical statistics.\nbrown, p. j. (1993) measurement, regression, and\ncalibration. oxford: clarendon press.\nburnham, k. p. and anderson, d. r. (2002) model\nselection and multi-model inference: a practical\ninformation theoretic approach. second edition.\nnew york: springer.\ncarlin, b. p. and louis, t. a. (2000) bayes and\nempirical bayes methods for data analysis. second\nedition. london: chapman & hall.\ncarroll, r. j. and ruppert, d. (1988) transformation\nand weighting in regression. london: chapman & hall.\ncasella, g. and berger, r. l. (1990) statistical inference.\nbelmont, california: wadsworth & brooks/cole.\ncastillo, e., guti\u00b4errez, j. m. and hadi, a. s. (1997)\nexpert systems and probabilistic network models.\nnew york: springer.\n\ncatchpole, e. a. and morgan, b. j. t. (1997) detecting\nparameter redundancy. biometrika 84, 187\u2013196.\n\nchatfield, c. (1988) problem-solving: a statistician\u2019s\nguide. london: chapman & hall.\n\nchatfield, c. (1995) model uncertainty, data mining and\nstatistical inference (with discussion). journal of the\nroyal statistical society series a 158, 419\u2013466.\n\nchatfield, c. (1996) the analysis of time series. fifth\nedition. london: chapman & hall.\n\nchatfield, c. and collins, a. j. (1980) introduction to\nmultivariate analysis. london: chapman & hall.\n\nchatterjee, s. and hadi, a. s. (1988) sensitivity analysis\nin linear regression. new york: wiley.\n\nchellappa, r. and jain, a. (eds) (1993) markov random\nfields: theory and application. new york: academic\npress.\n\ncheng, r. c. h. and traylor, l. (1995) non-regular\nmaximum likelihood problems (with discussion).\njournal of the royal statistical society series b 57, 3\u201344.\n\ncleveland, w. s. (1993) vizualizing data. new jersey:\nhobart press.\n\ncleveland, w. s. (1994) the elements of graphing\ndata. revised edition. new jersey: hobart press.\n\nclifford, p. (1990) markov random fields in statistics. in\ndisorder in physical systems: a volume in honour of\njohn m. hammersley, eds g. r. grimmett and d. j. a.\nwelsh, pp. 19\u201332. oxford: clarendon press.\n\ncobb, g. w. (1998) introduction to design and analysis\nof experiments. new york: springer.\n\ncochran, w. g. and cox, g. m. (1959) experimental\ndesigns. second edition. new york: wiley.\n\ncoles, s. g. (2001) an introduction to the statistical\nmodeling of extreme values. new york: springer.\n\ncollett, d. (1991) modelling binary data. london:\nchapman & hall.\n\ncollett, d. (1995) modelling survival data in medical\nresearch. london: chapman & hall.\n\ncook, r. d. (1977) detection of influential observations\nin linear regression. technometrics 19, 15\u201318.\n\ncook, r. d. and weisberg, s. (1982) residuals and\ninfluence in regression. london: chapman & hall.\n\ncopas, j. b. (1999) what works?: selectivity models\nand meta-analysis. journal of the royal statistical\nsociety series a 162, 96\u2013109.\n\ncopas, j. b. and li, h. g. (1997) inference for\nnon-random samples (with discussion). journal of the\nroyal statistical society series b 59, 55\u201395.\n\ncowell, r. g., dawid, a. p. and lauritzen, s. l. (1999)\nprobabilistic networks and expert systems. new york:\nspringer.\n\n "}, {"Page_number": 714, "text": "702\n\nbibliography\n\ncowles, m. k. and carlin, b. p. (1996) markov chain\nmonte carlo convergence diagnostics: a comparative\nreview. journal of the american statistical association\n91, 883\u2013904.\n\ncox, d. r. (1958) planning of experiments. new york:\nwiley.\n\ncox, d. r. (1959) the analysis of exponentially\ndistributed life-times with two types of failure. journal\nof the royal statistical society series b 21,\n411\u2013421.\n\ncox, d. r. (1970) analysis of binary data. london:\nchapman & hall.\n\ncox, d. r. (1971) the choice between alternative\nancillary statistics. journal of the royal statistical\nsociety series b 33, 251\u2013255.\n\ncox, d. r. (1972) regression models and life tables\n(with discussion). journal of the royal statistical\nsociety series b 34, 187\u2013220.\n\ncox, d. r. (1978) some remarks on the role in statistics\nof graphical methods. applied statistics 27, 4\u20139.\n\ncox, d. r. (1979) a note on the graphical analysis of\nsurvival data. biometrika 66, 188\u2013190.\n\ncox, d. r. (1983) a remark on censoring and surrogate\nresponse variables. journal of the royal statistical\nsociety series b 45, 391\u2013393.\n\ncox, d. r. (1990) role of models in statistical analysis.\nstatistical science 5, 169\u2013174.\n\ncox, d. r. (1992) causality: some statistical aspects.\njournal of the royal statistical society series a 155,\n291\u2013301.\n\ncox, d. r. and davison, a. c. (1989) prediction for\nsmall subgroups. philosophical transactions of the\nroyal society of london, series b 325, 185\u2013187.\n\ncox, d. r. and hinkley, d. v. (1974) theoretical\nstatistics. london: chapman & hall.\n\ncox, d. r. and isham, v. (1980) point processes.\nlondon: chapman & hall.\n\ncox, d. r. and lewis, p. a. w. (1966) the statistical\nanalysis of series of events. london: chapman & hall.\n\ncox, d. r. and miller, h. d. (1965) the theory of\nstochastic processes. london: chapman & hall.\n\ncox, d. r. and oakes, d. (1984) analysis of survival\ndata. london: chapman & hall.\n\ncox, d. r. and reid, n. (1987) parameter orthogonality\nand approximate conditional inference (with\ndiscussion). journal of the royal statistical society\nseries b 49, 1\u201339.\n\ncox, d. r. and reid, n. (2000) the theory of the\ndesign of experiments. london: chapman & hall.\n\ncox, d. r. and snell, e. j. (1968) a general definition of\nresiduals (with discussion). journal of the royal\nstatistical society series b 30, 248\u2013275.\n\ncox, d. r. and snell, e. j. (1981) applied statistics:\nprinciples and examples. london: chapman & hall.\n\ncox, d. r. and snell, e. j. (1989) analysis of binary\ndata. second edition. london: chapman & hall.\n\ncox, d. r. and wermuth, n. (1996) multivariate\ndependencies: models, analysis and interpretation.\nlondon: chapman & hall.\n\ncraig, p. s., goldstein, m., seheult, a. h. and smith,\nj. a. (1998) constructing partial prior specifications for\nmodels of complex physical systems (with discussion).\nthe statistician 47, 37\u201353.\n\ncressie, n. a. c. (1991) statistics for spatial data. new\nyork: wiley.\n\ncrowder, m. j., kimber, a. c., smith, r. l. and\nsweeting, t. j. (1991) statistical analysis of reliability\ndata. london: chapman & hall.\n\ncruddas, a. m., reid, n. and cox, d. r. (1989) a time\nseries illustration of approximate conditional likelihood.\nbiometrika 76, 231\u2013237.\n\ndalal, s. r., fowlkes, e. b. and hoadley, b. (1989) risk\nanalysis of the space shuttle: pre-challenger prediction\nof failure. journal of the american statistical\nassociation 84, 945\u2013957.\n\ndaley, d. j. and vere-jones, d. (1988) an introduction\nto the theory of point processes. new york: springer.\n\ndaniels, h. e. (1954) saddlepoint approximations in\nstatistics. annals of mathematical statistics 25,\n631\u2013650.\n\ndaniels, h. e. (1987) tail probability approximations.\ninternational statistical review 54, 37\u201348.\n\ndavison, a. c. (2001) biometrika centenary: theory\nand general methodology. biometrika 88, 13\u201352.\nreprinted in biometrika: one hundred years, edited by\nd. m. titterington and d. r. cox. oxford university\npress, [11]\u2013[50].\n\ndavison, a. c. and hinkley, d. v. (1997) bootstrap\nmethods and their application. cambridge: cambridge\nuniversity press.\n\ndavison, a. c. and smith, r. l. (1990) models for\nexceedances over high thresholds (with discussion).\njournal of the royal statistical society series b 52,\n393\u2013442.\n\ndavison, a. c. and snell, e. j. (1991) residuals and\ndiagnostics. in statistical theory and modelling: in\nhonour of sir david cox, frs, eds d. v. hinkley,\nn. reid and e. j. snell, pp. 83\u2013106. london: chapman\n& hall.\n\n "}, {"Page_number": 715, "text": "bibliography\n\n703\n\ndavison, a. c. and tsai, c.-l. (1992) regression model\ndiagnostics. international statistical review 60,\n337\u2013353.\n\ndawid, a. p. (2000) causality without counterfactuals\n(with discussion). journal of the american statistical\nassociation 95, 407\u2013448.\n\nde finetti, b. (1974) theory of probability: volume 1.\nnew york: wiley.\n\nde finetti, b. (1975) theory of probability: volume 2.\nnew york: wiley.\n\nde stavola, b. l. (1988) testing departures from time\nhomogeneity in multistate markov processes. applied\nstatistics 37, 242\u2013250.\n\ndegroot, m. h. (1986a) a conversation with david\nblackwell. statistical science 1, 40\u201353.\n\ndegroot, m. h. (1986b) a conversation with charles\nstein. statistical science 1, 454\u2013462.\n\ndegroot, m. h. (1987a) a conversation with george\nbox. statistical science 2, 239\u2013258.\n\ndegroot, m. h. (1987b) a conversation with c. r. rao.\nstatistical science 2, 53\u201367.\n\ndempster, a. p., laird, n. m. and rubin, d. b. (1977)\nmaximum likelihood from incomplete data via the em\nalgorithm (with discussion). journal of the royal\nstatistical society series b 39, 1\u201338.\n\ndesmond, a. and moore, j. (1991) darwin. london:\npenguin.\n\ndiggle, p. j. (1983) statistical analysis of spatial point\npatterns. london: academic press.\n\ndiggle, p. j. (1990) time series: a biostatistical\nintroduction. oxford: clarendon press.\n\ndiggle, p. j., liang, k.-y. and zeger, s. l. (1994)\nanalysis of longitudinal data. oxford: clarendon press.\n\ndobson, a. j. (1990) an introduction to generalized\nlinear models. london: chapman & hall.\n\ndraper, n. r. and smith, h. (1981) applied regression\nanalysis. second edition. new york: wiley.\n\neco, u. (1984) the name of the rose. london: pan\nbooks.\n\nedwards, a. w. f. (1972) likelihood. cambridge:\ncambridge university press.\n\nedwards, d. (2000) introduction to graphical\nmodelling. second edition. new york: springer.\n\nefron, b. (1986) double exponential families and their\nuse in generalized linear regression. journal of the\namerican statistical association 81, 709\u2013721.\n\nefron, b. (1988) computer-intensive methods in\nstatistical regression. siam review 30, 421\u2013449.\n\nefron, b. (1996) empirical bayes methods for\ncombining likelihoods (with discussion). journal of the\namerican statistical association 91, 538\u2013565.\nefron, b. and hinkley, d. v. (1978) assessing the\naccuracy of the maximum likelihood estimator:\nobserved versus expected fisher information.\nbiometrika 65, 457\u2013481.\nefron, b. and thisted, r. (1976) estimating the number\nof unseen species: how many words did shakespeare\nknow? biometrika 63, 435\u2013448.\nefron, b. and tibshirani, r. j. (1993) an introduction to\nthe bootstrap. new york: chapman & hall.\nembrechts, p., kl\u00a8uppelberg, c. and mikosch, t. (1997)\nmodelling extremal events for insurance and finance.\nberlin: springer.\nfaddy, m. j. and fenlon, j. s. (1999) stochastic\nmodelling of the invasion process of nematodes in fly\nlarvae. applied statistics 48, 31\u201337.\nfan, j. and gijbels, i. (1996) local polynomial\nmodelling and its applications. london: chapman &\nhall.\nfeigl, p. and zelen, m. (1965) estimation of exponential\nsurvival probabilities with concomitant information.\nbiometrics 21, 826\u2013838.\nferguson, t. s. (1967) mathematical statistics: a\ndecision-theoretic approach. new york: academic\npress.\n\nfernholtz, l. t. and morgenthaler, s. (2000) a\nconversation with john w. tukey and elizabeth tukey.\nstatistical science 15, 79\u201394.\nfienberg, s. e. (1980) the analysis of cross-classified\ncategorical data. second edition. cambridge,\nmassachussetts: mit press.\n\nfindley, d. f. and parzen, e. (1995) a conversation with\nhirotugu akaike. statistical science 10, 104\u2013117.\nfirth, d. (1991) generalized linear models. in statistical\ntheory and modelling: in honour of sir david cox,\nfrs, eds d. v. hinkley, n. reid and e. j. snell, pp.\n55\u201382. london: chapman & hall.\n\nfirth, d. (1993) recent developments in\nquasi-likelihood methods. bulletin of the 49th session of\nthe international statistical institute pp. 341\u2013358.\n\nfisher, r. a. (1922) on the mathematical foundations of\ntheoretical statistics. philosophical transactions of the\nroyal society of london, series a 222, 309\u2013368.\nfisher, r. a. (1925) theory of statistical estimation.\nproceedings of the cambridge philosophical society 22,\n700\u2013725.\nfisher, r. a. (1934) two new properties of\nmathematical likelihood. proceedings of the royal\nsociety of london, series a 144, 285\u2014307.\n\n "}, {"Page_number": 716, "text": "704\n\nbibliography\n\nfisher, r. a. (1935a) the design of experiments.\nedinburgh: oliver and boyd.\nfisher, r. a. (1935b) the logic of inductive inference.\njournal of the royal statistical society 98, 39\u201354.\nfisher, r. a. (1956) statistical methods and scientific\ninference. edinburgh: oliver and boyd.\nfisher, r. a. (1990) statistical methods, experimental\ndesign, and scientific inference. oxford: clarendon\npress.\nfisher, r. a. and tippett, l. h. c. (1928) limiting\nforms of the frequency distributions of the largest or\nsmallest member of a sample. proceedings of the\ncambridge philosophical society 24, 180\u2013190.\n\nfisher box, j. (1978) r. a. fisher: the life of a\nscientist. new york: wiley.\n\nfishman, g. s. (1996) monte carlo concepts,\nalgorithms, and applications. new york: springer.\n\nfleiss, j. l. (1986) the design and analysis of clinical\nexperiments. new york: wiley.\n\nfleming, t. r. and harrington, d. p. (1991) counting\nprocesses and survival analysis. new york: wiley.\n\nforster, j. j., mcdonald, j. w. and smith, p. w. f. (1996)\nmonte carlo exact conditional tests for log-linear and\nlogistic models. journal of the royal statistical society\nseries b 58, 445\u2013453.\n\nfraser, d. a. s. (1968) the structure of inference. new\nyork: wiley.\n\nfraser, d. a. s. (1979) inference and linear models.\nnew york: mcgraw hill.\n\nfraser, d. a. s. (2003) likelihood for component\nparameters. biometrika 90, toappear.\n\nfrome, e. l. (1983) the analysis of rates using poisson\nregression models. biometrics 39, 665\u2013674.\n\ngamerman, d. (1997) markov chain monte carlo:\nstochastic simulation for bayesian inference. london:\nchapman & hall.\ngaver, d. p. and o\u2019muircheartaigh, i. g. (1987) robust\nempirical bayes analysis of event rates. technometrics\n29, 1\u201315.\ngelfand, a. e., hills, s. e., racine-poon, a. and smith,\na. f. m. (1990) illustration of bayesian inference in\nnormal data models using gibbs sampling. journal of\nthe american statistical association 85, 972\u2013985.\n\ngelfand, a. e. and smith, a. f. m. (1990)\nsampling-based approaches to calculating marginal\ndensities. journal of the american statistical\nassociation 85, 398\u2013409.\n\ngelman, a., carlin, j. b., stern, h. s. and rubin, d. b.\n(1995) bayesian data analysis. london: chapman &\nhall.\n\ngeman, s. and geman, d. (1984) stochastic relaxation,\ngibbs distributions, and the bayesian restoration of\nimages. ieee transactions on pattern analysis and\nmachine intelligence 6, 721\u2013741.\n\ngilks, w. r., richardson, s. and spiegelhalter, d. j.\n(eds) (1996) markov chain monte carlo in practice.\nlondon: chapman & hall.\n\ngilks, w. r. and wild, p. (1992) adaptive rejection\nsampling for gibbs sampling. applied statistics 41,\n337\u2013348.\n\nglonek, g. f. v. and mccullagh, p. (1995) multivariate\nlogistic models. journal of the royal statistical society\nseries b 57, 533\u2013546.\n\ngodambe, v. p. (1985) the foundations of finite sample\nestimation in stochastic processes. biometrika 72,\n419\u2013428.\n\ngodambe, v. p. (ed.) (1991) estimating functions.\noxford: clarendon press.\n\ngoldstein, h. (1995) multilevel statistical methods.\nsecond edition. london: edward arnold.\n\ngouri\u00b4eroux, c. (1997) arch models and financial\napplications. new york: springer.\n\ngreen, p. j. (1984) iteratively reweighted least squares\nfor maximum likelihood estimation and some robust and\nresistant alternatives (with discussion). journal of the\nroyal statistical society series b 46, 149\u2013192.\n\ngreen, p. j. (1995) reversible jump markov chain\nmonte carlo computation and bayesian model\ndetermination. biometrika 82, 711\u2013732.\n\ngreen, p. j. (2001) a primer on markov chain monte\ncarlo. in complex stochastic systems, eds\nc. kl\u00a8uppelberg, o. e. barndorff-nielsen and d. r. cox,\npp. 1\u201362. london: chapman & hall.\n\ngreen, p. j. and silverman, b. w. (1994) nonparametric\nregression and generalized linear models: a\nroughness penalty approach. london: chapman &\nhall.\n\ngreenland, s. (2001) letter to the editor. the american\nstatistician 55, 172.\n\ngrimmett, g. r. and stirzaker, d. r. (2001) probability\nand random processes. third edition. oxford:\nclarendon press.\ngrimmett, g. r. and welsh, d. j. a. (1986) probability:\nan introduction. oxford: clarendon press.\ngumbel, e. j. (1958) statistics of extremes. new york:\ncolumbia university press.\n\nguttorp, p. (1991) statistical inference for branching\nprocesses. new york: wiley.\n\nguttorp, p. (1995) stochastic modelling of scientific\ndata. london: chapman & hall.\n\n "}, {"Page_number": 717, "text": "bibliography\n\n705\n\nhall, p. g. and heyde, c. c. (1980) martingale limit\ntheory and its application. new york: academic press.\n\nhampel, f. r., ronchetti, e. m., rousseeuw, p. j. and\nstahel, w. a. (1986) robust statistics: the approach\nbased on influence functions. new york: wiley.\n\nhartley, h. o. and rao, j. n. k. (1967)\nmaximum-likelihood estimation for the mixed analysis\nof variance model. biometrika 54, 93\u2013108.\n\nhastie, t. j. and loader, c. (1993) local regression:\nautomatic kernel carpentry (with discussion). statistical\nscience 8, 120\u2013143.\n\nhastie, t. j. and tibshirani, r. j. (1990) generalized\nadditive models. london: chapman & hall.\n\nhastings, w. k. (1970) monte carlo sampling methods\nusing markov chains and their applications. biometrika\n57, 97\u2013109.\n\nheitjan, d. f. (1994) ignorability in general\nincomplete-data models. biometrika 81, 701\u2013708.\n\nhenderson, c. r. (1953) estimation of variance and\ncovariance components. biometrics 9, 226\u2013252.\n\nhenderson, r. and matthews, j. n. s. (1993) an\ninvestigation of changepoints in the annual number of\ncases of haemolytic uraemic syndrome. applied\nstatistics 42, 461\u2013471.\n\nheyde, c. c. (1997) quasi-likelihood and its\napplication: a general approach to optimal parameter\nestimation. new york: springer.\n\nheyde, c. c. and seneta, e. (eds) (2001) statisticians of\nthe centuries. new york: springer.\nhills, s. e. (1987) contribution to the discussion of cox,\nd. r. and reid, n., parameter orthogonality and\napproximate conditional inference. journal of the royal\nstatistical society series b 49, 23\u201324.\n\nhinkley, d. v. (1985) transformation diagnostics for\nlinear models. biometrika 72, 487\u2013496.\n\nhoaglin, d. c., mosteller, f. and tukey, j. w. (eds)\n(1983) understanding robust and exploratory data\nanalysis. new york: wiley.\n\nhoaglin, d. c., mosteller, f. and tukey, j. w. (eds)\n(1985) exploring data tables, trends, and shapes.\nnew york: wiley.\nhoaglin, d. c., mosteller, f. and tukey, j. w. (eds)\n(1991) fundamentals of exploratory analysis of\nvariance. new york: wiley.\n\nhoel, d. g. and walburg, h. e. (1972) statistical\nanalysis of survival experiments. journal of the national\ncancer institute 49, 361\u2013372.\n\nhoerl, a. e. and kennard, r. w. (1970a) ridge\nregression: biased estimation for nonorthogonal\nproblems. technometrics 12, 661\u2013676.\n\nhoerl, a. e. and kennard, r. w. (1970b) ridge\nregression: applications to nonorthogonal problems.\ntechnometrics 12, 69\u201382.\n\nhoerl, a. e., kennard, r. w. and hoerl, r. w. (1985)\npractical use of ridge regression: a challenge met.\napplied statistics 34, 114\u2013120.\n\nhoeting, j. a., madigan, d., raftery, a. e. and volinsky,\nc. t. (1999) bayesian model averaging: a tutorial (with\ndiscussion). statistical science 14, 382\u2013417.\n\nholland, p. w. (1986) statistics and causal inference\n(with discussion). journal of the american statistical\nassociation 81, 945\u2013970.\n\nhougaard, p. (1984) life table methods for\nheterogeneous populations: distributions describing the\nheterogeneity. biometrika 71, 75\u201383.\n\nhougaard, p. (2000) analysis of multivariate survival\ndata. new york: springer.\n\nhuber, p. j. (1981) robust statistics. new york: wiley.\n\nhurvich, c. m. and tsai, c.-l. (1989) regression and\ntime series model selection in small samples. biometrika\n76, 297\u2013307.\nhurvich, c. m. and tsai, c.-l. (1990) the impact of\nmodel selection on inference in linear regression. the\namerican statistician 44, 214\u2013217.\n\nhurvich, c. m. and tsai, c.-l. (1991) bias of the\ncorrected aic criterion for underfitted regression and\ntime series models. biometrika 78, 499\u2013509.\nisham, v. (1981) an introduction to spatial point\nprocesses and markov random fields. international\nstatistical review 49, 21\u201343.\n\nisham, v. s. (1991) modelling stochastic phenomena. in\nstatistical theory and modelling: in honour of sir\ndavid cox, frs, eds d. v. hinkley, n. reid and e. j.\nsnell, pp. 177\u2013203. london: chapman & hall.\njamshidian, m. and jennrich, r. i. (1997) acceleration\nof the em algorithm by using quasi-newton methods.\njournal of the royal statistical society series b 59,\n569\u2013587.\n\njeffreys, h. (1961) theory of probability. third edition.\noxford: clarendon press.\n\njelinski, z. and moranda, p. b. (1972) software\nreliability research. in statistical computer performance\nevaluation, ed. w. freiberger, pp. 465\u2013484. london:\nacademic press.\n\njensen, f. v. (2001) bayesian networks and decision\ngraphs. new york: springer.\n\njensen, j. l. (1995) saddlepoint approximations.\noxford: clarendon press.\n\nj\u00f8rgensen, b. (1997a) the theory of linear models.\nnew york: chapman & hall.\n\n "}, {"Page_number": 718, "text": "706\n\nbibliography\n\nj\u00f8rgensen, b. (1997b) the theory of dispersion models.\nnew york: chapman & hall.\n\nkadane, j. b. and wolfson, l. j. (1998) experiences in\nelicitation (with discussion). the statistician 47, 3\u201319.\n\nkalbfleisch, j. d. (1974) some efficiency calculations\nfor survival distributions. biometrika 61, 31\u201338.\n\nkalbfleisch, j. d. and prentice, r. l. (1973) marginal\nlikelihoods based on cox\u2019s regression and life model.\nbiometrika 60, 267\u2013278.\n\nkalbfleisch, j. d. and prentice, r. l. (1980) the\nstatistical analysis of failure time data. new york:\nwiley.\n\nkalbfleisch, j. g. (1985) probability and statistical\ninference. second edition, volume 2. new york:\nspringer.\n\nkaplan, e. l. and meier, p. (1958) nonparametric\nestimation from incomplete observations. journal of the\namerican statistical association 53, 457\u2013481.\n\nkarr, a. f. (1991) point processes and their statistical\ninference. second edition. new york: marcel dekker.\n\nkass, r. e. and wasserman, l. (1996) the selection of\nprior distributions by formal rules. journal of the\namerican statistical association 91, 1343\u20131370.\n\nkeiding, n. (1990) statistical inference in the lexis\ndiagram. philosophical transactions of the royal\nsociety of london, series a 332, 487\u2013509.\n\nkendall, m. g. and stuart, a. (1973) the advanced\ntheory of statistics, volume 2: inference and\nrelationship. third edition. london: griffin.\n\nkendall, m. g. and stuart, a. (1976) the advanced\ntheory of statistics, volume 3: design and analysis, and\ntime series. third edition. london: griffin.\n\nkendall, m. g. and stuart, a. (1977) the advanced\ntheory of statistics, volume 1: distribution theory.\nfourth edition. london: griffin.\n\nkenward, m. g. and molenberghs, g. (1998) likelihood\nbased frequentist inference when data are missing at\nrandom. statistical science 13, 236\u2013247.\n\nkinderman, r. and snell, j. l. (1980) markov random\nfields and their applications. volume 1 of\ncontemporary mathematics. providence, rhode island:\namerican mathematical society.\nklein, j. p. (1992) semiparametric estimation of random\neffects using the cox model based on the em algorithm.\nbiometrics 48, 175\u2013806.\nklein, j. p. and moeschberger, m. l. (1997) survival\nanalysis: techniques for censored and truncated data.\nnew york: springer.\n\nkullback, s. and leibler, r. a. (1951) on information\nand sufficiency. annals of mathematical statistics 22,\n79\u201386.\nk\u00a8unsch, h. r. (2001) state space and hidden markov\nmodels. in complex stochastic systems, eds\nc. kl\u00a8uppelberg, o. e. barndorff-nielsen and d. r. cox,\npp. 109\u2013173. london: chapman & hall.\nkuonen, d. (1999) saddlepoint approximations for\ndistributions of quadratic forms in normal variables.\nbiometrika 86, 929\u2013935.\nlauritzen, s. l. (1996) graphical models. oxford:\nclarendon press.\nlauritzen, s. l. (2001) causal inference from graphical\nmodels. in complex stochastic systems, eds\nc. kl\u00a8uppelberg, o. e. barndorff-nielsen and d. r. cox,\npp. 63\u2013107. london: chapman & hall.\nlauritzen, s. l. and richardson, t. s. (2002) chain\ngraph models and their causal interpretations (with\ndiscussion). journal of the royal statistical society\nseries b 64, 321\u2013361.\nlauritzen, s. l. and spiegelhalter, d. j. (1988) local\ncomputations with probabilities on graphical structures\nand their application to expert systems (with\ndiscussion). journal of the royal statistical society\nseries b 50, 157\u2013224.\nleadbetter, m. r., lindgren, g. and rootz\u00b4en, h. (1983)\nextremes and related properties of random sequences\nand processes. new york: springer.\nlee, p. m. (1997) bayesian statistics: an introduction.\nsecond edition. london: edward arnold.\nlehmann, e. l. (1983) theory of point estimation. new\nyork: wiley.\nlehmann, e. l. (1990) model specification: the views\nof fisher and neyman, and later developments.\nstatistical science 5, 160\u2013168.\nleonard, t. and hsu, j. s. j. (1999) bayesian methods:\nan analysis for statisticians and interdisciplinary\nresearchers. cambridge university press.\nleonard, t., hsu, j. s. j. and ritter, c. (1994) the\nlaplacian t-approximation in bayesian inference.\nstatistica sinica 4, 127\u2013142.\nli, g. (1985) robust regression. in exploring data\ntables, trends, and shapes, eds f. m. d. c. hoaglin and\nj. w. tukey, pp. 281\u2013343. new york: wiley.\nli, w.-h. (1997) molecular evolution. sunderland, ma:\nsinauer.\nliang, k., zeger, s. l. and qaqish, b. (1992)\nmultivariate regression analyses for categorical data\n(with discussion). journal of the royal statistical\nsociety series b 54, 3\u201340.\n\nknight, k. (2000) mathematical statistics. new york:\nchapman & hall.\n\nlindley, d. v. (1985) making decisions. new york:\nwiley.\n\n "}, {"Page_number": 719, "text": "bibliography\n\n707\n\nlindley, d. v. (2000) the philosophy of statistics (with\ncomments). the statistician 49, 293\u2013337.\n\nlindley, d. v. and scott, w. f. (1984) new cambridge\nelementary statistical tables. cambridge: cambridge\nuniversity press.\n\nlindsay, b. g. (1995) mixture models: theory,\ngeometry, and applications. number 5 in nsf-cbms\nregional conference series in probability and statistics.\nhayward, ca: institute for mathematical statistics.\n\nlinhart, h. and zucchini, w. (1986) model selection.\nnew york: wiley.\n\nlittle, r. j. a. and rubin, d. b. (1987) statistical\nanalysis with missing data. new york: wiley.\n\nlloyd, c. j. (1992) effective conditioning. australian\njournal of statistics 34, 241\u2013260.\n\nloader, c. (1999) local regression and likelihood.\nnew york: springer.\n\nmacdonald, i. l. and zucchini, w. (1997) hidden\nmarkov and other models for discrete-valued time\nseries. london: chapman & hall.\n\nmallows, c. l. (1973) some comments on c p.\ntechnometrics 15, 661\u2013675.\n\nmantel, n. and haenszel, w. (1959) statistical aspects\nof the analysis of data from retrospective studies of\ndisease. journal of the national cancer institute 22,\n719\u2013748.\n\nmardia, k. v., kent, j. t. and bibby, j. m. (1979)\nmultivariate analysis. london: academic press.\n\nmaritz, j. s. and lwin, t. (1989) empirical bayes\nmethods. second edition. london: chapman & hall.\n\nmccullagh, p. (1987) tensor methods in statistics.\nlondon: chapman & hall.\n\nmccullagh, p. (1991) quasi-likelihood and estimating\nfunctions. in statistical theory and modelling: in\nhonour of sir david cox, frs, eds d. v. hinkley,\nn. reid and e. j. snell, pp. 265\u2013286. london: chapman\n& hall.\n\nmccullagh, p. (1992) conditional inference and cauchy\nmodels. biometrika 79, 247\u2013259.\n\nmccullagh, p. and nelder, j. a. (1989) generalized\nlinear models. second edition. london: chapman &\nhall.\n\nmcculloch, c. e. (1997) maximum likelihood\nalgorithms for generalized linear mixed models. journal\nof the american statistical association 92, 162\u2013170.\n\nmcculloch, c. e. and searle, s. r. (2001) generalized,\nlinear, and mixed models. new york: wiley.\n\nmclachlan, g. j. and krishnan, t. (1997) the em\nalgorithm and extensions. new york: wiley.\n\nmcleish, d. and small, c. g. (1994) hilbert space\nmethods in probability and statistical inference.\nnew york: wiley.\n\nmcquarrie, a. d. r. and tsai, c.-l. (1998) regression\nand time series model selection. singapore: world\nscientific.\n\nmeng, x.-l. and van dyk, d. (1997) the em algorithm\n\u2014 anold folk-song sung to a fast new tune (with\ndiscussion). journal of the royal statistical society\nseries b 59, 511\u2013567.\n\nmetropolis, n., rosenbluth, a. w., rosenbluth, m. n.,\nteller, a. h. and teller, e. (1953) equations of state\ncalculations by fast computing machines. journal of\nchemical physics 21, 1087\u20131091.\nmiller, a. j. (1990) subset selection in regression.\nlondon: chapman & hall.\nmiller, r. g. (1981) survival analysis. new york: wiley.\nmolenberghs, g., kenward, m. g. and goetghebeur, e.\n(2001) sensitivity analysis for incomplete contingency\ntables: the slovenian plebiscite case. applied statistics\n50, 15\u201329.\nmorgan, b. j. t. (1984) elements of simulation.\nlondon: chapman & hall.\nmorris, c. n. (1982) natural exponential families with\nquadratic variance functions. annals of statistics 10,\n65\u201380.\nmorris, c. n. (1983) parametric empirical bayes\ninference: theory and applications. journal of the\namerican statistical association 78, 47\u201365.\nmosteller, f. and tukey, j. w. (1977) data analysis and\nregression. reading, massachussetts: addison\u2013wesley.\n\nnelder, j. a. and wedderburn, r. w. m. (1972)\ngeneralized linear models. journal of the royal\nstatistical society series a 135, 370\u2013384.\nnelson, w. d. and hahn, g. j. (1972) linear estimation\nof a regression relationship from censored data.\npart 1 \u2014 simple methods and their application (with\ndiscussion). technometrics 14, 247\u2013276.\nneopolitan, e. (1990) probabilistic reasoning in expert\nsystems. new york: wiley.\nneyman, j. and pearson, e. s. (1967) joint statistical\npapers. cambridge university press.\nneyman, j. and scott, e. l. (1948) consistent estimates\nbased on partially consistent observations. econometrica\n16, 1\u201332.\nnorris, j. r. (1997) markov chains. cambridge:\ncambridge university press.\noakes, d. (1991) life-table analysis. in statistical\ntheory and modelling: in honour of sir david cox,\nfrs, eds d. v. hinkley, n. reid and e. j. snell,\npp. 107\u2013128. london: chapman & hall.\n\n "}, {"Page_number": 720, "text": "708\n\nbibliography\n\noakes, d. (1999) direct calculation of the information\nmatrix via the em algorithm. journal of the royal\nstatistical society series b 61, 479\u2013482.\n\nogata, y. (1988) statistical models for earthquake\noccurrences and residual analysis for point processes.\njournal of the american statistical association 83, 9\u201327.\n\no\u2019hagan, a. (1988) probability: methods and\nmeasurement. london: chapman & hall.\n\no\u2019hagan, a. (1998) eliciting expert beliefs in\nsubstantial practical applications (with discussion). the\nstatistician 47, 21\u201335.\n\npace, l. and salvan, a. (1997) principles of statistical\ninference from a neo-fisherian perspective. singapore:\nworld scientific.\n\npatterson, h. d. and thompson, r. (1971) recovery of\ninter-block information when block sizes are unequal.\nbiometrika 58, 545\u2013554.\n\npearl, j. (1988) probabilistic reasoning in intelligent\nsystems: networks of plausible inference.\nsan francisco: morgan kaufmann.\npearl, j. (2000) causality: models, reasoning and\ninference. cambridge: cambridge university press.\npearson, e. s. and hartley, h. o. (1976) biometrika\ntables for statisticians. third edition, volumes 1 and 2.\nlondon: biometrika trust: university college.\npercival, d. b. and walden, a. t. (1993) spectral\nanalysis for physical applications: multitaper and\nconventional univariate techniques. cambridge:\ncambridge university press.\npirazzoli, p. a. (1982) maree estreme a venezia\n(periodo 1872\u20131981). acqua aria 10, 1023\u20131039.\npitman, e. j. g. (1938) the estimation of location and\nscale parameters of a continuous population of any given\nform. biometrika 30, 391\u2013421.\npitman, e. j. g. (1939) tests of hypotheses concerning\nlocation and scale parameters. biometrika 31, 200\u2013215.\np\u00a8otscher, b. m. (1991) effects of model selection on\ninference. econometric theory 7, 163\u2013185.\nprentice, r. l. and gloeckler, l. a. (1978) regression\nanalysis of grouped survival data with application to\nbreast cancer data. biometrics 34, 57\u201367.\nprentice, r. l., kalbfleisch, j. d., peterson, a. v.,\nflournoy, n., farewell, v. t. and breslow, n. e. (1978)\nthe analysis of failure times in the presence of\ncompeting risks. biometrics 34, 541\u2013554.\npriestley, m. b. (1981) spectral analysis and time\nseries. london: academic press.\nprum, b., rodolphe, f. and de turckheim, e. (1995)\nfinding words with unexpected frequencies in\ndeoxyribonucleic acid sequences. journal of the royal\nstatistical society series b 57, 205\u2013220.\n\nraftery, a. e. (1988) analysis of a simple debugging\nmodel. applied statistics 37, 12\u201322.\n\nraiffa, h. and schlaifer, r. (1961) applied statistical\ndecision theory. cambridge, mass: mit press.\n\nrao, c. r. (1973) linear statistical inference and its\napplications. second edition. new york: wiley.\n\nrawlings, j. o. (1988) applied regression analysis: a\nresearch tool. pacific grove, california: wadsworth &\nbrooks/cole.\n\nreid, n. (1988) saddlepoint methods and statistical\ninference (with discussion). statistical science 3,\n213\u2013238.\n\nreid, n. (1994) a conversation with sir david cox.\nstatistical science 9, 439\u2013455.\n\nreid, n. (1995) the roles of conditioning in inference\n(with discussion). statistical science 10, 138\u2013199.\n\nreid, n. (2003) asymptotics and the theory of\ninference. annals of statistics, toappear.\n\nresnick, s. i. (1987) extreme values, point processes\nand regular variation. new york: springer.\n\nreynolds, p. s. (1994) time-series analyses of beaver\nbody temperatures. in case studies in biometry, eds\nn. lange, l. ryan, l. billard, d. r. brillinger,\nl. conquest and j. greenhouse, pp. 211\u2013228.\nnew york: wiley.\n\nrice, j. a. (1988) mathematical statistics and data\nanalysis. belmont, california: wadsworth &\nbrooks/cole.\n\nrichardson, s. and green, p. j. (1997) on bayesian\nanalysis of mixtures with an unknown number of\ncomponents (with discussion). journal of the royal\nstatistical society series b 59, 731\u2013792.\n\nripley, b. d. (1981) spatial statistics. new york: wiley.\n\nripley, b. d. (1987) stochastic simulation. new york:\nwiley.\n\nripley, b. d. (1988) statistical inference for spatial\nprocesses. cambridge: cambridge university press.\n\nrobert, c. p. (2001) the bayesian choice. second\nedition. new york: springer.\n\nrobert, c. p. and casella, g. (1999) monte carlo\nstatistical methods. new york: springer.\n\nrobinson, g. k. (1991) that blup is a good thing: the\nestimation of random effects (with discussion).\nstatistical science 3, 15\u201351.\n\nroeder, k. (1990) density estimation with confidence\nsets exemplified by superclusters and voids in galaxies.\njournal of the american statistical association 85,\n617\u2013624.\n\n "}, {"Page_number": 721, "text": "bibliography\n\n709\n\nrolski, t., schmidli, h., schmidt, v. and teugels, j.\n(1999) stochastic processes for insurance and finance.\nchichester: wiley.\n\nross, s. m. (1996) bayesians should not resample a\nprior sample to learn about the posterior. american\nstatistician 50, 116.\n\nrousseeuw, p. j. and leroy, a. m. (1987) robust\nregression and outlier detection. new york: wiley.\n\nrubin, d. b. (1976) inference and missing data (with\ndiscussion). biometrika 63, 581\u2013592.\n\nrubin, d. b. (1987) multiple imputation for\nnonresponse in surveys. new york: wiley.\n\nrubinstein, r. y. (1981) simulation and the monte\ncarlo method. new york: wiley.\n\nschafer, g. (1976) a mathematical theory of evidence.\nprinceton, nj: princeton university press.\n\nscheff\u00b4e, h. (1959) analysis of variance. new york:\nwiley.\n\nschervish, m. j. (1995) theory of statistics. new york:\nspringer.\n\nschwartz, g. (1978) estimating the dimension of a\nmodel. annals of statistics 6, 461\u2013464.\n\nscott, d. w. (1992) multivariate density estimation:\ntheory, practice, and visualization. new york: wiley.\n\nsearle, s. r. (1971) linear models. new york: wiley.\n\nsearle, s. r., casella, g. and mcculloch, c. e. (1992)\nvariance components. new york: wiley.\n\nseber, g. a. f. (1977) linear regression analysis.\nnew york: wiley.\n\nseber, g. a. f. (1985) multivariate observations.\nnew york: wiley.\n\nself, s. g. and liang, k.-y. (1987) asymptotic\nproperties of maximum likelihood estimators and\nlikelihood ratio tests under nonstandard conditions.\njournal of the american statistical association 82,\n605\u2013610.\n\nsen, a. and srivastava, m. (1990) regression analysis:\ntheory, methods, and applications. new york: springer.\n\nseverini, t. a. (1999) an empirical adjustment to the\nlikelihood ratio statistic. biometrika 86, 235\u2013247.\n\nseverini, t. a. (2000) likelihood methods in statistics.\noxford: clarendon press.\n\nshao, j. (1999) mathematical statistics. new york:\nspringer.\n\nsheather, s. j. and jones, m. c. (1991) a reliable\ndata-based bandwidth selection method for kernel\ndensity estimation. journal of the royal statistical\nsociety series b 53, 683\u2013690.\n\nsheehan, n. a. (2000) on the application of markov\nchain monte carlo methods to genetic analyses on\ncomplex pedigrees. international statistical review 68,\n83\u2013110.\n\nshephard, n. g. (1996) statistical aspects of arch and\nstochastic volatility. in time series models in\neconometrics, finance and other fields, eds d. r. cox,\nd. v. hinkley and o. e. barndorff-nielsen, pp. 1\u201367.\nlondon: chapman & hall.\n\nsilverman, b. w. (1986) density estimation for\nstatistics and data analysis. london: chapman & hall.\n\nsilvey, s. d. (1970) statistical inference. london:\nchapman & hall.\n\nsilvey, s. d. (1980) optimal design. london: chapman\n& hall.\n\nsimonoff, j. s. (1996) smoothing methods in statistics.\nnew york: springer.\n\nskovgaard, i. m. (1987) saddlepoint expansions for\nconditional distributions. journal of applied probability\n24, 875\u2013887.\n\nskovgaard, i. m. (1990) on the density of minimum\ncontrast estimators. annals of statistics 18, 779\u2013789.\n\nskovgaard, i. m. (1996) an explicit large-deviation\napproximation to one-parameter tests. bernoulli 2,\n145\u2013166.\nsmith, a. f. m. (1995) a conversation with dennis\nlindley. statistical science 10, 305\u2013319.\nsmith, a. f. m. and gelfand, a. e. (1992) bayesian\nstatistics without tears: a sampling-resampling\nperspective. american statistician 46, 84\u201388.\nsmith, j. q. (1988) decision analysis: a bayesian\napproach. london: chapman & hall.\n\nsmith, p. w. f., forster, j. j. and mcdonald, j. w. (1996)\nmonte carlo exact tests for square contingency tables.\njournal of the royal statistical society series a 159,\n309\u2013321.\n\nsmith, r. l. (1985) maximum likelihood estimation in a\nclass of non-regular cases. biometrika 72, 67\u201392.\n\nsmith, r. l. (1989a) extreme value analysis of\nenvironmental time series: an example based on ozone\ndata (with discussion). statistical science 4, 367\u2013393.\n\nsmith, r. l. (1989b) a survey of nonregular problems.\nbulletin of the international statistical institute 53,\n353\u2013372.\n\nsmith, r. l. (1990) extreme value theory. in handbook\nof applicable mathematics, supplement, eds\nw. ledermann, e. lloyd, s. vajda and c. alexander,\nchapter 14. chichester: wiley.\n\nsmith, r. l. (1994) nonregular regression. biometrika\n81, 173\u2013183.\n\n "}, {"Page_number": 722, "text": "710\n\nbibliography\n\nsmith, r. l. (1997) introduction to besag (1974) spatial\ninteraction and the statistical analysis of lattice systems.\nin breakthroughs in statistics, volume 3, eds\ns. kotz and n. l. johnson, pp. 285\u2013291. new york:\nspringer.\ns\u00f8rensen, m. (1999) on asymptotics of estimating\nfunctions. brazilian journal of probability and statistics\n13, 111\u2013136.\nspiegelhalter, d. j., dawid, a. p., lauritzen, s. l. and\ncowell, r. g. (1993) bayesian analysis in expert\nsystems. statistical science 8, 219\u2013283.\nspiegelhalter, d. j. and smith, a. f. m. (1982) bayes\nfactors for linear and log-linear models with vague prior\ninformation. journal of the royal statistical society\nseries b 44, 377\u2013387.\nspiegelhalter, d. j., thomas, a., best, n. g. and gilks,\nw. r. (1996a) bugs 0.5: bayesian inference using\ngibbs sampling (version ii). cambridge: mrc\nbiostatistics unit.\nspiegelhalter, d. j., thomas, a., best, n. g. and gilks,\nw. r. (1996b) bugs 0.5 examples volume 1 (version\nii). cambridge: mrc biostatistics unit.\nspiegelhalter, d. j., thomas, a., best, n. g. and gilks,\nw. r. (1996c) bugs 0.5 examples volume 2 (version\nii). cambridge: mrc biostatistics unit.\nstein, c. (1956) inadmissibility of the usual estimator\nfor the mean of a multivariate normal distribution. in\nproceedings of the 3rd berkeley symposium on\nmathematical statistics and probability, volume 1,\npp. 197\u2013206. university of california press: berkeley,\nca.\n\nstephens, m. (2000) bayesian analysis of mixture\nmodels with an unknown number of components: an\nalternative to reversible jump methods. annals of\nstatistics 28, 40\u201374.\nstigler, s. m. (1986) the history of statistics: the\nmeasurement of uncertainty before 1900. cambridge,\nma: belknap press.\n\nstirzaker, d. r. (1994) elementary probability.\ncambridge: cambridge university press.\nstone, m. (1974) cross-validatory choice and\nassessment of statistical predictions (with discussion).\njournal of the royal statistical society series b 36,\n111\u2013147.\n\nstone, m. and brooks, r. j. (1990) continuum\nregression: cross-validated sequentially constructed\nprediction embracing ordinary least squares, partial least\nsquares and principal components regression (with\ndiscussion). journal of the royal statistical society\nseries b 52, 237\u2013269.\n\ntanner, m. a. (1996) tools for statistical inference:\nmethods for the exploration of posterior distributions\n\nand likelihood functions. third edition. new york:\nspringer.\n\ntaylor, g. l. and prior, a. m. (1938) blood groups in\nengland. annals of eugenics 8, 343\u2013355.\n\nthatcher, a. r. (1999) the long-term pattern of adult\nmortality and the highest attained age (with discussion).\njournal of the royal statistical society series a 162,\n5\u201343.\n\ntherneau, t. m. and grambsch, p. m. (2000) modeling\nsurvival data: extending the cox model. new york:\nspringer.\n\nthisted, r. and efron, b. (1987) did shakespeare\nwrite a newly-discovered poem? biometrika 74,\n445\u2013455.\n\nthompson, e. a. (2001) monte carlo methods on\ngenetic structures. in complex stochastic systems, eds\nc. kl\u00a8uppelberg, o. e. barndorff-nielsen and d. r. cox,\npp. 175\u2013218. london: chapman & hall.\n\ntierney, l. and kadane, j. b. (1986) accurate\napproximations for posterior moments and marginal\ndensities. journal of the american statistical\nassociation 81, 82\u201386.\n\ntierney, l., kass, r. e. and kadane, j. b. (1989)\napproximate marginal densities of nonlinear functions.\nbiometrika 76, 425\u2013433.\n\ntitterington, d. m., smith, a. f. m. and makov, u. e.\n(1985) statistical analysis of finite mixture\ndistributions. new york: wiley.\n\ntong, h. (1990) non-linear time series: a dynamical\nsystem approach. oxford: clarendon press.\n\ntsay, r. s. (2002) analysis of financial time series.\nnew york: wiley.\n\ntsiatis, a. a. (1998) competing risks. in encyclopedia\nof biostatistics, eds p. armitage and t. colton,\nvolume 1, pp. 824\u2013834. new york: wiley.\n\ntufte, e. r. (1983) the visual display of quantitative\ninformation. cheshire, connecticut: graphics press.\n\ntufte, e. r. (1990) envisioning information. cheshire,\nconnecticut: graphics press.\n\ntukey, j. w. (1949) one degree of freedom for\nnon-additivity. biometrics 5, 232\u2013242.\n\ntukey, j. w. (1977) exploratory data analysis.\nreading, massachussetts: addison\u2013wesley.\n\nvan der vaart, a. w. (1998) asymptotic statistics.\ncambridge university press.\n\nvan lieshout, m. n. m. (2000) markov point processes\nand their applications. singapore: world scientific.\n\nwand, m. p. and jones, m. c. (1995) kernel smoothing.\nlondon: chapman & hall.\n\n "}, {"Page_number": 723, "text": "bibliography\n\n711\n\nwedderburn, r. w. m. (1974) quasi-likelihood\nfunctions, generalized linear models, and the\ngauss\u2013newton method. biometrika 61, 439\u2013447.\n\nweisberg, s. (1985) applied linear regression. second\nedition. new york: wiley.\n\nwelsh, a. h. (1996) aspects of statistical inference.\nnew york: wiley.\n\nwermuth, n. and lauritzen, s. l. (1990) on substantive\nresearch hypotheses, conditional independence graphs\nand graphical chain models (with discussion).\njournal of the royal statistical society series b 52,\n21\u201372.\n\nwetherill, g. b. (1986) regression analysis with\napplications. london: chapman & hall.\n\nwhittaker, j. (1990) graphical models in applied\nmultivariate statistics. new york: wiley.\nwild, p. and gilks, w. r. (1993) algorithm as 287:\nadaptive rejection sampling from log-concave density\nfunctions. applied statistics 42, 701\u2013709.\nwood, s. n. (2000) modelling and smoothing parameter\nestimation with multiple quadratic penalties. journal of\nthe royal statistical society series b 62, 413\u2013428.\nwoods, h., steinour, h. h. and starke, h. r. (1932)\neffect of composition of portland cement on heat\nevolved during hardening. industrial engineering and\nchemistry 24, 1207\u20131214.\nyates, f. (1937) the design and analysis of factorial\nexperiments. technical report, imperial bureau of soil\nscience, harpenden. technical communication 35.\n\n "}, {"Page_number": 724, "text": "name index\n\naalen, o. o., 218, 222, 555\nagresti, a., 349, 555\nakaike, h., 152, 409\nalmond, r., 293\nandersen, p. k., 555\nanderson, d. r., 156, 409\nanderson, t. w., 293\nandrews, d. f., 692\nappleton, d. r., 258\naquinas, t., 150\narnold, b. c., 49\nartes, r., 173\nashford, j. r., 509\natkinson, a. c., 409, 464\navery, p. j., 225, 294\nazzalini, a., 348, 555, 560\n\nbalakrishnan, n., 49\nbarndorff-nielsen, o. e., 156, 218, 638,\n\n691, 692\n\nbartlett, m. s., 340, 349, 692\nbasawa, i. v., 348\nbasu, d., 649\nbayes, t., 11\nbellio, r., 692\nbelsley, d. a., 409\nberan, j., 293\nberan, r. j., 522\nberger, j. o., 638\nberger, r. l., 348\nbernardo, j. m., 638\nbernoulli, j., 30\nbesag, j. e., 255, 292, 533, 626, 638,\n\n639, 692\n\nbest, n. g., 579, 638\nbibby, j. m., 256, 293\nbickel, p. j., 49, 348\nbillingsley, p., 292\nbishop, y. m., 554\nbissell, a. f., 515\n\n712\n\nblackwell, d. h., 309\nbloomfield, p., 293\nborgan, \u00f8., 555\nbowman, a. w., 348, 555, 560\nbox, g. e. p., 356, 389, 391, 409, 413,\n\n421, 464, 638, 640\n\nbrazzale, a. r., 692\nbreslow, n. e., 218, 221\nbrillinger, d. r., 293\nbrockwell, p. j., 293\nbrooks, r. j., 409\nbrooks, s. p., 638\nbrown, b. w., 485, 491\nbrown, l. d., 218\nbrown, p. j., 409\nbr\u00b4emaud, p., 292\nburnham, k. p., 156, 409\nburns, e., 17\n\ncaffo, b., 349\ncalment, j., 196\ncarlin, b. p., 638\ncarlin, j. b., 638\ncarroll, r. j., 409\ncasella, g., 90, 348, 463, 464, 638\ncastillo, e., 293\ncatchpole, e. a., 149, 156\ncauchy, a. l., 33\nchai, p., 470\nchatfield, c., 48, 156, 293, 409\nchatterjee, s., 409\nchellappa, r., 292\ncheng, r. h. c., 156\ncleveland, w. s., 49\nclifford, p., 292, 692\ncobb, g. w., 463\ncochran, w. g., 453, 463\ncoles, s. g., 293\ncollett, d., 218, 554\ncollins, a. j., 293\n\ncook, r. d., 394, 409\ncopas, j. b., 208, 218\ncoull, b. a., 349\ncowell, r. g., 251, 253, 293\ncowles, m. k., 638\ncox, d. r., 4, 48, 49, 156, 218, 220, 292,\n\n293, 348, 349, 389, 391, 401, 409,\n432, 463, 464, 541, 543, 554, 555,\n557, 559, 564, 638, 691, 692, 693,\n695\n\ncox, g. m., 453, 463\ncraig, p. s., 638\ncram\u00b4er, h., 302\ncressie, n. a. c., 293\ncrowder, m. j., 218\ncruddas, a. m., 692\n\ndalal, s. r., 6\ndaley, d. j., 293\ndaniels, h. e., 692\ndarwin, c. r., 1, 2\ndavis, r. a., 293\ndavison, a. c., 156, 293, 409, 554, 555,\n\n557, 692\n\ndawid, a. p., 251, 253, 293, 464\nde finetti, b., 619, 638\nde stavola, b. l., 227\nde turckheim, e., 292\ndegroot, m. h., 302, 309, 389, 635\ndempster, a. p., 218\ndesmond, a., 1\ndiggle, p. j., 293, 555, 692\ndirac, p. a. m., 310\ndobson, a. j., 554\ndoksum, k. a., 49, 348\ndonev, a. n., 464\ndraper, n. r., 409\n\neco, u., 150\nedgeworth, f. y., 671\n\n "}, {"Page_number": 725, "text": "name index\n\nedwards, a. w. f., 156\nedwards, d., 292, 464\nefron, b., 376, 409, 496, 515, 628, 691\nembrechts, p., 278, 293\n\nfaddy, m. j., 294\nfan, j., 555\nfarewell, v. t., 218, 221\nfeigl, p., 541\nfenlon, j. s., 294\nferguson, t. s., 638\nfienberg, s. e., 554\nfindley, d. f., 152\nfirth, d., 554, 555\nfisher box, j., 3\nfisher, n. i., 522\nfisher, r. a., 2, 3, 90, 135, 156, 293,\n\n348, 349, 463, 464, 637, 691\n\nfishman, g. s., 90\nfleiss, j. l., 464\nfleming, t. r., 549, 555\nflournoy, n., 218, 221\nforster, j. j., 692\nfowlkes, e. b., 6\nfraser, d. a. s., 218, 692\nfrench, j. m., 258\nfrome, e. l., 8\n\ngalton, f., 3\ngamerman, d., 638\ngauss, j. c. f., 62, 374\ngaver, d. p., 600\ngelfand, a. e., 459, 618, 638\ngelman, a., 638\ngeman, d., 292, 638\ngeman, s., 292, 638\ngijbels, i., 555\ngilks, w. r., 93, 579, 638\ngill, r. d., 555\ngloeckler, l. a., 554\nglonek, g. f. v., 555\ngodambe, v. p., 325, 348\ngoetghebeur, e., 218\ngoldstein, h., 464\ngoldstein, m., 638\ngossett, w. s., 33\ngouri\u00b4eroux, c., 293\ngrambsch, p. m., 555\n\n713\n\ngreen, p. j., 219, 533, 554, 555, 561,\n\n626, 638\n\ngreenland, s., 349\ngreenwood, m., 197\ngrimmett, g. r., 49, 292, 293\ngumbel, e. j., 279, 293\nguti\u00b4errez, j. m., 293\nguttorp, p., 292, 348\n\njanossy, l., 159\njeffreys, h., 575, 637\njelinski, z., 299\njennrich, r. i., 218\njensen, f. v., 293\njensen, j. l., 692\njones, m. c., 348, 555\nj\u00f8rgensen, b., 409, 554\n\nhadi, a. s., 293, 409\nhaenszel, w., 558\nhahn, g. j., 615\nhall, p. g., 348\nhammersley, j. m., 292\nhampel, f. r., 348, 409\nharrington, d. p., 549, 555\nhartley, h. o., 63, 464, 692\nhastie, t. j., 555\nhastings, w. k., 638\nheitjan, d. f., 218\nhenderson, c. r., 464\nhenderson, r., 142, 225, 294\nheyde, c. c., 348, 555\nhigdon, d., 533, 626, 638\nhills, s. e., 459, 695\nhinkley, d. v., 156, 348, 349, 409, 691,\n\n692\n\nhoadley, b., 6\nhoaglin, d. c., 49\nhoel, d. g., 200\nhoerl, a. e., 409\nhoerl, r. w., 409\nhoeting, j. a., 638\nholland, p. w., 464, 554\nhollander, m., 485\nhougaard, p., 222, 292, 555\nhsu, j. s. j., 638\nhuber, p. j., 321, 409\nhunter, j. s., 356, 421, 464\nhunter, w. g., 356, 421, 464\nhurvich, c. m., 409\nh\u00a8ardle, w., 560\n\nisham, v. s., 292, 293\nising, e., 248\n\njain, a., 292\njamshidian, m., 218\n\nkadane, j. b., 638\nkalbfleisch, j. d., 218, 221, 562, 692\nkalbfleisch, j. g., 156\nkaplan, e. l., 197\nkarr, a. f., 293\nkass, r. e., 638\nkeiding, n., 218, 555\nkendall, m. g., 49, 160\nkennard, r. w., 409\nkent, j. t., 256, 293\nkenward, m. g., 218, 223\nkimber, a. c., 218\nkinderman, r., 292\nkiss, d., 159\nklein, j. p., 218, 555, 564\nkl\u00a8uppelberg, c., 278, 293\nknight, k., 49, 156\nkrishnan, t., 218\nkuh, e., 409\nkullback, s., 123\nkuonen, d., 560\nk\u00a8unsch, h. r., 292\n\nlaird, n. m., 218\nlaplace, p.-s., 22, 62, 637\nlauritzen, s. l., 251, 253, 292, 293,\n\n464\n\nleadbetter, m. r., 293\nlee, p. m., 638\nlehmann, e. l., 48, 348, 349\nleibler, r. a., 123\nleonard, t., 638\nleroy, a. m., 409\nlewis, p. a. w., 293, 693\nli, g., 409\nli, h. g., 218\nli, w.-h., 295\nliang, k.-y., 156, 505, 555, 692\nlindgren, g., 293\n\n "}, {"Page_number": 726, "text": "714\n\nlindley, d. v., 63, 638\nlindsay, b. g., 219\nlinhart, h., 409\nlittle, r. j. a., 218\nlloyd, c. j., 692\nloader, c., 555\nlouis, t. a., 638\nlwin, t., 638\n\nmacdonald, i. l., 292\nmadigan, d., 638\nmakov, u. e., 219\nmallows, c. l., 409\nmantel, n., 558\nmardia, k. v., 256, 293\nmaritz, j. s., 638\nmarkov, a. a., 228\nmatthews, j. n. s., 142\nmccullagh, p., 49, 554, 555, 558, 692,\n\n693\n\nmcculloch, c. e., 219, 463, 464\nmcdonald, j. w., 692\nmclachlan, g. j., 218\nmcleish, d., 348\nmcquarrie, a. d. r., 409\nmeier, p., 197\nmendel, g., 160\nmeng, x.-l., 218\nmengersen, k., 533, 626, 638\nmetropolis, n., 638\nmikosch, t., 278, 293\nmiller, a. j., 409\nmiller, h. d., 292\nmiller, n., 356\nmiller, r. g., 218\nmoeschberger, m. l., 218, 555\nmolenberghs, g., 218, 223\nmolli\u00b4e, a., 638\nmoore, j., 1\nmoranda, p. b., 299\nmorgan, b. j. t., 90, 149, 156\nmorris, c. n., 218, 639\nmosteller, f., 48, 49\n\nnadaraya, e. a., 522\nnagaraja, h. n., 49\nnelder, j. a., 554, 555, 558\nnelson, w. d., 615\n\nneopolitan, e., 293\nneyman, j., 90, 335, 348, 349, 646\nnorris, j. r., 292\n\no\u2019hagan, a., 638\no\u2019muircheartaigh, i. g., 600\noakes, d., 4, 192, 217, 218\nogata, y., 288\n\npace, l., 156, 218\npareto, v., 41\nparzen, e., 152\npatterson, h. d., 464, 692\npearl, j., 293, 464\npearson, e. s., 63, 90, 335, 348, 349\npearson, k., 135, 335\npercival, d. b., 293\npeterson, a. v., 218, 221\npirazzoli, p. a., 162\npitman, e. j. g., 218\npoisson, s. d., 23\npope john xxii, 150\nprentice, r. l., 218, 221, 554, 692\npresley, e., 559\npriestley, m. b., 293\nprior, a. m., 135\nprum, b., 292\np\u00a8otscher, b. m., 409\n\nqaqish, b., 505, 555\n\nracine-poon, a., 459\nraftery, a. e., 596, 638, 644\nraiffa, h., 638\nrao, c. r., 224, 302, 309\nrao, j. n. k., 464, 692\nrawlings, j. o., 409, 469\nreid, n., 389, 463, 691, 692, 695\nresnick, s. i., 293\nreynolds, p. s., 266\nriani, m., 409\nrice, j. a., 348\nrichardson, s., 219, 638\nrichardson, t. s., 292\nrilke, r. m., 174\nripley, b. d., 90, 293, 638\nritter, c., 638\nrobert, c. p., 90, 638\n\nname index\n\nrobinson, g. k., 464\nrodolphe, f., 292\nroeder, k., 214\nrolski, t., 293\nronchetti, e. m., 348, 409\nrootz\u00b4en, h., 293\nrosenbluth, a. w., 638\nrosenbluth, m. n., 638\nross, s. m., 618\nrousseeuw, p. j., 348, 409\nrubenstein, r. y., 90\nrubin, d. b., 218, 618, 638\nruppert, d., 409\nr\u00b4enyi, a., 40\n\nsalvan, a., 156, 218\nschafer, g., 577\nscheff\u00b4e, h., 409, 464\nschervish, m. j., 104\nschlaifer, r., 638\nschmidli, h., 293\nschmidt, v., 293\nschwartz, g., 409\nscott, d. j., 348\nscott, d. w., 348\nscott, e. l., 646\nscott, w. f., 63\nsearle, s. r., 409, 463, 464\nseber, g. a. f., 293, 409\nseheult, a. h., 638\nself, s. g., 156\nsen, a., 409\nseverini, t. a., 691, 692\nshao, j., 348\nsheather, s. j., 348\nsheehan, n. a., 292\nshephard, n., 293\nsilverman, b. w., 348, 555, 561\nsilvey, s. d., 156, 348, 464\nsimonoff, j. s., 555\nsimpson, e. h., 257\nskovgaard, i. m., 691, 692\nslutsky, e. e., 31\nsmall, c. g., 348\nsmith, a. f. m., 219, 459, 596, 618,\n\n638\n\nsmith, h., 409\nsmith, j. a., 638\n\n "}, {"Page_number": 727, "text": "name index\n\nsmith, j. q., 638\nsmith, p. w. f., 692\nsmith, r. l., 156, 218, 292, 293\nsnell, e. j., 49, 401, 409, 432, 541, 554\nsnell, j. l., 292\nspeed, t. p., 464\nspiegelhalter, d. j., 251, 253, 293, 579,\n\n596, 638\n\nsrivastava, m., 409\nstafford, j. e., 692\nstahel, w. a., 348, 409\nstarke, h. r., 355\nstein, c., 635, 639\nsteinour, h. h., 355\nstephens, m., 638\nstern, h. s., 638\nstigler, s. m., 3\nstirzaker, d. r., 49, 292, 293\nstone, m., 348, 409\nstuart, a., 49, 160\nstudent, see gossett, w. s.\nsweeting, t. j., 218\ns\u00f8rensen, m., 348\n\ntanner, m. a., 218, 638\ntaylor, g. l., 135\nteller, a. h., 638\nteller, e., 638\n\nteugels, j., 293\nthatcher, a. r., 194\ntherneau, t. m., 555\nthisted, r. , 628\nthomas, a., 579, 638\nthompson, e. a., 292\nthompson, r., 464, 692\ntiao, g. c., 638\ntibshirani, r. j., 409, 555\ntidwell, p. w., 413\ntierney, l., 638\ntippett, l. h. c., 293\ntitterington, d. m., 219\ntong, h., 293\ntraylor, l., 156\ntsai, c.-l., 409, 554\ntsay, r. s., 293\ntsiatis, a., 218\ntufte, e. r., 49\ntukey, j. w., 48, 49, 197, 391,\n\n409\n\nvan dyck, d., 218\nvan lieshout, m. n. m., 293\nvanderpump, m. p. j., 258\nvere-jones, d., 293\nvolinsky, c. t., 638\nvon mises, r., 174\n\n715\n\nwalden, a. t., 293\nwand, m. p., 348, 555\nwarburg, h. e., 200\nwasserman, l., 638\nwatson, g. s., 522\nwedderburn, r. w. m., 554,\n\n555\n\nweibull, w., 50\nweisberg, s., 409\nwelsch, r. e., 409\nwermuth, n., 292, 464\nwetherill, g. b., 409\nwhittaker, j., 292\nwild, p., 93\nwilliam of ockham, 150\nwolfson, l. j., 638\nwolpert, r. l., 638\nwood, s. n., 555\nwoods, h., 355\n\nyates, f., 463\nyork, j., 638\nyule, g. u., 257\n\nzeger, s. l., 505, 555,\n\n692\n\nzelen, m., 541\nzucchini, w., 292, 409\n\n "}, {"Page_number": 728, "text": "example index\n\n2 \u00d7 2 table, 666\n22 factorial experiment, 439, 442\n23 factorial experiment, 441\n3 \u00d7 2 layout, 384\n\nabo blood group system, 137, 475\nadaptive rejection sampling, 82\narma process, 270\nautoregressive process, 267\naverage, 30, 52\n\nbeaver body temperature data, 266, 268\nbelief network, 251\nbernoulli distribution, 30, 104, 566, 568\nbernoulli probability, 576\nbernoulli trials, 570\nbeta distribution, 172\nbinary matched pairs, 683\nbinomial distribution, 6, 30, 56, 58, 110,\n\n169, 180, 345, 481\n\nbirth data, 17, 19, 20, 23, 25, 26, 41, 54,\n\n60, 83, 135, 177\n\nbirth process, 288\nbivariate normal density, 608\nblalock\u2013taussig shunt data, 192, 198\nblood data, 450\nblood group data, 175\nbox\u2013cox transformation, 389\nboxplot, 20\nbranching process, 324\nbreast cancer data, 226, 230, 241\n\ncake data, 453\ncalcium data, 469, 478, 678\ncapture-recapture model, 106\ncardiac surgery data, 579, 621\ncat heart data, 446\ncauchy distribution, 33, 96, 120\ncement data, 354, 379, 381, 399, 593\ncensoring, 112\n\n716\n\nchallenger data, 6, 97, 100, 122, 130,\n\n603\n\nchi-squared distribution, 45\nchick bone data, 432\nchimpanzee learning data, 485\ncloth fault data, 514\ncovariance and correlation, 32\ncycling data, 356, 362, 372, 388, 395,\n\n444\n\ndanish fire data, 277, 285, 328\ndiagnostic test, 567\ndichotomization, 488\ndirac comb, 310\ndirectional data, 172\ndiscrimination, 631\ndna data, 225, 230, 234, 236\n\nempirical distribution function, 19, 30\nepidemiology, 8\nexponential and log-normal, 148\nexponential distribution, 39, 78, 95, 105,\n108, 144, 145, 168, 192, 314, 326,\n344\n\nexponential family, 312, 336, 340, 573\nexponential sample, 53\nexponential transformation, 34\nexponential trend, 276\neye data, 505\n\nfield concrete mixer data, 434, 445\nfive-state markov chain, 232\nforensic evidence, 584\nfrailty, 202\nftse data, 266, 271, 273\n\ngeneralized linear model, 541\ngeneralized pareto distribution, 688\ngenetic pedigree, 249\ngrouped data, 368\n\nhalf-normal distribution, 79\nhistogram, 19\nhuber estimator, 321\nhuman lifetime data, 194\nhus data, 142, 177, 583\n\nimage, 245\nising model, 248\n\njacamar data, 470, 483, 502\njapanese earthquake data, 288, 518, 525\njeffreys\u2013lindley paradox, 586\n\nlaplace distribution, 22, 24\nleukaemia data, 541, 545\nlinear exponential family, 682, 689\nlinear model, 317\nlink function, 482\nlocation model, 183\nlocation-scale model, 61, 185, 187, 576,\n\n588\n\nlog-linear model, 498, 502, 503\nlog-logistic distribution, 190\nlog-normal mean, 303, 304\nlog-normal mean, 645\nlogistic distribution, 202, 316\nlogistic regression, 6, 108, 490, 498, 505,\n\n665, 676\n\nlongitudinal data, 457\nlung cancer data, 8, 503\n\ngalaxy data, 213\ngamma distribution, 23, 35, 53, 57, 181,\n\nmagnesium data, 208\nmaize data, 1, 67, 68, 74, 309, 329, 332,\n\n190, 339, 674\n\n365, 372, 381\n\ngeneralized additive model, 541\ngeneralized gamma distribution, 132\n\nmarkov chain, 245, 247\nmaths marks data, 256, 259, 261, 263\n\n "}, {"Page_number": 729, "text": "example index\n\n717\n\nmeasuring machines, 569\nmixture distribution, 213\nmodel selection, 153\nmoment estimators, 316\nmotorette data, 615\nmouse data, 200, 546\nmoving average process, 269\nmultinomial distribution, 47, 175, 475\nmultiplicative model, 359\nmultivariate normal distribution, 72, 73\n\nnegative binomial distribution, 211, 512\nneyman\u2013scott problem, 646\nnodal involvement data, 490, 676, 684\nnon-additivity, 390\nnon-linear model, 503, 678\nnormal deviance, 472\nnormal distribution, 45, 78, 80, 88, 111,\n116, 121, 129, 178, 180, 280, 312,\n481, 574, 580, 613, 627, 633\n\nnormal hierarchical model, 620\nnormal linear model, 474, 589, 649, 656,\n\n681\n\nnormal mean, 333, 337\nnormal median, 41\nnormal mixture distribution, 145\nnormal nonlinear model, 474\nnormal variance, 301\nnuclear plant data, 401, 404, 664\n\npartial spline model, 537\npbc data, 549\npermutation group, 184\npermutation test, 341\npig diet data, 431\npigeon data, 172\npneumoconiosis data, 508\npoisons data, 391, 436, 440\npoisson birth process, 98, 108, 146\npoisson distribution, 23, 46, 59, 94, 153,\n\n170, 177, 311, 313, 340, 481, 670\n\npoisson mean, 310\npoisson process, 112, 287\npolynomial regression, 354\npositron emission tomography, 216\npremier league data, 498\nprobability weighted moment estimators,\n\n317\n\npublication bias, 208\npump failure data, 600\n\nrandom effects model, 610\nrandom sample, 106\nrat growth data, 459\nratio, 34\nregression model, 648\nrenewal process, 287\nrestricted likelihood, 690\nrounding, 113\n\none-way layout, 459\norder statistics, 16\northogonal polynomials, 383\noverdispersion, 512\n\npareto distribution, 41\npartial likelihood, 656\n\nsample moments, 15, 24\nsample shape, 18\nsample variance, 31\nscatterplot, 20\nshakespeare\u2019s vocabulary data, 629\nshoe data, 421\nsign test, 331, 332, 334\n\nsimulated data, 376\nsimulation study, 403\nsmoking and the grim reaper, 258,\n\n494\n\nspring barley data, 533, 538, 622\nspring failure data, 4, 95, 96, 100, 120,\n\n127, 132, 154\n\nstraight-line regression, 186, 322, 354,\n\n361, 394\n\nstudent t distribution, 140, 653\nstudent t statistic, 84\nstudent t test, 330, 332, 341, 342\nstudentized statistic, 32\nsurveying a triangle, 361\nsurvival data, 376\n\nteaching methods data, 427\ntoxoplasmosis data, 515, 527, 628\ntrimmed average, 86\ntwo-sample model, 341, 365\ntwo-state markov chain, 231, 240\ntwo-way contingency table, 135\n\nulcer data, 495, 666\nuniform distribution, 38, 103, 167, 170,\n\n180, 304, 312, 647, 669\n\nvariance function, 171\nvenice sea level data, 161, 164, 165, 186,\n\n205, 475, 477\n\nvon mises distribution, 172\n\nweibull distribution, 96, 117, 127, 130,\n\n189, 319\n\nweighted least squares, 514\nwilcoxon signed-rank test, 331, 332\n\nyarmouth sea level data, 281\n\n "}, {"Page_number": 730, "text": "index\n\n2 \u00d7 2 table, 135, 137, 492\u2013496, 546,\n\n557, 666, 697\n\nbayesian analysis, 642\nsmall-sample analysis, 494\n\nc p, 404, 408, 413\nf distribution, 65\u201368, 76, 140, 486\nf statistic, 367, 378\nn (\u00b5, \u03c3 2) distribution, see normal\n\ndistribution\n\nformula, 651, 665, 674\n\no and o notation, 35\n\u2217\np\nt distribution, see student t distribution\n\u03b1-particle data, 696\n\u03c7 2 distribution, see chi-squared\n\ndistribution\n\np-value , see significance level\n\nabo blood group system, 137, 475\naccelerated life model, 541\u2013553\nacceptance-rejection algorithm, see\n\nrejection algorithm\n\nadaptive rejection sampling, 82, 93,\n\n626\n\nadded variable, 414\nadjusted dependent variable, 474\nadmissibility, 633\nage-specific failure rate, see hazard\n\nfunction\n\naic, 152, 235, 236, 308, 404, 407, 408,\n\n413\n\ncorrected, 403, 404, 524\n\nair-conditioning failure data, 696\nakaike information criterion, see aic\n\n152\n\nalofi rainfall data, 697\nanalysis of covariance, 446, 448\nanalysis of deviance, see deviance,\n\nanalysis of\n\nanalysis of variance, 378\u2013386\n\nlatin square, 434\none-way layout, 426\nrandom effects, 451\n\n718\n\nsplit-unit experiment, 455\ntwo-way layout, 431, 437, 465\n\nancillary statistic, 646\u2013656, 693, 698\nanderson\u2013darling statistic, 328\narch process, 272\narima process, 271\narma process, 270, 297, 697\nasymptotic relative efficiency, 51\nautocorrelation function, 267\nautoregressive moving average process,\n\nsee arma process\n\nautoregressive process, 109, 267, 274,\n\n618\n\naverage, 15, 16, 24, 30, 41, 52, 54, 66,\n\n67, 74, 75\n\ntrimmed, 16, 86\n\nbackfitting, 536, 623\nbackshift operator, 270\nbackward recurrence time, 298\nbalanced incomplete block design, 432\nbandwidth, 307, 308, 520\nbarndorff-nielsen\u2019s formula, see p\n\n\u2217\n\nformula\n\nbartlett adjustment, 340\nbasu\u2019s theorem, 590, 649\nbayes factor, 582\u2013587, 593, 595, 596,\n\n640\n\napproximate, 598\n\nbayes information criterion, see bic\nbayes risk, 632\nbayes rule, 631\nbayes\u2019 theorem, 11, 565\u2013568\nbayesian model averaging, 638\nbeaver body temperature data, 266, 268,\n\n698\n\nbeetle data, 697\nbelief network, 251\nbernoulli distribution, 31, 89, 566, 568\n\nbayesian analysis, 594, 637, 639\ninformation, 115\n\njeffreys prior, 575, 576\nsufficient statistic, 104\nbernoulli trials, 570, 577\nbest linear unbiased predictor (blup),\n\n458, 463, 464, 467\n\nbeta distribution, 89, 172, 219, 566\nbeta function, 168\nbeta-binomial distribution, 518, 636, 698\nbi-weibull distribution, 189, 190\nbias, 300\nbic, 152, 404, 599, 641\nbinary data, 487\u2013492, 517, 554, 684, 698\n\ncomplete separation, 489\nconditional inference, 694\nconditional likelihood, 665, 683\ndeviance, 497, 559\ndichotomization, 488\nmodel checking, 490\n\nbinomial distribution, 8, 61\n\nbayesian analysis, 636\nconfidence interval, 56, 58, 62, 345\nconjugate prior, 579\ncumulants, 49\nestimation, 314\nexponential family, 169, 180\ninformation, 110\northogonal parameter, 691\npoisson approximation, 49\nrelation to bernoulli distribution, 30\nsufficient statistic, 315\ntest, 91\nvariance function, 481\n\nbiological control data, 294\nbirth data, 17, 19, 20, 23, 25, 26, 41, 54,\n\n59, 60, 76, 83, 135, 177, 696\n\nbirth order data, 158\nbirth process, 288\nbiweight, 375, 376\nblalock\u2013taussig shunt data, 192, 198\nblocking, 419\nblood data, 450, 610\nblood group data, 175, 697\nboiling point data, 697\n\n "}, {"Page_number": 731, "text": "index\n\n719\n\nbootstrap, 376\nbox\u2013cox transformation, 389, 391, 455,\n\ncholesky decomposition, 89, 623\nclassical inference, see repeated\n\n551\n\nbox\u2013tidwell transformation, 412\nboxplot, 20\nbranching process, 324\nbreakdown point, 17, 27\nbreast cancer data, 226, 230, 241\nbrownian bridge, 696\nbrownian motion, 92\nbrush and spin plot, 696\nburt twins data, 697\n\ncake data, 453\ncalcium data, 469, 478, 678\ncalibration, 415\ncapture-recapture model, 106, 149\ncardiac surgery data, 579, 621\ncase diagnostics, see model checking\ncat heart data, 446\ncauchy distribution, 33, 48, 693, 698\n\ninformation, 120\nlikelihood, 96, 100, 101, 127\nsimulation, 81, 91\n\ncauchy\u2013schwarz inequality, 36\ncausal inference, 423, 464\ncement data, 354, 379, 381, 385, 399,\n\n408, 593, 697\n\ncensoring, 4, 190, 217, 641\n\ndiscrete data, 193\ninformation, 112\nleft, 191\nrandom, 190\nright, 5, 112, 191\ntype i, 190, 220\ntype ii, 190, 220\n\ncentral limit theorem, 30, 696\nchallenger data, 6, 97, 100, 122, 130,\n\n139, 603, 697\n\nchangepoint, 142, 583, 698\ncharacteristic function, 44, 48\ncherry tree data, 697\nchi-squared distribution, 63\u201364, 67, 76,\n\n139, 219, 558\n\ncumulants, 45\nnoncentral, 51\nsimulation, 78\n\nchi-squared statistic, 133\nchick bone data, 432, 697\nchimpanzee learning data, 485\n\nsampling\n\nclique, 245, 254, 255\ncloth data, 698\ncloth fault data, 514\ncoal-mining disaster data, 698\ncoefficient of variation, 51\ncollinearity, 398, 697\ncompeting risks, 198\u2013201, 218, 221\ncompleteness, 311, 315\n\nbounded, 311, 340\n\ncomponents of variance, 449\u2013464\ncomputer bug data, 299, 643\ncondition number, 398\nconditional inference, 143, 177\nconditional predictive ordinate, 589\nconditionality principle, 569, 639\nconfidence interval, 54\nequi-tailed, 56, 343\ninterpretation, 58\nmaximum likelihood estimate, 120\nnormal linear model, 371\none-sided, 56\nstudent t, 67, 90, 92\ntwo-sample, 74\n\nconfidence limit, 343\nconservative, 345\n\nconfiguration, 186, 187, 650, 655\nconfounding, 420, 442, 448, 466\nconjugate density, 573\nconsistency, 29\n\nstrong, 123\n\nconstructed variable, 391, 413, 487\ncontingency table, 135, 500\u2013507\ncontinuation ratio model, 510\ncontinuity correction, 671\ncontrast, 443, 445, 465\ncontrol variate, 85\nconvergence, 28\u201337\n\ndiagnostics, 607, 638\nin distribution, 30, 31\nin probability, 28, 36\n\ncook statistic, 362, 394, 396\n\napproximate, 477\n\ncorrelation, 32, 36, 69, 90, 347\n\npartial, 261, 264\n\ncorrelogram, 267, 297\n\npartial, 267\n\ncount data, 498\u2013511\n\ncounterfactual, 424\ncounting process, 552\ncovariance, 32, 36, 68\n\nmatrix, 68\npartial, 261\n\ncovariate, see explanatory variable\ncoverage error, 345\ncram\u00b4er\u2013rao lower bound, 302, 319, 325,\n\n377\n\nmultivariate, 304\n\ncram\u00b4er\u2013von mises statistic, 328\ncredible set, 579, 594, 640, 641\n\nhighest posterior density (hpd), 579\n\ncritical region, 333\n\ninvariant, 342\nsimilar, 339, 665\nunbiased, 337\nuniformly most powerful, 336\n\ncross-validation, 308, 314, 395, 399,\n\n408, 524, 533, 537\n\ngeneralized, 399, 524, 525, 533, 537\ncumulant-generating function, 44\u201348,\n\n167, 487, 671\n\ncut, 182, 501\ncycling data, 356, 362, 372, 388, 395,\n\n444, 466\n\ndaily rainfall data, 293\ndanish fire data, 277, 285, 328, 688\ndata augmentation, 638, 698\nde finetti\u2019s theorem, 619\ndecision rule, 631\n\nminimax, 633, 637\n\ndecision theory, 631\u2013636, 638\ndefective distribution, 189\ndelta method, 33\u201335, 59, 122\n\nseveral variables, 34\n\ndependent data, 323\u2013324\ndesign matrix, 354\ndetailed balance, 231, 238, 613\ndeviance, 471, 483, 556, 559\n\nanalysis of, 484, 486\nnormal, 472\noverdispersed, 515\npenalized likelihood, 537\nscaled, 471, 483\ndiagnostic test, 567\ndifferencing, 271, 274\ndirac comb, 310, 315\ndirac delta function, 12\n\n "}, {"Page_number": 732, "text": "720\n\ndirected deviance statistic, see signed\n\nlikelihood ratio statistic\n\ndirectional data, 172\ndirichlet distribution, 181\ndiscrimination, 631, 637\ndispersion parameter, 480, 487\ndistribution constant, 184, 647, 649\ndna data, 225, 230, 234, 236, 292\ndouble exponential distribution, see\n\nlaplace distribution\n\ndummy variable, 356\n\nedgeworth series, 671, 672\nefficiency, 111\n\nasymptotic relative, 303\n\neigendecomposition, 230, 237, 238\nem algorithm, 210\u2013218, 223, 296, 297,\n\n463, 563, 638, 697\n\nempirical bayes, 627\u2013638\nempirical distribution function (edf),\n\n19, 30, 277, 278\n\nempirical logistic transform, 36, 490,\n\n509, 559\nendpoint, 146\nenvelope simulation, see rejection\n\nalgorithm\n\nequivalence relation, 107\nequivariant estimator, 185\nergodic average, 230, 608\nergodic model, 323\nerror\n\ntype i, 333\ntype ii, 333\n\nestimate, 23\nestimating equation, 316, 512\n\ngeneralized, 507\n\nestimating function, 315\u2013325, 555\n\noptimal, 318\n\nestimation, 300\u2013315\n\nefficient, 303\nnon-regular, 304\nunbiased, 300\n\nestimator, 23\nevolutionary distance data, 295\nexchangeability, 619, 626\nexpectation space, 169\nexpected information, 109\u2013115, 124,\n\n138, 144, 166, 179, 575\n\ncomparison with observed, 120\ntransformation, 156\n\nexpert system, 293\nexplanatory variable, 4, 161\nexponential distribution, 79, 119, 350,\n\n680\n\nbayesian analysis, 639, 641\ncensored, 112, 220\nconditional inference, 693\nconfidence interval, 314\nestimation, 697\nexponential family, 168\nfailure time, 6\ngrouped, 159\nhazard, 188, 192\nlack-of-memory property, 39\nlikelihood, 95, 125, 127\nmixture, 149\nnested in weibull, 96, 130\norder statistics, 39\northogonal parameter, 691, 695\nprobability plot, 26\nshifted, 145, 149, 350\nsimulation, 78, 89, 91\nsufficient statistic, 105, 108\ntest, 326, 344\ntruncated, 698\n\nexponential family, 166\u2013183, 215, 218,\n\n336, 340, 350, 493, 636, 680\n\n( p, q), 174\ncomplementary mean parameter, 689\ncompleteness, 312\nconditional density, 180\nconditional inference, 674\nconjugate prior, 573, 577, 578, 639\ncurved, 174, 182, 677\ninference, 176\nlikelihood, 179\nlinear, 490\nmarginal density, 180\nminimal representation, 172\nnatural, 167, 172\norder p, 171, 176, 573\norder 1, 167, 168\nregular, 167\nsteep, 170, 220\n\nexponential scores, 26, 40\nexponential tilting, 167, 168\neye data, 505\n\nfactor, 356\n\ncrossed, 452\n\nfactorial experiment, 356, 391, 436, 439,\n\n441, 442, 444, 448, 697\n\nreplicated, 436\n\nfactorization criterion, 104, 410, 566\nfield concrete mixer data, 434, 445, 448\n\nindex\n\nfinancial data, 33\nfir seedling data, 640\nfirst-passage time, 229, 243\nfisher information, see expected\n\ninformation\n\nfisher scoring, 118\nfitted value, 361, 362\nforce of mortality, see hazard function\nforensic evidence, 584\nforward recurrence time, 298\nfrailty, 201\u2013202, 218, 221, 555, 563\n\nshared, 562\n\nfrequentist inference, see repeated\n\nsampling\n\nftse data, 266, 271, 273, 697\nfull conditional density, 245, 605\nfunnel plot, 209\n\ngalaxy data, 213\ngamma distribution, 23, 64, 487\n\ncumulants, 48\nestimation, 57, 696\nexponential family, 181, 182, 219\ngeneralized, 132\nhazard, 190\ninformation, 115\ninverse, 182\nprobability plot, 26\nsimulation, 89\nsmall-sample inference, 674\ntest, 339\n\ngamma function, 23, 617\n\nproperties, 27\n\ngarch process, 273\ngauss\u2013markov theorem, 374\ngaussian distribution, see normal\n\ndistribution\n\ngeneralized additive model, 538, 541,\n\n555, 623, 698\n\ngeneralized extreme-value distribution,\n\n50, 279, 291\n\ngeneralized linear model, 480\u2013487, 518,\n\n541, 554\u2013556, 558\n\ngeneralized pareto distribution, 284, 286,\n\n291, 292, 299, 317, 688, 697\n\ngenetic linkage data, 223\ngenetic pedigree, 249\ngeometric distribution, 229\n\nbayesian analysis, 639, 640\nexponential family, 219\ninformation, 115\n\n "}, {"Page_number": 733, "text": "index\n\nlikelihood, 101\nrelation to negative binomial, 50\nsimulation, 89\n\ngibbs sampler, 605\u2013612, 618, 621, 638,\n\n642, 643, 698\n\ngoodness of fit, 131\u2013138, 177, 327\n\nposterior predictive, 592\n\ngraeco-latin square, 466\ngraph\n\nancestral subset, 255\ndirected acyclic, 249\u2013253, 255\nmoral, 250, 251, 255, 262, 265,\n\n296\n\ngraphical design, 21, 28\ngraphical model, 260\u2013266, 292\ngreenwood\u2019s formula, 197\ngroup, 183\ngroup action, 183\ngroup transformation model, 183\u2013188,\n\n218, 329\n\ncomposite, 187\n\nnull, 325\nsimple, 326\n\nhypothesis test, 325\u2013348, 582\n\ncomparison, 333\ninvariant, 342\nnonparametric, 331\none-sided, 329, 337, 350\nrandomized, 336, 347\nrelation to confidence interval,\n\n343\u2013346\n\nsimilar, 339\ntwo-sided, 329, 337\n\nh\u00a8older\u2019s inequality, 182\n\nignorable non-response, 204\nimage analysis, 245, 292\nimaginary observations, 596\nimportance sampling, 87, 618, 641\nbayesian application, 602\u2013605\nratio estimator, 603\nraw estimator, 87\nweight, 87\n\ngrouped data, 368, 414\ngumbel distribution, 203, 279, 297, 413,\n\nincidence matrix, 533\ninference function, see estimating\n\n475\n\nhalf-normal distribution, 79, 696\nhalf-normal plot, 444\nhammersley\u2013clifford theorem, 246,\n\n253, 255, 292, 296, 605\n\nhat matrix, 362, 369, 385, 413\nhazard function, 188, 203, 275, 286\n\nbathtub, 190\ncause-specific, 198, 221\ncumulative, 189\nestimation, 220\nhead size data, 697\nheaviside function, 12\nhermite polynomial, 672\nhierarchical model, 464, 638\n\nbayesian, 619\u2013627\npoisson, 600, 698\n\nhistogram, 19, 305, 349\nhotelling\u2019s t 2 statistic, 260\nhuber estimator, 321, 325, 350, 375, 376\nhuman lifetime data, 194\nhus data, 142, 177, 583, 697, 698\nhypergeometric distribution, 495, 557\nhyperparameter, 573\nhypothesis\n\nalternative, 326\ncomposite, 326, 339\u2013343\n\nfunction\n\ninfinitesimal generator, 238\ninfluence, 394, 477, 539\ninfluence function, 321\ninformation\n\nexpected, 222\n\ninformation distance, see\n\nkullback\u2013leibler discrepancy\n\ninformation sandwich, 147, 151,\n\n377\n\nintensity function\ncomplete, 286\nconditional, 288\n\ninteraction, 424, 436, 439, 466\n\nfirst-order, 440\n\ninterest-preserving reparametrization,\n\n645\n\ninterquartile range (iqr), 17, 20, 37, 43,\n\n61\n\ninterval estimation, 313\u2013314\ninvariant, 184\n\nmaximal, 184, 186, 329, 343\n\ninverse gamma distribution, 580, 588,\n\n640\n\ninverse gaussian distribution, 182, 487,\n\n680\n\ninverse probability, 637\ninversion algorithm, 78\u201379, 89\n\n721\n\niqr, see interquartile range\nising model, 248\niterated expectation, 65\n\njacamar data, 470, 483, 502\njapanese earthquake data, 288, 518,\n\n525\n\njeffreys prior, 639\njeffreys\u2013lindley paradox, 586\njensen\u2019s inequality, 123\n\nkaplan\u2013meier estimator, see\nproduct-limit estimator\n\nkernel, 306, 520\neffective, 521\ntricube, 520\n\nkernel density estimation, 697\nkernel density estimator, 305\u2013309, 314,\n\n608\n\nkolmogorov\u2013smirnov statistic, 328\nkronecker delta, 12\nkullback\u2013leibler discrepancy, 123, 147,\n\n150\n\nkurtosis, 46\n\nlake constance data, 697\nlaplace approximation, 596\u2013602, 617,\n\n636, 638, 641\n\nposterior density, 599\nposterior distribution, 599\n\nlaplace distribution, 22, 24, 28, 85, 125,\n\n157, 698\n\nregression model, 377\nlaplace transform, 312\nlarge deviation region, 652\nlatin square, 434, 438, 446, 466, 697\nlaw of small numbers, see poisson\n\napproximation to binomial\ndistribution\n\nleast squares estimation, 163, 359\u2013369,\n\n697\n\ngeneralized, 369\ngeometrical interpretation, 362, 369,\n\n378\n\niterative generalized (igls), 463\niterative weighted, 472\u2013476, 479,\n\n554\u2013556\npenalized, 560\nrestricted iterative generalized\n\n(rigls), 659\n\nrobustness, 376\nweighted, 368, 369, 409, 514\n\n "}, {"Page_number": 734, "text": "722\n\nleast trimmed squares estimation, 376\nleukaemia data, 541, 545, 697, 698\nleverage, 362, 393, 394, 476, 539\nlexis diagram, 191, 218, 561\nlikelihood, 94\n\nbasic properties, 99\u2013100\ncomplete-data, 210, 215\nconditional, 557, 646, 665, 677, 683,\n\n694\n\ndependent data, 98\nexponential family, 179\ninterpretation, 100\u2013101\nlocal, 527, 540\nlog, 99\nmarginal, 646, 656, 665\nmodified, 458\nmodified profile, 680\u2013691, 694\nnon-regular properties, 140\u2013148, 697\nobserved-data, 210\npartial, 544, 545, 554, 561, 656, 665\npenalized, 156, 531, 535, 555\nprofile, 117, 140, 544, 680, 694\npure, 101\nquadratic summary, 109, 125\nrelative, 99, 102, 109, 119\nreparametrization, 99, 116\nrestricted, 458, 657, 690\nsummary, 101\n\nlikelihood equation, 116\nlikelihood principle, 568\u2013571, 589, 638,\n\n639\n\ninverse, 482, 486\nlog, 482, 486\nlog-log, 488, 497\nlogit, 8, 484, 488, 490, 497\nprobit, 488, 558\n\nlizards data, 697\nlocal characteristic, see full conditional\n\ndensity\n\nlocal polynomial estimation, 519\u2013530,\n\n539, 555\n\nbias and variance, 522, 529\ndegrees of freedom, 521\nsmoothing parameter, 523\n\nlocation, 15, 27, 61\nlocation model, 183, 649, 655\nlocation-scale model, 61, 157, 185,\n\n187\n\nbayesian analysis, 639\nconditional inference, 694\ngoodness of fit, 588\njeffreys prior, 576\northogonal parameter, 695\n\nlog odds, 169\nlog rank test, 545\nlog-concave density, 81\nlog-linear model, 498\u2013500, 554, 556,\n\n559, 697\n\nlog-logistic distribution, 190\nlog-normal distribution, 37, 91, 190, 303,\n\n304\n\nlikelihood ratio statistic, 126\u2013139, 330,\n\nprobability plot, 26\n\n340, 366\n\ngeneralized, 128\nlarge-sample distribution, 126, 138\nsigned, see signed likelihood ratio\n\nstatistic\n\nlinear congruential generator, 77, 696\nlinear exponential family\n\nconditional inference, 683\nmodified profile likelihood, 682\northogonal parameter, 689\n\nlinear mixed model, 456, 463, 657\nlinear model, 353\u2013417, 479, 554, 661\n\nbayesian analysis, 641\nnormal, 359, 370\u2013374\nterms, 380\n\nlinear predictor, 480\nlinear process, 269\nlink function, 480\n\nbinary data, 488, 497\ncanonical, 482, 487\ncomplementary log-log, 488, 497\nidentity, 482\n\nlogistic distribution, 156, 202, 316\nlogistic regression, 97, 100, 122, 130,\n\n490\u2013492, 498, 505, 509, 515, 636,\n665, 676, 683, 684, 697, 698\n\nsufficient statistic, 108\n\nlongitudinal data, 456, 457, 555, 660\nlook-up method, 78\nloss function, 631\nlowess, 521\nlung cancer data, 8, 503, 693\n\nm-estimator, 375, 376\nmad, see median absolute deviation\nmagical mystery formula, 651\nmagnesium data, 208\nmain effect, 440\nmaize data, 1, 67, 68, 74, 129, 130, 140,\n309, 329, 332, 365, 372, 381, 386,\n410, 580\n\nmanaus river height data, 697\n\nindex\n\nmantel\u2013haenszel test, 558\nmarginal model, 505, 554, 559\nmarkov chain, 225\u2013245, 292, 606,\n\n697\n\nclassification of states, 229\ncontinuous-time, 237\nfirst-order, 234, 247, 293, 660\ngeometrically ergodic, 231\ninhomogeneous, 242\nreversible, 231, 238\nsecond-order, 236, 254, 293\nsimulation, 244\nstationary, 228\ntwo-state, 231, 240\nvariable-order, 235\nzeroth-order, 234\n\nmarkov chain monte carlo, 605\u2013617,\n\n638, 642, 666\n\noutput analysis, 607\n\nmarkov process, 98, 267, 268, 273\n\ncontinuous-time, 294, 295\n\nmarkov property, 98, 228, 244\n\nglobal, 251, 262, 296\nlocal, 251, 253, 254, 296\npairwise, 296\n\nmarkov random field, 244\u2013255, 292,\n\n293, 622, 626\n\nmartingale, 323, 552\nmasking, 388\nmatched pairs, 372\nmaths marks data, 256, 259, 261, 263\nmax stability, 291\nmaximum likelihood estimator, 102,\n\n115\u2013126, 210, 324, 346\n\ncomputation, 115\nconditional, 677\nconsistent, 122\nlarge-sample distribution, 118, 124\nusual regularity conditions, 118\n\nmean, 22\nmean excess life function, 203\nmean parameter, 169\nmean residual life plot, 285\nmean squared error, 300\u2013305\n\nintegrated, 308\n\nmeasuring machines, 569\nmedian, 331\nmedian absolute deviation (mad), 17,\n\n28, 43\n\nmeta-analysis, 206, 223\nmethod of moments, see moment\n\nestimator\n\n "}, {"Page_number": 735, "text": "index\n\nmetropolis\u2013hastings algorithm,\n\n612\u2013617, 626, 638, 642, 667,\n698\n\nrandom walk, 613, 618, 643\nmichaelis\u2013menton model, 695\nmidrange, 50\nmillet plant data, 697\nminimal representation, 172, 182\nminimax decision rule, 633\nminimum variance unbiased estimation,\n\n309\u2013313\n\nmissing at random (mar), 204, 205,\n\n222\n\nmissing completely at random (mcar),\n\n204, 205\n\nmissing data, 203\u2013218\nmissing information principle, 211\nmixture distribution, 213, 219, 639\nmode, 27\nmodel\n\naveraging, 407, 592\nnested, 127, 131, 133, 139, 586\nnon-linear, 503\nparametric, 22\nsaturated, 471\nselection, 150\u2013155\nuncertainty, 83, 406\nwrong, 147, 377\n\nmodel building\n\nlinear model, 397\u2013408\nmodel checking, 476\u2013479\n\nbayesian, 587\u2013592\nlinear model, 386\u2013397\n\nmoderate deviation region, 652\nmoment estimator, 316, 636\nmoment-generating function, 37, 44,\n\n481, 487\n\nmoments, 44\u201348\nmonte carlo integration, 87\nmotorette data, 615, 698\nmouse data, 200, 546\nmoving average process, 269, 274, 297\nmultilevel model, 461, 464\nmultimodal distribution, 642\nmultinomial distribution, 37, 139, 475,\n\n654\n\ncumulants, 47, 51\nestimation, 697\nexponential family, 175, 220\nfit, 133\n\nmultiplicative model, 359\n\nmultivariate t distribution, 297\nmultivariate normal distribution, 138,\n\n247, 255\u2013267\n\nnadaraya\u2013watson estimator, 522, 530,\n\n540\n\nnatural cubic spline, 530, 533, 535,\n\n560\n\nnatural observation, 168\nnatural parameter, 168\nnegative binomial distribution, 630\n\nexponential family, 219\ngenesis, 50\nlikelihood, 517\northogonal parameter, 691\n\nneighbourhood system, 244\u2013246,\n\n248\n\nnested variation, 450\nnetwork information criterion, 152\nneurological data, 697\nnewton\u2013raphson algorithm, 116, 211,\n\n213, 215, 217, 473\n\nneyman\u2013pearson lemma, 335, 632\nnodal involvement data, 490, 676,\n\n684\n\nnon-additivity test, 390, 391, 486\nnon-ignorable non-response (nin), 204,\n\n205\n\n723\n\nsample median, 41\nsampling, 613, 642\nsimulation, 78, 80, 91, 696\nstandard, 63\nsufficient statistic, 109, 125\ntest, 139, 140, 159, 333, 337\ntrivariate, 72, 73\nunbiased estimation, 301, 312\n\nnormal equations, 360\nnormal hierarchical model, 620\nnormal linear model, 474, 479, 593,\n\n681\n\nbayesian analysis, 589, 595\n\nnormal nonlinear model, 474, 479\nnormal scores plot, 26, 387\nnotation, 12\nnuclear plant data, 401, 404, 664\nnull distribution, 326\n\nobservational study, 10, 648\nobserved at random (oar), 217\nobserved information, 102, 109\u2013115,\n\n138, 144, 166, 179\n\ntransformation, 156\n\nockham\u2019s razor, 150, 378\noffset, 498\none-way layout, 426, 438, 449, 459, 465,\n\n467\n\nnonlinear model, 678, 695\nnonlinearity, 389\nnormal distribution, 62\u201363, 159, 347,\n\nopinion polling, 56, 58\norbit, 183\norder statistic, 37\u201344, 106, 186, 190,\n\n481, 646\n\nbayesian analysis, 580, 640\nbivariate, 70, 71, 90, 207, 608\nconfidence interval, 121\nconjugate prior, 574\ncumulants, 45, 70\nempirical bayes analysis, 627\nexponential family, 178\nextremes, 280\ngoodness of fit, 588\ninformation, 111\njeffreys prior, 577\nlikelihood, 116, 129, 180\nlinear combination, 45, 72\u201373, 90, 92,\n\n164, 165\n\nmixture, 85, 145, 149, 644, 697\nmodified profile likelihood, 690\nmultivariate, 68\u201377, 89, 90, 220,\n\n259\u2013266, 697\n\northogonal parameter, 689\nrisk, 633\nrounding, 114, 616\n\n220, 296, 350\n\nextreme, 41\nsummary, 16\n\norder statistics, 276, 279\nordinal response, 507\u2013510, 555\northogonal polynomials, 383, 445,\n\n697\n\noutlier, 17, 149, 320, 388, 697\noverdispersion, 177, 511\u2013518, 527,\n\n698\n\npaired comparison, 3, 140, 419, 421,\n\n425\n\npanel data, 225\nparameter, 3, 22\n\nidentifiable, 144\ninterest, 127, 645\nnuisance, 127, 645\northogonal, 487, 685\u2013690\nredundant, 144, 149\nspace, 94, 140\n\n "}, {"Page_number": 736, "text": "724\n\nparametrization, 23\ncorner-point, 440\n\npareto distribution, 41, 348, 594\npartial likelihood, 561\npartial spline model, 537\npbc data, 549, 698\npea data, 160\npearson\u2019s statistic, 135, 140, 160, 177,\n234, 237, 483, 485, 497, 517, 696\n\npeople data, 696\npermutation group, 184\npermutation test, 341, 352\npig diet data, 431, 438\npigeon data, 172\npivot, 53, 61, 67, 313, 343\n\napproximate, 56, 74\nbasis of test, 60\nexact, 139\nproperties, 56\nstudent t, 66\ntwo-sample, 74\n\nplotting position, 26\npneumoconiosis data, 508\npoint process, 274\u2013293\n\nclustering, 277\nlength-biased sampling, 298\nmarked, 288\norderly, 275, 286\nself-exciting, 289\nspatial, 293\nthinning, 291\n\npoisons data, 391, 436, 440, 464\npoisson approximation to binomial\n\ndistribution, 49, 282\n\npoisson birth process, 98, 108, 146\npoisson dispersion test, 177\npoisson distribution, 9, 23, 49, 142, 511\nbayesian analysis, 626, 637, 640, 644\ncomplete, 311\nconditional inference, 694\nconfidence interval, 61, 160, 697\nconjugate prior, 573, 577\ncumulants, 46\nestimation, 696\nexponential family, 170, 177, 340, 481\ngoodness of fit, 135\nlikelihood, 94\nmarginal inference, 665, 693\nmixture, 697\northogonal parameter, 690\nsaddlepoint approximation, 670\nsufficient statistic, 109\n\nindex\n\ntruncated, 37, 696\nunbiased estimation, 310, 313, 315\nvariance stabilization, 59\n\nprobability weighted moment estimators,\n\n317\n\nproduct moment correlation coefficient,\n\npoisson process, 40, 274\u2013287, 293, 486,\n\nsee correlation\n\n498, 562, 693, 697\n\nbayesian analysis, 596, 643\nempirical bayes analysis, 629\nestimation, 696\nhomogeneous, 277, 285\ninformation, 112\ninhomogeneous, 283, 299, 557, 643,\n\n697, 698\nintensity, 275\nsimulation, 298\npollution data, 697\npolynomial regression, 354\npositive stable distribution, 563\npositivity condition, 246, 253, 254\npositron emission tomography, 216\nposterior density, 566\n\nmarginal, 578\nnormal approximation, 578\n\nposterior predictive density, 568, 577,\n\n602, 617\n\nnormal linear model, 591\npoisson distribution, 577\n\npotential, 246\npower, 333\nlocal, 338\n\nprediction, 60\u201361, 150, 568, 592\nprediction decomposition, 98\nprediction interval, 60, 165\n\nnormal linear model, 371, 372\n\npremier league data, 498\nprincipal components, 397\nprinciple of insufficient reason, 577, 637\nprinciple of parsimony, see ockham\u2019s\n\nrazor\n\nprior density, 566, 572\u2013577, 638\n\nconjugate, 567, 573, 640\nelicitation, 638\nignorance, 574\nimproper, 574, 580, 640\njeffreys, 575\nnon-informative, 574\n\nprobability integral transform, 39\nprobability plot, 26, 28, 131, 203\n\nexponential, 159, 277, 278, 286, 696\ngumbel, 297\nhalf-normal, 696\nnormal, 49, 63, 92, 165, 179, 696\nweibull, 50\n\nproduct-limit estimator, 196\u2013198\nprofile likelihood, 127\u2013131, 479\nproportion data, 487\u2013498\nproportional hazards model, 543\u2013555,\n\n562, 563, 656, 698\n\nproportional odds model, 508\nprospective study, 493\npseudo-random numbers, 77\u201378\npublication bias, 206\u2013210\npump failure data, 160, 600, 644, 698\n\nquantile, 22\nquantile-quantile (q-q) plot, 26, 28\nquartile, 16\nquasi-likelihood, 512\u2013517, 555, 698\nquasi-random numbers, see\n\npseudo-random numbers\n\nrandom effects model, 449, 456, 458, 610\nrandom sample, 21\u201324\n\nnormal, 66\u201368\n\nrandomization, 417\u2013426\n\ndistribution, 422\n\nrandomized block design, 429\nrank statistic, 561\nrao\u2013blackwell theorem, 309\nrare events, see statistics of extremes\nrat growth data, 459\nratio, 34\nratio of uniforms algorithm, 81, 91\nregression-scale model, 661\u2013665, 681,\n\n691\n\nrejection algorithm, 79\u201382, 89\n\nadaptive, 81\n\nrelevant subset, 647\nrenewal process, 287\nrepeated measures, 456\nrepeated sampling, 52, 58, 119\nreplication, 464\nresidual\n\nbinary response, 492\ncox\u2013snell, 541, 548\ndeletion, 362, 395, 590, 591\ndeviance, 477, 517, 548, 551\nmartingale, 548, 551, 552\n\n "}, {"Page_number": 737, "text": "index\n\npearson, 517\nproperties, 386\nraw, 165, 179, 362, 370, 387, 554, 588\nserial correlation, 387\nstandardized, 362, 387, 396, 414, 479,\n\n590, 649\n\nstandardized deviance, 477, 479, 485,\n\n556\n\nstandardized pearson, 477, 479, 556\ntime series, 269, 274\n\nresidual sum of squares, 163, 361, 371\nresistant statistic, 17\nresponse, 161\nrestricted maximum likelihood\n\nestimation (reml), 458, 461, 464,\n467, 659, 665\n\nretrospective study, 493\nreturn level, 280, 688\nreturn period, 280\nreversibility, 607\nridge regression, 398, 697\nrisk function, 632\nrisk set, 193, 543\nrobustness, 319\u2013322\nroughness penalty, 216, 530\u2013535\nrounding, 113, 115, 145\nrug, 19\nrun, 229, 243\nr\u00b4enyi representation, 40\n\nsaddlepoint approximation, 560, 638,\n\n668\u2013673, 680, 698\n\ndouble, 670, 675\n\nsaddlepoint equation, 669\nsalinity data, 697\nsample, 15\n\naverage, 51\nmaximum, 16, 50, 279, 291\nmean, see average\nmedian, 16, 20, 37, 41, 51, 61, 157,\n\n324\n\nminimum, 16, 41, 42, 50, 279\nmoment, 15, 24, 75\nquantile, 16\nrange, 50\nshape, 18, 28\nskewness, 28\nspace, 94\nspace derivative, 652, 682\nvariance, 15, 25, 28, 31, 50, 66\u201368, 74,\n\n75\n\nsampling variation, 24\u201325\n\n725\n\nsampling-importance resampling (sir),\n\nspring failure data, 4, 95, 96, 100, 119,\n\n618\n\n120, 127, 130, 132, 154\n\nsandwich covariance matrix, 507, 513\nscale, 15, 27, 61\n\nstandard error, 52\nstationarity, 267, 607, 641\n\nchoice, 58\n\nscale model, 347, 639\nscatterplot, 4, 20\n\nmatrix, 256\n\nsecond-order, 267\nstrict, 267\nstatistic, 15\nstatistical formulae\n\nscore statistic, 116, 138, 144, 149, 315,\n\nmindless repetition, 88\n\n338, 346\n\nscore test, 132, 338\nseed, 78\nseed germination data, 698\nselection bias, 210, 218\nself-consistency, 223\nsemiparametric regression, 518\u2013540, 555\nshakespeare\u2019s vocabulary data, 629\nshoe data, 421\nshort time series, 659\nshrinkage, 459, 621, 625, 628, 634\nsign test, 331, 332, 334\nsigned likelihood ratio statistic, 128, 346\n\nmodified, 653, 663, 676\nsignificance level, 325, 582\n\nconflict with likelihood principle, 570\ninterpretation, 326\nmid- p, 495, 671\n\nsignificance trace, 525\nsimple random sample, see random\n\nsample\n\nsimpson\u2019s paradox, 256\u2013258\nsimulation, 77\u201390\nsize, 333\nskew-normal distribution, 91, 160\nskewness, 18, 46\nslash distribution, 85\nslutsky\u2019s lemma, 31\u201333\nsmoking and the grim reaper, 258,\n\n494\n\nsmoother, 518\n\nlinear, 532, 539\n\nsmoothing matrix, 521, 537\nspacing, 43, 277\nspectral decomposition, 73, 397\nspeed limit data, 697\nspeed of light data, 696\nspline, 555\nsplit-unit experiment, 452\nspring barley data, 533, 538, 622, 626\n\nstatistical genetics, 292\nstatistics of extremes, 278\u2013286, 293\nstein effect, 635\nstem-and-leaf display, 28\nstirling\u2019s formula, 617\nstochastic matrix, 229\nstraight-line regression, 115, 159,\n\n161\u2013166, 186, 219, 220, 317, 322,\n353, 354, 361, 394, 410, 412, 413,\n697\n\nstudent t distribution, 64\u201365, 74, 76, 85,\n\n140, 187, 651\nlinear model, 374\nsimulation, 78\n\nstudent t statistic, 67, 129, 139, 140,\n\n164, 368, 696\n\ncoverage, 696\nregression, 379\nrobustness, 84\n\nstudent t test, 330, 332, 341, 342\nstudentized statistic, 32\nsufficiency principle, 569, 639\nsufficient partition, 107, 109\nsufficient statistic, 103\u2013108, 176\n\nminimal, 107, 166, 410, 566\nsum of squares, 163, 360, 380\n\northogonal, 382\npenalized, 532\n\nsurrogate variable, 564\nsurvival data, 188\u2013203, 218, 376,\n\n540\u2013554\n\nsurvivor function, 203\nswan of avon, 629\nsymbolic rank deficiency, 149\n\nteaching methods data, 427\nteak plant data, 697\ntest, 60\n\nmonte carlo exact, 668, 680\nunimodality, 697\n\ntest statistic, 325\nthreshold stability, 291\n\n "}, {"Page_number": 738, "text": "726\n\ntime series, 266\u2013274, 293\ntime-dependent covariate, 547,\n\n562\n\ntitanic data, 697\ntolerance distribution, 488, 508\ntolerance interval, see prediction interval\n\n60\n\ntoxoplasmosis data, 515, 527, 636\n\nempirical bayes analysis, 628\n\ntransformation, 58, 122, 697\n\nexponential, 34\ninterest-preserving, 129\nsymmetrizing, 558\nvariance-stabilizing, 59\n\ntransition matrix, 229\ntransition probability, 228\ntrinomial distribution, 508\ntwo-sample model, 3, 73\u201375, 140, 341,\n\n365, 372, 419, 425\n\nbayesian analysis, 595\n\ntwo-way layout, 429, 464, 485,\n\n538\n\nulcer data, 495, 668, 697\nunemployment rate, 61\n\nuniform distribution, 669\nbayesian analysis, 594\nexponential family, 180\nexponential tilting, 167, 170\nlikelihood, 103, 109, 149\nnot complete, 312\norder statistic, 43\norder statistics, 38, 50, 51\nsimulation, 77\nunbiased estimation, 304, 315\n\nunit-treatment additivity, 421, 424\nurine data, 698\n\nvariability band, 525\nvariable selection, 400\n\nc p, 403, 412\nbackward elimination, 400, 408, 412\nforward selection, 400, 408, 412\nlikelihood criteria, 402\nstepwise, 400, 408, 412, 697\n\nindex\n\nvenice sea level data, 161, 164, 165, 186,\n\n205, 465, 475, 477\n\nvolatility, 272\nvon mises distribution, 172, 174\n\nweak law of large numbers, 28,\n\n152\n\nweibull distribution, 50, 553\n\nbayesian analysis, 615\nestimation, 697\nhazard, 189\ninformation, 157\nlikelihood, 96, 100, 117, 125, 127,\n\n130, 154\n\nmoment estimation, 319\n\nweight of evidence, 583\nwhite noise process, 267\nwiener process, 696\nwilcoxon signed-rank test, 331, 332,\n\nvariance function, 59, 170, 182, 481, 512\n\n351\n\nlinear, 171, 511\nquadratic, 511, 517\n\nvariance reduction, 85\u201389\nvariance-stabilizing transformation, 170\nvariance-time curve, 288, 298\n\nwilcoxon two-sample test, 351\nwishart distribution, 260\n\nyahoo share price data, 92\nyarmouth sea level data, 281\n\n "}]}
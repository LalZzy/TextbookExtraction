{"Filename": "cateregression", "Pages": [{"Page_number": 1, "text": " "}, {"Page_number": 2, "text": " "}, {"Page_number": 3, "text": "regression for categorical data\n\nthis book introduces basic and advanced concepts of modern categori-\ncal regression with a focus on the structuring constituents of regression.\nmeant for statisticians, applied researchers, and students, it includes many\ntopics not normally included in books on categorical data analysis, includ-\ning recent developments in flexible and high-dimensional regression.\n\nin addition to standard methods such as logit and probit models and\ntheir extensions to multivariate settings, the book presents more recent\ndevelopments in regularized regression with a focus on the selection of\npredictors; tools for flexible nonparametric regression that yield fits that\nare closer to the data; advanced models for count data; nonstandard tree-\nbased ensemble methods; and tools for the handling of both nominal\nand ordered categorical predictors. issues of prediction are explicitly\nconsidered in a chapter that introduces standard and newer classification\ntechniques.\n\nsoftware including an r package that contains datasets and code for\nmost of the examples is available from http://www.stat.uni-muenchen\n.de/\u223ctutz/catdata.\n\ndr. gerhard tutz is a professor of statistics in the department of statistics\nat ludwig-maximilians university, munich. he was formerly a professor\nat the technical university berlin. he is the author or co-author of nine\nbooks and more than 100 papers.\n\n "}, {"Page_number": 4, "text": "cambridge series in statistical and probabilistic mathematics\n\neditorial board\n\nz. ghahramani (department of engineering, university of cambridge)\nr. gill (mathematical institute, leiden university)\nf. p. kelly (department of pure mathematics and mathematical statistics, university\nof cambridge)\nb. d. ripley (department of statistics, university of oxford)\ns. ross (department of industrial and systems engineering, university of southern\ncalifornia)\nm. stein (department of statistics, university of chicago)\n\nthis series of high-quality upper-division textbooks and expository monographs covers\nall aspects of stochastic applicable mathematics. the topics range from pure and applied\nstatistics to probability theory, operations research, optimization, and mathematical pro-\ngramming. the books contain clear presentations of new developments in the field and\nalso of the state of the art in classical methods. while emphasizing rigorous treatment of\ntheoretical methods, the books also contain applications and discussions of new techniques\nmade possible by advances in computational practice.\n\na complete list of books in the series can be found at http://www.cambridge.org/uk/\n\nseries/sseries.asp?code=cspm.\n\nrecent titles include the following:\n\n8. a user\u2019s guide to measure theoretic probability, by david pollard\n9. the estimation and tracking of frequency, by b. g. quinn and e. j. hannan\n10. data analysis and graphics using r, by john maindonald and john braun\n11. statistical models, by a. c. davison\n12. semiparametric regression, by david ruppert, m. p. wand, and r. j. carroll\n13. exercises in probability, by loic chaumont and marc yor\n14. statistical analysis of stochastic processes in time, by j. k. lindsey\n15. measure theory and filtering, by lakhdar aggoun and robert elliott\n16. essentials of statistical inference, by g. a. young and r. l. smith\n17. elements of distribution theory, by thomas a. severini\n18. statistical mechanics of disordered systems, by anton bovier\n19. the coordinate-free approach to linear models, by michael j. wichura\n20. random graph dynamics, by rick durrett\n21. networks, by peter whittle\n22. saddlepoint approximations with applications, by ronald w. butler\n23. applied asymptotics, by a. r. brazzale, a. c. davison, and n. reid\n24. random networks for communication, by massimo franceschetti and ronald meester\n25. design of comparative experiments, by r. a. bailey\n26. symmetry studies, by marlos a. g. viana\n27. model selection and model averaging, by gerda claeskens and nils lid hjort\n28. bayesian nonparametrics, edited by nils lid hjort et al.\n29. from finite sample to asymptotic methods in statistics, by pranab k. sen, julio m.\n\nsinger, and antonio c. pedrosa de lima\n\n30. brownian motion, by peter m\u00a8orters and yuval peres\n31. probability, by rick durrett\n\n "}, {"Page_number": 5, "text": "regression for categorical data\n\ngerhard tutz\nludwig-maximilians universit\u00a8at\n\n "}, {"Page_number": 6, "text": "cambridge university press\ncambridge, new york, melbourne, madrid, cape town,\nsingapore, s\u02dcao paulo, delhi, tokyo, mexico city\n\ncambridge university press\n32 avenue of the americas, new york, ny 10013-2473, usa\n\nwww.cambridge.org\ninformation on this title: www.cambridge.org/9781107009653\nc(cid:3) gerhard tutz 2012\n\nthis publication is in copyright. subject to statutory exception\nand to the provisions of relevant collective licensing agreements,\nno reproduction of any part may take place without the written\npermission of cambridge university press.\n\nfirst published 2012\n\nprinted in the united states of america\n\na catalog record for this publication is available from the british library.\n\nlibrary of congress cataloging in publication data\n\ntutz, gerhard.\nregression for categorical data / gerhard tutz.\n\np.\n\ncm. \u2013 (cambridge series in statistical and probabilistic mathematics)\n\nisbn 978-1-107-00965-3 (hardback)\n1. regression analysis.\nqa278.2.t88 2011\n519.5\n\n2011000390\n\n36\u2013dc22\n\n(cid:4)\n\n2. categories (mathematics)\n\ni. title.\n\nii. series.\n\nisbn 978-1-107-00965-3 hardback\nadditional resources for this publication at http://www.stat.uni-muenchen.de/\u223ctutz/catdata.\n\ncambridge university press has no responsibility for the persistence or accuracy of urls for\nexternal or third-party internet web sites referred to in this publication and does not\nguarantee that any content on such web sites is, or will remain, accurate or appropriate.\n\n "}, {"Page_number": 7, "text": "contents\n\npreface\n\n1\n\nintroduction\n1.1 categorical data: examples and basic concepts .\n1.2 organization of this book .\n1.3 basic components of structured regression . . . . . . .\n1.4 classical linear regression . .\n.\n1.5 exercises\n\n. . . . . .\n. . . .\n\n. .\n. . . .\n\n. . . .\n\n. . . .\n\n. . . .\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . .\n\n. . . . . . . . . .\n. . . . . . . . . . . . .\n\n. . . .\n\n. . .\n. .\n. . .\n.\n. .\n\n2 binary regression: the logit model\n\n2.1 distribution models for binary responses and basic concepts . . . . . . . . .\n. . .\n2.2 linking response and explanatory variables\n. .\n2.3 the logit model\n. . . . .\n.\n2.4 the origins of the logistic function and the logit model . . . . .\n2.5 exercises\n. .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . .\n\n. .\n. . . . .\n\n. . . . . . . . .\n\n. . . . . .\n\n. . . .\n\n. . . .\n\n. . . .\n\n. .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n3 generalized linear models\n.\n\n.\n\n. . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n.\n.\n\n. .\n\n. .\n\n. . . .\n\n. . . . .\n\n. . . . . .\n\n3.1 basic structure .\n3.2 generalized linear models for continuous responses . . . . . . .\n.\n3.3 glms for discrete responses\n. .\n.\n3.4 further concepts\n.\n.\n. . . . .\n3.5 modeling of grouped data . .\n.\n. .\n3.6 maximum likelihood estimation .\n. . .\n. . . .\n3.7\n.\n. . . .\n3.8 goodness-of-fit for grouped observations\n. . .\n3.9 computation of maximum likelihood estimates\n3.10 hat matrix for generalized linear models . . . .\n3.11 quasi-likelihood modeling . . .\n.\n3.12 further reading . . .\n3.13 exercises\n.\n.\n\n. . . .\n. . . .\n. . . . . . . . .\n\n. .\n. . . . .\n. . .\n\n. . . . . . . . . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\ninference\n\n. . . . .\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . . . .\n. . . . . . . . .\n. . . . . . . . .\n\n. . . .\n\n. . . . . .\n. . . .\n\n. . . .\n.\n.\n. .\n.\n. . . .\n. .\n.\n. . .\n. . .\n. .\n.\n. .\n. . . . . . .\n\n. . . . .\n\n. . . .\n\n. . . . . . . . . . .\n\n. . . . . . . . . . .\n\n. . . . . . . . . .\n\nix\n\n1\n1\n5\n6\n15\n27\n\n29\n29\n33\n37\n48\n49\n\n51\n51\n53\n56\n60\n62\n63\n67\n72\n75\n76\n78\n79\n79\n\n4 modeling of binary data\n\n.\n\n4.1 maximum likelihood estimation .\n4.2 discrepancy between data and fit\n.\n4.3 diagnostic checks\n.\n4.4 structuring the linear predictor\n.\n4.5 comparing non-nested models\n.\n4.6 explanatory value of covariates .\n.\n4.7 further reading . . .\n4.8 exercises\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . .\n\n.\n. . .\n.\n.\n.\n.\n.\n.\n\n. . . . .\n. . .\n. . .\n. . .\n. . . . .\n. . . .\n\n. . . . . .\n\n. . . . . . . . . .\n\n. . . .\n\n. . . . . . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . . . . . . . .\n\n. . . . . .\n. . . . . .\n. . . . . .\n\n. . . . . . . . . .\n. . . . . . . . . .\n. . . . . . . . . .\n\n. . . . .\n\n. . . . . . . . . . .\n. . . . . . . . . . . . .\n\n. . . .\n\n. . . .\n. . .\n. .\n\n81\n82\n87\n93\n. . . . 101\n. . . . 113\n. . . . 114\n. . 119\n. . 120\n\nv\n\n "}, {"Page_number": 8, "text": "vi\n\ncontents\n\n5 alternative binary regression models\n\n5.1 alternative links in binary regression .\n.\n5.2 the missing link .\n.\n5.3 overdispersion .\n.\n5.4 conditional likelihood .\n.\n5.5 further reading . . .\n5.6 exercises\n.\n.\n\n. . . .\n. . . .\n. .\n.\n. . . .\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . .\n\n. . . . . . . . . . . . .\n\n. . . .\n. . . .\n. . . . . .\n\n. . . . . . . . . . . . .\n. . . . . . . . . . . . .\n. . . . . . . . . .\n\n. . . .\n\n. . . . . .\n\n. . . . . . . . . .\n\n. . . .\n. . . . . . . . . . . . .\n\n. . . .\n\n123\n. . . . 123\n. . 130\n. . 132\n. 138\n. . 140\n. . 140\n\n6 regularization and variable selection for parametric models\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n. . .\n\n. . . . . . . . .\n. . . . . . . . .\n\n6.1 classical subset selection . .\n.\n6.2 regularization by penalization .\n6.3 boosting methods . .\n.\n6.4 simultaneous selection of link function and predictors . .\n6.5 categorical predictors .\n.\n6.6 bayesian approach .\n.\n6.7 further reading . . .\n6.8 exercises\n.\n.\n\n. . . . . . .\n. . .\n. . .\n.\n\n. . . . . . . . . .\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . .\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n. . . . .\n. . . . .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . .\n\n143\n. . . . . . . . 144\n. . . . . . . . 145\n. . . . . . 163\n. . . . . . . 170\n. . . . . . . . . . . . 173\n. . . . . . 178\n. . . . . . 179\n. . . . . . . . . 179\n\n. . . .\n. . . .\n\n. . . .\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n7 regression analysis of count data\n.\n.\n. .\n\ninference for the poisson regression model\n\n7.1 the poisson distribution .\n7.2 poisson regression model\n7.3\n7.4 poisson regression with an offset\n7.5 poisson regression with overdispersion .\n7.6 negative binomial model and alternatives . . . . .\n7.7 zero-inflated counts\n7.8 hurdle models\n. . .\n7.9 further reading . . .\n7.10 exercises\n.\n\n. . .\n. . .\n. . .\n.\n\n. . .\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . .\n\n. . . . . .\n\n. . . . . . . . . .\n. . . . . . . . . .\n. . . . . . . . . .\n\n181\n. 182\n. . . . . . . . 185\n. . . . . 186\n. 190\n. . . . . . . 192\n. . . . . . . . . 194\n. . . . . . 198\n. . . . . . 200\n. . . . . . 203\n. 204\n\n. . . .\n. . . .\n. . . .\n\n. . . . . . . .\n. . . . . . . . .\n\n. . . . .\n\n. .\n\n. . . . . . . . . . .\n\n. . . . .\n\n. . . . . .\n\n. . . . . . . .\n\n. .\n\n. . . . . . . . . . .\n\n. . . . . . . .\n\n. .\n\n. . . . . . . . . . .\n\n8 multinomial response models\n\n. . . . .\n\n. . . . . . . . . . . . . .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n. . .\n\n. . . . . . . . .\n\n. . . . .\n.\n\n. .\n. . . . . . .\n\ninference for multicategorical response models .\n\n8.1 the multinomial distribution .\n8.2 the multinomial logit model\n8.3 multinomial model as random utility model\n8.4 structuring the predictor\n8.5 logit model as multivariate generalized linear model\n8.6\n8.7 multinomial models with hierarchically structured response\n8.8 discrete choice models .\n. . . .\n8.9 nested logit model .\n.\n8.10 regularization for the multinomial model\n8.11 further reading . . .\n.\n8.12 exercises\n\n. . . . . . . . .\n. . . . . . . .\n\n. . . . . . . .\n. . . . . . . .\n\n. .\n. . . . .\n\n. . . . .\n. . . .\n\n. . . .\n. .\n\n. . . . . .\n\n. . . . .\n\n. . . .\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . . .\n\n. . . .\n. . . . . .\n\n207\n. 209\n. . . . . . . . 210\n. . . . . 215\n. . . . . . . . . . . . 215\n. . . . . . . 217\n. . . . . 218\n. . . . . . . .\n. 223\n. . . . . . . . . 226\n. . . . . . . . . . 231\n. . . . . . . . . 233\n. . . . . . . . . . 238\n. 239\n\n. . . . . . . . . . .\n\n9 ordinal response models\n9.1 cumulative models .\n9.2 sequential models\n.\n9.3 further properties and comparison of models . .\n9.4 alternative models .\n9.5\n\n.\ninference for ordinal models .\n\n. . . . .\n. . . . . .\n\n. . . . .\n. . . . .\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . .\n. . . . .\n\n. . . . . . . . . . .\n. . . . . . . . . . .\n\n. . . .\n\n. . . . . . . . .\n\n. . . . .\n\n. . . . . . . . . . .\n\n. . . . . .\n\n. . . . . . . . . .\n\n241\n. . 243\n. . 252\n. . . 255\n. . 257\n. 261\n\n "}, {"Page_number": 9, "text": "contents\n\n9.6 further reading . . .\n9.7 exercises\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n. . . . .\n. . . .\n\n. . . . .\n\n. . . . . . . . . . .\n. . . . . . . . . . . . .\n\n. . . .\n\nvii\n\n. . 265\n. . 265\n\n10 semi- and non-parametric generalized regression\n\n10.1 univariate generalized non-parametric regression . . .\n. . . . .\n10.2 non-parametric regression with multiple covariates . . . . . . .\n10.3 structured additive regression .\n10.4 functional data and signal regression .\n10.5 further reading . . .\n.\n10.6 exercises\n\n. . . . .\n. .\n\n. . . . . . . . .\n\n. . . . . . .\n\n. . . . .\n\n. . . .\n\n. . . .\n\n. . . .\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . . .\n. . . . . .\n\n269\n. 269\n. 285\n. . . . . . . . . . . . 289\n. . . 307\n. . 313\n. . . . 314\n\n. . . . .\n\n. . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n11 tree-based methods\n\n11.1 regression and classification trees\n11.2 multivariate adaptive regression splines\n11.3 further reading . . .\n11.4 exercises\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . .\n. .\n\n. . . .\n\n. . . . . . . . . . . . .\n\n. . . .\n\n. . . . . . . . .\n\n. . . . .\n. . . . . . . . . . .\n\n. . . . .\n\n. . . . . . . . . . . . .\n\n317\n. . . . . . . . . 317\n. . 328\n. . 329\n. . . . 329\n\n12 the analysis of contingency tables: log-linear and graphical models\n. . . . . . . . . .\n\n. . . . . .\n\n. .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. .\n\n. . . . . . . . .\n\n. . . . . . . . .\n\n12.1 types of contingency tables .\n12.2 log-linear models for two-way tables . . . .\n12.3 log-linear models for three-way tables . . . .\n12.4 specific log-linear models\n12.5 log-linear and graphical models for higher dimensions . . . . .\n12.6 collapsibility .\n12.7 log-linear models and the logit model . . . .\n12.8 inference for log-linear models .\n12.9 model selection and regularization . .\n12.10 mosaic plots .\n.\n12.11 further reading . .\n12.12 exercises .\n.\n\n. . .\n. . . . .\n. . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . .\n\n. . . . . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . .\n\n. . . . .\n\n. . . .\n\n. . . .\n\n. . .\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . .\n\n. . . .\n\n. . . . .\n\n. . . .\n. . . . . .\n\n331\n. 332\n. . . 335\n. . 338\n. 341\n. 345\n. . 348\n. . . 349\n. . . . 350\n. . . 354\n. . . . . . . 357\n. . 358\n. . . . . . . 359\n\n. . . . .\n\n. . . . .\n\n. . . . . . . . . . . .\n\n. . . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n13 multivariate response models\n.\n\n363\n13.1 conditional modeling .\n. . 365\n13.2 marginal parametrization and generalized log-linear models . .\n. . . . . . . 370\n13.3 general marginal models: association as nuisance and gees . . . . . . . . . 371\n. 385\n13.4 marginal homogeneity .\n.\n13.5 further reading . . .\n. . 392\n. . . . 393\n13.6 exercises\n.\n.\n\n. .\n. . . . .\n. .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . .\n\n. . . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . .\n\n. . . .\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n14 random effects models and finite mixtures\n\n. . . .\n\n14.1 linear random effects models for gaussian data . . . .\n14.2 generalized linear mixed models . . .\n14.3 estimation methods for generalized mixed models . . .\n14.4 multicategorical response models . . .\n14.5 the marginalized random effects model\n14.6 latent trait models and conditional ml . . . .\n14.7 semiparametric mixed models .\n.\n14.8 finite mixture models\n.\n14.9 further reading . . .\n14.10 exercises .\n.\n.\n\n. .\n. .\n. . . . .\n. . .\n\n. . . . . .\n. . . . . .\n\n. . . . .\n\n. . . .\n\n. . . .\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . .\n\n. . . . . . . . .\n. . . . .\n\n. . . . .\n\n. . . . . . . . .\n\n. . . . .\n\n. . . . . . . . .\n\n. . . . . . . . .\n\n. . . . .\n\n. . . . . . . . . .\n. . . . . . . . . .\n\n. . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . .\n\n395\n. . . 396\n. . . 402\n. 407\n. . . 416\n. . 419\n. . . 420\n. 420\n. 422\n. . 426\n. . . . . . . 427\n\n. . . .\n. . . .\n\n. . . . .\n\n "}, {"Page_number": 10, "text": "viii\n\n15 prediction and classification\n\n.\n\n.\n\n. . . . . .\n\n15.1 basic concepts of prediction . .\n15.2 methods for optimal classification . . . . . . . .\n15.3 basics of estimated classification rules . . .\n15.4 parametric classification methods . . . . . . . .\n.\n15.5 non-parametric methods . . . .\n.\n.\n15.6 neural networks\n15.7 examples .\n.\n.\n.\n15.8 variable selection in classification .\n.\n15.9 prediction of ordinal outcomes\n.\n15.10 model-based prediction . . .\n.\n.\n15.11 further reading . .\n15.12 exercises .\n.\n.\n.\n\n. . . . . .\n. . . . . . . .\n. . .\n. .\n.\n. . . . . .\n. . . . .\n. . . . . . . .\n\n. . . .\n\n. . . .\n\n.\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . . .\n\n. . . . . .\n\n. . . . . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . .\n\n. . . . . .\n\n. . . . .\n\n. . . . . . . . . . .\n. . . . . . .\n\n. . . . . .\n\ncontents\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . .\n\n429\n. 430\n. . . . . . . . . . 438\n. 445\n. . . . . . . . . . 451\n. 457\n. . . . . . . . . 468\n. . . . . . . 471\n. . . . 473\n. 474\n. 480\n. . 481\n. . 482\n\n. . . . . . . .\n. . . . . . . . . .\n\n. . . . . . .\n\na distributions\n\na.1 discrete distributions .\n.\na.2 continuous distributions . . .\n\n.\n\n.\n\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n. . . .\n. .\n\n. . . .\n. . . . . .\n\n. . . . . . . . . . . . .\n. . . . . . . . . .\n\n. . . .\n\n485\n. . 485\n. 487\n\nb some basic tools\n\nb.1 linear algebra . . .\n.\nb.2 taylor approximation .\nb.3 conditional expectation, distribution .\nb.4 em algorithm .\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . . .\n\n. . .\n. . . . . . .\n\n. . . .\n\n. . . .\n\n. . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . .\n\n490\n. . . . . . 490\n. . . . . . . . . . . . 491\n. . . 493\n. . . . . . . . . 494\n\n. . . . .\n\nc constrained estimation\n\nc.1 simplification of penalties\nc.2 linear constraints . .\n.\nc.3 fisher scoring with penalty term . . . . . . .\n\n. . .\n.\n.\n\n.\n. . .\n\n.\n.\n\n.\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . .\n\n. . . . .\n\n. . . . . . . . . .\n\n. . . .\n\n. . . . . . . . .\n\n496\n. . . . . . . . 496\n. . . . . . 498\n. . . 499\n\n. . . . .\n\nd kullback-leibler distance and information-based criteria of model fit\n\nd.1 kullback-leibler distance . .\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . .\n\n. . . . .\n\n500\n. . . . . . . . 500\n\ne numerical integration and tools for random effects modeling\n\ne.1 laplace approximation .\n.\ne.2 gauss-hermite integration .\ne.3 inversion of pseudo-fisher matrix . . .\n\n. . . . . . .\n. . . . . . .\n. . . . .\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n\n504\n. . . . . . . . .\n. . . 504\n. . . . . . . . . . . . 505\n. . . . . . . . . . 507\n\n. . . .\n. . . .\n. . . . . .\n\nlist of examples\n\nbibliography\n\nauthor index\n\nsubject index\n\n509\n\n513\n\n545\n\n554\n\n "}, {"Page_number": 11, "text": "preface\n\nthe focus of this book is on applied structured regression modeling for categorical data. there-\nfore, it is concerned with the traditional problems of regression analysis: finding a parsimonious\nbut adequate model for the relationship between response and explanatory variables, quantify-\ning the relationship, selecting the influential variables, and predicting the response given ex-\nplanatory variables.\n\nthe objective of the book is to introduce basic and advanced concepts of categorical regres-\nsions with the focus on the structuring constituents of regressions. the term \"categorical\" is\nunderstood in a wider sense, including also count data. unlike other texts on categorical data\nanalysis, a classical analysis of contingency tables in terms of association analysis is considered\nonly briefly. for most contingency tables that will be considered as examples, one or more of\nthe involved variables will be treated as the response. with the focus on regression modeling,\nthe generalized linear model is used as a unifying framework whenever possible. in particular,\nparametric models are treated within this framework.\n\nin addition to standard methods like the logit and probit models and their extensions to\nmultivariate settings, more recent developments in flexible and high-dimensional regressions\nare included. flexible or non-parametric regressions allow the weakening of the assumptions\non the structuring of the predictor and yield fits that are closer to the data. high-dimensional\nregression has been driven by the advance of quantitative genetics with its thousands of mea-\nsurements. the challenge, for example in gene expression data, is in the dimensions of the\ndatasets. the data to be analyzed have the unusual feature that the number of variables is much\nhigher than the number of cases. flexible regression as well as high-dimensional regression\nproblems call for regularization methods. therefore, a major topic in this book is the use of\nregularization techniques to structure predictors.\n\nspecial topics that distinguish it from other texts on categorical data analysis include the\n\nfollowing:\n\n\u2022 non-parametric regressions that let the data determine the shape of the functional rela-\n\ntionships with weak assumptions on the underlying structure.\n\n\u2022 selection of predictors by regularized estimation procedures that allow one to apply cat-\n\negorical regression to higher dimensional modeling problems.\n\n\u2022 the focus on regression includes alternative models like the hurdle model and zero-\n\ninflated regression models for count data, which are beyond generalized linear models.\n\n\u2022 non-standard tree-based ensemble methods that provide excellent tools for prediction.\n\n\u2022 issues of prediction are explicitly considered in a chapter that introduces standard and\nnewer classification techniques, including the prediction of ordered categorical responses.\n\n\u2022 the handling of categorical predictors, nominal as well as ordered ones. regularization\nprovides tools to select predictors and to determine which categories should be collapsed.\n\nix\n\n "}, {"Page_number": 12, "text": "x\n\npreface\n\nthe present book is based on courses on the modeling of categorical data that i gave at tech-\nnical university berlin and my home university, ludwig-maximilians universit\u00e4t m\u00fcnchen.\nthe students came from different fields \u2013 statistics, computer science, economics, business,\nsociology \u2013 but most of them were statistics students. the book can be used as a text for such\ncourses that include students from interface disciplines. another audience that might find the\ntext helpful is applied researchers and working data analysts from fields where quantitative\nanalysis is indispensable, for example, biostatisticians, econometricians, and social scientists.\nthe book is written from the perspective of an applied statistician, and the focus is on basic\nconcepts and applications rather than formal mathematical theory. since categorical data anal-\nysis is such a wide field, not all approaches can be covered. for topics that are neglected, for\nexample, exact tests and correlation models, an excellent source is always alan agresti\u2019s book\ncategorical data analysis (wiley, 2002).\n\nmost of the basic methods for categorical data analysis are available in statistical packages\nlike sas and spss or the free package r (r development core team, 2010). software in-\ncluding an r package that contains most of the datasets and code for the examples is available\nfrom http://www.stat.uni-muenchen.de/~tutz/catdata. some references to r packages are given\nin the text, but code is available in the package only. when using the package one should be\nfamiliar with r. one of the many tutorials available at the r site might help. also, introductory\nbooks that explicitly treat the use of r with some applications to categorical data like everitt\nand hothorn (2006) and faraway (2006) could be helpful.\n\ni had much help with computational issues in the examples; thanks to jan gertheiss, se-\nbastian petry, felix heinzl, andreas groll, gunther schauberger, sarah maierhofer, wolfgang\np\u00f6\u00dfnecker, and lorenz uhlmann. i also want to thank barbara nishnik and johanna brandt for\ntheir skillful typing and elise oranges for all the corrections in grammar \u2013 thanks for all the\ncommas. it was a pleasure to work with lauren cowles from cambridge university press.\n\ngerhard tutz\n\n "}, {"Page_number": 13, "text": "chapter 1\n\nintroduction\n\ncategorical data play an important role in many statistical analyses. they appear whenever\nthe outcomes of one or more categorical variables are observed. a categorical variable can be\nseen as a variable for which the possible values form a set of categories, which can be finite\nor, in the case of count data, infinite. these categories can be records of answers (yes/no)\nin a questionnaire, diagnoses like normal/abnormal resulting from a medical examination, or\nchoices of brands in consumer behavior. data of this type are common in all sciences that\nuse quantitative research tools, for example, social sciences, economics, biology, genetics, and\nmedicine, but also engineering and agriculture.\n\nin some applications all of the observed variables are categorical and the resulting data\ncan be summarized in contingency tables that contain the counts for combinations of possible\noutcomes. in other applications categorical data are collected together with continuous vari-\nables and one may want to investigate the dependence of one or more categorical variables on\ncontinuous and/or categorical variables.\n\nthe focus of this book is on regression modeling for categorical data. this distinguishes\nbetween explanatory variables or predictors and dependent variables. the main objectives are\nto find a parsimonious model for the dependence, quantify the effects, and potentially predict\nthe outcome when explanatory variables are given. therefore, the basic problems are the same\nas for normally distributed response variables. however, due to the nature of categorical data,\nthe solutions differ. for example, it is highly advisable to use a transformation function to\nlink the linear or non-linear predictor to the mean response, to ensure that the mean is from\nan admissible range. whenever possible we will embed the modeling approaches into the\nframework of generalized linear models. generalized linear models serve as a background\nmodel for a major part of the text. they are considered separately in chapter 3.\n\nin the following we first give some examples to illustrate the regression approach to cate-\ngorical data analysis. then we give an overview on the content of this book, followed by an\noverview on the constituents of structured regression.\n\n1.1 categorical data: examples and basic concepts\n1.1.1 some examples\nthe mother of categorical data analysis is the (2 \u00d7 2)-contingency table.\nexample data may be given in that simple form.\n\nin the following\n\nexample 1.1: duration of unemployment\nthe contingency table in table 2.3 shows data from a study on the duration of employment. duration\n\n1\n\n "}, {"Page_number": 14, "text": "2\n\nchapter 1. introduction\n\nof unemployment is given in two categories, short-term unemployment (less than 6 months) and long-\nterm employment (more than 6 months). subjects are classified with respect to gender and duration of\nunemployment.\nit is quite natural to consider gender as the explanatory variable and duration as the\nresponse variable.\n\ntable 1.1: cross-classification of gender and duration of unemployment.\n\ngender\n\nmale\nfemale\n\nduration\n\n\u2264 6 months\n\n> 6 months\n\n167\n175\n\ntotal\n\n570\n413\n\n403\n238\n\na simple example with two influential variables, one continuous and the other categorical,\n\nis the following.\n\nexample 1.2: car in household\nin a sample of n = 6071 german households (german socio-economic household panel) various char-\nacteristics of households have been collected. here the response of interest is if a household has at least\none car (y = 1) or not (y = 01). covariates that may be considered influential are income of household\nin euros and type of household: (1) one person in household, (2) more than one person with children, (3)\nmore than one person without children). in figure 1.1 the relative frequencies for having a car are shown\nfor households within intervals of length 50. the picture shows that the link between the probability of\nowning a car and income is certainly non-linear.\n\nin many applications the response variable has more than two outcomes, for example, when\na customer has to choose between different brands or when the transport mode is chosen. in\nsome applications the response may take ordered response categories.\n\ni\n\ns\ne\nc\nn\ne\nu\nq\ne\nr\nf\n \ne\nv\ni\nt\na\ne\nr\n\nl\n\n0\n1\n\n.\n\n8\n.\n0\n\n6\n.\n0\n\n4\n.\n0\n\n2\n.\n0\n\n0\n.\n0\n\n**\n*\n*\n***\n*\n*\n*\n*\n*\n*****\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n****\n*\n*\n*\n*\n*\n\n*\n\n******\n*\n\n******************\n****\n*\n\n***\n\n*\n\n*\n\n*\n\n*\n*\n******\n*\n*\n**\n*****\n*\n*\n*\n*\n*\n*\n*\n***\n*\n*\n*\n*\n*\n*\n\n*\n*\n\n*\n\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n\n*\n\n*\n\n*\n\n*\n\n*\n\n*\n\n*\n\n*\n*\n*\n*\n**\n*\n*\n\n*\n*\n*\n*\n*\n*\n\n0\n\n1000\n\n2000\n\n3000\n\n4000\n\nnetincome\n\nfigure 1.1: car data, relative frequencies within intervals of length 50, plotted against\nnet income in euros.\n\n "}, {"Page_number": 15, "text": "1.1. categorical data: examples and basic concepts\n\n3\n\nexample 1.3: travel mode\ngreene (2003) investigated the choice of travel mode of n = 840 passengers in australia. the available\ntravel modes were air, train, bus, and car. econometricians want to know what determines the choice\nand study the influence of potential predictor variables as, for example, travel time in vehicle, cost, or\nhousehold income.\n\nexample 1.4: knee injuries\nin a clinical study focusing on the healing of sports-related injuries of the knee, n = 127 patients were\ntreated. by random design, one of two therapies was chosen. in the treatment group an anti-inflammatory\nspray was used, while in the placebo group a spray without active ingredients was used. after 3, 7, and 10\ndays of treatment with the spray, the mobility of the knee was investigated in a standardized experiment\nduring which the knee was actively moved by the patient. the pain y occurring during the movement was\nassessed on a five-point scale ranging from 1 for no pain to 5 for severe pain. in addition to treatment, the\ncovariate age was measured. a summary of the outcomes for the measurements after 10 days of treatment\nis given in table 1.2. the data were provided by kurt ulm (imse munich, germany).\n\ntable 1.2: cross-classification of pain and treatment for knee data.\n\nno pain\n\n1\n\n17\n19\n\n2\n\n8\n26\n\n3\n\n14\n11\n\n4\n\n20\n6\n\nsevere pain\n\n5\n\n4\n2\n\n63\n64\n\nplacebo\ntreatment\n\na specific form of categorical data occurs when the response is given in the form of counts,\n\nas in the following examples.\n\nexample 1.5: insolvent companies in berlin\nthe number of insolvent firms is an indicator of the economic climate; in particular, the dependence on\ntime is of special interest. table 1.3 shows the number of insolvent companies in berlin from 1994 to\n1996.\n\ntable 1.3: number of insolvent companies in berlin.\n\nmonth\n\njan.\n\nfeb. march april may\n\njune\n\njuly\n\naug.\n\nsep. oct.\n\nnov. dec.\n\n1994\n1995\n1996\n\n69\n80\n88\n\n70\n80\n123\n\n93\n108\n108\n\n55\n70\n92\n\n73\n81\n84\n\n68\n89\n89\n\n49\n80\n116\n\n97\n88\n97\n\n97\n93\n102\n\n67\n80\n108\n\n72\n78\n84\n\n77\n83\n73\n\nexample 1.6: number of children\nthere is ongoing research on the birthrates in western countries. by use of microdata one can try to find\nthe determinants that are responsible for the number of children a woman has during her lifetime. here\nwe will consider data from the german general social survey allbus, which contains data on all aspects\nof life in germany. interesting predictors, among others, are age, level, and duration of education.\n\n "}, {"Page_number": 16, "text": "4\n\nchapter 1. introduction\n\nin some applications the focus is not on the identification and interpretation of the depen-\ndence of a response variable on explanatory variables, but on prediction. for categorical re-\nsponses prediction is also known as classification or pattern recognition. one wants to allocate\na new observation into the class it stems from with high accuracy.\n\nexample 1.7: credit risk\nthe aim of credit scoring systems is to identify risk clients. based on a set of predictors, one wants to\ndistinguish between risk and non-risk clients. a sample of 1000 consumers credit scores collected at a\ngerman bank contains 20 predictors, among them duration of credit in months, amount of credit, and\npayment performance in previous credits. the dataset was published in fahrmeir and hamerle (1984),\nand it is also available from the uci machine learning repository.\n\n1.1.2 classification of variables\n\nthe examples illustrate that variables in categorical data analysis come in different types. in\nthe following some classifications of variables are given.\n\nscale levels: nominal and ordinal variables\n\nvariables for which the response categories are qualitative without ordering are called nominal.\nexamples are gender (male/female), choice of brand (brand a, . . . , brand k), color of hair, and\nnationality. when numbers 1, . . . , k are assigned to the categories, they have to be understood\nas mere labels. any one-to-one mapping will do. statistical analysis should not depend on the\nordering, or, more technically, it should be permutation invariant.\n\nfrequently the categories of a categorical variable are ordered. examples are severeness of\nsymptoms (none, mild, moderate, marked) and degree of agreement in questionnaires (strongly\ndisagree, mildly disagree,. . . ,strongly agree). variables of this type are measured on an ordinal\nscale level and are often simply called ordinal. with reference to the finite number of categories,\nthey are also called ordered categorical variables. statistical analysis may or may not use the\nordering. typically methods that use the ordering of categories allow for more parsimonious\nmodeling, and, since they are using more of the information content in the data, they should be\npreferred. it should be noted that for ordinal variables there is no distance between categories\navailable. therefore, when numbers 1, . . . , k are assigned to the categories, only the ordering\nof these labels may be used, but not the number itself, because it cannot be assumed that the\ndistances are equally spaced.\n\nvariables that are measured on metric scale levels (interval or ratio scale variables) repre-\nsent measurements for which distances are also meaningful. examples are duration (seconds,\nminutes, hours), weight, length, and also number of automobiles in household (0, 1, 2, . . . ).\nfrequently metric variables are also called quantitative, in contrast to nominal variables, which\nare called qualitative. ordinal variables are somewhat in between. ordered categorical vari-\nables with few categories are sometimes considered as qualitative, although the ordering has\nsome quantitative aspect.\n\na careful definition and reflection of scale levels is found in particular in the psychology\nliterature. measuring intelligence is no easy task, so psychologists needed to develop some\nfoundation for their measurements and developed an elaborated mathematical theory of mea-\nsurement (see, in particular, krantz et al., 1971).\n\n "}, {"Page_number": 17, "text": "1.2. organization of this book\n\n5\n\ndiscrete and continuous variables\nthe distinction between discrete and continuous variables is completely unrelated to the con-\ncept of scale levels.\nit refers only to the number of values a variable can take. a discrete\nvariable has a finite number of possible values or values that can at least be listed. thus count\ndata like the number of accidents with possible values from 0, 1, . . . are considered discrete.\nthe possible values of a continuous variable form an interval, although, in practice, due to the\nlimitations of measuring instruments, not all of the possible values are observed.\n\nwithin the scope of this book discrete data like counts are considered as categorical. in\nparticular, when the mean of a discrete response variable is small it is essential to recognize the\ndiscrete nature of the data.\n\n1.2 organization of this book\nthe chapters may be grouped into five different units. after a brief review of basic issues in\nstructured regression and classical normal distribution regression within this chapter, in the first\nunit, consisting of chapters 2 through 7, the parametric modeling of univariate categorical re-\nsponse variables is discussed. in chapter 2 the basic regression model for binary response, the\nlogit or logistic regression model, is described. chapter 3 introduces the class of generalized\nlinear models (glms) into which the logit model as well as many other models in this book\nmay be embedded. in chapters 4 and 5 the modeling of binary response data is investigated\nmore closely, including inferential issues but also the structuring of ordered categorical pre-\ndictors, alternative link functions, and the modeling of overdispersion. chapter 6 extends the\napproaches to high-dimensional predictors. the focus is on appropriate regularization methods\nthat allow one to select predictor variables in cases where simple fitting methods fail. chapter\n7 deals with count data as a special case of discrete response.\n\nchapters 8 and 9 constitute the second unit of the book. they deal with parametric multi-\nnomial response models. chapter 8 focuses on unordered multinomial responses, and chapter\n9 discusses models that make use of the order information of the response variable.\n\nthe third unit is devoted to flexible non-linear regression, also called non-parametric regres-\nsion. here the data determine the shape of the functional form with much weaker assumptions\non the underlying structure. non-linear smooth regression is the subject of chapter 10. the\nmodeling approaches are presented as extensions of generalized linear models. one section is\ndevoted to functional data, which are characterized by high-dimensional but structured regres-\nsors that often have the form of a continuous signal. tree-based modeling approaches, which\nprovide an alternative to additive and smooth models, are discussed in chapter 11. the method\nis strictly non-parametric and conceptually very simple. by binary recursive partitioning the\nfeature space is partitioned into a set of rectangles, and on each rectangle a simple model is\nfitted. instead of obtaining parameter estimates, one obtains a binary tree that visualizes the\npartitioning of the feature space.\n\nchapter 12 is devoted to the more traditional topic of contingency analysis. the main instru-\nment is the log-linear model, which assumes a poisson distribution, a multinomial distribution,\nor a product-multinomial distribution. for poisson-distributed response there is a strong con-\nnection to count data as discussed in chapter 7, but now all predictors are categorical. when\nthe underlying distribution is multinomial, log-linear models and in particular graphical models\nare used to investigate the association structure between the categorical variables.\n\nin the fifth unit multivariate regression models are examined. multivariate responses occur\nif several responses together with explanatory variables are measured on one unit. in particular,\nrepeated measurements that occur in longitudinal studies are an important case. the challenge\nis to link the responses to the explanatory variables and to account for the correlation between\n\n "}, {"Page_number": 18, "text": "6\n\nchapter 1. introduction\n\nresponses. in chapter 13, after a brief overview, conditional and marginal models are outlined.\nsubject-specific modeling in the form of random effects models is considered in chapter 14.\n\nthe last unit, chapter 15, examines prediction issues. for categorical data the problem is\nstrongly related to the common classification problem, where one wants to find the true class\nfrom which a new observation stems. classification problems are basically diagnostic problems\nwith applications in medicine when one wants to identify the type of the disease, in pattern\nrecognition when one aims at recognition of handwritten characters, or in economics when one\nwants to identify risk clients in credit scoring. in the last decade, in particular, the analysis of\ngenetic data has become an interesting field of application for classification techniques.\n\n1.3 basic components of structured regression\nin the following the structuring components of regression are considered from a general point\nof view but with special emphasis on categorical responses. this section deals with the various\nassumptions made for the structuring of the independent and the dependent variables.\n\n1.3.1 structured univariate regression\nregression methods are concerned with two types of variables, the explanatory (or independent)\nvariables x and the dependent variables y. the collection of methods that are referred to as\nregression methods have several objectives:\n\n\u2022 modeling of the response y given x such that the underlying structure of the influence of\n\nx on y is found.\n\n\u2022 quantification of the influence of x on y.\n\n\u2022 prediction of y given an observation x.\n\nin regression the response variable y is also called the regressand, the dependent variable, and\nthe endogeneous variable. alternative names for the independent variables x are regressors,\nexplanatory variables, exogeneous variables, predictor variables, and covariates.\n\nregression modeling uses several structural components. in particular, it is useful to dis-\ntinguish between the random component, which usually is specified by some distributional\nassumption, and the components, which specify the structuring of the covariates x. more\nspecifically, in a structured regression the mean \u03bc (or any other parameter) of the dependent\nvariable y is modeled as a function in x in the form\n\n\u03bc = h(\u03b7(x)),\n\nwhere h is a transformation and \u03b7(x) is a structured term. a very simple form is used in\nclassical linear regression, where one assumes\n\n\u03bc = \u03b20 + x1\u03b21 + \u00b7\u00b7\u00b7 + xp\u03b2p = \u03b20 + xt \u03b2\n\nwith the parameter vector \u03b2t = (\u03b21, . . . , \u03b2p) and the vector of covariates xt = (x1, . . . , xp).\nthus, classical linear regression assumes that the mean \u03bc is directly linked to a linear predictor\n\u03b7(x) = \u03b20 + xt \u03b2. covariates determine the mean response by a linear term, and the link\nh is the identity function. the distributional part in classical linear regression follows from\nassuming a normal distribution for y|x.\nin binary regression, when the response takes a value of 0 or 1, the mean corresponds to the\nprobability p (y = 1|x). then the identity link h is a questionable choice since the probabilities\n\n "}, {"Page_number": 19, "text": "1.3. basic components of structured regression\n\n7\n\nare between 0 and 1. a transformation h that maps \u03b7(x) into the interval [0, 1] typically yields\nmore appropriate models.\n\nin the following, we consider ways of structuring the dependence between the mean and\nthe covariates, with the focus on discrete response data. to keep the structuring parts separated,\nwe will begin with the structural assumption on the response, which usually corresponds to\nassuming a specific distributional form, and then consider the structuring of the influential term\nand finish by considering the link between these two components.\n\nstructuring the dependent variable\na common way of modeling the variability of the dependent variable y is to assume a dis-\ntribution that is appropriate for the data. for binary data with y \u2208 {0, 1}, the distribution is\ndetermined by \u03c0 = p (y = 1). as special case of the binomial distribution it is abbreviated by\nb(1, \u03c0). for count data y \u2208 {0, 1, 2, . . .}, the poisson distribution p (\u03bb) with mass function\n\u2212\u03bb/x!, x = 0, 1, . . . is often a good choice. an alternative is the negative binomial\nf(x) = \u03bbxe\ndistribution, which is more flexible than the poisson distribution. if y is continuous, a common\nassumption is the normal distribution. however, it is less appropriate if the response is some\nduration for which y \u2265 0 has to hold. then, for example, a gamma-distribution \u03b3(\u03bd, \u03b1) that\nhas positive support might be more appropriate. in summary, the choice of the distributional\nmodel mainly depends on the kind of response that is to be modeled. figures 1.2 and 1.3 show\nseveral discrete and continuous distributions, which may be assumed. each panel shows two\ndistributions that can be thought of as referring to two distinct values of covariates. for the nor-\nmal distribution model where only the mean depends on covariates, the distributions referring\nto different values of covariates are simply shifted versions of each other. this is quite different\nfor response distributions like the poisson or the bernoulli distribution. here the change of the\nmean, caused by different values of covariates, also changes the shape of the distribution. this\nphenomenon is not restricted to discrete distributions but is typically found when responses are\ndiscrete.\n\nsometimes the assumption of a specific distribution, even if it reflects the type of data\ncollected, is too strong to explain the variability in responses satisfactorily. in practice, one\noften finds that count data and relative frequencies are more variable than is to be expected\nunder the poisson and the binomial distributions. the data show overdispersion. consequently,\nthe structuring of the responses should be weakened by taking overdispersion into account.\n\none step further, one may even drop the assumption of a specific distribution. instead of\nassuming a binomial or a poisson distribution, one only postulates that the link between the\nmean and a structured term, which contains the explanatory variables, is correctly specified.\nin addition, one can specify how the variance of the response depends on explanatory vari-\nables. the essential point is that the assumptions on the response are very weak, within quasi-\nlikelihood approaches structuring of the response in the form of distributional assumptions is\nnot necessary.\n\nstructuring the influential term\nit is tempting to postulate no structure at all by allowing \u03b7(x) to be any function. what works in\nthe unidimensional case has severe drawbacks if xt = (x1, . . . , xp) contains many variables.\nit is hard to explain how a covariate xj determines the response if no structure is assumed.\nmoreover, estimation becomes difficult and less robust. thus often it is necessary to assume\nsome structure to obtain an approximation to the underlying functional form that works in\npractice. structural assumptions on the predictor can be strict or more flexible, with the degree\nof flexibility depending on the scaling of the predictor.\n\n "}, {"Page_number": 20, "text": "8\n\nchapter 1. introduction\n\ny \u223c b(1, \u03c0)\n\ny \u223c p (\u03bb)\n\ny1, . . . , yk \u223c m(n, \u03c01, . . . , \u03c0k)\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n\u22121\n\n0.2\n\n0.1\n\n0\n\n0.6\n\n0.4\n\n0.2\n\n0\n\n\u22120.5\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n1\n\n2\n\n3\n\n4\n\nfigure 1.2: binomial, poisson, and multinomial distributions. each panel shows two\ndifferent distributions.\n\nlinear predictor\nthe most common form is the linear structure\n\n\u03b7(x) = \u03b20 + xt \u03b2,\n\nwhich is very robust and allows simple interpretation of the parameters. often it is necessary to\ninclude some interaction terms, for example, by assuming\n\n\u03b7(x) = \u03b20 + x1\u03b21 + \u00b7\u00b7\u00b7 + xp\u03b2p + x1x2\u03b212 + x1x3\u03b213 + \u00b7\u00b7\u00b7 + x1x2x3\u03b2123\n\n= zt \u03b2.\n\nby considering zt = (1, x1, . . . , xp, x1x2, . . . , x1x2x3, . . . ) as variables, one retains the linear\nstructure. for estimating and testing (not for interpreting) it is only essential that the structure\nis linear in the parameters. when explanatory variables are quantitative, interpreting the param-\neters is straightforward, especially in the linear model without interaction terms.\n\n "}, {"Page_number": 21, "text": "1.3. basic components of structured regression\n\n9\n\ny \u223c n(\u03bc, \u03c32)\n\ny \u223c \u03b3(\u03bd, \u03b1)\n\n0.5\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0\n\n0.4\n\n0.2\n\n0\n0\n\n\u22124\n\n\u22122\n\n0\n\n2\n\n4\n\n6\n\n8\n\n2\n\n4\n\n6\n\n8\n\n10\n\nfigure 1.3: normal and gamma-distributions.\n\ncategorical explanatory variables\ncategorical explanatory variables, also called factors, take values from a finite set 1, . . . , k,\nwith the numbers representing the factor levels. they cannot be used directly within the linear\npredictor because one would falsely assume fixed ordering of the categories with the distances\nbetween categories being meaningful. that is not the case for nominal variables, not even for\nordered categorical variables. therefore, specific structuring is needed for factors. common\nstructuring uses dummy variables and again yields a linear predictor. the coding scheme de-\npends on the intended use and on the scaling of the variable. several coding schemes and\ncorresponding interpretations of effects are given in detail in section 1.4.1. the handling of\nordered categorical predictors is also considered in section 4.4.3.\n\nwhen a categorical variable has many categories, the question arises of which categories can\nbe distinguished with respect to the response. should categories be collapsed, and if so, which\nones? the answer depends on the scale level. while for nominal variables, for which categories\nhave no ordering, any fusion categories seems sensible, for ordinal predictors collapsing means\nfusing adjacent categories. figure 1.4 shows a simple application. it shows the effect of the\nurban district and the year of construction on the rent per square meter in munich. urban district\nis a nominal variable that has 25 categories, year of construction is an ordered predictor, where\ncategories are defined by decades. the coefficient paths in figure 1.4 show how, depending on\na tuning parameter, urban districts and decades are combined. it turns out that only 10 districts\nare really different, and the year of construction can be combined into 8 distinct categories (see\nalso section 6.5).\n\n "}, {"Page_number": 22, "text": "10\n\nchapter 1. introduction\n\nurban district\n\nyear of construction\n\n0\n\n.\n\n0\n\n5\n\n.\n\n0\n\u2212\n\n0\n\n.\n\n1\n\u2212\n\n5\n\n.\n\n1\n\u2212\n\nt\n\ni\n\nn\ne\nc\ni\nf\nf\n\ne\no\nc\n \ny\nm\nm\nu\nd\n\nt\n\ni\n\nn\ne\nc\ni\nf\nf\n\ne\no\nc\n \ny\nm\nm\nu\nd\n\n5\n\n.\n\n1\n\n0\n\n.\n\n1\n\n5\n\n.\n\n0\n\n0\n\n.\n\n0\n\n5\n\n.\n\n0\n\u2212\n\n0\n\n.\n\n1\n\u2212\n\n2000s\n1990s\n\n1980s\n\n1970s\n\n1960s\n\n1950s\n\n1930s\n1940s\n\n1920s\n\n18\n5\n\n12\n2\n\n3\n\n4\n\n13\n\n10\n\n9\n\n8\n\n15\n25\n\n17\n19\n\n23\n\n11\n\n24\n\n16\n\n6\n\n20\n21\n\n7\n\n14\n\n22\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\ns smax\n\ns smax\n\nfigure 1.4: effects of urban district and year of construction (in decades) on rent per\nsquare meter.\n\nadditive predictor\nfor quantitative explanatory variables, a less restrictive assumption is\n\n\u03b7(x) = f(1)(x1) + \u00b7\u00b7\u00b7 + f(p)(xp),\n\nwhere f(j)(xj) are unspecified functions. thus one retains the additive form, which still allows\nsimple interpretation of the functions f(j) by plotting estimates but the approach is much less\nrestrictive than in the linear predictor. an extension is the inclusion of unspecified interactions,\nfor example, by allowing\n\n\u03b7(x) = f(1)(x1) + \u00b7\u00b7\u00b7 + f(p)(xp) + f(13)(x1, x3),\n\nwhere f(13)(x1, x3) is a function depending on x1 and x3.\n\nfor categorical variables no function is needed because only discrete values occur. thus,\nwhen, in addition to quantitative variables, x1, . . . , xp, categorical covariates are available, they\nare included in an additional linear term, zt \u03b3, which is built from dummy variables. then one\nuses the partial linear predictor\n\n\u03b7(x) = f(1)(x1) + \u00b7\u00b7\u00b7 + f(p)(xp) + \u03b3.\n\nadditive structure with effect modifiers\nif the effect of a covariate, say gender (x1), depends on age (x2) instead of postulating an\ninteraction model of the form \u03b7 = \u03b20 + x1\u03b21 + x2\u03b22 + x1x2\u03b212, a more flexible model is given\nby\n\n\u03b7 = \u03b22(x2) + x1\u03b212(x2),\n\n "}, {"Page_number": 23, "text": "1.3. basic components of structured regression\n\n11\n\nwhere \u03b22(x2) is the smooth effect of age and \u03b212(x2) is the effect of gender (x1), which is\nallowed to vary over age. both functions \u03b22(.) and \u03b212(.) are unspecified and the data determine\ntheir actual form.\n\ntree-based methods\nan alternative way to model interactions of covariates is the recursive partitioning of the predic-\ntor space into sets of rectangles. the most popular method, called cart for classification and\nregression trees, constructs for metric predictors partitions of the form {x1 \u2264 c1}\u2229\u00b7\u00b7\u00b7\u2229{xm \u2264\ncm}, where c1, . . . , cm are split-points from the regions of the variables x1, . . . , xm. the splits\nare constructed successively beginning with a first split, producing, for example, {x1 \u2264 c1},\n{x1 > c1}. then these regions are split further. the recursive construction scheme allows us to\npresent the resulting partition in a tree. a simple example is the tree given in figure 1.5, where\ntwo variables are successively split by using split-points c1, . . . , c4. the first split means that\nthe dichotomization into {x1 \u2264 c1} and {x1 > c1} is of major importance for the prediction\nof the outcome. finer prediction rules are obtained by using additional splits, for example, the\nsplit of the region {x1 \u2264 c1} into {x2 \u2264 c2} and {x2 > c2}. the big advantage of trees is that\nthey are easy to interpret and the visualization makes it easy to communicate the underlying\nstructure to practitioners.\n\nthe link between covariates and response\nclassical linear regression assumes \u03bc = \u03b7(x) with \u03b7(x) = xt \u03b2. for binary regression models,\nthe more general form \u03bc = h(\u03b7(x)) is usually more appropriate, since h may be chosen such\nthat \u03bc takes values in the unit interval [0, 1]. typically h is chosen as a distribution function,\nfor example, the logistic distribution function h(\u03b7) = exp(\u03b7)/(1 + exp(\u03b7)) or the normal\ndistribution function. the corresponding models are the so-called logit and probit models.\nhowever, any distribution function that is strictly monotone may be used as a response function\n(see section 5.1).\n\n|\n\nx1 \u2264 c1\n\nx2 \u2264 c2\n\nx2 \u2264 c3\n\nx2 \u2264 c4\n\nfigure 1.5: example tree for two variables.\n\n "}, {"Page_number": 24, "text": "12\n\nchapter 1. introduction\n\nin many applications h is considered as known. then, of course, there is the danger of\nmisspecification. it is often more appropriate to consider alternative transformation functions\nand choose the one that yields the best fit. alternatively, one can estimate the transformation\nitself, and therefore let the data determine the form of the transformation function (section 5.2).\n\n1.3.2 structured multicategorical regression\nwhen the response is restricted to a fixed set of possible values, the so-called response cat-\negories, the typical assumption for the distribution is the multinomial distribution. when\n1, . . . , k denote the response categories of variable y , the multinomial distribution specifies\nthe probabilities \u03c01(x), . . . , \u03c0k(x), where \u03c0r(x) = p (y = r|x).\n\nthe simple structure of univariate regression models is no longer appropriate because the re-\nsponse is multivariate. one has to model the dependence of all the probabilities \u03c01(x), . . . , \u03c0k(x)\non the explanatory variables. this may be accomplished by a multivariate model that has the\nbasic structure\n\n\u03c0r(x) = hr(\u03b71(x), . . . , \u03b7k(x)), r = 1, . . . , k \u2212 1,\n\nwhere hr, r = 1, . . . , k\u22121, are transformation functions that are specific for the category. since\nprobabilities sum up to one, it is sufficient to specify k \u2212 1 of the k components. by using the\n(k\u22121)-dimensional vectors \u03c0(x)t = (\u03c01(x), . . . , \u03c0k\u22121(x)), \u03b7(x))t = (\u03b71(x), . . . , \u03b7k\u22121(x)),\nmodels have the closed form\n\n\u03c0(x) = h(\u03b7(x)).\n\nthe choice of the transformation function depends on the scale level of the response. if the\nresponse is nominal, for example, when modeling the choice of different brands or the choice\nof transport mode, other response functions are more appropriate than in the ordinal case, when\nthe response is given on a rating scale with categories like very good, good, fair, poor, and very\npoor (see chapter 8 for nominal and chapter 9 for ordinal responses).\n\nthe structuring of the predictor functions is in analogy to univariate responses. strict linear\n\nstructures assume\n\n\u03b7r(x) = xt \u03b2r,\n\nwhere the parameter vector depends on the category r. by defining an appropriate design ma-\ntrix, one obtains a multivariate generalized linear model form \u03c0(x) = h(x\u03b2). more flexible\npredictors use additive or partial additive structures of the form\n\n\u03b7r(x) = f(r1)(x1) + \u00b7\u00b7\u00b7 + f(rp)(xp),\n\nwith functions depending on the category.\n\n1.3.3 multivariate regression\nin many studies several response variables are observed for each unit. the responses may refer\nto different variables or to the same measurements that are observed repeatedly. the latter case\nis found in particular in longitudinal studies where measurements on an individual are observed\nat several times under possibly varying conditions. in both cases the response is a multivariate\ni = (yi1, . . . , yim), which collects the measurements on unit i. since\nrepresented by a vector yt\nmeasurements taken on one unit or cluster tend to be more similar, one has to assume that in\ngeneral the measurements are correlated.\n\n "}, {"Page_number": 25, "text": "1.3. basic components of structured regression\n\n13\n\nstructuring the dependent variables\nthe structuring of the vector of dependent variables by assuming an appropriate distribution\nis not as straightforward as in classical multivariate regression, where a multivariate normal\ndistribution is assumed. multivariate, normally distributed responses have been extensively\ninvestigated for a long time. they are simply structured with a clear separation of the mean and\ncorrelation structures, which are sufficient to define the distribution.\n\nwhen the marginals, that is, single responses yit, are discrete it is harder to find a sparse\nrepresentation of the total response vector. although the form of the distribution may have a\nsimple form, the number of parameters can be extremely high. if, for example, the marginals\nare binary with yit \u2208 {0, 1}, the total distribution is multinomial but determined by 2m prob-\nabilities for the various combinations of outcomes. with m = 10 measurements, the number\nof parameters is 1024. simple measures like the mean of the marginals and the correlations\nbetween components do not describe the distribution sufficiently.\n\none strategy that is used in marginal modeling is to model the mean structure of the\nmarginals, which uses univariate regression models, and in addition specify an association\nstructure between components that does not have to be the correct association structure. an\nalternative approach uses random effects. one assumes that the components are uncorrelated\ngiven a fixed but unobserved latent variable, which is shared by the measurements within one\nunit or cluster.\nin both cases one basically uses parameterizations of the discrete marginal\ndistributions.\n\nstructuring the influential term\nthe structuring of the influential term is more complex than in univariate response models\nbecause covariates may vary across measurements within one cluster. for example, in longitu-\ndinal studies, where the components refer to repeated measurements across time, the covariates\nthat are to be included may also vary across time. although the specification is more complex,\nin principle, the same form of predictors as in univariate regression applies. one can use linear\nterms or more flexible additive terms. however, new structuring elements are useful, and two\nof them are the following.\n\nin random effects models one models explicitly the heterogeneity of clustered responses by\nassuming cluster-specific random effects. for example, the binary response of observation t on\ncluster i with covariate vector xit at measurement t may be modeled by\n\np (yit = 1|xit, bi) = h(\u03b7it),\n\n\u03b7it = bi + xt\n\nit\u03b2.\n\nwhile \u03b2 is a fixed effect that is common to all clusters, each cluster has its own cluster-specific\nrandom effect bi, which models the heterogeneity across clusters. more generally, one can\nassume that not only the intercept but also the slopes of the variables are cluster-specific. then\none has to specify which components have category-specific effects. moreover, one has to\nspecify a distribution for these category-specific effects. thus, some additional structuring and\ntherefore decision making by the modeler is needed.\n\nanother effect structure that can be useful in repeated measurements is the variation of\n\neffect strength across time, which can be modeled by letting the parameter depend on time:\n\n\u03b7it = bi + xt\n\nit\u03b2t.\n\nmarginal modeling approaches are given in detail in chapter 13, and random effects models are\nfound in chapter 14.\n\n "}, {"Page_number": 26, "text": "14\n\nchapter 1. introduction\n\n1.3.4 statistical modeling\nstatistical modelling refers to the process of using models to extract information from data. in\nstatistics that means, in particular, to separate the systematic effects or underlying patterns from\nthe random effects. a model, together with its estimation method, can be seen as a measuring\ninstrument. like magnifying glasses and telescopes serve as instruments to uncover structures\nnot seen to the unarmed eye, a model allows one to detect patterns that are in the data but not\nseen without the instrument. what is seen depends on the instrument. only those patterns are\nfound for which the instrument is sensitive. for example, linear models allow one to detect\nlinear structures. if the underlying pattern is non-linear, they fail and the results can be very\nmisleading. or, effect modifiers are detected only if the model allows for them. in that sense\nthe model determines what is found. more flexible models allow one to see more complex\nstructures, at least if reliable estimation methods are found. in the same way as a telescope\ndepends on basic conditions like the available amount of light, statistical models depend on\nbasic conditions like the sample size and the strength of the underlying effects. weak patterns\ntypically can be detected only if much information is available.\n\nthe use and choice of models is guided by different and partly contradictory objectives.\nmodels should be simple but should account for the complexity of the underlying structure.\nsimplicity is strongly connected to interpretability. users of statistical models mostly prefer\ninterpretable models over black boxes. in addition, the detection of interpretable and simple\npatterns and therefore understanding is the essential task of science. in science, models often\nserve to understand and test subject-matter hypotheses about underlying processes.\n\nthe use of models, parametric or more flexible, is based on the assumption that a stochastic\ndata model has generated the data. the statistician is expected to use models that closely\napproximate the data driving the model, quantify the effects within the model, and account for\nthe estimation error. ideally, the analysis also accounts for the closeness of the model to the\ndata-generating model, for example, in the form of goodness-of-fit tests.\n\na quite different objective that may determine the choice of the model is the exactness of\nthe prediction. one wants to use that model that will give the best results in terms of predic-\ntion error when used on future data. then black box machines, which do not try to uncover\nlatent structures, also apply and may show excellent prediction results. ensemble methods like\nrandom forests or neural networks work in that way. breiman (2001b) calls them algorithmic\nmodels in contrast to data models, which assume that a stochastic data-generating model is\nbehind the data. algorithmic models are less models than an approach to find good predic-\ntion rules by designing algorithms that link the predictors to the responses. especially in the\nmachine learning community, where the handling of data is guided by a more pragmatic view,\nalgorithms with excellent prediction properties in the form of black boxes are abundant.\n\nwhat type of model is to be preferred depends mainly on the objective of the scientific\nquestion. if prediction is the focus, intelligently designed algorithms may serve the purpose\nwell. if the focus is on understanding and interpretation, data models are to be preferred. the\nchoice of the model depends on the structures that are of interest to the user and circumstances\nlike sample size and strength of the parameters. when effects are weak and the sample size is\nsmall, simple parametric models will often be more stable than models that allow for complex\npatterns. as a model does not fit all datasets, for a single dataset different models that uncover\ndifferent structures may be appropriate. typically there is not a single best model for a set of\ndata.\n\nin the following chapters we will predominantly consider tools for data models; that is\nalternative models, estimation procedures, and diagnostic tools will be discussed. in the last\nchapter, where prediction is the main issue, algorithmic models/methods will also be included.\nspecification of models is treated throughout the book in various forms, ranging from classical\n\n "}, {"Page_number": 27, "text": "1.4. classical linear regression\n\n15\n\ntest procedures to examine parameters to regularization techniques that allow one to select\nvariables or link functions.\n\nthere is a rich literature on model selection. burnham and anderson (2002) give an exten-\nsive account of the information-theoretic approach to model selection; see also claeskens and\nhjort (2008) for a survey on the research in the field. the alternative modeling cultures, data\nmodeling versus algorithmic modeling, was treated in a stimulating article by breiman (2001b).\nhis strong opinion on stochastic data models is worth reading, in particular together with the\nincluded and critical discussion.\n\n1.4 classical linear regression\nsince the linear regression model is helpful as a background model, in this section a brief\noverview of classical linear regression is given. the section may be skipped if one feels familiar\nwith the model. it is by no means a substitute for a thorough introduction to gaussian response\nmodels, but a reminder of the basic concepts. parametric regression models including linear\nmodels are discussed in detail in many statistics books, for example, cook and weisberg (1982),\nryan (1997), harrell (2001), and fahrmeir et al. (2011).\n\nthe basic multiple linear regression model is often given in the form\n\ny = \u03b20 + x1\u03b21 + \u00b7\u00b7\u00b7 + xp\u03b2p + \u03b5,\n\nwhere \u03b5 is a noise variable that fulfills e(\u03b5) = 0. thus, for given x1, . . . , xp the expecta-\ntion of the response variable \u03bc = e(y|x1, . . . , xp) is specified as a linear combination of the\nexplanatory variables x1, . . . , xp:\n\n\u03bc = \u03b20 + x1\u03b21 + \u00b7\u00b7\u00b7 + xp\u03b2p = \u03b20 + xt \u03b2,\n\nwith the parameter vector \u03b2t = (\u03b21, . . . , \u03b2p) and vector of covariates xt = (x1, . . . , xp).\n\n1.4.1 interpretation and coding of covariates\nwhen interpreting the parameters of linear models it is useful to distinguish between quanti-\ntative (metrically scaled) covariates and categorical covariates, also called factors, which are\nmeasured on a nominal or ordinal scale level.\n\nquantitative explanatory variables\nfor a quantitative covariate like age, the linear model has the form\ne(y|x) = \u03b20 + x\u03b2a = \u03b20 + age \u2217 \u03b2a.\n\nfrom e(y|x + 1)\u2212 e(y|x) = \u03b2a it is immediately seen that \u03b2a reflects the change of the mean\nresponse if the covariate is increased by one unit. if the response is income in dollars and the\ncovariate age is given in years, the units of \u03b2a are dollars per year and \u03b2a is the change of\nexpected income resulting from increasing the age by one year.\n\nbinary explanatory variables\na binary covariate like gender (g) has to be coded, for example, as a (0-1)-variable in the form\n\n(cid:2)\n\nxg =\n\n1 male\n0\n\nfemale.\n\n "}, {"Page_number": 28, "text": "16\n\nchapter 1. introduction\n\nthen the interpretation is the same as for quantitative variables. if the response is income in\ndollars, the parameter \u03b2g in\n\ne(y|x) = \u03b20 + xg\u03b2g\n\nrepresents the change in mean income if xg is increased by one unit. since xg has only two\nvalues, \u03b2g is equivalent to the increase or decrease of the mean response resulting from the\ntransition from xg = 0 to xg = 1. thus \u03b2g is the difference in mean income between men\nand women.\n\nmulticategorical explanatory variables or factors\nlet a covariate have k possible categories and categories just reflect labels. this means that\nthe covariate is measured on a nominal scale level. often covariates of this type are called\nfactors or factor variables. a simple example is in the medical profession p , where the possible\ncategories gynaecologist, dermatologist, and so on, are coded as numbers 1, . . . , k. when\nmodeling a response like income, a linear term p \u2217 \u03b2p will produce nonsense since it assumes\na linear relationship between the response and the values of p \u2208 {1, . . . , k} , but the coding for\nprofession by numbers is arbitrary. to obtain parameters that are meaningful and have simple\ninterpretations one defines dummy variables. one possibility is dummy or (0-1)-coding.\n\n(0-1)-coding of p \u2208 {1, . . . , k}\n\n(cid:2)\n\nxp (j) =\n\n1 if p = j\n0 otherwise\n\nwhen an intercept is in the model only k \u2212 1 variables can be used. otherwise, one would have\ntoo many parameters that would not be identifiable. therefore, one dummy variable is omitted\nand the corresponding category is considered the reference category. when one chooses k as\nthe reference category the linear predictor is determined by the first k \u2212 1 dummy variables:\n\ne(y|p ) = \u03b20 + xp (1)\u03b2p (1) + . . . + xp (k\u22121)\u03b2p (k\u22121).\n\n(1.1)\n\ninterpretation of the parameters follows directly from considering the response for different\nvalues of p :\n\ne(y|p = i) = \u03b20 + \u03b2p (i),\n\ni = 1, . . . k \u2212 1, e(y|p = k) = \u03b20.\n\n\u03b20 is the mean for the reference category k and \u03b2p (i) is the increase or decrease of the mean\nresponse in comparison to the reference category k. of course any category can be used as the\nreference. thus, for a categorial variable, k \u2212 1 functionally independent dummy variables are\nintroduced. a particular choice of the reference category determines the set of variables, or in\nthe terminology of analysis of variance, the set of contrasts. the use of all k dummy variables\nresults in overparameterization because they are not functionally independent. the sum over\nall k dummy variables yields 1, and therefore \u03b20 would not be identifiable.\n\nan alternative coding scheme is effect coding, where categories are treated in a symmetric\n\nway.\n\n "}, {"Page_number": 29, "text": "1.4. classical linear regression\n\n17\n\neffect coding of p \u2208 {1, . . . , k}\n\n\u23a7\u23aa\u23a8\n\u23aa\u23a91\n\u22121\n0\n\nxp (j) =\n\nif p = j\nif p = k,\notherwise\n\nj = 1, . . . , k \u2212 1\n\nthe linear predictor (1.1) now yields\n\ne(y|p = i) = \u03b20 + \u03b2p (i) , i = 1, . . . , k \u2212 1,\ne(y|p = k) = \u03b20 \u2212 \u03b2p (1) \u2212 . . . \u2212 \u03b2p (k\u22121).\n\nit is easily seen that\n\n\u03b20 =\n\n1\nk\n\ne(y|p = j)\n\nk(cid:7)\n\nj=1\n\nis the average response across the categories and\n\n\u03b2p (j) = e(y|p = j) \u2212 \u03b20\n\nis the deviation of category j from the average response level given by \u03b20. although only k \u2212 1\ndummy variables are used, interpretation does not refer to a particular category. there is no\nreference category as in the case of (0-1)-coding. for the simple example of four categories one\nobtains\n\nxp (1) xp (2) xp (3)\n0\n0\n1\n\u22121\n\n0\n1\n0\n\u22121\n\n1\n0\n0\n\u22121\n\n.\n\n1\np 2\n3\n4\n\nalternative coding schemes are helpful, for example, if the categories are ordered. then a split\nthat distinguishes between categories below and above a certain level often reflects the ordering\nin a better way.\n\nsplit-coding of p \u2208 {1, . . . , k}\n(cid:2)\n\nxp (j) =\n\n1\n0\n\nif p > j\notherwise\n\nj = 1, . . . , k \u2212 1\n\nwith split-coding, the model e(y|p ) = \u03b20 + xp (1)\u03b2p (1) + . . . + xp (k\u22121)\u03b2p (k\u22121) yields\n\ne(y|p = 1) = \u03b20,\ne(y|p = i) = \u03b20 + \u03b2p (1) + \u00b7\u00b7\u00b7 + \u03b2p (i\u22121) , i = 2, . . . , k\n\n "}, {"Page_number": 30, "text": "18\n\nand therefore the coefficients\n\nchapter 1. introduction\n\n\u03b20 = e(y|p = 1),\n\n\u03b2p (i) = e(y|p = i + 1) \u2212 e(y|p = i) , i = 1, . . . , k \u2212 1.\n\nthus the coefficients \u03b2p (i), i = 1, . . . , k \u2212 1 represent the difference in expectation when the\nfactor level increases from i to i + 1. they may be seen as the stepwise change over categories\ngiven in the fixed order 1, . . . , k, with 1 serving as the reference category where the process\nstarts. in contrast to (0-1)-coding and effect coding, split-coding uses the ordering of categories:\n\u03b2p (1) + \u00b7\u00b7\u00b7 + \u03b2p (i\u22121) can be interpreted as the change in expectation for the transition from\ncategory 1 to category i with intermediate categories 2, 3, . . . , i \u2212 1.\n\nfor further coding schemes see, for example, chambers and hastie (1992). the structuring\nof the linear predictor is examined in more detail in chapter 4, where models with binary\nresponses are considered.\n\n1.4.2 linear regression in matrix notation\nlet the observations be given by (yi, xi1, . . . , xip) for i = 1, . . . , n, where yi is the response\nand xi1, . . . , xip are the given covariates. the model takes the form\n\nyi = \u03b20 + xi1\u03b21 + \u00b7\u00b7\u00b7 + xip\u03b2p + \u03b5i, i = 1, . . . n.\n\nwith xt\n\ni = (1, xi1, . . . xip), \u03b2t = (\u03b20, \u03b21, . . . \u03b2p) it may be written more compact as\n\nyi = xt\n\ni \u03b2 + \u03b5i,\n\nwhere xi has length \u02dcp = p + 1. in matrix notation one obtains\n\n\u23a4\n\u23a5\u23a5\u23a5\u23a5\u23a6 =\n\n\u23a1\n\u23a2\u23a2\u23a2\u23a2\u23a3\n\ny1\n...\n...\nyn\n\n\u23a1\n\u23a2\u23a2\u23a2\u23a31 x11\n\n1 x21\n...\n1 xn1\n\n\u23a4\n\u23a5\u23a5\u23a5\u23a6 +\n\n\u23a4\n\u23a5\u23a5\u23a5\u23a6\n\n\u23a1\n\u23a2\u23a2\u23a2\u23a3\u03b20\n\n\u03b21\n...\n\u03b2p\n\n\u23a4\n\u23a5\u23a5\u23a5\u23a5\u23a6\n\n\u23a1\n\u23a2\u23a2\u23a2\u23a2\u23a3\n\n\u03b51\n...\n...\n\u03b5n\n\n. . . x1p\n. . . x2p\n\n. . . xnp\n\nor simply\n\nwhere\n\ny = x\u03b2 + \u03b5,\n\nyt = (y1, . . . , yn) is the vector of responses;\n\nx is the design matrix, which is composed from the explanatory variables;\n\n\u03b2t = (\u03b20, . . . , \u03b2p) is the (p + 1)-dimensional parameter vector;\n\n\u03b5t = (\u03b51, . . . , \u03b5n) is the vector of errors.\n\ncommon assumptions are that the errors have expectation zero, e(\u03b5i) = 0, i = 1, . . . , n; the\nvariance of each error component is given by var(\u03b5i) = \u03c32, i = 1, . . . , n, called homoscedas-\nticity or homogeneity; and the error components from different observations are uncorrelated,\ncov(\u03b5i, \u03b5j) = 0, i (cid:8)= j.\n\n "}, {"Page_number": 31, "text": "1.4. classical linear regression\n\n19\n\nmultiple linear regression\n\nyi = xt\n\ni \u03b2 + \u03b5i\n\nor y = x\u03b2 + \u03b5\n\nassumptions:\n\ne(\u03b5i) = 0,\n\nvar(\u03b5i) = \u03c32,\n\ncov(\u03b5i, \u03b5j) = 0, i (cid:8)= j\n\nin matrix notation the assumption of homogeneous variances may be condensed into e(\u03b5) =\n0, cov(\u03b5) = \u03c32i.\nif one assumes in addition that responses are normally distributed, one\npostulates \u03b5 \u223c n(0, \u03c32i). it is easy to show that the assumptions carry over to the observable\nvariables yi in the form\n\ne(y|x) = x\u03b2, y \u223c n(x\u03b2, \u03c32i).\n\nthe last representation may be seen as a structured representation of the classical linear model,\nwhich gives the distributional and systematic components separately without using the noise\nvariable \u03b5. with \u03bc denoting the mean response vector with components \u03bci = xt\ni \u03b2 one has the\nfollowing form.\n\nmultiple linear regression with normally distributed errors\n\n\u03bc = x\u03b2, y \u223c n(x\u03b2, \u03c32i)\n\nthe separation of the structural and distributional components is an essential feature that\n\nforms the basis for the extension to more general models considered in chapter 3.\n\n1.4.3 estimation\nleast-squares estimation\na simple criterion to obtain estimates of the unknown parameter \u03b2 is the least-squares criterion,\nwhich minimizes\n\nn(cid:7)\n\ni=1\n\nq(\u03b2) =\n\n(yi \u2212 xt\n\ni \u03b2)2.\n\nthus the parameter \u02c6\u03b2, which minimizes q(\u03b2), is the parameter that minimizes the squared\n\u02c6\u03b2. the choice of the\ndistance between the actual observation yi and the predicted value \u02c6yi = xt\ni\nsquared distance has the advantage that an explicit form of the estimate is available. it means\nthat large discrepancies between yi and \u02c6yi are taken more seriously than for the euclidean\ndistance |yi \u2212 \u02c6yi|, which would be an alternative criterion to minimize. simple calculation\nshows that the derivative of q(\u03b2) has the form\n\nn(cid:7)\n\ni=1\n\n\u2202q(\u03b2)\n\n\u2202\u03b2s\n\n=\n\ni \u03b2)xis,\n\n2(yi \u2212 xt\n(cid:14)\n\ns = 0, . . . , p, where xi0 = 1. a minimum can be expected if the derivative equals zero. thus\n\u02c6\u03b2) = 0. in vector notation\nthe least-squares estimate has to fulfill the equation\n\ni xis(yi \u2212 xt\n\ni\n\n "}, {"Page_number": 32, "text": "20\n\none obtains the form\n\nn(cid:7)\n\ni=1\n\nxiyi =\n\n(cid:7)\n\ni\n\nxixt\ni\n\n\u02c6\u03b2,\n\nchapter 1. introduction\n\nwhich may be written in the form of the normal equation x t y = x t x \u02c6\u03b2. assuming that the\ninverse of x t x exists, an explicit solution is given by\n\n\u02c6\u03b2 = (x t x)\n\n\u22121x t y.\n\nmaximum likelihood estimation\nthe least-squares estimate is strongly connected to the maximum likelihood (ml) estimate if\none assumes that the error is normally distributed. the normal distribution contains a quadratic\nterm. thus it is not surprising that the ml estimate is equivalent to minimizing squared dis-\ntances. if one assumes \u03b5i \u223c n(0, \u03c32) or, equivalently, yi \u223c n(xt\ni \u03b2, \u03c32), the conditional\nlikelihood (given x1, . . . , xn) is given by\n\nl(\u03b2, \u03c32) =\n\n1\u221a\n2\u03c0\u03c32\n\nexp(\u2212(yi \u2212 xt\n\ni \u03b2)2/(2\u03c32)).\n\nthe corresponding log-likelihood has the form\n\nn(cid:15)\n\ni=1\n\nn(cid:7)\n\nl(\u03b2, \u03c32) = \u2212 1\n2\u03c32\n= \u2212 1\n\n(yi \u2212 xt\n2\u03c32 q(\u03b2) \u2212 n\n\ni=1\n\n2\n\ni \u03b2)2 \u2212 n\n2\nlog(2\u03c0) \u2212 n log(\u03c32).\n\nlog(2\u03c0) \u2212 log(\u03c32)\n\nas far as \u03b2 is concerned, maximization of the log-likelihood is equivalent to minimizing the\nsquared distances q(\u03b2). simple derivation shows that maximization of l(\u03b2, \u03c32) with respect\nto \u03c32 yields\n\nn(cid:7)\n\ni=1\n\nm l =\n\u02c6\u03c32\n\n1\nn\n\n(yi \u2212 xt\n\ni\n\n\u02c6\u03b2)2.\n\nit is noteworthy that the maximum likelihood estimate \u02c6\u03b2 (which is equivalent to the least-\nsquares estimate) does not depend on \u03c32. thus the parameter \u03b2 is estimated without reference\nto the variability of the response.\n\nproperties of estimates\na disadvantage of the ml estimate \u02c6\u03c32\nestimate is given by\n\nm l is that it underestimates the variance \u03c32. an unbiased\n\n\u02c6\u03c32 =\n\n1\n\nn \u2212 (p + 1)\n\n(yi \u2212 xt\n\ni\n\n\u02c6\u03b2)2,\n\nn(cid:7)\n\ni=1\n\nwhere the correction in the denominator reflects the number of estimated parameters in \u02c6\u03b2,\nwhich is p + 1 since an intercept is included. the essential properties of estimates are given in\nthe so-called gauss-markov theorem. assuming for all observations e(\u03b5i) = 0, var(\u03b5i) = \u03c32,\ncov(\u03b5i, \u03b5j) = 0, i (cid:8)= j, one obtains\n(1) \u02c6\u03b2 and \u02c6\u03c32 are unbiased, that is, e(\u02c6\u03b2) = \u03b2, e(\u02c6\u03c32) = \u03c32.\n\n "}, {"Page_number": 33, "text": "1.4. classical linear regression\n\n21\n\n(2) cov(\u02c6\u03b2) = \u03c32(x t x)\u22121.\n(3) \u02c6\u03b2 is the best linear unbiased estimate of \u03b2. this means that, for any vector, c var(ct \u02c6\u03b2) \u2264\nvar(ct \u02dc\u03b2) holds where \u02dc\u03b2 is an unbiased estimator of \u03b2, which has the form \u02dc\u03b2 = ay + d\nfor some matrix a and vector d.\n\nestimators in linear multiple regression\n\nleast-squares estimate\n\nunbiased estimate of \u03c32\n\n\u02c6\u03b2 = (x t x)\n\n\u22121x t y\nn(cid:7)\n\n\u02c6\u03c32 =\n\n1\n\nn \u2212 p \u2212 1\n\ni=1\n\n(yi \u2212 xt\n\ni \u03b2)2\n\n1.4.4 residuals and hat matrix\nfor single observations the discrepancy between the actual observation and the fitted value\n\u02c6yi = xt\ni\n\n\u02c6\u03b2 is given by the simple residual\n\nri = yi \u2212 xt\n\ni\n\n\u02c6\u03b2.\n\nit is a preliminary indicator for ill-fitting observations, that is, observations that have large resid-\nuals. since the classical linear model assumes that the variance is the same for all observations,\none might suspect that the residuals also have the same variance. however, because \u02c6\u03b2 depends\non all of the observations, they do not. thus, for the diagnosis of an ill-fitting value, one has\nto take the variability of the estimate into account. for the derivation of the variance a helpful\ntool is the hat matrix. consider the vector of residuals given by\n\nr = y \u2212 \u02c6y = y \u2212 hy = (i \u2212 h)y,\n\nwhere h is the projection matrix h = x(x t x)\u22121x t . the matrix h is called the hat\nmatrix because one has \u02c6y = hy; thus h maps \u02c6y into y. h is a projection matrix because\nit is symmetric and idempotent, that is, h 2 = h. it represents the projection of the observed\nvalues into the space spanned by h. the decomposition\n\ny = \u02c6y + y \u2212 \u02c6y = hy + (i \u2212 h)y\n\nis orthogonal because \u02c6yt (y \u2212 \u02c6y) = yt h(i \u2212 h)y = yt (h \u2212 h)y = 0. the covariance\nof r is easily derived by\n\ncov(r) = (i \u2212 h) cov(y)(i \u2212 h) = \u03c32(i \u2212 h \u2212 h + h 2) = \u03c32(i \u2212 h).\n\ntherefore one obtains with the diagonal elements from h = (hij) the variance var(ri) =\n\u03c32(1 \u2212 hii). scaling to the same variance produces the form\n\n\u02dcri =\n\nri\u221a\n1 \u2212 hii\n\n,\n\n "}, {"Page_number": 34, "text": "22\nwith var(\u02dcri) = \u03c32. if, in addition, one divides by the estimated variance \u02c6\u03c32 = (rt r)/(n\u2212 p\u2212\n1), where p + 1 is the length of xi, one obtains the studentized residual\n\nchapter 1. introduction\n\n\u2217\ni =\n\nr\n\n\u02dcri\n\u02c6\u03c3\n\n= yi \u2212 \u02c6\u03bci\n\u221a\n1 \u2212 hii\n\n\u02c6\u03c3\n\n,\n\nwhich behaves much like a student\u2019s t random variable except for the fact that the numerator\nand denominator are not independent.\n\nthe hat matrix itself is a helpful tool in diagnosis. from \u02c6y = hy it is seen that the element\nhij of the hat matrix h = (hij) shows the amount of leverage or influence exerted on \u02c6yi by\nyj. since h depends only on x, this influence is due to the \"design\" and not to the dependent\nvariable. the most interesting influence is that of yi on the fitted value \u02c6yi, which is reflected by\nthe diagonal element hii. for the projection matrix h one has\n\nn(cid:7)\n\nrank(h) =\n\nhii = p + 1\n\nand 0 \u2264 hii \u2264 1. therefore, (p + 1)/n is the average size of a diagonal element. as a rule of\nthumb, an x-point for which hii > 2(p + 1)/n holds is considered a high-leverage point (e.g.,\nhoaglin and welsch, 1978).\n\ni=1\n\ncase deletion as diagnostic tool\nin case deletion let the deletion of observation i be denoted by subscript (i). thus x (i) denotes\nthe matrix that is obtained from x by omitting the ith row; in \u03bc(i), y(i) the ith observation\ncomponent is also omitted. let \u02c6\u03b2(i) denote the least-squared estimate resulting from the re-\nduced dataset. the essential connection between the full dataset and the reduced set is given\nby\n\n\u22121 = (x t x)\n\n\u22121 + (x t x)\n\n(i)x (i))\n(1.2)\ni (x t x)\u22121xi is the diagonal element of h. one obtains after some computa-\n\ni (x t x)\n\n\u22121/(1 \u2212 hii),\n\n\u22121xixt\n\n(x t\n\nwhere hii = xt\ntion\n\n\u02c6\u03b2(i) = \u02c6\u03b2 \u2212 (x t x)\n\n\u22121xiri/(1 \u2212 hii).\n\nthus the change in \u03b2 that results if the ith observation is omitted may be measured by\n\n\u03b4i \u02c6\u03b2 = \u02c6\u03b2 \u2212 \u02c6\u03b2(i) = (x t x)\n\n\u22121xiri/(1 \u2212 hii).\n\nagain, the diagonal element of the hat matrix plays an important role. large values of hii yield\nvalues \u02c6\u03b2(i), which are distinctly different from \u02c6\u03b2.\n\nthe simple deletion residual is given by\n\nr(i) = yi \u2212 \u02c6\u03bc(i),\n\nwhere \u02c6\u03bc(i) = xt\ni \u03b2(i). it measures the deviation of yi from the value predicted by the model\nfitted to the remaining points and therefore reflects the accuracy of the prediction. from \u02c6\u03bc(i) =\n(i)x (i))\u22121xi = \u03c32(1 + h(i)), where h(i) =\ni \u03b2(i) one obtains var(r(i)) = \u03c32 + \u03c32xt\nxt\n(i)x (i))\u22121xi. it follows easily from equation (1.2) that h(i) is given by h(i) = hii/(1 \u2212\ni (x t\nxt\nhii). one obtains the standardized value\nr(i)(cid:16)\n\ni (x t\n\n\u02dcr(i) =\n\n,\n\n1 + h(i)\n\n "}, {"Page_number": 35, "text": "1.4. classical linear regression\n\n23\n(i)r(i)/(n \u2212 p \u2212 1) one obtains the studentized version\n\nwhich has variance \u03c32. with \u02c6\u03c32\n\n(i) = rt\n\u02dcr(i)\n\u02c6\u03c3(i)\n\n=\n\n(cid:16)\nyi \u2212 \u02c6\u03bc(i)\n\n.\n\nr\n\n\u02c6\u03c3(i)\n\n\u2217\n(i) =\n\n1 + h(i)\n\nthe studentized deletion residual is related to the studentized residual by r\n\n= yi \u2212 \u02c6\u03bci\n\u221a\n1 \u2212 hii\n\u02c6\u03c3(i)\ni (\u02c6\u03b2 \u2212 (x t x)\u22121xiri/(1 \u2212 hii)) =\n\u02c6\u03b2(i) = xt\nthe last transformation follows from \u02c6\u03bc(i) = xt\ni (x t x)\u22121xiri/(1 \u2212 hii) = \u02c6\u03bci \u2212 hiiri/(1 \u2212 hii) = \u02c6\u03bci \u2212 rih(i). therefore one has\n\u02c6\u03bci \u2212 xt\ni\nyi \u2212 \u02c6\u03bc(i) = (yi \u2212 \u02c6\u03bci)(1 + h(i)).\n\u2217\n\u221a\n(i) = ri\u02c6\u03c3/\u02c6\u03c3(i).\nit represents a standardization of the scaled residual (yi \u2212 \u02c6\u03bci)/\n1 \u2212 hi, which has variance \u02c6\u03c32\nby an estimate of \u03c32, which does not depend on the ith observation. therefore, when normality\nholds, the standardized case deletion residual is distributed as student\u2019s t with (n \u2212 p) degrees\n\u2217\ni as the studentized residuals with internal\nof freedom. cook and weisberg (1982) refer to r\n\u2217\ni \u2019s are also called cross-\nstudentization, in contrast to external studentization for r\nvalidatory or jackknife residuals. rawlings et al. (1998) used the term studentized residuals for\n\u2217\ni . for more details on residuals see cook and weisberg (1982).\nr\n\n\u2217\n(i). the r\n\nresiduals\n\n\u02c6\u03b2\n\n\u2212 xt\n\ni\n\nri = yi \u2212 \u02c6\u03bci = yi\ni = yi \u2212 \u02c6\u03bci\n\u221a\n1 \u2212 hii\n\n\u02c6\u03c3\n\n\u2217\n\nr\n\nsimple residual\n\nstudentized residual\n\ncase deletion residual\n\nr(i) = yi \u2212 \u02c6\u03bc(i) = yi \u2212 xt\n\ni\n\n\u02c6\u03b2(i)\n\nstudentized case deletion residual\n\n\u2217\n\n(i) = yi \u2212 \u02c6\u03bci\n1 \u2212 hii\n\n\u02c6\u03c3(i)\n\n\u221a\n\nr\n\n1.4.5 decomposition of variance and coefficient of determination\nthe sum of squared deviations from the mean may be partitioned in the following way:\n\nn(cid:7)\n\nn(cid:7)\n\nn(cid:7)\n\n(yi \u2212 \u00afy)2 =\n\n(\u02c6\u03bci \u2212 \u00afy)2 +\n\n(\u02c6\u03bci \u2212 yi)2,\n\n(1.3)\n\ni=1\n\ni=1\n\ni=1\n\ni yi/n is the mean over the responses. the partitioning has the form sst =\n\n(cid:14)\nn(cid:7)\n\nwhere \u00afy =\nssr + sse, where\n\nsst =\n\n(yi \u2212 \u00afy)2 is the total sum of squares, which represents the total variation in y\n\ni=1\n\nthat is to be explained by x-variables;\n\n "}, {"Page_number": 36, "text": "n(cid:7)\nn(cid:7)\n\ni=1\n\n24\n\n(\u02c6\u03bci \u2212 \u00afy)2 (r for regression) is the regression sum of squares built from the\n\nchapter 1. introduction\n\nssr =\n\nsquared deviations of the fitted values around the mean;\n\nsse =\n\n(\u02c6\u03bci\u2212yi)2 (e for error) is the sum of the squared residuals, also called the error\n\ni=1\n\nsum of squares.\n\nthe partitioning (c.3) may also be seen from a geometric view. the fitted model based on the\nleast-squares estimate \u02c6\u03b2 is given by\n\n\u02c6\u03bc = x \u02c6\u03b2 = x(x t x)\n\n\u22121x t y = hy,\n\nwhich represents a projection of y into the space span(x), which is spanned by the columns of\nx. since span(x) contains the vector 1 and projections are linear operators, one obtains with\npx denoting the projection into span(x) and \u00afyt = (\u00afy, . . . , \u00afy) the orthogonal decomposition\n\ny \u2212 \u00afy = \u02c6\u03bc \u2212 \u00afy + y \u2212 \u02c6\u03bc,\n\nwhere \u02c6\u03bc\u2212 \u00afy = hy \u2212 \u00afy is the projection of y \u2212 \u00afy into span(x) and y \u2212 \u02c6\u03bc = y \u2212 hy is from\nthe orthogonal complement of span(x) such that (y \u2212 \u02c6\u03bc)t (\u02c6\u03bc \u2212 \u00afy) = 0.\n\n(cid:14)\nthe coefficient of determination is defined by\n(cid:14)\ni(\u02c6\u03bci \u2212 \u00afy)2\ni(yi \u2212 \u00afy)2 = 1 \u2212\n\nssr\nsst\n\nr2 =\n\n=\n\n(cid:14)\n(cid:14)\ni(\u02c6\u03bci \u2212 yi)2\ni(yi \u2212 \u00afyi)2 .\n\nthus r2 gives the proportion of variation explained by the regression model and therefore is a\nmeasure for the adequacy of the linear regression model.\n\nfrom the definition it is seen that r2 is not defined for the trivial case where yi = \u00afy, i =\n\n1, . . . , n, which is excluded here. extreme values of r2 are\n\nr2 = 0 \u21d4 \u02c6\u03bci = \u00afy,\nr2 = 1 \u21d4 \u02c6\u03bci = yi, that is, all observations are on a line with slope unequal to 0.\n\nthat is, a horizontal line is fitted;\n\nalthough r2 is often considered as a measure of goodness-of-fit, it hardly reflects goodness-\nof-fit in the sense that a high value of r2 tells us that the underlying model is a linear regression\nmodel. though built from residuals, r2 compares the residuals of the model that specifies a\nlinear effect of variables x and the residuals of the simple intercept model (the null model).\nthus it measures the additional explanatory values of variable vector x within the linear model.\nit cannot be used to decide whether a model shows appropriate fit. rather, r2 and its gener-\nalizations measure the strength of association between covariates and the response variable. if\nr2 is large, the model should be useful since some aspect of the association between the re-\nsponse and covariates is captured by the linear model. on the other hand, if r2 is close to zero,\nthat does not mean that the model has a bad fit. on the contrary, if a horizontal line is fitted\ni(\u02c6\u03bci \u2212 yi)2\n(r2 = 0), the data may be very close to the fitted data, since any positive value\nis possible. r2 = 0 just means that there is no linear association between the response and\nthe linear predictor beyond the horizontal line. r2 tells how much of the variation is explained\nby the included variables within the linear approach. it is a relative measure that reflects the\nimprovement by the inclusion of predictors as compared to the simple model, where only the\nconstant term is included.\n\n(cid:14)\n\nnow we make some additional remarks to avoid misrepresentation. that r2 is not a tool\nto decide if the linear model is true or not may be easily seen from considering an underlying\n\n "}, {"Page_number": 37, "text": "1.4. classical linear regression\n\n25\n\nlinear model. let a finite number of observations be drawn from the range of x-values. now, in\naddition to the sample of size n, let n0 observations be drawn at a fixed design point x0. then,\nfor n0 \u2192 \u221e, it follows that \u00afy \u2192 \u03bc0 = e(y|x0) and \u02c6\u03bci \u2192 \u03bc0 for i (cid:13) n such that r2 \u2192 0.\nthis means that although the linear model is true, r2 approaches zero and therefore cannot be\na measure for the truth of the model. on the other hand, if a non-linear model is the underlying\nmodel and observations are only drawn at two distinct design points, r2 will approach 1 since\ntwo points may always be fitted by a line. the use of r2 is restricted to linear models. there\nare examples where r2 can be larger than 1 if a non-linear function is fitted by least squares\n(see exercise 1.4).\n\n1.4.6 testing in multiple linear regression\nthe most important tests are for the hypotheses\n\nh0 : \u03b21 = \u00b7\u00b7\u00b7 = \u03b2p = 0\n\nh1 : \u03b2i (cid:8)= 0 for at least one variable\n\n(1.4)\n\nand\n\nh0 : \u03b2i = 0\n\n(1.5)\nthe first null hypothesis asks if there is any explanatory value of the covariates, whereas the lat-\nter concerns the question if one specific variable may be omitted \u2013 given that all other variables\nare included. the test statistics may be easily derived as special cases of linear hypotheses (see\nnext section). for normally distributed responses one obtains for h0 : \u03b2j = 0\n\nh1 : \u03b2i\n\n(cid:8)= 0.\n\nt =\n\n\u02c6\u03b2j\n\n\u02c6cov( \u02c6\u03b2j)\n\n\u223c t(n \u2212 p \u2212 1),\n\n\u221a\n\najj with ajj denoting the jth diagonal element of (x t x)\u22121 and h0 is\nwhere \u02c6cov( \u02c6\u03b2j) = \u02c6\u03c3\nrejected if |t| > t1\u2212\u03b1/2(n \u2212 p \u2212 1). for h0 : \u03b21 = . . . = \u03b2p = 0 and normally distributed\nresponses one obtains\n\nf = n \u2212 p \u2212 1\n\np\n\nr2\n1 \u2212 r2 =\n\n(sst\u2212 sse)/p\nsse /(n \u2212 p \u2212 1)\n\nh0\u223c f (p, n \u2212 p \u2212 1)\n\nand h0 is rejected if f > f1\u2212\u03b1(p, n \u2212 p \u2212 1). the f -test for the global hypothesis h0 : \u03b21 =\n\u00b7\u00b7\u00b7 = \u03b2p = 0 is often given within an analysis-of-variance (anova) framework. consider\nagain the partitioning of the total sum of squares:\n\nsst = ssr + sse,\n\n(y \u2212 \u00afy)t (y \u2212 \u00afy) = (\u02c6\u03bc \u2212 \u00afy)t (\u02c6\u03bc \u2212 \u00afy) + (y \u2212 \u02c6\u03bc)t (y \u2212 \u02c6\u03bc).\n\nif the regression model holds, the error sum of squares has a scaled \u03c72-distribution, sse \u223c\n\u03c32\u03c72(n\u2212p\u22121). the degrees of freedom follow from considering the residuals r = y\u2212x \u02c6\u03b2 =\ny \u2212 hy = (i \u2212 h)y, which represent an orthogonal projection by use of the projection\nmatrix i \u2212 h. since sse is equivalent to the squared residuals, sse = rt r , and i \u2212 h has\nrank n \u2212 p \u2212 1, one obtains \u03c32\u03c72(n \u2212 p \u2212 1). if the regression model holds and in addition\n\u03b21 = \u00b7\u00b7\u00b7 = \u03b2p = 0, then one obtains for sse and ssr the \u03c72-distributions\n\nsst \u223c \u03c32\u03c72(n \u2212 1), ssr \u223c \u03c32\u03c72(p).\n\nin addition, in this case ssr and sse are independent. the corresponding means squares are\ngiven by\n\nmse = sse /(n \u2212 p \u2212 1), msr = ssr /p.\n\nit should be noted that while sse and ssr sum up to sst, the sum of mse and msr does not\ngive the average over all terms.\n\n "}, {"Page_number": 38, "text": "26\n\nchapter 1. introduction\n\ntable 1.4: anova table for multiple linear regression.\n\nsource of variation\n\nregression\n\nerror\n\nssr =\n\nsse =\n\nn\n\nss\n\n(cid:2)\ni=1(\u02c6yi \u2212 \u00afyi)2\n(cid:2)\ni=1(\u02c6\u03bci \u2212 yi)2\n\nn\n\ndf\n\nm s\n\np\n\nm sr = ssr\np\nn \u2212 (p + 1) m se = sse\nn\u2212p\u22121\n\nsubmodels and the testing of linear hypotheses\na general framework for testing all kinds of interesting hypotheses is the testing of linear hy-\npotheses given by\n\nh0 : c\u03b2 = \u03be\n\nh1 : c\u03b2 (cid:8)= \u03be.\n\u239e\nthe simple null hypothesis h0 : \u03b21 = \u03b22 = \u00b7\u00b7\u00b7 = \u03b2p, = 0 turns into\n\u239b\n\u239e\n\u239e\n\u239f\u239f\u239f\u239f\u23a0\n\u239c\u239c\u239c\u239d0\n\u239f\u239f\u239f\u23a0 =\n\u239f\u239f\u239f\u23a0 .\n\n\u239b\n\u239c\u239c\u239c\u239d\u03b20\n\n\u239b\n\u239c\u239c\u239c\u239c\u239d\n\nh0 :\n\n1\n\n\u03b21\n...\n\u03b2p\n\n0\n...\n0\n\n...\n\n1\n\n0 1\n...\n...\n0\n\nthe hypothesis h0 : \u03b2i = 0 is given by h0 : (0 . . . 1 . . . 0)\u03b2 = 0. comparisons of covariates\nof the form h0 : \u03b2i = \u03b2j are given by\n\nh0 : (0, . . . , 1, . . . ,\u22121, . . . 0)\u03b2 = 0,\n\nwhere 1 corresponds to \u03b2i and \u22121 corresponds to \u03b2j. hypotheses like h0 : \u03b21 = \u00b7\u00b7\u00b7 = \u03b2p = 0\nor h0 : \u03b2i = 0 are linear hypotheses, and the corresponding models may be seen as submodels\nof the multiple regression model. they are submodels because the parameter space is more\nrestricted than in the original multiple regression model.\nlet the more general \u02dcm be a submodel of m ( \u02dcm \u2282 m), where m is the unrestricted\nmultiple regression model and \u02dcm is restricted to a linear subspace of dimension (p + 1) \u2212 s,\nthat is, rank(c) = s. for example, if the restricted model contains only the intercept, one has\nrank(c) = 1 and the restricted model specifies a subspace of dimension one. let \u02c6\u03b2 denote the\nusual least-squares estimate for the multiple regression model and \u02dc\u03b2 be the restricted estimate\nthat minimizes\n\n(y \u2212 x\u03b2)t (y \u2212 x\u03b2)\n\nunder the restriction c\u03b2 = \u03be. using lagrange multipliers yields\n\u22121c t ]\n\n\u02dc\u03b2 = \u02c6\u03b2 \u2212 (x t x)\n\n\u22121c t [c(x t x)\n\n\u22121[c \u02c6\u03b2 \u2212 \u03be].\n\n(1.6)\n\none obtains two discrepancies, namely, the discrepancy between m and the data and the dis-\ncrepancy between \u02dcm and the data. as a discrepancy measure one may use a residual or error\nsums of squares:\n\nsse(m) =\n\nsse( \u02dcm) =\n\n\u2212 xt\n\ni\n\n(yi\n\n\u02c6\u03b2)2 = (y \u2212 x \u02c6\u03b2)t (y \u2212 x \u02c6\u03b2),\n\n\u2212 xt\n\ni\n\n(yi\n\n\u02dc\u03b2)2 = (y \u2212 x \u02dc\u03b2)t (y \u2212 x \u02dc\u03b2).\n\nn(cid:7)\nn(cid:7)\n\ni=1\n\ni=1\n\n "}, {"Page_number": 39, "text": "1.5. exercises\n\n27\n\nsince \u02dcm is a more restricted model, sse( \u02dcm) tends to be greater than sse(m). one may\ndecompose the discrepancy sse( \u02dcm) by considering\n\nsse( \u02dcm) = sse(m) + sse( \u02dcm|m),\n\n(1.7)\nwhere sse( \u02dcm|m) = sse( \u02dcm) \u2212 sse(m) is the increase of residuals that results from us-\ning the more restrictive model \u02dcm instead of m. it may also be seen as the amount of variation\nexplained by m but not by \u02dcm. the notation refers to the interpretation as a conditional discrep-\nancy; sse( \u02dcm|m) is the discrepancy of \u02dcm within model m, that is, the additional discrepancy\nbetween data and model. this results from fitting \u02dcm instead of the less restrictive model m.\nthe decomposition (1.7) may be used for testing the fit of \u02dcm given that m is an accepted\nmodel. this corresponds to testing h0 : c\u03b2 = \u03be (corresponding to \u02dcm) within the multiple\nregression model (corresponding to m).\n\nan important property of the decomposition (1.7) is that it is based on orthogonal compo-\n\nnents. behind (1.7) is the trivial decomposition\n\ny \u2212 x \u02dc\u03b2 = (x \u02c6\u03b2 \u2212 x \u02dc\u03b2) + (y \u2212 x \u02c6\u03b2),\n\n(1.8)\nwhere y \u2212 x \u02c6\u03b2 is orthogonal to x \u02c6\u03b2 \u2212 x \u02dc\u03b2, i.e. (y \u2212 x \u02c6\u03b2)t (x \u02c6\u03b2 \u2212 x \u02dc\u03b2) = 0. decomposi-\ntion (1.7) follows from (1.8) by considering sse( \u02dcm) = (y \u2212 x \u02dc\u03b2)t (y \u2212 x \u02dc\u03b2), sse(m) =\n(y \u2212 x \u02c6\u03b2)t (y \u2212 x \u02c6\u03b2). from (1.8) and (1.6) the explicit form of sse( \u02dcm|m) follows as\n\nsse( \u02dcm|m) = (c \u02c6\u03b2 \u2212 \u03be)t [c(x t x)\n\n\u22121c t ]\n\n\u22121(c \u02c6\u03b2 \u2212 \u03be).\n\nif m holds, sse(m) is \u03c72-distributed with sse(m)/\u03c32 \u223c \u03c72(n \u2212 p \u2212 1); if \u02dcm holds (h0 is\ntrue), sse( \u02dcm|m)/\u03c32 \u223c \u03c72(s) and sse(m) and sse( \u02dcm|m) are independent. one obtains\n\nsse( \u02dcm)\n\u03c32\u03c72(n \u2212 p \u2212 1 + s)\nif\n\n\u02dcm holds\n\n= sse( \u02dcm|m)\n\n\u03c32\u03c72(s)\nif\n\n\u02dcm holds\n\n+ sse(m)\n\n\u03c32\u03c7(n \u2212 p \u2212 1)\nif m holds\n\nthus, if \u02dcm holds,\n\nf =\n\n(cid:23)\n(cid:24)\nsse( \u02dcm) \u2212 sse(m)\n/s\nsse(m)/(n \u2212 p \u2212 1)\n\n\u223c f (s, n \u2212 p \u2212 1),\n\nwhich may be used as test statistics for h0 : c\u03b2 = \u03be. h0 is rejected if f is larger than the\n(1 \u2212 \u03b1)-quantile f1\u2212\u03b1(s, n \u2212 p \u2212 1).\n\n1.5 exercises\n\n1.1 consider a linear model that specifies the rent to pay as a function of the size of the flat and the city\n(with data for 10 cities available). let the model be given as\n\ne(y|size, c = i) = \u03b20 + size \u2217 \u03b2s + \u03b2c(i),\n\ni = 1, . . . , 10,\n\nwhere \u03b2c(i) represents the effect of the city. since the parameters \u03b2c(1), . . . , \u03b2c(10) are not identifiable,\none has to specify an additional constraint.\n\n(cid:2)\n\n(a) give the model with dummy variables by using the symmetric side constraint\n\n(b) give the model with dummy variables by specifying a reference category.\n\ni \u03b2c(i) = 0.\n\n "}, {"Page_number": 40, "text": "28\n\nchapter 1. introduction\n\n(d) specify c and \u03be of the linear hypothesis h0 : c\u03b2 = \u03be if you want to test if rent does not vary\n\nover cities.\n\n(e) what is the meaning if the hypothesis h0 : \u03b2c(j) = 0 for fixed j holds?\n(f) find the transformation that transforms parameters with a reference category into parameters with\n\na symmetric side constraint, and vice versa for a general number of cities k.\n\n1.2 the r package catdata provides the dataset rent.\n(a) use descriptive tools to learn about the data.\n(b) fit a linear regression model with response rent (net rent in euro) and explanatory variables size\n\n(size in square meters) and rooms (number of rooms). discuss the results.\n\n(c) fit a linear regression model with response rent and the single explanatory variable rooms. com-\n\npare with the results from (b) and explain why the coefficients differ even in sign.\n\n1.3 the dataset rent from r package catdata contains various explanatory variables.\n\n(a) use the available explanatory variables when fitting a linear regression model with the response\nrent. include polynomial terms and dummy variables if necessary. evaluate if explanatory variables\ncan be excluded.\n\n(b) fit a linear model with the response rentm (rent per square meter) by using the available explanatory\n\nvariables. discuss the effects and compare to the results from (a).\n\n1.4 kockelkorn (2000) considers the model yi = \u03bc(xi) + \u03b5i with \u03bc(x) = x\u03b2 if x \u2265 0 and \u03bc(x) =\n\u2212(\u2212x)\u03b2 if x < 0. for some z > 0 let observations (yi, xi) be given by {(0, 1), (0,\u22121), (\u2212z3,\u2212z),\n(z3, z)}.\n\n(a) compute the value \u03b2 that minimizes the least-squares criterion.\n(b) compute r2 as a function of z and investigate what values r2 takes.\n\n "}, {"Page_number": 41, "text": "chapter 2\n\nbinary regression: the logit model\n\ncategorical regression has the same objectives as metric regression. it aims at an economic\nrepresentation of the link between covariables considered as the independent variables and the\nresponse as the dependent variable. moreover, one wants to evaluate the influence of the in-\ndependent variables regarding their strength and the way they exert their influence. predicting\nnew observations can be based on adequate modeling of the response pattern.\n\ncategorical regression modeling differs from classical normal regression in several ways.\nthe most crucial difference is that the dependent variable y follows a quite different distribu-\ntion. a categorical response variable can take only a limited number of values, in contrast to\nnormally distributed variables, in which any value might be observed. in the simplest case of\nbinary regression the response takes only two values, usually coded as y = 0 and y = 1. one\nconsequence is that the scatterplots look different. figure 2.1 shows data from the household\npanel described in example 1.2. the outcomes \"car in household\" (y = 1) and \"no car in\nhousehold\" (y = 0) are plotted against net income (in euros). it is seen that for low income the\nresponses y = 0 occur more often, whereas for higher income y = 1 is observed more often.\nhowever, the structural connection between the response and the covariate is hardly seen from\nthis representation. therefore, in figure 2.1 the relative frequencies for owning a car are shown\nfor households within intervals of length 50. the picture shows that a linear connection is cer-\ntainly not the best choice. this leads to the second difference, which concerns the link between\nthe covariates and the mean of the response. although the covariates might enter the model as\na linear term in the same way as in classical regression models, the link between response and\nlinearly structured explanatory variables usually has to be modified to avoid improper models.\nthus at least two structuring elements have to be modified: the distribution of the response and\nthe link between the response and the explanatory variables.\n\nin the following, first distributions of y are considered and then methods for structuring the\nlink between the response and the independent variables are outlined. the chapter introduces\nthe concept of models for binary responses. estimation and inference are considered in chapter\n4, and extensions are given in chapter 5.\n\n2.1 distribution models for binary responses and basic\n\nconcepts\n\n2.1.1 single binary variables\nlet the binary response y be coded by y = 1 and y = 0. in example 1.2, the two outcomes\nrefer to \u201ccar in household\u201d and \u201cno car in household\u201d. often y = 1 is considered as success\n\n29\n\n "}, {"Page_number": 42, "text": "30\n\nchapter 2. binary regression: the logit model\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n0\n\n.\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n0\n1\n\n.\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n0\n\n.\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\ni\n\ns\ne\nc\nn\ne\nu\nq\ne\nr\nf\n \n\ne\nv\ni\nt\n\nl\n\na\ne\nr\n\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|||\n| | |\n|\n|\n||\n|\n|\n|||\n|\n||\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|||\n| |\n| |\n| | |\n|\n|\n|\n||\n|\n|\n| |\n|\n|\n|\n||\n|\n|\n|\n|\n|\n||\n|\n||\n|\n|\n|\n|\n|| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n||\n|\n| | |\n|\n|\n|||\n||\n| |\n|\n|\n||\n|\n|\n|||\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|| |\n|\n|\n|\n|\n| |\n|\n||\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|||\n|\n|\n|\n|\n| |\n|\n| |\n|\n|||\n|\n|| | |\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n| |\n|\n|\n|\n|||\n|\n|\n|\n|\n||\n|\n| |\n|\n|\n|\n|| |\n|\n| |\n|\n| |\n|\n|\n|\n|\n|\n|\n| |\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n| ||\n|\n|\n| |\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|||\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|||\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n||\n| |\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n||\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n| | |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n| |\n| ||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n||\n|\n|\n|\n|\n|\n| ||\n|\n|\n|||\n||\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n| |\n|\n||\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n| |\n||\n|\n|\n|\n|\n|| |\n|\n| |\n|\n|\n||\n|\n|\n|\n| |\n|\n||\n|\n|\n|\n|\n|\n|\n|\n| | |\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| ||\n|\n|||\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n||\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n| |\n| |\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|||\n||\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n||\n||\n|\n|\n| |\n|\n||\n|\n|\n|||\n|\n||\n| ||\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n| |\n|||||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||||\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n| |\n| |\n|\n|\n| |\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n| |||\n|\n| | |\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n||\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||||\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n| |\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|||\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n| |\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|| |\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n| |\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n| ||\n|\n|\n||\n||\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n| ||\n|\n|\n|\n|\n||\n| |\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n||||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n| |\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n||\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|||||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n| |\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n| |\n| |\n|\n|\n| | |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n| |\n||\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n||\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| | |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n| |\n|\n|\n||| |\n|\n| |\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|| |\n||\n|\n||\n|\n|\n|\n|\n|\n|| |\n|\n| |\n|\n|||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n| |\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n\n|\n|\n|\n|\n|\n|\n|\n|\n|\n\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n\n|\n\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| ||\n|\n|\n|\n| |\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n| |\n|\n||\n|\n|\n||\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n| |\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n| |\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n| |\n|\n|\n|\n|\n||\n|\n||\n|\n|\n|\n|\n|||\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n| |\n|\n|\n| ||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| ||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|||\n|\n|\n|\n|\n|\n| |\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|| |\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n\n|\n|\n\n|\n|\n|\n|\n|\n|\n\n|\n\n|\n\n|\n|\n\n|\n|\n|\n\n|\n\n|\n\n0\n\n1000\n\n2000\n\n3000\n\n4000\n\nnetincome\n\n*\n***\n*\n*\n*\n*\n**\n*\n*\n*****\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n****\n*\n*\n*\n*\n*\n\n*\n\n******\n*\n\n******************\n****\n*\n\n***\n\n*\n\n*\n\n*\n\n*\n*\n******\n*\n*\n**\n*****\n*\n*\n*\n*\n*\n*\n*\n***\n*\n*\n*\n*\n*\n*\n\n*\n*\n\n*\n\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n\n*\n\n*\n\n*\n\n*\n\n*\n\n*\n\n*\n\n*\n*\n*\n*\n**\n*\n*\n\n*\n*\n*\n*\n*\n*\n\n0\n\n1000\n\n2000\n\n3000\n\n4000\n\nnetincome\n\nfigure 2.1: car data: upper panel shows the raw data with y = 1 for \u201ccar in household\u201d\nand y = 0 for \u201cno car in household\u201d plotted against net income in euros. lower panel\nshows the relative frequencies within intervals of length 50, plotted against net income.\n\nand y = 0 as failure, a convention that will be used in the following. the distribution of the\nsimple binary random variable y \u2208 {0, 1} is completely characterized by the probability\n\n\u03c0 = p (y = 1).\n\nthe probability for y = 0 is then given by p (y = 0) = 1 \u2212 \u03c0. the mean of y is simply\ncomputed by\n\ne(y) = 1 \u00d7 \u03c0 + 0 \u00d7 (1 \u2212 \u03c0) = \u03c0.\n\ntherefore, the response probability \u03c0 represents the mean of the binary distribution. the vari-\nance is computed equally simple by\n\nvar(y) = e(y \u2212 e(y))2 = (1 \u2212 \u03c0)2\u03c0 + (0 \u2212 \u03c0)2(1 \u2212 \u03c0) = \u03c0(1 \u2212 \u03c0).\n\nit is seen that the variance is completely determined by \u03c0 and depends on \u03c0 with minimal value\nzero at \u03c0 = 0 and \u03c0 = 1 and maximal value at \u03c0 = 1/2. this is in accordance with intuition;\nif \u03c0 = 0, only y = 0 can be observed; consequently, the variance is zero since there is no\nvariability in the responses. the same holds for \u03c0 = 1, where only y = 1 can be observed.\n\n "}, {"Page_number": 43, "text": "2.1. distribution models for binary responses and basic concepts\n\n31\n\n2.1.2 the binomial distribution\nin many applications a binary variable is observed repeatedly and the focus is on the number\nof successes (occurrence of y = 1). the classical example is the flipping of a coin n times\nand then counting the number of trials where heads came up. more interestingly, the trials may\nrefer to a standardized treatment of n persons and the outcome is the number of persons for\nwhom treatment was successful. the same data structure is found if in example 1.2 income\nis measured in categories, where categories refer to intervals of length 50. considering the\nhouseholds within one interval as having the same response distribution, one has repeated trials\nwith n denoting the number of households within a specific interval.\nthe basic assumptions underlying the binomial distribution are that the random variables\ny1, . . . , yn with fixed n are binary, yi \u2208 {0, 1} with the same response probability \u03c0 = p (yi =\n1), i = 1, . . . , n, and are independent. then the number of successes y = y1 + \u00b7\u00b7\u00b7 + yn in n\ntrials is called a binomial random variable, y \u223c b(n, \u03c0), and has the distribution function\n\n(cid:26)\n\u03c0r(1 \u2212 \u03c0)n\u2212r,\n\n(cid:25)\n\nn\nr\n\np (y = r) =\n\nr = 1, . . . , n.\n\nthe mean and variances of a binomial variable are easily calculated. one obtains\n\ne(y) = n\u03c0, var(y) = n\u03c0(1 \u2212 \u03c0).\n\nthe random variable y counts the number of successes in n trials. often it is useful to look at\nthe relative frequencies or proportions y/n rather than at the number of successes y. the form\nof the distribution remains the same; only the support changes since for n trials y takes values\nfrom {0, 1, . . . , n}, whereas y/n takes values from {0, 1/n, . . . , 1}. the probability function\nof y/n is given by\n\n(cid:25)\n\n(cid:26)\n\nn\nnz\n\np (y/n = z) =\n\n\u03c0nz(1 \u2212 \u03c0)n\u2212nz,\n\nwhere z \u2208 {0, 1/n, . . . (n \u2212 1)/n, 1}. the distribution of y/n is called a scaled binomial\ndistribution, frequently abbreviated by y/n \u223c b(n, \u03c0)/n. one obtains\n\ne(y/n) = \u03c0, var(y/n) = \u03c0(1 \u2212 \u03c0)/n.\n\nit is seen that y/n has mean \u03c0. therefore, the relative frequency is a natural and unbiased\nestimate for the underlying \u03c0. the variance of the estimate y/n depends on \u03c0 and n with\nlarge values for \u03c0 close to 0.5 and small values if \u03c0 approaches 0 or 1. generally the variance\ndecreases with increasing n. it is noteworthy that in figure 2.1 the estimates vary in their degree\nof trustworthiness since the sample sizes and underlying probabilities vary across intervals.\n\nodds, logits, and odds ratios\na binary response variable is distinctly different from a continuous response variable. while the\nessential characteristics of a continuous response variable are the mean \u03bc = e(y), the variance\n\u03c32 = var(y), skewness, and other measures, which frequently may vary independently, in a\nbinary response variable all these characteristics are determined by only one value, which often\nis chosen as the probability of y = 1. instead of using \u03c0 as an indicator of the random behavior\nof the response, it is often useful to consider some transformation of \u03c0. of particular importance\nare odds and log-odds (also called logits), which are functions of \u03c0.\n\n "}, {"Page_number": 44, "text": "32\n\nchapter 2. binary regression: the logit model\n\nodds\n\u03b3(\u03c0) = \u03c0\n1\u2212\u03c0\n\nlog odds or logits\nlogit (\u03c0) = log(\u03b3(\u03c0)) = log( \u03c0\n\n1\u2212\u03c0 )\n\nthe odds \u03b3(\u03c0) = \u03c0/(1 \u2212 \u03c0) are a directed measure that compares the probability of the oc-\ncurrence of y = 1 and the probability of the occurrence of y = 0. if y = 1 is considered a\n\"success\" and y = 0 a \"failure\", a value \u03b3 = 1/4 means that failure is four times as likely as a\nsuccess. while \u03b3 compares y = 1 to y = 0, the inverse compares y = 0 to y = 1, yielding\n\n\u03b3 = p (y = 1)\np (y = 0) ,\n\n1\n\u03b3\n\n= p (y = 0)\np (y = 1) .\n\ntherefore, if odds are considered as functions of \u03c0, one obtains\n\n\u03b3(1 \u2212 \u03c0) =\n\n1 \u2212 \u03c0\n\u03c0\n\n=\n\n1\n\n\u03b3(\u03c0)\n\nbecause \u03b3(1 \u2212 \u03c0) corresponds to comparing y = 0 to y = 1. odds fulfill\n\nfor the log-odds the relation is additive:\n\n\u03b3(\u03c0)\u03b3(1 \u2212 \u03c0) = 1.\n(cid:26)\n\n(cid:25)\n\n1 \u2212 \u03c0\n\u03c0\n\nlogit(1 \u2212 \u03c0) = log\n\n= \u2212 logit(\u03c0),\n\nand therefore\n\nlogit(\u03c0) + logit(1 \u2212 \u03c0) = 0.\n\ncomparing two groups\nthe concept of odds and log-odds may be illustrated by the simple case of comparing two\ngroups on a binary response. the groups can be two treatment groups \u2013 a treatment group\nand a control group \u2013 in a clinical trial or correspond to two populations, for example, men\nand women. the data are usually collected in a (2\u00d72)-contingency table with the underlying\nprobabilities given in the following table.\n\n1\n\u03c01\n\u03c02\n\n1\n2\n\n0\n\ny\n1 \u2212 \u03c01\n1 \u2212 \u03c02\n\n1\n1\n\nin the table \u03c0t = p (y = 1|t = t), t = 1, 2, denotes the probability of response y = 1,\ncorresponding to success. the probability for failure is then determined by p (y = 0|t = t) =\n1\u2212 p (y = 1|t = t). a comparison of the two groups may be based on various measures. one\nmay use the difference of success probabilities:\n\nwhich implies the difference of failures, since p (y = 0|t = 1) \u2212 p (y = 0|t = 2) = \u03c02 \u2212 \u03c01.\ngroups are equivalent with respect to the response if the difference is zero. an alternative\nmeasure is the proportion, frequently called the relative risk:\n\nd12 = \u03c01 \u2212 \u03c02,\n\nr12 = \u03c01/\u03c02,\n\n "}, {"Page_number": 45, "text": "2.2. linking response and explanatory variables\n\n33\n\nwhich has an easy interpretation. a relative risk of 2 means that the probability of success in\ngroup 1 is twice the probability of success in group 2. there is no difference between groups\nwhen the relative risk is 1.\n\nanother quite attractive measure compares the odds rather than the probabilities. the odds\n\nratio between groups 1 and 2 is\n\n\u03b312 = \u03c01/(1 \u2212 \u03c01)\n\u03c02/(1 \u2212 \u03c02)\n\n= \u03b3(\u03c01)\n\u03b3(\u03c02) .\n\nthe ratio of the odds \u03b3(\u03c01) and \u03b3(\u03c02) compares odds instead of ratios of probabilities of suc-\ncess. the odds ratio \u03b321 between groups 2 and 1, \u03b321 = \u03b3(\u03c02)/\u03b3(\u03c01), is simply the inverse,\n\u03b321 = 1/\u03b312. groups are equivalent as far as the response is concerned if \u03b321 = \u03b312 = 1. the\nodds ratio also measures the association between rows and columns, which is the association\nbetween the grouping variable and the response.\n\nrather than measuring association by odds ratios, one can use the log-transformed odds\n\nratios\n\nlog(\u03b312) = log\n\n(cid:25)\n\n(cid:26)\n\n.\n\n\u03b3(\u03c01)\n\u03b3(\u03c02)\n\nwhile odds ratios can equal any non-negative number, log-odds ratios have the advantage that\nthey are not restricted at all. a value of 0 means that the groups are equivalent.\n\nregression modeling in the case of two groups means modeling the response y as a function\nof a binary predictor. the models for \u03c0t have to distinguish between t = 1 and t = 2. a\nmodel that uses the probabilities itself has the form\n\n\u03c01 = \u03b20 + \u03b2,\n\n\u03c02 = \u03b20,\n\nand the effect \u03b2 is equivalent to the difference in probabilities. this means in particular that the\neffect is restricted to the interval [\u22121, 1]. a more attractive model is the linear model for the\nlog-odds which has the form\n\nlog(\u03c02/(1 \u2212 \u03c02)) = \u03b20,\n\n(2.1)\n\nand one easily derives that \u03b2 is equal to the log-odds ratio and exp(\u03b2) is equal to the odds ratio:\n\nlog (\u03c01/(1 \u2212 \u03c01)) = \u03b20 + \u03b2,\n(cid:26)\n\n(cid:25)\n\n\u03b2 = log(\u03b312) = log\n\n\u03b3(\u03c01)\n\u03b3(\u03c02)\n\n,\n\nexp(\u03b2) = \u03b321 = \u03b3(\u03c02)\n\u03b3(\u03c01) .\n\ntherefore, parameters correspond to common measures of association. the value \u03b2 = 0, which\nmeans that no effect is present, corresponds to the case of no association, where the odds ratio\nis 1 and the log odds ratio is 0. since model (2.1) parameterizes the logits, it is called the logit\nmodel, a model that will be considered extensively in the following. model (2.1) is just the\nsimplest version of a logit model where only one binary predictor is included.\n\n2.2 linking response and explanatory variables\n2.2.1 deficiencies of linear models\nlet (yi, xi), i = 1, . . . , n, be observations with binary response yi \u2208 {0, 1} on a covariate\nvector xi. one could try to link yi and xi by using the classical regression model\n\nyi = \u03b20 + xt\n\ni \u03b2 + \u03b5i,\n\n(2.2)\n\n "}, {"Page_number": 46, "text": "34\n\nchapter 2. binary regression: the logit model\n\nwhere yi is the response given xi and \u03b5i represents the noise with e(\u03b5i) = 0. several problems\narise from this approach. first, the structural component e(yi) = \u03b20 + xt\ni \u03b2, which specifies\nthe assumed dependence of the mean of y on the covariates xi, is given by\n\n\u03c0i = \u03b20 + xt\n\ni \u03b2,\n\nwhere \u03c0i = p (yi = 1|xi). thus the range where a model of this type may hold is restricted\nseverely. since \u03c0i \u2208 [0, 1] even for the simplest model with univariate predictor \u03c0i = \u03b20 + xi\u03b2,\nit is easy to find regressor values xi such that \u03c0i > 1 or \u03c0i < 0 if \u03b2 (cid:8)= 0. the second\nproblem concerns the random component of the model. from \u03c0i = \u03b20 + xt\ni \u03b2 it follows\nthat \u03b5i takes only two values, \u03b5i \u2208 {1 \u2212 \u03c0i,\u2212\u03c0i}. moreover, the variance of \u03b5i is given by\nvar(\u03b5i) = var(yi) = \u03c0i(1\u2212 \u03c0i), which is the usual variance of a binary variable. therefore the\nmodel is heteroscedastic, with the variance being directly connected to the mean. in summary,\nthe usual linear regression model that assumes homogenous variances, continuous noise, and\nlinear influence for all values of covariates is unsatisfactory in several ways. nevertheless, as is\nshown in the next section, one may obtain a binary regression model by using the continuous\nregression model as a background model.\n\n2.2.2 modeling binary responses\nin the following we show two ways to obtain more appropriate models for binary responses.\nthe first gives some motivation for a model by considering underlying continuous responses\nthat yield a rather general family of models. the second is based on conditional normal distri-\nbution models that yield a more specific model.\n\nbinary responses as dichotomized latent variables\nbinary regression models can be motivated by assuming that a linear regression model holds\nfor a continuous variable that underlies the binary response.\nin many applications one can\nimagine that an underlying continuous variable steers the decision process that results in a\ncategorical outcome. for example, the decision to buy a car certainly depends on the wealth of\nthe household. if the wealth exceeds a certain threshold, a car is bought (yi = 1); if not, one\nobserves yi = 0. in a bioassay, where yi = 1 stands for death of an animal depending on the\ndosage of some poison, the underlying variable that determines the response may represent the\ndamage on vital bodily functions.\ni \u03b2 + \u03b5i holds for a\nlatent response \u02dcyi. moreover, let \u2212\u03b5i have the distribution function f , which does not depend\non xi. the essential concept is to consider yi as a dichotomized version of the latent variable\n\u02dcyi with the link between the observable variable yi and the latent variable \u02dcyi given by\n\nto formalize the concept, let us assume that the model \u02dcyi = \u03b30 + xt\n\nyi = 1\n\nif\n\n\u02dcyi \u2265 \u03b8,\n\nwhere \u03b8 is some unknown threshold. one obtains\n\n\u03c0i = \u03c0(xi) = p (yi = 1|xi) = p (\u02dcyi \u2265 \u03b8) = p (\u03b30 + xt\n\ni \u03b2 + \u03b5i \u2265 \u03b8)\n\n= p (\u2212\u03b5i \u2264 \u03b30 \u2212 \u03b8 + xt\n\ni \u03b2),\nwhere \u03b20 = \u03b30 \u2212 \u03b8. the resulting model has the simple form\ni \u03b2).\n\n\u03c0(xi) = f (\u03b20 + xt\n\ni \u03b2) = f (\u03b20 + xt\n\n(2.3)\n\n(2.4)\n\n "}, {"Page_number": 47, "text": "2.2. linking response and explanatory variables\n\n35\n\nin this model the mean of the response is still determined by a term that is linear in xi but the\nconnection between \u03c0i and the linear term \u03b7i = \u03b20 + xt\ni \u03b2 involves some transformation that\nis determined by the distribution function f .\n\nthe basic assumption behind the derivation of model (2.4) is that the observable variable\nyi is a coarser, even binary, version of the latent variable \u02dcyi. it is important that the derivation\nfrom latent variables be seen as a mere motivation for the binary response models. the resulting\nmodel and parameters may be interpreted without reference to any latent variable. this is also\nseen from the handling of the threshold \u03b8. since it is never observed and is not identifiable,\nit vanishes in the parameter \u03b20 = \u03b30 \u2212 \u03b8. a point in favor of the derivation from the latent\ni \u03b2) is always from the admissible range \u03c0i \u2208 [0, 1] because f\nmodel is that \u03c0i = f (\u03b20 + xt\nis a distribution function for which f (\u03b7) \u2208 [0, 1] always holds. a simple example is the probit\nmodel,\n\n\u03c0(xi) = \u03c6(\u03b20 + xt\n\ni \u03b2),\n\nwhere \u03c6 is the distribution function of the standardized normal distribution n(0, 1). the more\nwidely used model is the logit model,\n\n\u03c0(xi) = f (\u03b20 + xt\n\ni \u03b2),\n\nwhere f is the logistic distribution function f (\u03b7) = exp(\u03b7)/(1 + exp(\u03b7)). an alternative\nderivation of the logit model is given in the following.\n\nmodeling the common distribution of a binary and a\ncontinuous distribution\nthe motivation by latent variables yields the very general class of models (2.4), where f may\nbe any continuous distribution function. models of this type will be considered in chapter 5.\nhere we want to motivate the choice of a special distribution by considering a model for the\ntotal distribution of y, x.\nlet us consider the bivariate random variable (y, x), where y \u2208 {0, 1} is a binary variable\nand x is a vector of continuous random variables. the most common continuous distribution\nis the normal distribution. but, rather than assuming x to be normally distributed in the total\npopulation, let us assume that x is conditionally normally distributed within the groups that are\ndetermined by y = 0 and y = 1. one assumes\n\nx|y = r \u223c n(\u03bcr, \u03c3),\n\nwhere the covariance matrix \u03c3 is the same for all conditions y = r. simple derivations based\non bayes\u2019 theorem shows that y|x is determined by\n\n\u03c0(x) = p (y = 1|x) =\n\nwhere\n\nexp(\u03b20 + xt \u03b2)\n1 + exp(\u03b20 + xt \u03b2) ,\n\n(cid:25)\n\n\u03b20 = \u22121\n(cid:3)\n1\u03c3\n2 \u03bc\n\u22121(\u03bc1\n\u03b2 = \u03c3\n\n\u22121\u03bc1 +\n\u2212 \u03bc0).\n\n1\n2 \u03bc0\u03c3\n\n\u22121\u03bc0 + log\n\np (y = 1)\np (y = 0)\n\n(2.5)\n\n(cid:26)\n\n,\n\nit is seen that model (2.5) is of the general form (2.4), with f being the logistic distribution\nfunction f (\u03b7) = exp(\u03b7)/(1 + exp(\u03b7)). a more general form is considered in exercise 2.1.\n\n "}, {"Page_number": 48, "text": "36\n\nchapter 2. binary regression: the logit model\n\n1\n\n0\n\n1\n\n0\n\n\u22123\n\n\u22122\n\n\u22121\n\n0\n\n1\n\n2\n\n3\n\n\u22123\n\n\u22122\n\n\u22121\n\n0\n\n1\n\n2\n\n3\n\nfigure 2.2: logistic regression model resulting from conditionally normally distributed\nx|y = i \u223c n(\u00b11, \u03c32), \u03c32 = 0.16 (upper panel), \u03c32 = 1 (lower panel).\n\nto illustrate the functional form that is specified by the logistic model, let us consider the\nsimple case of a one-dimensional variable x. the derivation from the normal distribution is\nillustrated in figure 2.2. let x be normally distributed with x|y = 0 \u223c n(\u22121, \u03c32), x|y =\n1 \u223c n(1, \u03c32); then one obtains \u03b2 = 2/\u03c32. it is seen from figure 2.2 how the response is\ndetermined by the variance within the subpopulations. the resulting logistic response function\nis rather flat for large \u03c3(\u03c3 = 1) and distinctly steeper for smaller \u03c3(\u03c3 = 0.4). thus, if the\ngroups determined by y = 0 and y = 1 are well separated by the covariate x (small \u03c3), the\nresponse \u03c0(x) varies strongly with x.\n\nbasic form of binary regression models\n\nin structured regression models one often distinguishes between two components, the struc-\ntural component and the random component. a general form of the structural component is\n\u03bc = h(\u03b7(x)), where \u03bc denotes the mean, h is a transformation function, and \u03b7(x) denotes a\nstructured predictor. in the case of a binary variable, the structural component that specifies the\nmean response has exactly this form, given by\n\n\u03c0i = f (\u03b20 + xt\n\ni \u03b2),\n\n(2.6)\n\nwith the linear predictor \u03b7(xi) = \u03b20 + xt\ni \u03b2 and the distribution function f . for a binary\nresponse yi, the random component that describes the variation of the response is already spec-\nified by (2.6). the distribution of yi is determined uniquely if \u03c0i is fixed. in particular, the\nvariance is a simple function of \u03c0i, since var(yi) = \u03c0i(1 \u2212 \u03c0i).\n\n "}, {"Page_number": 49, "text": "2.3. the logit model\n\n37\n\n2.3 the logit model\nin the following we consider the logit model, which is the most widely used model in binary\nregression. alternative models will be considered in chapter 5.\n\n2.3.1 model representations\nin the previous section the logit model was derived from the assumption that covariates are\nnormally distributed given the response categories. collecting the predictors in x, where an\nintercept is included, the basic form is given by\n\n\u03c0(x) = f (\u03b7(x)),\n\nwith f (\u03b7) = exp(\u03b7)/(1 + exp(\u03b7)) and the linear predictor \u03b7(x) = xt \u03b2. simple derivation\nshows that the following equivalent representations of the model hold.\n\nbinary logit regression model\n\n\u03c0(x) =\n\nexp(xt \u03b2)\n\n1 + exp(xt \u03b2)\n\nor\n\nor\n\n\u03c0(x)\n(cid:25)\n1 \u2212 \u03c0(x)\n\n(cid:26)\n\n= exp(xt \u03b2)\n\nlog\n\n\u03c0(x)\n1 \u2212 \u03c0(x)\n\n= xt \u03b2\n\nthe different forms of the model focus on different aspects of the dependence of the re-\nsponse on the covariates. the first form shows how the response probability \u03c0(x) is determined\nby the covariates x. the second form shows the dependence of the odds \u03c0(x)/(1 \u2212 \u03c0(x))\non the covariates. the third form shows that in the case of the logit model, the logits log\n(\u03c0(x))/(1 \u2212 \u03c0(x)) depend linearly on the covariates.\n\n2.3.2 logit model with continuous predictor\nin the following we consider first properties of the model for the simple case of univariate pre-\ndictors. the logistic regression model for a one-dimensional covariate x postulates a monotone\nassociation between \u03c0(x) = p (y = 1|x) and a covariate x of the form\n\n\u03c0(x) =\n\nexp(\u03b20 + x\u03b2)\n1 + exp(\u03b20 + x\u03b2) ,\n\n(2.7)\n\nwhere \u03b20, \u03b2 are unknown parameters. the function that determines the form of the response is\nthe logistic distribution function f (\u03b7) = exp(\u03b7)/(1 + exp(\u03b7)). the functional form of f is\ngiven in figure 2.3, where it is seen that f is strictly monotone with f (0) = 0.5. in addition,\nf is symmetric, that is, f (\u03b7) = 1 \u2212 f (\u2212\u03b7) holds for all \u03b7. f transforms the linear predictor\n\u03b20 + x\u03b2 such that \u03c0(x) is between 0 and 1 and \u03c0(x) is monotone in x.\nfrom \u03c0(x) = f (\u03b20 + x\u03b2) one obtains that \u03c0(x) = 0.5 if \u03b20 + x\u03b2 = 0 or, equivalently,\nif x = \u2212\u03b20/\u03b2. thus x = \u2212\u03b20/\u03b2 may be used as an anchor point on the x-scale where\n\n "}, {"Page_number": 50, "text": "38\n\nchapter 2. binary regression: the logit model\n\n0\n\n.\n\n1\n\n8\n0\n\n.\n\n6\n0\n\n.\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n0\n\n.\n\n\u22126\n\n\u22124\n\n\u22122\n\n0\n\n2\n\n4\n\n6\n\nfigure 2.3: logistic distribution function f (\u03b7) = exp(\u03b7)/(1 + exp(\u03b7)).\n\n\u03c0(x) = 0.5. the slope of the function \u03c0(x) is essentially determined by \u03b2. if \u03b2 is large and\npositive, the probability increases strongly for increasing x. if \u03b2 is negative, it decreases with\nthe decrease being stronger if \u03b2 is very small. figure 2.4 shows the function \u03c0(x) for several\nvalues of \u03b2.\n\n\u22126\n\n\u22124\n\n\u22122\n\n0\n\n2\n\n4\n\n6\n\n0\n\n.\n\n1\n\n8\n0\n\n.\n\n6\n\n.\n\n0\n\n4\n0\n\n.\n\n2\n0\n\n.\n\n0\n0\n\n.\n\n0\n.\n1\n\n8\n.\n0\n\n6\n.\n0\n\n4\n.\n0\n\n2\n.\n0\n\n0\n.\n0\n\n\u22126\n\n\u22124\n\n\u22122\n\n0\n\n2\n\n4\n\n6\n\nfigure 2.4: response curve \u03c0(x) = f (\u03b20 + \u03b2x) for \u03b20 = 0 and \u03b2 = 0.4, \u03b2 = 1,\n\u03b2 = 2 (top) and for \u03b20 = 0 and \u03b2 = \u22120.4, \u03b2 = \u22121, \u03b2 = \u22122 (bottom).\n\n "}, {"Page_number": 51, "text": "2.3. the logit model\n\ni\ni\n\ns\ns\ne\ne\nc\nc\nn\nn\ne\ne\nu\nu\nq\nq\ne\ne\nr\nr\nf\nf\n \n \ne\ne\nv\nv\ni\ni\nt\nt\na\na\ne\ne\nr\nr\n\nl\nl\n\n0\n0\n.\n.\n1\n1\n\n8\n8\n.\n.\n0\n0\n\n6\n6\n.\n.\n0\n0\n\n4\n4\n.\n.\n0\n0\n\n2\n2\n.\n.\n0\n0\n\n0\n0\n.\n.\n0\n0\n\n39\n\n***\n\n*\n***\n*\n*\n*\n*\n**\n*\n*****\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n****\n*\n*\n*\n*\n*\n\n*\n\n******\n*\n\n******************\n****\n*\n\n*\n\n*\n\n*\n\n*\n*\n******\n*\n*\n**\n*****\n*\n*\n*\n*\n*\n*\n*\n***\n*\n*\n*\n*\n*\n*\n\n*\n*\n\n*\n\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n\n*\n\n*\n\n*\n\n*\n\n*\n\n*\n\n*\n\n*\n*\n*\n*\n**\n*\n*\n\n*\n*\n*\n*\n*\n*\n\n0\n0\n\n1000\n1000\n\n2000\n2000\n\n3000\n3000\n\n4000\n4000\n\nnetincome\nnetincome\n\nfigure 2.5: car in household against income in euros.\n\nexample 2.1: car in household\nin figure 2.5, the logit model is fitted to the household data, where \u03c0(x) = p (y = 1|x) is the probability\nthat at least one car is owned. the estimates are \u02c6\u03b20 = \u22121.851, \u02c6\u03b2 = 0.00209. the model suggests a dis-\ntinct increase of the probability with increasing net income. figure 2.5 also shows the relative frequencies\nin intervals of length 50. it is seen that the fit is quite good for higher income but less satisfactory for low\nincome.\n\nit is often useful to work with the alternative representations of the model:\n\n\u03c0(x)\n1 \u2212 \u03c0(x)\n\n= exp(\u03b20 + x\u03b2)\n\n(2.8)\n\nand\n\nlog( \u03c0(x)\n1 \u2212 \u03c0(x)\n\n) = \u03b20 + x\u03b2.\n\n(2.9)\nrepresentation (2.8) is based on the odds \u03c0(x)/(1 \u2212 \u03c0(x)) = p (y = 1|x)/p (y = 0|x),\nwhereas the left-hand side of (2.9) represent the log-odds or logits for a given x. thus (2.8)\nmay be seen as the odds representation of the model while (2.9) gives the logit representation.\nsince it is easier to think in odds than in log-odds, most program packages give an estimate of\ne\u03b2 in addition to an estimate of \u03b2.\nwhen interpreting parameters for the logit model one has to take into account the logit\ntransformation. for the linear model e(y|x) = \u03b20 + x\u03b2, the parameter \u03b2 is simply the change\nin mean response if the x-value increases by one unit, that is, \u03b2 = e(y|x + 1) \u2212 e(y|x). for\nthe logit model, the corresponding change is measured in logits. let\n\nlogit(x) = log(\u03b3(x)) = log\n\n(cid:25)\n\n(cid:26)\n\n\u03c0(x)\n1 \u2212 \u03c0(x)\n\ndenote the logit or log-odds at value x. then, from logit(x) = \u03b20 + x\u03b2, it follows that \u03b2 is\ngiven by\n\n\u03b2 = logit(x + 1) \u2212 logit(x).\n\n "}, {"Page_number": 52, "text": "40\n\nchapter 2. binary regression: the logit model\n\nthus \u03b2 measures the change in logits if x is increased by one unit. alternatively, one might use\nthe form (2.8) of the logit model:\n\n\u03c0(x)/(1 \u2212 \u03c0(x)) = exp(\u03b20 + x\u03b2) = e\u03b2o(e\u03b2)x.\n\nit is seen that e\u03b2o represents the odds at value x = 0 and the odds change by the factor e\u03b2 if x\nincreases by one unit. with \u03b3(x) = \u03c0(x)/(1 \u2212 \u03c0(x)) denoting the odds at a covariate value x,\none obtains\n\ne\u03b2 = \u03c0(x + 1)/(1 \u2212 \u03c0(x + 1))\n\n\u03c0(x)/(1 \u2212 \u03c0(x))\n\n= \u03b3(x + 1)\n\u03b3(x)\n\n.\n\n(2.10)\n\nthus e\u03b2 is the factor by which the odds \u03b3(x) increase or decrease (depending on \u03b2 > 0 or\n\u03b2 < 0) if x is increased by one unit. as a proportion between \u03b3(x + 1) and \u03b3(x), the term e\u03b2 is\nan odds ratio. the odds ratio is a measure of the dependence of y on x. it reflects how strong\nthe odds change if x increases by one unit. it is noteworthy that the logit model assumes that\nthe odds ratios given by (2.10) do not depend on x. therefore, interpretation of \u03b2 or e\u03b2 is very\nsimple.\n\nlet us consider the car data for several scalings of net income. the first model uses income\nin euros as the predictor. alternatively, one might center income around some fixed value,\nsay 1500, such that the variable x corresponds to net income \u2212 1500. as a third version we\nconsider the covariate (net income\u22121500)/1000 such that one unit corresponds to 1000 euros.\nthe estimates for all three models are given in table 2.1. for the first model, e \u02c6\u03b20 = 0.157\ncorresponds to the odds that there is a car in the household (instead of no car) for zero income.\nfor income centered around 1500, e \u02c6\u03b20 = 3.6 refers to the odds at income 1500 euros. in\nthe first two models the change in odds by increasing income by one euro is determined by\nthe factor e \u02c6\u03b2 = 1.002, which does not help much in terms of interpretation. for a rescaled\ncovariate, measured in 1000 euros, the factor is e \u02c6\u03b2 = 8.064, which gives some intuition for the\nincrease in odds when income is increased by one unit, which means 1000 euros.\n\ntable 2.1: parameter for logit model with predictor income.\n\nparameter\n\u02c6\u03b20 = \u22121.851\n\u02c6\u03b2 = 0.00209\n\nincome in euros\n\nodds\ne \u02c6\u03b20 = 0.157\ne \u02c6\u03b2 = 1.002\n\nincome in euros, centered at euro 1500\n\nparameter\n\u02c6\u03b20 = 1.281\n\u02c6\u03b2 = 0.00209\n\nodds\ne \u02c6\u03b20 = 3.600\ne \u02c6\u03b2 = 1.002\n\nincome in thousands of euros, centered at euro 1500\n\nparameter\n\u02c6\u03b20 = 1.281\n\u02c6\u03b2 = 2.088\n\nodds\ne \u02c6\u03b20 = 3.600\ne \u02c6\u03b2 = 8.069\n\nan alternative way to get some understanding of the parameter values has been suggested\nby cox and snell (1989). it may be shown that 1/\u03b2 is approximately the distance between the\n\n "}, {"Page_number": 53, "text": "2.3. the logit model\n\n41\n\n75% point and the 50% point of the estimated logistic curve. the distance between the 95%\npoint and the 50% point is approximately 3/\u03b2. from 1/ \u02c6\u03b2 = 478 for the first model of the car\ndata (and 1/ \u02c6\u03b2 = 0.478 for the scaling in 1000 euros) one gets some intuition for the increase\nin the logistic curve without having to plot it. as a general rule one might keep the following\nin mind:\n\nchange in x by \u00b11/\u03b2:\nchange in x by \u00b13/\u03b2:\n\nchange in \u03c0 from \u03c0 = 0.5 to 0.5 \u00b1 0.23,\nchange in \u03c0 from \u03c0 = 0.5 to 0.5 \u00b1 0.45.\n\nthe value \u03c0 = 0.5 itself occurs for the x-value \u2212\u03b20/\u03b2.\n\nmultivariate predictor\nwhen several continuous covariates are available the linear predictor may have the form\n\n\u03b7(x) = \u03b20 + x1\u03b21 + . . . + xm\u03b2m.\n\nthe interpretation is the same as for univariate predictors; however, one should be aware that\nthe other variables are present. from\n\nlogit(x) = log\n\n\u03c0(x)\n1 \u2212 \u03c0(x)\n\n= \u03b20 + x1\u03b21 + . . . + xm\u03b2m\n\nand\n\n\u03b3(x) = \u03c0(x)\n1 \u2212 \u03c0(x)\n\n= e\u03b20(e\u03b21)x1 . . . (e\u03b2m)xm\n\none derives immediately that \u03b2j corresponds to the additive change in logits when variable \u03b2j\nis increased by one unit while all other variables are kept fixed,\n\n\u03b2j = logit(x1, . . . , xj + 1, . . . , xm) \u2212 logit(x1, . . . , xm),\n\nand e\u03b2j corresponds to the multiplicative change in odds when xj is increased by one unit,\n\ne\u03b2i = \u03b3(x1, . . . , xj + 1, . . . , xm)\n\n\u03b3(x1, . . . , xm)\n\n.\n\nit is essential that the effects of predictors measured by \u03b2j or e\u03b2j assume that all other values\nare kept fixed. a strong assumption is implied here, namely that the effect of a covariate is\nthe same, whatever values the other variables take. if that is not the case, one has to include\ninteraction effects between predictors (see chapter 4).\n\nexample 2.2: vasoconstriction\na classical example in logistic regression is the vasoconstriction data that were used by finney (1947).\nthe data (figure 2.2) were obtained in a carefully controlled study in human physiology where a reflex\n\u201cvasoconstriction\u201d may occur in the skin of the digits after taking a single deep breath. the response y\nis the occurrence (y = 1) or non-occurrence (y = 0) of vasoconstriction in the skin of the digits of one\nsubject after he or she inhaled a certain volume of air at a certain rate. the responses of three subjects are\navailable. the first contributed 9 responses, the second contributed 8 responses, and the third contributed\n22 responses.\n\nalthough the data represent repeated measurements, usually independent observations were assumed.\nthe effect of the volume of air and inspiration rate on the occurrence of vasoconstriction may be based on\nthe binary logit model\n\nlogit(x) = log(\n\n\u03c0(x)\n1 \u2212 \u03c0(x)\n\n) = \u03b20 + volume\u03b21 + rate\u03b22.\n\n "}, {"Page_number": 54, "text": "42\n\nchapter 2. binary regression: the logit model\n\nthen interpretation of the parameters refers to the multiplicative change in odds when the variables are\nincreased by one unit. alternatively, one can apply the logit model with log-transformed variables,\n\nlogit(x) = log(\n\nwhich is equivalent to\n\n\u03c0(x)\n1 \u2212 \u03c0(x)\n\n\u03c0(x)\n1 \u2212 \u03c0(x)\n\n) = \u03b20 + log(volume)\u03b21 + log(rate)\u03b22,\n\n= e\u03b20 volume\u03b21rate\u03b22 .\n\nthen the effect of the covariates volume and rate is multiplicative on the odds. when rate changes by the\nfactor c the odds change by the factor c\u03b21. the maximum likelihood estimates for the log-transformed\ncovariates are \u03b20 = \u22122.875, \u03b21 = 5.179, \u03b22 = 4.562. the logit model for the original variables yields\n\u03b20 = \u22129.5296, \u03b21 = 3.8822, \u03b22 = 2.6491. in both models the covariates are highly significant.\n\ntable 2.2: vasoconstriction data.\n\nindex\n\nvolume\n\nrate\n\ny\n\nindex\n\nvolume\n\nrate\n\ny\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n3.70\n3.50\n1.25\n0.75\n0.80\n0.70\n0.60\n1.10\n0.90\n0.90\n0.80\n0.55\n0.60\n1.40\n0.75\n2.30\n3.20\n0.85\n1.70\n\n0.825\n1.090\n2.500\n1.500\n3.200\n3.500\n0.750\n1.700\n0.750\n0.450\n0.570\n2.750\n3.000\n2.330\n3.750\n1.640\n1.600\n1.415\n1.060\n\n1\n1\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n0\n\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n1.80\n0.40\n0.95\n1.35\n1.50\n1.60\n0.60\n1.80\n0.95\n1.90\n1.60\n2.70\n2.35\n1.10\n1.10\n1.20\n0.80\n0.95\n0.75\n1.30\n\n1.800\n2.000\n1.360\n1.350\n1.360\n1.780\n1.500\n1.500\n1.900\n0.950\n0.400\n0.750\n0.030\n1.830\n2.200\n2.000\n3.330\n1.900\n1.900\n1.625\n\n1\n0\n0\n0\n0\n1\n0\n1\n0\n1\n0\n1\n0\n0\n1\n1\n1\n0\n0\n1\n\n2.3.3 logit model with binary predictor\nwhen both the predictor and the response variable are binary the usual representation of data is\nin a (2\u00d72)-contingency table, as in the next example.\n\nexample 2.3: duration of unemployment\na simple example of a dichotomous covariate is given by the contingency table in table 2.3, which\nshows data from a study on the duration of unemployment. the duration of unemployment is given in\ntwo categories, short-term unemployment (less than 6 months) and long-term employment (more than\n6 months). subjects are classified with respect to gender and duration of unemployment. gender is\nconsidered as the explanatory variable and duration as the response variable.\n\nwhen a binary variable is used as an explanatory variable it has to be coded. thus, instead\nof g = 1 for males and g = 2 females, one uses a dummy variable. there are two forms in\ncommon use, (0-1)-coding and effect coding.\n\n "}, {"Page_number": 55, "text": "2.3. the logit model\n\n43\n\ntable 2.3: cross-classification of gender and duration of unemployment.\n\n\u2264 6 months\n\nduration\n\n> 6 months marginals odds\n\nlog-odds\n\ngender\n\nmale\nfemale\n\n403\n238\n\n167\n175\n\n570\n413\n\n2.413\n1.360\n\n0.881\n0.307\n\n= e\u03b20\n\n\u03c0(xg)\n1 \u2212 \u03c0(xg)\n(cid:26)\n\n= logit(xg = 0),\n\nlogit model with (0-1)-coding of covariates\nlet xg denote the dummy variable for g in (0-1)-coding, given by xg = 1 for males and\nxg = 0 for females. with y denoting the dichotomous response, specified by y = 1 for short-\nterm unemployment and y = 0 for long-term unemployment and \u03c0(xg) = p (y = 1|xg)), the\ncorresponding logit model has the form\n\n(cid:27)\n\n(cid:28)xg .\n\ne\u03b2\n\n(2.11)\n\n(cid:25)\n\nlog\n\n\u03c0(xg)\n1 \u2212 \u03c0(xg)\n\n= \u03b20 + xg\u03b2\n\n(cid:26)\n\n(cid:25)\n\nit is immediately seen that \u03b20 is given by\n\n\u03b20 = log\n\n(cid:25)\n\n\u03c0(xg = 0)\n1 \u2212 \u03c0(xg = 0)\n(cid:26)\n\n(cid:25)\n\nwhich corresponds to the logits in the reference category, where xg = 0. the effect of the\ncovariate takes the form\n\n(cid:26)\n\n\u03c0(xg = 1)\n1 \u2212 \u03c0(xg = 1)\n\n\u03b2 = log\n= logit(xg = 1) \u2212 logit(xg = 0),\n\n\u2212 log\n\n\u03c0(xg = 0)\n1 \u2212 \u03c0(xg = 0)\n\nwhich is the additive change in logits for the transition from xg = 0 to xg = 1. a simpler\ninterpretation holds for the transformed parameters\n\ne\u03b20 = \u03c0(xg = 0)\n1 \u2212 \u03c0(xg = 0)\n\n= \u03b3(xg = 0),\n\nwhich corresponds to the odds in the reference category xg = 0, and\n= \u03b3(xg = 1)\n\u03b3(xg = 0)\n\ne\u03b2 = \u03c0(xg = 1)/(1 \u2212 \u03c0(xg = 1)\n\u03c0(xg = 0)/(1 \u2212 \u03c0(xg = 0))\n\n= \u03b3(1|0),\n\nwhich corresponds to the odds ratio between xg = 1 and xg = 0. it may also be seen as the\nfactor by which the logits change if xg = 0 is replaced by xg = 1.\n\nlogit model with (0-1)-coding\n\n\u03b20 = logit(xg = 0)\n\u03b2 = logit(xg = 1) \u2212 logit(xg = 0)\n\ne\u03b20 = \u03b3(xg = 0)\ne\u03b2 = \u03b3(xg = 1)\n\u03b3(xg = 0)\n\n "}, {"Page_number": 56, "text": "44\n\nchapter 2. binary regression: the logit model\n\nexample 2.4: duration of unemployment\nfor the contingency table in example 2.3 one obtains for the logit model with gender given in (0-1)-coding\n\n\u02c6\u03b20 = 0.307,\n\n\u02c6\u03b20 = 1.360,\n\ne\n\n\u02c6\u03b2 = 0.574,\n\n\u02c6\u03b2 = 1.774.\n\ne\n\nit is usually easier to interpret e\u03b2 rather than \u03b2 itself. while \u03b2 refers to the change in logits, e\u03b2 gives the\n\u02c6\u03b2 = 1.774 means that the odds of short-time unemployment for men are almost twice\nodd ratio. thus e\nthe odds for women.\n\nlogit model with effect coding\nan alternative form of the logit model is given by\np (y = 1|g = i)\n1 \u2212 p (y = 1|g = i)\n\n(cid:25)\n\nlog\n\n(cid:26)\n\n= \u03b20 + \u03b2i.\n\nhere \u03b2i is the effect of the factor g if g = i. but since only two logits are involved, namely,\nfor g = 1 and g = 2, one needs a restriction to make the parameters \u03b20, \u03b21, \u03b22 identifiable.\nthe symmetric constraint \u03b21 + \u03b22 = 0 (or \u03b22 = \u2212\u03b21) is equivalent to the model\n\n(cid:25)\n\n(cid:26)\n\nlog\n\n\u03c0(xg)\n1 \u2212 \u03c0(xg)\n\n= \u03b20 + xg\u03b2,\n\nwhere \u03b2 = \u03b21 and xg is given in effect coding, that is, it is given by xg = 1 for males\nand xg = \u22121 for females. with \u03b3(xg) = \u03c0(xg)/(1 \u2212 \u03c0(xg)) one obtains \u03b3(xg = 1) =\ne\u03b20 e\u03b2, \u03b3(xg = \u22121) = e\u03b20 e\u2212\u03b2. simple computation shows that\n\n\u03b20 =\n\n1\n2\n\n(logit(xg = 1) + logit(xg = \u22121))\n\nis the arithmetic mean over the logits of categories g = 1 and g = 2. therefore,\n\ne\u03b20 = (\u03b3(xg = 1)\u03b3(xg = \u22121))1/2\n\nis the geometric mean over the odds \u03b3(xg = 1) and \u03b3(xg = \u22121), representing some sort of\n\"baseline\" odds. for \u03b2 one obtains\n\nwhich is half the change in logits for the transition from xg = \u22121 to xg = 1. thus\n\n\u03b2 =\n\n1\n2\n\n(logit(xg = 1) \u2212 logit(xg = \u22121)),\n\ne\u03b2 = (\u03b3(xg = 1)/\u03b3(xg = \u22121)) 1\n\n2 = \u03b3(xg = 1|xg = \u22121)1/2\n\nis the square root of the odds ratio. since \u03b3(xg = 1) = e\u03b20 e\u03b2, the term e\u03b2 is the factor that\nmodifies the \"baseline\" odds e\u03b20 to obtain \u03b3(xg = 1). to obtain \u03b3(xg = \u22121) one has to use\nthe factor e\u2212\u03b2.\n\nlogit model with effect coding\n\n\u03b20 =\n\n\u03b2 =\n\n1\n2\n1\n2\n\n(logit(xg = 1) + logit(xg = \u22121))\n(logit(xg = 1) \u2212 logit(xg = \u22121))\n\ne\u03b20 = (\u03b3(xg = 1)\u03b3(xg = \u22121))1/2\ne\u03b2 = \u03b3(xg = 1|xg = \u22121) 1\n\n2\n\n "}, {"Page_number": 57, "text": "2.3. the logit model\n\n45\n\nexample 2.5: duration of unemployment\nfor the data in example 2.3 one obtains for the logit model with gender given in effect coding\n\n\u02c6\u03b20 = 0.594,\n\n\u02c6\u03b20 = 1.811,\n\ne\n\n\u02c6\u03b2 = 0.287,\n\n\u02c6\u03b2 = 1.332.\n\ne\n\nas in (0-1)-coding it is easier to interpret e\u03b2 rather than \u03b2 itself. while \u02c6\u03b20 is the average (arithmetic\n\u02c6\u03b20 = 1.811 is the geometric mean over the odds. one obtains the odds in the male\n\u02c6\u03b2 = 1.332, which shows that the odds are better in the male population.\n\u2212 \u02c6\u03b2 = 1/1.332 yields the odds in the female population.\n\nmean) over logits, e\npopulation by applying the factor e\nmultiplication with e\n\n2.3.4 logit model with categorical predictor\nin the more general case, categorical covariates have more than just two outcomes. as an\nexample let us again consider the duration of unemployment given in two categories, short-\nterm unemployment (less than 6 months) and long-term employment (more than 6 months), but\nnow depending on education level. in table 2.4 subjects are classified with respect to education\nlevel and duration of unemployment. education level is considered the explanatory variable\nand duration the response variable. interpretation of the parameters is similar to the binary\ncovariate case. in the following we will again distinguish between the the restrictions yielding\n(0-1)-coding and effect coding.\n\ntable 2.4: cross-classification of level of education and duration of unemployment.\n\nduration\n\n\u2264 6 months > 6 months\n\nno specific training\n\nlow level training\n\nhigh level training\n\nuniversity degree\n\n1\n\n2\n\n3\n\n4\n\n202\n\n307\n\n87\n\n45\n\n96\n\n162\n\n66\n\n18\n\n298\n\n469\n\n153\n\n63\n\nlogit model with (0-1)-coding\nconsider a categorical covariable or factor a with categories a \u2208 {1, . . . , i} and let \u03c0(i) =\np (y = 1|a = i) denote the response probability. then a general form of the logit model is\ngiven by\n\nlog( \u03c0(i)\n1 \u2212 \u03c0(i)\n\n) = \u03b20 + \u03b2i\n\n(2.12)\nsince one has only i logits, log(\u03c0(i)/(1\u2212\u03c0(i))), i = 1, ..., i, but i+1 parameters \u03b20, \u03b21, ..., \u03b2i,\na constraint for the parameters is necessary. one possibility is to set \u03b2i = 0, thereby defining\ni as the reference category. then the intercept \u03b20 = log(\u03c0(i)/(1 \u2212 \u03c0(i))) is the logit for the\nreference category i and\n\n= e\u03b20 e\u03b2i .\n\nor\n\n\u03c0(i)\n1 \u2212 \u03c0(i)\n\n(cid:25)\n\n(cid:26)\n\n(cid:25)\n\n(cid:26)\n\n\u03b2i = log\n\n\u03c0(i)\n1 \u2212 \u03c0(i)\n\n\u2212 log\n\n\u03c0(i)\n1 \u2212 \u03c0(i)\n\n= log \u03b3(i|i)\n\nis the additive change in logits if a = i is replaced by a = i, which is equivalent to the\nlogarithm of the odds ratio of a = i to a = i. alternatively, one may consider the exponentials\n\ne\u03b20 = \u03c0(i)\n\n1 \u2212 \u03c0(i),\n\ni = \u03b3(i|i).\ne\u03b2\n\n "}, {"Page_number": 58, "text": "46\n\nchapter 2. binary regression: the logit model\n\nthe latter is the odds ratio of a = i to a = i. the model (2.12) may be given in the form of a\nregression model by using the dummy variables xa(i) = 1 if a = i, and xa(i) = 0 otherwise.\nthen one has a model with multiple predictor xa(1), . . . , xa(i\u22121)\n\nlog\n\n\u03c0(i)\n1 \u2212 \u03c0(i)\n\n= \u03b20 + xa(1)\u03b21 + \u00b7\u00b7\u00b7 + xa(i\u22121)\u03b2i\u22121.\n\nexample 2.6: duration of unemployment with predictor education level\nfor the data in table 2.4 one obtains the odds and the parameters for the logit model in (0-1)-coding as\ngiven in table 2.5.\n\ntable 2.5: odds and parameters for logit model with predictor education level.\n\nlevel\n\n1\n2\n3\n4\n\nshort\nterm term odds e\u03b2i\n\nlong\n\n202\n307\n87\n45\n\n96\n162\n66\n18\n\n\u03b2i\n\n\u03b3(1/4) = 0.843 \u22120.170\n\u03b3(2/4) = 0.761 \u22120.273\n\u03b3(3/4) = 0.529 \u22120.637\n\u03b3(4/4) = 1\n\n0\n\nstandard\n\nerror\n\n0.30\n0.29\n0.32\n\n0\n\nparameters with (0-1)-coding\n\n(cid:23)\n\n(cid:24)\n\n\u03c0(i)\n\u03b20 = log\n1\u2212\u03c0(i)\n\u03b2i = log(\u03b3(i|i))\n\ne\u03b20 = \u03c0(i)\n1\u2212\u03c0(i)\ni = \u03b3(i|i)\ne\u03b2\n\nlogit model with effect coding\nan alternative restriction on the parameters in model (2.12)) is the symmetric restriction \u03b21 +\n... + \u03b2i = 0. by considering \u03b2i = \u2212\u03b21 \u2212 ... \u2212 \u03b2i\u22121 one obtains\n\ni = 1, ..., i \u2212 1,\n\n= \u03b20 + \u03b2i ,\n= \u03b20 \u2212 \u03b21 \u2212 ... \u2212 \u03b2i\u22121.\n\nthis is equivalent to the form\n\nlog\n\n= \u03b20 + xa(1)\u03b21 + ... + xa(i\u22121)\u03b2i\u22121,\n\nwhere the xa(i)\u2019s are given in effect coding, that is, xa(i) = 1 if a = i; xa(i) = \u22121 if a = k;\nand xa(i) = 0 otherwise. one obtains for the parameters\n\n(cid:26)\n\ni(cid:7)\n\ni=1\n\n\u03c0(i)\n(cid:26)\n1 \u2212 \u03c0(i)\n\n=\n\n1\ni\n\n\u03b20 =\n\nlog\n\nlog(\u03b3(i)),\n\n\u03b2i = log\n\n\u03c0(i)\n1 \u2212 \u03c0(i)\n\n\u2212 \u03b20 = log(\u03b3(i)) \u2212 \u03b20.\n\n(cid:26)\n(cid:26)\n\n(cid:25)\n\n(cid:25)\n(cid:25)\n\nlog\n\nlog\n\n(cid:25)\n\n\u03c0(i)\n1 \u2212 \u03c0(i)\n\u03c0(i)\n1 \u2212 \u03c0(i)\n(cid:26)\n\n\u03c0(i)\n1 \u2212 \u03c0(i)\ni(cid:7)\n(cid:25)\n\n1\ni\n\ni=1\n\n "}, {"Page_number": 59, "text": "2.3. the logit model\n\n47\n\ntherefore \u03b20 corresponds to the arithmetic mean of logits across all categories of a and is some\nsort of a baseline logit, whereas \u03b2i represents the (additive) deviation of category i from this\nbaseline. for e\u03b20 one obtains\n\ne\u03b20 = (\u03b3(1) \u00b7 . . . \u00b7 \u03b3(i))1/i ,\n\nwhich is the geometric mean over the odds. since \u03b3(i) = e\u03b20 e\u03b2i, e\u03b2i represents the factor that\ntransforms the baseline odds into the odds of population i.\n\nparameters with effect coding\n\n(cid:14)\n\ni\n\n\u03b20 = 1\n\u03b2i = log(\u03b3(i)) \u2212 \u03b20\ni\n\ni=1 log(\u03b3(i))\n\ne\u03b20 = (\u03b3(1) \u00b7 . . . \u00b7 \u03b3(i))1/i\ne\u03b2i = \u03b3(i) e\u2212\u03b20\n\nlogit model with several categorical predictors\nif more than one predictor is included, the interpretation of parameters is pretty much the same.\nlet us consider two predictors, a \u2208 {1, . . . , i} and b \u2208 {1, . . . , j}, and the main effect model\nthat contains dummies but no interactions. with \u03c0(a = i, b = j) = p (y = 1|a = i, b = j)\nthe model with the reference category i for factor a and j for factor b is given by\n\nlog( \u03c0(a = i, b = j)\n1 \u2212 \u03c0(a = i, b = j)\n= \u03b20 + xa(1)\u03b2a(1) + ... + xa(i\u22121)\u03b2a(i\u22121) + xb(1)\u03b2b(1) + ... + xb(j\u22121)\u03b2b(j\u22121),\n\n)\n\nwhere xa(i), xb(j) are 0-1 dummy variables. it is easily derived (exercise 2.2) that the param-\neters have the form given in the following box.\n\nparameters with (0-1)-coding\n\n= \u03b3(a = i, b = j),\n\ne\u03b20 = \u03c0(a = i, b = j)\n1 \u2212 \u03c0(a = i, b = j)\nodds for a = i, b = j\ne\u03b2a(i) = \u03c0(a = i, b = j)/(1 \u2212 \u03c0(a = i, b = j))\n\u03c0(a = i, b = j)/(1 \u2212 \u03c0(a = i, b = j))\nto a = i,\nodds ratio compares a = i\ne\u03b2b(j) = \u03c0(a = i, b = j)/(1 \u2212 \u03c0(a = i, b = j))\n\u03c0(a = i, b = j)/(1 \u2212 \u03c0(a = i, b = j))\nto b = j,\nodds ratio compares b = j\n\n= \u03b3(a = i, b = j)\n\u03b3(a = i, b = j) ,\nany b = j\n\n= \u03b3(a = i, b = j)\n\u03b3(a = i, b = j),\nany a = i\n\n "}, {"Page_number": 60, "text": "48\n\nchapter 2. binary regression: the logit model\n\nthe exponential of \u03b20 corresponds to the odds for the reference category (a = i, b =\nj), the exponential of \u03b2a(i) corresponds to the odds ratio between a = i, and the reference\ncategory a = i for any category of b. for \u03b2b(j), the corresponding odds ratio does not\ndepend on a. it should be noted that this simple interpretation no longer holds if interactions\nare included.\n\n2.3.5 logit model with linear predictor\nin the general case one has several covariates, some of which are continuous and some of which\nare categorical. then one may specify a logit model logit(x) = \u03b7(x) with the linear predictor\ngiven by\n\n\u03b7(x) = xt \u03b2 = \u03b20 + x1\u03b21 + . . . + xp\u03b2p,\n\nwhich yields\n\nlog( \u03c0(x)\n1 \u2212 \u03c0(x)\n\n) = \u03b20 + x1\u03b21 + . . . + xp\u03b2p.\n\nthe x-values in the linear predictor do not have to be the original variables. when variables are\ncategorical the x-values represent dummy variables; when variables are continuous they can\nrepresent transformations of the original variables. components within the linear predictor can\nbe\n\ni , x3\n\ni , . . . ,\n\nxi, x2\nxa(i), . . . , xa(i\u22121),\nxa(i) \u00b7 xb(j),\ni = 1, . . . , i \u2212 1,\nj = 1, . . . , j \u2212 1;\nxixb(j), j = 1, . . . , j,\n\nmain effects or polynomial terms built from variable xi;\nmain effects (dummy variables) as transformations of categorical\nvariable a \u2208 {1, . . . , i};\ninteractions (products of dummy variables) between two factor\na \u2208 {1, . . . , i}, b \u2208 {1, . . . , j};\n\ninteractions between metrical variable xi and categorical variable\nb \u2208 {1, . . . , j}.\n\none may also include products of more than two variables (or dummies), which means interac-\ntions of a higher order. the structuring of the linear predictor is discussed more extensively in\nsection 4.4.\n\n2.4 the origins of the logistic function and the logit model\nin this chapter and in some of the following the logistic function occurs quite often. therefore\nsome remarks on the origins of the function seem warranted. we will follow cramer (2003),\nwho gives a careful account of the historical development.\n\na simple model for the growth of populations assumes that the growth rate dw(t)/dt is\ndirectly proportional to the size w(t) at time point t. the corresponding differential equation\ndw(t)/dt = \u03b2w(t) has the solution w(t) = c exp(\u03b2t), where c is a constant. this exponential\ngrowth model model means unopposed growth and is certainly not realistic over a wide range\nof time.\n\nan alternative model that has been considered by the belgian astronomer quetelet (1795\u2013\n1874) and the mathematician verhulst (1804\u20131849) may be derived from a differential equation\nwith an extra term. it is assumed that the rate is given by\n\ndw(t)/dt = \u02dc\u03b2w(t)(s \u2212 w(t)),\n\nwhere s denotes the upper limit or saturation level. the growth rate is now determined by the\nsize w(t) but also by the term (s \u2212 w(t)), which has the opposite effect, that is, the large size\ndecelerates growth. by transforming the equation to\n\ndf (t)/dt = \u03b2f (t)(1 \u2212 f (t)),\n\n "}, {"Page_number": 61, "text": "2.5. exercises\n\n49\n\nwhere f (t) = w(t)/s and \u03b2 = s \u02dc\u03b2, one obtains that the solution has the form of the logistic\nfunction\n\nf (t) = exp(\u03b1 + \u03b2t)/(1 + exp(\u03b1 + \u03b2t))\n\nwith constant \u03b1. therefore, the logistic function results as the solution of a rather simple growth\nmodel. it is widely used in modeling population size but also in marketing when the objective\nis the modeling of market penetration of new products.\n\nthe logit model came up much later than the probit model. cramer (2003) traces the roots\nof the probit model back to fechner (1801\u20131887), who is still well known to psychologists\nbecause of fechner\u2019s law, which relates the physical strength of a stimulus and its strength\nas perceived by humans.\nit seems to have been reinvented several times until bliss (1934)\nintroduced the term probit for probability unit. the probit model used to be the classical model\nof bioassays. berkson (1994) introduced as an alternative the logit model, which was easier to\nestimate. the logit model was not well received, but after the ideological conflict had abated\nthe logit model was widely adopted in the late 1950s.\n\n2.5 exercises\n\n2.1 consider the bivariate random variable (y, x), where y \u2208 {0, 1} is a binary variable and x is a vector\nof continuous random variables. assume that x is conditionally normally distributed within the groups\ndetermined by y = 0 and y = 1, that is, x|y = r \u223c n(\u03bcr, \u03c3 r), where the covariance matrix \u03c3 r depends\non the category y = r. use bayes\u2019 theorem to derive the conditional distribution y|x and show that one\nobtains a logit model that is determined by a linear predictor, but x-variables themselves are included in\nquadratic form.\n2.2 consider a binary logit model with two factors, a \u2208 {1, . . . , i} and b \u2208 {1, . . . , j}, and linear\npredictor \u03b7 = \u03b20 + xa(1)\u03b2a(1) + ... + xa(i\u22121)\u03b2a(i\u22121) + xb(1)\u03b2b(1) + ... + xb(j\u22121)\u03b2b(j\u22121). show\nthat the parameters can be represented as log odds and log odds ratios.\n\n2.3 a logit model is used to model the probability of a car in household depending on the factor \"type of\nhousehold\" (1: household includes more than one person and children, 2: household includes more than\none person without children, 3: one-person household). the logit model uses two (0-1)-dummy variables\nfor type 1 and type 2.\n(a) write down the model and interpret the parameters, which were estimated as \u03b20 = \u22120.35, \u03b21 =\n\n2.37, \u03b22 = 1.72.\n\n(b) compute the parameters if category 1 is chosen as the reference category.\n(c) let the model now be given in effect coding with parameters \u02dc\u03b21, \u02dc\u03b21. find the parameters \u02dc\u03b21, \u02dc\u03b21\n(d) give a general form of how parameters for a factor a \u2208 {1, . . . , k} in (0-1)-coding \u03b2j can be\n\nand interpret.\n\ntransformed into parameters in effect coding \u02dc\u03b2j.\n\n2.4 a logit model for the data from the previous exercise was fitted, but now including a linear effect of\nincome. one obtains parameter estimates \u03b20 = \u22122.06, \u03b21 = 1.34, \u03b22 = 0.93, and \u03b2income = 0.0016.\n\n(a) write down the model and interpret the parameters.\n(b) why do these parameters differ from the parameters in the previous exercise?\n\n2.5 in a treatment study one wants to investigate the effect of age (x1) in years and dose (x2) in milligrams\n(mg) on side effects. the response is headache (y = 1: headache; y = 1: none). one fits a logit model\nwith the predictor \u03b7 = \u03b20 + x1\u03b21 + x2\u03b22.\n\n "}, {"Page_number": 62, "text": "50\n\nchapter 2. binary regression: the logit model\n\n(a) one obtains \u03b20 = \u22123.4, \u03b21 = 0.02, \u03b22 = 0.15. interpret the parameters.\n(b) plot the logits of headache as a function of dose for a patient of age 40 and a patient of age 60.\n(c) plot the (approximate) probability of headache as a function of dose for a patient of age 40 and a\n\npatient of age 60.\n\n(d) give the probability of headache for a patient of age 40 when the dose is 5.\nassume now that an interaction effect has to be included and one has the predictor \u03b7 = \u03b20 + x1\u03b21 +\nx2\u03b22 + x1x2\u03b212 with values \u03b20 = \u22123.8, \u03b21 = 0.02, \u03b22 = 0.15, \u03b212 = 0.005.\n\n(a) interpret the effect of dose if age is fixed at 40 (60).\n(b) plot the logits of headache as a function of dose for a patient of age 40 and a patient of age 60.\n(c) plot the (approximate) probability of headache as a function of dose for a patient of age 40 and a\n\npatient of age 60.\n\n(d) how can the effect of dose be interpreted if age is fixed at 40 (60)?\n\n "}, {"Page_number": 63, "text": "chapter 3\n\ngeneralized linear models\n\nin this chapter we embed the logistic regression model as well as the classical regression model\ninto the framework of generalized linear models. generalized linear models (glms), which\nhave been proposed by nelder and wedderburn (1972), may be seen as a framework for han-\ndling several response distributions, some categorical and some continuous, in a unified way.\nmany of the binary response models considered in later chapters can be seen as generalized\nlinear models, and the same holds for part of the count data models in chapter 7.\n\nthe chapter may be read as a general introduction to generalized linear models; continuous\nresponse models are treated as well as categorical response models. therefore, parts of the\nchapter can be skipped if the reader is interested in categorical data only. basic concepts like\nthe deviance are introduced in a general form, but specific forms that are needed in categorical\ndata analysis will also be given in the chapters where the models are considered. nevertheless,\nthe glm is useful as a background model for categorical data modeling, and since mccullagh\nand nelder\u2019s (1983) book everybody working with regression models should be familiar with\nthe basic concept.\n\n3.1 basic structure\na generalized linear model is composed from several components. the random component\nspecifies the distribution of the conditional response yi given xi, whereas the systematic com-\nponent specifies the link between the expected response and the covariates.\n\n(1) random component and distributional assumptions\n\ngiven xi, the yi\u2019s are (conditionally) independent observations from a simple exponential fam-\nily. this family has a probability density function or mass function of the form\n\n(cid:29)\n\nyi\u03b8i \u2212 b(\u03b8i)\n\n\u03c6i\n\n(cid:30)\n\n+ c(yi, \u03c6i)\n\n,\n\n(3.1)\n\nf(yi|\u03b8i, \u03c6i) = exp\n\nwhere\n\n\u03b8i\n\u03c6i\nb(.) and c(.)\n\nis the natural parameter of the family,\nis a scale or dispersion parameter, and\nare specific functions corresponding to the type of the family.\n\nas will be outlined later, several distributions like the binomial, normal, or poisson distribution\nare members of the simple exponential family.\n\n51\n\n "}, {"Page_number": 64, "text": "52\n\nchapter 3. generalized linear models\n\n(2) systematic component\n\nthe systematic component is determined by two structuring components, the linear term and\nthe link between the response and the covariates. the linear part that gives the glm its name\nspecifies that the variables xi enter the model in linear form by forming the linear predictor\n\n\u03b7i = xt\n\ni \u03b2,\n\nwhere \u03b2 is an unknown parameter vector of dimension p. the relation between the linear part\nand the conditional expectation \u03bci = e(yi|xi) is determined by the transformation\n\nor, equivalently, by\n\nwhere\n\n\u03bci = h(\u03b7i) = h(xt\n\ni \u03b2),\n\ng(\u03bci) = \u03b7i = xt\n\ni \u03b2,\n\n(3.2)\n\n(3.3)\n\nh\ng\n\nis a known one-to-one response function,\nis the so-called link function, that is, the inverse of h.\n\nequations (3.2) and (3.3) reflect equivalent ways to specify how the mean of the response\nvariable is linked to the linear predictor. the response function h in (3.2) shows how the linear\npredictor has to be transformed to determine the expected mean. equation (3.3) shows for\nwhich transformation of the mean the model becomes linear. a simple example is the logistic\nmodel, where the mean \u03bci corresponds to the probability of success \u03c0i. in this case one has the\ntwo forms\n\n\u03c0i =\n\nexp(xt\n\ni \u03b2)\ni \u03b2) ,\n1 + exp(xt\n\nyielding the response function h(\u03b7i) = exp(\u03b7i)/(1 + exp(\u03b7i)), and\n\nlog( \u03c0i\n1 \u2212 \u03c0i\n\n) = xt\n\nwhere the link function g = h\n\n(3.4)\n\u22121 is specified by the logit transformation g(\u03c0) = log(\u03c0/(1\u2212\u03c0)).\nbased on the latter form, which corresponds to (3.3), it is seen that a glm is a linear model\nfor the transformed mean where additionally it is assumed that the response has a distribution\nin the simple exponential family. a specific generalized linear model is determined by\n\ni \u03b2,\n\n\u2022 the type of the exponential family that specifies the distribution of yi|xi;\n\n\u2022 the form of the linear predictor, that is, the selection and coding of covariates;\n\n\u2022 the response or link function.\n\nbefore considering the various models that fit into this framework, let us make some remarks\non simple exponential families: in simple exponential families the natural parameter is linked\nto the mean of the distribution. thus the parameter \u03b8i may be seen as \u03b8i = \u03b8(\u03bci), where \u03b8 is\nconsidered as a transformation of the mean. parametrization of specific distributions most often\nuses different names and also different sets of parameters; for example, \u03bbi is often used in the\ncase of the poisson distribution and the exponential distribution. these parameters determine\nuniquely the mean \u03bci and therefore the natural parameter \u03b8i.\n\n "}, {"Page_number": 65, "text": "3.2. generalized linear models for continuous responses\n\n53\n\n3.2 generalized linear models for continuous responses\n3.2.1 normal linear regression\nthe normal linear regression model is usually given with an error term in the form\n\nyi = xt\n\ni \u03b2 + \u0001i\n\nwith normal error, \u0001i \u223c n(0, \u03c32). alternatively, the model may be specified in glm terminol-\nogy by\n\nyi|xi \u223c n(\u03bci, \u03c32)\n\nand\n\n\u03bci = \u03b7i = xt\n\ni \u03b2.\n\nthe form separates the distribution from the systematic component. while yi|xi \u223c n(\u03bci, \u03c32)\nassumes that the response is normal with the variance not depending on the observation, the\nlink between the mean and the predictor is provided by assuming \u03bci = \u03b7i = xt\ni \u03b2. thus,\nthe classical linear model uses the identity as a link function. it is easily seen that the normal\ndistribution is within the exponential family by considering\n\nf(y) = exp\n\n( y \u2212 \u03bc\n\n\u03c3\n\n\u22121\n2\n\n\u221a\n)2 \u2212 log(\n\ny\u03bc \u2212 \u03bc2/2\n\n\u03c32\n\n\u2212 y2\n2\u03c32\n\n\u221a\n\u2212 log(\n\n2\u03c0 \u03c3)\n\n.\n\n(cid:30)\n\n(cid:29)\n\n2\u03c0 \u03c3)\n\n= exp\n\n(cid:29)\n\n(cid:30)\n\ntherefore, the natural parameter and the function b are given by\n\n\u03b8(\u03bc) = \u03bc,\n\nb(\u03b8) = \u03b82/2 = \u03bc2/2, \u03c6 = \u03c32.\n\nthe separation of random and systematic components makes it easy to allow for alternative\nlinks between the mean and the predictors. for example, if the response is income or reaction\ntime, the responses are expected to be positive. then, a more appropriate link that at least\nensures that means are positive is\n\n\u03bc = exp(\u03b7) = exp(xt \u03b2).\n\nof course, the influence of the covariates and consequently the interpretation of the parameters\ndiffer from those in the classical linear model. in contrast to the linear model,\n\n\u03bc = xt \u03b2 = x1\u03b21 + . . . + xp\u03b2p,\n\nwhere the change of xj by one unit means an additive effect of \u03b2j on the expectation. the\nmodified link\n\n\u03bc = exp(x1\u03b21 + . . . + xp\u03b2p) = ex1\u03b21 \u00b7 . . . \u00b7 exp\u03b2p\n\nspecifies that the change of xj by one unit has a multiplicative effect on \u03bc by the factor e\u03b2j ,\nsince e(xj +1)\u03b2j = exj \u03b2j e\u03b2j . in figure 3.1 the normal regression model is illustrated for one\nexplanatory variable. the left picture shows the linear model and the right picture the log-link\nmodel. the straight line and the curve show the means as functions of x; the densities of the\nresponse are shown only at three distinct x-values.\n\n3.2.2 exponential distribution\nin cases where responses are strictly non-negative, for example, in the analysis of duration time\nor survival, the normal distribution model is rarely adequate. a classical distribution that is\noften used when time is the response variable is the exponential distribution\n\nf(y) = \u03bbe\n\n\u2212\u03bby = exp(\u2212\u03bby + log(\u03bb)),\n\ny \u2265 0.\n\n "}, {"Page_number": 66, "text": "54\n\nchapter 3. generalized linear models\n\n6\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\n1\n\u2212\n\n6\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\n1\n\u2212\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\nfigure 3.1: normal regression with identity link (left) and with log-link (right).\n\nwith \u03b8 = \u2212\u03bb, \u03c6 = 1, and b(\u03b8) = \u2212 log(\u2212\u03b8), the exponential distribution is of the simple expo-\nnential family type. since the expectation and variance of the exponential distribution are given\nby 1/\u03bb and 1/\u03bb2, it is seen that in contrast to the normal distribution the variance increases\nwith increasing expectation. thus, although there is a fixed link between the expectation and\nthe variance, the distribution model captures an essential property that is often found in real\ndatasets. the so-called canonical link, which fulfills \u03b8(\u03bc) = \u03b7, is given by\n\ng(\u03bc) = \u2212 1\n\u03bc\n\nor h(\u03b7) = \u2212 1\n\u03b7\n\n.\n\nsince \u03bc > 0, the linear predictor is restricted to \u03b7 = xt \u03b2 < 0, which implies severe restrictions\non \u03b2. therefore, often a more adequate link function is given by the log-link\n\ng(\u03bc) = log(\u03bc) or h(\u03b7) = exp(\u03b7),\n\nyielding \u03bc = exp(\u03b7) = exp(xt \u03b2).\n\n3.2.3 gamma-distributed responses\nsince the exponential distribution is a one-parameter distribution, its flexibility is rather re-\nstricted. a more flexible distribution model for non-negative responses like duration or insur-\nance claims is the \u03b3-distribution. with \u03bc > 0 denoting the expectation and \u03bd > 0 the shape\nparameter, the gamma-distribution has the form\n\n\u03bd\n\n(cid:26)\n\n(cid:26)\n\n(cid:25)\n(cid:25)\n(cid:26)\n(cid:25)\u2212(1/\u03bc)y \u2212 log(\u03bc)\n\u2212 \u03bd\n\u03bc\n+ \u03bd log(\u03bd) + (\u03bd \u2212 1) log(y) \u2212 log(\u03b3(\u03bd))\n\ny\u03bd\u22121 exp\n\n\u03bd\n\u03bc\n\ny\n\n.\n\nf(y) =\n\n1\n\n\u03b3(\u03bd)\n\n= exp\n\n1/\u03bd\n\nin exponential family parameterization one obtains the dispersion parameter \u03c6 = 1/\u03bd and\n\u03b8(\u03bc) = \u22121/\u03bc, b(\u03b8) = \u2212 log(\u2212\u03b8). in contrast to the exponential distribution, the dispersion\nparameter is not fixed. while it is \u03c6 = 1 for the exponential distribution, it is an additional pa-\nrameter in the \u03b3-distribution. as is seen from figure 3.2, the parameter \u03bd is a shape parameter.\nfor 0 < \u03bd < 1, f(y) decreases monotonically, whereas for \u03bd > 1 the density has a mode at\ny = \u03bc \u2212 \u03bc/\u03bd and is positively skewed. usually the \u03b3-distribution is abbreviated by \u03b3(\u03bd, \u03b1),\n\n "}, {"Page_number": 67, "text": "3.2. generalized linear models for continuous responses\n\n55\n\nwhere \u03b1 = \u03bd/\u03bc. when using the expectation as a parameter, as we did in the specification of\nthe density, we will write \u03b3(\u03bd, \u03bd\n\n\u03bc).\n\nthe variance of the gamma-distribution is given by var(y) = \u03bd/\u03b12 = \u03bc2/\u03bd. thus the\nvariance depends strongly on the expectation, an effect that is often found in practice. the\ndependence may be characterized by the coefficient of variation. the coefficient of variation,\ngiven by c = \u03c3/\u03bc, is a specific measure of variation that scales the standard deviation by the\n\u221a\nexpectation. for gamma-distributions, the coefficient of variation for the ith observation is\ngiven by \u03c3i/\u03bci = \u03bci/(\n\u03bd. since it does not depend on the observation, one may\nset c = \u03c3i/\u03bci. therefore, the assumption of a gamma-distribution implies that the coefficient\nof variation is held constant across observations. it is implicitly assumed that large means are\nlinked to large variances. this is in contrast to the assumption that is often used for normal\ndistributions, when variances are assumed to be constant over observations.\n\n\u03bd\u03bci) = 1/\n\n\u221a\n\ny\n\n1.4\n\n1.2\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0  \n0\n\n\u03bc=1, \u03bd=2\n\u03bc=1, \u03bd=5\n\u03bc=1, \u03bd=10\n\n\u03bc=1, \u03bd=0.5\n\u03bc=1, \u03bd=1   \n\ny\n\n1.4\n\n1.2\n\n1  \n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n0  \n0\n\n7\nx\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\nx\n\nfigure 3.2: gamma-distributions for several \u03bc,\u03bd.\n\nthe canonical link for the gamma-disribution is the same as for the exponential distribution.\nfigure 3.3 shows the exponential and the gamma regression model for the log-link function.\nwe can see how the shifting of the mean along the logarithmic function changes the form of the\ndistribution. in contrast to the normal model, where densities are simply shifted, for gamma-\ndistributed responses, the form of the densities depends on the mean. moreover, figure 3.3\nshows that densities are positive only for positive x-values. for the normal model shown in\nfigure 3.1 the log-link ensures that the mean is positive, but nevertheless the model also allows\nnegative values. thus, for a strictly positive-valued response the normal model is often not a\ngood choice, but, of course, the adequacy of the model depends on the values of x that are\nmodeled and the variance of the response.\n\n3.2.4 inverse gaussian distribution\n\nan alternative distribution with a strictly non-negative response, which can be used to model\nresponses like duration, is the inverse gaussian-distribution. in its usual form it is given by the\ndensity\n\n(cid:25)\n\n(cid:26)\n\n(cid:29)\n\n(cid:30)\n\nf(y) =\n\n\u03bb\n\n2\u03c0y3\n\n1/2\n\nexp\n\n\u2212 \u03bb\n2\u03bc2y\n\n(y \u2212 \u03bc)2\n\n,\n\ny > 0,\n\n "}, {"Page_number": 68, "text": "56\n\nchapter 3. generalized linear models\n\n6\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\n1\n\u2212\n\n6\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\n1\n\u2212\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\nfigure 3.3: exponential (left) and gamma-distributed (right) regression model with log-\nlink.\n\nwith abbreviation ig(\u03bc, \u03bb), where \u03bc, \u03bb > 0 are the determining parameters. straightforward\nderivation yields\n\n(cid:29)\n\n(cid:30)\n\ny(\u22121/(2\u03bc2)) + 1/\u03bc\n\n1/\u03bb\n\n\u2212 \u03bb\n2y\n\n\u2212 1\n2\n\nlog(\u03bb2\u03c0) \u2212 3\n2\n\nlog(y)\n\n,\n\nf(y) = exp\n\nand therefore\n\n\u03b8 = \u2212 1\n2\u03bc2 ,\n\nb(\u03b8) = \u22121/\u03bc = \u2212\u221a\u22122\u03b8, \u03c6 = 1/\u03bb,\n\nc(y, \u03c6) = \u22121/(2y\u03c6) \u2212 1\n2\n\nlog(2\u03c0/\u03c6) \u2212 3\n2\n\nlog(y).\n\nthe canonical link function, for which \u03b8(\u03bc) = \u03b7 holds, is given by\nh(\u03b7) = \u2212 1\u221a\n2\u03b7\n\ng(\u03bc) = \u2212 1\n2\u03bc2\n\nor\n\n,\n\nwhich implies the severe restriction \u03b7 = xt \u03b2 > 0. a link function without these problems is\nthe log-link function g(\u03bc) = log(\u03bc) and thus ih(\u03b7) = exp(\u03b7).\n\nthe inverse gaussian distribution has several interesting properties, including that the ml\n\nestimates of the mean \u03bc and the dispersion 1/\u03bb, given by\n\nn(cid:7)\n\n(cid:7)\n\n\u02c6\u03bc =\n\n1\nn\n\nyi,\n\ni=1\n\n1\n\u02c6\u03bb\n\n=\n\n1\nn\n\n(\n\n1\nyi\n\n\u2212 1\n\u00afy\n\n),\n\nare independent. this is similar to the normal distribution, for which the sample mean and the\nsample variance are independent. based on the independence, tweedie (1957) suggested an\nanalog of the analysis of variance for nested designs (see also folks and chhikara, 1978).\n\n3.3 glms for discrete responses\n3.3.1 models for binary data\nthe simplest case of a discrete response is when only \"success\" or \"failure\" is measured with\nthe outcome y \u2208 {0, 1}. the bernoulli distribution has for y \u2208 {0, 1} the probability mass\nfunction\n\n(cid:29)\n\n(cid:26)\n\n(cid:25)\n\n(cid:30)\n+ log(1 \u2212 \u03c0)\n\n,\n\nf(y) = \u03c0y(1 \u2212 \u03c0)1\u2212y = exp\n\ny log\n\n\u03c0\n1 \u2212 \u03c0\n\n "}, {"Page_number": 69, "text": "3.3. glms for discrete responses\n\n57\n\n6\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\n1\n\u2212\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\nfigure 3.4: inverse gaussian-distributed model with \u03bb = 3 and log-link.\n\nwhere \u03c0 = p (y = 1) is the probability for \"success.\" with \u03bc = \u03c0 it is an exponential family\nwith \u03b8(\u03c0) = log(\u03c0/(1 \u2212 \u03c0)), b(\u03b8) = log(1 + exp(\u03b8)) = \u2212 log(1 \u2212 \u03c0), \u03c6 = 1.\n\nthe classical link that yields the logit model is\n\n\u03c0 =\n\nexp(\u03b7)\n\n1 + exp(\u03b7) ,\n\ng(\u03c0) = log\n\n(cid:25)\n\n(cid:26)\n\n\u03c0\n1 \u2212 \u03c0\n\n(see chapter 2). alternatively, any strictly monotone distribution function f like the normal\ndistribution or extreme value distributions may be used as a response function, yielding \u03c0 =\nf (xt\n\ni \u03b2), with the response and link functions given by h(\u03b7) = f (\u03b7), g(\u03c0) = f\n\n\u22121(\u03c0).\n\n3.3.2 models for binomial data\nif experiments that distinguish only between \"success\" and \"failure\" are repeated independently,\nit is natural to consider the number of successes or the proportion as the response variable. for\nm trials one obtains the binomially distributed response \u02dcy \u2208 {0, . . . , m}. the probability\nfunction has the parameters m and the probability \u03c0 of success in one trial. for \u02dcy \u2208 {0, . . . , m}\nit has the form\n\n(cid:26)\n\u03c0 \u02dcy(1 \u2212 \u03c0)m\u2212\u02dcy = exp\n\n(cid:25)\n\nm\n\u02dcy\n\n\u23a7\u23a8\n\u23a9 \u02dcy\n\nf(\u02dcy) =\n\n(cid:23)\n\n(cid:24)\n\nm log\n\n\u03c0\n1\u2212\u03c0\n\n+ log(1 \u2212 \u03c0)\n\n1/m\n\n+ log\n\n(cid:25)\n\n(cid:26)\u23ab\u23ac\n\u23ad .\n\nm\n\u02dcy\n\nby considering the proportion of successes y = \u02dcy/m instead of the number of successes \u02dcy,\none obtains an exponential family with the same specifications as for binary responses: \u03bc =\ne(\u02dcy/m) = \u03c0, \u03b8(\u03c0) = log(\u03c0/(1 \u2212 \u03c0)), and b(\u03b8) = (1 + exp(\u03b8)) = \u2212 log(1 \u2212 \u03c0). only\nthe dispersion parameter is different, given as \u03c6 = 1/m. the distribution of y has the usual\nbinomial form\n\n(cid:25)\n\n(cid:26)\n\u03c0my(1 \u2212 \u03c0)m\u2212my =\n\n(cid:25)\n\n(cid:26)\n\u03c0 \u02dcy(1 \u2212 \u03c0)m\u2212\u02dcy,\n\nf(y) =\n\nm\nmy\n\nm\n\u02dcy\n\nbut with values y \u2208 {0, 1/m, . . . , 1}. because the support is different from the usual binomial\ndistribution, it is called the scaled binomial distribution. it consists of a simple rescaling of the\nnumber of successes to proportions and therefore changes the support.\n\n "}, {"Page_number": 70, "text": "58\n\nchapter 3. generalized linear models\n\nfor the binomial distribution the specification of the dispersion parameter differs from that\nfor the other distributions considered here. with indices one has for observation yi = \u02dcyi/mi the\ndispersion parameter \u03c6i = 1/mi, where mi is the number of replications. because mi is fixed,\nthe dispersion is fixed (and known) but may depend on the observations since the number of\nreplications may vary across observations. in contrast to the other distributions, the dispersion\ndepends on i.\n\nan alternative way of looking at binomial data is by considering them as grouped observa-\ntions, that is, grouping of replications (see section 3.5). for the special case m = 1 there is no\ndifference between the binomial and the rescaled binomial distributions. of course, the binary\ncase may be treated as a special case of the binomial case. consequently, the link and response\nfunctions are treated in the same way as in the binary case.\n\n3.3.3 poisson model for count data\n\ndiscrete responses often take the form of counts, for example, the number of insurance claims\nor case numbers in epidemiology. contingency tables may be seen as counts that occur as\nentries in the cells of the table. a simple distribution for count data is the poisson distribution,\nwhich for integer values y \u2208 {0, 1, . . .} and parameter \u03bb > 0 has the form\n\nf(y) = \u03bby\ny! e\n\n\u2212\u03bb = exp{y log(\u03bb) \u2212 \u03bb \u2212 log(y!)}.\n\nwith expectation \u03bc = \u03bb the parameters of the exponential family are given by \u03b8(\u03bc) = log(\u03bc),\nb(\u03b8) = exp(\u03b8) = \u03bc, \u03c6 = 1. a sensible choice of the link function should account for the\nrestriction \u03bb > 0. thus a widely used link function is the log-link yielding\n\nlog(\u03bb) = xt \u03b2 or \u03bb = exp(xt \u03b2), respectively.\n\nthe distribution is shown for three distinct x-values in figure 3.5. it is seen that differing means\nimply different shapes of the distribution. while the distribution of the response is skewed for\nlow means, it is nearly symmetric for large values of the mean.\n\ny\n\n6\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\n\u22121\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\nfigure 3.5: poisson regression with log-link.\n\n6\nx\n\n "}, {"Page_number": 71, "text": "3.3. glms for discrete responses\n\n59\n\n3.3.4 negative binomial distribution\nan alternative distribution for count data is the negative binomial distribution, which has mass\nfunction\n\n(cid:25)\n\n(cid:26)\n\n\u03bd\n\n(cid:25)\n\n(cid:26)\n\ny\n\nf(\u02dcy) =\n\n\u03b3(\u02dcy + \u03bd)\n\n\u03bd\n\n\u02dc\u03bc\n\n\u03b3(\u02dcy + 1)\u03b3(\u03bd)\n\n\u02dc\u03bc + \u03bd\n\n\u02dc\u03bc + \u03bd\n\n,\n\ny = 0, 1, . . . ,\n\n(3.5)\n\nwhere \u03bd, \u02dc\u03bc > 0 are parameters. we will use the abbreviation nb(\u03bd, \u03bc). the distribution may be\nmotivated in several ways. it may be seen as a mixture of poisson distributions in the so-called\ngamma-poisson model. the model assumes that the parameter \u03bb of the poisson distribution\nis itself a random variable that is gamma-distributed with \u03bb \u223c \u03b3(\u03bd, \u03bd\n\u02dc\u03bc) with shape parameter\n\u03bd and expectation \u02dc\u03bc. given \u03bb, it is assumed that \u02dcy is poisson-distributed, \u02dcy|\u03bb \u223c p(\u03bb). then\nthe marginal distribution of \u02dcy is given by (3.5). since it is often more appropriate to assume\nthat the total counts result from heterogeneous sources with individual parameters, the negative\nbinomial model is an attractive alternative to the poisson model. from the variance of the\ngamma-distribution \u02dc\u03bc2/\u03bd, it is seen that for \u03bd \u2192 \u221e the mixture of poisson distributions\nshrinks to just one poisson distribution and one obtains the poisson model as the limiting case.\nthe expectation and variance of the negative binomial are given by\n\ne(\u02dcy) = \u02dc\u03bc,\n\nvar(\u02dcy) = \u02dc\u03bc + \u02dc\u03bc2/\u03bd.\n\nthus, for \u03bd \u2192 \u221e, one obtains e(\u02dcy) = var(\u02dcy), which is in accordance with the poisson\ndistribution. the parameter \u03bd may be seen as an additional dispersion parameter that yields a\nlarger variation for small values. thus it is more appropriate to consider 1/\u03bd as an indicator for\nthe amount of variation.\n\nfor integer-valued \u03bd the negative binomial is also considered in the form\n\n\u03c0\u03bd(1 \u2212 \u03c0)y\n\ny = 0, 1, . . . ,\n\n(3.6)\n\n(cid:25)\n\n(cid:26)\n\nf(y) =\n\n\u03bd + y \u2212 1\n\u03bd \u2212 1\n\nwhere \u03c0 = \u03bd/(\u02dc\u03bc + \u03bd) \u2208 (0, 1) may be seen as an alternative parameter with a simple inter-\npretation. if independent bernoulli variables with probability of occurrence \u03c0 are considered,\nthen the negative binomial distribution (3.6) reflects the probability for the number of trials\nthat are necessary in addition to \u03bd to obtain \u03bd hits. the most familiar case is \u03bd = 1, where\none considers the number of trials (plus one) that are necessary until the first hit occurs. the\ncorresponding geometric distribution is a standard distribution, for example, in fertility studies\nwhere the number of trials until conception is modeled.\n\nwithin the exponential family framework one obtains with \u03c0 = \u03bd/(\u02dc\u03bc + \u03bd) from (3.5)\n\n(cid:29)\n\nf(\u02dcy) = exp\n\n[log(\u03c0) + (\u02dcy/\u03bd) log(1 \u2212 \u03c0)] /(1/\u03bd) + log\n\n\u03b3(\u02dcy + \u03bd)\n\n\u03b3(\u02dcy + 1)\u03b3(\u03bd)\n\n(cid:25)\n\n(cid:26)(cid:30)\n\n.\n\nfor fixed \u03bd one has a simple exponential family for the scaled response y = \u02dcy/\u03bd and the\ndispersion \u03c6 = 1/\u03bd. since \u02dcy/\u03bd is considered as the response, one has expectation \u03bc = e(y) =\n\u02dc\u03bc/\u03bd and therefore \u03b8(\u03bc) = log(1 \u2212 \u03c0) = log(\u03bc/(\u03bc + 1)) and b(\u03b8) = \u2212 log(1 \u2212 exp(\u03b8)). the\ncanonical link model that fulfills \u03b8(\u03bc) = \u03b7 is given by\n\n(cid:25)\n\n(cid:26)\n\nlog\n\n\u03bc\n\n\u03bc + 1\n\n= \u03b7\n\nor \u03bc =\n\nexp(\u03b7)\n1 \u2212 exp(\u03b7) .\n\n "}, {"Page_number": 72, "text": "60\nthe canonical link may cause problems because, for \u03b7 \u2192 0, one has \u03bc \u2192 \u221e. for the log-link,\nlog(\u03bc) = \u03b7\n\nor \u03bc = exp(\u03b7); however, the predictor \u03b7 is not restricted.\n\nchapter 3. generalized linear models\n\nthe negative binomial response y = \u02dcy/\u03bd is scaled by the specified parameter \u03bd. thus, when\n\ntreated within the framework of glms, the parameter has to be fixed in advance.\n\n3.4 further concepts\n3.4.1 means and variances\nthe distribution of the responses is assumed to be in the exponential family f(yi|\u03b8i, \u03c6i) =\nexp{(yi\u03b8i \u2212 b(\u03b8i))/\u03c6i + c(yi, \u03c6i)}. in the previous sections examples have been given for\nthe dependence of the natural parameters \u03b8i on \u03bci and the parameters that characterize the\ndistribution. for example, for the bernoulli distribution one obtains \u03b8i = \u03b8(\u03bci) in the form\n\u03b8i = log(\u03bci/(1 \u2212 \u03bci)), and since \u03bci = \u03c0i, one has \u03b8i = log(\u03c0i/(1 \u2212 \u03c0i)).\n\nin general, in the exponential families the mean is directly related to the function b(\u03b8i) in\n\nthe form\n\n(cid:3)\n\n\u03bci = b\n\n(\u03b8i) = \u2202b(\u03b8i)/\u2202\u03b8,\n\nand for the variances one obtains\n\ni = var(yi) = \u03c6ib\n\u03c32\n\n(cid:3)(cid:3)\n\n(\u03b8i) = \u03c6i\u22022b(\u03b8i)/\u2202\u03b82.\n\n(3.7)\n\n(3.8)\n\nthus the variances are composed from the dispersion parameter \u03c6i and the so-called vari-\n(cid:3)(cid:3)(\u03b8i). as is seen from (3.7) and (3.8), in glms there is a strict link between\nance function b\nthe mean \u03bci and the variance since both are based on derivatives of b(\u03b8). because \u03b8i de-\npends on the mean through the functional form \u03b8i = \u03b8(\u03bci), the variance function is a func-\ntion of the mean, that is, v(\u03bci) = \u22022b(\u03b8i)/\u2202\u03b82, and the variance can be written as \u03c32\ni =\n\u03c6iv(\u03bci). for the normal distribution one obtains v(\u03bci) = 1, and for the poisson v(\u03bci) = \u03bci\n(see table 3.1).\n\nthe link between the mean and variance includes the dispersion parameter \u03c6i. however, the\nlatter is not always an additional parameter. it is fixed for the exponential, bernoulli, binomial,\nand poisson distributions. only for the normal, gamma, negative binomial, and inverse gaus-\nsian is it a parameter that may be chosen data-dependently. in all these cases the dispersion has\nthe general form\n\n\u03c6i = \u03c6ai,\n\nwhere ai is known with ai = 1/mi for the binomial distribution and ai = 1 otherwise. the\nparameter \u03c6 is the actual dispersion that is known (\u03c6 = 1 for exponential, bernoulli, binomial,\npoisson) or an additional parameter. the only case where ai (cid:8)= 1 is the binomial distribution,\nwhich may be considered as replications of bernoulli variables.\n\n(cid:3)\n\nmeans and variances\n\u03bci = b\ni = \u03c6ib\n\u03c32\n\n(\u03b8i) = \u03c6iv(\u03bci)\n\n(\u03b8i)\n(cid:3)(cid:3)\n\n "}, {"Page_number": 73, "text": "3.4. further concepts\n\n61\n\ntable 3.1: exponential family of distributions.\n\n(cid:3)\n\n(cid:4)\n\nf (yi|\u03b8i, \u03c6i) = exp\n\nyi\u03b8i\u2212b(\u03b8i)\n\n\u03c6i\n\n+ c(yi, \u03c6i)\n\n, \u03c6i = \u03c6ai\n\n(a) components of the exponential family\n\ndistribution\n\nnotation\n\n\u03c6\n\n\u03c32\n\n1\n\n1\n\u03bd\n\n1/\u03bb\n\n1\n\n1\n\n1\n\n1\n\u03bd\n\nai\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\nmi\n1\n\n1\n\nnormal\nexponential\n\ngamma\ninverse\ngaussian\nbernoulli\nbinomial\n(rescaled)\npoisson\nnegative\nbinomial\n(rescaled)\n\ndistribution\n\nnormal\n\nexponential\n\ngamma\ninverse\ngaussian\n\nbernoulli\n\nbinomial\npoisson\nnegative\nbinomial\n\nn(\u03bci, \u03c32)\ne(\u03bbi)\n\u03b3(\u03bd, \u03bd\n\u03bci\n\n)\n\nig(\u03bci, \u03bb)\n\nb(1, \u03c0i)\n\nb(mi, \u03c0i)/mi\n\np(\u03bbi)\n\n\u03bci\n\n\u03bci\n\n1/\u03bbi\n\n\u03bci\n\n\u03c0i\n\n\u03c0i\n\n\u03bbi\n\n\u03b8(\u03bci)\n\n\u03bci\n\u22121/\u03bci\n\u22121/\u03bci\n\u22121/(2\u03bc2\ni )\nlog( \u03bci\n1\u2212\u03bci\n)\n\n)\n\nlog( \u03bci\n1\u2212\u03bci\nlog(\u03bci)\n\nb(\u03b8i)\n\u03b82\ni /2\n\u2212 log(\u2212\u03b8i)\n\u2212 log(\u2212\u03b8i)\n\u2212(\u22122\u03b8i)1/2\nlog(1 + exp(\u03b8i))\n\nlog(1 + exp(\u03b8i))\n\nexp(\u03b8i)\n\nnb(\u03bd, \u03bd(1\u2212\u03c0i)\n\n\u03c0i\n\n)/\u03bd\n\n\u03bd(1\u2212\u03c0i)\n\n\u03c0i\n\n/\u03bd\n\nlog( \u03bci\n\n\u03bci+1 ) \u2212 log(1 \u2212 e\u03b8)\n\n\u03bci = b\n\n(cid:2)\n\n(\u03b8i)\n\n\u03b8i\n\n\u03b8i\n\n\u03bci = \u03b8i\n\u03bci = \u2212 1\n\u03bci = \u2212 1\n\u03bci = (\u22122\u03b8)\n\u03bci = exp(\u03b8i)\n1+exp(\u03b8i)\n\u03bci = exp(\u03b8i)\n1+exp(\u03b8i)\n\u03bbi = exp(\u03b8i)\n\n\u22121/2\n\n(b) expectation and variance\n\n(cid:2)(cid:2)\n\n(\u03b8i)\n\nvar. fct. b\n\n1\n\n\u03bc2\ni\n\n\u03bc2\ni\n\n\u03bc3\ni\n\u03c0i(1 \u2212 \u03c0i)\n\u03c0i(1 \u2212 \u03c0i)\n\u03bbi\n\n(cid:2)(cid:2)\n\n(\u03b8i)\n\nvariance \u03c6i b\n\u03c32\n\n\u03bc2\ni\n\u03bc2\ni\n\u03bd\n\n\u03bc3\ni /\u03bb\n\u03c0i(1 \u2212 \u03c0i)\n\n\u03c0i(1 \u2212 \u03c0i)\n\n1\nmi\n\u03bbi\n\n\u03bci = exp(\u03b8i)\n1\u2212exp(\u03b8i)\n\n\u03bc(1 + \u03bc)\n\n\u03bc(1+\u03bc)\n\n\u03bd\n\n3.4.2 canonical link\n\nthe choice of the link function depends on the distribution of the response. for example,\nif y is non-negative, a link function is appropriate that specifies non-negative means without\nrestricting the parameters. for each distribution within the simple exponential family there is\none link function that has some technical advantages, the so-called canonical link. it links the\nlinear predictor directly to the canonical parameter in the form\n\n\u03b8i = xt\n\ni \u03b2.\n\nsince \u03b8i is determined as a function \u03b8(\u03bci), the canonical link g may be derived from the general\nform g(\u03bci) = xt\ni \u03b2 as the transformation that transforms \u03bci to \u03b8i. in table 3.1 the canonical\n\n "}, {"Page_number": 74, "text": "62\n\nchapter 3. generalized linear models\n\nlinks are given, and one obtains, for example,\n\ng(\u03bc) = \u03bc\ng(\u03bc) = log(\u03c0/(1 \u2212 \u03c0))\ng(\u03bc) = \u22121/\u03bc\n\nfor the normal distribution,\nfor the bernoulli distribution,\nfor the gamma-distribution.\n\nthe last example shows that the canonical link might not always be the best choice because\ni \u03b2 or \u03bci = \u22121/xt\n\u22121/\u03bci = xt\ni \u03b2 implies severe restrictions on \u03b2 arising from the restriction\nthat \u03bci has to be non-negative.\n\n3.4.3 extensions including offsets\nwhen modeling insurance claims or numbers of cases yi within a time interval, the time interval\nmay depend on i and therefore may vary across observations. with \u03b4i denoting the underlying\ntime interval for observation yi, one may assume poisson-distributed responses yi \u223c p(\u03b4i\u03bbi),\nwhere \u03bbi is the underlying intensity for one unit of time, which may be any time unit like\nminutes, days, or months. a sensible approach to modeling will not specify the expectation\nof yi, which is \u03b4i\u03bbi, but the intensity \u03bbi in dependence on covariates and include the time\nintervals as known constants. the model\n\nyields for the expectation \u03bci = e(yi)\n\n\u03bbi = exp(xt\n\ni \u03b2)\n\n\u03bci = \u03b4i\u03bbi = exp(log(\u03b4i) + xt\n\ni \u03b2).\n\nsince yi follows a poisson distribution one has a glm but with a known additive constant in\nthe predictor. constants of this type are called offsets; they are not estimated but considered as\nfixed and known. in the special case where all observations are based on the same length of the\ntime interval, that is, \u03b4i = \u03b4, the offset log(\u03b4i) is omitted because it cannot be distinguished\nfrom the intercept within xt\n\ni \u03b2. for more examples see section 7.4.\n\n3.5 modeling of grouped data\nin the previous sections observations have been given in the ungrouped form (yi, xi), i =\n1, . . . , n. often, for example, if covariates are categorial or in experimental studies, several of\nthe covariate values x1, . . . , xn will be identical. thus the responses for fixed covariate vectors\nmay be considered as replications with identical mean. by relabeling the data one obtains the\nform\n\n(yij, xi),\n\ni = 1, . . . , n,\n\nj = 1, . . . , ni,\n\nwhere observations yi1, . . . , yini have a fixed covariate vector xi with ni denoting the sample\nsize at covariate value xi, yielding the total sum of observations n = n1 + . . . + nn . since\nmeans depend on covariates, one has\n\n\u03bci = \u03bcij = e(yij) = h(xt\n\ni \u03b2),\n\nj = 1, . . . , ni,\n\nand also the natural parameter \u03b8i = \u03b8(\u03bci) depends on i only. let the dispersion parameter\n\u03c6i = \u03c6 be constant over replications yij, j = 1, . . . , ni. then one obtains for the mean over\nindividual responses at covariate value xi, \u00afyi = 1\nni\n(\u00afyi\u03b8i \u2212 b(\u03b8i))\n\nni\nj=1 yij, the density or mass function\n\n(cid:29)\n\n(cid:30)\n\n+ \u00afc(yi1, . . . , yini , \u03c6)\n\n,\n\n(3.9)\n\nf(\u00afyi) = exp\n\n(cid:14)\n\n\u03c6/ni\n\n "}, {"Page_number": 75, "text": "3.6. maximum likelihood estimation\n\n63\n\nwhere \u00afc(. . .) is a modified normalizing function (exercise 3.3). thus, the mean has an exponen-\ntial family distribution with the same natural parameter \u03b8i and function b(\u03b8i) as for ungrouped\ndata. however, the dispersion parameter has changed. with the value\n\n\u03c6i = \u03c6/ni\n\nit reflects the reduced dispersion due to considering the mean \u00afyi across ni observations. for\ngrouped data one has\n\n\u03bci = e(\u00afyi) = e(yij) = b\n\n(cid:3)\n\n(\u03b8i)\n\ni = var(\u00afyi) =\n\u03c32\n\n1\nni\n\nvar(yij) = \u03c6\nni\n\n(cid:3)(cid:3)\n\nb\n\n(\u03b8i).\n\n(cid:3)(cid:3)(\u03b8(\u03bci)) is the same as for ungrouped observations, but the\n\nthe variance function v(\u03bci) = b\nvariance is var(\u00afyi) = (\u03c6/ni)v(\u03bci).\n\nconsequently, one obtains for grouped data, considering \u00afy1, . . . , \u00afyn as responses, a glm\nwith the dispersion given by \u03c6i = \u03c6/ni. this is exactly what happens in the transition from\nthe bernoulli response to the scaled binomial response. the binomial response may be seen as\ngrouped data from bernoulli responses with local sample size given by ni.\n\nin the grouped case we will use n as the number of grouped observations with local sample\nin both cases the\n\nsizes n1, . . . , nn . for ungrouped data the number of observations is n.\ndispersion parameter is denoted by \u03c6i.\n\nin general, the mean yi over replications yij, . . . , yini does not have the same type of dis-\ntribution as the original observations yij. for example, in poisson or binomially distributed\nresponses the mean is not integer-valued and thus is not poisson or binomially distributed,\nni\nrespectively. however, for these distributions the sum \u02dcyi =\nj=1 yij has the same type of\ndistribution as the replications.\n\n(cid:14)\n\n3.6 maximum likelihood estimation\nfor glms the most widely used method of estimation is maximum likelihood. the basic\nprinciple is to construct the likelihood of the unknown parameters for the sample data, where\nthe likelihood represents the joint probability or probability density of the observed data, con-\nsidered as a function of the unknown parameters. maximum likelihood (ml) estimation for\nall glms has a common form. this is due to the assumption that the responses come from\nan exponential family. the essential feature of the simple exponential family with density\nf(yi|\u03b8i, \u03c6i) = exp{(yi\u03b8i \u2212 b(\u03b8i))/\u03c6i + c(yi, \u03c6i)} is that the mean and variance are given by\n\ne(yi) = \u2202b(\u03b8i)/\u2202\u03b8,\n\nvar(yi) = \u03c6i\u22022(\u03b8i)/\u2202\u03b82,\n\nwhere the parameterization is in the canonical parameter \u03b8i. as will be seen, the likelihood and\nits logarithm, the log-likelihood, are determined by the assumed mean and variance.\n\nlog-likelihood and score function\nfrom the exponential family one obtains for independent observations y1, . . . , yn the log-\nlikelihood\n\nn(cid:7)\n\nn(cid:7)\n\nl(\u03b2) =\n\nli(\u03b8i) =\n\n(yi\u03b8i \u2212 b(\u03b8i))/\u03c6i,\n\nwhere the term c(yi, \u03c6i) is omitted because it does not depend on \u03b8i and therefore not on \u03b2.\nfor the maximization of the log-likelihood one computes the derivation s(\u03b2) = \u2202l(\u03b2)/\u2202\u03b2,\n\ni=1\n\ni=1\n\n "}, {"Page_number": 76, "text": "64\n\nchapter 3. generalized linear models\n\nwhich is called the score function. for the computation it is useful to consider the parameters\nas resulting from transformations in the form \u03b8i = \u03b8(\u03bci), \u03bci = h(\u03b7i), \u03b7i = xt\ni \u03b2. one has the\ntransformation structure\n\n\u03b7i\n\nh\n(cid:2)\ng = h\n\n\u22121\n\n\u03bci\n\n\u03b8\n(cid:2)\n\u03bc = \u03b8\n\n\u22121\n\n\u03b8i\n\nyielding \u03b8i = \u03b8(\u03bci) = \u03b8(h(\u03b7i)). then the score function s(\u03b2) = \u2202l(\u03b2)/\u2202\u03b2 is given by\n\ns(\u03b2) = \u2202l(\u03b2)\n\u2202\u03b2\n\n=\n\n\u2202li(\u03b8i)\n\n\u2202\u03b8(\u03bci)\n\n\u2202h(\u03b7i)\n\n\u2202\u03b8\n\n\u2202\u03bc\n\n\u2202\u03b7\n\n\u2202\u03b7i\n\u2202\u03b2\n\n.\n\nn(cid:7)\n\ni=1\n\nwith \u03bci = \u03bc(\u03b8i) denoting the transformation of \u03b8i into \u03bci one obtains\n\n(cid:3)\n\n(cid:25)\n= (yi \u2212 b\n\u2202\u03bc(\u03b8i)\n\n=\n\n\u2202li\n\u2202\u03b8\n\u2202\u03b8(\u03bci)\n\n\u2202\u03bc\n\n(cid:26)\u22121\n(cid:26)\u22121\n(\u03b8i))/\u03c6i = (yi \u2212 \u03bci)/\u03c6i,\n\n(cid:25)\n\n=\n\n\u22022b(\u03b8i)\n\n\u2202\u03b82\n\n= \u03c6i/ var(yi),\n\n\u2202\u03b7i\n\u2202\u03b2\n\n= xi,\n\nand therefore the score function\n\ns(\u03b2) =\n\nn(cid:7)\n\ni=1\n\nsi(\u03b2) =\n\nxi\n\n\u2202h(\u03b7i)\n\n\u2202\u03b7\n\n(yi \u2212 \u03bci)\nvar(yi) .\n\nwith \u03c32\n\ni = \u03c6iv(\u03bci) = var(yi), the estimation equation s(\u02c6\u03b2) = 0 has the form\n\n\u2202\u03b8\n\nn(cid:7)\n\ni=1\n\nn(cid:7)\n\ni=1\n\nxi\n\n\u2202h(\u03b7i)\n\n\u2202\u03b7\n\n(yi \u2212 \u03bci)\n\u03c6iv(\u03bci)\n\n= 0.\n\n(3.10)\n\nin (3.10) the response (or link) function is found in the specification of the mean \u03bci = h(xt\ni \u03b2)\nand in the derivative \u2202h(\u03b7i)/\u2202\u03b7, whereas from higher moments of the distribution of yi only\ni = \u03c6iv(\u03bci) is needed. since \u03c6i = \u03c6ai, the dispersion parameter \u03c6 may be\nthe variance \u03c32\ncanceled out and the estimate \u02c6\u03b2 does not depend on \u03c6.\n\nfor the canonical link the estimation equation simplifies. since \u03b8i = \u03b7i = xt\n\ni \u03b2, the score\n\n(cid:14)\n\nfunction reduces to s(\u03b2) =\n\ni(\u2202li/\u2202\u03b8)(\u2202\u03b7i/\u2202\u03b2) and one obtains\n\nn(cid:7)\n\ns(\u03b2) =\n\nxi(yi \u2212 \u03bci)/\u03c6i.\n\ni=1\n\nin particular, one has \u2202h(\u03b7i)/\u2202\u03b7 = var(yi)/\u03c6i if the canonical link is used.\n\nin matrix notation the score function is given by\n\ns(\u03b2) = x t d \u03c3\n\n\u22121(y \u2212 \u03bc),\n\nwhere x t = (x1, . . . , xn) is the design matrix, d = diag (\u2202h(\u03b71)/\u2202\u03b7, . . . , \u2202h(\u03b7n)/\u2202\u03b7)\nn) is the covariance matrix, and\nis the diagonal matrix of derivatives, \u03c3 = diag (\u03c32\nyt = (y1, . . . , yn), \u03bct = (\u03bc1, . . . , \u03bcn) are the vectors of observations and means. sometimes\n\u22121dt , yielding s(\u03b2) =\nit is useful to combine d and \u03c3 into the weight matrix w = d \u03c3\nx t w d\n\n\u22121(y \u2212 \u03bc) and f (\u03b2) = x t w x.\n\n1, . . . , \u03c32\n\n "}, {"Page_number": 77, "text": "3.6. maximum likelihood estimation\n\n65\n\ninformation matrix\nin maximum likelihood theory the information matrix determines the asymptotic variance. the\nobserved information matrix is given by\n\nf obs(\u03b2) = \u2212 \u22022l(\u03b2)\n\n\u2202\u03b2\u2202\u03b2t =\n\n(cid:26)\n\n(cid:25)\n\u2212 \u22022l(\u03b2)\n\u2202\u03b2i\u2202\u03b2j\n\n.\n\ni,j\n\nits explicit form shows that it depends on the observations and therefore is random. the (ex-\npected) information or fisher matrix, which is not random, is given by\n\nf (\u03b2) = e(f obs(\u03b2)).\n\nfor the derivation it is essential that e(s(\u03b2)) = 0 and that e(\u2212\u22022li/\u2202\u03b2\u2202\u03b2t ) = e((\u2202li/\u2202\u03b2)\n(\u2202li/\u2202\u03b2t )), which holds under general assumptions (see, for example, cox and hinkley,\n1974). thus one obtains\n\nf (\u03b2) = e\n\nsi(\u03b2)si(\u03b2)t\n\n\"\nn(cid:7)\nn(cid:7)\n\ni=1\n\n(cid:25)\n\n=\n\nxixt\ni\n\ni=1\n\n\u2202h(\u03b7i)\n\n\u2202\u03b7\n\n\"\nn(cid:7)\n\ni=1\n\n#\n\n(cid:26)\n\n= e\n\n2\n\n/\u03c32\ni ,\n\n(cid:25)\n\n(cid:26)\n\nxixt\ni\n\n\u2202h(\u03b7i)\n\n\u2202\u03b7\n\n2 (yi \u2212 \u03bci)2\nvar(yi)2\n\n#\n\ni = var(yi). by using the design matrix x one obtains the information matrix f (\u03b2)\n\nwhere \u03c32\nin the form\n\n(cid:25)(cid:23)\n\n(cid:23)\n\nf (\u03b2) = x t w x,\n\n1, . . . ,\n\n\u2202h(\u03b7n)\n\n\u2202\u03b7\n\n2\n\n/\u03c32\nn\n\n(cid:24)\n\n(cid:26)\n\n(cid:24)\n/\u03c32\n\u22121dt .\n\n2\n\nwhere w = diag\n\n\u03b7\nthe matrix form w = d \u03c3\n\n\u2202h(\u03b71)\n\nfor the canonical link the corresponding simpler form is\n\nis a diagonal weight matrix that has\n\nf (\u03b2) =\n\nxixt\n\ni \u03c32\n\ni /\u03c62\n\ni = x t w x,\n\nwith weight matrix w = (\u03c32\nn) and the observed information is identical to the\ninformation matrix, f obs(\u03b2) = f (\u03b2). it is immediately seen that for the normal distribution\nmodel with a (canonical) identity link one has with \u03c6i = \u03c32\n\ni = \u03c32 the familiar form\n\n1, . . . , \u03c32\n\nn/\u03c62\n\n1/\u03c62\n\nf (\u03b2) =\n\nxixt\n\ni /\u03c32 = x t x/\u03c32.\n\nin this case it is well known that the covariance of the estimator \u02c6\u03b2 is given by cov(\u02c6\u03b2) =\n\u03c32(x t x)\u22121 = f (\u03b2)\u22121. for glms the result holds only asymptotically (n \u2192 \u221e). with\n\ni=1\n\n\u02c6cov(\u02c6\u03b2) \u2248 (x\n\n(cid:3) \u02c6w x)\n\n\u22121,\n\nwhere \u02c6w means the evaluation of w at \u02c6\u03b2, that is, \u2202h(\u03b7i)/\u2202\u03b7 is replaced by \u2202h(\u02c6\u03b7i)/\u2202\u03b7, \u02c6\u03b7i =\nxt\ni\n\n\u02c6\u03b2, and \u03c32\nit should be noted that in the grouped observations case the form of the likelihood, score\nfunction, and fisher matrix are the same; only the summation index n has to be replaced by n.\n\ni = \u03c6iv(\u02c6\u03bci), \u02c6\u03bci = h(\u02c6\u03b7i).\n\nn(cid:7)\n\ni=1\n\nn(cid:7)\n\n "}, {"Page_number": 78, "text": "66\n\nchapter 3. generalized linear models\n\nlog-likelihood\n\nscore function\n\nn(cid:7)\n(yi\u03b8i \u2212 b(\u03b8i))/\u03c6i\n\nl(\u03b2) =\n\ni=1\n\nn(cid:7)\n\ns(\u03b2) = \u2202l(\u03b2)\n\u2202\u03b2\n\n= x t d\u03c2\n\n\u2202h(\u03b7i)\n\n(yi \u2212 \u03bci)\n\ni=1\n\nxi\n\n\u2202\u03b7\n\n\u03c32\ni\n\n=\n\u22121(y \u2212 \u03bc) = x t w d\n(cid:25)\n\n(cid:26)\n\nn(cid:7)\n\n=\n\nxixt\ni\n\u22121dx = x t w x\n\ni=1\n\n(cid:25)\n\u2212 \u22022l(\u03b2)\n\u2202\u03b2\u2202\u03b2t\n\n\u22121(y \u2212 \u03bc)\n(cid:26)\n\n2\n\n\u2202h(\u03b7i)\n\n\u2202\u03b7\n\n/\u03c32\ni\n\n= x t d\u03c2\n\ninformation matrix\n\nf (\u03b2) = e\n\nif \u03c6 is an unknown (normal, gamma-distribution), the moments estimate is\n\n\u02c6\u03c6 =\n\n1\nn \u2212 p\n\n(yi \u2212 \u02c6\u03bci)2\nv(\u02c6\u03bci)\n\n.\n\nn(cid:7)\n\ni=1\n\n(cid:7)\n\ni\n\nfor the normal model \u02c6\u03c6 reduces to the usual unbiased and consistent estimates \u02c6\u03c6 = \u02c6\u03c32 =\n\n(yi \u2212 \u02c6\u03bci)2/(n \u2212 p). for the gamma-distribution one obtains\n(cid:26)\n\n(cid:25)\n\nn(cid:7)\n\n\u02c6\u03c6 =\n\n1\n\u02c6\u03bd\n\n=\n\n1\nn \u2212 p\n\nyi \u2212 \u02c6\u03bci\n\n2\n\n.\n\n\u02c6\u03bci\n\ni=1\n\nin the approximation \u03c6 has to be replaced by \u02c6\u03c6 when computing f (\u02c6\u03b2). the likelihood, score\nfunction, and fisher matrix are summarized in a box. in the matrix form derivations are col-\nlected in the matrix d = diag (\u2202h(\u03b71)/\u2202\u03b7, . . . , \u2202h(\u03b7n)/\u2202\u03b7). by using w and d, the de-\npendence on \u03b2 is suppressed, and actually one has w = w (\u03b2), d = d(\u03b2).\nunder regularity conditions the ml estimate \u02c6\u03b2 exists and is unique asymptotically (n \u2192\n\u221e). it is consistent and the distribution may be approximated by a normal distribution with\nthe covariance given by the inverse fisher matrix. more precisely, under assumptions that\nn(\u02c6\u03b2 \u2212 \u03b2) asymptoti-\nensure the convergence of f (\u02c6\u03b2)/n to a limit f 0(\u02c6\u03b2), one obtains for\ncally a normal distribution n(0, f 0(\u02c6\u03b2)\u22121). for finite n one uses the approximation cov(\u02c6\u03b2) \u2248\nf 0(\u02c6\u03b2)\u22121/n, which is approximated by f (\u02c6\u03b2)\u22121. for regularity conditions see haberman\n(1977) and fahrmeir and kaufmann (1985). bias correction by approximation of the first-order\nbias of ml estimates was investigated by cordeiro and mccullagh (1991) and firth (1993).\n\n\u221a\n\n(cid:14)\n\nf (\u02c6\u03b2) = x t \u02c6w x =\n\n\u02c6\u03b2 a\u223c n\nn\ni=1 xixt\ni\n\n(cid:27)\napproximation\n(cid:23)\n(cid:24)\n\u03b2, f (\u02c6\u03b2)\u22121\n\n(cid:28)\n\n,\n\n\u2202h(\u02c6\u03b7i)\n\n\u2202\u03b7\n\n2\n\n/( \u02c6\u03c6v(\u02c6\u03bci))\n\n "}, {"Page_number": 79, "text": "3.7. inference\n\n67\n\nthe unifying concept of glms may be seen in the common form of the log-likelihood, the\nscore function (which determines the estimation equation), and the information matrix (which\ndetermines the variances of estimators). specific models result from specific choices of\n\n- the link or response function, yielding the derivative matrix d, which contains \u2202h(\u03b7i)/\u2202\u03b7;\n\n- the distribution, yielding the covariance matrix \u03c3 , which contains \u03c32\n\ni = var(yi);\n\n- the explanatory variables, which determine the design matrix x.\n\nin glms these constituents may be chosen freely. in principle, any link function can be com-\nbined with any distribution and any set of explanatory variables. of course there are combina-\ntions of links and distributions that are more sensible than others.\n\n3.7 inference\nmain questions in inference concern\n\n- the adequacy of the model or goodness-of-fit of the model,\n\n- the relevance of explanatory variables,\n\n- the explanatory value of the model.\n\nin the following these questions are considered in a different order. first the deviance is in-\ntroduced, which measures the discrepancy between the observations and the fitted model. the\ndeviance is a tool for various purposes. the relevance of the explanatory variables may be in-\nvestigated by comparing the deviance of two models, the model that contains the variable in\nquestion and the model where this variable is omitted. moreover, for grouped observations the\ndeviance may be used as a goodness-of-fit statistic.\n\n3.7.1 the deviance\nwhen fitting a glm one wants some measure for the discrepancy between the fitted model and\nthe observations. the deviance is a measure for the discrepancy that is based on the likelihood\nratio statistic for comparing nested models. the nested models that are investigated are the\nglm that is under investigation and the most general possible model. this so-called saturated\nmodel fits the data exactly by assuming as many parameters as observations.\n\nlet l(y; \u02c6\u03bc, \u03c6) denote the maximum of the log-likelihood of the model where yt = (y1, . . . ,\n\u02c6\u03b2) represent the fitted values based\nyn) represents the data, \u02c6\u03bct = (\u02c6\u03bc1, . . . , \u02c6\u03bcn), \u02c6\u03bci = h(xt\ni\non the ml estimate \u02c6\u03b2; and the dispersion of observations has the form \u03c6i = \u03c6ai with known\nai. for the saturated model that matches the data exactly one has \u02c6\u03bc = y and the log-likelihood\nis given by l(y; y, \u03c6). with \u03b8(\u02c6\u03bci), \u03b8(yi) denoting the canonical parameters of the glm under\ninvestigation and the saturated model, respectively, the deviance is given by\n\nd(y, \u02c6\u03bc) = \u2212\u03c62{l(y; \u02c6\u03bc, \u03c6) \u2212 l(y; y, \u03c6)}\n\nn(cid:7)\n\n= 2\n\n{yi(\u03b8(yi) \u2212 \u03b8(\u02c6\u03bci)) \u2212 (b(\u03b8(yi)) \u2212 b(\u03b8(\u02c6\u03bci)))}/ai.\n\ni=1\n\nd(y, \u02c6\u03bc) is known as deviance of the model under consideration while d+(y, \u02c6\u03bc) = d(y, \u02c6\u03bc)/\u03c6\nis the so-called scaled deviance. the deviance is linked to the likelihood ratio statistic\n\n "}, {"Page_number": 80, "text": "68\n\nchapter 3. generalized linear models\n\ntable 3.2: deviances for several distributions.\n\ndistribution\n\ndeviance\n\nnormal\n\ngamma\n\nbernoulli\n\npoisson\n\nd(y, \u02c6\u03bc) =\n\nd(y, \u02c6\u03bc) = 2\n\nd(y, \u02c6\u03bc) = 2\n\nd(y, \u02c6\u03bc) = 2\n\n(cid:9)\n\n+\n\ni=1\n\ni=1\n\n(cid:8)\n\n(cid:7)\n\nyi\n\u02c6\u03bci\n\n(yi \u2212 \u02c6\u03bci)\n\nn(cid:5)\n(yi \u2212 \u02c6\u03bci)2\n(cid:6)\nn(cid:5)\n\u2212 log\nn(cid:5)\n(yi \u2212 \u02c6\u03bci)2/(\u02c6\u03bc2\ni yi)\n(cid:7)\nn(cid:5)\n+ (1 \u2212 yi) log\n(cid:7)\nn(cid:5)\n\u2212 [(yi \u2212 \u02c6\u03bci)]\n\n(cid:6)\n(cid:6)\n\nyi log\n\nyi log\n\ni=1\n\ni=1\n\n\u02c6\u03bci\n\nyi\n\u02c6\u03bci\nyi\n\u02c6\u03bci\n\ni=i\n\n(cid:7)\n\n(cid:6)\n\n1 \u2212 yi\n1 \u2212 \u02c6\u03bci\n\ninverse gaussian d(y, \u02c6\u03bc) =\n\n\u03bb = \u22122{l(y; \u02c6\u03bc, \u03c6) \u2212 l(y; y, \u03c6)}, which compares the current model to the saturated model\nby d(y, \u02c6\u03bc) = \u03c6\u03bb.\n\nsimple derivation yields the deviances given in table 3.2. for the normal model, the de-\n(cid:14)\nviance is identical to the error or residual sum of squares sse and the scaled deviance takes\nthe form sse /\u03c32. for the bernoulli distribution, one has \u03b8(\u03bci) = log(\u03bci/(1 \u2212 \u03bci)) and one\ni d(yi, \u02c6\u03c0i), where d(yi, \u02c6\u03c0i) = \u2212 log(1 \u2212 |yi \u2212 \u02c6\u03c0i|) (for more details\nobtains d(y, \u02c6\u03bc) = 2\nsee section 4.2). in the cases of the poisson and the gamma deviances, the last term given in\nbrackets [. . .] can be omitted if the model includes a constant term because then the sum over\nthe terms is zero.\n\nthe deviance as a measure of discrepancy between the observations and the fitted model\nmay be used in an informal way to compare the fit of two models. for example, two models\nwith the same predictor but differing link functions can be compared by considering which\none has the smaller deviance. however, there is no simple way to interpret the difference\nbetween the deviances of these models. this is different if the difference of deviances is used\nfor nested models, for example, to investigate the relevance of terms in the linear predictor. the\ncomparison of models with and without the term in question allows one to make a decision\nbased on significance tests with a known asymptotic distribution. the corresponding analysis\nof deviance (see section 3.7.2) generalizes the analysis of variance, which is in common use\nfor normal linear models.\n\nfor ungrouped data some care has to be taken in the interpretation as a goodness-of-fit mea-\nsure. as an absolute measure of goodness-of-fit, which allows one to decide if the model has\nsatisfactory fit or not, the deviance for ungrouped observations is appropriate only in special\ncases. for the interpretation of the value of the deviance it would be useful to have a bench-\nmark in the form of an asymptotic distribution. since the deviance may be derived as a likeli-\nhood ratio statistic, it is tempting to assume that the deviance is asymptotically \u03c72-distributed.\nhowever, in general, the deviance does not have an asymptotic \u03c72-distribution in the limit for\nn \u2192 \u221e. standard asymptotic theory of likelihood ratio statistics for nested models assumes\nthat the ranks of the design matrices that build the two models and therefore the degrees of\nfreedom are fixed for increasing sample size. in the present case this theory does not apply\nbecause the degrees of freedom of the saturated model increase with n. this is already seen in\nthe case of the normal distribution, where d(y, \u02c6\u03bc) = (n \u2212 p)\u02c6\u03c32 \u223c \u03c32\u03c72(n \u2212 p). for n \u2192 \u221e,\nthe limiting distribution does not have a \u03c72- distribution with fixed degrees of freedom. similar\neffects occur for binary data.\n\n "}, {"Page_number": 81, "text": "3.7. inference\n\n69\n\nthis is different if one considers the deviance of the binomial distribution or the poisson\ndistribution. the binomial distribution may be seen as a replication version of the bernoulli\ndistribution. thus, when the number of replications increases, ni \u2192 \u221e, the proportion yi is\nasymptotically normally distributed. for the poisson distribution, asymptotic normality of the\nobservations follows if \u03bci \u2192 \u221e for each observation. in these cases the \u03c72-distribution may\nbe used as an approximation (see section 3.8).\n\n3.7.2 analysis of deviance and the testing of hypotheses\nlet us consider the nested models \u02dcm \u2282 m, where m is a given glm with \u03bci = h(xt\ni \u03b2), and\n\u02dcm is a submodel that is characterized by the linear restriction c\u03b2 = \u03be, where c is a known\n(s \u00d7 p)-matrix with rank(c) = s \u2264 p and \u03be is an s-dimensional vector. this means that \u02dcm\ncorresponds to the null hypothesis h0 : c\u03b2 = \u03be, which specifies a simpler structure of the\npredictor.\n\nanalysis of deviance\nwith \u02dc\u03bct = (\u02dc\u03bc1, . . . , \u02dc\u03bcn) denoting the fitted values for the restricted model \u02dcm and \u02c6\u03bct =\n(\u02c6\u03bc1, . . . , \u02c6\u03bcn) denoting the fit for model m, one obtains the corresponding deviances\n\nd(m) = \u2212\u03c62{l(y, \u02c6\u03bc; \u03c6) \u2212 l(y, y; \u03c6)},\nd( \u02dcm) = \u2212\u03c62{l(y, \u02dc\u03bc; \u03c6) \u2212 l(y, y; \u03c6)}.\n\nthe difference of deviances\n\nd( \u02dcm|m) = d( \u02dcm) \u2212 d(m) = \u22122\u03c6{l(y, \u02dc\u03bc; \u03c6) \u2212 l(y, \u02c6\u03bc; \u03c6)}\n\n(3.11)\ncompares the fits of models \u02dcm and m. the difference of scaled deviances d( \u02dcm|m)/\u03c6 is\neqiuivalent to the likelihood ratio statistic for testing h0. similar to the partitioning of the\nsum of squares in linear regression, one may consider the partitioning of the deviance of the\nrestricted model \u02dcm into\n\nd( \u02dcm) = d( \u02dcm|m) + d(m).\n\nd( \u02dcm|m) gives the increase in discrepancy between the data and the fit if model \u02dcm is fit-\nted instead of the less restrictive model m. for normal distributions this corresponds to the\npartitioning of the sum of squares\n\nsse( \u02dcm) = sse( \u02dcm|m) + sse(m)\n\n(see section 1.4.6), which, for the special model where \u02dcm contains only an intercept, reduces\nto sst = ssr + sse. in the normal case one obtains for sse(m) a \u03c32\u03c72(n\u2212 p)-distribution,\nif m holds (p denotes the dimension of the predictor in m). if \u02dcm holds, sse( \u02dcm|m) and\nsse(m) are independent with sse( \u02dcm|m) \u223c \u03c32\u03c72(s) and sse( \u02dcm) \u223c \u03c32\u03c72(n + s \u2212 p). for\ntesting h0 one uses the f -statistic\n\n{sse( \u02dcm) \u2212 sse(m)}/s\n\n\u223c f(s, n \u2212 p),\nwhere \u02c6\u03c32 = sse(m)/(n \u2212 p). in the general case of glms one uses\n\n\u02c6\u03c32\n\nd( \u02dcm) \u2212 d(m)\n\n\u03c6\n\n= d( \u02dcm|m)\n\n\u03c6\n\n,\n\n "}, {"Page_number": 82, "text": "70\n\nchapter 3. generalized linear models\n\nwhich under mild restrictions is asymptotically \u03c72(s)-distributed. this means that the differ-\nence\n\nd( \u02dcm) \u2212 d(m) = d( \u02dcm|m)\n\nhas an asymptotically a \u03c6\u03c72(s)-distribution.\n\nwhen using the \u03c72-approximation the deviance has to be scaled by 1/\u03c6. for the binomial\n(\u03c6i = 1/ni, \u03c6 = 1) bernoulli, exponential, and poisson (\u03c6 = 1) distributions one may use\nthe difference d( \u02dcm) \u2212 d(m) directly, whereas for the normal, gamma, and inverse gaussian\ns d( \u02dcm|m)/ \u02c6\u03c6 has\nthe dispersion parameter has to be estimated. in the normal regression case 1\nf(s, n \u2212 p)-distribution. in the general case the approximation by the f-distribution may be\nused if \u02c6\u03c6 is consistent for \u03c6, has approximately a scaled \u03c72-distribution, and d( \u02dcm) \u2212 d(m)\nand \u02c6\u03c6 are approximately independent (jorgenson, 1987). in analogy to the anova table, in\nnormal regression one obtains a table for the analysis of deviance (see table 3.3).\n\ntable 3.3: analysis of deviance table.\n\nd( \u02dcm )\nd(m )\n\ndf\nn \u2212 p + s\nn \u2212 p\n\ncond. deviance\n\ndf\n\nd( \u02dcm|m )\n\ns\n\nit should be noted that only the difference of deviances d( \u02dcm|m) has asymptotically a \u03c6\u03c72(s)-\ndistribution. the degrees of freedom of d( \u02dcm) have the basic structure \"number of observations\nminus number of fitted parameters.\" in \u02dcm, by considering an additional s-dimensional restric-\ntion, the effective parameters in the model are reduced to p \u2212 s, yielding df = n \u2212 (p \u2212 s) =\nn\u2212 p + s. in the case of grouped data, the deviances d( \u02dcm) and d(m) themselves are asymp-\ntotically distributed with d( \u02dcm) \u223c \u03c72(n \u2212p+s), and \u03c72(n \u2212p), where n denotes the number\nof grouped observations (see section 3.8). while d( \u02dcm) and d(m) are different for grouped\nand ungrouped data, the difference d( \u02dcm) \u2212 d(m) is the same.\n\nnext we give a summary of results on distributions for the classical linear case and the\n\ndeviances within the glm framework. for the classical linear model one has\n\nfor grouped data within the glm framework, one has the asymptotic distributions\n\nsse( \u02dcm)\n\n\u03c32\u03c72(n \u2212 p + s)\n\nif \u02dcm holds\n\n= sse( \u02dcm|m) +\n\n\u03c32\u03c72(s)\nif \u02dcm holds\n\nd( \u02dcm)\n\n\u03c6\u03c72(n \u2212 p + s)\nif \u02dcm holds\ngrouped data\n\n= d( \u02dcm|m) +\n\n\u03c6\u03c72(s)\n\nif \u02dcm holds\n\nsse(m)\n\u03c32\u03c72(n \u2212 p)\nif m holds\n\nd(m)\n\n\u03c6\u03c72(n \u2212 p)\nif m holds\ngrouped data\n\nthe approach may be used to test sequences of nested models,\n\nm1 \u2282 m2 \u2282 . . . \u2282 mm,\n\nby using the successive differences (d(mi) \u2212 d(mi+1))/\u03c6. the deviance of the most restric-\ntive model is given as sum of these differences:\n\nd(m1) = (d(m1) \u2212 d(m2)) + (d(m2) \u2212 d(m3))\n+ . . . + (d(mm\u22121) \u2212 d(mm)) + d(mm)\n\n= d(m1|m2) + . . . + d(mm\u22121|mm) + d(mm).\n\n "}, {"Page_number": 83, "text": "3.7. inference\n\n71\nthus the discrepancy of the model m1 is the sum of the \"conditional\" deviances d(mi|mi+1) =\nd(mi)\u2212 d(mi+1) and the discrepancy between the most general model mm and the saturated\nmodel. however, when one starts from a model mm and considers sequences of simpler mod-\nels, one should be aware that different sequences of submodels are possible (see section 4.4.2).\n\n3.7.3 alternative test statistics for linear hypotheses\nthe analysis of deviance tests if a model can be reduced to a model that has a simpler structure\nin the covariates. the simplified structure is specified by the null hypothesis h0 of the pair of\nhypotheses\n\nh0 : c\u03b2 = \u03be\n\nagainst h1 : c\u03b2 (cid:8)= \u03be,\n\nwhere rank(c) = s. alternative test statistics that can be used are the wald test and the score\nstatistic.\n\nwald test\nthe wald statistic has the form\n\n(cid:27)\nc \u02c6\u03b2 \u2212 \u03be\n\n(cid:28)\n\nt\n\n$\n\nc f\n\n%\u22121\n\n(cid:27)\n\n(cid:28)\n\n.\n\nc \u02c6\u03b2 \u2212 \u03be\n\n\u22121(\u02c6\u03b2) c t\n\nw =\n\nit uses the weighted distance between the unrestricted estimate c \u02c6\u03b2 of c\u03b2 and its hypothetical\nvalue \u03be under h0. the weight is derived from the distribution of the difference (c \u02c6\u03b2 \u2212 \u03be),\nfor which one obtains asymptotically cov(c \u02c6\u03b2 \u2212 \u03be) = c f\n\u22121(\u02c6\u03b2) c t . therefore, w is the\n\u22121(\u02c6\u03b2) c t )\u22121/2(c \u02c6\u03b2 \u2212 \u03be), and one obtains\nsquared length of the standardized estimate (c f\nfor w under h0 an asymptotic \u03c72(s)-distribution.\n\nan advantage of the wald statistic is that it is based on the ml estimates of the full\nmodel. therefore, it is not necessary to compute an additional fit under h0. this is why\nmost program packages give significance tests for single parameters in terms of the wald statis-\ntic. when a single parameter is tested with h0 : \u03b2j = 0, the corresponding matrix c is\nc = (0, 0, . . . , 1, . . . , 0). then the wald statistic has the simple form\n\nw =\n\n\u03b22\nj\n\u02c6ajj\n\n,\n\nwhere \u02c6ajj is the jth diagonal element of the estimated inverse fisher matrix f\nasymptotically \u03c72(1)-distributed, one may also consider the square root,\n\n\u22121. since w is\n\n\u221a\n\nz =\n\nw = \u03b2j(cid:16)\n(cid:16)\n\n,\n\n\u02c6ajj\n\nwhich follows asymptotically a standard normal distribution. thus, for single parameters, pro-\ngram packages usually give the standard error\n\n\u02c6ajj and the p-value based on z.\n\nscore statistic\nthe score statistic is based on the following consideration: the score function s(\u03b2) for the\nunrestricted model is the zero vector if it is evaluated at the unrestricted ml estimate \u02c6\u03b2. if,\nhowever, \u02c6\u03b2 is replaced by the mle \u02dc\u03b2 under h0, s(\u02dc\u03b2) will be significantly different from zero\nif h0 is not true. since the covariance of the score function is approximately the fisher matrix,\none uses the score statistic,\n\n(cid:27)\n\n(cid:28)\n\n(cid:27)\n\n(cid:28)\n\n(cid:27)\n\n(cid:28)\n\n\u02dc\u03b2\n,\nwhich is the squared weighted score function evaluated at \u02dc\u03b2.\n\nu = s\n\n\u02dc\u03b2\n\n\u02dc\u03b2\n\nf\n\ns\n\nt\n\n\u22121\n\n "}, {"Page_number": 84, "text": "72\n\nchapter 3. generalized linear models\n\nan advantage of the wald and score statistics is that they are properly defined for models\nwith overdispersion since only the first and second moments are involved. all test statistics\nhave the same asymptotic distribution. if they are differing strongly, that may be seen as a\nhint that the conditions for asymptotic results may not hold. a survey on asymptotics for test\nstatistics was given by fahrmeir (1987).\n\ntest statistics for linear hypotheses\nh1 : c\u03b2 (cid:8)= \u03be\n\nh0 : c\u03b2 = \u03be\nwith\n\nrank(c) = s\n\nlikelihood ratio statistic\n\n\u03bb = \u22122{l(y; \u02dc\u03bc, \u02c6\u03c6) \u2212 l(y, \u02c6\u03bc, \u02c6\u03c6)}\n= (d(y; \u02dc\u03bc, \u02c6\u03c6) \u2212 d(y, \u02c6\u03bc, \u02c6\u03c6))/ \u02c6\u03c6\n\nwald statistic\n\nw = (c \u02c6\u03b2 \u2212 \u03be)t (cf\n\n\u22121(\u02c6\u03b2)c t )\n\n\u22121(c \u02c6\u03b2 \u2212 \u03be)\n\nscore statistic\n\napproximation\n\nu = s(\u02dc\u03b2)t f\n\n\u22121(\u02dc\u03b2)s(\u02dc\u03b2)\n\n\u03bb, w, u \u223c \u03c72(s)\n\n3.8 goodness-of-fit for grouped observations\nit has already been mentioned that for grouped observations the deviance has an asymptotic\n\u03c72-distribution. hence, it may be used to test the model fit.\n\n3.8.1 the deviance for grouped observations\nthe analysis of deviance and alternative tests provide an instrument that helps to decide if a\nmore parsimonious model \u02dcm may be chosen instead of the more general model m, where\n\u02dcm \u2282 m. the test statistics may be seen as tools to investigate the fit of model \u02dcm given model\nm. however, they are of limited use for investigating if a model is appropriate for the given\ndata, that is, the model fit compared to the data. the only possibility would be to choose m as\nthe saturated model. but then the deviance has no fixed distribution in the limit.\n\na different situation occurs if replications are available.\n\nif, for a fixed covariate vector\nxi, independent replications yi1, . . . , yini are observed, the mean across replications \u00afyi =\nj yij/ni again represents a glm and the deviance for the means \u00afy1, . . . , \u00afyn may be used.\nfor grouped data with response \u00afyi, i = 1, . . . , n, the essential difference is that the scale pa-\nrameter is given as \u03c6i = \u03c6/ni, where \u03c6 is the dispersion for the single observations. since\n\n(cid:14)\n\n "}, {"Page_number": 85, "text": "3.8. goodness-of-fit for grouped observations\n\n73\n\ngrouped observations yi, . . . yini share the same predictor value xi, the log-likelihood is given\nby\n\nn(cid:7)\n\nni(cid:7)\n\ni=1\n\nj=1\n\nl =\n\n(yij\u03b8i \u2212 b(\u03b8i))/\u03c6 + c(yij, \u03c6) =\n\nn(cid:7)\n\n\u00afyi\u03b8i \u2212 b(\u03b8i)\n\n\u03c6/ni\n\ni=1\n\nn(cid:7)\n\nni(cid:7)\n\ni=1\n\nj=1\n\n+\n\nc(yij, \u03c6).\n\nthus maximization with respect to \u03b2 yields the same results if l is maximized in the ungrouped\nor grouped form. the deviance, however, changes for grouped data because the saturated model\nfor data (\u00afyi, xi), i = 1, . . . , n, means that only the n observations \u00afy, . . . , \u00afyn have to be fitted\nperfectly.\n\u02c6\u03b2), where \u03bci = e(\u00afyi) =\n\nwith \u00afyt = (\u00afy1, . . . , \u00afyn ), \u02c6\u03bct = (\u02c6\u03bc1, . . . , \u02c6\u03bcn ), and \u02c6\u03bci = h(xt\n\ni\n\ne(yij), j = 1, . . . , ni, the deviance for grouped observations has the form\n\nd(\u00afy, \u02c6\u03bc) = \u2212 \u03c6 2{l(\u00afy; \u02c6\u03bc, \u03c6) \u2212 l(\u00afy; \u00afy, \u03c6)}\n\nn(cid:7)\n\n= 2\n\n{\u00afyi(\u03b8(\u00afyi) \u2212 \u03b8(\u02c6\u03bci)) \u2212 (b(\u03b8(\u00afyi)) \u2212 b(\u03b8(\u02c6\u03bci)))}ni.\n\ni=1\n\nthe deviances for various distributions are given in table 3.4. for ungrouped data with\nn = n, ni = 1, one obtains the deviances as given in table 3.2. the grouped deviance for\nbernoulli variables is equivalent to the deviance of the binomial distribution. the reason is\nobvious because the binomial distribution implicitly assumes replications.\n\ntable 3.4: deviances for grouped observations.\n\ndistribution\n\ndeviance for grouped observations\n\nnormal\n\nd(\u00afy, \u02c6\u03bc) =\n\ngamma\n\nd(\u00afy, \u02c6\u03bc) = 2\n\ninverse gaussian d(\u00afy, \u02c6\u03bc) =\n\n(cid:7)\n\n\u00afyi \u2212 \u02c6\u03bci\n\n\u02c6\u03bci\n\nni\n\ni=1\n\n(cid:7)\n\n(cid:6)\n\u2212 log\n\nn(cid:5)\nni(\u00afyi \u2212 \u02c6\u03bci)2\n(cid:6)\nn(cid:5)\n\u00afyi\nn(cid:5)\n\u02c6\u03bci\nni(\u00afyi \u2212 \u02c6\u03bci)2/(\u02c6\u03bc2\ni \u00afyi)\n(cid:7)\n(cid:6)\nn(cid:5)\n(cid:7)\nn(cid:5)\n\n(cid:6)\n(cid:6)\n\nni \u00afyi log\n\n\u00afyi\n\u02c6\u03bci\n\n(cid:6)\n\ni=1\n\ni=1\n\ni=1\n\n+\n\nbernoulli\n\npoisson\n\nd(\u00afy, \u02c6\u03bc) = 2\n\nd(\u00afy, \u02c6\u03bc) = 2\n\nni\n\n\u00afyi log\n\ni=1\n\n\u00afyi\n\u02c6\u03bci\n\n+ ni(1 \u2212 \u00afyi) log\n(cid:7)\n\u2212 (\u00afyi \u2212 \u02c6\u03bci)\n\n(cid:7)(cid:7)\n\n(cid:6)\n\n1 \u2212 \u00afyi\n1 \u2212 \u02c6\u03bci\n\nthe advantage of replications or grouped data is that for this kind of data the scaled de-\nviance or likelihood ratio statistic d(\u00afy, \u02c6\u03bc)/\u03c6 provides a goodness-of-fit statistic that may be\nused to test if the model is appropriate for the data. the maximal likelihood l(\u00afy; \u00afy, \u03c6) is\nthe likelihood of a model with n parameters, one per covariate value xi, where only the\nassumption of distribution with independent, identically distributed responses is made. thus\nd(\u00afy, \u02c6\u03bc)/\u03c6 may be used to test the current model, implying the form of the linear predic-\ntors and a specific link function, against the distribution model. under fixed cells asymptotics\n(n fixed, ni \u2192 \u221e, ni/n \u2192 ci, ci > 0) and regularity conditions one obtains for d(\u00afy, \u02c6\u03bc)/\u03c6\na limiting \u03c72-distribution with n \u2212 p degrees of freedom, where p denotes the dimension of\nthe predictor xi. if d(\u00afy, \u02c6\u03bc)/\u03c6 is larger than the 1 \u2212 \u03b1 quantile of \u03c72(n \u2212 p), the model is\nquestionable as a tool for investigating the connection between covariates and responses.\n\nit should be noted that the difference between likelihoods for different models and therefore\nthe analysis of variance yields the same results for ungrouped and grouped modeling. for two\n\n "}, {"Page_number": 86, "text": "74\nmodels \u02dcm \u2282 m and corresponding fits \u02c6\u03bci, \u02dc\u03bci , the difference d(\u00afy, \u02dc\u03bc) \u2212 d(\u00afy, \u02c6\u03bc) for grouped\ndata are the same as the difference d(y, \u02dc\u03bc) \u2212 d(y, \u02c6\u03bc) for ungrouped observations. therefore\n(d(\u00afy, \u02dc\u03bc) \u2212 d(\u00afy, \u02c6\u03bc))/\u03c6 is asymptotically \u03c72(s)-distributed, where s is the difference between\nthe number of parameters in m and \u02dcm.\n\nchapter 3. generalized linear models\n\nfor the normal linear model alternative tests for the lack-of-fit are available. the partition-\n\ning of (ungrouped) least squares data (yij, xi), j = 1, . . . , ni, yields\n\nn(cid:7)\n\nni(cid:7)\n\nn(cid:7)\n\nn(cid:7)\n\nni(cid:7)\n\n(yij \u2212 \u02c6\u03bci)2 =\n\nni(\u00afyi \u2212 \u02c6\u03bci)2 +\n\n(yij \u2212 \u00afyi)2,\n\ni=1\n\nj=1\n\ni=1\n\ni=1\n\nj=1\n\nwhich has the form\n\nd( \u02dcm) = d( \u02dcm|m) + d(m),\n\nwhere \u02dcm stands for the linear model and m for a model where only yij = \u03bci + \u03b5ij with\n\u03b5ij \u223c n(0, \u03c32) is assumed. since in computing d(m) no assumption on linearity is assumed,\nit is also called the pure error sum of squares and d( \u02dcm|m) the lack of fit sum of squares. by\nuse of the mean squares one obtains for h 0 : \u03bc = x\u03b2 against h1 : \u03bc (cid:8)= x\u03b2 the f -statistic\n\n(cid:14)\n(cid:14)\n\nf =\n\nn\n\n(cid:14)\ni=1 ni(\u00afyi \u2212 \u02c6\u03bci)2/(n \u2212 p)\nj=1(yij \u2212 \u00afyi)2/(n \u2212 n)\n\nni\n\nn\ni=1\n\n\u223c f(n \u2212 p, n \u2212 n).\n\nlinearity is dismissed if f > f1\u2212\u03b1(n \u2212 p, n \u2212 n). when using the f -statistic, not all levels\nof covariates need to have repeated observations, only some of the ni\u2019s have to be larger than 1.\nhowever, the test is still based on the assumptions that responses are normally distributed and\nhave variance \u03c32. note that d( \u02dcm|m) is the deviance for grouped observations.\n\n3.8.2 pearson statistic\n\nan alternative measure for the discrepancy between the data and the model is the pearson\nstatistic:\n\nn(cid:7)\n\ni=1\n\np =\n\u03c72\n\n(\u00afyi \u2212 \u02c6\u03bci)2\nv(\u02c6\u03bci)/ni\n\n,\n\n(3.12)\n\nwhere \u00afyi is the mean for grouped observations, \u02c6\u03bci is the estimated mean, and v(\u02c6\u03bci) is the\ncorresponding variance function that is linked to the variance by var(yi) = v(\u03bci)\u03c6/ni. if fixed\ncells asymptotics applies (n fixed, ni \u2192 \u221e, ni/n \u2192 ci, ci > 0), \u03c72\np is asymptotically \u03c72-\np has an approximately \u03c6\u03c72(n\u2212p)-distribution. the dispersion parameter\ndistributed, that is, \u03c72\n\u03c6 has to be known and fixed since the estimation of \u03c6 is based on this statistic. replacing \u03c6 by\np/(n \u2212 p) would yield the trivial\nthe dispersion estimate from grouped observations \u02c6\u03c6n = \u03c72\nresult \u03c72\n\np/ \u02c6\u03c6n = n \u2212 p.\n\n "}, {"Page_number": 87, "text": "3.9. computation of maximum likelihood estimates\n\n75\n\ngoodness-of-fit for grouped observations\n\nn(cid:7)\n\ndeviance\n\nd = \u2212 \u03c6 2\n\npearsons \u03c72\n\ni=1\n\n\u03c72 =\n\n{l (\u00afyi; \u02c6\u03bci, \u03c6) \u2212 l(\u00afyi; \u00afyi, \u03c6)}\nn(cid:7)\n\n(\u00afyi \u2212 \u02c6\u03bci)2\nv(\u02c6\u03bci)\n\nni\n\ni=1\n\nfixed cells asymptotic (n fixed, ni \u2192 \u221e, ni/n \u2192 ci, ci > 0)\n\nd, \u03c72 approximatively \u03c6 \u03c72(n \u2212 p)\n\n3.9 computation of maximum likelihood estimates\nmaximum likelihood estimates are obtained by solving the equation s(\u02c6\u03b2) = 0. in general,\nthere is no closed form of the estimate available, and iterative procedures have to be applied. in\nmatrix notation the score function is given by\n\ns(\u03b2) = x t d \u03c3\n\n\u22121(y \u2212 \u03bc) = x t w d\n\n\u22121(y \u2212 \u03bc),\n\nwhere in d, \u03c3 , and w the dependence on \u03b2 is suppressed (see section 3.6).\n\nthe newton-raphson method is an iterative method for solving non-linear equations. start-\ning with an initial guess \u03b2(0), the solution is found by successive improvement. let \u03b2(k) denote\nthe estimate in the kth step, where k = 0 is the initial estimate. if s(\u03b2(k)) (cid:8)= 0, one considers\nthe linear taylor approximation\n\ns(\u03b2) \u2248 slin(\u03b2) = s(\u02c6\u03b2\n\n(k)) + \u2202s(\u02c6\u03b2\n\n(k))\n\n\u2202\u03b2\n\n(\u03b2 \u2212 \u02c6\u03b2\n\n(k)).\n\ninstead of solving s(\u02c6\u03b2) = 0 one solves slin(\u02c6\u03b2) = 0, yielding\n\n\u02c6\u03b2 = \u02c6\u03b2\n\n(k) \u2212 ( \u2202s(\u02c6\u03b2\n\n(k))\n\n\u2202\u03b2\n\n\u22121s(\u02c6\u03b2\n)\n\n(k)).\n\nsince \u2202s(\u03b2)/\u2202\u03b2 = \u22022l(\u03b2)/\u2202\u03b2\u2202\u03b2t , one obtains with the hessian matrix h(\u03b2) = \u22022l(\u03b2)/\u2202\u03b2\u2202\u03b2t\nthe new estimate\n\n\u02c6\u03b2\n\n(k+1) = \u02c6\u03b2\n\n(k))\nor, by using the observed information matrix f obs(\u03b2) = \u2212h(\u03b2),\n(k)).\n\n(k+1) = \u02c6\u03b2\n\n\u22121s(\u02c6\u03b2\n\n(k))\n\n(k))\n\n(k) + f obs(\u02c6\u03b2\n\n\u02c6\u03b2\n\n(k) \u2212 h(\u02c6\u03b2\n\n\u22121s(\u02c6\u03b2\n\niterations are carried out until the changes between successive steps are smaller than a specified\nthreshold \u03b5. iteration is stopped if\n(cid:16) \u02c6\u03b2\n\n(k) (cid:16) / (cid:16) \u02c6\u03b2\n\n(k+1) \u2212 \u02c6\u03b2\n\n(k) (cid:16)< \u03b5.\n\n "}, {"Page_number": 88, "text": "76\n\nchapter 3. generalized linear models\n\nconvergence is usually fast, with the number of correct decimals in the approximation roughly\ndoubling at each iteration.\n\nan alternative method is the newton method with fisher scoring. the essential difference\nis that the observed information matrix f obs is replaced by the expected information f (\u03b2) =\ne(f obs(\u03b2)) (or h(\u03b2) by \u2212f (\u03b2)), yielding\n\n\u02c6\u03b2\n\n(k+1) = \u02c6\u03b2\n\n(k) + f (\u02c6\u03b2\n\n(k)).\n\n(3.13)\n\nthe iterative scheme (3.13) may alternatively be seen as an iterative weighted least-squares\nfitting procedure. let pseudo- or working observations be given by\n\n(k))\n\n\u22121s(\u02c6\u03b2\n(cid:26)\u22121\n\n(cid:25)\n\n\u02dc\u03b7i(\u02c6\u03b2) = xt\n\ni\n\n\u02c6\u03b2 +\n\n\u2202h(\u03b7i)\n\n\u2202\u03b7\n\n(yi \u2212 \u03bci(\u02c6\u03b2))\n\nand \u02dc\u03b7(\u02c6\u03b2)t = ( \u02dc\u03b71(\u02c6\u03b2), . . . , \u02dc\u03b7n(\u02c6\u03b2)) denote the vector of pseudo-observations given by \u02dc\u03b7(\u02c6\u03b2) =\nx \u02c6\u03b2 + d(\u02c6\u03b2)\u22121(y \u2212 \u02c6\u03bc). one obtains by simple substitution\n\u22121x t w (\u02c6\u03b2\n\n(k+1) = (x t w (\u02c6\u03b2\n\n(k))\u02dc\u03b7(\u02c6\u03b2\n\n(k))x)\n\n(k)).\n\n\u02c6\u03b2\n\n(k+1)\n\nthus \u02c6\u03b2\n( \u02dc\u03b7i\n\n(k), xi), i = 1, . . . , n, with the weight w (\u02c6\u03b2\nfor a canonical link one obtains f n(\u03b2) = x t w x with w = \u03c6\n\nhas the form of a weighted least-squares estimate for the working observations\n\u22121, \u03c6 = diag(\u03c61,\n\n(k)) depending on the iteration.\n\u22121\u03c3 \u03c6\n\n. . . , \u03c6n) and score function s(\u03b2) = x t \u03c6\n\n\u22121(y \u2212 \u03bc) and therefore\n\n\u02c6\u03b2\n\n(k+1) = \u02c6\u03b2\n\n(k) + (x t w x)\n\n\u22121x t \u03c6\n\n\u22121(y \u2212 \u03bc),\n\nwhich corresponds to least-squares fitting\n\nwith \u02dc\u03b7(\u03b2) = x\u03b2 + w\n\n\u22121x t w \u03b7(\u02c6\u03b2\n\u02c6\u03b2\n(k+1) = (x t w x)\n\u22121(y \u2212 \u03bc). if \u03c6 = \u03c6i, one obtains\n\u22121\u03c6\n\n(k))\n\n\u02c6\u03b2\n\n(k+1) = \u02c6\u03b2\n\n(k) + \u03c6(x t \u03c3 x)\n\n\u22121x t (y \u2212 \u03bc).\n\n3.10 hat matrix for generalized linear models\nwith weight matrix w (\u03b2) = d(\u03b2)\u03c3(\u03b2)\u22121d(\u03b2)t , the iterative fitting procedure has the\nform\n\n\u02c6\u03b2\n\n(k+1) = (x t w (\u02c6\u03b2\n\n(k))x)\n\n\u22121x t w (\u02c6\u03b2\n\n(k))\u02dc\u03b7(\u02c6\u03b2\n\n(k)).\n\nat convergence one obtains\n\n\u22121x t w (\u02c6\u03b2)\u02dc\u03b7(\u02c6\u03b2).\nthus \u02c6\u03b2 may be seen as the least-squares solution of the linear model\n\n\u02c6\u03b2 = (x t w (\u02c6\u03b2)x)\n\nw t /2\u02dc\u03b7(\u02c6\u03b2) = w t /2x\u03b2 + \u02dc\u03b5,\n\nwhere in w = w (\u02c6\u03b2) the dependence on \u02c6\u03b2 is suppressed. the corresponding hat matrix has\nthe form\n\nh = w t /2x(x t w x)\n\n\u22121x t w 1/2.\n\n "}, {"Page_number": 89, "text": "3.10. hat matrix for generalized linear models\n\n77\n\nsince the matrix h is idempotent and symmetric, it may be seen as a projection matrix for\nwhich tr(h) = rank(h) holds. moreover, one obtains for the diagonal elements of h =\n(hij) 0 \u2264 hii \u2264 1 and tr(h) = p (if x has full rank).\nit should be noted that, in contrast to the normal regression model, the hat matrix depends\non \u02c6\u03b2 because w = w (\u02c6\u03b2). the equation w t /2\u02c6\u03b7 = hw t /2\u02dc\u03b7(\u03b2) shows how the hat matrix\nmaps the adjusted variable \u02dc\u03b7(\u03b2) into the fitted values \u02c6\u03b7. thus h may be seen as the matrix\nthat maps the adjusted observation vector w t /2\u02dc\u03b7 into the vector of \"fitted\" values w t /2\u02c6\u03b7,\nwhich is a mapping on the transformed predictor space.\n\nfor the linear model the hat matrix represents a simple projection having the form \u02c6\u03bc = hy.\n\nin the case of generalized linear models, it may be shown that approximatively\n\n\u22121/2(\u02c6\u03bc \u2212 \u03bc) (cid:17) h\u03c2\n\n\u22121/2(y \u2212 \u03bc)\n\n\u03c3\n\n(3.14)\n\nholds, where \u03c3 = \u03c3(\u02c6\u03b2). thus h may be seen as measure of the influence of y on \u02c6\u03bc in\nstandardized units of changes. from (4.6) follows\n\u02c6\u03bc \u2212 \u03bc (cid:17) \u03c3 1/2h\u03c2\n\n(3.15)\n\u22121/2,\nsuch that the influence in unstandardized units is given by the projection matrix \u03c3 1/2h\u03c2\nwhich is idempotent but not symmetric. note that for the normal regression model with an\nidentity link one has w = i/\u03c32, h = x(x t x)\u22121x t , and (4.6) and (3.15) hold exactly.\nfrom (3.15) one derives the approximation\n\n\u22121/2(y \u2212 \u03bc),\n\ncov(y \u2212 \u02c6\u03bc) (cid:17) \u03c3 1/2(i \u2212 h)\u03c3 t /2.\n\nan alternative property is obtained from considering the estimating equation x t \u02c6w \u02c6d\n\u02c6\u03bc) = 0, which yields directly\n\n\u2212t (y \u2212\n\np x,w \u02c6d\n\n\u2212t \u02c6\u03bc = p x,w \u02c6d\n\n\u2212t\n\n(3.16)\nwith p x,w = x(x t \u02c6w (\u02c6\u03b2)x)\u22121x t \u02c6w being the projection on the subspace span (z) with\nrespect to the matrix w , which means an orthogonal projection with respect to the product\n1 w x2, x1, x2 \u2208 irn. p z,w is idempotent but not symmetric. thus the projections of\nxt\n\u2212t \u02c6\u03bc on the \u03b7-space are identical. from (3.16) one obtains\nthe transformed values \u02c6d\neasily\n\ny, \u02c6d\n\n\u2212t\n\ny,\n\nhw t /2d\n\n\u2212t \u02c6\u03bc = hw t /2d\n\n\u2212t y,\n\nwhich yields\n\n\u22121/2 \u02c6\u03bc = h\u03c2\n\n\u22121/2y,\n\nh\u03c2\n\nmeaning that the orthogonal projections (based on h) of standardized values \u03c3\nare identical. with \u03c7 = \u03c3\n\n\u22121/2(y \u2212 \u02c6\u03bc) denoting the standardized residual, one has\n\n\u22121/2 \u02c6\u03bc, \u03c3\n\n\u22121/2y\n\nh\u03c7 = 0 and (i \u2212 h)\u03c7 = \u03c7.\n\nthere is a strong connection to the \u03c72-statistic since \u03c72 = \u03c7t \u03c7. the matrix h has the\nform h = (w t /2x)(x t w 1/2w t /2x)\u22121(x t w 1/2), which shows that the projection is\ninto the subspace that is spanned by the columns of x t w 1/2. the essential difference from\nordinary linear regression is that the hat matrix does depend not only on the design but also on\nthe fit.\n\n "}, {"Page_number": 90, "text": "78\n\nchapter 3. generalized linear models\n\n3.11 quasi-likelihood modeling\nin generalized linear models it is assumed that the true density of the responses follows an\nexponential family. however, in applications it is not too infrequently found that the implicitly\nspecified variation of the responses is not consistent with the variation of the data. approaches\nthat use only the first two moments of the response distribution are based on so-called quasi-\nlikelihood estimates (wedderburn, 1974; mccullagh and nelder, 1989). when using quasi-\nlikelihood estimates, the exponential family assumption is dropped, and the mean and variance\nstructures are separated. no full distributional assumptions are necessary. under appropriate\nconditions, parameters can still be estimated consistently, and asymptotic inference is possible\nunder appropriate modifications.\n\nquasi-likelihood approaches assume, like glms, that the mean and variance structures are\n\ncorrectly specified by\n\ne(yi|xi) = \u03bci = h(xt \u03b2),\n\nvar(yi|xi) = \u03c32\n\ni (\u03bci) = \u03c6v(\u03bci),\n\n(3.17)\n\nwhere v(\u03bc) is a variance function and \u03c6 is a dispersion parameter. the main difference from\nglms is that the mean and variance do not have to be specified by an exponential family. the\nusual maximum likelihood estimates for glms are obtained by setting the score function equal\nto zero:\n\ns(\u02c6\u03b2) = x t d(\u02c6\u03b2)\u03c3\n\n(3.18)\n\u22121(\u02c6\u03b2)),\nthe ml estimation equation uses the link (in d(\u02c6\u03b2)) and the variance function (in \u03c3\nbut no higher moments. the solution of (3.18) yields the ml estimates when the mean and\nvariance correspond to an exponential family. the quasi-likelihood (ql) estimates are obtained\nwhen the specification of the mean and the variance is given by (3.17) without reference to an\nexponential family. one may understand\n\n\u22121(\u02c6\u03b2)(y \u2212 \u03bc) = 0.\n\nsq(\u03b2) = x t d(\u03b2)\u03c3\n\n\u22121(\u03b2)(y \u2212 \u03bc)\n\nwith the specifications (3.17) as a quasi-score function with the corresponding estimation equa-\ntion sq(\u02c6\u03b2) = 0. it is also possible to construct a quasi-likelihood function q(\u03b2, \u03c6) that has\nthe derivative sq(\u03b2) = \u2202q/\u2202\u03b2 (nelder and pregibon, 1987; mccullagh and nelder, 1989).\nit can be shown that the asymptotic properties are similar to those for glms. in particular,\none obtains asymptotically a normal distribution with the covariance given in the form of a\npseudo-fisher matrix:\n\nn(cid:7)\n\ni=1\n\n(cid:25)\n\n(cid:26)\n\n\u2202h(\u03b7i)\n\n\u2202\u03b7\n\nf q(\u03b2) =\n\nxixt\ni\n\n2\n\n/\u03c32\n\ni = x t d\u03c2\n\n\u22121dx = x t w x\n\n(see also wedderburn, 1974; mccullagh and nelder, 1989).\n\nit should be noted that quasi-likelihood models weaken the distributional assumptions con-\nsiderably. one obtains estimates of parameters without assuming a specific distribution. one\njust has to specify the mean and the variance and is free to select a variance function v(\u03bci) that\nis not determined by a fixed distribution.\n\na major area of application is the modeling of overdispersion. for example, in count data\nthe assumption of the poisson distribution means that the variance depends on the mean in the\ni \u03b2). in quasi-likelihood approaches one might assume that the\nform var(yi) = \u03bci = exp(xt\nvariance is var(yi) = \u03c6\u03bci = \u03c6 exp(xt\ni \u03b2), with an additional unknown dispersion parameter\n\u03c6. since the poisson distribution holds for \u03c6 = 1 only, one does not assume the poisson model\nto hold (see sections 5.3 and 7.5).\n\n "}, {"Page_number": 91, "text": "3.13. exercises\n\n79\n\nmore flexible models allow the variance function to depend on additional parameters (e.g.,\nnelder and pregibon, 1987). then the variance function has the form \u03c6v(\u03bc; \u03b1), where \u03b1 is an\nadditional unknown parameter. an example is v(\u03bc; \u03b1) = \u03bc\u03b1. for fixed \u03b1, the quasi-likelihood\nestimate \u02c6\u03b2 (and an estimate \u02c6\u03c6) is obtained by solving the corresponding estimation equation.\nestimation of \u03b1, given \u02c6\u03b2 and \u02c6\u03c6, can be carried out by the method of moments. cycling between\nthe two steps until convergence gives a joint estimation procedure. asymptotic results for \u02c6\u03b2\nremain valid if \u03b1 is replaced by a consistent estimate \u02c6\u03b1.\n\nwithin quasi-likelihood approaches one can also model that the dispersion parameter de-\npends on covariates. then one assumes \u03bc = h(xt \u03b2), \u03c6 = \u03c6(zt \u03b3), var(y) = \u03c6v(\u03bc), where\nz is a vector of covariates affecting the dispersion parameter. cycling between the estimating\nequation for \u03b2 and an estimation equation for \u03b3 yields estimates for both parameters. alter-\nnatively, a joint estimation of parameters can be obtained by the general techniques for fitting\nlikelihood and quasi-likelihood models described by gay and welsch, 1988. consistent estima-\ntion of \u03b2 and \u03b1 requires that not only the mean but also the dispersion parameter be correctly\nspecified (for details see pregibon, 1984; nelder and pregibon, 1987; efron, 1986; mccullagh\nand nelder, 1989; nelder, 1992).\n\n3.12 further reading\nsurveys and books. a source book on generalized linear models is mccullagh and nelder\n(1989). multivariate extensions are covered in fahrmeir and tutz (2001). shorter introductions\nwere given by dobson (1989) and firth (1991). a bayesian perspective on generalized linear\nmodels is outlined in dey et al.\n(2000). more recently, a general treatment of regression,\nincluding glms was given by fahrmeir et al. (2011).\n\nquasi-likelihood. quasi-likelihood estimates were considered by wedderburn (1974), mc-\ncullagh (1983), and mccullagh and nelder (1989). efficiency of quasi-likelihood estimates\nwas investigated by firth (1987). a rigorous mathematical treatment was given by heyde\n(1997). asymptotic properties were also studied by xia et al. (2008).\n\nr packages. glms can be fitted by use of the model fitting functions glm from the mass\n\npackage. many tools for diagnostics and inferences are available.\n\n3.13 exercises\n\n(cid:2)\n\n3.1 let independent observations yi1, . . . , yini have a fixed covariate vector xi with ni denoting the\nsample size at covariate value xi. the model to be examined has the form \u03bci = \u03bcij = e(yij) = h(xt\ni \u03b2).\n(a) let observations be binary with yij \u2208 {0, 1}. show that the distribution of the mean \u00afyi =\nni\n1\nj=1 yij also has the form of a simple exponential family. compare the canonical parame-\nni\nter and the dispersion parameter with the values of the exponential family of the original binary\nresponse.\n\n(b) show that the mean \u00afyi always has the form of a simple exponential family if the mean of yij has\n\nthe form of a simple exponential family.\n\n3.2 let observations (yi, xi), i = 1, . . . , n, with binary response yi \u2208 {0, 1} be given. the used models\nare the logit model p (yi = 1|xi) = \u03c6(xt\ni \u03b2), with f denoting the logistic distribution function, and the\nprobit model p (yi = 1|xi) = \u03c6(xt\ni \u03b2), with \u03c6 denoting the standard normal distribution function. give\nthe log-likelihood function and derive the score function s(\u03b2), the matrix of derivatives d(\u03b2), and the\nvariance matrix \u03c3 (\u03b2) for both models. in addition, give the observed and expected information matrices.\n\n3.3 consider a glm with poisson-distributed responses.\n\n "}, {"Page_number": 92, "text": "80\n\nchapter 3. generalized linear models\n\n(a) derive the fisher matrix for the canonical link function.\n(b) show that the asymptotic distribution for grouped data in n groups has approximate covariance\n\ncov(\u02c6\u03b2) \u2248 (x t diag( \u02c6\u03bc)x)\n\n\u22121, where log( \u02c6\u03bc) = x\u03b2.\n\n3.4 let the independent observations yi1, . . . , yini , observed at predictor value xi, follow a poisson dis-\ntribution, yij \u223c p(\u03bbij). then one obtains for the sum \u02dcyi =\nj \u03bbij.\ndiscuss modeling strategies for the observations. consider in particular models for single variables yij,\nfor accumulated counts \u02dcyi, and for average counts \u02dcyi/ni.\n\nj=1 yij \u223c p(\u02dc\u03bbi), where \u02dc\u03bbi =\n\nni\n\n(cid:2)\n\n(cid:2)\n\n3.5 let (yi, xi) denote independent observations. a linear model with log-transformed responses is given\nby log(yi) = xt\n\ni \u03b2 + \u0001i, \u0001i \u223c n(0, \u03c32). compare the model to the glm\n\u03bci = exp(xt\n\nyi|xi \u223c n(\u03bci, \u03c32)\n\nand\n\ni \u03b2),\n\nand explain the difference between the two models.\n\n3.6 the r package catdat provides the data set rent.\n\n(a) find a glm with response rent (net rent in euro) and the explanatory variables size (size in square\nmeters) and rooms (number of rooms) that fits the data well. try several distribution functions like\ngaussian and gamma and try alternative links.\n\n(b) discuss strategies to select a model from the models fitted in (a).\n\n "}, {"Page_number": 93, "text": "chapter 4\n\nmodeling of binary data\n\nin chapter 2 in particular, the logit model is considered as one specific binary regression model.\nin this section we will discuss modeling issues for the more general binary regression model\n\np (yi = 1|xi) = \u03c0(xi) = h(xt\n\ni \u03b2),\n\nwhere the response function h is a fully specified function, which in the case of the logit model\nis the logistic distribution function h(\u03b7) = exp(\u03b7)/(1 + exp(\u03b7)). a general parametric binary\nregression model is determined by the link function (the inverse of the response function) and\nthe linear predictor. while the link function determines the functional form of the response\nprobabilities, the linear predictor determines which variables are included and in what form\nthey determine the response. in particular, when categorical and metric variables are present,\nthe linear predictor can, in addition to simple linear terms, contain polynomial versions of\ncontinuous variables, dummy variables, and interaction effects. therefore some care should be\ntaken when specifying constituents of the model like the linear predictor. statistical regression\nmodeling always entails a series of decisions concerning the structuring of the dependence of\nthe response on the predictor. various aspects are important when making these decisions,\namong them are the following:\n\n\u2022 discrepancy between data and model. does the fit of the model support the inferences\n\ndrawn from the model?\n\n\u2022 relevance of variables and form of the linear predictor. which variables should be in-\n\ncluded and how?\n\n\u2022 explanatory power of the covariates.\n\n\u2022 prognostic power of the model.\n\n\u2022 choice of link function. which link function fits the data well and has a simple interpre-\n\ntation.\n\nfigure 4.1 illustrates aspects of regression modeling and the corresponding evaluation in-\nstruments. since the evaluation of a model starts with parameter estimation, it is at the top\nof the panel. when estimates have been obtained one can deal with problems concerning the\nappropriateness of the model, the specification of the predictor, and the obtained explanatory\nvalue. some of the tools that can be used to cope with these problems are given at the bottom of\nthe panel. it should be noted that these aspects are not independent. a model should represent\nan appropriate approximation of the data when one investigates if the linear predictor may be\n\n81\n\n "}, {"Page_number": 94, "text": "82\n\nchapter 4. modeling of binary data\n\nfigure 4.1: aspects of regression modeling.\n\nsimplified. on the other hand, the specification determines the goodness-of-fit of the model.\nthere is also a strong link between the specification of the linear predictor and the explanatory\nvalue of the covariates. only the focus is different. while the specification of the linear predic-\ntor aims at finding an adequate form of the covariates and at reducing the variables to a set of\nvariables that is really needed, the explanatory value of a model aims at quantifying the effect\nof the covariates within the model.\n\nsince the estimation of parameters plays such a prominent role in modeling, the first section\nof this chapter is devoted to estimation, in particular maximum likelihood (ml) estimation.\nsince most of the tools considered in this chapter are likelihood-based, we will defer alternative\nestimation concepts to later sections. in this chapter the focus is on tools; therefore, only part\nof the model structures will be discussed. for example, the specification of the link function\nand weaker distributional assumptions will be considered later, in chapter 5.\n\nwe will consider here in particular tools for the following aspects:\n\n\u2022 discrepancy between data and fit: global goodness-of-fit of a model (section 4.2)\n\n\u2022 diagnostic tools for single observations (section 4.3)\n\n\u2022 specification of the linear predictor (section 4.4)\n\n\u2022 explanatory value of covariates (section 4.6)\n\n4.1 maximum likelihood estimation\nwhen a model is assumed to represent a useful relationship between an observed response\nvariable and several explanatory variables, the first step in inference is the estimation of the\nunknown parameters. in the linear logit model\np (yi = 1|xi) =\n\nexp(xt\n\ni \u03b2)\ni \u03b2) ,\n1 + exp(xt\n\ni \u03b2), the parameters to be estimated are\nas well as in the more general model \u03c0(xi) = h(xt\nthe regression parameters \u03b2. in the following we consider the more general model, where h\nis a fully specified function. the logit model uses h(\u03b7) = exp(\u03b7)/(1 + exp(\u03b7)). alternative\nresponse functions like the normal distribution function, which yields the probit model, will be\nconsidered in chapter 5.\n\n "}, {"Page_number": 95, "text": "4.1. maximum likelihood estimation\n\n83\n\nthe most widely used general method of estimation in binary regression models is maxi-\nmum likelihood. the basic principle is to construct the likelihood of the unknown parameters\nfor the sample data. the likelihood represents the joint probability or probability density of the\nobserved data, considered as a function of the unknown parameters.\n\nin the following we will distinguish between the case of single binary responses and the\nmore general case of scaled binomials (or proportions) \u00afyi. a proportion \u00afyi is computed from\nni independent binary observations observed at the same measurement point xi.\n\nsingle binary responses\nfor independent observations (yi, xi), i = 1, . . . , n, with yi \u2208 {0, 1}, the likelihood for the\nconditional responses yi|xi is given by\nn(cid:15)\n\nl(\u03b2) =\n\n\u03c0(xi)yi(1 \u2212 \u03c0(xi))1\u2212yi .\n\ni=1\n\neach term in the product represents the probability that yi is observed since \u03c0(xi)yi(1 \u2212\n\u03c0(xi))1\u2212yi simplifies to \u03c0(xi) if yi = 1 and to 1 \u2212 \u03c0(xi) if yi = 0. the product is built\nsince the observations y1, . . . , yn, given x1, . . . , xn, are considered as independent. the max-\nimum likelihood estimates of \u03b2 are those values \u02c6\u03b2 that maximize the likelihood. it is usually\nmore convenient to maximize the log-likelihood rather than the likelihood itself. since the log-\narithm is a strictly monotone transformation, the obtained values \u02c6\u03b2 will be the same. therefore,\nthe log-likelihood\n\nl(\u03b2) = log(l(\u03b2)) =\n\nli(\u03b2) =\n\nyi log(\u03c0(xi)) + (1 \u2212 yi) log(1 \u2212 \u03c0(xi))\n\ni=1\n\ni=1\n\nis used. the value \u02c6\u03b2 that maximizes l(\u03b2) can be obtained by solving the system of equations\n\u2202l(\u03b2)/\u2202\u03b2 = 0. it is common to consider the derivatives that yield the equations as functions\nof \u03b2. one considers the so-called score function:\n\nwhich has the form\n\ns(\u03b2) = \u2202l(\u03b2)/\u2202\u03b2 = (\u2202l(\u03b2)/\u2202\u03b21, . . . , \u2202l(\u03b2)/\u2202\u03b2p)t ,\n\nn(cid:7)\n\ni=1\n\ns(\u03b2) =\n\nsi(\u03b2) =\n\nn(cid:7)\n\ni=1\n\nxi\n\n\u2202h(\u03b7i)\n\n\u2202\u03b7\n\n(yi \u2212 \u03c0(xi))\n\n\u03c32\ni\n\n,\n\ni = var(yi) = \u03c0(xi)(1 \u2212 \u03c0(xi)). the maximum likelihood (ml)\nwhere \u03b7i = xt\nestimate is then found by solving s(\u02c6\u03b2) = 0. the system of equations has to be solved iteratively\n(see section 3.9).\n\ni \u03b2 and \u03c32\n\nfor the logit model one obtains by simple calculation\n\nn(cid:7)\n\nn(cid:7)\n\nyielding the score function s(\u03b2) =\nto be solved are\n\nn\n\ni=1 xi(yi \u2212 \u03c0(xi)). therefore, the likelihood equations\n\nn(cid:7)\n\ni=1\n\n=\n\n\u2202l(\u03b2)\n\u2202\u03b2j\n\n(cid:14)\nn(cid:7)\n\nxij(yi \u2212 \u03c0(xi)),\nn(cid:7)\n\nxijyi =\n\nxij\u03c0(xi)),\n\ni=1\n\ni=1\n\n "}, {"Page_number": 96, "text": "(cid:14)\n\n84\n\nchapter 4. modeling of binary data\n\nn\n\ni=1 xijyi, j = 1 . . . , p for \u03b2 to their expected values\n\nwhich equate the sufficient statistics\n(exercise 4.3).\nin maximum likelihood theory, the asymptotic variance of the estimator is determined by\nthe information or fisher matrix f (\u03b2) = e(\u2212 \u22022l(\u03b2)\n\u2202\u03b2\u2202\u03b2t ). as derived in chapter 3, f (\u03b2) is\ngiven by f (\u03b2) = x t w x, where x with x t = (x1, . . . , xn) is the design matrix and\nw = diag (( \u2202h(\u03b71)\n\n)2/\u03c32\n\n)2/\u03c32\n\n1, . . . , ( \u2202h(\u03b7n)\n\n\u2202\u03b7\n\n\u2202\u03b7\n\n(cid:14)\n\nn) is a diagonal weight matrix.\ni \u03c32\n\nn\ni=1 xixt\n\nfor the logit model one obtains the simpler form f (\u03b2) =\n\ni = x t w x with\n\nweight matrix w = (\u03c32\n\n1, . . . , \u03c32\n\nn). an approximation to the covariance of \u02c6\u03b2 is given by\n\n\u02c6cov(\u02c6\u03b2) \u2248 f (\u03b2)\n\n\u22121 = (x t w x)\n\n\u22121,\n\nwhere in practice w is replaced by \u02c6w , which denotes the evaluation of w at \u02c6\u03b2.\n\ngrouped data: estimation for binomially distributed responses\nin many applications, for given values of the covariates, several independent binary responses\nare observed. since p (y = 1| x) = \u03c0(x) is assumed to depend on x only, the mean is\nassumed to be the same for all the binary observations collected at this value. more formally,\nlet yi1, . . . , yini denote the independent dichotomous responses collected at value xi and let\nx1, . . . , xn denote the distinct values of covariates or measurement points where responses are\nobserved. the model has the form\n\np (yij = 1|xi) = h(xt\ni \u03b2), where \u03c0(xi) = p (yij = 1|xi), j = 1, . . . , ni.\n\ni \u03b2), j = 1, . . . , ni,\n\nor simpler \u03c0(xi) = h(xt\n1, . . . , n, (yij \u223c b(1, \u03c0(xi)) or, equivalently, the binomial distribution of yi1 + \u00b7\u00b7\u00b7 + yini\nb(ni, \u03c0(xi)). for the collection of binary variables the likelihood has the form\n\nfor the purpose of estimation one may use the original binary variables yi1, . . . , yini, i =\n\u223c\n\nl(\u03b2) =\n\n\u03c0(xi)yij (1 \u2212 \u03c0(xi))1\u2212yij\n\n\u03c0(xi)yi(1 \u2212 \u03c0(xi))ni\u2212yi ,\n\n=\n\n(4.1)\nwhere yi = yi1 + \u00b7\u00b7\u00b7 + yini denotes the number of successes. the likelihood for the number\n(cid:26)\nof successes yi \u223c b(ni, \u03c0(xi)) has the form\n\n(cid:25)\n\ni=1\n\nlbin(\u03b2) =\n\n\u03c0(xi)yi(1 \u2212 \u03c0(xi))ni\u2212yi .\n\nni\nyi\n\nthe binary observations likelihood l(\u03b2) and the binomial likelihood lbin differ in the binomial\nfactor, which is irrelevant in maximization because it does not depend on \u03b2. therefore, the\nrelevant part of the log-likelihood (omitting the constant) is\n\nl(\u03b2) = log(l(\u03b2)) =\n\n=\n\nyi log(\u03c0(xi)) + (ni \u2212 yi) log(1 \u2212 \u03c0(xi))\n\n(cid:25)\n\n(cid:26)\n\nni{\u00afyi log\n\n\u03c0(xi)\n1 \u2212 \u03c0(xi)\n\n+ log(1 \u2212 \u03c0(xi))},\n\n(4.2)\n\nni(cid:15)\n\nj=1\n\nn(cid:15)\nn(cid:15)\n\ni=1\n\nn(cid:15)\n\ni=1\n\nn(cid:7)\nn(cid:7)\n\ni=1\n\ni=1\n\n "}, {"Page_number": 97, "text": "(cid:14)\n\n4.1. maximum likelihood estimation\n\n85\n\nn\n\nwhere \u00afyi = yi/ni denotes the relative frequency. the score function is given by s(\u03b2) =\ni=1 xi(\u2202h(\u03b7i)/\u2202\u03b7)(\u00afyi \u2212 \u03c0(xi))/var(\u00afyi), and the fisher matrix, which is needed for the\nstandard errors, is\n\nf (\u03b2) = e(\u2212\u2202l2(\u03b2)/\u2202\u03b2\u2202\u03b2t ) =\n\nn(cid:7)\n\nni\n\ni=1\n\n(\u2202h(xt\ni \u03b2)/\u2202\u03b7)2\ni \u03b2)(1 \u2212 h(xt\n\nh(xt\n\ni \u03b2)) xixt\ni .\n\n(cid:14)\ni=1 nixi(\u00afyi \u2212 \u03c0(xi)) and the\n\nn\n\nfor the logit model one obtains the score function s(\u03b2) =\nfisher matrix\n\nn(cid:7)\n\nf (\u03b2) =\n\nnih(xt\n\ni \u03b2)(1 \u2212 h(xt\n\ni \u03b2))xixt\ni .\n\nan approximation to the covariance of \u02c6\u03b2 is again given by the inverse information matrix.\n\ni=1\n\nasymptotic properties\nunder regularity assumptions, ml estimators for binary regression models have several favor-\nable asymptotic properties (n \u2192 \u221e, where n = n1 + \u00b7\u00b7\u00b7 + nn in the binomial case). the\nml estimator exists and is unique asymptotically; it is consistent and asymptotically normally\ndistributed with \u02c6\u03b2 \u223c n(\u03b2, f (\u02c6\u03b2)\u22121). moreover, it is asymptotically efficient compared to a\nwide class of other estimates. general results specifying the necessary regularity conditions\nwere given by haberman (1977) and fahrmeir and kaufmann (1985).\n\nexistence of maximum likelihood estimates\nfor a finite sample size it may happen that ml estimates do not exist. it is easy to construct\ndata structures for which the estimates tend to infinity. for a univariate predictor, let the data be\nseparated such that all data with a predictor below a fixed value c have response 0 and above that\nvalue response 1. then the best fit is obtained when the parameter for the predictor becomes\ninfinitely large. the general case was investigated by albert and anderson (1984) and santner\nand duffy (1986). they call a dataset completely separated if there exists a vector \u03b8 such that\n\ni \u03b8 > 0 if yi = 1 and xt\nxt\n\ni \u03b8 < 0 if yi = 0\n\nhold for i = 1, . . . , n, where xi contains an intercept. it is called quasi-completely separated if\nthere exists a vector \u03b8 such that\n\ni \u03b8 \u2265 0 if yi = 1 and xt\nxt\n\ni \u03b8 \u2264 0 if yi = 0\n\nholds for i = 1, . . . , n. a dataset is said to have overlap if there is no complete separation\nand no quasi-complete separation. they show that the ml estimate exists if and only if the\ndataset has overlap. thus, from a geometric point of view, ml estimates exist if there is no\nhyperplane that separates the 0 and 1 responses. christmann and rousseeuw (2001) showed\nhow to compute the smallest number of observations that need to be removed to make the ml\nestimate non-existent. this number of observations is called the regression depth and measures\nthe amount of separation between 0 and 1 responses. for literature on alternative estimators, in\nparticular robust procedures, see section 4.7.\n\nestimation conditioned on predictor values\nml estimation as considered in the previous section is conditional on the x-values. although\nboth variables y and x can be random, estimating the parameters of a model for p (y = 1|x)\n\n "}, {"Page_number": 98, "text": "86\n\nchapter 4. modeling of binary data\n\ndoes not depend on the marginal distribution of x. therefore, the ml estimates have the same\nform in cases where one has a total sample of iid observations (yi, xi) or a sample of responses\nconditional on x-values.\n\nin applications one also finds samples conditional on the response.\n\nin such a stratified\nsample one observes x-values given y = 1 and x-values given y = 0. a common case is\ncase-control studies in biomedicine, where y = 1 refers to cases and y = 0 are controls. in\nboth populations the potential risk factors x are observed given the population. in economet-\nrics, this type of sampling is often called choice-based sampling, referring to the sampling of\ncharacteristics of a choice maker given his or her choice was made.\n\nthe association between response y and predictor x as captured in the logit model also can\nbe inferred from samples that are conditional on responses. let us consider the most simple\ncase of one binary predictor. therefore, one has y \u2208 {0, 1} and x \u2208 {0, 1}. the coefficient \u03b2\nin the logit model logit(p (y = 1|x)) = \u03b20 + x\u03b2 is given by \u03b2 = log(\u03b3), where \u03b3 is the odds\nratio, which contains the association between y and x. however, the odds ratio \u03b3 can be given\nin two forms:\n\n\u03b3 = p (y = 1|x = 1)/p (y = 0|x = 1)\np (y = 1|x = 0)/p (y = 0|x = 0)\n\n= p (x = 1|y = 1)/p (x = 0|y = 1)\np (x = 1|y = 0)/p (x = 0|y = 0) .\n\nthe first form corresponds to the logit model logit(p (y = 1|x)) = \u03b20 + x\u03b2, the second form\nto the logit model logit(p (x = 1|y)) = \u02dc\u03b20 + y \u02dc\u03b2. for the latter model, which models response\nx given y, the parameter that contains the association between these variables is the same,\n\u02dc\u03b2 = log(\u03b3) = \u03b2. therefore, ml estimation of the latter model, based on a sample given y,\nyields an estimate of the coefficient \u03b2 of the original logit model logit(p (y = 1|x)) = \u03b20 +x\u03b2.\nasymptotic properties hold for the transformed model logit(p (x = 1|y)) = \u02dc\u03b20 + y \u02dc\u03b2.\n\nin general, the use of estimators for samples that are conditional on y may be motivated by\nthe specific structure of the logit model. therefore, we go back to the derivation of the binary\nlogit model to assume that predictors are normally distributed given y = r (section 2.2.2).\nwith f(x|r) denoting the density given y = r and p(r) = p (y = r) denoting the marginal\nprobability, it follows from bayes\u2019 theorem that\n\np (y = 1|x) =\n\nexp{log([p(1)f(x|1)]/[p(0)f(x|0)])}\n1 + exp{log([p(1)f(x|1)]/[p(0)f(x|0)])} .\n\ntherefore, the linear logit model holds if\n\nlog p(1)f(x|1)\np(0)f(x|0)\n\n= \u03b20 + xt \u03b2,\n\nholds. the equivalent form,\n\nlog f(x|1)\nf(x|0)\n\n= \u03b20 \u2212 log(p(1)/p(0)) + xt \u03b2,\n\nshows that a logit model holds if log(f(x|1)/f(x|0)) has a linear form that contains the term\nxt \u03b2 and only the intercept depends on the marginal probabilities. the essential point is that the\nmarginals determine only the intercept. thus, for identical densities but different marginal prob-\nabilities, the coefficient \u03b2, which measures the association between y and x, is unchanged. the\nargument holds more generally when the linear term xt \u03b2 is replaced by a function \u03b7(xt , \u03b2);\nlinearity in x is just the most prominent case. it is one of the strengths of the logit model that\nthe parameter \u03b2 does not depend on the marginal probabilities.\n\n "}, {"Page_number": 99, "text": "4.2. discrepancy between data and fit\n\n87\n\nn(cid:7)\n\nnevertheless, the likelihood for a given y differs from the likelihood given predictors. by\n\nusing f(xi|yi) = p (yi|xi)f(xi)/p(yi), one obtains for the log-likelihood conditional on y\n\nlcond =\n\nlog(p (yi|xi)) + log(f(xi)) \u2212 log(p(yi)).\n\ni=1\n\nthe first term on the right-hand side is equivalent to the conditional log-likelihood given x-\nvalues. the second term corresponds to the marginal distribution of x, which can be maximized\nby the empirical distribution. the third term refers to the marginal distribution of y, which is\nfixed by the sampling and changes the intercept. of course it has to be shown that maximization\nof lcond, which includes non-parametric maximization of the marginal distribution of x, yields\ncoefficient estimates \u03b2 that have the properties of ml estimates. for more details of choice-\nbased sampling see prentice and pyke (1979), carroll et al. (1995), and scott and wild (1986).\n\n(cid:14)\n\n4.2 discrepancy between data and fit\n4.2.1 the deviance\nbefore drawing inferences from a fitted model it is advisable to critically assess the fit. thus,\nwhen fitting a regression model one usually wants some measure for the discrepancy between\nthe fitted model and the observations. it should be obvious that the sum of the squared residuals\ni(yi \u2212 \u02c6\u03c0i)2 that is used in normal regression models is inappropriate because it is designed\nfor a symmetric distribution like the normal and in addition assumes homogeneous variances.\nit does not account for the specific type of distribution (and noise) when responses are binary.\nif the unknown parameters are estimated by maximum likelihood, a common measure for the\ndiscrepancy between the data and the fitted model that is specific for the underlying distribution\nof responses is the deviance. the deviance may be seen as a comparison between the data fit\nand the perfect fit.\n\nthe deviance is strongly related to basic concepts of statistics. a basic test statistic that is\n\nused to evaluate nested models is the likelihood ratio statistic:\n\u03bb = \u22122 log l(submodel)\nl(model)\n\n,\n\nwhere l(model) represents the maximal likelihood when a model is fit and l(submodel) rep-\nresents the maximal likelihood if a more restrictive model, a so-called submodel, is fit. by\nconsidering the submodel as the binary regression model and the model as the most general\npossible model (with perfect fit), one considers\n\n\u03bb = \u22122{log l(fitted submodel) \u2212 log l(fitted model)}\n= \u22122{l(fitted submodel) \u2212 l(fitted model)}.\n\nthe most general model that produces a perfect fit is often called the saturated model. it is\nassumed to have as many parameters as observations: therefore it is the perfect fit.\n\ndeviance for binary response\nsuppose that the data are given by (yi, xi), i = 1, . . . , n, where yi \u2208 {0, 1}. let l(y; \u02c6\u03c0) denote\nthe log-likelihood for the fitted model with yt = (y1, . . . , yn), \u02c6\u03c0t = (\u02c6\u03c01, . . . , \u02c6\u03c0n), \u02c6\u03c0i =\n\u02c6\u03b2). the perfectly fitting model is represented by the likelihood l(y; y), where\n\u02c6\u03c0(xi) = h(xt\ni\n\n "}, {"Page_number": 100, "text": "88\n\nchapter 4. modeling of binary data\n\nthe fitted values are the observations themselves. then the deviance for the binary responses is\ngiven by the difference of l(y, y) and l(y, \u02c6\u03c0):\n\n(cid:26)\nd(y, \u02c6\u03c0) = 2{l(y, y) \u2212 l(y, \u02c6\u03c0)}\n\n(cid:25)\n\n&\n\nyi\n\u02c6\u03c0i\n\nyi log\n\n+ (1 \u2212 yi) log(\n\n1 \u2212 yi\n1 \u2212 \u02c6\u03c0i\n{yi log(\u02c6\u03c0i) + (1 \u2212 yi) log(1 \u2212 \u02c6\u03c0i)},\n\n)\n\n(cid:2)\nn(cid:7)\nn(cid:7)\n\ni=1\n\ni=1\n\n= 2\n\n= \u22122\n\n(4.3)\n\nwhere the convention 0\u00b7\u221e = 0, is used. since l(y, y) = 0, the deviance reduces to d(y, \u02c6\u03c0) =\n\u22122l(y, \u02c6\u03c0). an alternative form of the deviance is\nn(cid:7)\n(cid:29) \u2212 log(\u02c6\u03c0i)\n\nd(y, \u02c6\u03c0) = 2\n\nd(yi, \u02c6\u03c0i),\n\nwhere\n\n(4.4)\n\ni=1\n\nthe simpler form,\n\nd(yi, \u02c6\u03c0i) =\n\n\u2212 log(1 \u2212 \u02c6\u03c0i)\n\nyi = 1\nyi = 0.\nd(yi, \u02c6\u03c0i) = \u2212 log(1 \u2212 |yi \u2212 \u02c6\u03c0i|),\n\nshows that for binary data the deviance implicitly uses the difference between observations and\nfitted values. from the latter form it is also seen that d(y, \u02c6\u03c0) \u2265 0 and d(y, \u02c6\u03c0) = 0 only if\n\u02c6\u03c0 = y.\n\nin extreme cases the deviance degenerates as a measure of goodness-of-fit. consider the\nsimple case of a single sample and n independent bernoulli variables. let n1 denote the number\nof \u201chits\u201d (yi = 1) and n2 = n \u2212 n1 the number of observations yi = 0. since \u02c6\u03c0i is the same\nfor all observations, one obtains with \u02c6\u03c0 = n1/n\n\nd(y, \u02c6\u03c0) = \u22122(n1 log(\u02c6\u03c0) + n2 log(1 \u2212 \u02c6\u03c0))\n\n= \u22122n{\u02c6\u03c0 log(\u02c6\u03c0) + (1 \u2212 \u02c6\u03c0) log(1 \u2212 \u02c6\u03c0)},\n\nwhich is completely determined by the relative frequency \u02c6\u03c0 and therefore does not reflect the\ngoodness-of-fit. the asymptotic distribution depends heavily on \u03c0 with a degenerate limit as\nn \u2192 \u221e.\n\nanother concept that is related to the deviance is the kullback-leibler distance, which is a\ngeneral directed measure for the distance between two distributions (appendix d). for discrete\ndistributions with support {z1, . . . , zm} and the vectors of probabilities for the two probability\nfunctions given by \u03c0 = (\u03c01, . . . , \u03c0m)t and \u03c0\n\n\u2217\nm)t , it has the form\n\n\u2217\n1, . . . , \u03c0\n\n\u2217 = (\u03c0\nm(cid:7)\n\nkl(\u03c0, \u03c0\n\n\u2217\n\n) =\n\n\u03c0i log( \u03c0i\n\u2217\n\u03c0\ni\n\n).\n\ni=1\n\nn(cid:7)\n\nconsidering the data as a degenerate mass function (1\u2212 yi, yi) with support {0, 1}, one obtains\n\nd(y, \u02c6\u03c0) = 2\n\nkl((1 \u2212 yi, yi), (1 \u2212 \u02c6\u03c0i, \u02c6\u03c0i)).\n\nthe maximum likelihood estimator may be motivated as a minimum distance estimator that\nminimizes the sums of the kullback-leibler distances considered in the last equation.\n\ni=1\n\n "}, {"Page_number": 101, "text": "4.2. discrepancy between data and fit\n\n89\n\ndeviance for proportions\nlet the data be given in the form (yi, xi), i = 1, . . . , n, where the yi\u2019s are binomially dis-\ntributed. each response variable yi \u223c b(ni, \u03c0(xi)) can be thought of as being composed from\nni binary variables. therefore one should distinguish between the total number of (binary)\nobservations n = n1 + \u00b7\u00b7\u00b7 + nn and the number of observed binomials. the model-specific\nprobabilities \u03c0i = \u03c0(xi) depend only on the measurement points x1, . . . , xn ; thus only n\npotentially differing probabilities are specified.\nlet \u00afyt = (\u00afy1, . . . , \u00afyn ) denote the observations where \u00afyi = yi/ni represents the relative\nfrequencies that are linked to yi = yi1 +\u00b7\u00b7\u00b7+yini\n\u223c b(ni, \u03c0i), the number of successes for the\nrepeated trials at measurement point xi. then the corresponding difference of log-likelihoods\nbased on (4.2) is given by\nn(cid:7)\n\nd(\u00afy, \u02c6\u03c0) = 2(l(\u00afy, \u00afy) \u2212 l(\u00afy, \u02c6\u03c0)) = 2\n\nni{\u00afyi log(\n\n) + (1 \u2212 \u00afyi) log(\n\n1 \u2212 \u00afyi\n1 \u2212 \u02c6\u03c0i\n\n)}.\n\n\u00afyi\n\u02c6\u03c0i\n\ni=1\n\nthe essential difference between the deviance for single binary observations and the deviance\nfor binomial distributions is in the definition of the saturated model. while in the first case\nobservations y1, . . . , yn are fitted perfectly by the saturated model, in the latter case the means\n\u00afy1, . . . , \u00afyn are fitted perfectly. this has severe consequences when trying to use the deviance\nas a measure of the goodness-of-fit of the model.\n\ndeviance as goodness-of-fit statistic\nsince the deviance may be derived as a likelihood ratio statistic, it is tempting to assume that the\ndeviance is asymptotically \u03c72-distributed. however, this does not hold for the binary variables\ndeviance d(y, \u02c6\u03c0) if n \u2192 \u221e. the reason is that the degrees of freedom are not fixed; they\nincrease with the sample size. thus there is no benchmark (in the form of an approximate\ndistribution) to which the absolute value of d(y, \u02c6\u03bc) may be compared. the case is different for\nbinomial distributions. if n, the number of measurement points, is fixed, and ni \u2192 \u221e for i =\n1, . . . , n, then the degrees of freedom are fixed, and, if the model holds, d(\u00afy, \u02c6\u03c0) is under weak\nconditions asymptotically \u03c72-distributed with d(\u00afy, \u02c6\u03c0) \u223c(a) \u03c72(n \u2212 dim(x)), where dim(x)\ndenotes the length of the vector x that is equivalent to the number of estimated parameters.\nthus, for ni sufficiently large, the \u03c72-distribution provides a benchmark to which the value of\nd(\u00afy, \u02c6\u03c0) may be compared. usually the model is considered to show an unsatisfactory fit, if\nd(\u00afy, \u02c6\u03c0) is larger than the 1 \u2212 \u03b1 quantile of the \u03c72-distribution for user-specified significance\nvalue \u03b1.\n\nnevertheless, since program packages automatically give the deviance, it is tempting to\ncompare the value of the deviance to the (often absurdly high) degrees of freedom that are\nfound for ungrouped binary data (see example 4.1). although the deviance does not provide\na goodness-of-fit statistic for simple binary observations, it is useful in residual analysis and\ninformal comparisons of link functions (see sections 4.3 and 5.1).\n\ntable 4.1: main effects model for unemployment data.\n\nestimate\n\nstd. error\n\nz-value\n\npr(>|z|)\n\nintercept\n\n1.2140\nage \u22120.0312\n0.6710\n\ngender\n\n0.2032\n0.0060\n0.1393\n\n5.97\n\u22125.18\n4.82\n\n0.0000\n0.0000\n0.0000\n\n "}, {"Page_number": 102, "text": "90\n\nchapter 4. modeling of binary data\n\nexample 4.1: unemployment\nin a study on the duration of unemployment with sample size n = 982 we distinguish between short-term\nunemployment (\u2264 6 months) and long-term unemployment (> 6 months). short-term unemployment is\nconsidered as success (y = 1). the response and the covariates are gender (1: male; 0: female) and age\nranging from 16 to 61 years of age. from the estimates of coefficients in table 4.1 it is seen that the pro-\nbability of short-term unemployment is larger for males than for females and decreases with age. the\ndeviance for ungrouped data, where the response is a 0-1 variable, is 1224.1 on 979 df. that deviance\ncannot be considered as a goodness-of-fit statistic. however, if data are grouped with the response repre-\nsented by the binomial distribution given a fixed combination of gender and age, the deviance is 87.16 on\n88 df. for the grouped case, 92 combinations of gender and age effects are possible; n = 91 of these\ncombinations are found in the data; therefore, the df are 91 \u2212 3 since three parameters have been fitted.\nin the grouped case the deviance has an asymptotic distribution and one finds that the main effect model\nis acceptable.\n\nexample 4.2: commodities in household\nin table 4.2 the fit of two linear logit models is shown. in the first model the response is \"car in household,\"\nand in the second model it is \"personal computer (pc) in household.\" for both models the only covariate\nis net income and only one-person households are considered. since the responses are strictly binary, the\ndeviance cannot be considered as a goodness-of-fit statistic that is asymptotically \u03c72-distributed. thus\nthe values of the deviance cannot be compared to the degrees of freedom. but since the number of\nobservations and the linear predictor are the same for both models, one might compare the deviance of the\ntwo models in an informal way. the linear logit model seems to fit the data better when the response \"pc\nin household\" is considered rather than \"car in household.\" however, it is not evaluated whether the fit is\nsignificantly better.\n\ntable 4.2: effects of net income on alternative responses for linear logit model.\n\n\u02c6\u03b20\n\u22122.42\n\u22123.88\n\n\u02c6\u03b2\n\n0.0019\n0.0011\n\ndeviance\n\n1497.7\n614.8\n\ndf\n\n1294\n1294\n\ncar\npc\n\n4.2.2 pearson statistic\nwhen proportions are considered and the ni is large, the deviance yields a goodness-of-fit\nstatistic. an alternative statistic in this setting is the pearson statistic:\n\nn(cid:7)\n\ni=1\n\np =\n\u03c72\n\n(\u00afyi \u2212 \u02c6\u03c0i)2\n\u02c6\u03c0i(1 \u2212 \u02c6\u03c0i) ,\n\nni\n\nwhere \u02c6\u03c0i = h(xt\ncally \u03c72-distributed with n \u2212 dim(x) degrees of freedom given the model holds.\ni\n\n\u02c6\u03b2). if n is fixed and ni \u2192 \u221e for i = 1, . . . , n, then \u03c72\n\np is also asymptoti-\n\nthe pearson statistic is familiar from introductory texts on statistics.\n\nin the analysis of\n\ncontingency tables it usually takes the form\n\n(observed cell counts\u2212expected cell counts)2\n\nexpected cell counts\n\n,\n\n(4.5)\n\n(cid:7)\n\ncells\n\n\u03c72 =\n\n "}, {"Page_number": 103, "text": "4.2. discrepancy between data and fit\n\n91\n\nn(cid:7)\n\nwhere the expected cell counts are computed under the assumption that the model under inves-\ntigation holds. thus one considers the discrepancy between the actual counts and what is to be\nexpected if the model holds. the denominator is a weight on the squared differences that takes\ndifferences less serious if the expected cell counts are large. in fact, this weighting scheme is\na standardization to obtain the asymptotic \u03c72-distribution. this is seen from showing that \u03c72\np\nhas the form (4.5). the fitting of the binary regression model \u03c0i = h(xt\ni \u03b2) may be seen within\nthe framework of contingency analysis with x1, . . . , xn refering to the rows and y = 0, y = 1\nrefering to the columns. then the probabilities within one row (corresponding to xi) are given\nby \u03c0i, 1 \u2212 \u03c0i; the cell counts are given by ni\u00afyi, ni \u2212 ni\u00afyi; and the (estimated) expected cell\ncounts are given by ni\u02c6\u03c0i, ni \u2212 ni\u02c6\u03c0i. from (4.5) one obtains\n\n(ni\u00afyi \u2212 ni\u02c6\u03c0i)2\n\n(ni\u00afyi \u2212 ni\u02c6\u03c0i)2\n\n,\n\ni=1\n\n+\n\nni\u02c6\u03c0i\n\n\u03c72 =\n\np . the essential difference be-\np is that in the form (4.5) the sum is across all cells of the\np the sum is only across the different values of xi,\n\nni \u2212 ni\u02c6\u03c0i\nwhich, after simple derivation, turns out to be equivalent to \u03c72\ntween the representation (4.5) and \u03c72\ncorresponding contingency table, while in \u03c72\nwhich correspond to rows.\nthe deviance and pearson\u2019s \u03c72 have the same asymptotic distribution. more concisely, one\npostulates for fixed n that n = n1 + \u00b7\u00b7\u00b7 + nn \u2192 \u221e and ni/n \u2192 \u03bbi, where \u03bbi \u2208 (0, 1). to\ndistinguish these assumptions from the common asymptotical conditions where only n \u2192 \u221e is\nassumed, one uses the term fixed cells asymptotics, referring to the assumption that the number\nof rows n is fixed. if the values of the deviance d and \u03c72\np differ strongly, this may be taken\nas a hint that the requirement of fixed cells asymptotics does not hold and both test statistics\nare not reliable (see also section 8.6.2, where alternative asymptotic concepts that hold for the\nmore general power-divergence family of goodness-of-fit statistics are briefly discussed).\n\ngoodness-of-fit statistics for proportions\n\npearson statistic\n\np =\n\u03c72\n\ndeviance\n\n(cid:29)\n\nni\n\n\u00afyi log\n\nn(cid:7)\n\ni=1\n\nd = 2\n\napproximation (ni/n \u2192 \u03bbi)\n\nn(cid:7)\n\ni=1\n\n(cid:25)\n\n\u00afyi\n\u02c6\u03c0i\n\nni\n\n(cid:26)\n\n(\u00afyi \u2212 \u02c6\u03c0i)2\n\u02c6\u03c0i(1 \u2212 \u02c6\u03c0i)\n\n+ (1 \u2212 \u00afyi) log\n\n(cid:26)(cid:30)\n\n(cid:25)\n\n1 \u2212 \u00afyi\n1 \u2212 \u02c6\u03c0i\n\n\u03c72\n\np , d\n\n(a)\u223c \u03c72(n \u2212 p)\n\na general problem with global or omnibus tests to assess the fit of a parametric model is\nthat large values of the test statistic indicate lack-of-fit but do not show the particular reason for\nthe lack-of-fit. it is only by comparing fits that one may get some insight about its nature. in\nsection 4.4 nested models are compared that differ in the predictor. in that case the difference of\ndeviances has a fixed asymptotic distribution. in the case of different link functions, considered\nin section 5.1, the models are not nested, but a comparison of the goodness-of-fit measures may\nshow which link function has the best fit.\n\n "}, {"Page_number": 104, "text": "92\n\nchapter 4. modeling of binary data\n\n4.2.3 goodness-of-fit for continuous predictors\nthe deviance and pearson\u2019s \u03c72\np may be used to assess the adequacy of the model when the\nresponse is binomial with not too small cell counts. they cannot be used when predictors are\ncontinuous since then cell counts would always be one and there would be no benchmark in\nthe form of a \u03c72-statistic available (see also example 4.2). an easy way out seems to be to\ncategorize all of the continuous variables. however, then one looses power and it works only in\nthe case of one or two continuous variables. when more continuous variables are in the model\nit is not clear how to find categories that contain enough observations. several methods have\nbeen proposed to assess the fit of a model when continuous variables are present. we briefly\nconsider some of them in the following.\n\nhosmer-lemeshow test\nhosmer and lemeshow (1980) proposed a pearson-type test statistic where the categorizing is\nbased on the fitted values. one orders the responses according to the fitted probabilities and then\nforms n equally sized groups. thus the quantiles of the response values determine the groups.\nmore precisely, the n/n observations with the smallest fitted probabilities form the first group,\nthe next n/n observations form the second group, and so on. hosmer and lemeshow (1980)\nproposed to use n = 10 groups that are called \"deciles of risk.\"\n\nlet yij denote the jth observation in the ith group, where i = 1, . . . , n, j = 1, . . . , ni.\nni\nj=1 yij/ni, is compared to the\nni\nj=1 \u02c6\u03c0ij/ni, where \u02c6\u03c0ij denotes the fitted value for\n\nthen the average of the observations of the ith group, \u00afyi =\naverage of the fitted probabilities, \u02c6\u03c0i =\nobservation yij in a pearson-type statistic:\n\n(cid:14)\n\n(cid:14)\nn(cid:7)\n\ni=1\n\nhl =\n\u03c72\n\n(\u00afyi \u2212 \u02c6\u03c0i)2\n\u02c6\u03c0i(1 \u2212 \u02c6\u03c0i) .\n\nni\n\nthe test statistic has a rather complicated distribution, but hosmer and lemeshow (1980)\nshowed by simulations that the asymptotic distribution can be approximated by a \u03c72-distribution\nwith df = n \u2212 2.\n\nsince the grouping is based on the model that is assumed to hold, one cannot expect good\npower for the hosmer-lemeshow statistic, and indeed the test statistic has moderate power.\nmore importantly, as for all global tests, a large value of the test statistic indicates lack-of-fit\nbut does not show if, for example, the linear term has been misspecified.\n\na methodology that is similar to the hosmer-lemeshow approach but makes use of cat-\negorical variables in the model has been proposed by pulkstenis and robinson (2002). an\nadjusted hosmer-lemeshow test that uses an alternative standardization was proposed by pi-\ngeon and heyse (1999). kuss (2002) showed that hosmer-lemeshow-type tests are numerically\nunstable.\n\nalternative tests\nalternative test strategies may be based on the score test. brown (1982) embedded the logit\nmodel into a family of models and proposed a test for the specific parameters that select the\nlogit model. tsiatis (1980) divided the space of the covariate into distinct regions and tested if\na term that is constant in these regions may be omitted from the linear predictor.\n\nan alternative approach for goodness-of-fit tests uses non-parametric regression techniques.\n(1989) proposed a pseudo-likelihood ratio test by using a kernel-smoothed\nazzalini et al.\nestimate of the response probabilities. for binomial data, yi \u223c b(ni, \u03c0(xi)), a smooth\n\n "}, {"Page_number": 105, "text": "4.3. diagnostic checks\n\n93\n\nnonparametric estimate of the response probability (for a unidimensional predictor) is\n\n\u02c6\u03c0(x) =\n\nyik( x \u2212 xi\n\nh\n\n)/\n\nnik( x \u2212 xi\n\nh\n\n).\n\nn(cid:7)\n\ni=1\n\nn(cid:7)\n\ni=1\n\nthe hypotheses to be investigated are\n\nh0 : \u03c0(x) = \u03c0(x, \u03b2) specified by a parametric model\nh1 : \u03c0(x) is a smooth function.\n\nthe corresponding pseudo-likelihood ratio statistic is\n\nn(cid:7)\n\ni=1\n\nyi log(\n\n\u02c6\u03c0(xi)\n\u03c0(x, \u02c6\u03b2)\n\n) + (ni \u2212 yi) log(\n\n1 \u2212 \u02c6\u03c0(xi)\n1 \u2212 \u03c0(x, \u02c6\u03b2)\n\n),\n\nwhere \u02c6\u03c0(x) is the smoothed estimate based on a selected smoothing parameter h. since the test\nstatistic is not asymptotically \u03c72, the null hypothesis behavior of the test statistic is examined\nby simulating data from the fitted parametric model.\n\ni\n\n(cid:14)\n\n(cid:14)\n\nh ). the test statistic has the form t =\n\n(cid:14)\ni k( x\u2212xi\nh )/\n{k(x\u2212 xi)/h)}2/\n\nrather than estimating the probability by smoothing techniques, one may also use smooth\nestimates of the standardized residuals. lecessie and van houwelingen (1991) smooth the (un-\n(cid:14)\ngrouped) pearson residuals r(xi) = rp (yi, \u02c6\u03c0i) (see next section) to obtain \u02dcr(x) =\ni r(xi)\nk( x\u2212xi\ni \u02dcr(xi)v(xi), where v(xi) =\ni k((x\u2212 xi)/h)2 is the inverse of the variance of the smoothed resid-\nuals. lecessie and van houwelingen (1991) derive the mean and the variance and demonstrate\nthat the test statistic may be approximated by a normal distribution. the equivalence to a\nscore test in a random effects model was shown by lecessie and van houwelingen (1995). the\napproaches may be extended to multivariate predictors by using multivariate kernels but are\nrestricted to few dimensions since kernel-based estimates for higher dimensions suffer from the\ncurse of dimensionality (see section 10.1.4).\n\n(cid:14)\n\n4.3 diagnostic checks\ngoodness-of-fit tests provide only global measures of the fit of a model. they tell nothing about\nthe reason for a bad fit. regression diagnostics aims at identifying reasons for it. diagnostic\nmeasures should in particular identify observations that are not well explained by the model as\nwell as those that are influential for some aspect of the fit.\n\n4.3.1 residuals\nthe goodness-of-fit statistics from the preceding section provide global measures for the dis-\ncrepancy between the data and the fit. in particular, if the fit is bad, one wants to know if all\nthe observations contribute to the lack-of-fit or if the effect is due to just some observations.\nresiduals measure the agreement between single observations and their fitted values and help\nto identify poorly fitting observations that may have a strong impact on the overall fit of the\nmodel. in the following we consider responses from a binomial distribution.\n\nfor scaled binomial data the pearson residual has the form\n\nrp (\u00afyi, \u02c6\u03c0i) =\n\n(cid:16)\n\n\u00afyi \u2212 \u02c6\u03c0i\n\u02c6\u03c0i(1 \u2212 \u02c6\u03c0i)/ni\n\n.\n\n "}, {"Page_number": 106, "text": "94\n\nchapter 4. modeling of binary data\n\n(cid:14)\n\nit is just the raw residual scaled by the estimated standard deviation of \u00afyi. it is also the signed\np =\nsquare root of the corresponding component of the pearson statistics that has the form \u03c72\ni rp (\u00afyi, \u02c6\u03c0i)2. for small ni the distribution of rp (\u00afyi, \u02c6\u03c0i) is rather skewed, an effect that is\n\nameliorated by using the transformation to anscombe residuals:\n\nra(\u00afyi, \u02c6\u03c0i) =\n\n\u2019\n\nwhere t(u) =\nconsider an approximation to\n\nu\n0 s\n\n\u221a\nni\n\nt(\u00afyi) \u2212 [t(\u02c6\u03c0i) + (\u02c6\u03c0i(1 \u2212 \u02c6\u03c0i))\u22121/3(2\u02c6\u03c0i \u2212 1)/6ni]\n\n(\u02c6\u03c0i(1 \u2212 \u02c6\u03c0i))1/6\n\n,\n\n\u22121/3(1 \u2212 s)\u22121/3ds (see pierce and schafer, 1986). ansombe residuals\n\n(cid:16)\nt(\u00afyi) \u2212 e(t(\u00afyi))\nvar(t(\u00afyi))\n\nby use of the delta method, which yields\n\n(cid:16)\nt(\u00afyi) \u2212 t(e(t(\u00afyi)))\nt(cid:3)(e(\u00afyi))\nvar(\u00afyi)\n\n.\n\nthe pearson residual cannot be expected to have unit variance because the variance of the\nresidual has not been taken into account. the standardization in rp (\u00afyi; \u02c6\u03c0i) just uses the es-\ntimated standard deviation of \u00afyi. as shown in section 3.10, the variance of the residual vec-\ntor may be approximated by \u03c3 1/2(i \u2212 h)\u03c3 t /2, where the hat matrix is given by h =\nw t /2x(x t w x)\u22121x t w 1/2. therefore, when looking for ill-fitting observations, one\nprefers the standardized pearson residuals:\n\nrp,s(\u00afyi, \u02c6\u03c0i) =\n\n(1 \u2212 hii)\u02c6\u03c0i(1 \u2212 \u02c6\u03c0i)/ni\n\n,\n\n\u00afyi \u2212 \u02c6\u03c0i\n\n(cid:16)\n\u221a\n\n(cid:14)\n\nwhere hii denotes the ith diagonal element of h. the standardized pearson residuals are\nsimply the pearson residuals divided by\n\nalternative residuals derive from the deviance. from the basic form of the deviance d =\ni rd(yi, \u02c6\u03c0i)2 one obtains\n\n1 \u2212 hii.\n(cid:25)\n\n(\nni{\u00afyi log\nrd(\u00afyi, \u02c6\u03c0i) = sign(\u00afyi \u2212 \u02c6\u03c0i)\n(cid:16)\nrd(yi, \u02c6\u03c0i) = sign(yi \u2212 \u02c6\u03c0i)\n\n(cid:26)\n\n\u00afyi\n\u02c6\u03c0i\n\nwhere sign(\u00afyi \u2212 \u02c6\u03c0i) is 1 when \u00afyi \u2265 \u02c6\u03c0i and is \u22121 when \u00afyi < \u02c6\u03c0i. for the special case ni = 1 it\nsimplifies to\n\na transformation that yields a better approximation to the normal distribution is the adjusted\nresiduals:\n\nrd\u02c6a(\u00afyi, \u02c6\u03c0i) = rd(\u00afyi, \u02c6\u03c0i) + (1 \u2212 2\u02c6\u03c0i)/\n\n(cid:25)\n\n(cid:26)\n},\n\n1 \u2212 \u00afyi\n1 \u2212 \u02c6\u03c0i\n\n+ (1 \u2212 \u00afyi) log\n\n\u2212 log(1 \u2212 |yi \u2212 \u02c6\u03c0i|).\n\n(cid:16)\nni\u02c6\u03c0i(1 \u2212 \u02c6\u03c0i) \u2217 36.\n\u221a\n\n1 \u2212 hii :\n\nstandardized deviance residuals are obtained by dividing by\n\u221a\nrd,s(\u00afyi, \u02c6\u03c0i) = rd(\u00afyi, \u02c6\u03c0i)\n1 \u2212 hii\n\n.\n\ntypically residuals are visualized in a graph. in an index plot the residuals are plotted against\nthe observation number, or index. it shows which observations have large values and may be\nconsidered outliers. for finding systematic deviations from the model it is often more infor-\nmative to plot the residuals against the fitted linear predictor. if one suspects that particular\n\n "}, {"Page_number": 107, "text": "4.3. diagnostic checks\n\n95\n\nvariables should be transformed before being included in the linear predictor, one may also plot\nresiduals against the ordered fitted values of that explanatory variable.\n\nan alternative graph compares the standardized residuals to the order statistic of an n(0, 1)-\nsample. in this plot the residuals are ordered and plotted against the corresponding quantiles of\na normal distribution. if the model is correct and residuals can be expected to be approximately\nnormally distributed (depending on local sample size), the plot should show approximately a\nstraight line as long as outliers are absent.\n\nexample 4.3: unemployment\nin a study on the duration of unemployment with sample size n = 982 we distinguish between short-\nterm unemployment (\u2264 6 months) and long-term unemployment (> 6 months). for illustration, a linear\nlogit model is fitted with the covariate age, ranging from 16 to 61 years of age. figure 4.2 shows the\nfitted response function. it is seen that in particular for older unemployed persons, the fitted values tend\nto be larger than the observed proportions. the effect is also seen in the plot of residuals against fitted\nvalues in figure 4.3. the quantile plot in figure 4.3 shows a rather straight line but the slope is rather\nsteep, indicating that the variability of deviances differs from that of a standard normal distribution. a\nless restrictive nonparametric fit of the data is considered in example 10.1.\n\nfigure 4.2: observations and fitted probabilities for unemployment data plotted against\nage.\n\nfigure 4.3: deviance residuals for unemployment data plotted against fitted values (left)\nand quantile plot (right).\n\n "}, {"Page_number": 108, "text": "96\n\nchapter 4. modeling of binary data\n\ntable 4.3: short- and long-term unemployment depending on age\n\nobserv.\n\nage\n\nobserv.\n\nage\n\nobserv.\n\nage\n\n1\n\n16\n\n2\n3\n\n17\n\n32\n\n8\n3\n\n33\n\n48\n\n10\n3\n\n2\n\n17\n\n11\n8\n\n18\n\n33\n\n14\n7\n\n34\n\n49\n\n7\n4\n\n3\n\n18\n\n31\n9\n\n19\n\n34\n\n11\n8\n\n35\n\n50\n\n3\n5\n\n4\n\n19\n\n42\n20\n\n20\n\n35\n\n13\n9\n\n36\n\n51\n\n8\n5\n\n5\n\n20\n\n50\n17\n\n21\n\n36\n\n15\n7\n\n37\n\n52\n\n3\n2\n\n6\n\n21\n\n54\n26\n\n22\n\n37\n\n6\n7\n\n38\n\n53\n\n1\n6\n\n7\n\n22\n\n43\n16\n\n23\n\n38\n\n5\n6\n\n39\n\n54\n\n2\n4\n\n8\n\n23\n\n35\n16\n\n24\n\n39\n\n11\n7\n\n40\n\n55\n\n2\n3\n\n9\n\n24\n\n25\n12\n\n25\n\n40\n\n9\n4\n\n41\n\n56\n\n2\n1\n\n10\n\n25\n\n27\n8\n\n26\n\n41\n\n14\n5\n\n42\n\n57\n\n3\n6\n\n11\n\n26\n\n21\n10\n\n27\n\n42\n\n7\n6\n\n43\n\n58\n\n2\n4\n\n12\n\n27\n\n21\n10\n\n28\n\n43\n\n13\n8\n\n44\n\n59\n\n2\n7\n\n13\n\n28\n\n19\n7\n\n29\n\n44\n\n4\n2\n\n45\n\n60\n\n3\n5\n\n14\n\n29\n\n22\n8\n\n30\n\n45\n\n10\n6\n\n46\n\n61\n\n0\n1\n\n1\n0\n\n1\n0\n\n1\n0\n\n15\n\n30\n\n17\n10\n\n31\n\n46\n\n9\n4\n\n16\n\n31\n\n14\n11\n\n32\n\n47\n\n9\n6\n\nexample 4.4: food-stamp data\ndata that have often been used in diagnostics of binary data are the food-stamp data from k\u00fcnsch et al.\n(1989), which consist of n = 150 persons, 24 of whom participated in the federal food-stamp program.\nthe response indicates participation, and the predictor variables represent the dichotomous variables\ntenancy (ten), supplemental income (sup), as well as the log-transformation of the monthly income,\nlog(monthly income + 1) (lmi). k\u00fcnsch et al. (1989) show that two values are poorly accounted for\nby the logistic model and are most influential for the ml fit. if these two observations and four other\nobservations are left out, the data have no overlap and the ml estimate does not exist. figure 4.4 shows\nquantile plots for pearson and anscombe residuals. it is seen that anscombe residuals are much closer to\nthe normal distribution than pearson residuals.\n\nfigure 4.4: pearson (left) and anscombe (right) residuals for food-stamp data.\n\n4.3.2 hat matrix and influential observations\nthe iteratively reweighted least-squares fitting (see section 3.9) that can be used to compute\nthe ml estimate has the form\n\n\u02c6\u03b2\n\n(k+1) = (x t w (\u02c6\u03b2\n\n(k))x)\n\n\u22121x t w (\u02c6\u03b2\n\n(k))\u02dc\u03b7(\u02c6\u03b2\n\n(k))\n\n "}, {"Page_number": 109, "text": "4.3. diagnostic checks\n\n97\nwith the adjusted variable \u02dc\u03b7(\u02c6\u03b2) = \u02c6\u03b7 + d(\u02c6\u03b2)\u22121(y \u2212 \u03bc(\u02c6\u03b2)), where \u02c6\u03b7 = x \u02c6\u03b2 and the diagonal\nn) and d(\u02c6\u03b2) = (\u2202h(\u02c6\u03b71)/\u2202\u03b7,\nmatrices w = diag ((\u2202h(\u02c6\u03b71)/\u03b7)2/\u03c32\n. . . , \u2202h(\u02c6\u03b7n)/\u2202\u03b7). at convergence one obtains\n\n1, . . . , (\u2202h(\u02c6\u03b7n)/\u2202\u03b7)2/\u03c32\n\n\u22121x t w (\u02c6\u03b2)\u02dc\u03b7(\u02c6\u03b2).\nthus \u02c6\u03b2 may be seen as the least-squares solution of the linear model\n\n\u02c6\u03b2 = (x t w (\u02c6\u03b2)x)\n\nw 1/2\u02dc\u03b7(\u02c6\u03b2) = w 1/2x\u03b2 + \u02dc\u03b5,\n\nwhere, in w = w (\u02c6\u03b2), the dependence on \u02c6\u03b2 is suppressed. the corresponding hat matrix has\nthe form\n\nh = w 1/2x(x t w x)\n\n\u22121x t w 1/2.\n\nsince the matrix h is idempotent and symmetric, it may be seen as a projection matrix for\nwhich tr(h) = rank(h) holds. moreover, one obtains for the diagonal elements of h =\n(hij) 0 \u2264 hij \u2264 1 and tr(h) = p (if x has full rank).\n\nthe equation w 1/2\u02c6\u03b7 = hw 1/2\u02dc\u03b7(\u03b2) shows how the hat matrix maps the adjusted variable\n\u02dc\u03b7(\u03b2) into the fitted values \u02c6\u03b7. thus h may be seen as the matrix that maps the adjusted\nobservation vector w 1/2\u02dc\u03b7 into the vector of \"fitted\" values w 1/2\u02c6\u03b7, which is a mapping on the\ntransformed predictor space. moreover, it may be shown that approximately\n\n\u22121/2(\u02c6\u03bc \u2212 \u03bc) (cid:17) h\u03c2\n\n\u22121/2(y \u2212 \u03bc)\n\n\u03c3\n\n(4.6)\n\nholds, where \u03c3 = \u03c3(\u02c6\u03b2). thus h = (hij) may be seen as a measure of the influence of y on\n\u02c6\u03bc in standardized units of changes.\n\nin summary, large values of hii should be useful in detecting influential observations. but\nit should be noted that, in contrast to the normal regression model, the hat matrix depends on\n\u02c6\u03b2 because w = w (\u02c6\u03b2). the essential difference from an ordinary linear regression is that the\nhat matrix does depend not only on the design but also on the fit.\n\na further property that is not too hard to derive is\n\u22121/2 \u02c6\u03bc = h\u03c2\n\nh\u03c2\n\n\u22121/2y,\n\nmeaning that the orthogonal projection (based on h) of standardized values \u03c3\nare identical. with \u03c7 = \u03c3\n\n\u22121/2(y \u2212 \u02c6\u03bc) denoting the standardized residual, one has\n\n\u22121/2 \u02c6\u03bc, \u03c3\n\n\u22121/2y\n\nh\u03c7 = 0 and (i \u2212 h)\u03c7 = \u03c7.\n\np = \u03c7t \u03c7. the matrix h has the\nthere is a strong connection to the pearson \u03c72\nform h = (w t /2x)(x t w 1/2w t /2x)\u22121(x t w 1/2), which shows that the projection is\ninto the subspace that is spanned by the columns of x t w 1/2.\n\np statistic since \u03c72\n\n4.3.3 case deletion\na strategy to investigate the effect of single observations on the parameter estimates is to com-\npare the estimate \u02c6\u03b2 with the estimate \u02c6\u03b2(i), obtained from fitting the model to the data without\nthe ith observation. an overall measure that includes all the components of the vector of coef-\nficients is due to cook (1977). cook\u2019s distance for observation i has the form\n\u2212 \u02c6\u03b2)t x t w x(\u02c6\u03b2(i)\n\n\u2212 \u02c6\u03b2) = (\u02c6\u03b2(i)\n\n\u2212 \u02c6\u03b2)t cov(\u02c6\u03b2)\n\nci = (\u02c6\u03b2(i)\n\n\u22121(\u02c6\u03b2(i)\n\n\u2212 \u02c6\u03b2).\n\n "}, {"Page_number": 110, "text": "98\n\nchapter 4. modeling of binary data\n\nit may be seen as a confidence interval displacement diagnostic resulting from the exclusion\nof the ith observation. it is derived from the asymptotic confidence region for \u02c6\u03b2 given by the\nlikelihood distance \u22122{l(\u03b2) \u2212 l(\u02c6\u03b2)} = c. by approximation of l(\u03b2) by a second-order taylor\napproximation at \u02c6\u03b2 one obtains\n\nci = (\u03b2 \u2212 \u02c6\u03b2)t cov(\u02c6\u03b2)\n\n\u22121(\u03b2 \u2212 \u02c6\u03b2).\n\ncook\u2019s distance is obtained by replacing \u03b2 by \u02c6\u03b2(i) and using an estimate of cov(\u02c6\u03b2)\u22121 that is\ncomposed from previously defined elements.\ncomputation of \u02c6\u03b2(i) requires an iterative procedure for each observation. this may be\navoided by using approximations. one may approximate \u02c6\u03b2(i) by a one-step estimate, which is\nobtained by performing one fisher scoring step starting from \u02c6\u03b2. the corresponding approxi-\nmation has the form\n\nci,1 = hiir2\n\np,i/(1 \u2212 hii)2,\n\nwhere rp,i = rp (\u00afyi, \u02c6\u03c0i) is the pearson residual for the ith observation and hii is the ith diagonal\nelement of the hat matrix h. the approximation is based on the one-step approximation of\n\u02c6\u03b2(i):\n\n\u02c6\u03b2(i)\n\n\u2248 \u02c6\u03b2 \u2212 w1/2\n\nii rp,i(x t w x)\n\n\u22121xi/(1 \u2212 hii),\n\nwhere xi is the covariate vector of observation i and wii is the ith diagonal element of w .\n\nlarge values of cook\u2019s measure ci or its approximation indicate that the ith observation\nits presence determines the value of the parameter vector. a useful way of\n\nis influential.\npresenting this measure of influence is in an index plot.\n\nexample 4.5: unemployment\ncook\u2019s distances for unemployment data (figure 4.5) show that observations 33, 38, 44, which correspond\nto ages 48, 53, 59, are influential. all three observations are rather far from the fit.\n\nfigure 4.5: cook distance for unemployment data.\n\nexample 4.6: exposure to dust (non-smokers)\nin a study on the effects of dust on bronchitis conducted in a german plant, the observed covariates were\nmean dust concentration at working place in mg/m3 (dust), duration of exposure in years (years), and\n\n "}, {"Page_number": 111, "text": "4.3. diagnostic checks\n\n99\n\nfigure 4.6: dust exposure data, non-smokers with bronchitis (left panel), non-smokers\nwithout bronchitis (right panel).\n\nsmoking (1: yes; 0: no). bronchitis is considered the binary response (1: present; 0: not present). the\ntotal sample was n = 1246. in previous analysis the focus has been on the estimation of threshold limiting\nvalues (ulm, 1991; k\u00fcchenhoff and ulm, 1997).\n\nin the following we will first consider the subsample of non-smokers. figure 4.6 shows the obser-\nvations for workers with bronchitis and without bronchitis. it is seen that there is one person with an\nextreme value in the observation space. since interaction terms and quadratic terms turn out not to con-\ntribute significantly to the model fit, in the following the main effect logit model is used. table 4.4 shows\nthe estimated coefficient for the main effects model. while years of exposure is highly significant, dust\nconcentration is not significant in the subsample of non-smokers. figure 4.7 shows cook\u2019s distances for\nthe subsample. observations that show large values of cook\u2019s distance are observations 730, 1175, 1210,\nwhich have values (1.63, 8), (8, 32), (8, 13) for (dust, years); all three observations correspond to persons\nwith bronchitis. the observations are not extreme in the range of years, which is the influential variable.\nthe one observation (15.04, 27), which is very extreme in the observation space, corresponds to observa-\ntion 1245, which is the last in the plot of cook\u2019s distances and has a rather small value of cook\u2019s distance.\nthe fit without that observation, as given in table 4.5, shows that the coefficient for concentration of dust\n\ntable 4.4: main effects model for dust exposure data (non-smokers).\n\nestimate\nintercept \u22123.1570\n0.0053\n0.0532\n\ndust\nyears\n\nstd. error\n\n0.4415\n0.0564\n0.0132\n\nz-value\n\u22127.15\n0.09\n4.04\n\npr(>|z|)\n\n0.0000\n0.9248\n0.0001\n\ntable 4.5: main effects model for dust exposure data without one observation (non-\nsmokers).\n\nestimate\nintercept \u22123.1658\n0.0120\n0.0529\n\ndust\nyears\n\nstd. error\n\n0.4419\n0.0580\n0.0131\n\nz-value\n\u22127.16\n0.21\n4.03\n\npr(>|z|)\n\n0.0000\n0.8361\n0.0001\n\n "}, {"Page_number": 112, "text": "100\n\nchapter 4. modeling of binary data\n\nfigure 4.7: cook distances for dust data (non-smokers).\n\nhas distinctly changed. however, the variable dust shows no significant effect, and therefore it is only a\nconsequence that the cook\u2019s distance is small.\n\nexample 4.7: exposure to dust\nin the following the exposure data, including non-smokers, is used. it turns out that for the full dataset\nconcentration of dust, years of exposure, and smoking are significantly influential (see table 4.6). again\ninteraction effects between the covariates can be omitted. figure 4.8 shows the observation space for all\n\ntable 4.6: main effects model for dust exposure data.\n\nestimate\n(intercept) \u22123.0479\n0.0919\n0.0402\n0.6768\n\ndust\nyears\nsmoking\n\nstd. error\n\nz-value\n0.2486 \u221212.26\n3.95\n0.0232\n0.0062\n6.47\n3.88\n0.1744\n\npr(>|z|)\n\n0.0000\n0.0001\n0.0000\n0.0001\n\nfigure 4.8: dust exposure data.\n\n "}, {"Page_number": 113, "text": "4.4. structuring the linear predictor\n\n101\n\nfigure 4.9: cook distances for dust exposure data.\n\nof the observations. it is seen that also in the full dataset one observation is positioned very extreme in the\nobservation space. that observation (1246) is seen to have an extreme value of cook\u2019s distance (figure\n4.9). when that extreme value is excluded, coefficient estimates for the variables years and smoking are\nsimilar to the estimates for the full dataset. however, coefficients differ for the variable concentration of\ndust by about 8% (see table 4.7). since observation 1246 is very far away from the data and the mean\nexposure is a variable that is not easy to measure exactly, one might suspect that the value of that variable\nis not trustworthy. one might consider the observation an outlier that should be omitted since it yields\nbiased estimates.\n\ntable 4.7: main effects model for dust exposure data without one observation.\n\nestimate\n(intercept) \u22123.0620\n0.0992\n0.0398\n0.6816\n\ndust\nyears\nsmoking\n\nstd. error\n\nz-value\n0.2491 \u221212.29\n4.15\n0.0239\n6.40\n0.0062\n0.1745\n3.91\n\npr(>|z|)\n\n0.0000\n0.0000\n0.0000\n0.0001\n\n4.4 structuring the linear predictor\nthe structuring of the linear predictor, and in particular the coding of categorical predictors,\nhave already been considered briefly in section 1.4. in the following we again have a look at\nthe linear predictor and introduce a notation scheme due to wilkinson and rogers (1973).\n\n4.4.1 the linear predictor: continuous predictors, factors, and\n\ninteractions\n\nthe parametric binary regression model\n\n\u03c0(x) = p (y = 1|x) = h(xt \u03b2)\n\n "}, {"Page_number": 114, "text": "102\n\nchapter 4. modeling of binary data\n\nthat is considered here contains the linear predictor \u03b7 = xt \u03b2. in the simplest case the linear\npredictor contains the p variables x1, . . . , xp in a main effect model of the form\n\n\u03b7 = \u03b20 + x1\u03b21 + \u00b7\u00b7\u00b7 + xp\u03b2p.\n\nhowever, frequently one finds that some interaction terms are necessary in the predictor, yield-\ning a linear predictor of the form\n\n\u03b7 = \u03b20 + x1\u03b21 + \u00b7\u00b7\u00b7 + xp\u03b2p + x1x2\u03b212 + x1x3\u03b213 + \u00b7\u00b7\u00b7 .\n\nwhen some of the covariates are continuous, polynomial terms also may be included in the\nlinear predictor, which means that the predictor is still linear in the parameters but not linear in\nthe variables. if, for example, x1 is continuous, a more flexible predictor containing non-linear\neffects of x1 is\n\n\u03b7 = \u03b20 + x1\u03b21 + \u00b7\u00b7\u00b7 + xp\u03b2p + x2\n\n1\u03b2(2)\n\n1 + x3\n\n1\u03b2(3)\n\n1\n\n\u00b7\u00b7\u00b7 .\n\nin many applications the influential variables are factors that may take a finite number of values.\nas in classical regression, these categorical covariates are included in the predictor in the form\nof dummy variables. when a categorical variable or factor a takes values in {1, . . . , k} the\nlinear predictor has the form\n\n\u03b7 = \u03b20 + xa(1)\u03b2a(1) + . . . + xa(k\u22121)\u03b2a(k\u22121),\n\nwhere xa(i) are dummy variables. in (0-1)-coding one has xa(i) = 1 if a = i and xa(i) = 0\notherwise, which implies that a = k is used as a reference category. when an additional\ncontinuous covariate x is available, the predictor of the main effects model becomes\n\n\u03b7 = \u03b20 + x\u03b2x + xa(1)\u03b2a(1) + \u00b7\u00b7\u00b7 + xa(k\u22121)\u03b2a(k\u22121).\n\n(4.7)\n\nthe inclusion of an interaction between the continuous predictor and the categorical variables\nmeans that all the products xxa(i) are included, yielding the more complicated predictor\n\n\u03b7 = \u03b20 + x\u03b2x + xa(1)\u03b2a(1) + \u00b7\u00b7\u00b7 + xa(k\u22121)\u03b2a(k\u22121) + xxa(1)\u03b2x,a(1) + \u00b7\u00b7\u00b7\n+ xxa(k\u22121)\u03b2x,a(k\u22121).\n\n(4.8)\n\ninteraction of this form means that the effect of one covariate is modified by the other. from\n\nthe restructuring\n\n\u03b7 = \u03b20 + \u00b7\u00b7\u00b7 + x(\u03b2x + xa(1)\u03b2x,a(1) + \u00b7\u00b7\u00b7 ) + \u00b7\u00b7\u00b7\n\none sees that the slope of x is modified by the categorical variable, yielding different slopes for\ndifferent categories of a. the restructuring\n\n\u03b7 = \u03b20 + \u00b7\u00b7\u00b7 + xa(i)(\u03b2a(i) + x\u03b2x,a(i)) + \u00b7\u00b7\u00b7\n\nshows how the effects of variable a are modified by the value of x. interactions between two\nfactors a \u2208 {1, . . . , ka} and b \u2208 {1, . . . , kb} may be modeled by including all products\nof dummy variables xa(i)xb(j). for the interpretation of interactions that is familiar from\nlinear modeling, one should have in mind that the the effects do not refer directly to the mean\ne(y|x) but to the transformed mean g(e(y|x)), which in the case of the logit model is the\nlogits log(\u03c0(x))/(1 \u2212 \u03c0(x)). in particular, for categorical predictors, interaction parameters\nhave simple interpretations in terms of odds ratios (exercise 4.4).\n\nsince the form of the linear predictor is rather tedious to write down, it is helpful to use a\nnotation that is due to wilkinson and rogers (1973). the notation uses operators to combine\npredictors in model formula terms. let x, z denote continuous, metrically scaled variables and\na, b, c denote factors.\n\n "}, {"Page_number": 115, "text": "4.4. structuring the linear predictor\n\n103\n\nbasic model term\nthe model terms x and a stand for themselves. however, the linear predictor that is built\ndepends on the type of variable. for a continuous variable x, the model term x means that\nx\u03b2x is included. for the factor a, the term to be included is a set of dummy variables with\ncorresponding weights xa(i)\u03b2a(i).\n\nthe + operator\nthe + operator in a model formula means that the corresponding terms are added in the al-\ngebraic expression of the predictor. therefore x + a means that the predictor has the form\n(4.7).\n\nthe dot operator\nthe dot operator is used to form products of the constituent terms. for example, the model\nterm x.z refers to the algebraic expression xz\u03b2x,z, whereas x.a refers to the algebraic ex-\npression containing all products xxa(i), which is the interaction term in (4.8). of course x.z is\nequivalent to z.x. polynomial terms of continuous variables my be built by x.x, referring to\nx2\u03b2. the dot operator dominates the + operator, so that a.b + x is equivalent to (a.b) + x.\nthe notation of higher interaction terms is straightforward. a.b.c denotes inclusion of the set\nof products xa(i)xb(j)xc(k). of course it helps that the dot operator is commutative, that is,\n(a.b).c is equivalent to a.(b.c). it should be noted that sometimes different notations are\nused for the dot operator. for example, the statistical software r uses the notation a : b for\na.b.\n\nthe crossing operator\nthe crossing operator is helpful for including marginal effects. the model term a \u2217 b is an\nabbreviation for a+ b + a.b. similarly, the term a\u2217 b\u2217 c means that the main effects and all\ntwo factor interactions are included. it is abbreviated as a+b+c+a.b+a.c+b.c+a.b.c.\ncrossing is distributive, meaning that a \u2217 (b + c) is equivalent to a \u2217 b + a \u2217 c (and to\na + b + c + a.b + a.c).\n\ntable 4.8: wilkinson-rogers notation for linear predictions with metric covariates x, z\nand factors a, b.\n\nmodel term linear predictor\n\nlinear predictor\n\nsingle\n\naddition\n\nx\na\n\nx + z\na + b\nx + a\n\ninteraction\n\nx.z\na.b\nx.a\nhierarchical x \u2217 z\ninteraction\na \u2217 b\nx \u2217 a\n\nx\u03b2x\n\n(cid:2)\n(cid:2)\n\nxz\u03b2x,z\n\n(cid:2)\n(cid:2)\n\ns xa(s)\u03b2a(s)\n\nx\u03b2x + z\u03b2z\n\n(cid:2)\n(cid:2)\ns xb(s)\u03b2b(s)\ns xa(s)\u03b2a(s)\n\ns xa(s)\u03b2a(s) +\n\nx\u03b2x +\n\ns,r xa(s)xb(r)\u03b2ab(s,r)\ns xxa(s)\u03b2xa(s)\n\n(cid:2)\nr xb(r)\u03b2b(r)\n\nx\u03b2x + z\u03b2z + xz\u03b2xz\n\n(cid:2)\n(cid:2)\ns xa(s)\u03b2a(s) +\n(cid:2)\n\n(cid:2)\ns,r xa(s)xb(r)\u03b2ab(s,r)\ns xa(s)\u03b2a(s)\ns xxa(s)\u03b2xa(s)\n\n+\nx\u03b2x +\n+\n\na = i, b = j\nx\u03b2x\n\u03b2a(i)\n\nx\u03b2x + z\u03b2z\n\u03b2a(i) + \u03b2b(j)\nx\u03b2x + \u03b2a(i)\n\nxz\u03b2xz\n\u03b2ab(ij)\nx\u03b2xa(i)\n\nx\u03b2x + z\u03b2z + xz\u03b2xz\n\n\u03b2a(i) + \u03b2b(j) + \u03b2ab(ij)\n\nx\u03b2x + \u03b2a(i) + x\u03b2xa(i)\n\n "}, {"Page_number": 116, "text": "104\n\nchapter 4. modeling of binary data\n\ntable 4.8 shows model terms in the notation of wilkinson-rogers together with the corre-\nsponding linear predictors for continuous covariates x, z and factors a, b. it should be noted\nthat the interactions built by these model terms are a specific form of parametric interaction. al-\nternative concepts of interactions may be derived (see also section 10.3.3 for smooth interaction\nterms).\n\n4.4.2 testing components of the linear predictor\nmost interesting testing problems concerning the linear predictor are linear hypotheses of the\nform\n\nh0 : c\u03b2 = \u03be against h1 : c\u03b2 (cid:8)= \u03be,\n\nwhere c is a fixed matrix of full rank s \u2264 p and \u03be is a fixed vector. in the simplest case one\ntests if one parameter can be omitted by considering\n\nh0 : \u03b2j = 0 against h1 : \u03b2j (cid:8)= 0,\n\nwhich has the form of a linear hypothesis with c = (0, . . . , 1, . . . , 0) and \u03be = 0. the general\nno-effects hypothesis\n\nh0 : \u03b2 = 0 against h1 : \u03b2 (cid:8)= 0\n\ncorresponds to a linear hypothesis with c denoting the unit matrix and \u03be = 0. if one wants to\ntest if a factor a has no effect, one has to test simultaneously if all the corresponding parameters\nare zero:\n\nh0 : \u03b2a(1) = \u00b7\u00b7\u00b7 = \u03b2a(k\u22121) = 0 against h1 : \u03b2a(j) (cid:8)= 0 for one j.\n\nit is easily seen that in this case one also tests linear hypotheses. general test statistics for linear\nhypotheses have already been given in section 3.7.2. therefore, in the following the tests are\nconsidered only briefly.\n\nlikelihood ratio statistic and the analysis of deviance\nlet m denote the model with linear predictor \u03b7 = xt \u03b2 and \u02dcm denote the submodel that is\nconstrained by c\u03b2 = \u03be. by using the notation of section 4.2, the likelihood ratio statistic that\ncompares models \u02dcm and m has the form\n\n\u03bb = \u22122{l(y, \u02dc\u03c0) \u2212 l(y, \u02c6\u03c0)},\n\n\u02dc\u03b2), denotes the fit of submodel \u02dcm and \u02c6\u03c0t = (\u02c6\u03c01, . . . , \u02c6\u03c0n),\nwhere \u02dc\u03c0t = (\u02dc\u03c01, . . . , \u02dc\u03c0n), \u02dc\u03c0i = h(xt\ni\n\u02c6\u03b2), denotes the fit of model m. since for a binary distribution the deviances of\n\u02c6\u03c0i = h(xt\nmodels \u02dcm and m are given by \u22122l(y, \u02dc\u03c0) and \u22122l(y, \u02c6\u03c0), respectively, \u03bb is equivalent to the\ni\ndifference between deviances:\n\n\u03bb = d(y, \u02dc\u03c0) \u2212 d(y, \u02c6\u03c0) = d( \u02dcm) \u2212 d(m)\n\nand has under mild conditions an asymptotic \u03c72-distribution with s = rg(c) degrees of free-\ndom. it is noteworthy that the difference of deviances yields the same value if computed from\nbinomial responses or from the single binary variables that build the binomial response.\nas shown in section 3.7.2, a sequence of nested models m1 \u2282 m2 \u2282 \u00b7\u00b7\u00b7 \u2282 mm can\nbe tested by considering the difference between successive models d(mi|mi+1) = d(mi) \u2212\nd(mi+1) within the decomposition\n\nd(m1) = d(m1|m2) + \u00b7\u00b7\u00b7 + d(mm\u22121|mm) + d(mm).\n\n "}, {"Page_number": 117, "text": "4.4. structuring the linear predictor\n\n105\n\ntable 4.9: empirical odds and log-odds for duration of unemployment.\n\ngender\n\neducation level\n\nshort term long term odds\n\nlog-odds\n\nm\n\nw\n\n1\n2\n3\n4\n\n1\n2\n3\n4\n\n97\n216\n56\n34\n\n105\n91\n31\n11\n\n45\n81\n32\n9\n\n51\n81\n34\n9\n\n2.155\n2.667\n1.750\n3.778\n\n2.059\n1.123\n0.912\n1.222\n\n0.768\n0.989\n0.560\n1.330\n\n0.722\n0.116\n-0.092\n0.201\n\ntable 4.10: hierarchies for level (l) and gender (g).\n\nmodel\n\ndeviance\n\ndf (p-value)\n\ncond. deviance\n\ndf (p-value)\n\ntested effect\n\n1\n\n32.886\n\n7 (0.000)\n\n1+l\n\n26.329\n\n4 (0.000)\n\n1+l+g\nl\u2217g\n1\n\n7.521\n\n3 (0.057)\n\n0\n\n0\n\n32.886\n\n7 (0.000)\n\n1+g\n\n14.928\n\n6 (0.021)\n\n1+l+g\nl\u2217g\n\n7.521\n\n3 (0.057)\n\n0\n\n0\n\n6.557\n\n3 (0.087)\n\nl(g ignored,\nl.g ignored)\n\n18.808\n\n1 (0.000)\n\ng(l.g ignored,\nl taken into account)\n\n7.521\n\n3 (0.057)\n\nl.g\n\n17.959\n\n1 (0.000)\n\ng(l ignored,\nl.g ignored)\n\n7.406\n\n3 (0.060)\n\nl(l.g ignored,\ng taken into account)\n\n7.521\n\n3 (0.057)\n\nl.g\n\nthe result is usually presented in an analysis of deviance table that gives the deviances of\nmodels, their differences, and the corresponding degrees of freedom (see example 4.8).\n\nexample 4.8: duration of unemployment\nwith the response duration of unemployment (1: short-term unemployment, less than 6 months; 0: long-\nterm unemployment) and the covariates gender (1: male; 0: female) and level of education (1: lowest, up\nto 4: highest, university degree), one obtains the data given in table 4.9. analysis is based on the grouped\ndata structure given in this table. the saturated logit model is given by\n\nlogit(\u03c0(g, l)) = \u03b20 + xg\u03b2g + xl\u03b2l + xgxl\u03b2gl,\n\nwhich can be abbreviated by g \u2217 l. the testing of \u03b2gl, \u03b2g, \u03b2l yields the analysis of deviance table\ngiven in table 4.10. it contains the deviances and the differences of the two sequences of nested models\n1 \u2282 1 + l \u2282 1 + l + g \u2282 g\u2217 l and 1 \u2282 1 + g \u2282 1 + l + g \u2282 g\u2217 l. in both sequences the intercept\nmodel is the strongest and the saturated model is the weakest. thus different paths between these models\ncan be tested. starting from the saturated model, the first transition seems possible, since d = 7.52 on 3\ndf, which corresponds to a p-value of 0.057. however, in the first sequence the next transition to model\n1 + l yields the difference of deviances 18.81 on 1 df; therefore gender cannot be omitted. in the other\nsequence one considers the transition to model 1 + g, obtaining the difference of deviances 7.41 on 3 df,\nwhich corresponds to a a p-value of 0.06. although simplification seems possible, the model 1 + g does\n\n "}, {"Page_number": 118, "text": "106\n\nchapter 4. modeling of binary data\n\nfigure 4.10: analysis of deviance for unemployment data with factors level (l) and\ngender (g).\n\nnot fit will since the deviance 14.93 on 3 df is rather large, corresponding to a p-value of 0.02. therefore,\nsimplification beyond the main effect model does not seem to be warranted.\n\nalternative test statistics\nalternatives to the likelihood ratio statistic are the wald test and the score test for linear hy-\npotheses given by\n\nw = (c \u02c6\u03b2 \u2212 \u03be)t [cfff\n\n\u22121(\u02c6\u03b2)ccc t ]\n\n\u22121(c \u02c6\u03b2 \u2212 \u03be),\n\nand\n\nu = st (\u02dc\u03b2)fff\n\n\u22121(\u02dc\u03b2)s(\u02dc\u03b2).\n\nasymptotically, all three test statistics, the likelihood ratio statistic, the wald test, and the score\ntest, have the same distribution \u03bb, w, u\n\n(a)\u223c \u03c72(rank ccc).\n\ncontrasts and quasi-variances\nlet us again consider the influence of a factor a that takes values in {1, . . . , k}. the corre-\nsponding linear predictor without constraints has the form\n\n\u03b7 = \u03b20 + xa(1)\u03b2a(1) + . . . + xa(k)\u03b2a(k).\n\nsince the parameters \u03b2a(1), . . . , \u03b2a(k) are not identifiable, side constraints have to be used. for\nexample, by choosing the reference category k one sets \u03b2a(k) = 0. among the full set of param-\neters \u03b2a(1), . . . , \u03b2a(k) only contrasts, that is, linear combinations ct \u03b2 with ct = (c1, . . . , ck),\n\n "}, {"Page_number": 119, "text": "4.4. structuring the linear predictor\n\n107\n\ntable 4.11: estimates, standard errors, quasi-standard-errors and quasi-variances for\nduration of unemployment data.\n\nlevel\n\nestimate\n\nstandard quasi-standard quasi-variance\n\n\u02c6\u03b2a(i)\n\u22120.170\n\u22120.273\n\u22120.637\n\n0\n\nerror\n\n0.305\n0.295\n0.323\n\n0\n\n1\n2\n3\n4\n\nerror\n\n0.124\n0.097\n0.163\n0.278\n\n0.015\n0.009\n0.026\n0.077\n\n(cid:14)\n\ni ci = 0, \u03b2t = (\u03b2a(1), . . . , \u03b2a(k)), are identified. a simple contrast is \u03b2a(i) \u2212 \u03b2a(j), which\ncompares levels i and j. for illustration we consider again the effect of the education level on\nthe binary response duration of unemployment (example 2.6). table 4.11 shows the estimates\ntogether with standard errors for reference category 4. the given standard errors refer to the\ncontrasts \u02c6\u03b2a(i) \u2212 \u02c6\u03b2a(4) because \u02c6\u03b2a(4) = 0. the disadvantage of the presentation of standard\nerrors for fixed reference category is that all other contrasts are not seen. an alternative pre-\nsentation of standard errors uses so-called quasi-variances, which have been proposed by firth\nand de menezes (2004). quasi-variances q1, . . . , qk are constructed such that the variance of\na contrast, var(ct \u02c6\u03b2), is approximately\ni qi. the corresponding \"quasi-standard-errors\"\nare given by q1/2\n. quasi-variances (or quasi-standard-errors) can be used to deter-\nmine the variance of contrasts. consider the quasi-variances given in table 4.11. for exam-\nple, the approximate standard error for \u02c6\u03b2a(1) \u2212 \u02c6\u03b2a(4) can be computed from quasi-variances\nby (0.015 + 0.077)1/2, or from quasi-standard-errors, based on pythagorean calculation, by\n(0.1242 + 0.2782)1/2, yielding 0.303, which is a rather good approximation of the standard er-\nror of \u02c6\u03b2a(1) given in table 4.11. but with the quasi-variances given in table 4.11 also standard\nerrors for \u02c6\u03b2a(1) \u2212 \u02c6\u03b2a(2) can be computed yielding (0.015 + 0.009)1/2 = 0.155, which is rather\nlarge when compared to \u02c6\u03b2a(1) \u2212 \u02c6\u03b2a(2) = 0.103. quasi-variances are a helpful tool in reporting\n\n, . . . , q1/2\n\nk\ni=1 c2\n\n(cid:14)\n\nk\n\n1\n\nfigure 4.11: quasi-standard-errors for duration of unemployment data.\n\n "}, {"Page_number": 120, "text": "108\n\nchapter 4. modeling of binary data\n\nstandard errors for the level of factors. they are useful for investigating contrasts but should\nnot be misinterpreted as standard errors for the parameters themselves. firth and de menezes\n(2004) give conditions under which the approximation is exact and investigate the accuracy in\na variety of settings.\n\n4.4.3 ordered categorical predictors\nthe inclusion of categorical covariates in the linear predictor may strongly increase the dimen-\nsion of the predictor space. since a factor a that takes values in {1, . . . , k} means that k \u2212 1\ndummy variables have to be included, the number of observations within one factor level may\nbecome very small. in particular, if one wants to model interactions of factors, empty cells (for\ncombinations of factors) will occur if the number of levels is large. the consequence frequently\nis that estimates of effects do not exist. in that case practioners typically join some factor cate-\ngories to reduce the dimension of the predictor space, but at the cost of loosing information.\nthe coding in dummy variables for factors is based on the nominal scale of the factor.\nthe categories {1, . . . , k} represent simple labels for levels of the factor; the ordering of the\ncategories is arbitrary and should not be used. in practice, however, the categories of a factor are\noften ordered, and the variable that constitutes the predictor is an ordered categorical variable.\nthen the ordering should be used to obtain a more sparse representation and therefore more\nstable estimates. one way to use the ordering is to define assigned scores to the categories of the\nfactor. although that \"solution\" to the dimensional problem is widespread, it is not satisfactory.\nby assigning scores and using a linear or quadratic model for the scores one definitely assumes\na higher scale level for the factor. however, linear or quadratic terms are appropriate only when\nthe influential variable is metrically scaled, albeit discrete. in particular, differences of values of\nthe variable have to be meaningful. for ordered factors this is usually not the case. if the factor\nlevels represent subjective judgements like \"strong agreement,\" \"slight agreement,\" and \"strong\ndisagreement,\" the assigned scores are typically useless. interpretation depends on the assigned\nscores, which are to a certain extent arbitrary, and different sets of scores yield different effect\nstrengths.\n\nit is slightly different when the ordinal scale of the factor is due to an underlying continuous\nvariable. for example, often variables like income brackets and categorized age are available\nand it is known how the categories have been built. for a categorized covariate age, available\nin categories [20, 29), [30, 39), . . . , [60, 80), one may build mid-range scores and use them as\na substitute for the true underlying age. then one approximates the unobserved covariate age\nin the predictor. what works for covariate age may work less well when the predictor is cate-\ngorized income. since only categories are available, it is hard to know what values hide in the\nhighest interval, because it has been learned that incomes can be extremely high. then the mid-\nrange score of the last category is a mere guess. in addition, the score of the highest category\nis at the boundary of the predictor space and therefore tends to be influential. thus one has an\ninfluential observation but has to choose a score.\n\nan alternative to assigning scores that takes the ordering seriously but does not assume\nknowledge of the true score is to use penalized estimates where the penalty explicitly uses the\nordering of categories. let the predictor of a single factor a with ordered categories 1, . . . , k\nbe given in the form\n\n\u03b7 = \u03b20 + xa(2)\u03b22 + . . . + xa(k)\u03b2k,\n\nwhere xa(2), . . . , xa(k) denotes k \u2212 1 dummy variables in (0\u20131)-coding, therefore implicitly\nusing category 1 as the reference category. however, instead of maximizing the usual log-\nlikelihood l(\u03b2), estimates of the parameter vector \u03b2t = (\u03b20, \u03b22, . . . , \u03b2k) are obtained by\n\n "}, {"Page_number": 121, "text": "4.4. structuring the linear predictor\n\n109\n\nmaximizing the penalized log-likelihood:\n\nlp(\u03b2) = l(\u03b2) \u2212 \u03bb\n\n2 p (\u03b2),\n\nwhere the penalty term is given by\n\np (\u03b2) =\n\nk(cid:7)\n\nj=2\n\n(\u03b2j \u2212 \u03b2j\u22121)2\n\nwith \u03b21 = 0. therefore, the differences of adjacent parameters are penalized with the strength\nof the penalty determined by \u03bb. for \u03bb = 0, maximization of lp(\u03b2) yields the usual maximum\nlikelihood estimate whereas \u03bb \u2192 \u221e yields \u02c6\u03b2j = 0, j = 1, . . . , k. for an appropriately chosen\n\u03bb, the penalty restricts the variability of the parameters across the response categories, thereby\nassuming a kind of \"smooth effect\" of the ordinal categories on the dependent variable. the\nmethod is strongly related to the penalization techniques considered in chapter 6.\nand d is the ((k \u2212 1) \u00d7 k)-matrix:\n\u239b\n\nin matrix form the penalty is given by p (\u03b2) = \u03b2t dt d\u03b2 = \u03b2t k\u03b2, where k = dt d\n\n\u239e\n\n\u239c\u239c\u239c\u239c\u239c\u239c\u239d\n\n0\n1\n0 \u22121\n0\n...\n0\n\n0\n\n0\n\n. . .\n\n1\n\u22121\n...\n. . .\n\n1\n\n0\n\nd =\n\n0\n...\n\n\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 .\n\n...\n\n\u22121 1\n\n(4.9)\n\nthe corresponding penalized score function is\n\nn(cid:7)\n\ni=1\n\nsp(\u03b2) =\n\nxi\n\n\u2202h(\u03b7i)\n\n\u2202\u03b7\n\n(yi \u2212 \u03bci)/\u03c32\n\ni\n\n\u2212 \u03bbk\u03b2.\n\nthe estimation equation sp(\u02c6\u03b2p) = 0 may be solved by iterative pseudo-fisher scoring:\n\n\u02c6\u03b2\n\n(k+1)\np\n\n= \u02c6\u03b2\n\n(k)\n\np + f p(\u02c6\u03b2\n\n(k)\n\np )\n\n\u22121sp(\u02c6\u03b2\n\n\u22121,\n\n(k)\n\np )\n\nwhere f p(\u03b2) = f (\u03b2) \u2212 \u03bbk and f (\u03b2) = e(\u2212\u22022l/\u2202\u03b2\u2202\u03b2t ). approximate covariances are\nobtained by the sandwich matrix\n\ncov(\u02c6\u03b2p) \u2248 (f (\u02c6\u03b2p) + \u03bbk)\n\n\u22121f (\u02c6\u03b2p)(f (\u02c6\u03b2p) + \u03bbk)\n\n\u22121.\n\nan advantage of the penalized estimate \u02c6\u03b2p is that it will exist even in cases where the\nunpenalized ml estimate \u02c6\u03b2 does not exist. unconstrained estimates may not exist even in\nthe simple case of a single factor. for the logit model, unconstrained estimates are given by\n\u02c6\u03b20 = log(p1/(1 \u2212 p1)), \u02c6\u03b2j = log(pj/(1 \u2212 pj)) \u2212 \u02c6\u03b20, j = 2, . . . , p, with pj denoting the\nobserved relative frequencies in category j. therefore, estimates do not exist if one of the\nrelative frequencies is zero or one. in the more general case, where the intercept term is replaced\nby a linear term xt \u03b2x, which contains an additional vector of predictors, non-existence will\noccur more frequently.\n\n "}, {"Page_number": 122, "text": "110\n\nchapter 4. modeling of binary data\n\nalternative representation\nbefore demonstrating the advantages of penalized estimates, an alternative representation of\nthe approach by use of split-coding is given. let us separate the weights on predictors from\nthe intercept by considering the decomposition \u03b2t = (\u03b20, \u03b2t\nc ). in analogy the matrix d is\ndecomposed into d = [0|dc]. then the penalty may be given as\n\nj(\u03b2) = \u03b2t\n\nc kc\u03b2c,\n\n(4.10)\n\nc dc and \u03b2t\n\nc = (\u03b22, . . . , \u03b2k). the penalized maximization problem can be\nwhere kc = dt\n, which is a lower\ntransformed into a problem with a simpler penalty by use of the inverse d\ntriangular matrix with 1 on and below the diagonal. by using the parametrization \u02dc\u03b2 = dc\u03b2c\nwith \u02dc\u03b2\n\nt = ( \u02dc\u03b21, . . . , \u02dc\u03b2k\u22121) the penalty \u03b2t\n\nc kc\u03b2c turns into the simpler form\n\n\u22121\nc\n\nk\u22121(cid:7)\n\nj(\u03b2) = \u02dc\u03b2\n\nt \u02dc\u03b2 =\n\n\u02dc\u03b22\ni ,\n\ni=1\n\nwhich is a ridge penalty on the transformed parameters. the corresponding transformation\nof the design matrix is based on the decomposition x = [1|x c]. one obtains x\u03b2 =\n[1\u03b20|x cd\nis given by the vector (\u02dcxa(1), . . . ,\n\u02dcxa(k\u22121)) = (xa(2), . . . , xa(k))d\n\n, where the dummies \u02dcxa(i) are given in split-coding:\n\n\u02dc\u03b2]. then one row of the design matrix x cd\n\n\u22121\nc\n\n\u22121\nc\n\n\u22121\nc\n\n(cid:2)\n\n\u02dcxa(i) =\n\n1\n0\n\nif a > i\notherwise,\n\n(cid:14)\n\nwhich is a coding scheme that distinguishes between categories {1, . . . , i} and {i + 1, . . . , k}\n(for split-coding compare section 1.4.1). the transformation \u02dc\u03b2 = dc\u03b2c yields the parameters\n\u02dc\u03b21 = \u03b22, \u02dc\u03b22 = \u03b23 \u2212 \u03b22, \u02dc\u03b2k\u22121 = \u03b2k \u2212 \u03b2k\u22121, which are used in the corresponding predictor\n\u02dc\u03b2i. the transformation shows that the smoothness penalty \u03b2t k\u03b2\n\u03b7 = \u03b20 +\ncan be represented as a ridge penalty for the parameters corresponding to split-coding, and\nthus smoothness across categories is transformed into penalizing transitions between groups of\nadjacent categories.\n\nk\u22121\ni=1 \u02dcxa(i)\n\nexample 4.9: simulation\nin a small simulation study the penalized regression approach is compared to pure dummy coding and a\nbinary regression model with a linear predictor that takes the group labels as (metric) independent variable.\nthe underlying model is the logit model p (y = 1) = exp(xt \u03b2)/(1 + exp(xt \u03b2)). as true values of \u03b2\nwe assume an approximately linear structure (figure 4.12, top left) and an obviously non-linear coefficient\nvector (figure 4.12, bottom left). in each of the 100 simulations n = 330 values of x were generated.\nfigure 4.12 shows the mean squared error for the estimation of \u03b2. it is seen that the linear predictor\nperforms well when the underlying structure is approximately linear but fails totally when the structure\nis non-linear. simple dummy coding should adapt to both scenarios, but due to estimation uncertainty it\nperforms worse than the linear predictor approach in the first scenario but better in the second scenario.\nthe penalized estimate with smoothed dummies outperforms both approaches distinctly. it adapts well to\nthe approximately linear and to the distinctly non-linear structure.\n\nso far we assumed a single independent variable, but models with several predictors are an\nobvious extension. only the penalty matrix has to be modified. now one uses a block-diagonal\nstructure with the blocks given by the penalty matrix for a single ordered predictor.\nin the\nfollowing example three ordinal predictors are used.\n\n "}, {"Page_number": 123, "text": "4.4. structuring the linear predictor\n\n111\n\nfigure 4.12: true coefficient vectors (left) and squared errors (right) for the considered\nmethods after 100 simulation runs with \u03c32 = 2.\n\nexample 4.10: choice of coffee brand\nin this example the binary response is coffee brand, which is only separated into cheap coffee from a\ngerman discounter and real branded products. the ordinal predictors are monthly income (in four cate-\ngories), social class (in five categories), and age group in five categories, below 25, 25\u201339, 40\u201349, 50\u201359,\nabove 59). while one might consider some sort of midpoints for age group and monthly income, it is\nhardly imaginable for social class. table 4.12 shows the estimated coefficients of corresponding dummy\nvariables for a logit model with group labels as predictors. the comparison refers to linear modeling, pure\ndummy and smoothed ordered dummies. it can be seen that penalization yields much less variation in the\ncoefficients for single variables when compared to pure dummy coding.\n\nto investigate the methods\u2019 performance in terms of prediction accuracy, the data were split randomly\ninto training (n = 100) and test (m = 100) data. the training data are used to fit the model, the test set\nfor evaluation only. as a measure of prediction accuracy we take the sum of squared deviance residuals\n(ssdr) on the test set. it is remarkable that the pure dummy model cannot be fitted due to complete data\nseparation in 68% of the splits. figure 4.13 summarizes the results in terms of ssdr after 200 random\nsplits. it is seen that smooth dummies are to be preferred, in particular because the fit of pure dummies\nexists only in 32% of the splits.\n\nas the previous example shows, ml estimates often do not exist when simple dummy\ncoding is used. penalized estimates have the advantage that estimates exist under weaker\n\n "}, {"Page_number": 124, "text": "112\n\nchapter 4. modeling of binary data\n\ntable 4.12: coefficients of corresponding dummy variables, estimated by the use of a\n(generalized) linear model, i.e., logit model, with group labels as predictors, penalized\nregression types i and ii (\u03bb = 10 in each case), and a logit model based on pure dummy\ncoding.\n\nintercept\n\nincome\n\nsocial class\n\nage group\n\nlinear model\n\u22120.36\n0.02\n0.03\n0.05\n\u22120.28\n\u22120.56\n\u22120.84\n\u22121.12\n\u22120.10\n\u22120.20\n\u22120.30\n\u22120.40\n\nsmooth dummies\n\u22120.81\n\u22120.05\n\u22120.02\n\u22120.04\n\u22120.14\n\u22120.31\n\u22120.39\n\u22120.56\n0.06\n\u22120.09\n0.05\n\u22120.18\n\n2\n3\n4\n\n2\n3\n4\n5\n\n2\n3\n4\n5\n\npure dummy\n\u22120.38\n\u22120.13\n0.29\n0.17\n\u22120.92\n\u22121.39\n\u22121.28\n\u22121.96\n0.79\n\u22120.29\n0.84\n\u22120.04\n\nfigure 4.13: performance (in terms of ssdr) of a (generalized) linear regression on\nthe group labels, penalized regression for smooth dummy coefficients, and a pure dummy\nmodel (for the latter only the 68% successful estimates have been used) (left). observed\nvalues for all considered methods; (right) ssdr values relative to linear model.\n\nassumptions. it may be seen as an disadvantage that an additional tuning parameter has to be\nselected. however, this is easily done for example by cross-validation or minimization of infor-\nmation criteria like the aic. for details and more simulations and applications, see gertheiss\nand tutz (2009b).\n\nthe ridge-type penalty ensures the existence of estimates and yields a smooth estimation of\neffects for ordered categorical predictors. an alternative strategy is to search for the categories\nthat actually have different effects, or, in other words, to identify the categories that can be\ncollapsed with respect to the dependent variable. in the case of ordered categorical predictors,\ncollapsing naturally refers to adjacent categories. this selection and collapsing approach for\nordered predictors is considered within selection procedures in section 6.5.\n\n "}, {"Page_number": 125, "text": "4.5. comparing non-nested models\n\n113\n\n4.5 comparing non-nested models\nthe analysis of deviance is a useful tool that allows one to distinguish between relevant and\nirrelevant terms in the linear predictor. the underlying strategy is based on a comparison of\nthe nested models. one typically compares the model that contains the term in question to the\nmodel in which the term is omitted.\n\nanalysis of deviance cannot be used if the models to be compared are non-nested, because\ndifferences of deviances have no standard distribution. if two models are not nested, one should\ndistinguish between two cases. when the two models have the same number of parameters one\ncan compute goodness-of-fit statistics, if available, and at least find out which model shows\na better fit to the data. although there is no benchmark for the comparison in the form of a\nstandard distribution, it shows how strong the goodness-of-fit varies across models. however,\nif in addition the models to be compared have different numbers of parameters, goodness-of-fit\nwill tend to favor the model that contains more parameters because the additional flexibility of\nthe model will yield a better fit. various criteria that take the number of parameters into account\nhave been proposed. a widely used criterion is akaike\u2019s information criterion which he named\naic, for \"an information criterion\" (akaike, 1973). it is defined by\n\naic = \u22122 log(l(\u02c6\u03b2)) + 2 \u00b7 (number of fitted parameters),\n\nwhere log(l(\u02c6\u03b2)) denotes the log-likelihood of the fitted model evaluated at the ml estimate \u02c6\u03b2.\nthe second term may be seen as a penalty accounting for the number of fitted parameters. for\nbinary response models with ungrouped data the deviance is given by d(y, \u02c6\u03c0) = \u22122 log(l(\u02c6\u03b2))\nand one obtains\n\naic = d(y, \u02c6\u03c0) + 2 \u00b7 (number of fitted parameters).\n\ntherefore, in the aic criterion the number of fitted parameters is added to the deviance as a\nmeasure for the discrepancy between the data and the fit. the correction to the deviance may\nbe derived by asymptotic reasoning. aic has been shown to be an approximately unbiased\nestimate of the mean log-density of a new independent data set (see also appendix d). a\ncareful investigation of aic and alternative model choice criteria was given by burnham and\nanderson (2002). while aic is an information-theoretic measure based on kullback-leibler\ndistance, the bic (for bayesian information criterion) has been derived in a bayesian context\nby schwarz (1978). it has the form\n\nbic = \u22122 log(l(\u02c6\u03b2)) + log(n) \u00b7 (number of fitted parameters).\n\naic and bic are not directly comparable since the underlying targets differ. based on its\nderivation as an unbiased estimate of the mean density of a new dataset with equal sample\nsize, aic is specific for the sample size at hand.\nin contrast, derivation of bic assumes\na true generating model, independent of sample size, although selection of the true model is\nobtained only asymptotically. for a comparison of these selection criteria including multimodel\ninference, see burnham and anderson (2004).\n\nakaike\u2019s criterion\n\naic = \u22122 log(l) + 2 \u00b7 (number of fitted parameters)\n\nbayesian information criterion\n\nbic = \u22122 log(l) + log(n) \u00b7 (number of fitted parameters)\n\n "}, {"Page_number": 126, "text": "114\n\nchapter 4. modeling of binary data\n\nalthough a comparison of non-nested models by testing is not straightforward, some meth-\nods were proposed in the econometric literature; see, for example, vuong (1989) , who proposed\nlikelihood ratio tests for model selection with tests on non-nested hypotheses. an alternative\nstrategy is to compare the models in terms of prediction accuracy. when the sample is split\nseveral times into a learning dataset (for fitting of the model) and a test dataset (for evaluating\nprediction performance) one chooses the model that has the better performance (see chapter\n15).\n\n4.6 explanatory value of covariates\nwhen using a regression model one is usually interested in describing the strength of the rela-\ntion between the dependent variable and the covariates. in classical linear regression the most\nwidely used measure is the squared multiple correlation coefficient r2, also called the coef-\nficient of determination. the strength of r2 is its simple interpretation as the proportion of\nvariation explained by the regression model. as a descriptive measure it may be derived from\nthe partioning of squared residuals:\n\nn(cid:7)\n(yi \u2212 \u00afy)2 =\n\nn(cid:7)\n\nn(cid:7)\n\n(\u02c6\u03bci \u2212 \u00afy)2 +\n\n(\u02c6\u03bci \u2212 yi)2,\n\n(4.11)\n\ni=1\n\ni=1\n\ni=1\n\n\u02c6\u03b2 denotes the fitted\nwhere \u00afy denotes the mean across observations y1, . . . , yn and \u02c6\u03bci = xt\ni\nvalues when \u02c6\u03b2 is estimated by least squares and xi contains an intercept. the term on the left-\nhand side is the total sum of squares (sst), the first term on the right-hand side corresponds\nto the sum of squares explained by regression (ssr), and the second term is the error sum of\nsquares (sse). the empirical coefficient of determination r2 is defined by\n\n(cid:14)\n(cid:14)\ni(\u02c6\u03bci \u2212 \u00afy)2\ni(yi \u2212 \u00afy)2 .\n\nr2 =\n\nssr\nsst\n\n=\n\nwith sst being the total variation without covariates, r2 gives the proportion of variation\nexplained by the covariates. if \u02c6\u03bci is obtained by least-squares fitting, r2 is equivalent to the\nsquared correlation between the observations yi and fitted values \u02c6\u03bci (for more on r2 see also\nsection 1.4.5).\nthe partitioning of squared residuals (4.11) may be seen as an empirical version of the\ndecomposition of variance into \u201dbetween variance\" var(e(y|x)) (explained by regression on\nx) and \u201dwithin variance\" e(var(y|x)) (or error):\n\nvar(y) = var(e(y|x)) + e(var(y|x)),\n\nwhich holds without distributional assumptions for random variables y, x. thus r2 is an em-\npirical version of the true or theoretical proportion of explained variance:\n\nt = var(e(y|x))/ var(y) = {var(y) \u2212 e(var(y|x))}/ var(y),\nr2\n\nwhich represents the population coefficient of determination. for the population measure r2\nt\none has a similar property as for the empirical coefficient of determination. it represents the\nproportion of explained variance but also equals the squared correlation between the random\nvariables y and e(y|x):\n\nt = var(e(y|x))/ var(y) = cor(y, e(y|x))2.\nr2\n\n(4.12)\n\n "}, {"Page_number": 127, "text": "4.6. explanatory value of covariates\n\n115\n\ntherefore, in the linear model the empirical coefficient of determination r2, obtained from\nleast-squares fitting, as well as the empirical correlation coefficient are estimates of r2\nt .\n\nthe extension to glms has some pitfalls. although the equivalence (4.12) holds more\ngenerally for glms, the empirical correlation coefficient between the data and values fitted by\nml estimation is not the same as r2 = ssr / sst. moreover, in glms it is more difficult\nto interpret r2\nt as a proportion of the explained variation. for example, when responses are\nbinary, the mean and variance of the response are strictly linked. therefore, var(y|x) is not\nfixed but varies with x. this is different from the case of a normally distributed vector (y, x)\nfor which var(y|x) is a constant value and therefore the variability of e(y|x) and var(y|x),\nwhich determine the decomposition of var(y), are strictly separated. in addition, the population\nmeasure r2\nt can be severely restricted to small values, although a strong relation between the\npredictor and the response is present. cox and wermuth (1992) considered a linear regression\nmodel for binary responses with a single explanatory variable, p (y = 1|x) = \u03c0 + \u03b2(x \u2212 \u03bcx),\nx/(\u03c0(1\u2212\u03c0))\nwhere x has mean \u03bcx and variance \u03c32\nt =\nis primarily determined by the variance of x, and for a sensible choice of the variance r2\n0.36 is the largest value that can be achieved. therefore, explained variation as a proportion of\nthe explained variance is restricted to rather small values.\n\nx, and \u03c0 = p (y = 1). the value of r2\n\nt = \u03b22\u03c32\n\nthe different interpretations of the coefficient of determination in the linear model have led\nto various measures for non-linear models, most of them more descriptive in nature. although\nthere is no widely accepted direct analog to r2 from least-squares regression, a number of r2\nfor binary response models are in common use. we give some of them in the following.\n\n4.6.1 measures of residual variation\nthe coefficient of determination is a member of a more general class of measures that aim at\nthe proportional decrease in variation obtained in going from a simple model to a more complex\nmodel that includes explanatory variables. the measures have the general form\n\nr(m0|m) = d(m0) \u2212 d(m)\n\nd(m0)\n\n,\n\n(4.13)\n\nwhere d(m) measures the (residual) variation of model m and d(m0) the variation of the\nsimpler model m0, which typically is the intercept model (compare efron, 1978). the propor-\ntional reduction in variation measure (4.13) can be seen as a descriptive statistic or a popula-\ntion measure depending on the measure of variation that is used. for example, the empirical\ncoefficient of determination r2 is obtained by using d(m0) = sst , d(m) = sse; the\npopulation value is based on the corresponding distribution models and uses d(m0) = var(y),\nd(m) = e(var(y|x)). however, in most applications measures of the form (4.13) are used\nas descriptive tools without reference to an underlying population value. the form (4.13) has\nmore generally been referred to as a measure of proportional reduction in loss (or error) that is\nused to quantify reliability (cooil and rust, 1994).\n\nleast squares as measures of variation are linked to the estimation in linear models with\nnormally distributed responses. for generalized linear models the preferred estimation method\nis maximum likelihood. the corresponding variation measure is the deviance, which explicitly\nuses the underlying distribution. let l(\u02c6\u03b2) denote the maximal log-likelihood of the fitted model,\nl( \u02c6\u03b20) denote the log-likelihood of the intercept model, and l(sat) be the log-likelihood of the\nsaturated model. with d(m) = d(\u02c6\u03b2) = \u22122(l(\u02c6\u03b2) \u2212 l(sat)), d(m0) = d( \u02c6\u03b20) = \u22122(l( \u02c6\u03b20) \u2212\nl(sat)) one obtains the deviance-based measure\n\ndev = d( \u02c6\u03b20) \u2212 d(\u02c6\u03b2)\n\nr2\n\nd( \u02c6\u03b20)\n\n= l(\u02c6\u03b2) \u2212 l( \u02c6\u03b20)\nl(sat) \u2212 l( \u02c6\u03b20)\n\n.\n\n "}, {"Page_number": 128, "text": "116\n\nchapter 4. modeling of binary data\n\nr2\nsimple intercept model to the deviance of the intercept model.\n\ndev compares the reduction of the deviance when the regression model is fitted instead of the\nfor ungrouped binary observations one has d(\u02c6\u03b2) = \u22122l(\u02c6\u03b2) and the coefficient is equiva-\n\nlent to mcfadden\u2019s (1974) likelihood ratio index (also called pseudo-r2):\n\ndev = l( \u02c6\u03b20) \u2212 l(\u02c6\u03b2)\n\nr2\n\n.\n\nl( \u02c6\u03b20)\n\nit is seen that 0 \u2264 r2\ndev = 0 if\nr2\ndev = 1 if d(\u02c6\u03b2) = 0, that is, if the model shows perfect fit, \u02c6\u03c0i = yi.\nr2\n\n\u2264 1 with\nl( \u02c6\u03b20) = l(\u02c6\u03b2), that is, if all other parameters have zero estimates:\n\ndev\n\ndev is directly linked to the likelihood ratio statistic \u03bb by \u03bb = r2\n\nr2\nasymptotic \u03c72-distribution.\n\nm f (\u22122l( \u02c6\u03b20)), which has\n\nit should be noted that the deviance-based measure depends on the level of aggregation\nbecause the deviance for grouped observations differs from the deviance for individual obser-\nvations. deviances based on ungrouped data are to be preferred because if one fits a model\nthat contains many predictors, the fit can be perfect for grouped data yielding d(\u02c6\u03b2) = 0, al-\nthough individual observations are not well explained. as an explanatory measure for future\ndata, which will will come as individual data it is certainly insufficient. for the use of deviances\nfor grouped observations see also theil (1970) and goodman (1971).\n\nalthough the deviance has some appeal when considering glms, alternative measures of\nvariation can be used. one candidate is squared error with d(m0) = sst , d(m) = ssr,\n(cid:14)\nyielding r2\nse (e.g., efron, 1978) . for binary observations, the use of the empirical variance\nd(m0) = n\u00afp(1 \u2212 \u00afp), where \u00afp is the proportion of ones in the total sample, and d(m) =\n(cid:14)\ni \u02c6\u03c0i(1 \u2212 \u02c6\u03c0i) yields gini\u2019s concentration measure:\n\u2212 n\u00afp2\ni \u02c6\u03c02\nn\u00afp(1 \u2212 \u00afp) ,\n\ng =\n\ni\n\nwhich was also discussed by haberman (1982) (exercise 4.7).\n\na prediction-oriented criterion that has been proposed uses the predictions based on \u00afp (with-\n\nout covariates) and \u02c6\u03c0i (with covariates). the variation measures d are defined by\n\n(cid:7)\n\ni\n\n(cid:7)\n\ni\n\nd(m) =\n\nl(yi, \u02c6\u03c0i), d(m0) =\n\nl(yi, \u00afp),\n\nwhere l is a modified (0\u20131) loss function with l(y, \u02c6\u03c0) = 1 if |y \u2212 \u02c6\u03c0| > 0.5, and l(y, \u02c6\u03c0) = 0.5\nif |y \u2212 \u02c6\u03c0| = 0.5, and l(y, \u02c6\u03c0) = 0 if |y \u2212 \u02c6\u03c0| < 0.5. the corresponding measure rclass compares\nthe number of misclassified observations (\"non-hits\") obtained without covariates to the number\nof misclassified observations obtained by using the model m, where the simple classification\nrule \u02c6y = 1 if \u02c6\u03c0i > 0.5 and \u02c6y = 0 if \u02c6\u03c0i < 0.5, with an adaptation for ties, is used. the measure\nuses the threshold 0.5, which corresponds to prior probabilities p (y = 1) = p (y = 0) in\nterms of classification (see chapter 15). the measure has a straightforward interpretation but\ndepends on the fixed threshold. it is also equivalent to goodman and kruskal\u2019s \u03bb (goodman\nand kruskal, 1954); see also van houwelingen and cessie (1990). a problem with measures\nlike the number of misclassified errors is that they are a biased measure of the underlying true\nerror rate in future samples. since the sample is used to estimate a classification rule and to\nevaluate its performance, it underestimates the true error. the resulting error is also called\na reclassification error. better measures are based on cross-classification or leaving-one-out\nversions (for details see section 15.3).\n\n "}, {"Page_number": 129, "text": "4.6. explanatory value of covariates\n\n117\n\nmeasures for explanatory value of covariates\n\npseudo-r2\n\nsquared error\n\ngini\n\n.\n\ndev = l( \u02c6\u03b20) \u2212 l(\u02c6\u03b2)\n\nr2\n\nn\n\nl( \u02c6\u03b20)\n(cid:14)\n(cid:14)\ni=1(yi \u2212 \u02c6\u03c0i)2\nse = 1 \u2212\ni=1(yi \u2212 \u02c6y)2\nr2\n(cid:14)\n\u2212 n\u00afp2\ni \u02c6\u03c02\nn\u00afp(1 \u2212 \u00afp) .\n\ng =\n\nn\n\ni\n\nreduction in classification error\n\nrclass =\n\ncox and snell\n\nnon-hits(m0) \u2212 non-hits(m)\n\n,\n\nnon-hits(m0)\n\n\"\n\n#\n\n2/n\n\nlr = 1 \u2212\nr2\n\nl( \u02c6\u03b20)\nl(\u02c6\u03b2)\n\n4.6.2 alternative likelihood-based measures\nsince the coefficient of determination has several interpretations, alternative generalizations are\npossible. cox and snell (1989) considered\n\n\"\n\n#\n\nlr = 1 \u2212\nr2\n\nl( \u02c6\u03b20)\nl(\u02c6\u03b2)\n\n2/n\n\n,\n\nwhere l( \u02c6\u03b20), l(\u02c6\u03b2) denote the likelihoods of the two models. when l( \u02c6\u03b20), l(\u02c6\u03b2) are the like-\nlr reduces to the standard r2 of a classical linear\nlihoods of a normal response model, r2\nregression. with a binary regression model r2\nlr cannot obtain a value of one even if the model\npredicts perfectly. therefore, a correction has been suggested by nagelkerke (1991). he pro-\nlr/(1 \u2212 l( \u02c6\u03b20))2/n, which is a simple rescaling by using the maximal\nposed using r2\nvalue (1 \u2212 l( \u02c6\u03b20))2/n that can be obtained by r2\n\ncorr = r2\n\nlr.\n\n4.6.3 correlation-based measures\nin most program packages, summary measures that are based on the correlation between ob-\nservations yi and fit \u02c6\u03c0i are given. one candidate is the (squared) correlation between yi and\n\u02c6\u03c0i. some motivation for the use of the squared correlation is that in the linear model and\nleast-squares fitting the squared correlation is equivalent to the ratio of the explained variation\nssr/sst , although the equivalence does not hold for glms. zheng and agresti (2000) pre-\nfer the correlation to its square because it has a familiar interpretation and works on the original\nscale of the observations.\n\n "}, {"Page_number": 130, "text": "118\n\nchapter 4. modeling of binary data\n\nwidely used association measures in logistic regression are rank-based measures. concor-\ndance measures compare pairs of tupels (yi, \u02c6\u03c0i) built from observation yi and the corresponding\nprediction \u02c6\u03c0i. let a pair be given by (yi, \u02c6\u03c0i), (yj, \u02c6\u03c0j), where yi < yj. the pair is called concor-\ndant if the same ordering holds for the predictions, \u02c6\u03c0i < \u02c6\u03c0j; it is discordant if \u02c6\u03c0i > \u02c6\u03c0j holds.\nthe pair is tied if \u02c6\u03c0i = \u02c6\u03c0j. if yi = yj holds, the pair is also tied. if a pair is concordant, discor-\ndant, or tied, it can be expressed by using the sign function: sign(d) = 1 if d > 0, sign(d) = 0\nif d = 0, and sign(d) = \u22121 if d < 0. then the product sign(yi \u2212 yj) sign(\u02c6\u03c0i \u2212 \u02c6\u03c0j) takes value\n1 for concordant pairs and value \u22121 for discordant pairs. several measures in common use are\ngiven in a separate box. the measures are also given in an alternative form since they can be\nexpressed by using nc for the number of concordant pairs, nd for the number of discordant\npairs, and n for the number of pairs with different observations yi (cid:8)= yj.\n\nrank-based measures\n\nkendall\u2019s \u03c4a\n\n\u03c4a =\n\n(cid:7)\n\nsign(yi \u2212 yj) sign(\u02c6\u03c0i \u2212 \u02c6\u03c0j)/(n(n \u2212 1)/2)\n\ni<j\n\n= (nc \u2212 nd)/(n(n \u2212 1)/2),\n(cid:7)\n\nsign(yi \u2212 yj) sign(\u02c6\u03c0i \u2212 \u02c6\u03c0j)/\n\nsomers\u2019 d\n\nds =\n\n(cid:7)\n\ni<j\n\nsign(yi \u2212 yj)2\n\ni<j\n\n= (nc \u2212 nd)/n,\ngoodman and kruskal\u2019s \u03b3\n\n(cid:7)\n\nsign(yi \u2212 yj) sign(\u02c6\u03c0i \u2212 \u02c6\u03c0j)/\n\n\u03b3 =\n= (nc \u2212 nd)/(nc + nd),\n\ni<j\n\n(cid:7)\n\ni<j\n\nsign(yi \u2212 yj)2 sign(\u02c6\u03c0i \u2212 \u02c6\u03c0j)2\n\na disadvantage of rank-based association measures is that in many applications they can\nif the predicted values remain monotonic,\nnot distinguish between different link functions.\nlink functions that yield quite different fits yield the same association value. the correlation\ncoefficient used by zheng and agresti (2000)\nis able to distinguish between link functions\nbecause it uses the exact estimate, although it is more sensitive to outliers.\n\nin general, association measures between observations and fit try to quantify how strong the\nlink between yi and \u02c6\u03c0i is. they may be used as descriptive statistics, but one should be aware of\nwhat they are measuring. while the correlation coefficient measures linear dependence, rank-\nbased procedures measure the association between ordered values and therefore monotonicity.\nthey reflect in particular goodness-of fit in the sample. one should be cautious with squared\nvalues of association measures, considered, for example, by mittlb\u00f6ck and schemper (1996).\nin particular, for squared rank-based measures it is unclear what they are measuring. more-\nover, one should not take them at face value because as descriptive statistics they are random\nvariables depending on the sample, a fact that is often ignored. a more sensible approach first\n\n "}, {"Page_number": 131, "text": "4.7. further reading\n\n119\n\ndefines a population measure and then finds ways to estimate it. for most useful measures\nthere is an underlying population measure. for example, the empirical correlation coefficient\nhas the theoretic analog cor(y, e(y|x)). also, kendall\u2019s \u03c4a and somers\u2019 d may be seen as\nestimates of an underlying measure (nicely described by newson, 2002 ). the advantage of an\nunderlying population value is that one can study the properties of the empirical measures as\nan estimator. that is a non-trivial task because one cannot expect (yi, \u02c6\u03c0i) to be iid observations\neven when (yi, xi) are iid observations and the empirical measure will not be the best estimator\nfor the population value. zheng and agresti (2000) used the population correlation coefficient\ncor(y, e(y|x)) and investigate bias and mse for several estimators and show that in particular\nthe cross-validation estimator has poor performance.\n\nin various studies measures have been compared. for example, mittlb\u00f6ck and schemper\n(1996) investigated the performance of most of the measures considered here. they demon-\nstrated that the squared correlation coefficient, r2\nse, and g are very similar over a wide range\nof values and are numerically consistent with the empirical coefficient of determination when\na linear model is an appropriate approximation, which means for small values of r2. squared\n\u03c4a, \u03b3, and r2\nclass were found to yield quite different values. however, it cannot be expected that\nall the measures will behave in the same way since different population measures are behind.\nnumerical consistency or inconsistency is interesting, but in cases where the linear model is\ninappropriate nothing can be inferred on the accuracy of a descriptive statistic if what one is\ntrying to estimate is not well defined.\n\nof course population values should have an intuitively clear interpretation. the kullback-\nleibler distance measure, which is behind the deviance, might not be very convincing for prac-\ntioners. simple measures with clear interpretations are measures that represent performance in\nclassification. behind the number of hits is the probability of a correct classification. there-\nfore, population measure and estimate refer to interesting quantities. but the number of hits\nrefers to hits in the learning sample and therefore can be an overoptimistic estimate. to infer\non the performance in future samples and find appropriate estimates, one should not rely on the\nempirical analog but find alternative estimators. prediction-based measures and estimators are\nconsidered in more detail in chapter 15.\n\ngeneral definitions of population measures that quantify the strength of the relation between\na response variable and a vector of covariates have been given by joe (1989), osius (2004),\nsoofi et al. (2000). van der linde and tutz (2008) considered r2 measures derived from\nsymmetric kullback-leibler discrepancies.\n\n4.7 further reading\nrobust estimators. with the development of diagnostic tools for binary regression models (e.g.,\npregibon, 1981; landwehr et al., 1984; fowlkes, 1987), estimates that are robust against out-\nliers have been suggested. the resistant fitting procedure proposed by pregibon (1982) is based\non the downgrading of the influence of observations with high residuals. copas (1988) consid-\nered the substantial bias of resistant fitting, which yields numerically larger coefficients, yield-\ning a more extreme fit, closer to 0 or 1. he considered a bias-corrected version and proposed\na misclassification model where transpositions between the possible outcomes 0 and 1 happen\nwith a small probability. carroll and pederson (1993) studied an estimate that is closely related\nto copas\u2019 misclassification estimate but which is consistent for the logistic model. rousseeuw\nand christmann (2003) considered estimates that are robust against separation and connected\nto the approach used by tutz and leitenstorfer (2006). an interesting approach to robust fit-\nting by a forward search through the data was proposed by atkinson and riani (2000). an\nalternative form of robustification is the use of shrinkage estimators as considered in chapter 6.\n\n "}, {"Page_number": 132, "text": "120\n\nchapter 4. modeling of binary data\n\nweighted least-squares estimator. grizzle, starmer, and koch (1969) proposed an least-\nsquares estimator for categorical responses. the so-called grizzle-starmer-koch approach was\nvery influential in the modeling of categorical response data. although it allows for an explicit\nform of the estimate it has the disadvantage that it can not be used for continuous predictors.\nwith the computational facilities available today ml estimates are widely preferred.\n\nr packages. glms can be fitted by use of the model fitting functions glm from the mass\npackage. for the selection and smoothing of ordinal predictors one can use the package ord-\npens. quasi-variances can be computed with qvcalc.\n\n4.8 exercises\n\n4.1 derive the likelihood of a binary response model \u03c0(xi) = h(xt\n(yi, xi), i = 1, . . . , n, with yi \u2208 {0, 1}.\n\ni \u03b2) for independent observations\n\n(a) find the derivatives \u2202l(\u03b2)/\u2202\u03b2j for the components of \u03b2 by elementary differentiation.\n(b) show that the resulting score function s(\u03b2) = \u2202l(\u03b2)/\u2202\u03b2 is that of a generalized linear model.\n(c) derive the entries of the matrix of second derivatives h(\u03b2) = \u2202l2(\u03b2)/\u2202\u03b2\u2202\u03b2t .\n(d) determine f (\u03b2) = \u2212 e h(\u03b2) and show that it has the form of the fisher matrix of a glm.\n\n4.2\n\n(a) derive the likelihood of a binary response model p (yij = 1|xi) = \u03c0(xi) = h(xt\n\ndent binomial observations (yi, xi), i = 1, . . . , n, with yi = yi1 + \u00b7\u00b7\u00b7 + yini\n\ni \u03b2) for indepen-\n\u223c b(ni, \u03c0(xi)).\n(b) show that the resulting likelihood equation for the logit model has the form t = e(t ), where\n\n(cid:2)\n\nt =\n\ni yixi is a sufficient statistic of \u03b2 and t is the observed value.\n\n(c) derive the sufficient statistic for the logit model when only two populations are distinguished,\n\nnamely males (x = 1) and females (x = \u20131) and interpret the components of the statistic.\n\n4.3 the following table shows a comparison of a new agent and an active control with the binary response\n1 for much improvement and 0 for improvement (compare example 9.8). the model to be considered\nis the logit model with predictor \u03b7 = \u03b20 + xa\u03b2, where xa = 1 represents the new agent and xa = 0\nrepresents the active control.\n\ndrug\n\n1: new agent\n0: active control\n\n1\n\n24\n11\n\n0\n\n37\n51\n\n(a) fit the model by using the underlying single binary observations, that is, use 24 binary responses\n\nwith response 1 and xa = 1, etc, and examine the significance of the parameter estimates.\n\n(b) fit the model by using the binomial distribution, where one considers only two populations, the\n\nfirst with 61 observations the second with 62 observations. compare with the results from (a)\n\n(c) compute the null deviance d0 (deviance for the model with intercept only) and the deviance for\nthe model dm for the single and grouped observations cases. how can the difference of deviances\nbe used to examine the goodness-of-fit of the model?\n\n4.4 consider a binary logit model with two factors a \u2208 {1, . . . , i}, b \u2208 {1, . . . , j}. it can be given by\nlog(\u03c0(a = i, b = j))/(1 \u2212 \u03c0(a = i, b = j)) = \u03b20 + \u03b2a(i) + \u03b2b(j) + \u03b2ab(ij) with side constraints\n\u03b2a(i) = \u03b2b(j) = \u03b2ab(ij) = \u03b2ab(ij) = 0 for all i, j or with linear predictor \u03b7(a = i, b = j) =\n\u03b20 +xa(1)\u03b2a(1) +\u00b7\u00b7\u00b7+xa(i\u22121)\u03b2a(i\u22121) +xb(1)\u03b2b(1) +\u00b7\u00b7\u00b7+xb(j\u22121)\u03b2b(j\u22121) +xa(1)xb(1)\u03b2ab(11) +\n\u00b7\u00b7\u00b7 + xa(i\u22121)xb(j\u22121)\u03b2ab(i\u22121,j\u22121), where xa(i), xb(j) are dummy variables in 0\u20131 coding.\n\n "}, {"Page_number": 133, "text": "4.8. exercises\n\n121\n\n(a) show how the parameters can be represented as functions of odds and odds ratios (one obtains,\nfor example, e\u03b2ab(ij) = log (\u03b3(i, j)/\u03b3(i, j))/\u03b3(i, j)/\u03b3(i, j), where \u03b3(i, j) = \u03c0(a = i, b =\nj)/(1 \u2212 \u03c0(a = i, b = j)).\n\n(b) interpret the parameters in terms of odds ratios.\n(c) let the probability of being a regular reader of a specific journal depending on gender and age be\n\ngiven by the following table:\n\nage\n\nyoung (1) old (0)\n\ngender\n\nmale (1)\nfemale (0)\n\n0.5\n0.8\n\n0.4\n0.6\n\ncompute the parameters of the corresponding logit model and interpret them.\n\n(d) compute the parameter estimates of a saturated logit model for the data in table 4.9 and interpret\n\nthem.\n\n4.5 the dataset dust that was used in example 4.6 is available from package catdata (or at http://www.stat\n.uni-muenchen.de/sfb386/ under the name \"chronic bronchitis and dust concentration\").\n\n(a) fit models for non-smokers and smokers separately that include quadratic terms of dust concentra-\ntion and years of exposure. decide what effects are needed. consider alternatively models that use\nlog-transformed predictors. compare the fitted models.\n\n(b) fit an appropriate model that includes smoker status as a predictor.\n\n4.6 table 2.2 shows the vasoconstriction dataset.\n\n(a) compare the logit model with predictors volume and rate to the logit model with log-transformed\n\npredictors.\n\n(b) investigate if an interaction term is needed.\n(c) consider binary response models with alternative link functions and decide on an appropriate\n\nmodel.\n\n4.7 show that for binary observations the proportional reduction in variance (d(m0) \u2212 d(m ))/d(m0)\ni \u2212 n\u00afp2)/(n\u00afp(1 \u2212 \u00afp)) if one uses the empirical\n(cid:2)\nis equivalent to gini\u2019s concentration measure g = (\nvariance d(m0) = n\u00afp(1 \u2212 \u00afp), where \u00afp is the proportion of ones in the total sample, and d(m ) =\ni \u02c6\u03c0i(1 \u2212 \u02c6\u03c0i).\n\ni \u02c6\u03c02\n\n(cid:2)\n\n "}, {"Page_number": 134, "text": " "}, {"Page_number": 135, "text": "chapter 5\n\nalternative binary regression models\n\nin this chapter we will first consider alternatives to the logit link. although the logit model\nhas some advantages, which are discussed in section 5.1.3, alternative link functions may be\nmore appropriate in concrete applications. moreover, we will consider extensions of the sim-\nple binary regression model that allow for overdispersion in the response and the conditional\nlikelihood approach.\n\n5.1 alternative links in binary regression\nas in chapter 4, we will consider models of the form \u03c0(x) = h(xt \u03b2), but in this section we\nwill denote them by\n\n\u03c0(x) = f (xt \u03b2).\n\nthe use of f for the response function refers to the derivation of the models from latent vari-\nables models, where f is the distribution function of the latent variable. therefore, the response\n\u22121\nfunctions used here are strictly monotone distribution functions. the inverse functions f\ncorrespond to the link. in the following, common choices for link and response functions are\nmotivated.\n\n5.1.1 binary response models\nprobit model\na widely used model, particularly in economics, is the probit model, which is based on the\nstandard normal distribution \u03c6(\u03b7) = (2\u03c0)\u22121/2\n\n\u2212x2/2dx.\n\n\u2019\n\n\u03b7\u2212\u221e e\n\nprobit model\n\n\u03c0(x) = \u03c6(xt \u03b2), \u03c6\n\n\u22121(\u03c0(x)) = xt \u03b2\n\nin applications the probit model usually yields approximately the same results as the logit\nmodel. the goodness-of-fit is comparable, the same variables turn out to be relevant, and\np\u2212values are about the same, although the values of the estimates should not be compared di-\nrectly (see section 5.1.2). very large sample sizes are needed to distinguish between the logit\n\n123\n\n "}, {"Page_number": 136, "text": "124\n\nchapter 5. alternative binary regression models\n\nand the probit model. one may consider it as a drawback that the response function has no ex-\nplicit form and that parameters do not have the same simple interpretation in terms of log-odds\nas in the logit model; nevertheless, the results are similar.\n\ncomplementary log-log model and log-log model\na distribution function that is distinctly different from the logistic distribution function is the\nminimum extreme value (or gompertz) distribution f (\u03b7) = 1 \u2212 exp(\u2212 exp(\u03b7)). while the\nlogistic distribution function is symmetric, the gompertz distribution is asymmetric (see figure\n5.1 for the density distribution function). the model has the following representations.\n\ncomplementary log-log model\n\n\u03c0(x) = 1 \u2212 exp(\u2212 exp(xt \u03b2))\n\nlog(\u2212 log(1 \u2212 \u03c0(x))) = xt \u03b2 (5.1)\n\nthe name complementary log-log model derives from the second form, where one sees that the\nlink is log-log effecting the complementary probability 1 \u2212 \u03c0(x).\n\nuniform distribution\nnormal distribution\nlogistic distribution\nextreme minimal value distribution\n\n\u22122\n\n0\n\n2\n\nuniform distribution\nnormal distribution\nlogistic distribution\nextreme minimal value distribution\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n\u22122\n\n0\n\n2\n\nfigure 5.1: response functions that correspond to distribution functions. upper panel\nshows distribution functions of uniform distribution, normal distribution, logistic distri-\nbution, and minimum extreme value distribution; lower panel shows the corresponding\ndensities.\n\n "}, {"Page_number": 137, "text": "5.1. alternative links in binary regression\n\n125\n\na closely related model is the log-log model. the complementary log-log model (5.1)\nmodels \u03c0(x) = p (y = 1|x). since the two possible values of y, y = 1 and y = 0, may be\ninterchanged, one might use model (5.1) as well for the response y = 0, assuming 1 \u2212 \u03c0(x) =\n1 \u2212 exp(\u2212 exp(xt \u03b2)), which yields the log-log model.\n\nlog-log model\n\n\u03c0(x) = exp(\u2212 exp(\u2212xt \u03b2))\n\nlog(\u2212 log(\u03c0(x))) = \u2212xt \u03b2\n\nthe use of \u2212xt \u03b2 rather than xt \u03b2 (which is only important for the interpretation of pa-\nrameters) has the advantage that the model has the form \u03c0(x) = f (xt \u03b2)), where f is the\nmaximum value (or gumbel) distribution f (\u03b7) = exp(\u2212 exp(\u2212\u03b7)).\nfor symmetrical distributions like the logistic distribution it is just a matter of taste if one\nuses response y or the transformed response \u02dcy = 1 \u2212 y. if y = 1 corresponds to success,\n\u02dcy = 1 corresponds to failure. if \u03b2 is the parameter vector of the logistic model when modeling\nsuccess, \u2212\u03b2 is the parameter vector when modeling failure. gomertz and gumbel distributions\nare not symmetrical. they are connected in the following way: if a random variable \u03b5 follows\nthe gompertz distribution f (\u03b7), the variable \u2212\u03b5 follows the gumbel distribution 1 \u2212 f (\u2212\u03b7),\nand vice versa. if \u03b2 is the parameter vector of the gompertz model for response y, \u2212\u03b2 is the\nparameter vector of the gumbel model for response \u02dcy. however, the gumbel and the gompertz\nmodels for fixed response y are not equivalent. goodness-of-fit as well as parameter vectors\nwill differ.\n\nexponential model\n\nsuppose that z is a count variable taking values 0, 1, 2 . . . , which may refer to the number of\ncars in a household or in medicine to the number of symptoms. an often useful approximation\nto the distribution of z is the poisson distribution that has probability function p (z = z) =\n\u2212\u03bb\u03bbz/z!, z = 0, 1, 2, . . . , where \u03bb represents the expectation of z. if one is interested only in\ne\nthe dichotomization z = 0 or z > 0 (no car in household versus at least one car in household),\none obtains for the dichotomous variable\n\n(cid:29)\n\ny =\n\n1 z > 0\n0 z = 0\n\nthat\n\np (y = 1) = p (z > 0) = 1 \u2212 p (z = 0) = 1 \u2212 e\n\n\u2212\u03bb.\n\nby assuming that the expectation \u03bb depends on the covariates in a linear way, \u03bb = xt \u03b2,\none obtains the exponential model. of course the complementary log-log model also can be\nmotivated in this way, because the specification \u03bb = exp(xt \u03b2) yields the complementary\nlog-log model.\n\n "}, {"Page_number": 138, "text": "126\n\nchapter 5. alternative binary regression models\n\nexponential distribution or complementary log model\n\u2212 log(1 \u2212 \u03c0(x)) = xt \u03b2\n\u03c0(x) = 1 \u2212 exp(\u2212xt \u03b2)\n\nsince the poisson distribution is strongly connected to the exponential distribution, which is\nthe waiting time until the next event in a poisson process, it is not surprising that the exponential\ndistribution model may also be motivated by a waiting time distribution model. let a duration\ntime (survival or sojourn time) t have exponential distribution t \u223c e(\u03bb) with distribution\nfunction f (x) = 1 \u2212 exp(\u2212\u03bbx), \u03bb > 0. at time point \u03c4 one considers the dichotomization\n\n(cid:29)\n\ny =\n\n1 t < \u03c4\n0 t \u2265 \u03c4,\n\nwhich determines if the process t has ended or not until then; one obtains with parameterization\n\u03bb = xt \u03b3\n\np (y = 1) = p (t < \u03c4) = 1 \u2212 exp(\u2212\u03c4 xt \u03b3) = 1 \u2212 exp(\u2212xt \u03b2),\n\nwhere \u03b2 = \u03c4 \u03b3.\n\nthe motivation by dichotomizations of counts or waiting times shows that the model might\nbe appropriate in many applications. sometimes a problem is the non-convergence of estimates.\nit may arise because the link function is not differentiable everywhere. since f (x) = 1 \u2212\nexp(\u2212\u03bbx), x \u2265 0, the model actually has the form \u03c0(x) = 1 \u2212 exp(\u2212xt \u03b2) if xt \u03b2 \u2265 0 and\n\u03c0(x) = 0 if xt \u03b2 < 0. the simple form \u03c0(x) = 1 \u2212 exp(\u2212xt \u03b2) holds only if xt \u03b2 \u2265 0,\nwhich implies severe restrictions on the parameter space. for discussions and applications see\nwacholder (1986), baumgarten et al. (1989), guess and crump (1978), whittemore (1983),\nand cornell and speckman (1967). a nice overview on modeling with this link function is\nfound in piegorsch (1992).\nif \u03c0(x) is replaced by 1\u2212\u03c0(x), one obtains the simple log-link or exponential model, which\nmay be seen as a model that uses the distribution function f (\u03b7) = exp(\u03b7) for \u03b7 \u2264 0.\n\nexponential or log-link model\n\n\u03c0(x) = exp(xt \u03b2)\n\nlog(\u03c0(x)) = xt \u03b2\n\ncauchy model\na model that is offered by some program packages is based on the cauchy distribution. the\n\"cauchit\" link uses the (standard) cauchy distribution function f (\u03b7) = tan\u22121(\u03b7)/\u03c0 + 1/2,\nwhere tan\u22121 = arctan is the inverse of the tangens and \u03c0 = 3.14159 . . . . the cauchy\ndistribution is somewhat peculiar because it has no mean, variance, or higher moments de-\nfined, although the mode and median are well defined and are both equal to zero.\nit coin-\ncides with the student\u2019s t-distribution with one degree of freedom. the cauchit link function\ng(u) = tan(\u03c0(u \u2212 1/2)) yields the following model.\n\n "}, {"Page_number": 139, "text": "5.1. alternative links in binary regression\n\n127\n\ncauchy model\n\n\u03c0(x) = tan\n\n\u22121(xt \u03b2)/\u03c0 + 1/2\n\ntan(\u03c0(\u03c0(x) \u2212 1\n2\n\n)) = xt \u03b2\n\n(5.2)\n\nit should be noted that \u03c0(x) denotes the probability whereas \u03c0 in (5.2) it denotes the fixed\nand well-known number \u03c0 = 3.14159 . . . . when compared to the normal distribution, the\ncauchy distribution has heavier tails, thus allowing more extreme values than the normal distri-\nbution. for the modeling of binary responses it is attractive when observations occur for which\nthe linear predictor is large in absolute value, indicating that the outcome is rather certain and\nyet the outcome is different. the model is more tolerant to these \"outliers\" than the logit or\nprobit model. an early reference to the cauchit link model is morgan and smith (1993), where\nan example is given in which cauchit performs better than the probit link.\n\nidentity link model\nin a normal regression model the most widely used link is the identity link yielding e(yi) =\nxt\ni \u03b2. with some care it can also be used in binary regressions. however, the assumption\n\u03c0(xi) = xt\ni \u03b2 distinctly ignores that \u03c0(x) is restricted to the unit interval. nevertheless, it may\nbe applied in cases where the covariate space is strongly limited in a way that the restriction to\nthe unit interval holds. in example 4.1 the logistic model yields an almost straight line and the\nlogistic model and the linear model yield similar results (see figure 4.3).\n\nidentity link model\n\n\u03c0(x) = xt \u03b2\n\n5.1.2 comparing link functions\nthe models considered in this section have the basic form \u03c0(xi) = f (xt\ni \u03b2), where f is a\ndistribution function. figure 5.1 shows the response functions for several of these models. at\nfirst sight the response functions seem to differ rather strongly and therefore should yield quite\ndifferent discrepancies between data and fits. however, one should be aware that the distri-\nbution functions used as links are not comparable because they refer to different means and\nvariances. for example, the standard normal distribution that is used in the normal model has\nmean zero and variance one while the logistic distribution (underlying the logistic regression\nmodel) has mean zero and variance \u03c02 (where \u03c0 = 3.14159. . . ). thus, it is no wonder that pa-\nrameter estimates for the probit model and the logit model usually are quite different (although\nhaving about the same exploratory value). it is useful to consider again the derivation of binary\nregression models from latent regression models. in section 2.2.2, it has been shown that all\nmodels of the form \u03c0(xi) = f (\u03b20 + xt\ni \u03b2) (with separated intercept) may be derived from\ni \u03b2 \u2212 \u03b5i, where \u03b5i has the distribution\nan underlying continuous response model, \u02dcyi = \u03b30 + xt\nfunction f and yi = 1 if \u02dcyi is above some threshold \u03b8. one obtains\n\nyi = 1 \u21d4 \u02dcyi = \u03b30 + xt\n\ni \u03b2 \u2212 \u03b5i \u2265 \u03b8 \u21d4 \u03b5i \u2264 \u03b20 + xt\n\ni \u03b2,\n\n "}, {"Page_number": 140, "text": "128\n\nchapter 5. alternative binary regression models\n\nuniform distribution\nnormal distribution\nlogistic distribution\nextreme minimal value distribution\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n\u22122\n\n0\n\n2\n\nfigure 5.2: response functions, standardized to mean zero and variances one, for several\nmodels.\n\nwhere \u03b20 = \u03b30\u2212\u03b8. if one wants to compare parameters of models with different link functions,\none should at least assume that the distribution functions \u03b5i have the same mean and variance.\nstandardization of \u03b5i yields\n\n(cid:16)\n\nyi = 1 \u21d4 \u03b5i \u2212 e(\u03b5i)\n\nvar(\u03b5i)\n\n(cid:16)\n\n\u2264 \u03b30 \u2212 \u03b8 \u2212 e(\u03b5i)\nvar(\u03b5i)\n\n\u03b21(cid:16)\n\nvar(\u03b5i)\n\n+ xi1\n\n+ ... + xip\n\n\u03b2p(cid:16)\n\nvar(\u03b5i)\n\nwith the \"standardized\" parameters\n\n(cid:16)\n\n\u02dc\u03b20 = \u03b20 \u2212 e(\u03b5i)\n\nvar(\u03b5i)\n\n,\n\n\u02dc\u03b2i =\n\n\u03b2i(cid:16)\n\nvar(\u03b5i)\n\n.\n\nwith fstand(\u03b7) denoting the standardized distribution function (centered around zero with vari-\nance one), the models\n\n\u03c0(x) = f (\u03b20 + xt \u03b2)\n\nand \u03c0(x) = fstand( \u02dc\u03b20 + xt \u02dc\u03b2)\n\nare equivalent. it is seen from figure 5.2 that the standardized response (distribution) functions\nare not so far apart. in particular, the normal and the logistic distribution functions are quite\nclose. therefore, it is quite natural that they yield similar goodness-of-fit, although parameter\nestimates for the unstandardized versions will differ.\n\nuniform distribution\nnormal distribution\nlogistic distribution\nextreme minimal value distribution\n\n2\n\n1\n\n0\n\n1\n\u2212\n\n2\n\u2212\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\nfigure 5.3: link functions f\n\n\u22121(\u03c0) plotted against \u03c0.\n\n "}, {"Page_number": 141, "text": "5.1. alternative links in binary regression\n\n129\n\ntable 5.2 shows the fit of several binary response models for the response \"car in household\"\nwith net income in the linear predictor. it is seen that the original estimates \u02c6\u03b2 are quite different\nbut the standardized values are similar. in this dataset the cauchy model showed the best fit,\nfollowed by the logit model.\n\ntable 5.1: means and variances of \u03b5 for several models.\n\ndistrib.\n\nmean\nvariance\n\nlogit\n\nlogistic\n\nprobit\nnormal\n\n0\n\n\u03c02/3\n\n0\n1\n\nmin. extr. value\n\n\u22120.577\n\u03c02/6\n\ncomplementary log-log\n\nlog-log\n\nmax. extr. value\n\ncompl. exp\nexponential\n\n0.577\n\u03c02/6\n\n1\n1\n\ntable 5.2: estimates and standardized estimates for several link functions modeling the\ndependence of car ownership on net income.\n\n\u02c6\u03b20\n\n\u22122.424\nlogit\n\u22121.399\nprobit\nc.log-log \u22121.743\n\u22122.655\ncauchy\n\n\u02dc\u03b20(standardized)\n\n\u22121.334\n\u22121.399\n\u22120.909\n\n\u02c6\u03b2\n\n\u02dc\u03b2(standardized)\n\ndeviance\n\n0.0020\n0.0011\n0.0010\n0.0023\n\n0.0011\n0.0011\n0.0008\n\n1497.7\n1505.0\n1538.2\n1479.7\n\n5.1.3 choice between models and advantages of logit models\nfor the choice of the link function, in particular, two aspects are relevant: goodness-of-fit and\nease of interpretation. if one focusses on goodness-of-fit, one simply chooses the model that\nshows the best fit. nevertheless, one always should be aware that only a selection of possible\nfits has been considered, so the best fit is only the best fit among the considered ones.\nin\naddition, differences of deviances have no standard distribution since there is no hierarchical\norder between the models. if responses are binomially distributed with not too small sample\nsizes ni, the deviance can be compared to the \u03c72-distribution showing if the models have a\ndistinct lack-of-fit. an alternative approach to the selection of link functions is to estimate the\nfit in a non-parametric way. one can also fit parametric families of link functions that include\nthe classical link functions as members. then parameter estimates decide on the specific link\nfunction (for both approaches see section 5.2).\n\nif the deviances are not too different, a result that is usually found when comparing the logit\nmodel with the probit model, issues of interpretation become dominant. the logit model has\nseveral advantages that make it the most widely used model:\n\n(1) the parameters are easily interpretable in terms of log-odds (for \u03b2) or odds (for exp(\u03b2)).\n\n(2) if all of the covariates are categorical, the hypothesis h0 : \u03b2 = 0 that none of the covari-\nates has any exploratory value is equivalent to the statement that the response variable\nand the covariates are independent. for single predictors the hypothesis h0 : \u03b2j = 0\ncorresponds to the conditional independence of the response given the other variables\n(compare chapter 12).\n\n(3) the model is linked to the normal distribution, which plays a central role in statistics, by\n\nthe derivation from the assumption x|y = i \u223c n(\u03bci, \u03c3); see section 2.2.2.\n\n "}, {"Page_number": 142, "text": "chapter 5. alternative binary regression models\n\n130\n(4) the logit link is the canonical link function, linking the natural parameter log(\u03c0/(1\u2212 \u03c0))\ndirectly to the linear predictor. one consequence is that the conditions for the existence of\nmaximum likelihood estimates are weaker because the log-likelihood function is concave\nunder weak conditions (see fahrmeir and kaufmann, 1985).\n\n(5) the effects of variables may also be estimated if observations are drawn from the condi-\ntional distribution of x given y instead of the usual form of observing responses y given\nx. in econometrics this is called choice-based sampling (see section 4.1).\n\nsome motivation for the logit model may also be obtained from looking at the origins of the\nlogistic distribution function (see section 2.4).\n\n5.2 the missing link\nin section 5.1 binary regression models with known link function were considered. although\nvarious link functions may be used, there is always the danger that the true link function might\nnot be among them or, more realistically, that none of the link functions provides a sufficiently\ngood approximation to the data. this may be important since it has been demonstrated that\nmisspecification of the link function can lead to substantial bias in the regression parameters\n(see czado and santner, 1992 , for binomial responses). similar results have been demonstrated\nfor single-index models by horowitz and h\u00e4rdle (1996).\n\nparametric families of link functions\none approach to obtain more flexible models is to fit the parametric families of link functions.\nseveral families have been proposed, including the link functions, that are in common use;\nsee prentice (1976), pregibon (1980), aranda-ordaz (1983), morgan (1985), stukel (1988),\nczado (1992), and czado (1997). when using families of response functions, the common\nresponse function f (\u03b7) is replaced by f (\u03b7, \u03c8) with additional parameter \u03c8. as an example, let\nus consider a useful family allowing for right-tail modification of the logistic link (see czado,\n1997):\n\nwhere\n\nh(\u03b7, \u03c8) =\n\nf (\u03b7, \u03c8) =\n\nexp(h(\u03b7, \u03c8))\n\n1 + exp(h(\u03b7, \u03c8)) ,\n\n(cid:2)\n\n(\u03b7+1)\u03c8\u22121\n\n\u03c8\n\n\u03b7\n\n\u03b7 > 0\notherwise.\n\nif \u03c8 = 1, the logistic link results; for \u03c8 < 1 (\u03c8 > 1) the right tail is heavier (lighter) than for\nthe logistic distribution. families of this type have the advantage that the parameterization is\northogonal in a neighbourhood around \u03b2 = 0. when using families of response functions there\nthat corresponds to the canonical link. in the case of the right-tail\nis usually a parameter \u03c8\nmodification family one has \u03c8\n\n\u2217 = 1.\n\n\u2217\n\na common approach to decide on the link is to treat it as a testing problem:\n\nh0 : \u03c8 = \u03c8\n\n\u2217\n\nagainst h1 : \u03c8 (cid:8)= \u03c8\n\n\u2217\n\n.\n\nif h0 is not rejected, one keeps the canonical link; if h0 is rejected, \u03c8 and the regression param-\neters are estimated jointly to obtain the mle \u02c6\u03b4 = (\u02c6\u03b2, \u02c6\u03c8). asymptotic theory, including strong\n\n "}, {"Page_number": 143, "text": "5.2. the missing link\n\n131\n\nconsistency and asymptotic normal distributions was derived by czado and munk (2000). the\nuse of the testing problem as a tool of model selection has been critized by various authors.\nin particular, czado and munk (2000) point out that for large sample sizes h0 is frequently\nrejected in favor of h1, although the mean space of both link functions is almost indistinguish-\nable and therefore not scientifically relevant. they propose an alternative testing strategy that\ntakes a measure of discrepancy between the response functions into account.\n\nnon-parametric fitting of link functions\nfamilies of link functions have the advantage that the link function is estimated by using pa-\nrameters, and by fitting a larger family one avoids having to estimate separately non-nested\nmodels. a disadvantage is that the functions are still restricted to belong to the specified family.\nmore flexible models are obtained by estimating link functions non-parametrically.\n\nespecially in the economic literature, models of this type are known under the name single-\n\nindex models. a single-index model has the form\n\n\u03bci = h(xt\n\ni \u03b2),\n\ni \u03b21) + \u00b7\u00b7\u00b7 + hm(xt\n\nwhere h is a smooth but unspecified function and \u03bci = e(yi|xi) denotes the conditional mean\ngiven covariates xi. the linear predictor xt\ni \u03b2 is known as the single index. the model may be\nseen as a special case of a projection pursuit regression, which assumes that \u03bci has the additive\nform h1(xt\ni \u03b2m) with unknown functions h1, . . . , hm, which transform\nthe indices; see friedman and st\u00fctzle (1981). several approaches to the estimation of single-\nindex models have been proposed in the literature (for references see the end of the chapter).\nhowever, in single-index models typically the response is assumed to be metrically scaled,\nand often a normal distribution is assumed. moreover, single-index models do not assume\nthat the function h(.) is monotone. therefore, the approaches are less helpful when one wants\nto estimate the unknown link function in generalized models. although monotonicity is not\nneeded when searching for a single index xt\ni \u03b2, it is useful for interpreting the parameters. if\nthe link function h(.) is not monotone, it is hard to interpret the parameters because a positive\ncoefficient might increase or decrease the mean depending on the value of the other predictors.\nthus, what might be helpful for dimension reduction is less helpful for fitting models that have\neasy interpretations.\n\nestimation of the unknown link function when the underlying distribution is from a simple\nexponential family was considered, for example, by weisberg and welsh (1994), ruckstuhl\nand welsh (1999), and muggeo and ferrara (2008). weisberg and welsh (1994) proposed es-\ntimating regression coefficients using the canonical link and then estimating the link via kernel\nsmoothers given the estimated parameters. then the parameters are re-estimated. alternating\nbetween estimation of link and parameters yields consistent estimates.\n\nthe basic principle of alternating between these two estimates was also used by yu and\nruppert (2002), but instead of kernel smoothers the unknown function is approximated by an\nexpansion in basis functions. the approach may be outlined briefly as follows. for the unknown\nlink function one uses the expansion\n\nm(cid:7)\n\nh(\u03b7i) =\n\n\u03b1j\u03c6j(\u03b7i),\n\ni \u03b2. to make the problem identifiable, ||\u03b2\u03b2\u03b2|| = 1 is postulated, where ||.|| denotes\nwhere \u03b7i = xt\nthe euclidean norm. moreover, \u03b2 contains no intercept; it is included in h(.). splines are\nobtained by using the truncated power series basis functions bj(.) of degree q, which have\n\nj=1\n\n "}, {"Page_number": 144, "text": "132\n\nchapter 5. alternative binary regression models\n\nalso been used, for example, by ruppert (2002). thus the functions have the form b1(\u03b7) =\n1, b2(\u03b7) = \u03b7, bq+1(\u03b7) = \u03b7q, bq+j(\u03b7) = |\u03b7 \u2212 \u03c4j|+, j > 1, where \u03c41, \u03c42, . . . are fixed knots.\nin a p-spline regression (see section 10.1.3), usually a rather high number of equidistant knots\nis used (say m = 20 or 40) and the smoothness of the function estimate is controlled by an\nappropriate penalization. ruppert (2002) suggested penalizing the squared coefficients that\nbelong to the truncated powers, that is,\n\n(cid:14)\n\nm\nj=q+2 \u03b12\nj .\n\nlet the response vector be given by yt = (y1, . . . , yn) and the design matrix by x =\n(x(1), . . . , x(p)), where x(j) = (x1j, . . . , xnj)t denotes the observations of the jth covariate,\nj = 1, . . . , p. then, in the simplest case of normally distributed responses, an estimator of the\nsingle-index model is formulated as a minimizer of the penalized least-squares criterion:\n\nq(\u03b1\u03b1\u03b1, \u03b2\u03b2\u03b2) = (y \u2212 \u03c6(\u03b7\u03b7\u03b7)\u03b1\u03b1\u03b1)t (y \u2212 \u03c6(\u03b7\u03b7\u03b7)\u03b1\u03b1\u03b1) + \u03bbp \u03b1\u03b1\u03b1t p\u03b1\u03b1\u03b1,\n(5.3)\nwhere \u03b7\u03b7\u03b7 = x\u03b2\u03b2\u03b2, \u03c6(\u03b7\u03b7\u03b7) = (\u03c61(\u03b7\u03b7\u03b7), . . . , \u03c6m(\u03b7\u03b7\u03b7)) = (1, \u03b7\u03b7\u03b7, . . . , \u03b7\u03b7\u03b7q, (\u03b7\u03b7\u03b7 \u2212 1\u03c41)q\n+, . . . , (\u03b7\u03b7\u03b7 \u2212 1\u03c4m)q\n+),\n\u03c6j(\u03b7\u03b7\u03b7) = (\u03c6j(\u03b71), . . . , \u03c6j(\u03b7n))t , p = diag{0q+1, 1m}, and \u03bbp is a penalization parameter. yu\nand ruppert (2002) suggest solving (5.3) by using common non-linear least-squares routines\nwhile leitenstorfer and tutz (2011) and tutz and petry (2011) used boosting techniques.\n\nin the case of glms, it is more appropriate to fit the model \u03bci = h0(h(\u03b7i)), where h0(.) is a\nfixed transformation function, which has to be chosen, and the inner function h(.) is considered\nas unknown and has to be estimated. typically, the choice of h0(.) depends on the distribution\nof the response. when the response is binary, a canonical choice is the logistic distribution\nfunction. the main advantage of specifying a fixed link function is that it may be selected such\nthat the predictor is automatically mapped into the admissible range of the mean response. the\nexpansion in basis functions is applied to the inner function h(.).\n\nm\n\n(l)\n\n(l)\n\n1 , . . . , \u03c6(l)\n\nn )t with \u03c6(l)\n\nrameters of the link function \u03b1, where h(\u03b7i) =\nlet \u02c6\u03b2\nand \u02c6\u03b7(l) = x \u02c6\u03b2\nmoreover, \u03c6(l) = (\u03c6(l)\ndesign matrix for the basis functions. then two steps are iterated:\n\nestimates are obtained by iteratively estimating the regression coefficients \u03b2 and the pa-\nj=1 \u03b1j\u03c6j(\u03b7i) = \u03b1t \u03c6i. in matrix notation,\ndenote the parameter estimate and the fitted predictor in the lth step.\ni ))t is the current\n(l\u22121)\nthe model \u03bc = h0((\u03c6(l\u22121))t \u03b1(l)) is fitted by one step of penalized fisher scoring that uses\nthe matrix of derivations \u02c6d(l\u22121) = diag(\u2202h0(\u02c6h(l\u22121)(\u02c6\u03b7(l\u22121)\n)/\u2202h(l\u22121)(\u03b7)) evaluated at the es-\ntimate of the previous step, the diagonal matrix of variances evaluated at h0(\u02c6h(l\u22121)(\u03b7(l\u22121)\n)),\nand a penalty matrix ph that penalizes the second derivation of the estimated (approximated)\nresponse function (for penalties see section 10.1.3).\n\nestimation of basis coefficients for a fixed predictor. for a fixed predictor \u02c6\u03b7(l\u22121) = x\u02c6\u03b2\n\ni ), . . . , \u03c6m(\u02c6\u03b7(l)\n\ni = (\u03c61(\u02c6\u03b7(l)\n\ni\n\ni\n\nestimation of regression coefficients for a fixed response function. for h(.) fixed, one fits\n\n(cid:14)\n\n\u02c6\u03b2\n\n(l) = (x t \u02c6d\n\nthe model \u03bc = h0(h(x\u03b2(l)). fisher scoring has the form\n\u22121x t \u02c6d\n\n(l\u22121)\n\u03b7 x)\n(l\u22121) = diag(\u2202h0(\u02c6h(l\u22121)(\u02c6\u03b7(l\u22121)\n(l\u22121)\ni\n\nwhere \u02c6d\u03b7\nvalues of the previous iteration and \u02c6\u03c3\n\n(l\u22121))\n\n(l\u22121)\n\u03b7\n\n\u22121 \u02c6d\n\n( \u02c6\u03c3\n\n(l\u22121)\n\u03b7\n\n(l\u22121))\n\n\u22121(y \u2212 \u02c6\u03bc(l\u22121)),\n\n( \u02c6\u03c3\n\n(5.4)\n\n))/\u2202\u03b7) is the matrix of derivatives evaluated at the\nis the variance from the previous step.\n\nthe second step can be modified to include a selection step that includes the most relevant\n\npredictor within a boosting procedure; see section 6.4, where more details are given.\n\n5.3 overdispersion\nin practice it is not too rarely found that models have large deviance although there seems to\nbe no systematic lack-of-fit. additional noise that is not accounted for may make the responses\n\n "}, {"Page_number": 145, "text": "5.3. overdispersion\n\n133\n\nmore variable than is to be expected under the assumed distribution model. the data show\noverdispersion. although underdispersion, which signals lower variability than expected, is\nalso found, it is much rarer. there are several strategies for dealing with overdispersion, but\nfirst we consider potential sources for the phenomenon in binary data.\n\n5.3.1 sources of overdispersion\ncorrelated observations\nwhen considering binomial data yi = yi1 + \u00b7\u00b7\u00b7 + yini\n\u223c b(ni, \u03c0i) in prevous sections it was\nassumed that y11, . . . , y1n1 , y21, . . . , yn nn are independent given the covariates. in particular,\nif measurements yi1, . . . , yini are collected at one unit, this assumption may be violated.\n\nif one assumes that yi1, . . . , yini , yij \u223c b(1, \u03c0i) are correlated, one obtains\n\nni(cid:7)\n\nni(cid:7)\n\nj=1\n\nj=1\n\n(cid:7)\n\nr(cid:5)=s\n\nvar(yi) = var(\n\nyij) =\n\nvar(yij) +\n\ncov(yir, yis).\n\nby using var(yij) = \u03c0i(1 \u2212 \u03c0i), cov(yir, yis) = \u03c1(var(yir) var(yis))1/2 with \u03c1 denoting the\ncorrelation coefficient, one has\n\nvar(yi) = ni\u03c0i(1 \u2212 \u03c0i)[1 + (ni \u2212 1)\u03c1] = ni\u03c0i(1 \u2212 \u03c0i)\u03c6i\n\nwith dispersion parameter \u03c6i = 1 + (ni \u2212 1)\u03c1. the resulting effects are\n\n\u2022 for ni = 1 one has no overdispersion, since \u03c6i = 1;\n\n\u2022 for a positive correlation, \u03c1 > 0, the variability is larger than expected under the binomial\n\nprobability model (provided ni > 1);\n\n\u2022 for a negative correlation, \u03c1 < 0, underdispersion is found (assuming ni > 1); however,\nsince \u03c6i \u2265 0 has to hold, the negative correlation is restricted by \u03c1 \u2265 \u22121/(ni \u2212 1).\n\nunobserved heterogeneity\na possible source of heterogeneity is that unobserved or unobservable variables induce extra\nvariability in y. let us assume that there is a latent variable di that selects a probability and the\nresponse yi given the selected value (and observed covariates) has the usually assumed binomial\ndistribution (see also williams, 1982). to be more specific, the following is assumed.\n(1) a latent variable di \u2208 [0, 1] with\ne(di) = \u03c0i,\n\nvar(di) = \u03b4\u03c0i(1 \u2212 \u03c0i), \u03b4 \u2265 0.\n\nselects a value \u03d1i from [0, 1].\n\n(2) given di = \u03d1i, the response has the binomial distribution\nyi|di = \u03d1i \u223c b(ni, \u03d1i).\n\nby using e(e(y |x)) = e(y ) and var(y ) = var e(y |x) + e(var(y |x)), one obtains for\nthe marginal distribution of yi\n\ne(yi) = ni\u03c0i,\n\nvar(yi) = ni\u03c0i(1 \u2212 \u03c0i)\u03c6i,\n\n "}, {"Page_number": 146, "text": "chapter 5. alternative binary regression models\n\n134\nwhere \u03c6i = 1 + (ni \u2212 1)\u03b4. thus the variable yi has the usual mean to be expected under the\nbinomial model, but the variances are inflated. if \u03b4 > 0 and ni > 1, the variability is larger\nthan in the binomial model. the limiting case \u03b4 = 0 means that the latent variable has zero\nvariance. then the latent variable has no effect and the binomial model holds.\n\nit is noteworthy that the overdispersion resulting from correlated responses and the assump-\ntion of an underlying latent variable yields almost the same model for overdispersion. a dif-\nference is that the latent variable approach allows only overdispersion whereas the correlation\nmodel also allows for a restricted form of underdispersion.\n\nthere are several strategies for dealing with overdispersion data. one is the explicit mod-\neling of heterogeneity and an example is the beta-binomial model, which is considered in the\nfollowing. alternatively, one can assume a normal distribution for the unobserved heterogeneity\nor a finite mixture; both modeling approaches will be treated in chapter 14. a second strategy\nis to use generalized estimation functions, which are also considered in the next section.\n\n5.3.2 beta-binomial model\na candidate for the distribution of the latent variable di \u2208 [0, 1] is the beta distribution (see\nappendix a). if one assumes di \u223c beta(ai, bi) with parameters ai, bi > 0, the means and\nvariances are given by\n\ne(di) = \u03c0i = ai\n\nai + bi\n\n,\n\nvar(di) = \u03c0i(1 \u2212 \u03c0i)/(ai + bi + 1).\ni \u03b2), one obtains var(di) = \u03b4i\u03c0i(1 \u2212 \u03c0i) with the\n\nassuming the parametric model \u03c0i = h(xt\ndispersion parameter \u03b4i = 1/(ai + bi + 1).\n\nthe marginal distribution of yi is called the beta-binomial distribution; the explicit form is\n\nobtained by integrating with respect to the density of the beta distribution:\n\np (yi; ni, ai, bi) =\n\n=\n\n(cid:26)\n\n)\n) (cid:25)\np (yi|di = \u03d1i)p(\u03d1i)d \u03d1i\n(cid:26)\n(cid:25)\ni (1 \u2212 \u03d1i)ni\u2212yi\n\u03b3(ai)\u03b3(bi) \u03d1ai\u22121\nni\n\u03d1yi\nyi\n(ai + yi \u2212 1)yi(bi + ni \u2212 yi \u2212 1)ni\u2212yi\n\n\u03b3(ai + bi)\n\ni\n\n,\n\n(1 \u2212 \u03d1i)bi\u22121d \u03d1i\n\n(ai + bi + ni \u2212 1)ni\nwhere (k)r = k(k \u2212 1) . . . (k \u2212 r + 1). an alternative form is\n\n=\n\nni\nyi\n\np (yi; ni, ai, bi) = b(ai + yi, bi + ni \u2212 yi)\n\nb(ai, bi)\n\n,\n\nthe model contains the parameters \u03b2 implicitly.\n\nyi \u2208 {0, 1 . . . , ni}, where b(p, q) = \u03b3(p)\u03b3(q)/\u03b3(p + q) is the beta function.\ninstead of ai, bi one may use the pa-\nrameters \u03c0i = ai/(ai + bi), \u03b4i = 1/(ai + bi + 1), which yields ai = \u03c0i(1 \u2212 \u03b4i)/\u03b4i, bi =\n(1 \u2212 \u03c0i)(1 \u2212 \u03b4i)/\u03b4i and turns the density into b(\u03c0i(1 \u2212 \u03b4i)/\u03b4i + yi, (1 \u2212 \u03c0i)(1 \u2212 \u03b4i)/\u03b4i +\nni \u2212 yi)/b(\u03c0i(1 \u2212 \u03b4i)/\u03b4i, (1 \u2212 \u03c0i)(1 \u2212 \u03b4i)/\u03b4i). for simplicity it is often assumed that \u03b4i = \u03b4\nis the same for all observations. then, assuming that the beta-binomial distribution holds, one\nspecifies the mean by \u03c0i = h(xt\ni \u03b2) with a fixed response function h. thus the mean is given\nby \u03bci = ni\u03c0 and the variance by var(yi = ni\u03c0(1 \u2212 \u03c0)[1 + (ni \u2212 1)\u03b4], with \u03b4 = 0 representing\nthe limiting case of a binomial distribution.\n\nbeta-binomial models cannot be treated within the framework of generalized linear models.\ncrowder (1987) and hinde and d\u00e9metrio (1998) gave algorithms for solving the maximum\n\n "}, {"Page_number": 147, "text": "5.3. overdispersion\n\n135\n\nlikelihood equations. the latter obtained the fit by iterating between estimates of \u03b2 for fixed\n\u03b4 and estimates of \u03b4 for fixed \u03b2. prentice (1986) considered a more general model, where \u03b4i\ncould also depend on covariates.\n\n5.3.3 generalized estimation functions and quasi-likelihood\nexplicit parametric modeling of heterogeneity as used in the derivation of the beta-binomial\nmodel has several drawbacks. numerical integration can be avoided only for specific distribu-\ntions. more seriously, the assumption of a specific distribution function determines inferences,\nalthough it is hard to validate. to avoid the restrictive assumption of a specific latent variable\none can use generalized estimation equations that are based on quasi-likelihood approaches (see\nalso section 3.11).\nin quasi-likelihood approaches it is not necessary to specify a distribution for the responses.\nmuch weaker, one only specifies the first two moments. for count data yi with yi \u2208 {0, 1 . . . , ni}\none might assume that the means and variances are given by\n\ne(yi) = ni\u03c0i = nih(xt\n\ni \u03b2),\n\nvar(yi) = ni\u03c0i(1 \u2212 \u03c0i)\u03c6,\n\nwhich corresponds to an overdispersed binomial distribution. for proportions \u00afyi = pi = yi/ni\none has e(\u00afyi) = \u03c0i = h(xt\n\ni \u03b2), var(\u00afyi) = \u03c0i(1 \u2212 \u03c0i)\u03c6/ni.\n\nthe essential point in these equations is that the mean and variance are specified, with the\n\nvariance having the simple form of an inflated binomial variance:\n\nvar(yi) = v(\u03c0i)\u03c6,\n\nn(cid:7)\n\nwhere v(\u03c0i) is a known (or fully specified) variance function. this means that the functional\nform of the variance (depending on covariates) is assumed to be known. only the scaling factor\n\u03c6 is unknown and has to be estimated. an estimate of \u03c6 is\n\nn(cid:7)\n\ni=1\n\n=\n\n1\n\nn \u2212 p\n\n(\u00afyi \u2212 \u02c6\u03c0i)2\nv(\u03c0i)/ni\n\n.\n\n1\n\n\u02c6\u03c6 =\n\n(yi \u2212 \u02c6\u03bci)2\nniv(\u03c0i)\nfor v(\u03c0i) = \u03c0i(1 \u2212 \u03c0i) one obtains \u02c6\u03c6 = \u03c72\nstatistic.\n\nn \u2212 p\n\ni=1\n\np /(n \u2212 p), where \u03c72\n\np is pearson\u2019s goodness-of-fit\nthe variance function v(\u03c0i) = \u03c0i(1 \u2212 \u03c0i) is very easy to handle. one fits the ordinary\nbinary regression model and uses the ml estimate. to obtain the correct covariances matrix of\n\u02c6\u03b2 one multiplies the maximum likelihood covariance by \u02c6\u03c6 since the covariance is approximated\nby cov(\u03b2) \u2248 \u02c6\u03c6f (\u03b2)\u22121, where f (\u02c6\u03b2) is the fisher matrix of the ordinary model. maximum\nlikelihood standard errors are multiplied by\n\n\u02c6\u03c6 and t statistics are divided by\n\nan alternative estimate of \u03c6, based on the deviance, is \u02dc\u03c6 = d/(n \u2212 p). it is comparable to\n\u02c6\u03c6 if all ni\u2019s are of similar size. while \u02c6\u03c6 is also consistent for small local samples, this does not\nhold for \u02dc\u03c6 (compare mccullagh and nelder, 1989).\n\n*\n\n*\n\n\u02c6\u03c6.\n\nthe estimation equation that is used to obtain an estimate of \u03b2 has the form\n\nn(cid:7)\n\ni=1\n\n(cid:3)(xt\ni \u03b2)\nh\nvar(pi)\n\nxi\n\n(pi \u2212 h(xt\n\ni \u03b2)) = 0.\n\n(5.5)\n\ni\u03b2)(1\u2212\n(cid:3)\nwhen the variance of pi is the inflated binomial variance, specified by var(pi) = \u03c6h(x\n(cid:3)\ni\u03b2))/ni, the solution of equation (5.5) does not depend on \u03c6. of course, when \u03c6 = 1 and\nh(x\n\n "}, {"Page_number": 148, "text": "136\nv(\u03c0i) = \u03c0i(1 \u2212 \u03c0i) is assumed, equation (5.5) is equivalent to the ml estimation equation for\nthe corresponding binomial model.\n\nchapter 5. alternative binary regression models\n\nwhen using these estimates it is assumed that the mean and variance are correctly spec-\nified. more generally, one may consider equation (5.5) as a generalized estimation function\nunder the assumption that only the mean is correctly specified, whereas the variance is consid-\nered a working covariance that does not have to be the variance of the data-generating model\n(e.g., gourieroux et al., 1984). it may be shown that under regularity conditions one obtains\n\u22121), where the sandwich\nan asymptotically normally distributed estimate \u02c6\u03b2 \u223c n(\u03b2, \u02c6f\nmatrix \u02c6f\n\nis determined by\n\n\u22121 \u02c6v \u02c6f\n\n\u22121 \u02c6v \u02c6f\n\n\u22121\n\nn(cid:7)\n\ni=1\n\n\u02c6f =\n\nxixt\ni\n\n(cid:3)(xt\n\u02c6\u03b2)2\nh\n\u02c6var(pi)\n\ni\n\n\u02c6v =\n\nxixt\ni\n\nh\n\n\u02c6\u03b2)2\n\n(cid:3)(xt\n\u02c6var(pi)2 (pi \u2212 h(xt\n\ni\n\ni\n\n\u02c6\u03b2))2\n\ng(cid:7)\n\ni=1\n\n\u02c6\u03b2)).\n\n(cid:3)\nwith \u02c6var(pi) = \u02c6\u03c6v(h(x\ni\nalthough the inflated binomial variance is easy to handle, the assumption of correlated re-\nsponses as well as the modeling by latent variables suggest that for count data yi \u2208 {0, 1 . . . , ni}\nvariances are given by var(yi) = ni\u03c0i(1 \u2212 \u03c0i)[1 + (ni \u2212 1)\u03b4], where \u03b4 is a dispersion param-\neter. the corresponding quasi-likelihood approach, which assumes that the mean and variance\nare correctly specified, is less easy to handle because the dispersion parameter does not cancel\nout. williams (1982) proposed an algorithm that iterates between estimates of \u03b2 for fixed \u03b4 and\nestimates of \u03b4 for fixed \u03b2.\n\nliang and mccullagh (1993) compared various approaches to the modeling of overdisper-\nsion; an overview can be found in poortema (1999). lambert and roeder (1995) introduced a\nconvexity plot that detects overdispersion, relative variance curves, and tests that help to under-\nstand the nature of the overdispersion. relative variance curves and tests sometimes distinguish\nthe source of the overdispersion better than score tests.\n\nexample 5.1: teratology\nin a teratology experiment considered by moore and tsiatis (1991) and liang and mccullagh (1993), 58\nrats on iron-deficient diets were assigned to four groups (see table 5.3). in the first group only placebo\ninjections were given, and in the other groups iron supplements were given. the animals were impregnated\nand sacrificed after three weeks. the response was whether the fetus was dead (yij = 1) for each fetus in\neach rat\u2019s litter.\n\ntable 5.3: response counts of (litter size, number dead) for 58 litters of rats in low-iron\nteratology study.\n\ngroup 1: untreated (low iron)\n(10,1)(11,4)(12,9)(4,4)(10,10)(11,9)(9,9)(11,11)(10,10)(10,7)(12,12)\n(10,9)(8,8)(11,9)(6,4)(9,7)(14,14)(12,7)(11,9)(13,8)(14,5)(10,10)\n(12,10)(13,8)(10,10)(14,3)(13,13)(4,3)(8,8)(13,5)(12,12)\ngroup 2: injections days 7 and 10\n(10,1)(3,1)(13,1)(12,0)(14,4)(9,2)(13,2)(16,1)(11,0)(4,0)(1,0)(12,0)\ngroup 3: injections days 0 and 7\n(8,0)(11,1)(14,0)(14,1)(11,0)\ngroup 4: injections weekly\n(3,0)(13,0)(9,2)(17,2)(15,0)(2,0)(14,1)(8,0)(6,0)(17,0)\n\nsource: moore and tsiatis (1991)\n\nsince the observations for the ith litter yi1, . . . yini were measured on one female rat, one might\nsuspect overdispersion. let yij \u223c b(1, \u03c0i) denote the response of the jth fetus in litter i and\n\n "}, {"Page_number": 149, "text": "5.3. overdispersion\n\nyi = yi1 + \u00b7\u00b7\u00b7 + yini\nstructural component one assumes for \u03c0ij = e(yij) a logit model\n\n137\n\u223c b(ni, \u03c0i) denote the number dead out of the ni fetuses in litter i. as a\n\nlogit(\u03c0ij) = \u03b20 + xg(2)\u03b22 + xg(3)\u03b23 + xg(4)\u03b24,\n\nwhere xg(i) is a (0-1)-dummy variable with xg(i) = 1 if the observation is from group i and 0 otherwise.\nthe naive approach assumes that all observations y11, y12, . . . are independent binary variables. one\nobtains a deviance of 173.45 on 54 degrees of freedom, which, however, should not be interpreted as a\ngoodness-of-fit statistic. in table 5.4 results are given for the approaches that yield the same estimates but\ndiffering standard errors. the independence model assumes that all binary observations are independent.\nthe quasi-likelihood approach with an inflated binomial variance uses var(yi) = \u03c6ni\u03c0i(1 \u2212 \u03c0i). in\nthe weaker generalized estimation approach, independence was used as a working covariance. it is seen\nthat standard errors are larger when overdispersion is taken into account. the simple independence model\nis certainly not appropriate. in table 5.5 estimates are given for the beta-binomial model and for the\nmixed model approach, estimated by penalized quasi-likelihood and gauss-hermite quadrature with 14\nquadrature points. the mixed model assumes that each unit (rat) has its own intercept, which follows a\nnormal distribution. therefore, heterogeneity across units is modeled quite flexibly (for details see chapter\n14). it is seen that the estimates for the beta-binomial model are slightly smaller than the estimated values\nin table 5.4. for the mixed model approach estimates are distinctly larger, an effect that is frequently\nfound in subject-specific models (chapter 14). in addition, a discrete mixture model with two components\nwas fitted. the model assumed that the response was a mixture of two components that have distinct\nintercepts (see chapter 14). the two intercepts of the components, which are not given in the table,\nwere \u22120.211 and 2.458. the estimated effects are quite close to the estimates for the mixed model with\nnormally distributed random effects.\n\ntable 5.4: estimates and standard errors for independence model, quasi-likelihood\nmodel, and generalized estimation functions fitted to teratology data with logit link.\n\nestimates\n\nstandard errors\n\nindependence\n\nmodel\n\n0.129\n0.331\n0.731\n0.476\n\u2212\n\n1.144\n\u22123.323\n\u22124.476\n\u22124.130\n\n\u03b20\n\u03b22\n\u03b23\n\u03b24\n\n\u02c6\u03c6\n\nquasi-likelihood\n\nvar(yi) = \u03c6ni\u03c0i(1 \u2212 \u03c0i)\n\ngee\n\nindependence\n\n0.219\n0.560\n1.237\n0.806\n\n2.865\n\n0.275\n0.440\n0.610\n0.576\n\n1.007\n\ntable 5.5: estimates for beta-binomial model and several mixture logit models fitted to\nteratology data\n\nbeta-binomial\n\npen quasi-likelihood gauss-hermite\n\ndiscrete mixture\n\nmixed models\n\n1.345 (0.244)\n\u03b20\n\u03b22 \u22123.087 (0.521)\n\u03b23 \u22123.865 (0.863)\n\u03b24 \u22123.919 (0.684)\n\n1.687 (0.306)\n1.802 (0.362)\n\u22124.130 (0.614) \u22124.515 (0.736)\n\u22125.274 (0.981) \u22125.855 (1.189)\n\u22125.109 (0.747) \u22125.594 (0.919)\n\u02c6\u03c3 = 1.533\n\n\u02c6\u03c3 = 1.456\n\n-\n\u22124.309(0.481)\n\u22125.509(0.824)\n\u22125.082(0.595)\n\n "}, {"Page_number": 150, "text": "138\n\nchapter 5. alternative binary regression models\n\n5.4 conditional likelihood\nin some applications the model involves parameters that are of minor interest to the investigator.\nthese parameters are often called nuisance or incidental parameters. for example, in a clinical\ntrial that compares the effect of a treatment (new drug or new therapy) with a current standard,\nlet the response be \u2019success\u2019 or \u2019failure\u2019. suppose that data on the effect of treatment are\navailable from g sources or strata.\nin a multi-center clinical trial the strata are the medical\ncenters in which the trials are taken. one obtains for each stratum a 2 \u00d7 2 table, the table for\nthe ith stratum has the form\n\ntreatment\ncontrol\n\nsuccess\n\nyi1\nyi2\nyi\u00b7\n\nfailure\nni1 \u2212 yi1\nni2 \u2212 yi2\nni \u2212 yi\u00b7\n\nni1\nni2\nni\n\nproportions\npi1 = yi1/ni1\npi2 = yi2/ni2\n\nthe main effect logit model, which models the effect of the stratum i and the treatment on\n\nthe binary response (y = 1 for success and y = 0 for failure), has the form\n\nlog( \u03c0(i, xt )\n1 \u2212 \u03c0(i, xt )\n\n) = \u03b2i + xt \u03b2,\n\n(5.6)\n\nwhere \u03c0(i, xt ) is the probability of success given the stratum i and the treatment group speci-\nfied by xt (xt = 1 for treatment and xt = 0 for control). the treatment effect is given by \u03b2\nwhile \u03b2i represents the stratum effect.\n\nthe problem with the linear logistic model is that it contains g parameters, which have to\nbe estimated on the basis of 2g observed binomial proportions. thus for large g maximum\nlikelihood estimates will hardly be efficient. moreover, the parameter of interest is \u03b2, and the\nparameters \u03b21, . . . , \u03b2g\u22121 are nuisance parameters. when testing the hypothesis \"no treatment\neffect\" (h0 : \u03b2 = 0) one alternative is to condition on the success totals yi1 + yi2 = yi\u00b7,\nobtaining a hypergeometric distribution that does not depend on the nuisance parameters.\n\nthe hypergeometric distribution\nsuppose that independently two random samples of size n1, n2 are drawn and it is observed\nwhether attribute a occurs or not. one obtains a 2 \u00d7 2 table with fixed marginals n1, n2. the\nfollowing table gives numbers of subjects who possess the attribute in question.\n\npopulation 1\npopulation 2\n\na\n\ny = y11\n\ny21\nna\n\n\u00afa\ny12\ny22\nn \u00afa\n\nn1\nn2\nn\n\nfrom the marginals, n1, n2 and therefore n = n1 + n2 are fixed, while na and n \u00afa are random.\nif also the marginals na, n \u00afa are fixed, the observations in the table no longer follow a binomial\ndistribution. the selection is the same as when drawing n1 marbles from a box which contains\nn marbles, na of them possessing attribute a. thus the distribution of y = y11 is given by the\nhypergeometric distribution\n\np (y|n, na) =\n\nna\ny\n\n(cid:25)\n\n(cid:26)\n\n(cid:25)\n\nn1\ny\n\n=\n\n(cid:26)\nn \u00afa\nn1 \u2212 y\n\n(cid:26)(cid:25)\n(cid:25)\n\nn\nn1\n\n(cid:26)\n\n(cid:26)(cid:25)\n(cid:25)\n\nn\nna\n\n(cid:26)\nn2\nna \u2212 y\n\n,\n\n(5.7)\n\n "}, {"Page_number": 151, "text": "5.4. conditional likelihood\n\n139\nwhere the range of possible values for y is given by the integers, satisfying l = max{0, n1 \u2212\n(n \u2212 na)} \u2264 y \u2264 min{n1, na} = u. the notation p (y|n, na) shows that the distribution\na = (na, n \u00afa) and nt = (n1, n2). the second\nof y is conditionally on the marginal counts nt\nform in (5.7) results from the symmetry of the problem. distribution (5.7) is abbreviated by\nh(n, na). the hypergeometric distribution may also be derived directly from the independent\nbinomial distributions y11 \u223c b(n1, \u03c0), y21 \u223c b(n2, \u03c0). then the conditional distribution of\ny = y11 conditionally on y11 + y21 = na is given by (5.7).\n\nthe non-central hypergeometric distribution\nin the more general case the response probability will be different for the two populations. by\nassuming independent binomial distributions\n\none obtains for the conditional distribution of y = y11, conditionally on y11 + y21 = na, the\nnon-central hypergeometric distribution h(n, na, \u03b3), given by\n\ny11 \u223c b(n1, \u03c01), y21 \u223c b(n2, \u03c02),\n(cid:26)\n\n(cid:26)(cid:25)\n\n(cid:25)\n\nwhere po(\u03b3) is the polynomial in \u03b3,\n\np (y|n, na, \u03b3) =\n(cid:25)\nu(cid:7)\n\npo(\u03b3) =\n\nn1\nj\n\nj=l\n\n\u03b3y\n\n,\n\nn1\ny\n\n(cid:26)(cid:25)\n\nn2\nna \u2212 y\npo(\u03b3)\n(cid:26)\n\n\u03b3j,\n\nn2\nna \u2212 j\n(cid:26)(cid:25)\n\n(cid:29)(cid:25)\n\nn1\ny\n\nn2\nna \u2212 y\n\n(cid:26)(cid:30)\n\nand \u03b3 = {\u03c01/(1 \u2212 \u03c01)}/{\u03c02/(1 \u2212 \u03c02)} is the odds ratio. the non-central hypergeometric\ndistribution follows a simple exponential family with the (conditional) log likelihood given by\n\ny log(\u03b3) \u2212 log(po(\u03b3)) + log\n\nthis has the form y\u03b8 \u2212 b(\u03b8) + c(y) with \u03b8 = log(\u03b3) and b(\u03b8) = log(po(e\u03b8)), yielding\n\n(cid:3)\ne(y) = b\n(\u03b8) = p1(e\u03b8)/po(e\u03b8),\n(\u03b8) = p2(e\u03b8)/po(e\u03b8) \u2212 {p1(e\u03b8)/po(e\u03b8)}2,\n(cid:3)(cid:3)\nvar(y) = b\n\nwhere p1(e\u03b8) = \u2202po(\u03b8)/\u2202\u03b8, p2(e\u03b8) = \u22022po(\u03b8)/\u2202\u03b82 given by\n\n(cid:26)(cid:25)\n\n(cid:26)\n\npr(\u03b3) =\n\nn1\nj\n\nn2\nna \u2212 j\n\njr\u03b3j\n\n(cid:25)\n\nn(cid:7)\n\nj=l\n\n(see mccullagh and nelder, 1989).\n\nwhen testing the nullhypothesis h0 : \u03b2 = 0 in model (5.6) conditioning on the success\n\ntotals yi1 + yi2 = yi\u00b7 for each stratum yields the hypergeometric distribution\n\nyi1|yi\u00b7 \u223c h(ni\u00b7, yi\u00b7, e\u03b2),\n\ni\u00b7 = (ni1, ni2), yt\n\ni\u00b7 = (yi1 + yi2, ni \u2212 yi1 \u2212 yi2) are the marginals under h0. then the\nwhere nt\nconditional likelihood involves only one parameter, e\u03b2. one obtains for the mean and variance\nof the hypergeometric distribution\nyi\u00b7\nni\n\n(cid:25)\n1 \u2212 yi\u00b7\nni\n\nni \u2212 ni1\nni \u2212 1 .\n\nvar(yi1) = ni1\n\ne(yi1) = ni1\n\nyi\u00b7\nni\n\n(cid:26)\n\n(5.8)\n\n,\n\n "}, {"Page_number": 152, "text": "140\n\nchapter 5. alternative binary regression models\n\na statistic for the nullhypothesis may be based on the difference between observed counts and\nthe expected values of the nullhypothesis. a test statistic that has been proposed by mantel and\nhaenszel (1959), and in a similar form by cochran (1954), is\n(cid:14)\ni=1(yi1 \u2212 e(yi1))}2\n\ng\n\nt =\n\n(5.9)\n\ng\n\ni=1 var(yi1)\n\nin the mantel-haenszel statistic a continuity correction is included, yielding\n\n{(cid:14)\n{|(cid:14)\n\ng\n\nm h =\n\u03c72\n\n(cid:14)\ni=1(yi1 \u2212 e(yi1))| \u2212 0.5}2\n\ng\n\ni=1 var(yi1)\n\nwith e(yi1) and var(yi1) derived from the hypergeometric distribution under h0(5.8). by\nusing the observed proportions pi1 = yi1/ni1, pi2 = yi2/ni2 it may be rewritten as\n\n+\n\n|(cid:14)\n\n(cid:14)\n\ng\ni=1\n\nni1ni2\n\n,\n(pi1 \u2212 pi2)| \u2212 0.5\n\n2\n\nni\nni1ni2\n\nni\u22121 \u00afpi(1 \u2212 \u00afpi)\n\nm h =\n\u03c72\n\nwhere \u00afpi = (ni1pi1+ni2pi2)/ni. cochran (1954) proposed the statistic (5.9) with the variances\nreplaced by variances derived from the two binomials for treatment and control. under the\nnullhypothesis the tests have a large-sample \u03c72-distribution. for more details on these tests see\nagresti (2002). a general treatment of conditional likelihood approaches is given in mccullagh\nand nelder (1989). an alternative approach to reduce the number of parameters is to assume\nthat the stratum-specific parameters are random effects (see chapter 14).\n\n5.5 further reading\nsingle-index model. several approaches to the estimation of single-index models have been\nproposed. one popular technique is based on an average derivative estimation, which exploits\nthe fact that the average gradient of h(x(cid:3)\ni\u03b2\u03b2\u03b2) is proportional to \u03b2\u03b2\u03b2 (see powell et al., 1989; hris-\ntache et al., 2001). an m-estimation that considers the unknown link function as an infinite-\ndimensional nuisance parameter was considered by klein and spady (1993) and h\u00e4rdle et al.\n(1993). weisberg and welsh (1994) proposed an algorithm that alternates between the estima-\ntion of \u03b2\u03b2\u03b2 and h(.). yu and ruppert (2002) suggested using penalized regression splines. they\nreported more stable estimates compared to earlier approaches based on local regression (e.g.,\ncarroll et al., 1997). xia et al. (2002) proposed a symbiosis of sliced inverse regression aver-\nage derivative estimation and local linear smoothing. naik and tsai (2001) proposed a model\nselection criterion for single-index models that selects variables and also smoothing parameters\nfor the unknown link function.\n\nr packages. binary response models including quasi-likelihood models can be fitted with\nthe function glm. various link function can be specified in the family function. generalized\nestimation functions are available in the library gee, function gee. the beta-binomial model\ncan be fitted by the function vglmin the library vgam. for the fitting procedures of mixed and\nfinite mixture models see chapter 14.\n\n5.6 exercises\n\n5.1 the dataset dust is available from package catdata (or at http://www.stat.uni-muenchen.de/sfb386/\nunder the name \"chronic bronchitis and dust concentration\").\n\n "}, {"Page_number": 153, "text": "5.6. exercises\n\n141\n\n(a) fit models with different link functions for non-smokers and smokers separately.\n(b) compare the fitted models and discuss model selection.\n\n5.2 the dataset birth is available from package catdata. consider the binary response \"did a perineal tear\noccur\" and explanatory variables weight, height, head circumference of child, and month of birth.\n\n(a) fit models with different link functions.\n(b) select an appropriate model.\n\n5.3 the package flexmix contains the data set betablocker, which is from a 22-center clinical trial of beta-\nblockers for reducing mortality after myocardial infarction (see also aitkin, 1999). in addition to centers,\nthere is only one explanatory variable, treatment, coded as 0 for control and 1 for beta-blocker treatment.\n(a) for the 44 binomial observations, fit the simple logit model, an appropriate quasi-likelihood model,\n\nand by using generalized estimation equations. compare the effects and standard errors.\n\n(b) test if the treatment by beta-blockers has an effect by using test statistics based on conditional\n\nlikelihood.\n\n "}, {"Page_number": 154, "text": " "}, {"Page_number": 155, "text": "chapter 6\n\nregularization and variable selection for\nparametric models\n\nin several chapters we discussed parametric regression modeling for a moderate number of\nexplanatory variables based on maximum likelihood methods. in some areas of application,\nhowever, the number of explanatory variables may be very high. for example, in genetics,\nwhere binary regression is a frequently used tool, the number of predictors may be even larger\nthan the number of predictors.\nin this \"p > n problem\" maximum likelihood and similar\nestimators are bound to fail. typical data of this type are microarray data, where the expressions\nof thousands of predictors (genes) are observed and only some hundred samples are available.\nfor example, the dataset considered by golub et al. (1999a), which constitutes a milestone\nin the classification of cancer, consists of gene expression intensities for 7129 genes of 38\nleukemia patients, from which 27 were diagnosed with acute lymphoblastic leukemia and the\nremaining patients acute myeloid leukemia.\n\nin high-dimensional problems the reduction of the predictor space is the most important\nissue. a reduction technique with a long history is stepwise variable selection. however,\nstepwise variable selection as a discrete process is extremely variable. the results of a variable\nselection procedure may be determined by small changes in the data. the effect is often poor\nperformance (see, e.g., frank and friedman, 1993). moreover, it is challenging to investigate\nthe sampling properties of stepwise variable selection procedures.\n\nan alternative to stepwise subset selection is regularization methods. ridge regression is a\nfamiliar regularization method that adds a simple penalty term to the log-likelihood and thereby\nshrinks estimates toward zero.\nin recent years several alternative regularization techniques\nbased on penalties have been proposed, including methods that perform \"smooth\" variable se-\nlection. these methods select variables simultaneously via optimizing a penalized likelihood,\nand hence allow one to estimate standard errors. in the following we consider several penalty\nmethods as well as boosting techniques, which are ensemble methods but also serve as regular-\nization methods in structured regression.\n\nimportant aspects for regression modeling by regularization techniques are\n\n\u2022 existence of unique estimates \u2013 this is where maximum likelihood estimates often fail;\n\n\u2022 prediction accuracy \u2013 a model should be able to yield a decent prediction of the outcome;\n\n\u2022 sparseness and interpretation \u2013 thea parsimonious model that contains the strongest ef-\n\nfects is easier to interpret than a big model with hardly any structure.\n\n143\n\n "}, {"Page_number": 156, "text": "144 chapter 6. regularization and variable selection for parametric models\n\nwe start with the conventional stepwise selection procedures and then consider regularized\nestimates. most of the methods focus on variable selection, but regularization can also be help-\nful when one wants to know which categories of a categorical predictor should be distinguished.\n\n6.1 classical subset selection\nin subset selection the predictor space is reduced by retaining only a subset of the variables.\nthe main strategies are best subset selection, forward selection, backward selection, and a\ncombination of the latter two methods.\n\nbest subset selection aims at finding the best subset of predictors among all subsets of the\nvariables x1, . . . , xp. \"best\" may be defined by minimizing some criterion like aic or bic.\nfor binary or poisson-distribution models, where estimates have to be computed iteratively, the\nfull subset selection is extremely demanding if the number of variables is large.\n\nforward selection seeks a path through all possible subsets by sequentially adding one\npredictor into the model. the decision for a predictor may be based on test statistics. let m\ndenote the current model and mr the model m with the additional variable xr. then, a test on\nthe significance of variable xr within model mr is given by the difference of the deviances:\n\nd(m|mr) = d(m) \u2212 d(mr).\n\none selects that variable xr0 for which the corresponding p-value pr is minimized, r0 =\narg minr pr, provided that the p-value is below some prechosen inclusion level \u03b1in. the pro-\ncedure aims to select variables rather than single terms or parameters. this means that, in a\nmixture of variables, some of them metric and some of them categorical, the differences of de-\nviances d(m|mr) compare models of different sizes. while a metric covariate typically con-\ntributes only one term (or parameter) to the model, a categorical predictor, unless it is binary,\nwill contribute more than one term (parameter) to the predictor. in cases where each variable\ncorresponds to one term in the linear predictor, for example, in a main effect model with only\nmetric predictors, minimization of the p-values is equivalent to maximizing d(m|mr). then\none implicitly chooses the variable that most improves the fit, since the deviance d(mr0) as\na measure for the discrepancy between data and model is minimized. the procedure stops\nwhen no additional variable contributes significantly to the model m. one should be aware\nthat the significance level \u03b1in is a threshold rather than a significance level. since many tests\nare performed, one has a multiple test problem and control of the multiple significance level is\ndifficult.\n\nalternative test statistics that might be computationally easier to handle are the score test\nand the wald test. fahrmeir and frost (1992) suggested the score test and computed the test\nstatistic by efficient sweeps of the inverse information matrix. since deviances use maximum\nlikelihood for both models m and mr, one might also run into problems with the existence\nof ml estimates. in contrast, the score test uses only the estimates of the restricted model;\ncomputation of the parameter estimates for the larger model is not required.\n\nbackward selection starts with the full model and sequentially deletes predictors. the\nchoice of the variable to delete is again typically based on test statistics. let m denote the\ncurrent model and m(cid:2)r the model m without predictor r. then the deviance\n\nd(m(cid:2)r|m) = d(m(cid:2)r) \u2212 d(m)\n\ntests the significance of variable xr within model m. one selects the variable r0 for which\nthe corresponding p-value pr is maximal provided it is above some pre-chosen exclusive level\n\u03b1out. the procedure stops when each predictor in the model has a p-value below the level \u03b1out.\n\n "}, {"Page_number": 157, "text": "6.2. regularization by penalization\n\n145\n\ncomputationally more efficient procedures may be obtained by using the wald test (fahrmeir\nand frost, 1992). backward selection strategies are restricted to cases where an estimate for the\nfull model exists. this is often a problem when the number of predictors is large.\n\nforward and backward strategies may also be combined. after a new variable has been\ntaken into the model (forward step) one investigates if one of the other variables in the model\nmay now be deleted by performing a backward step. both steps have to be controlled by\ninclusion and exclusion thresholds \u03b1in and \u03b1out, which, however, provide only local control of\nthe model search.\n\nbest subset selection and forward/backward strategies have several disadvantages. as al-\nready noted, subset selection is a discrete process, either a variable is in or out of the model, and\ntherefore extremely variable. the instability of stepwise regression models was demonstrated\nfor example by breiman (1996b). moreover, one should be very cautious with the interpretation\nof the found effects. standard errors computed for the final model are not trustworthy because\nthey simply ignore the model search. taking the model search into account would yield much\nlarger standard errors.\n\nsubset selection has been studied extensively for normal distribution models; see, for exam-\nple, seeber (1977), miller (1989), and furnival and wilson (1974). the latter gave an efficient\nalgorithm that performs best subset selection up to 30 or 40 predictors. lawless and singhal\n(1978, 1987) developed efficient screening and all-subsets procedures for generalized linear\nmodels by use of likelihood ratio statistics. these methods were discussed within a more gen-\neral framework by fahrmeir and frost (1992).\n\n6.2 regularization by penalization\nregularization methods that are derived from maximum likelihood estimates are based on the\npenalized log-likelihood:\n\nn(cid:7)\n\ni=1\n\nlp(\u03b2) =\n\nli(\u03b2) \u2212 \u03bb\n\n2 j(\u03b2),\n\n(cid:14)\n\n\u22121(\u03b2)), where f\n\nwhere li(\u03b2) is the usual log-likelihood contribution of the ith observation, \u03bb is a tuning pa-\nrameter, and j(\u03b2) is a functional that penalizes the size of the parameters. by maximizing the\npenalized log-likelihood lp(\u03b2) one seeks estimates that are close to usual ml estimate but with\nregularized parameters. for example, the ridge penalty, which is one of the oldest penalization\np\nmethods, uses the penalty j(\u03b2) =\nj=1 \u03b22\nj . it penalizes the length of the parameter \u03b2 and\nyields estimates that are shrunk toward zero.\nthere is a good reason for penalizing the length of the parameter. segerstedt (1992) showed\nthat under regularity assumptions the mean of the squared length of the ml estimate, e((cid:16)\u02c6\u03b2(cid:16)2),\nis asymptotically (cid:16)\u03b2(cid:16)2 + tr(f\n\u22121(\u03b2) denotes the fisher matrix at the true\nvalue \u03b2, which is an approximation to the covariance of \u02c6\u03b2. therefore, most common regular-\nization techniques impose a penalty on the size of the regression coefficients, yielding shrunk\nestimates, which in particular have reduced variance. shrinkage methods are in particular use-\nful for obtaining estimates in applications where the use of the ml estimator involves problems.\nin a simple gaussian linear regression the ml estimate is obtained by solving the estimation\nequation (x t x)\u03b2 = x t y, which is easily solved if x t x is of full rank and the inverse\n(x t x)\u22121 exists. in the case of collinearity, x t x is not of full rank, the ml estimate is not\nunique, and one has to determine which parts of \u03b2 still may be estimated. moreover, it has been\nshown that collinearity leads to poor performance of the estimators. figure 6.1 illustrates the\ninstability of ml estimates for strongly correlated data. it shows the estimates for two datasets\nthat were drawn from the same underlying linear structure (same non-zero coefficients, \u03b2j = 5,\n\n "}, {"Page_number": 158, "text": "146 chapter 6. regularization and variable selection for parametric models\n\nfor the first six variables, zero coefficients for the other variables). it is seen that estimates\ntake quite extreme values, since for strongly correlated predictors high positive values of esti-\nmated coefficients balance negative values. the effect is a high variability of estimates. for\neach drawing one obtains quite different estimates that are far from the true values. shrinkage\nestimators like ridge regression estimators, however, yield much smaller values (open circles\nin figure 6.1), which are distinctly closer to the true values. shrinkage methods become im-\nportant especially when many predictors are available and therefore the corresponding design\nmatrix x is very large and usually contains redundant columns. similar effects occur in a bi-\nnary regression, where an additional problem occurs, since ml estimates do not exist if the data\nmay be separated (see section 4.1). when many predictors are available, the tendency that data\nstructures occur in which the responses y = 0 and y = 1 are separated increases strongly.\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\nindex\n\n0\n3\n\n0\n2\n\n0\n1\n\n0\n\n0\n2\n\u2212\n\n0\n3\n\n0\n2\n\n0\n1\n\n0\n\n0\n1\n\u2212\n\ns\nt\nn\ne\nc\ni\nf\nf\n\ni\n\ne\no\nc\n \n\nd\ne\n\nt\n\na\nm\n\ni\nt\ns\ne\n\ns\nt\nn\ne\nc\ni\nf\nf\n\ni\n\ne\no\nc\n \n\nd\ne\n\nt\n\na\nm\n\ni\nt\ns\ne\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\nindex\n\nfigure 6.1: maximum likelihood estimates for two datasets with correlated data; the first\nsix variables are influential, and all other variables have zero coefficient.\n\nsome shrinkage methods may also be seen as a continuous alternative to the selection of\npredictors. since variables are retained or discarded, variable selection is a strictly discrete\nprocess that often exhibits high variance in prediction error. shrinkage methods reduce the\ninfluence of variables in a much smoother way and therefore show less variability. although\nshrinkage methods may be seen as providing alternative (more stable) methods to estimate the\n\"true\" regression coefficients, a more pragmatic view is that shrinkage methods yield regression\nmodels, true or not, that in particular in high-dimensional problems show better prediction error\nthan usual maximum likelihood estimates. therefore, the selection of the tuning parameter is\noften based on an estimate of the prediction error.\n\n "}, {"Page_number": 159, "text": "6.2. regularization by penalization\n\n147\n\nin the following let the predictor be given by \u03b7i = xt\n\ni \u03b2, where \u03b2t = (\u03b20, \u03b21, . . . , \u03b2p) and\nxi contains a constant term. frequently used penalties are of the bridge penalty type (frank\nand friedman, 1993):\n\nj(\u03b2) =\n\n|\u03b2j|\u03b3,\n\n\u03b3 > 0.\n\n(6.1)\n\np(cid:7)\n\nj=1\n\nfor \u03b3 = 2 one obtains a ridge regression (hoerl and kennard, 1970) for \u03b3 = 1, the so-called\nlasso (tibshirani, 1996 ). alternatively, estimates are found by maximizing the log-likelihood\n\nn(cid:7)\n\ni=1\n\nl(\u03b2) =\n\nli(\u03b2)\n\n|\u03b2j|\u03b3 \u2264 t.\n\nsubject to the constraint\n\np(cid:7)\n\n(6.2)\nfor \u03b3 \u2265 1, this approach is equivalent to maximizing the penalized likelihood with \u03bb \u2265 0 since\nthe constraint area is convex (fu, 1998). maximization of lp is usually referred to as a penalized\nregression whereas maximization of l subject to (6.2) is called a constrained regression. in the\nfollowing we will consider penalties of the bridge penalty type and others.\n\nj=1\n\n6.2.1 ridge regression\nridge regression as introduced by hoerl and kennard (1970) for linear models and extended\nto glm type models by nyquist (1991) is based on the penalty j(\u03b2) =\nj yielding the\npenalized log-likelihood\n\np\nj=1 \u03b22\n\n(cid:14)\n\nfor deriving estimates it is useful to rewrite the penalty in the form\n\nlp(\u03b2) =\n\np(cid:7)\n\nj=1\n\n\u03b22\nj .\n\np(cid:7)\n\ni=1\n\nli(\u03b2) \u2212 \u03bb\n2\np(cid:7)\n\nj(\u03b2) =\n\nj = \u03b2t p \u03b2,\n\u03b22\n\nj=1\n\nwhere p = (pij) differs from the (p + 1) \u00d7 (p + 1) identity matrix only by having p11 = 0\ninstead of p11 = 1. the corresponding penalized score function sp(\u03b2) is given by\n\nn(cid:7)\n\nsp(\u03b2) =\n\nxi\n\n\u2202h(\u03b7i)\n\n\u2202\u03b7\n\n(yi \u2212 \u03bci)/\u03c32\n\ni\n\n\u2212 \u03bbp \u03b2,\n\nyielding the estimation equation\n\ni=1\n\nx t d(\u03b2)\u03c3\n\n\u22121(\u03b2)(y \u2212 \u03bc) \u2212 \u03bbp \u03b2 = 0,\n\nwhere yt = (y1, . . . , yn), \u03bct = (\u03bc1, . . . , \u03bcn), x t = (x1 . . . xn), d(\u03b2) = diag(\u2202h(\u03b71)/\u2202\u03b7,\n. . . \u2202h(\u03b7r)/\u2202\u03b7), \u03c3(\u03b2) = diag(\u03c32\ni = var(yi). for the normal distribution model\none obtains with \u03c32\n\ni = \u03c32, d = i, \u02dc\u03bb = \u03bb\u03c32 the explicit solution\n\n1, . . . , \u03c32\n\nn), \u03c32\n\n\u02c6\u03b2 = (x t x + \u02dc\u03bbp )\n\n\u22121x t y,\n\nwhich is the maximum likelihood estimate except for the term \u02dc\u03bbp . for the covariance one\nobtains cov(\u02c6\u03b2) = \u03c32(x t x + \u03bb\u03c32p )\u22121x t x(x t x + \u03bb\u03c32p )\u22121.\n\n "}, {"Page_number": 160, "text": "148 chapter 6. regularization and variable selection for parametric models\n\nfor generalized linear models iterative procedures, for example, fisher scoring, have to be\n\nused. fisher scoring for solving sp(\u02c6\u03b2) = 0 has the form\n\n\u02c6\u03b2\n\n(k+1) = \u02c6\u03b2\n\n(k) + f p(\u02c6\u03b2\n\n(k))\n\n\u22121sp(\u02c6\u03b2\n\n(k)),\n\n(cid:14)\n\nwhere f p(\u03b2) = e(\u2212\u2202lp/\u2202\u03b2\u2202\u03b2t ) = f (\u03b2) + \u03bbp with f (\u03b2) being the usual fisher matrix\nand w (\u03b2) = diag((\u2202h(\u02c6\u03b71)/\u2202\u03b7)2/\u03c32\n1, . . . )). by adding \u03bb to the diagonal of f (\u03b2), the matrix\nf p(\u03b2) becomes invertible even if f (\u03b2) is not.\n\nthe penalty term (\u03bb/2)\n\nj contains only one tuning parameter \u03bb, which determines\nthe amount of shrinkage for all \u03b2j. since the parameter \u03b2j depends on the scaling of the\ncorresponding covariate xj, solutions of sp(\u03b2) = 0 are not equivariant under scaling of the co-\nvariates. therefore, usually covariates are standardized before solving the estimation equation.\nridge estimates have nice properties. under weak conditions that hold for generalized\nlinear models, estimates exist and are unique for \u03bb > 0 (fu, 1998). for small \u03bb, the resulting\nestimate will be mildly biased and the covariance may be approximated by\n\nj \u03b22\n\n\u02c6cov(\u02c6\u03b2) = (x t w (\u02c6\u03b2)x + \u03bbp )\n\n\u22121(x t w (\u02c6\u03b2)x)(x t w (\u02c6\u03b2)x + \u03bbp )\n\n\u22121.\n\nearly attempts to generalized ridge regression were restricted to logistic regression; see ander-\nson and blair (1982), schaefer et al. (1984), and duffy and santner (1989). ridge regression\nin generalized linear models has been investigated by nyquist (1991), segerstedt (1992), and\nlecessie (1992).\n\nridge, lasso and \n\n elastic net penalties\n\n2\na\n\nt\n\ne\nb\n\n0\n1\n\n.\n\n5\n0\n\n.\n\n0\n0\n\n.\n\n.\n\n5\n0\n\u2212\n\n0\n.\n1\n\u2212\n\n\u22121.0\n\n\u22120.5\n\n0.0\n\n0.5\n\n1.0\n\nbeta1\n\nfigure 6.2: constraint regions for the ridge penalty (circle), the lasso (diamond), and the\nelastic net (in between) for a two-dimensional predictor space.\n\nexample 6.1: heart disease\nthe selection of variables by regularization will be illustrated by the use of the heart disease data that are\navailable from the r package glmpath (see park and hastie, 2007). the data contain 462 observations on\n9 variables and the binary response coronary heart disease. the explanatory variables are sbp (systolic\nblood pressure), tobacco (cumulative tobacco), ldl (low density lipoprotein cholesterol), adiposity, famhist\n(family history of heart disease), typea (type a behavior), obesity, alcohol (current alcohol consumption),\nand age (age at onset). figure 6.3 shows the coefficient buildups for the ridge estimate based on 10-fold\n\n "}, {"Page_number": 161, "text": "6.2. regularization by penalization\n\n149\n\ncross-validation (standardized explanatory variables, package lqa). parameter estimates are not plotted\nagainst \u03bb but against (cid:9)\u03b2(cid:9)/ max(cid:9)\u03b2(cid:9), where max(cid:9)\u03b2(cid:9) denotes the maximum value that (cid:9)\u03b2(cid:9) can take.\ntherefore, small values of (cid:9)\u03b2(cid:9) correspond to large values of \u03bb and large values of (cid:9)\u03b2(cid:9) to small values of\n\u03bb. for \u03bb = 0 one obtains the ml estimates on the right-hand side. it is seen that, depending on the value\nof the smoothing parameter, the estimates are shrunk toward zero.\n\nridge\n\n0\n1\n\n5\n\n0\n\n5\n\u2212\n\nage\n\nfamhist\ntypea\nldltobacco\n\nsbpadiposity\n\nalcohol\n\nobesity\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\nfigure 6.3: coefficient paths for heart disease data when using ridge (package lqa; ver-\ntical line shows estimate selected by 10-fold cross-validation).\n\n6.2.2 l1-penalty: the lasso\nridge regression often achieves better prediction performance than maximum likelihood based\nregression. however, ridge regression does not produce a parsimonious model, since all vari-\nables are retained. with a large number of predictors one often wants to determine a smaller\nsubset that contains the strongest variables. tibshirani (1996) proposed a new technique, called\nthe lasso, for \"least absolute shrinkage and selection operator\", that shrinks some coefficients\nand sets others to 0. it tends to avoid the high variability of subset selection while producing a\nsparse model that shows good prediction performance. the lasso uses the l1-penalty\n\nj(\u03b2) =\n\n|\u03b2j|,\n\n(6.3)\n\np(cid:7)\n\nj=1\n\nwhich is a member of the bridge penalty family. in signal regression the l1-penalization ap-\nproach has also been called basis pursuit (chen et al., 2001). it was originally proposed for\nthe linear model in the constrained regression version, which means that the log-likelihood is\nmaximized subject to the constraint\n\np(cid:7)\n\n|\u03b2j| \u2264 t\n\nj=1\n\nfor some t. the constrained regression version is helpful for illustrating why lasso often pro-\nduces coefficients that are exactly zero.\n\nfigure 6.2 shows the constraint regions for the ridge penalty, the lasso, and the elastic net\n(the latter will be introduced in the next section). the constraint region for the ridge penalty is\n\n "}, {"Page_number": 162, "text": "150 chapter 6. regularization and variable selection for parametric models\n\nfigure 6.4: constraint regions for the ridge penalty (circle), the lasso (diamond), and the\nelastic net (in between) together with the log-likelihood functions for negatively correlated\npredictors (left) and positively correlated predictors (right) for binary regression models.\n\n2\n\n1\n\n2\n\n1 + \u03b22\n\n| + |\u03b22\n\n\u2264 t; for the lasso one obtains the diamond |\u03b22\n\n| \u2264 t. figure 6.4 shows\nthe disk \u03b22\nthe constraint regions for the ridge penalty and the lasso penalty in the two-dimensional case\ntogether with the contours of the likelihood function for a binary regression model. the left\npicture shows the contours of the log-likelihood function for a binary logit model with n = 30\nand negatively correlated predictors (\u0001 = \u22120.87), and the picture on the right shows the log-\nlikelihood for n = 30 and positively correlated predictors (\u0001 = 0.87). the point within the\ncontours is the ml estimate for the specific dataset, and the penalized estimate is represented by\nthe point where the contours touch the penalty region. maximization of the likelihood subject\nto the constraint region yields an estimate that is closer to zero than the ml estimate. of course,\nthe amount of shrinkage depends on the size of the constraint region, which is determined by\nt or, equivalently, by \u03bb. the advantage of the lasso over ridge regression is that the constraint\nregion is not smooth. since the diamond has distinct corners, if a solution occurs at a corner,\nthen one parameter is set to zero. the same happens in higher dimensions, but the constraint\nregions are harder to visualize. for three dimensions (see figure 6.5), if the penalty region\ntouches at a corner, two parameters are set to zero; if it touches at a connection between two\ncorners, one parameter is set to zero.\n\nfigure 6.5: constraint regions for the lasso (diamond) for a three-dimensional predictor\nspace.\n\n "}, {"Page_number": 163, "text": "6.2. regularization by penalization\n\n151\n\nin contrast to linear regression, in binary regression the contours for a finite sample size\nare only approximately elliptical. an elliptical approximation is obtained by using a second-\norder taylor approximation of the log-likelihood at the maximum likelihood estimate \u02c6\u03b2m l,\nobtaining\n\nl(\u03b2) \u2248 l(\u02c6\u03b2m l) +\n\n1\n2\n\n(\u03b2 \u2212 \u02c6\u03b2m l)t f (\u02c6\u03b2m l)(\u03b2 \u2212 \u02c6\u03b2m l),\n\nwhere f (\u02c6\u03b2m l) is the fisher matrix. for normal regression models the approximation is exact\nand one obtains (apart from constants)\nl(\u03b2) = l(\u02c6\u03b2m l) \u2212 1\n\n2\u03c32 (\u03b2 \u2212 \u02c6\u03b2m l)t x t x(\u03b2 \u2212 \u02c6\u03b2m l),\n\nand therefore a quadratic function centered at \u02c6\u03b2m l. for a large sample size, the contours of\nthe likelihood also show an almost elliptical form for binary models.\n\nby using the l1-penalty (6.3) the lasso does both, continuous shrinkage and automatic\nvariable selection, simultaneously. concerning prediction error, it has been shown that the\nperformance is not better than ridge regression in any case. in a comparison of the lasso, ridge,\nand bridge regression it has been shown that neither of them uniformly dominates the other two\n(see tibshirani, 1996; fu, 1998). the big advantage of the lasso is its sparse representation,\nwhich makes it attractive for practitioners.\n\nthe implicit shrinkage of the lasso can be illustrated by the idealized case of orthonormal\ncolumns in the design matrix of a linear model. then the lasso penalty \u02dc\u03bbj(\u03b2) yields the soft\nthresholding rule\n\n\u02c6\u03b2j = s(\u03b2m l\n\nj\n\n, \u02dc\u03bb) = sign( \u02c6\u03b2m l\n\nj\n\n)(| \u02c6\u03b2m l\n\nj\n\n| \u2212 \u02dc\u03bb)+,\n\nj\n\ndenotes the ml estimate of the jth component, \u02dc\u03bb = \u03bb\u03c32, and (z)+ = 1, if z \u2265 0,\nwhere \u03b2m l\n(z)+ = 0, if z < 0. figure 6.6 shows the estimated values as functions of the ml estimate. it\nis seen that estimates are set to zero if the ml estimate is below some threshold and are shrunk\nif the ml estimate is above the threshold. the term \"soft thresholding\" was built to distinguish\nit from hard thresholding, where estimates are set to zero if the ml estimate is below some\nthreshold and retained if the ml estimate is above the threshold (also given in figure 6.6).\nmoreover, estimates for scad are included, which will be considered in section 6.2.5.\n\nfor the lasso in linear models, various computational procedures were proposed. tibshirani\n(1996) used a combined quadratic programming method, fu (1998) gave a modified newton-\nraphson algorithm and introduced the shooting algorithm, and osborne et al. (2000) considered\nthe lasso and its dual. fan and li (2001) proposed an alternative algorithm based on quadratic\napproximations. a fast implementation for large-scale logistic regression with the lasso has\nbeen presented by genkin et al. (2004). alternative approaches aim at the estimation of the\nentire path of the coefficient estimates as \u03bb varies, to find estimates \u02c6\u03b2(\u03bb), 0 < \u03bb < \u221e. efron\net al. (2004) proposed lars, which determines the exact piecewise linear coefficient paths for\nlasso in the linear case. however, in the case of glms, the paths are not piecewise linear.\npark and hastie (2007) proposed an efficient path algorithm for generalized linear models\nthat uses the predictor-corrector method. rosset (2004) suggested a general path-following\nalgorithm that can be used for any loss function. zhao and yu (2004) proposed a boosted lasso,\nwhich includes backward steps. an extremely fast algorithm based on pathwise coordinate\noptimization was given by friedman et al. (2007), friedman et al. (2010).\nit uses coordinate\ndescent methods, which have been proposed earlier (for example fu, 1998) but were not fully\nappreciated at the time.\n\n "}, {"Page_number": 164, "text": "152 chapter 6. regularization and variable selection for parametric models\n\nlasso\n\nhard\n\n4\n\n2\n\n0\n\n2\n\u2212\n\n4\n\u2212\n\n4\n\n2\n\n0\n\n2\n\u2212\n\n4\n\u2212\n\n\u22124\n\n\u22122\n\n0\n\u03b2\nml\nj\n\n2\n\n4\n\n\u22124\n\n\u22122\n\n2\n\n4\n\n0\n\u03b2\nml\nj\n\nscad\n\n4\n\n2\n\n0\n\n2\n\u2212\n\n4\n\u2212\n\n\u22124\n\n\u22122\n\n2\n\n4\n\n0\n\u03b2\nml\nj\n\nfigure 6.6: estimates for lasso, hard thresholding, and scad when columns in the\ndesign matrix are orthonormal.\n\n1 , \u03b2t\n\nthe adaptive lasso\nvariable selection procedures are often discussed in terms of oracle properties, which refer to\nthe identification of the right subset model. for parameter vector \u03b2 let a = {j : \u03b2j (cid:8)= 0}\ndenote the active set, where |a| = p0 < p. for simplicity, let \u03b2 be partitioned into \u03b2t =\n2 ), where \u03b21 represents the active set and \u03b22 = 0. then oracle properties means that (1)\n(\u03b2t\n(cid:8)= 0 and \u02c6\u03b22 = 0, and (2) the optimal\nestimates \u02c6\u03b2\nestimation rate is obtained, so that the estimator performs as well as if the underlying model\nwere known. a selection procedure is consistent if asymptotically the right subset model is\nfound, limn p (an = a) = 1, where an is the active set for n observations. zou (2006) showed\nthat lasso variable selection can be inconsistent and gave necessary conditions for consistency.\nhe proposed an extended version of lasso, for which the penalty has the form\n\n2 ) must asymptotically satisfy \u02c6\u03b21\n\nt = (\u02c6\u03b2\n\n1 , \u02c6\u03b2\n\nt\n\nt\n\nj(\u03b2) =\n\nwj|\u03b2j|,\n\n(6.4)\n\nwhere wj are known weights. by using weights on coefficients the variables are not equally\npenalized, which adds some flexibility. he showed that for cleverly chosen data-dependent\n\nj=1\n\np(cid:7)\n\n "}, {"Page_number": 165, "text": "6.2. regularization by penalization\n\n153\n\nweights the adaptive lasso has oracle properties. one choice of weights is based on a root-n\nconsistent estimator \u02dc\u03b2 of \u03b2, for example, the ml estimate. then weights are fixed by wj =\n1/| \u02dc\u03b2j|\u03b3, for fixed chosen \u03b3 > 0. the oracle properties that zou (2006) derived for the adaptive\nlasso use that for growing sample size the weights for zero-coefficients get inflated, whereas the\nweights on non-zero-coefficients converge to a finite constant. moreover, zou (2006) showed\nthat the adaptive lasso leads to near-minimax-optimal estimators.\n\nexample 6.2: heart disease\nfigure 6.7 shows the coefficient buildups for the lasso and the adaptive lasso (standardized explanatory\nvariables; package lqa) plotted against (cid:9)\u03b2(cid:9)/ max(cid:9)\u03b2(cid:9). the vertical line shows the regularization obtained\nwhen using 10-fold cross-validation. it is seen that, in contrast to the ridge, not all variables are found to\nbe influential. based on 10-fold cross-validation one concludes that the variables alcohol and adiposity\ncan be omitted. moreover, it is seen that for this dataset the adaptive lasso is very close to the simple\nlasso.\n\nlasso\n\nadaptive lasso\n\n5\n1\n\n0\n1\n\n5\n\n0\n\n5\n\u2212\n\nage\n\nfamhist\ntypea\nldltobacco\n\nsbpadiposity\n\nalcohol\n\nobesity\n\n5\n1\n\n0\n1\n\n5\n\n0\n\n5\n\u2212\n\nage\n\nfamhist\ntypea\nldltobacco\n\nsbpadiposity\n\nalcohol\n\nobesity\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\nfigure 6.7: lasso coefficient paths for heart disease data (package lqa; vertical line\nshows estimate selected by 10-fold cross-validation).\n\ncategorical predictors and the group lasso\nthe lasso as considered in the previous section selects individual predictors. that approach is\nsensible when all variables are of the same type, for example, if all the variables are continuous,\nor all are binary. for a mixture of predictors, some of them categorical and some of them binary,\nthe penalty is unsatisfactory.\nif the categorical predictor (factor) is represented by dummy\nvariables, the lasso penalty selects individual dummy variables instead of whole factors, and\nthe solution depends on how the dummy variables are encoded. a sensible procedure should\nselect whole factors or continuous variables. the group lasso proposed by yuan and lin (2006)\ncan overcome these problems.\n\nlet the p-dimensional predictor be structured as xt\n\ni,g), where xij corre-\nsponds to the jth group of variables. a group of variables may refer to the dummy variables of\none factor, with dfj denoting the number of the variables in the jth group. a continuous variable\nthat has a linear form within the predictor obviously has dfj = 1. a group of variables may also\nrefer to interactions between factors or between factors and continuous variables, where dfj is\n\ni1, . . . , xt\n\ni = (xt\n\n "}, {"Page_number": 166, "text": "154 chapter 6. regularization and variable selection for parametric models\n\nthe number of individual interaction terms. correspondingly the parameter vector is partitioned\ninto subvectors, \u03b2t = (\u03b2t\n\ng). the group lasso uses the penalty\n\n1 , . . . , \u03b2t\n\ng(cid:7)\n\n(cid:16)\n\nj(\u03b2) =\n\ndfj(cid:16)\u03b2j\n\n(cid:16)2,\n\nj=1\n\nj,dfj\n\n(cid:16)2 = (\u03b22\n\nj1 + \u00b7\u00b7\u00b7 + \u03b22\n\nwhere (cid:16)\u03b2j\n)1/2 is the l2-norm of the parameters of the jth group. the\npenalty encourages sparsity in the sense that either \u02c6\u03b2j = 0 or \u03b2js (cid:8)= 0 for s = 1, . . . , dfj. for a\ngeometrical interpretation of the penalty, see yuan and lin (2006). meier et al. (2008) showed\nthat under sparsity the resulting estimates are consistent even when the number of predictors is\nlarger than the sample size. the penalty may be seen as a special case of the composite absolute\npenalty family proposed by zhao et al. (2009).\n\n6.2.3 the elastic net\nalthough the lasso has several advantages, it has some severe limitations, pointed out by zou\nand hastie (2005).\nif there are high correlations between predictors, it has been observed\nthat ridge regression dominates the lasso (tibshirani, 1996). as a variable selection method\nit is restricted to n variables. in the p > n case, the lasso selects at most n variables before\nit saturates. moreover, the lasso does not necessarily have a unique solution, since the penalty\nterm is not strictly convex. a point of special interest concerns the variables that are selected by\nthe lasso. if there is a group of variables among which the correlations are very high, the lasso\ntends to select only one of the variables as a representative. in particular, this way of selecting\nvariables is different from the elastic net, proposed by zou and hastie (2005). the elastic net\ndoes automatic variables selection, and, rather than selecting one representative, it can select\ngroups of correlated variables. according to zou and hastie it works \"like a stretchable fishing\nnet that retains all the big fish.\" the elastic net uses the elastic net criterion\n\np(cid:7)\n\np(cid:7)\n\nj(\u03b2) = \u03bb1\n\n|\u03b2j| + \u03bb2\n\n\u03b22\nj ,\n\n(6.5)\n\nj=1\n\nj=1\n\nj\n\nj \u03b22\nj\n\n(cid:14)\n\n|\u03b2j| + \u03b1\n\n(cid:14)\nwhich depends on two tuning parameters, \u03bb1, \u03bb2 > 0. the elastic net penalty is a convex\ncombination of the lasso and the ridge penalty. in constraint form it may be written as (1 \u2212\n\u2264 t for some t and tuning parameter \u03b1 = \u03bb2/(\u03bb1 + \u03bb2). with \u03b1 \u2208 [0, 1]\n\u03b1)\nthe lasso and ridge are limiting cases. for illustration, the contour plots of the elastic net penalty,\nthe lasso, and the ridge are shown in figure 6.2. zou and hastie (2005) called (6.5) the na\u00efve\nelastic net criterion and proposed a rescaled solution \u02c6\u03b2 = (1 + \u03bb2)\u02c6\u03b2net, where \u02c6\u03b2net is the\npenalized least-squares solution of the na\u00efve elastic net criterion. they give several reasons for\nchoosing (1 + \u03bb2) as scaling factor.\n\nthe interesting property of the elastic net is the grouping effect. a regression method ex-\nhibits the grouping effect if the regression coefficients of a group of highly correlated variables\ntend to be equal, up to a change of sign if negatively correlated. zou and hastie (2005) show\nthat for penalized least-squares problems the coefficient paths of predictors xi and xj with\nsample correlation \u0001ij are confined by\n| \u02c6\u03b2i \u2212 \u02c6\u03b2j|/\n\n2(1 \u2212 \u0001ij),\n\n(cid:7)\n\n*\n\n|yi| \u2264 1\n\u03bb2\n\ni\n\nwhere \u02c6\u03b2i, \u02c6\u03b2j are na\u00efve net solutions with parameters \u03bb1, \u03bb2. if xi and xj are highly correlated,\n(\u0001ij \u2192 1), the coefficient paths of xi and xj are very close. thus the elastic net shows the\n\n "}, {"Page_number": 167, "text": "6.2. regularization by penalization\n\n155\n\ngrouping the effect, which is important, for example, in genetics, where groups of genes that\nare relevant are to be selected (\"grouped selection\").\n\nto illustrate why the grouping effect is useful we use the idealized example given by zou\nand hastie (2005). with z1 and z2 being two independent u(0, 20) variables, the response is\ngenerated as n(z1 + 0.1z2, 1). it is assumed that one observes only noisy versions of z1 and\nz2:\n\nx1 = z1 + \u00011,\nx4 = z2 + \u00014,\n\nx2 = \u2212z1 + \u00012,\nx5 = \u2212z2 + \u00015,\n\nx3 = z1 + \u00013,\nx6 = z2 + \u00016,\n\nwhere \u0001i are independent identically distributed n(0, 1/16). the variables x1, x2, and x3 may\nbe considered as forming one group and x4, x5, and x6 as forming a second group. figure 6.8\nshows the coefficient buildups for the lasso and a method that shows the grouping effect for\nsample size n = 100. the method used is blockboost, which is described in the next section.\nit is seen that blockboost selects the variables x1, x2, and x3, and the corresponding estimates\nare (up to sign) identical. the strong group consistency of x1, x2, and x3 is distinctly identified.\nlasso shows quite different coefficient buildups, selecting as strongly influential the variables\nx1 and x3 and, with rather weak effect, x2. while the coefficient paths for blockboost reflect\nthe high correlation of x1, x2 and x3, the paths of the lasso are rather irregular. the elastic net\nbehaves quite similar to blockboost (compare zou and hastie, 2005).\n\nlasso\n\nblockboost (lambda = 10)\n\ns\nt\n\ni\n\nn\ne\nc\ni\nf\nf\n\ne\no\nc\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n2\n\n.\n0\n\u2212\n\n4\n\n.\n\n0\n\u2212\n\n6\n1\n3\n\n5\n\n2\n\n4\n\ns\nt\n\ni\n\nn\ne\nc\ni\nf\nf\n\ne\no\nc\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n2\n\n.\n\n0\n\u2212\n\n4\n.\n0\n\u2212\n\n1\n3\n6\n\n5\n\n4\n\n2\n\n1\n\n2\n\n3\n\n4\n\ndf\n\n5\n\n6\n\n7\n\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\n\ndf\n\nfigure 6.8: coefficient buildups for lasso (left) and blockboost (right) of the hidden\nfactors example. vertical lines indicate the degrees of freedom corresponding to the tuning\nparameter(s) chosen by 10-fold cross-validation.\n\n6.2.4 alternative estimators with grouping effect\n\nthe grouping effect of the elastic net gives similar coefficients to highly correlated variables.\nmore recently, alternative penalties were proposed that aim at the grouping effect.\n\n "}, {"Page_number": 168, "text": "156 chapter 6. regularization and variable selection for parametric models\n\noscar\nbondell and reich (2008) proposed a method called oscar, for octagonal shrinkage and\nclustering algorithm for regression. the method shrinks, like the lasso, some coefficients\nto zero, but in addition yields the exact equality of some of the coefficients. the predictors\nwith equal coefficients form clusters that are represented by a single coefficient. oscar in\n\nconstrained form uses the restriction(cid:7)\n\n(cid:7)\n\n|\u03b2j| + c\n\nmax{|\u03b2j|,|\u03b2k|} \u2264 t\n\nj\n\nj<k\n\nfor some tuning parameters t > 0, c \u2265 0. the parameter c controls the relative weighting of the\nl1-norm and the pairwise l\u221e-norm. with c = 0 the lasso is a special case. for two predictors,\nthe constraint region forms an octagon. the vertices on the diagonals and on the axis encourage\nequality of coefficients (when a vertice on the diagonal is hit) and sparsity (when a vertice on\nthe axis is hit). varying c changes the angle formed in the octagon, yielding a diamond if c = 0\nand a square if c \u2192 \u221e.\n\nc from the penalized version j(\u03b2) = \u03bb{(cid:14)\n\nthe exact grouping property derived by bondell and reich (2008) uses parameters \u03bb and\nj<k max{|\u03b2j|,|\u03b2k|}. for signed co-\nefficients, so that \u02c6\u03b2j \u2265 0 for all j, they show that for the linear model there exists a c0 such\nthat\n\n(cid:14)\n\nj\n\n|\u03b2j| + c\n*\n\u22121|y|\n\n0 \u2264 c0 \u2264 2\u03bb\n\n2(1 \u2212 \u0001jk)\n\nand \u02c6\u03b2j = \u02c6\u03b2k for all c \u2265 c0. here \u0001jk denotes the correlation between the variables xj and xk,\nand the response vector y is centered. therefore, there is a threshold on c, which can be very\nsmall when \u0001jk is close to one, such that the coefficients are equal and therefore form a cluster.\na representation of oscar\u2019s penalty region as a polytope is found in petry and tutz (2012).\nthe oscar for glms was considered by petry and tutz (2011).\n\nexample 6.3: heart disease\nfor elastic net and oscar, which contain the lasso as a special case, the coefficient paths based on 10-\nfold cross-validation for the heart disease data are very close to the paths found for lasso. to illustrate\nthe grouping effect we show the coefficient paths for c = 0.2 and c = 0.5, which enforce the grouping\nproperty. figure 6.9 shows the resulting coefficient buildups. it is seen that for strong smoothing some\neffects are set equal.\n\noscarr (c=0.2)\n\noscarr (c=0.5)\n\n0\n1\n\n5\n\n0\n\n5\n\u2212\n\nage\n\nfamhist\ntypea\nldltobacco\n\nsbpadiposity\n\nalcohol\n\nobesity\n\n0\n1\n\n5\n\n0\n\n5\n\u2212\n\nage\n\nfamhist\ntypea\nldltobacco\n\nsbpadiposity\n\nalcohol\n\nobesity\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\nfigure 6.9: oscar coefficient paths for heart disease data (two values of c).\n\n "}, {"Page_number": 169, "text": "6.2. regularization by penalization\n\n157\n\ncorrelation-based penalties\nan alternative approach that aims at the grouping effect uses explicitly the correlation between\npairs of predictors. the correlation-based penalty has the form\n\np\u22121(cid:7)\n\n(cid:7)\n\n(cid:29)\n\ni=1\n\nj>i\n\njc(\u03b2) =\n\n(\u03b2i \u2212 \u03b2j)2\n1 \u2212 \u0001ij\n\n+\n\n(\u03b2i + \u03b2j)2\n\n1 + \u0001ij\n\n= 2\n\n\u03b22\ni\n\n\u2212 2\u0001ij\u03b2i\u03b2j + \u03b22\n\nj\n\n1 \u2212 \u00012\n\nij\n\n,\n\n(6.6)\n\n(cid:30)\n\np\u22121(cid:7)\n\n(cid:7)\n\ni=1\n\nj>i\n\nwhere \u0001ij denotes the (empirical) correlation between the ith and the jth predictors. it is de-\nsigned to focus on the grouping effect, that is, its highly correlated effects show comparable\nvalues of estimates (| \u02c6\u03b2i| \u2248 | \u02c6\u03b2j|) with the sign being determined by positive or negative correla-\ntion. for strong positive correlation (\u0001ij \u2192 1) the first term becomes dominant, having the ef-\nfect that estimates for \u03b2i, \u03b2j are similar ( \u02c6\u03b2i \u2248 \u02c6\u03b2j). for strong negative correlation (\u0001ij \u2192 \u22121)\nthe second term becomes dominant and \u02c6\u03b2i will be close to \u2212 \u02c6\u03b2j. consequently, for weakly\ncorrelated data the performance is quite close to the ridge penalty.\n\nfigure 6.10 shows the two-dimensional contour plots for selected values of \u0001 together with\nthe constraint region for the ridge penalty and the lasso. it is seen that contours for the ridge\nand lasso are highly symmetric; x1 = 0 is an axis of symmetry as well as x2 = 0. in contrast,\nthe constrained region for the correlation-based estimator is an ellipsoid that becomes narrower\nwith increasing correlation. spectral decomposition of jc(\u03b2) yields eigenvectors (1, 1) and\n(1,\u22121) with corresponding eigenvalues \u03bb/(1 \u2212 \u0001) and \u03bb/(1 + \u0001). thus, for \u0001 > 0, the first\neigenvalue becomes dominant while for \u0001 < 0 it is the second eigenvalue that determines the\norientation of the ellipsoid. when computing the penalized least-squares criterion, the effect\nis that, for \u0001 > 0, estimates are preferred for which the components \u02c6\u03b21, \u02c6\u03b22 are similar; for\n\u0001 < 0, similarity of \u02c6\u03b21 and \u2212 \u02c6\u03b22 is preferred. this may be seen from the contour plots, since\nfor \u0001 > 0 the increase in pc(\u03b2) is slower when moving in the direction of the first eigenvector\n(1, 1) than in the orthogonal direction (1,\u22121). for \u0001 < 0, the eigenvalue corresponding to\n(1,\u22121) is larger, and therefore parameter values where \u03b21 is close to \u2212\u03b22 are preferred. thus\nthe use of penalty pc implies shrinkage, with the strength of shrinkage being determined by \u03bb,\nbut shrinkage differs from ridge shrinkage, which occurs for the special case \u0001ij = 0.\n(cid:8)= 1 for i (cid:8)= j. then jc(\u03b2\u03b2\u03b2) is strictly convex and the esti-\nmate exists and is unique. for linear models an explicit solution to the penalized least-squares\nproblem is obtained, called the correlation-based estimator:\n\nassume that \u03bb > 0 and \u00012\nij\n\n\u02c6\u03b2\u02c6\u03b2\u02c6\u03b2c = (x t x + \u03bbm)\n\n\u22121x t y,\n\n(6.7)\n\nwhere xt = (x1 . . . xn) is the design matrix, y collects the responses, yt = (y1, . . . , yn),\nand m is a matrix that is determined by the correlations \u0001ij, i, j = 1, . . . , p. the explicit form\nexploits that the correlation-based penalty (9.7) can be written as a quadratic form:\n\njc(\u03b2\u03b2\u03b2) = \u03b2t m \u03b2,\ns(cid:5)=i 1/(1 \u2212 \u00012\n\n(6.8)\nis) if i = j, and mij = \u22122\u0001ij/(1 \u2212 \u00012\nwhere m = (mij) has entries mij = 2\nij)\nif i (cid:8)= j. for glm-type models the ml estimate is obtained by penalized fisher scoring (see\nsection 6.2.1). although the correlation-based penalty enforces the grouping effect with good\nresults in simulations, it does not enforce sparsity. therefore, tutz and ulbricht (2009) pro-\nposed a specific form of blockwise boosting, called blockboost. to obtain the grouping effect\nof the correlation-based estimator combined with variable selection, a boosting procedure is\nused that updates in each step the coefficients of more than one variable. the procedure dif-\nfers from common componentwise boosting, where just one variable is selected and the cor-\nresponding coefficient is adjusted. the algorithm is able to handle high-dimensional data and,\n\n(cid:14)\n\n "}, {"Page_number": 170, "text": "158 chapter 6. regularization and variable selection for parametric models\n\nridge, lasso and \n\n elastic net penalties\n\n2\na\ne\nb\n\nt\n\n0\n\n.\n\n1\n\n5\n0\n\n.\n\n0\n\n.\n\n0\n\n.\n\n5\n0\n\u2212\n\n.\n\n0\n1\n\u2212\n\n\u22121.0\n\n\u22120.5\n\n0.0\n\n0.5\n\n1.0\n\nbeta1\n\ncorrelation\u2212based penalty with \n\n positive correlations\n\ncorrelation\u2212based penalty with \n\n negative correlations\n\n2\na\nt\n\ne\nb\n\n0\n.\n1\n\n5\n.\n0\n\n0\n.\n0\n\n5\n.\n0\n\u2212\n\n0\n.\n1\n\u2212\n\n2\na\nt\n\ne\nb\n\n0\n.\n1\n\n5\n.\n0\n\n0\n.\n0\n\n5\n.\n0\n\u2212\n\n0\n.\n1\n\u2212\n\n\u22121.0\n\n\u22120.5\n\n0.0\n\n0.5\n\n1.0\n\n\u22121.0\n\n\u22120.5\n\n0.0\n\n0.5\n\n1.0\n\nbeta1\n\nbeta1\n\nfigure 6.10: top panel: two-dimensional contour plots for ridge, lasso (dashed line),\nand elastic net with \u03b1 = 0.5 (dotted line). lower panel left: contour plots of correlation-\nbased penalty for positive correlations: \u0001 = 0.5 (solid line), \u0001 = 0.8 (dashed line), and\n\u0001 = 0.99 (dotted line). lower panel right: contour plots of correlation-based penalty for\nnegative correlations: \u0001 = \u22120.5 (solid line), \u0001 = \u22120.8 (dashed line), and \u0001 = \u22120.99\n(dotted line).\n\ndepending on the tuning parameter, enforces sparsity. the grouping effect is demonstrated\nin figure 6.8, where coefficients are plotted against degrees of freedom for the simulation\ndescribed in section 6.2.3.\nit is seen that lasso fails to recognize the grouping structure in\ncontrast to blockboost. for more details on the correlation-based approach in linear mod-\nels, see tutz and ulbricht (2009). glm-type models were considered by ulbricht and tutz\n(2008). an alternative way is to combine correlation-based penalties and the l1-penalty into the\nform\n\njc(\u03b2\u03b2\u03b2) = \u03bb1\n\n|\u03b2j| + \u03bb2\u03b2t m \u03b2.\n\n(6.9)\n\nanbari and mkhadri (2008) demonstrated that the penalty shows good performance in many\napplications.\n\nj=1\n\np(cid:7)\n\n "}, {"Page_number": 171, "text": "6.2. regularization by penalization\n\n159\n\nfusion-type estimators\ndaye and jeng (2009) proposed the penalty\n\nj(\u03b2\u03b2\u03b2) = \u03bb1\n\n|\u03b2j| + \u03bb2\n\np(cid:7)\n\nj=1\n\n(cid:7)\n\ni<j\n\nwij(\u03b2i \u2212 sign(\u0001ij)\u03b2j)2,\n\nwhere sign(\u0001ij) denotes the sign of the correlation coefficient \u0001ij taking values 1 or \u20131. the\npenalty combines the lasso with a term that enforces the fusion of variables. when \u03bb2 is large\nand weights are positive, the second term enforces \u02c6\u03b2i \u2248 \u02c6\u03b2j for positive correlation between\nxi and xj and \u02c6\u03b2i \u2248 \u2212 \u02c6\u03b2j for negative correlation. because of the tendency to fuse predic-\ntors, daye and jeng (2009) call the resulting estimator a weighted fusion estimator. they\nuse the correlation-driven weights wij = |\u0001ij|\u03b3/(1 \u2212 |\u0001ij|\u03b3), where \u03b3 is an additional tuning\nparameter, and derive conditions that make the estimator sign consistent for the linear model.\nsign consistency is somewhat stronger than variable selection consistency and postulates that\nasymptotically the sign of the estimate is the same as the sign of the true parameter.\n\nthe penalty belongs to the general family of combination penalties\n\nj(\u03b2\u03b2\u03b2) = \u03bb1(cid:16)\u03b2(cid:16)1 + \u03bb2(cid:16)\u03b2(cid:16)r\n2,\nwhere (cid:16)\u03b2(cid:16)1 = (|\u03b21| + \u00b7\u00b7\u00b7 + |\u03b2p|) is the l1-norm and (cid:16)\u03b2(cid:16)2\nr = \u03b2t r\u03b2 is the squared norm\nbuilt with matrix r. for r = i one obtains the elastic net, for r = m one obtains (6.9),\nand for specific r one obtains the weighted fusion estimator. it should be noted that all com-\nbination penalties (6.10) can be reformulated as lasso problems by simple data augmentation,\nand therefore algorithms that compute lasso solutions can be used (see, for example, zou and\nhastie, 2005).\n\n(6.10)\n\nin their derivation of the penalty daye and jeng (2009) refer to the fused lasso, which was\nproposed by tibshirani et al. (2005). however, the latter enforces a fusion of predictors by using\na lasso-type estimator instead of a ridge-type estimator for the differences. the corresponding\npairwise fused lasso estimator has the form\n\np(cid:7)\n\n(cid:7)\n\nj(\u03b2\u03b2\u03b2) = \u03bb1\n\n|\u03b2j| + \u03bb2\n\n|\u03b2i \u2212 \u03b2j|,\n\nj=1\n\ni<j\n\nor with weights and correlation-based penalty,\n\nj(\u03b2\u03b2\u03b2) = \u03bb1\n\n|\u03b2j| + \u03bb2\n\np(cid:7)\n\nj=1\n\n(cid:7)\n\ni<j\n\nwij|\u03b2i \u2212 sign(\u0001ij)\u03b2j|.\n\nfor applications see petry et al. (2011). the fused lasso itself is considered in more detail in\nsection 10.4.4.\n\n6.2.5 scad\nan alternative penalty that yields simultaneous estimation and selection has been proposed by\nfan and li (2001). they identified three properties that a penalized estimator should have and\nderived an appropriate penalty. the resulting estimator should be nearly unbiased for large\nunknown coefficients (unbiasedness), it should automatically set small estimated coefficients to\n\n "}, {"Page_number": 172, "text": "160 chapter 6. regularization and variable selection for parametric models\n\nzero (sparsity), and it should be continuous in the data to avoid instability in model prediction\n(continuity). the penalty function considered by fan and li (2001) has the additive form\n\np(cid:7)\n\np(cid:7)\n\n\u03bbj(\u03b2) = \u03bb\n\np(|\u03b2j|) =\n\np\u03bb(|\u03b2j|),\n\nj=1\n\nj=1\n\nwhere p(.) are penalty functions that, in the most general form, can also depend on the variable.\nthe functions p\u03bb(|\u03b2|) = \u03bbp(|\u03b2|) are introduced to allow that the penalty may depend on \u03bb.\nobviously, ridge and lasso are special cases with functions p(|\u03b2j|) = |\u03b2j|2 and p(|\u03b2j|) = |\u03b2j|,\nrespectively. the continuously differentiable penalty function proposed by fan and li is the\nsmoothly clipped absolute derivation (scad) penalty, defined by its derivative:\n\n\u03bb(\u03b2) = \u03bb{i(\u03b2 \u2264 \u03bb) +\n(cid:3)\np\n\n(a\u03bb \u2212 \u03b2)+\n(a \u2212 1)\u03bb\n\ni(\u03b2 > \u03bb)},\n\nfor some a > 2 and \u03b2 > 0, where (x)+ = x if x > 0 and 0 otherwise. the penalty corresponds\nto a quadratic spline function with knots at \u03bb and a\u03bb. figure 6.11 shows the penalty and its\nderivative. it is seen that for small values the penalty is similar to the lasso penalty whereas for\nlarger values the penalty levels off.\n\nto gain some insight about the effect of penalty functions, it is common to study the linear\nregression model with orthonormal columns in the design matrix. then it may be shown that\n|\u2212\u03bb)+,\nthe lasso penalty p(|\u03b2|) = \u03bb|\u03b2| yields the soft thresholding rule \u02c6\u03b2j = sign( \u02c6\u03b2m l\nwhere \u03b2m l\n\ndenotes the ml estimate and scad yields\n\n)(| \u02c6\u03b2m l\n\nj\n\nj\n\nj\n\n\u23a7\u23aa\u23a8\n\u23aa\u23a9sign( \u02c6\u03b2m l\n{(a \u2212 1) \u02c6\u03b2m l\n\u02c6\u03b2m l\n\nj\n\nj\n\nj\n\n\u02c6\u03b2j =\n\n)(| \u02c6\u03b2m l\n\n| \u2212 \u03bb)+\n\u2212 sign(\u03b2m l\n\nj\n\nj\n\n)a\u03bb}/a \u2212 2\n\nif\nif\nif\n\nj\n\n| < 2\u03bb\n| \u02c6\u03b2m l\n2\u03bb < | \u02c6\u03b2m l\n| > a\u03bb\n| \u02c6\u03b2m l\n\nj\n\nj\n\n| \u2264 a\u03bb\n\n(see fan and li, 2001). usually these penalties are compared to the hard thresholding penalty\nfunction p\u03bb(|\u03b2|) = \u03bb2 \u2212 (|\u03b2| \u2212 \u03bb)2i(|\u03b2| < \u03bb), which sets the estimate to zero if the ml\nestimate is below some threshold and retains the ml estimate if it is above the threshold, that\n| > \u03bb). it is seen from figure 6.6 that scad and hard thresholding avoid\nis, \u02c6\u03b2j = \u02c6\u03b2m l\nbias for large coefficients, in contrast to lasso. scad shares with the lasso the continuity of the\nresulting function (for details see fan and li, 2001).\n\ni(| \u02c6\u03b2m l\n\nj\n\nj\n\nfor generalized linear models, the penalized log-likelihood has to be minimized. since the\nscad penalty functions are singular at the origin and do not have continuous second-order\nderivations for the computation, fan and li use that they can be locally approximated by a\nquadratic function. the proposed local quadratic approximations also applies to the lasso and\nhard thresholding penalties.\n\n1 , \u03b2t\n\nan advantage of the scad penalty is its oracle property. let the parameter vector \u03b2 be\n2 ) and assume \u03b22 = 0. with i(\u03b2) denoting the full fisher matrix\npartitioned into \u03b2t = (\u03b2t\nand j1(\u03b21) the fisher matrix and knowing \u03b22 = 0, it may be shown that \u02c6\u03b2\n2 ) must\nasymptotically satisfy \u02c6\u03b22 = 0 and \u02c6\u03b21 is asymptotic normal with covariance matrix j1(\u03b21)\u22121\nif n1/2\u03bbn \u2192 \u221e. this means that asymptotically the estimator performs as well as if \u03b22 = 0\nwere known. the penalized estimate is root-n consistent if \u03bbn \u2192 0 and converges at the rate\nop(n\n\u03bb(|\u03b2j|), where s(\u03b2) is the\n(cid:3)\nusual score function of a glm. the derivative on the first term on the right-hand side yields the\nusual negative information matrix. the derivative of the second term has to be approximated\n\n\u22121/2 + an), where an = max{p\n(cid:3)\nthe penalized score function has the form sp(\u03b2) = s(\u03b2) \u2212 \u03c3jp\n\u03bbn\n\n(|\u03b2j|), \u03b2j (cid:8)= 0}.\n\nt = (\u02c6\u03b2\n\n1 , \u02c6\u03b2\n\nt\n\nt\n\n "}, {"Page_number": 173, "text": "6.2. regularization by penalization\n\n161\n\nridge\n\nridge\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\n\u22124\n\n\u22122\n\n0\n\u03b2\n\nlasso\n\n\u22124\n\n\u22122\n\n0\n\u03b2\n\nscad\n\n\u22124\n\n\u22122\n\n0\n\u03b2\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\n2\n\n4\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n\u03b2\n\nlasso\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\n2\n\n4\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n\u03b2\n\nscad\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\n2\n\n4\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\nfigure 6.11: components p\u03bb(\u03b2) (left) and derivations p\nfor ridge, lasso, and scad.\n\n\u03b2\n\n(cid:2)\n\u03bb(\u03b2) (right) of penalty functions\n\nbecause derivatives do not exist. fan and li (2001) used a quadratic approximation based on\nan initial estimate \u03b20, which is given by p\u03bb(|\u03b2j|) = p\n|) sign(\u03b2j) \u2248 {p\n\u03bb(|\u03b20j|)/|\u03b20j|}\u03b2j\n(cid:3)\nwhen \u03b2j (cid:8)= 0. let \u02c6\u03b2\n1 = ( \u02c6\u03b211, . . . \u02c6\u03b21p) denote the non-vanishing components of \u02c6\u03b2, obtained\nfrom sp(\u02c6\u03b2) = 0. then the corresponding sandwich formula that approximates the covariance\nof \u02c6\u03b21 is\n\n\u03bb(|\u03b2j\n(cid:3)\n\nt\n\n\u02c6cov(\u02c6\u03b2) \u2248 [i(\u03b21) + p\u03bb(\u02c6\u03b21)]\n\nwhere i(\u03b21) = \u2212\u2202l(\u02c6\u03b21)/\u2202\u03b2\u2202\u03b2t and p\u03bb(\u02c6\u03b21) = diag(p\naccording to fan and li (2001), the formula has good accuracy for moderate sample sizes.\n\n\u22121,\n\u22121 \u02c6cov(sp( \u02c6\u03b21))[i(\u03b21) + p\u03bb(\u02c6\u03b21)]\n\u03bb(|\u03b201|)/|\u03b201|, . . . , p\n\u03bb(|\u03b20p|)/|\u03b20p|).\n(cid:3)\n(cid:3)\n\n "}, {"Page_number": 174, "text": "162 chapter 6. regularization and variable selection for parametric models\n\n6.2.6 the dantzig selector\nthe dantzig selector was proposed by candes and tao (2007) and generalized to glms by\njames and radchenko (2008). like the lasso, it obtains variable selection by using an l1\npenalty to shrink the coefficients toward zero. however, the penalty is used in a different way.\nfor simplicity, let the response function h of the model \u03bc = h(xt \u03b2) be the canonical response\nfunction. then the generalized dantzig selector criterion is\n\nmin(cid:16)\u02dc\u03b2(cid:16)1\n|+\u00b7\u00b7\u00b7+|\u02dc\u03b2p\n\nsubject to |xt\n| represents, the l1-norm, xt\n\n.j(y \u2212 \u02dc\u03bc)| \u2264 \u03bb, j = 1, . . . , p,\n\nwhere (cid:16)\u02dc\u03b2(cid:16)1 = |\u02dc\u03b21\n.j = (x1j, . . . , xnj) is the jth column\nof the design matrix, and y, \u02dc\u03bc denote the vector of observations and fitted values, respectively.\n.j(y\u2212 \u02dc\u03bc)| \u2264 \u03bb, where \u03bb is the tuning parameter, is based on\nthe constraint region defined by |xt\nthe score function, which with a canonical link has the form s(\u03b2) = x t (y \u2212 \u03bc). therefore,\n.j(y\u2212\u03bc) represents the jth component of the score function. while ml estimates are obtained\nxt\n.j(y \u2212 \u02dc\u03bc)| \u2264 \u03bb represents a weaker condition\nwhen xt\ndepending on the value of \u03bb.\n\n.j(y \u2212 \u02c6\u03bc) = 0 for all j, the constraint |xt\n\none of the strengths of the dantzig selector is that for linear models it can be formu-\nlated as a linear programming problem and therefore also can be efficiently computed for\nhigh-dimensional problems. for linear problems, the constraint region has the simple form\n.j(y \u2212 \u02dcx \u02dc\u03b2)| \u2264 \u03bb. for glms, one can use that ml estimates are obtained iteratively by a\n|xt\nweighted least-squares algorithm. for a general link function, the score function has the form\n\u22121(y \u2212 \u03bc), where d = diag (\u2202h(\u03b71)/\u2202\u03b7, . . . \u2202h(\u03b7n)/\u2202\u03b7) is the diagonal\ns(\u03b2) = x t d \u03c3\n\u22121dt , one step of the fisher scoring\nmatrix of derivatives. with weight matrix w = d \u03c3\niteration for obtaining ml estimates has the form\n\n\u02c6\u03b2\n\n(k+1) = (x t w (\u02c6\u03b2\n\n(k))x)\n\n\u22121x t w (\u02c6\u03b2\n\n(k))\u02dc\u03b7(\u02c6\u03b2\n\n(k))\n\nwith a vector of pseudo-observations \u02dc\u03b7(\u02c6\u03b2) = x \u02c6\u03b2 + d(\u02c6\u03b2)\u22121(y \u2212 \u02c6\u03bc). the implicitly used\nweighted least-squares estimate is equivalent to a least-squares estimate for a design matrix\nw (\u02c6\u03b2\n(k)). therefore, instead of solving the\nscore equations, one uses an iterative procedure that, for given parameter \u02c6\u03b2\n, computes the\nlinear model dantzig selector:\n\n(k))1/2x and pseudo-response w (\u02c6\u03b2\n\n(k))1/2\u02dc\u03b7(\u02c6\u03b2\n\n(k)\n\nmin(cid:16)\u02dc\u03b2(cid:16)1\n\nsubject to |xt\n\n.jw (\u02c6\u03b2\n\n(k))(\u02dc\u03b7(\u02c6\u03b2\n\n(k)) \u2212 x \u02dc\u03b2)| \u2264 \u03bb, j = 1, . . . , p,\n\n(k+1)\n\nyielding the new estimate \u02c6\u03b2\n. james and radchenko (2008) also gave an algorithm for\nfitting the generalized dantzig selector path. by computing the whole path efficiently, cross-\nvalidation on a fine grid can be performed. efficient computing is needed since james and\nradchenko (2008) use two shrinkage parameters. the reason is that the dantzig selector tends\nto overshrink the coefficients. if strong shrinkage (large \u03bb) is applied so that noisy variables are\nexcluded, the estimates of coefficients are too small. if small \u03bb is selected, noisy variables tend\nto be included.\n\nexample 6.4: heart disease\nfigure 6.12 shows the coefficient buildups for scad (a = 3; package lqa) and the dantzig selector\n(standardized explanatory variables ) plotted against (cid:9)\u03b2(cid:9)/ max(cid:9)\u03b2(cid:9). although the paths are differing,\nselection based on 10-fold cross-validation yields similar coefficients.\n\n "}, {"Page_number": 175, "text": "6.3. boosting methods\n\n163\n\nscad\n\ndantzig selector\n\n5\n1\n\n0\n1\n\n5\n\n0\n\n5\n\u2212\n\n5\n1\n\n0\n1\n\n5\n\n0\n\n5\n\u2212\n\nage\n\nfamhist\ntypea\nldltobacco\n\nsbpadiposity\n\nalcohol\n\nobesity\n\nage\n\nfamhist\ntypea\nldltobacco\n\nsbpadiposity\n\nalcohol\n\nobesity\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\nfigure 6.12: scad and dantzig coefficient paths for heart disease data (package lqa;\nvertical line shows estimate selected by 10-fold cross-validation).\n\n6.3 boosting methods\nboosting methods were originally developed in the machine learning community as a means to\nimprove classification (e.g., shapire, 1990). they have been proposed as ensemble methods,\nwhich rely on generating multiple predictions and averaging across the individual predictions.\nlater it was shown that boosting can be seen as the fitting of an additive structure by minimizing\nspecific loss functions (see friedman, 2001; friedman et al., 2000). b\u00fchlmann and yu (2003)\nand b\u00fchlmann (2006) proposed and investigated boosted estimators in the context of a linear\nregression with the focus on l2 loss. in regressions, boosting may be seen as a regularization\ntechnique that also allows one to select predictors. for more background on boosting see also\nsection 15.5.3.\n\n6.3.1 boosting for linear models\nbefore considering the boosting of generalized linear models, we consider briefly the familiar\ncase of normal regression models. let the underlying regression structure be given by e(y|x) =\n\u03b7(x) and data be given by (yi, xi), i = 1, . . . , n.\n\nboosting is based on fitting a structured function that is supposed to approximate \u03b7(x). the\nfitting of a structured function (a learner in machine learning terminology) is considered as a\nbase procedure. ensemble methods are based on averaging across several such procedures. let\n\u02c6g(x,{ui, xi}) denote the base procedure at value x based on input data {ui, xi}, which are\nnot necessarily the original data {yi, xi}. when fitting linear models, the base procedure uses\na linear function g(x,{ui, xi}) = \u02dcxt \u03b3, where \u02dcx is usually a subvector of x. in one step of the\nprocedure one does not aim at estimating the whole vector \u03b2 (from predictor \u03b7 = xt \u03b2) but at\nimproving the estimate of the parameters \u03b3 that correspond to a subset of \u03b2. a basic boosting\nalgorithm then is given by:\n\nstep 1 (initialization)\n\ngiven data {yi, xi}, fit the base procedure to yield the function estimate \u03b7(0)(.) =\n\u02c6g(.,{yi, xi}).\nstep 2 (iteration)\n\nfor l = 0, 1, 2, . . . , compute the residuals ui = yi \u2212 \u02c6\u03b7(l)(xi) and fit the base pro-\ncedure to the current data {ui, xi}. the fit \u02c6g(.,{ui, xi}) is an estimate based on the\n\n "}, {"Page_number": 176, "text": "164 chapter 6. regularization and variable selection for parametric models\n\noriginal predictor variables and the current residuals. the improved fit is obtained by the\nupdate\n\n\u02c6\u03b7(l+1)(.) = \u02c6\u03b7(l)(.) + \u02c6g(.,{ui, xi}).\n\nthe iteration is stopped by applying a stopping criterion, for example, aic or a cross-\nvalidation measure. boosting in this form iteratively improves the fit by adding the fit of a base\nlearner, which reduces the discrepancy between the current residual and the fit. it may be seen\nas forward stepwise additive modeling.\nb\u00fchlmann and yu (2003) thoroughly investigated l2 boosting, which utilizes least-squares\nestimates as a fitting procedure. thereby, in one step one minimizes \u03c3i(ui \u2212 \u02c6g(xi,{ui, xi})2.\nl2 boosting may also be derived as a gradient descent algorithm (see section 15.5.3) and, as\nmentioned by b\u00fchlmann and yu, is nothing else more than repeated least-squares fitting of\nresiduals, which for one boosting step has already been proposed by tukey (1977) under the\nname \"twicing.\" for linear models, which assume that the conditional mean \u03b7(x) = e(y|x) has\nthe form \u03b7(x) = xt \u03b2, a simple least squares-fitting of a linear predictor g(x,{ui, xi}) = xt \u03b2\nwithin one boosting step would simply yield the usual least-squares estimate after one step.\ntherefore, to obtain a regularized estimate, alternative base procedures have to be used.\n\nan approach that implicitly selects variables is componentwise boosting. componentwise\nboosting means that each part of the linear predictor is refitted separately, and among the fitted\nparts one is selected to be used in the update. in its simplest form, componentwise boosting\nrefits only one coefficient, which is selected by some optimality criterion. b\u00fchlmann (2006)\nproposed a componentwise linear least-squares algorithm for linear models by using the base\nlearner g(x,{ui, xi}) = \u03b3\u02c6sx\u02c6s, where \u02c6\u03b3j is the usual least-squares estimate resulting from using\nonly the jth variable, \u02c6\u03b3j = \u03c3iuixij/\u03c3ix2\n\nij (centered predictors), and\n\n\u02c6s = arg min\n1\u2264j\u2264p\n\n(ui \u2212 \u02c6\u03b3jxij)2\n\nn(cid:7)\n\ni=1\n\ndetermines which variable is selected. thus, the base procedure in componentwise linear least-\nsquares boosting performs a linear least-squares regression against the one selected variable that\nreduces the residual sum of squares the most. the actual refit typically uses \u02c6g(x,{ui, xi}) =\n\u03bd\u03b3\u02c6sx\u02c6s, where the parameter \u03bd is a fixed shrinkage parameter, in order to obtain a weak learner\nj , j (cid:8)= \u02c6s,\n(see next section). the corresponding refit of parameters is given by \u02c6\u03b2(l+1)\n\u02c6\u03b2(l+1)\n\u02c6s + \u03bd\u03b3\u02c6s. since in high-dimensional settings usually not all of the predictors are\nselected before the stopping criterion is reached, the procedure selects variables automatically.\nb\u00fchlmann (2006) showed that the procedure is consistent for underlying regression functions,\nwhich are sparse in terms of the l1-norm.\n\n= \u02c6\u03b2(l)\n\n= \u02c6\u03b2(l)\n\n\u02c6s\n\nj\n\nboosting procedures are based on \u201cweak\u201d learners, a concept that has been derived in the\nmachine learning community (freund and schapire, 1997). in classification, a weak learner\nmay be considered as an estimator that is slightly better than guessing. in regression, a weak\nlearner refers to small step sizes within the algorithm. therefore, the update step in linear\nleast-squares boosting uses\n\n\u02c6\u03b7(r+1)(.) = \u03b7(r)(.) + \u03bd\u02c6g(.,{ui, xi}),\n\nwhere \u03bd is a shrinkage parameter, for example, \u03bd = 0.1. small step sizes (small \u03bd) make the\nboosting algorithm slow and require a larger number of iterations, but improve the performance.\nsmall values of \u03bd have been shown to avoid early overfitting of the procedure.\n\n "}, {"Page_number": 177, "text": "6.3. boosting methods\n\n165\n\n6.3.2 boosting for generalized linear models\nin generalized linear models least-squares estimates are not the best choice because they do not\nrelate adequately to the underlying error structure. a better choice is likelihood-based boosting,\nwhich more generally aims at maximizing the log-likelihood rather than minimizing the squared\nresiduals. for fixed link functions, likelihood-based approaches iteratively estimate the linear\npredictor, \u03b7i = xt\n\ni \u03b2, which is linked to the mean \u03bci = e(yi|xi) by \u03bci = h(\u03b7i).\n\none difference between the l2 boost and a generalized linear model boosting is that in\nthe iteration step one cannot fit a glm to the residuals because, for example, with binary\ndata, residuals are not from {0, 1}. the role of the residuals is taken by the offset. the basic\nlikelihood-based boosting algorithm (genboost), which is also used in chapter 15, has the\nfollowing form:\n\nlikelihood boosting (genboost)\n\nstep 1 (initialization)\n\nfor given data (yi, xi), i = 1, . . . , n, fit the intercept model \u03bc(0)(x) = h(\u03b20) by maxi-\nmizing the likelihood, yielding \u03b7(0) = \u02c6\u03b20, \u02c6\u03bc(0) = h( \u02c6\u03b20), \u02c6\u03b2\n\n(0) = ( \u02c6\u03b20, 0, . . . , 0)t .\n\nstep 2 (iteration)\n\nfor l = 0, 1, 2, . . . , fit the model\n\n\u03bci = h(\u02c6\u03b7(l)(xi) + \u03b7(xi, \u03b3))\n\nto data (yi, xi), i = 1, . . . , n, where \u02c6\u03b7(l)(xi) is treated as an offset and the predictor\nis estimated by fitting the parametrically structured term \u03b7(xi, \u03b3), obtaining \u02c6\u03b3. the\nimproved fit is obtained by\n\n\u02c6\u03b7(l+1)(xi) = \u02c6\u03b7(l)(xi) + \u02c6\u03b7(xi, \u02c6\u03b3),\n\n\u02c6\u03bc(l+1)\ni\n\n= h(\u02c6\u03b7(l+1)(xi)).\n\nthe improved parameter \u02c6\u03b2\n\n(l+1)\n\nis obtained by adding \u02c6\u03b3 to the components of \u02c6\u03b2\n\n(l)\n\n.\n\none candidate for fitting is fisher scoring, which is familiar from generalized linear model\n\nfitting. one first has to compute the pseudo-responses and weights:\n\ni = yi \u2212 \u02c6\u03bc(l)\ni )/\u2202\u03b7\n\n\u2202h(\u02c6\u03b7(l)\n\ni\n\n\u02dc\u03b7(l)\n\n, w(l)\n\ni =\n\n(\u2202h(\u02c6\u03b7(l)\ni )/\u2202\u03b7)2\n\u03c32\ni\n\n,\n\nand then compute the weighted regression with weights w(l)\nto\ni\nobtain \u02c6\u03b3. it is noteworthy that the pseudo-responses, in contrast to their usual definition, do not\ninclude a linear term, because the previous fit is contained in the offset. for the logit model one\nhas \u2202h(\u02c6\u03b7i)/\u2202\u03b7 = h(\u03b7i)/(1 \u2212 h(\u03b7i)), and therefore the pseudo-responses and weights simplify\nto\n\nand dependent variables \u02dc\u03b7(l)\ni\n\n\u02dc\u03b7i = yi \u2212 \u02c6\u03bc(l)\n\n\u02c6\u03c0(l)(1 \u2212 \u02c6\u03c0(l)) , wi = \u02c6\u03c0(l)(1 \u2212 \u02c6\u03c0(l)).\n\ni\n\nmore concretely, let us consider the linear predictor \u03b7(xi, \u03b3) = \u02dcxt\n\ni \u03b3, where \u02dcxi is a spec-\nified subvector of xi. for example, when using \u03b7(xi, \u03b3) = xij\u03b3j, \u02dcxi contains only the jth\n\n "}, {"Page_number": 178, "text": "166 chapter 6. regularization and variable selection for parametric models\n\ncovariate as a candidate for updating. one computes within the iteration steps one-step fisher\nscoring estimates:\n\n\u02c6\u03b3 = ( \u02dcx\n\nt\n\nw (l) \u02dcx)\n\n\u22121 \u02dcx\n\nt\n\nw (l)\u02dc\u03b7(l),\n\nwhere \u02dcx is the design matrix built from the vectors \u02dcxi, w (l) is a diagonal matrix that contains\nthe weights, w(l)\n, and \u02dc\u03b7(l) contains the pseudo-responses \u02dc\u03b7(l)\n(for fisher scoring see chapter\ni\ni\n3, section 3.9). since the linear predictor \u03b7(xi, \u03b3) = \u02dcxt\ni \u03b3 is fitted, refitting refers only to\nthe components contained in \u02dcxi, that is, \u02c6\u03b2(l+1)\nj + \u02c6\u03b3j if xij is contained in \u02dcxi (with \u02c6\u03b3j\ndenoting the corresponding estimate) and \u02c6\u03b2(l+1)\n\nif xij is not contained in \u02dcxi.\n\n= \u02c6\u03b2(l)\n= \u02c6\u03b2(l)\n\nto obtain a weak learner \u02c6\u03b3 can be replaced by \u03bd \u02c6\u03b3 with \u03bd denoting a shrinkage parameter.\none-step fisher scoring, starting with the zero vector, can also be given in the form \u02c6\u03b3 =\n(f (l))\u22121s(l), where f (l) = \u02dcx\nw (l)\u02dc\u03b7(l). weak\nlearners may also be obtained in the spirit of ridge estimation by using a penalized fisher matrix\nf (l) + \u03bbi, where \u03bb is chosen large.\n\nw (l) \u02dcx is the fisher matrix and s(l) = \u02dcx\n\nt\n\nt\n\nj\n\nj\n\nj\n\nin componentwise boosting within the fitting step, a selection step is included that deter-\nmines which of the parameters is refitted. in the simplest case one fits all one-covariate models\n\u03b7(xi, \u03b3) = \u03b3jxij, j = 0, . . . , p, as candidates, obtaining \u02c6\u03b3j, and then selects the variable\nthat has the strongest impact on the improvement of the fit. a criterion is the improvement in\ndeviance:\n\ndev(\u02c6\u03b7(l)) \u2212 dev(\u02c6\u03b7new(j)),\n\nwhere \u02dc\u03b7new(j) is based on the parameter vector in which only the jth component is updated\nto \u02c6\u03b2(l)\n(l+1) = ( \u02c6\u03b2(l)\nj + \u02c6\u03b3j. when the sth variable is selected, the new parameter vector is \u02c6\u03b2\n0 ,\n. . . , \u02c6\u03b2(l)\ns + \u02c6\u03b3s, . . . )t . selection of the parameter that is actually updated within one step can\nalso be based on information criteria like aic or bic.\n\nin the case of the logit model, likelihood-based boosting is equivalent to the logitboost\nalgorithm for two classes that were proposed by friedman et al. (2000). the advantage of\ngenboost is that it applies to all kinds of link functions and exponential family responses. the\nfirst extension to the exponential family setting was given by ridgeway (1999). he gave two\nsimilar, though slightly different algorithms for boosting exponential family models (for details\nof fisher scoring-based algorithms, see also tutz and binder, 2007).\n\nexample 6.5: heart disease\nfigure 6.13 shows the coefficient buildups for likelihood-based boosting with 500 iterations. the left plot\nwas computed with the package gamboost, and the right plot uses the quadratic approximation used in\nthe package mboost. the resulting paths are quite similar; however, mboost is much faster.\n\nblockwise boosting\nthe strategy to update just one variable is rather limited, and it is especially inadequate if\ncategorical variables are in the predictor. a categorical predictor that takes k categories will be\nrepresented by k \u2212 1 dummy variables in the linear predictor. if one wants to avoid having the\nresulting selection depend on the coding scheme, the parameters for all the dummy variables\nrepresenting one variable should be refitted simultaneously. then the structured term to be fitted\nis \u03b7(xi, \u03b3) = xt\nir\u03b3r, where xir is a vector of dummy variables corresponding to a categorical\nvariable.\n\nin general, to obtain a selection of relevant terms, the base procedures that are used typically\ncontain only a small number of variables. in the extreme case only one coefficient is refitted; in\n\n "}, {"Page_number": 179, "text": "6.3. boosting methods\n\n167\n\nglmboost\n\nglmboost (mboost)\n\n5\n1\n\n0\n1\n\n5\n\n0\n\n5\n\u2212\n\nage\n\nfamhist\ntypea\nldltobacco\n\nsbpadiposity\n\nalcohol\n\nobesity\n\n5\n1\n\n0\n1\n\n5\n\n0\n\n5\n\u2212\n\nage\n\nfamhist\ntypea\nldltobacco\n\nsbp\nadiposity\n\nalcohol\n\nobesity\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\nfigure 6.13: glmboost coefficient paths for heart disease data (package mboost (right),\nvertical line shows estimate selected by 10-fold cross-validation).\n\nother cases, it can be a group of coefficients. a list of parametrically structured terms that may\nbe fitted is\n\n\u2022 \u03b7(xi, \u03b3) = xir\u03b3r, which specifies the linear effect of the rth covariate;\n\n\u2022 \u03b7(xi, \u03b3) = \u03b30 + xir\u03b3r, which specifies the intercept and the linear effect of the rth\n\ncovariate;\n\n\u2022 \u03b7(xi, \u03b3) = xt\n\nical variable;\n\nir\u03b3r, where xir is a vector of dummy variables corresponding to a categor-\n\n\u2022 \u03b7(xi, \u03b3) = xirxis\u03b3rs, representing an interaction between the rth and the sth covariates;\n\n\u2022 \u03b7(xi, \u03b3) = xirxt\n\nr \u03b3rs, representing an interaction between the rth variable and the sth\n\ncategorical variable given by a vector of dummy variables.\n\nthese parametrically structured terms define the learner that is used. in general, the pa-\nrameter \u03b3 is a vector that refers to a group or block of variables that define the design matrix\n\u02dcx. the update by adding \u02c6\u03b3 is done blockwise. when one fits all one-covariate models with\nintercepts \u03b7(xi, \u03b3) = \u03b30 + \u03b3rxir, r = 1, . . . , p, as candidates, after selection of the best update,\ns, the update is given by \u02c6\u03b2\n\n(l+1) = ( \u02c6\u03b2(l)\n\n0 + \u02c6\u03b30, . . . , \u02c6\u03b2(l)\n\nj + \u02c6\u03b3s, . . . )t .\n\nin addition, in some cases it is not sensible to let the procedure select among all the vari-\nables. for example, in treatment studies, the treatment should be considered as a mandatory\nvariable that is always included in the predictor. therefore, one should distinguish between\nmandatory and optional predictors. the more general concept of blockwise boosting allows\none to distinguish between these types of variables and to refit groups of variables.\n\nblockwise (or partial) boosting means that in the lth iteration selected components of the\nparameter vector are re-estimated. the selection is determined by a specific structuring of the\nparameters (variables). let the parameter indices v = {1, . . . , p} be partitioned into disjoint\nsets by v = vc \u222a vo1 \u222a . . . \u222a voq, where vc stands for the (mandatory) parameters (variables)\nthat have to be included in the analysis, and vo1, . . . , voq represent blocks of parameters that are\noptional. a block vor may refer to all the parameters that refer to a multicategorical variable,\nsuch that not only parameters but variables are evaluated. candidates in the refitting process are\nall combinations vc \u222a vor, r = 1, . . . , q, representing combinations of necessary and optional\nvariables. componentwise boosting that refers to single coefficients is the special case where\nvc = \u2205, voj = {j}.\n\n "}, {"Page_number": 180, "text": "168 chapter 6. regularization and variable selection for parametric models\n\nthe base procedure that is used in refitting steps refers to the fitting of a shrinked estimator\n(weak learner) to the candidate sets and the selection of the \"least\" candidate. as usual, fitting\nfor generalized linear models means maximizing the log-likelihood. since a shrinked version\nhas better performance, a shrinkage estimator, for example, the ridge estimator with large \u03bb,\nis used. moreover, since the boosting procedure itself means an iterative refitting of residuals,\nwithin one refitting step of the boosting algorithm we use one-step fisher scoring rather than a\ncomplete fit.\nmore technically, let vm = vc \u222a vom denote the indices of parameters to be considered for\nrefitting and x vm denote the corresponding submatrix of the full design matrix (x.1, . . . , x.p).\nthen the partial boosting algorithm is given by:\n\nblockwise/partial likelihood boosting (genpartboostr)\n\nstep 1: initialization\n\nfit model \u03bci = h(\u03b20) by iterative fisher scoring to obtain \u02c6\u03b2\n\u02c6\u03b7(0) = x \u02c6\u03b2\n\n(0)\n\n.\n\n(0) = ( \u02c6\u03b20, 0, . . . , 0)t ,\n\nstep 2: iteration\n\nfor l = 1, 2, . . .\n\n(a) estimation: estimation for candidate sets vm, m = 1, . . . , q, corresponds to fitting\n\nof the model\n\n\u03bc = h(\u02c6\u03b7(l\u22121) + x vm\u03b3vm),\n\nwhere \u02c6\u03b7(l\u22121) = x \u02c6\u03b2\nformed by one step of fisher scoring by use of a weak learner for \u03b3vm.\n\nis treated as an offset (fixed constant). fitting is per-\n\n(l\u22121)\n\n(b) selection: for candidate sets vm, m = 1, . . . , q, the set vm0 is selected that im-\n\nproves the fit maximally.\n\n(c) update: one sets\n\n(cid:2)\n\n\u02c6\u03b3(l) =\n\n\u02c6\u03b3vm,j\n0\n(l\u22121) + \u02c6\u03b3(m), \u02c6\u03b7(l) = x \u02c6\u03b2\n\n(l)\n\nj \u2208 vm0\nj /\u2208 vm0,\n\n(l) = \u02c6\u03b2\n\u02c6\u03b2\ncomponentwise.\n\n, \u02c6\u03bc(l) = h(x \u02c6\u03b2\n\n(l)), where h is applied\n\nexample 6.6: abortion of treatment\nwe illustrate the application of simple ridge boosting and partial boosting with real data from 344 admis-\nsions at a psychiatric hospital. the (binary) response variable to be investigated is whether treatment is\naborted by the patient against physicians\u2019 advice (about 55% for this group of patients). from a total of 101\nvariables available, 8 variables likely to be relevant were identified: age, number of previous admissions\n(\u201cnoupto\u201d), cumulative length of stay (\u201closupto\u201d) and the 0/1-variables indicating a previous ambulatory\ntreatment (\u201cprevamb\u201d), no second diagnosis (\u201cnosec\u201d), second diagnosis \u201cpersonality disorder\u201d (\u201cpers-\ndis\u201d), somatic problems (\u201csomprobl\u201d), homelessness (\u201chomeless\u201d), and joblessness (\u201cjobless\u201d). based on\nsubject matter considerations, the two variables that relate to secondary diagnosis (\u201cnosec\u201d and \u201cpersdis\u201d)\nare mandatory members of the response set and no penalty is applied to their estimates. this illustrates\n\n "}, {"Page_number": 181, "text": "6.3. boosting methods\n\n169\n\nfigure 6.14: coefficient buildups for abortion of treatment data when using lasso (upper\nleft), ridge (upper right), and boosted ridge (lower panel) with mandatory variables.\n\nthe effect of augmenting an unpenalized model with a few mandatory variables with optional predictors.\nfigure 6.14 shows the coefficient buildup in the course of the boosting steps for partial boosting contrasted\nwith simple ridge boosting (lower panel) and the lasso (upper left panel). the arrows indicate the number\nof steps chosen by aic (for partial boosting and simple ridge boosting) and 10-fold cross-validation (for\nthe lasso repeated 10 times). it can be seen that the mandatory components introduce a very different\nstructure in coefficient buildups. one interesting feature is the slow decrease of the estimate for \u201cpersdis\u201d\nbeginning with boosting step 8. this indicates some overshooting of the initial estimate that is corrected\nwhen additional predictors are included. to identify relevant variables we used all 101 predictors and\ndivided the data into a training set of size 270 and a test set of size 74. the lasso (with cross-validation)\nreturned 13 predictors with a prediction error of 0.392. partial boosting (using six mandatory response set\nelements relating to the secondary diagnosis) with penalty varying from 500 to 10000 returned 15 to 19\npredictors and a prediction error between 0.378 and 0.392.\n\nvariable selection by regularization is a very active research area. modifications and im-\nprovements are proposed and properties of existing estimates are investigated in many journals.\ntherefore, consideration of advantages and disadvantages tends to be preliminary. neverthe-\nless, in table 6.1 some properties of currently available procedures are listed.\n\n "}, {"Page_number": 182, "text": "170 chapter 6. regularization and variable selection for parametric models\n\ntable 6.1: properties of regularized estimators.\n\nridge\n\nestimates exist. prediction performance better than for ml estimates.\nexplicit solution for linear model.\n\nno selection of predictors.\n\nlasso\n\nselects predictors, sparse representation. oracle properties hold for adaptive lasso.\n\ntends to select one predictor from a group of highly correlated predictors.\nnot necessarily consistent (but adaptive lasso is).\n\nelastic net\n\nselects predictors. shows the grouping property.\n\ntwo tuning parameters have to be selected.\n\noscar\n\nselects predictors, exact grouping property. clustering of predictors available.\n\ntwo tuning parameters have to be selected.\n\ncorrelation-based\n\ngrouping property.\n\nscad\n\ndoes not select predictors (boosted version does)\n\nnearly unbiased for large unknown coefficients.\nautomatically sets small estimated coefficients to zero (sparsity).\ncontinuous in the data to avoid instability in model prediction (continuity).\noracle property.\n\ndantzig selector\n\nvariable selection included. can be computed efficiently.\n\ncomponentwise\n\nselects variables.\n\nboosting\n\ninference hard to obtain.\n\n6.4 simultaneous selection of link function and\n\npredictors\n\nwhen predictors are selected one typically assumes that the link function is known. how-\never, if the assumed link function is wrong, the performance of the selection procedures can\nbe strongly affected. for illustration, let us consider a small simulation study. let the gen-\nerating model be a poisson model with the true response function having sigmoidal form\nht (\u03b7) = 10/(1 + exp(\u22125 \u00b7 \u03b7)). let the parameter vector of length p = 20 be given by\n\u03b2t = (0.2, 0.4, \u22120.4, 0.8, 0, . . . , 0) and covariates be drawn from a normal distribution\nx \u223c n(0p, \u03c3) with \u03c3 = {\u03c3ij}i, j\u2208{1, ..., p}, where \u03c3ij = 0.5, i (cid:8)= j, \u03c3ii = 1. we generate\nn = 50 datasets with n = 200 observations and fit the model by using the usual maximum\nlikelihood (ml) procedure based on the canonical log-link (without variable selection). in addi-\ntion, we apply three alternative fitting methods that include variable selection: a non-parametric\nflexible link procedure considered in the following, the lasso for generalized linear models, and\na componentwise boosting procedure. while the flexible link procedure selects a link function,\nml estimates as well as lasso and boosting use the canonical link. it is seen from figure 6.15\nthat the best results are obtained if the link function is estimated non-parametrically. in particu-\nlar, the parameters of the predictors that are not influential are estimated more stable and closer\nto zero. the dominance of the flexible procedure is also seen in figure 6.16, which shows the\nmean squared error for the estimation of the parameter vector and the predictive deviance on an\nindependently drawn test dataset with n = 1000.\n\nthe flexible procedure shown in figure 6.15 is an extension of the non-parametric estima-\ntion procedure considered in section 5.2. one fits the model \u03bci = h0(h(\u03b7i)), where h0(.)\nis a fixed transformation function, for example, the canonical link, and the inner function\n\n "}, {"Page_number": 183, "text": "6.4. simultaneous selection of link function and predictors\n\n171\n\nflex link\n\nmboost \n\n0\n1\n\n.\n\n5\n0\n\n.\n\n0\n0\n\n.\n\n.\n\n5\n0\n\u2212\n\n0\n.\n1\n\n5\n.\n0\n\n0\n.\n0\n\n5\n.\n0\n\u2212\n\n0\n1\n\n.\n\n5\n0\n\n.\n\n0\n0\n\n.\n\n.\n\n5\n0\n\u2212\n\n1\n\n3\n\n5\n\n7\n\n9 11\n\n14\n\n17\n\n20\n\n1\n\n3\n\n5\n\n7\n\n9 11\n\n14\n\n17\n\n20\n\nlasso\n\nmle\n\n0\n.\n1\n\n5\n.\n0\n\n0\n.\n0\n\n5\n.\n0\n\u2212\n\n1\n\n3\n\n5\n\n7\n\n9 11\n\n14\n\n17\n\n20\n\n1\n\n3\n\n5\n\n7\n\n9 11\n\n14\n\n17\n\n20\n\nfigure 6.15: resulting estimates of coefficient vector in simulation study for flexible\nlink, boosting, lasso, and ml.\n\n\u03b2\ne\ns\nm\n\n2\n1\n0\n\n.\n\n0\n\n8\n0\n0\n\n.\n\n0\n\n4\n0\n0\n\n.\n\n0\n\n0\n0\n0\n\n.\n\n0\n\n0\n0\n0\n4\n\n0\n0\n5\n3\n\n0\n0\n0\n3\n\n0\n0\n5\n2\n\n0\n0\n0\n2\n\n0\n0\n5\n1\n\n0\n0\n0\n1\n\ne\nc\nn\na\nv\ne\nd\n\ni\n\nflex link mboost\n\nlasso\n\nmle\n\nflex link mboost\n\nlasso\n\nmle\n\nfigure 6.16: mean squared error for parameter vector and predictive deviance for simu-\nlation setting.\n\n "}, {"Page_number": 184, "text": "(cid:14)\n\n172 chapter 6. regularization and variable selection for parametric models\n\nm\n\n(l)\n\n(l)\n\nh(.) is considered as unknown and is estimated by assuming an expansion in basis functions\nh(\u03b7i) =\n\nj=1 \u03b1j\u03c6j(\u03b7i) = \u03b1t \u03c6i.\n\ni = (\u03c6(l)\n\nestimates are obtained by iteratively estimating the regression coefficients \u03b2 and the param-\ndenote the parameter\nn )t with\n1 , . . . , \u03c6(l)\ni ))t is the current design matrix for the basis functions. within a\n\neters of the link function \u03b1. in matrix notation, let \u02c6\u03b2\nestimate and the fitted predictor in the lth step. moreover, \u03c6(l)\n\u03c6(l) = (\u03c61(\u02c6\u03b7(l)\nboosting-type procedure two steps are iterated:\n\ni ), . . . , \u03c6m(\u02c6\u03b7(l)\n\nand \u02c6\u03b7(l) = x \u02c6\u03b2\n\nboosting for fixed predictor. for a fixed predictor \u02c6\u03b7(l\u22121) = x \u02c6\u03b2\n\n, the estimation of the\nresponse function corresponds to fitting the model \u03bc = h0((\u03c6(l\u22121))t \u02c6\u03b1(l\u22121) +(\u03c6(l\u22121))t \u02c6a(l)),\nwhere \u03c6(l\u22121))t \u02c6\u03b1(l\u22121) is a fixed offset that represents the previously fitted value. one step of\npenalized fisher scoring has the form\n(cid:24)\n\u02c6a(l) = \u03bdh((\u03c6(l\u22121))t \u02c6d\nt \u02c6d\n\n(l\u22121))\u22121 \u02c6d\n(l\u22121))\u22121(y \u2212 \u02c6\u03bc(l\u22121)),\n\n(l\u22121)\u03c6(l\u22121) + \u03bbhph)\u22121\u00b7\n\n(l\u22121)( \u02c6\u03c3\n(l\u22121)( \u02c6\u03c3\n\n\u03c6(l\u22121)\n\n(l\u22121)\n\n(cid:23)\n\n\u00b7\n\ni\n\ni\n\n(l\u22121)\n\nwhere \u02c6d(l\u22121) = diag(\u2202h0(\u02c6h(l\u22121)(\u02c6\u03b7(l\u22121)\n)/\u2202h(l\u22121)(\u03b7)) is the estimate of the derivative matrix\nevaluated at the estimate of the previous step and \u02c6\u03c3\nis the diagonal matrix of variances\nevaluated at h0(\u02c6h(l\u22121)(\u03b7(l\u22121)\n)). ph is the penalty matrix that penalizes the second derivation\nof the estimated (approximated) response function and the shrinkage parameter is fixed by\n\u03bdh = 0.1.\ncomponentwise boosting for fixed response function. let h(.) be fixed and the design\nmatrix have the form x = (x1|...|xp) with corresponding response vector y = (y1, ..., yn)t .\ncomponentwise boosting means to update one parameter within one boosting step. therefore,\none fits the model \u03bc = h0(h(x \u02c6\u03b2\nis a fixed offset and only the\nvariable xj is included in the model. then penalized fisher scoring for the parameter bj has\nthe form\n\n(l\u22121) + xjbj)), where x \u02c6\u03b2\n\n(l\u22121)\n\n\u02c6b(l)\nj = \u03bdp(xt\n\nj\n\n\u02c6d\n\n(l\u22121)\n\u03b7\n(l\u22121)\n\u03b7\n\n(l\u22121)\n\u03b7\n\n(l\u22121))\n\nxj)\n\n\u22121 \u02c6d\n\n\u22121xt\n( \u02c6\u03c3\n= diag(\u2202h0(\u02c6h(l\u22121)(\u02c6\u03b7(l\u22121)\n(l\u22121)\n\nj\n\nwhere \u03bdp = 0.1, \u02c6d\nated at the values of the previous iteration and \u02c6\u03c3\n\ni\n\n(l\u22121)\n\u03b7\n\n\u02c6d\n\n( \u02c6\u03c3\n\n(l\u22121))\n\n\u22121(y \u2212 \u02c6\u03bc(l\u22121)),\n\n))/\u2202\u03b7) is the matrix of derivatives evalu-\n\nis the variance from the previous step.\n\nin each step of the boosting algorithm it is decided if the regression coefficients or the\ncoefficients of the basis functions are updated; for details see tutz and petry (2011). the\nadvantage of boosting techniques is that variable selection is included. by updating only one\nof the coefficients and stopping the updating procedure appropriately, one obtains the relevant\npredictors.\n\nexample 6.7: demand for medical care\nin example 7.6, count data with the number of physician office visits as the response variable were con-\nsidered. here we consider a poisson model with flexible link and the same predictors as in example 7.6.\nfigure 6.17 shows the estimated response functions plotted against the linear predictor. the canonical log-\nlink is a strictly increasing function while the non-parametrically estimated response function becomes flat\nfor large values of the predictor. the canonical link seems not to be appropriate. this is supported by\nan improved prediction error in subsamples when the flexible link function is used (see tutz and petry,\n2011).\n\n "}, {"Page_number": 185, "text": "6.5. categorical predictors\n\n173\n\nflex link\n\nmle\n\n0\n3\n\n5\n2\n\n0\n2\n\n5\n1\n\n0\n1\n\n5\n\n0\n\n \n\n0\n3\n\n5\n2\n\n0\n2\n\n5\n1\n\n0\n1\n\n5\n\n0\n\n \n\n\u22122\n\n\u22121\n\n0\n\n1\n\neta\n\n2\n\n3\n\n4\n\n0.5\n\n1.0\n\n2.0\n\n2.5\n\n1.5\n\neta\n\nfigure 6.17: response functions for medical care data against linear predictor: flexible\nlink function (left) and canonical log link (right).\n\n6.5 categorical predictors\nthe selection of categorical variables has already been briefly discussed for lasso-type penalties\nand boosting approaches. in the following we will consider further approaches. when predic-\ntors are categorical selection should distinguish between two cases: the selection of variables\nand the selection of effects within variables. if one wishes to select variables one has to select\ngroups of variables because a categorical variable is represented by several dummy variables.\nif, however, one enforces selection among all terms in the linear predictor, including dummies,\na selection strategy like the lasso will select single dummy variables with the effect that the\ncoding scheme that has been used on the categorical predictors determines the result. to avoid\nsuch effects one should distinguish between the two problems:\n\n\u2022 which categorical predictors should be included in the model?\n\n\u2022 which categories within one categorical predictor should be distinguished?\n\nthe latter problem is concerned with one single variable and poses the question of which\ncategories differ from one another with respect to the dependent variable. or, to put it in a\ndifferent way, which categories should be collapsed? the answer to that question depends on\nthe scale level of the predictor; one should distinguish between nominal and ordered categories\nbecause of their differing information content. we will first consider selection strategies for\neffects within variables and then the selection of variables as groups of possibly regularized\neffects.\n\n6.5.1 selection within categorical predictors\nlet us first consider just one categorical predictor a \u2208 {1, . . . , k}, which is included in the\npredictor by the use of dummy variables in the form \u03b7 = \u03b20 +\nj xa(j)\u03b2j. then, when\ncomputing a penalized estimate, for example, by use of a lasso-type penalty,\n\n(cid:14)\n\n(cid:7)\n\nj(\u03b2) =\n\n|\u03b2j|,\n\nthe shrinkage effect depends on the coding scheme that is used. for simplicity, let the cate-\ngorical predictor a have only three categories, a \u2208 {1, 2, 3}, which are coded by two dummy\n\nj\n\n "}, {"Page_number": 186, "text": "174 chapter 6. regularization and variable selection for parametric models\n\nvariables xa(2), xa(3). if (0\u20131)-coding (xa(j) = 1 if a = j and xa(j) = 0 otherwise) is used,\nshrinkage refers to the difference between the first and second categories and the difference\nbetween the third and first categories, since the first category is implicitly used as a reference\n(\u03b2a(1) = 0). if effect coding is used (xa(j) = 1 if a = j, xa(j) = \u22121 if a = k, xa(j) = 0\notherwise), shrinkage refers to the effect of factor levels with the global level across categories\nas the reference point because implicitly \u03b2a(1) + \u03b2a(2) + \u03b2a(3) = 0 is assumed. therefore, the\nselection of parameters, which is enforced by the lasso penalty, yields parameters that depend\non the coding scheme. if a parameter, say |\u03b2j|, is set to zero, that means that in the case of\n(0\u20131)-coding, category j and the reference category cannot be distinguished. but collapsing\nthe categories always refers to the reference category; all other possible combinations of cate-\ngories are ignored. to allow collapsing of any two categories, alternative penalties, which are\nconsidered in the following, have to be used.\n\n(cid:7)\n\nclustering of categories for nominal predictor\nfor nominal predictor variables with many categories, a useful strategy is to search for clusters\nof categories with similar effects. the objective is to reduce the k categories to a smaller number\nof categories that form clusters; the effect of categories within one cluster is supposed to be the\nsame, but responses will differ across clusters. with (0\u20131)-coding and reference category 1,\nthat is, \u03b21 = 0, a fusion-type penalty that enforces clustering is\n\nj(\u03b2) =\n\nwij|\u03b2i \u2212 \u03b2j| = w21|\u03b22| + \u00b7\u00b7\u00b7 + wk1|\u03b2k| + w32|\u03b23 \u2212 \u03b22| + . . . ,\n\n(6.11)\n\ni>j\n\nwhere wij is an additional weight that may depend on the sample sizes within the categories.\nthe penalty enforces the selection among effects \u03b8ij = \u03b2i \u2212 \u03b2j, i = 1, . . . , k \u2212 1, i > j.\nsince the ordering of the dummy varables xa(1), . . . , xa(k) is arbitrary, all differences \u03b2i \u2212 \u03b2j\nare used. for large \u03bb, the penalty \u03bbj(\u03b2) tends to form clusters of categories; for \u03bb \u2192 \u221e, all\nparameter estimates become zero and the categorical predictor is excluded.\n\nthe penalty is very useful when the predictor has many categories. for small sample size\nas compared to the number of categories, the ml estimate becomes unstable.\nin contrast,\nregularized estimates are much more stable, and with the selection effect of the l1-penalty on\ndifferences they allow one to form clusters.\n\nordered categories\nan interesting case is selection strategies for ordered predictors. ordered categories contain\nmore information than unordered categories, but the information has not been used in penalty\n(6.11). since now the ordering of dummy coefficients is meaningful, a useful penalty for (0-1)-\ncoding is\n\nj(\u03b2) =\n\nwi|\u03b2i \u2212 \u03b2i\u22121| = w2|\u03b22| + w3|\u03b23 \u2212 \u03b22| + . . . ,\n\n(6.12)\n\nk(cid:7)\n\ni=2\n\nwith \u03b21 = 0. by putting the l1-penalty on differences of adjacent categories, the procedure\ntends to fuse adjacent categories and select groups of categories that may actually be distin-\nguished. therefore, an ordered categorical predictor with many categories is reduced to a cate-\ngorical predictor that is formed by the resulting clusters of categories. typically the number of\nclusters is much smaller than the original number of categories. for \u03bb \u2192 \u221e, all parameter esti-\nmates will become zero and the predictor is excluded since categories cannot be distinguished.\nit should be noted that the penalty can also be given in the simpler form of a (weighted)\nlasso when split-coding of predictors is used. when using the split-coded predictors \u02dcxa(1), . . . ,\n\n "}, {"Page_number": 187, "text": "6.5. categorical predictors\n\n175\n\n(cid:14)\n\u02dcxa(k\u22121) with \u02dcxa(i) = 1 if a > i and \u02dcxa(i) = 0 otherwise, the corresponding penalty for\nparameters \u02dc\u03b21 = \u03b22, \u02dc\u03b22 = \u03b23 \u2212 \u03b22, \u02dc\u03b2k\u22121 = \u03b2k \u2212 \u03b2k\u22121 used in the predictor \u03b7 = \u03b20 +\n\nk\u22121\ni=1 \u02dcxa(i)\n\n\u02dc\u03b2i is\n\nk\u22121(cid:7)\n\nwi| \u02dc\u03b2j|,\n\nwhich is a weighted lasso-type penalty. for the transformation between (0-1)-coding and split-\ncoding see also section 4.4.3.\n\nj=1\n\nboth penalties, (6.11) for nominal predictors and (6.12) for ordinal predictors, show good\nclustering properties and allow one to reduce the number of categories. fusion methodology\ngoes back at least to land and friedman (1997). the penalty for ordered categories is a modifi-\ncation of the fused lasso penalty proposed by tibshirani et al. (2005) (see also section 10.4.4).\nthe penalty for nominal predictors was considered by bondell and reich (2009) and gertheiss\nand tutz (2010). a further advantage of these penalties is that they have desirable asymptotic\nproperties.\nlet us consider nominal factors first. let \u03b8 = (\u03b821, \u03b831, . . . , \u03b8k,k\u22121)t denote the vector of\npairwise differences \u03b8ij = \u03b2i \u2212 \u03b2j. furthermore, let c = {(i, j) : \u03b2\nj , i > j} denote the\n\u2217\n\u2217\ni that are truly\nset of indices i > j corresponding to differences of (true) dummy coefficients \u03b2\nnon-zero, and let cn denote the set corresponding to those difference that are estimated to be\n\u2217\nc denote the true vector of pairwise differences included in\nnon-zero with sample size n. let \u03b8\nc, and \u02c6\u03b8c the corresponding estimate based on \u02c6\u03b2. moreover, let weights have the form\n\n(cid:8)= \u03b2\n\n\u2217\ni\n\nwij = \u03c6ij(n)| \u02c6\u03b2(ls)\n\ni\n\n|\u22121,\n\nj\n\n\u2212 \u02c6\u03b2(ls)\n\u221a\n\ni\n\ndenotes the ordinary least-squares estimates, and for increasing n one has \u03c6ij(n) \u2192\nwhere \u02c6\u03b2(ls)\nqij (0 < qij < \u221e) for all i, j. if \u03bb = \u03bbn with \u03bbn/\nn \u2192 0 and \u03bbn \u2192 \u221e, and all class-wise\nsample sizes ni satisfy ni/n \u2192 ci, where 0 < ci < 1, then one obtains for the linear model\n\u221a\nn(\u02c6\u03b8c \u2212 \u03b8\nc) \u2192d n(0, \u03c3) (for specific matrix \u03c3 ) and limn\u2192\u221e p (cn = c) = 1. therefore,\n\u2217\nasymptotically the right clusters are identified.\n} denote\na similar property holds for ordered predictors. now let c = {i > 1 : \u03b2\n\u2217\nthe set of indices corresponding to the differences of neighboring (true) dummy coefficients \u03b2\nthat are truly non-zero, and again let cn denote the set corresponding to those differences that\ni\nare estimated to be non-zero. the vector of first differences \u03b4i = \u03b2i \u2212 \u03b2i\u22121, i = 2, . . . , k, is\n\u2217\nnow denoted as \u03b4 = (\u03b42, . . . , \u03b4k)t . in analogy to the unordered case, let \u03b4\nc denote the true\nvector of (first) differences included in c, and \u02c6\u03b4c the corresponding estimate. with weights\n\n(cid:8)= \u03b2\n\n\u2217\ni\u22121\n\n\u2217\ni\n\nwi = \u03c6i(n)| \u02c6\u03b2(ls)\n\ni\n\n\u2212 \u02c6\u03b2(ls)\ni\u22121\n\n|\u22121\n\nand the same conditions for \u03c6i(n) and n as for nominal factors, one obtains\nn(0, \u03c3) and limn\u2192\u221e p (cn = c) = 1 (see gertheiss and tutz, 2010. ).\n\n\u221a\nn(\u02c6\u03b4c \u2212 \u03b4\n\nc) \u2192d\n\u2217\n\n6.5.2 selection of variables combined with clustering of categories\nwhen several categorical predictors are available, with the lth variable having categories 1, . . . , kl\nl = (\u03b2l1, . . . , \u03b2lkl), a combination of variable selec-\nand the corresponding parameter vector \u03b2t\ntion and clustering is obtained by the penalty\n\np(cid:7)\n\nl=1\n\nj(\u03b2) =\n\njl(\u03b2l),\n\n(6.13)\n\n "}, {"Page_number": 188, "text": "(cid:7)\n\n176 chapter 6. regularization and variable selection for parametric models\n\n(cid:7)\n\nwith\n\njl(\u03b2l) =\n\n|\u03b2li \u2212 \u03b2lj|,\n\nw(l)\nij\n\nor\n\njl(\u03b2l) =\n\n|\u03b2li \u2212 \u03b2l,i\u22121|,\n\nw(l)\ni\n\ni>j\n\ni\n\ndepending on the scale level of the predictor xl. the first expression refers to nominal covari-\nates, the second to ordinal ones. at first sight the penalty seems to enforce clustering only.\nhowever, since \u03b2l1 = 0 is fixed for l = 1, . . . , p, a predictor is automatically excluded if all\nof its categories form one cluster. due to the (additive) form of the penalty, theoretic results\ngeneralize to the case of multiple categorial inputs. for ordered categories the grouping into\nadjacent categories is enforced, while for unordered categories the clustering into not neces-\nsarily adjacent categories is enforced. with an appropriately chosen smoothing parameter, one\nautomatically selects variables and the relevant information within variables.\nj )/n}1/2, where n(l)\n\nin applications the weight function has to be specified. bondell and reich (2009) proposed\ni denotes the number\n\nij = (kl + 1)\u22121{(n(l)\n\ni + n(l)\n\nweights determined by w(l)\nof observations on level i of predictor xl.\n\nexample 6.8: munich rent\nall larger german cities compose so-called rent standards to obtain a decision-making instrument avail-\nable to tenants, landlords, renting advisory boards, and experts. these rent standards are used in particular\nfor the determination of the local comparative rent. for the composition of the rent standards, a repre-\nsentative random sample is drawn from all relevant households. the data analyzed here come from 2053\nhouseholds interviewed for the munich rent standard 2003. the response is monthly rent per square meter\nin euro. the predictors are ordered as well as unordered and also include binary factors. they include ur-\nban district (nominal, labeled by numbers 1, . . . , 25), year of construction (ordered classes [1910, 1919],\n[1920, 1929], . . .), number of rooms (taken as an ordinal factor with levels 1, 2, . . . , 6), quality of resi-\ndential area (ordinal, with levels \"fair,\" \"good,\" \"excellent\"), floor space (square meters, given in ordered\nclasses (0, 30), [30, 40), [40, 50), . . ., [140,\u221e)), hot water supply (binary, yes/no), central heating (bi-\nnary, yes/no), tiled bathroom (binary, yes/no), supplementary equipment in bathroom (binary, yes/no), and\nwell-equipped kitchen (binary, yes/no). the data can be downloaded from the data archive of the depart-\nment of statistics at the university of munich (http://www.stat.uni-muenchen.de/service/datenarchiv). in\norder to find relevant variables and identify clusters of categories that have the same effect on the predictor\nregularized estimates with penalty term (6.13) are computed. with weights chosen by cross-validation,\nthe only predictor that is completely excluded from the model is the binary factor, which indicates if sup-\nplementary equipment in the bathroom is available. however, some categories of nominal and ordinal\npredictors are clustered, for example, houses constructed in the 1930s and 1940s, or urban districts 14, 16,\n22, and 24. the original 25 districts of munich have been reduced to merely 10 categories, which have\ndiffering rent levels (see table 6.2). the regularization paths given in figure 1.4 show how categories are\ncombined. for the districts, which are treated as nominal, any combination of categories is allowed. for\nthe year of construction ordering over decades is assumed. the regularization paths show how adjacent\ncategories are fused to build clusters. a map of munich with clusters (figure 6.18) illustrates the 7 found\nclusters.\n\n6.5.3 selection of variables\n\nthe selection of whole variables refers to the selection of groups of corresponding dummy\nvariables. within the framework of boosting, the selection of groups is easily obtained by\nthe use of blockwise boosting, where blocks refer to one categorical predictor. an alternative\nstrategy is the group lasso considered in section 6.2.2. it tends to select the whole group of\n\n "}, {"Page_number": 189, "text": "6.5. categorical predictors\n\n177\n\ntable 6.2: estimated regression coefficients for munich rent standard data using adaptive\nweights with refitting, and (cross-validation score minimizing) s/smax = 0.61.\n\nlabel\n\ncoefficient\n\npredictor\n\nintercept\n\nurban district\n\nyear of construction\n\nnumber of rooms\n\nquality of residential area\n\nfloor space (m2)\n\nno\nhot water supply\nno\ncentral heating\ntiled bathroom\nno\nsuppl. equipment in bathroom yes\nwell equipped kitchen\nyes\n\n14, 16, 22, 24\n11, 23\n7\n8, 10, 15, 17, 19, 20, 21, 25\n6\n9\n13\n2, 4, 5, 12, 18\n3\n\n1920s\n1930s, 1940s\n1950s\n1960s\n1970s\n1980s\n1990s, 2000s\n\n4, 5, 6\n3\n2\n\ngood\nexcellent\n[140, \u221e)\n[90, 100), [100, 110), [110, 120),\n[120, 130), [130, 140)\n[60, 70), [70, 80), [80, 90)\n[50, 60)\n[40, 50)\n[30, 40)\n\n12.597\n\u22121.931\n\u22121.719\n\u22121.622\n\u22121.361\n\u22121.061\n\u22120.960\n\u22120.886\n\u22120.671\n\u22120.403\n\u22121.244\n\u22120.953\n\u22120.322\n0.073\n0.325\n1.121\n1.624\n\u22120.502\n\u22120.180\n0.000\n\n0.373\n1.444\n\u22124.710\n\u22123.688\n\u22123.443\n\u22123.177\n\u22122.838\n\u22121.733\n\u22122.001\n\u22121.319\n\u22120.562\n0.506\n1.207\n\ncoefficients linked to one categorical predictor. the basic group lasso uses the penalty\n\ng(cid:7)\n\n(cid:16)\n\nj(\u03b2) =\n\ndfj(cid:16)\u03b2j\n\n(cid:16)2,\n\nj=1\n\n(cid:16)2 = (\u03b22\n\nwhere (cid:16)\u03b2j\npartitioned predictor xt\nshrunken (by use of a ridge-type penalty), but there is no selection effect within the group.\n\n)1/2 and \u03b2j denotes the parameter of the jth group from the\ni,g). thus the group of coefficients collected in \u03b2j is\n\nj1 +\u00b7\u00b7\u00b7 + \u03b22\ni = (xt\n\nj,dfj\ni1, . . . , xt\n\nfor ordered categories one can incorporate smoothing across categories by using the trans-\nformation from the preceding section. with (0-1)-coding and the first category as a reference\ncategory of a predictor with kj categories, one replaces the term (cid:16)\u03b2j\n\n(cid:16)2 in the penalty by\n\n(cid:16)\u03b2t\n\nj k j\u03b2j\n\n(cid:16)2,\n\n "}, {"Page_number": 190, "text": "178 chapter 6. regularization and variable selection for parametric models\n\n\u22121.9307\n\n0\n\nfigure 6.18: map of munich indicating clusters of urban districts; colors correspond to\nestimated dummy coefficients from table 6.2.\n\nj dj and dj is the ((kj \u22121)\u00d7(kj \u22121))-matrix given in equation (4.9) without\nwhere k j = dt\nj = (\u03b2j2, . . . , \u03b2jk). as shown in section 4.4.3, the penalty can be\nthe first column and \u03b2t\ntransformed into a penalty that uses split-coding of covariates. the transformation is helpful\nbecause then software designed for the group lasso can be used to fit the model (for more details\nand examples see gertheiss et al., 2011).\n\n6.6 bayesian approach\nin bayesian approaches to regularization, a prior distribution p(\u03b8) is specified together with a\nsampling model p(y|\u03b8) for observations y. then the updated knowledge after the data have\nbeen seen is given by the posterior distribution\n\n\u2019\n\np(\u03b8|y) = p(y|\u03b8)p(\u03b8)\np(y|\u03b8)p(\u03b8)d\u03b8\n\n\u221d p(y|\u03b8)p(\u03b8).\n\nin regression modeling, typically the unknown parameter is the vector of coefficients \u03b2 and one\nobtains\n\np(\u03b2|y) \u221d p(y|\u03b2)p(\u03b2).\n\nthe posterior mode estimator, which maximizes the posterior distribution, may be obtained by\nmaximizing the logarithm of the posterior, yielding\n\nargmax\u03b2 = argmax\u03b2(l(\u03b2) + log(p(\u03b2))),\n\nwhere l(\u03b2) = log(p(y|\u03b2)) is the log-likelihood. therefore, the posterior mode estimator is\nequivalent to the penalized likelihood estimator for an appropriately chosen prior distribution.\nfor the gaussian prior \u03b2 \u223c n(0, \u03c4 2i), the log-prior has the form of the ridge penalty,\n\nwhich, apart from additive constants, has the form\n\nlog(p(\u03b2)) = \u2212 1\n\n2\u03c4 2 \u03b2t \u03b2.\n\n "}, {"Page_number": 191, "text": "6.8. exercises\n\n179\n\nfor the normal distribution linear model with fixed variance \u03c32, the posterior is given by\n\nwhere \u03bb = \u03c32/\u03c4 2. the assumption of an iid laplace prior (also called double-exponential\nprior), which has density\n\n\u03b2|y \u223c n((x t x + \u03bbi)\n\n\u22121),\n\n\u22121x t y, \u03c32(x t x + \u03bbi)\np(cid:15)\n\nyields, apart from constants, the lasso penalty\n\nj=1\n\np(\u03b2) =\n\n\u2212\u03bb|\u03b2j|\n\n\u03bb\n2 e\np(cid:7)\n\nlog(p(\u03b2)) = \u2212\u03bb\n\n|\u03b2j|.\n\nj=1\n\ntherefore, frequentist regularization corresponds to posterior mode estimation if all the other\nparameters, for example dispersion parameters, are fixed. however, from a bayesian point\nof view, maximizing the posterior is not the best way to obtain estimates. a fully bayesian\napproach will use the mean or median of the posterior to estimate the coefficient vector. the\nbayesian lasso, proposed by park and casella (2008), uses posterior median estimates. it does\nnot automatically perform variable selection but provides standard errors and bayesian credible\nintervals that can be used to select variables. park and casella (2008) used the representation of\nthe double-exponential distribution as a mixture of normals to generate a simple gibbs sampler.\na direct representation of the posterior distribution was used by hans (2009). an overview\nof regularization techniques, including bridge regression penalties and elastic net penalties, is\ngiven by fahrmeir and kneib (2009).\n\n6.7 further reading\nalternative methods and surveys. one of the first methods to obtain variable selection by\nshrinkage was the non-negative garotte, proposed by breiman (1995). although it is consistent\n(zou, 2006), its performance is often poor in highly correlated settings. an overview on regu-\nlarization in linear models is found in hastie, tibshirani, and friedman (2009). more recently,\nb\u00fchlmann and van de geer (2011) gave a thorough mathematical treatment of regularization\nmethods.\n\nr packages. lasso and elastic-net regularized generalized linear models can be fitted with\nthe r package glmnet, which allows one to fit gaussian, binomial, poisson, and multinomial\nresponses (friedman et al., 2008). an alternative is glmpath (park and hastie, 2007), which\nfits models with gaussian, binomial, and poisson responses. the package penalized (goeman,\n2010) is designed for the cox model but also fits logit and poisson distribution models. the\ngroup lasso for metric response, the logit, and the log-linear poisson model can be fitted by use\nof the r package grplasso (meier et al., 2008). the package ordpens is able to handle ordinal\npredictors. boosting is available in the packages mboost (hothorn et al., 2009) and gamboost.\n\n6.8 exercises\n\n6.1 consider the simple linear regression model.\n\n(a) give the lasso and ridge estimators as functions of the ml estimate for a one-dimensional predictor.\n\n(b) give the lasso and ridge estimators as functions of the ml estimate for an orthogonal design.\n\n "}, {"Page_number": 192, "text": "180 chapter 6. regularization and variable selection for parametric models\n\n6.2 show by using a second-order taylor approximation that the log-likelihood of a glm at the maximum\n2 (\u03b2 \u2212 \u02c6\u03b2m l)t f (\u02c6\u03b2m l)(\u03b2 \u2212 \u02c6\u03b2m l), where\nlikelihood estimate \u02c6\u03b2m l can be approximated by l(\u02c6\u03b2m l) + 1\nf (\u02c6\u03b2m l) denotes the fisher matrix.\n\n6.3 the birth weight data, which have also been considered by hosmer and lemeshow (1989) and ven-\nables and ripley (2002), are available from the r package mass (dataset birthwt). the data contain 189\nobservations that were collected at baystate medical center, springfield, massachusetts, during 1986.\nthe binary response is an indicator of birth weight less than 2.5 kg. the predictor variables are mother\u2019s\nage in years (age), mother\u2019s weight in pounds at last menstrual period (lwt), mother\u2019s race (1 = white, 2 =\nblack, 3 = other), smoking status during pregnancy (smoke), number of previous premature labours (ptl),\nhistory of hypertension (ht), presence of uterine irritability (ui), and number of physician visits during the\nfirst trimester (ftv).\n\n(a) use regularized estimates to fit a binary regression model; use in particular ml, ridge, lasso,\nscad, elastic net, and boosting. r packages that might be useful are mentioned in the previous\nsection.\n\n(b) compare the performance of the fitting methods in terms of prediction error by splitting the dataset\n\nseveral times into training data (for fitting) and test data (for evaluation of prediction).\n\n "}, {"Page_number": 193, "text": "chapter 7\n\nregression analysis of count data\n\nin many applications the response variable is given in the form of event counts, where an event\ncount refers to the number of times an event occurs. simple examples are\n\n\u2022 number of insolvent firms within a fixed time internal,\n\n\u2022 number of insurance claims within a given period of time,\n\n\u2022 number of epileptic seizures per day,\n\n\u2022 number of cases with a specific disease in epidemiology.\n\nin all of theses examples the response y may be viewed as a non-negative integer-valued random\nvariable with y \u2208 {0, 1, 2, . . .}. although in many applications there is an upper bound for the\nresponse, because the number of firms or potential insurance claims is finite, the upper bound\nis often very large and considered as irrelevant in modeling. in other cases, for example, for the\nnumber of epileptic seizures, no upper bound is given. in the following some further examples\nare given.\n\nexample 7.1: number of children\nthe german general social survey allbus provides micro data, which allow one to model the dependence\nof the number of children on explanatory variables. we will consider women only and the predictors age\nin years (age), duration of school education (dur), nationality (nation, 0: german, 1: otherwise), religion\n(answer categories to \"god is the most important in man\", 1: strongly agree,. . . , 5: strongly disagree, 6:\nnever thought about it), university degree (univ, 0: no, 1: yes).\n\nexample 7.2: encephalitis\nin a study on the occurrence of encephalitis in central europe (karimi et al., 1998), the number of cases\nof herpes encephalitis in children was observed between 1980 and 1993 in bavaria and lower saxony.\ntable 7.1 shows the resulting counts.\n\nthe count data in example 7.2 form a contingency table. therefore, one might think of\nusing tools for the analysis of contingency tables, a topic that is treated extensively in chapter\n12. classical analysis of contingency tables treats the rows and columns as factors and thus\ndoes not not use the full information available in the potential predictors. it seems more appro-\npriate to model time as a metric variable rather than a qualitative variable. then the regression\nproblem is determined by the qualitative explanatory variable country and the metric covariate\ntime.\n\n181\n\n "}, {"Page_number": 194, "text": "182\n\nchapter 7. regression analysis of count data\n\ntable 7.1: encephalitis infection in children.\n\nbavaria\n\nlower saxony\n\n1980\n1981\n1982\n1983\n1984\n1985\n1986\n1987\n1988\n1989\n1990\n1991\n1992\n1993\n\n1\n0\n1\n2\n2\n3\n8\n5\n13\n12\n6\n13\n10\n12\n\n2\n1\n2\n5\n4\n\u2013\n\u2013\n6\n7\n7\n7\n3\n4\n2\n\nthe benchmark model for count data is the poisson distribution. therefore, we will first\nconsider the poisson regression model, which can be treated within the framework of gener-\nalized linear models. one of the disadvantages of the poisson distribution is that it is a one\nparameter distribution. consequently, the poisson regression is frequently not flexible enough\nto adapt to the given data. one step to more flexible models is the inclusion of an overdis-\npersion parameter (section 7.5). an alternative, more flexible model is the negative binomial\nmodel, which can be motivated as a mixture model (section 7.6). models for data that show\noverdispersion through excess zeros are considered in section 7.7 and section 7.8.\n\n7.1 the poisson distribution\nthe poisson distribution is a standard model for count data and was derived as a limiting case\nof the binomial by poisson (1837). the discrete random variable y is poisson-distributed with\nintensity or rate parameter \u03bb, \u03bb > 0, if the density is given by\n\n(cid:2)\n\np (y = y) =\n\n\u2212\u03bb\n\n\u03bby\ny! e\n0\n\nfor y \u2208 {0, 1, 2, . . .}\notherwise.\n\n(7.1)\n\nan abbreviation is y \u223c p(\u03bb). figure 7.1 shows several examples of densities of poisson-\ndistributed variables. a closer look at the form of the densities is obtained by considering the\nproportion of probabilities for counts y and y \u2212 1 (y \u2265 1) given by\n\np (y = y)\np (y = y \u2212 1)\n\n=\n\n\u2212\u03bb/y!\n\n\u03bbye\n\n\u03bby\u22121e\u2212\u03bb/(y \u2212 1)!\n\n= \u03bb\ny\n\n.\n\nif \u03bb < 1 one has \u03bb/y < 1 and therefore the density is decreasing across integers, the largest\nprobability occurs at y = 0. if \u03bb > 1, the probabilities are increasing up to the integer value\nof \u03bb, [\u03bb]; for y > [\u03bb] the density is decreasing. thus, for non-integer-valued \u03bb the density is\nunimodal with the mode given by [\u03bb]. if \u03bb is integer-valued, the probabilities p (y = \u03bb) and\np (y = \u03bb \u2212 1) are equal.\n\nthe first two central moments of the poisson distribution are given by e(y ) = var(y ) = \u03bb.\nequality of the mean and variances is often referred to as the equidispersion property of the\npoisson distribution. thus, in contrast to the normal distribution, for which the mean and\nvariance are unlinked, the poisson distribution implicitly models stronger variability for larger\n\n "}, {"Page_number": 195, "text": "7.1. the poisson distribution\n\n183\n\np(3)\n\n10\n\n11\n\n12\n\n0\n0 1 2 3 4 5 6 7 8 9\n\n2\n\n4\n\n6\n\n8\n\n0\n2\n\n.\n\n0\n\n5\n1\n\n.\n\n0\n\n0\n1\n\n.\n\n0\n\n5\n0\n0\n\n.\n\n0\n\n.\n\n0\n\np(0.5)\n\n10\n\n11\n\n12\n\np(15)\n\n0\n0 1 2 3 4 5 6 7 8 9\n\n2\n\n4\n\n6\n\n8\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n0\n1\n0\n\n.\n\n6\n0\n0\n\n.\n\n2\n0\n\n.\n\n0\n\n0\n\n.\n\n0\n\n0\n\n10\n\n20\n\n30\n\nfigure 7.1: probability mass functions of poisson distributions with \u03bb = 0.5, 3, 15.\n\nmeans, a property that is often found in real-life data. on the other hand, in real-life data one\nfrequently finds that the variance exceeds the mean, and the effect is overdispersion, which has\nto be modeled separately (see section 7.5).\n\nin the following some connections between distributions and some additional properties\nof the poisson distribution are given. the connections between distributions help us to get a\nclearer picture of the range of applications of the poisson distribution.\n\npoisson distribution as the law of rare events\nthe poisson distribution may be obtained as a limiting case of the binomial distribution. let y\ndenote the total number of successes in a large number n of independent bernoulli trials with\nthe successes probability \u03c0 being small and linked to the number of trials by \u03c0 = \u03bb/n. as an\nexample, one may consider the number of incoming telephone calls in a fixed time internal of\nunit length. let \u03bb denote the fixed mean number of calls. now consider the division of the\ntime interval into n subintervals with equal width. for small intervals, each interval may be\nconsidered as one trial with the success being defined as an incoming call. then the number of\ntotal incoming calls can be modeled by a binomial distribution with the probability specified by\n\u03bb/n. then it may be shown that for increasing n the binomial distribution becomes the poisson\ndistribution.\nmore formally, let y have a binomial distribution with parameter n and \u03c0 = \u03bb/n, y \u223c\n\nb(n, \u03c0 = \u03bb/n). then one has\n\n(cid:25)\n\n(cid:26)\n\nn\ny\n\n\u03c0y(1 \u2212 \u03c0)n\u2212y = \u03bby\ny! e\n\n\u2212\u03bb.\n\nlim\nn\u2192\u221e\nn\u03c0=\u03bb\n\nthe law of rare events refers to this derivation from the binomial distribution, where the num-\nber of trials increases while the probability of success decreases correspondingly. however,\nthe term is somewhat misleading because the mean \u03bb may be arbitrarily large. the poisson\ndistribution is not restricted to small values of the mean.\n\n "}, {"Page_number": 196, "text": "184\n\nchapter 7. regression analysis of count data\n\nthe phone calls example is an idealization with some missing details. for example, in one\ntime subinterval more than one phone call could occur. a more concise derivation for this\nexample is obtained by considering poisson processes.\n\npoisson process\nthe poisson distribution is closely linked to the poisson process. let {n(t), t \u2265 0} be a\ncounting process with n(t) denoting the event counts up to time t. n(t) is a non-negative\nand integer-valued random variable, and the process is a collection of these random variables\nsatisfying the property that n(s) \u2264 n(t) if s < t. the poisson process is a specific counting\nprocess that has to fulfill several properties. with n(t, t + \u03b4t) denoting the number of counts\nin interval (t, t + \u03b4t), one postulates\n\n(a) independence of intervals\n\nfor disjunct time intervals (s, s + \u03b4s) and (t, t + \u03b4t), the increments n(s, s + \u03b4s) and\nn(t, t + \u03b4t) are independent.\n\n(b) stationarity\n\nthe distribution of the counts in the interval (t, t + \u03b4t) depends only on the length of the\ninterval \u03b4t (it does not depend on t).\n\n(c) intensity rate\n\nthe probability of no or one event in interval (t, t + \u03b4t) is given by\n\np (n(t, t + \u03b4t) = 1) = \u03bb\u03b4t + o(\u03b4t),\np (n(t, t + \u03b4t) = 0) = 1 \u2212 \u03bb\u03b4t + o(\u03b4t),\n\nwhere o(h) denotes a remainder term with the property o(h)/h \u2192 0 as h \u2192 0.\n\nfor this process, the number of events occurring in the interval (t, t + \u03b4t) is poisson-\n\ndistributed with mean \u03bb\u03b4t:\n\nn(t, t + \u03b4t) \u223c p(\u03bb\u03b4t).\n\nin particular, one has n(t) = n(0, \u03b4t) \u223c p(\u03bb\u03b4t). to obtain a poisson distribution in the\nphone call example one has to postulate not only that the probability of occurrence in one\ntime interval depends only on the length of the interval but also the independence of counts in\nintervals, stationarity, and that the probability that no or one event has a specific form in the\nlimit.\n\nthe poisson process is also strongly connected to the exponential distribution. if the poisson\nprocess is a valid model, the waiting time between events follows an exponential distribution.\nit is immediately seen that for the waiting time for the first event, w1, the outcome w1 > t\noccurs if no events occur in the interval (0, t):\n\np (w1 > t) = p (n(0, t) = 0) = e\n\n\u2212\u03bbt .\n\ntherefore, w1 follows the exponential distribution with parameter \u03bb. the same distribution\nmay be derived for the waiting time between any two events. alternative characterizations of\nthe poisson distributions are considered, for example, in cameron and trivedi (1998).\n\nfurther properties\n\n(1) sums of independent poisson-distributed random variables are poisson-distributed. more\ni yi \u223c\n\n(cid:14)\nconcrete, if yi \u223c p(\u03bbi), i = 1, 2, . . . , are independent and\np(\n\ni \u03bbi < \u221e, then\n\ni \u03bbi).\n\n(cid:14)\n\n(cid:14)\n\n "}, {"Page_number": 197, "text": "7.2. poisson regression model\n\n185\n\n(2) there is a strong connection between the poisson and the multinomial distribution. if\ny1, . . . , yn are independent poisson variables, yi \u223c p(\u03bbi), and one conditions on the\ntotal sum n0 = y1, +\u00b7\u00b7\u00b7 + yn , one obtains for (y1, . . . , yn ) given n0 the multinomial\ndistribution m(n0, (\u03c01, . . . , \u03c0n )) with \u03c0i = \u03bbi/(\u03bb1 + \u00b7\u00b7\u00b7 + \u03bbn ).\n\n(3) for large values of \u03bb the poisson distribution p(\u03bb) may be approximated by the normal\ndistribution n(\u03bb, \u03bb). it is seen from figure 7.1 that for small \u03bb the distribution is strongly\nskewed. for larger values of \u03bb the distribution becomes more symmetric. for details of\napproximations see, for example, mccullagh and nelder (1989), chapter 6.\n\n7.2 poisson regression model\nthe poisson regression model is the standard model for count data. let (yi, xi) denote n\nindependent observations and \u03bci = e(yi|xi). one assumes that yi|xi is poisson-distributed\nwith mean \u03bci, and that the mean is determined by\n\n\u03bci = h(xt\n\ni \u03b2)\n(7.2)\n\u22121 denotes the response function. since the poisson\nwhere g is a known link function and h = g\ndistribution is from the simple exponential family, the model is a generalized linear model. the\nmost widely used model uses the canonical link function by specifying\n\ng(\u03bci) = xt\n\ni \u03b2,\n\nor\n\n\u03bci = exp(xt\n\ni \u03b2)\n\nor\n\nlog(\u03bci) = xt\n\ni \u03b2.\n\nsince the logarithm of the conditional mean is linear in the parameters, the model is called a\nlog-linear model.\n\nthe log-linear version of the model is particulary attractive because interpreting the param-\neters is very easy. the model implies that the conditional mean given xt = (x1, . . . , xp) has a\nmultiplicative form given by\n\n\u03bc(x) = exp(xt \u03b2) = ex1\u03b21 . . . exp\u03b2p .\n\nthus e\u03b2j represents the multiplicative effect on \u03bc(x) if the variable xj changes by one unit to\nxj + 1 (given that the rest of the variables are fixed). one obtains\n\n\u03bc(x1, . . . , xj + 1, . . . , xp)\n\n\u03bc(x1, . . . , xj, . . . , xp)\n\n= e\u03b2j\n\nor, equivalently,\n\nlog \u03bc(x1, . . . , xj + 1, . . . , xp) \u2212 log \u03bc(x1, . . . , xj, . . . , xp) = \u03b2j.\n\nwhile \u03b2j is the change in log-means if xj increases by one unit, e\u03b2j is the multiplicative effect,\nwhich is easier to interpret because it directly effects upon the mean.\n\nfor illustration, let us consider example 1.5, where the dependent variable is the number of\n\ninsolvent firms and there is only one covariate, namely, time. the log-linear poisson model\n\nlog(\u03bc) = \u03b20 + time\u03b2\n\nspecifies the number of insolvent firms in dependence on time (1 to 36 for january 1994 to\ndecember 1996). one obtains the estimates \u02c6\u03b20 = 4.25 and \u02c6\u03b2 = 0.0097, yielding e \u02c6\u03b2 = 1.01.\ntherefore, the log-mean increases additively by 0.0097 every month or, more intuitively, the\nmean increases by the factor 1.01 every month.\n\n "}, {"Page_number": 198, "text": "186\n\nchapter 7. regression analysis of count data\n\nthe canonical link has the additional advantage that the mean \u03bc(x) is always positive\nwhatever values the (estimated) parameters take. for alternative models like the linear model,\n\u03bci = xt\ni \u03b2 problems may occur when the mean is predicted for new observations xi, although\n\u03bci may take admissible values in the original sample. therefore, in most applications one uses\nthe log-link. nevertheless, the results can be misleading if the link is grossly misspecified.\nwhen nothing is known about the link one can also use non-parametric approaches to link\nspecification (see section 5.2 and section 6.4).\n\n7.3 inference for the poisson regression model\nmaximum likelihood estimation\nfor inference, the whole machinery of generalized linear models may be used. for model (7.2)\none obtains the log-likelihood\n\nl(\u03b2) =\n\nyi log(\u03bci) \u2212 \u03bci \u2212 log(yi!) =\n\nyi log(h(xt\n\ni \u03b2)) \u2212 h(xt\n\ni \u03b2) \u2212 log(yi!).\n\nn(cid:7)\n\nn(cid:7)\n\nf (\u03b2) = e(\u2212\u22022l(\u03b2)/\u2202\u03b2\u2202\u03b2t ) =\n\n(cid:3)(xt\nh\nh(xt\n\ni \u03b2)2\ni \u03b2) .\n\n(7.3)\n\nfor the canonical link h(\u03b7) = exp(\u03b7), these terms simplify to\n\ni=1\n\ni=1\nthe score function s(\u03b2) = \u2202l(\u03b2)/\u2202\u03b2 is given by\n(cid:3)(xt\ni \u03b2)\nh\nh(xi\u03b2)\n\nn(cid:7)\n\ns(\u03b2) =\n\nxi\n\ni=1\n\nand the fisher matrix is given by\n\ni \u03b2)),\n\n(yi \u2212 h(xt\nn(cid:7)\n\nxixt\ni\n\ni=1\n\nn(cid:7)\nn(cid:7)\nn(cid:7)\n\ni=1\n\ni=1\n\nl(\u03b2) =\n\ns(\u03b2) =\n\nf (\u03b2) =\n\ni \u03b2 \u2212 exp(xt\n\ni \u03b2) \u2212 log(yi!),\n\nyixt\n\nxi(yi \u2212 exp(xt\n\ni \u03b2)),\n\nxixt\n\ni exp(xt\n\ni \u03b2).\n\ni=1\n\nunder regularity conditions, \u02c6\u03b2 defined by s(\u02c6\u03b2) = 0 is consistent and asymptotically normal\ndistributed:\n\n(a)\u223c n(\u03b2, f (\u03b2)\n\n\u22121),\n\n\u02c6\u03b2\n\nwhere f (\u03b2) may be replaced by f (\u02c6\u03b2) to obtain standard errors.\n\ndeviance and goodness-of-fit\nthe deviance as a measure of discrepancy between the fit and the data compares the log-\nlikelihood of the fitted value for observation yi, denoted by li(\u02c6\u03bci) = yi log(\u02c6\u03bci)\u2212 \u02c6\u03bci \u2212 log(yi!),\nto the log-likelihood of the perfect fit li(yi) = yi log(yi) \u2212 yi \u2212 log(yi!), yielding\n+ [(\u02c6\u03bci \u2212 yi)]}.\n\nli(\u02c6\u03bci) \u2212 li(yi) = 2\n\nd = \u22122\n\n{yi log\n\n(cid:7)\n\n(cid:7)\n\n(cid:25)\n\n(cid:26)\n\nyi\n\u02c6\u03bci\n\ni\n\ni\n\n "}, {"Page_number": 199, "text": "7.3. inference for the poisson regression model\n\n187\nif an intercept is included, the term in brackets, \u02c6\u03bci \u2212 yi, may be omitted. within the frame-\nwork of glms the deviance is used as a goodness-of-fit statistic with a known asymptotic\ndistribution when the observations are grouped. let yi1, . . . , yini, i = 1, . . . , n, denote inde-\npendent observations at a fixed measurement point xi with yit \u223c p(\u02dc\u03bci), \u02dc\u03bci = h(xt\ni \u03b2). then\nt=1 yit \u223c p(ni \u02dc\u03bci), which may be written as yi \u223c p(\u03bci), where \u03bci = ni \u02dc\u03bci.\nyi = ni\u00afyi =\nwhen using \u03bci, the deviance for grouped observations has the same form as for single observa-\ntions:\n\n(cid:14)\n\nni\n\nd = 2\n\n{yi log\n\n+ [(\u02c6\u03bci \u2212 yi)]}.\n\n(7.4)\n\nn(cid:7)\n\ni=1\n\n(cid:25)\n\n(cid:26)\n\nyi\n\u02c6\u03bci\n\nd is asymptotically \u03c72-distributed with n \u2212 p degrees of freedom, where p is the dimension\nof the parameter vector. the underlying asymptotic concept is fixed cells asymptotic, where\nn is fixed and ni \u2192 \u221e for all i. in the form (7.4), where ni is only implicitly contained\nin \u03bci = ni \u02dc\u03bci, one assumes \u03bci \u2192 \u221e. the assumption \u03bci \u2192 \u221e is slightly more general\nbecause one does not have to assume that yi is composed from repeated measurements. as an\nalternative goodness-of fit statistic one may also use the pearson statistic considered in chapter\n3. the specific form is given in the box.\n\ngoodness-of-fit for poisson regression model\n\n(cid:25)\n\n(cid:26)\n\nyi log\n\nyi\n\u02c6\u03bci\n(yi \u2212 \u02c6\u03bci)2\n\n\u02c6\u03bci\n\nn(cid:7)\nn(cid:7)\n\ni=1\n\ni=1\n\nd = 2\n\np =\n\u03c72\n\nfor \u03bci \u2192 \u221e one obtains the approximation\n\nd, \u03c72\np\n\n\u223c \u03c72(n \u2212 p)\n\nthe use of the deviance and the pearson statistic depends on whether asymptotic results\napply. usually one expects all of the means to be larger than three. fienberg (1980) showed\nthat the approximation might work even if for a small percentage the mean is only one; see also\nread and cressie (1988). if d and \u03c72\np are quite different, one might always suspect that the\napproximation is inadequate.\n\ntesting of hierarchical models\nthe deviance may also be used to test hierarchical models \u02dcm \u2282 m, where \u02dcm is determined by\na linear hypothesis c\u03b2 = \u03be. the difference between deviances for the fit of model \u02dcm (yielding\n\u02dc\u03b2 and \u02dc\u03bci) and model m (yielding \u02c6\u03b2 and \u02c6\u03bci) is\n\n(cid:7)\n\ni\n\nd( \u02dcm|m) = 2\n\nyi log(\n\n\u02dc\u03bci\n\u02c6\u03bci\n\n) + (\u02c6\u03bci \u2212 \u02dc\u03bci),\n\nwhich has asymptotically a \u03c72-distribution with the degrees of freedom given by the rank of c.\n\n "}, {"Page_number": 200, "text": "188\n\nchapter 7. regression analysis of count data\n\ntable 7.2: parameter estimates and log-linear poisson model for number of children.\n\nestimate\n\nstd. error\n\nz-value\n\npr(>|z|)\n\nintercept\nage\nage2\nage3\nage4\ndur\ndur2\nnation\ngod2\ngod3\ngod4\ngod5\ngod6\nuniv\n\n\u201312.280\n0.935\n\u20130.025\n0.000\n\u20130.000\n0.112\n\u20130.008\n0.056\n\u20130.010\n\u20130.144\n\u20130.128\n\u20130.036\n\u20130.092\n0.637\n\n1.484\n0.124\n0.004\n0.000\n0.000\n0.067\n0.003\n0.138\n0.059\n0.068\n0.071\n0.067\n0.075\n0.173\n\n\u20138.27\n7.55\n\u20136.57\n5.78\n\u20135.14\n1.68\n\u20132.77\n0.41\n\u20131.73\n\u20132.14\n\u20131.80\n\u20130.54\n\u20131.23\n3.68\n\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0929\n0.0054\n0.6816\n0.0826\n0.0327\n0.0711\n0.5886\n0.2182\n0.0002\n\ntable 7.3: deviances for poisson model, number of children as response.\n\ndf\n\ndifference\n\ndf\n\ndeviance\n\nall effects\nage\ndur\nnationality\nreligion\nuniv\n\n4\n2\n1\n5\n1\n\n215.5\n40.3\n0.2\n6.7\n13.5\n\n1747\n1751\n1749\n1748\n1752\n1748\n\n1718.6\n1940.7\n1758.9\n1718.8\n1725.3\n1732.1\n\nexample 7.3: number of children\nfor the data described in example 7.1 a log-linear poisson model with the number of children as the\ndependent variable has been fitted. table 7.2 shows the estimates. since it is hardly to be expected that\nthe metric predictors age and duration of school education have a linear effect, the polynomial terms are\nincluded in the predictor, which turn out to be highly significant. the only variable that seems to be not\ninfluential is nationality. however, the analysis of deviance, given in table 7.3, shows that the effect of\nthe variable religion is also not significant. the table gives the deviance of the model that contains all\nthe predictors and the differences between deviances when one predictor is omitted. since the effects of\npolynomial terms are hard to see from the estimates, the effects are plotted in figure 7.2, with all other\ncovariates considered as fixed. it is seen that especially for women between 20 and 40, age makes a\ndifference as far as the expected number of children is concerned. the duration of school years shows that\nunfortunately the time spent in school decreases the number of children. for a more flexible modelling\nsee example 10.4, chapter 10.\n\nexample 7.4: encephalitis\nfor the encephalitis dataset, the number of infections is modeled in dependence on country (bav=1:\nbavaria, bav=0: lower saxony) and time (1\u201314, corresponding to 1980\u20131993). the compared mod-\nels are the log-linear poisson model, the normal distribution model with log-link and the identity link.\ntable 7.4 shows the fits with an interaction effect between country and time. when using a normal dis-\ntribution model, the log-linear model is to be preferred because its log-likelihood is larger. comparison\nacross distributions cannot be recommended because log-likelihoods are not comparable. the points in\n\n "}, {"Page_number": 201, "text": "7.3. inference for the poisson regression model\n\n189\n\n0\n2\n\n.\n\n5\n\n.\n\n1\n\n0\n\n.\n\n1\n\n5\n0\n\n.\n\nn\ne\nr\nd\n\nl\ni\n\nh\nc\n\n \nf\n\no\n\n \nr\ne\nb\nm\nu\nn\n\ni\n\nr\no\nt\nc\nd\ne\nr\np\n\n \nr\na\ne\nn\nl\n\ni\n\n5\n\n.\n\n0\n\n0\n\n.\n\n0\n\n.\n\n5\n0\n\u2212\n\n0\n\n.\n\n1\n\u2212\n\n5\n\n.\n\n1\n\u2212\n\nn\ne\nr\nd\n\nl\ni\n\nh\nc\n\n \nf\n\no\n\n \nr\ne\nb\nm\nu\nn\n\n0\n\n.\n\n2\n\n5\n\n.\n\n1\n\n0\n\n.\n\n1\n\n5\n\n.\n\n0\n\n20\n\n40\n\n60\n\n80\n\n5\n\n10\n\n15\n\n20\n\nage\n\nduration of school education\n\ni\n\nr\no\nt\nc\nd\ne\nr\np\n\n \nr\na\ne\nn\nl\n\ni\n\n5\n\n.\n\n0\n\n0\n0\n\n.\n\n.\n\n5\n0\n\u2212\n\n0\n\n.\n\n1\n\u2212\n\n20\n\n40\n\n60\n\n80\n\n5\n\n10\n\n15\n\n20\n\nage\n\nduration of school education\n\nfigure 7.2: number of children versus age and duration of education\n\ntable 7.4: models for encephalitis data.\n\nlog-linear poisson model\n\nlog-linear normal model\n\nlinear normal model\n\nestimate\n\np-value\n\nestimate\n\np-value\n\nestimate\n\np-value\n\nintercept\ntime\ntime2\nbav\nbav.time\n\n-0.255\n0.513\n-0.030\n-1.587\n0.211\n\n0.622\n0.000\n0.0001\n0.006\n0.003\n\n-0.223\n0.499\n-0.029\n-1.478\n0.198\n\n0.705\n0.0002\n0.0002\n0.017\n0.001\n\n0.397\n1.154\n-0.065\n-4.414\n0.853\n\n0.815\n0.014\n0.030\n0.014\n0.000\n\nlog-likelihood\n\n-47.868\n\n-51.398\n\n-54.905\n\nfavor of the poisson model are that data are definitely discrete and the equidispersion property of the\npoisson model. it is seen from figure 7.3 that large means tend to have larger variability.\n\nexample 7.5: insolvent firms\nfor the number of insolvent firms between 1994 and 1996 (see example 1.5) a log-linear poisson model\nis fitted with time as the predictor. time is considered as a number from 1 to 36, denoting months, starting\nwith january 1994 and ending with december 1996. since the counts are not too small, one might also\nfit a model that assumes normally distributed responses. the models that are compared are the log-linear\nmodel log(\u03bc) = \u03b20 + x\u03b21 and the model log(\u03bc) = \u03b20 + x\u03b21 + x2\u03b22 with x \u2208 {1, . . . , 36}.\n\n "}, {"Page_number": 202, "text": "190\n\ns\nn\no\n\ni\nt\nc\ne\n\nf\n\nn\n\ni\n \nf\n\no\n\n \nr\ne\nb\nm\nu\nn\n\n2\n1\n\n0\n1\n\n8\n\n6\n\n4\n\n2\n\nchapter 7. regression analysis of count data\n\nlower saxony\n\nbavaria\n\ns\nn\no\n\ni\nt\nc\ne\n\nf\n\nn\n\ni\n \nf\n\no\n\n \nr\ne\nb\nm\nu\nn\n\n2\n1\n\n0\n1\n\n8\n\n6\n\n4\n\n2\n\n1980 1982\n\n1984\n\n1986\n\n1988\n\n1990\n\n1992\n\n1980\n\n1982\n\n1984\n\n1986\n\n1988\n\n1990\n\n1992\n\nyear\n\nyear\n\nfigure 7.3: estimated means against time for log-linear poisson model for encephalitis\ndata, lower saxony (upper panel) and bavaria (lower panel).\n\nthe results given in table 7.5 show that the quadratic terms seem to be unnecessary. nevertheless, in\n\nfigure 7.4, which shows the mean response against months, the quadratic term is included.\n\ntable 7.5: log-linear poisson models for insolvency data.\n\nestimate\n\nstandard error\n\nz-value\n\n\u03b20\n\u03b21\n\u03b22\n\n4.192\n0.020\n\u20130.00027\n\n0.062\n0.007\n0.00019\n\n67.833\n2.677\n\u20131.408\n\n0\n2\n1\n\n0\n0\n1\n\n0\n8\n\n0\n6\n\nl\n\nv\no\ns\nn\n\ni\n\nl\n\ni\n\ns\na\nu\nd\ns\ne\nr\n \nd\ne\nr\na\nu\nq\ns\n\n0\n0\n8\n\n0\n0\n6\n\n0\n0\n4\n\n0\n0\n2\n\n0\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\n70\n\n75\n\n80\n\n85\n\n90\n\n95\n\ncase\n\nfitted values\n\nfigure 7.4: log-linear model for insolvency data plotted against months (left) and\nsquared residuals (right) against fitted values.\n\n7.4 poisson regression with an offset\nin the standard log-linear poisson model it is assumed that the log-mean of the response depends\ndirectly on the covariates in linear form, log(\u03bci) = xt\ni \u03b2. in many applications, the response\nresults from different levels of aggregation, and it is more appropriate to model the underlying\n\n "}, {"Page_number": 203, "text": "7.4. poisson regression with an offset\n\n191\n\ndriving force. for example, in epidemiology, if the incidence of infectious diseases is studied,\ncount data may refer to geographical districts with varying population sizes. thus the number\nof people at risk has to be taken into account. the same effect is found if the counts are\nobserved in different time intervals. let us consider the latter case. then, one can use the\nstrong connection between the poisson distribution and the poisson process as described in\nsection 7.1. by assuming a poisson process with intensity rate \u03bb, one obtains for the counts in\nintervals of length \u03b4 the poisson distribution y \u223c p(\u03bb\u03b4). consequently, the mean \u03bc = \u03bb\u03b4\ndepends on the length of the interval. let data be given by (yi, xi), where yi denotes the\ncounts in intervals of length \u03b4i.\nif counts arise from a poisson process with intensity rate\n\u03bbi (depending on xi), one obtains yi \u223c p(\u03b4i\u03bbi). if the dependence of the intensity rate is\nmodeled in log-linear form, log(\u03bbi) = xt\n\ni \u03b2, one obtains for mean counts \u03bci = e(yi|xi)\n\nlog(\u03bci) = log(\u03b4i) + xt\n\ni \u03b2\n\n(7.5)\n\nor, equivalently,\n\n\u03bci = exp(log(\u03b4i) + xt\n\ni \u03b2).\n\ntherefore, the specification of the driving force, represented by \u03bbi, yields a model with a fixed\nterm log(\u03b4i) in it. for an application, see santner and duffy (1989).\n\na similar form of the model results if one has different levels of aggregation. let inde-\npendent responses yi1, . . . , yini be observed for fixed values of explanatory variables xi. if\nyit \u223c p(\u03bbi), one obtains for the sum of responses\n\nni(cid:7)\n\nyi =\n\nyit = ni\u00afyi \u223c p(ni\u03bbi),\n\nt=1\n\nwhere \u00afyi = yi/n. since the local sample sizes ni vary across measurements, one obtains with\nlog(\u03bbi) = xt\n\ni \u03b2 for the mean responses \u03bci = e(yi|xi)\n\nlog(\u03bci) = log(ni) + xt\n\ni \u03b2.\n\n(7.6)\n\nthe number ni can be given implicitly, for example, as the population size in a given geo-\ngraphical district when one looks at the number of cases of a specific disease. from the form\nlog(\u03bci/ni) = xt\ni \u03b2 it is seen that the appropriately standardized rate of occurrence is modeled\nrather than the number of cases itself.\n\nmodels (7.5) and (7.6) have the general form\n\nlog(\u03bci) = \u03b3i + xt\n\ni \u03b2,\n\n(cid:14)\n\nwhere \u03b3i is a known parameter \u03b3i = log(\u03b4i) for the varying length of time intervals, and\n\u03b3i = log(ni) for varying sample sizes. the parameter \u03b3i may be treated as an offset that\nremains fixed across interactions. the log-likelihood and the score function are given by l(\u03b2) =\ni \u03b2)).\n\ni \u03b2) \u2212 log(yi!) and s(\u03b2) =\n\ni xi(yi \u2212 exp(\u03b3i + xt\n\ni yi log(\u03b3i + xt\n\n(cid:14)\n\nthe fisher matrix has the form\n\ni \u03b2) \u2212 exp(\u03b3i + xt\nn(cid:7)\n\nn(cid:7)\n\nf (\u03b2) =\n\nxixt\n\ni exp(\u03b3i + xt\n\ni \u03b2)) =\n\nxixt\n\ni exp(xt\n\ni \u03b2) exp(\u03b3i).\n\ni=1\n\ni=1\n\nit is directly seen how \u03b3i determines the accuracy of the estimates. since the ml estimate\n\u02c6\u03b2 has approximate covariance cov(\u02c6\u03b2) \u2248 f (\u02c6\u03b2)\u22121, standard errors decrease with increasing\nparameters \u03b3i. as is to be expected, larger time intervals or larger sample sizes yield better\nestimates.\n\n "}, {"Page_number": 204, "text": "192\n\nchapter 7. regression analysis of count data\n\n7.5 poisson regression with overdispersion\nin many applications count data are overdispersed, with the conditional variance exceeding\nthe conditional mean. one cause for this can be unmodeled heterogeneity among subjects.\nin the following several modeling approaches that account for overdispersion are considered.\nthe first one is based on quasi-likelihood, and the second one models the heterogeneity among\nsubjects explicitly. a specific model that models heterogeneity explicitly is the gamma-poisson\nor negative binomial model considered in section 7.6. also, models for excess zeros like the\nzero-inflated model (section 7.7) and the hurdle model (section 7.8) imply overdispersion.\n\n7.5.1 quasi-likelihood methods\ni \u03b2) and yi|xi \u223c p(\u03bci).\nmaximum likelihood estimates are based on the assumption \u03bci = h(xt\nthe estimates are obtained by setting the score function equal to zero. these assumptions may\nbe weakened within the quasi-likelihood framework (see section 3.11, chapter 3). the link\nbetween the mean and the linear predictor has the usual glm form \u03bci = h(xt\ni \u03b2), but instead\nof assuming a fixed distribution for yi only a mean\u2013variance relationship is assumed. the ml\nestimation equation s(\u02c6\u03b2) = 0 has the general form\nyi \u2212 \u03bci\n\u03c32\ni\n\nn(cid:7)\n\n\u2202\u03bci\n\u2202\u03b7\n\n= 0,\n\n(7.7)\n\nxi\n\ni=1\n\ni is the variance that has the form \u03c32\n\nwhere \u03bci = h(\u03b7i) and \u03c32\ni = \u03c6v(\u03bci) with variance function\nv(\u03bci). since \u03c6 cancels out, the estimation depends only on the specification of the mean and\nthe variance function. for the poisson distribution, the latter has the form v(\u03bci) = \u03bci. for\nalternative variance functions, which do not necessarily correspond to a poisson distribution,\n(7.7) is considered as the estimation equation yielding quasi-likelihood estimates.\n\nmodel with overdispersion parameter\na simple quasi-likelihood approach uses the variance function v(\u03bci) = \u03bci, which yields the\ni = \u03c6\u03bci for some unknown constant \u03c6. the case \u03c6 > 1 represents the overdispersion\nvariance \u03c32\nof the poisson model, and the case \u03c6 < 1, which is rarely found in applications, is called the\ni = \u03c6\u03bci is used in (7.7), \u03c6 drops out. thus the estimation equation is\nunderdispersion. if \u03c32\nidentical to the likelihood equation for poisson models. consequently, parameter estimates are\nidentical. however, the variance is inflated by overdispersion, since one obtains the asymptotic\ncovariance\n\ncov(\u02c6\u03b2) = \u03c6f (\u03b2)\n\n\u22121,\n\nwith f (\u03b2) denoting the fisher matrix from equation (7.3). wedderburn (1974) proposed esti-\nmating the dispersion parameter by\n\n\u02c6\u03c6 =\n\n1\nn \u2212 p\n\n(yi \u2212 \u02c6\u03bci)2\n\n\u02c6\u03bci\n\n,\n\nn(cid:7)\n\ni=1\n\nwhere p is the number of model parameters and n \u2212 p is a degrees-of-freedom correction. the\nmotivation for this estimator is that the variance function v(\u03bci) = \u03bci implies e(yi\u2212\u03bci)2 = \u03c6\u03bci\nand hence \u03c6 = e((yi \u2212 \u03bci)2/\u03bci). \u02c6\u03c6 is motivated as a moment estimator with a degrees-\nof-freedom correction. there is a strong connection to the pearson statistic, because \u02c6\u03c6 =\np /(n \u2212 p). the approximation e(\u03c72\np ) \u2248 n \u2212 p holds if \u03c32 = \u03bc is the underlying variance.\n\u03c72\nfor \u03c32 = \u03c6\u03bc, one has e(\u03c72\n\np /\u03c6) \u2248 n \u2212 p and therefore e(\u03c72\n\np /(n \u2212 p)) \u2248 \u03c6.\n\n "}, {"Page_number": 205, "text": "7.5. poisson regression with overdispersion\n\n193\n\nin summary, the variance \u03c32 = \u03c6\u03bc is easy to handle. one fits the usual poisson model\nand uses the ml estimate. to obtain the correct covariance matrix of \u02c6\u03b2 one multiplies the\nmaximum likelihood covariance by \u02c6\u03c6. maximum likelihood standard errors are multiplied by\n\n*\n\n*\n\n\u02c6\u03c6 and t-statistics are divided by\n\n\u02c6\u03c6.\n\nalternative variance functions\nalternative variance functions usually continue to model the variance as a function of the mean.\na general variance function that is in common use has the form\n\nv(\u03bci) = \u03bci + \u03b3\u03bcm\ni\n\nfor a fixed value of m. the choice m = 2 corresponds to the assumption of the negative\nbinomial distribution (see section 7.6) while the choice m = 1 yields v(\u03bci) = (1 + \u03b3)\u03bci.\nhence, the case m = 1 is equivalent to assuming v(\u03bci) = \u03c6\u03bci. breslow (1984) used the\nnegative binomial type variance within a quasi-likelihood approach.\n\n(2008). like zeileis et al.\n\nexample 7.6: demand for medical care\ndeb and trivedi (1997) analyzed the demand for medical care for individuals, aged 66 and over, based on\na dataset from the u.s. national medical expenditure survey in 1987/88. the data are available from the\narchive of the journal of applied econometrics and the journal of statistical software; see also kleiber\nand zeileis (2008), and zeileis et al.\n(2008) we consider the number of\nphysician/non-physician office and hospital outpatient visits (ofp) as dependent variable. the regressors\nused in the present analysis are the number of hospital stays (hosp), self-perceived health status (poor,\nmedium, excellent), number of chronic conditions (numchron), age, marital status, and number of years\nof education (shool). since the effects vary across gender, only male patients are used in the analysis.\ntable 7.6 shows the fits of a log-linear poisson model without and with overdispersion (residual deviance\nis 9665.7 on 1770 degrees of freedom). with an estimated overdispersion parameter \u02c6\u03c6 = 7.393 the\ndata are highly overdispersed. the negative binomial model (table 7.8) shows similar effects but slightly\nsmaller standard errors (\u02c6\u03bd = 1.079, with standard error 0.048, and the residual deviance is \u22129607.73).\nthe poisson model yields the log-likelihood value \u22127296.398 (df = 8), and the negative binomial model,\nwhich uses just one more parameter, reduces the likelihood to \u22124803.867(df = 9).\n\ntable 7.6: log-linear poisson and quasi-poisson models for health care data (males).\n\nestimate\n\npoisson\nstd. error\n\nquasi-poisson\n\np-value\n\nstd. error\n\np-value\n\nintercept\nhosp\nhealthpoor\nhealthexcellent\nnumchron\nage\nmarried[yes]\nschool\n\n0.746\n0.188\n0.221\n\u20130.229\n0.153\n0.004\n0.132\n0.043\n\n0.136\n0.009\n0.030\n0.045\n0.007\n0.017\n0.027\n0.003\n\n0.000\n0.000\n0.000\n0.000\n0.000\n0.833\n0.000\n0.000\n\n0.370\n0.025\n0.081\n0.122\n0.020\n0.047\n0.073\n0.008\n\n0.044\n0.000\n0.006\n0.060\n0.000\n0.938\n0.072\n0.000\n\nin the preceding example overdispersion was found, which occurs quite frequently in ap-\nplications. an exception is the log-linear model for the number of children in example 7.3.\nwhen fitting a log-linear model with variance \u03c6\u03bci, one obtains the estimate \u02c6\u03c6 = 0.847, which\nmeans weak underdispersion. therefore, p-values for the corresponding parameter estimates\nare slightly smaller than the values given in table 7.2.\n\n "}, {"Page_number": 206, "text": "194\n\nchapter 7. regression analysis of count data\n\n7.5.2 random effects model\none possible cause for overdispersion in the poisson model is unobserved heterogeneity among\nsubjects. a way of handling heterogeneity is to model it explicitly. it is assumed that the mean\nof observation yi is given by\n\n\u03bbi = bi\u03bci = bi exp(xt\n\ni \u03b2),\n\n(7.8)\n\nwhere bi is a subject-specific parameter, which is itself drawn from a mixing distribution. it\nrepresents the heterogeneity of the population that is not captured by the observed variables xi.\nthe model assumption yi \u223c p(\u03bbi) is understood conditionally for given bi (and xi).\n\nmodel (7.8) may also be written in the form\n\n\u03bbi = exp(ai + xt\n\ni \u03b2),\n\nwhere ai = log(bi) is a random intercept within the linear predictor. with f(bi) denoting the\ndensity of bi, the marginal probability of the response value yi is obtained in the usual way as\n\n)\n\np (yi) =\n\nf(yi|bi) f(bi)dbi.\n\nthere are various ways of specifying the distribution of bi and ai, respectively. hinde (1982)\nassumes a normal distribution for ai = log(bi). then bi follows the log-normal distribution.\nfor the specific normal distribution ai \u223c n(\u2212\u03c32/2, \u03c32) one obtains for bi the mean e(bi) = 1\nand the variance var(bi) = exp(\u03c32) \u2212 1. in particular e(bi) = 1 is a sensible choice for the\nmodel (7.8), where \u03bbi is given by \u03bbi = bi exp(xt\ni \u03b2), since then the log-linear poisson model\nis the limiting case when the variance of bi tends to zero. dean et al. (1989) consider a random\neffects model by using the inverse normal distribution. an alternative choice that yields an\nexplicit marginal distribution is based an the gamma mixing distribution. the corresponding\npoisson-gamma model is considered in the next section.\n\nin general, maximization of the marginal likelihood is computationally intensive because\nthe integrals have to be approximated, for example, by gauss-hermite integration (see chapter\n14).\n\n7.6 negative binomial model and alternatives\nquasi-likelihood methods seem to provide a sufficiently flexible tool for the estimation of\noverdispersed count data. the assumptions are weak; by using only the first two moments\none does not have to specify a distribution function. nevertheless, parametric models have\nadvantages. in particular, they are useful as building blocks of mixture models as considered\nin sections 7.7 and 7.8. the type of overdispersion found in these mixture models cannot be\nmodeled within the framework of quasi-likelihood methods. therefore, in the following we\nwill consider alternative parametric models.\n\nthere are several distribution models that are more flexible than the poisson model but\ninclude it as a limiting case. a frequently used model is the negative binomial distribution.\nin contrast to the poisson distribution, it is a two-parameter distribution and therefore more\nflexible than the poisson model; in particular, it can model overdispersed counts. in the fol-\nlowing we first consider the negative binomial model, which can be derived as a mixture of\npoisson distributions. the second extension that will be considered is the generalized poisson\ndistribution.\n\n "}, {"Page_number": 207, "text": "7.6. negative binomial model and alternatives\n\n195\n\nnegative binomial model as gamma-poisson-model\na specific choice for the mixing distribution in model (7.8), which allows a closed form of\nthe marginal distribution, is the gamma-distribution. the gamma-distribution bi \u223c \u03b3(\u03bd, \u03b1) is\ngiven by the density\n\n(cid:2)\n\nf(bi) =\n\n0\n\u03b3(\u03bd) b\u03bd\u22121\n\u03b1\u03bd\n\ni\n\ne\u2212\u03b1bi\n\nbi \u2264 0\nbi > 0.\n\nthe mean and variance are e(bi) = \u03bd/\u03b1, var(bi) = \u03bd/\u03b12. if one assumes for the random\nparameter bi the gamma-distribution \u03b3(\u03bd, \u03bd), the mean fulfills e(bi) = 1 and one obtains for\nthe marginal probability\n\n)\n) (cid:25)\nf(yi|bi)f(bi)d bi\n(cid:25)\n(bi\u03bci)yi\n\n\u2212bi\u03bci\n\nyi!\n\ne\n\n\u03b3(yi + \u03bd)\n\n(cid:26)(cid:25)\n\n\u03bci\n\np (yi) =\n\n=\n\n=\n\n(cid:26)\n\nd bi\n\n\u03bd\u03bd\n\n(cid:26)\n(cid:25)\n\u03b3(\u03bd) b\u03bd\u22121\n\ni\n\ne\n\n\u2212\u03bdbi\n\n(cid:26)\n\nyi\n\n\u03bd\n\n\u03b3(\u03bd)\u03b3(yi + 1)\n\n\u03bci + \u03bd\n\n\u03bci + \u03bd\n\n\u03bd\n\n.\n\n(7.9)\n\nthe density (7.9) represents the negative binomial distribution nb(\u03bd, \u03bci), with mean and vari-\nance given by\n\ne(yi) = \u03bci = exp(xt\n\ni \u03b2),\n\nvar(yi) = \u03bci + \u03bc2\n\ni /\u03bd.\n\nwhile the mean is the same as for the simple poisson model, the variance exceeds the poisson\ni /\u03bd. the poisson model may be seen as a limiting case (\u03bd \u2192 \u221e). the scaling of\nvariances by \u03bc2\n\u03bd is such that small values signal strong overdispersion when compared to the poisson model,\nwhile for large values of \u03bd the model is similar to the poisson model. therefore, 1/\u03bd is consid-\nered the dispersion parameter. for illustration, figure 7.5 shows three densities of the negative\nbinomial distribution. it is seen that nb(100,3), which is is close to the poisson distribution,\nis much more concentrated around the mean than nb(5,3). for known \u03bd the negative binomial\nmodel can be estimated within the glm framework.\nin summary, the negative binomial model was motivated by the assumptions yi|\u03bbi \u223c p(\u03bbi),\nbi \u223c \u03b3(\u03bd, \u03bd), \u03bbi = bi\u03bci and is given by\n\nyi|xi \u223c nb(\u03bd, \u03bci), \u03bci = exp(xt\n\ni \u03b2).\n\n(7.10)\n\nthe additional dispersion parameter makes the model more flexible than the simple poisson\nmodel. if the parameter \u03bd is fixed, for example, by \u03bd = 1, which corresponds to the geometric\ndistribution, the additional flexibility is lost, but nevertheless variance functions that do not\npostulate equidispersion are used.\n\nalternatively, the gamma-poisson model may be derived from assuming that yi is condi-\ntionally poisson-distributed p (\u03bbi) for given \u03bbi and specifying \u03bbi as a random variable that is\ngamma-distributed \u03b3(\u03bdi, \u03bdi\n\u03bci\n\n) with density function\n\nf(\u03bbi) =\n\n1\n\n\u03b3(\u03bdi)\n\n( \u03bdi\n\u03bci\n\n)\u03bdi \u03bb\u03bdi\u22121\n\ni\n\nexp(\u2212 \u03bdi\n\u03bci\n\n\u03bbi)\n\nfor \u03bbi > 0. then one has mean e(\u03bbi) = \u03bci and variance var(\u03bbi) = \u03bc2\nbetween the mean and the linear predictor is specified by \u03bci = exp(xt\nconditional distribution yi|\u03bbi \u223c p(exp(xt\n\nif the link\ni \u03b2), one obtains for the\ni \u03b2)) and for the marginal distribution the discrete\n\ni /\u03bdi.\n\n "}, {"Page_number": 208, "text": "196\n\nchapter 7. regression analysis of count data\n\nnb(100,3)\n\nnb(5,3)\n\n0\n2\n\n.\n\n0\n\n0\n1\n\n.\n\n0\n\n0\n0\n\n.\n\n0\n\n0\n\n5\n\n10\n\n15\n\n0\n\n5\n\n10\n\n15\n\nnb(10,10)\n\n0\n2\n\n.\n\n0\n\n5\n1\n\n.\n\n0\n\n0\n1\n\n.\n\n0\n\n5\n0\n\n.\n\n0\n\n0\n0\n\n.\n\n0\n\n8\n0\n\n.\n0\n\n4\n0\n.\n0\n\n0\n0\n.\n0\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\nfigure 7.5: probability mass functions of negative binomial distributions.\n\ndensity\n\n)\n\n(cid:25)\nf(yi|\u03bbi)f(\u03bbi)d \u03bbi\n\u03b3(yi + \u03bdi)\n\n\u03bci\n\n(cid:26)\n\nyi\n\n(cid:25)\n\n\u03b3(yi + 1)\u03b3(\u03bdi)\n\n\u03bci + \u03bdi\n\np (yi; \u03bdi, \u03bci) =\n\n=\n\n(cid:26)\n\n\u03bdi\n\n.\n\n\u03bdi\n\n\u03bci + \u03bdi\n\n(7.11)\n\ndensity (7.11) is the negative binomial distribution function with mean e(yi) = \u03bci and variance\nvar(yi) = \u03bci + 1\n\u03bc2\ni . therefore, the negative binomial model (7.10) may also be motivated by\nthe assumptions yi|\u03bbi \u223c p(\u03bbi), \u03bbi \u223c \u03b3(\u03bd, \u03bd\n\u03bdi\n\n), \u03bci = exp(xt\n\ni \u03b2).\n\nthe essential advantage of the negative binomial model over the poisson model is that, by\nintroducing a second parameter, more flexible variance structures are possible. however, the\nvariance var(yi) = \u03bci + \u03bc2\ni /\u03bd is also restrictive in a certain sense. since \u03bd > 0, the variance\ncan only be larger than assumed in the poisson model. therefore, underdispersion cannot\nbe modeled adequately by the negative binomial model. when underdispersion occurs, as in\nexample 7.3, where \u02c6\u03c6 = 0.847, the fitting of the negative binomial value typically yields the\npoisson model with a very large value \u02c6\u03bd.\n\n\u03bci\n\nexample 7.7: insolvent firms\nas in example 7.5, a log-linear link is assumed, log(\u03bc) = \u03b20 + x\u03b21 + x2\u03b22, where x denotes the\nmonths ranging from 1 to 36. the fitted models are the log-linear poisson model, the log-linear poisson\nmodel with dispersion parameter \u03c6, the log-linear negative binomial model, and a mixture model that\nassumes a normal distribution of individual effects ai. since the counts are rather large, in addition a\nnormal distribution model with log-link has been fitted. it is seen from table 7.7 that the quadratic effect\nseems negligible for all models. the poisson model is certainly not the best choice since the data are\n\n "}, {"Page_number": 209, "text": "7.6. negative binomial model and alternatives\n\n197\n\noverdispersed. the estimate \u02c6\u03c6 = 2.313 signals that the variance is about twice what one would expect for\nthe poisson model. a check for overdispersion is shown in figure 7.4 (right panel), where the quadratic\nresiduals are plotted against the fitted values. the straight line shows what is to be expected when the\npoisson model holds, namely, e(yi \u2212 \u03bci)2 = \u03bci. the smooth fit shows that squared residual tends to\nbe much larger than expected. overdispersion is also seen from the fit of the negative binomial model.\ni /\u03bd. for values \u03bci \u2208 [70, 100] and\nwhen compared to the poisson model, the variance increases by \u03bc2\n\u02c6\u03bd = 77.93, that means a substantial increase in variance between 63 and 128. the estimated effects\nfor the models compare well with the exception of the poisson model, for which the standard errors are\ndefinitely too small. aic for the poisson model was 306.82; for the normal distribution model, which is\nmore flexible, one gets the smaller value 296.54; the smallest value, 296.27, is obtained by the negative\nbinomial model.\n\ntable 7.7: log-linear model for insolvencies (standard errors in brackets).\n\nlog-linear\n\npoisson model\n\nlog-linear\ndispersion\n\n\u03c6 = 1\n\npoisson model\n\nnegative\n\nlog-linear\n\nmixture\n\nbinomial model\n\nnormal distribution gauss-hermite\n\nvar(yi) = \u03bci\n\nvar(yi) = \u03c6\u03bci\n\nvar(yi) = \u03bci + \u03bc2\n\ni\n\u03bd\n\nvariance\n\nintercept\n\nmonth\n\nmonth2\n\n4.192\n(0.062)\n\n0.020\n(0.007)\n\n\u20130.00026\n(0.00019)\n\n4.192\n(0.094)\n\n0.020\n\n(0.0112)\n\n\u20130.00026\n(0.00028)\n\ndispersion\n\n\u2014\n\n\u02c6\u03c6 = 2.313\n\n4.195\n(0.086)\n\n0.019\n(0.011)\n\n\u20130.00025\n(0.00027)\n\n\u02c6\u03bd = 77.93\n\n(35.49)\n\nmodel\n\n\u03c32\n\n4.184\n(0.101)\n\n0.021\n(0.01?)\n\n\u20130.00029\n(0.00029)\n\n\u02c6\u03c3 = 13.90\n\n4.186\n(0.086)\n\n0.0196\n(0.0105)\n\n\u20130.00026\n(0.00027)\n\n\u02c6\u03c3 =0.113\n(0.025)\n\nexample 7.8: demand for medical care\ntable 7.8 shows the estimates of the negative binomial model for the medical care data (example 7.6).\nit is seen that the effects are similar to the effects of a quasi-poisson model, but the standard errors are\nslightly smaller (\u02c6\u03bd = 1.079, with standard error 0.048, and the residual deviance is \u22129607.73). the\npoisson model yields the log-likelihood value \u22127296.398 (df = 8), and the negative binomial model,\nwhich uses just one more parameter, reduces the likelihood to \u22124803.867 (df = 9).\n\ntable 7.8: negative binomial model for health care data (males).\n\nestimate\n\nstd. error\n\np-value\n\nintercept\nhosp\nhealthpoor\nhealthexcellent\nnumchron\nage\nmarried[yes]\nschool\n\n0.556\n0.245\n0.255\n\u20130.206\n0.182\n0.021\n0.148\n0.040\n\n0.333\n0.033\n0.083\n0.096\n0.020\n0.042\n0.063\n0.007\n\n0.094\n0.000\n0.002\n0.032\n0.000\n0.622\n0.020\n0.000\n\n "}, {"Page_number": 210, "text": "198\n\nchapter 7. regression analysis of count data\n\ngeneralized poisson distribution\nan alternative distribution that allows for overdispersion is the generalized poisson distribution,\nwhich was investigated in detail in consul (1998). a random variable y follows a generalized\npoisson distribution with parameters \u03bc > 0 and \u03b3, y \u223c gp (\u03bc, \u03b3), if the density is given by\n\n\u23a7\u23a8\n\u23a9 \u03bc(\u03bc + y(\u03b3 \u2212 1))y\u22121\u03b3\n\ny!\n\n0\n\np (y = y) =\n\n\u2212ye\n\n\u2212(\u03bc+y(\u03b3\u22121))/\u03b3\n\nfor y \u2208 {0, 1, 2, . . .}\nif \u03b3 < 1.\nfor y > m,\n\nadditional constraints on the parameters are \u03b3 \u2265 max{1/2, 1 \u2212 \u03bc/m}, where m \u2265 4 is the\nlargest natural number such that \u03bc + m(\u03b3 \u2212 1) > 0 if \u03b3 < 1.\n\nit is seen that the distribution becomes the poisson distribution for \u03b3 = 1. for small values\nof \u03b3 the generalized poisson distribution is very similar to the negative binomial distribution;\nfor large values the negative binomial distribution puts more mass on small values of y. for the\ngeneralized poisson distribution one obtains\n\ne(y ) = \u03bc var(y ) = \u03b32\u03bc.\n\nthe parameter \u03b32 can be seen as a dispersion parameter; for \u03b32 > 1 one obtains greater disper-\nsion than for the poisson model, and for \u03b32 < 1 one obtains underdispersion. an advantage of\nthe generalized poisson distribution is that the dispersion parameter also allows for underdisper-\nsion in contrast to the negative binomial model, for which the variance is var(y ) = \u03bc + \u03bc2/\u03bd.\nlike the negative binomial model, the generalized poisson distribution can be derived as a\nmixture of poisson distributions (joe and zhu, 2005). gschoessl and czado (2006) fitted a re-\ngression model based on the generalized poisson distribution and compared several models for\noverdispersion from a bayesian perspective.\n\n7.7 zero-inflated counts\nin many applications one observes more zero counts than is consistent with the poisson (or\nan alternative count data) model; the data display overdispersion through excess zeros. often\none may think of data as resulting from a mixture of distributions. if a person is asked, \"how\nmany times did you eat mussels in the past 3 months?\" one records zero responses from people\nwho never eat mussels and from those who do but happen not to have done so during the time\ninterval in question.\n\nin general, a zero-inflated count model may be motivated from a mixture of two subpop-\nulations, the non-responders who are \"never at risk\" and the responders who are at risk. with\nc denoting the class indicator of subpopulations (ci = 1 for responders and ci = 0 for non-\nresponders) one obtains the mixture distribution\n\np (yi = y) = p (yi = y|ci = 1)\u03c0i + p (yi = y|ci = 0)(1 \u2212 \u03c0i),\n\nwhere \u03c0i = p (ci = 1) are the mixing probabilities. when one assumes that counts within the\nresponder subpopulation are poisson-distributed, one obtains with p (yi = 0|ci = 0) = 1\n\np (yi = 0) = p (yi = 0|ci = 1)\u03c0i + (1 \u2212 \u03c0i) = \u03c0i e\n\n\u2212\u03bci +1 \u2212 \u03c0i,\n\nand for y > 0\n\np (yi = y) = p (yi = y|ci = 1)\u03c0i = \u03c0i e\n\n\u2212\u03bci \u03bcy\n\ni /y!,\n\nwhere \u03bci is the mean of the poisson distribution of population ci = 1. one obtains\ni = \u03c0i\u03bci(1 + \u03bci(1 \u2212 \u03c0i))\n\nvar(yi) = \u03c0i\u03bci + \u03c0i(1 \u2212 \u03c0i)\u03bc2\n\ne(yi) = \u03c0i\u03bci,\n\n "}, {"Page_number": 211, "text": "7.7. zero-inflated counts\n\n199\n\n(exercise 7.4). since var(yi) > e(yi), excess zeros imply overdispersion if \u03c0i < 1. of course,\nthe poisson model is included as the special case where all observations refer to responders and\n\u03c0i = 1.\nwhen covariates are present one may specify a poisson distribution model for y|ci = 1 and\na binary response model for ci \u2208 {0, 1}, for example,\n\nlog(\u03bci) = xt\n\ni \u03b2,\n\nlogit(\u03c0i) = zt\n\ni \u03b3,\n\nwhere xi, zi may be different sets of covariates. the simplest mixture model assumes only an\nintercept in the binary model, logit(\u03c0i) = \u03b30. for increasing \u03b30 one obtains in the limit the\npoisson model without zero inflation.\n\nthe joint log-likelihood function after omitting constants is given by\n\nn(cid:7)\nn(cid:7)\n\ni=1\n\nl =\n\nli(yi)\n\ni(yi = 0) log{1 +\n\n=\n+ (1 \u2212 i(yi = 0)){zt\n\ni=1\n\nexp(zt\n\ni \u03b3)\n1 + exp(zt\n\ni \u03b3)\ni \u03b3 \u2212 log(1 + exp(zt\n\n(exp(\u2212 exp(xt\ni \u03b3)) \u2212 exp(xt\n\ni \u03b2) \u2212 1))}\n\ni \u03b2) + yi(xt\n\ni \u03b2)},\n\nwhere i(yi = 0) denotes an indicator variable that takes value 1 if yi = 0, and 0 otherwise.\nlambert (1992) suggested the use of the em algorithm to maximize the log-likelihood. zeileis\net al. (2008) obtain ml estimates by using optimization functions from r and allow to specify\nstarting values estimated by the em algorithm. they compute the covariance matrix as the\nnumerically determined hessian matrix.\n\nthe zero-inflated poisson model has been extended to the zero-inflated generalized poisson\nmodel in which the poisson distribution is replaced by the generalized poisson distribution\n(famoye and singh, 2003; famoye and singh, 2006; gupta et al., 2004; czado et al., 2007).\nthe resulting family of models is rather large, comprising a zero-inflated poisson regression\nand a generalized poisson regression. min and czado (2010) discussed the use of the wald and\nlikelihood ratio tests for the investigation of zero inflation (or zero deflation).\n\nexample 7.9: demand for medical care\ntable 7.9 shows the fit of a zero-inflated model for the health care data. the counts are modeled as a log-\nlinear poisson model, with all variables included. the binary response uses the logit link with intercept\nonly. therefore, it is assumed that all probabilities \u03c0i are equal. the estimate \u22121.522 corresponds to a\nprobability of 0.295, which means that the portion of responders is not very large and overdispersion has\nto be expected. that is in agreement with the fitted quasi-poisson model (table 7.6). table 7.10 shows\nthe fit of the zero-inflated model when all the variables can have an effect on the mixture component.\nwith the exception of the health status, all variables seem to contribute to the inflation component. for\nthe log-likelihood of the fitted models one obtains \u22126544 on 9 df (zero-inflated poisson model with an\nintercept for the logit model) and \u22126455 on 16 df (zero-inflated poisson model with all variables in both\ncomponents). compared to the poisson model with log-likelihood \u22127296.398 (df = 8), already the zero-\ninflated model with just an intercept in the binary component is a distinct improvement. it is noteworthy\nthat the results differ with respect to significance. the predictor married is not significant when a quasi-\npoisson or a negative binomial model is fitted but seems not neglectable when a zero-inflated model is\nfitted.\n\n "}, {"Page_number": 212, "text": "200\n\nchapter 7. regression analysis of count data\n\ntable 7.9: zero-inflated models, poisson and logit, for health care data (males).\n\ncount model coefficients (poisson with log link)\n\nestimate\n\nstd. error\n\nz-value\n\np-value\n\nintercept\nhosp\nhealthpoor\nhealthexcellent\nnumchron\nage\nmarried[yes]\nschool\n\n1.639\n0.165\n0.241\n\u22120.176\n0.103\n\u22120.048\n0.009\n0.029\n\n0.140\n11.678\n0.009\n17.477\n0.030\n8.078\n0.047 \u22123.771\n13.527\n0.008\n0.018 \u22122.721\n0.027\n0.342\n10.134\n0.003\n\n0.0\n0.0\n0.0\n0.0\n0.0\n0.007\n0.732\n0.0\n\nzero-inflation model coefficients (binomial with logit link)\n\nestimate\n\nstd. error\n\nz-value\n\np-value\n\nintercept\n\n-1.522\n\n0.063\n\n-24.10\n\n0.0\n\ntable 7.10: zero-inflated models, poisson and logit, for health care data (males).\n\ncount model coefficients (poisson with log link)\n\nestimate\n\nstd. error\n\nz-value\n\np-value\n\n(intercept)\nhosp\nhealthpoor\nhealthexcellent\nnumchron\nage\nmarried[yes]\nschool\n\n1.672\n0.165\n0.240\n\u22120.163\n0.101\n\u22120.050\n0.005\n0.028\n\n0.140\n11.980\n0.009\n17.451\n0.030\n8.057\n0.045 \u22123.587\n0.008\n13.351\n0.017 \u22122.839\n0.168\n0.027\n0.003\n9.919\n\n0.0\n0.0\n0.0\n0.0003\n0.0\n0.0045\n0.8663\n0.0\n\nzero-inflation model coefficients (binomial with logit link)\n\nestimate\n\nstd. error\n\nz-value\n\np-value\n\n(intercept)\nhosp\nhealthpoor\nhealthexcellent\nnumchron\nage\nmarried[yes]\nschool\n\n3.152\n\u22120.604\n0.214\n0.260\n\u22120.477\n\u22120.348\n\u22120.700\n\u22120.092\n\n0.890\n3.541\n0.156 \u22123.869\n0.245\n0.874\n0.213\n1.221\n0.065 \u22127.305\n0.115 \u22123.042\n0.148 \u22124.745\n0.017 \u22125.505\n\n0.0004\n0.0001\n0.3822\n0.2221\n0.0\n0.0024\n0.0\n0.0\n\n7.8 hurdle models\nan alternative model that is able to account for excess zeros is the hurdle models (mullahy,\n1986; creel and loomis, 1990). it allows one to model overdispersion through excess zeros\nfor baseline models such as the poisson model and the negative binomial model. the model\nspecifies two processes that generate the zeros and the positives. the combination of both\nmodels, a binary model that determines whether the outcome is zero or positive and a truncated-\nat-zero count model, gives the model.\n\n "}, {"Page_number": 213, "text": "7.8. hurdle models\n\n201\nin general, one assumes that f1, f2 are the probability mass functions with support {0, 1,\n\n2, . . .}. the hurdle model is given by\n\np (y = 0) = f1(0),\n\np (y = r) = f2(r)\n\n1 \u2212 f1(0)\n1 \u2212 f2(0) , r = 1, 2, . . . .\n\nthe model may be seen as a stage-wise decision model. at the first stage a binary variable c\ndetermines whether a count variable has a zero or a positive outcome. c = 1 means that the\n\"hurdle is crossed\" and the outcome is positive, while c = 0 means that zero will be observed.\nthe binary decision between zero and a positive outcome is determined by the f1-distribution\nin the form\n\np (c = 1) = 1 \u2212 f1(0), p (c = 0) = f1(0).\n\nat the second stage the condition distribution given c is specified. if the hurdle is crossed, the\nresponse is determined by the truncated count model with probability mass function\n\np (y = r|c = 1) = f2(r)/(1 \u2212 f2(0))\n\nr = 1, 2, . . .\n\nif the hurdle is not crossed, the probability for zero outcome is 1, p (y = 0|c = 0) = 1. one\nobtains the hurdle model from p (y = r) = p (y = r|c = 0)p (c = 0) + p (y = r|c =\n1)p (c = 1) , which yields\n\np (y = 0) = p (c = 0) = f1(0)\np (y = r) = p (y = r|c = 1)p (c = 1)\n\n= {f2(r)/(1 \u2212 f2(0))}(1 \u2212 f1(0)), r = 1, 2, . . . .\n\nthe derivation shows that the hurdle model is a finite mixture of the truncated count model\np (y = r|c = 1) and the degenerate distribution p (y = r|c = 0). in contrast to the zero\ninflated-counts models from section 7.7, c is an observed variable and not an unobservable\nmixture. the truncated count model is determined by the probability mass function f2, which\nhas been called the parent process by mullahy (1986). if f1 = f2, the model collapses to the\nparent model f2.\nthe model is quite flexible and allows for both under- and overdispersion. this is seen by\nconsidering the mean and variance. with \u03b3 = (1\u2212f1(0))/(1\u2212f2(0)) = p (y > 0)/(1\u2212f2(0)),\nthe mean is given by\n\n\u221e(cid:7)\n\ne(y) =\n\nr f2(r)\u03b3 = p (y > 0) e(y|y > 0)\n\nr=1\n\nand the variance has the form\n\nvar(y) = p (y > 0) var(y|y > 0) + p (y > 0)(1 \u2212 p (y > 0)) e(y|y > 0)2.\n\nlet us consider as a specific model, the hurdle poisson model, which assumes that f2 is the\nprobability mass function of a poisson distribution with mean \u03bc2. let y2 denote the corre-\nsponding random variable (poisson distribution with mean \u03bc2). then one has e(y2) = \u03bc2 and\n\u03bc2 = var(y2) = e(y2\n2 = \u03bc2(1 + \u03bc2). one obtains for\nthe mean and variance of y\n\n2) = \u03bc2 + \u03bc2\n\n2) \u2212 e(y2)2, yielding e(y2\n\u221e(cid:7)\n\n\u221e(cid:7)\nr2f2(r)\u03b3 \u2212 (\n\nr=1\n\nr=1\n\ne(y) = \u03b3\u03bc2,\n\nvar(y) =\n\nr f2(r)\u03b3)2 = \u03bc2(1 + \u03bc2)\u03b3 \u2212 \u03bc2\n\n2\u03b32,\n\n "}, {"Page_number": 214, "text": "202\n\nand therefore\n\nchapter 7. regression analysis of count data\n\nvar(y)\ne(y)\n\n= 1 + \u03bc2(1 \u2212 \u03b3).\n\nthis means that for the non-trivial case \u03bc2 > 0 one obtains overdispersion if 0 < \u03b3 < 1 and\nunderdispersion if 1 < \u03b3 < (1 + \u03bc2)/\u03bc2, where the upper threshold is determined by the\nrestriction var(y) > 0. for \u03b3 = 1, the hurdle poisson becomes the poisson model.\n\nthe hurdle model is determined by the choices of f1 and f2. there is much flexibility\nbecause f1 and f2 may be poisson, geometric, or negative binomial distributions. moreover,\nthe distributions do not have to be the same. one can also combine a binary logit model for the\ntruncated (right-censored at y = 1) distribution of f1 and a poisson or negative binomial model\nfor f2.\n\nconcrete parameterizations are obtained by linking the two distributions to explanatory\nvariables. for illustration we consider the hurdle poisson model where both f1 and f2 corre-\nspond to poisson distributions with means \u03bc1 and \u03bc2, respectively. for observations (yi, xi)\none may specify for\n\n\u03bci1 = exp(xt\n\ni \u03b21), \u03bci2 = exp(xt\n\ni \u03b22),\n\nyielding the model\n\np (yi = 0) = exp(\u2212\u03bci1),\np (yi = r) = \u03bcr\ni2\nr!\n\n\u2212\u03bci2 1 \u2212 exp(\u2212\u03bci1)\n1 \u2212 exp(\u2212\u03bci2) .\ne\n(cid:25)\n(cid:7)\n\n1 \u2212 e\u2212\u03bci1\n1 \u2212 e\u2212\u03bci2\n\n\u03bcyi\ni2\nyi!\n\nlog\n\n(cid:26)\n\n,\n\n\u2212\u03bci2\n\ne\n\nthe log-likelihood is given by\nl(\u03b21, \u03b22) = \u2212\n\n(cid:7)\n\n\u03bci1 +\n\nwhich decomposes into l(\u03b21, \u03b22) = l1(\u03b21) + l2(\u03b22) with\n\nyi=0\n\nyi>0\n\n(cid:7)\n\n(cid:7)\nl1(\u03b21) = \u2212\n(cid:7)\n\nyi=0\n\nl2(\u03b22) =\n\nlog(1 \u2212 e\n\n\u2212\u03bci1),\n\u03bci1 +\nyi log(\u03bci2) \u2212 \u03bci2 \u2212 log(1 \u2212 e\n\nyi>0\n\n\u2212\u03bci2) \u2212 log(yi!).\n\nyi>0\n\nsince the components depend on a one-parameter vector, only the two components can be\nmaximized separately. in general, the regressors for the two model components do not have\nto be the same. but, if the same regressors as well as the same count models are used, as in\nthe preceding poisson example, a test of the hypothesis \u03b21 = \u03b22 tests whether the hurdle is\nneeded. although most hurdle models use the hurdle zero, the specification of more general\nmodels where the hurdle is some positive number is straightforward.\n\nexample 7.10: demand for medical care\ntable 7.11 and table 7.12 show the fits of hurdle models for the health care data. the counts are modeled\nas a log-linear poisson model, with all variables included, and the binary response uses the logit link with\nintercept only (table 7.11) or with all the covariates included (table 7.12). for the log-likelihood of the\nfitted models one obtains \u22126549 on 9 df (hurdle model with an intercept for the logit model) and \u22126456\non 16 df (hurdle model with all variables in both components). both models have a much better fit than\nthe simple poisson model (\u22127296.398 on df = 8). moreover, the model with all covariates in the binary\ncomponent is to be preferred over the pure intercept model. a comparison of the hurdle model and the\n\n "}, {"Page_number": 215, "text": "7.9. further reading\n\n203\n\ntable 7.11: hurdle model, poisson and logit, for health care data (males).\n\ncount model coefficients (truncated poisson with log link)\n\nestimate\n\nstd. error\n\nz-value\n\np-value\n\nintercept\nhosp\nhealthpoor\nhealthexcellent\nnumchron\nage\nmarriedyes\nschool\n\n1.673\n0.165\n0.240\n\u22120.164\n0.101\n\u22120.050\n0.005\n0.028\n\n0.139\n12.001\n0.009\n17.450\n0.030\n8.062\n0.046 \u22123.592\n13.346\n0.008\n0.017 \u22122.848\n0.170\n0.027\n0.003\n9.920\n\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.86\n0.0\n\nzero hurdle model coefficients (binomial with logit link)\n\nestimate\n\nstd. error\n\nz-value\n\np-value\n\nintercept\n\n1.483\n\n0.061\n\n24.28\n\n0.0\n\ntable 7.12: hurdle model, poisson and logit, for health care data (males).\n\ncount model coefficients (truncated poisson with log link)\n\nestimate\n\nstd. error\n\nz-value\n\np-value\n\n(intercept)\nhosp\nhealthpoor\nhealthexcellent\nnumchron\nage\nmarriedyes\nschool\n\n1.673\n0.165\n0.240\n\u22120.164\n0.101\n\u22120.050\n0.005\n0.028\n\n0.139\n12.001\n0.009\n17.450\n0.030\n8.062\n0.046 \u22123.592\n13.346\n0.008\n0.017 \u22122.848\n0.170\n0.027\n9.920\n0.003\n\n0.0\n0.0\n0.0\n0.0003\n0.0\n0.0043\n0.8652\n0.0\n\nzero hurdle model coefficients (binomial with logit link):\n\n(intercept)\nhosp\nhealthpoor\nhealthexcellent\nnumchron\nage\nmarried[yes]\nschool\n\nestimate\n\u22123.104\n0.611\n\u22120.199\n\u22120.274\n0.482\n0.336\n0.690\n0.094\n\nstd. error\n\nz-value\n0.871 \u22123.564\n0.155\n3.944\n0.244 \u22120.815\n0.208 \u22121.318\n7.476\n0.064\n3.010\n0.118\n4.743\n0.146\n0.016\n5.711\n\np-value\n\n0.0004\n0.0\n0.4151\n0.1876\n0.0\n0.0026\n0.0\n0.0\n\nzero-inflated model shows that the parameter estimates and p-values are comparable; only the signs for the\nparameters of the mixture have changed since the response y = 0 is modeled in the hurdle models.\n\n7.9 further reading\nsurveys and books. a source book for the modeling of count data that includes many ap-\nplications is cameron and trivedi (1998). an econometric view on count data is outlined in\nwinkelmann (1997) and kleiber and zeileis (2008). the negative binomial model is treated\nextensively in hilbe (2011)\n\ntests on zero inflation. tests that investigate the need for zero inflation have been suggested\nfor the case of constant overdispersion. the most widely used test is the score test, because it\n\n "}, {"Page_number": 216, "text": "204\n\nchapter 7. regression analysis of count data\n\nrequires only the fit under the null model; see van den broek (1995), deng and paul (2005),\nand gupta et al. (2004).\n\nhurdle models. the poisson hurdle and the geometric and hurdle have been examined by\nmullahy (1986), and hurdle negative binomial models have been considered by pohlmeier and\nulrich (1995). zeileis et al. (2008) describe how regression models for count data, including\nzero-inflated and hurdle models, can be fitted in r.\n\nr packages. glms as the poisson model can be fitted by using of the model fitting func-\ntions glm from the mass package. many tools for diagnostic and inference are available. mass\nalso allows one to fit negative binomial models with fixed dispersion parameters (function neg-\native.binomial) and for estimating regression parameters and dispersion parameters (function\nglm.nb). estimation procedures for zero-inflated and hurdle models are available in the pscl\npackage (for details see zeileis et al., 2008).\n\n7.10 exercises\n\n7.1 in example 1.5, the dependent variable is the number of insolvent firms depending on year and month\n(see table 1.3).\n\n(a) consider time as the only covariate ranging from 1 to 36. fit a log-linear poisson model, an\noverdispersed model, a negative binomial model, and a log-linear normal distribution model with\nlinear time (compare to example 7.7).\n\n(b) fit the models from part (a) with the linear predictor determined by the factors year and month and\n\nan interaction effect if needed.\n\n(c) discuss the difference between the models fitted in parts (a) and (b).\n\ntable 7.13: cellular differentiation data from piegorsch et al. (1988).\n\nnumber of cells\ndifferentiating\n\ndose\n\ndose\n\nof tnf(u/ml)\n\nof ifn(u/ml)\n\n11\n18\n20\n39\n22\n38\n52\n69\n31\n68\n69\n128\n102\n171\n180\n193\n\n0\n0\n0\n0\n1\n1\n1\n1\n10\n10\n10\n10\n100\n100\n100\n100\n\n0\n4\n20\n100\n0\n4\n20\n100\n0\n4\n20\n100\n0\n4\n20\n100\n\n7.2 the r package pscl provides the dataset biochemists.\n\n(a) use descriptive tools to learn about the data.\n\n(b) fit a zero-inflated poisson model and a hurdle model by using the r package pscl.\n\n "}, {"Page_number": 217, "text": "7.10. exercises\n\n205\n\n7.3 investigate the effect of explanatory variables on the number of children for men in analogy to example\n7.3 by using the dataset children from the the package catdat.\n\n7.4 for the zero-inflated count model a mixture of two subnopulations is assumed, with c denoting the\nclass indicator (ci = 1, for responders and ci = 0 for non-responders). when one assumes a poisson\n\u2212\u03bci +1 \u2212 \u03c0i, p (yi = y) = \u03c0i e\nmodel if ci = 1, one has p (yi = 0) = \u03c0i e\ni /y!. show that mean\nand variance have the form e(yi) = \u03c0i\u03bci, var(yi) = \u03c0i\u03bci(1 + \u03bci(1 \u2212 \u03c0i)).\n\n\u2212\u03bci \u03bcy\n\n7.5 table 7.13, which is reproduced from piegorsch et al. (1988), shows data from a biomedical study\nof the immuno-activating ability of two agents, tnf (tumor necrosis factor) and ifn (interferon). both\nagents induce cell differentiation. the number of cells that exhibited markers of differentiation after\nexposure to tnf and/or ifn was recorded. at each of the 16 dose combinations of tnf/inf, 200 cells\nwere examined. it is of particular interest to investigate if the two agents stimulate cell differentiation\nsynergistically or independently.\n\n(a) fit a log-linear poisson model that includes an interaction term and investigate the effects.\n(b) use diagnostic tools to investigate the model fit.\n(b) fit alternative log-linear models that allow for overdispersion and compare the results to the pois-\n\nson model.\n\n "}, {"Page_number": 218, "text": " "}, {"Page_number": 219, "text": "chapter 8\n\nmultinomial response models\n\nin many regression problems the response is restricted to a fixed set of possible values, the\nso-called response categories. response variables of this type are called polytomous or multi-\ncategory responses. in economical applications, the response categories may refer to the choice\nof different brands or to the choice of the transport mode (example 1.3). in medical appli-\ncations, the response categories may represent different side effects of medical treatment or\nseveral types of infection that may follow an operation. most rating scales have fixed response\ncategories that measure, for example, the medical condition after some treatment in categories\nlike good, fair, and poor or the severeness of symptoms in categories like none, mild, moderate,\nmarked. these examples show that there are at least two cases to be distinguished, namely,\nthe case where response categories are mere labels that have no inherent ordering and the case\nwhere categories are ordered. in the first case, the response y is measured on a nominal scale.\ninstead of using the numbers 1, . . . , k for the response categories, any set of k numbers would\ndo. in the latter case, the response is measured on an ordinal scale, where the ordering of the\ncategories and the corresponding numbers may be interpreted but not the distance or spacing\nbetween categories. figures 8.1 and 8.2 illustrate different scalings of response categories. in\nthe nominal case the response categories are given in an unsystematic way, while in the ordinal\ncase the response categories are given on a straight line, thus illustrating the ordering of the\ncategories.\n\nanother type of response category that contains more structure than the nominal case but\nis not captured by simple ordering occurs in the form of nested or hierarchical response cate-\ngories. figure 8.3 shows an example where the basic response is in the categories \"no infec-\ntion,\" \"infection type i\", and \"infection type ii.\" however, for infection type i two cases have\nto be distinguished, namely, infection with and without additional complications. thus, one\nhas splits on two levels, first the split into basic categories and then the conditional split within\noutcome \"infection type i.\"\n\nin this chapter we will consider the modeling of responses with unordered categories. mod-\neling of ordered response categories is treated in chapter 9. in the following some examples\nare given.\n\nexample 8.1: preference for political parties\ntable 8.1 shows counts from a survey on the preference for political parties. the four german parties\nwere the christian democratic union (cdu), the social democratic party (spd), the green party, and\nthe liberal party (fdp). the covariates are gender and age in categories.\n\n207\n\n "}, {"Page_number": 220, "text": "208\n\nchapter 8. multinomial response models\n\nbrand 1\n\nbrand 2\n\nbrand 3\n\nbrand 4\n\nbrand 5\n\nfigure 8.1: choice of brand as nominal response categories.\n\n1\n\nnone\n\n2\n\nmild\n\n3\n\n4\n\nmoderate\n\nmarked\n\nfigure 8.2: severness of symptoms as ordered categories.\n\n1\nno infection\n\n3\ninfection type i without\ncomplication\n\n2\ninfection type i with com-\nplication\n\n4\ninfection type ii\n\nfigure 8.3: type of infection as nested structure.\n\ntable 8.1: cross-classification of preference for political parties and gender.\n\ngender\n\nage\n\ncdu/csu\n\nspd green party\n\nfdp\n\npreferred party\n\nmale\n\nfemale\n\n1\n2\n3\n4\n1\n2\n3\n4\n\n114\n134\n114\n339\n42\n88\n90\n413\n\n224\n226\n174\n414\n161\n171\n168\n375\n\n53\n42\n23\n13\n44\n60\n31\n14\n\n10\n9\n8\n30\n5\n10\n8\n23\n\n "}, {"Page_number": 221, "text": "8.1. the multinomial distribution\n\n209\n\nexample 8.2: addiction\nin a survey people were asked, \"is addiction a disease or are addicts weak-willed ?\" the response was in\nthree categories, \"addicts are weak-willed,\" \"addiction is a disease,\" or both alternatives hold. one wants\nto investigate how the response depends on predictors like gender, age, and education level. the dataset\nis available at http://www.stat.uni-muenchen.de/service/datenarchiv/sucht/sucht.html.\n\n8.1 the multinomial distribution\nthe multinomial distribution is a natural generalization of the binomial distribution. it allows\nfor more than two possible outcomes. for example, in a sample survey respondents might be\nasked for their preference for political parties. then the number of outcomes will depend on\nthe number of competing parties.\n\nlet the possible outcomes be denoted by 1, . . . , k, which occur with probabilities \u03c01, . . . , \u03c0k.\nfor the random variable y , which takes values 1, . . . , k, one has the simple relationship p (y =\nr) = \u03c0r. however, the categories of the random variable y hide that the response is gen-\nuinely multivariate, since each response category refers to a dimension of its own. a more\nappropriate representation is by a vector-valued random variable. in the general form of the\nmultinomial distribution one usually considers a sample of, say, m responses. then the com-\nponents of the vector yt = (y1, . . . , yk) give the cell counts in categories 1, . . . , k. the vector\nyt = (y1, . . . , yk) has probability mass function\n\n(cid:2)\n\nf(y1, . . . , yk) =\n\n1 . . . \u03c0yk\n\nk\n\nm!\n\ny1!\u00b7\u00b7\u00b7yk! \u03c0y1\n0\n\nyi \u2208 {0, . . . , m}, \u03c3iyi = m\notherwise.\n\na response (vector) with this probability mass function follows a multinomial distribution with\nparameters m and \u03c0t = (\u03c01, . . . , \u03c0k). of course the probabilities are restricted by \u03c0i \u2208\n[0, 1], \u03c3i\u03c0i = 1.\nsince \u03c3iyi = m there is some redundancy in the representation. therefore, one often uses\nfor the representation the shorter vector yt = (y1, . . . , yq), q = k\u22121, obtaining for the relevant\npart of the mass function\n\nf(y1, . . . , yq) =\n\nm!\n\ny1! . . . yq!(m \u2212 y1 \u00b7\u00b7\u00b7 \u2212 yq)! \u03c0y1\n\u00b7 (1 \u2212 \u03c01 \u2212 \u00b7\u00b7\u00b7 \u2212 \u03c0q)m\u2212y1\u2212...\u2212yq .\n\n1 . . . \u03c0yq\nq\n\nin the following the abbreviation y \u223c m(m, \u03c0) for the multinomial distribution will always\nrefer to the latter version with q = k \u2212 1 components. in this representation it also becomes\nobvious that the binomial distribution is a special case of the multinomial distribution where\nk = 2 (q = 1), since\n\n(cid:25)\n\n(cid:26)\n\nf(y1) =\n\nm!\n\ny1!(m \u2212 y1)! \u03c0y1\n\n1 (1 \u2212 \u03c01)m\u2212y1 =\n\nm\ny1\n\n1 (1 \u2212 \u03c01)m\u2212y1.\n\u03c0y1\n\nfor the components of the multinomial distribution yt = (y1, . . . yq) one derives\ne(yi) = m\u03c0i, var(yi) = m\u03c0i(1 \u2212 \u03c0i), cov(yi, yj) = \u2212m\u03c0i\u03c0j\n\n(exercise 8.1). in vector form, the covariance is given as cov(y) = m(diag(\u03c0)\u2212 \u03c0\u03c0t ), where\n\u03c0t = (\u03c01, . . . , \u03c0q) and diag(\u03c0) is a diagonal matrix with entries \u03c01, . . . , \u03c0q.\n\n "}, {"Page_number": 222, "text": "210\n\nchapter 8. multinomial response models\n\nfor single observations, where only one respondent (m = 1) is considered, one obtains\ny = r \u21d4 yr = 1, with probabilities given by \u03c0r = p (y = r) = p (yr = 1) and possible\noutcome vectors of length k \u2212 1 given by (1, 0, . . . ), (0, 1, 0, . . . ) . . . (0, 0, . . . , 1).\nthe scaled multinomial distribution uses the vector of relative frequencies \u00afy = (y1/m,\n. . . , yq/m) = yt /m. it has mean e(\u00afy) = \u03c0 and covariance matrix cov(\u00afy) = (diag(\u03c0) \u2212\n\u03c0\u03c0t )/m.\n\n8.2 the multinomial logit model\n\nthe multinomial logit model is the most widely used regression model that links a categorical\nresponse variable with unordered categories to explanatory variables. again let y \u2208 {1, . . . , k}\ndenote the response in categories 1, . . . , k and yt = (y1, . . . , yk) the corresponding multino-\nmial distribution (for m = 1). let x be a vector of explanatory variables. the binary logit\nmodel (chapter 2) has the form\n\nor, equivalently,\n\np (y = 1|x) =\n\nexp(xt \u03b2)\n\n1 + exp(xt \u03b2)\n\n(cid:25)\n\nlog\n\np (y = 1|x)\np (y = 2|x)\n\n(cid:26)\n\n= xt \u03b2.\n\nthe multinomial logit model uses the same linear form of logits. but instead of only one logit,\none has to consider k \u2212 1 logits. one may specify\n\n= xt \u03b2r,\n\nr = 1, . . . , q,\n\n(8.1)\n\nwhere the log-odds compare p (y = r|x) to the probability p (y = k|x). in this presentation\nk serves as the reference category since all probabilities are compared to the last category. it\nshould be noted that the vector \u03b2r depends on r because comparison of y = r to y = k should\nbe specific for r. the q logits log(p (y = 1|x)/p (y = k|x)), . . . , log(p (y = q|x)/p (y =\nk|x)) specified in (8.1) determine the response probabilities p (y = 1|x), . . . , p (y = k|x)\nr=1 p (y = r|x) =\nuniquely. from p (y = r|x) = p (y = k|x) exp(xt \u03b2r) one obtains\nk\u22121\np (y = k|x)\nr=1 exp(xt \u03b2r). by adding p (y = k|x) on the left- and right-hand sides one\nk\u22121\nobtains\n\n(cid:14)\n\n(cid:14)\n\np (y = k|x) =\n\n1 +\n\n1\n\nk\u22121\nr=1 exp(xt \u03b2r)\n\n.\n\n(cid:14)\n\nwhen p (y = k|x) is inserted into (8.1) one obtains the probabilities of the multinomial model\ngiven in the following box.\n\n(cid:25)\n\nlog\n\np (y = r|x)\np (y = k|x)\n\n(cid:26)\n\n "}, {"Page_number": 223, "text": "8.2. the multinomial logit model\n\n211\n\n(cid:25)\n\nlog\n\np (y = r|x)\np (y = k|x)\n\n(cid:26)\n\nmultinomial logit model with reference category k\n\n= xt \u03b2r,\n\nr = 1, . . . , k \u2212 1,\n\n(8.2)\n\nor, equivalently,\n\np (y = r|x) =\n\np (y = k|x) =\n\n1 +\n\n1 +\n\n(cid:14)\n(cid:14)\n\nexp(xt \u03b2r)\nk\u22121\ns=1 exp(xt \u03b2s)\n\n1\n\nk\u22121\ns=1 exp(xt \u03b2s)\n\nr = 1, . . . , k \u2212 1,\n\n(8.3)\n\n,\n\n.\n\nthe representation of the nomnial logit model depends on the choice of the reference cate-\ngory. instead of k, any category from 1, . . . , k could have been chosen as the reference category.\nthe necessity to specify a reference category is due to the constraint \u03c3rp (y = r|x) = 1. the\nconsequence of this constraint is that only q = k \u2212 1 response categories may be specified; the\nremaining probability is implicitly determined. a generic form of the logit model is given by\n\np (y = r|x) =\n\n(cid:14)\n\nexp(xt \u03b2r)\nk\ns=1 exp(xt \u03b2s)\n\n,\n\n(8.4)\n\nwhere additional side constraints have to be specified to fulfill \u03c3rp (y = r|x) = 1.\nit is\nobvious that without side constraints the parameters \u03b21, . . . \u03b2k are not identifiable. if \u03b2r is\nreplaced by \u03b2r +c with c denoting some fixed vector, the form (8.4) also holds with parameters\n\u02dc\u03b2r = \u03b2r + c.\n\ngeneric multinomial logit model\np (y = r|x) =\n\nexp(xt \u03b2r)\ns=1 exp(xt \u03b2s)\n\n\u03c3k\n\n(8.5)\n\nwith optional side constraints\n\nk = (0, . . . , 0)\n\u03b2t\nr0 = (0, . . . , 0)\n\u03b2t\n\nreference category k\nreference category r0\n\n\u03b2s = (0, . . . , 0)\n\nsymmetric side constraint\n\nk(cid:7)\n\ns=1\n\n "}, {"Page_number": 224, "text": "212\n\nchapter 8. multinomial response models\n\nside constraints\nthe side constraint \u03b2k = 0 immediately yields the logit model with reference category k. if\none chooses the side constraints \u03b2r0 = 0, one obtains\n\n(cid:25)\n\n(cid:26)\n\nlog\n\np (y = r|x)\np (y = r0|x)\n\n= xt \u03b2r,\n\nwhich is equivalent to choosing r0 as the reference category. it should be noted that the choice\nif r0 is the reference\nof the reference category is essential for interpreting the parameters.\ncategory, \u03b2r determines the logits log(p (y = r|x)/p (y = r0|x)).\n\na symmetric form of the side constraint is given by\n\nk(cid:7)\n\ns=1\n\n\u03b2s = 0.\n\nthen parameter interpretation is quite different; it refers to the \"median\" response. let the\nmedian response be defined by the geometric mean\n\ngm(x) = k\n\np (y = s|x))1/k.\n\nk(cid:15)\n\ns=1\n\n-../ k(cid:15)\np (y = s|x) = (\n(cid:25)\n(cid:26)\n\ns=1\n\np (y = r|x)\ngm(x)\n\nlog\n\n= xt \u03b2r\n\nthen one can derive from (8.5)\n\n(exercise 8.2). therefore, \u03b2r reflects the effects of x on the logits when p (y = r|x) is\ncompared to the geometric mean response gm(x).\n\nit should be noted that whatever side constraint is used, the log-odds between two response\n\nprobabilities and the corresponding weight are given by\n\n(cid:26)\n\n(cid:25)\n\nlog\n\np (y = r|x)\np (y = s|x)\n(cid:14)\n\n= xt (\u03b2r\n\n\u2212 \u03b2s),\n\nwhich follows from (8.5) for any choice of response categories r, s \u2208 {1, . . . , k}. the transfor-\nmations between different side constraints are rather simple. let \u03b21, . . . , \u03b2q denote the vectors\n\u2217\nwith side constraint \u03b2k = 0 and \u03b2\nq denote the vectors with symmetric side constraints.\n\u2217\nthen one obtains \u03b2r = 2\u03b2\ns (exercise 8.3).\n\n\u2217\n1, . . . , \u03b2\ns(cid:5)=r,s<k \u03b2\n\nthe following example illustrates the interpretation of effects in the simple case with just\n\n\u2217\nr +\n\none categorical covariate.\n\nexample 8.3: preference for political parties\nlet us model the data from table 8.1 with gender as the single explanatory variable (1: female, 0: male).\nthe effect of gender on the preference is investigated by use of the logit model\n\n(cid:4)\n\np (y = r|x)\np (y = 1|x)\n\n(cid:3)\n\nlog\n\n(cid:4)\n\n= \u03b2r0 + xg\u03b2r,\n\n(cid:3)\n(cid:3)\n\nwhere xg = 1 for female respondents and xg = 0 for male respondents. implicitly category 1 (cdu)\nhas been chosen as the reference category (\u03b210 = 0). the interpretation of parameters follows from\n\n\u03b2r0 = log\n\n\u03b2r = log\n\np (y = r|xg = 0)\np (y = 1|xg = 0)\np (y = r|xg = 1)/p (y = 1|xg = 1)\np (y = r|xg = 0)/p (y = 1|xg = 0)\n\n,\n\n(cid:4)\n\n,\n\ne\u03b2r =\n\ne\u03b2l0 =\n\np (y = r|xg = 0)\np (y = 1|xg = 0)\np (y = r|xg = 1)/p (y = 1|xg = 1)\np (y = r|xg = 0)/p (y = 1|xg = 0)\n\n,\n\n.\n\n "}, {"Page_number": 225, "text": "8.2. the multinomial logit model\n\n213\n\nthus e\u03b2r0 represents the odds of preference for party r instead of reference party 1 for male respondents,\nand e\u03b2r represents the odds ratio that compares the odds for female respondents to the odds for male\nrespondents. the parameters are given in table 8.2. for example, for male respondents, the odds of\npreference for party 3 instead of reference party 1 are 0.187. a comparison of the odds from female and\nmale respondents yields 1.259, signaling that female respondents prefer the green party stronger than\nmale respondents.\n\ntable 8.2: parameter estimates for party preference data with covariate gender and ref-\nerence category cdu.\n\ncdu (1)\nspd (2)\n\n\u03b20r\n0\n0.392\ngreens (3) \u22121.677\nliberals (4) \u22122.509\n\n1\n\ne\u03b20r\n\n\u03b2r\n0\n1.480 \u22120.068\n0.230\n0.187\n0.081 \u22120.112\n\nfemale young\n\nfemale old\n\ne\u03b2r\n\n1\n0.934\n1.259\n0.894\n\ngreen\n\ncdu\n\nliberals\n\nspd\n\nmale young\n\nmale old\n\nfigure 8.4: star-plots for subpopulations of party preference data.\n\na simple way to visualize the response probabilities is by use of star-plots, which are in\ncommon use in multivariate statistics. the star-plot applied to response probabilities codes the\nprobabilities or relative frequencies into the length of the rays emanating from the center of the\nplot. figure 8.4 shows the resulting star-plots of relative frequencies for four subpopulations of\nthe party preference data including one symmetric plot that serves to label the rays. it illustrates\nthe strong effects of gender and age. in all the plots a strong preference for spd is seen. but\nin the younger population there is a much stronger tendency toward the green party than in\nthe older population; older voters prefer the cdu. the shifting of preference toward the green\nparty is much stronger for females.\n\n "}, {"Page_number": 226, "text": "214\n\nchapter 8. multinomial response models\n\nin more complex models, and when continuous predictors are included, it can be advanta-\ngeous to represent the exponentials of parameters rather than subpopulations as star-plots. for\nillustration we will consider the main effect model for the party preference data (see also exer-\ncise 8.5). table 8.3 shows the fitted parameters and the exponentials. the latter represent the\nodds ratios and therefore the modification of the probabilities in comparison to the reference\ncategory. figure 8.5 shows the corresponding star-plots. the first star-plot, which gives the\nexponentials of the intercept, represents the fitted odds in the reference population (male, age\ncategory 1). in all the plots the reference category among responses is cdu and the correspond-\ning ray length is 1. the other plots of the exponentials of parameters show the modifications\nresulting from the covariates. it is in particular seen that females have a stronger tendency to-\nward the green party when compared to the reference category of gender (male). for age, with\nreference category 1, it is seen that especially in age category 4 the tendency toward the green\nparty is strongly reduced.\n\ntable 8.3: parameter estimates and exponentials for party preference data with covariates\ngender and age and reference category cdu.\n\nintercept\ngender\nage2\nage3\nage4\n\nexp(intercept)\nexp(gender)\nexp(age2)\nexp(age3)\nexp(age4)\n\ncdu\n\nspd greens\nliberals\n0.905 \u22120.656 \u22122.3090\n0\n0 \u22120.006\n0.429 \u22120.0916\n0 \u22120.321 \u22120.328 \u22120.1100\n0 \u22120.386 \u22120.898 \u22120.1930\n0 \u22120.854 \u22122.910 \u22120.2970\n0.099\n1\n0.912\n1\n0.895\n1\n1\n0.824\n0.743\n1\n\n0.518\n1.535\n0.720\n0.407\n0.054\n\n2.471\n0.994\n0.725\n0.679\n0.425\n\ngreen\n\ncdu\n\nliberals\n\nspd\n\nreference\n\ngender\n\nage2\n\nage3\n\nage4\n\nfigure 8.5: star-plots of exponentials of fitted parameters for main effect model of party\npreference data.\n\n "}, {"Page_number": 227, "text": "8.4. structuring the predictor\n\n215\n\n8.3 multinomial model as random utility model\nin section 8.2 the multinomial logit model was considered as a generalization of the binary\nlogit model. there is an alternative motivation of the model that is not based on binary models\nbut on random utilities. although random utilities are considered more extensively within the\nframework of discrete choice models (section 8.8), they are briefly sketched here because it\nhelps to structure the linear predictor.\n\nlet ur be an unobservable random utility associated with the rth response category. for\nexample, ur is the (subjective) utility of a brand when the choice is among brands 1, . . . , k or\nthe \"attractiveness\" of the rth political party. let ur be given by\n\nur = ur + \u03b5r,\n\nwhere ur is a fixed value, representing the utility associated with the rth response category, and\n\u03b51, . . . , \u03b5k are iid random variables with distribution function f . now let the response y be\ndetermined by the principle of maximum random utility, which specifies the link between the\nobservable y and the unobservable random utility by\n\ny = r \u21d4 ur = max\n\nj=1,...,k\n\nuj.\n\ntherefore, the alternative r is chosen that maximizes the random utility. if one assumes that\n\u03b5r, . . . , \u03b5k are iid variables with distribution function f (x) = exp(\u2212 exp(\u2212x)), which is the\ngumbel or maximum extreme value distribution, one obtains\n\np (y = r) =\n\n(cid:14)\n\nexp(ur)\nk\nj=r exp(uj)\n\n(e.g., yellott, 1977; mcfadden, 1973). the resulting model corresponds to the generic form of\nthe logit model (8.5). the fixed utilities are unique only up to additive constants. therefore,\none needs some additional side constraints; for example, one may consider the differences\nur \u2212 uk, r = 1, . . . , k \u2212 1, which is equivalent to considering k as the reference category. as\nshown in the next section, if one lets the fixed utilities u1, . . . , uk depend linearly on covariates\none obtains the parametric multinomial logit model.\n\n8.4 structuring the predictor\nthe covariates come into the multinomial logit model in the form of linear predictors:\n\n\u03b7r = xt \u03b2r.\n\nin the same way as in univariate response models, the linear predictor may contain dummy\nvariables for categorical covariates, polynomial terms for continuous variables, and interaction\nterms between both types of variables.\n\napart from these transformations of the original observations for the structuring of the lin-\near predictor, it is often useful to distinguish between two types of covariates, namely, global\nand category-specific variables. for example, when an individual chooses among alternatives\n1, . . . , k, one may model the effect of characteristics of the individual like age and gender,\nwhich are global variables, but also account for measured attributes of the alternatives 1, . . . , k,\n\n "}, {"Page_number": 228, "text": "216\n\nchapter 8. multinomial response models\n\nwhich are category-specific variables. when the choice refers to transportation mode, the po-\ntential attributes are price and duration, which vary across the alternatives and therefore are\ncategory-specific.\n\nlet x denote the individual characteristics and w1, . . . , wk denote the set of attributes of\nalternatives, where wr are the attributes of category r. the first type of variable is called global,\nand the latter type category-specific. then the set of linear predictors may be generalized to\n\n\u03b7r = xt \u03b2r + (wr \u2212 wk)t \u03b1,\n\nr = 1, . . . , k \u2212 1,\n\n(8.6)\n\nwhere k is chosen as the reference category. the first term specifies the effect of the global\nvariables, and the second term specifies the effect of the difference wr \u2212 wk on the choice\nbetween category r and the reference category. when wr stands for the price of alternative r,\nit is quite natural to assume that the choice between alternatives r and k is determined by the\ndifference.\n\nthe predictor (8.6) may be derived by maximizing the latent utilities. let the latent utility\n\nof category r be specified by ur = xt \u03b3r + wt\n\nur \u2212 uk = xt (\u03b3r\n\nr \u03b1. then the difference is\n\u2212 \u03b3k) + (wr \u2212 wk)t \u03b1,\n\n\u2212 \u03b3k). of course one may also specify\nwhich has the form given in (8.6) with \u03b2r = (\u03b3r\ninteractions between the two types of variables. let, for example, xg be a dummy variable for\ngender and wr denote the price of alternative r. then the model\n\n\u03b7r = \u03b20r + xg\u03b2g + (wr \u2212 wk)\u03b11 + xg(wr \u2212 wk)\u03b12\n\nallows that the effect of prices depends on gender.\n\nformally, it is not necessary to distinguish between global and category-specific variables.\none may always define one long vector of variables that contains all the specified variables.\nfor example, the predictor with only global variables \u03b7r = xt \u03b2r may also be written as\n\u03b7r = (0t , . . . , xt , . . . , 0t )\u03b2, where \u03b2t = (\u03b2t\nq ). the model with category-specific\n\u2212 wt\nvariables \u03b7r = xt \u03b2r + (wt\nk )\u03b2,\nr\nwhere \u03b2 is now given as \u03b2t = (\u03b2t\nq , \u03b1t ). thus one always obtains the form \u03b7r =\nxt\n\nk )\u03b1 has the form \u03b7r = (0t , . . . , xt , . . . , 0t , wt\n1 , . . . , \u03b2t\nr \u03b2, where xr may or may not depend on r.\n\n1 , . . . , \u03b2t\n\n\u2212 wt\n\nin econometrics, sometimes also for category-specific predictors category-specific effects\nr \u03b1r with category-specific effect \u03b1r yields the\n\nare assumed. the latent utility ur = xt \u03b3r + wt\ndifference of utilities that defines the linear predictor for reference category k\n\nr\n\n\u03b7r = ur \u2212 uk = xt \u03b2r + wt\n\nr \u03b1r \u2212 wt\n\nk \u03b1k,\n\nwhere \u03b2r = (\u03b3r\nthe k \u2212 1 \u03b2-parameters \u03b21, . . . , \u03b2k\u22121, and the k \u03b1-vectors \u03b11, . . . , \u03b1k.\n\n\u2212 \u03b3k). the total set of parameters that defines the total vector \u03b2 now contains\n\nexample 8.4: travel mode\nthe choice of travel mode of n = 840 passengers in australia was investigated by greene (2003). the\ndata are available from the r package ecdat. the alternatives of travel mode were air, train, bus, and\ncar, which have frequencies 0.276, 0.300, 0.142, and 0.280. air serves as the reference category. as\ncategory-specific variables we consider travel time in vehicle (timevc) and cost, and as the global variable\nwe consider household income (income). the estimates given in table 8.4 show that income seems to be\ninfluential for the preference of train and bus over airplane. moreover, time in vehicle seems to matter for\nthe preference of the travel mode. cost turns out to be non-influential if income is in the predictor (see\nalso exercise 8.10).\n\n "}, {"Page_number": 229, "text": "8.5. logit model as multivariate generalized linear model\n\n217\n\ntable 8.4: estimated coefficients for travel mode data.\n\nestimate\n\nstd. error\n\nz-value\n\npr(>|z|)\n\ntrain\nbus\ncar\ntrain:income\nbus:income\ncar:income\ntimevc\ncost\n\n3.525\n2.278\n1.533\n\u22120.056\n\u22120.035\n\u22120.002\n\u22120.003\n\u22120.001\n\n0.654\n0.717\n0.706\n0.012\n0.013\n0.010\n0.001\n0.005\n\n5.381\n3.174\n2.170\n\u22124.588\n\u22122.705\n\u22120.226\n\u22123.274\n\u22120.293\n\n0.0\n0.001\n0.029\n0.0\n0.006\n0.820\n0.001\n0.769\n\n8.5 logit model as multivariate generalized linear model\nfor simplicity, let \u03c0r = p (y = r|x,{wj}) denote the response probability for one observation\ny with covariates x,{wj}, where wj are category-specific attributes. then, with \u02dcwr = wr \u2212\nwk and the linear predictor \u03b7r = xt \u03b2r + \u02dcwt\nr \u03b1, one may write the q equations that specify the\nnominal logit model with reference category k in matrix form by\n\n\u239e\n\u239b\n\u239f\u23a0 =\n\u239c\u239dlog (\u03c01/(1 \u2212 \u03c01 \u2212 \u00b7\u00b7\u00b7 \u2212 \u03c0q))\nlog (\u03c0q/(1 \u2212 \u03c01 \u2212 \u00b7\u00b7\u00b7 \u2212 \u03c0q))\n\n\u02dcwt\n1\n...\n\u02dcwt\nq\nthe predictor for the rth logit log(\u03c0r/(1 \u2212 \u03c01 \u2212 . . . \u2212 \u03c0q)) has the form\n\n\u239b\n\u239c\u239dxt\n\n...\n\nxt\n\n0\n\n0\n\n...\n\n\u239e\n\u239f\u23a0\n\n\u239e\n\u239f\u239f\u239f\u23a0 .\n\n\u239b\n\u239c\u239c\u239c\u239d\u03b21\n\n...\n\u03b2q\n\u03b1\n\n(8.7)\n\n\u03b7r = xt \u03b2r + \u02dcwt\n\nr \u03b1 = (0, . . . , 0, xt , 0, . . . , \u02dcwt\n\nr )\u03b2 = xr\u03b2,\n\nwhere \u03b2t = (\u03b2t\ndesign vector. thus the general form of (8.7) is\n\n1 , . . . \u03b2t\n\nq , \u03b1t ) and xr = (0, . . . , 0, xt , 0, . . . , 0, \u02dcwr) is the corresponding\n\ng(\u03c0) = x\u03b2,\n\nwhere \u03c0t = (\u03c01, . . . , \u03c0q) is the vector the of response probabilities, x is a design matrix that\ncorresponds to the total parameter vector \u03b2, and g is the link function. for the logit model (8.7)\n(cid:26)\nthe vector-valued link function g = (g1, . . . , gq) : rq \u2192 rq is given by\n\n(cid:25)\n\ngr(\u03c01, . . . , \u03c0q) = log\n\n\u03c0r\n\n1 \u2212 \u03c01 \u2212 \u00b7\u00b7\u00b7 \u2212 \u03c0q\n\n.\n\nas usual in generalized linear models, an equivalent form is\n\n\u03c0 = h(x\u03b2),\n\nwhere h = (h1, . . . , hq) = g\nnents\n\n\u22121 is the response function, which in the present case has compo-\n\nhr(\u03b71, . . . , \u03b7q) =\n\n1 +\n\n(cid:14)\nexp(\u03b7r)\ns=1 exp(\u03b7s) .\nq\n\nthus, for one observation, the nominal logit model has the general form\n\ng(\u03c0) = x\u03b2 or \u03c0 = h(x\u03b2)\n\n "}, {"Page_number": 230, "text": "218\n\nchapter 8. multinomial response models\n\nfor an appropriately chosen vector-valued link function, design matrix, and parameter vector.\nfor the given data (yi, xi), i = 1, . . . , n, one has\n\ng(\u03c0(xi)) = x i\u03b2 or \u03c0(xi) = h(x i\u03b2),\n\nwhere \u03c0(x)t = (\u03c01(x), . . . , \u03c0q(x)), \u03c0r(xi) = p (yi = r|xi), and x i is composed from the\ncovariates xi.\n\n8.6 inference for multicategorical response models\nlet the data be given by (yi, xi), i = 1, . . . , n, where xi contains all the covariates that are\nobserved, including category-specific variables. given xi, one assumes a multinomial distri-\n\u223c m(ni, \u03c0i). this means that at measurement point xi one has ni observations.\nbution, yi\nin particular, if xi contains categorical variables, usually more than one observation is col-\nlected at a fixed value of covariates. for example, in a sample survey respondents may be\ncharacterized by gender and educational level. then, for fixed values of gender and educational\nlevel, one usually observes more than one response. this may be seen as a grouped data case,\nwhere the number of counts stored in yi is the sum of the responses of single respondents,\nwith each one having multinomial distribution m(1, \u03c0i). instead of the multinomial distribu-\n\u223c m(ni, \u03c0i) one may also consider the scaled multinomials or proportions \u00afyi = yi/ni,\ntion yi\nwhich are also denoted by pi = \u00afyi. the components of pt\ni = (pi1, . . . , piq) contain the relative\nfrequencies, and pir is the proportion of observations in category r. the proportions \u00afyi = pi\nhave the advantage that e(pi) = \u03c0i, whereas for yi = nipi one has e(yi) = nipi. the model\nthat is assumed to hold has the form\n\ng(\u03c0i) = x i\u03b2 or \u03c0i = h(x i\u03b2).\n\n8.6.1 maximum likelihood estimation\ni =\nthe multinomial distribution has the form of a multivariate exponential family. let yt\n(yi1, . . . , yiq) \u223c m(ni, \u03c0i), i = 1, . . . , n, denote the multinomial distribution with k = q + 1\ncategories. then the probability mass function is\n\nf(yi) =\n\nni!\n\n(cid:27)\nyi1!\u00b7\u00b7\u00b7 yiq!(ni \u2212 yi1 \u2212 \u00b7\u00b7\u00b7 \u2212 yiq)! \u03c0yi1\n(cid:27)\n\n(cid:28)\n\u00b7 \u00b7\u00b7\u00b7 \u00b7 \u03c0yiq\n(cid:28)\ni \u03b8i + ni log(1 \u2212 \u03c0i1 \u2212 . . . \u2212 \u03c0iq) + log(ci)\nyt\ni \u03b8i + log(1 \u2212 \u03c0i1 \u2212 . . . \u2212 \u03c0iq)]/(1/ni) + log(ci)\n[pt\n\ni1\n\n= exp\n= exp\n\n,\n\niq (1 \u2212 \u03c0i1 \u2212 . . . \u2212 \u03c0iq)(ni\u2212yi1\u2212...\u2212yiq)\n\ni = (\u03b8i1, . . . , \u03b8iq), \u03b8ir = log(\u03c0ir/(1 \u2212 \u03c0i1 \u2212 \u00b7\u00b7\u00b7 \u2212\nwhere the canonical parameter vector is \u03b8t\n\u03c0iq)), the dispersion parameter is 1/ni, and ci = ni!/(yi1! . . . yiq!(ni \u2212 yi1 \u2212\u00b7\u00b7\u00b7\u2212 yiq)!). one\nobtains the likelihood\n\nl(\u03b2) =\n\nci \u03c0yi1\ni1\n\n\u00b7 \u00b7\u00b7\u00b7 \u00b7 \u03c0yiq\n\niq (1 \u2212 \u03c0i1 \u2212 \u00b7\u00b7\u00b7 \u2212 \u03c0iq)ni\u2212yi1\u2212\u00b7\u00b7\u00b7\u2212yiq\n\nand the log-likelihood l(\u03b2) =\n\ni=1 li(\u03c0i) with\n\nli(\u03c0i) = ni\n\npir log\n\n\u03c0ir\n\n1 \u2212 \u03c0i1 \u2212 \u00b7\u00b7\u00b7 \u2212 \u03c0iq\n\n(cid:26)\n\n&\n+ log(1 \u2212 \u03c0i1 \u2212 \u00b7\u00b7\u00b7 \u2212 \u03c0iq)\n\n(8.8)\n\nn(cid:15)\n\ni=1\n\n(cid:2)\n\nq(cid:7)\n\nr=1\n\n+ log(ci).\n\n(cid:14)\n(cid:25)\n\nn\n\n "}, {"Page_number": 231, "text": "8.6. inference for multicategorical response models\n\n219\n\nthe score function s(\u03b2) = \u2202l(\u03b2)/\u2202\u03b2 for the model \u03c0i = h(x i\u03b2) has the form\n\nn(cid:7)\n\ni=1\n\ns(\u03b2) =\n\nx t\n\ni di(\u03b2)\u03c3\n\n\u22121\ni\n\n(\u03b2)(pi\n\n\u2212 \u03c0i),\n\nwhere \u03b7i = x i\u03b2 and di(\u03b2) = \u2202h(\u03b7i)/\u2202\u03b7 = (\u2202g(\u03c0i)/\u2202\u03c0)\u22121. it should be noted that the\nmatrix di(\u03b2) with entries \u2202hr(\u03b7i)/\u2202\u03b7s is not a symmetric matrix. the covariance \u03c3 i(\u03b2) is\ndetermined by the multinomial distribution and has the form\n\n\u239b\n\u239c\u239c\u239c\u239d\u03c0i1(1 \u2212 \u03c0i1)\n\n\u2212\u03c0iq\u03c0i1\n\n\u03c3 i(\u03b2) =\n\n1\nni\n\n= [ diag (\u03c0i) \u2212 \u03c0i\u03c0t\n\ni ]/ni.\n\n\u2212\u03c0i1\u03c0i2\n\u03c0i2(1 \u2212 \u03c0i2)\n\n. . .\n\n...\n\n\u239e\n\u239f\u239f\u239f\u23a0\n\n\u2212\u03c0i1\u03c0iq\n\n\u03c0iq(1 \u2212 \u03c0iq)\n\nin closed form one obtains\n\ns(\u03b2) = x t d(\u03b2)\u03c3(\u03b2)\n\n\u22121(p \u2212 \u03c0) = x t w (\u03b2)d(\u03b2)\n\n\u2212t (p \u2212 \u03c0),\n\n1 , . . . , x t\n\nn ), d(\u03b2) and \u03c3(\u03b2) are block-diagonal matrices with blocks\nwhere x t = (x t\ndi(\u03b2), \u03c3 i(\u03b2), respectively, and w (\u03b2) = d(\u03b2)\u03c3(\u03b2)\u22121d(\u03b2)t is a block-diagonal matrix\nwith blocks w i(\u03b2) = di(\u03b2)\u03c3 i(\u03b2)\u22121di(\u03b2)t . for \u03c3 i(\u03b2)\u22121 an explicit form is available\n(see exercise 8.6).\n\n(cid:24)\n\n(cid:23)\n\nthe expected information or fisher matrix f (\u03b2) = e\n\n\u2212\u2202l/\u2202\u03b2\u2202\u03b2t\n\n= cov(s(\u03b2)) has\n\nthe form\n\nf (\u03b2) =\n\nn(cid:7)\n\nx t\n\ni w i(\u03b2)x i = x t w (\u03b2)x.\n\ni=1\n\nthe blocks w i(\u03b2) in the weight matrix can also be given in the form w i(\u03b2) = ( \u2202g(\u03c0i)\n\u2202\u03c0t \u03c3 i(\u03b2)\n\u2202\u03c0 )\u22121, which is an approximation to the inverse of the covariance of g(pi) when the model\n\u2202g(\u03c0i)\nholds. for the logit model, which corresponds to the canonical link, simpler forms of the score\nfunction and the fisher matrix can be found (see exercise 8.4).\nthe estimate \u02c6\u03b2 is under regularity conditions asymptotically (n1+\u00b7\u00b7\u00b7+nn \u2192 \u221e) normally\n\ndistributed with\n\n(a)\u223c n(\u03b2, f (\u02c6\u03b2)\n\n\u22121);\n\n\u02c6\u03b2\n\nfor details see fahrmeir and kaufmann (1985). the score function and fisher matrix have the\nsame forms as in univariate glms, namely, s(\u03b2) = x t d(\u03b2)\u03c3(\u03b2)\u22121(p \u2212 \u03c0) and f (\u03b2) =\nx t w (\u03b2)x. but for multicategorical responses the design matrix is composed from matrices\nfor single observations, and the weight matrix w (\u03b2) as well as the matrix of derivatives d(\u03b2)\nare block-diagonal matrices in contrast to the univariate models, where w (\u03b2) and d(\u03b2) are\ndiagonal matrices.\n\nseparate fitting of binary models\nwhen one considers only two categories, say r and k, the multinomial model looks like a binary\nlogit model. if k is chosen as reference category, one has\n\nlog( p (y = r|x)\np (y = k|x)\n\n) = xt \u03b2r.\n\n "}, {"Page_number": 232, "text": "220\n\nchapter 8. multinomial response models\n\ntherefore, the parameters \u03b2r can also be estimated by fitting a binary logit model using only\nobservations in categories r and k. the resulting estimates are conditional on classification in\ncategories r and k. the estimates obtained by fitting k \u2212 1 separate binary models differ from\nthe estimates obtained from the full likelihood of the multinomial model. in particular, they tend\nto have larger standard errors, although the effect is usually small if the reference category is\nchosen as the category with most of the observations (begg and gray, 1984). another advantage\nof the multinomial likelihood is that the testing of hypotheses that refer to parameters that are\nlinked to different categories is straightforward, for example, by using likelihood ratio tests.\n\n8.6.2 goodness-of-fit\nas in univariate glms, the goodness-of-fit may be checked by the pearson statistic and the\ndeviance. again asymptotic results are obtained only for grouped data, where the number of\nrepetitions ni taken at observation vector xi is not too small.\n\npearson statistic\nwhen considering the discrepancy between observations and fit, one should have in mind that\nresponses are vector-valued. one wants to compare the observation vectors pi = yi/n and\nthe fitted vector \u03c0i, where both vectors have dimension q, since one category (in our case\nthe last one) is omitted from the vector. by using for the last category the observation pik =\n1 \u2212 pi1 \u2212 \u00b7\u00b7\u00b7 \u2212 piq and the fit \u02c6\u03c0ik = 1 \u2212 \u02c6\u03c0i1 \u2212 \u00b7\u00b7\u00b7 \u2212 \u02c6\u03c0iq, one defines the quadratic pearson\nresidual of the ith observation by\n\np (pi, \u02c6\u03c0i) =\n\u03c72\n\n(pir \u2212 \u02c6\u03c0ir)2\n\n\u02c6\u03c0ir\n\n.\n\nk(cid:7)\n\nni\n\nr=1\n\nn(cid:7)\n\nthe corresponding pearson statistic is given by\n\nn(cid:7)\n\np =\n\u03c72\n\np (pi, \u02c6\u03c0i) =\n\u03c72\n\ni=1\n\ni=1\n\n\u2212 \u02c6\u03c0i)t \u03c3\n\n\u22121\ni\n\n(\u02c6\u03b2)(pi\n\n\u2212 \u02c6\u03c0i),\n\n(pi\n\nwhere the last form is derived by explicitly deriving the inverse of the q \u00d7 q-matrix \u03c3 i(\u02c6\u03b2) =\n[ diag (\u02c6\u03c0i) \u2212 \u03c0i\u03c0t\n\ni ]/ni.\n\ndeviance\nthe deviance for multinomial (grouped) observations is given by d = 2\nyielding\n\n(cid:25)\n\n(cid:26)\n\nn(cid:7)\n\nk(cid:7)\n\n(cid:14)\n\nn\n\ni=1 li(pi) \u2212 li(\u02c6\u03c0i),\n\nthe corresponding quadratic deviance residuals are given by\n\nd = 2\n\nni\n\ni=1\n\nr=1\n\npir log\n\npir\n\u02c6\u03c0ir\n\n(cid:25)\n\npir\n\u02c6\u03c0ir\n\n.\n\n(cid:26)\n\n.\n\nk(cid:7)\n\nr=1\n\nd(pi, \u02c6\u03c0i) = 2ni\n\u03c72\n\npir log\n\nunder the assumptions of the fixed cells asymptotic (ni/n \u2192 \u03bbi \u2208 (0, 1)) and regularity\np and d are asymptotically \u03c72-distributed with n(k\u2212 1)\u2212 p degrees of freedom,\nconditions, \u03c72\nwhere n is the number of (grouped) observations, k is the number of response categories and\np is the number of estimated parameters.\n\n "}, {"Page_number": 233, "text": "8.6. inference for multicategorical response models\n\n221\n\ngoodness-of-fit tests\n\npearson statistic\n\nn(cid:7)\n\ni=1\n\np =\n\u03c72\n\nwith\n\np (pi, \u02c6\u03c0i) = ni\n\u03c72\n\n(pir \u2212 \u02c6\u03c0ir)2/\u02c6\u03c0ir\n\np (pi, \u02c6\u03c0i)\n\u03c72\n\nk(cid:7)\n\nr=1\n\ndeviance\n\nn(cid:7)\n\ni=1\n\nd =\n\nwith\n\nd(pi, \u02c6\u03c0i) = 2ni\n\u03c72\n\npir log\n\nd(pi, \u02c6\u03c0i)\n\u03c72\n\nk(cid:7)\n\nr=1\n\n(cid:26)\n\n(cid:25)\n\npir\n\u02c6\u03c0ir\n\np are compared to the asymptotic \u03c72-distribution with n(k \u2212 1) \u2212 p\n\nd, \u03c72\ndegrees of freedom (ni/n \u2192 \u03bbi \u2208 [0, 1]).\n\nas in the case of the binomial distribution, one should be cautious when using these goodness-\n\nof-fit statistics if ni is small. for the ungrouped case, for example, if covariates are continuous,\none has ni = 1 for all observations and the deviance becomes\ni(yi = r) log(\u02c6\u03c0ir) = \u22122\n\nd = \u22122\n\nn(cid:7)\n\nk(cid:7)\n\nlog(\u02c6\u03c0iyi),\n\nr=1\n\ni=1\n\nwhere yi \u2208 {1, . . . , k} denotes the ith observation, and i is the indicator function with i(a) = 1\nif a holds and i(a) = 0 otherwise. the value of d should certainly not be compared to quantiles\nof the \u03c72-distribution since one has n observations and n(k\u22121) degrees of freedom. for small\nvalues of ni alternative asymptotic concepts should be used (see also next section).\n\npower-divergence family\na general family of goodness-of-fit statistics that comprises the deviance and the pearson statis-\ntic is the power-divergence family, which for \u03bb \u2208 (\u2212\u221e,\u221e) has the form\n\nn(cid:7)\n\ns\u03bb =\n\nsd\u03bb(pi, \u02c6\u03c0i),\n\nwhere\n\nsd\u03bb(pi, \u02c6\u03c0i) =\n\ni=1\n\n2ni\n\n\u03bb(\u03bb + 1)\n\nk(cid:7)\n\nr=1\n\npir\n\n0(cid:23)\n\n1\n\n.\n\n\u03bb \u2212 1\n\n(cid:24)\n\npir\n\u02c6\u03c0ir\n\n(8.9)\n\n "}, {"Page_number": 234, "text": "(cid:14)\n\n(cid:14)\n\nr(pir \u2212 \u02c6\u03c0ir)2/pir.\n\nchapter 8. multinomial response models\n\n222\nas special cases, one obtains for \u03bb = 1 the pearson statistic and for the limit \u03bb \u2192 0 the de-\nviance. however, the family includes further statistics that have been proposed in the literature;\nin particular one obtains for the limit \u03bb \u2192 \u22121 kullback\u2019s minimum discrimination information\nr \u02c6\u03c0ir log(\u02c6\u03c0ir/pir) and for \u03bb = \u22122 neyman\u2019s minimum\nstatistic with sd\u22121(pi, \u02c6\u03c0i) = ni\nmodified \u03c72-statistic with sd\u22122(pi, \u02c6\u03c0i) = ni\nunder the assumptions of fixed cells asymptotics one obtains the same asymptotic \u03c72-\ndistribution for any \u03bb, if estimates \u02c6\u03c0ir are best asymptotically normal (ban-) distributed es-\ntimates, as for example the ml estimates or estimates obtained from minimizing s\u03bb. fixed\ncells asymptotics postulates in particular fixed numbers of groups and large values of ni.\nif the number of observations ni at a fixed value xi is small, the usual asymptotic fails.\nan alternative is increasing cells asymptotics, which allows that with increasing sample size\nn = n1 + \u00b7\u00b7\u00b7 + nn \u2192 \u221e the number of groups also increases n \u2192 \u221e. however, under in-\ncreasing cells asymptotics the asymptotic distribution is normal and depends on \u03bb. therefore,\nif goodness-of-fit statistics like the deviance and pearson statistic differ strongly, one might\nsuspect that fixed asymptotics does not apply. for more details on increasing cells asymptotics\nsee read and cressie (1988) and osius and rojek (1992).\n\nfurther test statistics are variations of the tests for binary responses considered in section\n4.2.3. one is the hosmer-lemeshow statistic, which has been extended to the multinomial\nmodel by pigeon and heyse (1999), and the other is based on smoothing of residuals. it has been\nadapted to the multinomial model by goeman and le cessie (2006). of course problems found\nin the binary case, namely, low power of the hosmer-lemeshow statistic and the restriction to\nlow dimensions for smoothed residuals, carry over to the multinomial case.\n\n8.6.3 diagnostic tools\nhat matrix\nfor multicategory response models, the iteratively reweighted least-squares fitting procedure\n(see chapter 3) has the form\n\nwith \u02dc\u03b7(\u02c6\u03b2\n\n(l)) = x \u02c6\u03b2 + d\n\n\u2212 \u03c0i(\u03b2)), and one obtains at convergence\n\n\u22121x t w (\u03b2(l))\u02dc\u03b7(\u02c6\u03b2\n\n(l)),\n\n\u02c6\u03b2\n\n(l+1) = (x t w (\u03b2(l))x)\n\u22121(\u02c6\u03b2)t (pi\n\u02c6\u03b2 = (x t w (\u03b2)x)\n\n\u22121x t w (\u03b2)\u02dc\u03b7(\u02c6\u03b2).\n\nthe corresponding hat matrix is\n\nh = w t /2(\u02c6\u03b2)xf\n\n\u22121(\u02c6\u03b2)x t w 1/2(\u02c6\u03b2).\n\nh is an (n q \u00d7 n q)-matrix with blocks h ij. the (q \u00d7 q)-matrix h ii corresponds to the ith\nobservation. as indicators for leverage one can use det(h ii) or tr(h ii).\n\nresiduals\nthe vector of raw residuals is given by pi\npearson residual:\n\n\u2212 \u02c6\u03c0i. correcting for the variances of pi yields the\n(cid:14)\n\n\u2212 \u02c6\u03c0i),\n\n(\u02c6\u03b2)(pi\n\n\u22121/2\ni\n\nrp (pi, \u02c6\u03c0i) = \u03c3\n\ni rp (pi, \u02c6\u03c0i)t rp (pi, \u02c6\u03c0i). a plot of the squared\nwhich forms the pearson statistic \u03c72\npearson residuals shows which observations have a strong impact on the goodness-of-fit. the\npearson residuals themselves are vector-valued and show how well categories are fitted.\n\np =\n\n "}, {"Page_number": 235, "text": "8.7. multinomial models with hierarchically structured response\n\n223\n\nstandardized pearson residuals, which correct for the variance of the pearson residual, have\n\nthe form\n\nrp (pi, \u02c6\u03c0i) = i \u2212 h\n\n\u22121/2\nii \u03c3\n\u2212 \u02c6\u03c0i) (cid:17) \u03c3 1/2\n\n(\u02c6\u03b2)(pi\n\n\u2212 \u02c6\u03c0i),\n\u22121/2\ni\n(\u02c6\u03b2)(i\u2212h ii)\u03c3 t /2\n\n(\u02c6\u03b2). the approximation\nwhich uses the approximation cov(pi\nmay be derived in the same way as in the univariate case (see section 3.10). more details on\nregression diagnostics are found in lesaffre and albert (1989).\n\ni\n\ni\n\ntesting components of the linear predictor\nlinear hypotheses concerning the linear predictor have the form\nh0 : c\u03b2 = \u03be against h1 : c\u03b2 (cid:8)= \u03be,\n\nwhere c is a fixed matrix of full rank s \u2264 p and \u03be is a fixed vector. let, for example, the linear\npredictor contain only two variables, such that \u03b7r = \u03b2ro + x1\u03b2r1 + x2\u03b2r2. then the hypothesis\nthat variable x1 has no influence has the form\n\nh0 : \u03b211 = \u00b7\u00b7\u00b7 = \u03b2q1 = 0 against h1 : \u03b2r1 (cid:8)= 0 for one r.\n\nit is easy to find a matrix c and a vector \u03be that form the corresponding linear hypothesis.\nhypotheses like that make it necessary to treat the multicategorical model as a multivariate\nmodel. since the hypothesis involves parameters that correspond to more than one response\ncategory, the fitting of q separate binary models could not be used directly to test if h0 holds.\nthe test statistics in common use are the same as in univariate glms, namely, the likelihood\nratio statistic, the wald test, and the score test. the form is the same as given in section 4.4;\none just has to replace the score function and the fisher matrix by their multivariate analogs.\n\ntest procedures serve to determine if the variables have significant weights. another aspect\nis the explanatory value of the predictors, which can be evaluated for example by r-squared\nmeasures. for measures of this type see amemiya (1981).\n\nexample 8.5: addiction\nwe refer to example 8.2. in the survey people were asked, \"is addiction a disease or are addicts weak-\nwilled?\" the response was in three categories, 0: addicts are weak-willed, 1: addiction is a disease, 2:\nboth. table 8.5 shows the coefficients of the multinomial logit model with the covariates gender (0: male;\n1: female), age in years, and university degree (0: no; 1: yes). category 0 was chosen as the reference\ncategory. it is seen that women show a stronger tendency to accept addiction as a disease than men. the\nsame effect is found for respondents with a university degree. age also shows a significant effect, and at\nleast a quadratic effect should be included since the inclusion of a quadratic effect reduces the deviance\nby 32.66. figure 8.6 shows the estimated probabilities against age for males and females with a university\ndegree (compare also the smooth modeling in example 10.5).\n\n8.7 multinomial models with hierarchically structured\n\nresponse\n\nin some applications the response categories have some inherent grouping. for example, when\nthe response categories in a clinical study are given by {no infection, infection type i, infection\ntype ii}, it is natural to consider the two latter response categories as one group {infection}.\nwhen investigating the effect of the predictors on these responses, one might want to take\n\n "}, {"Page_number": 236, "text": "224\n\nchapter 8. multinomial response models\n\ntable 8.5: estimated coefficients for addiction survey with quadratic effect of age.\n\nestimates\n\nintercept gender\n\u22123.720\n\u22123.502\n\n0.526\n0.356\n\nuniv\n\n1.454\n0.936\n\nage\nage2\n0.184 \u22120.002\n0.135 \u22120.001\n\nstandard errors\n\n0.546\n0.596\n\n0.201\n0.224\n\n0.257\n0.290\n\n0.029\n0.030\n\n0.0003\n0.0003\n\n1\n2\n\n1\n2\n\ngender = male\n\ngender = female\n\n0\n\n.\n1\n\n8\n.\n0\n\n6\n.\n0\n\n4\n.\n0\n\n2\n.\n0\n\n0\n.\n0\n\n0\n\n.\n1\n\n8\n.\n0\n\n6\n.\n0\n\n4\n.\n0\n\n2\n.\n0\n\n0\n.\n0\n\n20\n\n40\n\n60\n\n80\n\n20\n\n40\n\n60\n\n80\n\nage\n\nage\n\nfigure 8.6: estimated probabilities for addiction data with quadratic effect of age (cate-\ngory 0: solid line; category 1: dotted line; category 2: dashed line).\n\nthe similarities between the response categories into account to obtain effects with a simple\ninterpretation.\n\na hierarchical model is obtained by first modeling the response in groups of homogeneous\nresponse categories, and in a second step the response within the groups is modeled. in general,\nlet the response categories k = {1, . . . , k} be subdivided into basic sets s1, . . . , sm, where\nk = s1 \u222a \u00b7\u00b7\u00b7 \u222a sm. in the first step, let the logit model be\nexp(xt \u03b2t)\ns\u2208k exp(xt \u03b2s) .\nin the second step, the conditional response given st is modeled as\n(cid:14)\n\np (y \u2208 st|x) =\n\np (y = r|y \u2208 st, x) =\n\n(cid:14)\n\n)\n\n.\n\nr\n\nexp(xt \u03b2(st)\ns\u2208st\n\nexp(xt \u03b2(st)\n\ns\n\n)\n\nthe parameters have to be restricted by side constraints on two levels, for the responses in\ns1, . . . , sm and for the conditional response. for example, one might use sm as a reference on\nthe first level by setting \u03b2m to zero and choose one category from each set st as a reference on\nthe second level by setting \u03b2rt to zero for one category rt \u2208 st.\nof response on category r, which for r \u2208 st has the simple form\n\nfor the derivation of the maximum likelihood estimates one needs the marginal probability\n\np (y = r|x) = p (y = r|y \u2208 st)p (y \u2208 st).\n\n "}, {"Page_number": 237, "text": "8.7. multinomial models with hierarchically structured response\n\n225\n\u223c m(ni, \u03c0i), i = 1, . . . , n, \u03c0i =\n\ntherefore, assuming multinomially distributed responses yi\n\u03c0(xi), the log-likelihood is\n\nn(cid:7)\n\nk(cid:7)\n\nl =\n\nyir log(\u03c0ir) =\n\nyir log(\u03c0ir).\n\ni=1\n\nr=1\n\nit decomposes into l = lg({\u03b2t\n\n}) +\n\n(cid:14)\n\nn(cid:7)\n\nm(cid:7)\n\n(cid:7)\n\ni=1\n\nt=1\n\nr\u2208st\n}), where\n\nm\n\nt=1 lt({\u03b2(st)\nn(cid:7)\n\nm(cid:7)\n\nr\n\ni=1\n\nt=1\n\nlg({\u03b2t\n\n}) =\n\nyist log(p (yi \u2208 st)),\n\n(cid:14)\n\nwith yist =\ndepending only on \u03b21, . . . , \u03b2m, and\n\nr\u2208st\n\nyir, is the log-likelihood for the grouped observations on the first level,\n\nlt({\u03b2(st)\n\nr\n\n}) =\n\nn(cid:7)\n\n(cid:7)\n\nr\u2208st\n\ni=1\n\nyir log(p (yi \u2208 r|yi \u2208 st))\n\n, r \u2208 st}. since the log-\nis the log-likelihood for responses within st depending on {\u03b2(st)\nlikelihood decomposes into additive terms, each part of the log-likelihood may be fitted sepa-\nrately by fitting the corresponding logit model. likelihood ratio tests for individual parameters\napply on the level of each model. however, wald tests, which are typically used to examine\nsingle parameters, and the corresponding p-values are not trustworthy because they are based\nonly on the components of the total model (exercise 8.8).\n\nr\n\nit should be noted that the assumption of a logit model on both levels yields a model that\nis not equivalent to a one-step logit model. if a logit model holds for all categories, one easily\nderives that the conditional model p (y = r|y \u2208 st, x) is again a logit model, but that does\nnot hold for the probabilities p (y \u2208 st|x).\n\nexample 8.6: addiction\nwe refer again to the addiction data (example 8.5). the response was in three categories, 0: addicts are\nweak-willed; 1: addiction is a disease; 2: both, which are grouped into s1 = {0, 1}, s2 = {2}. therefore,\nthe binary logit model that distinguishes between s1 and s2 models if the respondents think that a single\ncause is responsible for addiction. in the second step, the logit model compares category 1 to category 0,\ngiven that the response is in categories {0, 1}. from the estimates in tables 8.6 and 8.7 it is seen that the\ncovariates have no effect on the distinction between categories {0, 1} and {2} but have strong effects on\nthe distinction between causes given that they think that one cause is behind addiction.\n\ntable 8.6: estimated probabilities for addiction data with response in category 2 com-\npared to categories {0, 1}.\n\nestimate\n\nstd. error\n\nz-value\n\npr(>|z|)\n\nuniversity\n\nintercept\n2.1789\ngender \u22120.0172\n0.0895\nage \u22120.0342\n0.0001\nage2\n\n0.5145\n0.1828\n0.2067\n0.0255\n0.0003\n\n4.23\n\u22120.09\n0.43\n\u22121.34\n0.45\n\n0.000\n0.925\n0.665\n0.179\n0.650\n\n "}, {"Page_number": 238, "text": "226\n\nchapter 8. multinomial response models\n\ntable 8.7: estimated probabilities for addiction data comparing category 1 to category\n0, given the response is in categories {0, 1}.\n\nestimate\nintercept \u22123.5468\n0.5433\ngender\n1.4656\nuniversity\nage\n0.1720\nage2 \u22120.0017\n\nstd. error\n\n0.5443\n0.2055\n0.2601\n0.0284\n0.0003\n\nz-value\n\u22126.52\n2.64\n5.64\n6.06\n\u22125.19\n\npr(>|z|)\n\n0.000\n0.008\n0.000\n0.000\n0.000\n\n8.8 discrete choice models\nsince many economic decisions involve choice among discrete alternatives, probabilistic choice\nmodels have become an important research area in contemporary econometrics. in contrast to\nearlier approaches to model demand on an aggregate level, probabilistic choice models focus\non the modeling of individual behavior. in the following some basic concepts are given. they\nprovide a motivation for the multinomial logit model but also give rise to alternative models.\nmore extensive treatments of probabilistic choice have been given, for example, by mcfadden\n(1981).\nlet k = {1, . . . , k} denote the total set of alternatives. for a subset b \u2282 k of available\nalternatives let pb(r) denote the probability of choosing r \u2208 b, given that a selection must be\nmade from set b. a probabilistic choice system may be described as a tupel:\n\n(k,b,{pb, b \u2208 b}) ,\n\nwhere b is a family of subsets from k. a simple example is the one-member family b = {k}.\nthen one considers only selections from the full set of alternatives k. in a pair comparison\nsystem with b = {{i, j}, i, j \u2208 k}}, the selection is among two alternatives, where all com-\nbinations of alternatives are presented. in a complete choice experiment b contains all subsets\nk \u2282 b with |k| \u2265 2.\nables ur, r \u2208 k, such that\n\na probabilistic choice system is called a random utility model\n\nif there exist random vari-\n\npb(r) = p (ur = max\ns\u2208b\n\n{us}).\n\n(8.10)\n\nthe random utility ur represents the utility attached to alternative r. usually it is a latent vari-\nable that cannot be measured directly. its interpretation depends on the application; it may be\nthe subjective utility of a travel mode or of a brand in commodity purchases. if the choice prob-\nabilities have a representation (8.10), one says that the random utility maximization hypothesis\nholds.\n\nlet the random utility ur have the form ur = ur + \u03b5r, where ur is the structural part or\nnon-random fixed utility and \u03b5r is a noise variable. the fixed utility ur is determined by the\ncharacteristics of the decision maker and the attributes of the alternatives, while the random\nvariable \u03b5r represents the residual variation.\n\nrandom utility models\nequation (8.10) may also be seen as a way of constructing choice probabilities. let us consider\na set b = {i1, . . . , im}, i1 < \u00b7\u00b7\u00b7 < im. then the choice probabilities p (r) = pb(r) are\n\n "}, {"Page_number": 239, "text": "8.8. discrete choice models\n\n227\n\nobtained as\n\npb(ir) = p (uir\n= p (uir\n= p (uir\n\u2212ui1\n\n)\n\nuir\n\nfor\n\n\u2265 uj\n\u2265 ui1 , . . . , uir\n\u2265 \u03b5i1\n\u2212 ui1\nuir\u2212uim)\n\nj \u2208 b)\n\n\u2265 uim)\n\u2212 \u03b5ir , . . . , uir\n\n\u2212 uim\n\n\u2265 \u03b5im\n\n\u2212 \u03b5ir)\n\n=\n\n. . .\n\n\u2212\u221e\n\n\u2212\u221e\n\nfb,r(\u03b5i1ir , . . . , \u03b5imir)d \u03b5i1ir . . . d \u03b5imir\n\n\u2212 ui1, . . . , uir\n\n\u2212 uim),\n\n= fb,r(uir\n\n(8.11)\n\u2212 \u03b5ir , is the (m \u2212 1)-dimensional vector\nwhere \u03b5t\nof differences; fb,r is the density of \u03b5b,r; and fb,r is the cumulative distribution function of\n\u03b5b,r.\n\nb,r = (\u03b5i1ir , . . . , \u03b5imir), with \u03b5isir = \u03b5is\n\nlet us consider the simple case where the full set of alternatives k forms the choice set.\nthen any distribution of \u03b5t = (\u03b51, . . . , \u03b5k) will generate a discrete choice model. a famil-\niar model results when one assumes that \u03b51, . . . , \u03b5k are iid variables with marginal distribution\nfunction f (x) = exp(\u2212 exp(\u2212x)), which is the gumbel or maximum extreme value distri-\nbution. then one obtains for the cumulative distribution function of the differences (\u03b51 \u2212\n\u03b5r, . . . , , \u03b5k \u2212 \u03b5r)\n\nfm\u22121(x1, . . . , xm\u22121) =\n\n1 +\n\n(cid:14)\n\n1\ni=1 exp(\u2212xi)\nm\u22121\n\nand therefore the logit model\n\npk(r) =\n\n1 +\n\n(cid:14)\nj(cid:5)=r exp(\u2212(ur \u2212 uj))\n\n1\n\n(cid:14)\n\n=\n\nexp(ur)\nk\nj=1 exp(uj)\n\n(e.g. yellott, 1977). one obtains the familiar multinomial logit model with predictors by as-\nsuming that one has a vector x that characterizes the decision maker and the attributes wr\nconnected to alternative r that form the latent fixed utility:\n\nur = xt \u03b3r + wt\n\nr \u03b1.\n\nthen the model has the familiar form\n\n(cid:25)\n\n(cid:26)\n\nlog\n\npk(r)\npk(k)\n\n= ur \u2212 uk = xt (\u03b3r\n= xt \u03b2r + (wr \u2212 wk)t \u03b1,\n\n\u2212 \u03b3k) + (wr \u2212 wk)t \u03b1\n\nwhich is equivalent to the logit model considered in section 8.4.\n\nrandom utility models that assume iid distributions for the noise variables have a long\nhistory. in psychophysics, thurstone (1927) proposed the law of comparative judgement, which\nis based on normally distributed variables. generally, in the iid case, all differences have the\nsame distribution and one obtains with \u03c32\n\n0 = var(\u03b5i) for the covariance of differences:\n\ncov(\u03b5i \u2212 \u03b5r, \u03b5j \u2212 \u03b5r) = e(\u03b5i \u2212 \u03b5r)(\u03b5j \u2212 \u03b5r)\n\n= e(\u03b5i\u03b5j \u2212 \u03b5i\u03b5r \u2212 \u03b5r\u03b5j + \u03b52\n\nr) = e(\u03b52\n\nr) = \u03c32\n0.\n\nthen the covariance matrix of the differences is given by \u03c3 0 = \u03c32\nthat \u03b5i is normally distributed with \u03b5i \u223c n(0, \u03c32\nutility the multinomial probit model\n\n0(i + 11t ). if one assumes\n0), one obtains from maximizing the random\n\npk(r) = \u03c60,\u03c3 0(ur \u2212 u1, . . . , ur \u2212 uk),\n\n "}, {"Page_number": 240, "text": "228\nwhere \u03c60,\u03c3 0 is the (k\u2212 1)-dimensional cumulative distribution function of the normal distribu-\ntion n(0, \u03c3 0). in psychology, the model is also known as thurstone\u2019s case v . the multinomial\nprobit model is harder to use for higher numbers, of alternatives because the integral has to be\ncomputed numerically.\n\nchapter 8. multinomial response models\n\nindependence from irrelevant alternatives\nsimple models like the multinomial logit model imply a property that has caused some dis-\ncussion in economics. the problem occurs if decisions for more than just one fixed set of\nalternatives are investigated. for example, in a complete choice experiment, the choice prob-\nabilities are determined for every subset of alternatives. then the logit model for subset b,\nderived from the maximization of random utilities ur, has the form\n\n(cid:14)\nand one obtains for any subset b, c, r, s \u2208 b \u2229 c\n= pc(r)\npc(s)\n\npb(r)\npb(s)\n\npb(r) =\n\nexp(ur)\nj\u2208b exp(uj) ,\n\n=\n\nexp(ur)\nexp(us) .\n\n(8.12)\n\nthis means that the proportion of probabilities is identical when different sets of alternatives\nare considered. the system of choice probabilities satisfies luce\u2019s choice axiom (luce, 1959),\nwhich implies that the choice probabilities are independent from irrelevant alternatives.\n\nmcfadden (1986) calls the independence from irrelevant alternatives a blessing and a curse.\nan advantage is that if it holds, it makes it possible to infer choice behavior with multiple\nalternatives using data from simple experiments like paired comparisons. a disadvantage is\nthat it is a rather strict assumption that may not hold for heterogeneous patterns of similarities\nencountered in economics. a famous problem that illustrates the case is the \"red bus\u2013blue bus\"\nproblem that has been used by mcfadden (see, e.g., hausman and wise, 1978). suppose a\ncommuter has the initial alternatives of driving or taking a red bus with the odds given by\n\np{driving,red bus}(driving)\np{driving,red bus}(red bus)\n\n= 1.\n\nthen an additional alternative becomes available, namely, a blue bus that is identical in all\nrespects to the red bus, except color. if the logit model holds, it is seen from (8.12) that the\nodds of choosing the driving alternative over the red bus remain the same. since the odds for\nthe choice between the red and the blue bus are\n\np{red bus, blue bus}(red bus)\np{red bus, blue bus}(blue bus)\n\n= 1,\n\none obtains for any b for all paired comparisons\n= pb(driving)\npb(blue bus)\n\npb(driving)\npb(red bus)\n\n= pb(red bus)\npb(blue bus)\n\n= 1\n\nand therefore\n\np{1,2,3}(driving) = p{1,2,3}(red bus) = p{1,2,3}(blue bus) = 1/3.\n\nthis is a counterintuitive result because the additional \"irrelevant\" alternative blue bus has de-\ncreased the choice probability of driving substantially. problems of this type occur not only for\n\n "}, {"Page_number": 241, "text": "8.8. discrete choice models\n\n229\n\nthe logit model. the probit model based on iid distributions has a similar property as demon-\nstrated by hausman and wise (1978). in fact, the same sort of counterintuitive results are found\nfor all choice systems that share a property called simple scalability (krantz, 1964). simple\nscalability means that there exist scales v1, . . . , uk and functions f2, . . . , fk that determine the\nchoice probability for b = {i1, . . . , im} by\n\npb(r) = fm(vir , . . . , vim),\n\nwhere fm is strictly increasing in the first argument and strictly decreasing in the remaining\nm \u2212 1 arguments. it is easily shown that simple scalability holds for the multinomial logit\nmodel.\nwhenever for all r, s \u2208 b(cid:2)c and t \u2208 c\n\ntversky (1972) shows that simple scalability is equivalent to order independence that holds\n\npb(r) \u2265 pb(s)\n\n\u21d4\n\npc\u222a{r}(t) \u2264 pc\u222a{s}(t).\n\nthis is a weaker version of the independence of irrelevant alternatives, which implies that only\nthe order of pb(r) and pb(s), and not necessarily their ratio, is independent of b.\n\nthe independence of irrelevant alternatives raises problems when one wants to combine re-\nsults from different choice sets, which is frequently wanted in econometric applications. how-\never, if the choice set is fixed, and each person faces the full set of alternatives, the counterin-\ntuitive results have no relevance. for the simultaneous treatment of different choice sets, the\nmost widely used model in econometrics is mcfadden\u2019s nested multinomial model, which is\nsketched in the following section.\n\npair comparison models\n\nin pair comparison systems only two alternatives are compared at a time. therefore, the family\nof subsets that is considered is given by b = {{i, j}, i, j \u2208 k}}. the resulting pair comparison\nmodels are useful in psychometrics to scale stimuli or in marketing to scale the attractiveness\nof product brands. moreover, it is often used in sport competitions when one wants to measure\nthe ability of a team or a player. the most frequently used model is the logistic model\n\np{r,s}(r) =\n\nexp(ur \u2212 us)\n1 + exp(ur \u2212 us) ,\n\n(8.13)\n\nwhich results from the assumption of a gumbel distribution for the residual variation. it is also\ncalled the btl (bradley-terry-luce) model, with reference to bradley and terry (1952) and\nluce (1959).\nif ur > us, the probability that stimuli r is preferred over s (or that player r will win\nagainst player s) increases with the difference of attractiveness (or ability) ur \u2212 us. since\nties are not allowed, in the simple model the probability is 0.5 if ur = us. the advantage\nof the model is that one obtains the attractiveness of stimuli on a one-dimensional scale, and\nestimates can be used to predict the future outcome. when one assumes that the comparisons\nare independent, simple logit models apply for estimation. the set of stimuli can be treated as\na factor and the design matrix specifies the differences between the alternatives. of course a\nreference alternative has to be chosen, for example, by setting u1 = 0.\n\n "}, {"Page_number": 242, "text": "230\n\nchapter 8. multinomial response models\n\nthe model is easily extended to incorporate an order effect. by specifying\n\np{r,s}(r) =\n\nexp(\u03b1 + ur \u2212 us)\n1 + exp(\u03b1 + ur \u2212 us) ,\n\n(8.14)\n\none obtains p{r,s}(r) = exp(\u03b1)/1 + exp(\u03b1) for the preference of r over s. in sports compe-\ntitions, \u03b1 represents the home advantage; when stimuli are rated it refers to the order in which\nthe stimuli are presented. moreover, category-specific variables can be included in the model;\nfor literature see section 8.11.\n\nexample 8.7: paired comparison\nrumelhart and greeno (1971) asked 234 college students for their preferences concerning nine famous\npersons. for each of the 36 pairs the subjects were instructed to choose the person with whom they would\nrather spend an hour discussing a topic of their choosing. table 8.8 shows the data, and table 8.9 gives\nthe estimates of the fitted btl model. one might want to distinguish between the effect of the profession\nand the effect of the person by including profession in the predictor. let the scale value be structured as\nur = wr1\u03b3r + wr2\u03b32 + \u03b4r, where wr1 = 1 if person r is a politician (0 otherwise), and wr2 = 1 if person\nr is a sportsman (0 otherwise). actor is the reference category and \u03b4r is the additional effect of the person\nwith reference \u03b42 = 0 for politicians, \u03b45 = 0 for sportsmen, and \u03b49 = 0 for actors. table 8.10 shows that\nsportsmen are distinctly preferred over actors, but not politicians.\n\ntable 8.8: pair comparison referring to politicians harold wilson (1), charles de gaulle\n(2), and lyndon b. johnson (3); sporstmen johnny unitas (4), carl yastrazemski (5), and\na. foyt (6); and actors brigitte bardot (7), elizabeth taylor (8), and sophia loren (9).\n\n1\n\n\u2013\n75\n71\n59\n51\n55\n61\n74\n92\n\n2\n\n159\n\u2013\n96\n70\n62\n74\n78\n112\n112\n\n3\n\n163\n138\n\u2013\n89\n77\n94\n96\n112\n114\n\n4\n\n175\n164\n145\n\u2013\n58\n119\n110\n148\n173\n\n5\n\n183\n172\n157\n176\n\u2013\n157\n139\n162\n173\n\n6\n\n179\n160\n140\n115\n77\n\u2013\n100\n142\n163\n\n7\n\n173\n156\n138\n124\n95\n134\n\u2013\n167\n186\n\n8\n\n160\n122\n122\n86\n72\n92\n67\n\u2013\n147\n\n9\n\n142\n122\n120\n61\n61\n71\n48\n87\n\u2013\n\n\u03c3\n\n1334\n1109\n989\n780\n553\n796\n699\n1004\n1160\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\ntable 8.9: estimated coefficients for pair comparison.\n\npersons\n\nui\n\nstandard deviation\n\npoliticians\n\nsportsmen\n\nactors\n\n1 wi\n2 ga\n3 jo\n\n4 un\n5 ya\n6 fo\n\n7 bb\n8 et\n9 sl\n\n-0.382\n0.106\n0.350\n\n0.772\n1.260\n0.739\n\n0.940\n0.319\n0.000\n\n0.066\n0.064\n0.064\n\n0.065\n0.067\n0.065\n\n0.065\n0.066\n0.064\n\n "}, {"Page_number": 243, "text": "8.9. nested logit model\n\n231\n\ntable 8.10: pair comparison data with effects of profession separated.\n\neffects\n\nstandard deviation\n\n\u03b31 (politicians)\n\u03b32 (sportsmen)\n\u03b41 (wi)\n\u03b43 (jo)\n\u03b44 (un)\n\u03b46 (fo)\n\u03b47 (bb)\n\u03b48 (et)\n\n0.087\n1.250\n\u22120.491\n0.246\n\u22120.491\n\u22120.518\n0.935\n0.317\n\n0.093\n0.078\n0.067\n0.064\n0.066\n0.067\n0.065\n0.065\n\n8.9 nested logit model\nthe assumption of iid noise variables \u03b5i yields models with certain weaknesses when some\nof the alternatives are similar. these weaknesses can be ameliorated when using the nested\nlogit model that has been proposed in several papers by mcfadden (1981, 1978). for illus-\ntration we adopt amemiya\u2019s (1985) approach to start with the red bus\u2013blue bus problem. let\nur = ur + \u03b5r, r = 1, 2, 3, be the utilities associated with driving, red bus, and blue bus. mc-\nfadden proposed taking into account the similarity between alternatives 2 and 3 by assuming\nthe bivariate extreme-value distribution function\n\nf (\u03b52, \u03b53) = exp{\u2212[exp(\u2212\u03b52/\u03b4) + exp(\u2212\u03b53/\u03b4)]\u03b4}\n\nwith \u03b4 \u2208 (0, 1]. for the distribution of \u03b51 the usual extreme value distribution f (\u03b51) =\nexp(\u2212 exp(\u2212\u03b51)) is assumed. the total distribution function for all residuals is\n\nf (\u03b51, \u03b52, \u03b53) = exp{\u2212 e\u03b51 \u2212[e\n\n\u2212\u03b52/\u03b4 + e\n\n\u2212\u03b53/\u03b4]\u03b4}.\n\nit may be shown that the correlation between \u03b51 and \u03b52 is \u00012 = 1 \u2212 \u03b42. in the independence\ncase \u0001 = 0 (\u03b4 = 1), f (\u03b51, \u03b52, \u03b53) becomes the product of the three extreme value (gumbel-)\ndistributions and the multinomial logit model follows. otherwise, one obtains the trichotomous\nnested logit model\n\nand\n\np (y = 1) =\n\nexp(u1)\n\nexp(u1) + [exp(u2/\u03b4) + exp(u3/\u03b4)]\u03b4 ,\n\np (y = 2|y \u2208 {2, 3}) =\n\nexp(u2/\u03b4)\n\nexp(u2/\u03b4) + exp(u3/\u03b4) .\n\n(8.15)\n\n(8.16)\n\nthe conditional model (8.16) that specifies the choice between the two similar alternative is\nagain a logit model. the choice between driving and taking buses, given by (8.15), is not a\nlogit model in the strict sense, although it is similar to a logit model, but with a sort of weighted\naverage of exp(u2) and exp(u3) in the denominator. for \u03b4 = 1 the model given by (8.15)\nand (8.16) simplifies to a simple logit model. therefore, the hypothesis of independence from\nirrelevant alternatives may be tested by testing \u03b4 = 1 against the alternative of the nested logit\nmodel (for an example see hausman and mcfadden, 1984).\n\nexample 8.8: red bus\u2013blue bus\nwhen using the nested logit model (8.17) and (8.18), the model constructed from the maximization of the\nrandom utilities does not suffer from the independence of the irrelevant alternatives. by defining\n\ng(y1, y2, y3) = y1 + (y1/\u03b4\n\n2 + y1/\u03b4\n\n3\n\n)\u03b4,\n\n "}, {"Page_number": 244, "text": "232\n\nchapter 8. multinomial response models\n\nthe underlying distribution function is\n\nf (\u03b51, \u03b52, \u03b53) = exp(\u2212g(e\nand one obtains for the choice among alternatives {1, 2, 3}\n\n\u2212\u03b51 , e\n\n\u2212\u03b52 , e\n\n\u2212\u03b53 )),\n\np{1,2,3}(y = 1) =\n\neu1\n\ng(eu1 , eu2 , eu3 )\n\n,\n\np{1,2,3}(y = 2) =\n\np{1,2,3}(y = 3) =\n\neu2/\u03b4\n\n(eu2/(\u03b4) + eu3/\u03b4)\u03b4g(eu1 , eu2 , eu3 )\n\neu3/\u03b4\n\n(eu2/(\u03b4) + eu3/\u03b4)\u03b4g(eu1 , eu2 , eu3 )\n\n,\n\n.\n\nfor the choice between alternatives 1 and 2 one obtains\n\np{1,2}(y = 1) =\n\neu1\n\neu1 + eu2\n\n,\n\np{1,2}(y = 2) = 1 \u2212 p{1,2}(y = 1),\n\nand therefore\n\np{1,2,3}(y = 1)\np{1,2,3}(y = 2)\n\n(cid:11)=\n\np{1,2}(y = 1)\np{1,2}(y = 2)\n\n.\n\nin the general case let the set of alternatives k = {1, . . . , k} be partitioned into distinct\nsets s1, . . . , sm of similar alternatives, such that k = s1 \u222a \u00b7\u00b7\u00b7 \u222a sm. the joint distribution\nproposed by mcfadden has the form\n\nf (\u03b51, . . . , \u03b4k) = exp\n\n\u03b1t[\n\nexp(\u2212\u03b5r/\u03b4t)]\u03b4t\n\n.\n\none obtains the nested logit model\n\np (y \u2208 st) =\n\n(cid:14)\n\n,\n\n(8.17)\n\n&\n\n(cid:7)\n\nr\u2208st\n\n(cid:2)\n\n\u2212 m(cid:7)\n(cid:27)(cid:14)\n\nt=1\n\n\u03b1t\nm\ni=1 \u03b1i[\n\nr\u2208st\n\n(cid:14)\n(cid:14)\n\n(cid:28)\n\nexp(ur/\u03b4t)\n\n\u03b4t\n\nr\u2208si\n\nexp(ur/\u03b4i)]\u03b4i\n\np (y = r|y \u2208 st) =\n\nexp(ur/\u03b4t)\ni\u2208st\n\nexp(ui/\u03b4t) .\n\n(8.18)\n\nthe predictors come into the model by specifying ur = xt\nr \u03b2. the resulting model may be\nestimated by maximum likelihood methods. however, for large-scale models the computation\nbecomes troublesome. therefore, often a two-step method is used that first estimates \u03b2/\u03b41 by\nusing (8.18). then the estimates are plugged into (8.17) to obtain estimates of \u03b41 and \u03b1t (for\ndetails see mcfadden, 1981).\n\nmcfadden (1978) introduced a more general model based on the generalized extreme-value\ndistribution. the distributions considered in the three alternatives case and the more general\ncase of partioning into m subsets may be generalized to\n\nf (\u03b51, . . . , \u03b5k) = exp(\u2212g(e\n\n\u2212\u03b51 , . . . , e\n\n\u2212\u03b5k));\n\nwhere g satisfies the conditions\n(1) g(y1, . . . , yk) \u2265 0 for\n\nyi \u2265 0;\n\n "}, {"Page_number": 245, "text": "8.10. regularization for the multinomial model\n\n233\n\n(2)\n\nyi\u2192\u221e g(y1, . . . , yk) = \u221e,\n\nlim\n\ni = 1, . . . , k;\n\n(3) g(\u03b1y1, . . . , \u03b1yk) = \u03b1\u03c8g(y1, . . . , yk);\n\u2265 0\n\u2264 0\n\n(4) \u2202gs(y1, . . . , yk)/(\u2202yi1 , . . . , \u2202yis)\n\n(cid:2)\n\ns\n\nis odd\n\nif\nif k is even.\n\nwhen ur = ur + \u03b5r the probabilities following from maximization of the random utility are\ngiven by\n\n\u2202g\n\u2202yr\n\np (y = r) = eur\n\n(8.19)\nthe multinomial logit model follows as the special case where g(y1, . . . , ym) = y1 +\u00b7\u00b7\u00b7 + yk.\nthe nested logit model with the similar alternatives given by the partition s1, . . . , sm is based\non g(y1, . . . , ym) =\n\n(eu1 , . . . , euk)/g(eu1 , . . . , euk).\n\n(cid:14)\n\n(cid:14)\n\ny1/\u03b4\nr\n\n]\u03b4.\n\nm\n\nthe models considered so far may be seen as implying two levels of nesting with the original\nset being subdivided into the sets s1, . . . , sm. in the general concept, three and higher level\nnested models also may be constructed (see mcfadden, 1981). although models based on\ngeneralized extreme-value distributions are more general, in applications the nested logit model\nis the dominating model from these class of models.\n\nt=1 \u03b1t[\n\nr\u2208st\n\n8.10 regularization for the multinomial model\nin chapter 6 various regularization methods, including variable selection, were discussed. in\nthe following the methods are extended to the case of multicategorical responses. let us con-\nsider the multinomial logit model in symmetrical form:\n\np (y = r|x) =\n\nexp(\u03b2r0 + xt \u03b2r)\ns=1 exp(\u03b2s0 + xt \u03b2s)\n\n\u03c3k\n\n,\n\n(8.20)\n\nwhere the intercept has been separated from the predictors. regularization methods based on\npenalization are again based on the penalized log-likelihood:\n\nn(cid:7)\n\ni=1\n\nlp(\u03b2) =\n\nli(\u03b2) \u2212 \u03bb\n\n2 j(\u03b2),\n\nwhere li(\u03b2) is the usual log-likelihood contribution of the ith observation, \u03bb is a tuning pa-\nrameter, and j(\u03b2) is a functional that penalizes the size of the parameters. maximizing the\npenalized log-likelihood lp(\u03b2) rather than the unpenalized log-likelihood is advantageous in\nparticular when many predictors are available and ml estimates do not exist.\n\n(cid:14)\n\nridge-type penalties\nfor a one-dimensional response, one of the simplest penalties is the ridge penalty, which uses\nthe functional j(\u03b2) =\ni . it penalizes the length of the parameter \u03b2 and yields estimates\nthat are shrinked towards zero. in the multicategorical case one has not only one parameter\nvector but a collection of parameter vectors \u03b21, . . . , \u03b2k, which, for reasons of identifiability,\nhave to be constrained. when parameters are penalized it is natural to use the symmetric side\nconstraint\n\ns = (0, . . . , 0) with a ridge penalty that has the form\n\np\ni=1 \u03b22\n\n(cid:14)\n\ns=1 \u03b2t\n\nk\n\nk(cid:7)\n\np(cid:7)\n\np(cid:7)\n\nr=1\n\nj=1\n\nj=1\n\nj(\u03b2) =\n\nrj =\n\u03b22\n\n\u03b2t\n\n.j\u03b2.j,\n\n(8.21)\n\n "}, {"Page_number": 246, "text": "234\n\nchapter 8. multinomial response models\n\np(cid:7)\n\n.j = (\u03b21j, . . . , \u03b2kj) collects the parameters associated with the jth variable and the\n\nwhere \u03b2t\nside constraints 1t \u03b2.j = 0 have to hold for j = 0, . . . , p.\n\nit is noteworthy that then the penalty for the side constraint \u03b2k = 0, which specifies k\nas reference category, does not have the usual form of the ridge penalty. this may be seen\nfrom looking at the transformations of parameters when changing constraints. let \u02dc\u03b2\n.j =\n( \u02dc\u03b21j, . . . , \u02dc\u03b2k\u22121,j)t denote the shortened vector of parameters with side constraints \u02dc\u03b2kj = 0,\n.j = (\u03b21j, . . . , \u03b2k\u22121,j)t the shortened vector with symmetric side con-\nj = 0, . . . , p, and \u03b2c\nstraints \u03b2k,j = \u2212\u03b21j \u2212 \u00b7\u00b7\u00b7 \u2212 \u03b2qj, q = k \u2212 1. then the transformation \u03b2c\n\u239e\nc\n.j is given\nby\n\u239f\u239f\u239f\u23a0\n\n\u239b\n\u239c\u239c\u239c\u239d \u03b21j\n\n\u239e\n\u239f\u239f\u239f\u23a0 =\n\n\u00b7\u00b7\u00b7 \u2212 1\n\n.j = t \u02dc\u03b2\n\n\u239b\n\u239c\u239c\u239c\u239d\n\n\u2212 1\n...\n\n(8.22)\n\nk\n\nk\n\nc\n\n\u239e\n\u239f\u239f\u239f\u239f\u23a0\n\n\u02dc\u03b21j\n\u02dc\u03b22j\n...\n\n...\n\nk\u22121\nk\n\u2212 1\n...\n\u2212 1\n\u22121\u03b2c\nthe inverse transformation \u02dc\u03b2\nsingle parameters the transformations are\n\n.j = t\n\n\u03b22j\n...\n\n\u03b2k\u22121,j\n\nk\n\nk\n\nc\n\n\u22121 = i + 11t , where 1t = (1, . . . , 1). for\n\nk\u22121\nk\n\n\u02dc\u03b2k\u22121,j.\n\n.j uses t\n\n\u239b\n\u239c\u239c\u239c\u239c\u239d\n(cid:7)\n\n\u03b2rj = ((k \u2212 1) \u02dc\u03b2rj \u2212\n\n\u02dc\u03b2sj)/k,\n\n\u02dc\u03b2rj = 2\u03b2rj +\n\n\u03b2sj,\n\ns(cid:5)=q\n\nr = 1, . . . , k \u2212 1. some derivation shows that penalty (8.21) is equivalent to\n\n(cid:7)\n\ns(cid:5)=q\n\np(cid:7)\n\nj(\u03b2) =\n\n(\u03b2c\n\n.j)t t\n\n\u22121\u03b2c\n\n.j\n\nor j(\u02dc\u03b2) =\n\n(\u02dc\u03b2\n\nc\n\n.j)t t \u02dc\u03b2\n\nc\n.j,\n\n(8.23)\n\nwhich uses only the k \u2212 1 parameters that can be identified.\n\nj=1\n\nj=1\n\nwhen computing estimates it is helpful to have the penalty in a closed form with the pa-\nr = (\u03b2r0, . . . , \u03b2rp) denote\nrameters given in the usual ordering. let \u03b2t = (\u03b2t\nthe whole vector with symmetric side constraints, where it is assumed that the covariate vector\nx contains an intercept and p predictors. the corresponding parameter vector with reference\nr = ( \u02dc\u03b2r0, . . . , \u02dc\u03b2rp). then a closed form of the penalty is\ncategory k is \u02dc\u03b2\n\n1 , . . . , \u02dc\u03b2\n\n1 , . . . , \u03b2t\n\nt = (\u02dc\u03b2\n\nq ), \u03b2t\n\nq ), \u02dc\u03b2\n\nt\n\nt\n\nt\n\nt\n\nt 1\n\n\u02dc\u03b2,\n\nj(\u03b2) = \u03b2t t 0\u03b2 or j(\u02dc\u03b2) = \u02dc\u03b2\n\nwhere t 0 = t\nfrom the usual identity matrix by having values zero in the first column.\n\n(8.24)\n\u22121 \u2297 i 0, t 1 = t \u2297 i 0, with i 0 denoting a (p + 1)\u00d7 (p + 1)-matrix that differs\nthen penalized estimates for the constraint \u02dc\u03b2kj = 0 can be computed by using the design\nmatrix and link function given in section 8.5, where the logit model is shown to be a mul-\ntivariate glm. the penalized estimates are obtained by solving sp(\u02dc\u03b2) = 0, where sp(\u02dc\u03b2) is\nthe penalized score function sp(\u02dc\u03b2) = s(\u02dc\u03b2) \u2212 \u03bb\n\u02dc\u03b2. iterative fisher scoring uses the\ncorresponding penalized fisher matrix f p(\u02dc\u03b2) = f (\u02dc\u03b2) + \u03bbt 1. for the definition of s(\u02dc\u03b2) and\nf (\u02dc\u03b2), see section 8.6.1.\n\np\nj=1 t 1\n\n(cid:14)\n\nalternatively, one can work with symmetric side constraints. however, then the design\nmatrix has to be adapted to symmetric side constraints. with the design matrix x for reference\n\u22121\u2297\ncategory k (see section 8.5) one has x \u02dc\u03b2, which transforms into x 0\u03b2, where x 0 = x(t\n\u22121\u2297i)\u03b2. zahid and tutz (2009) demonstrated in simulation studies that ridge\ni) since \u02dc\u03b2 = (t\nestimates outperform ml estimates in terms of mean squared errors that refer to the estimation\nof the parameter vector and the underlying probability vector. a further advantage of ridge\n\n "}, {"Page_number": 247, "text": "8.10. regularization for the multinomial model\n\n235\n\nestimation is that estimates can be computed even when ml estimates fail to exist. a penalized\nregression for the multinomial logit model with ridge-type penalties was also investigated by\nzhu and hastie (2004).\n\npenalties including selection of parameters and predictors\na general penalty for multicategorical responses has the additive form\n\nk(cid:7)\n\np(cid:7)\n\n\u03bbj(\u03b2) =\n\np\u03bb(|\u03b2rj|),\n\nr=1\n\nj=1\n\nwhere p\u03bb(|\u03b2|) are penalty functions that may depend on \u03bb. the multicategorical ridge and lasso\nare special cases with function p\u03bb(|\u03b2rj|) = \u03bb|\u03b2rj|2 and p\u03bb(|\u03b2rj|) = \u03bb|\u03b2rj|, respectively. since\nshrinkage should not depend on the reference category, the penalties should use the symmetric\nconstraints that transform to different functions when reference categories are used.\n\nmultinomial logistic regressions with lasso-type estimates that use a reference category\nwere considered by krishnapuram et al. (2005) with the focus on classification. friedman et al.\n(2010) used the elastic net penalty p\u03bb(|\u03b2rj|) = \u03bb{(1/2)(1 \u2212 \u03b1)|\u03b2rj|2 + \u03b1|\u03b2rj|} and proposed\nusing coordinate descent in the form of partial newton steps. however, in that approach selec-\ntion refers to parameters and not to predictors. therefore, it might occur that a predictor is kept\nin the model because only one of the parameters linked to that predictor is needed. with the\nfocus on variable selection it seems more appropriate to penalize the group of parameters that\nis associated with one variable. penalties of that type are a modification of (8.21). a lasso-type\npenalty that is similar to the grouped lasso but where grouping refers to response categories\nrather than groups of dummy variables associated to a categorical predictor is\n\np(cid:7)\n\np(cid:7)\n\nj(\u03b2) =\n\n||\u03b2.j\n\n||2 =\n\n1j + \u00b7\u00b7\u00b7 + \u03b22\n(\u03b22\n\nkj)1/2.\n\nj=1\n\nj=1\n\nit enforces the selection of the predictors in the sense that whole predictors are deleted. the\npenalty is a special case of the composite absolute penalty family proposed by zhao et al.\n(2009). in multicategorical modeling the predictor can also include category-specific covari-\nates. the corresponding predictor of the multinomial model has the form \u03b7r = xt \u03b2r +\n(wrk)t \u03b1, where \u03b1t = (\u03b11, . . . , \u03b1m) weights the category-specific variables wrk (see sec-\ntion 8.4). selection of both types of variables, global and category-specific ones, is enforced by\nusing\n\np(cid:7)\n\nm(cid:7)\n\nj(\u03b2) = \u03b3\n\n||\u03b2.j\n\n||2 + (1 \u2212 \u03b3)\n\n||\u03b1j||,\n\nwhere \u03b3 steers the amount of penalization exerted on the two types of variables.\n\nj=1\n\nj=1\n\nexample 8.9: party choice\nin spatial election theory it is assumed that each voter has a utility function in a finite-dimensional space\nthat characterizes the profiles of parties and voters (see, for example, thurner and eymann, 2000). let the\nlatent utility for voter i and party r be given by uir = xt\nir\u03b1, where the xi\u2019s are global variables\nthat characterize the voter and the wir\u2019s represent category-specific variables that are specific for the\nparty (compare section 8.4). in spatial election theory the category-specific variables are determined as\nthe distance between the position of the voter on a specific scale (the \"ideal point\") and the perceived\nposition of the party on this scale. the scales represent policy dimensions, for example, the attitude\ntoward the use of nuclear energy. we consider data from the german longitudinal election study with the\n\ni \u03b3r + wt\n\n "}, {"Page_number": 248, "text": "236\n\nchapter 8. multinomial response models\n\nunion\n\nwest\n\nage\n\n\u03b2\n\n\u03b2\n\n8\n0\n\n.\n\n5\n\n2\n\n34\n\n6\n0\n\n.\n\n4\n0\n\n.\n\n2\n0\n\n.\n\n0\n0\n\n.\n\n4\n\n2\n\n5\n\n3\n\n5\n0\n\n.\n\n4\n0\n\n.\n\n3\n0\n\n.\n\n2\n0\n\n.\n\n1\n0\n\n.\n\n0\n0\n\n.\n\n\u03b2\n\n0.0\n\n0.5\n\n1.0\n\n\u03bb\n\n1.5\n\n2.0\n\n0.0\n\n0.5\n\n1.5\n\n2.0\n\n1.0\n\n\u03bb\n\n\u03b2\n\n0\n0\n0\n\n.\n\n.\n\n1\n0\n0\n\u2212\n\n.\n\n2\n0\n0\n\u2212\n\n3\n0\n\n.\n\n0\n\u2212\n\n2\n\n5\n3\n\n4\n\n0.0\n\n0.5\n\n1.0\n\n\u03bb\n\n1.5\n\n2.0\n\ndemocracy\n\nreligion_2\n\nreligion_3\n\n5\n\n3\n4\n2\n\n0\n1\n\n.\n\n8\n\n.\n\n0\n\n6\n.\n0\n\n4\n.\n0\n\n2\n.\n0\n\n0\n.\n0\n\n\u03b2\n\n0\n0\n\n.\n\n2\n.\n0\n\u2212\n\n4\n.\n0\n\u2212\n\n6\n.\n0\n\u2212\n\n3\n\n2\n\n4\n\n5\n\n\u03b2\n\n34\n\n5\n\n2\n\n5\n0\n\n.\n\n4\n\n.\n\n0\n\n3\n.\n0\n\n2\n.\n0\n\n1\n.\n0\n\n0\n.\n0\n\n0.0\n\n0.5\n\n1.0\n\n\u03bb\n\n1.5\n\n2.0\n\n0.0\n\n0.5\n\n1.5\n\n2.0\n\n0.0\n\n0.5\n\n1.0\n\n\u03bb\n\n1.5\n\n2.0\n\n1.0\n\n\u03bb\n\nfigure 8.7: coefficient buildups for selected global variables of party choice data.\n\nalternatives christian democratic union (cdu: 1), the social democratic party (spd: 2), the green party\n(3), the liberal party (fdp: 4), and the left party (die linke: 5). the global predictors are age, political\ninterest (1: less interested; 0: very interested), religion (1: evangelical; 2: catholic; 3: otherwise), regional\nprovenance (west; 1: former west germany; 0: otherwise), gender (1: male; 0: female), union (1: member\nof a union; 0: otherwise), satisfaction with the functioning of democracy (democracy; 1: not content; 0:\ncontent), unemployment (1: currently unemployed; 0: otherwise), and high school degree (1: yes; 0:\nno). the included policy dimensions are attitude toward the immigration of foreigners, attidude toward\nthe use of nuclear energy, and the positioning on a political left-right scale (left). figure 8.7 shows the\nbuildups of global variables resulting from lasso-type regularization. only variables that turned out to be\ninfluential are shown. the vertical line shows the selected smoothing parameter based on cross-validation.\nfigure 8.8 shows the buildups for the category-specific variables. it is seen that only the left-right scale is\ninfluential; the other two dimensions can be neglected. for the interpretation it is useful to consider again\ni \u03b2r + (wir \u2212 wik)t \u03b1\nthe construction of the predictor. the linear predictor has the form \u03b7ir = \u03b2r0 + xt\nwith the predictor wir denoting the distance between the position of the voter on a specific scale and the\nperceived position of the party on this scale. therefore, if for the rth party this distance is larger than for\n\n\u03b3\n\n.\n\n1\n0\n\u2212\n\n.\n\n3\n0\n\u2212\n\n.\n\n5\n0\n\u2212\n\n.\n\n7\n0\n\u2212\n\nr\n\nl\n\n0\n\ncategory\u2212specific\n\n2\n\n4\n\n6\n\n8\n\n10\n\n\u03bb\n\nfigure 8.8: coefficient buildups for category-specific variables of party choice data (l\ndenotes left right scale, r denotes the rest).\n\n "}, {"Page_number": 249, "text": "8.10. regularization for the multinomial model\n\n237\n\nthe kth party, the probability for the rth party is reduced because the weight \u03b1 on the left-right scale is\nnegative.\n\nalternatively, one can use likelihood-based boosting techniques as given in section 6.3.2.\nthe likelihood procedure is essentially the same as genboost, but the underlying glm now is\nmultivariate. to obtain predictor selection rather than parameter selection, one includes within\none step just one variable and selects the variable that improves the fit maximally. by including\none variable at a time, all the parameters that refer to that variable are updated. with k response\ncategories that means k\u22121 parameters for one metric variable. if predictors are categorical, the\nnumber increases because all the dummy variables for all the response categories are included.\nfor details see zahid and tutz (2010).\n\ntable 8.11: parameter estimates for the contraceptive methods data (response categories:\nno-use, long-term, and short-term) with lasso approach and boosting. for the boosting\nestimates, deviance was used as the stopping criterion with 10-fold cross-validation, and\ndeviance was also used for predictor selection.\n\nlasso\n\nboosting\n\npredictor\n\nno-use\n\nlong-term short-term\n\nno-use\n\nlong-term short-term\n\nwife.age\n\nwife.edu2\nwife.edu3\nwife.edu4\n\nhusband.edu2\nhusband.edu3\nhusband.edu4\n\nchildren\nwife.religion\n\nwife.working\n\nhusband.job2\nhusband.job3\nhusband.job4\n\nsol.index2\nsol.index3\nsol.index4\n\nmedia\n\n0.0468\n\u22120.0371\n\u22120.3581\n\u22120.9607\n0\n0\n0\n\u22120.3449\n0.3522\n\u22120.0130\n0\n0\n\u22120.4590\n\u22120.3402\n\u22120.4528\n\u22120.6841\n0.5316\n\n0\n\n0.6058\n1.0610\n1.3694\n\u22120.7669\n\u22120.5997\n\u22120.5686\n0\n\u22120.1795\n0\n\u22120.4072\n\u22120.2178\n0\n\n0.0219\n0.2433\n0.2440\n\n0\n\n\u22120.0593\n0\n0\n0\n\n1.0697\n1.2333\n1.0176\n\n0\n\n0\n\n0.1527\n\n0.0162\n0.2897\n0.0312\n\n0\n0\n0\n\n0\n\n0.3683\n\u22120.1072\n\u22120.2872\n\u22120.5445\n\u22120.0397\n\u22120.0872\n\u22120.0764\n\u22120.4979\n0\n\n0\n\n0\n0\n0\n\n0\n0\n0\n\n0\n\n0.0715\n\n0.1303\n0.3390\n0.5760\n\u22120.2146\n\u22120.2770\n\u22120.2628\n0.2407\n\n\u22120.4397\n\u22120.0231\n\u22120.0518\n\u22120.0315\n0.2543\n0.3642\n0.3392\n\n0.2572\n\n0\n\n0\n\n0\n0\n0\n\n0\n0\n0\n\n0\n\n0\n\n0\n\n0\n0\n0\n\n0\n0\n0\n\n0\n\nexample 8.10: contraceptive method\na subset of the 1987 national indonesia contraceptive prevalence survey is available from the uci\nmachine learning repository. the sample comprises 1473 married women who were either not pregnant\nor did not know if they were at the time of interview. the problem is to analyze the current contraceptive\nmethod choice (no use, long-term methods, or short-term methods) of a woman based on 10 demographic\nand socio-economic characteristics as: wife\u2019s age, wife\u2019s education (wife.edu; 1 = low, 2, 3, 4 = high),\nhusband\u2019s education (husband.edu; 1 = low, 2, 3, 4 = high), number of children ever born, wife\u2019s religion\n(0 = non-islam, 1 = islam), wife\u2019s current working (0 = yes, 1 = no), husband\u2019s occupation (categorical:\n1, 2, 3, 4), standard of living (sol.index; 1 = low, 2, 3, 4 = high), and media exposure (0 = good, 1 = not\ngood).\n\n "}, {"Page_number": 250, "text": "238\n\nchapter 8. multinomial response models\n\nfigure 8.9: coefficient buildups obtained by boosting for contraceptive method choice\ndata. the vertical dotted line represents the optimal boosting iteration number on the basis\nof 10-fold cross-validation when deviance is used as a stopping criterion.\n\na multinomial logit model with selection of variables was fitted by using the blockwise boosting\nalgorithm with the deviance being used for variable selection among the candidate predictors. the op-\ntimal number of boosting iterations was decided by deviance with 10-fold cross-validation. for metric\nand nominal predictors, a ridge-type estimator was used for candidate predictors; for ordered predictors,\nadjacent categories were penalized to obtain a smooth estimate of effects. alternatively, lasso estimates,\nwhich select parameters instead of variables, were obtained by use of the glmnet package (friedman et al.,\n2010). the optimal value of the penalty term was also based on 10-fold cross-validation.\n\nestimates are given in table 8.11. since in the lasso approach all variables are found relevant when\nat least one predictor (or dummy associated with the categorical predictor) has non-zero estimate(s) for at\nleast one response category, one obtains no reduction of predictors. in contrast, boosting performs variable\nselection by grouping all the parameters associated with one predictor. consequently, it recommends just\nfour informative predictors. the informative predictors include two continuous predictors, that is, wife\u2019s\nage and number of children ever born, and two categorical predictors (with all of their categories), that is,\nwife\u2019s education and husband\u2019s education. the coefficient buildup for boosting is shown in figure 8.9.\nfor more details and further examples see zahid and tutz (2010).\n\n8.11 further reading\ndiscrete choice. an overview on probabilistic choice models was given by mcfadden (1981).\nmodels for discrete choice are mostly treated in the econometric literature, for example, by\ngreene (2003) and maddala (1983). estimation of nested logit models was investigated by\nmcfadden (1981), amemiya (1978), ben-akiva and lerman (1985), hausman and mcfadden\n(1984), b\u00f6rsch-supan (1987), and brownstone and small (1989).\n\npaired comparison models. an overview on pair comparison models was given by bradley\n(1976, 1984). basic modeling concepts were considered by yellott (1977) and colonius (1980).\n\n "}, {"Page_number": 251, "text": "8.12. exercises\n\n239\n\nmarketing applications are found in dillon et al. (1993). extensions to incorporate ties were\ngiven by davidson (1970) and rao and kupper (1967); more general model with ordered re-\nsponse categories were proposed by agresti (1992a), tutz (1986), and b\u00f6ckenholt and dillon\n(1997). the inclusion of category-specific variables was discussed in dittrich et al. (1998).\n\nr packages. multinomial response models with global predictors can be fitted using the\nfunction multinom from the package nnet. models that contain global and category-specific\npredictors can be fitted by using the function mlogit from the package mlogit. paired compar-\nison models can be fitted by using prefmod or bradleyterry2. the package glmnet (friedman\net al., 2010) fits the lasso and elastic net for multionmial responses.\n\n8.12 exercises\n\n8.1 consider the multinomial distribution yt = (y1, . . . , yk) \u223c m(m, \u03c0), where \u03c0t = (\u03c01, . . . , \u03c0k) is\na vector of probabilities restricted by \u03c0i \u2208 [0, 1],\n\ni \u03c0i = 1.\n\n(cid:2)\n\n(a) give an experiment, for example an urn model, for which the outcomes follow a multinomial\n\ndistribution.\n\n(a) how are single components yi distributed?\n(b) derive the mean e(y), the variance of components var(yi), and the covariance cov(yi, yj).\n\n(cid:5)\ns=1 \u03b2s = 0 for the multinomial logit model. show that then\ns=1 p (y = s|x))1/k is the geometric\n\n8.2 assume the symmetric side constraint\nlog(p (y = r|x)/gm (x)) = xt \u03b2r holds, where gm (x) = (\nmean response.\n8.3 consider the multinomial logit model for k categories with parameters \u03b21, . . . , \u03b2q, q = k\u22121, and side\nconstraint \u03b2k = 0. alternatively, one can use parameters \u03b2\n\n\u2217\nq and side constraint\n\n\u2217\n1, . . . , \u03b2\n\n\u2217\nj = 0.\n\n(cid:2)\n\nk\n\nk\n\nj \u03b2\n\n(cid:2)\n\n(a) derive the transformations between the parameters with reference category k and the symmetric\n\nside constraint.\n\n(b) interpret the parameters in table 8.2.\n\n(c) give the parameters of the logit model that was used to obtain table 8.2 when the symmetric side\n\nconstraint is used.\n\n8.4 equation (8.8) shows the likelihood contribution of the ith observation for multinomial data in terms\nof relative frequencies for the categories (pir denotes the relative frequency of category r, and ni the\nnumber of observations for fixed covariate predictor xi).\n\n(a) show that the score function of the multinomial logit model has the form s(\u03b2) =\n\n\u03c0i).\n\n(b) show that the fisher matrix of the multinomial logit model has the form f (\u03b2) =\n\n\u03c3 i(\u03b2)x i.\n\n(cid:2)\n\nn\n\ni=1 nix t\n\ni (pi\n\n\u2212\n\n(cid:2)\n\nn\n\ni=1 n2\n\ni x t\ni\n\n8.5 investigate if an interaction between gender and age is needed for the party preference data in table 8.1.\n\n8.6 the covariance of a multinomial distribution with k categories, n observations, and vector of proba-\nbilities \u03c0t = (\u03c01, . . . , \u03c0q), q = k \u2212 1, has covariance matrix \u03c3 = [ diag ( \u02c6\u03c0) \u2212 \u03c0\u03c0t ]/n.\n\n\u22121 = (\u03c3ij) has entries \u03c3ii = n/\u03c0i +n/(1\u2212(cid:2)\n\n(a) show that the inverse covariance matrix \u03c3\n\nq\n\nr=1 \u03c0r),\n\n\u03c3ij = n/(1 \u2212(cid:2)\n\nq\n\nr=1 \u03c0r), i (cid:11)= j.\n\n "}, {"Page_number": 252, "text": "240\n\nchapter 8. multinomial response models\n\n(b) show that the squared pearson residual \u03c72\n\n\u2212 \u02c6\u03c0i) , where \u02c6\u03c0t\n\ni = (\u02c6\u03c0i1, . . . , \u02c6\u03c0iq), pt\n\npi = ni(pir \u2212 \u02c6\u03c0ir)2/\u02c6\u03c0ir is equivalent to (pi\n\n\u2212 \u02c6\u03c0i)t\ni = (pi1, . . . , piq), and \u03c3 i( \u02c6\u03b2) = [ diag ( \u02c6\u03c0i)\u2212\n\n\u03c3 i(\u02c6\u03b2)\n\u02c6\u03c0i \u02c6\u03c0t\n\n\u22121(pi\ni ]/ni.\n\nc\n\n8.7 let \u02dc\u03b2\nside constraint \u02dc\u03b2kj = 0, j = 0, . . . , p and \u03b2c\nconstraints. derive the matrix t 0 that transforms \u02dc\u03b2\naddition, show that (8.21) is equivalent to (8.23).\n\n.j = (\u02dc\u03b21j, . . . , \u02dc\u03b2k\u22121,j)t denote the vector of parameters corresponding to variable j with\n.j = (\u03b21j, . . . , \u03b2k\u22121,j)t the vector with symmetric side\n\u22121. in\n\nc\n.j and its inverse t\n\n.j = t \u02dc\u03b2\n\n.j into \u03b2c\n\n.j, \u03b2c\n\nc\n\nr\n\n, where rt \u2208 st.\n\n8.8 the hierarchically structured multinomial model for subsets of response categories s1, . . . , sm can\nbe given as log(p (y \u2208 st|x)/p (y \u2208 sm|x)) = xt \u03b2t, log(p (y = r|y \u2208 st, x)/p (y = rt|y \u2208\nst, x)) = xt \u03b2(st)\n(a) consider the case of four response categories with subsets given by s1 = {1, 2} and s2 = {3, 4}.\nshow that the model has the form of a multivariate glm g(\u03c0) = x\u03b2 and derive the link function.\n(b) derive the score function and the fisher matrix for the general case. do the standard errors of co-\nefficients differ from the standard errors obtained by the fitting of model components (for example\nfitting of log(p (y \u2208 st|x)/p (y \u2208 sm|x)) = xt \u03b2t)?\n(c) use the dataset addiction from the package catdata with subsets s1 = {0, 1} and s2 = {2}\n(compare example 8.5). fit the model with the covariates gender, university, and age and test if\ncovariates can be omitted by use of the likelihood ratio test.\n\n8.9 the r package mass contains data on the cross-classification of people in caithness, scotland, by\neye and hair color. the dataset, named caith, is given as a 4 by 5 table with rows the eye colors (blue,\nlight, medium, dark) and columns the hair colors (fair, red, medium, dark, black); see also venables and\nripley (2002). consider eye color as the predictor and hair colour as the response. fit a multinomial logit\nmodel and investigate the significance of the effects.\n\n8.10 the r package mlogit can be used to fit multinomial models with global and category-specific predic-\ntors. the dataset modechoice (package ecdat) contains various predictors for the choice of travel mode\nin australia. fit the appropriate models and interpret the results.\n\n8.11 the r library faraway contains the dataset nes96, which is a subset of the 1996 american national\nelection study (see faraway, 2006; rosenstone et al., 1997). consider the response party identification\nwith the categories democratic, liberal, and republican.\n\n(a) fit a multinomial logit model containing age education level and income group of the respondents\n\nand visualize the dependence of the response categories on the explanatory variable\n\n(b) include further explanatory variables and decide which are relevant.\n\n "}, {"Page_number": 253, "text": "chapter 9\n\nordinal response models\n\nwhen the response categories in a regression problem are ordered one can find simpler models\nthan the multinomial logit model. the multinomial logit wastes information because the order-\ning of categories is not explicitly used. therefore, often more parameters than are really needed\nare in the model. in particular with categorical data, parsimonious models are to be preferred\nbecause the information content in the response is always low.\n\ndata analysts who are not familiar with ordinal models usually seek solutions by inadequate\nmodeling. if the number of response categories is high, for example in rating scales, they ignore\nthat the response is ordinal and use classical regression models that assume that the response\nis at least on an interval scale. thereby they also ignore that the response is categorical. the\nresult is often spurious effects. analysts who are aware of the ordinal scale but are not familiar\nwith ordinal models frequently use binary regression models by collapsing outcomes into two\ngroups of response categories. the effect is a loss of information. armstrong and sloan (1989)\ndemonstrated that the binary model may attain only between 50 and 75% efficiency relative to\nan ordinal model for a five-level ordered response; see also steadman and weissfeld (1998),\nwho in addition consider polytomous models as alternatives.\n\none may distinguish between two types of ordinal categorical variables, grouped continu-\nous variables and assessed ordinal categorical variables (anderson, 1984). the first type is\na mere categorized version of a continuous variable, which in principle can be observed itself.\nfor example, when the duration of unemployment (example 9.1) is categorized into short-term,\nmedium-term, and long-term unemployment, the underlying response variable is the duration of\nunemployment in days. the second type of ordered variable arises when an assessor processes\nan unknown amount of information, leading to the judgement of the grade of the ordered cate-\ngorical scale. this sort of variable is found in the knee injury study (example 1.4), where pain\nafter treatment is assumed on a five-point scale representing an assessed categorical variable.\nalso, the retinopathy status in example 9.2 was assessed by the physician without reference to\nan underlying continuous measurement.\n\ntable 9.1: cross-classification of pain and treatment for knee data.\n\nno pain\n\n1\n17\n19\n\n2\n8\n26\n\n3\n14\n11\n\n4\n20\n6\n\nsevere pain\n\n5\n4\n2\n\n63\n64\n\nplacebo\ntreatment\n\n241\n\n "}, {"Page_number": 254, "text": "242\n\nchapter 9. ordinal response models\n\nexample 9.1: unemployment\nin unemployment studies one often distinguishes between short-term, medium-term, and long-term un-\nemployment, obtaining a categorical response variable with three categories y \u2208 {1, 2, 3}. then one\nmay investigate the effect of gender, age, education level, and other differentiating covariates on the three\nlevels.\n\nexample 9.2: retinopathy\nin a 6-year followup study on diabetes and retinopathy status reported by bender and grouven (1998) the\ninteresting question is how the retinopathy status is associated with risk factors. the considered risk factor\nis smoking (sm = 1: smoker, sm = 0: non-smoker) adjusted for the known risk factors diabetes duration\n(diab) measured in years, glycosylated hemoglobin (gh), which is measured in percent, and diastolic\nblood pressure (bp) measured in mmhg. the response variable retinopathy status has three categories (1:\nno retinopathy; 2: nonproliferative retinopathy; 3: advanced retinopathy or blind).\n\ntable 9.2: types of ordinal models and dichotomous variables yr that are used.\n\n(cid:29)\n\nyr =\n\ncumulative-type model, dichotomization into groups\n1 y \u2208 {1, . . . , r}\n[1, . . . , r|r + 1, . . . , k]\n0 y \u2208 {r + 1, . . . , k}\nsequential-type model, dichotomization given y \u2265 r\ngiven y \u2265 r\n1, . . . , [r|r + 1, . . . , k]\nadjacent-type model, dichotomization given y \u2208 {r, r + 1}\n\n1 y = r\n0 y > r\n\nyr =\n\n(cid:29)\n\n(cid:29)\n\n1, . . . , [r|r + 1], . . . , k\n\nyr =\n\n1 y = r\n0 y \u2265 r + 1\n\ngiven y \u2208 {r, r + 1}\n\none way of modeling ordinal responses is to start from binary models. there are several ap-\nproaches to construct ordinal response models from binary response models. these approaches\ndiffer in how the ordered categories 1, . . . , k are transformed into a binary response. the sim-\nplest approach is to consider a split between categories r and r + 1, yielding the grouped\nresponse categories {1, . . . , r} and {r + 1, . . . , k}. the cumulative approach uses these group-\nings by considering the binary variable yr = 1 if y \u2264 r and yr = 0 if y > r. for the response\ny , the binary response model p (yr = 1|x) = f (xt \u03b2r) turns into\np (y \u2264 r|x) = f (xt \u03b2r), r = 1, . . . , q,\n\nwhere f is the response function of the binary model and \u03b2r may depend on the splitting that is\nmodeled. alternatively, one may consider it as a sequential decision where the transition from\ncategory r to category r + 1 given category r or higher follows a binary model. the binary\nmodel distinguishes between y = r and y > r given y \u2265 r. one obtains the sequential-type\nmodel\n\np (y = r|y \u2265 r, x) = f (xt \u03b2r), r = 1, . . . , q.\n\n "}, {"Page_number": 255, "text": "9.1. cumulative models\n\n243\n\nthe third approach is based on the consideration of adjacent categories. given two adjacent\ncategories, the binary model distinguishes between these two categories by using\n\np (y = r|y \u2208 {r, r + 1}) = f (xt \u03b2r), r = 1, . . . , q.\n\ntable 9.2 illustrates the three types of dichotomizations that determine the models. brack-\nets denote which response categories are used when dichotomizing and the \"|\" determines the\n(conditional) split. in the following the use of these models is motivated and applications are\ngiven.\n\n9.1 cumulative models\nthe most frequently used model is the cumulative model, which was propagated by mccullagh\n(1980).\n\n9.1.1 simple cumulative model\na simple form of the cumulative type model may be derived from the assumption that the\nobserved categories represent a coarser (categorical) version of an underlying (continuous) re-\ngression model. let \u02dcy be an underlying latent variable that follows a regression model:\n\n\u02dcy = \u2212xt \u03b3 + \u0001,\n\nwhere \u0001 is a noise variable with continuous distribution function f . furthermore, let the link\nbetween the observable categories and the latent variable be given by\n\ny = r \u21d4 \u03b30,r\u22121 < \u02dcy \u2264 \u03b30r,\n\nwhere \u2212\u221e = \u03b300 < \u03b301 < \u00b7\u00b7\u00b7 < \u03b30k = \u221e are thresholds on the latent scale. one obtains\nimmediately\n\np (y \u2264 r|x) = p (\u2212xt \u03b3 + \u0001 \u2264 \u03b30r) = p (\u0001 \u2264 \u03b30r + xt \u03b3) = f (\u03b30r + xt \u03b3).\n\nthe model is essentially a univariate response model since it is assumed that a univariate\nresponse \u02dcy is acting in the background. the response y is just a coarser version of \u02dcy where\nthe thresholds \u03b30r determine the preference for categories and the covariates produce a shifting\non the latent scale. figure 9.1 demonstrates the shifting for the simple example where the\nresponse is given in three categories (short-term, medium-term, long-term unemployment) and\nthe only covariate is age. the slope \u03b3 of the latent variable is negative, yielding an increase\nof e( \u02dcy ) = \u2212x\u03b3 with increasing age. it is seen how the probability of category 1 (short-term\nunemployment) decreases with increasing age. often one might give some interpretation for the\nlatent variable. in the case of unemployment, \u02dcy might represent the lack of opportunities in the\nlabour market, resulting in high categories if \u02dcy is high. in medical examples, where differing\ndegrees of illness are observed, \u02dcy may stand for the latent damage arising from the patient\nhistory that is contained in the covariates. although often there is an interpretation of the latent\nvariable, it is essential to note that the resulting model may be used without reference to latent\nvariables, which may be considered only as a motivation for the model. the advantage of the\nmodel is its simplicity. due to the derivation from the latent regression model with slope \u03b3 the\neffect of x does not depend on the category; it is a so-called global effect. since the cumulative\nprobabilities p (y \u2264 r|x) are parameterized, the model is called the (simple) cumulative or\nthreshold model.\n\n "}, {"Page_number": 256, "text": "244\n\nchapter 9. ordinal response models\n\ny\nt\n\nt\n\n1\n\nt\n\n2\n\n1\n6\n\n3\n9\n\n6\n2\n\nx\n\nfigure 9.1: cumulative model for predictor age and three response categories of duration\nof unemployment.\n\nthreshold (simple cumulative) model\n\np (y \u2264 r|x) = f (\u03b30r + xt \u03b3)\n\np (y = r|x) = f (\u03b30r + xt \u03b3) \u2212 f (\u03b30,r\u22121 + xt \u03b3),\n\nwhere \u2212\u221e = \u03b300 \u2264 \u03b301 \u2264 \u00b7\u00b7\u00b7 \u2264 \u03b30k = \u221e\n\n(9.1)\n\nthe model may be seen as a series of binary regression models with common regression pa-\nrameters. by considering the split of the response categories into {1, . . . , r} and {r + 1, . . . , k}\nand defining yr = 1 if y \u2208 {1, . . . , r}, yr = 0 if y = {r + 1, . . . , k}, one has p (y \u2264 r|x) =\np (yr = 1|x) and (9.1) is a binary regression model for yr \u2208 {0, 1}. of course it is implied that\nthe regression parameter \u03b3 is the same for all splits. that makes the model simple and allows\neasy interpretation of the regression parameter. one consequence of having one regression pa-\nrameter is that the intercepts have to be ordered, \u03b301 \u2264 ... \u2264 \u03b30k. otherwise, the probabilities\ncould be negative. the ordering results naturally from the derivation as a coarser version of the\nunderlying continuous response \u02dcy .\n\nthe most widely used model is the cumulative logit model or the proportional odds model,\n\nwhere f is the logistic distribution function.\n\ncumulative logit model (proportional odds model)\n\n(cid:25)\n\nlog\n\np (y \u2264 r|x)\np (y > r|x)\n\n(cid:26)\n\n= \u03b30r + xt \u03b3\n\n "}, {"Page_number": 257, "text": "9.1. cumulative models\n\n245\n\nthe name \u201cproportional odds model\u201d is due to a specific property called strict stochastic order-\ning, which holds for all simple cumulative models. consider two populations that are charac-\nterized by the covariate values x and \u02dcx. then one obtains\n\n\u22121(p (y \u2264 r|x)) \u2212 f\n\n\u22121(p (y \u2264 r|\u02dcx) = (x \u2212 \u02dcx)t \u03b3,\n\nf\n\n(9.2)\nmeaning that the comparison of (transformed) cumulative probabilities p (y \u2264 r|x) and p (y \u2264\nr|\u02dcx) does not depend on the category. for the logit model, (9.2) takes the form\n\np (y \u2264 r|x)/p (y > r|x)\np (y \u2264 r|\u02dcx)/p (y > r|\u02dcx)\n\n= exp((x \u2212 \u02dcx)t \u03b3).\n\nthus the comparison of populations in terms of cumulative odds p (y \u2264 r|x)/p (y > r|x)\ndoes not depend on the category. that means that if, for example, the cumulative odds in\npopulation x are twice the cumulative odds in population \u02dcx, and this holds for all the categories.\nthis makes interpreting the effects simple because interpretation only refers to the effects of\nx without refering to specific categories. the same holds for all models of the form (9.1);\nhowever, a comparison for alternative models does not refer to cumulative odds but to different\nscalings.\n\ntable 9.3: proportional odds model with linear age for knee data.\n\nvariable\n\nestimate\n\nstandard error wald\n\np-value odds ratio\n\ntherapy\ngender\nage\n\n0.943\n\u22120.049\n0.015\n\n0.335\n0.373\n0.016\n\n7.91\n0.02\n0.87\n\n0.004\n0.893\n0.349\n\n2.568\n0.985\n1.015\n\ntable 9.4: proportional odds model with quadratic age effect for knee data.\n\nvariable\n\nestimate\n\nstandard error wald\n\np-value odds ratio\n\ntherapy\ngender\nage\nage2\n\n0.944\n0.082\n\u22120.001\n0.006\n\n0.338\n0.378\n0.018\n0.002\n\n7.78\n0.04\n0.01\n8.86\n\n0.005\n0.826\n0.924\n0.002\n\n2.570\n1.085\n0.999\n1.006\n\nexample 9.3: knee injuries\nas a simple application let us first investigate the treatment effect from table 9.1, thereby ignoring all the\ncovariates. without using ordinal models one can test if there is an association between treatment and pain\nlevel. the simple \u03c72-test of independence yields 18.20 on 4 degrees of freedom. with a p-value of 0.001\nthe effect of treatment is highly significant. however, association tests do not show how the treatment\neffect is linked to the pain level.\n\nthe quantification of effect strength may be obtained by fitting an ordinal model. the fitting of\nthe proportional odds model yields for the treatment effect 0.893 with standard error 0.328, showing a\ndistinct treatment effect. the corresponding odds ratio is given by e0.893 = 2.442, which means that, for\nany dichotomization of response categories, the (estimated) cumulative odds ratio comparing treatment to\na placebo is\n\np (y \u2264 r|treatment)/p (y > r|treatment)\np (y \u2264 r|placebo)/p (y \u2265 r|placebo)\n\n= 2.442.\n\nthus the odds for the low response category, meaning lesser pain, as compared to higher categories are\ndistinctly higher in the treatment group.\n\n "}, {"Page_number": 258, "text": "246\n\nchapter 9. ordinal response models\n\nmore generally, the ordinal regression model allows one to model the treatment effect (1: treatment;\n0: placebo) as well as the effect of the covariates on pain during movement. the covariates are gender (1:\nmale; 0: female) and age in years, where age has been centered around 30 years. we consider the main\neffect proportional odds model with predictor\n\n\u03b7r = interceptr + treatment\u03b3t + gender\u03b3g + age\u03b3a\n\nand a model with a quadratic effect of age having predictor\n\n\u03b7r = interceptr + treatment\u03b3t + gender\u03b3g + age\u03b3a + age2\u03b3a2 .\n\ntables 9.3 and 9.4 show the parameter estimates. in the main effect model only therapy seems to be\ninfluential; therapy and age have non-significant effects. however, the assumption of a linear effect of age\nis not supported by the data. deviances are given by 372.24 for the linear age effect model and 362.88 for\nthe quadratic effect of age. with a difference in deviances of 9.36, one concludes that the quadratic effect\nshould not be omitted, since \u03c72\n.95(1) = 3.84. the significance of the quadratic effect is also supported\nby the wald test, which shows p-value 0.002. when the covariate effects of gender and age are taken into\naccount the treatment effect is slightly stronger. one obtains 0.944, corresponding to odds ratio 2.57, as\ncompared to 0.893 and odds ratio 2.44 for the simple model where covariates are ignored.\n\nthe cumulative model was propagated by mccullagh (1980) and has been widely used\nsince this influential paper has been published. earlier versions of the cumulative logistic model\nhave been suggested by snell (1964), walker and duncan (1967), and williams and grizzle\n(1972). since interpretation is very simple, it is tempting to use the simple cumulative model.\nhowever, the results could be misleading if the proportional odds assumption, implied by the\nmodel, does not hold (see example 9.5). therefore, it is advisable to check if the assumption is\nappropriate. this may be done by embedding the model into a more general model (see section\n9.1.3). however, first we will consider alternative cumulative models, which have different link\nfunctions.\n\n9.1.2 alternative link functions\n\nthe link function of the cumulative model results from the distribution of the noise variable\n\u03b5 when the model is derived from the latent regression model \u02dcy = \u2212xt \u03b3 + \u03b5. alternative\nmodels are obtained by assuming different distributions for the noise variable \u03b5. figure 9.2\nshows the densities of the underlying regression model \u02dcy = \u2212xt \u03b3 + \u03b5 for two subpopulations\ncharacterized by different values of the covariates x1 and x2 and four different distributions.\nthe dashed lines represent the cutoff points when the continuous response \u02dcy is cut into slices\nthat represent the coarser categorical responses.\n\ncumulative extreme value models\nif one assumes for \u03b5 the minimum extreme value (or gompertz) distribution f (\u03b7) = 1 \u2212\nexp(\u2212 exp(\u03b7)), one obtains the cumulative minimum extreme value model, which has several\nrepresentations.\n\n "}, {"Page_number": 259, "text": "9.1. cumulative models\n\n247\n\n\u22124\n\n\u22122\n\n0\n\n2\n\n4\n\n6\n\n8\n\n\u22124\n\n\u22122\n\n0\n\n2\n\n4\n\n6\n\n8\n\n\u22124\n\n\u22122\n\n0\n\n2\n\n4\n\n6\n\n8\n\n4\n.\n0\n\n3\n.\n0\n\n2\n.\n0\n\n1\n.\n0\n\n0\n.\n0\n\n4\n.\n0\n\n3\n.\n0\n\n2\n\n.\n\n0\n\n1\n\n.\n\n0\n\n0\n\n.\n\n0\n\n4\n\n.\n\n0\n\n3\n\n.\n\n0\n\n2\n\n.\n\n0\n\n1\n\n.\n\n0\n\n0\n\n.\n\n0\n\n4\n\n.\n\n0\n\n3\n\n.\n\n0\n\n2\n\n.\n\n0\n\n1\n\n.\n\n0\n\n0\n\n.\n\n0\n\n\u22124\n\n\u22122\n\n0\n\n2\n\n4\n\n6\n\n8\n\nfigure 9.2: distribution of underlying responses for two subpopulations (logistic, nor-\nmal, gompertz, and gumbel distributions).\n\ncumulative minimum extreme value (gompertz) model\n\nor proportional hazards model\n\np (y \u2264 r|x) = 1 \u2212 exp(\u2212 exp(\u03b30r + xt \u03b3))\n\np (y = r|y \u2265 r, x) = 1 \u2212 exp(\u2212 exp(\u02dc\u03b30r + xt \u03b3))\n\nlog(\u2212 log(p (y > r|x)) = \u03b30r + xt \u03b3\n\nor\n\nor\n\n "}, {"Page_number": 260, "text": "248\n\nchapter 9. ordinal response models\n\nthe second form of the model is simply derived by using the reparameterization \u02dc\u03b30r =\nlog{exp(\u03b30r) \u2212 exp(\u03b30,r\u22121)}. although the thresholds \u03b30r have to be ordered by \u03b301 \u2264 \u00b7\u00b7\u00b7 <\n\u03b30q, there is no such restriction on the transformed parameter \u02dc\u03b30r. the formulation of the\nmodel in conditional probabilities p (y = r|y \u2265 r, x) shows the strong relation to sequential-\ntype models (section 9.2), which can be considered as discrete hazard models. by defining\n\u03bb(r, x) = p (y = r|y \u2265 r, x) as the discrete hazard, reflecting that a process stops in category\nr, given it is reached, the model represents a discrete version of the proportional hazards or cox\nmodel, which is widely used in continuous time survival analysis (see section 9.2). therefore\nthe model is also called the grouped cox model or the proportional hazards model.\n\nfor the proportional hazards model, the strict stochastic ordering property for two popula-\n\ntions x, \u02dcx may be given in the form\n\nlog\n\nor\n\n(cid:26)\n\n(cid:25)\n\np (y > r|y \u2265 r, x)\np (y > r|y \u2265 r, \u02dcx)\nlog(p (y > r|x))\nlog(p (y > r|\u02dcx))\n\n= \u2212 exp((x \u2212 \u02dcx)t \u03b3)\n\n= exp((x \u2212 \u02dcx)t \u03b3).\n\nthus the proportion of the logarithmic hazards p (y > r|x)/p (y \u2265 r|x) does not depend on\nthe categories.\nthe (gumbel) distribution function f (\u03b7) = exp(\u2212 exp(\u03b7)).\n\nan alternative model is the cumulative maximum extreme-value model, which is based on\n\ncumulative maximum extreme-value (gumbel) model\n\nor\n\np (y \u2264 r|x) = exp(\u2212 exp(\u03b30r + xt \u03b3))\nlog(\u2212 log(p (y \u2264 r|x)) = \u03b30r + xt \u03b3\n\nthe model is equivalent to assuming the proportional hazards model (minimum extreme-\nvalue model) for the transformed response yr = k + 1 \u2212 y , which reverses the ordering of\nthe response categories. however, since extreme-value distributions are not symmetric, the\nmaximum extreme-value model differs from the minimum extreme-value model.\n\nprobit model\neconometricians often prefer the ordinal probit model, which assumes \u03b5 to have a standard\nnormal distribution \u03c6 yielding\n\np (y \u2264 r|x) = \u03c6(\u03b30r + xt \u03b3).\n\nin applications the fit typically is very close to the fit of a cumulative logit model. what may\nbe seen as an disadvantage is that the parameters cannot be interpreted in terms of (cumulative)\nodds.\n\nexample 9.4: knee injuries\na comparison of link functions for the knee injuries data is given in table 9.5. the underlying model\nincludes a quadratic term of age since for all the models the quadratic effect cannot be omitted. it is\n\n "}, {"Page_number": 261, "text": "9.1. cumulative models\n\n249\n\nseen that the logit model and the gompertz model have the best fit. although the gompertz model has a\nslightly better fit, the logit model might be a better choice because interpretation of the parameters is in\nterms of the (cumulative) odds ratios.\n\ntable 9.5: deviances for ordinal models with alternative link functions (knee injuries\ndata).\n\ncumulative\n\nsequential\n\nlogit\nprobit\ngumbel\ngompertz\n\n362.88\n364.45\n368.89\n361.31\n\n360.45\n361.12\n361.77\n361.31\n\n9.1.3 general cumulative models\nthe cumulative models considered in the previous sections assume that the effect of covariates\ndoes not depend on the category. more general, however, the effect the of covariates may vary\nacross categories. the general cumulative model may be seen as a combination of the binary\nresponse models for the splitting of categories into groupings {1, . . . , r},{r + 1, . . . , k}.\n\ncumulative model\np (y \u2264 r|x) = f (\u03b30r + xt \u03b3r),\n\nr = 1, . . . , q\n\nsince for each split a binary regression model with separate parameters is assumed, the model\nmay be seen as a series of binary regression models for dependent binary response variables.\nin its general form it has as many parameters as the multinomial response model. more-\n\u2264 \u03b30r + xt \u03b3r for all x and all categories r, since\nover, it is implied that \u03b30,r\u22121 + xt \u03b3r\u22121\np (y \u2264 r \u2212 1|x) \u2264 p (y \u2264 r|x) has to hold for all categories. the model serves as a general\nmodel into which simple cumulative models may be embedded and which may be simplified\nby investigating if at least some of the predictors have regression parameters that do not depend\non the category. the most used model is the cumulative logit model.\n\ncumulative logit model\n\nor\n\np (y \u2264 r|x) =\nlog p (y \u2264 r|x)\np (y > r|x)\n\nexp(\u03b30r + xt \u03b3r)\n\n1 + exp(\u03b30r + xt \u03b3r)\n\n= \u03b30r + xt \u03b3r\n\nthe parameter \u03b3r may be interpreted as in the binary model, where \"binary\" refers to the group-\ning {1, . . . , r},{r + 1, . . . , k}. however, the simple interpretation from the simple cumulative\n(cid:8)= \u03b3r+1, the effect of x has to be interpreted with respect to\nmodel is lost. since in general \u03b3r\nthe dichotomization it refers to.\n\n "}, {"Page_number": 262, "text": "250\n\nchapter 9. ordinal response models\n\na mixture of types of effects results, if part of the covariates has an effect that does not\ndepend on the category. let x be partitioned into x1 and x2, xt = (x1, x2). then the logit-\ntype model or partial proportional odds model is given by\n\n(cid:25)\n\nlog\n\np (y \u2264 r|x)\np (y > r|x)\n\n(cid:26)\n\n= \u03b30r + xt\n\n1 \u03b3 + xt\n\n2 \u03b3r,\n\nwhere x1 has a global effect and x2 has a category-specific effect. then the cumulative odds\nare only proportional (independent of the category) for the variables collected in x1. consider\ntwo populations that differ only in the first set of variables, with the populations specified by\n(x1, x2) and (\u02dcx1, x2). then one obtains that\n\np (y \u2264 r|x1, x2)/p (y > r|x1, x2)\np (y \u2264 r|\u02dcx1, x2)/p (y > r|\u02dcx1, x2)\n\n= exp((x1 \u2212 \u02dcx1)t \u03b3)\n\ndoes not depend on r. therefore, the (cumulative) odds for the variables in x1 are still propor-\ntional.\n\nthe simple- and the general cumulative-type models differ in the linear predictor. while\n\nthe simple model uses the predictor\n\nthe general model is based on\n\n\u03b7r = \u03b30r + xt \u03b3,\n\nr = 1, . . . , q,\n\n\u03b7r = \u03b30r + xt \u03b3r,\n\nr = 1, . . . , q.\n\n(9.3)\n\n(9.4)\n\nsometimes it is useful to reparameterize the effects such that the general predictor has the form\n\n\u03b7r = \u03b30r + xt \u03b3 + xt \u02dc\u03b3r,\n\nwhere \u02dc\u03b3r has to fulfill some side constraints. by using category 1 as the reference category\n\u2212 \u03b31, r = 1, . . . , q,\nand setting \u02dc\u03b31 = 0, the parameters are given by \u03b3 = \u03b31, and \u02dc\u03b3r = \u03b3r\nj \u02dc\u03b3j = 0.\nrepresent the deviation of \u03b3r from \u03b31. an alternative side constraint is\n\nin an extended wilkinson rogers notation the predictor (9.3), where only the intercept\n\n(cid:14)\n\ndepends on the category, may be abbreviated by\n\nand the predictor (9.4) by\n\ny + x1 + \u00b7\u00b7\u00b7 + xp\n\ny \u2217 x1 + \u00b7\u00b7\u00b7 + y \u2217 xp.\n\nthe use of y instead of y is motivated by the fact that y = (y1, . . . , yq) is multinomially\ndistributed with yr = 1 if y = r and yr = 0 if y (cid:8)= r. thus y + x1 + \u00b7\u00b7\u00b7 + xp may be read as\ny1+\u00b7\u00b7\u00b7+yq+x1+\u00b7\u00b7\u00b7+xp and y\u2217x1+\u00b7\u00b7\u00b7+y\u2217xp as y1+\u00b7\u00b7\u00b7+yq+y1x1+\u00b7\u00b7\u00b7+yqx1+\u00b7\u00b7\u00b7+yqxp,\ncorresponding to the terms actually needed in the predictor.\n\nabbreviation\ny + x1 + \u00b7\u00b7\u00b7 + xp\ny \u2217 x1 + \u00b7\u00b7\u00b7 + y \u2217 xp\n\nlinear predictor y = r\n\u03b30r + xt \u03b3\n(cid:3)\n\u03b30r + x\n\n\u03b3r + xt \u02dc\u03b3r.\n\n "}, {"Page_number": 263, "text": "9.1. cumulative models\n\n251\n\n9.1.4 testing the proportional odds assumption\na crucial assumption of the proportional odds models is that cumulative odds are proportional,\nmeaning that for two values of predictors x and \u02dcx the proportion\n\np (y \u2264 r|x)/p (y > r|x)\np (y \u2264 r|\u02dcx)/p (y > r|\u02dcx)\n\n= exp((x \u2212 \u02dcx)t \u03b3)\n\ndoes not depend on the category. when the cumulative model is seen as a submodel of the\ncorresponding general cumulative logit model with category specific effects \u03b3r, the assumption\nis equivalent to testing the null hypothesis\n\nh0 : \u03b31 = \u00b7\u00b7\u00b7 = \u03b3q.\n\nmore specifically, one may investigate if the proportional odds assumption holds for single\nvariables by testing\n\nh0 : \u03b31j = \u00b7\u00b7\u00b7 = \u03b3qj,\n\nwhich means that the jth predictor has the global effect and therefore fulfills that for two values\nxj, \u02dcxj\n\np (y \u2264 r|x1, . . . , xj, . . . xp)/p (y > r|x1, . . . , xj, . . . , xp)\np (y \u2264 r|x1, . . . , \u02dcxj, . . . , xp)/p (y > r|x1, . . . , \u02dcxj, . . . , xp)\n\n(9.5)\n\ndoes not depend on the category. the odds ratio (9.5) compares the odds for two proportions\nthat differ only in the jth covariate. since the hypotheses are linear, one can use the likelihood\nratio test, the wald test, or the score test, applied to the parameters of the category-specific\nmodel.\n\nexample 9.5: retinopathy data\nthe risk factors in the retinopathy study from bender and grouven (1998) are smoking (sm), duration of\ndiabetes (diab), glycosylated hemoglobin (gh), and blood pressure (bp). in a first step the proportional\nodds model is fitted. from the estimates in table 9.6 one might infer that smoking has no effect (\u22120.254\nwith standard deviation 0.192). however, the inference is based on the assumption that the proportional\nodds model holds. if it does not hold, the results are not very trustworthy. therefore, one should first in-\nvestigate if the proportional odds assumption is realistic. it turns out that the deviance for the proportional\nodds model is 904.14. for the cumulative model with all parameters specified as category-specific one\nobtains 892.45. a comparison yields 11.69 on 4 degrees of freedom, which means that the proportional\nodds assumption should be rejected. table 9.7 shows the model fits for a hierarchy of models, where\n[\n] denotes that the effect is category-specific; otherwise it is assumed to be global. it is seen that the\neffects of diab and sm should be specified as category-specific. table 9.6 shows the estimates for the\ncorresponding model and the proportional odds model. while the effect of smoking seems neglectable\nwhen fitting the proportional odds model, it seems to be influential when fitting the partial proportional\nodds model. for the split between the first category and the others, the effect of smoking is \u22120.399 with\na standard error of 0.205 yielding a z-value of 1.946, which makes smoking at least suspicious. since the\neffect is very weak for the split between the first two categories and the last one (0.062 with standard error\n0.804), it appears to be neglectable when the category-specific effect of smoking is ignored by using the\nill-fitting proportional odds model.\n\nas is seen from the retinopathy example, inference from the proportional odds model might\nbe misleading if the model has a poor fit. bender and grouven (1998) already used the retinopa-\nthy example to demonstrate this effect. they proposed considering the fitting of separate binary\n\n "}, {"Page_number": 264, "text": "252\n\nchapter 9. ordinal response models\n\ntable 9.6: parameter estimates for proportional odds model and partial proportional odds\nmodel with category-specific effects of sm and diab for retinopathy data.\n\nproportional odds model\n\npartial proportional odds model\n\nestimates\n\nstandard error\n\nestimates\n\nstandard error\n\n\u03b301\n\u03b302\ngh\nbp\nsm[1]\nsm[2]\ndiab[1]\ndiab[2]\n\n12.302\n13.673\n\u22120.459\n\u22120.072\n\u22120.254\n\u22120.140\n\n1.290\n1.317\n0.074\n0.013\n0.192\n\n0.013\n\n12.188\n13.985\n\u22120.468\n\u22120.071\n\u22120.399\n0.062\n\u22120.129\n\u22120.163\n\n1.293\n1.356\n0.074\n0.014\n0.205\n0.804\n0.014\n0.017\n\ntable 9.7: model fits for partial proportional odds model (retinopathy data).\n\npartial proportional odds models\n\ndeviance\n\nproport. odds effect\n\ndiff.\n\ndf\n\n[sm ], [diab], [gh], [bp ]\n[sm ], [diab], [gh], bp\n[sm ], [diab], gh, bp\n[sm ], diab, gh, bp\nsm, diab, gh, bp\n\n892.45\n892.67\n893.77\n897.97\n904.14\n\nbp\ngh\ndiab\nsm\n\n0.22\n1.10\n4.20\n6.17\n\n1\n1\n1\n1\n\nmodels to groupings of response categories. a problem with fitting separate models is that like-\nlihood ratio fits cannot be based directly on the separate fits. however, brant (1990) showed a\nway to base test procedures for the proportional odds assumption on separate fits.\n\nof course, even if the test on proportional odds shows no significant result, it is no guarantee\nthat the proportional odds model holds, at least when continuous predictors are involved and no\ngoodness-of-fit tests are available.\n\none problem that occurs when testing the proportional odds assumption by likelihood ratio\nstatistics is that estimates for the full model with category-specific regression parameters have\nto exist. since strong restrictions on the parameter space are involved, that is not always the\ncase. an alternative is the score statistic:\n\nu = s(\u02dc\u03b2)t f\n\n\u22121(\u02dc\u03b2)s(\u02dc\u03b2),\n\nwhere s(\u02dc\u03b2) is the score function and f (\u02dc\u03b2) denotes the fisher matrix for the larger model,\nevaluated at \u02dc\u03b2, which is the maximum likelihood estimates of the submodel, that is, the propor-\ntional or partial proportional odds model. the advantage is that only the fit of the proportional\nodds model is needed.\n\n9.2 sequential models\nin many applications one can imagine that the categories 1, . . . , k are reached successively.\noften this is the only way to obtain a response in a higher category. for example, in duration\nmodels where categories refer to the duration of unemployment, it is obvious that long-term\nunemployment can only be observed if previously short-term unemployment was a step of the\nprocess. analogously, if the response categories refer to the number of cars in a household,\none can assume that the cars are bought successively. the observed number of cars represents\nthe (preliminary) end of a process. consequently, the modeling might reflect the successive\ntransition to higher categories in a stepwise model.\n\n "}, {"Page_number": 265, "text": "9.2. sequential models\n\n253\n\n9.2.1 basic model\nlet the process start in category 1. the decision between category {1} and categories {2, . . . , k}\nis determined in the first step by a dichotomous response model\np (y = 1|x) = f (\u03b301 + xt \u03b31).\n\nif y = 1, the process stops. if y \u2265 2, the second step is a decision between category {2} and\ncategories {3, . . . , k} and is determined by\n\np (y = 2|y \u2265 2, x) = f (\u03b302 + xt \u03b32).\n\nin general in the rth step the decision between category {r} and categories {r + 1, . . . , k} is\nmodeled by the binary model\n\np (y = r|y \u2265 r, x) = f (\u03b30r + xt \u03b3r).\n\nin addition to the stepwise modeling it is only assumed that the decision between the cate-\ngory reached and higher categories is determined by the same binary model that has response\nfunction f . the collection of steps represents the sequential model.\n\nsequential model\n\np (y = r|y \u2265 r, x) = f (\u03b30r + xt \u03b3r), r = 1, . . . , q\n\nin this general form the transition to higher categories in the rth step is determined by category-\nspecific effects \u03b3r. thus the effects of the covariates may depend on the category that makes it a\nmodel with (possibly too) many parameters. a simplifying assumption is that the effects are the\nsame in each step, that is, \u03b31 = . . . \u03b3q = \u03b3 and only intercepts \u03b30r, . . . , \u03b30q depend on the step\nunder consideration. this simplification corresponds to the simple cumulative model where the\neffects of covariates x are also global, meaning that \u03b3 does not depend on the category.\n\nthe response probabilities in the sequential model are given by\n\nr\u22121(cid:15)\n\ns=1\n\np (y = r|x) = p (y = r|y \u2265 r, x)\nr\u22121(cid:15)\n\np (y > s|y \u2265 s, x)\n\n= f (\u03b30r + xt \u03b3r)\n\n(1 \u2212 f (\u03b30s + xt \u03b3s)).\n\ns=1\n\n(see also section 9.5.2). sequential models are closely related to discrete time survival mod-\nels. let the categories refer to discrete time, measured in days, months, or years. then\nthe conditional probabilities \u03bb(r, x) = p (y = r|y \u2265 r, x) represent the discrete hazard\nfunction for given predictor x. the discrete hazard \u03bb(r, x) is the probability that a dura-\ntion (\"survival\") ends in the time interval that corresponds to category r given this time in-\nterval is reached. for example, it represents the probability of finding a job in month r given\nthe person was unemployed during the previous months. the response probability, given by\np (y = r|x) = \u03bb(r, x)\nr\u22121\ni=1 \u03bb(i, x), has an easy interpretation. it represents that the first\nr \u2212 1 intervals were \"survived,\" but the duration ends in category r; the transition to the next\ncategory was not successful. for discrete survival models many extensions have been consid-\nered that also can be used in ordinal modeling; see, for example, fahrmeir (1994), and tutz and\nbinder (2004); an overview is given in fahrmeir and tutz (2001).\n\n2\n\n "}, {"Page_number": 266, "text": "(cid:25)\n\n(cid:25)\n\n254\n\nchapter 9. ordinal response models\n\nfor the modeling of ordinal data, the most common sequential model is the sequential logit\n\nmodel, where f (\u03b7) = exp(\u03b7)/(1 + exp(\u03b7)) is the logistic distribution function.\n\nsequential logit model (continuation ratio logits model)\n\np (y = r|y \u2265 r, x) =\n\nor\n\nlog\n\np (y = r|x)\np (y > r|x)\n\nexp(\u03b30r + xt \u03b3r)\n\n1 + exp(\u03b30r + xt \u03b3r)\n\n(cid:26)\n\n= \u03b30r + xt \u03b3r\n\nthe logits log(p (y = r|x)/p (y > r|x)) compare the response category r to the response\ncategories {r + 1, . . . , k} and may be considered as conditional logits, given y \u2265 r, since the\ndichotomization into {r} and {r + 1, . . . , k} refers only to categories r, r + 1 . . . , k. they are\nalso referred to as continuation ratio logits.\n\nin contrast to the cumulative model, there is no restriction on the parameters, which makes\nestimation easier. however, the general model contains many parameters because each transi-\ntion has its own parameter \u03b3r. a simpler version is the sequential model with global effects:\n\n(cid:26)\n\nlog\n\np (y = r|x)\np (y > r|x)\n\n= \u03b30r + xt \u03b3,\n\nwhere \u03b3 does not depend on the category. as for the simple cumulative model, the model\nimplies strict stochastic ordering. for two populations x and \u02dcx one obtains that the proportion\nof odds\n\np (y = r|x)/p (y > r|x)\np (y = r|\u02dcx)/p (y > r|\u02dcx)\n\n= exp((x \u2212 \u02dcx)t \u03b3)\n\ndoes not depend on the category.\n\ntable 9.8: deviances for several models of retinopathy data.\n\ny+y\u2217sm+y\u2217diab+y\u2217gh+y\u2217bp\ny+y\u2217sm+diab+y\u2217gh+y\u2217bp\ny+y\u2217sm+diab+gh+y\u2217bp\ny+y\u2217sm+diab+gh+bp\ny+sm+diab+gh+bp\n\ndeviance\n\neffect\n\ndiff\n\ndf\n\n891.419\n891.443\n891.469\n891.977\n897.710\n\ny\u2217diab\ny\u2217gh\ny\u2217bp\ny\u2217sm\n\n0.024\n0.026\n0.508\n5.679\n\n1\n1\n1\n1\n\nexample 9.6: retinopathy data\ntable 9.8 shows deviances for several sequential logit models for the retinopathy data from examples 9.2\nand 9.5. if the variable has a category-specific parameter \u03b3r, it is denoted as an interaction with y. for\nexample, y(cid:16) sm means that xsm\u03b3r is in the linear predictor. it is seen that one has to specify a category-\nspecific effect of smoking (sm). for the rest of the variables, \u2013 diabetes (diab), glycosylated hemoglobin\n(gh), and blood pressure (bp) \u2013 there is no need for category-specific effects. for the model\n\np (y = r|x)\np (y > r|x)\n\nlog(\n\n) = \u03b30r + xsm\u03b2sm,r + diab\u03b2d + gh\u03b2gh + bp\u03b2bp\n\none obtains deviance 891.977. the model fits slightly better than the corresponding cumulative model.\nthe estimates are given in table 9.9. it is seen that smoking seems to have an effect on the transition\nbetween category 1 and category 2 such that the probability of a transition is reduced when smoking.\n\n "}, {"Page_number": 267, "text": "9.3. further properties and comparison of models\n\n255\n\ntable 9.9: parameter estimates for the sequential logit model for retinopathy data.\n\nestimates\n\nstderror wald-chi-square\n\np-value odds-ratio\n\n\u03b301\n\u03b302\ndiab\ngh\nbp\nsm[1]\nsm[2]\n\n11.127\n10.915\n\u22120.128\n\u22120.424\n\u22120.062\n\u22120.377\n0.490\n\n1.168\n1.213\n0.012\n0.067\n0.012\n0.202\n0.312\n\n90.67\n80.92\n108.78\n39.83\n26.04\n3.47\n2.46\n\n0.00\n0.00\n0.00\n0.00\n0.00\n0.06\n0.11\n\n0.87\n0.65\n0.93\n0.68\n1.63\n\n9.2.2 alternative links\nof course any binary response model may be used to model the transition between categories r\nand r + 1, yielding different sequential models. an especially interesting model follows from\nassuming a gompertz link. the link function f (\u03b7) = 1 \u2212 exp(\u2212 exp(\u03b7)), which corresponds\nto the gompertz distribution, yields the sequential minimum extreme-value model\n\np (y = r|y \u2265 r, x) = 1 \u2212 exp(\u2212 exp(\u03b30r + xt \u03b3r)).\n\nif \u03b31 = \u00b7\u00b7\u00b7 = \u03b3q = \u03b3, the model is equivalent to the proportional hazards model from section\n9.1.3. thus, for this special distribution and assumed global effects, the cumulative model is\nequivalent to the sequential model.\n\n9.3 further properties and comparison of models\n9.3.1 collapsibility\nsince the categories of an ordinal response variable are sometimes slightly arbitrary, it is\npreferable that grouping the response categories does not change the conclusions drawn from\nthe fitted models. let the response categories {1, . . . , k} be partitioned into subsets si =\n{mi\u22121 + 1, . . . , mi}, i = 1, . . . , t, where m0 = 0, mt = k, and consider the simpler response\nvariable \u00afy = i if y \u2208 si. a model is called collapsible with reference to a parameter if\nthe parameter is unchanged by the transition from y to the coarser version \u00afy . for the simple\ncumulative model one gets immediately\n\np ( \u00afy \u2264 i|x) = f (\u03b30mi + xt \u03b3).\n\nthus the cumulative model is collapsible with reference to the parameters \u03b3. moreover, it is\ncollapsible for the threshold parameters \u03b30r, where of course some rearranging of categories has\nto be kept in mind. in general, the sequential model is not collapsible. of course, exceptions are\nthe models where cumulative and sequential approaches coincide (see next section). however,\nthe importance of collapsibility should not be overrated. it is merely a secondary criterion for\nordinal variables. if a response pattern is changed by collapsing categories, the influence of\nexplanatory variables may also change.\n\n9.3.2 equivalence of cumulative and sequential models\ncumulative and sequential models are based on quite different approaches to the modeling of\nthe response variable. the response in cumulative models may be seen as a coarser version of\nthe latent variable \u02dcy , whereas the sequential model is constructed by assuming that the response\nis determined step-by-step. thus one might conclude that the resulting models are distinct.\n\n "}, {"Page_number": 268, "text": "256\n\nchapter 9. ordinal response models\n\nindeed, in general, the cumulative and the sequential approaches yield different models.\nhowever, there are two exceptions. the first is the proportional hazards model (sequential\nminimum extreme-value model). for the simple versions with only global parameters \u03b3, the\ncumulative model is equivalent to the sequential model, which was first noted by laara and\nmatthews (1985), for the general case see tutz (1991).\nthe second exception is the exponential response model, which uses the exponential distri-\nbution f (\u03b7) = 1\u2212 exp(\u2212\u03b7). a simple derivation shows that the general sequential model with\nglobal and category-specific effects for covariates x and z,\n\np (y = r|y \u2265 r) = 1 \u2212 exp(\u2212(\u03b30r + xt \u03b3 + zt \u03b3r)),\n\nis equivalent to the cumulative model\n\n(cid:14)\n\np (y \u2264 r|x) = 1 \u2212 exp(\u2212(\u02dc\u03b30r + xt \u02dc\u03b3r,z + zt \u02dc\u03b3r)),\n\n(cid:14)\n\nr\n\nj=1 \u03b30j, \u02dc\u03b3r,z = r\u03b3, \u02dc\u03b3r =\n\nr\nwhere \u02dc\u03b30r =\nj=1 \u03b3r. it should be noted that global parameters\nin the sequential model turn into category-specific parameters in the cumulative model. if all\nthe parameters are category-specific, the two models are equivalent. however, the model is not\n\u2265 0 may often be violated and\noften used because the restriction \u02dc\u03b7r = \u02dc\u03b30r + zt \u03b3r,z + xt \u03b3r\nspecial software is needed to fit the model with that restriction.\n\n9.3.3 cumulative models versus sequential models\nalthough the cumulative model is often considered as the ordinal model, both types of models,\nthe cumulative model and the sequential, are ordinal models. both model types have their\nadvantages and drawbacks. in the following we give some points concerning the differences.\nfor simplicity, logit models are considered.\ninterpretation of parameters. the interpretation of parameters clearly depends on the\nmodel type. for the cumulative model the parameters refer to the cumulative odds p (y \u2264\nr|x)/p (y > r|x), while for the sequential model they refer to continuation ratios p (y =\nr|x)/p (y > r|x). it depends on the application and the viewpoint of the user which parame-\nterization is more appropriate. if the underlying process is a sequential process, it is tempting\nto use the sequential model.\n\nflexible modeling and existence of estimates. a drawback of the cumulative model is that\nfor complex models (with category-specific parameters) iterative estimation procedures fre-\nquently fail to converge. this is due to the restriction on the parameter space that makes more\ncarefully designed iterative procedures necessary, in particular when higher dimensional pre-\ndictors are modeled. a consequence is that often less flexible models with global rather than\ncategory-specific effects are fitted for cumulative models. since parameters do not have to be\nordered, the sequential model can be more easily extended to more complex modeling prob-\nlems, for example, to model monotonicity (tutz, 2005).\n\ncollapsibility. an advantage of the cumulative model is that the parameters remain the\n\nsame when the categories are taken together. due to the model structure\nlog(p (y \u2264 r|x)/p (y > r|x)) = \u03b30r + xt \u03b3r,\n\nwhich is based on the dichotomization {1, 2, . . . , r},{r + 1, . . . , k}, the parameter values re-\nmain the same if one combines two or more categories using, for example, responses {1, 2},\n3, 4, . . . , k rather than 1, 2, . . . , k.\n\n "}, {"Page_number": 269, "text": "9.4. alternative models\n\n257\n\n9.4 alternative models\n9.4.1 cumulative model with scale parameters\nmccullagh (1980) introduced the cumulative-type model\n\n(cid:25)\n\n(cid:26)\n\np (y \u2264 r|x) = f\n\n\u03b30r + xt \u03b3\n\n\u03c4x\n\n,\n\n(9.6)\n\nwhich contains an additional scale parameter \u03c4x. in cases where the concentration in response\ncategories varies across populations, the model is more appropriate than the simple cumulative\nmodel. the simple cumulative model was motivated by an underlying continuous regression\nmodel \u02dcy = \u2212xt \u03b3 + \u03b5, where the distribution of \u03b5 does not depend on x. thus the model\nassumes that with varying x the probability mass is merely shifted on the latent scale. if the\nprobability mass is more concentrated in one population and spread out in other populations,\nthe simple cumulative model will be unable to model the varying dispersion. the following\nexample has been used by mccullagh (1980) to motivate the model.\n\nexample 9.7: eye vision\ntable 9.10 gives stuart\u2019s (1953) quality of vision data for men and women as treated by mccullagh (1980).\nfrom the data it is obvious that women are more concentrated in the middle categories while men have\nrelatively high proportions in the extreme categories. since the cumulative model is based on a shifting of\ndistributions on the underlying continuum, the model is not appropriate.\n\ntable 9.10: quality of right eye vision in men and women.\n\nvision quality\n\nhighest (1)\n\n2\n\n3\n\nlowest (4)\n\nmen\nwomen\n\n1053\n1976\n\n782\n2256\n\n893\n2456\n\n514\n789\n\n9.4.2 hierarchically structured models\nin many examples the ordered response categories may naturally be divided into subgroups of\ncategories such that the categories within groups are homogenous but the groups themselves\ndiffer in interpretation. for example, the response categories of ordered responses might be in\ndistinct groups indicating improvement, no change, and change for the worse.\n\nexample 9.8: arthritis\ntable 9.11 shows data that have been analyzed previously by mehta et al. (1984). for patients with acute\nrheumatoid arthritis a new agent was compared with an active control. each patient was evaluated on\na five-point assessment scale ranging from \"much improved\u201d to \"much worse.\u201d there are three groups\nof categories that are strictly different: the improvement group, the no change group, and the group of\nchange for the worse.\n\nin examples like the arthritis data it might be appropriate to fit a model with more structure\nin the response than is used in simple models for ordinal data. a hierarchical model is obtained\nby first modeling the response groups of homogenous response categories and then modeling\nthe response within groups. generally, let the categories 1, . . . , k be subdivided into basic sets\ns1, . . . , st\u00b4, where si = {mi\u22121 +1, . . . , mi}, m0 = 0, mt = k. in the first step the response in\n\n "}, {"Page_number": 270, "text": "258\n\nchapter 9. ordinal response models\n\ntable 9.11: clinical trial of a new agent and an active control (mehta et al., 1984).\n\nglobal assessment\nno\n\nimprovement\n\ndrug\n\nmuch\n\nimprovement\n\nnew agent\nactive control\n\n24\n11\n\n37\n51\n\nchange\n\n21\n22\n\nworse\n\nmuch\nworse\n\n19\n21\n\n6\n7\n\none of the sets is determined by a cumulative model with \u03b30. in the second step the conditional\nresponse given si is determined by a cumulative model with parameters that are linked to si.\nfrom these two steps one obtains the model\n\np (y \u2208 ti|x) =f (\u03b8i + xt \u03b30),\np (y \u2264 r|y \u2208 si, x) =f (\u03b8ir + xt \u03b3i),\n\n(9.7)\n\nwhere ti = s1 \u222a \u00b7\u00b7\u00b7 \u222a si, \u03b81 < . . . < \u03b8t\u22121 < \u03b8t = \u221e, \u03b8i,mi\u22121+1 < . . . < \u03b8i,mi\u22121 < \u03b8i,mi =\n\u221e, i = 1, . . . . . . , t.\nin model (9.7) the effect of the explanatory variables on the dependent variable is modeled\nin two ways. first the effect on the chosen subsets is parameterized, and then the conditional\neffect within the subsets is investigated. an advantage of the model is that different parameters\nare involved at different stages.\nin the arthritis example, and the choice between the basic\nsets is determined by the parameter \u03b30, and the choice on the finer level is determined by the\nparameters \u03b3i. the choice between the basic sets, for example, the alternative improvement\nor no improvement, may be influenced by different strengths or even by different variables\nthan the choice within the \"improvement\" set and the \"no improvement\" set. the first step that\nmodels the global effect may refer to a different biological mechanism than the second step.\nin attitude questionnaires the individual might first choose the global response level, that is,\nagreement or no agreement, and in a subsequent process decide for the strength of agreement or\nno agreement. different covariates may be responsible for decisions on different levels. after\nrearranging the data, estimation can be based on common software for the fitting of simple\ncumulative models (exercise 9.7).\n\nthe hierarchically structured model may also be used as an alternative to the scale model\n(9.6). in the eye vision example, it allows one to model the varying dispersion and in addition\nto test the symmetry of the underlying process.\n\ntable 9.12: analysis of eye data; z-values in brackets.\n\nstructured model\n\nhierarchically model with model with\n\u03b31 = \u2212\u03b32\n0.27\n0.09\n0.86\n0.0\n0.25\n\u22120.25\n2.96\n\n0.27\n0.08\n0.84\n0.0 (0.0)\n0.22(7.63)\n\u22120.29 (\u22128.47)\n0.0\n\n\u03b30 = 0\n0.27\n0.08\n0.84\n-\n0.22\n\u22120.29\n\u22127\n\n3.5 \u00b7 10\n\n\u03b81\n\u03b811\n\u03b823\n\u03b30\n\u03b31\n\u03b32\ndeviance\n\nexample 9.9: eye vision\nthe basic sets that determine the analysis here are chosen by a grouping in categories of eye vision\nabove average, that is, s1 = {1, 2}, and below average, that is, s2 = {3, 4}. with those basic sets the\n\n "}, {"Page_number": 271, "text": "9.4. alternative models\n\n259\n\nhierarchically structured cumulative logit model has been fitted. let the explaining variable sex be given\nby 1 (male) and \u22121 (female). for this example, the two-step model is equivalent to the saturated model.\nhowever, it is a saturated model with a specific parameterization that allows for reduction within this\nparameterization.\nlet \u03bb(x) = log(p (y \u2208 s1|x)/p (y \u2208 s2|x)) denote the log-odds that the quality of eye vision is\nabove average. for the estimated log-odds one obtains \u02c6\u03bb(1) \u2212 \u02c6\u03bb(\u22121) = 0.000025. thus there is almost\nno difference between men and women with respect to quality of eye vision when quality is considered as\na dichotomous variable with the categories \"above average\" and \"below average.\" for the interpretation\nof \u03b31 and \u03b32 one has to distinguish between two populations: the population with quality of eye vision\nabove average and the population with quality of eye vision below average. for s \u2282 {1, 2, 3, 4} let\n\u03bbs(r|x) = log(p (y = r|y \u2208 s, x)/p (y (cid:11)= r|y \u2208 s, x)) denote the conditional log-odds and let\n\u03bbs(r) = \u03bbs(r|x = 1) \u2212 (\u03bbs(r|x = \u22121)) denote the difference of conditional log-odds between men\nand women. from \u03bb(1,2)(1) = \u2212\u03bb(1,2)(2) = 2\u03b31 and \u03bb(3,4)(4) = \u2212\u03bb(3,4)(3) = \u22122\u03b32 we get the\nestimates \u02c6\u03bb(1,2)(2) = \u22122\u02c6\u03b31 = \u22120.430 and \u03bb(3,4)(3) = 2\u02c6\u03b32 = \u22120.583. the parameter \u03b31 represents the\ndifference between men and women with respect to category 1 in the population \"above average,\" \u2212\u03b32\nrepresents the same difference with respect to category 4 in the population \"below average.\" from the\nvalues of \u03b31 and \u2212\u03b32 it is seen that the effect strength of gender is about the same in both populations.\nin the discussion of mccullagh\u2018s (1980) paper, atkinson refers to the finding of heim (1970) that, for\nmany characteristics, men tend to be more extreme than women, even if the means of the two sexes are\nthe same. this effect is captured by the hierachically structured response with the parameters \u03b31 and\n\u2212\u03b32 representing measures for the strength of the tendency versus the extremes. the hypothesis of no\ndifferences between the two sexes with respect to performance below and above average has the form\nh0 : \u03b30 = 0. though the number of observations is very high and therefore the \u03c72-statistic generally will\ntend to rejection of non-saturated models, the hierarchically structured model with \u03b30 = 0 fits the data\nso well that the deviance is almost vanishing (see table 9.12). another interesting hypothesis formalizes\nwhether the tendency to extreme categories is the same in both populations. the corresponding hypothesis\ntakes the form h0 : \u03b31 = \u2212\u03b32. with deviance 2.96, the model is not rejected. quite different from the\nfit of the non-saturated versions of the hierarchically structured cumulative model, the cumulative model\nyields deviance 128.39 and, as expected, has a very poor fit, whereas the model with \u03b31 = \u2212\u03b32 fits the\ndata very well. the hierarchically structured model allows one to test for the tendency versus the middle.\nin contrast, the scale model (9.6) does not allow for different strengths toward the middle because it does\nnot distinguish between the upper and lower categories (see also exercise 9.7).\n\ntable 9.13: analysis of arthritis data.\n\n\u02c6\u03b30\n0.073\n\n\u02c6\u03b31\n\n1.101\n\n\u02c6\u03b32\n0.054\n\nlog-likelihood\n\n\u2212315.502\n\ndeviance\n\n0.008\n\ndf\n\n1\n\n-\n\n-\n\n0.55\n\n0.55\n\n\u22120.02\n\n-\n\n\u2212315.541\n\u2212315.545\n\u2212319.132\n\n0.087\n\n0.094\n\n7.269\n\n2\n\n3\n\n4\n\nhierarchically\nstructured model\nmodel with\n\u03b30 = 0\nmodel with\n\u03b30 = \u03b32 = 0\nmodel with\n\u03b30 = \u03b31 = \u03b32 = 0\n\nexample 9.10: arthritis\ntable 9.13 shows the fit for the hierarchically structured response model where the grouping is in s1 =\n{1, 2}, s2 = {3}, s3 = {4, 5} and the predictor is 1 for agent and 0 for control. it is seen that the\nmodel with \u03b30 = \u03b32 = 0 fits well. the only relevant effect seems to be \u03b31, which distinguishes between\nresponse categories 1 and 2.\n\n "}, {"Page_number": 272, "text": "260\n\nchapter 9. ordinal response models\n\nhierarchically structured models are conditional models, in which the conditioning is on\nsubgroups of categories. very similar models apply in cases where the response categories are\npartially ordered. sampson and singh (2002) give an example from psychiatry where anxiety is\nmeasured in the categories \"no anxiety,\" \"mild anxiety,\" \"anxiety with depression,\" and \"severe\nanxiety.\" only parts of these responses can be considered ordered. for a general framework for\nthe modeling of partially ordered data see zhang and ip (2011).\n\n9.4.3 stereotype model\nanderson (1984) introduced the so-called stereotype regression model, which in the simple\none-dimensional form is given by\n\np (y = r|x) =\n\n1 +\n\n(cid:14)\nexp(\u03b30r \u2212 \u03c6rxt \u03b3)\ni=1 exp(\u03b30r \u2212 \u03c6ixt \u03b3) ,\n\nq\n\nr = 1, . . . , q. to get an ordered regression model, the parameters \u03c61, . . . , \u03c6k must fulfill the\nconstraints\n\n1 = \u03c61 > \u00b7\u00b7\u00b7 > \u03c6k = 0.\n\nmost often the model is estimated without imposing the constraints a priori. however, if the\nestimated values \u02c6\u03c6i are allowed to determine the ordering of categories, the order is a result of\nthe model and not a trait of the variable considered. then the model is not an ordinal regression\nmodel because it makes no use of the information provided by the ordering. it is a model that\ngenerates an ordering rather than a model for ordered response categories. anderson (1984)\nalso considered the concept of indistinguishability, meaning that response categories are indis-\ntinguishable if x is not predictive between these categories. a comparison of the proportional\nodds model and the stereotype model was given by holtbr\u00fcgge and schuhmacher (1991); see\nalso greenland (1994).\n\n9.4.4 models with scores\nanother type of model assumes given scores for the categories of the response y . williams and\ngrizzle (1972) consider the model\n\nk(cid:7)\n\n(cid:27)\ny = r|x\n\n(cid:28)\n\nsrp\n\n= xt \u03b3,\n\nr=1\n\nwhere s1, . . . , sk are given scores. instead of using the support {1, . . . , k}, one may consider\ny \u2208 {s1, . . . , sk} and write the model as\n(cid:27)\ny = sr|x\n\nk(cid:7)\n\n= xt \u03b3.\n\nsrp\n\n(cid:28)\n\nr=1\n\nobviously, models of this type are not suited for responses that are measured on the ordinal\nscale level. by introducing scores, a higher scale level is assumed for the discrete response. for\nfurther scores in ordinal modeling see agresti (2009).\n\n9.4.5 adjacent categories logits\nan alternative type of model is based on adjacent categories logits (e.g., agresti, 2009). the\nmodel\n\n(cid:27)\ny = r|x\n\n(cid:28)\n\n/p\n\n(cid:27)\ny = r \u2212 1|x\n\n(cid:28)%\n\nlog\n\np\n\n$\n\n= xt \u03b3r\n\n "}, {"Page_number": 273, "text": "9.5. inference for ordinal models\n\n261\nis based on the consideration of the adjacent categories {r \u2212 1, r}. logits are built locally for\n(cid:27)\nthese adjacent categories. another form of the model is\ny = r|y \u2208 {r, r + 1}, x\n\n(cid:28)\n\np\n\n= f (xt \u03b3r),\n\nwhere f is the logistic distribution function. the latter form shows that the logistic distribution\nfunction may be substituted for any strictly monotone increasing distribution function. more-\nover, it shows that it may be considered as a dichotomous response model given y \u2208 {r, r +1}.\nvery similar models are used in item response theory (e.g., masters, 1982,) and are often misin-\nterpreted as sequential process models. the model may also be considered as the corresponding\nregression model that is obtained from the row-column (rc) association model considered by\ngoodman (1979, 1981a, b).\n\n9.5 inference for ordinal models\nordinal regression models are specific cases of multivariate generalized linear models. hence,\nthe machinery of multivariate glms as given in section 8.6 can be used. in the following,\nfirst the embedding into the framework of glms is outlined. for sequential models maximum\nlikelihood estimates may be obtained in a simpler way. since transitions between categories\ncorrespond to binary decisions, estimation may be performed by using binary models. this\napproach is given in section 9.5.2.\n\nthe general form of the multivariate glm for categorical responses is given by\n\ng(\u03c0i) = x i\u03b2 or \u03c0i = h(x i\u03b2),\n\ni = (\u03c0i1, . . . , \u03c0iq) is the vector of response probabilities, g is the (multivariate) link\nwhere \u03c0t\n\u22121 is the inverse link function. the representation of ordinal models as mul-\nfunction, and h = g\ntivariate glms is given separately for cumulative-type models (section 9.5.1) and sequential\nmodels (section 9.5.2).\n\n9.5.1 cumulative models\nthe simple cumulative model has the form\n\np (yi \u2264 r|xi) = f (\u03b30r + xt\n\ni \u03b3),\n\nwhere f is a fixed distribution function that, in a multivariate glm, should not be confused\nwith the response function. one obtains immediately for the probabilities \u03c0ir = p (yi = r|xi)\n\ni \u03b3),\ni \u03b3, r = 1, . . . , q, denote the\n\n\u03c0ir = f (\u03b30r + xt\n\ni \u03b3) \u2212 f (\u03b30,r\u22121 + xt\nwhere \u2212\u221e = \u03b300 < \u03b301 < \u00b7\u00b7\u00b7 < \u03b30k = \u221e. let \u03b7ir = \u03b30r + xt\nrth linear predictor. then\n\u239e\n\n\u03c0ir = f (\u03b7ir) \u2212 f (\u03b7i,r\u22121).\n\u239b\n\none obtains the matrix form\n\n\u239e\n\u239f\u23a0 = h\n\n\u239b\n\u239c\u239d \u03c0i1\n\n...\n\u03c0iq\n\n1\n\n\u23a7\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9\n\n\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d\n\n1\n\n...\n\n...\n\n\u239b\n\u239c\u239c\u239c\u239c\u239c\u239d\n\n\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0\n\n\u239e\n\u239f\u239f\u239f\u239f\u239f\u23a0\n\n\u23ab\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23ac\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23ad ,\n\n\u03b301\n\u03b302\n...\n\u03b30q\n\u03b3\n\nxt\ni\n...\n...\n...\n1 xt\ni\n\n "}, {"Page_number": 274, "text": "262\n\nchapter 9. ordinal response models\n\nwhere all parameters are collected in \u03b2t = (\u03b301, . . . , \u03b30q, \u03b3t ). the components of the q-\ndimensional response function h = (h1, . . . , hq) : rq \u2192 rq are given by\n\nhr(\u03b7i1, . . . , \u03b7iq) = f (\u03b7ir) \u2212 f (\u03b7i,r\u22121).\n\nthe link function (inverse response function) is easily derived from the form\n\n\u22121(p (yi \u2264 r|xi)) = \u03b30r + xt\ni \u03b3\n\nf\n\nor, equivalently,\none immediately obtains for g = (g1, . . . , gq) : rq \u2192 rq\n\n\u22121(\u03c0i1 + \u00b7\u00b7\u00b7 + \u03c0ir) = \u03b7ir.\n\nf\n\ngr(\u03c0i1, . . . , \u03c0iq) = f\n\n\u22121(\u03c0i1 + \u00b7\u00b7\u00b7 + \u03c0ir).\n\n(9.8)\n\nfor example, for the logistic distribution function f (\u03b7) = exp(\u03b7)/(1 + exp(\u03b7)), the inverse is\ngiven by f\n\n\u22121(\u03c0) = log(\u03c0/(1 \u2212 \u03c0)). therefore, (9.8) is equivalent to\n\n(cid:25)\n\n(cid:26)\n\nlog\n\n\u03c0i1 + \u00b7\u00b7\u00b7 + \u03c0ir\n1 \u2212 \u03c0i1 \u2212 \u00b7\u00b7\u00b7 \u2212 \u03c0ir\n\n= \u03b7ir,\n\nand hence\n\none obtains\n\ngr(\u03c0i1, . . . , \u03c0iq) = log\n\n\u23a7\u23aa\u23a8\n\u23aa\u23a9\n\n\u239b\n\u239c\u239d \u03c0i1\n\n...\n\u03c0iq\n\ng\n\n\u239e\n\u239f\u23a0\n\n\u23ab\u23aa\u23ac\n\u23aa\u23ad =\n\n\u239b\n\n\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d\n\n1\n\n1\n\n(cid:25)\n\n...\n\n(cid:26)\n\n\u03c0i1 + \u00b7\u00b7\u00b7 + \u03c0ir\n1 \u2212 \u03c0i1 \u2212 \u00b7\u00b7\u00b7 \u2212 \u03c0ir\n\u239e\n\u239b\n\u239c\u239c\u239c\u239c\u239c\u239d\n\n\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0\n\nxt\ni\n...\n...\n...\n1 xt\ni\n\n...\n\n.\n\n\u03b301\n\u03b302\n...\n\u03b30q\n\u03b3\n\n\u239e\n\u239f\u239f\u239f\u239f\u239f\u23a0 .\n\nthe more general model with category-specific parameters has the form\n\nthe derivation of the response and link function is the same as for the simple model. the only\ndifference is in the linear predictor, which now has components\n\np (yi \u2264 r|xi) = f (\u03b30r + xt\n(cid:25)\n\ni \u03b3r).\n\n(cid:26)\n\n\u03b7ir = \u03b30r + xt\n\ni \u03b3r = (1, xt\ni )\n\n\u03b30r\n\u03b3r\n\n.\n\nhence, in the general model, the design matrix is different. one obtains with an identical\nresponse function as for the simple cumulative model\n\n\u239e\n\u239f\u23a0 = h\n\n\u239b\n\u239c\u239d \u03c0i1\n\n...\n\u03c0iq\n\n\u239b\n\u239c\u239c\u239c\u239d 1\n\n\u23a7\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9\n\nxt\ni\n\nxt\ni\n\n1\n\n...\n\n1\n\n...\n\nxt\ni\n\n\u239e\n\u239f\u239f\u239f\u23a0\n\n\u239b\n\n\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d\n\n\u239e\n\n\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0\n\n\u23ab\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23ac\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23ad\n\n,\n\n\u03b301\n...\n\u03b30q\n\u03b31\n...\n\u03b3q\n\nwhere the collection of all parameters is given by \u03b2t = (\u03b301, . . . , \u03b30q, \u03b3t\n\n1 , . . . , \u03b3t\n\nq ).\n\n "}, {"Page_number": 275, "text": "263\n\n9.5. inference for ordinal models\n\n9.5.2 sequential models\nsequential model as multivariate glm\nthe general sequential model has the form\n\np (yi = r|yi \u2265 r, xi) = f (\u03b30r + xt\n\ni \u03b3r)\n\nor, equivalently,\n\n\u03c0ir = p (yi = r|xi) = f (\u03b30r + xt\n\ni \u03b3r)\n\nr\u22121(cid:15)\n\nj=1\n\n(1 \u2212 f (\u03b30j + xt\n\ni \u03b3j)).\n\ni \u03b3r denote the rth linear predictor. then one obtains directly for the\n\nlet \u03b7ir = \u03b30r + xt\nmultivariate response function h = (h1, . . . , hq) : rq \u2192 rq\nr\u22121(cid:15)\n\nhr(\u03b7i1, . . . , \u03b7iq) = f (\u03b7ir)\n\n(1 \u2212 f (\u03b7ij))\n\n(9.9)\n\nand for g = h\n\n\u22121\n\nj=1\n\ngr(\u03c0i1, . . . , \u03c0iq) = f\n\n\u22121(\u03c0ir/(1 \u2212 \u03c0i1 \u2212 \u00b7\u00b7\u00b7 \u2212 \u03c0i,r\u22121)).\n\nthe design matrix is the same as for the category-specific cumulative model. the simpler form\nof the model with \u03b31 = . . . = \u03b3q also has response function (9.9), but the design matrix\nsimplifies to the form of the design matrix of the simple cumulative model.\n\nmaximum likelihood estimation by binary response models\nmaximum likelihood estimates for sequential models may be obtained by fitting binary regres-\nsion models. the binary responses that are used correspond to the transition between categories.\nin the following the equivalence of likelihoods is derived by representing the probability of a\ncategorical response as a product of discrete hazards. let \u03bb(r) denote the conditional transition\nprobability\n\n\u03bb(r) = p (y = r|y \u2265 r),\n\n2\nwhich is parameterized in the sequential model. \u03bb is also called a discrete hazard, in particular\nwhen categories refer to discrete time. for the responses one generally obtains p (y \u2265 r) =\ns=1(1 \u2212 \u03bb(s)) (exercise 9.1). therefore one has\nr\u22121\n\np (y = r) = p (y = r|y \u2265 r)p (y \u2265 r) = \u03bb(r)\n\n(1 \u2212 \u03bb(s)).\n\nr\u22121(cid:15)\n\ns=1\n\nlet the ith observation yi take value ri and \u03bb(r|xi) denote the conditional hazard, given the\ncovariates xi. then the likelihood contribution of observation yi is given by\n\nli = p (yi = ri|xi) = \u03bb(ri|xi)\n\n(1 \u2212 \u03bb(s|xi)).\n\nri\u22121(cid:15)\n\ns=1\n\nby defining the response vector (yi1, . . . , yiri) = (0, . . . , 0, 1), the contribution to the likeli-\nhood may be written as\n\nri(cid:15)\n\ns=1\n\nli =\n\n\u03bb(s|xi)yis(1 \u2212 \u03bb(s|xi))1\u2212yis .\n\n "}, {"Page_number": 276, "text": "264\n\nchapter 9. ordinal response models\n\nli is equivalent to the likelihood of a binary response model with observations yi1, . . . , yiri.\nsince for a sequential model the hazard \u03bb(s|xi) has the form of a binary regression model with\nresponse function f (.), one obtains with \u03bb(s|xi) = f (\u03b7is) the likelihood\n\nli =\n\nf (\u03b7ij)yij (1 \u2212 f (\u03b7ij))1\u2212yij .\n\nri(cid:15)\n\nj=1\n\nn(cid:7)\n\nri(cid:7)\n\nthe total likelihood, given by l = l1 \u00b7 . . . \u00b7 ln, yields the log-likelihood\n\nl =\n\nyij log(f (\u03b7ij)) + (1 \u2212 yij) log(1 \u2212 f (\u03b7ij)).\n\n(9.10)\n\ni=1\n\nj=1\n\ntherefore, the likelihood of the sequential model\n\np (yi = r|yi \u2265 r, xir) = f (\u03b7ir)\n\nis equivalent to the likelihood of the binary model\n\np (yir = 1|xir) = f (\u03b7ir)\n\nfor observations yi1 = 0, . . . , yi,ri\u22121 = 0, yir = 1, i = 1, . . . , n.\n\nit should be noted that the log-likelihood (9.10) has r1 + \u00b7\u00b7\u00b7 + rn binary observations even\nthough the original number of observations is n. a simple recipe for fitting the sequential model\nis to generate for each observation the binary observations yi1, . . . yiri and the corresponding\ndesign variables and then fit the binary model as if the observations were independent.\n\nlet us consider the example of a simple sequential model for which the linear predictor\nhas the form \u03b7ir = \u03b3or + xt\ni \u03b3. to have the usual form xt \u03b2 for the linear predictor of a\nbinary model one defines the parameter vector \u03b2t = (\u03b301, . . . \u03b30q, \u03b3t ) and adapts the covariate\nvector to the observations under consideration. if y = ri, the ri binary observations and design\nvariables are given by\n\nbinary observations\n\n0\n0\n0\n...\n1\n\n1\n0\n0\n\n0\n\ndesign variables\n0\n0\n0\n1\n0\n0\n\n. . .\n. . .\n. . .\n\n0\n0\n1\n\nxt\ni\nxt\ni\n\n0\n\n0\n\n1\n\n0\n\nxt\ni\n\nas a simple example, let us consider a response y \u2208 {1, . . . , 5} and the covariates age (in\nyears) and height (in cm). for a given covariate vector (35,180) let the response be in category\n3. then one obtains three binary observations of the binary model given by\n\nbinary observations\n\n0\n0\n1\n\n1\n0\n0\n\ndesign variables\n0\n1\n0\n\n35\n35\n35\n\n0\n0\n1\n\n0\n0\n0\n\n180\n180\n180\n\nthe equivalence of the likelihood of the multinomial response model and the binary model\n(with derived observations) may be used to obtain maximum likelihood estimates. the binary\nobservations that are used are strongly related to the underlying multinomial response variables.\n\u223c m(1, (\u03c0i1, . . . , \u03c0iq)) denote the original observation. then an observation in cate-\nlet yi\ni = (0, . . . , 1, . . . , 0). the binary observations used in (9.10) are the first ri\ngory ri yields yt\n\n "}, {"Page_number": 277, "text": "9.7. exercises\n\n265\n\nobservations of the vector. thus the binary observations are a truncated version of the underly-\ning multinomial response vector (exercise 9.2). although these observations yield a likelihood\nwhere yi1, . . . , yir look like independent observations, they are not. this means that one can-\nnot use the usual methods of inference for binary data. when computing standard errors and\ninvestigating the goodness-of-fit one should use the instruments for multinomial distributions\ngiven in chapter 8, section 8.6.\n\n9.6 further reading\nsurveys and basic papers. a careful survey that covers the methods for the analysis of ordinal\ncategorical data is agresti (2009). a shorter overview on ordered categorical data was given by\nliu and agresti (2005). an important paper that had a strong influence on the development of\nregression models for ordered response categories is mccullagh (1980).\n\nefficiency. comparisons between the cumulative model and the binary model are found\nin armstrong and sloan (1989) and steadman and weissfeld (1998). agresti (1986) gave an\nextension of r2-type association measures to ordinal data.\n\nprediction. the use of ordinal models for the prediction of ordinal responses was inves-\ntigated by rudolfer et al.\n(1991),\nanderson and phillips (1981), and tutz and hechenbichler (2005). prediction for ordinal data\nis further investigated in section 15.9.\n\n(1995), campbell and donner (1989), campbell et al.\n\ncumulative link models. genter and farewell (1985) consider a generalized link function\nthat includes the probit model, complementary log-log models, and other links. they show that\nthe links may be discriminated for moderate sample sizes. models with a dispersion effect of\nthe type (9.6) have been considered by nair (1987) and hamada and wu (1996).\n\nmodels with assigned scores. williams and grizzle (1972) proposed a model with assigned\nscores for the ordered response categories. lipsitz, fitzmaurice, and molenberghs (1996) con-\nsider goodness-of-fit tests for models of this type.\n\nbayesian models. albert and chib (2001) present a bayesian analysis of the sequential\n\nprobit model.\n\npartial proportional odds models. partial proportional odds models have been investigated\n\nby cox (1995), brant (1990), and peterson and harrell (1990).\n\ngoodness-of-fit tests. lipsitz et al.\n\n(1996) extended the hosmer-lemeshow statistic,\nwhich assesses goodness-of-fit when continuous covariates are present, to the ordinal setting.\nhowever, the tests are based on assigned scores, which is somewhat beyond ordinal response\nmodeling. pulkstenis and robinson (2004) proposed a modification for cases when continuous\nand categorical variables are present as an extension of their test in the binary setting (pulkstenis\nand robinson, 2002).\n\nr packages. the proportional odds model can be fitted with function vglm from the package\nvgam and with function lrm from the package design. it can be also be fitted with the function\npolr from the mass library. attention has to be paid to the algebraic signs of the coefficients.\nthese are inverse to the definition of the porportional odds models used here. with the function\nvglm from the library vgam, both cumulative and sequential models can be fitted.\n\n9.7 exercises\n\n(cid:5)\n9.1 let y \u2208 {1, . . . , k} denote an ordered categorical response variable and \u03bb(r) = p (y = r|y \u2265 r)\ns=1(1 \u2212 \u03bb(s)) holds and the\ndenote conditional transition probabilities. show that p (y \u2265 r) =\nr\u22121\ns=1(1 \u2212 \u03bb(s)).\nr\u22121\nprobability for category r is given by p (y = r) = \u03bb(r)\n\n(cid:5)\n\n "}, {"Page_number": 278, "text": "(cid:2)\n\n266\n\nchapter 9. ordinal response models\n\nq(cid:6)\n\n9.2 let (y1 . . . yk) follow a multinomial distribution m (m, (\u03c01, . . . , \u03c0k)), where\nr \u03c0r = 1. let\n\u03bb(r) = p (y = r|y \u2265 r) denote the discrete hazard, that is, the probability that category r is observed\ngiven y \u2265 r.\n\n(a) show that the probability mass function of the multinomial distribution can be represented as the\n\nproduct of binomial distributions,\n\np (y1 = y1, . . . , yq = yq) =\n\ni=1\n\nbi\u03bb(i)yi (1 \u2212 \u03bb(i))m\u2212y1\u2212\u00b7\u00b7\u00b7\u2212yi\u22121 ,\n\nwhere bi = (m \u2212 y1 \u2212 \u00b7\u00b7\u00b7 \u2212 yi\u22121)!/(yi!(m \u2212 y1 \u2212 \u00b7\u00b7\u00b7 \u2212 yi)!).\n\n(b) let m = 1 and k = 5. show that the probability for the multinomial response vector (0, 0, 1, 0, 0)\n\ndepends only on the first three hazards.\n\n9.3 table 9.1 gives the cross-classification of pain and treatment for the knee data.\n\n(a) fit a cumulative model with general predictor \u03b7r = \u03b30r + pain\u03b3r and investigate if parameters\n\nhave to be category-specific.\n\n(b) fit a sequential model with general predictor \u03b7r = \u03b30r + pain\u03b3r and investigate if parameters have\n\nto be category-specific.\n\n(c) interpret the parameters for both modeling approaches and compare the interpretations.\n\n9.4 ananth and kleinbaum (1997) consider data from a clinical trial of a single-dose post-operative anal-\ngesic clinical trial. four drugs were randomized to patients and the responses were recorded on a five-level\nordinal scale that has been reduced to four categories because of sparse cell counts. the data are given in\ntable 9.14.\n\n(a) fit the proportional odds model and test if the proportional odds assumption holds.\n(b) fit the sequential model and compare with the cumulative model fits.\n\ntable 9.14: rating of drugs from clinical trial.\n\ndrug\n\nc15&c60\nz100&ec4\n\nrating of the drugs\nfair good\n\nvery good\n\n18\n4\n\n20\n13\n\n5\n34\n\npoor\n\n17\n10\n\ntotal\n\n61\n60\n\n9.5 in a questionnaire it was evaluated what effect the expectation of students for finding adequate em-\nployment has on their motivation. the data are given in table 9.15.\n\n(a) fit the proportional odds model and test if the proportional odds assumption holds.\n(b) fit the sequential model and compare with the cumulative model fits.\n\n9.6 the data set children from the package catdata contains the response variable number of children and\ncovariates age, duration of school education, and nationality (see example 7.1). consider the categorized\nresponse number of children 1, 2, 3, or above.\n\n(a) fit the proportional odds model and test if the proportional odds assumption holds.\n(b) fit the sequential model and compare with the cumulative model fits.\n(c) discuss the differences between treating the response as poisson-distributed and as an ordinal re-\n\nsponse.\n\n "}, {"Page_number": 279, "text": "9.7. exercises\n\n267\n\ntable 9.15: motivation of students.\n\neffect on motivation\n\nfaculty\n\nage\n\noften\n\nnegative\n\npsychology\n(1)\n\nphysics\n(2)\n\nteaching\n(3)\n\n\u2264 21(1)\n22\u201323 (2)\n> 24 (3)\n\u2264 21\n22\u201323\n> 24\n\u2264 21\n22\u201323\n> 24\n\n(1)\n\n5\n1\n3\n4\n3\n1\n4\n8\n14\n\nsometimes\nnegative\n(2)\n\n18\n5\n3\n9\n6\n7\n10\n6\n4\n\nnone or\nmixed\n\nsometimes\n\npositive\n\n(3)\n\n29\n13\n11\n39\n33\n28\n11\n10\n14\n\n(4)\n\n3\n2\n3\n6\n7\n7\n0\n0\n0\n\noften\npositive\n\n(5)\n\n0\n1\n5\n2\n2\n2\n3\n0\n1\n\n9.7 the hierarchically structured cumulative model has the form p (y \u2208 s1\u222a\u00b7\u00b7\u00b7\u222asi|x) = f (\u03b8i+xt \u03b30),\np (y \u2264 r|y \u2208 si, x) = f (\u03b8ir + xt \u03b3i).\n\n(a) show that the likelihood decomposes into a sum of two components, the first containing the pa-\n\nrameters {\u03b8i, \u03b30\n\n} and the second containing the parameters {\u03b8ir, \u03b3i\n\n}.\n\n(b) explain how common software for the fitting of cumulative models can be used to fit the hierarchi-\n\ncally structured cumulative model.\n(c) fit the hierarchically structured cumulative model to the eye data given in table 9.10 by specifying\ns1 = {1, 2}, s2 = {3, 4}. also fit the simplified version with \u03b30 = 0. compare the fit of the\nmodels to the fit of the simple cumulative model.\n\n(d) can the hypothesis h0 : \u03b3i = 0 be tested by using the separate fits of the components?\n\n "}, {"Page_number": 280, "text": " "}, {"Page_number": 281, "text": "chapter 10\n\nsemi- and non-parametric generalized\nregression\n\nmost of the models considered in the previous chapters are members of the generalized linear\nmodels family and have the form g(\u03bc) = xt \u03b2, with link function g. the models are non-\nlinear because of the link function, but nonetheless they are parametric, because the effect of\ncovariates is based on the linear predictor \u03b7 = xt \u03b2. in many applications, parametric models\nare too restrictive. for example, in a linear logit model with a unidimensional predictor it is\nassumed that the response probability is either strictly increasing or decreasing over the whole\nrange of the predictor given that the covariate has an effect at all.\n\nexample 10.1: duration of unemployment\nwhen duration of unemployment is measured by two categories, short-term unemployment (1: below 6\nmonths) and long-term employment (0: above 6 months), an interesting covariate is age of the unemployed\nperson. figure 10.1 shows the fits of a linear logistic model, a model with additional quadratic terms, and\na model with cubic terms. the most restrictive model is the linear logistic model, which implies strict\nmonotonicity of the probability depending on age. it is seen that the fit is rather crude and unable to fit the\nobservations at the boundary. the quadratic and the cubic logistic models show better fit to the data but\nstill lack flexibility. non-parametric fits, which will be considered in this chapter, are also given in figure\n10.1. they show that the probability of short-term unemployment seems to be rather constant up to about\n45 years of age but then strongly decreases. the methods behind these fitted curves will be considered in\nsection 10.1.3\n\nin this chapter less restrictive modeling approaches, which allow for a more flexible func-\ntional form of the predictor, are considered. we start with the simple case of a generalized\nnon-parametric regression with a univariate predictor variable. the extension to multiple pre-\ndictors is treated in the following section. then structured additive approaches that also work\nin higher dimensions are considered.\n\n10.1 univariate generalized non-parametric regression\nin this section we consider non-parametric regression models for the simple case of a univariate\npredictor variable. the functional form of \u03bc(x) = e(y|x) is not restricted by a linear term, but\noften it is implicitly assumed that it is smoothly varying when x is a continuous variable. rather\nthan considering \u03bc(x) itself, we consider more flexible forms of the predictor \u03b7(x), which is\n\n269\n\n "}, {"Page_number": 282, "text": "270\n\nchapter 10. semi- and non-parametric generalized regression\n\nfigure 10.1: probability of short-term unemployment plotted against age. left upper\npanel shows the fit of a linear, quadratic, and cubic logistic regression model; right upper\npanel shows local fits, and the lower panel shows fits based on the expansion in basis\nfunctions.\n\nlinked to \u03bc(x) by the usual link or response function:\n\n\u03bc(x) = h(\u03b7(x))\n\nor\n\ng(\u03bc(x)) = \u03b7(x)\n\n\u22121. the choice of h is rather arbitrary, because its\nfor fixed (strictly increasing) h and g = h\nmain role is to ensure that \u03bc is in an admissible range. for example, the logit link ensures that\n\u03bc is in the interval [0, 1] whatever value \u03b7(x) takes. therefore, when modeling \u03b7(x) one does\nnot have to worry about admissible ranges. most often, the canonical link is used because it\nallows for a somewhat simpler estimation.\n\n10.1.1 regression splines and basis expansions\na simple way to obtain non-linear predictors \u03b7(x) is to use non-linear transformations of x. a\nconvenient transformation is provided by polynomials. that means one uses predictors of the\nform \u03b7(x) = \u03b20 + \u03b21x + . . . + \u03b2mxm rather than the simple linear predictor \u03b7(x) = \u03b20 + \u03b21x.\nby including a finite number of power functions one obtains the familiar form of a generalized\npolynomial regression. however, this strategy has some severe drawbacks. for example, single\nobservations may have a strong influence on remote areas of the fitted function, and increasing\nthe degree of the polynomial, although adding flexibility, may yield highly fluctuating curves.\nan alternative is polynomial splines, which are considered in the following. rather than\nfitting a polynomial to the whole domain of x, polynomial splines fit polynomials only within\n\n "}, {"Page_number": 283, "text": "10.1. univariate generalized non-parametric regression\n\n271\n\nm(cid:7)\n\nsmall ranges of the predictor domain. they have the nice property that they still may be repre-\nsented in the form\n\n\u03b7(x) =\n\n\u03b2j\u03c6j(x),\n\n(10.1)\n\nj=1\n\nwhere \u03c6j is the jth fixed transformation of x, also called the jth basis function. the general\nform (10.1) is a linear basis expansion in x that makes estimation rather easy, because, once the\nbasis functions \u03c6j have been determined, the predictor is linear in these new variables. fitting\nprocedures may use the whole framework of generalized linear models.\n\nregression splines: truncated power series basis\npolynomial regression splines are obtained by dividing the domain of x into contiguous inter-\nvals and representing the unknown function \u03b7(x) by a separate polynomial in each interval. in\naddition, the polynomials are supposed to join smoothly at the knots.\n\nthe boundaries of the intervals are determined by a chosen sequence of breakpoints or\nknots \u03c41 < . . . < \u03c4ms from the domain of the predictor [a, b]. with the additional knots\n\u03c40 = a, \u03c4ms+1 = b, a function s defined on [a, b] is called a spline function of degree k (order\nk + 1) if\n\ns is a polynomial of degree k on each interval [\u03c4j, \u03c4j+1], j = 0, . . . , ms, and\nthe derivatives of s(x) up to order k \u2212 1 exist and are continuous on [a, b].\n\nthus polynomial regression splines of degree k form polynomials on the intervals and are\ncontinuous on the whole interval [a, b]; more technically, s \u2208 ck\u22121[a, b], where ck\u22121[a, b]\ndenotes the set of functions for which the (k \u2212 1)th derivative s(x)(k\u22121) is continuous on the\ninterval [a, b]. an important special case is cubic splines, which are polynomials of degree 3\nin each interval and have first and second derivatives at the knots and therefore on the whole\ninterval.\n\nfor given knots, splines of degree k may be represented by the so-called truncated power\n\nseries basis in the form\n\ns(x) = \u03b20 + \u03b21x + . . . + \u03b2kxk +\n\n\u03b2k+i(x \u2212 \u03c4i)k\n+,\n\n(10.2)\n\nms(cid:7)\n\ni=1\n\nwhere (x \u2212 \u03c4i)k\n\n+ are truncated power functions defined by\n\n(cid:29)\n\n(x \u2212 \u03c4)k\n\n+ =\n\n(x \u2212 \u03c4)k\n0\n\nif x \u2265 \u03c4\nif x < \u03c4.\n\nit is easily seen that s(x) from (10.2) has derivations up to order k \u2212 1 that are continuous. the\nrepresentation (10.2) is a linear combination of the basis functions\n\n\u03c61(x) = 1, \u03c62(x) = x, . . . , \u03c6k+1(x) = xk,\n\n\u03c6k+2(x) = (x \u2212 \u03c41)k\n\n+, . . . , \u03c6k+ms+1(x) = (x \u2212 \u03c4k)k\n+,\n\nwhich form the truncated power series basis of degree k (order k + 1). in general, for given\nknots, splines of fixed degree form a vector space of dimension k+ms+1. the truncated power\nseries functions form a basis of that vector space. therefore, in the space of spline functions,\nthe representation is unique, in the sense that only one combination of parameters is able to\nrepresent s(x).\n\n "}, {"Page_number": 284, "text": "272\n\nchapter 10. semi- and non-parametric generalized regression\n\nnow let (10.2) model the unknown predictor \u03b7(x) and the data be given by (yi, xi), i =\n1, . . . , n. then the parameters \u03b2t = (\u03b20, . . . , \u03b2k+ms) may be estimated by maximizing the\nlog-likelihood for the model g(\u03bci) = \u03b7(xi) with the linear predictor\n\n\u03b7(xi) = \u03c6t\n\ni \u03b2,\n\ni = (\u03c61(xi), . . . , \u03c6k+ms+1(xi)). the log-likelihood function, the score function, the\nwhere \u03c6t\nfisher matrix, and the iterative fitting procedure from a generalized linear model are directly\napplicable by using for the ith observation the multiple design vector xi = \u03c6i.\n\nof course, first knots have to be selected. typically the knots are chosen to be evenly spaced\nthrough the range of observed x-values or placed at quantiles of the distribution of x-values.\nmore elaborate strategies to select knots have also been proposed (for references see section\n10.5).\n\n5\n.\n1\n\n0\n.\n1\n\n5\n.\n0\n\n0\n.\n0\n\n0\n\n\u03c41 \u03c42 \u03c43 \u03c44 \u03c45 \u03c46\n\n5\n.\n1\n\n0\n.\n1\n\n5\n.\n0\n\n0\n.\n0\n\n5\n.\n0\n\u2212\n\n0\n\n\u03c41 \u03c42 \u03c43 \u03c44 \u03c45 \u03c46\n\nfigure 10.2: truncated power series basis of degrees 1 and 3.\n\nrepresentation by b-splines\nthe truncated power series representation of splines has the disadvantage that they are numer-\nically unstable for a large number of knots. a basis that is numerically more stable and is in\ncommon use is the b-spline basis.\n\na b-spline function of degree k with knots \u03c4i, . . . \u03c4i+k+1 is defined recursively by\n\n(cid:29)\n\nbi,1(x) =\n\nbi,r+1(x) = x \u2212 \u03c4i\n\u03c4i+r \u2212 \u03c4i\n\n1\n0 otherwise,\n\nif x \u2208 [\u03c4i, \u03c4i+1]\nbi,r(x) + \u03c4i+r+1 \u2212 x\n\u03c4i+r+1 \u2212 \u03c4i+1\n\nbi+1,r(x),\n\nr = 1, . . . , k (dierckx, 1993, p. 8). the last function of this iteration, bi,k+1(x), represents\nthe b-spline function of degree k (order k + 1). by introducing additional knots \u03c4\u2212k \u2264 \u00b7\u00b7\u00b7 \u2264\n\u03c40 = a, b = \u03c4ms+1 \u2264 \u00b7\u00b7\u00b7 \u2264 \u03c4ms+k+1, every spline of degree k has a unique representation in\nk + ms + 1 basis functions:\n\nms(cid:7)\n\ni=\u2212k\n\ns(x) =\n\n\u03b2ibi(x),\n\nwhere in bi(x) = bi,k+1(x) the index k + 1 is omitted. b-splines are very appealing because\nthe basis functions are strictly local. therefore, single fitted basis functions have no effect on\nremote areas. figure 10.3 shows two b-spline bases, for degree 1 and degree 3 for equally and\nunequally spaced knots.\n\nlet us consider b-splines of degree k (order k + 1). then the essential properties of b-\n\nsplines may be summarized as follows:\n\n "}, {"Page_number": 285, "text": "10.1. univariate generalized non-parametric regression\n\n273\n\n0\n.\n1\n\n8\n.\n0\n\n6\n.\n0\n\n4\n.\n0\n\n2\n.\n0\n\n8\n.\n0\n\n6\n.\n0\n\n4\n.\n0\n\n2\n.\n0\n\n0\n.\n0\n\n\u03c41\n\n\u03c42\n\n\u03c43\n\n\u03c44\n\n\u03c45\n\n\u03c46\n\n\u03c47\n\n\u03c48\n\n\u03c49 \u03c410\n\n0\n.\n0\n\n\u03c41\n\n\u03c42\n\n\u03c43\n\n\u03c44\n\n\u03c45\n\n\u03c46\n\n\u03c47\n\n\u03c48\n\n\u03c49 \u03c410\n\n0\n.\n1\n\n8\n.\n0\n\n6\n.\n0\n\n4\n.\n0\n\n2\n.\n0\n\n8\n.\n0\n\n6\n.\n0\n\n4\n.\n0\n\n2\n.\n0\n\n0\n.\n0\n\n\u03c41\u03c42\u03c43\n\n\u03c44 \u03c45\n\n\u03c46 \u03c47 \u03c48\n\n\u03c49\n\n\u03c410\n\n0\n.\n0\n\n\u03c41\u03c42\u03c43\n\n\u03c44 \u03c45\n\n\u03c46 \u03c47 \u03c48\n\n\u03c49\n\n\u03c410\n\nfigure 10.3: b-splines of degrees 1 (left panels) and 3 (right panels). in the upper panels\nthe knots are equally spaced; in the lower panels they are not equally spaced.\n\n- a b-spline consists of k + 1 polynomial pieces, each of degree k.\n\n- the polynomial pieces join at k inner knots; at the joining knots, derivates up to order\nk \u2212 1 are continuous.\n\n- the spline function is positive on a domain spanned by k + 2 knots; elsewhere it is zero.\n\n- at a given x, k + 1 b-splines are non-zero.\n\n- except at boundaries, the spline function overlaps with 2k polynomial pieces of its neigh-\n\nbors.\n\n- b-splines sum up to 1 at each point,\n\n(cid:14)\n\nibi(x) = 1.\n\nit should be noted that b-splines as defined above form a basis of the vector space of splines\nwith fixed degrees and knots. any spline may be represented by the truncated power se-\nries basis or b-splines. the transformation between bases has the form \u03c6t (x) = l\u03c6b(x),\nwhere \u03c6t (x)t = (\u03c6t,1(x), \u03c6t,2(x), . . . ) is a vector containing the truncated power series\nbasis, \u03c6b(x)t = (\u03c6b,1(x), \u03c6b,2(x), . . . ) contains the b-spline basis, and l is a square in-\nt = (\u03c6t (x1) . . . \u03c6t (xn)),\nvertible matrix. for the design matrices built from the data, \u03c6t\n\u03c6t\n\nb = (\u03c6b(x1) . . . \u03c6b(xn)), one obtains \u03c6t = \u03c6bl.\n\nalternative basis functions\nthe linear basis functions approach assumes that \u03b7(x) may be approximated by\n\nm(cid:7)\n\ni=1\n\n\u03b7(x) =\n\n\u03b2i\u03c6i(x),\n\n(10.3)\n\n "}, {"Page_number": 286, "text": "274\n\nchapter 10. semi- and non-parametric generalized regression\n\nwhere \u03c6i(.) are fixed basis functions. the truncated power series basis and b-splines span the\nspace of polynomial splines and yield polynomials on each interval. when k is odd, another set\nof basis functions with this property is given by\n\n\u03c61(x) = 1, \u03c62(x) = x, . . . , \u03c6k+1(x) = xk, \u03c6k+i(x) = |x \u2212 \u03c4i|k, i = 1, . . . , ms.\n\nthe basis functions \u03c6k+i(x), i = 1, . . . , ms, depend only on the distances between x and the\nknots and may be represented by\n\n|x \u2212 \u03c4i|k = r(|x \u2212 \u03c4i|), where\n\nr(u) = uk.\n\nan advantage of the dependence on the distance is that basis functions may be easily defined\nfor higher dimensional predictor variables. for p-dimensional x and knots \u03c4 1, . . . , \u03c4 m one\nconsiders more generally basis functions of the form\n\nr((cid:16)x \u2212 \u03c4 i(cid:16)/\u03bbi),\n\n\u221a\nwhere (cid:16)z(cid:16) =\nzt z denotes the length of vector z. basis functions of this form are called\nradial basis functions and are frequently used in the machine learning community. often they\nhave the form of a localized density. for example, the gaussian radial basis function has the\nform\n\n\u03c6i(x) = exp(\u2212(cid:16)x \u2212 \u03c4 i(cid:16)2\n\n),\n\n2\u03c32\ni\n\nwhere \u03c4i is the center of the basis function and \u03c32\nthe spread of the basis function. for simplicity one may use \u03c32\n\ni is an additional parameter that determines\n\ni = \u03c32.\n\nwhen using radial basis functions on a non-equally spaced grid it is often useful to use\nrenormalized basis functions to obtain more support in areas where all the basis functions are\nsmall. renormalized basis functions of the form\n\n(cid:14)\n\nr((cid:16)x \u2212 \u03c4 i(cid:16)/\u03bb)\ni=1 r((cid:16)x \u2212 \u03c4 i(cid:16)/\u03bb)\n\nm\n\n\u03c6i(x) =\n\n(cid:14)\n\u2019\n\nm\n\nshare with b-splines the property\n\n\u221a\nan alternative choice of basis function that is particularly useful when modeling seasonal\n2sin(2\u03c0kx). it\n\u03c6i(x)\u03c6j(x) = \u03b4ij, where \u03b4ij denotes the kronecker delta\n\nj=1 \u03c6j(x) = 1.\n\u221a\n2cos(2\u03c0kx), \u03c62k+1(x) =\n\neffects is the fourier basis \u03c6(x) = 1, \u03c62k(x) =\nis an orthonormal basis that fulfills\nfunction.\n\nfurther alternatives are wavelet bases, which are very efficient for metric responses (e.g.,\nogden, 1997; vidakovic, 1999) or thin plate splines. the latter provide an elegant way to\nestimate in multiple predictor cases (see, e.g., wood, 2006b, 2006c).\n\nas has been shown, the fitting of basis functions is easily done within the framework of\ngeneralized linear models when the basis function (including knots) has been chosen. however,\nsince one has to estimate as many parameters as basis functions, one has to restrict oneself to\nrather few basis functions to obtain estimates. thus the form of the unknown predictor is\nseverely restricted. more flexible approaches are obtained when many basis functions are used\nbut estimates are regularized (see section 10.1.3).\n\n10.1.2 smoothing splines\nwhen using regression splines it is necessary to choose knot locations and basis functions.\nthis adds some degree of subjectivity into the model fitting process. an alternative approach\n\n "}, {"Page_number": 287, "text": "10.1. univariate generalized non-parametric regression\n\n275\n\nthat is not based on pre-specified knots starts with a criterion that explicitly contains the two\nconflicting aims of fitting, faith with the data and low roughness of the fitted function. one\naims at finding the function \u03b7(x) that maximizes\n\nli(yi, \u03b7(xi)) \u2212 1\n2 \u03bb\n\n(cid:3)(cid:3)\n\n(\u03b7\n\n(u))2du,\n\n(10.4)\n\n(cid:3)\n\n(cid:3)(cid:3)\n\nwhere li(yi, \u03b7(xi)) is the log-likelihood contribution of observation yi, fitted by \u03b7(xi), and \u03b7(.)\nbeing square\nis assumed to have continuous first and second derivatives, \u03b7\nintegrable. the generalized spline smoothing criterion (10.4) is a compromise between faith\nwith the data, represented by a large log-likelihood, and smoothness of the estimated func-\n(cid:3)(cid:3)(u)du). the influence of the\ntion, with smoothness defined by the penalty term \u03bb/2\npenalty term is determined by the tuning parameter \u03bb, with \u03bb = 0 meaning no restriction\nand \u03bb \u2192 \u221e postulating that the second derivative \u03b7\nbecomes zero. by using the deviance\nd({yi},{\u03b7(xi)}) = \u2212\u03c6\u03c3ili(yi, \u03b7(xi)) as a measure for the discrepancy between the fit and\n)\nthe data, maximization of (10.4) turns into the minimization of\n\n, with \u03b7\n\nand \u03b7\n\n\u2019\n\n(\u03b7\n\n(cid:3)(cid:3)\n\n(cid:3)(cid:3)\n\nd({yi},{\u03b7(xi)}) +\n\n(cid:3)(cid:3)\n\n(\u03b7\n\n(u))2du,\n\nwhere the dispersion \u03c6 is included in the tuning parameter \u03bb. for normally distributed responses\none obtains the penalized least-squares criterion:\n\n)\n\n1\n2 \u03bb\n)\n\n(yi \u2212 \u03b7(xi))2 + \u03bb\n\n(cid:3)(cid:3)\n\n(\u03b7\n\n(u))2du.\n\nn(cid:7)\n\ni=1\n\nn(cid:7)\n\ni=1\n\nthe solution \u02c6\u03b7\u03bb(x) is a so-called natural cubic smoothing spline. this means that the function\n\u02c6\u03b7\u03bb(x) is a cubic polynomial on intervals [xi, xi+1], first and second derivatives are continu-\nous at the observation points, and the second derivative is zero at the boundary points x1, xn.\nsmoothing splines arising from the penalized least-squares problem have been considered by\nreinsch (1967); see also green and silverman (1994) for the more general case. for the re-\nsulting splines, the observations themselves correspond to the knots of the polynomial splines.\nsince the solution has the form of cubic splines with n knots, optimization with respect to a\nset of functions reduces to a finite-dimensional optimization problem. it may be shown that the\nevaluations \u02c6\u03b7i = \u02c6\u03b7(xi) at the observed points x1 < \u00b7\u00b7\u00b7 < xn may be computed by maximiza-\ntion of a penalized likelihood:\n\nlp(\u03b7) =\n\nli(yi, \u03b7i) \u2212 1\n\n2 \u03bb\u03b7t k\u03b7,\n\n(10.5)\n\nwhere \u03b7t = (\u03b71, . . . , \u03b7n) and k is a matrix that depends only on the differences xi+1 \u2212 xi\n(see green and silverman, 1994). thus, in principle, estimates \u02c6\u03b7 may be computed by using the\nfisher scoring algorithm with a simple penalty term. however, if the number of observations\nand therefore knots is large, computation by fisher scoring is not very efficient. in particular,\nif one has to handle more than one covariate, the computational costs become very severe. for\nthe penalized least-squares problem, an algorithm that explicitly uses that the matrix k implies\nbanded matrices has been given by reinsch (1967).\nfor normally distributed responses, maximization of lp(\u03b7) corresponds to the penalized\nleast-squares problem that has the solution \u02c6\u03b7 = (i + \u03bbk)\u22121y, where i is the identity matrix\nand yt = (y1, . . . , yn). therefore one has a simple linear smoother \u02c6\u03b7 = s\u03bby with a smoother\n\nn(cid:7)\n\ni=1\n\n "}, {"Page_number": 288, "text": "chapter 10. semi- and non-parametric generalized regression\n\n276\nmatrix s\u03bb = (i + \u03bbk)\u22121. however, computation should not be based on this explicit form;\nreinsch (1967) proposed an efficient algorithm that makes efficient use of the band matrices\nthat constitute the matrix k.\n\nsmoothing splines provide some background for the use of spline functions. the maxi-\nmization problem is clearly defined on functional spaces and does not imply that the function\none seeks is a spline. nevertheless, the solution is cubic splines with as many basis functions\nas observations. in practice, regression splines, in particular when estimated by discrete penal-\nization techniques (see next section), yield similar estimates with fewer knots than smoothing\nsplines. in addition, when using regression splines, the number of knots may be determined by\nthe user.\n\n10.1.3 penalized estimation\nthe big advantage of the linear basis function approach \u03b7(x) =\nj \u03b2j\u03c6j(x) is that for fixed\nbasis functions one has a linear predictor and estimation may be based on procedures that are\nfamiliar from generalized linear models. however, to be sufficiently flexible, one has to use\na fine grid of intervals for regression splines or many basis functions when using radial basis\nfunctions.\n\n(cid:14)\n\none strategy is to select basis functions in a stepwise way; alternatively, one uses many basis\nfunctions right from the start. using as many basis functions as possible should ameliorate the\ndependence on the chosen knots. the difficulty with many basis functions is the instability\nor even non-existence of estimates. one approach to overcome this problem is to regularize\nestimation by including a penalty term as in the derivation of smoothing splines. rather than\nusing simple maximum likelihood estimation one maximizes the penalized log-likelihood\n\nlp(\u03b2) = l(\u03b2) \u2212 \u03bb\n\n2 j(\u03b7),\n\n\u2019\n\nwhere l(\u03b2) is the familiar log-likelihood function, \u03bb is a tuning parameter, and j(\u03b7) is a penalty\nfunctional measuring the roughness of \u03b7.\n\n(\u03b7\n\na roughness penalty that has been used in the derivation of smoothing splines is j(\u03b7) =\n(cid:3)(cid:3)(u))2 du as a global measure for the curvature of the function \u03b7(x). when maximizing lp,\none looks for a compromise between data fit and smoothness of the estimated predictor \u03b7(x).\nthe parameter \u03bb controls the trade-off between the smoothness of \u03b7(x) and the faith with the\ndata. large values of \u03bb enforce smooth functions with small variance but possibly big bias,\nwhereas small values of \u03bb allow wiggly function estimates with high variance.\n\na somewhat more general penalty is\n\n)\n\nb\n\n(\u03b7(d)(u))2du,\n\nj(\u03b7) =\n\na\n\nwhere \u03b7(d) denotes the dth derivative and [a, b] is the range of observations. when using linear\nbasis functions \u03b7(x) = \u03c3j\u03b2j\u03c6j(x) one obtains with \u03c6(d)(u)t = (\u03c6(d)\nm (u)) the\nparameterized penalty\n\n1 (u), . . . \u03c6(d)\n\nj(\u03b7) =\n\nwhere k = (kij) has entries\n\n)\n\n(cid:16)\u03b2t \u03c6(d)(u)(cid:16)2du = \u03b2t k\u03b2,\n)\n\n\u03c6(d)\ni\n\n(u)\u03c6(d)\n\nj (u)du.\n\nb\n\nkij =\n\na\n\n(10.6)\n\n "}, {"Page_number": 289, "text": "10.1. univariate generalized non-parametric regression\n\n277\n\nthe penalty (10.6) has the advantage that it may be used for any spacing of knots. the penalized\nlikelihood takes the simple form\n\n2 \u03b2t k\u03b2,\nwhich is familiar from shrinkage estimators (see chapter 6).\n\nlp(\u03b2) = l(\u03b2) \u2212 \u03bb\n\nthe effect of a penalty usually becomes clear by considering the limit. when polynomial\nsplines of degree k are used, k should be chosen such that d < k, since otherwise derivatives\nwould not exist. for d < k, the penalty (10.6) shrinks toward a polynomial of degree d \u2212 1.\nthus, for \u03bb \u2192 \u221e, a polynomial of degree d\u22121 is fitted. for example, when using cubic splines\nand derivatives of the second order, d = 2, \u03bb \u2192 \u221e produces a linear fit. this means that by\nfitting splines, part of the function is not penalized. by reparameterization the unpenalized part\ncan be separated from the rest, yielding a simpler penalty (for details see appendix c).\n\nfor specific patterns of knots alternative penalties may be used. however, the choice of a\nsensible penalty should be linked to the choice of the basis functions. if one uses the truncated\npower function basis (10.2) and equally spaced knots, an adequate penalty is also\n\nms(cid:7)\n\nj =\n\n\u03b22\ni ,\n\ni=k+1\n\nwhich penalizes only coefficients of the truncated power functions. since only coefficients of\nthe truncated power functions are penalized, the fit is shrunk toward the log-likelihood fit of a\npolynomial of degree k. for \u03bb \u2192 \u221e one fits the polynomial model \u03b7(x) = \u03b20+\u03b21x+...+\u03b2kxk.\n\npenalized splines\nan alternative and widely used penalty is based on differences. marx and eilers (1998) pro-\nposed using b-splines with equally spaced knots and penalties of the form\n\njd =\n\n(\u03b4d\u03b2j)2 = \u03b2t k d\u03b2,\n\n(10.7)\n\nm(cid:7)\n\nj=d+1\n\nwhere \u03b4 is the difference operator, operating on adjacent b-spline coefficients, that is, \u03b4\u03b2j =\n\u03b2j \u2212 \u03b2j\u22121, \u03b42\u03b2i = \u03b4(\u03b2j \u2212 \u03b2j\u22121) = \u03b2j \u2212 2\u03b2j\u22121 + \u03b2j\u22122. the method is referred to as\np-splines (for penalized splines). the corresponding matrix kd has a banded structure and\nsimply represents the differences in matrix form.\nlet the (m \u2212 1) \u00d7 m-matrix d1 be given by\n\n\u239b\n\u239c\u239c\u239c\u239d\n\nd1 =\n\n\u22121\n1\n0\n0 \u22121\n1\n0 \u22121\n0\n...\n\n0\n. . .\n0\n. . .\n1\n. . .\n... \u22121\n\n0\n0\n0\n\n1\n\n\u239e\n\u239f\u239f\u239f\u23a0 .\n\nthen the vector of first differences is d1\u03b2 and the corresponding penalty term is \u03b2t k1\u03b2 =\n1 d1\u03b2. the differences of order d as given in (10.7) are obtained from dd\u03b2 = d1dd\u22121\u03b2\n\u03b2t dt\nand kd = dt\nd dd. by building differences of differences, the dimension reduces such that kd\nis a (m\u2212d)\u00d7(m\u2212d)-matrix. the choice of the order of differences determines the smoothness\nof the intended estimate. gijbels and verhaselt (2010) proposed a data-driven procedure for the\nchoice of the order based on the aic criterion.\n\n "}, {"Page_number": 290, "text": "278\n\nchapter 10. semi- and non-parametric generalized regression\n\nthe penalty (10.7) is useful for b-splines but also for bases like the radial basis functions.\nthe only restriction is that the knots should be equally spaced. b-splines have the advantage\nthat in the limit, with strong smoothing, a polynomial is fitted. if a penalty of order d is used\nand the degree of the b-spline is higher than, d, for large values of \u03bb the fit will approach a\npolynomial of degree d \u2212 1. marx and eilers (1998) also use equally spaced knots when the\ndata are far from being uniformly distributed. however, when the spacings of the data are very\nirregular, knots at quantiles seem more appropriate.\n\nif one changes the basis, of course, the corresponding penalty changes. in general, when\nusing basis functions, the linear predictor has the form \u03b7 = \u03c6\u03b2, where \u03c6 is the design matrix\ncomposed from evaluations of basis functions, more precisely, \u03c6 = (\u03c6ij) is a (n \u00d7 m)-matrix\nwith entries \u03c6ij = \u03c6j(xi). the transformation between the truncated power series and the b-\nsplines is given by \u03c6t = \u03c6bl, where l is a square invertible matrix, and \u03c6t and \u03c6b denote\nthe matrices for the truncated power series and b-splines, respectively. let the predictor for\nb-splines be \u03b7 = \u03c6b\u03b2b with penalty \u03b2t\nbk b\u03b2b. then one obtains for the truncated power\nseries \u03b7 = \u03c6t \u03b2t = \u03c6bl\u03b2t = \u03c6b\u03b2b with \u03b2b = l\u03b2t . the corresponding penalty in\n\u03b2t -parameters is \u03b2t\n\nt kt \u03b2t , where k t = lt kbl.\n\nt lt kbl\u03b2t = \u03b2t\n\nbkb\u03b2b = \u03b2t\n\nbayesian view of difference penalties\nwithin a bayesian framework parameters are restricted by assuming a prior distribution on the\nspace of parameters. the bayesian analog to differences of order d are random walks of order\nd. first-and second-order random walks for equidistant knots may be specified by\n\n\u03b2j = \u03b2j\u22121 + uj,\n\nj = 2, . . .\n\nand\n\n\u03b2j = 2\u03b2j\u22121 \u2212 \u03b2j\u22122 + uj,\n\nj = 3, . . . ,\n\nwhere uj \u223c n(0, \u03c4 2). for initial values one assumes diffuse priors p(\u03b21) and p(\u03b21, \u03b22), re-\nspectively. alternatively, one can assume \u03b2j|\u03b2j\u22121 \u223c n(\u03b2j\u22121, \u03c4 2) for the first-order random\nwalk and \u03b2j|\u03b2j\u22121, \u03b2j\u22122 \u223c n(2\u03b2j\u22121 \u2212 \u03b2j\u22122, \u03c4 2) for the second-order random walk. the order\nof the random walk determines the structures within the fitted parameters. the first-order walk\ntends to fit a constant for the coefficients, whereas a second-order random walk implies a linear\ntrend, which becomes obvious from \u03b2j \u2212 \u03b2j\u22121 = \u03b2j\u22121 \u2212 \u03b2j\u22122 + \u03b5i. the variance parameter\nacts as a smoothing parameter. if \u03c4 is very small, the coefficients have to be close to a constant\nor a linear trend. if \u03c4 is large, the coefficients can deviate from the specified trend.\n\nthe corresponding joint distributions of the parameter vectors have the form of an (im-\n\nproper) gaussian distribution:\n\np(\u03b2|\u03c4 2) \u221d exp(\u2212 1\n\n2\u03c4 2 \u03b2t kd\u03b2),\n\nwhere the precision matrix kd is given by kd = dt\nbrezger (2004a), brezger and lang (2006), and rue and held (2005).\n\nd dd. for more details, see lang and\n\nmaximizing the penalized likelihood\nfor fixed knots and a given \u03bb, the penalties considered in this section yield the penalized like-\nlihood\n\nlp(\u03b2) = l(\u03b2) \u2212 \u03bb\n\n2 \u03b2t k\u03b2,\n\n(10.8)\n\n "}, {"Page_number": 291, "text": "10.1. univariate generalized non-parametric regression\n\n279\n\nwhere k is a symmetric matrix and l(\u03b2) is the log-likelihood of the linear basis predictors\n\u03b7i = \u03b7(xi) = \u03c6t\ni \u03b2, \u03c6t\nin matrix notation, the corresponding\npenalized score function is\n\ni = (\u03c61(xi), . . . , \u03c6m(xi)).\n\nsp(\u03b2) = \u03c6t d(\u03b2) \u03c3(\u03b2)\n\n\u22121(y \u2212 \u03bc) \u2212 \u03bbk\u03b2,\n\nwhere \u03c6t = (\u03c61, . . . , \u03c6n) is the design matrix, d(\u03b2) = diag (\u2202h(\u03b71)/\u2202\u03b7, . . . , \u2202h(\u03b7n)/\u2202\u03b7)\nn) is the covariance matrix, and\nis the diagonal matrix of derivatives, \u03c3(\u03b2) = diag (\u03c32\nyt = (y1, . . . , yn), \u03bct = (\u03bc1, . . . , \u03bcn) are the vectors of observations and means.\n\n1, . . . , \u03c32\n\nthe estimation equation sp(\u02c6\u03b2) = 0 may be solved by iterative pseudo-fisher scoring:\n\n\u02c6\u03b2\n\n(k+1) = \u02c6\u03b2\n\n(k) + f p(\u02c6\u03b2\n\n(k))\n\n\u22121sp(\u02c6\u03b2\n\n(k)),\n\nwhere f p(\u03b2) = f (\u03b2) + \u03bbk and f (\u03b2) = e(\u2212\u2202l/\u2202\u03b2\u2202\u03b2t ) = \u03c6t w (\u03b2)\u03c6 with w (\u03b2) =\nd(\u03b2) \u03c3(\u03b2)\u22121d(\u03b2)t . in the same way as for generalized linear models, pseudo-fisher scor-\ning can be written as iteratively reweighted least-squares fitting:\n\u22121\u03c6t w (\u02c6\u03b2\n\n(k+1) = (\u03c6t w (\u02c6\u03b2\n\n(k))\u03c6 + \u03bbk)\n\n(k))\u02dc\u03b7(\u02c6\u03b2\n\n(k))\n\n\u02c6\u03b2\n\nwith the adjusted variable \u02dc\u03b7(\u02c6\u03b2) = \u02c6\u03b7 + d(\u02c6\u03b2)\u22121(y \u2212 \u03bc(\u02c6\u03b2)), where \u02c6\u03b7 = \u03c6\u02c6\u03b2. at convergence\none obtains\n\n\u02c6\u03b2 = (\u03c6t w (\u02c6\u03b2)\u03c6)\n\n\u22121\u03c6t w (\u02c6\u03b2)\u02dc\u03b7(\u02c6\u03b2).\n\nthe hat matrix may be approximated by\n\nh = w (\u02c6\u03b2)t /2\u03c6(\u03c6t w (\u02c6\u03b2)\u03c6 + \u03bbk)\n\n\u22121\u03c6t w (\u02c6\u03b2)1/2.\n\napproximate covariances are obtained by the sandwich matrix:\n\ncov(\u02c6\u03b2) \u2248 (f (\u02c6\u03b2) + \u03bbk)\n\n\u22121f (\u02c6\u03b2)(f (\u02c6\u03b2) + \u03bbk)\n\n\u22121.\n\n(cid:14)\ni(yi \u2212 \u03c6t\n\nin the case of normally distributed responses and an identity link, maximization of the penal-\ni \u03b2)2 +\nized likelihood is equivalent to solving the penalized least-squares problem\n\u02dc\u03bb\u03b2t k\u03b2, where \u02dc\u03bb = \u03bb\u03c32. the solution has the simple form\n\u22121\u03c6t y\n\n\u02c6\u03b2\u03bb = (\u03c6t \u03c6 + \u02dc\u03bbk)\n\nwith design matrix \u03c6t = (\u03c61, . . . \u03c6n) and yt = (y1, . . . , yn). then the fitted values are given\nby \u02c6y = \u03c6\u02c6\u03b2\u03bb = s\u03bby with s\u03bb = \u03c6(\u03c6t \u03c6 + \u02dc\u03bbk)\u22121\u03c6t .\n\neffective degrees of freedom of a smoother\nin linear regression the trace of the hat matrix, which is defined by \u02c6y = hy, may be used to\ncompute the degrees of freedom of the fit, since\n\ntr(h) = number of fitted parameters.\n\nwith an n\u00d7p full rank design matrix x, the hat matrix in linear regression is x(x t x)\u22121x t .\nfor linear smoothers, given by \u02c6\u03bc = s\u03bby, the trace of s\u03bb plays the same role and tr(s\u03bb) may\nbe interpreted as the number of parameters used in the fit or the effective degrees of freedom of\nthe fit. in particular, when responses are normally distributed and h is the identity, one obtains\nlinear smoothers. for penalized regression splines one obtains s\u03bb = \u03c6(\u03c6t \u03c6 + \u02dc\u03bbk)\u22121\u03c6t .\n\n "}, {"Page_number": 292, "text": "chapter 10. semi- and non-parametric generalized regression\n\n280\nwhen \u02dc\u03bb = 0 s\u03bb is the usual hat matrix, \u03c6(\u03c6t \u03c6)\u22121\u03c6t with design matrix \u03c6 and the number\nof columns of the full rank matrix \u03c6 are the degrees of freedom. if \u02dc\u03bb is very large, the model is\nless flexible and the effective degrees of freedom will be much smaller.\n\nan alternative way to determine the degrees of freedom is associated with the unpenalized\nestimate \u02c6\u03b2u resulting from \u02dc\u03bb = 0. wood (2006a) considers the penalized estimate of \u03b2 in the\nform\n\n\u02c6\u03b2 = (\u03c6t \u03c6 + \u02dc\u03bbk)\n= (\u03c6t \u03c6 + \u02dc\u03bbk)\n\n\u22121\u03c6t y = (\u03c6t \u03c6 + \u02dc\u03bbk)\n\u22121\u03c6t \u03c6\u02c6\u03b2u\n\n\u22121\u03c6t \u03c6(\u03c6t \u03c6)\n\n\u22121\u03c6t y\n\nsince the unpenalized estimate is \u02c6\u03b2u = (\u03c6t \u03c6)\u22121\u03c6t y. then the transformation from \u02c6\u03b2u to \u02c6\u03b2\nhas the form \u02c6\u03b2 = m \u02c6\u03b2u with\n\nm = (\u03c6t \u03c6 + \u02dc\u03bbk)\n\n\u22121\u03c6t \u03c6.\n\nthe diagonal elements mii of m may be seen as an approximate measure of how much the pe-\nnalized estimate will change when \u02c6\u03b2 changes by one unit. in the unpenalized setting the ith pa-\nrameter \u03b2i has one degree of freedom, which changes approximately by a factor mii. however,\nas wood (2006a) notes, there is no general guarantee that mii > 0. as an approximation mii is\nuseful to investigate the effective degrees of freedom associated with the ith parameter. more-\nover, the effective degrees of freedom, determined by the trace, tr(m), is equivalent to the de-\ngrees of freedom obtained by tr(s\u03bb) since tr(\u03c6t \u03c6 + \u02dc\u03bbk)\u22121\u03c6t \u03c6 = tr(\u03c6(\u03c6t + \u03bbk)\u22121\u03c6t ).\nin the general case, when \u03bc = h(\u03b7) is fitted, smoothers are not linear and the effective\n\ndegrees of freedom may be defined as the trace of the corresponding generalized hat matrix:\n\nh = \u03c6t (\u03c6t w \u03c6 + \u03bbk)\n\n\u22121\u03c6.\n\nindividual degrees of freedom for separate components may again be derived by considering\n\u22121(y \u2212 \u02c6\u03bc)\nthe fitting at convergence. with a vector of pseudo-observations \u02dc\u03b7(\u02c6\u03b2) = \u03c6\u02c6\u03b2 + d\none has\n\n\u02c6\u03b2 = (\u03c6t w \u03c6 + \u03bbk)\n= (\u03c6t w \u03c6 + \u03bbk)\n= m(\u03c6t w \u03c6)\n\n\u22121\u03c6t w \u02dc\u03b7(\u02c6\u03b2u),\n\n\u22121\u03c6t w \u02dc\u03b7(\u02c6\u03b2)\n\u22121\u03c6t w \u03c6(\u03c6t w \u03c6)\n\n\u22121\u03c6t w \u02dc\u03b7(\u02c6\u03b2)\n\nwhere m = (\u03c6t w \u03c6 + \u03bbk)\u22121\u03c6t w \u03c6 and \u02dc\u03b7(\u02c6\u03b2u) = (\u03c6t w \u03c6)\u22121\u03c6t w \u02dc\u03b7(\u02c6\u03b2) is an ap-\nproximation to the unpenalized estimate (\u03c6t w \u03c6)\u22121\u03c6t w \u03b7(\u02c6\u03b2u). again, one has tr(h) =\ntr(m).\n\nexample 10.2: duration of unemployment\nfor the duration of unemployment data one obtains the fitted functions given in figure 10.4. the fitted\nbasis functions were cubic b-splines on an equally spaced grid. the left panel shows the unpenalized fit,\nand the right panel shows the fit based on a first-order difference penalty (smoothing parameter chosen\nby generalized cross-validation). in addition, the weighted basis functions that produce the fit are shown.\nit is seen that the unpenalized fit is close to the data but very wiggly. as an estimate of the underlying\nfunction it is hardly convincing. the penalized estimate is much smoother and also aesthetically more\nsatisfying.\n\nthe penalized likelihood for classical smoothing splines (10.5) has the same form as the\nlikelihood for regression splines (10.8). the main difference is that for smoothing splines the\n\n "}, {"Page_number": 293, "text": "10.1. univariate generalized non-parametric regression\n\n281\n\nvector of evaluations \u03b7 is n-dimensional, whereas for regression splines it is much smaller,\ndepending on the number of basis functions. the latter approach uses a smaller number of\nbasis functions. smoothers that use considerably less than n basis functions are usually called\nlow-rank smoothers, while smoothers that use n basis functions are called full-rank smoothers.\nsmall-rank smoothers may reduce the computation time without loosing much in accuracy. for\nlinear smoothers, the effect of the reduction to fewer dimensions was treated extensively by\nhastie (1996).\n\nt\n\nl\n\nn\ne\nm\ny\no\np\nm\ne\nn\nu\n \nm\nr\ne\nt\n\u2212\n\nt\nr\no\nh\ns\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n.\n0\n\n2\n.\n0\n\n0\n.\n0\n\nt\n\nl\n\nn\ne\nm\ny\no\np\nm\ne\nn\nu\n \nm\nr\ne\nt\n\u2212\n\nt\nr\no\nh\ns\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n.\n0\n\n2\n.\n0\n\n0\n.\n0\n\n20\n\n30\n\n40\n\n50\n\n60\n\n20\n\n30\n\n40\n\n50\n\n60\n\nage/years\n\nage/years\n\nfigure 10.4: probability of short-term unemployment plotted against age; cubic splines\non equally spaced grid, not penalized (left panel) and penalized (right panel) by first-order\ndifferences.\n\n10.1.4 local regression\nlocalizing techniques provide a quite different approach to the estimation of the unknown\npredictor function \u03b7(x). in a local regression estimation of \u03b7(x) at a target value, x is obtained\nby fitting a parametric model locally in a neighborhood around x. the parametric model to\nbe fitted is a model with polynomial terms, giving the procedure its name, local polynomial\nregression. taylor expansions provide some motivation for the approach: if one assumes that\nthe function \u03b7(x) is sufficiently smooth, it may be approximated by a taylor expansion (see\nappendix b):\n\n\u03b7(xi) = \u03b7(x) + \u03b7\n\n(cid:3)\n\n(x)(xi \u2212 x) + \u03b7\n\n(cid:3)(cid:3)\n\n(x)(xi \u2212 x)2/2 + . . . ,\n\nwhich, for fixed x, has the form\n\n\u03b7(xi) \u2248 \u03b20 + \u03b21(xi \u2212 x) + \u03b22(xi \u2212 x)2 + . . . ,\n\nwhere the dependence of the parameters \u03b2j on the target value x is suppressed. since \u03b7(xi)\nmay be approximated by a polynomial, one fits (for fixed x) locally the model with polynomial\npredictors. when an approximation by second-order terms is used, one uses the predictors\n\n\u03b7i = \u03b7(xi) = \u02dcxt\ni = (1, (xi \u2212 x), (xi \u2212 x)2), \u03b2t = (\u03b20, \u03b21, \u03b22).\n\u02dcxt\n\ni \u03b2,\n\nlocal fitting means maximization of the local log-likelihood:\n\nn(cid:7)\n\ni=1\n\nlx(\u03b2) =\n\nli(\u03b2)w\u03bb(x, xi),\n\n "}, {"Page_number": 294, "text": "282\n\nchapter 10. semi- and non-parametric generalized regression\n\nwhere li(\u03b2) is the log-likelihood for ith observation of the model with predictors \u03b7(xi), and\nw\u03bb(x, xi) is a weighting function depending on the target value x and the observation xi, and,\nin addition, contains a smoothing parameter \u03bb. common weight functions are based on ker-\nnels. when a kernel or density function, which is a continuous symmetric function that fulfills\n\n\u2019\n\nk(u)du = 1, the corresponding kernel weight is\n\nw\u03bb(x, xi) \u221d k( x \u2212 xi\n\n\u03bb\n\n).\n\ncandidates are the gaussian kernel, where k is the standard gaussian density, or more refined\n4(1 \u2212 u2) for |u| \u2264 1 and zero otherwise. the\nkernels like the epanechnikov kernel k(u) = 3\nsmoothing parameter (or window width) \u03bb determines how local the estimate is. for small\n\u03bb the weights decrease fast with increasing distance |x \u2212 xi| and estimates are based solely\non observations from the close neighborhood of x. for large \u03bb the decrease is very slow; in\nthe extreme case (\u03bb \u2192 \u221e) each observation has the same weight and one fits the polynomial\nregression model to the full dataset.\nlet \u02c6\u03b2 = ( \u02c6\u03b20, \u02c6\u03b21, \u02c6\u03b22)t denote the estimate resulting from the maximization of lx(\u03b2) based\non an approximation by second-order terms. then the estimates \u02c6\u03b7(x) and \u02c6\u03bc(x) at target value\nx are given by\n\n\u02c6\u03b7(x) = \u02c6\u03b20, \u02c6\u03bc(x) = h( \u02c6\u03b20).\n\nof course the estimates have to be computed on a grid of target values to obtain the functions\n\u02c6\u03bc(.) and \u02c6\u03b7(.).\n\nsince local likelihood is a likelihood with multiplicative weights, computing \u02c6\u03b2 (for target\nvalue x) is straightforward by using weighted scoring techniques. the local score function\nsx(\u03b2) = \u2202lx(\u03b2)/\u2202\u03b2 has the form\n\nn(cid:7)\n\n(cid:14)\n(cid:14)\n\nn(cid:7)\n\n\u22122\ni\n\nn(cid:7)\n\ni=1\n\nsx(\u03b2) =\n\nsi(\u03b2)w\u03bb(x, xi) =\n\n\u22122\n\u02dcxi(\u2202h(\u03b7i)/\u2202\u03b7)\u03c3\ni\n\n(yi \u2212 \u03bci)w\u03bb(x, xi).\n\ni=1\n\ni=1\n\nthe solution of sx(\u02c6\u03b2) = 0 can be obtained by weighted iterative fisher scoring:\n\n\u02c6\u03b2\n\n(k+1) = \u02c6\u03b2\n\n(k) + f \u03bb(\u02c6\u03b2\n\n(k))\n\n\u22121sx(\u02c6\u03b2\n\n(k)),\n\n(10.9)\n\nn\n\ni=1 \u02dcxi \u02dcxt\n\ni (\u2202h(\u03b7i)/\u2202\u03b7)\u03c3\n\nwhere f \u03bb(\u03b2) =\n. the simplest polynomial that can be fitted\nin a local regression is a polynomial of degree zero. then one fits the model \u03bci = h(\u03b20)\nfor all observations. when using the canonical link, the estimation equation sx( \u02c6\u03b20) = 0 for\ni(yi \u2212 \u02c6\u03bc(x))w\u03bb(x, xi) = 0 and the resulting estimate is a weighted sum of\ntarget value x is\nobservations:\n\n\u02c6\u03bc(x) =\n\ns\u03bb(x, xi)yi,\n\n(cid:14)\n\n(10.10)\n\n(cid:14)\n\nj w\u03bb(x, xj)), for which\n\nwhere the weights are given by s\u03bb(x, xi) = w\u03bb(x, xi)/(\n1 holds. the estimate (10.10) illustrates nicely the effect of localizing. rather than fitting a\npolynomial parametric regression model, one estimates \u03bc(x) as a weighted mean over obser-\nvations from a neighborhood of x. the resulting estimate is not based on the assumption of\nan underlying parametric model (which might not be identical to the data generating model).\nwhen a polynomial of degree larger than zero is fitted, the effect is also an averaging over\nneighborhood observations, but with a much more complicated weighting scheme. the sim-\nple estimate (10.10) with kernel weights has been proposed for metric responses by nadaraya\n(1964) and watson (1964) and is called the nadaraya-watson estimator.\n\ni s\u03bb(x, xi) =\n\n "}, {"Page_number": 295, "text": "10.1. univariate generalized non-parametric regression\n\n283\n\nfitting of higher polynomials often results in smoother estimates and has less problems\nwith the bias in boundary regions. in particular, polynomials with odd degree show so-called\nboundary carpentry, which means that local fitting automatically adapts to the special data\nsituation at the boundary of the design (see hastie and loader, 1993). local polynomial re-\ngression for metric responses has a long tradition, which is nicely reviewed in cleveland and\nloader (1996). the extension to a wider class of distributions started with the concept of local\nlikelihood estimation, which was formulated by tibshirani and hastie (1987), who gave an ex-\ntensive treatment of asymptotic properties of local regression. loader (1994) gave an overview\nof localizing techniques including classification problems.\n\none problem with local estimates is that they work well only in low to moderate dimensions\nbecause they suffer from the so-called curse of dimensionality (bellman, 1961). it has many\nmanifestations, which are nicely examined in hastie et al. (2009). one demonstration they\nused considers data that are uniformly distributed in a p-dimensional unit hypercube. if one\nwants to capture a fraction r of observations about a target point, the needed edge length (for\neach dimension) is r1/p. therefore, if r = 0.1, that is, 10 percent of the data are to be captured,\nwith p = 10 one needs a cube with edge length 0.8, which is hardly local. if one wants to\nestimate with the same accuracy as in low dimensions, the sample size typically has to grow\nexponentially with the dimension (for examples and details see hastie et al., 2009, pp. 22ff).\n\n10.1.5 selection of smoothing parameters\nwhen smoothers are applied several quantities have to be specified. with regression splines\none has to specify the locations of knots and the degree of the polynomial; for localized esti-\nmates one has to choose the polynomial and the type of localization. but, most important, one\nhas to select a smoothing parameter that determines the smoothness of the estimate and there-\nfore crucially determines the performance of the smoothing procedure. in particular, for linear\nsmoothers there is a wide body of literature investigating asymptotically efficient smoothing\nprocedures. in the following we consider some simple data-driven procedures that may be also\nused for non-linear smoothers.\n\ncross-validation\nthe naive approach to smoothing parameter selection is to select the parameter that optimizes\nthe goodness-of-fit. the back side of this approach is that the data at hand are well fitted but the\nfit is rather wiggly, hardly approximates the underlying curve, and generalizes badly. a simple\ncure for this problem is cross-validation, where the data are split into disjoint sets. part of the\ndata is used to fit a curve and the performance is evaluated on the data that have not been used\nin fitting. thus the fit has not seen the new data. in k-fold cross-validation the data are split\ninto k roughly equal-sized parts; then, in turn, k \u2212 1 of these parts are used to fit the curve\nand the performance is evaluated on the part of the data that has been left out. the extreme\ncase is leaving-one-out cross-validation, where only one observation is used to evaluate future\nperformance.\nin the general setting a measure for the performance is predictive deviance. let s =\n{(yi, xi), i = 1, . . . , n} denote the full dataset, let \u02c6\u03b7\\i(x) be the fitted value at x based on\nobservations s \\ (yi, xi), and let \u02c6\u03bc\\i(x) = h(\u02c6\u03b7\\i(x)), where the dependence on \u03bb is sup-\npressed in the notation. then performance on future observations may be measured by the\nleaving-one-out predictive deviance:\n\n(cid:7)\n\n(yi,xi)\u2208s\n\ndcv (\u03bb) =\n\nd(yi, \u02c6\u03bc\\i(xi)),\n\n "}, {"Page_number": 296, "text": "284\n\nchapter 10. semi- and non-parametric generalized regression\n\nwhere d(yi, \u02c6\u03bc\\i(xi)) is the contribution to the deviance at value yi when \u02c6\u03bc\\i(xi) is the fitted\nvalue. for a normally distributed y one obtains the usual squared error, d(yi, \u02c6yi\\i) = (yi \u2212\n\u02c6\u03bc\\i(xi))2, and for binary distributions one obtains the likelihood value\n\n(cid:29) \u2212 log(\u02c6\u03c0\\i(xi))\n\n\u2212 log(1 \u2212 \u02c6\u03c0\\i(xi))\n\nyi = 1\nyi = 0.\n\nd(yi, \u02c6\u03c0\\i(xi)) = \u2212 log(1 \u2212 |yi \u2212 \u02c6\u03c0\\i(xi)|) =\n\nfor non-normal distributions the deviance is more appropriate than the simple squared error\nbecause it measures the discrepancy between the data and the fit by taking the underlying dis-\ntribution into account.\n\nsimple cross-validation is often replaced by generalized cross-validation. with d(\u03bb) =\n(yi,xi)\u2208s(yi, \u02c6\u03bc(xi)) denoting the deviance for the smoothing parameter \u03bb, the criterion to be\n\n(cid:14)\n\nminimized is\n\ngcv(\u03bb) =\n\nd(\u03bb)/n\n\n(1 \u2212 tr(h)/n)2 ,\n\nwhere tr(h) is the effective degrees of freedom, determined by the hat matrix h (see next\nsection). an information-based criterion is the extension of aic from section 4.5, which has\nthe form\n\naic(\u03bb) = d(\u03bb) + 2tr(h)\u03c6,\n\nwhere \u03c6 is the dispersion parameter, which is equal to 1 for binary data. for more details see\nhastie and tibshirani (1990) and wood (2006a).\n\nlikelihood-based approaches\nlikelihood-based approaches to the selection of the smoothing parameter use the connection to\nmixed models. an excellent introduction to spline-based smoothing with mixed model repre-\nsentations was given by wand (2003). an extensive treatment is found in the book of ruppert\net al. (2003).\n\nlet us consider the truncated power series as a basis function. then the predictor may\nbe spilt into components by \u03b7(xi) = xt\ni \u03b3 + zt\ni ), \u03b3t =\ni b, where xt\n+, . . . , (xi \u2212 \u03c4ms)k\n(\u03b20, . . . , \u03b2k), zt\n+), bt = (\u03b2k+1, . . . , \u03b2k+ms). in matrix\nform one has \u03b7 = x\u03b3 + zb, where x has rows xt\ni and z has rows zt\ni . only the parameters\ncollected in b are penalized by use of the penalty term (\u03bb/2)j = (\u03bb/2)(cid:16)b(cid:16) = (\u03bb/2)\u03b2t kt \u03b2,\nwhere \u03b2t = (\u03b3t , bt ) and k t is a block-diagonal matrix:\n\ni = ((xi \u2212 \u03c41)k\n\ni = (1, xi, . . . , xk\n\n(cid:25)\n\n(cid:26)\n\n0 0\n0 i k\u00d7k\n\nk t =\n\n.\n\n(10.11)\nthe penalized log-likelihood lp(\u03b2) = l(\u03b2) \u2212 (\u03bb/2)\u03b2t kt \u03b2 has the same form as the (ap-\nproximative) log-likelihood obtained in chapter 14 for the mixed model lp = log(f(y|b, \u03b3))\u2212\n(1/2)bt q\n\n\u22121\nb b (see equation (14.31) in chapter 14).\n\nfor the mixed model considered in chapter 14, the parameters in the decomposition \u03b7 =\nx\u03b3 + zb are considered as fixed effects and random effects, respectively. while \u03b3 is a fixed\nparameter, b is a random effect, for which normal distribution, b \u223c n(0, qb), is assumed.\ntherefore, maximization of the penalized likelihood corresponds to fitting a mixed model with\ncovariance matrix cov(b) = i k\u00d7k/\u03bb. thus the smoothing parameter \u03bb corresponds to the\nb of iid random effects \u03b2k+i, i = 1, . . . , ms. large variances correspond\ninverse variance 1/\u03c32\nto small values of \u03bb and therefore weak smoothing, whereas small variances correspond to\nstrong smoothing. the mixed model approach may be used to select the smoothing parameter\nby computing restricted ml (reml) estimates estimates of \u03c32\nb,reml (see\n\nb to obtain \u02c6\u03bbreml = 1/\u02c6\u03c32\n\n "}, {"Page_number": 297, "text": "10.2. non-parametric regression with multiple covariates\n\n285\n\nb,reml.\n\nchapter 14) if \u03c6 = 1. when \u03c6 is not given it has also to be estimated, and one obtains \u02c6\u03bbreml =\n\u02c6\u03c6/\u02c6\u03c32\nin the case of glms, the mixed model approach to the selection of smoothing\nparameters relies on the assumption that the approximation to the log-likelihood, which uses\nthe laplace approximation, holds. although the approximation can be bad for typical mixed\nmodels with few observations in one cluster, for the selection of smoothing parameters the\nscenario is different. clusters refer to spline functions, and with increasing sample size the\nnumber of observations for each spline function (cluster) increases. in that scenario the laplace\napproximation works (for details see kauermann et al., 2009). a method that allows for locally\nadaptive smoothing was proposed by krivobokova, crainiceanu, and kauermann (2008).\n\n10.2 non-parametric regression with multiple covariates\nin generalized linear models it is assumed that the mean \u03bc(x) = e(y|x) is determined by\n\u03bc(x) = h(\u03b7(x)) or g(\u03bc(x)) = \u03b7(x) with the predictor having the linear form \u03b7(x) = xt \u03b2.\nin a non-parametric regression the link between the mean and the predictor is still determined\nby the link function g, but the functional form of \u03b7(x) is much more flexible. in the most\nflexible case \u03b7(x) is estimated non-parametrically without imposing further constraints. the\nmethods for one-dimensional predictors, which were considered in the proceeding sections,\ncan be extended to that case. we will start with the expansion in basis functions and then\nconsider localizing techniques. more structured modeling of predictors will be considered in\nthe following sections.\n\n10.2.1 expansion in basis functions\nthe linear basis functions approach now assumes that \u03b7(x) can be approximated by\n\nm(cid:7)\n\n\u03b7(x) =\n\n\u03b2j\u03c6j(x),\n\nj=1\n\nwhere \u03c6j(.) are fixed basis functions with p-dimensional argument x. basis functions in higher\ndimensions may be generated from one-dimensional basis functions. for simplicity, let us\nconsider the two-dimensional case. if \u03c61j, j = 1, . . . , m1, is a basis for representing functions\nof x1, and \u03c62l, l = 1, . . . , m2, is a basis for x2, the tensor-product basis is defined by\n\n\u03c6jl(x1, x2) = \u03c61j(x1)\u03c62l(x2).\n\n(10.12)\n\nfigure 10.5 illustrates two-dimensional basis functions that are built as products of cubic b-\nsplines. the design matrix for the tensor-product basis easily derives from the matrices for\nthe marginal smooths. let \u03c61 = (\u03c61j(xi1))i,j denote the n \u00d7 m1 design matrix for the first\nvariable and \u03c62 = (\u03c62l(xi2))i,l denote the n\u00d7 m2 design matrix for the second variable. then,\nby ordering the \u03b2jl\u2019s in \u03b2t = (\u03b2t\nj. = (\u03b2j1, . . . , \u03b2jm2), one obtains the design\n\u2297 \u03c62, where \u2297 is the usual kronecker product and \u03b7(xi) corresponds to the ith\nmatrix \u03c6 = \u03c61\nentry in \u03c6\u03b2. the function \u03b7(x) is then represented by\n\n1., . . . , \u03b2t\n\nm1.), \u03b2t\n\n(cid:7)\n\n\u03b7(x) =\n\n\u03b2jl\u03c6jl(x).\n\nalternatively, basis functions may be derived by selecting a set of knots \u03c4 1, . . . , \u03c4 m \u2208 r2 and\ndefining\n\n\u03c6j(x) = \u03c6((cid:16)x \u2212 \u03c4 j(cid:16)/\u03b3),\n\nj,l\n\n "}, {"Page_number": 298, "text": "286\n\nchapter 10. semi- and non-parametric generalized regression\n\nwhere \u03c6 is a one-dimensional basis function, for example, a radial basis function, and \u03b3 deter-\nmines the spread of the function.\n\nsince the representation is linear in known basis functions, it is straightforward to apply\nmaximum likelihood techniques for obtaining estimates of coefficients. it is somewhat harder\nto formulate appropriate penalty terms, however. before specifying the form of the penalty let\nus consider a general functional penalty approach.\n\nfigure 10.5: two-dimensional b-splines of degree 3. one single basis function (left),\nfunctions built from two b-splines in the first dimension, and three b-splines in the second\ndimension (right).\n\n10.2.2 smoothing splines\none-dimensional smoothing splines seek to find a regularized function that in some sense is\nclose to the data. the concept generalizes to higher dimensions. for data (yi, xi), xi \u2208 r2,\nand \u03b7i = \u03b7(xi) one considers the criterion\n\nn(cid:7)\n\n(yi \u2212 \u03b7i)2 + \u03bbj(\u03b7),\n\n(10.13)\n\nwhere\n\n) )\n\nj(\u03b7) =\n\ni=1\n\n( \u22022\u03b7\n\u2202x2\n1\n\n)2 + 2( \u22022\u03b7\n\u2202x1\u2202x2\n\n)2 + ( \u22022\u03b7\n\u2202x2\n2\n\n)2dx1dx2\n\nis a penalty functional for stabilizing a function \u03b7 in r2. the smooth two-dimensional surface\nresulting from minimizing (10.13) is known as a thin-plate spline. the penalty j(\u03b7) is a rough-\nness penalty that penalizes the wiggliness of the function by using squared derivatives. the\ngeneral form for p dimensions is found, for example, in duchon (1977).\nthe effect of \u03bb is to find a compromise between faith to the data and smoothness. for\n\u03bb \u2192 \u221e the solution approaches a plane, and for \u03bb \u2192 \u221e one obtains an interpolating function\nof the data. it is remarkable that while the criterion is defined over an infinitesimal space, the\nsolution has a finite-dimensional representation:\n\n\u03b7(x) = \u03b20 + \u03b21x1 + \u03b22x2 +\n\n\u02dc\u03b2j\u03c6j(x),\n\nn(cid:7)\n\nj=1\n\n "}, {"Page_number": 299, "text": "10.2. non-parametric regression with multiple covariates\n\n287\nwhere \u03c6j(x) = \u03c6((cid:16)x\u2212xj(cid:16)), \u03c6(z) = z2 log(z2) are specific radial basis functions. the vectors\nof coefficients, \u03b2 and \u02dc\u03b2, have to be estimated, subject to the linear constraints that t t \u02dc\u03b2 = 0,\nwhere tij = xj\u22121\ncorresponds to the design of the polynomial term. the polynomial term\nrepresents the space of functions for which the penalty is zero; penalized estimates are shrunk\ntoward the corresponding plane. the criterion to be minimized becomes\n\ni\n\n(cid:16)y \u2212 p \u02dc\u03b2 \u2212 t \u03b2(cid:16)2 + \u03bb\u02dc\u03b2p \u02dc\u03b2\n\nsubject to t t \u02dc\u03b2 = 0, where the penalty matrix is p = (pij), pij = \u03c6j(xi).\n\nthe problem with thin splines is the high computational burden. therefore, one truncates\nthe space of basis functions \u03c6j while leaving the polynomial term. the properties of the result-\ning thin plate regression splines are nicely summarized in wood (2006a), pp. 157ff. for more\ndetails on thin plate splines see wahba (1990) and green and silverman (1994). a general\ntreatment of regularization methods connected to reproducing hilbert spaces is found in wahba\n(1990), and a brief introduction was given by hastie et al. (2001).\n\n10.2.3 penalized regression splines\nthe finite-dimensional representation of smoothing splines has as many parameters as obser-\nvations. therefore, the use of regression splines together with appropriate smoothing offers\na low-dimensional alternative with a reduced computational burden. the choices of the basis\nfunctions and the basis dimension may be seen as part of the model specification. in partic-\nular, the basis dimension should be chosen not too small in order to provide flexibility of the\nfit. the number of basis functions itself is not so crucial because the smoothing parameters\nconstrain the actual degrees of freedom. a knot-based approximation to smoothing splines in\ntwo dimensions is obtained by specifying knots \u03c4 1, . . . , \u03c4 m \u2208 r2 and using the approximation\n\n\u03b7(x) = \u03b20 + \u03b21x1 + \u03b22x2 +\n\n\u02dc\u03b2j\u03c6((cid:16)x \u2212 \u03c4 j(cid:16)),\n\nm(cid:7)\n\nj=1\n\nt\n\nwhere the \u03c6\u2019s are the radial basis functions, \u03c6(z) = z2 log(z2). the fitting uses the penalty term\np \u02dc\u03b2, where p = (\u03c6((cid:16)\u03c4 i \u2212 \u03c4 j(cid:16)))ij. the radial basis functions approach treats smoothness\n\u03bb\u02dc\u03b2\nin all directions equally. the resulting fit is invariant to rotation. what comes as an advantage in\nbivariate geographical applications, where the variables correspond to longitude and latitude,\nmight be problematic if the two variables have quite different scalings. an alternative is to\nuse tensor-product basis functions \u03c6, which are given by (10.12) for two dimensions. the\ncorresponding predictor is\n\n\u03b7(x) =\n\n\u03b2jl\u03c61j(x1)\u03c62l(x2).\n\nif the basis functions \u03c61j and \u03c62l are linked to equally spaced knots (m1 for x1 and m2 for x2),\none can use the difference penalty in two dimensions:\n\n\u03bb1\u03b2p 1\u03b2 + \u03bb2\u03b2p 2\u03b2 = \u03bb1\n\n(\u03b2jl \u2212 \u03b2j\u22121,l)2 + \u03bb2\n\nl=1\n\nj=2\n\nj=1\n\nl=2\n\nthe penalty controls the variation of parameters \u03b2jl in both directions; the first term controls\nthe smoothness in the x1-direction and the second in the x2-direction. more generally, one\nmight use differences of order d. let \u03b41 denote the age difference operator for the first index.\nthen the first-order difference \u03b41\u03b2jl = (\u03b2jl \u2212 \u03b2j\u22121,l) generalizes to \u03b42\n1\u03b2j2 = \u03b41(\u03b41\u03b2jl) and\n\nm1(cid:7)\n\nm2(cid:7)\n(\u03b2jl \u2212 \u03b2j,l\u22121)2.\n\n(cid:7)\n\nj,l\n\nm2(cid:7)\n\nm1(cid:7)\n\n "}, {"Page_number": 300, "text": "288\n1\u03b2jl = \u03b4d\u22121\n\u03b4d\nof the form\n\nchapter 10. semi- and non-parametric generalized regression\n\n1 \u03b41\u03b2jl. by defining the corresponding difference matrices one obtains a penalty\n\n\u03bb\u03b2t p 1\u03b2 + \u03bb2\u03b2t p 2\u03b2 = \u03bb1\u03b2t (dt d \u2297 i m2)\u03b2,\nm1.), \u03b2t\n\n1., . . . , \u03b2t\n\nwhere \u03b2 = (\u03b2t\nj. = (\u03b2j1, . . . , \u03b2jm2), and d represents the difference matrices.\npenalties of this type with product b-splines were used, for example, by currie et al. (2004)\nwhen smoothing two-dimensional mortality ranks; see also eilers and marx (2003) for an ap-\nplication to signal regression.\n\n10.2.4 local estimation in two dimensions\nlocalizing techniques generalize easily to two and higher dimensions. in the two-dimensional\ni = (xi1, xi2) denote the ith observation.\ncase let xt = (x1, x2) denote the target value and xt\none can fit locally the linear model\n\n\u03b7i = \u03b7(xi) = \u02dcxt\n\ni \u03b2,\n\ni = (1, (xi1\u2212x1), (xi2\u2212x2), (xi1\u2212x1)(xi2\u2212x2) and \u03b2t = (\u03b20, \u03b21, \u03b22, \u03b212). when\nwhere \u02dcxt\nfitting the model by maximizing the local likelihood one uses vector-valued weights determined\nby kernels. the candidates are product kernels,\n\nw\u03bb(x, xi) \u221d p(cid:15)\n\nk( xij \u2212 xj\n\n\u03bbj\n\n),\n\nand distance-based kernels,\n\nj=1\n\nw\u03bb(x, xi) \u221d k(\n\n(cid:16)xij \u2212 xj(cid:16)\n\n\u03bb\n\n).\n\nestimates may be obtained by local fisher scoring as given in (10.9).\n\nalthough smoothing that is free of any structured assumptions may be extended to more\nthan two dimensions, the computational burden increases, and, more severe, it is hard to visu-\nalize the resulting fit. bivariate smoothing yields a surface estimate that is easily visualized and\ncorresponds to the important case of a two-variable interaction, that is, more specifically, the\ninteraction between two continuous variables, effecting on a response variable.\n\nexample 10.3: exposure to dust (smokers)\nin example 4.6 the effects of dust on bronchitis were investigated. the observed covariates were mean\ndust concentration at working place in mg/m3 (dust), duration of exposure in years (years), and smoking\n(1: yes; 0: no). the binary response refers to the presence of bronchitis (1: present; 0: not present).\nfigure 10.6 shows the fit of a logit model including the effects of dust concentration and years without\nfurther structuring for smokers. the method used is penalized splines by using default values of the r\npackage mgcv. it is seen that both covariates seem to effect on the probability of bronchitis. when using\nthe more structured generalized additive model that assumes the linear predictor\n\n\u03b7 = s(dust) + s(years)\n\n(see section 10.3.1), the effects of the two variables are separated. figure 10.7 shows the fitted smooth\neffects.\n\n "}, {"Page_number": 301, "text": "10.3. structured additive regression\n\n289\n\ns\n\n(\n\nd\n\nu\n\ns\n\nt\n\n,\n\ny\n\ne\n\na\n\nr\n\ns\n\n,\n\n6\n\n.\n\n8\n\n9\n\n)\n\nars\nye\n\ndust\n\nfigure 10.6: effect of concentration of dust and years of employment on probability of\nbronchitis (smokers).\n\n)\n5\n2\n\n.\n\n4\n\n,\nt\ns\nu\nd\n(\ns\n\n2\n\n1\n\n0\n\n1\n\u2212\n\n2\n\u2212\n\n3\n\u2212\n\n)\n6\n4\n\n.\n\n3\n,\ns\nr\na\ne\ny\n(\ns\n\n2\n\n1\n\n0\n\n1\n\u2212\n\n2\n\u2212\n\n3\n\u2212\n\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n20\n\n30\n\n40\n\n50\n\ndust\n\nyears\n\nfigure 10.7: generalized additive model including the effects of concentration of dust\nand years of employment on probability of bronchitis (smokers).\n\n10.3 structured additive regression\n\nwith a moderate to large number of explanatory variables it is often useful to assume some weak\nstructure in the predictor. in a structured additive regression (star) one keeps the additive form\nbut lets the components be determined in a much more flexible way than in a linear regression.\ntherefore one assumes that the predictor has the form\n\n\u03b7(x) = f(1)(v1) + . . . f(m)(vm),\n\nwith unspecified unknown functions f(1)(.), . . . , f(m)(.) and the vj denoting generic covariates\nof different types and dimensions. in the simplest form vj represents one covariate xj, yielding\ngeneralized additive models. in the following, we will start with that model and consider the\ngeneral model at the end of the section.\n\n "}, {"Page_number": 302, "text": "290\n\nchapter 10. semi- and non-parametric generalized regression\n\n10.3.1 generalized additive models\nin generalized additive models (gams) it is assumed that the underlying predictor \u03b7(x) can be\nrepresented in the additive form\n\n\u03b7(x) = \u03b20 + f(1)(x1) + \u00b7\u00b7\u00b7 + f(p)(xp),\n\n(10.14)\n\nwhere the f(j)(.) are unspecified functions.\nin the following we consider fitting gams by\npenalized regression splines, which is a straightforward extension of the penalized fitting of\none-dimensional models, and fitting procedures that are based on backfitting. various alterna-\ntive procedures have been proposed; for references see section 10.5.\n\npenalized regression splines\none way to estimate a gam is to set it up as a penalized generalized linear model. to this end\none specifies a basis for each smooth function and defines what is meant by the smoothness of\na function by specifying a corresponding penalty. let the functions be represented by\n\nmj(cid:7)\n\nf(j)(xj) =\n\n\u03b2js\u03c6js(xj),\n\nwhere \u03c6j1, . . . \u03c6jmj denote the basis functions for the jth variable. for data (yi, xi), i =\n1, . . . , n, one obtains the linear predictor\n\ns=1\n\n\u03b7i = \u03b7(xi) = \u03b20 + \u03c6t\n\ni1\u03b21 + \u00b7\u00b7\u00b7 + \u03c6t\n\nip\u03b2p = \u03c6t\n\ni \u03b2,\n\nij = (\u03c6j1(xij) . . . \u03c6jmi(xij)) are the evaluations of the basis functions for the jth vari-\nwhere \u03c6t\nj = (\u03b2j1, . . . \u03b2jmj ) are the corresponding weights. by collecting evaluations\nable at xij and \u03b2t\nand parameters one obtains the linear predictor \u03c6t\ni \u03b2, which specifies the generalized linear\nmodel g(\u03bci) = \u03c6t\ni \u03b2. as in the unidimensional case, the basis should be sufficiently large\nto be able to approximate the unknown underlying functions. the resulting high-dimensional\nfitting problem typically needs penalization. the penalty term specifies what is meant by the\nsmoothness of the underlying function and has to be linked to the chosen basis. for example, if\none chooses b-splines with equally spaced knots, an appropriate penalty is given by penalizing\nthe squared differences between the adjacent basis functions:\n\nj(\u03b2) =\n\n(\u03b2j,s+1 \u2212 \u03b2j,s)2,\n\n(cid:7)\n\n\u03bbj\n\ns\n\np(cid:7)\n(cid:14)\n\nj=1\n\nwhich has the general form j(\u03b2) =\nj p j\u03b2j. if p j = kd from (10.7), one penalizes\nthe differences of dth order. it is most convenient to work with penalties that are given as a\nquadratic form in the coefficients, since penalized likelihood estimation becomes rather simple.\nmaximization of the penalized likelihood\n\nj \u03bbj\u03b2t\n\np(cid:7)\nuses the penalized score function sp(\u03b2) = s(\u03b2) \u2212(cid:14)\n(cid:14)\n\nlp(\u03b2) = l(\u03b2) \u2212 1\n2\n\nj=1\n\n\u03bbj\u03b2t\n\nj p j\u03b2j\n\np\nj=1 \u03bbjp j\u03b2j and estimates may be ob-\ntained by iterative fisher scoring by using the penalized fisher matrix f p(\u03b2) = f (\u03b2) +\n\nj \u03bbjp j. in matrix notation, the linear predictor \u03b7t = (\u03b71, . . . , \u03b7n) has the form\n\n\u03b7 = \u03b201 + \u03c6(1)\u03b21 + \u00b7\u00b7\u00b7 + \u03c6(p)\u03b2p = \u03c6\u03b2\n\n "}, {"Page_number": 303, "text": "10.3. structured additive regression\n\n291\n\nand the penalty matrix is \u02dck = blockdiag(\u03bb1p 1, . . . , \u03bbpp p). then approximate covariances\nare obtained by the sandwich matrix:\n\ncov(\u02c6\u03b2) \u2248 (f (\u02c6\u03b2) + \u02dck)\n\n\u22121f (\u02c6\u03b2)(f (\u02c6\u03b2) + \u02dck)\n\n\u22121.\n\n(10.15)\n\n(comare section 10.1.3).\n\nin the preceding presentation it has been ignored that the functions in (10.14) are not identi-\nfiable because the shifting of functions can be compensated by the shifting of other functions or\nthe intercept. for example, the linear predictor is unchanged if one uses the intercept \u03b20 + c and\nthe function f(1)(x1)\u2212 c for x1. to obtain an identifiable model one has to center the functions.\na suitable constraint is that the evaluations at observations sum up to zero:\n\nn(cid:7)\n\nf(j)(xij) = 0, j = 1, . . . , p,\n\ni=1\n\nor, equivalently, 1t \u03c6\u03b2 = 0. since this is a linear constraint on the parameters, it is easily han-\ndled by reparameterization (see appendix c). one can find a matrix z that satisfies 1t \u03c6z = 0\nand \u03b2 = z\u03b2u, where the \u03b2u\u2019s are unconstrained parameters. the corresponding penalty term\ntakes the form \u03b2t zp z\u03b2, where p is the block-diagonal matrix with entries p 1, . . . p p.\n\nan advantage of the penalized regression splines approach is that it also works in more\n\ngeneral additive models. in the partially linear model,\n\n\u03b7(x, \u03b3) = \u03b20 + f(1)(x1) + \u00b7\u00b7\u00b7 + f(p)(xp) + zt \u03b3,\n\n(10.16)\n\nin addition to the additive term, a linear term zt \u03b3 is included that specifies the linear effect of\nadditional covariates z. in particular, when some of the covariates are categorical, the simple\nadditive model has to be extended, since the effect of a binary covariate is determined by one\nparameter, not by a function (when no interaction effects are assumed). therefore, within an\nadditive approach, categorical variables are included by dummy variables in the form of a linear\nterm. for the partially linear model the linear predictor has the form\n\n\u03b7i = \u03b7(xi, \u03b3) = \u03c6t\n\ni \u03b2 + zt\n\ni \u03b3.\n\nthe penalized log-likelihood is the same as in the additive model, but the parameter to be\nestimated is (\u03b2, \u03b3). in particular, the penalty term is the same since only the weights \u03b2 have to\nbe penalized.\n\nit should be noted that for gams the choice of the link function might be relevant. when\none fits the model \u03bc(x) = h(\u03b7(x)) with a one-dimensional predictor, there is not much to\nchoose when \u03b7(x) is unspecified. but the link function helps to confine the mean \u03bc(x) within\nthe admissible but otherwise determines only the scaling of the predictor. this is different for an\nadditively structured predictor. when additivity holds for a specific link function, for example,\nthe logit transformation, it does not hold for other link functions. therefore, it might be useful\nto compare the fits of gams with alternative link functions.\n\nselection of smoothing parameters\nwhen fitting a generalized additive model one has to select the smoothing parameters \u03bb1, . . . , \u03bbp,\nwhere \u03bbj determines the amount of smoothing for the jth component. therefore, one has\nto select smoothing parameters from a p-dimensional space. although cross-validation is\nstraightforward, it is limited to the case of small p, since the selection of smoothing param-\neters on a grid implies heavy computational effort. an alternative is to rely on likelihood-\nbased approaches that make use of the mixed model methodology. when using truncated\n\n "}, {"Page_number": 304, "text": "292\n\nchapter 10. semi- and non-parametric generalized regression\n\npower series, the penalized likelihood becomes lp(\u03b2) = l(\u03b2) \u2212(cid:14)\n\nj kt \u03b2j with\nk t from (10.11). in the same way as for univariate smoothing problems, the form of the pe-\nnalized likelihood is the same as the (approximative) likelihood obtained for the mixed model\nlp = logf(y|b, \u03b3) \u2212 (1/2)bt q\n\u22121\nb b, but now q is a block-diagonal matrix with the blocks\ncorresponding to the components to be smoothed. reml estimates may be used to obtain the\nsmoothing parameters.\n\nj(\u03bbj/2)\u03b2t\n\ndistributional results\na general reliable theory for the smooth components of a gam does not seem to be available,\nalthough some results were given by hastie and tibshirani (1990), and wood (2006b, 2006a).\nin the following we sketch an approximate test procedure following wood (2006a). let \u03b2j\ndenote the coefficients for a single smooth term. then, if \u03b2j = 0, the expectation e(\u02c6\u03b2j)\nwill be approximately zero. as an approximate covariance matrix of the estimate one uses the\ncorresponding submatrix of (10.15), denoted by v \u03b2j . if v \u03b2j is of full rank, p-values for testing\nthe null hypothesis \u03b2j = 0 are obtained by using the fact that \u03b2t\n\u03b2j is approximately\n\u03c72(d)-distributed, where d is the dimension of \u03b2j. if v \u03b2j has rank r < d, one uses the test\nstatistic \u03b2t\nis the rank r pseudo-inverse of v \u03b2j . testing relies on the\napproximate \u03c72(r)-distribution. if an unknown scale parameter \u03c6 is involved, one can use an\n\u03b2j/r)/( \u02c6\u03c6/(n \u2212 edf)), where\napproximate f-distribution f(r, edf ) for the test statistic (\u03b2t\nedf is the estimated degrees of freedom of the model.\n\n\u03b2j, where v\n\n\u22121\n\u03b2j\n\n\u2212\n\u03b2j\n\n\u2212\n\u03b2j\n\n\u2212\n\u03b2j\n\nj v\n\nj v\n\nj v\n\nthe resulting p-values rely on several approximations. in particular, the estimation of the\nsmoothing parameters is not sufficiently accounted for. therefore, when smoothing parameters\nare estimated they tend to be smaller than they should be, and thus smooth terms are found\nrelevant that are not (for more details see wood, 2006a). alternative testing procedures can be\nbased on the mixed model approach to the fitting of gams; see, for example, ruppert et al.\n(2003) or the bayesian methods outlined in wood (2006a).\n\nsimultaneous selection of variables and amount of smoothing\nan alternative approach to the fitting of generalized additive models that implicitly selects the\nrelevant variables and the amount of smoothing utilizes boosting techniques. boosting concepts\nwere already considered in section 6.3 and are treated extensively in section 15.5.3; here we\nonly briefly consider the extension of the variable selection method from section 6.3. the main\nidea of componentwise boosting is greedy forward stagewise additive modeling. for the fitting\nof additive models, this means that, in an iterative process within one stage, just one component\nof the additive term is selected and refitted without adjusting the other components. thus,\nwithin one step the model\n\n\u03bci = h(\u02c6\u03b7(l)(xi) + xt\n\nij\u03b3)\n\nis fitted, where \u02c6\u03b7(l)(xi) is the fit from the previous step (treated as an offset) and xij is the\nvector of basis functions for the jth component. therefore, only the jth component is refitted.\nwhen using one-step fisher scoring, the fit has the form\n\u22121\u03c6t\n\n\u22121(y \u2212 \u02c6\u03bc),\n\n\u02c6w \u02c6d\n\n\u02c6\u03b7j,new = \u03c6j(\u03c6t\n\n\u02c6w \u03c6j + \u03bbk)\n\nj\n\nj\n\nwhere \u03c6j is the design matrix of the jth component and \u02c6w , \u02c6d, \u02c6\u03bc are evaluated at the values\nfrom the previous step, \u02c6\u03b7t = (\u02c6\u03b7(l)(x1), . . . , \u02c6\u03b7(l)(xn). it should be noted that the fisher step\nstarts with \u03b3 = 0, since the previous fits are contained in the off-set. boosting becomes efficient\n\n "}, {"Page_number": 305, "text": "10.3. structured additive regression\n\n293\n\nby using so-called weak learners, which means that in each step one tries to improve the fit\nonly slightly by using large \u03bb. therefore, \u03bb is not an optimally chosen smoothing parameter\nbut has to be chosen simply large. the different amounts of smoothing for the components are\nobtained be updating only that component that gains the most within one step of the algorithm;\ncomplex functions are updated more often than simple functions. in addition, the selection of\ncomponents to update implies variable selection for the whole procedure. the actual tuning of\nthe fitting procedure lies in the stopping criterion. when the iterative procedure is stopped based\non cross-validation or an information-based criterion only variables are included that have been\nselected previously.\nin vectorized form, the fitting of a generalized model means determining of the additive\nlinear predictor \u02c6\u03b7 = f 1 + \u00b7\u00b7\u00b7 + f p, where \u02c6\u03b7t = (\u02c6\u03b71, . . . , \u02c6\u03b7n) is the fitted predictor and\nj = (f1j, . . . , fnj), fij = f(j)(xij) is the fitted vector of the jth variable at observation\nf t\nx1j, . . . , xnj. with dev(\u02c6\u03b7) denoting the deviance when \u02c6\u03b7 is the fitted predictor, the fitting step\nincluding variable selection may given in the following form (for details see tutz and binder,\n2006).\n\ngamboost with penalized regression splines\n\nfor l = 0, 1, . . . :\n\n1. estimation step: for s = 1, . . . , p compute\n\n\u02c6f s,new = \u03c6j(\u03c6t\n\nj\n\n\u02c6w \u03c6 + \u03bbk)\n\n\u22121\u03c6t\n\nj\n\n\u02c6w \u02c6d\n\n\u22121(y \u2212 \u02c6\u03bc),\n\n(10.17)\n\nwhere \u02c6w , \u02c6d, \u02c6\u03bc are evaluated at \u02c6\u03b7t = (\u02c6\u03b7(l)(x1), . . . , \u02c6\u03b7(l)(xn)).\n\n2. selection step: set f s = f (l)\n\ns + f s,new yielding \u02c6\u03b7s,new.\ncompute j = arg maxs{dev(\u02c6\u03b7(l))} \u2212 dev(\u02c6\u03b7s,new)).\n\n3. update: set f (l+1)\n\nj\n\n= f (l)\n\nj + f j,new.\n\nvariable selection for gams by removing a smooth term from the predictor can also be\nbased on the use of properly chosen penalty terms. the smoothing penalty in the penalized\nlikelihood has the general form \u03bbj\u03b2t\nj p j\u03b2j, with p j denoting the smoothing matrix for the\njth variable. the penalty generates a smooth function fj(.) but does not select variables. even\nin the limiting case \u03bbj \u2192 \u221e, the function component in the null space of the penalty is not\npenalized and therefore not selected. for example, if one uses natural cubic splines, the linear\npart of the function is not penalized and will remain in the predictor. therefore, an appropriate\npenalty has to separate the function component in the null space, which is not penalized from\nthe shrinking component of the function. one can consider the eigenvalue decomposition p j =\nu j\u03bbju t\nj with eigenvector matrix u j and diagonal eigenvalue matrix \u03bbj. since part of the\nfunction is not penalized, \u03bbj contains zero eigenvalues. marra and wood (2011) included\nt\nan extra term into the penalty and obtained \u03bbj\u03b2t\nj ,\nwith \u02dcu j denoting the matrix of eigenvectors corresponding to zero eigenvalues. the smoothing\nparameter \u02dc\u03bbj can shrink the function components in the null space of the penalty to zero. marra\nand wood also gave an alternative version that avoids the double penalty. alternatively, avalos\net al.\n(2007) included a lasso term for the component in the null space. an approach that\nmodifies the backfitting procedure considered in the next section to include variable selection\nwas given by belitz and lang (2008).\n\n\u02dcp j\u03b2j, where \u02dcp j = \u02dcu j \u02dcu\n\nj p j\u03b2j + \u02dc\u03bbj\u03b2t\n\nj\n\n "}, {"Page_number": 306, "text": "294\n\nchapter 10. semi- and non-parametric generalized regression\n\nunivariate smoothers and the backfitting algorithm\nwhen hastie and tibshirani (1990) propagated additive modeling, the backfitting algorithm\nwas the most widely used algorithm for the fitting of additive models. the basic idea behind\nthe algorithm is to fit one component at a time based on the present partial residuals. therefore,\nonly univariate smoothers are needed and all kinds of smoothers can be used.\n\nfor a description of the algorithm let us start with the additive model (where the link func-\ntion is the identity function) and then consider the more general case. for data (yi, xi), i =\n1, . . . , n, with xt\n\ni = (xi1, . . . , xip), the additive model has the form\n\u03bci = \u03b7i = \u03b20 + f(1)(xi1) + \u00b7\u00b7\u00b7 + f(p)(xip).\n\nin matrix notation one obtains\n\ny = \u03b201 + f 1 + \u00b7\u00b7\u00b7 + f p + \u03b5,\n\nwhere yt = (y1, . . . , yn), f t\n\u03b5t = (\u03b51, . . . , \u03b5n) is a noise vector. by omitting the noise vector one has approximatively\n\nj = (f (j)(x1j), . . . , f (j)(xnj)) is the vector of evaluations, and\n\n(cid:7)\n\n\u2248 y \u2212\n\nf j\n\nf i.\n\ntherefore, when f i, i (cid:8)= j, have been estimated, y \u2212(cid:14)\npartial residuals. now one may use a univariate smoother treating y \u2212(cid:14)\n\n\u02c6f i may be seen as a vector of\n\u02c6f i as the vector\nof responses that depends smoothly on the jth variable. for a simple linear smoother with\nsmoother matrix sj one obtains in the sth cycle the update\n\ni(cid:5)=j\n\ni(cid:5)=j\n\ni(cid:5)=j\n\n\u239b\n\u239dy \u2212\n\n(cid:7)\n\ni(cid:5)=j\n\n\u239e\n\u23a0 .\n\n(s\u22121)\ni\n\n\u02c6f\n\n\u02c6f\n\n(s)\n\nj = sj\n\n(10.18)\n\nwithin one cycle of the iterative procedure all of the estimates of f 1, . . . , f p are updated.\nthere are two modifications to this basic procedure. the first concerns the residuals within one\niteration cycle. when estimating f j, j > 1, within cycle s the estimates \u02c6f\n(s)\nj\u22121 are\nalready computed. therefore, instead of (10.18) one uses the update\n\n1 , . . . , \u02c6f\n\n(s)\n\n\u239b\n\u239dy \u2212\n\n(cid:7)\n\n(cid:7)\n\n\u239e\n\u23a0 .\n\nf (s)\nj = sj\n\n\u2212\n\nf (s)\n\ni\n\nf (s\u22121)\n\ni\n\ni<j\n\ni>j\n\n(cid:14)\n\nthe second modification concerns the constant term, \u03b20, which could be incorporated within\n(cid:3)\nany of the functions. therefore, one first fits a vector of constants \u02c6f\n0 = (\u00afy, . . . , \u00afy), where\n\u00afy =\n\ni yi/n.\n\nbackfitting algorithm\n\n(1) initialize for s = 0 by f (0)\n\nj = (0, . . . , 0), j = 1, . . . , p.\n\n(2)\n\na. increase s by one.\n\n "}, {"Page_number": 307, "text": "10.3. structured additive regression\n\n295\n\nb. compute for j = 1, . . . , p the update\n\n\u239b\n\u239dy \u2212 \u02c6f 0\n\n(cid:7)\n\n\u2212\n\n\u2212\n\n\u02c6f\n\n(s)\ni\n\n(cid:7)\n\ni<j\n\ni>j\n\n\u239e\n\u23a0 .\n\n(s\u22121)\ni\n\n\u02c6f\n\n\u02c6f\n\n(s)\n\nj = sj\n\nc. stop when \u02c6f\n\n(s)\n\np do not change when compared to \u02c6f\n\n(s\u22121)\n1\n\n, . . . , \u02c6f\n\n(s\u22121)\np\n\n.\n\nthe backfitting or gauss-seidel algorithm may be shown to solve the system of equations\n\n(s)\n\n1 , . . . , \u02c6f\n\u239b\n\u239c\u239c\u239c\u239d i\n\ns1\ns2\ni\n...\n...\nsp sp\n\n\u239e\n\u239f\u239f\u239f\u23a0\n\n\u239b\n\u239c\u239c\u239c\u239df 1\n\nf 2\n...\nf p\n\n. . . s1\n. . . s2\n...\ni\n\n. . .\n\n\u239e\n\u239f\u239f\u239f\u23a0 ,\n\n\u239b\n\u239c\u239c\u239c\u239ds1y\n\n\u239e\n\u239f\u239f\u239f\u23a0 =\n\u239b\n\u239dy \u2212\n\ns2y\n...\nspy\n\n(cid:7)\n\ni(cid:5)=j\n\n\u239e\n\u23a0 .\n\nsjf i\n\nwhich has in the jth row(cid:7)\n\nsjf i + f j = sjy or f j = sj\n\ni(cid:5)=j\n\n(10.19)\n\nwhen fitting generalized additive models one has to take the link function and the form of\nthe response distribution into account. let the penalized likelihood that is to be maximized be\ngiven in matrix notation as\n\nlp(f 1, . . . , f p) = l(f 1, . . . , f p) \u2212 1\n2\n\n\u03bbjf t\n\nj kjf j,\n\n(10.20)\n\n(cid:7)\n\nj\n\nwhere k j is a penalty matrix for the jth component. maximization of (10.20) may be based\non fisher scoring, which generates a new estimate \u02c6f\n(k) +\n{e(\u2202lp/\u2202f)}\u22121\u2202lp/\u2202f , \u02c6f\np ). in each iteration one has a system of equations\nof the form (10.19), which may be solved by an inner loop of fisher scoring steps. a sim-\nple form of the algorithm that uses linear smoothers to fit pseudo-observations is given in the\nfollowing.\n\nby computing \u02c6f\n\n(k+1) = \u02c6f\n\n1 , . . . , f t\n\nt = (f t\n\n(k+1)\n\nbackfitting algorithm with fisher scoring\n\n(1) initialize by setting \u02c6f\n\n(0)\n\n0 = (g(\u00afy), . . . , g(\u00afy))t \u02c6f\n\n(0)\n\nj = (0, . . . , 0)t , j = 1, . . . , p, s = 0.\n\n(2) for s = 0, 1, . . . :\n\n(i) compute the current pseudo-observations\n\nzi = \u03b7(s)\n\ni + yi \u2212 h(\u03b7(s)\n(cid:14)\n\nd(s)\ni\n\ni\n\n)\n\n,\n\n)/\u2202\u03b7 with \u03b7(s)\n\ni = \u02c6f\n\n(s)\n\n0 +\n\np\nj=1\n\n\u02c6f\n\n(s)\nj\n\nand the weights w(s)\n\ni =\n\ni = \u2202h(\u03b7(s)\n\ni\n\nwhere d(s)\n(d(s)\n\ni /\u03c3(s))2.\n\n "}, {"Page_number": 308, "text": "296\n\nchapter 10. semi- and non-parametric generalized regression\n\n(ii) inner backfitting loop for observations z1, . . . , zn:\n\n(0)\n\n(a) initialize \u02dcf\n(b) for j = 1, . . . , p compute\n\nj = \u02c6f\n\n(s)\nj\n\n, j = 1, . . . , p, \u02dcf\n\n(0)\n\n0 = (\u00afz, . . . , \u00afz)t , \u00afz =\n\n\u239b\n\u239dzi \u2212\n\n(cid:7)\n\n(cid:7)\n\n\u02dcf\n\n(0)\ni\n\n\u2212\n\n\u02dcf\n\n(1)\ni\n\ni<j\n\ni>j\n\n\u239e\n\u23a0 ,\n\n\u02dcf\n\n(1)\n\nj = sj\n\n(cid:14)\n\ni zi/n.\n\nwith the corresponding smoother matrix sj including the weights.\n(s+1)\nj\n\n, j = 1, . . . , p.\n\n= \u02dcf\n\n(1)\nj\n\n(iii) set \u02c6f\n\n(3) comparison of \u02c6f\n\n(s+1)\nj\n\nand \u02c6f\n\n(s)\nj\n\n, j = 1, . . . , p determines upon termination.\n\na careful derivation and discussion of properties of the backfitting algorithm was given by buja\net al. (1989). opsomer and ruppert (1997), opsomer (2000), and kauermann and opsomer\n(2004) derived further results and investigated asymptotic properties.\n\n5\n0\n\n.\n\n0\n0\n\n.\n\n.\n\n5\n0\n\u2212\n\n.\n\n0\n1\n\u2212\n\n.\n\n5\n1\n\u2212\n\n.\n\n0\n2\n\u2212\n\n5\n\n.\n\n2\n\u2212\n\n5\n0\n\n.\n\n0\n0\n\n.\n\n.\n\n5\n0\n\u2212\n\n.\n\n0\n1\n\u2212\n\n.\n\n5\n1\n\u2212\n\n.\n\n0\n2\n\u2212\n\n5\n\n.\n\n2\n\u2212\n\n20\n\n40\n\n60\n\n80\n\n5\n\n10\n\nage\n\n15\n\ndur\n\n20\n\nfigure 10.8: estimated effects of age and duration of education on number of children.\n\nexample 10.4: number of children\nin example 7.3 the number of children was modeled in dependence on the predictors age in years (age),\nduration of school education (dur), nationality (nation; 0: german; 1: otherwise), religion (answer cate-\ngories to \"god is the most important in man\"; 1: strongly agree,. . . ; 5: strongly disagree; 6: never thought\nabout it), university degree (univ; 0: no; 1: yes). the fitted model was a log-linear poisson model with\npolynomial terms. the partially additive model considered here has the predictor\n\n\u03b7 = s(age) + s(dur) + nation \u2217 \u03b2n + god2\n\n\u2217 \u03b2g2 + \u00b7\u00b7\u00b7 + god6\n\n\u2217 \u03b2g6 + univ \u2217 \u03b2u.\n\nthe curves in figure 10.8 differ from the curves with polynomial terms (figure 7.2). while the curves\nwith polynomial terms decrease for large values of age, the estimates assuming the smooth effects of age\nwithout being restricted to polynomials show a stable level above 40 years of age. parameter estimates for\nthe partially additive model are given in table 10.1.\n\n "}, {"Page_number": 309, "text": "10.3. structured additive regression\n\n297\n\ntable 10.1: parameter estimates for poisson model with number of children as response.\n\nestimate\n\nstd. error\n\nz-value\n\npr(>|z|)\n\n(intercept)\nnation\ngod2\ngod3\ngod4\ngod5\ngod6\nuniv1\n\ns(age)\ns(dur)\n\n0.42292\n0.08040\n\u22120.10819\n\u22120.14317\n\u22120.13138\n\u22120.04899\n\u22120.10644\n0.55647\nedf\n\n7.369\n2.322\n\n0.04970\n0.13876\n0.05914\n0.06784\n0.07092\n0.06703\n0.07517\n0.17130\nref.df\n\n8.289\n2.997\n\n8.510\n0.579\n\u22121.829\n\u22122.110\n\u22121.852\n\u22120.731\n\u22121.416\n3.249\nchi.sq\n\n172.71\n31.77\n\n0.0\n0.56232\n0.06736\n0.03482\n0.06396\n0.46481\n0.15675\n0.00116\np-value\n\n0.0\n0.0\n\nexample 10.5: addiction\nhere we consider again the addiction data (example 8.2). the response was in three categories (0 : addicts\nare weak-willed; 1: addiction is a disease; 2: both). two additive logit models were fitted, one comparing\ncategory {1} and category {0} and the other comparing category {2} and category {0}. in both models\nthe predictor has the form\n\n\u03b7 = s(age) + gender \u2217 \u03b2g + universitydegree \u2217 \u03b2u.\n\nthe resulting probabilities for the subpopulation with university degree are shown in figure 10.9.\nin\ncontrast to figure 8.6, which shows the fit of a parametric model with the quadratic effect of age, it is seen\nfrom figure 10.9 that the probability of category {0} remains almost constant above 30 years of age. the\nincrease seen in figure 8.6 seems to be caused by the restrictive quadratic modeling of age.\n\nmen with university degree\n\nwomen with university degree\n\n0\n.\n1\n\n8\n.\n0\n\n6\n0\n\n.\n\n4\n0\n\n.\n\n2\n0\n\n.\n\n0\n0\n\n.\n\n0\n.\n1\n\n8\n.\n0\n\n6\n0\n\n.\n\n4\n0\n\n.\n\n2\n0\n\n.\n\n0\n0\n\n.\n\n20\n\n40\n\n60\n\n80\n\nage\n\n20\n\n40\nageindex\n\n60\n\n80\n\nfigure 10.9: estimated probabilities for addiction data with smooth effect of age (cate-\ngory 0: solid line; category 1: dotted line; category 2: dashed line).\n\nin addition, a hierarchical model was fitted (compare example 8.6). the model comprises two binary\nmodels. in the first, category {2} was compared with {0, 1}; in the second, category {1} was compared\nwith category {0}, given that the response was in {0, 1}. in both models a predictor with a smooth effect\nof age was fitted. figure 10.10 shows the corresponding effects of age for both binary models. age seems\nto have an almost linear effect on the grouped binary response {2} against {0, 1}, but given a single cause\n\n "}, {"Page_number": 310, "text": "298\n\nchapter 10. semi- and non-parametric generalized regression\n\nit is assumed (either weak-willed or a disease) that the effect is non-linear with a strong increase in the\nbeginning and a rather stable effect above 25 years of age. tables 10.2 and 10.3 show the estimated effects.\n\n2\n\n1\n\n0\n\n1\n\u2212\n\n2\n\u2212\n\n1\n\n0\n\n1\n\u2212\n\n2\n\u2212\n\n3\n\u2212\n\n4\n\u2212\n\n20\n\n40\n\n60\n\n80\n\n20\n\n40\n\n60\n\n80\n\nage\n\nage\n\nfigure 10.10: estimated effects of age for addiction data, category {2} against category\n{0, 1} (left) and {1} against {0} given response in {0, 1} (right).\n\ntable 10.2: parameter estimates for addiction data, hierarchical model, category {2}\nagainst categories {0, 1}.\n\n(intercept)\ngender\nuniversity\n\ns(age)\n\nestimate\n\u22121.154\n0.025\n\u22120.130\nedf\n\n3.526\n\nstd. error\n\n0.148\n0.183\n0.209\n\nref.df\n\n4.411\n\nz-value\n\u22127.768\n0.139\n\u22120.622\nchi.sq\n\npr(> |z|)\n0.0\n0.889\n0.534\n\np-value\n\n21.34\n\n0.0004\n\ntable 10.3: parameter estimates for addiction data, hierarchical model, {1} against {0}\ngiven response in {0, 1}.\n\n(intercept)\ngender\nuniversity\n\ns(age)\n\nestimate\n\u22120.175\n0.604\n1.376\n\nedf\n\n4.018\n\nstd. error\n\n0.158\n0.208\n0.264\n\nref.df\n\n4.518\n\nz-value\n\u22121.103\n2.899\n5.203\n\npr(> |z|)\n0.270\n0.003\n0.0\n\nchi.sq\n\np-value\n\n50.41\n\n0.0\n\n10.3.2 extension to multicategorical response\nwhen the response is in categories with an underlying multinomial distribution it is useful to\ndistinguish between ordered response categories (chapter 9) and unordered response categories\n(chapter 8). the extension to the additive structuring of covariates is very easy for simple\n\n "}, {"Page_number": 311, "text": "10.3. structured additive regression\n\n299\nordinal models. for example, the simple cumulative model has the form p (y \u2264 r|x) = f (\u03b7r),\nwhere the rth predictor is \u03b7r = \u03b30r + xt \u03b3. additive structuring uses\n\n\u03b7r = \u03b30r + f(1)(x1) + \u00b7\u00b7\u00b7 + f(p)(xp),\n\n(cid:14)\n\nif one assumes the form f(j)(xj) =\n\nwhere \u03b301 \u2264 \u00b7\u00b7\u00b7 \u2264 \u03b30k has to hold for the cumulative model. by expanding the unknown\nfunctions in basis functions, estimation procedures including a penalty term are developed\nmj\ns=1 \u03b2js\u03c6js(xj), one obtains for\nstraightforwardly.\ndata (yi, xi), i = 1, . . . , n, the linear predictor \u03b7ir = \u03b30r + \u03c6t\nip\u03b2p, where\nij = (\u03c6j1(xij) . . . \u03c6jmj (xij)) are the evaluations of basis functions for the jth variable at\n\u03c6t\nxij. by collecting all the evaluations into one matrix and the parameters into one vector one\nobtains for the ith observation the vector \u03b7i = (\u03b7i1, . . . , \u03b7i,k\u22121)t = \u03c6i\u03b2. by using the esti-\nmation procedures from chapter 9 and including a penalty term that penalizes differences of\nadjacent basis functions one obtains a smoothed estimate. in this simple case all of the effects of\ncovariates are assumed to be global; therefore, the effects do not vary over response categories.\nthe more general model with category-specific effects uses the k \u2212 1 unknown functions\n\ni1\u03b21 + \u00b7\u00b7\u00b7 + \u03c6t\n\n\u03b7r = \u03b30r + f(1),r(x1) + \u00b7\u00b7\u00b7 + f(p),r(xp),\n\n, r = 1, . . . , k \u2212 1.\n\nnow the effects of the covariates are category-specific, which is denoted by the subscript r. for\nthis model and all combinations of global and category-specific covariates, expansion in basis\nfunctions again yields a linear predictor of the form \u03b7i = \u03c6i\u03b2 and appropriate penalty terms\nwill stabilize estimation.\nin the case of k unordered categories, a basic model is the multinomial logit model p (y =\nr|x) = exp(\u03b7r)/\u03c3s exp(\u03b7s) with predictors \u03b7r = xt \u03b2r. the extension to additive predictors\nalso has to use category-specific functions:\n\n\u03b7r = \u03b30r + f(1),r(x1) + \u00b7\u00b7\u00b7 + f(p),r(xp),\n\nwhere, for identifiability reasons, one predictor is set to zero (for example, \u03b7k = 0). while\nthe simple ordinal model with global effects only is basically one-dimensional, the multinomial\nmodel is always (k \u2212 1)-dimensional, and the functions compare the effect of covariates to a\nreference category. but, again, the estimation methods for additive models can be extended by\nusing the multivariate model presentation of the logit model given in chapter 8. specific soft-\nware is needed for the fitting. in simple cases where one can structure the response categories\nhierarchically, one can also use software for binary models (see example 10.5).\n\nhastie and tibshirani (1990) considered backfitting procedures for the fitting of the ordinal\nproportional-odds model, yee and wild (1996) put the estimation of multinomial models within\nthe framework of vector generalized additive models and used the backfitting algorithm, yee\nand hastie (2003) considered the reduced rank version. tutz and scholz (2004) distinguished\nbetween global and category-specific effects; flexible modeling for discrete choice data was\nconsidered by abe (1999). general semiparametrically structured regression models have also\nbeen considered by tutz (2003), and kauermann and tutz (2003).\n\nexample 10.6: preference for political parties\nin section 8.2 the preference for political parties dependent on gender and age was investigated with age\ngiven in four categories. now we include age as a continuous variable and allow for smooth effects. the\nconsidered german parties are the christian democratic union (cdu), which serves as the reference; the\nsocial democratic party (spd, category 1); the green party (category 2); and the liberal party (fdp,\ncategory 3). figure 10.11 shows the fitted effects of age by use of the package vgam. the preference\nfor the spd over the cdu seems to decrease between 30 and 70 years of age, whereas the preference for\n\n "}, {"Page_number": 312, "text": "300\n\nchapter 10. semi- and non-parametric generalized regression\n\nthe green party over the cdu decreases over the whole range, showing that in particular younger people\nprefer the greens. the comparison between the liberal party and the cdu is less distinct (see also figure\n8.5, which was built with categorized age).\n\n2\n\n:\n)\n\ne\ng\na\n(\ns\n\n2\n\n0\n\n2\n\u2212\n\n6\n\u2212\n\n20\n\n40\n\n60\n\n80\n\n20\n\n40\n\n60\n\n80\n\nage\n\nage\n\n1\n\n:\n)\n\ne\ng\na\n(\ns\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\u2212\n\n.\n\n8\n0\n\u2212\n\n3\n:\n)\n\ne\ng\na\n(\ns\n\n2\n\n1\n\n0\n\n1\n\u2212\n\n20\n\n40\n\n60\n\n80\n\nage\n\nfigure 10.11: effect of age resulting from the fitting of an additive model for party\npreference with reference category cdu.\n\n10.3.3 structured interactions\nadditive models have the attractive feature that effects of single predictors can be plotted sep-\narately. thus, the effect of predictors is easily visualized. however, the strong underlying\nassumption is that the effect of one predictor does not depend on the values at which other\nvariables are fixed. for arbitrary values of the other variables it is assumed that the effect of\none predictor is the same. although the assumption is much weaker than the linear predictor\nassumption, it is still strong enough to sometimes make the fit to the data unsatisfactory.\n\ninteraction effects occur if the effect of one predictor on the response depends on the values\nof other predictors. then the effect of the predictor cannot be separated from the fixed values\nof the other predictors. in the following we consider some approaches to structured interaction\nterms. an alternative approach to model interactions based on tree-based methods is given in\nchapter 11.\n\nvarying-coefficients models\na specific way of modeling interactions is by so-called varying-coefficients models, which\nwere proposed by hastie and tibshirani (1993).\nin varying-coefficient models a two-way\ninteraction is specified by modeling the effect of one predictor linearly but the slope may vary\n\n "}, {"Page_number": 313, "text": "10.3. structured additive regression\n\n301\n\nnon-parametrically as a function of the second predictor. the general form of the predictor for\nbinary or continuous variables z1, . . . , zp and x0, x1, . . . , xp is\n\n\u03b7 = \u03b20(x0) + z1\u03b21(x1) + \u00b7\u00b7\u00b7 + zp\u03b2p(xp),\n\n(10.21)\n\nwhere the \u03b2j(xj) are unspecified functions of predictions. thus the z-variables have a linear\neffect, but the effects are modified by the x-variables. the x-variables are called effect modi-\nfiers. it should be noted that (10.21) may contain simple additive terms. for example, if z1 and\nz2 are constant, z1 \u2261 z2 \u2261 1, the linear predictor contains the additive terms \u03b21(x1) + \u03b22(x2).\nfor simplicity in the following we consider only single interaction terms.\n\ndiscrete-by-continuous interaction\na simple case of the varying coefficient model specifies the interaction of a binary variable\nz \u2208 {0, 1} and a continuous variable x by\n\n\u03b7(z, x) = \u03b20(x) + z\u03b21(x),\n\nwhere the intercept term \u03b20(x) and the slope are functions of x. formally, one obtains the\nmodel from (10.21) by setting z1 = z, x0 = x1 = x. the model assumes that the effect of z\nmay vary with x. it also implies that the effect of x is determined by different functions for the\ntwo values of z:\n\n\u03b7(1, x) = \u03b20(x) + \u03b21(x),\n\n\u03b7(0, x) = \u03b20(x).\n\n(cid:14)\n\nthus \u03b20(x) represents the curve for the reference population z = 0, and \u03b21(x) represents the\nadditive modification of the \u201dbaseline\u201d function \u03b20(x). when the effects \u03b2j(xi), j = 0, 1, are\ni \u03b2j(xi) = 0, the model can also be given in a form that separates\ncentered, for example by\nmain effects and interaction terms. one obtains \u03b7(z, x) = \u03b10 + z\u03b1 + \u03b20(x) + z\u03b21(x), yielding\n\u03b7(1, x) = \u03b10 + \u03b1 + \u03b20(x) + \u03b21(x), \u03b7(0, x) = \u03b10 + \u03b20(x).\nin the more general case of a discrete variable a \u2208 {1, . . . , k}, the varying-coefficients\n\nmodel may be written with (0-1)-dummy variables xa(1), . . . , xa(k) as\n\n\u03b7(a, x) = \u03b20(x) + xa(2)\u03b22(x) + \u00b7\u00b7\u00b7 + xa(k)\u03b2k(x),\n(cid:14)\n\n(cid:14)\n\nor with centered effects as \u03b7(a, x) = \u03b11 +\n\nk\n\nj=2 xa(j)\u03b1j + \u03b20(x) +\n\nk\n\nj=2 xa(j)\u03b2j(x).\n\ncontinuous-by-discrete interaction\nfor the binary effect-modifier x \u2208 {0, 1} and the continuous variable z, the predictor \u03b7(z, x) =\n\u03b20(x) + z\u03b2(x) implies\n\n\u03b7(z, 1) = \u03b20(1) + z\u03b2(1),\n\n\u03b7(z, 0) = \u03b20(0) + z\u03b2(0),\n\nwhich means that the effect of z varies across values of x. simple reparameterization shows\nthat the predictor is the same as in simple interaction modeling, which uses the predictor \u03b20 +\nx\u03b2x + z\u03b2z + xz\u03b2xz. in the more general case of a discrete variable a \u2208 {1, . . . , k} one obtains\n\u03b7(z, a = i) = \u03b20(i) + z\u03b2(i).\n\ncontinuous-by-continuous interactions\nwhen both interacting variables are continuous one can distinguish between the fully non-\nparametric approach and interaction terms where one variable is modeled parametrically. with\n\n "}, {"Page_number": 314, "text": "302\n\nchapter 10. semi- and non-parametric generalized regression\n\nvariables xi and xj the fully non-parametric approach assumes a term f(xi, xj) in the additive\npredictor (10.14), yielding\n\n\u03b7(x) = \u03b20 + f(1)(x1) + \u00b7\u00b7\u00b7 + f(p)(xp) + f(ij)(xi, xj).\n\nthese interaction terms, which treat both variables symmetrically, can be modeled by tensor\nproducts or radial basis functions as considered in section 10.2.\n\nin the asymmetrical case, one has to decide which of the two variables is modeled paramet-\nrically. let x and z denote the two variables, and let x be modeled non-parametrically. then\none specifies a varying coefficient model with predictor\n\n\u03b7(z, x) = \u03b20(x) + z\u03b2(x).\n\nof course the model assumes more structure than a model where the combined effect of x, z is\nunspecified.\n\nestimation\nestimation in varying-coefficients models with continuous effect modifiers is very similar to\nestimation in additive models. the main difference is that the unknown functions are multiplied\nby some known predictor. when using an expansion in basis functions, the unknown functions\nhave the form\n\n(cid:7)\n\n\u03b2j(x) =\n\n\u03b4l\u03c6(j)\n\nl\n\n(x).\n\nl\n\n(cid:7)\n\nthen the interaction term zj\u03b2j(xj) in predictor (10.21) becomes, for data xij, zij, i = 1, . . . , n,\n\nzij\u03b2j(xij) =\n\n\u03b4l(zij\u03c6(j)\n\nl\n\n(xij)),\n\nl\n\n(xij) is known for fixed basis functions.\n\nwhich is again linear since zij\u03c6(j)\n\nl\n\nmarx and eilers (1998) used b-splines with penalization, and ruppert et al. (2003) gave\nseveral examples based on the truncated power series and an extensive discussion of interaction\nmodeling. alternatively, in particular, if only one effect modifier is included, local polynomial\nregression methods may be used; see, for example, kauermann and tutz (2001) and tutz and\nkauermann (1997). for examples that use alternative smoothers with the backfitting algorithm\nsee hastie and tibshirani (1993). many alternative methods have been proposed, in particular\nfor the linear model; see, for example, fan and zhang (1999), wang and xia (2009), and leng\n(2009).\n\nin the case of categorical effect modifiers, regularization techniques for categorical pre-\ndictors are useful because the number of parameters to be estimated tends to be large. if p\npredictors and an effect modifying variable a \u2208 {1, . . . , k} are available, (p + 1)k parameters\nhave to be estimated. to restrict estimation to the relevant effects, regularization techniques as\nin section 6.5 can be used. in particular one wants to know which categories of a have to be\nj zj\u03b2j(i)\ndistinguished and which variables are influential. with the predictor \u03b7(z, a = i) =\nand nominal effect modifier a one includes into the log-likelihood the penalty term\n\n(cid:14)\n\np(cid:7)\n\n(cid:7)\n\np(cid:7)\n\nk(cid:7)\n\nj(\u03b2) =\n\n|\u03b2j(r) \u2212 \u03b2j(s)| +\n\n|\u03b2j(r)|\n\nj=0\n\nr>s\n\nj=1\n\nr=1\n\ntogether with a smoothing parameter that steers the strength of regularization. the first term\nenforces the collapsing of categories, while the second term enforces the selection of covariates.\n\n "}, {"Page_number": 315, "text": "10.3. structured additive regression\n\n303\n\ncollapsing of categories yields clusters of categories for which the covariate zj has the same\neffect on the dependent variable. for ordered effect modifier a modified penalty term that\nenforces clustering of adjacent categories should be used; for details see gertheiss and tutz\n(2011). in the following we give an example for the case of a categorical effect modifier.\n\nage\n\nage2\n\njob tenure\n\nt\n\ni\n\nn\ne\nc\ni\nf\nf\n\ne\no\nc\n\nt\n\ni\n\nn\ne\nc\ni\nf\nf\n\ne\no\nc\n\n6\n0\n0\n\n.\n\n5\n0\n\n.\n\n0\n\n4\n0\n\n.\n\n0\n\n3\n0\n0\n\n.\n\n2\n0\n0\n\n.\n\n1\n0\n\n.\n\n0\n\n0\n0\n0\n\n.\n\n6\n0\n0\n0\n\n.\n\n5\n0\n0\n\n.\n\n0\n\n4\n0\n0\n0\n\n.\n\n3\n0\n0\n0\n\n.\n\n2\n0\n0\n\n.\n\n0\n\n1\n0\n0\n0\n\n.\n\n0\n0\n0\n.\n0\n\nt\n\ni\n\nn\ne\nc\ni\nf\nf\n\ne\no\nc\n\n4\n0\n\u2212\ne\n1\n\u2212\n\n4\n0\n\u2212\ne\n3\n\u2212\n\n4\n0\n\u2212\ne\n5\n\u2212\n\n4\n0\n\u2212\ne\n7\n\u2212\n\nt\n\ni\n\nn\ne\nc\ni\nf\nf\n\ne\no\nc\n\n2\n1\n0\n0\n\n.\n\n8\n0\n0\n\n.\n\n0\n\n4\n0\n0\n0\n\n.\n\n0\n0\n0\n0\n\n.\n\n0.0\n\n0.2\n\n0.4\n0.6\ns smax\n\n0.8\n\n1.0\n\n0.0\n\n0.2\n\nbody height\n\n0.4\n0.6\ns smax\n\nmarried\n\n0.8\n\n1.0\n\n0.0\n\n0.2\n\n0.4\n0.6\ns smax\n\n0.8\n\n1.0\n\nblue\u2212collar worker\n\n0\n1\n\n.\n\n0\n\n5\n0\n\n.\n\n0\n\nt\n\nt\n\ni\n\nn\ne\nc\ni\nf\nf\n\ne\no\nc\n\ni\n\nn\ne\nc\ni\nf\nf\n\ne\no\nc\n\n0\n0\n0\n\n.\n\n5\n0\n\n.\n0\n\u2212\n\n.\n\n0\n0\n\n1\n\n.\n\n0\n\u2212\n\n.\n\n2\n0\n\u2212\n\n3\n\n.\n\n0\n\u2212\n\n0.0\n\n0.2\n\n0.4\n0.6\ns smax\n\n0.8\n\n1.0\n\n0.0\n\n0.2\n\n0.4\n0.6\ns smax\n\n0.8\n\n1.0\n\n0.0\n\n0.2\n\n0.4\n0.6\ns smax\n\n0.8\n\n1.0\n\nfigure 10.12: coefficient buildups for income data with effect modifier gender, dashed\nlines denote coefficients for men, solid lines for women.\n\nexample 10.7: income data\nas response we consider monthly log-income depending on age (in years), squared age, job tenure (in\nmonths), body height (cm), married (yes/no), abitur (allows access to university, yes/no), and blue-collar\nworker (yes/no). as the effect modifier we consider gender. the data stem from the socio-economic panel\nstudy (soep), which is a longitudinal study of private households in germany. in figure 10.12 six of the\neight gender-dependent coefficients are plotted against varying degrees of smoothing. the dashed lines\nrefer to males and the solid lines to females. the vertical lines indicate the chosen degree of regularization\nbased on cross-validation. it is seen that the effects of most predictors are modified by gender; only for\n\n "}, {"Page_number": 316, "text": "304\n\nchapter 10. semi- and non-parametric generalized regression\n\nbody height there is no difference between sexes. a strong effect is found in particular for the variable\nmarried. for males the effect of being married is positive and for females negative. therefore, as far as\nincome is concerned, being married has an positive effect for males but not for females. for more details\nsee gertheiss (2011) and gertheiss and tutz (2011).\n\n10.3.4 structured additive regression modeling\nthe model terms considered in the previous sections can be combined to obtain a rather gen-\neral model for the effect of explanatory variables on the dependent variable. in the structured\nadditive regression (star) model one assumes for observation i that the predictor has the form\n\n\u03b7(xi) = f(1)(vi1) + . . . f(m)(vim) + vt\n\ni,m+1\u03b3,\n\n(10.22)\n\nwith unspecified functions f(1)(.), . . . , f(p)(.) and the vij denoting generic covariates of differ-\nent types and dimensions that are built from xi. in addition to the term that contains unknown\nfunctions a linear term vt\ni,m+1\u03b3 is included. that term is needed in particular when the set of\nexplanatory variables contains categorical variables, which have to be represented by dummy\nvariables.\n\nthe generic covariates and the corresponding functions can take several forms:\n\u2022 with vij = xij one obtains the gam predictor f(1)(xi1) + \u00b7\u00b7\u00b7 + f(p)(xip);\n\u2022 if vij = (xir, xis)t with two-dimensional function f(j), an interaction term f(j)(xir, xis)\n\ncan be included in the additive predictor;\n\n\u2022 if vij = (xir, xis)t , a varying coefficients term xirf(j)(xis) with the one-dimensional\n\nfunction f(j) can be included.\n\nfahrmeir et al. (2004) investigated a more general form, in which the observation index is\nconsidered to be a generic index. for example, for longitudinal data that are observed at time\npoints t \u2208 {1, . . . , t} at different locations the index i can be replaced by it, which denotes the\nith observation at time t. then the predictor can also include\n\n\u2022 ftime(t), which represents a possibly nonlinear trend;\n\n\u2022 vt\n\ni \u03b3 = \u03b3i0, which represents an individual parameter for the ith observation (specified\nby indicator variables vi);\n\n\u2022 fspat(sit), which represents a spatially correlated effect of location sit.\n\nthese extended versions represent repeated measurements, which will be considered in chap-\nters 13 and 14.\n\nregularized estimation of a star model typically uses an additive penalty that includes a\nseparate penalty for each included component that contains an unspecified function. if the linear\nterm contains many variables or categorical variables with many categories, the parameter in the\nlinear term also should be penalized. bayesian estimates for very general generalized additive\nregression approaches were considered by fahrmeir et al.\n(2004) and fahrmeir and kneib\n(2009).\n\nin particular, if many variables are available, selection of the relevant terms is important. an\napproach that combines estimation and selection is boosting, which we considered previously\nfor simple additive models. a basic boosting algorithm for generally structured models is the\nfollowing.\n\n "}, {"Page_number": 317, "text": "10.3. structured additive regression\n\n305\n\ngamboost for structured additive regression\n\nfor l = 0, 1, . . .:\n\n1. estimation step: for s = 1, . . . , m, fit the model\n\n\u03bci = h(\u02c6\u03b7(l)(xi) + f(s)(vis)),\n\nwhere \u02c6\u03b7(l)(xi) is the fit from the previous step (treated as an off-set) to obtain estimates\n\u02c6f(s),new(vis).\n\n2. selection step: compare the fit of the models \u03bci = h(\u02c6\u03b7(l)(xi) + \u02c6f(s),new(vis)), s =\n1, . . . , m, by using the deviance or some information criterion. let j denote the index of\nthat model for which the criterion is minimized.\n\n3. update: set for j only \u02c6f (l+1)\n\n(j)\n\n\u02c6f (l)\n(s)(.).\n\n(.) = \u02c6f (l)\n\n(j)(.) + \u02c6f(j),new(.), and for s (cid:8)= j set \u02c6f (l+1)\n\n(s)\n\n(.) =\n\nhow the model is fitted depends on the type of function f(s)(.). if the function is one-\ndimensional, other methods are to be used than for two-dimensional functions. some care is\nneeded to obtain learners (estimators) that show the same degree of weakness. kneib et al.\n(2009) showed how the complexity of the fit can be adapted by using simple quadratic fitting\nprocedures. the next example shows how components are selected starting from a complex\ncandidate model.\n\nexample 10.8: forest health\nthe health status of beeches at 83 observation plots located in a northern bavarian forest district has\nbeen assessed in visual forest health inventories carried out between 1983 and 2004 (see also kneib and\nfahrmeir, 2006; kneib and fahrmeir, 2008). the health status is classified on an ordinal scale, where the\nnine possible categories denote different degrees of defoliation. the domain is divided in 12.5% steps,\nranging from healthy trees (0% defoliation) to trees with 100% defoliation. since the data become rel-\natively sparse already for a medium amount of defoliation, we will model the dichotomized response\nvariable defoliation with categories 1 (defoliation above 25%) and 0 (defoliation less or equal to 25%).\nthe collected data have a temporal and a spatial component. it has to be assumed that trees measured\nat the same plot are correlated. the covariates in the dataset are age (age of the tree in years, continu-\nous), time (calendar time, continuous, 1983\u2264time\u22642004), elevation (elevation above sea level in meters,\ncontinuous), inclination (inclination of slope in percent, continuous), soil (depth of soil layer in centime-\nters, continuous), ph (ph value in 0\u20132 cm depth, continuous), canopy (density of forest canopy in percent,\ncontinuous), stand (type of stand; categorical; 1: deciduous forest; \u22121: mixed forest), fertilization (cat-\negorical, 1: yes; \u22121: no), humus (thickness of humus layer in five categories, ordinal; higher categories\nrepresent higher proportions), moisture (level of soil moisture, categorical; 1: moderately dry; 2: moder-\nately moist; 3: moist or temporary wet), and saturation (base saturation, ordinal; higher categories indicate\nhigher base saturation).\n\nthe data include a temporal and a spatial component. in particular, the longitudinal structure calls\nfor the incorporation of plot-specific effects. previous studies described in kneib and fahrmeir (2006)\nsuggest the presence of interaction effects and the non-linear influences of some continuous predictors.\nbased on these results we consider a logit model with the candidate predictor\n\n\u03b7(x) = vt \u03b2 + f1(ph) + f2(canopy) + f3(soil) + f4(inclination) + f5(elevation)\n\n+f6(time) + f7(age) + f8(time, age) + f9(sx, sy) + bplot,\n\n "}, {"Page_number": 318, "text": "306\n\nchapter 10. semi- and non-parametric generalized regression\n\ncanopy density\n\ndepth of soil layer\n\n6\n.\n0\n\n4\n.\n0\n\n2\n.\n0\n\n0\n.\n0\n\n2\n.\n0\n\u2212\n\n4\n.\n0\n\u2212\n\n6\n.\n0\n\u2212\n\n0.02\n\n0.01\n\n0.00\n\n\u22120.01\n\n6\n.\n0\n\n4\n.\n0\n\n2\n.\n0\n\n0\n.\n0\n\n2\n.\n0\n\u2212\n\n4\n.\n0\n\u2212\n\n6\n.\n0\n\u2212\n\n0.0\n\n0.2\n\n0.4\ncorrelated spatial effect\n\n0.6\n\n0.8\n\n1.0\n\n10\n\n20\n\n30\n\n40\nuncorrelated random effect\n\n50\n\n2.0\n\n1.5\n\n1.0\n\n0.5\n\n0.0\n\n\u22120.5\n\n\u22121.0\n\n2\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n200\n\n150\n\nage of the tree\n\n100\n\n50\n\n1985\n\n2000\n\n1995\n  y e a r\n\n1990\n\nc a l e n d a r\n\nfigure 10.13: identified effects for forest health data: non-linear effects of canopy den-\nsity and depth of soil, spatial effect, plot-specific effects, and interaction between age and\nyear.\n\nwhere v contains the parametric effects of the categorical covariates and the base learners for the smooth\neffects f1, . . . , f7 are specified as univariate cubic penalized splines with 20 inner knots and a second-\norder difference penalty. for both the interaction effect f8 and the spatial effect f9 we assume bivariate\ncubic penalized splines with first-order difference penalties and 12 inner knots for each of the directions.\nthe plot-specific random effect bplot is assumed to be gaussian with the random effects variance fixed\nsuch that the base learner has one degree of freedom. similarly, all univariate and bivariate non-parametric\neffects are decomposed into parametric parts and non-parametric parts with one degree of freedom each.\nthe stopping criterion was determined by a booststrapping procedure.\n\nafter applying the stopping rule, no effect was found for the ph value, inclination of slope, and\nelevation above sea level. the univariate effects for age and calendar time where strictly parametric linear\nbut the interaction effect turned out to be very influential. the sum of both the linear main effects and the\nnon-parametric interaction is shown in figure 10.13. the spatial effect was selected only in a relatively\nsmall number of iterations, whereas the random effect was the component selected most frequently. we\ncan therefore conclude that the spatial variation in the data set seems to be present mostly very locally.\nfor canopy density and soil depth, non-linear effects where identified as visualized in figure 10.13 (for\nmore details see kneib et al., 2009).\n\n "}, {"Page_number": 319, "text": "10.4. functional data and signal regression\n\n307\n\n10.4 functional data and signal regression\nin recent years functional data analysis (fda) has become an important tool to model data\nwhere the unit of observation is a curve or in general a function. functional responses are\nrelated to repeated measurements, which are treated in chapters 13 and 14. here we focus on\nfunctional predictors, which can be considered a strongly structured and high-dimensional form\nof predictor. we will start with some examples.\n\nexample 10.9: canadian weather data\nthe data taken from ramsey and silverman (2005) give the average daily precipitation and temperature at\n35 canadian weather stations. it can be downloaded from the related website http://www.functionaldata.org.\nlike ramsay and silverman, we try to predict the logarithm of the total annual precipitation from the pat-\ntern of temperature variation through the year. figure 10.14 shows the temperature profiles (in degrees\ncelsius) of the weather stations across the year, averaged over the years 1960 to 1994. like ramsey and\nsilverman (2005) we will consider the base 10 logarithm of the total annual precipitation as response\nvariable.\n\n102.1[mm]\n\n103.5[mm]\n\ne\nr\nu\nt\na\nr\ne\np\nm\ne\nt\n\n0\n2\n\n0\n1\n\n0\n\n0\n1\n\u2212\n\n0\n2\n\u2212\n\n0\n3\n\u2212\n\n0\n\n100\n\n200\n\nday\n\n300\n\nfigure 10.14: temperature profiles of 35 canadian weather stations.\n\nexample 10.10: mass spectrometry\nin mass spectrometry\u2013based predictive proteomics one often wants to distinguish between healthy patients\nand severely ill patients by using mass spectrometry data. we use data from petricoin et al. (2002), which\nis available from the national cancer institute via http://home.ccr.cancer.gov/ncifdaproteomics. the goal\nis to discriminate prostate cancer from benign prostate conditions by proteomic pattern diagnosis. all in\nall, 69 seldi-tof blood serum spectra from cancer patients and 253 from patients with benign conditions\nare given. each spectrum is composed of measurements at approximately 15,200 points defined by mass\nover charge ratio m/z values; see petricoin et al. (2002) for more details. in some applications the full\nspectrum is used as input to a classifier that itself selects variables. then features are selected in terms of\nsingle m/z values. due to horizontal variability, however, the results are difficult to interpret, because the\nsame m/z values do not necessarily correspond to the same feature. therefore, hoefsloot et al. (2008)\n\n "}, {"Page_number": 320, "text": "308\n\nchapter 10. semi- and non-parametric generalized regression\n\nmanually clustered m/z values at the end of the analysis. alternatively, feature selection in terms of peak\ndetection and peak alignment tibshirani et al. (2004) is often performed before employing a classifier, as,\nfor example, described in barla et al. (2008).\n\n10.4.1 functional model for univariate response\nlet the data be given by (yi, xi(t)), i = 1, . . . , n, where yi is the response variable and\nxi(t), t \u2208 i, denotes a function defined on an interval i \u2208 r, also called the signal. a gen-\neralized functional linear model for scalar responses assumes that the mean \u03bci depends on the\nfunctional predictor xi(t) in the form g(\u03bci) = \u03b7i or \u03bci = h(\u03b7i), with the linear predictor\ndetermined by\n\n)\n\n\u03b7i = \u03b20 +\n\nxi(t)\u03b2(t)dt,\n\n(10.23)\n\nwhere \u03b2(.) is a parameter function. when using the identity link, h = id, one obtains the\nfunctional linear model\n\n)\n\nyi = \u03b20 +\n\nxi(t)\u03b2(t)dt + \u03b5i,\n\n(10.24)\n\nwhere \u03b5i with e(\u03b5i) = 0 represents the noise variable. in the functional model, the usual sum-\nmation over a finite-dimensional space is replaced by an integral over the infinite-dimensional\nspace. the parameter function \u03b2(.) is the functional analog of the parameter vector in a gen-\neralized linear regression. instead of considering parameters one investigates the effect of the\nsignal by considering the parameter function that is usually plotted against t. since \u03b2(.) is a\nfunction, it has to be estimated as such.\n\nin practice, the signal is only ever observed at a finite set of points from i, and one might\n\nconsider the dicretized version of the generalized functional model with the predictor\n\np(cid:7)\n\n\u03b7i = \u03b20 +\n\nxij\u03b2j,\n\nj=1\n\n(10.25)\n\nwhere xij = xi(tj), \u03b2j = \u03b2(tj) for values t1 < \u00b7\u00b7\u00b7 < tp, tj \u2208 i. most often the values\nt1, . . . , tp are equidistant, tj+1 \u2212 tj = \u03b4.\n\nhowever, estimation within the framework of generalized linear models is hardly an option,\nsince the number of predictors typically is too large when the predictor is functional. of course,\nvariable selection and regularization methods like ridge regression, the lasso, and the elastic net\n(see chapter 6) could be used. but they are not up to the challenge of functional predictors\nbecause they do not make use of the ordering of the predictor. in most applications the space i\non which the signals and the coefficient functions are defined is a metric space, and a distance\nmeasure is available that measures the distance between two measurement points ti and tj.\nthus, the appropriate methods should use the metric information that is available. variable\nselection strategies like the lasso may be seen as equivariant methods, referring to the fact that\nthey are equivariant to permutations of the predictor indices and therefore less appropriate for\nfunctional predictors. in contrast, spatial methods regularize by utilizing the spatial nature of\nthe predictor index (compare land and friedman, 1997).\n\n "}, {"Page_number": 321, "text": "10.4. functional data and signal regression\n\n309\n\n10.4.2 the fda approach\nsince the signal xi(t) and the unknown parameter function \u03b2(t) are functions in t, one might\nexpand both functions in basis functions:\n\nm1(cid:7)\n\nm(cid:7)\n\nxi(t) =\n\n\u03beij\u03c6j(t),\n\n\u03b2(t) =\n\n\u03b2l\u03c6\u03b2\n\nl (t),\n\nj=1\n\nl=1\n\nwhere \u03c6j(.) and \u03c6\u03b2\npredictor\n\ni (.) are fixed known basis functions defined on i. then one obtains for the\n\nxi(t)\u03b2(t)dt = \u03b20 +\n\n\u03beij\u03b2l\u03c6j(t)\u03c6\u03b2\n\nl (t)dt\n\n\u03b7i = \u03b20 +\n\n= \u03b20 +\n\n\u2019\n\nwith \u03c6jl =\n\n) (cid:7)\n\n(cid:7)\n\nj\n\nl\n\n\u03c6j(t)\u03c6\u03b2\n\nl (t)dt.\n\n)\n(cid:7)\n\n(cid:7)\n\n)\n(cid:14)\n\n\u03b2l\n\nl\n\nj\n\n\u03beij\n\nm(cid:7)\n\n\u03b7i = \u03b20 +\n\n\u03b2lzil,\n\nl=1\n\n\u03c6j(t)\u03c6\u03b2\n\nl (t)dt and zit =\n\nm1\nj=1 \u03beij\u03c6jl one has the linear predictor\n\nwhich is from m-dimensional space. therefore, the problem is reduced to a linear predictor\nproblem in m dimensions. the reduction works only when the parameters \u03beij are known.\ntherefore, in the first step of fda, the corresponding parameters \u03beij for each signal are esti-\nmated (yielding \u03beij). in the second step, one estimates the parameters of the linear predictor\n\nm(cid:7)\n\nl=1\n\n)\n\n(cid:14)\n\nm1(cid:7)\n\nj=1\n\n\u03b7i = \u03b20 +\n\n\u03b2l\u02dczil = \u02dczt\n\ni \u03b2,\n\ni = (1, \u02dczi1, . . . , \u02dczim1), \u02dczil =\n\n\u02c6\u03beij\u03c6jl, \u03b2t = (\u03b20, \u03b21, . . . \u03b2m). the first step is an\nwhere \u02dczt\nunsupervised learning step in the sense that the response is not used to fit the expansion of the\nsignal in basis functions. the second step explicitly connects the parameters to the observed\nresponses (compare ramsey and silverman, 2005).\n\nj\n\nan alternative approach has been proposed by james (2002). he uses the expansion of xi(t)\n\nto obtain\n\n\u03b7i = \u03b20 +\n\n\u03beij\n\n\u03c6j(t)\u03b2(t)dt = zt\n\ni \u03bei,\n\n\u2019\n\n\u2019\n\ni = (1,\n\ni = (\u03b20, \u03bei1, \u03bei2, . . . ). he treats the pa-\nwith zt\nrameters \u03bei as missing data and uses the em algorithm (see appendix b) to maximize the\nlikelihood.\n\n\u03c62(t)\u03b2(t)dt, . . . ), \u03bet\n\n\u03c61(t)\u03b2(t)dt,\n\n10.4.3 penalized signal regression\nin penalized signal regression only the parameter function is expanded in known basis func-\ntions, \u03b2(t) =\n\nl (t), yielding the linear predictor\n\nl \u03b2l\u03c6\u03b2\n\n(cid:14)\n\n)\n\nm(cid:7)\n\n)\n\nm(cid:7)\n\n\u03b7i =\n\nxi(t)\u03b2(t)dt =\n\n\u03b2l\n\nxi(t)\u03c6\u03b2\n\nl (t)dt =\n\n\u03b2lzil = zt\n\ni \u03b2,\n\nl=1\n\nl=1\n\n "}, {"Page_number": 322, "text": "\u2019\n\n310\n\nchapter 10. semi- and non-parametric generalized regression\n\nxi(t)\u03c6\u03b2\nl (t)dt can be computed from the observed signal xi(i) and the known\nwhere zil =\nbasis functions \u03c6\u03b2\nl (i). in contrast to the fda approach, the signal is not approximated by an\nexpansion in basis functions. the dimension of the predictor is again determined by the number\nof basis functions that are used to describe the parameter function. for fixed basis functions\nestimation can be carried out by using the methods from the preceding sections. one maximizes\nthe penalized log-likelihood,\n\nlp(\u03b2) = l(\u03b2) \u2212 \u03bb\n\n2 \u03b2t k\u03b2,\n\nwhere k is a penalty matrix that penalizes the differences of parameters linked to adjacent\nbasis functions (see section 10.1.3).\n\nthe signal regression approach with b-splines was propagated by marx and eilers (1999).\n\nmarx and eilers (2005) extended the method to a multidimensional signal regression.\n\n(cid:14)\n\n10.4.4 fused lasso\ntibshirani et al. (2005) proposed a regularization method called the fused lasso that deals with\nmetrically structured data and applies to functional data. in general, they consider the model\n\u03b7i = \u03b20 +\nj xij\u03b2j, with an ordering in the predictors xi1, xi2, . . . . a special case is the\nfunctional model when the signal has been discretized (equation (10.25)). the fused lasso uses\nthe penalized log-likelihood\n\nlp(\u03b2) =\n\nli(\u03b2) \u2212 \u03bb1\n2\n\n|\u03b2j| + \u03bb2\n2\n\n|\u03b2j \u2212 \u03b2j\u22121|,\n\np(cid:7)\n\nj=1\n\np(cid:7)\n\nj=2\n\nn(cid:7)\n\ni=1\n\nin the original form tibshirani et al.\n\nwhere li(\u03b2) is the usual log-likelihood contribution of the ith observation and \u03bb1, \u03bb2 are tun-\ning parameters.\n(2005) used the constraint represen-\ntation and restricted themselves to the linear model. the effect of the penalty is twofold:\nthe first penalty encourages sparsity in the coefficients while the second penalty encourages\nsparsity in the differences between adjacent parameters. therefore, the second term lets few\nvalues of parameters be different, and the resulting parameter curve tends to be a step func-\ntion. it is seen from figure 10.15, which shows among other procedures the fused lasso for\nthe canadian weather data, that solutions can be very sparse; only three parameter values are\ndistinguished.\n\ntibshirani et al. (2005) gave an algorithm for the linear model and discussed the asymptotic\nresults. land and friedman (1997) coined the term \"variable fusion\" in an earlier paper, in\nwhich only the second penalty term was used. the method is related to the methods considered\nin chapter 6.\n\n10.4.5 feature extraction in signal regression\nmost signal regression methods fit a smooth parameter function. areas of the signal that are\nnot relevant are supposed to be seen from the fitted curve. when the fitted parameter function\nis close to zero one might infer that these parts of the signal do not exert influence on the\nresponse. if, however, one suspects that the whole signal is not relevant, this assumption can\nbe incorporated in the fitting procedure by trying to select relevant features from the signal. in\nsome applications, as, for example, in proteomics, where one wants to find influential proteins,\nfeature selection is the central issue; identification of the relevant parts in the signal is more\nimportant than prediction.\n\n "}, {"Page_number": 323, "text": "10.4. functional data and signal regression\n\n311\n\ncommon selection procedures like the lasso and the elastic net will not perform best be-\ncause the order information of signals is not used. the fused lasso can be seen as a better\nfeature selection method but is restricted to step functions. an alternative method is block-\nwise boosting, which can be seen as a generalization of componentwise boosting. in common\ncomponentwise boosting (compare chapter 15) the selection step usually refers to the single\ncomponents of the vector xi = (xi1, . . . , xip)t . predictors that are never selected get value\nzero. in blockwise boosting, the selection and refitting refer to groups or blocks of variables.\nwith the focus on signal regression one has to define what the relevant parts of the signal\nmight be. since a signal xi(.) may be seen as a mapping xi : i \u2192 r, one can utilize the metric\nthat is available on i. a potentially relevant part of the signal xi(.) may be characterized by\n{xi,u (t)|t \u2208 u(t0)}, where u(t0) is defined as a neighborhood of t0 \u2208 i, that is, u(t0) =\n{t|(cid:16)t\u2212 t0(cid:16) \u2264 \u03b4} for the metric (cid:16).(cid:16) on i. for the digitized signal, the potentially relevant signal\npart turns into groups of variables {xij|tj \u2208 u(t0)}. simple subsets that may be used in the\nrefitting procedure are us = uk(ts) = [ts, ts + (k \u2212 1)\u03b4], k \u2208 {1, 2, . . .}. for k = 1 one\nobtains the limiting case of single variables {xi(ts)}, for k = 2 one gets pairs of variables, and\nso on.\n\nlet x (s) denote the design matrix of variables from us, that is, x (s) has rows (xi(ts),\n. . . , xi(ts+k\u22121)) = (xis, . . . , xi,s+k\u22121). an update step of blockwise boosting will be based\non estimating the vector b(s) = (b(ts), . . . , b(ts+k\u22121))t from the data (u, x (s)), where u =\n(u1, . . . , un)t denotes the current residual. straightforward maximum likelihood estimation\ncannot be recommended, since variables in x (s) tend to be highly correlated. smoother coeffi-\ncients can be obtained by penalizing differences between the adjacent coefficients. as a param-\neter estimate within one boosting step one may use the generalized ridge estimator with penalty\nterm (\u03bb/2)\u03b2t \u03c9\u03b2, where the penalty matrix is \u03c9 = dt d, with d11 = 1, dr,r\u22121 = \u22121,\ndk+1,k = 1 and zero otherwise, r = 2, . . . , k. by upper left and lower right 1\u2019s in d differ-\nences to zero coefficients of neighboring but not selected values are penalized.\n\nupdate steps based on residuals use in the mth iteration estimates\n\nb(m) = (x (sm)t x (sm) + \u03bb\u03c9)\n\n\u22121x (sm)t u(m).\n\nadditional weights are needed for binary predictors and one uses\n\nb(m) = (x (sm)t w (m)x (sm) + \u03bb\u03c9)\n\nwhere the working response is z(m) = (z(m)\nweight matrix w (m) = diag(w(m)\nm block sm producing minimum error is selected.\n\n, . . . , w(m)\n\n, . . . , z(m)\nn ), w(m)\n\n1\n\n1\n\n\u22121x (sm)t w (m)z(m),\ni = (yi \u2212 \u03c0(m\u22121)\n\u00b7 (1 \u2212 \u03c0(m\u22121)\n\nn )t , z(m)\ni = \u03c0(m\u22121)\n\ni\n\ni\n\ni\n\n)/w(m)\n) with\n). in iteration\n\ni\n\nfor continuous responses and gertheiss and tutz (2009c)\n\nfor details of the blockwise boosting procedure in signal regression see tutz and gertheiss\n(2010)\nfor binary responses. an\nalternative approach that performs feature selection within a quantile regression framework by\nuse of a structured elastic net was given by slawski (2010).\n\nexample 10.11: canadian weather data\nan illustration of the differences between methods is given in figure 10.15, where the coefficient functions\nresulting from the lasso, fused lasso, ridge regression, generalized ridge regression with first-difference\npenalty, functional data approaches (ramsey and silverman, 2005), and blockboost are shown for the\ncanadian weather data, which has become some sort of benchmark dataset. it is seen that the lasso selects\nonly few variables, that is, measurement points, theoretically at most n variables. here, selecting only a\nfew variables means selecting only few days, whose mean temperature is assumed to be relevant for the\n\n "}, {"Page_number": 324, "text": "312\n\nchapter 10. semi- and non-parametric generalized regression\n\n(a) lasso\n\n(b) fused lasso\n\n5\n2\n0\n\n.\n\n0\n\n0\n1\n0\n\n.\n\n0\n\n5\n0\n0\n\n.\n\n0\n\u2212\n\n4\n0\n\u2212\ne\n5\n\n4\n0\n\u2212\ne\n5\n\u2212\n\nt\n\ni\n\nn\ne\nc\ni\nf\nf\n\ne\no\nc\n \n\ni\n\nn\no\ns\ns\ne\nr\ng\ne\nr\n\nt\n\ni\n\nn\ne\nc\ni\nf\nf\n\ne\no\nc\n \n\ni\n\nn\no\ns\ns\ne\nr\ng\ne\nr\n\nt\n\ni\n\nn\ne\nc\ni\nf\nf\n\ne\no\nc\n \n\ni\n\nn\no\ns\ns\ne\nr\ng\ne\nr\n\n0\n\n100\n\n300\n\n200\n\nday\n\n(c) ridge\n\nt\n\ni\n\nn\ne\nc\ni\nf\nf\n\ne\no\nc\n \n\ni\n\nn\no\ns\ns\ne\nr\ng\ne\nr\n\n0\n\n100\n\n200\n\nday\n\n300\n\n(e) functional data approach\n\n4\n0\n\u2212\ne\n8\n\n4\n0\n\u2212\ne\n6\n\n4\n0\n\u2212\ne\n4\n\n4\n0\n\u2212\ne\n2\n\n0\n0\n+\ne\n0\n\n4\n0\n\u2212\ne\n2\n\u2212\n\nt\n\ni\n\nn\ne\nc\ni\nf\nf\n\ne\no\nc\n \n\ni\n\nn\no\ns\ns\ne\nr\ng\ne\nr\n\n4\n0\n\u2212\ne\n8\n\n4\n0\n\u2212\ne\n4\n\n0\n0\n+\ne\n0\n\n4\n0\n\u2212\ne\n6\n\n4\n0\n\u2212\ne\n2\n\n4\n0\n\u2212\ne\n2\n\u2212\n\n2\n1\n0\n0\n\n.\n\n0\n\n6\n0\n0\n0\n\n.\n\n0\n\n0\n0\n0\n0\n0\n\n.\n\nt\n\ni\n\nn\ne\nc\ni\nf\nf\n\ni\n\ne\no\nc\n \nn\no\ns\ns\ne\nr\ng\ne\nr\n\n0\n\n100\n\n200\n\nday\n\n300\n\n(d) generalized ridge\n\n0\n\n100\n\n200\n\nday\n\n300\n\n(f) blockwise boosting\n\n0\n\n100\n\n200\n\nday\n\n300\n\n0\n\n100\n\n200\n\nday\n\n300\n\nfigure 10.15: regression coefficients estimated by various methods: lasso, fused lasso,\nridge, generalized ridge with first-difference penalty, functional data approach, and block-\nwise boosting. temperature profiles of 35 canadian weather stations as predictors; log\ntotal annual precipitation as response.\n\ntotal annual precipitation. by construction, a ridge regression takes into account all variables. smoothing\nthe coefficient function is possible by penalizing differences between adjacent coefficients, but still almost\nevery day\u2019s temperature is considered to be important. the fused lasso and blockboost select only some\nperiods instead, that is, some weeks in late autumn / early winter. the smooth blockboost estimates result\nfrom penalizing (first) differences between adjacent coefficients. details of the procedures are given in\ntutz and gertheiss (2010). the functional data approach here uses fourier basis functions for smoothing\nboth the functional regressors and the coefficient function, which has the effect that the end of december\nshows the same effect as the beginning of january.\n\nexample 10.12: prostate cancer\nfigure 10.16 shows the the mass spectrometry data (example 10.10) together with the parameters that\nwere extracted as relevant. the data show the mean curves for healthy patients (solid line) and patients\nsuffering from prostate cancer (dashed line). the curves themselves hardly show how to discriminate the\ntwo groups of patients. however, in particular, the area containing small values of mass charge ratio seem\nto be relevant for discrimination. therefore, the area between 0 and 800 is given separately, together with\nthe corresponding estimated values. the major discrimatory power is contained in rather few values of\nthe signal. for a comparison with alternative methods in terms of discrimination power, see gertheiss and\ntutz (2009c).\n\n "}, {"Page_number": 325, "text": "10.5. further reading\n\n313\n\nfigure 10.16: mass spectrometry data together with selected parameter values by boost-\ning procedure for the whole spectrum (above) and selected area (below).\n\n10.5 further reading\nsurveys and books. generalized additive models are treated extensively in hastie and tibshi-\nrani (1990). green and silverman (1994) give a thorough account of the roughness penalty\napproach to smoothing, and localizing techniques are extensively discussed by fan and gijbels\n(1996) and loader (1999). semiparametric modeling with the focus on low-rank smoothers\nand the mixed model representation of penalized splines is outlined by ruppert et al. (2003),\nand ruppert et al. (2009) review the developments in the field during 2003\u20132007. a source\nbook for generalized additive models and the accompanying r package mgcv is wood (2006a).\n\nknot selection approaches. stepwise selection of knots in regression splines goes back at\nleast to smith (1982). wand (2000) gives an overview and proposed a procedure that general-\nizes the method of stone et al. (1997). an alternative procedure, proposed by osborne et al.\n(1998), is based on the least absolute shrinkage and regression operator (tibshirani, 1996). a\nstrategy based on boosting techniques was given by leitenstorfer and tutz (2007). he and ng\n(1999) proposed a method that is based on quantile regression techniques for smoothing prob-\nlems (see, e.g., koenker et al., 1994). knot selection from a bayesian perspective was treated\nby smith and kohn (1996), denison et al. (1998), and lang and brezger (2004b).\n\nfitting of gams. various estimation procedures have been proposed for the estimation of\nfunctions f(j). we considered penalized regression splines that were used by marx and eil-\ners (1998), and wand (2000), and backfitting procedures in which the component functions\nare estimated iteratively by unidimensional smoothers (hastie and tibshirani, 1990). alterna-\ntive estimators were proposed by linton and h\u00e4rdle (1996), who use the marginal integration\nmethod. gu and wahba (1993) and gu (2002) treat estimation problems within the framework\nof smoothing splines, and bayesian approaches for flexible semiparametric models have been\nconsidered, for example, by fahrmeir and lang (2001). for methods of smoothing parame-\nter selection see gu and wahba (1991, 1993), and wood (2000). wood (2004) gave a nice\noverview of available methods.\n\nhigh-dimensional additive models and variable selection. avalos et al. (2007) proposed\na selection procedure for additive components based on lasso-type penalties, and an alternative\napproach with lasso penalties was given by zheng (2008). marra and wood (2011) proposed a\n\n "}, {"Page_number": 326, "text": "314\n\nchapter 10. semi- and non-parametric generalized regression\n\ngarrote-type estimator, and belitz and lang (2008) a modification of backfitting. a procedure\nthat uses boosting techniques to select influential components was proposed by tutz and binder\n(2006), and binder and tutz (2008). meier et al. (2009) proposed an efficient method for\nvariable selection based on the group lasso and gave asymptotic results.\n\nasymptotics for p-splines. asymptotic properties for the linear model were investigated by\nli and ruppert (2008) and claeskens et al. (2009). kauermann et al. (2009) also considered\nthe extension to glms, including a fully bayesian viewpoint.\n\nr packages. a very versatile package for the fitting of gams is mgcv; the function gam fits\ngeneralized additive models. fitting procedures based on boosting are available in the packages\ngamboost and mboost. functional data may be fitted by use of the package fda. the vgam\npackage (yee, 2010) allows one to fit multinomial and ordinal additive models.\n\n10.6 exercises\n(cid:2)\n\n10.1 for data (yi, xi), i = 1, . . . , n, a local estimate of \u03bc(x) at target value x can be obtained by min-\ni(yi \u2212 \u03bc(x))2k\u03bb(xi \u2212 x), where k\u03bb(.) = k(.)/\u03bb, with k denoting a\nimizing the weighted sum\ncontinuous symmetric kernel function fulfilling\n\nk(u)du = 1.\n\n(cid:7)\n\n(a) show that the mininimization problem corresponds to a local regression problem and derive the\n\nestimate \u02c6\u03bc(x).\n\n(b) derive the criterion to be minimized in a local regression with a polynomial of degree zero and\n\nbinary responses yi. find the estimate \u02c6\u03bc(x).\n\n(c) interpret the estimates from (a) and (b) when the uniform kernel k(u) = 1 for 0 \u2264 u \u2264 1 is used.\n(d) derive the bias for the estimates from (a) and (b). is the estimate unbiased in the limiting case\n\n\u03bb \u2192 \u221e?\n\n(cid:2)\n\nn\n\ni=1 li(yi, \u03b7i) \u2212 \u03bb\n\n2 \u03b7t k\u03b7, where \u03b7t =\n10.2 let a penalized likelihood have the form lp(\u03b7) =\n(\u03b71, . . . , \u03b7n) and k is a fixed matrix. derive the maximizer of lp(\u03b7) if yi is normally distributed, and\nyi = \u03b7i + \u0001i, with noise variable \u0001i.\n\n10.3 table 7.1 shows the number of cases of encephalitis in children observed between 1980 and 1993 in\nbavaria and lower saxony.\n\n(a) use local regression methods to investigate the variation of counts over years for each country\n\nseparately.\n\n(b) compare the results for different degrees of the polynomial fit. can the fit of a non-localized\n\npolynomial regression be obtained as a special case? what favors the local fit?\n\n(c) compare with the fitted curves for regression splines and smoothing splines.\n\n(d) discuss the dependence of the fit on the distributional assumption and the link.\n\n10.4 table 7.13 shows the number of differentiating cells together with the dose of tnf and ifn.\n\n(a) investigate the effect of tnf and ifn on the number of differentiating cells by fitting an additive\n\nmodel.\n\n(b) compare the fit of an additive model to the fit of a response surface without further constraints.\n\n(c) discuss the dependence of the fit on the distributional assumption and the link.\n\n "}, {"Page_number": 327, "text": "10.6. exercises\n\n315\n\n10.5 the dataset dust from the package catdata contains the binary response variable bronchitis and\nexplanatory variables dust concentration and duration of exposure.\n\n(a) fit a smooth model without further restrictions and an additive model to the dust data for non-\n\nsmokers and compare.\n\n(b) compare to the results of the fitting of a glm.\n\n10.6 the dataset addiction from the package catdata contains the response in the three categories 0:\naddicts are weak-willed; 1: addiction is a disease; 2: both. fit two additive logit models, one comparing\ncategory {1} and category {0} and the other comparing category {2} and category {0}.\n\n(a) fit smooth models with only predictor age allowing for an unspecified functional form. visualize\n\nthe resulting probabilities.\n\n(b) fit smooth models with predictor age allowing for an unspecified functional form and additional\n\ncovariates. visualize the resulting probabilities and compare with (a).\n\n "}, {"Page_number": 328, "text": " "}, {"Page_number": 329, "text": "chapter 11\n\ntree-based methods\n\ntree-based models provide an alternative to additive and smooth models for regression prob-\nlems. the method has its roots in automatic interaction detection (aid), proposed by morgan\nand sonquist (1963). the most popular modern version is due to breiman et al. (1984) and\nis known by the name classification and regression trees, often abbreviated as cart, which is\nalso the name of a program package. the method is conceptually very simple. by binary recur-\nsive partitioning the feature space is partitioned into a set of rectangles, and on each rectangle\na simple model (for example, a constant) is fitted.\n\nthe approach is different from that given by fitting parametric models like the logit model,\nwhere linear combinations of predictors are at the core of the method. rather than getting\nparameters, one obtains a binary tree that visualizes the partitioning of the feature space. if\nonly one predictor is used, the one-dimensional feature space is partitioned, and the resulting\nestimate is a step function that may be considered a non-parametric (but rough) estimate of the\nregression function.\n\n11.1 regression and classification trees\n\nin the following the basic concepts of classification and regression trees (carts) are given.\nthe term \"regression\" in carts refers to metrically scaled outcomes, while \"classification\"\nrefers to the prediction of underlying classes. by considering the underlying classes as the\noutcomes of a categorical response variable, classification can be treated within the general\nframework of regression.\n\nlet us first illustrate the underlying principle for a one-dimensional response variable y and\na two-dimensional predictor. in the first step one chooses a predictor and a split-point to split\nthe predictor space into two regions. in each region the response is modeled by the mean in that\nregion, and choice of the split is guided by the best fit to the data. for example, in figure 11.1\nthe first split yields the partition {x1 \u2264 c1}, {x1 > c1}. in the next step one or both of the\nobtained regions are split into new regions by using the same criterion for the selection of the\npredictor and the split-point. in figure 11.1, the region {x1 \u2264 c1} is split by variable x2 at\nsplit-point c2, yielding the partition {x2 \u2264 c2} \u2229 {x1 \u2264 c1}, {x2 > c2} \u2229 {x1 \u2264 c1}. the\nregion {x1 > c1} is split at c3, yielding {x2 \u2264 c3} \u2229 {x1 > c1}, {x2 > c3} \u2229 {x1 > c1}. in\nour example, in a last step, the region {x2 > c2} \u2229 {x1 \u2264 c1} is split into {x1 \u2264 c4} \u2229 {x2 >\nc2} \u2229 {x1 \u2264 c1} and {x1 > c4} \u2229 {x2 > c2} \u2229 {x1 \u2264 c1}. in the end one obtains five regions,\nr1, . . . , r5. since in each region the response is modeled by the mean, one obtains a regression\n\n317\n\n "}, {"Page_number": 330, "text": "318\n\nchapter 11. tree-based methods\n\nc3\n\n2\nx\n\nc2\n\nx2 (cid:100) c4\n\nc4\n\nc1\nx1\n\n|\n\nx1 (cid:100) c1\n\nx2 (cid:100) c2\n\nx2 (cid:100) c3\n\nfigure 11.1: partition of a two-dimensional space and the corresponding tree.\n\nmodel with a piecewise constant fit:\n\n\u02c6\u03bc(x) =\n\n5(cid:7)\n\ni=1\n\n\u03b3ii((x1, x2) \u2208 ri).\n\na node of the tree corresponds to a subset of the predictor space. the root is the top node con-\nsisting of r2, and the terminal nodes or leaves of the tree correspond to the regions r1, . . . , r5.\nthe partition may be seen from the upper panel of figure 11.1 or, simpler, from the tree below\nit.\n\nin general, trees may be seen as a hierarchical way to describe a partition of the predictor\nspace. for more than two predictor variables it is hard to visualize the partition of the space.\nbut the tree represents the partition in a unique way. moreover, the tree yields an interpretable\nstructure of how the predictors are linked to the response since response is modeled constant\nwithin the nodes. a further advantage of the hierarchical structuring is that the growing of a\ntree may be described locally by specifying how a given node a, corresponding to a subset of\npredictor space, is partitioned into a left and right daughter nodes, which correspond to subsets\na1, a2 of a.\n\n "}, {"Page_number": 331, "text": "11.1. regression and classification trees\n\n319\n\nto obtain an algorithm for the growing of a tree one has to decide on several issues. in\nparticular, the type of partitioning and the split criterion have to be chosen, and one has to\ndecide when to stop splitting.\n\n11.1.1 standard splits\nfor the trees considered here, the splits are restricted to \"standard splits,\" which means that\neach partition of node a into subsets a1, a2 is determined by only one variable. the splits to\nbe considered depend on the scale of the variable:\n\n\u2022 for metrically scaled and ordinal variables, the partition into two subsets has the form\n\na \u2229 {xi \u2264 c}, a \u2229 {xi > c},\n\nbased on the threshold c on variable xi.\n\u2022 for categorical variables without ordering xi \u2208 {1, . . . , ki}, the partition has the form\n\na \u2229 s, a \u2229 \u00afs,\n\nwhere s is a non-empty subset s \u2282 {1, . . . , ki} and \u00afs = {1, . . . , ki} \\ s is the comple-\nment.\n\nthe number of possible splits is determined by the scaling of the variable and the data that are\ngiven by (yi, xi), 1 = 1, . . . , n. for continuous variables one typically has n \u2212 1 thresholds to\nconsider; when the data are discrete the number of thresholds usually reduces. for categorical\nvariables on a nominal scale (without ordering) one obtains 2n\u22121 \u2212 1 non-empty pairs s, \u00afs,\ns \u2282 {1, . . . , ki}, if all the categories have been observed. an alternative approach that was\nproposed by quinlan (1986) uses multiple splits instead of binary splits when the predictors are\ncategorical. then one obtains as many nodes as there are categories.\n\n11.1.2 split criteria\nwhen considering all possible splits one has to decide on the value of a split, in order to select\none. several criteria for the values of a split have been proposed. one approach is based on test\nstatistics, while others use impurity measures like the gini index. we will start with test-based\nprocedures.\n\ntest-based splits\nlet a be a given node that is to be split into two subsets a1, a2 and y be a one-dimensional\nresponse variable with mean \u03bc(x) = e(y|x). it should be noted that only the subset of ob-\nservations (yi, xi) with xi \u2208 a is used when the splitting of a is investigated. typically the\nmodel to be fitted specifies that the mean is constant over regions of the predictor space. one\ncompares the fit of the model\n\nma :\n\n\u03bc(x) = \u03bc for x \u2208 a\n\nto the fit of the model\n\nma1,a2 :\n\n\u03bc(x) = \u03bc1 for x \u2208 a1, \u03bc(x) = \u03bc2 for x \u2208 a2,\n\n "}, {"Page_number": 332, "text": "chapter 11. tree-based methods\n\n320\nwhere a1 and a2 are subsets of a, defined by a1 = a \u2229 {xi \u2264 c}, a2 = a \u2229 {xi > c} if\nxi is a metrically scaled predictor. the model ma specifies that the response is homogeneous\nacross the region a, whereas ma1,a2 specifies that the response is homogeneous across the\nsubsets a1 and a2. to evaluate of the improvement in fit when model ma1,a2 is fitted instead\nof model ma, one needs some measure for the goodness-of-fit of these models. a measure that\napplies if the responses are from a simple exponential family is the difference of deviances:\n\nd(ma|ma1,a2) = d(ma) \u2212 d(ma1,a2)\n\nwhere d(ma), d(ma1,a2) denote the deviances of the models ma and ma1,a2 respectively.\nsince the deviance of a model measures the discrepancy between the data and the model fit,\nthe difference d(ma|ma1,a2) represents the increase in discrepancy when model ma is fitted\ninstead of model ma1,a2. therefore, one selects the partition a1, a2, for which the difference\nin discrepancies is maximal. the procedure is equivalent to the selection of the partition a1, a2,\nwhich shows the minimal discrepancy between the data and the fit. this is easily seen by\nrewriting the criterion in the form\n\nd(ma|ma1,a2) = d(ma) \u2212 {d(ma1) + d(ma2)},\n\n(11.1)\nwhere d(mai) denotes the deviance of the model mai : \u03bc(x) = \u03bci for x \u2208 ai, fitted for\nobservation (yi, xi) with xi \u2208 ai. it should be noted that the representation of d(ma1,a2)\nas {d(ma1) + d(ma2)} is possible because disjunct subsets of observations are used. since\nd(ma) is fixed, one selects the partition a1, a2 that has the smallest deviance {d(ma1) +\nd(ma2)}. this also means that one selects the partition that shows the best fit to the data.\n\nsince ma is a submodel of ma1,a2, the difference of deviances is also a test that inves-\ntigates if model ma holds, given ma1,a2 holds. therefore, the difference is abbreviated by\nd(ma|ma1,a2). of course alternative test statistics may be used to this end. in the following\nwe consider some examples for specific responses.\n\ndichotomous responses\nfor dichotomous data, encoded by y \u2208 {1, 2} or y \u2208 {1, 0}, the data may be arranged in a\n(2 \u00d7 2)-table. with ni(aj) denoting the number of observations with response y = i within\nregion aj one obtains the table in table 11.1. the marginals are given by n(aj) = n1(aj) +\nn2(aj) and ni(a) = ni(a1) + ni(a2). the total sum n(a) = n(a1) + n(a2) is the number\nof observations with xi \u2208 a.\n\ntable 11.1: contingency table for node a, split into a1, a2.\n\ny\n\n1\n\n2\n\nmarginals\n\na1\na2\n\nn1(a1)\nn1(a2)\n\nn2(a1)\nn2(a2)\n\nn1(a)\n\nn2(a)\n\nn(a1)\nn(a2)\n\nn(a)\n\nwith \u03c0(x) = p (y = 1|x) denoting the mean, the models to be compared are ma : \u03c0(x) =\n\n\u03c0 for x \u2208 a and ma1,a2 : \u03c0(x) = \u03c0i for x \u2208 ai. the corresponding deviances are\n\nd(ma) = \u22122{n1(a) log(p(a)) + n2(a) log(1 \u2212 p(a))},\nd(mas) = \u22122{n1(as) log(p(as)) + n2(as) log(1 \u2212 p(as))},\n\n "}, {"Page_number": 333, "text": "11.1. regression and classification trees\n\nyielding\n\nd(ma|ma1,a2 ) = 2\n\n(cid:9)\n\nn(ai)\n\np(ai) log(\n\n2(cid:8)\n\ni=1\n\n) + {1 \u2212 p(ai)} log(\n\np(ai)\np(a)\n\n1 \u2212 p(ai)\n1 \u2212 p(a)\n\n321\n\n(cid:10)\n\n)\n\n,\n\n(11.2)\n\nwhere p(a) = n1(a)/n(a) and p(ai) = n1(ai)/n(ai) are the ml estimates under ma and\nma1,a2, respectively.\n\nfor given nodes a and partition a1, a2, the conditional deviance has asymptotically a \u03c72-\ndistribution with 1 df. thus the \"best\" partition may also be found by looking for the smallest\np-value amongst all partitions. alternative test statistics may be used to test the homogeneity of\nthe response within a. one may use pearson\u2019s \u03c72-statistic (see section 4.4.2) or an exact test,\nlike fisher\u2019s test. by using fishers\u2019s test one obtains exact p-values, which seems advantageous.\nthe following example demonstrates the fitting of a tree when the predictor space is one-\n\ndimensional.\n\nexample 11.1: duration of unemployment\nin example 10.1, the effects of age on the probability of short-term unemployment were investigated by\nnon-parametric modeling. in contrast to these non-parametric smooth fits, the fitting of a tree yields a\nstep function because the predictor space is partitioned into regions in which the response probability is\nheld constant. the split-points show where the probability changes. figure 11.2 shows the resulting fit\nobtained by the r package party and the corresponding tree. it is seen that the fitted tree distinguishes\nonly three regions with the first split-point at 52 and the second split point at 29 years of age.\n\nmulticategorical response\nfor multicategorical response y \u2208 {1, . . . , k} and node a, partitioned into a1, a2, cross-\nclassification is given by the contingency table in table 11.2.\n\ntable 11.2: contingency table for node a, partitioned into a1, a2.\n\n1\n\ny\n\n2\n\na1\na2\n\nn1(a1)\nn1(a2)\n\nn2(a1)\nn2(a2)\n\nn1(a)\n\nn2(a)\n\n. . .\n\n. . .\n. . .\n\n. . .\n\nk\n\nmarginals\n\nnk(a1)\nnk(a2)\n\nnk(a)\n\nn(a1)\nn(a2)\n\nn(a)\n\nas before, let pr(a) = nr(a)/n(a), pr(aj) = nr(aj)/n(aj) denote the relative fre-\n\nquencies. then, the deviances have the form\n\nand the difference of deviances is\n\nd(ma) = \u22122\n2(cid:7)\n\nnr(ai) log(pr(a))\n\nr=1\n\nn(ai)\n\nk(cid:7)\n\nnr(ai) log\n\n(cid:25)\n\n(cid:26)\n\n.\n\npr(ai)\npr(a)\n\ni=1\n\nr=1\n\nd(ma|ma1,a2) = 2\n\nthe test statistic is asymptotically \u03c72-distributed with k \u2212 1 df, as is the corresponding pearson\nstatistic. for exact tests see, for example, agresti (2002).\n\nk(cid:7)\n\n "}, {"Page_number": 334, "text": "322\n\nchapter 11. tree-based methods\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n0\n\n.\n\n2\n0\n\n.\n\n0\n0\n\n.\n\n20\n\n30\n\n40\n\nage\n\n50\n\n60\n\n1\nage\n\np < 0.001\n\n\u2264 52\n\n> 52\n\n2\nage\n\np = 0.037\n\n\u2264 29\n\n> 29\n\nnode 3 (n = 573)\n\n1\n\n1\n\n1\n\nnode 4 (n = 355)\n\n1\n\n1\n\nnode 5 (n = 54)\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n2\n\n0\n\n2\n\n0\n\n2\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n\nfigure 11.2: fitted function and tree for the probability of short-term unemployment\ndepending on age.\n\nsplitting by impurity measures\nan alternative way to obtain splitting rules is by considering the \"impurities\" of nodes. let us\nconsider a multicategorical response y \u2208 {1, . . . , k} with response probabilities \u03c01(a), . . . ,\n\u03c0k(a) within region a. then the impurity of node a may be measured, according to breiman\net al. (1984), by measures of the form\n\ni(a) = \u03c6(\u03c01(a), . . . , \u03c0k(a)),\n\nwhere \u03c6 is an impurity function, which is symmetrical in its arguments and takes its minimal\nvalue if the distribution is concentrated on one response category, that is, if one of the prob-\nabilities \u03c01(a), . . . , \u03c0k(a) takes value 1. moreover, one postulates that \u03c6(1/k, . . . , 1/k) \u2265\n\u03c6(\u03c01, . . . , \u03c0k) holds for all probabilities \u03c01, . . . , \u03c0k. a commonly used measure of this type is\ni(cid:5)=j \u03c0i\u03c0j, yielding the impurity\n\nthe gini index, which uses the impurity function \u03c6(\u03c0) = \u2212(cid:14)\n\nig(a) = 1 \u2212 k(cid:7)\n\nr=1\n\n(\u03c0r(a))2.\n\n "}, {"Page_number": 335, "text": "11.1. regression and classification trees\n\nanother is based on the entropy \u03c6(\u03c0) = \u2212(cid:14)\nie(a) = \u2212 k(cid:7)\n\nr \u03c0r log(\u03c0r), yielding the impurity\n\n\u03c0r(a) log(\u03c0r(a)).\n\n323\n\nr=1\n\nit is easily seen that both measures take value zero if one of the probabilities \u03c01(a), . . . , \u03c0k(a)\nhas value 1 (and the rest 0). the node is considered as \"purest\" because the probability mass is\nconcentrated within one response category. consequently, the impurity takes its smallest value,\nnamely, zero. for a uniform distribution across response categories, \u03c01(a) = . . . = \u03c0k(a) =\n1/k, the impurity takes its maximal value.\n\nhaving defined what impurity means, we now can consider criteria that are apt to reduce\n\nimpurities by splitting. the decrease in impurities may be measured by\n\u03b4i(a|a1, a2) = i(a) \u2212 {w1i(a1) + w2i(a2)},\n\n(11.3)\n\nwhere w1, w2 are additional weights that add up to one. by selecting the split that has minimal\nimpurity w1i(a1) + w2i(a2), one maximizes the decrease in impurity.\n\nthe measures considered so far are population versions of impurity, defined for probabilities\n\u03c01(a), . . . , \u03c0k(a). in the empirical versions one replaces the probabilities \u03c01(a), . . . , \u03c0k(a)\nby relative frequencies \u02c6\u03c01(a) = n1(a)/n(a), . . . , \u02c6\u03c0k(a) = nk(a)/n(a) with nr(a), n(a)\ntaken from table 11.2. for example, the empirical version of the gini-based impurity has the\nform\n\nig,emp(a) = 1 \u2212 k(cid:7)\n\n(nr(a)/n(a))2.\n\nthe weights in (11.3) are typically chosen by wi = n(ai)/n(a) and measure the proportion of\nobservations in ai and observations in a. if impurity is measured by entropy, one obtains for\nthe empirical difference\n\n\u03b4ie ,emp =\n\n2n(a) d(ma).\n\nsince n(a) is fixed, the maximization of the decrease in impurity measured by entropy is\nequivalent to the deviance criterion (11.1). therefore, for a given a, the deviance criterion may\nbe seen as a special case of impurity minimization.\n\nthe criterion (11.3) is a local criterion that steers the selection in one step of the algorithm.\nit may also be linked to the concept of impurity of the whole tree. let the empirical version of\nimpurity of a tree be defined by\n\nr=1\n\n1\n\n(cid:7)\n\ni(tree) =\n\n\u02c6p(a)iemp(a),\n\nterminal nodes a\n\nwhere \u02c6p(a) = n(a)/n is an estimate of the probability that an observation reaches node a and\niemp(a) is the empirical version of i(a), where the probabilities \u03c0r(a) have been replaced by\n\u02c6\u03c0r(a) = nr(a)/n(a). if node a is split into a1 and a2, the difference in impurity is\ni(tree before splitting of a) \u2212 i(tree with splitting into a1, a2) = \u02c6p(a)\u03b4i(a|a1, a2).\ntherefore, for a fixed a, a maximal decrease of impurity of a by splitting maximally decreases\nthe impurity of the tree. when using the entropy one obtains for the impurity of the tree\n\n(cid:25)\n\n(cid:26)\n\n(cid:7)\n\nk(cid:7)\n\nterminal nodes\n\nr=1\n\ni(tree) = \u2212 1\nn\n\nnr(a) log\n\nnr(a)\nn(a)\n\n= \u2212 1\n2n\n\nd(tree),\n\n "}, {"Page_number": 336, "text": "324\n\nchapter 11. tree-based methods\n\nwhere d(tree) is the deviance of the partition generated by the tree. the deviance d(tree)\ncompares the fit of the \"model\" partition into final nodes to the (ungrouped) saturated model\nfor the training set; see ciampi et al. (1987) and clark and pregibon (1992).\n\n11.1.3 size of a tree\nwhen growing a tree the impurity of the tree decreases and the partition gets finer. therefore, it\nmight be tempting to grow a rather large tree. however, there is a severe danger of overfitting. a\ntree with terminal nodes that contain only few observations often generalizes badly. prediction\nin future datasets is worse than for moderate-sized trees. therefore, the growing of a tree has\nto be stopped at an appropriate size. simple step criteria are\n\n- stop if a node contains fewer than nstop observations.\n\n- stop if the splitting criterion is above or below a fixed threshold (for example, if the\n\np-value is above pstop or the improvement in decrease is below a fixed value).\n\nan alternative strategy to obtain a tree of an appropriate size is to grow a very large tree and\nthen prune the tree. growing a large tree typically means stopping when some small minimal\nnode size (say 3 observations in the node) is reached. a procedure for tree pruning proposed\nby breiman et al. (1984) is based on a cost complexity criterion of the form\n\nc\u03b1(t ) = c(t ) + \u03b1|t|,\n\nwhere |t| is the number of terminal nodes, also called the size of tree t , \u03b1 \u2265 0 is a tuning pa-\nrameter, and c(t ) is a measure of the goodness-of-fit or the performance of the partition given\nby the terminal nodes. candidates for c(t ) are the deviance or the entropy of the partition or\nas a measure of performance of the prediction on the training set or a validation set. the tuning\nparameter \u03b1 may be chosen by m-fold cross-validation or by the prediction in a validation set.\nit governs the trade-off between tree size and closeness to the data. for large \u03b1 one obtains a\nsmall tree, whereas \u03b1 = 0 yields very large trees.\n\npruning a tree t means that the nodes are collapsed, thereby obtaining a subtree of t or,\nmore precisely, a rooted subtree, which means that the subtree has the same root as t . it may\nbe shown that for each \u03b1 there is a unique smallest subtree that minimizes c\u03b1(t ). the trees\nmay be found by successively collapsing the nodes, yielding a finite sequence of trees that are\noptimal with respect to c\u03b1(t ) for specific intervals of \u03b1-values. for details see breiman et al.\n(1984) or ripley (1996), chapter 7.\n\nan alternative, quite attractive way to determine the size of a tree was proposed by hothorn\net al. (2006). they proposed a unified framework for recursive partitioning that embeds tree-\nstructured regression models into a well-defined theory of conditional inference procedures.\nthe splitting is stopped when the global null hypothesis of independence between the response\nand any of the predictors cannot be rejected at a pre-specified nominal significance level \u03b1.\nthe method explicitly accounts for the involved multiple test problem. by separating variable\nselection and splitting procedure one arrives at an unbiased recursive partitioning scheme that\nalso avoids the selection bias toward predictors with many possible splits or missing values\n(for selection bias see also section 11.1.5). since the method employs p-values for variable\nselection, it does not rely on pruning.\n\nexample 11.2: exposure to dust\nin example 11.2 only the effects of duration on bronchitis were examined. now we include the con-\ncentration of dust and smoking. we fit a conditional independence tree by using the ctree function of\n\n "}, {"Page_number": 337, "text": "11.1. regression and classification trees\n\n325\n\nthe r package party (hothorn et al., 2006). in each node a significance test on independence between\nany of the predictors and the response is performed and a split is established when the p-value is smaller\nthan a pre-specified significance level. figure 11.3 shows the resulting tree. the first split distinguishes\nbetween lesser and more than 15 years of exposure. when exposure was for more than 15 years it is\ndistinguished between smokers and non-smokers. subpopulations are split further with respect to years\nand concentration of exposure.\n\n1\n\nyears\n\np < 0.001\n\n\u2264 15\n\n> 15\n\n3\n\nsmoke\n\np = 0.002\n\n\u2264 0\n\n> 0\n\n4\n\nyears\n\np = 0.035\n\n7\n\ndust\n\np < 0.001\n\n\u2264 37\n\n> 37\n\n\u2264 4.81\n\n> 4.81\n\nnode 2 (n = 308)\n1\n\n0\n\nnode 5 (n = 177)\n1\n\n0\n\nnode 6 (n = 63)\n1\n\n0\n\nnode 8 (n = 447)\n1\n\n0\n\nnode 9 (n = 251)\n1\n\n0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n1\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\nfigure 11.3: trees for exposure data depending on years of exposure.\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n\n11.1.4 advantages and disadvantages of trees\nthe major advantages of trees, which made them popular, are the following:\n\n\u2022 trees are easy to interpret and the visualization makes it easy to communicate the un-\nderlying structure to practitioners. practitioners can understand what model is fitted and,\ndepending on their statistical experience, are often happier with trees than with (general-\nized) linear models.\n\n "}, {"Page_number": 338, "text": "326\n\nchapter 11. tree-based methods\n\n\u2022 trees provide a non-parametric modeling approach with high flexibility.\n\n\u2022 trees have a built-in interaction detector. by splitting successively in different variables,\n\ninteraction effects between two and more variables are captured.\n\n\u2022 trees are invariant to monotone transformations of predictor variables. this is a big\nadvantage over most parametric models, where it seriously matters if predictors are, for\nexample, log-transformed or not.\n\n- trees, in particular in the form of random trees (see section 15.5.2), are among the best\navailable classifiers with high prediction accuracy. averaging over trees makes powerful\npredictors.\n\u2022 trees are not restricted to the case of few predictors. even in the p (cid:13) n case, when\none has more predictors than observations, trees apply, because they automatically select\nvariables. the implicit selection of variables may be seen as an advantage by itself.\nthus, trees turn out to be very useful in high-dimensional problems like the modeling of\ngene expression data, where one has few observations but sometimes several thousands\nof predictors.\n\n\u2022 missing values are easy to handle.\n\nof course trees also have disadvantages:\n\n- if the underlying structure is linear or, more generally, additive, many splits are needed\nto approximate that structure. the high number of splits typically will not be reached\nbecause it might appear to be overfitting.\n\n- when the predictor is one-dimensional, the fitted structure is not smooth. one obtains a\n\nstep function that is visually not very pleasing.\n\n- trees are often unstable. a small change in the data might result in very different splits.\nsince the growing is a hierarchical process, one different split is propagated down the\ntree, resulting in quite different trees. even more stable split criteria are not a remedy to\nthe problem. a consequence is that interpretation should be done very cautiously. trees\nare more an exploratory technique than a stable model.\n\n- trees provide only a rough approximation of the underlying structure.\n\n11.1.5 further issues\nprediction\n(cid:14)\nwhen a tree has been grown and one has decided on its size, a response is predicted in each ter-\nminal node. one uses all the observations in the terminal node a to obtain the average response\nvalue ya = ave(yi|xi \u2208 a) in regression and the majority vote ya = argmaxj(\ni i(yi =\nj|xi \u2208 a)) in classification trees.\n\nrandom forests\nrandom forests are a combination of many trees that show much better performance in predic-\ntion than single trees. they are composed as so-called ensemble methods, which means that\nvarious predictors are aggregated for prediction. random methods are considered in the context\nof prediction (section 15.5.2).\n\n "}, {"Page_number": 339, "text": "11.1. regression and classification trees\n\n327\n\nmissing values\nwhen considering the problem of missing values, one should distinguish between the fitting\nprocedure and the application on future samples. as far as the fitting procedure is concerned,\ntrees can easily handle missing values since splitting uses only one variable at a time. therefore,\nin an available case strategy one uses all the observations that are available for the variable to\nbe split. when predictor values are missing in future data one uses surrogate variables that best\nmimic the split in the training data achieved by the selected variable.\n\nselection bias\nwhen the predictor variables vary in the number of distinct observations, for example, due\nto missing values, variables are systematically preferred or penalized. loh and shih (1997)\ndemonstrated that predictor variables with more distinct values are systematically preferred over\nvariables with less distinct values when pearson\u2019s \u03c72-statistic is used for continuous predictor\nand binary outcomes.\nin contrast, loh and shih (1997) showed that in classification trees\nselection is biased toward selection of variables with a lower number of distinct values, caused\nby missing values, if the gini index is used as a split criterion. strobl et al. (2007) showed\nthat partially counteracting effects are at work when the variables vary in the number of distinct\nvalues. one effect is due to multiple comparisons. when a variable has more categories, more\nbinary partitions are evaluated and consequently variables with many categories are preferred\nover variables with a smaller number of possible values. a quite different effect is the bias\ntoward variables with many missing values that has been observed for metric variables. the\neffect is somewhat counterintuitive because many missing values means that the number of\npossible splits is reduced. of course splitting is based on the common available case strategy,\nwhich means that for each variable one uses the data that have been observed for that variable.\nsince the number of missing values may vary strongly across variables, the resulting selection\nbias may be severe. strobl et al. (2007) investigated the effects for trees obtained by gini-based\nsplits by investigating bias and variance effects. in the binary response case the impurity based\non the gini index is given by ig(a) = 2\u03c01(a)(1 \u2212 \u03c01(a)). the empirical version has the\nform ig,emp = 2\u02c6\u03c01(a)(1\u2212 \u02c6\u03c01(a)), where \u02c6\u03c01(a) = n1(a)/n(a). it turns out that the mean is\ngiven by e(ig,emp) = {(n(a)\u22121)/n(a)}ig(a), which means that the underlying impurity is\nunderestimated. the effect carries over to the empirical decrease of impurity. it may be shown\nthat e(\u03b4ig,emp(a|a1, a2)) = ig(a)/n(a). this means that for uninformative variables,\nwhere the expectation should be zero, but with an underlying distinctly positive gini index\nig, the decrease in impurity is a biased estimate. the bias is small for a large sample size\nbut becomes influential if n(a) is small, which occurs when many observations are missing.\ntherefore, selection by the difference in impurity tends to select uninformative variables when\nthe number of available observations for that variable is small. selection bias may be avoided\nby using unbiased classification trees like the ones proposed by hothorn et al. (2006).\n\nmaximally selected statistics\nto avoid selection bias one should separate variable selection from cut-point selection.\nin\nparticular, the comparison of test statistics across variables and cut-points yields biased results\nsince different numbers of splits are invoked for different variables. one approach to separate\nvariable selection from cut-point selection is to compute p-values for each variable by using the\nmaximally selected statistic. the basic idea behind maximally selected statistics is to consider\nthe distribution of the selection process. when a split-point is selected based on a test statistic\nor association measure ti for all the possible split-points i = 1, . . . , m, one investigates the\ndistribution of tmax = maxi=1,...,mti. the p-value of the distribution of tmax provides a\n\n "}, {"Page_number": 340, "text": "328\n\nchapter 11. tree-based methods\n\nmeasure for the relevance of a predictor that does not depend on the number of split-points\nsince the number has been taken into account.\n\nthe maximally selected statistic approach was proposed by miller and siegmund (1982)\nand has been extended by hothorn and lausen (2003), shih (2004), and shih and tsai (2004).\nstrobl et al. (2007) extended the approach to gini-based splitting rules.\n\n11.2 multivariate adaptive regression splines\nmultivariate adaptive regression splines (mars), introduced by friedman (1991), may be seen\nas a modification of tree methodology. the first split in carts can be seen as fitting a model\nwith predictor\n\n\u03b7(x) = \u03b31i((xj \u2264 cj) + \u03b32i((xj > cj),\n\nwhere the variable xj is split at split-point cj. in the next step, one of the regions, {xj \u2264 cj} or\n{xj > cj}, is split further. if {xj \u2264 cj} is split by use of variable xl, the fitted model has the\nform\n\n\u03b7(x) = \u03b31i((xj \u2264 cj)i((xl \u2264 cl) + \u03b32i((xj \u2264 cj)i((xl > cl) + \u03b33i((xj > c),\n\nwhere, of course, the denotation of the \u03b3-parameters has changed. therefore, cart method-\nology fits a model of the form\n\nm(cid:7)\n\n\u03b7(x) =\n\n\u03b3ihi(x),\n\ni\n\nwhere the functions hi(x) are built as products of simple (indicator) functions that depend\non only one component of the vector x. the functions themselves are found adaptively by\nrecursive partitioning.\n\nin mars the products of indicator functions are replaced by piecewise linear basis func-\n\ntions given by\n\n(x \u2212 t)+ and\n\n(t \u2212 x)+,\n\nwhere the subscript denotes the positive part, that is, (a)+ = a if a > 0 and 0 otherwise. both\nfunctions, (x \u2212 t)+ and (t \u2212 x)+, are piecewise linear with a knot at the value t and are called\na reflected pair. the observation values of the corresponding variable are used as knots. as in\ncarts, the given \"model\" is enlarged by adding products with basis functions, but in mars\nthe used basis functions are reflected pairs. at each stage one adds to the given model a term\nthat is constructed from former basis functions hi(x) in the form\n\n\u03b31,newhi(x)(xj \u2212 t)+\u03b32,newhi(x)(t \u2212 xj)+.\n\nthus a pair of new basis functions is added. the new model is fit by common ml methods for\nlinearly structured predictors. selection of the basis function hi(x) that is enlarged to products,\nthe variable that is used, and the knot are all based on goodness-of-fit.\n\nthe procedure typically starts with the basis function h1(x) = 1. then, in the first step, it\n\nis enlarged to\n\nwhere h2(x) = (xj \u2212 tj)+, h3(x) = (tj \u2212 xj)+. in the next step one adds a term of the form\n\n\u03b31h1(x) + \u03b32h2(x) + \u03b33h3(x),\n\n\u03b34hi(x)(xl \u2212 tl)+\u03b35hi(x)(tl \u2212 xl)+,\n\n "}, {"Page_number": 341, "text": "11.4. exercises\n\n329\n\nwhere hi(x) is any previously used function h1(x), h2(x) or h3(x). in general, one adds a\npair of basis functions that are built as products from basis functions that were already in the\nmodel. thus multiway products are always built from previous basis functions.\n\ntypically not all possible products are allowed. the inclusion of a pair of basis functions\nis restricted such that a predictor can appear at most once in a product. otherwise higher\norder powers of a predictor, which would make the procedure more unstable, could occur.\nan additional optional restriction is often useful. by setting an upper limit on the order of\ninteraction one obtains a fit that has an easier interpretation. for example, if the upper limit is\ntwo, only basis functions that contain products of two piecewise linear functions are allowed\nin the predictor term. the resulting fit includes the interaction between only two predictors. it\nmay be seen as a less structured form of interaction modeling than the approaches considered\nin section 10.3.3.\n\naside from the use of piecewise linear functions instead of indicator functions the main\ndifference from carts is that, in carts, given basis functions are always modified in one\nstep of the fitting procedure, whereas in mars new basis functions are added to the given\nbasis functions. therefore mars does not yield a simple tree like cart does.\n\nsimilar as in trees, mars may be based on increasing the predictor and subsequent pruning\n(friedman, 1991). stone et al. (1997) considered a modification of mars called polymars,\nwhich allows for a multidimensional response variable and defines an allowable space by listing\nits basis functions. stone et al. (1997) gave a more general treatment of polynomial splines and\ntensor products within an approach called extended linear modeling.\n\n11.3 further reading\nbasic literature and overviews. the roots of classification and regression trees are in automatic\ninteraction detection, as proposed by morgan and sonquist (1963). the most popular methods\nare cart, outlined in breiman et al. (1984), and the c4.5 algorithm (and its predecessor id3),\nwhich were proposed by quinlan (1986, 1993). the main difference between the approaches\nis that c4.5 produces for categorical variables as many nodes as there are categories (multiple\nor k-ary splitting) whereas cart uses binary splitting rules. zhang and singer (1999) gave an\noverview on recursive partitioning in the health sciences, and strobl, malley, and tutz (2009)\ngave an introduction including random forests with applications in psychology.\n\nmultiple and ordinal regression. an extension of classification trees to multiple binary\noutcomes was given by zhang (1998). piccarreta (2008) considered classification trees for\nordinal response variables.\n\nhigh-dimensional predictors. trees and random forests have been successfully used in\ngenetics; see, for example, diaz-uriarte and de andres (2006a), lunetta et al. (2004), and\nhuang et al. (2005).\n\nr packages. trees can be fitted by using the function rpart from the package rpart. con-\nditional unbiased recursive partitioning is available in the function ctree of the package party\n(hothorn et al., 2006). the function cforest from the same package fits random forests, which\nare also available in randomforest. mars can be fitted by use of mars from mda.\n\n11.4 exercises\n\nentropy \u03c6(\u03c0) = \u2212(cid:2)\n\n11.1 for which values of \u03c01, . . . , \u03c0k,\n\n(cid:2)\n\ni \u03c0i = 1, do the gini index \u03c6(\u03c0) = \u2212(cid:2)\n\ni(cid:4)=j \u03c0i\u03c0j and the\n\nr \u03c0r log(\u03c0r), \u03c0t = (\u03c01, . . . , \u03c0k) take their maximal and minimal values?\n\n "}, {"Page_number": 342, "text": "330\n\nchapter 11. tree-based methods\n\n11.2 fit a tree for the dust data (package catdata) that investigates the effect of duration of exposure on\nbronchitis and compare it with the fit of a logit model with the linear and quadratic effects of duration of\nexposure.\n\n11.3 fit a tree for the heart disease data considered in example 6.3 (dataset heart from package catdata)\nand discuss the results in comparison to the fit of a linear logistic model.\n\n "}, {"Page_number": 343, "text": "chapter 12\n\nthe analysis of contingency tables:\nlog-linear and graphical models\n\ncontingency tables, or cross-classified data, come in various forms, differing in dimensions,\ndistributional assumptions, and margins. in general, they may be seen as a structured way of\nrepresenting count data. they were already used to represent data in binary and multinomial\nregression problems when explanatory variables were categorical (chapters 2 and 8). also,\ncount data with categorical explanatory variables (chapter 7) may be given in the form of\ncontingency tables.\n\nin this chapter log-linear models are presented that may be seen as regression models or\nassociation models, depending on the underlying distribution. three types of distributions are\nconsidered: the poisson distribution, the multinomial, and the product-multinomial distribution.\nwhen the underlying distribution is a poisson distribution, one considers regression problems\nas in chapter 7. when the underlying distribution is multinomial, or product-multinomial, one\nhas more structure in the multinomial response than in the regression problems considered in\nchapters 2 and 8. in those chapters the response is assumed to be multinomial without further\nstructuring, whereas in the present chapter the multinomial response arises from the consid-\neration of several response variables that together form a contingency table. then one wants\nto analyze the association between these variables. log-linear models provide a common tool\nto investigate the association structure in terms of independence or conditional independence\nbetween variables. several examples of contingency tables have already been given in previous\nchapters. two more examples are the following.\n\nexample 12.1: birth data\nin a survey study several variables have been collected that are linked to the birth process (see also\nboulesteix, 2006). table 12.1 shows the data for the variables gender of the child (g, 1: male; 2: fe-\nmale), if membranes did rupture before the beginning of labor (m, 1: yes; 0: no), if a cesarean section has\nbeen applied (c, 1: yes, 0: no) and if birth has been induced (i, 1: yes; 0: no). the association between\nthe four variables is unknown and shall be investigated.\n\nexample 12.2: leukoplakia\ntable 12.2 shows data from a study on leukoplakia, which is a clinical term used to describe patches of\nkeratosis visible as adherent white patches on the membranes of the oral cavity. it shows the alcohol\nintake in grams of alcohol, smoking habits, and presence of leukoplakia. the objective is the analysis of\nthe association between disease and risk factors (data are taken from hamerle and tutz, 1980).\n\n331\n\n "}, {"Page_number": 344, "text": "332\n\nchapter 12. the analysis of contingency tables\n\ntable 12.1: contingency table for birth data with variables gender (g), membranes (m),\ncesarean section (c), and induced birth (i).\n\ninduced\n\ngender membranes\n\ncesarean\n\n0\n\n1\n\n2\n\n0\n\n1\n\n0\n\n1\n\n0\n1\n0\n1\n0\n1\n0\n1\n\n177\n37\n104\n9\n137\n24\n74\n8\n\n1\n\n45\n18\n16\n7\n53\n12\n15\n2\n\ntable 12.2: contingency table for oral leukoplakia.\n\nleukoplakia (a)\n\nalcohol\n\nsmoker\n\nyes\n\nno\n\nno\n\n0 g, 40 g\n\n40 g, 80 g\n\n> 80 g\n\nyes\nno\nyes\nno\nyes\nno\nyes\nno\n\n26\n8\n38\n43\n4\n14\n1\n3\n\n10\n8\n8\n24\n1\n17\n0\n7\n\n12.1 types of contingency tables\nin particular, three types of contingency tables and their corresponding scientific questions will\nbe studied. the first type of contingency table occurs if cell counts are poisson-distributed\ngiven the configuration of the cells. then the counts themselves represent the response, and\nthe categorical variables that determine the cells are the explanatory variables. for example,\nin table 1.3 in chapter 1, the number of firms with insolvency problems may be considered as\nthe response while the year and month represent the explanatory variables. the total number of\ninsolvent firms is not fixed beforehand and is itself a realization of a random variable.\n\nin the second type of contingency table a fixed number of subjects is observed and the cell\ncounts represent the multivariate response given the total number of observations. the common\nassumption is that cell counts have a multinomial distribution. contingency tables of this type\noccur if a fixed number of individuals is cross-classified with respect to variables like gender\nand preference for distinct political parties (see table 8.1 in chapter 8). the analysis for this\ntype of contingency table may focus on the association between gender and preference for\nparties. alternatively, one might be interested in modeling the preference as the response given\ngender as the explanatory variable.\n\nthe third type of contingency table is found, for example, in clinical trials. table 9.1 shows\ncross-classified data that have been collected by randomly allocating patients to one of two\ngroups, a treatment group and a group that receives a placebo. after 10 days of treatment\nthe pain occuring during movement of the knee is assessed on a five-point scale. the natural\n\n "}, {"Page_number": 345, "text": "12.1. types of contingency tables\n\n333\n\nresponse in this example is the level of pain given the treatment group. the number of people\nin the two groups is fixed, while the counts themselves are random variables. the level of\npain, given the treatment group, is a multivariate response, usually modeled by a multinomial\ndistribution.\nin general, two-way (i \u00d7 j)-contingency tables with i rows and j columns may be de-\n\nscribed by\n\nxij\nxa \u2208 {1, . . . , i}\nxb \u2208 {1, . . . , j}\n\n= counts in cell\n\n(i, j),\nrepresenting the rows,\nrepresenting the columns.\n\nthe observed contingency table has the form\n\nxb\n\n1\n\n2\n1 x11 x12\n...\n2 x21\n...\ni xi1\nx+1\n\n. . .\n. . .\n\n(cid:14)\n\nj\n\n. . .\n. . . x1j x1+\n...\n\n...\n...\nxij xi+\nx+j\n\n...\n\nxa ...\n\n(cid:14)\n\nwhere xi+ =\ndenotes the sum over that index.\n\nj=1 xij, x+j =\n\nj\n\n\u02c6ixij denote the marginal counts. the subscript \"+\"\n\ni=1\n\nthe three types of contingency tables may be distinguished by the distribution that is being\n\nassumed.\n\ntype 1: poisson distribution (number of insolvent firms)\nit is assumed that x11, . . . , xij are independent poisson-distributed random variables, xij \u223c\np (\u03bbij). the total number of counts x11 + \u00b7\u00b7\u00b7 + xij as well as the marginal counts are\nrandom variables. the natural model considers the counts as the response and xa and xb as\nexplanatory variables.\n\ntype 2: multinomial distribution (gender and preference for political parties)\nfor a fixed number of subjects one observes independently the response tupel (xa, xb) with\npossible outcomes {(1, 1), . . . , (i, j)}. the cells of the table represent the ij possible out-\ncomes. the resulting cell counts follow a multinomial distribution (x11, . . . , xij) \u223c m(n,\n(\u03c011, . . . , \u03c0ij)), where \u03c0ij = p (xa = i, xb = j) denotes the probability of a response in\ncell (i, j). the probabilities {\u03c0ij}, or, in vector form, \u03c0t = (\u03c011, . . . \u03c0ij), represent the joint\ndistribution of xa and xb.\n\ntype 3: product-multinomial distribution (treatment and pain)\nin contrast to type 2, now one set of marginal counts is fixed. in the treatment and pain example\nthe row tables are fixed by n1 = x1+, n2 = x2+. given the treatment group, one observes for\neach individual the response xb \u2208 {1, . . . , j}. the cell counts for the given treatment group i\nfollow a multinomial distribution:\n\n(xi1, . . . , xij) \u223c m(ni, (\u03c0i1, . . . \u03c0ij)),\n\nwhere \u03c0ij now denotes the conditional probability \u03c0ij = p (xb = j|xa = i). the cell counts\nof the total contingency table follow a product-multinomial distribution. the natural modeling\n\n "}, {"Page_number": 346, "text": "334\n\nchapter 12. the analysis of contingency tables\n\nis to consider xb as the response variable and xa as the explanatory variable. this modeling\napproach follows directly from the design of the study. of course, if the column totals are fixed,\nthe natural response is xa with xb as the explanatory variable.\n\nmodels and types of contingency tables\nthe three types of contingency tables differ by the way the data are collected. when considering\ntypical scientific questions, to be investigated by the analysis of contingency tables, one finds a\nhierarchy within the types of tables. while the poisson distribution is the most general, allowing\nfor various types of analyses, the product-multinomial contingency table is the most restrictive.\nthe hierarchy is due to the possible transformations of distributions by conditioning.\n\npoisson and multinomial distributions\nlet xij, i = 1, . . . , i, j = 1, . . . , j follow independent poisson distributions, xij \u223c p (\u03bbij).\nthen the conditional distribution of (x11, . . . , xij) given n =\nij xij is multinomial. more\nconcrete, one has\n\n(cid:14)\n\n(x11, . . . , xij)|\n\nxij = n \u223c m(n, ( \u03bb11\n\n\u03bb\n\n, . . . ,\n\n\u03bbij\n\u03bb\n\n)),\n\n(cid:14)\n\nwhere \u03bb =\nij \u03bbij. therefore, given that one has poisson-distributed cell counts, by condi-\ntioning one obtains a structured multinomial distribution that is connected to the response tupel\n(xa, xb). hence, by conditioning on n one may study the response (xa, xb), its marginal\ndistribution, and the association between (xa, xb).\n\n(cid:7)\n\ni,j\n\nmultinomial and product-multinomial distributions\nlet (x11, . . . , xij) have multinomial distribution, (x11, . . . , xij) \u223c m(n, (\u03c011, . . . , \u03c0ij)).\nby conditioning on the row margins ni+ =\nj xij one obtains the product-multinomial dis-\ntribution with probability mass function\n\nf(x11, . . . , xij) =\n\n(cid:14)\n\nni+!\n\nxi1! . . . xij! \u03c0xi1\n\n1|i . . . \u03c0xij\nj|i ,\n\nwhere \u03c0j|i = \u03c0ij/\nmultinomial distribution:\n\nj \u03c0ij = \u03c0ij/\u03c0i+. thus the cell counts of one row given ni+ have a\n(xi1, . . . , xij) \u223c m(ni+, (\u03c01|i, . . . , \u03c0j|i))\nand the i multinomials corresponding to the rows are independent.\n\nif the poisson distribution generates the counts in the table, one may consider the counts\ngiven xa and xb within a regression framework, or one may condition on the total sample size\nn and model the marginal distribution and the association of xa and xb based on the multi-\nnomial distribution. one may also go one step further and condition on the marginal counts\nof the rows (columns) and consider the regression model where xb (xa) is the response and\nxa (xb) is the explanatory variable. if the multinomial distribution generates the contingency\ntable, one may consider (xa, xb) as the response or choose one of the two variables by con-\nditioning on the other one. in this sense the poisson distribution contingency table is the most\nversatile. since the poisson, multinomial, and product-multinomial distributions may be treated\nwithin a general framework, in the following \u03bcij = e(xij) is used rather than n\u03c0ij (or ni\u03c0ij),\nwhich would be more appropriate for the multinomial (or product-multinomial) distribution. in\ntable 12.3 we summarize the types of distributions and the modeling approaches.\n\n(cid:14)\ni(cid:15)\n\ni=1\n\n "}, {"Page_number": 347, "text": "12.2. log-linear models for two-way tables\n\n335\n\ntable 12.3: types of two-way contingency tables and modeling approaches.\n\npoisson\ndistribution\n\nmultinomial\ndistribution\n\nregression (xa, xb) \u2192 counts\n\nassociation between xa and xb (conditional on n)\nregression xa \u2192 xb (conditional on xi+)\nregression xb \u2192 xa (conditional on x+j)\n\nassociation between xa and xb (conditional on n)\nregression xa \u2192 xb (conditional on xi+)\nregression xb \u2192 xa (conditional on x+j)\n\nproduct-multinomial\ndistribution\nxi+ fixed\nx+j fixed\n\nregression xa \u2192 xb\nregression xb \u2192 aa\n\n12.2 log-linear models for two-way tables\nconsider an (i \u00d7 j)-contingency table {xij}. let \u03bcij = e(xij) denote the mean, where\n\u03bcij = n\u03c0ij = np (xa = i, xb = j) for the multinomial distribution, \u03bcij = ni+p (xb =\nj xij, and \u03bcij = n+jp (xa = i|xb = j) if one\nj|xa = i) if one conditions on ni+ =\nconditions on n+j =\ni xij. the general log-linear model for two-way tables has the form\n\n(cid:14)\n\n(cid:14)\n\nlog(\u03bcij) = \u03bb0 + \u03bba(i) + \u03bbb(j) + \u03bbab(ij)\n\n(12.1)\n\nor, equivalently,\n\n\u03bcij = e\u03bb0 e\u03bba(i) e\u03bbb(j)e\u03bbab(ij) .\n\nsince model (12.1) contains too many parameters, identifiability requires constraints on the\nparameters. two sets of constraints are in common use, the symmetrical constraints and con-\nstraints that use a baseline parameter:\n\nsymmetrical constraints\n\ni(cid:7)\n\nj(cid:7)\n\ni(cid:7)\n\ni=1\n\n\u03bbb(j) =\n\n\u03bbab(ij) =\n\nj(cid:7)\n\nj=1\n\n\u03bbab(ij) = 0\n\nfor all i, j.\n\n\u03bba(i) =\n\ni=1\n\nj=1\n\nbaseline parameters set to zero\n\n\u03bba(i) = \u03bbb(j) = \u03bbab(ij) = \u03bbab(ij) = 0 for all i, j.\n\nthe symmetrical constraints are identical to the constraints used in analysis-of-variance\n(anova). in anova the dependence of a response variable on categorical variables, called\nfactors, is studied. in particular, one is often interested in interaction effects. there is a strong\nsimilarity between anova and poisson contingency tables, where the counts represent the re-\nsponse and the categorical variables xa and xb form the design. the main difference is that\nanova models assume a normal distribution for the response whereas in log-linear models for\ncount data the response is integer-valued.\n\nthese sets of constraints are closely related to the coding of dummy variables. symmetrical\nconstraints refer to effect coding whereas the choice of baseline parameters is equivalent to\n\n "}, {"Page_number": 348, "text": "336\n\nchapter 12. the analysis of contingency tables\n\nchoosing a reference category in dummy coding (see section 1.4.1). model (12.1) may be also\nwritten with dummy variables, yielding\n\nlog(\u03bcij) = \u03bb0 + \u03bba(1)xa(1) + \u00b7\u00b7\u00b7 + \u03bba(i\u22121)xa(i\u22121)\n+ \u03bbb(1)xb(1) + \u00b7\u00b7\u00b7 + \u03bbb(j\u22121)xb(j\u22121)\n+ \u03bbab(1,1)xa(1)xb(1) + \u00b7\u00b7\u00b7 + \u03bbab(i\u22121,j\u22121)xa(i\u22121)xb(j\u22121),\n\nwhere xa(1), . . . are dummy variables coding a = i and xb(1), . . . are dummy variables cod-\ning b = j. this form is usually too clumsy and will be avoided. however, it is easily seen that\nthe effect coding of dummy variables is equivalent to the symmetric constraints, and choosing\n(xa = i, xb = j) as reference categories in dummy coding is equivalent to using baseline\nparameters. one should keep in mind that baseline parameters that refer to reference categories\nmay be chosen arbitrarily; different software use different constraints.\n\nthe sets of constraints given above apply for poisson distribution tables. for multinomial\nij xij = n\n\nand product-multinomial tables, additional constraints are needed to ascertain that\nj xij = ni+ (product-multinomial, fixed row sums) hold:\n(multinomial) and\n\n(cid:14)\n\n(cid:14)\n\nadditional constraint for multinomial tables\n\n(cid:7)\n\ni,j\n\ne\u03bb0 e\u03bba(i) e\u03bbb(j)e\u03bbab(ij) = n.\n\nadditional constraints for product-multinomial tables\n\nj(cid:7)\n\ne\u03bb0 e\u03bba(i) e\u03bbb(j)e\u03bbab(ij) = ni+,\n\ni = 1, . . . , i\n\n(for ni+ fixed),\n\nj=1\n\ni(cid:7)\n\ne\u03bb0 e\u03bba(i) e\u03bbb(j)e\u03bbab(ij) = n+j,\n\nj = 1, . . . , j (for n+j fixed).\n\ni=1\n\ntable 12.4: log-linear model for two-way tables.\n\nlog-linear model for two-way tables\n\nlog(\u03bcij) = \u03bb0 + \u03bba(i) + \u03bbb(j) + \u03bbab(i,j)\n\nconstraints:\n\ni(cid:7)\n\ni=1\n\n\u03bba(i) =\n\nj(cid:7)\n\nj=1\n\n\u03bbb(j) =\n\ni(cid:7)\n\ni=1\n\n\u03bbab(ij) =\n\nj(cid:7)\n\nj=1\n\n\u03bbab(ij) = 0\n\nor\n\n\u03bba(i) = \u03bbb(j) = \u03bbab(ij) = \u03bbab(ij) = 0\n\nmodel (12.1) is the most general model for two-way contingency tables, the so-called satu-\nrated model. it is saturated since it represents only a reparameterization of the means {\u03bcij}, and\n\n "}, {"Page_number": 349, "text": "12.2. log-linear models for two-way tables\n\n337\n\nany set of means {\u03bcij} (\u03bcij > 0) may be represented by the parameters \u03bbb, \u03bba(i), \u03bbb(j), \u03bbab(ij),\ni = 1, . . . , i, j = 1, . . . , j.\n\nconsequently, not much insight is gained by considering the saturated log-linear model.\n\nthe most important submodel is the log-linear model of independence,\n\nlog(\u03bcij) = \u03bb0 + \u03bba(i) + \u03bbb(j),\n\n(12.2)\nwhere it is assumed that \u03bbab(ij) = 0. this is no longer a saturated model because it implies\nsevere restrictions on the underlying regression or association structure. the restriction has\ndifferent meanings, depending on the distribution of the cell counts xij. for the poisson dis-\ntribution it simply means that there is no interaction effect of the variables xa and xb when\neffecting on the cell counts. for the multinomial model it is helpful to consider the multiplica-\ntive form of (12.2):\n\n\u03bcij = np (xa = i, xb = j) = e\u03bb0 e\u03bba(i) e\u03bbb(j).\n\n(12.3)\n\nthis means that the probability p (xa = i, xb = j) may be written in a multiplicative form\nwith factors depending only on xa or xb. taking constraints into account, it is easily derived\nthat (12.3) is equivalent to assuming that xa and xb are independent random variables, or,\nequivalently, that p (xa = i, xb = j) = p (xa = i)p (xb = j) holds. that property gives\nthe model its name.\n\nfor the product-multinomial table (row marginals ni+ fixed) one has\n(cid:14)\n\u03bcij = ni+p (xb = j|xa = i) = e\u03bb0 e\u03bba(i) e\u03bbb(j).\nj e\u03bb0 e\u03bba(i) e\u03bbb(j) = ni+ one obtains\np (xb = j|xa = i) = e\u03bbb(j)/\n\n(cid:7)\n\ne\u03bbb(r),\n\nwith the constraint\n\nr\n\nwhich means that the response xb does not depend on the variable xa. thus the model\npostulates that the response probabilities are identical across rows:\n\np (xb = j|xa = 1) = . . . = p (xb = j|xa = i),\n\nwhich means homogeneity across rows. considering it is a regression model with xb as the\nresponse and xa as the explanatory variables, it means that xa has no effect on xb. the\ninterpretation of the models is summarized in the following.\n\n\u2022 poisson distribution: no interaction effect of xa and xb on counts.\n\n\u2022 multinomial distribution: xa and xb are independent.\n\n\u2022 product-multinomial distribution: response xb does not depend on xa (fixed row\n\nmarginals), and response xa does not depend on xb (fixed column marginals).\n\ntests for the null hypothesis h0 : \u03bcab(ij) = 0 for all i, j have different interpretations.\nif h0 is not rejected, for poisson distribution tables it means that the interaction term is not\nsignificant. for multinomial distribution tables, the test is equivalent to testing the independence\nbetween xa and xb. if xa and xb are random variables and data have been collected as\npoisson counts, by conditioning on xa and xb, the interpretation as a test for independence\nalso holds for poisson tables (by conditioning on n). of course, in applications where xa and\nxb refer to experimental conditions, that interpretation is useless. consider example 7.2 in\nchapter 7 where the counts of cases of encephalitis are modeled depending on country and\ntime. these explanatory variables are experimental conditions rather than random variables,\nand it is futile to try to investigate the independence of these conditions.\n\n "}, {"Page_number": 350, "text": "338\n\nchapter 12. the analysis of contingency tables\n\nparameters and odds ratio\nthe parameters of the saturated model (12.1) with symmetric constraints are easily computed\nas\n\n(cid:7)\n(cid:7)\n\ni,j\n\n1\nij\n1\ni\n\ni\n\n\u03bb =\n\n\u03bbb(j) =\n\n(cid:7)\n\nj\n\nlog(\u03bcij),\n\n\u03bba(i) =\n\n1\nj\n\nlog(\u03bcij) \u2212 \u03bb,\n\nlog(\u03bcij) \u2212 \u03bb,\n\n\u03bbab(ij) = log(\u03bcij) \u2212 \u03bb \u2212 \u03bba(i) \u2212 \u03bbb(j).\n\nthe parameters \u03bba(i), \u03bbb(j) are the main effects, and \u03bbab(ij) is a two-factor interaction.\nfor multinomial and product-multinomial distributions, an independent measure of associ-\nation that is strongly linked to two-factor interactions is the odds ratio. for the simple (2 \u00d7\n2)-contingency table the odds ratio has the form\n\n\u03b3 = \u03c011/\u03c012\n\u03c021/\u03c022\n\n= p (xa = 1, xb = 1)/p (xa = 1, xb = 2)\np (xa = 2, xb = 1)/p (xa = 2, xb = 2)\n= p (xb = 1|xa = 1)/p (xb = 2|xa = 1)\np (xb = 1|xa = 2)/p (xb = 2|xa = 2).\n\nby using \u03bcij = n\u03c0ij (multinomial distribution) or \u03bcij = ni+\u03c0ij (product-multinomial, fixed\nrows) one obtains for the log-linear model with symmetrical constraints\n\nlog(\u03b3) = 4\u03bbab(11),\n\nand for the model with the last category set to zero log(\u03b3) = \u03bbab(11). thus \u03b3 is a direct\nfunction of the two-factor interaction. the connection to independence is immediately seen:\n\u03bbab(11) = 0 is equivalent to \u03b3 = 1, which means independence of the variables xa and xb\n(multinomial distribution) or homogeneity (product-multinomial distribution).\nin the general case of (i \u00d7 j)-contingency tables one considers the odds ratio formed by\nthe (2 \u00d7 2)-subtable built from rows {i1, i2} and columns {j1, j2} with cells {(i1, j1), (i1, j2),\n(i2, j1), (i2, j2)}. the corresponding odds ratio\n\n\u03b3(i1i2)(j1j2) = \u03c0i1j1 /\u03c0i1j2\n\u03c0i2j1 /\u03c0i2j2\n\nmay be expressed in two-factor interactions by\n\nlog(\u03b3(i1i2)(j1j2)) = \u03bbab(i1j1) + \u03bbab(i2,j2) \u2212 \u03bbab(i2j1) \u2212 \u03bbab(i1j2).\n\n12.3 log-linear models for three-way tables\nthree-way tables are characterized by three categorical variables, xa \u2208 {1, . . . , i}, xb \u2208\n{1, . . . , j}, and xc \u2208 {1, . . . , k}, which refer to rows, columns, and layers of the table. let\n{xijk} denote the collection of cell counts, where\ndenotes the counts in cell (i, j, k),\nthat is, the number of observations with xa = i, xb = j, xc = k.\n\nxijk\n\n(cid:14)\n\nthe general form of three-way tables is given in table 12.5. throughout this section the\nconvention is used that the subscript \u201d + \u201d denotes the sum over that index, for example,\nxij+ =\nk xijk. the types of contingency tables are in principle the same as for two-way\ntables. however, now there are more variants of conditioning.\n\n "}, {"Page_number": 351, "text": "12.3. log-linear models for three-way tables\n\n339\n\ntable 12.5: general form of three-way tables.\n\nxa xb\n1\n1\n\n2\n\n...\ni\n\n2\n...\nj\n\n1\n\n2\n...\nj\n...\n1\n\n2\n...\nj\n\n1\n\nxc\n\n2\n\nx111 x112\n\nx121 x122\n...\nx1j1\n\n\u00b7 \u00b7 \u00b7\n\nx211 x212\n\nx221 x222\n...\n\u00b7 \u00b7 \u00b7\nx2j1\n...\n...\nxi11 xi12\n\nxi21 xi22\n...\nxij1\n\n\u00b7 \u00b7 \u00b7\n\n\u00b7 \u00b7 \u00b7\nk\n\u00b7 \u00b7 \u00b7 x11k\n\nx11+\n...\n\nx1jk x1j+\n\n\u00b7 \u00b7 \u00b7 x21k\n\nx21+\n...\n\nx2jk x2j+\n\n...\n\n...\n\u00b7 \u00b7 \u00b7 xi1k xi1+\n\n...\n\nxijk xij+\n\ntype 1: poisson distribution\nit is assumed that the xijk are independent poisson-distributed random variables, xijk \u223c\np (\u03bbijk). the total number of counts n =\nijk xijk as well as the marginal counts are random\nvariables. the natural model considers the counts as the response and xa, xb, and xc as\nexplanatory variables, which might refer to experimental conditions or random variables.\n\n(cid:14)\n\ntype 2: multinomial distribution\nfor a fixed number of subjects one observes the tupel (xa, xb, xc) with possible outcomes\n{(1, 1, 1), . . . , (i, j, k)}. the count in cell (i, j, k) is the number of observations with xa =\ni, xb = j, xc = k. the counts {xijk} follow a multinomial distribution m(n,{\u03c0ijk}),\nwhere \u03c0ijk = p (xa = i, xb = j, xc = k). for three- and higher dimensional tables the\nnotation {xijk} and {\u03c0ijk} is preferred over the representation as vectors.\n\ntype 3: product-multinomial distribution\nthere are several variants of the product-multinomial distribution. either one of the variables\n(xa or xb or xc) is a design variable, meaning that the corresponding marginals are fixed,\nor two of them are design variables, meaning that two-dimensional margins are fixed. let us\nconsider as an example of the first variant the table that results from the design variable xa.\nthis means ni++ = xi++ is fixed and\n\n(xi11, . . . , xijk) \u223c m(ni++, (\u03c0i11, . . . , \u03c0ijk)),\n\n(12.4)\nwhere \u03c0ijk = p (xb = j, xc = k|xa = i). an example of the second variant (two design\nvariables) is obtained by letting xa and xb be design variables, that is, nij+ = xij+ is fixed\nand\n\n(xij1, . . . , xijk) \u223c m(nij+, (\u03c0ij1, . . . , \u03c0ijk)),\n\n(12.5)\n\n "}, {"Page_number": 352, "text": "chapter 12. the analysis of contingency tables\n\n340\nwhere \u03c0ijk = p (xc = k|xa = i, xb = j). hence, only xc is a response variable, and the\nnumber of observations for (xa, xb) = (i, j) is given.\nit should be noted that there is again a hierarchy among distributions. if {xijk} have a pois-\nson distribution, conditioning on n = \u03c3ijkxijk yields the multinomial distribution {xijk} \u223c\nm(n,{\u03c0ijk}), where \u03c0ijk = \u03bbijk/\u03c3ijk\u03bbijk. further conditioning on ni++ = \u03c3jkxijk\nyields the product of the multinomial distributions (12.4). if, in addition, one conditions on\nnij+ = \u03c3kxijk, one obtains the product of distributions (12.5).\n\ntable 12.6: log-linear model for three-way tables with constraints.\n\nlog-linear model for three-way tables\n\nlog(\u03bcijk) = \u03bb0 + \u03bba(i) + \u03bbb(j) + \u03bbc(k)\n\n+ \u03bbab(ij) + \u03bbac(ik) + \u03bbbc(jk) + \u03bbabc(ijk).\n\n(cid:7)\n(cid:7)\n\n(cid:7)\n\nk\n\n(cid:7)\n\n(cid:7)\n\ni\n\n\u03bba(i) =\n\n\u03bbb(j) =\n\n\u03bbc(k) = 0,\n\nj\n\u03bbab(ij) =\n\n(cid:7)\n\n\u03bbab(ij) =\n\n\u03bbac(ik) =\n\nj\n\n(cid:7)\n\n=\n\n\u03bbbc(jk) =\n\n\u03bbbc(jk) = 0,\n\nj\n\n\u03bbabc(ijk) =\n\nk\n\n\u03bbabc(ijk) =\n\nj\n\nk\n\n(cid:7)\n\nconstraints:(cid:7)\n(cid:7)\n\ni\n\ni\n\n(cid:7)\n\ni\n\nor\n\n(cid:7)\n\nk\n\n\u03bbac(ik)\n\n\u03bbabc(ijk) = 0,\n\n\u03bba(i) = \u03bbb(j) = \u03bbc(k) = 0,\n\u03bbab(i,j) = \u03bbab(i,j) = \u03bbac(ik) = \u03bbac(ik) = \u03bbbc(jk) = \u03bbbc(jk) = 0,\n\u03bbabc(ijk) = \u03bbabc(ijk) = \u03bbabc(ijk) = 0.\n\nlet in general \u03bcijk = e(xijk) denote the mean of the cell counts. then the general form\n\nof the three-dimensional log-linear model is\n\nlog(\u03bcijk) = \u03bb0 + \u03bba(i) + \u03bbb(j) + \u03bbc(k) + \u03bbab(ij) + \u03bbac(ik) + \u03bbbc(jk) + \u03bbabc(ijk).\n\nfor the necessary constraints see table 12.6, where two sets of constraints are given: the\nset of symmetric constraints corresponding to anova models and the set of constraints based\non reference categories. for the multinomial and the product-multinomial models, additional\nconstraints are needed, which are easily derived from the restrictions n =\nijk xijk, and\nso on. the model has three types of parameters: the three-factor interactions \u03bbabc(ijk), the\ntwo-factor interactions \u03bbab(ij), \u03bbac(ik) and \u03bbbc(jk), and the main effects \u03bba(i), \u03bbb(j), \u03bbc(k).\nthe general model is saturated, which means that it has as many parameters as means \u03bcijk and\nconsequently every dataset (without empty cells) yields a perfect fit by setting \u02c6\u03bcijk = xijk.\n\n(cid:14)\n\n "}, {"Page_number": 353, "text": "12.4. specific log-linear models\n\n341\n\nmore interesting models are derived from the general model by omitting groups of parame-\nters corresponding to interaction terms. the attractive feature of log-linear models is that most\nof the resulting models have an interpretation in terms of independence or conditional indepen-\ndence. in general, categorical variables xa, xb, xc are independent if\n\np (xa = i, xb = j, xc = k) = p (xa = i)p (xb = j)p (xc = k)\n\nholds for all i, j, k. conditional independence of xa and xb given xc (in short xa\u22a5xb|xc)\nholds if, for all i, j, k,\n\np (xa = i, xb = j|xc = k) = p (xa = i|xc = k)p (xb = j|xc = k).\n\nhierarchical models\nthe interesting class of models that may be interpreted in terms of (conditional) independence\nare the hierarchical models. a model is called hierarchical if it includes all lower order terms\ncomposed from variables contained in a higher order term. for example, if a model contains\n\u03bbbc(jk), it also contains the marginals \u03bbb(j) and \u03bbc(k). hierarchical models may be abbrevi-\nated by giving the terms of highest order. for example, the symbol ab/ac denotes the model\ncontaining \u03bbab, \u03bbac, \u03bba, \u03bbb, \u03bbc (and a constant term). further examples are given in table\n12.7. the notation is very similar to the wilkinson-rogers notation (see section 4.4), which,\nfor the model ab/ac is, a \u2217 b + a \u2217 c. the latter form is itself shorthand for the extended\nwilkinson-rogers notation a.b + a.c + a + b + c.\n\ngraphical models\nmost of the hierarchical log-linear models for three-way tables are also graphical models,\nwhich are considered in more detail in section 12.5. the basic concept is only sketched here.\nif a graph is drawn by linking variables for which the two-factor interaction is contained in the\nmodel, one obtains a simple graph. if in the resulting graph there is no connection between the\ngroups of variables, these groups of variables are independent. if two variables are connected\nonly by edges through the third variable, the two variables are conditionally independent given\na third variable. for examples, see table 12.7, and for a more concise definition of graphical\nmodels see section 12.5.\n\n12.4 specific log-linear models\nin the following, the types of hierarchical models for three-way tables are considered under\nthe assumption that xa, xb, xc represent random variables (i.e., multinomial contingency\ntables).\n\ntype 0: saturated model\nthe saturated model is given by\n\nlog(\u03bcijk) = \u03bb0 + \u03bba(i) + \u03bbb(j) + \u03bbc(k)\n\n+ \u03bbab(i,j) + \u03bbac(ik) + \u03bbbc(jk) + \u03bbabc(ijk).\n\nit represents a reparameterization of the means {\u03bcijk} without implying any additional structure\n(except \u03bcijk > 0).\n\n "}, {"Page_number": 354, "text": "342\n\nchapter 12. the analysis of contingency tables\n\ntype 1: no three-factor interaction\nthe model\n\nlog(\u03bcijk) = \u03bb0 + \u03bba(i) + \u03bbb(j) + \u03bbc(k) + \u03bbab(ij) + \u03bbac(ik) + \u03bbbc(jk)\n\ncontains only two-factor interactions and is denoted by ab/ac/bc. since the three-factor\ninteraction is omitted, the model has to imply restrictions on the underlying probabilities. to\nsee what the model implies, it is useful to look at the conditional association of two variables\ngiven a specific level of the third variable.\n\nlet us consider the odds ratios of xa and xb given xc = k and for simplicity assume\nthat all variables are binary. then the conditional association measured by the odds ratio has\nthe form\n\n\u03b3(xa, xb|xc = k) = p (xa = 1, xb = 1|xc = k)/p (xa = 2, xb = 1|xc = k)\np (xa = 1, xb = 2|xc = k)/p (xa = 2, xb = 2|xc = k)\nand is built from the (2 \u00d7 2)-table formed by xa and xb for a fixed level xc = k. by using\n\u03bcijk = n\u03c0ijk and\n\n\u03b3(xa, xb|xc = k) = \u03c011k/\u03c021k\n\u03c012k/\u03c022k\n\n= \u03bc11k/\u03bc21k\n\u03bc12k/\u03bc22k\n\none obtains for the model without three-factor interactions that all terms depending on k cancel\nout and therefore \u03b3(xa, xb|xc = k) does not depend on k. this means that the conditional\nassociation between xa and xb given xc = k does not depend on the level k. whatever\nthe conditional association between these two variables, strong or weak or not present, it is the\nsame for all levels of xc, and thus xc does not modify the association between xa and xb.\nthe same holds if xa and xb have more than two categories, where conditional association\nis measured by odds ratios of (2 \u00d7 2)- subtables built from the total table. moreover, since the\nmodel is symmetric in the variables, it is also implied that the conditional association between\nxa and xc given xb = j does not depend on j and the conditional association between xb\nand xc given xa = i does not depend on i.\n\nit should be noted that the model without the three-factor interaction does not imply that two\nvariables are independent of the third variable. there might be a strong dependence between\n{xa, xb} and xc, although the conditional association of xa and xb given xc = k does\nnot depend on the level of xc. the model is somewhat special because it is the only log-linear\nmodel for three-way tables that is not a graphical model and therefore cannot be represented by\na simple graph. it is also the only model that cannot be interpreted in terms of independence or\nconditional independence of variables.\n\ntype 2: only two two-factor interactions contained\na model of this type is the model ac/bc, given by\n\nlog(\u03bcijk) = \u03bb0 + \u03bba(i) + \u03bbb(j) + \u03bbc(k) + \u03bbac(ik) + \u03bbbc(jk).\n\nif the model holds, the variables xa and xb are conditionally independent, given xc, or ,\n\nmore formally,\n\np (xa = i, xb = j|xc = k) = p (xa = i|xc = k)p (xb = j|xc = k).\n\nthis may be easily derived by using that the model is equivalent to postulating that \u03bcijk =\n\u03bci+k\u03bc+jk/\u03bc++k. it means that conditionally the variables xa and xb are not associated.\n\n "}, {"Page_number": 355, "text": "12.4. specific log-linear models\n\n343\n\ntable 12.7: graphical models for three-way tables.\n\nlog-linear model\n\nregressors of logit-model\n(with response xc )\n\nab/ac\n\nxb, xc conditionally independent, given xa\n\nab/bc\n\nxa, xc conditionally independent, given xb\n\nac/bc\n\nxa, xb conditionally independent, given xc\n\na/bc\n\nxa independent of (xb, xc )\n\nac/b\n\n(xa, xc ) independent of xb\n\nab/c\n\n(xa, xb) independent of xc\n\na/b/c\n\nxa, xb, xc are dependent\n\na\n\na\n\na\n\na\n\na\n\na\n\na\n\nb\n\nc\n\nb\n\nc\n\nb\n\nc\n\nb\n\nc\n\nb\n\nc\n\nb\n\nc\n\nb\n\nc\n\n1, xa\n\n1, xb\n\n1, xa, xb\n\n1, xb\n\n1, xa\n\n1\n\n1\n\nhowever, that does not mean that there is no marginal association between xa and xb. xa\nand xb may be strongly associated when xc is ignored. the model is a graphical model,\nwith the graph given in table 12.7. the graph contains edges between xa and xc as well as\nbetween xb and xc but not between xa and xb. it illustrates that xa and xb have some\nconnection through the common variable xc. and that is exactly the meaning of the graph:\ngiven xc, the variables xa and xb are independent because the connection between xa and\nxb is only through xc.\n\nthe other two models of this type are ab/ac and ab/bc (shown in table 12.7). the first\npostulates that xb and xc are conditionally independent given xa, and the latter postulates\nthat xa and xc are conditionally independent given xb.\n\n "}, {"Page_number": 356, "text": "344\n\nchapter 12. the analysis of contingency tables\n\ntype 3: only one two-factor interaction contained\na model of this type is the model a/bc, given by\n\nlog(\u03bcijk) = \u03bb0 + \u03bba(i) + \u03bbb(j) + \u03bbc(k) + \u03bbbc(jk),\n\nwhich contains only main effects and one two-factor interaction. by simple derivation one\nobtains that the model postulates that xa is jointly independent of xb and xc. this means\nthat the groups of variables {a} and {b, c} are independent, or, more formally,\np (xa = i, xb = j, xc = k) = p (xa = i)p (xb = j, xc = k).\n\nthe model implies stronger restrictions on the underlying probability structure than the model\nac/bc because now, in addition, the two-factor interaction \u03bbac is omitted. the correspond-\ning graph in table 12.7 is very suggestive. there is no edge between the variable xa and the\ntwo variables xb, xc; the two groups of variables are well separated, corresponding to the\ninterpretation of the model that xa and xb, xc are independent.\n\ntype 4: main effects model\nthe model has the form\n\nlog(\u03bcijk) = \u03bb0 + \u03bba(i) + \u03bbb(j) + \u03bbc(k).\n\nit represents the independence of variables xa, xb, xc:\n\np (xa = i, xb = j, xc = k) = p (xa = i)p (xb = j)p (xc = k),\n\nmeaning in particular that all the variables are mutually independent.\n\nfigure 12.1 shows the hierarchy of log-linear models. it is obvious that the model ab/bc\nis a submodel of ab/bc/ac since the latter is less restrictive than the former. but not\n\nabc\n6\n\nab/ac/bc\n*\ny\n\n6\n\nab/ac\n\ny\n\n6\n\nab/bc\n\n*\n\ny\n\nac/bc\n\n*\n\n6\n\nab/c\n\ny\n\nac/b\n6\n\na/b/c\n\nbc/a\n\n*\n\nfigure 12.1: hierarchy of three-dimensional log-linear models.\n\ntype 0\n\ntype 1\n\ntype 2\n\ntype 3\n\ntype 4\n\n "}, {"Page_number": 357, "text": "12.5. log-linear and graphical models for higher dimensions\n\n345\n\nall models are nested. for example, there is no hierarchy between the models ab/ac and\nab/bc. the possible models form a lattice with semi-ordering.\n\ntable 12.8 shows what restrictions are implied by omitting interaction terms. for exam-\nple, the model ab/ac implies that \u03bcijk = \u03bcij+\u03bci+k/\u03bci++ holds. when the sampling is\nmultinomial, it is easily derived what that means for conditional probabilities and therefore for\nthe interpretation of the assumed association structure. the corresponding graphs are given in\ntable 12.7.\n\ntable 12.8: graphical models and interpretation for three-way-tables.\n\nab/ac\n\n\u03bbabc = \u03bbbc = 0\n\nab/bc\n\n\u03bbabc = \u03bbac = 0\n\nac/bc\n\n\u03bbabc = \u03bbab = 0\n\na/bc\n\n\u03bbabc = \u03bbab = \u03bbac = 0\n\nac/b\n\n\u03bbabc = \u03bbab = \u03bbbc = 0\n\nab/c\n\n\u03bbabc = \u03bbac = \u03bbbc = 0\n\n\u03bcij+\u03bci+k\n\n\u03bcijk =\np (xb, xc|xa) = p (xb|xa)p (xc|xa)\n\n\u03bci++\n\n\u03bc+jk\u03bcij+\n\n\u03bcijk =\np (xa, xc|xb) = p (xa|xb)p (xc|xb)\n\n\u03bc+j+\n\n\u03bci+k\u03bc+jk\n\n\u03bcijk =\np (xa, xb|xc ) = p (xa|xc )p (xb|xc )\n\n\u03bc++k\n\n\u03bci++\u03bc+jk\n\n\u03bcijk =\np (xa, xb, xc ) = p (xa)p (xb, xc )\n\n\u03bc+++\n\n\u03bci+k\u03bc+j+\n\n\u03bcijk =\np (xa, xb, xc ) = p (xa, xc )p (xb)\n\n\u03bc+++\n\n\u03bcij+\u03bc++k\n\n\u03bcijk =\np (xa, xb, xc ) = p (xa, xb)p (xc )\n\n\u03bc+++\n\na/b/c\n\n\u03bbabc = \u03bbab = \u03bbac = 0\n\n\u03bcijk =\n\n\u03bci++\u03bc+j+\u03bc++k\n\n\u03bc2\n\n+++\n\n\u03bbbc = 0\n\np (xa, xb, xc ) = p (xa)p (xb)p (xc )\n\nmodels for product-multinomial contingency tables\nwhile all models in figure 12.1 apply for multinomial contingency tables, not all models may\nbe built for product-multinomial contingency tables, because marginal sums that are fixed by\ndesign have to be fitted by the model. in general, if margins are fixed by design, the correspond-\ning interaction term has to be contained in the model. for example, if the two-dimensional\nmargins xij+ =\nk xijk are fixed by design, the model has to contain the interaction \u03bbab.\nthe model ac/bc is not a valid model because it fits the margins xi+k and x+jk but does\nnot contain \u03bbab (see also lang, 1996a ; bishop et al., 1975; agresti, 2002).\n\n(cid:14)\n\n12.5 log-linear and graphical models for higher\n\ndimensions\n\nlog-linear models for dimensions higher than three have basically the same structure, but the\nnumber of possible interaction terms and the number of possible models increase. for exam-\nple, in four-way tables a four-factor interaction term can be contained. it is helpful that for\nhierarchical models the same notation applies as in lower dimensional models. an example of\n\n "}, {"Page_number": 358, "text": "346\n\nchapter 12. the analysis of contingency tables\n\na four-way model is abc/ad, which is given by\n\nlog(\u03bcijkl) = \u03bb0 + \u03bba(i) + \u03bbb(j) + \u03bbc(k) + \u03bbd(l)\n\n+ \u03bbab(ij) + \u03bbac(ik) + \u03bbbc(jk) + \u03bbad(il) + \u03bbabc(ijk).\n\nthe model contains only one three-factor interaction and only four two-factor interactions but\nall main effects. for interpreting higher dimensional tables, representing them as graphical\nmodels is a helpful tool.\n\ngraphical models\nto obtain models that have simple interpretations in terms of conditional interpretations it is\nuseful to restrict consideration to subclasses of log-linear models. we already made the restric-\ntion to hierarchical models. a log-linear model is hierarchical if the model includes all lower\norder terms composed from variables contained in a higher order term. a further restriction is\npertaining to graphical models:\n\na log-linear model is graphical if, whenever the model contains all two-factor interac-\ntions generated by a higher order interaction, the model also contains the higher order\ninteraction.\n\nin three-way tables there is only one log-linear model that is not graphical, namely, the\nmodel ab/ac/bc. that model contains all two-factor interactions \u03bbab, \u03bbac, \u03bbbc, which are\ngenerated as marginal parameters of the three-factor interaction \u03bbabc, but \u03bbabc itself is not\ncontained in the model.\n\na graphical model has a graphical representation that makes it easy to see what types of\nconditional independence structures are implied. the representation is based on mathematical\ngraph theory, outlined, for example, in whittaker (1990) and lauritzen (1996). in general, a\ngraph consists of two sets: the sets of vertices, k, and the set of edges, e. the set of edges\nconsists of pairs of elements from k, e \u2282 k \u00d7 k. in graphical log-linear models, the vertices\ncorrespond to variables and the edges correspond to pairs of variables. therefore, we will set\nk to k = {a, b, c, . . .} and an element from e has the form (a, c). in undirected graphs,\nthe type of graph that is considered here, if (a, b) is in e, (b, a) is also in e and the edge\nor line between a and b is undirected. a chain between vertices a and c is determined by a\nsequence of distinct vertices v1, . . . , vm, vi \u2208 k. the chain is given by the sequence of edges\n[av1/v1v2/ . . . /vmc] for which (vi, vi+1) as well as (av1), (vmc) are in e. this means that\na chain represents a sequence of variables leading from one variable to another within the graph.\nalthough a and c may be identical, a vertex between a and c may not be included more than\nonce. therefore circles are avoided. the left graph in figure 12.2 contains, for example, the\nchains [ba/ac], [ca/ab], [ab], and the right graph contains the chain [ab/bc/ca].\n\na\n\nb\n\nc\n\na\n\nb\n\nc\n\nfigure 12.2: graphs for log-linear models ab/ac (left) and abc (right).\n\n "}, {"Page_number": 359, "text": "12.5. log-linear and graphical models for higher dimensions\n\n347\n\nchains are important for interpreting the model. the left graph in figure 12.2 corresponds\nto the model ab/ac, which implies that xb and xc are conditionally independent given a.\nif one looks at the paths that connect b and c, one can see that any paths connecting b and c\ninvolve a. this property of the graph may be read as the conditional independence of xb and\nxc given xa.\n\nfor the correspondence of graphical log-linear models and graphs it is helpful to consider\nthe largest sets of vertices that include all possible edges between them. a set of vertices for\nwhich all the vertices are connected by edges is called complete. the corresponding vertices\nform a complete subgraph. a complete set that is not contained in any other complete set is\ncalled a maximal complete set or a clique. the cliques determine the graphical linear model and\ncorrespond directly to the notation defining the model. for example, the model ab/ac has\nthe cliques {ab} and {ac}. the saturated model abc that contains all possible edges has\nthe maximal complete set or clique {abc}. an example of a higher dimensional model is the\nmodel abc/ad, which is a graphical model. the model has the cliques {abc}, {ad} (see\nfigure 12.3 for the graph). figure 12.3 also shows the graphs for the saturated model abcd\nand the model abc/abd/de.\n\nthe strength of graphing log-linear models becomes obvious in higher dimensional tables.\nthe basic tool for interpreting graphical log-linear models is a result by darroch et al. (1980) :\n\nlet the sets f0, f1, f2 denote disjoint subsets of the variables in a graphical log-linear\nmodel. the factors in f1 are conditionally independent of the factors in f2 given f0 if\nand only if every chain between a factor in f1 and a factor in f2 involves at least one\nfactor in f0. then f0 is said to separate the subgraphs formed by f1 and f2.\n\nb\n\na\n\nb\n\na\n\nb\n\na\n\nabcd\n\nabc/ad\n\nabc/abd/de\n\nc\n\nd\n\nc\n\nd\n\nc\n\nd\n\ne\n\nfigure 12.3: graphs for log-linear models in multiway tables.\n\n "}, {"Page_number": 360, "text": "348\n\nchapter 12. the analysis of contingency tables\n\nfor the model ab/ac (see graph in figure 12.2) one may consider f1 = {b}, f2 =\n{c}, and f0 = {a}. the conditional independence of xb and xc given xa, in short,\nxb \u22a5 xc|xa, follows directly from the result of darroch et al.\n(1980). for the model\nabc/abd/de (see graph in figure 12.3) one may build several subsets of variables. by\nconsidering f1 = {a, b, c}, f2 = {e}, and f0 = {d}, one obtains that {xa, xb, xc}\nare conditionally independent of xe given xd, {xa, xb, xc} \u22a5 xe|xd. it is said that\nxd separates the subgraphs formed by {xa, xb, xc} and {xe}. by considering f1 =\n{b, c}, f2 = {e}, f0 = {a, d} one obtains that {xb, xc} are conditionally independent\nof xe given {xa, xd}. it is seen that for higher dimensional models several independence\nstructures usually are involved when considering a graphical log-linear model.\n\nmarginal independence occurs if there are no chains in the graph that connect two groups of\nvariables. the graph corresponding to model ab/c (see graph in table 12.7) contains no chain\nbetween {a, b} and {c}. the implication is that the variables {xa, xb} are independent of\nxc, in short, {xa, xb} \u22a5 xc. a model may also imply certain marginal independence\nstructures. for example, the model ab/bc/cd implies conditional independence relations\nxa \u22a5 xd|{xb, xc} and xa \u22a5 {xc, xd}|xb, which include all four variables, but also\nxa \u22a5 xc|xb, which concerns the marginal distribution of xa, xb, xc.\n\nas was already seen in three-way tables, not all log-linear models are graphical. that raises\nthe question of how to interpret a log-linear model that is not graphical. fortunately, any log-\nlinear model can be embedded in a graphical model. for the interpretation one uses the smallest\ngraphical model that contains the specific model. since the specific model is a submodel of\nthat graphical model, all the (conditional) independence structures of the larger model also\nhave to hold for the specific model. that strategy does not always work satisfactorily. the\nsmallest graphical model that contains the three-way table model ab/ac/bc is the saturated\nmodel, which implies no independence structure. but the model ab/ac/bc also has no\nsimple interpretation in terms of conditional independence, although excluding the three-factor\ninteraction restricts the association structure between variables.\n\n12.6 collapsibility\nin general, association in marginal tables differs from association structures found in the full\ntable. for example, xa and xb can be conditionally independent given xc = k, even if\nthe variables xa and xb are marginally dependent. marginal dependence means that the\nassociation is considered in the marginal table obtained from collapsing over the categories of\nthe other variables; that is, the other variables are ignored (see also exercise 12.6).\n\nthe question arises under which conditions is it possible to infer on the association struc-\nture from marginal tables. let us consider a three-way table with means \u03bcijk. the marginal\nassociation between the binary factors xa and xb, measured in odds ratios, is determined by\n\n\u03bc11+/\u03bc12+\n\u03bc21+/\u03bc22+\n\n,\n\nwhile the conditional association between xa and xb given xc = k is determined by\n\n\u03bc11k/\u03bc12k\n\u03bc21k/\u03bc22k\n\n.\n\none can show that the association is the same if the model ab/ac holds (compare exercise\n12.13).\n\n "}, {"Page_number": 361, "text": "12.7. log-linear models and the logit model\n\n349\n\nin general, the association is unchanged if groups of variables are separated:\n\nlet the sets f1, f2, f0 denote the disjoint subsets of the variables in a graphical log-linear\nmodel. if every chain between a factor in f1 and a factor in f2 involves at least one factor\nin f0, the association among the factors in f1 and f0 can be examined in the marginal\ntable obtained from collapsing over the factors in f2. in the same way, the association\namong the factors in f2 and f0 can be examined in the marginal table obtained from\ncollapsing over the factors in f1\n\n(compare darroch et al., 1980, and bishop et al., 2007). therefore, if f0 separates the sub-\ngraphs formed by f1 and f2, one can collapse over f2 (or f1, respectively). in the model\nab/ac considered previously, the association between xa and xb as well as the association\nbetween xa and xc can be examined from the corresponding marginal tables.\n\n12.7 log-linear models and the logit model\nthe log-linear models for contingency tables may be represented as logit models. let us con-\nsider a three-way table with categorical variables xa, xb, xc. the most general log-linear\nmodel is the saturated model:\n\nlog(\u03bcijk) = \u03bb0 + \u03bba(i) + \u03bbb(j) + \u03bbc(k)\n\n+ \u03bbab(ij) + \u03bbac(ik) + \u03bbbc(jk) + \u03bbabc(ijk).\n\nfor multinomial tables, for which \u03bcijk = n\u03c0ijk, one obtains the logit model with reference\ncategory (xa = i, xb = j, xc = k):\n\n= \u03b3a(i) + \u03b3b(j) + \u03b3c(k) + \u03b3ab(ij) + \u03b3ac(ik) + \u03b3bc(jk) + \u03b3abc(ijk),\n\nwhere the \u03b3-parameters are obtained as differences, for example, \u03b3a(i) = \u03bba(i) \u2212 \u03bba(i),\n\u03b3abc(ijk) = \u03bbabc(ijk)\u2212\u03bbabc(ijk). the parametrization of the model, which uses reference\ncategories for the variables xa, xb, xc, reflects that a structured multinomial distribution is\ngiven. in contrast to the simple multinomial distributions considered in chapter 8, the distri-\nbution of the response is determined by three separate variables that structure the multinomial\ndistribution.\n\n(cid:25)\n\n(cid:26)\n\nlog\n\n\u03c0ijk\n\u03bcijk\n\nlogit models with selected response variables\nconsider now that xc is chosen as a response variable. then one obtains for multinomial\ntables\n\n(cid:25)\n\n(cid:26)\n\n(cid:25)\n\n(cid:26)\n\n(cid:25)\n\n\u03bcijr\n\u03bcijk\n\n= log\n\np (xa = i, xb = j, xc = k)\np (xa = i, xb = j, xc = k)\n\n= log\n\n\u03c0r|ij\n\u03c0k|ij\n\n(cid:26)\n\n,\n\nwhere \u03c0r|ij = p (xc = r|xa = i, xb = j). by using the saturated model, which always\nholds, one obtains from easy derivation the multinomial logit model\n\nlog\n\n(cid:25)\n\n(cid:26)\n\n= \u03b30r + \u03b3a(i),r + \u03b3b(j),r + \u03b3ab(ij),r,\n\np (xc = r|xa = i, xb = j)\np (xc = k|xa = i, xb = j)\n\nlog\n\nwhere\n\n\u03b30r = \u03bbc(r) \u2212 \u03bbc(k),\n\n\u03b3b(j),r = \u03bbbc(jr) \u2212 \u03bbbc(jk),\n\n\u03b3a(i),r = \u03bbac(ir) \u2212 \u03bbac(ik),\n\n\u03b3ab(ij),r = \u03bbabc(ijr) \u2212 \u03bbabc(ijk).\n\n "}, {"Page_number": 362, "text": "350\n\nchapter 12. the analysis of contingency tables\n\nan alternative form of the model, which uses dummy variables, is\n\n(cid:25)\n\n(cid:26)\n\n\u03c0r|ij)\n\u03c0k|ij)\n\n= \u03b30r + \u03b3a(1),rxa(1) + \u00b7\u00b7\u00b7 + \u03b3b(1),rxb(1) + \u00b7\u00b7\u00b7 +\n\nlog\n\u03b3ab(11),rxa(1)xb(1) + \u00b7\u00b7\u00b7 + \u03b3ab(i\u22121,j\u22121),rxa(i\u22121)xb(j\u22121).\n\n(cid:14)\n\n(cid:14)\n\nthe constraints on the \u03bb-parameters and therefore the type of coding of dummy variables\ni \u03bbac(ik) = 0 transforms into\ncarry over to the \u03b3-parameters. for example, the constraint\n\ni \u03b3a(i),r = 0.\nin summary, by choosing one variable as the response variable, the log-linear model of\nassociation between xa, xb, xc turns into a regression model. if the log-linear model is a\nsubmodel of the saturated model, some \u03b3 terms are not contained in the corresponding logit\nmodel. for example, by assuming the log-linear model ab/ac (meaning that xb and xc are\nconditionally independent) one obtains the logit model\np (xc = r|xa=i, xb = j)\np (xc = k|xa = i, xb = j)\n\n= \u03b30r + \u03b3a(i),r,\n\n(cid:26)\n\n(cid:25)\n\nlog\n\nwhich contains only the explanatory variable xa. since xb and xc are conditionally inde-\npendent given xa, it is quite natural that xb does not effect on xc since it is associated with\nxc only through xa. in table 12.7 the explanatory variables of logit models with response\nxc are given together with the underlying log-linear model. it is seen that model ab/bc as\nwell as model a/bc yield a logit model with xb as the only explanatory variable. model\nab/bc is weaker than a/bc. since the effect of the variable xa on xc is already omitted\nif model ab/bc holds, it is naturally omitted if an even stronger model holds.\n\n12.8 inference for log-linear models\nlog-linear models may be embedded into the framework of generalized linear models. for all\nthree sampling schemes \u2013 poisson distribution, multinomial distribution, and product-multinomial\ndistribution \u2013 the response distribution is in the exponential family. the log-linear model has\nthe form assumed in glms, where the mean is linked to the linear predictor by a transformation\nfunction. thus maximum likelihood estimation and testing are based on the methods developed\nin chapters 3 and 8. advantages of log-linear models are that maximum likelihood estimates\nare sometimes easier to compute and sufficient statistics have a simple form. in the following\nthe results are briefly stretched.\n\n12.8.1 maximum likelihood estimates and minimal sufficient statistics\nfor simplicity, the poisson distribution is considered for three-way models. let all of the pa-\nrameters be collected in one parameter vector \u03bb. from the likelihood function\n\ni(cid:15)\n\nj(cid:15)\n\nk(cid:15)\n\ni=1\n\nj=1\n\nk=1\n\nl(\u03bb) =\n\none obtains the log-likelihood\n\n(cid:7)\n\nl(\u03bb) =\n\nxijk log(\u03bcijk) \u2212\n\ni,j,k\n\ni,j,k\n\ni,j,k\n\n\u2212\u03bcijk\n\n\u03bcxijk\nijk\nxijk! e\n(cid:7)\n\n\u03bcijk \u2212\n\n(cid:7)\n\nlog(xijk!).\n\n "}, {"Page_number": 363, "text": "12.8. inference for log-linear models\n\n351\n\nwith \u03bcijk parameterized as the saturated log-linear model one obtains by rearranging terms\n(and omitting constants)\n\nl(\u03bb) = n\u03bb0 +\n\nxi++\u03bba(i) +\n\nx+j+\u03bbb(j) +\n\nx++k\u03bbc(k)\n\n(cid:7)\n(cid:7)\n\nk\n\nxi+k\u03bbac(ik) +\n\nx+jk\u03bbbc(jk)\n\nj,k\n\nexp(\u03bb0 + \u03bba(i) + ... + \u03bbabc(ijk)).\n\n(cid:7)\n\ni\n\n(cid:7)\n(cid:7)\n\ni,j\n\ni,j,k\n\n+\n\n+\n\nxij+\u03bbab(ij) +\nxijk\u03bbabc(ijk) \u2212\n(cid:14)\n\n(cid:7)\n\nj\n\n(cid:7)\n(cid:7)\n\ni,k\n\ni,j,k\n\nthe form of the log-likelihood remains the same when non-saturated models are considered.\nfor example, if \u03bbabc = 0, the term\ni,j,k xijk\u03bbabc(ijk) is omitted. since the poisson is\nan exponential family distribution, the factors on parameters represent sufficient statistics that\ncontain all the information about parameters. this means that for non-saturated models the\nparameter estimates are determined by marginal sums. for example, the likelihood of the inde-\npendence model a/b/c contains only the marginal sums xi++, x+j+, x++k. it is noteworthy\nthat the sufficient statistics, which are even minimal statistics, correspond directly to the symbol\nfor the model. table 12.9 gives the sufficient statistics for the various types of log-linear models\nfor three-way tables.\n\nas usual, maximum likelihood estimates are obtained by setting the derivations of the log-\n\nlikelihood equal to zero. the derivative for one of the parameters, say \u03bbab(ij), is given by\n\n\u2202l(\u03bb)\n\n\u2202\u03bbab(ij)\n\n= xij+ \u2212\n\nexp(\u03bb0 + \u03bba(i) + . . . ) = xij+ \u2212 \u03bcij+.\n\n(cid:7)\n\nk\n\nfrom \u2202l(\u03bb)/\u2202\u03bbab(ij) = 0 one obtains immediately xij+ = \u02c6\u03bcij+. hence, computing the max-\nimum likelihood estimates reduces to solving the equations that equal the sufficient statistics to\ntheir expected values. for example, for the independence model one has to solve the system of\nequations\n\nxi++ = \u02c6\u03bci++,\n\nx+j+ = \u02c6\u03bc+j+,\n\nx++k = \u02c6\u03bc++k,\n\n(12.6)\n\ni = 1, . . . , i, j = 1, . . . j, k = 1, . . . , k.\n\nif the log-linear model is represented in the general vector form\n\nlog(\u03bc) = x\u03bb\n\nwith \u03bc containing all the expected cell counts and x denoting the corresponding design matrix,\nthe likelihood equations equating sufficient statistics to expected values have the form\n\nx t x = x t \u03bc,\n\nwhere x is the vector of cell counts (for three-way tables xt = (x111, x112, . . . , xijk)).\n\ntable 12.9: log-linear models and sufficient statistics for three-way tables.\n\nabc\nab/ac/bc\nab/ac\na/bc\na/b/c\n\n{xijk}\n{xij+}, {xi+k}{x+jk}\n{xij+}, {xi+k}\n{xi++}, {x+jk}\n{xi++}, {x+j+}, {x++k}\n\n "}, {"Page_number": 364, "text": "352\n\nchapter 12. the analysis of contingency tables\n\nsolving these equations can be very easy. for example, the solution of (12.6) is directly\n\ngiven by\n\n\u02c6\u03bcijk = n\n\nxi++\n\nx+j+\n\nx++k\n\nn\n\nn\n\nn\n\n.\n\nthe form mimics \u03bcijk = n\u03c0i++\u03c0+j+\u03c0++k. direct estimates are available for all log-linear\nmodels for three-way tables except ab/ac/bc . a general class of models for which direct\nestimates exist is decomposable models. a model is called decomposable if it is graphical and\nchordal, where chordal means that every closed chain [av1/v1v2/ . . . /vma] (for which the\nstarting point and end point are identical) that involves at least four distinct edges has a shortcut.\na simple example is the model ab/bc/cd/ad, which is represented by a rectangle. it is not\ndecomposable because the chain [ab/bc/cd/da] has no shortcut. it becomes decompos-\nable by adding one more edge, ac or bd, yielding the model abc/acd or abd/bcd,\nrespectively. a more extensive treatment of direct estimates was given, for example by bishop\net al. (1975).\n\nif no direct estimates are available, iterative procedures as in glms can be used. an alter-\nnative, rather stable procedure that is still used for log-linear models is iterative proportional\nfitting, also called the deming-stephan algorithm (deming and stephan, 1940). it iteratively\nfits the marginals, which for the example of the independence model are given in 12.6. it works\nfor direct estimates as well as for models, for which no direct estimates exist.\n\nfor graphical models the density can always be represented in the form\n\nf({xijk}) =\n\n1\nz0\n\n\u03c6cl(xcl),\n\n(cid:15)\n\ncl\n\nwhere the sum is over the cliques, z0 is a normalizing constant, and the \u03c6cl(xcl) are so-called\nclique potentials depending on observations xcl within the subgraph formed by cl. the clique\npotentials do not have to be density functions but contain the dependencies in cl. therefore,\nthe estimation is based on marginals that are determined by the cliques (for general algorithms\nbased on the decomposition see, for example, lauritzen, 1996).\n\nfor poisson sampling, the fisher matrix f (\u02c6\u03bb) has the simple form x t diag(\u03bc)x, yielding\n\nthe approximation\n\ncov(\u02c6\u03bb) \u2248 (x t diag(\u02c6\u03bc)x)\n\n\u22121.\n\nfor multionomial sampling one has to separate the intercept, which is fixed by the sample size.\nfor the corresponding model log(\u03bc) = \u03bb01+x\u03b3 the fisher matrix is x t (diag(\u03bc)\u2212\u03bc\u03bct )x,\nyielding the approximation\n\ncov(\u02c6\u03b3) \u2248 (x t (diag(\u02c6\u03bc) \u2212 \u02c6\u03bc\u02c6\u03bct )x)\n\n\u22121.\n\nml estimates for both sampling distributions can be computed within a closed framework. let\n\u03bc =\n\ni \u03b3) denote the total expected cell counts and\n\ni exp(\u03bb0 + xt\n\ni \u03bci =\n\n\u03c0i = \u03bci(cid:14)\n\n(cid:14)\n\n=\n\nj \u03bcj\n\nexp(\u03bb0 + xt\ni \u03b3)\nj exp(\u03bb0 + xt\n\nj \u03b3)\n\n=\n\nexp(xt\ni \u03b3)\nj exp(xt\n\nj \u03b3)\n\n(cid:14)\n\nthe relative expected cell counts, which do not depend on \u03bb0. with x =\ntotal count one obtains the log-likelihood for the poisson distribution:\n\ni xi representing the\n\n(cid:7)\n\n(cid:7)\n\n(cid:7)\n\nl(\u03bb0, \u03b3) =\n\nxi log(\u03bci) \u2212\n\n\u03bci =\n\nxi(\u03bb0 + xt\n\ni \u03b3) \u2212 \u03bc = x\u03bb0 +\n\nxi(xt\n\ni \u03b3) \u2212 \u03bc.\n\ni\n\ni\n\ni\n\ni\n\n(cid:14)\n(cid:7)\n\n(cid:14)\n\n(cid:14)\n\n "}, {"Page_number": 365, "text": "12.8. inference for log-linear models\n\n353\n\nby including x log(\u03bc) \u2212 x log(\u03bc) one obtains the additive decomposition\n\n(cid:7)\n\n(cid:7)\ni \u03b3) \u2212 x log(\n\nl(\u03bb0, \u03b3) = {\n\nxi(xt\n\nexp(xt\n\ni \u03b3))} + {x log(\u03bc) \u2212 \u03bc}.\n\ni\n\ni\n\n(cid:14)\n\nthe first term is the log-likelihood of a multinomial distribution (x1, x2, . . . ) \u223c m(x, \u03c0)),\nand the term x log(\u03bc) \u2212 \u03bc is the log-likelihood of a poison distribution x \u223c p (\u03bc). therefore,\nmaximization of the first term, which does not include the intercept, yields estimates \u02c6\u03b3 for\nmultinomial sampling, conditional on the number of cell counts x. maximization of the poisson\nlog-likelihood yields \u02c6\u03bc = x, which determines the estimate of \u03bb0, since \u03bc = c exp(\u03bb0), where\nj \u03b3) is just a scaling constant determined by the maximization of the first term.\nc =\na similar decomposition of the poisson log-likelihood holds for the product-multinomial\ndistribution. computation as well as inference can be based on the same likelihood with condi-\ntioning arguments. for details see palmgren (1981) and lang (1996a).\n\nj exp(xt\n\n12.8.2 testing and goodness-of-fit\nlet the cell counts be given by the vector xt = (x1, . . . , xn ), where n is the number of cells\nand only a single index is used for denoting the cell. the vector \u02c6\u03bc = (\u02c6\u03bc1, . . . , \u02c6\u03bcn ) denotes the\ncorresponding fitted means. for models with an intercept the deviance has the form\n\nd = 2\n\nxi log( xi\n\u02c6\u03bci\n\n).\n\n(12.7)\n\nwhen considering goodness-of-fit an alternative is pearson\u2019s \u03c72:\n\nn(cid:7)\n\ni=1\n\nn(cid:7)\n\ni=1\n\np =\n\u03c72\n\n(xi \u2212 \u02c6\u03bci)2\n\n\u02c6\u03bci\n\n.\n\n(cid:14)\n\nfor fixed n, both statistics have an approximate \u03c72-distribution if the assumed model holds and\nthe means \u03bci are large. the degrees of freedom are n \u2212 p, where p is the number of estimated\nparameters. the degrees of freedom are computed from the general rule\n\nnumber of parameters in the saturated model \u2013 number of parameters in the assumed\nmodel.\n\nmore concisely, the number of parameters is the number of linearly independent parameters.\ni \u03bba(i) = 0 implies that the effective number of parameters\nfor example, the restriction\n\u03bba(i), i = 1, . . . , i, is i \u2212 1 since \u03bba(i) = \u2212\u03bba(1) \u2212 \u00b7\u00b7\u00b7 \u2212 \u03bba(i\u22121).\nlet us consider an example for three-way tables. the saturated model has ijk parameters\n(corresponding to the cells) for poisson data, but ijk \u2212 1 parameters for multinomial data,\nijk \u03bcijk = 1 applies. for the independence model the number of param-\nsince the restriction\neters is determined by the i \u2212 1 parameters \u03bba(i), the j \u2212 1 parameters \u03bbb(j), and the k \u2212 1\nparameters \u03bbc(k). for poisson data one has an additional intercept that yields the difference:\n\n(cid:14)\n\ndf = ijk \u2212 (1 + i \u2212 1 + j \u2212 1 + k \u2212 1) = ijk \u2212 i \u2212 j \u2212 k + 2.\n\nfor multinomial data, the restriction\nby 1) and one obtains\n\nijk \u03bcijk = 1 applies (reducing the number of parameters\n\ndf = {ijk \u2212 1} \u2212 {i \u2212 1 + j \u2212 1 + k \u2212 1} = ijk \u2212 i \u2212 j \u2212 k + 2,\n\n(cid:14)\n\n "}, {"Page_number": 366, "text": "354\n\nchapter 12. the analysis of contingency tables\n\n(cid:14)\n\nwhich is the same as for poisson data. in general, the degrees of freedom of the approximate\n\u03c72-distribution are the same as for the sampling schemes. for obtaining asymptotically a \u03c72-\ni \u03bci \u2192 \u221e with \u03bci/\u03bcj being constant for poisson data and\ndistribution one has to assume\nn \u2192 \u221e for multinomial data and product-multinomial data, where in the latter case a constant\nratio between n and the sampled subpopulation is assumed (for a derivation of the asymptotic\ndistribution see, for example, christensen, 1997, section 2.3).\n\nthe analysis of deviance as given in section 3.7.2 provides test statistics for the comparison\nof models. models are compared by the difference in deviances. if \u02dcm is a submodel of m, one\nconsiders\n\nd( \u02dcm|m) = d( \u02dcm) \u2212 d(m).\n\n(12.8)\n\nthe deviance (12.7) may be seen as the difference between the fitted model and the saturated\nmodel since the deviance of the saturated model, which has a perfect fit, is zero.\n\na hierarchical submodel is always determined by assuming that part of the parameters\n\nequals zero. for example, the model ab/c assumes that\n\nh0 : \u03bbac(ik) = \u03bbbc(jk) = \u03bbabc(ijk) = 0 for all i, j, k.\n\nthe deviance for the model ab/c may also be seen as a test statistic of the null hypothesis\nh0. when using the difference (12.8) one implicitly tests that the parameters that are contained\nin m but not in \u02dcm are zero, given that model m holds.\n\nexample 12.3: birth data\nin the birth data example (example 12.1) the variables are gender of the child (g; 1: male; 2: female), if\nmembranes did rupture before the beginning of labour (m; 1: yes; 0: no), if cesarean section has been\napplied (c; 1: yes; 0: no) and if the birth has been induced (i; 1: yes; 0: no). the search for an adequate\nmodel is started by fitting models that contain all interaction terms of a specific order. let m ([m]) denote\nthe model that contains all m-factor interactions. for example, m ([1]) denotes the main effect model\ng/m/c/i. from table 12.10 it is seen that m ([3]), m ([2]) fit well but m ([1]) should be rejected. thus\none considers models between m ([2]) and m ([1]). starting from m ([2]), reduced models are obtained\nby omitting one of the six two-factor interactions at a time. for example, m ([2])\\gm denotes the model\nthat contains all two-factor interactions except gm. the difference of deviances, for example, for model\nm ([2]) and model m ([2])\\gm, is an indicator of the relevance of the interaction gm. it is seen that the\ninteractions m c, ci, and m i should not be omitted. the model g/m c/m i/ci shows a satisfying fit\nwhile further reduction by omitting g is inappropriate.\n\nthe model g/m c/m i/ci is not a graphical model. the smallest graphical model that contains\ng/m c/m i/ci is the model g/cm i, which is shown in figure 12.4.\nit means that i, c, m are\ninteracting but are independent of gender. the gender of the child seems not to be connected to the\nvariables membranes, cesarean section, and induced birth.\n\n12.9 model selection and regularization\nmodel selection is usually guided by the objective of the underlying study. if a specific associ-\nation structure is to be investigated, the analysis can be reduced to testing if certain interaction\nterms can be omitted, which is equivalent to testing the fit of the correspondingly reduced model\nor, more general, a sequence of models. when no specific hypotheses are to be investigated,\nmodel selection aims at a compromise between two competing goals: sparsity and goodness-\nof-fit. one wants to find models that are close to the data but have an economic representation\nthat allows a simple interpretation.\n\n "}, {"Page_number": 367, "text": "12.9. model selection and regularization\n\n355\n\ntable 12.10: deviances and differences for log-linear models for birth data.\n\ndev.\n\ndf\n\ndifferences\n\ndiff-df\n\ndiff-dev p-value\n\nmodel\n\nm([4])\nm([3])\nm([2])\nm([1])\nm([2]\\gm)\nm([2]\\mc)\nm([2]\\ci)\nm([2]\\gi)\nm([2]\\gc)\nm([2]\\mi)\n\n0\n0.834\n4.765\n28.915\n\n5.244\n9.965\n12.167\n6.971\n6.566\n10.100\n\n0\n1 m([3])\u2212m([4])\n5 m([2])\u2212m([3])\n11 m([1])\u2212m([2])\n6 m([2]\\gm) \u2212m([2])\n6 m([2]\\mc) \u2212 m([2])\n6 m([2]\\ci) \u2212 m([2])\n6 m([2]\\gi) \u2212 m([2])\n6 m([2]\\gc) \u2212 m([2])\n6 m([2]\\mi) - m([2])\n\nm(g/mc/mi/ci)\nm(mc/mi/ci)\n\n8.910\n19.428\n\n8 m(g/ci/mi/ci)-m([2])\n9 m(ci/mi/ci)-m([2])\n\n1\n4\n6\n\n1\n1\n1\n1\n1\n1\n\n3\n4\n\n0.834\n3.931\n24.150\n\n0.478\n5.200\n7.402\n2.206\n1.801\n5.334\n\n4.145\n14.663\n\n0.361\n0.415\n0.000\n\n0.489\n0.023\n0.007\n0.137\n0.180\n0.021\n\n0.246\n0.005\n\ng\n\n.\n\n0\n2\n\ny\n\n5\n1\n\n.\n\n0\n1\n\n.\n\ni\n\nc\n\nm\n\n1.0\n\n1.2\n\n1.4\n\n1.6\n\n1.8\n\n2.0\n\n2.2\n\nfigure 12.4: graphical model for birth data.\n\nx\n\nseveral model selection procedures for log-linear models were proposed. some try to ac-\ncount for the selection error by using multiple testing strategies, while others rely on screening\nprocedures (for references see section 12.11). more recently, regularization methods for the\nselection of log-linear and grahical models have been developed. the methods are particularly\nattractive for finding sparse solutions that fit the data well. in particular, in bioinformatics the\ngoal to identify relevant structure is very ambitious. with thousands of variables in genomics, it\nis to be seen if the selection strategies are sufficiently reliable. however, the strategies are also\nuseful when the number of variables is much smaller but too large for the fitting of all possible\nmodels.\na strategy that is strongly related to the regularization methods in chapter 6 has been given\nby dahinden et al. (2007). let x1, . . . , xp denote the factors, where xj \u2208 {1, . . . , kj} and\ni = {1, . . . , p} denote the index set of factors. by using subsets a \u2282 i to define the main and\ninteraction terms, the design matrix of the log-linear log(\u03bc) = x\u03bb can be decomposed into\n\nx = [x a1\n\n| . . .|x am],\n\nwhere x aj refers to a specific main or interaction term. for example, x{1,2} refers to the\ninteraction terms of variables x1, x2. correspondingly, let \u03bbaj denote the vector of main or\ninteraction parameters. the penalized log-likelihood, considered in chapter 6, has the form\nlp(\u03b2) = l(\u03b2) \u2212 \u03bb\n2 j(\u03b2), where l(\u03b2) is the usual log-likelihood, j(\u03b2) represents a penalty term\nand \u03bb is a tuning parameter. then the grouped lasso (section 6.2.2) can be applied by using the\n\n "}, {"Page_number": 368, "text": "356\n\npenalty\n\nchapter 12. the analysis of contingency tables\n\ng(cid:7)\n\n(cid:16)\n\nj(\u03bb) =\n\ndfj(cid:16)\u03bbaj\n\n(cid:16)2,\n\n(12.9)\n\nj=1\n\n(cid:16)2 = (\u03bb2\n\naj ,1 + \u00b7\u00b7\u00b7 + \u03bb2\n\nwhere (cid:16)\u03bbaj\n)1/2 is the l2-norm of the parameters of the jth group\nof parameters, which comprises dfj parameters. the penalty encourages sparsity in the sense\nthat either \u02c6\u03bbaj = 0 or \u03bbaj ,s (cid:8)= 0 for s = 1, . . . , dfj. if one has one binary variable x1 and a\nvariable x2 with three categories, for example, the interaction term comprises two parameters\n\u03bb12(11), \u03bb12(12) and the l2-norm of the parameters is (\u03bb2\n\naj ,dfj\n\n12(11) + \u03bb2\n\nwhen using the grouped lasso the resulting model will in general be non-hierarchical. of\ncourse, it is easy to fit the corresponding hierarchical model with all the necessary marginal\neffects included. however, if one single high-order interaction term is selected, the resulting\nmodel can be quite complex. therefore, dahinden et al. (2007) proposed starting the selection\nprocedure not only from the full model but from all models m([m]), which contains all m-\nfactor interactions. then the best model is selected.\na strategy that is quite common is to start from m([2]), which contains all two-factor\ninteractions. for binary variables xi \u2208 {0, 1}, the approach is usually based on the ising\nmodel, which assumes that the joint probabilities are given by\n\n12(12))1/2.\n\np (x1, . . . , xp) = exp(\n\n\u03b8jkxjxk \u2212 \u03c6(\u03b8)),\n\n(cid:7)\n\n(j,k)\u2208e\n\nwhere the normalizing function \u03b8 contains the parameters \u03b8jk, and the sum is over the edges\ne of a graphical model (see, for example, ravikumar et al., 2009). for technical reasons an\nartificial variable x0 = 1 and edges between x0 and all the other variables are included. for\nlog-linear models, the ising model with all possible edges is equivalent to the multinomial\nmodel that contains all two-factor interactions. for the conditional model, conditioned on the\nother variables, one obtains a main effect model, which in the parametrization of the ising\nmodel is given by\n\np (xj = 1|x1 = x1, . . . , xp = xp) =\n\nexp(\n\n1 + exp(\n\n(j,k)\u2208e \u03b8jkxk)\n(j,k)\u2208e \u03b8jkxk) .\n\n(cid:14)\n(cid:14)\n\n(exercise 12.15). the model is equivalent to a main effect logit model with response variable\nxj and explanatory variables xk that are linked to xj within the graph. if the relevant two-\nfactor interactions are identified, it is straightforward to identify the corresponding graphical\nmodel. however, starting from a two-factor interaction model has the disadvantage that all\nhigher interaction terms are neglected during the selection procedure. it might be more ap-\npropriate to enforce sparse modeling by administering stronger penalties on higher interaction\nterms or by strictly fitting hierarchical models within a boosting procedure.\n\nexample 12.4: birth data\nlet us consider again the birth data (example 12.3). figure 12.5 shows the coefficient build ups for the\nfitting of a log-linear model with two-factor interactions, where the two-factor interactions are penalized\nby (12.9), while main effects are not penalized. the coefficient buildups show the parameter estimates\nfor varying degrees of smoothing \u03bb; here they are plotted against (cid:9)\u03b2(cid:9). at the right end no penalty is\nexerted and the model that contains all two-factor interactions is fitted. the solid lines show the two-factor\ninteractions, and the dashed lines represent the main effects. since the main effects are not penalized,\nthey remain rather stable. the vertical lines in figure 12.5 show the models that are selected by use\nof aic and bic. the stronger criterion, bic, yields a model that contains only the strong interactions\nm c, m i, ci (figure 12.4). the graphical model that contains these interactions is the same as found\n\n "}, {"Page_number": 369, "text": "12.10. mosaic plots\n\n357\n\nbic\n\naic\n\n5\n0\n\n.\n\n0\n0\n\n.\n\n.\n\n5\n0\n\u2212\n\n.\n\n0\n1\n\u2212\n\n.\n\n5\n1\n\u2212\n\nc:i\ng:i\n\nm:g\nc:g\n\nm:i\n\nc:m\n\n0.0\n\n0.5\n\n1.0\n\n1.5\n\n2.0\n\n2.5\n\nfigure 12.5: coefficient buildups for a log-linear model with two-factor interactions\n(birth data).\n\nbefore, g/m c/m i/ci. if one uses aic, the rather weak interaction gi also has to be included. as was\nto be expected, bic yields a sparser model.\n\nexample 12.5: leukoplakia\nin example 12.2 one wants to examine the association between the occurrence of leukoplakia (l), alcohol\nintake in grams of alcohol (a), and smoking habits (s). figure 12.6 shows the coefficient buildups for\nthe penalized fitting of a log-linear model with two-factor interactions. aic as well as bic (vertical line)\nsuggest that the interaction between leucoplakia and alcohol intake is not needed. leukoplakia and alcohol\nseem to be conditional independent given smoking habits, yielding the model sl/sa (see also exercise\n12.11).\n\n1\n\n0\n\n1\n\u2212\n\n2\n\u2212\n\n3\n\u2212\n\ns:l\nl:a1\n\nl:a2\n\nl:a3\n\ns:a1\n\ns:a2\ns:a3\n\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\nfigure 12.6: coefficient buildups for a log-linear model with two-factor interactions,\nleucoplakia data; solid lines show the two-factor interactions, and the dashed lines show\nthe main effects.\n\n12.10 mosaic plots\nfor low-dimensional contingency tables as in the preceding example a helpful graphical repre-\nsentation is the mosaic plot. it starts as a square with length one. the square is divided first into\nbars whose widths are proportional to the relative frequencies associated with the first categor-\nical variable. then each bar is split vertically into bars that are proportional to the conditional\n\n "}, {"Page_number": 370, "text": "358\n\nchapter 12. the analysis of contingency tables\n\nprobabilities of the second categorical variable. additional splits can follow for further vari-\nables. in figure 12.7, left panel, the first split distinguishes between leukoplakia, yes or no, and\nthe widths of the columns are proportional to the percentage of observations. the vertical splits\nrepresent the conditional relative frequencies within the columns and are proportional to the\nheights of the boxes. under independence, the heights would be the same, which is not the case\nin this example. in the right panel of figure 12.7, one more split that represents the conditional\nrelative frequencies of alcohol consumption given the status of leukoplakia and smoking is in-\ncluded. it is seen that the distribution of alcohol consumption varies considerably with smoking\nbehavior, but the variation is much weaker if one compares the two groups with and without\nleukoplakia. this supports that the interaction effect between smoking and alcohol is needed\nwhereas the association between leukoplakia and alcohol can be neglected.\n\nyes\n\nno\n\nno\n\nyes\n\n2\n\n3 4\n\nno\n\nno\n\n2\n\n34\n\nr\ne\nk\no\nm\ns\n\ns\ne\ny\n\no\nn\n\nr\ne\nk\no\nm\ns\n\ns\ne\ny\n\no\nn\n\nleukoplakia\n\nleukoplakia\n\nfigure 12.7: mosaic plots for leukoplakia data. left: leukoplakia and smoking; right:\nleukoplakia, smoking and alcohol consumption (alcohol in categories no, 1, 2, 4).\n\nfurther reading\n\n12.11\nsurveys on log-linear and graphical models. an early summary of log-linear models was\ngiven by bishop et al. (1975), also available as bishop et al. (2007). more on log-linear models\nis also found in christensen (1997). an applied treatment of graphical models is in the book of\nwhittaker (2008), and a more mathematical treatment is found in lauritzen (1996).\n\nmodel selection. selection among models by multiple test procedures was considered\nby aitkin (1979, 1980). alternative strategies including screening procedures were given by\nbrown (1976), benedetti and brown (1978), and edwards and havranek (1987).\n\nordinal association and smoothing. ordinal association models, which use assigned scores,\nwere considered by goodman (1979, 1981b, 1983, 1985), haberman (1974), and agresti and\nkezouh (1983). an overview was given by agresti (2009). smoothing for sparse tables was\ninvestigated by simonoff (1983, 1995), overviews are found in simonoff (1996), simonoff and\ntutz (2000).\n\nexact inference. a survey of exact inference for contingency tables was given by agresti\n\n(1992b); see also agresti (2001).\n\nr packages. log-linear models can be fitted by use of the r-function loglin from the pack-\nage stats, which applies an iterative-proportional-fitting algorithm. the function loglm from the\n\n "}, {"Page_number": 371, "text": "12.12. exercises\n\n359\n\npackage mass provides a front-end to loglin, to allow log-linear models to be specified and fit-\nted in a manner similar to that of other fitting functions, such as glm. the function mosaicplot\nfrom the package vcd can be used to generate mosaic plots.\n\n12.12 exercises\n\n12.1 let y1, . . . , yn denote independent poisson variables, yi \u223c p(\u03bbi). show that one obtains for\n(y1, . . . , yn ) conditional on the total sum n0 = y1, +\u00b7\u00b7\u00b7 + yn , the multinomial distribution m(n0,\n(\u03c01, . . . , \u03c0n )) with \u03c0i = \u03bbi/(\u03bb1 + \u00b7\u00b7\u00b7 + \u03bbn ).\n12.2 consider the log-linear model for a (2 \u00d7 2)-contingency table:\n\nlog(\u03bcij) = \u03bb0 + \u03bba(i) + \u03bbb(j) + \u03bbab(ij)\n\nwith multinomial distribution and appropriate constraints.\n\n(a) derive the parameters \u03bba(i), \u03bbb(j), \u03bbab(ij) as functions of odds and odds ratios for symmetrical\n\nconstraints and when the last category parameter is set to zero.\n\n(b) show that \u03bbab(ij) = 0 is equivalent to the independence of the variables xa, xb, which generate\n\nthe rows and columns.\n\n12.3 consider the log-linear model for a (2 \u00d7 2)-contingency table:\n\nlog(\u03bcij) = \u03bb0 + \u03bba(i) + \u03bbb(j) + \u03bbab(ij),\n\nwhich describes the distribution of product-multinomial sampling with fixed marginals xi..\n\n(a) specify the appropriate constraints for the parameters.\n(b) show that \u03bbab(ij) = 0 is equivalent to the homogeneity of the response xa across levels of xb.\n\n12.4 show that the log-linear model ab/ac in a multinomial distribution (i \u00d7 j \u00d7 k)-contingency table\nis equivalent to assuming that the variables xb and xc are conditionally independent given xa.\n\n12.5 show that for the log-linear model ab/ac, ml estimates of means are given by \u02c6\u03bcijk = xij+xi+k/\nxi++. use the estimation equations that have to hold.\n12.6 find a set of probabilities {\u03c0ijk} for three-way tables, where \u03c0ijk = p (xa = i, xb = j, xc = k),\nsuch that xa and xb are conditionally independent given xc = k but the variables xa and xb are\n(marginally) dependent.\n\n12.7 compute the parameters of a three-way contingency table as functions of the underlying means\n{\u03bcijk} for symmetric side constraints.\n\n12.8 consider the log-linear models\n\nab/ac/ad/de, abc/bcd/bde/cde, ab/bce/cde/ae.\n\nare these models graphical? if they are, draw the graph; if not, give the smallest graphical model that\nincludes the corresponding model and draw the graph.\n\n12.9\n\n(a) interpret the model ae/bc/cd/bd.\n\n(b) give all the independence relations that are implied by the model ab/bc/cd.\n\n "}, {"Page_number": 372, "text": "360\n\nchapter 12. the analysis of contingency tables\n\ntable 12.11: regular reader of women\u2019s journal with employment, age, and education.\n\nworking(w)\n\nyes\n\nage (a)\n18 \u2013 29\n\n30 \u2013 39\n\n40 \u2013 49\n\nno\n\n18 \u2013 29\n\n30 \u2013 39\n\n40 \u2013 49\n\nregular reader\n\nyes\n\nno\n\neducation (e)\n\nl1\nl2\nl3\nl4\nl1\nl2\nl3\nl4\nl1\nl2\nl3\nl4\nl1\nl2\nl3\nl4\nl1\nl2\nl3\nl4\nl1\nl2\nl3\nl4\n\n1\n32\n20\n8\n9\n31\n11\n5\n1\n12\n5\n1\n3\n12\n19\n14\n1\n12\n14\n4\n11\n14\n8\n1\n\n14\n49\n34\n3\n23\n57\n26\n7\n33\n50\n11\n7\n24\n41\n20\n13\n37\n68\n43\n7\n54\n53\n15\n3\n\n12.10 the contingency table 12.11 shows data from a survey on the reading behavior of women (hamerle\nand tutz, 1980). the cells are determined by working (yes/no), age in categories, education level (l1 to\nl4), and if the women is a regular reader of a specific journal. find an appropriate log-linear model for\nthe data.\n\n12.11 fit log-linear models for the leucoplakia data (table 12.2) and select an appropriate model (compare\nto example 12.5).\n\n12.12 in contingency table 12.12 defendants in cases of multiple murders in florida between 1976 and\n1987 are classified with respect to death penalty, race of defendent, and race of victim (see agresti, 2002;\nradelet and pierce, 1991).\n\n(a) investigate the association between defendant\u2019s race and death penalty when the victim\u2019s race is\n\nignored (from the marginal table).\n\n(b) investigate the association between defendant\u2019s race and death penalty when the victim\u2019s race is\n\ntaken into account.\n\n(c) use mosaic plots to visualize the considered models.\n\n12.13 consider the marginal association between the binary factors xa and xb, measured in odds ratios:\n\nshow that the value is the same as for the conditional association between xa and xb given xc = k:\n\n\u03bc11+/\u03bc12+\n\u03bc21+/\u03bc22+\n\n.\n\n\u03bc11k/\u03bc12k\n\u03bc21k/\u03bc22k\n\n.\n\n "}, {"Page_number": 373, "text": "12.12. exercises\n\n361\n\ntable 12.12: death penalty verdict by defendant\u2019s race and victim\u2019s race.\n\nvictims\u2019s\nrace\n\nwhite\n\nblack\n\ndefendant\u2019s\n\nrace\n\nwhite\nblack\nwhite\nblack\n\ndeath penalty\nyes\nno\n\n53\n11\n0\n4\n\n414\n37\n16\n139\n\n12.14 consider the saturated log-linear model for three variables x,xb, xc and a multinomial distribu-\ntion. derive the parameters of the logit model with reference category (xa = i, xb = j, xc = k).\nwhich constraints hold for the parameters?\n12.15 for binary variables xi \u2208 {0, 1}, the ising model specifies that the joint probabilities are given\n(j,k)\u2208e \u03b8jkxjxk \u2212 \u03c6(\u03b8)), where the normalizing function \u03b8 contains the\nby p (x1, . . . , xp) = exp(\nparameters \u03b8jk, and the sum is over the edges e of a graphical model. let x0 = 1 denote an additional\nvariable and let all edges (0j), corresponding to the term \u03b80jx0xj, be included.\n\n(cid:2)\n\n(a) show that the conditional probability p (xj = 1|x1 = x1, . . . , xp = xp) follows a main effect\n\nlogit model that contains the parameters {\u03b8jk|(j, k) \u2208 e, k (cid:11)= j}.\n\n(b) show that the log-linear model with all two-factor interactions is an ising model.\n\n "}, {"Page_number": 374, "text": " "}, {"Page_number": 375, "text": "chapter 13\n\nmultivariate response models\n\nin many studies the objective is to model more than one response variable. for each unit in\nthe sample a vector of correlated response variables, together with explanatory variables, is\nobserved. two cases are most important:\n\n\u2022 repeated measurements, when the same variable is measured repeatedly at different times\n\nor/and under different conditions;\n\n\u2022 different response variables, observed on one subject or unit in the sample.\n\nrepeated measurements occur in most longitudinal studies. for example, in a longitudinal\nstudy measurements on an individual may be observed at several times under possibly vary-\ning conditions. in example 1.4 (chapter 1) an active ingredient is compared to a placebo by\nobserving the healing after 3, 7, and 10 days of treatment. in example 13.1, the number of\nepileptic seizures is considered at each of four two-week periods. although they often do,\nrepeated responses need not refer to different times. response variables may also refer to dif-\nferent questions in an interview or to the presence of different commodities in a household. in\nexample 13.2 the two, possibly correlated responses are the type of birth (ceaserian or not)\nand the stay of the child in intensive care (yes or no). responses may also refer to a cluster\nof subjects; for example, when the health status of the members of a family is investigated,\nthe observed responses form a cluster linked to one family. in example 10.8, where the health\nstatus of trees is investigated, clusters are formed by the trees measured at the same spot. it\nhas to be expected that observations within a cluster are less different than observations from\ndifferent clusters. therefore, one has to assume that the observations are correlated.\n\nexample 13.1: epilepsy\nthall and vail (1990) used data from a randomized clinical trial in which 59 patients with epilepsy were\nrandomized into groups receiving an antiepileptic drug (progabide) or a placebo. the number of seizures\nsuffered in each of four two-week periods were recorded along with a baseline seizure count for the\nweeks prior to treatment. in addition, the age of the patient is known (see also everitt and hothorn, 2006;\nfitzmaurice et al., 2004).\n\nexample 13.2: birth data\nin the birth study (see also example 12.1) there are several responses that can be linked to explanatory\nvariables. we will focus on the bivariate response cesarean section (c; 1: yes; 0: no) and intensive care\n(ic; 1: yes; 0: no). explanatory variables are gender of child (g; 1: male; 2: female), weight of child, age\nof mother, and number of previous pregnancies.\n\n363\n\n "}, {"Page_number": 376, "text": "364\n\nchapter 13. multivariate response models\n\nin regression modeling for correlated categorical responses, one can distinguish several\n\nmain approaches:\n\n\u2022 conditional models specify the conditional distribution of each component of the re-\nsponse vector given other components and covariates. in the case of repeated measure-\nments, conditional models often model the transition between categories and are called\ntransition models.\n\n\u2022 marginal models focus on the specification of the marginal distribution, that is, the distri-\nbution of the single components of the response vector. the marginal response is modeled\nconditionally on covariates but not on other responses.\n\n\u2022 cluster-specific approaches like random effects models allow for cluster- or subject-\nspecific effects. thus each cluster has its own effect, and the responses are conditional\non the covariates and cluster-specific effects.\n\nin the following we give simple examples of these approaches. for simplicity, let the\nresponse variables yi1, . . . , yit , observed together with covariate vector xi, be binary with\nyit \u2208 {0, 1}. given xi, the binary variables yi1, . . . , yit form a contingency table with 2t\ncells, with the response being multinomially distributed. when t is large, it is hard to spec-\nify the full multinomial distribution with its 2t \u2212 1 parameters, especially when the modeling\nshould account for the effects of potentially continuous covariates.\n\nconditional model\nwhen measurements yi1, . . . , yit are repeated measurements taken for unit i at times t =\n1, . . . , t , the response at time t may depend on previous responses. the simple transition\nmodel\n\np (yit = 1|xi, yi,t\u22121) = h(\u03b20 + yi,t\u22121 + xt\n\ni \u03b2)\n\nassumes that the response at time t depends on the covariates xi as well as on the previous\nresponse. the model contains an autoregressive term of order one, and the response is condi-\ntional on yi,t\u22121 and xi. interpretation of \u03b2 should take the presence of the autoregressive term\ninto account. typically one assumes independence of the responses given the covariates and\nprevious responses.\n\nmarginal model\na typical marginal model specifies the marginal responses in the form\n\np (yit = 1|xit) = h(\u03b20 + xt\n\nit\u03b2),\n\nwhere the simple explanatory variable xi is replaced by a sequence of explanatory variables\nxi1, . . . , xit , which can vary across measurements. it should be noted that only the marginal\nprobabilities are modeled and not the whole multinomial distribution. ml estimation becomes\nhard when t is large. therefore, one frequently considers the association between yi1, . . . , yit\nas nuisance parameters and estimates by solving generalized estimation equations.\n\nsubject-specific models\na simple model that contains subject-specific intercepts is\np (yit = 1|xit) = h(\u03b2i + xt\n\nit\u03b2),\n\n "}, {"Page_number": 377, "text": "13.1. conditional modeling\n\n365\n\nwhere \u03b2 is a fixed effect that is common to all clusters but the \u03b2i\u2019s are subject- or cluster-\nspecific individual effects. thus each cluster has its own response level, a flexibility that seems\nappropriate in many applications. by assuming individual effects, the number of parameters is\nconsiderably increased, in particular when the \u03b2i\u2019s are assumed to be fixed effects. therefore,\nin subject-specific approaches it is often assumed that the individual effects, denoted by bi,\nare random following a specified distribution function, for example, the normal distribution\nn(0, \u03c32\n\nb ). the corresponding random effects model is\n\np (yit = 1|xit, bi) = h(bi + xt\n\nit\u03b2).\n\nthe distribution of the total response vector (yi1, . . . , yit ) is obtained by assuming the condi-\ntional independence of yi1, . . . , yit given bi.\n\nparameters in marginal and subject-specific approaches have different interpretations. if\none of the explanatory variables increases by one unit, the linear predictor of the marginal\nmodel, \u03b20+xt\nit\u03b2, increases or decreases by the corresponding parameter, which in turn changes\nthe response probability. the same happens for the linear predictor of the random effects model\nbi + xt\nit\u03b2. but since all other explanatory variables are fixed, the change in the linear predic-\ntor starts from different levels for different individuals. because h typically is a non-linear\nfunction, for example, the logistic distribution function, the effect on the response probability\ndepends on the individual parameter bi. therefore, the effect is subject-specific or what can\nbe called a within-subject effect. it is conditional on the random effect value. in contrast, the\neffect of parameters in marginal models is the same across all clusters; the effects are also\ncalled population-averaged effects. the difference vanishes when h is the identity, as is often\nassumed in normal distribution models. for non-linear models like the logit model, however,\nthe population-averaged effects of marginal models tend to be smaller than subject-specific ef-\nfects (for details see chapter 14). for more discussion on the distinction in interpretation see\nalso neuhaus et al. (1991) and agresti (1993c).\n\nin the remainder of the chapter we will consider conditional and marginal models. first, in\nsection 13.1, we give conditional approaches including transition models and symmetric con-\nditional models. then likelihood-based approaches to marginal modeling are briefly sketched.\nthe general marginal model that treats association as a nuisance and uses generalized estima-\ntion equations for estimation is outlined in section 13.2. in section 13.4 we consider a specific\nproblem in marginal modeling termed marginal homogeneity. we investigate if the distribution\nof responses changes over measurements. the modeling of random effects is considered in a\nseparate chapter (chapter 14).\n\n13.1 conditional modeling\n13.1.1 transition models and response variables with a natural order\nin many applications the responses have some natural ordering. in particular, when measure-\nments are taken at discrete time points the sequence yt, t = 1, 2, . . . , denoting measurements\nat time t, forms a stochastic process. it is natural to assume that the responses at time t depend\non previously observed responses, which serve as explanatory variables.\n\nmodels that make use of that dependence structure are based on the decomposition\n\np (y1, . . . , yt|x) = p (y1|x) \u00b7 p (y2|y1, x)\u00b7\u00b7\u00b7 p (yt|y1, . . . , yt\u22121, x),\n\n(13.1)\n\nwhere yt has discrete values that form the state space of the process.\n\na general type of model is obtained by modeling the transitions within the decomposition\n\n(13.1) through a generalized linear model:\n\np (yt = r|y1, . . . , yt\u22121, x) = h(zt\n\nt \u03b2),\n\n(13.2)\n\n "}, {"Page_number": 378, "text": "366\n\nchapter 13. multivariate response models\n\nwhere zt = z(y1, . . . , yt\u22121, x) is a function of previous outcomes y1, . . . , yt\u22121 and the vector\nof explanatory variables x. since the response is determined by previous outcomes, conditional\nmodels of this type are sometimes called data-driven.\n\nmarkov chains\na markov chain is a special transition model of the type (13.2). without further explanatory\nvariables one considers the discrete stochastic process yt, t = 1, 2, . . . , where yt \u2208 {1, . . . , m}.\nthe process is called a kth-order markov chain if the conditional distribution of yt, given\nyt\u22121, . . . , y1, is identical to the distribution of yt, given yt\u22121, . . . , yt\u2212k. thus the future of the\nprocess depends on the past only through the states at the previous k measurements. the sim-\nplest model is the first-order markov model, which assumes p (yt|yt\u22121, . . . , y1) = p (yt|yt\u22121)\nfor all t. it can be specified as a multicategorical logit model:\n\nlog(p (yt = r|yt\u22121)/p (yt = m|yt\u22121)) = xa(1)\u03b2r1 + \u00b7\u00b7\u00b7 + xa(m)\u03b2rm,\n\nwhere xa(r) = i(yt\u22121 = r), with i(.) denoting the indicator function, represents (0-1)-coding\nof the response at time t \u2212 1. since the parameters do not depend on time, the markov chain is\nhomogeneous.\nwhen additional explanatory variables are available markov-type transition models of the\nfirst order assume p (yt = r|y1, . . . , yt\u22121, x) = p (yt|yt\u22121, x). for binary outcomes a simple\nmodel is\n\n3\n(cid:28)\n(cid:27)\n(cid:26)\n(cid:25)\np (y1 = 0|x)\np (y1 = 1|x)\np (yt = 1|y1, . . . , yt\u22121, x)\np (yt = 0|y1, . . . , yt\u22121, x)\n\nlog\n\nlog\n\n= \u03b201 + xt \u03b21,\n\n= \u03b20t + xt \u03b2 + yt\u22121\u03b3t.\n\nregressive models\nmarkov chain models aim to model the transitions for a larger number of repeated measure-\nments. however, conditional models also apply when the sequence of responses is small and\nfixed. in the birth study (example 13.2) one considers various explanatory variables like age of\nmother and gender of child on more than one response. in particular, one considers cesarian\nsection (yes/no) and duration in intensive care (in categories). it is natural to model first the\ndependence of cesarean section (y1) on the covariates and then the dependence of duration in\nintensive care (y2) on the covariates and whether it was a cesarean section. therefore, one\nmodels in the second step the conditional response y1|y2.\n\nfor binary responses y1, . . . , yt bonney (1987) called such models regressive logistic mod-\n\n(cid:25)\n\nels. they have have the form\n\nlog\n\np (yt = 1|y1, . . . , yt\u22121, xt)\np (yt = 0|y1, . . . , yt\u22121, xt)\n\n(cid:26)\n\n= \u03b20 + xt\u03b2 + \u03b31y1 + \u00b7\u00b7\u00b7 + \u03b3t\u22121yt\u22121.\n\nthus the number of included previous outcomes depends on the component yt, but no markov-\ntype assumption is implied. if the variables are multicategorical (with possibly varying numbers\nof categories), regressive models may be based on the modeling approaches for categorical\nresponses as building blocks.\n\nestimation\nlet the data be given by (yit, xit), i = 1, . . . , n, t = 1, . . . , ti. let \u03bcit = e(yit|hit) de-\nnote the conditional mean specified by a simple exponential family and hit = {xi1, . . . , xit,\n\n "}, {"Page_number": 379, "text": "13.1. conditional modeling\n\n367\nyi1, . . . , yi,t\u22121} denote the history of observation i. for a univariate response the model has the\nform\n\n\u03bcit = h(zt\n\nit\u03b2),\n\nwhere zit = zit(hit) is a function of the history. if xi collects the fixed covariates linked to\nthe ith observation, the density of responses yi1, . . . , yit has the decomposition\n\nf(yi1, . . . , yiti\n\n|xi) =\n\nf(yit|yi1, . . . , yi,t\u22121, xi).\n\nti(cid:15)\n\nt=1\n\nfor stochastic covariates one obtains\n\nf(yi1, . . . , yiti\n\n|xi) =\n\nf(yit|yi1, . . . , yi,t\u22121, xi)\n\nti(cid:15)\n\nt=1\n\nti(cid:15)\n\nf(xit|xi1, . . . , xi,t\u22121),\n\nt=1\n\nwhere it is assumed that the covariate process xit|xi1, . . . , xi,t\u22121 does not depend on previous\noutcomes yi1, . . . , yi,t\u22121. if the last factor is not informative for the parameters, it may be omit-\nted in inference, yielding a partial likelihood. under the assumption f(yit|yi1, . . . , yi,t\u22121, xi) =\nf(yit|hit), the (partial) log-likelihood has the form\n\nn(cid:7)\n\n(cid:7)\n\ni=1\n\nt\n\nl(\u03b2) =\n\nlog(f(yit|hit)).\n\nsummation is over t = 1, . . . , ti if all the responses are specified but reduces if in a markov-\ntype model only transitions given lagged responses of order k are specified. then the estimates\nare conditional given the starting values yi1, . . . , yi,k. the conditional score function is\n\ns(\u03b2) =\n\nzit(\u2202h(\u03b7it)/\u2202\u03b7)(yit \u2212 \u03bcit)/\u03c32\n\nit = x t d\u03c2\n\n\u22121(y \u2212 \u03bc),\n\nn(cid:7)\n\n(cid:7)\n\ni=1\n\nt\n\nit = cov(yit|hit) is the conditional variance.\nwhere \u03bcit is the conditional expectation and \u03c32\nthe matrices x, d, \u03c3 collect design vectors, derivatives, and variances, and the vectors y, \u03bc\n(cid:26)\ncollect the responses and means. the conditional information matrix has the form\n\n(cid:25)\n\nf (\u03b2) =\n\nzitzt\nit\n\n\u2202h(\u03b7it)\n\n2\n\n\u2202\u03b7\n\n/\u03c32\n\nit = x t d\u03c2\n\n\u22121dx.\n\nn(cid:7)\n\n(cid:7)\n\ni=1\n\nt\n\n\u22121dx, but the matrices are composed as in section 8.6.1.\n\nmodels may be fitted by using standard software, where yi1, yi1|yi2,. . . , yiti\n\nfor categorical responses the data are given in the form (yit, xit), where yit \u2208 {1, . . . , m}. the\ncorresponding mean is a vector of probabilities and the model has the form \u03c0it = h(z t\nit\u03b2),\nwhere the covariate vector is replaced by the design matrix z it. the score function and the\n\u22121(y \u2212 \u03bc), f (\u03b2) =\nconditional information matrix again have the form s(\u03b2) = x t d\u03c2\nx t d\u03c2\n|yi1, . . . , yi,ti\u22121\nare treated as separate observations. in the case of small ti and large n, inference may be em-\nbedded into the framework of glms. for n = 1 the model with lagged variables represents an\nautoregressive time series model and asymptotically (t \u2192 \u221e) the covariance cov(\u02c6\u03b2) can be\napproximated by the inverse conditional information matrix (for details see kaufmann, 1987;\nfahrmeir and kaufmann, 1987). for an extensive treatment of categorical time series see also\nkedem and fokianos (2002). in cases with more than one unit observed across time, the in-\nverse information matrix may serve as an approximation of the covariance of the estimate but\nrigorous proofs seem not to be available.\n\n "}, {"Page_number": 380, "text": "368\n\nchapter 13. multivariate response models\n\n13.1.2 symmetric conditional models\nif there is no natural ordering of the response variables, one can model them in a symmetric\nway by conditioning on the outcome of the other variables. for simplicity let the response\nvariables y1, . . . , yt , observed together with covariate vector x, be binary with yi \u2208 {0, 1}.\ngiven x, the binary variables y1, . . . , yq form a contingency table with 2t cells. the response\n}), where n(x) is the number of observations\nis multinomially distributed, m(n(x),{\u03c0r1...rt\nat covariate x, and\n\n\u03c0r1...rt = p (y1 = r1, . . . , yt = rt|x)\n\nis the probability of observing the response (r1, . . . , rt ). multinomial responses have already\nbeen treated in chapter 8, but for less structured multinomials. the multinomial distribution\nconsidered here is determined by t binary distributions, therefore forming a strongly structured\nmultinomial distribution. also, models for multinomial distributions that are formed by several\ncategorical responses have been considered before (chapter 12), but without further covari-\nates. in the following it is illustrated that the underlying parametrization is conditional, so that\nmarginal models that include covariates have to be based on more general log-linear models.\n\nwithout covariates the multinomial distribution can be reparameterized as a saturated log-\n\nlinear model (see chapter 12). with (0-1)-coding one obtains\n\nlog(\u03c0y1...yt ) = \u03bb0 + y1\u03bb1 + \u00b7\u00b7\u00b7 + yt \u03bbt + y1y2\u03bb12 + \u00b7\u00b7\u00b7 + y1 . . . yt \u03bb1...t\n\nor, in the form of the logit model,\n\nlog( \u03c0y1...yt\n\u03c00...0\n\n) = y1\u03bb1 + \u00b7\u00b7\u00b7 + yt \u03bbt + y1y2\u03bb12 + \u00b7\u00b7\u00b7 + y1 . . . yt \u03bb1...t ,\n\n(13.3)\n\nwhere \u03c00...0 = p (y0 = 0, . . . , yt = 0) is the reference category.\n\ny2\n\n1\n\u03c011\n\u03c001\n\n0\n\u03c010\n\u03c000\n\ny1\n\n1\n0\n\nin the simplest case, t = 2, one has a (2 \u00d7 2)-contingency table and one easily computes\n\n\u03bb1 = log(\u03c010/\u03c000) = log( p (y1 = 1|y2 = 0)\np (y1 = 0|y2 = 0)\n\u03bb2 = log(\u03c001/\u03c000) = log( p (y2 = 1|y1 = 0)\np (y2 = 0|y1 = 0)\n\u03bb12 = log( \u03c011/\u03c001\n\u03c010/\u03c000\n\n) = p (y1 = 1|y2 = 1)/p (y1 = 0|y2 = 1)\np (y1 = 1|y2 = 0)/p (y1 = 0|y2 = 0) .\n\n) = logit(p (y1 = 1|y2 = 0)),\n) = logit(p (y2 = 1|y1 = 0)),\n\nthus, \u03bb1 represents the log-odds that compare y1 = 1 to y1 = 0, given y2 = 0, and \u03bb2\nrepresents the log-odds that compare y2 = 1 to y2 = 0, given y1 = 0. both parameters contain\nthe effect of one variable given that the other variable takes the value zero. in this sense the\nparametrization is conditional. the interaction parameter \u03bb12 is a log-odds ratio where in the\nnumerator y1 = 1 is compared to y1 = 0 for y2 = 1 and in the denominator y1 = 1 is compared\nto y1 = 0 for y2 = 0.\n\n "}, {"Page_number": 381, "text": "13.1. conditional modeling\n\n369\n\nin the general case with t response variables one obtains for the main and two-factor inter-\n\naction parameters\n\n\u03bbj = log( p (yj = 1|yl = 0, l (cid:8)= j)\np (yj = 0|yl = 0, l (cid:8)= j)\n\u03bbjr = log( p (yj = 1|yr = 1, yl = 0, l (cid:8)= j, r)/p (yj = 0|yr = 1, yl = 0, l (cid:8)= j, r)\np (yj = 1|yr = 0, yl = 0, l (cid:8)= j, r)/p (yj = 0|yr = 0, yl = 0, l (cid:8)= j, r)\n\n) = logit(p (yj = 1|yl = 0, l (cid:8)= j)),\n\n).\n\nthe crucial point is that the parameters characterize the conditional probabilities of the subsets\nof responses given particular values for all the other variables.\n\nas shown by fitzmaurice and laird (1993) for binary vector yt = (y1, . . . , yt ), the density\n\ncan also be given in the form\n\nf(y) = exp(yt \u03bb + wt \u03b3 \u2212 c(\u03bb, \u03b3)),\n\nwhere wt = (y1y2, y1y3, . . . , yt\u22121yt , . . . , y1y2 . . . yt ), \u03bbt = (\u03bb1, . . . , \u03bbt ), \u03b3t = (\u03bb12, . . . ,\n\u03bb12...t ). c(\u03bb, \u03b3) is a normalizing constant given by\n\n(cid:7)\n\nexp(c(\u03bb, \u03b3)) =\n\nexp(rt \u03bb + wt \u03b3),\n\n(cid:14)\n\nr\n\nwhere the sum is over all vectors r \u2208 {0, 1}t . the density is a special case of the partly\nexponential family used by zhao et al. (1992). in the case of two binary variables one obtains\nthe simple form f(y) = exp(y1\u03bb1 + y1\u03bb2 + y1y2\u03bb12 \u2212 c(\u03bb, \u03bb12)), with c(\u03bb, \u03bb12) = log(1 +\nexp(\u03bb1) + exp(\u03bb2) + exp(\u03bb1 + \u03bb2 + \u03bb12).\n\na parsimonious class of conditional logistic models, in which the individual binary vari-\n(1987). they considered the\n\nables are treated symmetrically, was considered by qu et al.\nconditional response probability\n\n(cid:27)\n\n(cid:28)\n\np (yt = 1|yk, k (cid:8)= t; xt) = h\n\n\u03b1(wt; \u03b8) + xt\n\nt \u03b2t\n\n,\n\n(13.4)\n\nwhere h is the logistic function and \u03b1 is an arbitrary function of a parameter \u03b8 and the sum\nwt =\nk(cid:5)=t yk. in model (13.4), \u03b1 depends on the conditioning y\u2019s, whereas the covariate\neffects are kept constant. for the case of two components (y1, y2), the sums w1, w2 reduce to\ny2, y1, respectively. then the simplest choice is a logistic model that includes the conditioning\nresponse as a further covariate:\n\np (yt = 1|yk, k (cid:8)= t; xt) = h(\u03b80 + \u03b81yk + xt\n\nt \u03b2t),\n\nt, k = 1, 2.\n\n(13.5)\n\nother choices for \u03b1(w; \u03b8) were discussed by qu et al. (1987) and conolly and liang (1988).\nthe joint density p (y1, . . . , yt ; xt\nt ) can be derived from (13.4); however, it involves\na normalizing constant, which is a complicated function of the unknown parameters \u03b8 and \u03b2;\nsee prentice (1988) and rosner (1984). full likelihood estimation may be avoided by using a\nquasi-likelihood approach (conolly and liang, 1988).\n\n1 , . . . , xt\n\nwhen using symmetric conditional models one should be aware that one conditions on\ncovariates and the other responses; one is modeling the effect of covariates given other re-\nsponses. therefore, the effect of covariates is measured having already accounted for the effect\nof the other responses. then the covariate effects could be conditioned away because the other\nresponses might also be related to the covariates. moreover, the effects will change if one con-\nsiders a subset of responses, because the conditioning changes. therefore, the dimension of the\nresponse vector should be the same for all observations.\n\n "}, {"Page_number": 382, "text": "370\n\nchapter 13. multivariate response models\n\n13.2 marginal parametrization and generalized log-linear\n\nmodels\n\nfrequently the primary scientific objective is the modeling of covariates on marginal responses,\nfor example, by parameterizing the marginal logits logit(p (yt = 1|yt = 0)). then one has to\nuse representations of the response patterns that differ from the ones considered in the previous\nsection. let us consider again the simple case of two binary responses y1, y2. the response is\ndetermined by the probabilities \u03c011, \u03c010, \u03c001, \u03c000, or, more precisely, by three of these proba-\nbilities. the representation as an exponential family uses the three parameters \u03bb1, \u03bb2, \u03b3, where\n\u03b3 = \u03bb12 is the log-odds ratio for the variables y1, y2. alternatively, one can also use the\nparameters\n\n\u03c01+ = \u03c011 + \u03c010,\n\n\u03c0+1 = \u03c011 + \u03c001,\n\n\u03b3,\n\nwhere p (y1 = 1) = \u03c01+, p (y2 = 1) = \u03c0+1 are the marginal probabilities and \u03b3 is the\nlog-odds ratio. the sets {\u03c011, \u03c010, \u03c001, \u03c000}, {\u03bb1, \u03bb2, \u03b3}, and {\u03c01+, \u03c0+1, \u03b3} are equivalent\nrepresentations of the (2 \u00d7 2)-probability table. the set of parameters {\u03c01+, \u03c0+1, \u03b3} is espe-\ncially attractive when marginal probabilities are of interest. it is also straightforward to include\ncovariates. a simple model that links covariates to these parameters is the model considered by\npalmgren (1989):\n\nlogit(\u03c01+) = xt \u03b21,\n\nlogit(\u03c0+1) = xt \u03b22,\n\nlog(\u03b3) = xt \u03b23.\n\n(13.6)\n\nit should be noted that transforming between different representations is not always easy. of\ncourse, marginal probabilities and log-odds ratios are defined as functions of the response prob-\nabilities \u03c011, \u03c010, \u03c001, \u03c000, but the representation of the response probabilities as functions of\n{\u03c01+, \u03c0+1, \u03b3} is tedious (see equation (13.11)). the parameters from the set {\u03c01+, \u03c0+1, \u03b3} are\nindependent in the sense that any choice \u03c01+, \u03c0+1 \u2208 (0, 1), \u03b3 \u2208 r, yields valid probabilities.\nmoreover, the parameter vector (\u03c01+, \u03c0+1) is orthogonal to \u03b3, meaning that the information\nmatrix is a block-diagonal matrix (cox and reid, 1987).\n\nin the case of two binary response variables, the association can be modeled by just one pa-\nrameter. in the general case, however, in addition to marginals, association terms of higher order\nare needed, so it is advisable to restrict parametric models that include explanatory variables\nto the specification of only a few of them. a class of models that can be used is generalized\nlog-linear models, which are considered briefly in the following.\n\nlog-linear models, as considered in chapter 12, can be represented in the general form\nlog(\u03c0) = x\u03bb, with \u03c0 containing all the probabilities for individual cells and x denoting a de-\nsign matrix. for three binary response variables the vector \u03c0 is given by (\u03c0111, \u03c0112, . . . , \u03c0222).\nmarginals can be obtained by using a linear transformation a\u03c0, where a is a matrix that con-\ntains zeros and ones only. with an appropriately chosen a one can build the general vector of\nmarginals:\n\na\u03c0 = (\u03c01++, \u03c0+1+, . . . , \u03c011+, . . . , \u03c0222)t\n\nor the shorter vector that contains only univariate and bivariate marginals:\n\na\u03c0 = (\u03c01++, \u03c0+1+, . . . , \u03c011+, . . . , \u03c0+22)t .\n\nin a second step one can specify the logarithmic contrasts of interest,\n\n\u03b7 = c log(a\u03c0),\n\n "}, {"Page_number": 383, "text": "13.3. general marginal models: association as nuisance and gees\n\n371\n\nbased on a chosen matrix c. for example, the univariate contrasts are\n\n\u03b71 = log(\u03c01++) \u2212 log(\u03c02++) = logit(p (y1 = 1)),\n\u03b72 = log(\u03c0+1+) \u2212 log(\u03c0+2+) = logit(p (y2 = 1)),\n\u03b73 = log(\u03c0++1) \u2212 log(\u03c0++2) = logit(p (y3 = 1)),\n\nand the bivariate contrasts are\n\n\u03b712 = log(\u03c011+) \u2212 log(\u03c012+) \u2212 log(\u03c021+) + log(\u03c022+),\n\u03b713 = log(\u03c01+1) \u2212 log(\u03c01+2) \u2212 log(\u03c02+1) + log(\u03c02+2),\n\u03b723 = log(\u03c0+11) \u2212 log(\u03c0+12) \u2212 log(\u03c0+21) + log(\u03c0+22).\n\nthe link to the explanatory variable is obtained by specifying a design matrix for the chosen\ncontrasts. with the univariate marginals specified by\n\n\u03b7t = logit(p (yt = 1)) = xt \u03b2t\n\none obtains a model that has the form of a generalized log-linear model:\n\nc log(a\u03c0) = x\u03bb,\n\nwhich is obviously a generalization of the log-linear model log(\u03c0) = x\u03bb. of course, bivariate\nand trivariate marginals also can be linked to the explanatory variables, yielding more complex\nmodels. for multicategorical responses different contrasts have to be used.\n\nmaximum likelihood estimation for the generalized log-linear model is not easy and special-\nized software is needed. for details see mccullagh and nelder (1989), chapter 6, fitzmaurice\nand laird (1993), lang and agresti (1994), lang (1996b), glonek and mccullagh (1995), and\nglonek (1996). an extended outline of the full ml approach to marginal models was given by\nbergsma et al. (2009). for the alternative marginalized random effects model see section 14.5.\n\nexample 13.3: birth data\ntable 13.1 shows the fitted values of model (13.6) for the birth study (example 13.2). the two response\nvariables are cesarean section (c, 1: yes; 0: no) and intensive care (ic; 1: yes; 0 :no). explanatory\nvariables are gender of child (g; 1: male; 2: female), weight, age of mother, and number of previous\npregnancies (1: no previous pregnancy; 2: one pregnancy; 3: more than 2 pregnancies). it is seen that the\nage of the mother has an effect on both outcomes, and also the odds ratio depends on age. the number\nof previous pregnancies has an effect on treatment in intensive care in the case of more than one previous\npregnancies.\n\n13.3 general marginal models: association as nuisance\n\nand gees\n\nin the following we will consider an alternative methodology for the fitting of marginal mod-\nels. the approach is more general and applies as well to alternative marginal distributions. the\nbasic concept is to consider the association as a nuisance parameter and to find generalized esti-\nmation equations that use a working covariance matrix. let yi1, . . . , yiti denote the univariate\nmeasurements on the ith unit. the focus is on modeling how the marginal measurement yit\n\n "}, {"Page_number": 384, "text": "372\n\nchapter 13. multivariate response models\n\ntable 13.1: estimated regression coefficients for birth data with response ceaserian sec-\ntion (1), intensive care (2), and odds ratio between the two responses (3).\n\nestimate\n\nstandard error\n\nz-value\n\n(intercept):1\n(intercept):2\n(intercept):3\nweight:1\nweight:2\nweight:3\nagemother:1\nagemother:2\nagemother:3\nas.factor(gender)2:1\nas.factor(gender)2:2\nas.factor(gender)2:3\nas.factor(previous)1:1\nas.factor(previous)1:2\nas.factor(previous)1:3\nas.factor(previous)2:1\nas.factor(previous)2:2\nas.factor(previous)2:3\n\n3.651\n\u22121.058\n6.101\n\u22120.002\n\u22120.0007\n\u22120.0005\n0.0118\n0.079\n\u22120.171\n\u22120.165\n\u22120.260\n0.286\n\u22120.611\n\u22120.592\n1.398\n0.513\n\u22122.226\n4.127\n\n1.036\n3.521\n0.805 \u22121.314\n2.142\n2.848\n0.0002 \u22128.863\n0.0002 \u22124.458\n0.0005 \u22120.906\n0.0289\n0.407\n0.0231\n3.442\n0.076 \u22122.258\n0.247 \u22120.665\n0.190 \u22121.372\n0.599\n0.479\n0.376 \u22121.621\n0.255 \u22122.316\n0.905\n1.543\n0.493\n1.039\n0.780 \u22122.852\n1.918\n2.150\n\ndepends on a covariate vector xit. for ease of presentation let the variables be collected in the\nresponse vector yt\n\ni = (yi1, . . . , yiti) and the covariate vector xt\n\ni = (xt\n\ni1, . . . , xt\niti\n\nthe marginal model assumes that the marginal means are specified correctly, whereas the\nvariance structure is not necessarily the variance that generates the data. in detail, a marginal\nmodel is structured in the following way:\n\n).\n\n(1) the marginal means are given by\n\n\u03bcit = e(yit|xit) = h(xt\n\nit\u03b2),\n\nwhere h is a fixed response function.\n\n(2) the marginal variance is specified by\n\nit = var(yit|xit) = \u03c6v(\u03bcit),\n\u03c32\n\nwhere v is a known variance function and \u03c6 a dispersion parameter.\n\n(3) in addition, a working covariance structure is specified by\n\n\u02dccov(yis, yit) = c(\u03bcis, \u03bcit; \u03b1), s (cid:8)= t,\n\nwhere c is a known function depending on an unknown covariate vector \u03b1.\n\n(13.7)\n\n(13.8)\n\n(13.9)\n\nin matrix notation the model may be given in the form \u03bci = h(x i\u03b2), where the response\nfunction h transforms the single components of x i\u03b2 and the covariates are collected in the\nmatrix x t\n\ni = (xi1 . . . xiti).\n\nthe essential point in marginal models is that the dependence of the response yi on the\ncovariates xi can be investigated without assuming that the working covariance is correctly\nspecified. while it is assumed that the relationship between the measurements yi and the co-\nvariates is specified correctly by the marginal means given in (13.7), in general, the true variance\n\n "}, {"Page_number": 385, "text": "13.3. general marginal models: association as nuisance and gees\n\n373\n\nmarginal models\n\ndata:\n\ni = (yi1, . . . , yiti),\nyt\ni = (xt\niti),\ni1, . . . , xt\nxt\n|x1, . . . , xn\ny1, . . . , yn\n\ni = 1, . . . , n\nindependent\n\n(1) marginal mean\n\n(2) working covariance\n\n\u03bci = h(x i\u03b2)\n\nw i = w i(\u03b2, \u03b1, \u03c6),\n\nwith diagonal elements \u03c32\nand off-diagonal entries c(\u03bcis, \u03bcit; \u03b1), s (cid:8)= t\n\nit = \u03c6v(\u03bcit)\n\nis not equal to the working variance. therefore, it is distinguished between the underlying (but\nunknown) covariance of yi,\n\n\u03c3 i = cov(yi) = (cov(yis, yit))st,\n\nand the working covariance of yi,\n\nw i = w i(\u03b2, \u03b1, \u03c6),\n\nwhich has \u03c6v(\u03bci1), . . . , \u03c6v(\u03bciti) in the diagonal and c(\u03bcis, \u03bcit; \u03b1), s (cid:8)= t, as off-diagonal\nentries. since the true covariance is unknown, the matrix w i is used to establish the estimation\nequations. however, in specific cases, for example, binary responses, it can happen that it is not\neven a valid covariance matrix. therefore, in general, it should be considered as a weight matrix\nwithin the estimation equations. although its main use is that of a weight matrix, we will use\nthe traditional name \"working covariance matrix,\" since it is structured as a covariance matrix.\nin the following the dependence on \u03c6 is often suppressed and the simpler notation w i(\u03b2, \u03b1)\nis used.\n\nthere are two main approaches for specifying working covariances. liang and zeger (1986)\nand prentice (1988) use correlations, whereas lipsitz et al. (1991) and liang et al. (1992) use\nthe odds ratio, which is more appropriate for binary responses.\n\nworking correlation matrices\nthe working covariance matrix may be determined by assuming a simple correlation structure\nbetween the components of yi. since the relation between the covariance and correlation has\nthe simple form cov(yis, yit) = \u0001st\u03c3is\u03c3it, with the correlation between yis and yit given by\n\u0001st = 1 for s = t, the working covariance matrix has the form\n\nw i = w i(\u03b2, \u03b1) = c1/2\n\ni\n\n(\u03b2)ri(\u03b1)c1/2\n\ni\n\n(\u03b2),\n\n "}, {"Page_number": 386, "text": "374\n\nchapter 13. multivariate response models\n\ni1, . . . , \u03c32\niti\n\nwhere c i(\u03b2) = diag(\u03c32\n) is the diagonal matrix of variances, specified by (13.8),\nand ri(\u03b1) is the assumed correlation structure. the simplest choice is the working indepen-\ndence model, where ri(\u03b1) = i, with i denoting the identity matrix. in that case the parameter\n\u03b1 may be dropped. in the equicorrelation model a more flexible covariance structure is obtained\nby using corr(yis, yit) = \u03b1, yielding a correlation matrix ri(\u03b1) that has ones in the diagonal\nand \u03b1 in the off-diagonals. correlation structures of that type occur if measurements share\na common intercept (see section 14.1.1). if the observations yi1, yi2, . . . represent repeated\nmeasurements over time, it is often appropriate to assume that the correlations will decrease\nwith the distance between measurements. a correlation structure of this type is the exponential\ncorrelation model ri(\u03b1) = (\u03b1\n\n|s\u2212t|)s,t, \u03b1 \u2265 0.\n\nworking correlation matrices\n\nuncorrelated\nequicorrelation\nexponential correlation ri(\u03b1) = (\u03b1\n\nri(\u03b1) = i\nri(\u03b1) = (\u03b1i(s(cid:5)=t))s,t, \u03b1 \u2265 0\n|s\u2212t|)s,t, \u03b1 \u2265 0\n\nwhen working correlation matrices are specified in the form of equicorrelation or exponen-\ntial correlation models it is implicitly assumed that the correlation can be the same over the\npossible values of the covariates. what works for normally distributed responses, where the\nmean structure is separated from the covariance structure, fails when the responses are categor-\nical. especially for binary variables yis, yit \u2208 {0, 1}, the specification of a correlation matrix\nimplies rather strong constraints. by definition, the correlation \u0001ist = corr(yis, yit) is given by\n\n\u0001ist = p (yis = 1, yit = 1) \u2212 \u03c0is\u03c0it\n{\u03c0is(1 \u2212 \u03c0is)\u03c0it(1 \u2212 \u03c0it)}1/2\n\n,\n\nwhere \u03c0is = p (yis = 1|xis). since p (yis = yit = 1) is constrained by max{0, \u03c0is +\n\u03c0it \u2212 1} \u2264 p (yis = yit = 1) \u2264 min(\u03c0is, \u03c0it), the range for admissible correlations may be\nnarrowed down considerably. it may be shown that the range of \u0001ist depends on the odds ratio\nist = {\u03c0is/(1\u2212 \u03c0is)}/{\u03c0it/(1\u2212 \u03c0it)}. one obtains the inequalities\n*\nof marginal probabilities \u03b3m\nist,\u2212\n\u03b3m\n\n\u2264 \u0001ist \u2264 min\n\n\u2212(cid:16)\n\n(cid:29)(cid:16)\n\n1/\u03b3m\nist\n\n1/\u03b3m\nist\n\n*\n\n(13.10)\n\n(cid:29)\n\n(cid:30)\n\n(cid:30)\n\n\u03b3m\nist,\n\nmax\n\n;\n\nsee, for example, mcdonald (1993). this means that the constraint is very strong if the marginal\nprobabilities differ strongly; for example, if \u03c0is = 0.1, \u03c0it = 0.5, the maximal correlation is\n1/3. if the association is weak, the constraint will not be all that important and the specification\nof correlation matrices will work fine. however, for a strong association the constraint is severe.\nin particular, crowder (1995) and chaganty and joe (2004) pointed out that the restriction\n(13.10) has the consequence that, for binary observations, working matrices that are specified\nby correlations can be far from proper covariance matrices. since the probabilities are functions\nof the covariates, \u03c0it = \u03c0(xit), the marginal odds ratios also depend on the covariates, \u03b3ist =\n\u03b3(xis, xit). when the covariates vary across a wide range, for example, when the covariates\nare normally distributed, then inequality (13.10) may shrink the range of correlations to a single\npoint 0. when the working covariance matrix is not a proper covariance matrix it serves merely\nas a weight matrix.\n\n "}, {"Page_number": 387, "text": "13.3. general marginal models: association as nuisance and gees\n\n375\n\nspecification by odds ratios\nin addition to the problems with the range for admissible correlations, simple correlation struc-\ntures like the equicorrelation model ignore that for binary variables the correlation depends on\nthe marginals. an alternative and more appropriate way of specifying the dependence between\nbinary observations is based on odds ratios. odds ratios are easy to interpret and have desirable\nproperties. in particular, odds ratios do not depend on the marginal probabilities. let the odds\nratio for the variables yis, yit, s (cid:8)= t be given by\n\n\u03b3ist = p (yis = 1, yit = 1)/p (yis = 0, yit = 1)\np(yis = 1, yit = 0)/p (yis = 0, yit = 0) .\n\nthe covariance between yis and yit is then given by cov(yis, yit) = e(yisyit) \u2212 \u03c0is\u03c0it, which\nis a function of \u03b3ist, since\n\n\u03b3ist (cid:8)= 1\n\u03b3ist = 1,\n\n(13.11)\n\ne(yisyit) = p (yit = yit = 1)\n\n\u23a7\u23a8\n\u23a9 1 \u2212 (\u03c0is + \u03c0it)(1 \u2212 \u03b3ist) \u2212 s(\u03c0is, \u03c0it, \u03b3ist)\n\n2(\u03b3st \u2212 1)\n\n),\n\n=\n\n\u03c0is\u03c0it\n\n${1 \u2212 (\u03c0is + \u03c0it)(1 \u2212 \u03b3ist)}2 \u2212 4(\u03b3ist \u2212 1)\u03b3ist\u03c0is\u03c0it\n\n%\n\n1/2 (see lip-\nwhere s(\u03c0is, \u03c0it, \u03b3ist) =\nsitz et al., 1991; liang et al., 1992). thus cov(yis, yit) is a function of \u03c0is, \u03c0it, \u03b3ist and the\nworking covariance may be specified by\n\n\u02dccov(yis, yit) = c(\u03c0is, \u03c0it, \u03b1), s (cid:8)= t,\n\nit = \u03c0it(1 \u2212 \u03c0it) one ob-\nwhere \u03b1 contains the odds ratios {\u03b3ist, s (cid:8)= t}. with the diagonals \u03c32\ntains the working covariance w i. when \u03b3ist is fixed the variation of the marginal probabilities\nover the covariate vectors implies a variation of the correlation between yis and yit. there-\nfore, a constant correlation between variables, as is assumed, for example, by eqicorrelation is\navoided.\n\nto reduce the number of parameters odds ratios are often themselves parameterized. the\nsimplest model is \u03b3ist = 1, which corresponds to uncorrelated components yis, yit. in most\ncases the model\n\n\u03b3ist = \u03b3\n\nfor all\n\ns (cid:8)= t,\n\nwhich corresponds to the assumption of equicorrelation, will be closer if not identical to the un-\nderlying true structure. alternatively, odds ratios may be a function of the covariates following\nthe log-linear model log(\u03b3ist) = \u03b1t wist, where wist is a vector of covariates.\n\nodds ratio modeling\n\n\u03b3ist = 1\nuncorrelated\nequicorrelation \u03b3ist = \u03b3\nflexible\n\n\u03b3ist = exp(\u03b1t wist)\n\nsome examples of marginal models, specified by a marginal dependence on the covariates,\nmarginal variance, and working covariance, are the following, where corr(yis, yit) denotes the\nspecified correlation between yis, yit:\n\n "}, {"Page_number": 388, "text": "376\n\nbinary responses\n\nchapter 13. multivariate response models\n\n(1) logit(\u03bcit) = xt\n\nit\u03b2, where \u03bcit = \u03c0it = p (yit = 1|xit),\n\nit = \u03c0it(1 \u2212 \u03c0it),\n\n(2) \u03c32\n(3) \u02dccov(yis, yit) = 0, s (cid:8)= t (independence working matrix) or \u03b3ist = \u03b3(= \u03b1) (equal odds\n\nratio).\n\ncount data\n\n(1) \u03bcit = exp(xt\n\nit\u03b2),\n\nit = \u03c6\u03bcit,\n\n(2) \u03c32\n(3) corr(yis, yit) = \u03b1, s (cid:8)= t (equicorrelation).\nof course normal distribution models also can be specified as marginal models. for example,\none can specify the mean by \u03bcit = xt\nit = \u03c6 = \u03c32,\n\u02dccov(yis, yit) = \u03b1st.\n\nit\u03b2) and the variances by \u03c32\n\nit\u03b2 or \u03bcit = log(xt\n\n13.3.1 generalized estimation approach\nin linear models for gaussian responses, the specification of means and covariances determines\nthe likelihood. however, in the case of discrete dependent variables, the likelihood also depends\non higher order moments. ml estimation usually becomes computationally cumbersome, even\nif additional assumptions are made. the generalized estimation approach circumvents these\nproblems by using only the mean structure,\n\n\u03bci(\u03b2) = h(x i\u03b2),\n\nwhere \u03bci = e(yi\nw i(\u03b2, \u03b1, \u03c6).\n\ni = (xi1, . . . , xiti), and the working covariance\n\nfor fixed \u03b1, \u03c6, the generalized estimation equation (gee) for parameter \u03b2 is given by\n\nx t\n\ni di(\u03b2)w\n\n\u22121\ni\n\n(\u03b2, \u03b1)(yi\n\n\u2212 \u03bci(\u03b2)) = 0,\n\n(13.12)\n\n|xi1, . . . , xiti), x t\nn(cid:7)\n\ni=1\n\nwhere di(\u03b2) = diag(\u2202h(\u03b7i1)/\u2202\u03b7, . . . , \u2202h(\u03b7iti)/\u2202\u03b7) contains the derivatives (and in w i the\ndependence on \u03c6 is suppressed). equation (13.12) may be seen as s\u03b2(\u03b2, \u03b1) = 0, where\n\nn(cid:7)\nn(cid:7)\n\ni=1\n\ns\u03b2(\u03b2, \u03b1) =\n\n=\n\n\u2202\u03bci(\u03b2)\n\n\u2202\u03b2\n\n\u22121\ni\n\nw\n\n(\u03b2, \u03b1)(yi\n\n\u2212 \u03bci(\u03b2))\n\nx t\n\ni di(\u03b2)w\n\n\u22121\ni\n\n(\u03b2, \u03b1)(yi\n\n\u2212 \u03bci(\u03b2))\n\ni=1\n\nis a quasi-score function. for a correctly specified covariance w i = \u03c3 i and the linear gaussian\nmodel, s\u03b2 represents the usual score function that is the derivative \u2202l/\u2202\u03b2 of the log-likelihood\ni = (yi1, yi2), and the correctly specified\nl. also, in the case of only two binary variables, yt\nw i, solving (13.12) is equivalent to maximizing the likelihood. the reason is that for two\nbinary variables the distribution of yi is determined by \u03c0i1, \u03c0i2 and \u03b3i12, whereas pairwise\n\n "}, {"Page_number": 389, "text": "13.3. general marginal models: association as nuisance and gees\n\n377\n\ncorrelations do not determine the distribution completely for more than two components. the\nsolution of (13.12) for fixed \u03b1 and \u03c6 may be obtained iteratively by\n\n\u02c6\u03b2\n\n(k+1) = \u02c6\u03b2\ni di(\u02c6\u03b2)w i(\u02c6\u03b2)\u22121di(\u02c6\u03b2)x i represents the quasi-information ma-\nwhere \u02dcf (\u02c6\u03b2, \u03b1) =\ntrix. the estimation of \u03b1 depends on the assumed structure. in the following we give some\nexamples.\n\n(k) + \u02dcf (\u02c6\u03b2\n\ni=1 x t\n\n, \u03b1),\n\n, \u03b1)\n\n(k)\n\n(k)\n\nn\n\n\u22121s\u03b2(\u02c6\u03b2\n\n(cid:14)\n\nworking independence model\nif the working correlations are specified by ri(\u03b1) = i, no parameter \u03b1 has to be estimated.\nthe working covariance has the form\n\nw i(\u03b2) = diag(\u03c32\n\ni1, . . . \u03c32\n\niti)\n\nit = var(yit). then, solving equation (13.12) is equivalent to a usual ml\nwith the variances \u03c32\nestimation under the assumption that all observations yi1, . . . , yntn are independent and the\nvariances are correctly specified. therefore, the usual software for ml estimation may be used.\nif an (over-)dispersion parameter is present, it may be estimated by the method of moments.\n\nstructured correlation model\nif ri(\u03b1) depends on a parameter \u03b1, several approaches have been proposed for the estimation.\nliang and zeger (1986) use a method of moments based on pearson residuals:\n\nthe estimation of \u03b1 depends on the choice of ri(\u03b1). for equicorrelation,\n\n\u02c6rit = yit \u2212 \u02c6\u03bcit\n(v(\u02c6\u03bcit)) 1\nti(cid:7)\nn(cid:7)\nthe dispersion parameter is estimated consistently by\n\n2\n\n\u02c6\u03c6 =\n\n1\n\nn \u2212 p\n\n\u02c6r2\nit,\n\ni=1\n\nt=1\n\n.\n\nn(cid:7)\n\nti.\n\nn =\n\nn(cid:7)\n\ni=1\n\n(cid:7)\n\ni=1\n\nk>j\n\n\u02c6rik\u02c6rij.\n\n1\n\n2 ti(ti \u2212 1) \u2212 p}\n\n1\n\n\u02c6\u03b1 =\n\nn\ni=1\n\n\u02c6\u03c6{(cid:14)\nn(cid:7)\n\n\u02c6r =\n\n1\nn \u02c6\u03c6\n\nv\n\ni=1\n\n\u2212 1\n2\ni\n\n\u2212 \u02c6\u03bci)(yi\n\n\u2212 \u02c6\u03bci)t v\n\n(yi\n\n\u2212 1\n2\ni\n\n,\n\nif the cluster sizes are all equal to m and small compared to n, an unspecified r = r(\u03b1) can\nbe estimated by\n\nis. cycling between fisher scoring steps for \u03b2 and\n\nmore generally, one may define for each cluster the vector wt\n\nwhere v i is a diagonal matrix containing \u03c32\nestimation of \u03b1 and \u03c6 leads to a consistent estimation of \u03b2.\ni = (wi12, wi13, . . . , witi\u22121ti)\nof products wijk = (yij \u2212 \u03bcij)(yik \u2212 \u03bcik) and the vector ni = e(wi) of corresponding ex-\npectations. then \u03b1 is estimated by the additional estimation equation\n\u22121(wi \u2212 ni) = 0.\n\ns\u03b1(\u03b2, \u03b1) =\n\nn(cid:7)\n\ncov(wi)\n\n(13.13)\n\n\u2202ni\n\u2202\u03b1\n\ni=1\n\nsolving (13.12) and (13.13) yields estimates of \u02c6\u03b2, \u02c6\u03b1. this procedure has been called gee1 by\nliang et al. (1992) and liang and zeger (1993).\n\n "}, {"Page_number": 390, "text": "378\n\nchapter 13. multivariate response models\n\nasymptotic properties and extensions\nthe strength of the gee1 approach is that \u03b2 may be estimated consistently even if the working\ncovariance is not equal to the driving covariance matrix. one only has to assume that \u02c6\u03b1 is\nconsistent for some \u03b1o (see liang and zeger, 1986). asymptotically, n \u2192 \u221e, one obtains\n\nwhere the sandwich matrix v\n\n\u02c6\u03b2 \u223c n(\u03b2, v\n\n\u22121\nw v \u03c3 v\n\n\u22121\nw ),\n\n\u22121\nw v \u03c3 v\n\n\u22121\nw has the components\n\n\u2202\u03bci(\u03b2)\n\n\u2202\u03b2\n\nw i(\u03b2, \u03b1)\n\n\u22121 \u2202\u03bci(\u03b2)\n\nx t\n\ni di(\u03b2)w i(\u03b2, \u03b1)\n\n\u2202\u03b2t\n\u22121di(\u03b2)x i,\n\ni=1\n\nn(cid:7)\nn(cid:7)\nn(cid:7)\nn(cid:7)\n\ni=1\n\ni=1\n\ni=1\n\nv w =\n\n=\n\nv \u03c3 =\n\n=\n\n\u2202\u03bci(\u03b2)\n\n\u2202\u03b2\n\nw i(\u03b2, \u03b1)\n\n\u22121\u03c3 i(\u03b2, \u03b1)w i(\u03b2, \u03b1)\n\n\u22121 \u2202\u03bci(\u03b2)\n\nx t\n\ni di(\u03b2)w i(\u03b2, \u03b1)\n\n\u22121\u03c3 i(\u03b2, \u03b1)w i(\u03b2, \u03b1)\n\n\u2202\u03b2t\n\u22121di(\u03b2)x i.\n\n(cid:14)\n\nsome motivation for the asymptotic behavior is obtained by considering a taylor approxima-\n\u2212 \u03bci) and \u02c6\u03b2 solves\ntion. omitting arguments one has s\u03b2(\u03b2, \u03b1) =\ns\u03b2(\u02c6\u03b2, \u03b1) = 0. by taylor approximation one obtains\n\ni=1 x t\n\ni diw\n\n\u22121\ni\n\n(yi\n\nn\n\nwhich yields\n\n0 = s\u03b2(\u02c6\u03b2, \u03b1) \u2248 s\u03b2(\u03b2, \u03b1) + \u2202s\u03b2\n\n\u2202\u03b2t (\u02c6\u03b2 \u2212 \u03b2),\n\n\u02c6\u03b2 \u2212 \u03b2 \u2248 (\u2212 \u2202s\u03b2\n\u2202\u03b2t )\n\n\u22121s\u03b2(\u03b2, \u03b1).\n\nif \u2202s\u03b2/\u2202\u03b2t is replaced by its expectation, one obtains the sandwich\n\u22121 cov(s\u03b2(\u03b2, \u03b1)) e(\u2212 \u2202s\u03b2\n\u2202\u03b2t )\n\n\u22121,\n\ncov(\u02c6\u03b2) \u2248 e(\u2212 \u2202s\u03b2\n\u2202\u03b2t )\n\u22121\ni \u03c3 iw\n\ni diw\n\n\u22121\ni dix i.\n\nwhere cov(s\u03b2(\u03b2, \u03b1)) = x t\n\none drawback of the gee1 approach is that in estimating \u03b2 and \u03b1 it acts as if they were\nindependent of each other. therefore, little information from \u03b2 is used when estimating \u03b1,\nwhich may lead to significant loss of \u03b1 information. as a remedy, zhao and prentice (1990)\nand liang et al. (1992) discuss estimating the total parameter vector \u03b4t = (\u03b2t , \u03b1t ) jointly by\nsolving\n\ns(\u03b1, \u03b2) =\n\n\u2202(\u03bci, ni)\n\n\u2202\u03b4\n\ncov(yi, wi)\n\n\u22121\n\n= 0.\n\n(13.14)\n\n(cid:26)\n\n(cid:25)\n\n\u2212 \u03bci\nyi\nwi \u2212 ni\n\nn(cid:7)\n\ni=1\n\nthis expanded procedure, which estimates \u03b2 and \u03b1 simultaneously, has been termed gee2.\nthe parameter \u03b2 is estimated consistently with asymptotic normal distribution with mean 0\nand a covariance that can be consistently estimated by\n\ncov(\u02c6\u03b2) = v\n\n\u22121\nw v \u03c3 v\n\n\u22121\nw\n\n "}, {"Page_number": 391, "text": "13.3. general marginal models: association as nuisance and gees\n\n379\n\napproximation n \u2192 \u221e\n\nasymptotics gee1\n\n\u02c6\u03b2 \u223c n(\u03b2, v\n\n\u22121\nw v \u03c3 v\n\n\u22121\nw )\n\nsandwich\n\nv w =\n\nv \u03c3 =\n\nn(cid:7)\nn(cid:7)\n\ni=1\n\ni=1\n\nx t\n\ni di(\u03b2)w i(\u03b2, \u03b1)\n\n\u22121di(\u03b2)x i,\n\nx t\n\ni di(\u03b2)w i(\u03b2, \u03b1)\n\n\u22121\u03c3 i(\u03b2, \u03b1)w i(\u03b2, \u03b1)\n\n\u22121di(\u03b2)x i\n\nwith\n\nv w =\n\nv \u03c3 =\n\nn(cid:7)\nn(cid:7)\n\ni=1\n\ni=1\n\n\u2202\u03bci(\u03b2)\n\n\u2202\u03b2\n\n\u2202\u03bci(\u03b2)\n\n\u2202\u03b2\n\ncov(yi, wi)\n\ncov(yi, wi)\n\n\u22121 \u2202\u03bci(\u03b2)\n(cid:25)\n\u2202\u03b2t ,\n\u2212 \u02c6\u03bci\nyi\nwi \u2212 \u02c6\u03b7i\n\n\u22121\n\n(cid:26)(cid:25)\n\n(cid:26)\n\nt\n\n\u2212 \u02c6\u03bci\nyi\nwi \u2212 \u02c6\u03b7i\n\ncov(yi, wi)\n\n\u22121 \u2202\u03bci(\u03b2)\n\u2202\u03b2t ,\n\nevaluated at \u02c6\u03b4; see liang et al. (1992). one drawback of the gee2 approach is that the consis-\ntency derived from (13.14) depends on the correct specifications of both \u03bci and cov(yi). liang\net al. (1992) discuss extensions where cov(yi, wi) is replaced by a working covariance that\ndepends on higher order parameters obtaining consistency of \u03b4 if the higher order parameters\nare estimated consistently.\n\nrigorous investigations of the asymptotic behavior of gge estimates were given by xie\n\nand yang (2003) and wang (2011).\n\nexample 13.4: knee injuries\nin the knee injuries study (example 1.4) pain was recorded for each subject after 3, 7, and 10 days\nof treatment. the effect of treatment and the covariates gender and age (centered around 30) may be\ninvestigated by use of a marginal model. to keep the model simple, the pain level was dichotomized by\ngrouping the two lowest pain levels and the three higher pain levels. with \u03c0it = p (yit = 1|xi) the\nmarginal model has the form\n\nlogit(\u03c0it) = \u03b20 + xt,i\u03b2r + xg,i\u03b2g + agei\u03b2a + age2\n\ni \u03b2a2 ,\n\nwhere xt,i = 1 for treatment, xt,i = 0 for placebo and xg,i = 1 for females, xg,i = 0 for males. in\ntable 13.2 naive standard errors have been obtained by assuming the working correlation to be correct,\nwhereas the robust standard errors are based on the sandwich matrix. it is seen that there is quite some\ndifference between the naive and robust standard errors if independence is used as working covariance.\nwhen the working covariance gets more complex, possibly coming closer to the true underlying covari-\nance structure, the difference between naive and robust estimates become rather small. in table 13.3\nestimates and standard errors are given by fitting a logistic model and simply ignoring that the responses\nare clustered. the standard errors in this case are rather small and should be severely biased. for inde-\npendence and an exchangeable structure one obtains the effect of therapy by \u22120.673, which corresponds\n\n "}, {"Page_number": 392, "text": "380\n\nchapter 13. multivariate response models\n\ntable 13.2: estimated regression coefficients, and naive and robust standard errors for\nthree different choices of r(\u03b1). naive standard errors are obtained by assuming the work-\ning correlation matrix to be correct.\n\ncorrelation structure: independent\n\nestimate\n\nnaive s.e.\n\nrobust s.e.\n\nintercept\ntherapy (treatment)\ngender (female)\nage\nage2\n\n1.172\n\u22120.673\n0.265\n0.013\n\u22120.006\n\n0.284\n0.223\n0.241\n0.012\n0.001\n\n0.448\n0.334\n0.366\n0.017\n0.017\n\ncorrelation structure: exchangeable\n\nestimate\n\nnaive s.e.\n\nrobust s.e.\n\nintercept\ntherapy (treatment)\ngender (female)\nage\nage2\n\n1.172\n\u22120.673\n0.265\n0.013\n\u22120.006\n\n0.423\n0.333\n0.360\n0.012\n0.002\n\n0.448\n0.334\n0.366\n0.017\n0.002\n\ncorrelation structure: exponential correlation\n\nestimate\n\nnaive s.e.\n\nrobust s.e.\n\nintercept\ntherapy (treatment)\ngender (female)\nage\nage2\n\n1.122\n\u22120.748\n0.192\n0.012\n\u22120.006\n\n0.427\n0.333\n0.361\n0.018\n0.002\n\n0.443\n0.328\n0.362\n0.017\n0.002\n\ntable 13.3: estimated regression coefficients and standard errors for a glm with logit\nlink.\n\nestimate\n\nstd. error\n\n(intercept)\ntherapy (treatment)\ngender (female)\nage\nage2\n\n1.172\n\u22120.673\n0.265\n0.013\n\u22120.006\n\n0.282\n0.221\n0.239\n0.012\n0.001\n\nto the odds ratio 0.510, signaling a drastic reduction of pain when an active ingredient is used rather than\na placebo. it is seen from table 13.2 that the estimates and the robust standard errors are the same for\nthe independence model and the exchangeable correlation structure. the effect holds more generally. if\nall the covariates are cluster-specific, it can be shown that the independence model and the exchangeable\ncorrelation model yield the same estimates (see exercise 13.3).\n\nthe marginal model considered in example 13.4 has the simple structure \u03c0it = h(xt\n\ni \u03b2),\nwhere the covariate xi does not vary over repeated measurements and \u03b2 is the same for all\nresponses. but in general marginal models are more flexible.\nin the general form one has\n\u03c0it = h(xt\nit\u03b2), with covariates that can vary over measurements. that can also be used to\nmodel that the effects are specific for the response. that is especially needed if the responses\ndiffer in substance, as in the following example, where the first response is cesarean section\nand the second is treatment in intensive care. the used model with response-specific effects,\n\n\u03c0i1 = h(xt\n\ni \u03b21),\n\n\u03c0i2 = h(xt\n\ni \u03b22),\n\n "}, {"Page_number": 393, "text": "13.3. general marginal models: association as nuisance and gees\n\n381\n\nis of the general form \u03c0it = h(xt\n(0t , xt\n\ni ), and \u03b2t = (\u03b2t\n\n1 , \u03b2t\n\n2 ), where 0 is a vector of zeros.\n\nit\u03b2). one simply has to define xt\n\ni1 = (xt\n\ni , 0t ), xt\n\ni2 =\n\nexample 13.5: birth data\ntable 13.4 shows the fitted values of a marginal logit model (independent correlation structure) with\nresponse-specific parameters \u03c0it = h(xt\ni \u03b2t) for the birth study (examples 13.2 and 13.3). the two\nresponse variables are cesarean section (c; 1: yes; 0: no) and intensive care (ic; 1: yes; 0: no). the\nexplanatory variables are the same as in example 13.3 (gender of child, 1:male; 2:female), weight, age\nof mother, number of previous pregnancies, (1: no previous pregnancy; 2: one pregnancy; 3: more than\n2 pregnancies). it is seen that the effects are about the same as for the model that includes the effects on\nodds ratios (table 13.1). also, the similarity of naive values and values that are based on the sandwich\nmatrix suggests that correlation between responses given covariates is weak.\n\ntable 13.4: estimated regression coefficients for birth data with responses ceaserian sec-\ntion and intensive care; marginal model with independent working correlation, estimated\nscale parameter: 1.216.\n\nestimate\n\nnaive z\n\nrobust z\n\ninterceptint\ninterceptces\nweightint\nweightces\nagemotherint\nagemotherces\nsexint\nsexces\npreviousint1\npreviousces1\npreviousint2\npreviousces2\n\n4.161\n3.498\n\u22120.992 \u22121.109\n\u22120.002 \u22128.163\n\u22120.001 \u22124.095\n0.007\n0.215\n0.079\n3.096\n\u22120.208 \u22120.751\n\u22120.309 \u22121.462\n\u22120.457 \u22121.111\n\u22120.595 \u22122.097\n1.169\n0.636\n\u22122.136 \u22122.576\n\n3.789\n\u22121.097\n\u22128.334\n\u22124.019\n0.233\n3.319\n\u22120.842\n\u22121.638\n\u22121.268\n\u22122.249\n1.080\n\u22122.684\n\ntypes of covariates and loss of efficiency\nin applications one often has two types of variables, cluster-specific or cluster-level covari-\nates that vary across clusters but are constant within one cluster, and within-cluster covariates\nthat vary across the measurements taken within one cluster. in treatment studies dosage may\noften be a within-cluster covariate when it varies across repeated measurements. a simple\nexample of a cluster-specific covariate in treatment studies is gender. if yi1, . . . , yiti repre-\nsent repeated measurements, within-cluster covariates are time-varying covariates in contrast\nto cluster-specific covariates, which are time-invariant. in experimental designs within-cluster\ncovariates may be the same for all of the clusters, for example, if a treatment varies across time\nbut in the same way for all individuals. to distinguish it from the usual within-cluster covariate,\nit will be called a fixed (by design) within-cluster covariate.\n\nresponses and covariates for these differing types of covariates are given in the form\n\nresponses\ncluster-specific (for example, gender)\n\nyi1, . . . , yiti\nxi, . . . , xi\nxi1, . . . , xiti within-cluster (time-varying subject-specific covariate)\nxi1, . . . , xiti fixed within-cluster (fixed treatment plan).\n\n "}, {"Page_number": 394, "text": "382\n\nchapter 13. multivariate response models\n\nif the working covariance is not equal to the underlying true covariance, usually some efficiency\nis lost. the loss of efficiency depends on the covariance and the discrepancy between the\ncovariance and the working covariance as well as on the design. fitzmaurice (1995) showed that\nthe assumption of independence can lead to a considerable loss of efficiency when the responses\nare strongly correlated and the design includes a within-cluster covariate that is not invariant\nacross clusters. in particular the coefficient associated with that covariate may be estimated\nrather inefficiently. mancl and leroux (1996) showed that asymptotic efficiency may be low\neven for little within-cluster variation (see also wang and carey, 2003). on the other hand,\nit has been demonstrated for cluster-level covariates and fixed within-cluster covariates that\nasymptotic relative efficiency is near unity when the gee with independence or equicorrelation\nworking correlation is compared to the maximum likelihood estimates (see fitzmaurice, 1995).\nmancl and leroux (1996) demonstrated that asymptotic efficiency is one if all the covariates\nare mean-balanced in the sense that the cluster means are constant across clusters.\n\n13.3.2 marginal models for multinomial responses\nlet yi1, . . . , yiti denote measurements on the ith unit with yit taking values from {1, . . . , m}.\nthe marginal probability of responses, depending on the covariates xit, is denoted by \u03c0itr =\np (yit = r|xit), r = 1, . . . , m. the underlying multinomial distribution uses dummy variables\n(yit1, . . . , yitq), q = m \u2212 1, where yitr = 1 if yit = r and yitr = 0 otherwise. it is given by\n\nit = (yit1, . . . , yitq) \u223c m(ni, \u03c0t\nyt\n\nit = (\u03c0it1, . . . , \u03c0itq)).\n\nin the same way as for univariate marginal models, one specifies the marginal response prob-\nabilities together with a working covariance matrix. candidates for marginal models are the\nnominal logit model (chapter 8) and the ordinal models (chapter 9), depending on the re-\nsponse. in detail, the specification is given by:\n\n(1) the marginal vector of probabilities is assumed to be correctly specified by\n\n\u03c0it = h(x t\n\nit\u03b2),\n\n(13.15)\n\nwhere x it is composed from the covariates xit.\n\n(2) the marginal covariance of response vector yit has the usual form of multinomial co-\n\nvariances:\n\nv it = cov(yit\n\n|xit) = diag(\u03c0it) \u2212 \u03c0it\u03c0t\nit.\n\n(13.16)\n\n(3) the covariance between vectors yit and yis is specified as a working covariance matrix\n\nw ist = w ist(\u03b2, \u03b1), where \u03b1 denotes a vector of association parameters.\n\nfor the total covariance of the measurements on unit i one obtains the block matrix\n\n\u239b\n\u239c\u239d v i1 w i12\n\n...\n\n...\n\u00b7\u00b7\u00b7\n\nw iti1\n\n\u239e\n\u239f\u23a0 .\n\n\u00b7\u00b7\u00b7 w i1ti\n\nv iti\n\nw i =\n\nby collecting the components in matrices one obtains with x t\n\ni = (x t\n\ni1, . . . , x t\n\niti) the\n\nsame generalized estimation equation as in section 13.3.1,\n\nx t\n\ni di(\u03b2)w\n\n\u22121\ni\n\n(\u03b2, \u03b1)(yi\n\n\u2212 \u03bci(\u03b2)) = 0,\n\n(13.17)\n\nn(cid:7)\n\ni=1\n\n "}, {"Page_number": 395, "text": "13.3. general marginal models: association as nuisance and gees\n\n383\n\nwhere di(\u03b2) is a block-diagonal matrix with blocks di1(\u03b2), . . . , diti(\u03b2), which contain the\nderivatives. the variances of estimates are again approximated by the sandwich matrix\n\ncov(\u02c6\u03b2) = v\n\n\u22121\nw v \u03c3 v\n\n\u22121\nw .\n\nas in the binary response case, specification of the working covariance typically uses odds\nratios. but in the multinomial case one has to specify the covariance cov(yis, yit), which is a\n(q \u00d7 q)-matrix with elements cov(yisl, yitm). the corresponding odds ratio is\n\u03b3ist(l, m) = p (yisl = 1, yitm = 1)/p (yisl = 0, yitm = 1)\np (yisl = 1, yitm = 0)/p (yisl = 0, yitm = 0) .\n\nthe choice \u03b3ist(l, m) = 1, s (cid:8)= t, corresponds to uncorrelated responses, whereas \u03b3ist(l, m) =\n\u03b3lm, s (cid:8)= t, corresponds to the equicorrelation structure.\n\nfor ordered response categories, in particular when the cumulative model is used, it is\nadvantageous to use a different representation of the multinomial responses. let yit(l) =\ni(yit \u2264 l), l = 1, . . . , m \u2212 1 denote a dummy variable that codes if the response is below\nor in category l and \u03c0it(l) = p (yit \u2264 l|xit) denote the corresponding probability. then\nthe cumulative model has the simple form logit(\u03c0it(l)) = xt\nitl\u03b2, where xitl contains the\nit = (yit(1), . . . , yit(m\u22121)) is given\ncategory-specific intercept. the covariance of the vector yt\nit = (\u03c0it(1), . . . , \u03c0it(m\u22121)). the corre-\nby v it = cov(yit\nsponding odds ratios that specify the working covariance are the so-called global odds ratios:\n\n|xit) = diag(\u03c0it) \u2212 \u03c0it\u03c0t\n\nit, where \u03c0t\n\n\u03b3ist(l, m) = p (yis \u2264 l, yit \u2264 m)/p (yis > l, yit \u2264 m)\np (yis \u2264 l, yit > m)/p (yis > l, yit > m) .\n\nmarginal gee models based on global odds ratios were considered by williamson et al. (1995),\nfahrmeir and pritscher (1996), and heagerty and zeger (1996). miller et al.\n(1993) used\npairwise correlations for specifying the working covariance.\n\nmodels for ordered categorical outcomes with time-dependent parameters have been pro-\nposed by stram et al. (1988) and stram and wei (1988). they consider the marginal cumulative\nresponse model \u03c0it = h(x it\u03b2t), in which the parameters vary over t. the combined estimate\n\u02c6\u03b2 = (\u02c6\u03b21, . . . , \u02c6\u03b2t ) becomes asymptotically normal (n \u2192 \u221e), and its asymptotic covariance\nmatrix can be estimated empirically. zeger (1988) showed that \u02c6\u03b2 can be viewed as the solution\nof a gee with working correlation matrix r(\u03b1) = i, and that the covariance matrix is identical\nto the one obtained from the gee approach. in a similar approach, moulton and zeger (1989)\ncombine the estimated coefficients at each time point by using bootstrap methods or weighted\nleast-squares methods (see also davis, 1991).\n\n13.3.3 penalized gee approaches\nif covariates are collinear, the usual gee methodology fails because estimates may not exist.\nhowever, by using shrinkage methods, estimates can be stabilized.\n\npenalized estimates as considered in chapter 6 are based on the the penalized maximum\ni li(\u03b2)\u2212 \u03bb\u03c3jp(\u03b2j), where p(\u03b2j) is a penalty function, for example, p(\u03b2j) =\nlikelihood lp =\n|\u03b2j|\u03b3. the corresponding estimation equation is defined by setting the penalized score function\nsp(\u03b2) = \u2202lp/\u2202\u03b2 equal to zero.\n\n(cid:14)\n\nfor marginal models the maximum likelihood is no longer available. but considering\ns\u03b2(\u03b2, \u03b1) as a quasi-score function, a corresponding gee may be derived. when setting\ns\u03b2(\u03b2) = 0 the corresponding penalty matrix is given by \u03bbp , where p = diag (\u2202p(\u03b21)/\u2202\u03b2, . . . ,\n\n "}, {"Page_number": 396, "text": "n(cid:7)\n\ni=1\n\nn(cid:7)\n\ni=1\n\n384\n\u2202p(\u03b2p)/\u2202\u03b2). for p(\u03b2j) = |\u03b2j|\u03b3 one obtains \u2202p(\u03b2j)/\u2202\u03b2 = \u03b3sign(\u03b2j)|\u03b2j|\u03b3\u22121. the corre-\nsponding penalized gee is given by\n\nchapter 13. multivariate response models\n\nx t\n\ni di(\u03b2)w\n\n\u22121\ni\n\n(\u03b2, \u03b1)(yi\n\n\u2212 \u03bci(\u03b2)) \u2212 \u03bbp = 0,\n\n(13.18)\n\nwhere \u03bb > 0 is a fixed smoothing parameter and \u03b3 determines the type of penalization. the\nchoice \u03b3 = 2 yields the penalty term \u03b3sign(\u03b2j)|\u03b2j|\u03b3\u22121 = 2\u03b2j and the simple ridge-type\npenalized gee\n\nx t\n\ni di(\u03b2)w\n\n\u22121\ni\n\n(\u03b2, \u03b1)(yi\n\n\u2212 \u03bci(\u03b2)) \u2212 2\u03bb\u03b2 = 0.\n\nthe solution of equation (13.18) is computed in rather the same way as in a gee. it only has to\ntake care of the penalization term. for example, an iterative computation of \u02c6\u03b2 for fixed \u03b1, \u03c6 is\ngiven by\n\n\u02c6\u03b2\n\n(k+1) = \u02c6\u03b2\n\n(k) + \u02c6f p(\u02c6\u03b2\n\n(k)\n\n, \u03b1)\n\n\u22121sp,\u03b2(\u02c6\u03b2\n\n(k)\n\n, \u03b1),\n\nwhere\n\n\u02c6f p(\u02c6\u03b2, \u03b1) =\n\u2202p\nsp,\u03b2(\u02c6\u03b2, \u03b1) = x t\n\ni x t\ni di(\u02c6\u03b2) \u02c6w i(\u02c6\u03b2, \u03b1)(yi\n\n= diag(\u22022p(\u03b21)/\u2202\u03b22, . . . , \u22022p(\u03b2p)/\u2202\u03b22),\n\u2212 \u03bci(\u02c6\u03b2)) \u2212 \u03bbp .\n\n(\u02c6\u03b2, \u03b1)di(\u02c6\u03b2)t x i + \u2202p ,\n\ni di(\u02c6\u03b2)w\n\n\u22121\ni\n\n\u221a\nfor \u03bb > 0, \u03b3 \u2265 1, the resulting estimator is unique under weak conditions; for \u03bb = o(\nn),\nit is consistent and asymptotically normally distributed (fu, 2003). an approximation to the\ncovariance of \u02c6\u03b2 is given by v\n\n\u22121\nw , where\n\n\u22121\nw v \u03c3 v\n\n(cid:14)\n\n(cid:14)\n(cid:14)\n\nv w =\nv \u03c3 =\n\ni x t\ni x t\n\ni diw\ni di\u03c2\n\n\u22121\ni dix i + \u03bb\u2202p ,\n\u22121\ni dix i.\n\n13.3.4 generalized additive marginal models\nthe assumption of linear predictors in marginal modeling can be weakened by including ad-\nditive functions in the predictor. instead of the linear predictor \u03b7it = xt\nit\u03b2 one assumes the\nadditive structure\n\n\u03b7it = \u03b20 + f(1)(xit1) + \u00b7\u00b7\u00b7 + f(p)(xitp),\n\nwhere the f(j)(.) are unspecified, unknown functions and xt\nit = (xit1, . . . , xitp). for the\nunknown functions smoothing methods like localization or splines as considered in chap-\nter 10 can be applied. when the functions are expanded in basis functions, f(j)(xitj) =\n\n(cid:14)\n\nmj\ns=1 \u03b2js\u03c6js(xitj), one obtains the linear predictor\n\n\u03b7it = \u03b20 + \u03c6t\n\nit1\u03b21 + \u00b7\u00b7\u00b7 + \u03c6t\n\nitp\u03b2p = \u03c6t\n\nit\u03b2,\n\nitj = (\u03c6j1(xitj), . . . , \u03c6jmi(xitj)) are the evaluations of the basis functions for the jth\nwhere \u03c6t\nvariable at xitj. with evaluations collected in \u03c6it and \u03b2 denoting the whole parameter vector\none has a linearly structured predictor. after specifying the marginal variance and the working\ncovariance as in (13.8) and (13.9) one can use the penalized gee:\n\nn(cid:7)\n\ni=1\n\n\u03c6t\n\ni di(\u03b2)w\n\n\u22121\ni\n\n(\u03b2, \u03b1)(yi\n\n\u2212 \u03bci(\u03b2)) \u2212 1\n2\n\n\u03bbjp j\u03b2j = 0,\n\np(cid:7)\n\nj=1\n\n "}, {"Page_number": 397, "text": "(cid:14)\n\n13.4. marginal homogeneity\n\n385\n\n(cid:14)\n\ni = (\u03c6i1, . . . , \u03c6it ), and di, w i denote the matrix of derivatives and the working\nwhere \u03c6t\ncovariance, respectively. the penalty term (1/2)\nj \u03bbjp j\u03b2j can be seen as the derivative of\nthe penalty (1/2)\nj p j\u03b2j, which is typically used to penalize the log-likelihood, if it\nis available. the choice of the penalty matrix p j depends on the used basis functions and the\ntype of penalization one wants to impose (see chapter 10). the use of individual smoothing\nparameters can raise severe selection problems, and it might be useful to assume that \u03bbj = \u03bb,\nj = 1, . . . , p.\n\nj \u03bbj\u03b2t\n\nfor the extension of gee approaches to contain additive components see wild and yee\n(1996). non-parametric modeling of predictors in gees based on local regression techniques\nwere given by carroll et al. (1998); for longitudinal data with ordinal responses see kauermann\n(2000) and heagerty and zeger (1998).\n\n13.4 marginal homogeneity\nwhen the same variable is measured repeatedly at different times or under different conditions\none often wants to investigate if the distribution has changed over measurements or conditions.\nin the simplest case of two binary measurements (yi1, yi2) on subject i the two distributions\nare the same if the hypothesis of marginal homogeneity p (yi1 = 1) = p (yi2 = 1) holds. a\nsimple example is given in table 13.5, which shows the measured pain levels before the begin-\nning of treatment and after 10 days of treatment for the placebo group (compare example 13.4).\nof course one has to assume that the two responses are correlated, but correlation is of minor\ninterest. the more interesting question is if the pain level is the same for both measurements\nalthough there was placebo treatment only. an example with differing conditions for measure-\nments is given in table 13.6. the data were collected in a psychiatric ward at the university\nof regensburg; 177 members of the nursing staff were asked if patients talk about problems\nwith their partner to them. if the hypothesis of marginal homogeneity holds, the probability\nof talking about problems does not depend on the gender of the patient. comparisons of the\nresponse rates between correlated proportions are also found in the comparison of treatments\nor in problems of establishing equivalence or noninferiority between two medical diagnostic\nprocedures (berger and sidik, 2003).\n\ntable 13.5: measured pain levels before beginning of treatment (measurement 1) and\nafter 10 days (measurement 2) for placebo group.\n\nmeasurement 1\n\nlow\nhigh\n\nhigh\n\nmeasurement 2\nlow\n12\n13\n25\n\n0\n37\n37\n\n12\n50\n62\n\ntable 13.6: talking about problems with partner depending on gender of patient.\n\nfemale patient\n\nyes\nno\n\nmale patient\nyes\n39\n13\n52\n\nno\n62\n63\n125\n\n101\n76\n177\n\nthe methods considered in the following do not apply only in cases where the same sub-\nject is measured repeatedly. it is only needed that the observations are linked, forming so-called\nmatched pairs. matched pairs arise when each observation in one sample pairs with an observa-\ntion in an other sample. for example, in case control studies, frequently a population is stratified\n\n "}, {"Page_number": 398, "text": "386\n\nchapter 13. multivariate response models\n\nwith respect to some control variable and one individual per stratum is chosen randomly to be\ngiven the treatment and one is given the control. the responses on the two individuals, matched,\nfor example, according to gender and age, cannot be considered as independent. therefore, as\nin longitudinal studies, where the responses of the same individual are observed repeatedly, one\nhas to account for the dependence of the responses.\n\nwhen investigating marginal distributions for correlated measurements, marginal models\nare a natural instrument. but it can be useful to model the heterogeneity by including subject-\nspecific parameters.\n\n13.4.1 marginal homogeneity for dichotomous outcome\nthe general form of repeated measurements or matched pairs with a binary response is de-\ntermined by the bivariate variables (y1, y2) measured on each individual or stratum. one ob-\ntains a (2 \u00d7 2)-table containing the number of individuals with responses from {(0, 0), (0, 1),\n(1, 0), (1, 1)}.\nlet \u03c0ij = p (y1 = i, y2 = j) denote the probability of outcomes (i, j) with i, j \u2208 {0, 1},\nnij denote the number of pairs with outcomes (i, j), and pij = nij/n the sample proportion.\nthe counts {nij} are treated as a sample from the multinomial distribution m(n,{\u03c0ij}). as\nusual, the subscript \"+\" denotes the sum over that index. the table of probabilities and the\ntable of counts are shown in table 13.7.\n\ntable 13.7: table of probabilities and table of counts.\n\ny1\n\n1\n0\n\ny2\n\n0\n1\n\u03c010\n\u03c011\n\u03c001\n\u03c000\n\u03c0+1 \u03c0+0\n\n\u03c01+\n\u03c00+\n1\n\ny2\n\n1\n1 n11\n0 n01\n\ny1\n\n0\nn10 n1+\nn00 n0+\nn\n\nn+1 n+0\n\ncomparison of the marginal distribution focuses on the difference:\n\n\u03b4 = \u03c0+1 \u2212 \u03c01+.\n\nwhen \u03c0+1 = \u03c01+, then \u03c0+0 = \u03c00+ also, and the table shows marginal homogeneity.\n(2 \u00d7 2)-tables marginal homogeneity is equivalent to \u03c010 = \u03c001, since\n\nin\n\n\u03b4 = \u03c0+1 \u2212 \u03c01+ = \u03c011 + \u03c001 \u2212 \u03c011 \u2212 \u03c010 = \u03c001 \u2212 \u03c010.\n\nthe hypothesis of marginal homogeneity,\n\nh0 : \u03c0+1 = \u03c01+ (or \u03b4 = 0),\n\nmay be tested by using mcnemar\u2019s test (mcnemar, 1947)\n(n01 \u2212 n10)2\nn01 + n10\n\nm =\n\n,\n\nwhich in large samples is approximately \u03c72-distributed with df = 1, if h0 holds. mcnemar\u2019s\ntest is based on the ml estimate of \u03b4 and has the form m = \u02c6\u03b42/\u02c6\u03c3(\u02c6\u03b4), where\n\n\u02c6\u03b4 = n+1\nn\n\n\u2212 n1+\nn\n\n= n01 \u2212 n10\n\nn\n\n "}, {"Page_number": 399, "text": "13.4. marginal homogeneity\n\n387\n\nis the ml estimate of \u03b4 and \u02c6\u03c3(\u02c6\u03b4) = (n01 + n10)/n2 is an estimate of the standard deviation\nderived under the null hypothesis.\n\nan alternative test that suggests itself is the exact sign test. it is based on the equivalence\nof the hypotheses h0 : \u03c0+1 = \u03c01+ and h0 : \u03c001 = \u03c010. by conditioning on observations\nn01 + n10 one has to test the hypothesis h0 : \u03c001 = 1/2. conditioning on observations\nn01 + n10 may be seen as using the variables\n\n(cid:29)\n\n\u02dcyi =\n\n1\n(yi1, yi2) = (0, 1)\n0 (yi1, yi2) = (1, 0),\n\n(cid:14)\n\n(cid:14)\n\ni \u02dcyi, applies. the corresponding p-value is given by pcs = 2\n\nwhere yi1, yi2 denote the original observations. then the hypothesis h0 : \u03c001 = 1/2 corre-\nsponds to h0 : e(\u02dcyi) = 0.5 and the conditional sign test, based on the binomial distribution\nb(i; n01 +\nof\nn10, 1/2), where b(.; t, \u03c0) denotes the cumulative distribution function of the binomial distri-\nbution with parameters t and \u03c0. in both the examples given in tables 13.5 and 13.6, the exact\ntest as well as mcnemar\u2019s test reject the null hypothesis of homogeneity (m = 13 for table\n13.5 and m = 32.01 for table 13.6).\n\nmin(n01,n10)\ni=0\n\n(cid:14)\n\nboth test statistics may also be used in one-sided test problems. for mcnemar\u2019s test statistic\none uses that the signed square root of m has approximately a standard normal distribution.\ni=0 b(i, n01 + n10, 1/2) for h0 : \u03c0+1 \u2264 \u03c01+ and\nthe exact test uses the p-value pcs =\nb(i; n01 + n10, 1/2) for h0 : \u03c0+1 \u2265 \u03c01+. both test statistics depend only\npcs =\non so-called \"discordant pairs,\" which means on cases classified in different categories for the\ntwo observations. the observations n00 + n11 are considered as irrelevant for inference (see\nalso next section).\n\nn01+n10\ni=n01\n\nn01\n\n(cid:14)\n\nlikelihood ratio statistic and alternatives\nthe counts {nij} are tested as a sample from the multinomial distribution m(n,{\u03c0ij}). when\ninvestigating the underlying probabilities it is helpful to use that the multinomial distribution\ncan be expressed as a product of the binomial distributions (see also lloyd, 2008). this means\nthat the probability mass function of the multinomial, denoted by m(n11, n10, n01, n11), may\nbe expressed as\n\n(13.19)\nwhere b(x; n, \u03c0) denotes the probability of outcomes x in a binomial distribution b(n, \u03c0), and\n\nb(t; n, \u03c6)b(n01; t, \u03b7)b(n11; n \u2212 t, \u03c8),\n\nt = n01 + n10 is the number of discordant responses;\n\u03c6 = \u03c001 + \u03c010 is the probability of discordant responses, p (y1 (cid:8)= y2);\n\u03b7 = (\u03c001)/(\u03c001 + \u03c010) is the probability that a discordant response favors\n\ny2 = 1, p (y2 = 1|y1 (cid:8)= y2);\n\n\u03c8 = (\u03c011)/(\u03c011 + \u03c000) is the probability that response 1 is favored when the\n\nresponses are equal, p (y1 = y2 = 1|y1 = y2).\n\nthe first term in (13.19) corresponds to the occurrence of discordant pairs. conditional\non this, the second and third terms correspond to counts n01 within the t discordant pairs and\ncounts n11 within the n \u2212 t concordant pairs. the decomposition (13.19) is a representation\nthat turns the original parameters {\u03c0ij} into the parameters \u03c6, \u03b7, \u03c8. to test the hypothesis\nh0 : \u03b4 = 0 it is important that the parameter \u03b7 may also be written as\n\n\u03b7 = \u03b4 + \u03c6\n2\u03c6\n\n.\n\n "}, {"Page_number": 400, "text": "388\n\nchapter 13. multivariate response models\n\nwith t = n01 + n10, the likelihood in this parameterization is given by\n\nl(\u03c6, \u03b7, \u03c8) = \u03c6t(1 \u2212 \u03c6)n\u2212t\u03b7n01(1 \u2212 \u03b7)t\u2212n01\u03c8n11(1 \u2212 \u03c8)n\u2212t\u2212n11 .\n\nthe first two blocks contain the parameters \u03c6 and \u03b7, or, equivalently \u03b4 and \u03c6, while the last\nblock contains only \u03c8 = p (y1 = y2 = 1|y1 = y2), which does not contribute to differences in\nthe marginal distributions. therefore one considers the reduced likelihood by omitting the last\nterm \u03c8n11(1 \u2212 \u03c8)n\u2212t\u2212n11. by using \u03b4, \u03c6 rather than \u03b7, \u03c6 one obtains the likelihood\n\nl(\u03c6, \u03b4) = \u03c6t(1 \u2212 \u03c6)n\u2212t(\u03b4 + \u03c6)n01(\u03c6 \u2212 \u03b4)t\u2212n01(2\u03c6)\n\u2212t\n\n\u2212t(1 \u2212 \u03c6)n\u2212t(\u03b4 + \u03c6)n01(\u03c6 \u2212 \u03b4)t\u2212n01 .\n\n= 2\n\nit is easily shown that maximization yields the estimates \u02c6\u03b4 = (2n01 \u2212 t)/n = (n01 \u2212 n10)/n,\n\u02c6\u03c6 = t/n (exercise 13.6). the likelihood ratio test\n\n\u03bb = 2(l(\u02c6\u03b4, \u02c6\u03c6) \u2212 l(0, \u02c6\u03c60))\n\nuses the estimate \u02c6\u03c60, which is obtained under the restriction \u03b4 = 0. simple computation shows\nthat \u02c6\u03c6 = \u02c6\u03c60. one obtains\n\n(cid:26)\n\u03bb = 2n01 log(n01) + 2n10 log(n10) \u2212 2t log(t) + t log(2)\n= 2n01 log\n\n+ 2n10 log\n\n2n01\n\n2n10\n\n(cid:25)\n\n(cid:26)\n\n(cid:25)\n\n,\n\nn01 + n10\n\nn01 + n10\n\nwhich may be compared to a \u03c72(1)-distribution.\n\nmcnemar\u2019s test statistic and the likelihood ratio statistic are asymptotically equivalent, al-\nthough they may differ for small samples. for example, one obtains m = 32.01 and \u03bb = 34.80\nfor table 13.6. but for table 13.5, which contains fewer observations, the statistics are not\ncomparable since \u03bb does not exist. both test statistics approximate the p-value by using the\nasymptotic distribution. there has been some effort to obtain more concise p-values. ap-\nproaches differ in the way they handle the nuisance parameter \u03c6. when the tests are viewed\nunconditionally, the distributions of the test statistics depend on \u03c6. one strategy is to maximize\nthe p-value over \u03c6 (suissa and shuster, 1991). alternatively, one may use partial maximization\nover \u03c6 with a penalty on the range of maximization (berger and sidik, 2003). methods that\nreplace \u03c6 by the maximum likelihood estimate under the null hypothesis \u02c6\u03c6 and then maximize\nthe p-value have been proposed by lloyd (2008), who also gives an overview of exact methods.\n\n13.4.2 regression approach to marginal homogeneity\nlet y1, y2 denote the pair of binary measurements with yt \u2208 {0, 1} that produces the counts in\nthe (2 \u00d7 2)-table. a marginal regression model for the two measurements is given by\n\np (yt = 1) = \u03b40 + xt\u03b4,\n\n(13.20)\n\nwhere x1 = 0, x2 = 1. simple computation shows that\n\n\u03b4 = p (y2 = 1) \u2212 p (y1 = 1) = \u03c0+1 \u2212 \u03c01+\n\nis equivalent to the distance considered in section 13.4.1. the parameter \u03b40 is determined as\n\u03b40 = p (y1 = 1) = \u03c0+1, yielding \u03b40 \u2212 \u03b4 = \u03c01+.\n\nof course, for dependent variables, the regression model is not fully specified by \u03b40 and \u03b4\nsince \u03b40 and \u03b4 determine only the marginal distributions p (y1 = 1) and p (y2 = 1), and not\n\n "}, {"Page_number": 401, "text": "13.4. marginal homogeneity\n\n389\n\np (y1 = y2 = 1). the additional parameter \u03c6 = \u03c001 + \u03c010 completes the specification. the\nmodel may be represented by \u03b40, \u03b4, \u03c6 or \u03c6, \u03b7, \u03c8 from the previous section. model (13.20) is a\nlinear model for probabilities. although the parameters have a simple interpretation in terms\nof probabilities, it is often preferable to use alternative parameterizations. instead of a linear\nmodel one can consider the marginal logistic model\n\nlogit(p (yt = 1)) = \u03b20 + xt\u03b2.\n\nthen the parameter \u03b2 has the form\n\n(cid:25)\n\n\u03b2 = log\n\np (y2 = 1)/(1 \u2212 p (y2 = 1))\np (y1 = 1)/(1 \u2212 p (y1 = 1))\n\n(cid:26)\n\n,\n\nwhich represents odds ratios built from marginal distributions. although the parameterization\nis different from the linear model, the hypothesis of marginal homogeneity in both cases is\nequivalent to the vanishing of the parameter that is connected to xt (h0 : \u03b4 = 0 for the linear,\nh0 : \u03b2 = 0 for the logit model). a full parameterization is determined by specifying appro-\npriately an additional parameter. then the likelihood estimation would refer to an alternative\nset of parameters, but essentially the inference would be the same as for the linear model. the\nadvantage of the logistic model is a different one. it is better suited for conditional approaches,\nwhich are considered in the next section.\n\nconditional logit models\nalthough it is not directly observed, in general one has to assume that individuals differ in more\naspects than are modeled in a marginal model, where it is only distinguished between the first\nand second responses, that is, xt = 0 or xt = 1. the natural heterogeneity may be modeled\nexplicitly by assuming that each observation has its own subject-specific parameter. models\nof this type were considered by cox (1958) for matched pairs and in psychometrics (rasch\n(1961)).\n\nnow let (yi1, yi2) denote the pair of observations collected for subject i. a model that\n\nallows for heterogeneity among subjects is the subject-specific conditional model:\n\nlogit(p (yit = 1)) = \u03b20i + xt\u03b2,\n\nwhere \u03b20i is a subject-specific parameter and p (yit = 1) is the conditional probability given\nsubject i. one obtains\n\n\u03b2 = log\n\nwhich does not depend on i, and\n\n(cid:25)\n\np (yi2 = 1)/(1 \u2212 p (yi2 = 1))\np (yi1 = 1)/(1 \u2212 p (yi1 = 1))\n\n(cid:25)\n\n\u03b20i = log\n\np (yi1 = 1)\n1 \u2212 p (yi1 = 1)\n\n(cid:26)\n\n,\n\n(cid:26)\n\n,\n\nwhich depends on i. the parameter \u03b2, which quantifies the effect of the measurement, is a\ncommon effect and does not vary over observations. the hypothesis of marginal homogeneity\nis equivalent to \u03b2 = 0. if \u03b2 = 0 holds, one has\n\n(cid:25)\n\n(cid:26)\n\n(cid:25)\n\n\u03b20i = log\n\np (yi1 = 1)\n1 \u2212 p (yi1 = 1)\n\n= log\n\np (yi2 = 1)\n1 \u2212 p (yi2 = 1)\n\n(cid:26)\n\n.\n\n "}, {"Page_number": 402, "text": "390\n\nchapter 13. multivariate response models\n\nthis means that the marginals for the two measurements may vary across subjects but are\nidentical given the subject.\n\nparameter estimation for the subject-specific model is difficult because the number of pa-\nrameters can be huge. before considering fitting procedures, let us extend the model slightly by\nincluding the effects of additional covariates. the subject-specific model with covariates has\nthe form\n\nlogit(p (yit = 1)) = \u03b20i + xt\n\nit\u03b2,\n\n(13.21)\n\nwhere xit is a vector of predictors that varies across subjects and measurements.\n\nin the following we will consider the conditional maximum likelihood. the alternative\nestimation procedure that is based on the assumption of random effects will be considered later,\nin chapter 14.\n\nconditional maximum likelihood estimation\n\ngiven the parameters, a common assumption is that the two measurements are independent.\nthis means that the association between the measurements that is found on the population level\nis due to the heterogeneity of the observations. on the subject level, given the parameters, the\nobservations are independent. then the probability for the occurrence of measurements yi1, yi2\nis\n\n\"\n\"\n\np (yi1, yi2) =\n\n\u00d7\n\ne\u03b20i+xt\n\ni1\u03b2\n1 + e\u03b20i+xt\n\ni1\u03b2\n\ne\u03b20i+xt\n\ni2\u03b2\n1 + e\u03b20i+xt\n\ni2\u03b2\n\n(cid:26)\n(cid:26)\n\n1\u2212yi1\n\n1\u2212yi2\n\n#\nyi1(cid:25)\n#\nyi2(cid:25)\n\n1\n\n1 + e\u03b20i+xt\n\ni1\u03b2\n\n1\n\ni2\u03b2\n\n1 + e\u03b20i+xt\ni1+yi2xt\n\ni2)\u03b2,\n\n= ci\n\ne(yi1+yi2)\u03b20i+(yi1xt\n\ni1\u03b2)\u22121(1 + e\u03b20i+xt\n\ni2\u03b2)\u22121. conditional maximum likelihood estimation\nwhere ci = (1 + e\u03b20i+xt\nuses that the parameters \u03b20i may be eliminated by conditioning on the sufficient statistic ti =\nyi1 + yi2, which is on the subject level. there are two uninteresting cases, namely, ti = 0, since\nthen p (yi1 = yi2 = 0|yi1 + yi2 = 0) = 1, and ti = 2, since then p (yi1 = yi2 = 1|yi1 + yi2 =\n2) = 1. for the case yi1 + yi2 = 1 one derives\n\np (yi1 + yi2 = 1) = p (yi1 = 1, yi2 = 0) + p (yi1 = 0, yi2 = 1) = cie\u03b20i(ext\n\ni1\u03b2 + ext\n\ni2\u03b2),\n\nyielding\n\np ((yi1, yi2) = (0, 1)|yi1 + yi2 = 1) = exp(xt\np ((yi1, yi2) = (1, 0)|yi1 + yi2 = 1) = exp(xt\n\ni2\u03b2)/[exp(xt\ni1\u03b2)/[exp(xt\n\ni1\u03b2) + exp(xt\ni1\u03b2) + exp(xt\n\ni2\u03b2)],\ni2\u03b2)].\n\nthe resulting conditional probabilities specify a binary logit model with outcomes (1, 0) and\n(0, 1). by defining y\n\n\u2217\ni = 1 if (yi1, yi2) = (0, 1) and y\n\n\u2217\ni = 0 if (yi1, yi2) = (1, 0), one obtains\nexp((xi2 \u2212 xi1)t \u03b2)\n1 + exp((xi2 \u2212 xi1)t \u03b2) ,\n\np (y\n\ni = 1|yi1 + yi2 = 1) =\n\u2217\n\n "}, {"Page_number": 403, "text": "13.4. marginal homogeneity\n\n391\nwhich is a binary logit model with predictor xi2 \u2212 xi1. the conditional likelihood, given\nti = yi1 + yi2 = 1, has the form\n\n(yi1,yi2)=(0,1)\n\nlc(\u03b2) =\n\n=\n\n(cid:15)\n\ni:ti=1\n\ni2\u03b2)\n\nexp(xt\ni1\u03b2) + exp(xt\n\nexp(xt\n\nexp((xi2 \u2212 xi1)t \u03b2)\n1 + exp((xi2 \u2212 xi1)t \u03b2)\n\n(cid:26)y\n\ni2\u03b2)\n\u2217\ni\n\n(cid:25)\n\ni1\u03b2)\n\nexp(xt\ni1\u03b2) + exp(xt\n\ni2\u03b2)\n\nexp(xt\n\n(cid:26)\n\n1\u2212y\n\n\u2217\ni\n\n.\n\n(yi1,yi2)=(1,0)\n1\n\n1 + exp((xi2 \u2212 xi1)t \u03b2\n\n(cid:15)\n(cid:25)\n\n(cid:15)\n\nthus conditional maximum likelihood estimates may be obtained by fitting a logistic model\ni and a predictor given by xi2 \u2212 xi1 without an intercept (see also\n\u2217\n\nwith an artificial response y\nbreslow et al., 1978, agresti, 2002).\nit should be mentioned that it is essential that the covariates xit vary with t. for covariates\nthat characterize only the subject but not the measurement one obtains xis \u2212 xit = 0 and no\ninference on \u03b2 is possible. when estimating \u03b2 by conditioning on sufficient statistics, variables\nthat depend only on i are eliminated together with the nuisance parameters \u03b20i. the effect may\nalso be seen by considering two sets of variables, \u02dcxi, which does not depend on i, and xit,\nwhich varies across measurements. then the corresponding logit model logit(p (yit = 1)) =\nit, where \u02dc\u03b20i + \u02dcxt\n\u02dc\u03b2\n\u03b20i + \u02dcxt\ni\n\u02dc\u03b2, which do not vary\nis the subject-specific effect that includes the subject-specific effects \u02dcxt\ni\nacross measurements.\n\nit\u03b2 can be reparameterized to logit(p (yit = 1)) = \u02dc\u03b20i + xt\n\n\u02dc\u03b2 + xt\n\ni\n\nfor the simple case, where xi1 = 0 for measurement 1 and xi2 = 1 for measurement 2, the\n\nconditional likelihood, conditional on ti = 1, simplifies to\n\nlc(\u03b2) =\n\nexp(\u03b2)\n\n1 + exp(\u03b2)\n\n1\n\n1 + exp(\u03b2)\n\nyi1\n\n=\n\nexp(\u03b2)n01\n\n(1 + exp(\u03b2))n10+n01\n\n,\n\n(cid:25)\n\n(cid:15)\n\ni:ti=1\n\n(cid:25)\n\nyi2\n\n(cid:26)\n\n(cid:26)\n\n(cid:25)\n\n(cid:26)\n\n(cid:16)\n\nyielding the conditional ml estimate and standard error\n\n\u02c6\u03b2 = log\n\nn01\nn10\n\n,\n\ns( \u02c6\u03b2) =\n\n1/n01 + 1/n10.\n\nfor table 13.6 one obtains \u02c6\u03b2 = 0.67 with a standard error 0.305. the estimated marginal odds\nratio exp( \u02c6\u03b2) = 4.7 suggests that the odds for talking about problems with a partner are much\nhigher for female patients.\n\nthe advantage of the conditional approach to estimating parameters is that simple logit\nmodels may be used for estimation; the drawback is that only within-cluster effects can be\nestimated. between-cluster effects, which vary across clusters, are lost by conditioning on the\nsubject. an alternative approach that allows one to estimate within-cluster effects and models\nheterogeneity over subjects is provided by random effects models, which are considered in\nchaper 14.\n\n13.4.3 marginal homogeneity for multicategorical outcome\nlet the two measurements taken from the same individual or a matched pair be given by\n(y1, y2), where yt \u2208 {1, . . . , k}. the observations may be summarized in a square (k \u00d7 k)-\ntable containing the number of individuals with responses from {(1, 1), . . . , (k, k)}. let nij\ndenote the number of pairs with outcomes (i, j), pij = nij/n the sample proportion, and\n\u03c0ij = p (y1 = i, y2 = j) the underlying probability of outcomes (i, j). then the counts {nij},\nwhich form the contingency table, are a sample from the multinomial distribution m(n,{\u03c0ij}).\n\n "}, {"Page_number": 404, "text": "392\n\nchapter 13. multivariate response models\n\nmarginal homogeneity holds if\n\np (y1 = r) = p (y2 = r)\n\nfor all\n\nr.\n\nby use of logit models for the marginal distributions,\n\nlog(p (yt = r)/p (yt = k)) = \u03b20r + xt\u03b2r,\n\nfor all\n\nr,\n\n(13.22)\n\nwhere x1 = 0, x2 = 1, and the marginal homogeneity hypothesis corresponds to\n\nh0 : \u03b21 = \u00b7\u00b7\u00b7 = \u03b2k.\n\nfitting of the marginal homogeneity model is obtained by maximizing the multinomial likeli-\nhood subject to the constraints (13.22). the full model may be compared to the constrained\nmodel by the use of likelihood ratio tests (see lipsitz et al., 1990). for alternative test strategies\nsee bhapkar (1966).\n\nfor ordered response categories a sparser representation is obtained by using cumulative\nlogit models for the marginals. also, tests may profit from using the order information in the\nmarginals, resulting in more powerful tests (see agresti, 2009). the conditional ml approach\nfor multinomial margins was discussed by conaway (1989).\n\nsymmetry and quasi-symmetry\nmarginal homogeneity without covariates can also be tested within the log-linear models frame-\nwork by using quasi-symmetry log-linear models. with \u03c0rs = p (y1 = r, y2 = s) denoting the\nprobability for an observation in cell (r, s) of a square contingency table, the quasi-symmetry\nmodel has the form\n\nlog(\u03c0rs) = \u03bb + \u03bb1(r) + \u03bb2(s) + \u03bb12(rs),\n\nwhere \u03bb12(rs) = \u03bb12(sr). when, in addition, \u03bb1(r) = \u03bb2(r) holds, one obtains symmetry, that\nis, p (y1 = r, y2 = s) = p (y1 = s, y2 = r). the corresponding symmetry log-linear model\nmay also be given in the simpler form\n\nlog(\u03c0rs) = \u03bb + \u03bbr + \u03bbs + \u03bb12(rs).\n\nit can be shown that symmetry is equivalent to quasi-symmetry and marginal homogeneity\nholding simultaneously (see agresti, 2002). therefore, given that quasi-symmetry holds, a test\nof symmetry may be composed by comparing the fit of the quasi-symmetry model with the fit\nof the symmetry model. the difference of deviances,\n\nd(symmetry|quasi-symmetry) = d(symmetry) \u2212 d(quasi-symmetry),\n\nfollows asymptotically a \u03c72-distribution with k \u2212 1 degrees of freedom.\n\nquasi-symmetry models have been further investigated by conaway (1989), agresti (1997),\ntjur (1982), and goodman (1968). ordinal quasi-symmetry models were discussed by agresti\n(1993b). for a concise overview, see agresti (2002).\n\n13.5 further reading\nsurveys on multivariate models. molenberghs and verbeke (2005) discuss various models\nfor discrete longitudinal data. bergsma et al. (2009) gave an account of the full maximum\nlikelihood approach to marginal models. matched samples and correlated binary data are also\n\n "}, {"Page_number": 405, "text": "13.6. exercises\n\n393\n\nconsidered in fleiss et al. (2003). as always, the books of agresti (2002, 2009) give an\nexcellent overview.\n\nfurther marginal models for ordered responses. dale (1986) proposed a marginal regres-\nsion model for bivariate ordered responses that is based on the plackett distribution. the model\nwas extended to the multi-dimensional case by molenberghs and lesaffre (1994).\n\nsemiparametric marginal models. semiparametric approaches to the modeling of longi-\ntudinal data based on kernel methods was considered by lin and carroll (2006); welsh et al.\n(2002); and zhu, fung, and he (2008).\n\nvariable selection for marginal models. cantoni et al. (2005) proposed a variable selection\n\nprocedure that is based on a generalized version of mallow\u2019s cp.\n\nr packages. simple marginal binary regression with two responses and a specified odds-\nratio can be fitted by using the r-function vglm from the versatile package vgam. marginal\nmodels based on gees are available in the library gee. alternatively, one can use the package\ngeepack, which provides functions for the fitting of binary marginal models (function geeglm)\nand ordinal responses (ordgee).\n\n13.6 exercises\n\n13.1 a simple (2 \u00d7 2)-contingency table can be parameterized in various ways. one possibility is in\nprobabilities \u03c011, \u03c010, \u03c001, \u03c000, where \u03c0ij = p (y1 = i, y2 = j). alternatively, one can use the marginals\ntogether with the odds ratio {\u03c01+, \u03c0+1, \u03b3} or the parameters \u03bb1, \u03bb2, \u03bb12 of the log-linear model. yet\nanother parameterization uses \u03c6 = \u03c001 + \u03c010, \u03b7 = \u03c001\n\n, \u03c8 = \u03c011\n\n.\n\n\u03c001+\u03c010\n\n\u03c011+\u03c000\n\n(a) discuss the use of these parametrizations with respect to modeling problems. what parameteriza-\n\ntion is to be preferred in which application?\n\n(b) show how the parameterizations transform into each other.\n\n13.2 for three binary variables in (0-1)-coding the logit model with y1, y2 as the response variables and\ny3 as the covariate can be given in the form\n\np (y1, y2|y3)\n\np (y1 = 0, y2 = 0|y3)\n\nlog(\n\n) = y1\u03bb1 + y2\u03bb2 + y1y2\u03bb12 + y1y3\u03bb13 + y2y3\u03bb23 + y1y2y3\u03bb123.\n\n(a) give the parameters \u03bb1, \u03bb2, and \u03bb13 as functions of probabilities.\n(b) give an alternative model that uses marginal parameterization and compare the interpretation of\n\nthe parameters of the two models.\n\n13.3 show that the estimates based on the generalized estimation equation (13.12) are equal for the in-\ndependence and the equicorrelation model if all the covariates are cluster-specific and the number of\nobservations is the same for all clusters.\n\n13.4 the epilepsy dataset (example 13.1) is available at http://www.biostat.harvard.edu/ fitzmaur/ala.\n\n(a) show the number of seizures for the four two-week periods in a boxplot.\n\n(b) fit marginal models with alternative correlation structures and investigate the effect of treatment.\n\n13.5 the r package fahrmeir contains the dataset ohio, which is a subset from the harvard study of\nair pollution and health (laird et al., 1984). for 537 children from ohio, examined annually from ages\n7 to 10, binary responses, with yit = 1 for the presence and yit = 0 for the absence of respiratory\ninfection, are given. fit marginal models that model the influence of mother\u2019s smoking status and of age\n\n "}, {"Page_number": 406, "text": "394\n\nchapter 13. multivariate response models\n\non children\u2019s respiratory infection. try several working covariances and investigate if an interaction effect\nof mother\u2019s smoking status and age is needed.\n\n13.6 let m(n11, n10, n01, n11) denote the probability mass function of the multinomial distribution for a\n(2 \u00d7 2)-contingency table, m (n,{\u03c0ij}).\n\n(a) show that the probability mass function of the multinomial distribution can be expressed as\n\nb(t; n, \u03c6)b(n01; t, \u03b7)b(n11; n \u2212 t, \u03c8),\n\nwhere b(x; n, \u03c0) denotes the probability of outcomes x in a binomial distribution b(n, \u03c0), t =\nn01 + n10, \u03c6 = \u03c001 + \u03c010, \u03b7 = (\u03c001)/(\u03c001 + \u03c010), \u03c8 = (\u03c011)/(\u03c011 + \u03c000).\n\u2212t(1\u2212\u03c6)n\u2212t(\u03b4+\u03c6)n01 (\u03c6\u2212\u03b4)t\u2212n01\nyields the ml estimates \u02c6\u03b4 = (n01 \u2212 n10)/n, \u03c8 = (\u03c011)/(\u03c011 + \u03c000).\n\n(b) show that maximization of the reduced likelihood l(\u03c6, \u03b4) = 2\n\n "}, {"Page_number": 407, "text": "chapter 14\n\nrandom effects models and finite\nmixtures\n\nin chapter 13 the marginal modeling approach has been used to model observations that occur\nin clusters. an alternative approach to dealing with repeated measurements is by modeling ex-\nplicitly the heterogeneity of the clustered responses. by postulating the existence of unobserved\nlatent variables, the so-called random effects, which are shared by the measurement within a\ncluster, one introduces correlation between the measurements within clusters.\n\nthe introduction of cluster-specific parameters has consequences on the interpretation of\nparameters. responses are modeled given covariates and cluster-specific terms. therefore, in-\nterpretation is subject-specific, in contrast to marginal models, which have population-averaged\ninterpretations. for illustration let us consider example 13.4, where a binary response indicat-\ning pain depending on treatment and other covariates is measured repeatedly. when each indi-\nvidual has its own parameter, which represents the individual\u2019s sensitivity to pain, modeling of\nthe response given the covariates and the individual level means that effects are measured on the\nindividual level. for non-linear models, which are the standard in categorical regressions, the\neffect strength will differ from the effect strength found in marginal modeling without subject-\nspecific parameters. the difference will be discussed in more detail in section 14.2.1 for the\nsimple case of binary response models with random intercepts.\n\nexplicit modeling of heterogeneity by random effects is typically found in repeated mea-\n\nsurements, as in the pain study. in the following we give two more examples.\n\nexample 14.1: aids study\nthe data were collected within the multicenter aids cohort study (macs), which has followed nearly\n5000 gay or bisexual men from baltimore, pittsburgh, chicago, and los angeles since 1984 (see kaslow\net al., 1987; zeger and diggle, 1994). the study includes 1809 men who were infected with hiv when the\nstudy began and another 371 men who were seronegative at entry and seroconverted during the follow-up.\nin our application we use 369 seroconverters with 2376 measurements over time. the interesting response\nvariable is the number or percent of cd4 cells, by which progression of disease may be assessed. the\ncovariates include years since seroconversion, packs of cigarettes a day, recreational drug use (yes/no),\nnumber of sexual partners, age, and a mental illness score. the main interest is in the typical time course\nof cd4 cell decay and the variability across subjects (see also zeger and diggle, 1994). figure 14.1 shows\nthe data together with an estimated overall smooth effect of time on cd4 cell decay. for more details see\nexample 14.6.\n\n395\n\n "}, {"Page_number": 408, "text": "396\n\nchapter 14. random effects models and finite mixtures\n\n4\nd\nc\n\n0\n0\n0\n3\n\n0\n0\n0\n2\n\n0\n0\n0\n1\n\n0\n0\n5\n\n0\n\n\u22122\n\n0\n\n2\n\n4\n\ntime\n\nfigure 14.1: data from multicenter aids cohort study (macs) and smoothed time\neffect.\n\nexample 14.2: recovery scores\ndavis (1991) considered post-surgery recovery data.\nin a randomized study, 60 children undergoing\nsurgery were treated with one of four dosages of an anaesthetic. upon admission to the recovery room\nand at minutes 5, 15, and 30 following admission, recovery scores were assigned on a categorical scale\nranging from 1 (least favorable) to 7 (most favorable). therefore, one has four repetitions of a variable\nhaving 7 categories. one wants to model how recovery scores depend on covariables as dosage of the\nanaesthetic (four levels), duration of surgery (in minutes), and age of the child (in months).\n\nrandom effects models are a flexible tool for modeling correlated data. however, one\nconsequence of flexibility is that additional choices are necessary. in addition to the usual mod-\neling issues concerning the choice of the link function and the specifications of the explanatory\nvariables, one has to specify which effects have to be considered random and how they are\ndistributed.\n\nalthough we are focussing on discrete responses, it is helpful to consider first the special\ncase of a linear mixed model with normally distributed responses. some of the estimation\nconcepts derived for the classical normal distribution case can be used as building blocks for\nthe estimation of generalized mixed models.\n\n14.1 linear random effects models for gaussian data\n14.1.1 random effects for clustered data\n\ni = (yi1,...,yiti) denote the vector of observations on unit i (i = 1, . . . , n) and xit, zit\n\nlet yt\ndenote the covariates associated with response yit.\n\na linear random effects model can be defined as a two-stage model. at the first stage one\n\nassumes that the normally distributed response is specified by\n\nyit = xt\n\nit\u03b2 + zt\n\nitbi + \u03b5it,\n\n(14.1)\n\n "}, {"Page_number": 409, "text": "14.1. linear random effects models for gaussian data\n\n397\n\nwhere xit, zit are design vectors built from covariates, \u03b2 is a population-specific parameter, and\nbi is a cluster-specific effect. it is assumed that the noise variables are centered, e(\u03b5it) = 0,\nhomogeneous, var(\u03b5it) = \u03c32, and uncorrelated cov(\u03b5is, \u03b5it) = 0, s (cid:8)= t. moreover, \u03b5it is nor-\nmally distributed. more precisely, in (14.1) it is assumed that yit given xit, zit, bi is normally\ndistributed, that is,\n\nyit|xit, zit, bi \u223c n(\u03bcit, \u03c32),\n\nwhere \u03bcit = xt\n\nit\u03b2 + zt\n\nitbi denotes the structural term. in matrix form one obtains\n\nyi = x i\u03b2 + z ibi + \u03b5i,\n\n(14.2)\n\ni = (yi1, . . . , yiti), x t\n\nwhere yt\n\u03b5it i) is the vector of within-cluster errors, \u03b5i \u223c n(0, \u03c32i).\n\ni = (xi1, . . . , xiti), z t\n\ni = (zi1, . . . , ziti) and \u03b5t\n\ni = (\u03b5i1, . . . ,\n\nat the second stage it is assumed that the cluster-specific effects bi vary independently\n\nacross clusters, where the common assumption is the normal distribution:\n\nbi \u223c n(0, q).\n\n(14.3)\n\nin addition it is assumed that \u03b5i and bi are uncorrelated.\n\nsince the conditional distribution of yi and the random effect bi are normally distributed,\none obtains for the distribution of yi (given x i, z i) the marginal version of the random effects\nmodel:\n\n\u223c n(0, \u03c32i +ziqz t\n\nwhere \u03b5\ntic with a specific covariance structure \u03c32i + z iqz t\n\ni (exercise 14.1).\n\nyi = x i\u03b2 + \u03b5\n\n(14.4)\ni ). the corresponding linear regression model is heteroscedas-\n\n\u2217\ni\n\n\u2217\ni ,\n\ngaussian random effects model for clustered data\nyi = x i\u03b2 + z ibi + \u03b5i, bi \u223c n(0, q), \u03b5i \u223c n(0, \u03c32i)\n\nmarginal version\n\u223c n(0, v i), v i = \u03c32i + z iqzt\n\u2217\ni\n\ni\n\n\u2217\ni , \u03b5\n\nyi = x i\u03b2 + \u03b5\n\nrandom intercept model\na simple case is the random effects analysis of covariance model, where the only random effects\nare the random intercepts. therefore, each cluster/individual has its own response level. the\nmodel is given by\n\nyit = \u03b20 + xt\n\nit\u03b3 + bi + \u03b5it = (1, xit)t \u03b2 + bi + \u03b5it,\n\nwith bi \u223c n(0, \u03c32\nhas mean \u03b20 and variance \u03c32\n\nb ) independent of \u03b5it \u223c n(0, \u03c32). then the cluster-specific intercept \u03b20 + bi\n\nb . it is easily derived that the covariance of yi( given x i) is\n\ncov(yi) = \u03c32i + \u03c32\n\nb 11t =\n\n\u239b\n\u239c\u239d\u03c32 + \u03c32\n\nb\n\n\u03c32\nb\n\n\u03c32\nb\n\n\u03c32 + \u03c32\nb\n\n\u00b7\u00b7\u00b7\n...\n\n\u239e\n\u239f\u23a0 .\n\n\u03c32\nb\n\n\u03c32 + \u03c32\nb .\n\n "}, {"Page_number": 410, "text": "398\n\nchapter 14. random effects models and finite mixtures\n\nthus the observations in the same cluster are correlated. one has\n\nvar(yit) = \u03c32 + \u03c32\n\nb , cov(yis, yit) = \u03c32\n\nb\n\ns (cid:8)= t.\n\nfor\n\nthe variances \u03c32 and \u03c32\ntively. for s (cid:8)= t one obtains the intraclass correlation coefficient \u03c1(yis, yit) = \u03c32\nwhich is also called the variance components ratio.\n\nb are called elementary- and cluster-level variance components, respec-\nb ),\nb /(\u03c32 + \u03c32\n\n14.1.2 general linear mixed model\nthe model for clustered data is a special case of the more general linear mixed model (lmm):\n\n0\n\n1\n\n(cid:25)0\n\n1\n\n0\n\n,\n\n0\n0\n\nqb 0\n0\nr\n\n1(cid:26)\n\ny = x\u03b2 + zb + \u03b5,\n\nb\n\u03b5\n\n\u223c n\n\n,\n\n(14.5)\n\nwhere the covariances of the random effect vector and the noise are given by cov(b) = qb and\ncov(\u03b5) = r. in (14.5), the vector y has length n with x and z having proper dimensions. the\ncorresponding marginal version of the model, which shows how the responses are correlated,\nhas the form\n\ny = x\u03b2 + \u03b5\n\n\u2217\n\n,\n\nwhere \u03b5\n\n\u2217 \u223c n(0, v ), with covariance matrix v = zqbzt + r.\n\nit is easily seen that the random effects model (14.2) is a special mixed effects model. by\nn ), z = diag(z1,\nn ), bt = (bt\ndefining yt = (yt\nn ), qb = diag(q, . . . , q), r = diag(\u03c32i, . . . , \u03c32i), one ob-\n. . . , z n), \u03b5t = (\u03b5t\ntains the general form (14.5) of the mixed model. the number of observations in (14.5) is\nn = t1 + \u00b7\u00b7\u00b7 + tn.\n\n1 , . . . , yt\n1 , . . . , \u03b5t\n\nn ), x t = (x t\n\n1 , . . . , x t\n\n1 , . . . , bt\n\nthe general linear model includes multilevel models that are needed when clustering occurs\non more than one level and the clusters are hierarchically nested. for example, in educational\nstudies on the first level the classes form clusters, and on the second levels the students form\nclusters (within classes). repeated measurements taken on the student level can include class-\nspecific and student-specific effects. the general model also includes crossed effects models.\nfor example, in experimental studies where a sample of subjects has to solve a set of tasks re-\npeatedly, it can be appropriate to include random effects for the subjects and the task, obtaining\nan additive term that includes two random effects. then, with a multiple index, one row of the\ngeneral model has the form yij = xij + b1i + b2j + \u03b5ij, where b1i refers to the subjects and\nb2j refers to the tasks. here the focus is on clustered data, but the general model is useful for a\nclosed representation.\n\nlinear mixed model (lmm)\n\n0\n\n1\n\n(cid:25)0\n\n1\n\n1(cid:26)\n\n0\n\nqb 0\n0\nr\n\n,\n\n0\n0\n\ny = x\u03b2 + zb + \u03b5,\n\nb\n\u03b5\n\n\u223c n\n\nmarginal version\n\u2217 \u223c n(0, v ), v = zqbz t + r\n\n, \u03b5\n\n\u2217\n\ny = x\u03b2 + \u03b5\n\n "}, {"Page_number": 411, "text": "14.1. linear random effects models for gaussian data\n\n399\n\n4\n6\n\n\u03c32\n\n56\n\n(re)m l\n\n5\n\nq\n\n74\n\nestimation of variances\n\n7\n\n\u03b2\n\n6\n\nbest prediction\n\n4567\n74\n{bi}\nblup\n\n5\n\nfigure 14.2: fitting of random effects models.\n\n14.1.3 inference for gaussian response\none approach to estimating parameters distinguishes between the structural parameters \u03b2, \u03c32, q\nand the random effects b1, . . . , bn. estimation is obtained by\n\n\u2022 estimation of fixed effects \u03b2 and covariances \u03c32, q (more general qb and r) using\n\nmaximum likelihood (ml) or restricted maximum likelihood (reml)\n\u2022 prediction of random effects {bi} using best prediction \u02c6bi = e(bi|yi)\n\nan alternative approach is to find estimates of \u03b2 and b1, . . . , bn simultaneously and estimate\nthe variances separately. both approaches are visualized in figure 14.2. in the following the\nunderlying estimation methods are discussed briefly.\n\nmaximum likelihood\nfrom the marginal model (14.4) one obtains up to constants the log-likelihood\n\nl(\u03b2, \u03c32, q) =\n\n\u22121\n2\n\nlog |v i| \u2212 1\n2\n\n\u2212 x i\u03b2)t v\n\n\u22121\ni\n\n(yi\n\n\u2212 x i\u03b2),\n\n(yi\n\nn(cid:7)\n\ni=1\n\nwhere v i = \u03c32i + z iqzt\nequations \u2202l/\u2202\u03b2 = 0, \u2202l/\u2202\u03c32 = 0, \u2202l/\u2202q = 0. the first equation yields\n\ni . the computation of ml estimates is based on solving the score\n\nn(cid:7)\n\ni=1\n\nx t\ni\n\n\u02c6v\n\n\u22121\ni\n\n(yi\n\n(x t\ni\n\n\u02c6v\n\n\u22121\ni x i)\n\n\u2212 x i \u02c6\u03b2) = 0,\n(cid:2)\n&\u22121\n\nn(cid:7)\n\nx t\ni\n\n\u02c6v\n\nand therefore\n\n(cid:2)\n\n\u02c6\u03b2 =\n\nn(cid:7)\n\nor, with \u02c6v = diag( \u02c6v 1, . . . , \u02c6v n),\n\ni=1\n\ni=1\n\n\u02c6\u03b2 = (x t \u02c6v\n\n\u22121\n\n\u22121x t \u02c6v\n\nx)\n\n\u22121\n\ny.\n\n&\n\n,\n\n\u22121\ni yi\n\n(14.6)\n\n(14.7)\n\nthus, if \u02c6v i is found, \u02c6\u03b2 has the familiar form of a weighted least-squares estimate. if v i is\nknown, by setting \u02c6v i = v i the generalized least-squares estimator \u02c6\u03b2 is unbiased. the solu-\ntions of the second and third score equations \u2202l/\u2202\u03c32 = 0, \u2202l/\u2202q = 0 usually make iterative\nprocedures necessary (see, e.g., longford, 1993, chapter 2.3). for given v the estimate \u02c6\u03b2 can\nbe justified as a best linear unbiased estimator (blue) for \u03b2. for a normally distributed y, it\nis also the uniformly minimum variance unbiased estimator (umvue).\n\n "}, {"Page_number": 412, "text": "400\n\nchapter 14. random effects models and finite mixtures\n\nbest prediction\nfor a normally distributed bi, \u03b5i the model (14.2) yields\n\n(cid:25)\n\n(cid:26)\n\n(cid:26)\n\n(cid:25)\n\n(cid:25)(cid:25)\n\n\u223c n\n\nyi\nbi\n\nx\u03b2\n0\n\n,\n\nv i\nqz t\ni\n\nziq\n\nq\n\n(cid:26)(cid:26)\n\nand the posterior has the form\n\n\u22121z iq).\nthus, for known q, v i, \u03b2, the best predictor is given by the posterior mean:\n\n\u2212 x i\u03b2), q \u2212 qzt\n\n\u223c n(qz t\ni v\n\nbi|yi\n\n\u22121\ni\n\n(yi\n\ni q\n\n\u02c6bi = e(bi|yi) = qzt\ni v\n\n\u22121\ni\n\n(yi\n\n\u2212 x i\u03b2),\n\n(14.8)\n\nwhich for normally distributed responses coincides with the posterior mode.\n\nestimation of variances\nin general, ml estimates of v can be found by maximizing the profile-likelihood. by substi-\ntuting (14.6) into the log-likelihood, the profile-likelihood for v i(v ) is given by\n\nlpr(v ) = \u22121\n2\nlog |v | \u2212 1\n2\n\n= \u22121\n2\n\nlog |v | \u2212 1\n2\n\n(y \u2212 x \u02c6\u03b2)t v\n\n\u22121(y \u2212 x \u02c6\u03b2)\n\n(yt v\n\n\u22121{i \u2212 x(x t v\n\n\u22121x)\n\n\u22121x t v\n\n\u22121}y).\n\nmaximization with respect to the parameters that specify v i(v ) yields the ml estimates.\n\na criticism of the maximum likelihood estimation in regression models is that the estimator\nof variances is biased. even in a simple linear regression with y = x\u03b2+\u03b5, \u03b5 \u223c n(0, \u03c32i) one\nobtains the ml estimator \u02c6\u03c32 = (y\u2212x \u02c6\u03b2)t (y\u2212x \u02c6\u03b2)/n, which has mean e(\u02c6\u03c32) = \u03c32(n\u2212p)/n\nand therefore has a downward bias that increases with the number of estimated parameters\np. the reason is that the ml estimator fails to take into account the uncertainty about the\nregression parameters: \u02c6\u03c32 would be unbiased if \u02c6\u03b2 could be replaced by \u03b2. the same is to be\nexpected from ml estimators in random effects models.\n\na solution to the bias problem is to estimate variances without reference to \u02c6\u03b2. a restricted\nmaximum likelihood (patterson and thomson, 1971) is based on the construction of a likelihood\nthat depends only on a complete set of error contrasts and therefore does not depend on \u03b2. an\nerror contrast is a linear combination of y that has zero expectation, that is, e(ut y) = 0. in\na simple linear regression a set of linearly independent error contrasts is i \u2212 x(x t x)\u22121x.\nfor random effects models harville (1976) derived the likelihood of error contrasts; see also\nsearle et al. (1992). the resulting criterion is the restricted log-likelihood:\n\nlr(v ) = lpr(v ) \u2212 1\n2\n\nlog |x t v\n\n\u22121x|,\n\nwhich yields restricted maximum likelihood (reml) estimates. computations of ml and\nreml estimates are based on iterative procedures.\n\nbest linear unbiased prediction (blup)\nan approach that aims at estimating of parameters and random effects simultaneously (sep-\narated from the estimation of variances) is based on the joint density of yi and bi, which is\nmaximized with respect to \u03b2 and b1, . . . , bn. one uses the closed form,\n\ny = x\u03b2 + zb + \u03b5,\n\n "}, {"Page_number": 413, "text": "14.1. linear random effects models for gaussian data\n\n401\n\nwith covariance given by\n\n(cid:25)\n\n(cid:26)\n\n(cid:25)\n\ncov\n\nb\n\u03b5\n\n=\n\n(cid:26)\n\n(cid:25)\n\n=\n\n(cid:26)\n\n\u03c32,\n\n\u02dcqb\n0\n\n0\n\u02dcr\n\n0\nqb\n0 r\n\nwhere \u02dcqb and \u02dcr are known. thus it is assumed that the structure of cov(bi) is known up to\na constant. this is the case, for example, for random intercepts where \u02dcqb = (\u03c32\nb /\u03c32)i and\n\u02dcr = i. the joint density of y and b is normal and maximization with respect to \u03b2 and b\nrequires minimizing:\n\n(y \u2212 x\u03b2 \u2212 zb)t r\n\n\u22121(y \u2212 x\u03b2 \u2212 zb) + bt q\n\n\u22121\nb b.\n\n(14.9)\n\nminimization corresponds to a generalized least-squares with a penalty term.\n\ndifferentiating the joint density with respect to \u03b2 and b yields henderson\u2019s \"mixed model\n\nequations\" for \u03b2 and b:\n\n\u22121x \u02c6\u03b2 + x t r\n\nx t r\n\u22121x \u02c6\u03b2 + (z t r\n\nz t r\n\n\u22121z\u02c6b = x t r\n\n\u22121y,\n\n\u22121z + q\n\n\u22121\nb )\u02c6b = z t r\n\n\u22121y.\n\nmaximization of the joint density of y and b is not a common maximum likelihood estimate\nsince b is not a parameter. therefore, estimates of b that result from solving these equations\nare often called predictions, while estimations of fixed effects are called estimations. it should\nbe noted that the mixed model equations and the other formulas in this section may be written\nwith qb, r, or \u02dcqb, \u02dcr. some matrix algebra (robinson, 1991) shows that the mixed model\nequations yield\n\n\u02c6\u03b2 = [x t v\n\n\u22121x]\n\n\u22121x t v\n\n\u22121y,\n\nwhere v = r + zqbzt . thus \u02c6\u03b2 is equivalent to the generalized least-squares estimator\n\u22121(y \u2212 x \u02c6\u03b2), which\n(14.7) for a known covariance matrix. for \u02c6b one obtains \u02c6b = v\nis equivalent to\n\n\u22121z t r\n\n\u02c6b = qbzt v\n\n\u22121(y \u2212 x \u02c6\u03b2).\n\n\u22121\nwith v = diag(v 1, . . . , v n) and qb = diag(q, . . . , q) one obtains \u02c6bi = qz t\n(yi \u2212 xi \u02c6\u03b2), which is equivalent to (14.8). therefore, the best linear unbiased prediction yields\ni\nan estimate \u02c6\u03b2 that is equivalent to the ml estimate for a known covariance and predictions \u02c6bi\nthat correspond to the posterior mean. by using (14.9), the estimates \u02c6\u03b2, \u02c6b can be written in\nclosed form as\n\ni v\n\n0\n\n1\n\n\u02c6\u03b2\n\u02c6b\n\n= (c t r\n\n\u22121y,\n\n\u22121c + b)\n0\n\n\u22121c t r\n1\n\n0\n0\n0 q\n\n\u22121\nb\n\n,\n\nwhere c = [x z] and\n\nb =\n\nand one obtains the fitted values\n\n\u02c6y = blup(y) = x \u02c6\u03b2 + z\u02c6b = c(ct r\n\n\u22121c + b)\n\n\u22121c t r\n\n\u22121y.\n\n(14.10)\n\nruppert et al. (2003) call (14.10) the \"ridge regression\" formulation of blup. it shows that \u02c6\u03b2\nand \u02c6b are estimated by weighted least squares with a penalty term that penalizes b.\n\n "}, {"Page_number": 414, "text": "402\n\nchapter 14. random effects models and finite mixtures\n\n\u02c6\u03b2 = (x t v\n\n\u22121x)\u22121x t v\n\n\u22121y ml estimator\n\n(if v is ml estimator)\nblup (for given v )\n\n\u02c6bi = qz t\n\ni v\n\n\u22121\ni\n\n(yi\n\n\u2212 x i\u03b2)\n\nvariance v i\n\nblup (for given v )\n\nml or reml\n\n14.2 generalized linear mixed models\ngeneralized linear mixed models extend generalized models to permit random effects as well\nas fixed effects in the linear predictor. we will first consider simple models that contain only\na random intercept and discuss the differences between random effects models and marginal\nmodels.\n\n14.2.1 binary response models with random intercepts\nlogistic-normal model\ni = (yi1, . . . , yiti) denote the observation of one cluster, where yit \u2208 {0, 1} is the binary\nlet yt\nresponse for cluster i and measurement t. the simple logistic-normal random intercept model\nwith covariates xit is given by\n\nlogit(p (yit = 1|xit, bi)) = xt\n\nit\u03b2 + bi,\n\ni = 1, . . . , n, t = 1, . . . , ti.\n\n(14.11)\n\nwhile \u03b2 is a fixed effect, bi is considered to be a cluster-specific random effect. it is assumed\nthat the {bi} are independent n(0, \u03c32\nb) variates and the yi1, . . . , yiti are conditionally inde-\npendent given bi, therefore the name logistic-normal model. in (14.11), heterogeneity across\nclusters is explicitly modeled by allowing each cluster to have its own intercept. since bi has\nmean e(bi) = 0, it is natural to incorporate a fixed intercept in the design vector xit.\n\nmodel (14.11) is a non-linear mixed model since the linear predictor \u03b7it = xt\n\nin the conditional model (14.11) the regression parameter \u03b2 measures the change in logits\nper unit change in a covariate, controlling for all other variables, including the random effect\nbi. interpretation of \u03b2 is always conditional on bi.\nit\u03b2 + bi effects\nupon the logit transformed mean \u03bcit = e(yit|xit, bi). non-linearity has severe consequences\nif one wants to interpret the effect of xit on the marginal probability p (yit = 1|xit). consider\na linear random effects model e(yit|xit, bi) = xt\nb ). then the\nmarginal mean e(yit|xit) = xt\nit\u03b2 is again specified in a linear way and \u03b2 is the parameter that\ndetermines the response. the crucial point is that for model (14.11) the marginal probability is\n\nit\u03b2 + bi, where bi \u223c n(0, \u03c32\n\n)\n\np (yit = 1|xit) =\n\np (yit = 1|xit, bi)p(bi)dbi,\n\nwhere p(bi) denotes the density of bi. with p (yit = 1|xit, bi) given by the conditional logit\nmodel\n\np (yit = 1|xit, bi) =\n\nexp(xt\n\n1 + exp(xt\n\nit\u03b2 + bi)\nit\u03b2 + bi) ,\n\nthe marginal probability p (yit = 1|xit) is not a linear logistic model. a simple approximation\nof the marginal probability may be derived from a taylor expansion (compare cramer, 1991).\n\n "}, {"Page_number": 415, "text": "14.2. generalized linear mixed models\n\n403\n\nwith f denoting the logistic distribution function f (\u03b7) = exp(\u03b7)/(1+exp(\u03b7)), the conditional\nmodel is given by\n\np (yit = 1|xit, bi) = f (xt\n\nit\u03b2 + bi).\n\na taylor approximation of second order (appendix b) at \u03b7i = xt\nf (\u03b7i) + f\nobtains with \u03c32\n\n(cid:3)(\u03b7i) = \u2202f (\u03b7i)/\u2202\u03b7, f\n\n(cid:3)(\u03b7i)bi + 1\n2 f\n\ni , where f\n\n(cid:3)(cid:3)(\u03b7i)b2\n\nb = var(bi)\np (yit = 1|xit) = ebi p (yit = 1|xit, bi) \u2248 f (\u03b7i) +\n\n(cid:3)(cid:3)\n\n(\u03b7i)var(bi)\n\n1\n2 f\n\nit\u03b2 yields f (\u03b7i + bi) \u2248\n(cid:3)(cid:3)(\u03b7i) = \u22022f (\u03b7i)/\u2202\u03b72. one\n\n= f (\u03b7i) +\n\n1\n\n2 f (\u03b7i)(1 \u2212 f (\u03b7i))(1 \u2212 2f (\u03b7i))\u03c32\n\nb .\n\nfigure 14.3 shows the (approximative) dependence of the marginal response on f (\u03b7i) for sev-\nb . it is seen that for large values of \u03b7i (corresponding to f (\u03b7i) > 0.5) the\neral values of \u03c32\nmarginal probability is shrunk toward 0.5, whereas for small values of \u03b7i (corresponding to\nf (\u03b7i) < 0.5) the marginal probability is larger than f (\u03b7i). this means that marginal proba-\nit\u03b2). consequently, if one fits\nbilities are always closer to 0.5 when compared to f (\u03b7i) = f (xt\nthe marginal model\n\np (yit = 1|xit) = f (xt\n\nit\u03b2m),\n\nthe parameter estimates \u02c6\u03b2\nfits a marginal model, the effect strength \u03b2 can be strongly underestimated.\n\nare attenuated as compared to \u03b2. if one ignores heterogeneity and\n\nm\n\nfigure 14.4 visualizes the effect in an alternative way. the panels show the response\n\u2019\ncurves for the conditional probabilities f (x\u03b2 + bi) as functions in x for various fixed inter-\ncepts bi and the marginal response averaged over the conditional responses, p (yit = 1|x) =\nf (x\u03b2 + bi)p(bi)dbi. the latter is given as a superimposed thick curve. it is seen that larger\nvalues of the variance \u03c32\nb yield flatter marginal response curves. the marginal response curves\nare also sigmoidal but not logistic functions. therefore, if heterogeneity is in the data and the\nconditional logistic-normal model holds, ignoring the heterogeneity by fitting a marginal logis-\ntic model means misspecification, that is, one is fitting a model that does not hold. as is shown\nin the following the connection between conditional and marginal models is slightly different\nin probit-normal models.\n\n1\n\np (yit = 1)\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n\n0\n\n\u03c32 = 1\n\n\u03c32 = 4\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\nf (\u03b7i)\n\nfigure 14.3: dependence of marginal probability on f (\u03b7i).\n\n "}, {"Page_number": 416, "text": "404\n\nchapter 14. random effects models and finite mixtures\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n0\n\n.\n\n1\n\n8\n\n.\n\n0\n\n6\n\n.\n\n0\n\n4\n\n.\n\n0\n\n2\n\n.\n\n0\n\n0\n\n.\n\n0\n\n\u22125\n\n0\n\nx\n\n5\n\n\u22125\n\n0\n\nx\n\n5\n\nfigure 14.4: mixture of logistic responses for \u03c32\npanel).\n\nb = 1 (left panel) and \u03c32\n\nb = 4 (right\n\nprobit-normal model and alternatives\nas shown in chapter 2, binary response models may be derived from latent regression models.\nit\u03b2 + \u03b5it + bi, where \u2212\u03b5it and \u2212bi are independent\nlet a latent response \u02dcyit be given by \u02dcyit = xt\nand have the distribution function f\u03b5, fb. by assuming that the observed response yit = 1\noccurs if \u02dcyit > 0, one obtains the conditional model\np (yit = 1|xit, bi) = p (\u2212\u03b5it \u2264 xt\n\nit\u03b2 + bi) = f\u03b5(xt\n\nit\u03b2 + bi).\n\nthe corresponding marginal model is given by\n\n)\n\np (yit = 1|xit) =\n\nf\u03b5(xt\n\nit\u03b2 + bi)p(bi)dbi\n\np (yit = 1|xit) = p (\u2212\u03b5it \u2212 bi \u2264 xt\n\nor\nwhere, for symmetrically distributed bi, fm is the distribution function of \u2212\u03b5it \u2212 bi. while the\nconditional model has the response function f\u03b5, the marginal model has the response function\nfm, which usually differs from f\u03b5. the marginal response function f\u03b5 can also be derived\nfrom the integral form of the marginal response probability.\n\nif \u03b5it and bi are normally distributed, \u03b5it \u223c n(0, 12), bi \u223c n(0, \u03c32\n\nit\u03b2) = fm(xt\n\nb ), one assumes the\n\nit\u03b2),\n\nconditional probit model\n\nwhere \u03c6 is the distribution function of the standardized normal distribution n(0, 1). then the\nmarginal model is also a probit model given by\n\nit\u03b2 + bi),\n\np (yit = 1|xit, bi) = \u03c6(xt\n#\n\n\"\n\nit\u03b2(cid:16)\n\nxt\n1 + \u03c32\nb\n\np (yit = 1|xit) = \u03c6\n\n= \u03c6(xt\n\nit\u03b2m),\n\n(cid:16)\n\n1 + \u03c32\nwhere the parameter vector of the marginal version is given by \u03b2m = \u03b2/\nb . again,\nmarginal effects are attenuated as compared to the conditional effects. for large \u03c32\nb , the marginal\neffects \u03b2m are much closer to zero than the conditional effects \u03b2. although marginal and\n\n "}, {"Page_number": 417, "text": "14.2. generalized linear mixed models\n\n405\n\nconditional effects usually differ, for probit models the model structure is the same for the\nmarginal and the conditional model. the model is closed in the sense that the distributions\nassociated with the marginal and conditional link functions are of the same family. more gen-\nerally, one also gets closed models for mixtures of normals; see caffo et al. (2007).\n\n14.2.2 generalized linear mixed models approach\na general form of the model is obtained by considering the response variables yi and covariates\nxi, zi, i = 1, . . . , n. one assumes for the mean response, conditionally on the random effect\nb, denoted by \u03bci = e(yi|xi, zi, b), that the link to the predictor is given by\n\ng(\u03bci) = xt\n\ni \u03b2 + zt\n\ni b or \u03bci = h(xt\n\ni \u03b2 + zt\n\ni b),\n\n\u22121 is the response function. while the \u03b2\u2019s are\nwhere g represents the link function and h = g\nthe fixed effects model parameters, b is a random effect, for which a distribution is assumed.\nmost often one assumes b \u223c n(0, qb) for an unknown covariance matrix qb. in addition, it\nis assumed that y1, y2, . . . , yn are independent given b. hence one assumes that the correlation\nbetween observations is due to the common latent variable b. considering the model as an\nextension of glms, it is assumed that the distribution of yi is in the exponential family. the\nstructural part of the model is often given in matrix form as\n\ng(\u03bc) = x\u03b2 + zb or \u03bc = h(x\u03b2 + zb),\n\n(14.12)\n\nwhere \u03bct = (\u03bc1, . . . , \u03bcn ), x and z have rows xi and zi, respectively, and g and h are\nunderstood componentwise.\n\ngeneralized linear mixed model (glmm)\n\nwith components\n\nand\n\ng(\u03bc) = x\u03b2 + zb\n\ng(\u03bci) = xt\n\ni \u03b2 + zt\ni b\n\ne(b) = 0, cov(b) = qb,\n\ny1, . . . , yn independent given b\n\n14.2.3 generalized linear mixed models for clustered data\nan important special case of the generalized linear mixed model occurs when data are col-\nlected in clusters. a typical example is repeated measurements, where the response variable is\nobserved repeatedly for each unit. a unit may be an individual for which a response is observed\nunder different conditions or a family with the measurements taken on the members of the fam-\nily. for clustered data the random effects usually are cluster-level terms; the single random\neffects are specific for the individuals. as in section 14.2.1, in the notation a double index is\nused to represent the cluster and the observation within the cluster. let yit denote observation\nt in cluster i, t = 1, . . . , ti, collected in yt\n\ni = (yi1, . . . , yiti).\n\n "}, {"Page_number": 418, "text": "406\n\nchapter 14. random effects models and finite mixtures\n\nwith explanatory variables xit, zit that may depend on i and t, one considers the condi-\ntional means \u03bcit = e(yit|xit, zit, bi), where bi is the cluster-specific random effect. the\nstructural assumption is\n\ng(\u03bcit) = xt\n\nit\u03b2 + zt\n\nitbi\n\nor \u03bcit = h(xt\n\nit\u03b2 + zt\n\nitbi),\n\n(14.13)\n\nor, simpler, g(\u03bcit) = \u03b7it, where \u03b7it = xt\nitbi is the linear predictor. moreover, it is\nassumed that the random effects b1, . . . , bn are independent with e(bi) = 0 and have density\np(bi, q), where q represents the covariance matrix of a single random effect, cov(bi) = q. in\nthe following we give some examples for the specification of the explanatory variables and the\nrandom effects.\n\nit\u03b2 + zt\n\nrandom intercept models\nin section 14.2.1, the random intercept model for binary observations was considered. in gen-\neral, for random intercept models, the linear predictor of observation t in cluster i is given\nby\n\n\u03b7it = bi + xt\n\nit\u03b2,\n\nwhere bi is the cluster-specific intercept. thus the effect of the covariates is determined by the\nfixed effects \u03b2, but the response strength may vary across clusters. by specifying zit = 1 one\nobtains the random intercept model for clustered data from the general form (14.13).\n\nrandom slopes\nthe random intercept model assumes that only the response level varies across individuals\n(or, more general, clusters). however, for example, the effect of a drug may also vary across\nindividuals. the potential heterogeneity of this sort may be modeled by allowing for random\nslopes of the explanatory variable. the linear predictor of the model, where all variables have\ncluster-specific slopes, has the form\n\n\u03b7it = xt\n\nit\u03b2 + xt\n\nitbi,\n\nwhere \u03b2 represents the common effect of covariates and bi represents the cluster-specific devia-\ntions from \u03b2. since one assumes e(bi) = 0, the mean predictor is given by ebi(xt\nit(\u03b2 +bi)) =\nit\u03b2, where ebi denotes expectation with respect to bi. most often, only part of the explanatory\nxt\nvariables are assumed to have cluster-specific slopes. then the variables zit represent a sub-\n(2)).\nvector of xit. let xit be decomposed into xt\nby setting zit = xit(2) and assuming\n\nit(2)) and \u03b2 into \u03b2t = (\u03b2t\n\nit = (xt\n\nit(1), xt\n\n(1), \u03b2t\n\n\u03b7it = xt\n\nit\u03b2 + zt\n\nitbi = xt\n\nit(1)\u03b2(1) + xt\n\nit(2)(\u03b2(2) + bi),\n\nthe variables xit(1) have fixed effects whereas the variables xit(2) have effects that vary across\nclusters.\n\ngeneralized linear mixed model for clustered data\n\ng(\u03bcit) = xt\n\nit\u03b2 + zt\n\nitbi\n\nwith\n\ne(bi) = 0, cov(bi) = q\n\nindependent given {bi}\n\nyi1, . . . , ynti\n\n "}, {"Page_number": 419, "text": "14.3. estimation methods for generalized mixed models\n\n407\n\nit is easily seen that the model for clustered data has the form of the general model (14.12).\nn ), one has for\n\nby collecting the cluster-specific random effects in one vector, bt = (bt\nsingle observations the form\n\n1 , . . . , bt\n\ng(\u03bcit) = xt\n\nit\u03b2 + \u02dczt\n\nitb,\n\nit = (0t , . . . , zt\n\nwhere \u02dczt\nit, . . . , 0t ). the matrix x in (14.12) contains the vectors xit as rows;\nthe rows of z are given by the vectors \u02dczit. the covariance of the total random effects vector\nb is given as a block-diagonal matrix qb = diag(q, . . . , q). by collecting the observations\nof one cluster in a vector one obtains the matrix form g(\u03bci) = x i\u03b2 + zibi, with x i and\nz i containing observations xit, zit, t = 1, . . . , ti. stacking the matrices together yields the\ngeneral form g(\u03bc) = x\u03b2 + zb, where x t = (x t\nn ) and z is a block-diagonal\nmatrix with blocks z i.\n\n1 , . . . , x t\n\nfor clustered data the assumption of conditional independence given the random effects\n\nsimplifies to\n\nf(y|b; \u03b2) =\n\nn(cid:15)\n\n|bi; \u03b2) with f(yi\n\n|bi; \u03b2) =\n\nf(yi\n\nf(yit|bi; \u03b2),\n\nti(cid:15)\n\ni=1\n\nt=1\n\nwhere yt = (yt\n\n1 , . . . , yt\n\nn ), yt\n\ni = (yi1, . . . , yiti), represents the whole set of responses.\n\n(marginal)ml\n\nposterior mean/mode\n\n4\n\n56\n\n7\n\nq\n\n\u03b2\n\n6\n\n4567\n74\n{bi}\n\n5\n\nfigure 14.5: fitting of generalized linear models with random effects.\n\n\u03b4\n\n14.3 estimation methods for generalized mixed models\nseveral approaches to fitting generalized linear models may be distinguished. maximum like-\nlihood approaches aim at estimating the structural parameters \u03b2 and q by maximizing the\nmarginal log-likelihood. predictors for the random effects bi are then derived as posterior\nmean estimators (see sections 14.3.1 and 14.3.2). alternatively, one may separate the esti-\nmation of the variance q from the estimation/prediction of \u03b2 and bi, which are collected in\n\u03b4t = (\u03b2t , bt\n\nn ) (see section 14.3.3).\n\n1 , . . . , bt\n\n14.3.1 marginal maximum likelihood estimation by integration\n\ntechniques\n\nthe fixed parameters \u03b2 and qb can be estimated by maximization of the marginal log-likelihood\n\nl(\u03b2, qb) = log(\n\nf(y|b; qb)p(b; qb)db).\n\nin the case of crossed random effects, numerical maximizing is very cumbersome because the\nintegral does not simplify. but it works relatively well when the random effects come in clusters\n(model (14.13)). then the log-likelihood takes the form\n\nl(\u03b2, q) =\n\nli(\u03b2, q),\n\n(14.14)\n\n)\n\nn(cid:7)\n\ni=1\n\n "}, {"Page_number": 420, "text": "408\n\nwhere\n\n)\n\nli(\u03b2, q) = log(\n\nf(yi\n\nchapter 14. random effects models and finite mixtures\n\n) ti(cid:15)\n\n|bi; \u03b2)p(bi; q)dbi) = log(\n\nt=1\n\nf(yit|bi; \u03b2)p(bi; q)dbi)\n\nis the contribution of observation yt\nto bi by use of p(bi; q), the density of random effects within clusters.\n\ni = (yi1, . . . , yiti) that results from integration with respect\n\none approach that works for low-dimensional bi is to approximate the integral in li(\u03b2, q)\nby numerical or monte carlo quadrature techniques. for estimating it is often useful to consider\nthe standardized random effects\n\nai = q\n\n\u22121/2bi,\n\nwhere q1/2 denotes the left cholesky factor, which is a lower triangular matrix, so that q =\n\u22121/2 denoting the inverse of q1/2, one\nq1/2qt /2, where t denotes the transpose. with q\nobtains cov(ai) = i. by using matrix algebra (e.g., magnus and neudecker, 1988, p. 30) one\nobtains the linear predictor\n\n\u03b7it = xt\n\nit\u03b2 + zt\n\nitq1/2ai = xt\n\nit\u03b2 + (at\n\ni\n\n\u2297 zt\n\nit)\u03b8,\n\nwhere \u2297 is the kronecker product and \u03b8 denotes the vectorization of q1/2, \u03b8 = vec(q1/2). for\nthe simple case of a scalar random effect one has var(bi) = \u03c32\nb and ai is given by ai = bi/\u03c3b.\nwith \u03b8 = \u03c3b the linear predictor simplifies to\n\u03b7it = xt\n\nit\u03b2 + ai\u03c3b.\n\nthe likelihood contribution li(\u03b2, q) = log li(\u03b2, q) for the standardized random effects ai is\ngiven by\n\nli(\u03b2, q) =\n\n|ai; \u03b2, q)p(ai)dai =\n\nf(yi\n\nf(yit|ai; \u03b2, q)p(ai)dai,\n\nand p(ai) denotes the (standardized) density of ai, which has zero mean and covariance matrix\ni.\n\nt=1\n\nfor a low dimension of ai, the integral may be approximated by integration techniques like\ngauss-hermite (for normally distributed ai) or monte carlo techniques. the gauss-hermite\napproximation has the form\n\n) ti(cid:15)\n\n)\n\nlgh\n\ni\n\n(\u03b2, q) =\n\nvjf(yi\n\n|dj; \u03b2, q),\n\n(cid:7)\n\nj\n\nwhere dj denotes fixed quadrature points and vj denotes fixed weights that are associated with\ndj. quadrature points and weights are given, for example, in stroud and secrest (1966) and\nabramowitz and stegun (1972). for more details see appendix e.1.\na simple monte carlo technique approximates the likelihood by\n|dij; \u03b2, q),\n\nm(cid:7)\n\n(\u03b2, q) =\n\nlm c\n\nf(yi\n\ni\n\n1\nm\n\nj=1\n\nwhere dij are m iid drawings from the mixing density p(ai).\nin both, gauss-hermite and\nmonte carlo approximations, the random effects ai are replaced by known values, quadrature\npoints, or random drawings. that makes it possible to compute estimates of \u03b2 and q within the\ngeneralized linear model framework with predictors \u03b7itj = xt\nit)\u03b8, j = 1, . . . , m,\nwhere \u03b8 = vec(q1/2). the embedding into the glm framework is given in the next section\nfor the gauss-hermite procedure.\n\nit\u03b2 + (dt\n\n\u2297 zt\n\nj\n\n "}, {"Page_number": 421, "text": "14.3. estimation methods for generalized mixed models\n\n409\n\n(\u03b2, q) is a function of \u03b1t = (\u03b2t , \u03b8t ) with the pre-\n\u2297 zt\n\ndirect maximization by using glm methodology\nthe gauss-hermite approximation lgh\ndictor having the form \u03b7it = xt\nit\u03b2 + (at\n|dj; \u03b1)\n\n\u2202f(yi\n\ni\n\ni\n\n= f(yi\n\none obtains the score approximation\n\n\u2202\u03b1\n\nit)\u03b8. by using\n|dj; \u03b1) \u2202 log f(yi\nm(cid:7)\n\n\u2202\u03b1\n\n|dj; \u03b1)\n\n,\n\nsgh\ni\n\n(\u03b1) = \u2202 log lgh\ni\n\u2202\u03b1\n\n(\u03b1)\n\n=\n\nij (\u03b1) \u2202 log f(yi\ncgh\n\n\u2202\u03b1\n\nj=1\n\n(cid:14)\n\nij (\u03b1) = vjf(yi|dj; \u03b1)\n\ncgh\n\nk vkf(yi\n\n|dk; \u03b1) , with\n\n(cid:7)\n\nj\n\nwhere\n\n|dj; \u03b1)\n\n,\n\n(14.15)\n\nij (\u03b1) = 1,\ncgh\n\n(14.16)\n\ndenote weight factors that depend on the parameters \u03b1. the derivative\n|dj, \u03b1)/\u2202\u03b2t , \u2202 log f(yi\n\n|dj, \u03b1)/\u2202\u03b8t )\ncorresponds to the score function of the glm e(\u02dcyitj) = h(\u03b7itj) with the predictors\n\n|dj, \u03b1)/\u2202\u03b1t = (\u2202 log f(yi\n\n\u2202 log f(yi\n\n\u03b7itj = xt\n\nit\u03b2 + (dt\n\nj\n\n\u2297 zt\n\nit)\u03b8 = \u02dcxt\n\nitj\u03b1\n\n(14.17)\n\nfor observations \u02dcyitj, t = 1, . . . , ti, j = 1, . . . , m, where m is the number of quadrature\npoints and \u02dcyitj = yit. this means that the original ti observations for cluster i become tim\nobservations in the corresponding glm. the essential point in (14.17) is that the unknown ai\nis replaced by the known quadrature point dj, yielding a weighted score function of glm type.\nmles for \u03b1 have to be computed by an iterative procedure such as newton-raphson or\nfisher scoring. both algorithms imply the calculation of the observed or expected informa-\ntion matrix. since the weights cgh\ndepend on the parameters to be estimated, the analytical\nderivation of information matrices is cumbersome. an alternative is to calculate the observed\ninformation matrix by numerical differentiation of sgh. a direct maximization procedure for\ncumulative logit and probit models is considered by hedeker and gibbons (1994). for more\ndetails see hinde (1982), anderson and aitkin (1985), and fahrmeir and tutz (2001, section\n7.4).\n\nij\n\nif the number of quadrature points in a gauss-hermite quadrature is large enough, the\napproximation of the likelihood becomes sufficiently accurate. thus, as n and the number of\nquadrature points tend to infinity, the mles for \u03b2 will be consistent and asymptotically normal\nunder the usual regularity conditions. a procedure that may reduce the number of quadrature\npoints is the adaptive gauss-hermite quadrature (liu and pierce, 1994; pinheiro and bates,\n1995; hartzel et al., 2001). an adaptive quadrature is based on the log-likelihood (14.14); it\nfirst centers the modes with respect to the mode of the function being integrated and in addition\nscales them according to the curvature (see also appendix e.1).\n\nindirect maximization based on the em algorithm\nindirect maximization of the log-likelihood (14.14) can be obtained by use of an em algorithm\n(for the general form see appendix b). the em algorithm distinguishes between the observable\ndata, given by the response vector y, and the unobservable data, given by at = (at\nn ).\nthe complete data log-density is\n\n1 , . . . , at\n\nn(cid:7)\n\nn(cid:7)\n\nlog f(y, a; \u03b1) =\n\nlog f(yi\n\n|ai; \u03b1) +\n\ni=1\n\ni=1\n\nlog p(ai).\n\n(14.18)\n\n "}, {"Page_number": 422, "text": "410\n\nchapter 14. random effects models and finite mixtures\n\nin the e-step of the (s + 1)th em cycle, one determines the conditional expectation, given the\ndata y and an estimate \u03b1(s) from the previous em cycle:\n\n)\n\nlog(f(y, a; \u03b1))f(a|y; \u03b1(s))da,\n\nm(\u03b1|\u03b1(s)) = e{log f(y, a; \u03b1)|y; \u03b1(s)} =\nwhere the density f(a|y; \u03b1(s)) denotes the posterior\n\n2\n2\n\n\u2019\n\n2\n\nf(a|y; \u03b1(s)) =\n\nn\n\ni=1 f(yi\n\nn\ni=1\n\n|ai; \u03b1(s))\ni=1 p(ai)\n|ai; \u03b1(s))p(ai)dai\nf(yi\n\nn\n\n,\n\n(14.19)\n\nn(cid:7)\n\u2019\n\ni=1\n\nwhich is obtained from bayes\u2019 theorem and the conditional independence given the random\neffects. one obtains\nm(\u03b1|\u03b1(s)) =\n\n|ai; \u03b1) + log g(ai)]f(yi\n\n|ai; \u03b1(s))p(ai)dai,\n\n[log f(yi\n\n\u22121\ni\n\nk\n\n)\n\n|ai; \u03b1(s))g(ai)dai does not depend on the parameters \u03b1 and the\nwhere the factor ki =\nre-parameterized random effects ai. the integral in m(\u03b1|\u03b1(s)) has to be approximated, for\nexample, by gauss-hermite or monte carlo integration. when assuming a normal distribution\nfor the random effects, using the gauss-hermite integration yields the approximation\n\nf(yi\n\nn(cid:7)\n\n(cid:7)\n\nm gh(\u03b1|\u03b1(s)) =\n\ncgh\nij\n\n[log f(yi\n\nwith the weight factors\n\ni=1\n\nj\n\nij = vjf(yi\ncgh\nvkf(yi\n\n|dj; \u03b1(s))\n|dk; \u03b1(s))\n\n(cid:7)\n\nk\n\n,\n\nj\n\n|dj; \u03b1) + log p(dj)]\n(cid:7)\n\nij = 1,\ncgh\n\n(14.20)\n\nwhere dj denotes the fixed quadrature points and vj denotes fixed weights associated with dj.\nthe essential point, which makes maximization easier, is that the weights cgh\ndo not depend\non \u03b1, in contrast to the weights in direct maximization given in (14.16).\nin the m-step the function m(\u03b1|\u03b1(p)) is maximized with respect to \u03b1. for the gauss-\n\nij\n\nhermite approximation one obtains the derivative\n\n\u2202m gh(\u03b1|\u03b1(p))\n\n\u2202\u03b1\n\n=\n\n|dij; \u03b1)\n\n\u2202 log f(yi\n\u2202\u03b1\n\ncgh\nij\n\n,\n\n(14.21)\n\nn(cid:7)\n\nm(cid:7)\n\ni=1\n\nj=1\n\nwhich has the same form as (14.15) but with weights that do not depend on \u03b1. solving the\nequation \u2202m gh(\u03b1|\u03b1(s))/\u2202\u03b1 = 0 uses that \u2202m gh(\u03b1|\u03b1(s))/\u2202\u03b1 corresponds to the weighted\nscore function of the glm e(\u02dcyitj) = h(\u03b7itj) with predictors\n\n\u03b7itj = xt\n\nit\u03b2 + (dt\n\nj\n\n\u2297 zt\n\nit)\u03b8 = \u02dcxt\n\nitj\u03b1\n\n(14.22)\n\nfor observations \u02dcyitj, t = 1, . . . , ti, j = 1, . . . , m, where m is the number of quadrature\npoints and \u02dcyitj = yit. the resulting em algorithm is often slow but rather stable and simple to\nimplement.\n\nthe em algorithm has been used by hinde (1982), brillinger and preisler (1983), anderson\nand hinde (1988), and jansen (1990) for one-dimensional random effects; anderson and aitkin\n(1985) and im and gianola (1988) for bivariate random effects; and tutz and hennevogl (1996)\nfor ordinal models. an elaborate monte carlo technique was proposed by booth and hobert\n(1999).\n\n "}, {"Page_number": 423, "text": "14.3. estimation methods for generalized mixed models\n\n411\n\n14.3.2 posterior mean estimation of random effects\nprediction of random effects bi may be based on the posterior density of bi given the observa-\ntions yt = (yt\nn ). due to the independence assumptions, the posterior of bi depends\nonly on yi and one obtains\n\n1 , . . . , yt\n\nf(bi|yi; \u03b2, q) = f(yi\nf(yi\n\n|bi; \u03b2)p(bi; q)\n|bi; \u03b2)p(bi; q)dbi\n\n.\n\n\u2019\n\nafter replacing \u03b2 and q by estimates \u02c6\u03b2, \u02c6q one obtains the posterior mean estimate:\n\n)\n\n\u02c6bi =\n\nbif(bi|yi, \u02c6\u03b2, \u02c6q)dbi.\n\nevaluation of integrals again makes approximation techniques, for example, numerical or monte\ncarlo techniques, necessary.\n\n14.3.3 penalized quasi-likelihood estimation for given variance\nalternative approaches to fitting random effects models yield a penalized log-likelihood (or\nquasi-log-likelihood) for the estimation of \u03b2 and b1, . . . , bn. let \u03b2 and b1, . . . , bn be collected\nin \u03b4t = (\u03b2t , bt\nn ) and q be known. the \"joint maximization\" approaches aim at\nestimating \u03b2 and b1, . . . , bn together.\n\n1 , . . . , bt\n\nmotivation as posterior mode estimator\nsince b1, . . . , bn are independent, the assumption of a flat prior on \u03b2 (cov(\u03b2) \u2192 \u221e) yields\n\np(\u03b4; q) \u221d n(cid:15)\n2\n\ni=1\n\n\u2019 2\n\np(bi; q),\n\n2\n\nwhich depends only on the covariance cov(bi) = q. bayes\u2019 theorem yields\n\nf(\u03b4|{yi\n\n}; q) =\n\n|bi, \u03b2)\n\nn\n\ni=1 f(yi\n\nn\n\ni=1 p(bi, q)\n\n|bi, \u03b2)p(bi; q)db1 . . . dbnd\u03b2\n\nn\n\ni=1 f(yi\n\n.\n\n(14.23)\n\na posterior mean implies a heavy computational burden. however, posterior mode estimation\nturns out to be a feasible alternative. by using only the nominator of (14.23), maximization of\n(14.23) is equivalent to maximizing the log-posterior\n\nn(cid:7)\n\nlog(f(yi\n\n|bi; \u03b2)) +\n\nlog p(bi; q)\n\ni=1\n\ni=1\n\nwith respect to \u03b4. for normally distributed random effects one obtains (after dropping irrelevant\nterms)\n\nlp(\u03b4) =\n\nlog (f(yi\n\n|bi, \u03b2)) \u2212 1\n2\n\n\u22121bi.\n\nbt\ni q\n\n(14.24)\n\nn(cid:7)\n\ni=1\n\nn(cid:7)\n\nn(cid:7)\n\ni=1\n\n "}, {"Page_number": 424, "text": "412\n\nchapter 14. random effects models and finite mixtures\n\nmotivation by laplace\u2019s approximation\nfor the general mixed model (14.12) the marginal likelihood has the form\n\n)\n\nl(\u03b2, qb) =\n\nf(y|b; \u03b2) p(b; qb)db.\n)\n\nfor normally distributed b \u223c n(0, qb) one obtains\n\nl(\u03b2, qb) = |qb\n\n|\u22121/2(2\u03c0)\n\n\u2212q/2\n\nexp{log(f(y|b; \u03b2)) \u2212 1\n\nb b}db,\n\u22121\n\n2 bt q\n\nq = dim(b). with \u03ba\u03b2(b) = \u2212 log f(y|b; \u03b2) + 1\nlaplace approximation (see appendix e.1) yields\n\n2 bt q\n\nb b, the integrand is exp(\u2212\u03ba\u03b2(b)) and\n\u22121\n\nl(\u03b2, qb) \u2248 |qb\n\n|\u22121/2 exp(\u2212\u03ba\u03b2(\u02dcb)) |\u22022\u03ba\u03b2(\u02dcb)/\u2202b\u2202bt )|\u22121/2,\n\nwhere \u02dcb minimizes \u03ba\u03b2(b). one obtains the approximative log-likelihood\n\nl(\u03b2, qb) \u2248 \u2212\u03ba\u03b2(\u02dcb) \u2212 1\n2\n\nlog(|qb\n\n|) \u2212 1\n2\n\nlog |\u22022\u03ba(\u02dcb)/\u2202b\u2202bt|.\n\nthe second derivative \u22022\u03ba\u03b2(b)/\u2202b\u2202bt has the form\n\n\u22022\u03ba(b)\n\u2202b\u2202bt = zt d\u03c2\n\n\u22121dt z + q\n\n\u22121\nb + m ,\n\nwhere d = diag(\u2202h(\u03b71)/\u2202\u03b7, . . . , \u2202h(\u03b7n )/\u2202\u03b7), \u03c3 = diag(\u03c32\ni = var(bi), and\nthe remainder term m has expectation zero. thus, by ignoring m and inserting \u03ba\u03b2(\u02dcb), the\napproximative log-likelihood has the form\nl(\u03b2, qb) \u2248 log f(y|\u02dcb, \u03b2) \u2212 1\n\u02dcb \u2212 1\n2\n2\n= log f(y|\u02dcb, \u03b2) \u2212 1\n2\n\n|) \u2212 1\n|))\n2\n\u22121dzqb + i|.\n\n\u22121\nb\nlog |z t d\u03c2\n\nlog(|z t d\u03c2\n\n\u22121dz + q\n\nlog(|qb\n\n\u02dcb \u2212 1\n2\n\n1, . . . , \u03c32\n\nn ), \u03c32\n\n\u22121\nb\n\n\u22121\nb\n\nq\n\nq\n\n\u02dcb\n\n\u02dcb\n\nt\n\nt\n\nbreslow and clayton (1993) also ignore the last term. since \u02dcb is the minimum of \u03ba\u03b2(b), by\ndefinition of \u03ba\u03b2(b) it may also be seen as the maximum of the penalized likelihood:\n\nlp(\u03b4) = log(f(y|b, \u03b2)) \u2212 1\n\n2 bt q\n\n\u22121\nb b,\n\n(14.25)\n\nwhich is the general form of (14.24).\n\nsolution of the penalized likelihood problem\nlp(\u03b4) may be maximized by solving sp(\u03b4) = (\u2202lp(\u03b4)/\u2202\u03b2t , \u2202lp(\u03b4)/\u2202bt )t = 0, where the\nfirst term is given by\n\n\u2202lp(\u03b4)\n\n= x t d(\u03b4)\u03c3\n\n\u22121(y \u2212 \u03bc(\u03b4)),\n\nand the second is\n\n\u2202\u03b2\n\n\u2202lp(\u03b4)\n\n\u2202b\n\n= z t d(\u03b4)\u03c3\n\n\u22121(\u03b4)(y \u2212 \u03bc(\u03b4)) \u2212 q\n\n\u22121\nb b.\n\n "}, {"Page_number": 425, "text": "14.3. estimation methods for generalized mixed models\n\n413\n\nin closed form the penalized likelihood and the corresponding score function, which use pre-\ndictor \u03b7 = x\u03b2 + zb, are given as\n\nwhere \u02dcx = [x|z] with\n\nlp(\u03b4) = log(f(y|b, \u03b2)) \u2212 1\n2 \u03b4t k\u03b4,\n\u22121(y \u2212 \u03bc) \u2212 k\u03b4,\n(cid:26)\n\nsp(\u03b4) = \u02dcx\n\n(cid:25)\n\nd\u03c2\n\nt\n\nk =\n\n0\n0\n\u22121\n0 q\nb\n\n.\n\niterative pseudo-fisher scoring has the form\n\n\u02c6\u03b4\n\n(k+1) = \u02c6\u03b4\n\n(k) + f\n\nwith f p(\u03b4) = \u02dcx\n\nt\n\nw \u02dcx + k, w (\u03b4) = d(\u03b4)\u03c3\n\n\u02c6\u03b4\n\n(k+1) = ( \u02dcx\n\nt\n\nw (\u02c6\u03b4\n\n(k)) \u02dcx + k)\n\n\u22121\np\n\n\u02c6\u03b4\n\nsp\n\n(cid:27)\n\n(k)(cid:28)\n\n(cid:27)\n(k)(cid:28)\n\u02c6\u03b4\n\u22121(\u03b4)d(\u03b4)t . an alternative form is\n\u22121 \u02dcx\n\n(k))\u02dc\u03b7(\u02c6\u03b4\n\nw (\u02c6\u03b4\n\n(k)),\n\nt\n\n,\n\n(14.26)\n\n\u22121(\u03b4)(y \u2212 \u03bc(\u03b4)). therefore, one step of pseudo-\nwith pseudo-observations \u02dc\u03b7(\u03b4) = \u02dcx\u03b4 + d\nfisher scoring solves the system of equations for the best linear unbiased estimation in normal\n1\nresponse models (harville, 1977):\n\n1\n\n0\n\n0\n\nx t w x x t w z\nz t w x q\n\n\u22121\nb + zt w z\n\n\u02c6\u03b4\n\n(k+1) =\n\nx t w\nz t w\n\n\u02dc\u03b7(\u02c6\u03b4\n\n(k)),\n\nwhere the dependence of matrices w and d on \u03b4 is suppressed. the solutions have the form\n\n\u02c6\u03b2\n\n(k+1) = [x t v\n\n\u22121x]\n\u02c6b\n(k+1) = qbz t v\n\u22121 + zqbzt .\n\n\u22121x t v\n\u22121(y \u2212 x \u02c6\u03b2\n\n\u22121\u02dc\u03b7(\u02c6\u03b4\n(k)),\n\n(k)),\n\nwith v = w\n\nfor clustered data yit with predictor \u03b7it = xt\n\n= z t\n\ni di(\u03b4)\u03c3\n\n\u2202lp(\u03b4)\n\u2202bi\n\u2202lp(\u03b4)\n\n\u2202\u03b2\n\nn(cid:7)\n\ni=1\n\n=\n\nx t\n\ni di(\u03b4)\u03c3\n\nit\u03b2 + zt\n\u22121\ni\n\n(yi\n\nitbi the score functions simplify to\n\u2212 \u03bci) \u2212 q\n\u22121\ni\n\n\u2212 \u03bci),\n\n\u22121bi,\n\n(yi\n\nwhere di(\u03b4) = diag(\u2202h(\u03b7i1)/\u2202\u03b7, \u2202h(\u03b7i2)/\u2202\u03b7, . . . ), \u03c3 i = diag(\u03c3i1, \u03c3i2, . . . ). iterative solu-\ntions for single effects are given as\n\n\u02c6b\n\n(k+1)\ni\n\n= qz t\n\ni v\n\n\u22121\ni\n\n(yi\n\n\u2212 x i \u02c6\u03b2\n\n(k)).\n\ndetails for the inversion of the pseudo-fisher matrix f p(\u03b4) are given in the appendix (see\ne.1).\n\n14.3.4 estimation of variances\nthe penalized log-likelihood approach considered in section 14.3.3 yields estimates of \u03b4t =\nn ) under the assumption that q is known. in the following we consider estima-\n(\u03b2t , bt\ntion methods for q.\n\n1 , . . . , bt\n\n "}, {"Page_number": 426, "text": "414\n\nchapter 14. random effects models and finite mixtures\n\nreml-type estimates\nfor the estimation of variances breslow and clayton (1993) maximize the profile likelihood\nthat is associated with the normal theory model. with \u03b2 replaced by \u02c6\u03b2 one maximizes\n\nl(\u02c6\u03b2, qb) = \u22121\n2\n\nlog(|v |) \u2212 1\n2\n\nlog(|x t v\n\n\u22121x|) \u2212 1\n2\n\n(\u02dc\u03b7(\u02c6\u03b4) \u2212 x \u02c6\u03b2)t v\n\n\u22121(\u02dc\u03b7(\u02c6\u03b4) \u2212 x \u02c6\u03b2)\n\n\u22121(\u03b4)(y\u2212\u03bc(\u03b4)). typically, the\nwith respect to qb, with pseudo-observations \u02dc\u03b7(\u03b4) = \u02dcx\u03b4 +d\nunknown matrix qb is parameterized, qb = qb(\u03b3), where some structure of qb is assumed,\nand maximization refers to the parameter \u03b3. in practice, one iterates between one step of fisher\nscoring (yielding \u02c6\u03b4\n\n) and one step of maximizing l(\u02c6\u03b2, qb) (yielding \u02c6q\n\n(k)\nb ).\n\n(k)\n\nalternative estimates\nestimates obtained by iteratively improving the estimates of \u03b4 and q may also be based on an\nalternative estimation of variances. a simple estimate, which can be derived as an approximate\nem algorithm, uses the posterior mode estimates \u02c6\u03b4\n(evaluated\nat \u02c6\u03b4\n\nand posterior curvatures \u02c6v\n\n) by computing\n\n(k)\nii\n\n(k)\n\n(k)\n\nn(cid:7)\n\ni=1\n\n\u02c6q\n\n(k) =\n\n1\nn\n\n( \u02c6v\n\n(k)\n\nii + \u02c6b\n\n(k)\ni\n\n(\u02c6b\n\n(k)\ni\n\n)t ).\n\njoint maximization of a penalized log-likelihood with respect to parameters and random\neffects appended by estimation of the variance of random effects can be justified in various\nways (see also schall, 1991; wolfinger, 1994; mcculloch and searle, 2001). the derivation of\nbreslow and clayton (1993) is often referred to as a penalized quasi-likelihood (pql) because\nit uses the more general concept of quasi-likelihood. although modifications were proposed\n(breslow and lin, 1995; lin and breslow, 1996) joint maximization algorithms tend to un-\nderestimate the variance and therefore the true values of the random effects (see, for example,\nmcculloch, 1997). in particular, for binary data in small clusters performance might be poor.\nsimilar approaches have been used by stiratelli et al. (1984) for binary logistic models, harville\nand mee (1984) for cumulative models, and wong and mason (1985) for multilevel analysis.\n\nerror approximation\nif the cluster sizes are large enough, one can use the normal approximation\n\n\u02c6\u03b4 \u223c n(\u03b4, f p(\u03b4)\n\n\u22121)\n\nto evaluate standard errors of \u02c6\u03b4.\n\nexample 14.3: knee injuries\nin the knee injuries study pain was recorded for each subject after 3, 7, and 10 days of treatment. the\nmarginal effects of treatment and the covariates gender and age were evaluated in example 13.4. again\nwe consider the dichotomized response and fit the random effects logit model:\n\nlogit(p (yit = 1|xi)) = bi + xt,i\u03b2r + xg,i\u03b2g + agei\u03b2a + age2\n\ni \u03b2a2 ,\n\nwhere xt,i = 1 for treatment, xt,i = 0 for placebo, and xg,i = 1 for females, xt,i = 0 for males.\ntable 14.1 shows the estimated parameters resulting from a gauss-hermite quadrature with 20 quadrature\npoints (gh(20)), a penalized quasi-likelihood (pql), and the marginal model with an exchangeable cor-\nrelation structure. for ease of interpretation the variable age has been centered around 30. the estimated\nstandard deviation of the mixing distribution is 3.621 for gh(20) and 2.706 for pql. since pql tends\n\n "}, {"Page_number": 427, "text": "14.3. estimation methods for generalized mixed models\n\n415\n\nto underestimate the variation of random effects, it was to be expected that the value is smaller than for a\ngauss-hermite quadrature. consequently, the estimated fixed effects are also smaller for pql. however,\nboth procedures yield distinctly larger parameter estimates than the marginal model. of course the param-\neters of the marginal model are population-averaged effects, whereas the parameters of random effects\nmodels are measured on the individual level and therefore the estimates cannot be compared directly (see\nalso the introductory remark to this chapter and section 14.2.1).\n\ntable 14.1: knee data binary; gauss-hermite, penalized quasi-likelihood, and marginal\nmodels.\n\ncoef\n\ngauss-hermite\npr > |z|\n0.005\n0.027\n0.494\n0.460\n0.003\n\n3.054\n\u22121.861\n0.607\n0.032\n\u22120.015\n\nintercept\nfactor(th)2\nfactor(sex)1\nage\nage2\n\npql\n\ncoef\n\n2.142\n\u22121.294\n0.414\n0.022\n\u22120.010\n\npr> |z|\n0.003\n0.023\n0.506\n0.463\n0.001\n\nmarginal\n\ncoef\n\nrobust z\n\n1.172\n\u22120.673\n0.265\n0.013\n\u22120.006\n\n2.617\n\u22122.016\n0.724\n0.788\n\u22123.085\n\n14.3.5 bayesian approaches\nin bayesian approaches all the unknown parameters are considered as random variables steming\nfrom prior distributions. thus the structural assumption is considered as conditional on \u03b2 and\nbi and has the form\n\ne(yit|\u03b2, bi) = h(x it\u03b2 + z itbi).\n\n(14.27)\n\nin addition to a mixing distribution for random effects p(bi) one has to specify a prior\ndistribution p(\u03b2). this can be done in a hierarchical way by specifying, for example, a normal\ndistribution p(bi|q) \u223c n(0, q) together with a prior for the hyperparameter q. a better\nchoice than jeffreys prior, which may be improper, is the assumption of a highly dispersed\ninverse wishart distribution q \u223c iwr(\u03be, \u03c8) with density\n\np(q) \u221d |q|\u2212(\u03be+m+1)/2 exp(\u2212tr(\u03c8q\n\n\u22121)/2),\n\nwhere m is the dimension of bi and \u03be and \u03c8 are hyperparameters that have to be fixed (see\nbesag et al., 1995). in the one-dimensional case the distribution reduces to the inverse gamma\ndistribution\n\np(\u03c32|\u03b1, \u03b2) = \u03b2\u03b1\u03c32(\u2212\u03b1\u22121)exp(\u2212\u03b2/\u03c32)\n\n\u03b3(\u03b1)\n\nwith shape parameter \u03b1 > 0 and scale parameter \u03b2 > 0.\nif one assumes conditional independence among the response variables yit|bi, \u03b2, the ran-\ndom effects bi|q, the regression parameters \u03b2, and the hyperparameter q, one obtains for the\nposterior distribution\n\np(\u03b2, b1, . . . , bn, q|data) \u221d n(cid:15)\n\nti(cid:15)\n\nn(cid:15)\n\nf(yit|\u03b2, bi), p(\u03b2)\n\np(bi|q)p(q),\n\ni=1\n\nt=1\n\ni=1\n\n "}, {"Page_number": 428, "text": "416\nand the full conditionals p(\u03b2|\u00b7), p(bi|\u00b7), p(q|\u00b7), given the data and the rest of parameters, sim-\nplify to\n\nchapter 14. random effects models and finite mixtures\n\nti(cid:15)\n\np(\u03b2|\u00b7) \u221d n(cid:15)\np(bi|\u00b7) \u221d ti(cid:15)\np(q|\u00b7) \u221d n(cid:15)\n\ni=1\n\nt=1\n\ni=1\n\nf(yit|\u03b2, bi)p(\u03b2),\n\nt=1\n\nf(yit|\u03b2, bi)p(bi|q),\n\nf(bi|q)p(q).\n\n(cid:14)\n\nthe latter conditional p(q|\u00b7) is again an inverse wishart with updated parameters \u03be + n/2\nand \u03c8 + 1\ni . for computational details see zeger and karim (1991) and gamerman\n2\n(1997). an extensive treatment of bayesian mixed models is found in fahrmeir and kneib\n(2010); see also chib and carlin (1999), kinney and dunson (2007), and t\u00fcchler (2008).\n\ni=1 bibt\n\nn\n\n14.4 multicategorical response models\nin the knee injury example pain was originally measured on a five-point scale. with ordered\ncategories like that, which are subjective judgements, heterogeneity is to be expected since each\nindividual has its own sensitivity for pain. therefore, modeling a subject-specific parameter\nseems warranted. in the following we consider the extension to multicategorical responses.\n\nordered response categories\nlet yit denote observation t on unit i (i = 1, . . . , n, t = 1, . . . , ti), where yit \u2208 {1, . . . , k}.\nthen a simple ordinal mixed model of the cumulative type is\n\np (yit \u2264 r|xit) = f (bi + \u03b30r + xt\n\nit\u03b3),\n\nwhere the xit\u2019s are design vectors built from covariates, \u03b3 is a population-specific parameter,\nand bi is a cluster-specific random effect, which follows a mixing distribution. the correspond-\ning sequential model has the form\n\np (yit = r|yit \u2265 r, x) = f (bi + \u03b30r + xt\n\nit\u03b3).\n\nin both models, the random effect bi represents an individual response level that differs across\nsubjects.\n\nas shown in section 9.5, both models can be written as multivariate glms if no random\n\neffect is present. with random effects one obtains the general form\n\ng(\u03c0it) = x it\u03b2 + z itbi\n\nor \u03c0it = h(x it\u03b2 + z itbi),\n\nit = (\u03c0it1, . . . , \u03c0itq), q = k \u2212 1, is the vector of response probabilities, g is the\nwhere \u03c0t\n\u22121 is the inverse link function. the matrix z it specifies\n(multivariate) link function, and h = g\nit = (1, . . . , 1).\nthe structure of the random effects. for simple random intercepts the matrix is z t\nin the case of random slopes, the matrix contains the corresponding variables. in the more\ngeneral case with category-specific random effects bir, problems may occur when using the\ncumulative model since the intercepts have to be ordered. however, the assumption \u03b301 + bi1 \u2264\n\u03b302 + bi2 \u2264 . . . cannot hold if a normal distribution for bir is assumed. the problem can be\n\n "}, {"Page_number": 429, "text": "14.4. multicategorical response models\n\n417\n\novercome by reparameterizing the thresholds (see tutz and hennevogl, 1996). an advantage\nof the sequential model is that random intercepts are not restricted and binary mixed model\nsoftware can be used to fit the model (see problem 14.3).\n\nexample 14.4: knee injuries\nin the knee injuries study pain was recorded for each subject after 3, 7, and 10 days of treatment. a\nsimple binary model with a random intercept and dichotomized responses was investigated in example\n14.3. now we fit the sequential random effects logit model:\n\nlogit(p (yit = r|yit \u2265 r, xi)) = bi + \u03b20r + xt,i\u03b2t + agei\u03b2a + age2\n\ni \u03b2a2 ,\n\nwhere xt,i = 1 for treatment, xt,i = 0 for placebo, and age (centered around 30) is included as a\nlinear and quadratic term. gender has been omitted because it turned out not to be influential. table 14.2\nshows the estimated parameters resulting from a gauss-hermite quadrature with 25 quadrature points (r\nfunction lmer) and penalized quasi-likelihood (pql; r function glmmpql). the standard deviation of\nthe random effect is 5.425, with a standard error 0.630 for pql and 5.910 for gauss-hermite. in contrast\nto the dichotomized response now age should not be neglected. the effect of therapy now refers to the\ntransition between categories of pain with a distinct effect of therapy. an alternative is the cumulative\nmodel\n\nlogit(p (yit \u2264 r|xi)) = bi + \u03b20r + xt,i\u03b2r + agei\u03b2a + age2\n\ni \u03b2a2 .\n\nbecause of restrictions on the parameters the model is harder to fit. the r function clmm yields estimates\nfor a laplace approximation and a gauss-hermite approximation, but no standard errors are available.\nwith standard deviation 6.37 for the laplace approximation and 6.25 for the gauss-hermite, heterogeneity\nseems to be even stronger than for the sequential model.\n\ntable 14.2: knee data sequential model; gauss-hermite and penalized quasi-likelihood.\n\ngauss-hermite\ncoef\np-value\n\npql\n\ncoef\n\np-value\n\ntherapy\nage\nage2\n\n2.402\n0.002\n0.023\n\n0.031\n0.976\n0.001\n\n2.113\n0.0003\n0.017\n\n0.037\n0.996\n0.006\n\nexample 14.5: recovery scores\nin the recovery data (example 14.2) recovery scores were measured repeatedly on a categorical scale\nimmediately after admission to the recovery room and at minutes 5, 15, and 30 following admission. the\ncovariates are dosage of the anaesthetic (four levels), duration of surgery (in minutes), and age of the child\n(in months). the cumulative model that is considered is\n\nlogit(p (yit \u2264 r|xi)) =bi + \u03b20r + dose1i\u03b2d1 + dose2i\u03b2d2 + dose3i\u03b2d3 + durationi\u03b2du\n\n+agei\u03b2a + age2\n\ni \u03b2a2 + rep1i\u03b2r1 + rep2i\u03b2r2 + rep3i\u03b2r3,\n\nwhere dosei and repi are (0-1)-dummies for dose and repetition, respectively. instead of fitting the whole\nmodel we used a boosting algorithm that successively includes the relevant variables. figure 14.6 shows\nthe coefficient buildups for the standardized predictors. it is seen that the dominant influence comes from\nthe replications; all other predictors are neglected (see also tutz and groll, 2010a, and exercise 14.4)\n\n "}, {"Page_number": 430, "text": "418\n\nchapter 14. random effects models and finite mixtures\n\n\u03b2^\n\n0\n\n.\n\n3\n\n5\n2\n\n.\n\n0\n2\n\n.\n\n5\n\n.\n\n1\n\n0\n1\n\n.\n\n5\n\n.\n\n0\n\n0\n0\n\n.\n\nreplication1\n\nreplication2\n\nreplication3\n\ndose1 dose2 dose3 ageage2duration\n\n0\n\n100 200 300 400 500 600 706\n\nnumber of boosting steps\n\nfigure 14.6: coefficient buildup for recovery data.\n\nmixed multinomial logit model\nin the multinomial logit model (see chapter 8),\n\np (yit = r|x) =\n\n1 +\n\nexp(\u03b7itr)\nk\u22121\ns=1 exp(\u03b7its)\n\nr = 1, . . . , k \u2212 1,\n\n,\n\n(cid:14)\n\nrandom intercepts are included by assuming \u03b7itr = bir + xt \u03b2r, where the vector of random\nintercepts, bi = (bi1, . . . , bi,k\u22121), is normally distributed, n(0, q). then the probability of\nresponse in one of the categories {1, . . . , k} may vary across individuals.\n\ngeneralized linear mixed model for clustered data\n\ng(\u03c0it) = x t\n\nit\u03b2 + z t\n\nitbi\n\nwith\n\ne(bi) = 0, cov(bi) = q\n\nin the same way as for univariate responses one can find closed forms by collecting the\nobservations of one cluster in a vector. let the underlying observations that have mean \u03c0it be\nit = (yit1, . . . , yitq), where yits = 1 if yit = s. then the whole set of responses\ngiven as yt\nis yt = (yt\n), and \u03c0i = e(yi) has the matrix form g(\u03c0i) =\n1 , . . . , yt\nx i\u03b2 + zibi, with x i and z i containing matrices x it, z it, t = 1, . . . , ti. stacking matrices\ntogether yields the general form g(\u03bc) = x\u03b2 + zb, where x t = (x t\nn ) and z\nis a block-diagonal matrix with blocks zi, bt = (bt\nn ). with that notation and small\nadaptations one may use the estimation concepts for univariate glmms.\n\ni1, . . . , yt\niti\n\n1 , . . . , x t\n\n1 , . . . , bt\n\ni = (yt\n\nn ), yt\n\ncumulative type random effects models were considered by harville and mee (1984),\njansen (1990), and tutz and hennevogl (1996); adjacent categories type models were con-\nsidered by hartzel et al. (2001). hartzel, agresti, and caffo (2001) investigated multinomial\nrandom effects models.\n\n "}, {"Page_number": 431, "text": "14.5. the marginalized random effects model\n\n419\n\n14.5 the marginalized random effects model\nrandom effects models parameterize the mean response conditional on covariates and random\neffects. the interpretation of parameters is subject-specific and refers to the change in the\nmean response having accounted for all the conditioning variables including the random effects.\nsince it is implicitly assumed that the distribution of the random effects is correct, violation\nof the random effects distribution will affect the estimation and interpretation of parameters.\nmarginal models as considered in chapter 13 do not depend on the random effects distribution\nbecause they do not assume random effects. marginal models that include random effects but\nare less sensitive to the assumed distribution are the marginalized random effects models or the\nmarginalized latent variable models.\n\nmarginalized random effects models specify a marginal model that is connected to a latent\nvariable model (see fitzmaurice and laird, 1993; azzalini, 1994; heagerty, 1999; heagerty and\nzeger, 1996). let \u03bcit denote the marginal mean, \u03bcit = e(yit|xit). one specifies a regression\nstructure for the marginal mean by\n\n(14.28)\n\u22121 is the link function. the second component of the model describes the depen-\nwhere g = h\ndence among measurements within a cluster by conditioning on the random effect bi, assuming\n\ng(\u03bcit) = xt\n\n\u03bcit = h(xt\n\nit\u03b2)\n\nit\u03b2,\n\nor\n\nit = h(\u03b4it + bit) or\n\u03bcc\n\ng(\u03bcc\n\nit) = \u03b4it + bit,\n\n(14.29)\n\nit = e(yit|xi, bi) with xt\n\ni = (xi1, . . . , xiti) is the conditional mean given xi and\nwhere \u03bcc\nbi. moreover, one assumes that yi1, . . . yiti are conditionally independent given bi and the\ncovariates xi. for bi given the covariates a distribution is assumed, for example,\n\nbi \u223c n(0, qi),\n\nwhere qi is a function of a parameter vector \u03b1. when g is the logit function one obtains the\nmarginally specified logistic-normal model.\n\nthe value \u03b4it in the conditional model (??) is implicitly determined by the integral that\n(b)db, where\nit = var(bit). thus, \u03b4it is\nit. given these values, the integral can be solved for \u03b4it using numerical\n\nlinks the marginal mean and the conditional distribution, \u03bcit =\np0,\u03c32\na function of \u03bcit and \u03c32\nintegration.\n\nis the normal density function with mean zero and variance \u03c32\n\nh(\u03b4it + b)p0,\u03c32\n\n\u2019\n\nit\n\nit\n\nthe marginalized model specified by (14.28) and (14.29) parameterizes the marginal re-\nsponse, which determines the interpretation of \u03b2, but accounts for heterogeneity and the re-\nsulting correlation by additionally assuming a random effects model. the parameter \u03b2 mea-\nsures the change in the marginal response due to a change in covariates, that is, the effect of\ncovariates averaged over the distribution within subgroups that are defined by the covariates.\nheagerty (1999) illustrates the interpretation of the heterogeneity parameter by considering the\nmarginally specified logistic-normal model with bit = bi0 and bi0 \u223c n(0, \u03c32). then the indi-\nvidual variation is seen from the model representation logit e(yit|xit, zi) = \u03b4it + \u03c3zi. thus\n\u03c3 measures the variation between individuals within a group that is defined by the measured\ncovariates xi, which also determine \u03b4it. in the more general case of bit varying over t one\nassumes, for example, an autoregressive model. then the parameters in qi are measures of\nvariation across t and individuals. the regression coefficients have interpretations that refer to\nthe mean change given the observed covariates, averaged over unobserved latent variables.\n\nfor ml estimation and estimation-equation approaches to estimation and inference see\nheagerty (1999). a generalization to multilevel models is given by heagerty and zeger (2000).\n\n "}, {"Page_number": 432, "text": "420\n\nchapter 14. random effects models and finite mixtures\n\n14.6 latent trait models and conditional ml\nthe modeling and measurement of heterogeneity has a long tradition in psychometrics. in item\nresponse theory latent traits that are to be measured often refer to abilities like \"intelligence.\"\nthe famous rasch model (rasch, 1961) assumes that the probability of participant i solving\ntest item t follows a logit model,\n\nlogit(p (yit = 1|\u03b1i, \u03b2j)) = \u03b1i + \u03b2j,\n\n(14.30)\n\nwhere \u03b1i is a person parameter that represents the ability and \u03b2j is an item parameter that\nrepresents the easiness of an item. both are measured on the same latent scale. with \u03b4j = \u2212\u03b2j\ndenoting the difficulty of an item one obtains the more intuitive form logit(p (yit|\u03b1i, \u03b2j)) =\n\u03b1i \u2212 \u03b4j, which shows that the probability of solving the item grows with the difference between\nthe ability of the person and the difficulty of the item, \u03b1i \u2212 \u03b4j.\n\n(cid:14)\n\nfor n persons trying to solve t items, model (14.30) is a model for repeated binary measure-\nments with fixed effects. each item has its own effect and the heterogeneity of the population\nis represented by the person parameters. fixed person parameters have the effect that usual\nml estimates are not consistent for growing n since the number of parameters increases with\nthe number of persons. an alternative is conditional maximum likelihood estimation. by con-\nt yit, i = 1, . . . , n, which are sufficient statistics, the\nditioning on the sum of solved items\ndependence on the person parameters \u03b1i is eliminated. although conditional ml has the ad-\nvantage that no distributional assumption is needed, a drawback is that between-cluster effects\nare conditioned away and cannot be estimated (compare the conditional ml estimates in sec-\ntion 13.4.2, where only two binary measurements are assumed). in addition, the sum of solved\nitems forms a set of sufficient statistics only for the logit link. an alternative that is used in\nmodern item response theory is the modeling of the person parameters as random effects. then\nitem parameters and the effect of explanatory variables that explain the item difficulty can be\nestimated by marginal ml estimates.\n\nextensions of the rasch model to the proportional odds model and corresponding condi-\ntional ml estimates were proposed by agresti and lang (1993) and agresti (1993a, 1993d).\nfor an overview on generalized versions of the rasch model that include explanatory variables\nsee de boeck and wilson (2004).\n\n14.7 semiparametric mixed models\nin the generalized linear mixed models as considered in the previous sections, the effect of\ncovariates has been modeled linearly. however, the assumption of a strictly parametric form is\nfrequently too restrictive when modeling longitudinal data. an alternative is to use smoothing\nmethods as considered in chapter 10.\n\ngeneralized linear mixed models for clustered data assume g(\u03bcit) = \u03b7it, where the linear\npredictor has the form \u03b7it = xt\nitbi. for a more general additive predictor let the\ndata be given by (yit, xit, uit, zit), i = 1, . . . , n, t = 1, . . . , ti, where yit is the response\nit =\nfor observation t within cluster i and xt\n(zit1, . . . , zits) are vectors of covariates, which may vary across clusters and observations. then\nthe additive semiparametric mixed model has the predictor\n\nit = (uit1, . . . , uitm), zt\n\nit = (xit1, . . . , xitp), ut\n\nit\u03b2 + zt\n\n\u03b7it = xt\n\nit\u03b2 +\n\n\u03b1(j)(uitj) + zt\n\nitbi = \u03bcpar\n\nit + \u03bcadd\n\nit + \u03bcrand\n\nit\n\n,\n\nm(cid:7)\n\nwhere\n\nj=1\n\n "}, {"Page_number": 433, "text": "m(cid:7)\n\nj=1\n\n14.7. semiparametric mixed models\n\n421\n\n(cid:14)\n\n\u03bcpar\nit = xt\nit =\n\u03bcadd\n\nit = zt\n\u03bcrand\n\nit\u03b2 is a linear parametric term;\nm\nj=1 \u03b1(j)(uitj) is an additive term with unspecified influence functions \u03b1(1), . . . , \u03b1(m);\nitbi contains the cluster-specific random effect bi, bi \u223c n (0, q(\u03c1)), where q(\u03c1)\n\nis a parameterized covariance matrix.\n\nlet the unknown function be expanded in basis functions \u03b1(j)(u) =\n\u03b1t\n\u03c6t\n\nj \u03c6j(u) by using the basis functions given in chapter 10. with \u03b1t = (\u03b1t\nit = (\u03c61(uit1)t , . . . , \u03c6m(uitm)t ) one obtains the linear predictor\n\nm\n\ns=1 \u03b1(j)\n\ns \u03c6(j)\n1 , . . . , \u03b1t\n\ns (u) =\nm) and\n\n(cid:14)\n\n\u03b7it = xt\n\nit\u03b2 + \u03c6t\n\nit\u03b1 + zt\n\nitbi,\n\nwhich corresponds to a glmm with fixed parameters \u03b2, \u03b1 and random effects bi. in matrix\nnotation one obtains for one cluster \u03b7i = x i\u03b2 + \u03c6i\u03b1 + zibi, and \u03b7 = x\u03b2 + \u03c6\u03b1 + zb for\nthe whole predictor.\n\nsimultaneous estimations of fixed parameters and random effects, collected in \u03b4t = (\u03b2t , \u03b1t ,\n\nn ) can be based on the methods given in section 14.3.3. when many basis functions\nbt\n1 , . . . , bt\nare used, the coefficients that refer to basis functions should be penalized. by using a penalized\nlikelihood one obtains the double penalized log-likelihood:\n\nlp(\u03b4) = log(f(y|b, \u03b2)) \u2212 1\n2\n\nj kj\u03b1j \u2212 1\n\n\u03bbj\u03b1t\n\n2 bt q\n\n\u22121\nb b,\n\n(14.31)\n\nwhere kj penalizes the coefficients \u03b1j. in closed form the penalty has the form \u2212 1\n2 \u03b4t k\u03b4,\n\u22121\nwith block-diagonal matrix k = block(0, \u03bb1k1, . . . , \u03bbmkm, q\nb ). typically, the variance\nof random effects depends on the parameters, qb = qb(\u03c1). considering the penalties on co-\nefficients \u03b1j as functions of \u03bbj, kj(\u03bbj) = \u03bbjkj, one obtains the penalized log-likelihood\nof a glmm with variance parameters \u03bb1, . . . , \u03bbm, \u03c1, which can be estimated simultaneously\nby ml or reml methods. the approach to consider the smoothing parameters as variance\ncomponents in a mixed model was already helpful in the selection of smoothing parameters\n(section 10.1.5). the penalized log-likelihood (14.31) just includes a variance component that\nactually refers to a random effect. therefore, models with additive terms and random effects\nare embedded quite naturally within the mixed modeling representation of smoothing (see also\nwood, 2006a; ruppert et al., 2003; lin and zhang, 1999). a procedure that automatically se-\nlects relevant functions and performs well in high dimensions was given by tutz and reithinger\n(2007).\n\nexample 14.6: aids study\nfor the aids cohort study macs (example 14.1) the covariates were years since seroconversion, recre-\national drug use (yes/no), number of sexual partners, age, and a mental illness score (cesd). since the\nforms of the effects are not known, time since seroconversion, age, and the mental illness score may be\nconsidered as unspecified additive effects. we consider the semiparametric mixed model with poisson\ndistribution, log-link, and the linear predictor \u03b7it = \u03bcpar\n\nit + bit, where\n\nit + \u03bcadd\n\n\u03bcpar\nit = \u03b20 + drugsit\u03b2d + partnersit\u03b2p , \u03bcadd\n\nit = \u03b1t (timeit) + \u03b1a(agei) + \u03b1c (cesdit).\n\nfitting was performed using function gamm from the r package mgcv. figure 14.7 shows the smooth\neffects of time and the mental illness score. the corresponding p-values were below 0.002. the effect of\nage is not shown because it is linear with slope and very close to zero (p-value 0.66). it is seen that there is\na slight decrease in cd4 cells for increasing values of the mental illness score. the coefficients of number\nof partners and drugs were 0.036 and 0.003, both of them having p-value larger than 0.1. when additive\n\n "}, {"Page_number": 434, "text": "422\n\nchapter 14. random effects models and finite mixtures\n\ncomponents are selected by an appropriate boosting procedure, age is not included (see groll and tutz,\n2011a). slightly different functions are found when the response, after some transformation, is treated as\napproximately normally distributed (see tutz and reithinger, 2007).\n\n \n\n \n\n2\n0\n\n.\n\n0\n\n.\n\n0\n\n.\n\n2\n0\n\u2212\n\n4\n.\n0\n\u2212\n\n6\n.\n0\n\u2212\n\n2\n.\n0\n\n0\n.\n0\n\n2\n.\n0\n\u2212\n\n4\n.\n0\n\u2212\n\n6\n.\n0\n\u2212\n\n\u22122\n\n0\n\n2\n\n4\n\ntime\n\n0\n\n10\n\n20\ncesd\n\n30\n\n40\n\n50\n\nfigure 14.7: estimated effect of time and mental illness score for aids study.\n\n14.8 finite mixture models\nin cross-sectional data as well as in repeated measurements, a bad fit of a model can be due to\nunobserved heterogeneity. when an important categorical covariate has not been observed and\ntherefore not included in the predictor, one observes a mixture of responses that follow different\nmodels. instead of assuming a smooth distribution for the unobserved part heterogeneity can be\nmodeled by allowing a finite mixture of models. in finite mixtures of generalized linear models\nit is assumed that the density or mass function of observation y given x is a mixture:\n\nf(y|x) =\n\n\u03c0jfj(y|x, \u03b2j, \u03c6j),\n\n(14.32)\n\nj=1\n\nwhere fj(y|x, \u03b2j, \u03c6j) represents the jth component of the mixture that follows a simple ex-\nponential family parameterized by the parameter vector from the model \u03bcj = e(y|x, j) =\nh(xt \u03b2j) and dispersion parameter \u03c6j. the unknown component weights follow\nj=1 \u03c0j =\n1, \u03c0j > 0, j = 1, . . . , m.\n\n(cid:14)\n\nm\n\nm(cid:7)\n\n "}, {"Page_number": 435, "text": "14.8. finite mixture models\n\n423\n\nrather than allowing all the parameter vectors to be varying across components, it is of-\nten more appropriate to let only part of the variables, for example, the intercepts, vary across\ncomponents. a simple example is the finite mixture of two logit models, where one assumes\n\np (y = 1|x) = \u03c01p1(y = 1|x) + (1 \u2212 \u03c01)p2(y = 1|x).\n\nfor the components one can assume that all the weights differ:\n\nlogit(pj(y = 1|x)) = \u03b2j0 + xt \u03b2j,\nor one can assume that only intercepts are component-specific:\nlogit(pj(y = 1|x)) = \u03b2j0 + xt \u03b2.\n\nthe latter is a random intercept model that uses finite mixtures. in particular, the case where\nonly intercepts are component-specific yields a flexible model with a straightforward interpre-\ntation. let the means be specified by \u03bcj = h(\u03b2j0 + xt \u03b2) and \u03c32\nj denote the variance of the\njth component. since the mean and variance of the response are given by\n\nm(cid:7)\n\nm(cid:7)\n\nm(cid:7)\n\n\u03bc =\n\n\u03c0j\u03bcj,\n\n\u03c32 =\n\n\u2212 \u03bc2 +\n\n\u03c0j\u03c32\nj\n\n\u03c0j\u03bc2\nj\n\nj=1\n\nj=1\n\nj=1\n\n(exercise 14.6), the mean structure may also be given as\n\n(cid:14)\n\n\u03bc = h0(xt \u03b2),\n\nj \u03c0jh(\u03b2j0 + \u03b7) is the response function with linear predictor \u03b7 = xt \u03b2.\nwhere h0(\u03b7) =\none obtains a model for which the link function is determined by the data. this is similar\nto the estimation of link functions considered in section 5.2 with basis functions given by\nh(\u03b210 + \u03b7), . . . , h(\u03b2m0 + \u03b7). the link \u03bc = h0(xt \u03b2) allows for easy interpretation of the\nparameters that has to refer to the estimated link function h0(.). moreover, the link function\nh0(.) automatically maps the predictor into the admissible range if h(.) does. if the model\ncontains more than one component and the variances of the components are equal, the model\nimplies a larger variance than for the single components, which makes it a tool for modeling\noverdispersion (see also section 5.3).\n\nmodels of that type are considered by follmann and lambert (1989), and aitkin (1999).\nfollmann and lambert (1989) investigated the identifiability of finite mixtures of binomial\nregression models and gave sufficient identifiability conditions for mixing at the binary and the\nbinomial level. gr\u00fcn and leisch (2008) consider identifiability for mixtures of multinomial\nlogit models.\nwhen modeling repeated measurements the mixture distribution {\u03c01, . . . , \u03c0m} replaces the\nassumption of a continuous distribution of random intercepts. with large m it can be seen as a\nnon-parametric approach to approximate the unknown mixture distribution of the parameters.\nmore general models in which the mixing distribution also depends on covariates are given by\nmclachlan and peel (2000).\n\nestimation\nfor given data yit|xit, i = 1, . . . , n, t = 1, . . . , ti, the log-likelihood to be maximized is\n\nl({\u03b2j, \u03c6j}) =\n\nlog(\n\n\u03c0jfj(yi\n\n|xi)),\n\nn(cid:7)\n\nm(cid:7)\n\ni=1\n\nj=1\n\n "}, {"Page_number": 436, "text": "ti\n\n424\n\n|xi) =\n\nchapter 14. random effects models and finite mixtures\n\n2\nt=1 fj(yit|xit, \u03b2j, \u03c6j). direct maximization is tedious; therefore indi-\nwhere fj(yi\nrect maximization based on the em algorithm is often used. when using the em algorithm\n(appendix b) one distiguishes between the observable data and the unobservable data. in the\ni = (yi1, . . . , yiti) are observable whereas the mixture component\ncase of finite mixtures, yt\nis unobservable. let the latter be given by 0-1 variables zi1, . . . , zim, where zij = 1 denotes\nthat the ith observation is from the jth mixture component. by collecting all parameters in the\nparameter \u03b8 one has\n\n|zir = 1, xi, \u03b8) = fr(yi\n\n|xi, \u03b2r) =\n\nf(yi\n\nfj(yi\n\n|xi, \u03b2j)zij .\n\ni = (zi1, . . . , zim) is multinomially distributed with probability vector \u03c0t = (\u03c01, . . . ,\n\nsince zt\n\u03c0m) the complete density for yi, zi is\n\nm(cid:15)\n\nj=1\n\nm(cid:15)\n\nm(cid:15)\n\nfj(yi\n\n|zi, xi, \u03b2j)zij\n\n\u03c0zij\nj\n\nj=1\n\nj=1\n\nf(yi, zi|xi, \u03b8) = f(yi\n\nyielding the complete log-likelihood\n\nn(cid:7)\n\n|zi, xi, \u03b8)f(zi|\u03b8) =\nm(cid:7)\n\nn(cid:7)\n\nlc(\u03b8) =\n\nlog(f(yi, zi|xi, \u03b8)) =\n\nzij(log(\u03c0j) + log(fj(yi\n\n|zi, xi, \u03b2j)).\n\ni=1\n\ni=1\n\nj=1\n\nwithin the iterative em algorithm an estimate \u03b8(s) is improved by computing the expectation\nm(\u03b8|\u03b8(s)) = e lc(\u03b8) with respect to the conditional density f(z|y, \u03b8(s)). with\n\nf(zir = 1|yi, xi) = f(yi\n\none obtains\n\nm(\u03b8|\u03b8(s)) =\n\nj fj(yi\n\nij = \u03c0(s)\n\nwhere \u03c0(s)\nj )/(\nputes in the e-step the weights \u03c0(s)\nnew estimates\n\n|xi, \u03b2(s)\nn(cid:7)\n\n\u03c0(s+1)\nj\n\n=\n\n1\nn\n\n|xi)\n\n|xi)/f(yi\n\n|xi) = \u03c0rfr(yi\n\n|zir = 1, xi)f(zir = 1)/f(yi\nn(cid:7)\n\n\u03c0(s)\nij (log(\u03c0j) + log(fj(yi\n\nm(cid:7)\n(cid:14)\nr )). thus, for given \u03b8(s) one com-\nij and in the m-step maximizes m(\u03b8|\u03b8(s)), which yields the\n\n|zi, xi, \u03b2j)),\n\nr=1 \u03c0(s)\n\nr fr(yi\n\nj=1\n\ni=1\n\nm\n\n|xi, \u03b2(s)\nn(cid:7)\n\n\u03c0(s)\nij\n\n\u03b2(s+1)\n\nj\n\n= argmax\u03b2j\n\n\u03c0(s)\nij log(fj(yi\n\ni=1\n\ni=1\n\n|xi, \u03b2j)).\n\nj\n\ncomputation of \u03b2(s+1)\ncan be based on familiar maximization tools because one maximizes a\nweighted log-likelihood with known weights. in the case where only intercepts are component-\nspecific, the derivatives are very similar to the score function used in a gauss-hermite quadra-\nture and a similar em algorithm applies with an additional calculation of the mixing distribution\n{\u03c01, . . . , \u03c0m} (see aitkin, 1999).\n\nmaximization is more difficult in the general case, where one has to distinguish carefully\nbetween fixed effects, which do not depend on the mixture component, and component-specific\neffects. then part of the variables have fixed effects and part have component-specific effects.\nin the case of repeated measurements the component can also be linked to the units or clusters.\nlet c = {1, . . . , n} denote the set of units that are observed. then one specifies in a random\nintercept model for the jth component\n\nlogit(pj(yit = 1|xit)) = \u03b2j(i) + xt\n\nit\u03b2,\n\n "}, {"Page_number": 437, "text": "14.8. finite mixture models\n\n425\n\nwhere \u03b2j(i) denotes that component membership is fixed for each unit, that is, \u03b2j(i) = \u03b2j for\nall i \u2208 cj, where c1, . . . , cm is a disjunct partition of c. therefore the units are clustered into\nsubsets with identical intercepts. gr\u00fcn and leisch (2007) describe how to use their r package\nflexmix and give various applications. an extensive treatment of mixture models is given by\nfr\u00fchwirth-schnatter (2006).\n\nexample 14.7: beta-blockers\nthe dataset is from a 22-center clinical trial of beta-blockers for reducing mortality after myocardial in-\nfarction (see also aitkin, 1999; mclachlan and peel, 2000; gr\u00fcn and leisch, 2007). in addition to the\ncenters there is only one explanatory variable, treatment, coded as 0 for control and 1 for beta-blocker\ntreatment. therefore one has 44 binomial observations. the estimated treatment effects for various mod-\nels are given in table 14.3. fitting of a simple logit model with treatment as a fixed effect and ignoring\nthe effects of the center (glm treatment) yields a deviance of 305.76 with 42 df, which reduces to 23.62\nwith 21 df if the center is included as a factor (glm treatment + center). therefore, some heterogeneity\nacross hospitals is to be suspected. a gauss-hermite quadrature yields a comparable effect that is quite\nstable over quadrature points (glmm gh(quadrature points)). estimates for 4 and 20 quadrature points\nare given in table 14.3. for the standard deviation one obtains in both cases 0.513, with standard error\n0.085. the fitting of discrete mixtures uses an intercept that is linked to the centers. the estimates are\ngiven for three and four components, which have bic 341.42 and 346.76, respectively. therefore, in\nterms of bic, three components are to be preferred. discrete mixture models yield about the same treat-\nment effect as the other procedures. for the discrete mixture model with three components, the estimated\ncomponent weights were 0.512, 0.249, 0.239 with corresponding sizes 24, 10, 10. table 14.4 shows the\nestimates.\n\ntable 14.3: estimates and standard errors for several logit models fitted to beta-blocker\ndata.\n\nglm treatment\nglm treatment + center\nglmm gh(4) treatment\nglmm gh(20) treatment\ndiscrete(3)\ndiscrete(4)\n\ntreatment effect\n\u22120.257\n\u22120.261\n\u22120.252\n\u22120.252\n\u22120.258\n\u22120.260\n\nstd. error\n\n0.049\n0.049\n0.057\n0.057\n0.049\n0.049\n\ntable 14.4: estimates and standard errors for mixture model with three components\nfitted to beta-blocker data.\n\ntreatment\nintercept comp 1\nintercept comp 2\nintercept comp 3\n\nestimate\n\u22120.258\n\u22122.250\n\u22121.609\n\u22122.833\n\nstd. error\n\n0.049\n0.040\n0.055\n0.075\n\nz-value\n\u22125.17\n\u221255.52\n\u221228.88\n\u221237.74\n\npr(>|z|)\n\n0.0\n0.0\n0.0\n0.0\n\nexample 14.8: knee injuries\nfor the knee injuries study with dichotomized responses (example 14.3) discrete mixture models with\nrandom intercepts were fitted with the intercepts being linked to the subjects. it turns out that bic is the\nsmallest for the two component mixture models for which estimates are given in table 14.5. comparison\n\n "}, {"Page_number": 438, "text": "426\n\nchapter 14. random effects models and finite mixtures\n\nto table 14.1 shows that the significant coefficients are quite similar to the coefficients obtained for a\ngauss-hermite quadrature.\n\ntable 14.5: knee data binary, discrete mixture (2).\n\ncoef\n\u22121.841\nfactor(th)2\nfactor(sex)1 \u22120.101\nage\n0.020\n\u22120.006\nage2\n\npr(>|z|)\n\n0.0003\n0.798\n0.330\n0.000\n\nfor repeated measurements, finite mixture models provide a non-parametric alternative to\nrandom effects models with a specified distribution of random effects. frequently only a few\ncomponents are needed, and the estimates of explanatory variables do not change much when\nthe number of components is increased. an advantage of non-parametric approaches is that one\ndoes not have to choose a fixed distribution, a choice that might affect the estimates (for refer-\nences on misspecification see section 14.9). however, when random effects are multivariate,\nrandom effects models with normally distributed effects have the advantage that the distribution\nis described by relatively few (correlation) parameters.\n\n14.9 further reading\nlinear and generalized linear mixed models. detailed expositions of linear mixed models are\nfound in hsiao (1986), lindsey (1993), and jones (1993). a book with many applications and\nreferences to the use of sas as software is verbeke and molenberghs (2000). longford (1993)\nand mcculloch and searle (2001) discuss linear as well as generalized mixed models. general\ntreatments of longitudinal data including mixed models were given by diggle et al. (2002) and\nmolenberghs and verbeke (2005).\n\nrandom effects distribution. misspecification of the model for the random effects can\naffect the likelihood-based inference. although earlier papers found that ml estimates are not\nseverely biased (neuhaus, hauck, and kalbfleisch, 1992), more recently it has been shown\nthat substantial bias can result; see heagerty and kurland (2001), agresti et al. (2004), and\nneuhaus et al.\n(1992). non-parametric approaches were proposed by chen and davidian\n(2002) and magder and zeger (1996). huang (2009) proposed diagnostic methods for random-\neffect misspecification. claeskens and hart (2009) proposed tests for the assumption of the\nnormal distribution.\n\nvariable selection in random effects and mixture models. for the selection of variables\nin random effects models l1-penalty terms can be included in the marginal likelihood or the\npenalized quasi-likelihood. for linear mixed models procedures of that type were proposed\nby ni et al. (2010). alternatively, one can use boosting techniques; see tutz and reithinger\n(2007), tutz and groll (2010a), tutz and groll (2010b). groll and tutz (2011b) used lasso\ntype penalties in generalized linear mixed models; ibrahim, zhu, garcia, and guo (2011) also\ninclude the selection of random effects. for mixture models khalili and chen (2007) proposed\npenalty-driven selection procedures that select single coefficients.\n\nr packages. the package glmmml allows one to fit glms with random intercepts by\nmaximum likelihood and numerical integration via a gauss-hermite quadrature. estimation\nof random effects models with ordinal responses is performed by the function clmm from the\npackage ordinal. the function glmmpql from the package mass fits glmms, using the\npenalized quasi-likelihood. the package flexmix provides procedures for fitting finite mixture\n\n "}, {"Page_number": 439, "text": "14.10. exercises\n\n427\n\nmodels including binomial, poisson, and gaussian mixtures. the function glmer from the\npackage lme4 fits by using the adaptive gauss-hermite approximation proposed by liu and\npierce (1994). the versatile package mgcv contains the function gamm, which allows one to fit\nglmms that contain smooth functions.\n\n14.10 exercises\n\n14.1 the gaussian random effects model for clustered data is given by yi = x i\u03b2 + z ibi + \u03b5i, where\nbi \u223c n (0, q), \u03b5i \u223c n (0, \u03c32i) and bi and \u03b5i are independent. derive the distribution of \u03b5\n\u2217\ni in the\nmarginal representation yi = x i\u03b2 + \u03b5\n\ni \u223c n (0, v i).\n\u2217\n14.2 the r package flexmix provides the dataset betablocker.\n\n\u2217\ni , \u03b5\n\n(a) use descriptive tools to learn about the data.\n(b) fit a glmm by using a gauss-hermit quadrature. choose an appropriate number of quadrature\n\npoints.\n\n(c) fit a glmm by using a laplace approximation and compare to (b).\n(d) fit a discrete mixture model with random intercepts linked to centers. choose an appropriate\n\nnumber of mixing components.\n\n(e) fit a discrete mixture model with random intercepts and treatment effect linked to centers. choose\n\nan appropriate number of mixing components and compare with (d).\n\n14.3 consider the sequential mixed model with category-specific random intercepts\n\np (yit = r|yit \u2265 r, x) = f (\u03b30r + bir + xt\n\nit\u03b3),\n\n, r = 1, . . . , k \u2212 1,\n\nwhere bt\n\ni = (bi1, . . . , bi,k\u22121) is normally distributed.\n\n(a) show how the model can be fitted by using binary mixed model methodology (see also section\n\n9.5.2).\n\n(b) fit a sequential model with random intercepts for the recovery score data (example 14.2).\n(c) fit a cumulative model with random intercepts for the recovery score data and compare with the\n\nresults from (b).\n\n14.4 consider the cumulative model from example 14.2. if estimation becomes unstable, also consider a\nrougher response that is obtained by collapsing of the categories.\n\n(a) fit cumulative models for the separate measurements.\n(b) fit a glmm by gauss-hermite integration and penalized quasi-likelihood methods.\n(c) use in the glmm a linear trend over repetitions instead of dummy variables.\n\n14.5 consider again the epilepsy data from example 13.1 (available at http://biosun1.harvard.edu/ fitz-\nmaur/ala).\n\n(a) fit a model with random intercepts and investigate the effect of treatment.\n(b) compare the estimates to the marginal model fits from exercise 13.4.\n\n(cid:2)\n\n(a) show that the mean and variance of the mixture are given by \u03bc =\n\n14.6 let a finite mixture model be given by f (y) =\nvariance of the jth component.\n\nm\n\nj=1 \u03c0jfj(y), and let \u03bcj, \u03c32\n\nj denote the mean and\nj \u2212\nj . (the variance can be computed directly or by using the variance decomposition.)\nj . is the\n\nj=1 \u03c0j\u03bcj, \u03c32 =\n\nj \u03c0j\u03c32\n\n(cid:2)\n\n(cid:2)\n\n(cid:2)\n\nm\n\n(b) show that the variance of the mixture is at least as large as the mean variance\n\nvariance of the mixture also larger than the variance of any single component, that is, \u03c32 \u2265 \u03c32\nj ?\n\nj \u03c0j\u03c32\n\n\u03bc2 +\n\nj \u03c0j\u03bc2\n\n(cid:2)\n\n "}, {"Page_number": 440, "text": " "}, {"Page_number": 441, "text": "chapter 15\n\nprediction and classification\n\nin prediction problems one considers a new observation (y, x). while the predictor value x is\nobserved, y is unknown and is to be predicted. in general, the unknown y may be from any\ndistribution, continuous or discrete, depending on the prediction problem. when the unknown\nvalue is categorical we will often denote it by y , with y taking values from {1, . . . , k}. then\nprediction means to find the true underlying value from the set {1, . . . , k}. the problem is\nstrongly related to the common classification problem where one wants to find the true class\nfrom which the observation stems. when the numbers 1, . . . , k denote the underlying classes,\nthe classification problem has the same structure as the prediction problem. classification prob-\nlems are basically diagnostic problems. in medical applications one wants to identify the type\nof disease, in pattern recognition one might aim at recognizing handwritten characters, and in\ncredit scoring (example 1.7) one wants to identify risk clients. sometimes the distinction be-\ntween prediction and classification is philosophical. in credit scoring, where one wants to find\nout if a client is a risk client, one might argue that it is a prediction problem since the classifica-\ntion lies in the future. nevertheless, it is mostly seen as a classification problem, implying that\nthe client is already a risk client or not. the following example illustrates the uncertainty of\nclassification rules by giving the true distribution of a simple indicator given the class. usually\nthe distribution is not known and one has to start from data (example 15.2).\n\nexample 15.1: drug use\nin some companies it is not unusual that applicants for a job are tested for drug use. marylin vos savant,\nknown as the person with the highest iq, discussed in the gainesville sun the use of a diagnostic test that\nis .95 accurate if the person is a user or a non-user. the probabilities of test results given the class are\ngiven in the following table.\n\ntest positive\n\ntest negative\n\nuser\nnon-user\n\n0.95\n0.05\n\n0.05\n0.95\n\ntests are usually not 100% reliable. one distinguishes between sensitivity, which is the probability\nof a positive test, given the signal (user) is present, and specificity, which is the probability of a negative\ntest, given no signal (non-user) is present. in the table above sensitivity as well as specificity are .95. if\na test like that is used to infer on the drug use of a person selected at random, the question is how well it\nperforms.\n\nexample 15.2: glass identification\nthe identification of types of glass can be very important in criminological investigations since at the\n\n429\n\n "}, {"Page_number": 442, "text": "430\n\nchapter 15. prediction and classification\n\nscene of the crime, the glass left can be used as evidence. a dataset coming from usa forensic science\nservice distinguishes between seven types of glass (four types of window glass and three types of non-\nwindow); predictors are the refractive index and the oxide content of various minerals like na, fe, and\nk. the data have been used by ripley (1996) and others. it is available from the uci machine learning\nrepository.\n\nin the last decade in particular, the analysis of genetic data has become an interesting field\nof application for classification techniques. for example, gene expression data may be used\nto distinguish between tumor classes and to predict responses to treatment (e.g., golub et al.,\n1999b). the challenge in gene expression data is in the dimension of the datasets. with about\n30, 000 genes in the human genome, the data to be analyzed have the unusual feature that the\nnumber of variables (genes) is much higher than the number of cases (tumor samples), which\nis referred to as the \"p > n\" case. standard classification procedures fail in the \"p > n\" case.\ntherefore, one of the main goals with this type of data is variable selection, which will be\nconsidered in a separate section.\n\nclassification methods run under several names; they are also referred to as pattern recog-\nnition in technical applications, discriminant analysis in statistics, and supervised learning in\nthe machine learning community. statistical approaches are well covered in mclachlan (1992)\nand ripley (1996). ripley also includes methods developed in the machine learning commu-\nnity. bishop (2006) is guided by pattern recognition and machine learning and hastie et al.\n(2009) consider regression and classification from a general statistical learning viewpoint.\n\nthe main objective of prediction is high accuracy, where of course accuracy has to be\ndefined, for example, by the use of loss functions. in some applications, for instance, in recog-\nnizing handwritten characters, accuracy is indeed prevailing. in other fields, like credit scoring,\nit is often found unsatisfying to use a black box for prediction purposes. users prefer to know\nwhich variables, in what form, determine the prediction, and in particular how trustworthy the\nprediction is. thus, users often favor simple rules in prediction, for instance, linear discrimi-\nnation, which captures the importance of predictors by parameters. what users usually want is\na combination; they want a simple model of the underlying structure that in addition has good\nprediction properties. a simple model will usually yield good prediction performance only if it\nis a fair approximation of the underlying structure. if that structure is complicated, no simple\nmodel will provide a good approximation and performance will be poor. then, approaches\nthat are highly efficient but do not identify an interpretable structure should be chosen. in the\nfollowing we will consider several simple prediction rules that are based on statistical mod-\nels and therefore are user friendly but also non-parametric approaches that do not imply an\neasy-to-interpret structure but also show excellent performance for complicated distributions.\n\nwe will first consider the basic concepts of prediction that apply in general prediction prob-\nlems. if one is interested in particular in classification rules, one can also skip the next section\nand start with section 15.2.\n\n15.1 basic concepts of prediction\nlet a new observation (y, x) be randomly drawn from the distribution. while x is observed,\ny is unknown and is to be predicted. the prediction is denoted by \u02c6y(x), which is based on a\nmapping \u02c6y : x (cid:24)\u2192 \u02c6y(x). let the performance of a prediction rule be measured by a loss function\nl(y, \u02c6y(x)). classical losses for metric responses are the squared loss l2(y, \u02c6y) = (y \u2212 \u02c6y)2 and\nthe l1 norm l1(y, \u02c6y) = |y \u2212 \u02c6y|. it is useful to distinguish between two cases: the case where\nthe prediction rule \u02c6y is considered as given and the case where \u02c6y is an estimated rule. we will\nfirst consider fixed rules and how they may be chosen optimally.\n\n "}, {"Page_number": 443, "text": "15.1. basic concepts of prediction\n\n431\n\nfor randomly drawn (y, x) the actual prediction error is given by\n\n)\n\ney,x l(y, \u02c6y(x)) = ex ey|x l(y, \u02c6y(x)) =\n\ney|x l(y, \u02c6y(x))f(x)dx,\n\n(15.1)\n\nwhere f(x) is the marginal distribution of x. ey,x l(y, \u02c6y(x)) is a measure for the mean predic-\ntion error averaged across all possible predictors x. for x fixed it reduces to ey|x l(y, \u02c6y(x)).\n\noptimal prediction\nwhen minimization of the actual prediction error is considered as the criterion for choosing\n\u02c6y(x), one may ask what the optimal prediction rule is. the answer, of course, depends on the\nloss function. since it suffices to minimize the conditional loss, given x, the optimal prediction\nat value x is determined by\n\nfor the quadratic loss one obtains as a minimizer\n\n\u02c6yopt(x) = argmincl(y, c).\n\n\u02c6yopt(x) = e(y|x),\n\nwhich is also called a regression function. when using the l1-norm one obtains the median\n\n\u02c6yopt(x) = med(y|x).\n\nthe loss function lp(y, \u02c6y) = |y \u2212 \u02c6y| + (2p \u2212 1)(y \u2212 \u02c6y), with p \u2208 (0, 1), yields the p-quantile\n\n\u02c6yopt(x) = inf{y0|p (y \u2264 y0|x) \u2265 p},\n\nalso known as a quantile regression. therefore, the loss determines the functional of the con-\nditional distribution y|x that is specified. the mean, the median, and the quantile represent\ndifferent forms of regression.\n\nestimated prediction rule\nmost often \u02c6y(x) is based on a fitted model. for example, one fits a model for the conditional\nmean \u03bc(x) = e(y|x) = h(x, \u03b8) and then uses the estimated mean at x, \u02c6\u03bc(x) = h(x, \u02c6\u03b8), for\nprediction. then the actual prediction error is a random variable that depends on the sample\ns = {(yi, xi), i = 1, . . . , n} that was used to obtain the estimate. the dependence on the\nsample can be made explicit by the denotation \u02c6ys(x) = \u02c6\u03bc(x). different models should be\ncompared by the mean actual prediction error, also known as the test or generalization error:\n\ne ey,x l(y, \u02c6ys(x)),\n\nwhere e denotes the expectation over the learning sample. the mean actual prediction error\nalso reflects the uncertainty, which stems from the estimation of the prediction rule. thus it\nrefers to the uncertainty of the total system and not only to the data at hand. for fixed x the\ncorresponding generalization error is e ey|x l(y, \u02c6ys(x)).\n\nhaving fitted a model, one also wants to estimate its performance in future samples. simple\nestimates that apply the predictor retrospectively to the sample, obtaining the so-called apparent\nerror rate, tend to be overly optimistic and have to be bias corrected.\n\n "}, {"Page_number": 444, "text": "432\n\nchapter 15. prediction and classification\n\n15.1.1 squared error loss\nin the following we will first consider basic concepts in the well-investigated case of a squared\nerror loss. for a squared error the predictive mean squared error (pmse; actual prediction\nerror) of the prediction \u02c6\u03bc(x) is given by\n\npmse = ey,x(y \u2212 \u02c6\u03bc(x))2 = ex ey|x(y \u2212 \u03bc(x))2 + ex ey|x(\u03bc(x) \u2212 \u02c6\u03bc(x))2\n\n= ex \u03c32\n\ny|x + ex(\u03bc(x) \u2212 \u02c6\u03bc(x))2,\n\ny|x = ey|x(y \u2212 \u03bc(x))2 is the conditional variance of y|x. for fixed x pmse reduces\nwhere \u03c32\ny|x + (\u03bc(x) \u2212 \u02c6\u03bc(x))2. it is seen that the precision of the prediction at\nto pmse(x) = \u03c32\nx depends on the variance at x and the precision of the (model-based) estimate \u02c6\u03bc(x). the\nlatter term depends on the sample; better estimates will result in a smaller term. in contrast,\nthe conditional variance does not depend on the sample and reflects the basic variability of y|x,\nwhich cannot be reduced by increasing the sample size or using better estimates.\n\nlet us consider a familiar example, namely, a prediction within the classical linear model.\ni \u03b2 + \u0001i, \u0001i \u223c n(0, \u03c32) one obtains\n\nunder the assumption of the classical linear model yi = xt\n\npmse(x) = \u03c32 + (\u03bc(x) \u2212 xt \u02c6\u03b2)2,\n\nwhere \u03bc(x) = xt \u03b2. let \u02c6\u03b2 denote the least-squares estimate. if the classical linear model\nis the operating model, one obtains, conditionally on the observations x1, . . . , xn, the mean\nactual prediction error:\n\ne pmse(x) = \u03c32 + e(xt \u03b2 \u2212 xt \u02c6\u03b2)2 = \u03c32 + e(xt (\u02c6\u03b2 \u2212 \u03b2)(\u02c6\u03b2 \u2212 \u03b2)t x)\n\n= \u03c32(1 + xt (x t x)\n\n\u22121x).\n\nwhile the uncertainty contained in \u03c32 is fixed by the underlying model, the uncertainty of the\nprediction depends on the predictor value x as well as on the design matrix. the actual predic-\nton error pmse(x) is a random variable, since \u02c6\u03b2 is. in contrast, the mean actual prediction is\nnot random and includes the variability due to the random responses in the learning sample.\nfor the assessment of prediction accuracy let the model be yi = \u03bci + \u03b5i, \u03b5i \u223c n(0, \u03c32\n\ni ). a\nsimple estimate of the generalization error uses the fit retrospectively on the data that generated\nthe fit, yielding the apparent error rate:\n\nn(cid:7)\n(yi \u2212 \u02c6\u03bci)2.\n\ni=1\n\neapp =\n\n1\nn\n\nfor the linear model with homogeneous variances it is well known that eapp is a biased estimate\nof the variance. correcting for the number of estimated parameters, p, yields the unbiased\nestimate {n/(n \u2212 p)}eapp.\n\ni = (y1, . . . , yn) collect the observations at predictor values x1, . . . , xn and let yt\n\nthe apparent error rate estimates the error at the predictor values given in the sample.\n0 =\nlet yt\n(y01, . . . , y0n) denote a vector of new, independent observations, where y0i is taken at predictor\nvalue xi. then ei = (yi \u2212 \u02c6\u03bci)2 may be seen as an estimate of e e0(y0i \u2212 \u02c6\u03bci)2, where e0 refers\nto the expectation of y0i and e to the expectation over the sample. for the mean at xi one\nobtains\n(15.2)\nwhere one uses e(yi\u2212 \u03bci)2 = e0(y0i \u2212 \u03bci)2, yielding e e0(y0i \u2212 \u02c6\u03bci)2 = e(yi \u2212 \u03bci)2 + e(\u03bci \u2212\n\u02c6\u03bci)2 and the expectations in the decomposition\n\ne e0(y0i \u2212 \u02c6\u03bci)2 = e{(yi \u2212 \u02c6\u03bci)2 + 2 cov(\u02c6\u03bci, yi)},\n\n(yi \u2212 \u02c6\u03bci)2 = (yi \u2212 \u03bci)2 + (\u02c6\u03bci \u2212 \u03bci)2 \u2212 2(yi \u2212 \u03bci)(\u02c6\u03bci \u2212 \u03bci)\n\n "}, {"Page_number": 445, "text": "15.1. basic concepts of prediction\n\n433\n(compare efron, 2004). thus the apparent error term (yi \u2212 \u02c6\u03bci)2 has to be corrected by the\ncovariance between the estimate at xi and the observation at xi.\n\nfor a homogeneous covariance and a linear estimate \u02c6\u03bc = m y one obtains from (15.2)\n\nn(cid:7)\n\ni=1\n\ne(\n\ne0(y0i \u2212 \u02c6\u03bci)2) = e(neapp + 2\u03c32 trace (m)).\n\nthe right-hand side corresponds to mallow\u2019s cp , which may be given as\n\nn(cid:7)\n\ncp =\n\n(yi \u2212 \u02c6\u03bci)2 + 2\u02c6\u03c32 trace (m),\n\ni=1\n\nwhere for a linear model with p parameters one has trace (m) = p and \u03c32 has to be estimated.\nthe term 2\u03c32 trace (m) may be considered the optimism of the apparent error rate. in general,\none has to add the (unobservable) term \u03c3i2 cov(\u02c6\u03bci, yi) to the apparent error term to obtain an\nunbiased estimate. efron (2004) calls the additional term a covariance penalty.\n\nin the gaussian case, the cov(\u02c6\u03bci, yi) has the form \u03c32 e(\u2202 \u02c6\u03bci/\u2202yi), yielding stein\u2019s un-\nbiased risk estimate (sure) for the total prediction error, which uses the correction term\n2\u03c32\u03c3i\u2202 \u02c6\u03bci/\u2202yi (stein, 1981). an alternative way to estimate the covariance cov(\u02c6\u03bci, yi) is\nbased on bootstrap techniques (see efron, 2004; ye, 1998).\n\nthe penalty correction term may also be used to define the degrees of freedom of a fit.\nfor the linear model with fit \u02c6\u03bc = m y, where m is a projection matrix, trace (m) is the\ndimension of the projected space. more generally, one may consider the degrees of freedom\n\nn(cid:7)\n\ndf =\n\ncov(\u02c6\u03bci, yi)/\u03c32\n\n(compare ye, 1998).\n\ni=1\n\npredicting a distribution\nfrom a more general point of view, if x is observed, prediction may be understood as finding\nthe density of y given x, \u02c6f(y|x). the prediction error then is measured as a discrepancy\nbetween the true density f(. |x) and the estimated density \u02c6f(. |x). a measure that applies is\nthe kullback-leibler discrepancy:\n\nkl(f(. |x), \u02c6f(. |x)) = ef log(f(. |x)/ \u02c6f(. |x)),\n\nwhere expectation is based on the underlying density f. if one assumes that the underlying\ndensity is a normal distribution n(\u03bc(x), \u03c32) and the estimated density is n(\u02c6\u03bc(x), \u02c6\u03c32), it is\neasily shown that one obtains\n\nkl(f, \u02c6f) = {\u03c32 + (\u03bc(x) \u2212 \u02c6\u03bc(x))2}/(2\u02c6\u03c32) + log\n\n\u02c6\u03c3\n\u03c3\n\n\u2212 1\n2 ,\n\nwhich, for given \u02c6\u03c3, is a scaled version of pmse(x) = \u03c32 + (\u03bc(x)\u2212 \u02c6\u03bc(x))2. thus, for normal\ndistributions, the kullback-leibler discrepancy is strongly linked to the mean squared error.\n\n15.1.2 discrete data\nin discrete data let the new observation be given by (y, x), with y taking values from {1, . . . , k}.\nwhen x is observed, prediction is often understood as finding a prediction rule \u02c6y = \u02c6y (x) with\n\n "}, {"Page_number": 446, "text": "chapter 15. prediction and classification\n\n434\n\u02c6y taking values from {1, . . . , k}. prediction in this sense is often called classification since\none wants to find the true underlying class. y is the indicator for the underlying class and \u02c6y\nis the diagnosis. in the following, finding the point prediction \u02c6y will be referred to as a direct\nprediction or classification.\nwhen a statistical model is used for prediction one often first estimates the probability\np (y = r|x) and then derives a classification rule from these probabilities. but then more\ninformation is available than a simple assignment to classes. the conditional probabilities\ncontain additional information about the precision of the classification rule derived from it.\ntherefore, in a more general sense one might view the estimation of the conditional distribution\ngiven x, which is identical to the estimated probabilities p (y = r|x), r = 1, . . . , k, as the\nprediction rule. depending on the type of prediction one has in mind, different loss functions\napply. in the following we will first consider loss functions for direct predictions and then loss\nfunctions for the case where the prediction is based on an estimate of the underlying probability.\nsince the actual or expected loss is given by (15.1), it again suffices to minimize the conditional\nloss given x. therefore, in the notation, for the most part, x is dropped.\n\ndirect prediction\nloss functions for direct predictions come in two forms. one may use y and \u02c6y as arguments\nor one may refer to the multivariate nature of the response by using vector-valued arguments.\nthe latter form is often useful to see the connection to common loss functions like the quadratic\nloss.\nlet y \u2208 {1, . . . , k} denote the categorical response and yt = (y1, . . . , yk) denote the\ncorresponding vector-valued response, where yi = 1 if y = i and yi = 0 otherwise. let (y, x)\nwith yt = (y1, . . . , yk) denote a new observation and \u02c6yt = (\u02c6y1, . . . , \u02c6yk) the prediction for the\ngiven value x, where \u02c6yr \u2208 {0, 1} and\n\n(cid:14)\n\none may again consider functions like the quadratic or l1 loss, which are in common use\nfor continuous response variables. however, for categorical responses with observations and\ndirect predictions as arguments, the quadratic loss and the l1-norm loss\n\nr \u02c6yr = 1.\n\nl2(y, \u02c6y) =\n\n(yr \u2212 \u02c6yr)2, l1(y, \u02c6y) =\n\n|yr \u2212 \u02c6yr|\n\nreduce to twice the simple 0-1 loss 2l01(y, \u02c6y), where\n\nl01 has value 0 if the prediction is correct and 1 if it is incorrect. with arguments y, \u02c6y \u2208\n{1, . . . , k} the 0-1 loss has the form\n\nk(cid:7)\n\nr=1\n\nk(cid:7)\n\nr=1\n\n(cid:2)\n\n1 y (cid:8)= \u02c6y\n0 y = \u02c6y.\n\n(cid:2)\n0 y = \u02c6y\n1 y (cid:8)= \u02c6y .\n\nl01(y, \u02c6y) =\n\nl01(y, \u02c6y ) =\n\nthe 0-1 function plays a central role in classification since the corresponding actual prediction\nerror at a fixed value x is given by\n\ne l01(y, \u02c6y) = p (y (cid:8)= \u02c6y|x) = 1 \u2212 p (y = \u02c6y|x),\n\nwhich is the actual probability of misclassification. the optimal prediction rule obtained by\nminimizing e l01(y, \u02c6y) is known as the bayes classifier and is given by\np (y = i|x).\n\nif p (y = r|x) = max\n\n\u02c6y (x) = r\n\ni=1,...,k\n\n "}, {"Page_number": 447, "text": "15.1. basic concepts of prediction\n\n435\n\nin the case of two classes it reduces to the intuitively appealing rule\n\u02c6y (x) = 1 if p (y = 1|x) \u2265 0.5.\n\nthe bayes classifier minimizes the probability of misclassification by minimizing p (y (cid:8)= \u02c6y|x).\nit is the central issue of section 15.2, where classification methods that minimize the probability\nof misclassification are considered extensively. the bayes rule results from minimizing the\nexpected 0-1 loss. therefore, in direct predictions all loss functions that reduce to 0-1 loss\nwhen given in the form l01(y, \u02c6y) yield the bayes classifier. the reason is that minimization of\ne l01(y, \u02c6y) or e l01(y, \u02c6y ) is over discrete values, and in the latter form over \u02c6y \u2208 {1, . . . , k}.\nif minimization is over real-valued approximations of the true vector y, the quadratic loss or\nl1 loss will yield different expected losses and therefore different allocation rules. the effect\nis investigated in the next section.\n\nprediction based on estimated probabilities\nfor categorical data prediction may also be understood as finding the best real-valued approx-\nimation to the unknown y, which usually is an estimate \u02c6\u03c0t = (\u02c6\u03c01, . . . , \u02c6\u03c0k) of the underlying\nprobabilities. then the corresponding loss functions have the form l(y, \u02c6\u03c0). slightly more gen-\nerally, one can also consider the discrepancy between the true probability function represented\nby \u03c0t = (\u03c01, . . . , \u03c0k) and the estimated probability function \u02c6\u03c0t = (\u02c6\u03c01, . . . , \u02c6\u03c0k), and derive\nthe loss l(y, \u02c6\u03c0) from it. possible discrepancy measures are the squared loss:\n\nl2(\u03c0, \u02c6\u03c0) =\n\nl1(\u03c0, \u02c6\u03c0) =\n\n(\u03c0r \u2212 \u02c6\u03c0r)2,\n\n|\u03c0r \u2212 \u02c6\u03c0r|,\n\nr\n\n(cid:7)\n(cid:7)\n(cid:7)\n\nr\n\nthe l1-norm:\n\nand the kullback-leibler discrepancy:\n\nlkl(\u03c0, \u02c6\u03c0) =\n\n\u03c0r log(\u03c0r/\u02c6\u03c0r).\n\nr\n\n(cid:14)\nthe kullback-leibler discrepancy uses explicitly that the underlying distribution is the multi-\n(cid:14)\nnomial distribution with parameter \u03c0. further possible measures are the pearson chi-squared\nr(\u03c0r \u2212 \u02c6\u03c0r)2/\u02c6\u03c0r and the neyman-pearson discrepancy np(\u03c0, \u02c6\u03c0) =\ndiscrepancy p (\u03c0, \u02c6\u03c0) =\nr(\u03c0r \u2212 \u02c6\u03c0r)2/\u03c0r. when \u02c6\u03c0 is considered as an estimate of a new observation y, loss functions\nlike l2 and l1 do not reduce to the 0-1 loss. let us consider a new observation y \u2208 {1, . . . , k}\nor yt = (y1, . . . , yk), yr \u2208 {0, 1}, and the corresponding loss l(y, \u02c6\u03c0) and actual prediction\nerror e l(y, \u02c6\u03c0). for the quadratic loss one obtains the quadratic score, which is also known as\nbrier score:\n\n(cid:7)\n\nl2(y, \u02c6\u03c0) = (1 \u2212 \u02c6\u03c0y )2 +\n\n\u02c6\u03c02\nr .\n\nr(cid:5)=y\n\nin the literature on scoring rules usually the reward for estimating an outcome y by the distribu-\ntion \u02c6\u03c0 is measured and the corresponding brier score is considered to be \u2212l2(y, \u02c6\u03c0). however,\nhere the loss function perspective is preferred. when using the quadratic score one obtains for\nthe actual prediction error\n\n(cid:2)\n\nk(cid:7)\n(yr \u2212 \u02c6\u03c0r)2\n\n&\n\nk(cid:7)\n\ne(l2(y, \u02c6\u03c0)) = e\n\n= l2(\u03c0, \u02c6\u03c0) +\n\nvar(yr)\n\nr=1\n\nr=1\n\n(exercise 15.2). therefore, the prediction error is determined by the distance between the\ntrue probability \u03c0 and the estimate \u02c6\u03c0 and a term that depends on the true probability only.\ne(l2(y, \u03c0)) obtains its minimum value if \u03c0 = \u02c6\u03c0, yielding l2(\u03c0, \u02c6\u03c0) = 0.\n\n "}, {"Page_number": 448, "text": "436\n\nchapter 15. prediction and classification\n\nfor the kullback-leibler discrepancy one obtains the distance between a new observation\n\ny and its estimate \u02c6\u03c0 as\n\nlkl(y, \u02c6\u03c0) = \u2212 log(\u02c6\u03c0y ) = \u2212 k(cid:7)\n\nyr log(\u02c6\u03c0r),\n\nr=1\n\nwhich is called the logarithmic score. it is strongly linked to the deviance since for multinomial\ndistribution the deviance is twice the kullback-leibler distance. for example, for a binary\ndistributions the deviance in its usual form is given as d(y, \u02c6\u03c0) = \u22122 log(\u02c6\u03c0) if y = 1 and\nd(y, \u02c6\u03c0) = \u22122 log(1 \u2212 \u02c6\u03c0) if y = 0. therefore, for the vector yt = (y1, y2) encoding the two\nclasses one obtains d(y, \u02c6\u03c0) = 2lkl((y1, y2), (\u02c6\u03c01, \u02c6\u03c02)). the relation is the reason why, for\nmultinomial distributions, the logarithmic score when applied to new observations is also called\nthe predictive deviance. as for the squared loss, a decomposition into a term that is determined\nby the distance between the true probability \u03c0 and the estimate \u02c6\u03c0 and a term that does depend\non the true probability only can be found. one obtains e(lkl(y, \u02c6\u03c0)) = lkl(\u03c0, \u02c6\u03c0)+entr(\u03c0),\n\nwith entr(\u03c0) = \u2212(cid:14)\n\nk\n\nr=1 \u03c0r log(\u03c0r) denoting the entropy of probability vector \u03c0.\n\nthe link between a direct prediction and a prediction as an estimation of probabilities\nis found by considering how the estimation of probabilities turns into a prediction of the\nnew observation. usually the prediction of y or y is derived from \u02c6\u03c0 by setting \u02c6y = r if\n\u02c6\u03c0r = max\n\u02c6\u03c0i. uniqueness can be obtained by assigning an observation to the class with the\ni=1,...,k\nsmallest number if the estimated probabilities are equal. the maximization procedure can be\nincluded into the loss function by using\n\nlb(\u03c0, \u02c6\u03c0) =\n\n\u03c0r(1 \u2212 indr(\u02c6\u03c0)),\n\nk(cid:7)\n\nr=1\n\n\u23a7\u23a8\n\u23a91 \u03c0r = max\n\notherwise\n\n0\n\ni=1,...,k\n\n\u03c0i, \u03c0r > \u03c0i for i < r\n\nwhere\n\nindr(\u03c0) =\n\nis an indicator function for optimal classification. the corresponding loss,\n\nlb(y, \u02c6\u03c0) =\n\nyr(1 \u2212 indr(\u02c6\u03c0)) = indy (\u02c6\u03c0),\n\n(cid:7)\n\nis equivalent to a 0-1 loss, but built with argument \u02c6\u03c0. for the underlying function one obtains\n\nlb(\u03c0, \u02c6\u03c0) =\n\n\u03c0r(1 \u2212 indr(\u02c6\u03c0)) =\n\n\u03c0r = 1 \u2212 \u03c0 \u02c6y ,\n\nr=1\n\nr:\u02c6\u03c0r<maxi \u02c6\u03c0i\n\nwhich is the probability of class \u02c6y that has been determined as an estimated bayes classifier.\nsince e(lb(y, \u02c6\u03c0)) = lb(\u03c0, \u02c6\u03c0) holds, lb(\u03c0, \u02c6\u03c0) can be considered the expected loss or risk\nof the estimated bayes classifier.\n\nthere is a strong difference between using lb(y, \u02c6\u03c0) or a loss like the quadratic loss\nl2(y, \u02c6\u03c0). the expectation of the quadratic loss is minimal only if \u03c0 = \u02c6\u03c0, whereas mini-\nmization of the expectation of lb(y, \u02c6\u03c0) is also obtained for estimates \u02c6\u03c0 that have the same\nindicator function as \u03c0. while the quadratic loss aims at exact estimations of probabilities, 0-1\nloss ignores the precision of these estimates and focuses solely on misclassification. therefore,\n\nk(cid:7)\n\nr=1\n\nk(cid:7)\n\n "}, {"Page_number": 449, "text": "15.1. basic concepts of prediction\n\n437\n\nwhen prediction is based on estimated probabilities, the losses l(y, \u02c6\u03c0) may be used as empir-\nical measures for the precision of the predictions. for the quadratic loss one obtains the brier\nscore, and for the kullback-leibler loss the logarithmic score. both give more precise measures\nfor the accuracy of prediction than the simple 0-1 loss.\n\nwithin the framework of scoring rules one considers the reward of a prediction in the form\nof scores instead of loss functions. however, by using the negative values of scores one ob-\ntains loss functions. a rigorous treatment of score functions is given by gneiting and raftery\n(2007), who also consider alternative scoring rules like the spherical score and the beta fam-\nily score, which are proposed by buja et al. (2005). in particular, one distinguishes between\nproper, strictly proper, and improper scores. in terms of loss functions, a function is proper\nif e l\u03c0(y, \u03c0) \u2264 e l\u03c0(y, \u02dc\u03c0) for all distributions \u03c0, \u02dc\u03c0. it is strictly proper if that holds with\nequality if and only if \u03c0 = \u02dc\u03c0. brier scores and logarithmic scores are strictly proper but the\n0-1-loss is merely proper.\n\ntable 15.1: loss functions, scores, and actual prediction error.\n\n(cid:2)\n\nl2(\u03c0, \u02c6\u03c0) =\nl2(y, \u02c6\u03c0) = (1 \u2212 \u02c6\u03c0y )2 +\n\nk\n\nr=1(\u03c0r \u2212 \u02c6\u03c0r)2\n(cid:2)\nr(cid:4)=y \u02c6\u03c02\n(cid:2)\nr\n\ne(l2(y, \u02c6\u03c0)) = l2(\u03c0, \u02c6\u03c0) +\n\n(cid:2)\n\nk\n\nr=1 \u03c0r(1 \u2212 \u03c0r)\nr=1 \u03c0r(1 \u2212 indr( \u02c6\u03c0)) = \u03b5(x)\n\nk\n\nlb(\u03c0, \u02c6\u03c0) =\nlb(y, \u02c6\u03c0) = 1 \u2212 indy ( \u02c6\u03c0) = 1\n\n(cid:2)\n\n2\n\n|yr \u2212 indr( \u02c6\u03c0)|\n\nk\nr=1\n\ne(lb(y, \u02c6\u03c0)) = lb(\u03c0, \u02c6\u03c0)\n\n(cid:2)\n\nlkl(\u03c0, \u02c6\u03c0) =\n\nk\nlkl(y, \u02c6\u03c0) = \u2212 log(\u02c6\u03c0y ) = \u2212(cid:2)\nr=1 \u03c0r log(\u03c0r/\u02c6\u03c0r)\nk\n(cid:2)\nr=1 yr log(\u02c6\u03c0r)\n\ne(lkl(y, \u02c6\u03c0)) = lkl(\u03c0, \u02c6\u03c0) +\n\n\u2212\u03c0r log(\u03c0r)\n\nk\nr=1\n\nquadratic loss\n\nquadratic or brier score\n\nactual prediction error\n\nbayes loss\n\n0\u20131-loss\n\nactual prediction error\n\nkullback-leibler discrepancy\n\nlogarithmic score\n\nactual prediction error\n\na direct prediction of a class in the form \u02c6yt = (\u02c6y1, . . . , \u02c6yk) if yt = (y1, . . . , yk) is\nobserved may be seen as a special case of a degenerate distribution. for the bayes loss, as\nwell as for l2(y, \u02c6y) and l1(y, \u02c6y), one obtains the 0-1 loss l01 and, therefore, e(l(y, \u02c6y))\nis the probability of misclassification. for the kullback-leibler discrepancy, however, one has\nlkl(y, \u02c6y) = kl(y, \u02c6y ) = \u2212 log(\u02c6\u03c0y ), where y \u2208 {1, . . . , k} is the observation. alternative\nloss functions that are usefull in classification are the hinge loss and the exponential loss, which\nwill be considered later.\n\nloss for univariate discrete response\nin the preceding sections implicitly the response was assumed to be multinomial. for univariate\ndiscrete responses, for example, count data with y \u2208 {0, 1, . . .} alternative loss functions can be\nuseful. a distance measure that explicitly depends on the assumed distribution may be derived\nfrom the deviance, which in generalized linear models measures the discrepancy between the\ndata and the fit. it may also be used for future observations in the form of a predictive deviance\nthat measures the discrepancy between the prediction and the observation. the deviance is an\nappropriate measure since its scaling is based on the underlying distribution. for a poisson\n\n "}, {"Page_number": 450, "text": "438\n\nchapter 15. prediction and classification\n\n(cid:14)\n\ndistributed response, it is given by d(y, \u02c6\u03bc) = 2y log(y/\u02c6\u03bc), where \u02c6\u03bc is the estimated mean. a\ndisadvantage of the predictive deviance is that it is sensible only if the assumed distribution\nassumption holds.\n\nin this respect, the kullback-leibler distance is more general. for a discrete response it has\nthe general form lkl(\u03c0, \u02c6\u03c0) =\ni \u03c0i log(\u03c0i/\u02c6\u03c0i), where \u03c0t = (\u03c01, \u03c02, . . . ) is the vector of the\ntrue probabilities and \u02c6\u03c0t = (\u02c6\u03c01, \u02c6\u03c02, . . . ) is the estimate. instead of using dummy variables, a\nnew response y \u2208 {0, 1, . . .} may be identified with the degenerate distribution that puts weight\ny = (\u03b41(y), \u03b42(y), . . . ), \u03b4y(y) = 1, \u03b4y(z) = 0, z (cid:8)= y.\n1 at y, represented by the vector \u03b4t\nthe corresponding kullback-leibler distance is again the logarithmic score lkl(\u03b4y, \u02c6\u03c0) =\n\u2212 log(\u02c6\u03c0y). it measures the distance between y and the estimate \u02c6\u03c0 simply by the negative log-\ntransformed probability at the new observation y. the distance does not depend on the assumed\ndistribution and therefore may be used quite generally without reference to a specific discrete\ndistribution (although the range of the distance depends on the underlying distribution). the\ndecomposition considered previously also holds for the case with infinite support. one obtains\n\u2212\u03c0r log(\u03c0r). the\nthe actual prediction error in the form e(lkl(\u03b4y, \u02c6\u03c0)) = lkl(\u03c0, \u02c6\u03c0) +\nfirst term in the equation is the error due to estimation of the underlying probability distribution,\nwhereas the second term gives the error that cannot be avoided and which is a function of the\ntrue distribution.\n\n(cid:14)\n\na criticism of scores like the logarithmic score is that the predictive distribution \u02c6\u03c0 is only\nevaluated at the observation y. therefore, it does not take the whole predictive distribution\ninto account. as gneiting and raftery (2007) postulate, a desirable predictive distribution\nshould be as sharp as possible and well calibrated. sharpness refers to the concentration of the\ndistribution and calibration to the agreement between the distribution and the observation. for\nnominal data, quantification of the distance tends to become trivial. but if y represents count\ndata, a more appropriate loss function derived from the continuous ranked probability score\n(gneiting and raftery, 2007) is\n\nr\n\n(cid:7)\n(\u02c6\u03c0(r) \u2212 ind(y \u2264 r))2,\n\nlrp s(y, \u02c6\u03c0) =\n\n(15.3)\nwhere \u02c6\u03c0(r) = \u02c6\u03c01 + \u00b7\u00b7\u00b7 + \u02c6\u03c0r. for binary data, it is a sum over quadratic (or brier) scores\nand takes the closeness between the whole distribution and the observed value into account. it\nmay also be used for categorial ordered responses, which actually are not uni-dimensional but\ncontain at least order information. for further measures see gneiting and raftery (2007).\n\nr\n\n15.2 methods for optimal classification\nin this section we consider how the predictor space that contains the observations x can be\npartitioned to obtain optimal classification rules. the objective is a direct prediction, that is,\none wants to find the the true class y \u2208 {1, . . . , k} when only x from a new observation (y, x)\nis observed.\n\n15.2.1 bayes rule and the minimization of the rate of misclassification\nin the following we consider direct prediction rules that usually are derived within the frame-\nwork of discriminant analysis. therefore we focus on the classification problem with a finite\nnumber of classes, y \u2208 {1, . . . , k}. some denotations that are used in the following are\n\n\u2022 p (r) = p (y = r), r = 1, . . . , k, for the prior.\n\u2022 p (r|x) = p (y = r|x), r = 1, . . . , k, for the conditional probability for class r given\nx, also called posterior probability.\n\n "}, {"Page_number": 451, "text": "15.2. methods for optimal classification\n\n439\n\u2022 f(x|1), . . . , f(x|k) for the densities within classes, which may refer to continuous or\ndiscrete distributions.\n\u2022 f(x) = p (1)f(x|1) + \u00b7\u00b7\u00b7 + p (k)f(x|k) for the mixture density.\nlet a classifier or classification rule be defined as a mapping\n\n\u03b4 : rp (cid:24)\u2192 {1, . . . , k}\n\nx (cid:24)\u2192 \u03b4(x).\n\na basic classification rule that can be derived from decision theory arguments (see next section)\nand has been already considered briefly in the preceding section is the bayes rule. the bayes\nrule has the simple form\n\n\u2217\n\n\u03b4\n\n(x) = r \u21d0\u21d2 p (r|x) = max\n\ni=1,...,k\n\np (i|x).\n\n(15.4)\n\nthe bayes rule explains itself. for a given x one chooses the class for which the poste-\nrior probability p (r|x) takes its maximal value. the rule is strongly connected to 0-1 loss\nl01(y, \u02c6y ) = 0 if y = \u02c6y and l01(y, \u02c6y ) = 1 if y (cid:8)= \u02c6y . when considering the 0-1 loss one\nmay distinguish between several error rates that are connected to the rule. for a fixed classifier\n\u03b4 the prediction error for randomly drawn (y, x) has the form\n\n\u03b5 = e l01(y, \u03b4(x)) = p (y (cid:8)= \u03b4(x)) = 1 \u2212 p (y = \u03b4(x))\n\nand is also known as the global rate of misclassification or the global probability of misclas-\nsification. from p (y (cid:8)= \u03b4(x)) it is seen that \u03b5 is the probability that an observation (y, x) is\nmisclassified. conditioning on x yields the error rate of misclassification conditional on x:\n\n\u03b5(x) = ey|x l01(y, \u03b4(x)) = p (y (cid:8)= \u03b4(x)|x) = 1 \u2212 p (y = \u03b4(x)|x) =\n\n= 1 \u2212 p (\u03b4(x)|x).\n\nthe connection between these error rates is given by\n\n)\n\n\u03b5 = ex \u03b5(x) =\n\n\u03b5(x)f(x)dx.\n\nit is obvious from \u03b5(x) = 1\u2212p (\u03b4(x)|x) how to choose the best prediction with reference to the\n0-1 loss for a fixed x. one just has to choose \u03b4(x) such that p (\u03b4(x)|x) takes its maximal value.\nsince \u03b5 is an integral across \u03b5(x), the optimal error rate is obtained by choosing the optimal\nrule for each value x. therefore, the bayes rule is optimal when the 0-1 loss is considered and\none obtains:\n\nthe bayes rule\n\n\u2217\n\n\u03b4\n\n(x) = r \u21d0\u21d2 p (r|x) = max\n\ni=1,...,k\n\np (i|x)\n\nminimizes the global probability of misclassification\n\nthe bayes rule obtains the optimal error rate:\n\n)\n\n\u03b5opt = 1 \u2212\n\np (r|x)f(x)dx,\n\nmax\nr=1,...,k\n\n "}, {"Page_number": 452, "text": "440\nwith the optimal value at x given as \u0001opt(x) = 1 \u2212 maxr=1,...,k p(r|x).\n\nchapter 15. prediction and classification\n\nfurther error rates are the individual errors:\n\n\u03b5rs = p (\u03b4(x) = s|y = r) =\n\n)\n\nf(x|r)d x,\n\nx:\u03b4(x)=s\n\nwhich represent the probability of classifying into class s if the underlying true class is r.\nindividual error rates are linked to the global error rate by\n\n\u03b5 = p (\u03b4(x) (cid:8)= y ) =\n\np (\u03b4(x) (cid:8)= r|y = r)p (r) =\n\n\u03b5rs p (r).\n\n(15.5)\n\nk(cid:7)\n\n(cid:7)\n\ns(cid:5)=r\n\nr=1\n\nk(cid:7)\n\nr=1\n\nwhen looking for good classifiers that minimize the global error, the main problem is that\np (r|x) usually is unknown and has to be estimated. for the estimated rate one obtains for a\ngiven rule \u02c6y = \u03b4(x) the actual error rate \u03b5. of course for the actual error rate \u03b5opt \u2264 \u03b5 holds.\n\nerror rates\n\nglobal rate of misclassification\n\n\u03b5 = p (y (cid:8)= \u03b4(x))\n\nconditional rate of misclassification (conditional on x)\n\nindividual error rates\n\n\u03b5(x) = 1 \u2212 p (\u03b4(x)|x)\n\n\u03b5rs = p (\u03b4(x) = s|y = r)\n\n15.2.2 classification with discriminant functions\nthe bayes rule as given above uses directly the posterior probabilities p (y = r|x). alternative\nforms of the classification rule can be formulated by using the prior and the densities of the\npredictors given the class. it is helpful to consider the bayes rule as a maximizer of so-called\ndiscriminant functions. for each x let the discriminant functions dr(x), r = 1, . . . , k, contain\nsome measure for the plausibility that observation x comes from class r. if one defines dr(x) =\np (r|x), one obtains the bayes rule in the form\n\n\u2217\n\n\u03b4\n\n(x) = r \u21d0\u21d2 dr(x) = max\n\ni=1,...,k\n\ndi(x).\n\n(15.6)\n\nalternative forms that yield the same classification rule can be obtained by using bayes\u2019 theo-\nrem\n\np (r|x) = f(x|r)p (r)\n\nf(x)\n\n(cid:14)\n= f(x|r)p (r)\ni=1 p (i)f(x|i)\n\nk\n\n.\n\n "}, {"Page_number": 453, "text": "15.2. methods for optimal classification\n\n441\n\nfigure 15.1: bayes classifications for p(1) = p(2) = 0.5 (upper panel) and p(1) =\n0.6, p(2) = 0.4 (lower panel).\n\ncomparison of two classes by use of the bayes rule has the form\n\np (r|x) \u2265 p (s|x) \u21d0\u21d2\n\u21d0\u21d2\n\u21d0\u21d2 log(f(x|r)) + log(p (r)) \u2265 log(f(x|s)) + log(p (s)).\n\n\u2265 f(x|s)p (s)\nf(x|r)p (r)\nf(x|r)p (r) \u2265 f(x|s)p (s)\n\nf(x)\n\nf(x)\n\nthus the maximization of p (r|x) over the classes is also obtained by using a maximization of\ndiscriminant functions with the functions given in one of the following forms:\n\ndr(x) = f(x|r)p (r)/f(x),\n\ndr(x) = log(f(x|r)) + log(p (r)).\n\ndr(x) = p (r|x),\ndr(x) = f(x|r)p (r),\nthe different forms may be used to show different aspects of the bayes rule and to simplify\nthe classification rule. the form dr(x) = f(x|r)p (r) shows that the bayes classification\nis equivalent to classifying an observation into class r if the weighted density of the class is\nthe maximal one. figure 15.1 shows the classification rule for a one-dimensional predictor\nwith equal priors p (1) = p (2) and with priors p (1) = 0.6, p (2) = 0.4.\nit is seen that\nthe increase in prior probability from p (1) = 0.5 to p (1) = 0.6 shifgts the cut-off to the\nright. therefore, the area where classification is into class 1 increases. the logarithmic form of\nthe density may be used to obtain a simple form of the bayes rule when the distribution of the\n\n "}, {"Page_number": 454, "text": "442\n\nchapter 15. prediction and classification\n\npredictor is normal. the case of normal distributions will be considered in the next section as an\nexample.\n\nfor discrete predictors with given distributions of indicators given the classes, one can col-\n\nlect the information in the table as follows:\n\nindicator values\n\nx1\n\np (x1|1)\n\n...\n\n. . .\n\nxm\n\np (xm|1)\n\n.\n.\n.\n\np (x1|k)\n\n. . .\n\np (xm|k)\n\nclasses\n\n1\n.\n.\n.\nk\n\n.\n\n.\n\npriors\n\np (1)\n\n.\n.\n.\n\np (k)\n\nfor fixed value xi one obtains the posterior probabilities from bayes\u2019 theorem as p (r|xi) =\n\n(cid:14)\np (xi|r)p (r)/(\nexample 15.3: drug use\nif one assumes that 10 % of the population uses drugs, the probabilities of the test from example 15.1 are\ngiven by\n\ni=1 p (i)p (xi|i)).\n\nk\n\nx = 1 (positive)\n\nx = 0 (negative)\n\npriors\n\n1 (user)\n2 (non-user)\n\n0.10\n0.90\none computes p (1|x = 1) = 0.68, p (2|x = 1) = 0.32, yielding the bayes rule \u03b4\n\n0.95\n0.05\n\n0.05\n0.95\n\n(x) = 1 if x = 1.\nthe corresponding error is \u0001(x = 1) = 1 \u2212 p (1|x = 1) = 0.32. therefore, 32% of the applicants that\ntest positive will be falsely classified as drug users. thus, for x = 1, the classification rule is not very\nconvincing. however, for x = 0, the results differ. one computes p (1|x = 0) = 0.006, p (2|x = 0) =\n(x) = 2 if x = 0 with error \u0001(x = 0) = 1 \u2212 p (2|x = 0) = 0.006.\n0.994, yielding the bayes rule \u03b4\nit is obvious that the accuracy of the prediction strongly depends on the observation that is considered.\nthe global error rate, which is 0.05 in the present case, is just an overall measure. the variation of the\naccuracy of the prediction across values of the predictor is a general property of classification rules (see\nalso exercise 15.4).\n\n\u2217\n\n\u2217\n\n15.2.3 discrimination with normally distributed predictors\nlet the predictors be normally distributed within classes, that is, one assumes x|y = r \u223c\nn(\u03bcr, \u03c3 r). a simplifying assumption that is often used is that the covariances are the same\nin this homogeneous case one assumes \u03c3 = \u03c3 1 = . . . = \u03c3 k. then the\nin each class.\ndiscriminant function dr(x) = log(f(x|r)) + log(p (x)) takes the form\n\ndr(x) = \u22121\n2\n\n(x \u2212 \u03bcr)t \u03c3\n\n\u22121(x \u2212 \u03bcr) \u2212 p\n2\n\nlog(2\u03c0) \u2212 1\n2\n\nlog(|\u03c3|) + log(p (r)).\n\nlet us investigate how the maximization of dr(x) discriminates between two classes, namely,\nit is easily seen that a comparison of dr(x) and ds(x) yields a minimum\nclasses r and s.\ndistance classifier with a shift. if p (r) = p (s), one prefers class r over class s if\n(x \u2212 \u03bcr)t \u03c3\n\n\u22121(x \u2212 \u03bcr) \u2264 (x \u2212 \u03bcs)t \u03c3\n\n\u22121(x \u2212 \u03bcs);\n\nthis means that the mahalanobis distance between x and the center of the rth class \u03bcr is smaller\nthan the mahalanobis distance between x and \u03bcs. for unequal priors one has to include a shift\ndetermined by log(p (r)/p (s)).\n\n "}, {"Page_number": 455, "text": "15.2. methods for optimal classification\n\n443\n\nby building differences between dr(x) and ds(x) one obtains after simple computation\n\nwhere\n\ndr(x) \u2212 ds(x) = \u03b20rs + xt \u03b2rs,\n\n\u03b20rs = \u22121\n\u03b2rs = \u03c3\n\n2 \u03bct\nr \u03c3\n\u22121(\u03bcr\n\n\u22121\u03bcr +\n\u2212 \u03bcs).\n\n1\n2 \u03bct\n\ns \u03c3\n\n\u22121\u03bcs + log\n\n(cid:25)\n\n(cid:26)\n\n,\n\np (r)\np (s)\n\nthis means that the classifier is linear with the rule:\n\nprefer class r over s if \u03b20rs + xt \u03b2rs\n\n\u2265 0.\n\n(15.7)\n\n(15.8)\n\ngeometrically, the two classes r and s are separated by a hyperplane. for the simple two-class\ncase one obtains\n\n\u03b4(x) = 1 if \u03b20 + xt \u03b2 \u2265 0,\n\nwhere \u03b20 = \u03b2012, \u03b2 = \u03b212. the function s(x) = \u03b20 + xt \u03b2 is sometimes called a score that\ndistinguishes between classes 1 and 2.\n\nestimated versions of the classifier are obtained by replacing the class centers \u03bcr and \u03c3\nby estimates from a sample (see section 15.4). when the number of predictors is not too large,\nlinear classifiers of the form (15.8) are attractive because only a few parameters have to be\nestimated. on the other hand, they are only appropriate if the classes can be separated linearly.\nit is interesting to consider an alternative representation of the linear classification rule. for\n\nthe case of two classes, the posterior is given by\n\nf(x|1)p (1)\n\nexp(a)\n\np (1|x) =\n\n(15.9)\nwhere a = log{[f(x|1)p (1)]/[f(x|2)p (2)]} = d1(x) \u2212 d2(x). therefore, one has a linear\nlogistic regression model:\n\nf(x|1)p (1) + f(x|2)p (2)\n\n1 + exp(a) ,\n\n=\n\np (1|x) =\n\nexp(\u03b20 + xt \u03b2)\n1 + exp(\u03b20 + xt \u03b2) ,\n\nwith the parameters \u03b20, \u03b2 given by \u03b20 = \u03b2012, \u03b2 = \u03b212 from (15.7). although the lin-\near logistic regression model derives from the assumption of normally distributed predictors,\nx|y = r \u223c n(\u03bcr, \u03c3), the model holds under more general assumptions.\n\nthe linear multinomial logistic model for k classes,\n\n(cid:14)\n\np (r|x) =\n\n1 +\n\nexp(\u03b20r + xt \u03b2r)\nk\u22121\nj=1 exp(\u03b20j + xt \u03b2j)\n\n,\n\nmay be derived in the same way by assuming x|y = r \u223c n(\u03bcr, \u03c3), yielding \u03b20r =\n\u03b20rk, \u03b2r = \u03b2rk from (15.7) (exercise 15.5).\nin the more general case, with potentially differing covariances, x|y = r \u223c n(\u03bcr, \u03c3 r),\nthe classification rule does not simplify to a linear form. then, the difference between dis-\ncriminant functions dr(x) \u2212 ds(x) contains quadratic terms x2\np as well as interaction\nterms xixj. when the terms \u03bcr and \u03c3 r are replaced by estimates one refers to the method\nas quadratic discrimination. the rule is more flexible, but many more parameters have to be\nestimated, which for small sample sizes often yields inferior results than simple linear discrim-\nination.\n\n1, . . . , x2\n\n "}, {"Page_number": 456, "text": "444\n\nchapter 15. prediction and classification\n\n15.2.4 bayes rule for general loss functions\nclassification rules may be considered within the more general framework of decision theory.\non the basis of an observed feature vector x, the decision \u03b4(x) = r corresponds to claiming\nthat x is from class r. decision theory requires a loss function l(y, \u03b4(x)) for penalizing errors\nin classification. typically one assumes\n\n(cid:29)\n\nl(r, s) =\n\n0\n\nif\n> 0 if\n\nr = s\nr (cid:8)= s\n\n(correct decision)\n(wrong decision).\n\nthe criterion for the performance of a classification rule \u03b4 is the expected loss or the total bayes\nrisk:\n\nr = e(l(y, \u03b4(x))),\n\nwhere (y, x) is a randomly drawn observation. by conditioning on x the risk can be written as\n\nk(cid:7)\n\nr = ex ey |x(l(y, \u03b4(x))) = ex\n\nl(i, \u03b4(x))p (i|x)\n\n)\n\n=\n\nk(cid:7)\n\ni=1\n\ni=1\n\nl(i, \u03b4(x))p (i|x)f(x)dx.\n\ntherefore, the total risk is a weighted average across the conditional risk (conditional on x):\n\nk(cid:7)\n\ni=1\n\nr(x) =\n\nl(i, \u03b4(x))p (i|x).\n\nthat shows how the total risk may be minimized. the optimal classifier (with loss l), called\nthe bayes rule (with loss l), is given by\n\n\u03b4(x) = r \u21d4 k(cid:7)\n\ni=1\n\nk(cid:7)\n\ni=1\n\nl(i, r)p (i|x) = min\n\nj=1,...,k\n\nl(i, j)p (i|x).\n\n(15.10)\n\nit minimizes the total bayes risk r. when ties occur they can be broken arbitrarily. the value r\nfor the bayes rule is called the minimum bayes risk. some authors (for example, ripley, 1996)\ncall it the minimal r bayes risk while fukunaga (1990) calls it the bayes error. the minimum\nbayes risk obtained by (15.10) is the best one can achieve if the posterior probabilities are\nknown.\n\nit is immediately seen that the bayes rule with loss function may be written as a maximizer\n\nof discriminant functions in the form (15.6) by using the discriminant functions\n\nit is easy to show that for the symmetrical loss function\n\ndr(x) = \u2212 k(cid:7)\n(cid:29)\n\ni=1\n\nl(i, r)p (i|x).\n\nl(r, s) =\n\n0\nc\n\nif\nif\n\nr = s\nr (cid:8)= s,\n\nwhich differs from the 0-1 loss just by a constant c, the bayes rule is given by maximizing the\n\u2217(x) = r \u21d4 p (r|x) = maxj=1,...,k p (j|x). for the loss function\nposterior probability, \u03b4\n\n(cid:29)\n\nl(r, s) =\n\n0\nc/p (r)\n\nif\nif\n\nr = s\nr (cid:8)= s,\n\n "}, {"Page_number": 457, "text": "15.3. basics of estimated classification rules\n\n445\n\nwhich specifies that the loss is proportional to 1/p (r), one obtains the maximum likelihood\n(ml) rule:\n\n\u03b4(x) = r \u21d4 f(x|r) = max\n\nf(x|j).\n\nj=1,...,k\n\nit is also possible to extend the decisions to incorporate the decision d, which means \"being in\ndoubt\" (e.g., ripley, 1996). when the corresponding loss function is defined by\n\n\u23a7\u23a8\n\u23a9 0\n\nl(r, s) =\n\nif\n1\nif\nd if\n\nr = s\nr (cid:8)= s, s \u2208 {1, . . . , k}\ns = d\n\n(cid:29)\n\nthe classifier that minimizes the total risk is given by\n\n\u03b4(x) =\n\nif p (r|x) = maxj=1,...,k p (j|x)\n\nr\nd if p (j|x) \u2264 1 \u2212 d for all\n\nj.\n\nand p (r|x) > 1 \u2212 d\n\n15.3 basics of estimated classification rules\n15.3.1 samples and error rates\nthe optimal classification rule is easily derived when the class densities (and priors) or the\nposterior probabilities are known. in practice, the classification rule has to be estimated and \u03b4\nis replaced by \u02c6\u03b4. therefore one needs a training set or learning sample that is used to obtain\nthe estimated classifier \u02c6\u03b4. there are different forms of learning sets. one may distinguish them\nas follows:\n\n\u2022 total sample, where (yi, xi), i = 1, . . . , n are iid variables;\n\u2022 stratified sample, conditional on classes, where xir|y = r, i = 1, . . . , nr, are iid vari-\nables;\n\n\u2022 stratified sample, conditional on x, where y (x)\n\ni\n\n, i = 1, . . . , n(x), are iid variables.\n\nthe type of learning sample determines how a classification rule may be estimated. based\n\non the optimal bayes rule (for l01-loss) one classifies by\n\u02c6\u03b4(x) = r \u21d4 \u02c6dr(x) = max\n\ni=1,...,k\n\n\u02c6di(x),\n\n(15.11)\n\nwhere the functions \u02c6dr(x) are estimated discriminant functions. if one has a total sample, the\ndiscriminant functions \u02c6dr(x) = \u02c6p (r|x) or \u02c6dr(x) = \u02c6f(x|r)p (r) can be used. the priors\np (r) in the latter form may also be replaced by the proportion \u02c6p (r) = nr/n. when one has\na stratified sample, conditional on classes, \u02c6dr(x) = \u02c6f(x|r)p (r) may be estimated directly,\nwhereas \u02c6p (r|x) is harder to obtain. the form \u02c6p (r|x) is more suitable for the stratified sample,\nconditional on x. unless stated otherwise, in the following we will assume that the learning\nsample is a total sample.\n\nwhen using estimated classification rules the error (for l01-loss) becomes\n\n\u03b5(\u02c6\u03b4) = p (\u02c6\u03b4(x) (cid:8)= y ) =\n\nf(x|r)p (r)dx,\n\n)\n\nk(cid:7)\n\n\u02c6\u03b4(x)(cid:5)=r\n\nr=1\n\nwhich is a random variable because it depends on the learning sample. the corresponding error\nrate when a new sample (y, x) is drawn is the expected actual rate of misclassification el \u03b5(\u02c6\u03b4),\nwhere el denotes that expectation is built over the learning sample.\n\n "}, {"Page_number": 458, "text": "446\n\nchapter 15. prediction and classification\n\n15.3.2 prediction measures\nin the following we consider prediction measures for classification. we focus on measures\nthat apply for arbitrary classification rules rather than giving measures that depend on specific\nassumptions, for example, a normal distribution of predictors. when investigating the perfor-\nmance of a classifier it is essential to distinguish between measures that are computed by use of\nthe learning sample or by use of new observations. therefore one distinguishes several types\nof samples:\n\n\u2022 the learning or training set l = {(yi, xi), i = 1, . . . , n}, from which the classifier is\nderived;\n\u2022 the test set, which is a sample of new observations t = {(yi, xi), i = 1, . . . , nt}, set\naside to assess the performance of the predictor;\n\u2022 a validation set v = {(yi, xi), i = 1, . . . , nv }, which is a sample of new observations\nused to choose hyperparameters.\n\nfor the assessment of performance typically the test set is used. the validation set is an addi-\ntional set that is used to optimize the classification rule if it contains additional parameters, for\nexample, the number of neighbors used in the nearest neighbors method. the terminology is\nnot standardized. some authors use the term \"validation set\" for the test set.\n\nempirical error rates\nthe apparent error rate that directly derives from the learning data is the reclassification or\nresubstitution error rate, which for the l01 loss is given by\n\n\u02c6\u03b5rs(\u02c6\u03b4) =\n\n1\nn\n\n(yi,xi)\u2208l\n\ni(\u02c6\u03b4(xi) (cid:8)= yi),\n\nwhere i(a) = 1 if a is true and zero otherwise. the resubstitution error is the apparent error\nfor the 0-1 loss. it tends to underestimate the expected actual error rate, since the learning set\nis used twice, once for deriving the classification rule and once for evaluating of the accuracy.\nit is to be expected that the rule performs better in the sample from which the rule is derived\nthan in future samples. especially if one uses complicated classification rules, one often may\nreduce the error in the learning sample to zero but naturally the perfect separation in the training\nsample cannot be expected to hold in the future (see also section 1.1).\n\na better criterion is the test error. if the data are split from the beginning into a learning set\n\nand a training set, one computes the empirical test error:\n\n\u02c6\u03b5t est(\u02c6\u03b4) =\n\n1\nnt\n\n(yi,xi)\u2208t\n\ni(\u02c6\u03b4(xi) (cid:8)= yi),\n\nwhere nt is the number of observations in the test set. it is an unbiased estimate of the expected\nactual error rate (for sample size n) but wastes data since only part of the data determines the\nclassifier.\n\nif one wants to not waste data by setting aside a test set, one can try to obtain better estimates\nfrom the learning set by dividing it several times. a strategy of this type is k-fold cross-\nvalidation. in k-fold cross-validation the data of the learning set is split into k roughly equal-\nsized parts. let t1, . . . , tk, where t1 \u222a \u00b7\u00b7\u00b7 \u222a tk = l, denote the partition of the learning\nsample. now for the part tr the classifier is derived from \u222aj(cid:5)=rtj and the error rates are\n\n(cid:7)\n\n(cid:7)\n\n "}, {"Page_number": 459, "text": "15.3. basics of estimated classification rules\n\n447\n\ncomputed in tr. therefore, tr are new data that have not been used for the estimation of the\nclassification rule. this is done for r = 1, . . . , k, yielding the k-fold cross-validation error\n\n\u02c6\u03b5k\u2212cv (\u02c6\u03b4) =\n\n1\nn\n\ni(yi (cid:8)= \u02c6\u03b4\\r(xi)),\n\nwhere \u02c6\u03b4\\r denotes that the classification rule is estimated from \u222aj(cid:5)=rtj.\n\nthe extreme case k = n, where one observation at a time is left out, is known as leave-\n\none-out cross-validation or leave-one-out error rate:\n\nk(cid:7)\n\n(cid:7)\n\nr=1\n\n(yi,xi)\u2208tr\n\n(cid:7)\n\n\u02c6\u03b5l00(\u02c6\u03b4) =\n\n1\nn\n\n(yi,xi)\u2208l\n\ni(yi (cid:8)= \u02c6\u03b4\\i(xi)),\n\nwhere \u02c6\u03b4\\i is estimated from l \\ (yi, xi). the leaving-one-out error rate is approximately unbi-\nased as an estimate of the expected actual error rate.\n\nan alternative to classical k-fold cross-validation is monte carlo cross-validation or sub-\nsampling. in contrast to classical cross-validation, subsampling does not use a fixed partition\nof the original dataset. the test sets are generated by several random splittings of the original\ndataset into learning sets and test sets. let (lr, tr), r = 1, . . . , k, denote the pairs of learning\nsamples lr and test sample tr with fixed samples sizes nl and nt , respectively. the common\nsize ratios are nl/nt and are 4/1 and 9/1. the resulting subsampling estimator has the form\n\nk(cid:7)\n\nr=1\n\n1\nnt\n\n(cid:7)\n\n(yi,xi)\u2208tr\n\n\u02c6\u03b5sub(\u02c6\u03b4) =\n\n1\nk\n\ni(yi (cid:8)= \u02c6\u03b4lr(xi)),\n\nwhere \u02c6\u03b4lr denotes the classifier based on the learning set lr. in contrast to classical cross-\nvalidation, the number of splittings k and the used sample sizes nl, nv are not linked; there-\nfore, the number of splittings may be chosen very large.\n\n\u2217\nr. the observations of the latter set, t\n\nan interesting, cleverly designed error rate is based on efron\u2019s bootstrap methodology.\n\u2217\nr of size n is drawn\nstarting from a dataset with observation size n, a bootstrap sample l\n\u2217\nr single observations may be represented several times.\nrandomly with replacement. thus, in l\n\u2217\n\u2217\nr forms the learning set and the corresponding test set t\nr is built from the observations that\nl\n\u2217\nr , are usually called\nwere selected when constructing l\n\u2217\nr the classifier\n\"out-of-bag\" observations. when the classifier is derived from observations l\nhas not \"seen\" the out-of-bag observations, and therefore these are perfect for evaluating the\n\u2217\nr has been fixed\nperformance in future samples. while the sample size of the learning sets l\nat n, due to the random drawing, the number of out-of-bag observations varies. therefore, the\n\u2217\nr is a random variable. the estimator of the error rate typically uses a large\nsample size nr of t\n\u2217\nr ), r = 1, . . . , b, denote the bootstrap samples and\nnumber of bootstrap samples. let (l\n\u2217\n\u02c6\u03b4l\u2217\nr denote the estimated classifier based on learning sample l\nr. the bootstrap estimators in\ncommon use are\nr )i(yi (cid:8)= \u02c6\u03b4l\u2217\n\u2217\n\n\u2217\nr, t\n\nr (xi))\n\nb\nr=1\n\nn\n\n\u02c6\u03b5b1(\u02c6\u03b4) =\n\n(cid:14)\n(cid:14)\n(cid:14)\ni=1 i(yi \u2208 t\n(cid:14)\n(cid:14)\nr=1 i(yi \u2208 t\n\nb\nr=1\n\nb\n\nn\n\n(cid:14)\nn(cid:7)\n\ni=1\n\n1\nn\n\ni=1 i(yi \u2208 t \u2217\nr )\nr )i(yi (cid:8)= \u02c6\u03b4l\u2217\n\u2217\n\nb\n\nr=1 i(yi \u2208 t \u2217\nr )\n\nr (xi))\n\n.\n\nand\n\n\u02c6\u03b5b2(\u02c6\u03b4) =\n\nthe first estimator uses all observations simultaneously, whereas the second estimator within\nthe sum computes the individual errors for single observations. for large b the estimators\nproduce nearly identical results (compare efron and tibshirani, 1997).\n\n "}, {"Page_number": 460, "text": "448\n\nchapter 15. prediction and classification\n\nby construction bootstrap estimators are upwardly biased estimates of the mean actual error\nrate, since only a fraction of the available data size n is used in deriving the classifier. efron\n(1983) proposed a modification that corrects for this bias. it is based on the observation that\nthe probability that a data point appears in the bootstrap sample is 1 \u2212 (1 \u2212 1/n)n, which for\nn > 40 may be approximated by .632. therefore, efron (1983) proposed a combination of the\nbootstrap estimator and the resubstitution error rate of the form\n\n\u02c6\u03b5.632(\u02c6\u03b4) = 0.368\u02c6\u03b5rs(\u02c6\u03b4) + 0.632\u02c6\u03b5b1(\u02c6\u03b4).\n\nthe estimator corrects for bias if the resubstitution error is moderately biased. however,\nfor highly overfitting classification rules the resubstitution error becomes very small and then\n\u02c6\u03b5.632(\u02c6\u03b4) may become overly optimistic. in cases of strong overfitting one should use a modi-\nfication of \u02c6\u03b5.632(\u02c6\u03b4), proposed by efron and tibshirani (1997) and called the .632+ estimator.\nthe modification puts more weight on the bootstrap error rate by use of a relative overfitting\nrate (for details see efron and tibshirani, 1997).\n\nwhen comparing estimates of prediction error the central issues are bias and computational\neffort. although the resubstitution error rate is easily computed, it cannot be recommended\nbecause of its optimistic bias. test errors make it necessary to set aside the test data. there-\nfore, the dataset used to compute the classifier is smaller than the available data, resulting in\npessimistic estimates, since more data could be used to compute the classifier. in small sample\nsettings classical cross-validation techniques have been critized because of their high variabil-\nity (braga-neto and dougherty, 2004). in contrast, molinaro et al. (2005) report small mean\nsquared errors also in small data settings. subsampling and bootstrap errors are stable alterna-\ntives but need computational power and are slightly pessimistic.\n\nreceiver operating characteristic curves (roc curves)\nreceiver operating characteristic curves (roc) are a device for measuring the predictive power\nin two class problems. typically the two classes are considered in an asymmetrical way. one\nclass is considered as the signal to be detected while the other class represents the no-signal\ncase.\nin medical classification problems, the signal frequently refers to a disease, while in\ncredit risk modeling one focusses on default events as the signal. for simplicity we will denote\nthe two classes by y = 1 for the signal to be detected and y = 0 for the no-signal class.\n\none often distinguishes between sensitivity and specificity, which are defined as follows:\n\nsensitivity : p (\u02c6y = 1|y = 1),\n\nspecificity : p (\u02c6y = 0|y = 0).\n\nsensitivity is the probability of detecting the signal if it is present. in medical applications\nsensitivity may represent the probability of predicting disease given the true state is disease,\nwhile in credit risks it means that the borrower is correctly classified as a defaulter. specificity,\non the other hand, describes the probability of correctly identifying the no-signal case, namely,\npredicting non-disease (non-defaulter) given the true state is non-disease (non-defaulter).\n\nsensitivity and specificity characterize the correct classification, which may also be de-\nscribed as \"hit\" and \"correct rejection.\" complementary events are characterized as \"miss\" when\nthe signal has falsely not been detected and \"false alarm\" when the signal has not been present\nbut has been diagnosed. borrowing terminology from hypothesis testing, missing the signal\nmay be seen as a type ii error while a false alarm corresponds to a type i error. table 15.2\nsummarizes the events.\nin many applications one is not only interested in the predictive power of a fixed rule that\ngives prediction \u02c6y \u2208 {0, 1}. rather than investigating the preference in terms of one hit rate\nand false alarm rate, one wants to evaluate how well a classification rule performs for different\n\n "}, {"Page_number": 461, "text": "15.3. basics of estimated classification rules\n\n449\n\ntable 15.2: classification results given y.\n\npositive\n\n1\n\n\u02c6y\n\nnegative\n\n0\n\ny\n\nsignal\nnoise\n\n1\n0\n\nhit/sensitivity\n\nmiss / type ii\n\nfalse alarm / type i\n\ncorrect rejection/specifity\n\ncut-offs. typically classification rules for two-class problems are based on an underlying score\nthat determines how classification is done. with the score representing the preference of class\ny = 1 over class y = 0, one considers the classification rule\n\nclassify \u02c6y = 1 if the score s(x) is above threshold c.\n\nthe roc curves give sensitivity and specificity for varying cut-offs. more concise, the roc\ncurve is a plot of sensitivity as a function of 1-specificity; it plots the hit probability against the\nfalse alarm rate. with score s(x) and cut-off value c one obtains the hit rate and the false alarm\nrate as\n\nhr(c) =p (\u02c6y = 1 based on cut-off c|y = 1) = p (s(x) > c|y = 1),\nfar(c) =p (\u02c6y = 1 based on cut-off c|y = 0) = p (s(x) > c|y = 0).\n\nroc curves are plots of (far(c), hr(c)) for varying cut-off values c. they are monotone\nincreasing functions in the positive quadrant. typically the curve has a concave shape con-\nnecting the points (0, 0) and (1, 1). it may be given as {(far(c), hr(c)), c \u2208 (\u2212\u221e,\u221e)} or\n{(t, roc(t)), t \u2208 (0, 1)}, where roc(t) maps t to hr(c), with c being the value for which\nfar(c) = t. it has some nice properties; for example, it is invariant to strictly increasing\ntransformations of the classification score s(x).\n\nthe curve is much more informative than a simple classification rule, which is based on just\none cut-off value. a classification score that perfectly separates two classes has for some value\nc hr(c) = 1 and far(c) = 0 and the roc curve is along the axes connecting (0, 0), (0, 1),\nand (1, 1). if the score is non-informative, one has roc(t) = t. discrimination is better the\ncloser the curve is to the left upper point (0, 1). for illustration, in figure 15.2 the empirical\nroc curves for two classes of the glass identification data (example 15.2) are given. it shows\nthe curves for three classification rules, namely, linear, quadratic, and logistic discrimination,\n\nt\n\ne\na\nr\n \n\ne\nv\ni\nt\ni\ns\no\np\ne\nu\nr\nt\n\n \n\n0\n1\n\n.\n\n8\n0\n\n.\n\n6\n\n.\n\n0\n\n4\n0\n\n.\n\n2\n0\n\n.\n\n0\n.\n0\n\nlda\nqda\nlogistic\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\nfalse positive rate\n\nfigure 15.2: roc curves for two classes of glass identification data\n\n "}, {"Page_number": 462, "text": "450\n\nchapter 15. prediction and classification\n\nwhich will be considered in the next sections.\nclassification rules; one does not dominate the others over the whole range.\n\nit is seen that there is no strict ordering of\n\nin applications, scores are found in various ways. with the optimal bayes classification\nin mind, the scores may be represented as differences or properties of discriminant functions.\nwith s(x) = d1(x)/d2(x) and dr(x) = f(x|r)p (r) one obtains\n\ns(x) > c \u21d4 f(x|1)p (1)\nf(x|2)p (2)\n\n\u2265 c.\n\nthe optimal bayes classification uses c = 1, yielding the minimal probability of misclassifica-\ntion. by rewriting the allocation rule as\n\ns(x) > c \u21d4 f(x|1)\nf(x|2)\n\n\u2265 c\n\np (2)\np (1) ,\n\none sees that varying c corresponds to varying the cut-off values for the proportion of densities.\nfor continuous x one obtains a closed curve, whereas for discrete x one obtains single points\nin the unit square that have to be connected to give a curve.\n\nas a curve, roc shows how the classification rate works for differing cut-offs. the higher\nthe area under the curve, the better the scoring or classification rule. therefore, the area under\nthe roc curve can be used as an indicator for the performance of a classification rule. given\nthat the curve is concave with end points (0,0) and (1,1), one often considers the area under the\nroc curve (auc):\n\n)\n\nauc =\n\nhr d(far) =\n\n)\n\n1\n\n0\n\nroc(t)dt\n\nor the area without the lower triangle, which gives the roc accuracy ratio:\n\n(cid:25))\n\n(cid:26)\n\n.\n\nroc accuracy ratio = 2\n\nhr d(far) \u2212 1\n2\n\nperfect discrimination rules have auc = 1; if the score has the same distribution in both\nclasses and therefore is uninformative, one obtains auc = 0.5. it can be shown that auc is\nequivalent to p (s1 > s0), where s1, s0 are scores for randomly drawn scores from classes 1 and\n0, respectively (see pepe, 2003). thus, auc represents the probability of correctly ordering\nthe classes.\n\nempirical roc curves that are based on estimated classification rules result from connect-\ning the finite number of points representing the hit rates and the false alarm rates for varying\ncut-off values c within a sample. one should keep in mind that the estimated curves are based\non a random sample. therefore they should not be taken at face value. campbell (1994) pro-\nposed simultaneous confidence intervals (see also jensen et al., 2000). in particular, when hit\nrates and false alarm values are estimated in the learning sample biased estimates are to be\nexpected.\n\nreceiver operating characteristic curves have found much attention in default risk mod-\nelling, since the basel committee on banking supervision stressed the supervisory risks as-\nsessment and early warning systems for banking institutions. for performance measures for\ncredit risk models see, for example, keenan and sobehart (1999) and sobehart et al. (2000).\nthe evaluation of tests in medicine has been considered by pepe (2003), zweig and campbell\n\n "}, {"Page_number": 463, "text": "15.4. parametric classification methods\n\n451\n\n(1993), lee and hsiao (1996), and campbell (1994). in applications it can be seen as a disad-\nvantage of roc curves and derived measures that performance is summarized over regions of\nthe curve that are not of interest. therefore, dodd and pepe (2003) proposed a partial auc,\nwhich restricts the curve to a relevant range of false alarm rates and gave a regression frame-\nwork for making inferences about covariate effects on the partial auc. an extension to ordinal\nclasses was given by toledano and gatsanis (1996).\n\n15.4 parametric classification methods\nin this section methods for obtaining estimated classification rules by use of parameterized\nclassifiers are considered. let xr1, . . . , xrnr , r = 1, . . . , k, denote the samples from classes\n1, . . . , k. if not stated otherwise, they stem from iid samples of (y, x) with sample size n =\nn1 + \u00b7\u00b7\u00b7 + nk.\n\n15.4.1 linear and quadratic discriminations\nplug-in rules\nit was shown in section 15.2 that if one assumes that predictors are normal with mean \u03bcr and\ncommon covariances matrix \u03c3 , the optimal bayes rule prefers class r over class s if\n\ndr(x) \u2212 ds(x) = \u03b20rs + xt \u03b2rs\n\n\u2265 0,\n\n(cid:14)\n\nwhere \u03b2rs = \u03c3\nin estimated rules one replaces \u03bcr by \u00afxr = 1\nnr\ncovariance matrix\n\n\u2212 \u03bcs) and \u03b20rs contains \u03bcr, \u03bcs and p(r), p(s) (see equation (15.7)).\nnr\ni=1 xri and \u03c3 by the pooled empirical\n\n\u22121(\u03bcr\n\nk(cid:7)\n\nnr(cid:7)\n\nr=1\n\ni=1\n\ns =\n\n1\nn \u2212 k\n\n(xri \u2212 \u00afxr)(xri \u2212 \u00afxr)t\n\nand thereby obtains \u02c6\u03b2rs and \u02c6\u03b20rs. the corresponding linear discrimination rule prefers class r\n\u2265 0. figure 15.3 shows the linear separation of observations from\nover class s if \u02c6\u03b20rs + xt \u02c6\u03b2rs\nthree classes that were generated from normal distributions.\n\nfigure 15.3: linear separation of observations from three classes.\n\n "}, {"Page_number": 464, "text": "452\n\nchapter 15. prediction and classification\n\nfisher\u2019s discriminant analysis\n\nthe linear classification rule obtained for normally distributed predictors with common covari-\nance can be motivated in a quite different way. fisher (1936) motivated linear discriminant\nanalysis by considering a criterion that separates two groups in a linear way. he proposed look-\ning for a linear combination y = at x of predictors such that the data are well separated in\ny-values. geometrically, the linear combination at x may be seen as a projection of obser-\nvations x on vector a. thus one is looking for directions that promise separation of classes.\nfigure 15.4 shows two projections of data, one that yields a perfect separation of classes and\none that does not separate classes perfectly. mathematically, the criterion to be maximized is\n\nq(a) =\n\n(\u00afy1 \u2212 \u00afy2)2\n1 + w2\nw2\n\n2\n\nunder a side constraint that fixes the length of a. typically one uses ||a|| = 1. in q(a) the\nvalues \u00afy1, \u00afy2 represent the means after projecting the observations from the two classes on a.\nwith yri = at xri being the linear combination of observation xri, one has\n\n\u00afyr =\n\n1\nnr\n\nyri =\n\n1\nnr\n\nat xri = at \u00afxr.\n\nnr(cid:7)\n\ni=1\n\nnr(cid:7)\n\nnr(cid:7)\n\ni=1\n\nnr(cid:7)\n\ntherefore \u00afyr is identical to the projection of the mean \u00afxr in class r. the values in the denomi-\nnator represent the empirical within-group variances after projection, with w2\n\nr given by\n\nw2\n\nr =\n\n(yri \u2212 \u00afyr)2 =\n\n(at xri \u2212 at \u00afxr)2.\n\ni=1\n\ni=1\n\nthe nominator of q(a) aims at separating the mean values of the classes, whereas the denomi-\nnator represents the variability within classes. thus two components are contained: separation\nof mean values and minimization of variability within classes. maximization of q(a) yields up\n\u22121(\u00afx1 \u2212 \u00afx2), which is the same as the weight vector in the nor-\nto constants the vector a = s\nmal distribution case with a common covariance matrix \u03c3 (exercise 15.6). however, fisher\u2019s\n\nfigure 15.4: two projections of data, one with a perfect separation of classes and one\nwith an imperfect separation.\n\n "}, {"Page_number": 465, "text": "15.4. parametric classification methods\n\n453\n\napproach does not assume normality, but starts from a plausible empirical criterion. it may also\nbe seen as motivation for the more general use of linear discriminant analysis, which is not\nrestricted to normal distributions. since the criterion is chosen in a sensible way, it also applies\nin cases where the distribution is non-normal, for example, in the mixed variables case, where\nmetric and binary predictors are included. indeed, linear discriminant analysis has been shown\nto be robust against deviations from the normal distribution.\n\nin the more general case with k classes, one maximizes the criterion\n\n(cid:14)\n\n(cid:14)\nr=1 nr(\u00afyr \u2212 \u00afy)2\n\nk\n\n,\n\nk\nr=1 w2\nr\n\n(cid:14)\n\n(cid:14)\n\nq(a) =\n\n(cid:14)\n\nwhere \u00afy = 1\nn\nan alternative way of presenting the criterion is\n\nnr\ni=1 at xri =\n\nk\nr=1\n\nk\n\nr=1(nr/n)at \u00afxr is the mean across all observations.\n\n(cid:14)\nr=1 nr(\u00afxr \u2212 \u00afx)(\u00afxr \u2212 \u00afx)t and w =\n\nq(a) = at ba\n(cid:14)\nat w a\n\nk\n\n,\n\n(cid:14)\n\nnr\n\nk\nr=1\n\ni=1(xri\u2212 \u00afxr)(xri\u2212 \u00afxr)t are the\nwhere b =\nbetween-group and within-group sums of squares. as a side constraint on a one typically uses\nat w a = 1. the extreme values are obtained by solving the generalized eigenvalue problem\nba = \u03bbw a, which yields the largest eigenvalue of the not-necessarily-symmetric matrix\n\u22121b. one obtains at most m = min{k \u2212 1, p} non-zero eigenvalues \u03bb1 \u2265 \u00b7\u00b7\u00b7 \u2265 \u03bbm with\nw\ni w aj = 0, i (cid:8)= j holds. therefore, they\ncorresponding eigenvalues a1, . . . , am, for which at\nare not orthogonal in the common sense but for a generalized metric that uses w (exercise\nin particular, a1 maximizes q(a). the canonical discriminant variables defined as\n15.7).\nur = at\nr x are used in classification by using the euclidean distance after projection. one\nclassifies into class r if\n\nm(cid:7)\n\ns=1\n\n{at\n\ns (x \u2212 \u00afxr)}2 = min\n\nj=1,...,k\n\n{at\n\ns (x \u2212 \u00afxj)}2.\n\nm(cid:7)\n\ns=1\n\nthus one chooses the class whose mean vector is closest to x in the space of canonical discrim-\ninant variables.\n\nquadratic discrimination\nfor normally distributed predictors x|y = r \u223c n(\u03bcr, \u03c3 r) the discriminant function dr(x) =\nlog(f(x|r)) + log(p(x)) has the form\nr (x \u2212 \u03bcr) \u2212 p\n\u22121\n2\n\nlog(|\u03c3 r|) + log(p(r)).\n\ndr(x) = \u22121\n2\n\nlog(2\u03c0) \u2212 1\n2\n\n(x \u2212 \u03bcr)t \u03c3\n\nestimated versions are obtained by using class-specific estimates \u02c6\u03c3 r (and estimates of \u03bcr).\nthe resulting classification rule is referred to as a quadratic discrimination since quadratic terms\nare contained in differences between estimated disciminant functions. figure 15.5 shows data\ngenerated from underlying normal distributions with different covariance matrices. the left\npanel shows the separation obtained from a linear discrimination rule, and the right panel shows\nthe estimated and the optimal classification regions for the quadratic discrimination rule.\n\nalthough quadratic discrimination is more flexible than linear discrimination, if the learning\nsample is not large, and the true variances are not very different, the linear rule often outper-\nforms the quadratic rule (e.g. marks and dunn, 1974). several approaches have been suggested\nto obtain a compromise between equal and unequal covariances by means of regularization; see\ncampbell (1980), friedman (1989), rayens and greene (1991), and flury (1986).\n\n "}, {"Page_number": 466, "text": "454\n\nchapter 15. prediction and classification\n\noptimale trenngrenze\ngesch\u00e4tzte trenngrenze, qda\n\nfigure 15.5: linear and quadratic separations of observations from two classes.\n\ndiagonal discriminant analysis\nlinear discrimination based on estimates of \u03c3 and \u03bc1, . . . , \u03bck, as well as on fisher\u2019s approach,\n\u22121, which might be quite un-\nrequire the computation of the inverse (estimated) covariance s\nstable when the number of predictors is large. simpler rules are obtained by assuming a more\nrestrictive distribution of x|y = r. by assuming x|y = r \u223c n(\u03bcr, \u03c3 d), where \u03c3 d is a\np), the optimal separation is linear, with differences of\ndiagonal matrix, \u03c3 d = diag(\u03c32\ndiscriminant functions dr(x) \u2212 ds(x) = \u03b20rs + xt \u03b2rs having the parameters\n\n1, . . . , \u03c32\n\np(cid:7)\n\n\u03b20rs = \u22121\n2\n\u03b2rsj = (\u03bcrj \u2212 \u03bcsj)2/\u03c32\nj .\n\n(\u03bcrj/\u03c3j)2 +\n\nj=1\n\n1\n2\n\np(cid:7)\n\nj=1\n\n(\u03bcsj/\u03c3j)2 + log(p(r)/p(s)),\n\nestimation of the parameters requires only that one obtains estimates of \u03bcr, . . . , \u03bck and \u03c32\n1, . . . ,\n\u03c32\np. although the underlying assumption seems artificial and overly simplistic, for classification\npurposes it turns out to be very stable in high-dimensional settings. dudoit et al. (2002) used the\nterm diagonal discriminant analysis and demonstrated that in gene expression data, which are\nnotoriously high-dimensional, large gains in accuracy may be obtained by ignoring correlations\nbetween genes.\n\nregularized discriminant analysis\nstability of the estimation of covariance matrices (or inverse covariance matrices) can also\nbe obtained by regularized estimates. one version of linear discriminant analysis uses ridge-\ntype estimates s + \u03bbi with tuning parameter \u03bb. a compromise between linear and quadratic\ndiscriminant analyses uses the regularized estimates of covariance matrices\n\n(1 \u2212 \u03bb) \u02c6\u03c3 r + \u03bb \u02c6\u03c3 ,\n\nwhere \u02c6\u03c3 is the pooled covariance matrix and \u02c6\u03c3 r are estimated class-specific covariances\n(friedman, 1989).\nin alternative approaches the pooled covariance matrix is replaced by a\nscaled version of the identity matrix. guo, hastie, and tibshirani (2007) considered the reg-\nularized estimate (1 \u2212 \u03bb) \u02c6\u03c3 + \u03bb\u02c6i and a version in which the correlation matrix is regular-\nized. a specific algorithm that uses the nearest shrunken centroids method (tibshirani, hastie,\n\n "}, {"Page_number": 467, "text": "15.4. parametric classification methods\n\n455\n\nnarasimhan, and chu (2003)) allows one to select variables. an overview of methods including\ndirect regularization of inverse covariance matrices is given by mkhadri et al. (1997).\n\n15.4.2 logistic discrimination\nit was shown in section 15.2 that the linear logistic model may be derived from the assumption\nof normally distributed predictors with a common covariance matrix. as in linear discrimina-\ntion, for example, by use of fisher\u2019s criterion, the separation of classes is based on a linear form\nof the predictors, although a non-linear transformation is used to transform the linear predictors\ninto probabilities. the logistic representation has several advantages. it holds not only when\npredictors are normal, but also if binary predictors follow a log-linear model and for other dis-\ntributions (day and kerridge, 1967; anderson, 1972). moreover, it offers a more direct way of\nestimating posterior probabilities and is easier to generalize.\n\nrather than replacing unknown covariances and means by estimates, logistic discrimination\ntypically is based on conditional maximum likelihood estimations, as outlined in section 4.1.\ntherefore one estimates parameters conditionally on predictors, which yields estimates that\nonly assumes that the logistic regression model holds, but not that the predictors are necessarily\nnormal. ml estimates are appropriate when the sample is drawn conditionally on the predictor\nor in total samples by conditioning on predictors. when the sample is conditional on classes,\none has to be careful with the intercepts. since priors cannot be estimated from samples that\nare stratified by classes, intercepts have to be corrected by known priors; see section 4.1 for\nestimating in stratified samples.\n\na problem that arises with maximum likelihood estimates is that they may become infinite.\nthat happens in particular if observations in some classes can be completely separated from\nothers. what comes as a problem when investigating the effect strength of predictors may be\nseen as an advantage when the objective is the separation of classes. however, the question\narises of whether perfect separation obtained in the learning sample generalizes to future data.\nby using shrinkage estimators (chapter 6), the problem of infinite estimates is easily avoided;\nfor example, zhu and hastie (2004) used a quadratically regularized estimate for the classi-\nfication of gene microarrays. for a discussion of the problem of \u03b2 \u2192 \u221e, see also albert\nand lesaffre (1986) and lesaffre and albert (1989). an early comparison of the efficiency\nof logistic discrimination compared to normal discriminant analysis is found in efron (1975).\nmclachlan (1992) gave a concise summary of logistic discrimination methods.\nlogistic discrimination may be extended to include more general forms by using the model\np (r|x) = exp(\u03b7r)/(1 +\nk\u22121\nj=1 exp(\u03b7j)), with \u03b7r having a more general form. the predic-\ntors may be specified as a parametric family of functions or may contain additive terms. the\nestimation of parameters (or functions of predictors) can be obtained by maximum likelihood\nmethods only when there are not too many predictors. an alternative approach that allows one\nto estimate parameters and simultaneously select variables is based on boosting techniques. al-\nthough these methods apply to parametric models, they are considered within a more general\nsetting in section 15.5.3.\n\n(cid:14)\n\n15.4.3 linear separation and support vector classifiers\nin the case of two classes, linear scoring rules have the form s(x) = \u03b20 + xt \u03b2, with classi-\nfication in class 1 if s(x) \u2265 0 and in class 2 otherwise. geometrically, the decision boundary\nh = {s(x) = 0} forms a (p \u2212 1)-dimensional separating hyperplane within the p-dimensional\npredictor space. for p = 2 this is a simple line. for two points x0 \u02dcx0 from hyperplane h one\nobtains (x0 \u2212 \u02dcx0)t \u03b2 = 0, and therefore \u03b2 is orthogonal to every vector in the hyperplane.\n\n "}, {"Page_number": 468, "text": "456\n\nchapter 15. prediction and classification\n\ni\n\n\u2217\n\ni + \u03b3i\u03b2/||\u03b2||, where xort\n\n= \u03b2/||\u03b2||) determines the orientation of\nthus \u03b2 (or, equivalently, the standardized value \u03b2\n0 \u03b2/||\u03b2|| = \u2212\u03b20/||\u03b2||, which shows\nthe hyperplane. for x0 \u2208 h one obtains xt\n\u2217\n= xt\n0 \u03b2\nthat the intercept determines the distance between the hyperplane and the origin. the orthog-\nonal distance \u03b3 between any point xi and the hyperplane can be found by considering the\ndecomposition xi = xort\nis the orthogonal projection of xi onto\ni = \u2212\u03b20 yields the distance\nthe hyperplane. multiplying both sides by \u03b2t and using \u03b2t xort\ni \u03b2)/||\u03b2|| = s(xi)/||\u03b2||. therefore, the scoring rule s(xi) is proportional to\n\u03b3i = (\u03b20 + xt\nthe distance between the point xi and the separating hyperplane. the distance is signed, taking\npositive and negative values depending on which side of the hyperplane the point is located.\nif classes 1 and 2 are represented by 1 and \u22121 the classification based on score s(x) =\n\u03b20 + xt \u03b2 can be given as \u03b4(x) = sign(\u03b20 + xt \u03b2) and an observation yi \u2208 {1,\u22121} is\ni \u03b2) \u2265 0, where the value yis(xi) measures the\nclassified correctly if yis(xi) = yi(\u03b20 + xt\ndistinctness of the classification. in the case where the training data (yi, xi), yi \u2208 {1,\u22121},\ni = 1, . . . , n, are separable, that is, all of them can be classified correctly, one may choose the\norientation of \u03b2 such that the distinctness (or margin) is maximized. one chooses \u03b20, \u03b2 with\ni \u03b2) \u2265 m. therefore, among the\n||\u03b2|| = 1 such that m is maximized subject to yi(\u03b20 + xt\n\u03b2s, for which yis(xi) > 0, i = 1, . . . , n, that one is chosen for which the distance between\nobservations and the hyperplane is at least m. a reformulation of the problem is\n\n||\u03b2||\n\nmin\n\u03b20,\u03b2\n\nsubject to yi(\u03b20 + xt\n\ni \u03b2) \u2265 1, i = 1, . . . , n\n\n(15.12)\n\n(cid:14)\n\n(exercise 15.10). if the observations are not separable in the feature space, the procedure has to\nbe modified by introducing so-called slack variables \u03be1, . . . , \u03ben and considering minimization\nof ||\u03b2|| subject to\n\nyi(\u03b20 + xt\n\ni \u03b2) \u2265 1 \u2212 \u03bei, i = 1, . . . , n,\n\n\u03bei \u2265 0,\n\n\u03bei \u2264 constant.\n\n(15.13)\n\n(cid:7)\n\ni\n\nthe resulting classifier is a so-called support vector classifier. the solution has the form \u02c6\u03b2 =\ni \u02c6\u03b1iyixi, where the estimated coefficients \u02c6\u03b1i are non-zero for only those observations for\nwhich the constraints are exactly met. these observations constitute the support vectors that\ngive the procedure its name. for details on how to find the solution, see, for example, hastie\net al. (2009).\n\nit is noteworthy that the support vector classifier uses a specific loss function. if yis(xi) <\n0, the observation is misclassified. the misclassification is more distinct if the value yis(xi)\nis small. therefore, the problem is to maximize yis(xi) or to minimize \u2212yis(xi), which is\nequivalent to minimizing the so-called hinge loss lh(y, s(x)) = 1 \u2212 ys(x). thus, the hinge\nloss is implicitly used in the minimization problem (15.12) that aims at maximizing the margin\nto obtain separated observations. for true values coded by {\u22121, 1}, the hinge or soft margin\nloss has the general form\n\nlh(y, \u02c6y) = 1 \u2212 y\u02c6yi(y\u02c6y < 1).\n\nwhen the expectation of the hinge loss is maximized over values \u02c6y \u2208 [\u22121, 1] one obtains the\noptimal prediction \u02c6y = sign(p (y = 1|x) \u2212 1/2), which is equivalent to the bayes classifier\n(exercise 15.11). therefore, for this loss function the bayes classifier can also be derived from\nreal-valued optimal predictions (see also section 15.5.3).\n\nin practice, one is not restricted to using the linear classifier s(x) = \u03b20 + xt \u03b2, but can\nenlarge the feature space using basis transformations. but perfect separation in constructed\nspaces may not generalize well. thus, even in enlarged spaces, the relaxed optimization prob-\nlem (15.13) is helpful. it can also be given in the form of a penalized estimate with some con-\nstant c, min\u03b20,\u03b2 ||\u03b2||2 + c\ni \u03b2) \u2265 1 \u2212 \u03bei, i = 1, . . . , n, \u03bei \u2265 0.\n\ni=1 \u03bei, subject to yi(\u03b20 + xt\n\n(cid:14)\n\nn\n\n "}, {"Page_number": 469, "text": "15.5. non-parametric methods\n\n457\n\nsince for fixed \u03b2 the optimal choice of \u03be is \u03be = lh(yi, s(xi)), the minimization can also\nbe formulated as the minimization of the hinge loss under regularization. more concretely,\none formulates it as a minimization of a regularized functional with solutions in a reproduc-\ning hilbert space, which is beyond the scope of this book (see, for example, blanchard et al.,\n2008). although the general support vector classifier is not a parametric classifier, it is included\nhere because of its strong connection to linear separation. support vector classifiers have been\nextensively studied in the machine learning community. steinwart and christmann (2008) col-\nlected their results in a book, and a short introduction from a statistical perspective is given by\nblanchard et al. (2008).\n\n15.5 non-parametric methods\nin the previous sections mostly classification rules were obtained by estimating parameters.\ntypically the parametrization is motivated by underlying distribution assumptions. non-para-\nmetric methods that do not rely on specific distributions provide more flexible classification\nrules that frequently show good performance if the complexity of the rule is restricted ade-\nquately.\n\n15.5.1 nearest neighborhood methods\nnearest neighborhood methods are non-parametric methods that require no model to be fit. the\nbasic concept uses only distances of observations in feature space and is due to fix and hodges\n(1951) (reprinted in agrawala, 1977).\nbased on a sample {(yi, xi), i = 1, . . . , n} and a distance in feature space d(x, \u02dcx), x, \u02dcx \u2208\nrp, one determines for a new observation x the k observation points that are closest in distance\nto x. this means that one seeks the nearest neighbors x(1), . . . , x(k) with\n\nd(x, x(1)) \u2264 \u00b7\u00b7\u00b7 \u2264 d(x, x(k)),\n\nwhere x(1), . . . , x(k) are values from the learning sample. with y(1), . . . , y(k) denoting the\ncorresponding classes, one classifies, using the majority vote rule, by\n\n\u02c6d(x) = r \u21d4 class r is the most frequent class in {y(1), . . . , y(k)}.\n\nin effect, for a given x, one looks within the learning sample for almost perfect matches of x\nand then classifies by using the class labels of these observations. in the extreme case k = 1,\none simply chooses the class from which the observation stems that is most similar to the value\nto be classified. if ties occur, they are broken at random. the resulting classifier is called the\nk-nearest-neighbor (k-nn) classifier.\n\nthe most simple and often surprisingly successful classification rule is 1-nn, where one\nchooses the class of the nearest neighbor.\nit has been shown that, asymptotically, the rule\nis suboptimal but with specific upper bounds. cover and hart (1967) gave an often referred to\nresult on asymptotic behavior. they show that the asymptotic error (n \u2192 \u221e) of 1-nn, denoted\nby \u03b5as(\u02c6\u03b41), and the optimal bayes error, denoted by \u03b5opt, fulfill\n\n\u03b5opt \u2264 \u03b5as(\u02c6\u03b41) \u2264 \u03b5opt(2 \u2212 k\n\nk \u2212 1 \u03b5opt),\n\nwhere k is the number of classes. the first inequality simply reflects that no classification\nrule can improve on the bayes error, while the second inequality gives an upper bound for the\n(asymptotic) error. for the two-class problems the upper bound is 2\u03b5opt(1\u2212\u03b5opt), and therefore\nit is always smaller than twice the optimal error.\n\n "}, {"Page_number": 470, "text": "458\n\nchapter 15. prediction and classification\n\nwhen using the nearest neighbor classification rule one has to choose k, the number of\nneighbors. one may expect that the rule improves with increasing k, although a very large\nk will result in a trivial classifier that does not depend on x. an interesting case is 2-nn\nas compared to 1-nn. in the 2-nn case one either has a majority of two (which yields the\nsame decision as 1-nn) or a tie, which will be broken at random. thus one might expect\nno improvement. indeed, the asymptotic error rates are the same. ripley (1996) shows more\ngenerally that, for the asymptotic error of the k-nn rule, denoted by \u03b5as(\u02c6\u03b4k), one has\n\n\u03b5as(\u02c6\u03b42k) = \u03b5as(\u02c6\u03b42k\u22121) \u2264 \u00b7\u00b7\u00b7 \u2264 \u03b5as(\u02c6\u03b42) = \u03b5as(\u02c6\u03b41).\n\nthus, increasing k can improve the error rate, and error rates for 2k-rules and (2k-1)-rules\nare identical for n \u2192 \u221e. a further result on large sample behavior was given by stone (1977).\nhe showed that the risk for the k-nn rule converges in probability to the bayes risk provided\nk \u2192 \u221e and k/n \u2192 0.\n\nthe distance measure d(., .), which is used when computing the nearest neighbors, should\nbe chosen carefully. for metric predictors, typically one uses the euclidean distance, after the\npredictors have been standardized to have mean 0 and variance 1. for binary variables and a\nmixture of different types of variables, other measures will be more appropriate. an alternative\nchoice of distance is based on the empirical correlation coefficient, used, for example, by dudoit\net al. (2002).\n\nwhen compared to alternative classifiers, the nearest neighborhood methods often perform\nremarkably well, even in high dimensions. in a comparison of several methods dudoit et al.\n(2002) used gene expression data with pre-selected variables ranging from p = 30 to p = 50.\nthe nearest neighborhood methods with cross-validatory choices of k did rather well when\ncompared to more sophisticated methods. for literature that deals with extended versions of\nnearest neighbor methods see section 15.11.\n\n15.5.2 random forests and ensemble methods\none of the most efficient classifiers that have been proposed in recent years are random forests\n(breiman, 2001a). random forests are an example of a wider group of methods, called ensem-\nble methods. when using ensembles one fits several models, for example, several trees, and lets\nthem vote for the most popular class. ensemble methods can be divided into two groups. in the\nfirst group the distribution of the training set is changed adaptively based on the performances\nof previous classifiers, whereas in the second group the distribution is unchanged. random\nforests are examples of the latter group. boosting methods, which will be considered in the\nnext section, are of the former type.\n\nbreiman (2001a) defines, rather generally, a random forest as a classifier consisting of a\ncollection of tree-structured classifiers where the parameters are iid random vectors. let us\nconsider some examples. in bagging (for bootstrap aggregation), proposed by breiman (1998),\ntrees are grown on bootstrap samples from the training set. a bootstrap sample is generated\nby uniform sampling from the training set with replacement. for each bootstrap sample one\nobtains a tree. an ensemble of trees may also be obtained by a random split selection (diet-\nterich, 2000). in a random split selection, at each node the split is selected from among the best\nsplits, where a fixed number of nsplit best splits is considered. for nsplit = 1 the ensemble\ncollapses and the original tree results. alternatively, one may select the training sets for the\ntrees by using random sets of weights on the observations in the training set or use a random\nselection of subsets of variables (e.g., ho, 1998).\n\nbreiman (1996a) favors random trees of the latter type, which use randomly selected inputs\n(or combinations of inputs) at each node to grow a new tree. the simplest version he considers,\n\n "}, {"Page_number": 471, "text": "15.5. non-parametric methods\n\n459\n\ncalled forest-ri for forests with random input, selects at random at each node a small group of\ninput variables to split on. trees are grown without pruning. he demonstrates that the method\ncompares favorably to methods based on reweighting sets like adaboost (see next section). in\nparticular, it was surprising that the selection of just one input variable at each node did very\nwell. improvement by selecting more variables was negligible. the more advanced version,\ncalled forest-rc for forests with random combinations, uses a linear combination of variables\nat each node. at a given node ni, combinations of combined input variables are selected\nwhere a combined input variable consists of nv randomly selected variables, added together\nwith coefficients that are uniform random numbers on [\u22121, 1]. the performance of the datasets\nconsidered by breiman (1996a) improves on forest-ri.\n\nin his paper breiman demonstrates not only that random forests work well; in addition he\ninvestigates the reason why that is so. in particular, he gives an upper bound for the gener-\nalization error. we will briefly summarize some of his results. let the classifier for a new\nobservation x be given by \u03b4\u03b8(x), where the vector \u03b8 contains the parameters that are randomly\ndrawn when generating a tree. for instance, in a random split selection based on the nsplit best\nsplits, the parameter contains the randomly drawn splits. in random forests the parameter \u03b8 is\ngenerated randomly, yielding a sequence \u03b81, \u03b82, . . . of iid parameters.\n\nthe margin function for a random forest is defined by\n\nmr(y, x) = p\u03b8(\u03b4\u03b8(x) = y ) \u2212 p\u03b8(\u03b4\u03b8(x) = \u02dcr),\n\nwhere \u02dcr = argmaxr(cid:5)=y p\u03b8(\u03b4\u03b8(x) = r) is the maximal probability for choosing a class that\nis not equal to y . mr(y, x) measures the extent to which the probability for the right class\nexceeds the probability for any other class. the strength of the random forest is the expectation\ns = ey,xmr(y, x). the margin function may also be written as\n\nmr(y, x) = e\u03b8{i(\u03b4\u03b8(x) = y ) \u2212 i(\u03b4\u03b8(x) = \u02dcr)},\n\nwhere i is the indicator function. by defining rmg\u03b8(y, x) = i(\u03b4\u03b8(x) = y ) \u2212 i(\u03b4\u03b8(x) = \u02dcr)\nas the raw margin function, the margin function is its expectation.\n\nbreiman (1996a) showed that the generalization error converges to py,x(mr(y, x) < 0) for\nalmost surely all sequences \u03b81, \u03b82, . . . , which explains why random forests are not apt to overfit\nas more trees are added. moreover, he showed that an upper bound for the generalization error\nis \u00af\u0001(1 \u2212 s2)/s2, where \u00af\u0001 is the mean correlation between the raw margin function of randomly\ngenerated trees indicated by \u03b8, \u02dc\u03b8 and s is the strength of the random forest. thus the bound of\nthe generalization error is mainly determined by the strength of the classifier in the forest and\nthe correlation between them measured is in terms of the raw margin function. thus strength\nand correlation yield guidelines for highly accurate random forests.\n\nin many studies random forests were shown to be among the best predictors. comparisons\nfor gene expression data were given by diaz-uriarte and de andres (2006b) and huang et al.\n(2005). moreover, random forests have a built-in selection procedure that selects the most\nimportant predictors. therefore, they can be also applied when the predictor space is high\ndimensional.\n\na disadvantage of random forests is that the contribution of single variables in the classifi-\ncation rule gets lost. importance measures try to measure this contribution for random forests.\na permutation accuracy importance measure, proposed by breiman (2001a), evaluates the dif-\nference in prediction accuracy between the trees based on the original observations and trees\nthat are built on data in which one predictor variable is randomly permuted, thereby breaking\nthe original association with the response variable. a disadvantage of the method might be\n\n "}, {"Page_number": 472, "text": "460\n\nchapter 15. prediction and classification\n\nthat the permutation also breaks the association with the other predictors. conditional variable\nimportance measures that try to avoid that effect were proposed by strobl et al. (2008).\n\n15.5.3 boosting methods\nin several chapters of this book boosting methods have been used for modeling purposes. how-\never, the roots of boosting techniques lie in the construction of accurate classification algo-\nrithms. therefore, in the following we consider basic boosting algorithms for classification.\nthe first well-established boosting algorithm, adaboost (freund and schapire, 1997), works in\nthe spirit of ensemble schemes. in contrast to the simple bagging strategy, it uses an adaptive\nupdating strategy for the weights in each step of the ensemble construction.\n\nlet a classifier based on learning set l have the form\n\n\u03b4(., l) : x \u2212\u2192 {1, . . . , k},\n\nx \u2212\u2192 \u03b4(x, l),\n\nwhere x denotes the space of covariates and \u03b4(x, l) is the predicted class for observation x.\n\nearly boosting approaches: adaboost\nin boosting the data are resampled adaptively and the predictors are aggregated by weighted\nvoting. the discrete adaboost procedure proposed by freund and schapire (1997) starts with\nweights w1 = \u00b7\u00b7\u00b7 = wnl = 1/nl. in the following we give the mth step of the algorithm (for\nthe case of two classes).\n\ndiscrete adaboost (mth step)\n\n1.\n\n(a) the current weights w1, . . . , wnl form the resampling probabilities. based on these\n\nprobabilities, the learning set lm is sampled from l with replacement.\n\n(b) the classifier \u03b4(., lm) is built based on lm.\n\n2. the learning set is run through the classifier \u03b4(., lm), yielding error indicators \u0001i = 1 if\n\nthe ith observation is classified incorrectly and \u0001i = 0 otherwise.\n\n3. with em =\n\nnl\n\ni=1 wi\u0001i, bm = (1 \u2212 em)/em and cm = log(bm), the resampling weights\n\n(cid:14)\n\nare updated for the next step by\n\nm(cid:14)\n\nwib\u0001i\nj=1 wjb\u0001j\nnl\n\nm\n\n(cid:14)\n\n=\n\nwi exp(cm\u0001i)\nj=1 wj exp(cm\u0001j) .\nnl\n\nwi,new =\n\nafter m steps, the aggregated voting for observation x is obtained by\n\n\"\n\nm(cid:7)\n\n#\n\nargmaxj\n\ncmi(\u03b4(x, lm) = j)\n\n.\n\nm=1\n\n(cid:14)\n\nwhile em is a weighted sum of errors, the parameters cm = log ((1 \u2212 em)/em) are log-\nodds comparing weighted hits to errors. it is easily seen that for the new weighting scheme\n\u0001i=0 wi,new = 0.5, and therefore in the next step the resampling\none has\nprobability put on the observations which have been misclassified in the mth step sums up\nto 0.5.\n\n\u0001i=1 wi,new =\n\n(cid:14)\n\n "}, {"Page_number": 473, "text": "15.5. non-parametric methods\n\n461\n\nthe algorithm as given above is based on weighted resampling. in alternative versions of\nboosting, the observations are not resampled. instead, the classifier is computed by weighting\nthe original observations by weights w1, . . . , wnl, which are updated iteratively. then \u03b4(., lm)\nshould be read as the classifier based on the current weights w1, . . . , wl (in the mth step).\nin the case of two classes, it is more common to use binary observations yi \u2208 {1, 0} or\n\u02dcyi \u2208 {1,\u22121} rather than class labels 1 and 2 as indicators. the coding \u02dcyi \u2208 {1,\u22121} is used\nespecially in the machine learning community. the class indicator y \u2208 {1, 2} transforms into\nthe binary case by using yi = 1 if yi = 1 and yi = 0 if yi = 2. the version \u02dcyi \u2208 {1,\u22121} is\nobtained from \u02dcyi = 2yi \u2212 1 .\nreal adaboost (friedman et al., 2000) uses real-valued classifier functions f(x, l) instead\nof \u03b4(x, l), with the convention that f(x, l) \u2265 0 corresponds to \u03b4(x, l) = 1 and f(x, l) < 0\ncorresponds to \u03b4(x, l) = 2.\n\nreal adaboost (mth step)\n\n1. based on weights w1, . . . , wnl the classifier \u03b4(., lm) is built.\n2. the learning set is run through the classifier \u03b4(., lm), yielding estimated class probabil-\n\nities p(xi) = \u02c6p (\u02dcyi = 1|xi).\n\n3. based on these probabilities a real-valued classifier is built by\n\nf(xi, lm) = 0.5 \u00b7 log\n\np(xi)\n1 \u2212 p(xi) ,\n\nand the weights are updated for the next step by\n\n(cid:14)\n\nwi,new =\n\nwi exp(\u2212\u02dcyif(xi, lm))\nj=1 wj exp(\u2212\u02dcyjf(xj, lm)) .\n\nnl\n\n(cid:14)\n\nm\n\nafter m steps the aggregated voting for observation x (for coding {1, \u20131}) is obtained\nm=1 f(x, lm)). in this version of real adaboost, either resampled observations or\nby sign(\nweighted observations may be used. the essential term in the updating is wiexp(\u2212\u02dcyif(xi, lm)),\nwhich, depending on hits (\u0001i = 0) and misclassifications (\u0001i = 1), has the form\n\n(cid:29)\n\nwiexp(\u2212\u02dcyif(xi, lm)) =\n\nwiexp(\u2212|f(xi, lm)|)\nwiexp(|f(xi, lm)|)\n\n\u0001i = 0\n\u0001i = 1.\n\nit is seen that for misclassified observations the weight wi is increased, whereas for correctly\nclassified observations wi is decreased. in order to ensure the existence of f(xi, lm), 1/nl is\nadded to the numerator and denominator of the fraction, yielding\n\nf(xi, lm) = 0.5 \u00b7 log\n\np(xi) + 1/nl\n1 \u2212 p(xi) + 1/nl\n\n.\n\nfunctional gradient descent boosting\nbreiman (1999) examined boosting from a game theoretical point of view and established for\nthe first time a connection between boosting and numerical optimization techniques. friedman\net al. (2000) elaborated these connections in more detail and established a close relationship\nbetween boosting algorithms and logistic regression. in friedman (2001), b\u00fchlmann and yu\n\n "}, {"Page_number": 474, "text": "462\n\nchapter 15. prediction and classification\n\n(2003), and b\u00fchlmann and hothorn (2007), the idea of boosting as an optimization technique\nin function space was investigated further. in the following we give the basic minimization\nconcept.\n\n(cid:14)\n\nfor a new observation (y, x) one considers the problem of minimizing e[l(y, g(x))] with\nrespect to g(x) for some specified loss function l(., .) and an appropriately chosen value\ng(x). a simple example is the squared error loss l2(y, g(x)) = (y \u2212 g(x))2, where g(x)\nis considered to represent an approximation of y. to obtain a practical implementation for\ni l(yi, g(xi))/nl.\nfinite datasets, one minimizes the empirical version of the expected loss,\nminimization is obtained iteratively by utilizing a steepest gradient descent approach. an\nessential ingredient of the method is the fitting of a structured function as an approximation to\ng(x). this fitting may be seen as a base procedure. depending on the modeling problem, one\nmay fit a regression tree or (in smoothing problems) a regression spline function. thus, in each\niteration step g(x) is approximated by an (parameterized) estimate \u02c6g(x,{ui, xi}) that is based\non input data {ui, xi}. the input data are not the original data but are generated during the\nfitting process by computing the derivatives\nui = \u2212 \u2202l(yi, g(x))\n\n|g(x)=g(xi),\n\ni = 1, . . . , n.\n\n(15.14)\n\n\u2202g(x)\n\nbasically the functional gradient descent algorithm consists of iteratively refitting these pseudo-\nresponse values. the final solution is attained in a stage-wise manner. we give in the following\na version of this algorithm, which is close to the gradient boost algorithm given in friedman\n(2001).\nlet \u02c6g(x,{ui, xi}) denote the base procedure at value x based on input data {ui, xi}, which\nare not necessarily the original data {yi, xi}.\n\nfunctional gradient descent boosting\n\nstep 1 (initialization)\n\ngiven the data {yi, xi}, fit a base procedure for initialization that yields the function es-\ntimate g(0)(x) = \u02c6g0(.,{yi, xi}). for example, a constant c is fitted, yielding g(0)(x) =\nargminc\n\ni=1 l(yi, c).\n\n(cid:14)\n\nn\n\n1\nn\n\nstep 2 (iteration) for l = 0, 1, . . .\n\n1. fitting step\n\ncompute the values of the negative gradient ui as given in (15.14), evaluated at\ng(l)(xi). fit a base procedure to the current data {ui, xi}. the fit \u02c6g(.,{ui, xi})\nis an estimate based on the original predictor variables and the current negative\ngradient vector.\n\n2. update step\n\nthe improved fit is obtained by the update\n\ng(l+1)(.) = g(l)(.) + \u03bd\u02c6g(.,{ui, xi}),\n\nwhere \u03bd \u2208 (0, 1] is a shrinkage parameter that should be sufficiently small.\n\nstep 3 (final estimator)\n\nobtain the final estimator after an optimized number of iterations lopt, that is, \u02c6g(lopt)(x).\n\n "}, {"Page_number": 475, "text": "15.5. non-parametric methods\n\n463\n\nlopt(cid:7)\n\nin contrast to the original version of gradientboost, this algorithm renounces an additional\nline search step between the fitting and the update step, which calibrates the shrinkage param-\neter \u03bd within each step. for obtaining an accurate estimate \u02c6g(lopt)(x) the constant \u03bd seems to\ndo well (see b\u00fchlmann and hothorn, 2007). however, the shrinkage parameter \u03bd plays an im-\nportant role in boosting algorithms. it makes the learner weak in the sense that within one step\nnot a perfect fit but a slightly better fit structured by the parametric learner is intended. small\nvalues of \u03bd have been shown to avoid early overfitting of the procedure. therefore, \u03bd should\nbe chosen rather small (e.g., \u03bd = 0.1). but its choice has to be balanced with the number of\niterations, because for very small values a large number of iterations is needed to obtain the\nbest results.\nan example for which the negative gradient has a very simple form is the squared error loss\nl2(y, g(x)) = (y \u2212 g(x))2. for the squared error loss the negative gradient vector consists\nof the simple residuals ui = \u2212\u2202l(yi, gi)/\u2202g = 2(yi \u2212 gi). therefore, in the fitting step,\na model is fit with the original responses replaced by the current residuals. essentially the\nsame procedure with just one boosting step was proposed already by tukey (1977). friedman\n(2001) noted that boosting by gradient descent is a stagewise strategy that is different from a\nstepwise approach that readjusts previously entered terms. since previously entered terms are\nnot adjusted, it is a greedy function approximation that performs forward stagewise additive\nmodeling. the additive fit becomes obvious from the final estimator, which has the form\n\n\u02c6g(lopt)(.) = \u02c6g0(.,{yi, xi}) +\n\n\u03bd\u02c6g(.,{u(j\u22121)\n\ni\n\n, xi}),\n\nj=1\n\nwhere u(j)\n\ni = \u2212\u2202l(yi, g(j)(xi))/\u2202g(x) are the negative gradient values from step j.\n\nfor the squared error loss, g(xi) is estimated as a direct approximation to e(yi|xi). how-\never, for a binary response the approximating function g(xi) may also denote an approxima-\ntion to a transformation of e(yi|xi). for binary responses, yi \u2208 {0, 1}, one often uses the logit\ntransformation. let \u03b7i = \u03b7(xi) = log(\u03c0i/(1 \u2212 \u03c0i)) denote the logits, let \u02dc\u03b7i = \u03b7i/2 denote the\nhalf-logits, and let \u02dcyi = 2yi \u2212 1 be the rescaled response (\u02dcyi \u2208 {\u22121, 1}). the corresponding\napproximation of yi is the probability\n\n\u03c0i = \u03c0(xi) =\n\nexp(\u03b7i)\n\n1 + exp(\u03b7i)\n\n=\n\nexp(\u02dc\u03b7i)\n\nexp(\u2212\u02dc\u03b7i) + exp(\u02dc\u03b7i) ,\n\nwhere \u03c0i denotes the probability of response one. then the usual negative binary log-likelihood\nfor binary data (or the half-deviance) \u2212li = \u2212{yi log(\u03c0i) + (1 \u2212 yi) log(1 \u2212 \u03c0i)} can be given\nas \u2212li = log(1 + e\n\u22122\u02dcyi \u02dc\u03b7i). it can be expressed as a loss function with\narguments yi and \u03c0(xi) as\n\n\u2212(2yi\u22121)\u03b7i) = log(1 + e\n\nllik(yi, \u03c0(xi)) = \u2212{yi log(\u03c0(xi)) + (1 \u2212 yi) log(1 \u2212 \u03c0(xi))},\n\nor with g(xi) set to be \u02dc\u03b7i in the form of a loss function with arguments \u02dcyi, g(xi):\n\nllik(\u02dcyi, g(xi)) = log(1 + e\n\n\u22122\u02dcyig(xi)).\n\nin the last form, which uses the superscript lik, the gradient descent algorithm implicitly fits a\nlogit model and g(xi) approximates the half-logits log(\u03c0i/(1\u2212 \u03c0i))/2. therefore, the approx-\nimation is on the level of the predictor after the logit transformation has been applied. when a\nstructured response is fitted to the gradient, for example, a linear term \u03b7i = xt\ni \u03b2, the gradient\ndescent algorithm fits a linear logit model. the use of \u02dc\u03b7i instead of \u03b7i and \u02dcyi \u2208 {\u22121, 1} instead\nof yi \u2208 {0, 1} is a convention that comes from the machine learning community and is useful\n\n "}, {"Page_number": 476, "text": "464\n\nchapter 15. prediction and classification\n\nto show the connection to other loss functions. it should be noted that for binary responses the\nlikelihood-based loss llik(yi, \u03c0(xi)) is equivalent to the logarithmic score, which can be seen\nas an empirical version of the kullback-leibler distance (section 15.1). an alternative loss\nfunction is the exponential loss,\n\nlexp(yi, gi) = exp(\u2212\u02dcyigi) = exp(\u2212(2yi \u2212 1)gi).\n\nthe role of gi in the exponential loss may be derived from looking at the population minimizer,\nwhich is given by\n\n\u2217\ng\n\n(x) = argming(x)ey|x(exp(\u2212\u02dcyg(x))) =\n\n1\n2\n\nlog( \u03c0(x)\n1 \u2212 \u03c0(x)\n\n) = \u02dc\u03b7(x).\n\ntherefore, gi corresponds again to the half-logits \u02dc\u03b7i as in the log-likelihood loss function llik\nand is on the level of the predictor in a logit model. it should be noted that the population\nminimizer of the likelihood-based loss llik, argming(x) = ey|xl(y, g(x)), is also \u02dc\u03b7(x). the\nexponential loss is just an alternative loss function that has the same population minimizer but\ndifferent computational properties. exponential loss is in particular of interest since it fills the\ngap between reweighting algorithms developed in the machine learning community and the\ngradient descent boosting algorithms, because it may be shown that adaboost is a boosting\nalgorithm that minimizes exponential loss (see friedman et al., 2000).\n\ntable 15.3: losses as functions of y \u2208 {0, 1} and the approximating \u03c0 and functions of\n\u02dcy \u2208 {\u22121, 1} and the approximating g(x).\n\n\u22122\u02dcy g(x))\n\nllik(\u02dcy, g(x)) = log(1 + e\nllik(y, \u03c0(x)) = \u2212{y log(\u03c0(x)) + (1 \u2212 y) log(1 \u2212 \u03c0(x))}\nlexp(\u02dcy, g(x)) = exp(\u2212\u02dcy g(x))\nlexp(y, \u03c0(x)) = ( 1\u2212\u03c0(x)\nl01(\u02dcy, g(x)) = i{\u02dcy g(x) \u2264 0} = i{sign g(x) (cid:8)= sign \u02dcy}\nl01(y, \u03c0(x)) = i{|y \u2212 \u03c0(x)| < 0.5}\n\n\u03c0(x) )(2y\u22121)/2\n\nwhen the focus is on classification, the loss functions should be evaluated with respect to\nthe underlying classification rule. thereby one should distinguish between y \u2208 {0, 1} and\n\u02dcy \u2208 {\u22121, 1}, which represent different scalings of the response. the approximation of \u02dcy may\nbe measured by g(x) while the approximation of y is usually measured by the corresponding\nprobability \u03c0(x) = exp(g(x)/(exp(g(x)) + exp(\u2212g(x)))). the classification rule for 0-1\nresponses is \u03b4(x) = 1 if \u03c0(x) > 0.5, and \u03b4(x) = 0 if \u03c0(x) < 0.5 and the corresponding 0-1\nloss is\n\nl01(y, \u03c0(x)) = i{\u03b4(x) (cid:8)= y} = i{|y \u2212 \u03c0(x)| < 0, 5}.\nfor the scaling \u02dcy \u2208 {\u22121, 1}, the classification rule may be given in the form \u03b4(x) = 1 if\ng(x) > 0, and \u03b4(x) = \u22121 if g(x) < 0 and the corresponding 0-1 loss is\n\nl01(\u02dcy, g(x)) = i{\u02dcyg(x) \u2264 0} = i{sign g(x) (cid:8)= sign \u02dcy}.\n\ntherefore, misclassification occurs only if one has \u02dcyg(x) \u2264 0, where \u02dcyg(x) is called a\n\"margin.\" for the \u02dcy-scaling, the margin plays an important role for the loss functions, since\n\n "}, {"Page_number": 477, "text": "15.5. non-parametric methods\n\n465\n\nlikelihood-based loss as well as exponential loss are monotone decreasing functions of the mar-\ngin. figure 15.6 shows the likelihood, exponential, and the 0-1 loss as a function of the margin\n\u02dcyg(x). in addition, we also show the hinge loss (1 \u2212 \u02dcyg(x))i(\u02dcyg(x) < 1) and the squared\nloss (\u02dcy \u2212 g(x))2 for \u02dcy = 1, since the losses in figure 15.6 may be seen as function of g(x)\nwhen \u02dcy = 1. table 15.3 collects the loss functions, given in the boosting form with \u02dcy-scaling,\nand the regression form with arguments y \u2208 {0, 1} and the approximating probability \u03c0(x).\n\nexponential loss\nbinomial deviance\nhinge loss\nl2\nbayes (0\u22121)\n\ns\ns\no\nl\n\n8\n\n6\n\n4\n\n2\n\n0\n\n\u22122\n\n\u22121\n\n0\n\ny~g\n\n1\n\n2\n\nfigure 15.6: loss functions l(\u02dcy, g(x)) as functions of \u02dcyg(x), \u02dcy \u2208 {\u22121, 1}. for\n\u02dcy = 1 they show the dependence on g(x).\n\nfriedman (2001) gave algorithms developed from gradient boosting for several specific loss\nfunctions. they include loss functions that are commonly used in the regression setting, like\nl2-loss, which means iteratively fitting the residuals, or huber-loss for robustification,\n\n(cid:29) |y \u2212 g(x)|2\n\nl(y, g(x)) =\n\n2\u03b4(|y \u2212 g(x)| \u2212 \u03b4/2)\n\nfor |y \u2212 g(x)| \u2264 \u03b4\notherwise.\n\nfigure 15.7 shows the l2-loss, the l1-loss, and the robust huber-loss.\n\nl2\nl1\nhuber\n\n\u03b4 = 1.3\n\ns\ns\no\nl\n\n0\n1\n\n8\n\n6\n\n4\n\n2\n\n0\n\n\u22123\n\n\u22122\n\n\u22121\n\n0\n\n1\n\n2\n\n3\n\nfigure 15.7: loss functions l(y, g(x)) as functions of y \u2212 g(x).\n\ny \u2212 g\n\n "}, {"Page_number": 478, "text": "466\n\nchapter 15. prediction and classification\n\nup until now it has not yet been specified how the improvement \u02c6g(.,{ui, xi}) is fitted\nwithin the functional gradient descent approach. in the regression setting with the squared error\nloss, the ui-values are the residuals 2(yi \u2212 gi), and it is straightforward to fit a regression\nmodel by using least-squares fitting where ui are the responses and xi are the covariates. for\nbinary observations the negative derivatives of the likelihood loss, \u2212\u2202llik(\u02dcyi, \u02dc\u03b7i)/\u2202\u03b7, are again\nresiduals of the form\n\n(cid:29)\n\n(cid:30)\n\n2\n\n(\u02dcyi + 1) \u2212\n\n1\n2\n\ne\u02dc\u03b7i\n\ne\u02dc\u03b7i + e\u2212\u02dc\u03b7i\n\n= 2{yi \u2212 \u03c0i}.\n\nin the same way as for metric responses, one can apply least-squares fitting with responses\n2{yi \u2212 \u02c6\u03c0i} (b\u00fchlmann and hothorn, 2007). an alternative approach that generalizes to ex-\nponential family response models uses a weighted update step that corresponds to a shrunk\nfisher-scoring step. the method is referred to as likelihood-based boosting and given in the\nnext section.\n\nlikelihood-based boosting\na particularly interesting case is likelihood-based boosting because it is strongly related to\nlikelihood-based inference as considered in most chapters. let yi be from an exponential family\ndistribution with mean \u03bci = e(yi|xi) and the link between the mean and the structuring term\nspecified in the usual form \u03bci = h(\u03b7i) or g(\u03bci) = \u03b7i with link function g. then the likelihood-\nbased generalized model boosting (genboost) tries to improve the predictor \u03b7(x) by greedy\nforward additive fitting. it has essentially the same form as gamboost for structured additive\nregressions as considered in section 10.3.4, but in the fitting step more general models can be\nused. the basic form of the algorithm (for simplicity without selection step) is\n\ngenboost\n\nstep 1 (initialization)\n\nfor the given data (yi, xi), i = 1, . . . , n, fit the intercept model \u03bc(0)(x) = h(\u03b70) by\nmaximizing the likelihood, yielding \u03b7(0) = \u02c6\u03b70, \u02c6\u03bc(0) = h(\u02c6\u03b70).\n\nstep 1 (iteration) for l = 0, 1, 2, . . . fit the model\n\n\u03bci = h(\u02c6\u03b7(l)(xi) + \u03b7(xi, \u03b3))\n\nto data (yi, xi), i = 1, . . . , n, where \u02c6\u03b7(l)(xi) is treated as an offset (fixed constant) and\nthe predictor is estimated by fitting the parametrically structured term \u03b7(xi, \u03b3), thus ob-\ntaining \u02c6\u03b3l. the improved fit is obtained by\n\n\u02c6\u03b7(l+1)(xi) = \u02c6\u03b7(l)(xi) + \u02c6\u03b7(xi, \u02c6\u03b3l),\n\n\u02c6\u03bc(l+1)\ni\n\n= h(\u02c6\u03b7(l+1)(xi)).\n\nthe term \u03b7(xi, \u03b3) in the fitted model represents the learner and is a structured function. in\nsection 10.3.4 the structuring was by one or higher dimensional smooth functions of predictors,\nand in section 6.3.2 linear functions were used. more generally, \u03b7(xi, \u03b3) can represent any\n\n "}, {"Page_number": 479, "text": "15.5. non-parametric methods\n\n467\n\nstructure one wants to fit, including, for example, trees. one more advantage of the general\nalgorithm is that alternative link functions can be used (not only the logit link as in logitboost\nproposed by friedman et al., 2000). moreover, the basic boosting algorithm also works in the\nsame way if the response is multivariate, but, of course, learners for the predictors have to be\nadapted.\n\nthe results of boosting depend in particular on the structuring of the predictor. in earlier sta-\ntistical boosting literature (breiman, 1998; friedman, hastie, and tibshirani, 2000; friedman,\n2001) carts were recommended. then the performance may depend on the tree depth and\nhigher tree depths may yield superior error rates; however, the results are hardly interpretable.\nby using stumps (trees with two terminal nodes), used by friedman et al. (2000) a decompo-\nsition of g(x) into univariate functions of each covariate is obtained. the advantage is that\nthe result can be visualized in a similar way as gam results. b\u00fchlmann and yu (2003) use\ncomponentwise smoothing splines as the base procedure for fitting and doing variable selection\nin additive models. tutz and binder (2006) extend a similar approach to gams.\n\nin recent years, many properties of boosting algorithms have been analyzed theoretically\nby both machine learning theorists and statisticians. among the most important results from\na statistical point of view are those of b\u00fchlmann and yu, 2003 for functional gradient descent\nboosting using the l2-loss. they verify a bias-variance trade-off, with the variance exponen-\ntially small increasing with the number of iterations l (b\u00fchlmann and yu, 2003). in addition,\nthey show that when a smoothing spline is used as the base procedure, the algorithm reaches\nthe optimal rate of convergence for a one-dimensional function estimation. furthermore, it\nis capable of capturing a higher degree of smoothness than the smoothing spline (b\u00fchlmann\nand yu, 2003). later, b\u00fchlmann (2006) investigated the aforementioned functional gradient\ndescent approach using the l2-loss and componentwise least-squares estimates as base pro-\ncedures. he showed that this algorithm provides a consistent estimator for high-dimensional\nmodels in which the number of covariates is allowed to grow exponentially with the sample\nsize under some sparseness assumptions.\n\nboosting approaches were shown to perform well in high-dimensional classification prob-\nlems where different phenotypes, mostly cancer types, are classified using microarray gene\nexpression data. while adaboost did not show superior performance in the comparison of\ndudoit et al. (2002), logitboost with simple modifications was very successful in the com-\nparison study of dettling and b\u00fchlmann (2003), where stumps were used as learners after the\npreselection of genes based on wilcoxon\u2019s test statistic.\n\nmultiple category case\nthe boosting approaches considered in the preceding sections mostly use a univariate response,\nwhich in classification means two classes. when the classes are given in k > 2 categories,\na one-against-all approach can be used (for example, dettling and b\u00fchlmann, 2003). let the\ndichotomous variable that distinguishes between category r and the other categories be given\nby y(r) = 1 if y = r and y(r) = 0 otherwise. the fitting procedures are applied to the binary\n, xi), yielding estimates of probabilities \u02c6p (r)(r|x) = p (r)(y(r) = 1|x) or\nresponse data (y(r)\ndichotomous classifiers \u02c6y(r)(x). classification rules are found by aggregating over splits. for\nthe fixed split that distinguishes between r and the rest, estimates of probabilities are\n\u02c6p (r)(y = j|x) = (1 \u2212 \u02c6p (r)(r|x))/(k \u2212 1),\nk(cid:7)\n\n\u02c6p (r)(y = r|x) = \u02c6p (r)(r|x),\n\nwhich are combined to\n\ni\n\n\u02c6p (y = r|x) =\n\n\u02c6p (j)(y = r|x)/k.\n\nj=1\n\n "}, {"Page_number": 480, "text": "468\nthe bayes rule applied to \u02c6p (y = r|x) yields the classifier. when dichotomous classifiers are\ngiven one can use the majority vote\n\nchapter 15. prediction and classification\n\n\u02c6y = r0,\n\nif\n\nr0 = argmaxr\n\n{\u02c6y(1)\nr + \u00b7\u00b7\u00b7 + \u02c6y(k)\n\nr\n\n}\n\n(15.15)\n\nover dummy variables\n\n\u02c6y(r)\nr (x) = \u02c6y(r)(x),\n\nj (x) = 1 \u2212 \u02c6y(r)(x),\n\u02c6y(r)\n\nj (cid:8)= r.\n\nthe rules are referred to as nominal aggregation (ensemble) rules, in contrast to the ordinal\naggregation rules considered in section 15.9. of course, if a multinomial or ordinal model is\nused in genboost, aggregation is not needed.\n\n15.6 neural networks\nneural networks were developed mainly in the machine learning community with some input\nfrom statisticians. they encompass many techniques, only part of them biologically motivated\nby the information processing in organisms. their motivation by biology and their performance\nin classification problems made them an attractive and somehow fancy tool. from a modeling\nviewpoint neural networks are just non-linear models. non-linearity is obtained by extracting\nlinear combinations of the inputs, which are then processed by use of non-linear transforma-\ntions. in that respect they are close to non-linear modeling by expansion in basis functions as\nconsidered in chapter 10. in this section only the basic concepts of neural networks, namely,\nfeed-forward networks and radial basis function networks, are considered. an extensive treat-\nment of neural networks in pattern recognition is found in ripley (1996) and bishop (2006).\n\n15.6.1 feed-forward networks\nfeed-forward networks may be seen as a general framework for non-linear functional mappings\nbetween the set of input variables x1, . . . , xp and the set of output variables y1, . . . , yq. feed-\nforward networks consist of successive layers of processing units where connections run only\nfrom every unit in one layer to every unit in the next layer. a simple example with only one\nintermediate layer of hidden units between input and output variables is given in figure 15.8.\n\nfigure 15.8: feed-forward network with one hidden layer.\n\n "}, {"Page_number": 481, "text": "15.6. neural networks\n\n469\n\nthe network has p inputs, k hidden units, and q output units. in addition, there is an input\nvariable x0 = 1 and a hidden unit z0 = 1 that represent the intercepts. in the context of neural\nnetworks these intercepts are called biases. the flow of information is from input to output\nunits, where the units are connected by specific transformations of linear terms. the basic\nprinciple with one hidden layer is given by the following two steps:\n\n1. connection between input and hidden units\n\nthe input of the hidden unit l is given as a linear combination of the input variables and\nan intercept (also called the bias). the output is obtained by a transformation of these\nlinear combinations given by\n\nzl = \u03c6(\u03b1l0 +\n\n\u03b1ljxj),\n\nwhere \u03c6 is a fixed transformation function called the activation function. the output zl\nof unit l is considered as the activation of this unit.\n\nj=1\n\n2. connection between hidden and output units\n\nthe input of the output unit yt is given in the same form by considering linear combina-\ntions of the outputs z1, . . . , zk of the hidden layer and an intercept. the output is obtained\nby using a fixed transformation or activation function \u02dc\u03c6 that yields\n\nyt = \u02dc\u03c6(wt0 +\n\nwtlzl).\n\np(cid:7)\n\nk(cid:7)\n\nl=1\n\nk(cid:7)\n\nthus two sorts of weights and two activation functions are involved. the \u03b1-weights determine\nthe input for the hidden units and are a combination of the input variables x1, . . . , xp. the\nw-weights determine the linear combination of the activations of the hidden units z1, . . . , zk.\nthe activation functions \u03c6 and \u02dc\u03c6 transform the input within the hidden and output layers, re-\nspectively. combining the two steps yields the form\n\nyt(x) = \u02dc\u03c6(wt0 +\n\nwtl\u03c6(\u03b1l0 +\n\n\u03b1ljxj)),\n\n(15.16)\n\nl=1\n\nj=1\n\nwhere yt(x), xt = (x1, . . . , xp), is the output resulting from input vector x.\n\nin the special case of a univariate response y (q = 1) in the weights wt0, . . . , wtk, the index\n\nt is omitted and one obtains\n\ny(x) = \u02dc\u03c6(w0 +\n\nk(cid:7)\n\nwl\u03c6(\u03b1l0 +\n\n\u03b1ljxj)).\n\n(15.17)\n\nl=1\n\nj=1\n\nup until now the activation functions \u03c6 and \u02dc\u03c6 have not been specified. one possibility for \u03c6 is\nthe heaviside function:\n\n(cid:29)\n\n\u03c6(a) =\n\na \u2265 0\n1\n0 a < 0.\n\nif it is used for the hidden units, these units behave as threshold units that send a signal or not.\nas the activation function in the output units, it makes sense only if the response variables are\n\np(cid:7)\n\np(cid:7)\n\n "}, {"Page_number": 482, "text": "470\ndichotomous with yi \u2208 {0, 1}. for continuous responses, sigmoid functions like the logistic\nfunction,\n\nchapter 15. prediction and classification\n\n\u03c6log(a) =\n\nor the \"tanh\" activation function,\n\n1\n\n1 + exp(\u2212a)\n\n=\n\nexp(a)\n\n1 + exp(a) ,\n\n\u03c6tan(a) = tanh(a) =\n\nexp(a) \u2212 exp(\u2212a)\nexp(a) + exp(\u2212a) ,\n\nare more appropriate. while \u03c6log(a) \u2208 [0, 1) one has \u03c6tan(a) \u2208 [\u22121, 1]. however, \u03c6log(a)\nand \u03c6tan(a) differ only through a linear transformation; more precisely, one has \u03c6tan(a/2) =\n\u03c6log(a) \u2212 1. therefore, networks based on \u03c6log or \u03c6tan are equivalent but have different values\nfor weights and biases. it should be noted that the activation functions are essential for making\nthe system identifiable. for example, if \u03c6 and \u02dc\u03c6 are chosen as linear functions or, simpler, as\n\u03c6(a) = \u02dc\u03c6(a) = a, one obtains y(x) = w0 +\nj=1 wl\u03b1ljxj), where the weights\non x1, . . . , xp are not unique since one may always choose, for example, \u02dcwl = 1, \u02dc\u03b1lj = wl\u03b1lj,\nand obtain wl\u03b1lj = \u02dcwl \u02dc\u03b1lj. moreover, if the activation functions are identity functions, the\nmodel collapses to a simple linear model.\n\nl=1(wl\u03b1l0 +\n\n(cid:14)\n\n(cid:14)\n\nh\n\np\n\nfor feed-forward networks, the estimation of parameters, also called the learning or training\nof the neural network, is usually based on a steepest gradient descent, which is called back-\npropagation in this setting. a detailed discussion of algorithms is given by ripley (1996),\nchapter 5.\n\n15.6.2 radial basis function networks\nradial basis function networks have the same architecture as feed-forward networks with one\nhidden layer, with the modification that no bias term is needed as input to the hidden units (see\nfigure 15.9). however, there are several modifications concerning the functional relationship\nbetween the layers. the essential difference is that the output of the hidden layers is not given\nas transformations of a linear input for unit l but has the form\n\nzl(x) = \u03c6l(x),\n\nfigure 15.9: radial basis function network.\n\n "}, {"Page_number": 483, "text": "15.7. examples\n\n471\n\nwhere \u03c6l is a radial basis function that is connected to unit l. the function \u03c6l is specific for\nunit l by using (unknown) parameters that characterize this unit. \u03c6l(x) has the general form\n\u03c6l(x) = \u03c6(x; \u03b8l), where \u03c6 is a function in x depending on the parameter vector \u03b8l. examples\nare the gaussian function\n\n(cid:25)\n\u2212(cid:16) x \u2212 \u03bcl\n2\u03c32\nl\n\n(cid:26)\n\n(cid:16)2\n\n\u03c6(x; \u03bcl, \u03c3l) = exp\n\nand localized basis functions of the type\n\n\u03c6(x; \u03b8l) = \u00af\u03c6((cid:16) x \u2212 \u03b8l (cid:16)),\n\nwhere \u00af\u03c6 is chosen by thin plate splines \u00af\u03c6(z) = z2 log(z), the linear function \u00af\u03c6(z) = z, or\n\u00af\u03c6(z) = (z2 + \u03c32)\u03b2, 0 < \u03b2 < 1, which, for \u03b2 = 1/2, yields the multi-quadratic function. based\non these basis functions the output is given as a linear combination:\n\nh(cid:7)\n\nyt(x) = wt0 +\n\nwtl\u03c6l(x).\n\nl=1\n\nthus the functional form of yt(x) is given as a linear weighted sum of basis functions, which\nis quite similar to the methods considered in chapter 10.\n\nradial basis functions may be motivated in different ways that yield specific basis functions.\na powerful approach is based on the theory of poggio and girosi (1990). an alternative moti-\nvation is by representing the distribution within classes as mixtures (see bishop, 2006,\nchapter 5).\n\nneural networks are strong competitors in classification problems. they are quite flexi-\nble and able to find complex classification regions, in contrast to simple rules like linear and\nquadratic discriminations. however, flexibility needs regularization; otherwise, overfitting with\nbad generalization errors can be expected. the training of neural networks is an art in itself.\nwith all the tuning parameters, like the number of hidden units, the architecture of the network\nitself (one or more layers), the choice of the activation function, and the placement of basis\nfunctions, neural networks become very flexible but also unstable. a more severe problem in\napplications is that they work like a black box. however, even though the prediction may be\nexcellent, practioners often do not like black boxes. they prefer to know how predictors enter\nthe prediction rule and how strong their effect is within the prediction rule. the implicit feature\nselection that is at work in neural networks makes interpreting of the impact of single variables\na challenge.\n\n15.7 examples\nin this section we give some applications of the methods that were discussed in the preceding\nsections. when comparing methods, fine tuning can often improve the performance of specific\nmethods considerably.\nin the following we use cross-validation for the selection of tuning\nparameters but otherwise use the default values of the used program packages. performance is\nevaluated in the test data; 50 random splits into training and test data were used.\n\nclassification typically focusses on misclassification error, although other loss functions\nmay be used in estimating the best empirical classifier. for example, support vector classifiers\nimplicitly use the hinge loss and adaboost minimizes exponential loss. when evaluating the\nperformance of classifiers, in particular if estimated probabilities are available, one should not\nrestrict consideration to simple misclassification errors but use alternative loss functions that\n\n "}, {"Page_number": 484, "text": "472\n\nchapter 15. prediction and classification\n\ninclude the precision of classification. in section 15.1, several loss functions were considered,\nin particular, the brier or quadratic score l2(y, \u02c6\u03c0) = (1\u2212\u02c6\u03c0y )2+\ni , and the logarithmic\nscore lkl(y, \u02c6\u03c0) = \u2212 log(\u02c6\u03c0y ). the former uses estimation of all the probabilities, whereas\nthe latter uses only the probability at the target value. but both use more information than the\nsimple classification decision.\n\ni(cid:5)=y \u02c6\u03c02\n\n(cid:14)\n\nexample 15.4: glass identification\nin example 15.2 the objective was the identification of types of glass based on the refractive index and\noxide content of various minerals like na, fe, k. figure 15.10 shows the performance of classifiers for two\nclasses (originally coded as types 1 and 2) and figure 15.11 for three classes (originally coded as types 1,\n2, and 7) in terms of misclassification errors and quadratic scores. the errors were computed for several\nsplits into learning and test sample. the functions lda and qda from the package mass were used for\nlinear and quadratic discriminations, glmnet was used to fit the lasso for the logit model (log), party was\nused to fit random forests (rf), mboost was used for boosting methods (bst), svm from e1071 was used for\nsupport vector machines (sv), and nnet was used for feed-forward networks. it is seen that in both cases\nthe quadratic discriminant analysis, although more flexible than the linear discriminant analysis, performs\nworse. the effect is even more distinct when the quadratic score is used to evaluate performance. the lasso\n\n5\n1\n\n.\n\n0\n\n0\n1\n\n.\n\n0\n\n5\n0\n\n.\n\n0\n\n0\n0\n\n.\n\n0\n\n6\n1\n\n4\n1\n\n2\n1\n\n0\n1\n\n8\n\n6\n\n4\n\n2\n\nlda\n\nqda\n\nlog\n\nrf\n\nbst\n\nnnet\n\nsv\n\nknn\n\nlda\n\nqda\n\nlog\n\nboost\n\nnnet\n\nsv\n\nfigure 15.10: misclassification rate (left) and quadratic score (right) in test samples for\ntwo classes of glass identification data.\n\n2\n1\n.\n0\n\n0\n1\n.\n0\n\n8\n0\n.\n0\n\n6\n0\n.\n0\n\n4\n0\n\n.\n\n0\n\n2\n0\n\n.\n\n0\n\n5\n3\n\n0\n3\n\n5\n2\n\n0\n2\n\n5\n1\n\n0\n1\n\n5\n\nlda\n\nqda\n\nrf\n\nnnet\n\nsv\n\nknn\n\nlda\n\nqda\n\nnnet\n\nsv\n\nfigure 15.11: misclassification rate (left) and quadratic score (right) in test samples for\nthree classes of glass identification data.\n\n "}, {"Page_number": 485, "text": "15.8. variable selection in classification\n\n473\n\n(log) and boosting (bst) for the logistic model with linear predictor are comparable to linear discriminant\nanalysis. the non-parametric methods, support vector machines (with gaussian kernel), random forests,\nand nearest neigbors (k = 5) show the best performance in terms of misclassification. the support vector\nmachine has the best performance in terms of squared error.\n\nexample 15.5: dlbcl data\nfor the illustration of high-dimensional classification problems we will use the dlbcl data available\nfrom the website http://www.gems-system.org/; see shipp et al. (2002). diffuse large b-cell lymphoma\n(dlbcl) is a lymphoid malignancy that is curable in less than 50% of patients. as predictors the ex-\npression of 6,817 genes in diagnostic tumor specimens from patients under treatment is available. the\nprediction target is to identify cured versus fatal or refractory disease. simple procedures, like linear or\nquadratic discriminant analysis, fail in high-dimensional datasets like this. therefore, regularized esti-\nmates and methods for high-dimensional classifications were applied by use of the bioconductor package\ncma (slawski et al. (2008)) and rda for regularized discriminant analysis. figure 15.12 shows the mis-\nclassification error for 25 random splits into training and test data. the methods used were lasso (las),\nridge, elastic net (en), regularized discriminant analysis with covariance matrix (rda), regularized discrim-\ninant analysis with correlation matrix (rdak), boosting (bst), nearest neighbors (knn), and random forests\n(rf). for this dataset interaction seems to be less important; knn and random forests, which are able to\nmodel interactions, perform worse than regularized discriminant analysis and ridge regression. classifi-\ncation procedures that select variables like elastic net and boosting show a slightly worse performance but\nare helpful to identify the genes that have discriminatory power.\n\n5\n2\n0\n\n.\n\n0\n2\n0\n\n.\n\n5\n1\n0\n\n.\n\n0\n1\n0\n\n.\n\n5\n0\n0\n\n.\n\n0\n0\n0\n\n.\n\nlas\n\nridge \n\nen\n\nrda\n\nrdak\n\nbst\n\nknn\n\nrf\n\nfigure 15.12: misclassification rate in test sample for dlbcl data.\n\n15.8 variable selection in classification\nin particular in high-dimensional settings, for example, in microarray-based prediction, where\nsimple classification rules fail, the selection of predictors is an important task. in microarray\ndata the focus is often on the selection of variables, which in that case correspond to genes.\nwhen one wants to identify the role of genes in distinguishing between diseases, prediction\naccuracy is often of minor importance. but careful selection of predictors is to be recommended\nnot only in the p > n case; for moderate numbers of predictors, too, the selection of relevant\nvariables tends to reduce noise and therefore improves prediction accuracy.\n\nkohavi and john (1998) distinguish between the wrapper and the filter approaches for se-\nlecting predictors. algorithms from the first class judge the usefulness of predictors for classi-\nfication based on the algorithm used for classification, whereas in the filter approach predictors\n\n "}, {"Page_number": 486, "text": "474\n\nchapter 15. prediction and classification\n\nare judged irrespectively of the classification algorithm. this means that the first variables are\nselected and then the selected variables are used for prediction.\n\npreselection of predictors\nin applications with some 10,000 variables it is common to do a preliminary selection of pre-\ndictors, in order to reduce the predictors to a number that can be handled by the classification\nprocedure. this filter approach is standard procedure in high-dimensional gene expression data.\nmost procedures are univariate, which means that the predictive strength of each variable is de-\ntermined by using that variable only. variables are ranked according to some criterion and the\nbest variables on the list are used in further analysis.\n\nseveral criteria have been proposed for continuous predictors. dudoit et al. (2002) used the\n\nratio of between-group to within-group sums of squares,\n\n(cid:14)\n(cid:14)\n\n(cid:14)\n(cid:14)\nr=1 i(yi = r)(\u00afxrj \u2212 \u00afx.j)\nr=1 i(yi = r)(xrj \u2212 \u00afxrj)\n\nk\n\nk\n\n,\n\nn\ni=1\nn\ni=1\n\nbssj\nw ssj\n\n=\n\nwhere \u00afx.j denotes the average of variable j across all samples and \u00afxrj denotes the average\nof variable j across samples belonging to class r. alternative criteria are the t-statistic and\nregularized variants, wilcoxon\u2019s rank sum statistic, the mann-whitney statistic, and heuristic\nsignal-to-noise ratios (golub et al., 1999b). for an overview on non-parametric criteria used\nin gene expression see troyanskaya et al. (2002). for an overview on multivariate selection\ncriteria for microarray-based classifiers see boulesteix et al. ().\n\nclassifier-based selection of predictors\nsome classification methods have a built-in selection procedure. for example, trees and random\nforests select predictors by construction. for other classification methods, like logistic discrim-\nination, one has to decide which predictors contribute to the prediction. for many procedures\nstepwise procedures based on test statistics (used in a forward or backward manner) have been\nproposed (see section 6.1 for stepwise procedures). better results are often obtained by using\nregularization methods like penalization (see chapter 6).\n\n15.9 prediction of ordinal outcomes\nwhen the categorical responses are ordinal the usual 0-1 loss, which is connected to the mini-\nmization of the misclassification error, is not appropriate because misclassification into adjacent\ncategories should be considered more acceptable than misclassification into more remote cate-\ngories. loss functions should take the ordinal nature of the response into account. when scores\ns1, . . . , sk can be assigned to the response categories 1, . . . , k the performance of classification\nrules can be evaluated by using loss functions for metric responses like the squared error in the\nform\n\nl(y, \u02c6y ) = (sy \u2212 s \u02c6y )2,\n\nwhere y denotes the true class and \u02c6y the estimated class. the simple score sr = r yields\nl(y, \u02c6y ) = (y \u2212 \u02c6y )2. then e(y |x) would be the best approximation to the response. how-\never, it is obvious that by computing the mean one uses more than the ordinal scale level; the\nresponses y are treated as metric, interval-scaled responses. therefore, if no scores are avail-\nable, the simple scores sr = r cannot be considered as appropriate for the ordinal scale level.\nusing misclassification errors and avoiding assigned scores does not solve the problem.\na simple example shows that misclassification cannot be the main objective when classes are\n\n "}, {"Page_number": 487, "text": "k(cid:7)\n\n\u03c0r|r \u2212 a|\n\n15.9. prediction of ordinal outcomes\n\n475\nordered. if one has ten categories with probabilities given by p (y = 1|x) = 0.28, p (y =\n8|x) = p (y = 9|x) = p (y = 10|x) = 0.24, misclassification is minimized by the bayes\nrule \u02c6y = 1. but the probability of a response in categories 8, 9 and 10 sums up to 0.72. of\ncourse, for unordered categories, adding of probabilities over classes would make no sense.\nthis is different for ordered classes. for ordered classes it is obvious that \u02c6y = 1 is a bad\nallocation rule; any value from 8, 9 and 10 would be more appropriate because the probability\nis concentrated in these categories. what may be learned from this simple example is that\nclassification should not be based on the bayes rule if the categories are ordered.\n\na more natural choice that takes the ordering of classes into account is the median of the\ndistribution of y over the ordered categories 1, . . . , k. in the example the median is class 8,\nwhich is close to the concentration of probability. in general, the median is strongly connected\nto the l1-norm l(y, \u02c6y ) = |y \u2212 \u02c6y |. the minimizer\n\nr0 = argmina e l(y, a) = argmina\n\nr=1\n\nyields the median of the distribution of y , which may be given as\n\nr0 = medy = argminr\n\n{\u03c0(r)|\u03c0(r) \u2265 0.5},\n\nwhere \u03c0(r) denotes the cumulative probability \u03c01 + \u00b7\u00b7\u00b7 + \u03c0r.\n\n15.9.1 ordinal response models\n\na simple way of utilizing the ordinal nature of the response is to use a parametric model for\nordinal responses (see chapter 9). a candidate is the cumulative-type model, which has the\nform p (y \u2264 r|x) = f (\u03b30r + xt \u03b3), where f is a fixed distribution function. the probabilities\nfor the response categories p (y = r|x) = f (\u03b30r + xt \u03b3) \u2212 f (\u03b30,r\u22121 + xt \u03b3) could be used\nto derive the bayes rule. better rules that take the ordering of categories into account can be\nderived from the underlying metric response. as shown in chapter 9, the cumulative model\ncan be derived from the assumption of an underlying latent variable that follows the regression\nmodel \u02dcy = \u2212xt \u03b3 + \u03b5, where \u03b5 is a noise variable with a continuous distribution function f .\nthen the responses are modeled by assuming\n\ny = r \u21d4 \u03b30,r\u22121 < \u02dcy \u2264 \u03b30r.\n\nwhen e(\u03b5) = 0 a simple classification rule that is obtained after replacing the parameters\n\nby estimates is\n\n\u02c6y = r \u21d4 \u02c6\u03b30,r\u22121 < \u2212xt \u02c6\u03b3 \u2264 \u02c6\u03b30r.\n\nthe rule implicitly uses the prediction of the latent variable \u2212xt \u02c6\u03b3 to derive where on the\nlatent scale the response is located. although it does not necessarily yield the median, it will\nfrequently be close to it. if e(\u03b5) = 0 does not hold, one has to include the mean into the\npredicted latent variable by using the estimate \u2212xt \u02c6\u03b3 + e(\u03b5). for the logistic (and probit)\ncumulative model e(\u03b5) = 0 holds, but not for models like the proportional hazards model.\nthe use of the logit model has been propagated by anderson and phillips (1981). of course\nalternative ordinal regression models like the sequential model can be be used to find estimates\nof probabilities that then are used to find the estimated median.\n\n "}, {"Page_number": 488, "text": "476\n\nchapter 15. prediction and classification\n\n15.9.2 aggregation over binary splits\nto avoid the use of artificially assigned scores but still use the ordering of responses one can\ncombine predictions that are obtained for splits within the categories 1, . . . , k. let the response\nbe split at category r into the sets {y \u2264 r} and {y > r}, r \u2208 {1, . . . , q = k \u2212 1}. the\ncorresponding dichotomous variables are\n\n(cid:2)\n\n1 y \u2264 r\n0 y > r,\n\ny(r) =\n\nwith the underlying probabilities given by \u03c0(r) = \u03c01 + \u00b7\u00b7\u00b7 + \u03c0r. for a fixed split at category r\nthe bayes rule is given by\n\n\u02c6y(r) = 1 \u21d0\u21d2 \u03c0(r) \u2265 1 \u2212 \u03c0(r).\n\nthe assignments resulting from the bayes rule applied to fixed splits can be combined into one\ndecision: select the category that is preferred for all splits. the resulting rule can be seen as a\nmajority vote over all splits. for the split at category r let the indicator functions be given by\n\ns = r + 1, . . . , k,\nif \u03c0(r) \u2265 0.5, which corresponds to prediction into categories {1, . . . , r}, and\n\ns = 1, . . . , r,\n\ns = 0,\ny(r)\n\ns = 1,\ny(r)\n\ns = 0,\ny(r)\n\ns = 1, . . . , r,\n\ns = 1,\ny(r)\n\ns = r + 1, . . . , k,\n\nif \u03c0(r) < 0.5, which corresponds to prediction into categories {r+1, . . . , k}. then the majority\nvote combines the decisions over all splits in the allocation rule\n\n\u02c6y = r0,\n\nif\n\nr0 = argmaxs\n\n{y(1)\ns + \u00b7\u00b7\u00b7 + y(k\u22121)\n\ns\n\n}.\n\n(15.18)\n\nin the ideal case, when the probabilities are known, the assignment resulting from the majority\nvote is into the median r0 = medy = argminr\n\n{\u03c0(r)|\u03c0(r) \u2265 0.5}.\n\nhowever, for estimated classification rules derived from dichotomization, the majority rule\nis an ensemble method, which only approximates the underlying median. then the probability\n\u03c0(r) is replaced by an estimate \u02c6\u03c0(r), which is obtained by fitting a binary response model\nto the data (yi(r), xi), i = 1, . . . , n. simple parametric models like the logit model can be\nused as binary response models. but non-parametric approaches like binary trees, random\nforests, and nearest neighborhood methods may also be applied. non-parametric methods do\nnot necessarily use the bayes rule based on estimates \u02c6\u03c0(r) but will directly yield assignments\ninto the split classes {1, . . . , r}, {r + 1, . . . , k}, which yields the indicator functions \u02c6y(r)\ns . the\ncorresponding ensemble is formed by the majority vote,\n{\u02c6y(1)\ns + \u00b7\u00b7\u00b7 + \u02c6y(k\u22121)\n\nr0 = argmaxs\n\n\u02c6y = r0,\n\n(15.19)\n\n}.\n\nif\n\ns\n\nwhen applying binary models one relies on the majority vote over binary splits. for fixed\nsplits, the allocation may be derived from the bayes rule based on the estimated probabilities\n\u02c6\u03c0(r). since the estimated probabilities \u02c6\u03c0(r) are based on the split at category r without us-\ning that the original response was in k categories, the resulting estimates do not have to fulfill\nthe natural order restriction \u02c6\u03c0(r) \u2264 \u02c6\u03c0(r + 1). therefore, there is no estimated vector of re-\nsponse probabilities for all categories and one does not necessarily assign into the class that\ncorresponds to the (estimated) median.\n\n "}, {"Page_number": 489, "text": "15.9. prediction of ordinal outcomes\n\n477\n\nperformance may improve by weighting the classification rules that result from dichotomiza-\ntion by taking into account where the split was made. for a fixed split at category r, one can\nuse\n\ns = 1/r,\n\u02c6y(r)\n\ns = 1, . . . , r,\n\n\u02c6y(r)\ns = 0,\n\ns = r + 1, . . . , k,\n\n(15.20)\n\nif the prediction is into categories {1, . . . , r} and\n\ns = 1/(k \u2212 r),\ny(r)\n\ns = 0,\ny(r)\n\ns = 1, . . . , r,\n\n(15.21)\nif the prediction is into categories {r + 1, . . . , k}. the weighting smoothes the 0-1-decision\nover the categories that are involved in the dichotomization. if estimates \u02c6\u03c0(r) are available,\ny(r)\ns\n\nin (15.20) and (15.21) can be replaced by the estimated probabilities\n\ns = r + 1, . . . , k,\n\n(cid:14)\n\ns = 1, . . . , r,\n\n\u02c6\u03c0(r)\ns = \u02c6\u03c0(r)/r,\n\ns = r + 1, . . . , k,\n\ns = (1 \u2212 \u02c6\u03c0(r))/(k \u2212 r),\n\u02c6\u03c0(r)\ns + \u00b7\u00b7\u00b7 + \u03c0(k\u22121)\n\n(15.22)\n} and also an ensemble-based\nyielding the majority vote r0 = argmaxs\nk\u22121\nr=1 \u02c6\u03c0(r)\nestimate of probabilities \u02c6\u03c0s =\ns .\nan alternative scheme to obtain estimated probabilities uses the estimates \u02c6\u03c0(1), . . . , \u02c6\u03c0(k \u2212\n1) obtained for the k \u2212 1 splits. after transforming them such that \u02dc\u03c0(1) \u2264 \u00b7\u00b7\u00b7 \u2264 \u02dc\u03c0(k \u2212 1), for\nexample, by monotone regression tools, one obtains estimated probabilities \u02c6\u03c0s = \u02dc\u03c0(s)\u2212 \u02dc\u03c0(s\u2212\n1), which can be directly used to predict and to compute the precision of the estimates. we will\nrefer to it as the monotonized ensemble method.\n\n{\u03c0(1)\n\ns\n\nas in unordered regression parametric regression models have the advantage that simple pa-\nrameter estimates show how the classification rule is obtained. however, parametric approaches\nare restricted to low dimensions. in terms of prediction, power non-parametric approaches often\nprevail over parametric models. the dichotomization approach has the advantage that all binary\nclassifiers are potential candidates. boosting methods, for example, a boosted logit model that\nautomatically selects predictors, can also be used. the latter has the advantage that it also ap-\nplies to high-dimensional data because of its built-in selection procedure. in most applications\nthe selected variables will vary across splits since the discriminatory power of variables usually\nvaries across splits.\n\nfor illustration several methods were applied to real datasets. we found that ensembles\nbased on (15.22) or the monotonized version did not differ strongly in terms of misclassifica-\ntion error. when performance is measured by taking the estimated probabilities into account the\nmonotonized version frequently shows better performance. therefore, in the following only the\nresults for the latter ensemble method are given. the classification methods considered com-\nprise parametric and non-parametric approaches. with the understanding that the ending e in\nthe notation refers to an ensemble that uses monotonized probabilities, the methods were cum:\ncumulative logit model (function polr from package mass); glme: binary logit model ensem-\nble (function glm); lassoe: lasso ensemble (function penalized from package penalized);\nbooste: logit boost ensemble (function glmboost from package mboost); wnn: weighted\nnearest neighbors (k = 7, r function kknn); wnne: weighted nearest neighbors ensemble;\nlda: linear discriminant analysis (function lda from package mass); ldae: linear discrim-\ninant analysis ensemble; rf: random forests (r function randomforest); and rfe: random\nforests ensemble.\n\nthe datasets were repeatedly split into learning and test set, and classification based on the\nmedian was applied to the learning set and accuracy of prediction evaluated in the test set. an\nimportant measure of performance is misclassification error. since responses are ordered, one\ncan also consider the distance between the prediction and the true value, although one implicitly\nuses a higher scale level than ordinal. a measure that only uses the ordinal scale level is the\nranked probability score given in (15.3). the major advantage of the latter measure is that\n\n "}, {"Page_number": 490, "text": "478\n\nchapter 15. prediction and classification\n\nm\nu\nc\n\ne\nm\nl\ng\n\ne\no\ns\ns\na\nl\n\ne\nt\ns\no\no\nb\n\nn\nn\nw\n\ne\nn\nn\nw\n\na\nd\nl\n\ne\na\nd\nl\n\nf\nr\n\ne\nf\nr\n\nm\nu\nc\n\ne\nm\nl\ng\n\ne\no\ns\ns\na\nl\n\ne\nt\ns\no\no\nb\n\nn\nn\nw\n\ne\nn\nn\nw\n\na\nd\nl\n\ne\na\nd\nl\n\nf\nr\n\ne\nf\nr\n\n0.80\n\n0.78\n\n0.76\n\n0.74\n\n0.72\n\n0.70\n\n0.68\n\n1.4\n\n1.3\n\n1.2\n\n1.1\n\n0.90\n\n0.88\n\n0.86\n\n0.84\n\n0.82\n\n0.80\n\n0.78\n\n0.76\n\nm\nu\nc\n\ne\nm\nl\ng\n\ne\no\ns\ns\na\nl\n\ne\nt\ns\no\no\nb\n\nn\nn\nw\n\ne\nn\nn\nw\n\na\nd\nl\n\ne\na\nd\nl\n\nf\nr\n\ne\nf\nr\n\nfigure 15.13: misclassifications, absolute differences, and ranked probability scores in\ntest data for the satisfaction data.\n\nit not only measures the correctness of the prediction but, by using the distance between the\nprediction and the estimated distribution over categories, it also takes the distinctness of the\nprediction into account.\n\nexample 15.6: satisfaction with life\nthe data stem from a household panel (german socio-economic panel, 2007). the response is the sat-\nisfaction with life in six ordered categories; 17 predictors were used, 4 of which were metrically scaled.\ntwenty splits into learning and test data were performed, and the sample size of the learning data was\n400. figure 15.13 shows the misclassification rates, the absolute differences between prediction and true\n\n "}, {"Page_number": 491, "text": "15.9. prediction of ordinal outcomes\n\n479\n\nvalue, and the ranked probability scores. it is seen that the absolute difference distinguishes stronger be-\ntween classification methods than the misclassification rate. classifiers that are not designed for ordinal\nresponses, like nearest neighbors, linear discriminant analysis, and random forests, improve strongly in\nterms of the absolute differences if the ensemble method is used. interaction seems not to be of major\nimportance because linear approaches perform quite well. approaches that include variable selection,\nlike the lasso and the boosted logit models, do not outperform other procedures in terms of misclassifi-\ncation but show better performance in absolute differences. in terms of the distinctness of the predicted\ndistribution, which is measured by the ranked probability score, nearest neighbor approaches in particular\nshow poor performance. by construction, nearest neighbors focus on exact predictions but yield inferior\ndistributions, and therefore poor performance is to be expected.\n\nm\nu\nc\n\ne\nm\nl\ng\n\ne\no\ns\ns\na\nl\n\ne\nt\ns\no\no\nb\n\nn\nn\nw\n\ne\nn\nn\nw\n\na\nd\nl\n\ne\na\nd\nl\n\nf\nr\n\ne\nf\nr\n\n0.24\n\n0.22\n\n0.20\n\n0.18\n\n0.16\n\n0.14\n\n0.12\n\n0.16\n\n0.14\n\n0.12\n\n0.10\n\nm\nu\nc\n\ne\nm\nl\ng\n\ne\no\ns\ns\na\nl\n\ne\nt\ns\no\no\nb\n\nn\nn\nw\n\ne\nn\nn\nw\n\na\nd\nl\n\ne\na\nd\nl\n\nf\nr\n\ne\nf\nr\n\nfigure 15.14: absolute differences and ranked probability scores in test data for housing\ndata.\n\nexample 15.7: housing data\nin the housing data, one wants to predict the price by using continuous and categorical predictors. the\ndataset is available from the uci repository (http://archive.ics.uci.edu/ml). four ordered categories of\nprices are considered with predictors crim: per capita crime rate by town, zn: proportion of residential\nland zoned for lots over 25,000 sq. ft., indus: proportion of non-retail business acres per town, chas:\ncharles river dummy variable, nox: nitric oxides concentration (parts per 10 million), rm: average num-\nber of rooms per dwelling, age: proportion of owner-occupied units built prior to 1940, dis: weighted\n\n "}, {"Page_number": 492, "text": "480\n\nchapter 15. prediction and classification\n\ndistances to five boston employment centres, rad: index of accessibility to radial highways, tax: full-\nvalue property-tax rate per 10, 000, ptratio: pupil-teacher ratio by town 12, b: 1000(bk \u2212 0.63)2,\nwhere bk is the proportion of blacks by town, and lstat: percent lower status of the population. the\ndataset, which comprises 506 observations, was split such that the training data have 400 observations.\nfigure 15.13 shows the performance of the classifiers. we omit the error rate because the results are very\nsimilar to the results for the absolute differences. for this dataset random forests are dominating, which\nmay be due to interaction effects. the transition to ensemble methods shows only slight improvement,\nand for nearest neighbors there is even a small increase in terms of absolute error.\n\n15.10 model-based prediction\nin the preceding sections the focus was on classification, where the response is in one of k\ncategories. in this section we will consider briefly the use of parametric models for general\nresponses. when a parametric or non-parametric model for the mean \u03bc = h(x; \u03b2) is fit, pre-\ndiction of the (univariate) response y0 at a new observation x may be constructed as\n\n\u02c6y0 = h(x; \u02c6\u03b2).\n\nwhen \u02c6y0 is not a valid response value, that is, not integer-valued, one can select the value\n\u02c6y0 that is closest to h(x; \u02c6\u03b2). that rule has been used in classification but also applies for\ncount data, where y \u2208 {0, 1, . . .}. if one assumes for the response y given x a specific density\nf(y; \u03b3(x)) that depends on parameter \u03b3(x), the whole response distribution is determined when\na parameter estimate \u02c6\u03b3(x) is found. for example, in a generalized linear model, the parameters\nthat determine \u03b3(x) are \u03b2 from the linear predictor \u03b7 = xt \u03b2 and a dispersion parameter \u03c6,\nyielding the estimated density f(y; \u02c6\u03b2, \u02c6\u03c6). then a prediction \u02c6y0 can be determined by the mean,\nthe median, or the mode of the estimated distribution.\n\nwhen a specific distribution is assumed, performance can be measured by distance mea-\nsures between the true and the estimated distributions, for example, kullback-leibler loss and\nlikelihood-based loss. however, these distance measures work only under the assumption that\nthe underlying distribution is true. then, they can be used, for example, to evaluate the impact\nof predictors. if one has major doubts about the assumed distribution, it is preferable to use\nmeasures that can be interpreted without reference to a specific distribution, for example, the\nsquared loss, the l1-norm, and measures based on the ranked probability score.\n\nexample 15.8: demand for medical care\nin chapter 7, several models for the medical care data were fitted, in particular, a poisson model, a negative\nbinomial model, a zero-inflated poisson model with only the intercept for the mixture (zero1), a zero-\ninflated poisson model with predictors in the mixture (zero2), a hurdle model with only the intercept for\nthe mixture (hurdle1), and a hurdle model with predictors in the mixture (hurdle2). here, only males with\nresponses smaller than 30 are included. the dataset was split various times with 600 men in the learning\nsample; prediction was investigated in the test sample. figure 15.15 shows the results in terms of absolute\nvalues of differences between the medians of the estimated distributions and the actual observation, and\nthe ranked probability score. it is seen that the negative binomial model dominates the other procedures\nin terms of the absolute difference; the zero inflation and hurdle models improve when covariates that\ndetermine the mixture are included. in terms of the ranked probability score, which also includes the\nconcentration of the predicted distribution, the negative binomial model dominates the poisson model\nmore distinctly, but now the hurdle models are the best performers. in the whole dataset the negative\nbinomial model has the smallest aic (value of 9291) followed by the zero inflation model and the hurdle\nmodel with covariates (value of 11 014).\n\n "}, {"Page_number": 493, "text": "15.11. further reading\n\n481\n\npoisson\n\nnegative\nbinomial\n\nzero\n\nzero\n\ninflated 1\n\ninflated 2\n\nhurdle 1\n\nhurdle 2\n\n7\n.\n3\n\n6\n.\n3\n\n5\n.\n3\n\n4\n.\n3\n\n9\n\n.\n\n2\n\n8\n\n.\n\n2\n\n7\n\n.\n\n2\n\n6\n\n.\n\n2\n\n5\n\n.\n\n2\n\n.\n\n4\n2\n\npoisson\n\nnegative\nbinomial\n\nzero\n\nzero\n\ninflated 1\n\ninflated 2\n\nhurdle 1\n\nhurdle 2\n\nfigure 15.15: absolute differences and ranked probability scores in test data for medical\ncare data.\n\nfurther reading\n\n15.11\nbooks. statistical approaches to classification are well covered in mclachlan (1992) and ripley\n(1996). bishop (2006) is guided by pattern recognition and machine learning, hastie et al.\n(2009) considered regression and classification from a general statistical learning viewpoint.\n\nsurveys on high-dimensional classifications. classification methods in high-dimensional\nsettings were compared by dudoit et al. (2002), romualdi et al. (2003), lee et al. (2005),\nstatnikov et al. (2005), and diaz-uriarte and de andres (2006b). an overview on the evaluation\nof microarray-based classifiers were given by boulesteix et al. ().\n\nnearest neighborhood methods. neighborhood methods with weighted neighbors were\nproposed by morin and raeside (1981), parthasarthy and chatterji (1990), and silverman and\njones (1989). paik and yang (2004) introduced a method called adaptive classification by mix-\ning (acm). they used combinations of many k-nn classifiers with different values for k and\ndifferent subsets of predictors to improve the results of one single k-nn prediction. friedman\n(1994) proposed local flexible weights for the predictors to account for their local relevance that\nis estimated by recursive partitioning techniques. alternative approaches which also choose the\nmetric adaptively were given by hastie and tibshirani (1996), and domeniconi et al. (2002).\na connection between these adaptively chosen metrics and random forests was derived by lin\n\n "}, {"Page_number": 494, "text": "482\n\nchapter 15. prediction and classification\n\nand jeon (2006). combinations of several neighborhood estimators in an ensemble were con-\nsidered by domeniconi and yan (2004) and gertheiss and tutz (2009a). more detailed large\nsample results as well as an overview on further methods of nearest neighborhoods were given\nin ripley (1996).\n\nordinal prediction the use of parametric models was investigated by anderson and phillips\n(1981), campbell and donner (1989), campbell et al. (1991), bell (1992), and rudolfer et al.\n(1995). comparisons of methods are also found in demoraes and dunsmore (1995) and coste\net al. (1997). piccarreta (2008) and frank and hall (2001) proposed using classification trees.\nensemble methods came up more recently; see tutz and hechenbichler (2005) for boosting\napproaches in ordinal classification. some approaches have been developed in the machine\nlearning community; chu and keerthi (2005) proposed support vector classification tools, and\nherbrich et al. (1999) investigated large margin boundaries for ordinal regression.\n\nclassifiers using r. the package mass contains the functions lda and qda for linear and\nquadratic discriminations, party provides random forests, and mboost boosting methods; sup-\nport vector algorithms are available in e1071. feed-forward networks can be fitted by use of\nnnet, regularized discriminant analysis by use of rda. most of the methods considered in this\nchapter are also accessible in an r bioconductor package called cma (slawski, daumer, and\nboulesteix, 2008).\n\n15.12 exercises\n\n15.1 let the optimal prediction be defined by \u02c6yopt = argmincl(y, c) for the loss function l(., .). show\nthat one obtains as minimizers\n\n(a) \u02c6yopt = e(y|x) for the quadratic loss,\n(b) \u02c6yopt = med(y|x) for the l1-norm,\n(c) the p-quantile \u02c6yopt = inf{y0|p (y \u2264 y0|x) \u2265 p} for the loss function lp(y, \u02c6y) = |y \u2212 \u02c6y| +\n(2p \u2212 1)(y \u2212 \u02c6y), with p \u2208 (0, 1).\n\n15.2 let \u02c6\u03c0t = (\u02c6\u03c01, . . . , \u02c6\u03c0k) denote an estimated vector of probabilities and yt = (y1, . . . , yk), yr \u2208\n{0, 1} a new observation. show that for the actual prediction error of the brier score\n\n(cid:11)\n\nk(cid:8)\n\n(cid:12)\n\nk(cid:8)\n\n(yr \u2212 \u02c6\u03c0r)2\n\n= l2(\u03c0, \u02c6\u03c0) +\n\nvar(yr)\n\nr=1\n\nr=1\n\ne(l2(y, \u02c6\u03c0)) = e\n\nholds.\n\n15.3 for fixed classifier \u03b4 the prediction error for randomly drawn (y, x) with 0-1 loss is defined as\n\u03b5 = e l01(y, \u03b4(x)). derive the connection to the error rate of misclassification conditional on x defined\nas \u03b5(x) = ey|x l01(y, \u03b4(x)) and the connection to the individual errors \u03b5rs = p(\u03b4(x) = s|y = r).\n15.4 in example 15.3 the probabilities are given by p (x = 1|y = 1) = 0.95, p (x = 1|y = 2) = 0.05,\nwhere y = 1 refers to user, y = 2 to non-user, and x = 1 to positive test results and x = 0 to negative\ntest results.\n\n(a) now let the prior be given by p (1) = 0.05. compute the bayes rule, the conditional error rates\n\u03b5(x), \u03b512, \u03b521, and the global error rate \u03b5(x). discuss the accuracy of the predictions in terms of\noverall accuracy and conditional accuracy.\n\n(b) what are the results for prior probability p (1) < 0.05?\n\n "}, {"Page_number": 495, "text": "15.12. exercises\n\n483\n\n15.5 there is a strong connection between the logit model and normally distributed predictors.\n\n(a) show that under the assumption of normally distributed predictors, x|y = r \u223c n (\u03bcr, \u03c3 ), one\nobtains for the posterior probabilities the linear multinomial logistic model for k classes p (r|x) =\nk\u22121\nj=1 exp(\u03b20j + xt \u03b2j). give the parameters \u03b20r, \u03b2r as functions of\nexp(\u03b20r + xt \u03b2r)/1 +\n\u03bcr, \u03c3 .\n(b) derive the logistic model for p (r|x) that holds if the predictors follow x|y = r \u223c n (\u03bcr, \u03c3 r).\n\n(cid:2)\n\n(cid:2)\n\n(cid:2)\n\n15.6 fisher\u2019s discriminant analysis for two classes uses the projections yri = at xri and maximizes the\ncriterion q(a) = (\u00afy1 \u2212 \u00afy2)2/(w2\n\ni=1(yri \u2212 \u00afyr)2.\n\nnr\ni=1 yri/nr, w2\n\n2) with \u00afyr =\n\n1 + w2\n\nr =\n\nnr\n\n(a) show that maximization of q(a) yields a solution that is proportional to a = s\n(b) show that the criterion for two classes is equivalent to the general criterion q(a) = at ba/at w a,\n\nk\n\nr=1 nr(\u00afxr \u2212 \u00afx)(\u00afxr \u2212 \u00afx)t , w =\n\nk\nr=1\n\ni=1(xri \u2212 \u00afxr)(xri \u2212 \u00afxr)t .\n\nnr\n\nwhere b =\n\n(cid:2)\n\n(cid:2)\n\n(cid:2)\n\n\u22121(\u00afx1 \u2212 \u00afx2).\n\n15.7 the fisher criterion that is to be maximized in the general case of k classes has the form q(a) =\nat ba/at w a, under the side constraint ||at w a|| = 1.\n\n(a) why is a side constraint needed? what are alternative choices for the side constraint and how does\n\nthe choice affect the maximization procedure?\n\n(b) use that there exists an invertible matrix h such that w = hh t , b = h\u03c9h t , with \u03c9 =\ndiag(\u03bb1, . . . \u03bbp), \u03bb1 \u2265 . . . \u2265 \u03bbp \u2265 0. show that the first m = min{k \u2212 1, p} columns of\n\u22121 are solutions of the generalized eigenvalue problem q(ai) = max q(a), where a is\n(h t )\nj w a = 0, j = 1, . . . , i\u2212 1 (for the existence of the matrices h and\nrestricted by at w a = 1, at\n\u03c9 see harville, 1997, who considers the generalized eigenvalue problem).\n\n15.8 roc curves are plots of (far(c), hr(c)) for varying cut-off values c, where far(c), hr(c) denote\nthe hit rate p (s(x) > c|y = 1) and the false alarm rate p (s(x) > c|y = 0). let the distribution of x in\nclasses 1 and 2 be given.\n\n(a) draw the roc curve if the distribution of x is the same in both classes.\n\n(b) draw the roc curve if the distributions of x in classes are not overlapping.\n\n(c) compute the hit and false alarm rates in terms of the normal distribution function if the distribution\nof x in class 0 is n(0,1) and in class 1 n(1,1). draw the approximate curve by computing some\nvalues.\n\n15.9 the optimal bayes classifier with loss function is given by equation (15.10).\n\n(a) show that the bayes classifier with symmetrical loss function l(r, s) = 0 if r = s, l(r, s) = c,\nwith constant c if r (cid:11)= s, yields the common bayes classifier that minimizes the misclassification\nerror.\n\n(b) show that the bayes classifier with loss function l(r, s) = 0 if r = s, l(r, s) = c/p (r) if r (cid:11)= s,\n\nyields the maximum likelihood rule.\n\ni \u03b2) \u2265 m, i = 1, . . . , n, is equivalent to\ni \u03b2) \u2265 1, i = 1, . . . , n. use that one can get rid of the constraint\n\n15.10 show that max\u03b20,\u03b2,||\u03b2||=1 m subject to yi(\u03b20 + xt\nmin\u03b20,\u03b2 ||\u03b2|| subject to yi(\u03b20 + xt\n||\u03b2|| = 1 by considering the constraint yi(\u03b20 + xt\n15.11 derive for binary observations with y \u2208 {\u22121, 1} the optimal prediction under hinge loss lh(y, \u02c6y) =\n1 \u2212 y \u02c6y by deriving the value c that minimizes elh(y, c) over c \u2208 [\u22121, 1].\n\ni \u03b2) \u2265 m||\u03b2||.\n\n "}, {"Page_number": 496, "text": "484\n\nchapter 15. prediction and classification\n\n15.12 the r package datasets contains anderson\u2019s well-known iris data. it contains four measurements\non three groups of iris.\n\n(a) find the classification rules by using classifiers like linear, quadratic, logistic discrimination, near-\nest neighborhood methods, trees, and random forests. compute the misclassification rate in the\nlearning sample.\n\n(b) split the dataset several times and use 80% of the data as the learning sample and the rest as test\ndata. compute the average test error. which classifiers perform best? compare the test errors to\nthe test errors from (a).\n\n15.13 in credit scoring one wants to identify risk clients. the dataset credit contains a sample of 1000\nconsumer credit scores collected at a german bank with 20 predictors. the dataset is available from the\nuci machine learning repository or the r package fahrmeir.\n\n(a) find the classification rules by using classifiers like linear, quadratic, logistic discrimination, near-\nest neighborhood methods, trees, and random forests. investigate which predictors are useful and\ndetermine the misclassification rate in the learning sample.\n\n(b) split the dataset several times and use 80% of the data as the learning sample and the rest as test\ndata. compute the average test error. which classifiers perform best? compare the test errors to\nthe test errors from (a).\n\n "}, {"Page_number": 497, "text": "appendix a\n\ndistributions\n\na.1 discrete distributions\na.1.1 binomial distribution\nthe variable y is binomially distributed, y \u223c b(n, \u03c0), if the probability mass function is given\nby\n\n(cid:29) (cid:27)\n\n(cid:28)\n\nf(y) =\n\nn\ny\n0\n\n\u03c0y(1 \u2212 \u03c0)n\u2212y\n\ny \u2208 {0, 1, . . . , n}\notherwise.\n\nthe parameters are n \u2208 in, \u03c0 \u2208 [0, 1]. one obtains e(y ) = n\u03c0, var(y ) = n\u03c0(1 \u2212 \u03c0).\n\na.1.2 poisson distribution\na variable y follows a poisson distribution, y \u223c p (\u03bb), if the probability mass function is\ngiven by\n\n(cid:29)\n\nf(y) =\n\n\u2212\u03bb\n\ny\u03b1\ny! e\n0\n\ny = 0, 1, 2, . . .\notherwise.\n\nthe parameter \u03bb > 0 determines e(y ) = \u03bb, var(y ) = \u03bb.\n\na.1.3 negative binomial distribution\nthe variable y \u2208 {0, 1, . . .} follows a negative binomial distribution, n b(\u03bd, \u03bc), if the mass\nfunction is given by\n\n(cid:26)\n\n(cid:25)\n\n(cid:26)\n\ny\n\n(cid:25)\n\n\u03bc\n\n\u03bc + \u03bd\n\n\u03bd\n\n\u03bc + \u03bd\n\n\u2212tdt. one has\n\nvar(y ) = \u03bc + \u03bc2/\u03bd.\n\nf(y) =\n\n\u03b3(y + \u03bd)\n\n\u2019 \u221e\n\n\u03b3(y + 1)\u03b3(\u03bd)\nt\u03bd\u22121e\n\n0\ne(y ) = \u03bc,\n\n\u03bd\n\n,\n\ny = 0, 1, . . .\n\nwhere \u03bd, \u03bc > 0 and \u03b3(\u03bd) =\n\nan alternative parameterization uses \u03c0 = \u03bd/(\u03bc + \u03bd), yielding n b(\u03bd, \u03bd(1 \u2212 \u03c0)/\u03c0) with\n\nf(y) =\n\n\u03b3(y + \u03bd)\n\n\u03b3(\u03bd + 1)\u03b3(\u03bd) \u03c0\u03bd(1 \u2212 \u03c0)y.\n\nthe mean and variance are given by\n\ne(y ) = \u03bd\n\n1 \u2212 \u03c0\n\u03c0\n\n,\n\nvar(y ) = \u03bd\n\n1 \u2212 \u03c0\n\u03c02 = \u03bc/\u03c0.\n\n485\n\n "}, {"Page_number": 498, "text": "486\n\nappendix a. distributions\n\nfor integer-valued \u03bd\u2019s the random variable y describes the number of trials needed (in addition\nto \u03bd) until \u03bd successes are observed. the underlying experiment consists of repeated trials,\nwhere it is assumed that one trial can result in just two possible outcomes (success or failure),\nthe probability of success, \u03c0, is the same on every trial, and the trials are independent. the\nexperiment continues until \u03bd successes are observed.\n\na.1.4 hypergeometric distribution\nthe variable y follows a hypergeometric distribution, y \u223c h(n, n, m), if the probability mass\nfunction is given by\n\n\u239b\n\u239d m\ny\n\n\u239e\n\u23a0\n\n\u239b\n\u239e\n\u239d n \u2212 m\n\u23a0\n\u239b\n\u239d n\nn\n\ny\n\u239e\n\u23a0\n\nf(y) =\n\n\u23a7\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9\n\nmax{0, m + n \u2212 n} \u2264 y\n\u2264 min{m, n}\n\ny \u2208 {0, 1, . . . , n}\notherwise,\n)\n\n1\n\nt\u03b1\u22121(1 \u2212 t)\u03b2\u22121dt.\n\n0\n\notherwise.\n\nthe distribution arises if n marbles are drawn at random from a box that contains n marbles,\nm of which are \"red\" and n \u2212 m are \"white.\" then y counts the number of red marbles and\none obtains\n\ne(y) = n\n\n, var(y) = n \u2212 n\n\nn \u2212 1 n(1 \u2212 m\n\nn\n\nm\nn\n\n) m\nn\n\n.\n\na.1.5 beta-binomial distribution\nthe variable y \u2208 {0, 1, . . . , n} follows a beta-binomial distribution, y \u223c betabin(n, \u03b1, \u03b2),\nif the mass function is given by\n\n(cid:2) (cid:27)\n\n(cid:28)\n\nn\ny\n0\n\nf(y) =\n\nb(\u03b1+y,\u03b2+n\u2212y)\n\nb(\u03b1,\u03b2)\n\nwhere \u03b1, \u03b2 > 0 and b(\u03b1, \u03b2) is the beta function defined as\n\nb(\u03b1, \u03b2) = \u03b3(\u03b1)\u03b3(\u03b2)/\u03b3(\u03b1 + \u03b2) =\n\n0\n\none has with \u03bc = \u03b1/(\u03b1 + \u03b2), \u03b4 = 1/(\u03b1 + \u03b2 + 1)\n\ne(y ) = n\u03bc,\n\nvar(y ) = n\u03bc(1 \u2212 \u03bc)[1 + (n \u2212 1)\u03b4].\n\nas \u03b4 \u2192 0, the beta-binomial distribution converges to the binomial distribution b(n, \u03bc).\n\na.1.6 multinomial distribution\nthe vector yt = (y1, . . . , yk) is multinomially distributed, y \u223c m(n, (\u03c01, . . . , \u03c0k)), if the\nprobability mass function is given by\n\nf(y1, . . . , yk) =\n\ny1!...yk! \u03c0y1\n\n1 . . . \u03c0yk\n\nk\n\n\u23a7\u23a8\n\u23a9 n!\n\n0\n\n(cid:14)\nyi \u2208 {0, . . . , n},\n\ni yi = n\notherwise,\n\n(cid:14)\n\ni \u03c0i = 1.\n\nwhere \u03c0t = (\u03c01, . . . , \u03c0k) is a probability vector, that is, \u03c0i \u2208 [0, 1],\none has\ne(yi) = n\u03c0i, var(yi) = n\u03c0i(1 \u2212 \u03c0i),\ncov(yi, yj) = \u2212n\u03c0i\u03c0j, i (cid:8)= j.\n\n "}, {"Page_number": 499, "text": "a.2. continuous distributions\n\n487\n\na.2 continuous distributions\na.2.1 normal distribution\n\"\nthe normal distribution, y \u223c n(\u03bc, \u03c32), has density\n\u22121\n2\n\n1\u221a\n2\u03c0\u03c3\n\nf(y) =\n\nexp\n\n#\n\n(cid:26)\n\n2\n\n(cid:25)\n\ny \u2212 \u03bc\n\u03c3\n\nwith e(y ) = \u03bc, var(y ) = \u03c32.\n\na.2.2 logistic distribution\na variable y has a logistic distribution, y \u223c logistic(\u03bc, \u03b2), if the density is given by\n\nf(y) =\n\n1\n\u03b2\n\nexp(\u2212 y\u2212\u03bc\n\u03b2 )\n(1 + exp(\u2212 y\u2212\u03bc\n\u03b2 ))2\n\nwith distribution function\n\nf (y) =\n\n1\n\n1 + exp(\u2212 y\u2212\u03bc\n\u03b2 )\n\none has e(y) = \u03bc, var(y) = \u03b22\u03c02/3.\n\nexp( y\u2212\u03bc\n\u03b2 )\n1 + exp( y\u2212\u03bc\n\u03b2 )\n\n.\n\n=\n\na.2.3 gumbel or maximum extreme value distribution\n(cid:25)\nthe gumbel distribution, y \u223c gu(\u03b1, \u03b2), has density\n\u2212 exp\n(cid:25)\n\n(cid:25)\n\u2212 y \u2212 \u03b1\n(cid:26)(cid:26)(cid:26)\n\nand distribution function\n\nf(y) =\n\n(cid:26)(cid:26)\n\n(cid:25)\n\nexp\n\nexp\n\n1\n\u03b2\n\n(cid:26)\n\n(cid:25)\n\u2212 y \u2212 \u03b1\n(cid:25)\n\u2212 exp\n\nf (y) = exp\n\n\u2212\n\n\u03b2\n\n\u03b2\n\n,\n\ny \u2212 \u03b1\n\u03b2\n\nwith\n\ne(y ) = \u03b1 + \u03b2\u03b3\n\n(\u03b3 = 0.5772),\n\nvar(y ) = \u03b22\u03c02/6.\n\na.2.4 gompertz or minimum extreme value distribution\n(cid:26)(cid:26)\nthe density of the gompertz distribution, y \u223c go(\u02dc\u03b1, \u03b2), is given by\ny \u2212 \u02dc\u03b1\n\u03b2\n(cid:26)(cid:26)\n\n(cid:25)\n\u2212 exp\n(cid:25)\n\nwith the distribution function\n\ny \u2212 \u02dc\u03b1\n\u03b2\n\nf(y) =\n\n(cid:25)\n\n(cid:25)\n\nexp\n\n1\n\u03b2\n\nexp\n\n(cid:26)\n(cid:25)\n\u2212 exp\n\nf (y) = 1 \u2212 exp\n\ny \u2212 \u02dc\u03b1\n\u03b2\n\nand\n\ne(y) = \u02dc\u03b1 \u2212 \u03b2\u03b1,\n\nvar(y) = \u03b22\u03c02/6.\n\nthere is a strong connection to the maximum extreme value distribution since y has a minimum\nextreme value distribution go(\u02dc\u03b1, \u03b2) if \u2212y has a maximum extreme value distribution gu (\u03b1, \u03b2),\nwhere \u02dc\u03b1 = \u2212\u03b1.\n\n "}, {"Page_number": 500, "text": "488\n\nappendix a. distributions\n\na.2.5 exponential distribution\na random variable y has an exponential distribution, y \u223c e(\u03b1), if the density function is\ngiven by\n\n(cid:29)\n\nf(y) =\n\n0\n\u03bbe\n\n\u2212\u03bby\n\ny < 0\ny \u2265 0,\n\nwhere \u03bb > 0. one has\n\ne(y ) =\n\n1\n\u03bb\n\n, var(y ) =\n\n1\n\u03bb2 .\n\na.2.6 gamma-distribution\na random variable y is gamma-distributed, y \u223c \u03b3(\u03bd, \u03b1), if it has the density function\n\nf(y) =\n\nwhere, for \u03bd > 0, \u03b3(\u03bd) is defined by\n\n(cid:29)\n\ny \u2264 0\ny > 0,\n\n\u2212\u03b1y\n\n0\n\u03b3(\u03bd) y\u03bd\u22121e\n\u03b1\u03bd\n) \u221e\n\n\u03b3(\u03bd) =\n\n0\n\nt\u03bd\u22121e\n\n\u2212tdt.\n\none obtains e(y ) = \u03bd/\u03b1, var(y ) = \u03bd/\u03b12. for \u03bd = 1, the exponential distribution is a special\ncase.\n\na.2.7 inverse gaussian distribution\nthe density of the inverse gaussian distribution, y \u223c ig(\u03bc, \u03bb), is given by\n\n(cid:25)\n\n(cid:26)\n\n1/2\n\n\u03bb\n\n2\u03c0y3\n\n(cid:29)\n\nexp\n\n\u2212 \u03bb\n2\u03c02y\n\n(y \u2212 \u03bc)2\n\n, y > 0,\n\n(cid:30)\n\nf(y) =\n\nwhere \u03bc, \u03bb > 0. one obtains\n\nfor \u03bc = 1, the distribution is also called the wald distribution.\n\ne(y ) = \u03bc,\n\nvar(y ) = \u03bc3/\u03bb.\n\na.2.8 dirichlet distribution\n(cid:14)\nthe vector zt = (z1, . . . , zk), k \u2265 2, is dirichlet distributed, z \u223c d(\u03b1), with \u03b1t =\n(cid:24)\n(\u03b11, . . . \u03b1k) if the density on the simplex {(z1, . . . , zk) :\ni=1 zi = 1, zi > 0} is given by\nk(cid:15)\n\nk\n\n(cid:23)(cid:14)\n2\n\n\u03b3\n\nk\ni=1 \u03b1i\ni=1 \u03b3(\u03b1i)\n\nk\n\ni=1\n\nwhere \u03b3() is the gamma function. an often-used reparameterization is obtained with\n\nf(z1, . . . , zk) =\n\n\u03b1i(cid:14)\n\nk\ni=1 \u03b1i\n\n\u03bci =\n\n,\n\ni = 1, . . . , k, k =\n\n,\n\ni\n\nz\u03b1i\u22121\nk(cid:7)\n\n\u03b1i.\n\ni=1\n\nwith vectors \u03bct = (\u03bc1, . . . , \u03bck) = \u03b1t /k one obtains\n\ne(z) = \u03bc,\n\nvar(zi) = \u03bci(1 \u2212 \u03bci)\nk + 1 .\n\n "}, {"Page_number": 501, "text": "a.2. continuous distributions\n\n489\n\na.2.9 beta distribution\nthe beta distribution, (beta(\u03b11, \u03b12)), is a special case of the dirichlet distribution where k = 2.\nit is enough to consider the first component, which has the density\n\nf(z) =\n\n\u03b3(\u03b11 + \u03b12)\n\n\u03b3(\u03b11)\u03b3(\u03b12) z\u03b11\u22121(1 \u2212 z)\u03b12\u22121.\n\nreparameterization yields\n\nwith density\n\none has for z = z1\n\n\u03bci =\n\n\u03b1i\n\n\u03b11 + \u03b12\n\nf(z) =\n\ne(z) = \u03bc1,\n\ni = 1, 2 k = \u03b11 + \u03b12\n\n\u03b3(k)\n\n\u03b3(k\u03bc1)\u03b3(k\u03bc2) z\u03bc1k\u22121(1 \u2212 z)\u03bc2k\u22121.\nvar(z) = \u03bc1(1 \u2212 \u03bc1)\n\n.\n\nk + 1\n\n "}, {"Page_number": 502, "text": "appendix b\n\nsome basic tools\n\nb.1 linear algebra\nderivatives\nthe log-likelihood is typically a real-valued function of a p-dimensional parameter. when\nmaximizing the likelihood one seeks parameter values where the derivative becomes zero.\nlet f : r p \u2192 r be a function of the vector x, that is, f(x) = f(x1, . . . , xp). the\n\nderivative of f with respect to x is determined by the vector of partial derivatives:\n\n\u2202f(x)\n\n\u2202x\n\n=\n\n\u239e\n\u239f\u239f\u23a0 .\n\n\u2202x1\n\n\u239b\n\u239c\u239c\u239d \u2202f (x)\n(cid:25)\n\n\u2202f (x)\n\u2202xp\n\n...\n\nthe transposed vector is denoted by\n\n0\n\n1\n\nt\n\n\u2202f(x)\n\n\u2202x\n\n= \u2202f(x)\n\n\u2202xt =\n\n\u2202f(x)\n\u2202x1\n\n(cid:26)\n\n.\n\n, . . . ,\n\n\u2202f(x)\n\u2202xp\n\nthe (asymptotic) covariance of the ml estimator is linked to the fisher matrix, which contains\nthe second derivative of the log-likelihood computed at the ml estimate. the matrix of second\n\u239b\nderivatives of f : r p \u2192 r is determined by\n\u239c\u239c\u239c\u239c\u239c\u239d\n\n\u2202f(x)\n\u2202x\u2202xt = \u2202\n\n\u239e\n\u239f\u239f\u239f\u239f\u239f\u23a0 .\n\n\u2202f (x)\n\u2202x1\u2202xp\n\n\u2202f (x)\n\u2202x1\u2202x2\n\n\u2202f (x)\n\u2202x2\u2202x1\n\n\u2202f (x)\n\u2202x2\n1\n\n\u2202f\n\u2202x\n\n(cid:25)\n\n(cid:26)\n\n\u2202xt\n\n...\n\n. . .\n\n=\n\n...\n\n...\n\n\u2202f (x)\n\u2202xp\u2202x1\n\n. . .\n\n\u2202f (x)\n\u2202xp\u2202xp\n\nthis so-called hesse matrix is symmetric if the function f has continuous second partial deriva-\ntives.\nfor a vector-valued function f : r p \u2192 r q, where f(x) = (f1(x), . . . , fq(x)), the deriva-\n\ntive is\n\n\u2202f(x)\n\n\u2202x\n\n=\n\n\u239b\n\u239c\u239c\u239d \u2202f1(x)\n\n\u2202x1\n\n...\n\n\u2202f1(x)\n\n\u2202xp\n\n\u239e\n\u239f\u239f\u23a0 =\n\n(cid:25)\n\n(cid:26)\n\n\u2202f1(x)\n\n\u2202x\n\n, . . . ,\n\n\u2202fq(x)\n\n\u2202x\n\n.\n\n. . .\n...\n. . .\n\n\u2202fq(x)\n\n\u2202x1\n\n...\n\n\u2202fq(x)\n\n\u2202xp\n\n490\n\n "}, {"Page_number": 503, "text": "b.2. taylor approximation\n\n491\n\ncovariance\nthe covariance (matrix) of a random vector xt = (x1, . . . , xk) is given by the symmetric\nmatrix\n\n\u239b\n\u239c\u239c\u239c\u239c\u239d\n\nvar(x1)\n\ncov(x1, x2)\n\n. . .\n\ncov(x1, xk)\n\n...\n\n...\n\ncov(x) =\n\ncov(x2, x1)\n\n...\n\ncov(xk, x1)\n\n. . .\n\nvar(xk)\n\n\u239e\n\u239f\u239f\u239f\u239f\u23a0 .\n\nfor the transformed random vector ax one obtains\n\ncov(ax) = aaa cov(x)aaat ,\n\nwhere a is any (n \u00d7 k)-matrix.\n\nsquare root of a matrix\nlet aaa be a symmetric and positive definite matrix. for a positive definite matrix xt aaax > 0\nholds for all x (cid:8)= 000. then there exists a matrix a1/2 such that\n\na = a1/2at /2\n\nholds, where aaat /2 = (aaa1/2)t denotes the transposed of aaa1/2. the matrix a1/2 is the left and\nat /2 is the right square root of a. both are not singular, that is, det(a1/2) (cid:8)= 0.\n\nin general, the square root of a matrix is not unique. a unique decomposition is based on\nthe cholesky decomposition, which in addition postulates that a1/2 is a lower triangular matrix\nand the conjugate transpose at /2 is an upper triangular matrix.\n\u22121. from a = a1/2at /2 one\nfrequently one uses the square root of an inverse matrix a\n\u22121/2 =\n\u2212t /2 = (at /2)\u22121, a\n\u22121 = (at /2)\u22121(a1/2)\u22121. with the denotation a\n\nobtains a\n(a1/2)\u22121 one obtains the decomposition of a\n\u22121 = a\na\n\n\u22121 as\n\u2212t /2a\n\n\u22121/2.\n\nthe square root of an inverse matrix is used, for example, if one wants to standardize a\n\u22121/2x\n\nrandom vector. let x have variance \u03c3 . it is easily derived that the transformed vector \u03c3\nhas variance i, that is, cov(\u03c3\n\n\u22121/2x) = i.\n\nb.2 taylor approximation\ntaylor\u2019s theorem asserts that any sufficiently smooth function can be locally approximated by\npolynomials. it is in particular useful for the motivation of local polynomial fits and the deriva-\ntion of asymptotic results.\n\nunivariate version\nlet f be a function that is n-times continuously differentiable on a closed interval and (n + 1)-\ntimes differentiable on the open interval. then one obtains for an inner point x0\n\nf(x) = f(x0) +\n\nf (r)(x0)\n\nr!\n\n(x \u2212 x0)r + f (n+1)(x0 + \u03d1(x \u2212 x0))\n\n(x \u2212 x0)n+1\n(n + 1)!\n\n,\n\nn(cid:7)\n\nr=1\n\nwhere x is from the interval, \u03d1 is a value for which 0 < \u03d1 < 1 holds, and f (r) = drf /dxr are\nthe r-order derivatives.\n\n "}, {"Page_number": 504, "text": "492\n\nappendix b. some basic tools\n\nmultivariate version\nlet f be a function that is n-times continuously differentiable on a ball in r p and (n+1)-times\ndifferentiable on the closure. then one obtains for any xxx\n\nn(cid:7)\n\n(cid:7)\n(cid:7)\n\nr1+\u00b7\u00b7\u00b7+rp=r\n\nr=0\n\nf(xxx) =\n\n+\n\nr1+\u00b7\u00b7\u00b7+rp=n+1\n\nr1! . . . rp!\n\n1\n\nr1! . . . rp!\n\n(x1 \u2212 x01)r1 . . . (x1 \u2212 x0p)rp\n\n1\n\n(x1 \u2212 x01)r1 . . . (x1 \u2212 x0p)rp\n\n\u2202rf(x0)\n1 . . . \u2202xrp\n\np\n\n\u2202xr1\n\u2202rf(x0 + \u03d1(x \u2212 x0))\n\n\u2202xr1\n\n1 . . . \u2202xrp\n\np\n\n,\n\nwhere xt\nremainder) is\n\n0 = (x01, . . . , x0p), xt = (x1, . . . , xp). the second-order approximation (without\n\np(cid:7)\n\nf(xxx) \u2248 f(x0) +\n\n(cid:7)\n\ni(cid:5)=j\n\n+\n\ni=1\n\u22022f(x0)\n\u2202xi\u2202xj\n\n\u2202f(x0)\n\n(xi \u2212 x0i) +\n\n\u2202xi\n(xi \u2212 x0i)(xj \u2212 x0j)\n\n1\n2\n\np(cid:7)\n\ni=1\n\n\u22022f(x0)\n\n\u2202x2\ni\n\n(xi \u2212 x0i)2\n\n= f(x0) + (x \u2212 x0)t \u2202f(x0)\n\n\u2202x\n\n+\n\n1\n2\n\n(x \u2212 x0)t \u2202f(x0)\n\n\u2202x\u2202xt (x \u2212 x0).\n\nfor a vector-valued function f = (f1, . . . , fs) : r p \u2192 r s one obtains for each component the\nfirst-order approximation\n\nfi(x) \u2248 fi(x0) + (x \u2212 x0)t \u2202fi(x0)\n\n\u2202x\n\n= fi(x0) + \u2202f(x0)\n\n\u2202xt (x \u2212 x0).\n\nthe function itself can be approximated through\n\nf(x) \u2248 f(xxx0) + \u2202f(x0)\n\n\u2202xt (x \u2212 x0).\n\ntaylor approximation and the asymptotic covariance of ml estimates\nthe taylor approximation is often used to derive asymptotic properties of the ml estimate \u02c6\u03b2.\nin the following we just give some motivation for the asymptotic covariance. for glms the\nfirst-order taylor approximation of the score function s(\u03b2) = \u2202l(\u03b2)/\u2202\u03b2 at \u03b2 yields\n\ns(\u02c6\u03b2) \u2248 s(\u03b2) + \u2202s(\u03b2)\n\n\u2202\u03b2t (\u02c6\u03b2 \u2212 \u03b2).\n\nsince s(\u02c6\u03b2) = 0 one obtains the approximation \u02c6\u03b2 \u2212 \u03b2 \u2248 (\u2212\u2202s(\u03b2)/\u2202\u03b2t )\u22121s(\u03b2). with\n\u2212\u2202s(\u03b2)/\u2202\u03b2t = \u2212\u22022l(\u03b2)/\u2202\u03b2\u2202\u03b2t = f obs(\u03b2) one obtains \u02c6\u03b2 \u2212 \u03b2 = f obs(\u03b2)\u22121s(\u03b2). if\n\u22121dx,\nthe observed fisher matrix f obs(\u03b2) is replaced by the fisher matrix f (\u03b2) = x t d\u03c2\none obtains the approximate covariance matrix of the ml estimate f (\u03b2)\u22121 cov(s(\u03b2)f (\u03b2)\u22121.\nsince cov(s(\u03b2) = e(s(\u03b2)s(\u03b2)t ) = f (\u03b2) the approximate covariance reduces to f (\u03b2)\u22121.\nto make the approximation work asymptotically regularity conditions are needed, in particular,\none assumes that f (\u02c6\u03b2)/n converges to a limit; for details see fahrmeir and kaufmann (1985).\n\n "}, {"Page_number": 505, "text": "b.3. conditional expectation, distribution\n\n493\n\nb.3 conditional expectation, distribution\nlet x, y be random variables with the joint density function f(x, y). then the conditional\ndensity of y given x = x is given by\n\nfy |x(y|x) = f(y, x)\nfx(x)\n\n\u2019\n\nfor any x such that fx(x) =\nis the conditional probability mass function:\n\nf(x, y)dy > 0. for discrete random variables x, y the analog\n\nfy |x(y|x) = p (y = y|x = x),\n\nwhere p (x = x) > 0.\n\nthe conditional distribution function of y given x = x is defined to be\n\nwhich has the form\n\nfor variables with conditional density fy |x and\n\nfy |x(y|x) = p (y \u2264 y|x = x),\n\n)\n\u2212\u221e fy |x(y|x)dx\nfy |x(y|x) =\n(cid:7)\n\nx\n\nfy |x(x|y) =\n\np (y = yi|x = x)\n\nyi\u2264y\n\n(cid:7)\n)\n\nyi\n\n\u23a7\u23aa\u23a8\n\u23aa\u23a9\n\n(cid:7)\n\u23a7\u23aa\u23a8\n)\n\u23aa\u23a9\n\nyi\n\nfor a discrete-valued y .\n\nthe conditional expectation of y given x, denoted by e(y |x), is a random variable that\nvaries with the values of x. if x takes the value x, the conditional expectation takes the value\n\ne(y |x) =\n\nyip (y = yi|x = x)\nyfy |x(y|x)dy\n\nfor discrete variables\n\nfor continuous-type variables.\n\nin a slightly more general form, one may consider the transformation h(y ) with the conditional\nexpectation of h(y ) given x taking the value\n\ne(h(y )|x) =\n\nh(yi)p (y = yi|x = x)\nh(y)fy |x(y|x)dy\n\nfor discrete variables\n\nfor continuous-type variables.\n\nif x takes the value x.\n\nthe higher moments follow from this definition. e(y r|x) is based on the transformation\n\nh(y) = yr. in particular, one has\n\nvar(y |x) = e({y \u2212 e(y |x = x)}2|x = x)\n= e(y 2|x = x) \u2212 e(y |x = x)2.\n\none obtains for the random variable e(y |x):\n(1) e(e(y |x)) = e(y ).\n\n "}, {"Page_number": 506, "text": "494\n(2) if e(y 2) < \u221e, one has\n\nappendix b. some basic tools\n\nvar(y ) = var(e(y |x)) + e(var(y |x)),\nwhere var(y |x) is given by var(y |x) = e(y 2|x) \u2212 e(y |x)2.\n\nthe decomposition of variance also holds for vectors y, x:\n\ncov(y) = covx(e(y|x)) + ex cov(y|x)\n\nwith cov(y|x) = e(yyt|x) + e(y|x) e(y|x)t .\n\nb.4 em algorithm\nthe basic em algorithm was proposed by dempster et al. (1977). the algorithm provides\na general device to obtain maximum likelihood estimators in incomplete data situations. let\ny \u2208 r n denote a vector of observed data and z \u2208 r m a vector of unobservable data. the\nhypothetical complete data are (y, z) and the incomplete data that were observed are y. let the\njoint density of the complete data, f(y, z; \u03b8), depend on an unknown parameter vector \u03b8 \u2208 \u03b8.\nthen the maximum likelihood estimator for \u03b8 can be obtained by maximizing the marginal\nlog-likelihood:\n\n)\n\nl(\u03b8) = log\n\nf(y, z; \u03b8)dz.\n\n(b.1)\n\nhowever, maximization my involve difficult integration procedures. indirect maximization by\nthe em algorithm helps to avoid numerical evaluations of the integral. with g(z|y; \u03b8) denoting\nthe conditional density of the unobservable data z, given the observed data y, one obtains for\nthe marginal density f(y; \u03b8) = f(y, z; \u03b8)/g(z|y; \u03b8). therefore, the marginal log-likelihood\nhas the form\n\nl(\u03b8) = log f(y, z; \u03b8) \u2212 log g(z|y; \u03b8).\n\n(b.2)\n\nsince z is unobservable, expectations are taken on both sides of (b.2) with respect to the con-\nditional density g(z|y; \u03b80) for fixed \u03b80 \u2208 \u03b8, obtaining\n\nl(\u03b8) = e\u03b80\n\n{log f(y, z; \u03b8)} \u2212 e\u03b80\n\n= m(\u03b8|\u03b80) \u2212 h(\u03b8|\u03b80),\n\n{log g(z|y; \u03b8)}\n)\n\n)\n\nlog f(y, z; \u03b8) g(z|y; \u03b80) dz, h(\u03b8|\u03b80) =\n\nwhere\nlog g(z|y; \u03b8) g(z|y; \u03b80) dz.\nm(\u03b8|\u03b80) =\nthe em algorithm maximizes l(\u03b8) iteratively by maximizing m(\u03b8|\u03b80) with respect to \u03b8,\nwhere \u03b80 is given at each cycle of the iteration. in contrast to the integral in (b.1), evaluation of\nthe integral in m(\u03b8|\u03b80) is straightforward for many applications. with \u02c6\u03b8\ndenoting a starting\nvalue for \u03b8, the (p + 1)-th cycle of the em algorithm consists of the following two steps:\n\n(0)\n\ne(xpectation) step: compute the expectation m(\u03b8|\u02c6\u03b8\n\nm(aximizing) step: the improved estimate \u02c6\u03b8\nas a function in \u03b8.\n\n(p+1)\n\n(p)).\nis obtained by maximizing m(\u03b8|\u02c6\u03b8\n\n(p))\n\n "}, {"Page_number": 507, "text": "b.4. em algorithm\n\n495\n\nthe em algorithm has the desirable property that the log-likelihood l always increases or re-\nmains at least constant at each cycle. if \u02c6\u03b8 maximizes m(\u03b8|\u03b80) for fixed \u03b80, one has m(\u02c6\u03b8|\u03b80) \u2265\nm(\u03b8|\u03b80) for all \u03b8\u2019s by definition and h(\u03b8|\u03b80) \u2264 h(\u03b80|\u03b80) for all \u03b8\u2019s by jensen\u2019s inequality,\nso that l(\u02c6\u03b8) \u2265 l(\u03b80) holds.\n\nconvergence of the log-likelihood sequence l(\u03b8(p)), p = 0, 1, 2 . . ., against a global or lo-\ncal maximum or a stationary point l\u2217 is ensured under weak regularity conditions concerning \u03b8\nand l(\u03b8) (see, e.g., dempster et al., 1977). however, if more than one maximum or stationary\npoint exists, convergence against one of these points depends on the starting value. moreover,\nconvergence of the log-likelihood sequence l(\u03b8(p)), p = 0, 1, 2, . . ., against l\u2217 does not imply\nthe convergence of (\u03b8(p)) against a point \u03b8\u2217 (wu, 1983; boyles, 1983). in general, conver-\ngence of (\u03b8(p)) requires stronger regularity conditions, which are ensured in particular for com-\nplete data densities f(y, z; \u03b8) of the simple or curved exponential family. the rate of conver-\ngence depends on the relative size of the unobservable information on \u03b8. if the information\nloss due to the missing z is a small fraction of the information in the complete data (y, z), the\nalgorithm converges rapidly. but the rate of convergence becomes rather slow for parameters \u03b8\nnear the boundary of \u03b8.\n\nan estimator for the variance-covariance matrix of the mle for \u03b8, for example, the ob-\nserved or expected information on \u03b8 in the observed data y, is not provided by the em algo-\nrithm. newton-raphson or other gradient methods that maximize (b.1) directly are generally\nfaster and yield an estimator for the variance-covariance matrix of the mle. however, the em\nalgorithm is simpler to implement and numerically more stable. an estimate for the variance-\ncovariance matrix of the mle is obtained if an additional analysis is applied after the last cycle\nof the em algorithm (see louis, 1982). the method can also be used to speed up the em\nalgorithm (see also meilijson, 1989).\n\nmany extensions of the em algorithm have been proposed; see, for example, mclachlan\nand krishnan (1997). in statistical versions of the em algorithm, the e-step consists in sim-\nulating the missing data from the conditional distribution; see celeux and diebolt (1985). in\nmonte carlo em (mcem), this step is replaced by monte carlo approximations (wei and tan-\nner, 1990). quasi\u2013monte carlo methods have been proposed by jank (2004). to reduce the\ncomputational costs delyon et al. (1999) proposed saem, a stochastic approximation version\nof the em algorithm.\n\n "}, {"Page_number": 508, "text": "appendix c\n\nconstrained estimation\n\nc.1 simplification of penalties\na common penalty that is easy to handle has the form\n\nj(\u03b4) = \u03b4t k\u03b4,\n\nwhere \u03b4t = (\u03b41, . . . , \u03b4m) and k is a symmetric (m \u00d7 m)-matrix. typically k has the form\nk = lt l, where l is an (r \u00d7 m)-matrix. a very simple form results when k is chosen as\nthe identity matrix. then one obtains the penalty term j(\u03b4) =\nj , which is used in ridge\nregressions.\none uses difference penalties. first-order differences use the ((m \u2212 1) \u00d7 m)-matrix\n\nwhen basis functions are defined on an equally spaced grid, as, for example, in p-splines,\n\nm\nj=1 \u03b42\n\n(cid:14)\n\n(c.1)\n\n\u239b\n\u239c\u239c\u239c\u239d\n\nl =\n\n\u22121\n\n1\u22121\n\n\u239e\n\u239f\u239f\u239f\u23a0 ,\n\n...\u22121 1\n\n1\n...\nm\u22121(cid:7)\n\n(\u03b4j+1 \u2212 \u03b4j)2\n\nwhich yields\n\nj(\u03b4) = \u03b4t lt l\u03b4 =\n\nj=1\n\n\u239b\n\u239c\u239c\u239c\u239c\u239c\u239d\n\nand the corresponding matrix\n\n1 \u22121\n\u22121\n...\n\n\u239e\n\u239f\u239f\u239f\u239f\u239f\u23a0 ,\n\u239e\nwhich has rank m \u2212 1. second-order differences are specified by\n\u239f\u239f\u239f\u23a0 ,\n\n2 \u22121\n...\n...\n\u22121\n2 \u22121\n\u22121\n1\n\n\u239b\n\u239c\u239c\u239c\u239d1 \u22122\n\n1\n1 \u22122\n...\n\nk =\n\nl =\n\n1\n...\n1 \u22122 1\n\n496\n\n "}, {"Page_number": 509, "text": "c.1. simplification of penalties\n\n497\n\nyielding\n\nj(\u03b4) =\n\nand the corresponding matrix\n\nk =\n\nwhich has rank m \u2212 2.\n\nm\u22122(cid:7)\n\nj=1\n\n1 \u22122\n\u22122\n1 \u22124\n\n\u239b\n\n\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d\n\n{(\u03b4j+2 \u2212 \u03b4j+1) \u2212 (\u03b4j+1 \u2212 \u03b4j)}2\n\u239e\n\n1\n5 \u22124\n1\n6 \u22124\n1\n...\n...\n...\n6 \u22124\n1 \u22124\n1 \u22124\n1 \u22122\n\n1\n5 \u22122\n1\n\n\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 ,\n\nsimplifying the penalty by reparameterization\nlet the penalty have the general form j(\u03b4) = \u03b4t k\u03b4, where k is a symmetric, non-negative\ndefinite (m \u00d7 m)-matrix.\n\nwhen k has full rank one obtains by defining \u03b4p = k1/2\u03b4 the simple reparameterized\n\npenalty\n\n\u03b4t k\u03b4 = \u03b4t\n\np \u03b4p = (cid:16)\u03b4p(cid:16)2.\n\nin the more general case, let k have rank r, r < m. then one wants a decomposition of \u03b4 into\nan unpenalized part and a penalized part of the form\n\n(c.2)\nwhere t is an m \u00d7 (m \u2212 r)-matrix and w is a m \u00d7 r-matrix. \u03b40 represents the unpenalized\npart and \u03b4p the penalized part. given the decomposition one obtains by simple matrix algebra\n\n\u03b4 = t \u03b40 + w \u03b4p,\n\n\u03b4t k\u03b4 = \u03b4t\n\n0 t t kt \u03b40 + \u03b4t\n\n0 t t kw \u03b4p + \u03b4t\n\np w t kt \u03b40 + \u03b4t\n\np w t kw \u03b4p.\n\n(c.3)\n\nif t and w are chosen such that t t kt = 0, t t kw = 0, and w t kw = i, one obtains\nthe simple form\n\n\u03b4t k\u03b4 = \u03b4t\n\np \u03b4p = (cid:16)\u03b4p(cid:16)2,\n\nwhich contains only the penalized parameters. in addition, it is required that the composed\nmatrix [t|w ] has full rank, in order to obtain a one-to-one transformation between the param-\neterizations \u03b4 and (\u03b40, \u03b4p). the effect of the reparameterization becomes especially obvious\nwhen a smoothing parameter is added. then the penalty becomes\n\n\u03bb\u03b4t k\u03b4 = \u03bb\u03b4t\n\np \u03b4p = \u03bb(cid:16)\u03b4p(cid:16)2.\n\nwhile \u03b4p is penalized with the effect that \u03b4p becomes zero for increasing \u03bb, the parameters in\n\u03b40 remain unaffected by \u03bb.\nfor the construction of the matrices t and w one starts by choosing w = lt (llt )\u22121\nfrom some factorization of the penalty matrix into k = lt l. then one has w t kw = i.\nthe matrix t is chosen as an (m\u2212 r)-dimensional basis of the null space of l, that is, lt = 0\nholds. then one has t t kt = t t lt lt = 0 and the first term in (c.3) can be omitted.\nmoreover, one obtains w t kt = w t lt lt = 0 and, in addition, the second and third\n\n "}, {"Page_number": 510, "text": "498\n\nappendix c. constrained estimation\n\nterms in (c.3) can be omitted. moreover, [lt t ] is non-singular and the transformation is\none-to-one.\n\nit should be noted that the decomposition k = lt l is not unique. a factorization of the\nmatrix k may be based on the spectral decomposition k = \u03b3 d\u03b3 t , where d is a diagonal\nmatrix containing the eigenvalues in descending order. then l = d1/2\u03b3 t yields k = lt l.\nalternatively, the matrix l can be used that defines a penalty. for example, with p-splines, the\nmatrix l is given as a matrix containing differences of the first or higher order.\n\nas examples let us consider the first- and second-order difference penalties for equally\nspaced p-splines. when using first differences it is obvious that adding a constraint c to the\nparameters, obtaining \u03b4j + c, does not change the penatly. this means that the overall level\nis not penalized. consequently, the matrix t that fulfills lt = 0 is t t = (1, . . . , 1) and\n[lt , t ] is non-singular. in the corresponding penalty, the overall level \u03b40 of the components in\n\u03b4 remains unpenalized. when second-order differences are used one obtains the matrix\n\n\u239e\n\u239f\u239f\u239f\u23a0 ,\n\n\u239b\n\u239c\u239c\u239c\u239d1\n\n\u03be1\n\u03be2\n\n1\n...\n1 \u03bem\n\nt =\n\nwith \u03bej = a + j/\u03b4, j = 1, . . . , m representing equidistant values (knots). the effect is that the\nlevel as well as the linear trend in the components \u03b4i from \u03b4t = (\u03b41, . . . , \u03b4m) are not penalized.\nmore generally, for b-splines with equidistant knots, the differences penalty of order d is\ndetermined by an ((m \u2212 d) \u00d7 m)-matrix l and llt has rank m \u2212 d. the corresponding\n(m \u00d7 d)-matrix t can be chosen by\n\n\u239b\n\u239c\u239c\u239c\u239c\u239d\n\n1\n\n\u00b7\u00b7\u00b7\n\n\u03be1\n...\n...\n\n1\n...\n1 \u03bem \u00b7\u00b7\u00b7\n\n\u239e\n\u239f\u239f\u239f\u239f\u23a0 ,\n\n1\n\n\u03bed\u22121\n...\n...\n\u03bed\u22121\n\nm\n\nt =\n\nwhich is spanned by a polynomial of degree d \u2212 1 defined by the knots of the b-spline. an\nearly reference to decompositions of the form given here is green (1987).\n\nc.2 linear constraints\nfrequently, it is necessary to impose constraints on the parameters in the linear predictor of the\nform\nwhere c is an m \u00d7 p-matrix of known coefficients. one approach to fitting subject to linear\nconstraints is to rewrite the model in terms of p \u2212 m unconstrained parameters by using the\nqr decomposition. according to the qr decomposition, any n \u00d7 m-matrix (m \u2264 n) can be\nwritten as\n\nc\u03b2 = 0,\n\n1\n\n$\n\n%0\n\nq|z\n\nr\n0\n\n,\n\nm = qr =\n\nwhere [q|z] is an n\u00d7n orthogonal matrix and r is an invertible m\u00d7m upper triangular matrix.\nthe matrix q is an n\u00d7 m-matrix with orthogonal columns. let the qr decomposition of ct\nbe given by\n\n1\n\n%0\n\n$\nq|z\n\nr\n0\n\n.\n\nc t =\n\n "}, {"Page_number": 511, "text": "c.3. fisher scoring with penalty term\n\nit is easily seen that any vector of the form\n\n\u03b2 = z\u03b2u\n\n499\n\nmeets the constraints for any value of the unrestricted p \u2212 m-dimensional parameter \u03b2u.\ntherefore, if one wants to fit a model with linear predictor \u03b7 = x\u03b2, subject to c\u03b2 = 0,\nfor example by minimizing (cid:16)y \u2212 x\u03b2(cid:16)2 subject to c\u03b2 = 0, one reformulates the problem in\nterms of \u03b2u. first one has to find the qr decomposition of ct , in particular the matrix z.\nthen one minimize (cid:16)y \u2212 xz\u03b2u\n\n(cid:16) w.r.t. \u03b2u, to obtain \u02c6\u03b2u, and then one sets \u02c6\u03b2 = z\u03b2u.\n\nit is not necessary to compute z explicitly because it is only necessary to compute xz\nand z\u03b2u, which may be done by simple matrix operations (householder rotations; see, for\nexample, wood, 2006a, appendix a5, a6).\n\nwhen linear constraints c\u03b2 = 0 are imposed in combination with a quadratic penalty\n\n\u03b2t k\u03b2, the quadratic penalty can be rewritten as\n\nthen, following (c.2), \u03b2u is decomposed into \u03b2u = t \u03b2u0 + w \u03b2up.\n\n\u03b2t k\u03b2 = \u03b2t\n\nu z t kz\u03b2u.\n\nc.3 fisher scoring with penalty term\nconsider the likelihood of a penalized glm:\n\np(cid:7)\n\nlp(\u03b2) =\n\nli(\u03b2) \u2212 \u03b2t \u03bb\u03b2.\n\ni=1\n\nthe corresponding penalized score function and the pseudo-fisher matrix have the form\n\nsp(\u03b2) = s(\u03b2) \u2212 \u03bb\u03b2, f p(\u03b2) = e(\u2212\u2202lp/\u2202\u03b2\u2202\u03b2t ) = f (\u03b2) + \u03bb,\n\nwith score function s(\u03b2) = x t d(\u03b2)\u03c3\nx t = (x1, . . . , xn), d(\u03b2) = diag((\u2202h(\u02c6\u03b71)/\u2202\u03b7)/\u03c32\nthen fisher scoring for solving sp(\u02c6\u03b2) = 0, which has the form\n\u22121sp(\u02c6\u03b2\n\n(k) + f p(\u02c6\u03b2\n\n(k+1) = \u02c6\u03b2\n\n(k))\n\n\u02c6\u03b2\n\n(k)),\n\n\u22121(\u03b2)(y\u2212\u03bc) and fisher matrix f (\u03b2) = x t w (\u03b2)x,\n\u22121(\u03b2)d(\u03b2).\n\n1, . . . )), w (\u03b2) = d(\u03b2)\u03c3\n\ncan also be given with pseudo-observations in the form\n\n(k)),\nwhere the pseudo-observations are \u02dc\u03b7(\u03b2) = x\u03b2 + d(\u03b2)\u22121(y \u2212 \u03bc(\u03b2)).\n\n(k+1) = (f (\u02c6\u03b2\n\n(k)) + \u03bb)\n\n(k))\u02dc\u03b7(\u02c6\u03b2\n\n\u02c6\u03b2\n\n\u22121x t w (\u02c6\u03b2\n\n "}, {"Page_number": 512, "text": "appendix d\n\nkullback-leibler distance and\ninformation-based criteria of model fit\n\nd.1 kullback-leibler distance\noften denotes the true or operating model and f =\nconsider two densities f\nf(.; \u03b8) is a parameterized candidate model. e\u2217 denotes expectation with respect to the true\ndensity f\n\n, f, where f\n\n\u2217\n\n\u2217\n\n\u2217\n\n.\n\nthe kullback-leibler distance between two (continuous) densities f\n\u2217(z)\nf(z)\n\n, f) = e\u2217 log( f\n\n\u2217(z)\nf(z)\n\n(z) log( f\n\nkl(f\n\n) =\n\n\u2217\n\n\u2217\n\nf\n\n)dz.\n\n)\n\n\u2217\n\n, f is defined as\n\n\u2217\n\n\u2217\n\n\u2217\n\n, f) (cid:8)= kl(f, f\n\n. it should be noted that kl(f\n\noften one uses the notation i(f\n, f), referring to the information lost when f is used to ap-\n, f) is not a distance in the usual sense because\nproximate f\n\u2217). more precisely, kl may be considered a directed or oriented distance\nkl(f\n. the kl distance has also been called the divergence or\nrepresenting the distance from f to f\nkl discrepancy or kullback-leibler information. the kl distance is always positive, except\nand f are identical. it is noteworthy that the kl distance is a\nwhen the two distributions f\ndistance between two statistical expectations. one obtains\n\n\u2217\n\n\u2217\n\n\u2217\n\n(cid:25)\n\n(cid:26)\n\n\u2217\n\nkl(f\n\n, f) = e\u2217 log\n\n)\n\n=\n\nlog(f\n\n\u2217(z)\nf\nf(z)\n\u2217\n\n(z))f\n\n)\n\n= e\u2217 log(f\n(z)dz \u2212\n\n\u2217\n\n\u2217\n\n(z)) \u2212 e\u2217 log(f(z))\n\nlog(f(z))f\n\n\u2217\n\n(z)dz.\n\n\u2217(z)), depends only on the unknown true distribution. since\nthe first expectation, e\u2217 log(f\nin actual data analysis the true distribution is never known, it can be considered as a con-\nstant. what one obtains is a measure of the relative distance, which might be used to compare\ncandidate models. for two candidate models f1, f2 one obtains kl(f\n, f1) =\n\u2212 e\u2217 log(f1(z)) + e\u2217 log(f2(z)), where the constant term has vanished. since the constant\nis irrelevant for comparisons of models, some authors define \u2212 e\u2217 log(f(z)) as the kullback-\nleibler discrepancy.\nthus, for candidate model f the discrepancy \u2212 e\u2217 log(f(z)) should be small. the crucial\nis known and f\npoint is that this discrepancy can only be computed if the operating model f\nis fully specified. if f is a parametric model f(z|\u03b8), then \u03b8 has to be estimated. let f(z|\u02c6\u03b8(y))\ndenote the density, with the parameter \u02c6\u03b8(y) being estimated from the sample y.\n\n, f1) \u2212 kl(f\n\n\u2217\n\n\u2217\n\n\u2217\n\n500\n\n "}, {"Page_number": 513, "text": "d.1. kullback-leibler distance\n\n501\n\nin a series of papers akaike (1973, 1974) proposed using for an applied kl model selection\n\nan estimate of\n\ney ez log f(z|\u02c6\u03b8(y)),\n\n(d.1)\nwhere y and z are independent random samples from the same distribution and both expecta-\n. thus ey and ez represent e\u2217. the selection\ntions are taken with respect to the true density f\ntarget ey ez log f(z|\u02c6\u03b8(y)) represents the (negative) expected kl discrepancy in a future sam-\nple z where expectation is taken with respect to the parameter generating sample y. therefore\nit gives the expected kl discrepancy if estimation is taken into account. it is essential that y\nand z are samples of the same size and distribution.\n\n\u2217\n\nan approximately unbiased estimation of (d.1) for large samples is the corrected log-\n\nlikelihood\n\nl(\u02c6\u03b8) \u2212 dim(\u03b8),\n\nwhere l(\u02c6\u03b8) is the log-likelihood evaluated at the estimate \u02c6\u03b8 and dim(\u03b8) is the dimensionality\nof the parameter \u03b8. akaike (1973) defined \"an information criterion\" (aic) by multiplying the\nrespected log-likelihood by \u22122 to obtain\n\naic = \u22122(l(\u02c6\u03b8) \u2212 dim(\u03b8)),\n\nwhich estimates the expected kl discrepancy \u22122 ey ez log f(z|\u02c6\u03b8(y)). a bias-corrected aic\ncriterion that is more appropriate for small sample sizes has been suggested by hurvich and\ntsai (1989). it has with d = dim(\u03b8) the form\n\naicc = \u22122(l(\u02c6\u03b8) \u2212 dn/(n \u2212 d \u2212 1)) = aic +\n\n2d(d + 1)\nn \u2212 d \u2212 1 .\n\nfor an extensive treatment of aic see burnham and anderson (2002).\n\nd.1.1 kullback-leibler and ml estimation\nif \u03b4y is considered a degenerate distribution with mass 1 on the data y, that is, \u03b4y(y) =\n1, \u03b4y(z) = 0, z (cid:8)= y and f = f(.; \u03b8) is a parameterized density, one obtains\n\nkl(\u03b4y, f(., \u03b8)) = \u2212 log(f(y; \u03b8)).\n\nthus the log-likelihood may be considered as the kullback-leibler discrepancy between f\u03b8\nand the \"data\" \u03b4y. the log-likelihood takes its maximal value with respect to \u03b8 if kl(\u03b4y, f\u03b8)\nis minimal.\nif y represents a vector yt = (y1, . . . , yn) containing independent observations, the dis-\ntribution representing the data is given by \u03b4y(z) = \u03b4y1(z1) \u00b7 . . . \u00b7 \u03b4yn(zn), where zt =\n(z1, . . . , zn). with f also denoting the distribution of y one obtains\n\n(cid:25)\n\n\u03b4y(z)\nf(z; \u03b8)\n\n(cid:26)\n\n= \u2212 log(f(y1, . . . , yn; \u03b8)) = \u2212 n(cid:7)\n\nkl(\u03b4y, f(., \u03b8)) = e\u03b4y log\n\nlog(f(yi; \u03b8)),\n\ni=1\n\nwhich equals the negative likelihood of observations y1, . . . , yn.\n\nd.1.2 kullback-leibler and discrete distributions\nfor discrete distributions with support {wi, i = 1, 2, . . .}, f\nhas the kullback-leibler distance\n\n\u2217\n\n\u2217\n\nkl (f\n\n, f) = e\u2217 log\n\n=\n\n\u2217\n\nf\n\n(wi) log\n\n(cid:26)\n\n(cid:25)\n\n\u2217(z)\nf\nf(z)\n\n(cid:7)\n\nwi\n\n(cid:25)\n\n(cid:26)\n\n.\n\n\u2217(wi)\nf\nf(wi)\n\nand f are mass functions and one\n\n "}, {"Page_number": 514, "text": "\u2217\n\n(cid:7)\n\n502\n\nappendix d. information-based criteria of model fit\n\nthe true mass function f\n\nmay be replaced by the vector \u03c0t = (\u03c01, \u03c02, . . . ), \u03c0i = f(wi),\nof the true probabilities and if f is the corresponding estimate \u02c6\u03c0t = (\u02c6\u03c01, \u02c6\u03c02, . . . ), \u02c6\u03c0i = f(wi),\none has\n\n(cid:26)\n\n(cid:25)\n\nkl(\u03c0, \u02c6\u03c0) =\n\n\u03c0i log\n\n\u03c0i\n\u02c6\u03c0i\n\n.\n\nif \u03c0 is replaced by the data in the form of relative frequencies pt = (p1, p2, . . . ), which means\nindependent observations have been assumed, one has\n\nkl(p, \u02c6\u03c0) =\n\n(cid:14)\n\npi log(pi) \u2212\n\npi log(\u02c6\u03c0i) = l(p; p) \u2212 l(p; \u02c6\u03c0i),\n\ni\n\ni\n\nwhere l(p; \u02c6\u03c0i) =\ni pi log(\u02c6\u03c0i) is the grouped form of the log-likelihood with p representing\nthe data. l(p, p) corresponds to the saturated model and does not depend on the parameters that\ndetermine the candidate model and therefore the estimates \u02c6\u03c0i. for the multinomial distribution\nand \u02c6\u03c0 the maximum likelihood estimate 2 kl(p, \u02c6\u03c0) is called the deviance.\n\n(cid:7)\n\ni\n\n(cid:7)\n\nd.1.3 kullback-leibler in generalized linear models\nlet f\nis from a natural exponential family:\n\n, f be replaced by the densities depending on the parameters f(y; \u03b8\n\n\u2217\n\n\u2217\n\n), f(y; \u03b8), where f\n\n9\n8\n(yt \u03b8 \u2212 b(\u03b8))/\u03c6 + c(y, \u03c6)\n(cid:26)\n\nf(y|\u03b8) = exp\n(cid:25)\n\n9\nwith \u03c6 being an additional dispersion parameter that is omitted in the notation. then one has\n)) \u2212 (yt \u03b8 \u2212 b(\u03b8))\n\n\u2217 \u2212 b(\u03b8\n\n), f(., \u03b8)) = e\u03b8\n\nkl(f(., \u03b8\n\n(yt \u03b8\n\n\u2217 log\n\n8\n\ne\u03b8\n\n=\n\n\u2217\n\n\u2217\n\n\u2217\n\n\u2217\n\n.\n\nf(y, \u03b8\n)\nf(y, \u03b8)\n\n1\n\u03c6\n\nconsidering the dependence of \u03b8 on the mean \u03bc in the form \u03b8 = \u03b8(\u03bc), one may consider the\nkullback-leibler distance between the distribution in which the mean is given as the observa-\ntion and the underlying mean:\n\n8\n\n9\n(yt \u03b8(y) \u2212 b(\u03b8(y)) \u2212 (yt \u03b8(\u03bc) \u2212 b(\u03b8(\u03bc))\n\n.\n\nkl(f(., \u03b8(y)), f(., \u03b8(\u03bc))) =\n\n1\n\u03c6\n\ne\u03b8(y)\n\nthis form of kullback-leibler is appropriate only in the non-degenerate case where \u03b8(y) is\nwell defined. for a single observation from the binomial distribution with y \u2208 {0, 1}, one has a\ndegenerated distribution with \u03b8(y) \u2208 {\u221e, \u2212\u221e}. however, in this case one may consider that\nf(y, \u03b8(y)) = 1 and therefore obtains\n\nkl(f(., \u03b8(y)), f(., \u03b8(\u03bc))) = kl(\u03b4y, f(., \u03b8(\u03bc)) = \u2212 log(f(y; \u03b8(\u03bc)),\n\nwhich is the negative log-likelihood contribution of observation y. it is equal to the kullback-\nleibler discrepancy where \u03b4y puts mass 1 on the observation y.\n\nfor independent observations yi1, . . . , yini at design point xi, one may consider the vector\nj yij/n as the mean observation. one obtains for the log-likelihood contribution of\n\n(cid:14)\n\n8\n\n9\nij\u03b8(\u02c6\u03bci) \u2212 b(\u03b8(\u02c6\u03bci))\nyt\n\n/\u03c6 = ni\n\n8\n\n9\ni \u03b8(\u02c6\u03bci) \u2212 b(\u03b8(\u02c6\u03bci))\n\u00afyt\n\n/\u03c6.\n\n\u00afyi =\nyi1, . . . , yini\n\nni(cid:7)\n\nj=1\n\nli(\u02c6\u03bci) =\n\n "}, {"Page_number": 515, "text": "d.1. kullback-leibler distance\n\n503\n\nthis is equivalent to li(\u02c6\u03bci) = kl(\u03b4yi , f(., \u03b8(\u02c6\u03bci)) = ni kl(\u03b4\u00afyi , f(., \u03b8(\u02c6\u03bci)). thus the de-\nviance based on grouped observations has the form\n\n(cid:2)\ng(cid:7)\n(cid:2)\ng(cid:7)\n\ni=1\n\n&\nli(\u02c6\u03bci) \u2212 li(\u00afyi)\n8\n\nni\n\ni=1\n\nd = \u22122\u03c6\n\n= \u22122\u03c6\n\n9&\nkl(\u03b4\u00afyi , f(., \u03b8(\u02c6\u03bci))) \u2212 kl(\u03b4\u00afyi , f(., \u03b8(\u00afyi)))\n\n.\n\nd.1.4 decomposition\nlet y\nmodel where \u03b8 = \u03b8(\u03bc(x)). then one obtains for the new observation (y, x)\n\n\u2217(x) = e(y|x) be the true expectation with parameter \u03b8\n\n\u2217\n\nand let f(., \u03b8) be a candidate\n\neyx kl(\u03b4y, f(., \u03b8))\n\n= eyx kl(\u03b4y, f(., \u03b8\n\n\u2217\n\n)) + ex kl(f(., \u03b8\n\n\u2217\n\n), f(., \u03b8))\n\n(d.2)\n\nderivation omitting x as an argument yields\n\neyx kl(\u03b4y, f(., \u03b8)) =\n\n= eyx e\u03b4y log \u03b4y(z)/f(z, \u03b8\nf(z, \u03b8)/f(z, \u03b8\n= eyx kl(\u03b4y, f(., \u03b8\n\n\u2217\n\n\u2217\n)\n\u2217\n)\n\n)) + ey,x e\u03b4y log(f(z, \u03b8\n\n\u2217\n\n)/f(z, \u03b8)).\n\nthe last term equals\n\u2217\n\neyx log(f(y, \u03b8\n\n)/f(y, \u03b8)) = ex ey log(f(y, \u03b8\n\n\u2217\n\n)/f(y, \u03b8)) = ex kl(f(., \u03b8\n\n\u2217\n\n), f(., \u03b8)).\n\nthe first term in (d.2) depends only on the distribution. it is easy to show that minimization\nof eyx kl(\u03b4y, f(.,|\u03b8)) yields expectation \u03bc(\u03b8), which is the closest to \u03bc(\u03b8\n) (see also hastie\nand tibshirani, 1986).\n\n\u2217\n\n "}, {"Page_number": 516, "text": "appendix e\n\nnumerical integration and tools for\nrandom effects modeling\n\ne.1 laplace approximation\nthe laplace approximation provides an approximation for integrals of the form\nwhen n is large (for example, de bruijn, 1981). it states that for a unidimensional \u03b8 one has\n\nenl(\u03b8)d\u03b8\n\n\u2019\n\nwhere \u02c6\u03b8 is the unique maximum of l(\u03b8) and \u02c6\u03c32 = \u22121/(\u22022l(\u02c6\u03b8)/\u2202\u03b82). the result is easily\nderived by using the taylor approximation of second order, l(\u03b8) \u2248 l(\u02c6\u03b8) + l(\u02c6\u03b8)(\u03b8 \u2212 \u02c6\u03b8) +\n2(\u22022l(\u02c6\u03b8)/\u2202\u03b82)(\u03b8 \u2212 \u02c6\u03b8)2.\n\n1\n\nfor a p-dimensional \u03b8 one obtains by the taylor approximation\n\n)\n\n)\n\nenl(\u03b8)d\u03b8 \u2248 exp(nl(\u02c6\u03b8))\n\n\u221a\n2\u03c0,\n\n\u02c6\u03c3\nn1/2\n\n*\n\n| \u02c6\u03c3|\nn1/2\n\nenl(\u03b8)d\u03b8 \u2248 enl(\u02c6\u03b8)\n\n(2\u03c0)p/2,\n\nwhere \u02c6\u03b8 is the unique maximum of l(\u03b8) and\n\nby substituting g(\u03b8) = enl(\u03b8) one obtains in the unidimensional case\n\n\u22121.\n\n\u02c6\u03c3 = (\u2212\u2202l(\u02c6\u03b8)/\u2202\u03b8\u2202\u03b8t )\n)\n\ng(\u03b8)d\u03b8 \u2248 g(\u02c6\u03b8)\u02c6\u03c3g\n\n\u221a\n2\u03c0,\n\nwhere \u02c6\u03c32\nate case one has\n\ng = \u02c6\u03c32/n = (\u2212 \u22022 log g(\u02c6\u03b8)\n)\n\n\u2202\u03b82\n\n)\u22121 and \u02c6\u03b8 is the unique maximum of g(\u03b8). for the multivari-\n\ng(\u03b8)d\u03b8 \u2248 g(\u02c6\u03b8)\u02c6\u03c3g(2\u03c0)p/2,\n\nwhere \u02c6\u03c32\n\ng = | \u2212 \u22022 log g(\u02c6\u03b8)/\u2202\u03b8\u2202\u03b8t|\u22121.\n\n504\n\n "}, {"Page_number": 517, "text": "e.2. gauss-hermite integration\n\n505\n\ne.2 gauss-hermite integration\nfor a unidimensional function g(x),\nh(x) exp(\u2212x2) by\n\n) \u221e\n\u2212\u221e h(x) exp(\u2212x2)dx \u2248 k(cid:7)\n\ni=1\n\nh(\u03bei)wi,\n\nthe gauss-hermite rule approximates integrals of\n\nwhere the node \u03bei is the ith zero of the hermite polynomial having degree k, and wi repre-\nsents fixed weights depending on k. tables for nodes and weights are given, for example, in\nabramowitz and stegun (1972). the approximation is exact if h(x) is a polynomial of degree\n2k \u2212 1. thus, by increasing k the approximation improves.\n)\n\u03bc one obtains)\n\nan often-used integral is based on the normal density. by simply substitutiing x =\n\n2\u03c0\u03c3z+\n\n\u221a\n\n\u22121/2\n\n\u221a\nh(\n\n2\u03c3z + \u03bc) exp(\u2212z2)dz\n\nh(x)\n\n1\u221a\n2\u03c0\u03c3\n\nand therefore the gauss-hermite approximation\n\n2\u03c32\n\n)dx = \u03c0\n\nexp(\u2212(x \u2212 \u03bc)2\n)\nh(x)\u03c6\u03bc,\u03c3(x) \u2248 k(cid:7)\n\n\u221a\nh(\n\n2\u03c3\u03bei + \u03bc)vi,\n\n(e.1)\n\ni=1\n\n\u221a\nwhere vi = wi/\n\u03c0 is the transformed weight, \u03bei is the tabulated ith zero of the hermite\n\u22121/2 exp(\u2212(x \u2212 \u03bc)2/(2\u03c32)) is the gaussian density\npolynomial, and \u03c6\u03bc,\u03c3(x) = (2\u03c0)\u22121/2\u03c3\nwith mean \u03bc and variance \u03c32.\n) \u221e\n\u2212\u221e g(t)dt.\n\nthe adaptive gauss-hermite quadrature aims at sampling in an appropriate region. thus \u03bc\n\nand \u03c3 are chosen deliberately. consider the integral for function g:\n\nby choosing\n\none obtains from (e.1) the adaptive gauss-hermite approximation:\n\n\u02c6\u03bc = arg max\n\ng(x)\n\n\u02c6\u03c3 = (\u2212 \u22022 log g(\u02c6\u03bc)\n\n\u2202\u03bc2\n\n\u22121,\n)\n\nx\n\n) \u221e\n\u2212\u221e g(t)dt =\n\u2248\n\n) \u221e\n(cid:7)\n\u221a\n\ng(t)\n\u03c6\u02c6\u03bc,\u02c6\u03c3(t) \u03c6\u02c6\u03bc,\u02c6\u03c3(t)dt\n\u2212\u221e\n\u221a\n(cid:7)\nh(\n\n2\u02c6\u03c3\u03bei + \u02c6\u03bc)vi\n\n\u221a\ni )g(\n\ni\n2\u03c0\u02c6\u03c3\n\n=\n\ni\n\nexp(\u03be2\n\n2\u02c6\u03c3\u03bei + \u02c6\u03bc)vi,\n\nwhere h(t) = g(t)/\u03c6\u02c6\u03bc,\u02c6\u03c3(t). for only one node one obtains the laplace approximation by\ninserting the node and weight of the corresponding hermite polynomial:\n\n\u221a\n\u221a\n2\u03c0\u02c6\u03c31 exp(\u03be2)g(\n\n\u221a\n\n2\u02c6\u03c31\u03be + \u02c6\u03bc)v1 =\n\n2\u03c0\u02c6\u03c3g(\u02c6\u03bc).\n\n) \u221e\n\u2212\u221e g(t)dt \u2248\n\nsince the laplace approximation turns out to be the special case with one node, liu and pierce\n(1994) call (e.1) the k-order laplace approximation.\n\n "}, {"Page_number": 518, "text": "506\n\nappendix e. tools for random effects modeling\n\ne.2.1 multivariate gauss-hermite integration\nthe m-dimensional gauss-hermite approximation has the form\n\n)\n\nh(x) exp(\u2212x\n\n(cid:3)\n\nx)dx =\n\nw(1)\ni1\n\n\u00b7 . . . \u00b7 w(m)\n\nim\n\nh(\u03be(1)\n\ni1 , . . . \u03be(m)\n\nim\n\n),\n\nk1(cid:7)\n\n\u00b7\u00b7\u00b7 km(cid:7)\n\ni1=1\n\nim=1\n\nwhere w(s)\nis\n(i1, . . . , im), \u03bet\n\ni = (\u03be(1)\n\nare the weights and \u03be(s)\nis\n), and wi = w(1)\ni1\nh(x) exp(\u2212xt x) =\n\ni1 , . . . \u03be(m)\n\n)\n\nim\n\n\u00b7 . . . \u00b7 w(m)\n(cid:7)\n\nwih(\u03bei).\n\ni\n\nare the nodes of the ith variable. with multiple index i for\n\nim , one obtains the form\n\nthe number of terms within the sum of the left-hand side can become rather large. if one uses\nk = k1 = . . . , km quadrature points in each component, the sum includes mk terms. therefore,\nm has to be rather small in order to reduce the computational task.\n\n\u2019\n\nfor the adaptive gauss-hermite integration one considers the general integral\n\nusing the density \u03c6\u03bc,\u03c3 of a normal distribution n(\u03bc, \u03c3), it may be written as\n\ng(t)dt. by\n\n)\n\n)\n)\n\ng(t)dt =\n\n=\n\n{ g(t)\n\u03c6\u03bc,\u03c3 (t)\ng(t)\n\n\u03c6\u03bc,\u03c3 (t)\n\n}\u03c6\u03bc,\u03c3 (t)dt\n|2\u03c0|\u2212m/2|\u03c3|\u22121/2 exp(\u22121\n2\n\nwith \u02dct = 1\u221a\nusual form of gauss-hermite integration:\n\n\u22121/2(t \u2212 \u03bc) \u21d4 t =\n)\n)\n\n\u03c3\n\n2\n\n(t \u2212 \u03bc)t \u03c3\n\n\u22121(t \u2212 \u03bc))dt.\n\n\u221a\n2\u03c3 1/2\u02dct + \u03bc and h(t) = g(t)/\u03c6\u03bc,\u03c3 (t), one obtains the\n\ng(t)dt =\n\n\u221a\n2\u03c3 1/2\u02dct + \u03bc)(2\u03c0)\nh(\n\n\u2212m/2\n\n\u221a\n(cid:3)\n2 exp(\u02dct\n\n\u02dct)d\u02dct.\n\nwith w(s)\nis\n\nand \u03be(s)\nis\n\n)\n\ndenoting the weights and nodes of the sth variable, one obtains\ng(t)dt \u2248 21/2(2\u03c0)\n\n2\u03c3 1/2\u03bei + \u03bc),\n\n\u221a\nh(\n\ni1 . . . w(m)\nw(1)\n\n\u2212m/2\n\nim\n\n(cid:7)\n\ni\n\nwhere \u03bet\n\ni = (\u03be(1)\n\n) and i denotes the multiple index (i1, . . . im). therefore,\n\nim\n\ni1 ; . . . \u03be(m)\ng(t)dt \u2248 21/2(2\u03c0)\n(cid:7)\n\n\u221a\n\n(cid:7)\n\n\u2212m/2\n\n|\u03c3|1/2w(1)\n\n=\n\n2\n\ni\n\n\u221a\ng(\n\nim\n\ni1 . . . w(m)\nw(1)\n|2\u03c0|\u2212m/2|\u03c3|\u22121/2 exp(\u2212\u03bet\n\u221a\ng(\n\n2\u03c3 1/2\u03bei + \u03bc)\ni \u03bei)\n\n2\u03c3 1/2\u03bei + \u03bc) exp(\u03bet\n\ni1 . . . w(m)\n\nim\n\ni\n\ni \u03bei).\n\nup until now \u03bc and \u03c3 have not been specified. to sample g(t) in a suitable range one chooses\n\u03bc as the mode of g(t) and \u03c3 = (\u2212\u22022/\u2202 log g(t)t\u2202tt )1/2.\n\nfor the log-likelihood of a mixed model one obtains\n\nli(\u03b2, q) =\n\u2248\n\n(cid:7)\n\n2\n\ni\n\nf(yi, bi, \u03b2)\u03c6o,q(bi)dbi\n\n|\u03c3|1/2w(1)\n\ni1 . . . w(m)\n\nim\n\n\u221a\n\n\u221a\n2\u03c3 1/2\u03bei + \u03bc) exp(\u03bet\n2\u03c3 1/2\u03bei + \u03bc)\u03c6o,q(\n\ni \u03bei).\n\nf(yi,\n\n)\n\n)\n\u221a\n\n "}, {"Page_number": 519, "text": "\u23a1\n\u23a2\u23a2\u23a2\u23a2\u23a2\u23a3\n\n\u23a1\n\u23a2\u23a2\u23a2\u23a2\u23a2\u23a3\n\ne.3. inversion of pseudo-fisher matrix\n\n507\n\ne.3 inversion of pseudo-fisher matrix\nthe pseudo-fisher matrix f p(\u03b4) from section 14.3.3 can be inverted by using the partitioning\ninto\n\nwith\n\nf p(\u03b4) =\n\nf \u03b2\u03b2 f \u03b21 f \u03b22\nf 1\u03b2 f 11\nf 2\u03b2\n...\nf n\u03b2\n\nf 22\n\n0\n\n\u00b7\u00b7\u00b7 f \u03b2n\n0\n\n...\n\nf nn\n\n(cid:26)\n\nn(cid:7)\n(cid:26)\n\ni=1\n\n=\n\n\u22022l(\u03b4)\n\u2202\u03b2\u2202bt\ni\n\n(cid:26)\n\n(cid:25)\n\n(cid:25)\n\n\u22022l(\u03b4)\n\u2202\u03b2\u2202\u03b2t\n(cid:25)\ni\u03b2 = \u2212e\n\u22022l(\u03b4)\n\u2202bi\u2202bt\ni\n\nf \u03b2\u03b2 = \u2212e\n\nf \u03b2i = f t\nf ii = \u2212e\n\nx t\n\ni di(\u03b4)\u03c3\n\n\u22121\ni\n\n(\u03b4)di(\u03b4)x i,\n\n= x t\n\ni di(\u03b4)\u03c3\n\n\u22121\ni\n\n(\u03b4)di(\u03b4)zi,\n\n= z t\n\ni di(\u03b4)\u03c3\n\n\u22121\ni\n\n(\u03b4)di(\u03b4)zi + q\n\n\u22121.\n\nby using standard results for inverting partitioned matrices one obtains for the inverse the easy-\nto-compute form\n\n\u22121(\u03b4) =\n\nf\n\nv \u03b2\u03b2 v \u03b21 v \u03b22\nv 1\u03b2 v 11 v 12\nv 2\u03b2 v 21 v 22\n...\n...\nv n\u03b2 v n1\n\n. . .\n\n. . . v \u03b2n\n. . . v 1n\n. . . v 2n\n...\n. . . v nn\n\nwith\n\nv\u03b2\u03b2 = (f \u03b2\u03b2 \u2212 n(cid:7)\n\nf \u03b2if\n\nv ii = f\n\n\u22121\nii + f\n\ni=1\n\u22121\nii f i\u03b2v \u03b2\u03b2f \u03b2if\n\n\u22121\nii f i\u03b2)\n\n\u22121, v \u03b2i = v t\n\u22121\nii\n\n, v ij = v t\n\ni\u03b2 = \u2212v \u03b2\u03b2f \u03b2if\n\n\u22121\nii ,\n\nji = f\n\n\u22121\nii f i\u03b2v \u03b2\u03b2f \u03b2jf\n\njj , i (cid:8)= j.\n\u22121\n\n\u23a4\n\u23a5\u23a5\u23a5\u23a5\u23a5\u23a6 ,\n\n\u23a4\n\u23a5\u23a5\u23a5\u23a5\u23a5\u23a6 ,\n\n "}, {"Page_number": 520, "text": " "}, {"Page_number": 521, "text": "list of examples\n\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\n1.7\n2.1\n2.2\n2.3\n2.4\n2.5\n2.6\n4.1\n4.2\n4.3\n4.4\n4.5\n4.6\n4.7\n4.8\n4.9\n4.10\n5.1\n6.1\n6.2\n6.3\n6.4\n6.5\n6.6\n6.7\n6.8\n7.1\n7.2\n7.3\n7.4\n7.5\n7.6\n7.7\n\n. . . . . . . .\n\n. . . . . . . . . . .\n\n. . . . . . . . . . .\n. . . . . . . . . . .\n\n. . . . . . . .\n. . . . .\n\n. . . . . . . . . .\n\n. . . . . . .\n. . . . . . . .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n\n. . . . .\n\n. . . . .\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . . . .\n\n. . . . . . . .\n\n. . . . .\n. . . . .\n\n. . . . . .\n. . . . .\n\n. . . . .\n. . . . . . . .\n\n. . . . . .\n. . . . .\n. . . .\n.\n. .\n. . . .\n. . . .\n. . . . .\n. . . . .\n. . . . . .\n. . . . . .\n. . . . . .\n\n.\nduration of unemployment\n.\ncar in household . . .\n.\n.\n.\n.\ntravel mode\n.\nknee injuries .\n.\n.\n.\ninsolvent companies in berlin .\n.\nnumber of children .\n.\n.\n.\ncredit risk .\n.\n.\n.\ncar in household . . .\n.\nvasoconstriction . . .\n.\n. . . . . .\nduration of unemployment\n.\n. . . . . .\n.\nduration of unemployment\nduration of unemployment\n. . . . . .\n.\nduration of unemployment with predictor education level\nunemployment\n.\ncommodities in household . . .\n.\nunemployment\n.\n.\nfood-stamp data . . .\nunemployment\n.\n.\nexposure to dust (non-smokers)\n.\nexposure to dust\n.\n.\nduration of unemployment\n.\n.\nsimulation .\n.\n.\nchoice of coffee brand .\n.\n.\n.\n.\n.\nteratology .\n.\n.\n.\n.\n.\nheart disease .\n.\n.\n.\n.\nheart disease .\n.\n.\nheart disease .\n.\n.\n.\n.\n.\n.\nheart disease .\n.\n.\n.\nheart disease .\n.\n.\nabortion of treatment\n.\n.\ndemand for medical care .\n.\n.\n.\nmunich rent\n.\n.\n.\nnumber of children .\n.\n.\nencephalitis .\n.\nnumber of children .\n.\n.\n.\n.\n.\nencephalitis .\n.\n.\ninsolvent firms .\n.\n.\n.\ndemand for medical care .\ninsolvent firms .\n.\n.\n\n.\n. . . . . .\n.\n. . . . .\n.\n. . . . . . .\n. . . . .\n. . . . . .\n.\n. . . . . .\n.\n. . . .\n. . . .\n. . . .\n. . . .\n. . . .\n. .\n. . . . . .\n. . . .\n. . . .\n. . . .\n. . . .\n. . . .\n. . . .\n. . . . . .\n. . . .\n\n. . . . .\n. . . . .\n. . . . .\n. . . . .\n. . . . .\n. . . . . . .\n\n. . . . .\n. . . . . .\n. . . .\n. . . . . .\n. . . .\n. . . . .\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n. . . . . . . .\n\n. . . . . . . .\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n. . . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . .\n\n. . . . .\n\n.\n.\n.\n.\n.\n.\n.\n\n. . .\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . . .\n\n. . . . .\n\n. . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . .\n. . . . . . . .\n. . . . . . . .\n. . . .\n\n. . .\n\n. . . .\n\n. . . .\n\n. . . .\n\n. . . . . . . .\n. . . .\n\n. . .\n. .\n. . .\n.\n.\n. . . . . .\n. . .\n. . .\n. .\n. .\n. . .\n. . .\n. . .\n. . . . . . .\n. . . . .\n.\n. . . . .\n. .\n. . . . .\n. . . . . . . . . .\n\n1\n2\n3\n3\n3\n3\n4\n39\n41\n42\n44\n45\n46\n90\n90\n95\n96\n98\n98\n. . 100\n. . . 105\n. 110\n. . . 111\n. . 136\n. . . 148\n. . . 153\n. . . 156\n. . . 162\n. . . 166\n. 168\n. . . 172\n. . . . . . 176\n. . . . . . 181\n. 181\n. . . . . . 188\n. .\n. 188\n. . . 189\n. . . 193\n. . . 196\n\n. . .\n. . .\n. . .\n. . .\n. . .\n\n. . . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. .\n\n. . . . . . . .\n\n. . . . . . . .\n\n. . . . . . . . . . . .\n\n. . . . . . .\n\n. . . . . . . .\n\n. . . . . . . . . . . .\n. . .\n. . . . . . . .\n. . .\n\n. . . . . . . .\n\n. . . . . . . .\n. . . . . . . .\n. . . . . . . .\n. . . . . . . .\n. . . . . . . .\n. . . . . .\n\n. . . . . . . .\n. . . . . . .\n\n. . . . . . . . . . .\n\n. . . . . . . .\n\n. . . . . . . . . . .\n\n509\n\n "}, {"Page_number": 522, "text": "510\n\nlist of examples\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\ndemand for medical care .\n7.8\n.\ndemand for medical care .\n7.9\ndemand for medical care .\n.\n7.10\npreference for political parties .\n8.1\naddiction .\n.\n8.2\npreference for political parties .\n8.3\n.\n.\n.\n.\ntravel mode\n8.4\n.\n.\n.\naddiction .\n.\n8.5\n.\n.\n.\naddiction .\n.\n8.6\n.\n.\n.\npaired comparison .\n8.7\n.\n.\n.\nred bus\u2013blue bus .\n8.8\n.\n.\nparty choice .\n.\n.\n8.9\n.\ncontraceptive method .\n.\n8.10\n.\n.\n.\nunemployment\n9.1\n.\n.\n.\nretinopathy . .\n9.2\n.\n.\nknee injuries .\n.\n9.3\n.\nknee injuries .\n.\n.\n9.4\n.\n.\nretinopathy data . . . .\n9.5\n.\n.\nretinopathy data . . . .\n9.6\n.\n.\n.\neye vision .\n9.7\n.\narthritis .\n.\n.\n.\n9.8\n.\n.\n.\neye vision .\n9.9\n.\narthritis .\n.\n.\n.\n9.10\n.\nduration of unemployment\n10.1\nduration of unemployment\n.\n10.2\nexposure to dust (smokers)\n10.3\n.\n.\n.\nnumber of children .\n10.4\naddiction .\n.\n.\n.\n.\n10.5\npreference for political parties .\n10.6\n.\n.\nincome data\n.\n10.7\n.\nforest health .\n.\n10.8\n.\ncanadian weather data .\n10.9\n.\n.\n10.10 mass spectrometry . .\n.\n10.11 canadian weather data .\n.\n10.12 prostate cancer .\n.\n.\n11.1\n11.2\n.\n.\n12.1\n.\n12.2\n.\n12.3\n12.4\n.\n.\n12.5\n.\n13.1\n.\n13.2\n13.3\n.\n.\n13.4\n.\n13.5\n.\n14.1\n14.2\n.\n\n.\n.\n.\n.\n.\n.\nduration of unemployment\nexposure to dust\n.\n.\n.\n.\nbirth data .\n.\n.\n.\n.\nleukoplakia .\n.\n.\n.\n.\nbirth data .\n.\nbirth data .\n.\n.\n.\n.\n.\n.\nleukoplakia .\n.\n.\n.\n.\nepilepsy .\n.\n.\n.\n.\n.\nbirth data .\n.\nbirth data .\n.\n.\n.\n.\n.\nknee injuries .\n.\n.\n.\nbirth data .\n.\n.\naids study .\n.\n.\nrecovery scores\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. . .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n. . .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. . .\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n. . . . . .\n. . . . . .\n\n. . . . . .\n. . . . . .\n. . . . . .\n. .\n.\n. .\n. . . . . .\n.\n.\n.\n. . . . . .\n. . . .\n.\n.\n. . . . .\n. . . .\n. . . .\n.\n.\n. . . .\n. . . . .\n. . . .\n. . . . .\n. . . . . .\n. . . . . .\n. .\n. . . .\n.\n. .\n.\n. . . . .\n. . . . . .\n. . . . . .\n.\n. . . . . .\n.\n. . . . . .\n.\n.\n. . . . .\n. . . . . .\n.\n. . . . . .\n. . . .\n. . . . . .\n. . . . . .\n. . . .\n. . . .\n. . . . . .\n. . . . . .\n. . . .\n. . . . . .\n.\n.\n\n. . . . . .\n\n. . . . . .\n. . . . . .\n. . . . . .\n\n. . . . . . . .\n. . . . . . . .\n. . . . . . . .\n\n. . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . .\n\n. . . . .\n\n. . . . . .\n\n. . . . . . . . . .\n\n. . . .\n. . . . . . . .\n. . . .\n\n. . . . . .\n\n. . . . . . .\n\n. . . . . . . .\n. . . . . . . .\n. . . . . . . .\n\n. . . . .\n. . . . .\n. . . . .\n\n. . . . . . . .\n. . . . . . . .\n. . . . . . . .\n\n. . . . . .\n. . . . . . . .\n. . . .\n. . . . .\n. . . . .\n\n. . . . . .\n\n. . . . . . .\n\n. . . . .\n\n. . . . . . . .\n. . . . . . . . . .\n\n. . . .\n\n. . . . .\n. . . . . . . . . . . .\n. . . . . . . .\n. . . . . . . .\n. . . . . . . . . .\n. . . . . . . . . .\n. . . . . . . .\n. . . . . . . . . .\n. . . . . . . .\n. . . . . . . . . .\n\n. . .\n\n. . . . .\n. . . .\n. . . . .\n. . . .\n\n. . . . . .\n. . . . . .\n\n. . . . . . . .\n. . . . . . . .\n\n. . . . . .\n\n. . . . . . . . . .\n\n. . . . . .\n\n. . . . . . .\n\n. . . . . . . .\n\n. . . . .\n\n. . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . .\n. . . .\n\n. . . . . . . . . . .\n. . . . . .\n\n. . . . . . .\n\n. . . . . . . . . .\n. . . . . . . . . .\n. . . . . . . . . .\n. . . . . . . . . . .\n. . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . .\n\n. . . . . .\n\n. . . . . . .\n\n. . . . .\n\n. . . . . . . .\n\n. . . . . .\n. . . . . .\n\n. . . . . . .\n. . . . . . .\n\n. . . . .\n. . . . .\n\n. . . . . . . .\n. . . . . . . .\n\n. . . . . .\n. . . . . .\n\n. . . . . . .\n. . . . . . .\n\n. . . . .\n\n. . . . . . . .\n\n. . . . . .\n\n. . . . . . .\n\n. . . . . . .\n. . . . . .\n\n. . . . . .\n\n. . . . . . . . . .\n\n. . . . . . .\n. . . .\n\n. . .\n. . . .\n\n. . .\n. . .\n. . . .\n. . . .\n. . .\n\n. . . 197\n. . . 199\n. . . 202\n. 207\n. 209\n. 212\n. . . . 216\n. 223\n. 225\n. 230\n. . . . 231\n. . . 235\n. . 237\n. . . . . 242\n. . 242\n. . . 245\n. . . 248\n. . 251\n. . 254\n. . . 257\n. . . . 257\n. . . 258\n. . . . 259\n. . . 269\n. . . 280\n. 288\n. . . . . . 296\n. 297\n. 299\n. . 303\n. . . . 305\n. . 307\n. . 307\n. . 311\n. . 312\n. . . 321\n. . 324\n. . . . 331\n. . . . . . 331\n. . . . 354\n. . . . 356\n. . . . . . 357\n. . .\n. . . 363\n. . . . 363\n. . . . 371\n. . . 379\n. . . . 381\n. . 395\n. . 396\n\n. . . .\n. . . .\n. . . .\n. . . .\n\n. . . .\n\n. . . .\n\n. . . .\n\n. . .\n\n "}, {"Page_number": 523, "text": "list of examples\n\n511\n\n14.3\n14.4\n14.5\n14.6\n14.7\n14.8\n15.1\n15.2\n15.3\n15.4\n15.5\n15.6\n15.7\n15.8\n\n.\n.\n\n.\n.\n\n.\n.\n.\n.\n\n.\n.\nknee injuries .\n.\n.\n.\nknee injuries .\n.\n. . .\nrecovery scores\n.\n.\n.\n.\naids study .\n.\n.\n.\n.\n.\nbeta-blockers .\n.\n.\n.\n.\nknee injuries .\n.\n.\n.\ndrug use .\n.\n.\n.\n.\nglass identification .\n.\n.\ndrug use .\n.\n.\n.\nglass identification .\n.\ndlbcl data .\n.\n.\n.\nsatisfaction with life .\n.\nhousing data .\n.\n.\ndemand for medical care .\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n. . . . .\n\n. . . . . . .\n\n. . . . . . . .\n. . . . . . . .\n\n.\n.\n. . . . .\n.\n. . . .\n.\n. . . .\n. . . .\n. . . .\n. . . .\n. . . .\n. . . .\n.\n. . . . . .\n\n. . . . .\n. . . . . . . .\n. . . . .\n. . . . . .\n. . . . .\n. . . . . .\n. . . .\n. . . . . .\n\n. . . . . . . .\n\n. . . . .\n. . . . .\n\n. . . . . . . .\n. . . . . . . .\n\n. . . . . . . . . . .\n. . . . . . .\n\n. . . . . .\n\n. . . . . . . .\n\n. . . . . . . .\n. . . . .\n. . . . . . . .\n. . . . . . .\n. . . . . . . .\n. . . . . . .\n\n. 414\n. 417\n. . 417\n. . 421\n. . . . . . 425\n. 425\n. . . 429\n. 429\n. . . 442\n. 472\n. 473\n. . . . . . 478\n. 479\n. . . 480\n\n. . .\n. . . . .\n. . .\n. . . . .\n. .\n\n. . . . . . . . . . . .\n\n. . . . . . .\n\n. . . . .\n\n. . . . . . . .\n\n. . . . . .\n\n. . . . . . . .\n\n "}, {"Page_number": 524, "text": " "}, {"Page_number": 525, "text": "bibliography\n\nabe, m. (1999). a generalized additive model for discrete-choice data. journal of business\n& economic statistics 17, 271\u2013284.\nabramowitz, m. and i. stegun (1972). handbook of mathematical functions. new york:\ndover.\nagrawala, a. k. (1977). machine recognition of patterns. new york: ieee press.\nagresti, a. (1986). applying r2-type measures to ordered categorical data. technometrics 28,\n133\u2013138.\nagresti, a. (1992a). analysis of ordinal categorical data. new york: wiley.\nagresti, a. (1992b). a survey of exact inference for contingency tables. statistical science 7,\n131\u2013153.\nagresti, a. (1993a). computing conditional maximum-likehood-estimates for generalized\nrasch models using simple loglinear models with diagonals parameters. scandinavian jour-\nnal of statistics 20, 63\u201371.\nagresti, a. (1993b). computing conditional maximum likelihood estimates for generalized\nrasch models using simple loglinear models with diagonal parameters. scandinavian journal\nof statistics 20, 63\u201372.\nagresti, a. (1993c). distribution-free fitting of logit models with random effects for repeated\ncategorical responses. statistics in medicine 12, 1969\u20131988.\nagresti, a. (1993d). distribution-free fitting of logit-models with random effects for re-\npeated categorical responses. statistics in medicine 12, 1969\u20131987.\nagresti, a. (1997). a model for repeated measurements of a multivariate binary response.\njournal of the american statistical association 92, 315\u2013321.\nagresti, a. (2001). exact inference for categorical data: recent advances and continuing\ncontroversies. statistics in medicine 20(17-18), 2709\u20132722.\nagresti, a. (2002). categorical data analysis. new york: wiley.\nagresti, a. (2009). analysis of ordinal categorical data, 2nd edition. new york: wiley.\nagresti, a., b. caffo, and p. ohman-strickland (2004). examples in which misspecification\nof a random effects distribution reduces efficiency, and possible remedies. computational\nstatistics and data analysis 47, 639\u2013653.\nagresti, a. and a. kezouh (1983). association models for multi-dimensional cross-\nclassifications of ordinal variables. communications in statistics, part a \u2013 theory meth. 12,\n1261\u20131276.\nagresti, a. and j. lang (1993). a proportional odds model with subject-specific effects for\nrepeated ordered categorical responses. biometrika 80, 527.\naitkin, m. (1979). a simultaneous test procedure for contingency table models. journal of\napplied statistics 28, 233\u2013242.\n\n513\n\n "}, {"Page_number": 526, "text": "514\n\nbibliography\n\nieee transactions on\n\naitkin, m. (1980). a note on the selection of log-linear models. biometrics 36, 173\u2013178.\naitkin, m. (1999). a general maximum likelihood analysis of variance components in gener-\nalized linear models. biometrics 55, 117\u2013128.\nakaike, h. (1973). in b. petrov and f. caski (eds.), information theory and the extension of\nthe maximum likelihood principle, second international symposium on information theory.\nakademia kiado.\nakaike, h. (1974). a new look at statistical model identification.\nautomatic control 19, 716\u2013723.\nalbert, a. and j. a. anderson (1984). on the existence of maximum likelihood estimates in\nlogistic regression models. biometrika 71, 1\u201310.\nalbert, a. and e. lesaffre (1986). multiple group logistic discrimination. computers and\nmathematics with applications 12, 209\u2013224.\nalbert, j. h. and s. chib (2001). sequential ordinal modelling with applications to survival\ndata. biometrics 57, 829\u2013836.\namemiya, t. (1978). on two-step estimation of a multivariate logit model. journal of econo-\nmetrics 19, 13\u201321.\namemiya, t. (1981). qualitative response models: a survey. journal of economic litera-\nture xix, 1483\u20131536.\nananth, c. v. and d. g. kleinbaum (1997). regression models for ordinal responses: a\nreview of methods and applications. international journal of epidemiology 26, 1323\u20131333.\nanbari, m. e. and a. mkhadri (2008). penalized regression combining the l1 norm and a cor-\nrelation based penalty. technical report 6746, institut national de recherche en informatique\net en automatique.\nanderson, d. a. and m. aitkin (1985). variance component models with binary response:\ninterviewer variability. journal of the royal statistical society series b 47, 203\u2013210.\nanderson, d. a. and j. p. hinde (1988). random effects in generalized linear models and the\nem algorithm. communications in statistics a \u2013 theory and methods 17, 3847\u20133856.\nanderson, j. a. (1972). separate sample logistic discrimination. biometrika 59, 19\u201335.\nanderson, j. a. (1984). regression and ordered categorical variables. journal of the royal\nstatistical society b 46, 1\u201330.\nanderson, j. a. and v. blair (1982). penalized maximum likelihood estimation in logistic\nregression and discrimination. biometrika 69, 123\u2013136.\nanderson, j. a. and r. r. phillips (1981). regression, discrimination and measurement mod-\nels for ordered categorical variables. applied statistics 30, 22\u201331.\naranda-ordaz, f. j. (1983). an extension of the proportional-hazard-model for grouped data.\nbiometrics 39, 109\u2013118.\narmstrong, b. and m. sloan (1989). ordinal regression models for epidemiologic data. amer-\nican journal of epidemiology 129, 191\u2013204.\natkinson, a. and m. riani (2000). robust diagnostic regression analysis. new york:\nspringer-verlag.\navalos, m., y. grandvalet, and c. ambroise (2007). parsimonious additive models. compu-\ntational statistics & data analysis 51(6), 2851\u20132870.\nazzalini, a. (1994). logistic regression for autocorrelated data with application to repeated\nmeasures. biometrika 81, 767\u2013775.\n\n "}, {"Page_number": 527, "text": "bibliography\n\n515\n\nazzalini, a., a. w. bowman, and w. h\u00e4rdle (1989). on the use of nonparametric regression\nfor linear models. biometrika 76, 1\u201311.\nbarla, a., g. jurman, s. riccadonna, s. merler, m. chierici, and c. furlanello (2008). ma-\nchine learning methods for predictive proteomics. briefings in bioinformatics 9, 119\u2013128.\nbaumgarten, m., p. seliske, and m. s. goldberg (1989). warning re. the use of glim macros\nfor the estimation of risk ratio. american journal of epidemiology 130, 1065.\nbegg, c. and r. gray (1984). calculation of polytomous logistic regression parameters using\nindividualized regressions. biometrika 71, 11\u201318.\nbelitz, c. and s. lang (2008). simultaneous selection of variables and smoothing parameters\nin structured additive regression models. computational statistics and data analysis 51,\n6044\u20136059.\nbell, r. (1992). are ordinal models useful for classification? statistics in medicine 11(1),\n133\u2013134.\nbellman, r. (1961). adaptive control processes. princeton university press.\nben-akiva, m. e. and s. r. lerman (1985). discrete choice analysis: theory and application\nto travel demand. cambridge, ma: mit press.\nbender, r. and u. grouven (1998). using binary logistic regression models for ordinal data\nwith non\u2013proportional odds. journal of clinical epidemiology 51, 809\u2013816.\nbenedetti, j. k. and m. b. brown (1978). strategies for the selection of loglinear models.\nbiometrics 34, 680\u2013686.\nberger, r. l. and k. sidik (2003). exact unconditional tests for a 2\u00d72 matched pairs design.\nstatistical methods in medical research 12, 91\u2013108.\nbergsma, w., m. croon, and j. hagenaars (2009). marginal models. new york: springer\u2013\nverlag.\nberkson, j. (1994). application of the logistic function to bio-assay. journal of the american\nstatistical association 9, 357\u2013365.\nbesag, j., p. j. green, d. higdon, and k. mengersen (1995). bayesian computation and\nstochastic systems. statistical science 10, 3\u201366.\nbhapkar, v. p. (1966). a note on the equivalence of two test criteria for hypotheses in cate-\ngorical data. journal of the american statistical association 61, 228\u2013235.\nbinder, h. and g. tutz (2008). a comparison of methods for the fitting of generalized additive\nmodels. statistics and computing 18, 87\u201399.\nbishop, c. m. (2006). pattern recognition and machine learning. new york: springer\u2013\nverlag.\nbishop, y., s. fienberg, and p. holland (1975). discrete multivariate analysis. cambridge,\nma: mit press.\nbishop, y., s. fienberg, and p. holland (2007). discrete multivariate analysis. new york:\nspringer\u2013verlag.\nblanchard, g., o. bousquet, and p. massart (2008). statistical performance of support vector\nmachines. annals of statistics 36(2), 489\u2013531.\nbliss, c. i. (1934). the method of probits. science 79, 38\u201339.\nb\u00f6ckenholt, u. and w. r. dillon (1997). modelling within \u2013 subject dependencies in ordinal\npaired comparison data. psychometrika 62, 412\u2013434.\nbondell, h. d. and b. j. reich (2008). simultaneous regression shrinkage, variable selection\nand clustering of predictors with oscar. biometrics 64, 115\u2013123.\n\n "}, {"Page_number": 528, "text": "516\n\nbibliography\n\nbondell, h. d. and b. j. reich (2009). simultaneous factor selection and collapsing levels in\nanova. biometrics 65, 169\u2013177.\nbonney, g. e. (1987). logistic regression for dependent binary observations. biometrics 43,\n951\u2013973.\nbooth, j. g. and j. p. hobert (1999). maximizing generalized linear mixed model likelihoods\nwith an automated monte carlo em algorithm. journal of the royal statistical society b 61,\n265\u2013285.\nb\u00f6rsch-supan, a. (1987). econometric analysis of discrete choice, with applications on the\ndemand for housing in the u.s. and west-germany. berlin: springer-verlag.\nboulesteix, a., c. strobl, t. augustin, and m. daumer. evaluating microarray-based classi-\nfiers: an overview. cancer informatics 6, 77\u201397.\nboulesteix, a.-l. (2006). maximally selected chi-squared statistics for ordinal variables. bio-\nmetrical journal 48, 451\u2013462.\nboyles, r. a. (1983). on the covergence of the em algorithm. journal of the royal statistical\nsociety b 45, 47\u201350.\nbradley, r. a. (1976). science, statistics, and paired comparison. biometrics 32, 213\u2013232.\nbradley, r. a. (1984). paired comparisons: some basic procedures and examples. in p. kr-\nishnaiah and p. r. sen (eds.), handbook of statistics, volume 4, pp. 299\u2013326. elsevier.\nbradley, r. a. and m. e. terry (1952). rank analysis of incomplete block designs, i: the\nmethod of pair comparisons. biometrika 39, 324\u2013345.\nbraga-neto, u. and e. r. dougherty (2004).\nsamplemicroarray classification? bioinformatics 20, 374\u2013380.\nbrant, r. (1990). assessing proportionality in the proportional odds model for ordinal logistic\nregression. biometrics 46, 1171\u20131178.\nbreiman, l. (1995). better subset regression using the nonnegative garrotte. technometrics 37,\n373\u2013384.\nbreiman, l. (1996a). bagging predictors. machine learning 24, 123\u2013140.\nbreiman, l. (1996b). heuristics of instability and stabilisation in model selection. annals of\nstatistics 24, 2350\u20132383.\nbreiman, l. (1998). arcing classifiers. annals of statistics 26, 801\u2013849.\nbreiman, l. (1999). prediction games and arcing algorithms. neural computation 11, 1493\u2013\n1517.\nbreiman, l. (2001a). random forests. machine learning 45, 5\u201332.\nbreiman, l. (2001b). statistical modelling. the two cultures. statistical science 16, 199\u2013231.\nbreiman, l., j. h. friedman, r. a. olshen, and j. c. stone (1984). classification and regres-\nsion trees. monterey, ca: wadsworth.\nbreslow, n. e. (1984). extra-poisson variation in log-linear models. applied statistics 33,\n38\u201344.\nbreslow, n. e. and d. g. clayton (1993). approximate inference in generalized linear mixed\nmodel. journal of the american statistical association 88, 9\u201325.\nbreslow, n. e., k. halvorsen, r. prentice, and c. sabai (1978). estimation of multiple relative\nrisk functions in matched case-control studies. american journal of epidemiology 108, 299\u2013\n307.\nbreslow, n. e. and x. lin (1995). bias correction in generalized linear mixed models with a\nsingle component of dispersion. biometrika 82, 81\u201391.\n\nis cross-validation valid for small-\n\n "}, {"Page_number": 529, "text": "bibliography\n\n517\n\nbrezger, a. and s. lang (2006). generalized additive regression based on bayesian p-splines.\ncomputational statistics and data analysis 50, 967\u2013991.\nbrillinger, d. r. and m. k. preisler (1983). maximum likelihood estimation in a latent variable\nproblem. in t. amemiya, s. karlin, and t. goodman (eds.), studies in econometrics, time\nseries, and multivariate statistics, pp. 31\u201365. new york: academic press.\nbrown, c. c. (1982). on a goodness-of-fit test for the logistic models based on score statistics.\ncommunicattions in statistics: theory and methods 11, 1087\u20131105.\nbrown, m. b. (1976). screening effects in multidimensional contingency tables. journal of\napplied statistics 25, 37\u201346.\nbrownstone, d. and k. small (1989). efficient estimation of nested logit models. journal of\nbusiness & economic statistics 7, 67\u201374.\nb\u00fchlmann, p. (2006). boosting for high-dimensional linear models. annals of statistics 34,\n559\u2013583.\nb\u00fchlmann, p. and t. hothorn (2007). boosting algorithms: regularization, prediction and\nmodel fitting (with discussion). statistical science 22, 477\u2013505.\nb\u00fchlmann, p. and s. van de geer (2011). statistics for high-dimensional data: methods,\ntheory and applications. springer-verlag new york.\nb\u00fchlmann, p. and b. yu (2003). boosting with the l2 loss: regression and classification.\njournal of the american statistical association 98, 324\u2013339.\nbuja, a., t. hastie, and r. tibshirani (1989). linear smoothers and additive models. annals\nof statistics 17, 453\u2013510.\nbuja, a., w. stuetzle, and y. shen (2005). loss functions for binary class probability esti-\nmation and classification: structure and applications. manuscript, department of statistics,\nuniversity of pennsylvania, philadelphia.\nburnham, k. and d. anderson (2004). multimodel inference \u2013 understanding aic and bic\nin model selection. sociological methods & research 33, 261\u2013304.\nburnham, k. p. and d. r. anderson (2002). model selection and multimodel inference: a\npractical information\u2013theoretic approach. new york: springer\u2013verlag.\ncaffo, b., m.-w. an, and c. rhode (2007). flexible random intercept models for binary\noutcomes using mixtures of normals. computational statistics & data analysis 51, 5220\u2013\n5235.\ncameron, a. c. and p. k. trivedi (1998). regression analysis of count data. econometric\nsociety monographs no. 30. cambridge: cambridge university press.\ncampbell, g. (1980). shrunken estimators in discriminant and canonical variate analysis.\napplied statistics 29, 5\u201314.\ncampbell, g. (1994). advances in statistical methodology for the evaluation of diagnostic and\nlaboratory tests. statistics in medicine 13, 499\u2013508.\ncampbell, m. k. and a. p. donner (1989). classification efficiency of multinomial logistic-\nregression relative to ordinal logistic-regression. journal of the american statistical associa-\ntion 84(406), 587\u2013591.\ncampbell, m. k., a. p. donner, and k. m. webster (1991). are ordinal models useful for\nclassification? statistics in medicine 10, 383\u2013394.\ncandes, e. and t. tao (2007). the dantzig selector: statistical estimation when p is much\nlarger than n. annals of statistics 35(6), 2313\u20132351.\ncantoni, e., j. flemming, and e. ronchetti (2005). variable selection for marginal longitudi-\nnal generalized linear models. biometrics 61, 507\u2013514.\n\n "}, {"Page_number": 530, "text": "518\n\nbibliography\n\ncarroll, r., s. wang, and c. wang (1995). prospective analysis of logistic case-control\nstudies. journal of the american statistical association 90(429).\ncarroll, r. j., j. fan, i. gijbels, and m. p. wand (1997). generalized partially linear single-\nindex models. journal of the american statistical association 92, 477\u2013489.\ncarroll, r. j. and s. pederson (1993). on robustness in the logistic regression model. journal\nof the royal statistical society b 55, 693\u2013706.\ncarroll, r. j., d. ruppert, and a. h. welsh (1998). local estimating equations. journal of the\namerican statistical association 93, 214\u2013227.\nceleux, g. and j. diebolt (1985). the sem algorithm: a probabilistic teacher algorithm\nderived fom em algorithm for the mixture problem. computational. statistics 2, 73\u201382.\nchaganty, n. and h. joe (2004). efficiency of generalized estimation equations for binary\nresponses. journal of the royal statistical society b 66, 851\u2013860.\nchambers, j. m. and t. j. hastie (1992). statistical models in s. pacific grove, ca:\nwadsworth brooks/cole.\nchen, j. and m. davidian (2002). a monte carlo em algorithm for generalized linear models\nwith flexible random effects distribution. biostatistics 3, 347\u2013360.\nchen, s. s., d. l. donoho, and m. a. saunders (2001). atomic decomposition by basis\npursuit. siam review 43(1), 129\u2013159.\nchib, s. and b. carlin (1999). on mcmc sampling in hierarchical longitudinal models. statis-\ntics and computing 9, 17\u201326.\nchristensen, r. (1997). log-linear models and logistic regression. new york: springer-\nverlag.\nchristmann, a. and p. j. rousseeuw (2001). measuring overlap in binary regression. compu-\ntational statistics and data analysis 37, 65\u201375.\nchu, w. and s. keerthi (2005). new approaches to support vector ordinal regression.\nin\nproceedings of the 22nd international conference on machine learning, pp. 145\u2013152. acm.\nciampi, a., c.-h. chang, s. hogg, and s. mckinney (1987). recursive partitioning: a\nversatile method for exploratory data analysis in biostatistics. in i. mcneil and g. umphrey\n(eds.), biostatistics. new york: d. reidel publishing.\nclaeskens, g. and j. d. hart (2009). goodness-of-fit tests in mixed models. test 18, 213\u2013\n239.\nclaeskens, g. and n. hjort (2008). model selection and model averaging. cambridge uni-\nversity press.\nclaeskens, g., t. krivobokova, and j. d. opsomer (2009). asymptotic properties of penalized\nspline estimators. biometrika 96(3), 529\u2013544.\nclark, l. and d. pregibon (1992). tree-based models. in j. chambers and t. hastie (eds.),\nstatistical models in s, pp. 377\u2013420. pacific grove, california: wadsworth & brooks.\ncleveland, w. s. and c. loader (1996). smoothing by local regression: principles and meth-\nods. in w. h\u00e4rdle and m. schimek (eds.), statistical theory and computational aspects of\nsmoothing, pp. 10\u201349. heidelberg: physica-verlag.\ncochran, w. (1954). some methods for strengthening the common \u03c7 2 tests. biometrics 10,\n417\u2013451.\ncolonius, h. (1980). representation and uniquness of the bradley-terry-luce model for\npaired comparisons. british journal of mathematical & statistical psychology 33, 99\u2013103.\nconaway, m. r. (1989). analysis of repeated categorical measurements with conditional\nlikelihood methods. journal of the american statistical association 84, 53\u201362.\n\n "}, {"Page_number": 531, "text": "bibliography\n\n519\n\nconolly, m. a. and k. y. liang (1988). conditional logistic regression models for correlated\nbinary data. biometrika 75, 501\u2013506.\nconsul, p. c. (1998). generalized poisson distributions. new york: marcel dekker.\ncooil, b. and r. t. rust (1994). reliability and expected loss: a unifying principle. psy-\nchometrika 59, 203\u2013216.\ncook, r. d. (1977). detection of influential observations in linear regression. technomet-\nrics 19, 15\u201318.\ncook, r. d. and s. weisberg (1982). residuals and influence in regression. london: chap-\nman & hall.\ncopas, j. b. (1988). binary regression models for contaminated data (with discussion). jour-\nnal of the royal statistical society b 50, 225\u2013265.\ncordeiro, g. and p. mccullagh (1991). bias correction in generalized linear models. journal\nof the royal statistical society. series b (methodological) 53, 629\u2013643.\ncornell, r. g. and j. a. speckman (1967). estimation for a simple exponential model. bio-\nmetrics 23, 717\u2013737.\ncoste, j., e. walter, d. wasserman, and a. venot (1997). optimal discriminant analysis for\nordinal responses. statistics in medicine 16(5), 561\u2013569.\ncover, t. m. and p. e. hart (1967). nearest neighbor pattern classification. ieee transactions\non information theory 13, 21\u201327.\ncox, c. (1995). location-scale cumulative odds models for ordinal data: a generalized non-\nlinear model approach. statistics in medicine 14, 1191\u20131203.\ncox, d. (1958). two further applications of a model for binary regression. biometrika 45,\n562\u2013565.\ncox, d. r. and d. v. hinkley (1974). theoretical statistics. london: chapman & hall.\ncox, d. r. and n. reid (1987). approximations to noncentral distributions. canadian journal\nof statistics 15, 105\u2013114.\ncox, d. r. and e. j. snell (1989). analysis of binary data (second edition). london; new\nyork: chapman & hall.\ncox, d. w. j. and n. wermuth (1992). a comment on the coefficient of determination for\nbinary responses. american statistician 46, 1\u20134.\ncramer, j. s. (1991). the logit model. new york: routhedge, chapman & hall.\ncramer, j. s. (2003). the origins and development of the logit model. manuscript, university\nof amsterdam and tinbergen institute.\ncreel, m. d. and j. b. loomis (1990). theoretical and empirical advantages of truncated\ncount data estimators for analysis of deer hunting in california. journal of agricultural eco-\nnomics 72, 434\u2013441.\ncrowder, m. (1995). on the use of working correlation matrix in using generalized linear\nmodels for repeated measuresh. biometrika 82, 407\u2013410.\ncrowder, m. j. (1987). beta-binomial anova for proportions. journal of the royal statisti-\ncal society 27, 34\u201337.\ncurrie, i., m. durban, and h. p. eilers (2004). smoothing and forecasting mortality rates.\nstatistical modelling 4, 279\u2013298.\nczado, c. (1992). on link selection in generalized linear models. in s. l. n. in statistics\n(ed.), advances in glim and statistical modelling. new york: springer\u2013verlag. 78, 60\u201365.\n\n "}, {"Page_number": 532, "text": "520\n\nbibliography\n\nczado, c. (1997). on selecting parametric link transformation families in generalized linear\nmodels. journal of statistical planning and inference 61(1), 125\u2013139.\nczado, c., v. erhardt, a. min, and s. wagner (2007). zero-inflated generalized poisson\nmodels with regression effects on the mean. dispersion and zero-inflation level applied to\npatent outsourcing rates. statistical modelling 7(2), 125\u2013153.\nczado, c. and a. munk (2000). noncanonical links in generalized linear models \u2013 when is\nthe effort justified? journal of statistical planning and inference 87(2), 317\u2013345.\nczado, c. and t. santner (1992). the effect of link misspecification on binary regression\ninference. journal of statistical planning and inference 33(2), 213\u2013231.\ndahinden, c., g. parmigiani, m. c. emerick, and p. b\u00fchlmann (2007). penalized likelihood\nfor sparse contingency tables with application to full-length cdna libraries. bmc bioinfor-\nmatics 8, 476.\ndale, j. r. (1986). global cross-ratio models for bivariate, discrete, ordered responses. bio-\nmetrics 42, 909\u2013917.\ndarroch, j. n., s. l. lauritzen, and t. p. speed (1980). markov fields and log-linear interaction\nmodels for contingency tables. annals of statistics 8(3), 522\u2013539.\ndavidson, r. (1970). on extending the bradley-terry model to accommodate ties in paired\ncomparison experiments. journal of the american statistical association 65, 317\u2013328.\ndavis, c. s. (1991). semi-parametric and non-parametric methods for the analysis of repeated\nmeasurements with applications to clinical trials. statistics in medicine 10, 1959\u20131980.\nday, n. and d. kerridge (1967). a general maximum likelihood discriminant. biometrics 23,\n313\u2013323.\ndaye, z. and x. jeng (2009). shrinkage and model selection weighted fusion. computational\nstatistics and data analysis 53, 1284\u20131298.\nde boeck, p. and m. wilson (2004). explanatory item response models: a generalized linear\nand nonlinear approach. springer verlag.\nde bruijn, n. g. (1981). asymptotic methods in analysis. dover.\ndean, c., j. f. lawless, and g. e. willmot (1989). a mixed poisson-inverse gaussian regres-\nsion model. the canadian journal of statistics 17, 171\u2013181.\ndeb, p. and p. k. trivedi (1997). demand for medical care by the elderly: a finite mixture\napproach. journal of applied econometrics 12(3), 313\u2013336.\ndelyon, b., m. lavielle, and e. moulines (1999). convergence of a stochastic approximation\nversion of the em algorithm. annals of statistics 27, 94\u2013128.\ndeming, w. e. and f. f. stephan (1940). on a least squares adjustment of a sampled frequency\ntable when the expected marginal totals are known. annals of mathematical statistics 11,\n427\u2013444.\ndemoraes, a. r. and i. r. dunsmore (1995). predictive comparisons in ordinal models. com-\nmunications in statistics \u2013 theory and methods 24(8), 2145\u20132164.\ndempster, a. p., n. m. laird, and d. b. rubin (1977). maximum likelihood from incomplete\ndata via the em algorithm. journal of the royal statistical society b 39, 1\u201338.\ndeng, p. and s. r. paul (2005). score tests for zero-inflation and over-dispersion in generalized\nlinear models. statistica sinica 15(1), 257\u2013276.\ndenison, d. g. t., b. k. mallick, and a. f. m. smith (1998). automatic bayesian curve\nfitting. journal of the royal statistical society b 60, 333\u2013350.\ndettling, m. and p. b\u00fchlmann (2003). boosting for tumor classification with gene expression\ndata. bioinformatics 19, 1061\u20131069.\n\n "}, {"Page_number": 533, "text": "bibliography\n\n521\n\ndey, d. k., s. k. ghosh, and b. k. mallick (2000). generalized linear models: a bayesian\nperspective. new york: marcel dekker.\ndiaz-uriarte, r. and s. de andres (2006a). gene selection and classification of microarray\ndata using random forest. bmc bioinformatics 7, 3.\ndiaz-uriarte, r. and s. a. de andres (2006b). gene selection and classification of microarray\ndata using random forest. bioinformatics 7, 3.\ndierckx, p. (1993). curve and surface fitting with splines. oxford: oxford science publica-\ntions.\ndietterich, t. (2000). an experimental comparison of three methods for constructing en-\nsembles of decision trees: bagging boosting and randomization. machine learning 40(2),\n139\u2013157.\ndiggle, p. j., p. j. heagerty, k. y. liang, and s. l. zeger (2002). analysis of longitudinal\ndata (second edition). london: chapman & hall.\ndillon, w. r., a. kumar, and m. de borrero (1993). capturing individual differences in\npaired comparisons: an extended btl model incorporating descriptor variables. journal of\nmarketing research 30, 42\u201351.\ndittrich, r., r. hatzinger, and w. katzenbeisser (1998). modelling the effect of subject-\nspecific covariates in paired comparison studies with an application to university rankings.\napplied statistics 47, 511\u2013525.\ndobson, a. j. (1989). introduction to statistical modelling. london: chapman & hall.\ndodd, l. and m. pepe (2003). partial auc estimation and regression. biometrics 59(3),\n614\u2013623.\ndomeniconi, c., j. peng, and d. gunopulos (2002). locally adaptive metric nearest-neighbor\nclassification.\nieee transactions on pattern analysis and machine intelligence 24, 1281\u2013\n1285.\ndomeniconi, c. and b. yan (2004). nearest neighbor ensemble. in proc. of the 17th interna-\ntional conference on pattern recognition, volume 1, pp. 228\u2013231.\nduchon, j. (1977). splines minimizing rotation-invariant semi-norms in solobev spaces. in\nw. schemp and k. zeller (eds.), construction theory of functions of several variables, pp.\n85\u2013100. berlin: springer-verlag.\ndudoit, s., j. fridlyand, and t. p. speed (2002). comparison of discrimination methods for\nthe classification of tumors using gene expression data. journal of the american statistical\nassociation 97, 77\u201387.\nduffy, d. e. and t. j. santner (1989). on the small sample properties of restricted maximum\nlikelihood estimators for logistic regression models. communication in statistics \u2013 theory &\nmethods 18, 959\u2013989.\nedwards, d. and t. havranek (1987). a fast model selection procedure for large family of\nmodels. journal of the american statistical association 82, 205\u2013213.\nefron, b. (1975). the efficiency of logistic regression compared to normal discriminant anal-\nysis. journal of the american statistical association 70, 892\u2013898.\nefron, b. (1978). regression and anova with zero\u2013one data: measures of residual variation.\njournal of the american statistical association 73, 113\u2013121.\nefron, b. (1983). estimating the error rate of a prediction rule:\nvalidation. journal of the american statistical association 78(382), 316\u2013331.\nefron, b. (1986). double exponential families and their use in generalized linear regression.\njournal of the american statistical association 81, 709\u2013721.\n\nimprovement on cross-\n\n "}, {"Page_number": 534, "text": "522\n\nbibliography\n\nefron, b. (2004). the estimation of prediction error: covariance penalties and cross-\nvalidation. journal of the american statistical association 99, 619\u2013632.\nefron, b., t. hastie, i. johnstone, and r. tibshirani (2004). least angle regression. annals of\nstatistics 32, 407\u2013499.\nefron, b. and r. tibshirani (1997). improvements on cross-validation: the .632+ bootstrap\nmethod. journal of the american statistical association 92, 548\u201360.\neilers, p. h. c. and b. d. marx (2003). multivariate calibration with temperature interaction\nusing two-dimensional penalized signal regression. chemometrics and intelligent laboratory\nsystems 66, 159\u2013174.\neveritt, b. and t. hothorn (2006). a handbook of statistical analyses using r. new york:\nchapman & hall.\nfahrmeir, l. (1987). asymptotic likelihood inference for nonhomogeneous observations.\nstatistische hefte (n.f.) 28, 81\u2013116.\nfahrmeir, l. (1994). dynamic modelling and penalized likelihood estimation for discrete time\nsurvival data. biometrika 81(2), 317.\nfahrmeir, l. and h. frost (1992). on stepwise variable selection in generalized linear regres-\nsion and time series models. computational statistics 7, 137\u2013154.\nfahrmeir, l. and a. hamerle (1984). multivariate statistische verfahren. berlin / new york:\nde gruyter.\nfahrmeir, l. and h. kaufmann (1985). consistency and asymptotic normality of the maximum\nlikelihood estimator in generalized linear models. annals of statistics 13, 342\u2013368.\nfahrmeir, l. and h. kaufmann (1987). regression model for nonstationary categorical time\nseries. journal of time series analysis 8, 147\u2013160.\nfahrmeir, l. and t. kneib (2009). bayesian regularisation in structured additive regression:\na unifying perspective on shrinkage, smoothing and predictor selection. statistics and com-\nputing 2, 203\u2013219.\nfahrmeir, l. and t. kneib (2010). bayesian smoothing and regression for longitudinal,\nspatial and event history data. oxford: clarendon press.\nfahrmeir, l., t. kneib, and s. lang (2004). penalized structured additive regression for space-\ntime data: a bayesian perspective . statistica sinica 14, 715\u2013745.\nfahrmeir, l., t. kneib, s. lang, and b. marx (2011). regression. models, methods and\napplications. berlin: springer verlag.\nfahrmeir, l. and s. lang (2001). bayesian inference for generalized additive mixed models\nbased on markov random field priors. applied statistics 50(2), 201\u2013220.\nfahrmeir, l. and l. pritscher (1996). regression analysis of forest damage by marginal models\nfor correlated ordinal responses. journal of environmental and ecological statistics 3, 257\u2013\n268.\nfahrmeir, l. and g. tutz (2001). multivariate statistical modelling based on generalized\nlinear models. new york: springer.\nfamoye, f. and k. p. singh (2003). on inflated generalized poisson regression models. ad-\nvanced applied statistics 3(2), 145\u2013158.\nfamoye, f. and k. p. singh (2006). zero-inflated generalized poisson model with an applica-\ntion to domestic violence data. journal of data science 4(1), 117\u2013130.\nfan, j. and i. gijbels (1996). local polynomial modelling and its applications. london:\nchapman & hall.\n\n "}, {"Page_number": 535, "text": "bibliography\n\n523\n\nfan, j. and r. li (2001). variable selection via nonconcave penalize likelihood and its oracle\nproperties. journal of the american statistical association 96, 1348\u20131360.\nfan, j. and w. zhang (1999). statistical estimation in varying coefficient models. annals of\nstatistics 27(5), 1491\u20131518.\nfaraway, j. (2006). extending the linear model with r. london: chapman & hall.\nfienberg, s. e. (1980). the analysis of cross-classified categorical data. cambridge: mit\npress.\nfinney, d. (1947). the estimation from individual records of the relationship between dose\nand quantal response. biometrika 34, 320\u2013334.\nfirth, d. (1987). on the efficiency of quasi-likelihood estimation. biometrika 74, 233\u2013245.\nfirth, d. (1991). generalized linear models. in d. v. hinkley, n. reid, and e. j. snell (eds.),\nstatistical theory and modelling. london: chapman & hall.\nfirth, d. (1993). bias reduction of maximum likelihood estimates. biometrika 80(1), 27\u201338.\nfirth, d. and r. de menezes (2004). quasi-variances. biometrika 91, 65.\nfitzmaurice, g. m. (1995). a caveat concerning independence estimating equations with\nmultivariate binary data. biometrics 51, 309\u2013317.\nfitzmaurice, g. m. and n. m. laird (1993). a likelihood-based method for analysing longi-\ntudinal binary responses. biometrika 80, 141\u2013151.\nfitzmaurice, g. m., n. m. laird, and j. h.ware (2004). applied longitudinal analysis. new\nyork: wiley.\nfix, e. and j. l. hodges (1951). discriminatory analysis-nonparametric discrimination: con-\nsistency properties. us air force school of aviation medicine, randolph field, texas.\nfleiss, j., b. levin, and c. paik (2003). statistical methods for rates and proportions. new\nyork: wiley.\nflury, b. (1986). proportionality of k covariance matrices. statistics and probability letters 4,\n29\u201333.\nfolks, j. l. and r. s. chhikara (1978). the inverse gaussian distribution and its statistical\napplication, a review (with discussion). journal of the royal statistical society b 40, 263\u2013289.\nfollmann, d. and d. lambert (1989). generalizing logistic regression by non-parametric\nmixing. journal of the american statistical association 84, 295\u2013300.\nfowlkes, e. b. (1987). some diagnosties for binary logistic regression via smoothing.\nbiometrika 74, 503\u2013515.\nfrank, e. and m. hall (2001). a simple approach to ordinal classification. machine learning:\necml 2001, 145\u2013156.\nfrank, i. e. and j. h. friedman (1993). a statistical view of some chemometrics regression\ntools (with discussion). technometrics 35, 109\u2013148.\nfreund, y. and r. e. schapire (1997). a decision-theoretic generalization of on-line learning\nand an application to boosting. journal of computer and system sciences 55, 119\u2013139.\nfr\u00fchwirth-schnatter, s. (2006). finite mixture and markov switching models. new york:\nspringer\u2013verlag.\nfriedman, j., t. hastie, and r. tibshirani (2008). glmnet: lasso and elastic-net regularized\ngeneralized linear models. r package version 1.1.\nfriedman, j. h. (1989). regularized discriminant analysis. journal of the american statistical\nassociation 84, 165\u2013175.\n\n "}, {"Page_number": 536, "text": "524\n\nbibliography\n\nfriedman, j. h. (1991). multivariate adaptive regression splines (with discussion). annals of\nstatistics 19, 1\u201367.\nfriedman, j. h. (1994). flexible metric nearest neighbor classification. technical report 113,\nstanford university, statistics department.\nfriedman, j. h. (2001). greedy function approximation: a gradient boosting machine. annals\nof statistics 29, 1189\u20131232.\nfriedman, j. h., t. hastie, h. h\u00f6fling, and t. tibshirani (2007). pathwise coordinate opti-\nmization. applied statistics 1(2), 302\u2013332.\nfriedman, j. h., t. hastie, and r. tibshirani (2000). additive logistic regression: a statistical\nview of boosting. annals of statistics 28, 337\u2013407.\nfriedman, j. h., t. hastie, and r. tibshirani (2010). regularization paths for generalized\nlinear models via coordinate descent. journal of statistical software 33(1), 1\u201322.\nfriedman, j. h. and w. st\u00fctzle (1981). projection pursuit regression. journal of the american\nstatistical association 76, 817\u2013823.\nfu, w. j. (1998). penalized regression: the bridge versus the lasso. journal of computational\nand graphical statistics 7, 397\u2013416.\nfu, w. j. (2003). penalized estimation equations. biometrics 59, 126\u2013132.\nfukunaga, k. (1990). introduction to statistical pattern recognition. san diego, california:\nacademic press.\nfurnival, g. m. and r. w. wilson (1974). regression by leaps and bounds. technometrics 16,\n499\u2013511.\ngamerman, d. (1997). efficient sampling from the posterior distribution in generalized linear\nmixed models. statistics and computing 7, 57\u201368.\ngay, d. m. and r. e. welsch (1988). maximum likelihood and quasi-likelihood for nonlinear\nexponential family regression models. journal of the american statistical association 83,\n990\u2013998.\ngenkin, a., d. lewis, and d. madigan (2004). large-scale bayesian logistic regression for\ntext categorization. technical report, rutgers university.\ngenter, f. c. and v. t. farewell (1985). goodness-of-link testing in ordinal regression models.\ncanadian journal of statistics 13, 37\u201344.\ngertheiss, j. (2011). feature extraction in regression and classification with structured\npredictors. cuvillier verlag.\ngertheiss, j., s. hogger, c. oberhauser, and g. tutz (2011). selection of ordinally scaled\nindependent variables with applications to international classification of functioning core sets.\njournal of the royal statistical society: series c, 377\u2013396.\ngertheiss, j. and g. tutz (2009a). feature selection and weighting by nearest neighbor\nensembles. chemometrics and intelligent laboratory systems 99, 30\u201338.\ngertheiss, j. and g. tutz (2009b). penalized regression with ordinal predictors. international\nstatistical review 77, 345\u2013365.\ngertheiss, j. and g. tutz (2009c). supervised feature selection in mass spectrometry based\nproteomic profiling by blockwise boosting. bioinformatics 8, 1076\u20131077.\ngertheiss, j. and g. tutz (2010). sparse modeling of categorial explanatory variables. annals\nof applied statistics 4, 2150\u20132180.\ngertheiss, j. and g. tutz (2011). regularization and model selection with categorial effect\nmodifiers. statistica sinica (to appear).\n\n "}, {"Page_number": 537, "text": "bibliography\n\n525\n\nthe analysis of crossclassified data:\n\ngijbels, i. and a. verhaselt (2010). p-splines regression smoothing and difference type of\npenalty. statistics and computing 4, 499\u2013511.\nglonek, g. f. v. and p. mccullagh (1995). multivariate logistic models. journal of the royal\nstatistical society 57, 533\u2013546.\nglonek, g. v. f. (1996). a class of regression models for multivariate categorical responses.\nbiometrika 83, 15\u201328.\ngneiting, t. and a. raftery (2007). strictly proper scoring rules, prediction, and estimation.\njournal of the american statistical association 102(477), 359\u2013376.\ngoeman, j. and s. le cessie (2006). a goodness-of-fit test for multinomial logistic regression.\nbiometrics 62, 980\u2013985.\ngoeman, j. j. (2010). l1 penalized estimation in the cox proportional hazards model. bio-\nmetrical journal 52, 70\u201384.\ngolub, t., d. slonim, p. tamayo, c. huard, m. gaasenbeek, j. mesirov, h. coller, m. loh,\nj. downing, m. caligiuri, c. bloomfield, and e. lander (1999a). molecular classifica-\ntion of cancer: class discovery and class prediction by gene expression monitoring. sci-\nence 286(5439), 531\u2013537.\ngolub, t. r., d. k. slonim, p. tamayo, c. huard, m. gaasenbeek, j. p. mesirov, h. coller,\nm. l. loh, j. r. downing, m. a. caligiuri, c. d. bloomfield, and e. s. lander (1999b).\nmolecular classification of cancer: class discovery and class prediction by gene expression\nmonitoring. science 286, 531\u2013537.\ngoodman, l. a. (1968).\nindependence, quasi-\nindependence and interaction in contingency tables with or without missing cells. journal\nof the american statistical association 63, 1091\u20131131.\ngoodman, l. a. (1971). the analysis of multidimensional contingency tables: stepwise\nprocedures and direct estimation methods for building models for multiple classifications.\ntechnometrics 13, 33\u201361.\ngoodman, l. a. (1979). simple models for the analysis of association in cross-classification\nhaving ordered categories. journal of the american statistical society 74, 537\u2013552.\ngoodman, l. a. (1981a). association models and canonical correlation in the analysis of\ncross-classification having ordered categories. journal of the american statistical associa-\ntion 76, 320\u2013334.\ngoodman, l. a. (1981b). association models and the bivariate normal for contingency tables\nwith ordered categories. biometrika 68, 347\u2013355.\ngoodman, l. a. (1983). the analysis of dependence in cross-classification having or-\ndered categories, using log-linear models for frequencies and log-linear models for odds.\nbiometrika 39, 149\u2013160.\ngoodman, l. a. (1985). the analysis of cross-classified data having ordered and/or ordered\ncategories. annals of statistics 13(1), 10\u201369.\ngoodman, l. a. and w. h. kruskal (1954). measures of associaton for cross classifications.\njournal of the american statistical association 49, 732\u2013764.\ngourieroux, c., a. monfort, and a. trognon (1984). pseudo maximum likelihood methods:\ntheory. econometrica 52, 681\u2013700.\ngreen, d. j. and b. w. silverman (1994). nonparametric regression and generalized linear\nmodels: a roughness penalty approach. london: chapman & hall.\ngreen, p. j. (1987). penalized likelihood for general semi-parametric regression models. in-\nternational statistical review 55, 245\u2013259.\n\n "}, {"Page_number": 538, "text": "526\n\nbibliography\n\nstatistics in\n\ngreene, w. (2003). econometric analysis. new jersey: prentice hall.\ngreenland, s. (1994). alternative models for ordinal logistic regression.\nmedicine 13, 1665\u20131677.\ngrizzle, j. e., c. f. starmer, and g. g. koch (1969). analysis of categorical data by linear\nmodels. biometrika 28, 137\u2013156.\ngr\u00fcn, b. and f. leisch (2007). fitting finite mixtures of generalized linear regressions in r.\ncomputational statistics & data analysis 51(11), 5247\u20135252.\ngr\u00fcn, b. and f. leisch (2008). identifiability of finite mixtures of multinomial logit models\nwith varying and fixed effects. journal of classification 25(2), 225\u2013247.\ngroll, a. and g. tutz (2011a). regularization for generalized additive mixed models by\nlikelihood-based boosting. technical report 110, lmu, department of statistics.\ngroll, a. and g. tutz (2011b). variable selection for generalized linear mixed models by\nl1-penalized estimation. technical report 108, lmu, department of statistics.\ngschoessl, s. and c. czado (2006). modelling count data with overdispersion and spatial\neffects. statistical papers 49(3), 531\u2013552.\ngu, c. (2002). smoothing splines anova models. new york: springer\u2013verlag.\ngu, c. and g. wahba (1991). minimizing gcv/gml scores with multiple smoothing pa-\nrameters via the newton method. siam journal on scientific and statistical computing 12(2),\n383\u2013398.\ngu, c. and g. wahba (1993). semiparametric analysis of variance with tensor product thin\nplate splines. journal of the royal statistical society series b \u2013 methodological 55(2), 353\u2013\n368.\nguess, h. a. and k. s. crump (1978). maximum likelihood estimation of dose-response\nfunctions subject to absolutely monotonic constraints. annals of statistics 6, 101\u2013111.\nguo, y., t. hastie, and r. tibshirani (2007). regularized linear discriminant analysis and its\napplication in microarrays. biostatistics 8(1), 86.\ngupta, p. l., r. c. gupta, and r. c. tripathi (2004). score test for zero inflated generalized\npoisson regression model. communications in statistics \u2013 theory and methods 33(1), 47\u201364.\nhaberman, s. j. (1974). loglinear models for frequency tables with ordered classifications.\nbiometrics 30, 589\u2013600.\nhaberman, s. j. (1977). maximum likelihood estimates in exponential response models. an-\nnals of statistics 5, 815\u2013841.\nhaberman, s. j. (1982). analysis of dispersion of multinomial responses. journal of the\namerican statistical association 77, 568\u2013580.\nhamada, m. and c. f. j. wu (1996). a critical look at accumulation analysis and related\nmethods. technometrics 32, 119\u2013130.\nhamerle, a. k. p. and g. tutz (1980). kategoriale reaktionen in multifaktoriellen versuch-\nspl\u00e4nen und mehrdimensionale zusammenhangsanalysen. archiv f\u00fcr psychologie, 53\u201368.\nhans, c. (2009). bayesian lasso regression. biometrika 96(1), 835\u2013845.\nh\u00e4rdle, w., p. hall, and h. ichimura (1993). optimal smoothing in single-index models.\nannals of statistics 21, 157\u2013178.\nharrell, f. (2001). regression modeling strategies. new york: springer\u2013verlag.\nhartzel, j., a. agresti, and b. caffo (2001). multinomial logit random effects models. statis-\ntical modelling 1, 81\u2013102.\n\n "}, {"Page_number": 539, "text": "bibliography\n\n527\n\nhartzel, j., i. liu, and a. agresti (2001). describing heterogenous effects in stratified ordinal\ncontingency tables, with applications to multi-center clinical trials. computational statistics\n& data analysis 35(4), 429\u2013449.\nharville, d. (1997). matrix algebra from a statistician\u2019s perspective. new york: springer\u2013\nverlag.\nharville, d. a. (1976). extension of the gauss-markov theorem to include the estimation of\nrandom effects. annals of statistics 4, 384\u2013395.\nharville, d. a. (1977). maximum likelihood approaches to variance component estimation\nand to related problems. journal of the american statistical association 72, 320\u2013338.\nharville, d. a. and r. w. mee (1984). a mixed-model procedure for analyzing ordered\ncategorical data. biometrics 40, 393\u2013408.\nhastie, t. (1996). pseudosplines. jrss, series b 58, 379\u2013396.\nhastie, t. and c. loader (1993). local regression: automatic kernel carpentry. statistical\nscience 8, 120\u2013143.\nhastie, t. and r. tibshirani (1986). generalized additive models (c/r: p. 310\u2013318). statist.\nsci. 1, 297\u2013310.\nhastie, t. and r. tibshirani (1990). generalized additive models. london: chapman & hall.\nhastie, t. and r. tibshirani (1993). varying-coefficient models. journal of the royal statis-\ntical society b 55, 757\u2013796.\nhastie, t. and r. tibshirani (1996). discriminant adaptive nearest-neighbor classification.\nieee transactions on pattern analysis and machine intelligence 18, 607\u2013616.\nhastie, t., r. tibshirani, and j. h. friedman (2001). the elements of statistical learning.\nnew york: springer-verlag.\nhastie, t., r. tibshirani, and j. h. friedman (2009). the elements of statistical learning\n(second edition). new york: springer-verlag.\nhausman, j. a. and d. mcfadden (1984). specification tests for the multinomial logit model.\neconometrika 52, 1219\u20131240.\nhausman, j. a. and d. a. wise (1978). a conditional probit model for qualitative choice: dis-\ncrete decisions recognizing interdependence and heterogeneous preference. econometrica 46,\n403\u2013426.\nhe, x. and p. ng (1999). cobs: qualitatively constrained smoothing via linear programming.\ncomputational statistics 14, 315\u2013337.\nheagerty, p. and b. f. kurland (2001). misspecified maximum likelihood estimates and gen-\neralised linear mixed models. biometrika 88, 973\u2013985.\nheagerty, p. and s. zeger (2000). marginalized multilevel models and likelihood inference.\nstatistical science 15(1), 1\u201319.\nheagerty, p. j. (1999). marginally specified logistic-normal models for longitudinal binary\ndata. biometrics 55, 688\u2013698.\nheagerty, p. j. and s. zeger (1998). lorelogram: a regression approach to exploring de-\npendence in longitudinal categorical responses. journal of the american statistical associa-\ntion 93(441), 150\u2013162.\nheagerty, p. j. and s. l. zeger (1996). marginal regression models for clustered ordinal\nmeasurements. journal of the american statistical association 91, 1024\u20131036.\nhedeker, d. and r. b. gibbons (1994). a random-effects ordinal regression model for multi-\nlevel analysis. biometrics 50, 933\u2013944.\n\n "}, {"Page_number": 540, "text": "528\n\nbibliography\n\nheim, a. (1970). intelligence and personality. harmondsworth: penguin.\nherbrich, r., t. graepel, and k. obermayer (1999). large margin rank boundaries for ordinal\nregression. advances in neural information processing systems, 115\u2013132.\nheyde, c. c. (1997). quasi-likelihood and its applications. new york: springer\u2013verlag.\nhilbe, j. (2011). negative binomial regression. cambridge university press.\nhinde, j. (1982). compound poisson regression models. in r. gilchrist (ed.), glim 1982\ninternational conference on generalized linear models, pp. 109\u2013121. new york: springer-\nverlag.\nhinde, j. and c. d\u00e9metrio (1998). overdispersion: models and estimation. computational\nstatistics & data analysis 27, 151\u2013170.\nho, t. k. (1998). the random subspace method for constructing decision forests. ieee trans.\non pattern analysis and machine intelligence 20, 832\u2013844.\nhoaglin, d. and r. welsch (1978). the hat matrix in regression and anova. american\nstatistician 32, 17\u201322.\nhoefsloot, h. c. j., s. smit, and a. k. smilde (2008). a classification model for the leiden\nproteomics competition. statistical applications in genetics and molecular biology 7, article\n8.\nhoerl, a. e. and r. w. kennard (1970). ridge regression: bias estimation for nonorthogonal\nproblems. technometrics 12, 55\u201367.\nholtbr\u00fcgge, w. and m. schuhmacher (1991). a comparison of regression models for the\nanalysis of ordered categorical data. applied statistics 40, 249\u2013259.\nhorowitz, j. and w. h\u00e4rdle (1996). direct semiparametric estimation of sngle-index models\nwith discrete covariates. journal of the american statistical association 91, 1623\u20139.\nhosmer, d. h. and s. lemeshow (1980). goodness-of-fit tests for the multiple logistic regres-\nsion model. communications in statistics \u2013 theory & methods 9, 1043\u20131069.\nhosmer, d. h. and s. lemeshow (1989). applied logistic regression. new york: wiley.\nhothorn, t., p. b\u00fchlmann, t. kneib, m. schmid, and b. hofner (2009). mboost: model-based\nboosting. r package version 2.0-0.\nhothorn, t., k. hornik, and a. zeileis (2006). unbiased recursive partitioning: a conditional\ninference framework. journal of computational and graphical statistics 15, 651\u2013674.\nhothorn, t. and b. lausen (2003). on the exact distribution of maximally selected rank\nstatistics. computational statistics and data analysis 43, 121\u2013137.\nhristache, m., a. juditsky, and v. spokoiny (2001). direct estimation of the index coefficient\nin a single-index model. annals of statistics 29, 595\u2013623.\nhsiao, c. (1986). analysis of panel data. cambridge: cambridge university press.\nhuang, x. (2009). diagnosis of random-effect model misspecification in generalized linear\nmixed models for binary response. biometrics 65, 361\u2013368.\nhuang, x., w. pan, s. grindle, x. han, y. chen, s. j. park, i. w. miller, and j. hall (2005).\na comparative study of discriminating human heart failure etiology using gene expression\nprofiles. bioinformatics 6, 205.\nhurvich, c. m. and c.-l. tsai (1989). regression and time series model selection in small\nsamples. bma 76, 297\u2013307.\nibrahim, j., h. zhu, r. garcia, and r. guo (2011). fixed and random effects selection in\nmixed effects models. biometrics 67, 495\u2013503.\n\n "}, {"Page_number": 541, "text": "bibliography\n\n529\n\nim, s. and d. gianola (1988). mixed models for bionomial data with an application to lamb\nmortality. applied statistics 37, 196\u2013204.\njames, g. (2002). generalized linear models with functional predictors. journal of the royal\nstatistical society b 64, 411\u2013432.\njames, g. m. and p. radchenko (2008). a generalized dantzig selector with shrinkage tuning.\nbiometrika , 127\u2013142.\njank, w. (2004). quasi-monte carlo sampling to improve the efficiency of monte carlo em.\ncomputational statistics & data analysis 48, 685\u2013701.\njansen, j. (1990). on the statistical analysis of ordinal data when extravariation is present.\napplied statistics 39, 74\u201385.\njensen, k., h. m\u00fcller, and h. sch\u00e4fer (2000). regional confidence bands for roc curves.\nstatistics in medicine 19(4), 493\u2013509.\njoe, h. (1989). relative entropy measures of multivariate dependence. journal of the ameri-\ncan statistical association 84, 157\u2013164.\njoe, h. and r. zhu (2005). generalized poisson distribution:\nthe property of mixture of\npoisson and comparison with negative binomial distribution. biometrical journal 47(2), 219\u2013\n229.\njones, r. h. (1993). longitudinal data with serial correlation: a state-space approach.\nlondon: chapman & hall.\njorgenson, b. (1987). exponential dispersion models. j. roy. stat. soc. ser. b 49, 127\u2013162.\nkarimi, a., a. windorfer, and j. dreesman (1998). vorkommen von zentralnerv\u00f6sen infek-\ntionen in europ\u00e4ischen l\u00e4ndern. technical report, schriften des nieders\u00e4chsischen landes-\ngesundheitsamtes.\nkaslow, r. a., d. g. ostrow, r. detels, j. p. phair, b. f. polk, and c. r. rinaldo (1987).\nthe multicenter aids cohort study: rationale, organization and selected characteristic of the\nparticipiants. american journal of epidemiology 126, 310\u2013318.\nkauermann, g. (2000). modelling longitudinal data with ordinal response by varying coeffi-\ncients. biometrics 56, 692\u2013698.\nkauermann, g., t. krivobokova, and l. fahrmeir (2009). some asymptotic results on general-\nized penalized spline smoothing. journal of the royal statistical society series b \u2013 statistical\nmethodology 71(part 2), 487\u2013503.\nkauermann, g. and j. opsomer (2004). generalized cross-validation for bandwidth selection\nof backfitting estimates in generalized additive models. journal of computational comuta-\ntional and graphical statistics 13(1), 66\u201389.\nkauermann, g. and g. tutz (2001). testing generalized and semiparametric models against\nsmooth alternatives. journal of the royal statistical society b 63, 147\u2013166.\nkauermann, g. and g. tutz (2003). semi- and nonparametric modeling of ordinal data. jour-\nnal of computational and graphical statistics 12, 176\u2013196.\nkaufmann, h. (1987). regression models for nonstationary categorical time series: asymp-\ntotic estimation theory. annals of statistics 15, 79\u201398.\nkedem, b. and k. fokianos (2002). regression models for time series analysis. new york:\nwiley.\nkeenan, s. c. and j. r. sobehart (1999). performance measures for credit risk models. re-\nsearch report 13, moody\u2018s risk management services.\nkhalili, a. and j. chen (2007). variable selection in finite mixture of regression models.\njournal of the american statistical association 102(479), 1025\u20131038.\n\n "}, {"Page_number": 542, "text": "530\n\nbibliography\n\nkinney, s. k. and d. b. dunson (2007). fixed and random effects selection in linear and\nlogistic models. biometrics 63, 690\u2013698.\nkleiber, c. and a. zeileis (2008). applied econometrics with r. new york: springer\u2013verlag.\nklein, r. l. and r. h. spady (1993). an efficient semiparametric estimator for binary response\nmodels. econometrica 61, 387\u2013421.\nkneib, t. and l. fahrmeir (2006). structured additive regression for categorical space-time\ndata: a mixed model approach. biometrics 62, 109\u2013118.\nkneib, t. and l. fahrmeir (2008). a space-time study on forest health. in r. chandler and\nm. scott (eds.), statistical methods for trend detection and analysis in the environmental\nsciences. new york: wiley.\nkneib, t., t. hothorn, and g. tutz (2009). variable selection and model choice in geoadditive\nregression models. biometrics 65, 626\u2013634.\nkockelkorn, u. (2000). lineare modelle. oldenbourg verlag.\nkoenker, r., p. ng, and s. portnoy (1994). quantile smoothing splines. biometrika 81, 673\u2013\n680.\nkohavi, r. and g. h. john (1998). the wrapper approach.\nin h. liu and h. motoda\n(eds.), feature extraction, construction and selection. a data mining perspective. dor-\ndrecht: kluwer.\nkrantz, d. h. (1964). conjoint measurement: the luce-tukey axiomatization and some\nextentions. journal of mathematical psychology 1, 248\u2013277.\nkrantz, d. h., r. d. luce, p. suppes, and a. tversky (1971). foundations of measurement,\nvolume 1. new york: academic press.\nkrishnapuram, b., l. carin, m. a. figueiredo, and a. j. hartemink (2005). sparse multino-\nmial logistic regression: fast algorithms and generalization bounds. ieee transactions on\npattern analysis and machine intelligence 27, 957\u2013968.\nkrivobokova, t., c. crainiceanu, and g. kauermann (2008). fast adaptive penalized splines.\njournal of computational and graphical statistics 17, 1\u201320.\nk\u00fcchenhoff, h. and k. ulm (1997). comparison of statistical methods for assessing threshold\nlimiting values in occupational epidemiology. computational statistics 12, 249\u2013264.\nk\u00fcnsch, h. r., l. a. stefanski, and r. j. carroll (1989). conditionally unbiased bounded-\ninfluence estimation in general regression models, with applications to generalized linear mod-\nels. journal of the american statistical association 84, 460\u2013466.\nkuss, o. (2002). global goodness-of-fit tests in logistic regression with sparse data. statistics\nin medicine 21, 3789\u20133801.\nlaara, e. and j. n. matthews (1985). the equivalence of two models for ordinal data.\nbiometrika 72, 206\u2013207.\nlaird, n. m., g. j. beck, and j. h. ware (1984). mixed models for serial categorical response.\nquoted in a. eckholm (1991). maximum likelihood for many short binary time series\n(preprint).\nlambert, d. (1992). zero-inflated poisson regression with an application to defects in manu-\nfacturing. technometrics 34, 1\u201314.\nlambert, d. and k. roeder (1995). overdispersion diagnostics for generalized linear models.\njournal of the american statistical association 90, 1225\u20131236.\nland, s. r. and j. h. friedman (1997). variable fusion: a new adaptive signal regression\nmethod. discussion paper 656, department of statistics, carnegie mellon university, pitts-\nburg.\n\n "}, {"Page_number": 543, "text": "bibliography\n\n531\n\nlandwehr, j. m., d. pregibon, and a. c. shoemaker (1984). graphical methods for assessing\nlogistic regression models. journal of the american statistical association 79, 61\u201371.\nlang, j. (1996a). on the comparison of multinomial and poisson log-linear models. journal\nof the royal statistical society b, 253\u2013266.\nlang, j. b. (1996b). maximum likelihood methods for a generalized class of log-linear mod-\nels. annals of statistics 24, 726\u2013752.\nlang, j. b. and a. agresti (1994). simultaneous modelling joint and marginal distributions of\nmultivariate categorical responses. journal of the american statistical association 89, 625\u2013\n632.\nlang, s. and a. brezger (2004a, mar). bayesian p-splines. journal of computational and\ngraphical statistics 13(1), 183\u2013212.\nlang, s. and a. brezger (2004b). bayesian p-splines. journal of computational and graphi-\ncal statistics 13, 183\u2013212.\nlauritzen, s. (1996). graphical models. new york: oxford university press.\nlawless, j. f. and k. singhal (1978). efficient screening of nonnormal regression models.\nbiometrics 34, 318\u2013327.\nlawless, j. f. and k. singhal (1987). ismod: an all-subsets regression program for general-\nized linear models. computer methods and programs in biomedicine 24, 117\u2013134.\nlecessie (1992). ridge estimators in logistic regression. applied statistics 41(1), 191\u2013201.\nlecessie, s. and j. c. van houwelingen (1991). a goodness-of-fit test for binary regression\nmodels, based on smoothing methods. biometrics 47, 1267\u20131282.\nlecessie, s. and j. c. van houwelingen (1995). goodness-of-fit tests for generalized linear\nmodels based on random effect models. biometrics 51, 600\u2013614.\nlee, j., m. park, and s. song (2005). an extensive comparison of recent classification tools\napplied to microarray data. computational statistics and data analysis 48, 869\u2013885.\nlee, w.-c. and c. k. hsiao (1996). alternative summary indices for the receiver operating\ncharacteristic curve. epidemiology 7, 605\u2013611.\nleitenstorfer, f. and g. tutz (2007). generalized monotonic regression based on b-splines\nwith an application to air pollution data. biostatistics 8, 654\u2013673.\nleitenstorfer, f. and g. tutz (2011). estimation of single-index models based on boosting\ntechniques. statistical modelling.\nleng, c. (2009). a simple approach for varying-coefficient model selection. journal of\nstatistical planning and inference 139(7), 2138\u20132146.\nlesaffre, e. and a. albert (1989). multiple-group logistic regression diagnostics. applied\nstatistics 38, 425\u2013440.\nli, y. and d. ruppert (2008). on the asymptotics of penalized splines. biometrika 95, 415\u2013\n436.\nliang, k.-y. and p. mccullagh (1993). case studies in binary dispersion. biometrics 49,\n623\u2013630.\nliang, k.-y. and s. zeger (1986). longitudinal data analysis using generalized linear models.\nbiometrika 73, 13\u201322.\nliang, k.-y. and s. zeger (1993). regression analysis for correlated data. annual reviews\npublic health 14, 43\u201368.\nliang, k.-y., s. zeger, and b. qaqish (1992). multivariate regression analysis for categorical\ndata (with discussion). journal of the royal statistical society b 54, 3\u201340.\n\n "}, {"Page_number": 544, "text": "532\n\nbibliography\n\nlin, x. and n. e. breslow (1996). bias correction in generalized linear mixed models with\nmultiple components of dispersion. journal of the american statistical association 91, 1007\u2013\n1016.\nlin, x. and r. carroll (2006). semiparametric estimation in general repeated measures prob-\nlems. journal of the royal statistical society, b, 68, 69\u201388.\nlin, x. and d. zhang (1999).\ninference in generalized additive mixed models by using\nsmoothing splines. journal of the royal statistical society. series b (statistical methodol-\nogy) 61, 381\u2013400.\nlin, y. and y. jeon (2006). random forests and adaptive nearest neighbors. journal of the\namerican statistical association 101, 578\u2013590.\nlindsey, j. j. (1993). models for repeated measurements. oxford: oxford university press.\nlinton, o. b. and w. h\u00e4rdle (1996). estimation of additive regression models with known\nlinks. biometrika 83, 529\u2013540.\nlipsitz, s., g. fitzmaurice, and g. molenberghs (1996). goodness-of-fit tests for ordinal\nresponse regression models. applied statistics 45, 175\u2013190.\nlipsitz, s., n. laird, and d. harrington (1990). finding the design matrix for the marginal\nhomogeneity model. biometrika 77, 353\u2013358.\nlipsitz, s., n. laird, and d. harrington (1991). generalized estimation equations for cor-\nrelated binary data: using the odds ratio as a measure of association in unbalanced mixed\nmodels with nested random effects. biometrika 78, 153\u2013160.\nliu, q. and a. agresti (2005). the analysis of ordinal categorical data: an overview and a\nsurvey of recent developments. test 14, 1\u201373.\nliu, q. and d. a. pierce (1994). a note on gauss-hermite quadrature. biometrika 81, 624\u2013\n629.\nlloyd, c. j. (2008). a new exact and more powerful unconditional test of no treatment effect\nfrom binary matched pairs. biometrics 64(3), 716\u2013723.\nloader, c. (1999). local regression and likelihood. new york: springer-verlag.\nloh, w. and y. shih (1997). split selection methods for classification trees. statistica sinica 7,\n815\u2013840.\nlongford, n. l. (1993). random effect models. new york: oxford university press.\nlouis, t. a. (1982). finding the observed information matrix when using the em algorithm.\njournal of the royal statistical society b 44, 226\u2013233.\nluce, r. d. (1959). individual choice behaviour. new york: wiley.\nlunetta, k., l. hayward, j. segal, and p. eerdewegh (2004). screening large-scale associa-\ntion study data: exploiting interactions using random forests. bmc genetics 5(1), 32.\nmaddala, g. s. (1983). limited-dependent and qualitative variables in econometrics. cam-\nbridge: cambridge university press.\nmagder, l. and s. zeger (1996). a smooth nonparametric estimate of a mixing distribution\nusing mixtures of gaussians. journal of the american statistical association 91, 1141\u20131151.\nmagnus, j. r. and h. neudecker (1988). matrix differential calculus with applications in\nstatistics and econometrics. london: wiley.\nmancl, l. a. and b. g. leroux (1996). efficiency of regression estimates for clustered data.\nbiometrics 52, 500\u2013511.\nmantel, n. and w. haenszel (1959). statistical aspects of the analysis of data from retrospec-\ntive studies. j. natl. cancer inst. 22, 719\u201348.\n\n "}, {"Page_number": 545, "text": "bibliography\n\n533\n\nmarks, s. and o. j. dunn (1974). discriminant functions when covariance matrices are un-\nequal. journal of the american statistical association 69.\nmarra, g. and s. wood (2011). practical variable selection for generalized additive models.\ncomputational statistics and data analysis 55, 2372\u20132387.\nmarx, b. d. and p. h. c. eilers (1998). direct generalized additive modelling with penalized\nlikelihood. computational statistics & data analysis 28, 193\u2013209.\nmarx, b. d. and p. h. c. eilers (1999). generalized linear regression on sampled signals and\ncurves: a p-spline approach. technometrics 41, 1\u201313.\nmarx, b. d. and p. h. c. eilers (2005). multidimensional penalized signal regression. tech-\nnometrics 47, 13\u201322.\nmasters, g. n. (1982). a rasch model for partial credit scoring. psychometrika 47, 149\u2013174.\nmccullagh, p. (1980). regression model for ordinal data (with discussion). journal of the\nroyal statistical society b 42, 109\u2013127.\nmccullagh, p. (1983). quasi-likelihood functions. annals of statistics 11, 59\u201367.\nmccullagh, p. and j. a. nelder (1989). generalized linear models (second edition). new\nyork: chapman & hall.\nmcculloch, c. and s. searle (2001). generalized, linear, and mixed models. new york:\nwiley.\nmcculloch, c. e. (1997). maximum likelihood algorithms for generalized linear mixed mod-\nels. journal of the american statistical association 92, 162\u2013170.\nmcdonald, b. w. (1993). estimating logistic regression parameters for bivariate binary data.\njournal of the royal statistical society b 55, 391\u2013397.\nmcfadden, d. (1973). conditional logit analysis of qualitative choice behaviour. in p. zarem-\nbka (ed.), frontiers in econometrics. new york: academic press.\nmcfadden, d. (1978). modelling the choice of residential location.\n(eds.), spatial interaction theory and residential location. amsterdam: north-holland.\nmcfadden, d. (1981). econometric models of probabilistic choice.\nin c. f. manski and\nd. mcfadden (eds.), structural analysis of discrete data with econometric applications, pp.\n198\u2013272. cambridge, ma: mit press.\nmcfadden, d. (1986). the choice theory approach to market research. marketing science 5,\n275\u2013297.\nmclachlan, g. and t. krishnan (1997). the em algorithm and extensions. new york: wiley.\nmclachlan, g. j. (1992). discriminant analysis and statistical pattern recognition. new\nyork: wiley.\nmclachlan, g. j. and d. peel (2000). finite mixture models. new york: wiley.\nmcnemar, q. (1947). note on the sampling error of the difference between correlated propor-\ntions or percentages. psychometrika 12, 153\u2013157.\nmehta, c. r., n. r. patel, and a. a. tsiatis (1984). exact significance testing to establish\ntreatment equivalence with ordered categorical data. biometrics 40, 819\u2013825.\nmeier, l., s. van de geer, and p. b\u00fchlmann (2008). the group lasso for logistic regression.\njournal of the royal statistical society, series b 70, 53\u201371.\nmeier, l., s. van de geer, and p. b\u00fchlmann (2009). high-dimensional additive modeling.\nthe annals of statistics 37, 3779\u20133821.\nmeilijson, i. (1989). a fast improvement to the em-algorithm on its own terms. journal of\nthe royal statistical society b 51, 127\u2013138.\n\nin a. karlquist et al.\n\n "}, {"Page_number": 546, "text": "534\n\nbibliography\n\nmiller, a. j. (1989). subset selection in regression. london: chapman & hall.\nmiller, m. e., c. s. davis, and r. j. landis (1993). the analysis of longitudinal polytomous\ndata: generalized estimated equations and connections with weighted least squares. biomet-\nrics 49, 1033\u20131044.\nmiller, r. and d. siegmund (1982). maximally selected chi-square statistics. biometrics 38,\n1011\u20131016.\nmin, a. and c. czado (2010). testing for zero-modification in count regression models.\nstatistica sinica 20, 323\u2013341.\nmittlb\u00f6ck, m. and m. schemper (1996). explained variation for logistic regression. statistic\nin medicine 15, 1987\u20131997.\nmkhadri, a., g. celeux, and a. nasroallah (1997). regularization in discriminant analysis:\nan overview. computational statistics & data analysis 23, 403\u2013423.\nmolenberghs, g. and e. lesaffre (1994). marginal modelling of correlated ordinal data using\na multivariate plackett distribution. journal of the american statistical association 89, 633\u2013\n644.\nmolenberghs, g. and g. verbeke (2005). models for discrete longitdinal data. new york:\nspringer\u2013verlag.\nmolinaro, a., r. simon, and r. m. pfeiffer (2005). predition error estimation: a comparison\nof resampling methods. bioinformatics 21, 3301\u20133307.\nmoore, d. f. and a. tsiatis (1991). robust estimation of the variance in moment methods for\nextra-binomial and extra-poisson variation. biometrics 47, 383\u2013401.\nmorgan, b. j. t. (1985). the cubic logistic model for quantal assay data. applied statistics 34,\n105\u2013113.\nmorgan, b. j. t. and d. m. smith (1993). a note on wadley\u2019s problem with overdispersion.\napplied statistics 41, 349\u2013354.\nmorgan, j. n. and j. a. sonquist (1963). problems in the analysis of survey data, and a\nproposal. journal of the american statistical association 58, 415\u2013435.\nmorin, r. l. and d. e. raeside (1981). a reappraisal of distance-weighted k-nearest neighbor\nclassification for pattern recognition with missing data. ieee transactions on systems, man\nand cybernetics 11, 241\u2013243.\nmoulton, l. and s. zeger (1989). analysing repeated measures in generalized linear models\nvia the bootstrap. biometrics 45, 381\u2013394.\nmuggeo, v. m. r. and g. ferrara (2008). fitting generalized linear models with unspecified\nlink function: a p-spline approach. computational statistics & data analysis 52(5), 2529\u2013\n2537.\nmullahy, j. (1986). specification and testing of some modified count data models. journal of\neconometrics 33, 341\u2013365.\nnadaraya, e. a. (1964). on estimating regression. theory of probability and applications 10,\n186\u2013190.\nnagelkerke, n. j. d. (1991). a note on a general definition of the coefficient of determination.\nbiometrika 78, 691\u2013692.\nnaik, p. a. and c. tsai (2001). single-index model selections. biometrika 88, 821\u2013832.\nnair, v. n. (1987). chi-squared-type tests for ordered alternatives in contingency tables.\njournal of the american statistical association 82, 283\u2013291.\nnelder, j. a. (1992). joint modelling of mean and dispersion. in p. van der heijden, w. jansen,\nb. francis, and g. seeber (eds.), statistical modelling. amsterdam: north-holland.\n\n "}, {"Page_number": 547, "text": "bibliography\n\n535\n\nnelder, j. a. and d. pregibon (1987). an extended quasi-likelihood function. biometrika 74,\n221\u2013232.\nnelder, j. a. and r. w. m. wedderburn (1972). generalized linear models. journal of the\nroyal statistical society a 135, 370\u2013384.\nneuhaus, j., w. hauck, and j. kalbfleisch (1992). the effects of mixture distribution. mis-\nspecification when fitting mixed effect logistic models. biometrika 79(4), 755\u2013762.\nneuhaus, j. m., j. d. kalbfleisch, and w. w. hauck (1991). a comparison of cluster-specific\nand population-averaged approaches for analyzing correlated binary data. international sta-\ntistical review 59, 25\u201335.\nnewson, r. (2002). parameters behind \"nonparametric\" statistics: kendall\u2019s tau, somers\u2019 d\nand median differences. the stata journal 2, 45\u201364.\nni, x., d. zhang, and h. h. zhang (2010). variable selection for semiparametric mixed\nmodels in longitudinal studies. biometrics 66, 79\u201388.\nnyquist, h. (1991). restricted estimation of generalized linear models. applied statistics 40,\n133\u2013141.\nogden, r. t. (1997). essential wavelets for statistical applications and data analysis.\nboston: birkh\u00e4user.\nopsomer, j. d. (2000). asymptotic properties of backfitting estimators. journal of multivari-\nate analysis 73, 166\u2013179.\nopsomer, j. d. and d. ruppert (1997). fitting a bivariate additive model by local polynomial\nregression. annals of statistics 25, 186\u2013211.\nosborne, m., b. presnell, and b. turlach (2000). on the lasso and its dual. journal of\ncomputational and graphical statistics 9(2), 319\u2013337.\nosborne, m. r., b. presnell, and b. a. turlach (1998). knot selection for regression splines\nvia the lasso. in s. weisberg (ed.), dimension reduction, computational complexity, and\ninformation, volume 30 of computing science and statistics, pp. 44\u201349.\nosius, g. (2004). the association between two random elements: a complete characterization\nand odds ratio models. metrika 60, 261\u2013277.\nosius, g. and d. rojek (1992). normal goodness-of-fit tests for parametric multinomial mod-\nels with large degrees of freedom. journal of the american statistical association 87, 1145\u2013\n1152.\npaik, m. and y. yang (2004). combining nearest neighbor classifiers versus cross-validation\nselection. statistical applications in genetics and molecular biology 3(12).\npalmgren, j. (1981). the fisher information matrix for log-linear models arguing condition-\nally in the observed explanatory variables. biometrika 68, 563\u2013566.\npalmgren, j. (1989). regression models for bivariate binary responses. uw biostatistics\nworking paper series.\npark, m. y. and t. hastie (2007). an l1 regularization-path algorithm for generalized linear\nmodels. journal of the royal statistical society b 69, 659\u2013677.\npark, t. and g. casella (2008). the bayesian lasso. journal of the american statistical\nassociation 103, 681\u2013686.\nparthasarthy, g. and b. n. chatterji (1990). a class of new knn methods for low sample\nproblems. ieee transactions on systems, man and cybernetics 20, 715\u2013718.\npatterson, h. and r. thomson (1971). recovery of inter-block information when block sizes\nare unequal. biometrika 58, 545\u2013554.\n\n "}, {"Page_number": 548, "text": "536\n\nbibliography\n\npepe, m. s. (2003). the statistical evaluation of medical tests for classification and predic-\ntion. new york: chapman & hall.\npeterson, b. and f. e. harrell (1990). partial proportional odds models for ordinal response\nvariables. applied statistics 39, 205\u2013217.\npetricoin, e. f., d. k. ornstein, c. p. paweletz, a. m. ardekani, p. s. hackett, b. a. hitt,\na. velassco, c. trucco, l. wiegand, k. wood, c. b. simone, p. j. levine, w. m. lineham,\nm. r. emmert-buck, s. m. steinberg, e. c. kohn, and l. a. liotta (2002). serum proteomic\npatterns for detection of prostate cancer. journal of the national cancer institute 94, 1576\u2013\n1578.\npetry, s. and g. tutz (2011). the oscar for generalized linear models. technical report 112,\nlmu, department of statistics.\npetry, s. and g. tutz (2012). shrinkage and variable selection by polytopes. journal of\nstatistical planning and inference 142, 48\u201364.\npetry, s., g. tutz, and c. flexeder (2011). pairwise fused lasso. technical report 102, lmu,\ndepartment of statistics.\npiccarreta, r. (2008). classification trees for ordinal variables. computational statistics 23(3),\n407\u2013427.\npiegorsch, w. (1992). complementary log regression for generalized linear models. the\namerican statistician 46, 94\u201399.\npiegorsch, w. w., c. r. weinberg, and b. h. margolin (1988). exploring simple independent\naction in multifactor tables of proportions. biometrics 44, 595\u2013603.\npierce, d. a. and d. w. schafer (1986). residuals in generalized linear models. journal of\nthe american statistical association 81, 977\u2013986.\npigeon, j. and j. heyse (1999). an improved goodness-of-fit statistic for probability prediction\nmodels. biometrical journal 41, 71\u201382.\npinheiro, j. c. and d. m. bates (1995). approximations to the log-likelihood function in the\nnonlinear mixed-effects model. journal of computational and graphical statistics 4, 12\u201335.\npoggio, t. and f. girosi (1990). regularization algorithms for learning that are equivalent to\nmultilayer networks. science 247, 978\u2013982.\npohlmeier, w. and v. ulrich (1995). an econometric model of the two-part decisionmaking\nprocess in the demand for health care. journal of human resources 30, 339\u2013361.\npoortema, k. l. (1999). on modelling overdispersion of counts. statistica neerlandica 53,\n5\u201320.\npowell, j. l., j. h. stock, and t. m. stoker (1989). semiparametric estimation of index\ncoefficients. econometrica 57, 1403\u20131430.\npregibon, d. (1980). goodness of link tests for generalized linear models. applied statis-\ntics 29, 15\u201324.\npregibon, d. (1981). logistic regression diagnostics. annals of statistics 9, 705\u2013724.\npregibon, d. (1982). resistant fits for some commonly used logistic models with medical\napplications. biometrics 38, 485\u2013498.\npregibon, d. (1984). review of generalized linear models by mccullagh and nelder. american\nstatistician 12, 1589\u20131596.\nprentice, r. and r. pyke (1979). logistic disease incidence models and case-control studies.\nbiometrika 66, 403.\nprentice, r. l. (1976). a generalization of the probit and logit methods for close response\ncurves. biometrics 32, 761\u2013768.\n\n "}, {"Page_number": 549, "text": "bibliography\n\n537\n\nprentice, r. l. (1986). binary regression using an extended beta-binomial distribution, with\ndiscussion of correlation induced by covariate measurement errors. journal of the american\nstatistical association 81, 321\u2013327.\nprentice, r. l. (1988). correlated binary regression with covariates specific to each binary\nobservation. biometrics 44, 1033\u20131084.\npulkstenis, e. and t. j. robinson (2002). two goodness-of-fit tests for logistic regression\nmodels with continuous covariates. statistics in medicine 21, 79\u201393.\npulkstenis, e. and t. j. robinson (2004). goodness-of-fit tests for ordinal response regression\nmodels. statistics in medicine 23, 999\u20131014.\nqu, y., g. w. williams, g. j. beck, and m. goormastic (1987). a generalized model of\nlogistic regression for clustered data. communications in statistics \u2013 theory and methods 16,\n3447\u20133476.\nquinlan, j. r. (1986). industion of decision trees. machine learning 1, 81\u2013106.\nquinlan, j. r. (1993). programs for machine learning. san francisco: morgan kaufmann\npublisherinc.\nr development core team (2010). r: a language and environment for statistical comput-\ning. vienna, austria: r foundation for statistical computing. isbn 3-900051-07-0.\nradelet, m. and g. pierce (1991). choosing those who will die: race and the death penalty\nin florida. florida law review 43, 1\u201334.\nramsey, j. o. and b. w. silverman (2005). functional data analysis. new york: springer\u2013\nverlag.\nrao, p. and l. kupper (1967). ties in paired-comparison experiments: a generalization of\nthe bradley-terry model. journal of the american statistical association 62, 194\u2013204.\nrasch, g. (1961). on general laws and the meaning of measurement in psychology.\nin\nj. neyman (ed.), proceedings of the fourth berkeley symposium on mathematical statistics\nand probability, berkeley.\nravikumar, p., m. wainwright, and j. lafferty (2009). high-dimensional graphical model\nselection using l1-regularized logistic regression. annals of statistics 3, 1287\u20131319.\nrawlings, j., s. pantula, and d. dickey (1998). applied regression analysis. new york:\nspringer\u2013verlag.\nrayens, w. and t. greene (1991). covariance pooling and stabilization for classification.\ncomputational statistics and data analysis 11, 17\u201342.\nread, i. and n. cressie (1988). goodness-of-fit statistics for discrete multivariate data.\nnew york: springer-verlag.\nreinsch, c. (1967). smoothing by spline functions. numerische mathematik 10, 177\u2013183.\nridgeway, g. (1999). generalization of boosting algorithms and applications of bayesian\ninference for massive datasets. ph. d. thesis, university of washington.\nripley, b. d. (1996). pattern recognition and neural networks. cambridge: cambridge\nuniversity press.\nrobinson, g. k. (1991). that blup is a good thing. the estimation of random effects. statistical\nscience 6, 15\u201351.\nromualdi, c., s. campanaro, d. campagna, b. celegato, n. cannata, s. toppo, g. valle,\nand g. lanfranchi (2003). pattern recognition in gene expression profiling using dna array:\na comparison study of different statistical methods applied to cancer classification. human\nmolecular genetics 12, 823\u2013836.\n\n "}, {"Page_number": 550, "text": "538\n\nbibliography\n\nrosenstone, s., d. kinder, and w. miller (1997). american national election study. mi:\ninter\u2013university consortium for political and social research.\nrosner, b. (1984). multivariate methods in orphthalmology with applications to other paired-\ndata situations. biometrics 40, 1025\u20131035.\nrosset, s. (2004). tracking curved regularized optimization solution paths. in advances in\nneural information processing systems, cambridge. mit press.\nrousseeuw, p. j. and a. christmann (2003). robustness against separation and outliers in\nlogistic regression. computational statistics and data analysis 43, 315\u2013332.\nruckstuhl, a. and a. welsh (1999). reference bands for nonparametrically estimated link\nfunctions. journal of computational and graphical statistics 8(4), 699\u2013714.\nrudolfer, s. m., p. c. watson, and e. lesaffre (1995). are ordinal models useful for classifi-\ncation? a revised analysis. journal of statistical computation simulation 52(2), 105\u2013132.\nrue, h. and l. held (2005). gaussian markov random fields.theory and applications.\nlondon: crc / chapman & hall.\nrumelhart, d. l. and j. g. greeno (1971). similarity between stimuli: an experimental test\nof the luce and restle choice methods. journal of mathematical psychology 8, 370\u2013381.\nruppert, d. (2002). selecting the number of knots for penalized splines. journal of compu-\ntational and graphical statistics 11, 735\u2013757.\nruppert, d., m. p. wand, and r. j. carroll (2003). semiparametric regression. cambridge:\ncambridge university press.\nruppert, d., m. p. wand, and r. j. carroll (2009). semiparametric regression during 2003 \u2013\n2007. electronic journal of statistics 3, 1193\u20131256.\nryan, t. (1997). modern regression methods. new york: wiley.\nsampson, a. and h. singh (2002). min and max scorings for two sample partially ordered\ncategorical data. journal of statistical planning and inference 107, 219\u2013236.\nsantner, t. j. and d. e. duffy (1986). a note on a. albert and j. a. anderson\u2019s conditions for\nthe existence of maximum likelihood estimates regression models. biometrika 73, 755\u2013758.\nsantner, t. j. and d. e. duffy (1989). the statistical analysis of discrete data. new york:\nspringer-verlag.\nschaefer, r. l., l. d. roi, and r. a. wolfe (1984). a ridge logistic estimate. communication\nin statistics, theory & methods 13, 99\u2013113.\nschall, r. (1991). estimation in generalised linear models with random effects. biometrika 78,\n719\u2013727.\nschwarz, g. (1978). estimating the dimension of a model. annals of statistics 6, 461\u2013464.\nscott, a. and c. wild (1986). fitting logistic models under case-control or choice based\nsampling. journal of the royal statistical society. series b (methodological) 48(2), 170\u2013182.\nsearle, s., g. casella, and c. mcculloch (1992). variance components. new york: wiley.\nseeber, g. (1977). linear regression analysis. new york: wiley.\nsegerstedt, b. (1992). on ordinary ridge regression in generalized linear models. communi-\ncations in statistics \u2013 theory and methods 21, 2227\u20132246.\nshapire, r. e. (1990). the strength of weak learnability. machine learning 5, 197\u2013227.\nshih, y.-s. (2004). a note on split selection bias in classification trees. computational statis-\ntics and data analysis 45, 457\u2013466.\nshih, y.-s. and h. tsai (2004). variable selection bias in regression trees with constant fits.\ncomputational statistics and data analysis 45, 595\u2013607.\n\n "}, {"Page_number": 551, "text": "bibliography\n\n539\n\nshipp, m., k. ross, p. tamayo, a. weng, j. kutok, r. aguiar, m. gaasenbeek, m. angelo,\nm. reich, g. pinkus, et al. (2002). diffuse large b-cell lymphoma outcome prediction by\ngene-expression profiling and supervised machine learning. nature medicine 8(1), 68\u201374.\nsilverman, b. w. and m. c. jones (1989). commentary on fix and hodges (1951): an impor-\ntant contribution to nonparametric discriminant analysis and density estimation. international\nstatistical review 57, 233\u2013238.\nsimonoff, j. (1995). smoothing categorical data. journal of statistical planning and infer-\nence 47, 41\u201369.\nsimonoff, j. s. (1983). a penalty function approach to smoothing large sparse contingency\ntables. annals of statistics 11, 208\u2013218.\nsimonoff, j. s. (1996). smoothing methods in statistics. new york: springer-verlag.\nsimonoff, j. s. and g. tutz (2000). smoothing methods for discrete data. in m. schimek (ed.),\nsmoothing and regression. approaches, computation and application. new york: wiley.\nslawski, m. (2010). the structured elastic net for quantile regression and support vector\nclassification. statistics and computing.\nslawski, m., m. daumer, and a.-l. boulesteix (2008). cma \u2013 a comprehensive bioconductor\npackage for supervised classification with high dimensional data. bmc bioinformatics 9, 439.\nsmith, m. and r. kohn (1996). nonparametric regression using bayesian variable selection.\njournal of econometrics 75, 317\u2013343.\nsmith, p. l. (1982). curve fitting and modeling with splines using statistical variable selection\ntechniques. report 166034, nasa.\nsnell, e. j. (1964). a scaling procedure for ordered categorical data. biometrics 20, 592\u2013607.\nsobehart, j., s. keenan, and r. stein (2000). validation methodologies for default risk models.\ncredit, 51\u201356.\nsoofi, e. s., j. j. retzer, and m. yasai-ardekani (2000). a framework for measuring the im-\nportance of variables with applications to management research and decision models. decision\nsciences 31, 595\u2013625.\nstatnikov, a., c. f. aliferis, i. tsamardinos, d. hardin, and s. levy (2005). a comprehen-\nsive evaluation of multicategory classification methods for microarray gene expression cancer\ndiagnosis. bioinformatics 21, 631\u2013643.\nsteadman, s. and l. weissfeld (1998). a study of the effect of dichotomizing ordinal data\nupon modelling. communications in statistics \u2013 simulation and computation 27(4), 871\u2013\n887.\nstein, c. (1981). estimation of the mean of a multivariate normal distribution. annals of\nstatistics 9, 1135\u20131151.\nsteinwart, i. and a. christmann (2008). support vector machines. springer verlag.\nstiratelli, r., n. laird, and j. h. ware (1984). random-effects models for serial observation\nwith binary response. biometrics 40, 961\u2013971.\nstone, c., m. hansen, c. kooperberg, and y. truong (1997). polynomial splines and their\ntensor products in extended linear modeling. the annals of statistics 25, 1371\u20131470.\nstone, c. j. (1977). consistent nonparametric regression (with discussion). annals of statis-\ntics 5, 595\u2013645.\nstram, d. o. and l. j. wei (1988). analyzing repeated measurements with possibly missing\nobservations by modelling marginal distributions. statistics in medicine 7, 139\u2013148.\n\n "}, {"Page_number": 552, "text": "540\n\nbibliography\n\nstram, d. o., l. j. wei, and j. h. ware (1988). analysis of repeated categorical outcomes\nwith possibly missing observations and time-dependent covariates. journal of the american\nstatistical association 83, 631\u2013637.\nstrobl, c., a.-l. boulesteix, and t. augustin (2007). unbiased split selection for classification\ntrees based on the gini index. computational statistics & data analysis 52, 483\u2013501.\nstrobl, c., a.-l. boulesteix, t. kneib, t. augustin, and a. zeileis (2008). conditional variable\nimportance for random forests. bmc bioinformatics 9(1), 307.\nstrobl, c., j. malley, and g. tutz (2009). an introduction to recursive partitioning: rationale,\napplication and characteristics of classification and regression trees, bagging and random\nforests. psychological methods 14, 323\u2013348.\nstroud, a. h. and d. secrest (1966). gaussian quadrature formulas. englewood cliffs, nj:\nprentice-hall.\nstukel, t. a. (1988). generalized logistic models. journal of the american statistical associ-\nation 83(402), 426\u2013431.\nsuissa, s. and j. j. shuster (1991). the 2\u00d72 method-pairs trial: exact unconditional design\nand analysis. biometrics 47, 361\u2013372.\nt\u00fcchler, r. (2008). bayesian variable selection for logistic models using auxiliary mixture\nsampling. journal of computational and graphical statistics 17, 76\u201394.\nthall, p. f. and s. c. vail (1990). some covariance models for longitudinal count data with\noverdispersion. biometrics 46, 657\u2013671.\ntheil, h. (1970). on the estimation of relationships involving qualitative variables. american\njournal of sociology 76(1), 103\u2013154.\nthurner, p. and a. eymann (2000). policy-specific alienation and indifference in the calculus\nof voting: a simultaneous model of party choice and abstention. public choice 102, 49\u201375.\nthurstone, l. l. (1927). a law of comparative judgement. psychological review 34, 273\u2013286.\ntibshirani, r. (1996). regression shrinkage and selection via the lasso. journal of the royal\nstatistical society b 58, 267\u2013288.\ntibshirani, r. and t. hastie (1987). local likelihood estimation. journal of the american\nstatistical association 82, 559\u2013568.\ntibshirani, r., t. hastie, b. narasimhan, and g. chu (2003). class prediction by nearest\nshrunken centroids, with applications to dna microarrays. statistical science, 104\u2013117.\ntibshirani, r., t. hastie, b. narasimhan, s. soltys, g. shi, a. koong, and q.-t. le (2004).\nsample classification from protein mass spectrometry, by \"\u2019peak probability contrasts\". bioin-\nformatics 20, 3034\u20133044.\ntibshirani, r., m. saunders, s. rosset, j. zhu, and k. kneight (2005). sparsity and smooth-\nness via the fused lasso. journal of the royal statistical society b 67, 91\u2013108.\ntjur, t. (1982). a connection between rasch\u2018s item analysis model and a multiplicative\npoisson modelb. scandinavian journal of statistics 9, 23\u201330.\ntoledano, a. and c. gatsanis (1996). ordinal regression methodology for roc curves derived\nfrom correlated data. statistics in medicine 15, 1807\u20131826.\ntroyanskaya, o. g., m. e. garber, p. o. brown, d. botstein, and r. b. altman (2002). non-\nparametric methods for identifying differentially expressed genes in microarray data. bioin-\nformatics 18, 1454\u20131461.\ntsiatis, a. a. (1980). a note on a goodness-of-fit test for the logistic regression model.\nbiometrika 67, 250\u2013251.\n\n "}, {"Page_number": 553, "text": "bibliography\n\n541\n\ntukey, j. (1977). exploratory data analysis. reading, pennsylvania: addison wesley.\ntutz, g. (1986). bradley-terry-luce models with an ordered response. journal of mathemat-\nical psychology 30, 306\u2013316.\ntutz, g. (1991). sequential models in ordinal regression. computational statistics & data\nanalysis 11, 275\u2013295.\ntutz, g. (2003). generalized semiparametrically structured ordinal models. biometrics 59,\n263\u2013273.\ntutz, g. (2005). modelling of repeated ordered measurements by isotonic sequential regres-\nsion. statistical modelling 5(4), 269\u2013287.\ntutz, g. and h. binder (2004). flexible modelling of discrete failure time including time-\nvarying smooth effects. statistics in medicine 23(15), 2445\u20132461.\ntutz, g. and h. binder (2006). generalized additive modeling with implicit variable selection\nby likelihood-based boosting. biometrics 62, 961\u2013971.\ntutz, g. and h. binder (2007). boosting ridge regression. computational statistics & data\nanalysis 51, 6044\u20136059.\ntutz, g. and j. gertheiss (2010). feature extraction in signal regression: a boosting technique\nfor functional data regression. journal of computational and graphical statistics 19, 154\u2013\n174.\ntutz, g. and a. groll (2010a). binary and ordinal random effects models including variable\nselection. technical report 97, lmu, department of statistics.\ntutz, g. and a. groll (2010b). generalized linear mixed models based on boosting.\nin\nt. kneib and g. tutz (eds.), statistical modelling and regression structures - festschrift in\nthe honour of ludwig fahrmeir, pp. 197\u2013215. physica.\ntutz, g. and k. hechenbichler (2005). aggregating classifiers with ordinal response structure.\njournal of statistical computation and simulation 75(5), 391\u2013408.\ntutz, g. and w. hennevogl (1996). random effects in ordinal regression models. computa-\ntional statistics and data analysis 22, 537\u2013557.\ntutz, g. and g. kauermann (1997). local estimators in multivariate generalized linear models\nwith varying coefficients. computational statistics 12, 193\u2013208.\ntutz, g. and f. leitenstorfer (2006). response shrinkage estimators in binary regression.\ncomputational statistics and data analysis 50, 2878\u20132901.\ntutz, g. and s. petry (2011). nonparametric estimation of the link function including variable\nselection. statistics and computing, to appear.\ntutz, g. and f. reithinger (2007). a boosting approach to flexible semiparametric mixed\nmodels. statistics in medicine 26, 2872\u20132900.\ntutz, g. and t. scholz (2004). semiparametric modelling of multicategorial data. journal of\nstatistical computation & simulation 74, 183\u2013200.\ntutz, g. and j. ulbricht (2009). penalized regression with correlation based penalty. statistics\nand computing 19, 239\u2013253.\ntversky, a. (1972). elimination by aspects: a theory of choice. psychological review 79,\n281\u2013299.\ntweedie, m. c. k. (1957). statistical properties of inverse gaussian distributions. i. annals\nof mathematical statistics 28(2), 362\u2013377.\nulbricht, j. and g. tutz (2008). boosting correlation based penalization in generalized linear\nmodels. in shalabh and c. heumann (eds.), recent advances in linear models and related\nareas. new york: springer\u2013verlag.\n\n "}, {"Page_number": 554, "text": "542\n\nbibliography\n\nulm, k. (1991). a statistical method for assessing a threshold in epidemiological studies.\nstatistics in medicine 10, 341\u2013348.\nvan den broek, j. (1995). a score test for zero inflation in a poisson distribution. biomet-\nrics 51(2), 738\u2013743.\nvan der linde, a. and g. tutz (2008). on association in regression: the coefficient of deter-\nmination revisited. statistics 42, 1\u201324.\nvan houwelingen, j. c. and s. l. cessie (1990). predictive value of statistical models. statis-\ntics in medicine 9, 1303\u20131325.\nvenables, w. n. and b. d. ripley (2002). modern applied statistics with s. fourth edition.\nnew york: springer\u2013verlag.\nverbeke and g. molenberghs (2000). linear mixed models for longitudinal data. new york:\nspringer\u2013verlag.\nvidakovic (1999). statistical modelling by wavelets. wiley series in probability and statis-\ntics. new york: wiley.\nvuong, q. (1989). likelihood ratio tests for model selection and non-nested hypotheses.\neconometrica 2, 307\u2013333.\nwacholder, s. (1986). binomial regression in glim: estimation risk ratios and risk differ-\nences. american journal of epidemiology 123, 174\u2013184.\nwahba, g. (1990). spline models for observational data. philadelphia: society for industrial\nand applied mathematics.\nwalker, s. h. and d. b. duncan (1967). estimation of the probability of an event as a function\nof several independent variables. biometrika 54, 167\u2013178.\nwand, m. p. (2000). a comparison of regression spline smoothing procedures. computational\nstatistics 15, 443\u2013462.\nwand, m. p. (2003). smoothing and mixed models. computational statistics 18(2), 223\u2013249.\nwang, h. and y. xia (2009). shrinkage estimation of the varying coefficient model. journal\nof the american statistical association 104(486), 747\u2013757.\nwang, l. (2011). gee analysis of clustered binary data with diverging number of covariates.\nann. statist. 39, 389\u2013417.\nwang, y.-f. and v. carey (2003). working correlation structure missclassification, estimation\nand covariate design: implicationa for generalized estimating equations. biometrika 90, 29\u2013\n41.\nwatson, g. s. (1964). smooth regression analysis. sankhy\u00afa, series a, 26, 359\u2013372.\nwedderburn, r. w. m. (1974). quasilikelihood functions, generalized linear models and the\ngauss-newton method. biometrika 61, 439\u2013447.\nwei, g. and m. tanner (1990). a monte carlo implementation of the em algorithm and the\npoor man\u2019s data augmentation algorithms. journal of the american statistical association 85,\n699\u2013704.\nweisberg, s. and a. h. welsh (1994). adapting for the missing link. annals of statistics 22,\n1674\u20131700.\nwelsh, a., x. lin, and r. carroll (2002). marginal longitudinal nonparametric regression.\njournal of the american statistical association 97(458), 482\u2013493.\nwhittaker, j. (1990). graphical models in applied multivariate statistics. chichester: wiley.\nwhittaker, j. (2008). graphical models in applied multivariate statistics. wiley publishing.\n\n "}, {"Page_number": 555, "text": "bibliography\n\n543\n\nwhittemore, a. s. (1983). transformations to linearity in binary regression. siam journal of\napplied mathematics 43, 703\u2013710.\nwild, c. j. and t. w. yee (1996). additive extensions to generalized estimating equation\nmethods. journal of the royal statistical society b58, 711\u2013725.\nwilkinson, g. n. and c. e. rogers (1973). symbolic description of factorial models for\nanalysis of variance. applied statistics 22, 392\u2013399.\nwilliams, d. a. (1982). extra binomial variation in logistic linear models. applied statis-\ntics 31, 144\u2013148.\nwilliams, o. d. and j. e. grizzle (1972). analysis of contingency tables having ordered\nresponse categories. journal of the american statistical association 67, 55\u201363.\nwilliamson, j. m., k. kim, and s. r. lipsitz (1995). analyzing bivariate ordinal data using a\nglobal odds ratio. journal of the american statistical association 90, 1432\u20131437.\nwinkelmann, r. (1997). count data models: econometric theory and application to labor\nmobility (second edition). berlin: springer-verlag.\nwolfinger, r. w. (1994). laplace\u2019s approximation for nonlinear mixed models. biometrika 80,\n791\u2013795.\nwong, g. y. and w. m. mason (1985). the hierarchical logistic regression model for multi-\nlevel analysis. journal of the american statistical association 80, 513\u2013524.\nwood, s. n. (2000). modelling and smoothing parameter estimation with multiple quadratic\npenalties. journal of the royal statistical society b 62, 413\u2013428.\nwood, s. n. (2004). stable and efficient multiple smoothing parameter estimation for gener-\nalized additive models. journal of the american statistical association 99, 673\u2013686.\nwood, s. n. (2006a). generalized additive models: an introduction with r. london: chap-\nman & hall/crc.\nwood, s. n. (2006b). on confidence intervals for generalized additive models based on pe-\nnalized regression splines. australian & new zealand journal of statistics 48, 445\u2013464.\nwood, s. n. (2006c). thin plate regression splines. journal of the royal statistical society,\nseries b 65, 95\u2013114.\nwu, j. c. f. (1983). on the covergence properties of the em-algorithm. annals of statistics 11,\n95\u2013103.\nxia, t., f. kong, s. wang, and x. wang (2008). asymptotic properties of the maximum\nquasi-likelihood estimator in quasi-likelihood nonlinear models. communications in statistics\n\u2013 theory and methods 37(15), 2358\u20132368.\nxia, y., h. tong, w. k. li, and l. zhu (2002). an adaptive estimation of dimension reduction.\njournal of the royal statistical society b 64, 363\u2013410.\nxie, m. and y. yang (2003). asymptotics for generalized estimating equations with large\ncluster sizes. the annals of statistics 31(1), 310\u2013347.\nye, j. m. (1998). on measuring and correcting the effects of data mining and model selection.\njournal of the american statistical association 93(441), 120\u2013131.\nyee, t. (2010). the vgam package for categorical data analysis. journal of statistical\nsoftware 32(10), 1\u201334.\nyee, t. and t. hastie (2003). reduced-rank vector generalized linear models. statistical\nmodelling 3, 15.\nyee, t. w. and c. j. wild (1996). vector generalized additive models. journal of the royal\nstatistical society b, 481\u2013493.\n\n "}, {"Page_number": 556, "text": "544\n\nbibliography\n\nyellott, j. i. (1977). the relationship between luce\u2019s choice axiom, thurstone\u2019s theory of\ncomparative judgement, and the double exponential distribution. journal of mathematical\npsychology 15, 109\u2013144.\nyu, y. and d. ruppert (2002). penalized spline estimation for partially linear single-index\nmodels. journal of the american statistical association 97, 1042\u20131054.\nyuan, m. and y. lin (2006). model selection and estimation in regression with grouped\nvariables. journal of the royal statistical society b 68, 49\u201367.\nzahid, f. m. and g. tutz (2009). ridge estimation for multinomial logit models with sym-\nmetric side constraints. technical report 67, lmu, department of statistics.\nzahid, f. m. and g. tutz (2010). multinomial logit models with implicit variable selection.\ntechnical report 89, department of statistics lmu.\nzeger, s. l. (1988). commentary. statistics in medicine 7, 161\u2013168.\nzeger, s. l. and p. j. diggle (1994). semi-parametric models for longitudinal data with appli-\ncation to cd4 cell numbers in hiv seroconverters. biometrics 50, 689\u2013699.\nzeger, s. l. and m. r. karim (1991). generalized linear models with random effects; a gibbs\u2019\nsampling approach. journal of the american statistical association 86, 79\u201395.\nzeileis, a., c. kleiber, and s. jackman (2008). regression models for count data in r. journal\nof statistical software 27.\nzhang, h. (1998). classification trees for multiple binary responses. journal of the american\nstatistical association 93, 180\u2013193.\nzhang, h. and b. singer (1999). recursive partitioning in the health sciences. new york:\nspringer\u2013verlag.\nzhang, q. and e. ip (2011). generalized linear model for partially ordered data. statistics in\nmedicine (to appear).\nzhao, l. p. and r. prentice (1990). correlated binary regression using a quadratic exponential\nmodel. biometrika 77, 642\u201348.\nzhao, l. p., r. l. prentice, and s. self (1992). multivariate mean parameter estimation by\nusing a partly exponential model. journal of the royal statistical society b 54, 805\u2013811.\nzhao, p., g. rocha, and b. yu (2009). the composite absolute penalties family for grouped\nand hierarchical variable selection. annals of statistics 37, 3468\u20133497.\nzhao, p. and b. yu (2004). boosted lasso. technical report, university of california, berkeley,\nusa.\nzheng, b. and a. agresti (2000). summarizing the predictive power of a generalized linear\nmodel. statistics in medicine 19, 1771\u20131781.\nzheng, s. (2008). selection of components and degrees of smoothing via lasso in high dimen-\nsional nonparametric additive models. computational statistics & data analysis 53, 164\u2013175.\nzhu, j. and t. hastie (2004). classification of gene microarrays by penalized logistic regres-\nsion. biostatistics 5, 427\u2013443.\nzhu, z., w. fung, and x. he (2008). on the asymptotics of marginal regression splines with\nlongitudinal data. biometrika 95(4), 907.\nzou, h. (2006). the adaptive lasso and its oracle properties. journal of the american statis-\ntical association 101(476), 1418\u20131429.\nzou, h. and t. hastie (2005). regularization and variable selection via the elastic net. journal\nof the royal statistical society b 67, 301\u2013320.\nzweig, m. and g. campbell (1993). receiver-operating characteristic (roc) plots: a funda-\nmental evaluation tool in clinical medicine. clinical chemistry 39, 561\u2013577.\n\n "}, {"Page_number": 557, "text": "author index\n\nabe, m., 299\nabramowitz, m., 408\nagrawala, a., 457, 458\nagrest, a:, 358\nagresti, a., 117, 118, 119, 140, 239, 260,\n265, 321, 345, 358, 360, 365, 371,\n391, 392, 393, 409, 418, 420, 426\n\naitkin, m., 358, 409, 410, 423, 424, 425\nakaike, h., 113, 501, 502\nalbert, a., 85, 455\nalbert, j., 265\naliferis, c., 481\naltman, r., 474\nambroise, ch., 293, 314\nan, m., 405\nananth, c., 266\nanbari, m., 158\nanderson, d., 15, 113, 409, 410\nanderson, j., 85, 148, 260, 265, 455, 475,\n\n482\n\nandr\u00e9s de, s., 329, 459, 481\naranda-ordaz, f., 130\nardekani, a., 307\narmstrong, b., 241, 265\natkinson, a., 119\naugustin, t., 327, 328, 460, 474, 481\navalos, m., 293, 314\nazzalini, a., 92, 419\n\nb\u00f6ckenholt i., 239\nb\u00f6rsch-supan a., 238\nb\u00fchlmann, p., 179, 314, 462, 463, 466, 467\nb\u00fchlmann, p., 154, 163, 164, 355, 356\nbarla, a., 308\nbates, d., 409\nbaumgarten, m., 126\nbeck, g., 369, 393\nbeelitz, c., 293, 314\nbell, r., 482\nbellman, r., 283\nben-akiva m., 238\nbender, r., 242, 251\nbenedetti, j., 358\nberger, r., 385, 388\nbergsma, m., 371\n\nberkson, j., 49\nbesag, j., 415\nbhapkar, v., 392\nbinder, h., 166, 253, 293, 314, 467\nbishop, y., 345, 349, 352, 358, 468, 471\nblair, v., 148\nblanchard, g., 457\nbliss, c., 49\nbloomfield, c., 430, 474\nbondell, h., 156, 175, 176\nbonney, g., 366\nbooth, j., 410\nborrero m., 239\nbotstein, d., 474\nboulesteix, a., 327, 328, 331, 460, 473,\n\n474, 481, 482\n\nbousquet, o., 457\nbowman, a., 92\nboyles, r., 495\nbradley, r., 229, 238\nbraga-neto, u., 448\nbrant, r., 252, 265\nbreiman, l., 14, 145, 179, 317, 322, 324,\n\n329, 458, 459, 461, 467\n\nbreslow, n., 193, 391, 412, 414\nbrezger, a., 278, 313\nbrillinger, d., 410\nbrown, c., 92\nbrown, m., 358\nbrown, p., 474\nbrownstone, d., 238\nbuja, a., 296, 437\nburnham, k., 15, 113\n\ncaffo, b., 405, 418, 426\ncaligiuri, m., 143, 430, 474\ncameron, a., 184, 203\ncampagna, d., 481\ncampanara, s., 481\ncampbell, g., 450, 451, 453\ncampbell, m., 265, 482\ncandes, e., 162\ncannata, n., 481\ncantoni, e., 393\ncarlin, b., 416\n\n "}, {"Page_number": 558, "text": "546\n\nauthor index\n\ncarroll, r., 87, 96, 119, 140, 284, 302, 313,\n\n385, 393, 401, 421\n\ncasella, g., 179, 400\nceleux, g., 455, 495\ncelgato, b., 481\nchambers, c., 18\nchang, c., 324\nchatterji, b., 481\nchen, j., 426\nchen, s., 149\nchen, y., 329, 459\ncheng, j., 482\nchhikara, r., 56\nchib, s., 265, 416\nchierici, m., 308\nchristensen, r., 354, 358\nchristmann, a., 85, 119, 457\nciampi, a., 324\nclaeskens, g., 15, 314, 426\nclark, l., 324\nclayton, d., 412, 414\ncleveland, w., 283\ncochran, w., 140\ncoller, h., 143, 430, 474\ncolonius h., 238\nconaway, m., 392\nconolly, m., 369\nconoway, m., 392\nconsul, p., 198\ncooil, b., 115\ncook, r., 15, 23, 97\ncopas, j., 119\ncordeiro, g., 66\ncornell, r., 126\ncoste, j., 482\ncover, t., 457\ncox, c., 265\ncox, d., 40, 65, 115, 117, 389, 482\ncrainiceanu, c., 285\ncramer, j., 48, 49, 402\ncreel, m., 200\ncressie, n., 187, 222\ncrowder,m., 134\ncrump, k., 126\ncurrie, i., 288\nczado, c., 130, 131, 198, 199\n\ndahinden, c., 355, 356\ndale, a., 393\ndarroch, j., 347, 348, 349\ndaumer, m., 473, 474, 481, 482\n\ndavidian, m., 426\ndavis, c., 383, 396\ndavis, l., 383\ndaye, z., 159\nde boek, p., 420\ndean, c., 194\ndeb, p., 193\ndeming, w., 352\ndemoraes, a., 482\ndempster, a., 494, 495\ndeng, d., 204\ndenison, d., 313\ndetels, r., 395\ndettling, m., 467\ndey, d., 79\ndeylon, b., 495\ndiaz-uriarte, r., 329, 459, 481\ndickey, d., 23\ndiebolt, j., 495\ndierckx, p., 272\ndietterich, t., 458\ndiggle, p., 395, 426\ndillon, w., 239\ndobson, a., 79\ndodd, l., 451\ndomeniconi, c., 481\ndonner, a., 265, 482\ndonoho, d., 149\ndowning, j., 143, 430, 474\ndreesman, j., 181\nduchon, j., 286\ndudoit, s., 454, 458, 467, 474, 481\nduffy, d., 85, 148, 191\nduncan, d., 246\ndunn, o., 453\ndunsmore, i., 482\ndunson, d., 416\nd\u00e9metrio, c., 134\n\nedwards, d., 358\neerdewegh, p., 329\nefron, b., 79, 115, 116, 151, 433, 447, 448,\n\n455\n\neilers, p., 277, 288, 302, 310, 313\nemerick, m., 355, 356\nemmert-buck, m., 307\neymann, a., 235\n\nfahrmeir, l., 15, 66, 72, 79, 85, 130, 144,\n145, 179, 253, 285, 304, 305, 313,\n314, 367, 383, 409, 416, 492\n\n "}, {"Page_number": 559, "text": "author index\n\nfamoye, f., 199\nfan, j., 140, 151, 159, 160, 161, 302, 313\nfarewell, v., 265\nferrara, g., 131\nfienberg, s., 187, 345, 349, 352, 358\nfinney, d., 41, 42\nfirth, d., 66, 79\nfitzmaurice, g., 265, 369, 371, 382, 419\nfix, e., 457\nfleiss, j, 393\nflemming, j., 393\nflury, b., 453\nfokianos, k., 367\nfolks, j., 56\nfollam, d., 423\nfowlkes, e., 119\nfr\u00fchwirth-schnatter, s., 425\nfrank, e., 482\nfrank, i., 143, 147\nfreund, y., 460\nfridlyand, j., 454, 458, 467, 474, 481\nfriedman, j., 131, 143, 147, 151, 163, 166,\n175, 239, 283, 287, 308, 310, 317,\n322, 324, 328, 329, 430, 453, 455,\n461, 462, 463, 464, 465, 467, 481\n\nfriend, y., 164\nfrost, h., 144, 145\nfu, w., 147, 148, 151, 384\nfukunaga, k., 444\nfung, w., 393\nfurlanello, c., 308\nfurnival, g., 145\n\ngaasenbeck, m., 143, 430, 474\ngamerman, d., 416\ngarber, m., 474\ngatsanis, c., 451\ngay, d., 79\ngenkin, a., 151\ngenter, f., 265\ngertheiss, j., 112, 175, 178, 303, 311, 312,\n\n482\nghosh, s., 79\ngianola, d., 410\ngibbons, r., 409\ngijbels, i., 140, 277, 313\ngirosi, f., 471\nglonek, g., 371\ngneiting, t., 437\ngoldberg, m., 126\n\n547\n\ngolub, t., 143, 430, 474\ngoodman, l., 116, 261, 358, 392\ngoormastic, m., 369\ngourieroux, c., 136\ngraepel, t., 482\ngrandvalet,y., 293, 314\ngreen, d., 275, 287, 313\ngreen, p., 415\ngreene, t., 453\ngreene, w., 216, 238\ngreenland, s., 260\ngreeno, j., 230\ngrindle, s., 329, 459\ngrizzle, j., 120, 246, 260, 265\ngroll, a., 417, 422, 426\ngrouven, u., 242, 251\ngruen, b., 425\ngschoessl, s., 198\ngu, c., 313\nguess, h., 126\ngunopulos, d., 481\nguo, y., 455\ngupta, p., 199, 204\ngupta, r., 199, 204\n\nh\u00e4rdle, w., 92\nhaagenars, j., 371\nhaberman, s., 66, 85, 116, 358\nhackett, p., 307\nhaenszel, w., 140\nhall, j., 329, 459\nhall, m., 482\nhall, p., 140\nhalversen, k., 391\nhamerle, a., 360\nhan, x., 329, 459\nhans, c., 179\nhansen, m., 313, 329\nhardin, d., 481\nharrell, f., 15, 265\nharrington, d., 392\nhart, j., 426\nhart, p., 457\nhartzel, j., 409, 418\nharville, d., 400, 413, 414, 418, 483\nhastie, t., 18, 148, 151, 154, 155, 159,\n163, 166, 179, 239, 281, 283, 284,\n287, 292, 294, 296, 299, 302, 308,\n313, 430, 455, 461, 464, 467,\n481\n\n "}, {"Page_number": 560, "text": "548\n\nauthor index\n\nhauck, w., 365, 426\nhauk, w., 426\nhausman, j., 228, 229, 231, 238\nhavranek, t., 358\nhayward, l., 329\nhe, x., 313, 393\nheagerty, p., 383, 385, 419, 426\nhechenbichler, k., 265, 482\nhedeker, d., 409\nheim, a., 259\nheld, l., 278\nhennevogl, w., 410, 417, 418\nherbrich, r., 482\nheyde, c., 79\nheyse, j., 92\nhigdon, d., 415\nhilbe, j., 203\nhinde, j., 134, 194, 409, 410\nhinkley, d., 65\nhitt, b., 307\nhjort, n., 15\nho, t., 458\nhoaglin, d., 22\nhobert, j., 410\nhodges, j., 457\nhoefsloot, h., 307\nhoerl, a., 147\nhogg, s., 324\nholland, p., 345, 349, 358\nholtbr\u00fcgge, w., 260\nhornik, k., 324, 325, 327, 329\nhorowitz, j., 130\nhosmer, d., 92, 180\nhothorn, t., 305, 324, 325, 327, 328, 329,\n\n462, 463, 466, 467\n\nhristache, m., 140\nhsiao, c., 451\nhsiao, ch., 426\nhuang, x., 426, 459\nhuard, c., 143, 430, 474\nhung, x., 329\nhurvich, c., 501\nh\u00e4rdle, w., 130, 140, 313\n\nibrahim, j., 426\nichimura, h., 140\nim, s., 410\nip, e., 260\n\njackman, s., 193, 199, 204\n\njames, g., 162, 309\njank, w., 495\njansen, j., 410, 418\njeng, x., 159\njensen, k., 450\njeon, y., 482\njoe, h., 119, 198\njohn, g., 473\njones, m., 481\njones, r., 426\njorgenson, b., 70\njuditsky, a., 140\njurman, g., 308\n\nkalbfleisch, j., 365, 426\nkarim, m., 416\nkarimi, a., 181\nkaslow, r., 395\nkauermann, g., 285, 296, 299, 302, 314,\n\n385\n\nkaufmann, h., 66, 85, 130, 367, 492\nkedem, b., 367\nkeenan, s., 450\nkem\u00e9ny, p., 360\nkennard, r., 147\nkezouh, a., 358\nkhalili, a., 426\nkim, k., 383\nkinney, s., 416\nkleiber, ch., 193, 199, 203, 204\nklein, r., 140\nkleinbaum, d., 266\nkneib, t., 15, 79, 179, 304, 305, 416\nkneight, k., 159, 175, 310\nkoch, g., 120\nkockelkorn, u., 28\nkoenker, r., 313\nkohn, e., 307\nkohn, r., 313\nkohovi, r., 473\nkong, f., 79\nkoong, a., 308\nkooperberg, c., 313, 329\nkrantz, d., 4, 229\nkrishnan, t., 495\nkrivobokova, t, 285\nkrivobokova, t., 314\nkruskal, w., 116\nkumar a., 239\nkurland, b., 426\n\n "}, {"Page_number": 561, "text": "author index\n\nkuss, o., 92\nk\u00fcnsch, h., 96\n\nlaara, e., 256\nlafferty, j., 356\nlaird, n., 369, 392, 393, 414, 419, 494,\n\n495\n\nlambert, d., 136, 199, 423\nland, s., 175, 308, 310, 313\nlander, e., 430, 474\nlandis, r., 383\nlandwehr, j., 119\nlanfranchi, g., 481\nlang, j., 345, 353, 371, 420\nlang, s., 15, 79, 278, 293, 304, 313, 314\nlauritzen, s., 346, 347, 348, 349, 352, 358\nlausen, b., 328\nlavielle, m., 495\nlawless, j., 145, 194\nle, q., 308\nlecessie, s., 93, 116, 148\nlee, j., 481\nlee, w., 451\nleisch, f., 425\nleitenstorfer, f., 119, 132, 313\nlemeshow, s., 92, 180\nleng, c., 302\nlerman s., 238\nleroux, b., 382\nlesaffre, e., 265, 393, 455, 482\nlevin, b., 393\nlevine, p., 307\nlevy, s., 481\nli, r., 151, 159, 160, 161\nli, w., 140\nli, y., 314\nliang, k., 136, 369, 373, 375, 377, 378,\n\n379, 426\n\nlin, x., 393, 414, 421\nlin, y., 153, 154, 482\nlindsey, j., 426\nlineham, w., 307\nlinton, o., 313\nliotta, l., 307\nlipsitz, s., 265, 383, 392\nliu, i., 409\nliu, q., 265, 409, 427\nlloyd, c., 387, 388\nloader, c., 283, 313\nloh, m., 143, 430, 474\n\n549\n\nloh, w., 327\nlongford, n., 399, 426\nloomis, j., 200\nlouis, t., 495\nluce, r., 4, 228, 229\nlunetta, k., 329\n\nm\u00fcller, h., 450\nmaddala, g., 238\nmagder, l., 426\nmagnus, j., 408\nmalley, j., 329\nmallick, b., 79, 313\nmancl, l., 382\nmantel, n., 140\nmargolin, b., 205\nmarks, s., 453\nmarra, g., 293, 313\nmarx, b., 15, 79, 277, 288, 302, 310, 313\nmason, w., 414\nmassart, p., 457\nmasters, g., 261\nmatthews, j., 256\nmccullagh, p., 51, 66, 78, 79, 135, 136,\n140, 185, 243, 246, 257, 265, 371\n\nmcculloch, c., 400, 414, 426\nmcfadden, d., 116, 215, 226, 228, 231,\n\n232, 233, 238\n\nmckinney, s., 324\nmclachlan, g., 423\nmclachlan, g., 425, 430, 455, 481, 495\nmcnemar, q., 386\nmee, r., 414, 418\nmehta c., 257\nmehta, c., 257\nmeier, l., 154, 179, 314\nmeilijson, i., 495\nmengerson, k., 415\nmerler, s., 308\nmesirov, j., 143, 430, 474\nmiller, a., 145\nmiller, i., 459\nmiller, l., 329\nmiller, m., 383\nmiller, r., 328\nmin, a., 199\nmitchell, h., 118, 119\nmkhadri, a., 158, 455\nmolenberghs, g., 265, 393, 426\nmolinaro, a., 448\n\n "}, {"Page_number": 562, "text": "550\n\nmonfort, a., 136\nmoore, d., 136\nmorgan, b., 127, 130\nmorgan, j., 317, 329\nmorin, r., 481\nmoulines, e., 495\nmoulton, l., 383\nmuggeo, v., 131\nmullahy, j., 200, 201, 204\nmunk, a., 131\n\nnadaraya, e., 282\nnagelkerke, n., 117\nnaik, p., 140\nnarasimhan, b., 308\nnasroallah, a., 455\nnelder, j., 51, 78, 79, 135, 140, 185\nneudecker, h., 408\nneuhaus, j., 365, 426\nnewson, r., 119\nng, p., 313\nni, x., 426\nnyquist, h., 147, 148\n\noberhauser, c., 178\nobermayer, k., 482\nogden, r., 274\nohman-strickland, p., 426\nolshen, r., 317, 322, 324, 329\nopsomer, j., 296, 314\nornstein, d., 307\nosborne, m., 151, 313\nosius, g., 119, 222\nostrow, d., 395\n\npaik, c., 393\npaik, m., 481\npalmgren, j., 353\npan, w., 329, 459\npantula, s., 23\npark, m., 148, 151, 481\npark, s., 329, 459\npark, t., 179\nparmigiani, g., 355, 356\nparthasarthy, g., 481\npatel, n., 257\npatterson, h., 400\npaul, s., 204\npaweletz, c., 307\npederson, s., 119\n\nauthor index\n\npeel, d., 423, 425\npeng, j., 481\npepe, m., 450, 451\npeterson, b., 265\npetricoin, e., 307\npetry, s., 132, 156\npfeiffer, r., 448\nphair, j., 395\nphillips, r., 265, 475, 482\npiccarreta, r., 329, 482\npiegorsch, w., 126, 205\npierce, d., 94, 409, 427\npierce, g., 360\npigeon, j., 92\npinheiro, j., 409\npoggio, t., 471\npohlmeier, w., 204\npollastri, g., 482\npolt, b., 395\npoortema, k., 136\nportnoy, s., 313\npowell, j., 140\npregibon, d., 78, 79, 119, 130, 324\npreisler, m., 410\nprentice, r., 87, 130, 135, 369, 391\npresnell, b., 151, 313\npritscher, l., 383\npulkstenis, e., 92, 265\npyke, r., 87\n\nqaqish, b., 373, 375, 377, 378, 379\nqu, y., 369\nquinlan, j., 319, 329\n\nradchenko, p., 162\nradelet, m., 360\nraeside, d., 481\nraftery, a., 437\nramsey, j., 307, 309, 311\nrasch, g., 389, 420\nravikumar, p., 356\nrawlings, j., 23\nrayens, w., 453\nread, i., 187, 222\nreich, b., 156, 175, 176\nreinsch, c., 275\nreithinger, f., 421, 422, 426\nrhode, c., 405\nriani, m., 119\nriccadonna, s., 308\n\n "}, {"Page_number": 563, "text": "author index\n\nridgeway, g., 166\nrinaldo, c., 395\nripley, b., 180, 324, 430, 444, 445, 458,\n\n468, 470, 481, 482\n\nrobinson, g., 401\nrobinson, t., 92, 265\nrocha, g., 154, 235\nroeder, k., 136\nrogers, c., 101, 102\nroi, l., 148\nrojek, d., 222\nromualdi, c., 481\nronchetti, e., 393\nrosner, b., 369\nrosset, s., 151, 159, 175, 310\nrotzer, j., 119\nrousseeuw, p., 85, 119\nrubin, d., 494, 495\nruckstuhl, a., 131\nrudolfer, s., 265, 482\nrue, h., 278\nrumelhart d., 230\nruppert, d., 131, 132, 140, 284, 296, 302,\n\n313, 314, 385, 401, 421\n\nrust, r., 115\nryan, t., 15\n\nsabai, c., 391\nsampson, a., 260\nsantner, t., 85, 130, 148, 191\nsaunders, m., 149, 159, 175, 310\nsch\u00e4fer, h., 450\nschaefer, p., 118, 119\nschaefer, r., 148\nschafer, d., 94\nschall, r., 414\nschapire, r., 164, 460\nscholz, t., 299\nschuhmacher, m., 260\nschwarz, h., 113\nscott, a., 87\nsearle, s., 400, 414, 426\nsecrest, d., 408\nseeber, g., 145\nsegal, j., 329\nsegerstedt, b., 145, 148\nself, s., 369\nseliske, p., 126\nshapire, r., 163\nshen, y., 437\n\n551\n\nshi, g., 308\nshih, y., 327, 328\nshoemaker, a., 119\nshuster, j., 388\nsidik, k., 385, 388\nsiegmund, d., 328\nsilverman, b., 275, 287, 307, 309, 311,\n\n313, 481\n\nsimon, r., 448\nsimone, c., 307\nsimonoff, j., 358\nsinger, b., 329\nsingh, h., 260\nsingh, k., 199\nsinghal, k., 145\nslawski, m., 311, 473, 482\nsloan, m., 241, 265\nslonim, d., 143, 430, 474\nsmall, k., 238\nsmilde, a., 308\nsmit, s., 307\nsmith, a., 313\nsmith, d., 127\nsmith, m., 313\nsmith, p., 313\nsnell, e., 40, 117, 246\nsobehart, j., 450\nsoltys, s., 308\nsong, s., 481\nsonquist, j., 317, 329\nsoofi, e., 119\nspady, r., 140\nspeckman, j., 126\nspeed, t., 347, 348, 349, 454, 458, 467,\n\n474, 481\nspokoiny, v., 140\nst\u00fctzle, w., 131\nstanikov, a., 481\nstarmer, c., 120\nsteadman, s., 241, 265\nstefanski, l., 96\nstegun, i., 408\nstein, c., 433\nstein, r., 450\nsteinberg, s., 307\nsteinwart, i., 457\nstephan, f., 352\nstiratelli, r., 414\nstock, j., 140\n\n "}, {"Page_number": 564, "text": "552\n\nauthor index\n\nstocker, t., 140\nstone, c., 313, 329, 458\nstone, j., 317\nstram, 383\nstram, d., 383\nstrobl, c., 327, 328, 329, 460, 474, 481\nstroud, a., 408\nstuetzle, w., 437\nstukel, t., 130\nsuissa, s., 388\nsuppes, p., 4\n\nt\u00fcchler, r., 416\ntamayo, p., 143, 430, 474\ntang, h, 140\ntanner, m., 495\ntao, t., 162\nterry m., 229\ntheil, h., 116\nthomson, r., 400\nthurner, p., 235\nthurstone, l., 227\ntibshirani, r., 147, 149, 151, 154, 159, 163,\n166, 175, 179, 239, 283, 284, 287,\n292, 294, 296, 299, 302, 308, 310,\n313, 430, 447, 448, 455, 461, 464,\n467, 481\n\ntjur, t., 392\ntoledano, a., 451\ntoppo, s., 481\ntripathi, r., 199, 204\ntrivedi, p., 184, 193, 203\ntrognon, a., 136\ntroyanskaya, o., 474\ntrucco, c., 307\ntruong, y., 313, 329\ntsai, c., 140\ntsai, h., 328\ntsamardinos, i., 481\ntsiatis a., 257\ntsiatis, a., 92, 136, 257\ntukey, j., 164, 463\nturlach, b., 151, 313\ntutz, g., 79, 112, 119, 132, 157, 158, 166,\n175, 178, 234, 239, 265, 299, 302,\n305, 311, 312, 313, 314, 360, 409,\n410, 417, 418, 421, 422, 426, 467,\n482\n\ntversky, a., 4, 229\ntweedie, m., 56\n\nulbricht, j., 157, 158\nulrich, v., 204\n\nvalle, g., 481\nvan de geer, s., 179, 314\nvan de geer, s., 154\nvan den broek, j., 204\nvan der linde, a:, 119\nvan houwelingen, j., 93, 116\nvelassco, a., 307\nvenables, w., 180\nvenot, d., 482\nverbeke, g., 426\nverhaselt, a., 277\nvidakovic, b., 274\nvoung, q., 114\n\nwacholder, s., 126\nwahba, g., 287, 313\nwainwright, m., 356\nwalker, s., 246\nwalter, e., 482\nwand, m., 140, 284, 302, 313, 401, 421\nwang, h., 302\nwang, l., 379\nwang, s., 87\nwang, sh., 79\nwang, x., 79\nwang, z., 482\nware, j., 383, 393, 414\nwassermann, d., 482\nwatson, g., 282\nwatson, p., 265, 482\nwebster, k., 265, 482\nwedderburn, r., 78, 79, 192\nwei, 383\nwei, g., 495\nwei, l., 383\nweinberg, c., 205\nweisberg, s., 15, 23, 131, 140\nweissfeld, l., 241, 265\nwelsch, r., 22, 79\nwelsh, a., 131, 140, 385, 393\nwermuth, n., 115\nwhittaker, j., 346, 358\nwhittemore, a., 126\nwiegand, l., 307\nwild, c., 87, 299, 385\nwilkinson, g., 101, 102\nwilliams, g., 369\n\n "}, {"Page_number": 565, "text": "author index\n\nwilliams, o., 246, 260, 265\nwilliams, r., 133, 136\nwilliamson, j., 383\nwillmot, g., 194\nwilson, m., 420\nwilson, r., 145\nwindorfer, a., 181\nwinklemann, r., 203\nwise, d., 228, 229\nwolfe, r., 148\nwolfinger, r., 414\nwong, g., 414\nwood, k., 307\nwood, s., 274, 280, 284, 287, 292, 293,\n\n313, 421\n\nwu, j., 495\n\nxia, t., 79\nxia, y., 140, 302\nxie, m., 379\n\nyang, y., 379, 481\nyasai-ardekani, m., 119\nye, m., 433\n\n553\n\nyee, t., 299, 314, 385\nyellott, j., 215, 227, 238\nyu, b., 151, 154, 163, 164, 235, 462, 467\nyu, y., 131, 132, 140\nyuan, m., 153, 154\n\nzahid, f., 234\nzeger, s., 373, 375, 377, 378, 379, 383,\n\n385, 395, 416, 419, 426\n\nzeileis, a., 193, 199, 203, 204, 324, 325,\n\n327, 329\nzhang, d., 421, 426\nzhang, h., 329, 426\nzhang, q., 260\nzhang, w., 302\nzhao, l., 151, 369\nzhao, p, 154, 235\nzheng, b., 117, 118, 119\nzheng, sh., 314\nzhu, j., 159, 175, 198, 310, 455\nzhu, l., 140\nzhu, z., 393\nzou, h., 152, 153, 154, 155, 159, 179\nzweig, m., 451\n\n "}, {"Page_number": 566, "text": "subject index\n\nactual prediction error, 431\nactual probability of misclassification,\n\n434\nadaboost, 460\n\ndiscrete, 460\nreal, 461\n\naic, 113, 284, 356, 501\nanalysis of deviance, 69, 354\nanscombe residuals, 94\napparent error rate, 432\nassociation measures, 118\nassociation models, 331\nauc, 450\n\nbackfitting algorithm, 294\nbagging, 458\nbase procedure, 462\nbasis expansions, 270\nbasis pursuit, 149\nbayes classifier, 434\nbayes risk, 444\nbayes rule, 439, 440\n\nfor general loss functions, 444\n\nbayes theorem, 440\nbayesian approach\nselection, 178\n\nbernoulli distribution, 56\nbest linear unbiased prediction, 400\nbeta distribution, 134, 489\nbeta function, 134\nbeta-binomial distribution, 486\nbeta-binomial model, 134\nbic, 113, 356\nbinary response, 29\n\nas dichotomized latent variables,\n\n34\n\nbinomial distribution, 31, 57, 485\nblup, 400\nboosting, 163, 292, 460\n\nfunctional gradient descent, 461\nlikelihood-based, 466\nlogitboost, 166, 467\nblockwise, 166\ncomponentwise, 164\nfor generalized linear models, 165\n\nbootstrap, 447\nbradley-terry-luce model, 229\nbrier score, 435, 438\n\ncanonical discriminant variables, 453\ncanonical link, 54, 61\ncase deletion, 97\ncategorical effect modifier, 302\ncategorical predictor\n\nselection, 173\nselection within, 173\n\ncategorized predictor\n\nselection, 176\n\ncategory-specific effect, 250\ncategory-specific variables, 215\ncauchy model, 126\nchain between vertices, 346\nchoice models, 226\nchoice-based sampling, 86\nchordal model, 352\nclassification, 429, 434\n\nlinear, 451\nnon-parametric, 457\nnormally distributed predictors, 442\nparametric, 451\nquadratic, 453\n\nclassification and regression trees, 317\nclassification trees, 317\nclique, 347\nclustering of categories, 174, 175\ncoding, 108\n\nsplit, 110\n\ncoefficient of determination, 24, 114\n\nempirical, 114\npopulation, 114\n\ncollapsibility, 255, 348\ncomplementary log-log model, 124\ncompletely separated, 85\nconcordance measures, 118\nconcordant pair, 118\nconditional independence, 331\nconditional maximum likelihood estimates,\n\n391, 420\n\nconstrained regression, 147\ncontingency tables, 332\n\n "}, {"Page_number": 567, "text": "subject index\n\ncontinuation ratio logits model, 254\ncontinuous ranked probability score, 438\ncontrasts, 107\ncook\u2019s distance, 97\ncorrelated response, 363\ncost complexity, 324\ncovariance penalty, 433\ncross-validation, 283, 446\n\nk-fold, 446\nerror, 447\nmonte carlo, 447\nfor selection of smoothing parameter,\n\n283\n\ngeneralized, 284\ncrossing operator, 103\ncumulative logit model, 244\ncumulative maximum extreme value model,\n\n248\n\ncumulative minimum extreme value model,\n\n246\n\ncumulative model, 243\ncumulative odds, 245\ncurse of dimensionality, 93, 283\n\ndantzig selector, 162\ndegrees of freedom of a fit, 433\ndeviance, 67, 87, 186, 220, 320\n\nfor binary response, 87\nfor grouped observations, 72\nfor log-linear models, 353\nfor multinomial distribution, 220\nfor proportions, 89\ngoodness-of-fit, 89\npredictive, 436\npartitioning, 69\npredictive, 437\nscaled, 67\n\ndiagnostic checks, 93, 222\ndiagonal discriminant analysis, 454\ndifferences of order d, 277\ndirect prediction, 434\ndirichlet distribution, 488\ndiscordant pair, 118\ndiscrete hazard, 248, 253, 263, 266\ndiscriminant analysis, 430\ndiscriminant functions, 440\ndiscrimination\nlinear, 451\nlogistic, 455\nquadratic, 453\n\n555\n\ndispersion parameter, 51\ndistribution, 227\n\nbeta, 489\nbeta, 134\nbeta-binomial, 486\nbinomial, 31, 135, 485\ndirichlet, 488\nexponential, 488\nexponential distribution, 53\ngamma-, 54, 488\ngeometric, 59\nhypergeometric, 486\ninverse gaussian, 55, 488\nlogistic, 35, 487\nmaximum extreme value, 487\nminimum extreme value distribution,\n\n487\n\nmultinomial, 333\nmultinomial, 331, 486\nnegative binomial, 59, 485\npoisson, 58, 331, 333, 485\nproduct-multinomial, 331, 338\nproduct-multinomial , 333\nscaled binomial, 31, 57\nbernoulli, 56\nbinomial, 57, 83, 84\ngompertz, 124\ngumbel, 125\nhypergeometric, 138\nmultinomial, 209\nnon-central hypergeometric, 139\nnormal, 487\nproduct-multinomial, 339, 350,\n\n353\n\ndot operator, 103\ndummy variables, 108\n\nedges, 346\neffect\n\nfixed, 405\nrandom, 396, 405\n\neffect modifier, 301\ncategorical, 302\n\neffective degrees of freedom of a smoother,\n\n279\n\nem algorithm, 409, 424, 494\nensemble methods, 458, 468, 476\nentropy, 323, 436\nequicorrelation model, 374\nequidispersion property, 182\n\n "}, {"Page_number": 568, "text": "556\n\nerror\n\ncross-validation, 447\nsquared, 432\ntest, 446\n\nerror rate\n\napparent, 432\nexpected actual, 445\nglobal rate of misclassification, 439\nindividual, 440\nleave-one-out, 447\noptimal, 439\nresubstitution, 446\nreclassification, 446\n\nestimation\n\nmaximum likelihood, 494\n\nexpected actual rate of misclassification, 445\nexponential correlation model, 374\nexponential distribution, 53, 184, 488\nexponential model, 125\n\nfalse alarm rate, 448\nfamilies of link functions, 130\nfeed-forward networks, 468\nfisher matrix, 65, 84, 166, 186, 191, 219,\n\n252, 272, 290, 352\n\nfisher scoring, 76, 98, 109, 148, 157, 162,\n166, 168, 172, 234, 275, 282, 292,\n295\n\nfixed cells asymptotics, 91, 222\nforward stagewise additive modelling, 463\nfourier basis, 274\nfused lasso, 159, 175, 310\n\ngamma-distribution, 54, 195, 488\ngamma-poisson-model, 195\ngauss-hermite approximation, 408\ngauss-hermite integration, 505\ngeneralized additive models, 290\ngeneralized estimation equation, 376, 382\ngeneralized estimation function, 135, 136,\n\n376\n\ngeneralized linear model, 51\n\nfor continuous responses, 53\nfor discrete responses, 56\n\ngeneralized log-linear model, 371\ngeneralized poisson distribution, 198\ngeometric distribution, 59\ngini index, 319, 322, 327\ngini\u2019s concentration measure, 116\nglobal effect, 250\n\nsubject index\n\nglobal odds ratios, 383\nglobal probability of misclassification, 439\nglobal rate of misclassification, 439\nglobal variables, 215\ngompertz distribution, 124\ngoodness-of-fit, 186, 220\n\nfor log-linear models, 353\n\ngraph, 346\ngraphical models, 341, 345, 346\ngreedy function approximation, 463\ngroup lasso, 153, 177\ngrouped cox model, 248\ngrouped data, 62\ngumbel distribution, 125, 215, 227\n\nhard tresholding, 151\nhat matrix, 76, 96, 97\n\nfor glm, 76\nhessian matrix, 75\nheterogeneity, 133, 194\n\nunobserved, 133\n\nheteroscedastic, 34\nhierarchical models, 341\nhierarchically structured response, 223\nhit rate, 448\nhosmer-lemeshow test, 92, 222, 265\nhuber-loss, 465\nhurdle models, 200, 204\nhypergeometric distribution, 486\n\nidentity link model, 127\nimpurity, 322\nimpurity function, 322\nimpurity measures, 322\nimpurity of a tree, 323\nincreasing cells asymptotics, 222\nindependence from irrelevant alternatives,\n\n228\n\nindividual errors, 440\ninflated binomial variance, 135\ninfluential observations, 96\ninformation matrix, 65\n\nexpected, 65\nobserved, 65\n\ninteraction, 288\n\nthree-factor, 340\ntwo-factor, 338, 340\ncontinuous-by-continuous, 301\ncontinuous-by-discrete, 301\ndiscrete-by-continuous, 301\n\n "}, {"Page_number": 569, "text": "subject index\n\ninteraction terms, 102\nintraclass correlation coefficient, 398\ninverse gaussian distribution, 55, 488\nising model, 356\niterative proportional fitting, 352\niterative weighted least-squares fitting, 76\n\nkullback-leibler discrepancy, 435\nkullback-leibler distance, 88, 500\n\nlaplace\u2019s approximation, 412\nlasso, 147\n\nfused, 159, 175, 310\nadaptive, 152\ngroup lasso, 153\nlaw of rare events, 183\nleave-one-out cross-validation, 447\nleave-one-out error rate, 447\nlikelihood\n\nmaximum likelihood estimation, 494\n\nlikelihood ratio index, 116\nlikelihood ratio statistic, 67, 87, 104, 199,\n\n220\n\nlikelihood-based boosting, 466\nlinear hypotheses, 104, 223\nlink function, 52, 81\n\nfamilies, 130\nnonparametric fitting, 131\nselection, 170\n\nlocal polynomial regression, 281\nlocal regression, 281\nlog-likelihood\n\npenalized, 145\n\nlog-linear model, 331\n\nfor higher dimensions, 345\nfor three-way tables, 338\nfor two-way tables, 335\ninference, 350\nof independence, 337\n\nlog-log model, 125\nlog-odds, 31, 105, 210, 212, 259, 368, 370\nlogarithmic score, 436, 438\nlogistic discrimination, 455\nlogistic distribution, 35, 487\nlogit, 31\nlogit model, 33, 35, 37, 57\n\nwith (0-1)-coding of covariates, 43\nwith categorical predictor, 45\nwith continuous predictor, 37\nwith effect coding, 44\n\n557\n\nwith multivariate predictor, 41\nwith binary predictor, 42\n\nlogitboost, 166, 467\nloss\n\nl1, 430\n0-1, 434\nexponential, 464\nhinge, 456, 465\nhuber, 465\nlikelihood, 463\nsoft margin, 456\nsquared, 430\nsymmetrical, 444\n\nlow-rank smoothers, 281\n\nmain effect model, 102\nmain effects, 338, 340\nmallow\u2018s cp, 433\nmantel-haenszel statistic, 140\nmargin, 464\nmarginal cumulative response model, 383\nmarginal homogeneity, 385, 389\n\nfor multi categorical outcome, 391\nregression approach, 388\nmarginal logistic model, 389\nmarginal model, 372\nmarginalized random effects model, 419\nmarkov chain, 366\nmarkov-type transition models, 366\nmars, 328\nmatched pairs, 385\nmaximal complete set, 347\nmaximum extreme value distribution, 215\nmaximum extreme value distribution distri-\n\nbution, 487\nmaximum likelihood\n\nestimation, 63, 82\nestimation asymptotics, 66\nmaximum likelihood rule, 445\nmean actual prediction error, 431\nminimum discrimination information, 222\nminimum extreme value distribution distri-\n\nbution, 487\n\nminimum modified \u03c72-statistic, 222\nmixed model equations, 401\nmodel\n\nadjacent categories, 260\nbayesian approaches to mixed mod-\n\nels, 415\n\nbeta-binomial, 134\n\n "}, {"Page_number": 570, "text": "558\n\ncauchy, 126\nchordal, 352\ncomplementary log-log model, 124\nconditional, 364\nconditional logit, 389\ncumulative, 242, 243, 261\ncumulative logit, 244\ncumulative maximum extreme value,\n\n248\n\ncumulative minimum extreme value,\n\n246\n\nfinite mixture, 422\nfor binary data, 56\nfor binomial data, 57\nfor marginal cumulative response, 383\ngamma-poisson, 195\ngeneral cumulative , 249\ngeneralized linear mixed, 402, 405\ngeneralized log-linear, 371\ngraphical, 341\ngrouped cox, 248\nhierarchically structured , 257\nhurdle, 200\nlinear random effects, 396\nlog-linear, 331, 335, 368\nlogistic-normal, 402\nlogit, 35, 37, 57\nmarginal, 364, 372\nmarginal for multinomial responses, 382\nmixed multinomial, 418\nmultinomial logit, 210\nmultinomial probit, 227\nmultivariate generalized linear model,\n\n217\n\nnegative binomial, 194\nordinal mixed, 416\nordinal probit, 248\nordinal response, 242\npartial proportional odds, 250\npoisson regression, 185\nprobit, 35, 123, 404\nproportional hazards, 248\nproportional odds, 244\nquasi-likelihood, 78\nquasi-symmetry, 392\nrandom effects, 194, 364\nrandom intercept, 397\nrandom utility, 226\nregressive, 366\n\nsubject index\n\nregularization for the multinomial model,\n\n233\n\nsemiparametric mixed, 420\nsequential, 242, 253, 263\nsequential logit, 254\nsequential minimum extreme value, 255\nsingle-index, 131\nstructured additive regression, 289, 304\nsubject-specific, 364\nsymmetric conditional models, 368\nthreshold, 243\ntree-based, 317\nvector generalized additive, 299\nzero-inflated poisson, 199\nadjacent categories, 243\nassociation, 331\nbradley-terry-luce, 229\ndata-driven, 366\nexponential, 125\ngeneralized linear, 51\nidentity link , 127\nlinear mixed, 398\nlog-log, 125\nnested logit, 231\npair comparison, 229\nprobit-normal, 404\nrasch, 420\nsaturated, 67\nstereotype regression, 260\ntransition, 366\nvarying-coefficients models, 300\n\nmodel selection\n\nfor log-linear models, 354\n\nmonte carlo approximation, 408\nmonte carlo cross-validation, 447\nmosaic plot, 357, 360\nmulticategory responses, 207\nmultinomial distribution, 209, 486\n\nscaled, 210\n\nmultinomial logit model, 210\nmultinomial probit model, 227\nmultivariate adaptive regression splines, 328\nmultivariate generalized linear model, 217\n\nnadaraya-watson estimator, 282\nnatural parameter, 51\nnearest neighbor classifier, 457\nnearest neighborhood methods, 457\nnegative binomial distribution, 59, 194, 195,\n\n485\n\n "}, {"Page_number": 571, "text": "subject index\n\nnegative binomial model, 194\nnested, 207\nnested logit model, 231\nneural networks, 468\nnewton method with fisher scoring, 76\nnewton-raphson method, 75\nneyman-pearson discrepancy, 435\nnominal scale, 207\nnonparametric regression, 269\nnormal distribution, 487\nnumerical integration, 504\n\nobserved information matrix, 65\nodds, 31\nodds ratio, 33, 43, 44\n\nglobal, 383\n\noffset, 62, 190\none-against-all approach, 467\noptimal prediction, 431\noracle, 152\noracle property, 160\nordered predictors, 108, 174\nordinal\n\nvariable assessed, 241\n\nordinal models\n\ninference, 261\n\nordinal probit model, 248\nordinal scale, 207\nordinal variable\n\ngrouped continuous, 241\n\noverdispersion, 133, 192, 195, 198, 200,\n\n202\n\npair comparison, 229\npartial proportional odds model, 250\npartially linear model, 291\npartially ordered response categories, 260\npattern recognition, 430\npearson chi-squared discrepancy, 435\npearson residual, 93, 220, 222\npearson statistic, 74, 90, 220\npenalized gee, 384\npenalized log-likelihood, 109, 233, 276\npenalized regression, 147\npenalty, 109\n\ncovariance, 433\nfused lasso, 159, 310\nfusion type, 174\nlasso, 175\nridge, 110, 384\n\n559\n\nsmoothness, 110\nadaptive lasso, 152\nbridge, 147\ncorrelation-based, 157\nelastic net, 154\nfusion type, 159\ngroup lasso, 153\nlasso, 149\noscar, 156\nscad, 159\nweighted fusion, 159\n\npoisson distribution, 58, 182, 485\npoisson process, 184\npoisson regression, 185\n\nwith offset, 190\nwith overdispersion, 192\n\npolynomial terms, 102\npopulation coefficient of determination, 114\npopulation-averaged effects, 365\nposterior mean estimate, 411\nposterior mode estimator, 411\npower-divergence family, 221\nprediction, 429, 430\n\nby binary splits, 476\nfor ordinal outcomes, 474, 482\noptimal, 431\nensemble method, 476\nestimated, 431\n\nprediction error\nactual, 431\nmean actual, 431\nmean squared, 432\npredictive deviance, 436\nprinciple of maximum random utility, 215\nprobabilistic choice, 226\nprobabilistic choice system, 226\nprobit model, 35, 123\nproper scores, 437\nproportional hazards model, 248\nproportional odds assumption\n\ntesting, 251\n\nproportional odds model, 244\nproportional reduction in loss, 115\nproportional reduction in variation measure,\n\n115\n\nproportions, 31\npruning, 324\npseudo likelihood ratio test, 92\npseudo-r2, 116\n\n "}, {"Page_number": 572, "text": "560\n\nquadratic score, 435\nquantile regression, 431\nquasi-information matrix, 377\nquasi-likelihood, 78, 135, 192, 411\nquasi-standard-errors, 107\nquasi-symmetry log-linear models, 392\nquasi-variance, 107\nquasicompletely separated, 85\n\nradial basis functions, 274\nrandom effects, 194, 395\n\nmarginalized model, 419\n\nrandom forests, 458\nrandom utility maximization hypothesis, 226\nrandom utility model, 215, 226\nrasch model, 420\nreclassification, 446\nrecursive partitioning, 317\nregression trees, 317\nregressive logistic models, 366\nregularization, 233\nregularized discriminant analysis, 454\nrelative risk, 32\nreml, 400, 414\nreml estimates, 284\nresidual\n\nanscombe, 94\ndeviance, 94\npearson, 93\nstandardized pearson, 94\npearson, 220, 222\n\nresponse categories\n\nordinal, 416\nnested, 207\nnominal, 207\nordered, 207\n\nresponse function, 52\nrestricted maximum likelihood, 400\nresubstitution error rate, 446\nridge penalty, 110, 233\nridge regression, 147\nroc accuracy ratio, 450\nroc curves, 448\n\nsaturated model, 67, 336, 341\nscaled binomial distribution, 31, 57\nscaled binomials, 83\nscaled deviance, 67\nscaled multinomial distribution, 210\nscaled multinomials, 218\n\nsubject index\n\nscore function, 63, 64, 83, 147, 162, 219,\n\n272, 279\n\nlocal, 282\npenalized, 160, 234, 290\n\nscore statistic, 71, 252\nscore test, 106, 144, 223\nscoring rule, 437\nsensitivity, 448\nsequential logit model, 254\nsequential minimum extreme value model,\n\n255\n\nsequential model, 253\nsimple exponential family, 51\nsimple scalability, 229\nsingle-index model, 131, 140\nsmoothness penalty, 110\nsoft margin loss, 456\nsoft tresholding, 151\nspecificity, 448\nspline\n\nb-spline function of degree, 272\ncubic splines, 271\nnatural cubic smoothing spline, 275\npenalized splines, 277\npolynomial splines, 270\nregression splines, 270\nsmoothing splines, 274, 286\nspline function of degree k, 271\n\nsplit criteria, 319\nsquare root of a matrix, 491\nstandard splits, 319\nsteepest gradient descent, 462\nstereotype regression model, 260\nstrict stochastic ordering, 245\nsubgraph, 347\nsubject-specific parameter, 365, 389\nsubsampling, 447\nsufficient statistics, 120, 350, 351, 420\nsum of squares, 69\nsupervised learning, 430\nsupport vector, 456\nsupport vector classifier, 456\nsymmetrical loss function, 444\nsymmetry log-linear model, 392\n\ntaylor approximation, 491\ntensor-product basis, 285\ntest error, 446\ntest set, 446\nthree-factor interactions, 340\n\n "}, {"Page_number": 573, "text": "subject index\n\nthreshold model, 243\ntraining set, 446\ntree\n\npruning, 324\nroot, 318\nsize, 324\nterminal node, 318\n\ntruncated power series basis, 271\ntwo-factor interactions, 338, 340\n\nunbiased risk estimate (sure), 433\nunderdispersion, 133, 192, 196, 198, 202\n\nvalidation set, 446\nvariable selection, 151, 152, 154, 157, 162,\n\n170, 172, 175, 179\n\nbackwards, 144\nbest subset, 144\nconsistency, 159\nfilter approach, 473\n\n561\n\nwrapper approach, 473\nforward, 144\n\nvariance components, 398\nvarying coefficients, 301\nvector generalized additive model, 299\nvector-valued link function, 217\nvertices, 346\n\nwald statistic, 71\nwald test, 71, 106, 223\nweak learner, 164, 463\nwilkinson rogers notation, 102\nworking correlation, 373\nworking correlation matrix, 374\nworking covariance, 372, 373, 378, 382\nworking independence model, 374, 377\n\nzero-inflated counts, 198\nzero-inflated poisson model, 199\nzero-inflation, 203\n\n "}, {"Page_number": 574, "text": " "}]}